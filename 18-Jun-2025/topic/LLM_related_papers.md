# Optimizing Length Compression in Large Reasoning Models 

**Title (ZH)**: 在大型推理模型中优化长度压缩 

**Authors**: Zhengxiang Cheng, Dongping Chen, Mingyang Fu, Tianyi Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2506.14755)  

**Abstract**: Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at this https URL. 

**Abstract (ZH)**: 大型推理模型中的精炼：一种新的效率原则及其应用 

---
# AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes 

**Title (ZH)**: AgentDistill：无需训练的通用MCP框代理精简 

**Authors**: Jiahao Qiu, Xinzhe Juan, Yimin Wang, Ling Yang, Xuan Qi, Tongcheng Zhang, Jiacheng Guo, Yifu Lu, Zixin Yao, Hongru Wang, Shilong Liu, Xun Jiang, Liu Leqi, Mengdi Wang  

**Link**: [PDF](https://arxiv.org/pdf/2506.14728)  

**Abstract**: While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents. 

**Abstract (ZH)**: 基于大型语言模型代理知识蒸馏的研究：AgentDistill框架 

---
# Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack 

**Title (ZH)**: 双身人方法：通过基于提示的可转移 adversarial 攻击打破 LLM 代理的角色一致性 

**Authors**: Daewon Kang, YeongHwan Shin, Doyeon Kim, Kyu-Hwan Jung, Meong Hi Son  

**Link**: [PDF](https://arxiv.org/pdf/2506.14539)  

**Abstract**: Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelgänger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelgänger method. The experimental results demonstrate that the Doppelgänger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack. 

**Abstract (ZH)**: 自大型语言模型出现以来，提示工程现在使快速、低 effort 地创建多样化的自主代理成为可能，这些代理已广泛投入使用。然而，这种便利性引发了对底层提示的安全性、鲁棒性和行为一致性的紧迫担忧，同时还提出了防止这些提示被用户滥用的迫切挑战。在本文中，我们提出了一种“双胞胎方法”来展示代理被劫持的风险，从而暴露系统指令和内部信息。接着，我们定义了“对抗转移导致提示对齐崩溃 (PACAT)”级别来评估对此类对抗转移攻击的脆弱性。我们还提出了一种“对抗转移警示 (CAT)”提示来抵御双胞胎方法。实验结果表明，双胞胎方法可以破坏代理的一致性并暴露其内部信息。相比之下，CAT 提示能够有效防御这种对抗攻击。 

---
# LLM-Powered Swarms: A New Frontier or a Conceptual Stretch? 

**Title (ZH)**: LLM驱动的集群：一片新天地还是概念 overstretched？ 

**Authors**: Muhammad Atta Ur Rahman, Melanie Schranz  

**Link**: [PDF](https://arxiv.org/pdf/2506.14496)  

**Abstract**: Swarm intelligence traditionally refers to systems of simple, decentralized agents whose local interactions lead to emergent, collective behavior. Recently, the term 'swarm' has been extended to describe AI systems like OpenAI's Swarm, where large language models (LLMs) act as collaborative agents. This paper contrasts traditional swarm algorithms with LLM-driven swarms exploring how decentralization, scalability, and emergence are redefined in modern artificial intelligence (AI). We implement and compare both paradigms using Boids and Ant Colony Optimization (ACO), evaluating latency, resource usage, and behavioral accuracy. The suitability of both cloud-based and local LLMs is assessed for the agent-based use in swarms. Although LLMs offer powerful reasoning and abstraction capabilities, they introduce new constraints in computation and coordination that challenge traditional notions of swarm design. This study highlights the opportunities and limitations of integrating LLMs into swarm systems and discusses the evolving definition of 'swarm' in modern AI research. 

**Abstract (ZH)**: swarm智能 traditionally refers to 系统中简单且分散的代理通过局部交互产生 emergent 集体行为。最近，术语 ‘swarm’ 已被扩展，以描述 如OpenAI的 Swarm这样的AI系统，其中大型语言模型 (LLMs) 作为协作代理。本文探讨了传统 swarm 算法与由LLM驱动的swarm之间的区别，研究了去中心化、可扩展性和涌现性在现代人工智能 (AI) 中的新定义。我们使用Boids和蚁群优化 (ACO) 实现并比较了这两种范式，评估了延迟、资源使用和行为准确性。评估了基于云和本地的LLM在swarm中基于代理使用的适用性。尽管LLM提供了强大的推理和抽象能力，但它们在计算和协调方面引入了新的约束，挑战了传统swarm设计的概念。本文探讨了将LLM集成到swarm系统中的机会和局限性，并讨论了现代AI研究中 ‘swarm’ 定义的演变。 

---
# Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning 

**Title (ZH)**: 不要胡编乱造：在大模型微调中保留无知意识 

**Authors**: William F. Shen, Xinchi Qiu, Nicola Cancedda, Nicholas D. Lane  

**Link**: [PDF](https://arxiv.org/pdf/2506.14387)  

**Abstract**: Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning. 

**Abstract (ZH)**: 现有工作在缓解大规模语言模型（LLM）微调中的灾难性遗忘时，主要关注保留特定数据或任务，而严重忽视了通过安全对齐培养的关键能力的退化，特别是模型忠实地表达无知的能力。本文表明，在常规微调过程中，这种能力会受到显著削弱，导致诸如幻觉等不希望出现的行为。为应对这一新颖且高度实际的问题，我们提出了一种名为SEAT的简单而有效的微调方法，该方法既能保持微调性能，又能保留模型固有的承认自身无知的能力。SEAT结合了两个关键组成部分：（1）稀疏训练以限制激活漂移，以及（2）一种新的实体扰动方法，带有KL散度正则化，旨在对抗知识纠缠。实验结果表明，SEAT在保留无知意识的同时显著优于基线模型，为LLM微调提供了更为 robust 的解决方案。 

---
# AviationLLM: An LLM-based Knowledge System for Aviation Training 

**Title (ZH)**: AviationLLM：基于LLM的航空培训知识系统 

**Authors**: Jia'ang Wan, Feng Shen, Fujuan Li, Yanjin Sun, Yan Li, Shiwen Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2506.14336)  

**Abstract**: Aviation training is a core link in ensuring flight safety, improving industry efficiency and promoting sustainable development. It not only involves flight simulation but also requires the learning of a great deal of professional aviation theory knowledge. In the existing training system, the knowledge is mainly imparted by the the instructors. However, the number of instructors is limited and the professional answers obtained from the Internet are not accurate enough, resulting in low training efficiency. To address this, we introduced LLM, but the basic pre-trained model cannot provide accurate answers to professional fields, so we fine-tuned it. Traditional Supervised Fine-Tuning (SFT) risk generating superficially plausible but factually incorrect responses due to insufficient data coverage. To address this, we employ Direct Preference Optimization(DPO). This paper proposes Retrieval-Augmented LLM Alignment via Direct Preference Optimization(RALA-DPO). We select open source pre-trained LLM Qwen and adapt it to aviation theory training through DPO-based domain alignment. Simultaneously, to mitigate hallucinations caused by training data biases, knowledge obsolescence, or domain knowledge gaps, we implement Retrieval-Augmented Generation(RAG) technology that combines generative and retrieval models. RALA-DPO effectively retrieves relevant information from external knowledge bases and delivers precise and high-quality responses through the generative model. Experimental results demonstrate that RALA-DPO can improve accuracy in response to professional aviation knowledge. With integrated RAG mechanisms, this system can further improve the accuracy of answers and achieve zero-cost knowledge updates simultaneously. 

**Abstract (ZH)**: 航空培训是确保飞行安全、提高行业效率和促进可持续发展的一个核心环节。它不仅涉及飞行模拟，还需要学习大量的专业航空理论知识。在现有的培训体系中，知识主要由教员传授。然而，教员数量有限，从网上获得的专业答案不够准确，导致培训效率低。为了解决这一问题，我们引入了大语言模型（LLM），但是基础预训练模型无法提供专业的准确答案，因此我们对它进行了微调。传统的监督微调（SFT）由于数据覆盖不全，存在生成表面合理但事实错误的回答的风险。为此，我们采用了直接偏好优化（DPO）。本文提出了一种名为基于DPO的检索增强LLM对齐方法（RALA-DPO）。我们选择了开源预训练LLM Qwen，并通过基于DPO的领域对齐将其应用于航空理论培训。同时，为了缓解由于训练数据偏差、知识过时或领域知识空白导致的幻觉，我们实施了结合生成和检索模型的检索增强生成（RAG）技术。RALA-DPO能够从外部知识库中检索相关的信息，并通过生成模型提供准确和高质量的回答。实验结果表明，RALA-DPO能够提高对专业航空知识的回答准确性。通过集成RAG机制，该系统可以进一步提高回答的准确性，并同时实现零成本的知识更新。 

---
# ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems 

**Title (ZH)**: ADRN：基于规则驱动决策系统的LLM驱动自动驾驶 

**Authors**: Fanzhi Zeng, Siqi Wang, Chuzhao Zhu, Li Li  

**Link**: [PDF](https://arxiv.org/pdf/2506.14299)  

**Abstract**: How to construct an interpretable autonomous driving decision-making system has become a focal point in academic research. In this study, we propose a novel approach that leverages large language models (LLMs) to generate executable, rule-based decision systems to address this challenge. Specifically, harnessing the strong reasoning and programming capabilities of LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based Decision Systems) framework, which integrates three core modules: the Information Module, the Agents Module, and the Testing Module. The framework operates by first aggregating contextual driving scenario information through the Information Module, then utilizing the Agents Module to generate rule-based driving tactics. These tactics are iteratively refined through continuous interaction with the Testing Module. Extensive experimental evaluations demonstrate that ADRD exhibits superior performance in autonomous driving decision tasks. Compared to traditional reinforcement learning approaches and the most advanced LLM-based methods, ADRD shows significant advantages in terms of interpretability, response speed, and driving performance. These results highlight the framework's ability to achieve comprehensive and accurate understanding of complex driving scenarios, and underscore the promising future of transparent, rule-based decision systems that are easily modifiable and broadly applicable. To the best of our knowledge, this is the first work that integrates large language models with rule-based systems for autonomous driving decision-making, and our findings validate its potential for real-world deployment. 

**Abstract (ZH)**: 如何构建可解释的自动驾驶决策系统已成为学术研究的焦点。本研究提出了一种新颖的方法，利用大型语言模型（LLMs）生成可执行的基于规则的决策系统以应对这一挑战。具体而言，借助LLMs的强大推理和编程能力，我们介绍了基于规则驱动的自动驾驶（ADRD：LLM-Driven Autonomous Driving Based on Rule-based Decision Systems）框架，该框架集成三个核心模块：信息模块、代理模块和测试模块。该框架通过信息模块首先聚合上下文驾驶场景信息，然后利用代理模块生成基于规则的驾驶策略，并通过与测试模块的持续交互逐步完善这些策略。广泛的实验评估表明，ADRD在自动驾驶决策任务中表现出优异性能。与传统强化学习方法和最先进的基于LLM的方法相比，ADRD在可解释性、响应速度和驾驶性能方面具有显著优势。这些结果突显了该框架实现复杂驾驶场景全面准确理解的能力，并强调了基于规则、易于修改且广泛适用的透明决策系统具有广阔前景。据我们所知，这是首次将大型语言模型与基于规则的系统集成用于自动驾驶决策的研究，我们的发现验证了其在实际部署中的潜在价值。 

---
# Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs 

**Title (ZH)**: 具有可验证奖励的强化学习隐式激励基模型进行正确推理 

**Authors**: Xumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu, Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang, Junjie Li, Ziming Miao, Jiang Bian, Mao Yang  

**Link**: [PDF](https://arxiv.org/pdf/2506.14245)  

**Abstract**: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning. 

**Abstract (ZH)**: 可验证奖励的强化学习（RLVR）正逐渐成为提升大型语言模型（LLMs）推理能力的有前景范式。然而，其有效性的关键悖论使其效果受到质疑：RLVR调优模型往往在解决方案查找的$Pass@K$度量上表现逊于基模型，这引发了一个假设，即RLVR只是以牺牲推理多样性为代价重新加权现有的推理路径。在本工作中，我们通过识别问题的根源——$Pass@K$度量本身是推理的不完善衡量标准，它奖励了可能源自错误或不完整的推理链条的正确最终答案，从而解决了这一矛盾。我们引入了一个更精确的评估指标$CoT$-$Pass@K$，该指标要求推理路径和最终答案都正确。我们提供了新的理论基础，正式化了RLVR与传统RL的独特区别在于其激励逻辑完整性的独特结构。我们的实验证据支持了这一观点：通过$CoT$-$Pass@K$，我们可以观察到RLVR可以激励正确的推理泛化。通过对训练动态的分析，我们发现这种增强的推理能力在训练早期出现，并且能平稳泛化。我们的工作提供了RLVR作用的清晰视角，提供了更可靠的评估方法，并证实了其能够真正推动机器推理进步的潜力。 

---
# ImpReSS: Implicit Recommender System for Support Conversations 

**Title (ZH)**: 隐式推荐系统支持对话推荐 

**Authors**: Omri Haller, Yair Meidan, Dudu Mimran, Yuval Elovici, Asaf Shabtai  

**Link**: [PDF](https://arxiv.org/pdf/2506.14231)  

**Abstract**: Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request. 

**Abstract (ZH)**: 基于大型语言模型的隐式推荐系统：ImpReSS在客户服务对话中的应用 

---
# From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models 

**Title (ZH)**: 从黑箱到透明思维：评估与增强多模态大语言模型的理论共情能力 

**Authors**: Xinyang Li, Siqi Liu, Bochao Zou, Jiansheng Chen, Huimin Ma  

**Link**: [PDF](https://arxiv.org/pdf/2506.14224)  

**Abstract**: As large language models evolve, there is growing anticipation that they will emulate human-like Theory of Mind (ToM) to assist with routine tasks. However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven assessment of ToM in multimodal large language models (MLLMs). Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that attention heads in multimodal large models can distinguish cognitive information across perspectives, providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that significantly enhances the model's exhibited ToM by adjusting in the direction of the attention head. 

**Abstract (ZH)**: 随着大型语言模型的发展，人们越来越期待它们能够模拟出类似人类的理论思维（ToM）以辅助处理常规任务。然而，现有的机器ToM评估方法主要集中在单模态模型上，并且大多将这些模型视为黑盒，缺乏对其内部机制的解释性探索。为此，本研究采用基于内部机制的方法，对多模态大型语言模型（MLLMs）的ToM进行解释驱动评估。具体而言，我们首先构建了一个多模态ToM测试数据集GridToM，该数据集包含了来自多个视角的多元信念测试任务和感知信息。接着，我们的分析显示，多模态大型模型的注意力头部能够区分不同视角下的认知信息，提供了ToM能力的证据。此外，我们提出了一种轻量级且无需训练的方法，通过调整指向注意力头部的方向显著增强了模型展现的ToM能力。 

---
# Collaborative Editable Model 

**Title (ZH)**: 协作可编辑模型 

**Authors**: Kaiwen Tang, Aitong Wu, Yao Lu, Guangda Sun  

**Link**: [PDF](https://arxiv.org/pdf/2506.14146)  

**Abstract**: Vertical-domain large language models (LLMs) play a crucial role in specialized scenarios such as finance, healthcare, and law; however, their training often relies on large-scale annotated data and substantial computational resources, impeding rapid development and continuous iteration. To address these challenges, we introduce the Collaborative Editable Model (CoEM), which constructs a candidate knowledge pool from user-contributed domain snippets, leverages interactive user-model dialogues combined with user ratings and attribution analysis to pinpoint high-value knowledge fragments, and injects these fragments via in-context prompts for lightweight domain adaptation. With high-value knowledge, the LLM can generate more accurate and domain-specific content. In a financial information scenario, we collect 15k feedback from about 120 users and validate CoEM with user ratings to assess the quality of generated insights, demonstrating significant improvements in domain-specific generation while avoiding the time and compute overhead of traditional fine-tuning workflows. 

**Abstract (ZH)**: 垂直领域大型语言模型（LLMs）在金融、医疗和法律等专业场景中发挥着重要作用；然而，它们的训练通常依赖大量标注数据和大量的计算资源，阻碍了快速开发和持续迭代。为应对这些挑战，我们介绍了协作可编辑模型（CoEM），该模型从用户的领域片段贡献中构建候选知识池，通过结合用户对话、评级和归因分析来识别高价值知识片段，并通过上下文提示将这些片段注入模型进行轻量级领域适应。借助高价值知识，LLM能够生成更准确且更具领域特定性的内容。在金融信息场景中，我们收集了约120名用户的15000条反馈，并使用用户评级验证CoEM的效果，评估生成洞察的质量，同时避免了传统微调工作流的时间和计算开销，显示出在领域特定生成方面的显著改进。 

---
# Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models 

**Title (ZH)**: 易碎的偏好：对大规模语言模型中顺序效应的深入探究 

**Authors**: Haonan Yin, Shai Vardi, Vidyanand Choudhary  

**Link**: [PDF](https://arxiv.org/pdf/2506.14092)  

**Abstract**: Large language models (LLMs) are increasingly used in decision-support systems across high-stakes domains such as hiring and university admissions, where decisions often involve selecting among competing alternatives. While prior work has noted positional order biases in LLM-driven comparisons, these biases have not been systematically dissected or linked to underlying preference structures. We provide the first comprehensive investigation of positional biases across multiple LLM architectures and domains, uncovering strong and consistent order effects, including a novel centrality bias not previously documented in human or machine decision-making. We also find a quality-dependent shift: when options are high quality, models exhibit primacy bias, but favor latter options when option quality is low. We further identify a previously undocumented bias favoring certain names over others. To distinguish superficial tie-breaking from true distortions of judgment, we introduce a framework that classifies pairwise preferences as robust, fragile, or indifferent. We show that order effects can lead models to select strictly inferior options, and that positional biases are typically stronger than gender biases. These findings suggest that LLMs are not merely inheriting human-like biases, but exhibit distinct failure modes not seen in human decision-making. We propose targeted mitigation strategies, including a novel use of the temperature parameter, to reduce order-driven distortions. 

**Abstract (ZH)**: 大型语言模型（LLMs）在高风险领域如招聘和大学录取中的决策支持系统中的位置偏见综合研究：新的中心性偏见与质量依赖性切换，未记录的名称偏好，以及判断扭曲的分类框架 

---
# Lightweight Relevance Grader in RAG 

**Title (ZH)**: 轻量级相关性评分器在RAG中 

**Authors**: Taehee Jeong  

**Link**: [PDF](https://arxiv.org/pdf/2506.14084)  

**Abstract**: Retrieval-Augmented Generation (RAG) addresses limitations of large language models (LLMs) by leveraging a vector database to provide more accurate and up-to-date information. When a user submits a query, RAG executes a vector search to find relevant documents, which are then used to generate a response. However, ensuring the relevance of retrieved documents with a query would be a big challenge. To address this, a secondary model, known as a relevant grader, can be served to verify its relevance. To reduce computational requirements of a relevant grader, a lightweight small language model is preferred. In this work, we finetuned llama-3.2-1b as a relevant grader and achieved a significant increase in precision from 0.1301 to 0.7750. Its precision is comparable to that of llama-3.1-70b. Our code is available at this https URL. 

**Abstract (ZH)**: 检索增强生成（RAG）通过利用向量数据库来提供更准确和及时的信息，以克服大规模语言模型（LLMs）的限制。当用户提交查询时，RAG 执行向量搜索以找到相关文档，这些文档随后用于生成响应。然而，确保检索到的文档与查询的相关性将是一个重大挑战。为此，可以部署一个次要模型，即相关性评分器，来验证其相关性。为了减少相关性评分器的计算需求，一个轻量级的小型语言模型更为优选。在此项工作中，我们对 llava-3.2-1b 进行了微调作为相关性评分器，并将精确度从 0.1301 显著提高到 0.7750，其精确度与 llava-3.1-70b 相当。我们的代码可在以下链接获取。 

---
# FormGym: Doing Paperwork with Agents 

**Title (ZH)**: FormGym: 用代理进行表格处理 

**Authors**: Matthew Toles, Rattandeep Singh, Isaac Song Zhou Yu  

**Link**: [PDF](https://arxiv.org/pdf/2506.14079)  

**Abstract**: Completing paperwork is a challenging and time-consuming problem. Form filling is especially challenging in the pure-image domain without access to OCR, typeset PDF text, or a DOM. For computer agents, it requires multiple abilities, including multi-modal understanding, information retrieval, and tool-use. We present a novel form-filling benchmark consisting of 432 fields spread across 55 documents and 3 tasks, requiring knowledge of 236 features per user. We find that baseline VLAs achieve less than 1% accuracy in most cases, primarily due to poor localization ability. GUI agents also struggle, scoring between 10.6-68.0% despite high cost and latency. Therefore, we also contribute FieldFinder, a tool to assist LLMs in identifying where to place text on a form. With FieldFinder, all models achieve equal or better performance in all six study conditions, with a maximum increase from 2% to 56%. 

**Abstract (ZH)**: 填写表格是一项具有挑战性和耗时的问题。在无OCR、无排版PDF文字或无DOM访问权限的纯图像域中，表格填写尤为具有挑战性。对于计算机代理而言，这需要多种能力，包括跨模态理解、信息检索和工具使用。我们提出了一种新型表格填写基准，包含55份文档中的432个字段和3项任务，每个用户需要掌握236个特征。我们发现，基准的多模态视觉语言模型在大多数情况下准确率低于1%，主要原因是定位能力较差。GUI代理也面临挑战，尽管成本和延迟较高，准确率也只能在10.6%-68.0%之间。因此，我们还贡献了FieldFinder工具，以帮助LLM识别人表格填写的文本位置。使用FieldFinder后，所有模型在所有六种研究条件下均实现了同等或更好的性能，性能提升最大可达56%。 

---
# SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine 

**Title (ZH)**: SANGAM: 通过蒙特卡洛树自我完善生成SystemVerilog断言 

**Authors**: Adarsh Gupta, Bhabesh Mali, Chandan Karfa  

**Link**: [PDF](https://arxiv.org/pdf/2506.13983)  

**Abstract**: Recent advancements in the field of reasoning using Large Language Models (LLMs) have created new possibilities for more complex and automatic Hardware Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the automatic generation of SVAs from industry-level specifications. The proposed framework utilizes a three-stage approach: Stage 1 consists of multi-modal Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine (MCTSr) algorithm for automatic reasoning about SVAs for each signal, and finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA assertions for each signal. The results demonstrated that our framework, SANGAM, can generate a robust set of SVAs, performing better in the evaluation process in comparison to the recent methods. 

**Abstract (ZH)**: Recent Advancements in Hardware Assertion Generation Using Large Language Models: Introduction of SANGAM, a SystemVerilog Assertion Generation Framework Guided by LLMs 

---
# ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users 

**Title (ZH)**: ProfiLLM：基于LLM的聊天机器人用户隐性 profiling 框架 

**Authors**: Shahaf David, Yair Meidan, Ido Hersko, Daniel Varnovitzky, Dudu Mimran, Yuval Elovici, Asaf Shabtai  

**Link**: [PDF](https://arxiv.org/pdf/2506.13980)  

**Abstract**: Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research. 

**Abstract (ZH)**: 尽管在对话AI方面取得了显著进展，但由大型语言模型（LLM）驱动的聊天机器人在根据个人用户特征（如技术专长、学习风格和沟通偏好）个性化响应时仍然存在困难。这种缺乏个性化在IT/网络安全（ITSec）等专门知识密集型领域尤为成问题，因为用户的知识水平差异很大。现有的聊天机器人个性化方法主要依赖于静态用户类别或显式自我报告的信息，限制了它们对用户 proficiency 不断变化的感知的适应能力。在本文中，我们提出了一种名为ProfiLLM的新框架，通过聊天机器人交互进行隐式和动态用户画像。该框架包括一个可应用于多种领域的分类体系，以及一种基于LLM的用户画像方法。为了证明ProfiLLM的有效性，我们将其应用于ITSec领域，通过故障排除交互来推断聊天机器人用户的技术水平。具体而言，我们开发了ProfiLLM[ITSec]，即ProfiLLM的ITSec特异化变体，并在263个合成用户中1,760条类人类聊天机器人对话上评估了其性能。结果显示，ProfiLLM[ITSec]能够在单次提示后迅速而准确地推断出ITSec画像，实际得分与预测得分之间的差距减少了55-65%，随后小幅波动并进一步细化。除了评估我们新的隐式和动态画像框架外，我们还提出了一种基于LLM的人设模拟方法、一个ITSec专长的结构化分类体系、我们的代码库以及一个聊天机器人交互数据集，以支持未来的研究。 

---
# LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning 

**Title (ZH)**: LocationReasoner：评估大型语言模型在真实世界选址推理中的性能 

**Authors**: Miho Koda, Yu Zheng, Ruixian Ma, Mingyang Sun, Devesh Pansare, Fabio Duarte, Paolo Santi  

**Link**: [PDF](https://arxiv.org/pdf/2506.13841)  

**Abstract**: Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at this https URL. 

**Abstract (ZH)**: 近年来，特别是通过强化后训练增强的大语言模型（LLMs）在推理能力方面取得了显著进步，如OpenAI o1和DeepSeek-R1模型所示。然而，这些能力主要在数学问题解决和代码生成等领域进行了评估，留下的问题是这些推理技能是否能泛化到复杂的现实世界场景中。在本文中，我们介绍了LocationReasoner基准，用于评估大语言模型在现实世界站点选址场景中的推理能力，该场景要求模型通过处理多样且复杂的时空、环境和物流约束来识别可行位置。该基准包含超过300个精心设计的、难度不一的查询，并配备了一个沙箱环境和基于约束的地理位置搜索的内部工具。 extensive评估显示，最先进的推理模型在现实世界场景中的表现并未显著优于其非推理前身，即使是最新的OpenAI o4模型也在30%的站点选址任务中失败。此外，像ReAct和Reflexion这样的代理策略由于过度推理，往往表现不如直接代码生成提示。鉴于大语言模型在整体和非线性推理方面的关键局限性，我们发布了LocationReasoner，以促进开发能够在现实世界决策任务中进行稳健且基于事实推理的大语言模型和代理。我们的基准代码和数据可在以下网址获取。 

---
# The NordDRG AI Benchmark for Large Language Models 

**Title (ZH)**: NordDRG AI基准测试：大型语言模型 

**Authors**: Tapio Pitkäranta  

**Link**: [PDF](https://arxiv.org/pdf/2506.13790)  

**Abstract**: Large language models (LLMs) are already being piloted for clinical coding and decision support. However, until now, no open benchmark has targeted the hospital funding layer where Diagnosis-Related Groups (DRG) determine reimbursement across many countries. We release NordDRG-AI-Benchmark, the first public test-bed that captures a complete DRG rule set and evaluates an LLM's ability to reason over multilingual diagnosis, procedure, and tariff logic.
The benchmark bundles three classes of artefacts: (i) definition tables with 20 interlinked tables covering DRG logic, ICD and NCSP codes, age/sex splits, and country flags; (ii) expert manuals and changelog templates describing real governance workflows; and (iii) a prompt pack of 14 CaseMix tasks that span code lookup, cross-table inference, multilingual terminology, and quality-assurance audits.
All artefacts are available at: this https URL
A baseline demonstration shows that five state-of-the-art LLMs perform very differently on the nine automatically verifiable tasks: o3 (OpenAI) scores 9 out of 9, GPT-4o and o4-mini-high score 7 out of 9, while Gemini 2.5 Pro and Gemini 2.5 Flash solve only 5 out of 9 and 3 out of 9, respectively. These results confirm that NordDRG-AI-Benchmark highlights domain-specific strengths and weaknesses that remain hidden in generic LLM benchmarks, offering a reproducible baseline for research on trustworthy automation in hospital funding. 

**Abstract (ZH)**: 大型语言模型（LLMs）已经在临床编码和决策支持中进行了试点。然而，直到现在，还没有公开基准针对医院资金层，其中诊断相关组（DRG）决定许多国家的报销标准。我们发布了NordDRG-AI-Benchmark，这是首个包含完整DRG规则集并评估LLM在多语言诊断、程序和费率逻辑推理能力的公开测试平台。 

---
# Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values 

**Title (ZH)**: 个性化宪法一致的代理超我：与多样人类价值观对齐的AI安全行为 

**Authors**: Nell Watson, Ahmed Amer, Evan Harris, Preeti Ravindra, Shujun Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2506.13774)  

**Abstract**: Agentic AI systems, possessing capabilities for autonomous planning and action, exhibit immense potential across diverse domains. However, their practical deployment is significantly hampered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the intricate task of providing deep, personalized contextual information without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected "Creed Constitutions"-encapsulating diverse rule sets-with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface (this http URL) with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs, achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at this https URL. 

**Abstract (ZH)**: 具有自主规划和行动能力的代理人工智能系统在多个领域展现出巨大的潜力。然而，它们的实际部署受到与多样化的人类价值观、复杂的安全需求以及特定合规性要求对齐的挑战。现有的对齐方法在提供深度个性化上下文信息时常常失败，同时又不引起虚构行为或操作效率低下。本文介绍了一种新的解决方案：一个“超我”代理，设计为代理人工智能的个性化监督机制。该系统通过引用用户选择的“信条宪法”（包含多种规则集）来动态指导AI规划，并可根据不可谈判的价值观调整遵守水平。实时合规监督者在执行前验证计划是否符合这些宪法和通用伦理底线。我们呈现了一个功能系统，包括一个演示界面（链接：this http URL）以及一个原型宪法共享门户，并通过模型上下文协议（MCP）成功与第三方模型集成。全面基准评估（HarmBench，AgentHarm）表明，我们的超我代理大幅减少了有害输出，实现了高达98.3%的有害评分减少，并且在有害行为集上的拒绝率接近完美（例如，与Claude Sonnet 4结合时，在AgentHarm的有害行为集上的拒绝率为100%）等领先的大规模语言模型（如Gemini 2.5 Flash和GPT-4o）。这种方法显著简化了个性化AI对齐，使代理系统更可靠地适应个体和文化背景，同时还能显著提高安全性。有关这项研究的概述和示例，请参见链接：this https URL。 

---
# A Variational Framework for Improving Naturalness in Generative Spoken Language Models 

**Title (ZH)**: 一种变分框架，用于提高生成语音语言模型的自然度 

**Authors**: Li-Wei Chen, Takuya Higuchi, Zakaria Aldeneh, Ahmed Hussen Abdelaziz, Alexander Rudnicky  

**Link**: [PDF](https://arxiv.org/pdf/2506.14767)  

**Abstract**: The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at this https URL. 

**Abstract (ZH)**: 大型语言模型在文本处理中的成功激励了其在语音建模中的应用。然而，由于语音是连续且复杂的，通常被离散化以进行自回归建模。源自自监督模型的语音令牌（即语义令牌）通常关注语音的语言方面，但忽略了语调信息。因此，基于这些令牌训练的模型可能生成缺乏自然度的语音。现有方法尝试通过向语义令牌中添加音高特征来解决这一问题。但仅靠音高无法全面表示语副语言属性的范围，选择合适的特征需要精细的手动工程。为克服这一问题，我们提出了一种端到端的变分方法，能够自动学习编码这些连续的语音属性以增强语义令牌。该方法消除了手动提取和选择语副语言特征的需要，而且能够根据人类评定产生优选的语音继续。代码、样本和模型可以在以下链接获取。 

---
# From Bytes to Ideas: Language Modeling with Autoregressive U-Nets 

**Title (ZH)**: 从字节到理念：自回归U-网的语言建模 

**Authors**: Mathurin Videau, Badr Youbi Idrissi, Alessandro Leite, Marc Schoenauer, Olivier Teytaud, David Lopez-Paz  

**Link**: [PDF](https://arxiv.org/pdf/2506.14761)  

**Abstract**: Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages. 

**Abstract (ZH)**: Tokenization Imposes Fixed Granularity on Input Text, While an Autoregressive U-Net Learns to Embed Its Own Tokens, Offering Multi-Scale Insights and Flexible Prediction Depths 

---
# Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs 

**Title (ZH)**: 环形精简：通过C3PO稳定增强学习实现的大规模可扩展推理 

**Authors**: Ring Team, Bin Hu, Cai Chen, Deng Zhao, Ding Liu, Dingnan Jin, Feng Zhu, Hao Dai, Hongzhi Luan, Jia Guo, Jiaming Liu, Jiewei Wu, Jun Mei, Jun Zhou, Junbo Zhao, Junwu Xiong, Kaihong Zhang, Kuan Xu, Lei Liang, Liang Jiang, Liangcheng Fu, Longfei Zheng, Qiang Gao, Qing Cui, Quan Wan, Shaomian Zheng, Shuaicheng Li, Tongkai Yang, Wang Ren, Xiaodong Yan, Xiaopei Wan, Xiaoyun Feng, Xin Zhao, Xinxing Yang, Xinyu Kong, Xuemin Yang, Yang Li, Yingting Wu, Yongkang Liu, Zhankai Xu, Zhenduo Zhang, Zhenglei Zhou, Zhenyu Huang, Zhiqiang Zhang, Zihao Wang, Zujie Wen  

**Link**: [PDF](https://arxiv.org/pdf/2506.14731)  

**Abstract**: We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code. 

**Abstract (ZH)**: Ring-lite：一种基于Mixture-of-Experts架构并通过强化学习优化的大规模语言模型，具备高效的稳健推理能力 

---
# Unified Software Engineering agent as AI Software Engineer 

**Title (ZH)**: 统一的软件工程代理作为AI软件工程师 

**Authors**: Leonhard Applis, Yuntong Zhang, Shanchao Liang, Nan Jiang, Lin Tan, Abhik Roychoudhury  

**Link**: [PDF](https://arxiv.org/pdf/2506.14683)  

**Abstract**: The growth of Large Language Model (LLM) technology has raised expectations for automated coding. However, software engineering is more than coding and is concerned with activities including maintenance and evolution of a project. In this context, the concept of LLM agents has gained traction, which utilize LLMs as reasoning engines to invoke external tools autonomously. But is an LLM agent the same as an AI software engineer? In this paper, we seek to understand this question by developing a Unified Software Engineering agent or USEagent. Unlike existing work which builds specialized agents for specific software tasks such as testing, debugging, and repair, our goal is to build a unified agent which can orchestrate and handle multiple capabilities. This gives the agent the promise of handling complex scenarios in software development such as fixing an incomplete patch, adding new features, or taking over code written by others. We envision USEagent as the first draft of a future AI Software Engineer which can be a team member in future software development teams involving both AI and humans. To evaluate the efficacy of USEagent, we build a Unified Software Engineering bench (USEbench) comprising of myriad tasks such as coding, testing, and patching. USEbench is a judicious mixture of tasks from existing benchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on USEbench consisting of 1,271 repository-level software engineering tasks, USEagent shows improved efficacy compared to existing general agents such as OpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for certain coding tasks, which provides hints on further developing the AI Software Engineer of the future. 

**Abstract (ZH)**: 大型语言模型技术的增长提高了自动编码的期望值。然而，软件工程不仅仅是编码，还涉及包括项目维护和演化在内的多种活动。在此背景下，LLM代理的概念受到了关注，这些代理利用大型语言模型作为推理引擎，自主调用外部工具。但LLM代理等同于AI软件工程师吗？在本文中，我们通过开发一个统一的软件工程代理（USEagent）来研究这一问题。不同于现有专为特定软件任务（如测试、调试和修复）构建的专用代理，我们的目标是构建一个能够协调和处理多种能力的统一代理。这给代理带来了在软件开发中处理复杂场景的潜力，如修复不完整的补丁、增加新功能或接手他人编写的代码。我们设想USEagent是未来AI软件工程师的一个初稿，它可以成为未来涉及AI和人类的软件开发团队的一员。为了评估USEagent的有效性，我们构建了一个统一的软件工程基准（USEbench），包括编码、测试和补丁等多种任务，以现有基准（如SWE-bench、SWT-bench和REPOCOD）中的任务为基础。在包含1,271个仓库级别的软件工程任务的USEbench评估中，USEagent相较于现有的通用代理（如OpenHands CodeActAgent）展示了更好的有效性。在某些编码任务上，USEagent仍存在能力上的差距，这为未来开发AI软件工程师提供了一些建议。 

---
# Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot 

**Title (ZH)**: 重访链式思考提示：零样本可能比少样本更强大 

**Authors**: Xiang Cheng, Chengyan Pan, Minjun Zhao, Deyang Li, Fangchao Liu, Xinyu Zhang, Xiao Zhang, Yong Liu  

**Link**: [PDF](https://arxiv.org/pdf/2506.14641)  

**Abstract**: In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \texttt{Qwen2.5-Max} and \texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars. 

**Abstract (ZH)**: 大型语言模型中上下文学习（ICL）中链式思考（CoT）范例的有效性探究：数学推理能力提升的局限性 

---
# AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation 

**Title (ZH)**: 除了调查之外还有什么？使用大型语言模型对关于调查动机的开放式调查回应进行编码 

**Authors**: Leah von der Heyde, Anna-Carolina Haensch, Bernd Weiß, Jessika Daikeler  

**Link**: [PDF](https://arxiv.org/pdf/2506.14634)  

**Abstract**: The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research. 

**Abstract (ZH)**: 近期大语言模型的发展及其更广泛的 accessibility 促进了其在调查研究中的应用讨论，包括对开放问卷答案的分类。由于具备语言能力，大语言模型可能是处理此类问题的有效替代方案，替代耗时的手动编码和监督机器学习模型的预训练。尽管已有研究主要关注英语答案或单一模型的非复杂主题，但尚不清楚其发现是否具有普适性，以及这些分类的质量与传统方法相比如何。在本研究中，我们通过使用德国参与调查原因的数据，探讨不同大语言模型在其他情境下分类开放问卷答案的可行程度，并对比多种尖端大语言模型和提示方法的表现，通过使用人类专家编码来评估其性能。不同大语言模型之间的整体性能差异巨大，只有微调模型才能实现可接受的预测性能。不同提示方法之间的性能差异取决于所使用的大语言模型。此外，大语言模型在不同参与调查原因类别上的分类不平等导致在不进行微调的情况下出现不同的分类分布。我们讨论了这些发现对开放问卷答案编码方法论研究及其实质性分析的影响，以及对处理此类数据的实践者的启示。本研究还强调了研究人员在选择大语言模型时代开放问卷答案分类的自动化方法时需要考虑的诸多权衡。我们的研究为大语言模型在调查研究中高效、准确、可靠的应用条件增添了研究基础。 

---
# Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models 

**Title (ZH)**: 面向集体道德推理的大型语言模型中概率聚合与目标嵌入优化 

**Authors**: Chenchen Yuan, Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci  

**Link**: [PDF](https://arxiv.org/pdf/2506.14625)  

**Abstract**: Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems. 

**Abstract (ZH)**: 大型语言模型在处理复杂多因素道德困境时常常出现分歧。为了应对这些分歧，我们提出了一种框架，将多个大型语言模型的道德判断综合为一个集体形成的道德判断，重新校准与该共识显著偏离的模型。我们的聚合机制融合了连续的道德可接受性评分（超越二元标签），并通过模型可靠性加权。对于错位的模型，我们采取目标向量优化程序微调道德哲学理论的令牌嵌入，同时最小化与共识的JS散度并保留语义完整性。实验表明，我们的方法能够建立稳健的共识并提高个体模型的准确性。这些发现强调了多模型数据驱动的道德对齐的价值及其对更安全、更一致的人工智能系统潜在的促进作用。 

---
# GenerationPrograms: Fine-grained Attribution with Executable Programs 

**Title (ZH)**: 生成程序：可执行程序的细粒度归因 

**Authors**: David Wan, Eran Hirsch, Elias Stengel-Eskin, Ido Dagan, Mohit Bansal  

**Link**: [PDF](https://arxiv.org/pdf/2506.14580)  

**Abstract**: Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable "code agent" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality. 

**Abstract (ZH)**: Recent大型语言模型（LLMs）在条件源文本生成方面取得了显著性能，但往往无法正确提供详细的归因，损害了验证性和可信度。此外，现有的归因方法未能解释模型如何及为何利用提供的源文档生成最终响应，限制了模型的可解释性。为克服这些挑战，我们提出了一个模块化生成框架GenerationPrograms，该框架受可执行“代码代理”架构最新进展的启发。与同时生成输出和归因或依赖于事后归因的常规生成方法不同，GenerationPrograms将过程分解为两个不同的阶段：首先，创建由模块化文本操作（如改写、压缩和融合）组成的可执行程序规划，这些操作明确针对查询进行定制；其次，按照程序指定的指令执行这些操作，从而生成最终响应。实证评估表明，GenerationPrograms在两个长文本问答任务和一个多文档摘要任务中，显著提高了文档级和句子级的归因质量。我们进一步证明，GenerationPrograms可以有效作为事后归因方法使用，在恢复准确归因方面优于传统技术。此外，GenerationPrograms生成的可解释程序通过模块级改进实现局部优化，进一步提升总体归因质量。 

---
# TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization 

**Title (ZH)**: TGDPO: 利用令牌级奖励指导以提高直接偏好优化效果 

**Authors**: Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, Hengshuang Zhao, Jiaya Jia  

**Link**: [PDF](https://arxiv.org/pdf/2506.14574)  

**Abstract**: Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at this https URL. 

**Abstract (ZH)**: Recent 进展表明，利用细粒度词级奖励模型可以显著提高Proximal Policy Optimization (PPO)在对齐大型语言模型方面的性能，来自人类反馈的强化学习 recent 进展。然而，利用此类词级奖励作为直接偏好优化（DPO）的指导存在挑战，因为DPO被形式化为一个序列级多臂 bandit 问题。为了解决这一挑战，本文将序列级 PPO 分解为一系列词级 proximal 政策优化问题，然后以词级奖励指导的方式重新表述词级 PPO 问题，从而可以推导出闭式最优词级策略和相应的词级奖励。利用获得的奖励和 Bradley-Terry 模型，本文为 DPO 建立了一个带有词级奖励指导的可计算损失函数框架，并提出了一种基于诱导 DPO 奖励的实用奖励指导。这种形式化允许不同词根据各自的奖励展示不同程度与参考策略的偏差。实验结果表明，与 DPO 相比，我们的方法在 MT-Bench 上的胜率提高了 7.5 个百分点，在 AlpacaEval 2 上提高了 6.2 个百分点，在 Arena-Hard 上提高了 4.3 个百分点。代码参见此 https URL。 

---
# AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs 

**Title (ZH)**: AlphaDecay：模块级权重衰减在LLM中实现重尾平衡 

**Authors**: Di He, Ajay Jaiswal, Songjun Tu, Li Shen, Ganzhao Yuan, Shiwei Liu, Lu Yin  

**Link**: [PDF](https://arxiv.org/pdf/2506.14562)  

**Abstract**: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines. 

**Abstract (ZH)**: AlphaDecay：基于重尾自我正则化理论的模块自适应权重衰减方法 

---
# Automatic Qiskit Code Refactoring Using Large Language Models 

**Title (ZH)**: 使用大型语言模型进行自动Qiskit代码重构 

**Authors**: José Manuel Suárez, Luis Mariano Bibbó, Joaquin Bogado, Alejandro Fernandez  

**Link**: [PDF](https://arxiv.org/pdf/2506.14535)  

**Abstract**: As quantum software frameworks evolve, developers face increasing challenges in maintaining compatibility with rapidly changing APIs. In this work, we present a novel methodology for refactoring Qiskit code using large language models (LLMs). We begin by extracting a taxonomy of migration scenarios from the different sources of official Qiskit documentation (such as release notes), capturing common patterns such as migration of functionality to different modules and deprecated usage. This taxonomy, along with the original Python source code, is provided as input to an LLM, which is then tasked with identifying instances of migration scenarios in the code and suggesting appropriate refactoring solutions. Our approach is designed to address the context length limitations of current LLMs by structuring the input and reasoning process in a targeted, efficient manner. The results demonstrate that LLMs, when guided by domain-specific migration knowledge, can effectively assist in automating Qiskit code migration. This work contributes both a set of proven prompts and taxonomy for Qiskit code migration from earlier versions to version 0.46 and a methodology to asses the capabilities of LLMs to assist in the migration of quantum code. 

**Abstract (ZH)**: 基于大型语言模型的Qiskit代码重构新型方法研究 

---
# RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition 

**Title (ZH)**: RAGtifier: 评估最新RAG系统中RAG生成方法的SIGIR LiveRAG竞赛zzle 

**Authors**: Tim Cofala, Oleh Astappiev, William Xion, Hailay Teklehaymanot  

**Link**: [PDF](https://arxiv.org/pdf/2506.14412)  

**Abstract**: Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR 2025 LiveRAG Challenge. 

**Abstract (ZH)**: Retrieval-Augmented Generation (RAG)增强了大型语言模型（LLMs）的能力，通过将其内部的参数化知识与外部的非参数化来源相结合，旨在提高事实的准确性和减少幻觉。LiveRAG 2025挑战赛探索了RAG解决方案，以最大化DataMorgana的QA配对数据集上的准确性，该数据集由单跳和多跳问题组成。该挑战提供了稀疏的OpenSearch和密集的Pinecone索引，限制模型使用参数不超过10亿的LLM，并且最终答案生成使用Falcon-3-10B。裁判LLM和人类评估者评估提交的答案。在挑战条件下探索不同的检索器组合和RAG解决方案，我们最终的解决方案使用了InstructRAG与Pinecone检索器和BGE重排序器的组合。我们的解决方案取得了准确率为1.13、忠实度为0.55的成绩，在SIGIR 2025 LiveRAG挑战赛中排名第四。 

---
# LLM-Powered Intent-Based Categorization of Phishing Emails 

**Title (ZH)**: LLM 助力基于意图的钓鱼邮件分类 

**Authors**: Even Eilertsen, Vasileios Mavroeidis, Gudmund Grov  

**Link**: [PDF](https://arxiv.org/pdf/2506.14337)  

**Abstract**: Phishing attacks remain a significant threat to modern cybersecurity, as they successfully deceive both humans and the defense mechanisms intended to protect them. Traditional detection systems primarily focus on email metadata that users cannot see in their inboxes. Additionally, these systems struggle with phishing emails, which experienced users can often identify empirically by the text alone. This paper investigates the practical potential of Large Language Models (LLMs) to detect these emails by focusing on their intent. In addition to the binary classification of phishing emails, the paper introduces an intent-type taxonomy, which is operationalized by the LLMs to classify emails into distinct categories and, therefore, generate actionable threat information. To facilitate our work, we have curated publicly available datasets into a custom dataset containing a mix of legitimate and phishing emails. Our results demonstrate that existing LLMs are capable of detecting and categorizing phishing emails, underscoring their potential in this domain. 

**Abstract (ZH)**: 网络钓鱼攻击仍然是现代网络安全的重要威胁，因为它们能够成功地欺骗人类用户和旨在保护它们的防御机制。传统的检测系统主要依赖于用户在收件箱中看不到的邮件元数据。此外，这些系统难以识别经验丰富的用户仅通过邮件文本就能识别的网络钓鱼邮件。本文探讨了大型语言模型（LLMs）在这种情况下检测邮件的实际潜力，重点关注邮件的意图。除了对网络钓鱼邮件进行二元分类外，本文还引入了一种意图类型分类体系，通过LLMs将邮件分类到不同的类别中，从而生成可操作的威胁信息。为了支持我们的工作，我们整理了公开可用的数据集，创建了一个自定义数据集，包含合法和网络钓鱼邮件的混编。我们的结果显示，现有的LLMs能够检测和分类网络钓鱼邮件，突显了它们在该领域的潜力。 

---
# Improving LoRA with Variational Learning 

**Title (ZH)**: 基于变分学习提升LoRA 

**Authors**: Bai Cong, Nico Daheim, Yuesong Shen, Rio Yokota, Mohammad Emtiyaz Khan, Thomas Möllenhoff  

**Link**: [PDF](https://arxiv.org/pdf/2506.14280)  

**Abstract**: Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning. 

**Abstract (ZH)**: 贝叶斯方法 recently被用于改善LoRA微调，尽管它们提高了校准性，但对其他指标（如准确性）的影响 marginal且有时甚至是有害的。此外，贝叶斯方法还会增加计算开销，并需要额外的技巧才能充分发挥作用。我们通过使用最近提出的一种变分算法IVON来解决这些问题。我们证明IVON易于实现，其成本与AdamW相当，但通过一种简单的后验剪枝技术，它可以显著改善许多指标。我们在大规模LLM（Llama和Qwen系列）上进行了详尽的实验，超越了现有IVON应用的规模。例如，我们在一套常识推理任务上微调了一个Llama-3.2-3B模型，并在准确性上比AdamW提高了1.3%，减少了5.4%的ECE，超过了AdamW和Laplace-LoRA、BLoB等其他最近的贝叶斯方法。总体而言，我们的结果表明，使用IVON的变分学习可以有效改善LoRA微调。 

---
# Re-Initialization Token Learning for Tool-Augmented Large Language Models 

**Title (ZH)**: 增强工具支持的大语言模型的重初始化 token 学习 

**Authors**: Chenghao Li, Liu Liu, Baosheng Yu, Jiayan Qiu, Yibing Zhan  

**Link**: [PDF](https://arxiv.org/pdf/2506.14248)  

**Abstract**: Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains. 

**Abstract (ZH)**: 大型语言模型在展示出色表现的同时，对于数值推理和计划生成等复杂任务表现出不足。将外部工具，如计算器和数据库，集成到大型语言模型（LLMs）中对于提升问题解决能力至关重要。当前方法通过给每个工具分配一个独特的标记，并通过标记预测的方式（类似词生成）来调用工具，但这种方法未能考虑到工具标记与词标记之间的关系，限制了预训练LLMs的适应性。为解决这一问题，我们提出了一种新的标记学习方法，从初始化的角度将工具标记与现有的词嵌入空间对齐，从而提升模型性能。我们首先基于工具名称或描述构建先验工具标记嵌入，并将其用于初始化和正则化可学习的工具标记嵌入，以此确保学习到的嵌入与词标记空间良好对齐，提高工具调用准确性。我们通过使用GSM8K-XL、FuncQA、KAMEL和VirtualHome数据集来评估方法在数值推理、基于知识的问答和体态计划生成等任务上的性能，并且结果显示，该方法在Cot、React、ICL和ToolkenGPT等近期基线方法上都表现出了明显的改进，表明我们的方法有效地通过相关标记跨领域地增强了LLMs的功能。 

---
# Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team 

**Title (ZH)**: Xolver: 多智能体推理结合全方位经验学习犹如奥林匹克团队 

**Authors**: Md Tanzib Hosain, Salman Rahman, Md Kishor Morol, Md Rizwan Parvez  

**Link**: [PDF](https://arxiv.org/pdf/2506.14234)  

**Abstract**: Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at this https URL. 

**Abstract (ZH)**: 尽管在复杂推理方面取得了显著进展，当前的大语言模型（LLMs）通常独立运行——处理每个问题时都不积累或整合经验知识。相比之下，诸如奥林匹克或者编程竞赛团队这样的专家解决问题者，会利用丰富的经验织锦：从教练那里吸收指导，从过去的问题中培养直觉，利用工具使用和库功能的知识，根据同伴的专业经验调整策略，在试错过程中不断精炼他们的推理，并学习其他相关问题的知识，即使在比赛期间也是如此。我们提出了一种无需训练的多agents推理框架Xolver，为黑盒LLM配备了持久且不断演化的综合经验记忆。Xolver整合了多种经验模态，包括外部和自我检索、工具使用、协作互动、agent驱动的评估以及迭代精炼。通过在推理时从相关策略、代码片段和抽象推理模式中学习，Xolver避免从头生成解决方案，从而从孤立推理转向经验感知的语言代理。基于开源和专有模型，Xolver在专门的推理代理中表现优异。即使使用轻量级的基础模型（例如QWQ-32B），它也经常超越包括Qwen3-235B、Gemini 2.5 Pro、o3和o4-mini-high在内的高级模型。使用o3-mini-high，它在GSM8K（98.1%）、AIME'24（94.4%）、AIME'25（93.7%）、Math-500（99.8%）和LiveCodeBench-V5（91.6%）上取得新的最佳成绩——强调全面经验学习是通向具备专家级推理能力的通才代理的关键一步。完整代码和数据可在以下链接获得。 

---
# GRAM: A Generative Foundation Reward Model for Reward Generalization 

**Title (ZH)**: GRAM: 一种生成式基础奖励模型用于奖励泛化 

**Authors**: Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Qiaozhi He, Murun Yang, Bei Li, Tong Xiao, Chunliang Zhang, Tongran Liu, Jingbo Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2506.14175)  

**Abstract**: In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models. 

**Abstract (ZH)**: 在大规模语言模型对齐中使用标记和未标记数据训练奖励模型的方法探究 

---
# S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models 

**Title (ZH)**: S$^4$C：基于语法和语义一致性推测采样的大型语言模型高效推理方法 

**Authors**: Tao He, Guang Huang, Yu Yang, Tianshi Xu, Sicheng Zhao, Guiguang Ding, Pengyang Wang, Feng Tian  

**Link**: [PDF](https://arxiv.org/pdf/2506.14158)  

**Abstract**: Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods. 

**Abstract (ZH)**: 具有句法和语义连贯性的推测采样框架（S$^4$C） 

---
# InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking 

**Title (ZH)**: InsertRank: LLMs可以基于BM25得分进行列表级重排序以提升性能 

**Authors**: Rahul Seetharaman, Kaustubh D. Dhole, Aman Bansal  

**Link**: [PDF](https://arxiv.org/pdf/2506.14086)  

**Abstract**: Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods. 

**Abstract (ZH)**: 基于大型语言模型的InsertRank检索增强器：通过利用BM25评分进一步提高检索性能 

---
# Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications 

**Title (ZH)**: Ace-CEFR -- 用于LLM应用的对话文本语言难度自动化评估数据集 

**Authors**: David Kogan, Max Schumacher, Sam Nguyen, Masanori Suzuki, Melissa Smith, Chloe Sophia Bellows, Jared Bernstein  

**Link**: [PDF](https://arxiv.org/pdf/2506.14046)  

**Abstract**: There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development. 

**Abstract (ZH)**: 有一种需求未被满足，即评估短对话文本的语言难度，特别是在训练和过滤大型语言模型（LLMs）方面。我们引入了Ace-CEFR数据集，该数据集包含经过专家标注的英语对话文本片段及其相应的文本难度级别。我们尝试了多种模型在Ace-CEFR上的表现，包括基于Transformer的模型和LLMs。结果显示，训练于Ace-CEFR的数据集上的模型能够比人类专家更准确地衡量文本难度，并且具有适应生产环境的延迟。最后，我们公开发布Ace-CEFR数据集供研究和开发使用。 

---
# Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders 

**Title (ZH)**: 驯服LLMs中的多义性：通过稀疏自编码器实现可验证的功能恢复 

**Authors**: Siyu Chen, Heejune Sheen, Xuyuan Xiong, Tianhao Wang, Zhuoran Yang  

**Link**: [PDF](https://arxiv.org/pdf/2506.14002)  

**Abstract**: We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability. 

**Abstract (ZH)**: 我们研究了使用稀疏自编码器（SAEs）从大型语言模型中实现理论上可验证的特征恢复所面临的挑战。现有的SAE训练算法通常缺乏严格的数学保证，并且受到超参数敏感性和不稳定性等实际限制的影响。为了应对这些问题，我们首先提出了一种新的统计框架来解决特征恢复问题，该框架包括一种新的特征可识别性概念，即将多义特征建模为潜在单义概念的稀疏混合。在此框架的基础上，我们引入了一种新的基于“偏差适应”的SAE训练算法。该算法能够自适应调整神经网络的偏差参数以确保适当的激活稀疏性。我们理论证明，在输入数据来自我们提出的统计模型的情况下，该算法可以正确恢复所有单义特征。此外，我们开发了一种改进的经验变体，即分组偏差适应（GBA），并在应用于最多含有15亿参数的大型语言模型时，展示了其相对于基准方法的优越性能。这项工作代表了在提供理论上恢复保证的SAE算法方面的一个基础性步骤，从而通过增强的机制解释提高了更透明和可信赖的AI系统的发展。 

---
# AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science 

**Title (ZH)**: AssistedDS：外域知识辅助LLMs在自动化数据科学中的基准研究 

**Authors**: An Luo, Xun Xian, Jin Du, Fangqiao Tian, Ganghua Wang, Ming Zhong, Shengchun Zhao, Xuan Bi, Zirui Liu, Jiawei Zhou, Jayanth Srinivasa, Ashish Kundu, Charles Fleming, Mingyi Hong, Jie Ding  

**Link**: [PDF](https://arxiv.org/pdf/2506.13992)  

**Abstract**: Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems. 

**Abstract (ZH)**: 大型语言模型（LLMs）已经推进了数据科学工作流程的自动化。然而，尚不清楚它们是否能够像人类数据科学家在实践中那样批判性地利用外部领域知识。为了回答这个问题，我们引入了AssistedDS（辅助数据科学），这是一个基准测试，旨在系统地评估LLMs在表格预测任务中处理领域知识的能力。AssistedDS包括具有明确生成机制的合成数据集和现实世界的Kaggle竞赛，每项任务都附有经过精心筛选的有益和对抗性文档。这些文档提供了针对数据清洗、特征工程和模型选择的领域特定见解。我们评估了最先进的LLMs识别和应用有益与有害领域知识的能力，评估提交的有效性、信息召回率和预测性能。我们的结果显示了三个关键发现：（1）LLMs经常不加批判地采用提供的信息，在引入对抗性内容时显著损害了其预测性能；（2）有益的指导往往不足以抵消对抗性信息的负面影响；（3）在Kaggle数据集中，LLMs常在处理时间序列数据、在不同折中一致地应用特征工程以及正确解释分类变量方面出现错误。这些发现突显示出了当前模型在批判性评估和利用专家知识方面的重要差距，强调了开发更 robust、知识导向的自动化数据科学系统的重要研究方向。 

---
# How Does LLM Reasoning Work for Code? A Survey and a Call to Action 

**Title (ZH)**: 大规模语言模型推理工作机制研究：面向代码的一篇综述与呼吁 

**Authors**: Ira Ceka, Saurabh Pujar, Irene Manotas, Gail Kaiser, Baishakhi Ray, Shyam Ramji  

**Link**: [PDF](https://arxiv.org/pdf/2506.13932)  

**Abstract**: The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. These advancements have extended into the domain of code, facilitating complex tasks such as code generation, translation, summarization, and repair. However, their utility for real-world deployment in-the-wild has only recently been studied, particularly on software engineering (SWE) tasks such as GitHub issue resolution. In this study, we examine the code reasoning techniques that underlie the ability to perform such tasks, and examine the paradigms used to drive their performance. Our contributions in this paper are: (1) the first dedicated survey on code reasoning for code tasks, highlighting overarching strategies, hybrid and agentic approaches; (2) a taxonomy of various techniques used to drive code reasoning; (3) a comprehensive overview of performance on common benchmarks and a showcase of new, under-explored benchmarks with high potential in SWE; (4) an exploration on how core properties of code can be used to explain different reasoning techniques; and (5) gaps and potentially under-explored areas for future research. 

**Abstract (ZH)**: 大型语言模型的崛起在广泛自然语言任务中带来了显著改进，这些进步已延伸至编程领域，促进了代码生成、翻译、摘要和修复等复杂任务。然而，它们在现实世界部署中的实用性仅近期得到了研究，特别是在软件工程任务如GitHub问题解决方面。本文研究了实现这些任务所需的代码推理技术及其背后的范式。本文的主要贡献包括：（1）首份专注于代码任务的代码推理专论，强调总体策略、混合和自主方法；（2）各种驱动代码推理技术的分类法；（3）对常见基准的综合性能概述，以及展示具有高潜力的新未探索基准在软件工程中的应用；（4）探讨代码核心属性如何解释不同推理技术；（5）未来研究中可能未被充分探索的空白领域。 

---
# Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations 

**Title (ZH)**: AQI对齐质量指数：超越拒绝样本：基于潜在几何、聚类发散和层wise聚合表示的内在对齐诊断指数 

**Authors**: Abhilekh Borah, Chhavi Sharma, Danush Khanna, Utkarsh Bhatt, Gurpreet Singh, Hasnat Md Abdullah, Raghav Kaushik Ravi, Vinija Jain, Jyoti Patel, Shubham Singh, Vasu Sharma, Arpita Vats, Rahul Raja, Aman Chadha, Amitava Das  

**Link**: [PDF](https://arxiv.org/pdf/2506.13901)  

**Abstract**: Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.
To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.
Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area. 

**Abstract (ZH)**: 对齐不再是奢侈品，而是必要条件。随着大型语言模型（LLMs）进入高风险领域如教育、医疗、治理和法律，其行为必须可靠地反映人类对齐的价值观和安全约束。然而，当前的评估仍严重依赖于行为代理，如拒绝率、G-Eval评分和毒性分类器，这些方法都存在关键的盲点。对齐的模型往往容易受到 Jailbreak、生成的随机性以及对齐伪装的影响。

为解决这一问题，我们引入了对齐质量指数（AQI）。这是一种新型的几何和提示不变的度量标准，通过分析潜在空间中安全和不安全激活之间的分离程度来实体现评估LLM对齐质量。通过结合Davies-Bouldin评分（DBS）、邓安指数（DI）、谢伊-比利指数（XBI）和卡林斯基-哈拉布扎指数（CHI）等多种度量标准，AQI捕捉聚类质量，以检测隐藏的对齐偏差和Jailbreak风险，即使在输出看似合规的情况下也是如此。AQI还可作为对齐伪装的早期预警信号，提供一种在不了解行为背景的情况下进行安全审计的稳健、解码不变工具。

此外，我们提出LITMUS数据集以在这些具有挑战性的条件下促进稳健评估。LITMUS在不同模型（DPO、GRPO和RLHF条件）下的实证测试表明，AQI与外部评委评分相关，并能揭示拒绝率指标未能揭示的漏洞。我们将我们的实现公开，以促进该领域的进一步研究。 

---
# Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles 

**Title (ZH)**: 使用多语言数字谜题探究语言和数学推理在语言模型中的交互作用 

**Authors**: Antara Raaghavi Bhattacharya, Isabel Papadimitriou, Kathryn Davidson, David Alvarez-Melis  

**Link**: [PDF](https://arxiv.org/pdf/2506.13886)  

**Abstract**: Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models. 

**Abstract (ZH)**: 不同语言中数值系统的构建和组合方式多样，尽管人类能够一致地适应这种多样性，大型语言模型在涉及跨语言数值系统的语言数学问题上却存在困难，而人类能够成功地解决这类问题。我们通过一系列实验解开语言和数学在数字中的作用，发现除非数学操作明确使用已知符号（如“+”、“×”等）标记，模型无法始终解决问题。进一步的消融实验探讨了数字构建和组合的个别参数如何影响性能。尽管人类利用对数字的语义理解推断隐含的组成结构，大型语言模型似乎缺乏这种隐含数字结构的概念。我们得出结论，从人类规模数据中的隐含模式灵活推断组合规则的能力仍然是当前推理模型面临的公开挑战。 

---
# FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development via Automatic Evaluation 

**Title (ZH)**: FrontendBench：一种通过自动评估来评估LLM在前端开发能力的基准测试 

**Authors**: Hongda Zhu, Yiwen Zhang, Bing Zhao, Jingzhe Ding, Siyao Liu, Tong Liu, Dandan Wang, Yanan Liu, Zhaojian Li  

**Link**: [PDF](https://arxiv.org/pdf/2506.13832)  

**Abstract**: Large Language Models (LLMs) have made significant strides in front-end code generation. However, existing benchmarks exhibit several critical limitations: many tasks are overly simplistic, test cases often lack rigor, and end-to-end validation is absent. These issues hinder the accurate assessment of model performance. To address these challenges, we present FrontendBench, a benchmark co-developed by humans and LLMs. FrontendBench categorizes tasks based on code functionality and incorporates interactive test scenarios, enabling a more comprehensive and practical evaluation of front-end code generation capabilities. The benchmark comprises 148 meticulously crafted prompt-test case pairs spanning five levels of web components, from basic UI elements to complex interactive features. Each task reflects realistic front-end development challenges. Furthermore, we introduce an automatic evaluation framework that executes generated code within a sandbox environment and assesses outcomes using predefined test scripts. This framework achieves a 90.54% agreement rate with expert human evaluations, demonstrating high reliability. We benchmark several state-of-the-art LLMs on FrontendBench and observe substantial performance disparities in handling real-world front-end tasks. These results highlight FrontendBench as a reliable and scalable benchmark, supporting consistent multimodal evaluation and providing a robust foundation for future research in front-end code generation. Our data and code will be released soon. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在前端代码生成方面取得了显著进展。然而，现有的基准测试存在几个关键限制：许多任务过于简单，测试案例通常缺乏严谨性，缺乏端到端验证。这些问题阻碍了对模型性能的准确评估。为解决这些挑战，我们提出了由人类和LLMs共同开发的FrontendBench基准测试。FrontendBench根据代码功能对任务进行分类，并结合了交互式测试场景，从而实现对前端代码生成能力更全面和实用的评估。该基准测试包含148个精心编制的提示-测试案例配对，涵盖五个级别的Web组件，从基本的UI元素到复杂的交互功能。每个任务都反映了实际的前端开发挑战。此外，我们介绍了一种自动评估框架，在沙箱环境中执行生成的代码，并使用预定义的测试脚本评估结果。该框架与专家人工评估的共识率达到了90.54%，显示出很高的可靠性。我们在FrontendBench上对几个最先进的LLMs进行了基准测试，发现它们在处理实际前端任务方面的性能存在显著差异。这些结果突显了FrontendBench作为可靠且可扩展基准测试的价值，支持一致的多模态评估，并为未来前端代码生成的研究提供了坚实的基础。我们的数据和代码即将发布。 

---
# Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge 

**Title (ZH)**: 使用大语言模型进行结构化程序合成：IPARC挑战赛的研究结果与见解 

**Authors**: Shraddha Surana, Ashwin Srinivasan, Michael Bain  

**Link**: [PDF](https://arxiv.org/pdf/2506.13820)  

**Abstract**: The IPARC Challenge, inspired by ARC, provides controlled program synthesis tasks over synthetic images to evaluate automatic program construction, focusing on sequence, selection, and iteration. This set of 600 tasks has resisted automated solutions. This paper presents a structured inductive programming approach with LLMs that successfully solves tasks across all IPARC categories. The controlled nature of IPARC reveals insights into LLM-based code generation, including the importance of prior structuring, LLMs' ability to aid structuring (requiring human refinement), the need to freeze correct code, the efficiency of code reuse, and how LLM-generated code can spark human creativity. These findings suggest valuable mechanisms for human-LLM collaboration in tackling complex program synthesis. 

**Abstract (ZH)**: IPARC挑战赛：受ARC启发，提供基于合成图像的受控程序合成任务，以评估自动程序构建能力，重点关注序列、选择和迭代。这套包含600个任务的挑战题库难以被自动化解决。本文提出了一种基于LLM的结构化归纳编程方法，成功解决了所有IPARC类别的任务。IPARC的受控性质揭示了基于LLM的代码生成中的见解，包括先前结构化的重要性、LLM辅助结构化（需要人类细化）、正确代码冻结的需求、代码重用的效率，以及LLM生成代码激发人类创造力的方式。这些发现表明了人类与LLM协作解决复杂程序合成问题的关键机制。 

---
# Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study 

**Title (ZH)**: 基于大型语言模型的路由器多智能体架构在基础设计自动化中的潜在应用：一项任务分类与专家遴选研究 

**Authors**: Sompote Youwai, David Phim, Vianne Gayl Murcia, Rianne Clair Onas  

**Link**: [PDF](https://arxiv.org/pdf/2506.13811)  

**Abstract**: This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice. 

**Abstract (ZH)**: 基于路由器的多Agent系统在智能任务分类和专家选择中的土木工程基础设计自动化研究 

---
# Dr. GPT Will See You Now, but Should It? Exploring the Benefits and Harms of Large Language Models in Medical Diagnosis using Crowdsourced Clinical Cases 

**Title (ZH)**: Dr. GPT 现在会见到你，但是应该吗？——基于 crowdsourced 临床案例探讨大型语言模型在医疗诊断中的利弊 

**Authors**: Bonam Mingole, Aditya Majumdar, Firdaus Ahmed Choudhury, Jennifer L. Kraschnewski, Shyam S. Sundar, Amulya Yadav  

**Link**: [PDF](https://arxiv.org/pdf/2506.13805)  

**Abstract**: The proliferation of Large Language Models (LLMs) in high-stakes applications such as medical (self-)diagnosis and preliminary triage raises significant ethical and practical concerns about the effectiveness, appropriateness, and possible harmfulness of the use of these technologies for health-related concerns and queries. Some prior work has considered the effectiveness of LLMs in answering expert-written health queries/prompts, questions from medical examination banks, or queries based on pre-existing clinical cases. Unfortunately, these existing studies completely ignore an in-the-wild evaluation of the effectiveness of LLMs in answering everyday health concerns and queries typically asked by general users, which corresponds to the more prevalent use case for LLMs. To address this research gap, this paper presents the findings from a university-level competition that leveraged a novel, crowdsourced approach for evaluating the effectiveness of LLMs in answering everyday health queries. Over the course of a week, a total of 34 participants prompted four publicly accessible LLMs with 212 real (or imagined) health concerns, and the LLM generated responses were evaluated by a team of nine board-certified physicians. At a high level, our findings indicate that on average, 76% of the 212 LLM responses were deemed to be accurate by physicians. Further, with the help of medical professionals, we investigated whether RAG versions of these LLMs (powered with a comprehensive medical knowledge base) can improve the quality of responses generated by LLMs. Finally, we also derive qualitative insights to explain our quantitative findings by conducting interviews with seven medical professionals who were shown all the prompts in our competition. This paper aims to provide a more grounded understanding of how LLMs perform in real-world everyday health communication. 

**Abstract (ZH)**: 大型语言模型在高 stakes 应用中的普及，如医学（自我）诊断和初步分类，引起了对其在与健康相关问题和查询使用中的有效性和适当性的伦理和实践关注，以及潜在的有害性。尽管已有工作考虑了大型语言模型回答专家撰写的健康查询/提示、医学考试题库中的问题或基于现有临床案例的查询的有效性，但这些现有研究完全忽视了对大型语言模型在回答普通用户通常提出的日常健康问题和查询的有效性的在野生环境中的评估，这对应了大型语言模型更常见的使用场景。为了填补这一研究缺口，本文介绍了采用新颖的众包方法评估大型语言模型在回答日常健康查询方面有效性的一场大学级竞赛的研究成果。在为期一周的时间内，共有34名参与者向四种可访问的大型语言模型提出了212个真实的（或虚构的）健康问题，由一组九名认证的内科医生团队评估了大型语言模型生成的回复。总体而言，我们的研究发现，内科医生认为76%的212个大型语言模型的回复是准确的。此外，在医疗专业人员的帮助下，我们研究了具备全面医学知识库支持的检索增强（RAG）版本的大型语言模型是否能提高其生成回复的质量。最后，我们通过对七名在我们的竞赛中看到所有提示的医疗专业人员进行访谈，获得了定性的洞见来解释我们的定量发现。本文旨在提供对大型语言模型在现实世界日常健康沟通中表现的更为现实的理解。 

---
# Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework 

**Title (ZH)**: 通过LLM和模型上下文协议增强临床决策支持和EHR Insights：一个开源MCP-FHIR框架 

**Authors**: Abul Ehtesham, Aditi Singh, Saket Kumar  

**Link**: [PDF](https://arxiv.org/pdf/2506.13800)  

**Abstract**: Enhancing clinical decision support (CDS), reducing documentation burdens, and improving patient health literacy remain persistent challenges in digital health. This paper presents an open-source, agent-based framework that integrates Large Language Models (LLMs) with HL7 FHIR data via the Model Context Protocol (MCP) for dynamic extraction and reasoning over electronic health records (EHRs). Built on the established MCP-FHIR implementation, the framework enables declarative access to diverse FHIR resources through JSON-based configurations, supporting real-time summarization, interpretation, and personalized communication across multiple user personas, including clinicians, caregivers, and patients. To ensure privacy and reproducibility, the framework is evaluated using synthetic EHR data from the SMART Health IT sandbox (this https URL), which conforms to the FHIR R4 standard. Unlike traditional approaches that rely on hardcoded retrieval and static workflows, the proposed method delivers scalable, explainable, and interoperable AI-powered EHR applications. The agentic architecture further supports multiple FHIR formats, laying a robust foundation for advancing personalized digital health solutions. 

**Abstract (ZH)**: 增强临床决策支持（CDS），减轻文档负担，提高患者健康素养仍然是数字健康领域持续的挑战。本文提出了一种基于代理的开源框架，通过Model Context Protocol (MCP) 集成大规模语言模型（LLMs）与HL7 FHIR数据，实现电子健康记录（EHR）的动态提取和推理。该框架建立在现有的MCP-FHIR实现基础上，通过JSON配置实现声明式的访问多种FHIR资源，支持跨多个用户角色（包括临床医生、护理人员和患者）的实时总结、解释和个人化沟通。为确保隐私和可重复性，该框架使用SMART Health IT sandbox提供的符合FHIR R4标准的合成EHR数据进行评估。与依赖硬编码检索和静态工作流的传统方法不同，所提出的方法可提供可扩展、可解释且互操作的基于AI的EHR应用程序。该代理架构进一步支持多种FHIR格式，为推进个性化数字健康解决方案奠定了坚实的基础。 

---
# ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries 

**Title (ZH)**: ClimateChat：设计数据与方法以调优大语言模型以回答气候变化查询 

**Authors**: Zhou Chen, Xiao Wang, Yuanhong Liao, Ming Lin, Yuqi Bai  

**Link**: [PDF](https://arxiv.org/pdf/2506.13796)  

**Abstract**: As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs. 

**Abstract (ZH)**: 随着全球气候变暖问题日益严峻，气候科学研究的需求持续增长。以大型语言模型（LLMs）为代表的自然语言处理技术在气候变暖研究中得到了广泛应用，为决策者和公众提供了必要的信息支持。一些研究通过构建气候变暖相关的指令数据并调优LLMs，提高了模型在相关任务上的性能。然而，当前研究在高效生成大量高精度气候变暖指令数据方面仍显不足，限制了气候变暖LLMs的进一步发展。本研究介绍了一种自动构建指令数据的方法，该方法利用文档中的事实和背景知识生成指令，并通过网络爬取和收集种子指令来增强指令数据的多样性。使用该方法构建了一个名为ClimateChat-Corpus的气候变暖指令数据集，用于调优开源LLMs，最终得到一个名为ClimateChat的LLM。评估结果显示，ClimateChat在气候变暖问答任务上的性能显著提升。此外，我们还评估了不同基础模型和指令数据对LLMs性能的影响，证明了其能够适应广泛的气候变暖科学研究任务，强调了选择合适的基础模型进行指令调优的重要性。本研究为构建气候变暖指令数据和训练特定于气候变暖的LLMs提供了有价值的参考和实证支持。 

---
# XGraphRAG: Interactive Visual Analysis for Graph-based Retrieval-Augmented Generation 

**Title (ZH)**: XGraphRAG：基于图检索增强生成的交互式可视化分析 

**Authors**: Ke Wang, Bo Pan, Yingchaojie Feng, Yuwei Wu, Jieyi Chen, Minfeng Zhu, Wei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2506.13782)  

**Abstract**: Graph-based Retrieval-Augmented Generation (RAG) has shown great capability in enhancing Large Language Model (LLM)'s answer with an external knowledge base. Compared to traditional RAG, it introduces a graph as an intermediate representation to capture better structured relational knowledge in the corpus, elevating the precision and comprehensiveness of generation results. However, developers usually face challenges in analyzing the effectiveness of GraphRAG on their dataset due to GraphRAG's complex information processing pipeline and the overwhelming amount of LLM invocations involved during graph construction and query, which limits GraphRAG interpretability and accessibility. This research proposes a visual analysis framework that helps RAG developers identify critical recalls of GraphRAG and trace these recalls through the GraphRAG pipeline. Based on this framework, we develop XGraphRAG, a prototype system incorporating a set of interactive visualizations to facilitate users' analysis process, boosting failure cases collection and improvement opportunities identification. Our evaluation demonstrates the effectiveness and usability of our approach. Our work is open-sourced and available at this https URL. 

**Abstract (ZH)**: 基于图的检索增强生成（GraphRAG）在通过外部知识库增强大型语言模型（LLM）的回答方面展现了巨大的能力。与传统的RAG相比，它引入了一种图作为中间表示，以更好地捕捉语料库中的结构化关系知识，提升生成结果的精确度和全面性。然而，由于图RAG复杂的信息处理管道以及在图构建和查询过程中涉及的大量大型语言模型调用，开发者通常面临分析图RAG在他们数据集上的效果的挑战，这限制了图RAG的可解释性和可访问性。本研究提出了一种可视化分析框架，帮助RAG开发者识别图RAG的关键召回并追踪这些召回通过图RAG管道的过程。基于此框架，我们开发了XGraphRAG原型系统，该系统包含一系列交互式可视化工具，以促进用户分析过程，增强故障案例收集和改进机会的识别。我们的评估证明了该方法的有效性和实用性。我们的工作已开源，并可从此链接获取：this https URL。 

---
# MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs 

**Title (ZH)**: MobiEdit: 资源高效的知识编辑用于个性化本地部署的语言模型 

**Authors**: Zhenyan Lu, Daliang Xu, Dongqi Cai, Zexi Li, Wei Liu, Fangming Liu, Shangguang Wang, Mengwei Xu  

**Link**: [PDF](https://arxiv.org/pdf/2506.13772)  

**Abstract**: Large language models (LLMs) are deployed on mobile devices to power killer applications such as intelligent assistants. LLMs pre-trained on general corpora often hallucinate when handling personalized or unseen queries, leading to incorrect or outdated responses. Knowledge editing addresses this by identifying and adjusting a small crucial portion of model weights, without compromising the general knowledge. However, prior knowledge editing methods are impractical to run on local devices due to the resource-heavy backpropagation (BP) needed for updates. We present MobiEdit, the first mobile knowledge editing framework that enables efficient LLM personalization on commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces full-precision BP with quantized forward-only gradient estimation, thus compatible with the energy-efficient mobile neural processing units (NPUs). MobiEdit replaces full-precision backpropagation with quantized forward-only gradient estimation, making it compatible with energy-efficient mobile NPUs. To further improve gradient estimation efficiency, we introduce two optimizations: an early stoping mechanism that adaptively terminates editing upon success and a prefix cache that reuses computation across steps. Our approach enables real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile devices with 7.6$\times$ less memory, 14.7 $\times$ less energy and 3.6$\times$ less latency compared to previous knowledge editing methods. 

**Abstract (ZH)**: 大规模语言模型（LLMs）部署在移动设备上以驱动智能助手等杀手级应用。预训练于通用语料库的大规模语言模型在处理个人化或未见过的查询时经常会发生幻觉，导致不正确或过时的响应。知识编辑通过识别并调整模型权重中的一个小而关键的部分来解决这一问题，而不牺牲一般知识。然而，先前的知识编辑方法由于需要进行更新的计算强度大的反向传播（BP），在本地设备上运行不实际。我们提出了MobiEdit，这是首个能够在商用现货（COTS）移动设备上实现高效大规模语言模型个性化的移动知识编辑框架。MobiEdit用量化前向传播梯度估计取代了全精度的反向传播，使其与节能的移动神经处理单元（NPUs）兼容。为了进一步提高梯度估计效率，我们引入了两种优化：一个适应性终止机制，可在成功后自动终止编辑，以及前缀缓存，可在步骤之间重用计算。我们的方法使MobiEdit能够在COTS移动设备上对一个3亿参数的模型（Qwen2.5-3B-Instruct）进行实时编辑，与之前的知识编辑方法相比，实现7.6倍的内存节省、14.7倍的能效提升和3.6倍的延迟减少。 

---
# LittleBit: Ultra Low-Bit Quantization via Latent Factorization 

**Title (ZH)**: LittleBit: 超低比特量化通过潜在因子分解 

**Authors**: Banseok Lee, Dongkyu Kim, Youngcheon You, Youngmin Kim  

**Link**: [PDF](https://arxiv.org/pdf/2506.13771)  

**Abstract**: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments. 

**Abstract (ZH)**: 小比特：一种用于极端大规模语言模型压缩的新方法 

---
