{'arxiv_id': 'arXiv:2508.04361', 'title': 'OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing', 'authors': 'Fuqing Bie, Shiyu Huang, Xijia Tao, Zhiqin Fang, Leyi Pan, Junzhe Chen, Min Ren, Liuyu Xiang, Zhaofeng He', 'link': 'https://arxiv.org/abs/2508.04361', 'abstract': 'While generalist foundation models like Gemini and GPT-4o demonstrate impressive multi-modal competence, existing evaluations fail to test their intelligence in dynamic, interactive worlds. Static benchmarks lack agency, while interactive benchmarks suffer from a severe modal bottleneck, typically ignoring crucial auditory and temporal cues. To bridge this evaluation chasm, we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate, but to probe the fusion and reasoning capabilities of agentic models across the full sensory spectrum. Built on a core philosophy of modality interdependence, OmniPlay comprises a suite of five game environments that systematically create scenarios of both synergy and conflict, forcing agents to perform genuine cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal models reveals a critical dichotomy: they exhibit superhuman performance on high-fidelity memory tasks but suffer from systemic failures in challenges requiring robust reasoning and strategic planning. We demonstrate that this fragility stems from brittle fusion mechanisms, which lead to catastrophic performance degradation under modality conflict and uncover a counter-intuitive "less is more" paradox, where removing sensory information can paradoxically improve performance. Our findings suggest that the path toward robust AGI requires a research focus beyond scaling to explicitly address synergistic fusion. Our platform is available for anonymous review at this https URL.', 'abstract_zh': '虽然通用基础模型如Gemini和GPT-4展示了令人印象深刻的多模态能力，但现有的评估未能测试其在动态交互世界中的智能水平。静态基准缺乏自主性，而交互基准则遭受严重的模态瓶颈，通常忽视了关键的听觉和时间线索。为了弥合这一评估鸿沟，我们介绍了一种名为OmniPlay的诊断基准，不仅用于评估，还用于探究具有自主性的模型在全感统领域的融合和推理能力。基于模态相互依赖的核心理念，OmniPlay包含五个游戏环境，系统地创造了协同与冲突的场景，迫使代理执行真正的跨模态推理。我们对六种顶级全模态模型的全面评估揭示了一个关键二分法：它们在高保真记忆任务中表现出超乎人类的表现，但在需要稳健推理和战略规划的挑战中表现出系统性失败。我们证明这种脆弱性源自脆弱的融合机制，在模态冲突下导致性能灾难性下降，并揭示了一个反直觉的“少即是多”悖论，即去除感统信息反而可能提高性能。我们的研究结果表明，通向稳健AGI的道路需要超越扩大规模，在明确解决协同融合方面进行研究。该平台可在此 https:// 进行匿名审查。', 'title_zh': '全方位播放：全方位模态模型在全方位模态游戏玩法规则评估'}
{'arxiv_id': 'arXiv:2508.04566', 'title': 'CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization', 'authors': 'Jinxing Zhou, Ziheng Zhou, Yanghao Zhou, Yuxin Mao, Zhangling Duan, Dan Guo', 'link': 'https://arxiv.org/abs/2508.04566', 'abstract': 'The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally localize events in untrimmed videos that occur simultaneously in both the audio and visual modalities. This paper explores DAVEL under a new and more challenging weakly-supervised setting (W-DAVEL task), where only video-level event labels are provided and the temporal boundaries of each event are unknown. We address W-DAVEL by exploiting \\textit{cross-modal salient anchors}, which are defined as reliable timestamps that are well predicted under weak supervision and exhibit highly consistent event semantics across audio and visual modalities. Specifically, we propose a \\textit{Mutual Event Agreement Evaluation} module, which generates an agreement score by measuring the discrepancy between the predicted audio and visual event classes. Then, the agreement score is utilized in a \\textit{Cross-modal Salient Anchor Identification} module, which identifies the audio and visual anchor features through global-video and local temporal window identification mechanisms. The anchor features after multimodal integration are fed into an \\textit{Anchor-based Temporal Propagation} module to enhance event semantic encoding in the original temporal audio and visual features, facilitating better temporal localization under weak supervision. We establish benchmarks for W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive experiments demonstrate that our method achieves state-of-the-art performance.', 'abstract_zh': '密集跨模态事件定位（DAVEL）任务旨在在同时发生在音频和视觉模态中的未修剪视频事件中进行时空定位。本文在新的更具挑战性的弱监督设置（W-DAVEL任务）下探索DAVEL，其中仅提供视频级事件标签而每个事件的时间边界未知。我们通过利用跨模态显著锚点来解决W-DAVEL，这些锚点在弱监督下可靠且预测良好，并且在音频和视觉模态之间表现出高度一致的事件语义。具体来说，我们提出了一种相互事件一致性评估模块，通过测量预测音频和视觉事件类别的差异来生成一致性评分。然后，该一致性评分用于跨模态显著锚点识别模块，该模块通过全局视频和局部时间窗口识别机制来识别音频和视觉锚点特征。经过多模态集成后的锚点特征被送入基于锚点的时空传播模块，以增强原始时空音频和视觉特征中的事件语义编码，在弱监督下实现更好的时空定位。我们在UnAV-100和ActivityNet1.3数据集上建立了W-DAVEL的基准。大量实验表明，我们的方法达到了最先进的性能。', 'title_zh': 'CLASP: 基于跨模态显著锚点的语义传播弱监督密集音频-视觉事件定位'}
{'arxiv_id': 'arXiv:2508.04427', 'title': 'Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models', 'authors': 'Md Raisul Kibria, Sébastien Lafond, Janan Arslan', 'link': 'https://arxiv.org/abs/2508.04427', 'abstract': 'Multimodal learning has witnessed remarkable advancements in recent years, particularly with the integration of attention-based models, leading to significant performance gains across a variety of tasks. Parallel to this progress, the demand for explainable artificial intelligence (XAI) has spurred a growing body of research aimed at interpreting the complex decision-making processes of these models. This systematic literature review analyzes research published between January 2020 and early 2024 that focuses on the explainability of multimodal models. Framed within the broader goals of XAI, we examine the literature across multiple dimensions, including model architecture, modalities involved, explanation algorithms and evaluation methodologies. Our analysis reveals that the majority of studies are concentrated on vision-language and language-only models, with attention-based techniques being the most commonly employed for explanation. However, these methods often fall short in capturing the full spectrum of interactions between modalities, a challenge further compounded by the architectural heterogeneity across domains. Importantly, we find that evaluation methods for XAI in multimodal settings are largely non-systematic, lacking consistency, robustness, and consideration for modality-specific cognitive and contextual factors. Based on these findings, we provide a comprehensive set of recommendations aimed at promoting rigorous, transparent, and standardized evaluation and reporting practices in multimodal XAI research. Our goal is to support future research in more interpretable, accountable, and responsible mulitmodal AI systems, with explainability at their core.', 'abstract_zh': '多模态学习在近年来取得了显著进展，特别是在注意力机制的整合下，显著提升了各种任务的性能。与此同时，可解释人工智能（XAI）的需求促进了旨在解读这些模型复杂决策过程的研究增长。本文系统文献综述分析了2020年1月至2024年初之间发表的相关研究，重点关注多模态模型的可解释性。基于XAI的更广泛目标，我们从模型架构、涉及的模态、解释算法和评估方法等多个维度审视了文献。分析表明，大多数研究集中在视觉-语言和纯语言模型上，注意力机制是主要的解释方法。然而，这些方法在捕捉不同模态间全面交互方面往往不足，而且跨领域模型架构差异进一步加剧了这一挑战。重要的是，我们发现，多模态环境下XAI的评估方法缺乏系统的、一致的、稳健的考量，且忽视了模态特异性认知和上下文因素。基于这些发现，我们提出了一套全面的建议，旨在促进多模态XAI研究中严格的、透明的、标准化的评估和报告实践。我们的目标是支持更具可解释性、可问责性和负责任性的多模态AI系统的研究，以解释性为核心。', 'title_zh': '解码多模态迷宫：基于解释性的多模态注意机制模型采用系统的综述'}
{'arxiv_id': 'arXiv:2508.04418', 'title': 'Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation', 'authors': 'Jinxing Zhou, Yanghao Zhou, Mingfei Han, Tong Wang, Xiaojun Chang, Hisham Cholakkal, Rao Muhammad Anwer', 'link': 'https://arxiv.org/abs/2508.04418', 'abstract': 'Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects in audible videos based on given reference expressions. Prior works typically rely on learning latent embeddings via multimodal fusion to prompt a tunable SAM/SAM2 decoder for segmentation, which requires strong pixel-level supervision and lacks interpretability. From a novel perspective of explicit reference understanding, we propose TGS-Agent, which decomposes the task into a Think-Ground-Segment process, mimicking the human reasoning procedure by first identifying the referred object through multimodal analysis, followed by coarse-grained grounding and precise segmentation. To this end, we first propose Ref-Thinker, a multimodal language model capable of reasoning over textual, visual, and auditory cues. We construct an instruction-tuning dataset with explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The object description inferred by Ref-Thinker is used as an explicit prompt for Grounding-DINO and SAM2, which perform grounding and segmentation without relying on pixel-level supervision. Additionally, we introduce R\\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and reasoning-intensive references for better evaluating model generalization. Our approach achieves state-of-the-art results on both standard Ref-AVSBench and proposed R\\textsuperscript{2}-AVSBench. Code will be available at this https URL.', 'abstract_zh': '参考听视融合分割（Ref-AVS）旨在基于给定的参考表达对可听视频中的目标对象进行分割。 prior works 通常依赖多模态融合学习潜在嵌入，并将其提示可调的 SAM/SAM2 解码器以进行分割，这需要强大的像素级监督并且缺乏可解释性。从显式参考理解的新视角出发，我们提出了 TGS-Agent，该方法将任务分解为思考-接地-分割过程，模仿人类推理程序，首先通过多模态分析识别被引用的对象，随后进行粗粒度接地和精确分割。为此，我们首先提出了 Ref-Thinker，这是一种能够处理文本、视觉和听觉线索的多模态语言模型。我们构建了一个包含显式对象意识的指令调优数据集，用于 Ref-Thinker 的微调。Ref-Thinker 推断的对象描述用作对 Grounding-DINO 和 SAM2 的显式提示，后者在不需要像素级监督的情况下执行接地和分割。此外，我们引入了 R²-AVSBench，这是一种新的基准，包含语言多样性和推理密集型的参考，以更好地评估模型的泛化能力。我们的方法在标准 Ref-AVSBench 和提出的 R²-AVSBench 上均取得了最先进的结果。代码将在此 URL 下开源。', 'title_zh': '三思而后分段：一种基于对象的推理代理用于引用视听分割'}
{'arxiv_id': 'arXiv:2508.03785', 'title': 'SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons', 'authors': 'Teodor Chiaburu, Vipin Singh, Frank Haußer, Felix Bießmann', 'link': 'https://arxiv.org/abs/2508.03785', 'abstract': 'While recent advances in foundation models have improved the state of the art in many domains, some problems in empirical sciences could not benefit from this progress yet. Soil horizon classification, for instance, remains challenging because of its multimodal and multitask characteristics and a complex hierarchically structured label taxonomy. Accurate classification of soil horizons is crucial for monitoring soil health, which directly impacts agricultural productivity, food security, ecosystem stability and climate resilience. In this work, we propose $\\textit{SoilNet}$ - a multimodal multitask model to tackle this problem through a structured modularized pipeline. Our approach integrates image data and geotemporal metadata to first predict depth markers, segmenting the soil profile into horizon candidates. Each segment is characterized by a set of horizon-specific morphological features. Finally, horizon labels are predicted based on the multimodal concatenated feature vector, leveraging a graph-based label representation to account for the complex hierarchical relationships among soil horizons. Our method is designed to address complex hierarchical classification, where the number of possible labels is very large, imbalanced and non-trivially structured. We demonstrate the effectiveness of our approach on a real-world soil profile dataset. All code and experiments can be found in our repository: this https URL', 'abstract_zh': '尽管基础模型的 recent 进展已在许多领域改善了最先进的技术，但一些经验科学中的问题仍未从中受益。例如，由于土层分类具有多模态和多任务特性以及复杂的分层标签分类体系，土壤剖面的准确分类仍然具有挑战性。准确的土层分类对于监测土壤健康至关重要，这直接影响农业生产力、粮食安全、生态系统稳定性和气候韧性。在本工作中，我们提出了一种多模态多任务模型 $\\textit{SoilNet}$，通过一个结构化的模块化管道来解决这一问题。我们的方法结合图像数据和时空元数据，首先预测深度标志，将土壤剖面划分为土层候选。每个区域都由一组特定于该土层的形状特征表征。最后，基于多模态特征向量的拼接，通过图基的标签表示预测土层标签，以考虑土壤剖面之间复杂的分层关系。我们的方法旨在解决复杂层次分类问题，其中可能的标签数量庞大、分布不均且结构复杂。我们在一个真实的土壤剖面数据集上展示了该方法的有效性。所有代码和实验都可以在我们的库中找到：this https URL。', 'title_zh': '土壤网络：一种多模态多任务模型，用于土壤层次分类'}
{'arxiv_id': 'arXiv:2508.03735', 'title': 'StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization', 'authors': 'Gopalji Gaur, Mohammadreza Zolfaghari, Thomas Brox', 'link': 'https://arxiv.org/abs/2508.03735', 'abstract': "Generating a coherent sequence of images that tells a visual story, using text-to-image diffusion models, often faces the critical challenge of maintaining subject consistency across all story scenes. Existing approaches, which typically rely on fine-tuning or retraining models, are computationally expensive, time-consuming, and often interfere with the model's pre-existing capabilities. In this paper, we follow a training-free approach and propose an efficient consistent-subject-generation method. This approach works seamlessly with pre-trained diffusion models by introducing masked cross-image attention sharing to dynamically align subject features across a batch of images, and Regional Feature Harmonization to refine visually similar details for improved subject consistency. Experimental results demonstrate that our approach successfully generates visually consistent subjects across a variety of scenarios while maintaining the creative abilities of the diffusion model.", 'abstract_zh': '使用文本到图像扩散模型生成连贯影像序列以讲述视觉故事中保持主题一致性：一种无需训练的方法及其应用', 'title_zh': 'StorySync: 不需训练的文本到图像生成中的区域谐振一致性'}
{'arxiv_id': 'arXiv:2508.03734', 'title': 'A Survey of Multimodal Ophthalmic Diagnostics: From Task-Specific Approaches to Foundational Models', 'authors': 'Xiaoling Luo, Ruli Zheng, Qiaojian Zheng, Zibo Du, Shuo Yang, Meidan Ding, Qihao Xu, Chengliang Liu, Linlin Shen', 'link': 'https://arxiv.org/abs/2508.03734', 'abstract': 'Visual impairment represents a major global health challenge, with multimodal imaging providing complementary information that is essential for accurate ophthalmic diagnosis. This comprehensive survey systematically reviews the latest advances in multimodal deep learning methods in ophthalmology up to the year 2025. The review focuses on two main categories: task-specific multimodal approaches and large-scale multimodal foundation models. Task-specific approaches are designed for particular clinical applications such as lesion detection, disease diagnosis, and image synthesis. These methods utilize a variety of imaging modalities including color fundus photography, optical coherence tomography, and angiography. On the other hand, foundation models combine sophisticated vision-language architectures and large language models pretrained on diverse ophthalmic datasets. These models enable robust cross-modal understanding, automated clinical report generation, and decision support. The survey critically examines important datasets, evaluation metrics, and methodological innovations including self-supervised learning, attention-based fusion, and contrastive alignment. It also discusses ongoing challenges such as variability in data, limited annotations, lack of interpretability, and issues with generalizability across different patient populations. Finally, the survey outlines promising future directions that emphasize the use of ultra-widefield imaging and reinforcement learning-based reasoning frameworks to create intelligent, interpretable, and clinically applicable AI systems for ophthalmology.', 'abstract_zh': '视觉障碍代表了全球健康的重大挑战，多模态成像提供了互补的信息，对于准确的眼科诊断至关重要。本综述系统地回顾了截至2025年的眼科多模态深度学习方法的最新进展。综述主要集中在两类方法上：任务特定的多模态方法和大型多模态基础模型。任务特定的方法是为特定的临床应用设计的，如病变检测、疾病诊断和图像合成。这些方法利用了包括彩色眼底摄影、光学相干断层扫描和血管造影等多种成像模态。另一方面，基础模型结合了复杂的视觉-语言架构和在多样化眼科数据集上预训练的大型语言模型。这些模型能够实现稳健的跨模态理解、自动临床报告生成和决策支持。综述严格评估了重要数据集、评价指标和方法学创新，包括自我监督学习、注意力融合和对比对齐。此外，综述还讨论了持续存在的挑战，如数据的可变性、标注不足、缺乏可解释性以及在不同患者群体间的泛化问题。最后，综述提出了有前景的未来方向，强调超广域成像和基于强化学习的推理框架的应用，以创建智能、可解释且适用于眼科的AI系统。', 'title_zh': '多模态眼科诊断综述：从任务特定方法到基础模型'}
{'arxiv_id': 'arXiv:2508.03722', 'title': 'Multimodal Video Emotion Recognition with Reliable Reasoning Priors', 'authors': 'Zhepeng Wang, Yingjian Zhu, Guanghao Dong, Hongzhu Yi, Feng Chen, Xinming Wang, Jun Xie', 'link': 'https://arxiv.org/abs/2508.03722', 'abstract': 'This study investigates the integration of trustworthy prior reasoning knowledge from MLLMs into multimodal emotion recognition. We employ Gemini to generate fine-grained, modality-separable reasoning traces, which are injected as priors during the fusion stage to enrich cross-modal interactions. To mitigate the pronounced class-imbalance in multimodal emotion recognition, we introduce Balanced Dual-Contrastive Learning, a loss formulation that jointly balances inter-class and intra-class distributions. Applied to the MER2024 benchmark, our prior-enhanced framework yields substantial performance gains, demonstrating that the reliability of MLLM-derived reasoning can be synergistically combined with the domain adaptability of lightweight fusion networks for robust, scalable emotion recognition.', 'abstract_zh': '本研究探讨了将可信先验推理知识集成到多模态情感识别中的方法。我们利用Gemini生成精细粒度且模态可分的推理痕迹，在融合阶段注入这些先验知识以丰富跨模态交互。为缓解多模态情感识别中的类别不平衡问题，我们引入了一种联合平衡跨类别和类内分布的Balanced Dual-Contrastive Learning损失函数。将该框架应用于MER2024基准测试，我们增强了的框架显著提升了性能，证明了来自MLLM的推理可靠性和轻量级融合网络的领域适应性可以协同增强鲁棒可扩展的情感识别。', 'title_zh': '基于可靠推理先验的多模态视频情感识别'}
{'arxiv_id': 'arXiv:2508.03715', 'title': 'Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors', 'authors': 'Bertram Fuchs, Mehdi Ejtehadi, Ana Cisnal, Jürgen Pannek, Anke Scheel-Sailer, Robert Riener, Inge Eriks-Hoogland, Diego Paez-Granados', 'link': 'https://arxiv.org/abs/2508.03715', 'abstract': 'Autonomic Dysreflexia (AD) is a potentially life-threatening condition characterized by sudden, severe blood pressure (BP) spikes in individuals with spinal cord injury (SCI). Early, accurate detection is essential to prevent cardiovascular complications, yet current monitoring methods are either invasive or rely on subjective symptom reporting, limiting applicability in daily file. This study presents a non-invasive, explainable machine learning framework for detecting AD using multimodal wearable sensors. Data were collected from 27 individuals with chronic SCI during urodynamic studies, including electrocardiography (ECG), photoplethysmography (PPG), bioimpedance (BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three commercial devices. Objective AD labels were derived from synchronized cuff-based BP measurements. Following signal preprocessing and feature extraction, BorutaSHAP was used for robust feature selection, and SHAP values for explainability. We trained modality- and device-specific weak learners and aggregated them using a stacked ensemble meta-model. Cross-validation was stratified by participants to ensure generalizability. HR- and ECG-derived features were identified as the most informative, particularly those capturing rhythm morphology and variability. The Nearest Centroid ensemble yielded the highest performance (Macro F1 = 0.77+/-0.03), significantly outperforming baseline models. Among modalities, HR achieved the highest area under the curve (AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature features contributed less to overall accuracy, consistent with missing data and low specificity. The model proved robust to sensor dropout and aligned well with clinical AD events. These results represent an important step toward personalized, real-time monitoring for individuals with SCI.', 'abstract_zh': '自主反射异常（AD）是一种以脊髓损伤（SCI）个体突然、严重的血压（BP）骤升为特征的潜在生命威胁状况。早期、准确的检测对预防心血管并发症至关重要，但当前的监测方法要么侵入性，要么依赖主观症状报告，限制了其在日常护理中的应用。本研究提出了一种使用多模态可穿戴传感器的非侵入性、可解释的机器学习框架，用于检测AD。数据来自27名慢性SCI个体在尿动力学研究期间使用的包括心电图（ECG）、光电容积描记法（PPG）、生物阻抗（BioZ）、体温、呼吸率（RR）和心率（HR）在内的指标，覆盖了三个商业设备。客观的AD标签是从同步的袖带式BP测量中推导出来的。经过信号预处理和特征提取后，使用BorutaSHAP进行稳健的特征选择，并使用SHAP值实现可解释性。我们训练了特定模态和设备的弱学习器，并使用堆叠集成元模型进行聚合。交叉验证通过参与者进行分层以确保泛化能力。HR和ECG提取的特征被识别为最具信息性，特别是那些捕捉节律形态和变异性的特征。最近邻中心集成模型表现最佳（宏F1值 = 0.77±0.03），显著优于基线模型。在不同的模态中，HR的表现最佳（AUC = 0.93），其次是ECG（0.88）和PPG（0.86）。RR和体温特征对整体准确性贡献较小，这与数据缺失和低特异性一致。该模型对传感器故障具有鲁棒性，并与临床AD事件吻合良好。这些结果代表了朝着个性化、实时监控SCI个体的重要一步。', 'title_zh': '使用多模态可穿戴传感器检测脊髓损伤个体的自主神经反射异常'}
