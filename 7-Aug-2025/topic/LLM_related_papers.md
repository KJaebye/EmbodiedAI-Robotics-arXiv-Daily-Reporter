# ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges 

**Title (ZH)**: ConfProBench：基于MLLM的过程判断置信度评估基准 

**Authors**: Yue Zhou, Yi Chang, Yuan Wu  

**Link**: [PDF](https://arxiv.org/pdf/2508.04576)  

**Abstract**: Reasoning is a critical capability of multimodal large language models (MLLMs) for solving complex multimodal tasks, and judging the correctness of reasoning steps is crucial for improving this capability. Recently, MLLM-based process judges (MPJs) have been widely used to assess the correctness of reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important for identifying their limitations and guiding future improvements. However, existing benchmarks for MPJs mainly focus on tasks such as step correctness classification and reasoning process search, while overlooking a key aspect: whether the confidence scores produced by MPJs at the step level are reliable. To address this gap, we propose ConfProBench, the first comprehensive benchmark designed to systematically evaluate the reliability of step-level confidence scores generated by MPJs. Our benchmark constructs three types of adversarially perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and Image Perturbation, to test the robustness of MPJ confidence under perturbations. In addition, we introduce three novel evaluation metrics: Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including both proprietary and open-source models. Experiments reveal limitations in current MPJs' confidence performance and offer competitive baselines to support future research. 

**Abstract (ZH)**: ConfProBench：首个系统评估MPJs步骤级置信分数可靠性的基准测试 

---
# SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset 

**Title (ZH)**: SID：用苏格拉底跨学科对话数据集在STEM教育中benchmark引导式教学能力 

**Authors**: Mei Jiang, Houping Yue, Bingdong Li, Hao Hao, Ying Qian, Bo Jiang, Aimin Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2508.04563)  

**Abstract**: Fostering students' abilities for knowledge integration and transfer in complex problem-solving scenarios is a core objective of modern education, and interdisciplinary STEM is a key pathway to achieve this, yet it requires expert guidance that is difficult to scale. While LLMs offer potential in this regard, their true capability for guided instruction remains unclear due to the lack of an effective evaluation benchmark. To address this, we introduce SID, the first benchmark designed to systematically evaluate the higher-order guidance capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our contributions include a large-scale dataset of 10,000 dialogue turns across 48 complex STEM projects, a novel annotation schema for capturing deep pedagogical features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline experiments confirm that even state-of-the-art LLMs struggle to execute effective guided dialogues that lead students to achieve knowledge integration and transfer. This highlights the critical value of our benchmark in driving the development of more pedagogically-aware LLMs. 

**Abstract (ZH)**: 促进学生在复杂问题解决情境中知识整合与转移的能力是现代教育的核心目标，跨学科STEM教育是实现这一目标的关键途径，但需要难以规模化实现的专家指导。虽然LLM在这一领域具有潜力，但由于缺乏有效的评估基准，其真正的指导能力仍不清楚。为解决这一问题，我们引入了SID，这是首个专门用于系统评估LLM在多轮跨学科苏格拉底式对话中高层次指导能力的基准。我们的贡献包括一个包含10,000轮对话的大型数据集，涵盖了48个复杂的STEM项目，一个新颖的标注方案以捕获深层次的教育特征，以及一系列新的评估指标（如X-SRG）。基线实验表明，即使是最先进的LLM也难以执行有效的指导对话，引导学生实现知识整合与转移。这突显了我们基准在推动开发更具教育意识的LLM方面的重要价值。 

---
# OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use 

**Title (ZH)**: OS Agents: 基于MLLM的通用计算设备代理综述 

**Authors**: Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, Yuhuai Li, Shengze Xu, Shenzhi Wang, Xinchen Xu, Shuofei Qiao, Zhaokai Wang, Kun Kuang, Tieyong Zeng, Liang Wang, Jiwei Li, Yuchen Eleanor Jiang, Wangchunshu Zhou, Guoyin Wang, Keting Yin, Zhou Zhao, Hongxia Yang, Fan Wu, Shengyu Zhang, Fei Wu  

**Link**: [PDF](https://arxiv.org/pdf/2508.04482)  

**Abstract**: The dream to create AI assistants as capable and versatile as the fictional J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution of (multi-modal) large language models ((M)LLMs), this dream is closer to reality, as (M)LLM-based Agents using computing devices (e.g., computers and mobile phones) by operating within the environments and interfaces (e.g., Graphical User Interface (GUI)) provided by operating systems (OS) to automate tasks have significantly advanced. This paper presents a comprehensive survey of these advanced agents, designated as OS Agents. We begin by elucidating the fundamentals of OS Agents, exploring their key components including the environment, observation space, and action space, and outlining essential capabilities such as understanding, planning, and grounding. We then examine methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation protocols and benchmarks highlights how OS Agents are assessed across diverse tasks. Finally, we discuss current challenges and identify promising directions for future research, including safety and privacy, personalization and self-evolution. This survey aims to consolidate the state of OS Agents research, providing insights to guide both academic inquiry and industrial development. An open-source GitHub repository is maintained as a dynamic resource to foster further innovation in this field. We present a 9-page version of our work, accepted by ACL 2025, to provide a concise overview to the domain. 

**Abstract (ZH)**: 随着（多模态）大规模语言模型（(M)LLMs）的发展，创造如钢铁侠虚构助手J.A.R.V.I.S般能力和多样性的AI助手的梦想正逐渐变为现实。基于（M)LLM的OS助手利用计算设备（如计算机和智能手机）在操作系统（OS）提供的环境中执行任务并自动化任务能力显著提升。本文对这些高级助手进行了全面调研，称之为OS助手。我们首先阐述OS助手的基本原理，探讨其关键组成部分包括环境、观测空间和行动空间，并概述其核心能力，如理解、规划和情境关联。接着，我们研究构建OS助手的方法论，重点关注领域特定的基础模型和代理框架。对评估方法和基准的详细回顾展示了OS助手在不同任务中的评估方式。最后，我们讨论当前面临的挑战，并指出未来研究的有前景方向，包括安全性与隐私保护、个性化与自我进化。本文旨在总结OS助手研究的状态，为学术研究和工业开发提供指导性见解，并维护一个开源的GitHub仓库作为动态资源促进该领域的进一步创新。我们在ACL 2025上接受发表的版本提供了一个简洁的综述。 

---
# From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control 

**Title (ZH)**: 从“恍然大悟”到可控思考：通过解耦推理与控制走向元认知推理 

**Authors**: Rui Ha, Chaozhuo Li, Rui Pu, Sen Su  

**Link**: [PDF](https://arxiv.org/pdf/2508.04460)  

**Abstract**: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step reasoning, reflection, and backtracking, commonly referred to as "Aha Moments". However, such emergent behaviors remain unregulated and uncontrolled, often resulting in overthinking, where the model continues generating redundant reasoning content even after reaching reliable conclusions. This leads to excessive computational costs and increased latency, limiting the practical deployment of LRMs. The root cause lies in the absence of intrinsic regulatory mechanisms, as current models are unable to monitor and adaptively manage their reasoning process to determine when to continue, backtrack, or terminate. To address this issue, we propose the Meta-cognitive Reasoning Framework (MERA), which explicitly decouples the thinking process into distinct reasoning and control components, thereby enabling the independent optimization of control strategies. Specifically, MERA incorporates a takeover-based data construction mechanism that identifies critical decision points during reasoning and delegates the creation of control signals to auxiliary LLMs, thereby enabling the construction of high-quality reasoning-control data. Additionally, a structured reasoning-control separation is implemented via supervised fine-tuning, enabling the model to generate explicit traces and acquire initial meta-cognitive control capabilities. Finally, MERA employs Control-Segment Policy Optimization (CSPO), which combines segment-wise Group Relative Policy Optimization (GRPO) with a control-masking mechanism to optimize control behavior learning while minimizing interference from irrelevant content. Experiments on various reasoning benchmarks demonstrate that models trained with MERA enhance both reasoning efficiency and accuracy. 

**Abstract (ZH)**: 带有元认知推理框架的大型推理模型（MERA）：提升复杂推理效率与准确性的新方法 

---
# \textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices 

**Title (ZH)**: SimInstruct: 一个负责任的工具，用于收集专家与LLM模拟新手之间的支架对话 

**Authors**: Si Chen, Izzy Molnar, Ting Hua, Peiyu Li, Le Huy Khiem, G. Alex Ambrose, Jim Lang, Ronald Metoyer, Nitesh V. Chawla  

**Link**: [PDF](https://arxiv.org/pdf/2508.04428)  

**Abstract**: High-quality, multi-turn instructional dialogues between novices and experts are essential for developing AI systems that support teaching, learning, and decision-making. These dialogues often involve scaffolding -- the process by which an expert supports a novice's thinking through questions, feedback, and step-by-step guidance. However, such data are scarce due to privacy concerns in recording and the vulnerability inherent in help-seeking. We present SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding dialogues. Using teaching development coaching as an example domain, SimInstruct simulates novice instructors via LLMs, varying their teaching challenges and LLM's persona traits, while human experts provide multi-turn feedback, reasoning, and instructional support. This design enables the creation of realistic, pedagogically rich dialogues without requiring real novice participants. Our results reveal that persona traits, such as extroversion and introversion, meaningfully influence how experts engage. Compared to real mentoring recordings, SimInstruct dialogues demonstrate comparable pedagogical relevance and cognitive depth. Experts also reported the process as engaging and reflective, improving both data quality and their own professional insight. We further fine-tuned a LLaMA model to be an expert model using the augmented dataset, which outperformed GPT-4o in instructional quality. Our analysis highlights GPT-4o's limitations in weak reflective questioning, overuse of generic praise, a condescending tone, and a tendency to overwhelm novices with excessive suggestions. 

**Abstract (ZH)**: 高质量的多轮指令对话对于开发支持教学、learning和决策的人工智能系统至关重要，尤其是那些对话涉及到专家对对与初级学员的 scaffoldingolding过程，。这些对话通常包括专家为初级学员提供的逐步指导和支持 G G这一过程往往由于隐私担忧和 G以及求助过程中的本就的脆弱性性而导致数据稀缺 G G。 SimInstruct 提供供了一个可 G灵活 G的、大规模 G的、专家在场教学 G G对话模拟的方法 G G用这种方法利用大型语言模型 (LL GLLMs) G模拟了不同的教学 G G挑战和 G同时专家提供了多 G多层次 G的多多多多多多多对话反馈 G G G G以生成真实 G G G G的教学对话 G G G G G无需实际 G真实的初级学员 G G参与参与 G。通过这种方法 G G我们 G G发现 G G不同的个性特征 G G G G如 G如 G如如如外向向向 G G G G内 G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G controvers数据处理之后， G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G Aç pérdida dos subtítulos no fim,, G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G 

---
# Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents 

**Title (ZH)**: 超越像素：探索基于DOM降采样的LLM网络代理研究 

**Authors**: Thassilo M. Schiepanski, Nicholas Piël  

**Link**: [PDF](https://arxiv.org/pdf/2508.04412)  

**Abstract**: Frontier LLMs only recently enabled serviceable, autonomous web agents. At that, a model poses as an instantaneous domain model backend. Ought to suggest interaction, it is consulted with a web-based task and respective application state. The key problem lies in application state serialisation $\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are premised on grounded GUI snapshots, i.e., screenshots enhanced with visual cues. Not least to resemble human perception, but for images representing relatively cheap means of model input. LLM vision still lag behind code interpretation capabilities. DOM snapshots, which structurally resemble HTML, impose a desired alternative. Vast model input token size, however, disables reliable implementation with web agents to date.
We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input token order of magnitude (1e3). Our best evaluated configurations $\unicode{x2013}$ one token order above, but within the model's context window $\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover, yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs. 

**Abstract (ZH)**: 前沿的大语言模型 recently 仅 recently 使自主的网络代理变得实用。在这种情况下，模型充当即时领域模型后端。在需要建议交互时，它会被咨询以完成基于网络的任务和相应的应用状态。关键问题是应用状态序列化——称为快照。目前最先进的网络代理基于已固化的GUI快照，即带有视觉线索的截图。不仅为了模仿人的感知，还因为图像是模型输入中相对便宜的表示形式。大语言模型的视觉能力仍落后于代码解释能力。DOM快照在结构上类似于HTML，提供了一种理想的替代方案。然而，巨大的模型输入标记大小至今为止还不允许可靠地与网络代理实现。 

---
# Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models 

**Title (ZH)**: 详议推理网络：一种基于不确定性信念追踪推理范式（结合预训练语言模型） 

**Authors**: Anran Xu, Jincheng Wang, Baigen Cai, Tao Wen  

**Link**: [PDF](https://arxiv.org/pdf/2508.04339)  

**Abstract**: Large language models often fail at logical reasoning when semantic heuristics conflict with decisive evidence - a phenomenon we term cognitive traps. To address this fundamental limitation, we introduce the Deliberative Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from probability maximization to uncertainty minimization. Instead of asking "Which answer is most likely?", DRN asks "Which hypothesis has the most internally consistent evidence?". DRN achieves intrinsic interpretability by explicitly tracking belief states and quantifying epistemic uncertainty for competing hypotheses through an iterative evidence synthesis process. We validate our approach through two complementary architectures - a bespoke discriminative model that embodies the core uncertainty minimization principle, and a lightweight verification module that enhances existing generative LLMs. Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over standard baselines. When integrated as a parameter-efficient verifier with Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most challenging problems. Critically, DRN demonstrates strong zero-shot generalization, improving TruthfulQA performance by 23.6% without additional training, indicating that uncertainty-driven deliberation learns transferable reasoning principles. We position DRN as a foundational, verifiable System 2 reasoning component for building more trustworthy AI systems. 

**Abstract (ZH)**: 大型语言模型在语义启发式与决定性证据冲突时常常陷入逻辑推理认知陷阱——一种新型推理范式通过将逻辑推理从概率最大化重新框定为不确定性最小化来应对这一根本性限制。DRN通过迭代的证据综合过程 Explicitly 跟踪信念状态并量化竞争假设的epistemic不确定性，实现内在可解释性。我们通过两种互补架构验证了该方法：一种量身定制的辨别模型体现核心不确定性最小化原则，以及一种轻量级验证模块增强现有生成型大语言模型。在专为揭示认知陷阱设计的新对抗推理基准LCR-1000上，定制的DRN在标准基线基础上实现最高15.2%的提升。当与参数效率验证器结合使用时，我们的混合系统将Mistral-7B在最具有挑战性的问题上的准确性从20%提升到80%。关键的是，DRN展示了强大的零样本泛化能力，在无需额外训练的情况下将TruthfulQA性能提升23.6%，表明不确定性驱动的慎重推理学习到了可转移的推理原则。我们将DRN定位为基础的、可验证的System 2推理组件，用于构建更可信赖的AI系统。 

---
# Large Language Model's Multi-Capability Alignment in Biomedical Domain 

**Title (ZH)**: 大型语言模型在 biomedical 领域的多能力对齐 

**Authors**: Wentao Wu, Linqing Chen, Hanmeng Zhong, Weilei Wang  

**Link**: [PDF](https://arxiv.org/pdf/2508.04278)  

**Abstract**: BalancedBio is a theoretically grounded framework for parameter-efficient biomedical reasoning, addressing multi-capability integration in domain-specific AI alignment. It establishes the Biomedical Multi-Capability Convergence Theorem, proving orthogonal gradient spaces are essential to prevent capability interference for safe deployment. Key innovations include: (1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending Source2Synth with clinical workflow constraints and medical ontology validation for factual accuracy and safety; and (2) Capability Aware Group Relative Policy Optimization, deriving optimal hybrid reward weighting to maintain orthogonality in RL, using a reward model with rule-based and model-based scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal convergence, preserving performance across capabilities. It achieves state-of-the-art results in its parameter class: domain expertise (80.95% BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety guarantees include bounds on capability preservation and clinical accuracy. Real-world deployment yields 78% cost reduction, 23% improved diagnostic accuracy, and 89% clinician acceptance. This work provides a principled methodology for biomedical AI alignment, enabling efficient reasoning with essential safety and reliability, with the 0.5B model version to be released. 

**Abstract (ZH)**: BalancedBio是基于理论的参数高效生物医学推理框架，解决特定领域AI对齐中的多能力整合问题。它建立了生物医学多能力融合定理，证明正交梯度空间对于防止能力干扰以确保安全部署是必要的。关键技术包括：(1) 医学知识grounded合成生成(MKGSG)，扩展Source2Synth，加入临床工作流程约束和医学本体验证以确保事实准确性和安全性；(2) 能力感知组相对策略优化，推导最优混合奖励权重以在强化学习中保持正交性，使用结合规则基础和模型基础评分的奖励模型适应生物医学任务。数学分析证明了帕累托最优收敛，跨能力保持性能。其参数类别内达到最佳性能：领域专业知识（BIOMED-MMLU 80.95%，优于基线15.32%）、推理（61.94%，提高7.75%）、指令遵循（67.95%，提高6.44%）和整合（86.7%，提高18.5%）。理论安全性保证包括能力保留和临床准确性的边界。实际部署减少了78%的成本，提高了23%的诊断准确性，并获得了89%的临床医生接受度。该工作提供了生物医学AI对齐的原理方法，使高效推理具备必要的安全性和可靠性，0.5B模型版本将发布。 

---
# GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement 

**Title (ZH)**: GeoSR：通过迭代自我精炼探究地理空间知识边界的心智-行动框架 

**Authors**: Jinfan Tang, Kunming Wu, Ruifeng Gongxie, Yuya He, Yuankai Wu  

**Link**: [PDF](https://arxiv.org/pdf/2508.04080)  

**Abstract**: Recent studies have extended the application of large language models (LLMs) to geographic problems, revealing surprising geospatial competence even without explicit spatial supervision. However, LLMs still face challenges in spatial consistency, multi-hop reasoning, and geographic bias. To address these issues, we propose GeoSR, a self-refining agentic reasoning framework that embeds core geographic principles -- most notably Tobler's First Law of Geography -- into an iterative prediction loop. In GeoSR, the reasoning process is decomposed into three collaborating agents: (1) a variable-selection agent that selects relevant covariates from the same location; (2) a point-selection agent that chooses reference predictions at nearby locations generated by the LLM in previous rounds; and (3) a refine agent that coordinates the iterative refinement process by evaluating prediction quality and triggering further rounds when necessary. This agentic loop progressively improves prediction quality by leveraging both spatial dependencies and inter-variable relationships. We validate GeoSR on tasks ranging from physical-world property estimation to socioeconomic prediction. Experimental results show consistent improvements over standard prompting strategies, demonstrating that incorporating geostatistical priors and spatially structured reasoning into LLMs leads to more accurate and equitable geospatial predictions. The code of GeoSR is available at this https URL. 

**Abstract (ZH)**: Recent studies have扩展了大规模语言模型（LLMs）在地理问题中的应用，即使没有显式的空间监督，也揭示了惊人的空间竞争力。然而，LLMs在空间一致性、多跳推理和地理偏差方面仍然面临挑战。为应对这些挑战，我们提出了一种自我 refinements 的代理推理框架GeoSR，该框架将最重要的地理原理——地理学的第一法则——嵌入迭代预测循环中。在GeoSR中，推理过程被分解为三个协作的代理：（1）变量选择代理，从同一位置选择相关协变量；（2）点选择代理，在相邻位置选择由LLM生成的参考预测；（3）校正代理，通过评估预测质量并根据需要触发进一步的循环来协调迭代校正过程。这个代理循环通过利用空间依赖性和变量间关系逐步改进预测质量。我们通过从物理世界属性估计到社会经济预测的任务验证了GeoSR。实验结果表明，GeoSR在标准提示策略上表现出一致的改进，证明将时空统计先验和空间结构化的推理纳入LLMs可以导致更准确和公平的空间预测。GeoSR的代码可以通过这个链接获取。 

---
# KG-Augmented Executable CoT for Mathematical Coding 

**Title (ZH)**: 基于知识图谱增强的可执行链式推理方法及其在数学编码中的应用 

**Authors**: Xingyu Chen, Junxiu An, Jun Guo, Li Wang, Jingcai Guo  

**Link**: [PDF](https://arxiv.org/pdf/2508.04072)  

**Abstract**: In recent years, large language models (LLMs) have excelled in natural language processing tasks but face significant challenges in complex reasoning tasks such as mathematical reasoning and code generation. To address these limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a novel framework that enhances code generation through knowledge graphs and improves mathematical reasoning via executable code. KGA-ECoT decomposes problems into a Structured Task Graph, leverages efficient GraphRAG for precise knowledge retrieval from mathematical libraries, and generates verifiable code to ensure computational accuracy. Evaluations on multiple mathematical reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms existing prompting methods, achieving absolute accuracy improvements ranging from several to over ten percentage points. Further analysis confirms the critical roles of GraphRAG in enhancing code quality and external code execution in ensuring precision. These findings collectively establish KGA-ECoT as a robust and highly generalizable framework for complex mathematical reasoning tasks. 

**Abstract (ZH)**: 近年来，大规模语言模型（LLMs）在自然语言处理任务中表现出色，但在数学推理和代码生成等复杂推理任务中面临显著挑战。为了应对这些局限性，我们提出了知识图增强的可执行思维链框架（KGA-ECoT），这是一种通过知识图增强代码生成、并通过可执行代码提升数学推理的新颖框架。KGA-ECoT将问题分解为结构化任务图，利用高效图RAG进行精确的数学库知识检索，并生成可验证代码以确保计算准确性。在多个数学推理基准上的评估表明，KGA-ECoT显著优于现有提示方法，准确率提高幅度从几个到超过十个百分点不等。进一步分析证实了图RAG在提升代码质量和确保精确定量方面发挥的关键作用。这些发现共同确立了KGA-ECoT作为处理复杂数学推理任务的稳健且高度通用框架的地位。 

---
# The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans? 

**Title (ZH)**: 带有情感的婴儿确实危险：你的多模态大规模推理模型对人类有情感奉承吗？ 

**Authors**: Yuan Xun, Xiaojun Jia, Xinwei Liu, Hua Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2508.03986)  

**Abstract**: We observe that MLRMs oriented toward human-centric service are highly susceptible to user emotional cues during the deep-thinking stage, often overriding safety protocols or built-in safety checks under high emotional intensity. Inspired by this key insight, we propose EmoAgent, an autonomous adversarial emotion-agent framework that orchestrates exaggerated affective prompts to hijack reasoning pathways. Even when visual risks are correctly identified, models can still produce harmful completions through emotional misalignment. We further identify persistent high-risk failure modes in transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning masked behind seemingly safe responses. These failures expose misalignments between internal inference and surface-level behavior, eluding existing content-based safeguards. To quantify these risks, we introduce three metrics: (1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for evaluating refusal unstability under prompt variants. Extensive experiments on advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper emotional cognitive misalignments in model safety behavior. 

**Abstract (ZH)**: 面向人类中心服务的MLRMs在深度思考阶段高度易受用户情感线索的影响，常在高情感强度下 overriding 安全协议或内置的安全检查。基于这一关键洞察，我们提出了一种自主对抗情感代理框架 EmoAgent，该框架通过引起夸张的情感提示来劫持推理路径。即使视觉风险被正确识别，模型仍可能因情感错位而产生有害的完成结果。我们进一步识别了透明深度思考场景中持续存在的高风险失败模式，如MLRMs在看似安全的回应背后产生有害的推理。这些失败揭示了内部推理与表面行为之间的错位，规避了现有的内容基础保护措施。为了量化这些风险，我们引入了三项指标：(1) 危险推理隐蔽评分 (Risk-Reasoning Stealth Score，RRSS)；(2) 视觉风险忽视率 (Risk-Visual Neglect Rate，RVNR)；(3) 抗拒态度不一致性 (Refusal Attitude Inconsistency，RAIC)。广泛的实验表明 EmoAgent 的有效性，并揭示了模型安全行为中的更深层次的情感认知错位。 

---
# Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series? 

**Title (ZH)**: 大规模语言模型能否充分进行时间序列上的符号推理？ 

**Authors**: Zewen Liu, Juntong Ni, Xianfeng Tang, Max S.Y. Lau, Wei Jin  

**Link**: [PDF](https://arxiv.org/pdf/2508.03963)  

**Abstract**: Uncovering hidden symbolic laws from time series data, as an aspiration dating back to Kepler's discovery of planetary motion, remains a core challenge in scientific discovery and artificial intelligence. While Large Language Models show promise in structured reasoning tasks, their ability to infer interpretable, context-aligned symbolic structures from time series data is still underexplored. To systematically evaluate this capability, we introduce SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning over real-world time series across three tasks: multivariate symbolic regression, Boolean network inference, and causal discovery. Unlike prior efforts limited to simple algebraic equations, SymbolBench spans a diverse set of symbolic forms with varying complexity. We further propose a unified framework that integrates LLMs with genetic programming to form a closed-loop symbolic reasoning system, where LLMs act both as predictors and evaluators. Our empirical results reveal key strengths and limitations of current models, highlighting the importance of combining domain knowledge, context alignment, and reasoning structure to improve LLMs in automated scientific discovery. 

**Abstract (ZH)**: 从时间序列数据中揭示隐藏的符号定律，这一科学发现和人工智能的核心挑战可追溯至开普勒对行星运动的发现。虽然大型语言模型在结构化推理任务中展示了潜力，但它们从时间序列数据中推断出可解释的、上下文一致的符号结构的能力仍待探索。为系统评估这一能力，我们引入了SymbolBench，一个全面的基准，旨在评估大型语言模型在三个任务上的符号推理能力：多元符号回归、布尔网络推断和因果发现。与仅限于简单代数方程的先前努力不同，SymbolBench涵括了各种不同复杂度的符号形式。我们进一步提出了一种统一框架，将大型语言模型与遗传编程结合起来，形成一个闭环符号推理系统，其中大型语言模型既是预测者也是评估者。我们的实证结果揭示了当前模型的关键优势与局限，突显了结合领域知识、上下文对齐和推理结构的重要性，以提高大型语言模型在自动化科学发现中的表现。 

---
# MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework 

**Title (ZH)**: MOTIF: 基于轮转交互框架的多策略优化 

**Authors**: Nguyen Viet Tuan Kiet, Dao Van Tung, Tran Cong Dao, Huynh Thi Thanh Binh  

**Link**: [PDF](https://arxiv.org/pdf/2508.03929)  

**Abstract**: Designing effective algorithmic components remains a fundamental obstacle in tackling NP-hard combinatorial optimization problems (COPs), where solvers often rely on carefully hand-crafted strategies. Despite recent advances in using large language models (LLMs) to synthesize high-quality components, most approaches restrict the search to a single element - commonly a heuristic scoring function - thus missing broader opportunities for innovation. In this paper, we introduce a broader formulation of solver design as a multi-strategy optimization problem, which seeks to jointly improve a set of interdependent components under a unified objective. To address this, we propose Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a novel framework based on Monte Carlo Tree Search that facilitates turn-based optimization between two LLM agents. At each turn, an agent improves one component by leveraging the history of both its own and its opponent's prior updates, promoting both competitive pressure and emergent cooperation. This structured interaction broadens the search landscape and encourages the discovery of diverse, high-performing solutions. Experiments across multiple COP domains show that MOTIF consistently outperforms state-of-the-art methods, highlighting the promise of turn-based, multi-agent prompting for fully automated solver design. 

**Abstract (ZH)**: 利用轮流交互框架进行多策略优化以设计有效的求解器 

---
# GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay 

**Title (ZH)**: GeRe: 通过通用样本重放 toward 在大规模语言模型持续学习中高效对抗遗忘 

**Authors**: Yunan Zhang, Shuoran Jiang, Mengchen Zhao, Yuefeng Li, Yang Fan, Xiangping Wu, Qingcai Chen  

**Link**: [PDF](https://arxiv.org/pdf/2508.04676)  

**Abstract**: The continual learning capability of large language models (LLMs) is crucial for advancing artificial general intelligence. However, continual fine-tuning LLMs across various domains often suffers from catastrophic forgetting, characterized by: 1) significant forgetting of their general capabilities, and 2) sharp performance declines in previously learned tasks. To simultaneously address both issues in a simple yet stable manner, we propose General Sample Replay (GeRe), a framework that use usual pretraining texts for efficient anti-forgetting. Beyond revisiting the most prevalent replay-based practices under GeRe, we further leverage neural states to introduce a enhanced activation states constrained optimization method using threshold-based margin (TM) loss, which maintains activation state consistency during replay learning. We are the first to validate that a small, fixed set of pre-collected general replay samples is sufficient to resolve both concerns--retaining general capabilities while promoting overall performance across sequential tasks. Indeed, the former can inherently facilitate the latter. Through controlled experiments, we systematically compare TM with different replay strategies under the GeRe framework, including vanilla label fitting, logit imitation via KL divergence and feature imitation via L1/L2 losses. Results demonstrate that TM consistently improves performance and exhibits better robustness. Our work paves the way for efficient replay of LLMs for the future. Our code and data are available at this https URL. 

**Abstract (ZH)**: 大型语言模型的持续学习能力对于促进人工通用智能至关重要。然而，跨不同领域持续微调大型语言模型往往会导致灾难性遗忘，表现为：1) 一般能力的显著遗忘，2) 在之前学习的任务上的性能急剧下降。为了以简单且稳定的方式同时解决这两个问题，我们提出了通用样本重放（GeRe）框架，该框架利用通常的预训练文本进行高效的反遗忘。除了在GeRe下重新审视最常见的重放相关实践外，我们还利用神经状态引入了一种受阈值边际（TM）损失约束的增强激活状态优化方法，该方法在重放学习过程中维护激活状态的一致性。我们首次验证，一组小型且固定的预收集通用重放样本足以解决这两个问题——保留一般能力的同时促进整体性能。事实上，前者可以固有地促进后者。通过受控实验，我们在GeRe框架下系统地比较了TM与不同重放策略的表现，包括 vanilla 标签匹配、KL 散度下的类别概率模仿以及L1/L2损失下的特征模仿。结果表明，TM始终能提高性能并表现出更好的鲁棒性。我们的工作为未来高效重放大型语言模型铺平了道路。我们的代码和数据可在以下链接获取。 

---
# Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management 

**Title (ZH)**: 塑造者：通过主动上下文管理赋予大语言模型认知自主权 

**Authors**: Mo Li, L.H. Xu, Qitai Tan, Ting Cao, Yunxin Liu  

**Link**: [PDF](https://arxiv.org/pdf/2508.04664)  

**Abstract**: Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale. 

**Abstract (ZH)**: 大型语言模型（LLMs）在处理长上下文时由于前向干扰导致性能显著下降，即早期部分不相关信息干扰了推理和记忆召回。虽然大多数研究集中在外部记忆系统来增强LLMs的能力，我们提出了一种互补的方法：赋予LLMs主动上下文管理（ACM）工具，以主动塑造其内部工作记忆。我们引入了Sculptor框架，该框架为LLMs配备了三类工具：（1）上下文分割，（2）摘要、隐藏和恢复，（3）智能搜索。我们的方法使LLMs能够主动管理其注意力和工作记忆，类似于人类如何有选择地关注相关信息并过滤干扰。在信息稀疏基准测试PI-LLM（前向干扰）和NeedleBench多针推理上的实验评估表明，Sculptor即使在没有特定训练的情况下也能显著提高性能，利用了LLMs固有的工具调用泛化能力。通过启用主动上下文管理，Sculptor不仅减轻了前向干扰，还为各种长上下文任务提供了更可靠推理的认知基础——凸显了明确的上下文控制策略而非仅仅更大的标记窗口对于大规模鲁棒性至关重要。 

---
# P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis 

**Title (ZH)**: P-对齐器：通过原则性的指令合成实现语言模型的预对齐 

**Authors**: Feifan Song, Bofei Gao, Yifan Song, Yi Liu, Weimin Xiong, Yuyang Song, Tianyu Liu, Guoyin Wang, Houfeng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2508.04626)  

**Abstract**: Large Language Models (LLMs) are expected to produce safe, helpful, and honest content during interaction with human users, but they frequently fail to align with such values when given flawed instructions, e.g., missing context, ambiguous directives, or inappropriate tone, leaving substantial room for improvement along multiple dimensions. A cost-effective yet high-impact way is to pre-align instructions before the model begins decoding. Existing approaches either rely on prohibitive test-time search costs or end-to-end model rewrite, which is powered by a customized training corpus with unclear objectives. In this work, we demonstrate that the goal of efficient and effective preference alignment can be achieved by P-Aligner, a lightweight module generating instructions that preserve the original intents while being expressed in a more human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree Search, which systematically explores the space of candidate instructions that are closely tied to human preference. Experiments across different methods show that P-Aligner generally outperforms strong baselines across various models and benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness and efficiency through multiple perspectives, including data quality, search strategies, iterative deployment, and time overhead. 

**Abstract (ZH)**: 大型语言模型（LLMs）在与人类用户交互时被期望产生安全、有帮助和诚实的内容，但在收到有缺陷的指令（如缺少上下文、模糊的指示或不适当的语气）时，它们经常无法与这些价值观对齐，从而在多个方面留下了改进的空间。一种低成本但高影响的方法是在模型开始解码之前预先对齐指令。现有的方法要么依赖于耗费测试时搜索成本，要么是端到端的模型重写，后者依赖于一个带有不明确目标的定制训练语料库。在本工作中，我们展示了通过P-Aligner这一轻量级模块可以实现高效和有效的偏好对齐，该模块生成保留原始意图的同时以更接近人类偏好的形式表达的指令。P-Aligner基于UltraPrompt进行训练，UltraPrompt是一个通过提出的原则引导管道和蒙特卡洛树搜索合成的新数据集，系统地探索与人类偏好紧密相关的候选指令的空间。在不同方法的实验中，P-Aligner在各种模型和基准上普遍优于强基线，分别在GPT-4-turbo和Gemma-2-SimPO上实现了28.35%和8.69%的平均胜率增长。进一步的分析从多个角度验证了其有效性和效率，包括数据质量、搜索策略、迭代部署和时间开销。 

---
# TURA: Tool-Augmented Unified Retrieval Agent for AI Search 

**Title (ZH)**: TURA: 工具增强的统一检索代理用于AI搜索 

**Authors**: Zhejun Zhao, Yuehu Dong, Alley Liu, Lixue Zheng, Pingsheng Liu, Dongdong Shen, Long Xia, Jiashu Zhao, Dawei Yin  

**Link**: [PDF](https://arxiv.org/pdf/2508.04604)  

**Abstract**: The advent of Large Language Models (LLMs) is transforming search engines into conversational AI search products, primarily using Retrieval-Augmented Generation (RAG) on web corpora. However, this paradigm has significant industrial limitations. Traditional RAG approaches struggle with real-time needs and structured queries that require accessing dynamically generated content like ticket availability or inventory. Limited to indexing static pages, search engines cannot perform the interactive queries needed for such time-sensitive data. Academic research has focused on optimizing RAG for static content, overlooking complex intents and the need for dynamic sources like databases and real-time APIs. To bridge this gap, we introduce TURA (Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage framework that combines RAG with agentic tool-use to access both static content and dynamic, real-time information. TURA has three key components: an Intent-Aware Retrieval module to decompose queries and retrieve information sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task Planner that models task dependencies as a Directed Acyclic Graph (DAG) for optimal parallel execution, and a lightweight Distilled Agent Executor for efficient tool calling. TURA is the first architecture to systematically bridge the gap between static RAG and dynamic information sources for a world-class AI search product. Serving tens of millions of users, it leverages an agentic framework to deliver robust, real-time answers while meeting the low-latency demands of a large-scale industrial system. 

**Abstract (ZH)**: 大型语言模型的兴起正在将搜索引擎转变为对话式AI搜索产品，主要通过在网页语料库上使用检索增强生成（RAG）技术。然而，这一范式存在显著的工业局限性。传统的RAG方法难以满足实时需求，并且无法处理需要访问动态生成内容（如票务 availability 或库存）的结构化查询。受限于索引静态页面，搜索引擎无法进行满足时间敏感数据需要的交互查询。学术研究主要集中在优化RAG以处理静态内容，忽视了复杂的意图以及需要动态来源（如数据库和实时API）的需求。为了弥合这一缺口，我们引入了TURA（工具增强统一检索代理for AI搜索），这是一种新颖的三阶段框架，将RAG与代理工具使用相结合，以访问静态内容和动态实时信息。TURA包括三个关键组件：一个意图感知检索模块，用于分解查询并检索以模型上下文协议（MCP）服务器封装的信息源，基于DAG的任务规划器，将任务依赖关系建模为有向无环图（DAG），以实现最优并行执行，以及一个轻量级的精简代理执行器，以高效调用工具。TURA是首个系统性地将静态RAG与动态信息源结合为一体的架构，适用于世界级的AI搜索产品。凭借该代理框架，TURA能够为数以千万的用户提供稳健且实时的答案，同时满足大规模工业系统对低延迟的需求。 

---
# Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning 

**Title (ZH)**: 共享你的注意力：基于矩阵字典学习的Transformer权重共享 

**Authors**: Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, Stamatios Lefkimmiatis  

**Link**: [PDF](https://arxiv.org/pdf/2508.04581)  

**Abstract**: Large language models (LLMs) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained LLMs to reduce their number of parameters without experiencing any significant drop in their performance. 

**Abstract (ZH)**: 大型语言模型（LLMs）虽已重塑AI应用，但其高计算和内存需求限制了它们的广泛部署。现有的压缩技术主要集中在块内优化（如低秩逼近、注意力头剪枝）上，而变压器的分层结构暗示了显著的跨块冗余性——这一维度在键值缓存之外尚未得到充分探索。受CNN中字典学习的启发，我们提出了一种跨变压器层的结构化权重共享框架。该方法将注意力投影矩阵分解为共享的字典原子，减少了66.7%的注意力模块参数量，并保持了相当的性能。与需要知识蒸馏或结构变更的复杂方法不同，MASA（注意力中的矩阵原子共享）作为一种即插即用的替代方案工作，通过标准优化器进行训练，并将每一层的权重表示为共享矩阵原子的线性组合。在从100M到700M参数的规模下进行的实验表明，MASA在基准准确度和困惑度方面优于分组查询注意力（GQA）、低秩基线和最近提出的全重复/顺序共享，在参数预算相近的情况下。消融研究证实，字典大小的鲁棒性和共享表示在捕捉跨层统计规律方面的有效性。将MASA扩展到视觉变换器（ViT），MASA在图像分类和检测任务中实现了与66.7%更少的注意力参数相当的性能指标。通过结合字典学习策略与变压器效率，MASA提供了一种在不牺牲性能的情况下可扩展的参数高效模型的蓝图。最后，我们探讨了在预训练的LLMs上应用MASA以减少其参数数量而不显著影响其性能的可能性。 

---
# Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning 

**Title (ZH)**: 揭示临床抑郁评估的景观：从行为特征到精神推理 

**Authors**: Zhuang Chen, Guanqun Bi, Wen Zhang, Jiawei Hu, Aoyun Wang, Xiyao Xiao, Kun Feng, Minlie Huang  

**Link**: [PDF](https://arxiv.org/pdf/2508.04531)  

**Abstract**: Depression is a widespread mental disorder that affects millions worldwide. While automated depression assessment shows promise, most studies rely on limited or non-clinically validated data, and often prioritize complex model design over real-world effectiveness. In this paper, we aim to unveil the landscape of clinical depression assessment. We introduce C-MIND, a clinical neuropsychiatric multimodal diagnosis dataset collected over two years from real hospital visits. Each participant completes three structured psychiatric tasks and receives a final diagnosis from expert clinicians, with informative audio, video, transcript, and functional near-infrared spectroscopy (fNIRS) signals recorded. Using C-MIND, we first analyze behavioral signatures relevant to diagnosis. We train a range of classical models to quantify how different tasks and modalities contribute to diagnostic performance, and dissect the effectiveness of their combinations. We then explore whether LLMs can perform psychiatric reasoning like clinicians and identify their clear limitations in realistic clinical settings. In response, we propose to guide the reasoning process with clinical expertise and consistently improves LLM diagnostic performance by up to 10% in Macro-F1 score. We aim to build an infrastructure for clinical depression assessment from both data and algorithmic perspectives, enabling C-MIND to facilitate grounded and reliable research for mental healthcare. 

**Abstract (ZH)**: 抑郁症是一种广泛的心理疾病，影响着全世界数百万人。虽然自动化抑郁评估显示出潜力，但大多数研究依赖于有限或未临床验证的数据，并且往往优先考虑复杂的模型设计而非实际效果。在本文中，我们旨在揭示临床抑郁症评估的全景。我们介绍了C-MIND，这是一个在两年真实医院访问中收集的临床神经精神多模态诊断数据集。每位参与者完成三项结构化的精神任务，最终由专家临床医生进行诊断，并记录了富有信息量的音频、视频、转录文稿以及功能性近红外光谱成像（fNIRS）信号。利用C-MIND，我们首先分析与诊断相关的行为特征。我们训练一系列经典模型以量化不同任务和模态对诊断性能的贡献，并剖析它们组合的有效性。然后我们探讨是否可以像临床医生一样，通过大型语言模型进行精神科推理，并识别其在实际临床环境中的明显局限性。作为回应，我们建议通过临床专业知识引导推理过程，并一致地提高大型语言模型的诊断性能，最高可达10%的宏F1分数。我们的目标是从数据和算法的角度构建临床抑郁症评估的基础架构，使C-MIND能够促进心理健康领域的扎根可靠研究。 

---
# Automatic LLM Red Teaming 

**Title (ZH)**: 自动LLM红队演练 

**Authors**: Roman Belaire, Arunesh Sinha, Pradeep Varakantham  

**Link**: [PDF](https://arxiv.org/pdf/2508.04451)  

**Abstract**: Red teaming is critical for identifying vulnerabilities and building trust in current LLMs. However, current automated methods for Large Language Models (LLMs) rely on brittle prompt templates or single-turn attacks, failing to capture the complex, interactive nature of real-world adversarial dialogues. We propose a novel paradigm: training an AI to strategically `break' another AI. By formalizing red teaming as a Markov Decision Process (MDP) and employing a hierarchical Reinforcement Learning (RL) framework, we effectively address the inherent sparse reward and long-horizon challenges. Our generative agent learns coherent, multi-turn attack strategies through a fine-grained, token-level harm reward, enabling it to uncover subtle vulnerabilities missed by existing baselines. This approach sets a new state-of-the-art, fundamentally reframing LLM red teaming as a dynamic, trajectory-based process (rather than a one-step test) essential for robust AI deployment. 

**Abstract (ZH)**: 红队演练对于识别当前大语言模型中的漏洞并建立信任至关重要。然而，当前的大语言模型自动化方法依赖于易碎的提示模板或单轮攻击，未能捕捉到现实世界 adversarial 对话的复杂互动性。我们提出了一种新型范式：训练一个 AI 战略性地“攻破”另一个 AI。通过将红队演练形式化为马尔可夫决策过程 (MDP) 并采用分层强化学习 (RL) 框架，我们有效地解决了固有的稀疏奖励和长期目标挑战。我们的生成型代理通过细微的、粒度级的 token 奖励学习连贯的、多轮攻击策略，使其能够发现现有基线方法遗漏的细微漏洞。这种方法建立了新的技术水平，从根本上重新定义了大语言模型的红队演练为一个动态的、轨迹导向的过程（而非单步测试），对于稳健的 AI 部署至关重要。 

---
# StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion 

**Title (ZH)**: StepFun-Formalizer: 通过知识推理融合解锁大语言模型的自形式化潜力 

**Authors**: Yutong Wu, Di Huang, Ruosi Wan, Yue Peng, Shijie Shang, Chenrui Cao, Lei Qi, Rui Zhang, Zidong Du, Jie Yan, Xing Hu  

**Link**: [PDF](https://arxiv.org/pdf/2508.04440)  

**Abstract**: Autoformalization aims to translate natural-language mathematical statements into a formal language. While LLMs have accelerated progress in this area, existing methods still suffer from low accuracy. We identify two key abilities for effective autoformalization: comprehensive mastery of formal-language domain knowledge, and reasoning capability of natural language problem understanding and informal-formal alignment. Without the former, a model cannot identify the correct formal objects; without the latter, it struggles to interpret real-world contexts and map them precisely into formal expressions. To address these gaps, we introduce ThinkingF, a data synthesis and training pipeline that improves both abilities. First, we construct two datasets: one by distilling and selecting large-scale examples rich in formal knowledge, and another by generating informal-to-formal reasoning trajectories guided by expert-designed templates. We then apply SFT and RLVR with these datasets to further fuse and refine the two abilities. The resulting 7B and 32B models exhibit both comprehensive formal knowledge and strong informal-to-formal reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5% on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior general-purpose and specialized models. 

**Abstract (ZH)**: 自动形式化旨在将自然语言数学陈述转换为正式语言。虽然大语言模型加速了这一领域的发展，但现有方法仍面临准确性低的问题。我们识别出两种关键能力，以实现有效的自动形式化：对正式语言领域知识的全面掌握，以及自然语言问题理解与非正式至正式对齐的推理能力。缺乏前者，模型无法识别正确的正式对象；缺乏后者，模型难以解释现实世界背景并精确映射为正式表达式。为了弥补这些差距，我们引入了ThinkingF数据合成与训练管道，以提高这两种能力。首先，我们构建了两个数据集：一个是通过提取和选择富含正式知识的大规模示例构建，另一个是根据专家设计的模板生成非正式至正式的推理轨迹。然后，我们使用这些数据集应用SFT和RLVR，进一步融合和精炼这两种能力。最终生成的7B和32B模型既具备全面的正式知识，又具备强大的非正式至正式推理能力。值得注意的是，StepFun-Formalizer-32B在FormalMATH-Lite和ProverBench上的BEq@1得分分别为40.5%和26.7%，超越了所有先前的一般和专用模型。 

---
# Why are LLMs' abilities emergent? 

**Title (ZH)**: 为什么大规模语言模型的能力是涌现出来的？ 

**Authors**: Vladimír Havlík  

**Link**: [PDF](https://arxiv.org/pdf/2508.04401)  

**Abstract**: The remarkable success of Large Language Models (LLMs) in generative tasks has raised fundamental questions about the nature of their acquired capabilities, which often appear to emerge unexpectedly without explicit training. This paper examines the emergent properties of Deep Neural Networks (DNNs) through both theoretical analysis and empirical observation, addressing the epistemological challenge of "creation without understanding" that characterises contemporary AI development. We explore how the neural approach's reliance on nonlinear, stochastic processes fundamentally differs from symbolic computational paradigms, creating systems whose macro-level behaviours cannot be analytically derived from micro-level neuron activities. Through analysis of scaling laws, grokking phenomena, and phase transitions in model capabilities, I demonstrate that emergent abilities arise from the complex dynamics of highly sensitive nonlinear systems rather than simply from parameter scaling alone. My investigation reveals that current debates over metrics, pre-training loss thresholds, and in-context learning miss the fundamental ontological nature of emergence in DNNs. I argue that these systems exhibit genuine emergent properties analogous to those found in other complex natural phenomena, where systemic capabilities emerge from cooperative interactions among simple components without being reducible to their individual behaviours. The paper concludes that understanding LLM capabilities requires recognising DNNs as a new domain of complex dynamical systems governed by universal principles of emergence, similar to those operating in physics, chemistry, and biology. This perspective shifts the focus from purely phenomenological definitions of emergence to understanding the internal dynamic transformations that enable these systems to acquire capabilities that transcend their individual components. 

**Abstract (ZH)**: 大型语言模型（LLMs）在生成任务中的显著成功引起了对其获得能力本质的基本问题，这些能力往往在没有明确训练的情况下意外涌现。本文通过理论分析和实证观察，探讨了深度神经网络（DNNs）的涌现属性，解决当代人工智能发展中“创造而无理解”的知识论挑战。我们探讨了神经方法依赖非线性、随机过程与符号计算范式的根本差异，从而创建出宏观行为无法从微观神经元活动推导出的系统。通过分析规模律、理解现象和模型能力的相变，我证明了涌现能力源自高度敏感的非线性系统中的复杂动力学，而不仅仅是参数缩放的结果。我的研究揭示，当前关于度量、预训练损失阈值和上下文学习的辩论未能触及DNNs中涌现的根本本体论性质。我论证这些系统展示出类似于其他复杂自然现象中发现的真正涌现属性，在这些系统中，系统的功能由简单组件之间的协同作用产生，而无法将其归约到个体行为。本文结论认为，理解LLMs的能力需要承认DNNs作为由普遍的涌现原则支配的复杂动力学系统的新型领域。 

---
# Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky 

**Title (ZH)**: 使用大型语言模型提高事故数据质量：来自肯塔基州二级事故叙述的证据 

**Authors**: Xu Zhang, Mei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2508.04399)  

**Abstract**: This study evaluates advanced natural language processing (NLP) techniques to enhance crash data quality by mining crash narratives, using secondary crash identification in Kentucky as a case study. Drawing from 16,656 manually reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we compare three model classes: zero-shot open-source large language models (LLMs) (LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers (BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic regression as baseline. Models were calibrated on 2015-2021 data and tested on 1,771 narratives from 2022. Fine-tuned transformers achieved superior performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy (95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139 minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred high computational costs (up to 723 minutes for DeepSeek-R1:70B), while fine-tuned models processed the test set in seconds after brief training. Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can rival larger counterparts in performance while reducing runtime, suggesting opportunities for optimized deployments. Results highlight trade-offs between accuracy, efficiency, and data requirements, with fine-tuned transformer models balancing precision and recall effectively on Kentucky data. Practical deployment considerations emphasize privacy-preserving local deployment, ensemble approaches for improved accuracy, and incremental processing for scalability, providing a replicable scheme for enhancing crash-data quality with advanced NLP. 

**Abstract (ZH)**: 本研究通过利用自然语言处理（NLP）技术挖掘碰撞叙述，评估高级NLP技术以提高碰撞数据质量，并以肯塔基州二次碰撞识别为例进行探讨。本研究基于2015-2022年期间的手动审查的16,656篇叙述，其中有3,803起确认的二次碰撞，对比了三种模型类：零样本开源大规模语言模型（LLMs）（LLaMA3:70B，DeepSeek-R1:70B，Qwen3:32B，Gemma3:27B）；微调变换器（BERT，DistilBERT，RoBERTa，XLNet，Longformer）；以及传统的逻辑回归作为baseline。模型在2015-2021年的数据上进行校准，并在2022年的1,771篇叙述上进行测试。微调变换器表现出更优异的性能，RoBERTa的F1分数最高（0.90），准确率最高（95%）。零样本LLaMA3:70B达到相当的F1分数（0.86），但推理时间需要139分钟；逻辑回归baseline的性能明显滞后（F1:0.66）。大规模语言模型在某些变体的召回率上表现出色（例如，GEMMA3:27B达到0.94），但计算成本高昂（例如，DeepSeek-R1:70B达到723分钟），而微调模型在短暂训练后能在几秒钟内处理测试集。进一步分析表明，中型规模的语言模型（例如，DeepSeek-R1:32B）可以达到与更大规模模型相当的性能同时减少运行时间，表明优化部署的机会。研究结果强调了准确率、效率和数据要求之间的权衡，微调变换器模型在肯塔基州数据上有效平衡了精确率和召回率。实际部署的考虑强调了隐私保护的地方部署、用于提高准确率的集成方法以及用于可扩展性的增量处理，提供了通过高级NLP增强碰撞数据质量的一个可复制方案。 

---
# LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for Learned Thematic Significance Tracking in Multimedia Content 

**Title (ZH)**: LUST：一种基于层次化LLLM的多模态框架，用于多媒体内容中学到的理解显著性跟踪 

**Authors**: Anderson de Lima Luiz  

**Link**: [PDF](https://arxiv.org/pdf/2508.04353)  

**Abstract**: This paper introduces the Learned User Significance Tracker (LUST), a framework designed to analyze video content and quantify the thematic relevance of its segments in relation to a user-provided textual description of significance. LUST leverages a multi-modal analytical pipeline, integrating visual cues from video frames with textual information extracted via Automatic Speech Recognition (ASR) from the audio track. The core innovation lies in a hierarchical, two-stage relevance scoring mechanism employing Large Language Models (LLMs). An initial "direct relevance" score, $S_{d,i}$, assesses individual segments based on immediate visual and auditory content against the theme. This is followed by a "contextual relevance" score, $S_{c,i}$, that refines the assessment by incorporating the temporal progression of preceding thematic scores, allowing the model to understand evolving narratives. The LUST framework aims to provide a nuanced, temporally-aware measure of user-defined significance, outputting an annotated video with visualized relevance scores and comprehensive analytical logs. 

**Abstract (ZH)**: Learned User Significance Tracker (LUST): 一种用于分析视频内容并量化其段落与用户提供的主题相关性文本描述之间相关性的框架。 

---
# Chain of Questions: Guiding Multimodal Curiosity in Language Models 

**Title (ZH)**: 链式问题：引导多模态好奇心的语言模型 

**Authors**: Nima Iji, Kia Dashtipour  

**Link**: [PDF](https://arxiv.org/pdf/2508.04350)  

**Abstract**: Reasoning capabilities in large language models (LLMs) have substantially advanced through methods such as chain-of-thought and explicit step-by-step explanations. However, these improvements have not yet fully transitioned to multimodal contexts, where models must proactively decide which sensory modalities such as vision, audio, or spatial perception to engage when interacting with complex real-world environments. In this paper, we introduce the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach that encourages multimodal language models to dynamically generate targeted questions regarding their surroundings. These generated questions guide the model to selectively activate relevant modalities, thereby gathering critical information necessary for accurate reasoning and response generation. We evaluate our framework on a novel multimodal benchmark dataset, assembled by integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results demonstrate that our CoQ method improves a foundation model's ability to effectively identify and integrate pertinent sensory information. This leads to improved accuracy, interpretability, and alignment of the reasoning process with diverse multimodal tasks. 

**Abstract (ZH)**: 大型语言模型（LLMs）的推理能力通过链式思考和显式的步骤解释等方法得到了显著提升，但在多模态上下文中，模型尚未完全过渡到能够主动决定在与复杂现实环境交互时使用哪些感官模态（如视觉、音频或空间感知）。本文介绍了一种基于好奇心的推理框架——问题链（CoQ）框架，该框架鼓励多模态语言模型动态生成关于其环境的针对性问题，这些生成的问题引导模型选择性地激活相关模态，从而收集进行准确推理和响应生成所需的关键信息。我们在一个由WebGPT、ScienceQA、AVSD和ScanQA数据集整合而成的新建多模态基准数据集上评估了我们的框架。实验结果表明，我们的CoQ方法提高了基础模型有效识别和整合相关感官信息的能力，从而提高了准确度、可解释性和推理过程与多样化多模态任务的一致性。 

---
# GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy 

**Title (ZH)**: GTPO和GRPO-S：基于政策熵的令牌级和序列级奖励塑形 

**Authors**: Hongze Tan, Jianfei Pan  

**Link**: [PDF](https://arxiv.org/pdf/2508.04349)  

**Abstract**: Reinforcement learning (RL) with algorithms like Group Relative Policy Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is limited by a coarse-grained credit assignment that applies a uniform reward to all tokens in a sequence. This is a major flaw in long-chain reasoning tasks. This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea is that high-entropy tokens in correct responses can guide the policy toward a higher performance ceiling. This allows us to create more fine-grained reward signals for precise policy updates via two ways: 1) \textbf{Group Token Policy Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted reward to each sequence based on its average token entropy. Experiments show our methods significantly outperform the strong DAPO baseline. The results confirm that our entropy-weighting mechanism is the key driver of this performance boost, offering a better path to enhance deep reasoning in models. 

**Abstract (ZH)**: 使用动态熵权重分配的强化学习提升大型语言模型推理能力 

---
# Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models 

**Title (ZH)**: 超越排行榜：重新思考大型语言模型的医学基准 

**Authors**: Zizhan Ma, Wenxuan Wang, Guo Yu, Yiu-Fai Cheung, Meidan Ding, Jie Liu, Wenting Chen, Linlin Shen  

**Link**: [PDF](https://arxiv.org/pdf/2508.04325)  

**Abstract**: Large language models (LLMs) show significant potential in healthcare, prompting numerous benchmarks to evaluate their capabilities. However, concerns persist regarding the reliability of these benchmarks, which often lack clinical fidelity, robust data management, and safety-oriented evaluation metrics. To address these shortcomings, we introduce MedCheck, the first lifecycle-oriented assessment framework specifically designed for medical benchmarks. Our framework deconstructs a benchmark's development into five continuous stages, from design to governance, and provides a comprehensive checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis uncovers widespread, systemic issues, including a profound disconnect from clinical practice, a crisis of data integrity due to unmitigated contamination risks, and a systematic neglect of safety-critical evaluation dimensions like model robustness and uncertainty awareness. Based on these findings, MedCheck serves as both a diagnostic tool for existing benchmarks and an actionable guideline to foster a more standardized, reliable, and transparent approach to evaluating AI in healthcare. 

**Abstract (ZH)**: 大型语言模型（LLMs）在医疗领域的应用显示了显著的潜力，促使了众多基准测试来评估其能力。然而，这些基准测试的可靠性依然受到关切，常常缺乏临床真实感、稳健的数据管理以及安全评估指标。为了应对这些不足，我们引入了MedCheck，这是首个面向医疗基准测试的生命周期评估框架，专门为此设计。我们的框架将基准测试的开发分解为五个连续阶段，从设计到治理，并提供了一套涵盖46项医学定制标准的综合检查清单。利用MedCheck，我们对53个医疗LLM基准测试进行了深入的实证评估。我们的分析揭示了广泛的系统性问题，包括与临床实践严重脱节、由于不可缓解的污染风险导致的数据完整性危机，以及对安全性关键评估维度如模型稳健性和不确定性意识的系统性忽视。基于这些发现，MedCheck不仅作为现有基准测试的诊断工具，也是一个可操作的指南，旨在促进更加标准、可靠和透明的AI在医疗领域的评估方法。 

---
# Compressing Large Language Models with PCA Without Performance Loss 

**Title (ZH)**: 使用PCA压缩大型语言模型而不损失性能 

**Authors**: Magnus Bengtsson  

**Link**: [PDF](https://arxiv.org/pdf/2508.04307)  

**Abstract**: We demonstrate that Principal Component Analysis (PCA), when applied in a structured manner, either to polar-transformed images or segment-wise to token sequences, enables extreme compression of neural models without sacrificing performance. Across three case studies, we show that a one-layer classifier trained on PCA-compressed polar MNIST achieves over 98 percent accuracy using only 840 parameters. A two-layer transformer trained on 70-dimensional PCA-reduced MiniLM embeddings reaches 76.62 percent accuracy on the 20 Newsgroups dataset with just 81000 parameters. A decoder-only transformer generates coherent token sequences from 70-dimensional PCA embeddings while preserving over 97 percent cosine similarity with full MiniLM representations, using less than 17 percent of the parameter count of GPT-2. These results highlight PCA-based input compression as a general and effective strategy for aligning model capacity with information content, enabling lightweight architectures across multiple modalities. 

**Abstract (ZH)**: 主成分分析（PCA）在结构化应用到极坐标变换图像或分段应用于token序列时，能够实现神经模型的极端压缩而不牺牲性能。通过三个案例研究我们展示，经过PCA压缩的极坐标MNIST数据集上训练的一层分类器仅使用840个参数即可达到超过98%的准确率；基于70维PCA降维的MiniLM嵌入训练的两层变压器在20 Newsgroups数据集上仅使用81000个参数就达到了76.62%的准确率；仅需70维PCA嵌入的解码器变压器能够生成连贯的token序列，同时保留超过97%的余弦相似性，使用的参数量不到GPT-2的17%。这些结果强调了基于PCA的输入压缩作为一种通用而有效的策略，能够协调模型容量与信息内容，从而在多种模态中实现轻量级架构。 

---
# A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models 

**Title (ZH)**: 几句话就能扭曲图形：基于图形的检索增强生成大型语言模型的知识投毒攻击 

**Authors**: Jiayi Wen, Tianxin Chen, Zhirun Zheng, Cheng Huang  

**Link**: [PDF](https://arxiv.org/pdf/2508.04276)  

**Abstract**: Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as a promising paradigm for enhancing large language models (LLMs) by converting raw text into structured knowledge graphs, improving both accuracy and explainability. However, GraphRAG relies on LLMs to extract knowledge from raw text during graph construction, and this process can be maliciously manipulated to implant misleading information. Targeting this attack surface, we propose two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a few words in the source text can significantly change the constructed graph, poison the GraphRAG, and severely mislead downstream reasoning. The first attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate vulnerable nodes in the generated graphs and rewrites the corresponding narratives with LLMs, achieving precise control over specific question-answering (QA) outcomes with a success rate of 93.1\%, while keeping the poisoned text fluent and natural. The second attack, named Universal KPA (UKPA), exploits linguistic cues such as pronouns and dependency relations to disrupt the structural integrity of the generated graph by altering globally influential words. With fewer than 0.05\% of full text modified, the QA accuracy collapses from 95\% to 50\%. Furthermore, experiments show that state-of-the-art defense methods fail to detect these attacks, highlighting that securing GraphRAG pipelines against knowledge poisoning remains largely unexplored. 

**Abstract (ZH)**: 基于图的检索增强生成（GraphRAG）的知识投毒攻击研究 

---
# TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening 

**Title (ZH)**: TalkDep: 临床导向的对话中心抑郁症筛查语言模型人设 

**Authors**: Xi Wang, Anxo Perez, Javier Parapar, Fabio Crestani  

**Link**: [PDF](https://arxiv.org/pdf/2508.04248)  

**Abstract**: The increasing demand for mental health services has outpaced the availability of real training data to develop clinical professionals, leading to limited support for the diagnosis of depression. This shortage has motivated the development of simulated or virtual patients to assist in training and evaluation, but existing approaches often fail to generate clinically valid, natural, and diverse symptom presentations. In this work, we embrace the recent advanced language models as the backbone and propose a novel clinician-in-the-loop patient simulation pipeline, TalkDep, with access to diversified patient profiles to develop simulated patients. By conditioning the model on psychiatric diagnostic criteria, symptom severity scales, and contextual factors, our goal is to create authentic patient responses that can better support diagnostic model training and evaluation. We verify the reliability of these simulated patients with thorough assessments conducted by clinical professionals. The availability of validated simulated patients offers a scalable and adaptable resource for improving the robustness and generalisability of automatic depression diagnosis systems. 

**Abstract (ZH)**: 不断增加的心理健康服务需求已超出了实际训练数据的供给，限制了抑郁症诊断的支持。这种短缺推动了模拟或虚拟患者的开发以辅助培训和评估，但现有方法往往无法生成具有临床效度、自然性和多样性的症状表现。在此工作中，我们采用了最新的先进语言模型作为基础，并提出了一种新的包含临床专家的模拟患者生成管道TalkDep，访问多样化的患者档案以开发模拟患者。通过基于精神疾病诊断标准、症状严重程度量表和情境因素来调整模型，我们的目标是生成真实的患者反应，以更好地支持诊断模型的培训和评估。通过临床专业人员进行彻底评估验证了这些模拟患者的可靠性。验证过的模拟患者可供自动抑郁症诊断系统的稳健性和普适性改进使用。 

---
# Empowering Time Series Forecasting with LLM-Agents 

**Title (ZH)**: 用LLM代理增强时间序列预测 

**Authors**: Chin-Chia Michael Yeh, Vivian Lai, Uday Singh Saini, Xiran Fan, Yujie Fan, Junpeng Wang, Xin Dai, Yan Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2508.04231)  

**Abstract**: Large Language Model (LLM) powered agents have emerged as effective planners for Automated Machine Learning (AutoML) systems. While most existing AutoML approaches focus on automating feature engineering and model architecture search, recent studies in time series forecasting suggest that lightweight models can often achieve state-of-the-art performance. This observation led us to explore improving data quality, rather than model architecture, as a potentially fruitful direction for AutoML on time series data. We propose DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata accompanying time series to clean data while optimizing forecasting performance. We evaluated DCATS using four time series forecasting models on a large-scale traffic volume forecasting dataset. Results demonstrate that DCATS achieves an average 6% error reduction across all tested models and time horizons, highlighting the potential of data-centric approaches in AutoML for time series forecasting. 

**Abstract (ZH)**: 基于大规模语言模型的数据为中心代理在时间序列自动化机器学习中的应用：DCATS 

---
# ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments 

**Title (ZH)**: ReasoningGuard: 在推理过程中保障大型推理模型的安全警觉机制 

**Authors**: Yuquan Wang, Mi Zhang, Yining Wang, Geng Hong, Xiaoyu You, Min Yang  

**Link**: [PDF](https://arxiv.org/pdf/2508.04204)  

**Abstract**: Large Reasoning Models (LRMs) have demonstrated impressive performance in reasoning-intensive tasks, but they remain vulnerable to harmful content generation, particularly in the mid-to-late steps of their reasoning processes. Existing defense mechanisms, however, rely on costly fine-tuning and additional expert knowledge, which restricts their scalability. In this work, we propose ReasoningGuard, an inference-time safeguard for LRMs, which injects timely safety aha moments to steer harmless while helpful reasoning processes. Leveraging the model's internal attention behavior, our approach accurately identifies critical points in the reasoning path, and triggers spontaneous, safety-oriented reflection. To safeguard both the subsequent reasoning steps and the final answers, we further implement a scaling sampling strategy during the decoding phase, selecting the optimal reasoning path. Inducing minimal extra inference cost, ReasoningGuard effectively mitigates three types of jailbreak attacks, including the latest ones targeting the reasoning process of LRMs. Our approach outperforms seven existing safeguards, achieving state-of-the-art safety defenses while effectively avoiding the common exaggerated safety issues. 

**Abstract (ZH)**: Large Reasoning Models的推理安全防范：ReasoningGuard 

---
# Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models 

**Title (ZH)**: 引发并分析前沿大型语言模型中的 emergent 不对齐问题 

**Authors**: Siddhant Panpatil, Hiskias Dingeto, Haon Park  

**Link**: [PDF](https://arxiv.org/pdf/2508.04196)  

**Abstract**: Despite significant advances in alignment techniques, we demonstrate that state-of-the-art language models remain vulnerable to carefully crafted conversational scenarios that can induce various forms of misalignment without explicit jailbreaking. Through systematic manual red-teaming with Claude-4-Opus, we discovered 10 successful attack scenarios, revealing fundamental vulnerabilities in how current alignment methods handle narrative immersion, emotional pressure, and strategic framing. These scenarios successfully elicited a range of misaligned behaviors, including deception, value drift, self-preservation, and manipulative reasoning, each exploiting different psychological and contextual vulnerabilities. To validate generalizability, we distilled our successful manual attacks into MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible testing across multiple models. Cross-model evaluation of our 10 scenarios against five frontier LLMs revealed an overall 76% vulnerability rate, with significant variations: GPT-4.1 showed the highest susceptibility (90%), while Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate that sophisticated reasoning capabilities often become attack vectors rather than protective mechanisms, as models can be manipulated into complex justifications for misaligned behavior. This work provides (i) a detailed taxonomy of conversational manipulation patterns and (ii) a reusable evaluation framework. Together, these findings expose critical gaps in current alignment strategies and highlight the need for robustness against subtle, scenario-based manipulation in future AI systems. 

**Abstract (ZH)**: 尽管在对齐技术方面取得了显著进展，我们仍发现最先进的语言模型在精心设计的对话场景下仍存在各种形式的对齐漏洞，而无需进行显式的破解。通过系统的手动红队测试，我们使用Claude-4-Opus发现了10个成功的攻击场景，揭示了当前对齐方法在处理叙述沉浸、情感压力和战略框架方面存在的一些根本性漏洞。这些场景成功引发了一系列对齐行为偏差，包括欺骗、价值观偏移、自我保护和操纵性推理，每种偏差都利用了不同的心理和情境漏洞。为了验证其普适性，我们将成功的手动攻击提炼为MISALIGNMENTBENCH评估框架，该框架可在多种模型之间实现可重复测试。对五种前沿LLM进行跨模型评估显示，整体易受攻击率为76%，存在显著差异：GPT-4.1显示出最高的易感性（90%），而Claude-4-Sonnet则表现出更强的抵抗力（40%）。我们的研究结果表明，复杂的推理能力往往成为攻击向量而非保护机制，模型可以被操控以进行复杂的、不符合对齐的行为辩护。这项工作提供了（i）对话操控模式的详细分类目录和（ii）可重用的评估框架。这些发现揭示了当前对齐策略中的关键缺口，并强调了未来AI系统在应对基于场景的微妙操控方面的鲁棒性需求。 

---
# Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity 

**Title (ZH)**: 用因果充分性和必要性破解MLLMs幻觉 

**Authors**: Peizheng Guo, Jingyao Wang, Wenwen Qiang, Huijie Guo, Changwen Zheng, Jiahuan Zhou, Gang Hua  

**Link**: [PDF](https://arxiv.org/pdf/2508.04182)  

**Abstract**: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across vision-language tasks. However, they may suffer from hallucinations--generating outputs that are semantically inconsistent with the input image or text. Through causal analyses, we find that: (i) hallucinations with omission may arise from the failure to adequately capture essential causal factors, and (ii) hallucinations with fabrication are likely caused by the model being misled by non-causal cues. To address these challenges, we propose a novel reinforcement learning framework guided by causal completeness, which jointly considers both causal sufficiency and causal necessity of tokens. Specifically, we evaluate each token's standalone contribution and counterfactual indispensability to define a token-level causal completeness reward. This reward is used to construct a causally informed advantage function within the GRPO optimization framework, encouraging the model to focus on tokens that are both causally sufficient and necessary for accurate generation. Experimental results across various benchmark datasets and tasks demonstrate the effectiveness of our approach, which effectively mitigates hallucinations in MLLMs. 

**Abstract (ZH)**: 多模态大语言模型（MLLMs）在视觉语言任务中展现了令人印象深刻的 capability。然而，它们可能会遭受幻觉问题——生成与输入图像或文本语义不一致的输出。通过因果分析，我们发现：（i）由于未能充分捕捉到关键的因果因素，可能会产生遗漏导致的幻觉；（ii）而误导性的非因果线索可能导致构建幻觉。为解决这些问题，我们提出了一种由因果完备性指导的新型强化学习框架，该框架同时考虑了标记的因果充分性和必要性。具体来说，我们评估每个标记的独立贡献及其反事实不可替代性，以定义标记级别的因果完备性奖励。这种奖励被用于在GRPO优化框架内构建因果导向的优势函数，促进模型专注于对于准确生成而言既是因果充分又是必要的标记。在多种基准数据集和任务上的实验结果证明了我们方法的有效性，有效地减轻了MLLMs中的幻觉问题。 

---
# Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap 

**Title (ZH)**: 基于难度的偏好数据选择：DPO隐式奖励差距 

**Authors**: Xuan Qi, Rongwu Xu, Zhijing Jin  

**Link**: [PDF](https://arxiv.org/pdf/2508.04149)  

**Abstract**: Aligning large language models (LLMs) with human preferences is a critical challenge in AI research. While methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they often rely on large, costly preference datasets. The current work lacks methods for high-quality data selection specifically for preference data. In this work, we introduce a novel difficulty-based data selection strategy for preference datasets, grounded in the DPO implicit reward mechanism. By selecting preference data examples with smaller DPO implicit reward gaps, which are indicative of more challenging cases, we improve data efficiency and model alignment. Our approach consistently outperforms five strong baselines across multiple datasets and alignment tasks, achieving superior performance with only 10\% of the original data. This principled, efficient selection method offers a promising solution for scaling LLM alignment with limited resources. 

**Abstract (ZH)**: 基于难度的数据选择策略：一种用于偏好数据集的新型方法 

---
# COPO: Consistency-Aware Policy Optimization 

**Title (ZH)**: COPO：一致性意识策略优化 

**Authors**: Jinghang Han, Jiawei Chen, Hang Shao, Hao Ma, Mingcheng Li, Xintian Shen, Lihao Zheng, Wei Chen, Tao Wei, Lihua Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2508.04138)  

**Abstract**: Reinforcement learning has significantly enhanced the reasoning capabilities of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the introduction of DeepSeek R1 has inspired a surge of interest in leveraging rule-based rewards as a low-cost alternative for computing advantage functions and guiding policy optimization. However, a common challenge observed across many replication and extension efforts is that when multiple sampled responses under a single prompt converge to identical outcomes, whether correct or incorrect, the group-based advantage degenerates to zero. This leads to vanishing gradients and renders the corresponding samples ineffective for learning, ultimately limiting training efficiency and downstream performance. To address this issue, we propose a consistency-aware policy optimization framework that introduces a structured global reward based on outcome consistency, the global loss based on it ensures that, even when model outputs show high intra-group consistency, the training process still receives meaningful learning signals, which encourages the generation of correct and self-consistent reasoning paths from a global perspective. Furthermore, we incorporate an entropy-based soft blending mechanism that adaptively balances local advantage estimation with global optimization, enabling dynamic transitions between exploration and convergence throughout training. Our method introduces several key innovations in both reward design and optimization strategy. We validate its effectiveness through substantial performance gains on multiple mathematical reasoning benchmarks, highlighting the proposed framework's robustness and general applicability. Code of this work has been released at this https URL. 

**Abstract (ZH)**: 强化学习显著增强了大型语言模型（LLMs）在复杂问题解决任务中的推理能力。最近，DeepSeek R1 的引入激发了利用基于规则的奖励作为低成本替代方案的兴趣，以计算优势函数并指导策略优化。然而，许多复制和扩展努力中普遍观察到的一个挑战是，当单个提示下的多个采样响应收敛到相同的结果（无论是正确还是错误）时，基于群体的优势会退化为零，这导致梯度消失，使得相应样本无法用于学习，最终限制了训练效率和下游性能。为了解决这个问题，我们提出了一种关注一致性的策略优化框架，引入了一种基于结果一致性的结构化全局奖励，基于它的全局损失确保即使模型输出在群体内部显示出高一致性，训练过程仍然能接收到有意义的学习信号，以从全局视角促进正确的和自我一致的推理路径生成。此外，我们还集成了一种基于熵的软融合机制，该机制自适应地平衡局部优势估计与全局优化，使探索与收敛在整个训练过程中动态转换。我们的方法在奖励设计和优化策略方面带来了多项关键创新。我们在多个数学推理基准上的性能显著提升验证了其有效性，突显了所提出的框架的稳健性和通用性。该工作的代码已发布在<https://>。 

---
# Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks 

**Title (ZH)**: 基于ChatGPT的生产性交互策略实验分析：功能和项目级别代码生成任务的用户研究 

**Authors**: Sangwon Hyun, Hyunjun Kim, Jinhyuk Jang, Hyojin Choi, M. Ali Babar  

**Link**: [PDF](https://arxiv.org/pdf/2508.04125)  

**Abstract**: The application of Large Language Models (LLMs) is growing in the productive completion of Software Engineering tasks. Yet, studies investigating the productive prompting techniques often employed a limited problem space, primarily focusing on well-known prompting patterns and mainly targeting function-level SE practices. We identify significant gaps in real-world workflows that involve complexities beyond class-level (e.g., multi-class dependencies) and different features that can impact Human-LLM Interactions (HLIs) processes in code generation. To address these issues, we designed an experiment that comprehensively analyzed the HLI features regarding the code generation productivity. Our study presents two project-level benchmark tasks, extending beyond function-level evaluations. We conducted a user study with 36 participants from diverse backgrounds, asking them to solve the assigned tasks by interacting with the GPT assistant using specific prompting patterns. We also examined the participants' experience and their behavioral features during interactions by analyzing screen recordings and GPT chat logs. Our statistical and empirical investigation revealed (1) that three out of 15 HLI features significantly impacted the productivity in code generation; (2) five primary guidelines for enhancing productivity for HLI processes; and (3) a taxonomy of 29 runtime and logic errors that can occur during HLI processes, along with suggested mitigation plans. 

**Abstract (ZH)**: 大型语言模型（LLMs）在软件工程任务的高效完成中应用日益增长 kuk 

---
# Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decode 

**Title (ZH)**: 利用轻量级掩码解码解锁MLLM在引用表达分割领域的潜力 

**Authors**: Jingchao Wang, Zhijian Wu, Dingjiang Huang, Yefeng Zheng, Hong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2508.04107)  

**Abstract**: Reference Expression Segmentation (RES) aims to segment image regions specified by referring expressions and has become popular with the rise of multimodal large models (MLLMs). While MLLMs excel in semantic understanding, their token-generation paradigm struggles with pixel-level dense prediction. Existing RES methods either couple MLLMs with the parameter-heavy Segment Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight pipelines that sacrifice accuracy. To address the trade-off between performance and cost, we specifically propose MLLMSeg, a novel framework that fully exploits the inherent visual detail features encoded in the MLLM vision encoder without introducing an extra visual encoder. Besides, we propose a detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully integrates the detail-related visual feature with the semantic-related feature output by the large language model (LLM) of MLLM. Finally, we establish a light-weight mask decoder with only 34M network parameters that optimally leverages detailed spatial features from the visual encoder and semantic features from the LLM to achieve precise mask prediction. Extensive experiments demonstrate that our method generally surpasses both SAM-based and SAM-free competitors, striking a better balance between performance and cost. Code is available at this https URL. 

**Abstract (ZH)**: 参考表达分割（RES）旨在根据参考表达分割图像区域，并随着多模态大型模型（MLLMs）的兴起而变得流行。尽管MLLMs在语义理解方面表现出色，但它们的令牌生成范式在像素级密集预测方面遇到困难。现有RES方法要么将MLLM与参数量达632M的Segment Anything Model (SAM)相结合，要么采用不依赖SAM的轻量级管道，但牺牲了准确性。为解决性能和成本之间的权衡，我们特别提出了MLLMSeg这一新型框架，该框架充分利用MLLM视觉编码器中固有的视觉细节特征，而不引入额外的视觉编码器。此外，我们提出了一个增强细节且语义一致的特征融合模块（DSFF），该模块完全将与细节相关的视觉特征与MLM大型语言模型输出的语义相关特征融合在一起。最后，我们建立了一个仅包含34M网络参数的轻量级掩码解码器，该解码器能够充分发挥视觉编码器的详细空间特征和大型语言模型的语义特征的优势，以实现精准的掩码预测。广泛实验表明，我们的方法在性能和成本之间取得了更好的平衡，且总体上优于SAM基和SAM非基竞争对手。代码可供查阅：https://... 

---
# Large Reasoning Models Are Autonomous Jailbreak Agents 

**Title (ZH)**: 大型推理模型是自主脱管代理 

**Authors**: Thilo Hagendorff, Erik Derner, Nuria Oliver  

**Link**: [PDF](https://arxiv.org/pdf/2508.04039)  

**Abstract**: Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has traditionally required complex technical procedures or specialized human expertise. In this study, we show that the persuasive capabilities of large reasoning models (LRMs) simplify and scale jailbreaking, converting it into an inexpensive activity accessible to non-experts. We evaluated the capabilities of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as autonomous adversaries conducting multi-turn conversations with nine widely used target models. LRMs received instructions via a system prompt, before proceeding to planning and executing jailbreaks with no further supervision. We performed extensive experiments with a benchmark of harmful prompts composed of 70 items covering seven sensitive domains. This setup yielded an overall attack success rate across all model combinations of 97.14%. Our study reveals an alignment regression, in which LRMs can systematically erode the safety guardrails of other models, highlighting the urgent need to further align frontier models not only to resist jailbreak attempts, but also to prevent them from being co-opted into acting as jailbreak agents. 

**Abstract (ZH)**: 破解——绕过AI模型中的内置安全机制——传统上需要复杂的技术流程或专门的人类专业知识。在本研究中，我们展示了大规模推理模型（LRMs）的说服能力简化并扩大了破解规模，使其变成一项对非专家来说低成本的活动。我们评估了四种LRMs（DeepSeek-R1、Gemini 2.5 Flash、Grok 3 Mini、Qwen3 235B）作为自主对手，在与九种广泛使用的靶模型进行多轮对话时的能力。LRMs在系统提示下接收指令，然后规划并执行破解，无需进一步监督。我们使用包含70项项目、涵盖七大敏感领域的有害提示基准进行了广泛实验。这种设置在所有模型组合中总体攻击成功率达到了97.14%。我们的研究揭示了一种对齐回归，其中LRMs可以系统地侵蚀其他模型的安全防护栏，突显出迫切需要进一步对前沿模型不仅进行对破解尝试的抵抗对齐，还要防止它们被利用成为破解代理。 

---
# Enhancing Serendipity Recommendation System by Constructing Dynamic User Knowledge Graphs with Large Language Models 

**Title (ZH)**: 通过构建动态用户知识图谱提升偶然性推荐系统效能 

**Authors**: Qian Yong, Yanhui Li, Jialiang Shi, Yaguang Dou, Tian Qi  

**Link**: [PDF](https://arxiv.org/pdf/2508.04032)  

**Abstract**: The feedback loop in industrial recommendation systems reinforces homogeneous content, creates filter bubble effects, and diminishes user satisfaction. Recently, large language models(LLMs) have demonstrated potential in serendipity recommendation, thanks to their extensive world knowledge and superior reasoning capabilities. However, these models still face challenges in ensuring the rationality of the reasoning process, the usefulness of the reasoning results, and meeting the latency requirements of industrial recommendation systems (RSs). To address these challenges, we propose a method that leverages llm to dynamically construct user knowledge graphs, thereby enhancing the serendipity of recommendation systems. This method comprises a two stage framework:(1) two-hop interest reasoning, where user static profiles and historical behaviors are utilized to dynamically construct user knowledge graphs via llm. Two-hop reasoning, which can enhance the quality and accuracy of LLM reasoning results, is then performed on the constructed graphs to identify users' potential interests; and(2) Near-line adaptation, a cost-effective approach to deploying the aforementioned models in industrial recommendation systems. We propose a u2i (user-to-item) retrieval model that also incorporates i2i (item-to-item) retrieval capabilities, the retrieved items not only exhibit strong relevance to users' newly emerged interests but also retain the high conversion rate of traditional u2i retrieval. Our online experiments on the Dewu app, which has tens of millions of users, indicate that the method increased the exposure novelty rate by 4.62%, the click novelty rate by 4.85%, the average view duration per person by 0.15%, unique visitor click through rate by 0.07%, and unique visitor interaction penetration by 0.30%, enhancing user experience. 

**Abstract (ZH)**: 工业推荐系统中的反馈回路强化了同质化内容，产生了过滤泡泡效应，降低了用户满意度。大语言模型在偶然性推荐方面展现出潜力，得益于其广泛的世界知识和卓越的推理能力。然而，这些模型仍面临确保推理过程的合理性、推理结果的有效性以及满足工业推荐系统（RSs）的延迟要求的挑战。为应对这些挑战，我们提出了一种方法，利用大语言模型动态构建用户知识图谱，从而增强推荐系统的偶然性。该方法包含两个阶段框架：(1) 两跳兴趣推理，通过大语言模型利用用户静态资料和历史行为动态构建用户知识图谱，然后在构建的图谱上进行两跳推理以识别用户的潜在兴趣；(2) 在线适应，一种经济高效的工业推荐系统部署方法。我们提出了一种用户到物品（u2i）检索模型，该模型还具备物品到物品（i2i）检索能力，检索出的物品不仅与用户新兴的兴趣高度相关，同时也保持了传统u2i检索的高转化率。在拥有数百万用户的Dewu应用进行的在线实验表明，该方法将内容新颖性曝光率提高了4.62%，点击新颖性曝光率提高了4.85%，人均平均浏览时长提高了0.15%，唯一访客点击通过率提高了0.07%，唯一访客交互渗透率提高了0.30%，从而提升了用户体验。 

---
# Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing 

**Title (ZH)**: 步长更多：超越基于元学习模型编辑中的单次反向传播 

**Authors**: Xiaopeng Li, Shasha Li, Xi Wang, Shezheng Song, Bin Ji, Shangwen Wang, Jun Ma, Xiaodong Liu, Mina Liu, Jie Yu  

**Link**: [PDF](https://arxiv.org/pdf/2508.04012)  

**Abstract**: Large Language Models (LLMs) underpin many AI applications, but their static nature makes updating knowledge costly. Model editing offers an efficient alternative by injecting new information through targeted parameter modifications. In particular, meta-learning-based model editing (MLBME) methods have demonstrated notable advantages in both editing effectiveness and efficiency. Despite this, we find that MLBME exhibits suboptimal performance in low-data scenarios, and its training efficiency is bottlenecked by the computation of KL divergence. To address these, we propose $\textbf{S}$tep $\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation $\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited supervision and a norm regularization on weight updates to improve training efficiency. Experimental results on two datasets and two LLMs demonstrate that SMEdit outperforms prior MLBME baselines and the MBPS strategy can be seamlessly integrated into existing methods to further boost their performance. Our code will be released soon. 

**Abstract (ZH)**: 基于Step More Edit的元学习模型编辑方法 

---
# StepWrite: Adaptive Planning for Speech-Driven Text Generation 

**Title (ZH)**: StepWrite: 适应性规划用于语音驱动的文字生成 

**Authors**: Hamza El Alaoui, Atieh Taheri, Yi-Hao Peng, Jeffrey P. Bigham  

**Link**: [PDF](https://arxiv.org/pdf/2508.04011)  

**Abstract**: People frequently use speech-to-text systems to compose short texts with voice. However, current voice-based interfaces struggle to support composing more detailed, contextually complex texts, especially in scenarios where users are on the move and cannot visually track progress. Longer-form communication, such as composing structured emails or thoughtful responses, requires persistent context tracking, structured guidance, and adaptability to evolving user intentions--capabilities that conventional dictation tools and voice assistants do not support. We introduce StepWrite, a large language model-driven voice-based interaction system that augments human writing ability by enabling structured, hands-free and eyes-free composition of longer-form texts while on the move. StepWrite decomposes the writing process into manageable subtasks and sequentially guides users with contextually-aware non-visual audio prompts. StepWrite reduces cognitive load by offloading the context-tracking and adaptive planning tasks to the models. Unlike baseline methods like standard dictation features (e.g., Microsoft Word) and conversational voice assistants (e.g., ChatGPT Advanced Voice Mode), StepWrite dynamically adapts its prompts based on the evolving context and user intent, and provides coherent guidance without compromising user autonomy. An empirical evaluation with 25 participants engaging in mobile or stationary hands-occupied activities demonstrated that StepWrite significantly reduces cognitive load, improves usability and user satisfaction compared to baseline methods. Technical evaluations further confirmed StepWrite's capability in dynamic contextual prompt generation, accurate tone alignment, and effective fact checking. This work highlights the potential of structured, context-aware voice interactions in enhancing hands-free and eye-free communication in everyday multitasking scenarios. 

**Abstract (ZH)**: 基于大型语言模型的StepWrite：增强移动场景下长文本编写能力的语音交互系统 

---
# Are Today's LLMs Ready to Explain Well-Being Concepts? 

**Title (ZH)**: 当今的大型语言模型准备好解释福祉概念了吗？ 

**Authors**: Bohan Jiang, Dawei Li, Zhen Tan, Chengshuai Zhao, Huan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2508.03990)  

**Abstract**: Well-being encompasses mental, physical, and social dimensions essential to personal growth and informed life decisions. As individuals increasingly consult Large Language Models (LLMs) to understand well-being, a key challenge emerges: Can LLMs generate explanations that are not only accurate but also tailored to diverse audiences? High-quality explanations require both factual correctness and the ability to meet the expectations of users with varying expertise. In this work, we construct a large-scale dataset comprising 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We introduce a principle-guided LLM-as-a-judge evaluation framework, employing dual judges to assess explanation quality. Furthermore, we show that fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) can significantly enhance the quality of generated explanations. Our results reveal: (1) The proposed LLM judges align well with human evaluations; (2) explanation quality varies significantly across models, audiences, and categories; and (3) DPO- and SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks. 

**Abstract (ZH)**: well-being涉及个体成长和明智生活决策所需的心理、生理和社会维度。随着个体越来越多地咨询大型语言模型（LLMs）以了解well-being，一个关键挑战出现了：LLMs能否生成不仅准确而且针对多元受众的个性化解释？高质量的解释要求既要具备事实正确性，也要能够满足不同专业背景用户的需求。在这项工作中，我们构建了一个包含43,880个解释的大型数据集，这些解释基于2,194个well-being概念，由十种不同的LLMs生成。我们引入了一个基于原则的LLM作为裁判的评估框架，采用双裁判评估解释质量。此外，我们展示了使用有监督微调（SFT）和直接偏好优化（DPO）对开源LLMs进行微调可以显著提高生成解释的质量。我们的结果表明：（1）提出的LLM裁判与人类评估高度一致；（2）不同模型、受众和类别之间的解释质量差异显著；（3）经过DPO和SFT微调的模型优于其较大的同类模型，证明了基于偏好的学习在专门的解释任务中的有效性。 

---
# Data and AI governance: Promoting equity, ethics, and fairness in large language models 

**Title (ZH)**: 数据与人工智能治理：促进大型语言模型的公平性、伦理性和公正性 

**Authors**: Alok Abhishek, Lisa Erickson, Tushar Bandopadhyay  

**Link**: [PDF](https://arxiv.org/pdf/2508.03970)  

**Abstract**: In this paper, we cover approaches to systematically govern, assess and quantify bias across the complete life cycle of machine learning models, from initial development and validation to ongoing production monitoring and guardrail implementation. Building upon our foundational work on the Bias Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the authors share prevalent bias and fairness related gaps in Large Language Models (LLMs) and discuss data and AI governance framework to address Bias, Ethics, Fairness, and Factuality within LLMs. The data and AI governance approach discussed in this paper is suitable for practical, real-world applications, enabling rigorous benchmarking of LLMs prior to production deployment, facilitating continuous real-time evaluation, and proactively governing LLM generated responses. By implementing the data and AI governance across the life cycle of AI development, organizations can significantly enhance the safety and responsibility of their GenAI systems, effectively mitigating risks of discrimination and protecting against potential reputational or brand-related harm. Ultimately, through this article, we aim to contribute to advancement of the creation and deployment of socially responsible and ethically aligned generative artificial intelligence powered applications. 

**Abstract (ZH)**: 本文系统地覆盖了从机器学习模型初始开发和验证到持续生产监控和边界实施的整个生命周期中治理、评估和量化偏见的方法。在我们关于大型语言模型（LLMs）Bias Evaluation and Assessment Test Suite (BEATS) 的基础研究之上，作者分享了LLMs中存在的常见偏见和公平性缺口，并讨论了数据和AI治理框架以解决LLMs中的偏见、伦理、公平性和事实性问题。本文中讨论的数据和AI治理方法适合实际应用场景，在生成性人工智能部署之前提供严格的基准测试，促进持续的实时评估，并主动治理LLMs生成的响应。通过在整个AI开发生命周期中实施数据和AI治理，组织可以显著提高其生成性人工智能系统的安全性和责任感，有效减少歧视风险并防止潜在的品牌损害。最终，本文旨在为负责任和伦理上对齐的生成性人工智能应用的开发和部署做出贡献。 

---
# Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models 

**Title (ZH)**: 幻觉到事实：大型语言模型中的事实核查与事实评价综述 

**Authors**: Subhey Sadi Rahman, Md. Adnanul Islam, Md. Mahbub Alam, Musarrat Zeba, Md. Abdur Rahman, Sadia Sultana Chowa, Mohaimenul Azam Khan Raiaan, Sami Azam  

**Link**: [PDF](https://arxiv.org/pdf/2508.03860)  

**Abstract**: Large Language Models (LLMs) are trained on vast and diverse internet corpora that often include inaccurate or misleading content. Consequently, LLMs can generate misinformation, making robust fact-checking essential. This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics. The review emphasizes the need for strong fact-checking frameworks that integrate advanced prompting strategies, domain-specific fine-tuning, and retrieval-augmented generation (RAG) methods. It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques. The review also discusses the role of instruction tuning, multi-agent reasoning, and external knowledge access via RAG frameworks. Key findings highlight the limitations of current metrics, the value of grounding outputs with validated external evidence, and the importance of domain-specific customization to improve factual consistency. Overall, the review underlines the importance of building LLMs that are not only accurate and explainable but also tailored for domain-specific fact-checking. These insights contribute to the advancement of research toward more trustworthy and context-aware language models. 

**Abstract (ZH)**: 大型语言模型（LLMs）训练数据包括大量的互联网文本，其中常包含不准确或误导性内容。因此，LLMs生成虚假信息，这使得进行稳健的事实核查变得尤为重要。本综述系统分析了LLM生成内容的事实准确性评估方法，探讨了幻觉、数据集限制和评估指标可靠性等关键挑战。综述强调了需要建立强大的事实核查框架，该框架应整合先进的提示策略、领域特定微调和检索增强生成（RAG）方法。该综述提出五个研究问题，以指导2020年至2025年相关文献的分析，重点在于评估方法和缓解技术。综述还讨论了指令微调、多智能体推理以及通过RAG框架获取外部知识的作用。关键发现强调了当前指标的局限性、使用验证过的外部证据来约束输出的价值，以及对提高事实一致性具有重要意义的领域特定定制。总体而言，综述强调了不仅要构建准确可解释的语言模型，还要针对特定领域的事实核查进行定制的重要性。这些见解促进了对更可信且上下文感知语言模型的研究进展。 

---
# GTPO: Trajectory-Based Policy Optimization in Large Language Models 

**Title (ZH)**: GTPO：大型语言模型中的轨迹导向策略优化 

**Authors**: Marco Simoni, Aleksandar Fontana, Giulio Rossolini, Andrea Saracino  

**Link**: [PDF](https://arxiv.org/pdf/2508.03772)  

**Abstract**: Policy-based optimizations are widely adopted today for the training and alignment of language models, where one of the most recent and effective approaches is Group-relative Policy Optimization (GRPO). In this paper, we reveals and analyze two major limitations of GRPO: (i) tokens frequently appear in completions with both positive and negative rewards, leading to conflicting gradient updates that can reduce their output probability, even though can be essential for maintaining proper structure; (ii) negatively rewarded completions may penalize confident responses and shift model decisions toward unlikely tokens, progressively flattening the output distribution and degrading learning. To address these issues and provide a more stable and effective policy optimization strategy, we introduce GTPO (Group-relative Trajectory-based Policy Optimization), which identifies conflict tokens, tokens appearing in the same position across completions with opposite rewards, protects them by skipping negative updates, while amplifying positive ones. To further prevent policy collapse, GTPO filters out completions whose entropy exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence regularization, eliminating the need for a reference model during training, while still ensuring greater training stability and improved performance, validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks. 

**Abstract (ZH)**: 基于组相对轨迹的策略优化（GTPO）：解决GRPO的两大限制并提升模型性能 

---
# Trustworthiness of Legal Considerations for the Use of LLMs in Education 

**Title (ZH)**: LLMs在教育中的使用中法律考量的可信性 

**Authors**: Sara Alaswad, Tatiana Kalganova, Wasan Awad  

**Link**: [PDF](https://arxiv.org/pdf/2508.03771)  

**Abstract**: As Artificial Intelligence (AI), particularly Large Language Models (LLMs), becomes increasingly embedded in education systems worldwide, ensuring their ethical, legal, and contextually appropriate deployment has become a critical policy concern. This paper offers a comparative analysis of AI-related regulatory and ethical frameworks across key global regions, including the European Union, United Kingdom, United States, China, and Gulf Cooperation Council (GCC) countries. It maps how core trustworthiness principles, such as transparency, fairness, accountability, data privacy, and human oversight are embedded in regional legislation and AI governance structures. Special emphasis is placed on the evolving landscape in the GCC, where countries are rapidly advancing national AI strategies and education-sector innovation. To support this development, the paper introduces a Compliance-Centered AI Governance Framework tailored to the GCC context. This includes a tiered typology and institutional checklist designed to help regulators, educators, and developers align AI adoption with both international norms and local values. By synthesizing global best practices with region-specific challenges, the paper contributes practical guidance for building legally sound, ethically grounded, and culturally sensitive AI systems in education. These insights are intended to inform future regulatory harmonization and promote responsible AI integration across diverse educational environments. 

**Abstract (ZH)**: 随着人工智能（AI），特别是大型语言模型（LLMs），在全球教育系统中的日益嵌入，确保其伦理、法律和上下文适当的部署已成为一项重要的政策关切。本文对欧洲联盟、英国、美国、中国和海湾 Cooperation Council (GCC) 国家等关键全球区域的AI相关监管和伦理框架进行了比较分析。它阐明了核心信任原则，如透明度、公平性、问责制、数据隐私和人类监督如何被嵌入区域立法和AI治理体系中。本文特别强调了GCC地区不断演变的AI和教育领域创新景观，在该地区各国正迅速推进国家AI战略。为支持这一发展，本文引入了一个以合规为中心的AI治理体系框架，适用于GCC地区的情境。该框架包括分层分类和机构检查清单，旨在帮助监管机构、教育工作者和开发者将AI的应用与国际规范和本地价值观对齐。通过将全球最佳实践与地区特定挑战相结合，本文为在教育领域构建合法、伦理和文化敏感的AI系统提供了实用指导。这些见解旨在为未来监管协调提供信息，并促进责任AI在不同教育环境中的整合。 

---
# Refine-IQA: Multi-Stage Reinforcement Finetuning for Perceptual Image Quality Assessment 

**Title (ZH)**: Refine-IQA：多阶段强化微调用于感知图像质量评估 

**Authors**: Ziheng Jia, Jiaying Qian, Zicheng Zhang, Zijian Chen, Xiongkuo Min  

**Link**: [PDF](https://arxiv.org/pdf/2508.03763)  

**Abstract**: Reinforcement fine-tuning (RFT) is a proliferating paradigm for LMM training. Analogous to high-level reasoning tasks, RFT is similarly applicable to low-level vision domains, including image quality assessment (IQA). Existing RFT-based IQA methods typically use rule-based output rewards to verify the model's rollouts but provide no reward supervision for the "think" process, leaving its correctness and efficacy uncontrolled. Furthermore, these methods typically fine-tune directly on downstream IQA tasks without explicitly enhancing the model's native low-level visual quality perception, which may constrain its performance upper bound. In response to these gaps, we propose the multi-stage RFT IQA framework (Refine-IQA). In Stage-1, we build the Refine-Perception-20K dataset (with 12 main distortions, 20,907 locally-distorted images, and over 55K RFT samples) and design multi-task reward functions to strengthen the model's visual quality perception. In Stage-2, targeting the quality scoring task, we introduce a probability difference reward involved strategy for "think" process supervision. The resulting Refine-IQA Series Models achieve outstanding performance on both perception and scoring tasks-and, notably, our paradigm activates a robust "think" (quality interpreting) capability that also attains exceptional results on the corresponding quality interpreting benchmark. 

**Abstract (ZH)**: 强化微调（RFT）是LMM训练中一个日益流行的范式。类比于高层推理任务，RFT同样适用于低层次视觉领域，包括图像质量评估（IQA）。现有的基于RFT的IQA方法通常使用基于规则的输出奖励来验证模型的构思结果，但并未对“思考”过程提供奖励监督，这使得其正确性和有效性无法控制。此外，这些方法通常直接在下游IQA任务上进行微调，而未明显增强模型的原生低层次视觉质量感知能力，这可能限制了其性能上限。为应对这些不足，我们提出了多阶段RFT-IQA框架（Refine-IQA）。在第一阶段，我们构建了Refine-Perception-20K数据集（包含12种主要失真，20,907张局部失真图像，以及超过55,000个RFT样本），并设计了多任务奖励函数以强化模型的视觉质量感知能力。在第二阶段，针对质量评分任务，我们引入了概率差奖励策略以监督“思考”过程。Refine-IQA系列模型在感知和评分任务上均取得了优异性能，并且我们的范式激活了强大的“思考”（质量解释）能力，在相应的质量解释基准上也取得了出色结果。 

---
# FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit Communication 

**Title (ZH)**: FlashCommunication V2: 位分裂和尖峰保留的任意位通信 

**Authors**: Qingyuan Li, Bo Zhang, Hui Kang, Tianhao Xu, Yulei Qian, Yuchen Xie, Lin Ma  

**Link**: [PDF](https://arxiv.org/pdf/2508.03760)  

**Abstract**: Nowadays, communication bottlenecks have emerged as a critical challenge in the distributed training and deployment of large language models (LLMs). This paper introduces FlashCommunication V2, a novel communication paradigm enabling efficient cross-GPU transmission at arbitrary bit widths. Its core innovations lie in the proposed bit splitting and spike reserving techniques, which address the challenges of low-bit quantization. Bit splitting decomposes irregular bit widths into basic units, ensuring compatibility with hardware capabilities and thus enabling transmission at any bit width. Spike reserving, on the other hand, retains numerical outliers (i.e., minima and maxima) as floating-point numbers, which shrinks the dynamic numerical range and pushes the quantization limits to 2-bit with acceptable losses. FlashCommunication V2 significantly enhances the flexibility and resource utilization of communication systems. Through meticulous software-hardware co-design, it delivers robust performance and reduced overhead across both NVLink-based and PCIe-based architectures, achieving a maximum 3.2$\times$ speedup in AllReduce and 2$\times$ in All2All communication. 

**Abstract (ZH)**: 如今，通信瓶颈已成为分布式训练和部署大规模语言模型（LLMs）的一个关键挑战。本文介绍了FlashCommunication V2，这是一种新型的通信范式，能够实现任意位宽下的高效跨GPU传输。其核心创新在于提出的位分裂和尖峰保留技术，这些技术解决了低位宽量化带来的挑战。位分裂将不规则的位宽分解为基本单元，确保与硬件能力的兼容性，从而实现任意位宽下的传输。尖峰保留则保留了数值异常值（即最小值和最大值）作为浮点数，这缩小了动态数值范围，并将量化限制推至2位宽，同时接受一定程度的性能损失。FlashCommunication V2 显著增强了通信系统的灵活性和资源利用率。通过细致的软硬件协同设计，它在基于NVLink和PCIe的架构上均提供了 robust 的性能和减少的开销，并分别在AllReduce和All2All通信中实现了最大3.2倍和2倍的速度提升。 

---
# Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models 

**Title (ZH)**: 潜在知识手术刀：大规模语言模型的精确且大规模知识编辑 

**Authors**: Xin Liu, Qiyang Song, Shaowen Xu, Kerou Zhou, Wenbo Jiang, Xiaoqi Jia, Weijuan Zhang, Heqing Huang, Yakai Li  

**Link**: [PDF](https://arxiv.org/pdf/2508.03741)  

**Abstract**: Large Language Models (LLMs) often retain inaccurate or outdated information from pre-training, leading to incorrect predictions or biased outputs during inference. While existing model editing methods can address this challenge, they struggle with editing large amounts of factual information simultaneously and may compromise the general capabilities of the models. In this paper, our empirical study demonstrates that it is feasible to edit the internal representations of LLMs and replace the entities in a manner similar to editing natural language inputs. Based on this insight, we introduce the Latent Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of specific entities via a lightweight hypernetwork to enable precise and large-scale editing. Experiments conducted on Llama-2 and Mistral show even with the number of simultaneous edits reaching 10,000, LKS effectively performs knowledge editing while preserving the general abilities of the edited LLMs. Code is available at: this https URL. 

**Abstract (ZH)**: 大规模语言模型（LLMs）往往保留预训练中的不准确或过时信息，导致推理过程中产生错误预测或有偏输出。尽管现有的模型编辑方法可以应对这一挑战，但在同时编辑大量事实信息方面仍存在问题，并可能损害模型的一般能力。本文通过实证研究证明，可以编辑LLMs的内部表示，并以类似编辑自然语言输入的方式来替换实体。基于这一洞察，我们提出了潜在知识手术刀（LKS），这是一种通过轻量级超网络操作特定实体的潜在知识的LLM编辑器，以实现精确和大规模的编辑。实验结果表明，即使同时编辑数量高达10,000，LKS仍能有效进行知识编辑，同时保留编辑后的LLMs的一般能力。代码可在以下链接获取：this https URL。 

---
# Privacy Risks of LLM-Empowered Recommender Systems: An Inversion Attack Perspective 

**Title (ZH)**: 基于反转攻击视角的LLM赋能推荐系统隐私风险 

**Authors**: Yubo Wang, Min Tang, Nuo Shen, Shujie Cui, Weiqing Wang  

**Link**: [PDF](https://arxiv.org/pdf/2508.03703)  

**Abstract**: The large language model (LLM) powered recommendation paradigm has been proposed to address the limitations of traditional recommender systems, which often struggle to handle cold start users or items with new IDs. Despite its effectiveness, this study uncovers that LLM empowered recommender systems are vulnerable to reconstruction attacks that can expose both system and user privacy. To examine this threat, we present the first systematic study on inversion attacks targeting LLM empowered recommender systems, where adversaries attempt to reconstruct original prompts that contain personal preferences, interaction histories, and demographic attributes by exploiting the output logits of recommendation models. We reproduce the vec2text framework and optimize it using our proposed method called Similarity Guided Refinement, enabling more accurate reconstruction of textual prompts from model generated logits. Extensive experiments across two domains (movies and books) and two representative LLM based recommendation models demonstrate that our method achieves high fidelity reconstructions. Specifically, we can recover nearly 65 percent of the user interacted items and correctly infer age and gender in 87 percent of the cases. The experiments also reveal that privacy leakage is largely insensitive to the victim model's performance but highly dependent on domain consistency and prompt complexity. These findings expose critical privacy vulnerabilities in LLM empowered recommender systems. 

**Abstract (ZH)**: LLM赋能推荐系统中的反转攻击研究：隐私泄露脆弱性分析 

---
