# HJRNO: Hamilton-Jacobi Reachability with Neural Operators 

**Title (ZH)**: HJRNO: Hamilton-Jacobi Reachability with Neural Operators 

**Authors**: Yankai Li, Mo Chen  

**Link**: [PDF](https://arxiv.org/pdf/2504.19989)  

**Abstract**: Ensuring the safety of autonomous systems under uncertainty is a critical challenge. Hamilton-Jacobi reachability (HJR) analysis is a widely used method for guaranteeing safety under worst-case disturbances. Traditional HJR methods provide safety guarantees but suffer from the curse of dimensionality, limiting their scalability to high-dimensional systems or varying environmental conditions. In this work, we propose HJRNO, a neural operator-based framework for solving backward reachable tubes (BRTs) efficiently and accurately. By leveraging the Fourier Neural Operator (FNO), HJRNO learns a mapping between value functions, enabling fast inference with strong generalization across different obstacle shapes, system configurations, and hyperparameters. We demonstrate that HJRNO achieves low error on random obstacle scenarios and generalizes effectively across varying system dynamics. These results suggest that HJRNO offers a promising foundation model approach for scalable, real-time safety analysis in autonomous systems. 

**Abstract (ZH)**: 确保自主系统在不确定条件下的安全性是一个关键挑战。哈密尔顿-雅各比可达性（HJR）分析是一种广泛用于在最坏情况干扰下提供安全性保证的方法。传统HJR方法虽能提供安全性保证，但受到维数灾的限制，限制了其在高维系统或变化环境条件下的可扩展性。在这项工作中，我们提出了HJRNO，一种基于神经算子的框架，用于高效准确地求解后向可达管（BRTs）。通过利用傅里叶神经算子（FNO），HJRNO学习值函数之间的映射，使得具有强泛化能力的快速推理成为可能，适用于不同的障碍形状、系统配置和超参数。我们展示了HJRNO在随机障碍情境下具有低误差，并且在不同系统动力学条件下具有良好的泛化能力。这些结果表明，HJRNO为自主系统中可扩展的实时安全性分析提供了有前景的基础模型方法。 

---
# Human-Centered AI and Autonomy in Robotics: Insights from a Bibliometric Study 

**Title (ZH)**: 以人为本的AI与机器人自主性：基于文献计量学的研究见解 

**Authors**: Simona Casini, Pietro Ducange, Francesco Marcelloni, Lorenzo Pollini  

**Link**: [PDF](https://arxiv.org/pdf/2504.19848)  

**Abstract**: The development of autonomous robotic systems offers significant potential for performing complex tasks with precision and consistency. Recent advances in Artificial Intelligence (AI) have enabled more capable intelligent automation systems, addressing increasingly complex challenges. However, this progress raises questions about human roles in such systems. Human-Centered AI (HCAI) aims to balance human control and automation, ensuring performance enhancement while maintaining creativity, mastery, and responsibility. For real-world applications, autonomous robots must balance task performance with reliability, safety, and trustworthiness. Integrating HCAI principles enhances human-robot collaboration and ensures responsible operation.
This paper presents a bibliometric analysis of intelligent autonomous robotic systems, utilizing SciMAT and VOSViewer to examine data from the Scopus database. The findings highlight academic trends, emerging topics, and AI's role in self-adaptive robotic behaviour, with an emphasis on HCAI architecture. These insights are then projected onto the IBM MAPE-K architecture, with the goal of identifying how these research results map into actual robotic autonomous systems development efforts for real-world scenarios. 

**Abstract (ZH)**: 自主机器人系统的发展为精确和一致地执行复杂任务提供了重要潜力。近期人工智能（AI）的进步使智能自动化系统更加 capable，以应对愈加复杂的挑战。然而，这种进步引发了关于此类系统中人类角色的问题。以人为中心的人工智能（HCAI）旨在平衡人类控制与自动化，确保性能提升的同时保持创造力、专业性和责任感。在实际应用中，自主机器人必须平衡任务性能、可靠性和可信度。结合HCAI原则可以增强人机协作并确保负责任的操作。

本文利用SciMAT和VOSViewer对Scopus数据库的数据进行文献计量分析，研究智能自主机器人系统的学术趋势、新兴主题以及AI在自适应机器人行为中的作用，重点在于HCAI架构。随后，这些见解被投射到IBM MAPE-K架构上，旨在识别这些研究结果如何映射到实际的自主机器人系统开发努力中的现实场景。 

---
# Automated Generation of Precedence Graphs in Digital Value Chains for Automotive Production 

**Title (ZH)**: 汽车生产中数字价值链中 precedence 图的自动生成 

**Authors**: Cornelius Hake, Christian Friedrich  

**Link**: [PDF](https://arxiv.org/pdf/2504.19835)  

**Abstract**: This study examines the digital value chain in automotive manufacturing, focusing on the identification, software flashing, customization, and commissioning of electronic control units in vehicle networks. A novel precedence graph design is proposed to optimize this process chain using an automated scheduling algorithm that employs mixed integer linear programming techniques. The results show significant improvements in key metrics. The algorithm reduces the number of production stations equipped with expensive hardware and software to execute digital value chain processes, while increasing capacity utilization through efficient scheduling and reduced idle time. Task parallelization is optimized, resulting in streamlined workflows and increased throughput. Compared to the traditional method, the automated approach has reduced preparation time by 50% and reduced scheduling activities, as it now takes two minutes to create the precedence graph. The flexibility of the algorithm's constraints allows for vehicle-specific configurations while maintaining high responsiveness, eliminating backup stations and facilitating the integration of new topologies. Automated scheduling significantly outperforms manual methods in efficiency, functionality, and adaptability. 

**Abstract (ZH)**: 本研究考察了汽车制造中的数字化价值链，着重于电动控制单元在车辆网络中的识别、软件烧录、定制和调试过程。提出了一种新颖的优先级图设计，通过使用混合整数线性规划技术的自动化调度算法来优化这一过程链。结果显示，在关键指标上取得了显著改进。该算法减少了用于执行数字化价值链过程的昂贵硬件和软件的生产站数量，通过高效的调度和减少闲置时间从而提高产能利用率。任务并行化得到了优化，实现了流畅的工作流程和更高的吞吐量。与传统方法相比，自动化方法将准备时间减少了50%，创建优先级图只需两分钟。算法的约束条件具有灵活性，允许针对特定车型进行配置，同时保持高响应性，消除了备用站并促进了新型拓扑结构的集成。自动化调度在效率、功能性和适应性方面显著优于手动方法。 

---
# ARMOR: Adaptive Meshing with Reinforcement Optimization for Real-time 3D Monitoring in Unexposed Scenes 

**Title (ZH)**: ARMOR：适用于未暴露场景的自适应网格生成与强化优化实时3D监控 

**Authors**: Yizhe Zhang, Jianping Li, Xin Zhao, Fuxun Liang, Zhen Dong, Bisheng Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.19624)  

**Abstract**: Unexposed environments, such as lava tubes, mines, and tunnels, are among the most complex yet strategically significant domains for scientific exploration and infrastructure development. Accurate and real-time 3D meshing of these environments is essential for applications including automated structural assessment, robotic-assisted inspection, and safety monitoring. Implicit neural Signed Distance Fields (SDFs) have shown promising capabilities in online meshing; however, existing methods often suffer from large projection errors and rely on fixed reconstruction parameters, limiting their adaptability to complex and unstructured underground environments such as tunnels, caves, and lava tubes. To address these challenges, this paper proposes ARMOR, a scene-adaptive and reinforcement learning-based framework for real-time 3D meshing in unexposed environments. The proposed method was validated across more than 3,000 meters of underground environments, including engineered tunnels, natural caves, and lava tubes. Experimental results demonstrate that ARMOR achieves superior performance in real-time mesh reconstruction, reducing geometric error by 3.96\% compared to state-of-the-art baselines, while maintaining real-time efficiency. The method exhibits improved robustness, accuracy, and adaptability, indicating its potential for advanced 3D monitoring and mapping in challenging unexposed scenarios. The project page can be found at: this https URL 

**Abstract (ZH)**: 未暴露环境的实时3D网状构建：ARMOR——一种场景自适应和基于强化学习的框架 

---
# A Time-dependent Risk-aware distributed Multi-Agent Path Finder based on A* 

**Title (ZH)**: 基于A*的时间依赖风险意识分布式多Agent路径规划器 

**Authors**: S Nordström, Y Bai, B Lindqvist, G Nikolakopoulos  

**Link**: [PDF](https://arxiv.org/pdf/2504.19593)  

**Abstract**: Multi-Agent Path-Finding (MAPF) focuses on the collaborative planning of paths for multiple agents within shared spaces, aiming for collision-free navigation. Conventional planning methods often overlook the presence of other agents, which can result in conflicts. In response, this article introduces the A$^*_+$T algorithm, a distributed approach that improves coordination among agents by anticipating their positions based on their movement speeds. The algorithm also considers dynamic obstacles, assessing potential collisions with respect to observed speeds and trajectories, thereby facilitating collision-free path planning in environments populated by other agents and moving objects. It incorporates a risk layer surrounding both dynamic and static entities, enhancing its utility in real-world applications. Each agent functions autonomously while being mindful of the paths chosen by others, effectively addressing the complexities inherent in multi-agent situations. The performance of A$^*_+$T has been rigorously tested in the Gazebo simulation environment and benchmarked against established approaches such as CBS, ECBS, and SIPP. Furthermore, the algorithm has shown competence in single-agent experiments, with results demonstrating its effectiveness in managing dynamic obstacles and affirming its practical relevance across various scenarios. 

**Abstract (ZH)**: 多代理路径寻找(Multi-Agent Path-Finding)专注于在共享空间内多个代理的协作路径规划，旨在实现无碰撞导航。传统的规划方法往往忽视其他代理的存在，可能导致冲突。为应对这一挑战，本文引入了A$^*_+$T算法，这是一种分布式方法，通过预测代理的运动位置来改善多代理之间的协调，同时考虑动态障碍物，评估潜在碰撞，从而在包含其他代理和移动物体的环境中实现无碰撞路径规划。该算法在动态和静态实体周围引入了一个风险层，增强了其实用性。每个代理在自主运行的同时，也考虑了其他代理选择的路径，有效解决了多代理情况下的复杂性。A$^*_+$T算法已在Gazebo仿真环境中严格测试，并与CBS、ECBS和SIPP等现有方法进行了对比。此外，该算法在单代理实验中也表现出色，结果证明其在管理动态障碍物方面的有效性，并且在各种场景中具有实际的相关性。 

---
# Making Physical Objects with Generative AI and Robotic Assembly: Considering Fabrication Constraints, Sustainability, Time, Functionality, and Accessibility 

**Title (ZH)**: 使用生成式人工智能和机器人装配制作物理对象：考虑加工约束、可持续性、时间、功能和可访问性 

**Authors**: Alexander Htet Kyaw, Se Hwan Jeon, Miana Smith, Neil Gershenfeld  

**Link**: [PDF](https://arxiv.org/pdf/2504.19131)  

**Abstract**: 3D generative AI enables rapid and accessible creation of 3D models from text or image inputs. However, translating these outputs into physical objects remains a challenge due to the constraints in the physical world. Recent studies have focused on improving the capabilities of 3D generative AI to produce fabricable outputs, with 3D printing as the main fabrication method. However, this workshop paper calls for a broader perspective by considering how fabrication methods align with the capabilities of 3D generative AI. As a case study, we present a novel system using discrete robotic assembly and 3D generative AI to make physical objects. Through this work, we identified five key aspects to consider in a physical making process based on the capabilities of 3D generative AI. 1) Fabrication Constraints: Current text-to-3D models can generate a wide range of 3D designs, requiring fabrication methods that can adapt to the variability of generative AI outputs. 2) Time: While generative AI can generate 3D models in seconds, fabricating physical objects can take hours or even days. Faster production could enable a closer iterative design loop between humans and AI in the making process. 3) Sustainability: Although text-to-3D models can generate thousands of models in the digital world, extending this capability to the real world would be resource-intensive, unsustainable and irresponsible. 4) Functionality: Unlike digital outputs from 3D generative AI models, the fabrication method plays a crucial role in the usability of physical objects. 5) Accessibility: While generative AI simplifies 3D model creation, the need for fabrication equipment can limit participation, making AI-assisted creation less inclusive. These five key aspects provide a framework for assessing how well a physical making process aligns with the capabilities of 3D generative AI and values in the world. 

**Abstract (ZH)**: 3D生成AI使从文本或图像输入快速便捷地创建3D模型成为可能，但由于物理世界的约束，将这些输出转化为物理对象仍具挑战性。最近的研究集中在提高3D生成AI的能力以生产可制造的输出，3D打印为主要的制造方法。然而，本文研讨会论文呼吁从更广泛的角度考虑制造方法与3D生成AI能力的契合性。作为案例研究，我们提出了一种使用离散机器人装配和3D生成AI生成物理对象的新系统。通过这项工作，我们基于3D生成AI的能力，确定了物理制作过程中的五个关键方面：1) 制造约束；2) 时间；3) 可持续性；4) 功能性；5) 可及性。这些五个关键方面为评估物理制作过程与3D生成AI能力及世界价值观的契合度提供了框架。 

---
# MISO: Multiresolution Submap Optimization for Efficient Globally Consistent Neural Implicit Reconstruction 

**Title (ZH)**: MISO：多分辨率子地图优化以实现高效的全局一致神经隐式重建 

**Authors**: Yulun Tian, Hanwen Cao, Sunghwan Kim, Nikolay Atanasov  

**Link**: [PDF](https://arxiv.org/pdf/2504.19104)  

**Abstract**: Neural implicit representations have had a significant impact on simultaneous localization and mapping (SLAM) by enabling robots to build continuous, differentiable, and high-fidelity 3D maps from sensor data. However, as the scale and complexity of the environment increase, neural SLAM approaches face renewed challenges in the back-end optimization process to keep up with runtime requirements and maintain global consistency. We introduce MISO, a hierarchical optimization approach that leverages multiresolution submaps to achieve efficient and scalable neural implicit reconstruction. For local SLAM within each submap, we develop a hierarchical optimization scheme with learned initialization that substantially reduces the time needed to optimize the implicit submap features. To correct estimation drift globally, we develop a hierarchical method to align and fuse the multiresolution submaps, leading to substantial acceleration by avoiding the need to decode the full scene geometry. MISO significantly improves computational efficiency and estimation accuracy of neural signed distance function (SDF) SLAM on large-scale real-world benchmarks. 

**Abstract (ZH)**: 神经隐式表示对同时定位与mapping（SLAM）产生了显著影响，使机器人能够从传感器数据中构建连续、可微分和高保真的3D地图。然而，随着环境规模和复杂性的增加，神经SLAM方法在后端优化过程中面临着新的挑战，以满足实时要求并保持全局一致性。我们提出了MISO，一种利用多分辨率子地图的分层优化方法，以实现高效的可扩展神经隐式重建。对于每个子地图内的局部SLAM，我们开发了一种包含学习初始化的分层优化方案，极大地减少了优化隐式子地图特征所需的时间。为了全局纠正估计漂移，我们开发了一种分层方法来对齐和融合多分辨率子地图，从而通过避免解码整个场景几何结构来实现显著加速。MISO在大型真实世界基准上的神经符号距离函数（SDF）SLAM计算效率和估计准确性得到了显著提升。 

---
# Advanced Longitudinal Control and Collision Avoidance for High-Risk Edge Cases in Autonomous Driving 

**Title (ZH)**: 高级纵向控制与碰撞避免技术在自动驾驶高风险边缘情况中的应用 

**Authors**: Dianwei Chen, Yaobang Gong, Xianfeng Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.18931)  

**Abstract**: Advanced Driver Assistance Systems (ADAS) and Advanced Driving Systems (ADS) are key to improving road safety, yet most existing implementations focus primarily on the vehicle ahead, neglecting the behavior of following vehicles. This shortfall often leads to chain reaction collisions in high speed, densely spaced traffic particularly when a middle vehicle suddenly brakes and trailing vehicles cannot respond in time. To address this critical gap, we propose a novel longitudinal control and collision avoidance algorithm that integrates adaptive cruising with emergency braking. Leveraging deep reinforcement learning, our method simultaneously accounts for both leading and following vehicles. Through a data preprocessing framework that calibrates real-world sensor data, we enhance the robustness and reliability of the training process, ensuring the learned policy can handle diverse driving conditions. In simulated high risk scenarios (e.g., emergency braking in dense traffic), the algorithm effectively prevents potential pile up collisions, even in situations involving heavy duty vehicles. Furthermore, in typical highway scenarios where three vehicles decelerate, the proposed DRL approach achieves a 99% success rate far surpassing the standard Federal Highway Administration speed concepts guide, which reaches only 36.77% success under the same conditions. 

**Abstract (ZH)**: 先进的驾驶辅助系统（ADAS）和高级驾驶系统（ADS）对于提高道路交通安全至关重要，但现有大多数实现主要关注前方车辆，忽视了跟随车辆的行为。这一不足往往导致在高密度、高速交通中，当中间车辆突然制动而后续车辆无法及时响应时发生连锁碰撞。为弥补这一关键缺口，我们提出了一种新颖的纵向控制与碰撞避免算法，将自适应巡航与紧急制动相结合。利用深度强化学习，我们的方法同时考虑领车和随车的行为。通过一个数据预处理框架来校准现实世界传感器数据，我们增强了训练过程的鲁棒性和可靠性，确保学习到的策略能够应对多种驾驶条件。在模拟高风险场景（如密集交通中的紧急制动）中，该算法有效防止了潜在的堆积碰撞，即使涉及重型车辆。此外，在典型高速公路上三辆车减速的场景中，所提出的DRL方法的成功率达到99%，远超美国联邦公路管理局速度概念指南在同一条件下的36.77%的成功率。 

---
# Coherence-based Approximate Derivatives via Web of Affine Spaces Optimization 

**Title (ZH)**: 基于一致性的近似导数通过仿射空间网络优化 

**Authors**: Daniel Rakita, Chen Liang, Qian Wang  

**Link**: [PDF](https://arxiv.org/pdf/2504.18790)  

**Abstract**: Computing derivatives is a crucial subroutine in computer science and related fields as it provides a local characterization of a function's steepest directions of ascent or descent. In this work, we recognize that derivatives are often not computed in isolation; conversely, it is quite common to compute a \textit{sequence} of derivatives, each one somewhat related to the last. Thus, we propose accelerating derivative computation by reusing information from previous, related calculations-a general strategy known as \textit{coherence}. We introduce the first instantiation of this strategy through a novel approach called the Web of Affine Spaces (WASP) Optimization. This approach provides an accurate approximation of a function's derivative object (i.e. gradient, Jacobian matrix, etc.) at the current input within a sequence. Each derivative within the sequence only requires a small number of forward passes through the function (typically two), regardless of the number of function inputs and outputs. We demonstrate the efficacy of our approach through several numerical experiments, comparing it with alternative derivative computation methods on benchmark functions. We show that our method significantly improves the performance of derivative computation on small to medium-sized functions, i.e., functions with approximately fewer than 500 combined inputs and outputs. Furthermore, we show that this method can be effectively applied in a robotics optimization context. We conclude with a discussion of the limitations and implications of our work. Open-source code, visual explanations, and videos are located at the paper website: \href{this https URL}{this https URL}. 

**Abstract (ZH)**: 计算导数是计算机科学及相关领域中的一个关键子程序，它提供了函数最陡上升或下降方向的局部表征。在这项工作中，我们认识到导数通常不是独立计算的；相反，计算导数序列的情况相当常见，每一项都与前一项有一定的关联。因此，我们提出通过重用之前相关计算中的信息来加速导数计算——这一一般策略称为“一致性”。我们通过一种新型方法——仿射空间网（WASP）优化——实现这一策略的第一种具体实例。这种方法在序列中的当前输入处提供了函数导数对象（即梯度、雅可比矩阵等）的精确近似。序列中的每项导数只需少量（通常是两次）函数前向传递，而不受函数输入和输出数量的影响。我们通过多项数值实验来展示该方法的有效性，将它与其他导数计算方法在基准函数上进行比较。我们证明，对于小型到中型函数（即大约合并输入和输出小于500的函数），我们的方法显著提高了导数计算的性能。此外，我们展示了该方法在机器人优化上下文中的应用效果。最后，我们讨论了该工作的局限性和意义。开源代码、可视化解释和视频可在论文网站：\[this https URL\]找到。 

---
# Mesh-Learner: Texturing Mesh with Spherical Harmonics 

**Title (ZH)**: 网状学习者：基于球谐函数的网格纹理化 

**Authors**: Yunfei Wan, Jianheng Liu, Jiarong Lin, Fu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2504.19938)  

**Abstract**: In this paper, we present a 3D reconstruction and rendering framework termed Mesh-Learner that is natively compatible with traditional rasterization pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e., texture filled with SH coefficients) into the learning process to learn each mesh s view-dependent radiance end-to-end. Images are rendered by interpolating surrounding SH Texels at each pixel s sampling point using a novel interpolation method. Conversely, gradients from each pixel are back-propagated to the related SH Texels in SH textures. Mesh-Learner exploits graphic features of rasterization pipeline (texture sampling, deferred rendering) to render, which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for robotics) that are based on rasterization pipelines. Our system can train vast, unlimited scenes because we transfer only the SH textures within the frustum to the GPU for training. At other times, the SH textures are stored in CPU RAM, which results in moderate GPU memory usage. The rendering results on interpolation and extrapolation sequences in the Replica and FAST-LIVO2 datasets achieve state-of-the-art performance compared to existing state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To benefit the society, the code will be available at this https URL. 

**Abstract (ZH)**: 一种与传统栅格化管道本征兼容的3D重建与渲染框架：Mesh-Learner 

---
# Stability Enhancement in Reinforcement Learning via Adaptive Control Lyapunov Function 

**Title (ZH)**: 基于自适应控制李亚普诺夫函数的强化学习稳定性增强 

**Authors**: Donghe Chen, Han Wang, Lin Cheng, Shengping Gong  

**Link**: [PDF](https://arxiv.org/pdf/2504.19473)  

**Abstract**: Reinforcement Learning (RL) has shown promise in control tasks but faces significant challenges in real-world applications, primarily due to the absence of safety guarantees during the learning process. Existing methods often struggle with ensuring safe exploration, leading to potential system failures and restricting applications primarily to simulated environments. Traditional approaches such as reward shaping and constrained policy optimization can fail to guarantee safety during initial learning stages, while model-based methods using Control Lyapunov Functions (CLFs) or Control Barrier Functions (CBFs) may hinder efficient exploration and performance. To address these limitations, this paper introduces Soft Actor-Critic with Control Lyapunov Function (SAC-CLF), a framework that enhances stability and safety through three key innovations: (1) a task-specific CLF design method for safe and optimal performance; (2) dynamic adjustment of constraints to maintain robustness under unmodeled dynamics; and (3) improved control input smoothness while ensuring safety. Experimental results on a classical nonlinear system and satellite attitude control demonstrate the effectiveness of SAC-CLF in overcoming the shortcomings of existing methods. 

**Abstract (ZH)**: 基于控制李apunov函数的软演员-评论家（SAC-CLF）：一种增强稳定性和安全性的强化学习框架 

---
# Snake locomotion learning search 

**Title (ZH)**: 蛇形运动学习搜索 

**Authors**: Sheng-Xue He  

**Link**: [PDF](https://arxiv.org/pdf/2504.19114)  

**Abstract**: This research introduces a novel heuristic algorithm known as the Snake Locomotion Learning Search algorithm (SLLS) designed to address optimization problems. The SLLS draws inspiration from the locomotion patterns observed in snakes, particularly serpentine and caterpillar locomotion. We leverage these two modes of snake locomotion to devise two distinct search mechanisms within the SLLS. In our quest to mimic a snake's natural adaptation to its surroundings, we incorporate a learning efficiency component generated from the Sigmoid function. This helps strike a balance between exploration and exploitation capabilities throughout the SLLS computation process. The efficacy and effectiveness of this innovative algorithm are demonstrated through its application to 60 standard benchmark optimization problems and seven well-known engineering optimization problems. The performance analysis reveals that in most cases, the SLLS outperforms other algorithms, and even in the remaining scenarios, it exhibits robust performance. This conforms to the No Free Lunch Theorem, affirming that the SLLS stands as a valuable heuristic algorithm with significant potential for effectively addressing specific optimization challenges. 

**Abstract (ZH)**: Snake Locomotion Learning Search算法（SLLS）在优化问题中的应用研究 

---
# Towards Automated Scoping of AI for Social Good Projects 

**Title (ZH)**: 面向社会公益项目的自动范围界定的人工智能研究 

**Authors**: Jacob Emmerson, Rayid Ghani, Zheyuan Ryan Shi  

**Link**: [PDF](https://arxiv.org/pdf/2504.20010)  

**Abstract**: Artificial Intelligence for Social Good (AI4SG) is an emerging effort that aims to address complex societal challenges with the powerful capabilities of AI systems. These challenges range from local issues with transit networks to global wildlife preservation. However, regardless of scale, a critical bottleneck for many AI4SG initiatives is the laborious process of problem scoping -- a complex and resource-intensive task -- due to a scarcity of professionals with both technical and domain expertise. Given the remarkable applications of large language models (LLM), we propose a Problem Scoping Agent (PSA) that uses an LLM to generate comprehensive project proposals grounded in scientific literature and real-world knowledge. We demonstrate that our PSA framework generates proposals comparable to those written by experts through a blind review and AI evaluations. Finally, we document the challenges of real-world problem scoping and note several areas for future work. 

**Abstract (ZH)**: 人工智能用于社会公益（AI4SG）旨在利用人工智能系统的强大能力解决复杂的社会挑战，这些挑战从当地的交通网络问题到全球的野生动物保护无所不包。然而，不论规模如何，许多AI4SG项目的关键瓶颈是问题界定过程——一个复杂且资源密集的任务，由于缺乏同时具备技术和领域专业知识的专业人士而受到限制。鉴于大型语言模型的杰出应用，我们提出了一种问题界定代理（PSA），该代理利用大型语言模型生成基于科学文献和现实世界知识的全面项目提案。我们通过盲审和AI评估证明，我们的PSA框架生成的提案与专家撰写的提案相当。最后，我们记录了实际问题界定过程中的挑战，并指出若干未来研究方向。 

---
# How Group Lives Go Well 

**Title (ZH)**: 如何让群体生活得更好 

**Authors**: John Beverley, Regina Hurley  

**Link**: [PDF](https://arxiv.org/pdf/2504.19968)  

**Abstract**: This paper explores the ontological space of group well being, proposing a framework for representing collective welfare, group functions, and long term contributions within an ontology engineering context. Traditional well being theories focus on individual states, often relying on hedonistic, desire satisfaction, or objective list models. Such approaches struggle to account for cases where individual sacrifices contribute to broader social progress, a critical challenge in modeling group flourishing. To address this, the paper refines and extends the Counterfactual Account (CT) of well being, which evaluates goodness of an event by comparing an individual's actual well being with a hypothetical counterpart in a nearby possible world. While useful, this framework is insufficient for group level ontologies, where well being depends on functional persistence, institutional roles, and historical impact rather than immediate individual outcomes. Drawing on Basic Formal Ontology (BFO), the paper introduces a model in which group flourishing is evaluated in terms of group functional, where members bear roles and exhibit persistence conditions akin to biological systems or designed artifacts. This approach enables semantic interoperability for modeling longitudinal social contributions, allowing for structured reasoning about group welfare, social institutions, and group flourishing over time. 

**Abstract (ZH)**: 本文探讨了群体福祉的本体空间，提出了一种在本体工程背景下表示集体福利、群体功能和长期贡献的框架。传统福祉理论主要关注个体状态，通常依赖于享乐主义、欲望满足或目标清单模型。这些方法难以解释个人牺牲如何贡献于更广泛的社会进步，这是在建模群体繁荣时面临的关键挑战。为此，本文对“反事实账户”（CT）的福祉观进行了细化和拓展，通过将事件的好坏与其在附近可能世界中的假设对应者进行对比来评价事件。虽然这种方法在某些方面很有用，但它不足以应用于群体层面的本体模型，因为群体福祉依赖于功能的持续性、制度角色以及历史影响，而不是即时的个体结果。基于基本形式本体论（BFO），本文引入了一个模型，其中群体繁荣通过群体功能来评估，成员承担角色并表现出类似于生物系统或设计产品的一致性条件。这种方法为建模纵向社会贡献提供了语义互操作性，允许对群体福利、社会制度以及时间上的群体繁荣进行结构化的推理。 

---
# Automated decision-making for dynamic task assignment at scale 

**Title (ZH)**: 大规模动态任务自动分配决策 

**Authors**: Riccardo Lo Bianco, Willem van Jaarsveld, Jeroen Middelhuis, Luca Begnardi, Remco Dijkman  

**Link**: [PDF](https://arxiv.org/pdf/2504.19933)  

**Abstract**: The Dynamic Task Assignment Problem (DTAP) concerns matching resources to tasks in real time while minimizing some objectives, like resource costs or task cycle time. In this work, we consider a DTAP variant where every task is a case composed of a stochastic sequence of activities. The DTAP, in this case, involves the decision of which employee to assign to which activity to process requests as quickly as possible. In recent years, Deep Reinforcement Learning (DRL) has emerged as a promising tool for tackling this DTAP variant, but most research is limited to solving small-scale, synthetic problems, neglecting the challenges posed by real-world use cases. To bridge this gap, this work proposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS. To this end, we introduce a DRL agent with two novel elements: a graph structure for observations and actions that can effectively represent any DTAP and a reward function that is provably equivalent to the objective of minimizing the average cycle time of tasks. The combination of these two novelties allows the agent to learn effective and generalizable assignment policies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP instances whose parameters are extracted from real-world logs through process mining. The experimental evaluation shows how the proposed DRL agent matches or outperforms the best baseline in all DTAP instances and generalizes on different time horizons and across instances. 

**Abstract (ZH)**: 动态任务分配问题（DTAP）涉及在实时匹配资源以最小化某些目标（如资源成本或任务循环时间）。本文考虑的是每项任务由随机活动序列组成的DTAP变体，在这种情况下，DTAP涉及决定将哪个员工分配给哪个活动以尽可能快地处理请求。近年来，深度强化学习（DRL）已成为解决此DTAP变体的一种有前途的工具，但大多数研究仅限于解决小型合成问题，而忽视了实际应用中面临的挑战。为弥合这一差距，本文提出了一种基于DRL的决策支持系统（DSS）以应对大规模实际DTAP问题。为此，我们引入了一种具有两项新颖元素的DRL代理：一种能够有效表示任何DTAP的图结构的观察和操作，并且一种可以证明等价于最小化任务平均循环时间目标的奖励函数。这两种新颖性的结合使代理能够学习适用于大规模实际DTAP的有效且可泛化的分配策略。所提出的DSS在通过过程挖掘从真实日志中提取参数的五个DTAP实例上进行了评估。实证评估展示了所提出的DRL代理在所有DTAP实例中与最佳基线相匹配或超越最佳基线的能力，并且在不同时间周期和跨实例中具有泛化能力。 

---
# Learning Efficiency Meets Symmetry Breaking 

**Title (ZH)**: 学习效率与对称性破缺相遇 

**Authors**: Yingbin Bai, Sylvie Thiebaux, Felipe Trevizan  

**Link**: [PDF](https://arxiv.org/pdf/2504.19738)  

**Abstract**: Learning-based planners leveraging Graph Neural Networks can learn search guidance applicable to large search spaces, yet their potential to address symmetries remains largely unexplored. In this paper, we introduce a graph representation of planning problems allying learning efficiency with the ability to detect symmetries, along with two pruning methods, action pruning and state pruning, designed to manage symmetries during search. The integration of these techniques into Fast Downward achieves a first-time success over LAMA on the latest IPC learning track dataset. Code is released at: this https URL. 

**Abstract (ZH)**: 基于学习的规划器利用图神经网络可以学习适用于大型搜索空间的搜索指导，但其在处理对称性方面的潜力尚未充分探索。在本文中，我们引入了一种图表示方法来表示规划问题，结合了学习效率和检测对称性的能力，并提出了两种修剪方法——动作修剪和状态修剪，这些方法旨在搜索过程中管理对称性。将这些技术集成到Fast Downward中，在最新的IPC学习赛道数据集上实现了首次成功超越LAMA。代码已发布于：this https URL。 

---
# From Evidence to Belief: A Bayesian Epistemology Approach to Language Models 

**Title (ZH)**: 从证据到信念：一种基于贝叶斯 epistemology 的语言模型方法 

**Authors**: Minsu Kim, Sangryul Kim, James Thorne  

**Link**: [PDF](https://arxiv.org/pdf/2504.19622)  

**Abstract**: This paper investigates the knowledge of language models from the perspective of Bayesian epistemology. We explore how language models adjust their confidence and responses when presented with evidence with varying levels of informativeness and reliability. To study these properties, we create a dataset with various types of evidence and analyze language models' responses and confidence using verbalized confidence, token probability, and sampling. We observed that language models do not consistently follow Bayesian epistemology: language models follow the Bayesian confirmation assumption well with true evidence but fail to adhere to other Bayesian assumptions when encountering different evidence types. Also, we demonstrated that language models can exhibit high confidence when given strong evidence, but this does not always guarantee high accuracy. Our analysis also reveals that language models are biased toward golden evidence and show varying performance depending on the degree of irrelevance, helping explain why they deviate from Bayesian assumptions. 

**Abstract (ZH)**: 本文从贝叶斯认识论的角度探讨语言模型的知识。我们探讨了语言模型在面对具有不同信息量和可靠性的证据时如何调整其置信度和响应。为了研究这些性质，我们创建了一个包含不同类型证据的数据集，并使用口头化的置信度、令牌概率和采样来分析语言模型的响应和置信度。我们观察到，语言模型并不总是遵循贝叶斯认识论：当面对真实证据时，语言模型能很好地遵循贝叶斯确认假设，但在遇到不同类型的证据时未能遵守其他贝叶斯假设。此外，我们展示了在强证据下语言模型可以表现出高置信度，但这并不总是保证高精度。我们的分析还揭示了语言模型对金色证据的偏见以及其在无关程度不同下的表现差异，有助于解释它们为何偏离贝叶斯假设。 

---
# Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio Access Networks 

**Title (ZH)**: 基于图强化学习的QoS感知负载均衡在开放无线接入网络中 

**Authors**: Omid Semiari, Hosein Nikopour, Shilpa Talwar  

**Link**: [PDF](https://arxiv.org/pdf/2504.19499)  

**Abstract**: Next-generation wireless cellular networks are expected to provide unparalleled Quality-of-Service (QoS) for emerging wireless applications, necessitating strict performance guarantees, e.g., in terms of link-level data rates. A critical challenge in meeting these QoS requirements is the prevention of cell congestion, which involves balancing the load to ensure sufficient radio resources are available for each cell to serve its designated User Equipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approach is developed to optimize the performance of Guaranteed Bit Rate (GBR) and Best Effort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoS and resource constraints. The proposed solution builds on Graph Reinforcement Learning (GRL), a powerful framework at the intersection of Graph Neural Network (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process, with states represented as graphs. QoS consideration are integrated into both state representations and reward signal design. The LB agent is then trained using an off-policy dueling Deep Q Network (DQN) that leverages a GNN-based architecture. This design ensures the LB policy is invariant to the ordering of nodes (UE or cell), flexible in handling various network sizes, and capable of accounting for spatial node dependencies in LB decisions. Performance of the GRL-based solution is compared with two baseline methods. Results show substantial performance gains, including a $53\%$ reduction in QoS violations and a fourfold increase in the 5th percentile rate for BE traffic. 

**Abstract (ZH)**: 下一代无线蜂窝网络预期能为新兴无线应用提供无与伦比的服务质量（QoS），需要严格的服务性能保证，例如链路级别的数据速率。满足这些QoS要求的关键挑战之一是防止小区拥塞，这涉及到平衡负载以确保每个小区有足够的射频资源来服务于其指定的用户设备（UE）。在此工作中，提出了一种新的QoS感知负载均衡（LB）方法，以在服务质量（QoS）和资源约束条件下优化多频段开放无线接入网络（O-RAN）中保证比特速率（GBR）和尽力而为（BE）流量的性能。所提出的解决方案基于图强化学习（GRL），这是一种图神经网络（GNN）和强化学习（RL）交叉领域的强大框架。QoS感知的负载均衡被建模为马尔可夫决策过程，状态由图表示。将QoS考虑纳入状态表示和奖励信号设计中。然后，使用基于GNN架构的off-policy对偶Deep Q网络（DQN）来训练负载均衡代理。这种设计确保负载均衡策略对节点（UE或小区）的顺序不变，灵活处理各种网络规模，并能够考虑负载均衡决策中的空间节点依赖性。基于GRL的解决方案与两种基线方法进行性能比较。结果显示显著的性能改进，包括QoS违约减少53%，以及BE流量的第5百分位速率提高了四倍。 

---
# Neurosymbolic Association Rule Mining from Tabular Data 

**Title (ZH)**: 神经符号化表数据关联规则挖掘 

**Authors**: Erkan Karabulut, Paul Groth, Victoria Degeler  

**Link**: [PDF](https://arxiv.org/pdf/2504.19354)  

**Abstract**: Association Rule Mining (ARM) is the task of mining patterns among data features in the form of logical rules, with applications across a myriad of domains. However, high-dimensional datasets often result in an excessive number of rules, increasing execution time and negatively impacting downstream task performance. Managing this rule explosion remains a central challenge in ARM research. To address this, we introduce Aerial+, a novel neurosymbolic ARM method. Aerial+ leverages an under-complete autoencoder to create a neural representation of the data, capturing associations between features. It extracts rules from this neural representation by exploiting the model's reconstruction mechanism. Extensive evaluations on five datasets against seven baselines demonstrate that Aerial+ achieves state-of-the-art results by learning more concise, high-quality rule sets with full data coverage. When integrated into rule-based interpretable machine learning models, Aerial+ significantly reduces execution time while maintaining or improving accuracy. 

**Abstract (ZH)**: 基于神经符号的关联规则挖掘方法Aerial+ 

---
# Logic-Based Artificial Intelligence Algorithms Supporting Categorical Semantics 

**Title (ZH)**: 基于逻辑的人工智能算法支持范畴语义学 

**Authors**: Ralph Wojtowicz  

**Link**: [PDF](https://arxiv.org/pdf/2504.19320)  

**Abstract**: This paper seeks to apply categorical logic to the design of artificial intelligent agents that reason symbolically about objects more richly structured than sets. Using Johnstone's sequent calculus of terms- and formulae-in-context, we develop forward chaining and normal form algorithms for reasoning about objects in cartesian categories with the rules for Horn logic. We also adapt first-order unification to support multi-sorted theories, contexts, and fragments of first-order logic. The significance of these reformulations rests in the fact that they can be applied to reasoning about objects in semantic categories that do not support classical logic or even all its connectives. 

**Abstract (ZH)**: 本文旨在将范畴逻辑应用于设计能够更丰富地对结构化对象进行符号推理的人工智能代理。借助约翰斯顿的带上下文的项和公式结构的测序演算，我们发展了应用于笛卡尔范畴的高斯逻辑规则的前向链接和规范形式推理算法。我们还将一阶统一化适配于多类型理论、上下文和一阶逻辑片段。这些重构的意义在于，它们可以应用于不支持经典逻辑或甚至所有联结词的语义范畴对象的推理。 

---
# A Design Framework for operationalizing Trustworthy Artificial Intelligence in Healthcare: Requirements, Tradeoffs and Challenges for its Clinical Adoption 

**Title (ZH)**: 面向医疗领域可信人工智能落地的设计框架：临床应用中的需求、权衡与挑战 

**Authors**: Pedro A. Moreno-Sánchez, Javier Del Ser, Mark van Gils, Jussi Hernesniemi  

**Link**: [PDF](https://arxiv.org/pdf/2504.19179)  

**Abstract**: Artificial Intelligence (AI) holds great promise for transforming healthcare, particularly in disease diagnosis, prognosis, and patient care. The increasing availability of digital medical data, such as images, omics, biosignals, and electronic health records, combined with advances in computing, has enabled AI models to approach expert-level performance. However, widespread clinical adoption remains limited, primarily due to challenges beyond technical performance, including ethical concerns, regulatory barriers, and lack of trust. To address these issues, AI systems must align with the principles of Trustworthy AI (TAI), which emphasize human agency and oversight, algorithmic robustness, privacy and data governance, transparency, bias and discrimination avoidance, and accountability. Yet, the complexity of healthcare processes (e.g., screening, diagnosis, prognosis, and treatment) and the diversity of stakeholders (clinicians, patients, providers, regulators) complicate the integration of TAI principles. To bridge the gap between TAI theory and practical implementation, this paper proposes a design framework to support developers in embedding TAI principles into medical AI systems. Thus, for each stakeholder identified across various healthcare processes, we propose a disease-agnostic collection of requirements that medical AI systems should incorporate to adhere to the principles of TAI. Additionally, we examine the challenges and tradeoffs that may arise when applying these principles in practice. To ground the discussion, we focus on cardiovascular diseases, a field marked by both high prevalence and active AI innovation, and demonstrate how TAI principles have been applied and where key obstacles persist. 

**Abstract (ZH)**: 人工智能（AI）在医疗健康领域的应用前景广阔，尤其是在疾病诊断、预后和患者护理方面。随着数字医疗数据（如影像、组学、生物信号和电子健康记录）的日益可用以及计算能力的提升，AI模型已经能够接近专家级表现。然而，临床广泛应用仍受到技术性能之外的挑战限制，包括伦理问题、监管障碍和信任缺失。为了解决这些问题，AI系统必须符合可信赖人工智能（Trustworthy AI，TAI）的原则，这些原则强调人类自主权和监督、算法稳健性、隐私和数据治理、透明度、偏见和歧视规避以及问责制。然而，医疗保健过程的复杂性（如筛查、诊断、预后和治疗）以及利益相关者多样性（包括临床医生、患者、提供者和监管机构）为TAI原则的应用带来了复杂性。为了弥合TAI理论与实际应用之间的差距，本文提出了一种设计框架，以支持开发人员将TAI原则嵌入医疗AI系统中。通过识别各种医疗保健过程中涉及的不同利益相关者，我们提出了一组与TAI原则相符的疾病通用要求，这些要求应被医疗AI系统所采纳。此外，我们探讨了在实践中应用这些原则时可能遇到的挑战和权衡。为了支撑讨论，我们聚焦于心血管疾病这一高发且活跃的AI创新领域，展示TAI原则的应用情况以及关键障碍所在。 

---
# A Dynamic Fuzzy Rule and Attribute Management Framework for Fuzzy Inference Systems in High-Dimensional Data 

**Title (ZH)**: 高维数据中模糊推理系统中动态模糊规则和属性管理框架 

**Authors**: Ke Liu, Jing Ma, Edmund M-K Lai  

**Link**: [PDF](https://arxiv.org/pdf/2504.19148)  

**Abstract**: This paper presents an Adaptive Dynamic Attribute and Rule (ADAR) framework designed to address the challenges posed by high-dimensional data in neuro-fuzzy inference systems. By integrating dual weighting mechanisms-assigning adaptive importance to both attributes and rules-together with automated growth and pruning strategies, ADAR adaptively streamlines complex fuzzy models without sacrificing performance or interpretability. Experimental evaluations on four diverse datasets - Auto MPG (7 variables), Beijing PM2.5 (10 variables), Boston Housing (13 variables), and Appliances Energy Consumption (27 variables) show that ADAR-based models achieve consistently lower Root Mean Square Error (RMSE) compared to state-of-the-art baselines. On the Beijing PM2.5 dataset, for instance, ADAR-SOFENN attained an RMSE of 56.87 with nine rules, surpassing traditional ANFIS [12] and SOFENN [16] models. Similarly, on the high-dimensional Appliances Energy dataset, ADAR-ANFIS reached an RMSE of 83.25 with nine rules, outperforming established fuzzy logic approaches and interpretability-focused methods such as APLR. Ablation studies further reveal that combining rule-level and attribute-level weight assignment significantly reduces model overlap while preserving essential features, thereby enhancing explainability. These results highlight ADAR's effectiveness in dynamically balancing rule complexity and feature importance, paving the way for scalable, high-accuracy, and transparent neuro-fuzzy systems applicable to a range of real-world scenarios. 

**Abstract (ZH)**: 一种适应性动态属性和规则（ADAR）框架：应对神经模糊推理系统中高维数据挑战 

---
# DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning 

**Title (ZH)**: DiCE扩展：机器学习中稳健的反事实解释方法 

**Authors**: Volkan Bakir, Polat Goktas, Sureyya Akyuz  

**Link**: [PDF](https://arxiv.org/pdf/2504.19027)  

**Abstract**: Explainable artificial intelligence (XAI) has become increasingly important in decision-critical domains such as healthcare, finance, and law. Counterfactual (CF) explanations, a key approach in XAI, provide users with actionable insights by suggesting minimal modifications to input features that lead to different model outcomes. Despite significant advancements, existing CF generation methods often struggle to balance proximity, diversity, and robustness, limiting their real-world applicability. A widely adopted framework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but lacks robustness, making CF explanations sensitive to perturbations and domain constraints. To address these challenges, we introduce DiCE-Extended, an enhanced CF explanation framework that integrates multi-objective optimization techniques to improve robustness while maintaining interpretability. Our approach introduces a novel robustness metric based on the Dice-Sorensen coefficient, ensuring stability under small input variations. Additionally, we refine CF generation using weighted loss components (lambda_p, lambda_d, lambda_r) to balance proximity, diversity, and robustness. We empirically validate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German Credit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch, TensorFlow). Results demonstrate improved CF validity, stability, and alignment with decision boundaries compared to standard DiCE-generated explanations. Our findings highlight the potential of DiCE-Extended in generating more reliable and interpretable CFs for high-stakes applications. Future work will explore adaptive optimization techniques and domain-specific constraints to further enhance CF generation in real-world scenarios. 

**Abstract (ZH)**: 可解释的人工智能(XAI)在医疗、金融和法律等决策关键领域变得日益重要。反事实(CF)解释作为XAI的关键方法，通过建议小幅度修改输入特征以导致不同模型结果，为用户提供实用洞察。尽管取得显著进展，现有CF生成方法往往难以平衡接近性、多样性和鲁棒性，限制了其实际应用。一种广泛采用的框架Diverse Counterfactual Explanations (DiCE) 强调多样性但缺乏鲁棒性，使得CF解释对扰动和领域约束敏感。为解决这些挑战，我们引入了DiCE-Extended，这是一种增强的CF解释框架，结合多目标优化技术以提高鲁棒性同时保持可解释性。我们的方法引入了基于Dice-Sorensen系数的新鲁棒性度量，确保在小输入变化下稳定性。此外，我们通过加权损失组件（lambda_p, lambda_d, lambda_r）细化CF生成，以平衡接近性、多样性和鲁棒性。我们在Benchmark数据集（COMPAS、Lending Club、German Credit、Adult Income）和多个ML后端（Scikit-learn、PyTorch、TensorFlow）上进行了实证验证。结果表明，与标准DiCE生成的解释相比，DiCE-Extended在CF有效性、稳定性和与决策边界对齐方面有所改进。我们的发现突显了DiCE-Extended在生成更可靠和可解释的CF以应对高风险应用方面的潜力。未来的工作将探索适应性优化技术和领域特定约束，进一步增强实际场景中的CF生成。 

---
# Use of Metric Learning for the Recognition of Handwritten Digits, and its Application to Increase the Outreach of Voice-based Communication Platforms 

**Title (ZH)**: 基于度量学习的手写数字识别及其在扩展基于语音的通信平台影响力中的应用 

**Authors**: Devesh Pant, Dibyendu Talukder, Deepak Kumar, Rachit Pandey, Aaditeshwar Seth, Chetan Arora  

**Link**: [PDF](https://arxiv.org/pdf/2504.18948)  

**Abstract**: Initiation, monitoring, and evaluation of development programmes can involve field-based data collection about project activities. This data collection through digital devices may not always be feasible though, for reasons such as unaffordability of smartphones and tablets by field-based cadre, or shortfalls in their training and capacity building. Paper-based data collection has been argued to be more appropriate in several contexts, with automated digitization of the paper forms through OCR (Optical Character Recognition) and OMR (Optical Mark Recognition) techniques. We contribute with providing a large dataset of handwritten digits, and deep learning based models and methods built using this data, that are effective in real-world environments. We demonstrate the deployment of these tools in the context of a maternal and child health and nutrition awareness project, which uses IVR (Interactive Voice Response) systems to provide awareness information to rural women SHG (Self Help Group) members in north India. Paper forms were used to collect phone numbers of the SHG members at scale, which were digitized using the OCR tools developed by us, and used to push almost 4 million phone calls. The data, model, and code have been released in the open-source domain. 

**Abstract (ZH)**: 基于 handwritten digits 的数据集及深度学习模型在实地项目监测评估中的应用：以印度北方农村妇女自我帮助组健康与营养意识项目为例 

---
# Evaluating AI-Driven Automated Map Digitization in QGIS 

**Title (ZH)**: 基于QGIS的AI驱动自动化地图数字化评价 

**Authors**: Diana Febrita  

**Link**: [PDF](https://arxiv.org/pdf/2504.18777)  

**Abstract**: Map digitization is an important process that converts maps into digital formats that can be used for further analysis. This process typically requires a deep human involvement because of the need for interpretation and decision-making when translating complex features. With the advancement of artificial intelligence, there is an alternative to conducting map digitization with the help of machine learning techniques. Deepness, or Deep Neural Remote Sensing, is an advanced AI-driven tool designed and integrated as a plugin in QGIS application. This research focuses on assessing the effectiveness of Deepness in automated digitization. This study analyses AI-generated digitization results from Google Earth imagery and compares them with digitized outputs from OpenStreetMap (OSM) to evaluate performance. 

**Abstract (ZH)**: 地图数字化是将地图转换为可用于进一步分析的数字格式的重要过程。随着人工智能的发展，可以利用机器学习技术来替代人工进行地图数字化。Deepness，或深度神经遥感，是一种先进的AI驱动工具，被设计并集成在QGIS应用程序中作为插件。本研究旨在评估Deepness在自动化数字化中的有效性。本研究分析了从Google Earth影像生成的AI数字化结果，并与OpenStreetMap (OSM)的数字化输出进行比较，以评估其性能。 

---
# Transformational Creativity in Science: A Graphical Theory 

**Title (ZH)**: 科学中的转移性创造力：一种图形理论 

**Authors**: Samuel Schapiro, Jonah Black, Lav R. Varshney  

**Link**: [PDF](https://arxiv.org/pdf/2504.18687)  

**Abstract**: Creative processes are typically divided into three types: combinatorial, exploratory, and transformational. Here, we provide a graphical theory of transformational scientific creativity, synthesizing Boden's insight that transformational creativity arises from changes in the "enabling constraints" of a conceptual space and Kuhn's structure of scientific revolutions as resulting from paradigm shifts. We prove that modifications made to axioms of our graphical model have the most transformative potential and then illustrate how several historical instances of transformational creativity can be captured by our framework. 

**Abstract (ZH)**: 创造过程通常分为三种类型：组合型、探索型和转化型。本文提供了转化型科学研究创造力的图形理论，综合了博登关于转化型创造力来源于概念空间“使能约束”的变化的见解以及库恩关于科学发展革命结构源自范式转变的理论。我们证明了对图形模型公理所做的修改具有最大的转化潜力，然后展示了如何通过我们的框架捕捉到多个历史上的转化型创造力实例。 

---
# Research on Personalized Medical Intervention Strategy Generation System based on Group Relative Policy Optimization and Time-Series Data Fusion 

**Title (ZH)**: 基于组相对策略优化和时间序列数据融合的个性化医疗干预策略生成系统研究 

**Authors**: Dingxin Lu, Shurui Wu, Xinyi Huang  

**Link**: [PDF](https://arxiv.org/pdf/2504.18631)  

**Abstract**: With the timely formation of personalized intervention plans based on high-dimensional heterogeneous time series information becoming an important challenge in the medical field today, electronic medical records, wearables, and other multi-source medical data are increasingly generated and diversified. In this work, we develop a system to generate personalized medical intervention strategies based on Group Relative Policy Optimization (GRPO) and Time-Series Data Fusion. First, by incorporating relative policy constraints among the groups during policy gradient updates, we adaptively balance individual and group gains. To improve the robustness and interpretability of decision-making, a multi-layer neural network structure is employed to group-code patient characteristics. Second, for the rapid multi-modal fusion of multi-source heterogeneous time series, a multi-channel neural network combined with a self-attention mechanism is used for dynamic feature extraction. Key feature screening and aggregation are achieved through a differentiable gating network. Finally, a collaborative search process combining a genetic algorithm and Monte Carlo tree search is proposed to find the ideal intervention strategy, achieving global optimization. Experimental results show significant improvements in accuracy, coverage, and decision-making benefits compared with existing methods. 

**Abstract (ZH)**: 基于Group Relative Policy Optimization和时间序列数据融合的个性化医疗干预策略生成系统 

---
# A Cognitive-Mechanistic Human Reliability Analysis Framework: A Nuclear Power Plant Case Study 

**Title (ZH)**: 一种认知-机制人类可靠性分析框架：以核电厂为例 

**Authors**: Xingyu Xiao, Peng Chen, Jiejuan Tong, Shunshun Liu, Hongru Zhao, Jun Zhao, Qianqian Jia, Jingang Liang, Haitao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2504.18604)  

**Abstract**: Traditional human reliability analysis (HRA) methods, such as IDHEAS-ECA, rely on expert judgment and empirical rules that often overlook the cognitive underpinnings of human error. Moreover, conducting human-in-the-loop experiments for advanced nuclear power plants is increasingly impractical due to novel interfaces and limited operational data. This study proposes a cognitive-mechanistic framework (COGMIF) that enhances the IDHEAS-ECA methodology by integrating an ACT-R-based human digital twin (HDT) with TimeGAN-augmented simulation. The ACT-R model simulates operator cognition, including memory retrieval, goal-directed procedural reasoning, and perceptual-motor execution, under high-fidelity scenarios derived from a high-temperature gas-cooled reactor (HTGR) simulator. To overcome the resource constraints of large-scale cognitive modeling, TimeGAN is trained on ACT-R-generated time-series data to produce high-fidelity synthetic operator behavior datasets. These simulations are then used to drive IDHEAS-ECA assessments, enabling scalable, mechanism-informed estimation of human error probabilities (HEPs). Comparative analyses with SPAR-H and sensitivity assessments demonstrate the robustness and practical advantages of the proposed COGMIF. Finally, procedural features are mapped onto a Bayesian network to quantify the influence of contributing factors, revealing key drivers of operational risk. This work offers a credible and computationally efficient pathway to integrate cognitive theory into industrial HRA practices. 

**Abstract (ZH)**: 基于认知机制的认知数字孪生框架（COGMIF）在传统人因可靠性分析（HRA）中的应用 

---
# QuantBench: Benchmarking AI Methods for Quantitative Investment 

**Title (ZH)**: QuantBench: 量化投资中人工智能方法的基准测试 

**Authors**: Saizhuo Wang, Hao Kong, Jiadong Guo, Fengrui Hua, Yiyan Qi, Wanyun Zhou, Jiahao Zheng, Xinyu Wang, Lionel M. Ni, Jian Guo  

**Link**: [PDF](https://arxiv.org/pdf/2504.18600)  

**Abstract**: The field of artificial intelligence (AI) in quantitative investment has seen significant advancements, yet it lacks a standardized benchmark aligned with industry practices. This gap hinders research progress and limits the practical application of academic innovations. We present QuantBench, an industrial-grade benchmark platform designed to address this critical need. QuantBench offers three key strengths: (1) standardization that aligns with quantitative investment industry practices, (2) flexibility to integrate various AI algorithms, and (3) full-pipeline coverage of the entire quantitative investment process. Our empirical studies using QuantBench reveal some critical research directions, including the need for continual learning to address distribution shifts, improved methods for modeling relational financial data, and more robust approaches to mitigate overfitting in low signal-to-noise environments. By providing a common ground for evaluation and fostering collaboration between researchers and practitioners, QuantBench aims to accelerate progress in AI for quantitative investment, similar to the impact of benchmark platforms in computer vision and natural language processing. 

**Abstract (ZH)**: 人工智能在量化投资领域的标准化基准平台：QuantBench的研究与实践 

---
# MINT: Multi-Vector Search Index Tuning 

**Title (ZH)**: MINT: 多向量搜索索引调优 

**Authors**: Jiongli Zhu, Yue Wang, Bailu Ding, Philip A. Bernstein, Vivek Narasayya, Surajit Chaudhuri  

**Link**: [PDF](https://arxiv.org/pdf/2504.20018)  

**Abstract**: Vector search plays a crucial role in many real-world applications. In addition to single-vector search, multi-vector search becomes important for multi-modal and multi-feature scenarios today. In a multi-vector database, each row is an item, each column represents a feature of items, and each cell is a high-dimensional vector. In multi-vector databases, the choice of indexes can have a significant impact on performance. Although index tuning for relational databases has been extensively studied, index tuning for multi-vector search remains unclear and challenging. In this paper, we define multi-vector search index tuning and propose a framework to solve it. Specifically, given a multi-vector search workload, we develop algorithms to find indexes that minimize latency and meet storage and recall constraints. Compared to the baseline, our latency achieves 2.1X to 8.3X speedup. 

**Abstract (ZH)**: 向量搜索在许多实际应用中发挥着 crucial 作用。除了单向量搜索，多向量搜索今天在多模态和多特征场景中变得尤为重要。在多向量数据库中，每一行是一个项目，每一列代表项目的特征，每个单元格是一个高维向量。在多向量数据库中，索引的选择对性能有显著影响。尽管关系数据库的索引调整已经被广泛研究，但多向量搜索的索引调整仍不清楚且具有挑战性。在本文中，我们定义了多向量搜索索引调整，并提出了一种解决该问题的框架。具体来说，给定一个多向量搜索工作负载，我们开发了算法来找到能够最小化延迟并满足存储和召回约束的索引。与基线相比，我们的延迟实现了 2.1 倍至 8.3 倍的加速。 

---
# Simplified and Secure MCP Gateways for Enterprise AI Integration 

**Title (ZH)**: 简化且安全的企业AI集成MCP网关 

**Authors**: Ivo Brett  

**Link**: [PDF](https://arxiv.org/pdf/2504.19997)  

**Abstract**: The increased adoption of the Model Context Protocol (MCP) for AI Agents necessitates robust security for Enterprise integrations. This paper introduces the MCP Gateway to simplify self-hosted MCP server integration. The proposed architecture integrates security principles, authentication, intrusion detection, and secure tunneling, enabling secure self-hosting without exposing infrastructure. Key contributions include a reference architecture, threat model mapping, simplified integration strategies, and open-source implementation recommendations. This work focuses on the unique challenges of enterprise-centric, self-hosted AI integrations, unlike existing public MCP server solutions. 

**Abstract (ZH)**: MCP网关：简化企业自托管MCP服务器集成的安全解决方案 

---
# Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery 

**Title (ZH)**: 使用Sentinel-2卫星影像监测消化液在农业作物中的应用 

**Authors**: Andreas Kalogeras, Dimitrios Bormpoudakis, Iason Tsardanidis, Dimitra A. Loka, Charalampos Kontoes  

**Link**: [PDF](https://arxiv.org/pdf/2504.19996)  

**Abstract**: The widespread use of Exogenous Organic Matter in agriculture necessitates monitoring to assess its effects on soil and crop health. This study evaluates optical Sentinel-2 satellite imagery for detecting digestate application, a practice that enhances soil fertility but poses environmental risks like microplastic contamination and nitrogen losses. In the first instance, Sentinel-2 satellite image time series (SITS) analysis of specific indices (EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after application on the soils of four different crop types in Thessaly, Greece. Furthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient Boosting and a Feed-Forward Neural Network), were used to investigate digestate presence detection, achieving F1-scores up to 0.85. The findings highlight the potential of combining remote sensing and ML for scalable and cost-effective monitoring of EOM applications, supporting precision agriculture and sustainability. 

**Abstract (ZH)**: 外源有机物在农业生产中的广泛应用要求对其进行监测以评估其对土壤和作物健康的影响。本研究利用光学Sentinel-2卫星图像评估消化物施用的检测，该实践虽能提高土壤肥力但可能带来微塑料污染和氮流失等环境风险。首先，通过对四个不同作物类型在希腊色萨利地区土壤上施用外源有机物后的特定指数（EOMI、NDVI、EVI）的时间系列Sentinel-2卫星图像分析，来表征外源有机物的光谱行为。此外，使用机器学习模型（包括随机森林、k-近邻、梯度提升和前馈神经网络），对消化物存在检测进行了研究，取得了高达0.85的F1分数。研究结果强调了将遥感技术和机器学习结合用于外源有机物应用的大规模和低成本监测的潜力，支持精准农业和可持续发展。 

---
# Mitigating Societal Cognitive Overload in the Age of AI: Challenges and Directions 

**Title (ZH)**: 在人工智能时代缓解社会认知过载：挑战与方向 

**Authors**: Salem Lahlou  

**Link**: [PDF](https://arxiv.org/pdf/2504.19990)  

**Abstract**: Societal cognitive overload, driven by the deluge of information and complexity in the AI age, poses a critical challenge to human well-being and societal resilience. This paper argues that mitigating cognitive overload is not only essential for improving present-day life but also a crucial prerequisite for navigating the potential risks of advanced AI, including existential threats. We examine how AI exacerbates cognitive overload through various mechanisms, including information proliferation, algorithmic manipulation, automation anxieties, deregulation, and the erosion of meaning. The paper reframes the AI safety debate to center on cognitive overload, highlighting its role as a bridge between near-term harms and long-term risks. It concludes by discussing potential institutional adaptations, research directions, and policy considerations that arise from adopting an overload-resilient perspective on human-AI alignment, suggesting pathways for future exploration rather than prescribing definitive solutions. 

**Abstract (ZH)**: AI时代的社会认知过载：对人类福祉和社会韧性的影响及应对策略 

---
# TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons 

**Title (ZH)**: TD-EVAL: 重新考察结合回合级精准度与对话级比较的任务导向对话评估 

**Authors**: Emre Can Acikgoz, Carl Guo, Suvodip Dey, Akul Datta, Takyoung Kim, Gokhan Tur, Dilek Hakkani-Tür  

**Link**: [PDF](https://arxiv.org/pdf/2504.19982)  

**Abstract**: Task-oriented dialogue (TOD) systems are experiencing a revolution driven by Large Language Models (LLMs), yet the evaluation methodologies for these systems remain insufficient for their growing sophistication. While traditional automatic metrics effectively assessed earlier modular systems, they focus solely on the dialogue level and cannot detect critical intermediate errors that can arise during user-agent interactions. In this paper, we introduce TD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework that unifies fine-grained turn-level analysis with holistic dialogue-level comparisons. At turn level, we evaluate each response along three TOD-specific dimensions: conversation cohesion, backend knowledge consistency, and policy compliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons to provide a measure of dialogue-level quality. Through experiments on MultiWOZ 2.4 and {\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the conversational errors that conventional metrics miss. Furthermore, TD-EVAL exhibits better alignment with human judgments than traditional and LLM-based metrics. These findings demonstrate that TD-EVAL introduces a new paradigm for TOD system evaluation, efficiently assessing both turn and system levels with a plug-and-play framework for future research. 

**Abstract (ZH)**: 基于任务的对话（TOD）系统正经历由大规模语言模型（LLMs）驱动的革命，然而这些系统日益复杂的评估方法仍显不足。虽然传统的自动评估指标有效评估了早期模块化系统，但它们仅关注对话层面，无法检测用户-代理交互过程中可能出现的关键中间错误。在本文中，我们提出了TD-EVAL（回合级和对话级评估）这一两步评估框架，该框架将细粒度的回合级分析与整体对话级比较统一起来。在回合级，我们根据对话一致性、后端知识一致性以及策略合规性这三大TOD特定维度评估每个响应。同时，我们设计了TOD Agent Arena，通过成对比较提供对话级质量的衡量。通过在MultiWOZ 2.4和τ-Bench上的实验，我们证明TD-EVAL能够有效识别传统指标未能捕捉到的对话错误。此外，TD-EVAL在衡量对话质量方面与人类判断的契合度优于传统和基于LLM的指标。这些发现表明TD-EVAL引入了TOD系统评估的新范式，高效地评估回合和系统层面，并为未来研究提供了一个即插即用的框架。 

---
# Enhancing short-term traffic prediction by integrating trends and fluctuations with attention mechanism 

**Title (ZH)**: 基于注意力机制整合趋势与波动的短期交通预测增强 

**Authors**: Adway Das, Agnimitra Sengupta, S. Ilgin Guler  

**Link**: [PDF](https://arxiv.org/pdf/2504.19967)  

**Abstract**: Traffic flow prediction is a critical component of intelligent transportation systems, yet accurately forecasting traffic remains challenging due to the interaction between long-term trends and short-term fluctuations. Standard deep learning models often struggle with these challenges because their architectures inherently smooth over fine-grained fluctuations while focusing on general trends. This limitation arises from low-pass filtering effects, gate biases favoring stability, and memory update mechanisms that prioritize long-term information retention. To address these shortcomings, this study introduces a hybrid deep learning framework that integrates both long-term trend and short-term fluctuation information using two input features processed in parallel, designed to capture complementary aspects of traffic flow dynamics. Further, our approach leverages attention mechanisms, specifically Bahdanau attention, to selectively focus on critical time steps within traffic data, enhancing the model's ability to predict congestion and other transient phenomena. Experimental results demonstrate that features learned from both branches are complementary, significantly improving the goodness-of-fit statistics across multiple prediction horizons compared to a baseline model. Notably, the attention mechanism enhances short-term forecast accuracy by directly targeting immediate fluctuations, though challenges remain in fully integrating long-term trends. This framework can contribute to more effective congestion mitigation and urban mobility planning by advancing the robustness and precision of traffic prediction models. 

**Abstract (ZH)**: 基于长短期信息融合的注意力机制辅助交通流预测框架 

---
# Capturing Aerodynamic Characteristics of ATTAS Aircraft with Evolving Intelligent System 

**Title (ZH)**: 基于演化智能系统的ATTAS飞机 aerodynamic 特征捕获 

**Authors**: Aydoğan Soylu, Tufan Kumbasar  

**Link**: [PDF](https://arxiv.org/pdf/2504.19949)  

**Abstract**: Accurate modeling of aerodynamic coefficients is crucial for understanding and optimizing the performance of modern aircraft systems. This paper presents the novel deployment of an Evolving Type-2 Quantum Fuzzy Neural Network (eT2QFNN) for modeling the aerodynamic coefficients of the ATTAS aircraft to express the aerodynamic characteristics. eT2QFNN can represent the nonlinear aircraft model by creating multiple linear submodels with its rule-based structure through an incremental learning strategy rather than a traditional batch learning approach. Moreover, it enhances robustness to uncertainties and data noise through its quantum membership functions, as well as its automatic rule-learning and parameter-tuning capabilities. During the estimation of the aerodynamic coefficients via the flight data of the ATTAS, two different studies are conducted in the training phase: one with a large amount of data and the other with a limited amount of data. The results show that the modeling performance of the eT2QFNN is superior in comparison to baseline counterparts. Furthermore, eT2QFNN estimated the aerodynamic model with fewer rules compared to Type-1 fuzzy counterparts. In addition, by applying the Delta method to the proposed approach, the stability and control derivatives of the aircraft are analyzed. The results prove the superiority of the proposed eT2QFNN in representing aerodynamic coefficients. 

**Abstract (ZH)**: 基于eT2QFNN的ATTAS飞机气动系数建模研究 

---
# Probabilistic and Causal Satisfiability: Constraining the Model 

**Title (ZH)**: 概率性和因果满足性：约束模型 

**Authors**: Markus Bläser, Julian Dörfler, Maciej Liśkiewicz, Benito van der Zander  

**Link**: [PDF](https://arxiv.org/pdf/2504.19944)  

**Abstract**: We study the complexity of satisfiability problems in probabilistic and causal reasoning. Given random variables $X_1, X_2,\ldots$ over finite domains, the basic terms are probabilities of propositional formulas over atomic events $X_i = x_i$, such as $P(X_1 = x_1)$ or $P(X_1 = x_1 \vee X_2 = x_2)$. The basic terms can be combined using addition (yielding linear terms) or multiplication (polynomial terms). The probabilistic satisfiability problem asks whether a joint probability distribution satisfies a Boolean combination of (in)equalities over such terms. Fagin et al. (1990) showed that for basic and linear terms, this problem is NP-complete, making it no harder than Boolean satisfiability, while Mossé et al. (2022) proved that for polynomial terms, it is complete for the existential theory of the reals.
Pearl's Causal Hierarchy (PCH) extends the probabilistic setting with interventional and counterfactual reasoning, enriching the expressiveness of languages. However, Mossé et al. (2022) found that satisfiability complexity remains unchanged. Van der Zander et al. (2023) showed that introducing a marginalization operator to languages induces a significant increase in complexity.
We extend this line of work by adding two new dimensions to the problem by constraining the models. First, we fix the graph structure of the underlying structural causal model, motivated by settings like Pearl's do-calculus, and give a nearly complete landscape across different arithmetics and PCH levels. Second, we study small models. While earlier work showed that satisfiable instances admit polynomial-size models, this is no longer guaranteed with compact marginalization. We characterize the complexities of satisfiability under small-model constraints across different settings. 

**Abstract (ZH)**: 我们研究概率性和因果推理中的可满足性问题复杂性。给定有限域上的随机变量$X_1, X_2,\ldots$，基本项是原子事件$X_i = x_i$的命题公式概率，如$P(X_1 = x_1)$或$P(X_1 = x_1 \vee X_2 = x_2)$。这些基本项可以使用加法（产生线性项）或乘法（产生多项式项）组合。概率性可满足性问题是询问一个联合概率分布是否满足这种项的（不）等式的布尔组合。Fagin等（1990）证明了对于基本和线性项，该问题是NP完全的，其复杂性不超过布尔可满足性问题；而Mossé等（2022）证明了对于多项式项，该问题等价于实数的存在性理论完备问题。
Pearl的因果层次（PCH）扩展了概率性设置，加入了干预性和反事实推理，丰富了语言的表达力。然而，Mossé等（2022）发现可满足性复杂性没有变化。Van der Zander等（2023）展示了引入边缘化操作符使语言复杂性显著增加。
我们在前人的研究基础上，通过约束模型添加了两个新维度。首先，我们固定了基础结构因果模型的图结构，受Pearl的do-因果记号的启发，对不同算术和PCH级别给出了几乎完整的情景。其次，我们研究了小模型。虽然早期的工作表明可满足实例存在多项式大小模型，但由于紧凑的边缘化，这已不再保证。我们研究了不同情境下，在小模型约束下的可满足性复杂性。 

---
# Attention Mechanism, Max-Affine Partition, and Universal Approximation 

**Title (ZH)**: 注意力机制、最大-affine 分区与通用逼近能力 

**Authors**: Hude Liu, Jerry Yao-Chieh Hu, Zhao Song, Han Liu  

**Link**: [PDF](https://arxiv.org/pdf/2504.19901)  

**Abstract**: We establish the universal approximation capability of single-layer, single-head self- and cross-attention mechanisms with minimal attached structures. Our key insight is to interpret single-head attention as an input domain-partition mechanism that assigns distinct values to subregions. This allows us to engineer the attention weights such that this assignment imitates the target function. Building on this, we prove that a single self-attention layer, preceded by sum-of-linear transformations, is capable of approximating any continuous function on a compact domain under the $L_\infty$-norm. Furthermore, we extend this construction to approximate any Lebesgue integrable function under $L_p$-norm for $1\leq p <\infty$. Lastly, we also extend our techniques and show that, for the first time, single-head cross-attention achieves the same universal approximation guarantees. 

**Abstract (ZH)**: 我们证明了带有最少附加结构的单层、单头自注意力和交叉注意力机制的通用近似能力。我们的关键洞察是将单头注意力视为一种输入域分区机制，为子区域分配不同的值。这使得我们可以设计注意力权重，使这种分配模仿目标函数。在此基础上，我们证明，在$L_\infty$范数下，单一自注意力层，前置线性变换之和，能够在紧致域上近似任意连续函数。此外，我们将这种构造扩展到在$L_p$范数下（对于$1\leq p <\infty$）近似任意勒贝格可积函数。最后，我们还扩展了这些技术，首次证明单头交叉注意力也能够获得相同的通用近似保证。 

---
# TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate 

**Title (ZH)**: TurboQuant: 临界在线向量量化与近最优失真率 

**Authors**: Amir Zandieh, Majid Daliri, Majid Hadian, Vahab Mirrokni  

**Link**: [PDF](https://arxiv.org/pdf/2504.19874)  

**Abstract**: Vector quantization, a problem rooted in Shannon's source coding theory, aims to quantize high-dimensional Euclidean vectors while minimizing distortion in their geometric structure. We propose TurboQuant to address both mean-squared error (MSE) and inner product distortion, overcoming limitations of existing methods that fail to achieve optimal distortion rates. Our data-oblivious algorithms, suitable for online applications, achieve near-optimal distortion rates (within a small constant factor) across all bit-widths and dimensions. TurboQuant achieves this by randomly rotating input vectors, inducing a concentrated Beta distribution on coordinates, and leveraging the near-independence property of distinct coordinates in high dimensions to simply apply optimal scalar quantizers per each coordinate. Recognizing that MSE-optimal quantizers introduce bias in inner product estimation, we propose a two-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL (QJL) transform on the residual, resulting in an unbiased inner product quantizer. We also provide a formal proof of the information-theoretic lower bounds on best achievable distortion rate by any vector quantizer, demonstrating that TurboQuant closely matches these bounds, differing only by a small constant ($\approx 2.7$) factor. Experimental results validate our theoretical findings, showing that for KV cache quantization, we achieve absolute quality neutrality with 3.5 bits per channel and marginal quality degradation with 2.5 bits per channel. Furthermore, in nearest neighbor search tasks, our method outperforms existing product quantization techniques in recall while reducing indexing time to virtually zero. 

**Abstract (ZH)**: 涡轮量化的矢量量化方法：克服现有方法限制，实现最优失真率 

---
# Mjölnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density 

**Title (ZH)**: Mjölnir：用于全球闪电闪光密度的深度学习参数化框架 

**Authors**: Minjong Cheon  

**Link**: [PDF](https://arxiv.org/pdf/2504.19822)  

**Abstract**: Recent advances in AI-based weather forecasting models, such as FourCastNet, Pangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep learning to emulate complex atmospheric dynamics. Building on this momentum, we propose Mjölnir, a novel deep learning-based framework for global lightning flash density parameterization. Trained on ERA5 atmospheric predictors and World Wide Lightning Location Network (WWLLN) observations at a daily temporal resolution and 1 degree spatial resolution, Mjölnir captures the nonlinear mapping between large-scale environmental conditions and lightning activity. The model architecture is based on the InceptionNeXt backbone with SENet, and a multi-task learning strategy to simultaneously predict lightning occurrence and magnitude. Extensive evaluations yield that Mollnir accurately reproduces the global distribution, seasonal variability, and regional characteristics of lightning activity, achieving a global Pearson correlation coefficient of 0.96 for annual mean fields. These results suggest that Mjölnir serves not only as an effective data-driven global lightning parameterization but also as a promising AI-based scheme for next-generation Earth system models (AI-ESMs). 

**Abstract (ZH)**: 基于AI的全球闪电闪络密度参数化方法Mjölnir Recent advances in AI-based weather forecasting models, such as FourCastNet, Pangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep learning to emulate complex atmospheric dynamics. Building on this momentum, we propose Mjölnir, a novel deep learning-based framework for global lightning flash density parameterization. Trained on ERA5 atmospheric predictors and World Wide Lightning Location Network (WWLLN) observations at a daily temporal resolution and 1-degree spatial resolution, Mjölnir captures the nonlinear mapping between large-scale environmental conditions and lightning activity. The model architecture is based on the InceptionNeXt backbone with SENet, and a multi-task learning strategy to simultaneously predict lightning occurrence and magnitude. Extensive evaluations yield that Mjölnir accurately reproduces the global distribution, seasonal variability, and regional characteristics of lightning activity, achieving a global Pearson correlation coefficient of 0.96 for annual mean fields. These results suggest that Mjölnir serves not only as an effective data-driven global lightning parameterization but also as a promising AI-based scheme for next-generation Earth system models (AI-ESMs). 

---
# PhenoAssistant: A Conversational Multi-Agent AI System for Automated Plant Phenotyping 

**Title (ZH)**: PhenoAssistant：一种自动化植物表型分析的会话多agent AI系统 

**Authors**: Feng Chen, Ilias Stogiannidis, Andrew Wood, Danilo Bueno, Dominic Williams, Fraser Macfarlane, Bruce Grieve, Darren Wells, Jonathan A. Atkinson, Malcolm J. Hawkesford, Stephen A. Rolfe, Tracy Lawson, Tony Pridmore, Mario Valerio Giuffrida, Sotirios A. Tsaftaris  

**Link**: [PDF](https://arxiv.org/pdf/2504.19818)  

**Abstract**: Plant phenotyping increasingly relies on (semi-)automated image-based analysis workflows to improve its accuracy and scalability. However, many existing solutions remain overly complex, difficult to reimplement and maintain, and pose high barriers for users without substantial computational expertise. To address these challenges, we introduce PhenoAssistant: a pioneering AI-driven system that streamlines plant phenotyping via intuitive natural language interaction. PhenoAssistant leverages a large language model to orchestrate a curated toolkit supporting tasks including automated phenotype extraction, data visualisation and automated model training. We validate PhenoAssistant through several representative case studies and a set of evaluation tasks. By significantly lowering technical hurdles, PhenoAssistant underscores the promise of AI-driven methodologies to democratising AI adoption in plant biology. 

**Abstract (ZH)**: 基于自然语言交互的PhenoAssistant：一个简化植物表型分析的AI驱动系统 

---
# Contextures: The Mechanism of Representation Learning 

**Title (ZH)**: 结构要素：表示学习的机制 

**Authors**: Runtian Zhai  

**Link**: [PDF](https://arxiv.org/pdf/2504.19792)  

**Abstract**: This dissertation establishes the contexture theory to mathematically characterize the mechanism of representation learning, or pretraining. Despite the remarkable empirical success of foundation models, it is not very clear what representations they learn, and why these representations are useful for various downstream tasks. A scientific understanding of representation learning is critical, especially at this point when scaling up the model size is producing diminishing returns, and designing new pretraining methods is imperative for further progress.
Prior work treated different representation learning methods quite differently, whereas the contexture theory provides a unified framework for analyzing these methods. The central argument is that a representation is learned from the association between the input X and a context variable A. We prove that if an encoder captures the maximum information of this association, in which case we say that the encoder learns the contexture, then it will be optimal on the class of tasks that are compatible with the context. We also show that a context is the most useful when the association between X and A is neither too strong nor too weak. The important implication of the contexture theory is that increasing the model size alone will achieve diminishing returns, and further advancements require better contexts.
We demonstrate that many pretraining objectives can learn the contexture, including supervised learning, self-supervised learning, generative models, etc. Then, we introduce two general objectives -- SVME and KISE, for learning the contexture. We also show how to mix multiple contexts together, an effortless way to create better contexts from existing ones. Then, we prove statistical learning bounds for representation learning. Finally, we discuss the effect of the data distribution shift from pretraining to the downstream task. 

**Abstract (ZH)**: 本论文建立了情境理论以数学化地描述表示学习或预训练的机制。尽管基础模型有显著的经验成功，但并不清楚它们学习了什么表示，以及这些表示为何对各种下游任务有用。对表示学习的科学理解至关重要，特别是在模型规模扩大产生递减回报的时刻，设计新的预训练方法对于进一步进展至关重要。

先前的工作对不同的表示学习方法处理得很不相同，而情境理论则提供了一个统一体系结构来分析这些方法。中心论点是，表示是从输入X与上下文变量A之间的关联中学习的。我们证明，如果编码器捕获了这种关联的最大信息，即我们说编码器学习了情境，则它在与该情境兼容的任务中将是最佳的。我们还表明，当X与A之间的关联既不过强也不过弱时，上下文最为有用。情境理论的重要含义是，增加模型规模本身将产生递减回报，进一步进展需要更好的情境。

我们证明了许多预训练目标可以学习情境，包括监督学习、自监督学习、生成模型等。然后，我们引入了两个一般目标——SVME和KISE，用于学习情境。我们还展示了如何将多个情境混合在一起，这是一种简单的方法来从现有情境中创建更好的情境。然后，我们证明了表示学习的统计学习界。最后，我们讨论了从预训练到下游任务数据分布变化的影响。 

---
# Hybrid Approach Combining Ultrasound and Blood Test Analysis with a Voting Classifier for Accurate Liver Fibrosis and Cirrhosis Assessment 

**Title (ZH)**: 融合超声与血液检测分析的投票分类器方法用于准确评估肝纤维化和肝硬化 

**Authors**: Kapil Kashyap, Sean Fargose, Chrisil Dabre, Fatema Dolaria, Nilesh Patil, Aniket Kore  

**Link**: [PDF](https://arxiv.org/pdf/2504.19755)  

**Abstract**: Liver cirrhosis is an insidious condition involving the substitution of normal liver tissue with fibrous scar tissue and causing major health complications. The conventional method of diagnosis using liver biopsy is invasive and, therefore, inconvenient for use in regular screening. In this paper,we present a hybrid model that combines machine learning techniques with clinical data and ultrasoundscans to improve liver fibrosis and cirrhosis detection accuracy is presented. The model integrates fixed blood test probabilities with deep learning model predictions (DenseNet-201) for ultrasonic images. The combined hybrid model achieved an accuracy of 92.5%. The findings establish the viability of the combined model in enhancing diagnosis accuracy and supporting early intervention in liver disease care. 

**Abstract (ZH)**: 肝硬化是一种隐匿性病症，涉及正常肝组织被纤维性瘢痕组织替代，并导致重大健康并发症。传统的肝活检诊断方法具有侵入性，因此不便于用于常规筛查。本文提出了一种将机器学习技术与临床数据和超声扫描结合的混合模型，以提高肝纤维化和肝硬化的检测准确性。该模型将固定血液检测概率与超声图像的深度学习模型预测（DenseNet-201）相结合。混合模型的综合应用实现了92.5%的准确率。研究结果证明了该联合模型在提高诊断准确性和支持肝病早期干预方面的有效性。 

---
# Generative AI in Education: Student Skills and Lecturer Roles 

**Title (ZH)**: 生成式AI在教育中的应用：学生技能与讲师角色 

**Authors**: Stefanie Krause, Ashish Dalvi, Syed Khubaib Zaidi  

**Link**: [PDF](https://arxiv.org/pdf/2504.19673)  

**Abstract**: Generative Artificial Intelligence (GenAI) tools such as ChatGPT are emerging as a revolutionary tool in education that brings both positive aspects and challenges for educators and students, reshaping how learning and teaching are approached. This study aims to identify and evaluate the key competencies students need to effectively engage with GenAI in education and to provide strategies for lecturers to integrate GenAI into teaching practices. The study applied a mixed method approach with a combination of a literature review and a quantitative survey involving 130 students from South Asia and Europe to obtain its findings. The literature review identified 14 essential student skills for GenAI engagement, with AI literacy, critical thinking, and ethical AI practices emerging as the most critical. The student survey revealed gaps in prompt engineering, bias awareness, and AI output management. In our study of lecturer strategies, we identified six key areas, with GenAI Integration and Curriculum Design being the most emphasised. Our findings highlight the importance of incorporating GenAI into education. While literature prioritized ethics and policy development, students favour hands-on, project-based learning and practical AI applications. To foster inclusive and responsible GenAI adoption, institutions should ensure equitable access to GenAI tools, establish clear academic integrity policies, and advocate for global GenAI research initiatives. 

**Abstract (ZH)**: 生成式人工智能工具（GenAI）如ChatGPT在教育中崭露头角，为教育者和学生带来了机遇与挑战，重塑了学习和教学的方法。本研究旨在识别和评估学生在教育中有效运用GenAI所需的关键技能，并为讲师提供将GenAI整合到教学实践中的策略。本研究采用混合方法，结合文献综述和定量调查，涉及来自南亚和欧洲的130名学生，以获取研究结果。文献综述确定了14项关键的学生技能以应对GenAI，其中AI素养、批判性思维和伦理AI实践最为关键。学生调查揭示了在提示工程、偏见意识和AI输出管理方面存在差距。在我们对讲师策略的研究中，确定了六个关键领域，其中GenAI整合和课程设计最受重视。研究结果强调将GenAI整合到教育中的重要性。虽然文献强调了伦理和政策开发，但学生更偏好基于实践的项目学习和实际的AI应用。为了促进包容性和负责任的GenAI采用，机构应确保GenAI工具的平等访问，制定清晰的学术诚信政策，并倡导全球性的GenAI研究倡议。 

---
# Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse DNNs on FPGAs 

**Title (ZH)**: RISC-V扩展的硬件/软件协同设计及其在FPGA上加速稀疏DNN的应用 

**Authors**: Muhammad Sabih, Abrarul Karim, Jakob Wittmann, Frank Hannig, Jürgen Teich  

**Link**: [PDF](https://arxiv.org/pdf/2504.19659)  

**Abstract**: The customizability of RISC-V makes it an attractive choice for accelerating deep neural networks (DNNs). It can be achieved through instruction set extensions and corresponding custom functional units. Yet, efficiently exploiting these opportunities requires a hardware/software co-design approach in which the DNN model, software, and hardware are designed together. In this paper, we propose novel RISC-V extensions for accelerating DNN models containing semi-structured and unstructured sparsity. While the idea of accelerating structured and unstructured pruning is not new, our novel design offers various advantages over other designs. To exploit semi-structured sparsity, we take advantage of the fine-grained (bit-level) configurability of FPGAs and suggest reserving a few bits in a block of DNN weights to encode the information about sparsity in the succeeding blocks. The proposed custom functional unit utilizes this information to skip computations. To exploit unstructured sparsity, we propose a variable cycle sequential multiply-and-accumulate unit that performs only as many multiplications as the non-zero weights. Our implementation of unstructured and semi-structured pruning accelerators can provide speedups of up to a factor of 3 and 4, respectively. We then propose a combined design that can accelerate both types of sparsities, providing speedups of up to a factor of 5. Our designs consume a small amount of additional FPGA resources such that the resulting co-designs enable the acceleration of DNNs even on small FPGAs. We benchmark our designs on standard TinyML applications such as keyword spotting, image classification, and person detection. 

**Abstract (ZH)**: RISC-V架构的定制化使其成为加速深度神经网络（DNNs）的有吸引力选择：基于硬件/软件协同设计的新型扩展研究 

---
# A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks 

**Title (ZH)**: 全面的词性标注以标准化库尔德语中部方言：库尔德自然语言处理任务的研究指南 

**Authors**: Shadan Shukr Sabr, Nazira Sabr Mustafa, Talar Sabah Omar, Salah Hwayyiz Rasool, Nawzad Anwer Omer, Darya Sabir Hamad, Hemin Abdulhameed Shams, Omer Mahmood Kareem, Rozhan Noori Abdullah, Khabat Atar Abdullah, Mahabad Azad Mohammad, Haneen Al-Raghefy, Safar M. Asaad, Sara Jamal Mohammed, Twana Saeed Ali, Fazil Shawrow, Halgurd S. Maghdid  

**Link**: [PDF](https://arxiv.org/pdf/2504.19645)  

**Abstract**: - The field of natural language processing (NLP) has dramatically expanded within the last decade. Many human-being applications are conducted daily via NLP tasks, starting from machine translation, speech recognition, text generation and recommendations, Part-of-Speech tagging (POS), and Named-Entity Recognition (NER). However, low-resourced languages, such as the Central-Kurdish language (CKL), mainly remain unexamined due to shortage of necessary resources to support their development. The POS tagging task is the base of other NLP tasks; for example, the POS tag set has been used to standardized languages to provide the relationship between words among the sentences, followed by machine translation and text recommendation. Specifically, for the CKL, most of the utilized or provided POS tagsets are neither standardized nor comprehensive. To this end, this study presented an accurate and comprehensive POS tagset for the CKL to provide better performance of the Kurdish NLP tasks. The article also collected most of the POS tags from different studies as well as from Kurdish linguistic experts to standardized part-of-speech tags. The proposed POS tagset is designed to annotate a large CKL corpus and support Kurdish NLP tasks. The initial investigations of this study via comparison with the Universal Dependencies framework for standard languages, show that the proposed POS tagset can streamline or correct sentences more accurately for Kurdish NLP tasks. 

**Abstract (ZH)**: 自然语言处理（NLP）领域在近十年间取得了显著扩展。日常的人类应用程序通过NLP任务得以实现，包括机器翻译、语音识别、文本生成与推荐、词性标注（POS）和命名实体识别（NER）。然而，由于缺乏必要的资源支持其发展，低资源语言如中央库尔德语（CKL）仍然未被充分研究。词性标注任务是其他NLP任务的基础；例如，词性标签集已经被标准化并应用于语言，以提供句子中词与词之间的关系，随后用于机器翻译和文本推荐。具体到CKL，目前使用的或提供的词性标签集既不标准化也不全面。为此，本研究提出了一种准确且全面的CKL词性标签集，旨在提高库尔德语NLP任务的性能。本文还收集了来自不同研究以及库尔德语语言专家的大部分词性标签，以标准化词性标签。所提出的词性标签集旨在标注大量CKL语料，并支持库尔德语NLP任务。通过与通用依存性框架（Universal Dependencies）进行初步比较，初始研究表明所提出的词性标签集能够更准确地为库尔德语NLP任务简化或修正句子。 

---
# Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection 

**Title (ZH)**: 轻量级适配器学习以实现更通用的遥感变化检测 

**Authors**: Dou Quan, Rufan Zhou, Shuang Wang, Ning Huyan, Dong Zhao, Yunan Li, Licheng Jiao  

**Link**: [PDF](https://arxiv.org/pdf/2504.19598)  

**Abstract**: Deep learning methods have shown promising performances in remote sensing image change detection (CD). However, existing methods usually train a dataset-specific deep network for each dataset. Due to the significant differences in the data distribution and labeling between various datasets, the trained dataset-specific deep network has poor generalization performances on other datasets. To solve this problem, this paper proposes a change adapter network (CANet) for a more universal and generalized CD. CANet contains dataset-shared and dataset-specific learning modules. The former explores the discriminative features of images, and the latter designs a lightweight adapter model, to deal with the characteristics of different datasets in data distribution and labeling. The lightweight adapter can quickly generalize the deep network for new CD tasks with a small computation cost. Specifically, this paper proposes an interesting change region mask (ICM) in the adapter, which can adaptively focus on interested change objects and decrease the influence of labeling differences in various datasets. Moreover, CANet adopts a unique batch normalization layer for each dataset to deal with data distribution differences. Compared with existing deep learning methods, CANet can achieve satisfactory CD performances on various datasets simultaneously. Experimental results on several public datasets have verified the effectiveness and advantages of the proposed CANet on CD. CANet has a stronger generalization ability, smaller training costs (merely updating 4.1%-7.7% parameters), and better performances under limited training datasets than other deep learning methods, which also can be flexibly inserted with existing deep models. 

**Abstract (ZH)**: 深度学习方法在遥感图像变化检测中的应用取得了令人鼓舞的性能。然而，现有方法通常为每个数据集训练特定的深度网络。由于不同数据集间数据分布和标注的显著差异，训练的数据集特定深度网络在其他数据集上的泛化性能较差。为解决这一问题，本文提出一种变化适配网络(CANet)以实现更通用和泛化的变化检测(CD)。CANet包含数据集共享模块和数据集特定模块。前者探索图像的判别特征，后者设计一种轻量级适配器模型，以应对不同数据集在数据分布和标注上的差异。轻量级适配器能以较小的计算成本快速泛化新的CD任务。具体地，本文在适配器中提出了一个有趣的变 region 面积掩模(IRC)，它可以自适应地关注感兴趣的变化对象，并减少不同数据集中标注差异的影响。此外，CANet为每个数据集采用了独特的批归一化层，以应对数据分布差异。与现有的深度学习方法相比，CANet可以在各种数据集上同时实现满意的CD性能。实验结果在几个公开数据集上验证了所提CANet在CD上的有效性和优势。CANet具有更强的泛化能力、更低的训练成本（仅更新4.1%-7.7%的参数）以及在有限训练数据集下的更好性能，并且可以灵活地插入现有的深度模型中。 

---
# WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution 

**Title (ZH)**: WILD：一种新的野生图像链接数据集用于合成图像归属研究 

**Authors**: Pietro Bongini, Sara Mandelli, Andrea Montibeller, Mirko Casu, Orazio Pontorno, Claudio Ragaglia, Luca Zanchetta, Mattia Aquilina, Taiba Majid Wani, Luca Guarnera, Benedetta Tondi, Paolo Bestagini, Irene Amerini, Francesco Denatale, Sebastiano Battiato, Mauro Barni  

**Link**: [PDF](https://arxiv.org/pdf/2504.19595)  

**Abstract**: Synthetic image source attribution is an open challenge, with an increasing number of image generators being released yearly. The complexity and the sheer number of available generative techniques, as well as the scarcity of high-quality open source datasets of diverse nature for this task, make training and benchmarking synthetic image source attribution models very challenging. WILD is a new in-the-Wild Image Linkage Dataset designed to provide a powerful training and benchmarking tool for synthetic image attribution models. The dataset is built out of a closed set of 10 popular commercial generators, which constitutes the training base of attribution models, and an open set of 10 additional generators, simulating a real-world in-the-wild scenario. Each generator is represented by 1,000 images, for a total of 10,000 images in the closed set and 10,000 images in the open set. Half of the images are post-processed with a wide range of operators. WILD allows benchmarking attribution models in a wide range of tasks, including closed and open set identification and verification, and robust attribution with respect to post-processing and adversarial attacks. Models trained on WILD are expected to benefit from the challenging scenario represented by the dataset itself. Moreover, an assessment of seven baseline methodologies on closed and open set attribution is presented, including robustness tests with respect to post-processing. 

**Abstract (ZH)**: 合成图像来源 attribution 是一个开放性的挑战，随着每年发布越来越多的图像生成器。生成技术的复杂性及其数量的增多，以及可用于此任务的高质量多样化开源数据集的稀缺性，使得训练和基准测试合成图像来源 attribution 模型极具挑战性。WILD 是一个新的野外图像链接数据集，旨在为合成图像 attribution 模型提供强大的训练和基准测试工具。该数据集基于10个流行的商业生成器构建，这些生成器构成了 attribution 模型的训练基础，并包含10个开放生成器，模拟了真实世界的野外场景。每个生成器包含1,000张图像，总共10,000张图像用于封闭集，10,000张图像用于开放集。其中一半的图像经过了广泛操作的后处理。WILD 允许在包括封闭集和开放集识别与验证以及对抗攻击的鲁棒 attribution 等广泛任务中对 attribution 模型进行基准测试。在 WILD 上训练的模型有望从数据集本身所代表的挑战性场景中受益。此外，还对七种基线方法在封闭集和开放集 attribution 以及后处理的鲁棒性方面的性能进行了评估。 

---
# Mapping the Italian Telegram Ecosystem 

**Title (ZH)**: 意大利电报生态系统映射 

**Authors**: Lorenzo Alvisi, Serena Tardelli, Maurizio Tesconi  

**Link**: [PDF](https://arxiv.org/pdf/2504.19594)  

**Abstract**: Telegram has become a major space for political discourse and alternative media. However, its lack of moderation allows misinformation, extremism, and toxicity to spread. While prior research focused on these particular phenomena or topics, these have mostly been examined separately, and a broader understanding of the Telegram ecosystem is still missing. In this work, we fill this gap by conducting a large-scale analysis of the Italian Telegram sphere, leveraging a dataset of 186 million messages from 13,151 chats collected in 2023. Using network analysis, Large Language Models, and toxicity detection tools, we examine how different thematic communities form, align ideologically, and engage in harmful discourse within the Italian cultural context. Results show strong thematic and ideological homophily. We also identify mixed ideological communities where far-left and far-right rhetoric coexist on particular geopolitical issues. Beyond political analysis, we find that toxicity, rather than being isolated in a few extreme chats, appears widely normalized within highly toxic communities. Moreover, we find that Italian discourse primarily targets Black people, Jews, and gay individuals independently of the topic. Finally, we uncover common trend of intra-national hostility, where Italians often attack other Italians, reflecting regional and intra-regional cultural conflicts that can be traced back to old historical divisions. This study provides the first large-scale mapping of the Italian Telegram ecosystem, offering insights into ideological interactions, toxicity, and identity-targets of hate and contributing to research on online toxicity across different cultural and linguistic contexts on Telegram. 

**Abstract (ZH)**: Telegram已成为政治 discourse和替代媒体的重要空间。然而，其缺乏 Moderation 导致了错误信息、极端主义和毒性的传播。尽管先前的研究关注了这些特定现象或主题，但它们大多单独进行，对 Telegram 生态系统的全面理解仍然不足。本文通过利用2023年收集的1.86亿条信息和13,151个聊天室的数据集，填补了这一空白，开展了大规模分析。利用网络分析、大规模语言模型和毒性检测工具，我们探讨了不同主题社区如何形成、在意识形态上如何一致以及如何在意大利文化背景下进行有害 discourse。结果表明，存在强烈的主题和意识形态同质性。我们还发现在特定地缘政治问题上，左翼和右翼的 rhetoric 共存于混合意识形态社区中。超越政治分析，我们发现毒性并非仅局限于少数极端聊天室，而是广泛地在高度毒性的社区中正常化。此外，我们发现意大利 discourse 主要针对黑人、犹太人和同性恋个体，与主题无关。最后，我们揭示了一种国内敌对的普遍趋势，其中意大利人经常攻击其他意大利人，反映了可追溯到旧历史分化的区域和区域内文化冲突。本文提供了意大利 Telegram 生态系统的首份大规模绘制，提供了关于意识形态互动、毒性以及仇恨目标的见解，并对 Telegram 不同文化和语言背景下在线毒性研究做出贡献。 

---
# Neural network task specialization via domain constraining 

**Title (ZH)**: 领域约束下的神经网络任务 specialization 

**Authors**: Roman Malashin, Daniil Ilyukhin  

**Link**: [PDF](https://arxiv.org/pdf/2504.19592)  

**Abstract**: This paper introduces a concept of neural network specialization via task-specific domain constraining, aimed at enhancing network performance on data subspace in which the network operates. The study presents experiments on training specialists for image classification and object detection tasks. The results demonstrate that specialization can enhance a generalist's accuracy even without additional data or changing training regimes: solely by constraining class label space in which the network performs. Theoretical and experimental analyses indicate that effective specialization requires modifying traditional fine-tuning methods and constraining data space to semantically coherent subsets. The specialist extraction phase before tuning the network is proposed for maximal performance gains. We also provide analysis of the evolution of the feature space during specialization. This study paves way to future research for developing more advanced dynamically configurable image analysis systems, where computations depend on the specific input. Additionally, the proposed methods can help improve system performance in scenarios where certain data domains should be excluded from consideration of the generalist network. 

**Abstract (ZH)**: 本文通过任务特定领域约束引入了神经网络专业化概念，旨在通过限制网络运行的数据子空间来提升网络性能。研究在图像分类和对象检测任务上进行了专家训练实验。结果表明，仅通过约束网络执行的类标签空间，即使不增加数据或改变训练制度，专业化也能提升通用模型的准确性。理论和实验分析表明，有效的专业化需要修改传统的微调方法，并约束到语义一致的子数据集。在调整网络之前提出专家提取阶段，以实现最大的性能提升。本文还分析了专业化过程中文本特征空间的变化。该研究为进一步开发依赖特定输入的更高级动态可配置图像分析系统奠定了基础。此外，提出的方法也有助于在某些数据领域应排除在通用网络考虑之外的场景中提升系统性能。 

---
# Arabic Metaphor Sentiment Classification Using Semantic Information 

**Title (ZH)**: 阿拉伯语隐喻情感分类基于语义信息 

**Authors**: Israa Alsiyat  

**Link**: [PDF](https://arxiv.org/pdf/2504.19590)  

**Abstract**: In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1] using newly designed automatic tools for sentiment classification for AMC based on semantic tags. The tool incorporates semantic emotional tags for sentiment classification. I evaluate the tool using standard methods, which are F-score, recall, and precision. The method is to show the impact of Arabic online metaphors on sentiment through the newly designed tools. To the best of our knowledge, this is the first approach to conduct sentiment classification for Arabic metaphors using semantic tags to find the impact of the metaphor. 

**Abstract (ZH)**: 本文讨论了使用基于语义标签的新设计自动工具对阿拉伯隐喻语料库(AMC)进行情感分类的测试。该工具结合了用于情感分类的语义情感标签。通过使用标准方法，即F分数、召回率和精确率来评估该工具。本文通过新设计的工具展示了阿拉伯在线隐喻对情感的影响。据我们所知，这是首次使用语义标签对阿拉伯隐喻进行情感分类以找出隐喻影响的方法。 

---
# Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction 

**Title (ZH)**: 点到四边形单元网：基于面预测的点云四边网生成 

**Authors**: Zezeng Li, Zhihui Qi, Weimin Wang, Ziliang Wang, Junyi Duan, Na Lei  

**Link**: [PDF](https://arxiv.org/pdf/2504.19545)  

**Abstract**: Quad meshes are essential in geometric modeling and computational mechanics. Although learning-based methods for triangle mesh demonstrate considerable advancements, quad mesh generation remains less explored due to the challenge of ensuring coplanarity, convexity, and quad-only meshes. In this paper, we present Point2Quad, the first learning-based method for quad-only mesh generation from point clouds. The key idea is learning to identify quad mesh with fused pointwise and facewise features. Specifically, Point2Quad begins with a k-NN-based candidate generation considering the coplanarity and squareness. Then, two encoders are followed to extract geometric and topological features that address the challenge of quad-related constraints, especially by combining in-depth quadrilaterals-specific characteristics. Subsequently, the extracted features are fused to train the classifier with a designed compound loss. The final results are derived after the refinement by a quad-specific post-processing. Extensive experiments on both clear and noise data demonstrate the effectiveness and superiority of Point2Quad, compared to baseline methods under comprehensive metrics. 

**Abstract (ZH)**: 基于点云的全四边形单元网格生成方法：Point2Quad 

---
# DISCO: learning to DISCover an evolution Operator for multi-physics-agnostic prediction 

**Title (ZH)**: DISCO: 学习发现多物理量无关的演变运算以进行预测 

**Authors**: Rudy Morel, Jiequn Han, Edouard Oyallon  

**Link**: [PDF](https://arxiv.org/pdf/2504.19496)  

**Abstract**: We address the problem of predicting the next state of a dynamical system governed by unknown temporal partial differential equations (PDEs) using only a short trajectory. While standard transformers provide a natural black-box solution to this task, the presence of a well-structured evolution operator in the data suggests a more tailored and efficient approach. Specifically, when the PDE is fully known, classical numerical solvers can evolve the state accurately with only a few parameters. Building on this observation, we introduce DISCO, a model that uses a large hypernetwork to process a short trajectory and generate the parameters of a much smaller operator network, which then predicts the next state through time integration. Our framework decouples dynamics estimation (i.e., DISCovering an evolution operator from a short trajectory) from state prediction (i.e., evolving this operator). Experiments show that pretraining our model on diverse physics datasets achieves state-of-the-art performance while requiring significantly fewer epochs. Moreover, it generalizes well and remains competitive when fine-tuned on downstream tasks. 

**Abstract (ZH)**: 我们提出了一种方法，使用短时序数据预测由未知时间偏微分方程（PDE）支配的动力系统的下一状态。虽然标准变压器为这个任务提供了一种自然的黑箱解决方案，但数据中的良好结构化演化算子表明，存在更定制化和高效的方法。特别是在PDE完全已知的情况下，经典数值求解器仅需少量参数即可准确地演化状态。基于这一观察，我们引入了DISCO模型，该模型使用一个大型超网络处理短时序并生成一个更小的算子网络的参数，然后通过时间积分预测下一状态。我们的框架将动力学估计（即从短时序中发现演化算子）与状态预测（即演化该算子）解耦。实验表明，使用多样化的物理数据集预训练我们的模型可以实现最先进的性能，同时所需训练周期显著较少。此外，该模型具有良好的泛化能力，在下游任务微调后仍保持竞争力。 

---
# Sharp higher order convergence rates for the Adam optimizer 

**Title (ZH)**: Adam优化器的锐锐高阶收敛速率分析 

**Authors**: Steffen Dereich, Arnulf Jentzen, Adrian Riekert  

**Link**: [PDF](https://arxiv.org/pdf/2504.19426)  

**Abstract**: Gradient descent based optimization methods are the methods of choice to train deep neural networks in machine learning. Beyond the standard gradient descent method, also suitable modified variants of standard gradient descent involving acceleration techniques such as the momentum method and/or adaptivity techniques such as the RMSprop method are frequently considered optimization methods. These days the most popular of such sophisticated optimization schemes is presumably the Adam optimizer that has been proposed in 2014 by Kingma and Ba. A highly relevant topic of research is to investigate the speed of convergence of such optimization methods. In particular, in 1964 Polyak showed that the standard gradient descent method converges in a neighborhood of a strict local minimizer with rate (x - 1)(x + 1)^{-1} while momentum achieves the (optimal) strictly faster convergence rate (\sqrt{x} - 1)(\sqrt{x} + 1)^{-1} where x \in (1,\infty) is the condition number (the ratio of the largest and the smallest eigenvalue) of the Hessian of the objective function at the local minimizer. It is the key contribution of this work to reveal that Adam also converges with the strictly faster convergence rate (\sqrt{x} - 1)(\sqrt{x} + 1)^{-1} while RMSprop only converges with the convergence rate (x - 1)(x + 1)^{-1}. 

**Abstract (ZH)**: 基于梯度下降的优化方法是机器学习中训练深度神经网络的方法。除了标准梯度下降方法之外，还经常考虑包含加速技术（如动量方法）和自适应技术（如RMSprop方法）的改进变体。近年来，提出得最多的一种复杂的优化方案可能是Kingma和Ba在2014年提出的Adam优化器。研究中的一个重要问题是探究这些优化方法的收敛速度。特别是，Polyak在1964年展示了标准梯度下降方法在严格局部极小值附近的收敛速率为(x - 1)(x + 1)^{-1}，而动量方法实现了更快的严格收敛速率(\sqrt{x} - 1)(\sqrt{x} + 1)^{-1}，其中x \in (1,\infty)是目标函数在局部极小值处海森矩阵的条件数（最大的和最小的特征值之比）。本文的关键贡献在于揭示Adam也以更快的严格收敛速率(\sqrt{x} - 1)(\sqrt{x} + 1)^{-1}收敛，而RMSprop的收敛速率为(x - 1)(x + 1)^{-1}。 

---
# Rethinking Label-specific Features for Label Distribution Learning 

**Title (ZH)**: 重新思考标签特定特征在标签分布学习中的作用 

**Authors**: Suping Xu, Chuyi Dai, Lin Shang, Changbin Shao, Xibei Yang, Witold Pedrycz  

**Link**: [PDF](https://arxiv.org/pdf/2504.19374)  

**Abstract**: Label distribution learning (LDL) is an emerging learning paradigm designed to capture the relative importance of labels for each instance. Label-specific features (LSFs), constructed by LIFT, have proven effective for learning tasks with label ambiguity by leveraging clustering-based prototypes for each label to re-characterize instances. However, directly introducing LIFT into LDL tasks can be suboptimal, as the prototypes it collects primarily reflect intra-cluster relationships while neglecting interactions among distinct clusters. Additionally, constructing LSFs using multi-perspective information, rather than relying solely on Euclidean distance, provides a more robust and comprehensive representation of instances, mitigating noise and bias that may arise from a single distance perspective. To address these limitations, we introduce Structural Anchor Points (SAPs) to capture inter-cluster interactions. This leads to a novel LSFs construction strategy, LIFT-SAP, which enhances LIFT by integrating both distance and direction information of each instance relative to SAPs. Furthermore, we propose a novel LDL algorithm, Label Distribution Learning via Label-specifIc FeaTure with SAPs (LDL-LIFT-SAP), which unifies multiple label description degrees predicted from different LSF spaces into a cohesive label distribution. Extensive experiments on 15 real-world datasets demonstrate the effectiveness of LIFT-SAP over LIFT, as well as the superiority of LDL-LIFT-SAP compared to seven other well-established algorithms. 

**Abstract (ZH)**: 基于结构锚点的标签特定特征学习分布（Label Distribution Learning via Label-specific Feature with Structural Anchor Points (LDL-LIFT-SAP)） 

---
# Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model 

**Title (ZH)**: 镜头背后的.doing：揭示图像地理定位中的隐私泄露以赋能多模态大规模推理模型 

**Authors**: Weidi Luo, Qiming Zhang, Tianyu Lu, Xiaogeng Liu, Yue Zhao, Zhen Xiang, Chaowei Xiao  

**Link**: [PDF](https://arxiv.org/pdf/2504.19373)  

**Abstract**: The increasing capabilities of agentic multi-modal large reasoning models, such as ChatGPT o3, have raised critical concerns regarding privacy leakage through inadvertent image geolocation. In this paper, we conduct the first systematic and controlled study on the potential privacy risks associated with visual reasoning abilities of ChatGPT o3. We manually collect and construct a dataset comprising 50 real-world images that feature individuals alongside privacy-relevant environmental elements, capturing realistic and sensitive scenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can predict user locations with high precision, achieving street-level accuracy (within one mile) in 60% of cases. Through analysis, we identify key visual cues, including street layout and front yard design, that significantly contribute to the model inference success. Additionally, targeted occlusion experiments demonstrate that masking critical features effectively mitigates geolocation accuracy, providing insights into potential defense mechanisms. Our findings highlight an urgent need for privacy-aware development for agentic multi-modal large reasoning models, particularly in applications involving private imagery. 

**Abstract (ZH)**: 具有代理多模态大模型推理能力的ChatGPT o3通过无意中的图像地理位置泄露隐私的能力不断增强：一项系统的和控制的研究 

---
# Flow Along the K-Amplitude for Generative Modeling 

**Title (ZH)**: 沿K振幅的流动用于生成建模 

**Authors**: Weitao Du, Shuning Chang, Jiasheng Tang, Yu Rong, Fan Wang, Shengchao Liu  

**Link**: [PDF](https://arxiv.org/pdf/2504.19353)  

**Abstract**: In this work, we propose a novel generative learning paradigm, K-Flow, an algorithm that flows along the $K$-amplitude. Here, $k$ is a scaling parameter that organizes frequency bands (or projected coefficients), and amplitude describes the norm of such projected coefficients. By incorporating the $K$-amplitude decomposition, K-Flow enables flow matching across the scaling parameter as time. We discuss three venues and six properties of K-Flow, from theoretical foundations, energy and temporal dynamics, and practical applications, respectively. Specifically, from the practical usage perspective, K-Flow allows steerable generation by controlling the information at different scales. To demonstrate the effectiveness of K-Flow, we conduct experiments on unconditional image generation, class-conditional image generation, and molecule assembly generation. Additionally, we conduct three ablation studies to demonstrate how K-Flow steers scaling parameter to effectively control the resolution of image generation. 

**Abstract (ZH)**: K-Flow：沿 $K$ 幅度流动的生成学习范式 

---
# Explanatory Summarization with Discourse-Driven Planning 

**Title (ZH)**: 基于话语驱动规划的解释性摘要生成 

**Authors**: Dongqi Liu, Xi Yu, Vera Demberg, Mirella Lapata  

**Link**: [PDF](https://arxiv.org/pdf/2504.19339)  

**Abstract**: Lay summaries for scientific documents typically include explanations to help readers grasp sophisticated concepts or arguments. However, current automatic summarization methods do not explicitly model explanations, which makes it difficult to align the proportion of explanatory content with human-written summaries. In this paper, we present a plan-based approach that leverages discourse frameworks to organize summary generation and guide explanatory sentences by prompting responses to the plan. Specifically, we propose two discourse-driven planning strategies, where the plan is conditioned as part of the input or part of the output prefix, respectively. Empirical experiments on three lay summarization datasets show that our approach outperforms existing state-of-the-art methods in terms of summary quality, and it enhances model robustness, controllability, and mitigates hallucination. 

**Abstract (ZH)**: 科学文档的概述通常包括解释以帮助读者理解复杂的概念或观点。然而，当前的自动摘要方法并未明确建模解释，这使得难以使解释内容的比例与手写摘要相一致。在本文中，我们提出了一种基于计划的方法，该方法利用论述框架来组织摘要生成并引导解释性句子，具体方式是通过提示对计划的响应。我们提出了两种基于论述的规划策略，其中计划作为输入的一部分或输出前缀的一部分进行条件约束。在三个低层次摘要数据集上的实验证明，我们的方法在摘要质量方面优于现有最先进的方法，并增强了模型的鲁棒性、可控性和减少了幻觉现象。 

---
# NSFlow: An End-to-End FPGA Framework with Scalable Dataflow Architecture for Neuro-Symbolic AI 

**Title (ZH)**: NSFlow：一种用于神经符号AI的可扩展数据流架构的端到端FPGA框架 

**Authors**: Hanchen Yang, Zishen Wan, Ritik Raj, Joongun Park, Ziwei Li, Ananda Samajdar, Arijit Raychowdhury, Tushar Krishna  

**Link**: [PDF](https://arxiv.org/pdf/2504.19323)  

**Abstract**: Neuro-Symbolic AI (NSAI) is an emerging paradigm that integrates neural networks with symbolic reasoning to enhance the transparency, reasoning capabilities, and data efficiency of AI systems. Recent NSAI systems have gained traction due to their exceptional performance in reasoning tasks and human-AI collaborative scenarios. Despite these algorithmic advancements, executing NSAI tasks on existing hardware (e.g., CPUs, GPUs, TPUs) remains challenging, due to their heterogeneous computing kernels, high memory intensity, and unique memory access patterns. Moreover, current NSAI algorithms exhibit significant variation in operation types and scales, making them incompatible with existing ML accelerators. These challenges highlight the need for a versatile and flexible acceleration framework tailored to NSAI workloads. In this paper, we propose NSFlow, an FPGA-based acceleration framework designed to achieve high efficiency, scalability, and versatility across NSAI systems. NSFlow features a design architecture generator that identifies workload data dependencies and creates optimized dataflow architectures, as well as a reconfigurable array with flexible compute units, re-organizable memory, and mixed-precision capabilities. Evaluating across NSAI workloads, NSFlow achieves 31x speedup over Jetson TX2, more than 2x over GPU, 8x speedup over TPU-like systolic array, and more than 3x over Xilinx DPU. NSFlow also demonstrates enhanced scalability, with only 4x runtime increase when symbolic workloads scale by 150x. To the best of our knowledge, NSFlow is the first framework to enable real-time generalizable NSAI algorithms acceleration, demonstrating a promising solution for next-generation cognitive systems. 

**Abstract (ZH)**: 基于FPGA的Neuro-Symbolic AI加速框架NSFlow 

---
# Anyprefer: An Agentic Framework for Preference Data Synthesis 

**Title (ZH)**: Anyprefer: 一种代理框架下的偏好数据合成方法 

**Authors**: Yiyang Zhou, Zhaoyang Wang, Tianle Wang, Shangyu Xing, Peng Xia, Bo Li, Kaiyuan Zheng, Zijian Zhang, Zhaorun Chen, Wenhao Zheng, Xuchao Zhang, Chetan Bansal, Weitong Zhang, Ying Wei, Mohit Bansal, Huaxiu Yao  

**Link**: [PDF](https://arxiv.org/pdf/2504.19276)  

**Abstract**: High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods often adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies since the reward model shares weights with the target model, thereby amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for aligning the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and the judge model collaborate together. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model's responses, mitigating biases in the rewarding process. In addition, a feedback mechanism is introduced to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment performance across four main applications, covering 21 datasets, achieving average improvements of 18.55% in five natural language generation datasets, 3.66% in nine vision-language understanding datasets, 30.05% in three medical image analysis datasets, and 16.00% in four visuo-motor control tasks. 

**Abstract (ZH)**: 高质量的偏好数据对于通过偏好学习使基础模型与人类价值观对齐至关重要。然而，此类数据的手动标注往往是耗时且昂贵的。最近的方法常常采用自奖励的方法，目标模型生成并标注自己的偏好数据，但这种方法会导致不准确性，因为奖励模型与目标模型共享权重，从而放大了固有的偏差。为解决这些问题，我们提出Anyprefer框架，旨在合成用于使目标模型对齐的高质量偏好数据。Anyprefer将数据合成过程构建成一个合作的双人马尔可夫博弈，其中目标模型和评判模型共同合作。在此过程中，引入了一系列外部工具以帮助评判模型准确地奖励目标模型的回应，减轻奖励过程中的偏差。此外，还引入了一种反馈机制以优化两模型的提示，增强合作并提高数据质量。合成的数据编译成一个新偏好数据集Anyprefer-V1，包含58,000个高质量的偏好对。广泛实验显示，Anyprefer在四个主要应用领域中显著提高了模型对齐性能，覆盖21个数据集，分别在五个自然语言生成数据集中平均提高了18.55%，在九个视觉-语言理解数据集中提高了3.66%，在三个医学图像分析数据集中提高了30.05%，在四个视觉-运动控制任务中提高了16.00%。 

---
# Balancing Creativity and Automation: The Influence of AI on Modern Film Production and Dissemination 

**Title (ZH)**: 平衡创新与自动化：AI对现代电影生产与传播的影响 

**Authors**: Yiren Xu  

**Link**: [PDF](https://arxiv.org/pdf/2504.19275)  

**Abstract**: The integration of Artificial Intelligence(AI) into film production has revolutionized efficiency and creativity, yet it simultaneously raises critical ethical and practical challenges. This study explores the dual impact of AI on modern cinema through three objectives: defining the optimal human-AI relationship, balancing creativity with automation, and developing ethical guidelines. By employing a mixed-method approach combining theoretical frameworks (auteur theory, human-technology relations) and case studies (The Safe Zone, Fast & Furious 7, The Brutalist), the research reveals that positioning AI as an "embodiment tool" rather than an independent "alterity partner" preserves human authorship and artistic integrity. Key findings highlight the risks of surveillance capitalism in AI-driven markets and the ethical dilemmas of deepfake technology. The study concludes with actionable recommendations, including international regulatory frameworks and a Human Control Index (HCI) to quantify AI involvement. These insights aim to guide filmmakers, policymakers, and scholars in navigating the evolving AI-cinema landscape while safeguarding cultural diversity and ethical standards. 

**Abstract (ZH)**: 人工智能在电影生产中的整合革新了效率和创造力，同时也引发了一系列关键的伦理和实践挑战。本研究通过三大目标探讨人工智能对现代 cinema 的双重影响：界定最优的人工智能关系、平衡创造力与自动化、制定伦理准则。通过结合理论框架（作者理论、人机关系）和案例研究（《安全区》、《速度与激情7》、《 brutalist》）的方法，研究揭示了将人工智能定位为“表现工具”而非独立的“另一种主体”的重要性，以保持人类的作者身份和艺术完整性。关键发现强调了人工智能驱动市场中的监视资本主义风险以及深度伪造技术的伦理困境。研究结论提出了可操作的建议，包括国际监管框架和人工智能参与度指数（HCI）来量化人工智能的参与程度。这些见解旨在指导制片人、政策制定者和学者在应对不断变化的 AI 电影 landscape 的同时，保护文化多样性和伦理标准。 

---
# TeleSparse: Practical Privacy-Preserving Verification of Deep Neural Networks 

**Title (ZH)**: TeleSparse: 实用的深度神经网络隐私保护验证方法 

**Authors**: Mohammad M Maheri, Hamed Haddadi, Alex Davidson  

**Link**: [PDF](https://arxiv.org/pdf/2504.19274)  

**Abstract**: Verification of the integrity of deep learning inference is crucial for understanding whether a model is being applied correctly. However, such verification typically requires access to model weights and (potentially sensitive or private) training data. So-called Zero-knowledge Succinct Non-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the capability to verify model inference without access to such sensitive data. However, applying ZK-SNARKs to modern neural networks, such as transformers and large vision models, introduces significant computational overhead.
We present TeleSparse, a ZK-friendly post-processing mechanisms to produce practical solutions to this problem. TeleSparse tackles two fundamental challenges inherent in applying ZK-SNARKs to modern neural networks: (1) Reducing circuit constraints: Over-parameterized models result in numerous constraints for ZK-SNARK verification, driving up memory and proof generation costs. We address this by applying sparsification to neural network models, enhancing proof efficiency without compromising accuracy or security. (2) Minimizing the size of lookup tables required for non-linear functions, by optimizing activation ranges through neural teleportation, a novel adaptation for narrowing activation functions' range.
TeleSparse reduces prover memory usage by 67% and proof generation time by 46% on the same model, with an accuracy trade-off of approximately 1%. We implement our framework using the Halo2 proving system and demonstrate its effectiveness across multiple architectures (Vision-transformer, ResNet, MobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens new directions for ZK-friendly model design, moving toward scalable, resource-efficient verifiable deep learning. 

**Abstract (ZH)**: TeleSparse：面向ZK的后处理机制以实现现代神经网络的实用零知识验证 

---
# Generative Adversarial Network based Voice Conversion: Techniques, Challenges, and Recent Advancements 

**Title (ZH)**: 基于生成对抗网络的语音转换：技术、挑战及 Recent Advancements 

**Authors**: Sandipan Dhar, Nanda Dulal Jana, Swagatam Das  

**Link**: [PDF](https://arxiv.org/pdf/2504.19197)  

**Abstract**: Voice conversion (VC) stands as a crucial research area in speech synthesis, enabling the transformation of a speaker's vocal characteristics to resemble another while preserving the linguistic content. This technology has broad applications, including automated movie dubbing, speech-to-singing conversion, and assistive devices for pathological speech rehabilitation. With the increasing demand for high-quality and natural-sounding synthetic voices, researchers have developed a wide range of VC techniques. Among these, generative adversarial network (GAN)-based approaches have drawn considerable attention for their powerful feature-mapping capabilities and potential to produce highly realistic speech. Despite notable advancements, challenges such as ensuring training stability, maintaining linguistic consistency, and achieving perceptual naturalness continue to hinder progress in GAN-based VC systems. This systematic review presents a comprehensive analysis of the voice conversion landscape, highlighting key techniques, key challenges, and the transformative impact of GANs in the field. The survey categorizes existing methods, examines technical obstacles, and critically evaluates recent developments in GAN-based VC. By consolidating and synthesizing research findings scattered across the literature, this review provides a structured understanding of the strengths and limitations of different approaches. The significance of this survey lies in its ability to guide future research by identifying existing gaps, proposing potential directions, and offering insights for building more robust and efficient VC systems. Overall, this work serves as an essential resource for researchers, developers, and practitioners aiming to advance the state-of-the-art (SOTA) in voice conversion technology. 

**Abstract (ZH)**: 基于生成对抗网络的声音转换：现状综述与未来研究方向 

---
# Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam Monte Carlo Simulations 

**Title (ZH)**: 基于机器学习的X射线束蒙特卡洛模拟中阴极尾效应建模 

**Authors**: Hussein Harb, Didier Benoit, Axel Rannou, Chi-Hieu Pham, Valentin Tissot, Bahaa Nasr, Julien Bert  

**Link**: [PDF](https://arxiv.org/pdf/2504.19155)  

**Abstract**: This study enhances Monte Carlo simulation accuracy in X-ray imaging by developing an AI-driven model for the anode heel effect, achieving improved beam intensity distribution and dosimetric precision. Through dynamic adjustments to beam weights on the anode and cathode sides of the X-ray tube, our machine learning model effectively replicates the asymmetry characteristic of clinical X-ray beams. Experimental results reveal dose rate increases of up to 9.6% on the cathode side and reductions of up to 12.5% on the anode side, for energy levels between 50 and 120 kVp. These experimentally optimized beam weights were integrated into the OpenGATE and GGEMS Monte Carlo toolkits, significantly advancing dosimetric simulation accuracy and the image quality which closely resembles the clinical imaging. Validation with fluence and dose actors demonstrated that the AI-based model closely mirrors clinical beam behavior, providing substantial improvements in dose consistency and accuracy over conventional X-ray models. This approach provides a robust framework for improving X-ray dosimetry, with potential applications in dose optimization, imaging quality enhancement, and radiation safety in both clinical and research settings. 

**Abstract (ZH)**: 本研究通过开发一种基于AI的模型来增强X射线成像中的蒙特卡洛模拟准确性，该模型实现了一改善的光束强度分布和剂量学精度，通过对X射线管阳极和阴极侧光束权重的动态调整，有效地复制了临床X射线光束的不对称特性。实验结果表明，对于50至120 kVp的能量水平，在阴极侧剂量率增加了至多9.6%，在阳极侧减少了至多12.5%。这些实验优化的光束权重被集成到OpenGATE和GGEMS蒙特卡洛工具包中，显著提高了剂量学模拟的准确性，并且生成的图像质量与临床成像高度一致。荧光和剂量行为验证显示，基于AI的模型能够密切模拟临床光束行为，提供了比常规X射线模型更高的剂量一致性和准确性。该方法为改善X射线剂量学提供了一个稳健的框架，具有在临床和研究环境中优化剂量、提高成像质量和保障辐射安全等方面的应用潜力。 

---
# BQSched: A Non-intrusive Scheduler for Batch Concurrent Queries via Reinforcement Learning 

**Title (ZH)**: BQSched：一种基于强化学习的非侵入式批并发查询调度器 

**Authors**: Chenhao Xu, Chunyu Chen, Jinglin Peng, Jiannan Wang, Jun Gao  

**Link**: [PDF](https://arxiv.org/pdf/2504.19142)  

**Abstract**: Most large enterprises build predefined data pipelines and execute them periodically to process operational data using SQL queries for various tasks. A key issue in minimizing the overall makespan of these pipelines is the efficient scheduling of concurrent queries within the pipelines. Existing tools mainly rely on simple heuristic rules due to the difficulty of expressing the complex features and mutual influences of queries. The latest reinforcement learning (RL) based methods have the potential to capture these patterns from feedback, but it is non-trivial to apply them directly due to the large scheduling space, high sampling cost, and poor sample utilization.
Motivated by these challenges, we propose BQSched, a non-intrusive Scheduler for Batch concurrent Queries via reinforcement learning. Specifically, BQSched designs an attention-based state representation to capture the complex query patterns, and proposes IQ-PPO, an auxiliary task-enhanced proximal policy optimization (PPO) algorithm, to fully exploit the rich signals of Individual Query completion in logs. Based on the RL framework above, BQSched further introduces three optimization strategies, including adaptive masking to prune the action space, scheduling gain-based query clustering to deal with large query sets, and an incremental simulator to reduce sampling cost. To our knowledge, BQSched is the first non-intrusive batch query scheduler via RL. Extensive experiments show that BQSched can significantly improve the efficiency and stability of batch query scheduling, while also achieving remarkable scalability and adaptability in both data and queries. For example, across all DBMSs and scales tested, BQSched reduces the overall makespan of batch queries on TPC-DS benchmark by an average of 34% and 13%, compared with the commonly used heuristic strategy and the adapted RL-based scheduler, respectively. 

**Abstract (ZH)**: 基于强化学习的非侵入式批并发查询调度器BQSched 

---
# Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments 

**Title (ZH)**: 快速可靠：随机环境中文适应决策制定者的任务采样与后验及多样性协同方法 

**Authors**: Yun Qu, Wang, Yixiu Mao, Yiqin Lv, Xiangyang Ji  

**Link**: [PDF](https://arxiv.org/pdf/2504.19139)  

**Abstract**: Task robust adaptation is a long-standing pursuit in sequential decision-making. Some risk-averse strategies, e.g., the conditional value-at-risk principle, are incorporated in domain randomization or meta reinforcement learning to prioritize difficult tasks in optimization, which demand costly intensive evaluations. The efficiency issue prompts the development of robust active task sampling to train adaptive policies, where risk-predictive models are used to surrogate policy evaluation. This work characterizes the optimization pipeline of robust active task sampling as a Markov decision process, posits theoretical and practical insights, and constitutes robustness concepts in risk-averse scenarios. Importantly, we propose an easy-to-implement method, referred to as Posterior and Diversity Synergized Task Sampling (PDTS), to accommodate fast and robust sequential decision-making. Extensive experiments show that PDTS unlocks the potential of robust active task sampling, significantly improves the zero-shot and few-shot adaptation robustness in challenging tasks, and even accelerates the learning process under certain scenarios. Our project website is at this https URL. 

**Abstract (ZH)**: 任务鲁棒适应在序列决策中一直是一个长期追求的目标。一些规避风险的策略，例如条件价值-at-风险原则，被纳入领域随机化或元增强学习中，以优先处理优化中需要昂贵密集评估的困难任务。效率问题促使了鲁棒主动任务采样的发展，在此过程中，使用风险预测模型替代策略评估。本工作将鲁棒主动任务采样的优化管道建模为马尔可夫决策过程，提出理论和实践见解，并在规避风险的情景中构建鲁棒性概念。重要的是，我们提出了一种易于实现的方法，称为后验与多样性能协同的任务采样（PDTS）。 extensive实验表明，PDTS 解锁了鲁棒主动任务采样的潜在能力，显著提高了在具有挑战性的任务中的零样本和少量样本适应鲁棒性，并且在某些情景下甚至加速了学习过程。我们的项目网站在此处：this https URL。 

---
# Beyond Levels of Driving Automation: A Triadic Framework of Human-AI Collaboration in On-Road Mobility 

**Title (ZH)**: 超越自动驾驶层级：道路上移动性中人机协作的三元框架 

**Authors**: Gaojian Huang, Yantong Jin, Wei-Hsiang Lo  

**Link**: [PDF](https://arxiv.org/pdf/2504.19120)  

**Abstract**: The goal of the current study is to introduce a triadic human-AI collaboration framework for the automated vehicle domain. Previous classifications (e.g., SAE Levels of Automation) focus on defining automation levels based on who controls the vehicle. However, it remains unclear how human users and AI should collaborate in real-time, especially in dynamic driving contexts, where roles can shift frequently. To fill the gap, this study proposes a triadic human-AI collaboration framework with three AI roles (i.e., Advisor, Co-Pilot, and Guardian) that dynamically adapt to human needs. Overall, the study lays a foundation for developing adaptive, role-based human-AI collaboration strategies in automated vehicles. 

**Abstract (ZH)**: 当前研究的目标是介绍一种适用于自动车辆领域的三元人类-AI协作框架。先前的分类（如SAE自动化等级）侧重于根据谁控制车辆来定义自动化水平。然而，在动态驾驶环境中，人类用户与AI如何实现实时协作，特别是在角色频繁变化的情况下，仍然不清楚。为填补这一空白，本研究提出了一种三元人类-AI协作框架，包含三种AI角色（即顾问、副驾和守护者），能够动态适应人类的需求。总体而言，本研究为在自动车辆中开发适应性和基于角色的人类-AI协作策略奠定了基础。 

---
# MIA-Mind: A Multidimensional Interactive Attention Mechanism Based on MindSpore 

**Title (ZH)**: MIA-Mind：一种基于MindSpore的多维度交互注意力机制 

**Authors**: Zhenkai Qin, Jiaquan Liang, Qiao Fang  

**Link**: [PDF](https://arxiv.org/pdf/2504.19080)  

**Abstract**: Attention mechanisms have significantly advanced deep learning by enhancing feature representation through selective focus. However, existing approaches often independently model channel importance and spatial saliency, overlooking their inherent interdependence and limiting their effectiveness. To address this limitation, we propose MIA-Mind, a lightweight and modular Multidimensional Interactive Attention Mechanism, built upon the MindSpore framework. MIA-Mind jointly models spatial and channel features through a unified cross-attentive fusion strategy, enabling fine-grained feature recalibration with minimal computational overhead. Extensive experiments are conducted on three representative datasets: on CIFAR-10, MIA-Mind achieves an accuracy of 82.9\%; on ISBI2012, it achieves an accuracy of 78.7\%; and on CIC-IDS2017, it achieves an accuracy of 91.9\%. These results validate the versatility, lightweight design, and generalization ability of MIA-Mind across heterogeneous tasks. Future work will explore the extension of MIA-Mind to large-scale datasets, the development of ada,ptive attention fusion strategies, and distributed deployment to further enhance scalability and robustness. 

**Abstract (ZH)**: 基于MindSpore的多维交互注意力机制MIA-Mind 

---
# AI Recommendations and Non-instrumental Image Concerns 

**Title (ZH)**: AI推荐与非工具性图片顾虑 

**Authors**: David Almog  

**Link**: [PDF](https://arxiv.org/pdf/2504.19047)  

**Abstract**: There is growing enthusiasm about the potential for humans and AI to collaborate by leveraging their respective strengths. Yet in practice, this promise often falls short. This paper uses an online experiment to identify non-instrumental image concerns as a key reason individuals underutilize AI recommendations. I show that concerns about how one is perceived, even when those perceptions carry no monetary consequences, lead participants to disregard AI advice and reduce task performance. 

**Abstract (ZH)**: 利用各自优势进行人机协作的前景日益受到关注，但在实践中有不少落空。本文通过在线实验发现，非工具性的图像顾虑是个体不充分利用AI推荐的关键原因。我表明，即使这些感知没有经济后果，人们对自身形象的担忧也会影响他们忽视AI建议并降低任务表现。 

---
# Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention 

**Title (ZH)**: 增强 cochlear implant 信号编码的标度点积注意力方法 

**Authors**: Billel Essaid, Hamza Kheddar, Noureddine Batel  

**Link**: [PDF](https://arxiv.org/pdf/2504.19046)  

**Abstract**: Cochlear implants (CIs) play a vital role in restoring hearing for individuals with severe to profound sensorineural hearing loss by directly stimulating the auditory nerve with electrical signals. While traditional coding strategies, such as the advanced combination encoder (ACE), have proven effective, they are constrained by their adaptability and precision. This paper investigates the use of deep learning (DL) techniques to generate electrodograms for CIs, presenting our model as an advanced alternative. We compared the performance of our model with the ACE strategy by evaluating the intelligibility of reconstructed audio signals using the short-time objective intelligibility (STOI) metric. The results indicate that our model achieves a STOI score of 0.6031, closely approximating the 0.6126 score of the ACE strategy, and offers potential advantages in flexibility and adaptability. This study underscores the benefits of incorporating artificial intelligent (AI) into CI technology, such as enhanced personalization and efficiency. 

**Abstract (ZH)**: 耳蜗植入物(CIs)在通过电信号直接刺激听神经来恢复重度到极重度感音神经性听力损失个体的听力方面发挥着关键作用。虽然传统的编码策略，如高级组合编码器(ACE)，已被证明有效，但它们受限于其适应性和精确性。本文探讨了使用深度学习(DL)技术为CIs生成电极图，并将我们的模型作为先进的替代方案。我们通过使用短时客观可懂度(STOI)指标评估重建音频信号的可懂度，将我们的模型性能与ACE策略进行了比较。结果显示，我们的模型实现了0.6031的STOI分数，接近ACE策略的0.6126分数，并在灵活性和适应性方面提供了潜在优势。本研究强调了将人工智能(AI)整合到CI技术中的益处，如增强的个性化和效率。 

---
# Generative Models for Fast Simulation of Cherenkov Detectors at the Electron-Ion Collider 

**Title (ZH)**: 生成模型在电子离子对撞机中快速模拟切伦科夫探测器 

**Authors**: James Giroux, Michael Martinez, Cristiano Fanelli  

**Link**: [PDF](https://arxiv.org/pdf/2504.19042)  

**Abstract**: The integration of Deep Learning (DL) into experimental nuclear and particle physics has driven significant progress in simulation and reconstruction workflows. However, traditional simulation frameworks such as Geant4 remain computationally intensive, especially for Cherenkov detectors, where simulating optical photon transport through complex geometries and reflective surfaces introduces a major bottleneck. To address this, we present an open, standalone fast simulation tool for Detection of Internally Reflected Cherenkov Light (DIRC) detectors, with a focus on the High-Performance DIRC (hpDIRC) at the future Electron-Ion Collider (EIC). Our framework incorporates a suite of generative models tailored to accelerate particle identification (PID) tasks by offering a scalable, GPU-accelerated alternative to full Geant4-based simulations. Designed with accessibility in mind, our simulation package enables both DL researchers and physicists to efficiently generate high-fidelity large-scale datasets on demand, without relying on complex traditional simulation stacks. This flexibility supports the development and benchmarking of novel DL-driven PID methods. Moreover, this fast simulation pipeline represents a critical step toward enabling EIC-wide PID strategies that depend on virtually unlimited simulated samples, spanning the full acceptance of the hpDIRC. 

**Abstract (ZH)**: 深度学习在实验核物理和粒子物理中的集成推动了模拟和重建工作流的重要进展。然而，传统的模拟框架如Geant4在计算上仍然密集，特别是对于切伦科夫探测器而言，其中光学光子在复杂几何结构和反射表面的传输模拟成为主要瓶颈。为了解决这一问题，我们提出了一个针对内部反射切伦科夫光（DIRC）探测器的开源独立快速模拟工具，重点关注未来的电子-离子对撞机（EIC）上的高性能DIRC（hpDIRC）。我们的框架采用了针对加速粒子识别（PID）任务的一系列生成模型，提供了与基于完整Geant4模拟的替代方案相比更具可扩展性和GPU加速的方案。该模拟包设计时考虑了易用性，使深度学习研究人员和物理学家能够高效地按需生成高保真大规模数据集，而无需依赖复杂的传统模拟堆栈。这种灵活性支持了新型DL驱动的PID方法的开发和基准测试。此外，这个快速模拟管道是向EIC范围内依赖于近乎无限模拟样本量的PID策略迈出的关键一步，覆盖hpDIRC的全接受度。 

---
# Improved Molecular Generation through Attribute-Driven Integrative Embeddings and GAN Selectivity 

**Title (ZH)**: 基于属性驱动综合嵌入和GAN选择性的分子生成改进方法 

**Authors**: Nandan Joshi, Erhan Guven  

**Link**: [PDF](https://arxiv.org/pdf/2504.19040)  

**Abstract**: The growing demand for molecules with tailored properties in fields such as drug discovery and chemical engineering has driven advancements in computational methods for molecular design. Machine learning-based approaches for de-novo molecular generation have recently garnered significant attention. This paper introduces a transformer-based vector embedding generator combined with a modified Generative Adversarial Network (GAN) to generate molecules with desired properties. The embedding generator utilizes a novel molecular descriptor, integrating Morgan fingerprints with global molecular attributes, enabling the transformer to capture local functional groups and broader molecular characteristics. Modifying the GAN generator loss function ensures the generation of molecules with specific desired properties. The transformer achieves a reconversion accuracy of 94% while translating molecular descriptors back to SMILES strings, validating the utility of the proposed embeddings for generative tasks. The approach is validated by generating novel odorant molecules using a labeled dataset of odorant and non-odorant compounds. With the modified range-loss function, the GAN exclusively generates odorant molecules. This work underscores the potential of combining novel vector embeddings with transformers and modified GAN architectures to accelerate the discovery of tailored molecules, offering a robust tool for diverse molecular design applications. 

**Abstract (ZH)**: 分子设计中用于生成具有定制性质的分子的计算方法需求增长促进了分子设计领域的进展。基于机器学习的从头分子生成方法近期引起了广泛关注。本文介绍了一种基于变换器的向量嵌入生成器与修改的生成对抗网络（GAN）相结合的方法，用于生成具有期望性质的分子。嵌入生成器利用了一种新颖的分子描述符，将Morgan指纹与全局分子属性相结合，使变换器能够捕捉局部官能团和更广泛的分子特性。修改GAN生成器损失函数确保生成具有特定期望性质的分子。变换器在将分子描述符转换回SMILES字符串时实现了94%的重构准确性，验证了所提出的嵌入对生成任务的效用。通过使用标记的香气化合物和非香气化合物数据集生成新型香气分子，该方法得到了验证。修改范围损失函数后，GAN仅生成香气分子。这项工作强调了将新颖的向量嵌入与变换器及修改的GAN架构相结合以加速定制分子发现的潜力，提供了一个强大的工具以应用于多种分子设计领域。 

---
# Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting 

**Title (ZH)**: 提升科学文本分类性能：基于数据集扩展和硬投票的微调模型 

**Authors**: Zhyar Rzgar K Rostam, Gábor Kertész  

**Link**: [PDF](https://arxiv.org/pdf/2504.19021)  

**Abstract**: Efficient text classification is essential for handling the increasing volume of academic publications. This study explores the use of pre-trained language models (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on the Web of Science (WoS-46985) dataset for scientific text classification. To enhance performance, we augment the dataset by executing seven targeted queries in the WoS database, retrieving 1,000 articles per category aligned with WoS-46985's main classes. PLMs predict labels for this unlabeled data, and a hard-voting strategy combines predictions for improved accuracy and confidence. Fine-tuning on the expanded dataset with dynamic learning rates and early stopping significantly boosts classification accuracy, especially in specialized domains. Domain-specific models like SciBERT and BioBERT consistently outperform general-purpose models such as BERT. These findings underscore the efficacy of dataset augmentation, inference-driven label prediction, hard-voting, and fine-tuning techniques in creating robust and scalable solutions for automated academic text classification. 

**Abstract (ZH)**: 高效的文本分类对于处理不断增加的学术出版物至关重要。本研究探索了在Web of Science (WoS-46985) 数据集上微调预训练语言模型（PLMs），包括BERT、SciBERT、BioBERT和BlueBERT，以进行科学文本分类。为提高性能，我们通过在WoS数据库中执行七项有针对性的查询，扩展数据集，检索每个类别1,000篇文章，与WoS-46985的主要类别对齐。PLMs对这些未标记数据进行标签预测，并采用硬投票策略结合预测以提高准确性和信心。使用动态学习率微调并结合提前停止显著提升了分类准确性，尤其是在专业领域。专有模型如SciBERT和BioBERT在多种场景下均优于通用模型BERT。这些发现强调了通过数据集扩充、推理驱动的标签预测、硬投票和微调技术创建稳健且可扩展的自动化学术文本分类解决方案的有效性。 

---
# \$PINN -- a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks 

**Title (ZH)**: \$PINN —— 一种域分解方法下的贝叶斯物理含知情神经网络 

**Authors**: Júlia Vicens Figueres, Juliette Vanderhaeghen, Federica Bragone, Kateryna Morozovska, Khemraj Shukla  

**Link**: [PDF](https://arxiv.org/pdf/2504.19013)  

**Abstract**: Physics-Informed Neural Networks (PINNs) are a novel computational approach for solving partial differential equations (PDEs) with noisy and sparse initial and boundary data. Although, efficient quantification of epistemic and aleatoric uncertainties in big multi-scale problems remains challenging. We propose \$PINN a novel method of computing global uncertainty in PDEs using a Bayesian framework, by combining local Bayesian Physics-Informed Neural Networks (BPINN) with domain decomposition. The solution continuity across subdomains is obtained by imposing the flux continuity across the interface of neighboring subdomains. To demonstrate the effectiveness of \$PINN, we conduct a series of computational experiments on PDEs in 1D and 2D spatial domains. Although we have adopted conservative PINNs (cPINNs), the method can be seamlessly extended to other domain decomposition techniques. The results infer that the proposed method recovers the global uncertainty by computing the local uncertainty exactly more efficiently as the uncertainty in each subdomain can be computed concurrently. The robustness of \$PINN is verified by adding uncorrelated random noise to the training data up to 15% and testing for different domain sizes. 

**Abstract (ZH)**: 物理知情神经网络（PINNs）是一种用于解决具有嘈杂和稀疏初始及边界数据的偏微分方程（PDEs）的新计算方法。尽管在大规模问题中高效地量化认识性和统计性不确定性仍然具有挑战性。我们提出了一种新的方法\$PINN，通过结合局部贝叶斯物理知情神经网络（BPINN）与域分解，使用贝叶斯框架计算PDEs的全局不确定性。通过在相邻子域界面处施加通量连续性，获得解决方案在子域间的连续性。为了展示\$PINN的有效性，我们在一维和二维空间域上的PDEs上进行了一系列计算实验。尽管采用了保守的PINNs（cPINNs），该方法可以无缝扩展到其他域分解技术。结果表明，提出的方案通过并发计算每个子域的局部不确定性，更高效地恢复了全局不确定性。通过向训练数据添加最大15%的不相关随机噪声并测试不同域尺寸来验证\$PINN的鲁棒性。 

---
# Feature Fusion Revisited: Multimodal CTR Prediction for MMCTR Challenge 

**Title (ZH)**: 特征融合再探：多模态点击率预测挑战赛中的多模态CTR预测 

**Authors**: Junjie Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2504.18961)  

**Abstract**: With the rapid advancement of Multimodal Large Language Models (MLLMs), an increasing number of researchers are exploring their application in recommendation systems. However, the high latency associated with large models presents a significant challenge for such use cases. The EReL@MIR workshop provided a valuable opportunity to experiment with various approaches aimed at improving the efficiency of multimodal representation learning for information retrieval tasks. As part of the competition's requirements, participants were mandated to submit a technical report detailing their methodologies and findings. Our team was honored to receive the award for Task 2 - Winner (Multimodal CTR Prediction). In this technical report, we present our methods and key findings. Additionally, we propose several directions for future work, particularly focusing on how to effectively integrate recommendation signals into multimodal representations. The codebase for our implementation is publicly available at: this https URL, and the trained model weights can be accessed at: this https URL. 

**Abstract (ZH)**: 随着多模态大型语言模型（MLLMs）的迅速发展，越来越多的研究人员正在探索其在推荐系统中的应用。然而，大型模型带来的高延迟为这种应用场景提出了重大挑战。EReL@MIR研讨会提供了一个宝贵的实验机会，以改进多模态表示学习在信息检索任务中的效率。作为竞赛要求的一部分，参与者被要求提交技术报告，详细说明其方法和发现。我们团队荣幸地获得了任务2 - 获胜者（多模态点击率预测）的奖项。在本文中，我们介绍了我们的方法和关键发现，并提出了几个未来工作的方向，特别是如何有效地将推荐信号集成到多模态表示中。我们的实现代码可在以下链接公开获取：this https URL，训练模型的权重可在此链接访问：this https URL。 

---
# Application of the Brain Drain Optimization Algorithm to the N-Queens Problem 

**Title (ZH)**: 脑力流失优化算法在N后问题中的应用 

**Authors**: Sahar Ramezani Jolfaei, Sepehr Khodadadi Hossein Abadi  

**Link**: [PDF](https://arxiv.org/pdf/2504.18953)  

**Abstract**: This paper introduces the application of the Brain Drain Optimization algorithm -- a swarm-based metaheuristic inspired by the emigration of intellectual elites -- to the N-Queens problem. The N-Queens problem, a classic combinatorial optimization problem, serves as a challenge for applying the BRADO. A designed cost function guides the search, and the configurations are tuned using a TOPSIS-based multicriteria decision making process. BRADO consistently outperforms alternatives in terms of solution quality, achieving fewer threats and better objective function values. To assess BRADO's efficacy, it is benchmarked against several established metaheuristic algorithms, including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Imperialist Competitive Algorithm (ICA), Iterated Local Search (ILS), and basic Local Search (LS). The study highlights BRADO's potential as a general-purpose solver for combinatorial problems, opening pathways for future applications in other domains of artificial intelligence. 

**Abstract (ZH)**: 本文介绍了脑力精英外迁优化算法在N后皇后问题中的应用。N后皇后问题作为一种经典的组合优化问题，为应用BRADO算法提出了挑战。设计的成本函数引导搜索，配置通过基于 TOPSIS 的多准则决策过程进行调整。BRADO 在解的质量方面始终优于其他替代方法，实现了更低的威胁数和更好的目标函数值。为了评估 BRADO 的有效性，将其与多个已确立的元启发式算法（包括粒子群优化算法（PSO）、遗传算法（GA）、帝国竞争算法（ICA）、迭代局部搜索（ILS）和基本局部搜索（LS））进行了基准测试。研究突出了 BRADO 作为求解组合问题通用求解器的潜力，为人工智能的其他领域应用开辟了途径。 

---
# GPU accelerated program synthesis: Enumerate semantics, not syntax! 

**Title (ZH)**: GPU加速的程序合成：列举语义，而非语法！ 

**Authors**: Martin Berger, Nathanaël Fijalkow, Mojtaba Valizadeh  

**Link**: [PDF](https://arxiv.org/pdf/2504.18943)  

**Abstract**: Program synthesis is an umbrella term for generating programs and logical formulae from specifications. With the remarkable performance improvements that GPUs enable for deep learning, a natural question arose: can we also implement a search-based program synthesiser on GPUs to achieve similar performance improvements? In this article we discuss our insights on this question, based on recent works~. The goal is to build a synthesiser running on GPUs which takes as input positive and negative example traces and returns a logical formula accepting the positive and rejecting the negative traces. With GPU-friendly programming techniques -- using the semantics of formulae to minimise data movement and reduce data-dependent branching -- our synthesiser scales to significantly larger synthesis problems, and operates much faster than the previous CPU-based state-of-the-art. We believe the insights that make our approach GPU-friendly have wide potential for enhancing the performance of other formal methods (FM) workloads. 

**Abstract (ZH)**: 基于GPU的程序合成研究 

---
# Revisiting Transformers through the Lens of Low Entropy and Dynamic Sparsity 

**Title (ZH)**: 重新审视Transformer：从低熵和动态稀疏性的视角 

**Authors**: Ruifeng Ren, Yong Liu  

**Link**: [PDF](https://arxiv.org/pdf/2504.18929)  

**Abstract**: Compression has been a critical lens to understand the success of Transformers. In the past, we have typically taken the target distribution as a criterion to evaluate a model's compression performance. Nevertheless,it often remains challenging to precisely assess how well the model achieves compression and to compare the information content of the learned distribution with that of the target distribution during compression,as the target distribution is typically unknown and entropy computation often incurs exponential cost. In this work, we explore these issues under a controlled experimental setup. We find that Transformers exhibit a unique inductive bias in data compression: beyond approaching the target distribution, they tend to favor learning lower-entropy distributions, with this tendency becoming more pronounced as the model size increases. This preference prevents Transformers from perfectly aligning with the target distribution, instead further compressing its information content. Furthermore, we show that the FFN module plays a critical role in driving this bias. In addition, while models remove informational redundancy from data during compression, they also exhibit redundancy within their parameters, which enables compression and can be characterized through dynamic sparsity. However, the dynamic sparsity patterns in Transformers, particularly in attention and FFN modules, demand further exploration. As for this, we show that larger Transformers show stronger preferences for bypassing attention computations via residual connections and have lower proportion of active neurons. Interestingly, we also find that training instability in larger models strongly correlates with sudden increases in dead neurons. Our work contributes to a deeper understanding of Transformers from the lens of entropy and dynamic sparsity. 

**Abstract (ZH)**: 压缩是理解Transformer成功的关键视角。以往我们通常以目标分布作为评估模型压缩性能的标准。然而，精确评估模型在压缩过程中的表现及其与目标分布的信息含量比较仍然具有挑战性，因为目标分布通常未知且熵的计算往往成本高昂。在本工作中，我们通过受控实验设计探讨这些挑战。我们发现，Transformer在数据压缩方面表现出独特的归纳偏见：除了接近目标分布外，它们倾向于学习低熵分布，这种倾向随着模型规模的增大变得更加明显。这种偏好阻止了Transformer完美地与目标分布对齐，反而进一步压缩了其信息内容。此外，我们证明了前馈模块在驱动这种偏见方面扮演着关键角色。同时，虽然模型在压缩过程中从数据中移除冗余信息，但它们的参数中也存在冗余，这有助于压缩并可以通过动态稀疏性来表征。然而，Transformer，特别是在注意力和前馈模块中的动态稀疏性模式需要进一步探索。在这方面，我们展示出，更大规模的Transformer更倾向于通过残差连接绕过注意计算，且活跃神经元的比例较低。有趣的是，我们还发现，大规模模型的训练不稳定性和突然增加的死亡神经元之间存在强相关性。我们的工作从熵和动态稀疏性的视角增进了对Transformer的理解。 

---
# UnifyFL: Enabling Decentralized Cross-Silo Federated Learning 

**Title (ZH)**: UnifyFL：使跨烟囱分割的去中心化联邦学习成为可能 

**Authors**: Sarang S, Druva Dhakshinamoorthy, Aditya Shiva Sharma, Yuvraj Singh Bhadauria, Siddharth Chaitra Vivek, Arihant Bansal, Arnab K. Paul  

**Link**: [PDF](https://arxiv.org/pdf/2504.18916)  

**Abstract**: Federated Learning (FL) is a decentralized machine learning (ML) paradigm in which models are trained on private data across several devices called clients and combined at a single node called an aggregator rather than aggregating the data itself. Many organizations employ FL to have better privacy-aware ML-driven decision-making capabilities. However, organizations often operate independently rather than collaborate to enhance their FL capabilities due to the lack of an effective mechanism for collaboration. The challenge lies in balancing trust and resource efficiency. One approach relies on trusting a third-party aggregator to consolidate models from all organizations (multilevel FL), but this requires trusting an entity that may be biased or unreliable. Alternatively, organizations can bypass a third party by sharing their local models directly, which requires significant computational resources for validation. Both approaches reflect a fundamental trade-off between trust and resource constraints, with neither offering an ideal solution. In this work, we develop a trust-based cross-silo FL framework called \proj, which uses decentralized orchestration and distributed storage. \proj provides flexibility to the participating organizations and presents synchronous and asynchronous modes to handle stragglers. Our evaluation on a diverse testbed shows that \proj achieves a performance comparable to the ideal multilevel centralized FL while allowing trust and optimal use of resources. 

**Abstract (ZH)**: 联邦学习（FL）是一种分布式的机器学习（ML）范式，在这种范式中，模型在多个设备（称为客户端）上的私有数据上进行训练，并在单一节点（称为聚合器）处合并，而不是直接聚合数据本身。许多组织利用FL以拥有更好的隐私感知的ML驱动决策能力。然而，由于缺乏有效的协作机制，组织往往独立操作而不是协作以增强其FL能力。挑战在于平衡信任与资源效率之间的关系。一种方法依赖于信任第三方聚合器来汇总所有组织的模型（多层次FL），但这需要信任一个可能有偏见或不可靠的实体。或者，组织可以直接共享其本地模型，这需要大量的计算资源用于验证。这两种方法都反映了一种基本的权衡，即信任与资源限制之间的权衡，两者均未提供理想的解决方案。在本工作中，我们开发了一种基于信任的跨孤岛联邦学习框架\proj，该框架采用去中心化的编排和分布式存储。\proj为参与组织提供了灵活性，并提供了同步和异步模式来处理落后者。我们在多样化的测试床上的评估表明，\proj在实现性能与理想的多层次集中式FL相当的同时，实现了信任和资源的最优利用。 

---
# Kinship Verification through a Forest Neural Network 

**Title (ZH)**: 基于森林神经网络的亲缘关系验证 

**Authors**: Ali Nazari, Mohsen Ebrahimi Moghaddam, Omidreza Borzoei  

**Link**: [PDF](https://arxiv.org/pdf/2504.18910)  

**Abstract**: Early methods used face representations in kinship verification, which are less accurate than joint representations of parents' and children's facial images learned from scratch. We propose an approach featuring graph neural network concepts to utilize face representations and have comparable results to joint representation algorithms. Moreover, we designed the structure of the classification module and introduced a new combination of losses to engage the center loss gradually in training our network. Additionally, we conducted experiments on KinFaceW-I and II, demonstrating the effectiveness of our approach. We achieved the best result on KinFaceW-II, an average improvement of nearly 1.6 for all kinship types, and we were near the best on KinFaceW-I. The code is available at this https URL 

**Abstract (ZH)**: 早期的方法使用面部表示进行亲属验证，这些表示的准确性低于从头学习的父母和儿童面部图像的联合表示。我们提出了一种利用面部表示并能达到与联合表示算法相当效果的方法，该方法采用了图神经网络的概念。此外，我们设计了分类模块的结构，并引入了一种新的损失组合，在训练网络时逐渐引入中心损失。我们在KinFaceW-I和II上进行了实验，证明了该方法的有效性。我们在KinFaceW-II上取得了最佳结果，各类亲属关系的平均改进接近1.6分，KinFaceW-I上接近最佳结果。代码可访问此链接。 

---
# SPD Learning for Covariance-Based Neuroimaging Analysis: Perspectives, Methods, and Challenges 

**Title (ZH)**: 基于协方差的神经影像学分析中SPD学习：视角、方法与挑战 

**Authors**: Ce Ju, Reinmar J. Kobler, Antoine Collas, Motoaki Kawanabe, Cuntai Guan, Bertrand Thirion  

**Link**: [PDF](https://arxiv.org/pdf/2504.18882)  

**Abstract**: Neuroimaging provides a critical framework for characterizing brain activity by quantifying connectivity patterns and functional architecture across modalities. While modern machine learning has significantly advanced our understanding of neural processing mechanisms through these datasets, decoding task-specific signatures must contend with inherent neuroimaging constraints, for example, low signal-to-noise ratios in raw electrophysiological recordings, cross-session non-stationarity, and limited sample sizes. This review focuses on machine learning approaches for covariance-based neuroimaging data, where often symmetric positive definite (SPD) matrices under full-rank conditions encode inter-channel relationships. By equipping the space of SPD matrices with Riemannian metrics (e.g., affine-invariant or log-Euclidean), their space forms a Riemannian manifold enabling geometric analysis. We unify methodologies operating on this manifold under the SPD learning framework, which systematically leverages the SPD manifold's geometry to process covariance features, thereby advancing brain imaging analytics. 

**Abstract (ZH)**: 神经成像为通过量化跨模态连接模式和功能架构来表征脑活动提供了关键框架。尽管现代机器学习显著提升了我们对神经处理机制的理解，解码任务特定特征必须应对固有的神经成像约束，例如原始电生理记录中的低信噪比、会话间的非平稳性和样本量有限。本文综述了基于协方差的神经成像数据的机器学习方法，其中通常在满秩条件下，对称正定（SPD）矩阵编码跨通道关系。通过赋予SPD矩阵流形度量（如仿射不变或对数欧几里得），这些流形形成Riemann流形，使其能够进行几何分析。本文在SPD学习框架下统一了在该流形上操作的方法，系统地利用SPD流形的几何特性处理协方差特征，从而推动脑成像分析的进步。 

---
# TSRM: A Lightweight Temporal Feature Encoding Architecture for Time Series Forecasting and Imputation 

**Title (ZH)**: TSRM：一种轻量级时间特征编码架构用于时间序列 forecasting 和 imputation 

**Authors**: Robert Leppich, Michael Stenger, Daniel Grillmeyer, Vanessa Borst, Samuel Kounev  

**Link**: [PDF](https://arxiv.org/pdf/2504.18878)  

**Abstract**: We introduce a temporal feature encoding architecture called Time Series Representation Model (TSRM) for multivariate time series forecasting and imputation. The architecture is structured around CNN-based representation layers, each dedicated to an independent representation learning task and designed to capture diverse temporal patterns, followed by an attention-based feature extraction layer and a merge layer, designed to aggregate extracted features. The architecture is fundamentally based on a configuration that is inspired by a Transformer encoder, with self-attention mechanisms at its core. The TSRM architecture outperforms state-of-the-art approaches on most of the seven established benchmark datasets considered in our empirical evaluation for both forecasting and imputation tasks. At the same time, it significantly reduces complexity in the form of learnable parameters. The source code is available at this https URL. 

**Abstract (ZH)**: 一种用于多变量时间序列预测和填充的时间序列表示模型（TSRM）的时间特征编码架构 

---
# Predicting Stress in Two-phase Random Materials and Super-Resolution Method for Stress Images by Embedding Physical Information 

**Title (ZH)**: 两相随机材料中的应力预测及嵌入物理信息的超分辨率应力图像方法 

**Authors**: Tengfei Xing, Xiaodan Ren, Jie Li  

**Link**: [PDF](https://arxiv.org/pdf/2504.18854)  

**Abstract**: Stress analysis is an important part of material design. For materials with complex microstructures, such as two-phase random materials (TRMs), material failure is often accompanied by stress concentration. Phase interfaces in two-phase materials are critical for stress concentration. Therefore, the prediction error of stress at phase boundaries is crucial. In practical engineering, the pixels of the obtained material microstructure images are limited, which limits the resolution of stress images generated by deep learning methods, making it difficult to observe stress concentration regions. Existing Image Super-Resolution (ISR) technologies are all based on data-driven supervised learning. However, stress images have natural physical constraints, which provide new ideas for new ISR technologies. In this study, we constructed a stress prediction framework for TRMs. First, the framework uses a proposed Multiple Compositions U-net (MC U-net) to predict stress in low-resolution material microstructures. By considering the phase interface information of the microstructure, the MC U-net effectively reduces the problem of excessive prediction errors at phase boundaries. Secondly, a Mixed Physics-Informed Neural Network (MPINN) based method for stress ISR (SRPINN) was proposed. By introducing the constraints of physical information, the new method does not require paired stress images for training and can increase the resolution of stress images to any multiple. This enables a multiscale analysis of the stress concentration regions at phase boundaries. Finally, we performed stress analysis on TRMs with different phase volume fractions and loading states through transfer learning. The results show the proposed stress prediction framework has satisfactory accuracy and generalization ability. 

**Abstract (ZH)**: 材料应力分析是材料设计中的重要组成部分。对于具有复杂微观结构的材料，如两相随机材料（TRMs），材料失效往往伴随着应力集中。两相材料中的相界面是应力集中关键。因此，相界面处应力预测的误差至关重要。在实际工程中，获取的材料微观结构图像的像素有限，限制了深度学习方法生成的应力图像的分辨率，使得难以观察应力集中区域。现有的图像超分辨率（ISR）技术都是基于数据驱动的监督学习。然而，应力图像具有自然的物理约束，为新的ISR技术提供了新思路。在本研究中，我们构建了TRMs的应力预测框架。首先，框架采用提出的多重组成U-net（MC U-net）预测低分辨率材料微观结构的应力。通过考虑微观结构的相界面信息，MC U-net有效减少了相界面处过度预测误差的问题。其次，提出了基于混合物理信息神经网络（MPINN）的方法进行应力ISR（SRPINN）。通过引入物理信息的约束，新方法不需要成对的应力图像进行训练，可以将应力图像的分辨率提升至任何倍数，从而实现相界面应力集中区域的多尺度分析。最后，通过迁移学习对不同相体积分数和加载状态的TRMs进行应力分析。结果显示，所提出的应力预测框架具有满意的准确性和泛化能力。 

---
# Introducing Interval Neural Networks for Uncertainty-Aware System Identification 

**Title (ZH)**: 引入区间神经网络实现不确定性感知系统辨识 

**Authors**: Mehmet Ali Ferah, Tufan Kumbasar  

**Link**: [PDF](https://arxiv.org/pdf/2504.18845)  

**Abstract**: System Identification (SysID) is crucial for modeling and understanding dynamical systems using experimental data. While traditional SysID methods emphasize linear models, their inability to fully capture nonlinear dynamics has driven the adoption of Deep Learning (DL) as a more powerful alternative. However, the lack of uncertainty quantification (UQ) in DL-based models poses challenges for reliability and safety, highlighting the necessity of incorporating UQ. This paper introduces a systematic framework for constructing and learning Interval Neural Networks (INNs) to perform UQ in SysID tasks. INNs are derived by transforming the learnable parameters (LPs) of pre-trained neural networks into interval-valued LPs without relying on probabilistic assumptions. By employing interval arithmetic throughout the network, INNs can generate Prediction Intervals (PIs) that capture target coverage effectively. We extend Long Short-Term Memory (LSTM) and Neural Ordinary Differential Equations (Neural ODEs) into Interval LSTM (ILSTM) and Interval NODE (INODE) architectures, providing the mathematical foundations for their application in SysID. To train INNs, we propose a DL framework that integrates a UQ loss function and parameterization tricks to handle constraints arising from interval LPs. We introduce novel concept "elasticity" for underlying uncertainty causes and validate ILSTM and INODE in SysID experiments, demonstrating their effectiveness. 

**Abstract (ZH)**: 基于区间神经网络的系统辨识不确定性量化框架 

---
# Preserving Seasonal and Trend Information: A Variational Autoencoder-Latent Space Arithmetic Based Approach for Non-stationary Learning 

**Title (ZH)**: 保留季节性和趋势信息：基于变分自编码器潜在空间算术的非平稳学习方法 

**Authors**: Hassan Wasswa, Aziida Nanyonga, Timothy Lynar  

**Link**: [PDF](https://arxiv.org/pdf/2504.18819)  

**Abstract**: AI models have garnered significant research attention towards predictive task automation. However, a stationary training environment is an underlying assumption for most models and such models simply do not work on non-stationary data since a stationary relationship is learned. The existing solutions propose making data stationary prior to model training and evaluation. This leads to loss of trend and seasonal patterns which are vital components for learning temporal dependencies of the system under study. This research aims to address this limitation by proposing a method for enforcing stationary behaviour within the latent space while preserving trend and seasonal information. The method deploys techniques including Differencing, Time-series decomposition, and Latent Space Arithmetic (LSA), to learn information vital for efficient approximation of trend and seasonal information which is then stored as embeddings within the latent space of a Variational Autoencoder (VAE). The approach's ability to preserve trend and seasonal information was evaluated on two time-series non-stationary datasets. For predictive performance evaluation, four deep learning models were trained on the latent vector representations of the datasets after application of the proposed method and all models produced competitive results in comparison with state-of-the-art techniques using RMSE as the performance metric. 

**Abstract (ZH)**: AI模型在时变数据上实现预测任务自动化的非站定行为建模与分析 

---
# Zero-Day Botnet Attack Detection in IoV: A Modular Approach Using Isolation Forests and Particle Swarm Optimization 

**Title (ZH)**: 基于孤立森林和粒子群优化的模块化IoV中零日恶意软件攻击检测方法 

**Authors**: Abdelaziz Amara korba, Nour Elislem Karabadji, Yacine Ghamri-Doudane  

**Link**: [PDF](https://arxiv.org/pdf/2504.18814)  

**Abstract**: The Internet of Vehicles (IoV) is transforming transportation by enhancing connectivity and enabling autonomous driving. However, this increased interconnectivity introduces new security vulnerabilities. Bot malware and cyberattacks pose significant risks to Connected and Autonomous Vehicles (CAVs), as demonstrated by real-world incidents involving remote vehicle system compromise. To address these challenges, we propose an edge-based Intrusion Detection System (IDS) that monitors network traffic to and from CAVs. Our detection model is based on a meta-ensemble classifier capable of recognizing known (Nday) attacks and detecting previously unseen (zero-day) attacks. The approach involves training multiple Isolation Forest (IF) models on Multi-access Edge Computing (MEC) servers, with each IF specialized in identifying a specific type of botnet attack. These IFs, either trained locally or shared by other MEC nodes, are then aggregated using a Particle Swarm Optimization (PSO) based stacking strategy to construct a robust meta-classifier. The proposed IDS has been evaluated on a vehicular botnet dataset, achieving an average detection rate of 92.80% for N-day attacks and 77.32% for zero-day attacks. These results highlight the effectiveness of our solution in detecting both known and emerging threats, providing a scalable and adaptive defense mechanism for CAVs within the IoV ecosystem. 

**Abstract (ZH)**: 车辆互联网（IoV）通过增强连接性与实现自动驾驶正在重塑交通运输。然而，这种增加的互联性引入了新的安全漏洞。僵尸网络恶意软件和网络攻击对连接和自动驾驶车辆（CAVs）构成了显著风险，如实际案例中远程车辆系统被攻破所展示的。为应对这些挑战，我们提出一种基于边缘的入侵检测系统（IDS），该系统监控连接和自动驾驶车辆（CAV）的网络流量。我们的检测模型基于一个元集成分类器，能够识别已知（Nday）攻击并检测已知未知（零日）攻击。该方法涉及在多接入边缘计算（MEC）服务器上训练多个孤立森林（IF）模型，每个IF专门用于识别特定类型的僵尸网络攻击。这些IF，无论是本地训练还是由其他MEC节点共享，然后通过基于粒子群优化（PSO）的堆叠策略进行聚合，构建一个稳健的元分类器。所提出的IDS在 vehicular botnet 数据集上进行了评估，对于已知攻击（N-day）的平均检测率为92.80%，对于未知攻击（零日）的检测率为77.32%。这些结果突显了我们解决方案在检测已知和新兴威胁方面的有效性，为IoV生态系统中的CAVs提供了可扩展且适应性强的防御机制。 

---
# Clones in the Machine: A Feminist Critique of Agency in Digital Cloning 

**Title (ZH)**: 机器中的克隆体：对数字克隆体中自主性的女性主义批判 

**Authors**: Siân Brooke  

**Link**: [PDF](https://arxiv.org/pdf/2504.18807)  

**Abstract**: This paper critiques digital cloning in academic research, highlighting how it exemplifies AI solutionism. Digital clones, which replicate user data to simulate behavior, are often seen as scalable tools for behavioral insights. However, this framing obscures ethical concerns around consent, agency, and representation. Drawing on feminist theories of agency, the paper argues that digital cloning oversimplifies human complexity and risks perpetuating systemic biases. To address these issues, it proposes decentralized data repositories and dynamic consent models, promoting ethical, context-aware AI practices that challenge the reductionist logic of AI solutionism 

**Abstract (ZH)**: 本文批评学术研究中的数字克隆现象，指出其如何体现人工智能解决方案主义。数字克隆通过复制用户数据以模拟行为，常被视为行为洞察的可扩展工具。然而，这种框架遮蔽了关于同意、自主权和代表性的伦理关切。本文结合女性主义自主权理论，认为数字克隆过分简化了人类的复杂性，并可能导致系统性偏见的持续存在。为应对这些问题，本文提议建立去中心化的数据存储库和动态同意模型，促进符合伦理、情境感知的人工智能实践，挑战人工智能解决方案主义的简化逻辑。 

---
# Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation 

**Title (ZH)**: 从创作者窃取工作流程：一种基于创作者启发的代理框架，结合迭代反馈循环以提高科学简短形式生成效果 

**Authors**: Jong Inn Park, Maanas Taneja, Qianwen Wang, Dongyeop Kang  

**Link**: [PDF](https://arxiv.org/pdf/2504.18805)  

**Abstract**: Generating engaging, accurate short-form videos from scientific papers is challenging due to content complexity and the gap between expert authors and readers. Existing end-to-end methods often suffer from factual inaccuracies and visual artifacts, limiting their utility for scientific dissemination. To address these issues, we propose SciTalk, a novel multi-LLM agentic framework, grounding videos in various sources, such as text, figures, visual styles, and avatars. Inspired by content creators' workflows, SciTalk uses specialized agents for content summarization, visual scene planning, and text and layout editing, and incorporates an iterative feedback mechanism where video agents simulate user roles to give feedback on generated videos from previous iterations and refine generation prompts. Experimental evaluations show that SciTalk outperforms simple prompting methods in generating scientifically accurate and engaging content over the refined loop of video generation. Although preliminary results are still not yet matching human creators' quality, our framework provides valuable insights into the challenges and benefits of feedback-driven video generation. Our code, data, and generated videos will be publicly available. 

**Abstract (ZH)**: 从科学论文生成具有吸引力且准确的短格式视频具有挑战性，由于内容复杂性和专家作者与读者之间的差距。现有的端到端方法往往存在事实不准确和视觉伪影的问题，限制了其在科学传播中的应用。为解决这些问题，我们提出SciTalk，一种新颖的多LLM代理框架，基于多种来源，如文本、图表、视觉风格和虚拟形象。受内容创作者工作流程的启发，SciTalk使用专门的代理进行内容摘要、视觉场景规划和文本及布局编辑，并嵌入迭代反馈机制，其中视频代理模拟用户角色，对之前迭代生成的视频给予反馈并精炼生成提示。实验评估表明，SciTalk在生成科学准确且具吸引力的内容方面优于简单的提示方法。尽管初步结果尚未达到人类创作者的质量水平，但我们的框架为基于反馈的视频生成的挑战和益处提供了宝贵的见解。我们的代码、数据和生成视频将公开提供。 

---
# World Food Atlas Project 

**Title (ZH)**: 世界食物地图集项目 

**Authors**: Ali Rostami, Z Xie, A Ishino, Y Yamakata, K Aizawa, Ramesh Jain  

**Link**: [PDF](https://arxiv.org/pdf/2504.18727)  

**Abstract**: A coronavirus pandemic is forcing people to be "at home" all over the world. In a life of hardly ever going out, we would have realized how the food we eat affects our bodies. What can we do to know our food more and control it better? To give us a clue, we are trying to build a World Food Atlas (WFA) that collects all the knowledge about food in the world. In this paper, we present two of our trials. The first is the Food Knowledge Graph (FKG), which is a graphical representation of knowledge about food and ingredient relationships derived from recipes and food nutrition data. The second is the FoodLog Athl and the RecipeLog that are applications for collecting people's detailed records about food habit. We also discuss several problems that we try to solve to build the WFA by integrating these two ideas. 

**Abstract (ZH)**: 一场冠状病毒 pandemic 正迫使全世界的人们“宅”在家里。在这种几乎不出门的生活状态下，我们意识到饮食对身体的影响。我们能做些什么来更好地了解食物并控制自己的饮食？为了给我们提供一些线索，我们正在尝试构建一个名为全球食物图谱（WFA）的知识库，收集世界各地所有关于食物的知识。在本文中，我们介绍了这一尝试的两个方面：第一个是食物知识图谱（FKG），它基于食谱和营养数据，以图形方式表示食物及其成分之间的关系；第二个是食记应用和食谱记录应用，用于收集人们的详细饮食习惯记录。我们还讨论了一些我们通过整合这两种方法试图解决的问题。 

---
# Explicit neural network classifiers for non-separable data 

**Title (ZH)**: 显式神经网络分类器用于非可分数据 

**Authors**: Patrícia Muñoz Ewald  

**Link**: [PDF](https://arxiv.org/pdf/2504.18710)  

**Abstract**: We fully characterize a large class of feedforward neural networks in terms of truncation maps. As an application, we show how a ReLU neural network can implement a feature map which separates concentric data. 

**Abstract (ZH)**: 我们从裁剪映射的角度完全刻画了一类前馈神经网络。作为应用，我们展示了ReLU神经网络如何实现一个能够分离同心数据的特征映射。 

---
# A Gradient-Optimized TSK Fuzzy Framework for Explainable Phishing Detection 

**Title (ZH)**: 一种基于梯度优化的TSK模糊框架，用于可解释的钓鱼检测 

**Authors**: Lohith Srikanth Pentapalli, Jon Salisbury, Josette Riep, Kelly Cohen  

**Link**: [PDF](https://arxiv.org/pdf/2504.18636)  

**Abstract**: Phishing attacks represent an increasingly sophisticated and pervasive threat to individuals and organizations, causing significant financial losses, identity theft, and severe damage to institutional reputations. Existing phishing detection methods often struggle to simultaneously achieve high accuracy and explainability, either failing to detect novel attacks or operating as opaque black-box models. To address this critical gap, we propose a novel phishing URL detection system based on a first-order Takagi-Sugeno-Kang (TSK) fuzzy inference model optimized through gradient-based techniques. Our approach intelligently combines the interpretability and human-like reasoning capabilities of fuzzy logic with the precision and adaptability provided by gradient optimization methods, specifically leveraging the Adam optimizer for efficient parameter tuning. Experiments conducted using a comprehensive dataset of over 235,000 URLs demonstrate rapid convergence, exceptional predictive performance (accuracy averaging 99.95% across 5 cross-validation folds, with a perfect AUC i.e. 1.00). Furthermore, optimized fuzzy rules and membership functions improve interoperability, clearly indicating how the model makes decisions - an essential feature for cybersecurity applications. This high-performance, transparent, and interpretable phishing detection framework significantly advances current cybersecurity defenses, providing practitioners with accurate and explainable decision-making tools. 

**Abstract (ZH)**: 基于梯度优化的Takagi-Sugeno-Kang模糊推理模型的钓鱼URL检测系统 

---
# The Philosophic Turn for AI Agents: Replacing centralized digital rhetoric with decentralized truth-seeking 

**Title (ZH)**: AI代理的哲学转向：用去中心化的真理探寻取代集中化的数字化修辞 

**Authors**: Philipp Koralus  

**Link**: [PDF](https://arxiv.org/pdf/2504.18601)  

**Abstract**: In the face of rapidly advancing AI technology, individuals will increasingly rely on AI agents to navigate life's growing complexities, raising critical concerns about maintaining both human agency and autonomy. This paper addresses a fundamental dilemma posed by AI decision-support systems: the risk of either becoming overwhelmed by complex decisions, thus losing agency, or having autonomy compromised by externally controlled choice architectures reminiscent of ``nudging'' practices. While the ``nudge'' framework, based on the use of choice-framing to guide individuals toward presumed beneficial outcomes, initially appeared to preserve liberty, at AI-driven scale, it threatens to erode autonomy. To counteract this risk, the paper proposes a philosophic turn in AI design. AI should be constructed to facilitate decentralized truth-seeking and open-ended inquiry, mirroring the Socratic method of philosophical dialogue. By promoting individual and collective adaptive learning, such AI systems would empower users to maintain control over their judgments, augmenting their agency without undermining autonomy. The paper concludes by outlining essential features for autonomy-preserving AI systems, sketching a path toward AI systems that enhance human judgment rather than undermine it. 

**Abstract (ZH)**: 面对迅速发展的AI技术，个体将越来越多地依赖AI代理来应对生活日益复杂化的问题，从而引起了保持人类自主性和行动能力的关键关注。本文探讨了AI决策支持系统引发的基本困境：要么因决策过于复杂而失去自主性，要么因外部控制的选择架构受到“助推”实践的影响而自主性被削弱。虽然“助推”框架最初似乎能够保护自由，但在AI驱动的大规模应用中，它威胁着削弱自主性。为了应对这一风险，本文提出在AI设计中体现哲学转向。AI应该被设计成促进分散式真理探寻和开放性探究，模仿苏格拉底式的哲学对话方法。通过促进个体和集体的适应性学习，这样的AI系统将使用户能够维持对其判断的控制，增强其自主性而不削弱其自主性。本文最后概述了保护自主性的AI系统的关键特征，勾勒出一条旨在增强而非削弱人类判断力的AI系统的发展路径。 

---
# Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines 

**Title (ZH)**: 使用合成数据和可配置扰动管道优化隐私-效用平衡 

**Authors**: Anantha Sharma, Swetha Devabhaktuni, Eklove Mohan  

**Link**: [PDF](https://arxiv.org/pdf/2504.18596)  

**Abstract**: This paper explores the strategic use of modern synthetic data generation and advanced data perturbation techniques to enhance security, maintain analytical utility, and improve operational efficiency when managing large datasets, with a particular focus on the Banking, Financial Services, and Insurance (BFSI) sector. We contrast these advanced methods encompassing generative models like GANs, sophisticated context-aware PII transformation, configurable statistical perturbation, and differential privacy with traditional anonymization approaches.
The goal is to create realistic, privacy-preserving datasets that retain high utility for complex machine learning tasks and analytics, a critical need in the data-sensitive industries like BFSI, Healthcare, Retail, and Telecommunications. We discuss how these modern techniques potentially offer significant improvements in balancing privacy preservation while maintaining data utility compared to older methods. Furthermore, we examine the potential for operational gains, such as reduced overhead and accelerated analytics, by using these privacy-enhanced datasets. We also explore key use cases where these methods can mitigate regulatory risks and enable scalable, data-driven innovation without compromising sensitive customer information. 

**Abstract (ZH)**: 本文探讨了利用现代合成数据生成和高级数据扰动技术的 estratégical 使用，以增强大型数据集的管理安全性、保持分析效用并提高运营效率，特别关注银行、金融服务和保险 (BFSI) 行业。我们对比了包括生成模型（如 GANs）、复杂的上下文感知 PII 转换、可配置的统计扰动和差分隐私在内的这些先进方法与传统匿名化方法。目标是创建现实且保护隐私的数据集，这些数据集对复杂的机器学习任务和分析具有高效用，这是在敏感数据行业（如 BFSI、医疗保健、零售和电信）中的一项关键需求。本文讨论了这些现代技术如何在保持数据效用的同时提供更强大的隐私保护，并探讨了使用这些增强隐私的数据集可能带来的运营收益，如减少开销和加速分析。此外，本文还探讨了这些方法在降低监管风险、促进可扩展的数据驱动创新方面的关键应用场景，同时不泄露敏感客户信息。 

---
# EnviroPiNet: A Physics-Guided AI Model for Predicting Biofilter Performance 

**Title (ZH)**: EnviroPiNet: 一个基于物理指导的AI模型，用于预测生物过滤器性能 

**Authors**: Uzma, Fabien Cholet, Domenic Quinn, Cindy Smith, Siming You, William Sloan  

**Link**: [PDF](https://arxiv.org/pdf/2504.18595)  

**Abstract**: Environmental biotechnologies, such as drinking water biofilters, rely on complex interactions between microbial communities and their surrounding physical-chemical environments. Predicting the performance of these systems is challenging due to high-dimensional, sparse datasets that lack diversity and fail to fully capture system behaviour. Accurate predictive models require innovative, science-guided approaches. In this study, we present the first application of Buckingham Pi theory to modelling biofilter performance. This dimensionality reduction technique identifies meaningful, dimensionless variables that enhance predictive accuracy and improve model interpretability. Using these variables, we developed the Environmental Buckingham Pi Neural Network (EnviroPiNet), a physics-guided model benchmarked against traditional data-driven methods, including Principal Component Analysis (PCA) and autoencoder neural networks. Our findings demonstrate that the EnviroPiNet model achieves an R^2 value of 0.9236 on the testing dataset, significantly outperforming PCA and autoencoder methods. The Buckingham Pi variables also provide insights into the physical and chemical relationships governing biofilter behaviour, with implications for system design and optimization. This study highlights the potential of combining physical principles with AI approaches to model complex environmental systems characterized by sparse, high-dimensional datasets. 

**Abstract (ZH)**: 环境生物技术，如饮用水生物过滤器，依赖于微生物群落与其周围物理-化学环境之间的复杂相互作用。由于存在高维度、稀疏且缺乏多样性的数据集，这些系统的表现预测具有挑战性。准确的预测模型需要创新的、以科学为导向的方法。在本研究中，我们首次将巴克莱派理论应用于生物过滤器性能建模。这一维数缩减技术识别出有意义的无量纲变量，从而提高预测精度并增强模型可解释性。利用这些变量，我们开发了环境巴克莱派神经网络（EnviroPiNet），这是一种以物理为导向的模型，与传统的数据驱动方法（包括主成分分析PCA和自动编码神经网络）进行了基准测试。我们的研究结果表明，EnviroPiNet模型在测试数据集上的R²值为0.9236，显著优于PCA和自动编码方法。巴克莱派变量还提供了指导生物过滤器行为的物理和化学关系的见解，对于系统设计和优化具有重要意义。本研究突显了将物理原理与人工智能方法结合以建模具有稀疏、高维度特征的复杂环境系统的潜力。 

---
# A Simple DropConnect Approach to Transfer-based Targeted Attack 

**Title (ZH)**: 基于迁移的 targeted 攻击的一种简单 DropConnect 方法 

**Authors**: Tongrui Su, Qingbin Li, Shengyu Zhu, Wei Chen, Xueqi Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2504.18594)  

**Abstract**: We study the problem of transfer-based black-box attack, where adversarial samples generated using a single surrogate model are directly applied to target models. Compared with untargeted attacks, existing methods still have lower Attack Success Rates (ASRs) in the targeted setting, i.e., the obtained adversarial examples often overfit the surrogate model but fail to mislead other models. In this paper, we hypothesize that the pixels or features in these adversarial examples collaborate in a highly dependent manner to maximize the success of an adversarial attack on the surrogate model, which we refer to as perturbation co-adaptation. Then, we propose to Mitigate perturbation Co-adaptation by DropConnect (MCD) to enhance transferability, by creating diverse variants of surrogate model at each optimization iteration. We conduct extensive experiments across various CNN- and Transformer-based models to demonstrate the effectiveness of MCD. In the challenging scenario of transferring from a CNN-based model to Transformer-based models, MCD achieves 13% higher average ASRs compared with state-of-the-art baselines. MCD boosts the performance of self-ensemble methods by bringing in more diversification across the variants while reserving sufficient semantic information for each variant. In addition, MCD attains the highest performance gain when scaling the compute of crafting adversarial examples. 

**Abstract (ZH)**: 基于转移的黑盒攻击问题研究：削弱扰动共适应以提高转移性 

---
# Severity Classification of Chronic Obstructive Pulmonary Disease in Intensive Care Units: A Semi-Supervised Approach Using MIMIC-III Dataset 

**Title (ZH)**: 重症监护病房中慢性阻塞性肺疾病严重程度分类：基于MIMIC-III数据集的半监督方法 

**Authors**: Akram Shojaei, Mehdi Delrobaei  

**Link**: [PDF](https://arxiv.org/pdf/2504.18593)  

**Abstract**: Chronic obstructive pulmonary disease (COPD) represents a significant global health burden, where precise severity assessment is particularly critical for effective clinical management in intensive care unit (ICU) settings. This study introduces an innovative machine learning framework for COPD severity classification utilizing the MIMIC-III critical care database, thereby expanding the applications of artificial intelligence in critical care medicine. Our research developed a robust classification model incorporating key ICU parameters such as blood gas measurements and vital signs, while implementing semi-supervised learning techniques to effectively utilize unlabeled data and enhance model performance. The random forest classifier emerged as particularly effective, demonstrating exceptional discriminative capability with 92.51% accuracy and 0.98 ROC AUC in differentiating between mild-to-moderate and severe COPD cases. This machine learning approach provides clinicians with a practical, accurate, and efficient tool for rapid COPD severity evaluation in ICU environments, with significant potential to improve both clinical decision-making processes and patient outcomes. Future research directions should prioritize external validation across diverse patient populations and integration with clinical decision support systems to optimize COPD management in critical care settings. 

**Abstract (ZH)**: 慢性阻塞性肺病（COPD）代表了全球重要的公共卫生负担，精准的病情严重程度评估在重症监护病房（ICU）的有效临床管理中尤为关键。本研究引入了一种基于MIMIC-III重症监护数据库的创新机器学习框架，以用于COPD严重程度分类，从而扩展了人工智能在重症监护医学中的应用。我们的研究开发了一个稳健的分类模型，结合了关键的ICU参数，如血液气体测量和生命体征，并采用半监督学习技术有效利用未标记数据以提升模型性能。随机森林分类器表现尤为突出，其在区分轻中度与重度COPD病例方面展现了卓越的辨别能力，准确率高达92.51%，ROC AUC值为0.98。这种机器学习方法为重症监护环境中的临床医生提供了切实可行、准确且高效的COPD严重程度评估工具，有望显著改善临床决策过程和患者预后。未来的研究方向应侧重于在多元患者群体中的外部验证，并与临床决策支持系统集成，以优化重症监护环境中COPD的管理。 

---
# Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations 

**Title (ZH)**: 基于几何感知的稳态偏微分方程的等变神经场表示推理 

**Authors**: Giovanni Catalani, Michael Bauerheim, Frédéric Tost, Xavier Bertrand, Joseph Morlier  

**Link**: [PDF](https://arxiv.org/pdf/2504.18591)  

**Abstract**: Recent advances in Neural Fields have enabled powerful, discretization-invariant methods for learning neural operators that approximate solutions of Partial Differential Equations (PDEs) on general geometries. Building on these developments, we introduce enf2enf, an encoder--decoder methodology for predicting steady-state Partial Differential Equations with non-parameterized geometric variability, based on recently proposed Equivariant Neural Field architectures. In enf2enf, input geometries are encoded into latent point cloud embeddings that inherently preserve geometric grounding and capture local phenomena. The resulting representations are then combined with global parameters and directly decoded into continuous output fields, thus efficiently modeling the coupling between geometry and physics. By leveraging the inductive biases of locality and translation invariance, our approach is able to capture fine-scale physical features as well as complex shape variations, thereby enhancing generalization and physical compliance. Extensive experiments on a high-fidelity aerodynamic dataset, a hyper-elastic material benchmark, and multi-element airfoil geometries, demonstrate that the proposed model achieves superior or competitive performance compared to state-of-the-art graph based, operator learning, and neural field methods. Notably, our method supports real time inference and zero-shot super-resolution, enabling efficient training on low-resolution meshes while maintaining high accuracy on full-scale discretizations. 

**Abstract (ZH)**: Recent advances in Neural Fields have enabled powerful, discretization-invariant methods for learning neural operators that approximate solutions of Partial Differential Equations (PDEs) on general geometries. Building on these developments, we introduce enf2enf, an encoder--decoder methodology for predicting steady-state Partial Differential Equations with non-parameterized geometric variability, based on recently proposed Equivariant Neural Field architectures. 

---
# A multilevel approach to accelerate the training of Transformers 

**Title (ZH)**: 多级方法加速Transformer训练 

**Authors**: Guillaume Lauga, Maël Chaumette, Edgar Desainte-Maréville, Étienne Lasalle, Arthur Lebeurrier  

**Link**: [PDF](https://arxiv.org/pdf/2504.18590)  

**Abstract**: In this article, we investigate the potential of multilevel approaches to accelerate the training of transformer architectures. Using an ordinary differential equation (ODE) interpretation of these architectures, we propose an appropriate way of varying the discretization of these ODE Transformers in order to accelerate the training. We validate our approach experimentally by a comparison with the standard training procedure. 

**Abstract (ZH)**: 本文探讨了多层方法在加速变压器架构训练中的潜在应用。通过将这些架构解释为常微分方程（ODE），我们提出了一种适当的方法，用于改变这些ODE变压器的离散化，以加速训练过程。我们通过与标准训练程序的比较进行实验验证。 

---
# Dynamic QoS Prediction via a Non-Negative Tensor Snowflake Factorization 

**Title (ZH)**: 基于非负张量雪花分解的动态QoS预测 

**Authors**: YongHui Xia, Lan Wang, Hao Wu  

**Link**: [PDF](https://arxiv.org/pdf/2504.18588)  

**Abstract**: Dynamic quality of service (QoS) data exhibit rich temporal patterns in user-service interactions, which are crucial for a comprehensive understanding of user behavior and service conditions in Web service. As the number of users and services increases, there is a large amount of unobserved QoS data, which significantly affects users'choice of services. To predict unobserved QoS data, we propose a Non-negative Snowflake Factorization of tensors model. This method designs a snowflake core tensor to enhance the model's learning capability. Additionally, it employs a single latent factor-based, nonnegative multiplication update on tensor (SLF-NMUT) for parameter learning. Empirical results demonstrate that the proposed model more accurately learns dynamic user-service interaction patterns, thereby yielding improved predictions for missing QoS data. 

**Abstract (ZH)**: 动态服务质量(QoS)数据在用户服务交互中表现出丰富的时空模式，对于全面理解和掌握用户行为和服务条件至关重要。随着用户和服务数量的增加，存在大量未观测到的QoS数据，这显著影响了用户的服务选择。为了预测未观测到的QoS数据，我们提出了一种张量的非负雪flate因式分解模型。该方法设计了一个雪flate核心张量以增强模型的学习能力，并采用基于单个潜在因子的非负矩阵更新（SLF-NMUT）进行参数学习。实证结果表明，所提出模型能够更准确地学习动态用户服务交互模式，从而提高对缺失QoS数据的预测准确性。 

---
# WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks 

**Title (ZH)**: WASP: 测试网络代理安全性对抗提示注入攻击 

**Authors**: Ivan Evtimov, Arman Zharmagambetov, Aaron Grattafiori, Chuan Guo, Kamalika Chaudhuri  

**Link**: [PDF](https://arxiv.org/pdf/2504.18575)  

**Abstract**: Web navigation AI agents use language-and-vision foundation models to enhance productivity but these models are known to be susceptible to indirect prompt injections that get them to follow instructions different from the legitimate user's. Existing explorations of this threat applied to web agents often focus on a single isolated adversarial goal, test with injected instructions that are either too easy or not truly malicious, and often give the adversary unreasonable access. In order to better focus adversarial research, we construct a new benchmark called WASP (Web Agent Security against Prompt injection attacks) that introduces realistic web agent hijacking objectives and an isolated environment to test them in that does not affect real users or the live web. As part of WASP, we also develop baseline attacks against three popular web agentic systems (VisualWebArena, Claude Computer Use, and Operator) instantiated with various state-of-the-art models. Our evaluation shows that even AI agents backed by models with advanced reasoning capabilities and by models with instruction hierarchy mitigations are susceptible to low-effort human-written prompt injections. However, the realistic objectives in WASP also allow us to observe that agents are currently not capable enough to complete the goals of attackers end-to-end. Agents begin executing the adversarial instruction between 16 and 86% of the time but only achieve the goal between 0 and 17% of the time. Based on these findings, we argue that adversarial researchers should demonstrate stronger attacks that more consistently maintain control over the agent given realistic constraints on the adversary's power. 

**Abstract (ZH)**: Web Agent Security against Prompt Injection Attacks 

---
# Feature Selection via GANs (GANFS): Enhancing Machine Learning Models for DDoS Mitigation 

**Title (ZH)**: 基于GAN的特征选择（GANFS）：增强DDoS缓解的机器学习模型性能 

**Authors**: Harsh Patel  

**Link**: [PDF](https://arxiv.org/pdf/2504.18566)  

**Abstract**: Distributed Denial of Service (DDoS) attacks represent a persistent and evolving threat to modern networked systems, capable of causing large-scale service disruptions. The complexity of such attacks, often hidden within high-dimensional and redundant network traffic data, necessitates robust and intelligent feature selection techniques for effective detection. Traditional methods such as filter-based, wrapper-based, and embedded approaches, each offer strengths but struggle with scalability or adaptability in complex attack environments. In this study, we explore these existing techniques through a detailed comparative analysis and highlight their limitations when applied to large-scale DDoS detection tasks. Building upon these insights, we introduce a novel Generative Adversarial Network-based Feature Selection (GANFS) method that leverages adversarial learning dynamics to identify the most informative features. By training a GAN exclusively on attack traffic and employing a perturbation-based sensitivity analysis on the Discriminator, GANFS effectively ranks feature importance without relying on full supervision. Experimental evaluations using the CIC-DDoS2019 dataset demonstrate that GANFS not only improves the accuracy of downstream classifiers but also enhances computational efficiency by significantly reducing feature dimensionality. These results point to the potential of integrating generative learning models into cybersecurity pipelines to build more adaptive and scalable detection systems. 

**Abstract (ZH)**: 分布式拒绝服务（DDoS）攻击是对现代网络系统持续演变的威胁，能够导致大规模服务中断。这类攻击的复杂性，常隐藏在高维和冗余的网络流量数据中，需要 robust 和智能的特征选择技术以实现有效的检测。传统方法如过滤器方法、包装方法和嵌入方法各有优势，但在复杂攻击环境中难以实现扩展或适应。在本研究中，我们通过详细的比较分析探讨了这些现有技术，并指出了它们在大规模DDoS检测任务中的局限性。在此基础上，我们引入了一种基于生成对抗网络（GAN）的特征选择（GANFS）方法，利用对抗学习动态来识别最具信息性的特征。通过仅在攻击流量上训练GAN，并对判别器进行扰动感知分析，GANFS 有效排名特征重要性，且无需全监督。使用 CIC-DDoS2019 数据集的实验评估表明，GANFS 不仅提高了下游分类器的精度，还通过显著减少特征维度提高了计算效率。这些结果表明，将生成学习模型集成到网络安全流程中有可能构建更适应性和可扩展的检测系统。 

---
# Backdoor Defense in Diffusion Models via Spatial Attention Unlearning 

**Title (ZH)**: 通过空间注意遗忘实现的扩散模型后门防御 

**Authors**: Abha Jha, Ashwath Vaithinathan Aravindan, Matthew Salaway, Atharva Sandeep Bhide, Duygu Nur Yaldiz  

**Link**: [PDF](https://arxiv.org/pdf/2504.18563)  

**Abstract**: Text-to-image diffusion models are increasingly vulnerable to backdoor attacks, where malicious modifications to the training data cause the model to generate unintended outputs when specific triggers are present. While classification models have seen extensive development of defense mechanisms, generative models remain largely unprotected due to their high-dimensional output space, which complicates the detection and mitigation of subtle perturbations. Defense strategies for diffusion models, in particular, remain under-explored. In this work, we propose Spatial Attention Unlearning (SAU), a novel technique for mitigating backdoor attacks in diffusion models. SAU leverages latent space manipulation and spatial attention mechanisms to isolate and remove the latent representation of backdoor triggers, ensuring precise and efficient removal of malicious effects. We evaluate SAU across various types of backdoor attacks, including pixel-based and style-based triggers, and demonstrate its effectiveness in achieving 100% trigger removal accuracy. Furthermore, SAU achieves a CLIP score of 0.7023, outperforming existing methods while preserving the model's ability to generate high-quality, semantically aligned images. Our results show that SAU is a robust, scalable, and practical solution for securing text-to-image diffusion models against backdoor attacks. 

**Abstract (ZH)**: 文本到图像扩散模型越来越容易受到后门攻击的影响，其中恶意修改训练数据会在特定触发器存在时导致模型生成非预期的输出。虽然分类模型已经开发出了广泛的防御机制，但由于生成模型具有高维输出空间，使其在检测和缓解细微扰动方面复杂，这些生成模型仍然基本未受到保护。特别是对扩散模型的防御策略研究不足。在本工作中，我们提出了空间注意去学习（SAU）这一新颖的技术，用于减轻扩散模型中的后门攻击。SAU利用潜在空间操控和空间注意机制来隔离并移除与后门触发器相关的潜在表示，确保精确而高效的恶意效果去除。我们在各种类型的后门攻击中评估了SAU，包括基于像素和基于样式触发器，并展示了其在达到100%触发器移除准确率方面的有效性。此外，SAU实现了CLIP得分为0.7023，优于现有方法，同时仍保持生成高质量、语义对齐图像的能力。我们的结果表明，SAU是一种稳健、可扩展且实用的解决方案，用于保护文本到图像扩散模型免受后门攻击。 

---
# RDI: An adversarial robustness evaluation metric for deep neural networks based on sample clustering features 

**Title (ZH)**: 基于样本聚类特征的深神经网络对抗 Robustness 评价指标：RDI 

**Authors**: Jialei Song, Xingquan Zuo, Feiyang Wang, Hai Huang, Tianle Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2504.18556)  

**Abstract**: Deep neural networks (DNNs) are highly susceptible to adversarial samples, raising concerns about their reliability in safety-critical tasks. Currently, methods of evaluating adversarial robustness are primarily categorized into attack-based and certified robustness evaluation approaches. The former not only relies on specific attack algorithms but also is highly time-consuming, while the latter due to its analytical nature, is typically difficult to implement for large and complex models. A few studies evaluate model robustness based on the model's decision boundary, but they suffer from low evaluation accuracy. To address the aforementioned issues, we propose a novel adversarial robustness evaluation metric, Robustness Difference Index (RDI), which is based on sample clustering features. RDI draws inspiration from clustering evaluation by analyzing the intra-class and inter-class distances of feature vectors separated by the decision boundary to quantify model robustness. It is attack-independent and has high computational efficiency. Experiments show that, RDI demonstrates a stronger correlation with the gold-standard adversarial robustness metric of attack success rate (ASR). The average computation time of RDI is only 1/30 of the evaluation method based on the PGD attack. Our open-source code is available at: this https URL. 

**Abstract (ZH)**: 深度神经网络(DNNs)对对抗样本非常敏感，这引发了对其在关键安全任务中可靠性的质疑。目前，对抗鲁棒性评估方法主要分为基于攻击和认证鲁棒性评估两大类。前者的评估既依赖于特定的攻击算法，又非常耗时，而后者的评估由于其分析性质，在实现上通常对于大型和复杂的模型来说较为困难。少数研究基于模型的决策边界对模型鲁棒性进行评估，但这些方法存在评估精度低的问题。为解决上述问题，我们提出了一种新的对抗鲁棒性评估指标——鲁棒性差异指数(RDI)，其基于样本聚类特征。RDI借鉴了聚类评估思路，通过分析决策边界两侧特征向量的类内和类间距离来量化模型鲁棒性，具有攻击独立性以及高效性。实验表明，RDI与基于攻击成功率(ASR)的鲁棒性金标准指标具有更强的相关性，平均计算时间仅为基于PGD攻击的评估方法的1/30。我们的开源代码可在以下链接获取：this https URL。 

---
# Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data: A Systematic Review 

**Title (ZH)**: 合成表格数据评估中的关键挑战与指南：一项系统回顾 

**Authors**: Nazia Nafis, Inaki Esnaola, Alvaro Martinez-Perez, Maria-Cruz Villa-Uriol, Venet Osmani  

**Link**: [PDF](https://arxiv.org/pdf/2504.18544)  

**Abstract**: Generating synthetic tabular data can be challenging, however evaluation of their quality is just as challenging, if not more. This systematic review sheds light on the critical importance of rigorous evaluation of synthetic health data to ensure reliability, relevance, and their appropriate use. Based on screening of 1766 papers and a detailed review of 101 papers we identified key challenges, including lack of consensus on evaluation methods, improper use of evaluation metrics, limited input from domain experts, inadequate reporting of dataset characteristics, and limited reproducibility of results. In response, we provide several guidelines on the generation and evaluation of synthetic data, to allow the community to unlock and fully harness the transformative potential of synthetic data and accelerate innovation. 

**Abstract (ZH)**: 生成合成表格数据具有挑战性，然而对其质量的评估更加困难或同样困难。本系统综述强调了对合成医疗数据进行严格评估的重要性，以确保其可靠性和相关性及其恰当的应用。基于筛选1766篇论文并详细审查101篇论文，我们识别了关键挑战，包括评估方法缺乏共识、评价指标使用不当、领域专家参与度有限、数据集特征报告不足以及结果重现性有限。为此，我们提供了生成和评估合成数据的若干指南，以允许社区充分利用合成数据的变革潜力并加速创新。 

---
