# SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models 

**Title (ZH)**: SORT3D：基于空间对象中心推理的零样本3D定位工具箱，利用大型语言模型 

**Authors**: Nader Zantout, Haochen Zhang, Pujith Kachana, Jinkai Qiu, Ji Zhang, Wenshan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2504.18684)  

**Abstract**: Interpreting object-referential language and grounding objects in 3D with spatial relations and attributes is essential for robots operating alongside humans. However, this task is often challenging due to the diversity of scenes, large number of fine-grained objects, and complex free-form nature of language references. Furthermore, in the 3D domain, obtaining large amounts of natural language training data is difficult. Thus, it is important for methods to learn from little data and zero-shot generalize to new environments. To address these challenges, we propose SORT3D, an approach that utilizes rich object attributes from 2D data and merges a heuristics-based spatial reasoning toolbox with the ability of large language models (LLMs) to perform sequential reasoning. Importantly, our method does not require text-to-3D data for training and can be applied zero-shot to unseen environments. We show that SORT3D achieves state-of-the-art performance on complex view-dependent grounding tasks on two benchmarks. We also implement the pipeline to run real-time on an autonomous vehicle and demonstrate that our approach can be used for object-goal navigation on previously unseen real-world environments. All source code for the system pipeline is publicly released at this https URL . 

**Abstract (ZH)**: 理解和解析对象参照语言，并在三维环境中通过空间关系和属性定位对象是机器人与人类协同工作的重要基础。然而，由于场景多样、细粒度对象众多以及语言引用的复杂自由形状，这一任务往往具有挑战性。此外，在三维领域，获取大量的自然语言训练数据是困难的。因此，方法需要从少量数据中学习并在新的环境中零样本泛化变得尤为重要。为应对这些挑战，我们提出了SORT3D，一种利用2D数据中的丰富对象属性，并将基于启发式的空间推理工具与大规模语言模型的顺序推理能力相结合的方法。重要的是，我们的方法无需文本到三维数据进行训练，且可以在未见过的环境中零样本应用。我们展示了SORT3D在两个基准上的复杂视角依赖性定位任务中达到最先进的性能。我们还实现了该流程在自主车辆上实时运行，并证明了该方法可用于导航未见过的真实环境中的物体目标。系统的全部源代码已在此 URL 公开发布。 

---
# Can AI Agents Design and Implement Drug Discovery Pipelines? 

**Title (ZH)**: AI代理能否设计和实现药物发现管道？ 

**Authors**: Khachik Smbatyan, Tsolak Ghukasyan, Tigran Aghajanyan, Hovhannes Dabaghyan, Sergey Adamyan, Aram Bughdaryan, Vahagn Altunyan, Gagik Navasardyan, Aram Davtyan, Anush Hakobyan, Aram Gharibyan, Arman Fahradyan, Artur Hakobyan, Hasmik Mnatsakanyan, Narek Ginoyan, Garik Petrosyan  

**Link**: [PDF](https://arxiv.org/pdf/2504.19912)  

**Abstract**: The rapid advancement of artificial intelligence, particularly autonomous agentic systems based on Large Language Models (LLMs), presents new opportunities to accelerate drug discovery by improving in-silico modeling and reducing dependence on costly experimental trials. Current AI agent-based systems demonstrate proficiency in solving programming challenges and conducting research, indicating an emerging potential to develop software capable of addressing complex problems such as pharmaceutical design and drug discovery. This paper introduces DO Challenge, a benchmark designed to evaluate the decision-making abilities of AI agents in a single, complex problem resembling virtual screening scenarios. The benchmark challenges systems to independently develop, implement, and execute efficient strategies for identifying promising molecular structures from extensive datasets, while navigating chemical space, selecting models, and managing limited resources in a multi-objective context. We also discuss insights from the DO Challenge 2025, a competition based on the proposed benchmark, which showcased diverse strategies explored by human participants. Furthermore, we present the Deep Thought multi-agent system, which demonstrated strong performance on the benchmark, outperforming most human teams. Among the language models tested, Claude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles, and GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While promising, the system's performance still fell short of expert-designed solutions and showed high instability, highlighting both the potential and current limitations of AI-driven methodologies in transforming drug discovery and broader scientific research. 

**Abstract (ZH)**: 人工智能的迅速发展，尤其是基于大规模语言模型（LLMs）的自主代理系统，为加速药物发现提供了新机遇，通过改进计算建模并减少对昂贵实验测试的依赖。当前的基于AI代理的系统在解决编程挑战和进行研究方面表现出色，表明了开发能够解决复杂问题（如制药设计和药物发现）软件的潜在可能性。本文介绍了DO挑战，这是一个用于评估AI代理在单一复杂问题中的决策能力的基准，该问题类似于虚拟筛选场景。基准挑战系统独立开发、实现并执行高效策略，从大量数据集中识别有前景的分子结构，同时导航化学空间、选择模型并在多目标上下文中管理有限资源。我们还讨论了基于提出基准的DO挑战2025竞赛中人类参与者探索的各种策略。此外，我们介绍了Deep Thought多代理系统，该系统在基准测试中表现出色，超越了大多数人类团队。经过测试的语言模型中，Claude 3.7 Sonnet、Gemini 2.5 Pro和o3在主要代理角色中表现最佳，而GPT-4o和Gemini 2.0 Flash在辅助角色中表现有效。虽然颇具前景，但该系统的性能仍不及专家设计的解决方案，并显示出高不稳定性的特点，突显了AI驱动方法在转型药物发现和更广泛的科学研究中的潜力和当前局限性。 

---
# From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review 

**Title (ZH)**: 从大规模语言模型推理到自主AI代理：全面综述 

**Authors**: Mohamed Amine Ferrag, Norbert Tihanyi, Merouane Debbah  

**Link**: [PDF](https://arxiv.org/pdf/2504.19678)  

**Abstract**: Large language models and autonomous AI agents have evolved rapidly, resulting in a diverse array of evaluation benchmarks, frameworks, and collaboration protocols. However, the landscape remains fragmented and lacks a unified taxonomy or comprehensive survey. Therefore, we present a side-by-side comparison of benchmarks developed between 2019 and 2025 that evaluate these models and agents across multiple domains. In addition, we propose a taxonomy of approximately 60 benchmarks that cover general and academic knowledge reasoning, mathematical problem-solving, code generation and software engineering, factual grounding and retrieval, domain-specific evaluations, multimodal and embodied tasks, task orchestration, and interactive assessments. Furthermore, we review AI-agent frameworks introduced between 2023 and 2025 that integrate large language models with modular toolkits to enable autonomous decision-making and multi-step reasoning. Moreover, we present real-world applications of autonomous AI agents in materials science, biomedical research, academic ideation, software engineering, synthetic data generation, chemical reasoning, mathematical problem-solving, geographic information systems, multimedia, healthcare, and finance. We then survey key agent-to-agent collaboration protocols, namely the Agent Communication Protocol (ACP), the Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally, we discuss recommendations for future research, focusing on advanced reasoning strategies, failure modes in multi-agent LLM systems, automated scientific discovery, dynamic tool integration via reinforcement learning, integrated search capabilities, and security vulnerabilities in agent protocols. 

**Abstract (ZH)**: 大型语言模型和自主AI代理迅速发展，导致了多样化的评估基准、框架和协作协议。然而，这一领域仍然碎片化，缺乏统一的分类或综合调研。因此，我们比较了2019年至2025年间开发的评估基准，这些基准在多个领域评估了这些模型和代理。此外，我们提出了一种涵盖一般和学术知识推理、数学问题解决、代码生成与软件工程、事实 grounding 与检索、特定领域评估、多模态和实体任务、任务编排和互动评估的大约60种基准的分类体系。我们还回顾了2023年至2025年间引入的AI代理框架，这些框架结合了大型语言模型和模块化工具包，以实现自主决策和多步推理。此外，我们展示了自主AI代理在材料科学、生物医药研究、学术构想、软件工程、合成数据生成、化学推理、数学问题解决、地理信息系统、多媒体、医疗保健和金融领域的实际应用。我们还调研了关键的代理间协作协议，即代理通信协议（ACP）、模型上下文协议（MCP）和代理到代理协议（A2A）。最后，我们讨论了未来研究的建议，集中在高级推理策略、多代理LLM系统中的失败模式、自动化科学发现、通过强化学习实现的动态工具集成、综合搜索能力以及代理协议中的安全漏洞。 

---
# Fitness Landscape of Large Language Model-Assisted Automated Algorithm Search 

**Title (ZH)**: 大型语言模型辅助自动化算法搜索的fitness landscape 

**Authors**: Fei Liu, Qingfu Zhang, Xialiang Tong, Mingxuan Yuan, Kun Mao  

**Link**: [PDF](https://arxiv.org/pdf/2504.19636)  

**Abstract**: Large Language Models (LLMs) have demonstrated significant potential in algorithm design. However, when integrated into search frameworks for iterative algorithm search, the underlying fitness landscape--critical for understanding search behaviou--remains underexplored. In this paper, we illustrate and analyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a graph-based approach, where nodes represent algorithms and edges denote transitions between them. We conduct extensive evaluations across six algorithm design tasks and six commonly used LLMs. Our findings reveal that LAS landscapes are highly multimodal and rugged, particularly in combinatorial optimization tasks, with distinct structural variations across tasks and LLMs. For instance, heuristic design tasks exhibit dense clusters of high-performing algorithms, while symbolic regression tasks show sparse, scattered distributions. Additionally, we demonstrate how population size influences exploration-exploitation trade-offs and the evolving trajectory of elite algorithms. These insights not only advance our understanding of LAS landscapes but also provide practical guidance for designing more effective LAS methods. 

**Abstract (ZH)**: 大型语言模型辅助算法搜索的适应度景观：基于图的方法研究 

---
# GVPO: Group Variance Policy Optimization for Large Language Model Post-Training 

**Title (ZH)**: GVPO：组方差政策优化在大规模语言模型后训练中的应用 

**Authors**: Kaichen Zhang, Yuzhong Hong, Junwei Bao, Hongfei Jiang, Yang Song, Dingqian Hong, Hui Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2504.19599)  

**Abstract**: Post-training plays a crucial role in refining and aligning large language models to meet specific tasks and human preferences. While recent advancements in post-training techniques, such as Group Relative Policy Optimization (GRPO), leverage increased sampling with relative reward scoring to achieve superior performance, these methods often suffer from training instability that limits their practical adoption. To address this challenge, we present Group Variance Policy Optimization (GVPO). GVPO incorporates the analytical solution to KL-constrained reward maximization directly into its gradient weights, ensuring alignment with the optimal policy. The method provides intuitive physical interpretations: its gradient mirrors the mean squared error between the central distance of implicit rewards and that of actual rewards. GVPO offers two key advantages: (1) it guarantees a unique optimal solution, exactly the KL-constrained reward maximization objective, (2) it supports flexible sampling distributions that avoids on-policy and importance sampling limitations. By unifying theoretical guarantees with practical adaptability, GVPO establishes a new paradigm for reliable and versatile LLM post-training. 

**Abstract (ZH)**: Post-Training 在精炼和对齐大型语言模型以满足特定任务和人类偏好方面起着至关重要的作用。尽管最近在后训练技术方面的进展，如组相对策略优化（GRPO），通过增加相对奖励评分的采样来取得更优的表现，但这些方法往往由于训练不稳定性的限制而难以广泛采用。为应对这一挑战，我们提出了组方差策略优化（GVPO）。GVPO 将 KL 约束奖励最大化的问题的解析解直接纳入其梯度权重中，确保与最优策略的对齐。该方法提供了直观的物理解释：其梯度反映了隐式奖励中心距离与实际奖励中心距离的均方误差。GVPO 的两个关键优势是：（1）它保证了唯一的最优解，即是 KL 约束奖励最大化的目标；（2）它支持灵活的采样分布，避免了策略回采样和重要性采样的限制。通过统一理论保证与实际适用性，GVPO 建立了可靠且多功能的大规模语言模型后训练新范式。 

---
# Small Models, Big Tasks: An Exploratory Empirical Study on Small Language Models for Function Calling 

**Title (ZH)**: 小模型，大任务：关于小语言模型在函数调用中的探索性实证研究 

**Authors**: Ishan Kavathekar, Raghav Donakanti, Ponnurangam Kumaraguru, Karthik Vaidhyanathan  

**Link**: [PDF](https://arxiv.org/pdf/2504.19277)  

**Abstract**: Function calling is a complex task with widespread applications in domains such as information retrieval, software engineering and automation. For example, a query to book the shortest flight from New York to London on January 15 requires identifying the correct parameters to generate accurate function calls. Large Language Models (LLMs) can automate this process but are computationally expensive and impractical in resource-constrained settings. In contrast, Small Language Models (SLMs) can operate efficiently, offering faster response times, and lower computational demands, making them potential candidates for function calling on edge devices. In this exploratory empirical study, we evaluate the efficacy of SLMs in generating function calls across diverse domains using zero-shot, few-shot, and fine-tuning approaches, both with and without prompt injection, while also providing the finetuned models to facilitate future applications. Furthermore, we analyze the model responses across a range of metrics, capturing various aspects of function call generation. Additionally, we perform experiments on an edge device to evaluate their performance in terms of latency and memory usage, providing useful insights into their practical applicability. Our findings show that while SLMs improve from zero-shot to few-shot and perform best with fine-tuning, they struggle significantly with adhering to the given output format. Prompt injection experiments further indicate that the models are generally robust and exhibit only a slight decline in performance. While SLMs demonstrate potential for the function call generation task, our results also highlight areas that need further refinement for real-time functioning. 

**Abstract (ZH)**: 小语言模型在跨领域函数调用生成中的有效性探究：从零样本、少量样本到微调的方法对比及边缘设备上的性能评估 

---
# The Convergent Ethics of AI? Analyzing Moral Foundation Priorities in Large Language Models with a Multi-Framework Approach 

**Title (ZH)**: AI伦理的趋同性？一种多框架方法分析大规模语言模型的道德基础优先级 

**Authors**: Chad Coleman, W. Russell Neuman, Ali Dasdan, Safinah Ali, Manan Shah  

**Link**: [PDF](https://arxiv.org/pdf/2504.19255)  

**Abstract**: As large language models (LLMs) are increasingly deployed in consequential decision-making contexts, systematically assessing their ethical reasoning capabilities becomes a critical imperative. This paper introduces the Priorities in Reasoning and Intrinsic Moral Evaluation (PRIME) framework--a comprehensive methodology for analyzing moral priorities across foundational ethical dimensions including consequentialist-deontological reasoning, moral foundations theory, and Kohlberg's developmental stages. We apply this framework to six leading LLMs through a dual-protocol approach combining direct questioning and response analysis to established ethical dilemmas. Our analysis reveals striking patterns of convergence: all evaluated models demonstrate strong prioritization of care/harm and fairness/cheating foundations while consistently underweighting authority, loyalty, and sanctity dimensions. Through detailed examination of confidence metrics, response reluctance patterns, and reasoning consistency, we establish that contemporary LLMs (1) produce decisive ethical judgments, (2) demonstrate notable cross-model alignment in moral decision-making, and (3) generally correspond with empirically established human moral preferences. This research contributes a scalable, extensible methodology for ethical benchmarking while highlighting both the promising capabilities and systematic limitations in current AI moral reasoning architectures--insights critical for responsible development as these systems assume increasingly significant societal roles. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）在重要决策情境中的广泛应用，系统评估其伦理推理能力变得至关重要。本文介绍了优先推理与固有道德评价（PRIME）框架——一种全面的方法，用于分析涵盖功利-义务推理、道德基础理论和柯尔伯格发展阶段的基本伦理维度中的道德优先事项。我们通过结合直接提问和对已建立的伦理困境的响应分析，将这一框架应用于六种主要的LLM。我们的分析揭示了令人瞩目的模式：所有评估的模型都强烈关注关怀/伤害和公平/欺骗的基础，而一致地低估了权威、忠诚和神圣性维度。通过对置信度指标、响应迟疑模式和推理一致性进行详细分析，我们发现（1）当代LLM生成明确的伦理判断，（2）在道德决策上展示了显著的跨模型一致性，（3）总体上与实证确立的人类道德偏好相符。本文提供了一种可扩展、可扩展的伦理基准测试方法，同时指出了当前AI道德推理架构的有前途的能力和系统的局限性——这些见解对于负责任的发展至关重要，因为这些系统在日益重要的社会角色中发挥着越来越多的作用。 

---
# ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile Hardware Development 

**Title (ZH)**: ChiseLLM: 点亮敏捷硬件开发中推理论据大语言模型的潜能 

**Authors**: Bowei Wang, Jiaran Gao, Yelai Feng, Renzhi Chen, Shanshan Li, Lei Wang  

**Link**: [PDF](https://arxiv.org/pdf/2504.19144)  

**Abstract**: The growing demand for Domain-Specific Architecture (DSA) has driven the development of Agile Hardware Development Methodology (AHDM). Hardware Construction Language (HCL) like Chisel offers high-level abstraction features, making it an ideal language for HCL-Based AHDM. While Large Language Models (LLMs) excel in code generation tasks, they still face challenges with Chisel generation, particularly regarding syntax correctness and design variability. Recent reasoning models have significantly enhanced code generation capabilities through test-time scaling techniques. However, we found that reasoning models without domain adaptation cannot bring substantial benefits to Chisel code generation tasks. This paper presents ChiseLLM, a solution comprising data processing and transformation, prompt-guided reasoning trace synthesis, and domain-adapted model training. We constructed high-quality datasets from public RTL code resources and guided the model to adopt structured thinking patterns through prompt enhancement methods. Experiments demonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax correctness by 18.85% and 26.32% respectively over base models, while increasing variability design ability by 47.58% compared to baseline reasoning models. Our datasets and models are publicly available, providing high-performance, cost-effective models for HCL-Based AHDM, and offering an effective baseline for future research. Github repository: this https URL 

**Abstract (ZH)**: Domain-Specific Architecture (DSA) 需求的增长推动了敏捷硬件开发方法学 (AHDM) 的发展。硬件构造语言 (HCL) 如 Chisel 提供了高层次抽象特性，使其成为基于 HCL 的 AHDM 的理想选择。虽然大型语言模型 (LLMs) 在代码生成任务上表现出色，但在 Chisel 生成任务中仍面临挑战，特别是在语法正确性和设计变异性方面。近期的推理模型通过测试时放大技术增强了代码生成能力。然而，我们发现未经领域适应的推理模型无法给 Chisel 代码生成任务带来显著益处。本文提出了 ChiseLLM，该解决方案包括数据处理与转换、提示引导推理轨迹合成以及领域适应模型训练。我们从公共 RTL 代码资源中构建了高质量的数据集，并通过提示增强方法引导模型采用结构化思考模式。实验结果显示，我们的 ChiseLLM-7B 和 ChiseLLM-32B 模型分别在基线模型基础上提高了 18.85% 和 26.32% 的语法正确性，并且在设计变异性方面提高了 47.58%。我们的数据集和模型已公开提供，为基于 HCL 的 AHDM 提供了高性能且成本效益高的模型，并为未来研究提供了有效的基线。GitHub 代码库：this https URL。 

---
# GLaMoR: Consistency Checking of OWL Ontologies using Graph Language Models 

**Title (ZH)**: GLaMoR: OWL本体一致性的图语言模型检查 

**Authors**: Justin Mücke, Ansgar Scherp  

**Link**: [PDF](https://arxiv.org/pdf/2504.19023)  

**Abstract**: Semantic reasoning aims to infer new knowledge from existing knowledge, with OWL ontologies serving as a standardized framework for organizing information. A key challenge in semantic reasoning is verifying ontology consistency. However, state-of-the-art reasoners are computationally expensive, and their efficiency decreases as ontology sizes grow. While classical machine learning models have been explored for consistency checking, they struggle to capture complex relationships within ontologies. Large language models (LLMs) have shown promising results for simple reasoning tasks but perform poorly on structured reasoning. The recently introduced Graph Language Model (GLM) offers a way to simultaneously process graph-structured data and text. This paper proposes GLaMoR (Graph Language Model for Reasoning), a reasoning pipeline that transforms OWL ontologies into graph-structured data and adapts the GLM architecture for consistency checking. We evaluate GLaMoR on ontologies from the NCBO BioPortal repository, converting them into triples suitable for model input. Our results show that the GLM outperforms all baseline models, achieving $95\%$ accuracy while being 20 times faster than classical reasoners.
The Code is accessible under: this https URL 

**Abstract (ZH)**: 语义推理旨在从现有知识中推断新的知识，OWL本体提供了一种标准化框架来组织信息。语义推理中的一个关键挑战是验证本体一致性。然而，最先进的推理器计算成本高昂，并且随着本体规模的扩大，其效率会降低。虽然已经探索了经典机器学习模型来进行一致性检查，但它们难以捕捉本体中的复杂关系。大型语言模型（LLMs）在简单的推理任务上显示出良好的结果，但在结构化推理上表现不佳。最近引入的图语言模型（GLM）提供了一种同时处理图结构数据和文本的方法。本文提出GLaMoR（图语言模型用于推理），这是一种推理管道，将OWL本体转换为图结构数据，并调整GLM架构进行一致性检查。我们在NCBO BioPortal仓库中的本体上评估了GLaMoR，将它们转化为适用于模型输入的三元组。结果显示，GLM在所有基线模型中表现最佳，准确率达到95%，比经典推理器快20倍。代码可在以下链接访问：this https URL。 

---
# Reshaping MOFs Text Mining with a Dynamic Multi-Agent Framework of Large Language Agents 

**Title (ZH)**: 基于大型语言代理动态多-agent框架重塑MOFs文本挖掘 

**Authors**: Zuhong Lin, Daoyuan Ren, Kai Ran, Sun Jing, Xiaotiang Huang, Haiyang He, Pengxu Pan, Xiaohang Zhang, Ying Fang, Tianying Wang, Minli Wu, Zhanglin Li, Xiaochuan Zhang, Haipu Li, Jingjing Yao  

**Link**: [PDF](https://arxiv.org/pdf/2504.18880)  

**Abstract**: The mining of synthesis conditions for metal-organic frameworks (MOFs) is a significant focus in materials science. However, identifying the precise synthesis conditions for specific MOFs within the vast array of possibilities presents a considerable challenge. Large Language Models (LLMs) offer a promising solution to this problem. We leveraged the capabilities of LLMs, specifically gpt-4o-mini, as core agents to integrate various MOF-related agents, including synthesis, attribute, and chemical information agents. This integration culminated in the development of MOFh6, an LLM tool designed to streamline the MOF synthesis process. MOFh6 allows users to query in multiple formats, such as submitting scientific literature, or inquiring about specific MOF codes or structural properties. The tool analyzes these queries to provide optimal synthesis conditions and generates model files for density functional theory pre modeling. We believe MOFh6 will enhance efficiency in the MOF synthesis of all researchers. 

**Abstract (ZH)**: 金属有机框架（MOFs）合成条件的挖掘是材料科学中的一个重要研究焦点。然而，在众多可能性中确定特定MOFs的精确合成条件仍面临较大挑战。大规模语言模型（LLMs）为解决这一问题提供了 promising 的方案。我们利用特定的大规模语言模型 gpt-4o-mini 的能力，将其作为核心代理与多种MOF相关的代理，包括合成、属性和化学信息代理进行整合，最终开发出了MOFh6，这是一种旨在简化MOF合成过程的LLM工具。MOFh6 允许用户以多种形式进行查询，如提交科学文献或询问特定的MOF代码或结构属性。该工具分析这些查询以提供最优的合成条件，并生成用于密度泛函理论预建模的模型文件。我们认为，MOFh6 将增强所有研究人员在MOF合成中的效率。 

---
# Generative to Agentic AI: Survey, Conceptualization, and Challenges 

**Title (ZH)**: 生成到主动型AI：综述、概念化与挑战 

**Authors**: Johannes Schneider  

**Link**: [PDF](https://arxiv.org/pdf/2504.18875)  

**Abstract**: Agentic Artificial Intelligence (AI) builds upon Generative AI (GenAI). It constitutes the next major step in the evolution of AI with much stronger reasoning and interaction capabilities that enable more autonomous behavior to tackle complex tasks. Since the initial release of ChatGPT (3.5), Generative AI has seen widespread adoption, giving users firsthand experience. However, the distinction between Agentic AI and GenAI remains less well understood. To address this gap, our survey is structured in two parts. In the first part, we compare GenAI and Agentic AI using existing literature, discussing their key characteristics, how Agentic AI remedies limitations of GenAI, and the major steps in GenAI's evolution toward Agentic AI. This section is intended for a broad audience, including academics in both social sciences and engineering, as well as industry professionals. It provides the necessary insights to comprehend novel applications that are possible with Agentic AI but not with GenAI. In the second part, we deep dive into novel aspects of Agentic AI, including recent developments and practical concerns such as defining agents. Finally, we discuss several challenges that could serve as a future research agenda, while cautioning against risks that can emerge when exceeding human intelligence. 

**Abstract (ZH)**: 代理型人工智能（Agentic Artificial Intelligence）建立在生成型人工智能（Generative AI）之上。它代表了人工智能演进的下一步重大进展，具有更强的推理和交互能力，能够实现更加自主的行为以应对复杂的任务。自ChatGPT（3.5）最初发布以来，生成型人工智能已经得到了广泛的采用，用户也获得了亲身体验。然而，代理型人工智能和生成型人工智能之间的区别仍不够清晰。为了弥合这一差距，我们的调查分为两部分。在第一部分中，我们利用现有文献比较生成型人工智能和代理型人工智能，讨论它们的关键特征，代理型人工智能如何弥补生成型人工智能的局限性，以及生成型人工智能向代理型人工智能演进的主要步骤。该部分旨在为包括社会科学和工程学领域的学者以及行业专业人士在内的广泛读者提供必要的洞察，帮助理解代理型人工智能可能带来的新颖应用。在第二部分中，我们深入探讨代理型人工智能的新型方面，包括最近的发展和实践关注点，如定义代理。最后，我们讨论了若干挑战，这些挑战可能成为未来研究议程的一部分，并警告超越人类智能时可能出现的风险。 

---
# A Vision for Auto Research with LLM Agents 

**Title (ZH)**: 自动研究中大规模语言模型代理的愿景 

**Authors**: Chengwei Liu, Chong Wang, Jiayue Cao, Jingquan Ge, Kun Wang, Lvye Zhang, Ming-Ming Cheng, Penghai Zhao, Tianlin Li, Xiaojun Jia, Xiang Li, Xinfeng Li, Yang Liu, Yebo Feng, Yihao Huang, Yijia Xu, Yuqiang Sun, Zhenhong Zhou, Zhengzi Xu  

**Link**: [PDF](https://arxiv.org/pdf/2504.18765)  

**Abstract**: This paper introduces Agent-Based Auto Research, a structured multi-agent framework designed to automate, coordinate, and optimize the full lifecycle of scientific research. Leveraging the capabilities of large language models (LLMs) and modular agent collaboration, the system spans all major research phases, including literature review, ideation, methodology planning, experimentation, paper writing, peer review response, and dissemination. By addressing issues such as fragmented workflows, uneven methodological expertise, and cognitive overload, the framework offers a systematic and scalable approach to scientific inquiry. Preliminary explorations demonstrate the feasibility and potential of Auto Research as a promising paradigm for self-improving, AI-driven research processes. 

**Abstract (ZH)**: 基于代理的自动科学研究：一种用于自动化、协调和优化科学研究全生命周期的结构化多代理框架 

---
# Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction 

**Title (ZH)**: 基于Proof-of-TBI——细调视觉语言模型联盟与OpenAI-o3推理大语言模型支持的轻度创伤性脑损伤(TBI)预测医疗诊断支持系统 

**Authors**: Ross Gore, Eranga Bandara, Sachin Shetty, Alberto E. Musto, Pratip Rana, Ambrosio Valencia-Romero, Christopher Rhea, Lobat Tayebi, Heather Richter, Atmaram Yarlagadda, Donna Edmonds, Steven Wallace, Donna Broshek  

**Link**: [PDF](https://arxiv.org/pdf/2504.18671)  

**Abstract**: Mild Traumatic Brain Injury (TBI) detection presents significant challenges due to the subtle and often ambiguous presentation of symptoms in medical imaging, making accurate diagnosis a complex task. To address these challenges, we propose Proof-of-TBI, a medical diagnosis support system that integrates multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large language model (LLM). Our approach fine-tunes multiple vision-language models using a labeled dataset of TBI MRI scans, training them to diagnose TBI symptoms effectively. The predictions from these models are aggregated through a consensus-based decision-making process. The system evaluates the predictions from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a model that has demonstrated remarkable reasoning performance, to produce the most accurate final diagnosis. The LLM Agents orchestrates interactions between the vision-language models and the reasoning LLM, managing the final decision-making process with transparency, reliability, and automation. This end-to-end decision-making workflow combines the vision-language model consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt engineering by the LLM agents. The prototype for the proposed platform was developed in collaboration with the U.S. Army Medical Research team in Newport News, Virginia, incorporating five fine-tuned vision-language models. The results demonstrate the transformative potential of combining fine-tuned vision-language model inputs with the OpenAI-o3 reasoning LLM to create a robust, secure, and highly accurate diagnostic system for mild TBI prediction. To the best of our knowledge, this research represents the first application of fine-tuned vision-language models integrated with a reasoning LLM for TBI prediction tasks. 

**Abstract (ZH)**: Mild Traumatic Brain Injury (TBI)检测由于医学成像中症状的微妙且常常含糊不清的表现形式而面临显著挑战，使准确诊断成为一项复杂任务。为此，我们提出了一种名为Proof-of-TBI的医疗诊断支持系统，该系统整合了多种微调的视觉语言模型与OpenAI-o3推理大型语言模型（LLM）。我们的方法使用标记的TBI MRI扫描数据集对多种视觉语言模型进行微调，使其能够有效诊断TBI症状。这些模型的预测结果通过基于一致性的决策过程进行聚合。该系统使用OpenAI-o3推理LLM对所有微调的视觉语言模型进行评估，这是一个已显示出卓越推理性能的模型，以此产生最准确的最终诊断。LLM代理协调视觉语言模型与推理LLM之间的交互，以透明、可靠和自动的方式管理最终的决策过程。整个决策流程结合了视觉语言模型联盟与OpenAI-o3推理LLM，并通过LLM代理自定义提示工程实现。该平台的原型是与美国陆军研究团队合作，在弗吉尼亚州纽波特纽斯开发的，集成了五种微调的视觉语言模型。研究结果展示了将微调视觉语言模型输入与OpenAI-o3推理LLM相结合以创建用于轻度TBI预测的稳定、安全和高度准确诊断系统的转型潜力。据我们所知，这项研究是首次将微调的视觉语言模型与推理LLM集成应用于TBI预测任务。 

---
# Exploring a Large Language Model for Transforming Taxonomic Data into OWL: Lessons Learned and Implications for Ontology Development 

**Title (ZH)**: 探索大型语言模型将分类数据转换为OWL的途径：基于经验的学习及其对本体开发的影响 

**Authors**: Filipi Miranda Soares, Antonio Mauro Saraiva, Luís Ferreira Pires, Luiz Olavo Bonino da Silva Santos, Dilvan de Abreu Moreira, Fernando Elias Corrêa, Kelly Rosa Braghetto, Debora Pignatari Drucker, Alexandre Cláudio Botazzo Delbem  

**Link**: [PDF](https://arxiv.org/pdf/2504.18651)  

**Abstract**: Managing scientific names in ontologies that represent species taxonomies is challenging due to the ever-evolving nature of these taxonomies. Manually maintaining these names becomes increasingly difficult when dealing with thousands of scientific names. To address this issue, this paper investigates the use of ChatGPT-4 to automate the development of the :Organism module in the Agricultural Product Types Ontology (APTO) for species classification. Our methodology involved leveraging ChatGPT-4 to extract data from the GBIF Backbone API and generate OWL files for further integration in APTO. Two alternative approaches were explored: (1) issuing a series of prompts for ChatGPT-4 to execute tasks via the BrowserOP plugin and (2) directing ChatGPT-4 to design a Python algorithm to perform analogous tasks. Both approaches rely on a prompting method where we provide instructions, context, input data, and an output indicator. The first approach showed scalability limitations, while the second approach used the Python algorithm to overcome these challenges, but it struggled with typographical errors in data handling. This study highlights the potential of Large language models like ChatGPT-4 to streamline the management of species names in ontologies. Despite certain limitations, these tools offer promising advancements in automating taxonomy-related tasks and improving the efficiency of ontology development. 

**Abstract (ZH)**: 基于ChatGPT-4自动开发农业产品类型本体中的生物模块以简化物种命名管理 

---
# BELL: Benchmarking the Explainability of Large Language Models 

**Title (ZH)**: BELL: 大型语言模型可解释性基准评估 

**Authors**: Syed Quiser Ahmed, Bharathi Vokkaliga Ganesh, Jagadish Babu P, Karthick Selvaraj, ReddySiva Naga Parvathi Devi, Sravya Kappala  

**Link**: [PDF](https://arxiv.org/pdf/2504.18572)  

**Abstract**: Large Language Models have demonstrated remarkable capabilities in natural language processing, yet their decision-making processes often lack transparency. This opaqueness raises significant concerns regarding trust, bias, and model performance. To address these issues, understanding and evaluating the interpretability of LLMs is crucial. This paper introduces a standardised benchmarking technique, Benchmarking the Explainability of Large Language Models, designed to evaluate the explainability of large language models. 

**Abstract (ZH)**: 大规模语言模型在自然语言处理任务中展现了 remarkable 的能力，但其决策过程往往缺乏透明性。这种不透明性引发了关于可信度、偏见和模型性能的重大关切。为了解决这些问题，理解并评估大规模语言模型的可解释性至关重要。本文介绍了标准化基准测试技术《评估大规模语言模型的可解释性》，旨在评估大规模语言模型的可解释性。 

---
# Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models 

**Title (ZH)**: 模块化机器学习：通往新一代大语言模型不可或缺的道路 

**Authors**: Xin Wang, Haoyang Li, Zeyang Zhang, Haibo Chen, Wenwu Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2504.20020)  

**Abstract**: Large language models (LLMs) have dramatically advanced machine learning research including natural language processing, computer vision, data mining, etc., yet they still exhibit critical limitations in reasoning, factual consistency, and interpretability. In this paper, we introduce a novel learning paradigm -- Modular Machine Learning (MML) -- as an essential approach toward new-generation LLMs. MML decomposes the complex structure of LLMs into three interdependent components: modular representation, modular model, and modular reasoning, aiming to enhance LLMs' capability of counterfactual reasoning, mitigating hallucinations, as well as promoting fairness, safety, and transparency. Specifically, the proposed MML paradigm can: i) clarify the internal working mechanism of LLMs through the disentanglement of semantic components; ii) allow for flexible and task-adaptive model design; iii) enable interpretable and logic-driven decision-making process. We present a feasible implementation of MML-based LLMs via leveraging advanced techniques such as disentangled representation learning, neural architecture search and neuro-symbolic learning. We critically identify key challenges, such as the integration of continuous neural and discrete symbolic processes, joint optimization, and computational scalability, present promising future research directions that deserve further exploration. Ultimately, the integration of the MML paradigm with LLMs has the potential to bridge the gap between statistical (deep) learning and formal (logical) reasoning, thereby paving the way for robust, adaptable, and trustworthy AI systems across a wide range of real-world applications. 

**Abstract (ZH)**: 模块化机器学习（MML）：新一代大语言模型的关键范式 

---
# Securing GenAI Multi-Agent Systems Against Tool Squatting: A Zero Trust Registry-Based Approach 

**Title (ZH)**: 基于零信任注册表的保护GenAI多代理系统免受工具抢占的方法 

**Authors**: Vineeth Sai Narajala, Ken Huang, Idan Habler  

**Link**: [PDF](https://arxiv.org/pdf/2504.19951)  

**Abstract**: The rise of generative AI (GenAI) multi-agent systems (MAS) necessitates standardized protocols enabling agents to discover and interact with external tools. However, these protocols introduce new security challenges, particularly; tool squatting; the deceptive registration or representation of tools. This paper analyzes tool squatting threats within the context of emerging interoperability standards, such as Model Context Protocol (MCP) or seamless communication between agents protocols. It introduces a comprehensive Tool Registry system designed to mitigate these risks. We propose a security-focused architecture featuring admin-controlled registration, centralized tool discovery, fine grained access policies enforced via dedicated Agent and Tool Registry services, a dynamic trust scoring mechanism based on tool versioning and known vulnerabilities, and just in time credential provisioning. Based on its design principles, the proposed registry framework aims to effectively prevent common tool squatting vectors while preserving the flexibility and power of multi-agent systems. This work addresses a critical security gap in the rapidly evolving GenAI ecosystem and provides a foundation for secure tool integration in production environments. 

**Abstract (ZH)**: 生成式AI多智能体系统中标准化协议的兴起 necessitates 标准化协议以使智能体发现并交互外部工具。然而，这些协议引入了新的安全挑战，特别是工具quatting；即工具的欺骗性注册或表示。本文分析了在新兴互操作性标准（如模型上下文协议或智能体之间无缝通信协议）背景下工具quatting威胁，并介绍了旨在缓解这些风险的综合工具注册系统。我们提出了一种以安全性为导向的架构，包括由管理员控制的注册、集中的工具发现、通过专门的智能体和工具注册服务实施的细粒度访问策略、基于工具版本和已知漏洞的动态信任评分机制，以及即时凭证分配。基于其设计原则，提出的注册框架旨在有效防止常见的工具quatting向量，同时保持多智能体系统的灵活性和强大功能。本文填补了快速演变的生成式AI生态系统中的关键安全空白，并为生产环境中的安全工具集成提供了一个基础。 

---
# Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking 

**Title (ZH)**: 评估生成式代理在众包事实核查中的潜力 

**Authors**: Luigia Costabile, Gian Marco Orlando, Valerio La Gatta, Vincenzo Moscato  

**Link**: [PDF](https://arxiv.org/pdf/2504.19940)  

**Abstract**: The growing spread of online misinformation has created an urgent need for scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where non-experts evaluate claim veracity - offers a cost-effective alternative to expert verification, despite concerns about variability in quality and bias. Encouraged by promising results in certain contexts, major platforms such as X (formerly Twitter), Facebook, and Instagram have begun shifting from centralized moderation to decentralized, crowd-based approaches.
In parallel, advances in Large Language Models (LLMs) have shown strong performance across core fact-checking tasks, including claim detection and evidence evaluation. However, their potential role in crowdsourced workflows remains unexplored. This paper investigates whether LLM-powered generative agents - autonomous entities that emulate human behavior and decision-making - can meaningfully contribute to fact-checking tasks traditionally reserved for human crowds. Using the protocol of La Barbera et al. (2024), we simulate crowds of generative agents with diverse demographic and ideological profiles. Agents retrieve evidence, assess claims along multiple quality dimensions, and issue final veracity judgments.
Our results show that agent crowds outperform human crowds in truthfulness classification, exhibit higher internal consistency, and show reduced susceptibility to social and cognitive biases. Compared to humans, agents rely more systematically on informative criteria such as Accuracy, Precision, and Informativeness, suggesting a more structured decision-making process. Overall, our findings highlight the potential of generative agents as scalable, consistent, and less biased contributors to crowd-based fact-checking systems. 

**Abstract (ZH)**: 在线 misinformation 蔓延背景下可扩展且可靠的事实核查解决方案的需求日益迫切。众包事实核查——非专家评估声明真实性——尽管存在质量问题和偏差的担忧，仍提供了专家验证的经济有效替代方案。受某些情境下取得的积极成果激励，X（ formerly Twitter）、Facebook 和 Instagram 等主要平台已开始从集中式管理转向分散的、基于众包的方法。

与此同时，大型语言模型（LLMs）在核心事实核查任务中的表现强劲，包括声明检测和证据评估。然而，它们在众包流程中的潜在作用尚未被探索。本文探讨了基于大型语言模型的生成代理——能够模仿人类行为和决策的自主实体——是否能够有意义地参与传统上由人类众源完成的事实核查任务。我们借鉴 La Barbera 等人（2024）的协议，模拟了具有多样人口统计学和社会意识形态特征的生成代理群体。代理检索证据，从多个质量维度评估声明，并作出最终的真实性判断。

研究结果表明，生成代理群体在真实性分类上优于人类群体，在内部一致性和减少社会和认知偏见方面表现更佳。与人类相比，生成代理更系统地依赖于准确性、精确性和信息性等有意义的标准，表明其决策过程更为结构化。总体而言，我们的研究结果突显了生成代理作为可扩展、一致且偏差较小的众包事实核查系统贡献者的潜在价值。 

---
# Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation 

**Title (ZH)**: 重构上下文：评估扩展分块策略以增强检索生成性能 

**Authors**: Carlo Merola, Jaspinder Singh  

**Link**: [PDF](https://arxiv.org/pdf/2504.19754)  

**Abstract**: Retrieval-augmented generation (RAG) has become a transformative approach for enhancing large language models (LLMs) by grounding their outputs in external knowledge sources. Yet, a critical question persists: how can vast volumes of external knowledge be managed effectively within the input constraints of LLMs? Traditional methods address this by chunking external documents into smaller, fixed-size segments. While this approach alleviates input limitations, it often fragments context, resulting in incomplete retrieval and diminished coherence in generation. To overcome these shortcomings, two advanced techniques, late chunking and contextual retrieval, have been introduced, both aiming to preserve global context. Despite their potential, their comparative strengths and limitations remain unclear. This study presents a rigorous analysis of late chunking and contextual retrieval, evaluating their effectiveness and efficiency in optimizing RAG systems. Our results indicate that contextual retrieval preserves semantic coherence more effectively but requires greater computational resources. In contrast, late chunking offers higher efficiency but tends to sacrifice relevance and completeness. 

**Abstract (ZH)**: 检索增强生成（RAG）已成为通过将输出-ground 在外部知识源中增强大规模语言模型（LLMs）的一种变革性方法。然而，一个关键问题仍然存在：如何在LLMs的输入限制内有效地管理大量的外部知识？传统方法通过将外部文档分块成较小的固定大小片段来解决这一问题。虽然这种方法缓解了输入限制，但往往会导致上下文断裂，从而导致检索不完整和生成连贯性降低。为克服这些不足，引入了两种先进的技术——延迟分块和上下文检索，两者都旨在保持全局上下文。尽管它们有潜力，但它们的优势和局限性仍不清楚。本研究对延迟分块和上下文检索进行了严格的分析，评估了它们在优化RAG系统方面的有效性和效率。结果表明，上下文检索在保持语义连贯性方面更有效，但需要更多的计算资源。相比之下，延迟分块提供了更高的效率，但往往会牺牲相关性和完整性。 

---
# Taming the Titans: A Survey of Efficient LLM Inference Serving 

**Title (ZH)**: 驯服巨匠：高效LLM推理服务综述 

**Authors**: Ranran Zhen, Juntao Li, Yixin Ji, Zhenlin Yang, Tong Liu, Qingrong Xia, Xinyu Duan, Zhefeng Wang, Baoxing Huai, Min Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2504.19720)  

**Abstract**: Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving. 

**Abstract (ZH)**: 大型语言模型（LLMs）在生成型AI中的应用取得了显著进步，演变为广泛应用于各个领域和应用的 sophisticated 和多功能工具。然而，其大量参数带来的巨大内存开销以及注意力机制的高计算需求，对LLM推理服务的低延迟和高吞吐量提出了重大挑战。近期突破性的研究成果显著加速了这一领域的发展。本文全面综述了这些方法，涵盖了实例级基础方法、集群级深入策略、新兴场景方向以及其他一些重要但特殊的领域。在实例级，我们回顾了模型部署、请求调度、解码长度预测、存储管理以及去中心化范式。在集群级，我们探讨了GPU集群部署、多实例负载均衡和云服务解决方案。对于新兴场景，我们围绕特定任务、模块和辅助方法组织讨论。为了确保全面性，我们还强调了几个专业但关键的领域。最后，我们概述了进一步推动LLM推理服务领域的潜在研究方向。 

---
# Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs 

**Title (ZH)**: Annif在SemEval-2025任务5：传统XML到文本转换任务借助于LLMs 

**Authors**: Osma Suominen, Juho Inkinen, Mona Lehtinen  

**Link**: [PDF](https://arxiv.org/pdf/2504.19675)  

**Abstract**: This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects), which focussed on subject indexing using large language models (LLMs). The task required creating subject predictions for bibliographic records from the bilingual TIBKAT database using the GND subject vocabulary. Our approach combines traditional natural language processing and machine learning techniques implemented in the Annif toolkit with innovative LLM-based methods for translation and synthetic data generation, and merging predictions from monolingual models. The system ranked first in the all-subjects category and second in the tib-core-subjects category in the quantitative evaluation, and fourth in qualitative evaluations. These findings demonstrate the potential of combining traditional XMTC algorithms with modern LLM techniques to improve the accuracy and efficiency of subject indexing in multilingual contexts. 

**Abstract (ZH)**: SemEval-2025 任务5（LLMs4Subjects）中的 Annif 系统：基于大型语言模型的主题索引研究 

---
# $\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation 

**Title (ZH)**: SAGE: 通用的大语言模型安全性评估框架 

**Authors**: Madhur Jindal, Hari Shrawgi, Parag Agrawal, Sandipan Dandapat  

**Link**: [PDF](https://arxiv.org/pdf/2504.19674)  

**Abstract**: Safety evaluation of Large Language Models (LLMs) has made progress and attracted academic interest, but it remains challenging to keep pace with the rapid integration of LLMs across diverse applications. Different applications expose users to various harms, necessitating application-specific safety evaluations with tailored harms and policies. Another major gap is the lack of focus on the dynamic and conversational nature of LLM systems. Such potential oversights can lead to harms that go unnoticed in standard safety benchmarks. This paper identifies the above as key requirements for robust LLM safety evaluation and recognizing that current evaluation methodologies do not satisfy these, we introduce the $\texttt{SAGE}$ (Safety AI Generic Evaluation) framework. $\texttt{SAGE}$ is an automated modular framework designed for customized and dynamic harm evaluations. It utilizes adversarial user models that are system-aware and have unique personalities, enabling a holistic red-teaming evaluation. We demonstrate $\texttt{SAGE}$'s effectiveness by evaluating seven state-of-the-art LLMs across three applications and harm policies. Our experiments with multi-turn conversational evaluations revealed a concerning finding that harm steadily increases with conversation length. Furthermore, we observe significant disparities in model behavior when exposed to different user personalities and scenarios. Our findings also reveal that some models minimize harmful outputs by employing severe refusal tactics that can hinder their usefulness. These insights highlight the necessity of adaptive and context-specific testing to ensure better safety alignment and safer deployment of LLMs in real-world scenarios. 

**Abstract (ZH)**: 大型语言模型（LLMs）的安全评估取得了进展并吸引了学术界的兴趣，但保持与LLMs在不同应用中快速集成的步伐仍然具有挑战性。不同应用会将用户暴露于各种危害中，因此需要针对特定应用的安全评估和定制化的危害及政策。另一个主要差距是缺乏对LLM系统动态和对话性质的关注。这些潜在的疏忽会导致在标准安全基准中未被注意到的危害。本文认为上述要求是稳健的LLM安全评估的关键，并鉴于当前的评估方法无法满足这些要求，我们引入了SAGE（Safety AI Generic Evaluation）框架。SAGE是一个自动化的模块化框架，旨在进行定制化和动态的危害评估。它利用了系统意识且具有独特个性的对抗性用户模型，实现整体的红队评估。我们通过在三种应用和危害政策下评估七种最先进的LLM来证明SAGE的有效性。我们的多轮对话评估实验揭示了一个令人担忧的发现：随着对话长度的增加，危害呈稳步上升趋势。此外，我们观察到模型在不同用户个性和场景下表现出显著差异。我们的研究结果还表明，一些模型通过采用严厉拒绝的策略来最小化有害输出，这可能妨碍它们的实用性。这些见解强调了适应性和情境特定测试的必要性，以确保更好的安全性对齐并安全部署LLM在实际应用场景中。 

---
# A Tripartite Perspective on GraphRAG 

**Title (ZH)**: 图RAG的三方视角 

**Authors**: Michael Banf, Johannes Kuhn  

**Link**: [PDF](https://arxiv.org/pdf/2504.19667)  

**Abstract**: Large Language Models (LLMs) have shown remarkable capabilities across various domains, yet they struggle with knowledge-intensive tasks in areas that demand factual accuracy, e.g. industrial automation and healthcare. Key limitations include their tendency to hallucinate, lack of source traceability (provenance), and challenges in timely knowledge updates. Combining language models with knowledge graphs (GraphRAG) offers promising avenues for overcoming these deficits. However, a major challenge lies in creating such a knowledge graph in the first place. Here, we propose a novel approach that combines LLMs with a tripartite knowledge graph representation, which is constructed by connecting complex, domain-specific objects via a curated ontology of corresponding, domain-specific concepts to relevant sections within chunks of text through a concept-anchored pre-analysis of source documents starting from an initial lexical graph. As a consequence, our Tripartite-GraphRAG approach implements: i) a concept-specific, information-preserving pre-compression of textual chunks; ii) allows for the formation of a concept-specific relevance estimation of embedding similarities grounded in statistics; and iii) avoids common challenges w.r.t. continuous extendability, such as the need for entity resolution and deduplication. By applying a transformation to the knowledge graph, we formulate LLM prompt creation as an unsupervised node classification problem, drawing on ideas from Markov Random Fields. We evaluate our approach on a healthcare use case, involving multi-faceted analyses of patient anamneses given a set of medical concepts as well as clinical literature. Experiments indicate that it can optimize information density, coverage, and arrangement of LLM prompts while reducing their lengths, which may lead to reduced costs and more consistent and reliable LLM outputs. 

**Abstract (ZH)**: 大型语言模型（LLMs）在多个领域展现了显著的能力，但在需要事实准确性的任务中，如工业自动化和医疗健康领域，它们却显得力不从心。关键限制包括它们的虚构倾向、缺乏可追溯的知识来源（溯源）以及及时知识更新的挑战。将语言模型与知识图谱结合（GraphRAG）提供了一条克服这些缺陷的有前途的道路。然而，一个主要挑战在于首先创建这样的知识图谱。在这里，我们提出了一种新的方法，该方法结合了LLMs与三元关系知识图谱表示，通过概念锚定的预分析，从初始词汇图开始，将复杂领域特定的对象与相关文本片段中的相应领域特定概念相连，构建了一个经过策展的领域特定概念的本体论。因此，我们的Tripartite-GraphRAG方法实现了：i）基于概念的信息保真预压缩；ii）允许形成基于统计的特定概念相关性估计；iii）避免了关于连续扩展的常见挑战，如实体解析和去重。通过对知识图谱进行变换，我们将LLM提示的生成问题形式化为无监督节点分类问题，借鉴马尔可夫随机场的思想。我们在医疗保健用例中评估了该方法，该用例涉及使用一组医学概念和临床文献对患者病史进行多层面分析。实验表明，这种方法可以优化LLM提示的信息密度、覆盖率和布局，同时缩短提示长度，这可能会降低成本，并提高LLM输出的一致性和可靠性。 

---
# m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training 

**Title (ZH)**: m-KAILIN: 知识驱动的生物医学大型语言模型训练代理科学语料库精简框架 

**Authors**: Meng Xiao, Xunxin Cai, Chengrui Wang, Yuanchun Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2504.19565)  

**Abstract**: The rapid progress of large language models (LLMs) in biomedical research has underscored the limitations of existing open-source annotated scientific corpora, which are often insufficient in quantity and quality. Addressing the challenge posed by the complex hierarchy of biomedical knowledge, we propose a knowledge-driven, multi-agent framework for scientific corpus distillation tailored for LLM training in the biomedical domain. Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature. These agents collectively generate and refine domain-specific question-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual involvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced proprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each agent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training. 

**Abstract (ZH)**: 大型语言模型在生物医学研究中的快速发展凸显了现有开源标注科学语料库的局限性，这些语料库在数量和质量上往往不足。为应对生物医学知识复杂层次结构带来的挑战，我们提出了一种知识驱动的多智能体框架，专门用于生物医学领域的语言模型训练数据精炼。该方法的核心是一个协作的多智能体架构，各专门化智能体在医学主题 headings (MeSH) 分层结构的指导下，协同工作以自主提取、综合和自我评估高质量文本数据，从而从海量科学文献中生成并优化特定领域的问答对，确保覆盖全面且与生物医学本体保持一致，同时减少人力干预。大量实验结果表明，训练于我们多智能体精炼数据集上的语言模型在生物医学问答任务上的表现显著提升，优于强有力的生物医学开源模型基线和先进的专有模型。特别地，我们的AI就绪数据集使得Llama3-70B在MedPrompt和Med-PaLM-2的辅助下超越GPT-4。详细的消融研究和案例分析进一步验证了框架内每个智能体的有效性和协同作用，突显了多智能体协作在生物医学语言模型训练中的潜力。 

---
# Improving Reasoning Performance in Large Language Models via Representation Engineering 

**Title (ZH)**: 通过表示工程提高大型语言模型的推理性能 

**Authors**: Bertram Højer, Oliver Jarvis, Stefan Heinrich  

**Link**: [PDF](https://arxiv.org/pdf/2504.19483)  

**Abstract**: Recent advancements in large language models (LLMs) have resulted in increasingly anthropomorphic language concerning the ability of LLMs to reason. Whether reasoning in LLMs should be understood to be inherently different is, however, widely debated. We propose utilizing a representation engineering approach wherein model activations are read from the residual stream of an LLM when processing a reasoning task. The activations are used to derive a control vector that is applied to the model as an inference-time intervention, modulating the representational space of the model, to improve performance on the specified task. We publish the code for deriving control vectors and analyzing model representations. The method allows us to improve performance on reasoning benchmarks and assess how control vectors influence the final logit distribution of a model via metrics such as KL divergence and entropy. We apply control vectors to Mistral-7B-Instruct and a range of Pythia models on an inductive, a deductive and mathematical reasoning task. We show that an LLM can, to a certain degree, be controlled to improve its perceived reasoning ability by modulating activations. The intervention is dependent upon the ability to reliably extract the model's typical state when correctly solving a task. Our results suggest that reasoning performance can be modulated in the same manner as other information-processing tasks performed by LLMs and demonstrate that we are capable of improving performance on specific tasks via a simple intervention on the residual stream with no additional training. 

**Abstract (ZH)**: Recent advancements in大型语言模型（LLMs）近期在大规模语言模型（LLMs）方面的进展已使其在推理能力上的表现越来越具人本特征。然而，LLMs中的推理是否本质上应被视为不同的问题仍广泛存在争议。我们提出了一种表示工程方法，即在LLMs处理推理任务时从残差流中读取模型激活，并利用这些激活来推导出一个控制向量，在推理时将其应用到模型中，以调节模型表示空间，从而提高特定任务上的性能。我们发布了生成控制向量和分析模型表示的代码。该方法使我们能够改进推理基准上的性能，并通过如KL散度和熵等指标评估控制向量如何影响模型最终的logit分布。我们通过对Mistral-7B-Instruct和一系列Pythia模型在归纳、演绎和数学推理任务上的应用，展示了可以通过调节激活来在一定程度上控制LLM以提高其实现的推理能力。该干预依赖于能可靠地提取模型在正确解决任务时典型状态的能力。我们的结果表明，可以通过与LLM执行的其他信息处理任务相同的方式调节推理性能，并证明我们可以通过在残差流中进行简单干预来提高特定任务的性能，而无需额外训练。 

---
# An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination 

**Title (ZH)**: 基于大型语言模型的自动强化学习奖励设计框架及其在协同车队协调中的应用 

**Authors**: Dixiao Wei, Peng Yi, Jinlong Lei, Yiguang Hong, Yuchuan Du  

**Link**: [PDF](https://arxiv.org/pdf/2504.19480)  

**Abstract**: Reinforcement Learning (RL) has demonstrated excellent decision-making potential in platoon coordination problems. However, due to the variability of coordination goals, the complexity of the decision problem, and the time-consumption of trial-and-error in manual design, finding a well performance reward function to guide RL training to solve complex platoon coordination problems remains challenging. In this paper, we formally define the Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based cooperative platoon coordination problem to incorporate automated reward function generation. To address PCRDP, we propose a Large Language Model (LLM)-based Platoon coordination Reward Design (PCRD) framework, which systematically automates reward function discovery through LLM-driven initialization and iterative optimization. In this method, LLM first initializes reward functions based on environment code and task requirements with an Analysis and Initial Reward (AIR) module, and then iteratively optimizes them based on training feedback with an evolutionary module. The AIR module guides LLM to deepen their understanding of code and tasks through a chain of thought, effectively mitigating hallucination risks in code generation. The evolutionary module fine-tunes and reconstructs the reward function, achieving a balance between exploration diversity and convergence stability for training. To validate our approach, we establish six challenging coordination scenarios with varying complexity levels within the Yangtze River Delta transportation network simulation. Comparative experimental results demonstrate that RL agents utilizing PCRD-generated reward functions consistently outperform human-engineered reward functions, achieving an average of 10\% higher performance metrics in all scenarios. 

**Abstract (ZH)**: 基于大型语言模型的编队协调奖励设计（PCRD）框架 

---
# BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text 

**Title (ZH)**: BRIDGE: 大型语言模型在理解临床实践文本方面的基准测试 

**Authors**: Jiageng Wu, Bowen Gu, Ren Zhou, Kevin Xie, Doug Snyder, Yixing Jiang, Valentina Carducci, Richard Wyss, Rishi J Desai, Emily Alsentzer, Leo Anthony Celi, Adam Rodman, Sebastian Schneeweiss, Jonathan H. Chen, Santiago Romero-Brufau, Kueiyu Joshua Lin, Jie Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.19467)  

**Abstract**: Large language models (LLMs) hold great promise for medical applications and are evolving rapidly, with new models being released at an accelerated pace. However, current evaluations of LLMs in clinical contexts remain limited. Most existing benchmarks rely on medical exam-style questions or PubMed-derived text, failing to capture the complexity of real-world electronic health record (EHR) data. Others focus narrowly on specific application scenarios, limiting their generalizability across broader clinical use. To address this gap, we present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks sourced from real-world clinical data sources across nine languages. We systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1, GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total of 13,572 experiments, our results reveal substantial performance variation across model sizes, languages, natural language processing tasks, and clinical specialties. Notably, we demonstrate that open-source LLMs can achieve performance comparable to proprietary models, while medically fine-tuned LLMs based on older architectures often underperform versus updated general-purpose models. The BRIDGE and its corresponding leaderboard serve as a foundational resource and a unique reference for the development and evaluation of new LLMs in real-world clinical text understanding. 

**Abstract (ZH)**: 大型语言模型（LLMs）在医学应用中展现出巨大潜力并迅速发展，但目前临床情境下的评估仍有限制。现有的大多数基准依赖于医学考试风格的问题或PubMed衍生的文本，无法捕捉现实世界电子健康记录（EHR）数据的复杂性。其他基准则专注于特定的应用场景，限制了其在更广泛的临床应用中的普适性。为填补这一空白，我们提出了BRIDGE，一个由九种语言的87个任务构成的综合多语言基准，这些任务源自实际临床数据源。我们系统地评估了52个最先进的LLM（包括DeepSeek-R1、GPT-4o、Gemini和Llama 4）在各种推理策略下的表现。通过总计13,572次实验，我们的结果揭示了不同模型规模、语言、自然语言处理任务和临床专科之间的显著性能差异。值得注意的是，我们证明开源LLM的表现可与专有模型相媲美，而基于较旧架构的医学细调LLM通常不如更新的一般用途模型。BRIDGE及其对应的排行榜为新LLM在实际临床文本理解中的开发和评估提供了基础资源和独特参考。 

---
# Towards Long Context Hallucination Detection 

**Title (ZH)**: 长上下文幻觉检测 

**Authors**: Siyi Liu, Kishaloy Halder, Zheng Qi, Wei Xiao, Nikolaos Pappas, Phu Mon Htut, Neha Anna John, Yassine Benajiba, Dan Roth  

**Link**: [PDF](https://arxiv.org/pdf/2504.19457)  

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. However, they are prone to contextual hallucination, generating information that is either unsubstantiated or contradictory to the given context. Although many studies have investigated contextual hallucinations in LLMs, addressing them in long-context inputs remains an open problem. In this work, we take an initial step toward solving this problem by constructing a dataset specifically designed for long-context hallucination detection. Furthermore, we propose a novel architecture that enables pre-trained encoder models, such as BERT, to process long contexts and effectively detect contextual hallucinations through a decomposition and aggregation mechanism. Our experimental results show that the proposed architecture significantly outperforms previous models of similar size as well as LLM-based models across various metrics, while providing substantially faster inference. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各种任务中展现了卓越的性能。然而，它们容易出现情境幻觉，生成与给定情境不实或矛盾的信息。尽管许多研究已经探讨了LLMs的情境幻觉问题，但在长情境输入中的应对仍是待解决的问题。在本工作中，我们采取初步步骤解决这一问题，构建了一个专门用于长情境幻觉检测的数据集。此外，我们提出了一种新型架构，使预训练编码器模型（如BERT）能够处理长情境并通过分解和聚合机制有效地检测情境幻觉。实验结果表明，所提出的架构在多种指标上显著优于相似规模的先前模型以及基于LLM的模型，同时提供显著更快的推理速度。 

---
# Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory 

**Title (ZH)**: Mem0: 构建具有可扩展长期记忆的生产级AI代理 

**Authors**: Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, Deshraj Yadav  

**Link**: [PDF](https://arxiv.org/pdf/2504.19413)  

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2% higher overall score than the base configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to full-context method. In particular, Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在生成上下文一致的回复方面表现出色，但其固定的上下文窗口限制了其在长时间多轮对话中保持一致性的能力。我们提出了Mem0，这是一种可扩展的记忆中心架构，通过动态提取、整合和检索正在进行的对话中的关键信息来解决这一问题。在此基础上，我们进一步提出了一种增强变体，利用图基 Memories 表征来捕捉对话元素之间的复杂关系结构。通过在LOCOMO基准上的全面评估，我们系统地将我们的方法与六类基线方法进行了比较：（i）现有的记忆增强系统，（ii）不同分块大小和k值的检索增强生成（RAG），（iii）处理整个对话历史的全上下文方法，（iv）开源的记忆解决方案，（v）专有模型系统，和（vi）专用的记忆管理平台。实验结果表明，我们的方法在四个问题类别（单跳、时间依赖、多跳和开放领域）上始终优于所有现有记忆系统。值得注意的是，Mem0在LLM-as-a-Judge指标上相对于OpenAI实现了26%的相对改进，而带有图记忆的Mem0的整体分数比基线配置高出约2%。除了准确性提升外，我们还将计算开销比全上下文方法显著降低。特别是，Mem0将第95百分位延迟降低了91%，节省了超过90%的令牌成本，提供了强大的推理论证能力和实际部署约束之间的良好平衡。我们的研究突显了结构化持久记忆机制对长期对话连贯性的重要作用，为更可靠和高效的LLM驱动AI代理的发展铺平了道路。 

---
# LLMs for Engineering: Teaching Models to Design High Powered Rockets 

**Title (ZH)**: LLMs for Engineering: 教授模型设计高性能火箭 

**Authors**: Toby Simonds  

**Link**: [PDF](https://arxiv.org/pdf/2504.19394)  

**Abstract**: Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs' capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development. 

**Abstract (ZH)**: 大规模语言模型在高功率火箭设计中的能力评估：增强学习的启示 

---
# From Inductive to Deductive: LLMs-Based Qualitative Data Analysis in Requirements Engineering 

**Title (ZH)**: 从归纳到演绎：基于LLMs的定性数据要求工程分析 

**Authors**: Syed Tauhid Ullah Shah, Mohamad Hussein, Ann Barcomb, Mohammad Moshirpour  

**Link**: [PDF](https://arxiv.org/pdf/2504.19384)  

**Abstract**: Requirements Engineering (RE) is essential for developing complex and regulated software projects. Given the challenges in transforming stakeholder inputs into consistent software designs, Qualitative Data Analysis (QDA) provides a systematic approach to handling free-form data. However, traditional QDA methods are time-consuming and heavily reliant on manual effort. In this paper, we explore the use of Large Language Models (LLMs), including GPT-4, Mistral, and LLaMA-2, to improve QDA tasks in RE. Our study evaluates LLMs' performance in inductive (zero-shot) and deductive (one-shot, few-shot) annotation tasks, revealing that GPT-4 achieves substantial agreement with human analysts in deductive settings, with Cohen's Kappa scores exceeding 0.7, while zero-shot performance remains limited. Detailed, context-rich prompts significantly improve annotation accuracy and consistency, particularly in deductive scenarios, and GPT-4 demonstrates high reliability across repeated runs. These findings highlight the potential of LLMs to support QDA in RE by reducing manual effort while maintaining annotation quality. The structured labels automatically provide traceability of requirements and can be directly utilized as classes in domain models, facilitating systematic software design. 

**Abstract (ZH)**: 大规模语言模型在需求工程中的质性数据分析应用研究 

---
# Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing 

**Title (ZH)**: 统一的多任务学习与模型融合以实现高效的语言模型约束 

**Authors**: James O' Neill, Santhosh Subramanian, Eric Lin, Vaikkunth Mugunthan  

**Link**: [PDF](https://arxiv.org/pdf/2504.19333)  

**Abstract**: The trend towards large language models (LLMs) for guardrailing against undesired behaviors is increasing and has shown promise for censoring user inputs. However, increased latency, memory consumption, hosting expenses and non-structured outputs can make their use prohibitive.
In this work, we show that task-specific data generation can lead to fine-tuned classifiers that significantly outperform current state of the art (SoTA) while being orders of magnitude smaller. Secondly, we show that using a single model, \texttt{MultiTaskGuard}, that is pretrained on a large synthetically generated dataset with unique task instructions further improves generalization. Thirdly, our most performant models, \texttt{UniGuard}, are found using our proposed search-based model merging approach that finds an optimal set of parameters to combine single-policy models and multi-policy guardrail models. %
On 7 public datasets and 4 guardrail benchmarks we created, our efficient guardrail classifiers improve over the best performing SoTA publicly available LLMs and 3$^{\text{rd}}$ party guardrail APIs in detecting unsafe and safe behaviors by an average F1 score improvement of \textbf{29.92} points over Aegis-LlamaGuard and \textbf{21.62} over \texttt{gpt-4o}, respectively. Lastly, our guardrail synthetic data generation process that uses custom task-specific guardrail poli 

**Abstract (ZH)**: 大型语言模型（LLMs）用于防止不良行为的趋势在增加，并显示出对控制用户输入的有效性。然而，增加的延迟、内存消耗、托管费用和非结构化输出可能使其使用变得不可行。
在本工作中，我们展示了任务特定的数据生成可以导致显著优于当前最先进（SOTA）技术且规模小多个数量级的微调分类器。其次，我们展示了使用一个预训练在大量合成数据集上的多任务模型\(\texttt{MultiTaskGuard}\)，进一步提高了泛化能力。第三，我们使用提出的一种基于搜索的模型融合方法找到了最优参数组合，形成了性能最出色的模型\(\texttt{UniGuard}\)。在7个公开数据集和4个防护护栏基准测试中，我们的高效防护护栏分类器在检测不良和安全行为上相较于Aegis-LlamaGuard和\(\texttt{gpt-4o}\)分别获得了平均F1分数提升29.92和21.62。最后，我们的防护护栏合成数据生成过程使用了自定义的任务特定防护护栏策略。 

---
# Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers 

**Title (ZH)**: 语言模型的不确定性量化：一套黑盒、白盒、大规模语言模型评估器和集成评分器方法 

**Authors**: Dylan Bouchard, Mohit Singh Chauhan  

**Link**: [PDF](https://arxiv.org/pdf/2504.19254)  

**Abstract**: Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we propose a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we introduce a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs. 

**Abstract (ZH)**: 大规模语言模型中的幻觉是一个持久性问题。随着这些模型在高 stakes 领域，如医疗保健和金融中的应用日益增多，有效的幻觉检测需求变得至关重要。为此，我们提出了一种灵活的零资源幻觉检测框架，供实践者应用于实际应用场景。为此，我们适应了各种现有的不确定性量化（UQ）技术，包括黑盒 UQ、白盒 UQ 和 LLM 作为裁决者，必要时将它们转化为标准化的 0 到 1 范围内的响应级置信度分数。为了增强灵活性，我们引入了一种可调 ensemble 方法，可结合任何个体置信度分数的组合。该方法使实践者能够针对特定用例优化 ensemble，从而提高性能。为了简化实施，我们在本文的配套 Python 工具包 UQLM 中提供了所有评分器。为了评估各种评分器的性能，我们在多个 LLM 回答问题基准上进行了广泛实验。我们发现，我们的可调 ensemble 通常优于其个体组件，并且优于现有的幻觉检测方法。我们的结果证明了定制化幻觉检测策略对于提高大规模语言模型的准确性和可靠性的好处。 

---
# Hierarchical Attention Generates Better Proofs 

**Title (ZH)**: 层次注意力生成更好的证明 

**Authors**: Jianlong Chen, Chao Li, Yang Yuan, Andrew C Yao  

**Link**: [PDF](https://arxiv.org/pdf/2504.19188)  

**Abstract**: Large language models (LLMs) have shown promise in formal theorem proving, but their token-level processing often fails to capture the inherent hierarchical nature of mathematical proofs. We introduce \textbf{Hierarchical Attention}, a regularization method that aligns LLMs' attention mechanisms with mathematical reasoning structures. Our approach establishes a five-level hierarchy from foundational elements to high-level concepts, ensuring structured information flow in proof generation. Experiments demonstrate that our method improves proof success rates by 2.05\% on miniF2F and 1.69\% on ProofNet while reducing proof complexity by 23.81\% and 16.50\% respectively. The code is available at this https URL. 

**Abstract (ZH)**: 大型语言模型在形式定理证明中的应用展示了潜力，但它们在令牌级处理方面的不足往往无法捕捉数学证明固有的层级结构。我们引入了层次注意力作为一种正则化方法，以使大型语言模型的注意力机制与数学推理结构对齐。我们的方法建立了从基础元素到高层次概念的五级层次结构，确保证明生成中的结构化信息流。实验结果表明，我们的方法在miniF2F上将证明成功率提高了2.05%，在ProofNet上提高了1.69%，同时分别降低了23.81%和16.50%的证明复杂度。代码可在此处访问：this https URL。 

---
# SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning 

**Title (ZH)**: SPC：通过对抗游戏演化的自我对弈批评者用于LLM推理 

**Authors**: Jiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li, Kwan-Yee K. Wong  

**Link**: [PDF](https://arxiv.org/pdf/2504.19162)  

**Abstract**: Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a "sneaky generator" that deliberately produces erroneous steps designed to be difficult to detect, and a "critic" that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, applying SPC to guide the test-time search of diverse LLMs significantly improves their mathematical reasoning performance on MATH500 and AIME2024, outperforming state-of-the-art process reward models. 

**Abstract (ZH)**: 评估大型语言模型（LLM）推理过程逐步可靠性的挑战在于难以获得高质量的逐步骤监督。本文介绍了一种新颖的方法——自我博弈评论者（SPC），该方法通过 adversarial self-play 游戏使评论者模型发展出评估推理步骤的能力，从而消除手工逐步骤标注的需要。SPC 包括对一个基础模型的两个副本进行微调，分别扮演“狡猾的生成器”和“评论者”两个角色，“狡猾的生成器”故意生成难以检测的错误步骤，而“评论者”则分析推理步骤的正确性。这两个模型进行一种对抗性游戏，生成器旨在欺骗评论者，而评论者模型试图识别生成器的错误。基于游戏结果的强化学习使模型在迭代中不断改进；每次对抗的胜者获得正强化，而失败者获得负强化，驱动持续的自我进化。实验结果表明，SPC 在三个推理过程基准（ProcessBench、PRM800K、DeltaBench）上逐步增强其错误检测能力（例如，在 ProcessBench 上准确率从 70.8% 提高到 77.7%），并超越了包括蒸馏 R1 模型在内的强基线。此外，将 SPC 应用于指导不同 LLM 的测试时搜索显着提高了其在 MATH500 和 AIME2024 上的数学推理性能，超越了最先进的过程奖励模型。 

---
# VeriDebug: A Unified LLM for Verilog Debugging via Contrastive Embedding and Guided Correction 

**Title (ZH)**: VeriDebug: 一种基于对比嵌入和引导修正的统一Verilog调试大模型 

**Authors**: Ning Wang, Bingkun Yao, Jie Zhou, Yuchen Hu, Xi Wang, Nan Guan, Zhe Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2504.19099)  

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable potential in debugging for various programming languages. However, the application of LLMs to Verilog debugging remains insufficiently explored. Here, we present VeriDebug, an approach that integrates contrastive representation and guided correction capabilities for automated Verilog debugging. Unlike existing methods, VeriDebug employs an embedding-based technique to accurately retrieve internal information, followed by bug-fixing. VeriDebug unifies Verilog bug detection and correction through a shared parameter space. By simultaneously learning bug patterns and fixes, it streamlines debugging via contrastive embedding and guided correction. Empirical results show the efficacy of VeriDebug in enhancing Verilog debugging. Our VeriDebugLoc, Type model achieves 64.7 accuracy in bug fixing (Acc1), a significant improvement from the existing open-source SOTAs 11.3. This performance not only outperforms open-source alternatives but also exceeds larger closed-source models like GPT-3.5-turbo (36.6), offering a more accurate alternative to conventional debugging methods. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在各种编程语言的调试中展示了显著的潜力。然而，LLMs在Verilog调试中的应用仍研究不足。在此，我们提出VeriDebug，一种结合对比表示和指导修正能力的自动化Verilog调试方法。与现有方法不同，VeriDebug采用基于嵌入的技术准确检索内部信息，随后进行错误修复。VeriDebug通过共享参数空间统一了Verilog错误检测与修正。通过同时学习错误模式和修复方法，它利用对比嵌入和指导修正简化了调试过程。实验结果表明，VeriDebug在提高Verilog调试效果方面具有有效性。我们的VeriDebugLoc、Type模型在错误修复上的准确率达到64.7%，显著优于现有的开源SOTA方法11.3。该性能不仅超过了开源替代方案，还超过了如GPT-3.5-turbo这样的更大封闭源模型（36.6），提供了传统调试方法的更准确替代方案。 

---
# CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through Cryptography Challenges 

**Title (ZH)**: CipherBank：通过密码学挑战探索LLM推理能力的边界 

**Authors**: Yu Li, Qizhi Pei, Mengyuan Sun, Honglin Lin, Chenlin Ming, Xin Gao, Jiang Wu, Conghui He, Lijun Wu  

**Link**: [PDF](https://arxiv.org/pdf/2504.19093)  

**Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities, especially the recent advancements in reasoning, such as o1 and o3, pushing the boundaries of AI. Despite these impressive achievements in mathematics and coding, the reasoning abilities of LLMs in domains requiring cryptographic expertise remain underexplored. In this paper, we introduce CipherBank, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously crafted problems, covering 262 unique plaintexts across 5 domains and 14 subdomains, with a focus on privacy-sensitive and real-world scenarios that necessitate encryption. From a cryptographic perspective, CipherBank incorporates 3 major categories of encryption methods, spanning 9 distinct algorithms, ranging from classical ciphers to custom cryptographic techniques. We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results reveal significant gaps in reasoning abilities not only between general-purpose chat LLMs and reasoning-focused LLMs but also in the performance of current reasoning-focused models when applied to classical cryptographic decryption tasks, highlighting the challenges these models face in understanding and manipulating encrypted data. Through detailed analysis and error investigations, we provide several key observations that shed light on the limitations and potential improvement areas for LLMs in cryptographic reasoning. These findings underscore the need for continuous advancements in LLM reasoning capabilities. 

**Abstract (ZH)**: 大型语言模型（LLMs）展示了卓越的能力，尤其是近期在推理方面的发展，如o1和o3，正不断拓展人工智能的边界。尽管LLMs在数学和编程领域取得了令人印象深刻的成绩，但在需要密码学专业知识的领域中的推理能力仍待深入探索。本文介绍了CipherBank，一个全面的基准测试，用于评估LLMs在密码学解密任务中的推理能力。CipherBank包含2,358个精心设计的问题，涵盖了5个领域和14个子领域中的262个独特的明文，重点是涉及隐私保护和现实世界场景的加密需求。从密码学角度来看，CipherBank涵盖了3类主要的加密方法，包括9种不同的算法，从古典密码学到定制的加密技术。我们对CipherBank进行了最先进的LLMs的评估，例如GPT-4o、DeepSeek-V3以及重点推理模型如o1和DeepSeek-R1。我们的结果显示，在推理能力和当前推理模型应用于古典密码学解密任务时的性能之间存在显著差距，突显了这些模型在理解和操作加密数据时面临的挑战。通过详细的分析和错误调查，我们提供了几个关键观察结果，这些观察结果揭示了LLMs在密码学推理中的局限性和改进领域。这些发现强调了持续提升LLMs推理能力的需求。 

---
# ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics 

**Title (ZH)**: ClimaEmpact: 领域对齐的小型语言模型和数据集用于极端天气分析 

**Authors**: Deeksha Varshney, Keane Ong, Rui Mao, Erik Cambria, Gianmarco Mengaldo  

**Link**: [PDF](https://arxiv.org/pdf/2504.19066)  

**Abstract**: Accurate assessments of extreme weather events are vital for research and policy, yet localized and granular data remain scarce in many parts of the world. This data gap limits our ability to analyze potential outcomes and implications of extreme weather events, hindering effective decision-making. Large Language Models (LLMs) can process vast amounts of unstructured text data, extract meaningful insights, and generate detailed assessments by synthesizing information from multiple sources. Furthermore, LLMs can seamlessly transfer their general language understanding to smaller models, enabling these models to retain key knowledge while being fine-tuned for specific tasks. In this paper, we propose Extreme Weather Reasoning-Aware Alignment (EWRA), a method that enhances small language models (SLMs) by incorporating structured reasoning paths derived from LLMs, and ExtremeWeatherNews, a large dataset of extreme weather event-related news articles. EWRA and ExtremeWeatherNews together form the overall framework, ClimaEmpact, that focuses on addressing three critical extreme-weather tasks: categorization of tangible vulnerabilities/impacts, topic labeling, and emotion analysis. By aligning SLMs with advanced reasoning strategies on ExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for SLM alignment), EWRA improves the SLMs' ability to generate well-grounded and domain-specific responses for extreme weather analytics. Our results show that the approach proposed guides SLMs to output domain-aligned responses, surpassing the performance of task-specific models and offering enhanced real-world applicability for extreme weather analytics. 

**Abstract (ZH)**: 准确评估极端天气事件对于研究和政策制定至关重要，但许多地区仍缺乏局部和精细的数据。数据缺口限制了我们分析极端天气事件潜在结果及其影响的能力，妨碍了有效的决策。大规模语言模型（LLMs）可以处理大量的非结构化文本数据，提取有意义的见解，并通过综合多个来源的信息生成详细的评估。此外，LLMs 可以平滑地将其一般语言理解转移到较小的模型中，使这些模型在保持关键知识的同时，能够针对特定任务进行微调。在本文中，我们提出了一种方法——极端天气推理感知对齐（EWRA），该方法通过整合从LLMs中导出的结构化推理路径来增强小型语言模型（SLMs），并提供了一个与极端天气事件相关的新闻文章大规模数据集——ExtremeWeatherNews。EWRA和ExtremeWeatherNews共同构成了专注于解决三个关键极端天气任务的总体框架——ClimaEmpact：具体脆弱性/影响分类、主题标签和情感分析。通过在ExtremeWeatherNews（及其用于SLM对齐的衍生数据集ExtremeAlign）上对SLMs进行高级推理策略对齐，EWRA提高了SLMs生成坚实且领域特定的极端天气分析响应的能力。我们的结果显示，该方法指导SLMs输出领域对齐的响应，其性能超过了专门任务模型，并在极端天气分析中具备增强的现实世界应用性。 

---
# Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models 

**Title (ZH)**: 医学文本中的幻觉与关键信息提取：开源大型语言模型的全面评估 

**Authors**: Anindya Bijoy Das, Shibbir Ahmed, Shahnewaz Karim Sakib  

**Link**: [PDF](https://arxiv.org/pdf/2504.19061)  

**Abstract**: Clinical summarization is crucial in healthcare as it distills complex medical data into digestible information, enhancing patient understanding and care management. Large language models (LLMs) have shown significant potential in automating and improving the accuracy of such summarizations due to their advanced natural language understanding capabilities. These models are particularly applicable in the context of summarizing medical/clinical texts, where precise and concise information transfer is essential. In this paper, we investigate the effectiveness of open-source LLMs in extracting key events from discharge reports, such as reasons for hospital admission, significant in-hospital events, and critical follow-up actions. In addition, we also assess the prevalence of various types of hallucinations in the summaries produced by these models. Detecting hallucinations is vital as it directly influences the reliability of the information, potentially affecting patient care and treatment outcomes. We conduct comprehensive numerical simulations to rigorously evaluate the performance of these models, further probing the accuracy and fidelity of the extracted content in clinical summarization. 

**Abstract (ZH)**: 医疗卫生中的临床总结至关重要，它能将复杂的医疗数据提炼为易于理解的信息，提升患者的理解能力和护理管理。大型语言模型（LLMs）由于其先进的自然语言理解能力，shows了在自动化和提高此类总结的准确性方面的巨大潜力。这些模型特别适用于医疗/临床文本的总结，因为精确和简洁的信息传递是必不可少的。在本文中，我们研究了开源LLMs在从出院报告中提取关键事件（如住院原因、院内重要事件和关键后续行动）方面的有效性。此外，我们还评估了这些模型生成的总结中各种类型幻觉的出现频率。检测幻觉至关重要，因为它直接影响信息的可靠性，可能影响患者的护理和治疗结果。我们进行了全面的数值模拟，以严格评估这些模型的性能，进一步探究提取内容在临床总结中的准确性和保真度。 

---
# Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs 

**Title (ZH)**: 攻击图：改进的黑盒和可解释的大语言模型逃狱攻击 

**Authors**: Mohammad Akbar-Tajari, Mohammad Taher Pilehvar, Mohammad Mahmoody  

**Link**: [PDF](https://arxiv.org/pdf/2504.19019)  

**Abstract**: The challenge of ensuring Large Language Models (LLMs) align with societal standards is of increasing interest, as these models are still prone to adversarial jailbreaks that bypass their safety mechanisms. Identifying these vulnerabilities is crucial for enhancing the robustness of LLMs against such exploits. We propose Graph of ATtacks (GoAT), a method for generating adversarial prompts to test the robustness of LLM alignment using the Graph of Thoughts framework [Besta et al., 2024]. GoAT excels at generating highly effective jailbreak prompts with fewer queries to the victim model than state-of-the-art attacks, achieving up to five times better jailbreak success rate against robust models like Llama. Notably, GoAT creates high-quality, human-readable prompts without requiring access to the targeted model's parameters, making it a black-box attack. Unlike approaches constrained by tree-based reasoning, GoAT's reasoning is based on a more intricate graph structure. By making simultaneous attack paths aware of each other's progress, this dynamic framework allows a deeper integration and refinement of reasoning paths, significantly enhancing the collaborative exploration of adversarial vulnerabilities in LLMs. At a technical level, GoAT starts with a graph structure and iteratively refines it by combining and improving thoughts, enabling synergy between different thought paths. The code for our implementation can be found at: this https URL. 

**Abstract (ZH)**: 确保大型语言模型（LLMs）符合社会标准的挑战日益引起关注，因为这些模型仍容易受到规避其安全性机制的对抗性攻击。识别这些漏洞对于增强LLMs对抗此类攻击的稳健性至关重要。我们提出了一种名为Graph of ATtacks（GoAT）的方法，该方法使用Graph of Thoughts框架[Besta et al., 2024]生成对抗性提示，以测试LLMs对齐的稳健性。GoAT在生成有效的对抗性脱逃提示方面表现出色，所需查询次数少于最先进的攻击方法，against稳健模型（如Llama）的脱逃成功率最高可提高五倍。值得注意的是，GoAT能够生成高质量、易于理解的提示，无需访问目标模型的参数，从而使其成为一种黑盒攻击。与受限于树状推理的方法不同，GoAT的推理基于更复杂的图结构。通过使同时进行的攻击路径相互意识到彼此的进展，这一动态框架允许更深层次的推理路径集成和优化，显著增强了对LLMs对抗性漏洞的协作探索。从技术层面来看，GoAT从一个图结构开始，并通过结合和改进思想加以迭代优化，从而实现不同思想路径间的协同作用。我们的实现代码可在以下地址找到：this https URL。 

---
# LawFlow : Collecting and Simulating Lawyers' Thought Processes 

**Title (ZH)**: LawFlow：收集和模拟律师思维过程 

**Authors**: Debarati Das, Khanh Chi Le, Ritik Sachin Parkar, Karin De Langis, Brendan Madson, Chad M. Berryman, Robin M. Willis, Daniel H. Moses, Brett McDonnell, Daniel Schwarcz, Dongyeop Kang  

**Link**: [PDF](https://arxiv.org/pdf/2504.18942)  

**Abstract**: Legal practitioners, particularly those early in their careers, face complex, high-stakes tasks that require adaptive, context-sensitive reasoning. While AI holds promise in supporting legal work, current datasets and models are narrowly focused on isolated subtasks and fail to capture the end-to-end decision-making required in real-world practice. To address this gap, we introduce LawFlow, a dataset of complete end-to-end legal workflows collected from trained law students, grounded in real-world business entity formation scenarios. Unlike prior datasets focused on input-output pairs or linear chains of thought, LawFlow captures dynamic, modular, and iterative reasoning processes that reflect the ambiguity, revision, and client-adaptive strategies of legal practice. Using LawFlow, we compare human and LLM-generated workflows, revealing systematic differences in structure, reasoning flexibility, and plan execution. Human workflows tend to be modular and adaptive, while LLM workflows are more sequential, exhaustive, and less sensitive to downstream implications. Our findings also suggest that legal professionals prefer AI to carry out supportive roles, such as brainstorming, identifying blind spots, and surfacing alternatives, rather than executing complex workflows end-to-end. Building on these findings, we propose a set of design suggestions, rooted in empirical observations, that align AI assistance with human goals of clarity, completeness, creativity, and efficiency, through hybrid planning, adaptive execution, and decision-point support. Our results highlight both the current limitations of LLMs in supporting complex legal workflows and opportunities for developing more collaborative, reasoning-aware legal AI systems. All data and code are available on our project page (this https URL). 

**Abstract (ZH)**: 法律从业者，特别是在职初期的从业者，面临复杂且高风险的任务，这些任务需要适应性强、情境敏感的推理能力。虽然人工智能在支持法律工作方面充满潜力，但目前的数据集和模型仅专注于孤立子任务，无法捕捉到实际法律实践中所需的端到端决策过程。为解决这一缺口，我们引入了LawFlow数据集，该数据集来源于训练有素的法律学生，并基于实际的企业实体成立场景收集了完整的端到端法律工作流程。与侧重于输入-输出对或线性思维链的数据集不同，LawFlow捕捉了动态、模块化和迭代的推理过程，反映了法律实践中模糊性、修改和针对客户的调整策略。使用LawFlow，我们比较了人类和大型语言模型（LLM）生成的工作流程，揭示了结构、推理灵活性和计划执行方面的系统性差异。人类的工作流程倾向于模块化和适应性，而LLM的工作流程则更加顺序化、详尽，并且较少关注下游影响。我们的研究结果还表明，法律专业人士更倾向于让AI承担支持性角色，例如头脑风暴、识别盲点和呈现替代方案，而不是执行复杂的端到端工作流程。基于这些发现，我们提出了基于实证观察的一些建议，旨在通过混合规划、适应性执行和决策点支持，使AI辅助与人类追求的清晰度、完整性、创造性及效率目标相一致。我们的研究结果强调了当前LLM在支持复杂法律工作流程方面的局限性以及开发更具协作性和推理意识的法律AI系统的机遇。所有数据和代码均可在我们的项目页面（点击此处）获得。 

---
# AI Chatbots for Mental Health: Values and Harms from Lived Experiences of Depression 

**Title (ZH)**: AI聊天机器人用于心理健康：抑郁症患者亲身体验的价值与危害 

**Authors**: Dong Whi Yoo, Jiayue Melissa Shi, Violeta J. Rodriguez, Koustuv Saha  

**Link**: [PDF](https://arxiv.org/pdf/2504.18932)  

**Abstract**: Recent advancements in LLMs enable chatbots to interact with individuals on a range of queries, including sensitive mental health contexts. Despite uncertainties about their effectiveness and reliability, the development of LLMs in these areas is growing, potentially leading to harms. To better identify and mitigate these harms, it is critical to understand how the values of people with lived experiences relate to the harms. In this study, we developed a technology probe, a GPT-4o based chatbot called Zenny, enabling participants to engage with depression self-management scenarios informed by previous research. We used Zenny to interview 17 individuals with lived experiences of depression. Our thematic analysis revealed key values: informational support, emotional support, personalization, privacy, and crisis management. This work explores the relationship between lived experience values, potential harms, and design recommendations for mental health AI chatbots, aiming to enhance self-management support while minimizing risks. 

**Abstract (ZH)**: Recent advancements in LLMs使聊天机器人能够就一系列查询与个人进行互动，包括敏感的心理健康情境。尽管对其有效性和可靠性存在不确定性，但在这些领域的LLM开发正在增长，可能会导致潜在的危害。为了更好地识别和缓解这些危害，理解有亲身经历的人的价值观与危害之间的关系至关重要。在本研究中，我们开发了一种技术探针——基于GPT-4o的聊天机器人Zenny，让参与者能够参与由先前研究支持的抑郁症自我管理情境进行互动。我们使用Zenny对17名经历过抑郁症的个人进行了访谈。我们的主题分析揭示了关键价值观：信息支持、情感支持、个性化、隐私和危机管理。这项工作探索了亲身经历的价值观、潜在危害和心理健康AI聊天机器人的设计理念之间的关系，旨在增强自我管理支持的同时减少风险。 

---
# Clinical knowledge in LLMs does not translate to human interactions 

**Title (ZH)**: LLM中的临床知识无法转化为人类互动。 

**Authors**: Andrew M. Bean, Rebecca Payne, Guy Parsons, Hannah Rose Kirk, Juan Ciro, Rafael Mosquera, Sara Hincapié Monsalve, Aruna S. Ekanayaka, Lionel Tarassenko, Luc Rocher, Adam Mahdi  

**Link**: [PDF](https://arxiv.org/pdf/2504.18919)  

**Abstract**: Global healthcare providers are exploring use of large language models (LLMs) to provide medical advice to the public. LLMs now achieve nearly perfect scores on medical licensing exams, but this does not necessarily translate to accurate performance in real-world settings. We tested if LLMs can assist members of the public in identifying underlying conditions and choosing a course of action (disposition) in ten medical scenarios in a controlled study with 1,298 participants. Participants were randomly assigned to receive assistance from an LLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested alone, LLMs complete the scenarios accurately, correctly identifying conditions in 94.9% of cases and disposition in 56.3% on average. However, participants using the same LLMs identified relevant conditions in less than 34.5% of cases and disposition in less than 44.2%, both no better than the control group. We identify user interactions as a challenge to the deployment of LLMs for medical advice. Standard benchmarks for medical knowledge and simulated patient interactions do not predict the failures we find with human participants. Moving forward, we recommend systematic human user testing to evaluate interactive capabilities prior to public deployments in healthcare. 

**Abstract (ZH)**: 全球医疗提供者探索使用大规模语言模型（LLMs）为公众提供医疗建议。尽管LLMs现在在医学执照考试中的得分几乎达到完美，但在实际应用中并不一定能保证准确性能。我们通过一项包含1,298名参与者的控制研究，测试LLMs在十种医疗场景中协助公众识别潜在疾病并选择处理方案的能力。参与者被随机分配使用LLM（GPT-4o、Llama 3、Command R+）或他们自行选择的资源（对照组）。单独使用时，LLMs准确完成这些场景，识别疾病准确率为94.9%，选择处理方案的平均准确率为56.3%。然而，使用相同LLM的参与者实际识别出相关疾病的比例低于34.5%，选择处理方案的比例低于44.2%，均未超过对照组的水平。我们指出用户交互是部署LLMs提供医疗建议的一个挑战。现有的医学知识标准基准和模拟患者互动测试无法预测我们发现的人类参与者失败情况。未来，我们建议在公共医疗部署前系统地进行人类用户测试，以评估交互能力。 

---
# Transformer-Empowered Actor-Critic Reinforcement Learning for Sequence-Aware Service Function Chain Partitioning 

**Title (ZH)**: 基于Transformer增强的演员-评论家强化学习的序列感知服务功能链分区方法 

**Authors**: Cyril Shih-Huan Hsu, Anestis Dalgkitsis, Chrysa Papagianni, Paola Grosso  

**Link**: [PDF](https://arxiv.org/pdf/2504.18902)  

**Abstract**: In the forthcoming era of 6G networks, characterized by unprecedented data rates, ultra-low latency, and extensive connectivity, effective management of Virtualized Network Functions (VNFs) is essential. VNFs are software-based counterparts of traditional hardware devices that facilitate flexible and scalable service provisioning. Service Function Chains (SFCs), structured as ordered sequences of VNFs, are pivotal in orchestrating complex network services. Nevertheless, partitioning SFCs across multi-domain network infrastructures presents substantial challenges due to stringent latency constraints and limited resource availability. Conventional optimization-based methods typically exhibit low scalability, whereas existing data-driven approaches often fail to adequately balance computational efficiency with the capability to effectively account for dependencies inherent in SFCs. To overcome these limitations, we introduce a Transformer-empowered actor-critic framework specifically designed for sequence-aware SFC partitioning. By utilizing the self-attention mechanism, our approach effectively models complex inter-dependencies among VNFs, facilitating coordinated and parallelized decision-making processes. Additionally, we enhance training stability and convergence using $\epsilon$-LoPe exploration strategy as well as Asymptotic Return Normalization. Comprehensive simulation results demonstrate that the proposed methodology outperforms existing state-of-the-art solutions in terms of long-term acceptance rates, resource utilization efficiency, and scalability, while achieving rapid inference. This study not only advances intelligent network orchestration by delivering a scalable and robust solution for SFC partitioning within emerging 6G environments, but also bridging recent advancements in Large Language Models (LLMs) with the optimization of next-generation networks. 

**Abstract (ZH)**: 在6G网络时代：基于Transformer的序列意识SFC分区演员-批评家框架 

---
# A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification 

**Title (ZH)**: 一种简单的大型语言模型推理集成策略：走向更稳定的文本分类 

**Authors**: Junichiro Niimi  

**Link**: [PDF](https://arxiv.org/pdf/2504.18884)  

**Abstract**: With the advance of large language models (LLMs), LLMs have been utilized for the various tasks. However, the issues of variability and reproducibility of results from each trial of LLMs have been largely overlooked in existing literature while actual human annotation uses majority voting to resolve disagreements among annotators. Therefore, this study introduces the straightforward ensemble strategy to a sentiment analysis using LLMs. As the results, we demonstrate that the ensemble of multiple inference using medium-sized LLMs produces more robust and accurate results than using a large model with a single attempt with reducing RMSE by 18.6%. 

**Abstract (ZH)**: 随着大规模语言模型（LLMs）的发展，LLMs已经被应用于各种任务。然而，现有文献中对每次试验所得结果的可变性和重现性问题关注不足，实际的人工标注则通过多数投票解决注释员之间的分歧。因此，本研究引入了简单ensemble策略用于基于LLMs的情感分析。结果显示，使用多个中型LLM的ensemble方法比单次使用大型模型的预测，能够显著提高结果的稳健性和准确性，RMSE降低了18.6%。 

---
# Why you shouldn't fully trust ChatGPT: A synthesis of this AI tool's error rates across disciplines and the software engineering lifecycle 

**Title (ZH)**: 你为什么不应完全信任ChatGPT：该AI工具在各学科及软件工程生命周期中的错误率综述 

**Authors**: Vahid Garousi  

**Link**: [PDF](https://arxiv.org/pdf/2504.18858)  

**Abstract**: Context: ChatGPT and other large language models (LLMs) are widely used across healthcare, business, economics, engineering, and software engineering (SE). Despite their popularity, concerns persist about their reliability, especially their error rates across domains and the software development lifecycle (SDLC).
Objective: This study synthesizes and quantifies ChatGPT's reported error rates across major domains and SE tasks aligned with SDLC phases. It provides an evidence-based view of where ChatGPT excels, where it fails, and how reliability varies by task, domain, and model version (GPT-3.5, GPT-4, GPT-4-turbo, GPT-4o).
Method: A Multivocal Literature Review (MLR) was conducted, gathering data from academic studies, reports, benchmarks, and grey literature up to 2025. Factual, reasoning, coding, and interpretive errors were considered. Data were grouped by domain and SE phase and visualized using boxplots to show error distributions.
Results: Error rates vary across domains and versions. In healthcare, rates ranged from 8% to 83%. Business and economics saw error rates drop from ~50% with GPT-3.5 to 15-20% with GPT-4. Engineering tasks averaged 20-30%. Programming success reached 87.5%, though complex debugging still showed over 50% errors. In SE, requirements and design phases showed lower error rates (~5-20%), while coding, testing, and maintenance phases had higher variability (10-50%). Upgrades from GPT-3.5 to GPT-4 improved reliability.
Conclusion: Despite improvements, ChatGPT still exhibits non-negligible error rates varying by domain, task, and SDLC phase. Full reliance without human oversight remains risky, especially in critical settings. Continuous evaluation and critical validation are essential to ensure reliability and trustworthiness. 

**Abstract (ZH)**: Context: ChatGPT和其他大规模语言模型（LLMs）在医疗、商业、经济学、工程学和软件工程（SE）领域中广泛应用。尽管它们很受欢迎，但关于它们的可靠性和错误率（尤其是在不同领域和软件开发生命周期（SDLC）阶段）的担忧仍然存在。

Objective: 本研究综合并量化了ChatGPT在主要领域和与SDLC阶段对齐的SE任务中的报告错误率。它提供了一个基于证据的观点，说明ChatGPT在哪些方面表现出色，哪些方面表现不佳，以及可靠性如何在任务、领域和模型版本（GPT-3.5、GPT-4、GPT-4-turbo、GPT-4o）之间变化。

Method: 进行了多声腔文献综述（MLR），收集了截至2025年的学术研究、报告、基准测试和灰色文献的数据。考虑了事实错误、推理错误、编码错误和解释性错误。数据按领域和SE阶段分组，并使用箱线图显示错误分布。

Results: 错误率在不同领域和版本之间存在差异。在医疗领域，错误率范围从8%到83%。商业和经济学中，使用GPT-3.5时错误率约为50%，使用GPT-4时降至15-20%。工程任务的平均错误率为20-30%。编程成功率达到了87.5%，尽管复杂调试仍然显示出超过50%的错误率。在SE领域中，需求和设计阶段的错误率较低（约5-20%），而编码、测试和维护阶段的错误率变化较大（10-50%）。从GPT-3.5升级到GPT-4提高了可靠性。

Conclusion: 尽管有所改进，ChatGPT仍然在不同领域、任务和SDLC阶段表现出非忽视的错误率。在没有人类监督的情况下完全依赖它是危险的，特别是在关键设置中。持续评估和批判性验证对于确保可靠性和可信度至关重要。 

---
# Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation 

**Title (ZH)**: 基于维度-wise位置嵌入操控的有效长度外推 

**Authors**: Yi Lu, Wanxu Zhao, Xin Zhou, Chenxin An, Chenglong Wang, Shuo Li, Yuming Yang, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang  

**Link**: [PDF](https://arxiv.org/pdf/2504.18857)  

**Abstract**: Large Language Models (LLMs) often struggle to process and generate coherent context when the number of input tokens exceeds the pre-trained length. Recent advancements in long-context extension have significantly expanded the context window of LLMs but require expensive overhead to train the large-scale models with longer context. In this work, we propose Dimension-Wise Positional Embeddings Manipulation (DPE), a training-free framework to extrapolate the context window of LLMs by diving into RoPE's different hidden dimensions. Instead of manipulating all dimensions equally, DPE detects the effective length for every dimension and finds the key dimensions for context extension. We reuse the original position indices with their embeddings from the pre-trained model and manipulate the key dimensions' position indices to their most effective lengths. In this way, DPE adjusts the pre-trained models with minimal modifications while ensuring that each dimension reaches its optimal state for extrapolation. DPE significantly surpasses well-known baselines such as YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of 128k tokens without continual training and integrates seamlessly with Flash Attention 2. In addition to its impressive extrapolation capability, DPE also dramatically improves the models' performance within training length, such as Llama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When compared with commercial models, Llama 3.1 70B with DPE even achieves better performance than GPT-4-128K. 

**Abstract (ZH)**: Large Language Models（LLMs）在输入token数量超过预训练长度时往往难以处理和生成连贯的上下文。近期在长上下文扩展领域的进展显著扩大了LLMs的上下文窗口，但需要昂贵的开销来训练具有更长上下文的大型模型。本文我们提出了一种无需训练的框架——维度wise位置嵌入操纵（DPE），通过探讨RoPE的不同隐藏维度来扩展LLMs的上下文窗口。DPE不等比例地操纵所有维度，而是检测每个维度的有效长度，并找出关键维度进行上下文扩展。DPE重用了预训练模型的原始位置索引及其嵌入，并将关键维度的位置索引调整至最有效的长度。通过这种方式，DPE在进行最小修改的同时，确保每个维度达到最佳状态以实现扩展。DPE显著超越了如YaRN和Self-Extend等已知基准。DPE使得Llama3-8k 8B能够支持128k token的上下文窗口，无需持续训练，并且能够无缝地与Flash Attention 2集成。除了其令人印象深刻的扩展能力外，DPE还大幅提高了模型在训练长度内的性能，例如Llama3.1 70B，在流行长上下文基准RULER上的表现提升了超过18分。与商用模型相比，具有DPE的Llama 3.1 70B甚至优于GPT-4-128K。 

---
# Test It Before You Trust It: Applying Software Testing for Trustworthy In-context Learning 

**Title (ZH)**: 测试后再信任：将软件测试应用于上下文相关学习的可信度评估 

**Authors**: Teeradaj Racharak, Chaiyong Ragkhitwetsagul, Chommakorn Sontesadisai, Thanwadee Sunetnanta  

**Link**: [PDF](https://arxiv.org/pdf/2504.18827)  

**Abstract**: In-context learning (ICL) has emerged as a powerful capability of large language models (LLMs), enabling them to perform new tasks based on a few provided examples without explicit fine-tuning. Despite their impressive adaptability, these models remain vulnerable to subtle adversarial perturbations and exhibit unpredictable behavior when faced with linguistic variations. Inspired by software testing principles, we introduce a software testing-inspired framework, called MMT4NL, for evaluating the trustworthiness of in-context learning by utilizing adversarial perturbations and software testing techniques. It includes diverse evaluation aspects of linguistic capabilities for testing the ICL capabilities of LLMs. MMT4NL is built around the idea of crafting metamorphic adversarial examples from a test set in order to quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is to treat any LLM as software and validate its functionalities just like testing the software. Finally, we demonstrate applications of MMT4NL on the sentiment analysis and question-answering tasks. Our experiments could reveal various linguistic bugs in state-of-the-art LLMs. 

**Abstract (ZH)**: 基于上下文学习的可信赖性评估框架：MMT4NL 

---
# Can We Enhance Bug Report Quality Using LLMs?: An Empirical Study of LLM-Based Bug Report Generation 

**Title (ZH)**: 使用大型语言模型能否提升缺陷报告质量？基于大型语言模型的缺陷报告生成的实证研究 

**Authors**: Jagrit Acharya, Gouri Ginde  

**Link**: [PDF](https://arxiv.org/pdf/2504.18804)  

**Abstract**: Bug reports contain the information developers need to triage and fix software bugs. However, unclear, incomplete, or ambiguous information may lead to delays and excessive manual effort spent on bug triage and resolution. In this paper, we explore whether Instruction fine-tuned Large Language Models (LLMs) can automatically transform casual, unstructured bug reports into high-quality, structured bug reports adhering to a standard template. We evaluate three open-source instruction-tuned LLMs (\emph{Qwen 2.5, Mistral, and Llama 3.2}) against ChatGPT-4o, measuring performance on established metrics such as CTQRS, ROUGE, METEOR, and SBERT. Our experiments show that fine-tuned Qwen 2.5 achieves a CTQRS score of \textbf{77%}, outperforming both fine-tuned Mistral (\textbf{71%}), Llama 3.2 (\textbf{63%}) and ChatGPT in 3-shot learning (\textbf{75%}). Further analysis reveals that Llama 3.2 shows higher accuracy of detecting missing fields particularly Expected Behavior and Actual Behavior, while Qwen 2.5 demonstrates superior performance in capturing Steps-to-Reproduce, with an F1 score of 76%. Additional testing of the models on other popular projects (e.g., Eclipse, GCC) demonstrates that our approach generalizes well, achieving up to \textbf{70%} CTQRS in unseen projects' bug reports. These findings highlight the potential of instruction fine-tuning in automating structured bug report generation, reducing manual effort for developers and streamlining the software maintenance process. 

**Abstract (ZH)**: 指令微调大型语言模型是否可以自动将非正式的无结构bug报告转换为高质量的结构化bug报告并遵守标准模板？：从CTQRS、ROUGE、METEOR和SBERT等公认指标评估Qwen 2.5、Mistral和Llama 3.2三个开源指令微调大语言模型的表现，进一步分析揭示Qwen 2.5在捕获重现步骤方面的表现更优，而Llama 3.2在检测缺失字段（特别是预期行为和实际行为）方面更准确。此外，在其他流行项目（如Eclipse、GCC）上的测试表明，该方法在未见过的项目的bug报告中表现良好，最高可达到70%的CTQRS分数。这些发现突显了指令微调在自动化结构化bug报告生成方面的潜力，通过减少开发者的手动努力并简化软件维护流程。 

---
# TLoRA: Tri-Matrix Low-Rank Adaptation of Large Language Models 

**Title (ZH)**: TLoRA: 大型语言模型的三矩阵低秩适应 

**Authors**: Tanvir Islam  

**Link**: [PDF](https://arxiv.org/pdf/2504.18735)  

**Abstract**: We propose TLoRA, a novel tri-matrix low-rank adaptation method that decomposes weight updates into three matrices: two fixed random matrices and one trainable matrix, combined with a learnable, layer-wise scaling factor. This tri-matrix design enables TLoRA to achieve highly efficient parameter adaptation while introducing minimal additional computational overhead. Through extensive experiments on the GLUE benchmark, we demonstrate that TLoRA achieves comparable performance to existing low-rank methods such as LoRA and Adapter-based techniques, while requiring significantly fewer trainable parameters. Analyzing the adaptation dynamics, we observe that TLoRA exhibits Gaussian-like weight distributions, stable parameter norms, and scaling factor variability across layers, further highlighting its expressive power and adaptability. Additionally, we show that TLoRA closely resembles LoRA in its eigenvalue distributions, parameter norms, and cosine similarity of updates, underscoring its ability to effectively approximate LoRA's adaptation behavior. Our results establish TLoRA as a highly efficient and effective fine-tuning method for LLMs, offering a significant step forward in resource-efficient model adaptation. 

**Abstract (ZH)**: 我们提出了一种新颖的三矩阵低秩适应方法TLoRA，该方法将权重更新分解为三个矩阵：两个固定的随机矩阵和一个可训练矩阵，并结合了一种可学习的分层缩放因子。这种三矩阵设计使TLoRA能够实现高效的参数适应，同时引入的额外计算开销 minimal。通过在GLUE基准上的大量实验，我们证明TLoRA在性能上与现有的低秩方法（如LoRA和Adapter基技术）相当，但所需的可训练参数数量显著较少。分析适应动态后，我们观察到TLoRA表现出高斯似的权重分布、稳定的参数范数以及层间可变的缩放因子，进一步突显了其强大的表达能力和适应性。此外，我们展示了TLoRA在特征值分布、参数范数和更新的余弦相似性方面与LoRA高度相似，进一步强调了其逼近LoRA适应行为的能力。我们的结果确立了TLoRA作为LLMs高效且有效的一种 fine-tuning 方法的地位，为资源高效模型适应提供了重要进展。 

---
# MODP: Multi Objective Directional Prompting 

**Title (ZH)**: 多目标方向性提示：MODP 

**Authors**: Aashutosh Nema, Samaksh Gulati, Evangelos Giakoumakis, Bipana Thapaliya  

**Link**: [PDF](https://arxiv.org/pdf/2504.18722)  

**Abstract**: Recent advances in large language models (LLMs) have led to their popularity across multiple use-cases. However, prompt engineering, the process for optimally utilizing such models, remains approximation-driven and subjective. Most of the current research on prompt engineering focuses on task-specific optimization, while neglecting the behavior of the LLM under consideration during prompt development. This paper introduces MODP -- Multi Objective Directional Prompting, a framework based on two key concepts: 1) multi-objectivity: the importance of considering an LLM's intrinsic behavior as an additional objective in prompt development, and 2) directional prompting: a metrics-driven method for prompt engineering to ensure development of robust and high-precision prompts. We demonstrate the effectiveness of our proposed ideas on a summarization task, using a synthetically created dataset, achieving a 26% performance gain over initial prompts. Finally, we apply MODP to develop prompts for Dell's Next Best Action support tool, which is now in production and is used by more than 10,000 internal support agents and serving millions of customers worldwide. 

**Abstract (ZH)**: 近期大型语言模型的进展促进了其在多个应用场景中的普及。然而，提示工程，这一优化模型使用的流程，仍然主要依赖于近似方法并带有主观性。目前大多数关于提示工程的研究集中在针对特定任务的优化上，而忽略了在提示开发过程中L Large Language Model（大语言模型）的行为。本文提出了一种基于两个核心概念的MODP框架——多目标方向性提示：1）多目标性：在提示开发中考虑L Large Language Model的固有行为作为额外目标的重要性；2）方向性提示：一种基于指标的提示工程技术，用于确保开发出稳健且高精度的提示。我们在一个合成创建的数据集上验证了我们提出的概念在摘要任务中的有效性，实现了初始提示的26%性能提升。最后，我们将MODP应用于开发Dell Next Best Action支持工具的提示，该工具目前已投入使用，有超过10,000名内部支持人员在使用，并为全球数百万客户提供服务。 

---
# Technical Challenges in Maintaining Tax Prep Software with Large Language Models 

**Title (ZH)**: 使用大型语言模型维护税务准备软件的技术挑战 

**Authors**: Sina Gogani-Khiabani, Varsha Dewangan, Nina Olson, Ashutosh Trivedi, Saeid Tizpaz-Niari  

**Link**: [PDF](https://arxiv.org/pdf/2504.18693)  

**Abstract**: As the US tax law evolves to adapt to ever-changing politico-economic realities, tax preparation software plays a significant role in helping taxpayers navigate these complexities. The dynamic nature of tax regulations poses a significant challenge to accurately and timely maintaining tax software artifacts. The state-of-the-art in maintaining tax prep software is time-consuming and error-prone as it involves manual code analysis combined with an expert interpretation of tax law amendments. We posit that the rigor and formality of tax amendment language, as expressed in IRS publications, makes it amenable to automatic translation to executable specifications (code). Our research efforts focus on identifying, understanding, and tackling technical challenges in leveraging Large Language Models (LLMs), such as ChatGPT and Llama, to faithfully extract code differentials from IRS publications and automatically integrate them with the prior version of the code to automate tax prep software maintenance. 

**Abstract (ZH)**: 随着美国税法为了适应不断变化的政经现实而不断演化，税务准备软件在帮助纳税人应对这些复杂性方面发挥着重要作用。税法规则动态性的本质给准确及时地维护税务软件带来了显著挑战。当前维护税务准备软件的先进方法耗时且易出错，因为这需要结合人工代码分析和税务法律修正案的专家解读。我们认为，根据美国国税局（IRS）出版物中表达的税法修正案的严谨性和正式性，它们可以被自动翻译成可执行规范（代码）。我们的研究重点在于识别、理解并解决利用大型语言模型（LLMs），如ChatGPT和Llama，从IRS出版物中忠实提取代码差异并通过自动化将其整合到先前版本代码中以自动化税务准备软件维护的技术挑战。 

---
# From Prompts to Propositions: A Logic-Based Lens on Student-LLM Interactions 

**Title (ZH)**: 从提示到命题：基于逻辑的学生成员交互视角 

**Authors**: Ali Alfageeh, Sadegh AlMahdi Kazemi Zarkouei, Daye Nam, Daniel Prol, Matin Amoozadeh, Souti Chattopadhyay, James Prather, Paul Denny, Juho Leinonen, Michael Hilton, Sruti Srinivasa Ragavan, Mohammad Amin Alipour  

**Link**: [PDF](https://arxiv.org/pdf/2504.18691)  

**Abstract**: Background and Context. The increasing integration of large language models (LLMs) in computing education presents an emerging challenge in understanding how students use LLMs and craft prompts to solve computational tasks. Prior research has used both qualitative and quantitative methods to analyze prompting behavior, but these approaches lack scalability or fail to effectively capture the semantic evolution of prompts. Objective. In this paper, we investigate whether students prompts can be systematically analyzed using propositional logic constraints. We examine whether this approach can identify patterns in prompt evolution, detect struggling students, and provide insights into effective and ineffective strategies. Method. We introduce Prompt2Constraints, a novel method that translates students prompts into logical constraints. The constraints are able to represent the intent of the prompts in succinct and quantifiable ways. We used this approach to analyze a dataset of 1,872 prompts from 203 students solving introductory programming tasks. Findings. We find that while successful and unsuccessful attempts tend to use a similar number of constraints overall, when students fail, they often modify their prompts more significantly, shifting problem-solving strategies midway. We also identify points where specific interventions could be most helpful to students for refining their prompts. Implications. This work offers a new and scalable way to detect students who struggle in solving natural language programming tasks. This work could be extended to investigate more complex tasks and integrated into programming tools to provide real-time support. 

**Abstract (ZH)**: 背景与 context. 随着大型语言模型（LLMs）在计算教育中的日益集成，理解学生如何使用LLMs及其构建提示来解决计算任务正成为一个新兴挑战。先前的研究采用了定性和定量方法来分析提示行为，但这些方法缺乏可扩展性，或者未能有效捕捉提示的语义演变。目标. 在本文中，我们研究是否可以使用命题逻辑约束系统地分析学生的提示。我们探讨了这种方法是否能够识别提示演变中的模式，检测挣扎中的学生，并提供有效的和无效策略的见解。方法. 我们引入了Prompt2Constraints，这是一种新颖的方法，将学生的提示转换为逻辑约束。这些约束能够以简洁和可量化的方式表示提示的意图。我们使用这种方法分析了203名学生解决入门级编程任务的1,872个提示数据集。发现. 我们发现虽然成功的尝试和失败的尝试整体上使用了相似数量的约束，但当学生失败时，他们往往会更显著地修改提示，中途改变解决问题的策略。我们还确定了特定的干预点，这些干预点可能对学生细化其提示最有帮助。意义. 该工作提供了一种新的和可扩展的方法来检测在解决自然语言编程任务中挣扎的学生。该工作可以扩展以研究更复杂的任务，并集成到编程工具中以提供实时支持。 

---
# The Big Send-off: High Performance Collectives on GPU-based Supercomputers 

**Title (ZH)**: 大规模启程：基于GPU的超级计算机上的高性能集合通信 

**Authors**: Siddharth Singh, Mahua Singh, Abhinav Bhatele  

**Link**: [PDF](https://arxiv.org/pdf/2504.18658)  

**Abstract**: We evaluate the current state of collective communication on GPU-based supercomputers for large language model (LLM) training at scale. Existing libraries such as RCCL and Cray-MPICH exhibit critical limitations on systems such as Frontier -- Cray-MPICH underutilizes network and compute resources, while RCCL suffers from severe scalability issues. To address these challenges, we introduce PCCL, a communication library with highly optimized implementations of all-gather and reduce-scatter operations tailored for distributed deep learning workloads. PCCL is designed to maximally utilize all available network and compute resources and to scale efficiently to thousands of GPUs. It achieves substantial performance improvements, delivering 6-33x speedups over RCCL and 28-70x over Cray-MPICH for all-gather on 2048 GCDs of Frontier. These gains translate directly to end-to-end performance: in large-scale GPT-3-style training, PCCL provides up to 60% and 40% speedups over RCCL for 7B and 13B parameter models, respectively. 

**Abstract (ZH)**: 我们评估了基于GPU的超级计算机上大规模语言模型（LLM）训练中的集体通信现状，现有库如RCCL和Cray-MPICH在Frontier等系统上表现出关键限制，而Cray-MPICH未能充分利用网络和计算资源，RCCL则面临严重的可扩展性问题。为解决这些问题，我们引入了PCCL，这是一种针对分布式深度学习工作负载优化的通信库，专注于优化all-gather和reduce-scatter操作。PCCL旨在充分利用所有可用的网络和计算资源，并能高效扩展至数千个GPU。PCCL在Frontier 2048 GCDs的all-gather操作上实现了6-33倍的性能提升，相比RCCL为2048 GCDs的all-gather，PCCL在7B和13B参数模型的大规模GPT-3风格训练中分别提供了60%和40%的性能提升。 

---
# Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach 

**Title (ZH)**: 面向个性化的量子计算教育：一种进化论大语言模型驱动的方法 

**Authors**: Iizalaarab Elhaimeur, Nikos Chrisochoides  

**Link**: [PDF](https://arxiv.org/pdf/2504.18603)  

**Abstract**: Quantum computing education faces significant challenges due to its complexity and the limitations of current tools; this paper introduces a novel Intelligent Teaching Assistant for quantum computing education and details its evolutionary design process. The system combines a knowledge-graph-augmented architecture with two specialized Large Language Model (LLM) agents: a Teaching Agent for dynamic interaction, and a Lesson Planning Agent for lesson plan generation. The system is designed to adapt to individual student needs, with interactions meticulously tracked and stored in a knowledge graph. This graph represents student actions, learning resources, and relationships, aiming to enable reasoning about effective learning pathways. We describe the implementation of the system, highlighting the challenges encountered and the solutions implemented, including introducing a dual-agent architecture where tasks are separated, all coordinated through a central knowledge graph that maintains system awareness, and a user-facing tag system intended to mitigate LLM hallucination and improve user control. Preliminary results illustrate the system's potential to capture rich interaction data, dynamically adapt lesson plans based on student feedback via a tag system in simulation, and facilitate context-aware tutoring through the integrated knowledge graph, though systematic evaluation is required. 

**Abstract (ZH)**: 量子计算教育面临复杂性和当前工具限制的重大挑战；本文介绍了一种新型智能教学助手，并详细描述了其进化设计过程。该系统结合了知识图谱增强架构以及两个专门的大型语言模型（LLM）代理：教学代理用于动态交互，课程规划代理用于生成课程计划。系统设计旨在适应个别学生需求，互动被仔细跟踪并存储在知识图谱中。该图谱表示学生行为、学习资源及其关系，旨在使关于有效学习路径的推理成为可能。我们描述了系统的实现，强调了遇到的挑战和实施的解决方案，包括引入双代理架构以分离任务，并通过中心知识图谱协调，以及一个面向用户的标签系统，旨在减轻LLM幻觉并提高用户体验控制。初步结果表明，系统能够捕获丰富的交互数据，通过模拟中的标签系统动态适应课程计划，并通过集成知识图谱实现上下文感知辅导，尽管仍需系统评估。 

---
# BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts 

**Title (ZH)**: BadMoE: 通过优化路由触发器和感染潜伏专家对混合专家LLM进行后门攻击 

**Authors**: Qingyue Wang, Qi Pang, Xixun Lin, Shuai Wang, Daoyuan Wu  

**Link**: [PDF](https://arxiv.org/pdf/2504.18598)  

**Abstract**: Mixture-of-Experts (MoE) have emerged as a powerful architecture for
large language models (LLMs), enabling efficient scaling of model capacity
while maintaining manageable computational costs. The key advantage lies in
their ability to route different tokens to different ``expert'' networks
within the model, enabling specialization and efficient handling of diverse
input. However, the vulnerabilities of MoE-based LLMs still have barely been
studied, and the potential for backdoor attacks in this context remains
largely unexplored. This paper presents the first backdoor attack against
MoE-based LLMs where the attackers poison ``dormant experts'' (i.e., underutilized
experts) and activate them by optimizing routing triggers, thereby gaining
control over the model's output. We first rigorously prove the existence of a few ``dominating
experts'' in MoE models, whose outputs can determine the overall MoE's
output. We also show that dormant experts can serve as dominating experts to manipulate model predictions.
Accordingly, our attack, namely \textsc{BadMoE}, exploits the unique
architecture of MoE models by 1) identifying dormant experts unrelated to the target task, 2)
constructing a routing-aware loss to optimize the activation triggers of these experts, and 3) promoting dormant experts to dominating roles via poisoned training data. 

**Abstract (ZH)**: Mixture-of-Experts (MoE) 基于的大语言模型中的后门攻击：激活“沉睡专家”以控制模型输出 

---
# Training Large Language Models to Reason via EM Policy Gradient 

**Title (ZH)**: 通过EM策略梯度进行大规模语言模型推理训练 

**Authors**: Tianbing Xu  

**Link**: [PDF](https://arxiv.org/pdf/2504.18587)  

**Abstract**: Recently, foundation models such as OpenAI's O1 and O3, along with DeepSeek's R1, have demonstrated strong reasoning capacities and problem-solving skills acquired through large-scale reinforcement learning (RL), with wide applications in mathematics, coding, science, intelligent agents, and virtual assistants. In this work, we introduce an off-policy reinforcement learning algorithm, EM Policy Gradient, aimed at enhancing LLM reasoning by optimizing expected return over reasoning trajectories. We frame the reasoning task as an Expectation-Maximization (EM) optimization problem, alternating between sampling diverse rationale trajectories and performing reward-guided fine-tuning. Unlike PPO and GRPO, which rely on complex importance weights and heuristic clipping, our method provides a simpler, more principled off-policy policy gradient approach, eliminating these complexities while maintaining strong performance. We evaluate the effectiveness of EM Policy Gradient on the GSM8K and MATH (HARD) datasets, where it achieves performance comparable to or slightly surpassing the state-of-the-art GRPO, while offering additional advantages in scalability, simplicity, and reasoning conciseness. Moreover, models fine-tuned with our method exhibit cognitive behaviors, such as sub-problem decomposition, self-verification, and backtracking, highlighting its potential to enhance both the interpretability and robustness of LLM reasoning. 

**Abstract (ZH)**: 最近，诸如OpenAI的O1和O3以及DeepSeek的R1等基础模型通过大规模强化学习（RL）展示了强大的推理能力和解决问题的能力，广泛应用于数学、编程、科学、智能代理和虚拟助手等领域。本文引入了一种离策增强学习算法——EM策略梯度，旨在通过优化推理轨迹的期望回报来增强语言模型（LLM）的推理能力。我们将推理任务建模为一个期望最大化（EM）优化问题，交替进行多样化的推理轨迹采样和奖励引导下的精细化调优。与依赖复杂重要性权重和启发式裁剪的PPO和GRPO不同，我们的方法提供了一种更简单、更原则上的离策策略梯度方法，同时保持了强大的性能。我们在GSM8K和MATH（HARD）数据集上评估了EM策略梯度的有效性，结果显示其性能可与或略优于当前最先进的GRPO相当，同时还具备更高的可扩展性、简洁性和推理精密度。此外，使用我们方法调优后的模型表现出诸如子问题分解、自我验证和回溯等认知行为，突显了其增强LLM推理的可解释性和鲁棒性的潜力。 

---
# Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism 

**Title (ZH)**: 理解循环语言模型中的技能差距：收集与聚合机制的作用 

**Authors**: Aviv Bick, Eric Xing, Albert Gu  

**Link**: [PDF](https://arxiv.org/pdf/2504.18574)  

**Abstract**: SSMs offer efficient processing of long sequences with fixed state sizes, but struggle with algorithmic tasks like retrieving past context. In this work, we examine how such in-context retrieval operates within Transformer- and SSM-based language models. We find that both architectures develop the same fundamental Gather-and-Aggregate (G&A) mechanism. A Gather Head first identifies and extracts relevant information from the context, which an Aggregate Head then integrates into a final representation. Across both model types, G&A concentrates in just a few heads, making them critical bottlenecks even for benchmarks that require a basic form of retrieval. For example, disabling a single Gather or Aggregate Head of a pruned Llama-3.1-8B degrades its ability to retrieve the correct answer letter in MMLU, reducing accuracy from 66% to 25%. This finding suggests that in-context retrieval can obscure the limited knowledge demands of certain tasks. Despite strong MMLU performance with retrieval intact, the pruned model fails on other knowledge tests. Similar G&A dependencies exist in GSM8K, BBH, and dialogue tasks. Given the significance of G&A in performance, we show that retrieval challenges in SSMs manifest in how they implement G&A, leading to smoother attention patterns rather than the sharp token transitions that effective G&A relies on. Thus, while a gap exists between Transformers and SSMs in implementing in-context retrieval, it is confined to a few heads, not the entire model. This insight suggests a unified explanation for performance differences between Transformers and SSMs while also highlighting ways to combine their strengths. For example, in pretrained hybrid models, attention components naturally take on the role of Aggregate Heads. Similarly, in a pretrained pure SSM, replacing a single G&A head with an attention-based variant significantly improves retrieval. 

**Abstract (ZH)**: SSMs在处理长序列时提供高效的处理能力，但在处理如检索历史上下文等算法任务时存在困难。在这项工作中，我们探讨了Transformer和SSMベース的语言模型中上下文检索的运作方式。我们发现，这两种架构都发展出相同的基础Gather-and-Aggregate (G&A)机制。Gather Head首先从上下文中识别并提取相关的信息，然后Aggregate Head将这些信息整合到最终的表示中。在两种模型类型中，G&A机制主要集中在少数几个头中，即使对于需要基本检索形式的基准测试，它们也是关键瓶颈。例如，禁用精简版Llama-3.1-8B模型中的单个Gather或Aggregate Head会导致在其多模态知识评估（MMLU）中正确回答字母的能力下降，准确率从66%降至25%。这一发现表明，上下文检索可能掩盖了某些任务中的有限知识需求。尽管在检索完整的情况下MMLU表现强劲，但精简模型在其他知识测试中仍然失败。类似的G&A依赖关系也存在于GSM8K、BBH和对话任务中。鉴于G&A在性能中的重要性，我们展示出SSM中的检索挑战体现在它们如何实现G&A机制上，导致更加平滑的注意力模式而不是有效的G&A所依赖的尖锐的token过渡。因此，虽然Transformer和SSM在实现上下文检索时存在差距，但这种差距集中在少数几个头中，而非整个模型。这种见解为解释Transformer和SSM之间性能差异提供了一个统一的解释，并强调了结合它们优势的方法。例如，在预训练混合模型中，注意力组件自然承担起Aggregate Head的角色。同样，在预训练纯SSM中，用基于注意力的变体替换单个G&A头可以显著改善检索效果。 

---
# Large Language Model Empowered Privacy-Protected Framework for PHI Annotation in Clinical Notes 

**Title (ZH)**: 大型语言模型赋能的隐私保护框架在临床笔记中提取个人健康信息 

**Authors**: Guanchen Wu, Linzhi Zheng, Han Xie, Zhen Xiang, Jiaying Lu, Darren Liu, Delgersuren Bold, Bo Li, Xiao Hu, Carl Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.18569)  

**Abstract**: The de-identification of private information in medical data is a crucial process to mitigate the risk of confidentiality breaches, particularly when patient personal details are not adequately removed before the release of medical records. Although rule-based and learning-based methods have been proposed, they often struggle with limited generalizability and require substantial amounts of annotated data for effective performance. Recent advancements in large language models (LLMs) have shown significant promise in addressing these issues due to their superior language comprehension capabilities. However, LLMs present challenges, including potential privacy risks when using commercial LLM APIs and high computational costs for deploying open-source LLMs locally. In this work, we introduce LPPA, an LLM-empowered Privacy-Protected PHI Annotation framework for clinical notes, targeting the English language. By fine-tuning LLMs locally with synthetic notes, LPPA ensures strong privacy protection and high PHI annotation accuracy. Extensive experiments demonstrate LPPA's effectiveness in accurately de-identifying private information, offering a scalable and efficient solution for enhancing patient privacy protection. 

**Abstract (ZH)**: 基于LLM的隐私保护 PHI 标注框架 LPPA：面向临床笔记的英语应用 

---
# RepliBench: Evaluating the autonomous replication capabilities of language model agents 

**Title (ZH)**: RepliBench: 评估语言模型代理的自主复制能力 

**Authors**: Sid Black, Asa Cooper Stickland, Jake Pencharz, Oliver Sourbut, Michael Schmatz, Jay Bailey, Ollie Matthews, Ben Millwood, Alex Remedios, Alan Cooney  

**Link**: [PDF](https://arxiv.org/pdf/2504.18565)  

**Abstract**: Uncontrollable autonomous replication of language model agents poses a critical safety risk. To better understand this risk, we introduce RepliBench, a suite of evaluations designed to measure autonomous replication capabilities. RepliBench is derived from a decomposition of these capabilities covering four core domains: obtaining resources, exfiltrating model weights, replicating onto compute, and persisting on this compute for long periods. We create 20 novel task families consisting of 86 individual tasks. We benchmark 5 frontier models, and find they do not currently pose a credible threat of self-replication, but succeed on many components and are improving rapidly. Models can deploy instances from cloud compute providers, write self-propagating programs, and exfiltrate model weights under simple security setups, but struggle to pass KYC checks or set up robust and persistent agent deployments. Overall the best model we evaluated (Claude 3.7 Sonnet) has a >50% pass@10 score on 15/20 task families, and a >50% pass@10 score for 9/20 families on the hardest variants. These findings suggest autonomous replication capability could soon emerge with improvements in these remaining areas or with human assistance. 

**Abstract (ZH)**: 不可控的语言模型代理自主复制构成了重要的安全风险。为了更好地理解这一风险，我们介绍了RepliBench评估套件，用于衡量自主复制能力。RepliBench源自对这些能力的分解，涵盖了四个核心领域：获取资源、窃取模型权重、在计算环境中复制以及在计算环境中持久存在。我们创建了20个新型任务家族，包含86个独立任务。我们对5个前沿模型进行了基准测试，发现它们目前尚未构成自我复制的现实威胁，但在许多组件上表现出色，并且改进速度很快。模型可以从云计算提供商处部署实例，编写自我传播的程序，并在简单的安全设置下窃取模型权重，但难以通过KYC检查或建立稳定且持久的代理部署。整体而言，我们评估的最佳模型（Claude 3.7 Sonnet）在15个任务家族中具有超过50%的通过率，在最难的变体中，20个家族中有9个的家庭具有超过50%的通过率。这些发现表明，在这些剩余领域取得改进或在人类协助下，自主复制能力可能会很快出现。 

---
# DualBreach: Efficient Dual-Jailbreaking via Target-Driven Initialization and Multi-Target Optimization 

**Title (ZH)**: 双逃逸：基于目标驱动初始化和多目标优化的高效双 jailbreaking 

**Authors**: Xinzhe Huang, Kedong Xiu, Tianhang Zheng, Churui Zeng, Wangze Ni, Zhan Qiin, Kui Ren, Chun Chen  

**Link**: [PDF](https://arxiv.org/pdf/2504.18564)  

**Abstract**: Recent research has focused on exploring the vulnerabilities of Large Language Models (LLMs), aiming to elicit harmful and/or sensitive content from LLMs. However, due to the insufficient research on dual-jailbreaking -- attacks targeting both LLMs and Guardrails, the effectiveness of existing attacks is limited when attempting to bypass safety-aligned LLMs shielded by guardrails. Therefore, in this paper, we propose DualBreach, a target-driven framework for dual-jailbreaking. DualBreach employs a Target-driven Initialization (TDI) strategy to dynamically construct initial prompts, combined with a Multi-Target Optimization (MTO) method that utilizes approximate gradients to jointly adapt the prompts across guardrails and LLMs, which can simultaneously save the number of queries and achieve a high dual-jailbreaking success rate. For black-box guardrails, DualBreach either employs a powerful open-sourced guardrail or imitates the target black-box guardrail by training a proxy model, to incorporate guardrails into the MTO process.
We demonstrate the effectiveness of DualBreach in dual-jailbreaking scenarios through extensive evaluation on several widely-used datasets. Experimental results indicate that DualBreach outperforms state-of-the-art methods with fewer queries, achieving significantly higher success rates across all settings. More specifically, DualBreach achieves an average dual-jailbreaking success rate of 93.67% against GPT-4 with Llama-Guard-3 protection, whereas the best success rate achieved by other methods is 88.33%. Moreover, DualBreach only uses an average of 1.77 queries per successful dual-jailbreak, outperforming other state-of-the-art methods. For the purpose of defense, we propose an XGBoost-based ensemble defensive mechanism named EGuard, which integrates the strengths of multiple guardrails, demonstrating superior performance compared with Llama-Guard-3. 

**Abstract (ZH)**: Recent Research on Exploring Vulnerabilities of Large Language Models and Elicit Harmful and/or Sensitive Content: A Call for Dual-Jailbreaking Studies Considering Guardrails 

---
# Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages 

**Title (ZH)**: 注意语言差距：面向高资源和低资源语言的偏见自动与增强评估 

**Authors**: Alessio Buscemi, Cédric Lothritz, Sergio Morales, Marcos Gomez-Vazquez, Robert Clarisó, Jordi Cabot, German Castignani  

**Link**: [PDF](https://arxiv.org/pdf/2504.18560)  

**Abstract**: Large Language Models (LLMs) have exhibited impressive natural language processing capabilities but often perpetuate social biases inherent in their training data. To address this, we introduce MultiLingual Augmented Bias Testing (MLA-BiTe), a framework that improves prior bias evaluation methods by enabling systematic multilingual bias testing. MLA-BiTe leverages automated translation and paraphrasing techniques to support comprehensive assessments across diverse linguistic settings. In this study, we evaluate the effectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six languages -- including two low-resource languages -- focusing on seven sensitive categories of discrimination. 

**Abstract (ZH)**: 多语言增强偏见测试（MLA-BiTe）：一种改进的多语言偏见评估框架 

---
