{'arxiv_id': 'arXiv:2507.01016', 'title': 'VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers', 'authors': 'Yating Wang, Haoyi Zhu, Mingyu Liu, Jiange Yang, Hao-Shu Fang, Tong He', 'link': 'https://arxiv.org/abs/2507.01016', 'abstract': 'In this paper, we introduce an innovative vector quantization based action tokenizer built upon the largest-scale action trajectory dataset to date, leveraging over 100 times more data than previous approaches. This extensive dataset enables our tokenizer to capture rich spatiotemporal dynamics, resulting in a model that not only accelerates inference but also generates smoother and more coherent action outputs. Once trained, the tokenizer can be seamlessly adapted to a wide range of downstream tasks in a zero-shot manner, from short-horizon reactive behaviors to long-horizon planning. A key finding of our work is that the domain gap between synthetic and real action trajectories is marginal, allowing us to effectively utilize a vast amount of synthetic data during training without compromising real-world performance. To validate our approach, we conducted extensive experiments in both simulated environments and on real robotic platforms. The results demonstrate that as the volume of synthetic trajectory data increases, the performance of our tokenizer on downstream tasks improves significantly-most notably, achieving up to a 30% higher success rate on two real-world tasks in long-horizon scenarios. These findings highlight the potential of our action tokenizer as a robust and scalable solution for real-time embodied intelligence systems, paving the way for more efficient and reliable robotic control in diverse application this http URL website: this https URL', 'abstract_zh': '本文介绍了一种基于目前最大的动作轨迹数据集构建的创新向量量化动作分词器，该数据集比以往方法的数据量多出100多倍。这一庞大的数据集使得我们的分词器能够捕捉丰富的时空动态，从而不仅加速了推理过程，还生成了更流畅和连贯的动作输出。训练完成后，该分词器可以无缝适应广泛的应用任务，从短时反应行为到长远规划。我们的工作的一个关键发现是，合成和真实动作轨迹之间的领域差异很小，这使得我们在训练过程中可以有效地利用大量的合成数据，而不影响实际性能。为了验证我们的方法，我们还在仿真环境和真实机器人平台上进行了广泛实验。结果表明，随着合成轨迹数据量的增加，我们的分词器在下游任务上的性能显著提高——特别是在长时序场景中，两个实际任务的成功率提高了30%以上。这些发现突显了我们动作分词器作为实时嵌入式智能系统稳健且可扩展解决方案的潜力，为在各种应用中实现更高效和可靠的机器人控制铺平了道路。', 'title_zh': 'VQ-VLA：通过扩展向量量化动作词典提高视觉-语言-动作模型性能'}
{'arxiv_id': 'arXiv:2507.00990', 'title': 'Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations', 'authors': 'Shivansh Patel, Shraddhaa Mohan, Hanlin Mai, Unnat Jain, Svetlana Lazebnik, Yunzhu Li', 'link': 'https://arxiv.org/abs/2507.00990', 'abstract': 'This work introduces Robots Imitating Generated Videos (RIGVid), a system that enables robots to perform complex manipulation tasks--such as pouring, wiping, and mixing--purely by imitating AI-generated videos, without requiring any physical demonstrations or robot-specific training. Given a language command and an initial scene image, a video diffusion model generates potential demonstration videos, and a vision-language model (VLM) automatically filters out results that do not follow the command. A 6D pose tracker then extracts object trajectories from the video, and the trajectories are retargeted to the robot in an embodiment-agnostic fashion. Through extensive real-world evaluations, we show that filtered generated videos are as effective as real demonstrations, and that performance improves with generation quality. We also show that relying on generated videos outperforms more compact alternatives such as keypoint prediction using VLMs, and that strong 6D pose tracking outperforms other ways to extract trajectories, such as dense feature point tracking. These findings suggest that videos produced by a state-of-the-art off-the-shelf model can offer an effective source of supervision for robotic manipulation.', 'abstract_zh': '机器人模仿生成视频（RIGVid）系统：一种仅通过模仿AI生成视频来执行复杂操作任务的系统', 'title_zh': '基于生成视频模仿的机器人操作'}
{'arxiv_id': 'arXiv:2507.00917', 'title': 'A Survey: Learning Embodied Intelligence from Physical Simulators and World Models', 'authors': 'Xiaoxiao Long, Qingrui Zhao, Kaiwen Zhang, Zihao Zhang, Dingrui Wang, Yumeng Liu, Zhengjie Shu, Yi Lu, Shouzheng Wang, Xinzhe Wei, Wei Li, Wei Yin, Yao Yao, Jia Pan, Qiu Shen, Ruigang Yang, Xun Cao, Qionghai Dai', 'link': 'https://arxiv.org/abs/2507.00917', 'abstract': 'The pursuit of artificial general intelligence (AGI) has placed embodied intelligence at the forefront of robotics research. Embodied intelligence focuses on agents capable of perceiving, reasoning, and acting within the physical world. Achieving robust embodied intelligence requires not only advanced perception and control, but also the ability to ground abstract cognition in real-world interactions. Two foundational technologies, physical simulators and world models, have emerged as critical enablers in this quest. Physical simulators provide controlled, high-fidelity environments for training and evaluating robotic agents, allowing safe and efficient development of complex behaviors. In contrast, world models empower robots with internal representations of their surroundings, enabling predictive planning and adaptive decision-making beyond direct sensory input. This survey systematically reviews recent advances in learning embodied AI through the integration of physical simulators and world models. We analyze their complementary roles in enhancing autonomy, adaptability, and generalization in intelligent robots, and discuss the interplay between external simulation and internal modeling in bridging the gap between simulated training and real-world deployment. By synthesizing current progress and identifying open challenges, this survey aims to provide a comprehensive perspective on the path toward more capable and generalizable embodied AI systems. We also maintain an active repository that contains up-to-date literature and open-source projects at this https URL.', 'abstract_zh': '人工通用智能（AGI）的追求使体态智能成为机器人研究的前沿领域。体态智能关注能够感知、推理和在物理世界中行动的代理。实现稳健的体态智能不仅需要高级感知和控制，还需要将抽象认知根植于现实世界的交互中。两种基础技术，物理模拟器和世界模型，已成为这一追求中的关键使能技术。物理模拟器为训练和评估机器人代理提供了受控的高保真环境，使复杂行为的安全和高效开发成为可能。相比之下，世界模型使机器人能够拥有其周围环境的内部表示，从而实现超越直接感官输入的预测规划和适应性决策。本文综述了通过整合物理模拟器和世界模型学习体态AI的近期进展。我们分析了这两种技术在增强智能机器人自主性、适应性和泛化性方面的互补作用，并讨论了外部模拟与内部建模之间的交互，以弥合模拟训练与实际部署之间的差距。通过综合当前进展并识别开放挑战，本文旨在提供一条通向更具能力和泛化性的体态AI系统的全面视角。我们还维护了一个活跃的存储库，该存储库包含最新的文献和开源项目，可通过此链接访问：[https://github.com/Qwen-Model/body-aware-ai_survey]。', 'title_zh': '一种综述：从物理模拟器和世界模型中学习 embodiet 智能'}
{'arxiv_id': 'arXiv:2507.00882', 'title': 'I Move Therefore I Learn: Experience-Based Traversability in Outdoor Robotics', 'authors': 'Miguel Ángel de Miguel, Jorge Beltrán, Juan S. Cely, Francisco Martín, Juan Carlos Manzanares, Alberto García', 'link': 'https://arxiv.org/abs/2507.00882', 'abstract': 'Accurate traversability estimation is essential for safe and effective navigation of outdoor robots operating in complex environments. This paper introduces a novel experience-based method that allows robots to autonomously learn which terrains are traversable based on prior navigation experience, without relying on extensive pre-labeled datasets. The approach integrates elevation and texture data into multi-layered grid maps, which are processed using a variational autoencoder (VAE) trained on a generic texture dataset. During an initial teleoperated phase, the robot collects sensory data while moving around the environment. These experiences are encoded into compact feature vectors and clustered using the BIRCH algorithm to represent traversable terrain areas efficiently. In deployment, the robot compares new terrain patches to its learned feature clusters to assess traversability in real time. The proposed method does not require training with data from the targeted scenarios, generalizes across diverse surfaces and platforms, and dynamically adapts as new terrains are encountered. Extensive evaluations on both synthetic benchmarks and real-world scenarios with wheeled and legged robots demonstrate its effectiveness, robustness, and superior adaptability compared to state-of-the-art approaches.', 'abstract_zh': '基于经验的准确 traversability 估计对于户外机器人在复杂环境下安全有效地导航是必不可少的。本文介绍了一种新颖的经验导向方法，使机器人能够基于先前的导航经验自主学习哪些地形是可通行的，而无需依赖大量预先标注的 数据集。该方法将高程和纹理数据整合到多层网格地图中，并使用在通用纹理数据集上训练的变分自编码器（VAE）进行处理。在初始的遥控阶段，机器人在环境中移动并收集感官数据。这些经验被编码成紧凑的特征向量，并使用 BIRCH 算法进行聚类，以高效地表示可通行区域。在部署过程中，机器人将新地形片段与学习到的特征聚类进行比较，以实时评估可通行性。该方法不需要使用针对特定场景的数据进行训练，能够在多种表面和平台上泛化，并能够动态适应新遇到的地形。在合成基准和现实世界场景中的广泛评估表明，该方法在有效性、稳健性和适应性方面优于现有方法。', 'title_zh': '我移动Therefore我学习：基于经验的户外机器人可通行性'}
{'arxiv_id': 'arXiv:2507.00833', 'title': 'HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning', 'authors': 'Zhi Jing, Siyuan Yang, Jicong Ao, Ting Xiao, Yugang Jiang, Chenjia Bai', 'link': 'https://arxiv.org/abs/2507.00833', 'abstract': 'For robotic manipulation, existing robotics datasets and simulation benchmarks predominantly cater to robot-arm platforms. However, for humanoid robots equipped with dual arms and dexterous hands, simulation tasks and high-quality demonstrations are notably lacking. Bimanual dexterous manipulation is inherently more complex, as it requires coordinated arm movements and hand operations, making autonomous data collection challenging. This paper presents HumanoidGen, an automated task creation and demonstration collection framework that leverages atomic dexterous operations and LLM reasoning to generate relational constraints. Specifically, we provide spatial annotations for both assets and dexterous hands based on the atomic operations, and perform an LLM planner to generate a chain of actionable spatial constraints for arm movements based on object affordances and scenes. To further improve planning ability, we employ a variant of Monte Carlo tree search to enhance LLM reasoning for long-horizon tasks and insufficient annotation. In experiments, we create a novel benchmark with augmented scenarios to evaluate the quality of the collected data. The results show that the performance of the 2D and 3D diffusion policies can scale with the generated dataset. Project page is this https URL.', 'abstract_zh': '用于类人机器人的二元灵巧操作模拟任务和高质量示范数据集匮乏：HumanoidGen——基于原子灵巧操作和LLM推理的自动化任务创建与示范收集框架', 'title_zh': 'HumanoidGen: 通过LLM推理进行双臂灵巧操作的数据生成'}
{'arxiv_id': 'arXiv:2507.00816', 'title': 'PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments', 'authors': 'Mengyun Wang, Bo Wang, Yifeng Niu, Chang Wang', 'link': 'https://arxiv.org/abs/2507.00816', 'abstract': 'Accurate dynamics modeling is essential for quadrotors to achieve precise trajectory tracking in various applications. Traditional physical knowledge-driven modeling methods face substantial limitations in unknown environments characterized by variable payloads, wind disturbances, and external perturbations. On the other hand, data-driven modeling methods suffer from poor generalization when handling out-of-distribution (OoD) data, restricting their effectiveness in unknown scenarios. To address these challenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN), which combines knowledge-driven and data-driven modeling methods by embedding physical constraints directly into the training process for robust quadrotor dynamics learning. Specifically, PI-WAN employs a Temporal Convolutional Network (TCN) architecture that efficiently captures temporal dependencies from historical flight data, while a physics-informed loss function applies physical principles to improve model generalization and robustness across previously unseen conditions. By incorporating real-time prediction results into a model predictive control (MPC) framework, we achieve improvements in closed-loop tracking performance. Comprehensive simulations and real-world flight experiments demonstrate that our approach outperforms baseline methods in terms of prediction accuracy, tracking precision, and robustness to unknown environments.', 'abstract_zh': '基于物理知识和风适应的网络（PI-WAN）：用于四旋翼精确轨迹跟踪的动力学建模', 'title_zh': 'PI-WAN：一种适应风力的物理导向网络，用于未知环境中四旋翼动力学预测'}
{'arxiv_id': 'arXiv:2507.00677', 'title': 'Learning Steerable Imitation Controllers from Unstructured Animal Motions', 'authors': 'Dongho Kang, Jin Cheng, Fatemeh Zargarbashi, Taerim Yoon, Sungjoon Choi, Stelian Coros', 'link': 'https://arxiv.org/abs/2507.00677', 'abstract': "This paper presents a control framework for legged robots that leverages unstructured real-world animal motion data to generate animal-like and user-steerable behaviors. Our framework learns to follow velocity commands while reproducing the diverse gait patterns in the original dataset. To begin with, animal motion data is transformed into a robot-compatible database using constrained inverse kinematics and model predictive control, bridging the morphological and physical gap between the animal and the robot. Subsequently, a variational autoencoder-based motion synthesis module captures the diverse locomotion patterns in the motion database and generates smooth transitions between them in response to velocity commands. The resulting kinematic motions serve as references for a reinforcement learning-based feedback controller deployed on physical robots. We show that this approach enables a quadruped robot to adaptively switch gaits and accurately track user velocity commands while maintaining the stylistic coherence of the motion data. Additionally, we provide component-wise evaluations to analyze the system's behavior in depth and demonstrate the efficacy of our method for more accurate and reliable motion imitation.", 'abstract_zh': '本文提出了一种控制框架，利用未结构化的现实世界动物运动数据生成类似动物且可用户操控的行为。我们的框架在遵循速度指令的同时再现原始数据集中的多种步态模式。首先，使用约束逆运动学和模型预测控制将动物运动数据转换为与机器人兼容的数据库，缩小动物与机器人在形态和物理上的差距。随后，基于变分自编码器的运动合成模块捕捉运动数据库中的多样化动作模式，并在响应速度指令时生成平滑过渡。所产生的运动用于强化学习反馈控制的参考，在物理机器人上部署。我们展示了该方法使四足机器人能够适应性切换步态并准确跟踪用户速度指令，同时保持运动数据的风格一致性。此外，我们提供了组件级评估来深入分析系统的性能，并展示了该方法在更准确可靠的运动模仿方面的有效性。', 'title_zh': '从无结构动物运动中学习可控模仿控制器'}
{'arxiv_id': 'arXiv:2507.00446', 'title': 'DIJE: Dense Image Jacobian Estimation for Robust Robotic Self-Recognition and Visual Servoing', 'authors': 'Yasunori Toshimitsu, Kento Kawaharazuka, Akihiro Miki, Kei Okada, Masayuki Inaba', 'link': 'https://arxiv.org/abs/2507.00446', 'abstract': "For robots to move in the real world, they must first correctly understand the state of its own body and the tools that it holds. In this research, we propose DIJE, an algorithm to estimate the image Jacobian for every pixel. It is based on an optical flow calculation and a simplified Kalman Filter that can be efficiently run on the whole image in real time. It does not rely on markers nor knowledge of the robotic structure. We use the DIJE in a self-recognition process which can robustly distinguish between movement by the robot and by external entities, even when the motion overlaps. We also propose a visual servoing controller based on DIJE, which can learn to control the robot's body to conduct reaching movements or bimanual tool-tip control. The proposed algorithms were implemented on a physical musculoskeletal robot and its performance was verified. We believe that such global estimation of the visuomotor policy has the potential to be extended into a more general framework for manipulation.", 'abstract_zh': '机器人在真实世界中移动，必须首先准确理解自己身体和持有的工具的状态。在此研究中，我们提出DIJE算法，用于估计每个像素的图像雅可比矩阵。该算法基于光学流计算和一个简化的卡尔曼滤波器，可以在整个图像中实时高效运行。它无需使用标记也不依赖于对机器人结构的了解。我们在一种自识别过程中使用DIJE，该过程可以 robust 地区分由机器人自身和外部实体引起的运动，即使运动重叠也是如此。我们还提出了一种基于DIJE的视觉伺服控制器，可以学习控制机器人的身体进行到达运动或双臂工具尖端控制。提出的算法已在物理肌骨骼机器人上实现，并验证了其性能。我们相信，这种全局视觉运动政策估计有可能扩展为更通用的操纵框架。', 'title_zh': 'DIJE: 密度图像雅可比估计在鲁棒机器人自我识别和视觉伺服中的应用'}
{'arxiv_id': 'arXiv:2507.00416', 'title': 'Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding', 'authors': 'Tao Lin, Gen Li, Yilei Zhong, Yanwen Zou, Bo Zhao', 'link': 'https://arxiv.org/abs/2507.00416', 'abstract': 'Vision-Language-Action (VLA) models have emerged as a promising framework for enabling generalist robots capable of perceiving, reasoning, and acting in the real world. These models usually build upon pretrained Vision-Language Models (VLMs), which excel at semantic understanding due to large-scale text pretraining. However, VLMs typically lack precise spatial understanding capabilities, as they are primarily tuned on 2D image-text pairs without 3D supervision. To address this limitation, recent approaches have incorporated explicit 3D inputs such as point clouds or depth maps, but this necessitates additional depth sensors or defective estimation. In contrast, our work introduces a plug-and-play module that implicitly injects 3D geometry features into VLA models by leveraging an off-the-shelf visual geometry foundation models. We design five spatially challenging tasks that require precise spatial understanding ability to validate effectiveness of our method. Extensive evaluations show that our method significantly improves the performance of state-of-the-art VLA models across diverse scenarios.', 'abstract_zh': 'Vision-Language-Action (VLA) 模型已成为一种有潜力的框架，用于实现能够在现实世界中感知、推理和行动的通用机器人。这些模型通常基于预训练的Vision-Language模型（VLMs），因大规模文本预训练而擅长语义理解。然而，VLMs通常缺乏精确的空间理解能力，因为它们主要是在缺乏3D监督的2D图像-文本配对上进行调整。为解决这一限制，最近的方法整合了显式的3D输入，如点云或深度图，但这需要额外的深度传感器或不准确的估计。相比之下，我们的工作引入了一个即插即用模块，通过利用现成的视觉几何基础模型隐式地将3D几何特征注入到VLA模型中。我们设计了五个需要精确空间理解能力的空间挑战任务，以验证我们方法的有效性。广泛的评估表明，我们的方法显著提高了最先进的VLA模型在各种场景中的性能。', 'title_zh': 'Evo-0：具有隐式空间理解的视觉-语言-动作模型'}
{'arxiv_id': 'arXiv:2507.00319', 'title': 'When Digital Twins Meet Large Language Models: Realistic, Interactive, and Editable Simulation for Autonomous Driving', 'authors': 'Tanmay Vilas Samak, Chinmay Vilas Samak, Bing Li, Venkat Krovi', 'link': 'https://arxiv.org/abs/2507.00319', 'abstract': 'Simulation frameworks have been key enablers for the development and validation of autonomous driving systems. However, existing methods struggle to comprehensively address the autonomy-oriented requirements of balancing: (i) dynamical fidelity, (ii) photorealistic rendering, (iii) context-relevant scenario orchestration, and (iv) real-time performance. To address these limitations, we present a unified framework for creating and curating high-fidelity digital twins to accelerate advancements in autonomous driving research. Our framework leverages a mix of physics-based and data-driven techniques for developing and simulating digital twins of autonomous vehicles and their operating environments. It is capable of reconstructing real-world scenes and assets (real2sim) with geometric and photorealistic accuracy and infusing them with various physical properties to enable real-time dynamical simulation of the ensuing driving scenarios. Additionally, it also incorporates a large language model (LLM) interface to flexibly edit the driving scenarios online via natural language prompts. We analyze the presented framework in terms of its fidelity, performance, and serviceability. Results indicate that our framework can reconstruct 3D scenes and assets with up to 97% structural similarity, while maintaining frame rates above 60 Hz. We also demonstrate that it can handle natural language prompts to generate diverse driving scenarios with up to 95% repeatability and 85% generalizability.', 'abstract_zh': '基于物理和数据驱动技术的统一框架：加速自主驾驶研究中的高保真数字孪生创建和优化', 'title_zh': '当数字孪生遇上大型语言模型：自动驾驶的现实、交互和可编辑模拟'}
{'arxiv_id': 'arXiv:2507.00273', 'title': 'Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation', 'authors': 'Yusuke Tanaka, Alvin Zhu, Quanyou Wang, Dennis Hong', 'link': 'https://arxiv.org/abs/2507.00273', 'abstract': "Reinforcement learning (RL) has enabled significant advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains. This omission can lead to inaccurate motion modeling and suboptimal policies, particularly for robots with high actuation complexity. This paper presents an end-to-end curriculum RL framework for BRUCE, a kid-sized humanoid robot featuring three distinct parallel mechanisms in its legs: a differential pulley, a 5-bar linkage, and a 4-bar linkage. Unlike prior approaches that rely on simplified serial approximations, we simulate all closed-chain constraints natively using GPU-accelerated MJX (MuJoCo), preserving the hardware's physical properties during training. We benchmark our RL approach against a Model Predictive Controller (MPC), demonstrating better surface generalization and performance in real-world zero-shot deployment. This work highlights the computational approaches and performance benefits of fully simulating parallel mechanisms in end-to-end learning pipelines for legged humanoids.", 'abstract_zh': '基于增强学习的BRUCE腿式人形机器人并联机构端到端课程学习框架', 'title_zh': '具有并行驱动的人形机器人面向机械智能的课程强化学习'}
{'arxiv_id': 'arXiv:2507.00268', 'title': 'Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems', 'authors': 'Oren Fivel, Matan Rudman, Kobi Cohen', 'link': 'https://arxiv.org/abs/2507.00268', 'abstract': "Deep reinforcement learning (DRL) has become a powerful tool for complex decision-making in machine learning and AI. However, traditional methods often assume perfect action execution, overlooking the uncertainties and deviations between an agent's selected actions and the actual system response. In real-world applications, such as robotics, mechatronics, and communication networks, execution mismatches arising from system dynamics, hardware constraints, and latency can significantly degrade performance. This work advances AI by developing a novel control-optimized DRL framework that explicitly models and compensates for action execution mismatches, a challenge largely overlooked in existing methods. Our approach establishes a structured two-stage process: determining the desired action and selecting the appropriate control signal to ensure proper execution. It trains the agent while accounting for action mismatches and controller corrections. By incorporating these factors into the training process, the AI agent optimizes the desired action with respect to both the actual control signal and the intended outcome, explicitly considering execution errors. This approach enhances robustness, ensuring that decision-making remains effective under real-world uncertainties. Our approach offers a substantial advancement for engineering practice by bridging the gap between idealized learning and real-world implementation. It equips intelligent agents operating in engineering environments with the ability to anticipate and adjust for actuation errors and system disturbances during training. We evaluate the framework in five widely used open-source mechanical simulation environments we restructured and developed to reflect real-world operating conditions, showcasing its robustness against uncertainties and offering a highly practical and efficient solution for control-oriented applications.", 'abstract_zh': '深度强化学习（DRL）已成为机器学习和AI中复杂决策的强大工具。然而，传统方法常常假设动作执行完美无缺，忽略了代理所选动作与实际系统响应之间的不确定性和偏差。在现实世界的应用中，如机器人技术、机电一体化和通信网络中，由于系统动力学、硬件限制和延迟等因素引起的动作执行不匹配会显著降低性能。本工作通过开发一种新颖的控制优化DRL框架，明确建模并补偿动作执行不匹配问题，而现有方法中对此挑战关注不足。我们的方法建立了一个结构化的两阶段过程：确定期望动作和选择合适的控制信号以确保正确执行。该方法在考虑动作不匹配和控制器校正的同时训练代理。通过将这些因素纳入训练过程，AI代理可以基于实际控制信号和预期结果优化期望动作，明确考虑执行误差。这种方法增强了系统的鲁棒性，确保在现实世界不确定性下决策的有效性。我们的方法通过弥合理想化学习与实际实施之间的差距，为工程实践提供了重大进步。它使在工程环境中操作的智能代理能够在训练过程中预期和调整执行错误和系统干扰。我们在五个我们重新构建和开发的广泛使用的开源机械仿真环境中评估了该框架，展示了其在面对不确定性时的鲁棒性，提供了一种高度实用和高效的控制导向应用解决方案。', 'title_zh': '基于控制优化的深度强化学习在人工智能自主系统中的应用'}
{'arxiv_id': 'arXiv:2507.00963', 'title': 'Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception', 'authors': 'Fan Wang, Giulia Perugia, Yuan Feng, Wijnand IJsselsteijn', 'link': 'https://arxiv.org/abs/2507.00963', 'abstract': "As social robots increasingly enter dementia care, concerns about deception, intentional or not, are gaining attention. Yet, how robotic design cues might elicit misleading perceptions in people with dementia, and how these perceptions arise, remains insufficiently understood. In this scoping review, we examined 26 empirical studies on interactions between people with dementia and physical social robots. We identify four key design cue categories that may influence deceptive impressions: cues resembling physiological signs (e.g., simulated breathing), social intentions (e.g., playful movement), familiar beings (e.g., animal-like form and sound), and, to a lesser extent, cues that reveal artificiality. Thematic analysis of user responses reveals that people with dementia often attribute biological, social, and mental capacities to robots, dynamically shifting between awareness and illusion. These findings underscore the fluctuating nature of ontological perception in dementia contexts. Existing definitions of robotic deception often rest on philosophical or behaviorist premises, but rarely engage with the cognitive mechanisms involved. We propose an empirically grounded definition: robotic deception occurs when Type 1 (automatic, heuristic) processing dominates over Type 2 (deliberative, analytic) reasoning, leading to misinterpretation of a robot's artificial nature. This dual-process perspective highlights the ethical complexity of social robots in dementia care and calls for design approaches that are not only engaging, but also epistemically respectful.", 'abstract_zh': '随着社会机器人越来越多地进入痴呆症护理领域，关于欺骗（故意与否）的关注度正在增加。然而，关于机器人设计线索如何引发痴呆症患者产生误导性感知以及这些感知是如何产生的理解仍然不足。在本综述研究中，我们探讨了26项关于痴呆症患者与物理社会机器人互动的研究。我们确定了四个可能影响欺骗性印象的关键设计线索类别：模拟生理征象的线索（如模拟呼吸）、社会意图（如嬉戏的动作）、熟悉的生物形象（如类动物的外形和声音），以及较少的揭示人工性的线索。对用户反馈的主题分析表明，痴呆症患者经常将生物学、社会和心理能力归因于机器人，并在意识与错觉之间动态转变。这些发现强调了痴呆症情境下本体感知的波动性。现有的关于机器人欺骗的定义往往基于哲学或行为主义的假设，但很少涉及认知机制。我们提出一个基于实证的方法论定义：当第一型（自动的、启发式的）处理过程主导了第二型（反思的、分析性的）推理，导致对机器人人工性的误读时，就发生了机器人欺骗。这种双重过程视角突出了社会机器人在痴呆症护理中的伦理复杂性，并呼吁设计方法不仅要有吸引力，还要具有认知尊重。', 'title_zh': '痴呆症患者的社会机器人：从设计到感知的欺骗现象文献综述'}
{'arxiv_id': 'arXiv:2507.00886', 'title': 'GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond', 'authors': 'Anna-Maria Halacheva, Jan-Nico Zaech, Xi Wang, Danda Pani Paudel, Luc Van Gool', 'link': 'https://arxiv.org/abs/2507.00886', 'abstract': 'As multimodal language models advance, their application to 3D scene understanding is a fast-growing frontier, driving the development of 3D Vision-Language Models (VLMs). Current methods show strong dependence on object detectors, introducing processing bottlenecks and limitations in taxonomic flexibility. To address these limitations, we propose a scene-centric 3D VLM for 3D Gaussian splat scenes that employs language- and task-aware scene representations. Our approach directly embeds rich linguistic features into the 3D scene representation by associating language with each Gaussian primitive, achieving early modality alignment. To process the resulting dense representations, we introduce a dual sparsifier that distills them into compact, task-relevant tokens via task-guided and location-guided pathways, producing sparse, task-aware global and local scene tokens. Notably, we present the first Gaussian splatting-based VLM, leveraging photorealistic 3D representations derived from standard RGB images, demonstrating strong generalization: it improves performance of prior 3D VLM five folds, in out-of-the-domain settings.', 'abstract_zh': '面向3D场景的以场景为中心的Gaussian splating语言-视觉模型', 'title_zh': 'GaussianVLM：以场景为中心的3D视觉语言模型，采用语言对齐的高斯斑点进行嵌入式推理和超越领域'}
{'arxiv_id': 'arXiv:2507.00611', 'title': 'Residual Reward Models for Preference-based Reinforcement Learning', 'authors': 'Chenyang Cao, Miguel Rogel-García, Mohamed Nabail, Xueqian Wang, Nicholas Rhinehart', 'link': 'https://arxiv.org/abs/2507.00611', 'abstract': "Preference-based Reinforcement Learning (PbRL) provides a way to learn high-performance policies in environments where the reward signal is hard to specify, avoiding heuristic and time-consuming reward design. However, PbRL can suffer from slow convergence speed since it requires training in a reward model. Prior work has proposed learning a reward model from demonstrations and fine-tuning it using preferences. However, when the model is a neural network, using different loss functions for pre-training and fine-tuning can pose challenges to reliable optimization. In this paper, we propose a method to effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM assumes that the true reward of the environment can be split into a sum of two parts: a prior reward and a learned reward. The prior reward is a term available before training, for example, a user's ``best guess'' reward function, or a reward function learned from inverse reinforcement learning (IRL), and the learned reward is trained with preferences. We introduce state-based and image-based versions of RRM and evaluate them on several tasks in the Meta-World environment suite. Experimental results show that our method substantially improves the performance of a common PbRL method. Our method achieves performance improvements for a variety of different types of prior rewards, including proxy rewards, a reward obtained from IRL, and even a negated version of the proxy reward. We also conduct experiments with a Franka Panda to show that our method leads to superior performance on a real robot. It significantly accelerates policy learning for different tasks, achieving success in fewer steps than the baseline. The videos are presented at this https URL.", 'abstract_zh': '基于偏好强化学习中的 residually 奖励模型（Residual Reward Model）方法', 'title_zh': '基于偏好 reinforcement learning 的残差奖励模型'}
{'arxiv_id': 'arXiv:2507.00271', 'title': 'User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the "Sunday Blues"', 'authors': 'Zhuochao Peng, Jiaxin Xu, Jun Hu, Haian Xue, Laurens A. G. Kolks, Pieter M. A. Desmet', 'link': 'https://arxiv.org/abs/2507.00271', 'abstract': 'While recent research highlights the potential of social robots to support mood regulation, little is known about how prospective users view their integration into everyday life. To explore this, we conducted an exploratory case study that used a speculative robot concept "Mora" to provoke reflection and facilitate meaningful discussion about using social robots to manage subtle, day-to-day emotional experiences. We focused on the "Sunday Blues," a common dip in mood that occurs at the end of the weekend, as a relatable context in which to explore individuals\' insights. Using a video prototype and a co-constructing stories method, we engaged 15 participants in imagining interactions with Mora and discussing their expectations, doubts, and concerns. The study surfaced a range of nuanced reflections around the attributes of social robots like empathy, intervention effectiveness, and ethical boundaries, which we translated into design considerations for future research and development in human-robot interaction.', 'abstract_zh': '虽然最近的研究强调了社交机器人在情绪调节方面潜在的应用价值，但人们对社交机器人融入日常生活的方式知之甚少。为了探索这一问题，我们通过使用一种推测性机器人概念“Mora”开展探索性案例研究，旨在激发反思并促进关于如何使用社交机器人来管理日常生活中的微妙情感体验的有意义讨论。我们将“周末 Blues”作为可共鸣的情境，以此探索个体的见解。我们利用视频原型和共构故事的方法，与15名参与者一起设想与Mora的互动，并讨论他们的期望、疑虑和担忧。研究揭示了关于社交机器人属性（如同理心、干预效果和伦理界限）的一系列细腻反思，并将其转化为未来人机交互研究和开发中的设计考虑。', 'title_zh': '用户对用于情绪调节的社会机器人的关注：以“周日 blues”为例'}
{'arxiv_id': 'arXiv:2507.00841', 'title': 'SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents', 'authors': 'Siyuan Liang, Tianmeng Fang, Zhe Liu, Aishan Liu, Yan Xiao, Jinyuan He, Ee-Chien Chang, Xiaochun Cao', 'link': 'https://arxiv.org/abs/2507.00841', 'abstract': 'With the wide application of multimodal foundation models in intelligent agent systems, scenarios such as mobile device control, intelligent assistant interaction, and multimodal task execution are gradually relying on such large model-driven agents. However, the related systems are also increasingly exposed to potential jailbreak risks. Attackers may induce the agents to bypass the original behavioral constraints through specific inputs, and then trigger certain risky and sensitive operations, such as modifying settings, executing unauthorized commands, or impersonating user identities, which brings new challenges to system security. Existing security measures for intelligent agents still have limitations when facing complex interactions, especially in detecting potentially risky behaviors across multiple rounds of conversations or sequences of tasks. In addition, an efficient and consistent automated methodology to assist in assessing and determining the impact of such risks is currently lacking. This work explores the security issues surrounding mobile multimodal agents, attempts to construct a risk discrimination mechanism by incorporating behavioral sequence information, and designs an automated assisted assessment scheme based on a large language model. Through preliminary validation in several representative high-risk tasks, the results show that the method can improve the recognition of risky behaviors to some extent and assist in reducing the probability of agents being jailbroken. We hope that this study can provide some valuable references for the security risk modeling and protection of multimodal intelligent agent systems.', 'abstract_zh': '面向多模态移动代理的安全挑战与应对策略', 'title_zh': 'SafeMobile：链级 Jailbreak 检测与多模移动代理自动化评估'}
{'arxiv_id': 'arXiv:2507.00257', 'title': 'Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning', 'authors': 'Davide Salaorni, Vincenzo De Paola, Samuele Delpero, Giovanni Dispoto, Paolo Bonetti, Alessio Russo, Giuseppe Calcagno, Francesco Trovò, Matteo Papini, Alberto Maria Metelli, Marco Mussi, Marcello Restelli', 'link': 'https://arxiv.org/abs/2507.00257', 'abstract': 'In recent years, \\emph{Reinforcement Learning} (RL) has made remarkable progress, achieving superhuman performance in a wide range of simulated environments. As research moves toward deploying RL in real-world applications, the field faces a new set of challenges inherent to real-world settings, such as large state-action spaces, non-stationarity, and partial observability. Despite their importance, these challenges are often underexplored in current benchmarks, which tend to focus on idealized, fully observable, and stationary environments, often neglecting to incorporate real-world complexities explicitly. In this paper, we introduce \\texttt{Gym4ReaL}, a comprehensive suite of realistic environments designed to support the development and evaluation of RL algorithms that can operate in real-world scenarios. The suite includes a diverse set of tasks that expose algorithms to a variety of practical challenges. Our experimental results show that, in these settings, standard RL algorithms confirm their competitiveness against rule-based benchmarks, motivating the development of new methods to fully exploit the potential of RL to tackle the complexities of real-world tasks.', 'abstract_zh': '近年来，强化学习（Reinforcement Learning，RL）取得了显著进展，实现了在广泛模拟环境中的超人类性能。随着研究转向在实际应用场景中部署RL，该领域面临一系列固有的新挑战，如状态-动作空间庞大、非 stationary特性和部分可观测性。尽管这些挑战至关重要，但在当前基准中往往被忽视，这些基准倾向于关注理想化的、完全可观测的和 stationary 的环境，常常未能明确纳入实际世界的复杂性。本文引进了Gym4ReaL，这是一个全面的现实环境套件，旨在支持在现实场景中运行和评估RL算法的发展。该套件包括一系列多样化的任务，使算法面临各种实际挑战。我们的实验结果表明，在这些环境中，标准RL算法在与基于规则的基准的竞争力得到了证实，这激励了开发新的方法，以充分发挥RL在应对实际任务复杂性方面的潜力。', 'title_zh': 'Gym4Real: 一套实时强化学习基准测试套件'}
{'arxiv_id': 'arXiv:2507.00030', 'title': 'Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments', 'authors': 'Abhishek Verma, Nallarasan V, Balaraman Ravindran', 'link': 'https://arxiv.org/abs/2507.00030', 'abstract': 'Deep Reinforcement Learning (DRL) has achieved remarkable success in complex sequential decision-making tasks, such as playing Atari 2600 games and mastering board games. A critical yet underexplored aspect of DRL is the temporal scale of action execution. We propose a novel paradigm that integrates contextual bandits with DRL to adaptively select action durations, enhancing policy flexibility and computational efficiency. Our approach augments a Deep Q-Network (DQN) with a contextual bandit module that learns to choose optimal action repetition rates based on state contexts. Experiments on Atari 2600 games demonstrate significant performance improvements over static duration baselines, highlighting the efficacy of adaptive temporal abstractions in DRL. This paradigm offers a scalable solution for real-time applications like gaming and robotics, where dynamic action durations are critical.', 'abstract_zh': '深度强化学习（DRL）在复杂序列决策任务中取得了显著成功，如玩Atari 2600游戏和掌握棋盘游戏。DRL的一个重要但尚未充分探索的方面是动作执行的时间尺度。我们提出了一种新的框架，将上下文臂与DRL集成以自适应选择动作持续时间，增强策略的灵活性和计算效率。该方法通过将上下文臂模块增强到深度Q网络（DQN）中，使模块能够基于状态上下文学习选择最优的动作重复率。实验结果表明，与静态持续时间基线相比，该方法在Atari 2600游戏中的性能有显著提升，突显了在DRL中自适应时序抽象的有效性。该框架为类似游戏和机器人领域的实时应用提供了可扩展的解决方案，其中动态动作持续时间至关重要。', 'title_zh': '基于上下文臂博弈的动态环境中自适应动作持续时间的深度强化学习'}
{'arxiv_id': 'arXiv:2507.00011', 'title': 'Novel RL approach for efficient Elevator Group Control Systems', 'authors': 'Nathan Vaartjes, Vincent Francois-Lavet', 'link': 'https://arxiv.org/abs/2507.00011', 'abstract': 'Efficient elevator traffic management in large buildings is critical for minimizing passenger travel times and energy consumption. Because heuristic- or pattern-detection-based controllers struggle with the stochastic and combinatorial nature of dispatching, we model the six-elevator, fifteen-floor system at Vrije Universiteit Amsterdam as a Markov Decision Process and train an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS). Key innovations include a novel action space encoding to handle the combinatorial complexity of elevator dispatching, the introduction of infra-steps to model continuous passenger arrivals, and a tailored reward signal to improve learning efficiency. In addition, we explore various ways to adapt the discounting factor to the infra-step formulation. We investigate RL architectures based on Dueling Double Deep Q-learning, showing that the proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a highly stochastic environment, and thereby outperforms a traditional rule-based algorithm.', 'abstract_zh': '在大型建筑中高效管理电梯交通对于减少乘客旅行时间和能耗至关重要。由于启发式或模式检测控制器难以处理调度的随机性和组合性，我们将位于阿姆斯特丹自由大学的六部电梯、十五层楼系统建模为马尔可夫决策过程，并训练了一个端到端强化学习（RL）电梯群控系统（EGCS）。关键创新包括一种新颖的动作空间编码以处理电梯调度的组合复杂性，引入基础设施步来模拟连续的乘客到达，并定制奖励信号以提高学习效率。此外，我们探索了各种方法来调整折扣因子以适应基础设施步的建模。基于对拼双深Q学习的RL架构的研究表明，所提出的基于RL的EGCS能够适应波动的交通模式，从高度随机的环境中学习，并因此优于传统的基于规则的算法。', 'title_zh': '新型 RL 方法在高效电梯群控系统中的应用'}
