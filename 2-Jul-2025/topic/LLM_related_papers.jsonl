{'arxiv_id': 'arXiv:2507.00979', 'title': 'Enhancing LLM Agent Safety via Causal Influence Prompting', 'authors': 'Dongyoon Hahm, Woogyeol Jin, June Suk Choi, Sungsoo Ahn, Kimin Lee', 'link': 'https://arxiv.org/abs/2507.00979', 'abstract': 'As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.', 'abstract_zh': '基于因果影响图的自主代理风险识别与缓解技术', 'title_zh': '通过因果影响提示增强LLM代理的安全性'}
{'arxiv_id': 'arXiv:2507.00726', 'title': 'Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess', 'authors': 'Dongyoon Hwang, Hojoon Lee, Jaegul Choo, Dongmin Park, Jongho Park', 'link': 'https://arxiv.org/abs/2507.00726', 'abstract': "While reinforcement learning (RL) for large language models (LLMs) has shown promise in mathematical reasoning, strategic reasoning for LLMs using RL remains largely unexplored. We investigate whether LLMs can develop strategic reasoning capabilities through RL in chess. To this end, we leverage a chess-pretrained action-value network to provide dense reward on the LLM's output move quality, which can be seen as a form of knowledge distillation. Our experiments show that our distillation-based dense rewards often outperform sparse binary rewards. However, surprisingly, all models plateau far below expert levels. We provide SFT and RL ablations on chess reasoning training and find evidence that this limitation stems from a deficit in the pretrained models' internal understanding of chess--a deficit which RL alone may not be able to fully overcome.", 'abstract_zh': 'while reinforcement learning for large language models in mathematical reasoning remains unexplored, strategic reasoning for llsms using rl through chess remains largely unexplored.', 'title_zh': '大型语言模型能发展出策略性推理能力吗？基于学习国际象棋的后训练洞见'}
{'arxiv_id': 'arXiv:2507.00432', 'title': 'Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning', 'authors': 'Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, Xiang Yue', 'link': 'https://arxiv.org/abs/2507.00432', 'abstract': 'Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.', 'abstract_zh': '大型语言模型（LLMs）中的数学推理能力成为了进步的象征，新模型在MATH和AIME等基准测试中迅速超越人类水平。但随着数学排行榜每周的进步，值得追问：这些进步反映的是更广泛的问题解决能力还是仅仅是狭隘的过拟合？为了回答这个问题，我们评估了超过20个开放参数的推理调优模型在广泛的任务中表现，包括数学、科学问答、代理规划、编码和标准指令跟随。我们惊讶地发现，大多数在数学中取得成功的模型无法将其优势转移到其他领域。为了严格研究这一现象，我们使用仅数学数据但在不同调优方法下对Qwen3-14B模型进行了控制实验。我们发现，强化学习（RL）调优模型在不同领域中表现出良好的泛化能力，而监督微调（SFT）调优模型往往忘记通用能力。潜在空间表示与标记空间分布转移分析揭示了SFT引起显著的表示和输出漂移，而RL保存通用领域的结构。我们的结果表明，需要重新思考标准后训练食谱，特别是在依赖SFT提炼的数据方面推进推理模型方面。', 'title_zh': '数学推理能否提升通用大语言模型的能力？理解大语言模型推理的迁移性'}
{'arxiv_id': 'arXiv:2507.00417', 'title': 'ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context', 'authors': 'Joongwon Kim, Anirudh Goyal, Liang Tan, Hannaneh Hajishirzi, Srinivasan Iyer, Tianlu Wang', 'link': 'https://arxiv.org/abs/2507.00417', 'abstract': 'We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework for training language models to reason like search algorithms, explicitly leveraging self-reflection, backtracking, and exploration in their outputs. Recently, training large language models (LLMs) via reinforcement learning (RL) has led to the advent of reasoning models with greatly enhanced reasoning capabilities. Open-source replications of reasoning models, while successful, build upon models that already exhibit strong reasoning capabilities along with search behavior observed even before RL. As a result, it is yet unclear how to boost the reasoning capabilities of other non-reasoner models including Llama 3. ASTRO teaches such models to internalize structured search behavior through a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over mathematical problem-solving trajectories. By converting search traces into natural language chain-of-thoughts that capture both successes and recoveries from failure, ASTRO bootstraps models with a rich prior for exploration during RL. We finetune our models on these search-derived traces and further improve performance via RL with verifiable rewards. We apply ASTRO to the Llama 3 family of models and achieve absolute performance gains of 16.0% on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon challenging problems that require iterative correction. Our results demonstrate that search-inspired training offers a principled way to instill robust reasoning capabilities into open LLMs.', 'abstract_zh': 'ASTRO：自动回归搜索教导推理器——一种训练语言模型进行搜索算法式推理的框架', 'title_zh': 'ASTRO：通过反思和回溯进行推理的语料库教学方法'}
{'arxiv_id': 'arXiv:2507.00181', 'title': 'ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline', 'authors': 'Georgios P. Georgiou', 'link': 'https://arxiv.org/abs/2507.00181', 'abstract': 'Despite the increasing use of large language models (LLMs) in education, concerns have emerged about their potential to reduce deep thinking and active learning. This study investigates the impact of generative artificial intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of students during academic writing tasks. The study employed an experimental design with participants randomly assigned to either an AI-assisted (ChatGPT) or a non-assisted (control) condition. Participants completed a structured argumentative writing task followed by a cognitive engagement scale (CES), the CES-AI, developed to assess mental effort, attention, deep processing, and strategic thinking. The results revealed significantly lower cognitive engagement scores in the ChatGPT group compared to the control group. These findings suggest that AI assistance may lead to cognitive offloading. The study contributes to the growing body of literature on the psychological implications of AI in education and raises important questions about the integration of such tools into academic practice. It calls for pedagogical strategies that promote active, reflective engagement with AI-generated content to avoid compromising self-regulated learning and deep cognitive involvement of students.', 'abstract_zh': '尽管大型语言模型（LLMs）在教育中的应用日益增多，但对其可能降低深度思考和主动学习的担忧也随之出现。本研究探讨了生成式人工智能（AI）工具，特别是ChatGPT，对学生在学术写作任务中认知参与度的影响。研究采用实验设计，参与者随机分配至AI辅助（ChatGPT）组或非辅助（控制）组。参与者完成了一项结构化的论证写作任务，随后进行了认知参与度量表（CES-AI）的评估，该量表用于评估心理努力、注意力、深层次加工和策略性思维。研究结果表明，ChatGPT组的认知参与度评分显著低于控制组。这些发现表明，AI辅助可能导致认知负担转移。本研究为人工智能在教育中的心理影响相关文献增添了新的内容，并提出了关于将此类工具融入学术实践的重要性问题。它呼吁采用促进积极、反思性参与AI生成内容的教育策略，以避免损害学生的自我调节学习和深层次认知参与。', 'title_zh': 'ChatGPT使思考变得“懒惰”：认知参与度下降的证据'}
{'arxiv_id': 'arXiv:2507.00092', 'title': "Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models", 'authors': 'Basab Jha, Firoj Paudel, Ujjwal Puri, Zhang Yuting, Choi Donghyuk, Wang Junhao', 'link': 'https://arxiv.org/abs/2507.00092', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities at solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but their decision-making processes remain somewhat blackbox. We introduce textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a 4-billion-parameter reasoning model, employs a metacognitive structure that reflects back via attention processes to identify major decision points and generate explanations of reasoning choices. While typical CoT approaches are directed towards forward reasoning generation, inverse reasoning provides insight into why specific reasoning chains were selected over others. Through thorough testing of logical reasoning puzzles, math problems and ethical dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy (74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for its task, and offers performance almost on par with models like Claude-3.5 Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework to reverse the attention flow, (iii) comprehensive evaluation frameworks for reasoning transparency, and (iv) evidence that increasing reasoning using inverse reasoning improves interpretability along with reasoning performance. Our work creates new avenues for transparent AI systems and closes significant gaps in AI safety, education, and scientific discovery.', 'abstract_zh': '大规模语言模型（LLMs）通过Chain-of-Thought（CoT）提示展示了在解决复杂推理任务方面的卓越能力，但其决策过程仍然相对黑箱。我们引入了文本逆向推理这一新型范式，使LLMs能够事后分解和解释其推理链。我们的方法应用于一个40亿参数的推理模型SAGE-nano，利用一种元认知结构，通过注意力机制进行反向反馈以识别主要决策点并生成推理选择的解释。虽然典型的CoT方法侧重于正向推理生成，逆向推理则提供了为什么选择特定的推理链而非其他链的见解。通过AQUA-RAT、CommonsenseQA和自定基准数据集中的逻辑推理谜题、数学问题和伦理困境的全面测试，我们展示了SAGE-nano在推理准确性（AQUA-RAT上74.6%）和解释质量（92.1%的人类偏好得分）方面处于前沿地位，并且在推理性能方面几乎与Claude-3.5 Sonnet或GPT-4o等模型不相上下。我们的贡献包括：（i）第一个严格的通过逆向推理进行LLM自我反思的框架，（ii）一种新的元学习框架来逆转注意力流，（iii）推理透明度的综合评估框架，以及（iv）证据表明，通过逆向推理增加推理可以提高解释性并提升推理性能。我们的工作为透明AI系统开辟了新的途径，并在AI安全、教育和科学发现方面填补了重要空白。', 'title_zh': '思考中的思考：SAGE-nano的逆向推理实现自我意识语言模型'}
{'arxiv_id': 'arXiv:2507.00054', 'title': 'Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation', 'authors': 'Shreyansh Padarha', 'link': 'https://arxiv.org/abs/2507.00054', 'abstract': "The push to compress and impart the proficiency of Large Language Models (LLMs) into more deployable and efficient Small Language Models (SLMs) has benefited from improvements in knowledge distillation (KD) techniques. These techniques allow a smaller student model to learn from a more capable and larger teacher model's responses. However, distillation often revolves around the student model merely copying the teacher's in-distribution responses, limiting its generalisability. This limitation is amplified on reasoning tasks and can be computationally expensive. In this study, we propose AdvDistill, a reward-guided dataset distillation framework. We utilise multiple generations (responses) from a teacher for each prompt and assign rewards based on rule-based verifiers. These varying and normally distributed rewards serve as weights when training student models. Our methods and their subsequent behavioural analysis demonstrate a significant improvement in student model performance for mathematical and complex reasoning tasks, showcasing the efficacy and benefits of incorporating a rewarding mechanism in dataset distillation processes.", 'abstract_zh': '将大型语言模型（LLMs）的 proficiency 压缩并 impart 到更具部署性和效率的小型语言模型（SLMs）中,得益于知识蒸馏（KD）技术的进步。这些技术允许较小的学生模型从更大、更有能力的教师模型的回答中学习。然而,蒸馏通常集中在让学生模型简单复制教师模型在分布内的回答上,限制了其泛化能力。这一限制在推理任务中尤为突出,可能会导致计算成本增加。在此研究中，我们提出 AdvDistill，一种基于奖励的数据集蒸馏框架。我们为每个提示利用教师的多轮生成（回答），并基于基于规则的验证器分配奖励。这些不同的且通常分布的奖励用于训练学生模型时的权重。我们的方法及其后续的行为分析证明，在数学和复杂推理任务中，学生模型的表现有了显著提升，展示了在数据集蒸馏过程中引入奖励机制的有效性和益处。', 'title_zh': '基于奖励引导数据集精炼增强SLMs的推理能力'}
{'arxiv_id': 'arXiv:2507.00041', 'title': 'TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables', 'authors': 'Varun Mannam, Fang Wang, Chaochun Liu, Xin Chen', 'link': 'https://arxiv.org/abs/2507.00041', 'abstract': "In talent management systems, critical information often resides in complex tabular formats, presenting significant retrieval challenges for conventional language models. These challenges are pronounced when processing Talent documentation that requires precise interpretation of tabular relationships for accurate information retrieval and downstream decision-making. Current table extraction methods struggle with semantic understanding, resulting in poor performance when integrated into retrieval-augmented chat applications. This paper identifies a key bottleneck - while structural table information can be extracted, the semantic relationships between tabular elements are lost, causing downstream query failures. To address this, we introduce TalentMine, a novel LLM-enhanced framework that transforms extracted tables into semantically enriched representations. Unlike conventional approaches relying on CSV or text linearization, our method employs specialized multimodal reasoning to preserve both structural and semantic dimensions of tabular data. Experimental evaluation across employee benefits document collections demonstrates TalentMine's superior performance, achieving 100% accuracy in query answering tasks compared to 0% for standard AWS Textract extraction and 40% for AWS Textract Visual Q&A capabilities. Our comparative analysis also reveals that the Claude v3 Haiku model achieves optimal performance for talent management applications. The key contributions of this work include (1) a systematic analysis of semantic information loss in current table extraction pipelines, (2) a novel LLM-based method for semantically enriched table representation, (3) an efficient integration framework for retrieval-augmented systems as end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks showing substantial improvements across multiple categories.", 'abstract_zh': '在人才管理体系中，关键信息经常以复杂的表格格式存在，给传统的语言模型带来了显著的检索挑战。当处理需要精确解析表格关系以实现准确信息检索和下游决策的人才文档时，这些挑战尤为明显。当前的表格提取方法在语义理解方面存在困难，导致将其集成到检索增强聊天应用中时性能不佳。本文指出了一个关键瓶颈——虽然可以提取结构化的表格信息，但表格元素之间的语义关系会丢失，从而导致下游查询失败。为了解决这个问题，我们提出了TalentMine，一种新颖的LLM增强框架，将提取的表格转换为语义丰富的表示。与依赖于CSV或文本线性化的传统方法不同，我们的方法采用专有的多模态推理来同时保留表格数据的结构和语义维度。在员工福利文档集合上的实验评估表明，TalentMine的表现优于标准的AWS Textract提取方法（准确率为0%）和AWS Textract视觉问答功能（准确率为40%）。我们的对比分析还表明，Claude v3 Haiku模型在人才管理应用中表现最优。本文的主要贡献包括：（1）系统分析当前表格提取流水线中的语义信息丢失情况；（2）一种基于LLM的语义丰富表格表示的新方法；（3）检索增强系统的高效集成框架，作为端到端系统；（4）针对人才分析任务的全面基准测试，显示出在多个类别上的显著改进。', 'title_zh': 'TalentMine：基于LLM的多模态人才表格提取与问答'}
{'arxiv_id': 'arXiv:2507.01006', 'title': 'GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning', 'authors': 'Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Leyi Pan, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianle Gong, Wenkai Li, Wei Jia, Xin Lyu, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuxuan Zhang, Zhanxiao Du, Zhenyu Hou, Zhao Xue, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang', 'link': 'https://arxiv.org/abs/2507.01006', 'abstract': 'We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then unlocks the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding, among others. To facilitate research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at this https URL.', 'abstract_zh': 'GLM-4.1V-Thinking：一种旨在促进通用多模态推理的视觉-语言模型', 'title_zh': 'GLM-4.1V-Thinking: 向通用多模态推理与可扩展强化学习的方向迈进'}
{'arxiv_id': 'arXiv:2507.00971', 'title': 'Reasoning as an Adaptive Defense for Safety', 'authors': 'Taeyoun Kim, Fahim Tajwar, Aditi Raghunathan, Aviral Kumar', 'link': 'https://arxiv.org/abs/2507.00971', 'abstract': 'Reasoning methods that adaptively allocate test-time compute have advanced LLM performance on easy to verify domains such as math and code. In this work, we study how to utilize this approach to train models that exhibit a degree of robustness to safety vulnerabilities, and show that doing so can provide benefits. We build a recipe called $\\textit{TARS}$ (Training Adaptive Reasoners for Safety), a reinforcement learning (RL) approach that trains models to reason about safety using chain-of-thought traces and a reward signal that balances safety with task completion. To build TARS, we identify three critical design choices: (1) a "lightweight" warmstart SFT stage, (2) a mix of harmful, harmless, and ambiguous prompts to prevent shortcut behaviors such as too many refusals, and (3) a reward function to prevent degeneration of reasoning capabilities during training. Models trained with TARS exhibit adaptive behaviors by spending more compute on ambiguous queries, leading to better safety-refusal trade-offs. They also internally learn to better distinguish between safe and unsafe prompts and attain greater robustness to both white-box (e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an effective, open recipe for training LLMs against jailbreaks and harmful requests by reasoning per prompt.', 'abstract_zh': '利用自适应分配测试时计算资源的推理方法已在数学和代码等易于验证的领域提升了LLM的性能。本文研究了如何利用这种方法训练具有一定程度安全漏洞鲁棒性的模型，并展示了这种方法的益处。我们构建了一个名为TARS（Training Adaptive Reasoners for Safety）的方案，这是一种强化学习方法，通过使用逻辑推理痕迹和平衡安全与任务完成的奖励信号来训练模型进行安全推理。为了构建TARS，我们确定了三个关键设计选择：（1）轻量级的预训练 Few-Shot Tuning 阶段，（2）混合包含有害、无害和模棱两可提示的提示以防止捷径行为，如过多的拒绝，以及（3）奖励函数以防止训练过程中推理能力退化。使用TARS训练的模型通过在模棱两可查询上投入更多计算资源表现出自适应行为，从而实现更好的安全拒绝权衡。它们还内部学习更好地区分安全和不安全的提示，并对白盒（如GCG）和黑盒攻击（如PAIR）表现出更强的鲁棒性。总体而言，我们的工作提供了一种有效且开放的方案，通过每提示推理来训练LLMs以抵御突破和有害请求。', 'title_zh': '推理作为适应性安全防御机制'}
{'arxiv_id': 'arXiv:2507.00953', 'title': 'From Sentences to Sequences: Rethinking Languages in Biological System', 'authors': 'Ke Liu, Shuanke Shen, Hao Chen', 'link': 'https://arxiv.org/abs/2507.00953', 'abstract': 'The paradigm of large language models in natural language processing (NLP) has also shown promise in modeling biological languages, including proteins, RNA, and DNA. Both the auto-regressive generation paradigm and evaluation metrics have been transferred from NLP to biological sequence modeling. However, the intrinsic structural correlations in natural and biological languages differ fundamentally. Therefore, we revisit the notion of language in biological systems to better understand how NLP successes can be effectively translated to biological domains. By treating the 3D structure of biomolecules as the semantic content of a sentence and accounting for the strong correlations between residues or bases, we highlight the importance of structural evaluation and demonstrate the applicability of the auto-regressive paradigm in biological language modeling. Code can be found at \\href{this https URL}{this http URL}', 'abstract_zh': '大型语言模型在自然语言处理中的范式也显示出在建模蛋白质、RNA和DNA等生物语言方面的潜力。自回归生成范式及评估指标已被从自然语言处理领域转移到生物序列建模中。然而，自然和生物语言内部的结构相关性存在根本差异。因此，我们重新审视生物系统中的语言概念，以更好地理解自然语言处理的成功如何有效地转移到生物领域。通过将生物分子的三维结构视为句子的语义内容，并考虑残基或碱基之间的强相关性，我们强调了结构评估的重要性，并展示了自回归范式在生物语言建模中的适用性。代码详见 \\href{this https URL}{此链接}。', 'title_zh': '从句子到序列：重新思考生物学系统中的语言'}
{'arxiv_id': 'arXiv:2507.00914', 'title': 'Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications', 'authors': 'Jindong Han, Yansong Ning, Zirui Yuan, Hang Ni, Fan Liu, Tengfei Lyu, Hao Liu', 'link': 'https://arxiv.org/abs/2507.00914', 'abstract': 'The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies. Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision. With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains. In this article, we focus on Urban LLM Agents, which are LLM-powered agents that are semi-embodied within the hybrid cyber-physical-social space of cities and used for system-level urban decision-making. First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features. Second, we survey the current research landscape from the perspective of agent workflows, encompassing urban sensing, memory management, reasoning, execution, and learning. Third, we categorize the application domains of urban LLM agents into five groups: urban planning, transportation, environment, public safety, and urban society, presenting representative works in each group. Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research. This survey aims to establish a foundation for the emerging field of urban LLM agents and to provide a roadmap for advancing the intersection of LLMs and urban intelligence. A curated list of relevant papers and open-source resources is maintained and continuously updated at this https URL.', 'abstract_zh': '智能城市的长久愿景是利用大数据和人工智能技术创建高效、宜居和可持续的城市环境。最近，大型语言模型（LLMs）的出现为实现这一愿景开辟了新的途径。凭借强大的语义理解和推理能力，LLMs可以部署为能在城市混合的赛博-物理-社会空间中半实体化的智能代理，用于系统级的城市决策。本文重点关注城市LLM代理，它们是 Powered by LLM 的代理，部分嵌入城市混合的赛博-物理-社会空间中，用于系统级的城市决策。首先，我们介绍城市LLM代理的概念，讨论其独特的能力和特性。其次，我们从代理工作流的角度回顾当前的研究状况，涵盖城市感知、记忆管理、推理、执行和学习。第三，我们将城市LLM代理的应用领域归类为五类：城市规划、交通、环境、公共安全和城市社会，并在每个领域展示代表性作品。最后，我们讨论对于实际部署至关重要的可信度和评估问题，并识别出若干未来研究亟待解决的问题。本文旨在为新兴的LLM城市代理领域建立基础，并为LLM与城市智能的交叉领域的发展提供路线图。相关论文和开源资源的精选列表在此 https://link/ 上持续维护和更新。', 'title_zh': '大型语言模型驱动的智能城市代理：概念、能力与应用'}
{'arxiv_id': 'arXiv:2507.00838', 'title': 'Stylometry recognizes human and LLM-generated texts in short samples', 'authors': 'Karol Przystalski, Jan K. Argasiński, Iwona Grabska-Gradzińska, Jeremi K. Ochab', 'link': 'https://arxiv.org/abs/2507.00838', 'abstract': 'The paper explores stylometry as a method to distinguish between texts created by Large Language Models (LLMs) and humans, addressing issues of model attribution, intellectual property, and ethical AI use. Stylometry has been used extensively to characterise the style and attribute authorship of texts. By applying it to LLM-generated texts, we identify their emergent writing patterns. The paper involves creating a benchmark dataset based on Wikipedia, with (a) human-written term summaries, (b) texts generated purely by LLMs (GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods (Dipper, T5). The 10-sentence long texts were classified by tree-based models (decision trees and LightGBM) using human-designed (StyloMetrix) and n-gram-based (our own pipeline) stylometric features that encode lexical, grammatical, syntactic, and punctuation patterns. The cross-validated results reached a performance of up to .87 Matthews correlation coefficient in the multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary classification, with the particular example of Wikipedia and GPT-4 reaching up to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed features characteristic of the encyclopaedic text type, individual overused words, as well as a greater grammatical standardisation of LLMs with respect to human-written texts. These results show -- crucially, in the context of the increasingly sophisticated LLMs -- that it is possible to distinguish machine- from human-generated texts at least for a well-defined text type.', 'abstract_zh': '本文探讨了将文体统计学作为区分由大型语言模型（LLMs）和人类创作的文本的方法，解决模型归因、知识产权和伦理AI使用等问题。通过广泛使用文体统计学来表征文本风格和归属作者，本文将它应用于LLM生成的文本，以识别其新兴的写作模式。本文基于维基百科创建了一个基准数据集，包括（a）由人类撰写的术语摘要，（b）由LLM（GPT-3.5/4、LLaMa 2/3、Orca和Falcon）纯生成的文本，（c）经过多种文本摘要方法（T5、BART、Gensim和Sumy）处理的文本，以及（d）通过改写方法（Dipper、T5）处理的文本。10句长的文本由基于树的模型（决策树和LightGBM）分类，使用人类设计（StyloMetrix）和基于n-克gram（我们的管道）的文体统计学特征，这些特征编码了词汇、语法、句法和标点符号模式。交叉验证结果在多类场景中达到了高达0.87的马特ews相关系数，在二分类中达到了0.79至1.0的准确率，特别是在维基百科和GPT-4的平衡数据集上达到了高达0.98的准确率。Shapley加性解释指出了百科全书文本类型特有的特征、过度使用的个别词汇，以及LLM相对于人类撰写的文本在语法标准性方面的更高程度的统一。这些结果显示，在日益复杂的LLM背景下，至少对于一种明确的文本类型，有可能区分机器生成的文本和人类生成的文本。', 'title_zh': '文体学识别短样本中的人类生成和LLM生成文本'}
{'arxiv_id': 'arXiv:2507.00817', 'title': 'CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs', 'authors': 'Jiaming Zhang, Rui Hu, Qing Guo, Wei Yang Bryan Lim', 'link': 'https://arxiv.org/abs/2507.00817', 'abstract': "Video Multimodal Large Language Models (V-MLLMs) have shown impressive capabilities in temporal reasoning and cross-modal understanding, yet their vulnerability to adversarial attacks remains underexplored due to unique challenges: complex cross-modal reasoning mechanisms, temporal dependencies, and computational constraints. We present CAVALRY-V (Cross-modal Language-Vision Adversarial Yielding for Videos), a novel framework that directly targets the critical interface between visual perception and language generation in V-MLLMs. Our approach introduces two key innovations: (1) a dual-objective semantic-visual loss function that simultaneously disrupts the model's text generation logits and visual representations to undermine cross-modal integration, and (2) a computationally efficient two-stage generator framework that combines large-scale pre-training for cross-model transferability with specialized fine-tuning for spatiotemporal coherence. Empirical evaluation on comprehensive video understanding benchmarks demonstrates that CAVALRY-V significantly outperforms existing attack methods, achieving 22.8% average improvement over the best baseline attacks on both commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5, InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves flexibility through implicit temporal coherence modeling rather than explicit regularization, enabling significant performance improvements even on image understanding (34.4% average gain). This capability demonstrates CAVALRY-V's potential as a foundational approach for adversarial research across multimodal systems.", 'abstract_zh': '跨模态语言-视觉对抗生成的CAVALRY-V框架：视频多模态大语言模型的对抗性研究', 'title_zh': 'CAVALRY-V：针对视频MLLMs的大型生成器框架 adversarial攻击'}
{'arxiv_id': 'arXiv:2507.00814', 'title': 'Many LLMs Are More Utilitarian Than One', 'authors': 'Anita Keshmirian, Razan Baltaji, Babak Hemmatian, Hadi Asghari, Lav R. Varshney', 'link': 'https://arxiv.org/abs/2507.00814', 'abstract': 'Moral judgment is integral to large language model (LLM) alignment and social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function collectively during collaboration, compared to individual agents. In human moral judgment, group deliberation leads to a utilitarian boost: a tendency to endorse norm violations that maximize benefits for the greatest number of people despite harms. We study whether a similar dynamic emerges in multi-agent LLM systems. We tested six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reasoned independently, and (2) Group, where they engaged in multi-turn discussions in pairs or triads. In personal moral dilemmas, where agents must decide to directly harm one individual to maximize the utility for others, all models found moral violations to be more acceptable when part of a group than individually, similar to human experiments. Some models endorsed actions that maximized overall well-being, even if they benefited strangers over familiar individuals. Others became more willing to violate moral norms in groups. However, while human groups show a similar action bias, the mechanism for their utilitarian boost differs from LLMs. Whereas the human shift comes from heightened sensitivity to decision outcomes, LLM groups show either reduced norm sensitivity or enhanced impartiality. This suggests that while the surface behavior of LLM collectives mimics human group reasoning, the underlying drivers differ. We discuss the implications for AI alignment, multi-agent design, and artificial moral reasoning.', 'abstract_zh': '道德判断是大规模语言模型（LLM）对齐和社会推理的关键组成部分。随着多智能体系统的兴起，理解LLM在合作中集体运行机制与其个体运行机制之间的差异变得至关重要。在人类道德判断中，团体审议带来了功利主义的提升：倾向于支持那些虽有害但能惠及最大多数人的规范违反行为。我们研究了类似动态是否在多智能体LLM系统中出现。我们测试了六种模型在两类道德困境上的表现：（1）独立模式，模型独立推理；（2）团体模式，模型以对话语轮或三人组的形式进行讨论。在涉及个体必须决定直接伤害某人以最大化他人的福利的个人道德困境中，所有模型在团体中比单独时认为道德违规行为更为可接受，类似于人类实验的结果。有些模型支持有利于整体福祉的行为，即便这些行为惠及陌生人而非熟人。其他模型则在团体中更愿意违反道德规范。然而，尽管人类团体表现出类似的行为偏见，其功利主义提升的机制与LLM不同。人类的转变来自对决策结果的敏感度增加，而LLM团体则显示出规范敏感度的降低或倾向于更公平的处理。这表明虽然LLM集体的表面行为模仿了人类团体推理，但底层驱动因素却不同。我们讨论了这些发现对AI对齐、多智能体设计和人工道德推理的含义。', 'title_zh': '许多大型语言模型比单一模型更具实用性。'}
{'arxiv_id': 'arXiv:2507.00769', 'title': 'LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing', 'authors': 'Daniel Fein, Sebastian Russo, Violet Xiang, Kabir Jolly, Rafael Rafailov, Nick Haber', 'link': 'https://arxiv.org/abs/2507.00769', 'abstract': 'Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising a held-out test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and a 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train Bradley Terry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models at this https URL, providing a vetted resource for reliable, automated evaluation and optimization of creative writing systems.', 'abstract_zh': '评估大型语言模型生成的创意写作仍具有挑战性，因为开放式的叙述缺乏 ground truths。缺乏高效的自动化评估方法，研究中使用了现成的语言模型作为零样本法官，但其可靠性在这一背景下尚不清楚。为了实现创意写作的稳健评估，我们引入了LitBench——首个标准化的创意写作验证基准和配对数据集，包含来自Reddit的2480个去偏见的人工标注故事对比测试集和43827对人工偏好标签的训练语料库。使用LitBench，我们（i） benchmark 了零样本 LLM 法官，（ii）训练了Bradley Terry和生成奖励模型，（iii）进行在线人类研究以验证奖励模型在新生成的LLM故事上的排名。我们的基准模型将Claude-3.7-Sonnet识别为最强的现成法官，其一致性达到73%；在训练的奖励模型中，Bradley-Terry模型和生成奖励模型都达到了78%的准确率，超越了所有现成的法官。在线人类研究进一步证实，我们的训练奖励模型在新的LLM生成故事中始终与人类偏好保持一致。我们在此网址https://提供LitBench和奖励模型，为创意写作系统的可靠和自动化评估与优化提供验证资源。', 'title_zh': 'LitBench: 用于创意写作可信评估的基准和数据集'}
{'arxiv_id': 'arXiv:2507.00724', 'title': 'Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features', 'authors': 'Linghui Zhu, Yiming Li, Haiqin Weng, Yan Liu, Tianwei Zhang, Shu-Tao Xia, Zhi Wang', 'link': 'https://arxiv.org/abs/2507.00724', 'abstract': 'Large vision models achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property for its owner. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to these personalized models. However, in this paper, we reveal that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized models by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by the output differences between the shadow and victim models. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously.', 'abstract_zh': '大型视觉模型通过Fine-tuning与私有数据结合，实现了各类下游任务的卓越性能，这些个性化模型成为其所有者的宝贵知识产权。然而，与传统DNN时代类似，模型窃取攻击也对这些个性化模型构成了重大风险。本文揭示，大多数现有防御方法（针对传统DNN设计），通常针对从头训练的模型，要么引入额外的安全风险，要么容易误判，甚至对Fine-tuned模型无效。为解决这些问题，本文提出了一种通过解耦常见特征来无害地验证个性化模型所有权的方法。总体而言，该方法分为三个主要阶段。首先，我们创建影子模型，保留受害模型的共性特征同时破坏数据集特定特征。我们通过计算影子模型和受害模型的输出差异来表示受害模型的数据集特定特征。然后，训练一个元分类器来识别被窃取模型，通过判断可疑模型是否包含受害模型的数据集特定特征来进行。在第三阶段，我们通过假设检验来进行模型所有权验证，以减轻随机性和增强鲁棒性。基准数据集上的 extensive 实验验证了该方法在同时检测不同类型的模型窃取方面的有效性。', 'title_zh': 'Holmes: 通过解耦通用特征以实现个性化大型视觉模型的有效且无害的所有权验证'}
{'arxiv_id': 'arXiv:2507.00665', 'title': 'SAFER: Probing Safety in Reward Models with Sparse Autoencoder', 'authors': 'Sihang Li, Wei Shi, Ziyuan Xie, Tao Liang, Guojun Ma, Xiang Wang', 'link': 'https://arxiv.org/abs/2507.00665', 'abstract': 'Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present sparse Autoencoder For Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. We apply SAFER to safety-oriented preference datasets and quantify the salience of individual features by activation differences between chosen and rejected responses. Using these feature-level signals, we design targeted data poisoning and denoising strategies. Experiments show that SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general chat performance. Our approach contributes to interpreting, auditing and refining reward models in high-stakes LLM alignment tasks. Our codes are available at this https URL. \\textit{This paper discusses topics related to large language model safety and may include discussions or examples that highlight potential risks or unsafe outcomes.}', 'abstract_zh': '基于人类反馈的强化学习（RLHF）是使大规模语言模型（LLMs）与人类价值观一致的关键范式，但其核心的奖励模型仍然 largely 不透明。在本文中，我们提出了稀疏自动编码器增强奖励模型（SAFER），这是一种通过机制分析解释和改进奖励模型的新框架。利用稀疏自动编码器（SAEs），我们揭示了奖励模型激活中的可由人类解释的特征，从而提供了与安全相关决策的见解。我们应用SAFER到以安全性为导向的偏好数据集上，并通过选择和拒绝响应之间的激活差异量化单个特征的重要性。使用这些特征级信号，我们设计了针对性的数据投毒和去噪策略。实验表明，SAFER可以在最小的数据修改下精确地损害或增强安全性对齐，而不牺牲通用聊天性能。我们的方法为高风险的大规模语言模型对齐任务解释、审计和改进奖励模型做出了贡献。我们的代码可在如下链接获得：this https URL。本文讨论了大规模语言模型安全性相关主题，可能包括强调潜在风险或不安全结果的讨论或示例。', 'title_zh': 'SAFER: 用稀疏自编码器探查奖励模型中的安全性'}
{'arxiv_id': 'arXiv:2507.00657', 'title': 'Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity', 'authors': 'Jacopo Nudo, Mario Edoardo Pandolfo, Edoardo Loru, Mattia Samory, Matteo Cinelli, Walter Quattrociocchi', 'link': 'https://arxiv.org/abs/2507.00657', 'abstract': 'We investigate how Large Language Models (LLMs) behave when simulating political discourse on social media. Leveraging 21 million interactions on X during the 2024 U.S. presidential election, we construct LLM agents based on 1,186 real users, prompting them to reply to politically salient tweets under controlled conditions. Agents are initialized either with minimal ideological cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one comparisons with human replies. We evaluate three model families (Gemini, Mistral, and DeepSeek) across linguistic style, ideological consistency, and toxicity. We find that richer contextualization improves internal consistency but also amplifies polarization, stylized signals, and harmful language. We observe an emergent distortion that we call "generation exaggeration": a systematic amplification of salient traits beyond empirical baselines. Our analysis shows that LLMs do not emulate users, they reconstruct them. Their outputs, indeed, reflect internal optimization dynamics more than observed behavior, introducing structural biases that compromise their reliability as social proxies. This challenges their use in content moderation, deliberative simulations, and policy modeling.', 'abstract_zh': '我们研究大型语言模型在模拟社交媒体政治 discourse 时的行为。利用美国2024年总统选举期间X平台上2100万次互动数据，我们基于1186名真实用户构建了大型语言模型代理，在受控条件下促使它们回复政治性显著的推特。代理模型根据最小意识形态线索（零样本）或近期推特历史（少样本）初始化，允许与人类回复进行一对一比较。我们评估了三种模型家族（Gemini、Mistral 和 DeepSeek）在语言风格、意识形态一致性和有害言论方面的表现。研究发现，更丰富的上下文改善了内部一致性，但也加剧了极化、修辞信号和有害语言。我们观察到一种新兴的扭曲现象，称之为“生成夸张”：系统性地将显著特征放大至经验性基准之上。我们的分析表明，大型语言模型重构用户而非模拟用户。其输出实际上反映了内部优化动态而非观察到的行为模式，引入了结构偏差，损害了其作为社会代理的可靠性。这挑战了它们在内容审核、 deliberative 模拟和政策建模中的应用。', 'title_zh': 'LLM社会代理中的生成性夸大：一致性、偏差与毒性'}
{'arxiv_id': 'arXiv:2507.00653', 'title': 'Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models', 'authors': 'Yilun Zhang', 'link': 'https://arxiv.org/abs/2507.00653', 'abstract': "The escalating computational costs of Large Language Model (LLM) inference have become a critical barrier to their widespread and sustainable deployment. While existing optimization strategies are effective, they are predominantly based on statistical heuristics or architectural modifications, lacking a guiding cognitive theory to manage the inference process itself. This paper aims to bridge this gap by introducing a novel paradigm: the Cognitive Load-Aware Inference (CLAI) framework, which operationalizes principles from Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$, and $GCL_{LLM}$), thereby reframing the inference process as a cognitive economics optimization problem: based on the intrinsic complexity of a problem ($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a fine-tuned model that internalizes these principles for spontaneous cognitive economy. Across a range of benchmarks in complex reasoning, long-context question answering, and code generation, our methods achieve significant reductions in token consumption (up to 45\\%) without sacrificing accuracy. Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose difficult problems, a key characteristic of human expert cognition. This work demonstrates that by emulating the brain's resource management strategies, we can build more efficient, robust, and capable artificial intelligence systems.", 'abstract_zh': '认知负载感知推理：大型语言模型的认知经济学优化框架', 'title_zh': '认知负荷 Aware 推理：一种优化大型语言模型令牌经济的神经符号框架'}
{'arxiv_id': 'arXiv:2507.00606', 'title': 'Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies', 'authors': 'Tao Xiong, Xavier Hu, Wenyan Fan, Shengyu Zhang', 'link': 'https://arxiv.org/abs/2507.00606', 'abstract': 'Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised this http URL experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.', 'abstract_zh': '大规模语言模型（LLMs）通过_CHAIN-OF-THOUGHT_（CoT）和_TREE-OF-THOUGHT_（ToT）等高级提示技术在复杂任务中表现出色，但它们对手工制造的任务特定提示的依赖限制了其适应性和效率。我们提出了一种混合推理（MoR）训练框架，该框架将多种推理策略嵌入到LLMs中，实现无需外部提示工程的自主任务适应性推理。MoR包括两个阶段：推理想法生成，使用如GPT-4o等模型创建推理链模板，以及SFT数据集构建，将模板与基准数据集配对进行监督学习。实验结果显示，MoR显著提升了性能，其中MoR150在CoT提示下实现了0.730的分数（2.2%的改进），相较于基线模型提高了13.5%。MoR消除了对任务特定提示的需求，提供了跨多种任务的鲁棒推理的一般化解决方案。', 'title_zh': '混合推理方法：教导大规模语言模型采用适应性策略进行推理'}
{'arxiv_id': 'arXiv:2507.00579', 'title': 'TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification', 'authors': 'Miriam Anschütz, Ekaterina Gikalo, Niklas Herbster, Georg Groh', 'link': 'https://arxiv.org/abs/2507.00579', 'abstract': 'Hallucinations are one of the major problems of LLMs, hindering their trustworthiness and deployment to wider use cases. However, most of the research on hallucinations focuses on English data, neglecting the multilingual nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3 - Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. We propose a two-part pipeline that combines retrieval-based fact verification against Wikipedia with a BERT-based system fine-tuned to identify common hallucination patterns. Our system achieves competitive results across all languages, reaching top-10 results in eight languages, including English. Moreover, it supports multiple languages beyond the fourteen covered by the shared task. This multilingual hallucination identifier can help to improve LLM outputs and their usefulness in the future.', 'abstract_zh': 'LLMs中的幻觉是阻碍其可信度和广泛应用的主要问题，但现有大多数关于幻觉的研究集中于英语数据，忽视了LLMs的多语言性质。本文描述了我们对SemEval-2025 Task-3 - Mu-SHROOM（多语言共享任务：幻觉及相关过度生成错误）的提交情况。我们提出了一种两阶段的处理管道，结合了利用Wikipedia进行基于检索的事实验证和一个微调后的BERT系统，用于识别常见的幻觉模式。我们的系统在所有语言中均取得了竞争力的结果，其中在八种语言中达到前10名的成绩，包括英语。此外，该系统还支持超出共享任务涵盖的十四种语言的多种语言。这种多语言幻觉标识器有助于提高LLM输出的质量及其未来的实用性。', 'title_zh': 'TUM-MiKaNi在SemEval-2025任务3中的研究：迈向多语言和知识驱动的非事实幻觉识别'}
{'arxiv_id': 'arXiv:2507.00509', 'title': 'TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search', 'authors': 'To Eun Kim, João Coelho, Gbemileke Onilude, Jai Singh', 'link': 'https://arxiv.org/abs/2507.00509', 'abstract': 'As conversational search engines increasingly adopt generation-based paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), the integration of advertisements into generated responses presents both commercial opportunities and challenges for user experience. Unlike traditional search, where advertisements are clearly delineated, generative systems blur the boundary between informational content and promotional material, raising concerns around transparency and trust. In this work, we propose a modular pipeline for advertisement management in RAG-based conversational systems, consisting of an ad-rewriter for seamless ad integration and a robust ad-classifier for detection. We leverage synthetic data to train high-performing classifiers, which are then used to guide two complementary ad-integration strategies: supervised fine-tuning of the ad-rewriter and a best-of-N sampling approach that selects the least detectable ad-integrated response among multiple candidates. Our evaluation focuses on two core questions: the effectiveness of ad classifiers in detecting diverse ad integration strategies, and the training methods that best support coherent, minimally intrusive ad insertion. Experimental results show that our ad-classifier, trained on synthetic advertisement data inspired by marketing strategies and enhanced through curriculum learning, achieves robust detection performance. Additionally, we demonstrate that classifier-guided optimization, through both fine-tuning and best-of-N sampling, significantly improves ad stealth, enabling more seamless integration. These findings contribute an adversarial co-evolution framework for developing more sophisticated ad-aware generative search systems and robust ad classifiers.', 'abstract_zh': '基于检索增强生成的对话系统中广告管理的模块化管道：透明与信任的平衡', 'title_zh': 'TeamCMU在Touché上的对抗共进化广告整合与检测研究：基于对话搜索环境'}
{'arxiv_id': 'arXiv:2507.00418', 'title': 'Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs', 'authors': 'Mohammad Firas Sada, John J. Graham, Elham E Khoda, Mahidhar Tatineni, Dmitry Mishin, Rajesh K. Gupta, Rick Wagner, Larry Smarr, Thomas A. DeFanti, Frank Würthwein', 'link': 'https://arxiv.org/abs/2507.00418', 'abstract': 'This study presents a benchmarking analysis of the Qualcomm Cloud AI 100 Ultra (QAic) accelerator for large language model (LLM) inference, evaluating its energy efficiency (throughput per watt) and performance against leading NVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform (NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90 billion parameters, are served using the vLLM framework. The QAic inference cards appears to be energy efficient and performs well in the energy efficiency metric in most cases. The findings offer insights into the potential of the Qualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications within the National Research Platform (NRP).', 'abstract_zh': '本研究对Qualcomm Cloud AI 100 Ultra (QAic) 加速器在大型语言模型（LLM）推理中的基准分析，评估了其能效（每瓦吞吐量）和在National Research Platform (NRP) 生态系统中与领先NVIDIA (A100, H200) 和AMD (MI300A) GPU的竞争性能。使用vLLM框架总共服务了15个开源LLM，参数范围从1.17亿到900亿。在多数情况下，QAic 推理卡在能效指标中表现出色。研究结果为Qualcomm Cloud AI 100 Ultra在National Research Platform (NRP) 中高性能计算（HPC）应用的潜力提供了见解。', 'title_zh': '在HPC集群中服务LLMs：Qualcomm Cloud AI 100 Ultra与高性能GPU的比较研究'}
{'arxiv_id': 'arXiv:2507.00378', 'title': 'iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing', 'authors': 'Xikai Sun, Fan Dang, Kebin Liu, Xin Miao, Zihao Yang, Haimo Lu, Yawen Zheng, Yunhao Liu', 'link': 'https://arxiv.org/abs/2507.00378', 'abstract': 'Conformance testing is essential for ensuring that protocol implementations comply with their specifications. However, traditional testing approaches involve manually creating numerous test cases and scripts, making the process labor-intensive and inefficient. Recently, Large Language Models (LLMs) have demonstrated impressive text comprehension and code generation abilities, providing promising opportunities for automation. In this paper, we propose iPanda, the first end-to-end framework that leverages LLMs to automate protocol conformance testing. Given a protocol specification document and its implementation, iPanda first employs a keyword-based method to automatically generate comprehensive test cases. Then, it utilizes a code-based retrieval-augmented generation approach to effectively interpret the implementation and produce executable test code. To further enhance code quality, iPanda incorporates an iterative self-correction mechanism to refine generated test scripts interactively. Finally, by executing and analyzing the generated tests, iPanda systematically verifies compliance between implementations and protocol specifications. Comprehensive experiments on various protocols show that iPanda significantly outperforms pure LLM-based approaches, improving the success rate (Pass@1) of test-code generation by factors ranging from 4.675 times to 10.751 times.', 'abstract_zh': '基于大规模语言模型的端到端协议一致性测试框架 iPanda', 'title_zh': 'iPanda: 一种用于符合性测试的智能协议测试与调试代理'}
{'arxiv_id': 'arXiv:2507.00352', 'title': 'An AST-guided LLM Approach for SVRF Code Synthesis', 'authors': 'Abanoub E. Abdelmalak, Mohamed A. Elsayed, David Abercrombie, Ilhami Torunoglu', 'link': 'https://arxiv.org/abs/2507.00352', 'abstract': 'Standard Verification Rule Format (SVRF) is essential for semiconductor applications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and Optical Proximity Correction (OPC) and it faces challenges as advancing nodes create complex design rules that renders traditional SVRF development ineffective and highlight an expertise gap. This paper introduces a novel methodology integrating Abstract Syntax Tree (AST) embedding and Retrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring semantic accuracy and error minimization through structural validation with domain-specific insights for precise code generation.\nWe evaluate different T5-based models and propose an innovative SVRF-specific scoring framework that complements standard metrics like BLEU and ROUGE-L. In our approach, AST provides rigorous structural validation, while RAG infuses relevant domain knowledge, effectively enhancing the code generation workflow.\nTesting on a comprehensive benchmark of 740 DRC rule implementations, our methodology demonstrates up to a 40\\% improvement in code generation accuracy compared to basic text-based fine-tuning process. This fusion of industry expertise with advanced coding strategies not only optimizes SVRF development under limited dataset constraints but also creates a more intuitive and efficient coding environment. Consequently, users can rapidly iterate through design cycles, reduce manual error correction, and significantly improve overall productivity.', 'abstract_zh': '一种基于Abstract Syntax Tree嵌入和Retrieval-Augmented Generation的Standard Verification Rule Format代码合成新方法', 'title_zh': 'AST引导的大语言模型方法用于SVRF代码合成'}
{'arxiv_id': 'arXiv:2507.00322', 'title': 'Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones', 'authors': 'Daking Rai, Samuel Miller, Kevin Moran, Ziyu Yao', 'link': 'https://arxiv.org/abs/2507.00322', 'abstract': 'Despite remarkable advances in coding capabilities, language models (LMs) still struggle with simple syntactic tasks such as generating balanced parentheses. In this study, we investigate the underlying mechanisms behind the persistence of these errors across LMs of varying sizes (124M-7B) to both understand and mitigate the errors. Our study reveals that LMs rely on a number of components (attention heads and FF neurons) that independently make their own predictions. While some components reliably promote correct answers across a generalized range of inputs (i.e., implementing "sound mechanisms\'\'), others are less reliable and introduce noise by promoting incorrect tokens (i.e., implementing "faulty mechanisms\'\'). Errors occur when the faulty mechanisms overshadow the sound ones and dominantly affect the predictions. Motivated by this insight, we introduce RASteer, a steering method to systematically identify and increase the contribution of reliable components for improving model performance. RASteer substantially improves performance on balanced parentheses tasks, boosting accuracy of some models from $0$% to around $100$% without impairing the models\' general coding ability. We further demonstrate its broader applicability in arithmetic reasoning tasks, achieving performance gains of up to around $20$%.', 'abstract_zh': '尽管编码能力取得了显著进步，语言模型（LMs）仍然在生成匹配括号等简单的句法任务上遇到困难。本研究探讨了不同规模（124M-7B）LMs中持续存在的错误背后的原因，旨在理解并减轻这些错误。研究发现，LMs依赖于多个独立预测的不同组件（注意力头和前馈神经元），其中一些组件在广泛输入范围内可靠地促进正确答案（即实现“健全机制”），而另一些则不够可靠，通过促进错误标记引入噪声（即实现“故障机制”）。错误发生时，故障机制会压倒健全机制，并主导预测结果。基于这一洞察，我们提出了RASteer steering方法，以系统地识别并增加可靠组件的贡献，以提高模型性能。RASteer在平衡括号任务上的性能大幅提升，某些模型的准确性从0%提升至约100%而不会损害模型的一般编程能力。此外，我们还展示了其在算术推理任务上的更广泛适用性，实现了最高约20%的性能提升。', 'title_zh': '干涉导致的失败：当故障机制覆盖了有效的机制时，语言模型会在匹配括号上出错。'}
{'arxiv_id': 'arXiv:2507.00275', 'title': 'Double Q-learning for Value-based Deep Reinforcement Learning, Revisited', 'authors': 'Prabhat Nagarajan, Martha White, Marlos C. Machado', 'link': 'https://arxiv.org/abs/2507.00275', 'abstract': "Overestimation is pervasive in reinforcement learning (RL), including in Q-learning, which forms the algorithmic basis for many value-based deep RL algorithms. Double Q-learning is an algorithm introduced to address Q-learning's overestimation by training two Q-functions and using both to de-correlate action-selection and action-evaluation in bootstrap targets. Shortly after Q-learning was adapted to deep RL in the form of deep Q-networks (DQN), Double Q-learning was adapted to deep RL in the form of Double DQN. However, Double DQN only loosely adapts Double Q-learning, forgoing the training of two different Q-functions that bootstrap off one another. In this paper, we study algorithms that adapt this core idea of Double Q-learning for value-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our aim is to understand whether DDQL exhibits less overestimation than Double DQN and whether performant instantiations of DDQL exist. We answer both questions affirmatively, demonstrating that DDQL reduces overestimation and outperforms Double DQN in aggregate across 57 Atari 2600 games, without requiring additional hyperparameters. We also study several aspects of DDQL, including its network architecture, replay ratio, and minibatch sampling strategy.", 'abstract_zh': '过度估计在强化学习中的普遍存在：从Q-learning到Deep Double Q-learning的研究', 'title_zh': '基于价值的深度强化学习中双Q学习的 revisit'}
{'arxiv_id': 'arXiv:2507.00258', 'title': 'Impact of Fine-Tuning Methods on Memorization in Large Language Models', 'authors': 'Jie Hou, Chuxiong Wu, Lannan Luo, Qiang Zeng', 'link': 'https://arxiv.org/abs/2507.00258', 'abstract': 'As the capabilities of pre-trained large language models (LLMs) continue to advance, the "pre-train and fine-tune" paradigm has become increasingly mainstream, leading to the development of various fine-tuning methods. However, the privacy risks arising from memorization during fine-tuning have received relatively little attention. To address this gap, we categorize popular fine-tuning approaches and assess their impact on memorization through the lens of membership inference attacks (MIAs). Our results show that, compared to parameter-based fine-tuning, prompt-based fine-tuning achieves competitive performance while exhibiting lower vulnerability to MIAs. Furthermore, prompt-based methods maintain low memorization regardless of model scale. These findings suggest that parameter-based fine-tuning is more prone to leaking private information, whereas prompt-based fine-tuning serves as a more privacy-preserving option.', 'abstract_zh': '随着预训练大型语言模型（LLMs）能力的不断进步，“预训练和微调”范式日益 mainstream，推动了各种微调方法的发展。然而，在微调过程中出现的隐私风险相对较少受到关注。为了填补这一空白，我们对流行的微调方法进行了分类，并通过成员推断攻击（MIAs）的视角评估它们对记忆化的影响。研究结果表明，与基于参数的微调相比，基于提示的微调在性能上具有竞争力，同时对MIAs的易感性较低。此外，基于提示的方法在模型规模变化时仍能保持低记忆化水平。这表明基于参数的微调更容易泄露私人信息，而基于提示的微调具有更好的隐私保护特性。', 'title_zh': '大型语言模型中微调方法对记忆影响的研究'}
{'arxiv_id': 'arXiv:2507.00239', 'title': 'Linearly Decoding Refused Knowledge in Aligned Language Models', 'authors': 'Aryan Shrivastava, Ari Holtzman', 'link': 'https://arxiv.org/abs/2507.00239', 'abstract': 'Most commonly used language models (LMs) are instruction-tuned and aligned using a combination of fine-tuning and reinforcement learning, causing them to refuse users requests deemed harmful by the model. However, jailbreak prompts can often bypass these refusal mechanisms and elicit harmful responses. In this work, we study the extent to which information accessed via jailbreak prompts is decodable using linear probes trained on LM hidden states. We show that a great deal of initially refused information is linearly decodable. For example, across models, the response of a jailbroken LM for the average IQ of a country can be predicted by a linear probe with Pearson correlations exceeding $0.8$. Surprisingly, we find that probes trained on base models (which do not refuse) sometimes transfer to their instruction-tuned versions and are capable of revealing information that jailbreaks decode generatively, suggesting that the internal representations of many refused properties persist from base LMs through instruction-tuning. Importantly, we show that this information is not merely "leftover" in instruction-tuned models, but is actively used by them: we find that probe-predicted values correlate with LM generated pairwise comparisons, indicating that the information decoded by our probes align with suppressed generative behavior that may be expressed more subtly in other downstream tasks. Overall, our results suggest that instruction-tuning does not wholly eliminate or even relocate harmful information in representation space-they merely suppress its direct expression, leaving it both linearly accessible and indirectly influential in downstream behavior.', 'abstract_zh': '通过 Jailbreak 提示访问的信息在语言模型中的可解码性研究', 'title_zh': '在对齐语言模型中线性解码拒绝的知识'}
{'arxiv_id': 'arXiv:2507.00214', 'title': 'Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning', 'authors': 'Mads Henrichsen, Rasmus Krebs', 'link': 'https://arxiv.org/abs/2507.00214', 'abstract': 'Standard classification models often map inputs directly to labels without explicit reasoning, potentially limiting their performance, robustness, and interpretability. This paper introduces a novel two-stage approach to enhance text classification by leveraging Large Language Model (LLM)-generated reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model (henceforth Llama-R-Gen) on a general-purpose reasoning dataset (syvai/reasoning-gen) to generate textual reasoning (R) given a question and its answer. In the second stage, this generally trained Llama-R-Gen is used offline to create an augmented training dataset for a downstream generative model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the input text (Q) and is trained to output the generated reasoning (R) immediately followed by the predicted emotion (A). We demonstrate this methodology on the dair-ai/emotion dataset for emotion classification. Our experiments show that the generative model trained to output reasoning and the emotion (Classifier Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy (for emotion prediction) compared to a baseline generative model trained solely to output the emotion (Classifier Q->A), highlighting the strong generalization capabilities of the reasoning generation and the benefit of explicit reasoning training. This work underscores the potential of LLM-generated reasonings for creating richer training datasets, thereby improving the performance of diverse downstream NLP tasks and providing explicit explanations.', 'abstract_zh': '标准分类模型常常直接将输入映射到标签，而未进行明确推理，这可能会限制其性能、鲁棒性和可解释性。本文提出了一种新的两阶段方法，通过利用大型语言模型（LLM）生成的推理来增强文本分类。在第一阶段，我们在一个通用推理数据集（syvai/reasoning-gen）上微调Llama-3.2-1B-Instruct模型（以下简称Llama-R-Gen），以生成给定问题及其答案的文本推理（R）。在第二阶段，这种通用训练的Llama-R-Gen被离线使用，为下游生成模型创建扩充训练数据集。基于Llama-3.2-1B-Instruct的下游模型仅接受输入文本（Q），并训练为输出生成的推理（R）后紧跟预测的情绪（A）。我们在dair-ai/emotion数据集上对情绪分类进行了此项方法的演示。实验结果表明，用于输出推理和情绪（Classifier Q->RA）的生成模型相比仅用于输出情绪的基线生成模型（Classifier Q->A），情绪预测的准确率提高了8.7个百分点，突显了推理生成的强泛化能力和显式推理训练的益处。这项工作强调了LLM生成推理在创建更丰富的训练数据集方面的潜力，从而改善了多种下游NLP任务的性能，并提供了明确的解释。', 'title_zh': '两阶段推理注入学习：借助LLM生成的推理改进分类'}
{'arxiv_id': 'arXiv:2507.00088', 'title': 'How large language models judge and influence human cooperation', 'authors': 'Alexandre S. Pires, Laurens Samson, Sennay Ghebreab, Fernando P. Santos', 'link': 'https://arxiv.org/abs/2507.00088', 'abstract': "Humans increasingly rely on large language models (LLMs) to support decisions in social settings. Previous work suggests that such tools shape people's moral and political judgements. However, the long-term implications of LLM-based social decision-making remain unknown. How will human cooperation be affected when the assessment of social interactions relies on language models? This is a pressing question, as human cooperation is often driven by indirect reciprocity, reputations, and the capacity to judge interactions of others. Here, we assess how state-of-the-art LLMs judge cooperative actions. We provide 21 different LLMs with an extensive set of examples where individuals cooperate -- or refuse cooperating -- in a range of social contexts, and ask how these interactions should be judged. Furthermore, through an evolutionary game-theoretical model, we evaluate cooperation dynamics in populations where the extracted LLM-driven judgements prevail, assessing the long-term impact of LLMs on human prosociality. We observe a remarkable agreement in evaluating cooperation against good opponents. On the other hand, we notice within- and between-model variance when judging cooperation with ill-reputed individuals. We show that the differences revealed between models can significantly impact the prevalence of cooperation. Finally, we test prompts to steer LLM norms, showing that such interventions can shape LLM judgements, particularly through goal-oriented prompts. Our research connects LLM-based advices and long-term social dynamics, and highlights the need to carefully align LLM norms in order to preserve human cooperation.", 'abstract_zh': '人类越来越多地依赖大型语言模型（LLMs）在社会环境中支持决策。先前的研究表明，这类工具影响人们的道德和政治判断。然而，基于LLM的社会决策长期影响尚不明了。当社会互动的评估依赖于语言模型时，人类合作将如何受到影响？这是一个紧迫的问题，因为人类合作往往受到间接 reciprocity、声誉以及评价他人互动能力的驱动。在此，我们评估了最新一代LLM对合作行为的评价。我们向21种不同的LLM提供了涵盖各种社会情境中个体合作或拒绝合作的大量范例，并询问这些互动应如何评价。此外，通过进化博弈论模型，我们评估了以这类LLM驱动的评价为主导的人群中的合作动态，考察LLM对人类利他性长期影响。我们发现，在评价与良好对手的合作时，LLM之间表现出显著的一致性。另一方面，在评价与有不良声誉的个体的合作时，我们注意到LLM之间存在差异。显示了这些差异可以显著影响合作的普遍程度。最后，我们测试了提示词以引导LLM规范，表明此类干预可以通过目标导向的提示词影响LLM的评价。我们的研究将LLM建议与长期社会动态联系起来，并强调了谨慎对齐LLM规范的必要性，以维护人类合作。', 'title_zh': '大型语言模型对人类合作的判断与影响'}
{'arxiv_id': 'arXiv:2507.00082', 'title': 'Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission', 'authors': 'Faranaksadat Solat, Joohyung Lee, Mohamed Seif, Dusit Niyato, H. Vincent Poor', 'link': 'https://arxiv.org/abs/2507.00082', 'abstract': "Hybrid Language Models (HLMs) combine the low-latency efficiency of Small Language Models (SLMs) on edge devices with the high accuracy of Large Language Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM inference, HLMs reduce latency and communication by invoking LLMs only when local SLM predictions are uncertain, i.e., when token-level confidence is low or entropy is high. However, ambiguous or low-confidence predictions still require frequent offloading to the LLM, leading to significant communication overhead in bandwidth-constrained settings. To address this, we propose FedHLM, a communication-efficient HLM framework that integrates uncertainty-aware inference with Federated Learning (FL). FedHLM's key innovation lies in collaboratively learning token-level uncertainty thresholds that govern when LLM assistance is needed. Rather than using static or manually tuned thresholds, FedHLM employs FL to optimize these thresholds in a privacy-preserving, distributed manner. Additionally, it leverages embedding-based token representations for Peer-to-Peer (P2P) resolution, enabling clients to reuse tokens inferred by semantically similar peers without engaging the LLM. We further introduce hierarchical model aggregation: edge servers refine local routing policies through client updates, while cross-cluster coordination aligns global decision boundaries. This layered design captures recurring uncertainty patterns, reducing redundant LLM queries. Experiments on large-scale news classification tasks show that FedHLM reduces LLM transmissions by over 95 percent with negligible accuracy loss, making it well-suited for scalable and efficient edge-AI applications.", 'abstract_zh': '混合语言模型（HLMs）结合了边缘设备上小型语言模型（SLMs）的低延迟高效性和中央服务器上大型语言模型（LLMs）的高准确性。不同于传统的端到端LLM推断，HLMs通过仅在局部SLM预测不确定时（即，当Token级置信度低或熵高时）调用LLM来减少延迟和通信。然而，含糊不清或低信心的预测仍然需要频繁地卸载到LLM，导致在带宽受限的环境中产生显著的通信开销。为了解决这一问题，我们提出了一种通信高效的HLM框架FedHLM，该框架将不确定性感知推断与联邦学习（FL）相结合。FedHLM的关键创新在于协作学习用于决定何时需要LLM协助的Token级不确定性阈值。FedHLM不使用静态或手动调整的阈值，而是通过Privacy-Preserving、分布式方式利用FL优化这些阈值。此外，FedHLM利用基于嵌入的Token表示进行Peer-to-Peer（P2P）解决，使客户端能够重复使用具有语义相似性的Peer推断出的Token，而不需调用LLM。我们进一步引入了层级模型聚合：边缘服务器通过客户端更新改进局部路由策略，而跨集群协调则统一全球决策边界。这种分层设计捕获了重复出现的不确定性模式，减少了冗余的LLM查询。大规模新闻分类任务的实验结果显示，FedHLM在几乎无准确率损失的情况下将LLM传输减少了95%以上，使其非常适合可扩展且高效的边缘AI应用。', 'title_zh': '基于联邦学习的高效令牌传输混合语言模型'}
{'arxiv_id': 'arXiv:2507.00081', 'title': 'State and Memory is All You Need for Robust and Reliable AI Agents', 'authors': 'Matthew Muhoberac, Atharva Parikh, Nirvi Vakharia, Saniya Virani, Aco Radujevic, Savannah Wood, Meghav Verma, Dimitri Metaxotos, Jeyaraman Soundararajan, Thierry Masquelin, Alexander G. Godfrey, Sean Gardner, Dobrila Rudnicki, Sam Michael, Gaurav Chopra', 'link': 'https://arxiv.org/abs/2507.00081', 'abstract': 'Large language models (LLMs) have enabled powerful advances in natural language understanding and generation. Yet their application to complex, real-world scientific workflows remain limited by challenges in memory, planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke Artificial Intelligence Agents Optimized for Research Goals), a modular agentic framework that allows LLM-based agents to autonomously plan, reason, and achieve robust and reliable domain-specific task execution. Agents are constructed dynamically from source code documentation and augmented with finite-state automata (FSA) memory, enabling persistent state tracking and context-aware decision-making. This approach eliminates the need for manual prompt engineering and allows for robust, scalable deployment across diverse applications via maintaining context across extended workflows and to recover from tool or execution failures. We validate SciBORG through integration with both physical and virtual hardware, such as microwave synthesizers for executing user-specified reactions, with context-aware decision making and demonstrate its use in autonomous multi-step bioassay retrieval from the PubChem database utilizing multi-step planning, reasoning, agent-to-agent communication and coordination for execution of exploratory tasks. Systematic benchmarking shows that SciBORG agents achieve reliable execution, adaptive planning, and interpretable state transitions. Our results show that memory and state awareness are critical enablers of agentic planning and reliability, offering a generalizable foundation for deploying AI agents in complex environments.', 'abstract_zh': '大规模语言模型（LLMs）已在自然语言理解和生成方面取得了强大的进展。然而，它们在复杂的真实世界科学工作流中的应用受限于内存、规划和工具集成的挑战。在这里，我们介绍了SciBORG（专为研究目标优化的科学定制人工智能代理），这是一种模块化代理框架，允许基于LLM的代理自主规划、推理和执行稳健可靠的领域特定任务。代理动态从源代码文档构建，并通过有限状态自动机（FSA）记忆增强，以实现持久状态跟踪和上下文相关的决策制定。这种方法消除了手动提示工程的需求，并通过在整个扩展工作流中保持上下文以及从工具或执行故障中恢复，实现了稳健且可扩展的应用部署。我们通过将SciBORG与物理和虚拟硬件（如用于执行用户指定反应的微波合成器）集成，并通过上下文相关的决策制定进行验证，并展示了其在利用多步计划、推理、代理间通信和协调执行探索性任务时，从PubChem数据库自主多步生物测定检索中的应用。系统基准测试表明，SciBORG代理实现了可靠执行、适应性规划和可解释的状态转换。我们的结果表明，记忆和状态意识是代理规划和可靠性实现的关键因素，为在复杂环境中部署AI代理提供了可泛化的基础。', 'title_zh': 'State and Memory Are All You Need for Robust and Reliable AI Agents'}
{'arxiv_id': 'arXiv:2507.00078', 'title': 'The language of time: a language model perspective on time-series foundation models', 'authors': 'Yi Xie, Yun Xiong, Zejian Shi, Hao Niu, Zhengfu Liu', 'link': 'https://arxiv.org/abs/2507.00078', 'abstract': "With the rise of large language models, the paradigm of training foundation models with massive parameter counts on vast datasets has been adopted in multiple domains to achieve remarkable success. Time series foundation models represent a significant extension of this paradigm, demonstrating exceptional expressive power, generalization, and cross-domain transferability. However, this gives rise to a fundamental paradox: time series data reflect distinct dynamical systems, making cross-domain transfer intuitively implausible, yet this is contradicted by the models' empirical success. To resolve this paradox, this paper investigates, from both theoretical and experimental perspectives, the representation learning mechanisms and generalization capabilities of patch-based time series foundation models. We argue that such models are not merely applying a new architecture but are fundamentally generalizing the representation paradigm of language models by extending deterministic vector-based representations to latent probabilistic distributional forms. Our theoretical analysis supports this framework by demonstrating that continuous time-series patches can be faithfully quantized into a discrete vocabulary whose key statistical properties are highly consistent with those of natural language. This generalization allows time series models to inherit the robust representation and transfer abilities of large language models, thereby explaining their superior performance in temporal tasks. Ultimately, our work provides a rigorous theoretical cornerstone for understanding, evaluating, and improving the safety and reliability of large-scale time series foundation models.", 'abstract_zh': '大规模语言模型兴起背景下时间序列基础模型的表示学习机制与泛化能力探究', 'title_zh': '时间的语言：时间序列基础模型的语言模型视角'}
{'arxiv_id': 'arXiv:2507.00075', 'title': 'Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap', 'authors': 'Yifan Sun, Yushan Liang, Zhen Zhang, Jiaye Teng', 'link': 'https://arxiv.org/abs/2507.00075', 'abstract': "Self-improvement is among the most prominent techniques within the realm of large language models (LLM), aiming to enhance the LLM performance without relying on external data. Despite its significance, generally how LLM performances evolve during the self-improvement process remains underexplored. In this paper, we theoretically model the training dynamics of self-improvement via the concept of solver-verifier gap. This is inspired by the conjecture that the performance enhancement of self-improvement stems from the gap between LLM's solver capability and verifier capability. Based on the theoretical framework, we further introduce how to predict the ultimate power of self-improvement using only information from the first few training epochs. We empirically validate the effectiveness of the theoretical model on various LLMs and datasets. Beyond self-improvement, we extend our analysis to investigate how external data influences these dynamics within the framework. Notably, we find that under limited external data regimes, such external data can be utilized at any stage without significantly affecting final performances, which accords with the empirical observations.", 'abstract_zh': '自改进是大型语言模型（LLM）领域最显著的技术之一，旨在不依赖外部数据的情况下提升LLM性能。尽管自改进对LLM性能的影响至关重要，但自改进过程中LLM性能如何演变仍缺乏探索。在本文中，我们通过引入求解器-验证者差距的概念，理论上建模了自改进的训练动力学。基于这一理论框架，我们进一步探讨如何仅使用前几轮训练_epoch_的信息来预测自改进的最终效果。我们通过在多种LLM和数据集上进行实证验证，验证了该理论模型的有效性。除了自改进外，我们还扩展分析了外部数据如何在该框架内影响这些动力学。值得注意的是，我们的研究发现，在外部数据有限的情况下，外部数据可以在任何阶段被利用而不会显著影响最终性能，这一发现与实证观察相符。', 'title_zh': '通过求解器-验证器差距构建的大语言模型自我提升训练动力学的理论模型'}
{'arxiv_id': 'arXiv:2507.00057', 'title': 'Estimating Correctness Without Oracles in LLM-Based Code Generation', 'authors': 'Thomas Valentin, Ardi Madadi, Gaetano Sapia, Marcel Böhme', 'link': 'https://arxiv.org/abs/2507.00057', 'abstract': 'Generating code from natural language specifications is one of the most successful applications of Large Language Models (LLMs). Yet, they hallucinate: LLMs produce outputs that may be grammatically correct but are factually incorrect. Without an existing, correct implementation (i.e., an oracle), can we quantify how likely the generated program is correct?\nIn this paper, we propose a measure of incorrectness, called incoherence, that can be estimated efficiently in the absence of an oracle and provides a lower bound on the error, i.e., the probability that the LLM-generated program for that specification is incorrect. Our experiments demonstrate an extraordinary effectiveness. For the average code generation task, our incoherence-based methodology can automatically identify about two-thirds of incorrect programs without reports of false positives. In fact, an oracle-based evaluation of LLMs can be reliably replaced by an incoherence-based evaluation. In particular, we find a very strong agreement between the ranking of LLMs by the number of programs deemed correct via an oracle (pass@1) and the ranking of LLMs by the number of programs deemed correct via our incoherence.', 'abstract_zh': '从自然语言规范生成代码是大型语言模型（LLMs）最成功的应用之一。然而，它们会产生幻觉：即生成的输出可能是语法正确的但却是事实错误的。没有现有正确实现（即 oracle）的情况下，我们能否量化生成的程序正确性的概率？\n在本文中，我们提出了一种称为不一致性的度量方法，可以在没有 oracle 的情况下高效估算，并提供错误的概率下界，即 LLM 生成的程序对于该规范是错误的概率。我们的实验表明了该方法的非凡效果。对于平均的代码生成任务，我们的基于不一致性的方法可以自动识别大约三分之二的错误程序，并且不会出现误报。事实上，使用 oracle 评估 LLM 可以可靠地被基于不一致性的评估所替代。特别是，我们发现通过 oracle 认定的正确程序数量对 LLM 进行排名（pass@1）与通过我们的不一致性认定的正确程序数量对 LLM 进行排名之间存在非常强的一致性。', 'title_zh': '基于大型语言模型的代码生成中无需或acles估计正确性'}
{'arxiv_id': 'arXiv:2507.00045', 'title': 'CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning', 'authors': 'Ming Li, Chenguang Wang, Yijun Liang, Xiyao Wang, Yuhang Zhou, Xiyang Wu, Yuqing Zhang, Ruiyi Zhang, Tianyi Zhou', 'link': 'https://arxiv.org/abs/2507.00045', 'abstract': "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have achieved near-ceiling scores on various existing benchmarks, motivating a demand for more challenging test tasks. These MLLMs have been reported to excel in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their potential as a detective who can notice minuscule cues in an image and weave them into coherent, situational explanations, leading to a reliable answer. But can they match the performance of excellent human detectives? To answer this question, we investigate some hard scenarios where GPT-o3 can still handle, and find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating. It is inspired by the social media requests that ask others to detect suspicious clues from photos shared by the poster's partner. We conduct extensive experiments and analysis to understand why existing MLLMs lack sufficient capability to solve this kind of task. CaughtCheating provides a class of challenging visual perception and reasoning tasks with great value and practical usage. Success in these tasks paves the way for MLLMs to acquire human-level detective perception and reasoning capabilities.", 'abstract_zh': '近期代理多模态大型语言模型（MLLMs）如GPT-o3已在多种现有基准上取得了接近天花板的分数，推动了对更具挑战性测试任务的需求。这些MLLMs在少数人类专家级任务上表现出色，例如GeoGuesser，反映出它们作为侦探的潜力，能够注意到图像中的微小线索并将其编织成连贯的情境解释，从而得出可靠的答案。但它们能否达到优秀人类侦探的性能？为了回答这个问题，我们调查了GPT-o3仍能够处理的一些困难场景，并发现了一个常见场景，其中o3的表现几乎降至零，我们将其命名为CaughtCheating。这一灵感来源于社交媒体上的请求，要求他人检测上传者合作伙伴共享的照片中的可疑线索。我们进行了广泛的实验和分析，以了解现有MLLMs为何缺乏解决此类任务所需的能力。CaughtCheating提供了一类具有重要价值和实际用途的视觉感知和推理挑战任务。在这些任务中的成功为MLLMs获取人类级侦探感知和推理能力铺平了道路。', 'title_zh': 'Caught Cheating: 是你的大规模语言模型擅长检测作弊吗？探索视觉感知与推理的边界'}
{'arxiv_id': 'arXiv:2507.00033', 'title': 'Moment Sampling in Video LLMs for Long-Form Video QA', 'authors': 'Mustafa Chasmai, Gauri Jagatap, Gouthaman KV, Grant Van Horn, Subhransu Maji, Andrea Fanelli', 'link': 'https://arxiv.org/abs/2507.00033', 'abstract': 'Recent advancements in video large language models (Video LLMs) have significantly advanced the field of video question answering (VideoQA). While existing methods perform well on short videos, they often struggle with long-range reasoning in longer videos. To scale Video LLMs for longer video content, frame sub-sampling (selecting frames at regular intervals) is commonly used. However, this approach is suboptimal, often leading to the loss of crucial frames or the inclusion of redundant information from multiple similar frames. Missing key frames impairs the model\'s ability to answer questions accurately, while redundant frames lead the model to focus on irrelevant video segments and increase computational resource consumption. In this paper, we investigate the use of a general-purpose text-to-video moment retrieval model to guide the frame sampling process. We propose "moment sampling", a novel, model-agnostic approach that enables the model to select the most relevant frames according to the context of the question. Specifically, we employ a lightweight moment retrieval model to prioritize frame selection. By focusing on the frames most pertinent to the given question, our method enhances long-form VideoQA performance in Video LLMs. Through extensive experiments on four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we demonstrate the effectiveness of the proposed approach.', 'abstract_zh': '近期视频大规模语言模型（Video LLMs）的发展显著推动了视频问答（VideoQA）领域的进步。尽管现有方法在短视频上表现良好，但它们往往难以处理长视频中的长程推理。为了扩展Video LLMs以应对更长的视频内容，常用的帧下采样方法（即以固定间隔选择帧）虽然实用，但并不理想，常常导致关键帧的丢失或包含多个相似帧的冗余信息。丢失关键帧会削弱模型准确回答问题的能力，而冗余帧则会让模型关注无关的视频片段，并增加计算资源消耗。在本文中，我们探讨了使用通用文本到视频时刻检索模型来指导帧采样过程。我们提出了一种名为“时刻采样”的新型、模型无关的方法，使模型能够根据问题的上下文选择最相关的帧。具体而言，我们采用轻量级的时刻检索模型来优先选择帧。通过专注于与给定问题最相关的帧，我们的方法增强了Video LLMs的长视频问答性能。通过在四个长视频问答数据集上使用四种最先进的Video LLMs进行广泛实验，我们验证了所提出方法的有效性。', 'title_zh': '视频LLMs中用于长视频QA的时刻采样方法'}
{'arxiv_id': 'arXiv:2507.00029', 'title': 'LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing', 'authors': 'Wenbing Li, Zikai Song, Hang Zhou, Yunyao Zhang, Junqing Yu, Wei Yang', 'link': 'https://arxiv.org/abs/2507.00029', 'abstract': "Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts (MoE) for adapting large language models (LLMs) to multiple tasks still exhibit prevailing limitations: they either swap entire attention/feed-forward layers for switch experts or bolt on parallel expert branches, diluting parameter efficiency and task fidelity. We propose the LoRA-Mixer, a modular and lightweight MoE framework that integrates LoRA experts. Our core innovation lies in replacing the projection matrices of the attention module's input/output linear layers with dynamically routed, task-specific LoRA experts. This design ensures seamless compatibility with diverse foundation models, including transformers and state space models (SSMs), by leveraging their inherent linear projection structures. The framework supports two operational paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a novel hard-soft routing strategy, or (2) direct deployment of pre-trained, frozen LoRA modules sourced from external repositories. To enable robust router training with limited data while ensuring stable routing decisions and maximizing expert reuse, we introduce an adaptive Specialization Balance Loss (SBL) that jointly optimizes expert balance and task-specific alignment. Extensive experiments on seven benchmark datasets, including MedQA, CoLA, SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base models, respectively. Compared with state-of-the-art methods, LoRA-Mixer achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively, using only 48% of the parameters, demonstrating its efficiency and strong performance.", 'abstract_zh': 'Recent Efforts to Combine Low-Rank Adaptation (LoRA) with Mixture-of-Experts (MoE) for Adapting Large Language Models (LLMs) to Multiple Tasks Still Exhibit Prevailing Limitations: Introducing LoRA-Mixer, a Modular and Lightweight MoE Framework', 'title_zh': 'LoRA-Mixer: 通过串联注意力路由协调模块化LoRA专家'}
{'arxiv_id': 'arXiv:2507.00026', 'title': 'ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models', 'authors': 'Jiale Ding, Xiang Zheng, Cong Wang, Wei-Bin Lee, Xingjun Ma, Yu-Gang Jiang', 'link': 'https://arxiv.org/abs/2507.00026', 'abstract': 'As Large Language Models (LLMs) are increasingly deployed as black-box components in real-world applications, evaluating their safety-especially under adversarial prompting-has become critical. Arguably, effective safety evaluations should be adaptive, evolving with LLM capabilities, and also cover a broad spectrum of harmful topics and real-world scenarios to fully expose potential vulnerabilities. Existing manual safety benchmarks, built on handcrafted adversarial prompts, are limited by their static nature and the intensive labor required to update them, making it difficult to keep pace with rapidly advancing LLMs. In contrast, automated adversarial prompt generation offers a promising path toward adaptive evaluation. However, current methods often suffer from insufficient adversarial topic coverage (topic-level diversity) and weak alignment with real-world contexts. These shortcomings stem from the exploration-exploitation dilemma in black-box optimization and a lack of real-world contextualization, resulting in adversarial prompts that are both topically narrow and scenario-repetitive. To address these issues, we propose Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses multi-objective reinforcement learning to fine-tune an adversarial LLM for generating topically diverse and contextually rich adversarial prompts. Experiments show that ROSE outperforms existing methods in uncovering safety vulnerabilities in state-of-the-art LLMs, with notable improvements in integrated evaluation metrics. We hope ROSE represents a step toward more practical and reality-oriented safety evaluation of LLMs. WARNING: This paper contains examples of potentially harmful text.', 'abstract_zh': '基于现实导向的安全评估：用于生成多样话题和丰富情境的对抗性提示的新框架', 'title_zh': 'ROSE: 面向现实安全评估的大规模语言模型安全性评价'}
{'arxiv_id': 'arXiv:2507.00018', 'title': 'Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections', 'authors': 'Bo Wang, Qinyuan Cheng, Runyu Peng, Rong Bao, Peiji Li, Qipeng Guo, Linyang Li, Zhiyuan Zeng, Yunhua Zhou, Xipeng Qiu', 'link': 'https://arxiv.org/abs/2507.00018', 'abstract': 'Post-training processes are essential phases in grounding pre-trained language models to real-world tasks, with learning from demonstrations or preference signals playing a crucial role in this adaptation. We present a unified theoretical framework bridging Supervised Fine-Tuning (SFT) and preference learning in Large Language Model (LLM) post-training. Through rigorous mathematical derivation, we demonstrate that both SFT and preference learning methods like Direct Preference Optimization (DPO) operate within the same optimal policy-reward subspace, with SFT representing a special case of implicit reward learning. Our analysis reveals a critical limitation in conventional SFT: the KL divergence term in distribution matching becomes constant with respect to the policy during optimization, failing to constrain model updates. To address this, we propose a simple yet effective learning rate reduction approach that yields significant performance improvements (up to \\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in instruction following tasks. Additionally, we derive alternative SFT objectives from various f-divergence functions that preserve the KL term during optimization, further enhancing post-DPO model performance. Finally, we extend the theoretical relationship between LLM logits and Q-functions from preference learning to the SFT context, providing mathematical derivations and experimental validation.', 'abstract_zh': 'Post-训练过程是将预训练语言模型接地到现实任务中的关键阶段，通过演示学习或偏好信号学习起着至关重要的作用。我们提出了一种统一的理论框架，将监督微调（SFT）和大规模语言模型（LLM）后训练中的偏好学习方法联系起来。通过 rigorous的数学推导，我们证明了SFT和偏好学习方法（如直接偏好优化DPO）都在相同的最优策略-奖励子空间内运作，SFT代表隐式奖励学习的一种特殊情况。我们的分析揭示了传统SFT的一个关键限制：分布匹配中的KL散度项在优化过程中对策略变得常数，无法约束模型更新。为此，我们提出了一种简单而有效的学习率降低方法，显著提高了指令跟随任务的表现（相对提升高达25%，绝对胜率提高6%）。此外，我们从各种f散度函数中推导出了替代SFT目标，这些目标在优化过程中保留了KL项，进一步增强了后DPO模型的表现。最后，我们将偏好学习中LLM输出与Q函数之间的理论关系扩展到SFT场景中，提供了数学推导和实验验证。', 'title_zh': '隐式奖励作为桥梁：SFT与DPO的统一视角'}
{'arxiv_id': 'arXiv:2507.00016', 'title': 'Gradient-based Fine-Tuning through Pre-trained Model Regularization', 'authors': 'Xuanbo Liu, Liu Liu, Fuxiang Wu, Fusheng Hao, Xianglong Liu', 'link': 'https://arxiv.org/abs/2507.00016', 'abstract': 'Large pre-trained models have demonstrated extensive applications across various fields. However, fine-tuning these models for specific downstream tasks demands significant computational resources and storage. One fine-tuning method, gradient-based parameter selection (GPS), focuses on fine-tuning only the parameters with high gradients in each neuron, thereby reducing the number of training parameters. Nevertheless, this approach increases computational resource requirements and storage demands. In this paper, we propose an efficient gradient-based and regularized fine-tuning method (GRFT) that updates the rows or columns of the weight matrix. We theoretically demonstrate that the rows or columns with the highest sum of squared gradients are optimal for updating. This strategy effectively reduces storage overhead and improves the efficiency of parameter selection. Additionally, we incorporate regularization to enhance knowledge transfer from the pre-trained model. GRFT achieves state-of-the-art performance, surpassing existing methods such as GPS, Adapter Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the total parameters on FGVC and VTAB datasets, respectively, demonstrating its high efficiency and effectiveness. The source code will be released soon.', 'abstract_zh': '大型预训练模型在多个领域展示了广泛的应用。然而，针对特定下游任务进行微调需要大量计算资源和存储。一种基于梯度的参数选择（GPS）方法仅微调每个神经元中梯度较高的参数，从而减少训练参数的数量。这一方法虽然减少了参数数量，但也增加了计算资源需求和存储需求。本文提出了一种高效且正则化的基于梯度的微调方法（GRFT），该方法更新权重矩阵的行或列。我们从理论上证明，具有最高梯度平方和的行或列是最优更新的对象。这种方法有效地减少了存储开销并提高了参数选择的效率。此外，我们引入了正则化以增强知识迁移能力。GRFT在FGVC和VTAB数据集上取得了最先进的性能，超过了现有的方法如GPS、Adapter Tuning和LoRA。值得注意的是，GRFT在FGVC和VTAB数据集上分别只需更新1.22%和0.30%的总参数，展示了其高效和有效性。不久将发布源代码。', 'title_zh': '基于梯度的微调通过预训练模型正则化'}
{'arxiv_id': 'arXiv:2507.00014', 'title': 'SWE-Bench-CL: Continual Learning for Coding Agents', 'authors': 'Thomas Joshi, Shayan Chowdhury, Fatih Uysal', 'link': 'https://arxiv.org/abs/2507.00014', 'abstract': "Large Language Models (LLMs) have achieved impressive results on static code-generation benchmarks, but real-world software development unfolds as a continuous stream of evolving issues, fixes, and feature requests. We introduce SWE-Bench-CL, a novel continual learning benchmark built on the human-verified SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By organizing GitHub issues into chronologically ordered sequences that reflect natural repository evolution, SWE-Bench-CL enables direct evaluation of an agent's ability to accumulate experience, transfer knowledge across tasks, and resist catastrophic forgetting. We complement the dataset with (i) a preliminary analysis of inter-task structural similarity and contextual sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented with a FAISS-backed semantic memory module, and (iii) a suite of specialized continual learning metrics -- including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and a generalized Composite Continual Learning Score and CL-F-beta score -- to capture the stability-plasticity trade-off. We outline a rigorous experimental protocol comparing memory-enabled and memory-disabled agents across diverse Python repositories. All code and data are publicly available at this https URL, providing the community with a reproducible platform for developing more adaptive and robust AI agents in software engineering.", 'abstract_zh': '大规模语言模型（LLMs）在静态代码生成基准测试中取得了令人印象深刻的成果，但实际的软件开发是连续不断的问题解决、修复和功能请求的演变过程。我们引入了SWE-Bench-CL，一个基于OpenAI和Princeton-NLP于2024年引入的人工验证SWE-Bench Verified数据集的新型连续学习基准。通过将GitHub问题组织成反映自然仓库演化的时序序列，SWE-Bench-CL使我们能够直接评估代理累积经验、在不同任务间迁移知识以及抵抗灾难性遗忘的能力。我们补充了该数据集，包括(i) 不同任务结构相似性和情境敏感性的初步分析，(ii) 基于LangGraph的交互式评估框架，辅以FAISS支持的语义记忆模块，以及(iii) 包括平均准确度、遗忘、正向/反向迁移、工具使用效率以及广义的综合连续学习评分和CL-F-beta评分等一系列专门的连续学习度量标准，以捕捉稳定性和灵活性之间的权衡。我们概述了在不同Python仓库中对比启用和未启用记忆代理的严格实验协议。所有代码和数据均可在以下网址公开访问，为软件工程中开发更适应性和鲁棒性的AI代理提供了可重复的平台。', 'title_zh': 'SWE-Bench-CL: 不断学习的编码代理'}
{'arxiv_id': 'arXiv:2507.00004', 'title': 'A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search', 'authors': 'Austin R. Ellis-Mohr, Anuj K. Nayak, Lav R. Varshney', 'link': 'https://arxiv.org/abs/2507.00004', 'abstract': "Large language models (LLMs) demand considerable computational, energy, and financial resources during both training and deployment. While scaling laws for training have guided much of the field's recent progress, inference costs now represent a significant and growing component of the overall resource burden, particularly for reasoning-focused models. Existing characterizations of compute-optimality that consider model size, dataset size, and inference tokens in isolation or in fixed combinations risk overlooking more efficient operating points. We introduce directed stochastic skill search (DS3), a general framework that represents inference as stochastic traversal over a learned skill graph. From a simplified yet expressive instantiation, we derive closed-form expressions for task success and compute cost across a wide range of inference strategies -- including chain-of-thought (CoT) and tree-of-thought (ToT) -- enabling comparative analysis as a function of task difficulty and model capability. To that end, we extend a prior first-principles tripartite graph framework of LLM training to incorporate inference, and separately bridge DS3 with empirical methods that characterize LLM scaling behavior. We theoretically recover empirically observed patterns, including: linear accuracy scaling with logarithmic compute; variation in preferred inference strategies as a function of task difficulty and model capability; emergent behavior elicited by reasoning even when performance plateaus under parameter scaling; and both best-of-N (BoN) and majority voting behavior captured within a unified analytical framework. By explicitly characterizing training-inference interdependencies, our framework deepens theoretical understanding and supports principled algorithmic design and resource allocation.", 'abstract_zh': '大型语言模型（LLMs）在训练和部署过程中消耗大量的计算、能源和财务资源。虽然训练的缩放定律在该领域近期的进展中发挥了重要作用，但推理成本现在已成为总体资源负担中一个显著且不断增长的组成部分，尤其是对于注重推理的模型。现有的关于计算效率的表征主要考虑模型规模、数据集规模和推理令牌在单独或固定组合中的影响，可能忽略了更高效的运行点。我们引入了定向随机技能搜索（DS3），这是一种一般框架，将推理表示为在学习技能图上的随机遍历。从一个简化但富有表现力的实例出发，我们推导出了广泛的推理策略下任务成功和计算成本的闭式表达式——包括链式思考（CoT）和树状思考（ToT），从而基于任务难度和模型能力进行比较分析。为此，我们将先前的LLM训练的第一原理三方图框架扩展到包括推理，并独立地将DS3与表征LLM缩放行为的实验方法结合起来。我们理论上重新推导了实验观察到的模式，包括：线性准确度随对数计算的增长；根据任务难度和模型能力选择的推理策略的变化；在参数缩放下性能饱和时由推理引发的新兴行为；以及在统一的分析框架中捕获的最佳选项和 majority 赞同行为。通过明确表征训练-推理相互依赖性，我们的框架加深了理论理解并支持基于原则的算法设计和资源分配。', 'title_zh': '推理推理计算扩展理论：通过定向随机技能搜索进行推理'}
{'arxiv_id': 'arXiv:2507.00002', 'title': 'Hypertokens: Holographic Associative Memory in Tokenized LLMs', 'authors': 'Christopher James Augeri', 'link': 'https://arxiv.org/abs/2507.00002', 'abstract': 'Large Language Models (LLMs) exhibit remarkable capabilities but suffer from apparent precision loss, reframed here as information spreading. This reframing shifts the problem from computational precision to an information-theoretic communication issue. We address the K:V and V:K memory problem in LLMs by introducing HDRAM (Holographically Defined Random Access Memory), a symbolic memory framework treating transformer latent space as a spread-spectrum channel. Built upon hypertokens, structured symbolic codes integrating classical error-correcting codes (ECC), holographic computing, and quantum-inspired search, HDRAM recovers distributed information through principled despreading. These phase-coherent memory addresses enable efficient key-value operations and Grover-style search in latent space. By combining ECC grammar with compressed sensing and Krylov subspace alignment, HDRAM significantly improves associative retrieval without architectural changes, demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can fortify transformer architectures.', 'abstract_zh': '大型语言模型（LLMs）表现出色，但存在明显的精度损失，这里重新定义为信息扩散问题。这种重新定义将问题从计算精度转变为信息论通信问题。我们通过引入HDRAM（量子定义随机访问存储器）解决了LLMs中的K:V和V:K内存问题，HDRAM是一种符号内存框架，将变压器潜空间视为扩展谱通道。基于超词、结构化符号编码（结合经典错误校正码）、全息计算和量子启发式搜索，HDRAM通过原理性的解扩展恢复分布式信息。这些相干记忆地址能够在潜空间中实现高效的键值操作和Grover风格搜索。通过结合经典错误校正码语法、压缩感知和Krylov子空间对齐，HDRAM在不改变架构的情况下显著提高了关联检索，展示了经典-全息-量子启发（CHQ）原则如何加强变压器架构。', 'title_zh': '超代词：标记化大语言模型中的全息关联记忆'}
