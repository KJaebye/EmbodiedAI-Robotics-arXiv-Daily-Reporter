{'arxiv_id': 'arXiv:2507.01016', 'title': 'VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers', 'authors': 'Yating Wang, Haoyi Zhu, Mingyu Liu, Jiange Yang, Hao-Shu Fang, Tong He', 'link': 'https://arxiv.org/abs/2507.01016', 'abstract': 'In this paper, we introduce an innovative vector quantization based action tokenizer built upon the largest-scale action trajectory dataset to date, leveraging over 100 times more data than previous approaches. This extensive dataset enables our tokenizer to capture rich spatiotemporal dynamics, resulting in a model that not only accelerates inference but also generates smoother and more coherent action outputs. Once trained, the tokenizer can be seamlessly adapted to a wide range of downstream tasks in a zero-shot manner, from short-horizon reactive behaviors to long-horizon planning. A key finding of our work is that the domain gap between synthetic and real action trajectories is marginal, allowing us to effectively utilize a vast amount of synthetic data during training without compromising real-world performance. To validate our approach, we conducted extensive experiments in both simulated environments and on real robotic platforms. The results demonstrate that as the volume of synthetic trajectory data increases, the performance of our tokenizer on downstream tasks improves significantly-most notably, achieving up to a 30% higher success rate on two real-world tasks in long-horizon scenarios. These findings highlight the potential of our action tokenizer as a robust and scalable solution for real-time embodied intelligence systems, paving the way for more efficient and reliable robotic control in diverse application this http URL website: this https URL', 'abstract_zh': '本文介绍了一种基于目前最大的动作轨迹数据集构建的创新向量量化动作分词器，该数据集比以往方法的数据量多出100多倍。这一庞大的数据集使得我们的分词器能够捕捉丰富的时空动态，从而不仅加速了推理过程，还生成了更流畅和连贯的动作输出。训练完成后，该分词器可以无缝适应广泛的应用任务，从短时反应行为到长远规划。我们的工作的一个关键发现是，合成和真实动作轨迹之间的领域差异很小，这使得我们在训练过程中可以有效地利用大量的合成数据，而不影响实际性能。为了验证我们的方法，我们还在仿真环境和真实机器人平台上进行了广泛实验。结果表明，随着合成轨迹数据量的增加，我们的分词器在下游任务上的性能显著提高——特别是在长时序场景中，两个实际任务的成功率提高了30%以上。这些发现突显了我们动作分词器作为实时嵌入式智能系统稳健且可扩展解决方案的潜力，为在各种应用中实现更高效和可靠的机器人控制铺平了道路。', 'title_zh': 'VQ-VLA：通过扩展向量量化动作词典提高视觉-语言-动作模型性能'}
{'arxiv_id': 'arXiv:2507.01008', 'title': 'DexWrist: A Robotic Wrist for Constrained and Dynamic Manipulation', 'authors': 'Martin Peticco, Gabriella Ulloa, John Marangola, Pulkit Agrawal', 'link': 'https://arxiv.org/abs/2507.01008', 'abstract': 'We present the DexWrist, a compliant robotic wrist designed to advance robotic manipulation in highly-constrained environments, enable dynamic tasks, and speed up data collection. DexWrist is designed to be close to the functional capabilities of the human wrist and achieves mechanical compliance and a greater workspace as compared to existing robotic wrist designs. The DexWrist can supercharge policy learning by (i) enabling faster teleoperation and therefore making data collection more scalable; (ii) completing tasks in fewer steps which reduces trajectory lengths and therefore can ease policy learning; (iii) DexWrist is designed to be torque transparent with easily simulatable kinematics for simulated data collection; and (iv) most importantly expands the workspace of manipulation for approaching highly cluttered scenes and tasks. More details about the wrist can be found at: this http URL.', 'abstract_zh': '我们提出DexWrist，一种符合人体工程学的机器人手腕，旨在促进在高度受限环境中的人机 manipulation，实现动态任务，并加速数据收集。DexWrist 设计接近人类手腕的功能能力，并在机械柔顺性和工作空间方面优于现有机器人手腕设计。DexWrist 可以通过以下方式增强策略学习：(i) 使远程操作更快，从而使数据收集更具可扩展性；(ii) 以更少的步骤完成任务，减少轨迹长度，从而简化策略学习；(iii) 设计为扭矩透明，并具有易于模拟的动力学，便于模拟数据收集；(iv) 最重要的是，DexWrist 扩展了接近高度杂乱场景和任务的 manipulation 工作空间。有关手腕的更多详细信息，请参阅：this http URL。', 'title_zh': 'DexWrist: 一种用于受限和动态操作的机器人手腕'}
{'arxiv_id': 'arXiv:2507.00990', 'title': 'Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations', 'authors': 'Shivansh Patel, Shraddhaa Mohan, Hanlin Mai, Unnat Jain, Svetlana Lazebnik, Yunzhu Li', 'link': 'https://arxiv.org/abs/2507.00990', 'abstract': 'This work introduces Robots Imitating Generated Videos (RIGVid), a system that enables robots to perform complex manipulation tasks--such as pouring, wiping, and mixing--purely by imitating AI-generated videos, without requiring any physical demonstrations or robot-specific training. Given a language command and an initial scene image, a video diffusion model generates potential demonstration videos, and a vision-language model (VLM) automatically filters out results that do not follow the command. A 6D pose tracker then extracts object trajectories from the video, and the trajectories are retargeted to the robot in an embodiment-agnostic fashion. Through extensive real-world evaluations, we show that filtered generated videos are as effective as real demonstrations, and that performance improves with generation quality. We also show that relying on generated videos outperforms more compact alternatives such as keypoint prediction using VLMs, and that strong 6D pose tracking outperforms other ways to extract trajectories, such as dense feature point tracking. These findings suggest that videos produced by a state-of-the-art off-the-shelf model can offer an effective source of supervision for robotic manipulation.', 'abstract_zh': '机器人模仿生成视频（RIGVid）系统：通过模仿AI生成的视频执行复杂的操作任务', 'title_zh': '基于生成视频 imitation 学习的机器人操作'}
{'arxiv_id': 'arXiv:2507.00984', 'title': 'Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation', 'authors': 'Xihang Yu, Rajat Talak, Jingnan Shi, Ulrich Viereck, Igor Gilitschenski, Luca Carlone', 'link': 'https://arxiv.org/abs/2507.00984', 'abstract': 'Modern warehouse automation systems rely on fleets of intelligent robots that generate vast amounts of data -- most of which remains unannotated. This paper develops a self-supervised domain adaptation pipeline that leverages real-world, unlabeled data to improve perception models without requiring manual annotations. Our work focuses specifically on estimating the pose and shape of boxes and presents a correct-and-certify pipeline for self-supervised box pose and shape estimation. We extensively evaluate our approach across a range of simulated and real industrial settings, including adaptation to a large-scale real-world dataset of 50,000 images. The self-supervised model significantly outperforms models trained solely in simulation and shows substantial improvements over a zero-shot 3D bounding box estimation baseline.', 'abstract_zh': '现代仓库自动化系统依赖于大量的智能机器人，这些机器人生成了大量数据，其中大部分未标注。本文开发了一种自我监督的领域适应管道，利用现实世界的未标注数据来改进感知模型，无需人工标注。我们的工作专注于估计箱子的姿态和形状，并提出了一种自我监督的箱子姿态和形状估计的正确并认证管道。我们广泛评估了该方法在多个模拟和现实工业环境中的性能，包括对一个包含50,000张图像的大规模现实世界数据集进行适应。自我监督模型显著优于仅在模拟中训练的模型，并在零样本3D边界框估计基线方面显示出大幅改进。', 'title_zh': '大规模仓库自动化中的盒子姿态、形状估计与领域适应'}
{'arxiv_id': 'arXiv:2507.00937', 'title': 'RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles', 'authors': 'David Hunt, Shaocheng Luo, Spencer Hallyburton, Shafii Nillongo, Yi Li, Tingjun Chen, Miroslav Pajic', 'link': 'https://arxiv.org/abs/2507.00937', 'abstract': 'Low-cost indoor mobile robots have gained popularity with the increasing adoption of automation in homes and commercial spaces. However, existing lidar and camera-based solutions have limitations such as poor performance in visually obscured environments, high computational overhead for data processing, and high costs for lidars. In contrast, mmWave radar sensors offer a cost-effective and lightweight alternative, providing accurate ranging regardless of visibility. However, existing radar-based localization suffers from sparse point cloud generation, noise, and false detections. Thus, in this work, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph neural network (GNN)-based framework to enhance radar point clouds, even in complex and dynamic environments. With an inference time of just 7.3 ms on the low-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such resource-constrained devices, requiring no additional computational resources. We evaluate its performance across key tasks, including localization, SLAM, and autonomous navigation, in three different environments. Our results demonstrate strong reliability and generalizability, making RaGNNarok a robust solution for low-cost indoor mobile robots.', 'abstract_zh': '低成本室内移动机器人随着家庭和商业空间中自动化程度的提高而流行。然而，现有的基于激光雷达和摄像头的解决方案在视觉遮挡环境中表现不佳，数据处理计算开销高，并且激光雷达成本高昂。相比之下，毫米波雷达传感器提供了一种成本效益高且体积轻的替代方案，能够在任何可见度条件下提供精确的距离测量。然而，现有的基于雷达的定位方法存在点云稀疏、噪声和误检等问题。因此，在这项工作中，我们引入了RaGNNarok，这是一种实时、轻量级且可泛化的基于图神经网络(GNN)的框架，用于增强雷达点云数据，即使在复杂和动态的环境中也是如此。在低成本的Raspberry Pi 5上，RaGNNarok的推理时间仅为7.3毫秒，即使在资源受限的设备上也能高效运行，无需额外的计算资源。我们在三个不同环境中评估了其在定位、SLAM和自主导航等关键任务上的性能。我们的结果表明，RaGNNarok具有很强的可靠性和泛化能力，是低成本室内移动机器人的稳健解决方案。', 'title_zh': 'RaGNNarok：一种用于增强无人驾驶地面车辆雷达点云的轻量级图神经网络'}
{'arxiv_id': 'arXiv:2507.00917', 'title': 'A Survey: Learning Embodied Intelligence from Physical Simulators and World Models', 'authors': 'Xiaoxiao Long, Qingrui Zhao, Kaiwen Zhang, Zihao Zhang, Dingrui Wang, Yumeng Liu, Zhengjie Shu, Yi Lu, Shouzheng Wang, Xinzhe Wei, Wei Li, Wei Yin, Yao Yao, Jia Pan, Qiu Shen, Ruigang Yang, Xun Cao, Qionghai Dai', 'link': 'https://arxiv.org/abs/2507.00917', 'abstract': 'The pursuit of artificial general intelligence (AGI) has placed embodied intelligence at the forefront of robotics research. Embodied intelligence focuses on agents capable of perceiving, reasoning, and acting within the physical world. Achieving robust embodied intelligence requires not only advanced perception and control, but also the ability to ground abstract cognition in real-world interactions. Two foundational technologies, physical simulators and world models, have emerged as critical enablers in this quest. Physical simulators provide controlled, high-fidelity environments for training and evaluating robotic agents, allowing safe and efficient development of complex behaviors. In contrast, world models empower robots with internal representations of their surroundings, enabling predictive planning and adaptive decision-making beyond direct sensory input. This survey systematically reviews recent advances in learning embodied AI through the integration of physical simulators and world models. We analyze their complementary roles in enhancing autonomy, adaptability, and generalization in intelligent robots, and discuss the interplay between external simulation and internal modeling in bridging the gap between simulated training and real-world deployment. By synthesizing current progress and identifying open challenges, this survey aims to provide a comprehensive perspective on the path toward more capable and generalizable embodied AI systems. We also maintain an active repository that contains up-to-date literature and open-source projects at this https URL.', 'abstract_zh': '人工通用智能（AGI）的追求使体态智能成为机器人研究的前沿领域。体态智能关注能够感知、推理和在物理世界中行动的代理。实现稳健的体态智能不仅需要高级感知和控制，还需要将抽象认知根植于现实世界的交互中。两种基础技术，物理模拟器和世界模型，已成为这一追求中的关键使能技术。物理模拟器为训练和评估机器人代理提供了受控的高保真环境，使复杂行为的安全和高效开发成为可能。相比之下，世界模型使机器人能够拥有其周围环境的内部表示，从而实现超越直接感官输入的预测规划和适应性决策。本文综述了通过整合物理模拟器和世界模型学习体态AI的近期进展。我们分析了这两种技术在增强智能机器人自主性、适应性和泛化性方面的互补作用，并讨论了外部模拟与内部建模之间的交互，以弥合模拟训练与实际部署之间的差距。通过综合当前进展并识别开放挑战，本文旨在提供一条通向更具能力和泛化性的体态AI系统的全面视角。我们还维护了一个活跃的存储库，该存储库包含最新的文献和开源项目，可通过此链接访问：[https://github.com/Qwen-Model/body-aware-ai_survey]。', 'title_zh': '一种综述：从物理模拟器和世界模型中学习 embodiet 智能'}
{'arxiv_id': 'arXiv:2507.00882', 'title': 'I Move Therefore I Learn: Experience-Based Traversability in Outdoor Robotics', 'authors': 'Miguel Ángel de Miguel, Jorge Beltrán, Juan S. Cely, Francisco Martín, Juan Carlos Manzanares, Alberto García', 'link': 'https://arxiv.org/abs/2507.00882', 'abstract': 'Accurate traversability estimation is essential for safe and effective navigation of outdoor robots operating in complex environments. This paper introduces a novel experience-based method that allows robots to autonomously learn which terrains are traversable based on prior navigation experience, without relying on extensive pre-labeled datasets. The approach integrates elevation and texture data into multi-layered grid maps, which are processed using a variational autoencoder (VAE) trained on a generic texture dataset. During an initial teleoperated phase, the robot collects sensory data while moving around the environment. These experiences are encoded into compact feature vectors and clustered using the BIRCH algorithm to represent traversable terrain areas efficiently. In deployment, the robot compares new terrain patches to its learned feature clusters to assess traversability in real time. The proposed method does not require training with data from the targeted scenarios, generalizes across diverse surfaces and platforms, and dynamically adapts as new terrains are encountered. Extensive evaluations on both synthetic benchmarks and real-world scenarios with wheeled and legged robots demonstrate its effectiveness, robustness, and superior adaptability compared to state-of-the-art approaches.', 'abstract_zh': '基于经验的准确 traversability 估计对于户外机器人在复杂环境下安全有效地导航是必不可少的。本文介绍了一种新颖的经验导向方法，使机器人能够基于先前的导航经验自主学习哪些地形是可通行的，而无需依赖大量预先标注的 数据集。该方法将高程和纹理数据整合到多层网格地图中，并使用在通用纹理数据集上训练的变分自编码器（VAE）进行处理。在初始的遥控阶段，机器人在环境中移动并收集感官数据。这些经验被编码成紧凑的特征向量，并使用 BIRCH 算法进行聚类，以高效地表示可通行区域。在部署过程中，机器人将新地形片段与学习到的特征聚类进行比较，以实时评估可通行性。该方法不需要使用针对特定场景的数据进行训练，能够在多种表面和平台上泛化，并能够动态适应新遇到的地形。在合成基准和现实世界场景中的广泛评估表明，该方法在有效性、稳健性和适应性方面优于现有方法。', 'title_zh': '我移动Therefore我学习：基于经验的户外机器人可通行性'}
{'arxiv_id': 'arXiv:2507.00833', 'title': 'HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning', 'authors': 'Zhi Jing, Siyuan Yang, Jicong Ao, Ting Xiao, Yugang Jiang, Chenjia Bai', 'link': 'https://arxiv.org/abs/2507.00833', 'abstract': 'For robotic manipulation, existing robotics datasets and simulation benchmarks predominantly cater to robot-arm platforms. However, for humanoid robots equipped with dual arms and dexterous hands, simulation tasks and high-quality demonstrations are notably lacking. Bimanual dexterous manipulation is inherently more complex, as it requires coordinated arm movements and hand operations, making autonomous data collection challenging. This paper presents HumanoidGen, an automated task creation and demonstration collection framework that leverages atomic dexterous operations and LLM reasoning to generate relational constraints. Specifically, we provide spatial annotations for both assets and dexterous hands based on the atomic operations, and perform an LLM planner to generate a chain of actionable spatial constraints for arm movements based on object affordances and scenes. To further improve planning ability, we employ a variant of Monte Carlo tree search to enhance LLM reasoning for long-horizon tasks and insufficient annotation. In experiments, we create a novel benchmark with augmented scenarios to evaluate the quality of the collected data. The results show that the performance of the 2D and 3D diffusion policies can scale with the generated dataset. Project page is this https URL.', 'abstract_zh': '基于人类形机器人双臂灵巧操作的自动化任务创建与演示收集框架', 'title_zh': 'HumanoidGen: 通过LLM推理进行双臂灵巧 manipulation 的数据生成'}
{'arxiv_id': 'arXiv:2507.00816', 'title': 'PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments', 'authors': 'Mengyun Wang, Bo Wang, Yifeng Niu, Chang Wang', 'link': 'https://arxiv.org/abs/2507.00816', 'abstract': 'Accurate dynamics modeling is essential for quadrotors to achieve precise trajectory tracking in various applications. Traditional physical knowledge-driven modeling methods face substantial limitations in unknown environments characterized by variable payloads, wind disturbances, and external perturbations. On the other hand, data-driven modeling methods suffer from poor generalization when handling out-of-distribution (OoD) data, restricting their effectiveness in unknown scenarios. To address these challenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN), which combines knowledge-driven and data-driven modeling methods by embedding physical constraints directly into the training process for robust quadrotor dynamics learning. Specifically, PI-WAN employs a Temporal Convolutional Network (TCN) architecture that efficiently captures temporal dependencies from historical flight data, while a physics-informed loss function applies physical principles to improve model generalization and robustness across previously unseen conditions. By incorporating real-time prediction results into a model predictive control (MPC) framework, we achieve improvements in closed-loop tracking performance. Comprehensive simulations and real-world flight experiments demonstrate that our approach outperforms baseline methods in terms of prediction accuracy, tracking precision, and robustness to unknown environments.', 'abstract_zh': '基于物理约束的适应性风速网络：一种结合知识驱动与数据驱动的方法', 'title_zh': 'PI-WAN：一种用于未知环境旋翼无人机动力学预测的物理告知风自适应网络'}
{'arxiv_id': 'arXiv:2507.00677', 'title': 'Learning Steerable Imitation Controllers from Unstructured Animal Motions', 'authors': 'Dongho Kang, Jin Cheng, Fatemeh Zargarbashi, Taerim Yoon, Sungjoon Choi, Stelian Coros', 'link': 'https://arxiv.org/abs/2507.00677', 'abstract': "This paper presents a control framework for legged robots that leverages unstructured real-world animal motion data to generate animal-like and user-steerable behaviors. Our framework learns to follow velocity commands while reproducing the diverse gait patterns in the original dataset. To begin with, animal motion data is transformed into a robot-compatible database using constrained inverse kinematics and model predictive control, bridging the morphological and physical gap between the animal and the robot. Subsequently, a variational autoencoder-based motion synthesis module captures the diverse locomotion patterns in the motion database and generates smooth transitions between them in response to velocity commands. The resulting kinematic motions serve as references for a reinforcement learning-based feedback controller deployed on physical robots. We show that this approach enables a quadruped robot to adaptively switch gaits and accurately track user velocity commands while maintaining the stylistic coherence of the motion data. Additionally, we provide component-wise evaluations to analyze the system's behavior in depth and demonstrate the efficacy of our method for more accurate and reliable motion imitation.", 'abstract_zh': '本文提出了一种控制框架，利用未结构化的现实世界动物运动数据生成类似动物且可用户操控的行为。我们的框架在遵循速度指令的同时再现原始数据集中的多种步态模式。首先，使用约束逆运动学和模型预测控制将动物运动数据转换为与机器人兼容的数据库，缩小动物与机器人在形态和物理上的差距。随后，基于变分自编码器的运动合成模块捕捉运动数据库中的多样化动作模式，并在响应速度指令时生成平滑过渡。所产生的运动用于强化学习反馈控制的参考，在物理机器人上部署。我们展示了该方法使四足机器人能够适应性切换步态并准确跟踪用户速度指令，同时保持运动数据的风格一致性。此外，我们提供了组件级评估来深入分析系统的性能，并展示了该方法在更准确可靠的运动模仿方面的有效性。', 'title_zh': '从无结构动物运动中学习可控模仿控制器'}
{'arxiv_id': 'arXiv:2507.00644', 'title': 'Parallel Transmission Aware Co-Design: Enhancing Manipulator Performance Through Actuation-Space Optimization', 'authors': 'Rohit Kumar, Melya Boukheddimi, Dennis Mronga, Shivesh Kumar, Frank Kirchner', 'link': 'https://arxiv.org/abs/2507.00644', 'abstract': 'In robotics, structural design and behavior optimization have long been considered separate processes, resulting in the development of systems with limited capabilities. Recently, co-design methods have gained popularity, where bi-level formulations are used to simultaneously optimize the robot design and behavior for specific tasks. However, most implementations assume a serial or tree-type model of the robot, overlooking the fact that many robot platforms incorporate parallel mechanisms. In this paper, we present a novel co-design approach that explicitly incorporates parallel coupling constraints into the dynamic model of the robot. In this framework, an outer optimization loop focuses on the design parameters, in our case the transmission ratios of a parallel belt-driven manipulator, which map the desired torques from the joint space to the actuation space. An inner loop performs trajectory optimization in the actuation space, thus exploiting the entire dynamic range of the manipulator. We compare the proposed method with a conventional co-design approach based on a simplified tree-type model. By taking advantage of the actuation space representation, our approach leads to a significant increase in dynamic payload capacity compared to the conventional co-design implementation.', 'abstract_zh': '机器人结构设计与行为优化的并行耦合协同设计方法', 'title_zh': '面向并行传输协同设计：通过执行空间优化提升 manipulator 性能'}
{'arxiv_id': 'arXiv:2507.00635', 'title': 'Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery', 'authors': 'Tinghe Hong, Shenlin Cai, Boyang Li, Kai Huang', 'link': 'https://arxiv.org/abs/2507.00635', 'abstract': "Ophthalmic surgical robots offer superior stability and precision by reducing the natural hand tremors of human surgeons, enabling delicate operations in confined surgical spaces. Despite the advancements in developing vision- and force-based control methods for surgical robots, preoperative navigation remains heavily reliant on manual operation, limiting the consistency and increasing the uncertainty. Existing eye gaze estimation techniques in the surgery, whether traditional or deep learning-based, face challenges including dependence on additional sensors, occlusion issues in surgical environments, and the requirement for facial detection. To address these limitations, this study proposes an innovative eye localization and tracking method that combines machine learning with traditional algorithms, eliminating the requirements of landmarks and maintaining stable iris detection and gaze estimation under varying lighting and shadow conditions. Extensive real-world experiment results show that our proposed method has an average estimation error of 0.58 degrees for eye orientation estimation and 2.08-degree average control error for the robotic arm's movement based on the calculated orientation.", 'abstract_zh': '眼科手术机器人通过减少人类外科医生的自然手震，提供卓越的稳定性和精确度，使在狭小手术空间中进行精细操作成为可能。尽管在开发基于视觉和力的控制方法方面取得了进步，但预手术导航仍然高度依赖手动操作，限制了一致性并增加了不确定性。现有的手术中眼球运动估计技术，无论是传统方法还是深度学习方法，都面临额外传感器依赖、手术环境中的遮挡问题以及面部检测的需要等挑战。为了解决这些局限性，本研究提出了一种结合机器学习与传统算法的创新眼球定位和跟踪方法，该方法无需特征点，能够在不同光照和阴影条件下保持稳定的虹膜检测和眼球运动估计。广泛的实际实验结果表明，所提出的方法在眼球方向估计中的平均误差为0.58度，在基于计算方向的机器人臂运动控制中的平均误差为2.08度。', 'title_zh': '眼科学手术中稳定追踪眼球注视方向'}
{'arxiv_id': 'arXiv:2507.00552', 'title': 'Generation of Indoor Open Street Maps for Robot Navigation from CAD Files', 'authors': 'Jiajie Zhang, Shenrui Wu, Xu Ma, Sören Schwertfeger', 'link': 'https://arxiv.org/abs/2507.00552', 'abstract': 'The deployment of autonomous mobile robots is predicated on the availability of environmental maps, yet conventional generation via SLAM (Simultaneous Localization and Mapping) suffers from significant limitations in time, labor, and robustness, particularly in dynamic, large-scale indoor environments where map obsolescence can lead to critical localization failures. To address these challenges, this paper presents a complete and automated system for converting architectural Computer-Aided Design (CAD) files into a hierarchical topometric OpenStreetMap (OSM) representation, tailored for robust life-long robot navigation. Our core methodology involves a multi-stage pipeline that first isolates key structural layers from the raw CAD data and then employs an AreaGraph-based topological segmentation to partition the building layout into a hierarchical graph of navigable spaces. This process yields a comprehensive and semantically rich map, further enhanced by automatically associating textual labels from the CAD source and cohesively merging multiple building floors into a unified, topologically-correct model. By leveraging the permanent structural information inherent in CAD files, our system circumvents the inefficiencies and fragility of SLAM, offering a practical and scalable solution for deploying robots in complex indoor spaces. The software is encapsulated within an intuitive Graphical User Interface (GUI) to facilitate practical use. The code and dataset are available at this https URL.', 'abstract_zh': '基于自主移动机器人在动态大规模室内环境中的应用，其部署依赖于可用的环境地图，而传统的通过SLAM生成地图在时间、劳动和鲁棒性方面存在显著限制。为解决这些挑战，本文提出了一种完整且自动化的系统，将建筑计算机辅助设计（CAD）文件转换为适用于鲁棒 lifelong 机器人导航的分层拓扑OpenStreetMap（OSM）表示。该核心方法涉及一个多阶段管道，首先从原始CAD数据中隔离关键的结构层，然后采用基于AreaGraph的拓扑分割将建筑布局划分为分层图中的可导航空间。此过程生成了一个全面且语义丰富的地图，并通过自动关联CAD源中的文本标签和统一合并多层建筑结构，进一步增强了这一模型。通过利用CAD文件中固有的永久结构信息，我们的系统规避了SLAM的低效性和脆弱性，提供了一种适用于复杂室内空间的实用且可扩展的机器人部署解决方案。该软件封装在直观的图形用户界面（GUI）中，以促进实际使用。相关代码和数据集可从以下链接获得。', 'title_zh': '基于CAD文件生成室内开放街道地图以供机器人导航'}
{'arxiv_id': 'arXiv:2507.00523', 'title': 'Edge Computing and its Application in Robotics: A Survey', 'authors': 'Nazish Tahir, Ramviyas Parasuraman', 'link': 'https://arxiv.org/abs/2507.00523', 'abstract': "The Edge computing paradigm has gained prominence in both academic and industry circles in recent years. By implementing edge computing facilities and services in robotics, it becomes a key enabler in the deployment of artificial intelligence applications to robots. Time-sensitive robotics applications benefit from the reduced latency, mobility, and location awareness provided by the edge computing paradigm, which enables real-time data processing and intelligence at the network's edge. While the advantages of integrating edge computing into robotics are numerous, there has been no recent survey that comprehensively examines these benefits. This paper aims to bridge that gap by highlighting important work in the domain of edge robotics, examining recent advancements, and offering deeper insight into the challenges and motivations behind both current and emerging solutions. In particular, this article provides a comprehensive evaluation of recent developments in edge robotics, with an emphasis on fundamental applications, providing in-depth analysis of the key motivations, challenges, and future directions in this rapidly evolving domain. It also explores the importance of edge computing in real-world robotics scenarios where rapid response times are critical. Finally, the paper outlines various open research challenges in the field of edge robotics.", 'abstract_zh': '边缘计算范式在学术和工业领域中日益凸显，通过在机器人中实施边缘计算设施和服务，它成为将人工智能应用部署到机器人中的关键使能器。时间敏感的机器人应用得益于边缘计算范式提供的减少延迟、提高移动性和位置感知性，这使得网络边缘处能实现实时数据处理和智能。尽管将边缘计算整合到机器人中有诸多优势，但近年来尚未有任何综述全面探讨这些好处。本文旨在通过突出边缘机器人领域的关键工作、考察最近的进步并深入探讨当前及新兴解决方案背后的挑战和动机来填补这一空白。特别是，本文对边缘机器人领域的最近发展进行了全方位评估，重点关注基础应用，并对关键动机、挑战及这一迅速发展的领域中的未来方向进行了深入分析。文章还探讨了在需要快速响应时间的现实机器人场景中边缘计算的重要性。最后，论文概述了边缘机器人领域的各种开放研究挑战。', 'title_zh': '边缘计算及其在机器人领域的应用：一个综述'}
{'arxiv_id': 'arXiv:2507.00464', 'title': 'A Miniature High-Resolution Tension Sensor Based on a Photo-Reflector for Robotic Hands and Grippers', 'authors': 'Hyun-Bin Kim, Kyung-Soo Kim', 'link': 'https://arxiv.org/abs/2507.00464', 'abstract': 'This paper presents a miniature tension sensor using a photo-reflector, designed for compact tendon-driven grippers and robotic hands. The proposed sensor has a small form factor of 13~mm x 7~mm x 6.5~mm and is capable of measuring tensile forces up to 200~N. A symmetric elastomer structure incorporating fillets and flexure hinges is designed based on Timoshenko beam theory and verified via FEM analysis, enabling improved sensitivity and mechanical durability while minimizing torsional deformation. The sensor utilizes a compact photo-reflector (VCNT2020) to measure displacement in the near-field region, eliminating the need for light-absorbing materials or geometric modifications required in photo-interrupter-based designs. A 16-bit analog-to-digital converter (ADC) and CAN-FD (Flexible Data-rate) communication enable efficient signal acquisition with up to 5~kHz sampling rate. Calibration experiments demonstrate a resolution of 9.9~mN (corresponding to over 14-bit accuracy) and a root mean square error (RMSE) of 0.455~N. Force control experiments using a twisted string actuator and PI control yield RMSEs as low as 0.073~N. Compared to previous research using photo-interrupter, the proposed method achieves more than tenfold improvement in resolution while also reducing nonlinearity and hysteresis. The design is mechanically simple, lightweight, easy to assemble, and suitable for integration into robotic and prosthetic systems requiring high-resolution force feedback.', 'abstract_zh': '基于光反射器的微型张力传感器及其在紧凑型肌腱驱动手指和机器人手中的应用', 'title_zh': '基于光电反射器的高分辨率微型张力传感器及其在机器人手和手指中的应用'}
{'arxiv_id': 'arXiv:2507.00446', 'title': 'DIJE: Dense Image Jacobian Estimation for Robust Robotic Self-Recognition and Visual Servoing', 'authors': 'Yasunori Toshimitsu, Kento Kawaharazuka, Akihiro Miki, Kei Okada, Masayuki Inaba', 'link': 'https://arxiv.org/abs/2507.00446', 'abstract': "For robots to move in the real world, they must first correctly understand the state of its own body and the tools that it holds. In this research, we propose DIJE, an algorithm to estimate the image Jacobian for every pixel. It is based on an optical flow calculation and a simplified Kalman Filter that can be efficiently run on the whole image in real time. It does not rely on markers nor knowledge of the robotic structure. We use the DIJE in a self-recognition process which can robustly distinguish between movement by the robot and by external entities, even when the motion overlaps. We also propose a visual servoing controller based on DIJE, which can learn to control the robot's body to conduct reaching movements or bimanual tool-tip control. The proposed algorithms were implemented on a physical musculoskeletal robot and its performance was verified. We believe that such global estimation of the visuomotor policy has the potential to be extended into a more general framework for manipulation.", 'abstract_zh': '机器人在真实世界中移动，必须首先准确理解自己身体和持有的工具的状态。在此研究中，我们提出DIJE算法，用于估计每个像素的图像雅可比矩阵。该算法基于光学流计算和一个简化的卡尔曼滤波器，可以在整个图像中实时高效运行。它无需使用标记也不依赖于对机器人结构的了解。我们在一种自识别过程中使用DIJE，该过程可以 robust 地区分由机器人自身和外部实体引起的运动，即使运动重叠也是如此。我们还提出了一种基于DIJE的视觉伺服控制器，可以学习控制机器人的身体进行到达运动或双臂工具尖端控制。提出的算法已在物理肌骨骼机器人上实现，并验证了其性能。我们相信，这种全局视觉运动政策估计有可能扩展为更通用的操纵框架。', 'title_zh': 'DIJE: 密度图像雅可比估计在鲁棒机器人自我识别和视觉伺服中的应用'}
{'arxiv_id': 'arXiv:2507.00443', 'title': 'Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems', 'authors': 'Reza Ahmadvand, Sarah Safura Sharif, Yaser Mike Banad', 'link': 'https://arxiv.org/abs/2507.00443', 'abstract': "Recent advances in multi-agent systems manipulation have demonstrated a rising demand for the implementation of multi-UAV systems in urban areas, which are always subjected to the presence of static and dynamic obstacles. Inspired by the collective behavior of tilapia fish and pigeons, the focus of the presented research is on the introduction of a nature-inspired collision-free formation control for a multi-UAV system, considering the obstacle avoidance maneuvers. The developed framework in this study utilizes a semi-distributed control approach, in which, based on a probabilistic Lloyd's algorithm, a centralized guidance algorithm works for optimal positioning of the UAVs, while a distributed control approach has been used for the intervehicle collision and obstacle avoidance. Further, the presented framework has been extended to the 3D space with a novel definition of 3D maneuvers. Finally, the presented framework has been applied to multi-UAV systems in 2D and 3D scenarios, and the obtained results demonstrated the validity of the presented method in dynamic environments with stationary and moving obstacles.", 'abstract_zh': '近期多无人机系统操控的进展展示了在城市区域内实施多无人机系统的需求日益增加，这些区域经常存在静态和动态障碍物。受到rays Saras鱼和鸽子群体行为的启发，本文研究的重点是介绍一种受自然启发的避免碰撞的多无人机编队控制方法，考虑了障碍物避障 maneuvers。本研究中开发的框架采用了半分布式控制方法，在此方法中，基于概率 Lloyd 算法的集中式引导算法用于实现无人机的理想定位，而分布式控制方法用于车辆间的碰撞及障碍物避障。此外，该框架扩展到三维空间，提出了三维 maneuvers 的新定义。最后，该框架应用于二维和三维的多无人机系统中，所得结果表明该方法在包含静态和移动障碍物的动态环境中是有效的。', 'title_zh': '基于鸽子启发的多无人机系统3D障碍检测与规避 maneuver 研究'}
{'arxiv_id': 'arXiv:2507.00435', 'title': 'RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation', 'authors': 'Yi Ru Wang, Carter Ung, Grant Tannert, Jiafei Duan, Josephine Li, Amy Le, Rishabh Oswal, Markus Grotz, Wilbert Pumacay, Yuquan Deng, Ranjay Krishna, Dieter Fox, Siddhartha Srinivasa', 'link': 'https://arxiv.org/abs/2507.00435', 'abstract': 'We present RoboEval, a simulation benchmark and structured evaluation framework designed to reveal the limitations of current bimanual manipulation policies. While prior benchmarks report only binary task success, we show that such metrics often conceal critical weaknesses in policy behavior -- such as poor coordination, slipping during grasping, or asymmetric arm usage. RoboEval introduces a suite of tiered, semantically grounded tasks decomposed into skill-specific stages, with variations that systematically challenge spatial, physical, and coordination capabilities. Tasks are paired with fine-grained diagnostic metrics and 3000+ human demonstrations to support imitation learning. Our experiments reveal that policies with similar success rates diverge in how tasks are executed -- some struggle with alignment, others with temporally consistent bimanual control. We find that behavioral metrics correlate with success in over half of task-metric pairs, and remain informative even when binary success saturates. By pinpointing when and how policies fail, RoboEval enables a deeper, more actionable understanding of robotic manipulation -- and highlights the need for evaluation tools that go beyond success alone.', 'abstract_zh': 'RoboEval: 一种揭示当前双臂操作策略局限性的模拟基准和结构化评估框架', 'title_zh': 'RoboEval: 基于结构化和可扩展评价的机器人 manipulation 测试'}
{'arxiv_id': 'arXiv:2507.00416', 'title': 'Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding', 'authors': 'Tao Lin, Gen Li, Yilei Zhong, Yanwen Zou, Bo Zhao', 'link': 'https://arxiv.org/abs/2507.00416', 'abstract': 'Vision-Language-Action (VLA) models have emerged as a promising framework for enabling generalist robots capable of perceiving, reasoning, and acting in the real world. These models usually build upon pretrained Vision-Language Models (VLMs), which excel at semantic understanding due to large-scale text pretraining. However, VLMs typically lack precise spatial understanding capabilities, as they are primarily tuned on 2D image-text pairs without 3D supervision. To address this limitation, recent approaches have incorporated explicit 3D inputs such as point clouds or depth maps, but this necessitates additional depth sensors or defective estimation. In contrast, our work introduces a plug-and-play module that implicitly injects 3D geometry features into VLA models by leveraging an off-the-shelf visual geometry foundation models. We design five spatially challenging tasks that require precise spatial understanding ability to validate effectiveness of our method. Extensive evaluations show that our method significantly improves the performance of state-of-the-art VLA models across diverse scenarios.', 'abstract_zh': 'Vision-Language-Action (VLA) 模型已成为一种有潜力的框架，用于实现能够在现实世界中感知、推理和行动的通用机器人。这些模型通常基于预训练的Vision-Language模型（VLMs），因大规模文本预训练而擅长语义理解。然而，VLMs通常缺乏精确的空间理解能力，因为它们主要是在缺乏3D监督的2D图像-文本配对上进行调整。为解决这一限制，最近的方法整合了显式的3D输入，如点云或深度图，但这需要额外的深度传感器或不准确的估计。相比之下，我们的工作引入了一个即插即用模块，通过利用现成的视觉几何基础模型隐式地将3D几何特征注入到VLA模型中。我们设计了五个需要精确空间理解能力的空间挑战任务，以验证我们方法的有效性。广泛的评估表明，我们的方法显著提高了最先进的VLA模型在各种场景中的性能。', 'title_zh': 'Evo-0：具有隐式空间理解的视觉-语言-动作模型'}
{'arxiv_id': 'arXiv:2507.00319', 'title': 'When Digital Twins Meet Large Language Models: Realistic, Interactive, and Editable Simulation for Autonomous Driving', 'authors': 'Tanmay Vilas Samak, Chinmay Vilas Samak, Bing Li, Venkat Krovi', 'link': 'https://arxiv.org/abs/2507.00319', 'abstract': 'Simulation frameworks have been key enablers for the development and validation of autonomous driving systems. However, existing methods struggle to comprehensively address the autonomy-oriented requirements of balancing: (i) dynamical fidelity, (ii) photorealistic rendering, (iii) context-relevant scenario orchestration, and (iv) real-time performance. To address these limitations, we present a unified framework for creating and curating high-fidelity digital twins to accelerate advancements in autonomous driving research. Our framework leverages a mix of physics-based and data-driven techniques for developing and simulating digital twins of autonomous vehicles and their operating environments. It is capable of reconstructing real-world scenes and assets (real2sim) with geometric and photorealistic accuracy and infusing them with various physical properties to enable real-time dynamical simulation of the ensuing driving scenarios. Additionally, it also incorporates a large language model (LLM) interface to flexibly edit the driving scenarios online via natural language prompts. We analyze the presented framework in terms of its fidelity, performance, and serviceability. Results indicate that our framework can reconstruct 3D scenes and assets with up to 97% structural similarity, while maintaining frame rates above 60 Hz. We also demonstrate that it can handle natural language prompts to generate diverse driving scenarios with up to 95% repeatability and 85% generalizability.', 'abstract_zh': '基于物理和数据驱动技术的统一框架：加速自主驾驶研究中的高保真数字孪生创建和优化', 'title_zh': '当数字孪生遇上大型语言模型：自动驾驶的现实、交互和可编辑模拟'}
{'arxiv_id': 'arXiv:2507.00273', 'title': 'Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation', 'authors': 'Yusuke Tanaka, Alvin Zhu, Quanyou Wang, Dennis Hong', 'link': 'https://arxiv.org/abs/2507.00273', 'abstract': "Reinforcement learning (RL) has enabled significant advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains. This omission can lead to inaccurate motion modeling and suboptimal policies, particularly for robots with high actuation complexity. This paper presents an end-to-end curriculum RL framework for BRUCE, a kid-sized humanoid robot featuring three distinct parallel mechanisms in its legs: a differential pulley, a 5-bar linkage, and a 4-bar linkage. Unlike prior approaches that rely on simplified serial approximations, we simulate all closed-chain constraints natively using GPU-accelerated MJX (MuJoCo), preserving the hardware's physical properties during training. We benchmark our RL approach against a Model Predictive Controller (MPC), demonstrating better surface generalization and performance in real-world zero-shot deployment. This work highlights the computational approaches and performance benefits of fully simulating parallel mechanisms in end-to-end learning pipelines for legged humanoids.", 'abstract_zh': '基于增强学习的BRUCE腿式人形机器人并联机构端到端课程学习框架', 'title_zh': '具有并行驱动的人形机器人面向机械智能的课程强化学习'}
{'arxiv_id': 'arXiv:2507.00268', 'title': 'Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems', 'authors': 'Oren Fivel, Matan Rudman, Kobi Cohen', 'link': 'https://arxiv.org/abs/2507.00268', 'abstract': "Deep reinforcement learning (DRL) has become a powerful tool for complex decision-making in machine learning and AI. However, traditional methods often assume perfect action execution, overlooking the uncertainties and deviations between an agent's selected actions and the actual system response. In real-world applications, such as robotics, mechatronics, and communication networks, execution mismatches arising from system dynamics, hardware constraints, and latency can significantly degrade performance. This work advances AI by developing a novel control-optimized DRL framework that explicitly models and compensates for action execution mismatches, a challenge largely overlooked in existing methods. Our approach establishes a structured two-stage process: determining the desired action and selecting the appropriate control signal to ensure proper execution. It trains the agent while accounting for action mismatches and controller corrections. By incorporating these factors into the training process, the AI agent optimizes the desired action with respect to both the actual control signal and the intended outcome, explicitly considering execution errors. This approach enhances robustness, ensuring that decision-making remains effective under real-world uncertainties. Our approach offers a substantial advancement for engineering practice by bridging the gap between idealized learning and real-world implementation. It equips intelligent agents operating in engineering environments with the ability to anticipate and adjust for actuation errors and system disturbances during training. We evaluate the framework in five widely used open-source mechanical simulation environments we restructured and developed to reflect real-world operating conditions, showcasing its robustness against uncertainties and offering a highly practical and efficient solution for control-oriented applications.", 'abstract_zh': '深度强化学习（DRL）已成為複雜決策機器學習和人工智能中的強大工具。然而，傳統方法往往假設動作執行完美的情況，忽略了代理選定動作與實際系統響應之間的不確定性和偏差。在實際應用中，例如機器人技術、機電系統和通訊網絡，由於系統動態、硬件限制和延遲等因素造成的動作執行失配，會顯著降低性能。本研究通過-developing a novel control-optimized DRL框架來提高人工智能，這種框架Explicitly建模并补偿动作执行失配，这一挑战在现有方法中被忽略。我们的方法建立了一个结构化的两阶段过程：确定期望的动作并选择合适的控制信号以确保正确执行。在训练过程中考虑了动作失配和控制器修正。通过将这些因素纳入训练过程，人工智能代理优化了期望动作，考虑到实际控制信号和预期结果，明确考虑执行错误。这种方法增强了鲁棒性，确保在现实世界的不确定性下决策仍然有效。我们的方法为工程实践提供了重大进步，通过填补理想化学习和实际实现之间的差距。它为在工程环境中操作的智能代理提供了在训练期间预测和调整执行错误和系统干扰的能力。我们在五个广泛使用的开源机械仿真环境中评估了该框架，这些环境是我们根据实际运行条件重新构建和开发的，展示了其在面对不确定性方面的鲁棒性，并提供了控制导向应用中的高度实用和高效的解决方案。', 'title_zh': '优化控制的深度强化学习在人工智能自主系统中的应用'}
{'arxiv_id': 'arXiv:2507.00236', 'title': 'Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving', 'authors': 'Chinmay Vilas Samak, Tanmay Vilas Samak, Bing Li, Venkat Krovi', 'link': 'https://arxiv.org/abs/2507.00236', 'abstract': 'Simulation-based design, optimization, and validation of autonomous driving algorithms have proven to be crucial for their iterative improvement over the years. Nevertheless, the ultimate measure of effectiveness is their successful transition from simulation to reality (sim2real). However, existing sim2real transfer methods struggle to comprehensively address the autonomy-oriented requirements of balancing: (i) conditioned domain adaptation, (ii) robust performance with limited examples, (iii) modularity in handling multiple domain representations, and (iv) real-time performance. To alleviate these pain points, we present a unified framework for learning cross-domain adaptive representations for sim2real transferable autonomous driving algorithms using conditional latent diffusion models. Our framework offers options to leverage: (i) alternate foundation models, (ii) a few-shot fine-tuning pipeline, and (iii) textual as well as image prompts for mapping across given source and target domains. It is also capable of generating diverse high-quality samples when diffusing across parameter spaces such as times of day, weather conditions, seasons, and operational design domains. We systematically analyze the presented framework and report our findings in the form of critical quantitative metrics and ablation studies, as well as insightful qualitative examples and remarks. Additionally, we demonstrate the serviceability of the proposed approach in bridging the sim2real gap for end-to-end autonomous driving using a behavioral cloning case study. Our experiments indicate that the proposed framework is capable of bridging the perceptual sim2real gap by over 40%. We hope that our approach underscores the potential of generative diffusion models in sim2real transfer, offering a pathway toward more robust and adaptive autonomous driving.', 'abstract_zh': '基于仿真训练、优化与验证的自主驾驶算法在多年发展中被证明至关重要。然而，其实现最终目标是从仿真到现实的有效过渡（sim2real）才是衡量其有效性的标准。现有sim2real转移方法难以全面解决自主性导向的需求，包括：（i）条件领域的适应性，（ii）有限样本下的鲁棒性能，（iii）对多种领域表示的模块化处理，以及（iv）实时性能。为缓解这些痛点，我们提出了一种统一框架，利用条件潜在扩散模型学习跨域自适应表示，以实现适用于sim2real转移的自主驾驶算法。该框架提供了利用（i）替代基础模型、（ii）少量样本的微调流水线以及（iii）文本和图像提示以跨越给定源域和目标域进行映射的选项。当沿时间、天气条件、季节和操作设计领域等参数空间扩散时，该框架还能够生成多样化的高质量样本。我们系统分析了所提出的框架，并以关键的定量指标、消融研究以及启发性的定性示例和注释的形式报告了我们的发现。此外，我们展示了该方法跨越端到端自主驾驶的sim2real差距的能力，通过行为克隆案例研究进行了验证。实验结果表明，所提出框架可将感知sim2real差距缩小超过40%。我们希望我们的方法强调生成式扩散模型在sim2real转移中的潜力，提供了一条通往更加鲁棒和适应型自主驾驶的途径。', 'title_zh': 'Sim2Real 蒸发：跨域自适应表示学习以实现可转移的自动驾驶'}
{'arxiv_id': 'arXiv:2507.00190', 'title': 'Rethink 3D Object Detection from Physical World', 'authors': 'Satoshi Tanaka, Koji Minoda, Fumiya Watanabe, Takamasa Horibe', 'link': 'https://arxiv.org/abs/2507.00190', 'abstract': 'High-accuracy and low-latency 3D object detection is essential for autonomous driving systems. While previous studies on 3D object detection often evaluate performance based on mean average precision (mAP) and latency, they typically fail to address the trade-off between speed and accuracy, such as 60.0 mAP at 100 ms vs 61.0 mAP at 500 ms. A quantitative assessment of the trade-offs between different hardware devices and accelerators remains unexplored, despite being critical for real-time applications. Furthermore, they overlook the impact on collision avoidance in motion planning, for example, 60.0 mAP leading to safer motion planning or 61.0 mAP leading to high-risk motion planning. In this paper, we introduce latency-aware AP (L-AP) and planning-aware AP (P-AP) as new metrics, which consider the physical world such as the concept of time and physical constraints, offering a more comprehensive evaluation for real-time 3D object detection. We demonstrate the effectiveness of our metrics for the entire autonomous driving system using nuPlan dataset, and evaluate 3D object detection models accounting for hardware differences and accelerators. We also develop a state-of-the-art performance model for real-time 3D object detection through latency-aware hyperparameter optimization (L-HPO) using our metrics. Additionally, we quantitatively demonstrate that the assumption "the more point clouds, the better the recognition performance" is incorrect for real-time applications and optimize both hardware and model selection using our metrics.', 'abstract_zh': '高精度低延迟的3D物体检测对于自主驾驶系统至关重要。虽然以往关于3D物体检测的研究通常基于均值平均精度（mAP）和延迟来评估性能，但它们通常未能解决速度与精度之间的权衡问题，例如60.0 mAP在100毫秒 vs 61.0 mAP在500毫秒。不同硬件设备和加速器之间的权衡量化评估至今未被探索，这一点对于实时应用至关重要。此外，它们忽略了运动规划中的碰撞避免影响，例如60.0 mAP可能导致更安全的运动规划或61.0 mAP可能导致高风险的运动规划。在本文中，我们介绍了延迟感知平均精度（L-AP）和规划感知平均精度（P-AP）作为新的评价指标，这些指标考虑了现实世界的物理概念和物理约束，为实时3D物体检测提供了更全面的评估方法。我们使用nuPlan数据集展示了我们指标在整套自主驾驶系统中的有效性，并评估了考虑硬件差异和加速器的3D物体检测模型。我们还通过我们指标的延迟感知超参数优化（L-HPO）开发了实时3D物体检测的最先进的性能模型。此外，我们定量展示了“点云越多，识别性能越好”的假设不适用于实时应用，并使用我们指标优化了硬件和模型选择。', 'title_zh': '重新思考物理世界中的3D目标检测'}
{'arxiv_id': 'arXiv:2507.00166', 'title': 'Novel Design of 3D Printed Tumbling Microrobots for in vivo Targeted Drug Delivery', 'authors': 'Aaron C. Davis, Siting Zhang, Adalyn Meeks, Diya Sakhrani, Luis Carlos Sanjuan Acosta, D. Ethan Kelley, Emma Caldwell, Luis Solorio, Craig J. Goergen, David J. Cappelleri', 'link': 'https://arxiv.org/abs/2507.00166', 'abstract': "This paper presents innovative designs for 3D-printed tumbling microrobots, specifically engineered for targeted in vivo drug delivery applications. The microrobot designs, created using stereolithography 3D printing technologies, incorporate permanent micro-magnets to enable actuation via a rotating magnetic field actuator system. The experimental framework encompasses a series of locomotion characterization tests to evaluate microrobot performance under various conditions. Testing variables include variations in microrobot geometries, actuation frequencies, and environmental conditions, such as dry and wet environments, and temperature changes. The paper outlines designs for three drug loading methods, along with comprehensive assessments thermal drug release using a focused ultrasound system, as well as biocompatibility tests. Animal model testing involves tissue phantoms and in vivo rat models, ensuring a thorough evaluation of the microrobots' performance and compatibility. The results highlight the robustness and adaptability of the proposed microrobot designs, showcasing the potential for efficient and targeted in vivo drug delivery. This novel approach addresses current limitations in existing tumbling microrobot designs and paves the way for advancements in targeted drug delivery within the large intestine.", 'abstract_zh': '本论文介绍了用于靶向体内药物传递应用的3D打印滚动微机器人创新设计。采用源自立体光刻3D打印技术的微机器人设计集成了永久微磁体，可通过旋转磁场激励系统实现操作。实验框架包括一系列运动特性测试，以评估微机器人在不同条件下的性能。测试变量包括微机器人几何形状的变化、激励频率以及环境条件，如干燥和湿润环境以及温度变化。论文概述了三种药物装载方法的设计，并进行了详细的聚焦超声系统下的热药物释放评估以及生物相容性测试。动物模型测试使用组织模拟物和活体大鼠模型，确保对微机器人的性能和兼容性进行全面评估。结果突显了所提出微机器人设计的 robustness 和适应性，展示了其在体内高效和靶向药物传递方面的潜力。该新颖方法解决了现有滚动微机器人设计的局限性，为进一步推动大肠靶向药物传递技术的发展铺平了道路。', 'title_zh': '3D打印旋转微机器人新型设计及其体内靶向药物递送应用'}
{'arxiv_id': 'arXiv:2507.00963', 'title': 'Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception', 'authors': 'Fan Wang, Giulia Perugia, Yuan Feng, Wijnand IJsselsteijn', 'link': 'https://arxiv.org/abs/2507.00963', 'abstract': "As social robots increasingly enter dementia care, concerns about deception, intentional or not, are gaining attention. Yet, how robotic design cues might elicit misleading perceptions in people with dementia, and how these perceptions arise, remains insufficiently understood. In this scoping review, we examined 26 empirical studies on interactions between people with dementia and physical social robots. We identify four key design cue categories that may influence deceptive impressions: cues resembling physiological signs (e.g., simulated breathing), social intentions (e.g., playful movement), familiar beings (e.g., animal-like form and sound), and, to a lesser extent, cues that reveal artificiality. Thematic analysis of user responses reveals that people with dementia often attribute biological, social, and mental capacities to robots, dynamically shifting between awareness and illusion. These findings underscore the fluctuating nature of ontological perception in dementia contexts. Existing definitions of robotic deception often rest on philosophical or behaviorist premises, but rarely engage with the cognitive mechanisms involved. We propose an empirically grounded definition: robotic deception occurs when Type 1 (automatic, heuristic) processing dominates over Type 2 (deliberative, analytic) reasoning, leading to misinterpretation of a robot's artificial nature. This dual-process perspective highlights the ethical complexity of social robots in dementia care and calls for design approaches that are not only engaging, but also epistemically respectful.", 'abstract_zh': '随着社会机器人越来越多地进入痴呆症护理领域，关于欺骗（故意与否）的关注度正在增加。然而，关于机器人设计线索如何引发痴呆症患者产生误导性感知以及这些感知是如何产生的理解仍然不足。在本综述研究中，我们探讨了26项关于痴呆症患者与物理社会机器人互动的研究。我们确定了四个可能影响欺骗性印象的关键设计线索类别：模拟生理征象的线索（如模拟呼吸）、社会意图（如嬉戏的动作）、熟悉的生物形象（如类动物的外形和声音），以及较少的揭示人工性的线索。对用户反馈的主题分析表明，痴呆症患者经常将生物学、社会和心理能力归因于机器人，并在意识与错觉之间动态转变。这些发现强调了痴呆症情境下本体感知的波动性。现有的关于机器人欺骗的定义往往基于哲学或行为主义的假设，但很少涉及认知机制。我们提出一个基于实证的方法论定义：当第一型（自动的、启发式的）处理过程主导了第二型（反思的、分析性的）推理，导致对机器人人工性的误读时，就发生了机器人欺骗。这种双重过程视角突出了社会机器人在痴呆症护理中的伦理复杂性，并呼吁设计方法不仅要有吸引力，还要具有认知尊重。', 'title_zh': '痴呆症患者的社会机器人：从设计到感知的欺骗现象文献综述'}
{'arxiv_id': 'arXiv:2507.00886', 'title': 'GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond', 'authors': 'Anna-Maria Halacheva, Jan-Nico Zaech, Xi Wang, Danda Pani Paudel, Luc Van Gool', 'link': 'https://arxiv.org/abs/2507.00886', 'abstract': 'As multimodal language models advance, their application to 3D scene understanding is a fast-growing frontier, driving the development of 3D Vision-Language Models (VLMs). Current methods show strong dependence on object detectors, introducing processing bottlenecks and limitations in taxonomic flexibility. To address these limitations, we propose a scene-centric 3D VLM for 3D Gaussian splat scenes that employs language- and task-aware scene representations. Our approach directly embeds rich linguistic features into the 3D scene representation by associating language with each Gaussian primitive, achieving early modality alignment. To process the resulting dense representations, we introduce a dual sparsifier that distills them into compact, task-relevant tokens via task-guided and location-guided pathways, producing sparse, task-aware global and local scene tokens. Notably, we present the first Gaussian splatting-based VLM, leveraging photorealistic 3D representations derived from standard RGB images, demonstrating strong generalization: it improves performance of prior 3D VLM five folds, in out-of-the-domain settings.', 'abstract_zh': '面向3D场景的以场景为中心的Gaussian splating语言-视觉模型', 'title_zh': 'GaussianVLM：以场景为中心的3D视觉语言模型，采用语言对齐的高斯斑点进行嵌入式推理和超越领域'}
{'arxiv_id': 'arXiv:2507.00756', 'title': 'Towards Open-World Human Action Segmentation Using Graph Convolutional Networks', 'authors': 'Hao Xing, Kai Zhe Boey, Gordon Cheng', 'link': 'https://arxiv.org/abs/2507.00756', 'abstract': 'Human-object interaction segmentation is a fundamental task of daily activity understanding, which plays a crucial role in applications such as assistive robotics, healthcare, and autonomous systems. Most existing learning-based methods excel in closed-world action segmentation, they struggle to generalize to open-world scenarios where novel actions emerge. Collecting exhaustive action categories for training is impractical due to the dynamic diversity of human activities, necessitating models that detect and segment out-of-distribution actions without manual annotation. To address this issue, we formally define the open-world action segmentation problem and propose a structured framework for detecting and segmenting unseen actions. Our framework introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional Network (EPGCN) with a novel decoder module for robust spatiotemporal feature upsampling. 2) Mixup-based training to synthesize out-of-distribution data, eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss that groups in-distribution actions while distancing out-of-distribution samples.\nWe evaluate our framework on two challenging human-object interaction recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets. Experimental results demonstrate significant improvements over state-of-the-art action segmentation models across multiple open-set evaluation metrics, achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and out-of-distribution detection performances (AUROC), respectively. Additionally, we conduct an in-depth ablation study to assess the impact of each proposed component, identifying the optimal framework configuration for open-world action segmentation.', 'abstract_zh': '开放世界人-物交互分割：一种日常活动理解的基础任务，对于辅助机器人、医疗健康和自主系统等应用至关重要。现有的基于学习的方法在封闭世界动作分割方面表现出色，但在开放世界场景中难以泛化，其中新动作不断出现。由于人类活动的动态多样性，收集完整的动作类别进行训练是不切实际的，因此需要能够检测和分割未标注动作的模型。为此，我们正式定义开放世界动作分割问题，并提出了一种结构化框架以检测和分割未见过的动作。该框架引入了三项关键创新：1）增强的分层图卷积网络（EPGCN），配有新颖的解码器模块，用于稳健的空间-时间特征上采样。2）基于Mixup的训练方法，合成未标注数据，减少对人工标注的依赖。3）一种新颖的时间聚类损失，用于将已知动作分组，并将未知样本区分开来。我们在两个具有挑战性的手-物交互识别数据集Bimanual Actions and 2 Hands and Object (H2O)上评估了我们的框架。实验结果表明，与最先进的动作分割模型相比，在多个开放集评估指标上取得了显著改进，在开放集分割（F1@50）和未标注检测性能（AUROC）方面分别实现了16.9%和34.6%的相对提升。此外，我们还进行了深入的消融研究，评估了每个提议组件的影响，并确定了开放世界动作分割的最佳框架配置。', 'title_zh': '基于图卷积网络的开放世界人体动作分割'}
{'arxiv_id': 'arXiv:2507.00752', 'title': 'Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation', 'authors': 'Hao Xing, Kai Zhe Boey, Yuankai Wu, Darius Burschka, Gordon Cheng', 'link': 'https://arxiv.org/abs/2507.00752', 'abstract': 'Accurate temporal segmentation of human actions is critical for intelligent robots in collaborative settings, where a precise understanding of sub-activity labels and their temporal structure is essential. However, the inherent noise in both human pose estimation and object detection often leads to over-segmentation errors, disrupting the coherence of action sequences. To address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g., 30 fps) motion data (skeleton and object detections) to mitigate fragmentation. Our framework introduces three key contributions. First, a sinusoidal encoding strategy that maps 3D skeleton coordinates into a continuous sin-cos space to enhance spatial representation robustness. Second, a temporal graph fusion module that aligns multi-modal inputs with differing resolutions via hierarchical feature aggregation, Third, inspired by the smooth transitions inherent to human actions, we design SmoothLabelMix, a data augmentation technique that mixes input sequences and labels to generate synthetic training examples with gradual action transitions, enhancing temporal consistency in predictions and reducing over-segmentation artifacts.\nExtensive experiments on the Bimanual Actions Dataset, a public benchmark for human-object interaction understanding, demonstrate that our approach outperforms state-of-the-art methods, especially in action segmentation accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.', 'abstract_zh': '多模态图卷积网络在人体动作时间分割中的应用：一种集成低帧率视觉数据与高帧率motion数据的方法', 'title_zh': '带有正弦编码的多模态图卷积网络在鲁棒人体动作分割中的应用'}
{'arxiv_id': 'arXiv:2507.00669', 'title': 'Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding', 'authors': 'Duc Cao-Dinh, Khai Le-Duc, Anh Dao, Bach Phan Tat, Chris Ngo, Duy M. H. Nguyen, Nguyen X. Khanh, Thanh Nguyen-Tang', 'link': 'https://arxiv.org/abs/2507.00669', 'abstract': '3D Visual Grounding (3DVG) involves localizing target objects in 3D point clouds based on natural language. While prior work has made strides using textual descriptions, leveraging spoken language-known as Audio-based 3D Visual Grounding-remains underexplored and challenging. Motivated by advances in automatic speech recognition (ASR) and speech representation learning, we propose Audio-3DVG, a simple yet effective framework that integrates audio and spatial information for enhanced grounding. Rather than treating speech as a monolithic input, we decompose the task into two complementary components. First, we introduce Object Mention Detection, a multi-label classification task that explicitly identifies which objects are referred to in the audio, enabling more structured audio-scene reasoning. Second, we propose an Audio-Guided Attention module that captures interactions between candidate objects and relational speech cues, improving target discrimination in cluttered scenes. To support benchmarking, we synthesize audio descriptions for standard 3DVG datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate that Audio-3DVG not only achieves new state-of-the-art performance in audio-based grounding, but also competes with text-based methods-highlighting the promise of integrating spoken language into 3D vision tasks.', 'abstract_zh': '基于音频的三维视觉定位（Audio-3D Visual Grounding）：一种结合音频和空间信息的简单有效框架', 'title_zh': 'Audio-3DVG：统一的音频-点云融合三维视觉定位'}
{'arxiv_id': 'arXiv:2507.00611', 'title': 'Residual Reward Models for Preference-based Reinforcement Learning', 'authors': 'Chenyang Cao, Miguel Rogel-García, Mohamed Nabail, Xueqian Wang, Nicholas Rhinehart', 'link': 'https://arxiv.org/abs/2507.00611', 'abstract': "Preference-based Reinforcement Learning (PbRL) provides a way to learn high-performance policies in environments where the reward signal is hard to specify, avoiding heuristic and time-consuming reward design. However, PbRL can suffer from slow convergence speed since it requires training in a reward model. Prior work has proposed learning a reward model from demonstrations and fine-tuning it using preferences. However, when the model is a neural network, using different loss functions for pre-training and fine-tuning can pose challenges to reliable optimization. In this paper, we propose a method to effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM assumes that the true reward of the environment can be split into a sum of two parts: a prior reward and a learned reward. The prior reward is a term available before training, for example, a user's ``best guess'' reward function, or a reward function learned from inverse reinforcement learning (IRL), and the learned reward is trained with preferences. We introduce state-based and image-based versions of RRM and evaluate them on several tasks in the Meta-World environment suite. Experimental results show that our method substantially improves the performance of a common PbRL method. Our method achieves performance improvements for a variety of different types of prior rewards, including proxy rewards, a reward obtained from IRL, and even a negated version of the proxy reward. We also conduct experiments with a Franka Panda to show that our method leads to superior performance on a real robot. It significantly accelerates policy learning for different tasks, achieving success in fewer steps than the baseline. The videos are presented at this https URL.", 'abstract_zh': '基于偏好强化学习的残差奖励模型：一种有效利用先验知识的方法', 'title_zh': '基于偏好强化学习的残差奖励模型'}
{'arxiv_id': 'arXiv:2507.00271', 'title': 'User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the "Sunday Blues"', 'authors': 'Zhuochao Peng, Jiaxin Xu, Jun Hu, Haian Xue, Laurens A. G. Kolks, Pieter M. A. Desmet', 'link': 'https://arxiv.org/abs/2507.00271', 'abstract': 'While recent research highlights the potential of social robots to support mood regulation, little is known about how prospective users view their integration into everyday life. To explore this, we conducted an exploratory case study that used a speculative robot concept "Mora" to provoke reflection and facilitate meaningful discussion about using social robots to manage subtle, day-to-day emotional experiences. We focused on the "Sunday Blues," a common dip in mood that occurs at the end of the weekend, as a relatable context in which to explore individuals\' insights. Using a video prototype and a co-constructing stories method, we engaged 15 participants in imagining interactions with Mora and discussing their expectations, doubts, and concerns. The study surfaced a range of nuanced reflections around the attributes of social robots like empathy, intervention effectiveness, and ethical boundaries, which we translated into design considerations for future research and development in human-robot interaction.', 'abstract_zh': '虽然最近的研究强调了社交机器人在情绪调节方面潜在的应用价值，但人们对社交机器人融入日常生活的方式知之甚少。为了探索这一问题，我们通过使用一种推测性机器人概念“Mora”开展探索性案例研究，旨在激发反思并促进关于如何使用社交机器人来管理日常生活中的微妙情感体验的有意义讨论。我们将“周末 Blues”作为可共鸣的情境，以此探索个体的见解。我们利用视频原型和共构故事的方法，与15名参与者一起设想与Mora的互动，并讨论他们的期望、疑虑和担忧。研究揭示了关于社交机器人属性（如同理心、干预效果和伦理界限）的一系列细腻反思，并将其转化为未来人机交互研究和开发中的设计考虑。', 'title_zh': '用户对用于情绪调节的社会机器人的关注：以“周日 blues”为例'}
{'arxiv_id': 'arXiv:2507.00209', 'title': 'SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures', 'authors': 'Fengyi Jiang, Xiaorui Zhang, Lingbo Jin, Ruixing Liang, Yuxin Chen, Adi Chola Venkatesh, Jason Culman, Tiantian Wu, Lirong Shao, Wenqing Sun, Cong Gao, Hallie McNamara, Jingpei Lu, Omid Mohareri', 'link': 'https://arxiv.org/abs/2507.00209', 'abstract': 'High-resolution imaging is crucial for enhancing visual clarity and enabling precise computer-assisted guidance in minimally invasive surgery (MIS). Despite the increasing adoption of 4K endoscopic systems, there remains a significant gap in publicly available native 4K datasets tailored specifically for robotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible surgical imaging and video dataset captured at a native 4K resolution, representing realistic conditions of robotic-assisted procedures. SurgiSR4K comprises diverse visual scenarios including specular reflections, tool occlusions, bleeding, and soft tissue deformations, meticulously designed to reflect common challenges faced during laparoscopic and robotic surgeries. This dataset opens up possibilities for a broad range of computer vision tasks that might benefit from high resolution data, such as super resolution (SR), smoke removal, surgical instrument detection, 3D tissue reconstruction, monocular depth estimation, instance segmentation, novel view synthesis, and vision-language model (VLM) development. SurgiSR4K provides a robust foundation for advancing research in high-resolution surgical imaging and fosters the development of intelligent imaging technologies aimed at enhancing performance, safety, and usability in image-guided robotic surgeries.', 'abstract_zh': '高分辨率成像对于增强视觉清晰度并助力精确的计算机辅助指导在微创手术（MIS）中至关重要。尽管4K内窥镜系统逐渐被采用，但专门用于机器人辅助MIS的公有原生4K数据集仍然存在显著差距。我们介绍了SurgiSR4K，这是首个可公开访问的原始4K分辨率的手术成像和视频数据集，代表了机器人辅助手术的真实条件。SurgiSR4K 包括多种视觉场景，如反射、工具遮挡、出血和软组织变形，精心设计以反映腹腔镜和机器人手术中常见的挑战。该数据集为需要高分辨率数据的各种计算机视觉任务打开了可能性，如超分辨率（SR）、烟雾去除、手术器械检测、3D组织重建、单目深度估计、实例分割、新颖视图合成及视觉语言模型（VLM）开发。SurgiSR4K 为推进高分辨率手术成像研究提供了坚实的基础，并促进了旨在提高图像引导机器人手术性能、安全性和用户体验的智能成像技术的发展。', 'title_zh': 'SurgiSR4K: 一种用于机器人辅助微创手术的高分辨率内镜视频数据集'}
{'arxiv_id': 'arXiv:2507.00076', 'title': 'Time Invariant Sensor Tasking for Catalog Maintenance of LEO Space objects using Stochastic Geometry', 'authors': 'Partha Chowdhury, Harsha M, Chinni Prabhunath Georg, Arun Balaji Buduru, Sanat K Biswas', 'link': 'https://arxiv.org/abs/2507.00076', 'abstract': 'Catalog maintenance of space objects by limited number of ground-based sensors presents a formidable challenging task to the space community. This article presents a methodology for time-invariant tracking and surveillance of space objects in low Earth orbit (LEO) by optimally directing ground sensors. Our methodology aims to maximize the expected number of space objects from a set of ground stations by utilizing concepts from stochastic geometry, particularly the Poisson point process. We have provided a systematic framework to understand visibility patterns and enhance the efficiency of tracking multiple objects simultaneously. Our approach contributes to more informed decision-making in space operations, ultimately supporting efforts to maintain safety and sustainability in LEO.', 'abstract_zh': '基于有限地面传感器的天基物体.catalog维护 presents a formidable challenging task to the space community. This article presents a methodology for time-invariant tracking and surveillance of space objects in low Earth orbit (LEO) by optimally directing ground sensors.', 'title_zh': '使用随机几何的LEO太空碎片catalog维护时间不变传感器任务规划'}
