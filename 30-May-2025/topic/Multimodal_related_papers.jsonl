{'arxiv_id': 'arXiv:2505.23745', 'title': "To Trust Or Not To Trust Your Vision-Language Model's Prediction", 'authors': 'Hao Dong, Moru Liu, Jian Liang, Eleni Chatzi, Olga Fink', 'link': 'https://arxiv.org/abs/2505.23745', 'abstract': "Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at this https URL.", 'abstract_zh': 'Vision-Language 模型 (VLMs) 在对接视觉和文本模态方面展现了强大的能力，使其在多模态理解和生成方面具备广泛的应用潜力。尽管它们在零样本学习和迁移学习场景中表现优异，但 VLMs 在分类时仍易出错，经常产生高置信度但错误的预测。这一局限性在安全关键领域造成了重大风险，错误的预测可能导致严重后果。在本文中，我们提出了 TrustVLM，这是一种无需训练的框架，旨在解决 VLM 预测可信度估计的关键挑战。基于对 VLMs 所观察到的模态差距的理解，以及某些概念在图像嵌入空间中更鲜明的表现，我们提出了一种新的置信度评分函数，利用这一空间来提高误分类检测能力。我们在 17 个不同的数据集上进行了严格评估，使用了 4 种架构和 2 种 VLMs，并展示了最先进的性能，相对于现有基线，AURC 提高了最多 51.87%，AUROC 提高了 9.14%，FPR95 提高了 32.42%。通过在无需重新训练的情况下提高模型的可靠性，TrustVLM 打开了在实际应用中安全部署 VLMs 的大门。代码将在以下链接获取：这个 https URL。', 'title_zh': '是信任还是不信任你的视觉-语言模型的预测'}
{'arxiv_id': 'arXiv:2505.23693', 'title': 'VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos', 'authors': 'Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao', 'link': 'https://arxiv.org/abs/2505.23693', 'abstract': 'MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation.', 'abstract_zh': 'MLLMs在合成视频理解上的评估：VF-Eval基准', 'title_zh': 'VF-Eval: 评估多模态大语言模型生成AIGC视频反馈的能力'}
{'arxiv_id': 'arXiv:2505.23308', 'title': 'Spoken question answering for visual queries', 'authors': 'Nimrod Shabtay, Zvi Kons, Avihu Dekel, Hagai Aronowitz, Ron Hoory, Assaf Arbelle', 'link': 'https://arxiv.org/abs/2505.23308', 'abstract': 'Question answering (QA) systems are designed to answer natural language questions. Visual QA (VQA) and Spoken QA (SQA) systems extend the textual QA system to accept visual and spoken input respectively.\nThis work aims to create a system that enables user interaction through both speech and images. That is achieved through the fusion of text, speech, and image modalities to tackle the task of spoken VQA (SVQA). The resulting multi-modal model has textual, visual, and spoken inputs and can answer spoken questions on images.\nTraining and evaluating SVQA models requires a dataset for all three modalities, but no such dataset currently exists. We address this problem by synthesizing VQA datasets using two zero-shot TTS models. Our initial findings indicate that a model trained only with synthesized speech nearly reaches the performance of the upper-bounding model trained on textual QAs. In addition, we show that the choice of the TTS model has a minor impact on accuracy.', 'abstract_zh': '视觉问答（VQA）和口语问答（SQA）系统扩展了文本问答系统，分别接受视觉和口语输入。本工作旨在通过结合语音和图像交互创建一个系统，该系统通过融合文本、语音和图像模态来解决口语VQA（SVQA）任务。最终的多模态模型接受文本、视觉和口语输入，并能对图像上的口语问题进行回答。训练和评估SVQA模型需要包含所有三种模态的数据集，但当前并不存在这样的数据集。我们通过使用两个零样本TTS模型合成了VQA数据集来解决这个问题。初步研究表明，仅使用合成语音训练的模型几乎达到了基于文本问答训练的上界模型的性能。此外，我们还表明，TTS模型的选择对准确性的影响较小。', 'title_zh': '基于口语的视觉查询问答'}
{'arxiv_id': 'arXiv:2505.23268', 'title': 'Unsupervised Transcript-assisted Video Summarization and Highlight Detection', 'authors': 'Spyros Barbakos, Charalampos Antoniadis, Gerasimos Potamianos, Gianluca Setti', 'link': 'https://arxiv.org/abs/2505.23268', 'abstract': 'Video consumption is a key part of daily life, but watching entire videos can be tedious. To address this, researchers have explored video summarization and highlight detection to identify key video segments. While some works combine video frames and transcripts, and others tackle video summarization and highlight detection using Reinforcement Learning (RL), no existing work, to the best of our knowledge, integrates both modalities within an RL framework. In this paper, we propose a multimodal pipeline that leverages video frames and their corresponding transcripts to generate a more condensed version of the video and detect highlights using a modality fusion mechanism. The pipeline is trained within an RL framework, which rewards the model for generating diverse and representative summaries while ensuring the inclusion of video segments with meaningful transcript content. The unsupervised nature of the training allows for learning from large-scale unannotated datasets, overcoming the challenge posed by the limited size of existing annotated datasets. Our experiments show that using the transcript in video summarization and highlight detection achieves superior results compared to relying solely on the visual content of the video.', 'abstract_zh': '视频消费是日常生活的关键部分，但观看完整视频可能会很乏味。为了应对这一问题，研究人员探讨了视频总结和亮点检测，以识别关键视频片段。虽然有些工作结合了视频帧和字幕，而另一些工作则使用强化学习（RL）进行视频总结和亮点检测，但据我们所知，现有工作未在RL框架内整合这两种模态。本文提出了一种多模态管道，利用视频帧及其对应的字幕生成更浓缩的视频版本，并使用模态融合机制检测亮点。该管道在RL框架下训练，奖励模型生成多样化且具代表性的总结，同时确保包括具有重要意义字幕内容的视频片段。无监督的训练方式可以从大规模未标注数据集中学习，克服现有标注数据集规模有限带来的挑战。我们的实验表明，在视频总结和亮点检测中使用字幕相比于仅依赖视频的视觉内容取得了更好的结果。', 'title_zh': '无监督的转录辅助视频摘要与亮点检测'}
