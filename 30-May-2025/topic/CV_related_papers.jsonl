{'arxiv_id': 'arXiv:2505.23612', 'title': 'Autoregressive Meta-Actions for Unified Controllable Trajectory Generation', 'authors': 'Jianbo Zhao, Taiyu Ban, Xiyang Wang, Qibin Zhou, Hangning Zhou, Zhihao Liu, Mu Yang, Lei Liu, Bin Li', 'link': 'https://arxiv.org/abs/2505.23612', 'abstract': "Controllable trajectory generation guided by high-level semantic decisions, termed meta-actions, is crucial for autonomous driving systems. A significant limitation of existing frameworks is their reliance on invariant meta-actions assigned over fixed future time intervals, causing temporal misalignment with the actual behavior trajectories. This misalignment leads to irrelevant associations between the prescribed meta-actions and the resulting trajectories, disrupting task coherence and limiting model performance. To address this challenge, we introduce Autoregressive Meta-Actions, an approach integrated into autoregressive trajectory generation frameworks that provides a unified and precise definition for meta-action-conditioned trajectory prediction. Specifically, We decompose traditional long-interval meta-actions into frame-level meta-actions, enabling a sequential interplay between autoregressive meta-action prediction and meta-action-conditioned trajectory generation. This decomposition ensures strict alignment between each trajectory segment and its corresponding meta-action, achieving a consistent and unified task formulation across the entire trajectory span and significantly reducing complexity. Moreover, we propose a staged pre-training process to decouple the learning of basic motion dynamics from the integration of high-level decision control, which offers flexibility, stability, and modularity. Experimental results validate our framework's effectiveness, demonstrating improved trajectory adaptivity and responsiveness to dynamic decision-making scenarios. We provide the video document and dataset, which are available at this https URL.", 'abstract_zh': '基于高层语义决策引导的可控轨迹生成对于自主驾驶系统至关重要。现有框架的重大局限性在于其依赖于固定未来时间间隔内的不变高层语义动作，导致与实际行为轨迹时间上的不一致。这种不一致导致规定的高层语义动作与生成的轨迹之间的无关关联，破坏了任务连贯性并限制了模型性能。为解决这一挑战，我们提出了一种名为自回归高层语义动作的方法，该方法整合到自回归轨迹生成框架中，为条件于高层语义动作的轨迹预测提供了统一且精确的定义。具体而言，我们将传统的长期间隔内的高层语义动作分解为帧级高层语义动作，使自回归高层语义动作预测与条件于高层语义动作的轨迹生成之间形成顺序交互。这种分解确保了每个轨迹片段与其对应的高层语义动作之间的严格对齐，在整个轨迹跨度内实现了任务表述的一致性和统一性，并显著减少了复杂性。此外，我们提出了一种分阶段预训练过程，将基本运动动力学的学习与高层次决策控制的整合解耦，提供了灵活性、稳定性和模块性。实验结果验证了我们框架的有效性，展示了在动态决策场景中的轨迹适应性和响应性改进。我们提供了视频文档和数据集，可在以下网址获取：this https URL。', 'title_zh': '自回归元任务用于统一可控轨迹生成'}
{'arxiv_id': 'arXiv:2505.22882', 'title': 'TwinTrack: Bridging Vision and Contact Physics for Real-Time Tracking of Unknown Dynamic Objects', 'authors': 'Wen Yang, Zhixian Xie, Xuechao Zhang, Heni Ben Amor, Shan Lin, Wanxin Jin', 'link': 'https://arxiv.org/abs/2505.22882', 'abstract': "Real-time tracking of previously unseen, highly dynamic objects in contact-rich environments -- such as during dexterous in-hand manipulation -- remains a significant challenge. Purely vision-based tracking often suffers from heavy occlusions due to the frequent contact interactions and motion blur caused by abrupt motion during contact impacts. We propose TwinTrack, a physics-aware visual tracking framework that enables robust and real-time 6-DoF pose tracking of unknown dynamic objects in a contact-rich scene by leveraging the contact physics of the observed scene. At the core of TwinTrack is an integration of Real2Sim and Sim2Real. In Real2Sim, we combine the complementary strengths of vision and contact physics to estimate object's collision geometry and physical properties: object's geometry is first reconstructed from vision, then updated along with other physical parameters from contact dynamics for physical accuracy. In Sim2Real, robust pose estimation of the object is achieved by adaptive fusion between visual tracking and prediction of the learned contact physics. TwinTrack is built on a GPU-accelerated, deeply customized physics engine to ensure real-time performance. We evaluate our method on two contact-rich scenarios: object falling with rich contact impacts against the environment, and contact-rich in-hand manipulation. Experimental results demonstrate that, compared to baseline methods, TwinTrack achieves significantly more robust, accurate, and real-time 6-DoF tracking in these challenging scenarios, with tracking speed exceeding 20 Hz. Project page: this https URL", 'abstract_zh': '基于物理的视觉跟踪框架：孪生跟踪在接触丰富的环境中实现未知动态物体的鲁棒性实时6自由度位姿跟踪', 'title_zh': 'TwinTrack: 融合视觉与接触物理实现未知动态目标的实时跟踪'}
{'arxiv_id': 'arXiv:2505.22805', 'title': 'Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation', 'authors': 'Siddharth Ancha, Sunshine Jiang, Travis Manderson, Laura Brandt, Yilun Du, Philip R. Osteen, Nicholas Roy', 'link': 'https://arxiv.org/abs/2505.22805', 'abstract': 'In order to navigate safely and reliably in off-road and unstructured environments, robots must detect anomalies that are out-of-distribution (OOD) with respect to the training data. We present an analysis-by-synthesis approach for pixel-wise anomaly detection without making any assumptions about the nature of OOD data. Given an input image, we use a generative diffusion model to synthesize an edited image that removes anomalies while keeping the remaining image unchanged. Then, we formulate anomaly detection as analyzing which image segments were modified by the diffusion model. We propose a novel inference approach for guided diffusion by analyzing the ideal guidance gradient and deriving a principled approximation that bootstraps the diffusion model to predict guidance gradients. Our editing technique is purely test-time that can be integrated into existing workflows without the need for retraining or fine-tuning. Finally, we use a combination of vision-language foundation models to compare pixels in a learned feature space and detect semantically meaningful edits, enabling accurate anomaly detection for off-road navigation. Project website: this https URL', 'abstract_zh': '为了在非道路和未结构化环境中安全可靠地导航，机器人必须检测与训练数据分布之外的异常。我们提出了一种生成合成方法进行像素级异常检测，无需假设OOD数据的性质。给定输入图像，我们使用生成式扩散模型合成一个编辑后的图像，移除异常同时保持剩余图像不变。然后，我们将异常检测形式化为分析扩散模型修改了哪些图像片段。我们提出了一种新的指导扩散推理方法，通过分析理想的指导梯度并推导出一种原则性的近似方法，引导扩散模型预测指导梯度。我们的编辑技术完全是测试时实现的，可以不需重新训练或微调就集成到现有工作流程中。最后，我们使用一组视觉-语言基础模型在学习特征空间中比较像素，检测语义上有意义的编辑，从而实现准确的异常检测以支持非道路导航。项目网站：this https URL。', 'title_zh': '合成异常：用于离路导航的生成性扩散模型异常检测'}
{'arxiv_id': 'arXiv:2505.23399', 'title': 'GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning', 'authors': 'Jusheng Zhang, Yijia Fan, Wenjun Lin, Ruiqi Chen, Haoyi Jiang, Wenhao Chai, Jian Wang, Keze Wang', 'link': 'https://arxiv.org/abs/2505.23399', 'abstract': 'We propose GAM-Agent, a game-theoretic multi-agent framework for enhancing vision-language reasoning. Unlike prior single-agent or monolithic models, GAM-Agent formulates the reasoning process as a non-zero-sum game between base agents--each specializing in visual perception subtasks--and a critical agent that verifies logic consistency and factual correctness. Agents communicate via structured claims, evidence, and uncertainty estimates. The framework introduces an uncertainty-aware controller to dynamically adjust agent collaboration, triggering multi-round debates when disagreement or ambiguity is detected. This process yields more robust and interpretable predictions. Experiments on four challenging benchmarks--MMMU, MMBench, MVBench, and V*Bench--demonstrate that GAM-Agent significantly improves performance across various VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid scale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5--6\\%, and still enhances strong models like GPT-4o by up to 2--3\\%. Our approach is modular, scalable, and generalizable, offering a path toward reliable and explainable multi-agent multimodal reasoning.', 'abstract_zh': 'GAM-Agent：一种基于博弈论的多智能体框架以增强视觉-语言推理', 'title_zh': 'GAM-Agent: 基于博弈论和不确定性感知的复杂视觉推理协作'}
{'arxiv_id': 'arXiv:2505.23759', 'title': "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint", 'authors': 'Heekyung Lee, Jiaxin Ge, Tsung-Han Wu, Minwoo Kang, Trevor Darrell, David M. Chan', 'link': 'https://arxiv.org/abs/2505.23759', 'abstract': 'Rebus puzzles, visual riddles that encode language through imagery, spatial arrangement, and symbolic substitution, pose a unique challenge to current vision-language models (VLMs). Unlike traditional image captioning or question answering tasks, rebus solving requires multi-modal abstraction, symbolic reasoning, and a grasp of cultural, phonetic and linguistic puns. In this paper, we investigate the capacity of contemporary VLMs to interpret and solve rebus puzzles by constructing a hand-generated and annotated benchmark of diverse English-language rebus puzzles, ranging from simple pictographic substitutions to spatially-dependent cues ("head" over "heels"). We analyze how different VLMs perform, and our findings reveal that while VLMs exhibit some surprising capabilities in decoding simple visual clues, they struggle significantly with tasks requiring abstract reasoning, lateral thinking, and understanding visual metaphors.', 'abstract_zh': '图像谜题：通过图像、空间布局和象征替代编码语言的视觉谜题，对当前视觉-语言模型（VLMs）构成了独特的挑战。', 'title_zh': '困惑于谜题：当视觉-语言模型无法得到提示时'}
{'arxiv_id': 'arXiv:2505.23751', 'title': 'REOrdering Patches Improves Vision Models', 'authors': 'Declan Kutscher, David M. Chan, Yutong Bai, Trevor Darrell, Ritwik Gupta', 'link': 'https://arxiv.org/abs/2505.23751', 'abstract': 'Sequence models such as transformers require inputs to be represented as one-dimensional sequences. In vision, this typically involves flattening images using a fixed row-major (raster-scan) order. While full self-attention is permutation-equivariant, modern long-sequence transformers increasingly rely on architectural approximations that break this invariance and introduce sensitivity to patch ordering. We show that patch order significantly affects model performance in such settings, with simple alternatives like column-major or Hilbert curves yielding notable accuracy shifts. Motivated by this, we propose REOrder, a two-stage framework for discovering task-optimal patch orderings. First, we derive an information-theoretic prior by evaluating the compressibility of various patch sequences. Then, we learn a policy over permutations by optimizing a Plackett-Luce policy using REINFORCE. This approach enables efficient learning in a combinatorial permutation space. REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%.', 'abstract_zh': '序列模型如变压器需要将输入表示为一维序列。在视觉领域，这通常涉及使用固定行-major（栅格扫描）顺序展开图像。虽然全自注意力是置换不变的，但现代长序列变压器越来越多地依赖于打破这一不变性的架构近似，从而引入了对块顺序的敏感性。我们表明，在这种情况下，块顺序显著影响模型性能，简单的替代方案如列-major或希尔伯特曲线可以带来显著的准确性变化。受此启发，我们提出了REOrder，一种发现任务最优块顺序的两阶段框架。首先，我们通过评估各种块序列的可压缩性推导出信息论先验。然后，通过使用REINFORCE优化Plackett-Luce策略来学习置换策略。这种方法能够在组合置换空间中实现高效的 Learning。REOrder在ImageNet-1K上的Top-1准确性上提高了最多3.01%，在Functional Map of the World上的准确性上提高了13.35%。', 'title_zh': 'REOrdering Patches Improves Vision Models'}
{'arxiv_id': 'arXiv:2505.23742', 'title': 'MAGREF: Masked Guidance for Any-Reference Video Generation', 'authors': 'Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, Chongyang Ma', 'link': 'https://arxiv.org/abs/2505.23742', 'abstract': 'Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation quality. In this paper, we propose MAGREF, a unified framework for any-reference video generation that introduces masked guidance to enable coherent multi-subject video synthesis conditioned on diverse reference images and a textual prompt. Specifically, we propose (1) a region-aware dynamic masking mechanism that enables a single model to flexibly handle various subject inference, including humans, objects, and backgrounds, without architectural changes, and (2) a pixel-wise channel concatenation mechanism that operates on the channel dimension to better preserve appearance features. Our model delivers state-of-the-art video generation quality, generalizing from single-subject training to complex multi-subject scenarios with coherent synthesis and precise control over individual subjects, outperforming existing open-source and commercial baselines. To facilitate evaluation, we also introduce a comprehensive multi-subject video benchmark. Extensive experiments demonstrate the effectiveness of our approach, paving the way for scalable, controllable, and high-fidelity multi-subject video synthesis. Code and model can be found at: this https URL', 'abstract_zh': '基于多参考主体的统一视频生成框架：引入掩码指导以实现多样参考图像和文本提示条件下的连贯多主体视频合成', 'title_zh': 'MAGREF: 任意参考视频生成的掩蔽指导'}
{'arxiv_id': 'arXiv:2505.23704', 'title': 'CLDTracker: A Comprehensive Language Description for Visual Tracking', 'authors': 'Mohamad Alansari, Sajid Javed, Iyyakutti Iyappan Ganapathi, Sara Alansari, Muzammal Naseer', 'link': 'https://arxiv.org/abs/2505.23704', 'abstract': "VOT remains a fundamental yet challenging task in computer vision due to dynamic appearance changes, occlusions, and background clutter. Traditional trackers, relying primarily on visual cues, often struggle in such complex scenarios. Recent advancements in VLMs have shown promise in semantic understanding for tasks like open-vocabulary detection and image captioning, suggesting their potential for VOT. However, the direct application of VLMs to VOT is hindered by critical limitations: the absence of a rich and comprehensive textual representation that semantically captures the target object's nuances, limiting the effective use of language information; inefficient fusion mechanisms that fail to optimally integrate visual and textual features, preventing a holistic understanding of the target; and a lack of temporal modeling of the target's evolving appearance in the language domain, leading to a disconnect between the initial description and the object's subsequent visual changes. To bridge these gaps and unlock the full potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive Language Description framework for robust visual Tracking. Our tracker introduces a dual-branch architecture consisting of a textual and a visual branch. In the textual branch, we construct a rich bag of textual descriptions derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with semantic and contextual cues to address the lack of rich textual representation. Experiments on six standard VOT benchmarks demonstrate that CLDTracker achieves SOTA performance, validating the effectiveness of leveraging robust and temporally-adaptive vision-language representations for tracking. Code and models are publicly available at: this https URL", 'abstract_zh': '视觉跟踪中持续视觉跟踪(VOT)仍然是计算机视觉中的一个基础且具有挑战性的任务，由于动态外观变化、遮挡和背景杂乱。传统的跟踪器主要依赖视觉线索，在复杂场景中经常力不从心。近期预训练语言模型(VLMs)在开放词汇检测和图像字幕等任务上的表现表明其在视觉跟踪中的潜在应用。然而，直接将VLMs应用于视觉跟踪受到关键限制：缺乏丰富的文本表示来语义上捕捉目标对象的细微之处，限制了语言信息的有效利用；不高效的特征融合机制无法最优整合视觉和文本特征，阻碍了对目标的整体理解；以及未建模目标随时间不断变化的外观在语言域中，导致初始描述与对象后续视觉变化之间存在脱节。为了解决这些差距并充分发挥VLMs在视觉跟踪中的潜力，我们提出了CLDTracker，一种新颖的整体语言描述框架，用于稳健的视觉跟踪。我们的跟踪器采用双支路架构，包括文本支路和视觉支路。在文本支路中，我们通过利用强大的VLMs（如CLIP和GPT-4V）构建丰富的文本描述集合，并结合语义和上下文线索来解决文本表示不丰富的问题。在六个标准VOT基准上的实验表明，CLDTracker取得了SOTA性能，验证了利用稳健和时间适应的视觉-语言表示对于跟踪的有效性。更多代码和模型请访问：this https URL。', 'title_zh': 'CLDTracker: 一种全面的视觉跟踪语言描述'}
{'arxiv_id': 'arXiv:2505.23637', 'title': 'Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging', 'authors': 'Dashti A. Ali, Richard K. G. Do, William R. Jarnagin, Aras T. Asaad, Amber L. Simpson', 'link': 'https://arxiv.org/abs/2505.23637', 'abstract': 'In medical image analysis, feature engineering plays an important role in the design and performance of machine learning models. Persistent homology (PH), from the field of topological data analysis (TDA), demonstrates robustness and stability to data perturbations and addresses the limitation from traditional feature extraction approaches where a small change in input results in a large change in feature representation. Using PH, we store persistent topological and geometrical features in the form of the persistence barcode whereby large bars represent global topological features and small bars encapsulate geometrical information of the data. When multiple barcodes are computed from 2D or 3D medical images, two approaches can be used to construct the final topological feature vector in each dimension: aggregating persistence barcodes followed by featurization or concatenating topological feature vectors derived from each barcode. In this study, we conduct a comprehensive analysis across diverse medical imaging datasets to compare the effects of the two aforementioned approaches on the performance of classification models. The results of this analysis indicate that feature concatenation preserves detailed topological information from individual barcodes, yields better classification performance and is therefore a preferred approach when conducting similar experiments.', 'abstract_zh': '在医疗图像分析中，特征工程在机器学习模型的设计和性能中发挥着重要作用。来自拓扑数据分析领域的持久同调（PH）展现了对数据扰动的稳健性和稳定性，并解决了传统特征提取方法中存在的问题，即输入的小变化会导致特征表示的大变化。使用PH，我们以持久条形码的形式存储持久的拓扑和几何特征，其中长条形代表全局拓扑特征，短条形包含数据的几何信息。当从2D或3D医疗图像计算出多个条形码时，可以采取两种方法构造每个维度的最终拓扑特征向量：首先聚合持久条形码，然后进行特征化，或者连接从每个条形码派生出来的拓扑特征向量。在本研究中，我们对多种多样的医疗成像数据集进行全面分析，比较这两种方法对分类模型性能的影响。分析结果表明，特征连接能够保留单个条形码中的详细拓扑信息，产生更好的分类性能，因此在进行类似实验时是一种更优选的方法。', 'title_zh': '比较持久 barcode 聚合和特征拼接对医学成像的影响'}
{'arxiv_id': 'arXiv:2505.23617', 'title': 'One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory', 'authors': 'Chenhao Zheng, Jieyu Zhang, Mohammadreza Salehi, Ziqi Gao, Vishnu Iyengar, Norimasa Kobori, Quan Kong, Ranjay Krishna', 'link': 'https://arxiv.org/abs/2505.23617', 'abstract': 'Effective video tokenization is critical for scaling transformer models for long videos. Current approaches tokenize videos using space-time patches, leading to excessive tokens and computational inefficiencies. The best token reduction strategies degrade performance and barely reduce the number of tokens when the camera moves. We introduce grounded video tokenization, a paradigm that organizes tokens based on panoptic sub-object trajectories rather than fixed patches. Our method aligns with fundamental perceptual principles, ensuring that tokenization reflects scene complexity rather than video duration. We propose TrajViT, a video encoder that extracts object trajectories and converts them into semantically meaningful tokens, significantly reducing redundancy while maintaining temporal coherence. Trained with contrastive learning, TrajViT significantly outperforms space-time ViT (ViT3D) across multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a large margin of 6% top-5 recall in average at video-text retrieval task with 10x token deduction. We also show TrajViT as a stronger model than ViT3D for being the video encoder for modern VideoLLM, obtaining an average of 5.2% performance improvement across 6 VideoQA benchmarks while having 4x faster training time and 18x less inference FLOPs. TrajViT is the first efficient encoder to consistently outperform ViT3D across diverse video analysis tasks, making it a robust and scalable solution.', 'abstract_zh': '有效的视频分词对于扩展用于长视频的变压器模型至关重要。当前方法使用时空片段进行视频分词，导致分词过多和计算效率低下。最优的分词策略会降低性能，并且在摄像头移动时几乎无法减少分词数量。我们引入了基于全景子对象轨迹的视频分词方法，该方法以运动轨迹而非固定片段来组织分词。我们的方法遵循基本的知觉原理，确保分词反映场景复杂性而非视频时长。我们提出了一种名为TrajViT的视频编码器，它提取对象轨迹并将其转换为语义上相关的分词，显著减少了冗余同时保持时间连贯性。通过对比学习训练，TrajViT在多个视频理解基准测试上显著优于时空ViT（ViT3D），例如，在视频-文本检索任务中，平均5%的Top-5召回率提高了6%，且分词数量减少了10倍。我们还展示了TrajViT作为现代VideoLLM的视频编码器时，比ViT3D在6个视频问答基准测试上平均性能提高了5.2%，同时训练时间快4倍，推理FLOPs少18倍。TrajViT是首个在多种视频分析任务中一致优于ViT3D的高效编码器，使其成为一个鲁棒且可扩展的解决方案。', 'title_zh': '一条轨迹，一个令牌：基于泛视图子对象轨迹的接地视频词化'}
{'arxiv_id': 'arXiv:2505.23454', 'title': 'LCB-CV-UNet: Enhanced Detector for High Dynamic Range Radar Signals', 'authors': 'Yanbin Wang, Xingyu Chen, Yumiao Wang, Xiang Wang, Chuanfei Zang, Guolong Cui, Jiahuan Liu', 'link': 'https://arxiv.org/abs/2505.23454', 'abstract': 'We propose the LCB-CV-UNet to tackle performance degradation caused by High Dynamic Range (HDR) radar signals. Initially, a hardware-efficient, plug-and-play module named Logarithmic Connect Block (LCB) is proposed as a phase coherence preserving solution to address the inherent challenges in handling HDR features. Then, we propose the Dual Hybrid Dataset Construction method to generate a semi-synthetic dataset, approximating typical HDR signal scenarios with adjustable target distributions. Simulation results show about 1% total detection probability improvement with under 0.9% computational complexity added compared with the baseline. Furthermore, it excels 5% over the baseline at the range in 11-13 dB signal-to-noise ratio typical for urban targets. Finally, the real experiment validates the practicality of our model.', 'abstract_zh': '我们提出LCB-CV-UNet以解决高动态范围(HDR)雷达信号引起的性能退化问题。首先，提出一种硬件高效、插件式模块对数连接块(LCB)，作为一种相位相干性保持解决方案，以应对处理HDR特征时固有的挑战。随后，我们提出双混合数据集构建方法，生成一个半合成数据集，可以调节目标分布来逼近典型HDR信号场景。仿真结果显示，在计算复杂度增加不到0.9%的情况下，总检测概率提高了约1%。此外，在11-13 dB信噪比下，对于城市目标，该方法在距离上优于基线5%。最后，实际实验验证了我们模型的实用性。', 'title_zh': 'LCB-CV-UNet：增强型高动态范围雷达信号检测器'}
{'arxiv_id': 'arXiv:2505.23444', 'title': 'CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis', 'authors': 'Runmin Jiang, Genpei Zhang, Yuntian Yang, Siqi Wu, Yuheng Zhang, Wanyue Feng, Yizhou Zhao, Xi Xiao, Xiao Wang, Tianyang Wang, Xingjian Li, Min Xu', 'link': 'https://arxiv.org/abs/2505.23444', 'abstract': 'Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of macromolecules, but developing robust models for downstream analysis is hindered by the scarcity of high-quality annotated data. While synthetic data generation has emerged as a potential solution, existing methods often fail to capture both the structural diversity of biological specimens and the complex, spatially varying noise inherent in cryo-EM imaging. To overcome these limitations, we propose CryoCCD, a synthesis framework that integrates biophysical modeling with generative techniques. Specifically, CryoCCD produces multi-scale cryo-EM micrographs that reflect realistic biophysical variability through compositional heterogeneity, cellular context, and physics-informed imaging. To generate realistic noise, we employ a conditional diffusion model, enhanced by cycle consistency to preserve structural fidelity and mask-aware contrastive learning to capture spatially adaptive noise patterns. Extensive experiments show that CryoCCD generates structurally accurate micrographs and enhances performance in downstream tasks, outperforming state-of-the-art baselines in both particle picking and reconstruction.', 'abstract_zh': '冷冻电子显微镜（cryo-EM）提供了接近原子级别的大分子成像，但下游分析中稳健模型的开发受限于高质量标注数据的稀缺性。虽然合成数据生成已成为一种潜在的解决方案，但现有方法往往无法捕捉到生物标本的结构多样性以及冷冻电子显微镜成像中固有的复杂且空间变化的噪声。为了克服这些限制，我们提出了一种名为CryoCCD的合成框架，该框架结合了生物物理建模与生成技术。具体而言，CryoCCD产生多层次的冷冻电子显微镜微图，通过组分异质性、细胞背景和基于物理的成像反映了现实的生物物理变异性。为了生成真实的噪声，我们采用条件扩散模型，并通过循环一致性保持结构保真度，同时通过掩码感知对比学习捕捉空间自适应的噪声模式。广泛的经验表明，CryoCCD生成结构准确的微图并在下游任务中增强了性能，在粒子挑选和重构方面均超过了最先进的基线方法。', 'title_zh': 'CryoCCD：基于生物物理建模的条件循环一致扩散方法用于冷冻电镜结构合成'}
{'arxiv_id': 'arXiv:2505.23406', 'title': 'Video Editing for Audio-Visual Dubbing', 'authors': 'Binyamin Manela, Sharon Gannot, Ethan Fetyaya', 'link': 'https://arxiv.org/abs/2505.23406', 'abstract': 'Visual dubbing, the synchronization of facial movements with new speech, is crucial for making content accessible across different languages, enabling broader global reach. However, current methods face significant limitations. Existing approaches often generate talking faces, hindering seamless integration into original scenes, or employ inpainting techniques that discard vital visual information like partial occlusions and lighting variations. This work introduces EdiDub, a novel framework that reformulates visual dubbing as a content-aware editing task. EdiDub preserves the original video context by utilizing a specialized conditioning scheme to ensure faithful and accurate modifications rather than mere copying. On multiple benchmarks, including a challenging occluded-lip dataset, EdiDub significantly improves identity preservation and synchronization. Human evaluations further confirm its superiority, achieving higher synchronization and visual naturalness scores compared to the leading methods. These results demonstrate that our content-aware editing approach outperforms traditional generation or inpainting, particularly in maintaining complex visual elements while ensuring accurate lip synchronization.', 'abstract_zh': '基于内容的视觉配音：一种新的方法以提高身份保留和同步性', 'title_zh': '视听配音的视频编辑'}
{'arxiv_id': 'arXiv:2505.23367', 'title': 'PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening', 'authors': 'Jeonghyeok Do, Sungpyo Kim, Geunhyuk Youk, Jaehyup Lee, Munchurl Kim', 'link': 'https://arxiv.org/abs/2505.23367', 'abstract': "PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with low-resolution multi-spectral (MS) images to generate high-resolution multi-spectral (HRMS) outputs. However, cross-modality misalignment -- caused by sensor placement, acquisition timing, and resolution disparity -- induces a fundamental challenge. Conventional deep learning methods assume perfect pixel-wise alignment and rely on per-pixel reconstruction losses, leading to spectral distortion, double edges, and blurring when misalignment is present. To address this, we propose PAN-Crafter, a modality-consistent alignment framework that explicitly mitigates the misalignment gap between PAN and MS modalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a single network to jointly reconstruct HRMS and PAN images, leveraging PAN's high-frequency details as auxiliary self-supervision. Additionally, we introduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism that bidirectionally aligns MS texture to PAN structure and vice versa, enabling adaptive feature refinement across modalities. Extensive experiments on multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the most recent state-of-the-art method in all metrics, even with 50.11$\\times$ faster inference time and 0.63$\\times$ the memory size. Furthermore, it demonstrates strong generalization performance on unseen satellite datasets, showing its robustness across different conditions.", 'abstract_zh': 'PAN增强旨在融合高分辨率全色（PAN）图像与低分辨率多光谱（MS）图像，生成高分辨率多光谱（HRMS）输出。然而，由于传感器布局、采集时间以及分辨率差异引起的跨模态错位引发了根本性的挑战。传统的深度学习方法假设像素级完美对齐，并依赖于逐像素重构损失，导致在存在错位时出现光谱失真、双边缘和模糊现象。为解决这一问题，我们提出PAN-Crafter，一种模态一致对齐框架，明确地缓解了PAN和MS模态间的对齐差距。核心上，模态自适应重构（MARs）使得单一网络能够联合重构HRMS和PAN图像，并利用PAN的高频细节作为辅助自监督。此外，我们引入了跨模态对齐感知注意力（CM3A），这是一种新颖的机制，双向对齐MS纹理至PAN结构和反之亦然，使模态间实现自适应特征精炼。在多个基准数据集上进行的大量实验表明，我们的PAN-Crafter在所有度量标准上均优于最新最优方法，同时具有50.11倍更快的推理时间和0.63倍的内存占用，并且在未见过的卫星数据集上展示了强大的泛化性能，证明其在不同条件下的鲁棒性。', 'title_zh': 'PAN-Crafter: 学习模态一致对齐以进行PANSharpening'}
{'arxiv_id': 'arXiv:2505.23331', 'title': 'Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization', 'authors': 'Matteo Gallici, Haitz Sáez de Ocáriz Borde', 'link': 'https://arxiv.org/abs/2505.23331', 'abstract': 'Fine-tuning pre-trained generative models with Reinforcement Learning (RL) has emerged as an effective approach for aligning outputs more closely with nuanced human preferences. In this paper, we investigate the application of Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual autoregressive (VAR) models. Our empirical results demonstrate that this approach enables alignment to intricate reward signals derived from aesthetic predictors and CLIP embeddings, significantly enhancing image quality and enabling precise control over the generation style. Interestingly, by leveraging CLIP, our method can help VAR models generalize beyond their initial ImageNet distribution: through RL-driven exploration, these models can generate images aligned with prompts referencing image styles that were absent during pre-training. In summary, we show that RL-based fine-tuning is both efficient and effective for VAR models, benefiting particularly from their fast inference speeds, which are advantageous for online sampling, an aspect that poses significant challenges for diffusion-based alternatives.', 'abstract_zh': '使用强化学习（RL） fine-tune 预训练生成模型以更好地与细微的人类偏好对齐', 'title_zh': '使用组相对策略优化细调下一代视觉自回归模型'}
{'arxiv_id': 'arXiv:2505.23313', 'title': 'Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition', 'authors': 'Weizhe Kong, Xiao Wang, Ruichong Gao, Chenglong Li, Yu Zhang, Xing Yang, Yaowei Wang, Jin Tang', 'link': 'https://arxiv.org/abs/2505.23313', 'abstract': 'Pedestrian Attribute Recognition (PAR) is an indispensable task in human-centered research and has made great progress in recent years with the development of deep neural networks. However, the potential vulnerability and anti-interference ability have still not been fully explored. To bridge this gap, this paper proposes the first adversarial attack and defense framework for pedestrian attribute recognition. Specifically, we exploit both global- and patch-level attacks on the pedestrian images, based on the pre-trained CLIP-based PAR framework. It first divides the input pedestrian image into non-overlapping patches and embeds them into feature embeddings using a projection layer. Meanwhile, the attribute set is expanded into sentences using prompts and embedded into attribute features using a pre-trained CLIP text encoder. A multi-modal Transformer is adopted to fuse the obtained vision and text tokens, and a feed-forward network is utilized for attribute recognition. Based on the aforementioned PAR framework, we adopt the adversarial semantic and label-perturbation to generate the adversarial noise, termed ASL-PAR. We also design a semantic offset defense strategy to suppress the influence of adversarial attacks. Extensive experiments conducted on both digital domains (i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the effectiveness of our proposed adversarial attack and defense strategies for the pedestrian attribute recognition. The source code of this paper will be released on this https URL.', 'abstract_zh': '行人属性识别（PAR）中的对抗攻击与防御框架', 'title_zh': '针对行人属性识别的对抗语义和标签扰动攻击'}
{'arxiv_id': 'arXiv:2505.23214', 'title': 'SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection', 'authors': 'Wenhao Xu, Shuchen Zheng, Changwei Wang, Zherui Zhang, Chuan Ren, Rongtao Xu, Shibiao Xu', 'link': 'https://arxiv.org/abs/2505.23214', 'abstract': "Infrared small target detection (ISTD) is vital for long-range surveillance in military, maritime, and early warning applications. ISTD is challenged by targets occupying less than 0.15% of the image and low distinguishability from complex backgrounds. Existing deep learning methods often suffer from information loss during downsampling and inefficient global context modeling. This paper presents SAMamba, a novel framework integrating SAM2's hierarchical feature learning with Mamba's selective sequence modeling. Key innovations include: (1) A Feature Selection Adapter (FS-Adapter) for efficient natural-to-infrared domain adaptation via dual-stage selection (token-level with a learnable task embedding and channel-wise adaptive transformations); (2) A Cross-Channel State-Space Interaction (CSI) module for efficient global context modeling with linear complexity using selective state space modeling; and (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively combines multi-scale features with a gating mechanism to balance high-resolution and low-resolution feature contributions. SAMamba addresses core ISTD challenges by bridging the domain gap, maintaining fine-grained details, and efficiently modeling long-range dependencies. Experiments on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly outperforms state-of-the-art methods, especially in challenging scenarios with heterogeneous backgrounds and varying target scales. Code: this https URL.", 'abstract_zh': '红外小目标检测（ISTD）在军事、海事及早期预警应用中的远程监视中至关重要。ISTD 面临的目标占图像小于 0.15% 和复杂背景下的低可区分度挑战。现有深度学习方法往往在下采样过程中造成信息丢失，并且在全球上下文建模上效率低下。本文提出 SAMamba，一种结合 SAM2 的分层特征学习与 Mamba 的选择性序列建模的新型框架。关键创新包括：（1）特征选择适配器（FS-Adapter），通过两阶段选择（基于可学习任务嵌入的标记级选择和通道级自适应变换）实现高效自然域到红外域适配；（2）跨通道状态空间交互（CSI）模块，通过选择性状态空间建模以线性复杂度实现高效全局上下文建模；以及（3）细节保留上下文融合（DPCF）模块，通过门控机制适配性地结合多尺度特征，平衡高分辨率和低分辨率特征的贡献。SAMamba 通过弥合域间差距、保持细粒度细节和高效建模远程依赖关系来应对核心 ISTD 挑战。实验结果表明，SAMamba 在 NUAA-SIRST、IRSTD-1k 和 NUDT-SIRST 数据集上的表现显著优于现有最先进的方法，尤其是在背景异构和目标尺度变化多样的复杂场景中。代码：https://this-url。', 'title_zh': 'SAMamba：带有分层视觉的自适应状态空间建模的小目标红外检测'}
{'arxiv_id': 'arXiv:2505.23161', 'title': 'Implicit Inversion turns CLIP into a Decoder', 'authors': "Antonio D'Orazio, Maria Rosaria Briglia, Donato Crisostomi, Dario Loi, Emanuele Rodolà, Iacopo Masi", 'link': 'https://arxiv.org/abs/2505.23161', 'abstract': "CLIP is a discriminative model trained to align images and text in a shared embedding space. Due to its multimodal structure, it serves as the backbone of many generative pipelines, where a decoder is trained to map from the shared space back to images. In this work, we show that image synthesis is nevertheless possible using CLIP alone -- without any decoder, training, or fine-tuning. Our approach optimizes a frequency-aware implicit neural representation that encourages coarse-to-fine generation by stratifying frequencies across network layers. To stabilize this inverse mapping, we introduce adversarially robust initialization, a lightweight Orthogonal Procrustes projection to align local text and image embeddings, and a blending loss that anchors outputs to natural image statistics. Without altering CLIP's weights, this framework unlocks capabilities such as text-to-image generation, style transfer, and image reconstruction. These findings suggest that discriminative models may hold untapped generative potential, hidden in plain sight.", 'abstract_zh': 'CLIP是一种训练用于在共享嵌入空间中对齐图像和文本的辨别模型。由于其多模态结构，它作为许多生成管道的基础骨架，其中解码器被训练将从共享空间映射回图像。在本文中，我们展示了即使不使用任何解码器、训练或微调，仅使用CLIP也能够实现图像合成。我们的方法优化了一种频率意识的隐式神经表示，通过在网络层中分层频率来促进从粗略到精细的生成。为了稳定这一逆向映射，我们引入了对抗鲁棒初始化、轻量级的正交普洛克赛斯投影以对齐局部文本和图像嵌入，以及融合损失以将输出锚定到自然图像统计。不改变CLIP的权重，这一框架解锁了从文本生成图像、风格迁移和图像重构等能力。这些发现表明，辨别模型可能隐藏着未开发的生成潜力。', 'title_zh': '隐式反转使CLIP转变为解码器'}
{'arxiv_id': 'arXiv:2505.23145', 'title': 'FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing', 'authors': 'Jeongsol Kim, Yeobin Hong, Jong Chul Ye', 'link': 'https://arxiv.org/abs/2505.23145', 'abstract': 'Recent inversion-free, flow-based image editing methods such as FlowEdit leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3, enabling text-driven manipulation by solving an ordinary differential equation (ODE). While the lack of exact latent inversion is a core advantage of these methods, it often results in unstable editing trajectories and poor source consistency. To address this limitation, we propose FlowAlign, a novel inversion-free flow-based framework for consistent image editing with principled trajectory control. FlowAlign introduces a flow-matching loss as a regularization mechanism to promote smoother and more stable trajectories during the editing process. Notably, the flow-matching loss is shown to explicitly balance semantic alignment with the edit prompt and structural consistency with the source image along the trajectory. Furthermore, FlowAlign naturally supports reverse editing by simply reversing the ODE trajectory, highlighting the reversible and consistent nature of the transformation. Extensive experiments demonstrate that FlowAlign outperforms existing methods in both source preservation and editing controllability.', 'abstract_zh': '基于流的无逆变换一致图像编辑方法FlowAlign及其原理导向的轨迹控制', 'title_zh': 'FlowAlign: 轨迹正则化、无逆过程的流动基于图像编辑'}
{'arxiv_id': 'arXiv:2505.23134', 'title': 'Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing', 'authors': 'Tongtong Su, Chengyu Wang, Jun Huang, Dongming Lu', 'link': 'https://arxiv.org/abs/2505.23134', 'abstract': 'Appearance editing according to user needs is a pivotal task in video editing. Existing text-guided methods often lead to ambiguities regarding user intentions and restrict fine-grained control over editing specific aspects of objects. To overcome these limitations, this paper introduces a novel approach named {Zero-to-Hero}, which focuses on reference-based video editing that disentangles the editing process into two distinct problems. It achieves this by first editing an anchor frame to satisfy user requirements as a reference image and then consistently propagating its appearance across other frames. We leverage correspondence within the original frames to guide the attention mechanism, which is more robust than previously proposed optical flow or temporal modules in memory-friendly video generative models, especially when dealing with objects exhibiting large motions. It offers a solid ZERO-shot initialization that ensures both accuracy and temporal consistency. However, intervention in the attention mechanism results in compounded imaging degradation with over-saturated colors and unknown blurring issues. Starting from Zero-Stage, our Hero-Stage Holistically learns a conditional generative model for vidEo RestOration. To accurately evaluate the consistency of the appearance, we construct a set of videos with multiple appearances using Blender, enabling a fine-grained and deterministic evaluation. Our method outperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The project page is at this https URL.', 'abstract_zh': '根据用户需求进行外观编辑是视频编辑中的关键任务。现有基于文本的方法常常导致用户意图的歧义，并限制了对对象特定方面进行精细控制的能力。为克服这些限制，本文提出了一种名为{Zero-to-Hero}的新方法，该方法侧重于基于参考的视频编辑，将编辑过程分解为两个独立的问题。该方法首先编辑一个参考帧以满足用户需求，然后在其他帧中一致地传播其外观。我们利用原帧之间的对应关系来引导注意力机制，这种方法在内存友好型视频生成模型中比先前提出的光流或时空模块更 robust，尤其是在处理大运动物体时。它提供了坚实的零-shot 初始化，确保了准确性和时空一致性。然而，在注意力机制中的干预会导致成像退化，表现为过度饱和的颜色和未知的模糊问题。从零阶段开始，我们的英雄阶段整体学习一个条件生成模型以进行视频修复。为了准确评估外观的一致性，我们使用Blender构建了一组具有多种外观的视频集，从而实现细粒度和确定性的评估。与最佳基线相比，我们的方法的PSNR提高了2.6 dB。项目页面链接为：this https URL。', 'title_zh': '从零到英雄：零-shot 初始化赋能基于参考的视频外观编辑'}
{'arxiv_id': 'arXiv:2505.23085', 'title': 'GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion', 'authors': 'Gwanghyun Kim, Xueting Li, Ye Yuan, Koki Nagano, Tianye Li, Jan Kautz, Se Young Chun, Umar Iqbal', 'link': 'https://arxiv.org/abs/2505.23085', 'abstract': "Estimating accurate and temporally consistent 3D human geometry from videos is a challenging problem in computer vision. Existing methods, primarily optimized for single images, often suffer from temporal inconsistencies and fail to capture fine-grained dynamic details. To address these limitations, we present GeoMan, a novel architecture designed to produce accurate and temporally consistent depth and normal estimations from monocular human videos. GeoMan addresses two key challenges: the scarcity of high-quality 4D training data and the need for metric depth estimation to accurately model human size. To overcome the first challenge, GeoMan employs an image-based model to estimate depth and normals for the first frame of a video, which then conditions a video diffusion model, reframing video geometry estimation task as an image-to-video generation problem. This design offloads the heavy lifting of geometric estimation to the image model and simplifies the video model's role to focus on intricate details while using priors learned from large-scale video datasets. Consequently, GeoMan improves temporal consistency and generalizability while requiring minimal 4D training data. To address the challenge of accurate human size estimation, we introduce a root-relative depth representation that retains critical human-scale details and is easier to be estimated from monocular inputs, overcoming the limitations of traditional affine-invariant and metric depth representations. GeoMan achieves state-of-the-art performance in both qualitative and quantitative evaluations, demonstrating its effectiveness in overcoming longstanding challenges in 3D human geometry estimation from videos.", 'abstract_zh': '从视频中估计准确且时间一致的人体三维几何是一个计算机视觉中的挑战性问题。现有方法主要针对单张图像进行优化，往往存在时间一致性差的问题，无法捕获细微的动力学细节。为解决这些问题，我们提出GeoMan，一种新型架构，用于从单目人体视频中生成准确且时间一致的深度和法线估计。GeoMan解决了两个关键挑战：高质量四维训练数据的稀缺性和需要度量深度估计来准确建模人体尺寸。为克服第一个挑战，GeoMan采用图像模型来估计视频第一帧的深度和法线，并指导视频扩散模型，将视频几何估计任务重新定义为从图像到视频的生成问题。这种设计将几何估计的重担转移到图像模型上，并简化了视频模型的角色，使其专注于复杂的细节，同时利用大规模视频数据集学习的先验知识。因此，GeoMan提高了时间一致性和泛化能力，同时只需少量四维训练数据。为解决准确人体尺寸估计的挑战，我们引入了一种根相对的深度表示，保留了关键的人体尺度细节，并且更容易从单目输入中估计，克服了传统齐次不变和度量深度表示的限制。GeoMan在定性和定量评估中均取得了领先性能，展示了其在从视频中估计人体三维几何方面的有效性。', 'title_zh': 'GeoMan: 基于图像到视频扩散模型的人体几何一致性估计'}
{'arxiv_id': 'arXiv:2505.23045', 'title': 'Multi-Sourced Compositional Generalization in Visual Question Answering', 'authors': 'Chuanhao Li, Wenbo Ye, Zhen Li, Yuwei Wu, Yunde Jia', 'link': 'https://arxiv.org/abs/2505.23045', 'abstract': 'Compositional generalization is the ability of generalizing novel compositions from seen primitives, and has received much attention in vision-and-language (V\\&L) recently. Due to the multi-modal nature of V\\&L tasks, the primitives composing compositions source from different modalities, resulting in multi-sourced novel compositions. However, the generalization ability over multi-sourced novel compositions, \\textit{i.e.}, multi-sourced compositional generalization (MSCG) remains unexplored. In this paper, we explore MSCG in the context of visual question answering (VQA), and propose a retrieval-augmented training framework to enhance the MSCG ability of VQA models by learning unified representations for primitives from different modalities. Specifically, semantically equivalent primitives are retrieved for each primitive in the training samples, and the retrieved features are aggregated with the original primitive to refine the model. This process helps the model learn consistent representations for the same semantic primitives across different modalities. To evaluate the MSCG ability of VQA models, we construct a new GQA-MSCG dataset based on the GQA dataset, in which samples include three types of novel compositions composed of primitives from different modalities. Experimental results demonstrate the effectiveness of the proposed framework. We release GQA-MSCG at this https URL.', 'abstract_zh': '多源成分泛化能力的研究及增强：以视觉问答为 contexts', 'title_zh': '多源组成性泛化在视觉问答中的应用'}
{'arxiv_id': 'arXiv:2505.22944', 'title': 'ATI: Any Trajectory Instruction for Controllable Video Generation', 'authors': 'Angtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yiding Yang, Chongyang Ma', 'link': 'https://arxiv.org/abs/2505.22944', 'abstract': 'We propose a unified framework for motion control in video generation that seamlessly integrates camera movement, object-level translation, and fine-grained local motion using trajectory-based inputs. In contrast to prior methods that address these motion types through separate modules or task-specific designs, our approach offers a cohesive solution by projecting user-defined trajectories into the latent space of pre-trained image-to-video generation models via a lightweight motion injector. Users can specify keypoints and their motion paths to control localized deformations, entire object motion, virtual camera dynamics, or combinations of these. The injected trajectory signals guide the generative process to produce temporally consistent and semantically aligned motion sequences. Our framework demonstrates superior performance across multiple video motion control tasks, including stylized motion effects (e.g., motion brushes), dynamic viewpoint changes, and precise local motion manipulation. Experiments show that our method provides significantly better controllability and visual quality compared to prior approaches and commercial solutions, while remaining broadly compatible with various state-of-the-art video generation backbones. Project page: this https URL.', 'abstract_zh': '我们提出了一种统一框架，用于视频生成中的运动控制，该框架通过基于轨迹的输入无缝整合摄像机运动、对象级平移和局部精细运动。与此前通过独立模块或特定任务设计处理这些运动类型的方 法不同，我们的方法通过对预训练的图像到视频生成模型的潜在空间进行轻量级运动注入，以用户定义的轨迹为依据提供一个连贯的解决方案。用户可以指定关键点和它们的运动路径来控制局部变形、整个对象的运动、虚拟摄像机的动力学或它们的组合。注入的轨迹信号指导生成过程，以产生时序一致且语义对齐的运动序列。我们的框架在多种视频运动控制任务中表现出色，包括风格化运动效果（例如，运动画笔）、动态视角变化和精确的局部运动操控。实验表明，与以前的方法和商业解决方案相比，我们的方法在可控性与视觉质量方面提供了显著改进，同时仍然与各种最先进的视频生成骨干网络兼容。项目页面: this https URL。', 'title_zh': 'ATI: 任意轨迹指令的可控视频生成'}
{'arxiv_id': 'arXiv:2505.22673', 'title': 'Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion', 'authors': 'Wasif Khan, Kyle B. See, Simon Kato, Ziqian Huang, Amy Lazarte, Kyle Douglas, Xiangyang Lou, Teng J. Peng, Dhanashree Rajderkar, John Rees, Pina Sanelli, Amita Singh, Ibrahim Tuna, Christina A. Wilson, Ruogu Fang', 'link': 'https://arxiv.org/abs/2505.22673', 'abstract': "Perfusion imaging is extensively utilized to assess hemodynamic status and tissue perfusion in various organs. Computed tomography perfusion (CTP) imaging plays a key role in the early assessment and planning of stroke treatment. While CTP provides essential perfusion parameters to identify abnormal blood flow in the brain, the use of contrast agents in CTP can lead to allergic reactions and adverse side effects, along with costing USD 4.9 billion worldwide in 2022. To address these challenges, we propose a novel deep learning framework called Multitask Automated Generation of Intermodal CT perfusion maps (MAGIC). This framework combines generative artificial intelligence and physiological information to map non-contrast computed tomography (CT) imaging to multiple contrast-free CTP imaging maps. We demonstrate enhanced image fidelity by incorporating physiological characteristics into the loss terms. Our network was trained and validated using CT image data from patients referred for stroke at UF Health and demonstrated robustness to abnormalities in brain perfusion activity. A double-blinded study was conducted involving seven experienced neuroradiologists and vascular neurologists. This study validated MAGIC's visual quality and diagnostic accuracy showing favorable performance compared to clinical perfusion imaging with intravenous contrast injection. Overall, MAGIC holds great promise in revolutionizing healthcare by offering contrast-free, cost-effective, and rapid perfusion imaging.", 'abstract_zh': '多任务自动生成多模态CT灌注图的深度学习框架（MAGIC）：实现无对比剂的灌注成像', 'title_zh': '基于生理学指导的生成多任务网络：无对比剂CT灌注成像'}
