{'arxiv_id': 'arXiv:2508.00580', 'title': 'OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery', 'authors': 'Raul Castilla-Arquillo, Carlos Perez-del-Pulgar, Levin Gerdes, Alfonso Garcia-Cerezo, Miguel A. Olivares-Mendez', 'link': 'https://arxiv.org/abs/2508.00580', 'abstract': 'Robot navigation in unstructured environments requires multimodal perception systems that can support safe navigation. Multimodality enables the integration of complementary information collected by different sensors. However, this information must be processed by machine learning algorithms specifically designed to leverage heterogeneous data. Furthermore, it is necessary to identify which sensor modalities are most informative for navigation in the target environment. In Martian exploration, thermal imagery has proven valuable for assessing terrain safety due to differences in thermal behaviour between soil types. This work presents OmniUnet, a transformer-based neural network architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T) imagery. A custom multimodal sensor housing was developed using 3D printing and mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a multimodal dataset in the Bardenas semi-desert in northern Spain. This location serves as a representative environment of the Martian surface, featuring terrain types such as sand, bedrock, and compact soil. A subset of this dataset was manually labeled to support supervised training of the network. The model was evaluated both quantitatively and qualitatively, achieving a pixel accuracy of 80.37% and demonstrating strong performance in segmenting complex unstructured terrain. Inference tests yielded an average prediction time of 673 ms on a resource-constrained computer (Jetson Orin Nano), confirming its suitability for on-robot deployment. The software implementation of the network and the labeled dataset have been made publicly available to support future research in multimodal terrain perception for planetary robotics.', 'abstract_zh': '火星探索中未结构化环境中的机器人导航需要多模态感知系统以支持安全导航。多模态性允许不同传感器收集的互补信息的整合。然而，这些信息必须通过专门设计用于利用异构数据的机器学习算法进行处理。此外，还需要确定哪些传感器模态在目标环境中最具导航信息。为此，本研究提出了基于转换器的 OmniUnet 神经网络架构，用于使用 RGB、深度和热成像（RGB-D-T）进行语义分割。我们开发了一个自定义的多模态传感器外壳，利用3D打印技术安装在火星自主漫游车测试平台（MaRTA）上，收集西班牙北部巴丹纳斯半沙漠区域的多模态数据集。该位置作为火星表面的代表性环境，具备诸如沙地、岩基和紧实土壤等多种地形类型。从该数据集中手动标注了一部分数据以支持网络的监督训练。模型在定量和定性评估中分别达到了80.37%的像素准确率，并在分割复杂未结构化地形方面表现出强劲性能。推断测试显示，在资源受限的计算机（Jetson Orin Nano）上平均每预测时间为673 ms，证实了其适用于机器人本体部署的适用性。网络的软件实现和标注数据集已公开发布，以支持未来在行星机器人多模态地形感知领域的研究。', 'title_zh': '全方位UNET：基于RGB、深度和红外成像的行星探测车不规则地形分割多模态网络'}
{'arxiv_id': 'arXiv:2508.00303', 'title': 'TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps', 'authors': 'Zehui Xu, Junhui Wang, Yongliang Shi, Chao Gao, Guyue Zhou', 'link': 'https://arxiv.org/abs/2508.00303', 'abstract': "This paper introduces TopoDiffuser, a diffusion-based framework for multimodal trajectory prediction that incorporates topometric maps to generate accurate, diverse, and road-compliant future motion forecasts. By embedding structural cues from topometric maps into the denoising process of a conditional diffusion model, the proposed approach enables trajectory generation that naturally adheres to road geometry without relying on explicit constraints. A multimodal conditioning encoder fuses LiDAR observations, historical motion, and route information into a unified bird's-eye-view (BEV) representation. Extensive experiments on the KITTI benchmark demonstrate that TopoDiffuser outperforms state-of-the-art methods, while maintaining strong geometric consistency. Ablation studies further validate the contribution of each input modality, as well as the impact of denoising steps and the number of trajectory samples. To support future research, we publicly release our code at this https URL.", 'abstract_zh': '本文介绍了TopoDiffuser，这是一种利用拓扑地图进行多模态轨迹预测的扩散框架，能够生成准确、多样化且符合道路几何的未来运动预测。通过将拓扑地图中的结构线索嵌入到条件扩散模型的去噪过程中，所提出的方法能够在不依赖显式约束的情况下，自然地遵循道路几何进行轨迹生成。多模态条件编码器将LiDAR观测、历史运动和路径信息融合为统一的鸟瞰视图（BEV）表示。在KITTI基准上的 extensive 实验表明，TopoDiffuser 在保持良好的几何一致性的同时超越了现有最先进的方法。消融研究进一步验证了每个输入模态的贡献，以及去噪步骤和轨迹样本数量的影响。为了支持未来的研究，我们在以下网址公开发布了我们的代码：this https URL。', 'title_zh': '基于拓扑扩散的多模态轨迹预测模型——拓扑地图辅助方法'}
{'arxiv_id': 'arXiv:2508.00589', 'title': 'Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving', 'authors': 'Stefan Englmeier, Max A. Büttner, Katharina Winter, Fabian B. Flohr', 'link': 'https://arxiv.org/abs/2508.00589', 'abstract': 'Autonomous driving systems must operate reliably in safety-critical scenarios, particularly those involving unusual or complex behavior by Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets is essential for robust evaluation and generalization, but retrieving such rare human behavior scenarios within the long tail of large-scale datasets is challenging. To support targeted evaluation of autonomous driving systems in diverse, human-centered scenarios, we propose a novel context-aware motion retrieval framework. Our method combines Skinned Multi-Person Linear (SMPL)-based motion sequences and corresponding video frames before encoding them into a shared multimodal embedding space aligned with natural language. Our approach enables the scalable retrieval of human behavior and their context through text queries. This work also introduces our dataset WayMoCo, an extension of the Waymo Open Dataset. It contains automatically labeled motion and scene context descriptions derived from generated pseudo-ground-truth SMPL sequences and corresponding image data. Our approach outperforms state-of-the-art models by up to 27.5% accuracy in motion-context retrieval, when evaluated on the WayMoCo dataset.', 'abstract_zh': '自动驾驶系统必须在涉及脆弱道路使用者（VRUs）的异常或复杂行为的安全关键场景中可靠运行。识别驾驶数据集中这些边缘案例对于稳健的评估和泛化至关重要，但在大规模数据集的长尾中检索此类罕见的人类行为场景具有挑战性。为了支持在多样的、以人为中心的场景中对自动驾驶系统的针对性评估，我们提出了一种新的上下文感知运动检索框架。我们的方法结合了基于SMPL的人体运动序列及其对应的视频帧，并将它们编码到与自然语言对齐的多模态嵌入空间中。我们的方法通过文本查询实现了对人类行为及其上下文的大规模检索。此外，我们还介绍了我们的数据集WayMoCo，它是Waymo开放数据集的扩展，包含从生成的伪地面真实SMPL序列和相应的图像数据中自动生成的动作和场景上下文描述。与现有的最佳模型相比，我们的方法在WayMoCo数据集上将运动-上下文检索的准确率提高了高达27.5%。', 'title_zh': '基于上下文的运动检索：使用开放词汇方法应用于自主驾驶'}
{'arxiv_id': 'arXiv:2508.00784', 'title': 'Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics', 'authors': 'Tom Or, Omri Azencot', 'link': 'https://arxiv.org/abs/2508.00784', 'abstract': 'Generative models achieve remarkable results in multiple data domains, including images and texts, among other examples. Unfortunately, malicious users exploit synthetic media for spreading misinformation and disseminating deepfakes. Consequently, the need for robust and stable fake detectors is pressing, especially when new generative models appear everyday. While the majority of existing work train classifiers that discriminate between real and fake information, such tools typically generalize only within the same family of generators and data modalities, yielding poor results on other generative classes and data domains. Towards a universal classifier, we propose the use of large pre-trained multi-modal models for the detection of generative content. Effectively, we show that the latent code of these models naturally captures information discriminating real from fake. Building on this observation, we demonstrate that linear classifiers trained on these features can achieve state-of-the-art results across various modalities, while remaining computationally efficient, fast to train, and effective even in few-shot settings. Our work primarily focuses on fake detection in audio and images, achieving performance that surpasses or matches that of strong baseline methods.', 'abstract_zh': '生成模型在图像和文本等领域取得了 remarkable 的成果，但恶意用户利用合成媒体传播虚假信息和生成深度假新闻。因此，需要 robust 和稳定的假内容检测器，特别是当新的生成模型每天出现时。尽管现有大部分工作训练分类器来区分真实和虚假信息，但这些工具通常只能在相同的生成器家族和数据模态内进行泛化，对其他生成类和数据领域表现较差。为了实现通用分类器，我们提出使用大规模预训练多模态模型来检测生成内容。实际上，我们证明这些模型的潜在代码自然地捕获了区分真实和虚假的信息。基于此观察，我们展示了在这些特征上训练的线性分类器可以在各种模态下获得最先进的结果，同时保持计算效率，快速训练，并在少量样本设置下仍然有效。我们的工作主要集中在音频和图像中的假内容检测，性能超过或匹敌强基线方法。', 'title_zh': '解析隐藏表示：更好的合成内容溯源的多模态层分析'}
{'arxiv_id': 'arXiv:2508.00665', 'title': 'Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI', 'authors': 'Maryam Mosleh, Marie Devlin, Ellis Solaiman', 'link': 'https://arxiv.org/abs/2508.00665', 'abstract': "Artificial intelligence-driven adaptive learning systems are reshaping education through data-driven adaptation of learning experiences. Yet many of these systems lack transparency, offering limited insight into how decisions are made. Most explainable AI (XAI) techniques focus on technical outputs but neglect user roles and comprehension. This paper proposes a hybrid framework that integrates traditional XAI techniques with generative AI models and user personalisation to generate multimodal, personalised explanations tailored to user needs. We redefine explainability as a dynamic communication process tailored to user roles and learning goals. We outline the framework's design, key XAI limitations in education, and research directions on accuracy, fairness, and personalisation. Our aim is to move towards explainable AI that enhances transparency while supporting user-centred experiences.", 'abstract_zh': '基于人工智能的自适应学习系统通过数据驱动的学习体验重新塑造教育，但许多系统缺乏透明度，无法提供决策过程的深刻见解。大多数可解释人工智能（XAI）技术侧重于技术输出，而忽视了用户角色和理解。本文提出了一种结合传统XAI技术、生成式人工智能模型和用户个性化定制的混合框架，以生成多模态、个性化的解释，满足用户需求。我们将可解释性重新定义为一种针对用户角色和学习目标定制的动态沟通过程。文中概述了该框架的设计、教育领域XAI的关键限制以及关于准确性和个性化的研究方向。我们的目标是朝着既能增加透明度又支持以用户为中心体验的可解释人工智能迈进。', 'title_zh': '基于数据中心的多模态解释型人工智能的透明自适应学习'}
{'arxiv_id': 'arXiv:2508.00576', 'title': 'MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models', 'authors': 'Zhanliang Wang, Kai Wang', 'link': 'https://arxiv.org/abs/2508.00576', 'abstract': 'Multimodal AI models have achieved impressive performance in tasks that require integrating information from multiple modalities, such as vision and language. However, their "black-box" nature poses a major barrier to deployment in high-stakes applications where interpretability and trustworthiness are essential. How to explain cross-modal interactions in multimodal AI models remains a major challenge. While existing model explanation methods, such as attention map and Grad-CAM, offer coarse insights into cross-modal relationships, they cannot precisely quantify the synergistic effects between modalities, and are limited to open-source models with accessible internal weights. Here we introduce MultiSHAP, a model-agnostic interpretability framework that leverages the Shapley Interaction Index to attribute multimodal predictions to pairwise interactions between fine-grained visual and textual elements (such as image patches and text tokens), while being applicable to both open- and closed-source models. Our approach provides: (1) instance-level explanations that reveal synergistic and suppressive cross-modal effects for individual samples - "why the model makes a specific prediction on this input", and (2) dataset-level explanation that uncovers generalizable interaction patterns across samples - "how the model integrates information across modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP faithfully captures cross-modal reasoning mechanisms, while real-world case studies demonstrate its practical utility. Our framework is extensible beyond two modalities, offering a general solution for interpreting complex multimodal AI models.', 'abstract_zh': '多模态AI模型已经在需要整合多种模态信息的任务中取得了令人印象深刻的性能，例如视觉和语言任务。然而，它们的“黑盒”性质在高风险应用中成为部署的主要障碍，特别是在需要可解释性和可信度的应用中。如何解释多模态AI模型中的跨模态交互仍然是一个重大挑战。尽管现有的模型解释方法，如注意力图和Grad-CAM，提供了对跨模态关系的粗略洞察，但它们无法精确量化解码不同模态之间的协同效应，并且仅适用于具有可访问内部权重的开源模型。本文介绍了一种基于Shapley相互作用指数的通用可解释性框架——MultiSHAP，该框架能够将多模态预测归因于精细视觉和文本元素（如图像片段和文本令牌）的成对交互，同时适用于开源和封闭源模型。我们的方法提供：(1) 单个示例级别的解释，揭示了特定输入上模型特定预测的协同和抑制的跨模态效应——“为什么模型对该输入做出这种特定预测”，以及(2) 数据集级别的解释，发现跨示例的可泛化的交互模式——“模型如何跨模态整合信息”。在公开的多模态基准测试上的实验证实，MultiSHAP 真实捕捉到了跨模态推理机制，而实际案例研究则展示了其实际应用价值。我们的框架可以扩展到超过两个模态，提供了一种解释复杂多模态AI模型的一般性解决方案。', 'title_zh': '基于Shapley值的多模态AI模型跨模态交互解释框架 MultiSHAP'}
{'arxiv_id': 'arXiv:2508.00378', 'title': 'CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding', 'authors': 'Shixin Yi, Lin Shang', 'link': 'https://arxiv.org/abs/2508.00378', 'abstract': 'Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in vision-language models (VLMs), but it often produces explanations that are linguistically fluent yet lack grounding in visual content. We observe that such hallucinations arise in part from the absence of an explicit verification mechanism during multi-step reasoning. To address this, we propose \\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with \\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces visual verification into the reasoning process. CoRGI follows a three-stage pipeline: it first generates a textual reasoning chain, then extracts supporting visual evidence for each reasoning step via a dedicated module (VEVM), and finally synthesizes the textual rationale with visual evidence to generate a grounded, verified answer. The framework can be integrated with existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR benchmark and find that it improves reasoning performance on two representative open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm the contribution of each step in the verification module, and human evaluations suggest that CoRGI leads to more factual and helpful explanations. We also examine alternative designs for the visual verification step and discuss potential limitations of post-hoc verification frameworks. These findings highlight the importance of grounding intermediate reasoning steps in visual evidence to enhance the robustness of multimodal reasoning.', 'abstract_zh': 'Chain-of-Thought (CoT) 提示在提高视觉语言模型 (VLMs) 的推理能力方面显示出潜力，但往往会产生在语言上流畅但在视觉内容基础上缺乏依据的解释。我们观察到，这种幻觉部分原因是多步推理过程中缺乏显式的验证机制。为此，我们提出了 CoRGI（基于视觉见解的推理链），这是一种模块化框架，将视觉验证引入推理过程。CoRGI 遵循三阶段管道：首先生成文本推理链，然后通过专门模块（VEVM）提取每一步推理的支持性视觉证据，最后综合文本理由与视觉证据生成基于视觉的验证答案。该框架可以与现有的 VLMs 集成而无需端到端重新训练。我们在 VCR 基准上评估了 CoRGI，发现它在 Qwen-2.5VL 和 LLaVA-1.6 这两个代表性开源 VLM 主干上提高了推理性能。消融研究证实了验证模块中每个步骤的贡献，并表明人类评估认为 CoRGI 产生了更准确和有帮助的解释。我们还探讨了视觉验证步骤的替代设计，并讨论了事后验证框架的潜在局限性。研究结果强调了将中间推理步骤与视觉证据联系起来的重要性，以增强多模态推理的鲁棒性。', 'title_zh': 'CoRGI：带有视觉接地的验证链式推理'}
{'arxiv_id': 'arXiv:2508.00323', 'title': 'Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning', 'authors': 'Jianyi Zhang, Xu Ji, Ziyin Zhou, Yuchen Zhou, Shubo Shi, Haoyu Wu, Zhen Li, Shizhao Liu', 'link': 'https://arxiv.org/abs/2508.00323', 'abstract': 'Evaluating the performance of visual language models (VLMs) in graphic reasoning tasks has become an important research topic. However, VLMs still show obvious deficiencies in simulating human-level graphic reasoning capabilities, especially in complex graphic reasoning and abstract problem solving, which are less studied and existing studies only focus on simple graphics. To evaluate the performance of VLMs in complex graphic reasoning, we propose ReasonBench, the first evaluation benchmark focused on structured graphic reasoning tasks, which includes 1,613 questions from real-world intelligence tests. ReasonBench covers reasoning dimensions related to location, attribute, quantity, and multi-element tasks, providing a comprehensive evaluation of the performance of VLMs in spatial, relational, and abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including closed-source and open-source models) and reveal significant limitations of current models. Based on these findings, we propose a dual optimization strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability of reasoning by decomposing layers, and ReasonTune enhances the task adaptability of model reasoning through training, all of which improves VLM performance by 33.5\\%. All experimental data and code are in the repository: this https URL.', 'abstract_zh': '评价视觉语言模型在图形推理任务中的性能已成为一个重要研究课题。然而，视觉语言模型在模拟人类级别的图形推理能力方面仍然表现出了明显的不足，尤其是在复杂图形推理和抽象问题解决方面，这些领域尚未得到充分研究，现有研究仅关注简单的图形。为了评估视觉语言模型在复杂图形推理中的性能，我们提出了ReasonBench，这是首个专注于结构化图形推理任务的评估基准，包含来自实际智能测试的1,613个问题。ReasonBench涵盖了与位置、属性、数量和多元素任务相关的推理维度，提供了对视觉语言模型在空间、关系和抽象推理能力方面性能的全面评估。我们 benchmark 了11个主流视觉语言模型（包括闭源和开源模型），揭示了当前模型的重要局限性。基于这些发现，我们提出了双重优化策略：图表推理链（DiaCoT）通过分解层级增强推理的可解释性，而ReasonTune通过训练增强模型推理的任务适应性，这些策略共同提高了视觉语言模型的性能33.5%。所有实验数据和代码可在以下仓库获取：this https URL。', 'title_zh': '俄狄浦斯与斯芬克斯：视觉语言模型在复杂图示推理中的基准测试与改进'}
{'arxiv_id': 'arXiv:2508.00395', 'title': 'Decouple before Align: Visual Disentanglement Enhances Prompt Tuning', 'authors': 'Fei Zhang, Tianfei Zhou, Jiangchao Yao, Ya Zhang, Ivor W. Tsang, Yanfeng Wang', 'link': 'https://arxiv.org/abs/2508.00395', 'abstract': 'Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm, has showcased remarkable effectiveness in improving the task-specific transferability of vision-language models. This paper delves into a previously overlooked information asymmetry issue in PT, where the visual modality mostly conveys more context than the object-oriented textual modality. Correspondingly, coarsely aligning these two modalities could result in the biased attention, driving the model to merely focus on the context area. To address this, we propose DAPT, an effective PT framework based on an intuitive decouple-before-align concept. First, we propose to explicitly decouple the visual modality into the foreground and background representation via exploiting coarse-and-fine visual segmenting cues, and then both of these decoupled patterns are aligned with the original foreground texts and the hand-crafted background classes, thereby symmetrically strengthening the modal alignment. To further enhance the visual concentration, we propose a visual pull-push regularization tailored for the foreground-background patterns, directing the original visual representation towards unbiased attention on the region-of-interest object. We demonstrate the power of architecture-free DAPT through few-shot learning, base-to-novel generalization, and data-efficient learning, all of which yield superior performance across prevailing benchmarks. Our code will be released at this https URL.', 'abstract_zh': '基于解耦与对齐概念的DAPT：一种有效提示调优框架', 'title_zh': '解耦再对齐：视觉去纠缠增强提示调优'}
{'arxiv_id': 'arXiv:2502.18148', 'title': 'NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts', 'authors': "Muhammad Farid Adilazuarda, Musa Izzanardi Wijanarko, Lucky Susanto, Khumaisa Nur'aini, Derry Wijaya, Alham Fikri Aji", 'link': 'https://arxiv.org/abs/2502.18148', 'abstract': "Indonesia is rich in languages and scripts. However, most NLP progress has been made using romanized text. In this paper, we present NusaAksara, a novel public benchmark for Indonesian languages that includes their original scripts. Our benchmark covers both text and image modalities and encompasses diverse tasks such as image segmentation, OCR, transliteration, translation, and language identification. Our data is constructed by human experts through rigorous steps. NusaAksara covers 8 scripts across 7 languages, including low-resource languages not commonly seen in NLP benchmarks. Although unsupported by Unicode, the Lampung script is included in this dataset. We benchmark our data across several models, from LLMs and VLMs such as GPT-4o, Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and show that most NLP technologies cannot handle Indonesia's local scripts, with many achieving near-zero performance.", 'abstract_zh': '印尼语及其原始书写系统的新型公共基准NusaAksara', 'title_zh': 'NusaAksara：一种多模态多语言基准，用于保存印尼土著文字'}
