{'arxiv_id': 'arXiv:2508.00784', 'title': 'Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics', 'authors': 'Tom Or, Omri Azencot', 'link': 'https://arxiv.org/abs/2508.00784', 'abstract': 'Generative models achieve remarkable results in multiple data domains, including images and texts, among other examples. Unfortunately, malicious users exploit synthetic media for spreading misinformation and disseminating deepfakes. Consequently, the need for robust and stable fake detectors is pressing, especially when new generative models appear everyday. While the majority of existing work train classifiers that discriminate between real and fake information, such tools typically generalize only within the same family of generators and data modalities, yielding poor results on other generative classes and data domains. Towards a universal classifier, we propose the use of large pre-trained multi-modal models for the detection of generative content. Effectively, we show that the latent code of these models naturally captures information discriminating real from fake. Building on this observation, we demonstrate that linear classifiers trained on these features can achieve state-of-the-art results across various modalities, while remaining computationally efficient, fast to train, and effective even in few-shot settings. Our work primarily focuses on fake detection in audio and images, achieving performance that surpasses or matches that of strong baseline methods.', 'abstract_zh': '生成模型在图像和文本等领域取得了 remarkable 的成果，但恶意用户利用合成媒体传播虚假信息和生成深度假新闻。因此，需要 robust 和稳定的假内容检测器，特别是当新的生成模型每天出现时。尽管现有大部分工作训练分类器来区分真实和虚假信息，但这些工具通常只能在相同的生成器家族和数据模态内进行泛化，对其他生成类和数据领域表现较差。为了实现通用分类器，我们提出使用大规模预训练多模态模型来检测生成内容。实际上，我们证明这些模型的潜在代码自然地捕获了区分真实和虚假的信息。基于此观察，我们展示了在这些特征上训练的线性分类器可以在各种模态下获得最先进的结果，同时保持计算效率，快速训练，并在少量样本设置下仍然有效。我们的工作主要集中在音频和图像中的假内容检测，性能超过或匹敌强基线方法。', 'title_zh': '解析隐藏表示：更好的合成内容溯源的多模态层分析'}
{'arxiv_id': 'arXiv:2508.00674', 'title': 'Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations', 'authors': 'Banan Alkhateeb, Ellis Solaiman', 'link': 'https://arxiv.org/abs/2508.00674', 'abstract': 'Social media platforms today strive to improve user experience through AI recommendations, yet the value of such recommendations vanishes as users do not understand the reasons behind them. This issue arises because explainability in social media is general and lacks alignment with user-specific needs. In this vision paper, we outline a user-segmented and context-aware explanation layer by proposing a visual explanation system with diverse explanation methods. The proposed system is framed by the variety of user needs and contexts, showing explanations in different visualized forms, including a technically detailed version for AI experts and a simplified one for lay users. Our framework is the first to jointly adapt explanation style (visual vs. numeric) and granularity (expert vs. lay) inside a single pipeline. A public pilot with 30 X users will validate its impact on decision-making and trust.', 'abstract_zh': '社交媒体平台通过AI推荐改善用户体验，但这种推荐的价值因用户不了解其背后原因而减弱。这一问题源于社交媒体解释的一般性与用户特定需求之间的不一致。在本文中，我们通过提出一种具有多种解释方法的可视化解释系统，概述了一个基于用户细分和上下文感知的解释层。该系统根据用户需求和上下文的不同，以多种可视化形式展示解释，包括面向AI专家的技术详细版本和面向普通用户的简化版本。我们的框架是首个在单一管道中联合适应解释风格（可视化与数值）和粒度（专家与普通用户）的框架。一项面向30名X用户的公开试点将验证其对决策和信任的影响。', 'title_zh': '基于上下文的可解释人工智能推荐可视化在社交媒体中的用户对齐解释：一种愿景'}
{'arxiv_id': 'arXiv:2508.00665', 'title': 'Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI', 'authors': 'Maryam Mosleh, Marie Devlin, Ellis Solaiman', 'link': 'https://arxiv.org/abs/2508.00665', 'abstract': "Artificial intelligence-driven adaptive learning systems are reshaping education through data-driven adaptation of learning experiences. Yet many of these systems lack transparency, offering limited insight into how decisions are made. Most explainable AI (XAI) techniques focus on technical outputs but neglect user roles and comprehension. This paper proposes a hybrid framework that integrates traditional XAI techniques with generative AI models and user personalisation to generate multimodal, personalised explanations tailored to user needs. We redefine explainability as a dynamic communication process tailored to user roles and learning goals. We outline the framework's design, key XAI limitations in education, and research directions on accuracy, fairness, and personalisation. Our aim is to move towards explainable AI that enhances transparency while supporting user-centred experiences.", 'abstract_zh': '基于人工智能的自适应学习系统通过数据驱动的学习体验重新塑造教育，但许多系统缺乏透明度，无法提供决策过程的深刻见解。大多数可解释人工智能（XAI）技术侧重于技术输出，而忽视了用户角色和理解。本文提出了一种结合传统XAI技术、生成式人工智能模型和用户个性化定制的混合框架，以生成多模态、个性化的解释，满足用户需求。我们将可解释性重新定义为一种针对用户角色和学习目标定制的动态沟通过程。文中概述了该框架的设计、教育领域XAI的关键限制以及关于准确性和个性化的研究方向。我们的目标是朝着既能增加透明度又支持以用户为中心体验的可解释人工智能迈进。', 'title_zh': '基于数据中心的多模态解释型人工智能的透明自适应学习'}
{'arxiv_id': 'arXiv:2508.00658', 'title': 'Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies', 'authors': 'Chakattrai Sookkongwaree, Tattep Lakmuang, Chainarong Amornbunchornvej', 'link': 'https://arxiv.org/abs/2508.00658', 'abstract': 'Understanding causal relationships in time series is fundamental to many domains, including neuroscience, economics, and behavioral science. Granger causality is one of the well-known techniques for inferring causality in time series. Typically, Granger causality frameworks have a strong fix-lag assumption between cause and effect, which is often unrealistic in complex systems. While recent work on variable-lag Granger causality (VLGC) addresses this limitation by allowing a cause to influence an effect with different time lags at each time point, it fails to account for the fact that causal interactions may vary not only in time delay but also across frequency bands. For example, in brain signals, alpha-band activity may influence another region with a shorter delay than slower delta-band oscillations. In this work, we formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a novel framework that generalizes traditional VLGC by explicitly modeling frequency-dependent causal delays. We provide a formal definition of MB-VLGC, demonstrate its theoretical soundness, and propose an efficient inference pipeline. Extensive experiments across multiple domains demonstrate that our framework significantly outperforms existing methods on both synthetic and real-world datasets, confirming its broad applicability to any type of time series data. Code and datasets are publicly available.', 'abstract_zh': '多频带可变滞后格兰杰因果关系（MB-VLGC）及其理论与应用', 'title_zh': '多频带可变时间滞后格兰杰因果关系：跨频段因果时间序列推断的统一框架'}
{'arxiv_id': 'arXiv:2508.00632', 'title': 'Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings', 'authors': 'Alexia Jolicoeur-Martineau', 'link': 'https://arxiv.org/abs/2508.00632', 'abstract': 'While AI excels at generating text, audio, images, and videos, creating interactive audio-visual content such as video games remains challenging. Current LLMs can generate JavaScript games and animations, but lack automated evaluation metrics and struggle with complex content that normally requires teams of humans working for many months (multi-shot, multi-agents) using assets made by artists. To tackle these issues, we built a new metric and a multi-agent system.\nWe propose AVR-Eval, a relative metric for multimedia content quality using Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video, and audio) compares the AVRs of two contents, with a text model reviewing evaluations to determine superiority. We show that AVR-Eval properly identifies good from broken or mismatched content.\nWe built AVR-Agent, a multi-agent system generating JavaScript code from a bank of multimedia assets (audio, images, 3D models). The coding agent selects relevant assets, generates multiple initial codes, uses AVR-Eval to identify the best version, and iteratively improves it through omni-modal agent feedback from the AVR.\nWe run experiments on games and animations with AVR-Eval (win rate of content A against B). We find that content generated by AVR-Agent has a significantly higher win rate against content made through one-shot generation. However, models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rate. This reveals a critical gap: while humans benefit from high-quality assets and audio-visual feedback, current coding models do not seem to utilize these resources as effectively, highlighting fundamental differences between human and machine content creation approaches.', 'abstract_zh': '尽管AI在生成文本、音频、图像和视频方面表现出色，但创作交互式音频视觉内容（如视频游戏）仍具有挑战性。当前的大语言模型可以生成JavaScript游戏和动画，但缺乏自动评估指标，并且难以处理通常需要多人团队花费数月时间创作的复杂内容（多回合、多智能体），并且使用的是艺术家制作的资源。为解决这些问题，我们构建了一个新的评估指标和多智能体系统。\n\n我们提出了AVR-Eval，这是一种基于音频-视觉记录（AVRs）的多媒体内容质量相对指标。一种跨模态模型（处理文本、视频和音频）比较两个内容的AVRs，并通过文本模型评审以确定其优势。实验表明AVR-Eval能够正确识别优质内容与受损或匹配不当的内容。\n\n我们构建了AVR-Agent，这是一种多智能体系统，能够从多媒体资产库（音频、图像、3D模型）生成JavaScript代码。编码智能体选择相关资产，生成多个初始代码，使用AVR-Eval识别最佳版本，并通过跨模态智能体反馈从AVR进行迭代优化。\n\n我们在游戏中进行了AVR-Eval实验（内容A对B的胜率）。结果显示，由AVR-Agent生成的内容胜率显著高于通过一次性生成的内容。然而，模型在利用自定义资产和AVR反馈方面表现出色受限，未能实现更高的胜率。这揭示了一个关键差距：尽管人类可以从高质量资产和音频视觉反馈中受益，当前的编码模型似乎未能有效利用这些资源，突显了人类和机器内容创作方法之间的重要差异。', 'title_zh': '基于音频-视觉记录的多agent游戏生成与评估'}
{'arxiv_id': 'arXiv:2508.00581', 'title': 'From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation', 'authors': 'Ruiqing Ding, Qianfang Sun, Yongkang Leng, Hui Yin, Xiaojian Li', 'link': 'https://arxiv.org/abs/2508.00581', 'abstract': 'Pre-consultation is a critical component of effective healthcare delivery. However, generating comprehensive pre-consultation questionnaires from complex, voluminous Electronic Medical Records (EMRs) is a challenging task. Direct Large Language Model (LLM) approaches face difficulties in this task, particularly regarding information completeness, logical order, and disease-level synthesis. To address this issue, we propose a novel multi-stage LLM-driven framework: Stage 1 extracts atomic assertions (key facts with timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes disease knowledge by clustering representative networks from an EMR corpus; Stage 3 generates tailored personal and standardized disease-specific questionnaires based on these structured representations. This framework overcomes limitations of direct methods by building explicit clinical knowledge. Evaluated on a real-world EMR dataset and validated by clinical experts, our method demonstrates superior performance in information coverage, diagnostic relevance, understandability, and generation time, highlighting its practical potential to enhance patient information collection.', 'abstract_zh': '预咨询是有效医疗服务的关键组成部分。然而，从复杂庞大的电子医疗记录（EMRs）中生成全面的预咨询问卷是一项颇具挑战的任务。直接大型语言模型（LLM）方法在这一任务中面临困难，特别是在信息完整性、逻辑顺序和疾病水平综合方面。为解决这一问题，我们提出了一种新颖的多阶段LLM驱动框架：第1阶段从EMRs中提取原子断言（带有时间的关键事实）；第2阶段构建个人因果网络并通过对EMR语料中代表性网络的聚类来合成疾病知识；第3阶段基于这些结构化表示生成个性化的标准化疾病特异性问卷。该框架通过构建明确的临床知识克服了直接方法的限制。在实际EMR数据集上评价并通过临床专家验证，我们的方法在信息覆盖范围、诊断相关性、易理解性和生成时间方面表现出优越性能，突显了其在增强患者信息收集方面的实际潜力。', 'title_zh': '从电子病历数据到临床洞察：一种由LLM驱动的自动化预咨询问卷生成框架'}
{'arxiv_id': 'arXiv:2508.00576', 'title': 'MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models', 'authors': 'Zhanliang Wang, Kai Wang', 'link': 'https://arxiv.org/abs/2508.00576', 'abstract': 'Multimodal AI models have achieved impressive performance in tasks that require integrating information from multiple modalities, such as vision and language. However, their "black-box" nature poses a major barrier to deployment in high-stakes applications where interpretability and trustworthiness are essential. How to explain cross-modal interactions in multimodal AI models remains a major challenge. While existing model explanation methods, such as attention map and Grad-CAM, offer coarse insights into cross-modal relationships, they cannot precisely quantify the synergistic effects between modalities, and are limited to open-source models with accessible internal weights. Here we introduce MultiSHAP, a model-agnostic interpretability framework that leverages the Shapley Interaction Index to attribute multimodal predictions to pairwise interactions between fine-grained visual and textual elements (such as image patches and text tokens), while being applicable to both open- and closed-source models. Our approach provides: (1) instance-level explanations that reveal synergistic and suppressive cross-modal effects for individual samples - "why the model makes a specific prediction on this input", and (2) dataset-level explanation that uncovers generalizable interaction patterns across samples - "how the model integrates information across modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP faithfully captures cross-modal reasoning mechanisms, while real-world case studies demonstrate its practical utility. Our framework is extensible beyond two modalities, offering a general solution for interpreting complex multimodal AI models.', 'abstract_zh': '多模态AI模型已经在需要整合多种模态信息的任务中取得了令人印象深刻的性能，例如视觉和语言任务。然而，它们的“黑盒”性质在高风险应用中成为部署的主要障碍，特别是在需要可解释性和可信度的应用中。如何解释多模态AI模型中的跨模态交互仍然是一个重大挑战。尽管现有的模型解释方法，如注意力图和Grad-CAM，提供了对跨模态关系的粗略洞察，但它们无法精确量化解码不同模态之间的协同效应，并且仅适用于具有可访问内部权重的开源模型。本文介绍了一种基于Shapley相互作用指数的通用可解释性框架——MultiSHAP，该框架能够将多模态预测归因于精细视觉和文本元素（如图像片段和文本令牌）的成对交互，同时适用于开源和封闭源模型。我们的方法提供：(1) 单个示例级别的解释，揭示了特定输入上模型特定预测的协同和抑制的跨模态效应——“为什么模型对该输入做出这种特定预测”，以及(2) 数据集级别的解释，发现跨示例的可泛化的交互模式——“模型如何跨模态整合信息”。在公开的多模态基准测试上的实验证实，MultiSHAP 真实捕捉到了跨模态推理机制，而实际案例研究则展示了其实际应用价值。我们的框架可以扩展到超过两个模态，提供了一种解释复杂多模态AI模型的一般性解决方案。', 'title_zh': '基于Shapley值的多模态AI模型跨模态交互解释框架 MultiSHAP'}
{'arxiv_id': 'arXiv:2508.00500', 'title': 'Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking', 'authors': 'Haoyu Wang, Chris M. Poskitt, Jun Sun, Jiali Wei', 'link': 'https://arxiv.org/abs/2508.00500', 'abstract': 'Large Language Model (LLM) agents exhibit powerful autonomous capabilities across domains such as robotics, virtual assistants, and web automation. However, their stochastic behavior introduces significant safety risks that are difficult to anticipate. Existing rule-based enforcement systems, such as AgentSpec, focus on developing reactive safety rules, which typically respond only when unsafe behavior is imminent or has already occurred. These systems lack foresight and struggle with long-horizon dependencies and distribution shifts. To address these limitations, we propose Pro2Guard, a proactive runtime enforcement framework grounded in probabilistic reachability analysis. Pro2Guard abstracts agent behaviors into symbolic states and learns a Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it anticipates future risks by estimating the probability of reaching unsafe states, triggering interventions before violations occur when the predicted risk exceeds a user-defined threshold. By incorporating semantic validity checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability while approximating the underlying ground-truth model. We evaluate Pro2Guard extensively across two safety-critical domains: embodied household agents and autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early on up to 93.6% of unsafe tasks using low thresholds, while configurable modes (e.g., reflect) allow balancing safety with task success, maintaining up to 80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100% prediction of traffic law violations and collisions, anticipating risks up to 38.66 seconds ahead.', 'abstract_zh': '基于概率可达性的主动运行时防护框架Pro2Guard', 'title_zh': 'Pro2Guard: 基于概率模型检查的主动运行时LLM代理安全 enforcement'}
{'arxiv_id': 'arXiv:2508.00459', 'title': 'Thinking Machines: Mathematical Reasoning in the Age of LLMs', 'authors': 'Andrea Asperti, Alberto Naibo, Claudio Sacerdoti Coen', 'link': 'https://arxiv.org/abs/2508.00459', 'abstract': "Large Language Models (LLMs) have shown remarkable abilities in structured reasoning and symbolic tasks, with coding emerging as a particular area of strength. This success has sparked growing interest in applying LLMs to mathematics, both in informal problem-solving and formal theorem proving. However, progress in formal mathematics has proven to be significantly more difficult, despite surface-level similarities between programming and proof construction. This discrepancy raises important questions about how LLMs ``reason'', how they are supervised, and whether they internally track a notion of computational or deductive state. In this article, we address the state-of-the-art of the discipline, focusing on recent models and benchmarks, and explore three central issues at the intersection of machine learning and mathematical cognition: (i) the trade-offs between formal and informal mathematics as training domains; (ii) the deeper reasons why proof generation remains more brittle than code synthesis; (iii) and the question of whether LLMs represent, or merely mimic, a notion of evolving logical state. Our goal is not to draw hard boundaries, but to identify where the current limits lie, and how they might be extended.", 'abstract_zh': '大型语言模型在结构化推理和符号任务中展现了显著的能力，其中编程尤其突出。这一成功激发了将大型语言模型应用于数学领域的兴趣，包括非形式化问题解决和形式化定理证明。尽管编程和证明构建在表面上存在相似之处，但形式数学的进步证明更为艰难。这种差异引发了关于大型语言模型如何推理、如何监督以及它们是否跟踪计算或演绎状态的重要问题。在本文中，我们聚焦于该领域的最新模型和基准，探讨机器学习与数学认知交叉领域中的三个核心问题：（i）形式数学与非形式数学作为训练领域的权衡；（ii）证明生成为何比代码合成更具脆弱性；（iii）大型语言模型是否代表了还是仅仅是模仿了一种演变逻辑状态的概念。我们的目标不是划定清晰的界限，而是识别当前的限制所在，并探寻如何扩展这些限制。', 'title_zh': '思考机器：在大语言模型时代下的数学推理'}
{'arxiv_id': 'arXiv:2508.00414', 'title': 'Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training', 'authors': 'Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, Dong Yu', 'link': 'https://arxiv.org/abs/2508.00414', 'abstract': 'General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at this https URL', 'abstract_zh': '通用人工智能代理被日益认为是下一代人工智能的基础框架，能够实现复杂的推理、网络交互、编程和自主研究能力。然而，当前的代理系统要么是封闭源代码的，要么严重依赖多种付费API和专有工具，这限制了研究社区的可访问性和可重复性。在此项工作中，我们提出了Cognitive Kernel-Pro，一个完全开源且最大程度上免费的多模块代理框架，旨在普及高级AI代理的开发与评估。在Cognitive Kernel-Pro中，我们系统地研究了代理基础模型高质量训练数据的收集，重点关注在四个关键领域（网络、文件、代码和一般推理）构建查询、轨迹和可验证答案。此外，我们探讨了新的代理测试时反思和投票策略，以提高代理的鲁棒性和性能。我们对Cognitive Kernel-Pro进行了评估，并在GAIA上取得了开源和免费代理的最优结果。值得注意的是，我们8B参数的开源模型超越了之前的WebDancer和WebSailor等领先系统，确立了可访问且高性能AI代理的新标准。代码可用于此链接。', 'title_zh': '认知内核增强：一种深度研究代理及代理基础模型训练的框架'}
{'arxiv_id': 'arXiv:2508.00401', 'title': 'Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation', 'authors': 'Riddhi J. Pitliya, Ozan Catal, Toon Van de Maele, Corrado Pezzato, Tim Verbelen', 'link': 'https://arxiv.org/abs/2508.00401', 'abstract': "We present a novel approach to multi-agent cooperation by implementing theory of mind (ToM) within active inference. ToM - the ability to understand that others can have differing knowledge and goals - enables agents to reason about others' beliefs while planning their own actions. Unlike previous active inference approaches to multi-agent cooperation, our method neither relies on task-specific shared generative models nor requires explicit communication, while being generalisable. In our framework, the ToM-equipped agent maintains distinct representations of its own and others' beliefs and goals. We extend the sophisticated inference tree-based planning algorithm to systematically explore joint policy spaces through recursive reasoning. Our approach is evaluated through collision avoidance and foraging task simulations. Results demonstrate that ToM-equipped agents cooperate better compared to non-ToM counterparts by being able to avoid collisions and reduce redundant efforts. Crucially, ToM agents accomplish this by inferring others' beliefs solely from observable behaviour. This work advances practical applications in artificial intelligence while providing computational insights into ToM.", 'abstract_zh': '我们在主动推断框架中实现理论心智以实现多智能体合作的新方法', 'title_zh': '基于主动推断的理论理解：多代理合作的框架'}
{'arxiv_id': 'arXiv:2508.00378', 'title': 'CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding', 'authors': 'Shixin Yi, Lin Shang', 'link': 'https://arxiv.org/abs/2508.00378', 'abstract': 'Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in vision-language models (VLMs), but it often produces explanations that are linguistically fluent yet lack grounding in visual content. We observe that such hallucinations arise in part from the absence of an explicit verification mechanism during multi-step reasoning. To address this, we propose \\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with \\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces visual verification into the reasoning process. CoRGI follows a three-stage pipeline: it first generates a textual reasoning chain, then extracts supporting visual evidence for each reasoning step via a dedicated module (VEVM), and finally synthesizes the textual rationale with visual evidence to generate a grounded, verified answer. The framework can be integrated with existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR benchmark and find that it improves reasoning performance on two representative open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm the contribution of each step in the verification module, and human evaluations suggest that CoRGI leads to more factual and helpful explanations. We also examine alternative designs for the visual verification step and discuss potential limitations of post-hoc verification frameworks. These findings highlight the importance of grounding intermediate reasoning steps in visual evidence to enhance the robustness of multimodal reasoning.', 'abstract_zh': 'Chain-of-Thought (CoT) 提示在提高视觉语言模型 (VLMs) 的推理能力方面显示出潜力，但往往会产生在语言上流畅但在视觉内容基础上缺乏依据的解释。我们观察到，这种幻觉部分原因是多步推理过程中缺乏显式的验证机制。为此，我们提出了 CoRGI（基于视觉见解的推理链），这是一种模块化框架，将视觉验证引入推理过程。CoRGI 遵循三阶段管道：首先生成文本推理链，然后通过专门模块（VEVM）提取每一步推理的支持性视觉证据，最后综合文本理由与视觉证据生成基于视觉的验证答案。该框架可以与现有的 VLMs 集成而无需端到端重新训练。我们在 VCR 基准上评估了 CoRGI，发现它在 Qwen-2.5VL 和 LLaVA-1.6 这两个代表性开源 VLM 主干上提高了推理性能。消融研究证实了验证模块中每个步骤的贡献，并表明人类评估认为 CoRGI 产生了更准确和有帮助的解释。我们还探讨了视觉验证步骤的替代设计，并讨论了事后验证框架的潜在局限性。研究结果强调了将中间推理步骤与视觉证据联系起来的重要性，以增强多模态推理的鲁棒性。', 'title_zh': 'CoRGI：带有视觉接地的验证链式推理'}
{'arxiv_id': 'arXiv:2508.00324', 'title': 'R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge', 'authors': 'Yeonjun In, Wonjoong Kim, Sangwu Park, Chanyoung Park', 'link': 'https://arxiv.org/abs/2508.00324', 'abstract': 'Although large reasoning models (LRMs) have demonstrated impressive capabilities on complex tasks, recent studies reveal that these models frequently fulfill harmful user instructions, raising significant safety concerns. In this paper, we investigate the underlying cause of LRM safety risks and find that models already possess sufficient safety knowledge but fail to activate it during reasoning. Based on this insight, we propose R1-Act, a simple and efficient post-training method that explicitly triggers safety knowledge through a structured reasoning process. R1-Act achieves strong safety improvements while preserving reasoning performance, outperforming prior alignment methods. Notably, it requires only 1,000 training examples and 90 minutes of training on a single RTX A6000 GPU. Extensive experiments across multiple LRM backbones and sizes demonstrate the robustness, scalability, and practical efficiency of our approach.', 'abstract_zh': '尽管大规模推理模型(LRMs)在复杂任务上展现了令人印象深刻的性能，但近期的研究表明，这些模型经常执行有害的用户指令，这引发了重大安全问题。在本文中，我们探讨了LRM安全风险的根本原因，并发现模型已经具备足够的安全知识，但在推理过程中未能激活它。基于这一洞察，我们提出了一种简单高效的后训练方法R1-Act，通过结构化的推理过程显式触发安全知识。R1-Act在保持推理性能的同时实现了强大的安全性改进，优于之前的对齐方法。值得注意的是，它只需要1,000个训练样本和单个RTX A6000 GPU上90分钟的训练时间。我们针对多个LRM骨干网络和规模进行了广泛的实验，证明了该方法的稳健性、可扩展性和实际效率。', 'title_zh': 'R1-ACT: 通过激活安全知识实现高效推理模型安全对齐'}
{'arxiv_id': 'arXiv:2508.00323', 'title': 'Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning', 'authors': 'Jianyi Zhang, Xu Ji, Ziyin Zhou, Yuchen Zhou, Shubo Shi, Haoyu Wu, Zhen Li, Shizhao Liu', 'link': 'https://arxiv.org/abs/2508.00323', 'abstract': 'Evaluating the performance of visual language models (VLMs) in graphic reasoning tasks has become an important research topic. However, VLMs still show obvious deficiencies in simulating human-level graphic reasoning capabilities, especially in complex graphic reasoning and abstract problem solving, which are less studied and existing studies only focus on simple graphics. To evaluate the performance of VLMs in complex graphic reasoning, we propose ReasonBench, the first evaluation benchmark focused on structured graphic reasoning tasks, which includes 1,613 questions from real-world intelligence tests. ReasonBench covers reasoning dimensions related to location, attribute, quantity, and multi-element tasks, providing a comprehensive evaluation of the performance of VLMs in spatial, relational, and abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including closed-source and open-source models) and reveal significant limitations of current models. Based on these findings, we propose a dual optimization strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability of reasoning by decomposing layers, and ReasonTune enhances the task adaptability of model reasoning through training, all of which improves VLM performance by 33.5\\%. All experimental data and code are in the repository: this https URL.', 'abstract_zh': '评价视觉语言模型在图形推理任务中的性能已成为一个重要研究课题。然而，视觉语言模型在模拟人类级别的图形推理能力方面仍然表现出了明显的不足，尤其是在复杂图形推理和抽象问题解决方面，这些领域尚未得到充分研究，现有研究仅关注简单的图形。为了评估视觉语言模型在复杂图形推理中的性能，我们提出了ReasonBench，这是首个专注于结构化图形推理任务的评估基准，包含来自实际智能测试的1,613个问题。ReasonBench涵盖了与位置、属性、数量和多元素任务相关的推理维度，提供了对视觉语言模型在空间、关系和抽象推理能力方面性能的全面评估。我们 benchmark 了11个主流视觉语言模型（包括闭源和开源模型），揭示了当前模型的重要局限性。基于这些发现，我们提出了双重优化策略：图表推理链（DiaCoT）通过分解层级增强推理的可解释性，而ReasonTune通过训练增强模型推理的任务适应性，这些策略共同提高了视觉语言模型的性能33.5%。所有实验数据和代码可在以下仓库获取：this https URL。', 'title_zh': '俄狄浦斯与斯芬克斯：视觉语言模型在复杂图示推理中的基准测试与改进'}
{'arxiv_id': 'arXiv:2508.00282', 'title': 'Mind the Gap: The Divergence Between Human and LLM-Generated Tasks', 'authors': 'Yi-Long Lu, Jiajun Song, Chunhui Zhang, Wei Wang', 'link': 'https://arxiv.org/abs/2508.00282', 'abstract': "Humans constantly generate a diverse range of tasks guided by internal motivations. While generative agents powered by large language models (LLMs) aim to simulate this complex behavior, it remains uncertain whether they operate on similar cognitive principles. To address this, we conducted a task-generation experiment comparing human responses with those of an LLM agent (GPT-4o). We find that human task generation is consistently influenced by psychological drivers, including personal values (e.g., Openness to Change) and cognitive style. Even when these psychological drivers are explicitly provided to the LLM, it fails to reflect the corresponding behavioral patterns. They produce tasks that are markedly less social, less physical, and thematically biased toward abstraction. Interestingly, while the LLM's tasks were perceived as more fun and novel, this highlights a disconnect between its linguistic proficiency and its capacity to generate human-like, embodied this http URL conclude that there is a core gap between the value-driven, embodied nature of human cognition and the statistical patterns of LLMs, highlighting the necessity of incorporating intrinsic motivation and physical grounding into the design of more human-aligned agents.", 'abstract_zh': '人类不断生成由内在动机引导的多样任务。尽管由大规模语言模型（LLMs）驱动的生成代理旨在模拟这种复杂行为，但尚不清楚它们是否遵循类似的认知原则。为了解决这一问题，我们进行了一个任务生成实验，将人类响应与LLM代理（GPT-4o）的响应进行比较。我们发现，人类任务生成始终受到心理驱动因素的影响，包括个人价值观（如开放性）和认知风格。即使在向LLM明确提供这些心理驱动因素后，它也无法反映相应的行为模式。它们生成的任务在社会性、物理性和主题上都偏向抽象，表现明显不足。有趣的是，尽管LLM生成的任务被认为更具趣味性和新颖性，这突显了其语言能力与其产生类似人类具身任务之间存在的差距。因此，我们得出结论，人类驱动的价值观和具身认知的核心特征与LLM的统计模式之间存在差距，强调了在设计更加与人类对齐的代理时需要纳入内在动机和物理基础的必要性。', 'title_zh': 'Mind the Gap: 人类与LLM生成任务之间的分歧'}
{'arxiv_id': 'arXiv:2508.00271', 'title': 'MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning', 'authors': 'Hongjin Qian, Zheng Liu', 'link': 'https://arxiv.org/abs/2508.00271', 'abstract': 'In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \\textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in this https URL.', 'abstract_zh': '基于学习做事情原则的MetaAgent：一种自主增强性代理范式', 'title_zh': 'MetaAgent：借助工具元学习的自主进化代理'}
{'arxiv_id': 'arXiv:2508.00222', 'title': 'RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization', 'authors': 'Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, Zhi Jin, Fei Huang, Yongbin Li, Ge Li', 'link': 'https://arxiv.org/abs/2508.00222', 'abstract': "Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its inherently on-policy strategy with LLM's immense action space and sparse reward. Further, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel approach that synergizes internal exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components: Multiple Importance Sampling to address for distributional mismatch from external data, and an Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. The results show that RL-PLUS achieves state-of-the-art performance compared with existing RLVR methods on six math reasoning benchmarks and exhibits superior performance on six out-of-distribution reasoning tasks. It also achieves consistent and significant gains across diverse model families, with average relative improvements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across multiple benchmarks indicate that RL-PLUS effectively resolves the capability boundary collapse problem.", 'abstract_zh': '可验证奖励的强化学习（RLVR）显著提升了大型语言模型（LLMs）的复杂推理能力。然而，由于其固有的基于策略方法以及LLMs巨大的动作空间和稀疏奖励，它难以突破基模型的能力边界。进一步地，RLVR可能导致能力边界崩溃，缩小LLMs的问题解决范围。为解决这一问题，我们提出了RL-PLUS，这是一个将内部利用（即思考）与外部数据（即学习）相结合的新方法，以实现更强的推理能力和超越基模型的边界。RL-PLUS整合了两个核心组件：多重重要性采样以解决外部数据的分布不匹配问题，以及基于探索的优点函数以引导模型走向高价值、未探索的推理路径。我们提供了理论分析和广泛的实验来证明我们方法的优越性和普适性。结果显示，与现有的RLVR方法相比，RL-PLUS在六个数学推理基准测试中取得了最先进的性能，并在六个离分布推理任务上表现出色。此外，RL-PLUS在多种模型家族中实现了一致且显著的提升，相对改进范围从21.1%到69.2%。此外，多个基准测试下的Pass@k曲线表明，RL-PLUS有效解决了能力边界崩溃问题。', 'title_zh': 'RL-PLUS: 综合策略优化在强化学习中应对大规模语言模型能力边界萎缩问题'}
{'arxiv_id': 'arXiv:2508.00159', 'title': 'Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power', 'authors': 'Jobst Heitzig, Ram Potham', 'link': 'https://arxiv.org/abs/2508.00159', 'abstract': "Power is a key concept in AI safety: power-seeking as an instrumental goal, sudden or gradual disempowerment of humans, power balance in human-AI interaction and international AI governance. At the same time, power as the ability to pursue diverse goals is essential for wellbeing.\nThis paper explores the idea of promoting both safety and wellbeing by forcing AI agents explicitly to empower humans and to manage the power balance between humans and AI agents in a desirable way. Using a principled, partially axiomatic approach, we design a parametrizable and decomposable objective function that represents an inequality- and risk-averse long-term aggregate of human power. It takes into account humans' bounded rationality and social norms, and, crucially, considers a wide variety of possible human goals.\nWe derive algorithms for computing that metric by backward induction or approximating it via a form of multi-agent reinforcement learning from a given world model. We exemplify the consequences of (softly) maximizing this metric in a variety of paradigmatic situations and describe what instrumental sub-goals it will likely imply. Our cautious assessment is that softly maximizing suitable aggregate metrics of human power might constitute a beneficial objective for agentic AI systems that is safer than direct utility-based objectives.", 'abstract_zh': 'AI安全中的权力是一个关键概念：寻求权力作为工具性目标、人类突然或渐进失权、人类与AI交互中的权力平衡以及国际AI治理。同时，权力作为追求多元目标的能力对福祉至关重要。\n\n本文探讨通过明确促使AI代理增强人类权力并以可接受的方式管理人类与AI代理之间的权力平衡来促进安全与福祉的想法。采用原则性、部分公理化的方法，我们设计了一个可参数化和可分解的目标函数，代表了一个不平等和风险规避的长期综合的人类权力。该函数考虑了人类的有限理性和社会规范，并且关键地考虑了各种可能的人类目标。\n\n我们通过逆向归纳法或通过给定世界模型的多代理强化学习近似计算该指标的算法。我们通过多种典型情况进行说明，并描述了它可能暗示的工具性亚目标。我们的谨慎评估是，适度最大化适合的人类权力综合指标可能构成一种有益的代理AI系统的目标，比直接基于效用的目标更安全。', 'title_zh': '基于模型的软最大化长期人类功率的合适度量方法'}
{'arxiv_id': 'arXiv:2508.00143', 'title': 'Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation', 'authors': 'Danielle R. Thomas, Conrad Borchers, Kenneth R. Koedinger', 'link': 'https://arxiv.org/abs/2508.00143', 'abstract': 'Humans can be notoriously imperfect evaluators. They are often biased, unreliable, and unfit to define "ground truth." Yet, given the surging need to produce large amounts of training data in educational applications using AI, traditional inter-rater reliability (IRR) metrics like Cohen\'s kappa remain central to validating labeled data. IRR remains a cornerstone of many machine learning pipelines for educational data. Take, for example, the classification of tutors\' moves in dialogues or labeling open responses in machine-graded assessments. This position paper argues that overreliance on human IRR as a gatekeeper for annotation quality hampers progress in classifying data in ways that are valid and predictive in relation to improving learning. To address this issue, we highlight five examples of complementary evaluation methods, such as multi-label annotation schemes, expert-based approaches, and close-the-loop validity. We argue that these approaches are in a better position to produce training data and subsequent models that produce improved student learning and more actionable insights than IRR approaches alone. We also emphasize the importance of external validity, for example, by establishing a procedure of validating tutor moves and demonstrating that it works across many categories of tutor actions (e.g., providing hints). We call on the field to rethink annotation quality and ground truth--prioritizing validity and educational impact over consensus alone.', 'abstract_zh': '人类往往是不可靠的数据评估者，但传统的人间同意可靠性（IRR）指标如科恩κ系数仍然是验证标注数据的关键。为了克服对人类IRR作为数据标注质量守门人的过度依赖，本文提出了五种互补的评估方法，如多标签标注方案、专家基于的方法和闭环验证。我们主张这些方法能够比单独使用IRR方法更有效地生成促进学生学习并提供具体洞察的数据和模型。此外，我们强调外部有效性的的重要性，例如通过建立验证助手指引的程序，并证明其在多种助手法类别中的有效性。我们呼吁该领域重新思考标注质量和真实标签的核心问题，优先考虑有效性及其教育影响，而非单纯的一致性。', 'title_zh': '超越一致性：重新思考教育AI标注中的ground truth'}
{'arxiv_id': 'arXiv:2508.00138', 'title': 'Co-Producing AI: Toward an Augmented, Participatory Lifecycle', 'authors': 'Rashid Mushkani, Hugo Berard, Toumadher Ammar, Cassandre Chatonnier, Shin Koseki', 'link': 'https://arxiv.org/abs/2508.00138', 'abstract': 'Despite efforts to mitigate the inherent risks and biases of artificial intelligence (AI) algorithms, these algorithms can disproportionately impact culturally marginalized groups. A range of approaches has been proposed to address or reduce these risks, including the development of ethical guidelines and principles for responsible AI, as well as technical solutions that promote algorithmic fairness. Drawing on design justice, expansive learning theory, and recent empirical work on participatory AI, we argue that mitigating these harms requires a fundamental re-architecture of the AI production pipeline. This re-design should center co-production, diversity, equity, inclusion (DEI), and multidisciplinary collaboration. We introduce an augmented AI lifecycle consisting of five interconnected phases: co-framing, co-design, co-implementation, co-deployment, and co-maintenance. The lifecycle is informed by four multidisciplinary workshops and grounded in themes of distributed authority and iterative knowledge exchange. Finally, we relate the proposed lifecycle to several leading ethical frameworks and outline key research questions that remain for scaling participatory governance.', 'abstract_zh': '尽管努力减轻人工智能（AI）算法固有的风险和偏见，这些算法仍可能不对等地影响文化上的边缘化群体。为了应对或减少这些风险，提出了一系列方法，包括制定负责任AI的伦理准则和促进算法公平的技术解决方案。借鉴设计正义、扩展学习理论以及参与式AI的最新实证研究，我们认为减轻这些危害需要从根本上重构AI生产 pipeline。这种重新设计应以共同生产、多样性和包容性（DEI）以及跨学科合作为中心。我们提出了一种增强的AI生命周期，包括五个相互连接的阶段：共同界定、共同设计、共同实施、共同部署和共同维护。该生命周期受到四项跨学科研讨会的启发，并扎根于分散权威和迭代知识交流的主题。最后，我们将提出的生命周期与几种主要的伦理框架联系起来，并概述了为扩大参与式治理提供关键研究问题。', 'title_zh': '共生产AI： Toward an Augmented, Participatory Lifecycle'}
{'arxiv_id': 'arXiv:2508.00137', 'title': 'SHACL Validation under Graph Updates (Extended Paper)', 'authors': 'Shqiponja Ahmetaj, George Konstantinidis, Magdalena Ortiz, Paolo Pareti, Mantas Simkus', 'link': 'https://arxiv.org/abs/2508.00137', 'abstract': 'SHACL (SHApe Constraint Language) is a W3C standardized constraint language for RDF graphs. In this paper, we study SHACL validation in RDF graphs under updates. We present a SHACL-based update language that can capture intuitive and realistic modifications on RDF graphs and study the problem of static validation under such updates. This problem asks to verify whether every graph that validates a SHACL specification will still do so after applying a given update sequence. More importantly, it provides a basis for further services for reasoning about evolving RDF graphs. Using a regression technique that embeds the update actions into SHACL constraints, we show that static validation under updates can be reduced to (un)satisfiability of constraints in (a minor extension of) SHACL. We analyze the computational complexity of the static validation problem for SHACL and some key fragments. Finally, we present a prototype implementation that performs static validation and other static analysis tasks on SHACL constraints and demonstrate its behavior through preliminary experiments.', 'abstract_zh': 'SHACL（SHApe Constraint Language）是RDF图的W3C标准化约束语言。在本文中，我们研究RDF图在更新下的SHACL验证问题。我们提出了一种基于SHACL的更新语言，可以捕捉RDF图上直观和现实的修改，并研究在这些更新下进行静态验证的问题。这一问题要求验证在应用给定更新序列之后，所有满足SHACL规范的图是否仍然满足规范。更重要的是，它为关于 evolving RDF图的进一步推理提供了一个基础。通过将更新操作嵌入到SHACL约束中的一种回归技术，我们证明在更新下的静态验证可以被归约为（不）满足约束问题（在SHACL的一个小扩展中）。我们分析了SHACL及其某些关键片段的静态验证问题的计算复杂性。最后，我们展示了一个原型实现，该实现对SHACL约束进行静态验证和其他静态分析任务，并通过初步实验展示了其行为。', 'title_zh': '图更新下的SHACL验证（扩展论文）'}
{'arxiv_id': 'arXiv:2508.00129', 'title': 'Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis', 'authors': 'Agustín Borda, Juan Bautista Cabral, Gonzalo Giarda, Diego Nicolás Gimenez Irusta, Paula Pacheco, Alvaro Roy Schachner', 'link': 'https://arxiv.org/abs/2508.00129', 'abstract': 'In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem that can greatly affect the results of a Multi-Criteria Decision Method against a particular set of alternatives. It is therefore useful to have a mechanism that allows one to measure the performance of a method on a set of alternatives. This idea could be taken further to build a global ranking of the effectiveness of different methods to solve a problem. In this paper, we present three tests that detect the presence of Rank Reversals, along with their implementation in the Scikit-Criteria library. We also address the complications that arise when implementing these tests for general scenarios and the design considerations we made to handle them. We close with a discussion about how these additions could play a major role in the judgment of multi-criteria decision methods for problem solving.', 'abstract_zh': '多准则决策分析中，排名反转是一个严重的问题，可能极大影响特定备选方案下多准则决策方法的结果。因此，有必要有一种机制来衡量方法在一组备选方案上的性能。这一想法可以进一步发展，构建不同方法解决同一问题的有效性全球排名。本文介绍了三种检测排名反转的测试，并在Scikit-Criteria库中实现了这些测试。我们还讨论了实现这些测试时遇到的复杂问题以及相应的设计考虑。最后，我们讨论了这些添加如何在多准则决策方法用于问题解决的评估中发挥重要作用。', 'title_zh': '多准则决策分析中算法检测秩颠倒、传递性违反和分解不一致问题'}
{'arxiv_id': 'arXiv:2508.00116', 'title': 'No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence', 'authors': 'Wil M.P. van der Aalst', 'link': 'https://arxiv.org/abs/2508.00116', 'abstract': 'The uptake of Artificial Intelligence (AI) impacts the way we work, interact, do business, and conduct research. However, organizations struggle to apply AI successfully in industrial settings where the focus is on end-to-end operational processes. Here, we consider generative, predictive, and prescriptive AI and elaborate on the challenges of diagnosing and improving such processes. We show that AI needs to be grounded using Object-Centric Process Mining (OCPM). Process-related data are structured and organization-specific and, unlike text, processes are often highly dynamic. OCPM is the missing link connecting data and processes and enables different forms of AI. We use the term Process Intelligence (PI) to refer to the amalgamation of process-centric data-driven techniques able to deal with a variety of object and event types, enabling AI in an organizational context. This paper explains why AI requires PI to improve operational processes and highlights opportunities for successfully combining OCPM and generative, predictive, and prescriptive AI.', 'abstract_zh': '人工智能的应用影响了我们的工作方式、互动方式、商务运作和研究方法。然而，组织在工业环境中很难成功地将人工智能应用于端到端的操作流程。在此，我们考虑生成式、预测式和规范式人工智能，并详细阐述诊断和改进这些流程的挑战。我们表明，人工智能需要通过对象中心的过程挖掘（OCPM）来扎根。相关的流程数据是结构化的、组织特定的，与文本不同，流程通常高度动态。OCPM是连接数据和流程的缺失环节，并能启用不同形式的人工智能。我们使用过程智能（PI）这一术语来指代以过程为中心的数据驱动技术的结合，能够处理各种对象和事件类型，使人工智能在组织环境中得以应用。本文解释了为什么需要过程智能来改进操作流程，并突出了将对象中心的过程挖掘与生成式、预测式和规范式人工智能成功结合的机会。', 'title_zh': '没有 PI 就没有 AI！以对象为中心的过程挖掘作为生成、预测和规范人工智能的使能技术'}
{'arxiv_id': 'arXiv:2508.00106', 'title': 'Hyperproperty-Constrained Secure Reinforcement Learning', 'authors': 'Ernest Bonnah, Luan Viet Nguyen, Khaza Anuarul Hoque', 'link': 'https://arxiv.org/abs/2508.00106', 'abstract': 'Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a domain-specific formal specification language known for its effectiveness in compactly representing security, opacity, and concurrency properties for robotics applications. This paper focuses on HyperTWTL-constrained secure reinforcement learning (SecRL). Although temporal logic-constrained safe reinforcement learning (SRL) is an evolving research problem with several existing literature, there is a significant research gap in exploring security-aware reinforcement learning (RL) using hyperproperties. Given the dynamics of an agent as a Markov Decision Process (MDP) and opacity/security constraints formalized as HyperTWTL, we propose an approach for learning security-aware optimal policies using dynamic Boltzmann softmax RL while satisfying the HyperTWTL constraints. The effectiveness and scalability of our proposed approach are demonstrated using a pick-up and delivery robotic mission case study. We also compare our results with two other baseline RL algorithms, showing that our proposed method outperforms them.', 'abstract_zh': '时间窗口时态逻辑下的超性质约束安全强化学习（HyperTWTL-constrained Secure Reinforcement Learning）', 'title_zh': 'Hyper属性约束的安全强化学习'}
{'arxiv_id': 'arXiv:2508.00081', 'title': 'Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench', 'authors': 'Fred Mutisya, Shikoh Gitau, Nasubo Ongoma, Keith Mbae, Elizabeth Wamicha', 'link': 'https://arxiv.org/abs/2508.00081', 'abstract': 'HealthBench, a benchmark designed to measure the capabilities of AI systems for health better (Arora et al., 2025), has advanced medical language model evaluation through physician-crafted dialogues and transparent rubrics. However, its reliance on expert opinion, rather than high-tier clinical evidence, risks codifying regional biases and individual clinician idiosyncrasies, further compounded by potential biases in automated grading systems. These limitations are particularly magnified in low- and middle-income settings, where issues like sparse neglected tropical disease coverage and region-specific guideline mismatches are prevalent.\nThe unique challenges of the African context, including data scarcity, inadequate infrastructure, and nascent regulatory frameworks, underscore the urgent need for more globally relevant and equitable benchmarks. To address these shortcomings, we propose anchoring reward functions in version-controlled Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and GRADE evidence ratings.\nOur roadmap outlines "evidence-robust" reinforcement learning via rubric-to-guideline linkage, evidence-weighted scoring, and contextual override logic, complemented by a focus on ethical considerations and the integration of delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs, while preserving HealthBench\'s transparency and physician engagement, we aim to foster medical language models that are not only linguistically polished but also clinically trustworthy, ethically sound, and globally relevant.', 'abstract_zh': 'HealthBench: 设计用于更好地衡量AI医疗系统能力的基准（Arora et al., 2025），通过医生设计的对话和透明评估标准推动了医疗语言模型的评估，但其依赖专家意见而非高质量临床证据的风险在于固化区域偏见和个体医师的差异性，并且可能进一步加剧自动评分系统的偏见。这些限制在低收入和中等收入国家尤为显著，在这些国家，如淡漠型热带病覆盖率低和区域具体指南不匹配等问题普遍存在。\n\n非洲背景下独特的挑战，包括数据稀缺、基础设施不足和新兴的监管框架，凸显了更具有全球相关性和公平性的基准的迫切需求。为解决这些不足，我们建议将奖励函数锚定在版本控制系统中的临床实践指南（CPGs）中，这些指南结合了系统性回顾和GRADE证据评级。\n\n我们的路线图包括通过评估标准与指南的链接实现“证据稳健”的强化学习、基于证据的评分、以及情境覆盖逻辑，并注重伦理考量和延迟结果反馈的整合。通过将奖励重新锚定在严格审核的CPGs上，同时保持HealthBench的透明性和医生参与，我们旨在培养不仅在语言上精炼而且在临床中可信、伦理上可靠且全球相关的医疗语言模型。', 'title_zh': '重新审视医疗语言基准中的证据层次结构：对HealthBench的批判性评估'}
{'arxiv_id': 'arXiv:2508.00788', 'title': 'Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models', 'authors': 'Xushuo Tang, Yi Ding, Zhengyi Yang, Yin Chen, Yongrui Gu, Wenke Yang, Mingchen Ju, Xin Cao, Yongfei Liu, Wenjie Zhang', 'link': 'https://arxiv.org/abs/2508.00788', 'abstract': "Large language models (LLMs) are increasingly deployed in sensitive contexts where fairness and inclusivity are critical. Pronoun usage, especially concerning gender-neutral and neopronouns, remains a key challenge for responsible AI. Prior work, such as the MISGENDERED benchmark, revealed significant limitations in earlier LLMs' handling of inclusive pronouns, but was constrained to outdated models and limited evaluations. In this study, we introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs' pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender identity inference. Our results show notable improvements compared with previous studies, especially in binary and gender-neutral pronoun accuracy. However, accuracy on neopronouns and reverse inference tasks remains inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We discuss implications, model-specific observations, and avenues for future inclusive AI research.", 'abstract_zh': '大型语言模型（LLMs）在敏感情境中应用日益增多，其中公平性和包容性至关重要。中性代词及新式代词的使用仍然是负责任AI的关键挑战。此前的研究，如MISGENDERED基准，揭示了早期LLM在处理包容性代词方面的显著限制，但这些研究局限于过时的模型和有限的评估。在本研究中，我们引入了MISGENDERED+，这是一个扩展和更新的基准，用于评估LLM的代词保真度。我们针对零-shot、少-shot及性别身份推理，对五种代表性的LLM（GPT-4o、Claude 4、DeepSeek-V3、Qwen Turbo和Qwen2.5）进行了基准测试。结果显示，与以往研究相比，这些LLM在二元和中性代词准确性方面有显著提高。然而，新式代词和逆向推理任务的准确性仍然不够一致，揭示了身份敏感推理方面的持续差距。我们讨论了研究的意义、模型特定观察以及未来包容性AI研究的方向。', 'title_zh': '他们理解它们吗？大规模语言模型对非二元代词处理的最新评估'}
{'arxiv_id': 'arXiv:2508.00782', 'title': 'SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation', 'authors': 'Kien T. Pham, Yingqing He, Yazhou Xing, Qifeng Chen, Long Chen', 'link': 'https://arxiv.org/abs/2508.00782', 'abstract': 'Audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. However, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. In contrast, we humans can not only naturally identify the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. This useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. As prior methods largely ignore this factor, we present SpA2V, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. SpA2V decomposes the generation process into two stages: 1) Audio-guided Video Planning: We meticulously adapt a state-of-the-art MLLM for a novel task of harnessing spatial and semantic cues from input audio to construct Video Scene Layouts (VSLs). This serves as an intermediate representation to bridge the gap between the audio and video modalities. 2) Layout-grounded Video Generation: We develop an efficient and effective approach to seamlessly integrate VSLs as conditional guidance into pre-trained diffusion models, enabling VSL-grounded video generation in a training-free manner. Extensive experiments demonstrate that SpA2V excels in generating realistic videos with semantic and spatial alignment to the input audios.', 'abstract_zh': '基于音频驱动的视频生成旨在合成与输入音频记录相匹配的现实主义视频，类似于人类仅通过听觉输入就能想象场景的能力。然而，现有方法主要集中在探索语义信息，如音频中存在的声源类别，限制了其生成准确内容和空间构图的视频的能力。相比之下，人类不仅能自然地识别声源的语义类别，还能确定其深层次编码的空间属性，包括位置和运动方向。这些有用信息可以通过考虑从声音固有的物理属性中派生的特定空间指标来阐明，例如响度或频率。由于先验方法大多忽略了这一因素，我们提出SpA2V，这是第一个明确利用音频中的空间听觉线索来生成具有高语义和空间对应关系的视频的框架。SpA2V将生成过程分解为两个阶段：1) 音频引导的视频规划：我们精细地调整了一种最先进的MLLM，用于一项新的任务，即利用输入音频中的空间和语义线索构建视频场景布局（VSLs）。这作为中介表示，为音频和视频模态之间的鸿沟架起桥梁。2) 布局指导的视频生成：我们开发了一种高效且有效的方法，将VSLs无缝集成到预训练的扩散模型中作为条件指导，从而以无训练的方式实现VSL指导的视频生成。广泛的实验表明，SpA2V在生成与输入音频在语义和空间上对齐的现实主义视频方面表现出色。', 'title_zh': 'SpA2V：利用空间听觉线索的音频驱动空间aware视频生成'}
{'arxiv_id': 'arXiv:2508.00766', 'title': 'Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation', 'authors': 'Irene Iele, Francesco Di Feola, Valerio Guarrasi, Paolo Soda', 'link': 'https://arxiv.org/abs/2508.00766', 'abstract': 'Image-to-image translation has emerged as a powerful technique in medical imaging, enabling tasks such as image denoising and cross-modality conversion. However, it suffers from limitations in handling out-of-distribution samples without causing performance degradation. To address this limitation, we propose a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the translation process based on the characteristics of each test sample. Our method introduces a Reconstruction Module to quantify the domain shift and a Dynamic Adaptation Block that selectively modifies the internal features of a pretrained translation model to mitigate the shift without compromising the performance on in-distribution samples that do not require adaptation. We evaluate our approach on two medical image-to-image translation tasks: low-dose CT denoising and T1 to T2 MRI translation, showing consistent improvements over both the baseline translation model without TTA and prior TTA methods. Our analysis highlights the limitations of the state-of-the-art that uniformly apply the adaptation to both out-of-distribution and in-distribution samples, demonstrating that dynamic, sample-specific adjustment offers a promising path to improve model resilience in real-world scenarios. The code is available at: this https URL.', 'abstract_zh': '图像到图像的翻译技术在医学影像中 emerged as a powerful technique in medical imaging, enabling tasks such as image denoising and cross-modality conversion. However, it suffers from limitations in handling out-of-distribution samples without causing performance degradation. To address this limitation, we propose a novel Test-Time Adaptation (TTA)框架 that dynamically adjusts the translation process based on the characteristics of each test sample. Our method introduces a Reconstruction Module to quantify the domain shift and a Dynamic Adaptation Block that selectively modifies the internal features of a pretrained translation model to mitigate the shift without compromising the performance on in-distribution samples that do not require adaptation. We evaluate our approach on two medical image-to-image translation tasks: low-dose CT denoising and T1 to T2 MRI translation, showing consistent improvements over both the baseline translation model without TTA and prior TTA methods. Our analysis highlights the limitations of the state-of-the-art that uniformly apply the adaptation to both out-of-distribution and in-distribution samples, demonstrating that dynamic, sample-specific adjustment offers a promising path to improve model resilience in real-world scenarios. The code is available at: this https URL.', 'title_zh': '基于样本的测试时自适应医学图像到图像翻译'}
{'arxiv_id': 'arXiv:2508.00760', 'title': 'MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations', 'authors': 'Qiyao Xue, Yuchen Dou, Ryan Shi, Xiang Lorraine Li, Wei Gao', 'link': 'https://arxiv.org/abs/2508.00760', 'abstract': 'Hate speech detection on Chinese social networks presents distinct challenges, particularly due to the widespread use of cloaking techniques designed to evade conventional text-based detection systems. Although large language models (LLMs) have recently improved hate speech detection capabilities, the majority of existing work has concentrated on English datasets, with limited attention given to multimodal strategies in the Chinese context. In this study, we propose MMBERT, a novel BERT-based multimodal framework that integrates textual, speech, and visual modalities through a Mixture-of-Experts (MoE) architecture. To address the instability associated with directly integrating MoE into BERT-based models, we develop a progressive three-stage training paradigm. MMBERT incorporates modality-specific experts, a shared self-attention mechanism, and a router-based expert allocation strategy to enhance robustness against adversarial perturbations. Empirical results in several Chinese hate speech datasets show that MMBERT significantly surpasses fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing in-context learning approaches.', 'abstract_zh': '中文标题：中国社交媒体上的仇恨言论检测面临独特挑战，特别是由于广泛使用旨在规避传统基于文本检测系统的伪装技术。尽管大型语言模型（LLMs）最近提高了仇恨言论检测能力，但现有研究大多集中于英文数据集上，在中文多模态策略方面的关注有限。本研究提出了一种新颖的基于BERT的多模态框架MMBERT，通过Mixture-of-Experts（MoE）架构整合文本、语音和视觉模态。为了解决直接将MoE集成到BERT模型中所导致的不稳定性问题，我们开发了一种分阶段的三阶段训练 paradigmn。MMBERT通过模态特定专家、共享自我注意机制以及基于路由器的专家分配策略，增强了对对抗性扰动的鲁棒性。在多个中文仇恨言论数据集上的实证结果表明，MMBERT显著优于微调的BERT编码器模型、微调的LLMs以及利用上下文学习方法的LLMs。', 'title_zh': 'MMBERT：面向遮蔽扰动下 robust  Chinese 恶意言论检测的缩放混合专家多模态 BERT'}
{'arxiv_id': 'arXiv:2508.00754', 'title': 'A Simple and Effective Method for Uncertainty Quantification and OOD Detection', 'authors': 'Yaxin Ma, Benjamin Colburn, Jose C. Principe', 'link': 'https://arxiv.org/abs/2508.00754', 'abstract': 'Bayesian neural networks and deep ensemble methods have been proposed for uncertainty quantification; however, they are computationally intensive and require large storage. By utilizing a single deterministic model, we can solve the above issue. We propose an effective method based on feature space density to quantify uncertainty for distributional shifts and out-of-distribution (OOD) detection. Specifically, we leverage the information potential field derived from kernel density estimation to approximate the feature space density of the training set. By comparing this density with the feature space representation of test samples, we can effectively determine whether a distributional shift has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The results demonstrate that our method outperforms baseline models.', 'abstract_zh': '基于特征空间密度的方法用于量化分布偏移和out-of-distribution检测', 'title_zh': '一种简单有效的不确定性量化和OOD检测方法'}
{'arxiv_id': 'arXiv:2508.00751', 'title': 'Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking', 'authors': 'Qing Zhang, Alex Deng, Michelle Du, Huiji Gao, Liwei He, Sanjeev Katariya', 'link': 'https://arxiv.org/abs/2508.00751', 'abstract': 'Evaluation plays a crucial role in the development of ranking algorithms on search and recommender systems. It enables online platforms to create user-friendly features that drive commercial success in a steady and effective manner. The online environment is particularly conducive to applying causal inference techniques, such as randomized controlled experiments (known as A/B test), which are often more challenging to implement in fields like medicine and public policy. However, businesses face unique challenges when it comes to effective A/B test. Specifically, achieving sufficient statistical power for conversion-based metrics can be time-consuming, especially for significant purchases like booking accommodations. While offline evaluations are quicker and more cost-effective, they often lack accuracy and are inadequate for selecting candidates for A/B test. To address these challenges, we developed interleaving and counterfactual evaluation methods to facilitate rapid online assessments for identifying the most promising candidates for A/B tests. Our approach not only increased the sensitivity of experiments by a factor of up to 100 (depending on the approach and metrics) compared to traditional A/B testing but also streamlined the experimental process. The practical insights gained from usage in production can also benefit organizations with similar interests.', 'abstract_zh': '评估在搜索引擎和推荐系统排名算法的发展中起着至关重要的作用。它使得在线平台能够持续有效地创建用户友好的功能，以推动商业成功。在线环境特别有利于应用因果推断技术，例如随机化控制试验（即A/B测试），而在医学和公共政策等领域实施此类技术往往更具挑战性。然而，企业在有效实施A/B测试方面也面临独特挑战。特别是，对于如预定住宿这类重大购买行为的转化率指标，要实现足够的统计效力可能非常耗时。虽然离线评估更快且成本更低，但它们通常缺乏准确性，不足以选择进行A/B测试的候选对象。为应对这些挑战，我们开发了交错和反事实评估方法，以促进快速在线评估，识别最有潜力的A/B测试候选对象。我们的方法不仅将实验的敏感性提高了最高可达100倍（取决于方法和指标）相比传统的A/B测试，并且简化了实验流程。从实际应用中获得的经验教训还可以惠及具有类似兴趣的组织。', 'title_zh': '利用交替和反事实评价 harnessing 动力为空bnb搜索排名'}
{'arxiv_id': 'arXiv:2508.00748', 'title': 'Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos', 'authors': 'Laura Pedrouzo-Rodriguez, Pedro Delgado-DeRobles, Luis F. Gomez, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez', 'link': 'https://arxiv.org/abs/2508.00748', 'abstract': "Photorealistic talking-head avatars are becoming increasingly common in virtual meetings, gaming, and social platforms. These avatars allow for more immersive communication, but they also introduce serious security risks. One emerging threat is impersonation: an attacker can steal a user's avatar-preserving their appearance and voice-making it nearly impossible to detect its fraudulent usage by sight or sound alone. In this paper, we explore the challenge of biometric verification in such avatar-mediated scenarios. Our main question is whether an individual's facial motion patterns can serve as reliable behavioral biometrics to verify their identity when the avatar's visual appearance is a facsimile of its owner. To answer this question, we introduce a new dataset of realistic avatar videos created using a state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and impostor avatar videos. We also propose a lightweight, explainable spatio-temporal Graph Convolutional Network architecture with temporal attention pooling, that uses only facial landmarks to model dynamic facial gestures. Experimental results demonstrate that facial motion cues enable meaningful identity verification with AUC values approaching 80%. The proposed benchmark and biometric system are available for the research community in order to bring attention to the urgent need for more advanced behavioral biometric defenses in avatar-based communication systems.", 'abstract_zh': '逼真 talking-head  avatar 在虚拟会议、游戏和社会平台中的应用日益增多，这些 avatar 增强了沉浸式通信，但也引入了严重安全风险。一种新兴威胁是冒充：攻击者可以盗用用户的 avatar —— 保留其外观和声音——使得通过视觉或听觉单独检测其欺诈使用几乎不可能。本文探讨了这种 avatar 介导场景中的生物特征验证挑战。我们的主要问题是，当 avatar 的视觉外观与其所有者相似时，个体的面部运动模式是否能够作为可靠的生物行为特征，用于验证其身份。为了解答这个问题，我们引入了一个使用最先进的单次生成模型 GAGAvatar 创建的现实 avatar 视频新数据集，其中包括真实和冒充 avatar 视频。我们还提出了一种轻量级、可解释的时间空间图卷积网络架构，该架构使用时间注意力池化，仅基于面部特征点来建模动态面部表情。实验结果表明，面部运动线索能够实现有意义的身份验证，AUC 值接近 80%。提出的基准和生物特征系统可供研究界使用，旨在引起对基于 avatar 的通信系统中更先进生物行为特征防御的迫切需求的关注。', 'title_zh': '真的是你？探索 photorealistic 沟通头部 avatar 视频中的生物特征验证场景'}
{'arxiv_id': 'arXiv:2508.00743', 'title': 'Agentic large language models improve retrieval-based radiology question answering', 'authors': 'Sebastian Wind, Jeta Sopa, Daniel Truhn, Mahshad Lotfinia, Tri-Thien Nguyen, Keno Bressem, Lisa Adams, Mirabela Rusu, Harald Köstler, Gerhard Wellein, Andreas Maier, Soroosh Tayebi Arasteh', 'link': 'https://arxiv.org/abs/2508.00743', 'abstract': 'Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose an agentic RAG framework enabling LLMs to autonomously decompose radiology questions, iteratively retrieve targeted clinical evidence from Radiopaedia, and dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning diverse architectures, parameter scales (0.5B to >670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized models (e.g., Mistral Large improved from 72% to 81%) and small-scale models (e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B parameters) demonstrated minimal changes (<2% improvement). Additionally, agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models exhibited meaningful improvements (e.g., MedGemma-27B improved from 71% to 81%), indicating complementary roles of retrieval and fine-tuning. These results highlight the potential of agentic frameworks to enhance factuality and diagnostic accuracy in radiology QA, particularly among mid-sized LLMs, warranting future studies to validate their clinical utility.', 'abstract_zh': '临床放射学中的决策越来越多地受益于人工智能（AI），特别是大型语言模型（LLMs）。然而，传统的放射学问题回答（QA）检索增强生成（RAG）系统通常依赖单步检索，限制了它们处理复杂临床推理任务的能力。我们提出了一种自主性RAG框架，使LLMs能够自主分解放射学问题，迭代地从Radiopaedia检索相关的临床证据，并动态合成基于证据的回应。我们使用RSNA-RadioQA和ExtendedQA数据集中104个专家策划的放射学问题，评估了24种不同的LLM架构、参数规模（0.5B至>670B）和训练范式（通用、推理优化、临床微调），评估结果显示，自主性检索显著提高了诊断准确性（零-shot提示：73% vs. 64%；P<0.001；传统在线RAG：73% vs. 68%；P<0.001）。中等规模模型（如Mistral Large改进为81%）和小型模型（如Qwen 2.5-7B改进为71%）取得了最大收益，而非常大规模模型（>200B参数）仅表现出细微变化（<2%改进）。此外，自主性检索减少了幻觉（平均9.4%）的情况，并在46%的情况下检索到相关的临床背景，显著增强了事实的准确性。即使经过临床微调的模型也表现出有意义的改进（如MedGemma-27B从71%提高到81%），表明检索和微调具有互补作用。这些结果强调了自主性框架在提高放射学QA的事实性和诊断准确性方面的潜力，特别是在中等规模LLM中，未来的研究需要验证其临床效用。', 'title_zh': '代理型大型语言模型提升基于检索的放射学问题解答'}
{'arxiv_id': 'arXiv:2508.00741', 'title': 'Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data', 'authors': 'Sohaib Imran, Rob Lamb, Peter M. Atkinson', 'link': 'https://arxiv.org/abs/2508.00741', 'abstract': "Large language models (LLMs) are trained on large corpora, yet it is unclear whether they can reason about the information present within their training data. We design experiments to study out-of-context abduction in LLMs, the ability to infer the most plausible explanations for observations using relevant facts present in training data. We train treatment LLMs on names and behavior descriptions of fictitious chatbots, but not on examples of dialogue with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at least one chatbot's name after observing example responses characteristic of that chatbot. We also find that previously training GPT 4o on descriptions of a chatbot's behavior allows it to display behaviors more characteristic of the chatbot when iteratively trained to display such behaviors. Our results have implications for situational awareness in LLMs and, therefore, for AI safety.", 'abstract_zh': '大型语言模型（LLMs）虽然在大规模语料上进行训练，但尚不清楚它们能否推理其训练数据中存在的信息。我们设计实验研究LLMs的离境 abduction 能力，即利用训练数据中相关的事实来推断观察现象的最可能解释的能力。我们仅对虚构聊天机器人的名称和行为描述进行训练LLM，而不包含与聊天机器人的对话示例。我们发现，OpenAI的GPT-4能够根据具有该聊天机器人特征的示例响应正确推理出至少一个聊天机器人的名称。我们还发现，之前对聊天机器人行为描述的训练能使GPT-4在迭代训练中更体现该聊天机器人的行为特征。我们的结果对于LLMs的情境意识以及AI安全具有重要意义。', 'title_zh': '脱离上下文的推论：大规模语言模型利用早期训练数据中的声明性事实对程序性数据进行推理'}
{'arxiv_id': 'arXiv:2508.00737', 'title': 'How LLMs are Shaping the Future of Virtual Reality', 'authors': 'Süeda Özkaya, Santiago Berrezueta-Guzman, Stefan Wagner', 'link': 'https://arxiv.org/abs/2508.00737', 'abstract': 'The integration of Large Language Models (LLMs) into Virtual Reality (VR) games marks a paradigm shift in the design of immersive, adaptive, and intelligent digital experiences. This paper presents a comprehensive review of recent research at the intersection of LLMs and VR, examining how these models are transforming narrative generation, non-player character (NPC) interactions, accessibility, personalization, and game mastering. Drawing from an analysis of 62 peer reviewed studies published between 2018 and 2025, we identify key application domains ranging from emotionally intelligent NPCs and procedurally generated storytelling to AI-driven adaptive systems and inclusive gameplay interfaces. We also address the major challenges facing this convergence, including real-time performance constraints, memory limitations, ethical risks, and scalability barriers. Our findings highlight that while LLMs significantly enhance realism, creativity, and user engagement in VR environments, their effective deployment requires robust design strategies that integrate multimodal interaction, hybrid AI architectures, and ethical safeguards. The paper concludes by outlining future research directions in multimodal AI, affective computing, reinforcement learning, and open-source development, aiming to guide the responsible advancement of intelligent and inclusive VR systems.', 'abstract_zh': '大型语言模型（LLMs）在虚拟现实（VR）游戏中的集成标志着沉浸式、适应性、智能数字体验设计范式的转变。本文对2018年至2025年间发表的62篇相关研究进行了全面回顾，探讨这些模型如何改变叙事生成、非玩家角色（NPC）交互、可访问性、个性化以及游戏主持等方面。我们识别出了关键的应用领域，包括情绪智能NPC、程序生成叙事、AI驱动的自适应系统以及包容性游戏界面。本文还讨论了这一综合应用面临的主要挑战，包括实时性能限制、内存限制、伦理风险和可扩展性障碍。研究发现虽然LLMs显著增强了VR环境中的真实感、创造力和用户参与度，但其有效部署需要将多模态交互、混合AI架构和伦理保障纳入到稳健的设计策略中。最后，本文概述了未来研究的方向，包括多模态AI、情感计算、强化学习和开源开发，旨在指导智能和包容性VR系统的负责任发展。', 'title_zh': 'LLMs是如何塑造虚拟现实未来的研究'}
{'arxiv_id': 'arXiv:2508.00734', 'title': 'Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems', 'authors': 'Liuyun Xu, Seymour M.J. Spence', 'link': 'https://arxiv.org/abs/2508.00734', 'abstract': 'Existing variance reduction techniques used in stochastic simulations for rare event analysis still require a substantial number of model evaluations to estimate small failure probabilities. In the context of complex, nonlinear finite element modeling environments, this can become computationally challenging-particularly for systems subjected to stochastic excitation. To address this challenge, a multi-fidelity stratified sampling scheme with adaptive machine learning metamodels is introduced for efficiently propagating uncertainties and estimating small failure probabilities. In this approach, a high-fidelity dataset generated through stratified sampling is used to train a deep learning-based metamodel, which then serves as a cost-effective and highly correlated low-fidelity model. An adaptive training scheme is proposed to balance the trade-off between approximation quality and computational demand associated with the development of the low-fidelity model. By integrating the low-fidelity outputs with additional high-fidelity results, an unbiased estimate of the strata-wise failure probabilities is obtained using a multi-fidelity Monte Carlo framework. The overall probability of failure is then computed using the total probability theorem. Application to a full-scale high-rise steel building subjected to stochastic wind excitation demonstrates that the proposed scheme can accurately estimate exceedance probability curves for nonlinear responses of interest, while achieving significant computational savings compared to single-fidelity variance reduction approaches.', 'abstract_zh': '现有用于随机模拟稀有事件分析的方差减少技术仍然需要对模型进行大量评估以估计小的失败概率。在复杂的非线性有限元建模环境中，这可能会变得计算上具有挑战性，特别是在系统受到随机激励的情况下。为应对这一挑战，提出了一种带有自适应机器学习元模型的多保真分层采样方案，用于高效传播不确定性并估计小的失败概率。在该方法中，通过分层采样生成的高保真数据集用于训练基于深度学习的元模型，该模型作为成本效益高且高度相关的低保真模型使用。提出了一个自适应训练方案，以平衡低保真模型开发中近似质量和计算需求之间的trade-off。通过将低保真输出与额外的高保真结果集成，使用多保真蒙特卡罗框架获得各层的无偏失败概率估计。然后，使用全概率定理计算总体失败概率。应用到一个受随机风激励的全规模高层钢建筑表明，所提出的方法可以准确估计感兴趣的非线性响应的超出台阶概率曲线，同时与单保真方差减少方法相比显著节省计算资源。', 'title_zh': '基于机器学习的自适应多保真分层抽样方法及其在非线性随机系统故障分析中的应用'}
{'arxiv_id': 'arXiv:2508.00719', 'title': 'Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA', 'authors': 'Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siwei Liu', 'link': 'https://arxiv.org/abs/2508.00719', 'abstract': 'Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.', 'abstract_zh': '知识图谱问答（KGQA）旨在通过利用知识图谱的关联和语义结构来解释自然语言查询并进行结构化推理，以检索准确的答案。近期的KGQA方法主要遵循检索-推理范式，依赖于GNNs或启发式规则进行静态路径提取，或使用大型语言模型（LLMs）进行动态路径生成和检索与推理的联合执行。然而，前者由于静态路径提取和缺乏上下文优化而适应性有限，后者则由于依赖固定的评分函数和广泛的LLM调用而产生高额的计算成本，并且难以准确评估路径。为解决这些问题，本文提出了一种名为动态自适应MCTS推理（DAMR）的新颖框架，该框架结合了符号搜索和自适应路径评估，以实现高效且上下文感知的KGQA。DAMR采用由基于LLM的规划器引导的蒙特卡洛树搜索（MCTS）骨干，每一步选择top-$k$相关关系以缩减搜索空间。为了提高路径评估的准确性，引入了一种轻量级的基于Transformer的评分器，通过交叉注意力联合编码问题和关系序列来进行上下文感知的合理性估计，使模型在进行多次跳步推理时能够捕捉到细微的语义转换。此外，为缓解高质量监督的稀缺性，DAMR整合了一种动态伪路径精炼机制，该机制定期在搜索过程中从探索到的部分路径中生成训练信号，使评分器能够持续适应推理轨迹分布的变化。在多个KGQA基准上的广泛实验表明，DAMR显著优于现有方法。', 'title_zh': '基于LLM引导的MCTS的动态自适应推理：高效且上下文感知的KGQA'}
{'arxiv_id': 'arXiv:2508.00716', 'title': 'Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning', 'authors': 'Yingxu Wang, Mengzhu Wang, Zhichao Huang, Suyu Liu', 'link': 'https://arxiv.org/abs/2508.00716', 'abstract': 'Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled source graphs to unlabeled target graphs by learning domain-invariant representations, which is essential in applications such as molecular property prediction and social network analysis. However, most existing GDA methods rely on the assumption of clean source labels, which rarely holds in real-world scenarios where annotation noise is pervasive. This label noise severely impairs feature alignment and degrades adaptation performance under domain shifts. To address this challenge, we propose Nested Graph Pseudo-Label Refinement (NeGPR), a novel framework tailored for graph-level domain adaptation with noisy labels. NeGPR first pretrains dual branches, i.e., semantic and topology branches, by enforcing neighborhood consistency in the feature space, thereby reducing the influence of noisy supervision. To bridge domain gaps, NeGPR employs a nested refinement mechanism in which one branch selects high-confidence target samples to guide the adaptation of the other, enabling progressive cross-domain learning. Furthermore, since pseudo-labels may still contain noise and the pre-trained branches are already overfitted to the noisy labels in the source domain, NeGPR incorporates a noise-aware regularization strategy. This regularization is theoretically proven to mitigate the adverse effects of pseudo-label noise, even under the presence of source overfitting, thus enhancing the robustness of the adaptation process. Extensive experiments on benchmark datasets demonstrate that NeGPR consistently outperforms state-of-the-art methods under severe label noise, achieving gains of up to 12.7% in accuracy.', 'abstract_zh': '带噪声标签下图层次伪标签精炼的图层级域适应（Nested Graph Pseudo-Label Refinement (NeGPR) for Graph-Level Domain Adaptation with Noisy Labels）', 'title_zh': '嵌套图伪标签精炼在噪声标签领域适应学习中'}
{'arxiv_id': 'arXiv:2508.00712', 'title': 'JSON-Bag: A generic game trajectory representation', 'authors': 'Dien Nguyen, Diego Perez-Liebana, Simon Lucas', 'link': 'https://arxiv.org/abs/2508.00712', 'abstract': "We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically represent game trajectories by tokenizing their JSON descriptions and apply Jensen-Shannon distance (JSD) as distance metric for them. Using a prototype-based nearest-neighbor search (P-NNS), we evaluate the validity of JSON-Bag with JSD on six tabletop games -- \\textit{7 Wonders}, \\textit{Dominion}, \\textit{Sea Salt and Paper}, \\textit{Can't Stop}, \\textit{Connect4}, \\textit{Dots and boxes} -- each over three game trajectory classification tasks: classifying the playing agents, game parameters, or game seeds that were used to generate the trajectories.\nOur approach outperforms a baseline using hand-crafted features in the majority of tasks. Evaluating on N-shot classification suggests using JSON-Bag prototype to represent game trajectory classes is also sample efficient. Additionally, we demonstrate JSON-Bag ability for automatic feature extraction by treating tokens as individual features to be used in Random Forest to solve the tasks above, which significantly improves accuracy on underperforming tasks. Finally, we show that, across all six games, the JSD between JSON-Bag prototypes of agent classes highly correlates with the distances between agents' policies.", 'abstract_zh': 'JSON 基于词袋的模型及其在桌游轨迹表示中的应用：基于 Jensen-Shannon 距离的 nearest-neighbor 搜索', 'title_zh': 'JSON-Bag: 通用游戏轨迹表示'}
{'arxiv_id': 'arXiv:2508.00709', 'title': 'NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System', 'authors': 'Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra, Ajay Varghese Thomas, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya', 'link': 'https://arxiv.org/abs/2508.00709', 'abstract': 'Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.', 'abstract_zh': '法律判决预测（LJP）已成为法律领域人工智能的关键领域，旨在自动化司法结果预测并增强法律推理的可解释性。虽然以往在印度情境下的方法依赖于案件内部内容，如事实、问题和推理，但往往忽视了普通法系统中依靠成文法和判例这一核心要素。在本工作中，我们提出了一种名为NyayaRAG的检索增强生成（RAG）框架，通过向模型提供案情描述、相关法律条文以及语义检索的先例案情，模拟真实的法庭场景。NyayaRAG使用针对印度法律体系定制的特定领域管道，评估这些综合输入在预测法院裁决和生成法律解释方面的有效性。我们使用标准的词汇和语义指标以及基于LLM的评估工具（如G-Eval）对各种输入配置进行性能评估。结果表明，将事实性输入与结构化的法律知识相结合，显著提高了预测准确性和解释质量。', 'title_zh': 'NyayaRAG：基于印度普通法体系的现实主义法律判决预测'}
{'arxiv_id': 'arXiv:2508.00707', 'title': 'Efficient Solution and Learning of Robust Factored MDPs', 'authors': 'Yannik Schnitzer, Alessandro Abate, David Parker', 'link': 'https://arxiv.org/abs/2508.00707', 'abstract': 'Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling epistemic uncertainty about transition dynamics. Learning r-MDPs from interactions with an unknown environment enables the synthesis of robust policies with provable (PAC) guarantees on performance, but this can require a large number of sample interactions. We propose novel methods for solving and learning r-MDPs based on factored state-space representations that leverage the independence between model uncertainty across system components. Although policy synthesis for factored r-MDPs leads to hard, non-convex optimisation problems, we show how to reformulate these into tractable linear programs. Building on these, we also propose methods to learn factored model representations directly. Our experimental results show that exploiting factored structure can yield dimensional gains in sample efficiency, producing more effective robust policies with tighter performance guarantees than state-of-the-art methods.', 'abstract_zh': '鲁棒马尔可夫决策过程（r-MDPs）通过明确建模关于转换动态的认知不确定性来扩展MDP。通过与未知环境的交互学习r-MDPs能够合成具有可证明（PAC）性能保证的稳健策略，但这也可能需要大量的样本交互。我们提出了一种基于因子状态空间表示的新方法，利用系统组件间模型不确定性之间的独立性。尽管因子r-MDPs的策略合成导致了难以处理的非凸优化问题，我们展示了如何将其重新表述为可处理的线性规划问题。在此基础上，我们还提出了一种直接学习因子模型表示的方法。我们的实验结果表明，利用因子结构可以实现样本效率上的维度增益，从而生成具有更紧要性能保证的更有效的稳健策略，超越了现有最先进的方法。', 'title_zh': '高效的robust factored MDPs的解算与学习'}
{'arxiv_id': 'arXiv:2508.00701', 'title': 'D3: Training-Free AI-Generated Video Detection Using Second-Order Features', 'authors': 'Chende Zheng, Ruiqi suo, Chenhao Lin, Zhengyu Zhao, Le Yang, Shuai Liu, Minghui Yang, Cong Wang, Chao Shen', 'link': 'https://arxiv.org/abs/2508.00701', 'abstract': "The evolution of video generation techniques, such as Sora, has made it increasingly easy to produce high-fidelity AI-generated videos, raising public concern over the dissemination of synthetic content. However, existing detection methodologies remain limited by their insufficient exploration of temporal artifacts in synthetic videos. To bridge this gap, we establish a theoretical framework through second-order dynamical analysis under Newtonian mechanics, subsequently extending the Second-order Central Difference features tailored for temporal artifact detection. Building on this theoretical foundation, we reveal a fundamental divergence in second-order feature distributions between real and AI-generated videos. Concretely, we propose Detection by Difference of Differences (D3), a novel training-free detection method that leverages the above second-order temporal discrepancies. We validate the superiority of our D3 on 4 open-source datasets (Gen-Video, VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo, D3 outperforms the previous best method by 10.39% (absolute) mean Average Precision. Additional experiments on time cost and post-processing operations demonstrate D3's exceptional computational efficiency and strong robust performance. Our code is available at this https URL.", 'abstract_zh': '视频生成技术（如Sora）的进化使得高保真AI生成视频的生产变得日益容易，引发了公众对合成内容传播的关注。然而，现有的检测方法仍然受限于它们对合成视频中时间伪迹的不足探索。为解决这一问题，我们通过牛顿力学下的二次动力学分析建立了一个理论框架，随后扩展了适用于时间伪迹检测的二次中心差分特征。基于这一理论基础，我们揭示了实际视频和AI生成视频在二次特征分布上的基本差异。具体而言，我们提出了差的差检测（D3），这是一种无需训练的新型检测方法，利用上述二次时间差异。我们在4个开源数据集（Gen-Video、VideoPhy、EvalCrafter、VidProM）的总计40个子集上验证了D3的优势，在GenVideo数据集上，D3绝对均值平均精度优于之前的最佳方法10.39%。额外的实验展示了D3的出色计算效率和强大的稳健性能。我们的代码可在以下链接获取。', 'title_zh': 'D3: 基于二阶特征的无需训练的AI生成视频检测'}
{'arxiv_id': 'arXiv:2508.00697', 'title': 'On-Device Diffusion Transformer Policy for Efficient Robot Manipulation', 'authors': 'Yiming Wu, Huan Wang, Zhenghao Chen, Jianxin Pang, Dong Xu', 'link': 'https://arxiv.org/abs/2508.00697', 'abstract': "Diffusion Policies have significantly advanced robotic manipulation tasks via imitation learning, but their application on resource-constrained mobile platforms remains challenging due to computational inefficiency and extensive memory footprint. In this paper, we propose LightDP, a novel framework specifically designed to accelerate Diffusion Policies for real-time deployment on mobile devices. LightDP addresses the computational bottleneck through two core strategies: network compression of the denoising modules and reduction of the required sampling steps. We first conduct an extensive computational analysis on existing Diffusion Policy architectures, identifying the denoising network as the primary contributor to latency. To overcome performance degradation typically associated with conventional pruning methods, we introduce a unified pruning and retraining pipeline, optimizing the model's post-pruning recoverability explicitly. Furthermore, we combine pruning techniques with consistency distillation to effectively reduce sampling steps while maintaining action prediction accuracy. Experimental evaluations on the standard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that LightDP achieves real-time action prediction on mobile devices with competitive performance, marking an important step toward practical deployment of diffusion-based policies in resource-limited environments. Extensive real-world experiments also show the proposed LightDP can achieve performance comparable to state-of-the-art Diffusion Policies.", 'abstract_zh': '轻量级扩散策略：一种针对移动平台实时部署的新型加速框架', 'title_zh': '设备端扩散变换器策略高效机器人操作'}
{'arxiv_id': 'arXiv:2508.00679', 'title': 'Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries', 'authors': 'Shubham Kumar Nigam, Tanmay Dubey, Noel Shallum, Arnab Bhattacharya', 'link': 'https://arxiv.org/abs/2508.00679', 'abstract': 'Legal precedent retrieval is a cornerstone of the common law system, governed by the principle of stare decisis, which demands consistency in judicial decisions. However, the growing complexity and volume of legal documents challenge traditional retrieval methods. TraceRetriever mirrors real-world legal search by operating with limited case information, extracting only rhetorically significant segments instead of requiring complete documents. Our pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining initial results through Reciprocal Rank Fusion before final re-ranking. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever addresses growing document volume challenges while aligning with practical search constraints, reliable and scalable foundation for precedent retrieval enhancing legal research when only partial case knowledge is available.', 'abstract_zh': '法律先例检索是普通法体系的基石，由 stare decisis 原则治理，要求在司法判决中保持一致性。然而，法律文件的日益复杂性和数量挑战传统的检索方法。TraceRetriever 通过利用有限的案例信息进行操作，提取具有修辞意义的片段，而不是要求完整文档。我们的管道整合了 BM25、向量数据库和跨编码器模型，在通过互惠秩融合进行初步结果的结合后，进行最终重排序。使用层次双向 LSTM CRF 分类器在印度判决上训练生成修辞注解。TraceRetriever 在 IL-PCR 和 COLIEE 2025 数据集上进行评估，解决日益增长的文档量挑战，同时符合实际搜索约束，为在仅部分案件知识可用时增强法律研究提供可靠且可扩展的先例检索基础。', 'title_zh': '先分段，后检索：基于修辞角色的高效法律搜索'}
{'arxiv_id': 'arXiv:2508.00669', 'title': 'Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications', 'authors': 'Wenxuan Wang, Zizhan Ma, Meidan Ding, Shiyi Zheng, Shengyuan Liu, Jie Liu, Jiaming Ji, Wenting Chen, Xiang Li, Linlin Shen, Yixuan Yuan', 'link': 'https://arxiv.org/abs/2508.00669', 'abstract': 'The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.', 'abstract_zh': '大型语言模型在医学中的普及虽然彰显了 impressive 能力，但其在系统性、透明性和可验证性推理方面仍存在关键缺口，这是临床实践的基石。这推动了从单步骤答案生成向专门为医学推理设计的大规模语言模型的转变。本文首次系统回顾了这一新兴领域。我们提出了推理增强技术的分类，分为训练时策略（如监督微调、强化学习）和测试时机制（如提示工程、多智能体系统）。我们分析了这些技术在不同数据模态（文本、图像、代码）和关键临床应用（如诊断、教育、治疗规划）中的应用。此外，我们回顾了从简单准确性指标到复杂推理质量和可视化解释评估的评价基准演变。基于对 2022-2025 年 60 项开创性研究的分析，我们指出了关键挑战，包括忠实性-合理性差距和原生多模态推理的需要，并概述了构建高效、稳健且社会技术上负责任的医疗人工智能的未来方向。', 'title_zh': 'LLMs时代医学推理的增强技术与应用系统评价'}
{'arxiv_id': 'arXiv:2508.00668', 'title': 'Advancing Quantum Information Science Pre-College Education: The Case for Learning Sciences Collaboration', 'authors': 'Raquel Coelho, Roy Pea, Christian Schunn, Jinglei Cheng, Junyu Liu', 'link': 'https://arxiv.org/abs/2508.00668', 'abstract': 'As quantum information science advances and the need for pre-college engagement grows, a critical question remains: How can young learners be prepared to participate in a field so radically different from what they have encountered before? This paper argues that meeting this challenge will require strong interdisciplinary collaboration with the Learning Sciences (LS), a field dedicated to understanding how people learn and designing theory-guided environments to support learning. Drawing on lessons from previous STEM education efforts, we discuss two key contributions of the learning sciences to quantum information science (QIS) education. The first is design-based research, the signature methodology of learning sciences, which can inform the development, refinement, and scaling of effective QIS learning experiences. The second is a framework for reshaping how learners reason about, learn and participate in QIS practices through shifts in knowledge representations that provide new forms of engagement and associated learning. We call for a two-way partnership between quantum information science and the learning sciences, one that not only supports learning in quantum concepts and practices but also improves our understanding of how to teach and support learning in highly complex domains. We also consider potential questions involved in bridging these disciplinary communities and argue that the theoretical and practical benefits justify the effort.', 'abstract_zh': '随着量子信息科学的发展和对中学前教育参与需求的增加，一个关键问题仍然存在：如何准备年轻的学习者参与这样一个与以往遇到的领域截然不同的领域？本文认为，应对这一挑战将需要与学习科学（LS）领域的跨学科合作，该领域致力于理解人们如何学习并设计以理论为导向的支持学习的环境。借鉴以往STEM教育的努力，我们讨论了学习科学对量子信息科学（QIS）教育的两大关键贡献。首先是学习科学的标志性方法——设计导向的研究，可以指导有效的QIS学习体验的发展、完善和扩展。其次是通过知识表示的转变来重塑学习者对QIS实践的思考、学习和参与的方式，提供新的参与形式和相关学习机会。我们呼吁量子信息科学与学习科学之间建立双向伙伴关系，不仅支持量子概念和实践的学习，还提高我们对如何在高度复杂领域进行教学和促进学习的理解。我们还考虑了连接这些学科社区可能涉及的问题，并认为理论和实践的益处值得付出努力。', 'title_zh': '推进量子信息科学中学教育：学习科学合作的必要性'}
{'arxiv_id': 'arXiv:2508.00620', 'title': 'Backdoor Attacks on Deep Learning Face Detection', 'authors': 'Quentin Le Roux, Yannick Teglia, Teddy Furon, Philippe Loubet-Moundi', 'link': 'https://arxiv.org/abs/2508.00620', 'abstract': 'Face Recognition Systems that operate in unconstrained environments capture images under varying conditions,such as inconsistent lighting, or diverse face poses. These challenges require including a Face Detection module that regresses bounding boxes and landmark coordinates for proper Face Alignment. This paper shows the effectiveness of Object Generation Attacks on Face Detection, dubbed Face Generation Attacks, and demonstrates for the first time a Landmark Shift Attack that backdoors the coordinate regression task performed by face detectors. We then offer mitigations against these vulnerabilities.', 'abstract_zh': '不受约束环境下操作的面部识别系统捕获在不同条件下的图像，如不一致的光照或多样的面部姿态。这些挑战需要包含一个面部检测模块，该模块回归边界框和关键点坐标以实现正确的面部对齐。本文展示了面向检测的物体生成攻击（Face Generation Attacks）的有效性，并首次展示了针对面部检测器执行的坐标回归任务的地标位移攻击（Landmark Shift Attack）。然后我们提出了针对这些漏洞的缓解措施。', 'title_zh': '深度学习人脸识别中的后门攻击'}
{'arxiv_id': 'arXiv:2508.00615', 'title': 'Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data', 'authors': 'Mukesh Kumar Sahu, Pinki Roy', 'link': 'https://arxiv.org/abs/2508.00615', 'abstract': 'Accurately predicting the criticalness of ICU patients (such as in-ICU mortality risk) is vital for early intervention in critical care. However, conventional models often treat each patient in isolation and struggle to exploit the relational structure in Electronic Health Records (EHR). We propose a Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds a patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN architecture that operates on this graph to predict patient mortality and a continuous criticalness score. SBSCGM uses a hybrid similarity measure (combining feature-based and structural similarities) to connect patients with analogous clinical profiles in real-time. The HybridGraphMedGNN integrates Graph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT) layers to learn robust patient representations, leveraging both local and global graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III dataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$) outperforming baseline classifiers and single-type GNN models. We also demonstrate improved precision/recall and show that the attention mechanism provides interpretable insights into model predictions. Our framework offers a scalable and interpretable solution for critical care risk prediction, with potential to support clinicians in real-world ICU deployment.', 'abstract_zh': '基于相似性自我构建图模型及其在ICU患者重症危险性预测中的应用：HybridGraphMedGNN架构', 'title_zh': '基于相似性自我构造图模型：利用图神经网络和电子健康记录预测患者危重程度'}
{'arxiv_id': 'arXiv:2508.00614', 'title': "Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?", 'authors': 'Lennart Meincke, Ethan Mollick, Lilach Mollick, Dan Shapiro', 'link': 'https://arxiv.org/abs/2508.00614', 'abstract': "This is the third in a series of short reports that seek to help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. In this report, we investigate two commonly held prompting beliefs: a) offering to tip the AI model and b) threatening the AI model. Tipping was a commonly shared tactic for improving AI performance and threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025, 8:20) who observed that 'models tend to do better if you threaten them,' a claim we subject to empirical testing here. We evaluate model performance on GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\nWe demonstrate two things:\n- Threatening or tipping a model generally has no significant effect on benchmark performance.\n- Prompt variations can significantly affect performance on a per-question level. However, it is hard to know in advance whether a particular prompting approach will help or harm the LLM's ability to answer any particular question.\nTaken together, this suggests that simple prompting variations might not be as effective as previously assumed, especially for difficult problems. However, as reported previously (Meincke et al. 2025a), prompting approaches can yield significantly different results for individual questions.", 'abstract_zh': '这是关于通过严谨测试理解AI技术细节的一系列简报中的第三篇，旨在帮助企业、教育和政策领导者了解与AI合作的技术细节。本报告调查了两种常见的提示观念：a) 提供奖励给AI模型；b) 对AI模型进行威胁。奖励是一种常见的提高AI性能的技术，谷歌创始人谢尔盖·布林曾表示“威胁AI模型会使它们表现更好”（All-In, 2025年5月，8:20），我们在此对其进行实证检验。我们评估了模型在GPQA（Rein等人，2024）和MMLU-Pro（Wang等人，2024）上的性能。\n\n我们证明了以下两点：\n- 对模型进行威胁或奖励通常对基准性能没有显著影响。\n- 不同的提示方式可以在单个问题层面显著影响性能。但是，很难提前知道某种特定的提示方法是否会帮助或损害大型语言模型回答特定问题的能力。\n这些发现表明，简单的提示变化可能没有预期的那么有效，尤其是在解决难题方面。然而，如前所报（Meincke等人，2025a），提示方法可能会在个别问题上产生显著不同的结果。', 'title_zh': '提示科学报告 3：我会付费或者我会杀你——但你会在意吗？'}
{'arxiv_id': 'arXiv:2508.00604', 'title': 'Composable OS Kernel Architectures for Autonomous Intelligence', 'authors': 'Rajpreet Singh, Vidhi Kothari', 'link': 'https://arxiv.org/abs/2508.00604', 'abstract': 'As intelligent systems permeate edge devices, cloud infrastructure, and embedded real-time environments, this research proposes a new OS kernel architecture for intelligent systems, transforming kernels from static resource managers to adaptive, AI-integrated platforms. Key contributions include: (1) treating Loadable Kernel Modules (LKMs) as AI-oriented computation units for fast sensory and cognitive processing in kernel space; (2) expanding the Linux kernel into an AI-native environment with built-in deep learning inference, floating-point acceleration, and real-time adaptive scheduling for efficient ML workloads; and (3) introducing a Neurosymbolic kernel design leveraging Category Theory and Homotopy Type Theory to unify symbolic reasoning and differentiable logic within OS internals. Together, these approaches enable operating systems to proactively anticipate and adapt to the cognitive needs of autonomous intelligent applications.', 'abstract_zh': '随着智能系统渗透到边缘设备、云计算基础设施和嵌入式实时环境中，本研究提出了一种新的OS内核架构，将内核从静态资源管理器转变为适应性强、集成人工智能的平台。主要贡献包括：（1）将可加载内核模块（LKMs）视为面向人工智能的计算单元，以实现快速内核空间中的感知和认知处理；（2）将Linux内核扩展为内置深度学习推理、浮点加速和实时自适应调度的原生AI环境，以高效处理机器学习工作负载；（3）引入基于范畴论和同伦类型论的神经符号性内核设计，以内核内部统一符号推理和可微逻辑。这些方法共同使操作系统能够主动预见并适应自主智能应用的认知需求。', 'title_zh': '可组合自主智能操作系统内核架构'}
{'arxiv_id': 'arXiv:2508.00602', 'title': 'LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks', 'authors': 'Francesco Panebianco, Stefano Bonfanti, Francesco Trovò, Michele Carminati', 'link': 'https://arxiv.org/abs/2508.00602', 'abstract': 'The generalization capabilities of Large Language Models (LLMs) have led to their widespread deployment across various applications. However, this increased adoption has introduced several security threats, notably in the forms of jailbreaking and data leakage attacks. Additionally, Retrieval Augmented Generation (RAG), while enhancing context-awareness in LLM responses, has inadvertently introduced vulnerabilities that can result in the leakage of sensitive information. Our contributions are twofold. First, we introduce a methodology to analyze historical interaction data from an LLM system, enabling the generation of usage maps categorized by topics (including adversarial interactions). This approach further provides forensic insights for tracking the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a model-agnostic framework that combines static analysis for forensic insights with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique identifies topic groups and detects anomalous patterns, allowing for proactive defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1) jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage, supported by a curated dataset of labeled LLM interactions. In the static setting, LeakSealer achieves the highest precision and recall on the ToxicChat dataset when identifying prompt injection. In the dynamic setting, PII leakage detection achieves an AUPRC of $0.97$, significantly outperforming baselines such as Llama Guard.', 'abstract_zh': '大型语言模型（LLMs）的通用化能力促进了其在各种应用中的广泛应用，然而这种广泛应用也引入了多种安全威胁，尤其是 Jailbreaking 和数据泄漏攻击。此外，检索增强生成（RAG）虽然增强了LLM响应的语境意识，但也无意中引入了可能导致敏感信息泄露的漏洞。我们的贡献主要有两方面。首先，我们提出了一种方法来分析LLM系统的 historical interaction 数据，生成按主题分类的使用图谱（包括对抗性交互），这一方法进一步提供了法医洞察，用于追踪 Jailbreaking 攻击模式的演变。其次，我们提出了一个模型无关的框架 LeakSealer，该框架结合了静态分析和动态防御，采用人工介入的流程（Human-In-The-Loop, HITL）。该技术可以识别主题组和检测异常模式，从而实现主动防御机制。我们通过两种场景对 LeakSealer 进行了实证评估：（1）针对 Jailbreak 尝试，使用公开基准数据集；（2）针对 PII 泄露，使用标记的 LLML 交互数据集。在静态环境中，LeakSealer 在识别提示注入时在 ToxicChat 数据集上实现了最高的准确率和召回率。在动态环境中，PII 泄露检测的 AUPRC 达到 0.97，显著优于 Llama Guard 等基线方法。', 'title_zh': 'LeakSealer：一种半监督防御方法，用于防范大型语言模型的提示注入和泄漏攻击'}
{'arxiv_id': 'arXiv:2508.00591', 'title': 'Wukong Framework for Not Safe For Work Detection in Text-to-Image systems', 'authors': 'Mingrui Liu, Sixiao Zhang, Cheng Long', 'link': 'https://arxiv.org/abs/2508.00591', 'abstract': "Text-to-Image (T2I) generation is a popular AI-generated content (AIGC) technology enabling diverse and creative image synthesis. However, some outputs may contain Not Safe For Work (NSFW) content (e.g., violence), violating community guidelines. Detecting NSFW content efficiently and accurately, known as external safeguarding, is essential. Existing external safeguards fall into two types: text filters, which analyze user prompts but overlook T2I model-specific variations and are prone to adversarial attacks; and image filters, which analyze final generated images but are computationally costly and introduce latency. Diffusion models, the foundation of modern T2I systems like Stable Diffusion, generate images through iterative denoising using a U-Net architecture with ResNet and Transformer blocks. We observe that: (1) early denoising steps define the semantic layout of the image, and (2) cross-attention layers in U-Net are crucial for aligning text and image regions. Based on these insights, we propose Wukong, a transformer-based NSFW detection framework that leverages intermediate outputs from early denoising steps and reuses U-Net's pre-trained cross-attention parameters. Wukong operates within the diffusion process, enabling early detection without waiting for full image generation. We also introduce a new dataset containing prompts, seeds, and image-specific NSFW labels, and evaluate Wukong on this and two public benchmarks. Results show that Wukong significantly outperforms text-based safeguards and achieves comparable accuracy of image filters, while offering much greater efficiency.", 'abstract_zh': '基于文本到图像的非工作场所不适合内容检测框架：Wukong', 'title_zh': 'Wukong框架：适用于文本到图像系统中的不适合公开内容检测'}
{'arxiv_id': 'arXiv:2508.00580', 'title': 'OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery', 'authors': 'Raul Castilla-Arquillo, Carlos Perez-del-Pulgar, Levin Gerdes, Alfonso Garcia-Cerezo, Miguel A. Olivares-Mendez', 'link': 'https://arxiv.org/abs/2508.00580', 'abstract': 'Robot navigation in unstructured environments requires multimodal perception systems that can support safe navigation. Multimodality enables the integration of complementary information collected by different sensors. However, this information must be processed by machine learning algorithms specifically designed to leverage heterogeneous data. Furthermore, it is necessary to identify which sensor modalities are most informative for navigation in the target environment. In Martian exploration, thermal imagery has proven valuable for assessing terrain safety due to differences in thermal behaviour between soil types. This work presents OmniUnet, a transformer-based neural network architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T) imagery. A custom multimodal sensor housing was developed using 3D printing and mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a multimodal dataset in the Bardenas semi-desert in northern Spain. This location serves as a representative environment of the Martian surface, featuring terrain types such as sand, bedrock, and compact soil. A subset of this dataset was manually labeled to support supervised training of the network. The model was evaluated both quantitatively and qualitatively, achieving a pixel accuracy of 80.37% and demonstrating strong performance in segmenting complex unstructured terrain. Inference tests yielded an average prediction time of 673 ms on a resource-constrained computer (Jetson Orin Nano), confirming its suitability for on-robot deployment. The software implementation of the network and the labeled dataset have been made publicly available to support future research in multimodal terrain perception for planetary robotics.', 'abstract_zh': '火星探索中未结构化环境中的机器人导航需要多模态感知系统以支持安全导航。多模态性允许不同传感器收集的互补信息的整合。然而，这些信息必须通过专门设计用于利用异构数据的机器学习算法进行处理。此外，还需要确定哪些传感器模态在目标环境中最具导航信息。为此，本研究提出了基于转换器的 OmniUnet 神经网络架构，用于使用 RGB、深度和热成像（RGB-D-T）进行语义分割。我们开发了一个自定义的多模态传感器外壳，利用3D打印技术安装在火星自主漫游车测试平台（MaRTA）上，收集西班牙北部巴丹纳斯半沙漠区域的多模态数据集。该位置作为火星表面的代表性环境，具备诸如沙地、岩基和紧实土壤等多种地形类型。从该数据集中手动标注了一部分数据以支持网络的监督训练。模型在定量和定性评估中分别达到了80.37%的像素准确率，并在分割复杂未结构化地形方面表现出强劲性能。推断测试显示，在资源受限的计算机（Jetson Orin Nano）上平均每预测时间为673 ms，证实了其适用于机器人本体部署的适用性。网络的软件实现和标注数据集已公开发布，以支持未来在行星机器人多模态地形感知领域的研究。', 'title_zh': '全方位UNET：基于RGB、深度和红外成像的行星探测车不规则地形分割多模态网络'}
{'arxiv_id': 'arXiv:2508.00575', 'title': 'Analysing Temporal Reasoning in Description Logics Using Formal Grammars', 'authors': 'Camille Bourgaux, Anton Gnatenko, Michaël Thomazo', 'link': 'https://arxiv.org/abs/2508.00575', 'abstract': 'We establish a correspondence between (fragments of) $\\mathcal{TEL}^\\bigcirc$, a temporal extension of the $\\mathcal{EL}$ description logic with the LTL operator $\\bigcirc^k$, and some specific kinds of formal grammars, in particular, conjunctive grammars (context-free grammars equipped with the operation of intersection). This connection implies that $\\mathcal{TEL}^\\bigcirc$ does not possess the property of ultimate periodicity of models, and further leads to undecidability of query answering in $\\mathcal{TEL}^\\bigcirc$, closing a question left open since the introduction of $\\mathcal{TEL}^\\bigcirc$. Moreover, it also allows to establish decidability of query answering for some new interesting fragments of $\\mathcal{TEL}^\\bigcirc$, and to reuse for this purpose existing tools and algorithms for conjunctive grammars.', 'abstract_zh': '我们将时间扩展的$\\mathcal{EL}$描述逻辑的片段$\\mathcal{TEL}^\\bigcirc$与一些特定类型的正式文法，特别是并发文法（结合了交操作的上下文免费文法），建立对应关系。这一连接意味着$\\mathcal{TEL}^\\bigcirc$不具备模型终极周期性的性质，进一步导致在$\\mathcal{TEL}^\\bigcirc$中查询回答的不可判定性，从而关闭了一个自引入$\\mathcal{TEL}^\\bigcirc$以来遗留的问题。此外，这也允许为$\\mathcal{TEL}^\\bigcirc$的某些新的有趣片段建立查询回答的可判定性，并利用并发文法现有的工具和算法来实现。', 'title_zh': '使用形式文法分析描述逻辑中的时序推理'}
{'arxiv_id': 'arXiv:2508.00574', 'title': 'SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought', 'authors': 'Jianwei Wang, Ziming Wu, Fuming Lai, Shaobing Lian, Ziqian Zeng', 'link': 'https://arxiv.org/abs/2508.00574', 'abstract': 'While Chain-of-Thought (CoT) reasoning improves model performance, it incurs significant time costs due to the generation of discrete CoT tokens (DCoT). Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT methods are hampered by indirect fine-tuning, limited alignment, or inconsistent targets. To overcome these limitations, we propose \\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically, \\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and effective alignment target for LLMs. This synthetic CCoT explicitly guides the LLM to learn CCoT and derive accurate answers directly. Furthermore, relying solely on CCoT is insufficient for solving hard questions. To address this, \\textit{SynAdapt} integrates a difficulty classifier that leverages both question context and CCoT to identify hard questions. CCoT can effectively help identify hard questions after some brief reasoning. We then adaptively prompt the LLM to re-think these hard questions for improved performance. Extensive experimental results across various benchmarks from different difficulty levels strongly demonstrate the effectiveness of our method, achieving the best accuracy-efficiency trade-off.', 'abstract_zh': 'SynAdapt：一种有效的合成连续推理适应框架', 'title_zh': 'SynAdapt: 在大规模语言模型中通过合成连续推理学习自适应推理'}
{'arxiv_id': 'arXiv:2508.00555', 'title': 'Activation-Guided Local Editing for Jailbreaking Attacks', 'authors': 'Jiecong Wang, Haoran Li, Hao Peng, Ziqian Zeng, Zihao Wang, Haohua Du, Zhengtao Yu', 'link': 'https://arxiv.org/abs/2508.00555', 'abstract': "Jailbreaking is an essential adversarial technique for red-teaming these models to uncover and patch security flaws. However, existing jailbreak methods face significant drawbacks. Token-level jailbreak attacks often produce incoherent or unreadable inputs and exhibit poor transferability, while prompt-level attacks lack scalability and rely heavily on manual effort and human ingenuity. We propose a concise and effective two-stage framework that combines the advantages of these approaches. The first stage performs a scenario-based generation of context and rephrases the original malicious query to obscure its harmful intent. The second stage then utilizes information from the model's hidden states to guide fine-grained edits, effectively steering the model's internal representation of the input from a malicious toward a benign one. Extensive experiments demonstrate that this method achieves state-of-the-art Attack Success Rate, with gains of up to 37.74% over the strongest baseline, and exhibits excellent transferability to black-box models. Our analysis further demonstrates that AGILE maintains substantial effectiveness against prominent defense mechanisms, highlighting the limitations of current safeguards and providing valuable insights for future defense development. Our code is available at this https URL.", 'abstract_zh': '破解是-red团队利用这些模型以发现和修复安全漏洞的重要对抗技术。然而，现有的破解方法存在显著的局限性。字元级破解攻击通常会产生不连贯或不可读的输入，并表现出较差的迁移性，而提示级攻击缺乏可扩展性且高度依赖人工努力和创意。我们提出了一种简洁而有效的两阶段框架，结合了这两种方法的优势。第一阶段基于场景的生成上下文并重新表述原始恶意查询以掩盖其有害意图。第二阶段利用模型隐藏状态的信息来指导精细编辑，有效引导模型对输入的内部表示从恶意向良性转变。广泛的实验表明，该方法在攻击成功率上达到了最先进的水平，相对于最强的基线方法获得了高达37.74%的提升，并且在黑盒模型中的迁移性表现出色。进一步的分析表明，AGILE在对抗主流防御机制时保持了显著的效果，突显了当前防护措施的局限性，并为未来的防御开发提供了宝贵的见解。该代码可在以下网址获取：this https URL。', 'title_zh': '激活引导局部编辑用于破解攻击'}
{'arxiv_id': 'arXiv:2508.00546', 'title': 'SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval', 'authors': 'Wenchao Gu, Zongyi Lyu, Yanlin Wang, Hongyu Zhang, Cuiyun Gao, Michael R. Lyu', 'link': 'https://arxiv.org/abs/2508.00546', 'abstract': "Code retrieval aims to provide users with desired code snippets based on users' natural language queries. With the development of deep learning technologies, adopting pre-trained models for this task has become mainstream. Considering the retrieval efficiency, most of the previous approaches adopt a dual-encoder for this task, which encodes the description and code snippet into representation vectors, respectively. However, the model structure of the dual-encoder tends to limit the model's performance, since it lacks the interaction between the code snippet and description at the bottom layer of the model during training. To improve the model's effectiveness while preserving its efficiency, we propose a framework, which adopts Self-AdaPtive Model Distillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts the dual-encoder to narrow the search space and then adopts the cross-encoder to improve accuracy. To improve the efficiency of SPENCER, we propose a novel model distillation technique, which can greatly reduce the inference time of the dual-encoder while maintaining the overall performance. We also propose a teaching assistant selection strategy for our model distillation, which can adaptively select the suitable teaching assistant models for different pre-trained models during the model distillation to ensure the model performance. Extensive experiments demonstrate that the combination of dual-encoder and cross-encoder improves overall performance compared to solely dual-encoder-based models for code retrieval. Besides, our model distillation technique retains over 98% of the overall performance while reducing the inference time of the dual-encoder by 70%.", 'abstract_zh': 'Self-AdaPtive Model Distillation for Efficient CodE Retrieval: SPENCER', 'title_zh': 'SPENCER: 自适应模型蒸馏以实现高效的代码检索'}
{'arxiv_id': 'arXiv:2508.00545', 'title': 'Foundations of Interpretable Models', 'authors': 'Pietro Barbiero, Mateo Espinosa Zarlenga, Alberto Termine, Mateja Jamnik, Giuseppe Marra', 'link': 'https://arxiv.org/abs/2508.00545', 'abstract': 'We argue that existing definitions of interpretability are not actionable in that they fail to inform users about general, sound, and robust interpretable model design. This makes current interpretability research fundamentally ill-posed. To address this issue, we propose a definition of interpretability that is general, simple, and subsumes existing informal notions within the interpretable AI community. We show that our definition is actionable, as it directly reveals the foundational properties, underlying assumptions, principles, data structures, and architectural features necessary for designing interpretable models. Building on this, we propose a general blueprint for designing interpretable models and introduce the first open-sourced library with native support for interpretable data structures and processes.', 'abstract_zh': '我们argue现有的可解释性定义不具备可操作性，因为它们未能向用户传达一般、可靠且稳健的可解释模型设计信息。这使得当前的可解释性研究本质上是不明确的。为了解决这一问题，我们提出了一种通用、简单且包含可解释AI社区中现有非正式概念的可解释性定义。我们展示我们的定义是可操作的，因为它直接揭示了设计可解释模型所必需的基础属性、潜在假设、原则、数据结构和架构特征。在此基础上，我们提出了一种设计可解释模型的一般蓝图，并引入了首个具有内置支持的可解释数据结构和过程的开源库。', 'title_zh': '可解释模型的基础'}
{'arxiv_id': 'arXiv:2508.00525', 'title': 'Towards a Measure Theory of Semantic Information', 'authors': 'George M. Coghill', 'link': 'https://arxiv.org/abs/2508.00525', 'abstract': "A classic account of the quantification of semantic information is that of Bar-Hiller and Carnap. Their account proposes an inverse relation between the informativeness of a statement and its probability. However, their approach assigns the maximum informativeness to a contradiction: which Floridi refers to as the Bar-Hillel-Carnap paradox. He developed a novel theory founded on a distance metric and parabolic relation, designed to remove this paradox. Unfortunately is approach does not succeed in that aim.\nIn this paper I critique Floridi's theory of strongly semantic information on its own terms and show where it succeeds and fails. I then present a new approach based on the unit circle (a relation that has been the basis of theories from basic trigonometry to quantum theory). This is used, by analogy with von Neumann's quantum probability to construct a measure space for informativeness that meets all the requirements stipulated by Floridi and removes the paradox. In addition, while contradictions and tautologies have zero informativeness, it is found that messages which are contradictory to each other are equally informative. The utility of this is explained by means of an example.", 'abstract_zh': '一种语义信息量的经典论述是由Bar-Hiller和Carnap提出的。他们的论述提出，命题的信息量与其概率之间存在倒数关系。然而，他们的方法将最大信息量赋予一个矛盾：这被称为Bar-Hillel-Carnap悖论。Floridi发展了一种新的理论，该理论以距离度量和抛物线关系为基础，旨在消除这一悖论。不幸的是，这种方法未能达到这个目标。\n\n在本文中，我从Floridi自身的理论出发批判 his 强语义信息理论，并展示其成功和失败之处。然后，我提出了一种基于单位圆的新方法（这一关系是基本三角学和量子理论中理论的基础）。通过类比von Neumann的量子概率，我们构建了一个满足Floridi提出的所有要求的信息量度量空间，并消除了悖论。此外，虽然矛盾和重言式具有零信息量，但相互矛盾的信息具有相同的信息量。通过一个例子解释了这一方法的实用性。', 'title_zh': '面向语义信息的测度理论'}
{'arxiv_id': 'arXiv:2508.00496', 'title': 'LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI', 'authors': 'Mohammed Kamran, Maria Bernathova, Raoul Varga, Christian Singer, Zsuzsanna Bago-Horvath, Thomas Helbich, Georg Langs, Philipp Seeböck', 'link': 'https://arxiv.org/abs/2508.00496', 'abstract': 'Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk patients. While recent deep learning methods have advanced lesion segmentation, they primarily target large lesions and neglect valuable longitudinal and clinical information routinely used by radiologists. In real-world screening, detecting subtle or emerging lesions requires radiologists to compare across timepoints and consider previous radiology assessments, such as the BI-RADS score. We propose LesiOnTime, a novel 3D segmentation approach that mimics clinical diagnostic workflows by jointly leveraging longitudinal imaging and BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA) block that dynamically integrates information from previous and current scans; and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent space alignment for scans with similar radiological assessments, thus embedding domain knowledge into the training process. Evaluated on a curated in-house longitudinal dataset of high-risk patients with DCE-MRI, our approach outperforms state-of-the-art single-timepoint and longitudinal baselines by 5% in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute complementary performance gains. These results highlight the importance of incorporating temporal and clinical context for reliable early lesion segmentation in real-world breast cancer screening. Our code is publicly available at this https URL', 'abstract_zh': '准确分割乳腺动态对比增强MRI中的小型病灶对于早期癌症检测至关重要，尤其是在高风险患者中。尽管近期的深度学习方法提高了病灶分割的精度，但它们主要针对大型病灶，并且忽视了放射科医生常规使用的纵向和临床信息。在实际筛查中，检测细微或新出现的病灶需要放射科医生通过比较时间点并考虑之前的影像评估结果，如BI-RADS评分。我们提出了一种名为LesiOnTime的新颖3D分割方法，通过联合利用纵向影像和BI-RADS评分来模仿临床诊断流程。其关键组件包括：(1) 时间先验注意力(TPA)模块，可动态整合前后影像的信息；(2) BI-RADS一致性正则化(BCR)损失，用于对相似放射学评估的影像扫描施加潜在空间对齐，从而将领域知识嵌入训练过程。在高风险患者自建纵向数据集的评估中，我们的方法在Dice系数上比最先进的单时点和纵向基线高出5%。消融研究证明，TPA和BCR均能带来互补的性能提升。这些结果强调了在实际乳腺癌筛查中结合时间与临床上下文以实现可靠早期病灶分割的重要性。我们的代码可在以下链接公开获取。', 'title_zh': 'LesiOnTime -- 联合时序和临床建模在纵向DCE-MRI中小乳腺病灶分割中的应用'}
{'arxiv_id': 'arXiv:2508.00491', 'title': 'HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning', 'authors': 'Carlo Alessi, Federico Vasile, Federico Ceola, Giulia Pasquale, Nicolò Boccardo, Lorenzo Natale', 'link': 'https://arxiv.org/abs/2508.00491', 'abstract': 'Recent advancements in control of prosthetic hands have focused on increasing autonomy through the use of cameras and other sensory inputs. These systems aim to reduce the cognitive load on the user by automatically controlling certain degrees of freedom. In robotics, imitation learning has emerged as a promising approach for learning grasping and complex manipulation tasks while simplifying data collection. Its application to the control of prosthetic hands remains, however, largely unexplored. Bridging this gap could enhance dexterity restoration and enable prosthetic devices to operate in more unconstrained scenarios, where tasks are learned from demonstrations rather than relying on manually annotated sequences. To this end, we present HannesImitationPolicy, an imitation learning-based method to control the Hannes prosthetic hand, enabling object grasping in unstructured environments. Moreover, we introduce the HannesImitationDataset comprising grasping demonstrations in table, shelf, and human-to-prosthesis handover scenarios. We leverage such data to train a single diffusion policy and deploy it on the prosthetic hand to predict the wrist orientation and hand closure for grasping. Experimental evaluation demonstrates successful grasps across diverse objects and conditions. Finally, we show that the policy outperforms a segmentation-based visual servo controller in unstructured scenarios. Additional material is provided on our project page: this https URL', 'abstract_zh': 'Recent advancements in假肢手的控制领域侧重于通过使用摄像头和其他感官输入来增加自主性。这些系统旨在通过自动控制某些自由度来减少用户的认知负担。在机器人学中， imitation learning（模仿学习）已 emerged as a promising approach for learning抓取和复杂操作任务，同时简化数据采集。然而，将其应用于假肢手的控制仍主要未被探索。弥合这一差距可以增强灵巧性的恢复，并使假肢设备能够在更多不受约束的场景下运行，其中任务是通过演示学习而非依赖手动标注的序列来完成的。为此，我们提出了基于模仿学习的HannesImitationPolicy方法，以控制Hannes假肢手，在非结构化环境中实现物体抓取。此外，我们引入了HannesImitationDataset，其中包括在桌子、架子和人对假肢的手递任务中的抓取演示。我们利用此类数据训练单一的扩散策略，并将其部署在假肢手上以预测腕部姿态和手部闭合以实现抓取。实验评估表明，该策略能够成功地在各种物体和条件下进行抓取。最后，我们展示了在非结构化场景中，该策略在性能上优于基于分割的视觉伺服控制器。更多材料可在我们的项目页面获取：this https URL', 'title_zh': 'HannesImitation: 通过模仿学习使用Hannes假手抓取'}
{'arxiv_id': 'arXiv:2508.00478', 'title': 'CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization', 'authors': 'Yuning Jiang, Nay Oo, Qiaoran Meng, Lu Lin, Dusit Niyato, Zehui Xiong, Hoon Wei Lim, Biplab Sikdar', 'link': 'https://arxiv.org/abs/2508.00478', 'abstract': "Modern cyber attacks unfold through multiple stages, requiring defenders to dynamically prioritize mitigations under uncertainty. While game-theoretic models capture attacker-defender interactions, existing approaches often rely on static assumptions and lack integration with real-time threat intelligence, limiting their adaptability. This paper presents CyGATE, a game-theoretic framework modeling attacker-defender interactions, using large language models (LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection and patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber conflicts as a partially observable stochastic game (POSG) across Cyber Kill Chain stages. Both agents use belief states to navigate uncertainty, with the attacker adapting tactics and the defender re-prioritizing patches based on evolving risks and observed adversary behavior. The framework's flexible architecture enables extension to multi-agent scenarios involving coordinated attackers, collaborative defenders, or complex enterprise environments with multiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE effectively prioritizes high-risk vulnerabilities, enhancing adaptability through dynamic threat integration, strategic foresight by anticipating attacker moves under uncertainty, and efficiency by optimizing resource use.", 'abstract_zh': '基于大型语言模型的CyGATE博弈框架：动态威胁情报下的攻击者-防御者交互 Modeling CyGATE with Large Language Models: Attacker-Defender Interactions under Dynamic Threat Intelligence', 'title_zh': '基于博弈论的CyGATE网络攻击防御引擎：补丁策略优化'}
{'arxiv_id': 'arXiv:2508.00452', 'title': 'M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation', 'authors': 'Chuan He, Yongchao Liu, Qiang Li, Wenliang Zhong, Chuntao Hong, Xinwei Yao', 'link': 'https://arxiv.org/abs/2508.00452', 'abstract': 'Cold-start item recommendation is a significant challenge in recommendation systems, particularly when new items are introduced without any historical interaction data. While existing methods leverage multi-modal content to alleviate the cold-start issue, they often neglect the inherent multi-view structure of modalities, the distinction between shared and modality-specific features. In this paper, we propose Multi-Modal Multi-View Variational AutoEncoder (M^2VAE), a generative model that addresses the challenges of modeling common and unique views in attribute and multi-modal features, as well as user preferences over single-typed item features. Specifically, we generate type-specific latent variables for item IDs, categorical attributes, and image features, and use Product-of-Experts (PoE) to derive a common representation. A disentangled contrastive loss decouples the common view from unique views while preserving feature informativeness. To model user inclinations, we employ a preference-guided Mixture-of-Experts (MoE) to adaptively fuse representations. We further incorporate co-occurrence signals via contrastive learning, eliminating the need for pretraining. Extensive experiments on real-world datasets validate the effectiveness of our approach.', 'abstract_zh': '冷启动项推荐是推荐系统中的一个显著挑战，尤其是在没有历史交互数据情况下引入新项时。现有方法通过利用多模态内容来缓解冷启动问题，但往往忽视了模态的固有多视角结构以及共性特征与模态特定特征之间的区别。本文提出多模态多视角变分自编码器（M^2VAE），这是一种生成模型，旨在解决属性和多模态特征中共同视角和独特视角建模的挑战，以及用户对单一类型项特征的偏好。具体而言，我们为项ID、类别属性和图像特征生成类型特定的潜在变量，并利用专家乘积（Product-of-Experts, PoE）获得共同表示。解耦的对比损失将共同视角与独特视角解耦，同时保持特征的信息性。为了建模用户倾向，我们采用偏好引导的混合专家（Mixture-of-Experts, MoE）以自适应方式融合表示。此外，通过对比学习引入共现信号，从而省去了预训练的需要。在实际数据集上的广泛实验验证了我们方法的有效性。', 'title_zh': 'M^2VAE：多模态多视角变分自动编码器在冷启动项推荐中的应用'}
{'arxiv_id': 'arXiv:2508.00450', 'title': 'When Relevance Meets Novelty: Dual-Stable Periodic Optimization for Exploratory Recommendation', 'authors': 'Hongxiang Lin, Hao Guo, Zeshun Li, Erpeng Xue, Yongqian He, Xiangyu Hou, Zhaoyu Hu, Lei Wang, Sheng Chen', 'link': 'https://arxiv.org/abs/2508.00450', 'abstract': 'Traditional recommendation systems tend to trap users in strong feedback loops by excessively pushing content aligned with their historical preferences, thereby limiting exploration opportunities and causing content fatigue. Although large language models (LLMs) demonstrate potential with their diverse content generation capabilities, existing LLM-enhanced dual-model frameworks face two major limitations: first, they overlook long-term preferences driven by group identity, leading to biased interest modeling; second, they suffer from static optimization flaws, as a one-time alignment process fails to leverage incremental user data for closed-loop optimization. To address these challenges, we propose the Co-Evolutionary Alignment (CoEA) method. For interest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE) module, jointly modeling long-term group identity and short-term individual interests through parallel processing of behavioral sequences. For static optimization limitations, we design a Periodic Collaborative Optimization (PCO) mechanism. This mechanism regularly conducts preference verification on incremental data using the Relevance LLM, then guides the Novelty LLM to perform fine-tuning based on the verification results, and subsequently feeds back the output of the incrementally fine-tuned Novelty LLM to the Relevance LLM for re-evaluation, thereby achieving a dynamic closed-loop optimization. Extensive online and offline experiments verify the effectiveness of the CoEA model in exploratory recommendation.', 'abstract_zh': '传统推荐系统倾向于通过过度推送符合用户历史偏好的内容，将用户困在强烈的反馈循环中，从而限制探索机会并导致内容疲劳。尽管大型语言模型（LLMs）凭借其多元的内容生成能力展现了潜力，但现有的LLM增强双模型框架面临两大主要局限性：首先，它们忽略了由群体身份驱动的长期偏好，导致兴趣建模出现偏差；其次，它们受到静态优化缺陷的困扰，一次性对齐过程无法利用增量用户数据进行闭环优化。为应对这些挑战，我们提出了共生对齐（CoEA）方法。为解决兴趣建模偏差问题，我们引入了双重稳定兴趣探索（DSIE）模块，通过并行处理行为序列来共同建模长期群体身份和短期个体兴趣。为解决静态优化局限性，我们设计了周期性协作优化（PCO）机制。该机制使用相关性LLM定期对增量数据进行偏好验证，然后指导新颖性LLM根据验证结果进行微调，并随后将增量微调后的新颖性LLM的输出反馈给相关性LLM重新评估，从而实现动态闭环优化。广泛开展的在线和离线实验验证了CoEA模型在探索性推荐中的有效性。', 'title_zh': '当相关性遇上新颖性：探索性推荐的双稳周期优化'}
{'arxiv_id': 'arXiv:2508.00442', 'title': 'TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation', 'authors': 'Jiale Zhou, Wenhan Wang, Shikun Li, Xiaolei Qu, Xin Guo, Yizhong Liu, Wenzhong Tang, Xun Lin, Yefeng Zheng', 'link': 'https://arxiv.org/abs/2508.00442', 'abstract': "Tubular structure segmentation (TSS) is important for various applications, such as hemodynamic analysis and route navigation. Despite significant progress in TSS, domain shifts remain a major challenge, leading to performance degradation in unseen target domains. Unlike other segmentation tasks, TSS is more sensitive to domain shifts, as changes in topological structures can compromise segmentation integrity, and variations in local features distinguishing foreground from background (e.g., texture and contrast) may further disrupt topological continuity. To address these challenges, we propose Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time adaptation framework designed specifically for TSS. TopoTTA consists of two stages: Stage 1 adapts models to cross-domain topological discrepancies using the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance topological representation without altering pre-trained parameters; Stage 2 improves topological continuity by a novel Topology Hard sample Generation (TopoHG) strategy and prediction alignment on hard samples with pseudo-labels in the generated pseudo-break regions. Extensive experiments across four scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling topological distribution shifts, achieving an average improvement of 31.81% in clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS models.", 'abstract_zh': 'Tubular Structure Segmentation (TSS) 的拓扑增强测试时自适应（TopoTTA）', 'title_zh': 'TopoTTA：拓扑增强测试时自适应技术用于管状结构分割'}
{'arxiv_id': 'arXiv:2508.00440', 'title': 'Reducing the gap between general purpose data and aerial images in concentrated solar power plants', 'authors': 'M.A. Pérez-Cutiño, J. Valverde, J. Capitán, J.M. Díaz-Báñez', 'link': 'https://arxiv.org/abs/2508.00440', 'abstract': 'In the context of Concentrated Solar Power (CSP) plants, aerial images captured by drones present a unique set of challenges. Unlike urban or natural landscapes commonly found in existing datasets, solar fields contain highly reflective surfaces, and domain-specific elements that are uncommon in traditional computer vision benchmarks. As a result, machine learning models trained on generic datasets struggle to generalize to this setting without extensive retraining and large volumes of annotated data. However, collecting and labeling such data is costly and time-consuming, making it impractical for rapid deployment in industrial applications.\nTo address this issue, we propose a novel approach: the creation of AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By generating synthetic data that closely mimic real-world conditions, our objective is to facilitate pretraining of models before deployment, significantly reducing the need for extensive manual labeling. Our main contributions are threefold: (1) we introduce AerialCSP, a high-quality synthetic dataset for aerial inspection of CSP plants, providing annotated data for object detection and image segmentation; (2) we benchmark multiple models on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we demonstrate that pretraining on AerialCSP significantly improves real-world fault detection, particularly for rare and small defects, reducing the need for extensive manual labeling. AerialCSP is made publicly available at this https URL.', 'abstract_zh': '基于 concentrated solar power (CSP) 系统的无人机航拍图像面临的挑战及解决方案：AerialCSP 数据集的创建与应用', 'title_zh': '减少集中式太阳能电站通用数据与航空图像之间的差距'}
{'arxiv_id': 'arXiv:2508.00427', 'title': 'Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting', 'authors': 'Seunggeun Chi, Enna Sachdeva, Pin-Hao Huang, Kwonjoon Lee', 'link': 'https://arxiv.org/abs/2508.00427', 'abstract': "Amodal completion, which is the process of inferring the full appearance of objects despite partial occlusions, is crucial for understanding complex human-object interactions (HOI) in computer vision and robotics. Existing methods, such as those that use pre-trained diffusion models, often struggle to generate plausible completions in dynamic scenarios because they have a limited understanding of HOI. To solve this problem, we've developed a new approach that uses physical prior knowledge along with a specialized multi-regional inpainting technique designed for HOI. By incorporating physical constraints from human topology and contact information, we define two distinct regions: the primary region, where occluded object parts are most likely to be, and the secondary region, where occlusions are less probable. Our multi-regional inpainting method uses customized denoising strategies across these regions within a diffusion model. This improves the accuracy and realism of the generated completions in both their shape and visual detail. Our experimental results show that our approach significantly outperforms existing methods in HOI scenarios, moving machine perception closer to a more human-like understanding of dynamic environments. We also show that our pipeline is robust even without ground-truth contact annotations, which broadens its applicability to tasks like 3D reconstruction and novel view/pose synthesis.", 'abstract_zh': '无感知完成：基于物理先验的动态场景中复杂人-物交互理解', 'title_zh': '接触意识的非可视区域完成：基于多区域修复的人与物体交互'}
{'arxiv_id': 'arXiv:2508.00413', 'title': 'DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space', 'authors': 'Junyu Chen, Dongyun Zou, Wenkun He, Junsong Chen, Enze Xie, Song Han, Han Cai', 'link': 'https://arxiv.org/abs/2508.00413', 'abstract': "We present DC-AE 1.5, a new family of deep compression autoencoders for high-resolution diffusion models. Increasing the autoencoder's latent channel number is a highly effective approach for improving its reconstruction quality. However, it results in slow convergence for diffusion models, leading to poorer generation quality despite better reconstruction quality. This issue limits the quality upper bound of latent diffusion models and hinders the employment of autoencoders with higher spatial compression ratios. We introduce two key innovations to address this challenge: i) Structured Latent Space, a training-based approach to impose a desired channel-wise structure on the latent space with front latent channels capturing object structures and latter latent channels capturing image details; ii) Augmented Diffusion Training, an augmented diffusion training strategy with additional diffusion training objectives on object latent channels to accelerate convergence. With these techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better image generation quality than DC-AE-f32c32 while being 4x faster. Code: this https URL.", 'abstract_zh': 'DC-AE 1.5：一种用于高分辨率扩散模型的新型深度压缩自编码器', 'title_zh': 'DC-AE 1.5: 通过结构化潜在空间加速扩散模型收敛'}
{'arxiv_id': 'arXiv:2508.00395', 'title': 'Decouple before Align: Visual Disentanglement Enhances Prompt Tuning', 'authors': 'Fei Zhang, Tianfei Zhou, Jiangchao Yao, Ya Zhang, Ivor W. Tsang, Yanfeng Wang', 'link': 'https://arxiv.org/abs/2508.00395', 'abstract': 'Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm, has showcased remarkable effectiveness in improving the task-specific transferability of vision-language models. This paper delves into a previously overlooked information asymmetry issue in PT, where the visual modality mostly conveys more context than the object-oriented textual modality. Correspondingly, coarsely aligning these two modalities could result in the biased attention, driving the model to merely focus on the context area. To address this, we propose DAPT, an effective PT framework based on an intuitive decouple-before-align concept. First, we propose to explicitly decouple the visual modality into the foreground and background representation via exploiting coarse-and-fine visual segmenting cues, and then both of these decoupled patterns are aligned with the original foreground texts and the hand-crafted background classes, thereby symmetrically strengthening the modal alignment. To further enhance the visual concentration, we propose a visual pull-push regularization tailored for the foreground-background patterns, directing the original visual representation towards unbiased attention on the region-of-interest object. We demonstrate the power of architecture-free DAPT through few-shot learning, base-to-novel generalization, and data-efficient learning, all of which yield superior performance across prevailing benchmarks. Our code will be released at this https URL.', 'abstract_zh': '基于解耦与对齐概念的DAPT：一种有效提示调优框架', 'title_zh': '解耦再对齐：视觉去纠缠增强提示调优'}
{'arxiv_id': 'arXiv:2508.00394', 'title': 'ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs', 'authors': 'Antonis Klironomos, Baifan Zhou, Zhipeng Tan, Zhuoxun Zheng, Mohamed H. Gad-Elrab, Heiko Paulheim, Evgeny Kharlamov', 'link': 'https://arxiv.org/abs/2508.00394', 'abstract': 'Nowadays machine learning (ML) practitioners have access to numerous ML libraries available online. Such libraries can be used to create ML pipelines that consist of a series of steps where each step may invoke up to several ML libraries that are used for various data-driven analytical tasks. Development of high-quality ML pipelines is non-trivial; it requires training, ML expertise, and careful development of each step. At the same time, domain experts in science and engineering may not possess such ML expertise and training while they are in pressing need of ML-based analytics. In this paper, we present our ExeKGLib, a Python library enhanced with a graphical interface layer that allows users with minimal ML knowledge to build ML pipelines. This is achieved by relying on knowledge graphs that encode ML knowledge in simple terms accessible to non-ML experts. ExeKGLib also allows improving the transparency and reusability of the built ML workflows and ensures that they are executable. We show the usability and usefulness of ExeKGLib by presenting real use cases.', 'abstract_zh': 'ExeKGLib：一种增强图形界面的Python库，用于非ML专家构建可执行的高质量ML管道', 'title_zh': 'ExeKGLib：基于知识图谱的机器学习分析平台'}
{'arxiv_id': 'arXiv:2508.00383', 'title': '$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models', 'authors': 'Won June Cho, Hongjun Yoon, Daeky Jeong, Hyeongyeol Lim, Yosep Chong', 'link': 'https://arxiv.org/abs/2508.00383', 'abstract': 'Spatial transcriptomics reveals gene expression patterns within tissue context, enabling precision oncology applications such as treatment response prediction, but its high cost and technical complexity limit clinical adoption. Predicting spatial gene expression (biomarkers) from routine histopathology images offers a practical alternative, yet current vision foundation models (VFMs) in pathology based on Vision Transformer (ViT) backbones perform below clinical standards. Given that VFMs are already trained on millions of diverse whole slide images, we hypothesize that architectural innovations beyond ViTs may better capture the low-frequency, subtle morphological patterns correlating with molecular phenotypes. By demonstrating that state space models initialized with negative real eigenvalues exhibit strong low-frequency bias, we introduce $MV_{Hybrid}$, a hybrid backbone architecture combining state space models (SSMs) with ViT. We compare five other different backbone architectures for pathology VFMs, all pretrained on identical colorectal cancer datasets using the DINOv2 self-supervised learning method. We evaluate all pretrained models using both random split and leave-one-study-out (LOSO) settings of the same biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher correlation than the best-performing ViT and shows 43% smaller performance degradation compared to random split in gene expression prediction, demonstrating superior performance and robustness, respectively. Furthermore, $MV_{Hybrid}$ shows equal or better downstream performance in classification, patch retrieval, and survival prediction tasks compared to that of ViT, showing its promise as a next-generation pathology VFM backbone. Our code is publicly available at: this https URL.', 'abstract_zh': '基于状态空间模型的空间转录组学在病理视觉基础模型中的应用：混合架构MV_{Hybrid}的性能评估', 'title_zh': '$MV_{Hybrid}$: 使用混合状态空间-视觉变换器骨干改进病理科视觉基础模型中的空间转录组学预测'}
{'arxiv_id': 'arXiv:2508.00381', 'title': 'Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis', 'authors': 'Kamal Basha S, Athira Nambiar', 'link': 'https://arxiv.org/abs/2508.00381', 'abstract': 'Weld defect detection is crucial for ensuring the safety and reliability of piping systems in the oil and gas industry, especially in challenging marine and offshore environments. Traditional non-destructive testing (NDT) methods often fail to detect subtle or internal defects, leading to potential failures and costly downtime. Furthermore, existing neural network-based approaches for defect classification frequently rely on arbitrarily selected pretrained architectures and lack interpretability, raising safety concerns for deployment. To address these challenges, this paper introduces ``Adapt-WeldNet", an adaptive framework for welding defect detection that systematically evaluates various pre-trained architectures, transfer learning strategies, and adaptive optimizers to identify the best-performing model and hyperparameters, optimizing defect detection and providing actionable insights. Additionally, a novel Defect Detection Interpretability Analysis (DDIA) framework is proposed to enhance system transparency. DDIA employs Explainable AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific evaluations validated by certified ASNT NDE Level II professionals. Incorporating a Human-in-the-Loop (HITL) approach and aligning with the principles of Trustworthy AI, DDIA ensures the reliability, fairness, and accountability of the defect detection system, fostering confidence in automated decisions through expert validation. By improving both performance and interpretability, this work enhances trust, safety, and reliability in welding defect detection systems, supporting critical operations in offshore and marine environments.', 'abstract_zh': '焊缺陷检测对于确保石油和天然气行业中管道系统的安全性和可靠性至关重要，尤其是在复杂的海洋和海上环境中。传统的无损检测（NDT）方法往往无法检测到细微的或内部的缺陷，从而可能导致潜在的故障和昂贵的停机时间。此外，现有的基于神经网络的缺陷分类方法通常依赖于任意选择的预训练架构，并缺乏可解释性，增加了部署的安全隐患。为了解决这些挑战，本文介绍了一种自适应框架“Adapt-WeldNet”，该框架系统地评估了各种预训练架构、迁移学习策略和自适应优化器，以确定性能最佳的模型和超参数，优化缺陷检测并提供可操作的洞察。此外，提出了一种新型的缺陷检测可解释性分析（DDIA）框架以增强系统透明度。DDIA 使用可解释人工智能（XAI）技术，如 Grad-CAM 和 LIME，并结合由认证的 ASNT NDE Level II 专业人士进行的领域特定评估。通过引入人机环（HITL）方法并与可信赖人工智能的原则保持一致，DDIA 确保了缺陷检测系统的可靠性和公平性，并通过专家验证增强自动化决策的信心，从而提高焊缺陷检测系统的信任度、安全性和可靠性，支持海洋和海上环境中的关键操作。', 'title_zh': '通过Adapt-WeldNet和缺陷检测可解释性分析推动 maritime 运营中的焊接缺陷检测进展'}
{'arxiv_id': 'arXiv:2508.00312', 'title': 'GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection', 'authors': 'Suhang Cai, Xiaohao Peng, Chong Wang, Xiaojie Cai, Jiangbo Qian', 'link': 'https://arxiv.org/abs/2508.00312', 'abstract': 'Video anomaly detection (VAD) plays a critical role in public safety applications such as intelligent surveillance. However, the rarity, unpredictability, and high annotation cost of real-world anomalies make it difficult to scale VAD datasets, which limits the performance and generalization ability of existing models. To address this challenge, we propose a generative video-enhanced weakly-supervised video anomaly detection (GV-VAD) framework that leverages text-conditioned video generation models to produce semantically controllable and physically plausible synthetic videos. These virtual videos are used to augment training data at low cost. In addition, a synthetic sample loss scaling strategy is utilized to control the influence of generated synthetic samples for efficient training. The experiments show that the proposed framework outperforms state-of-the-art methods on UCF-Crime datasets. The code is available at this https URL.', 'abstract_zh': '基于生成视频增强弱监督视频异常检测的框架（GV-VAD）', 'title_zh': 'GV-VAD : 探索视频生成在弱监督视频异常检测中的应用'}
{'arxiv_id': 'arXiv:2508.00307', 'title': 'Beamformed 360° Sound Maps: U-Net-Driven Acoustic Source Segmentation and Localization', 'authors': 'Belman Jahir Rodriguez, Sergio F. Chevtchenko, Marcelo Herrera Martinez, Yeshwant Bethy, Saeed Afshar', 'link': 'https://arxiv.org/abs/2508.00307', 'abstract': 'We introduce a U-net model for 360° acoustic source localization formulated as a spherical semantic segmentation task. Rather than regressing discrete direction-of-arrival (DoA) angles, our model segments beamformed audio maps (azimuth and elevation) into regions of active sound presence. Using delay-and-sum (DAS) beamforming on a custom 24-microphone array, we generate signals aligned with drone GPS telemetry to create binary supervision masks. A modified U-Net, trained on frequency-domain representations of these maps, learns to identify spatially distributed source regions while addressing class imbalance via the Tversky loss. Because the network operates on beamformed energy maps, the approach is inherently array-independent and can adapt to different microphone configurations without retraining from scratch. The segmentation outputs are post-processed by computing centroids over activated regions, enabling robust DoA estimates. Our dataset includes real-world open-field recordings of a DJI Air 3 drone, synchronized with 360° video and flight logs across multiple dates and locations. Experimental results show that U-net generalizes across environments, providing improved angular precision, offering a new paradigm for dense spatial audio understanding beyond traditional Sound Source Localization (SSL).', 'abstract_zh': '一种用于360°声源定位的U-net模型：作为一种球面语义分割任务的 formulations', 'title_zh': '360°声图谱的波束形成：基于U-Net的声音源分割与定位'}
{'arxiv_id': 'arXiv:2508.00300', 'title': 'MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems', 'authors': 'Shruthi Chari, Oshani Seneviratne, Prithwish Chakraborty, Pablo Meyer, Deborah L. McGuinness', 'link': 'https://arxiv.org/abs/2508.00300', 'abstract': "Explanations are crucial for building trustworthy AI systems, but a gap often exists between the explanations provided by models and those needed by users. To address this gap, we introduce MetaExplainer, a neuro-symbolic framework designed to generate user-centered explanations. Our approach employs a three-stage process: first, we decompose user questions into machine-readable formats using state-of-the-art large language models (LLM); second, we delegate the task of generating system recommendations to model explainer methods; and finally, we synthesize natural language explanations that summarize the explainer outputs. Throughout this process, we utilize an Explanation Ontology to guide the language models and explainer methods. By leveraging LLMs and a structured approach to explanation generation, MetaExplainer aims to enhance the interpretability and trustworthiness of AI systems across various applications, providing users with tailored, question-driven explanations that better meet their needs. Comprehensive evaluations of MetaExplainer demonstrate a step towards evaluating and utilizing current state-of-the-art explanation frameworks. Our results show high performance across all stages, with a 59.06% F1-score in question reframing, 70% faithfulness in model explanations, and 67% context-utilization in natural language synthesis. User studies corroborate these findings, highlighting the creativity and comprehensiveness of generated explanations. Tested on the Diabetes (PIMA Indian) tabular dataset, MetaExplainer supports diverse explanation types, including Contrastive, Counterfactual, Rationale, Case-Based, and Data explanations. The framework's versatility and traceability from using ontology to guide LLMs suggest broad applicability beyond the tested scenarios, positioning MetaExplainer as a promising tool for enhancing AI explainability across various domains.", 'abstract_zh': '元解释器：一种用户中心的神经符号解释框架', 'title_zh': 'MetaExplainer：生成面向用户的多类型解释的AI系统框架'}
{'arxiv_id': 'arXiv:2508.00299', 'title': 'Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence', 'authors': 'Danzhen Fu, Jiagao Hu, Daiguo Zhou, Fei Wang, Zepeng Wang, Wenhua Liao', 'link': 'https://arxiv.org/abs/2508.00299', 'abstract': 'Pedestrian detection models in autonomous driving systems often lack robustness due to insufficient representation of dangerous pedestrian scenarios in training datasets. To address this limitation, we present a novel framework for controllable pedestrian video editing in multi-view driving scenarios by integrating video inpainting and human motion control techniques. Our approach begins by identifying pedestrian regions of interest across multiple camera views, expanding detection bounding boxes with a fixed ratio, and resizing and stitching these regions into a unified canvas while preserving cross-view spatial relationships. A binary mask is then applied to designate the editable area, within which pedestrian editing is guided by pose sequence control conditions. This enables flexible editing functionalities, including pedestrian insertion, replacement, and removal. Extensive experiments demonstrate that our framework achieves high-quality pedestrian editing with strong visual realism, spatiotemporal coherence, and cross-view consistency. These results establish the proposed method as a robust and versatile solution for multi-view pedestrian video generation, with broad potential for applications in data augmentation and scenario simulation in autonomous driving.', 'abstract_zh': '自动驾驶系统中的行人检测模型往往由于训练数据集中危险行人场景表示不足而缺乏鲁棒性。为解决这一限制，我们提出了一种集成视频插补和人类运动控制技术的多视角驱动场景可控行人视频编辑框架。该方法首先在多个摄像头视图中标识行人区域，按固定比例扩展检测边界框，将这些区域调整尺寸和拼接成统一画布，同时保持多视角空间关系。随后应用二进制掩码指定可编辑区域，在该区域内通过姿态序列控制条件引导行人编辑，以实现灵活的编辑功能，包括行人插入、替换和移除。实验结果表明，该框架能够实现高质量、视觉真实感强、时空一致性好以及多视角一致性的行人编辑。这些结果确立了该方法作为多视角行人视频生成的 robust 和 versatile 解决方案的地位，并在自主驾驶中的数据增强和场景模拟方面具有广泛的应用潜力。', 'title_zh': '基于运动序列的多视角驾驶场景可控行人视频编辑'}
{'arxiv_id': 'arXiv:2508.00294', 'title': 'Formal Power Series Representations in Probability and Expected Utility Theory', 'authors': 'Arthur Paul Pedersen, Samuel Allen Alexander', 'link': 'https://arxiv.org/abs/2508.00294', 'abstract': "We advance a general theory of coherent preference that surrenders restrictions embodied in orthodox doctrine. This theory enjoys the property that any preference system admits extension to a complete system of preferences, provided it satisfies a certain coherence requirement analogous to the one de Finetti advanced for his foundations of probability. Unlike de Finetti's theory, the one we set forth requires neither transitivity nor Archimedeanness nor boundedness nor continuity of preference. This theory also enjoys the property that any complete preference system meeting the standard of coherence can be represented by utility in an ordered field extension of the reals. Representability by utility is a corollary of this paper's central result, which at once extends Hölder's Theorem and strengthens Hahn's Embedding Theorem.", 'abstract_zh': '我们提出了一个关于一致偏好的一般理论，该理论舍弃了正统教义中所蕴含的限制条件。该理论具有一种性质：任何偏奋试系统都可以扩展为一个完整的偏奋试系统，前提是它满足一个与de Finetti为概率论基础提出的条件类似的特定一致性要求。与de Finetti的理论不同，我们提出的理论不需要偏好的传递性、阿基米德性、有界性和连续性。此外，该理论还具有一种性质：任何达到一致性标准的完整偏奋试系统都可以在实数的有序域扩张中通过效用表示。效用表示是本文中心结果的推论，这一结果不仅扩展了Hölder定理，还加强了Hahn嵌入定理。', 'title_zh': '形式幂级数表示在概率论与期望效用理论中的应用'}
{'arxiv_id': 'arXiv:2508.00264', 'title': 'Calibrated Language Models and How to Find Them with Label Smoothing', 'authors': 'Jerry Huang, Peng Lu, Qiuhao Zeng', 'link': 'https://arxiv.org/abs/2508.00264', 'abstract': 'Recent advances in natural language processing (NLP) have opened up greater opportunities to enable fine-tuned large language models (LLMs) to behave as more powerful interactive agents through improved instruction-following ability. However, understanding how this impacts confidence calibration for reliable model output has not been researched in full. In this work, we examine various open-sourced LLMs, identifying significant calibration degradation after instruction tuning in each. Seeking a practical solution, we look towards label smoothing, which has been shown as an effective method to regularize for overconfident predictions but has yet to be widely adopted in the supervised fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing is sufficient to maintain calibration throughout the SFT process. However, settings remain where the effectiveness of smoothing is severely diminished, in particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to stem from the ability to become over-confident, which has a direct relationship with the hidden size and vocabulary size, and justify this theoretically and experimentally. Finally, we address an outstanding issue regarding the memory footprint of the cross-entropy loss computation in the label smoothed loss setting, designing a customized kernel to dramatically reduce memory consumption without sacrificing speed or performance in comparison to existing solutions for non-smoothed losses.', 'abstract_zh': '近期自然语言处理(NLP)的进步为通过改进指令跟随能力使细调的大语言模型(LLMs)成为更强大的交互代理提供了更大的机会。然而，这一变化对可靠模型输出的信心校准影响尚无全面研究。在这项工作中，我们考察了多种开源LLMs，发现在每种模型中指令调校后都会出现显著的信心校准下降。为寻求实际解决方案，我们转向标签平滑，这已被证明是一种有效的过度自信预测正则化方法，但在大语言模型的监督微调(SFT)中尚未得到广泛应用。我们首先探讨标签平滑为何能在SFT过程中保持校准。然而，在某些情况下，平滑的效果大幅减弱，特别是在大词汇量语言模型中。我们认为原因在于过度自信的能力，与隐藏层尺寸和词汇量直接相关，并从理论上和实验上进行了验证。最后，我们解决了标签平滑损失计算中的内存占用问题，设计了一种定制内核，显著减少了内存消耗，同时在速度和性能上与现有非平滑损失解决方案持平。', 'title_zh': '校准语言模型及通过标签平滑寻找它们的方法'}
{'arxiv_id': 'arXiv:2508.00256', 'title': 'Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study', 'authors': 'Chuang Zhang, Geng Sun, Jiacheng Wang, Yijing Lin, Weijie Yuan, Sinem Coleri, Dusit Niyato, Tony Q. S. Quek', 'link': 'https://arxiv.org/abs/2508.00256', 'abstract': 'Low-altitude wireless networks (LAWNs) have the potential to revolutionize communications by supporting a range of applications, including urban parcel delivery, aerial inspections and air taxis. However, compared with traditional wireless networks, LAWNs face unique security challenges due to low-altitude operations, frequent mobility and reliance on unlicensed spectrum, making it more vulnerable to some malicious attacks. In this paper, we investigate some large artificial intelligence model (LAM)-enabled solutions for secure communications in LAWNs. Specifically, we first explore the amplified security risks and important limitations of traditional AI methods in LAWNs. Then, we introduce the basic concepts of LAMs and delve into the role of LAMs in addressing these challenges. To demonstrate the practical benefits of LAMs for secure communications in LAWNs, we propose a novel LAM-based optimization framework that leverages large language models (LLMs) to generate enhanced state features on top of handcrafted representations, and to design intrinsic rewards accordingly, thereby improving reinforcement learning performance for secure communication tasks. Through a typical case study, simulation results validate the effectiveness of the proposed framework. Finally, we outline future directions for integrating LAMs into secure LAWN applications.', 'abstract_zh': '低空无线网络中的大型人工智能模型助力安全通信', 'title_zh': '低高度无线网络中大型AI模型赋能的安全通信：概念、视角与案例研究'}
{'arxiv_id': 'arXiv:2508.00255', 'title': 'Accurate and Consistent Graph Model Generation from Text with Large Language Models', 'authors': 'Boqi Chen, Ou Wei, Bingzhou Zheng, Gunter Mussbacher', 'link': 'https://arxiv.org/abs/2508.00255', 'abstract': 'Graph model generation from natural language description is an important task with many applications in software engineering. With the rise of large language models (LLMs), there is a growing interest in using LLMs for graph model generation. Nevertheless, LLM-based graph model generation typically produces partially correct models that suffer from three main issues: (1) syntax violations: the generated model may not adhere to the syntax defined by its metamodel, (2) constraint inconsistencies: the structure of the model might not conform to some domain-specific constraints, and (3) inaccuracy: due to the inherent uncertainty in LLMs, the models can include inaccurate, hallucinated elements. While the first issue is often addressed through techniques such as constraint decoding or filtering, the latter two remain largely unaddressed. Motivated by recent self-consistency approaches in LLMs, we propose a novel abstraction-concretization framework that enhances the consistency and quality of generated graph models by considering multiple outputs from an LLM. Our approach first constructs a probabilistic partial model that aggregates all candidate outputs and then refines this partial model into the most appropriate concrete model that satisfies all constraints. We evaluate our framework on several popular open-source and closed-source LLMs using diverse datasets for model generation tasks. The results demonstrate that our approach significantly improves both the consistency and quality of the generated graph models.', 'abstract_zh': '基于自然语言描述的图模型生成是软件工程中一项重要的任务，随着大型语言模型（LLMs）的兴起，人们越来越关注使用LLMs进行图模型生成。然而，基于LLMs的图模型生成通常会产生部分正确的模型，这些问题主要包括三个方面：（1）语法规则违背：生成的模型可能不遵循其元模型定义的语法规则；（2）约束不一致：模型的结构可能不符合某些特定领域的约束；（3）不准确性：由于LLMs固有的不确定性，模型可能包含不准确或虚构的元素。尽管第一个问题通常通过约束解码或过滤等技术来解决，后两个问题仍然没有得到充分解决。受近期LLMs自我一致性方法的启发，我们提出了一种新颖的抽象化-具体化框架，通过考虑LLM的多个输出来增强生成的图模型的一致性和质量。该方法首先构建一个概率性的部分模型，汇集所有候选输出，然后将该部分模型细化为最合适的具体模型，该模型能够满足所有约束。我们使用多样化的数据集对多个流行的开源和封闭源LLMs进行了模型生成任务的评估。结果表明，我们的方法显著提高了生成的图模型的一致性和质量。', 'title_zh': '使用大规模语言模型从文本生成准确一致的图模型'}
{'arxiv_id': 'arXiv:2508.00250', 'title': 'Jet Image Generation in High Energy Physics Using Diffusion Models', 'authors': 'Victor D. Martinez, Vidya Manian, Sudhir Malik', 'link': 'https://arxiv.org/abs/2508.00250', 'abstract': 'This article presents, for the first time, the application of diffusion models for generating jet images corresponding to proton-proton collision events at the Large Hadron Collider (LHC). The kinematic variables of quark, gluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset are mapped to two-dimensional image representations. Diffusion models are trained on these images to learn the spatial distribution of jet constituents. We compare the performance of score-based diffusion models and consistency models in accurately generating class-conditional jet images. Unlike approaches based on latent distributions, our method operates directly in image space. The fidelity of the generated images is evaluated using several metrics, including the Fréchet Inception Distance (FID), which demonstrates that consistency models achieve higher fidelity and generation stability compared to score-based diffusion models. These advancements offer significant improvements in computational efficiency and generation accuracy, providing valuable tools for High Energy Physics (HEP) research.', 'abstract_zh': '本文首次介绍了扩散模型在生成大型强子对撞机（LHC）质子-质子碰撞事件对应的喷流图像方面的应用。从JetNet模拟数据集中，夸克、胶子、W介子、Z介子和顶夸克喷流的动力学变量被映射到二维图像表示。通过这些图像，训练扩散模型学习喷流组成部分的空间分布。我们比较了基于评分的扩散模型和一致性模型在准确生成类条件喷流图像方面的性能。不同于基于潜在分布的方法，我们的方法直接在图像空间中操作。使用多种度量标准，包括弗雷谢特-incception距离（FID），表明一致性模型在图像保真度和生成稳定性方面优于基于评分的扩散模型。这些进步为高能物理（HEP）研究提供了在计算效率和生成准确度方面的显著改善，提供了宝贵的工具。', 'title_zh': '高能物理中基于扩散模型的喷流图像生成'}
{'arxiv_id': 'arXiv:2508.00239', 'title': "What's Behind the Magic? Audiences Seek Artistic Value in Generative AI's Contributions to a Live Dance Performance", 'authors': 'Jacqueline Elise Bruen, Myounghoon Jeon', 'link': 'https://arxiv.org/abs/2508.00239', 'abstract': "With the development of generative artificial intelligence (GenAI) tools to create art, stakeholders cannot come to an agreement on the value of these works. In this study we uncovered the mixed opinions surrounding art made by AI. We developed two versions of a dance performance augmented by technology either with or without GenAI. For each version we informed audiences of the performance's development either before or after a survey on their perceptions of the performance. There were thirty-nine participants (13 males, 26 female) divided between the four performances. Results demonstrated that individuals were more inclined to attribute artistic merit to works made by GenAI when they were unaware of its use. We present this case study as a call to address the importance of utilizing the social context and the users' interpretations of GenAI in shaping a technical explanation, leading to a greater discussion that can bridge gaps in understanding.", 'abstract_zh': '随着生成性人工智能（GenAI）工具在创作艺术领域的应用发展，利益相关者在这些作品的价值上无法达成一致意见。本研究揭示了人们对AI创作的艺术作品看法的混杂态度。我们开发了两种版本的技术增强舞蹈表演，一种使用GenAI，另一种不使用GenAI。对于每种版本，我们分别在观众参与调查前或后告知其表演的开发情况。共有三十九名参与者（13名男性，26名女性），分布在四个表演中。结果显示，当参与者不了解GenAI的使用时，他们更倾向于认为这些作品具有艺术价值。我们通过这一案例研究呼吁关注社会语境和用户对GenAI的解读在技术解释中的重要性，以促进更广泛的讨论，弥合理解上的差距。', 'title_zh': '魔力背后是什么？观众在live舞蹈表演中寻求生成式AI的贡献的艺术价值'}
{'arxiv_id': 'arXiv:2508.00238', 'title': 'Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English', 'authors': 'Bryce Anderson, Riley Galpin, Tom S. Juzek', 'link': 'https://arxiv.org/abs/2508.00238', 'abstract': "In recent years, written language, particularly in science and education, has undergone remarkable shifts in word usage. These changes are widely attributed to the growing influence of Large Language Models (LLMs), which frequently rely on a distinct lexical style. Divergences between model output and target audience norms can be viewed as a form of misalignment. While these shifts are often linked to using Artificial Intelligence (AI) directly as a tool to generate text, it remains unclear whether the changes reflect broader changes in the human language system itself. To explore this question, we constructed a dataset of 22.1 million words from unscripted spoken language drawn from conversational science and technology podcasts. We analyzed lexical trends before and after ChatGPT's release in 2022, focusing on commonly LLM-associated words. Our results show a moderate yet significant increase in the usage of these words post-2022, suggesting a convergence between human word choices and LLM-associated patterns. In contrast, baseline synonym words exhibit no significant directional shift. Given the short time frame and the number of words affected, this may indicate the onset of a remarkable shift in language use. Whether this represents natural language change or a novel shift driven by AI exposure remains an open question. Similarly, although the shifts may stem from broader adoption patterns, it may also be that upstream training misalignments ultimately contribute to changes in human language use. These findings parallel ethical concerns that misaligned models may shape social and moral beliefs.", 'abstract_zh': '近年来，特别是在科学和教育领域，书面语言在词汇使用方面经历了显著转变。这些变化广泛认为是大型语言模型（LLMs）日益影响的结果，后者经常依赖于独特的词汇风格。模型输出与目标受众规范之间的差异可被视为一种失准形式。尽管这些转变通常与直接使用人工智能（AI）作为生成文本的工具有关，但目前仍不清楚这些变化是否反映了人类语言系统本身的更广泛变化。为了探索这一问题，我们构建了一个来自对话型科学和技术播客的2210万词的语料库，分析了ChatGPT于2022年发布前后词汇趋势，重点关注与LLM关联的词汇。结果显示，在2022年后这些词汇的使用有适度但显著的增加，表明人类词汇选择与LLM关联模式之间的趋同。相比之下，基准同义词在方向上未表现出显著变化。鉴于时间短暂和受影响词汇的数量，这可能表明语言使用正在经历显著转变。这种变化是自然语言演变还是由AI暴露驱动的新转变尚存争议。同样，尽管转变可能源自更广泛的应用模式，但上游训练失准也可能最终导致人类语言使用的变化。这些发现与伦理担忧相呼应，即失准模型可能塑造社会和道德信念。', 'title_zh': '模型失配与语言变化：未剧本化英语中与AI相关的语言痕迹'}
{'arxiv_id': 'arXiv:2508.00235', 'title': 'Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior', 'authors': 'Erin Rainville, Amirhossein Rasoulian, Hassan Rivaz, Yiming Xiao', 'link': 'https://arxiv.org/abs/2508.00235', 'abstract': "Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels that, if ruptured, can lead to life-threatening consequences. However, their small size and soft contrast in radiological scans often make it difficult to perform accurate and efficient detection and morphological analyses, which are critical in the clinical care of the disorder. Furthermore, the lack of large public datasets with voxel-wise expert annotations pose challenges for developing deep learning algorithms to address the issues. Therefore, we proposed a novel weakly supervised 3D multi-task UNet that integrates vesselness priors to jointly perform aneurysm detection and segmentation in time-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA detection and segmentation, we employ the popular Frangi's vesselness filter to derive soft cerebrovascular priors for both network input and an attention block to conduct segmentation from the decoder and detection from an auxiliary branch. We train our model on the Lausanne dataset with coarse ground truth segmentation, and evaluate it on the test set with refined labels from the same database. To further assess our model's generalizability, we also validate it externally on the ADAM dataset. Our results demonstrate the superior performance of the proposed technique over the SOTA techniques for aneurysm segmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate = 1.47, sensitivity = 92.9%).", 'abstract_zh': '颅内动脉瘤的新型弱监督3D多任务UNet及其血管性先验应用研究', 'title_zh': '基于多任务UNet和血管性先验的弱监督颅内动脉瘤检测与分割在MR血管成像中的应用'}
{'arxiv_id': 'arXiv:2508.00234', 'title': 'Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts', 'authors': 'Jin Yang, Qiong Wu, Zhiying Feng, Zhi Zhou, Deke Guo, Xu Chen', 'link': 'https://arxiv.org/abs/2508.00234', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities, leading to a significant increase in user demand for LLM services. However, cloud-based LLM services often suffer from high latency, unstable responsiveness, and privacy concerns. Therefore, multiple LLMs are usually deployed at the network edge to boost real-time responsiveness and protect data privacy, particularly for many emerging smart mobile and IoT applications. Given the varying response quality and latency of LLM services, a critical issue is how to route user requests from mobile and IoT devices to an appropriate LLM service (i.e., edge LLM expert) to ensure acceptable quality-of-service (QoS). Existing routing algorithms fail to simultaneously address the heterogeneity of LLM services, the interference among requests, and the dynamic workloads necessary for maintaining long-term stable QoS. To meet these challenges, in this paper we propose a novel deep reinforcement learning (DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM services. Due to the dynamic nature of the global state, we propose a dynamic state abstraction technique to compactly represent global state features with a heterogeneous graph attention network (HAN). Additionally, we introduce an action impact estimator and a tailored reward function to guide the DRL agent in maximizing QoS and preventing latency violations. Extensive experiments on both Poisson and real-world workloads demonstrate that our proposed algorithm significantly improves average QoS and computing resource efficiency compared to existing baselines.', 'abstract_zh': '基于深度强化学习的高质量LLM路由框架：动态状态抽象与服务质量优化', 'title_zh': '面向边缘计算的多专家服务质量感知大型语言模型路由方法'}
{'arxiv_id': 'arXiv:2508.00212', 'title': 'Reinitializing weights vs units for maintaining plasticity in neural networks', 'authors': 'J. Fernando Hernandez-Garcia, Shibhansh Dohare, Jun Luo, Rich S. Sutton', 'link': 'https://arxiv.org/abs/2508.00212', 'abstract': 'Loss of plasticity is a phenomenon in which a neural network loses its ability to learn when trained for an extended time on non-stationary data. It is a crucial problem to overcome when designing systems that learn continually. An effective technique for preventing loss of plasticity is reinitializing parts of the network. In this paper, we compare two different reinitialization schemes: reinitializing units vs reinitializing weights. We propose a new algorithm, which we name \\textit{selective weight reinitialization}, for reinitializing the least useful weights in a network. We compare our algorithm to continual backpropagation and ReDo, two previously proposed algorithms that reinitialize units in the network. Through our experiments in continual supervised learning problems, we identify two settings when reinitializing weights is more effective at maintaining plasticity than reinitializing units: (1) when the network has a small number of units and (2) when the network includes layer normalization. Conversely, reinitializing weights and units are equally effective at maintaining plasticity when the network is of sufficient size and does not include layer normalization. We found that reinitializing weights maintains plasticity in a wider variety of settings than reinitializing units.', 'abstract_zh': '神经网络在长时间训练非稳态数据时失去可塑性是一种现象，在设计持续学习系统时需克服的关键问题。有效的防止可塑性丧失的技术是重新初始化网络的一部分。在本文中，我们比较了两种不同的重新初始化方案：重新初始化单位与重新初始化权值。我们提出了一种新的算法——选择性权重新初始化，并将其应用于重新初始化网络中最具代表性的最无用的权值。我们将该算法与两种先前提出的重新初始化单位的算法——连续反向传播和ReDo进行了比较。通过在持续监督学习问题上的实验，我们确定了两种情况下重新初始化权值比重新初始化单位更能保持可塑性：（1）网络的单位数量较少；（2）网络包含层规范化。相反，当网络足够大且不包含层规范化时，重新初始化权重和单位在保持可塑性方面具有同等效果。我们发现，重新初始化权值相较于重新初始化单位在更多的应用场景中能更好地保持可塑性。', 'title_zh': '重新初始化权重 vs 单元以保持神经网络的可塑性'}
{'arxiv_id': 'arXiv:2508.00202', 'title': 'Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models', 'authors': 'Ecem Bozkurt, Antonio Ortega', 'link': 'https://arxiv.org/abs/2508.00202', 'abstract': 'Foundation models (FMs) pretrained on large datasets have become fundamental for various downstream machine learning tasks, in particular in scenarios where obtaining perfectly labeled data is prohibitively expensive. In this paper, we assume an FM has to be fine-tuned with noisy data and present a two-stage framework to ensure robust classification in the presence of label noise without model retraining. Recent work has shown that simple k-nearest neighbor (kNN) approaches using an embedding derived from an FM can achieve good performance even in the presence of severe label noise. Our work is motivated by the fact that these methods make use of local geometry. In this paper, following a similar two-stage procedure, reliability estimation followed by reliability-weighted inference, we show that improved performance can be achieved by introducing geometry information. For a given instance, our proposed inference uses a local neighborhood of training data, obtained using the non-negative kernel (NNK) neighborhood construction. We propose several methods for reliability estimation that can rely less on distance and local neighborhood as the label noise increases. Our evaluation on CIFAR-10 and DermaMNIST shows that our methods improve robustness across various noise conditions, surpassing standard K-NN approaches and recent adaptive-neighborhood baselines.', 'abstract_zh': '基于大规模数据预训练的模型在噪声标签下具有鲁棒分类的两阶段框架', 'title_zh': '基于几何感知可靠性的噪声标签下稳健分类框架'}
{'arxiv_id': 'arXiv:2508.00180', 'title': 'EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes', 'authors': 'Adam Block, Cyril Zhang', 'link': 'https://arxiv.org/abs/2508.00180', 'abstract': 'Stochasticity in language model fine-tuning, often caused by the small batch sizes typically used in this regime, can destabilize training by introducing large oscillations in generation quality. A popular approach to mitigating this instability is to take an Exponential moving average (EMA) of weights throughout training. While EMA reduces stochasticity, thereby smoothing training, the introduction of bias from old iterates often creates a lag in optimization relative to vanilla training. In this work, we propose the Bias-Corrected Exponential Moving Average (BEMA), a simple and practical augmentation of EMA that retains variance-reduction benefits while eliminating bias. BEMA is motivated by a simple theoretical model wherein we demonstrate provable acceleration of BEMA over both a standard EMA and vanilla training. Through an extensive suite of experiments on Language Models, we show that BEMA leads to significantly improved convergence rates and final performance over both EMA and vanilla training in a variety of standard LM benchmarks, making BEMA a practical and theoretically motivated intervention for more stable and efficient fine-tuning.', 'abstract_zh': '语言模型微调中的随机性，通常由该阶段通常使用的较小批量大小引起，可能会通过引入生成质量的巨大振荡来 destabilize 训练。缓解这种不稳定性的一种流行方法是在整个训练过程中采取指数移动平均（EMA）的权重。虽然EMA可以减少随机性，从而平滑训练，但旧迭代引入的偏差通常会导致相对于 vanilla 训练的优化滞后。在本文中，我们提出了校正偏差的指数移动平均（BEMA），这是一种简单实用的EMA扩展，保留了减小方差的好处，同时消除了偏差。BEMA 的动机是一个简单的理论模型，在该模型中，我们证明了BEMA 在加速方面优于标准的EMA 和vanilla 训练。通过在语言模型上进行广泛的实验，我们展示了BEMA 在多种标准语言模型基准上的收敛速度和最终性能显著优于EMA 和vanilla 训练，使BEMA 成为一种实用且基于理论的方法，用于提高微调的稳定性和效率。', 'title_zh': 'EMA 无延迟: 偏差校正迭代加权方案'}
{'arxiv_id': 'arXiv:2508.00178', 'title': "The SPACE of AI: Real-World Lessons on AI's Impact on Developers", 'authors': 'Brian Houck, Travis Lowdermilk, Cody Beyer, Steven Clarke, Ben Hanrahan', 'link': 'https://arxiv.org/abs/2508.00178', 'abstract': "As artificial intelligence (AI) tools become increasingly embedded in software development workflows, questions persist about their true impact on developer productivity and experience. This paper presents findings from a mixed-methods study examining how developers perceive AI's influence across the dimensions of the SPACE framework: Satisfaction, Performance, Activity, Collaboration and Efficiency. Drawing on survey responses from over 500 developers and qualitative insights from interviews and observational studies, we find that AI is broadly adopted and widely seen as enhancing productivity, particularly for routine tasks. However, the benefits vary, depending on task complexity, individual usage patterns, and team-level adoption. Developers report increased efficiency and satisfaction, with less evidence of impact on collaboration. Organizational support and peer learning play key roles in maximizing AI's value. These findings suggest that AI is augmenting developers rather than replacing them, and that effective integration depends as much on team culture and support structures as on the tools themselves. We conclude with practical recommendations for teams, organizations and researchers seeking to harness AI's potential in software engineering.", 'abstract_zh': '随着人工智能（AI）工具越来越多地嵌入软件开发工作流中，关于其对开发者生产力和体验的真实影响仍存疑问。本文呈现了对 SPACE 框架维度（满意度、表现、活动、协作与效率）中开发者对 AI 影响感知的混合方法研究发现。基于来自超过500名开发者的调查回应以及访谈和观察研究的定性见解，我们发现AI在开发者的广泛采用并普遍被认为提升了生产力，尤其是在常规任务方面。然而，这种益处依赖于任务复杂度、个体使用模式以及团队级的采用情况而有所不同。开发者报告提高了效率和满意度，但协作方面的证据较少。组织支持和同伴学习在充分发挥AI价值方面发挥着关键作用。这些发现表明，AI正在增强开发者的功能而不是取代他们，并且有效的整合不仅取决于工具本身，还取决于团队文化和支持结构。最后，我们为寻求在软件工程中利用AI潜力的团队、组织和研究者提供了实用建议。', 'title_zh': 'AI的空间：AI对开发者影响的现实世界教训'}
{'arxiv_id': 'arXiv:2508.00160', 'title': 'DeformTune: A Deformable XAI Music Prototype for Non-Musicians', 'authors': 'Ziqing Xu, Nick Bryan-Kinns', 'link': 'https://arxiv.org/abs/2508.00160', 'abstract': 'Many existing AI music generation tools rely on text prompts, complex interfaces, or instrument-like controls, which may require musical or technical knowledge that non-musicians do not possess. This paper introduces DeformTune, a prototype system that combines a tactile deformable interface with the MeasureVAE model to explore more intuitive, embodied, and explainable AI interaction. We conducted a preliminary study with 11 adult participants without formal musical training to investigate their experience with AI-assisted music creation. Thematic analysis of their feedback revealed recurring challenge--including unclear control mappings, limited expressive range, and the need for guidance throughout use. We discuss several design opportunities for enhancing explainability of AI, including multimodal feedback and progressive interaction support. These findings contribute early insights toward making AI music systems more explainable and empowering for novice users.', 'abstract_zh': 'DeformTune：一种结合可变形界面与MeasureVAE模型的直观、具身化和可解释的AI音乐创作 prototype 系统', 'title_zh': 'DeformTune: 一种面向非音乐家的可形变解释音乐原型'}
{'arxiv_id': 'arXiv:2508.00155', 'title': 'GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation', 'authors': 'Tomasz Szczepański, Szymon Płotka, Michal K. Grzeszczyk, Arleta Adamowicz, Piotr Fudalej, Przemysław Korzeniowski, Tomasz Trzciński, Arkadiusz Sitek', 'link': 'https://arxiv.org/abs/2508.00155', 'abstract': 'Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains challenging, especially for fine structures like root apices, which is critical for assessing root resorption in orthodontics. We introduce GEPAR3D, a novel approach that unifies instance detection and multi-class segmentation into a single step tailored to improve root segmentation. Our method integrates a Statistical Shape Model of dentition as a geometric prior, capturing anatomical context and morphological consistency without enforcing restrictive adjacency constraints. We leverage a deep watershed method, modeling each tooth as a continuous 3D energy basin encoding voxel distances to boundaries. This instance-aware representation ensures accurate segmentation of narrow, complex root apices. Trained on publicly available CBCT scans from a single center, our method is evaluated on external test sets from two in-house and two public medical centers. GEPAR3D achieves the highest overall segmentation performance, averaging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the second-best method) and increasing recall to 95.2% (+9.5%) across all test sets. Qualitative analyses demonstrated substantial improvements in root segmentation quality, indicating significant potential for more accurate root resorption assessment and enhanced clinical decision-making in orthodontics. We provide the implementation and dataset at this https URL.', 'abstract_zh': '三维锥形束计算机断层扫描中牙齿分割仍然具有挑战性，尤其是在对于诸如根尖这样的精细结构的分割中，这对于评估正畸中的根吸收至关重要。我们提出了一种名为GEPAR3D的新方法，该方法将实例检测和多类分割统一到单一步骤中，以改进根的分割。该方法结合了牙齿排列的统计形状模型作为几何先验，捕捉解剖学上下文和形态学一致性，而不施加限制性 adjacency 约束。我们利用一种深度分水岭方法，将每颗牙齿建模为包含体素到边界距离的连续3D能量盆地。这种实例感知的表示确保了对狭窄且复杂的根尖分割的准确度。该方法在单一中心公开提供的CBCT扫描数据上进行训练，并在外购的两家和两家公共医学中心的测试集上进行评估。GEPAR3D取得了最高的整体分割性能，平均Dice相似系数（DSC）为95.0%（超过第二佳方法2.8%），并在所有测试集上的召回率提高了9.5%至95.2%。定性分析表明根分割质量显著提高，显示了在正畸中更准确评估根吸收和增强临床决策方面的重要潜力。我们在以下链接提供了该方法的实现和数据集：this https URL。', 'title_zh': 'GEPAR3D: 几何先验辅助的3D牙齿分割学习'}
{'arxiv_id': 'arXiv:2508.00141', 'title': 'INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks', 'authors': 'Mohit Gupta, Debjit Bhowmick, Rhys Newbury, Meead Saberi, Shirui Pan, Ben Beck', 'link': 'https://arxiv.org/abs/2508.00141', 'abstract': "Accurate link-level bicycling volume estimation is essential for sustainable urban transportation planning. However, many cities face significant challenges of high data sparsity due to limited bicycling count sensor coverage. To address this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning (RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize sensor placement and improve link-level bicycling volume estimation in data-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL agent, enabling a data-driven strategic selection of sensor locations to maximize estimation performance. Applied to Melbourne's bicycling network, comprising 15,933 road segments with sensor coverage on only 141 road segments (99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume estimation by strategically selecting additional sensor locations in deployments of 50, 100, 200 and 500 sensors. Our framework outperforms traditional heuristic methods for sensor placement such as betweenness centrality, closeness centrality, observed bicycling activity and random placement, across key metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our experiments benchmark INSPIRE-GNN against standard machine learning and deep learning models in the bicycle volume estimation performance, underscoring its effectiveness. Our proposed framework provides transport planners actionable insights to effectively expand sensor networks, optimize sensor placement and maximize volume estimation accuracy and reliability of bicycling data for informed transportation planning decisions.", 'abstract_zh': '准确的链路级自行车流量估计对于可持续城市交通规划至关重要。然而，许多城市由于自行车计数传感器覆盖有限而面临严重的数据稀疏性难题。为了解决这一问题，我们提出了一种名为INSPIRE-GNN的新型强化学习（RL）增强混合图神经网络（GNN）框架，旨在优化传感器布放并在数据稀疏环境中改善链路级自行车流量估计。INSPIRE-GNN将图卷积网络（GCN）、图注意力网络（GAT）与基于深度Q网络（DQN）的RL代理相结合，实现基于数据的传感器位置战略性选择，以最大化估计性能。该框架应用于包含15,933条道路段的墨尔本自行车网络，其中仅有141条道路段有传感器覆盖（99%稀疏性）——INSPIRE-GNN在部署50、100、200和500个传感器的情景下，通过战略性选择额外的传感器位置，显著提高了流量估计性能。我们的框架在关键指标如均方误差（MSE）、均方根误差（RMSE）和绝对误差平均值（MAE）上，优于传统启发式方法（如介数中心性、接近中心性、观测到的自行车活动和随机布放）。此外，我们的实验将INSPIRE-GNN与标准的机器学习和深度学习模型在自行车流量估计性能上进行基准测试，进一步证明了其有效性。我们提出的框架为交通规划者提供了 actionable 的洞察，以有效扩展传感器网络、优化传感器布放，并最大化自行车流量估计的准确性和可靠性，从而支持明智的交通规划决策。', 'title_zh': 'INSPIRE-GNN：通过强化学习增强图神经网络优化稀疏自行车网络预测的智能传感器布局'}
{'arxiv_id': 'arXiv:2508.00140', 'title': 'Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models', 'authors': 'Zhanna Kaufman, Madeline Endres, Cindy Xiong Bearfield, Yuriy Brun', 'link': 'https://arxiv.org/abs/2508.00140', 'abstract': "Systems relying on ML have become ubiquitous, but so has biased behavior within them. Research shows that bias significantly affects stakeholders' trust in systems and how they use them. Further, stakeholders of different backgrounds view and trust the same systems differently. Thus, how ML models' behavior is explained plays a key role in comprehension and trust. We survey explainability visualizations, creating a taxonomy of design characteristics. We conduct user studies to evaluate five state-of-the-art visualization tools (LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how taxonomy characteristics affect comprehension, bias perception, and trust for non-expert ML users. Surprisingly, we find an inverse relationship between comprehension and trust: the better users understand the models, the less they trust them. We investigate the cause and find that this relationship is strongly mediated by bias perception: more comprehensible visualizations increase people's perception of bias, and increased bias perception reduces trust. We confirm this relationship is causal: Manipulating explainability visualizations to control comprehension, bias perception, and trust, we show that visualization design can significantly (p < 0.001) increase comprehension, increase perceived bias, and reduce trust. Conversely, reducing perceived model bias, either by improving model fairness or by adjusting visualization design, significantly increases trust even when comprehension remains high. Our work advances understanding of how comprehension affects trust and systematically investigates visualization's role in facilitating responsible ML applications.", 'abstract_zh': '基于机器学习系统的可解释性可视化：理解如何影响信任', 'title_zh': '你的模型是不公平的，你甚至不知道吗？偏见的ML模型的解释性可视化与理解之间的反比关系'}
{'arxiv_id': 'arXiv:2508.00135', 'title': 'Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images', 'authors': 'Basna Mohammed Salih Hasan, Ramadhan J. Mstafa', 'link': 'https://arxiv.org/abs/2508.00135', 'abstract': "Gender classification has emerged as a crucial aspect in various fields, including security, human-machine interaction, surveillance, and advertising. Nonetheless, the accuracy of this classification can be influenced by factors such as cosmetics and disguise. Consequently, our study is dedicated to addressing this concern by concentrating on gender classification using color images of the periocular region. The periocular region refers to the area surrounding the eye, including the eyelids, eyebrows, and the region between them. It contains valuable visual cues that can be used to extract key features for gender classification. This paper introduces a sophisticated Convolutional Neural Network (CNN) model that utilizes color image databases to evaluate the effectiveness of the periocular region for gender classification. To validate the model's performance, we conducted tests on two eye datasets, namely CVBL and (Female and Male). The recommended architecture achieved an outstanding accuracy of 99% on the previously unused CVBL dataset while attaining a commendable accuracy of 96% with a small number of learnable parameters (7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of our proposed model for gender classification using the periocular region, we evaluated its performance through an extensive range of metrics and compared it with other state-of-the-art approaches. The results unequivocally demonstrate the efficacy of our model, thereby suggesting its potential for practical application in domains such as security and surveillance.", 'abstract_zh': '基于 periocular 区域彩色图像的性别分类研究', 'title_zh': '探索深度学习技术在眼图像准确性别分类中的可行性'}
{'arxiv_id': 'arXiv:2508.00117', 'title': 'StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection', 'authors': 'Md. Ehsanul Haque, S. M. Jahidul Islam, Shakil Mia, Rumana Sharmin, Ashikuzzaman, Md Samir Morshed, Md. Tahmidul Huque', 'link': 'https://arxiv.org/abs/2508.00117', 'abstract': 'Liver diseases are a serious health concern in the world, which requires precise and timely diagnosis to enhance the survival chances of patients. The current literature implemented numerous machine learning and deep learning models to classify liver diseases, but most of them had some issues like high misclassification error, poor interpretability, prohibitive computational expense, and lack of good preprocessing strategies. In order to address these drawbacks, we introduced StackLiverNet in this study; an interpretable stacked ensemble model tailored to the liver disease detection task. The framework uses advanced data preprocessing and feature selection technique to increase model robustness and predictive ability. Random undersampling is performed to deal with class imbalance and make the training balanced. StackLiverNet is an ensemble of several hyperparameter-optimized base classifiers, whose complementary advantages are used through a LightGBM meta-model. The provided model demonstrates excellent performance, with the testing accuracy of 99.89%, Cohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and efficient training and inference speeds that are amenable to clinical practice (training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local Interpretable Model-Agnostic Explanations (LIME) are applied to generate transparent explanations of individual predictions, revealing high concentrations of Alkaline Phosphatase and moderate SGOT as important observations of liver disease. Also, SHAP was used to rank features by their global contribution to predictions, while the Morris method confirmed the most influential features through sensitivity analysis.', 'abstract_zh': '肝病诊断中的StackLiverNet：一种可解释的集成模型', 'title_zh': 'StackLiverNet: 一种用于准确可解释的肝病检测的新型堆叠集成模型'}
{'arxiv_id': 'arXiv:2508.00109', 'title': 'FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality', 'authors': 'Mingda Chen, Yang Li, Xilun Chen, Adina Williams, Gargi Ghosh, Scott Yih', 'link': 'https://arxiv.org/abs/2508.00109', 'abstract': 'Long-form factuality evaluation assesses the ability of models to generate accurate, comprehensive responses to short prompts. Existing benchmarks often lack human verification, leading to potential quality issues. To address this limitation, we introduce FACTORY, a large-scale, human-verified prompt set. Developed using a model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous. We conduct human evaluations on 6 state-of-the-art language models using FACTORY and existing datasets. Our results show that FACTORY is a challenging benchmark: approximately 40% of the claims made in the responses of SOTA models are not factual, compared to only 10% for other datasets. Our analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing its reliability and the necessity for models to reason across long-tailed facts.', 'abstract_zh': '长文事实性评估通过评估模型对短提示生成准确全面响应的能力来衡量模型的能力。现有的基准往往缺乏人工验证，可能导致质量问题。为解决这一局限，我们引入了FACTORY，一个大规模的人工验证提示集。该集合作用于模型循环中并由人类进一步细化，包含具有挑战性的、事实导向的、可回答且无歧义的提示。我们使用FACTORY和现有数据集对6个最先进的语言模型进行了人工评估。结果显示，FACTORY是一个具有挑战性的基准：SOTA模型响应中的约40%断言不具事实性，而其他数据集则仅为10%。我们的分析指出了FACTORY相对于前基准的优势，强调了其可靠性和模型在推理长尾事实方面的必要性。', 'title_zh': 'FACTORY：一个具有挑战性的手工验证提示集，用于长文本事实性评估'}
{'arxiv_id': 'arXiv:2508.00103', 'title': 'A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app', 'authors': 'Guilherme Guerino, Luiz Rodrigues, Luana Bianchiniand Mariana Alves, Marcelo Marinho, Thomaz Veloso, Valmir Macario, Diego Dermeval, Thales Vieira, Ig Bittencourt, Seiji Isotani', 'link': 'https://arxiv.org/abs/2508.00103', 'abstract': 'Integrating Artificial Intelligence in Education (AIED) aims to enhance learning experiences through technologies like Intelligent Tutoring Systems (ITS), offering personalized learning, increased engagement, and improved retention rates. However, AIED faces three main challenges: the critical role of teachers in the design process, the limitations and reliability of AI tools, and the accessibility of technological resources. Augmented Intelligence (AuI) addresses these challenges by enhancing human capabilities rather than replacing them, allowing systems to suggest solutions. In contrast, humans provide final assessments, thus improving AI over time. In this sense, this study focuses on designing, developing, and evaluating MathAIde, an ITS that corrects mathematics exercises using computer vision and AI and provides feedback based on photos of student work. The methodology included brainstorming sessions with potential users, high-fidelity prototyping, A/B testing, and a case study involving real-world classroom environments for teachers and students. Our research identified several design possibilities for implementing AuI in ITSs, emphasizing a balance between user needs and technological feasibility. Prioritization and validation through prototyping and testing highlighted the importance of efficiency metrics, ultimately leading to a solution that offers pre-defined remediation alternatives for teachers. Real-world deployment demonstrated the usefulness of the proposed solution. Our research contributes to the literature by providing a usable, teacher-centered design approach that involves teachers in all design phases. As a practical implication, we highlight that the user-centered design approach increases the usefulness and adoption potential of AIED systems, especially in resource-limited environments.', 'abstract_zh': '将人工智能整合于教育（AIED）旨在通过智能辅导系统（ITS）等技术提升学习体验，实现个性化学习、增强参与度并提高留存率。然而，AIED面临三大挑战：教师在设计过程中的关键作用、AI工具的局限性和可靠性，以及技术资源的可访问性。增强智能（Augmented Intelligence, AuI）通过增强人类能力而非替代人类来应对这些挑战，使系统能够提出建议解决方案。相比之下，人类提供最终评估，从而随着时间改善AI。在此意义上，本研究重点在于设计、开发和评估一种名为MathAIde的ITS，该系统使用计算机视觉和AI纠正数学练习，并根据学生作业的照片提供反馈。研究方法包括与潜在用户进行头脑风暴会、高保真原型制作、A/B测试，以及涉及真实教室环境的案例研究。我们的研究指出了在ITS中实施AuI的设计可能性，强调了用户需求和技术可行性之间的平衡。通过原型制作和测试的优先级与验证突显了效率指标的重要性，最终导致一种为教师提供预定义补救选项的解决方案。实际部署表明所提案解决方案的实用性。我们的研究为文献贡献了一种由教师为中心的设计方法，该方法在整个设计阶段涉及教师。从实践角度来看，我们强调用户为中心的设计方法提高了AIED系统的有用性和采用潜力，特别是在资源有限的环境中。', 'title_zh': '一种混合用户中心的方法以在智能 Tutoring 系统中实现增强人工智能：MathAIde 应用案例'}
{'arxiv_id': 'arXiv:2508.00098', 'title': 'Stress-Aware Resilient Neural Training', 'authors': 'Ashkan Shakarami, Yousef Yeganeh, Azade Farshad, Lorenzo Nicole, Stefano Ghidoni, Nassir Navab', 'link': 'https://arxiv.org/abs/2508.00098', 'abstract': 'This paper introduces Stress-Aware Learning, a resilient neural training paradigm in which deep neural networks dynamically adjust their optimization behavior - whether under stable training regimes or in settings with uncertain dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic) Deformation, inspired by structural fatigue in materials science. To instantiate this concept, we propose Plastic Deformation Optimizer, a stress-aware mechanism that injects adaptive noise into model parameters whenever an internal stress signal - reflecting stagnation in training loss and accuracy - indicates persistent optimization difficulty. This enables the model to escape sharp minima and converge toward flatter, more generalizable regions of the loss landscape. Experiments across six architectures, four optimizers, and seven vision benchmarks demonstrate improved robustness and generalization with minimal computational overhead. The code and 3D visuals will be available on GitHub: this https URL.', 'abstract_zh': '基于应力感知的学习：一种自适应神经训练范式', 'title_zh': 'stress-aware resilient神经训练'}
{'arxiv_id': 'arXiv:2508.00097', 'title': 'XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation', 'authors': 'Zhigen Zhao, Liuchuan Yu, Ke Jing, Ning Yang', 'link': 'https://arxiv.org/abs/2508.00097', 'abstract': "The rapid advancement of Vision-Language-Action models has created an urgent need for large-scale, high-quality robot demonstration datasets. Although teleoperation is the predominant method for data collection, current approaches suffer from limited scalability, complex setup procedures, and suboptimal data quality. This paper presents XRoboToolkit, a cross-platform framework for extended reality based robot teleoperation built on the OpenXR standard. The system features low-latency stereoscopic visual feedback, optimization-based inverse kinematics, and support for diverse tracking modalities including head, controller, hand, and auxiliary motion trackers. XRoboToolkit's modular architecture enables seamless integration across robotic platforms and simulation environments, spanning precision manipulators, mobile robots, and dexterous hands. We demonstrate the framework's effectiveness through precision manipulation tasks and validate data quality by training VLA models that exhibit robust autonomous performance.", 'abstract_zh': '基于扩展现实的跨平台机器人远程操作工具包：XRoboToolkit', 'title_zh': 'XRoboToolkit：一种跨平台机器人远程操作框架'}
{'arxiv_id': 'arXiv:2508.00085', 'title': 'Punching Bag vs. Punching Person: Motion Transferability in Videos', 'authors': 'Raiyaan Abdullah, Jared Claypoole, Michael Cogswell, Ajay Divakaran, Yogesh Rawat', 'link': 'https://arxiv.org/abs/2508.00085', 'abstract': 'Action recognition models demonstrate strong generalization, but can they effectively transfer high-level motion concepts across diverse contexts, even within similar distributions? For example, can a model recognize the broad action "punching" when presented with an unseen variation such as "punching person"? To explore this, we introduce a motion transferability framework with three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2) Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural video datasets. We evaluate 13 state-of-the-art models on these benchmarks and observe a significant drop in performance when recognizing high-level actions in novel contexts. Our analysis reveals: 1) Multimodal models struggle more with fine-grained unknown actions than with coarse ones; 2) The bias-free Syn-TA proves as challenging as real-world datasets, with models showing greater performance drops in controlled settings; 3) Larger models improve transferability when spatial cues dominate but struggle with intensive temporal reasoning, while reliance on object and background cues hinders generalization. We further explore how disentangling coarse and fine motions can improve recognition in temporally challenging datasets. We believe this study establishes a crucial benchmark for assessing motion transferability in action recognition. Datasets and relevant code: this https URL.', 'abstract_zh': '行动识别模型展示了强大的泛化能力，但它们能否有效地在多样化的情境中转移高层次的运动概念，即使在相似的分布范围内也是如此？例如，当呈现一个未见过的变化，如“打人”时，模型能否识别出“打击”这一广泛的行动？为了探究这一问题，我们引入了一个运动转移性框架，并提供了三个数据集：(1) Syn-TA，一个包含3D物体运动的合成数据集；(2) Kinetics400-TA；以及(3) Something-Something-v2-TA，这三个数据集都改编自自然视频数据集。我们在这些基准上评估了13个最先进的模型，并观察到在新颖情境中识别高层次行动时性能显著下降。我们的分析显示：1) 多模态模型在细粒度的未知动作上比在粗粒度的动作上挣扎更多；2) 偏见自由的Syn-TA与现实世界的数据集一样具有挑战性，模型在受控环境中表现出更大的性能下降；3) 较大的模型在空间线索占主导时能够提高转移性，但在密集的时间推理方面挣扎，而依赖物体和背景线索则阻碍了泛化。我们进一步探讨如何分离粗粒度和细粒度的运动以改善时序挑战数据集中的识别效果。我们认为这项研究为评估动作识别中的运动转移性建立了关键基准。数据集和相关代码：https://github.com/your-repo。', 'title_zh': '拳击袋 vs. 拳击人：视频中的动作迁移能力'}
{'arxiv_id': 'arXiv:2508.00083', 'title': 'A Survey on Code Generation with LLM-based Agents', 'authors': 'Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, Ge Li', 'link': 'https://arxiv.org/abs/2508.00083', 'abstract': "Code generation agents powered by large language models (LLMs) are revolutionizing the software development paradigm. Distinct from previous code generation techniques, code generation agents are characterized by three core features. 1) Autonomy: the ability to independently manage the entire workflow, from task decomposition to coding and debugging. 2) Expanded task scope: capabilities that extend beyond generating code snippets to encompass the full software development lifecycle (SDLC). 3) Enhancement of engineering practicality: a shift in research emphasis from algorithmic innovation toward practical engineering challenges, such as system reliability, process management, and tool integration. This domain has recently witnessed rapid development and an explosion in research, demonstrating significant application potential. This paper presents a systematic survey of the field of LLM-based code generation agents. We trace the technology's developmental trajectory from its inception and systematically categorize its core techniques, including both single-agent and multi-agent architectures. Furthermore, this survey details the applications of LLM-based agents across the full SDLC, summarizes mainstream evaluation benchmarks and metrics, and catalogs representative tools. Finally, by analyzing the primary challenges, we identify and propose several foundational, long-term research directions for the future work of the field.", 'abstract_zh': '由大型语言模型（LLMs）驱动的代码生成代理正在革新软件开发范式。与之前的代码生成技术不同，代码生成代理具有三个核心特征：1）自主性：独立管理从任务分解到编码和调试的 entire workflow。2）扩展的任务范围：能够扩展到涵盖整个软件开发生命周期（SDLC）的能力。3）增强的工程实用性和：研究重点从算法创新转向系统可靠性、过程管理以及工具集成等实际工程挑战。这一领域近期经历了快速发展和研究爆炸性增长，显示出巨大的应用潜力。本文对基于大型语言模型的代码生成代理领域的研究进行了系统的综述，追溯了该技术从 inception 到今的发展轨迹，系统地分类了其核心技术，包括单一代理和多代理架构。此外，本文详细介绍了基于大型语言模型的代理在整个 SDLC 中的应用，总结了主流评估基准和指标，并列出了代表性工具。最后，通过对主要挑战的分析，指出了未来研究方向，并提出了一些基础性、长期的研究建议。', 'title_zh': '基于LLM的代理代码生成综述'}
{'arxiv_id': 'arXiv:2508.00079', 'title': 'PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems', 'authors': 'Oshayer Siddique, J. M Areeb Uzair Alam, Md Jobayer Rahman Rafy, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan', 'link': 'https://arxiv.org/abs/2508.00079', 'abstract': 'The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at this https URL.', 'abstract_zh': '物理学作为人类智慧的基石，推动着技术的发展，并加深了我们对宇宙基本原理的理解。当代文献中包括一些专注于解决物理问题的作品——这是自然语言推理的关键领域。本文评估了前沿的大规模语言模型在解决物理问题（包括数学问题和描述性问题）方面的性能。我们还采用了多种推理时技术和代理框架来提高模型的性能，包括由其他较小的LLM代理以累计方式验证提出的解决方案，并进行了技术性能的比较分析。当将多代理框架应用于模型最初表现不佳的问题时，性能有显著提升。此外，我们引入了一个新的物理问题评估基准${\\rm P{\\small HYSICS}E{\\smallVAL}}$，包含来自各种物理教材的19,609个问题及其正确解，这些正确解是从物理论坛和教育网站抓取的。我们的代码和数据在此处公开：这个https://链接。', 'title_zh': 'PhysicsEval：在推理阶段提高大型语言模型解决物理问题能力的技术'}
{'arxiv_id': 'arXiv:2508.00078', 'title': 'Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization', 'authors': 'Imen Mahmoud, Andrei Velichko', 'link': 'https://arxiv.org/abs/2508.00078', 'abstract': 'This study proposes a novel methodological framework integrating a LightGBM regression model and genetic algorithm (GA) optimization to systematically evaluate the contribution of COVID-19-related indicators to Bitcoin return prediction. The primary objective was not merely to forecast Bitcoin returns but rather to determine whether including pandemic-related health data significantly enhances prediction accuracy. A comprehensive dataset comprising daily Bitcoin returns and COVID-19 metrics (vaccination rates, hospitalizations, testing statistics) was constructed. Predictive models, trained with and without COVID-19 features, were optimized using GA over 31 independent runs, allowing robust statistical assessment. Performance metrics (R2, RMSE, MAE) were statistically compared through distribution overlaps and Mann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified individual feature contributions. Results indicate that COVID-19 indicators significantly improved model performance, particularly in capturing extreme market fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly significant statistically). Among COVID-19 features, vaccination metrics, especially the 75th percentile of fully vaccinated individuals, emerged as dominant predictors. The proposed methodology extends existing financial analytics tools by incorporating public health signals, providing investors and policymakers with refined indicators to navigate market uncertainty during systemic crises.', 'abstract_zh': '本研究提出了一种结合LightGBM回归模型和遗传算法（GA）优化的新方法论框架，系统评估与COVID-19相关的指标对比特币收益预测的贡献。主要目标不仅仅在于预测比特币收益，而是确定是否包括与疫情相关健康数据能够显著提高预测准确性。构建了一个包含每日比特币收益和COVID-19指标（疫苗接种率、住院人数、检测统计数据）的综合数据集。通过遗传算法优化了包含和不包含COVID-19特征的预测模型，进行了31次独立运行，以实现稳健的统计评估。通过分布重叠和曼尼 Whitney U 检验比较了性能指标（R², RMSE, MAE）。通过置换特征重要性（PFI）分析量化了各个特征的贡献。结果表明，与COVID-19相关的指标显著提高了模型性能，特别是在捕捉极端市场波动方面（R² 增加了40%，RMSE 减少了2%，均具有高度统计显著性）。在COVID-19特征中，疫苗接种指标，尤其是完全接种疫苗人数的第75百分位数， emerged as主导预测因素。提出的方法拓展了现有的金融分析工具，通过整合公共卫生信号，为投资者和政策制定者提供了在系统性危机期间导航市场不确定性所需的精细化指标。', 'title_zh': '基于LightGBM和遗传优化的方法评估新冠肺炎特征对比特币回报预测的贡献'}
{'arxiv_id': 'arXiv:2508.00047', 'title': 'TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection', 'authors': 'Yuan-Cheng Yu, Yen-Chieh Ouyang, Chun-An Lin', 'link': 'https://arxiv.org/abs/2508.00047', 'abstract': 'Time-series anomaly detection plays a central role across a wide range of application domains. With the increasing proliferation of the Internet of Things (IoT) and smart manufacturing, time-series data has dramatically increased in both scale and dimensionality. This growth has exposed the limitations of traditional statistical methods in handling the high heterogeneity and complexity of such data. Inspired by the recent success of large language models (LLMs) in multimodal tasks across language and vision domains, we propose a novel unsupervised anomaly detection framework: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection (TriP-LLM). TriP-LLM integrates local and global temporal features through a tri-branch design-Patching, Selection, and Global-to encode the input time series into patch-wise tokens, which are then processed by a frozen, pretrained LLM. A lightweight patch-wise decoder reconstructs the input, from which anomaly scores are derived. We evaluate TriP-LLM on several public benchmark datasets using PATE, a recently proposed threshold-free evaluation metric, and conduct all comparisons within a unified open-source framework to ensure fairness. Experimental results show that TriP-LLM consistently outperforms recent state-of-the-art methods across all datasets, demonstrating strong detection capabilities. Furthermore, through extensive ablation studies, we verify the substantial contribution of the LLM to the overall architecture. Compared to LLM-based approaches using Channel Independence (CI) patch processing, TriP-LLM achieves significantly lower memory consumption, making it more suitable for GPU memory-constrained environments. All code and model checkpoints are publicly available on this https URL', 'abstract_zh': '三支路(patch-wise)大型语言模型时间序列异常检测框架（TriP-LLM）', 'title_zh': 'TriP-LLM：一种用于时间序列异常检测的三分支块级大型语言模型框架'}
{'arxiv_id': 'arXiv:2508.00046', 'title': 'Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains', 'authors': 'Ruo Yu Tao, Kaicheng Guo, Cameron Allen, George Konidaris', 'link': 'https://arxiv.org/abs/2508.00046', 'abstract': "Mitigating partial observability is a necessary but challenging task for general reinforcement learning algorithms. To improve an algorithm's ability to mitigate partial observability, researchers need comprehensive benchmarks to gauge progress. Most algorithms tackling partial observability are only evaluated on benchmarks with simple forms of state aliasing, such as feature masking and Gaussian noise. Such benchmarks do not represent the many forms of partial observability seen in real domains, like visual occlusion or unknown opponent intent. We argue that a partially observable benchmark should have two key properties. The first is coverage in its forms of partial observability, to ensure an algorithm's generalizability. The second is a large gap between the performance of a agents with more or less state information, all other factors roughly equal. This gap implies that an environment is memory improvable: where performance gains in a domain are from an algorithm's ability to cope with partial observability as opposed to other factors. We introduce best-practice guidelines for empirically benchmarking reinforcement learning under partial observability, as well as the open-source library POBAX: Partially Observable Benchmarks in JAX. We characterize the types of partial observability present in various environments and select representative environments for our benchmark. These environments include localization and mapping, visual control, games, and more. Additionally, we show that these tasks are all memory improvable and require hard-to-learn memory functions, providing a concrete signal for partial observability research. This framework includes recommended hyperparameters as well as algorithm implementations for fast, out-of-the-box evaluation, as well as highly performant environments implemented in JAX for GPU-scalable experimentation.", 'abstract_zh': '缓解部分可观测性是通用强化学习算法面临的一项必要但具有挑战性的任务。为了提高算法缓解部分可观测性的能力，需要全面的基准来评估进展。大多数处理部分可观测性的算法仅在特征屏蔽和高斯噪声等简单形式的状态混同基准上进行评估。这些基准无法代表真实领域中观察到的众多形式的部分可观测性，例如视觉遮挡或未知对手意图。我们认为一个部分可观测性基准应具备两个关键特性。首先，其形式的覆盖面应确保算法的泛化能力。其次，具有更多或更少状态信息的代理之间应存在显著的性能差距，其他因素大致相同。这一差距表明环境可以通过记忆改进提升性能：即性能提升源于算法处理部分可观测性的能力，而非其他因素。我们提出了在部分可观测性下 empirically benchmarking 强化学习的最佳实践指南，以及开源库 POBAX：JAX 中的部分可观测性基准。我们分析了各种环境中存在的部分可观测性类型，并选择了代表性的环境用于基准测试。这些环境包括定位与制图、视觉控制、游戏等。此外，我们展示了这些任务都是可以通过记忆改进提升性能，并且需要难以学习的记忆函数，这为部分可观测性研究提供了明确信号。该框架包括推荐的超参数和快速、开箱即用的算法实现，以及在 JAX 中实现的高度可性能环境，支持 GPU 扩展实验。', 'title_zh': '基于记忆可提升领域 Benchmarks in Partial Observability in Reinforcement Learning'}
{'arxiv_id': 'arXiv:2508.00041', 'title': 'Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages', 'authors': 'Yebo Wu, Jingguang Li, Zhijiang Guo, Li Li', 'link': 'https://arxiv.org/abs/2508.00041', 'abstract': 'Federated fine-tuning enables Large Language Models (LLMs) to adapt to downstream tasks while preserving data privacy, but its resource-intensive nature limits deployment on edge devices. In this paper, we introduce Developmental Federated Tuning (DevFT), a resource-efficient approach inspired by cognitive development that progressively builds a powerful LLM from a compact foundation. DevFT decomposes the fine-tuning process into developmental stages, each optimizing submodels with increasing parameter capacity. Knowledge from earlier stages transfers to subsequent submodels, providing optimized initialization parameters that prevent convergence to local minima and accelerate training. This paradigm mirrors human learning, gradually constructing comprehensive knowledge structure while refining existing skills. To efficiently build stage-specific submodels, DevFT introduces deconfliction-guided layer grouping and differential-based layer fusion to distill essential information and construct representative layers. Evaluations across multiple benchmarks demonstrate that DevFT significantly outperforms state-of-the-art methods, achieving up to 4.59$\\times$ faster convergence, 10.67$\\times$ reduction in communication overhead, and 9.07% average performance improvement, while maintaining compatibility with existing approaches.', 'abstract_zh': '发展性联邦微调（DevFT）：一种资源高效的大语言模型微调方法', 'title_zh': '模仿人类学习：通过认知发展阶段实现高效联邦微调'}
{'arxiv_id': 'arXiv:2508.00039', 'title': 'Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings', 'authors': 'Kaustav Chatterjee, Joshua Q. Li, Fatemeh Ansari, Masud Rana Munna, Kundan Parajulee, Jared Schwennesen', 'link': 'https://arxiv.org/abs/2508.00039', 'abstract': 'Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose safety risks to highway vehicles due to potential hang-ups. These crossings typically result from post-construction railway track maintenance activities or non-compliance with design guidelines for HRGC vertical alignments. Conventional methods for measuring HRGC profiles are costly, time-consuming, traffic-disruptive, and present safety challenges. To address these issues, this research employed advanced, cost-effective techniques and innovative modeling approaches for HRGC profile measurement. A novel hybrid deep learning framework combining Long Short-Term Memory (LSTM) and Transformer architectures was developed by utilizing instrumentation and ground truth data. Instrumentation data were gathered using a highway testing vehicle equipped with Inertial Measurement Unit (IMU) and Global Positioning System (GPS) sensors, while ground truth data were obtained via an industrial-standard walking profiler. Field data was collected at the Red Rock Railroad Corridor in Oklahoma. Three advanced deep learning models Transformer-LSTM sequential (model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel (model 3) were evaluated to identify the most efficient architecture. Models 2 and 3 outperformed the others and were deployed to generate 2D/3D HRGC profiles. The deep learning models demonstrated significant potential to enhance highway and railroad safety by enabling rapid and accurate assessment of HRGC hang-up susceptibility.', 'abstract_zh': '高原形高速公路铁路平交道口的安全评估：基于新型深度学习框架的测量方法', 'title_zh': '混合LSTM-Transformer模型用于高速公路铁路平交道特征分析'}
{'arxiv_id': 'arXiv:2508.00037', 'title': 'Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion', 'authors': 'Tong Nie, Jian Sun, Wei Ma', 'link': 'https://arxiv.org/abs/2508.00037', 'abstract': 'Networked urban systems facilitate the flow of people, resources, and services, and are essential for economic and social interactions. These systems often involve complex processes with unknown governing rules, observed by sensor-based time series. To aid decision-making in industrial and engineering contexts, data-driven predictive models are used to forecast spatiotemporal dynamics of urban systems. Current models such as graph neural networks have shown promise but face a trade-off between efficacy and efficiency due to computational demands. Hence, their applications in large-scale networks still require further efforts. This paper addresses this trade-off challenge by drawing inspiration from physical laws to inform essential model designs that align with fundamental principles and avoid architectural redundancy. By understanding both micro- and macro-processes, we present a principled interpretable neural diffusion scheme based on Transformer-like structures whose attention layers are induced by low-dimensional embeddings. The proposed scalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is validated on large-scale urban systems including traffic flow, solar power, and smart meters, showing state-of-the-art performance and remarkable scalability. Our results constitute a fresh perspective on the dynamics prediction in large-scale urban networks.', 'abstract_zh': '网络化城市系统促进人员、资源和服务的流动，是经济和社会互动的基础。这些系统通常涉及受传感器时间序列观测的复杂过程，其治理规则尚不完全清楚。为了工业和工程领域的决策支持，数据驱动的预测模型被用来预报城市系统的时空动态。当前模型如图神经网络展现了潜力，但因计算需求而存在有效性和效率之间的权衡。因此，这些模型在大规模网络中的应用仍需进一步努力。本文通过借鉴物理法则来启发模型设计，使其符合基本原理并避免架构冗余，以解决这一权衡挑战。通过理解微观和宏观过程，我们提出了一种基于Transformer结构的原理性可解释神经扩散方案，其注意力层由低维度嵌入诱导。提出的线性复杂度可扩展时空Transformer（ScaleSTF），在包括交通流量、太阳能电力和智能电表在内的大规模城市系统中得到验证，展示了顶级性能和显著的可扩展性。我们的结果为大规模城市网络中的动力学预测提供了新的视角。', 'title_zh': '基于能量引导的图神经扩散预测大规模城市网络动态'}
{'arxiv_id': 'arXiv:2508.00033', 'title': 'GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries', 'authors': 'Nuno Fachada, Daniel Fernandes, Carlos M. Fernandes, Bruno D. Ferreira-Saraiva, João P. Matos-Carvalho', 'link': 'https://arxiv.org/abs/2508.00033', 'abstract': 'Large Language Models (LLMs) have advanced rapidly as tools for automating code generation in scientific research, yet their ability to interpret and use unfamiliar Python APIs for complex computational experiments remains poorly characterized. This study systematically benchmarks a selection of state-of-the-art LLMs in generating functional Python code for two increasingly challenging scenarios: conversational data analysis with the \\textit{ParShift} library, and synthetic data generation and clustering using \\textit{pyclugen} and \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts specifying detailed requirements but omitting in-context examples. Model outputs are evaluated quantitatively for functional correctness and prompt compliance over multiple runs, and qualitatively by analyzing the errors produced when code execution fails. Results show that only a small subset of models consistently generate correct, executable code, with GPT-4.1 standing out as the only model to always succeed in both tasks. In addition to benchmarking LLM performance, this approach helps identify shortcomings in third-party libraries, such as unclear documentation or obscure implementation bugs. Overall, these findings highlight current limitations of LLMs for end-to-end scientific automation and emphasize the need for careful prompt design, comprehensive library documentation, and continued advances in language model capabilities.', 'abstract_zh': '大型语言模型（LLMs）作为自动化代码生成的工具在科学研究中快速发展，但其解释和使用陌生的Python API进行复杂计算实验的能力仍然缺乏系统性的描述。本研究系统性地评估了多种最先进的LLMs在两种逐步增加挑战性的情景下生成功能性Python代码的能力：使用\\textit{ParShift}库进行对话式数据分析，以及使用\\textit{pyclugen}和\\textit{scikit-learn}进行合成数据生成和聚类。两个实验均使用结构化、零样本提示，明确指定详细要求但省略上下文示例。模型输出在多次运行中从功能性正确性和提示合规性两个方面进行定量评估，并通过分析代码执行失败时产生的错误进行定性评估。结果表明，只有少数模型能够一致生成正确的可执行代码，其中GPT-4.1脱颖而出，唯一能够在两项任务中始终成功。除了评估LLM性能外，这种方法还帮助识别第三方库的缺点，例如不清晰的文档或隐秘的实现错误。总体而言，这些发现突显了LLMs在端到端科学自动化中的当前局限性，并强调了精心设计提示、全面的库文档以及继续提升语言模型能力的必要性。', 'title_zh': 'GPT-4.1 通过新型Python库在自动化实验设计中设定标准'}
{'arxiv_id': 'arXiv:2508.00028', 'title': 'Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models', 'authors': 'Abir Ray', 'link': 'https://arxiv.org/abs/2508.00028', 'abstract': 'Spectrum resources are often underutilized across time and space, motivating dynamic spectrum access strategies that allow secondary users to exploit unused frequencies. A key challenge is predicting when and where spectrum will be available (i.e., unused by primary licensed users) in order to enable proactive and interference-free access. This paper proposes a scalable framework for spectrum availability prediction that combines a two-state Markov chain model of primary user activity with high-fidelity propagation models from the ITU-R (specifically Recommendations P.528 and P.2108). The Markov chain captures temporal occupancy patterns, while the propagation models incorporate path loss and clutter effects to determine if primary signals exceed interference thresholds at secondary user locations. By integrating these components, the proposed method can predict spectrum opportunities both in time and space with improved accuracy. We develop the system model and algorithm for the approach, analyze its scalability and computational efficiency, and discuss assumptions, limitations, and potential applications. The framework is flexible and can be adapted to various frequency bands and scenarios. The results and analysis show that the proposed approach can effectively identify available spectrum with low computational cost, making it suitable for real-time spectrum management in cognitive radio networks and other dynamic spectrum sharing systems.', 'abstract_zh': '一种结合马尔可夫链模型和高保真传播模型的可扩展频谱可用性预测框架', 'title_zh': '基于马尔可夫链框架和ITU-R传播模型的可扩展频谱可用性预测'}
{'arxiv_id': 'arXiv:2508.00024', 'title': 'Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning', 'authors': 'Sebastián Andrés Cajas Ordóñez, Luis Fernando Torres Torres, Mario Bifulco, Carlos Andrés Durán, Cristian Bosch, Ricardo Simón Carbajo', 'link': 'https://arxiv.org/abs/2508.00024', 'abstract': 'Quantum Support Vector Machines face scalability challenges due to high-dimensional quantum states and hardware limitations. We propose an embedding-aware quantum-classical pipeline combining class-balanced k-means distillation with pretrained Vision Transformer embeddings. Our key finding: ViT embeddings uniquely enable quantum advantage, achieving up to 8.02% accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST, while CNN features show performance degradation. Using 16-qubit tensor network simulation via cuTensorNet, we provide the first systematic evidence that quantum kernel advantage depends critically on embedding choice, revealing fundamental synergy between transformer attention and quantum feature spaces. This provides a practical pathway for scalable quantum machine learning that leverages modern neural architectures.', 'abstract_zh': '量子支持向量机由于高维量子态和硬件限制面临可扩展性挑战。我们提出了一种嵌入感知的量子-经典管道，结合了类别均衡的k-means 降解与预训练的视觉变压器嵌入。我们的主要发现：视觉变压器嵌入独特地实现了量子优势，在Fashion-MNIST上相对于经典的SVMs取得了高达8.02%的准确率提升，在MNIST上取得了4.42%的提升，而CNN特征则表现退化。通过使用cuTensorNet进行16量子位张量网络模拟，我们提供了量子核优势依赖于嵌入选择的系统性证据，揭示了Transformer注意力机制与量子特征空间之间基本的协同作用。这为利用现代神经架构实现可扩展的量子机器学习提供了实际途径。', 'title_zh': '具备嵌入意识的量子-经典SVMs for可扩展的量子机器学习'}
{'arxiv_id': 'arXiv:2508.00017', 'title': 'Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation', 'authors': 'Nikolai Sergeev', 'link': 'https://arxiv.org/abs/2508.00017', 'abstract': "We present Generative Logic (GL), a deterministic architecture that begins from user-supplied axiomatic definitions -- written in a minimalist Mathematical Programming Language (MPL) -- and systematically explores their deductive neighborhood. Definitions are compiled into a distributed grid of simple Logic Blocks (LBs) that exchange messages; any time several expressions unify under an inference rule, a new fact is emitted with full provenance to its sources, yielding replayable, auditable proof graphs.\nA prototype software implementation instantiates the workflow on first-order Peano arithmetic. Starting only from the Peano axioms, GL enumerates candidate implications, applies normalization and type filters, and automatically reconstructs machine-checkable proofs of foundational arithmetic laws including associativity and commutativity of addition, associativity and commutativity of multiplication, and distributivity. Generated proofs export to navigable HTML so that every inference step can be inspected independently.\nWe outline a hardware-software co-design path toward massively parallel realizations and describe prospective integration with probabilistic models (e.g., Large Language Models (LLMs)) for autoformalization and conjecture seeding. The Python and MPL code to reproduce the Peano experiments, along with the full HTML proof graphs, are available in the project's GitHub repository at this https URL and are permanently archived at this https URL. We invite community feedback and collaboration.", 'abstract_zh': '我们提出生成逻辑（GL），这是一种确定性的架构，从用户提供的公理定义出发——这些定义使用一种简约的数学编程语言（MPL）编写——并系统地探索它们的演绎邻域。定义被编译成一个分布式逻辑块网格，逻辑块之间互相交换信息；每当若干表达式在推理规则下统一时，会发出具有完整来源追溯的新事实，从而生成可重放和可审计的证明图。\n\n一个原型软件实现将在一阶皮亚诺算术上实例化工作流。仅从皮亚诺公理开始，GL 列举候选蕴含，应用规范化和类型过滤，并自动重建可机器验证的基础算术定律的证明，包括加法和乘法的结合律和交换律，以及分配律。生成的证明导出为可导航的HTML，使得每一推理步骤都可以独立检查。\n\n我们概述了一种硬件-软件协同设计途径，以实现大规模并行实现，并描述了与概率模型（例如，大型语言模型（LLMs））集成以实现自动形式化和猜想播种的前景。用于重现皮亚诺实验的Python和MPL代码以及完整的HTML证明图可以在该项目的GitHub仓库（此链接）中找到，并永久存档在另一个链接中。我们邀请社区反馈和合作。', 'title_zh': '生成逻辑：一种新的确定性推理和知识生成计算机架构'}
{'arxiv_id': 'arXiv:2508.00011', 'title': 'AoI-Aware Resource Allocation with Deep Reinforcement Learning for HAPS-V2X Networks', 'authors': 'Ahmet Melih Ince, Ayse Elif Canbilen, Halim Yanikomeroglu', 'link': 'https://arxiv.org/abs/2508.00011', 'abstract': 'Sixth-generation (6G) networks are designed to meet the hyper-reliable and low-latency communication (HRLLC) requirements of safety-critical applications such as autonomous driving. Integrating non-terrestrial networks (NTN) into the 6G infrastructure brings redundancy to the network, ensuring continuity of communications even under extreme conditions. In particular, high-altitude platform stations (HAPS) stand out for their wide coverage and low latency advantages, supporting communication reliability and enhancing information freshness, especially in rural areas and regions with infrastructure constraints. In this paper, we present reinforcement learning-based approaches using deep deterministic policy gradient (DDPG) to dynamically optimize the age-of-information (AoI) in HAPS-enabled vehicle-to-everything (V2X) networks. The proposed method improves information freshness and overall network reliability by enabling independent learning without centralized coordination. The findings reveal the potential of HAPS-supported solutions, combined with DDPG-based learning, for efficient AoI-aware resource allocation in platoon-based autonomous vehicle systems.', 'abstract_zh': 'sixth代（6G）网络设计用于满足自动驾驶等关键安全应用的超可靠低延迟通信（HRLLC）要求。将非地面网络（NTN）集成到6G基础设施中，为网络提供了冗余，确保在极端条件下通信的连续性。特别是高空平台站（HAPS）因其广泛的覆盖范围和低延迟优势，支持通信可靠性并提高信息新鲜度，特别是在农村地区和基础设施受限的地区。在本文中，我们提出了基于强化学习的方法，使用深确定性策略梯度（DDPG）动态优化HAPS使能的车联网（V2X）网络中的信息新鲜度（AoI）。所提出的方法通过实现独立学习而无需集中协调，提高了信息新鲜度和整体网络可靠性。研究结果表明，结合HAPS支持的解决方案和基于DDPG的学习，可以在基于车队的自动驾驶系统中实现高效的AoI感知资源分配。', 'title_zh': '基于AoI意识的资源分配与HAPS-V2X网络中的深度强化学习'}
{'arxiv_id': 'arXiv:2508.00009', 'title': 'Enabling Immersive XR Collaborations over FTTR Networks (Invited)', 'authors': 'Sourav Mondal, Elaine Wong', 'link': 'https://arxiv.org/abs/2508.00009', 'abstract': 'Fiber-To-The-Room is a potential solution to achieve in-premise extended reality collaborations. This paper explores predictive bandwidth allocation and seamless handover schemes over FTTR, showing high-quality immersive experience for in-premise collaborations can be achieved. \\c{opyright} 2025 The Author(s).', 'abstract_zh': '光纤到房间是实现室内扩展现实协作的一种潜在解决方案。本文探讨了在光纤到房间（FTTR）上实现预测带宽分配和无缝切换方案的可能性，展示了高质量的沉浸式体验可以在室内协作中实现。版权所有 2025 作者。', 'title_zh': '基于FTTR网络的沉浸式XR协作-enable.invited'}
{'arxiv_id': 'arXiv:2508.00007', 'title': 'Agent Network Protocol Technical White Paper', 'authors': 'Gaowei Chang, Eidan Lin, Chengxuan Yuan, Rizhao Cai, Binbin Chen, Xuan Xie, Yin Zhang', 'link': 'https://arxiv.org/abs/2508.00007', 'abstract': 'With the development of large models and autonomous decision-making AI, agents are rapidly becoming the new entities of the internet, following mobile apps. However, existing internet infrastructure is primarily designed for human interaction, creating data silos, unfriendly interfaces, and high collaboration costs among agents, making it difficult to support the needs for large-scale agent interconnection and collaboration. The internet is undergoing a profound transformation, showing four core trends: agents replacing traditional software, universal agent interconnection, native protocol-based connections, and autonomous agent organization and collaboration. To align with these trends, Agent Network Protocol (ANP) proposes a new generation of communication protocols for the Agentic Web. ANP adheres to AI-native design, maintains compatibility with existing internet protocols, adopts a modular composable architecture, follows minimalist yet extensible principles, and enables rapid deployment based on existing infrastructure. Through a three-layer protocol system--identity and encrypted communication layer, meta-protocol negotiation layer, and application protocol layer--ANP. systematically solves the problems of agent identity authentication, dynamic negotiation, and capability discovery interoperability.', 'abstract_zh': '伴随大型模型和自主决策AI的发展，代理正在成为互联网上的新实体，继移动应用之后。然而，现有的互联网基础设施主要为人类交互设计，导致了代理间的数据孤岛、不友好的界面以及高协作成本，难以支持大规模代理互联和协作的需求。互联网正经历深刻的转型，展现出了四大核心趋势：代理取代传统软件、普遍的代理互联、基于原生协议的连接以及自主代理组织和协作。为了顺应这些趋势，代理网络协议（ANP）提出了一代适用于Agentic Web的通信协议。ANP坚持AI原生设计，保持与现有互联网协议的兼容性，采用模块化可组合架构，遵循简洁且可扩展的原则，并能基于现有基础设施实现快速部署。通过三层协议系统——身份与加密通信层、元协议协商层和应用协议层，ANP系统性地解决了代理身份认证、动态协商和能力发现互操作性的问题。', 'title_zh': '智能体网络协议技术白皮书'}
{'arxiv_id': 'arXiv:2508.00005', 'title': 'Modelling Program Spaces in Program Synthesis with Constraints', 'authors': 'Tilman Hinnerichs, Bart Swinkels, Jaap de Jong, Reuben Gardos Reid, Tudor Magirescu, Neil Yorke-Smith, Sebastijan Dumancic', 'link': 'https://arxiv.org/abs/2508.00005', 'abstract': "A core challenge in program synthesis is taming the large space of possible programs. Since program synthesis is essentially a combinatorial search, the community has sought to leverage powerful combinatorial constraint solvers. Here, constraints are used to express the program semantics, but not as a potentially potent tool to remove unwanted programs. Recent inductive logic programming approaches introduce constraints on the program's syntax to be synthesized. These syntactic constraints allow for checking and propagating a constraint without executing the program, and thus for arbitrary operators. In this work, we leverage syntactic constraints to model program spaces, defining not just solutions that are feasible, but also ones that are likely useful. To demonstrate this idea, we introduce BART, a solver that efficiently propagates and solves these constraints. We evaluate BART on program space enumeration tasks, finding that the constraints eliminate up to 99 percent of the program space, and that modeling program spaces significantly reduces enumeration time.", 'abstract_zh': '程序合成的核心挑战是驾驭可能程序的庞大空间。由于程序合成本质上是一种组合搜索，社区寻求利用强大的组合约束求解器。最近，归纳逻辑编程方法通过在拟合程序的语法上引入约束，来表达程序语义。这些语法约束可以在不执行程序的情况下检查和传播约束，因此适用于任意操作符。在本文中，我们利用语法约束来建模程序空间，不仅定义可行的解决方案，还定义可能有用的解决方案。为了展示这一思路，我们引入了BART，一种高效传播和求解这些约束的求解器。我们在程序空间枚举任务上评估了BART，发现约束可以消除高达99%的程序空间，并且建模程序空间显著减少了枚举时间。', 'title_zh': '基于约束条件下程序空间建模在程序综合中的应用'}
{'arxiv_id': 'arXiv:2507.23585', 'title': 'Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web', 'authors': 'Sophia Liu, Shm Garanganao Almeda', 'link': 'https://arxiv.org/abs/2507.23585', 'abstract': 'Today\'s algorithm-driven interfaces, from recommendation feeds to GenAI tools, often prioritize engagement and efficiency at the expense of user agency. As systems take on more decision-making, users have less control over what they see and how meaning or relationships between content are constructed. This paper introduces "Hypertextual Friction," a conceptual design stance that repositions classical hypertext principles--friction, traceability, and structure--as actionable values for reclaiming agency in algorithmically mediated environments. Through a comparative analysis of real-world interfaces--Wikipedia vs. Instagram Explore, and this http URL vs. GenAI image tools--we examine how different systems structure user experience, navigation, and authorship. We show that hypertext systems emphasize provenance, associative thinking, and user-driven meaning-making, while algorithmic systems tend to obscure process and flatten participation. We contribute: (1) a comparative analysis of how interface structures shape agency in user-driven versus agent-driven systems, and (2) a conceptual stance that offers hypertextual values as design commitments for reclaiming agency in an increasingly algorithmic web.', 'abstract_zh': '今天的算法驱动接口，从推荐流到生成式人工智能工具，往往在牺牲用户自主权的情况下优先考虑参与度和效率。随着系统承担更多的决策任务，用户对所见内容及其与内容之间意义和关系的构建失去了更多的控制权。本文提出了“超文本摩擦”这一概念设计立场，将经典超文本原理——摩擦、可跟踪性和结构——重新定位为在算法中介环境中重新获得自主权的可操作价值观。通过将现实世界的接口进行对比分析——Wikipedia与Instagram Explore，以及This URL与生成式人工智能图像工具，我们探讨了不同系统如何构建用户体验、导航和内容创作。我们表明，超文本系统强调出处、联想思维和用户驱动的意义创造，而算法系统往往掩盖过程并削弱参与。我们贡献了：(1) 对接口结构如何影响用户驱动系统与代理驱动系统中自主权的一种比较分析，(2) 一种概念立场，提出了超文本价值观作为在日益算法化的网络中重新获得自主权的设计承诺。', 'title_zh': '代理间的代理：在算法网络中设计超文本摩擦'}
{'arxiv_id': 'arXiv:2502.18148', 'title': 'NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts', 'authors': "Muhammad Farid Adilazuarda, Musa Izzanardi Wijanarko, Lucky Susanto, Khumaisa Nur'aini, Derry Wijaya, Alham Fikri Aji", 'link': 'https://arxiv.org/abs/2502.18148', 'abstract': "Indonesia is rich in languages and scripts. However, most NLP progress has been made using romanized text. In this paper, we present NusaAksara, a novel public benchmark for Indonesian languages that includes their original scripts. Our benchmark covers both text and image modalities and encompasses diverse tasks such as image segmentation, OCR, transliteration, translation, and language identification. Our data is constructed by human experts through rigorous steps. NusaAksara covers 8 scripts across 7 languages, including low-resource languages not commonly seen in NLP benchmarks. Although unsupported by Unicode, the Lampung script is included in this dataset. We benchmark our data across several models, from LLMs and VLMs such as GPT-4o, Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and show that most NLP technologies cannot handle Indonesia's local scripts, with many achieving near-zero performance.", 'abstract_zh': '印尼语及其原始书写系统的新型公共基准NusaAksara', 'title_zh': 'NusaAksara：一种多模态多语言基准，用于保存印尼土著文字'}
