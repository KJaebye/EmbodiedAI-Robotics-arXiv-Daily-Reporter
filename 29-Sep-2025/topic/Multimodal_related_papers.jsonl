{'arxiv_id': 'arXiv:2509.22205', 'title': 'From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment', 'authors': 'Ke Ye, Jiaming Zhou, Yuanfeng Qiu, Jiayi Liu, Shihui Zhou, Kun-Yu Lin, Junwei Liang', 'link': 'https://arxiv.org/abs/2509.22205', 'abstract': 'Generalizing to long-horizon manipulation tasks in a zero-shot setting remains a central challenge in robotics. Current multimodal foundation based approaches, despite their capabilities, typically fail to decompose high-level commands into executable action sequences from static visual input alone. To address this challenge, we introduce Super-Mimic, a hierarchical framework that enables zero-shot robotic imitation by directly inferring procedural intent from unscripted human demonstration videos. Our framework is composed of two sequential modules. First, a Human Intent Translator (HIT) parses the input video using multimodal reasoning to produce a sequence of language-grounded subtasks. These subtasks then condition a Future Dynamics Predictor (FDP), which employs a generative model that synthesizes a physically plausible video rollout for each step. The resulting visual trajectories are dynamics-aware, explicitly modeling crucial object interactions and contact points to guide the low-level controller. We validate this approach through extensive experiments on a suite of long-horizon manipulation tasks, where Super-Mimic significantly outperforms state-of-the-art zero-shot methods by over 20\\%. These results establish that coupling video-driven intent parsing with prospective dynamics modeling is a highly effective strategy for developing general-purpose robotic systems.', 'abstract_zh': '在零样本设置下泛化至长时域操作任务仍然是机器人领域的核心挑战。Super-Mimic：一种通过直接从未脚本化的人类演示视频推断程序化意图实现零样本机器人模仿的分层框架', 'title_zh': '从观察到想象：通过人类示范与未来构想引导长期任务操控'}
{'arxiv_id': 'arXiv:2509.21986', 'title': 'Developing Vision-Language-Action Model from Egocentric Videos', 'authors': 'Tomoya Yoshida, Shuhei Kurita, Taichi Nishimura, Shinsuke Mori', 'link': 'https://arxiv.org/abs/2509.21986', 'abstract': 'Egocentric videos capture how humans manipulate objects and tools, providing diverse motion cues for learning object manipulation. Unlike the costly, expert-driven manual teleoperation commonly used in training Vision-Language-Action models (VLAs), egocentric videos offer a scalable alternative. However, prior studies that leverage such videos for training robot policies typically rely on auxiliary annotations, such as detailed hand-pose recordings. Consequently, it remains unclear whether VLAs can be trained directly from raw egocentric videos. In this work, we address this challenge by leveraging EgoScaler, a framework that extracts 6DoF object manipulation trajectories from egocentric videos without requiring auxiliary recordings. We apply EgoScaler to four large-scale egocentric video datasets and automatically refine noisy or incomplete trajectories, thereby constructing a new large-scale dataset for VLA pre-training. Our experiments with a state-of-the-art $\\pi_0$ architecture in both simulated and real-robot environments yield three key findings: (i) pre-training on our dataset improves task success rates by over 20\\% compared to training from scratch, (ii) the performance is competitive with that achieved using real-robot datasets, and (iii) combining our dataset with real-robot data yields further improvements. These results demonstrate that egocentric videos constitute a promising and scalable resource for advancing VLA research.', 'abstract_zh': '自中心视角视频记录了人类操作物体和工具的方式，提供了学习物体操作的多样化运动线索。与为训练视觉-语言-动作模型（VLAs）常用的昂贵且依赖专家的手动遥操作不同，自中心视角视频提供了一种可扩展的替代方案。然而，先前利用此类视频训练机器人策略的研究通常依赖于辅助注释，如详细的手动姿态记录。因此，仍不清楚VLAs能否直接从原始的自中心视角视频中训练。在本工作中，我们通过利用EgoScaler框架来应对这一挑战，该框架可以从自中心视角视频中提取六自由度的物体操作轨迹，无需辅助记录。我们将EgoScaler应用到四个大规模的自中心视角视频数据集，并自动优化了嘈杂或不完整的轨迹，从而构建了一个用于VLAs预训练的新大规模数据集。我们的实验使用最新的$\\pi_0$架构在模拟和真实机器人环境中产生了三个关键发现：(i) 使用我们的数据集预训练相比于从头训练，任务成功率提高了超过20%；(ii) 性能与使用真实机器人数据集达到的性能相竞争；(iii) 将我们的数据集与真实机器人数据集结合使用进一步提高了性能。这些结果表明，自中心视角视频是促进VLAs研究的一种有前景且可扩展的资源。', 'title_zh': '从第一人称视频中开发视觉-语言-动作模型'}
{'arxiv_id': 'arXiv:2509.22379', 'title': 'A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems', 'authors': 'Stefano Carlo Lambertenghi, Mirena Flores Valdez, Andrea Stocco', 'link': 'https://arxiv.org/abs/2509.22379', 'abstract': 'Simulation-based testing is a cornerstone of Autonomous Driving System (ADS) development, offering safe and scalable evaluation across diverse driving scenarios. However, discrepancies between simulated and real-world behavior, known as the reality gap, challenge the transferability of test results to deployed systems. In this paper, we present a comprehensive empirical study comparing four representative testing modalities: Software-in-the-Loop (SiL), Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing. Using a small-scale physical vehicle equipped with real sensors (camera and LiDAR) and its digital twin, we implement each setup and evaluate two ADS architectures (modular and end-to-end) across diverse indoor driving scenarios involving real obstacles, road topologies, and indoor environments. We systematically assess the impact of each testing modality along three dimensions of the reality gap: actuation, perception, and behavioral fidelity. Our results show that while SiL and ViL setups simplify critical aspects of real-world dynamics and sensing, MR testing improves perceptual realism without compromising safety or control. Importantly, we identify the conditions under which failures do not transfer across testing modalities and isolate the underlying dimensions of the gap responsible for these discrepancies. Our findings offer actionable insights into the respective strengths and limitations of each modality and outline a path toward more robust and transferable validation of autonomous driving systems.', 'abstract_zh': '基于仿真测试在自动驾驶系统开发中的作用：解决现实差距的全面实证研究', 'title_zh': '多模态自主驾驶系统现实差距评估'}
{'arxiv_id': 'arXiv:2509.22570', 'title': 'UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration', 'authors': 'Qi Mao, Tinghan Yang, Jiahao Li, Bin Li, Libiao Jin, Yan Lu', 'link': 'https://arxiv.org/abs/2509.22570', 'abstract': 'The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI agents is transforming human-AI collaboration into bidirectional, multimodal interaction. However, existing codecs remain optimized for unimodal, one-way communication, resulting in repeated degradation under conventional compress-transmit-reconstruct pipelines. To address this limitation, we propose UniMIC, a Unified token-based Multimodal Interactive Coding framework that bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or plain text, UniMIC employs compact tokenized representations as the communication medium, enabling efficient low-bitrate transmission while maintaining compatibility with LMMs. To further enhance compression, lightweight Transformer-based entropy models with scenario-specific designs-generic, masked, and text-conditioned-effectively minimize inter-token redundancy. Extensive experiments on text-to-image generation, text-guided inpainting, outpainting, and visual question answering show that UniMIC achieves substantial bitrate savings and remains robust even at ultra-low bitrates (<0.05bpp), without compromising downstream task performance. These results establish UniMIC as a practical and forward-looking paradigm for next-generation multimodal interactive communication.', 'abstract_zh': '大型多模态模型和基于云的AI代理的快速进展正在将人类-AI协作转变为双向的多模态交互。然而，现有的编解码器仍针对单模态、单向通信进行了优化，在传统的压缩-传输-重构管道中表现出重复退化。为解决这一局限，我们提出了一种名为UniMIC的统一基于tokens的多模态交互编码框架，该框架连接边缘设备和云AI代理。UniMIC 不传输原始像素或纯文本，而是使用紧凑的token化表示作为通信介质，从而实现高效的低比特率传输，同时保持与大型多模态模型的兼容性。为进一步提高压缩效果，UniMIC 使用了轻量级的基于Transformer的熵模型，这些模型具有特定场景的设计——通用型、掩码型和文本条件型，有效减少了tokens间的冗余。在文本到图像生成、文本引导的 inpainting、outpainting 和视觉问答等广泛实验中，UniMIC 实现了显著的比特率节省，并且即使在超低比特率（<0.05 bpp）下依然保持稳健，不损害下游任务性能。这些结果确立了UniMIC作为下一代多模态交互通信实用且前瞻性的范式。', 'title_zh': 'UniMIC：基于令牌的多模态交互编码用于人机协作'}
{'arxiv_id': 'arXiv:2509.22646', 'title': 'Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs', 'authors': 'Xingyu Fu, Siyi Liu, Yinuo Xu, Pan Lu, Guangqiuse Hu, Tianbo Yang, Taran Anantasagar, Christopher Shen, Yikai Mao, Yuanzhe Liu, Keyush Shah, Chung Un Lee, Yejin Choi, James Zou, Dan Roth, Chris Callison-Burch', 'link': 'https://arxiv.org/abs/2509.22646', 'abstract': 'Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension -- whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated -- has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation.', 'abstract_zh': '人类能否识别AI生成的（假）视频并提供有根据的理由？视频生成模型虽然取得了快速进步，但一个关键维度——人类是否能检测生成视频中的深度伪造痕迹，即时空相关的视觉特征，这些特征可以揭示视频是机器生成的——却很少被关注。我们引入了DeeptraceReward，这是第一个细粒度的空间和时间意识基准，用于为视频生成奖励标注人类感知到的假痕迹。该数据集包含4300个详细标注，涵盖3300个高质量生成视频。每个标注提供自然语言解释，指出包含感知到的痕迹的边界框区域，并标注精确的时间戳。我们将这些标注归类为9大类深度伪造痕迹，这些痕迹使人类能够识别视频为AI生成，并训练多模态语言模型（语言模型）作为奖励模型，以模仿人类判断和定位。在DeeptraceReward上，我们的7B奖励模型在假线索识别、定位和解释方面平均优于GPT-5 34.7%。有趣的是，我们观察到一个一致的难度梯度：二元假与真分类明显比细粒度深度伪造痕迹检测更容易；在后者中，从自然语言解释（最容易）到空间定位，再到时间标注（最难），性能逐渐下降。通过突出显示人类感知到的深度伪造痕迹，DeeptraceReward为社会意识强且可信的视频生成提供了一个严格的测试平台和训练信号。', 'title_zh': '通过多模态大语言模型学习AI生成视频的人感知假造性'}
{'arxiv_id': 'arXiv:2509.22615', 'title': 'Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting', 'authors': 'Yasmine Omri, Connor Ding, Tsachy Weissman, Thierry Tambe', 'link': 'https://arxiv.org/abs/2509.22615', 'abstract': 'Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy intensive and costly, and (ii) patch based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat aware input stem and a perceiver resampler, training only about 7% of the total parameters. On large DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K performance while compressing inputs 3 to 20x relative to pixels. While accuracy currently trails RGB encoders, our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission efficient for edge cloud learning.', 'abstract_zh': '现代视觉语言管道由在大量图像文本数据集上训练的RGB视觉编码器驱动。虽然这些管道使零-shot能力和跨任务转移能力显著增强，但仍继承了像素域中的两项结构性效率低下：（i）将密集的RGB图像从边缘设备传输到云端是耗能且成本高昂的；（ii）基于块的标记化会导致序列长度激增，给注意力预算和上下文限制带来压力。我们探索了2D高斯点绘（2DGS）作为替代视觉基础结构的选择：一种紧凑且空间自适应的表示，通过一组带有颜色的各向异性高斯函数参数化图像。我们开发了一个可扩展的2DGS管道，具有结构化初始化、亮度感知剪枝和批量CUDA内核，实现了比之前实现快90倍以上的拟合速度，并且GPU利用率约为97%。我们进一步将对比语言图像预训练（CLIP）适应到2DGS中，通过重用冻结的基于RGB的变压器骨干，结合一个轻量级的点绘感知输入茎和一个感知器重采样器，仅仅训练了约7%的总参数。在大规模DataComp子集上，GS编码器在相对像素压缩3到20倍输入的情况下，实现了具有意义的零-shot ImageNet-1K性能。尽管当前的准确性落后于RGB编码器，但我们的结果确立了2DGS作为一种可行的多模态基础结构，指出了架构瓶颈，并开启了既具有语义力量又具有传输效率的表示形式的研究道路。', 'title_zh': '从压缩图像表示中实现视觉-语言对齐的2D高斯点绘制方法'}
{'arxiv_id': 'arXiv:2509.22378', 'title': 'Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach', 'authors': 'Zijian Zhao, Dian Jin, Zijing Zhou', 'link': 'https://arxiv.org/abs/2509.22378', 'abstract': 'Recently, Image-to-Music (I2M) generation has garnered significant attention, with potential applications in fields such as gaming, advertising, and multi-modal art creation. However, due to the ambiguous and subjective nature of I2M tasks, most end-to-end methods lack interpretability, leaving users puzzled about the generation results. Even methods based on emotion mapping face controversy, as emotion represents only a singular aspect of art. Additionally, most learning-based methods require substantial computational resources and large datasets for training, hindering accessibility for common users. To address these challenges, we propose the first Vision Language Model (VLM)-based I2M framework that offers high interpretability and low computational cost. Specifically, we utilize ABC notation to bridge the text and music modalities, enabling the VLM to generate music using natural language. We then apply multi-modal Retrieval-Augmented Generation (RAG) and self-refinement techniques to allow the VLM to produce high-quality music without external training. Furthermore, we leverage the generated motivations in text and the attention maps from the VLM to provide explanations for the generated results in both text and image modalities. To validate our method, we conduct both human studies and machine evaluations, where our method outperforms others in terms of music quality and music-image consistency, indicating promising results. Our code is available at this https URL .', 'abstract_zh': '基于视觉语言模型的图像到音乐生成框架', 'title_zh': '无 effort 图像到音乐生成：一种可解释的 RAG 基础的多模态模型方法'}
{'arxiv_id': 'arXiv:2509.22331', 'title': 'Pedestrian Attribute Recognition via Hierarchical Cross-Modality HyperGraph Learning', 'authors': 'Xiao Wang, Shujuan Wu, Xiaoxia Cheng, Changwei Bi, Jin Tang, Bin Luo', 'link': 'https://arxiv.org/abs/2509.22331', 'abstract': 'Current Pedestrian Attribute Recognition (PAR) algorithms typically focus on mapping visual features to semantic labels or attempt to enhance learning by fusing visual and attribute information. However, these methods fail to fully exploit attribute knowledge and contextual information for more accurate recognition. Although recent works have started to consider using attribute text as additional input to enhance the association between visual and semantic information, these methods are still in their infancy. To address the above challenges, this paper proposes the construction of a multi-modal knowledge graph, which is utilized to mine the relationships between local visual features and text, as well as the relationships between attributes and extensive visual context samples. Specifically, we propose an effective multi-modal knowledge graph construction method that fully considers the relationships among attributes and the relationships between attributes and vision tokens. To effectively model these relationships, this paper introduces a knowledge graph-guided cross-modal hypergraph learning framework to enhance the standard pedestrian attribute recognition framework. Comprehensive experiments on multiple PAR benchmark datasets have thoroughly demonstrated the effectiveness of our proposed knowledge graph for the PAR task, establishing a strong foundation for knowledge-guided pedestrian attribute recognition. The source code of this paper will be released on this https URL', 'abstract_zh': '当前的人行属性识别（PAR）算法通常侧重于将视觉特征映射到语义标签，或尝试通过融合视觉和属性信息来增强学习。然而，这些方法未能充分利用属性知识和上下文信息以实现更准确的识别。尽管最近的工作开始考虑使用属性文本作为额外输入以增强视觉和语义信息之间的关联，但这些方法仍处于初级阶段。为解决上述挑战，本文提出构建一个多模态知识图谱，用于挖掘局部视觉特征与文本之间的关系，以及属性与广泛视觉上下文样本之间的关系。具体地，本文提出了一种全面考虑属性之间及其与视觉标记之间关系的有效多模态知识图谱构建方法。为有效建模这些关系，本文引入了一种基于知识图谱的跨模态超图学习框架，以增强标准的人行属性识别框架。在多个PAR基准数据集上的全面实验充分证明了我们提出的知识图谱在PAR任务中的有效性，为知识导向的人行属性识别奠定了坚实基础。本文的源代码将发布在https://this-url。', 'title_zh': '基于分层跨模态超图学习的行人属性识别'}
{'arxiv_id': 'arXiv:2509.22258', 'title': 'Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks', 'authors': 'Miao Jing, Mengting Jia, Junling Lin, Zhongxia Shen, Lijun Wang, Yuanyuan Peng, Huan Gao, Mingkun Xu, Shangyang Li', 'link': 'https://arxiv.org/abs/2509.22258', 'abstract': 'Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at this https URL as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.', 'abstract_zh': '近期视觉-语言模型在医学标准基准上的进展取得了显著性能，但其真实的临床推理能力仍不清楚。现有的数据集主要强调分类准确性，导致一种评估错觉，即模型表现 seeming 熟练但实际上在高风险诊断推理方面仍表现不佳。我们引入了 Neural-MedBench，这是一个紧凑但推理密集的基准，专门设计用于探究神经学多模态临床推理的极限。Neural-MedBench 结合了多序列 MRI 扫描、结构化电子健康记录和临床笔记，并涵盖了三个核心任务家族：鉴别诊断、病灶识别和推理生成。为确保可靠的评估，我们开发了一种结合 LLM 基础评分、临床医生验证和语义相似度度量的混合评分管道。通过系统评估当前最先进的视觉-语言模型，包括 GPT-4o、Claude-4 和 MedGemma，我们观察到与传统数据集相比，它们的表现出现了明显的下降。错误分析表明，推理失败而非感知错误主导了模型的不足之处。我们的研究结果强调了双轴评估框架的必要性：面向广泛的大数据集用于统计泛化，以及面向深度、紧凑的基准如 Neural-MedBench 用于推理准确性。我们在此 https://github.com/alibaba/Qwen-public 提供了 Neural-MedBench，作为一个开放和可扩展的诊断测试平台，该平台指导未来基准的扩展并使临床可信的 AI 的严格而经济有效的评估成为可能。', 'title_zh': '超越分类准确率：神经MedBench和对更深层次推理基准的需求'}
{'arxiv_id': 'arXiv:2509.21991', 'title': 'ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models', 'authors': 'Jewon Lee, Wooksu Shin, Seungmin Yang, Ki-Ung Song, DongUk Lim, Jaeyeon Kim, Tae-Ho Kim, Bo-Kyeong Kim', 'link': 'https://arxiv.org/abs/2509.21991', 'abstract': 'Efficient processing of high-resolution images is crucial for real-world vision-language applications. However, existing Large Vision-Language Models (LVLMs) incur substantial computational overhead due to the large number of vision tokens. With the advent of "thinking with images" models, reasoning now extends beyond text to the visual domain. This capability motivates our two-stage "coarse-to-fine" reasoning pipeline: first, a downsampled image is analyzed to identify task-relevant regions; then, only these regions are cropped at full resolution and processed in a subsequent reasoning stage. This approach reduces computational cost while preserving fine-grained visual details where necessary. A major challenge lies in inferring which regions are truly relevant to a given query. Recent related methods often fail in the first stage after input-image downsampling, due to perception-driven reasoning, where clear visual information is required for effective reasoning. To address this issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs reasoning-driven perception-leveraging multimodal context to determine where to focus. Our model can account for perceptual uncertainty, expanding the cropped region to cover visually ambiguous areas for answering questions. To this end, we develop simple yet effective reward components in a reinforcement learning framework for coarse-to-fine perception. Across multiple datasets, our approach delivers higher accuracy than the original model and competitive methods, with greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V* benchmark by 4.7 points while using only 23% of the vision tokens, achieving a 3x inference speedup. The code and models can be found at: this https URL.', 'abstract_zh': '高效处理高分辨率图像对于实际视觉-语言应用至关重要。然而，现有的大型视觉-语言模型因视觉标记数量庞大而产生显著的计算开销。随着“以图思考”模型的出现，推理已经从文本领域扩展到视觉领域。这一能力促使我们提出一种两阶段“粗细结合”的推理管道：首先，对下采样的图像进行分析以识别与任务相关的区域；然后，仅对这些区域进行全分辨率裁剪并在后续推理阶段进行处理。这种方法在必要时保留了细粒度的视觉细节，同时减少了计算成本。主要挑战在于推断哪些区域真正与给定查询相关。最近的相关方法往往在输入图像下采样后的第一阶段失效，因为在这种感知驱动的推理中，清晰的视觉信息对于有效推理是必要的。为解决这一问题，我们提出ERGO（高效推理与引导观察），它利用引导感知的多模态语境来进行推理驱动的感知。我们的模型可以考虑到感知的不确定性，扩展裁剪区域以覆盖视觉模糊区域以回答问题。为此，我们在一个粗细结合的感知框架下开发了简单有效的奖励组件。在多个数据集中，我们的方法在准确性和效率方面均优于原模型和竞品方法。例如，ERGO在V*基准测试中比Qwen2.5-VL-7B高出4.7分，仅使用了23%的视觉标记，实现了3倍的推理速度提升。相关代码和模型可在以下链接找到：this https URL。', 'title_zh': 'ERGO: 高效的高分辨率视觉理解方法for Vision-Language模型'}
{'arxiv_id': 'arXiv:2509.21375', 'title': 'Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis', 'authors': 'Aleksa Jelaca, Ying Jiao, Chang Tian, Marie-Francine Moens', 'link': 'https://arxiv.org/abs/2509.21375', 'abstract': 'Text-to-image generation has advanced rapidly with large-scale multimodal training, yet fine-grained controllability remains a critical challenge. Counterfactual controllability, defined as the capacity to deliberately generate images that contradict common-sense patterns, remains a major challenge but plays a crucial role in enabling creativity and exploratory applications. In this work, we address this gap with a focus on counterfactual size (e.g., generating a tiny walrus beside a giant button) and propose an automatic prompt engineering framework that adapts base prompts into revised prompts for counterfactual images. The framework comprises three components: an image evaluator that guides dataset construction by identifying successful image generations, a supervised prompt rewriter that produces revised prompts, and a DPO-trained ranker that selects the optimal revised prompt. We construct the first counterfactual size text-image dataset and enhance the image evaluator by extending Grounded SAM with refinements, achieving a 114 percent improvement over its backbone. Experiments demonstrate that our method outperforms state-of-the-art baselines and ChatGPT-4o, establishing a foundation for future research on counterfactual controllability.', 'abstract_zh': '基于文本的图像生成随着大规模多模态训练的进行取得了 rapid进展，但细粒度可控性仍然是一个关键挑战。反事实可控性，定义为能够故意生成与常识模式矛盾的图像的能力，仍然是一个重大挑战但对促进创造力和探索性应用至关重要。在本工作中，我们重点关注反事实尺寸（例如，在一个巨大按钮旁生成一只小型海象），并提出了一种自动提示工程框架，该框架将基础提示调整为生成反事实图像的修订提示。该框架包含三个组件：一个图像评估器，它通过识别成功的图像生成来指导数据集构建；一个监督提示重写器，产生修订提示；以及一个DPO训练的排序器，选择最优的修订提示。我们构建了首个反事实尺寸文本-图像数据集，并通过扩展并改进Grounded SAM来增强图像评估器，实现了其骨干模型114%的改进。实验结果表明，我们的方法优于最先进的基线和ChatGPT-4o，为未来关于反事实可控性的研究奠定了基础。', 'title_zh': '自动提示生成以实现创造性和反事实的文本到图像合成'}
{'arxiv_id': 'arXiv:2509.21365', 'title': 'MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation', 'authors': 'Zhicheng Du, Qingyang Shi, Jiasheng Lu, Yingshan Liang, Xinyu Zhang, Yiran Wang, Peiwu Qin', 'link': 'https://arxiv.org/abs/2509.21365', 'abstract': 'The multimodal relevance metric is usually borrowed from the embedding ability of pretrained contrastive learning models for bimodal data, which is used to evaluate the correlation between cross-modal data (e.g., CLIP). However, the commonly used evaluation metrics are only suitable for the associated analysis between two modalities, which greatly limits the evaluation of multimodal similarity. Herein, we propose MAJORScore, a brand-new evaluation metric for the relevance of multiple modalities (N modalities, N>=3) via multimodal joint representation for the first time. The ability of multimodal joint representation to integrate multiple modalities into the same latent space can accurately represent different modalities at one scale, providing support for fair relevance scoring. Extensive experiments have shown that MAJORScore increases by 26.03%-64.29% for consistent modality and decreases by 13.28%-20.54% for inconsistence compared to existing methods. MAJORScore serves as a more reliable metric for evaluating similarity on large-scale multimodal datasets and multimodal model performance evaluation.', 'abstract_zh': '多模态相关性评价指标MAJORScore：通过多模态联合表示首次评估N种及以上模态的相关性', 'title_zh': 'MAJORScore: 一种基于联合表示的多模态相关性评价新指标'}
{'arxiv_id': 'arXiv:2509.21339', 'title': 'Cross-Modal Retrieval with Cauchy-Schwarz Divergence', 'authors': 'Jiahao Zhang, Wenzhe Yin, Shujian Yu', 'link': 'https://arxiv.org/abs/2509.21339', 'abstract': "Effective cross-modal retrieval requires robust alignment of heterogeneous data types. Most existing methods focus on bi-modal retrieval tasks and rely on distributional alignment techniques such as Kullback-Leibler divergence, Maximum Mean Discrepancy, and correlation alignment. However, these methods often suffer from critical limitations, including numerical instability, sensitivity to hyperparameters, and their inability to capture the full structure of the underlying distributions. In this paper, we introduce the Cauchy-Schwarz (CS) divergence, a hyperparameter-free measure that improves both training stability and retrieval performance. We further propose a novel Generalized CS (GCS) divergence inspired by Hölder's inequality. This extension enables direct alignment of three or more modalities within a unified mathematical framework through a bidirectional circular comparison scheme, eliminating the need for exhaustive pairwise comparisons. Extensive experiments on six benchmark datasets demonstrate the effectiveness of our method in both bi-modal and tri-modal retrieval tasks. The code of our CS/GCS divergence is publicly available at this https URL.", 'abstract_zh': '有效的跨模态检索需要稳健的异构数据类型对齐。大多数现有方法专注于双模态检索任务，并依赖于如KL散度、最大均值偏差和相关对齐等分布对齐技术。然而，这些方法通常遭受关键限制，包括数值不稳定、对超参数敏感以及无法捕捉底层分布的全部结构。在此论文中，我们引入了Cauchy-Schwarz (CS) 分散度，这是一种无超参数的度量，能够提高训练稳定性和检索性能。我们还提出了一个受Hölder不等式启发的广义CS (GCS) 分散度。这一扩展可以通过双向循环比较方案直接对齐三个或更多模态，消除了所有两两比较的需要。在六个基准数据集上的大量实验表明，我们的方法在双模态和三模态检索任务中均有效。我们CS/GCS分散度的代码已在以下网址公开可用：this https URL。', 'title_zh': 'Cauchy-Schwarz 散度下的跨模态检索'}
