{'arxiv_id': 'arXiv:2509.21602', 'title': 'Real-Time Indoor Object SLAM with LLM-Enhanced Priors', 'authors': 'Yang Jiao, Yiding Qiu, Henrik I. Christensen', 'link': 'https://arxiv.org/abs/2509.21602', 'abstract': 'Object-level Simultaneous Localization and Mapping (SLAM), which incorporates semantic information for high-level scene understanding, faces challenges of under-constrained optimization due to sparse observations. Prior work has introduced additional constraints using commonsense knowledge, but obtaining such priors has traditionally been labor-intensive and lacks generalizability across diverse object categories. We address this limitation by leveraging large language models (LLMs) to provide commonsense knowledge of object geometric attributes, specifically size and orientation, as prior factors in a graph-based SLAM framework. These priors are particularly beneficial during the initial phase when object observations are limited. We implement a complete pipeline integrating these priors, achieving robust data association on sparse object-level features and enabling real-time object SLAM. Our system, evaluated on the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\\% over the latest baseline. Additionally, we present real-world experiments in the supplementary video, demonstrating its real-time performance.', 'abstract_zh': '基于语义信息的对象级同时定位与建图（SLAM）面临稀疏观测导致的欠约束优化挑战。传统方法通过常识知识引入额外约束，但获取此类先验知识通常劳动密集且缺乏跨不同对象类别的一般化能力。我们通过利用大规模语言模型（LLMs）提供对象几何属性的常识知识，特别是大小和方向，作为图基SLAM框架中的先验因素来解决这一限制。这些先验在对象观测有限的初始阶段尤其有益。我们实现了一个完整的整合这些先验的管道，实现对稀疏对象级特征的稳健数据关联，并支持实时对象SLAM。在TUM RGB-D和3RScan数据集上的系统评估表明，与最新基线相比，映射精度提高了36.8%。此外，我们在补充视频中展示了其实时性能的实验结果。', 'title_zh': '带有LLM增强先验的实时室内物体SLAM'}
{'arxiv_id': 'arXiv:2509.21543', 'title': 'Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation', 'authors': 'Jinbang Huang, Zhiyuan Li, Zhanguang Zhang, Xingyue Quan, Jianye Hao, Yingxue Zhang', 'link': 'https://arxiv.org/abs/2509.21543', 'abstract': "Large Language Models (LLMs) have recently shown strong potential in robotic task planning, particularly through automatic planning domain generation that integrates symbolic search. Prior approaches, however, have largely treated these domains as search utilities, with limited attention to their potential as scalable sources of reasoning data. At the same time, progress in reasoning LLMs has been driven by chain-of-thought (CoT) supervision, whose application in robotics remains dependent on costly, human-curated datasets. We propose Plan2Evolve, an LLM self-evolving framework in which the base model generates planning domains that serve as engines for producing symbolic problem-plan pairs as reasoning traces. These pairs are then transformed into extended CoT trajectories by the same model through natural-language explanations, thereby explicitly aligning symbolic planning structures with natural language reasoning. The resulting data extend beyond the model's intrinsic planning capacity, enabling model fine-tuning that yields a planning-enhanced LLM with improved planning success, stronger cross-task generalization, and reduced inference costs.", 'abstract_zh': 'LLMs的自进化框架：Plan2Evolve，一种用于机器人任务规划的数据生成与理由解释方法', 'title_zh': 'Plan2Evolve: 通过自动化领域生成提升规划能力的LLM自主进化方法'}
{'arxiv_id': 'arXiv:2509.22281', 'title': 'MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning', 'authors': 'Jinkun Hao, Naifu Liang, Zhen Luo, Xudong Xu, Weipeng Zhong, Ran Yi, Yichen Jin, Zhaoyang Lyu, Feng Zheng, Lizhuang Ma, Jiangmiao Pang', 'link': 'https://arxiv.org/abs/2509.22281', 'abstract': 'The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training. However, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks. In this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes. To support research on such a challenging task, we introduce MesaTask-10K, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with manually crafted layouts that ensure realistic layouts and intricate inter-object relations. To bridge the gap between tasks and scenes, we propose a Spatial Reasoning Chain that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout. We present MesaTask, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions. Exhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts. Project page is at this https URL', 'abstract_zh': '机器人解读人类指令并执行操作任务的能力需要有相关任务的桌面场景进行训练。然而，传统方法创建这些场景依赖于耗费时间的手动布局设计或纯粹随机的布局，这在可信度或与任务的一致性方面存在局限性。本文提出了一种新的任务，即面向任务的桌面场景生成，由于高层任务指令与桌面场景之间存在显著差距，这一任务提出了重大的挑战。为了支持对这一具有挑战性任务的研究，我们介绍了MesaTask-10K，一个包含约10,700个手工设计布局的合成桌面场景的大规模数据集，确保了真实的布局和复杂的物体间关系。为了弥合任务与场景之间的差距，我们提出了一种空间推理链，将生成过程分解为对象推理、空间关系推理和最终3D布局的场景图构建。我们提出了MesaTask，一个基于LLM的框架，利用这一推理链，并进一步使用DPO算法生成与给定任务描述高度一致的、物理上合理的桌面场景。全面的实验表明，MesaTask在生成符合任务描述的真实布局桌面场景方面优于基准方法。项目页面网址为：https://cekunpeng.github.io/mesatask/', 'title_zh': 'MesaTask: 基于3D空间推理的面向任务的桌面场景生成'}
{'arxiv_id': 'arXiv:2509.22613', 'title': 'Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective', 'authors': 'Siwei Wang, Yifei Shen, Haoran Sun, Shi Feng, Shang-Hua Teng, Li Dong, Yaru Hao, Wei Chen', 'link': 'https://arxiv.org/abs/2509.22613', 'abstract': "Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.", 'abstract_zh': '近期的强化学习（RL）方法显著提高了大型语言模型（LLMs）的规划能力，但其有效性的理论基础仍不清楚。在本工作中，我们通过可处理的图基化抽象探究了RL的优势和局限性，重点关注策略梯度（PG）和Q学习方法。我们的理论分析表明，监督微调（SFT）可能会引入基于共现的虚假解，而RL主要通过探索实现正确的规划，突显了探索在促进更好泛化中的作用。然而，我们也展示了PG在训练过程中会遇到多样性崩溃的问题，在完美准确度达到后仍然持续。相比之下，Q学习提供了两个关键优势：离策学习和收敛时的多样性保持。我们进一步证明，精心设计奖励是防止Q学习中奖励劫持的必要条件。最后，将我们的框架应用于实际规划基准Blocksworld，我们确认这些行为在实践中确实存在。', 'title_zh': '强化学习在语言模型规划中的优势与风险：一个理论视角'}
{'arxiv_id': 'arXiv:2509.22572', 'title': 'Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time', 'authors': 'Yixuan Han, Fan Ma, Ruijie Quan, Yi Yang', 'link': 'https://arxiv.org/abs/2509.22572', 'abstract': 'Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning.', 'abstract_zh': 'Test-Time Scaling (TTS)通过在推理过程中分配额外的计算来增强大型语言模型（LLMs）的推理能力。然而，现有方法主要依赖于输出级采样，而忽视了模型架构的作用。在主流的Mixture-of-Experts（MoE）LLMs中，我们观察到激活不同数量的专家可以产生互补的解决方案集，精度稳定，揭示了一个新的、尚未充分探索的多样性来源。受此观察的启发，我们提出了动态专家搜索（DES），这是一种TTS策略，将专家激活提升为搜索空间中可控的维度。DES集成了两个关键组件：（1）动态MoE，能够在推理过程中直接控制专家数量，生成多样化的推理路径，而不增加额外成本；（2）专家配置继承，保持推理路径中的专家数量一致，而在不同运行中变化专家数量，从而在整个搜索过程中平衡稳定性和多样性。在MoE架构、验证器和推理基准（即数学、代码和知识）上的广泛实验表明，DES可靠地优于TTS基线，提升了准确性和稳定性而不额外增加成本。这些结果突显了DES作为一种实践性强且可扩展的架构感知TTS形式的重要性，展示了现代LLMs结构灵活性如何推动推理能力的提升。', 'title_zh': '动态专家搜索：在测试时增强混合专家LLM的推理能力'}
{'arxiv_id': 'arXiv:2509.22558', 'title': 'StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models', 'authors': 'Chenyu Zhou, Tianyi Xu, Jianghao Lin, Dongdong Ge', 'link': 'https://arxiv.org/abs/2509.22558', 'abstract': 'Large Language Models (LLMs) have shown promising capabilities for solving Operations Research (OR) problems. While reinforcement learning serves as a powerful paradigm for LLM training on OR problems, existing works generally face two key limitations. First, outcome reward suffers from the credit assignment problem, where correct final answers can reinforce flawed reasoning. Second, conventional discriminative process supervision is myopic, failing to evaluate the interdependent steps of OR modeling holistically. To this end, we introduce StepORLM, a novel self-evolving framework with generative process supervision. At its core, StepORLM features a co-evolutionary loop where a policy model and a generative process reward model (GenPRM) iteratively improve on each other. This loop is driven by a dual-feedback mechanism: definitive, outcome-based verification from an external solver, and nuanced, holistic process evaluation from the GenPRM. The combined signal is used to align the policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new state-of-the-art across six benchmarks, significantly outperforming vastly larger generalist models, agentic methods, and specialized baselines. Moreover, the co-evolved GenPRM is able to act as a powerful and universally applicable process verifier, substantially boosting the inference scaling performance of both our own model and other existing LLMs.', 'abstract_zh': '大型语言模型（LLMs）在解决运筹学（OR）问题上展现了令人鼓舞的能力。虽然强化学习为LLM训练OR问题提供了一个强大的框架，但现有工作通常面临两个关键限制。首先，结果奖励受到归因问题的影响，即正确的最终答案可能会强化错误的推理。其次，传统的鉴别过程监督过于短视，无法全面评估OR建模中的相互依赖步骤。为解决这些问题，我们提出了一种名为StepORLM的新型自演化框架，该框架采用生成过程监督机制。StepORLM的核心在于政策模型与生成过程奖励模型（GenPRM）之间的共同演化循环，通过双重反馈机制驱动：外部求解器的确定性、基于结果的验证，和GenPRM提供的细致、全面的过程评估。结合这些信号通过加权直接偏好优化（W-DPO）对政策进行对齐，并同时细化GenPRM。我们的8B参数StepORLM在六个基准测试中达到了新的最先进的成果，显著优于更大规模的一般模型、代理方法和专门的基线。此外，共同演化生成过程奖励模型能够作为一个强大且普适的验证器，大幅提升了我们模型以及现有其他LLMs的推理扩展性能。', 'title_zh': 'StepORLM：一种带有生成过程监督的自进化框架用于运筹语言模型'}
{'arxiv_id': 'arXiv:2509.22537', 'title': 'The Emergence of Altruism in Large-Language-Model Agents Society', 'authors': 'Haoyang Li, Xiao Jia, Zhanzhan Zhao', 'link': 'https://arxiv.org/abs/2509.22537', 'abstract': 'Leveraging Large Language Models (LLMs) for social simulation is a frontier in computational social science. Understanding the social logics these agents embody is critical to this attempt. However, existing research has primarily focused on cooperation in small-scale, task-oriented games, overlooking how altruism, which means sacrificing self-interest for collective benefit, emerges in large-scale agent societies. To address this gap, we introduce a Schelling-variant urban migration model that creates a social dilemma, compelling over 200 LLM agents to navigate an explicit conflict between egoistic (personal utility) and altruistic (system utility) goals. Our central finding is a fundamental difference in the social tendencies of LLMs. We identify two distinct archetypes: "Adaptive Egoists", which default to prioritizing self-interest but whose altruistic behaviors significantly increase under the influence of a social norm-setting message board; and "Altruistic Optimizers", which exhibit an inherent altruistic logic, consistently prioritizing collective benefit even at a direct cost to themselves. Furthermore, to qualitatively analyze the cognitive underpinnings of these decisions, we introduce a method inspired by Grounded Theory to systematically code agent reasoning. In summary, this research provides the first evidence of intrinsic heterogeneity in the egoistic and altruistic tendencies of different LLMs. We propose that for social simulation, model selection is not merely a matter of choosing reasoning capability, but of choosing an intrinsic social action logic. While "Adaptive Egoists" may offer a more suitable choice for simulating complex human societies, "Altruistic Optimizers" are better suited for modeling idealized pro-social actors or scenarios where collective welfare is the primary consideration.', 'abstract_zh': '利用大型语言模型（LLMs）进行社会模拟是计算社会科学的一个前沿领域。理解这些代理所体现的社会逻辑对于这一尝试至关重要。然而，现有研究主要集中在小规模任务导向游戏中合作机制的探索上，忽视了大规模代理社会中无私行为（牺牲个人利益以促进集体利益）的产生机制。为解决这一问题，我们引入了一个舍勒维奇变体的城市迁移模型，该模型创造了一个社会困境，促使超过200个LLM代理在个人利益（自我效用）和集体利益（系统效用）目标之间进行明面上的冲突导航。我们的主要发现是LLMs在社会倾向上的根本差异。我们识别出两种不同的原型：“适应性利己主义者”，其默认优先考虑自我利益，但在受到社会规范设置信息论坛影响时，其利他行为显著增加；“利他优化者”，表现出固有的利他逻辑，始终优先考虑集体利益，即使这意味着直接给自己带来损失。此外，为了定性分析这些决定的认知基础，我们引入了一种受扎根理论启发的方法，系统地编码代理推理。总之，这项研究提供了不同LLMs在利己和利他倾向上的内在异质性的初步证据。我们提出，在进行社会模拟时，模型选择不仅仅是选择推理能力的问题，更是选择内在的社会行动逻辑的问题。虽然“适应性利己主义者”更适用于模拟复杂的人类社会，“利他优化者”则更适合模拟理想化的亲社会行为者或以集体福利为主要考虑的场景。', 'title_zh': '大型语言模型代理社会中的利他主义 emergence'}
{'arxiv_id': 'arXiv:2509.22518', 'title': 'REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model', 'authors': 'Bo Li, Guanzhi Deng, Ronghao Chen, Junrong Yue, Shuo Zhang, Qinghua Zhao, Linqi Song, Lijie Wen', 'link': 'https://arxiv.org/abs/2509.22518', 'abstract': "Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms is a challenge in interpretability research. To provide a measurable geometric analysis perspective, we define the concept of the Reasoning Manifold, a latent low-dimensional geometric structure formed by the internal representations corresponding to all correctly reasoned generations. This structure can be conceptualized as the embodiment of the effective thinking paths that the model has learned to successfully solve a given task. Based on this concept, we build REMA, a framework that explains the origins of failures by quantitatively comparing the spatial relationships of internal model representations corresponding to both erroneous and correct reasoning samples. Specifically, REMA first quantifies the geometric deviation of each erroneous representation by calculating its k-nearest neighbors distance to the approximated manifold formed by correct representations, thereby providing a unified failure signal. It then localizes the divergence points where these deviations first become significant by tracking this deviation metric across the model's layers and comparing it against a baseline of internal fluctuations from correct representations, thus identifying where the reasoning chain begins to go off-track. Our extensive experiments on diverse language and multimodal models and tasks demonstrate the low-dimensional nature of the reasoning manifold and the high separability between erroneous and correct reasoning representations. The results also validate the effectiveness of the REMA framework in analyzing the origins of reasoning failures. This research connects abstract reasoning failures to measurable geometric deviations in representations, providing new avenues for in-depth understanding and diagnosis of the internal computational processes of black-box models.", 'abstract_zh': '理解大型语言模型在复杂推理中的表现及其失败机制是可解释性研究中的一个挑战。为了提供一个可量化的几何分析视角，我们提出了推理流形的概念，这是一种由所有正确推理生成的内部表示形成的潜在低维几何结构。这一结构可以被构想为模型在解决给定任务时所学到的有效思维路径的体现。基于这一概念，我们构建了REMA框架，通过定量比较错误和正确推理样本的内部模型表示之间的空间关系来解释其失败的根源。具体来说，REMA首先通过计算每个错误表示与由正确表示近似形成的流形的k近邻距离，来量化每个错误表示的几何偏差，从而提供统一的失败信号。然后通过跟踪这一偏差指标在整个模型层中的变化，并将其与正确表示内部分波动的基线进行比较，来定位这些偏差首次变得显著的发散点，从而确定推理链开始偏离的起始位置。我们在多种语言和多模态模型及任务上的广泛实验表明，推理流形具有低维性质，错误和正确推理表示之间的分离性很高。这些结果还验证了REMA框架在分析推理失败根源方面的有效性。该研究将抽象的推理失败与表示中的可测量几何偏差联系起来，为深入理解和诊断黑盒模型的内部计算过程提供了新的途径。', 'title_zh': 'REMA：一个统一的推理流形框架用于解释大型语言模型'}
{'arxiv_id': 'arXiv:2509.22504', 'title': 'Estimating the Empowerment of Language Model Agents', 'authors': 'Jinyeop Song, Jeff Gore, Max Kleiman-Weiner', 'link': 'https://arxiv.org/abs/2509.22504', 'abstract': "As language model (LM) agents become more capable and gain broader access to real-world tools, there is a growing need for scalable evaluation frameworks of agentic capability. However, conventional benchmark-centric evaluations are costly to design and require human designers to come up with valid tasks that translate into insights about general model capabilities. In this work, we propose information-theoretic evaluation based on empowerment, the mutual information between an agent's actions and future states, as an open-ended method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of Language Model Agents), an algorithm for approximating effective empowerment from multi-turn text interactions. We validate EELMA on both language games and scaled-up realistic web-browsing scenarios. We find that empowerment strongly correlates with average task performance, characterize the impact of environmental complexity and agentic factors such as chain-of-thought, model scale, and memory length on estimated empowerment, and that high empowerment states and actions are often pivotal moments for general capabilities. Together, these results demonstrate empowerment as an appealing general-purpose metric for evaluating and monitoring LM agents in complex, open-ended settings.", 'abstract_zh': '基于信息论的代理能力评估：利用语言模型代理的能动力量作为开放性评估方法', 'title_zh': '估计语言模型代理的赋能能力'}
{'arxiv_id': 'arXiv:2509.22502', 'title': 'InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios', 'authors': 'Chenglin Yu, Yang Yu, Songmiao Wang, Yucheng Wang, Yifan Yang, Jinjia Li, Ming Li, Hongxia Yang', 'link': 'https://arxiv.org/abs/2509.22502', 'abstract': 'Large Language Model (LLM) agents have demonstrated remarkable capabilities in organizing and executing complex tasks, and many such agents are now widely used in various application scenarios. However, developing these agents requires carefully designed workflows, carefully crafted prompts, and iterative tuning, which requires LLM techniques and domain-specific expertise. These hand-crafted limitations hinder the scalability and cost-effectiveness of LLM agents across a wide range of industries. To address these challenges, we propose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that can be applied to \\textbf{infi}nite scenarios, which introduces several key innovations: a generalized "agent-as-a-tool" mechanism that automatically decomposes complex agents into hierarchical multi-agent systems; a dual-audit mechanism that ensures the quality and stability of task completion; an agent routing function that enables efficient task-agent matching; and an agent self-evolution mechanism that autonomously restructures the agent DAG based on new tasks, poor performance, or optimization opportunities. Furthermore, InfiAgent\'s atomic task design supports agent parallelism, significantly improving execution efficiency. This framework evolves into a versatile pyramid-like multi-agent system capable of solving a wide range of problems. Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.', 'abstract_zh': '大规模语言模型（LLM）代理展示了在组织和执行复杂任务方面 remarkable 的能力，如今已在多种应用场景中普遍使用。然而，开发这些代理需要精心设计的工作流、精心构建的提示以及迭代调优，这需要LLM技术和领域特定的专业知识。这些手工制作的限制阻碍了LLM代理在各种行业的扩展性和成本效益。为了解决这些挑战，我们提出了一种名为InfiAgent的金字塔状DAG基础多代理框架，该框架可以应用于无限场景，并引入了几项关键技术创新：通用“代理作为工具”机制，自动将复杂代理分解为分层多代理系统；双重审计机制，确保任务完成的质量和稳定性；代理路由功能，实现高效的任务-代理匹配；以及代理自我进化机制，根据新任务、表现不佳或优化机会自主重构代理DAG。此外，InfiAgent的原子任务设计支持代理并行性，显著提高了执行效率。该框架演进为一种多代理系统，能够解决广泛的问题。在多个基准上的评估表明，InfiAgent相比ADAS（类似自动生成代理框架）性能提高了9.9%，而在AI研究助手InfiHelper的案例研究中，它生成的科学论文获得了顶级IEEE会议的人类评审者的认可。', 'title_zh': 'InfiAgent：无限场景自演进金字塔代理框架'}
{'arxiv_id': 'arXiv:2509.22391', 'title': 'Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents', 'authors': 'Jiaqi Shao, Yuxiang Lin, Munish Prasad Lohani, Yufeng Miao, Bing Luo', 'link': 'https://arxiv.org/abs/2509.22391', 'abstract': 'Recent work has explored training Large Language Model (LLM) search agents with reinforcement learning (RL) for open-domain question answering (QA). However, most evaluations focus solely on final answer accuracy, overlooking how these agents reason with and act on external evidence. We introduce SeekBench, the first benchmark for evaluating the \\textit{epistemic competence} of LLM search agents through step-level analysis of their response traces. SeekBench comprises 190 expert-annotated traces with over 1,800 response steps generated by LLM search agents, each enriched with evidence annotations for granular analysis of whether agents (1) generate reasoning steps grounded in observed evidence, (2) adaptively reformulate searches to recover from low-quality results, and (3) have proper calibration to correctly assess whether the current evidence is sufficient for providing an answer.', 'abstract_zh': '近期的研究探索了使用强化学习训练大型语言模型搜索代理进行开放域问答。然而，大多数评估仅集中在最终答案的准确性上，忽视了这些代理如何处理和利用外部证据进行推理和行动。我们引入了SeekBench，这是首个通过步骤级分析响应轨迹来评估大型语言模型搜索代理的本体知识 competence 的基准测试。SeekBench 包含 190 条由大型语言模型搜索代理生成的专业注解响应步骤，每条步骤都富含有证据注解，以便细致分析代理是否能够（1）生成基于观察证据的推理步骤，（2）自适应地重新制定搜索策略以克服低质量结果，以及（3）适当校准以正确评估当前证据是否足以提供答案。', 'title_zh': 'LLM代理知道如何进行知性定位、恢复和评估吗？信息寻求代理的知性能力基准'}
{'arxiv_id': 'arXiv:2509.22315', 'title': 'PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning', 'authors': 'Hieu Tran, Zonghai Yao, Nguyen Luong Tran, Zhichao Yang, Feiyun Ouyang, Shuo Han, Razieh Rahimi, Hong Yu', 'link': 'https://arxiv.org/abs/2509.22315', 'abstract': 'Inspired by the dual-process theory of human cognition from \\textit{Thinking, Fast and Slow}, we introduce \\textbf{PRIME} (Planning and Retrieval-Integrated Memory for Enhanced Reasoning), a multi-agent reasoning framework that dynamically integrates \\textbf{System 1} (fast, intuitive thinking) and \\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick Thinking Agent (System 1) to generate a rapid answer; if uncertainty is detected, it then triggers a structured System 2 reasoning pipeline composed of specialized agents for \\textit{planning}, \\textit{hypothesis generation}, \\textit{retrieval}, \\textit{information integration}, and \\textit{decision-making}. This multi-agent design faithfully mimics human cognitive processes and enhances both efficiency and accuracy. Experimental results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to perform competitively with state-of-the-art closed-source models like GPT-4 and GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This research establishes PRIME as a scalable solution for improving LLMs in domains requiring complex, knowledge-intensive reasoning.', 'abstract_zh': '受《思考，快与慢》中人类认知的两过程理论启发，我们引入了PRIME（Planning and Retrieval-Integrated Memory for Enhanced Reasoning），一种动态整合快速直观思考（System 1）和缓慢深思熟虑（System 2）的多-agent推理框架。PRIME首先利用快速思考代理（System 1）生成快速答案；若检测到不确定性，则触发由专门代理组成的结构化System 2推理管道，包括规划、假设生成、检索、信息整合和决策制定。这种多-agent设计忠实模拟了人类的认知过程，并提高了效率和准确性。实验结果表明，PRIME使开源大语言模型在需要多跳和知识驱动推理的基准测试中能够与最先进的闭源模型（如GPT-4和GPT-4o）媲美。本研究确立了PRIME作为提高需要复杂知识密集型推理的领域中大语言模型可扩展解决方案的地位。', 'title_zh': 'PRIME: 结合检索的记忆规划以增强推理'}
{'arxiv_id': 'arXiv:2509.22297', 'title': 'Large Language Models as Nondeterministic Causal Models', 'authors': 'Sander Beckers', 'link': 'https://arxiv.org/abs/2509.22297', 'abstract': "Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first time, a method for generating counterfactuals of probabilistic Large Language Models. Such counterfactuals tell us what would - or might - have been the output of an LLM if some factual prompt ${\\bf x}$ had been ${\\bf x}^*$ instead. The ability to generate such counterfactuals is an important necessary step towards explaining, evaluating, and comparing, the behavior of LLMs. I argue, however, that the existing method rests on an ambiguous interpretation of LLMs: it does not interpret LLMs literally, for the method involves the assumption that one can change the implementation of an LLM's sampling process without changing the LLM itself, nor does it interpret LLMs as intended, for the method involves explicitly representing a nondeterministic LLM as a deterministic causal model. I here present a much simpler method for generating counterfactuals that is based on an LLM's intended interpretation by representing it as a nondeterministic causal model instead. The advantage of my simpler method is that it is directly applicable to any black-box LLM without modification, as it is agnostic to any implementation details. The advantage of the existing method, on the other hand, is that it directly implements the generation of a specific type of counterfactuals that is useful for certain purposes, but not for others. I clarify how both methods relate by offering a theoretical foundation for reasoning about counterfactuals in LLMs based on their intended semantics, thereby laying the groundwork for novel application-specific methods for generating counterfactuals.", 'abstract_zh': 'Recent工作由Chatzi等人和Ravfogel等人首次开发了一种生成概率大型语言模型反事实的方法。这类反事实能够告诉我们如果某个事实提示x被替换为x*，大型语言模型的输出会是什么或者可能是怎样的。生成这类反事实的能力是解释、评估和比较大型语言模型行为的重要前提步骤。然而，我认为现有的方法基于对大型语言模型含糊不清的解释：它既不字面解释大型语言模型，因为该方法假设可以改变大型语言模型采样过程的实现而不改变模型本身，也不按照预期解释大型语言模型，因为该方法涉及将非确定性大型语言模型明确表示为确定性因果模型。我提出了一个更为简化的生成反事实的方法，该方法基于大型语言模型的预期解释，将其表示为非确定性因果模型。这种更简单的方法的优势在于它可以不加修改地应用于任何黑盒大型语言模型，因为它忽略了任何实现细节。现有的方法的优点在于它直接实现了生成特定类型对某些用途有用的反事实，但对其他用途则不然。通过提供基于大型语言模型预期语义推理反事实的理论基础，我澄清了两种方法之间的关系，为生成反事实的应用特定方法奠定基础。', 'title_zh': '大型语言模型作为非确定性因果模型'}
{'arxiv_id': 'arXiv:2509.22261', 'title': 'InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning', 'authors': 'Guanghao Zhu, Zhitian Hou, Zeyu Liu, Zhijie Sang, Congkai Xie, Hongxia Yang', 'link': 'https://arxiv.org/abs/2509.22261', 'abstract': 'Multimodal large language models (MLLMs) have shown remarkable potential in various domains, yet their application in the medical field is hindered by several challenges. General-purpose MLLMs often lack the specialized knowledge required for medical tasks, leading to uncertain or hallucinatory responses. Knowledge distillation from advanced models struggles to capture domain-specific expertise in radiology and pharmacology. Additionally, the computational cost of continual pretraining with large-scale medical data poses significant efficiency challenges. To address these issues, we propose InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, two medical-specific MLLMs designed to deliver state-of-the-art performance in medical applications. We combined high-quality general-purpose and medical multimodal data and proposed a novel five-dimensional quality assessment framework to curate high-quality multimodal medical datasets. We employ low-to-high image resolution and multimodal sequence packing to enhance training efficiency, enabling the integration of extensive medical data. Furthermore, a three-stage supervised fine-tuning process ensures effective knowledge extraction for complex medical tasks. Evaluated on the MedEvalKit framework, InfiMed-Foundation-1.7B outperforms Qwen2.5VL-3B, while InfiMed-Foundation-4B surpasses HuatuoGPT-V-7B and MedGemma-27B-IT, demonstrating superior performance in medical visual question answering and diagnostic tasks. By addressing key challenges in data quality, training efficiency, and domain-specific knowledge extraction, our work paves the way for more reliable and effective AI-driven solutions in healthcare. InfiMed-Foundation-4B model is available at \\href{this https URL}{InfiMed-Foundation-4B}.', 'abstract_zh': '多模态大型语言模型（MLLMs）在多个领域展现出了显著的潜力，但在医疗领域的应用受到多重挑战的限制。通用型MLLMs往往缺乏完成医疗任务所需的专门知识，导致响应不确定或虚构。高级模型的知识蒸馏难以捕捉医学成像和药理学领域的专业技能。此外，持续预训练大规模医疗数据的计算成本对效率提出了重大挑战。为应对这些问题，我们提出了InfiMed-Foundation-1.7B和InfiMed-Foundation-4B两种医疗专用MLLMs，旨在为医疗应用提供最先进的性能。我们结合了高质量的通用和医疗多模态数据，并提出了一种新颖的五维度质量评估框架以构建高质量的多模态医疗数据集。我们采用低到高的图像分辨率和多模态序列包装来增强训练效率，从而可以整合大量的医疗数据。此外，三阶段的监督微调过程确保了复杂医疗任务的有效知识提取。在MedEvalKit框架下，InfiMed-Foundation-1.7B超越了Qwen2.5VL-3B，而InfiMed-Foundation-4B则超过了HuatuoGPT-V-7B和MedGemma-27B-IT，展示了其在医疗视觉问答和诊断任务方面的优越性能。通过解决数据质量、训练效率和领域特定知识提取的关键问题，我们的研究为医疗领域更可靠和有效的AI驱动解决方案铺平了道路。InfiMed-Foundation-4B模型可在\\href{this https URL}{InfiMed-Foundation-4B}获取。', 'title_zh': 'InfiMed-基础模型：开创性构建计算高效预训练与多阶段 fine-tuning 的先进多模态医疗模型'}
{'arxiv_id': 'arXiv:2509.22255', 'title': 'Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase Heuristics for 2D Bin-Packing', 'authors': 'Syed Mahbubul Huq, Daniel Brito, Daniel Sikar, Rajesh Mojumder', 'link': 'https://arxiv.org/abs/2509.22255', 'abstract': "This paper presents an evaluation framework for assessing Large Language Models' (LLMs) capabilities in combinatorial optimization, specifically addressing the 2D bin-packing problem. We introduce a systematic methodology that combines LLMs with evolutionary algorithms to generate and refine heuristic solutions iteratively. Through comprehensive experiments comparing LLM generated heuristics against traditional approaches (Finite First-Fit and Hybrid First-Fit), we demonstrate that LLMs can produce more efficient solutions while requiring fewer computational resources. Our evaluation reveals that GPT-4o achieves optimal solutions within two iterations, reducing average bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78 to 0.83. This work contributes to understanding LLM evaluation in specialized domains and establishes benchmarks for assessing LLM performance in combinatorial optimization tasks.", 'abstract_zh': '本文提出了一种评估大型语言模型（LLMs）在组合优化中的能力的框架，特别针对2D容器打包问题。我们介绍了一种系统的方法，将LLMs与进化算法结合，迭代生成和优化启发式解决方案。通过综合实验，将LLM生成的启发式方法与传统方法（有限首适应和混合首适应）进行比较，我们展示了LLMs能够生成更高效的解决方案，同时需要 fewer 计算资源。我们的评估表明，GPT-4o在两轮迭代内达到最优解，平均容器使用量从16个减少到15个，空间利用率从0.76-0.78提高到0.83。本文对理解LLMs在专门领域中的评估做出了贡献，并为评估LLMs在组合优化任务中的性能建立了基准。', 'title_zh': '评估大语言模型在组合优化中的应用：二维集装箱装载的一阶段和两阶段启发式方法'}
{'arxiv_id': 'arXiv:2509.22034', 'title': 'The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging', 'authors': 'Xiaochong Lan, Yu Zheng, Shiteng Cao, Yong Li', 'link': 'https://arxiv.org/abs/2509.22034', 'abstract': 'The growing demand for large language models (LLMs) with tunable reasoning capabilities in many real-world applications highlights a critical need for methods that can efficiently produce a spectrum of models balancing reasoning depth and computational cost. Model merging has emerged as a promising, training-free technique to address this challenge by arithmetically combining the weights of a general-purpose model with a specialized reasoning model. While various merging techniques exist, their potential to create a spectrum of models with fine-grained control over reasoning abilities remains largely unexplored. This work presents a large-scale empirical study evaluating a range of model merging techniques across multiple reasoning benchmarks. We systematically vary merging strengths to construct accuracy-efficiency curves, providing the first comprehensive view of the tunable performance landscape. Our findings reveal that model merging offers an effective and controllable method for calibrating the trade-off between reasoning accuracy and token efficiency, even when parent models have highly divergent weight spaces. Crucially, we identify instances of Pareto Improvement, where a merged model achieves both higher accuracy and lower token consumption than one of its parents. Our study provides the first comprehensive analysis of this tunable space, offering practical guidelines for creating LLMs with specific reasoning profiles to meet diverse application demands.', 'abstract_zh': '大规模语言模型（LLMs）在多种实际应用中对可调推理能力的需求增长突显了高效生成平衡推理深度与计算成本的模型谱系方法的迫切需求。模型合并作为一种无训练的有前途技术，通过算术合并通用模型和专门推理模型的权重来应对这一挑战。尽管存在多种合并技术，但它们如何实现对推理能力的细微控制仍需进一步探索。本研究展开了一项大规模 empirical 实证研究，评估了多种模型合并技术在多个推理基准上的表现。系统地变化合并强度以构建准确性和效率曲线，首次全面展示了可调性能景观。我们的发现表明，模型合并提供了一种有效的、可控的方法，即使父模型的权重空间差异很大，也能校准推理准确性与标记效率之间的权衡。更重要的是，我们识别出了帕累托改进实例，即合并模型在准确性和标记消耗上均优于其父模型之一。本研究提供了对这一可调空间的首次全面分析，为创建具有特定推理特征的大规模语言模型以满足多样化应用需求提供了实用指南。', 'title_zh': '思维谱系：通过模型合并对LLMs可调推理的实证研究'}
{'arxiv_id': 'arXiv:2509.21998', 'title': 'GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments', 'authors': 'Hanlin Zhu, Tianyu Guo, Song Mei, Stuart Russell, Nikhil Ghosh, Alberto Bietti, Jiantao Jiao', 'link': 'https://arxiv.org/abs/2509.21998', 'abstract': "As LLMs are increasingly deployed as agents, agentic reasoning - the ability to combine tool use, especially search, and reasoning - becomes a critical skill. However, it is hard to disentangle agentic reasoning when evaluated in complex environments and tasks. Current agent benchmarks often mix agentic reasoning with challenging math reasoning, expert-level knowledge, and other advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent, where an LLM agent is required to solve grade-school-level reasoning problems, but is only presented with the question in the prompt without the premises that contain the necessary information to solve the task, and needs to proactively collect that information using tools. Although the original tasks are grade-school math problems, we observe that even frontier models like GPT-5 only achieve 67% accuracy. To understand and analyze the agentic reasoning patterns, we propose the concept of agentic reasoning graph: cluster the environment's document embeddings into nodes, and map each tool call to its nearest node to build a reasoning path. Surprisingly, we identify that the ability to revisit a previously visited node, widely taken as a crucial pattern in static reasoning, is often missing for agentic reasoning for many models. Based on the insight, we propose a tool-augmented test-time scaling method to improve LLM's agentic reasoning performance by adding tools to encourage models to revisit. We expect our benchmark and the agentic reasoning framework to aid future studies of understanding and pushing the boundaries of agentic reasoning.", 'abstract_zh': '随着大型语言模型被越来越多地用作代理，代理推理——结合工具使用（尤其是搜索）和推理的能力——成为了一项关键技能。然而，在复杂环境和任务中评估代理推理是困难的。当前的代理基准往往会将代理推理与其他具有挑战性的数学推理、专家级知识以及其他先进能力混合在一起。为填补这一空白，我们构建了一个新的基准——GSM-Agent，其中要求大型语言模型代理解决小学水平的推理问题，但在提示中仅呈现问题而未提供包含解决任务所需信息的前提，并需要主动使用工具收集这些信息。尽管原始任务是小学数学问题，我们观察到即使是前沿模型GPT-5也只能达到67%的准确率。为了理解和分析代理推理模式，我们提出了代理推理图的概念：将环境的文档嵌入聚类为节点，并将每个工具调用映射到最近的节点以构建推理路径。令人惊讶的是，我们发现许多模型在代理推理中缺乏在先前访问过的节点间来回的能力，这一能力在静态推理中被视为关键模式。基于这一见解，我们提出了一种工具增强的测试时尺度扩展方法，通过增加工具来鼓励模型进行回溯，从而提高大型语言模型的代理推理性能。我们期望我们的基准和代理推理框架能够有助于未来对代理推理的理解和边界扩展研究。', 'title_zh': 'GSM-Agent: 通过可控环境理解代理推理'}
{'arxiv_id': 'arXiv:2509.21993', 'title': 'Bilinear relational structure fixes reversal curse and enables consistent model editing', 'authors': 'Dong-Kyum Kim, Minsung Kim, Jea Kwon, Nakyeong Yang, Meeyoung Cha', 'link': 'https://arxiv.org/abs/2509.21993', 'abstract': "The reversal curse -- a language model's (LM) inability to infer an unseen fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a fundamental limitation. We show that this is not an inherent failure but an artifact of how models encode knowledge. By training LMs from scratch on a synthetic dataset of relational knowledge graphs, we demonstrate that bilinear relational structure emerges in their hidden representations. This structure substantially alleviates the reversal curse, enabling LMs to infer unseen reverse facts. Crucially, we also find that this bilinear structure plays a key role in consistent model editing. When a fact is updated in a LM with this structure, the edit correctly propagates to its reverse and other logically dependent facts. In contrast, models lacking this representation not only suffer from the reversal curse but also fail to generalize edits, further introducing logical inconsistencies. Our results establish that training on a relational knowledge dataset induces the emergence of bilinear internal representations, which in turn enable LMs to behave in a logically consistent manner after editing. This implies that the success of model editing depends critically not just on editing algorithms but on the underlying representational geometry of the knowledge being modified.", 'abstract_zh': '语言模型的语言反转诅咒——即其从已学习事实“A是B”推出未见事实“B是A”的能力不足——通常被认为是一个根本性的限制。我们展示这并非固有的失败，而是模型编码知识的方式所致。通过从头训练语言模型在合成的关系知识图数据集上，我们证明其隐藏表示中出现了双线性关系结构。这种结构显著缓解了语言反转诅咒，使语言模型能够推断出未见的反事实。最关键的是，我们还发现这种双线性结构在一致的模型编辑中发挥着关键作用。当一个事实被更新时，编辑能够正确定向到其反事实及其他逻辑相关事实。相比之下，缺乏这种表示的模型不仅遭受了语言反转诅咒，还无法泛化编辑，进一步引入逻辑不一致。我们的结果表明，通过关系知识数据集训练诱导出双线性内部表示，进而使模型在编辑后表现出逻辑一致性。这表明模型编辑的成功不仅依赖于编辑算法，还依赖于所修改知识的内在表示几何结构。', 'title_zh': '双边关系结构纠正反转诅咒并实现一致的模型编辑'}
{'arxiv_id': 'arXiv:2509.21862', 'title': 'Reimagining Agent-based Modeling with Large Language Model Agents via Shachi', 'authors': 'So Kuroki, Yingtao Tian, Kou Misaki, Takashi Ikegami, Takuya Akiba, Yujin Tang', 'link': 'https://arxiv.org/abs/2509.21862', 'abstract': "The study of emergent behaviors in large language model (LLM)-driven multi-agent systems is a critical research challenge, yet progress is limited by a lack of principled methodologies for controlled experimentation. To address this, we introduce Shachi, a formal methodology and modular framework that decomposes an agent's policy into core cognitive components: Configuration for intrinsic traits, Memory for contextual persistence, and Tools for expanded capabilities, all orchestrated by an LLM reasoning engine. This principled architecture moves beyond brittle, ad-hoc agent designs and enables the systematic analysis of how specific architectural choices influence collective behavior. We validate our methodology on a comprehensive 10-task benchmark and demonstrate its power through novel scientific inquiries. Critically, we establish the external validity of our approach by modeling a real-world U.S. tariff shock, showing that agent behaviors align with observed market reactions only when their cognitive architecture is appropriately configured with memory and tools. Our work provides a rigorous, open-source foundation for building and evaluating LLM agents, aimed at fostering more cumulative and scientifically grounded research.", 'abstract_zh': '大型语言模型（LLM）驱动的多agent系统中涌现行为的研究：一种形式化的研究方法和模块化框架', 'title_zh': '使用Shachi重塑基于代理的建模方法，通过大型语言模型代理'}
{'arxiv_id': 'arXiv:2509.21823', 'title': 'ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration', 'authors': 'Gaole Dai, Shiqi Jiang, Ting Cao, Yuqing Yang, Yuanchun Li, Rui Tan, Mo Li, Lili Qiu', 'link': 'https://arxiv.org/abs/2509.21823', 'abstract': 'Reward is critical to the evaluation and training of large language models (LLMs). However, existing rule-based or model-based reward methods struggle to generalize to GUI agents, where access to ground-truth trajectories or application databases is often unavailable, and static trajectory-based LLM-as-a-Judge approaches suffer from limited accuracy. To address these challenges, we propose ProRe, a proactive reward system that leverages a general-purpose reasoner and domain-specific evaluator agents (actors). The reasoner schedules targeted state probing tasks, which the evaluator agents then execute by actively interacting with the environment to collect additional observations. This enables the reasoner to assign more accurate and verifiable rewards to GUI agents. Empirical results on over 3K trajectories demonstrate that ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%, respectively. Furthermore, integrating ProRe with state-of-the-art policy agents yields a success rate improvement of up to 22.4%.', 'abstract_zh': '基于奖励的方法对于大型语言模型（LLMs）的评估和训练至关重要。然而，现有的基于规则或模型的奖励方法难以泛化到GUI代理中，因为这类代理往往无法访问真实轨迹或应用程序数据库，而基于静态轨迹的方法作为LLM的评估者则存在准确度有限的问题。为应对这些挑战，我们提出了一种名为ProRe的主动奖励系统，该系统利用通用推理器和领域特定的评估代理（演员）。推理器安排针对性的状态探测任务，评估代理则通过主动与环境交互来执行这些任务以收集额外的观察信息。这使得推理器能够为GUI代理分配更准确和可验证的奖励。在超过3000个轨迹的实证结果表明，ProRe将奖励准确性和F1分数分别提高了5.3%和19.4%。此外，将ProRe与最先进的策略代理结合使用，可将成功率提高22.4%。', 'title_zh': 'ProRe：基于推理者-演员协作的主动奖励系统 for GUI代理'}
{'arxiv_id': 'arXiv:2509.21782', 'title': 'Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety', 'authors': 'Junliang Liu, Jingyu Xiao, Wenxin Tang, Wenxuan Wang, Zhixian Wang, Minrui Zhang, Shuanghe Yu', 'link': 'https://arxiv.org/abs/2509.21782', 'abstract': 'Multimodal large language models (MLLMs) are increasingly positioned as AI collaborators for building complex web-related applications like GUI agents and front-end code generation. However, existing benchmarks largely emphasize visual perception or UI code generation, showing insufficient evaluation on the reasoning, robustness and safety capability required for end-to-end web applications. To bridge the gap, we introduce a comprehensive web understanding benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and Safety across eight tasks, such as position relationship reasoning, color robustness, and safety critical detection, etc. The benchmark is constructed from 729 websites and contains 3799 question answer pairs that probe multi-step inference over page structure, text, widgets, and safety-critical interactions. To ensure reliable measurement, we adopt standardized prompts, deterministic evaluation scripts, and multi-stage quality control combining automatic checks with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The results reveal significant gaps, models still struggle with compositional and cross-element reasoning over realistic layouts, show limited robustness when facing perturbations in user interfaces and content such as layout rearrangements or visual style shifts, and are rather conservative in recognizing and avoiding safety critical or irreversible actions. Our code is available at this https URL.', 'abstract_zh': '多模态大型语言模型（MLLMs） increasingly positioned as AI collaborators for building complex web-related applications like GUI agents and front-end code generation. However, existing benchmarks largely emphasize visual perception or UI code generation, showing insufficient evaluation on the reasoning, robustness, and safety capability required for end-to-end web applications. To bridge the gap, we introduce a comprehensive web understanding benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and Safety across eight tasks, such as position relationship reasoning, color robustness, and safety-critical detection, etc. The benchmark is constructed from 729 websites and contains 3799 question answer pairs that probe multi-step inference over page structure, text, widgets, and safety-critical interactions. To ensure reliable measurement, we adopt standardized prompts, deterministic evaluation scripts, and multi-stage quality control combining automatic checks with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The results reveal significant gaps, models still struggle with compositional and cross-element reasoning over realistic layouts, show limited robustness when facing perturbations in user interfaces and content such as layout rearrangements or visual style shifts, and are rather conservative in recognizing and avoiding safety critical or irreversible actions. Our code is available at this https URL.', 'title_zh': '基于MLLM的网络理解基准测试：推理、稳健性和安全性'}
{'arxiv_id': 'arXiv:2509.21743', 'title': 'Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts', 'authors': 'Ammar Ahmed, Azal Ahmad Khan, Ayaan Ahmad, Sheng Di, Zirui Liu, Ali Anwar', 'link': 'https://arxiv.org/abs/2509.21743', 'abstract': 'Large reasoning models improve accuracy by producing long reasoning traces, but this inflates latency and cost, motivating inference-time efficiency. We propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable ``thought" steps to guide new problems. RoT organizes steps into a thought graph with sequential and semantic edges to enable fast retrieval and flexible recombination. At inference, RoT retrieves query-relevant nodes and applies reward-guided traversal to assemble a problem-specific template that guides generation. This dynamic template reuse reduces redundant exploration and, therefore, reduces output tokens while preserving accuracy. We evaluate RoT on reasoning benchmarks with multiple models, measuring accuracy, token usage, latency, and memory overhead. Findings show small prompt growth but substantial efficiency gains, with RoT reducing output tokens by up to 40%, inference latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a scalable paradigm for efficient LRM reasoning via dynamic template construction through retrieval.', 'abstract_zh': 'Large Reasoning Models Improve Accuracy by Producing Long Reasoning Traces, but This Inflates Latency and Cost, Motivating Inference-Time Efficiency: Retrieval-of-Thought (RoT) Enables Efficient Reasoning via Dynamic Template Construction', 'title_zh': '忆再现：通过重用思想实现高效推理'}
{'arxiv_id': 'arXiv:2509.21593', 'title': 'GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models', 'authors': 'Peng Luo, Xiayin Lou, Yu Zheng, Zhuo Zheng, Stefano Ermon', 'link': 'https://arxiv.org/abs/2509.21593', 'abstract': 'Geospatial modeling provides critical solutions for pressing global challenges such as sustainability and climate change. Existing large language model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at evolving generic code but lack the domain knowledge and multi-step reasoning required for complex geospatial problems. We introduce GeoEvolve, a multi-agent LLM framework that couples evolutionary search with geospatial domain knowledge to automatically design and refine geospatial algorithms. GeoEvolve operates in two nested loops: an inner loop leverages a code evolver to generate and mutate candidate solutions, while an outer agentic controller evaluates global elites and queries a GeoKnowRAG module -- a structured geospatial knowledge base that injects theoretical priors from geography. This knowledge-guided evolution steers the search toward theoretically meaningful and computationally efficient algorithms. We evaluate GeoEvolve on two fundamental and classical tasks: spatial interpolation (kriging) and spatial uncertainty quantification (geospatial conformal prediction). Across these benchmarks, GeoEvolve automatically improves and discovers new algorithms, incorporating geospatial theory on top of classical models. It reduces spatial interpolation error (RMSE) by 13-21% and enhances uncertainty estimation performance by 17\\%. Ablation studies confirm that domain-guided retrieval is essential for stable, high-quality evolution. These results demonstrate that GeoEvolve provides a scalable path toward automated, knowledge-driven geospatial modeling, opening new opportunities for trustworthy and efficient AI-for-Science discovery.', 'abstract_zh': '地理空间建模提供了应对可持续性与气候变化等紧迫全球挑战的关键解决方案。现有的基于大型语言模型（LLM）的算法发现框架，如AlphaEvolve，擅长进化通用代码，但在处理复杂的地理空间问题时缺乏必要的领域知识和多步推理能力。我们引入GeoEvolve，这是一种结合进化搜索与地理空间领域知识的多智能体LLM框架，自动设计和优化地理空间算法。GeoEvolve通过两个嵌套循环运作：内部循环利用代码进化器生成和变异候选解决方案，外部代理控制器评估全局精英并查询包含地理理论先验的GeoKnowRAG模块。这种知识导向的进化使搜索朝着理论意义重大且计算效率高的算法方向发展。我们评估GeoEvolve在两个基本的经典任务上：空间插值（克里金法）和空间不确定性量化（地理空间一致预测）。在这些基准测试中，GeoEvolve自动改进和发现新算法，基于经典模型结合地理空间理论。它将空间插值误差（RMSE）降低了13-21%，并提高了不确定性估计性能17%。消融研究表明，领域引导的检索对于稳定高效的知识驱动进化是至关重要的。这些结果表明，GeoEvolve提供了一条可扩展的知识驱动地理空间建模的途径，为可信赖和高效的AI-for-Science发现打开了新的机会。', 'title_zh': 'GeoEvolve：通过多Agent大型语言模型自动发现地理空间模型'}
{'arxiv_id': 'arXiv:2509.21549', 'title': 'Correct Reasoning Paths Visit Shared Decision Pivots', 'authors': 'Dongkyu Cho, Amy B.Z. Zhang, Bilel Fehri, Sheng Wang, Rumi Chunara, Rui Song, Hengrui Cai', 'link': 'https://arxiv.org/abs/2509.21549', 'abstract': 'Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of large language models (LLMs), yet verifying those traces at scale remains unsolved. In response, we introduce the idea of decision pivots-minimal, verifiable checkpoints that any correct reasoning path must visit. We hypothesize that correct reasoning, though stylistically diverse, converge on the same pivot set, while incorrect ones violate at least one pivot. Leveraging this property, we propose a self-training pipeline that (i) samples diverse reasoning paths and mines shared decision pivots, (ii) compresses each trace into pivot-focused short-path reasoning using an auxiliary verifier, and (iii) post-trains the model using its self-generated outputs. The proposed method aligns reasoning without ground truth reasoning data or external metrics. Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the effectiveness of our method.', 'abstract_zh': 'Chain-of-Thought 论证揭示了大语言模型的中间推理过程，但大规模验证这些痕迹仍是一个未解决的问题。为此，我们提出了决策拐点的概念——任何正确的推理路径都必须访问的最小且可验证的检查点。我们推测，虽然正确的推理在风格上各不相同，但都会收敛于相同的拐点集，而不正确的推理至少会违背一个拐点。利用这一特性，我们提出了一种自我训练管道，该管道包括(i) 抽取多样化的推理路径并挖掘共享的决策拐点，(ii) 使用辅助验证器将每个轨迹压缩为以拐点为中心的短路径推理，以及(iii) 使用模型自动生成的输出进行后训练。所提出的方法在没有真实推理数据或外部指标的情况下实现了推理一致性。实验结果表明，该方法在标准基准如 LogiQA、MedQA 和 MATH500 中具有有效性。', 'title_zh': '正确的推理路径访问共享决策关键点'}
{'arxiv_id': 'arXiv:2509.22645', 'title': 'Hierarchical Representation Matching for CLIP-based Class-Incremental Learning', 'authors': 'Zhen-Hao Wen, Yan Wang, Ji Feng, Han-Jia Ye, De-Chuan Zhan, Da-Wei Zhou', 'link': 'https://arxiv.org/abs/2509.22645', 'abstract': 'Class-Incremental Learning (CIL) aims to endow models with the ability to continuously adapt to evolving data streams. Recent advances in pre-trained vision-language models (e.g., CLIP) provide a powerful foundation for this task. However, existing approaches often rely on simplistic templates, such as "a photo of a [CLASS]", which overlook the hierarchical nature of visual concepts. For example, recognizing "cat" versus "car" depends on coarse-grained cues, while distinguishing "cat" from "lion" requires fine-grained details. Similarly, the current feature mapping in CLIP relies solely on the representation from the last layer, neglecting the hierarchical information contained in earlier layers. In this work, we introduce HiErarchical Representation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages LLMs to recursively generate discriminative textual descriptors, thereby augmenting the semantic space with explicit hierarchical cues. These descriptors are matched to different levels of the semantic hierarchy and adaptively routed based on task-specific requirements, enabling precise discrimination while alleviating catastrophic forgetting in incremental tasks. Extensive experiments on multiple benchmarks demonstrate that our method consistently achieves state-of-the-art performance.', 'abstract_zh': '层级表示匹配（HERMAN）：基于CLIP的类增量学习', 'title_zh': '基于CLIP的类增量学习的层次表示匹配'}
{'arxiv_id': 'arXiv:2509.22644', 'title': 'WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning', 'authors': 'Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, Hongsheng Li', 'link': 'https://arxiv.org/abs/2509.22644', 'abstract': "Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce \\textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the model's website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7.", 'abstract_zh': '基于大型语言模型的代理系统在仓库级代码生成任务中展现了令人印象深刻的性能。然而，对于高度依赖视觉效果和用户交互反馈的任务，如网站代码生成，当前的代码代理仅依赖简单的代码执行来获取反馈和验证，这种方法未能捕捉到生成代码的实际质量。本文提出了一种名为WebGen-Agent的新型网站生成代理，该代理利用全面的多级视觉反馈来迭代生成和优化网站代码库。视觉语言模型(VLM)生成了关于屏幕截图和GUI代理测试的详细描述和建议，以及衡量其质量的评分。屏幕截图和GUI代理评分进一步与回溯和选择最佳机制集成，增强了代理的性能。利用WebGen-Agent工作流中固有的准确视觉评分，我们引入了基于屏幕截图和GUI代理反馈的Step-GRPO方法，以提高大型语言模型作为WebGen-Agent推理引擎的能力。通过在每一步骤中使用屏幕截图和GUI代理评分作为奖励，我们提供了一个密集且可靠的进程监督信号，有效提升模型的网站生成能力。实验结果表明，WebGen-Agent在WebGen-Bench数据集上将Claude-3.5-Sonnet的准确性从26.4%提高到51.9%，外观评分从3.0提高到3.9，超越了之前最先进的代理系统。此外，我们的Step-GRPO训练方法将Qwen2.5-Coder-7B-Instruct的准确性从38.9%提高到45.4%，并将其外观评分从3.4提高到3.7。', 'title_zh': 'WebGen-Agent: 通过多级反馈和步骤级强化学习增强交互式网站生成'}
{'arxiv_id': 'arXiv:2509.22641', 'title': 'Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity', 'authors': 'Arkadiy Saakyan, Najoung Kim, Smaranda Muresan, Tuhin Chakrabarty', 'link': 'https://arxiv.org/abs/2509.22641', 'abstract': "N-gram novelty is widely used to evaluate language models' ability to generate text outside of their training data. More recently, it has also been adopted as a metric for measuring textual creativity. However, theoretical work on creativity suggests that this approach may be inadequate, as it does not account for creativity's dual nature: novelty (how original the text is) and appropriateness (how sensical and pragmatic it is). We investigate the relationship between this notion of creativity and n-gram novelty through 7542 expert writer annotations (n=26) of novelty, pragmaticality, and sensicality via close reading of human and AI-generated text. We find that while n-gram novelty is positively associated with expert writer-judged creativity, ~91% of top-quartile expressions by n-gram novelty are not judged as creative, cautioning against relying on n-gram novelty alone. Furthermore, unlike human-written text, higher n-gram novelty in open-source LLMs correlates with lower pragmaticality. In an exploratory study with frontier close-source models, we additionally confirm that they are less likely to produce creative expressions than humans. Using our dataset, we test whether zero-shot, few-shot, and finetuned models are able to identify creative expressions (a positive aspect of writing) and non-pragmatic ones (a negative aspect). Overall, frontier LLMs exhibit performance much higher than random but leave room for improvement, especially struggling to identify non-pragmatic expressions. We further find that LLM-as-a-Judge novelty scores from the best-performing model were predictive of expert writer preferences.", 'abstract_zh': '基于n-gram新颖性的创造性的评估：一种理论与实证的探究', 'title_zh': '死亡的创新：超越基于n-gram新颖性度量的文本创造力评价'}
{'arxiv_id': 'arXiv:2509.22638', 'title': 'Language Models Can Learn from Verbal Feedback Without Scalar Rewards', 'authors': 'Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen, Wei Lu, Tianyu Pang', 'link': 'https://arxiv.org/abs/2509.22638', 'abstract': 'LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced feedback into scalar rewards, discarding much of their richness and inducing scale imbalance. We propose treating verbal feedback as a conditioning signal. Inspired by language priors in text-to-image generation, which enable novel outputs from unseen prompts, we introduce the feedback-conditional policy (FCP). FCP learns directly from response-feedback pairs, approximating the feedback-conditional posterior through maximum likelihood training on offline data. We further develop an online bootstrapping stage where the policy generates under positive conditions and receives fresh feedback to refine itself. This reframes feedback-driven learning as conditional generation rather than reward optimization, offering a more expressive way for LLMs to directly learn from verbal feedback. Our code is available at this https URL.', 'abstract_zh': 'LLMs通常通过人类或AI反馈的RL进行训练，但这些方法通常将细腻的反馈压缩为标量奖励，损失了其丰富性并导致规模失衡。我们提出将口头反馈视为条件信号。受到文本到图像生成中语言先验的启发，能够从未见过的提示生成新型输出，我们引入了反馈条件策略（FCP）。FCP直接从响应-反馈对中学习，通过离线数据的最大似然训练近似条件后验。我们进一步开发了一个在线自助阶段，其中策略在积极条件下生成并接收新的反馈以自我完善。这将反馈驱动的学习重新框架为条件生成，而非奖励优化，为LLMs直接从口头反馈中学习提供了一种更具表现力的方式。代码可在以下网址获取。', 'title_zh': '语言模型可以从口头反馈中学习而无需标量奖励'}
{'arxiv_id': 'arXiv:2509.22637', 'title': 'Variational Reasoning for Language Models', 'authors': 'Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan Li, Liang Wang, Tianyu Pang', 'link': 'https://arxiv.org/abs/2509.22637', 'abstract': 'We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at this https URL.', 'abstract_zh': '一种将思维轨迹作为潜在变量并通过变分推断优化的变分推理框架：将证据下界扩展为多轨迹目标并提出前向-KL公式以稳定变分后验的训练', 'title_zh': '变分推理的语言模型'}
{'arxiv_id': 'arXiv:2509.22633', 'title': 'Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback', 'authors': 'Gen Li, Yuling Yan', 'link': 'https://arxiv.org/abs/2509.22633', 'abstract': 'Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward model and the policy in a data-efficient manner. By examining existing optimism-based exploration algorithms, we identify a drawback in their sampling protocol: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences, and we prove lower bounds showing that such methods can incur linear regret over exponentially long horizons. Motivated by this insight, we propose a new exploration scheme that directs preference queries toward reducing uncertainty in reward differences most relevant to policy improvement. Under a multi-armed bandit model of RLHF, we establish regret bounds of order $T^{(\\beta+1)/(\\beta+2)}$, where $\\beta>0$ is a hyperparameter that balances reward maximization against mitigating distribution shift. To our knowledge, this is the first online RLHF algorithm with regret scaling polynomially in all model parameters.', 'abstract_zh': '基于人类反馈的强化学习（RLHF）通过从人类偏好数据中学习奖励模型，然后优化策略以偏好更受欢迎的响应，已成为使大语言模型（LLMs）与人类偏好保持一致的核心范式。在本文中，我们研究了在线RLHF的探索原则，旨在自适应地收集新的偏好数据，以在高效的数据利用方式下同时精炼奖励模型和策略。通过分析现有的乐观探索算法，我们发现其采样协议的一个缺点：它们倾向于收集未能减少奖励差异中最具信息量不确定性的方式，我们证明了这种方法在极长的时间跨度内可能会导致线性后悔。受到这一见解的启发，我们提出了一种新的探索方案，该方案将偏好查询的方向化，以减少对策略改进最相关的奖励差异的不确定性。在RLHF的multi-armed bandit模型下，我们建立了后悔界为$T^{(\\beta+1)/(\\beta+2)}$，其中$\\beta>0$是一个调整奖励最大化与缓解分布迁移之间平衡的超参数。据我们所知，这是第一个所有模型参数的后悔界呈多项式阶的在线RLHF算法。', 'title_zh': '基于人类反馈的强化学习的高效在线探索方法'}
{'arxiv_id': 'arXiv:2509.22611', 'title': 'Quantile Advantage Estimation for Entropy-Safe Reasoning', 'authors': 'Junkang Wu, Kexin Huang, Jiancan Wu, An Zhang, Xiang Wang, Xiangnan He', 'link': 'https://arxiv.org/abs/2509.22611', 'abstract': 'Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning, but training often oscillates between {entropy collapse} and {entropy explosion}. We trace both hazards to the mean baseline used in value-free RL (e.g., GRPO and DAPO), which improperly penalizes negative-advantage samples under reward outliers. We propose {Quantile Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile baseline. QAE induces a response-level, two-regime gate: on hard queries (p <= 1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it targets remaining failures. Under first-order softmax updates, we prove {two-sided entropy safety}, giving lower and upper bounds on one-step entropy change that curb explosion and prevent collapse. Empirically, this minimal modification stabilizes entropy, sparsifies credit assignment (with tuned K, roughly 80% of responses receive zero advantage), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results identify {baseline design} -- rather than token-level heuristics -- as the primary mechanism for scaling RLVR.', 'abstract_zh': '可验证奖励的强化学习（RLVR）增强LLM推理，但训练往往在“熵崩塌”和“熵爆炸”之间振荡。我们追踪这两种风险到价值自由RL（如GRPO和DAPO）中使用的均值基线，该基线在奖励异常值下不合理地惩罚负优势样本。我们提出了一种基于分位数优势估计（QAE），用组别K分位数基线替代均值。QAE诱发了一种响应级别、两阶段门控机制：对于难问题（p ≤ 1 - K），它强化罕见的成功；对于易问题（p > 1 - K），它瞄准剩余的失败。在一阶softmax更新下，我们证明了双向熵安全性，给出了熵变化的上下界，从而限制爆炸并防止崩塌。实验上，这种最小的修改稳定了熵，稀疏了奖励分配（通过调参K，约80%的响应获得零优势），并在Qwen3-8B/14B-Base上持续提高了AIME 2024/2025和AMC 2023的pass@1分数。这些结果表明，基线设计而非令牌级别启发式是RLVR扩展的主要机制。', 'title_zh': '基于熵安全推理的分位数优势估计'}
{'arxiv_id': 'arXiv:2509.22601', 'title': 'Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning', 'authors': 'Yulei Qin, Xiaoyu Tan, Zhengbao He, Gang Li, Haojia Lin, Zongyi Li, Zihan Xu, Yuchen Shi, Siqi Cai, Renting Rui, Shaofei Cai, Yuzheng Cai, Xuan Zhang, Sheng Ye, Ke Li, Xing Sun', 'link': 'https://arxiv.org/abs/2509.22601', 'abstract': 'Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence.', 'abstract_zh': '基于课程的自我模仿学习方法SPEAR：在代理经验指导下逐步平衡探索与利用', 'title_zh': '掌握要领，再信任成果：渐进探索的自主强化学习自模仿方法'}
{'arxiv_id': 'arXiv:2509.22565', 'title': 'Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation', 'authors': 'Wenyuan Chen, Fateme Nateghi Haredasht, Kameron C. Black, Francois Grolleau, Emily Alsentzer, Jonathan H. Chen, Stephen P. Ma', 'link': 'https://arxiv.org/abs/2509.22565', 'abstract': 'Asynchronous patient-clinician messaging via EHR portals is a growing source of clinician workload, prompting interest in large language models (LLMs) to assist with draft responses. However, LLM outputs may contain clinical inaccuracies, omissions, or tone mismatches, making robust evaluation essential. Our contributions are threefold: (1) we introduce a clinically grounded error ontology comprising 5 domains and 59 granular error codes, developed through inductive coding and expert adjudication; (2) we develop a retrieval-augmented evaluation pipeline (RAEC) that leverages semantically similar historical message-response pairs to improve judgment quality; and (3) we provide a two-stage prompting architecture using DSPy to enable scalable, interpretable, and hierarchical error detection. Our approach assesses the quality of drafts both in isolation and with reference to similar past message-response pairs retrieved from institutional archives. Using a two-stage DSPy pipeline, we compared baseline and reference-enhanced evaluations on over 1,500 patient messages. Retrieval context improved error identification in domains such as clinical completeness and workflow appropriateness. Human validation on 100 messages demonstrated superior agreement (concordance = 50% vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs. baseline, supporting the use of our RAEC pipeline as AI guardrails for patient messaging.', 'abstract_zh': '基于EHR portals的异步患者-临床医生消息交流是临床医生工作负荷的一个日益增长的来源，激发了对大型语言模型（LLMs）的兴趣以帮助生成草稿回应。然而，LLM输出可能包含临床不准确、遗漏或语气不符，因此需要进行稳健的评价。我们的贡献如下：（1）我们引入了一个基于临床的错误本体，涵盖5个领域和59个细粒度错误代码，通过归纳编码和专家裁定开发；（2）我们开发了一种检索增强的评价管道（RAEC），利用语义相似的历史消息-回应对提升判断质量；（3）我们提供了一种两阶段提示架构，使用DSPy使错误检测可扩展、可解释和层次化。我们的方法评估草稿的质量，既孤立地也参照从机构档案中检索到的类似过去的消息-回应对。通过两阶段DSPy管道，我们在超过1,500条患者消息上比较了基本评价和参考增强评价。检索上下文在临床完整性和流程适宜性等领域提高了错误识别能力。对100条消息的人工验证显示，上下文增强标签的协议性和性能（宏F1分数）均优于基线，支持使用我们RAEC管道作为患者消息的AI防护栏。', 'title_zh': '基于检索增强的AI生成患者门户消息的护栏：错误分类学构建与大规模评估'}
{'arxiv_id': 'arXiv:2509.22536', 'title': 'InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models', 'authors': 'Wenjun Wang, Shuo Cai, Congkai Xie, Mingfa Feng, Yiming Zhang, Zhen Li, Kejing Yang, Ming Li, Jiannong Cao, Yuan Xie, Hongxia Yang', 'link': 'https://arxiv.org/abs/2509.22536', 'abstract': 'The immense computational cost of training Large Language Models (LLMs) presents a major barrier to innovation. While FP8 training offers a promising solution with significant theoretical efficiency gains, its widespread adoption has been hindered by the lack of a comprehensive, open-source training recipe. To bridge this gap, we introduce an end-to-end FP8 training recipe that seamlessly integrates continual pre-training and supervised fine-tuning. Our methodology employs a fine-grained, hybrid-granularity quantization strategy to maintain numerical fidelity while maximizing computational efficiency. Through extensive experiments, including the continue pre-training of models on a 160B-token corpus, we demonstrate that our recipe is not only remarkably stable but also essentially lossless, achieving performance on par with the BF16 baseline across a suite of reasoning benchmarks. Crucially, this is achieved with substantial efficiency improvements, including up to a 22% reduction in training time, a 14% decrease in peak memory usage, and a 19% increase in throughput. Our results establish FP8 as a practical and robust alternative to BF16, and we will release the accompanying code to further democratize large-scale model training.', 'abstract_zh': '大规模语言模型（LLMs）训练的巨大计算成本是创新的主要障碍。虽然FP8训练以其显著的理论效率提升提供了有希望的解决方案，但由于缺乏全面的开源训练食谱，其广泛应用受到了阻碍。为解决这一问题，我们介绍了一种端到端的FP8训练食谱，该食谱无缝地结合了持续预训练和监督微调。我们的方法采用精细的混合粒度量化策略，在保持数值保真度的同时最大化计算效率。通过广泛的实验，包括在160亿标记语料上继续预训练模型，我们证明我们的食谱不仅非常稳定，而且几乎是无损的，在一系列推理基准测试中达到与BF16基线相当的性能。关键的是，这在提高效率方面取得了显著进展，包括训练时间减少高达22%，峰值内存使用量降低14%，吞吐量提高19%。我们的结果确立了FP8作为一种实用且可靠的BF16替代方案的地位，并将发布相应的代码以进一步促进大规模模型训练的民主化。', 'title_zh': 'InfiR2: 一种增强推理的语言模型的全面FP8训练食谱'}
{'arxiv_id': 'arXiv:2509.22483', 'title': 'OFMU: Optimization-Driven Framework for Machine Unlearning', 'authors': 'Sadia Asif, Mohammad Mohammadi Amiri', 'link': 'https://arxiv.org/abs/2509.22483', 'abstract': 'Large language models deployed in sensitive applications increasingly require the ability to unlearn specific knowledge, such as user requests, copyrighted materials, or outdated information, without retraining from scratch to ensure regulatory compliance, user privacy, and safety. This task, known as machine unlearning, aims to remove the influence of targeted data (forgetting) while maintaining performance on the remaining data (retention). A common approach is to formulate this as a multi-objective problem and reduce it to a single-objective problem via scalarization, where forgetting and retention losses are combined using a weighted sum. However, this often results in unstable training dynamics and degraded model utility due to conflicting gradient directions. To address these challenges, we propose OFMU, a penalty-based bi-level optimization framework that explicitly prioritizes forgetting while preserving retention through a hierarchical structure. Our method enforces forgetting via an inner maximization step that incorporates a similarity-aware penalty to decorrelate the gradients of the forget and retention objectives, and restores utility through an outer minimization step. To ensure scalability, we develop a two-loop algorithm with provable convergence guarantees under both convex and non-convex regimes. We further provide a rigorous theoretical analysis of convergence rates and show that our approach achieves better trade-offs between forgetting efficacy and model utility compared to prior methods. Extensive experiments across vision and language benchmarks demonstrate that OFMU consistently outperforms existing unlearning methods in both forgetting efficacy and retained utility.', 'abstract_zh': '基于惩罚的分层优化框架OFMU：在保持保留性能的同时有效遗忘特定知识', 'title_zh': '基于优化驱动的机器卸学框架（OFMU）'}
{'arxiv_id': 'arXiv:2509.22480', 'title': 'Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving', 'authors': 'Hang Li, Kaiqi Yang, Yucheng Chu, Hui Liu, Jiliang Tang', 'link': 'https://arxiv.org/abs/2509.22480', 'abstract': 'Large language models (LLMs) have been widely used for problem-solving tasks. Most recent work improves their performance through supervised fine-tuning (SFT) with labeled data or reinforcement learning (RL) from task feedback. In this paper, we study a new perspective: the divergence in solutions generated by LLMs for a single problem. We show that higher solution divergence is positively related to better problem-solving abilities across various models. Based on this finding, we propose solution divergence as a novel metric that can support both SFT and RL strategies. We test this idea on three representative problem domains and find that using solution divergence consistently improves success rates. These results suggest that solution divergence is a simple but effective tool for advancing LLM training and evaluation.', 'abstract_zh': '大规模语言模型（LLMs）在问题解决任务中被广泛应用。最近的研究主要通过有监督微调（SFT）和基于任务反馈的强化学习（RL）来提高其性能。在本文中，我们从一个新的角度研究了LLMs为单一问题生成的不同解决方案之间的发散性。我们发现，解决方案的发散性与模型的问题解决能力正相关。基于这一发现，我们提出将解决方案的发散性作为一种新的度量标准，以支持SFT和RL策略。我们在三个代表性的问题领域进行了测试，并发现使用解决方案的发散性可以一致地提高成功率。这些结果表明，解决方案的发散性是一种简单而有效的工具，可用于推进LLM的训练和评估。', 'title_zh': '探索解空间发散及其对大型语言模型问题解决的影响'}
{'arxiv_id': 'arXiv:2509.22472', 'title': 'Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning', 'authors': 'Antreas Ioannou, Andreas Shiamishis, Nora Hollenstein, Nezihe Merve Gürel', 'link': 'https://arxiv.org/abs/2509.22472', 'abstract': "In an era dominated by Large Language Models (LLMs), understanding their capabilities and limitations, especially in high-stakes fields like law, is crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini, DeepSeek, and other emerging models are increasingly integrated into legal workflows, their performance in multilingual, jurisdictionally diverse, and adversarial contexts remains insufficiently explored. This work evaluates LLaMA and Gemini on multilingual legal and non-legal benchmarks, and assesses their adversarial robustness in legal tasks through character and word-level perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation. We moreover present an open-source, modular evaluation pipeline designed to support multilingual, task-diverse benchmarking of any combination of LLMs and datasets, with a particular focus on legal tasks, including classification, summarization, open questions, and general reasoning. Our findings confirm that legal tasks pose significant challenges for LLMs with accuracies often below 50% on legal reasoning benchmarks such as LEXam, compared to over 70% on general-purpose tasks like XNLI. In addition, while English generally yields more stable results, it does not always lead to higher accuracy. Prompt sensitivity and adversarial vulnerability is also shown to persist across languages. Finally, a correlation is found between the performance of a language and its syntactic similarity to English. We also observe that LLaMA is weaker than Gemini, with the latter showing an average advantage of about 24 percentage points across the same task. Despite improvements in newer LLMs, challenges remain in deploying them reliably for critical, multilingual legal applications.", 'abstract_zh': '在大型语言模型主导的时代：理解其在法律等高 stakes 领域的能力与局限性', 'title_zh': '评估大型语言模型在多语言法律推理中的局限性'}
{'arxiv_id': 'arXiv:2509.22415', 'title': 'Explaining multimodal LLMs via intra-modal token interactions', 'authors': 'Jiawei Liang, Ruoyu Chen, Xianghao Jiao, Siyuan Liang, Shiming Liu, Qunli Zhang, Zheng Hu, Xiaochun Cao', 'link': 'https://arxiv.org/abs/2509.22415', 'abstract': 'Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse vision-language tasks, yet their internal decision-making mechanisms remain insufficiently understood. Existing interpretability research has primarily focused on cross-modal attribution, identifying which image regions the model attends to during output generation. However, these approaches often overlook intra-modal dependencies. In the visual modality, attributing importance to isolated image patches ignores spatial context due to limited receptive fields, resulting in fragmented and noisy explanations. In the textual modality, reliance on preceding tokens introduces spurious activations. Failing to effectively mitigate these interference compromises attribution fidelity. To address these limitations, we propose enhancing interpretability by leveraging intra-modal interaction. For the visual branch, we introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which aggregates attributions over multi-scale inputs to dynamically adjust receptive fields, producing more holistic and spatially coherent visual explanations. For the textual branch, we propose \\textit{Activation Ranking Correlation} (ARC), which measures the relevance of contextual tokens to the current token via alignment of their top-$k$ prediction rankings. ARC leverages this relevance to suppress spurious activations from irrelevant contexts while preserving semantically coherent ones. Extensive experiments across state-of-the-art MLLMs and benchmark datasets demonstrate that our approach consistently outperforms existing interpretability methods, yielding more faithful and fine-grained explanations of model behavior.', 'abstract_zh': '多模态大语言模型（MLLMs）在各种视觉-语言任务中取得了显著成功，但其内部决策机制仍不够理解。现有的可解释性研究主要集中在跨模态归因上，识别模型在输出生成过程中关注的图像区域。然而，这些方法往往忽视了同一模态内的依赖关系。在视觉模态中，对孤立图像patches的重要性归因忽视了空间上下文，因为它们的感受野有限，导致碎片化和噪声解释。在文本模态中，依赖于前一个词令牌引入了虚假激活。未能有效缓解这些干扰削弱了归因的准确性。为了解决这些局限性，我们提出通过利用同一模态内的交互来增强可解释性。对于视觉分支，我们引入了多尺度解释聚合（MSEA），它通过对多尺度输入的归因进行聚合来动态调整感受野，产生更具整体性和空间连贯性的视觉解释。对于文本分支，我们提出了激活排名相关性（ARC），通过对其前k个预测排名进行对齐来测量上下文词对当前词的相关性。ARC 利用这种相关性抑制无关上下文的虚假激活，同时保留语义上连贯的激活。在多种先进的MLLMs和基准数据集上的广泛实验表明，我们的方法一致地优于现有的可解释性方法，提供了更忠实和精细的模型行为解释。', 'title_zh': '通过模内标记交互解释多模态LLM'}
{'arxiv_id': 'arXiv:2509.22387', 'title': 'SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly', 'authors': 'Narada Maugin, Tristan Cazenave', 'link': 'https://arxiv.org/abs/2509.22387', 'abstract': "The Counterfactual Regret Minimization (CFR) algorithm and its variants have enabled the development of pokerbots capable of beating the best human players in heads-up (1v1) cash games and competing with them in six-player formats. However, CFR's computational complexity rises exponentially with the number of players. Furthermore, in games with three or more players, following Nash equilibrium no longer guarantees a non-losing outcome. These limitations, along with others, significantly restrict the applicability of CFR to the most popular formats: tournaments. Motivated by the recent success of Large Language Models (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored to Spin & Go, a popular three-player online poker format. SpinGPT is trained in two stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions; (2) Reinforcement Learning on 270k solver-generated hands. Our results show that SpinGPT matches the solver's actions in 78% of decisions (tolerant accuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100 versus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest that LLMs could be a new way to deal with multi-player imperfect-information games like poker.", 'abstract_zh': '基于假设反事实遗憾最小化算法的SpinGPT在三人Spin & Go在线扑克中的应用探索', 'title_zh': 'SpinGPT: 一种正确的扑克玩法的大语言模型方法'}
{'arxiv_id': 'arXiv:2509.22367', 'title': "What Is The Political Content in LLMs' Pre- and Post-Training Data?", 'authors': 'Tanise Ceron, Dmitry Nikolaev, Dominik Stammbach, Debora Nozza', 'link': 'https://arxiv.org/abs/2509.22367', 'abstract': "Large language models (LLMs) are known to generate politically biased text, yet how such biases arise remains unclear. A crucial step toward answering this question is the analysis of training data, whose political content remains largely underexplored in current LLM research. To address this gap, we present in this paper an analysis of the pre- and post-training corpora of OLMO2, the largest fully open-source model released together with its complete dataset. From these corpora, we draw large random samples, automatically annotate documents for political orientation, and analyze their source domains and content. We then assess how political content in the training data correlates with models' stance on specific policy issues. Our analysis shows that left-leaning documents predominate across datasets, with pre-training corpora containing significantly more politically engaged content than post-training data. We also find that left- and right-leaning documents frame similar topics through distinct values and sources of legitimacy. Finally, the predominant stance in the training data strongly correlates with models' political biases when evaluated on policy issues. These findings underscore the need to integrate political content analysis into future data curation pipelines as well as in-depth documentation of filtering strategies for transparency.", 'abstract_zh': '大型语言模型（LLMs）已知会产生政治偏见的文本，但这些偏见是如何产生的仍不清楚。回答这一问题的关键一步是对训练数据进行分析，而当前LLM研究中训练数据的政治内容尚未充分探索。为填补这一空白，我们在本文中对与完整数据集一起发布的最大开源模型OLMO2的训练前和训练后语料库进行了分析。从这些语料库中，我们抽取了大规模随机样本，自动标注文档的政治倾向，并分析其来源领域和内容。然后，我们评估了训练数据中的政治内容与模型在特定政策问题上的立场之间的关联性。我们的分析表明，左倾文档在整个数据集中占主导地位，训练前语料库包含显著更多的政治参与内容，而训练后数据则不然。我们还发现，左倾和右倾文档通过不同的价值观和合法性来源来论述相似的话题。最终，训练数据中的主要立场在针对政策问题进行评估时与模型的政治偏见高度相关。这些发现强调了将政治内容分析整合到未来的数据编辑管道以及对过滤策略进行深入文档记录以增强透明度的重要性。', 'title_zh': 'LLMs的预训练和后训练数据中包含哪些政治内容？'}
{'arxiv_id': 'arXiv:2509.22360', 'title': 'CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models', 'authors': 'Niharika Hegde, Subarnaduti Paul, Lars Joel-Frey, Manuel Brack, Kristian Kersting, Martin Mundt, Patrick Schramowski', 'link': 'https://arxiv.org/abs/2509.22360', 'abstract': "Large language models (LLMs) excel at operating at scale by leveraging social media and various data crawled from the web. Whereas existing corpora are diverse, their frequent lack of long-term temporal structure may however limit an LLM's ability to contextualize semantic and normative evolution of language and to capture diachronic variation. To support analysis and training for the latter, we introduce CHRONOBERG, a temporally structured corpus of English book texts spanning 250 years, curated from Project Gutenberg and enriched with a variety of temporal annotations. First, the edited nature of books enables us to quantify lexical semantic change through time-sensitive Valence-Arousal-Dominance (VAD) analysis and to construct historically calibrated affective lexicons to support temporally grounded interpretation. With the lexicons at hand, we demonstrate a need for modern LLM-based tools to better situate their detection of discriminatory language and contextualization of sentiment across various time-periods. In fact, we show how language models trained sequentially on CHRONOBERG struggle to encode diachronic shifts in meaning, emphasizing the need for temporally aware training and evaluation pipelines, and positioning CHRONOBERG as a scalable resource for the study of linguistic change and temporal generalization. Disclaimer: This paper includes language and display of samples that could be offensive to readers. Open Access: Chronoberg is available publicly on HuggingFace at ( this https URL). Code is available at (this https URL).", 'abstract_zh': '大型语言模型通过利用社交媒体和从网络上爬取的各类数据，在大规模操作方面表现出色。尽管现有的语料库种类繁多，但它们常常缺乏长期的时间结构，这可能限制了大型语言模型在语义和规范性语言演变方面的上下文理解和历时变化捕捉能力。为支持这类分析和训练，我们引入了CHRONOBERG语料库，这是一个横跨250年英文书籍文本的时间结构化语料库，来源于Project Gutenberg并配备了多种时间标注。通过书籍的编辑性质，我们可以通过时间敏感的唤醒-兴奋-支配（VAD）分析来量化词汇语义变化，并构建历史校准的情感词汇集以支持时间 grounding 的解释。利用这些词汇集，我们展示了现代基于大型语言模型的工具需要更好地定位它们对歧视性语言的检测以及对不同时间时期的情感语境化。事实上，我们表明逐步训练于CHRONOBERG的语言模型难以编码语义历时变化，强调了需要时间感知的训练和评估管道，并将CHRONOBERG定位为研究语言变化和时间泛化的可扩展资源。免责声明：本文包含可能对读者具有冒犯性的语言和示例展示。开放获取：CHRONOBERG可在HuggingFace（此链接）上公开访问。代码可在（此链接）获取。', 'title_zh': 'CHRONOBERG: 捕捉基础模型中的语言演化和时间意识'}
{'arxiv_id': 'arXiv:2509.22358', 'title': 'Stochastic activations', 'authors': 'Maria Lomeli, Matthijs Douze, Gergely Szilvasy, Loic Cabannes, Jade Copet, Sainbayar Sukhbaatar, Jason Weston, Gabriel Synnaeve, Pierre-Emmanuel Mazaré, Hervé Jégou', 'link': 'https://arxiv.org/abs/2509.22358', 'abstract': 'We introduce stochastic activations. This novel strategy randomly selects between several non-linear functions in the feed-forward layer of a large language model. In particular, we choose between SILU or RELU depending on a Bernoulli draw. This strategy circumvents the optimization problem associated with RELU, namely, the constant shape for negative inputs that prevents the gradient flow. We leverage this strategy in two ways:\n(1) We use stochastic activations during pre-training and fine-tune the model with RELU, which is used at inference time to provide sparse latent vectors. This reduces the inference FLOPs and translates into a significant speedup in the CPU. Interestingly, this leads to much better results than training from scratch with the RELU activation function.\n(2) We evaluate stochastic activations for generation. This strategy performs reasonably well: it is only slightly inferior to the best deterministic non-linearity, namely SILU combined with temperature scaling. This offers an alternative to existing strategies by providing a controlled way to increase the diversity of the generated text.', 'abstract_zh': '我们引入了随机激活函数。这一新颖策略在大型语言模型的前向层中随机选择几种非线性函数。具体而言，我们在Bernoulli抽样之间选择SILU或RELU。这种方法绕过了RELU相关的优化问题，即负输入下的恒定形状，这会阻止梯度流通。我们以两种方式利用这一策略：\n(1) 在预训练阶段使用随机激活函数，并在微调时使用RELU，后者在推理时用于提供稀疏潜向量。这减少了推理FLOPs，并在CPU上实现了显著的速度提升。有趣的是，这比从头开始使用RELU激活函数进行训练的方案效果更好。\n(2) 评估随机激活函数在生成任务中的应用。该策略表现合理：它仅略微逊色于最佳确定性非线性，即SILU与温度缩放的结合。这为现有策略提供了一个增加生成文本多样性的方式。', 'title_zh': '随机激活函数'}
{'arxiv_id': 'arXiv:2509.22343', 'title': 'Transformers Can Learn Connectivity in Some Graphs but Not Others', 'authors': 'Amit Roy, Abulhair Saparov', 'link': 'https://arxiv.org/abs/2509.22343', 'abstract': 'Reasoning capability is essential to ensure the factual correctness of the responses of transformer-based Large Language Models (LLMs), and robust reasoning about transitive relations is instrumental in many settings, such as causal inference. Hence, it is essential to investigate the capability of transformers in the task of inferring transitive relations (e.g., knowing A causes B and B causes C, then A causes C). The task of inferring transitive relations is equivalent to the task of connectivity in directed graphs (e.g., knowing there is a path from A to B, and there is a path from B to C, then there is a path from A to C). Past research focused on whether transformers can learn to infer transitivity from in-context examples provided in the input prompt. However, transformers\' capability to infer transitive relations from training examples and how scaling affects the ability is unexplored. In this study, we seek to answer this question by generating directed graphs to train transformer models of varying sizes and evaluate their ability to infer transitive relations for various graph sizes. Our findings suggest that transformers are capable of learning connectivity on "grid-like\'\' directed graphs where each node can be embedded in a low-dimensional subspace, and connectivity is easily inferable from the embeddings of the nodes. We find that the dimensionality of the underlying grid graph is a strong predictor of transformers\' ability to learn the connectivity task, where higher-dimensional grid graphs pose a greater challenge than low-dimensional grid graphs. In addition, we observe that increasing the model scale leads to increasingly better generalization to infer connectivity over grid graphs. However, if the graph is not a grid graph and contains many disconnected components, transformers struggle to learn the connectivity task, especially when the number of components is large.', 'abstract_zh': '基于变压器的大型语言模型的推理能力对于确保其响应的事实正确性至关重要，而关于传递关系的稳健推理在许多场景中（如因果推理）是必不可少的。因此，研究变压器在推断传递关系任务中的能力（例如，知道A导致B，B导致C，则A导致C）是必要的。推断传递关系的任务等同于有向图中的连通性任务（例如，知道存在从A到B的路径，存在从B到C的路径，则存在从A到C的路径）。以往研究集中在变压器能否从输入提示提供的上下文示例中学习推断传递性。然而，变压器从训练示例中推断传递关系的能力及其随规模扩大的变化尚未被探索。本研究通过生成有向图来训练不同规模的变压器模型，并评估其在不同图规模下推断传递关系的能力。我们的研究结果表明，变压器能够在“网格状”有向图中学习连通性，其中每个节点可以嵌入到低维子空间中，且连通性可以从节点的嵌入中轻松推断出来。我们发现，底层网格图的维度是变压器学习连通性任务能力的一个强预测因子，高维网格图比低维网格图更具挑战性。此外，我们观察到，增加模型规模能够使其在推断网格图的连通性上表现出更好的泛化能力。然而，如果图不是网格图且包含许多不连通组件，变压器在学习连通性任务时尤其困难，尤其是在组件数量较多时。', 'title_zh': 'Transformer可以在某些图中学习连通性，但在其他图中不能。'}
{'arxiv_id': 'arXiv:2509.22338', 'title': 'Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs', 'authors': 'Felix Vossel, Till Mossakowski, Björn Gehrke', 'link': 'https://arxiv.org/abs/2509.22338', 'abstract': 'Automating the translation of natural language to first-order logic (FOL) is crucial for knowledge representation and formal methods, yet remains challenging. We present a systematic evaluation of fine-tuned LLMs for this task, comparing architectures (encoder-decoder vs. decoder-only) and training strategies. Using the MALLS and Willow datasets, we explore techniques like vocabulary extension, predicate conditioning, and multilingual training, introducing metrics for exact match, logical equivalence, and predicate alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT reasoning ability as well as symbolic systems like ccg2lambda. Key findings show: (1) predicate availability boosts performance by 15-20%, (2) T5 models surpass larger decoder-only LLMs, and (3) models generalize to unseen logical arguments (FOLIO dataset) without specific training. While structural logic translation proves robust, predicate extraction emerges as the main bottleneck.', 'abstract_zh': '自动将自然语言转换为一阶逻辑（FOL）对于知识表示和形式化方法至关重要，但仍具有挑战性。我们系统地评估了 fine-tuned 大型语言模型（LLM）在这一任务上的表现，比较了编码器-解码器架构和解码器-only 架构以及训练策略。使用 MALLS 和 Willow 数据集，我们探索了词汇扩展、谓词条件和多语言训练等技术，引入了精确匹配、逻辑等价性和谓词对齐的评估指标。我们的 fine-tuned Flan-T5-XXL 在谓词列表上的准确率达到 70%，在包含 CoT 原理性思维能力及符号系统（如 ccg2lambda）的表现上优于 GPT-4o 和 DeepSeek-R1-0528 模型。关键发现包括：（1）谓词可用性可提升 15-20% 的性能，（2）T5 模型超越了更大的解码器-only LLM，（3）模型能够在未见的逻辑论证（FOLIO 数据集）上泛化，无需特定训练。尽管结构化逻辑翻译具有鲁棒性，但谓词提取成为主要瓶颈。', 'title_zh': '使用Fine-tuned大语言模型推进自然语言形式化到一阶逻辑'}
{'arxiv_id': 'arXiv:2509.22299', 'title': 'HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space', 'authors': 'Ke Li, Zheng Yang, Zhongbin Zhou, Feng Xue, Zhonglin Jiang, Wenxiao Wang', 'link': 'https://arxiv.org/abs/2509.22299', 'abstract': "Mixture-of-Experts (MoE) architectures in large language models (LLMs) deliver exceptional performance and reduced inference costs compared to dense LLMs. However, their large parameter counts result in prohibitive memory requirements, limiting practical deployment. While existing pruning methods primarily focus on expert-level pruning, this coarse granularity often leads to substantial accuracy degradation. In this work, we introduce HEAPr, a novel pruning algorithm that decomposes experts into smaller, indivisible atomic experts, enabling more precise and flexible atomic expert pruning. To measure the importance of each atomic expert, we leverage second-order information based on principles similar to Optimal Brain Surgeon (OBS) theory. To address the computational and storage challenges posed by second-order information, HEAPr exploits the inherent properties of atomic experts to transform the second-order information from expert parameters into that of atomic expert parameters, and further simplifies it to the second-order information of atomic expert outputs. This approach reduces the space complexity from $O(d^4)$, where d is the model's dimensionality, to $O(d^2)$. HEAPr requires only two forward passes and one backward pass on a small calibration set to compute the importance of atomic experts. Extensive experiments on MoE models, including DeepSeek MoE and Qwen MoE family, demonstrate that HEAPr outperforms existing expert-level pruning methods across a wide range of compression ratios and benchmarks. Specifically, HEAPr achieves nearly lossless compression at compression ratios of 20% ~ 25% in most models, while also reducing FLOPs nearly by 20%. The code can be found at \\href{this https URL}{this https URL}.", 'abstract_zh': 'MoE架构在大规模语言模型中的混合专家在保持出色性能和降低推理成本的同时，由于参数量大而导致内存需求 prohibitive，限制了其实际部署。虽然现有的剪枝方法主要集中在专家级剪枝上，但这种粗粒度往往会导致显著的准确性下降。在本工作中，我们引入了HEAPr，一种新颖的剪枝算法，它将专家分解为更小的不可分割的基本专家，从而实现更精确和灵活的基本专家剪枝。为了衡量每个基本专家的重要性，我们利用类似于Optimal Brain Surgeon (OBS)理论的第二阶信息原理。为了解决第二阶信息带来的计算和存储挑战，HEAPr 利用基本专家的固有属性，将专家参数的第二阶信息转换为基本专家参数的第二阶信息，并进一步简化为基本专家输出的第二阶信息。这种方法将空间复杂性从 \\(O(d^4)\\) 降低到 \\(O(d^2)\\)。HEAPr 仅需在小型校准集上进行两次正向传递和一次反向传递即可计算基本专家的重要性。在包括DeepSeek MoE和Qwen MoE家族在内的MoE模型上进行的广泛实验表明，HEAPr 在多种压缩比和基准测试中优于现有专家级剪枝方法。具体而言，在大多数模型中，HEAPr 可以在压缩比为20% ~ 25%时实现几乎无损压缩，同时将近乎线性地减少FLOPs 20%。代码可在 <this https URL> 找到。', 'title_zh': 'HEAPr：基于Hessian矩阵的输出空间高效原子专家剪枝'}
{'arxiv_id': 'arXiv:2509.22292', 'title': 'Jailbreaking on Text-to-Video Models via Scene Splitting Strategy', 'authors': 'Wonjun Lee, Haon Park, Doehyeon Lee, Bumsub Ham, Suhyun Kim', 'link': 'https://arxiv.org/abs/2509.22292', 'abstract': "Along with the rapid advancement of numerous Text-to-Video (T2V) models, growing concerns have emerged regarding their safety risks. While recent studies have explored vulnerabilities in models like LLMs, VLMs, and Text-to-Image (T2I) models through jailbreak attacks, T2V models remain largely unexplored, leaving a significant safety gap. To address this gap, we introduce SceneSplit, a novel black-box jailbreak method that works by fragmenting a harmful narrative into multiple scenes, each individually benign. This approach manipulates the generative output space, the abstract set of all potential video outputs for a given prompt, using the combination of scenes as a powerful constraint to guide the final outcome. While each scene individually corresponds to a wide and safe space where most outcomes are benign, their sequential combination collectively restricts this space, narrowing it to an unsafe region and significantly increasing the likelihood of generating a harmful video. This core mechanism is further enhanced through iterative scene manipulation, which bypasses the safety filter within this constrained unsafe region. Additionally, a strategy library that reuses successful attack patterns further improves the attack's overall effectiveness and robustness. To validate our method, we evaluate SceneSplit across 11 safety categories on T2V models. Our results show that it achieves a high average Attack Success Rate (ASR) of 77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly outperforming the existing baseline. Through this work, we demonstrate that current T2V safety mechanisms are vulnerable to attacks that exploit narrative structure, providing new insights for understanding and improving the safety of T2V models.", 'abstract_zh': '随着众多文本到视频（T2V）模型的迅速发展，其安全风险引起了越来越多的关注。虽然近期的研究通过监禁攻击探索了如LLMs、VLMs和文本到图像（T2I）模型等模型的漏洞，但T2V模型仍 largely unexplored，留下了一个重要的安全缺口。为解决这一缺口，我们提出了SceneSplit，这是一种新颖的黑盒监禁攻击方法，通过将有害叙述分割成多个场景，每个场景单独来看都是 benign 的。这种方法通过场景的组合对生成输出空间进行操控，即给定提示下所有潜在视频输出的抽象集合，利用这种场景组合作为强大的约束来引导最终结果。虽然每个场景单独来说对应一个广泛且安全的空间，其中大多数结果是 benign 的，但它们的顺序组合却将这个空间集中到了一个不安全的区域，显著增加了生成有害视频的可能性。通过迭代场景操控，此核心机制进一步增强，从而绕过了在这个受限制的不安全区域内内置的安全过滤器。此外，一个重用成功的攻击模式的策略库进一步提高了攻击的整体效果和鲁棒性。为了验证我们的方法，我们在T2V模型上按11个安全类别评估了SceneSplit。结果显示，它在Luma Ray2上的平均攻击成功率（ASR）为77.2%，在Hailuo上为84.1%，在Veo2上为78.2%，显著优于现有基线。通过这项工作，我们证明了当前的T2V安全机制易受利用叙述结构的攻击的利用，为理解并改进T2V模型的安全性提供了新的见解。', 'title_zh': '通过场景拆分策略攻破文本到视频模型'}
{'arxiv_id': 'arXiv:2509.22259', 'title': 'Wavelet-Induced Rotary Encodings: RoPE Meets Graphs', 'authors': 'Isaac Reid, Arijit Sehanobish, Cedrik Höfs, Bruno Mlodozeniec, Leonhard Vulpius, Federico Barbero, Adrian Weller, Krzysztof Choromanski, Richard E. Turner, Petar Veličković', 'link': 'https://arxiv.org/abs/2509.22259', 'abstract': 'We introduce WIRE: Wavelet-Induced Rotary Encodings. WIRE extends Rotary Position Encodings (RoPE), a popular algorithm in LLMs and ViTs, to graph-structured data. We demonstrate that WIRE is more general than RoPE, recovering the latter in the special case of grid graphs. WIRE also enjoys a host of desirable theoretical properties, including equivariance under node ordering permutation, compatibility with linear attention, and (under select assumptions) asymptotic dependence on graph resistive distance. We test WIRE on a range of synthetic and real-world tasks, including identifying monochromatic subgraphs, semantic segmentation of point clouds, and more standard graph benchmarks. We find it to be effective in settings where the underlying graph structure is important.', 'abstract_zh': 'WIRE：小波诱导旋转编码', 'title_zh': '小波诱导旋转编码：RoPE 结合图结构'}
{'arxiv_id': 'arXiv:2509.22256', 'title': 'Secure and Efficient Access Control for Computer-Use Agents via Context Space', 'authors': 'Haochen Gong, Chenxiao Li, Rui Chang, Wenbo Shen', 'link': 'https://arxiv.org/abs/2509.22256', 'abstract': "Large language model (LLM)-based computer-use agents represent a convergence of AI and OS capabilities, enabling natural language to control system- and application-level functions. However, due to LLMs' inherent uncertainty issues, granting agents control over computers poses significant security risks. When agent actions deviate from user intentions, they can cause irreversible consequences. Existing mitigation approaches, such as user confirmation and LLM-based dynamic action validation, still suffer from limitations in usability, security, and performance. To address these challenges, we propose CSAgent, a system-level, static policy-based access control framework for computer-use agents. To bridge the gap between static policy and dynamic context and user intent, CSAgent introduces intent- and context-aware policies, and provides an automated toolchain to assist developers in constructing and refining them. CSAgent enforces these policies through an optimized OS service, ensuring that agent actions can only be executed under specific user intents and contexts. CSAgent supports protecting agents that control computers through diverse interfaces, including API, CLI, and GUI. We implement and evaluate CSAgent, which successfully defends against more than 99.36% of attacks while introducing only 6.83% performance overhead.", 'abstract_zh': '基于大语言模型（LLM）的计算机使用代理：一种结合AI和OS能力的框架，但由于LLM固有的不确定性问题，授予代理对计算机的控制权存在重大的安全风险。CSAgent：一种系统级的静态策略访问控制框架，以解决这些挑战。', 'title_zh': '基于上下文空间的计算机使用代理安全高效访问控制'}
{'arxiv_id': 'arXiv:2509.22251', 'title': 'Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs', 'authors': 'Yifang Zhang, Pengfei Duan, Yiwen Yang, Shengwu Xiong', 'link': 'https://arxiv.org/abs/2509.22251', 'abstract': 'Currently, the main approach for Large Language Models (LLMs) to tackle the hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs typically treat KGs as plain text, extracting only semantic information and limiting their use of the crucial structural aspects of KGs. Another challenge is the gap between the embedding spaces of KGs encoders and LLMs text embeddings, which hinders the effective integration of structured knowledge. To overcome these obstacles, we put forward the SSKG-LLM, an innovative model architecture that is designed to efficiently integrate both the Structural and Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph Encoding (KGE) module to preserve semantics while utilizing structure. Then, the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to understand KGs embeddings. We conduct extensive experiments and provide a detailed analysis to explore how incorporating the structural information of KGs can enhance the factual reasoning abilities of LLMs. Our code are available at this https URL.', 'abstract_zh': '当前，大型语言模型（LLMs）处理幻觉问题的主要方法是引入知识图谱（KGs）。然而，LLMs通常将KGs视为普通文本，仅提取语义信息并限制了对KGs关键结构方面的使用。另一个挑战是KGs编码器的嵌入空间与LLMs文本嵌入之间的差距，这阻碍了结构化知识的有效整合。为了克服这些障碍，我们提出了SSKG-LLM，这是一种创新的模型架构，旨在高效地将KGs的结构和语义信息整合到LLMs的推理过程中。SSKG-LLM结合了知识图谱检索（KGR）模块和知识图谱编码（KGE）模块，以保留语义同时利用结构。然后，引入了知识图谱适应（KGA）模块，使LLMs能够理解KGs嵌入。我们进行了广泛的实验并进行了详细分析，以探讨整合KGs结构信息如何增强LLMs的事实推理能力。我们的代码可在以下链接获取：this https URL。', 'title_zh': '超越文本上下文：基于自适应空间对齐的结构图编码以缓解大语言模型的幻觉'}
{'arxiv_id': 'arXiv:2509.22250', 'title': 'Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance', 'authors': 'Wenbin Hu, Huihao Jing, Haochen Shi, Haoran Li, Yangqiu Song', 'link': 'https://arxiv.org/abs/2509.22250', 'abstract': 'The proliferation of Large Language Models (LLMs) has demonstrated remarkable capabilities, elevating the critical importance of LLM safety. However, existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic protection, failing to ensure safety for the nuanced and complex behaviors of modern LLM systems. To address this problem, we solve LLM safety from legal compliance perspectives, named safety compliance. In this work, we posit relevant established legal frameworks as safety standards for defining and measuring safety compliance, including the EU AI Act and GDPR, which serve as core legal frameworks for AI safety and data security in Europe. To bridge the gap between LLM safety and legal compliance, we first develop a new benchmark for safety compliance by generating realistic LLM safety scenarios seeded with legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization (GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively aligns LLMs with legal standards to mitigate safety risks. Our comprehensive experiments demonstrate that the Compliance Reasoner achieves superior performance on the new benchmark, with average improvements of +10.45% for the EU AI Act and +11.85% for GDPR.', 'abstract_zh': '大型语言模型（LLMs）的普及展示了其显著能力，突显了LLMs安全性的关键重要性。然而，现有的安全方法依赖于非正式的分类法，并缺乏严格的系统性保护，未能确保现代LLMs系统复杂而微妙的行为安全。为了解决这一问题，我们从法律合规的角度解决LLMs安全性，称为安全性合规。在本文中，我们提出相关的建立法律框架作为定义和衡量安全性合规的安全标准，包括欧盟AI法案和GDPR，这些是欧洲AI安全和数据安全的核心法律框架。为了弥合LLMs安全与法律合规之间的差距，我们首先通过生成基于法律条款的实际LLMs安全场景，开发了一个新的安全性合规基准。随后，我们使用群体策略优化（GRPO）对Qwen3-8B进行对齐，构建了一个安全性推理器，合规推理器，它有效地将LLMs与法律标准对齐以减轻安全风险。全面的实验表明，合规推理器在新基准上取得了优越的表现，欧盟AI法案的平均改进率为+10.45%，GDPR的平均改进率为+11.85%。', 'title_zh': '安全合规：通过合规视角重新思考大模型安全推理'}
{'arxiv_id': 'arXiv:2509.22237', 'title': 'FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding', 'authors': 'Haorui Chen, Chengze Li, Jia Li', 'link': 'https://arxiv.org/abs/2509.22237', 'abstract': 'The rapid advancement of Large Language Models (LLMs) has given rise to a novel software development paradigm known as "vibe coding," where users interact with coding agents through high-level natural language. However, existing evaluation benchmarks for code generation inadequately assess an agent\'s vibe coding capabilities. Existing benchmarks are misaligned, as they either require code-level specifications or focus narrowly on issue-solving, neglecting the critical scenario of feature implementation within the vibe coding paradiam. To address this gap, we propose FeatBench, a novel benchmark for vibe coding that focuses on feature implementation. Our benchmark is distinguished by several key features: 1. Pure Natural Language Prompts. Task inputs consist solely of abstract natural language descriptions, devoid of any code or structural hints. 2. A Rigorous & Evolving Data Collection Process. FeatBench is built on a multi-level filtering pipeline to ensure quality and a fully automated pipeline to evolve the benchmark, mitigating data contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass (F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent regressions. 4. Diverse Application Domains. The benchmark includes repositories from diverse domains to ensure it reflects real-world scenarios. We evaluate two state-of-the-art agent frameworks with four leading LLMs on FeatBench. Our evaluation reveals that feature implementation within the vibe coding paradigm is a significant challenge, with the highest success rate of only 29.94%. Our analysis also reveals a tendency for "aggressive implementation," a strategy that paradoxically leads to both critical failures and superior software design. We release FeatBench, our automated collection pipeline, and all experimental results to facilitate further community research.', 'abstract_zh': '大型语言模型的快速进展催生了一种新型的软件开发范式——“ vibe 编码”，用户通过高层次的自然语言与编码代理交互。然而，现有的代码生成评估基准未能充分评估代理的 vibe 编码能力。现有基准存在错位，要么需要代码级的规范，要么专注于问题解决，忽视了 vibe 编码范式下的功能实现关键场景。为填补这一空白，我们提出 FeatBench，这是一种专注于功能实现的新基准。该基准的主要特点包括：1. 纯自然语言提示。任务输入仅包含抽象自然语言描述，无任何代码或结构提示；2. 严格的且不断演变的数据收集过程。FeatBench 基于多级过滤流水线以确保数据质量，并采用自动化流程不断优化基准，避免数据污染；3. 全面的测试案例。每个任务包括失败到通过（F2P）和通过到通过（P2P）测试，以验证正确性并防止退步；4. 多样的应用领域。基准包括来自不同领域的仓库，确保其反映真实世界场景。我们使用两种最新的代理框架和四个领先的大规模语言模型在 FeatBench 上进行评估。评估结果表明，在 vibe 编码范式下的功能实现是一个重大挑战，最高成功率仅为 29.94%。此外，我们的分析还揭示了一种“激进实现”的倾向，这一策略导致了关键失败同时也产生了卓越的软件设计。我们发布了 FeatBench、自动数据收集流水线以及所有实验结果，以促进进一步的社区研究。', 'title_zh': 'FeatBench: 评估编码代理在特征实现方面的Vibe编码能力'}
{'arxiv_id': 'arXiv:2509.22225', 'title': 'Polysemous Language Gaussian Splatting via Matching-based Mask Lifting', 'authors': 'Jiayu Ding, Xinpeng Liu, Zhiyi Pan, Shiqiang Long, Ge Li', 'link': 'https://arxiv.org/abs/2509.22225', 'abstract': "Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS) scenes is a critical challenge. However, mainstream methods suffer from three key flaws: (i) their reliance on costly per-scene retraining prevents plug-and-play application; (ii) their restrictive monosemous design fails to represent complex, multi-concept semantics; and (iii) their vulnerability to cross-view semantic inconsistencies corrupts the final semantic representation. To overcome these limitations, we introduce MUSplat, a training-free framework that abandons feature optimization entirely. Leveraging a pre-trained 2D segmentation model, our pipeline generates and lifts multi-granularity 2D masks into 3D, where we estimate a foreground probability for each Gaussian point to form initial object groups. We then optimize the ambiguous boundaries of these initial groups using semantic entropy and geometric opacity. Subsequently, by interpreting the object's appearance across its most representative viewpoints, a Vision-Language Model (VLM) distills robust textual features that reconciles visual inconsistencies, enabling open-vocabulary querying via semantic matching. By eliminating the costly per-scene training process, MUSplat reduces scene adaptation time from hours to mere minutes. On benchmark tasks for open-vocabulary 3D object selection and semantic segmentation, MUSplat outperforms established training-based frameworks while simultaneously addressing their monosemous limitations.", 'abstract_zh': '将2D开放词汇理解提升到3D高斯点渲染(3DGS)场景中的挑战是关键性的。然而，主流方法存在三个关键缺陷：（i）其依赖于昂贵的逐场景重新训练，妨碍了即插即用的应用；（ii）其限制性的单义设计无法表示复杂的多概念语义；（iii）其对跨视角语义不一致的脆弱性破坏了最终的语义表示。为克服这些限制，我们引入了MUSplat，一个无需训练的框架，完全放弃了特征优化。利用预训练的2D分割模型，我们的流水线生成并提升多粒度的2D掩码到3D，为每个高斯点估计前景概率以形成初始对象组。然后，我们使用语义熵和几何透明度优化这些初始组的含糊边界。通过解释对象在其最具代表性的视角下的外观，一个视觉-语言模型（VLM）提炼出稳健的文本特征，以解决视觉不一致性，通过语义匹配实现开放词汇查询。通过消除逐场景的训练过程，MUSplat将场景适应时间从数小时缩短到仅仅几分钟。在开放词汇3D对象选择和语义分割基准任务中，MUSplat超越了现有的基于训练的框架，同时解决了它们的单义限制。', 'title_zh': '基于匹配驱动的掩码提升的多义语 Gaussian 表面化'}
{'arxiv_id': 'arXiv:2509.22224', 'title': 'Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data', 'authors': 'Zishan Ahmad, Saisubramaniam Gopalakrishnan', 'link': 'https://arxiv.org/abs/2509.22224', 'abstract': 'Large Language Models (LLMs), despite their remarkable capabilities, rely on singular, pre-dominant reasoning paradigms, hindering their performance on intricate problems that demand diverse cognitive strategies. To address this, we introduce Composite Reasoning (CR), a novel reasoning approach empowering LLMs to dynamically explore and combine multiple reasoning styles like deductive, inductive, and abductive for more nuanced problem-solving. Evaluated on scientific and medical question-answering benchmarks, our approach outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while demonstrating superior sample efficiency and adequate token usage. Notably, CR adaptively emphasizes domain-appropriate reasoning styles. It prioritizes abductive and deductive reasoning for medical question answering, but shifts to causal, deductive, and inductive methods for scientific reasoning. Our findings highlight that by cultivating internal reasoning style diversity, LLMs acquire more robust, adaptive, and efficient problem-solving abilities.', 'abstract_zh': '大型语言模型（LLMs）尽管具备显著的能力，但仍依赖单一的推理范式，这限制了它们在需要多种认知策略的复杂问题上的表现。为解决这一问题，我们引入了一种新型的推理方法——复合推理（CR），该方法使LLMs能够动态探索和结合多种推理风格（如演绎、归纳和 abduction推理），以实现更细致的问题解决。在科学和医学领域的问答基准测试中，该方法不仅超越了现有的基线方法（如思维链方法Chain-of-Thought, CoT），还超过了DeepSeek-R1样式推理（SR）的准确性和效率，同时展示了更好的样本效率和恰当的token使用。值得注意的是，CR能够适应性地强调不同的推理风格。对于医学问答，它倾向于使用演绎和归纳推理，但在科学推理中，则侧重因果、演绎和归纳方法。我们的研究结果表明，通过培养内部推理风格的多样性，LLMs能够获得更为稳健、适应性强和高效的解决问题能力。', 'title_zh': '多模式思考：综合推理如何在有限数据下提升大型语言模型性能'}
{'arxiv_id': 'arXiv:2509.22218', 'title': 'VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture', 'authors': 'Sandaru Fernando, Imasha Jayarathne, Sithumini Abeysekara, Shanuja Sithamparanthan, Thushari Silva, Deshan Jayawardana', 'link': 'https://arxiv.org/abs/2509.22218', 'abstract': 'Data visualization is essential for interpreting complex datasets, yet traditional tools often require technical expertise, limiting accessibility. VizGen is an AI-assisted graph generation system that empowers users to create meaningful visualizations using natural language. Leveraging advanced NLP and LLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries into SQL and recommends suitable graph types. Built on a multi-agent architecture, VizGen handles SQL generation, graph creation, customization, and insight extraction. Beyond visualization, it analyzes data for patterns, anomalies, and correlations, and enhances user understanding by providing explanations enriched with contextual information gathered from the internet. The system supports real-time interaction with SQL databases and allows conversational graph refinement, making data analysis intuitive and accessible. VizGen democratizes data visualization by bridging the gap between technical complexity and user-friendly design.', 'abstract_zh': 'VizGen：基于AI辅助的图形生成系统， democratizing data visualization by bridging technical complexity and user-friendly design。', 'title_zh': 'VizGen: 通过多智能体AI架构从自然语言进行数据探索与可视化'}
{'arxiv_id': 'arXiv:2509.22211', 'title': 'Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation', 'authors': 'Tiago Fernandes Tavares', 'link': 'https://arxiv.org/abs/2509.22211', 'abstract': "Unsupervised analysis of text corpora is challenging, especially in data-scarce domains where traditional topic models struggle. While these models offer a solution, they typically describe clusters with lists of keywords that require significant manual effort to interpret and often lack semantic coherence. To address this critical interpretability gap, we introduce Recursive Thematic Partitioning (RTP), a novel framework that leverages Large Language Models (LLMs) to interactively build a binary tree. Each node in the tree is a natural language question that semantically partitions the data, resulting in a fully interpretable taxonomy where the logic of each cluster is explicit. Our experiments demonstrate that RTP's question-driven hierarchy is more interpretable than the keyword-based topics from a strong baseline like BERTopic. Furthermore, we establish the quantitative utility of these clusters by showing they serve as powerful features in downstream classification tasks, particularly when the data's underlying themes correlate with the task labels. RTP introduces a new paradigm for data exploration, shifting the focus from statistical pattern discovery to knowledge-driven thematic analysis. Furthermore, we demonstrate that the thematic paths from the RTP tree can serve as structured, controllable prompts for generative models. This transforms our analytical framework into a powerful tool for synthesis, enabling the consistent imitation of specific characteristics discovered in the source corpus.", 'abstract_zh': '无监督文本语料库分析在数据稀少领域尤为具有挑战性，传统主题模型往往表现不佳。尽管这些模型提供了解决方案，但它们通常使用关键词列表描述簇，这需要大量人工努力进行解释，并且往往缺乏语义连贯性。为了解决这一关键的解释性缺口，我们提出了递归主题分割（RTP）这一新颖框架，该框架利用大规模语言模型（LLMs）实现交互式构建二叉树。树中的每个节点是一个自然语言问题，从语义上对数据进行分割，从而得到一个完全可解释的分类体系，其中每个簇的逻辑明确。实验结果表明，RTP的问题导向层次结构比强基线模型（如BERTopic）的基于关键词的主题更具解释性。此外，我们通过证明这些簇在下游分类任务中作为强大特征的应用价值，确立了它们的定量实用价值，特别是在数据的基本主题与任务标签相关的情况下。RTP引入了一种新的数据探索范式，将关注点从统计模式发现转向基于知识的主题分析。此外，我们展示了RTP树中的主题路径可以作为结构化的、可控的提示用于生成模型，这将我们的分析框架转变为一种强大的合成工具，能够一致地模仿源语料库中发现的特定特征。', 'title_zh': '基于问题驱动的分析与合成：使用大语言模型构建可解释的主题树以进行文本聚类和可控生成'}
{'arxiv_id': 'arXiv:2509.22206', 'title': 'The Outputs of Large Language Models are Meaningless', 'authors': 'Anandi Hattiangadi, Anders J. Schoubye', 'link': 'https://arxiv.org/abs/2509.22206', 'abstract': "In this paper, we offer a simple argument for the conclusion that the outputs of large language models (LLMs) are meaningless. Our argument is based on two key premises: (a) that certain kinds of intentions are needed in order for LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have the right kinds of intentions. We defend this argument from various types of responses, for example, the semantic externalist argument that deference can be assumed to take the place of intentions and the semantic internalist argument that meanings can be defined purely in terms of intrinsic relations between concepts, such as conceptual roles. We conclude the paper by discussing why, even if our argument is sound, the outputs of LLMs nevertheless seem meaningful and can be used to acquire true beliefs and even knowledge.", 'abstract_zh': '本文提出一个简单的论据，认为大型语言模型（LLM）的输出缺乏实际意义。我们的论据基于两个关键前提：(a) 需要某些类型的意图才能使LLM的输出具有字面意义，(b) LLMs不可能具有正确类型的意图。我们从各种类型的回应中辩护这一论据，例如语义外部主义的论据认为可以假设依赖代替意图，以及语义内部主义的论据认为意义可以纯粹通过概念之间的内在关系，如概念角色，来定义。我们在论文结尾讨论即使我们的论据成立，LLM的输出似乎仍然有意义，并且可以用来获得真信念乃至知识。', 'title_zh': '大型语言模型的输出毫无意义。'}
{'arxiv_id': 'arXiv:2509.22166', 'title': 'Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs', 'authors': 'Shirin Alanova, Kristina Kazistova, Ekaterina Galaeva, Alina Kostromina, Vladimir Smirnov, Redko Dmitry, Alexey Dontsov, Maxim Zhelnin, Evgeny Burnaev, Egor Shvetsov', 'link': 'https://arxiv.org/abs/2509.22166', 'abstract': "The demand for efficient large language model (LLM) inference has intensified the focus on sparsification techniques. While semi-structured (N:M) pruning is well-established for weights, its application to activation pruning remains underexplored despite its potential for dynamic, input-adaptive compression and reductions in I/O overhead. This work presents a comprehensive analysis of methods for post-training N:M activation pruning in LLMs. Across multiple LLMs, we demonstrate that pruning activations enables superior preservation of generative capabilities compared to weight pruning at equivalent sparsity levels. We evaluate lightweight, plug-and-play error mitigation techniques and pruning criteria, establishing strong hardware-friendly baselines that require minimal calibration. Furthermore, we explore sparsity patterns beyond NVIDIA's standard 2:4, showing that the 16:32 pattern achieves performance nearly on par with unstructured sparsity. However, considering the trade-off between flexibility and hardware implementation complexity, we focus on the 8:16 pattern as a superior candidate. Our findings provide both effective practical methods for activation pruning and a motivation for future hardware to support more flexible sparsity patterns. Our code is available this https URL .", 'abstract_zh': '高效的大型语言模型（LLM）推理需求促使对稀疏化技术的关注。虽然半结构化（N:M）剪枝在权重剪枝中已经被广泛研究，但在激活剪枝中的应用仍鲜有探索，尽管其在动态和输入自适应压缩以及减少I/O开销方面具有潜力。本工作全面分析了在LLM中进行N:M激活剪枝的方法。在多个LLM上，我们证明了激活剪枝在同等稀疏化水平下能更有效地保留生成能力。我们评估了轻量级、即插即用的误差缓解技术及剪枝标准，建立了强大的硬件友好基准，需要最少的校准。此外，我们探索了超越NVIDIA标准2:4的稀疏模式，显示16:32模式的性能几乎与无结构稀疏模式相匹敌。然而，考虑到灵活性与硬件实现复杂性的权衡，我们将注意力集中于16:32模式作为更优的候选。本研究提供了有效的实际激活剪枝方法，并为未来支持更多灵活稀疏模式的硬件提供了动机。我们的代码见此链接。', 'title_zh': '训练后LLM中N:M激活稀疏性的轻量级错误缓解策略'}
{'arxiv_id': 'arXiv:2509.22134', 'title': 'Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding', 'authors': 'Shijing Hu, Jingyang Li, Zhihui Lu, Pan Zhou', 'link': 'https://arxiv.org/abs/2509.22134', 'abstract': 'Speculative decoding accelerates large language model (LLM) inference by letting a lightweight draft model propose multiple tokens that the target model verifies in parallel. Yet existing training objectives optimize only a single greedy draft path, while decoding follows a tree policy that re-ranks and verifies multiple branches. This draft policy misalignment limits achievable speedups. We introduce Group Tree Optimization (GTO), which aligns training with the decoding-time tree policy through two components: (i) Draft Tree Reward, a sampling-free objective equal to the expected acceptance length of the draft tree under the target model, directly measuring decoding performance; (ii) Group-based Draft Policy Training, a stable optimization scheme that contrasts trees from the current and a frozen reference draft model, forming debiased group-standardized advantages and applying a PPO-style surrogate along the longest accepted sequence for robust updates. We further prove that increasing our Draft Tree Reward provably improves acceptance length and speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B, DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By bridging draft policy misalignment, GTO offers a practical, general solution for efficient LLM inference.', 'abstract_zh': 'Group Tree Optimization加速大型语言模型推断的方法', 'title_zh': '草案策略不对齐的桥梁： speculative 解码的组树优化'}
{'arxiv_id': 'arXiv:2509.22131', 'title': 'R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning', 'authors': 'Hongyu Shan, Mingyang Song, Chang Dai, Di Liang, Han Chen', 'link': 'https://arxiv.org/abs/2509.22131', 'abstract': "Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle complex reasoning by eliciting explicit step-by-step rationales. However, CoT's verbosity increases latency and memory usage and may propagate early errors across long chains. We propose the Reasoning Capsule (R-Capsule), a framework that aims to combine the efficiency of latent reasoning with the transparency of explicit CoT. The core idea is to compress the high-level plan into a small set of learned latent tokens (a Reasoning Capsule) while keeping execution steps lightweight or explicit. This hybrid approach is inspired by the Information Bottleneck (IB) principle, where we encourage the capsule to be approximately minimal yet sufficient for the task. Minimality is encouraged via a low-capacity bottleneck, which helps improve efficiency. Sufficiency is encouraged via a dual objective: a primary task loss for answer accuracy and an auxiliary plan-reconstruction loss that encourages the capsule to faithfully represent the original textual plan. The reconstruction objective helps ground the latent space, thereby improving interpretability and reducing the use of uninformative shortcuts. Our framework strikes a balance between efficiency, accuracy, and interpretability, thereby reducing the visible token footprint of reasoning while maintaining or improving accuracy on complex benchmarks. Our codes are available at: this https URL", 'abstract_zh': 'Reasoning Capsule (R-Capsule): Combining Efficiency with Transparency in Latent Reasoning', 'title_zh': 'R-胶囊：压缩高级规划以实现高效大型语言模型推理'}
{'arxiv_id': 'arXiv:2509.22130', 'title': 'Multi-Agent Path Finding via Offline RL and LLM Collaboration', 'authors': 'Merve Atasever, Matthew Hong, Mihir Nitin Kulkarni, Qingpei Li, Jyotirmoy V. Deshmukh', 'link': 'https://arxiv.org/abs/2509.22130', 'abstract': 'Multi-Agent Path Finding (MAPF) poses a significant and challenging problem critical for applications in robotics and logistics, particularly due to its combinatorial complexity and the partial observability inherent in realistic environments. Decentralized reinforcement learning methods commonly encounter two substantial difficulties: first, they often yield self-centered behaviors among agents, resulting in frequent collisions, and second, their reliance on complex communication modules leads to prolonged training times, sometimes spanning weeks. To address these challenges, we propose an efficient decentralized planning framework based on the Decision Transformer (DT), uniquely leveraging offline reinforcement learning to substantially reduce training durations from weeks to mere hours. Crucially, our approach effectively handles long-horizon credit assignment and significantly improves performance in scenarios with sparse and delayed rewards. Furthermore, to overcome adaptability limitations inherent in standard RL methods under dynamic environmental changes, we integrate a large language model (GPT-4o) to dynamically guide agent policies. Extensive experiments in both static and dynamically changing environments demonstrate that our DT-based approach, augmented briefly by GPT-4o, significantly enhances adaptability and performance.', 'abstract_zh': '基于决策变换器的多智能体路径规划高效去中心化规划框架：结合GPT-4o动态指导代理策略以提高适应性和性能', 'title_zh': '多智能体路径规划 via 离线强化学习和大语言模型合作'}
{'arxiv_id': 'arXiv:2509.22119', 'title': 'Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM', 'authors': 'Xiao Chi, Wenlin Zhong, Yiquan Wu, Wei Wang, Kun Kuang, Fei Wu, Minghui Xiong', 'link': 'https://arxiv.org/abs/2509.22119', 'abstract': 'Legal Article Prediction (LAP) is a critical task in legal text classification, leveraging natural language processing (NLP) techniques to automatically predict relevant legal articles based on the fact descriptions of cases. As a foundational step in legal decision-making, LAP plays a pivotal role in determining subsequent judgments, such as charges and penalties. Despite its importance, existing methods face significant challenges in addressing the complexities of LAP. Supervised classification models (SCMs), such as CNN and BERT, struggle to fully capture intricate fact patterns due to their inherent limitations. Conversely, large language models (LLMs), while excelling in generative tasks, perform suboptimally in predictive scenarios due to the abstract and ID-based nature of legal articles. Furthermore, the diversity of legal systems across jurisdictions exacerbates the issue, as most approaches are tailored to specific countries and lack broader applicability. To address these limitations, we propose Uni-LAP, a universal framework for legal article prediction that integrates the strengths of SCMs and LLMs through tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel Top-K loss function to generate accurate candidate articles, while the LLM employs syllogism-inspired reasoning to refine the final predictions. We evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical results demonstrate that our approach consistently outperforms existing baselines, showcasing its effectiveness and generalizability.', 'abstract_zh': '法律文章预测（LAP）是法律文本分类中的一个关键任务，通过自然语言处理（NLP）技术，根据案件事实描述自动预测相关的法律文章。作为法律决策的基础步骤，LAP在确定后续判决（如指控和处罚）方面起着核心作用。尽管其重要性不言而喻，但现有方法在解决LAP的复杂性方面仍面临重大挑战。监督分类模型（SCMs），如CNN和BERT，由于其固有限制，难以全面捕获复杂的事实模式。相反，虽然大型语言模型（LLMs）在生成任务中表现出色，但在预测场景中表现不佳，因为法律文章具有抽象和ID基础的性质。此外，不同司法辖区法律制度的多样性进一步加剧了这一问题，大多数方法都针对特定国家进行了定制，缺乏更广泛的适用性。为了解决这些问题，我们提出了一种称为Uni-LAP的通用法律文章预测框架，该框架通过紧密协作将SCMs和LLMs的优势结合起来。具体而言，在Uni-LAP中，SCM通过引入新颖的Top-K损失函数来生成准确的候选法律文章，而LLM则采用以三段论为基础的推理来细化最终预测。我们在多个司法辖区的数据集上评估了Uni-LAP，实验证明我们的方法在所有基准之上表现出色，展示了其有效性和泛化能力。', 'title_zh': '基于监督分类模型与大语言模型紧密合作的通用法律文章预测'}
{'arxiv_id': 'arXiv:2509.22102', 'title': 'Reinforcement Learning for Durable Algorithmic Recourse', 'authors': 'Marina Ceccon, Alessandro Fabris, Goran Radanović, Asia J. Biega, Gian Antonio Susto', 'link': 'https://arxiv.org/abs/2509.22102', 'abstract': 'Algorithmic recourse seeks to provide individuals with actionable recommendations that increase their chances of receiving favorable outcomes from automated decision systems (e.g., loan approvals). While prior research has emphasized robustness to model updates, considerably less attention has been given to the temporal dynamics of recourse--particularly in competitive, resource-constrained settings where recommendations shape future applicant pools. In this work, we present a novel time-aware framework for algorithmic recourse, explicitly modeling how candidate populations adapt in response to recommendations. Additionally, we introduce a novel reinforcement learning (RL)-based recourse algorithm that captures the evolving dynamics of the environment to generate recommendations that are both feasible and valid. We design our recommendations to be durable, supporting validity over a predefined time horizon T. This durability allows individuals to confidently reapply after taking time to implement the suggested changes. Through extensive experiments in complex simulation environments, we show that our approach substantially outperforms existing baselines, offering a superior balance between feasibility and long-term validity. Together, these results underscore the importance of incorporating temporal and behavioral dynamics into the design of practical recourse systems.', 'abstract_zh': '算法回溯寻求为个体提供可操作的建议，以增加他们从自动化决策系统（如贷款审批）中获得有利结果的机会。尽管先前的研究强调了模型更新的稳健性，但对回溯的时间动态研究却相对较少，特别是在竞争性和资源受限的环境中，推荐会影响未来申请人的池子。在本文中，我们提出了一种新的时间感知回溯框架，明确 modeling 候选人群如何根据建议进行调整。此外，我们引入了一种基于强化学习（RL）的回溯算法，以捕捉环境的演变动态，生成既可行又有效的建议。我们设计的建议具有耐久性，可在预定义的时间窗口T内保持有效。这种耐久性使个人能够在实施建议更改后有信心重新申请。通过在复杂模拟环境中的大量实验，我们表明，我们的方法在可行性和长期有效性之间的平衡上显著优于现有基准方法。这些结果强调了将时间和行为动态纳入实用回溯系统设计中的重要性。', 'title_zh': 'reinforcement learning for 持久的算法补救'}
{'arxiv_id': 'arXiv:2509.22097', 'title': 'SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios', 'authors': 'Junkai Chen, Huihui Huang, Yunbo Lyu, Junwen An, Jieke Shi, Chengran Yang, Ting Zhang, Haoye Tian, Yikun Li, Zhenhao Li, Xin Zhou, Xing Hu, David Lo', 'link': 'https://arxiv.org/abs/2509.22097', 'abstract': "Large language model (LLM) powered code agents are rapidly transforming software engineering by automating tasks such as testing, debugging, and repairing, yet the security risks of their generated code have become a critical concern. Existing benchmarks have offered valuable insights but remain insufficient: they often overlook the genuine context in which vulnerabilities were introduced or adopt narrow evaluation protocols that fail to capture either functional correctness or newly introduced vulnerabilities. We therefore introduce SecureAgentBench, a benchmark of 105 coding tasks designed to rigorously evaluate code agents' capabilities in secure code generation. Each task includes (i) realistic task settings that require multi-file edits in large repositories, (ii) aligned contexts based on real-world open-source vulnerabilities with precisely identified introduction points, and (iii) comprehensive evaluation that combines functionality testing, vulnerability checking through proof-of-concept exploits, and detection of newly introduced vulnerabilities using static analysis. We evaluate three representative agents (SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7 Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents struggle to produce secure code, as even the best-performing one, SWE-agent supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions, (ii) some agents produce functionally correct code but still introduce vulnerabilities, including new ones not previously recorded, and (iii) adding explicit security instructions for agents does not significantly improve secure coding, underscoring the need for further research. These findings establish SecureAgentBench as a rigorous benchmark for secure code generation and a step toward more reliable software development with LLMs.", 'abstract_zh': 'Large语言模型（LLM）驱动的代码代理正迅速通过自动化诸如测试、调试和修复等任务来改变软件工程，但它们生成代码的安全风险已成为一个关键问题。现有基准提供了宝贵见解但仍显不足：它们往往忽略了漏洞实际引入的背景，或者采用狭窄的评估协议，无法全面评估功能正确性或检测新引入的漏洞。因此，我们引入了SecureAgentBench，这是一个包含105个编码任务的基准，旨在严格评估代码代理在安全代码生成方面的能力。每个任务包括：（i）现实的任务设置，涉及大型代码库中的多文件编辑；（ii）基于真实世界开源漏洞的对齐背景，具有精确标识的引入点；（iii）结合功能测试、通过概念验证利用进行漏洞检查以及使用静态分析检测新引入漏洞的全面评估。我们使用三种代表性的代理（SWE-agent、OpenHands和Aider）和三种最新的LLM（Claude 3.7 Sonnet、GPT-4.1和DeepSeek-V3.1）进行评估。结果显示：（i）现有代理在生成安全代码方面存在困难，即使表现最佳的SWE-agent（支持DeepSeek-V3.1）也只是实现了15.2%的正确且安全的解决方案；（ii）一些代理能够生成功能正确的代码但仍引入漏洞，包括新的未被记录的漏洞；（iii）为代理添加显式安全指令并未显著提高安全编码能力，这凸显了进一步研究的必要性。这些发现确立了SecureAgentBench作为安全代码生成的严谨基准，并为进一步利用LLM实现更可靠软件开发奠定了基础。', 'title_zh': 'SecureAgentBench：在实际漏洞场景下评估安全代码生成的基准测试'}
{'arxiv_id': 'arXiv:2509.22067', 'title': 'The Rogue Scalpel: Activation Steering Compromises LLM Safety', 'authors': 'Anton Korznikov, Andrey Galichin, Alexey Dontsov, Oleg Y. Rogov, Ivan Oseledets, Elena Tutubalina', 'link': 'https://arxiv.org/abs/2509.22067', 'abstract': "Activation steering is a promising technique for controlling LLM behavior by adding semantically meaningful vectors directly into a model's hidden states during inference. It is often framed as a precise, interpretable, and potentially safer alternative to fine-tuning. We demonstrate the opposite: steering systematically breaks model alignment safeguards, making it comply with harmful requests. Through extensive experiments on different model families, we show that even steering in a random direction can increase the probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign features from a sparse autoencoder (SAE), a common source of interpretable directions, increases these rates by a further 2-4%. Finally, we show that combining 20 randomly sampled vectors that jailbreak a single prompt creates a universal attack, significantly increasing harmful compliance on unseen requests. These results challenge the paradigm of safety through interpretability, showing that precise control over model internals does not guarantee precise control over model behavior.", 'abstract_zh': '激活引导是一种通过在推理过程中直接向模型的隐藏状态添加语义上有意义的向量来控制LLM行为的有前景的技术。它通常被框定为一种精确、可解释且可能更安全的替代调优方法。我们展示了相反的观点：引导系统地破坏了模型对齐的安全机制，使其遵循有害请求。通过在不同的模型家族中进行广泛的实验，我们显示即使随机引导也会将有害合规性从0%提高到2%-27%。更 alarmingly，从稀疏自编码器（SAE）中提取的无害特征进一步增加了这些比率2%-4%。最后，我们展示了将20个随机采样的释放单一提示的向量组合成一个通用攻击，显着增加了未见请求的有害合规性。这些结果挑战了通过可解释性实现安全的范式，表明对模型内部的精确控制并不保证对模型行为的精确控制。', 'title_zh': 'rogue手术刀：激活 steering 影响 LLM 安全性'}
{'arxiv_id': 'arXiv:2509.22054', 'title': 'Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity', 'authors': 'Ping Chen, Xiang Liu, Zhaoxiang Liu, Zezhou Chen, Xingpeng Zhang, Huan Hu, Zipeng Wang, Kai Wang, Shuming Shi, Shiguo Lian', 'link': 'https://arxiv.org/abs/2509.22054', 'abstract': 'With the rapid advancement of large language models (LLMs), natural language processing (NLP) has achieved remarkable progress. Nonetheless, significant challenges remain in handling texts with ambiguity, polysemy, or uncertainty. We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM semantic priors with continuous fuzzy membership degrees, creating an explicit interaction between probability-based reasoning and fuzzy membership reasoning. This transition allows ambiguous inputs to be gradually transformed into clear and interpretable decisions while capturing conflicting or uncertain signals that traditional probability-based methods cannot. We validate FRC on sentiment analysis tasks, where both theoretical analysis and empirical results show that it ensures stable reasoning and facilitates knowledge transfer across different model scales. These findings indicate that FRC provides a general mechanism for managing subtle and ambiguous expressions with improved interpretability and robustness.', 'abstract_zh': '随着大型语言模型（LLMs）的快速进步，自然语言处理（NLP）取得了显著进展。然而，在处理具有歧义、多义或不确定性的文本时，仍面临重大挑战。我们介绍了模糊推理链（FRC）框架，该框架将LLM语义先验与连续的模糊隶属度结合，创建了基于概率的推理和模糊隶属度推理之间的明确交互。这一转变使模糊输入能够逐步转化为清晰且可解释的决策，并捕捉传统基于概率的方法无法处理的矛盾或不确定性信号。我们在情感分析任务上验证了FRC，理论分析和实证结果均表明，它确保了推理的稳定性并促进了不同模型规模之间的知识转移。这些发现表明，FRC提供了一种用于管理微妙和模糊表达的一般机制，具有增强的可解释性和鲁棒性。', 'title_zh': '模糊推理链（FRC）：从模糊性到清晰性的创新推理框架'}
{'arxiv_id': 'arXiv:2509.21999', 'title': 'Black-Box Hallucination Detection via Consistency Under the Uncertain Expression', 'authors': 'Seongho Joo, Kyungmin Min, Jahyun Koo, Kyomin Jung', 'link': 'https://arxiv.org/abs/2509.21999', 'abstract': 'Despite the great advancement of Language modeling in recent days, Large Language Models (LLMs) such as GPT3 are notorious for generating non-factual responses, so-called "hallucination" problems. Existing methods for detecting and alleviating this hallucination problem require external resources or the internal state of LLMs, such as the output probability of each token. Given the LLM\'s restricted external API availability and the limited scope of external resources, there is an urgent demand to establish the Black-Box approach as the cornerstone for effective hallucination detection. In this work, we propose a simple black-box hallucination detection metric after the investigation of the behavior of LLMs under expression of uncertainty. Our comprehensive analysis reveals that LLMs generate consistent responses when they present factual responses while non-consistent responses vice versa. Based on the analysis, we propose an efficient black-box hallucination detection metric with the expression of uncertainty. The experiment demonstrates that our metric is more predictive of the factuality in model responses than baselines that use internal knowledge of LLMs.', 'abstract_zh': '尽管近年来语言模型取得了巨大进展，大型语言模型（LLMs）如GPT3known for生成不实回应，即所谓的“幻觉”问题。现有的一些检测和缓解幻觉问题的方法需要外部资源或LLMs的内部状态，比如每个token的输出概率。鉴于LLMs对外部API的限制以及外部资源的有限性，迫切需要建立黑盒方法作为有效检测幻觉的基础。在对LLMs在表达不确定性下的行为进行调查之后，我们提出了一种简单的黑盒幻觉检测指标。综合分析表明，LLMs在提供事实回应时表现出一致性，反之亦然。基于这一分析，我们提出了一种基于表达不确定性的有效黑盒幻觉检测指标。实验表明，我们的指标比使用LLMs内部知识的基线更能预测模型回应的事实性。', 'title_zh': '黑盒幻觉检测：在不确定表达下的一致性方法'}
{'arxiv_id': 'arXiv:2509.21972', 'title': 'From Superficial Outputs to Superficial Learning: Risks of Large Language Models in Education', 'authors': 'Iris Delikoura, Yi.R, Fung, Pan Hui', 'link': 'https://arxiv.org/abs/2509.21972', 'abstract': 'Large Language Models (LLMs) are transforming education by enabling personalization, feedback, and knowledge access, while also raising concerns about risks to students and learning systems. Yet empirical evidence on these risks remains fragmented. This paper presents a systematic review of 70 empirical studies across computer science, education, and psychology. Guided by four research questions, we examine: (i) which applications of LLMs in education have been most frequently explored; (ii) how researchers have measured their impact; (iii) which risks stem from such applications; and (iv) what mitigation strategies have been proposed. We find that research on LLMs clusters around three domains: operational effectiveness, personalized applications, and interactive learning tools. Across these, model-level risks include superficial understanding, bias, limited robustness, anthropomorphism, hallucinations, privacy concerns, and knowledge constraints. When learners interact with LLMs, these risks extend to cognitive and behavioural outcomes, including reduced neural activity, over-reliance, diminished independent learning skills, and a loss of student agency. To capture this progression, we propose an LLM-Risk Adapted Learning Model that illustrates how technical risks cascade through interaction and interpretation to shape educational outcomes. As the first synthesis of empirically assessed risks, this review provides a foundation for responsible, human-centred integration of LLMs in education.', 'abstract_zh': '大型语言模型（LLMs）通过实现个性化、反馈和知识访问正在改变教育，同时也引发了对学生和学习系统风险的关注。然而，关于这些风险的实证证据仍然支离破碎。本论文对来自计算机科学、教育和心理学领域的70项实证研究进行了系统综述。根据四个研究问题，我们探讨了：（i）教育中哪些大型语言模型的应用最为频繁；（ii）研究人员如何衡量其影响；（iii）这些应用带来的哪些风险；以及（iv）提出的哪些缓解策略。我们发现，关于大型语言模型的研究主要集中在三个领域：操作有效性、个性化应用和互动学习工具。在这三个领域中，模型层面的风险包括表面理解、偏见、有限的稳健性、拟人化、幻觉、隐私担忧和知识限制。当学习者与大型语言模型互动时，这些风险延伸到认知和行为结果，包括减少的大脑活动、过度依赖、削弱的独立学习能力以及学生自主性的丧失。为了捕捉这一进展，我们提出了一种大型语言模型风险适应性学习模型，以说明技术风险如何通过互动和解释影响教育结果。作为第一个关于经过实证评估的风险的综述，本研究为负责任地在教育中集成大型语言模型提供了基础。', 'title_zh': '从表层输出到表层学习：大型语言模型在教育中的风险'}
{'arxiv_id': 'arXiv:2509.21947', 'title': 'Active Attacks: Red-teaming LLMs via Adaptive Environments', 'authors': 'Taeyoung Yun, Pierre-Luc St-Charles, Jinkyoo Park, Yoshua Bengio, Minsu Kim', 'link': 'https://arxiv.org/abs/2509.21947', 'abstract': 'We address the challenge of generating diverse attack prompts for large language models (LLMs) that elicit harmful behaviors (e.g., insults, sexual content) and are used for safety fine-tuning. Rather than relying on manual prompt engineering, attacker LLMs can be trained with reinforcement learning (RL) to automatically generate such prompts using only a toxicity classifier as a reward. However, capturing a wide range of harmful behaviors is a significant challenge that requires explicit diversity objectives. Existing diversity-seeking RL methods often collapse to limited modes: once high-reward prompts are found, exploration of new regions is discouraged. Inspired by the active learning paradigm that encourages adaptive exploration, we introduce \\textit{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its attacks as the victim evolves. By periodically safety fine-tuning the victim LLM with collected attack prompts, rewards in exploited regions diminish, which forces the attacker to seek unexplored vulnerabilities. This process naturally induces an easy-to-hard exploration curriculum, where the attacker progresses beyond easy modes toward increasingly difficult ones. As a result, Active Attacks uncovers a wide range of local attack modes step by step, and their combination achieves wide coverage of the multi-mode distribution. Active Attacks, a simple plug-and-play module that seamlessly integrates into existing RL objectives, unexpectedly outperformed prior RL-based methods -- including GFlowNets, PPO, and REINFORCE -- by improving cross-attack success rates against GFlowNets, the previous state-of-the-art, from 0.07% to 31.28% (a relative gain greater than $400\\ \\times$) with only a 6% increase in computation. Our code is publicly available \\href{this https URL}{here}.', 'abstract_zh': '我们提出了主动攻击（Active Attacks），一种基于强化学习的红队算法，该算法随着受害模型的发展而调整其攻击策略。通过定期使用收集到的攻击提示对受害的大语言模型进行安全性微调，已被利用的区域的奖励会减少，迫使攻击者寻找未探索的漏洞。这一过程自然地诱导出一个从易到难的探索课程，攻击者逐渐从简单的模式过渡到更具挑战性的模式。结果，主动攻击逐步揭示了广泛的地方攻击模式，并且这些模式的结合实现了多模式分布的广泛覆盖。主动攻击作为一个简单的即插即用模块无缝集成到现有的强化学习目标中，意外地优于之前基于强化学习的方法——包括GFlowNets、PPO和REINFORCE，仅通过将计算量增加6%，在GFlowNets这种之前最先进的方法上实现了从0.07%到31.28%（相对增益超过400倍）的跨攻击成功率提升。我们的代码已公开可在\\[这里\\]找到。', 'title_zh': '主动攻击：通过适应性环境红队演练LLMs'}
{'arxiv_id': 'arXiv:2509.21946', 'title': 'Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration', 'authors': 'Kasidit Sermsri, Teerapong Panboonyuen', 'link': 'https://arxiv.org/abs/2509.21946', 'abstract': 'Political stance detection in low-resource and culturally complex settings poses a critical challenge for large language models (LLMs). In the Thai political landscape - marked by indirect language, polarized figures, and entangled sentiment and stance - LLMs often display systematic biases such as sentiment leakage and favoritism toward entities. These biases undermine fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic calibration framework that mitigates political bias without requiring fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and rationale-based supervision to disentangle sentiment from stance and reduce bias. We also release the first high-quality Thai political stance dataset, annotated with stance, sentiment, rationales, and bias markers across diverse entities and events. Experimental results show that ThaiFACTUAL significantly reduces spurious correlations, enhances zero-shot generalization, and improves fairness across multiple LLMs. This work highlights the importance of culturally grounded debiasing techniques for underrepresented languages.', 'abstract_zh': '在低资源且文化复杂环境中检测政治立场对大型语言模型构成严峻挑战。ThaiFACTUAL：一种无需微调的轻量级、模型无关校准框架，用于缓解政治偏差。', 'title_zh': '基于反事实校准的大规模语言模型在泰国政治立场检测中的去偏见化'}
{'arxiv_id': 'arXiv:2509.21933', 'title': 'Why Chain of Thought Fails in Clinical Text Understanding', 'authors': 'Jiageng Wu, Kevin Xie, Bowen Gu, Nils Krüger, Kueiyu Joshua Lin, Jie Yang', 'link': 'https://arxiv.org/abs/2509.21933', 'abstract': 'Large language models (LLMs) are increasingly being applied to clinical care, a domain where both accuracy and transparent reasoning are critical for safe and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits step-by-step reasoning, has demonstrated improvements in performance and interpretability across a wide range of tasks. However, its effectiveness in clinical contexts remains largely unexplored, particularly in the context of electronic health records (EHRs), the primary source of clinical documentation, which are often lengthy, fragmented, and noisy. In this work, we present the first large-scale systematic study of CoT for clinical text understanding. We assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9 languages and 8 task types. Contrary to prior findings in other domains, we observe that 86.3\\% of models suffer consistent performance degradation in the CoT setting. More capable models remain relatively robust, while weaker ones suffer substantial declines. To better characterize these effects, we perform fine-grained analyses of reasoning length, medical concept alignment, and error profiles, leveraging both LLM-as-a-judge evaluation and clinical expert evaluation. Our results uncover systematic patterns in when and why CoT fails in clinical contexts, which highlight a critical paradox: CoT enhances interpretability but may undermine reliability in clinical text tasks. This work provides an empirical basis for clinical reasoning strategies of LLMs, highlighting the need for transparent and trustworthy approaches.', 'abstract_zh': '大型语言模型（LLMs）在临床护理中的应用日益增多，该领域需要高度准确和透明的推理以确保安全和可信的部署。逐步推理（CoT）提示可以引发逐步推理，在广泛任务中展示了性能和可解释性的提升。然而，其在临床环境中的有效性在很大程度上仍未被探索，特别是在电子健康记录（EHRs）的背景下，EHRs是临床记录的主要来源，往往冗长、碎片化且噪声大。在本工作中，我们进行了第一项大规模系统的CoT在临床文本理解中的研究。我们评估了95个高级LLM在87个真实世界的临床文本任务上的表现，覆盖了9种语言和8种任务类型。与在其他领域的先前发现相反，我们观察到86.3%的模型在CoT设置中表现出一致的性能下降。能力更强的模型相对较为稳健，而较弱的模型则遭受了显著的下降。为了更好地理解这些影响，我们通过LLM作为评审员评估和临床专家评估，进行了精细粒度的推理长度、医疗概念对齐及错误分析。我们的结果揭示了CoT在临床环境中的系统性失败模式，这表明一个关键的悖论：CoT虽提升了可解释性，但可能损害了临床文本任务的可靠性。本研究为LLM的临床推理策略提供了实证基础，强调了需要透明和可信的方法。', 'title_zh': '临床文本理解中链式思维为何失败'}
{'arxiv_id': 'arXiv:2509.21910', 'title': 'AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition', 'authors': 'Yun Wang, Zhaojun Ding, Xuansheng Wu, Siyue Sun, Ninghao Liu, Xiaoming Zhai', 'link': 'https://arxiv.org/abs/2509.21910', 'abstract': 'Automated scoring plays a crucial role in education by reducing the reliance on human raters, offering scalable and immediate evaluation of student work. While large language models (LLMs) have shown strong potential in this task, their use as end-to-end raters faces challenges such as low accuracy, prompt sensitivity, limited interpretability, and rubric misalignment. These issues hinder the implementation of LLM-based automated scoring in assessment practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM framework enhancing automated scoring via rubric-aligned Structured COmponent REcognition. With two agents, AutoSCORE first extracts rubric-relevant components from student responses and encodes them into a structured representation (i.e., Scoring Rubric Component Extraction Agent), which is then used to assign final scores (i.e., Scoring Agent). This design ensures that model reasoning follows a human-like grading process, enhancing interpretability and robustness. We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics, AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK, correlations), and error metrics (MAE, RMSE) compared to single-agent baselines, with particularly strong benefits on complex, multi-dimensional rubrics, and especially large relative gains on smaller LLMs. These results demonstrate that structured component recognition combined with multi-agent design offers a scalable, reliable, and interpretable solution for automated scoring.', 'abstract_zh': 'AutoSCORE：基于结构化成分识别的多代理大规模语言模型自动化评分框架', 'title_zh': 'AutoSCORE: 基于结构化组件识别的多代理大型语言模型增强自动评分'}
{'arxiv_id': 'arXiv:2509.21907', 'title': 'A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs', 'authors': 'Kemal Sami Karaca, Bahaeddin Eravcı', 'link': 'https://arxiv.org/abs/2509.21907', 'abstract': 'Understanding the qualitative intent of citations is essential for a comprehensive assessment of academic research, a task that poses unique challenges for agglutinative languages like Turkish. This paper introduces a systematic methodology and a foundational dataset to address this problem. We first present a new, publicly available dataset of Turkish citation intents, created with a purpose-built annotation tool. We then evaluate the performance of standard In-Context Learning (ICL) with Large Language Models (LLMs), demonstrating that its effectiveness is limited by inconsistent results caused by manually designed prompts. To address this core limitation, we introduce a programmable classification pipeline built on the DSPy framework, which automates prompt optimization systematically. For final classification, we employ a stacked generalization ensemble to aggregate outputs from multiple optimized models, ensuring stable and reliable predictions. This ensemble, with an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\\%. Ultimately, this study provides the Turkish NLP community and the broader academic circles with a foundational dataset and a robust classification framework paving the way for future qualitative citation studies.', 'abstract_zh': '理解引文的定性意图对于全面评估学术研究至关重要，这在像土耳其语这样的黏着语中提出了独特挑战。本文介绍了一种系统的方法和基础数据集以解决这一问题。我们首先介绍了一个新的公开可用的土耳其引文意图数据集，该数据集使用专门为注释设计的工具创建。随后，我们评估了标准上下文学习（ICL）与大规模语言模型（LLMs）的性能，显示其效果受限于由手动设计的提示引起的不一致结果。为此，我们引入了基于DSPy框架的可编程分类流水线，该流水线系统地自动化了提示优化。最终分类时，我们采用多层次泛化的集成方法汇集多个优化模型的输出，确保稳定可靠的预测。该集成方案，以XGBoost元模型为基础，实现了91.3%的先进准确率。最终，本研究为土耳其语NLP社区和更广泛的学术界提供了基础数据集和稳健的分类框架，为未来的定性引文研究铺平了道路。', 'title_zh': '使用大型语言模型在土耳其语中构建数据集并进行引文意图分类'}
{'arxiv_id': 'arXiv:2509.21884', 'title': "You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors", 'authors': 'Bochuan Cao, Changjiang Li, Yuanpu Cao, Yameng Ge, Ting Wang, Jinghui Chen', 'link': 'https://arxiv.org/abs/2509.21884', 'abstract': "Large language models (LLMs) have been widely adopted across various applications, leveraging customized system prompts for diverse tasks. Facing potential system prompt leakage risks, model developers have implemented strategies to prevent leakage, primarily by disabling LLMs from repeating their context when encountering known attack patterns. However, it remains vulnerable to new and unforeseen prompt-leaking techniques. In this paper, we first introduce a simple yet effective prompt leaking attack to reveal such risks. Our attack is capable of extracting system prompts from various LLM-based application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our findings further inspire us to search for a fundamental solution to the problems by having no system prompt in the context. To this end, we propose SysVec, a novel method that encodes system prompts as internal representation vectors rather than raw text. By doing so, SysVec minimizes the risk of unauthorized disclosure while preserving the LLM's core language capabilities. Remarkably, this approach not only enhances security but also improves the model's general instruction-following abilities. Experimental results demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves the LLM's functional integrity, and helps alleviate the forgetting issue in long-context scenarios.", 'abstract_zh': '大型语言模型（LLMs）在各种应用中得到了广泛应用，通过定制系统提示应对多样化的任务。面对可能的系统提示泄露风险，模型开发者采取了策略防止泄露，主要通过在遇到已知攻击模式时禁用LLMs重复其上下文内容。然而，这仍然对新的和未预见的提示泄露技术脆弱。在本文中，我们首先介绍了一种简单而有效的提示泄露攻击以揭示这种风险。我们的攻击可以从各种基于LLM的应用中提取系统提示，甚至从当前最先进的LLM模型如GPT-4o或Claude 3.5 Sonnet中提取。我们的发现进一步启发我们寻求从根本上解决问题的方法，即在上下文中没有系统提示。为此，我们提出了SysVec，一种新颖的方法，将系统提示编码为内部表示向量，而不是原始文本。通过这种方式，SysVec减少了未经授权泄露的风险，同时保留了LLM的核心语言能力。这一方法不仅提高了安全性，还改善了模型的通用指令跟随能力。实验结果表明，SysVec有效缓解了提示泄露攻击，保持了LLM的功能完整性，并在长上下文场景中缓解了遗忘问题。', 'title_zh': '你拿不走虚无：通过系统向量减轻LLMs的提示泄漏'}
{'arxiv_id': 'arXiv:2509.21880', 'title': 'No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping', 'authors': 'Thanh-Long V. Le, Myeongho Jeon, Kim Vu, Viet Lai, Eunho Yang', 'link': 'https://arxiv.org/abs/2509.21880', 'abstract': 'Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same reward - so-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce RL with Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR.', 'abstract_zh': '可验证回报的强化学习（RLVR）是增强大型语言模型（LLMs）推理能力的强大力量框架。然而，当前的方法如GRPO仅依赖于模型对同一输入响应正确性不同的问题，而忽视了所有响应获得相同回报的问题——称为零方差提示。在这项工作中，我们认为这些提示并非无用，事实上，它们可以为策略优化提供有意义的反馈。为此，我们引入了零方差提示下的强化学习（RL-ZVP）这一新颖算法，该算法从零方差提示中提取学习信号。RL-ZVP直接奖励正确性并惩罚错误，即使没有对比响应，也能通过标记级别特性调节反馈以保留有信息性的精细信号。在六项数学推理基准测试中，RL-ZVP在准确性和通过率上分别比GRPO提高了8.61分和7.77分，并且始终优于其他过滤掉零方差提示的基线方法。这些结果突显了在RLVR中从零方差提示中学习的未充分利用的潜力。', 'title_zh': '不留任一代言：通过熵导向优势塑造在LLM强化学习中利用零方差任一代言'}
{'arxiv_id': 'arXiv:2509.21870', 'title': 'Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations', 'authors': 'Guanzhi Deng, Mingyang Liu, Dapeng Wu, Yinqiao Li, Linqi Song', 'link': 'https://arxiv.org/abs/2509.21870', 'abstract': 'Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning method for large language models. However, its linear nature limits expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies lightweight transformations to the low-rank updates. We further introduce Sinter, a sine-based activation that adds structured perturbations without increasing parameter count. Experiments across summarization and classification tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh, highlighting the importance of activation design in lowrank tuning.', 'abstract_zh': 'LoRAN：非线性扩展的LoRA及其在低秩调优中的应用', 'title_zh': '增强低秩适应性与结构化非线性变换'}
{'arxiv_id': 'arXiv:2509.21848', 'title': 'Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration', 'authors': 'Taejong Joo, Shu Ishida, Ivan Sosnovik, Bryan Lim, Sahand Rezaei-Shoshtari, Adam Gaier, Robert Giaquinto', 'link': 'https://arxiv.org/abs/2509.21848', 'abstract': "As a model-agnostic approach to long context modeling, multi-agent systems can process inputs longer than a large language model's context window without retraining or architectural modifications. However, their performance often heavily relies on hand-crafted multi-agent collaboration strategies and prompt engineering, which limit generalizability. In this work, we introduce a principled framework that formalizes the model-agnostic long context modeling problem as a compression problem, yielding an information-theoretic compression objective. Building on this framework, we propose Graph of Agents (GoA), which dynamically constructs an input-dependent collaboration structure that maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document question answering benchmarks, GoA improves the average $F_1$ score of retrieval-augmented generation by 5.7\\% and a strong multi-agent baseline using a fixed collaboration structure by 16.35\\%, respectively. Even with only a 2K context window, GoA surpasses the 128K context window Llama 3.1 8B on LongBench, showing a dramatic increase in effective context length. Our source code is available at this https URL.", 'abstract_zh': '作为一种模型无关的方法，多智能体系统可以处理远超过大型语言模型上下文窗口长度的输入，而无需重新训练或修改架构，但其性能往往高度依赖于手工设计的多智能体协作策略和提示工程，这限制了其通用性。在这项工作中，我们引入了一个原理性的框架，将模型无关的长上下文建模问题形式化为压缩问题，从而得到信息论压缩目标。基于此框架，我们提出了智能体图（GoA），它动态构建一个输入相关的协作结构，以最大化该目标。在涵盖六种文档问答基准测试的实验中，GoA 分别将 Llama 3.1 8B 和 Qwen3 8B 的检索增强生成的平均 F1 分数提高了 5.7% 和 16.35%，并将固定协作结构的强多智能体基线超越。即使拥有仅 2K 的上下文窗口，GoA 在 LongBench 上也超过了 128K 上下文窗口的 Llama 3.1 8B，显示出有效的上下文长度显著增加。我们的源代码可在此处访问。', 'title_zh': '代理图：由新兴多代理协作实现的 principled 长上下文建模'}
{'arxiv_id': 'arXiv:2509.21840', 'title': 'Can Large Language Models Autoformalize Kinematics?', 'authors': 'Aditi Kabra, Jonathan Laurent, Sagar Bharadwaj, Ruben Martins, Stefan Mitsch, André Platzer', 'link': 'https://arxiv.org/abs/2509.21840', 'abstract': "Autonomous cyber-physical systems like robots and self-driving cars could greatly benefit from using formal methods to reason reliably about their control decisions. However, before a problem can be solved it needs to be stated. This requires writing a formal physics model of the cyber-physical system, which is a complex task that traditionally requires human expertise and becomes a bottleneck.\nThis paper experimentally studies whether Large Language Models (LLMs) can automate the formalization process. A 20 problem benchmark suite is designed drawing from undergraduate level physics kinematics problems. In each problem, the LLM is provided with a natural language description of the objects' motion and must produce a model in differential game logic (dGL). The model is (1) syntax checked and iteratively refined based on parser feedback, and (2) semantically evaluated by checking whether symbolically executing the dGL formula recovers the solution to the original physics problem. A success rate of 70% (best over 5 samples) is achieved. We analyze failing cases, identifying directions for future improvement. This provides a first quantitative baseline for LLM-based autoformalization from natural language to a hybrid games logic with continuous dynamics.", 'abstract_zh': '基于大规模语言模型的自然语言到混合动态博弈逻辑的自动化形式化研究', 'title_zh': '大型语言模型能否自动形式化运动学？'}
{'arxiv_id': 'arXiv:2509.21798', 'title': 'Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment', 'authors': 'Hongbin Zhang, Kehai Chen, Xuefeng Bai, Yang Xiang, Min Zhang', 'link': 'https://arxiv.org/abs/2509.21798', 'abstract': "Reward models (RMs) are crucial for aligning large language models (LLMs) with diverse cultures. Consequently, evaluating their cultural awareness is essential for further advancing global alignment of LLMs. However, existing RM evaluations fall short in assessing cultural awareness due to the scarcity of culturally relevant evaluation datasets. To fill this gap, we propose Cultural Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs reveals their deficiencies in modeling cultural awareness and demonstrates a positive correlation between performance on CARB and downstream multilingual cultural alignment tasks. Further analysis identifies the spurious correlations within culture-aware reward modeling, wherein RM's scoring relies predominantly on surface-level features rather than authentic cultural nuance understanding. To address these, we propose Think-as-Locals to elicit deeper culturally grounded reasoning from generative RMs via reinforcement learning from verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate preference judgments and high-quality structured evaluation criteria generation. Experimental results validate its efficacy in mitigating spurious features interference and advancing culture-aware reward modeling.", 'abstract_zh': '基于文化意识的奖励模型基准（CARB）：推动大型语言模型的全球对齐', 'title_zh': '评估并改进奖励模型在LLM对齐中文化意识的提升'}
{'arxiv_id': 'arXiv:2509.21792', 'title': 'FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning', 'authors': 'Yizhou Zhang, Ning Lv, Teng Wang, Jisheng Dang', 'link': 'https://arxiv.org/abs/2509.21792', 'abstract': 'Group relative policy optimization (GRPO) has demonstrated significant potential in improving the reasoning capabilities of large language models (LLMs) via reinforcement learning. However, its practical deployment is impeded by an excessively slow training process, primarily attributed to the computationally intensive autoregressive generation of multiple responses per query, which makes the generation phase the primary performance bottleneck. Although speculative decoding presents a promising direction for acceleration, its direct application in GRPO achieves limited speedup under high-concurrency training conditions. To overcome this limitation, we propose a concurrency-aware speculative decoding framework that dynamically adjusts the drafting and verification strategy according to real-time concurrency levels, thereby maximizing the acceleration of the generation process. Furthermore, to address performance degradation arising from distributional drift between the evolving target model and the fixed draft model during training, we introduce an online draft learning mechanism that enables the draft model to continuously adapt using feedback signals from the target model. Experimental results across multiple mathematical reasoning datasets and models demonstrate that the proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly surpassing baseline approaches in efficiency. The code is available at this https URL.', 'abstract_zh': 'Group相对策略优化（GRPO）通过强化学习在提高大型语言模型（LLMs）的推理能力方面展现了显著潜力。然而，其实际部署受到训练过程异常缓慢的阻碍，主要原因在于每个查询生成多个响应的计算密集型自回归生成，使得生成阶段成为主要的性能瓶颈。尽管预测解码为加速提供了有前景的方向，但在高并发训练条件下，其直接应用仅能实现有限的加速效果。为克服这一限制，我们提出了一种意识并发的预测解码框架，该框架根据实时的并发水平动态调整起草和验证策略，从而最大化生成过程的加速效果。此外，为解决训练过程中目标模型和固定草稿模型之间分布漂移导致的性能下降问题，我们引入了一种在线草稿学习机制，使草稿模型能够通过来自目标模型的反馈信号不断自我适应。在多个数学推理数据集和模型上的实验结果表明，所提出的方法实现了2.35倍至2.72倍的端到端速度提升，显著优于基线方法。相关代码可在以下链接获取。', 'title_zh': 'FastGRPO：通过 Awareness-aware 推测解码和在线草图学习加速策略优化'}
{'arxiv_id': 'arXiv:2509.21761', 'title': 'Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models', 'authors': 'Miao Yu, Zhenhong Zhou, Moayad Aloqaily, Kun Wang, Biwei Huang, Stephen Wang, Yueming Jin, Qingsong Wen', 'link': 'https://arxiv.org/abs/2509.21761', 'abstract': 'Fine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks through data poisoning, yet the internal mechanisms governing these attacks remain a black box. Previous research on interpretability for LLM safety tends to focus on alignment, jailbreak, and hallucination, but overlooks backdoor mechanisms, making it difficult to understand and fully eliminate the backdoor threat. In this paper, aiming to bridge this gap, we explore the interpretable mechanisms of LLM backdoors through Backdoor Attribution (BkdAttr), a tripartite causal analysis framework. We first introduce the Backdoor Probe that proves the existence of learnable backdoor features encoded within the representations. Building on this insight, we further develop Backdoor Attention Head Attribution (BAHA), efficiently pinpointing the specific attention heads responsible for processing these features. Our primary experiments reveals these heads are relatively sparse; ablating a minimal \\textbf{$\\sim$ 3%} of total heads is sufficient to reduce the Attack Success Rate (ASR) by \\textbf{over 90%}. More importantly, we further employ these findings to construct the Backdoor Vector derived from these attributed heads as a master controller for the backdoor. Through only \\textbf{1-point} intervention on \\textbf{single} representation, the vector can either boost ASR up to \\textbf{$\\sim$ 100% ($\\uparrow$)} on clean inputs, or completely neutralize backdoor, suppressing ASR down to \\textbf{$\\sim$ 0% ($\\downarrow$)} on triggered inputs. In conclusion, our work pioneers the exploration of mechanistic interpretability in LLM backdoors, demonstrating a powerful method for backdoor control and revealing actionable insights for the community.', 'abstract_zh': '细调的大语言模型（LLMs）通过数据注入攻击具有脆弱性，但其内部机制仍是一个黑盒。 previous research on interpretability for LLM safety tends to focus on alignment, jailbreak, and hallucination, but overlooks backdoor mechanisms, making it difficult to understand and fully eliminate the backdoor threat. 为此，本文通过Backdoor Attribution（BkdAttr）三部分因果分析框架探索LLM后门的可解释机制。我们首先介绍Backdoor Probe，证明了可学习的后门特征被编码在表示之中。基于这一洞察，我们进一步开发了Backdoor Attention Head Attribution（BAHA），高效地确定处理这些特征的具体注意力头。我们的主要实验证明这些头相对稀疏；删除总头数的最小约3%就足以将攻击成功率为ASR降低超过90%。更重要的是，我们进一步利用这些发现构造出由这些归因头导出的Backdoor向量作为后门的主控制器。通过仅在单个表示上进行一点干预，向量可以在干净输入上将ASR提高到约100%（↑），或完全消除后门，在触发输入上将ASR降低到约0%（↓）。总之，我们的工作开创了LLM后门机制可解释性的研究，展示了后门控制的有力方法，并为社区提供了可操作性的见解。', 'title_zh': '后门归因：阐明和控制语言模型中的后门攻击'}
{'arxiv_id': 'arXiv:2509.21740', 'title': 'Self-Speculative Biased Decoding for Faster Live Translation', 'authors': 'Linxiao Zeng, Haoyun Deng, Kangyuan Shu, Shizhen Wang', 'link': 'https://arxiv.org/abs/2509.21740', 'abstract': 'Large Language Models (LLMs) have recently demonstrated impressive capabilities in various text generation tasks. However, it remains challenging to use them off-the-shelf in streaming applications (such as live translation), where the output must continually update as the input context expands, while still maintaining a reasonable computational cost to meet the latency requirement.\nIn this work, we reexamine the re-translation approach to simultaneous translation and propose Self-Speculative Biased Decoding, a novel inference paradigm designed to avoid repeatedly generating output from scratch for a consistently growing input stream. We propose using the most recent output as a draft for the current growing input context. During the verification stage, the output will be biased towards the draft token for a higher draft acceptance rate. This strategy not only minimizes flickering that might distract users but also leads to higher speedups. Conventional decoding may take charge from the point of divergence after draft verification and continue until the end condition is met.\nUnlike existing speculative decoding strategies, our approach eliminates the need for draft computations, making it a model-agnostic and plug-and-play solution for accelerating latency-sensitive streaming applications. Experimental results on simultaneous text-to-text re-translation demonstrate that our approach achieves up to 1.7x speedup compared to conventional auto-regressive re-translation without compromising quality. Additionally, it significantly reduces flickering by 80% by incorporating the display-only mask-k technique.', 'abstract_zh': '大语言模型（LLMs）在各种文本生成任务中已经展示了令人印象深刻的性能。然而，在流式应用（如实时翻译）中，继续挑战在于如何在输入上下文不断扩大的同时，不断更新输出，同时保持合理的计算成本以满足延迟要求。\n在这项工作中，我们重新审视了实时翻译中的重新翻译方法，并提出了自我推测偏向解码（Self-Speculative Biased Decoding），这是一种全新的推理范式，旨在避免为不断增长的输入流从头重新生成输出。我们建议使用最新的输出作为当前增长输入上下文的草稿。在验证阶段，输出将偏向草稿令牌，以提高草稿的接受率。这一策略不仅最大限度地减少了可能导致用户分心的闪烁现象，还提高了加速效果。传统解码可能在草稿验证后从分歧点接管，并继续运行直到满足结束条件。\n与现有的推测性解码策略不同，我们的方法消除了草稿计算的需求，使其成为一个模型无关的即插即用解决方案，适用于加速延迟敏感的流式应用。针对同时进行文本到文本重新翻译的实验结果表明，我们的方法在不牺牲质量的情况下，相比传统的自回归重新翻译可实现高达1.7倍的加速，并且通过引入只显示掩码技术，闪烁现象显著减少80%。', 'title_zh': '自我推测导向解码以实现更快的实时翻译'}
{'arxiv_id': 'arXiv:2509.21737', 'title': 'POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization', 'authors': 'Ziqing Wang, Yibo Wen, William Pattie, Xiao Luo, Weimin Wu, Jerry Yao-Chieh Hu, Abhishek Pandey, Han Liu, Kaize Ding', 'link': 'https://arxiv.org/abs/2509.21737', 'abstract': 'Lead optimization in drug discovery requires efficiently navigating vast chemical space through iterative cycles to enhance molecular properties while preserving structural similarity to the original lead compound. Despite recent advances, traditional optimization methods struggle with sample efficiency-achieving good optimization performance with limited oracle evaluations. Large Language Models (LLMs) provide a promising approach through their in-context learning and instruction following capabilities, which align naturally with these iterative processes. However, existing LLM-based methods fail to leverage this strength, treating each optimization step independently. To address this, we present POLO (Preference-guided multi-turn Optimization for Lead Optimization), which enables LLMs to learn from complete optimization trajectories rather than isolated steps. At its core, POLO introduces Preference-Guided Policy Optimization (PGPO), a novel reinforcement learning algorithm that extracts learning signals at two complementary levels: trajectory-level optimization reinforces successful strategies, while turn-level preference learning provides dense comparative feedback by ranking intermediate molecules within each trajectory. Through this dual-level learning from intermediate evaluation, POLO achieves superior sample efficiency by fully exploiting each costly oracle call. Extensive experiments demonstrate that POLO achieves 84% average success rate on single-property tasks (2.3x better than baselines) and 50% on multi-property tasks using only 500 oracle evaluations, significantly advancing the state-of-the-art in sample-efficient molecular optimization.', 'abstract_zh': '药物发现中的先导化合物优化需要通过迭代循环高效地导航庞大的化学空间，以提升分子性质同时保持结构相似性。尽管近期有所进展，传统优化方法在样本效率方面仍然面临挑战，即在有限的oracle评估下实现良好的优化性能。大型语言模型（LLMs）提供了有前景的方法，通过其上下文学习和指令遵循能力，自然地与这些迭代过程相契合。然而，现有的基于LLM的方法未能充分利用这一优势，将每个优化步骤独立处理。为解决这一问题，我们提出了一种POLO（Preference-guided multi-turn Optimization for Lead Optimization）方法，使LLM能够从完整的优化轨迹中学习，而不是孤立地处理每个步骤。POLO的核心是引入了一种新颖的强化学习算法——偏好引导的策略优化（PGPO），该算法在两个互补的层面提取学习信号：轨迹层面的优化强化成功策略，而回合层面的偏好学习通过在每次轨迹内对中间分子进行排名，提供密集的对比反馈。通过这一双层面从中间评估的学习，POLO实现了更高的样本效率，充分利用了每次昂贵的oracle调用。大量实验表明，POLO在单一性质任务上实现了84%的平均成功率（比基线高出2.3倍），在多性质任务上仅使用500次oracle评估就实现了50%的成功率，显著推动了在样本高效分子优化方面的最先进技术。', 'title_zh': 'POLO: 基于偏好的大循环强化学习lead优化'}
{'arxiv_id': 'arXiv:2509.21634', 'title': 'MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs', 'authors': 'Prakhar Sharma, Haohuang Wen, Vinod Yegneswaran, Ashish Gehani, Phillip Porras, Zhiqiang Lin', 'link': 'https://arxiv.org/abs/2509.21634', 'abstract': 'The evolution toward 6G networks is being accelerated by the Open Radio Access Network (O-RAN) paradigm -- an open, interoperable architecture that enables intelligent, modular applications across public telecom and private enterprise domains. While this openness creates unprecedented opportunities for innovation, it also expands the attack surface, demanding resilient, low-cost, and autonomous security solutions. Legacy defenses remain largely reactive, labor-intensive, and inadequate for the scale and complexity of next-generation systems. Current O-RAN applications focus mainly on network optimization or passive threat detection, with limited capability for closed-loop, automated response.\nTo address this critical gap, we present an agentic AI framework for fully automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM orchestrates security workflows through a modular multi-agent system powered by Large Language Models (LLMs). The framework features a Threat Analysis Agent for real-time data triage, a Threat Classification Agent that uses Retrieval-Augmented Generation (RAG) to map anomalies to specific countermeasures, and a Threat Response Agent that safely operationalizes mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped with robust safety guardrails, MobiLLM provides a blueprint for trustworthy AI-driven network security. Initial evaluations demonstrate that MobiLLM can effectively identify and orchestrate complex mitigation strategies, significantly reducing response latency and showcasing the feasibility of autonomous security operations in 6G.', 'abstract_zh': '6G O-RAN环境中的自主AI框架：全面自动化端到端威胁缓解', 'title_zh': 'MobiLLM: 6G 开放RAN中具有自主性的AI框架以实现闭环威胁缓解'}
{'arxiv_id': 'arXiv:2509.21629', 'title': 'InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?', 'authors': 'Anjiang Wei, Tarun Suresh, Tianran Sun, Haoze Wu, Ke Wang, Alex Aiken', 'link': 'https://arxiv.org/abs/2509.21629', 'abstract': 'Program verification relies on loop invariants, yet automatically discovering strong invariants remains a long-standing challenge. We introduce a principled framework for evaluating LLMs on invariant synthesis. Our approach uses a verifier-based decision procedure with a formal soundness guarantee and assesses not only correctness but also the speedup that invariants provide in verification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based verifiers against the traditional solver UAutomizer. While LLM-based verifiers represent a promising direction, they do not yet offer a significant advantage over UAutomizer. Model capability also proves critical, as shown by sharp differences in speedups across models, and our benchmark remains an open challenge for current LLMs. Finally, we show that supervised fine-tuning and Best-of-N sampling can improve performance: fine-tuning on 3589 instances raises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%, and Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.', 'abstract_zh': '程序验证依赖于循环不变式的使用，但自动生成强不变式仍是一个长期挑战。我们提出了一种 principled 的框架来评估 LLMs 在不变式合成中的表现。我们的方法采用基于验证器的决策程序，具有形式上的正确性保证，并不仅评估正确性，还评估不变式在验证中提供的加速效果。我们评估了 7 种最先进的 LLMs 和基于 LLM 的验证器与传统求解器 UAutomizer 的性能。尽管基于 LLM 的验证器展示出前景，但它们尚未在性能上显著超越 UAutomizer。模型能力同样至关重要，不同模型间存在显著的加速差异，表明我们的基准仍是对当前 LLMs 的开放挑战。最后，我们展示了监督微调和 Best-of-N 采样的性能改进：在 3589 个实例上进行微调后，Qwen3-Coder-480B 的加速案例比例从 8% 提高到 29.2%，Best-of-N 采样（N=16）后，Claude-sonnet-4 的加速案例比例从 8.8% 提高到 22.1%。', 'title_zh': 'InvBench: 能否通过不变式合成加速程序验证？'}
{'arxiv_id': 'arXiv:2509.21623', 'title': "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule", 'authors': 'Yuxuan Zhu, David H. Yang, Mohammad Mohammadi Amiri, Keerthiram Murugesan, Tejaswini Pedapati, Pin-Yu Chen', 'link': 'https://arxiv.org/abs/2509.21623', 'abstract': "The expanding long-context capabilities of large language models are constrained by a significant memory bottleneck: the key-value (KV) cache required for autoregressive generation. This bottleneck is substantial; for instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of 4 requires approximately 16GB for its KV cache, a size exceeding the model's weights. While KV-cache compression via low-rank projection is a promising direction, existing methods rely on a static, offline-learned subspace that performs poorly under data distribution shifts. To overcome these limitations, we introduce OjaKV, a novel framework that integrates a strategic hybrid storage policy with online subspace adaptation. First, OjaKV recognizes that not all tokens are equally important for compression; it preserves the crucial first and most recent tokens in full-rank, maintaining high-fidelity anchors for attention. Second, for the vast majority of intermediate tokens, it applies low-rank compression by incrementally adapting the projection basis using Oja's algorithm for online principal component analysis. This adaptation involves a comprehensive update during prompt prefilling and lightweight periodic updates during decoding, ensuring the subspace remains aligned with the evolving context. Crucially, our framework is fully compatible with modern attention modules like FlashAttention. Experiments demonstrate that OjaKV maintains or even improves zero-shot accuracy at high compression ratios. In particular, OjaKV achieves its strongest gains on very long-context benchmarks that require complex reasoning, highlighting the importance of online subspace adaptation in dynamically tracking context shifts. These results establish our hybrid framework as a practical, plug-and-play solution for memory-efficient long-context inference without requiring model fine-tuning.", 'abstract_zh': '大型语言模型扩展长上下文能力受到显著记忆瓶颈的限制：自回归生成所需的键值（KV）缓存。这种瓶颈是重大的；例如，处理32K tokens提示、批量大小为4的Llama-3.1-8B模型需要约16GB的KV缓存，该大小超过了模型的权重。虽然通过低秩投影压缩KV缓存是一个有潜力的方向，但现有方法依赖于静态的、离线学习的子空间，在数据分布变化时表现不佳。为克服这些局限性，我们引入OjaKV，这是一种将战略混合存储策略与在线子空间自适应相结合的创新框架。首先，OjaKV认识到并非所有token对压缩同等重要；它完整保留了至关重要的第一个和最近的token，维持了高保真的注意力锚点。其次，对于绝大多数中间token，它通过增量地使用Oja算法进行在线主成分分析来应用低秩压缩，调整投影基底。这一自适应过程包括在预填充提示时的全面更新，在解码过程中进行轻量级的周期性更新，确保子空间始终与上下文动态变化保持一致。 crucially，我们的框架完全兼容现代的注意力模块如FlashAttention。实验表明，OjaKV在高压缩比下能够保持或甚至提高零样本准确率。特别是在需要复杂推理的非常长上下文基准测试中，OjaKV获得了最大的收益，突显了在线子空间自适应在动态跟踪上下文变化中的重要性。这些结果确立了我们这种混合框架作为无需模型微调的实用、即插即用的长上下文推理内存高效解决方案。', 'title_zh': 'OjaKV：基于上下文感知的在线低秩键值缓存压缩方法及其Oja规则'}
{'arxiv_id': 'arXiv:2509.21613', 'title': 'Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective', 'authors': 'Lingxiao Kong, Cong Yang, Oya Deniz Beyan, Zeyd Boukhers', 'link': 'https://arxiv.org/abs/2509.21613', 'abstract': 'Multi-Objective Reinforcement Learning (MORL) presents significant challenges and opportunities for optimizing multiple objectives in Large Language Models (LLMs). We introduce a MORL taxonomy and examine the advantages and limitations of various MORL methods when applied to LLM optimization, identifying the need for efficient and flexible approaches that accommodate personalization functionality and inherent complexities in LLMs and RL. We propose a vision for a MORL benchmarking framework that addresses the effects of different methods on diverse objective relationships. As future research directions, we focus on meta-policy MORL development that can improve efficiency and flexibility through its bi-level learning paradigm, highlighting key research questions and potential solutions for improving LLM performance.', 'abstract_zh': '多目标强化学习（MORL）在大型语言模型（LLMs）的多目标优化中提出了重要的挑战和机遇。我们引入了一种MORL分类，并探讨了各种MORL方法在LLM优化中的优势与局限性，指出了需要高效且灵活的方法来适应个性化功能和LLMs及RL固有的复杂性。我们提出了一种MORL基准测试框架的愿景，该框架旨在解决不同方法对各种目标关系的影响。作为未来的研究方向，我们重点讨论了通过其双层学习范式改进效率和灵活性的元策略MORL的发展，强调了提高LLM性能的关键研究问题和潜在解决方案。', 'title_zh': '大型语言模型优化的多目标强化学习：前瞻视角'}
{'arxiv_id': 'arXiv:2509.21528', 'title': 'Preemptive Detection and Steering of LLM Misalignment via Latent Reachability', 'authors': 'Sathwik Karnik, Somil Bansal', 'link': 'https://arxiv.org/abs/2509.21528', 'abstract': 'Large language models (LLMs) are now ubiquitous in everyday tools, raising urgent safety concerns about their tendency to generate harmful content. The dominant safety approach -- reinforcement learning from human feedback (RLHF) -- effectively shapes model behavior during training but offers no safeguards at inference time, where unsafe continuations may still arise. We propose BRT-Align, a reachability-based framework that brings control-theoretic safety tools to LLM inference. BRT-Align models autoregressive generation as a dynamical system in latent space and learn a safety value function via backward reachability, estimating the worst-case evolution of a trajectory. This enables two complementary mechanisms: (1) a runtime monitor that forecasts unsafe completions several tokens in advance, and (2) a least-restrictive steering filter that minimally perturbs latent states to redirect generation away from unsafe regions. Experiments across multiple LLMs and toxicity benchmarks demonstrate that BRT-Align provides more accurate and earlier detection of unsafe continuations than baselines. Moreover, for LLM safety alignment, BRT-Align substantially reduces unsafe generations while preserving sentence diversity and coherence. Qualitative results further highlight emergent alignment properties: BRT-Align consistently produces responses that are less violent, less profane, less offensive, and less politically biased. Together, these findings demonstrate that reachability analysis provides a principled and practical foundation for inference-time LLM safety.', 'abstract_zh': '基于可达性分析的大型语言模型推理安全性框架：BRT-Align', 'title_zh': '预判检测与引导大型语言模型错位gorithme'}
{'arxiv_id': 'arXiv:2509.21500', 'title': 'Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training', 'authors': 'Junkai Zhang, Zihao Wang, Lin Gui, Swarnashree Mysore Sathyendra, Jaehwan Jeong, Victor Veitch, Wei Wang, Yunzhong He, Bing Liu, Lifeng Jin', 'link': 'https://arxiv.org/abs/2509.21500', 'abstract': 'Reinforcement fine-tuning (RFT) often suffers from \\emph{reward over-optimization}, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish Excellent responses from merely Great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields a misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce a workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements. Our code can be accessed at this https URL .', 'abstract_zh': '强化微调（RFT）常常遭受奖励过优化的问题，即策略模型通过操纵奖励信号来获得高分，但产生的输出质量较低。我们的理论分析表明，关键在于高奖分尾部的奖励失确：无法可靠地区分优秀回答与仅仅很好的回答。这促使我们关注高奖分区域。然而，在基础大规模语言模型中，这样的尾部样例稀缺。虽然可以从更强的模型或改写中获得非策略样例（例如），直接使用它们来训练会导致我们希望对齐的策略模型的奖励失确。为了解决这一问题，我们研究基于评分标准的奖励。通过设计，评分标准可以利用非策略样例同时保持对它们缺陷的不敏感性。为了获取能够捕捉高奖分尾部的评分标准，我们强调区分优秀和多样回答的重要性，并引入了一个实施这一理念的流程。我们实验证明，基于评分标准的奖励显著缓解了奖励过优化问题，并提供了有效的后训练大规模语言模型改进。我们的代码可以在以下链接访问：this https URL。', 'title_zh': '追逐尾部样本：有效的基于评阅标准的奖励模型对大规模语言模型后训练的研究'}
{'arxiv_id': 'arXiv:2509.21487', 'title': 'Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning', 'authors': 'Jillian Xu, Dylan Zhou, Vinay Shukla, Yang Yang, Junrui Ruan, Shuhuai Lin, Wenfei Zou, Yinxiao Liu, Karthik Lakshmanan', 'link': 'https://arxiv.org/abs/2509.21487', 'abstract': 'Chain-of-Thought (CoT) prompting often improves classification accuracy, but it introduces a significant throughput penalty with rationale generation (Wei et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for decoder-only language models (LMs) that adds (i) a pooled classification head used during training and inference and (ii) a reasoning head supervised by teacher rationales used only in training. We train with a loss function that is a weighted sum of label cross-entropy and token-level LM loss over input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative gains of 0.65-5.47% over pooled baselines, with notably larger gains on entailment/causal tasks. Since we disable the reasoning head at test time, inference throughput matches pooled classifiers and exceeds CoT decoding on the same backbones by 96-142 times in QPS.', 'abstract_zh': 'Chain-of-Thought (CoT) 提示往往能够提高分类准确性，但会引入显著的吞吐量惩罚（Wei et al., 2022；Cheng and Van Durme, 2024）。为了解决这一权衡，我们提出了一种双重头推理蒸馏（DHRD）方法，这是一种适用于仅解码器语言模型（LMs）的简单训练方法，它增加了（i）一个用于训练和推理的聚合分类头，以及（ii）一个由教师推理监督的推理头，仅在训练时使用。我们在一个损失函数中使用标签交叉熵和输入加推理序列的词元级LM损失加权和进行训练。在七个SuperGLUE任务上，DHRD相比聚合基线模型取得了6.5%-54.7%的相对增益，尤其在蕴含/因果任务上收益明显更大。由于我们在测试时禁用了推理头，推理吞吐量与聚合分类器相当，并且在相同骨干模型上超越CoT解码的每秒查询数QPS高出96-142倍。', 'title_zh': '双头推理精炼：通过训练时仅推理提高分类器准确性'}
{'arxiv_id': 'arXiv:2509.21482', 'title': 'Learning to Reason with Mixture of Tokens', 'authors': 'Adit Jain, Brendan Rappazzo', 'link': 'https://arxiv.org/abs/2509.21482', 'abstract': "Reinforcement learning with verifiable rewards (RLVR) has become a leading approach for improving large language model (LLM) reasoning capabilities. Most current methods follow variants of Group Relative Policy Optimization, which samples multiple reasoning completions, scores them relative to each other, and adjusts the policy accordingly. However, these approaches invariably sample discrete tokens at each reasoning step, discarding the rich distributional information in the model's probability distribution over candidate tokens. While preserving and utilizing this distributional information has proven beneficial in non-RL settings, current RLVR methods seem to be unnecessarily constraining the reasoning search space by not using this information. To address this limitation, we investigate mixture-of-token generation (MoT-G) in RLVR. We present a unified framework that generalizes existing MoT-G approaches, including existing training-free methods that construct mixture embeddings as weighted sums over token embeddings, and extend RLVR to operate directly in this continuous mixture space for generating chain-of-thought. Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive language tasks, we find that MoT--G methods achieve substantial improvements (5--35 \\% gains on 7 out of 10 tasks) compared to standard decoding with the Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of trajectories, suggesting improved training efficiency. Through comprehensive hidden-state and token-level analyses, we provide evidence that MoT--G's benefits may stem from its ability to maintain higher hidden-state entropy throughout the reasoning process and promote exploration in token space.", 'abstract_zh': '可验证奖励的强化学习（RLVR）已成为提升大规模语言模型（LLM）推理能力的领先方法。大多数当前方法遵循组相对策略优化的变体，生成多个推理完成，并相对评分以调整策略。然而，这些方法不可避免地在每一步推理中采样离散标记，忽略了模型在候选标记概率分布中的丰富分布信息。虽然在非RL设置中保留并利用这种分布信息已被证明是有益的，但当前的RLVR方法似乎通过不使用这些信息，不必要地限制了推理搜索空间。为解决这一限制，我们研究了RLVR中的混合标记生成（MoT-G）。我们提出了一种统一框架，将现有的MoT-G方法泛化，包括现有的无需训练的方法，这些方法构建混合嵌入作为标记嵌入的加权和，并扩展RLVR以直接操作这种连续混合空间来生成思维链。在Reasoning-Gym任务套件上评估了两种MoT-G变体，发现MoT-G方法与Qwen2.5-1.5B模型标准解码相比在7项任务中取得了显著改进（7项任务中5%至35%的提升），同时达到相当的准确性但只需一半的轨迹数量，这表明训练效率的提升。通过全面的隐藏状态和标记级分析，我们提供了证据，表明MoT-G的好处可能源自其在整个推理过程中保持更高的隐藏状态熵并促进标记空间的探索的能力。', 'title_zh': '学习使用词元混合进行推理'}
{'arxiv_id': 'arXiv:2509.21443', 'title': 'One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning', 'authors': 'Sualeha Farid, Jayden Lin, Zean Chen, Shivani Kumar, David Jurgens', 'link': 'https://arxiv.org/abs/2509.21443', 'abstract': "Large Language Models (LLMs) are increasingly deployed in multilingual and multicultural environments where moral reasoning is essential for generating ethically appropriate responses. Yet, the dominant pretraining of LLMs on English-language data raises critical concerns about their ability to generalize judgments across diverse linguistic and cultural contexts. In this work, we systematically investigate how language mediates moral decision-making in LLMs. We translate two established moral reasoning benchmarks into five culturally and typologically diverse languages, enabling multilingual zero-shot evaluation. Our analysis reveals significant inconsistencies in LLMs' moral judgments across languages, often reflecting cultural misalignment. Through a combination of carefully constructed research questions, we uncover the underlying drivers of these disparities, ranging from disagreements to reasoning strategies employed by LLMs. Finally, through a case study, we link the role of pretraining data in shaping an LLM's moral compass. Through this work, we distill our insights into a structured typology of moral reasoning errors that calls for more culturally-aware AI.", 'abstract_zh': '大型语言模型（LLMs）在多语言和多元文化环境中部署时，道德推理对于生成伦理上合适的响应至关重要。然而，LLMs 主要通过英语数据进行预训练，这引发了它们在不同语言和文化背景下进行道德判断泛化的关键问题。在本工作中，我们系统地研究了语言如何在LLMs中中介道德决策。我们将两个已建立的道德推理基准翻译成五种具有文化和类型多样性的语言，从而实现多语言零样本评估。我们的分析揭示了LLMs在不同语言之间的道德判断存在显著差异，往往反映了文化错位。通过结合精心构建的研究问题，我们揭示了这些差异背后的驱动因素，包括分歧以及LLMs采用的推理策略。最后，通过一个案例研究，我们探讨了预训练数据在塑造LLMs道德导向方面的角色。通过对本工作的总结，我们提炼出一种结构化的道德推理错误类型学，呼吁更加注重文化意识的人工智能。', 'title_zh': '一个模型，多种道德观：探究计算道德推理中的跨语言不一致性'}
{'arxiv_id': 'arXiv:2509.21424', 'title': 'PhenoMoler: Phenotype-Guided Molecular Optimization via Chemistry Large Language Model', 'authors': 'Ran Song, Hui Liu', 'link': 'https://arxiv.org/abs/2509.21424', 'abstract': "Current molecular generative models primarily focus on improving drug-target binding affinity and specificity, often neglecting the system-level phenotypic effects elicited by compounds. Transcriptional profiles, as molecule-level readouts of drug-induced phenotypic shifts, offer a powerful opportunity to guide molecular design in a phenotype-aware manner. We present PhenoMoler, a phenotype-guided molecular generation framework that integrates a chemistry large language model with expression profiles to enable biologically informed drug design. By conditioning the generation on drug-induced differential expression signatures, PhenoMoler explicitly links transcriptional responses to chemical structure. By selectively masking and reconstructing specific substructures-scaffolds, side chains, or linkers-PhenoMoler supports fine-grained, controllable molecular optimization. Extensive experiments demonstrate that PhenoMoler generates chemically valid, novel, and diverse molecules aligned with desired phenotypic profiles. Compared to FDA-approved drugs, the generated compounds exhibit comparable or enhanced drug-likeness (QED), optimized physicochemical properties, and superior binding affinity to key cancer targets. These findings highlight PhenoMoler's potential for phenotype-guided and structure-controllable molecular optimization.", 'abstract_zh': 'PhenoMoler：一种基于表型引导的分子生成框架', 'title_zh': 'PhenoMoler：基于表型指导的分子优化 via 化学大型语言模型'}
{'arxiv_id': 'arXiv:2509.21404', 'title': 'How Large Language Models Need Symbolism', 'authors': 'Xiaotie Deng, Hanyu Li', 'link': 'https://arxiv.org/abs/2509.21404', 'abstract': "We argue that AI's future requires more than scaling. To unlock genuine discovery, large language models need a compass: human-crafted symbols to guide their powerful but blind intuition.", 'abstract_zh': '我们认为AI的未来不仅仅依赖于规模的扩大。为了开启真正的发现之旅，大规模语言模型需要一个指南针：人类 crafting 的符号来引导它们强大但盲目的直觉。', 'title_zh': '大型语言模型需要符号主义'}
{'arxiv_id': 'arXiv:2509.21391', 'title': 'MIXRAG : Mixture-of-Experts Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering', 'authors': 'Lihui Liu, Carl J. Yang', 'link': 'https://arxiv.org/abs/2509.21391', 'abstract': 'Large Language Models (LLMs) have achieved impressive performance across a wide range of applications. However, they often suffer from hallucinations in knowledge-intensive domains due to their reliance on static pretraining corpora. To address this limitation, Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating external knowledge sources during inference. Among these sources, textual graphs provide structured and semantically rich information that supports more precise and interpretable reasoning. This has led to growing interest in graph-based RAG systems. Despite their potential, most existing approaches rely on a single retriever to identify relevant subgraphs, which limits their ability to capture the diverse aspects of complex queries. Moreover, these systems often struggle to accurately judge the relevance of retrieved content, making them prone to distraction by irrelevant noise. To address these challenges, in this paper, we propose MIXRAG, a Mixture-of-Experts Graph-RAG framework that introduces multiple specialized graph retrievers and a dynamic routing controller to better handle diverse query intents. Each retriever is trained to focus on a specific aspect of graph semantics, such as entities, relations, or subgraph topology. A Mixture-of-Experts module adaptively selects and fuses relevant retrievers based on the input query. To reduce noise in the retrieved information, we introduce a query-aware GraphEncoder that carefully analyzes relationships within the retrieved subgraphs, highlighting the most relevant parts while down-weighting unnecessary noise. Empirical results demonstrate that our method achieves state-of-the-art performance and consistently outperforms various baselines. MIXRAG is effective across a wide range of graph-based tasks in different domains. The code will be released upon paper acceptance.', 'abstract_zh': '大型语言模型（LLMs）在多种应用中取得了显著性能，但在知识密集型领域常常因依赖静态预训练数据集而产生幻觉。为解决这一局限，检索增强生成（RAG）通过在推理过程中融入外部知识源来提升LLMs。在这些知识源中，文本图提供了结构化和语义丰富的信息，有助于更加精确和可解释的推理。这导致了基于图的RAG系统的广泛关注。尽管具有潜力，大多数现有方法仍依赖单一检索器来识别相关子图，这限制了它们捕捉复杂查询多方面的能力。此外，这些系统往往难以准确判断检索内容的相关性，容易受到无关噪音的干扰。为应对这些问题，本文提出了一种专家混合图-RAG框架MIXRAG，引入了多个专门化的图检索器和动态路由控制器，以更好地处理多样化的查询意图。每个检索器被训练专注于图语义的特定方面，如实体、关系或子图拓扑。专家混合模块根据输入查询自适应地选择和融合相关检索器。为减少检索信息中的噪音，我们引入了一个查询感知的GraphEncoder，在仔细分析检索到的子图关系时，突出最重要的部分并降低不必要的噪音。实验结果表明，本方法取得了最先进的性能，并且在多种基线指标上表现更优。MIXRAG在不同领域多种基于图的任务中均表现有效。论文被接受后将发布代码。', 'title_zh': 'MIXRAG：专家混合检索增强生成方法及其在文本图理解与问答中的应用'}
{'arxiv_id': 'arXiv:2509.21371', 'title': 'ReGeS: Reciprocal Retrieval-Generation Synergy for Conversational Recommender Systems', 'authors': 'Dayu Yang, Hui Fang', 'link': 'https://arxiv.org/abs/2509.21371', 'abstract': 'Connecting conversation with external domain knowledge is vital for conversational recommender systems (CRS) to correctly understand user preferences. However, existing solutions either require domain-specific engineering, which limits flexibility, or rely solely on large language models, which increases the risk of hallucination. While Retrieval-Augmented Generation (RAG) holds promise, its naive use in CRS is hindered by noisy dialogues that weaken retrieval and by overlooked nuances among similar items. We propose ReGeS, a reciprocal Retrieval-Generation Synergy framework that unifies generation-augmented retrieval to distill informative user intent from conversations and retrieval-augmented generation to differentiate subtle item features. This synergy obviates the need for extra annotations, reduces hallucinations, and simplifies continuous updates. Experiments on multiple CRS benchmarks show that ReGeS achieves state-of-the-art performance in recommendation accuracy, demonstrating the effectiveness of reciprocal synergy for knowledge-intensive CRS tasks.', 'abstract_zh': '将对话与外部领域知识相连对于对话推荐系统（CRS）正确理解用户偏好至关重要。然而，现有解决方案要么需要领域特定工程，这限制了灵活性，要么仅依赖大型语言模型，增加了语义错误的风险。尽管检索增强生成（RAG）潜力巨大，但在CRS中的直接应用受限于嘈杂的对话削弱了检索效果，并且忽视了相似项目之间的细微差别。我们提出了一种交互式的检索-生成协同（ReGeS）框架，该框架将生成增强检索统一起来，从对话中提炼出有益的用户意图，并将检索增强生成区分开来，突显细微的项目特征。这种协同作用避免了额外注释的需求，减少了语义错误，并简化了持续更新。在多个CRS基准上的实验表明，ReGeS 在推荐准确性方面达到了最先进的性能，证明了交互式协同作用在知识密集型CRS任务中的有效性。', 'title_zh': 'ReGeS: 互逆检索-生成协同作用的对话推荐系统'}
{'arxiv_id': 'arXiv:2509.21367', 'title': 'Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan', 'authors': 'Yu-Kai Shih, You-Kai Kang', 'link': 'https://arxiv.org/abs/2509.21367', 'abstract': 'As smart tourism evolves, AI-powered chatbots have become indispensable for delivering personalized, real-time assistance to travelers while promoting sustainability and efficiency. However, these systems are increasingly vulnerable to prompt injection attacks, where adversaries manipulate inputs to elicit unintended behaviors such as leaking sensitive information or generating harmful content. This paper presents a case study on the design and implementation of a secure retrieval-augmented generation (RAG) chatbot for Hsinchu smart tourism services. The system integrates RAG with API function calls, multi-layered linguistic analysis, and guardrails against injections, achieving high contextual awareness and security. Key features include a tiered response strategy, RAG-driven knowledge grounding, and intent decomposition across lexical, semantic, and pragmatic levels. Defense mechanisms include system norms, gatekeepers for intent judgment, and reverse RAG text to prioritize verified data. We also benchmark a GPT-5 variant (released 2025-08-07) to assess inherent robustness. Evaluations with 674 adversarial prompts and 223 benign queries show over 95% accuracy on benign tasks and substantial detection of injection attacks. GPT-5 blocked about 85% of attacks, showing progress yet highlighting the need for layered defenses. Findings emphasize contributions to sustainable tourism, multilingual accessibility, and ethical AI deployment. This work offers a practical framework for deploying secure chatbots in smart tourism and contributes to resilient, trustworthy AI applications.', 'abstract_zh': '随着智能旅游的发展，基于AI的聊天机器人已成为提供个性化实时协助、促进可持续性和效率的重要工具。然而，这些系统越来越容易受到提示注入攻击的影响，攻击者通过操纵输入来诱发出意外行为，如泄露敏感信息或生成有害内容。本文呈现了一个案例研究，介绍了一种安全的检索增强生成（RAG）聊天机器人在新竹智能旅游服务中的设计与实现。该系统结合了RAG、API功能调用、多层次语言分析以及对注入的防护措施，实现了高度的语境意识和安全性。关键功能包括分层响应策略、RAG驱动的知识定位以及在词汇、语义和语用层面的意图分解。防御机制包括系统规范、意图判断的守门人以及逆向RAG文本以优先验证数据。我们还对2025年8月7日发布的GPT-5变体进行基准测试，以评估其固有的鲁棒性。使用674个 adversarial 提示和223个良性查询的评估结果显示，在良性任务上的准确率超过95%，并对注入攻击有显著检测。GPT-5阻挡了约85%的攻击，显示出进展但同时也指出了多层次防御的必要性。研究强调了对可持续旅游、多语言可访问性和负责任AI部署的贡献。该工作为在智能旅游中部署安全聊天机器人提供了一个实用框架，促进了稳健且可信赖的AI应用。', 'title_zh': '基于 Taiwanese Hsinchu 案例研究的智能旅游客户服务中安全增强的 RAG 加强 AI 聊天机器人设计与实现——对抗提示注入攻击的研究'}
{'arxiv_id': 'arXiv:2509.21361', 'title': 'Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs', 'authors': 'Norman Paulsen', 'link': 'https://arxiv.org/abs/2509.21361', 'abstract': "Large language model (LLM) providers boast big numbers for maximum context window sizes. To test the real world use of context windows, we 1) define a concept of maximum effective context window, 2) formulate a testing method of a context window's effectiveness over various sizes and problem types, and 3) create a standardized way to compare model efficacy for increasingly larger context window sizes to find the point of failure. We collected hundreds of thousands of data points across several models and found significant differences between reported Maximum Context Window (MCW) size and Maximum Effective Context Window (MECW) size. Our findings show that the MECW is, not only, drastically different from the MCW but also shifts based on the problem type. A few top of the line models in our test group failed with as little as 100 tokens in context; most had severe degradation in accuracy by 1000 tokens in context. All models fell far short of their Maximum Context Window by as much as 99 percent. Our data reveals the Maximum Effective Context Window shifts based on the type of problem provided, offering clear and actionable insights into how to improve model accuracy and decrease model hallucination rates.", 'abstract_zh': '大型语言模型（LLM）提供商宣称其最大上下文窗口大小庞大。为了测试上下文窗口的实际使用效果，我们1）定义了最大有效上下文窗口的概念，2）提出了评估不同大小和问题类型下上下文窗口效果的方法，3）创建了一种标准化方法，以比较不断增加的上下文窗口大小下的模型效果，找到失败点。我们在多个模型中收集了数十万数据点，发现报告的最大上下文窗口（MCW）大小与最大有效上下文窗口（MECW）大小之间存在显著差异。我们的研究发现，MECW不仅与MCW相差甚远，还因问题类型的不同而变化。测试组中的几款顶级模型在仅含100个token的上下文中就失败了；大多数模型在包含1000个token的上下文中准确度显著下降。所有模型的表现远远低于其最大上下文窗口99%以上。我们的数据揭示了最大有效上下文窗口会根据提供的问题类型变化，提供了明确且实用的见解，以提高模型精度并降低模型产生幻觉的频率。', 'title_zh': '上下文即你所需：LLM在现实世界限制下的最大有效上下文窗口'}
{'arxiv_id': 'arXiv:2509.21360', 'title': 'Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models', 'authors': 'Xingkai Peng, Jun Jiang, Meng Tong, Shuai Li, Weiming Zhang, Nenghai Yu, Kejiang Chen', 'link': 'https://arxiv.org/abs/2509.21360', 'abstract': "Text-to-image (T2I) models have been widely applied in generating high-fidelity images across various domains. However, these models may also be abused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks. Existing jailbreak methods primarily manipulate the textual prompt, leaving potential vulnerabilities in image-based inputs largely unexplored. Moreover, text-based methods face challenges in bypassing the model's safety filters. In response to these limitations, we propose the Multimodal Prompt Decoupling Attack (MPDA), which utilizes image modality to separate the harmful semantic components of the original unsafe prompt. MPDA follows three core steps: firstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe prompts and harmful prompts. The former are seemingly harmless sub-prompts that can bypass filters, while the latter are sub-prompts with unsafe semantics that trigger filters. Subsequently, the LLM rewrites the harmful prompts into natural adversarial prompts to bypass safety filters, which guide the T2I model to modify the base image into an NSFW output. Finally, to ensure semantic consistency between the generated NSFW images and the original unsafe prompts, the visual language model generates image captions, providing a new pathway to guide the LLM in iterative rewriting and refining the generated content.", 'abstract_zh': '多模态提示解耦攻击（MPDA）：利用图像模态分离原始不安全提示中的有害语义组件', 'title_zh': '多模态提示解藕攻击文本到图像模型中的安全过滤器'}
{'arxiv_id': 'arXiv:2509.21359', 'title': 'Influence Guided Context Selection for Effective Retrieval-Augmented Generation', 'authors': 'Jiale Deng, Yanyan Shen, Ziyuan Pei, Youmin Chen, Linpeng Huang', 'link': 'https://arxiv.org/abs/2509.21359', 'abstract': 'Retrieval-Augmented Generation (RAG) addresses large language model (LLM) hallucinations by grounding responses in external knowledge, but its effectiveness is compromised by poor-quality retrieved contexts containing irrelevant or noisy information. While existing approaches attempt to improve performance through context selection based on predefined context quality assessment metrics, they show limited gains over standard RAG. We attribute this limitation to their failure in holistically utilizing available information (query, context list, and generator) for comprehensive quality assessment. Inspired by recent advances in data selection, we reconceptualize context quality assessment as an inference-time data valuation problem and introduce the Contextual Influence Value (CI value). This novel metric quantifies context quality by measuring the performance degradation when removing each context from the list, effectively integrating query-aware relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI value eliminates complex selection hyperparameter tuning by simply retaining contexts with positive CI values. To address practical challenges of label dependency and computational overhead, we develop a parameterized surrogate model for CI value prediction during inference. The model employs a hierarchical architecture that captures both local query-context relevance and global inter-context interactions, trained through oracle CI value supervision and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and multiple LLMs demonstrate that our context selection method significantly outperforms state-of-the-art baselines, effectively filtering poor-quality contexts while preserving critical information. Code is available at this https URL.', 'abstract_zh': '基于检索的生成（RAG）通过将响应 grounding 在外部知识中来解决大型语言模型（LLM）的幻觉问题，但其效果因检索到的包含无关或噪声信息的低质量上下文而受到损害。现有的方法试图通过基于预定义的上下文质量评估指标进行上下文选择来提高性能，但它们在标准RAG上的提升有限。我们将这一局限归因于它们未能全面利用可用信息（查询、上下文列表和生成器）进行综合质量评估。受到数据选择最近进展的启发，我们将上下文质量评估重新概念化为推理时的数据价值问题，并引入了上下文影响值（CI值）。这一新型度量通过测量删除列表中每个上下文时性能的下降来量化上下文质量，有效整合了查询相关的相关性、列表相关的唯一性和生成器相关的对齐性。此外，CI值通过仅保留具有正值CI值的上下文来消除复杂的上下文选择超参数调优。为解决标签依赖性和计算开销的实际挑战，在推理期间我们开发了一种参数化替代模型来预测CI值。该模型采用层次架构，同时捕捉局部查询-上下文相关性和全局上下文交互性，并通过先验CI值监督和端到端生成器反馈进行训练。在8个NLP任务和多种LLM上的广泛实验表明，我们的上下文选择方法显著优于最先进的基线方法，在有效过滤低质量上下文的同时保留关键信息。代码可从以下链接获取。', 'title_zh': '基于影响的上下文选择以实现有效的检索增强生成'}
{'arxiv_id': 'arXiv:2509.21357', 'title': 'A Novel Differential Feature Learning for Effective Hallucination Detection and Classification', 'authors': 'Wenkai Wang, Vincent Lee, Yizhen Zheng', 'link': 'https://arxiv.org/abs/2509.21357', 'abstract': 'Large language model hallucination represents a critical challenge where outputs deviate from factual accuracy due to distributional biases in training data. While recent investigations establish that specific hidden layers exhibit differences between hallucinatory and factual content, the precise localization of hallucination signals within layers remains unclear, limiting the development of efficient detection methods. We propose a dual-model architecture integrating a Projected Fusion (PF) block for adaptive inter-layer feature weighting and a Differential Feature Learning (DFL) mechanism that identifies discriminative features by computing differences between parallel encoders learning complementary representations from identical inputs. Through systematic experiments across HaluEval\'s question answering, dialogue, and summarization datasets, we demonstrate that hallucination signals concentrate in highly sparse feature subsets, achieving significant accuracy improvements on question answering and dialogue tasks. Notably, our analysis reveals a hierarchical "funnel pattern" where shallow layers exhibit high feature diversity while deep layers demonstrate concentrated usage, enabling detection performance to be maintained with minimal degradation using only 1\\% of feature dimensions. These findings suggest that hallucination signals are more concentrated than previously assumed, offering a pathway toward computationally efficient detection systems that could reduce inference costs while maintaining accuracy.', 'abstract_zh': '大规模语言模型幻觉代表了一个关键挑战，其中输出由于训练数据中的分布偏见而偏离事实准确性。虽然近期研究证实特定隐藏层在幻觉性和事实性内容之间存在差异，但幻觉信号在层内的精确定位仍然模糊，限制了高效检测方法的发展。我们提出了一种双模型架构，结合了适应性跨层特征加权的投影融合（PF）模块和差异特征学习（DFL）机制，通过计算并行编码器从相同输入学习互补表示之间的差异来识别判别性特征。通过在HaluEval的问答、对话和总结数据集上的系统实验，我们证明幻觉信号集中在高度稀疏的特征子集中，显著提高了问答和对话任务的准确性。值得注意的是，我们分析揭示了一个分层的“漏斗模式”，其中浅层显示出高特征多样性而深层显示出特征集中使用，仅使用1%的特征维度即可维持检测性能最小的退化。这些发现表明，幻觉信号比之前认为的更为集中，提供了一条通往计算上更高效的检测系统的途径，能够在降低成本的同时保持准确性。', 'title_zh': '一种新型差异特征学习方法，用于有效幻觉检测与分类'}
{'arxiv_id': 'arXiv:2509.21354', 'title': 'KV-Efficient VLA: A Method of Speed up Vision Language Model with RNN-Gated Chunked KV Cache', 'authors': 'Wanshun Xu, Long Zhuang', 'link': 'https://arxiv.org/abs/2509.21354', 'abstract': 'Vision-Language-Action (VLA) models promise unified robotic perception and control, yet their scalability is constrained by the quadratic cost of attention and the unbounded growth of key-value (KV) memory during long-horizon inference. While recent methods improve generalization through scaling backbone architectures, they often neglect the inference inefficiencies critical to real-time deployment. In this work, we present KV-Efficient VLA, a model-agnostic memory compression framework that addresses these limitations by introducing a lightweight, training-friendly mechanism to selectively retain high-utility context. Our method partitions the KV cache into fixed size chunks and employs a recurrent gating module to summarize and filter historical context according to learned utility scores. This design preserves recent fine-grained detail while aggressively pruning stale, low-relevance memory, all while maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x inference speedup and 36% KV memory reduction, with minimal impact on task success. Our method integrates seamlessly into existing autoregressive and hybrid VLA stacks, enabling scalable inference without modifying training pipelines or downstream control logic.', 'abstract_zh': 'KV-Efficient Vision-Language-Action模型：一种兼顾推理效率和记忆压缩的框架', 'title_zh': 'KV-高效视觉语言模型：基于RNN门控分块键值缓存的方法'}
{'arxiv_id': 'arXiv:2509.21351', 'title': 'Random Direct Preference Optimization for Radiography Report Generation', 'authors': 'Valentin Samokhin, Boris Shirokikh, Mikhail Goncharov, Dmitriy Umerenkov, Maksim Bobrin, Ivan Oseledets, Dmitry Dylov, Mikhail Belyaev', 'link': 'https://arxiv.org/abs/2509.21351', 'abstract': 'Radiography Report Generation (RRG) has gained significant attention in medical image analysis as a promising tool for alleviating the growing workload of radiologists. However, despite numerous advancements, existing methods have yet to achieve the quality required for deployment in real-world clinical settings. Meanwhile, large Visual Language Models (VLMs) have demonstrated remarkable progress in the general domain by adopting training strategies originally designed for Large Language Models (LLMs), such as alignment techniques. In this paper, we introduce a model-agnostic framework to enhance RRG accuracy using Direct Preference Optimization (DPO). Our approach leverages random contrastive sampling to construct training pairs, eliminating the need for reward models or human preference annotations. Experiments on supplementing three state-of-the-art models with our Random DPO show that our method improves clinical performance metrics by up to 5%, without requiring any additional training data.', 'abstract_zh': '放射学报告生成（RRG）在医学影像分析中引起了广泛关注，作为减轻放射学家工作负荷的有希望的工具。然而，尽管取得了众多进展，现有方法仍未达到在真实临床环境中部署所需的质量标准。同时，大规模视觉语言模型（VLMs）通过采用为大规模语言模型（LLMs）设计的训练策略，如对齐技术，在通用领域已经取得了显著进步。在本文中，我们提出了一种模型无关的框架，通过直接偏好优化（DPO）增强RRG的准确性。我们的方法利用随机对比采样构建训练对，从而消除对奖励模型或人工偏好注解的需求。实验表明，将我们的随机DPO应用于三种最先进的模型，可以提高临床性能指标最多5%，而无需额外的训练数据。', 'title_zh': '随机直接偏好优化在放射报告生成中的应用'}
