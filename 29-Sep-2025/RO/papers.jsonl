{'arxiv_id': 'arXiv:2509.22653', 'title': 'See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation', 'authors': 'Chih Yao Hu, Yang-Sen Lin, Yuna Lee, Chih-Hai Su, Jie-Ying Lee, Shr-Ruei Tsai, Chin-Yang Lin, Kuan-Wen Chen, Tsung-Wei Ke, Yu-Lun Liu', 'link': 'https://arxiv.org/abs/2509.22653', 'abstract': 'We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harnesses VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, outperforming the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs. Project page: this https URL', 'abstract_zh': 'See, Point, Fly (SPF):一种基于视觉语言模型的无训练空地视觉与语言导航框架', 'title_zh': '见、指、飞：一种无需学习的通用无人飞行器导航VLM框架'}
{'arxiv_id': 'arXiv:2509.22652', 'title': 'Pixel Motion Diffusion is What We Need for Robot Control', 'authors': 'E-Ro Nguyen, Yichi Zhang, Kanchana Ranasinghe, Xiang Li, Michael S. Ryoo', 'link': 'https://arxiv.org/abs/2509.22652', 'abstract': 'We present DAWN (Diffusion is All We Need for robot control), a unified diffusion-based framework for language-conditioned robotic manipulation that bridges high-level motion intent and low-level robot action via structured pixel motion representation. In DAWN, both the high-level and low-level controllers are modeled as diffusion processes, yielding a fully trainable, end-to-end system with interpretable intermediate motion abstractions. DAWN achieves state-of-the-art results on the challenging CALVIN benchmark, demonstrating strong multi-task performance, and further validates its effectiveness on MetaWorld. Despite the substantial domain gap between simulation and reality and limited real-world data, we demonstrate reliable real-world transfer with only minimal finetuning, illustrating the practical viability of diffusion-based motion abstractions for robotic control. Our results show the effectiveness of combining diffusion modeling with motion-centric representations as a strong baseline for scalable and robust robot learning. Project page: this https URL', 'abstract_zh': '我们提出了DAWN（无需复杂机制只需扩散即可实现机器人控制），这是一种统一的基于扩散的框架，通过结构化像素运动表示将高层运动意图与低层机器人动作连接起来，实现语言条件下的机器人操作。在DAWN中，高层控制器和低层控制器都被建模为扩散过程，从而形成一个完全可训练、端到端的系统，并具有可解释的中间运动抽象。DAWN在具有挑战性的CALVIN基准测试中实现了最先进的结果，展示了其强大的多任务性能，并进一步在MetaWorld中验证了其有效性。尽管在模拟与现实之间的领域差距较大且缺乏真实世界数据的情况下，我们仅通过少量微调就实现了可靠的现实世界转移，这表明基于扩散的运动抽象对于机器人控制具有实际可行性。我们的结果证明了将扩散建模与以运动为中心的表示相结合作为可扩展且稳健的机器人学习强大基线的有效性。项目页面: this https URL', 'title_zh': '像素运动扩散是机器人控制所需的方法'}
{'arxiv_id': 'arXiv:2509.22643', 'title': 'VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search', 'authors': 'Wenkai Guo, Guanxing Lu, Haoyuan Deng, Zhenyu Wu, Yansong Tang, Ziwei Wang', 'link': 'https://arxiv.org/abs/2509.22643', 'abstract': 'Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations. To address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions. We further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries. We evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback. We conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation.', 'abstract_zh': 'Vision-Language-Action模型（VLAs）通过扩展示例学习在通用机器人操作任务中取得优异性能。然而，现有的VLAs仅限于预测短期下一步动作，在长轨迹任务中由于逐步偏差而面临挑战。为解决此问题，我们提出了一种插件框架VLA-Reasoner，该框架能够通过测试时间扩展示例来增强现成的VLAs，使其具备预见未来状态的能力。具体而言，VLA-Reasoner通过世界模型采样和展开可能的行动轨迹，生成未来状态，从而帮助其预见和推理潜在结果并搜索最优动作。我们进一步利用蒙特卡洛树搜索（MCTS）提高在大规模动作空间中搜索的效率，其中逐步的VLAs预测作为根节点。同时，我们引入基于核密度估计（KDE）的置信度采样机制，以在MCTS中实现高效的探索而无需冗余的VLAs查询。我们通过离线奖励塑造策略评估MCTS中的中间状态，以评分预测的未来场景并利用长期反馈纠正偏差。我们在模拟器和现实世界中进行了广泛实验，证明了我们提出的VLA-Reasoner在现有最先进VLAs的基础上取得了显著改善。我们的方法展示了通向可扩展的测试时间机器人操作计算潜在路径的可能性。', 'title_zh': 'VLA-Reasoner：通过在线蒙特卡洛树搜索增强视觉-语言-行动模型的推理能力'}
{'arxiv_id': 'arXiv:2509.22642', 'title': 'WoW: Towards a World omniscient World model Through Embodied Interaction', 'authors': 'Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, Zhiyuan Qin, Wanxin Tian, Kuangzhi Ge, Hao Li, Zezhong Qian, Anthony Chen, Qiang Zhou, Yueru Jia, Jiaming Liu, Yong Dai, Qingpo Wuwu, Chengyu Bai, Yu-Kai Wang, Ying Li, Lizhang Chen, Yong Bao, Zhiyuan Jiang, Jiacheng Zhu, Kai Tang, Ruichuan An, Yulin Luo, Qiuxuan Feng, Siyuan Zhou, Chi-min Chan, Chengkai Hou, Wei Xue, Sirui Han, Yike Guo, Shanghang Zhang, Jian Tang', 'link': 'https://arxiv.org/abs/2509.22642', 'abstract': "Humans develop an understanding of intuitive physics through active interaction with the world. This approach is in stark contrast to current video models, such as Sora, which rely on passive observation and therefore struggle with grasping physical causality. This observation leads to our central hypothesis: authentic physical intuition of the world model must be grounded in extensive, causally rich interactions with the real world. To test this hypothesis, we present WoW, a 14-billion-parameter generative world model trained on 2 million robot interaction trajectories. Our findings reveal that the model's understanding of physics is a probabilistic distribution of plausible outcomes, leading to stochastic instabilities and physical hallucinations. Furthermore, we demonstrate that this emergent capability can be actively constrained toward physical realism by SOPHIA, where vision-language model agents evaluate the DiT-generated output and guide its refinement by iteratively evolving the language instructions. In addition, a co-trained Inverse Dynamics Model translates these refined plans into executable robotic actions, thus closing the imagination-to-action loop. We establish WoWBench, a new benchmark focused on physical consistency and causal reasoning in video, where WoW achieves state-of-the-art performance in both human and autonomous evaluation, demonstrating strong ability in physical causality, collision dynamics, and object permanence. Our work provides systematic evidence that large-scale, real-world interaction is a cornerstone for developing physical intuition in AI. Models, data, and benchmarks will be open-sourced.", 'abstract_zh': '人类通过主动与世界互动来发展直觉物理理解，这一方法与当前依赖被动观察的视频模型（如Sora）存在明显差异，因此难以捉摸物理因果关系。这一观察使我们提出中心假设：世界模型中的真正物理直觉必须基于与真实世界进行广泛的因果丰富互动。为了验证这一假设，我们提出了WoW，一个在200万机器人交互轨迹上训练的140亿参数生成世界模型。我们的研究发现模型对物理的理解是一个可能结果的概率分布，导致随机不稳定性以及物理幻觉。此外，我们展示了通过SOPHIA这一方法可以主动限制这种新兴能力向物理现实靠拢，其中视觉-语言模型代理评估DiT生成的输出，并通过迭代演化语言指令引导其完善。此外，通过协同训练的反向动力学模型将这些优化的计划转化为可执行的机器人行动，从而完成想象到行动的闭环。我们建立了WoWBench，一个新的基准测试，专注于视频中的物理一致性和因果推理，WoW在人类和自主评估中均取得了最先进的性能，展示了强大的物理因果关系、碰撞动力学和物体持久性能力。我们的工作提供了大规模真实世界交互是开发AI物理直觉基石的系统性证据。模型、数据和基准将开源。', 'title_zh': 'WoW: 通过具身交互 towards 万物全知的世界模型'}
{'arxiv_id': 'arXiv:2509.22578', 'title': 'EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation', 'authors': 'Yuan Xu, Jiabing Yang, Xiaofeng Wang, Yixiang Chen, Zheng Zhu, Bowen Fang, Guan Huang, Xinze Chen, Yun Ye, Qiang Zhang, Peiyan Li, Xiangnan Wu, Kai Wang, Bing Zhan, Shuo Lu, Jing Liu, Nianfeng Liu, Yan Huang, Liang Wang', 'link': 'https://arxiv.org/abs/2509.22578', 'abstract': 'Imitation learning based policies perform well in robotic manipulation, but they often degrade under *egocentric viewpoint shifts* when trained from a single egocentric viewpoint. To address this issue, we present **EgoDemoGen**, a framework that generates *paired* novel egocentric demonstrations by retargeting actions in the novel egocentric frame and synthesizing the corresponding egocentric observation videos with proposed generative video repair model **EgoViewTransfer**, which is conditioned by a novel-viewpoint reprojected scene video and a robot-only video rendered from the retargeted joint actions. EgoViewTransfer is finetuned from a pretrained video generation model using self-supervised double reprojection strategy. We evaluate EgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After training with a mixture of EgoDemoGen-generated novel egocentric demonstrations and original standard egocentric demonstrations, policy success rate improves **absolutely** by **+17.0%** for standard egocentric viewpoint and by **+17.7%** for novel egocentric viewpoints in simulation. On real-world robot, the **absolute** improvements are **+18.3%** and **+25.8%**. Moreover, performance continues to improve as the proportion of EgoDemoGen-generated demonstrations increases, with diminishing returns. These results demonstrate that EgoDemoGen provides a practical route to egocentric viewpoint-robust robotic manipulation.', 'abstract_zh': '基于模仿学习的策略在机器人操作中表现良好，但当从单一主观视角训练时，它们在主观视角转换下往往会性能下降。为解决这一问题，我们提出了一种名为EgoDemoGen的框架，该框架通过重新目标化动作并在新颖主观视角框架下合成相应的主观视角观察视频，生成配对的新颖主观视角示范。所提出的生成视频修复模型EgoViewTransfer依据新颖视角重新投影的场景视频和仅包含机器人的视频进行微调，后者是根据重新目标化关节动作渲染的。EgoViewTransfer基于自监督双重重新投影策略从预训练的视频生成模型中微调。我们在仿真（RoboTwin2.0）和真实机器人上评估了EgoDemoGen。经过混合使用EgoDemoGen生成的新颖主观视角示范和原始标准主观视角示范训练后，标准主观视角下的策略成功率绝对提升17.0%，新颖主观视角下的策略成功率绝对提升17.7%。在真实机器人上，绝对提升分别为18.3%和25.8%。此外，随着EgoDemoGen生成示范的比例增加，性能持续提升，但边际效应递减。这些结果表明，EgoDemoGen为实现主观视角鲁棒的机器人操作提供了一种实用途径。', 'title_zh': 'EgoDemoGen: 新颖的主观示范生成 enables 视点鲁棒操作'}
{'arxiv_id': 'arXiv:2509.22573', 'title': 'MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data', 'authors': 'Farida Mohsen, Ali Safa', 'link': 'https://arxiv.org/abs/2509.22573', 'abstract': "Efficiently detecting human intent to interact with ubiquitous robots is crucial for effective human-robot interaction (HRI) and collaboration. Over the past decade, deep learning has gained traction in this field, with most existing approaches relying on multimodal inputs, such as RGB combined with depth (RGB-D), to classify time-sequence windows of sensory data as interactive or non-interactive. In contrast, we propose a novel RGB-only pipeline for predicting human interaction intent with frame-level precision, enabling faster robot responses and improved service quality. A key challenge in intent prediction is the class imbalance inherent in real-world HRI datasets, which can hinder the model's training and generalization. To address this, we introduce MINT-RVAE, a synthetic sequence generation method, along with new loss functions and training strategies that enhance generalization on out-of-sample data. Our approach achieves state-of-the-art performance (AUROC: 0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB input and supporting precise frame onset prediction. Finally, to support future research, we openly release our new dataset with frame-level labeling of human interaction intent.", 'abstract_zh': '高效检测人类与物联网机器人交互意图对于有效的无人机制导交互（HRI）和协作至关重要。在过去十年中，深度学习在该领域取得了进展，大多数现有方法依赖于多模态输入，如RGB与深度（RGB-D）结合，以分类传感器数据的时间序列窗口为交互或非交互。相比之下，我们提出了一种基于帧级精度预测人类交互意图的纯RGB管道，能够加快机器人响应速度并提高服务质量。意图预测的关键挑战是在真实世界的人机交互（HRI）数据集中固有的类别不平衡问题，这可能会影响模型的训练和泛化能力。为解决这一问题，我们引入了MINT-RVAE合成序列生成方法，以及新的损失函数和训练策略，以增强对未见数据的泛化能力。我们的方法在AUC-ROC方面达到最新技术水平（0.95），超越了先前的工作（0.90-0.912），同时仅需RGB输入并支持精确的帧起始点预测。最后，为了支持未来的研究，我们公开发布了带有帧级人类交互意图标注的新数据集。', 'title_zh': 'MINT-RVAE：基于RGB相机数据的基于人体姿态和情绪信息的人机交互意图预测模型'}
{'arxiv_id': 'arXiv:2509.22550', 'title': 'An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment', 'authors': 'Xiaoyun Qiu, Haichao Liu, Yue Pan, Jun Ma, Xinhu Zheng', 'link': 'https://arxiv.org/abs/2509.22550', 'abstract': "In mixed-traffic environments, where autonomous vehicles (AVs) interact with diverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous behaviors make safe and efficient lane change maneuvers highly challenging. Existing methods often oversimplify these interactions by assuming uniform patterns. We propose an intention-driven lane change framework that integrates driving-style recognition, cooperation-aware decision-making, and coordinated motion planning. A deep learning classifier trained on the NGSIM dataset identifies human driving styles in real time. A cooperation score with intrinsic and interactive components estimates surrounding drivers' intentions and quantifies their willingness to cooperate with the ego vehicle. Decision-making combines behavior cloning with inverse reinforcement learning to determine whether a lane change should be initiated. For trajectory generation, model predictive control is integrated with IRL-based intention inference to produce collision-free and socially compliant maneuvers. Experiments show that the proposed model achieves 94.2\\% accuracy and 94.3\\% F1-score, outperforming rule-based and learning-based baselines by 4-15\\% in lane change recognition. These results highlight the benefit of modeling inter-driver heterogeneity and demonstrate the potential of the framework to advance context-aware and human-like autonomous driving in complex traffic environments.", 'abstract_zh': '在包含自动驾驶车辆和人类驾驶车辆的复杂交通环境中，基于意图的变道框架：整合驾驶风格识别、合作感知决策和协同运动规划', 'title_zh': '基于意图的考虑异质动态合作的混合交通环境变道框架'}
{'arxiv_id': 'arXiv:2509.22498', 'title': 'HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes', 'authors': 'Katrina Ashton, Chahyon Ku, Shrey Shah, Wen Jiang, Kostas Daniilidis, Bernadette Bucher', 'link': 'https://arxiv.org/abs/2509.22498', 'abstract': 'Language-specified mobile manipulation tasks in novel environments simultaneously face challenges interacting with a scene which is only partially observed, grounding semantic information from language instructions to the partially observed scene, and actively updating knowledge of the scene with new observations. To address these challenges, we propose HELIOS, a hierarchical scene representation and associated search objective to perform language specified pick and place mobile manipulation tasks. We construct 2D maps containing the relevant semantic and occupancy information for navigation while simultaneously actively constructing 3D Gaussian representations of task-relevant objects. We fuse observations across this multi-layered representation while explicitly modeling the multi-view consistency of the detections of each object. In order to efficiently search for the target object, we formulate an objective function balancing exploration of unobserved or uncertain regions with exploitation of scene semantic information. We evaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and place benchmark in which perception is challenging due to large and complex scenes with comparatively small target objects. HELIOS achieves state-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also transfer to the real world without requiring additional data, as we illustrate by demonstrating it in a real world office environment on a Spot robot.', 'abstract_zh': '基于语言指定的移动操作任务在新型环境中的层次化场景表示与搜索目标', 'title_zh': 'HELIOS：层级探索语言导向开放场景交互'}
{'arxiv_id': 'arXiv:2509.22493', 'title': 'Ontological foundations for contrastive explanatory narration of robot plans', 'authors': 'Alberto Olivares-Alarcos, Sergi Foix, Júlia Borràs, Gerard Canal, Guillem Alenyà', 'link': 'https://arxiv.org/abs/2509.22493', 'abstract': "Mutual understanding of artificial agents' decisions is key to ensuring a trustworthy and successful human-robot interaction. Hence, robots are expected to make reasonable decisions and communicate them to humans when needed. In this article, the focus is on an approach to modeling and reasoning about the comparison of two competing plans, so that robots can later explain the divergent result. First, a novel ontological model is proposed to formalize and reason about the differences between competing plans, enabling the classification of the most appropriate one (e.g., the shortest, the safest, the closest to human preferences, etc.). This work also investigates the limitations of a baseline algorithm for ontology-based explanatory narration. To address these limitations, a novel algorithm is presented, leveraging divergent knowledge between plans and facilitating the construction of contrastive narratives. Through empirical evaluation, it is observed that the explanations excel beyond the baseline method.", 'abstract_zh': '人工代理决策的相互理解是确保人机交互可信和成功的关键。因此，机器人在必要时应做出合理决策并进行沟通。本文侧重于建模和推理两个竞争计划之间的比较，以便于机器人后续解释差异结果。首先，提出了一种新型本体模型来形式化和推理竞争计划之间的差异，从而实现对最合适的计划（如最短的、最安全的、最符合人类偏好的等）的分类。此外，还探讨了基于本体解释叙述基线算法的局限性。为克服这些局限性，本文提出了一种新型算法，该算法利用计划之间的分歧知识，促进对比叙述的构建。通过实证评估发现，这些解释超越了基线方法。', 'title_zh': '基于本体论基础的机器人计划对比解释性叙述'}
{'arxiv_id': 'arXiv:2509.22469', 'title': 'Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards', 'authors': 'Ben Rossano, Jaein Lim, Jonathan P. How', 'link': 'https://arxiv.org/abs/2509.22469', 'abstract': 'This paper proposes a task allocation algorithm for teams of heterogeneous robots in environments with uncertain task requirements. We model these requirements as probability distributions over capabilities and use this model to allocate tasks such that robots with complementary skills naturally position near uncertain tasks, proactively mitigating task failures without wasting resources. We introduce a market-based approach that optimizes the joint team objective while explicitly capturing coupled rewards between robots, offering a polynomial-time solution in decentralized settings with strict communication assumptions. Comparative experiments against benchmark algorithms demonstrate the effectiveness of our approach and highlight the challenges of incorporating coupled rewards in a decentralized formulation.', 'abstract_zh': '本文提出了一种异质机器人团队在任务需求不确定性环境下任务分配算法。我们将这些需求建模为能力的概率分布，并利用该模型分配任务，使得具有互补技能的机器人自然地接近不确定的任务，主动减轻任务失败风险而不浪费资源。我们介绍了一种基于市场的方法，在分散设置中优化联合团队目标的同时明确捕捉机器人之间的耦合奖励，提供严格的通信假设下的多项式时间解决方案。与基准算法的比较实验表明了我们方法的有效性，并突显了在分散公式中整合耦合奖励所面临的挑战。', 'title_zh': '具有强耦合机器人间奖励的不确定性意识多机器人任务分配'}
{'arxiv_id': 'arXiv:2509.22441', 'title': 'UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation', 'authors': 'Zhangyuan Wang, Yunpeng Zhu, Yuqi Yan, Xiaoyuan Tian, Xinhao Shao, Meixuan Li, Weikun Li, Guangsheng Su, Weicheng Cui, Dixia Fan', 'link': 'https://arxiv.org/abs/2509.22441', 'abstract': 'This paper presents UnderwaterVLA, a novel framework for autonomous underwater navigation that integrates multimodal foundation models with embodied intelligence systems. Underwater operations remain difficult due to hydrodynamic disturbances, limited communication bandwidth, and degraded sensing in turbid waters. To address these challenges, we introduce three innovations. First, a dual-brain architecture decouples high-level mission reasoning from low-level reactive control, enabling robust operation under communication and computational constraints. Second, we apply Vision-Language-Action(VLA) models to underwater robotics for the first time, incorporating structured chain-of-thought reasoning for interpretable decision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC) scheme compensates for fluid effects in real time without costly task-specific training. Experimental results in field tests show that UnderwaterVLA reduces navigation errors in degraded visual conditions while maintaining higher task completion by 19% to 27% over baseline. By minimizing reliance on underwater-specific training data and improving adaptability across environments, UnderwaterVLA provides a scalable and cost-effective path toward the next generation of intelligent AUVs.', 'abstract_zh': '水下VLA：一种结合多模态基础模型与实体智能系统的自主水下导航新型框架', 'title_zh': '水下VLA：自主水下导航的双脑视觉-语言-行动架构'}
{'arxiv_id': 'arXiv:2509.22434', 'title': 'An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics', 'authors': 'Margherita Martorana, Francesca Urgese, Ilaria Tiddi, Stefan Schlobach', 'link': 'https://arxiv.org/abs/2509.22434', 'abstract': 'Personal service robots are increasingly used in domestic settings to assist older adults and people requiring support. Effective operation involves not only physical interaction but also the ability to interpret dynamic environments, understand tasks, and choose appropriate actions based on context. This requires integrating both hardware components (e.g. sensors, actuators) and software systems capable of reasoning about tasks, environments, and robot capabilities. Frameworks such as the Robot Operating System (ROS) provide open-source tools that help connect low-level hardware with higher-level functionalities. However, real-world deployments remain tightly coupled to specific platforms. As a result, solutions are often isolated and hard-coded, limiting interoperability, reusability, and knowledge sharing. Ontologies and knowledge graphs offer a structured way to represent tasks, environments, and robot capabilities. Existing ontologies, such as the Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE), provide models for activities, spatial relationships, and reasoning structures. However, they often focus on specific domains and do not fully capture the connection between environment, action, robot capabilities, and system-level integration. In this work, we propose the Ontology for roBOts and acTions (OntoBOT), which extends existing ontologies to provide a unified representation of tasks, actions, environments, and capabilities. Our contributions are twofold: (1) we unify these aspects into a cohesive ontology to support formal reasoning about task execution, and (2) we demonstrate its generalizability by evaluating competency questions across four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how OntoBOT enables context-aware reasoning, task-oriented execution, and knowledge sharing in service robotics.', 'abstract_zh': '基于机器人和动作的本体（OntoBOT）：统一任务、动作、环境和能力的本体表示及其在服务机器人中的应用', 'title_zh': '统一建模任务、行动、环境与能力的本体理论'}
{'arxiv_id': 'arXiv:2509.22421', 'title': 'Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping', 'authors': 'Leonel Giacobbe, Jingdao Chen, Chuangchuang Sun', 'link': 'https://arxiv.org/abs/2509.22421', 'abstract': 'Grasping is a core task in robotics with various applications. However, most current implementations are primarily designed for rigid items, and their performance drops considerably when handling fragile or deformable materials that require real-time feedback. Meanwhile, tactile-reactive grasping focuses on a single agent, which limits their ability to grasp and manipulate large, heavy objects. To overcome this, we propose a learning-based, tactile-reactive multi-agent Model Predictive Controller (MPC) for grasping a wide range of objects with different softness and shapes, beyond the capabilities of preexisting single-agent implementations. Our system uses two Gelsight Mini tactile sensors [1] to extract real-time information on object texture and stiffness. This rich tactile feedback is used to estimate contact dynamics and object compliance in real time, enabling the system to adapt its control policy to diverse object geometries and stiffness profiles. The learned controller operates in a closed loop, leveraging tactile encoding to predict grasp stability and adjust force and position accordingly. Our key technical contributions include a multi-agent MPC formulation trained on real contact interactions, a tactile-data driven method for inferring grasping states, and a coordination strategy that enables collaborative control. By combining tactile sensing and a learning-based multi-agent MPC, our method offers a robust, intelligent solution for collaborative grasping in complex environments, significantly advancing the capabilities of multi-agent systems. Our approach is validated through extensive experiments against independent PD and MPC baselines. Our pipeline outperforms the baselines regarding success rates in achieving and maintaining stable grasps across objects of varying sizes and stiffness.', 'abstract_zh': '基于触觉反应的多代理模型预测控制 grasping：超越单代理能力的广泛对象抓取', 'title_zh': '基于学习的协作控制在双臂触觉反应式抓取中'}
{'arxiv_id': 'arXiv:2509.22356', 'title': 'RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation', 'authors': 'Enguang Liu, Siyuan Liang, Liming Lu, Xiyu Zeng, Xiaochun Cao, Aishan Liu, Shuchao Pang', 'link': 'https://arxiv.org/abs/2509.22356', 'abstract': 'The safety and reliability of embodied agents rely on accurate and unbiased visual perception. However, existing benchmarks mainly emphasize generalization and robustness under perturbations, while systematic quantification of visual bias remains scarce. This gap limits a deeper understanding of how perception influences decision-making stability. To address this issue, we propose RoboView-Bias, the first benchmark specifically designed to systematically quantify visual bias in robotic manipulation, following a principle of factor isolation. Leveraging a structured variant-generation framework and a perceptual-fairness validation protocol, we create 2,127 task instances that enable robust measurement of biases induced by individual visual factors and their interactions. Using this benchmark, we systematically evaluate three representative embodied agents across two prevailing paradigms and report three key findings: (i) all agents exhibit significant visual biases, with camera viewpoint being the most critical factor; (ii) agents achieve their highest success rates on highly saturated colors, indicating inherited visual preferences from underlying VLMs; and (iii) visual biases show strong, asymmetric coupling, with viewpoint strongly amplifying color-related bias. Finally, we demonstrate that a mitigation strategy based on a semantic grounding layer substantially reduces visual bias by approximately 54.5\\% on MOKA. Our results highlight that systematic analysis of visual bias is a prerequisite for developing safe and reliable general-purpose embodied agents.', 'abstract_zh': '机器人视觉偏差的系统量化：RoboView-Bias基准', 'title_zh': 'RoboView-偏差：机器人操控中体态代理视觉偏差的基准测试'}
{'arxiv_id': 'arXiv:2509.22296', 'title': 'Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm', 'authors': 'Joseph Hunt, Koyo Fujii, Aly Magassouba, Praminda Caleb-Solly', 'link': 'https://arxiv.org/abs/2509.22296', 'abstract': "Hospital patient falls remain a critical and costly challenge worldwide. While conventional fall prevention systems typically rely on post-fall detection or reactive alerts, they also often suffer from high false positive rates and fail to address the underlying patient needs that lead to bed-exit attempts. This paper presents a novel system architecture that leverages the Internet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction for proactive and personalized patient assistance. The system integrates a privacy-preserving thermal sensing model capable of real-time bed-exit prediction, with two coordinated robotic agents that respond dynamically based on predicted intent and patient input. This orchestrated response could not only reduce fall risk but also attend to the patient's underlying motivations for movement, such as thirst, discomfort, or the need for assistance, before a hazardous situation arises. Our contributions with this pilot study are three-fold: (1) a modular IoRT-based framework enabling distributed sensing, prediction, and multi-robot coordination; (2) a demonstration of low-resolution thermal sensing for accurate, privacy-preserving preemptive bed-exit detection; and (3) results from a user study and systematic error analysis that inform the design of situationally aware, multi-agent interactions in hospital settings. The findings highlight how interactive and connected robotic systems can move beyond passive monitoring to deliver timely, meaningful assistance, empowering safer, more responsive care environments.", 'abstract_zh': '基于物联网机器人技术的主动个性化患者辅助系统架构：减少医院患者跌倒挑战', 'title_zh': '超越检测——通过物联网机器人范式 orchestrating 人类-机器人-机器人协作'}
{'arxiv_id': 'arXiv:2509.22288', 'title': 'IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM', 'authors': 'Johan Hatleskog, Morten Nissov, Kostas Alexis', 'link': 'https://arxiv.org/abs/2509.22288', 'abstract': 'Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor graph node per measurement to compensate for the lack of time synchronization between radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this strategy results in a state creation rate of twice the individual sensor frequencies. This doubling of the number of states per second yields high optimization costs, inhibiting real-time performance on resource-constrained hardware. We introduce IMU-preintegrated radar factors that use high-rate inertial data to propagate the most recent LiDAR state to the radar measurement timestamp. This strategy maintains the node creation rate at the LiDAR measurement frequency. Assuming equal sensor rates, this lowers the number of nodes by 50 % and consequently the computational costs. Experiments on a single board computer (which has 4 cores each of 2.2 GHz A73 and 2 GHz A53 with 8 GB RAM) show that our method preserves the absolute pose error of a conventional baseline while simultaneously lowering the aggregated factor graph optimization time by up to 56 %.', 'abstract_zh': '基于IMU预积分的雷达-LiDAR惯性平滑器', 'title_zh': 'IMU-预积分雷达因子在异步雷达-激光雷达-惯性SLAM中的应用'}
{'arxiv_id': 'arXiv:2509.22287', 'title': 'Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities', 'authors': 'Stina Sundstedt, Mattias Wingren, Susanne Hägglund, Daniel Ventus', 'link': 'https://arxiv.org/abs/2509.22287', 'abstract': 'Preschool children with language vulnerabilities -- such as developmental language disorders or immigration related language challenges -- often require support to strengthen their expressive language skills. Based on the principle of implicit learning, speech-language therapists (SLTs) typically embed target morphological structures (e.g., third person -s) into everyday interactions or game-based learning activities. Educators are recommended by SLTs to do the same. This approach demands precise linguistic knowledge and real-time production of various morphological forms (e.g., "Daddy wears these when he drives to work"). The task becomes even more demanding when educators or parent also must keep children engaged and manage turn-taking in a game-based activity. In the TalBot project our multiprofessional team have developed an application in which the Furhat conversational robot plays the word retrieval game "Alias" with children to improve language skills. Our application currently employs a large language model (LLM) to manage gameplay, dialogue, affective responses, and turn-taking. Our next step is to further leverage the capacity of LLMs so the robot can generate and deliver specific morphological targets during the game. We hypothesize that a robot could outperform humans at this task. Novel aspects of this approach are that the robot could ultimately serve as a model and tutor for both children and professionals and that using LLM capabilities in this context would support basic communication needs for children with language vulnerabilities. Our long-term goal is to create a robust LLM-based Robot-Assisted Language Learning intervention capable of teaching a variety of morphological structures across different languages.', 'abstract_zh': '具有语言脆弱性的学前教育儿童——如发育性语言障碍或与移民相关语言挑战的儿童——通常需要支持以强化其表达语言技能。基于隐性学习的原则，言语-语言治疗师（SLTs）通常将目标形态结构（如第三人称-s）嵌入到日常生活互动或基于游戏的学习活动中。SLTs建议教育工作者采取同样的方法。这一方法要求具备精确的语言知识和在实时生产各种形态结构方面的能力（例如，“爸爸开车去工作时戴着这些”）。当教育工作者或家长还需要保持儿童参与并管理基于游戏的活动中的轮流发言时，任务变得更加具有挑战性。在TalBot项目中，我们的多学科团队开发了一个应用程序，其中Furhat对话机器人与儿童一起玩“Alias”词语接龙游戏，以提高语言技能。我们的应用程序目前利用大规模语言模型（LLM）来管理游戏、对话、情感反应和轮流发言。下一步，我们将进一步利用LLM的能力，使机器人在游戏过程中能够生成并交付特定的形态学目标。我们假设机器人在这个任务上能够超越人类。这种方法的创新之处在于，机器人最终可以成为儿童和专业人员的模型和导师，并且利用LLM的能力在这种情境下支持具有语言脆弱性儿童的基本沟通需求。我们的长期目标是创建一个基于LLM的机器人辅助语言学习干预措施，能够教授不同语言中的各种形态学结构。', 'title_zh': '利用大型语言模型辅助语言脆弱的学龄前儿童学习形态结构的学习方法'}
{'arxiv_id': 'arXiv:2509.22205', 'title': 'From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment', 'authors': 'Ke Ye, Jiaming Zhou, Yuanfeng Qiu, Jiayi Liu, Shihui Zhou, Kun-Yu Lin, Junwei Liang', 'link': 'https://arxiv.org/abs/2509.22205', 'abstract': 'Generalizing to long-horizon manipulation tasks in a zero-shot setting remains a central challenge in robotics. Current multimodal foundation based approaches, despite their capabilities, typically fail to decompose high-level commands into executable action sequences from static visual input alone. To address this challenge, we introduce Super-Mimic, a hierarchical framework that enables zero-shot robotic imitation by directly inferring procedural intent from unscripted human demonstration videos. Our framework is composed of two sequential modules. First, a Human Intent Translator (HIT) parses the input video using multimodal reasoning to produce a sequence of language-grounded subtasks. These subtasks then condition a Future Dynamics Predictor (FDP), which employs a generative model that synthesizes a physically plausible video rollout for each step. The resulting visual trajectories are dynamics-aware, explicitly modeling crucial object interactions and contact points to guide the low-level controller. We validate this approach through extensive experiments on a suite of long-horizon manipulation tasks, where Super-Mimic significantly outperforms state-of-the-art zero-shot methods by over 20\\%. These results establish that coupling video-driven intent parsing with prospective dynamics modeling is a highly effective strategy for developing general-purpose robotic systems.', 'abstract_zh': '在零样本设置下泛化至长时域操作任务仍然是机器人领域的核心挑战。Super-Mimic：一种通过直接从未脚本化的人类演示视频推断程序化意图实现零样本机器人模仿的分层框架', 'title_zh': '从观察到想象：通过人类示范与未来构想引导长期任务操控'}
{'arxiv_id': 'arXiv:2509.22199', 'title': 'MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training', 'authors': 'Haoyun Li, Ivan Zhang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Zhiqin Yang, Zhentao Zhang, Boyuan Wang, Chaojun Ni, Wenkang Qin, Xinze Chen, Yun Ye, Guan Huang, Zhenbo Song, Xingang Wang', 'link': 'https://arxiv.org/abs/2509.22199', 'abstract': 'Vision Language Action (VLA) models derive their generalization capability from diverse training data, yet collecting embodied robot interaction data remains prohibitively expensive. In contrast, human demonstration videos are far more scalable and cost-efficient to collect, and recent studies confirm their effectiveness in training VLA models. However, a significant domain gap persists between human videos and robot-executed videos, including unstable camera viewpoints, visual discrepancies between human hands and robotic arms, and differences in motion dynamics. To bridge this gap, we propose MimicDreamer, a framework that turns fast, low-cost human demonstrations into robot-usable supervision by jointly aligning vision, viewpoint, and actions to directly support policy training. For visual alignment, we propose H2R Aligner, a video diffusion model that generates high-fidelity robot demonstration videos by transferring motion from human manipulation footage. For viewpoint stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos via homography and inpaints occlusions and distortions caused by warping. For action alignment, we map human hand trajectories to the robot frame and apply a constrained inverse kinematics solver to produce feasible, low-jitter joint commands with accurate pose tracking. Empirically, VLA models trained purely on our synthesized human-to-robot videos achieve few-shot execution on real robots. Moreover, scaling training with human data significantly boosts performance compared to models trained solely on real robot data; our approach improves the average success rate by 14.7\\% across six representative manipulation tasks.', 'abstract_zh': 'MimicDreamer：将低成本人类演示转换为可应用于机器人的监督以弥合领域差距', 'title_zh': 'MimicDreamer: 将人类和机器人示范对齐以实现可扩展的联合学习训练'}
{'arxiv_id': 'arXiv:2509.22195', 'title': 'Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting', 'authors': 'Asher J. Hancock, Xindi Wu, Lihan Zha, Olga Russakovsky, Anirudha Majumdar', 'link': 'https://arxiv.org/abs/2509.22195', 'abstract': "Fine-tuning vision-language models (VLMs) on robot teleoperation data to create vision-language-action (VLA) models is a promising paradigm for training generalist policies, but it suffers from a fundamental tradeoff: learning to produce actions often diminishes the VLM's foundational reasoning and multimodal understanding, hindering generalization to novel scenarios, instruction following, and semantic understanding. We argue that this catastrophic forgetting is due to a distribution mismatch between the VLM's internet-scale pretraining corpus and the robotics fine-tuning data. Inspired by this observation, we introduce VLM2VLA: a VLA training paradigm that first resolves this mismatch at the data level by representing low-level actions with natural language. This alignment makes it possible to train VLAs solely with Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and averting catastrophic forgetting. As a result, the VLM can be fine-tuned on robot teleoperation data without fundamentally altering the underlying architecture and without expensive co-training on internet-scale VLM datasets. Through extensive Visual Question Answering (VQA) studies and over 800 real-world robotics experiments, we demonstrate that VLM2VLA preserves the VLM's core capabilities, enabling zero-shot generalization to novel tasks that require open-world semantic reasoning and multilingual instruction following.", 'abstract_zh': '基于机器人远程操作数据 fine-tuning 视觉-语言模型以创建视觉-语言-行动模型：解决灾难性遗忘的范式', 'title_zh': '动作即语言：在不发生灾难性遗忘的情况下Fine-Tuning VLMs为VLAs'}
{'arxiv_id': 'arXiv:2509.22175', 'title': 'DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions', 'authors': 'Quanzhou Li, Zhonghua Wu, Jingbo Wang, Chen Change Loy, Bo Dai', 'link': 'https://arxiv.org/abs/2509.22175', 'abstract': 'Learning to generate dual-hand grasps that respect object semantics is essential for robust hand-object interaction but remains largely underexplored due to dataset scarcity. Existing grasp datasets predominantly focus on single-hand interactions and contain only limited semantic part annotations. To address these challenges, we introduce a pipeline, SymOpt, that constructs a large-scale dual-hand grasp dataset by leveraging existing single-hand datasets and exploiting object and hand symmetries. Building on this, we propose a text-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand Affordance-aware Grasps for unseen objects. Our approach incorporates a novel dual-hand affordance representation and follows a two-stage design, which enables effective learning from a small set of segmented training objects while scaling to a much larger pool of unsegmented data. Extensive experiments demonstrate that our method produces diverse and semantically consistent grasps, outperforming strong baselines in both grasp quality and generalization to unseen objects. The project page is at this https URL.', 'abstract_zh': '学习生成尊重物体语义的双臂抓取对于稳健的手物交互至关重要，但由于数据集稀缺，这一领域仍处于很大程度上的未开发状态。现有的抓取数据集主要侧重于单手交互，并仅包含有限的语义部分标注。为了解决这些挑战，我们引入了一种名为SymOpt的管道，该管道通过利用现有单手数据集并利用物体和手的对称性构造了一个大规模的双臂抓取数据集。在此基础上，我们提出了一种文本引导的双臂抓取生成器DHAGrasp，用于生成未见物体的双臂可利用抓取。我们的方法结合了一种新颖的双臂可利用性表示，并采用两阶段设计，能够有效从少量分割的训练对象中学习，同时扩展到更大规模的非分割数据集。广泛的实验表明，我们的方法生成了多样且语义一致的抓取，在抓取质量和对未见物体的泛化方面均优于强基线方法。项目页面参见此链接：https://your-project-url.com', 'title_zh': 'DHAGrasp: 基于文本指令合成知觉aware双臂抓取'}
{'arxiv_id': 'arXiv:2509.22149', 'title': 'DemoGrasp: Universal Dexterous Grasping from a Single Demonstration', 'authors': 'Haoqi Yuan, Ziye Huang, Ye Wang, Chuan Mao, Chaoyi Xu, Zongqing Lu', 'link': 'https://arxiv.org/abs/2509.22149', 'abstract': 'Universal grasping with multi-fingered dexterous hands is a fundamental challenge in robotic manipulation. While recent approaches successfully learn closed-loop grasping policies using reinforcement learning (RL), the inherent difficulty of high-dimensional, long-horizon exploration necessitates complex reward and curriculum design, often resulting in suboptimal solutions across diverse objects. We propose DemoGrasp, a simple yet effective method for learning universal dexterous grasping. We start from a single successful demonstration trajectory of grasping a specific object and adapt to novel objects and poses by editing the robot actions in this trajectory: changing the wrist pose determines where to grasp, and changing the hand joint angles determines how to grasp. We formulate this trajectory editing as a single-step Markov Decision Process (MDP) and use RL to optimize a universal policy across hundreds of objects in parallel in simulation, with a simple reward consisting of a binary success term and a robot-table collision penalty. In simulation, DemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow Hand, outperforming previous state-of-the-art methods. It also shows strong transferability, achieving an average success rate of 84.6% across diverse dexterous hand embodiments on six unseen object datasets, while being trained on only 175 objects. Through vision-based imitation learning, our policy successfully grasps 110 unseen real-world objects, including small, thin items. It generalizes to spatial, background, and lighting changes, supports both RGB and depth inputs, and extends to language-guided grasping in cluttered scenes.', 'abstract_zh': '通用多指灵巧手抓取的学习是一项根本性的挑战，存在于机器人操作中。虽然最近的方法利用强化学习（RL）成功学习闭合环路的抓取策略，但高维度、长时域的探索固有难度需要复杂的奖励和课程设计，通常导致在多种物体上达到次优解。我们提出了一种简单的有效方法——DemoGrasp，用于学习通用灵巧抓取。我们从单一成功的抓取特定物体的示演示轨迹出发，并通过编辑这一轨迹中的机器人动作来适应新的物体和姿态：改变腕关节姿态决定抓取的位置，改变手关节角度决定抓取的方式。我们将这一轨迹编辑形式化为单一步骤的马尔可夫决策过程（MDP），并使用强化学习在仿真中并行优化数百种物体上的通用策略，奖励由二元成功项和机器人-桌面碰撞惩罚项构成。在仿真中，DemoGrasp在使用Shadow Hand时于DexGraspNet对象上达到了95%的成功率，超过了先前的最先进方法。此外，它也表现出强大的迁移性，在仅训练175个物体的情况下，对六组未见过的不同灵巧手模型，在多样物体数据集上达到了平均84.6%的成功率。通过基于视觉的模仿学习，我们的策略成功地抓取了110个未见过的真实世界物体，包括小型和细长物品。它能够适应空间、背景和光照变化，支持RGB和深度输入，并扩展到杂乱环境中基于语言引导的抓取。', 'title_zh': 'DemoGrasp: 从单次演示学习通用灵巧抓取'}
{'arxiv_id': 'arXiv:2509.22120', 'title': 'Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot', 'authors': 'Alireza Aliyari, Gholamreza Vossoughi', 'link': 'https://arxiv.org/abs/2509.22120', 'abstract': "The use of exoskeleton robots is increasing due to the rising number of musculoskeletal injuries. However, their effectiveness depends heavily on the design of control systems. Designing robust controllers is challenging because of uncertainties in human-robot systems. Among various control strategies, Model Predictive Control (MPC) is a powerful approach due to its ability to handle constraints and optimize performance. Previous studies have used linearization-based methods to implement robust MPC on exoskeletons, but these can degrade performance due to nonlinearities in the robot's dynamics. To address this gap, this paper proposes a Robust Nonlinear Model Predictive Control (RNMPC) method, called multi-stage NMPC, to control a two-degree-of-freedom exoskeleton by solving a nonlinear optimization problem. This method uses multiple scenarios to represent system uncertainties. The study focuses on minimizing human-robot interaction forces during the swing phase, particularly when the robot carries unknown loads. Simulations and experimental tests show that the proposed method significantly improves robustness, outperforming non-robust NMPC. It achieves lower tracking errors and interaction forces under various uncertainties. For instance, when a 2 kg unknown payload is combined with external disturbances, the RMS values of thigh and shank interaction forces for multi-stage NMPC are reduced by 77 and 94 percent, respectively, compared to non-robust NMPC.", 'abstract_zh': '随着肌肉骨骼损伤数量的增加，外骨骼机器人的使用正在增加。然而，其有效性高度依赖于控制系统的设计。由于人类-机器人系统中的不确定性，设计稳健的控制器具有挑战性。在各种控制策略中，模型预测控制（MPC）因其能够处理约束和优化性能而成为一种强大的方法。先前的研究使用线性化方法在外骨骼上实现稳健的MPC，但这些方法因机器人动力学中的非线性而可能削弱性能。为此，本文提出了一种称为分阶段NMPC的稳健非线性模型预测控制方法，通过求解非线性优化问题来控制两自由度外骨骼。该方法使用多个场景来表示系统不确定性。研究重点是在摆动阶段尽量减少人机交互力，特别是在机器人携带未知载荷的情况下。仿真和实验测试表明，所提出的方法显著提高了鲁棒性，优于非稳健NMPC。它在各种不确定性下实现了更低的跟踪误差和交互力。例如，当2kg未知载荷与外部干扰结合时，分阶段NMPC的股部和小腿交互力的均方根值分别比非稳健NMPC降低了77%和94%。', 'title_zh': '多阶段鲁棒非线性模型预测控制下肢外骨骼机器人'}
{'arxiv_id': 'arXiv:2509.22093', 'title': 'Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation', 'authors': 'Xiaohuan Pei, Yuxing Chen, Siyu Xu, Yunke Wang, Yuheng Shi, Chang Xu', 'link': 'https://arxiv.org/abs/2509.22093', 'abstract': 'Robotic manipulation with Vision-Language-Action models requires efficient inference over long-horizon multi-modal context, where attention to dense visual tokens dominates computational cost. Existing methods optimize inference speed by reducing visual redundancy within VLA models, but they overlook the varying redundancy across robotic manipulation stages. We observe that the visual token redundancy is higher in coarse manipulation phase than in fine-grained operations, and is strongly correlated with the action dynamic. Motivated by this observation, we propose \\textbf{A}ction-aware \\textbf{D}ynamic \\textbf{P}runing (\\textbf{ADP}), a multi-modal pruning framework that integrates text-driven token selection with action-aware trajectory gating. Our method introduces a gating mechanism that conditions the pruning signal on recent action trajectories, using past motion windows to adaptively adjust token retention ratios in accordance with dynamics, thereby balancing computational efficiency and perceptual precision across different manipulation stages. Extensive experiments on the LIBERO suites and diverse real-world scenarios demonstrate that our method significantly reduces FLOPs and action inference latency (\\textit{e.g.} $1.35 \\times$ speed up on OpenVLA-OFT) while maintaining competitive success rates (\\textit{e.g.} 25.8\\% improvements with OpenVLA) compared to baselines, thereby providing a simple plug-in path to efficient robot policies that advances the efficiency and performance frontier of robotic manipulation. Our project website is: \\href{this https URL}{this http URL}.', 'abstract_zh': '基于Vision-Language-Action模型的机器人操作需要在长时序多模态上下文中进行高效推理，其中对密集视觉标记的关注主导了计算成本。现有方法通过减少VLA模型内的视觉冗余来优化推理速度，但忽略了机器人操作各阶段的冗余变化。我们观察到，在粗粒度操作阶段视觉标记的冗余度高于细粒度操作阶段，并且与动作动态密切相关。受这一观察的启发，我们提出了一种集成文本驱动标记选择与动作感知轨迹门控的多模态剪枝框架——Action-aware Dynamic Pruning (ADP)。我们的方法引入了一个门控机制，根据近期的动作轨迹来条件化剪枝信号，并利用过去的运动窗口来适应性地调整标记保留比，从而在不同操作阶段平衡计算效率和感知精度。在LIBERO套件和多种真实场景上的广泛实验表明，与基线方法相比，我们的方法在保持竞争力的操作成功率的同时显著减少了FLOPs和动作推理延迟（例如，在OpenVLA-OFT上速度提升1.35倍），从而为高效的机器人策略提供了一条简单的插件路径，促进了机器人操作的效率和性能边界。', 'title_zh': '行动感知动态剪枝以实现高效视觉-语言-行动操作'}
{'arxiv_id': 'arXiv:2509.22065', 'title': 'Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot', 'authors': 'Ethan Fulcher, J. Diego Caporale, Yifeng Zhang, John Ruck, Feifei Qian', 'link': 'https://arxiv.org/abs/2509.22065', 'abstract': 'In-situ robotic exploration is an important tool for advancing knowledge of geological processes that describe the Earth and other Planetary bodies. To inform and enhance operations for these roving laboratories, it is imperative to understand the terramechanical properties of their environments, especially for traversing on loose, deformable substrates. Recent research suggested that legged robots with direct-drive and low-gear ratio actuators can sensitively detect external forces, and therefore possess the potential to measure terrain properties with their legs during locomotion, providing unprecedented sampling speed and density while accessing terrains previously too risky to sample. This paper explores these ideas by investigating the impact of gait on proprioceptive terrain sensing accuracy, particularly comparing a sensing-oriented gait, Crawl N\' Sense, with a locomotion-oriented gait, Trot-Walk. Each gait\'s ability to measure the strength and texture of deformable substrate is quantified as the robot locomotes over a laboratory transect consisting of a rigid surface, loose sand, and loose sand with synthetic surface crusts. Our results suggest that with both the sensing-oriented crawling gait and locomotion-oriented trot gait, the robot can measure a consistent difference in the strength (in terms of penetration resistance) between the low- and high-resistance substrates; however, the locomotion-oriented trot gait contains larger magnitude and variance in measurements. Furthermore, the slower crawl gait can detect brittle ruptures of the surface crusts with significantly higher accuracy than the faster trot gait. Our results offer new insights that inform legged robot "sensing during locomotion" gait design and planning for scouting the terrain and producing scientific measurements on other worlds to advance our understanding of their geology and formation.', 'abstract_zh': '基于就地行进的轮腿机器人地形感知研究：步态对 proprioceptive 地形感知精度的影响', 'title_zh': 'quadruped机器人步态设计对地形本体感受特性影响的研究'}
{'arxiv_id': 'arXiv:2509.22058', 'title': 'An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose', 'authors': 'Qifeng Wang, Weigang Li, Lei Nie, Xin Xu, Wenping Liu, Zhe Xu', 'link': 'https://arxiv.org/abs/2509.22058', 'abstract': 'As a key technology for autonomous navigation and positioning in mobile robots, light detection and ranging (LiDAR) odometry is widely used in autonomous driving applications. The Iterative Closest Point (ICP)-based methods have become the core technique in LiDAR odometry due to their efficient and accurate point cloud registration capability. However, some existing ICP-based methods do not consider the reliability of the initial pose, which may cause the method to converge to a local optimum. Furthermore, the absence of an adaptive mechanism hinders the effective handling of complex dynamic environments, resulting in a significant degradation of registration accuracy. To address these issues, this paper proposes an adaptive ICP-based LiDAR odometry method that relies on a reliable initial pose. First, distributed coarse registration based on density filtering is employed to obtain the initial pose estimation. The reliable initial pose is then selected by comparing it with the motion prediction pose, reducing the initial error between the source and target point clouds. Subsequently, by combining the current and historical errors, the adaptive threshold is dynamically adjusted to accommodate the real-time changes in the dynamic environment. Finally, based on the reliable initial pose and the adaptive threshold, point-to-plane adaptive ICP registration is performed from the current frame to the local map, achieving high-precision alignment of the source and target point clouds. Extensive experiments on the public KITTI dataset demonstrate that the proposed method outperforms existing approaches and significantly enhances the accuracy of LiDAR odometry.', 'abstract_zh': '基于可靠初始姿态的自适应ICP激光雷达SLAM方法', 'title_zh': '基于可靠初始姿态的自适应ICP激光雷达里程计'}
{'arxiv_id': 'arXiv:2509.22002', 'title': 'One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion', 'authors': 'Yuping Gu, Bangchao Huang, Haoran Sun, Ronghan Xu, Jiayi Yin, Wei Zhang, Fang Wan, Jia Pan, Chaoyang Song', 'link': 'https://arxiv.org/abs/2509.22002', 'abstract': 'While it is expected to build robotic limbs with multiple degrees of freedom (DoF) inspired by nature, a single DoF design remains fundamental, providing benefits that include, but are not limited to, simplicity, robustness, cost-effectiveness, and efficiency. Mechanisms, especially those with multiple links and revolute joints connected in closed loops, play an enabling factor in introducing motion diversity for 1-DoF systems, which are usually constrained by self-collision during a full-cycle range of motion. This study presents a novel computational approach to designing one-degree-of-freedom (1-DoF) overconstrained robotic limbs for a desired spatial trajectory, while achieving energy-efficient, self-collision-free motion in full-cycle rotations. Firstly, we present the geometric optimization problem of linkage-based robotic limbs in a generalized formulation for self-collision-free design. Next, we formulate the spatial trajectory generation problem with the overconstrained linkages by optimizing the similarity and dynamic-related metrics. We further optimize the geometric shape of the overconstrained linkage to ensure smooth and collision-free motion driven by a single actuator. We validated our proposed method through various experiments, including personalized automata and bio-inspired hexapod robots. The resulting hexapod robot, featuring overconstrained robotic limbs, demonstrated outstanding energy efficiency during forward walking.', 'abstract_zh': '基于多约束的单自由度冗余机械臂设计及其全周期自碰撞自由运动研究', 'title_zh': '具有能量效率高且自碰撞-free运动的一自由度超约束四肢机器人设计'}
{'arxiv_id': 'arXiv:2509.21986', 'title': 'Developing Vision-Language-Action Model from Egocentric Videos', 'authors': 'Tomoya Yoshida, Shuhei Kurita, Taichi Nishimura, Shinsuke Mori', 'link': 'https://arxiv.org/abs/2509.21986', 'abstract': 'Egocentric videos capture how humans manipulate objects and tools, providing diverse motion cues for learning object manipulation. Unlike the costly, expert-driven manual teleoperation commonly used in training Vision-Language-Action models (VLAs), egocentric videos offer a scalable alternative. However, prior studies that leverage such videos for training robot policies typically rely on auxiliary annotations, such as detailed hand-pose recordings. Consequently, it remains unclear whether VLAs can be trained directly from raw egocentric videos. In this work, we address this challenge by leveraging EgoScaler, a framework that extracts 6DoF object manipulation trajectories from egocentric videos without requiring auxiliary recordings. We apply EgoScaler to four large-scale egocentric video datasets and automatically refine noisy or incomplete trajectories, thereby constructing a new large-scale dataset for VLA pre-training. Our experiments with a state-of-the-art $\\pi_0$ architecture in both simulated and real-robot environments yield three key findings: (i) pre-training on our dataset improves task success rates by over 20\\% compared to training from scratch, (ii) the performance is competitive with that achieved using real-robot datasets, and (iii) combining our dataset with real-robot data yields further improvements. These results demonstrate that egocentric videos constitute a promising and scalable resource for advancing VLA research.', 'abstract_zh': '自中心视角视频记录了人类操作物体和工具的方式，提供了学习物体操作的多样化运动线索。与为训练视觉-语言-动作模型（VLAs）常用的昂贵且依赖专家的手动遥操作不同，自中心视角视频提供了一种可扩展的替代方案。然而，先前利用此类视频训练机器人策略的研究通常依赖于辅助注释，如详细的手动姿态记录。因此，仍不清楚VLAs能否直接从原始的自中心视角视频中训练。在本工作中，我们通过利用EgoScaler框架来应对这一挑战，该框架可以从自中心视角视频中提取六自由度的物体操作轨迹，无需辅助记录。我们将EgoScaler应用到四个大规模的自中心视角视频数据集，并自动优化了嘈杂或不完整的轨迹，从而构建了一个用于VLAs预训练的新大规模数据集。我们的实验使用最新的$\\pi_0$架构在模拟和真实机器人环境中产生了三个关键发现：(i) 使用我们的数据集预训练相比于从头训练，任务成功率提高了超过20%；(ii) 性能与使用真实机器人数据集达到的性能相竞争；(iii) 将我们的数据集与真实机器人数据集结合使用进一步提高了性能。这些结果表明，自中心视角视频是促进VLAs研究的一种有前景且可扩展的资源。', 'title_zh': '从第一人称视频中开发视觉-语言-动作模型'}
{'arxiv_id': 'arXiv:2509.21983', 'title': 'Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning', 'authors': 'Sigmund Hennum Høeg, Aksel Vaaler, Chaoqi Liu, Olav Egeland, Yilun Du', 'link': 'https://arxiv.org/abs/2509.21983', 'abstract': 'Constructing robots to accomplish long-horizon tasks is a long-standing challenge within artificial intelligence. Approaches using generative methods, particularly Diffusion Models, have gained attention due to their ability to model continuous robotic trajectories for planning and control. However, we show that these models struggle with long-horizon tasks that involve complex decision-making and, in general, are prone to confusing different modes of behavior, leading to failure. To remedy this, we propose to augment continuous trajectory generation by simultaneously generating a high-level symbolic plan. We show that this requires a novel mix of discrete variable diffusion and continuous diffusion, which dramatically outperforms the baselines. In addition, we illustrate how this hybrid diffusion process enables flexible trajectory synthesis, allowing us to condition synthesized actions on partial and complete symbolic conditions.', 'abstract_zh': '构建用于完成长期任务的机器人是人工智能领域的长期挑战。通过生成方法，尤其是扩散模型，由于其能够 modeling 连续化机器人轨迹以用于规划和控制的能力，这些方法引起了关注。然而，我们表明这些模型在涉及复杂决策的长期任务中表现不佳，通常会混淆不同的行为模式，导致失败。为解决这一问题，我们提出同时生成高级符号计划来增强连续轨迹生成。我们证明这种方法需要一种新颖的离散变量扩散与连续扩散混合，这明显优于基线方法。此外，我们展示了这种混合扩散过程如何允许灵活地合成轨迹，并使我们能够根据部分和完整的符号条件对生成的动作进行条件化。', 'title_zh': '混合扩散同时进行符号性和连续性规划'}
{'arxiv_id': 'arXiv:2509.21961', 'title': 'FlowDrive: moderated flow matching with data balancing for trajectory planning', 'authors': 'Lingguang Wang, Ömer Şahin Taş, Marlon Steiner, Christoph Stiller', 'link': 'https://arxiv.org/abs/2509.21961', 'abstract': 'Learning-based planners are sensitive to the long-tailed distribution of driving data. Common maneuvers dominate datasets, while dangerous or rare scenarios are sparse. This imbalance can bias models toward the frequent cases and degrade performance on critical scenarios. To tackle this problem, we compare balancing strategies for sampling training data and find reweighting by trajectory pattern an effective approach. We then present FlowDrive, a flow-matching trajectory planner that learns a conditional rectified flow to map noise directly to trajectory distributions with few flow-matching steps. We further introduce moderated, in-the-loop guidance that injects small perturbation between flow steps to systematically increase trajectory diversity while remaining scene-consistent. On nuPlan and the interaction-focused interPlan benchmarks, FlowDrive achieves state-of-the-art results among learning-based planners and approaches methods with rule-based refinements. After adding moderated guidance and light post-processing (FlowDrive*), it achieves overall state-of-the-art performance across nearly all benchmark splits.', 'abstract_zh': '基于学习的规划器对驾驶数据的长尾分布敏感。常见操作在数据集中占据主导，而危险或罕见场景则很少见。这种不平衡会使得模型倾向于频繁发生的案例，并在关键场景上性能下降。为了解决这一问题，我们比较了采样训练数据的平衡策略，并发现轨迹模式加权是一种有效的方法。我们随后介绍了FlowDrive，这是一个流匹配轨迹规划器，通过少量的流匹配步骤学习一个条件矫正流，直接将噪声映射到轨迹分布中。我们进一步引入了适度的在环指导，通过在流步骤之间注入小的扰动来系统地增加轨迹多样性，同时保持场景一致性。在nuPlan和交互侧重的interPlan基准测试中，FlowDrive达到了基于学习的规划器的最先进成果，并接近基于规则精化的传统方法。在增加适度指导和轻量级后处理（FlowDrive*）后，它在几乎所有基准测试划分上达到了整体最先进性能。', 'title_zh': 'FlowDrive: 带有数据平衡的流动匹配轨迹规划'}
{'arxiv_id': 'arXiv:2509.21955', 'title': 'Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception', 'authors': 'Divake Kumar, Sina Tayebati, Francesco Migliarba, Ranganath Krishnan, Amit Ranjan Trivedi', 'link': 'https://arxiv.org/abs/2509.21955', 'abstract': "Deep learning models in robotics often output point estimates with poorly calibrated confidences, offering no native mechanism to quantify predictive reliability under novel, noisy, or out-of-distribution inputs. Conformal prediction (CP) addresses this gap by providing distribution-free coverage guarantees, yet its reliance on fixed nonconformity scores ignores context and can yield intervals that are overly conservative or unsafe. We address this with Learnable Conformal Prediction (LCP), which replaces fixed scores with a lightweight neural function that leverages geometric, semantic, and task-specific features to produce context-aware uncertainty sets.\nLCP maintains CP's theoretical guarantees while reducing prediction set sizes by 18% in classification, tightening detection intervals by 52%, and improving path planning safety from 72% to 91% success with minimal overhead. Across three robotic tasks on seven benchmarks, LCP consistently outperforms Standard CP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it achieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object detection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding boxes. In path planning through cluttered environments, it improves success to 91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.\nThe method is lightweight (approximately 4.8% runtime overhead, 42 KB memory) and supports online adaptation, making it well suited to resource-constrained autonomous systems. Hardware evaluation shows LCP adds less than 1% memory and 15.9% inference overhead, yet sustains 39 FPS on detection tasks while being 7.4 times more energy-efficient than ensembles.", 'abstract_zh': '可学习形变预测（Learnable Conformal Prediction）在机器人任务中的应用', 'title_zh': '基于上下文感知非一致性函数的可学习区间预测方法及其在机器人规划与感知中的应用'}
{'arxiv_id': 'arXiv:2509.21928', 'title': 'SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks', 'authors': 'Jialiang Li, Wenzheng Wu, Gaojing Zhang, Yifan Han, Wenzhao Lian', 'link': 'https://arxiv.org/abs/2509.21928', 'abstract': 'Successfully solving long-horizon manipulation tasks remains a fundamental challenge. These tasks involve extended action sequences and complex object interactions, presenting a critical gap between high-level symbolic planning and low-level continuous control. To bridge this gap, two essential capabilities are required: robust long-horizon task planning and effective goal-conditioned manipulation. Existing task planning methods, including traditional and LLM-based approaches, often exhibit limited generalization or sparse semantic reasoning. Meanwhile, image-conditioned control methods struggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a novel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon Manipulation Tasks. SAGE utilizes semantic scene graphs as a structural representation for scene states. A structural scene graph enables bridging task-level semantic reasoning and pixel-level visuo-motor control. This also facilitates the controllable synthesis of accurate, novel sub-goal images. SAGE consists of two key components: (1) a scene graph-based task planner that uses VLMs and LLMs to parse the environment and reason about physically-grounded scene state transition sequences, and (2) a decoupled structural image editing pipeline that controllably converts each target sub-goal graph into a corresponding image through image inpainting and composition. Extensive experiments have demonstrated that SAGE achieves state-of-the-art performance on distinct long-horizon tasks.', 'abstract_zh': '面向长期操作任务的场景图感知指导与执行框架：SAGE', 'title_zh': 'SAGE: 场景图aware的指导与执行在长期 horizon 操作任务中'}
{'arxiv_id': 'arXiv:2509.21878', 'title': 'WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces', 'authors': 'Moses Gladson Selvamuthu, Tomoya Takahashi, Riichiro Tadakuma, Kazutoshi Tanaka', 'link': 'https://arxiv.org/abs/2509.21878', 'abstract': "Robotic manipulators capable of regulating both compliance and stiffness offer enhanced operational safety and versatility. Here, we introduce Worm Gear-based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator (VSA) that integrates a non-backdrivable worm gear. By decoupling the driving motor from external forces using this gear, WAVE enables precise force transmission to the joint, while absorbing positional discrepancies through compliance. WAVE is protected from excessive loads by converting impact forces into elastic energy stored in a spring. In addition, the actuator achieves continuous joint stiffness modulation by changing the spring's precompression length. We demonstrate these capabilities, experimentally validate the proposed stiffness model, show that motor loads approach zero at rest--even under external loading--and present applications using a manipulator with WAVE. This outcome showcases the successful decoupling of external forces. The protective attributes of this actuator allow for extended operation in contact-intensive tasks, and for robust robotic applications in challenging environments.", 'abstract_zh': '基于蜗杆的自适应可变弹性（WAVE）变量刚度执行器及其应用', 'title_zh': 'WAVE：基于蜗轮的自适应变弹性装置实现执行器与外部力解耦'}
{'arxiv_id': 'arXiv:2509.21873', 'title': 'Improved Vehicle Maneuver Prediction using Game Theoretic Priors', 'authors': 'Nishant Doshi', 'link': 'https://arxiv.org/abs/2509.21873', 'abstract': 'Conventional maneuver prediction methods use some sort of classification model on temporal trajectory data to predict behavior of agents over a set time horizon. Despite of having the best precision and recall, these models cannot predict a lane change accurately unless they incorporate information about the entire scene. Level-k game theory can leverage the human-like hierarchical reasoning to come up with the most rational decisions each agent can make in a group. This can be leveraged to model interactions between different vehicles in presence of each other and hence compute the most rational decisions each agent would make. The result of game theoretic evaluation can be used as a "prior" or combined with a traditional motion-based classification model to achieve more accurate predictions. The proposed approach assumes that the states of the vehicles around the target lead vehicle are known. The module will output the most rational maneuver prediction of the target vehicle based on an online optimization solution. These predictions are instrumental in decision making systems like Adaptive Cruise Control (ACC) or Traxen\'s iQ-Cruise further improving the resulting fuel savings.', 'abstract_zh': '传统的机动预测方法使用某种类型的分类模型来处理时空轨迹数据，以预测一组时间范围内的代理行为。尽管这些模型在精度和召回率方面表现最佳，但如果它们不包含整个场景的信息，则无法准确预测车道变更。级联游戏理论可以利用类似人类分层推理的方式，为组内每个代理可能做出的最合理决策提供依据。这可以用来建模彼此存在的不同车辆之间的互动，并由此计算出每个代理会做出的最合理决策。游戏理论评估的结果可以作为“先验知识”或与传统的基于运动的分类模型结合，以实现更准确的预测。所提出的方法假设目标车辆周围的车辆状态已知。该模块将基于在线优化解决方案输出目标车辆的最合理机动预测。这些预测对于如自适应巡航控制(ACC)或Traxen的iQ-Cruise之类的决策系统具有重要意义，进一步提高燃油节省效果。', 'title_zh': '基于博弈论先验的车辆 maneuvers 预测改进'}
{'arxiv_id': 'arXiv:2509.21810', 'title': 'Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors', 'authors': 'Ning Huang, Zhentao Xie, Qinchuan Li', 'link': 'https://arxiv.org/abs/2509.21810', 'abstract': 'Despite growing interest in developing legged robots that emulate biological locomotion for agile navigation of complex environments, acquiring a diverse repertoire of skills remains a fundamental challenge in robotics. Existing methods can learn motion behaviors from expert data, but they often fail to acquire multiple locomotion skills through a single policy and lack smooth skill transitions. We propose a multi-skill learning framework based on Conditional Adversarial Motion Priors (CAMP), with the aim of enabling quadruped robots to efficiently acquire a diverse set of locomotion skills from expert demonstrations. Precise skill reconstruction is achieved through a novel skill discriminator and skill-conditioned reward design. The overall framework supports the active control and reuse of multiple skills, providing a practical solution for learning generalizable policies in complex environments.', 'abstract_zh': '基于条件对抗运动先验的多技能学习框架：使 quadruped 机器人高效习得多样化的运动技能', 'title_zh': '基于条件对抗运动先验的学习多技能-legged运动'}
{'arxiv_id': 'arXiv:2509.21776', 'title': 'The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions', 'authors': 'Hyeonseong Kim, Roy El-Helou, Seungbeen Lee, Sungjoon Choi, Matthew Pan', 'link': 'https://arxiv.org/abs/2509.21776', 'abstract': 'Playful deception, a common feature in human social interactions, remains underexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice Cream (TIC) vendor routine, we investigate how bounded, culturally familiar forms of deception influence user trust, enjoyment, and engagement during robotic handovers. We design a robotic manipulator equipped with a custom end-effector and implement five TIC-inspired trick policies that deceptively delay the handover of an ice cream-shaped object. Through a mixed-design user study with 91 participants, we evaluate the effects of playful deception and interaction duration on user experience. Results reveal that TIC-inspired deception significantly enhances enjoyment and engagement, though reduces perceived safety and trust, suggesting a structured trade-off across the multi-dimensional aspects. Our findings demonstrate that playful deception can be a valuable design strategy for interactive robots in entertainment and engagement-focused contexts, while underscoring the importance of deliberate consideration of its complex trade-offs. You can find more information, including demonstration videos, on this https URL .', 'abstract_zh': '人在机器人交互中未充分探索的 playful deception：基于土耳其冰淇淋摊贩routine的研究', 'title_zh': '土耳其冰淇淋机器人：探究社交人机交互中的玩乐欺骗现象'}
{'arxiv_id': 'arXiv:2509.21723', 'title': 'VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Robotic Bimanual Manipulation', 'authors': 'Huayi Zhou, Kui Jia', 'link': 'https://arxiv.org/abs/2509.21723', 'abstract': 'Achieving generalizable bimanual manipulation requires systems that can learn efficiently from minimal human input while adapting to real-world uncertainties and diverse embodiments. Existing approaches face a dilemma: imitation policy learning demands extensive demonstrations to cover task variations, while modular methods often lack flexibility in dynamic scenes. We introduce VLBiMan, a framework that derives reusable skills from a single human example through task-aware decomposition, preserving invariant primitives as anchors while dynamically adapting adjustable components via vision-language grounding. This adaptation mechanism resolves scene ambiguities caused by background changes, object repositioning, or visual clutter without policy retraining, leveraging semantic parsing and geometric feasibility constraints. Moreover, the system inherits human-like hybrid control capabilities, enabling mixed synchronous and asynchronous use of both arms. Extensive experiments validate VLBiMan across tool-use and multi-object tasks, demonstrating: (1) a drastic reduction in demonstration requirements compared to imitation baselines, (2) compositional generalization through atomic skill splicing for long-horizon tasks, (3) robustness to novel but semantically similar objects and external disturbances, and (4) strong cross-embodiment transfer, showing that skills learned from human demonstrations can be instantiated on different robotic platforms without retraining. By bridging human priors with vision-language anchored adaptation, our work takes a step toward practical and versatile dual-arm manipulation in unstructured settings.', 'abstract_zh': '实现可泛化的双臂操作需要能够从最少的人类输入中高效学习并适应实际环境中的不确定性及多样化实体的系统。现有方法面临困境：模仿策略学习需要大量演示来覆盖任务变异性，而模块化方法在动态场景中往往缺乏灵活性。我们引入了VLBiMan框架，该框架通过任务感知分解从单个人类示例中提取可重用技能，保留不变的基础技能作为锚点，同时通过视觉-语言接地动态调整可调节组件。这种适应机制在背景变化、物体重定位或视觉杂乱的情况下解决了场景模糊性，而不需策略重新训练，利用语义解析和几何可行性约束。此外，该系统继承了类似人类的混合控制能力，能够灵活使用双臂进行同步和异步操作。广泛实验验证了VLBiMan在工具有用性和多对象任务中的表现，表明：（1）与模仿基线相比，演示需求大幅减少；（2）通过原子技能拼接实现长期任务的组合泛化；（3）对新型但语义相似的对象和外部干扰具有鲁棒性；（4）强大的跨实体迁移，表明从人类演示学到的技能可以在不同的机器人平台上实例化而无需重新训练。通过将人类先验与视觉-语言锚定的适应相结合，我们的工作朝着在非结构化环境中实现实用且多用途的双臂操作迈出了重要一步。', 'title_zh': 'VLBiMan: 视听锚定的一次性示范 enables 可泛化的双臂机器人操作'}
{'arxiv_id': 'arXiv:2509.21690', 'title': 'Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation', 'authors': 'Muqun Hu, Wenxi Chen, Wenjing Li, Falak Mandali, Zijian He, Renhong Zhang, Praveen Krisna, Katherine Christian, Leo Benaharon, Dizhi Ma, Karthik Ramani, Yan Gu', 'link': 'https://arxiv.org/abs/2509.21690', 'abstract': "Humanoid table tennis (TT) demands rapid perception, proactive whole-body motion, and agile footwork under strict timing -- capabilities that remain difficult for unified controllers. We propose a reinforcement learning framework that maps ball-position observations directly to whole-body joint commands for both arm striking and leg locomotion, strengthened by predictive signals and dense, physics-guided rewards. A lightweight learned predictor, fed with recent ball positions, estimates future ball states and augments the policy's observations for proactive decision-making. During training, a physics-based predictor supplies precise future states to construct dense, informative rewards that lead to effective exploration. The resulting policy attains strong performance across varied serve ranges (hit rate $\\geq$ 96% and success rate $\\geq$ 92%) in simulations. Ablation studies confirm that both the learned predictor and the predictive reward design are critical for end-to-end learning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute joints, the policy produces coordinated lateral and forward-backward footwork with accurate, fast returns, suggesting a practical path toward versatile, competitive humanoid TT.", 'abstract_zh': '人体形乒乓球（TT）需要在严格的时间限制下进行快速感知、全身先导运动和敏捷步法——这些能力对于统一控制器来说仍然是难题。我们提出了一种强化学习框架，该框架直接将球位观察映射到手臂打击和腿部运动的全身关节命令，并通过预测信号和密集的物理引导奖励加以强化。一个轻量级的学习预测器以最近的球位信息为输入，预测未来的球状态，并增强策略的观测以促进积极决策。在训练过程中，基于物理的预测器提供精确的未来状态，构建密集且有信息量的奖励，促进有效的探索。所得到的策略在各种发球范围内的模拟中表现出强大的性能（击球率≥96%，成功率≥92%）。消融研究证实，学习预测器和预测奖励设计对于端到端学习至关重要。该策略在物理Booster T1人型机器人上零样本部署，机器人拥有23个转动关节，能够产生协调的横向和前后步法，并能实现准确快速的回球，表明了一条通向下肢灵活且具有竞争力的人体形乒乓球路径。', 'title_zh': '面向多功能类人乒乓球：预测增强的统一强化学习'}
{'arxiv_id': 'arXiv:2509.21664', 'title': 'Generating Stable Placements via Physics-guided Diffusion Models', 'authors': 'Philippe Nadeau, Miguel Rogel, Ivan Bilić, Ivan Petrović, Jonathan Kelly', 'link': 'https://arxiv.org/abs/2509.21664', 'abstract': 'Stably placing an object in a multi-object scene is a fundamental challenge in robotic manipulation, as placements must be penetration-free, establish precise surface contact, and result in a force equilibrium. To assess stability, existing methods rely on running a simulation engine or resort to heuristic, appearance-based assessments. In contrast, our approach integrates stability directly into the sampling process of a diffusion model. To this end, we query an offline sampling-based planner to gather multi-modal placement labels and train a diffusion model to generate stable placements. The diffusion model is conditioned on scene and object point clouds, and serves as a geometry-aware prior. We leverage the compositional nature of score-based generative models to combine this learned prior with a stability-aware loss, thereby increasing the likelihood of sampling from regions of high stability. Importantly, this strategy requires no additional re-training or fine-tuning, and can be directly applied to off-the-shelf models. We evaluate our method on four benchmark scenes where stability can be accurately computed. Our physics-guided models achieve placements that are 56% more robust to forceful perturbations while reducing runtime by 47% compared to a state-of-the-art geometric method.', 'abstract_zh': '在多对象场景中稳定放置物体是机器人操作中的一个基本挑战，因为放置必须无穿插、建立精确的表面接触，并导致力的平衡。为了评估稳定性，现有方法依赖于运行仿真引擎或采用启发式、基于外观的评估。相比之下，我们的方法将稳定性直接集成到扩散模型的采样过程中。为此，我们查询离线的基于采样的规划器以收集多模态放置标签，并训练扩散模型以生成稳定的放置。扩散模型以场景和物体点云为条件，并作为几何感知的先验。我们利用基于评分生成模型的组合特性，将这种学习到的先验与稳定性感知的损失结合起来，从而增加从高稳定性区域采样的可能性。重要的是，这种策略不需要额外的重新训练或微调，并可以直接应用于现成的模型。我们在四个基准场景上评估了我们的方法，其中稳定性可以准确计算。我们的物理导向模型在承受外力扰动时的表现比最先进的几何方法更稳定，同时运行时间减少了47%。', 'title_zh': '基于物理引导的扩散模型生成稳定放置'}
{'arxiv_id': 'arXiv:2509.21602', 'title': 'Real-Time Indoor Object SLAM with LLM-Enhanced Priors', 'authors': 'Yang Jiao, Yiding Qiu, Henrik I. Christensen', 'link': 'https://arxiv.org/abs/2509.21602', 'abstract': 'Object-level Simultaneous Localization and Mapping (SLAM), which incorporates semantic information for high-level scene understanding, faces challenges of under-constrained optimization due to sparse observations. Prior work has introduced additional constraints using commonsense knowledge, but obtaining such priors has traditionally been labor-intensive and lacks generalizability across diverse object categories. We address this limitation by leveraging large language models (LLMs) to provide commonsense knowledge of object geometric attributes, specifically size and orientation, as prior factors in a graph-based SLAM framework. These priors are particularly beneficial during the initial phase when object observations are limited. We implement a complete pipeline integrating these priors, achieving robust data association on sparse object-level features and enabling real-time object SLAM. Our system, evaluated on the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\\% over the latest baseline. Additionally, we present real-world experiments in the supplementary video, demonstrating its real-time performance.', 'abstract_zh': '基于语义信息的对象级同时定位与建图（SLAM）面临稀疏观测导致的欠约束优化挑战。传统方法通过常识知识引入额外约束，但获取此类先验知识通常劳动密集且缺乏跨不同对象类别的一般化能力。我们通过利用大规模语言模型（LLMs）提供对象几何属性的常识知识，特别是大小和方向，作为图基SLAM框架中的先验因素来解决这一限制。这些先验在对象观测有限的初始阶段尤其有益。我们实现了一个完整的整合这些先验的管道，实现对稀疏对象级特征的稳健数据关联，并支持实时对象SLAM。在TUM RGB-D和3RScan数据集上的系统评估表明，与最新基线相比，映射精度提高了36.8%。此外，我们在补充视频中展示了其实时性能的实验结果。', 'title_zh': '带有LLM增强先验的实时室内物体SLAM'}
{'arxiv_id': 'arXiv:2509.21571', 'title': 'Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control', 'authors': 'HaoZhe Xu, Cheng Cheng, HongRui Sang, Zhipeng Wang, Qiyong He, Xiuxian Li, Bin He', 'link': 'https://arxiv.org/abs/2509.21571', 'abstract': 'Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots is essential for heterogeneous systems, yet most existing approaches target wheeled platforms whose limited mobility constrains exploration in complex terrains. Quadruped robots offer superior adaptability but undergo frequent posture variations, making it difficult to provide a stable landing surface for UAVs. To address these challenges, we propose an autonomous UAV-quadruped docking framework for GPS-denied environments. On the quadruped side, a Hybrid Internal Model with Horizontal Alignment (HIM-HA), learned via deep reinforcement learning, actively stabilizes the torso to provide a level platform. On the UAV side, a three-phase strategy is adopted, consisting of long-range acquisition with a median-filtered YOLOv8 detector, close-range tracking with a constraint-aware controller that integrates a Nonsingular Fast Terminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function (BF) to guarantee finite-time error convergence under field-of-view (FOV) constraints, and terminal descent guided by a Safety Period (SP) mechanism that jointly verifies tracking accuracy and platform stability. The proposed framework is validated in both simulation and real-world scenarios, successfully achieving docking on outdoor staircases higher than 17 cm and rough slopes steeper than 30 degrees. Supplementary materials and videos are available at: this https URL.', 'abstract_zh': 'GPS受限环境下自主无人机-四足机器人对接框架', 'title_zh': '自主无人机-四足机器人在复杂地形下的主动姿态对齐与约束感知控制 docking'}
{'arxiv_id': 'arXiv:2509.21563', 'title': 'PL-VIWO2: A Lightweight, Fast and Robust Visual-Inertial-Wheel Odometry Using Points and Lines', 'authors': 'Zhixin Zhang, Liang Zhao, Pawel Ladosz', 'link': 'https://arxiv.org/abs/2509.21563', 'abstract': 'Vision-based odometry has been widely adopted in autonomous driving owing to its low cost and lightweight setup; however, its performance often degrades in complex outdoor urban environments. To address these challenges, we propose PL-VIWO2, a filter-based visual-inertial-wheel odometry system that integrates an IMU, wheel encoder, and camera (supporting both monocular and stereo) for long-term robust state estimation. The main contributions are: (i) a novel line feature processing framework that exploits the geometric relationship between 2D feature points and lines, enabling fast and robust line tracking and triangulation while ensuring real-time performance; (ii) an SE(2)-constrained SE(3) wheel pre-integration method that leverages the planar motion characteristics of ground vehicles for accurate wheel updates; and (iii) an efficient motion consistency check (MCC) that filters out dynamic features by jointly using IMU and wheel measurements. Extensive experiments on Monte Carlo simulations and public autonomous driving datasets demonstrate that PL-VIWO2 outperforms state-of-the-art methods in terms of accuracy, efficiency, and robustness.', 'abstract_zh': '基于视觉的里程计在自动驾驶中因其低成本和轻量级配置而被广泛应用；然而，在复杂的户外城市环境中，其性能往往下降。为了解决这些问题，我们提出了PL-VIWO2，一个融合IMU、轮码盘和摄像头（支持单目和立体）的滤波器基视觉-惯性-轮里程计系统，用于长期鲁棒的状态估计。主要贡献包括：（i）一种新颖的线特征处理框架，利用二维特征点与直线之间的几何关系，实现快速和鲁棒的线跟踪和三角化，同时确保实时性能；（ii）一种SE(2)约束下的SE(3)轮预积分方法，利用地面车辆的平面运动特性以精确更新轮子状态；（iii）一种有效的运动一致性检查(MCC)，通过联合使用IMU和轮子测量来滤除动态特征。Monte Carlo仿真实验和公开的自动驾驶数据集上的大量实验表明，PL-VIWO2在精度、效率和鲁棒性方面优于现有方法。', 'title_zh': 'PL-VIWO2: 一种基于点与直线的轻量、快速且 robust 的视觉-惯性-陀螺仪里程计'}
{'arxiv_id': 'arXiv:2509.21543', 'title': 'Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation', 'authors': 'Jinbang Huang, Zhiyuan Li, Zhanguang Zhang, Xingyue Quan, Jianye Hao, Yingxue Zhang', 'link': 'https://arxiv.org/abs/2509.21543', 'abstract': "Large Language Models (LLMs) have recently shown strong potential in robotic task planning, particularly through automatic planning domain generation that integrates symbolic search. Prior approaches, however, have largely treated these domains as search utilities, with limited attention to their potential as scalable sources of reasoning data. At the same time, progress in reasoning LLMs has been driven by chain-of-thought (CoT) supervision, whose application in robotics remains dependent on costly, human-curated datasets. We propose Plan2Evolve, an LLM self-evolving framework in which the base model generates planning domains that serve as engines for producing symbolic problem-plan pairs as reasoning traces. These pairs are then transformed into extended CoT trajectories by the same model through natural-language explanations, thereby explicitly aligning symbolic planning structures with natural language reasoning. The resulting data extend beyond the model's intrinsic planning capacity, enabling model fine-tuning that yields a planning-enhanced LLM with improved planning success, stronger cross-task generalization, and reduced inference costs.", 'abstract_zh': 'LLMs的自进化框架：Plan2Evolve，一种用于机器人任务规划的数据生成与理由解释方法', 'title_zh': 'Plan2Evolve: 通过自动化领域生成提升规划能力的LLM自主进化方法'}
{'arxiv_id': 'arXiv:2509.21523', 'title': 'DroneFL: Federated Learning for Multi-UAV Visual Target Tracking', 'authors': 'Xiaofan Yu, Yuwei Wu, Katherine Mao, Ye Tian, Vijay Kumar, Tajana Rosing', 'link': 'https://arxiv.org/abs/2509.21523', 'abstract': 'Multi-robot target tracking is a fundamental problem that requires coordinated monitoring of dynamic entities in applications such as precision agriculture, environmental monitoring, disaster response, and security surveillance. While Federated Learning (FL) has the potential to enhance learning across multiple robots without centralized data aggregation, its use in multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely underexplored. Key challenges include limited onboard computational resources, significant data heterogeneity in FL due to varying targets and the fields of view, and the need for tight coupling between trajectory prediction and multi-robot planning. In this paper, we introduce DroneFL, the first federated learning framework specifically designed for efficient multi-UAV target tracking. We design a lightweight local model to predict target trajectories from sensor inputs, using a frozen YOLO backbone and a shallow transformer for efficient onboard training. The updated models are periodically aggregated in the cloud for global knowledge sharing. To alleviate the data heterogeneity that hinders FL convergence, DroneFL introduces a position-invariant model architecture with altitude-based adaptive instance normalization. Finally, we fuse predictions from multiple UAVs in the cloud and generate optimal trajectories that balance target prediction accuracy and overall tracking performance. Our results show that DroneFL reduces prediction error by 6%-83% and tracking distance by 0.4%-4.6% compared to a distributed non-FL framework. In terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has on average just 1.56 KBps data rate to the cloud.', 'abstract_zh': '多无人机目标跟踪的联邦学习框架', 'title_zh': '无人机联邦学习：多无人机视觉目标跟踪'}
{'arxiv_id': 'arXiv:2509.21496', 'title': 'Wall Inspector: Quadrotor Control in Wall-proximity Through Model Compensation', 'authors': 'Peiwen Yang, Weisong Wen, Runqiu Yang, Yingming Chen, Cheuk Chi Tsang', 'link': 'https://arxiv.org/abs/2509.21496', 'abstract': "The safe operation of quadrotors in near-wall urban or indoor environments (e.g., inspection and search-and-rescue missions) is challenged by unmodeled aerodynamic effects arising from wall-proximity. It generates complex vortices that induce destabilizing suction forces, potentially leading to hazardous vibrations or collisions. This paper presents a comprehensive solution featuring (1) a physics-based suction force model that explicitly characterizes the dependency on both rotor speed and wall distance, and (2) a suction-compensated model predictive control (SC-MPC) framework designed to ensure accurate and stable trajectory tracking during wall-proximity operations. The proposed SC-MPC framework incorporates an enhanced dynamics model that accounts for suction force effects, formulated as a factor graph optimization problem integrating system dynamics constraints, trajectory tracking objectives, control input smoothness requirements, and actuator physical limitations. The suction force model parameters are systematically identified through extensive experimental measurements across varying operational conditions. Experimental validation demonstrates SC-MPC's superior performance, achieving 2.1 cm root mean squared error (RMSE) in X-axis and 2.0 cm RMSE in Y-axis position control - representing 74% and 79% improvements over cascaded proportional-integral-derivative (PID) control, and 60% and 53% improvements over standard MPC respectively. The corresponding mean absolute error (MAE) metrics (1.2 cm X-axis, 1.4 cm Y-axis) similarly outperform both baselines. The evaluation platform employs a ducted quadrotor design that provides collision protection while maintaining aerodynamic efficiency. To facilitate reproducibility and community adoption, we have open-sourced our complete implementation, available at this https URL.", 'abstract_zh': '四旋翼无人机在近墙城区或室内环境中的安全操作受到墙缘引起的未建模气动效应的挑战，这些效应会产生复杂的涡流，引发不利的吸力力，可能导致振动或碰撞。本文提出了一种全面的解决方案，包括（1）一个基于物理的吸力力模型，明确表征旋翼速度和墙体距离的依赖关系，以及（2）一种吸力补偿的模型预测控制（SC-MPC）框架，旨在确保在墙缘操作期间准确稳定的轨迹跟踪。所提出的SC-MPC框架结合了动态模型的增强形式，将其作为因子图优化问题，整合系统动力学约束、轨迹跟踪目标、控制输入平滑要求以及执行器物理限制。通过广泛的实验测量系统地识别吸力力模型参数。实验验证表明，SC-MPC表现出色，在X轴位置控制中实现了2.1 cm的均方根误差（RMSE），在Y轴位置控制中实现了2.0 cm的RMSE——分别比级联比例-积分-微分（PID）控制提高了74%和79%，比标准MPC分别提高了60%和53%。相应的绝对平均误差（MAE）指标（1.2 cm X轴，1.4 cm Y轴）也优于两种基线。评价平台采用带有碰撞保护的涵道四旋翼设计，保持了空气动力学效率。为了促进可重复性和社区应用，我们开源了完整的实现，地址为[this url]。', 'title_zh': '墙检测器：基于模型补偿的接近墙体的四旋翼控制'}
{'arxiv_id': 'arXiv:2509.21445', 'title': 'Developing a Mono-Actuated Compliant GeoGami Robot', 'authors': 'Archie Webster, Lee Skull, Seyed Amir Tafrishi', 'link': 'https://arxiv.org/abs/2509.21445', 'abstract': 'This paper presents the design of a new soft-rigid robotic platform, "GeoGami". We leverage origami surface capabilities to achieve shape contraction and to support locomotion with underactuated forms. A key challenge is that origami surfaces have high degrees of freedom and typically require many actuators; we address repeatability by integrating surface compliance. We propose a mono-actuated GeoGami mobile platform that combines origami surface compliance with a geometric compliant skeleton, enabling the robot to transform and locomote using a single actuator. We demonstrate the robot, develop a stiffness model, and describe the central gearbox mechanism. We also analyze alternative cable-driven actuation methods for the skeleton to enable surface transformation. Finally, we evaluate the GeoGami platform for capabilities, including shape transformation and rolling. This platform opens new capabilities for robots that change shape to access different environments and that use shape transformation for locomotion.', 'abstract_zh': 'GeoGami：一种新颖的软-刚性机器人平台及其设计', 'title_zh': '开发一种单驱机动 compliant GeoGami 机器人'}
{'arxiv_id': 'arXiv:2509.21370', 'title': 'Language-in-the-Loop Culvert Inspection on the Erie Canal', 'authors': 'Yashom Dighe, Yash Turkar, Karthik Dantu', 'link': 'https://arxiv.org/abs/2509.21370', 'abstract': 'Culverts on canals such as the Erie Canal, built originally in 1825, require frequent inspections to ensure safe operation. Human inspection of culverts is challenging due to age, geometry, poor illumination, weather, and lack of easy access. We introduce VISION, an end-to-end, language-in-the-loop autonomy system that couples a web-scale vision-language model (VLM) with constrained viewpoint planning for autonomous inspection of culverts. Brief prompts to the VLM solicit open-vocabulary ROI proposals with rationales and confidences, stereo depth is fused to recover scale, and a planner -- aware of culvert constraints -- commands repositioning moves to capture targeted close-ups. Deployed on a quadruped in a culvert under the Erie Canal, VISION closes the see, decide, move, re-image loop on-board and produces high-resolution images for detailed reporting without domain-specific fine-tuning. In an external evaluation by New York Canal Corporation personnel, initial ROI proposals achieved 61.4\\% agreement with subject-matter experts, and final post-re-imaging assessments reached 80\\%, indicating that VISION converts tentative hypotheses into grounded, expert-aligned findings.', 'abstract_zh': 'Erie运河等渠道上的涵洞，最初建于1825年，需要定期检查以确保安全运行。由于老化、几何形状、光照差、天气和难以接近等因素，人工检查涵洞具有挑战性。我们引入了VISION，这是一个端到端、带有语言循环的自主系统，将大规模网络视觉-语言模型（VLM）与受限视角规划相结合，以实现涵洞的自主检查。通过简短的提示，VLM征集开放词汇的目标区域建议及其理由和置信度，融合了立体深度以恢复尺度，并由计划者根据涵洞约束命令重新定位动作以捕捉目标特写。该系统部署在Erie运河下的四足机器人上，实现了从观察、决策、移动到重新成像的闭环操作，并生成高分辨率图像以供详细报告，无需特定领域的微调。在纽约运河公司人员的外部评估中，初始目标区域建议与主题专家一致率为61.4%，最终重新成像后的评估达到80%，表明VISION将初步假设转化为与专家一致的确定性结果。', 'title_zh': 'Linguistic-in-the-Loop 沃尔什运河 culvert 检查'}
{'arxiv_id': 'arXiv:2509.22548', 'title': 'JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation', 'authors': 'Shuang Zeng, Dekang Qi, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Shiyi Liang, Mu Xu, Xing Wei', 'link': 'https://arxiv.org/abs/2509.22548', 'abstract': "Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: this https URL.", 'abstract_zh': '基于视觉-语言导航需要一种通过自然语言指令和连续视频流引导的具身代理在未见环境中导航。最近的基于视觉-语言导航的进步由多模态大型语言模型的强大语义理解推动。然而，这些方法通常依赖显式的语义记忆，如构建文本认知地图或存储历史视觉帧。这类方法遭受空间信息损失、计算冗余和内存膨胀的困扰，阻碍了高效的导航。受到人类导航中隐式场景表示的启发，类似于左脑的语义理解与右脑的空间认知，我们提出JanusVLN，一种新颖的基于双隐式神经记忆的视觉-语言导航框架，将空间-几何和视觉-语义记忆建模为各自的紧凑且固定大小的神经表示。该框架首先扩展了多模态大型语言模型，合并空间-几何编码器的3D先验知识，从而增强仅基于RGB输入的模型的时空推理能力。然后，从空间-几何和视觉-语义编码器构建历史键值缓存，并形成双隐式记忆，仅保留初始窗口和滑动窗口中的键值，避免冗余计算，实现高效的增量更新。大量实验结果表明，JanusVLN在性能上超过了20多种近期方法，取得了SOTA表现。例如，成功率为10.5%-35.5%优于使用多种数据类型的输入方法，为使用更多RGB训练数据的方法提高了3.6%-10.8%。这表明，提出的双隐式神经记忆作为一种新的范式，探索了未来基于视觉-语言导航研究的有希望的新方向。我们的项目页面：this https URL。', 'title_zh': 'JanusVLN：通过双隐式记忆解藕语义与空间性在视觉语言导航中的应用'}
{'arxiv_id': 'arXiv:2509.22442', 'title': 'Learning to Ball: Composing Policies for Long-Horizon Basketball Moves', 'authors': 'Pei Xu, Zhen Wu, Ruocheng Wang, Vishnu Sarukkai, Kayvon Fatahalian, Ioannis Karamouzas, Victor Zordan, C. Karen Liu', 'link': 'https://arxiv.org/abs/2509.22442', 'abstract': 'Learning a control policy for a multi-phase, long-horizon task, such as basketball maneuvers, remains challenging for reinforcement learning approaches due to the need for seamless policy composition and transitions between skills. A long-horizon task typically consists of distinct subtasks with well-defined goals, separated by transitional subtasks with unclear goals but critical to the success of the entire task. Existing methods like the mixture of experts and skill chaining struggle with tasks where individual policies do not share significant commonly explored states or lack well-defined initial and terminal states between different phases. In this paper, we introduce a novel policy integration framework to enable the composition of drastically different motor skills in multi-phase long-horizon tasks with ill-defined intermediate states. Based on that, we further introduce a high-level soft router to enable seamless and robust transitions between the subtasks. We evaluate our framework on a set of fundamental basketball skills and challenging transitions. Policies trained by our approach can effectively control the simulated character to interact with the ball and accomplish the long-horizon task specified by real-time user commands, without relying on ball trajectory references.', 'abstract_zh': '多阶段长时间任务中学习控制策略，如篮球动作 maneuvers，对于强化学习方法来说仍然具有挑战性，因为需要实现无缝的策略组合和技能之间的过渡。长时间任务通常由具有明确目标的独立子任务组成，但由具有模糊目标但在整个任务成功中至关重要的过渡子任务分隔。现有方法如专家混合和技能链在个体策略之间没有显著共享的探索状态或不同阶段之间缺乏明确的初始和终态的任务中表现不佳。在本文中，我们提出了一种新的策略集成框架，以实现多阶段长时间任务中具有模糊中间状态的大幅度不同运动技能的组合。在此基础上，我们进一步引入了一个高层软路由，以实现子任务之间的无缝和鲁棒过渡。我们通过一组基本的篮球技能和具有挑战性的过渡任务来评估该框架。通过我们方法训练得到的策略能够在不依赖球轨迹参考的情况下有效控制模拟角色与球互动，并实现由实时用户指令指定的长时间任务。', 'title_zh': '学习运球：组成长时_horizon篮球动作策略'}
{'arxiv_id': 'arXiv:2509.22407', 'title': 'EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer', 'authors': 'Zhehao Dong, Xiaofeng Wang, Zheng Zhu, Yirui Wang, Yang Wang, Yukun Zhou, Boyuan Wang, Chaojun Ni, Runqi Ouyang, Wenkang Qin, Xinze Chen, Yun Ye, Guan Huang', 'link': 'https://arxiv.org/abs/2509.22407', 'abstract': 'Vision-language-action (VLA) models increasingly rely on diverse training data to achieve robust generalization. However, collecting large-scale real-world robot manipulation data across varied object appearances and environmental conditions remains prohibitively time-consuming and expensive. To overcome this bottleneck, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates a generative data engine with an effective training pipeline. We introduce DreamTransfer, a diffusion Transformer-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos. DreamTransfer enables text-controlled visual editing of robot videos, transforming foreground, background, and lighting conditions without compromising 3D structure or geometrical plausibility. Furthermore, we explore hybrid training with real and generated data, and introduce AdaMix, a hard-sample-aware training strategy that dynamically reweights training batches to focus optimization on perceptually or kinematically challenging samples. Extensive experiments show that videos generated by DreamTransfer significantly outperform prior video generation methods in multi-view consistency, geometric fidelity, and text-conditioning accuracy. Crucially, VLAs trained with generated data enable robots to generalize to unseen object categories and novel visual domains using only demonstrations from a single appearance. In real-world robotic manipulation tasks with zero-shot visual domains, our approach achieves over a 200% relative performance gain compared to training on real data alone, and further improves by 13% with AdaMix, demonstrating its effectiveness in boosting policy generalization.', 'abstract_zh': 'Vision-语言-行动（VLA）模型越来越多地依赖多样化的训练数据以实现稳健的泛化。然而，收集跨越不同物体外观和环境条件的大规模真实世界机器人操作数据仍然是一个耗时且昂贵的问题。为克服这一瓶颈，我们提出了Embodied Manipulation Media Adaptation (EMMA)，一种将生成数据引擎与有效训练管道集成的VLA策略增强框架。我们引入了DreamTransfer，一种基于扩散Transformer的框架，用于生成多视角一致、几何学基础扎实的实体操作视频。DreamTransfer允许对机器人视频进行文本控制的视觉编辑，改变前景、背景和照明条件而不损害3D结构或几何合理性。此外，我们探索了使用真实数据和生成数据的混合训练，并引入了AdaMix，一种关注难样本的训练策略，动态调整训练批次的权重，以专注于感知或动态上具有挑战性的样本。大量实验表明，由DreamTransfer生成的视频在多视角一致性、几何保真度和文本条件准确性方面显著优于之前的视频生成方法。至关重要的是，使用生成数据训练的VLA能够仅通过单一外观的演示就泛化到未见过的对象类别和新的视觉领域。在零样本视觉领域的实际机器人操作任务中，我们的方法相较于仅使用真实数据训练的性能提高了超过200%，并且通过AdaMix进一步提高了13%，这表明其在提升策略泛化方面具有有效性。', 'title_zh': 'EMMA: 通过生成视觉转移实现现实世界机器人操作的泛化'}
{'arxiv_id': 'arXiv:2509.22402', 'title': 'ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation', 'authors': 'Nan Tang, Jing-Cheng Pang, Guanlin Li, Chao Qian, Yang Yu', 'link': 'https://arxiv.org/abs/2509.22402', 'abstract': "Reward design remains a critical bottleneck in visual reinforcement learning (RL) for robotic manipulation. In simulated environments, rewards are conventionally designed based on the distance to a target position. However, such precise positional information is often unavailable in real-world visual settings due to sensory and perceptual limitations. In this study, we propose a method that implicitly infers spatial distances through keypoints extracted from images. Building on this, we introduce Reward Learning with Anticipation Model (ReLAM), a novel framework that automatically generates dense, structured rewards from action-free video demonstrations. ReLAM first learns an anticipation model that serves as a planner and proposes intermediate keypoint-based subgoals on the optimal path to the final goal, creating a structured learning curriculum directly aligned with the task's geometric objectives. Based on the anticipated subgoals, a continuous reward signal is provided to train a low-level, goal-conditioned policy under the hierarchical reinforcement learning (HRL) framework with provable sub-optimality bound. Extensive experiments on complex, long-horizon manipulation tasks show that ReLAM significantly accelerates learning and achieves superior performance compared to state-of-the-art methods.", 'abstract_zh': '视觉强化学习（RL）中用于机器人操作的奖励设计仍然是一个关键瓶颈。在模拟环境中，奖励通常基于到达目标位置的距离进行设计。然而，在现实世界的视觉设置中，由于感觉和知觉的限制，这种精确的位置信息往往不可用。在本研究中，我们提出了一种通过从图像中提取的关键点隐式推断空间距离的方法。在此基础上，我们引入了预见奖励学习模型（ReLAM），这是一种新型框架，可以从无动作视频演示中自动生成密集的结构化奖励。ReLAM 首先学习一个预见模型，作为规划器，在最优路径到最终目标的过程中提出基于关键点的中间子目标，从而直接构建与任务几何目标对齐的结构化学习课程。基于预见的子目标，提供连续的奖励信号，在分层强化学习（HRL）框架下训练低层次的目标条件策略，并具有可证明的次优性界。在复杂的长时程操作任务上的广泛实验表明，ReLAM 显著加速了学习并达到了优于现有方法的性能。', 'title_zh': 'ReLAM: 学习预判模型以奖励视觉机器人操作'}
{'arxiv_id': 'arXiv:2509.22379', 'title': 'A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems', 'authors': 'Stefano Carlo Lambertenghi, Mirena Flores Valdez, Andrea Stocco', 'link': 'https://arxiv.org/abs/2509.22379', 'abstract': 'Simulation-based testing is a cornerstone of Autonomous Driving System (ADS) development, offering safe and scalable evaluation across diverse driving scenarios. However, discrepancies between simulated and real-world behavior, known as the reality gap, challenge the transferability of test results to deployed systems. In this paper, we present a comprehensive empirical study comparing four representative testing modalities: Software-in-the-Loop (SiL), Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing. Using a small-scale physical vehicle equipped with real sensors (camera and LiDAR) and its digital twin, we implement each setup and evaluate two ADS architectures (modular and end-to-end) across diverse indoor driving scenarios involving real obstacles, road topologies, and indoor environments. We systematically assess the impact of each testing modality along three dimensions of the reality gap: actuation, perception, and behavioral fidelity. Our results show that while SiL and ViL setups simplify critical aspects of real-world dynamics and sensing, MR testing improves perceptual realism without compromising safety or control. Importantly, we identify the conditions under which failures do not transfer across testing modalities and isolate the underlying dimensions of the gap responsible for these discrepancies. Our findings offer actionable insights into the respective strengths and limitations of each modality and outline a path toward more robust and transferable validation of autonomous driving systems.', 'abstract_zh': '基于仿真测试在自动驾驶系统开发中的作用：解决现实差距的全面实证研究', 'title_zh': '多模态自主驾驶系统现实差距评估'}
{'arxiv_id': 'arXiv:2509.22298', 'title': 'Trust and Human Autonomy after Cobot Failures: Communication is Key for Industry 5.0', 'authors': 'Felix Glawe, Laura Kremer, Luisa Vervier, Philipp Brauner, Martina Ziefle', 'link': 'https://arxiv.org/abs/2509.22298', 'abstract': "Collaborative robots (cobots) are a core technology of Industry 4.0. Industry 4.0 uses cyber-physical systems, IoT and smart automation to improve efficiency and data-driven decision-making. Cobots, as cyber-physical systems, enable the introduction of lightweight automation to smaller companies through their flexibility, low cost and ability to work alongside humans, while keeping humans and their skills in the loop. Industry 5.0, the evolution of Industry 4.0, places the worker at the centre of its principles: The physical and mental well-being of the worker is the main goal of new technology design, not just productivity, efficiency and safety standards. Within this concept, human trust in cobots and human autonomy are important. While trust is essential for effective and smooth interaction, the workers' perception of autonomy is key to intrinsic motivation and overall well-being. As failures are an inevitable part of technological systems, this study aims to answer the question of how system failures affect trust in cobots as well as human autonomy, and how they can be recovered afterwards. Therefore, a VR experiment (n = 39) was set up to investigate the influence of a cobot failure and its severity on human autonomy and trust in the cobot. Furthermore, the influence of transparent communication about the failure and next steps was investigated. The results show that both trust and autonomy suffer after cobot failures, with the severity of the failure having a stronger negative impact on trust, but not on autonomy. Both trust and autonomy can be partially restored by transparent communication.", 'abstract_zh': '协作机器人（cobots）是 Industry 4.0 的核心技术。Industry 4.0 通过网络物理系统、物联网和智能自动化提高效率和基于数据的决策。作为网络物理系统，协作机器人通过其灵活性、低成本和能够与人类合作的能力，使轻量级自动化技术能够应用到较小的公司，同时确保人类及其技能的参与。Industry 5.0 是 Industry 4.0 的进化，将工人置于其原则的中心：工人的身心健康是新技术设计的主要目标，而不仅仅是生产力、效率和安全标准。在此概念下，工人对协作机器人的信任和人类自主性非常重要。虽然信任对于有效和顺畅的交互至关重要，但工人的自主感对于内在动机和整体福祉是关键。由于技术系统的失败是不可避免的，本研究旨在探究系统失败如何影响工人对协作机器人的信任以及人类的自主性，以及如何进行恢复。因此，我们设置了 VR 实验（n=39）来调查协作机器人失败及其严重程度对人类自主性及对协作机器人信任的影响，还考察了透明沟通关于失败及其后续步骤的影响。结果表明，协作机器人失败后，信任和自主性都会受到影响，其中失败的严重性对信任的影响更为显著，但对自主性的影响较小。通过透明沟通，信任和自主性都可以部分恢复。', 'title_zh': '协作机器人故障后的人机信任与自主性：Industry 5.0 中的关键在于沟通'}
{'arxiv_id': 'arXiv:2509.22281', 'title': 'MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning', 'authors': 'Jinkun Hao, Naifu Liang, Zhen Luo, Xudong Xu, Weipeng Zhong, Ran Yi, Yichen Jin, Zhaoyang Lyu, Feng Zheng, Lizhuang Ma, Jiangmiao Pang', 'link': 'https://arxiv.org/abs/2509.22281', 'abstract': 'The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training. However, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks. In this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes. To support research on such a challenging task, we introduce MesaTask-10K, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with manually crafted layouts that ensure realistic layouts and intricate inter-object relations. To bridge the gap between tasks and scenes, we propose a Spatial Reasoning Chain that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout. We present MesaTask, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions. Exhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts. Project page is at this https URL', 'abstract_zh': '机器人解读人类指令并执行操作任务的能力需要有相关任务的桌面场景进行训练。然而，传统方法创建这些场景依赖于耗费时间的手动布局设计或纯粹随机的布局，这在可信度或与任务的一致性方面存在局限性。本文提出了一种新的任务，即面向任务的桌面场景生成，由于高层任务指令与桌面场景之间存在显著差距，这一任务提出了重大的挑战。为了支持对这一具有挑战性任务的研究，我们介绍了MesaTask-10K，一个包含约10,700个手工设计布局的合成桌面场景的大规模数据集，确保了真实的布局和复杂的物体间关系。为了弥合任务与场景之间的差距，我们提出了一种空间推理链，将生成过程分解为对象推理、空间关系推理和最终3D布局的场景图构建。我们提出了MesaTask，一个基于LLM的框架，利用这一推理链，并进一步使用DPO算法生成与给定任务描述高度一致的、物理上合理的桌面场景。全面的实验表明，MesaTask在生成符合任务描述的真实布局桌面场景方面优于基准方法。项目页面网址为：https://cekunpeng.github.io/mesatask/', 'title_zh': 'MesaTask: 基于3D空间推理的面向任务的桌面场景生成'}
{'arxiv_id': 'arXiv:2509.22271', 'title': 'Human Autonomy and Sense of Agency in Human-Robot Interaction: A Systematic Literature Review', 'authors': 'Felix Glawe, Tim Schmeckel, Philipp Brauner, Martina Ziefle', 'link': 'https://arxiv.org/abs/2509.22271', 'abstract': 'Human autonomy and sense of agency are increasingly recognised as critical for user well-being, motivation, and the ethical deployment of robots in human-robot interaction (HRI). Given the rapid development of artificial intelligence, robot capabilities and their potential to function as colleagues and companions are growing. This systematic literature review synthesises 22 empirical studies selected from an initial pool of 728 articles published between 2011 and 2024. Articles were retrieved from major scientific databases and identified based on empirical focus and conceptual relevance, namely, how to preserve and promote human autonomy and sense of agency in HRI. Derived through thematic synthesis, five clusters of potentially influential factors are revealed: robot adaptiveness, communication style, anthropomorphism, presence of a robot and individual differences. Measured through psychometric scales or the intentional binding paradigm, perceptions of autonomy and agency varied across industrial, educational, healthcare, care, and hospitality settings. The review underscores the theoretical differences between both concepts, but their yet entangled use in HRI. Despite increasing interest, the current body of empirical evidence remains limited and fragmented, underscoring the necessity for standardised definitions, more robust operationalisations, and further exploratory and qualitative research. By identifying existing gaps and highlighting emerging trends, this review contributes to the development of human-centered, autonomy-supportive robot design strategies that uphold ethical and psychological principles, ultimately supporting well-being in human-robot interaction.', 'abstract_zh': '人类自主性与代理感在用户福祉、动机及机器人在人机交互中的伦理部署中的重要性日益受到认可。鉴于人工智能的快速发展，机器人的能力和其作为同事和伴侣的潜力正在增长。本系统文献综述综合分析了2011年至2024年间发表的728篇文章中筛选出的22篇实证研究，探讨如何在人机交互中保存和促进人类的自主性与代理感。通过主题综合分析，揭示了五个可能有影响力的因素群：机器人适应性、沟通方式、拟人化、机器人的存在以及个体差异。通过心理测量量表或意图绑定范式评估，人类在工业、教育、健康护理、照护和酒店业等不同场景中的自主性和代理感感知有所不同。综述突出了这两个概念在理论上的差异，但它们在人机交互中仍然纠缠不清地使用。尽管人们对这一领域越来越感兴趣，但目前的实证证据仍然有限且碎片化，强调需要标准化定义、更稳健的操作化以及进一步的探索性和质性研究。通过识别现有差距并强调新兴趋势，本综述贡献了以人为本、支持自主性的机器人设计策略，最终支持人机交互中的福祉，并遵循伦理和心理原则。', 'title_zh': '人类自主性和主观操控感在人机交互中的作用：一项系统文献综述'}
{'arxiv_id': 'arXiv:2509.22137', 'title': 'Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach', 'authors': 'Seoyoung Lee, Seonbin Yoon, Seongbeen Lee, Hyesoo Kim, Joo Yong Sim', 'link': 'https://arxiv.org/abs/2509.22137', 'abstract': 'GUI task automation streamlines repetitive tasks, but existing LLM or VLM-based planner-executor agents suffer from brittle generalization, high latency, and limited long-horizon coherence. Their reliance on single-shot reasoning or static plans makes them fragile under UI changes or complex tasks. Log2Plan addresses these limitations by combining a structured two-level planning framework with a task mining approach over user behavior logs, enabling robust and adaptable GUI automation. Log2Plan constructs high-level plans by mapping user commands to a structured task dictionary, enabling consistent and generalizable automation. To support personalization and reuse, it employs a task mining approach from user behavior logs that identifies user-specific patterns. These high-level plans are then grounded into low-level action sequences by interpreting real-time GUI context, ensuring robust execution across varying interfaces. We evaluated Log2Plan on 200 real-world tasks, demonstrating significant improvements in task success rate and execution time. Notably, it maintains over 60.0% success rate even on long-horizon task sequences, highlighting its robustness in complex, multi-step workflows.', 'abstract_zh': 'GUI任务自动化简化了重复任务，但现有的基于LLM或VLM的规划执行代理在通用性、延迟和长时间连贯性方面存在缺陷。它们依赖单次推理或静态计划，这使它们在界面更改或复杂任务面前变得脆弱。Log2Plan通过结合结构化的两层规划框架和基于用户行为日志的任务挖掘方法，解决了这些问题，实现了稳健且灵活的GUI自动化。Log2Plan通过将用户命令映射到结构化任务字典来构建高级计划，从而实现一致且可泛化的自动化。为支持个性化和重用，Log2Plan采用从用户行为日志中识别用户特定模式的任务挖掘方法。这些高级计划随后通过解释实时GUI上下文，转化为具体的行动序列，确保在不同界面下执行的稳健性。我们对Log2Plan进行了200个真实世界的任务评估，展示了在任务成功率和执行时间方面的重要改进。特别是在长时间序列任务中，Log2Plan保持着超过60.0%的成功率，突显了其在复杂、多步工作流中的鲁棒性。', 'title_zh': 'Log2Plan: 一种结合任务挖掘方法的自适应GUI自动化框架'}
{'arxiv_id': 'arXiv:2509.22014', 'title': 'Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics', 'authors': 'Saurav Jha, Stefan K. Ehrlich', 'link': 'https://arxiv.org/abs/2509.22014', 'abstract': 'Healthcare robotics requires robust multimodal perception and reasoning to ensure safety in dynamic clinical environments. Current Vision-Language Models (VLMs) demonstrate strong general-purpose capabilities but remain limited in temporal reasoning, uncertainty estimation, and structured outputs needed for robotic planning. We present a lightweight agentic multimodal framework for video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer, it supports chain-of-thought reasoning, speech-vision fusion, and dynamic tool invocation. The framework generates structured scene graphs and leverages a hybrid retrieval module for interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark and a custom clinical dataset show competitive accuracy and improved robustness compared to state-of-the-art VLMs, demonstrating its potential for applications in robot-assisted surgery, patient monitoring, and decision support.', 'abstract_zh': '医疗机器人需要强大的多模态感知与推理能力以确保在动态临床环境中的安全性。当前的视觉-语言模型（VLMs）展现出较强的通用能力，但在时间推理、不确定性估计和机器人规划所需的结构化输出方面仍有限制。我们提出了一种轻量级的兼具自主性的多模态框架，用于基于视频的场景理解。将Qwen2.5-VL-3B-Instruct模型与SmolAgent基于的编排层结合，支持链式推理、视听融合以及动态工具调用。该框架生成结构化的场景图，并利用混合检索模块实现可解释和自适应的推理。在Video-MME基准和自定义的临床数据集上的评估表明，该框架在准确性和鲁棒性方面与最先进的VLMs具有竞争力，并展示了其在辅助手术、患者监控和决策支持等方面的应用潜力。', 'title_zh': '轻量级结构化多模态推理在机器人临床场景理解中应用'}
{'arxiv_id': 'arXiv:2509.21930', 'title': 'DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation', 'authors': 'Jiahui Wang, Changhao Chen', 'link': 'https://arxiv.org/abs/2509.21930', 'abstract': 'Visual navigation is essential for robotics and embodied AI. However, existing foundation models, particularly those with transformer decoders, suffer from high computational overhead and lack interpretability, limiting their deployment in resource-tight scenarios. To address this, we propose DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer selection based on scene complexity. It employs a trainable hard feature selector for sparse operations, enhancing efficiency and interpretability. Additionally, we integrate feature selection into an early-exit mechanism, with Bayesian Optimization determining optimal exit thresholds to reduce computational cost. Extensive experiments in real-world-based datasets and simulated environments demonstrate the effectiveness of DynaNav. Compared to ViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time, and 32.8% lower memory usage, while improving navigation performance across four public datasets.', 'abstract_zh': '动态可视化导航框架DynaNav在基于场景复杂性的特征和层选择中实现了高效性和可解释性，并通过集成特征选择到早期退出机制中，利用贝叶斯优化确定最优退出阈值以降低计算成本。广泛的实验证明了DynaNav的有效性。与ViNT相比，DynaNav在FLOPs上减少2.26倍，推理时间降低42.3%，内存使用减少32.8%，同时在四个公开数据集上提高了导航性能。', 'title_zh': 'DynaNav: 动态特征与层选择的高效视觉导航'}
{'arxiv_id': 'arXiv:2509.21464', 'title': 'Residual Vector Quantization For Communication-Efficient Multi-Agent Perception', 'authors': 'Dereje Shenkut, B.V.K Vijaya Kumar', 'link': 'https://arxiv.org/abs/2509.21464', 'abstract': 'Multi-agent collaborative perception (CP) improves scene understanding by sharing information across connected agents such as autonomous vehicles, unmanned aerial vehicles, and robots. Communication bandwidth, however, constrains scalability. We present ReVQom, a learned feature codec that preserves spatial identity while compressing intermediate features. ReVQom is an end-to-end method that compresses feature dimensions via a simple bottleneck network followed by multi-stage residual vector quantization (RVQ). This allows only per-pixel code indices to be transmitted, reducing payloads from 8192 bits per pixel (bpp) of uncompressed 32-bit float features to 6-30 bpp per agent with minimal accuracy loss. On DAIR-V2X real-world CP dataset, ReVQom achieves 273x compression at 30 bpp to 1365x compression at 6 bpp. At 18 bpp (455x), ReVQom matches or outperforms raw-feature CP, and at 6-12 bpp it enables ultra-low-bandwidth operation with graceful degradation. ReVQom allows efficient and accurate multi-agent collaborative perception with a step toward practical V2X deployment.', 'abstract_zh': '基于学习的特征编解码器ReVQom在保持空间身份的同时压缩中间特征，以提高多智能体协作感知的效率和准确性', 'title_zh': '基于通信高效性的多agent感知残差向量量化'}
{'arxiv_id': 'arXiv:2509.21405', 'title': 'Object Identification Under Known Dynamics: A PIRNN Approach for UAV Classification', 'authors': 'Nyi Nyi Aung, Neil Muralles, Adrian Stein', 'link': 'https://arxiv.org/abs/2509.21405', 'abstract': 'This work addresses object identification under known dynamics in unmanned aerial vehicle applications, where learning and classification are combined through a physics-informed residual neural network. The proposed framework leverages physics-informed learning for state mapping and state-derivative prediction, while a softmax layer enables multi-class confidence estimation. Quadcopter, fixed-wing, and helicopter aerial vehicles are considered as case studies. The results demonstrate high classification accuracy with reduced training time, offering a promising solution for system identification problems in domains where the underlying dynamics are well understood.', 'abstract_zh': '本研究探讨了在无人驾驶飞行器应用中已知动力学下的目标识别问题，通过物理信息残差神经网络结合学习与分类。所提出的方法利用物理信息学习进行状态映射和状态导数预测，同时softmax层实现多类别置信度估计。四旋翼机、固定翼机和直升机被用作案例研究。结果表明，该方法具有较高的分类准确性并减少了训练时间，为动力学规律明确领域的系统识别问题提供了有前景的解决方案。', 'title_zh': '已知动力学下的物体识别：一种用于无人机分类的PIRNN方法'}
{'arxiv_id': 'arXiv:2509.21386', 'title': 'ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar Data', 'authors': 'Anja Sheppard, Tyler Smithline, Andrew Scheffer, David Smith, Advaith V. Sethuraman, Ryan Bird, Sabrina Lin, Katherine A. Skinner', 'link': 'https://arxiv.org/abs/2509.21386', 'abstract': 'In this paper, we introduce ShipwreckFinder, an open-source QGIS plugin that detects shipwrecks from multibeam sonar data. Shipwrecks are an important historical marker of maritime history, and can be discovered through manual inspection of bathymetric data. However, this is a time-consuming process and often requires expert analysis. Our proposed tool allows users to automatically preprocess bathymetry data, perform deep learning inference, threshold model outputs, and produce either pixel-wise segmentation masks or bounding boxes of predicted shipwrecks. The backbone of this open-source tool is a deep learning model, which is trained on a variety of shipwreck data from the Great Lakes and the coasts of Ireland. Additionally, we employ synthetic data generation in order to increase the size and diversity of our dataset. We demonstrate superior segmentation performance with our open-source tool and training pipeline as compared to a deep learning-based ArcGIS toolkit and a more classical inverse sinkhole detection method. The open-source tool can be found at this https URL.', 'abstract_zh': '本论文引入了ShipwreckFinder，一个开源QGIS插件，用于从多束声纳数据中检测沉船。', 'title_zh': 'ShipwreckFinder：一个用于多波束声纳数据中沉船检测的QGIS工具'}
