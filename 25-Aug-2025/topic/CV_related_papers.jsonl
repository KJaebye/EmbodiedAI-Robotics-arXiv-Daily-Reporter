{'arxiv_id': 'arXiv:2508.15990', 'title': 'GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System', 'authors': 'Hung-Jui Huang, Mohammad Amin Mirzaee, Michael Kaess, Wenzhen Yuan', 'link': 'https://arxiv.org/abs/2508.15990', 'abstract': "Accurately perceiving an object's pose and shape is essential for precise grasping and manipulation. Compared to common vision-based methods, tactile sensing offers advantages in precision and immunity to occlusion when tracking and reconstructing objects in contact. This makes it particularly valuable for in-hand and other high-precision manipulation tasks. In this work, we present GelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing to estimate object pose over long periods and reconstruct object shapes with high fidelity. Unlike traditional point cloud-based approaches, GelSLAM uses tactile-derived surface normals and curvatures for robust tracking and loop closure. It can track object motion in real time with low error and minimal drift, and reconstruct shapes with submillimeter accuracy, even for low-texture objects such as wooden tools. GelSLAM extends tactile sensing beyond local contact to enable global, long-horizon spatial perception, and we believe it will serve as a foundation for many precise manipulation tasks involving interaction with objects in hand. The video demo is available on our website: this https URL.", 'abstract_zh': '准确感知物体的姿态和形状对于精确抓取和操作至关重要。与常见的基于视觉的方法相比，触觉传感在追踪和重构接触中物体时，在精度和对遮挡的免疫性方面具有优势，这使其特别适用于手部和其他高精度操作任务。在此项工作中，我们提出了一种名为GelSLAM的实时3D SLAM系统，该系统仅依赖触觉传感来长时间估计物体姿态并以高度保真度重构物体形状。与传统的基于点云的方法不同，GelSLAM使用触觉衍生的表面法线和曲率进行鲁棒跟踪和回环闭合。它可以实时低误差且低漂移地跟踪物体运动，并且即使对于低纹理物体（如木制工具），也能以亚毫米级精度重构形状。GelSLAM将触觉传感扩展到局部接触之外，以实现全局、长视程的空间感知，我们认为它将成为许多涉及手部与物体交互的精确操作任务的基础。更多内容请参见我们的网站：this https URL。', 'title_zh': 'GelSLAM：一种实时、高保真且鲁棒的3D触觉SLAM系统'}
{'arxiv_id': 'arXiv:2508.15972', 'title': 'UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation', 'authors': 'Zhaodong Jiang, Ashish Sinha, Tongtong Cao, Yuan Ren, Bingbing Liu, Binbin Xu', 'link': 'https://arxiv.org/abs/2508.15972', 'abstract': "Estimating the 6D pose of novel objects is a fundamental yet challenging problem in robotics, often relying on access to object CAD models. However, acquiring such models can be costly and impractical. Recent approaches aim to bypass this requirement by leveraging strong priors from foundation models to reconstruct objects from single or multi-view images, but typically require additional training or produce hallucinated geometry. To this end, we propose UnPose, a novel framework for zero-shot, model-free 6D object pose estimation and reconstruction that exploits 3D priors and uncertainty estimates from a pre-trained diffusion model. Specifically, starting from a single-view RGB-D frame, UnPose uses a multi-view diffusion model to estimate an initial 3D model using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise epistemic uncertainty estimates. As additional observations become available, we incrementally refine the 3DGS model by fusing new views guided by the diffusion model's uncertainty, thereby continuously improving the pose estimation accuracy and 3D reconstruction quality. To ensure global consistency, the diffusion prior-generated views and subsequent observations are further integrated in a pose graph and jointly optimized into a coherent 3DGS field. Extensive experiments demonstrate that UnPose significantly outperforms existing approaches in both 6D pose estimation accuracy and 3D reconstruction quality. We further showcase its practical applicability in real-world robotic manipulation tasks.", 'abstract_zh': '无模型六自由度物体姿态估计与重建', 'title_zh': 'UnPose：面向零样本姿态估计的不确定性引导扩散先验'}
{'arxiv_id': 'arXiv:2508.16465', 'title': 'HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images', 'authors': 'Anilkumar Swamy, Vincent Leroy, Philippe Weinzaepfel, Jean-Sébastien Franco, Grégory Rogez', 'link': 'https://arxiv.org/abs/2508.16465', 'abstract': 'Hand-object 3D reconstruction has become increasingly important for applications in human-robot interaction and immersive AR/VR experiences. A common approach for object-agnostic hand-object reconstruction from RGB sequences involves a two-stage pipeline: hand-object 3D tracking followed by multi-view 3D reconstruction. However, existing methods rely on keypoint detection techniques, such as Structure from Motion (SfM) and hand-keypoint optimization, which struggle with diverse object geometries, weak textures, and mutual hand-object occlusions, limiting scalability and generalization. As a key enabler to generic and seamless, non-intrusive applicability, we propose in this work a robust, keypoint detector-free approach to estimating hand-object 3D transformations from monocular motion video/images. We further integrate this with a multi-view reconstruction pipeline to accurately recover hand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely on pre-scanned object templates or camera intrinsics, and reaches state-of-the-art performance for the tasks of object-agnostic hand-object 3D transformation and shape estimation on the SHOWMe benchmark. We also experiment on sequences from the HO3D dataset, demonstrating generalization to unseen object categories.', 'abstract_zh': '基于单目运动视频/图像的手物3D变换与形状估计', 'title_zh': 'HOSt3R: 无需关键点的手-物体3D重建从RGB图像'}
{'arxiv_id': 'arXiv:2508.16026', 'title': 'NeuralMeshing: Complete Object Mesh Extraction from Casual Captures', 'authors': 'Floris Erich, Naoya Chiba, Abdullah Mustafa, Ryo Hanai, Noriaki Ando, Yusuke Yoshiyasu, Yukiyasu Domae', 'link': 'https://arxiv.org/abs/2508.16026', 'abstract': 'How can we extract complete geometric models of objects that we encounter in our daily life, without having access to commercial 3D scanners? In this paper we present an automated system for generating geometric models of objects from two or more videos. Our system requires the specification of one known point in at least one frame of each video, which can be automatically determined using a fiducial marker such as a checkerboard or Augmented Reality (AR) marker. The remaining frames are automatically positioned in world space by using Structure-from-Motion techniques. By using multiple videos and merging results, a complete object mesh can be generated, without having to rely on hole filling. Code for our system is available from this https URL.', 'abstract_zh': '如何在没有访问商业3D扫描器的情况下提取我们日常生活中遇到的物体的完整几何模型？在本文中，我们提出了一种从两段或多段视频生成物体几何模型的自动化系统。该系统要求在每个视频的至少一帧中指定一个已知点，该点可以使用棋盘格或增强现实（AR）标志等标记物自动确定。剩下的帧通过使用结构恢复运动技术自动定位到世界空间。通过使用多段视频并合并结果，可以生成完整的物体网格，而不需要依赖孔填充。我们的系统代码可以从以下网址获取：this https URL。', 'title_zh': '神经网格化：从随意拍摄中提取完整对象网格'}
{'arxiv_id': 'arXiv:2508.16577', 'title': 'MV-RAG: Retrieval Augmented Multiview Diffusion', 'authors': 'Yosef Dayani, Omer Benishu, Sagie Benaim', 'link': 'https://arxiv.org/abs/2508.16577', 'abstract': 'Text-to-3D generation approaches have advanced significantly by leveraging pretrained 2D diffusion priors, producing high-quality and 3D-consistent outputs. However, they often fail to produce out-of-domain (OOD) or rare concepts, yielding inconsistent or inaccurate results. To this end, we propose MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images from a large in-the-wild 2D database and then conditions a multiview diffusion model on these images to synthesize consistent and accurate multiview outputs. Training such a retrieval-conditioned model is achieved via a novel hybrid strategy bridging structured multiview data and diverse 2D image collections. This involves training on multiview data using augmented conditioning views that simulate retrieval variance for view-specific reconstruction, alongside training on sets of retrieved real-world 2D images using a distinctive held-out view prediction objective: the model predicts the held-out view from the other views to infer 3D consistency from 2D data. To facilitate a rigorous OOD evaluation, we introduce a new collection of challenging OOD prompts. Experiments against state-of-the-art text-to-3D, image-to-3D, and personalization baselines show that our approach significantly improves 3D consistency, photorealism, and text adherence for OOD/rare concepts, while maintaining competitive performance on standard benchmarks.', 'abstract_zh': '利用预训练的2D扩散先验的文本到3D生成方法取得了显著进展，产生了高质量且3D一致的输出。然而，它们往往无法生成跨域（OOD）或稀有概念，导致不一致或不准确的结果。为此，我们提出MV-RAG，这是一种新颖的文本到3D管道，首先从大型野外2D数据库中检索相关2D图像，然后基于这些图像对多视角扩散模型进行条件化，以生成一致且准确的多视角输出。通过一种新颖的混合策略对这种检索条件化的模型进行训练，该策略结合了结构化的多视角数据和多样的2D图像集合。这包括使用增强的条件视图训练多视角数据，这些视图模拟了视图特定重构的检索变化，同时使用一组检索的现实世界2D图像进行训练，采用独特的剔除视图预测目标：模型从其他视图预测剔除视图，从而从2D数据中推断3D一致性。为确保严格的OOD评估，我们引入了一组具有挑战性的OOD提示。与最先进的文本到3D、图像到3D和个人化基线方法相比，我们的方法显著提高了跨域/稀有概念的3D一致性、照片真实感和文本遵循性，同时在标准基准上保持了竞争力。', 'title_zh': 'MV-RAG: 提取增强多视图扩散'}
{'arxiv_id': 'arXiv:2508.16557', 'title': 'Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution', 'authors': 'Tainyi Zhang, Zheng-Peng Duan, Peng-Tao Jiang, Bo Li, Ming-Ming Cheng, Chun-Le Guo, Chongyi Li', 'link': 'https://arxiv.org/abs/2508.16557', 'abstract': "Diffusion-based real-world image super-resolution (Real-ISR) methods have demonstrated impressive performance. To achieve efficient Real-ISR, many works employ Variational Score Distillation (VSD) to distill pre-trained stable-diffusion (SD) model for one-step SR with a fixed timestep. However, due to the different noise injection timesteps, the SD will perform different generative priors. Therefore, a fixed timestep is difficult for these methods to fully leverage the generative priors in SD, leading to suboptimal performance. To address this, we propose a Time-Aware one-step Diffusion Network for Real-ISR (TADSR). We first introduce a Time-Aware VAE Encoder, which projects the same image into different latent features based on timesteps. Through joint dynamic variation of timesteps and latent features, the student model can better align with the input pattern distribution of the pre-trained SD, thereby enabling more effective utilization of SD's generative capabilities. To better activate the generative prior of SD at different timesteps, we propose a Time-Aware VSD loss that bridges the timesteps of the student model and those of the teacher model, thereby producing more consistent generative prior guidance conditioned on timesteps. Additionally, though utilizing the generative prior in SD at different timesteps, our method can naturally achieve controllable trade-offs between fidelity and realism by changing the timestep condition. Experimental results demonstrate that our method achieves both state-of-the-art performance and controllable SR results with only a single step.", 'abstract_zh': '基于扩散的现实世界图像超分辨率（Real-ISR）方法展示了出色的性能。为了实现高效的Real-ISR，许多工作利用变分分数蒸馏（VSD）将预训练的稳定扩散（SD）模型蒸馏出来，以固定时间步进行单步SR。然而，由于不同的噪声注入时间步，SD将表现出不同的生成先验。因此，固定时间步对于这些方法来说难以充分利用SD的生成先验，导致性能不佳。为了解决这一问题，我们提出了一种时间感知的单步扩散网络（TADSR）用于Real-ISR。我们首先引入一种时间感知的VAE编码器，根据时间步将同一图像投影到不同的潜在特征中。通过联合动态变化时间步和潜在特征，学生模型可以更好地与预训练SD的输入模式分布对齐，从而更有效地利用SD的生成能力。为了在不同时间步更好地激活SD的生成先验，我们提出了一种时间感知的VSD损失，该损失连接学生模型和教师模型的时间步，从而在条件时间步下生成更加一致的生成先验指导。此外，通过利用不同时间步的生成先验，我们的方法可以通过改变时间步条件自然地实现保真度与真实感之间的可控权衡。实验结果表明，我们的方法仅通过单步即可实现最先进的性能和可控的SR结果。', 'title_zh': '时空意识一歩扩散网络for实时图像超分辨率'}
{'arxiv_id': 'arXiv:2508.16527', 'title': 'Towards Open World Detection: A Survey', 'authors': 'Andrei-Stefan Bulzan, Cosmin Cernazanu-Glavan', 'link': 'https://arxiv.org/abs/2508.16527', 'abstract': "For decades, Computer Vision has aimed at enabling machines to perceive the external world. Initial limitations led to the development of highly specialized niches. As success in each task accrued and research progressed, increasingly complex perception tasks emerged. This survey charts the convergence of these tasks and, in doing so, introduces Open World Detection (OWD), an umbrella term we propose to unify class-agnostic and generally applicable detection models in the vision domain. We start from the history of foundational vision subdomains and cover key concepts, methodologies and datasets making up today's state-of-the-art landscape. This traverses topics starting from early saliency detection, foreground/background separation, out of distribution detection and leading up to open world object detection, zero-shot detection and Vision Large Language Models (VLLMs). We explore the overlap between these subdomains, their increasing convergence, and their potential to unify into a singular domain in the future, perception.", 'abstract_zh': '几十年来，计算机视觉致力于使机器能够感知外部世界。最初的局限性导致了高度专业化的领域的发展。随着每个任务的成功和研究的进展，越来越复杂的感知任务出现了。本文综述了这些任务的趋同过程，并以此提出了开放式世界检测（OWD），这是一种涵盖无类别泛化和视觉领域中一般适用检测模型的统称术语。我们从基础视觉子领域的历史出发，涵盖了构成当今顶尖技术景观的关键概念、方法和数据集。这涵盖了从早期的显著性检测、前景/背景分离、异常检测，一直到开放世界物体检测、零样本检测以及视觉大语言模型（VLLMs）。我们探讨了这些子领域之间的重叠、它们不断增加的趋同性及其未来有可能统一为单一感知领域的潜力。', 'title_zh': '向开放世界检测迈进：一项综述'}
{'arxiv_id': 'arXiv:2508.16397', 'title': 'A Lightweight Group Multiscale Bidirectional Interactive Network for Real-Time Steel Surface Defect Detection', 'authors': 'Yong Zhang, Cunjian Chen, Qiang Gao, Yi Wang, Bin Fang', 'link': 'https://arxiv.org/abs/2508.16397', 'abstract': 'Real-time surface defect detection is critical for maintaining product quality and production efficiency in the steel manufacturing industry. Despite promising accuracy, existing deep learning methods often suffer from high computational complexity and slow inference speeds, which limit their deployment in resource-constrained industrial environments. Recent lightweight approaches adopt multibranch architectures based on depthwise separable convolution (DSConv) to capture multiscale contextual information. However, these methods often suffer from increased computational overhead and lack effective cross-scale feature interaction, limiting their ability to fully leverage multiscale representations. To address these challenges, we propose GMBINet, a lightweight framework that enhances multiscale feature extraction and interaction through novel Group Multiscale Bidirectional Interactive (GMBI) modules. The GMBI adopts a group-wise strategy for multiscale feature extraction, ensuring scale-agnostic computational complexity. It further integrates a Bidirectional Progressive Feature Interactor (BPFI) and a parameter-free Element-Wise Multiplication-Summation (EWMS) operation to enhance cross-scale interaction without introducing additional computational overhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that GMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU and 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters. Additional evaluations on the NEU-CLS defect classification dataset further confirm the strong generalization ability of our method, demonstrating its potential for broader industrial vision applications beyond surface defect detection. The dataset and code are publicly available at: this https URL.', 'abstract_zh': '实时表面缺陷检测对于维护钢铁制造行业的产品质量和生产效率至关重要。尽管现有深度学习方法具有较高的准确率，但往往面临高计算复杂度和慢推理速度的问题，限制了其在资源受限的工业环境中的部署。最近的轻量级方法采用基于深度可分离卷积（DSConv）的多分支架构以捕获多尺度上下文信息。然而，这些方法往往增加了计算开销，并缺乏有效的跨尺度特征交互，限制了其充分利用多尺度表示的能力。为解决这些挑战，我们提出了GMBINet，这是一种轻量级框架，通过新颖的Group Multiscale Bidirectional Interactive (GMBI) 模块增强多尺度特征提取和交互。GMBI采用组级策略进行多尺度特征提取，确保计算复杂度与尺度无关。并且进一步整合了Bidirectional Progressive Feature Interactor (BPFI) 和参数免费的Element-Wise Multiplication-Summation (EWMS) 操作，增强了跨尺度交互而不引入额外的计算开销。实验结果表明，GMBINet 在 GPU 上以 1048 FPS、CPU 上以 16.53 FPS 的实时速度达到 512 分辨率时，仅使用 0.19 M 参数即可实现具有竞争力的准确率。在 NEU-CLS 缺陷分类数据集上的额外评估进一步证实了该方法的强泛化能力，展示了其在表面缺陷检测以外的更广泛工业视觉应用的潜力。数据集和代码可在以下链接获取：this https URL。', 'title_zh': '轻量级组多尺度双向互动网络[edge for 实时钢表面缺陷检测]'}
{'arxiv_id': 'arXiv:2508.16311', 'title': 'Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers', 'authors': 'Lucas Maisonnave, Karim Haroun, Tom Pegeot', 'link': 'https://arxiv.org/abs/2508.16311', 'abstract': 'Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where each attention head contributes to the final representation. However, their computational complexity and high memory demands due to MHSA hinders their deployment at the edge. In this work, we analyze and exploit information redundancy in attention maps to accelerate model inference. By quantifying the information captured by each attention head using Shannon entropy, our analysis reveals that attention heads with lower entropy, i.e., exhibiting more deterministic behavior, tend to contribute less information, motivating targeted compression strategies. Relying on these insights, we propose Entropy Attention Maps (EAM), a model that freezes the weights of low-entropy attention maps and quantizes these values to low precision to avoid redundant re-computation. Empirical validation on ImageNet-1k shows that EAM achieves similar or higher accuracy at $\\leq$20\\% sparsity in attention maps and competitive performance beyond this level for the DeiT and Swin Transformer models.', 'abstract_zh': 'Transformer模型依赖多头自注意力（MHSA）机制，其中每个注意力头都为最终表示做出贡献。然而，由于MHSA带来的计算复杂度和高内存需求限制了其在边缘设备上的部署。在本文中，我们分析并利用注意力图中的信息冗余来加速模型推理。通过使用香农熵量化每个注意力头捕获的信息，我们的分析表明，低熵的注意力头，即表现出更确定行为的注意力头，倾向于贡献较少信息，从而推动有针对性的压缩策略。基于这些洞见，我们提出了一种熵注意力图（EAM）模型，该模型冻结低熵注意力图的权重并将这些值量化为低精度以避免冗余重计算。在ImageNet-1k上的实证验证表明，EAM在注意力图稀疏度≤20%时可实现类似或更高的精度，并且对于DeiT和Swin Transformer模型，在此稀疏度之外仍具有竞争力的表现。', 'title_zh': '利用注意力图中的信息冗余进行视觉变换器的极端量化'}
{'arxiv_id': 'arXiv:2508.16225', 'title': 'An Investigation of Visual Foundation Models Robustness', 'authors': 'Sandeep Gupta, Roberto Passerone', 'link': 'https://arxiv.org/abs/2508.16225', 'abstract': 'Visual Foundation Models (VFMs) are becoming ubiquitous in computer vision, powering systems for diverse tasks such as object detection, image classification, segmentation, pose estimation, and motion tracking. VFMs are capitalizing on seminal innovations in deep learning models, such as LeNet-5, AlexNet, ResNet, VGGNet, InceptionNet, DenseNet, YOLO, and ViT, to deliver superior performance across a range of critical computer vision applications. These include security-sensitive domains like biometric verification, autonomous vehicle perception, and medical image analysis, where robustness is essential to fostering trust between technology and the end-users. This article investigates network robustness requirements crucial in computer vision systems to adapt effectively to dynamic environments influenced by factors such as lighting, weather conditions, and sensor characteristics. We examine the prevalent empirical defenses and robust training employed to enhance vision network robustness against real-world challenges such as distributional shifts, noisy and spatially distorted inputs, and adversarial attacks. Subsequently, we provide a comprehensive analysis of the challenges associated with these defense mechanisms, including network properties and components to guide ablation studies and benchmarking metrics to evaluate network robustness.', 'abstract_zh': '视觉基础模型（VFMs）在计算机视觉中无处不在，为物体检测、图像分类、分割、姿态估计和动作跟踪等多种任务提供动力。VFMs 利用诸如 LeNet-5、AlexNet、ResNet、VGGNet、InceptionNet、DenseNet、YOLO 和 ViT 等深度学习模型的开创性创新，为从生物特征验证到自动驾驶车辆感知，再到医学图像分析等多个关键计算机视觉应用领域提供卓越性能。这些应用领域对鲁棒性要求非常高，以在技术与最终用户之间建立信任。本文研究计算机视觉系统中至关重要的网络鲁棒性要求，以适应由光照、天气条件和传感器特性等因素影响的动态环境。我们探讨了广泛使用的经验性防御手段和鲁棒训练方法，以增强视觉网络在面对分布漂移、噪声和空间失真输入以及对抗攻击等现实挑战时的鲁棒性。随后，我们对这些防护机制所面临的挑战进行全面分析，包括网络特性与组件，以指导消融研究，并提供评估网络鲁棒性的基准度量标准。', 'title_zh': '视觉基础模型的稳健性研究'}
{'arxiv_id': 'arXiv:2508.16089', 'title': 'Two-flow Feedback Multi-scale Progressive Generative Adversarial Network', 'authors': 'Sun Weikai, Song Shijie, Chi Wenjie', 'link': 'https://arxiv.org/abs/2508.16089', 'abstract': 'Although diffusion model has made good progress in the field of image generation, GAN\\cite{huang2023adaptive} still has a large development space due to its unique advantages, such as WGAN\\cite{liu2021comparing}, SSGAN\\cite{guibas2021adaptive} \\cite{zhang2022vsa} \\cite{zhou2024adapt} and so on. In this paper, we propose a novel two-flow feedback multi-scale progressive generative adversarial network (MSPG-SEN) for GAN models. This paper has four contributions: 1) : We propose a two-flow feedback multi-scale progressive Generative Adversarial network (MSPG-SEN), which not only improves image quality and human visual perception on the basis of retaining the advantages of the existing GAN model, but also simplifies the training process and reduces the training cost of GAN networks. Our experimental results show that, MSPG-SEN has achieved state-of-the-art generation results on the following five datasets,INKK The dataset is 89.7\\%,AWUN The dataset is 78.3\\%,IONJ The dataset is 85.5\\%,POKL The dataset is 88.7\\%,OPIN The dataset is 96.4\\%. 2) : We propose an adaptive perception-behavioral feedback loop (APFL), which effectively improves the robustness and training stability of the model and reduces the training cost. 3) : We propose a globally connected two-flow dynamic residual network(). After ablation experiments, it can effectively improve the training efficiency and greatly improve the generalization ability, with stronger flexibility. 4) : We propose a new dynamic embedded attention mechanism (DEMA). After experiments, the attention can be extended to a variety of image processing tasks, which can effectively capture global-local information, improve feature separation capability and feature expression capabilities, and requires minimal computing resources only 88.7\\% with INJK With strong cross-task capability.', 'abstract_zh': '虽然扩散模型在图像生成领域取得了良好进展，但生成模型（GAN）依然有着巨大的发展空间，得益于其独特的优势，如WGAN、SSGAN及其变种等。本文提出了一种新颖的双流反馈多尺度渐进生成对抗网络（MSPG-SEN）以提升GAN模型。本文有四个贡献：1）我们提出了一种双流反馈多尺度渐进生成对抗网络（MSPG-SEN），不仅在保留现有GAN模型优势的同时，提高了图像质量和人类视觉感知，还简化了训练过程并降低了训练成本。实验结果显示，MSPG-SEN在以下五个数据集上实现了最先进的生成结果：INKK数据集是89.7%，AWUN数据集是78.3%，IONJ数据集是85.5%，POKL数据集是88.7%，OPIN数据集是96.4%。2）我们提出了一种自适应感知行为反馈环（APFL），有效提高了模型的鲁棒性和训练稳定性，并降低了训练成本。3）我们提出了一种全局连接的双流动态残差网络。经过消融实验，它能有效提高训练效率并大大增强泛化能力，具有更强的灵活性。4）我们提出了一种新的动态嵌入注意机制（DEMA）。经过实验，注意力机制可以扩展到多种图像处理任务，能够有效捕获全局-局部信息，提高特征分离能力和特征表达能力，并且仅需88.7%的计算资源，具有强跨任务能力。', 'title_zh': '两流反馈多尺度渐进生成对抗网络'}
{'arxiv_id': 'arXiv:2508.16030', 'title': 'CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars', 'authors': 'Jinyue Song, Hansol Ku, Jayneel Vora, Nelson Lee, Ahmad Kamari, Prasant Mohapatra, Parth Pathak', 'link': 'https://arxiv.org/abs/2508.16030', 'abstract': 'Automotive FMCW radars remain reliable in rain and glare, yet their sparse, noisy point clouds constrain 3-D object detection. We therefore release CoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, and GPS streams from multiple vehicles across diverse manoeuvres. Built on this data, we propose a unified cooperative-perception framework with middle- and late-fusion options. Its baseline network employs a multi-branch PointNet-style encoder enhanced with self-attention to fuse spatial, Doppler, and intensity cues into a common latent space, which a decoder converts into 3-D bounding boxes and per-point depth confidence. Experiments show that middle fusion with intensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 and consistently outperforms single-vehicle baselines. CoVeRaP thus establishes the first reproducible benchmark for multi-vehicle FMCW-radar perception and demonstrates that affordable radar sharing markedly improves detection robustness. Dataset and code are publicly available to encourage further research.', 'abstract_zh': '汽车FMCW雷达在雨天和强光下保持可靠性能，但稀疏且噪声较大的点云限制了三维目标检测。因此，我们发布了一个由多个车辆在多种操作下时间对齐的雷达、摄像头和GPS流组成的CoVeRaP合作数据集，共包含21 k帧数据。基于此数据，我们提出了一种统一的合作感知框架，包括中间融合和晚期融合选项。基线网络采用一种多分支PointNet样式的编码器，并通过自我注意力机制融合空间、多普勒和强度线索，将其转换到公共潜在空间中，解码器将其转化为三维边界框和每个点的深度置信度。实验结果显示，使用强度编码的中间融合在IoU 0.9时平均精确度均值最多可提高9倍，并且始终优于单车辆基线。因此，CoVeRaP确立了首个可再现的多车辆FMCW雷达感知基准，证明了负担得起的雷达共享显著提高了检测鲁棒性。数据集和代码已公开，以促进进一步研究。', 'title_zh': 'CoVeRaP: 合作式车联网毫米波FMCW雷达感知'}
{'arxiv_id': 'arXiv:2508.15986', 'title': 'Automated Multi-label Classification of Eleven Retinal Diseases: A Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic Dataset', 'authors': 'Jerry Cao-Xue, Tien Comlekoglu, Keyi Xue, Guanliang Wang, Jiang Li, Gordon Laurie', 'link': 'https://arxiv.org/abs/2508.15986', 'abstract': 'The development of multi-label deep learning models for retinal disease classification is often hindered by the scarcity of large, expertly annotated clinical datasets due to patient privacy concerns and high costs. The recent release of SynFundus-1M, a high-fidelity synthetic dataset with over one million fundus images, presents a novel opportunity to overcome these barriers. To establish a foundational performance benchmark for this new resource, we developed an end-to-end deep learning pipeline, training six modern architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and the RETFound foundation model) to classify eleven retinal diseases using a 5-fold multi-label stratified cross-validation strategy. We further developed a meta-ensemble model by stacking the out-of-fold predictions with an XGBoost classifier. Our final ensemble model achieved the highest performance on the internal validation set, with a macro-average Area Under the Receiver Operating Characteristic Curve (AUC) of 0.9973. Critically, the models demonstrated strong generalization to three diverse, real-world clinical datasets, achieving an AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGS glaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset. This work provides a robust baseline for future research on large-scale synthetic datasets and establishes that models trained exclusively on synthetic data can accurately classify multiple pathologies and generalize effectively to real clinical images, offering a viable pathway to accelerate the development of comprehensive AI systems in ophthalmology.', 'abstract_zh': '多标签深度学习模型在视网膜疾病分类的应用受大型专家标注临床数据集稀缺的限制，主要原因在于患者隐私担忧和高成本。最近发布的SynFundus-1M，这是一个高保真度的合成数据集，包含超过一百万张视网膜影像，为克服这一障碍提供了新机遇。为建立该新资源的基础性能基准，我们开发了一个端到端的深度学习管道，使用五折多标签分层交叉验证策略，对包括ConvNeXtV2、SwinV2、ViT、ResNet、EfficientNetV2和RETFound基础模型在内的六种现代架构进行训练，以分类十一种视网膜疾病。我们进一步通过堆叠出折预测与XGBoost分类器构建了一个元集成模型。最终，我们的集成模型在内部验证集上达到了最高的性能，宏平均受试者操作特征曲线下面积(AUC)为0.9973。关键的是，模型在三个不同真实临床数据集上展现出良好的泛化能力，分别在DR综合数据集中达到0.7972的AUC，在AIROGS青光眼数据集中达到0.9126的AUC，在多标签RFMiD数据集中达到0.8800的宏平均AUC。本研究为大规模合成数据集的未来研究提供了稳健的基准，并证明了仅使用合成数据训练的模型可以准确分类多种病理且有效泛化到真实临床影像，为加速眼科学中全面AI系统的开发提供了可行途径。', 'title_zh': '自动多标签分类 Eleven 种视网膜疾病：现代架构基准及大规模合成数据集上的元集成方法'}
{'arxiv_id': 'arXiv:2508.15985', 'title': 'Panoptic Segmentation of Environmental UAV Images : Litter Beach', 'authors': 'Ousmane Youme, Jean Marie Dembélé, Eugene C. Ezin, Christophe Cambier', 'link': 'https://arxiv.org/abs/2508.15985', 'abstract': 'Convolutional neural networks (CNN) have been used efficiently in several fields, including environmental challenges. In fact, CNN can help with the monitoring of marine litter, which has become a worldwide problem. UAVs have higher resolution and are more adaptable in local areas than satellite images, making it easier to find and count trash. Since the sand is heterogeneous, a basic CNN model encounters plenty of inferences caused by reflections of sand color, human footsteps, shadows, algae present, dunes, holes, and tire tracks. For these types of images, other CNN models, such as CNN-based segmentation methods, may be more appropriate. In this paper, we use an instance-based segmentation method and a panoptic segmentation method that show good accuracy with just a few samples. The model is more robust and less', 'abstract_zh': '卷积神经网络（CNN）在多个领域中得到了有效应用，包括环境挑战。实际上，CNN能够在监测海洋垃圾方面发挥重要作用，而海洋垃圾已成为全球性问题。无人机具有更高的分辨率且在局部区域适应性更强，因此在寻找和统计垃圾方面更具优势。由于沙滩是异质的，基础的CNN模型会遇到由沙色反射、人为足迹、阴影、藻类存在、沙丘、坑洞和轮胎痕迹等引起的大量推断问题。对于这类图像，基于CNN的分割方法，如实例分割方法和全景分割方法，可能更为合适。在本文中，我们使用了实例分割方法和全景分割方法，这两种方法仅通过少量样本就能显示出良好的准确性。该模型更具鲁棒性且更为简化。', 'title_zh': '环境 UAV 图像的全景分割：海滩垃圾识别'}
{'arxiv_id': 'arXiv:2508.15959', 'title': 'Representation Learning with Adaptive Superpixel Coding', 'authors': 'Mahmoud Khalil, Ahmad Khalil, Alioune Ngom', 'link': 'https://arxiv.org/abs/2508.15959', 'abstract': 'Deep learning vision models are typically tailored for specific modalities and often rely on domain-specific assumptions, such as the grid structures used by nearly all existing vision models. In this work, we propose a self-supervised model based on Transformers, which we call Adaptive Superpixel Coding (ASC). The key insight of our model is to overcome the limitations of traditional Vision Transformers, which depend on fixed-size and non-adaptive patch partitioning. Instead, ASC employs adaptive superpixel layers that dynamically adjust to the underlying image content. We analyze key properties of the approach that make it effective, and find that our method outperforms widely-used alternatives on standard image downstream task benchmarks.', 'abstract_zh': '基于变换器的自监督适配超像素编码（ASC）：克服传统视觉变换器的固定大小非自适应 patch 分割限制', 'title_zh': '自适应超像素编码的表示学习'}
{'arxiv_id': 'arXiv:2508.15883', 'title': 'Beyond Imaging: Vision Transformer Digital Twin Surrogates for 3D+T Biological Tissue Dynamics', 'authors': 'Kaan Berke Ugurlar, Joaquín de Navascués, Michael Taynnan Barros', 'link': 'https://arxiv.org/abs/2508.15883', 'abstract': "Understanding the dynamic organization and homeostasis of living tissues requires high-resolution, time-resolved imaging coupled with methods capable of extracting interpretable, predictive insights from complex datasets. Here, we present the Vision Transformer Digital Twin Surrogate Network (VT-DTSN), a deep learning framework for predictive modeling of 3D+T imaging data from biological tissue. By leveraging Vision Transformers pretrained with DINO (Self-Distillation with NO Labels) and employing a multi-view fusion strategy, VT-DTSN learns to reconstruct high-fidelity, time-resolved dynamics of a Drosophila midgut while preserving morphological and feature-level integrity across imaging depths. The model is trained with a composite loss prioritizing pixel-level accuracy, perceptual structure, and feature-space alignment, ensuring biologically meaningful outputs suitable for in silico experimentation and hypothesis testing. Evaluation across layers and biological replicates demonstrates VT-DTSN's robustness and consistency, achieving low error rates and high structural similarity while maintaining efficient inference through model optimization. This work establishes VT-DTSN as a feasible, high-fidelity surrogate for cross-timepoint reconstruction and for studying tissue dynamics, enabling computational exploration of cellular behaviors and homeostasis to complement time-resolved imaging studies in biological research.", 'abstract_zh': '理解生物组织的动态组织和稳态需要高分辨率的时间分辨成像，并结合能够从复杂数据集中提取可解释和预测性洞见的方法。在此，我们提出了Vision Transformer数字双胞胎代理网络(VT-DTSN)，这是一种用于预测建模生物组织3D+T成像数据的深度学习框架。通过利用通过DINO（无标签自精炼）预训练的Vision Transformers并采用多视图融合策略，VT-DTSN学习重建果蝇中肠的时间分辨动态，同时在成像深度上保持形态学和特征级别的完整性。该模型采用综合损失函数优先考虑像素级精度、感知结构和特征空间对齐，确保产生符合生物学意义的输出，适用于计算机模拟实验和假设检验。跨层和生物重复的评估表明，VT-DTSN具有稳健性和一致性，实现了低错误率和高结构相似性，同时通过模型优化保持高效的推理。这项工作将VT-DTSN确立为跨时间点重建和研究组织动态的可行高保真代理，促进了对细胞行为和稳态的计算探索，以补充生物学研究中的时间分辨成像研究。', 'title_zh': '超越成像：用于3D+T生物组织动态的视觉变换器数字孪生代理'}
{'arxiv_id': 'arXiv:2508.15782', 'title': 'Learning in Focus: Detecting Behavioral and Collaborative Engagement Using Vision Transformers', 'authors': 'Sindhuja Penchala, Saketh Reddy Kontham, Prachi Bhattacharjee, Sareh Karami, Mehdi Ghahremani, Noorbakhsh Amiri Golilarz, Shahram Rahimi', 'link': 'https://arxiv.org/abs/2508.15782', 'abstract': "In early childhood education, accurately detecting behavioral and collaborative engagement is essential for fostering meaningful learning experiences. This paper presents an AI-driven approach that leverages Vision Transformers (ViTs) to automatically classify children's engagement using visual cues such as gaze direction, interaction, and peer collaboration. Utilizing the Child-Play gaze dataset, our method is trained on annotated video segments to classify behavioral and collaborative engagement states (e.g., engaged, not engaged, collaborative, not collaborative). We evaluated three state-of-the-art transformer models: Vision Transformer (ViT), Data-efficient Image Transformer (DeiT), and Swin Transformer. Among these, the Swin Transformer achieved the highest classification performance with an accuracy of 97.58%, demonstrating its effectiveness in modeling local and global attention. Our results highlight the potential of transformer-based architectures for scalable, automated engagement analysis in real-world educational settings.", 'abstract_zh': '在早期 Childhood 教育中，准确检测行为性和协作性参与对于培养有意义的学习体验至关重要。本文提出了一种基于 Vision Transformers (ViTs) 的 AI 驱动方法，利用视觉得到的线索（如注视方向、互动和同伴协作）自动分类儿童的参与状态。利用 Child-Play 注视数据集，我们的方法通过对标注的视频片段进行训练，以分类行为性和协作性参与状态（例如，参与、不参与、协作、不协作）。我们评估了三种最先进的 transformer 模型：Vision Transformer (ViT)、Data-efficient Image Transformer (DeiT) 和 Swin Transformer。在这三种模型中，Swin Transformer 达到了最高的分类性能，准确率为 97.58%，展示了其在建模局部和全局注意力方面的效果。研究结果突显了基于 transformer 架构在实际教育场景中进行可扩展、自动参与分析的潜力。', 'title_zh': '聚焦学习：使用视觉转换器检测行为性和合作性参与'}
