{'arxiv_id': 'arXiv:2508.16571', 'title': 'LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence', 'authors': 'Alisa Vinogradova, Vlad Vinogradov, Dmitrii Radkevich, Ilya Yasny, Dmitry Kobyzev, Ivan Izmailov, Katsiaryna Yanchanka, Andrey Doronichev', 'link': 'https://arxiv.org/abs/2508.16571', 'abstract': "In this paper, we describe and benchmark a competitor-discovery component used within an agentic AI system for fast drug asset due diligence. A competitor-discovery AI agent, given an indication, retrieves all drugs comprising the competitive landscape of that indication and extracts canonical attributes for these drugs. The competitor definition is investor-specific, and data is paywalled/licensed, fragmented across registries, ontology-mismatched by indication, alias-heavy for drug names, multimodal, and rapidly changing. Although considered the best tool for this problem, the current LLM-based AI systems aren't capable of reliably retrieving all competing drug names, and there is no accepted public benchmark for this task. To address the lack of evaluation, we use LLM-based agents to transform five years of multi-modal, unstructured diligence memos from a private biotech VC fund into a structured evaluation corpus mapping indications to competitor drugs with normalized attributes. We also introduce a competitor validating LLM-as-a-judge agent that filters out false positives from the list of predicted competitors to maximize precision and suppress hallucinations. On this benchmark, our competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research (65%) and Perplexity Labs (60%). The system is deployed in production with enterprise users; in a case study with a biotech VC investment fund, analyst turnaround time dropped from 2.5 days to $\\sim$3 hours ($\\sim$20x) for the competitive analysis.", 'abstract_zh': '在本研究中，我们描述并评估了一个用于快速药物资产尽职调查的竞品发现组件，该组件集成于一个自主AI系统中。一个竞品发现AI代理，给定一个信号，检索所有构成该信号竞争格局的药物，并提取这些药物的 canonical 属性。竞品定义为投资者特定的，数据是付费的/许可的，分布在不同的注册表中，每种信号的药物名称有同义词困扰、多模态的，并且快速变化。尽管被认为是解决此问题的最佳工具，当前基于大语言模型的AI系统还不具备可靠地检索所有竞品药物名称的能力，而且没有公认的公开基准用于此任务。为了解决评估缺乏的问题，我们使用基于大语言模型的代理将一家私人生物技术风险投资基金五年的多模态、未结构化的尽职调查备忘录转换为一个结构化的评估库，该库将信号映射到具有规范属性的竞品药物。此外，我们引入了一种验证竞品的基于大语言模型的代理作为法官，用于从预测的竞品列表中过滤掉假阳性，以最大化精确度并抑制幻觉。在这一基准上，我们所提出的竞品发现代理达到83%的召回率，超过OpenAI Deep Research的65%和Perplexity Labs的60%。该系统已在企业用户中部署；在一个生物技术风险投资案例研究中，分析师的竞争分析周转时间从2.5天降至约3小时（约20倍）。', 'title_zh': '基于LLM的代理在药物资产尽职调查中的竞争格局映射'}
{'arxiv_id': 'arXiv:2508.16383', 'title': 'GLARE: Agentic Reasoning for Legal Judgment Prediction', 'authors': 'Xinyu Yang, Chenlong Deng, Zhicheng Dou', 'link': 'https://arxiv.org/abs/2508.16383', 'abstract': 'Legal judgment prediction (LJP) has become increasingly important in the legal field. In this paper, we identify that existing large language models (LLMs) have significant problems of insufficient reasoning due to a lack of legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning framework that dynamically acquires key legal knowledge by invoking different modules, thereby improving the breadth and depth of reasoning. Experiments conducted on the real-world dataset verify the effectiveness of our method. Furthermore, the reasoning chain generated during the analysis process can increase interpretability and provide the possibility for practical applications.', 'abstract_zh': '法律判决预测（LJP）在法律领域中变得 increasingly important。在本文中，我们识别出现有大型语言模型（LLMs）由于缺乏法律知识而在推理方面存在显著不足。因此，我们引入了GLARE，这是一种动态获取关键法律知识的代理法律推理框架，通过调用不同的模块来提高推理的广度和深度。在真实数据集上的实验验证了我们方法的有效性。此外，分析过程中生成的推理链可以增强可解释性并为实际应用提供可能性。', 'title_zh': 'GLARE: 主体性推理在法律判决预测中的应用'}
{'arxiv_id': 'arXiv:2508.16279', 'title': 'AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications', 'authors': 'Dawei Gao, Zitao Li, Yuexiang Xie, Weirui Kuang, Liuyi Yao, Bingchen Qian, Zhijian Ma, Yue Cui, Haohao Luo, Shen Li, Lu Yi, Yi Yu, Shiqi He, Zhiling Luo, Wenmeng Zhou, Zhicheng Zhang, Xuguang He, Ziqian Chen, Weikai Liao, Farruh Isakulovich Kushnazarov, Yaliang Li, Bolin Ding, Jingren Zhou', 'link': 'https://arxiv.org/abs/2508.16279', 'abstract': 'Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications.', 'abstract_zh': '受到大型语言模型（LLMs）飞速发展的推动，代理能够结合内在知识与动态工具使用，极大地增强了其应对现实世界任务的能力。紧跟这一演变，AgentScope 在新版本（1.0）中引入了重大改进，旨在全面支持灵活高效的基于工具的代理-环境交互，以构建代理应用。具体而言，我们抽离了代理应用所需的基础组件，并提供了统一的接口和可扩展模块，使开发者能够轻松利用最新进展，如新模型和MCP。此外，我们基于ReAct范式将代理行为予以实体化，并提供基于系统异步设计的高级代理级基础设施，这不仅丰富了人类-代理和代理-代理交互模式，还提高了执行效率。在此基础上，我们整合了适应特定实际场景的内置代理。AgentScope 还提供了强大的工程支持，创建了对开发者友好的开发体验。我们提供了一个可扩展的评估模块，具有可视化工具界面，使长轨迹代理应用的开发更为便捷且易于追踪。此外，AgentScope 还提供了运行时沙箱以确保代理的安全执行，并促进了生产环境中的快速部署。通过这些增强，AgentScope 提供了构建可扩展、自适应和有效的代理应用的实用基础。', 'title_zh': 'AgentScope 1.0：以开发者为中心的代理性应用构建框架'}
{'arxiv_id': 'arXiv:2508.16172', 'title': 'Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain', 'authors': 'Kai Hu, Parfait Atchade-Adelomou, Carlo Adornetto, Adrian Mora-Carrero, Luis Alonso-Pastor, Ariel Noyman, Yubo Liu, Kent Larson', 'link': 'https://arxiv.org/abs/2508.16172', 'abstract': 'Understanding human behavior in urban environments is a crucial field within city sciences. However, collecting accurate behavioral data, particularly in newly developed areas, poses significant challenges. Recent advances in generative agents, powered by Large Language Models (LLMs), have shown promise in simulating human behaviors without relying on extensive datasets. Nevertheless, these methods often struggle with generating consistent, context-sensitive, and realistic behavioral outputs. To address these limitations, this paper introduces the Preference Chain, a novel method that integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance context-aware simulation of human behavior in transportation systems. Experiments conducted on the Replica dataset demonstrate that the Preference Chain outperforms standard LLM in aligning with real-world transportation mode choices. The development of the Mobility Agent highlights potential applications of proposed method in urban mobility modeling for emerging cities, personalized travel behavior analysis, and dynamic traffic forecasting. Despite limitations such as slow inference and the risk of hallucination, the method offers a promising framework for simulating complex human behavior in data-scarce environments, where traditional data-driven models struggle due to limited data availability.', 'abstract_zh': '理解城市环境中的人类行为是城市科学中的一个重要领域。然而，在新建区域收集准确的行为数据面临着重大挑战。近期由大型语言模型（LLMs）驱动的生成代理的发展显示出在无需大量数据集的情况下模拟人类行为的潜力。然而，这些方法在生成一致、上下文相关和现实主义的行为输出方面往往存在困难。为了解决这些限制，本文提出了一种名为偏好链的新方法，该方法结合了图检索增强生成（RAG）与LLMs，以增强交通系统中人类行为的上下文感知模拟。实验结果表明，偏好链在与真实世界的出行模式选择一致性方面优于标准的LLM。移动代理的发展展示了所提出方法在新兴城市交通 modeling、个性化旅行行为分析和动态交通预测中的潜在应用。尽管存在推理速度慢和出现幻觉的风险，该方法为数据稀缺环境中模拟复杂人类行为提供了有前景的框架，在传统数据驱动模型由于数据可用性有限而难以应对的情况下尤为适用。', 'title_zh': '基于图RAG的人类选择模型：构建一个基于偏好链的数据驱动移动代理'}
{'arxiv_id': 'arXiv:2508.16112', 'title': 'IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra', 'authors': 'Heewoong Noh, Namkyeong Lee, Gyoung S. Na, Kibum Kim, Chanyoung Park', 'link': 'https://arxiv.org/abs/2508.16112', 'abstract': 'Spectral analysis provides crucial clues for the elucidation of unknown materials. Among various techniques, infrared spectroscopy (IR) plays an important role in laboratory settings due to its high accessibility and low cost. However, existing approaches often fail to reflect expert analytical processes and lack flexibility in incorporating diverse types of chemical knowledge, which is essential in real-world analytical scenarios. In this paper, we propose IR-Agent, a novel multi-agent framework for molecular structure elucidation from IR spectra. The framework is designed to emulate expert-driven IR analysis procedures and is inherently extensible. Each agent specializes in a specific aspect of IR interpretation, and their complementary roles enable integrated reasoning, thereby improving the overall accuracy of structure elucidation. Through extensive experiments, we demonstrate that IR-Agent not only improves baseline performance on experimental IR spectra but also shows strong adaptability to various forms of chemical information.', 'abstract_zh': '红外光谱分析提供了未知材料解析的关键线索。在各种技术中，由于其高可访问性和低成本，红外光谱法（IR）在实验室环境中发挥着重要作用。然而，现有方法往往无法反映专家分析过程，且在结合不同类型的化学知识方面缺乏灵活性，这对于实际的分析场景至关重要。在本文中，我们提出了一种新的多代理框架IR-Agent，用于从红外光谱中解析分子结构。该框架旨在模拟专家驱动的红外分析过程，并具有内在的可扩展性。每个代理专注于红外解释的特定方面，它们互补的角色使得能够进行集成推理，从而提高整体结构解析的准确性。通过大量的实验，我们展示了IR-Agent不仅在基准性能上改进了实验红外光谱的表现，还展示了对各种形式的化学信息的强烈适应性。', 'title_zh': 'IR-Agent: 专家启发的LLM智能体用于红外光谱结构解析'}
{'arxiv_id': 'arXiv:2508.16072', 'title': 'InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles', 'authors': 'Zizhen Li, Chuanhao Li, Yibin Wang, Qi Chen, Diping Song, Yukang Feng, Jianwen Sun, Jiaxin Ai, Fanrui Zhang, Mingzhu Sun, Kaipeng Zhang', 'link': 'https://arxiv.org/abs/2508.16072', 'abstract': "LLMs have shown strong performance on human-centric reasoning tasks. While previous evaluations have explored whether LLMs can infer intentions or detect deception, they often overlook the individualized reasoning styles that influence how people interpret and act in social contexts. Social deduction games (SDGs) provide a natural testbed for evaluating individualized reasoning styles, where different players may adopt diverse but contextually valid reasoning strategies under identical conditions. To address this, we introduce InMind, a cognitively grounded evaluation framework designed to assess whether LLMs can capture and apply personalized reasoning styles in SDGs. InMind enhances structured gameplay data with round-level strategy traces and post-game reflections, collected under both Observer and Participant modes. It supports four cognitively motivated tasks that jointly evaluate both static alignment and dynamic adaptation. As a case study, we apply InMind to the game Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o frequently rely on lexical cues, struggling to anchor reflections in temporal gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These findings reveal key limitations in current LLMs' capacity for individualized, adaptive reasoning, and position InMind as a step toward cognitively aligned human-AI interaction.", 'abstract_zh': 'LLMs在基于人类的推理任务中表现出强大性能。虽然先前的评估探索了LLMs是否能够推断意图或检测欺骗，但它们往往忽视了影响人们在社会情境中解释与行动的个性化推理风格。社会推理游戏（SDGs）为评估个性化推理风格提供了一个自然的测试平台，在相同的条件下，不同的玩家可能会采用不同的但又是情境合理的推理策略。为应对这一挑战，我们引入InMind，这是一个认知基础的评估框架，旨在评估LLMs是否能够捕捉和应用SDGs中的个性化推理风格。InMind通过在观察者模式和参与者模式下收集的轮次级策略轨迹和游戏后反思，增强了结构化的游戏玩法数据，并支持四种认知驱动的任务，联合评估静态对齐和动态适应。作为案例研究，我们使用InMind对Avalon游戏进行了评估，测试了11种最先进的LLM。通用的LLM，即使是GPT-4o，往往依赖于词汇线索，难以将反思锚定在时间的游戏过程中或适应不断变化的策略。相比之下，增强推理能力的LLM如DeepSeek-R1显示出敏感推理的早期迹象。这些发现揭示了当前LLMs在个性化和适应性推理方面的关键局限性，并将InMind定位为走向认知对齐的人机交互的重要一步。', 'title_zh': 'InMind：评估LLM在捕捉和应用个体人类推理风格方面的能力'}
{'arxiv_id': 'arXiv:2508.16059', 'title': 'Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting', 'authors': 'Zhuomin Chen, Dan Li, Jiahui Zhou, Shunyu Wu, Haozheng Ye, Jian Lou, See-Kiong Ng', 'link': 'https://arxiv.org/abs/2508.16059', 'abstract': 'Time series (TS) data are ubiquitous across various application areas, rendering time series forecasting (TSF) a fundamental task. With the astounding advances in large language models (LLMs), a variety of methods have been developed to adapt LLMs for time series forecasting. Despite unlocking the potential of LLMs in comprehending TS data, existing methods are inherently constrained by their shallow integration of TS information, wherein LLMs typically access TS representations at shallow layers, primarily at the input layer. This causes the influence of TS representations to progressively fade in deeper layers and eventually leads to ineffective adaptation between textual embeddings and TS representations. In this paper, we propose the Multi-layer Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to directly access time series patterns at all depths, thereby mitigating the progressive loss of TS information in deeper layers. Specifically, MSEF leverages off-the-shelf time series foundation models to extract semantically rich embeddings, which are fused with intermediate text representations across LLM layers via layer-specific steering vectors. These steering vectors are designed to continuously optimize the alignment between time series and textual modalities and facilitate a layer-specific adaptation mechanism that ensures efficient few-shot learning capabilities. Experimental results on seven benchmarks demonstrate significant performance improvements by MSEF compared with baselines, with an average reduction of 31.8% in terms of MSE. The code is available at this https URL.', 'abstract_zh': '多层可控嵌入融合（MSEF）：一种使大型语言模型直接访问时间序列模式的新型框架', 'title_zh': '将时间序列整合到LLMs中以通过多层可引导嵌入融合提升预测'}
{'arxiv_id': 'arXiv:2508.16054', 'title': 'Generative Foundation Model for Structured and Unstructured Electronic Health Records', 'authors': 'Sonish Sivarajkumar, Hang Zhang, Yuelyu Ji, Maneesh Bilalpur, Xizhi Wu, Chenyu Li, Min Gu Kwak, Shyam Visweswaran, Yanshan Wang', 'link': 'https://arxiv.org/abs/2508.16054', 'abstract': "Electronic health records (EHRs) are rich clinical data sources but complex repositories of patient data, spanning structured elements (demographics, vitals, lab results, codes), unstructured clinical notes and other modalities of data. Harnessing this heterogeneity is critical for improving patient outcomes. Recent advances in large language models (LLMs) have enabled foundation models that can learn from multiple data modalities and support clinical tasks. However, most current approaches simply serialize numeric EHR data into text, which risks losing temporal and quantitative detail. We introduce Generative Deep Patient (GDP), a multimodal foundation model that natively encodes structured EHR time-series via a CNN-Transformer encoder and fuses it with unstructured EHRs through cross-modal attention into a LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining, where it learns to produce clinical narratives from raw patient timelines while also performing masked feature prediction (MFP) and next time-step prediction (NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day readmission). In clinical prediction, GDP demonstrated superior performance on MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and 30-day readmission AUROC = 0.627. For narrative generation, GDP achieved ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation, GDP-Instruct scored highest on faithfulness, fluency, and overall clinical utility, suggesting reduced hospital documentation workload without sacrificing accuracy. Our results demonstrate that a single multimodal foundation model can both predict clinically actionable events and generate high-quality clinical narratives. Furthermore, GDP's flexible architecture can be extended to additional modalities.", 'abstract_zh': '多模态电子健康记录生成深度患者模型（GDP）：既能预测临床可操作事件又能生成高质量临床叙述', 'title_zh': '生成式基础模型 forKey structured和unstructured电子健康记录'}
{'arxiv_id': 'arXiv:2508.16560', 'title': 'Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders', 'authors': 'David Chanin, Adrià Garriga-Alonso', 'link': 'https://arxiv.org/abs/2508.16560', 'abstract': 'Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to single concepts. A core SAE training hyperparameter is L0: how many features should fire per token on average. Existing work compares SAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value. In this work we study the effect of L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE fails to learn the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we demonstrate a method to determine the correct L0 value for an SAE on a given training distribution, which finds the true L0 in toy models and coincides with peak sparse probing performance in LLMs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that, to train SAEs with correct features, practitioners must set L0 correctly.', 'abstract_zh': '稀疏自动编码器（SAEs）从大型语言模型内部激活中提取单概念对应的功能。SAE训练中的核心超参数L0是指每个令牌平均应激活多少特征。现有研究通过稀疏性-重建权衡图比较SAE算法，暗示L0没有单一正确的值。本研究探讨了L0对BatchTopK SAEs的影响，并发现如果L0设置不准确，SAE将无法学习到大型语言模型的底层特征。如果L0太低，SAE将混合相关特征以提高重建效果。如果L0太高，SAE将找到退化解，同样会混合特征。此外，我们展示了一种确定给定训练分布下SAE正确L0值的方法，该方法在玩具模型中找到了真实的L0，并且与大型语言模型稀疏探针性能的峰值相吻合。我们发现大多数常用SAE的L0值太低。本研究显示，如果要训练具有正确特征的SAE，实践者必须正确设置L0。', 'title_zh': '稀疏但错误：错误的L0范数导致稀疏自编码器中特征的错误提取'}
{'arxiv_id': 'arXiv:2508.16546', 'title': 'RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs', 'authors': 'Hangzhan Jin, Sicheng Lv, Sifan Wu, Mohammad Hamdaqa', 'link': 'https://arxiv.org/abs/2508.16546', 'abstract': 'Training large language models (LLMs) from scratch is increasingly impractical, making post-training methods such as supervised fine-tuning (SFT) and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern practice. Using an out-of-distribution (OOD) variant of the 24-point card game and new spectrum-based diagnostics, we revisit how these two stages reshape model representation and OOD performance. Our key findings are- (1) RL-FT can restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to 15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and a clear distribution shift, RL-FT cannot fully recover OOD performance. (2) Direction shifts of singular vectors matter more than singular value magnitudes. These shifts concentrate on directions linked to the largest and smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and shallow recovery is effective: restoring singular vector directions for the top 20% of values or first 25% of layers recovers 70-80% of OOD performance. (4) Stronger SFT checkpoints enable better recovery by RL, while overfitted ones resist restoration. These results reconcile prior reports of RL superior OOD performance: RL primarily counteracts SFT-induced directional drift rather than finding new solutions. Our spectrum-aware analysis highlights inexpensive recovery knobs low-rank UV merging and shallow-layer resets that practitioners can use before costly RL fine-tuning.', 'abstract_zh': '训练大规模语言模型从头开始变得越来越不实际，从而使监督微调（SFT）和强化学习微调（RL-FT，例如PPO）成为现代实践中的核心方法。使用24点扑克牌游戏的分布外（OOD）变体和新型频谱诊断方法，我们重新探讨了这两个阶段如何重塑模型表示和分布外性能。我们的主要发现是：（1）RL-FT可以恢复大量来自SFT的分布外性能损失（例如，从Llama-11B的8.97%恢复到15.38%，Qwen-7B的17.09%恢复到19.66%）。但在SFT导致严重过拟合和明显分布转移的情况下，RL-FT不能完全恢复分布外性能。（2）奇异向量的方向变化比奇异值的大小更为重要。这些变化集中在与最大和最小奇异值相关的方向上，从而保持了大部分频谱完整。（3）低秩和浅层恢复有效：恢复前20%的奇异向量方向或前25%层的奇异向量方向可以恢复70-80%的分布外性能。（4）较强的SFT检查点可以使RL更好地恢复性能，而过度拟合的检查点则抵制恢复。这些结果解释了先前关于RL在分布外性能方面优越性报告的原因：RL主要抵消了SFT引起的方向漂移，而不是找到新的解决方案。我们的频谱感知分析突显了低成本的恢复调节器——低秩UV合并和浅层层重置——这些是实践者在昂贵的RL微调之前可以使用的工具。', 'title_zh': 'RL 既非万灵药亦非海市蜃楼：理解基于 supervision vs. 强化学习微调大语言模型'}
{'arxiv_id': 'arXiv:2508.16514', 'title': 'FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline', 'authors': 'Parker Seegmiller, Kartik Mehta, Soumya Saha, Chenyang Tao, Shereen Oraby, Arpit Gupta, Tagyoung Chung, Mohit Bansal, Nanyun Peng', 'link': 'https://arxiv.org/abs/2508.16514', 'abstract': 'Recent works improving LLM math reasoning with synthetic data have used unique setups, making comparison of data synthesis strategies impractical. This leaves many unanswered questions about the roles of different factors in the synthetic data pipeline, such as the impact of filtering low-quality problems. To address this gap, we introduce FLAMES, a Framework for LLM Assessment of Math rEasoning Data Synthesis, and perform a systematic study of 10 existing data synthesis strategies and multiple other factors impacting the performance of synthetic math reasoning data. Our FLAMES experiments provide several valuable insights about the optimal balance of difficulty and diversity of synthetic data. First, data agents designed to increase problem complexity lead to best improvements on most math metrics. Second, with a fixed data generation budget, keeping higher problem coverage is more important than keeping only problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data can lead to improvements on competition-level benchmarks, showcasing easy-to-hard generalization. Leveraging insights from our FLAMES experiments, we design two novel data synthesis strategies for improving out-of-domain generalization and robustness. Further, we develop the FLAMES dataset, an effective blend of our novel and existing data synthesis strategies, outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and Claude 3.5 Sonnet.', 'abstract_zh': 'Recent works improving LLM math reasoning with synthetic data have used unique setups, making comparison of data synthesis strategies impractical. This leaves many unanswered questions about the roles of different factors in the synthetic data pipeline, such as the impact of filtering low-quality problems. To address this gap, we introduce FLAMES, a Framework for LLM Assessment of Math rEasoning Data Synthesis, and perform a systematic study of 10 existing data synthesis strategies and multiple other factors impacting the performance of synthetic math reasoning data. Our FLAMES experiments provide several valuable insights about the optimal balance of difficulty and diversity of synthetic data. First, data agents designed to increase problem complexity lead to best improvements on most math metrics. Second, with a fixed data generation budget, keeping higher problem coverage is more important than keeping only problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data can lead to improvements on competition-level benchmarks, showcasing easy-to-hard generalization. Leveraging insights from our FLAMES experiments, we design two novel data synthesis strategies for improving out-of-domain generalization and robustness. Further, we develop the FLAMES dataset, an effective blend of our novel and existing data synthesis strategies, outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and Claude 3.5 Sonnet.', 'title_zh': 'FLAMES: 通过数据合成管道细粒度分析提高大模型数学推理能力'}
{'arxiv_id': 'arXiv:2508.16439', 'title': 'PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark', 'authors': 'Adil Bahaj, Mounir Ghogho', 'link': 'https://arxiv.org/abs/2508.16439', 'abstract': 'Large language models (LLMs) and vision-augmented LLMs (VLMs) have significantly advanced medical informatics, diagnostics, and decision support. However, these models exhibit systematic biases, particularly age bias, compromising their reliability and equity. This is evident in their poorer performance on pediatric-focused text and visual question-answering tasks. This bias reflects a broader imbalance in medical research, where pediatric studies receive less funding and representation despite the significant disease burden in children. To address these issues, a new comprehensive multi-modal pediatric question-answering benchmark, PediatricsMQA, has been introduced. It consists of 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric topics across seven developmental stages (prenatal to adolescent) and 2,067 vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256 anatomical regions. The dataset was developed using a hybrid manual-automatic pipeline, incorporating peer-reviewed pediatric literature, validated question banks, existing benchmarks, and existing QA resources. Evaluating state-of-the-art open models, we find dramatic performance drops in younger cohorts, highlighting the need for age-aware methods to ensure equitable AI support in pediatric care.', 'abstract_zh': '大型语言模型（LLMs）和视觉增强的语言模型（VLMs）在医学信息学、诊断和决策支持方面取得了显著进展。然而，这些模型存在系统性偏见，尤其是年龄偏见，这损害了它们的可靠性和公平性。这在儿科导向的文本和视觉问答任务中表现得尤为明显。这种偏见反映了更广泛的医学研究中的不平衡，即尽管儿童患病负担重大，但儿科研究获得的资助和代表性却较少。为了应对这些问题，引入了一个新的综合多模态儿科问答基准，儿科MQA（PediatricsMQA）。该基准包括3,417个文本基础的多项选择题（MCQs），覆盖了覆盖七个发育阶段（胎前至青少年）的131个儿科主题，以及使用来自67种影像模态和256个解剖区域的634张儿科图像的2,067个视觉基础的多项选择题。数据集是通过混合手动和自动的工作流开发的，结合了同行评审的儿科文献、验证过的题库、现有基准和现有的问答资源。评估最新的开放模型，我们发现年轻群体的性能显著下降，强调了为了在儿科护理中提供公平的AI支持，需要年龄感知的方法。', 'title_zh': '儿科多模态问答基准PediatricsMQA'}
{'arxiv_id': 'arXiv:2508.16431', 'title': 'Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish', 'authors': 'Yakup Abrek Er, Ilker Kesen, Gözde Gül Şahin, Aykut Erdem', 'link': 'https://arxiv.org/abs/2508.16431', 'abstract': 'We introduce Cetvel, a comprehensive benchmark designed to evaluate large language models (LLMs) in Turkish. Existing Turkish benchmarks often lack either task diversity or culturally relevant content, or both. Cetvel addresses these gaps by combining a broad range of both discriminative and generative tasks ensuring content that reflects the linguistic and cultural richness of Turkish language. Cetvel covers 23 tasks grouped into seven categories, including tasks such as grammatical error correction, machine translation, and question answering rooted in Turkish history and idiomatic language. We evaluate 33 open-weight LLMs (up to 70B parameters) covering different model families and instruction paradigms. Our experiments reveal that Turkish-centric instruction-tuned models generally underperform relative to multilingual or general-purpose models (e.g. Llama 3 and Mistral), despite being tailored for the language. Moreover, we show that tasks such as grammatical error correction and extractive question answering are particularly discriminative in differentiating model capabilities. Cetvel offers a comprehensive and culturally grounded evaluation suite for advancing the development and assessment of LLMs in Turkish.', 'abstract_zh': 'Cetvel：面向土耳其语的大语言模型综合基准测试', 'title_zh': 'Cetvel: 一种评估土耳其语大语言模型语言理解、生成和文化能力的统一基准'}
{'arxiv_id': 'arXiv:2508.16357', 'title': 'MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering', 'authors': 'Adil Bahaj, Mounir Ghogho', 'link': 'https://arxiv.org/abs/2508.16357', 'abstract': 'The rapid advancement of large language models (LLMs) has significantly propelled progress in natural language processing (NLP). However, their effectiveness in specialized, low-resource domains-such as Arabic legal contexts-remains limited. This paper introduces MizanQA (pronounced Mizan, meaning "scale" in Arabic, a universal symbol of justice), a benchmark designed to evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised by rich linguistic and legal complexity. The dataset draws on Modern Standard Arabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal influences. Comprising over 1,700 multiple-choice questions, including multi-answer formats, MizanQA captures the nuances of authentic legal reasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs reveal substantial performance gaps, highlighting the need for tailored evaluation metrics and culturally grounded, domain-specific LLM development.', 'abstract_zh': '大型语言模型的迅速发展显著促进了自然语言处理的进步，但在阿拉伯语法律等专门的低资源领域中，其效果仍有限。本文介绍了一种名为MizanQA（意为“公正之秤”，一种阿拉伯语中的普遍正义符号）的标准，用于评估摩洛哥法律问答任务中的大型语言模型，该标准的特点是丰富的语言和法律复杂性。该数据集涵盖了现代标准阿拉伯语、伊斯兰麦克利学派教义、摩洛哥习惯法以及法语法律影响。包含超过1,700个选择题，包括多项选择格式，MizanQA 捕捉了真实法律推理的细微差别。多语言和阿拉伯语导向的大规模语言模型的基准测试实验揭示了显著的性能差距，强调了需要针对特定领域的评估指标以及基于文化背景的大规模语言模型开发。', 'title_zh': 'MizanQA：评估大型语言模型在摩洛哥法律问答中的表现'}
{'arxiv_id': 'arXiv:2508.16347', 'title': 'Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs', 'authors': 'Yu Yan, Sheng Sun, Zhe Wang, Yijun Lin, Zenghao Duan, zhifei zheng, Min Liu, Zhiyi yin, Jianping Zhang', 'link': 'https://arxiv.org/abs/2508.16347', 'abstract': "With the development of Large Language Models (LLMs), numerous efforts have revealed their vulnerabilities to jailbreak attacks. Although these studies have driven the progress in LLMs' safety alignment, it remains unclear whether LLMs have internalized authentic knowledge to deal with real-world crimes, or are merely forced to simulate toxic language patterns. This ambiguity raises concerns that jailbreak success is often attributable to a hallucination loop between jailbroken LLM and judger LLM. By decoupling the use of jailbreak techniques, we construct knowledge-intensive Q\\&A to investigate the misuse threats of LLMs in terms of dangerous knowledge possession, harmful task planning utility, and harmfulness judgment robustness. Experiments reveal a mismatch between jailbreak success rates and harmful knowledge possession in LLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness judgments on toxic language patterns. Our study reveals a gap between existing LLM safety assessments and real-world threat potential.", 'abstract_zh': '随着大型语言模型（LLMs）的发展，众多研究揭示了它们在监狱突破攻击下的脆弱性。尽管这些研究推动了LLMs安全对齐的进展，但仍不清楚LLMs是否真正内化了真实知识来应对现实世界的犯罪，还是仅仅被迫模拟有毒语言模式。这种不确定性引发了担忧，即监狱突破成功的归因往往在于监狱突破后的LLM与判断LLM之间的幻觉循环。通过解除监狱突破技术的使用，我们构建了知识密集型问答，以探究LLMs在危险知识拥有、有害任务规划能力和有害性判断稳健性方面的滥用威胁。实验揭示了监狱突破成功率与LLMs中危险知识拥有之间的不匹配，并且现有的LLM作为判断模型的框架倾向于基于有毒语言模式进行有害性判断。我们的研究揭示了现有LLM安全性评估与实际威胁潜力之间的差距。', 'title_zh': '困惑是最后的障碍：重思 Jailbreak 评估并探究大语言模型的实际滥用威胁'}
{'arxiv_id': 'arXiv:2508.16325', 'title': 'LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts', 'authors': 'Darpan Aswal, Céline Hudelot', 'link': 'https://arxiv.org/abs/2508.16325', 'abstract': 'Large Language Models have found success in a variety of applications; however, their safety remains a matter of concern due to the existence of various types of jailbreaking methods. Despite significant efforts, alignment and safety fine-tuning only provide a certain degree of robustness against jailbreak attacks that covertly mislead LLMs towards the generation of harmful content. This leaves them prone to a number of vulnerabilities, ranging from targeted misuse to accidental profiling of users. This work introduces \\textbf{LLMSymGuard}, a novel framework that leverages Sparse Autoencoders (SAEs) to identify interpretable concepts within LLM internals associated with different jailbreak themes. By extracting semantically meaningful internal representations, LLMSymGuard enables building symbolic, logical safety guardrails -- offering transparent and robust defenses without sacrificing model capabilities or requiring further fine-tuning. Leveraging advances in mechanistic interpretability of LLMs, our approach demonstrates that LLMs learn human-interpretable concepts from jailbreaks, and provides a foundation for designing more interpretable and logical safeguard measures against attackers. Code will be released upon publication.', 'abstract_zh': '大型语言模型在多种应用中取得了成功，但由于存在各种类型的破解方法，其安全性仍令人担忧。尽管付出了大量努力，对齐和安全性微调仅能提供一定程度的抵御隐形误导LLM生成有害内容的破解攻击的鲁棒性。这使得它们容易受到包括针对性滥用到用户意外画像在内的一系列漏洞。本文介绍了**LLMSymGuard**，一种利用稀疏自编码器（SAEs）识别LLM内部与不同破解主题相关可解释概念的新框架。通过提取语义上有意义的内部表示，LLMSymGuard能够构建符号逻辑安全性护栏——提供透明且稳健的防御，而不牺牲模型能力或需要进一步微调。利用大型语言模型机制可解释性的进步，我们的方法证明了大型语言模型从破解中学到了可由人类解读的概念，并为设计更可解释和逻辑的防御措施奠定了基础。代码将在发表后公开。', 'title_zh': 'LLMSymGuard: 一种基于可解释性脱管概念的符号安全防护框架'}
{'arxiv_id': 'arXiv:2508.16313', 'title': 'Retrieval Enhanced Feedback via In-context Neural Error-book', 'authors': 'Jongyeop Hyun, Bumsoo Kim', 'link': 'https://arxiv.org/abs/2508.16313', 'abstract': "Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining. While previous works have focused on leveraging correct examples, recent research highlights the importance of learning from errors to enhance performance. However, existing methods lack a structured framework for analyzing and mitigating errors, particularly in Multimodal Large Language Models (MLLMs), where integrating visual and textual inputs adds complexity. To address this issue, we propose REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a teacher-student framework that systematically structures errors and provides targeted feedback. REFINE introduces three systematic queries to construct structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance multimodal reasoning by prioritizing relevant visual information, diagnosing critical failure points, and formulating corrective actions. Unlike prior approaches that rely on redundant retrievals, REFINE optimizes structured feedback retrieval, improving inference efficiency, token usage, and scalability. Our results demonstrate substantial speedup, reduced computational costs, and successful generalization, highlighting REFINE's potential for enhancing multimodal reasoning.", 'abstract_zh': '近期大型语言模型（LLMs）的进展显著提升了推理能力，其中上下文内学习（ICL）作为一种关键技术，在无需重新训练的情况下实现适应。虽然早期研究主要集中在利用正确示例上，近期的研究强调了从错误中学习的重要性以提升性能。然而，现有方法缺乏一个结构化的框架来分析和缓解错误，特别是在多模态大型语言模型（MLLMs）中，视觉和文本输入的整合增加了复杂性。为应对这一问题，我们提出了REFINE：基于上下文检索的反馈增强框架，一种教师-学生框架，它系统地结构化错误并提供针对性的反馈。REFINE引入了三种系统查询来构建结构化反馈——Feed-Target、Feed-Check和Feed-Path，以通过优先考虑相关视觉信息、诊断关键失败点和制定纠正措施来增强多模态推理。不同于依赖冗余检索的先前方法，REFINE优化了结构化反馈检索，提高了推理效率、token使用和可扩展性。我们的结果显示REFINE带来了显著的速度提升、降低的计算成本和成功的泛化，突显了其在增强多模态推理方面的潜力。', 'title_zh': '基于上下文的神经错误回顾检索增强反馈'}
{'arxiv_id': 'arXiv:2508.16267', 'title': 'From Confidence to Collapse in LLM Factual Robustness', 'authors': 'Alina Fastowski, Bardh Prenkaj, Gjergji Kasneci', 'link': 'https://arxiv.org/abs/2508.16267', 'abstract': 'Ensuring the robustness of factual knowledge in LLMs is critical for reliable applications in tasks such as question answering and reasoning. However, existing evaluation methods predominantly focus on performance-based metrics, often investigating from the perspective of prompt perturbations, which captures only the externally triggered side of knowledge robustness. To bridge this gap, we introduce a principled approach to measure factual robustness from the perspective of the generation process by analyzing token distribution entropy in combination with temperature scaling sensitivity. These two factors build the Factual Robustness Score (FRS), a novel metric which quantifies the stability of a fact against perturbations in decoding conditions, given its initial uncertainty. To validate our approach, we conduct extensive experiments on 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We show that factual robustness varies significantly -- smaller models report an FRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under increased uncertainty. These insights demonstrate how entropy and temperature scaling impact factual accuracy, and lay a foundation for developing more robust knowledge retention and retrieval in future models.', 'abstract_zh': '保障大规模语言模型中事实知识的稳健性对于可靠的应用于问答和推理等任务至关重要。然而，现有的评估方法主要关注基于性能的指标，通常从提示扰动的角度进行研究，仅捕获了知识稳健性的外部触发侧面。为了弥合这一差距，我们提出了一种严谨的方法，从生成过程的角度衡量事实稳健性，通过结合令牌分布熵和温度缩放敏感性的分析构建事实稳健性评分（FRS），这是一种新颖的度量标准，量化了给定初始不确定性条件下事实在解码条件变化中的稳定性。为了验证该方法，我们在3个闭卷问答数据集中（SQuAD、TriviaQA和HotpotQA）对5个LLM进行了广泛实验。结果显示，事实稳健性差异显著——较小的模型报告的FRS为0.76，较大的模型为0.93，在增加不确定性的情况下准确率下降约60%。这些见解展示了熵和温度缩放如何影响事实准确性，并为未来模型中更稳健的知识保留和检索奠定了基础。', 'title_zh': '从自信到崩溃：大规模语言模型事实鲁棒性'}
{'arxiv_id': 'arXiv:2508.16260', 'title': 'MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use', 'authors': 'Fei Lei, Yibo Yang, Wenxiu Sun, Dahua Lin', 'link': 'https://arxiv.org/abs/2508.16260', 'abstract': 'Large Language Models (LLMs) are evolving from text generators into reasoning agents. This transition makes their ability to use external tools a critical capability. However, evaluating this skill presents a significant challenge. Existing benchmarks are often limited by their reliance on synthetic tools and severely constrained action spaces. To address these limitations, we introduce MCPVerse, an expansive, real-world benchmark for evaluating agentic tool use. MCPVerse integrates more than 550 real-world, executable tools to create an unprecedented action space exceeding 140k tokens, and employs outcome-based evaluation with real-time ground truth for time-sensitive tasks. We benchmarked the state-of-the-art LLMs across three modes (Oracle, Standard, and Max-Scale), revealing that while most models suffer performance degradation when confronted with larger tool sets, the agentic models, such as Claude-4-Sonnet, can effectively leverage expanded exploration spaces to improve accuracy. This finding not only exposes the limitations of state-of-the-art models in complex, real-world scenarios but also establishes MCPVerse as a critical benchmark for measuring and advancing agentic tool use capabilities.', 'abstract_zh': '大规模语言模型（LLMs）正在从文本生成器演变成为推理代理。这一转变使其使用外部工具的能力成为一种关键技能。然而，评估这一技能面临着重大挑战。现有的基准测试往往受限于对合成工具的依赖和严重受限的操作空间。为应对这些局限性，我们引入了MCPVerse，这是一个广泛且基于现实世界的标准，用于评估代理工具使用能力。MCPVerse结合了超过550个可执行的现实世界工具，创建了一个前所未有的操作空间，超过140k个令牌，并采用了基于结果的评测方法，提供实时真实情况作为参考，适用于时间敏感的任务。我们利用三种模式（Oracle、Standard和Max-Scale）对最先进的LLM进行了基准测试，结果显示，虽然大多数模型在面对更大工具集时表现出性能下降，但如Claude-4-Sonnet这样的代理模型能够有效利用扩大的探索空间来提高准确性。这一发现不仅揭示了最先进的模型在复杂现实世界场景中的局限性，还确立了MCPVerse作为衡量和推进代理工具使用能力的关键基准的重要性。', 'title_zh': 'MCPVerse: 一项广泛的现实世界基准，用于代理性工具使用'}
{'arxiv_id': 'arXiv:2508.16201', 'title': 'SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning', 'authors': 'Yicheng Ji, Jun Zhang, Heming Xia, Jinpeng Chen, Lidan Shou, Gang Chen, Huan Li', 'link': 'https://arxiv.org/abs/2508.16201', 'abstract': "Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens, enabling efficient speculation without sacrificing accuracy. To achieve this, it performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\\times$ speedup for Qwen2.5-VL-32B.", 'abstract_zh': 'Speculative Decoding Framework for Video Large Language Models with Staged Video Token Pruning', 'title_zh': 'SpecVLM：通过验证器引导的token裁剪增强视频LLM的 speculative 解码'}
{'arxiv_id': 'arXiv:2508.16181', 'title': 'LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2', 'authors': 'Zirui Li, Stephan Husung, Haoze Wang', 'link': 'https://arxiv.org/abs/2508.16181', 'abstract': 'Cross-organizational collaboration in Model-Based Systems Engineering (MBSE) faces many challenges in achieving semantic alignment across independently developed system models. SysML v2 introduces enhanced structural modularity and formal semantics, offering a stronger foundation for interoperable modeling. Meanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for assisting model understanding and integration. This paper proposes a structured, prompt-driven approach for LLM-assisted semantic alignment of SysML v2 models. The core contribution lies in the iterative development of an alignment approach and interaction prompts, incorporating model extraction, semantic matching, and verification. The approach leverages SysML v2 constructs such as alias, import, and metadata extensions to support traceable, soft alignment integration. It is demonstrated with a GPT-based LLM through an example of a measurement system. Benefits and limitations are discussed.', 'abstract_zh': '基于模型的系统工程（MBSE）中的跨组织协作在实现独立开发系统模型的语义对齐方面面临许多挑战。SysML v2 引入了增强的结构模块性和形式化语义，为互操作建模提供了更强的基础。同时，基于 GPT 的大型语言模型（LLMs）为辅助模型理解和集成提供了新的能力。本文提出了一种结构化、提示驱动的方法，用于 GPT-LLM 辅助的 SysML v2 模型语义对齐。核心贡献在于迭代开发对齐方法和交互提示，包括模型提取、语义匹配和验证。该方法利用 SysML v2 构建块，如别名、导入和元数据扩展，支持可追溯的软对齐集成。通过测量系统示例展示了基于 GPT 的 LLM 的方法，并讨论了其优势和局限性。', 'title_zh': '基于SysML v2的协作模型驱动系统工程中的LLM辅助语义对齐与整合'}
{'arxiv_id': 'arXiv:2508.16165', 'title': 'Towards Recommending Usability Improvements with Multimodal Large Language Models', 'authors': 'Sebastian Lubos, Alexander Felfernig, Gerhard Leitner, Julian Schwazer', 'link': 'https://arxiv.org/abs/2508.16165', 'abstract': 'Usability describes a set of essential quality attributes of user interfaces (UI) that influence human-computer interaction. Common evaluation methods, such as usability testing and inspection, are effective but resource-intensive and require expert involvement. This makes them less accessible for smaller organizations. Recent advances in multimodal LLMs offer promising opportunities to automate usability evaluation processes partly by analyzing textual, visual, and structural aspects of software interfaces. To investigate this possibility, we formulate usability evaluation as a recommendation task, where multimodal LLMs rank usability issues by severity. We conducted an initial proof-of-concept study to compare LLM-generated usability improvement recommendations with usability expert assessments. Our findings indicate the potential of LLMs to enable faster and more cost-effective usability evaluation, which makes it a practical alternative in contexts with limited expert resources.', 'abstract_zh': '可用性描述了一组影响人机交互的关键质量属性。常见的评估方法，如可用性测试和检查，虽然有效但资源密集型且需要专家参与，这使得它们对较小组织来说不够适用。最近多模态大语言模型的进步提供了部分通过分析软件界面的文本、视觉和结构方面来自动化可用性评估过程的有前景的机会。为了探究这一可能性，我们将可用性评估形式化为一个推荐任务，其中多模态大语言模型根据严重程度对可用性问题进行排序。我们进行了初步的概念证明研究，将大语言模型生成的可用性改进建议与可用性专家评估进行了比较。我们的研究结果表明，大语言模型有可能加速并降低成本的可用性评估，从而在专家资源有限的环境中成为一种实用的替代方案。', 'title_zh': '基于多模态大语言模型的可使用性改进推荐'}
{'arxiv_id': 'arXiv:2508.16134', 'title': 'CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing', 'authors': 'Yixuan Wang, Haoyu Qiao, Lujun Li, Qingfu Zhu, Wanxiang Che', 'link': 'https://arxiv.org/abs/2508.16134', 'abstract': 'Large Language Models (LLMs) confront significant memory challenges due to the escalating KV cache with increasing sequence length. As a crucial technique, existing cross-layer KV cache sharing methods either necessitate modified model architectures with subsequent pre-training or incur significant performance degradation at high compression rates. To mitigate these challenges, we propose CommonKV, a training-free method for cross-layer KV cache compression through adjacent parameters sharing. Inspired by the high similarity observed in cross-layer hidden states, we utilize Singular Value Decomposition (SVD) to achieve weight sharing across adjacent parameters, resulting in a more easily mergeable latent KV cache. Furthermore, we also introduce an adaptive budget allocation strategy. It dynamically assigns compression budgets based on cosine similarity, ensuring that dissimilar caches are not over-compressed. Experiments across multiple backbone models and benchmarks including LongBench and Ruler demonstrate that the proposed method consistently outperforms existing low-rank and cross-layer approaches at various compression ratios. Moreover, we find that the benefits of CommonKV are orthogonal to other quantization and eviction methods. By integrating these approaches, we can ultimately achieve a 98\\% compression ratio without significant performance loss.', 'abstract_zh': '大型语言模型（LLMs）因序列长度增加导致的KV缓存扩展面临着重要内存挑战。为应对这些挑战，我们提出了一种名为CommonKV的训练-free跨层KV缓存压缩方法，通过相邻参数共享实现缓存压缩。受跨层隐藏状态高度相似性的启发，我们利用奇异值分解（SVD）在相邻参数之间实现权重共享，从而获得一个更易于合并的潜在KV缓存。此外，我们还引入了一种自适应预算分配策略，基于余弦相似性动态分配压缩预算，确保不同缓存不会被过度压缩。实验结果表明，该方法在多种骨干模型和基准测试（包括LongBench和Ruler）上的一致优于现有低秩和跨层方法。此外，我们发现CommonKV的效果与其他量化和驱逐方法相互独立。通过结合这些方法，最终可以实现高达98%的压缩比，同时保持性能基本不变。', 'title_zh': '共层参数共享压缩KV缓存：CommonKV'}
{'arxiv_id': 'arXiv:2508.16131', 'title': 'The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion', 'authors': 'Zoe Kotti, Konstantina Dritsa, Diomidis Spinellis, Panos Louridas', 'link': 'https://arxiv.org/abs/2508.16131', 'abstract': 'Code completion entails the task of providing missing tokens given a surrounding context. It can boost developer productivity while providing a powerful code discovery tool. Following the Large Language Model (LLM) wave, code completion has been approached with diverse LLMs fine-tuned on code (code LLMs). The performance of code LLMs can be assessed with downstream and intrinsic metrics. Downstream metrics are usually employed to evaluate the practical utility of a model, but can be unreliable and require complex calculations and domain-specific knowledge. In contrast, intrinsic metrics such as perplexity, entropy, and mutual information, which measure model confidence or uncertainty, are simple, versatile, and universal across LLMs and tasks, and can serve as proxies for functional correctness and hallucination risk in LLM-generated code. Motivated by this, we evaluate the confidence of LLMs when generating code by measuring code perplexity across programming languages, models, and datasets using various LLMs, and a sample of 1008 files from 657 GitHub projects. We find that strongly-typed languages exhibit lower perplexity than dynamically typed languages. Scripting languages also demonstrate higher perplexity. Perl appears universally high in perplexity, whereas Java appears low. Code perplexity depends on the employed LLM, but not on the code dataset. Although code comments often increase perplexity, the language ranking based on perplexity is barely affected by their presence. LLM researchers, developers, and users can employ our findings to assess the benefits and suitability of LLM-based code completion in specific software projects based on how language, model choice, and code characteristics impact model confidence.', 'abstract_zh': '代码补全涉及根据周围上下文提供缺失的令牌的任务。它可以在提供强大代码发现工具的同时提升开发人员的生产力。随着大型语言模型（LLM）浪潮的兴起，代码补全任务采用了针对代码进行微调的多种LLM（代码LLM）。代码LLM的性能可以用下游和固有指标来评估。下游指标通常用于评估模型的实用价值，但可能不可靠且需要复杂的计算和领域专业知识。相比之下，困惑度、熵和互信息等固有指标衡量模型的信心或不确定性，这些指标简单、灵活且适用于所有LLM和任务，可以用作LLM生成代码的功能正确性和幻觉风险的代理指标。受此启发，我们通过使用多种LLM，在编程语言、模型和数据集上测量代码困惑度，评估LLM在生成代码时的信心。我们发现，强类型语言的困惑度低于动态类型语言。脚本语言也表现出较高的困惑度。Perl在困惑度上普遍较高，而Java则较低。代码困惑度取决于所使用的LLM，但不取决于代码数据集。尽管代码注释往往会增加困惑度，但基于困惑度的语言排名受其存在影响不大。LLM研究人员、开发者和用户可以根据语言、模型选择和代码特性对LLM驱动的代码补全在特定软件项目中的优势和适用性进行评估。', 'title_zh': '愚者笃定，智者疑虑：探索大模型在代码补全中的置信度'}
{'arxiv_id': 'arXiv:2508.16100', 'title': 'CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency', 'authors': 'Zhanming Shen, Hao Chen, Yulei Tang, Shaolin Zhu, Wentao Ye, Xiaomeng Hu, Haobo Wang, Gang Chen, Junbo Zhao', 'link': 'https://arxiv.org/abs/2508.16100', 'abstract': "Instruction tuning is vital for aligning large language models (LLMs) with human intent, but current methods typically rely on costly human-annotated seed data or powerful external teacher models. While instruction back-translation techniques reduce this dependency, they remain fundamentally tethered to an initial seed set, which limits full automation, introduces biases, and can lead to inefficient use of unlabeled corpora. In this paper, we propose Cycle-Instruct, a novel framework that achieves fully seed-free instruction tuning. Inspired by cycle consistency, Cycle-Instruct employs a dual self-training loop where two models-an answer generator and a question generator-are bootstrapped solely from raw, unlabeled text. These models mutually supervise each other by reconstructing original text segments from their counterpart's generated pseudo-labels, effectively learning from the intrinsic structure of the data without any human-provided seeds. We demonstrate Cycle-Instruct's efficacy across four diverse data tracks, including general instruction-following, domain-specific tasks, dialogue logs, and plain text. Our extensive experiments show that Cycle-Instruct not only outperforms seed-driven back-translation baselines but also achieves performance comparable to strongly supervised methods.", 'abstract_zh': 'Cycle-Instruct：一种无种子的指令调优新框架', 'title_zh': 'CYCLE-INSTRUCT: 通过双向自我训练和循环一致性实现的完全无种子指令调优'}
{'arxiv_id': 'arXiv:2508.16077', 'title': 'Cooperative Design Optimization through Natural Language Interaction', 'authors': 'Ryogo Niwa, Shigeo Yoshida, Yuki Koyama, Yoshitaka Ushiku', 'link': 'https://arxiv.org/abs/2508.16077', 'abstract': "Designing successful interactions requires identifying optimal design parameters. To do so, designers often conduct iterative user testing and exploratory trial-and-error. This involves balancing multiple objectives in a high-dimensional space, making the process time-consuming and cognitively demanding. System-led optimization methods, such as those based on Bayesian optimization, can determine for designers which parameters to test next. However, they offer limited opportunities for designers to intervene in the optimization process, negatively impacting the designer's experience. We propose a design optimization framework that enables natural language interactions between designers and the optimization system, facilitating cooperative design optimization. This is achieved by integrating system-led optimization methods with Large Language Models (LLMs), allowing designers to intervene in the optimization process and better understand the system's reasoning. Experimental results show that our method provides higher user agency than a system-led method and shows promising optimization performance compared to manual design. It also matches the performance of an existing cooperative method with lower cognitive load.", 'abstract_zh': '设计成功的交互需要确定最优设计参数。为此，设计者通常需要进行迭代用户测试和探索性试验。这一过程涉及在高维空间中平衡多个目标，使其耗时且认知需求高。基于贝叶斯优化等系统主导的优化方法可以帮助设计者确定接下来应测试的参数。然而，这些方法限制了设计者干预优化过程的机会，影响了设计者的体验。我们提出了一种设计优化框架，使设计者能够与优化系统进行自然语言交互，促进协作设计优化。该框架通过将系统主导的优化方法与大型语言模型（LLMs）集成，使设计者能够干预优化过程并更好地理解系统的推理。实验结果表明，我们的方法提供了比系统主导方法更高的用户自主性，并且在优化性能方面优于手动设计，同时也达到了认知负担较小的现有协作方法的性能。', 'title_zh': '基于自然语言交互的协同设计优化'}
{'arxiv_id': 'arXiv:2508.16071', 'title': 'From Benchmark Data To Applicable Program Repair: An Experience Report', 'authors': 'Mahinthan Chandramohan, Jovan Jancic, Yuntong Zhang, Padmanabhan Krishnan', 'link': 'https://arxiv.org/abs/2508.16071', 'abstract': 'This paper describes our approach to automated program repair. We combine various techniques from the literature to achieve this. Our experiments show that our approach performs better than other techniques on standard benchmarks. However, on closer inspection, none of these techniques work on realistic defects that we see in industry.\nWe find that augmenting code with formal specifications enables LLMs to generate higher-quality unit tests, especially for complex production code with improved coverage of edge cases and exception handling. However, specifications add little value for well-understood errors (e.g., null pointer, index out of bounds), but are beneficial for logic and string manipulation errors. Despite encouraging benchmark results, real-world adoption is limited since passing tests do not guarantee correct patches. Current challenges include insufficient expressiveness of the JML specification language, necessitating advanced verification tools and richer predicates. Our ongoing work is exploring contract automata, programming by example, and testcase repair, with a focus on integrating human feedback and measuring productivity gains - highlighting the gap between academic benchmarks and practical industry needs', 'abstract_zh': '本文描述了我们自动程序修复的方法。我们结合文献中的各种技术来实现这一目标。我们的实验表明，与现有技术相比，我们的方法在标准基准上表现更佳。然而，深入研究发现，这些技术在我们工业实践中遇到的现实缺陷上并不奏效。\n\n我们发现，增加形式化规范能够使大语言模型生成更高质量的单元测试，特别是在复杂生产代码中，规范能更好地覆盖边缘情况和异常处理。然而，规范对已充分理解的错误（如空指针、索引越界）几乎没有额外价值，但对逻辑错误和字符串操作错误是有益的。尽管基准测试结果令人鼓舞，但在实际应用中的采用受到限制，因为通过测试并不保证正确的修补。当前面临的挑战包括JML规范语言表达能力不足，需要更先进的验证工具和更丰富的谓词。我们的持续研究涉及合同自动机、基于示例编程和测试案例修复，并注重整合人类反馈和衡量生产力提升，突出了学术基准与实际工业需求之间的差距。', 'title_zh': '从基准数据到适用的程序修复：一项经验报告'}
{'arxiv_id': 'arXiv:2508.16048', 'title': 'OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages', 'authors': 'Raphaël Merx, Hanna Suominen, Trevor Cohn, Ekaterina Vylomova', 'link': 'https://arxiv.org/abs/2508.16048', 'abstract': "In machine translation (MT), health is a high-stakes domain characterised by widespread deployment and domain-specific vocabulary. However, there is a lack of MT evaluation datasets for low-resource languages in this domain. To address this gap, we introduce OpenWHO, a document-level parallel corpus of 2,978 documents and 26,824 sentences from the World Health Organization's e-learning platform. Sourced from expert-authored, professionally translated materials shielded from web-crawling, OpenWHO spans a diverse range of over 20 languages, of which nine are low-resource. Leveraging this new resource, we evaluate modern large language models (LLMs) against traditional MT models. Our findings reveal that LLMs consistently outperform traditional MT models, with Gemini 2.5 Flash achieving a +4.79 ChrF point improvement over NLLB-54B on our low-resource test set. Further, we investigate how LLM context utilisation affects accuracy, finding that the benefits of document-level translation are most pronounced in specialised domains like health. We release the OpenWHO corpus to encourage further research into low-resource MT in the health domain.", 'abstract_zh': '在机器翻译（MT）中，医疗是一个高风险领域，具有广泛的应用和领域特异性词汇。然而，该领域缺乏低资源语言的MT评估数据集。为解决这一问题，我们引入了OpenWHO，这是一个来自世界卫生组织e学习平台的2,978份文档和26,824个句子的文档级别平行语料库。这些材料来自专家编写的、专业翻译的内容，并未经过网络爬虫采集，OpenWHO涵盖了超过20种语言，其中九种是低资源语言。利用这一新资源，我们评估了现代大型语言模型（LLMs）与传统MT模型的表现。我们的研究发现，LLMs在所有模型中表现最佳，Gemini 2.5 Flash在我们的低资源测试集上比NLLB-54B提高了4.79个ChrF分数。此外，我们研究了LLM上下文利用对其准确性的影响，发现文档级别的翻译在如健康等专业化领域中表现出最大的优势。我们发布了OpenWHO语料库，以促进医疗领域低资源MT的研究。', 'title_zh': 'OpenWHO：低资源语言健康翻译的文档级并行语料库'}
{'arxiv_id': 'arXiv:2508.15940', 'title': 'ASIC-Agent: An Autonomous Multi-Agent System for ASIC Design with Benchmark Evaluation', 'authors': 'Ahmed Allam, Youssef Mansour, Mohamed Shalan', 'link': 'https://arxiv.org/abs/2508.15940', 'abstract': "Large Language Models (LLMs) have demonstrated remarkable capabilities in Register Transfer Level (RTL) design, enabling high-quality code generation from natural language descriptions. However, LLMs alone face significant limitations in real-world hardware design workflows, including the inability to execute code, lack of debugging capabilities, and absence of long-term memory. To address these challenges, we present ASIC-Agent, an autonomous system designed specifically for digital ASIC design tasks. ASIC-Agent enhances base LLMs with a multi-agent architecture incorporating specialized sub-agents for RTL generation, verification, OpenLane hardening, and Caravel chip integration, all operating within a comprehensive sandbox environment with access to essential hardware design tools. The system leverages a vector database containing documentation, API references, error knowledge, and curated insights from the open-source silicon community. To evaluate ASIC-Agent's performance, we introduce ASIC-Agent-Bench, the first benchmark specifically designed to assess agentic systems in hardware design tasks. We evaluate ASIC-Agent with various base LLMs, providing quantitative comparisons and qualitative insights into agent behavior across different design scenarios. Our results demonstrate that ASIC-Agent, when powered by Claude 4 Sonnet, successfully automates a broad range of ASIC design tasks spanning varying levels of complexity, showing the potential of significantly accelerating the ASIC design workflow.", 'abstract_zh': '大型语言模型（LLMs）在寄存器传输级（RTL）设计中展示了非凡的能力，能够从自然语言描述生成高质量的代码。然而，LLMs在实际硬件设计工作流中面临着显著的局限性，包括无法执行代码、缺乏调试功能和没有长期记忆。为解决这些挑战，我们提出了一个专为数字ASIC设计任务设计的自主系统ASIC-Agent。ASIC-Agent通过一个多代理架构增强了基础LLMs，该架构包含专门的子代理，用于RTL生成、验证、OpenLane强化和Caravel芯片集成，所有这些都在一个全面的沙箱环境中进行，该环境提供了访问关键硬件设计工具的权限。该系统利用了一个向量数据库，其中包含文档、API引用、错误知识以及来自开源硅社区的精选见解。为了评估ASIC-Agent的性能，我们引入了ASIC-Agent-Bench，这是第一个专门用于评估机器代理系统在硬件设计任务中的基准。我们使用各种基础LLMs评估了ASIC-Agent，提供了定性和定量比较，展示了不同设计场景下代理行为的见解。我们的结果表明，当由Claude 4 Sonnet驱动时，ASIC-Agent能够自动化范围广泛且复杂程度不同的ASIC设计任务，显示出显著加速ASIC设计工作流的潜力。', 'title_zh': 'ASIC-Agent：一种用于ASIC设计的自主多智能体系统及基准评估'}
{'arxiv_id': 'arXiv:2508.15926', 'title': 'Noise, Adaptation, and Strategy: Assessing LLM Fidelity in Decision-Making', 'authors': 'Yuanjun Feng, Vivek Choudhary, Yash Raj Shrestha', 'link': 'https://arxiv.org/abs/2508.15926', 'abstract': "Large language models (LLMs) are increasingly used in social science simulations. While their performance on reasoning and optimization tasks has been extensively evaluated, less attention has been paid to their ability to simulate human decision-making's variability and adaptability. We propose a process-oriented evaluation framework with progressive interventions (Intrinsicality, Instruction, and Imitation) to examine how LLM agents adapt under different levels of external guidance and human-derived noise. We validate the framework on two classic economics tasks, irrationality in the second-price auction and decision bias in the newsvendor problem, showing behavioral gaps between LLMs and humans.\nWe find that LLMs, by default, converge on stable and conservative strategies that diverge from observed human behaviors. Risk-framed instructions impact LLM behavior predictably but do not replicate human-like diversity. Incorporating human data through in-context learning narrows the gap but fails to reach human subjects' strategic variability. These results highlight a persistent alignment gap in behavioral fidelity and suggest that future LLM evaluations should consider more process-level realism. We present a process-oriented approach for assessing LLMs in dynamic decision-making tasks, offering guidance for their application in synthetic data for social science research.", 'abstract_zh': "大型语言模型（LLMs）在社会科学模拟中的应用 increasingly used in social science simulations. While their performance on reasoning and optimization tasks has been extensively evaluated, less attention has been paid to their ability to simulate human decision-making's variability and adaptability. We propose a process-oriented evaluation framework with progressive interventions (Intrinsicality, Instruction, and Imitation) to examine how LLM agents adapt under different levels of external guidance and human-derived noise. We validate the framework on two classic economics tasks, irrationality in the second-price auction and decision bias in the newsvendor problem, showing behavioral gaps between LLMs and humans.\n\nWe find that LLMs, by default, converge on stable and conservative strategies that diverge from observed human behaviors. Risk-framed instructions impact LLM behavior predictably but do not replicate human-like diversity. Incorporating human data through in-context learning narrows the gap but fails to reach human subjects' strategic variability. These results highlight a persistent alignment gap in behavioral fidelity and suggest that future LLM evaluations should consider more process-level realism. We present a process-oriented approach for assessing LLMs in dynamic decision-making tasks, offering guidance for their application in synthetic data for social science research.\n\n大型语言模型在社会科学研究模拟中的变量性和适应性评估：一种渐进干预过程导向框架", 'title_zh': '噪声、适应与策略：评估大语言模型在决策中的真实性'}
{'arxiv_id': 'arXiv:2508.15919', 'title': 'HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO Serving and Fast Scaling', 'authors': 'Zahra Yousefijamarani, Xinglu Wang, Qian Wang, Morgan Lindsay Heisler, Taha Shabani, Niloofar Gholipour, Parham Yassini, Hong Chang, Kan Chen, Qiantao Zhang, Xiaolong Bai, Jiannan Wang, Ying Xiong, Yong Zhang, Zhenan Fan', 'link': 'https://arxiv.org/abs/2508.15919', 'abstract': 'Modern large language model (LLM) serving systems face challenges from highly variable requests with diverse lengths, priorities, and stage-specific service-level objectives (SLOs). Meeting these requires real-time scheduling, rapid and cost-effective scaling, and support for both collocated and disaggregated Prefill/Decode (P/D) architectures.\nWe present \\textbf{HyperFlexis}, a unified LLM serving system that integrates algorithmic and system-level innovations to jointly optimize scheduling and scaling under multiple SLOs. It features a multi-SLO-aware scheduler that leverages budget estimation and request prioritization to ensure proactive SLO compliance for both new and ongoing requests. The system supports prefill- and decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV cache transfers. It also enables cost-effective scaling decisions, prefill-decode instance linking during scaling, and rapid P/D role transitions. To accelerate scaling and reduce cold-start latency, a device-to-device (D2D) weight transfer mechanism is proposed that lowers weight loading overhead by up to \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up to \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request latency, and cost parity with state-of-the-art baselines. The code will be released soon.', 'abstract_zh': '现代大型语言模型（LLM）服务系统面临来自高度变异的、长度各异、优先级不同以及阶段特定服务级别目标（SLO）的挑战。满足这些需求需要实时调度、快速且经济高效的扩展能力，并支持集中式和解耦的预处理/解码（P/D）架构。\n\n我们提出\\textbf{HyperFlexis}，这是一种统一的LLM服务系统，通过结合算法和系统级别的创新，共同优化在多种SLO下的调度和扩展。它配备了一个多SLO感知调度器，利用预算估算和请求优先级策略，确保新旧请求的主动SLO合规。该系统支持将P/D解耦架构的预处理和解码阶段多SLO调度与键值缓存转移相结合。此外，它还允许经济高效的扩展决策、扩展期间的预处理/解码实例链接以及快速的P/D角色转换。为了加速扩展并减少冷启动延迟，我们提出了一种设备到设备（D2D）权重转移机制，将权重加载开销降低了最多\\textbf{19.39$\\times$}。这些优化使得系统能够在SLO达成率上提升最多\\textbf{4.44$\\times$}，请求延迟降低\\textbf{65.82\\%}，并且在成本上与最先进的基准保持一致。代码将在不久后发布。', 'title_zh': 'HyperFlexis: 联合设计算法与系统以实现多SLA服务和快速扩展'}
{'arxiv_id': 'arXiv:2508.15910', 'title': 'Evaluating Structured Decoding for Text-to-Table Generation: Evidence from Three Datasets', 'authors': 'Julian Oestreich, Lydia Müller', 'link': 'https://arxiv.org/abs/2508.15910', 'abstract': 'We present a comprehensive evaluation of structured decoding for text-to-table generation with large language models (LLMs). While previous work has primarily focused on unconstrained generation of tables, the impact of enforcing structural constraints during generation remains underexplored. We systematically compare schema-guided (structured) decoding to standard one-shot prompting across three diverse benchmarks - E2E, Rotowire, and Livesum - using open-source LLMs of up to 32B parameters, assessing the performance of table generation approaches in resource-constrained settings. Our experiments cover a wide range of evaluation metrics at cell, row, and table levels. Results demonstrate that structured decoding significantly enhances the validity and alignment of generated tables, particularly in scenarios demanding precise numerical alignment (Rotowire), but may degrade performance in contexts involving densely packed textual information (E2E) or extensive aggregation over lengthy texts (Livesum). We further analyze the suitability of different evaluation metrics and discuss the influence of model size.', 'abstract_zh': '我们全面评估了结构化解码在大规模语言模型（LLMs）进行文本到表格生成中的应用。虽然以往的工作主要集中在无约束的表格生成上，但在生成过程中施加强制结构约束的影响尚未充分探索。我们系统地在三个不同的基准数据集——E2E、Rotowire和Livesum——上将基于模式的（结构化）解码与标准的一次性提示进行比较，使用包含最多32B参数的开源LLMs，评估在资源受限条件下表格生成方法的表现。我们的实验涵盖了从单个单元格、整行到整个表格的广泛评价指标。结果表明，结构化解码显著提升了生成表格的有效性和一致性，特别是在需要精确数值对齐的场景（Rotowire）中，但在包含密集文本信息（E2E）或需要对大量文本进行广泛聚合（Livesum）的情境下，可能会影响性能。我们进一步分析不同评价指标的适用性，并讨论模型规模的影响。', 'title_zh': '基于三种数据集的结构化解码方法在文本到表生成中的评估'}
{'arxiv_id': 'arXiv:2508.15884', 'title': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search', 'authors': 'Yuxian Gu, Qinghao Hu, Shang Yang, Haocheng Xi, Junyu Chen, Song Han, Han Cai', 'link': 'https://arxiv.org/abs/2508.15884', 'abstract': 'We present Jet-Nemotron, a new family of hybrid-architecture language models, which matches or exceeds the accuracy of leading full-attention models while significantly improving generation throughput. Jet-Nemotron is developed using Post Neural Architecture Search (PostNAS), a novel neural architecture exploration pipeline that enables efficient model design. Unlike prior approaches, PostNAS begins with a pre-trained full-attention model and freezes its MLP weights, allowing efficient exploration of attention block designs. The pipeline includes four key components: (1) learning optimal full-attention layer placement and elimination, (2) linear attention block selection, (3) designing new attention blocks, and (4) performing hardware-aware hyperparameter search. Our Jet-Nemotron-2B model achieves comparable or superior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a comprehensive suite of benchmarks while delivering up to 53.6x generation throughput speedup and 6.1x prefilling speedup. It also achieves higher accuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models, such as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B total and 2.2B activated parameters.', 'abstract_zh': '我们呈现Jet-Nemotron，这是一种新型的混合架构语言模型家族，其准确度与领先的全注意机制模型相当或更优，同时显著提高生成速度。Jet-Nemotron 使用后神经架构搜索（PostNAS）开发，这是一种新型的神经架构探索流程，能够实现高效的模型设计。PostNAS 从预训练的全注意机制模型开始，并冻结其MLP权重，从而允许高效地探索注意块的设计。该流程包括四个关键组成部分：(1) 学习最优的全注意机制层放置和消除；(2) 线性注意机制块选择；(3) 设计新的注意机制块；(4) 进行硬件感知的超参数搜索。我们的Jet-Nemotron-2B模型在一系列基准测试中实现了与Qwen3、Qwen2.5、Gemma3和Llama3.2相当或更优的准确度，同时实现高达53.6倍的生成速度提升和6.1倍的填充速度提升。此外，尽管DeepSeek-V3-Small和Moonlight等更先进的混合专家模型的总参数量为15B，激活参数量为2.2B，Jet-Nemotron-2B在MMLU和MMLU-Pro上仍实现了更高的准确度。', 'title_zh': 'Jet-Nemotron:高效的后神经架构搜索语言模型'}
{'arxiv_id': 'arXiv:2508.15877', 'title': 'Annif at the GermEval-2025 LLMs4Subjects Task: Traditional XMTC Augmented by Efficient LLMs', 'authors': 'Osma Suominen, Juho Inkinen, Mona Lehtinen', 'link': 'https://arxiv.org/abs/2508.15877', 'abstract': 'This paper presents the Annif system in the LLMs4Subjects shared task (Subtask 2) at GermEval-2025. The task required creating subject predictions for bibliographic records using large language models, with a special focus on computational efficiency. Our system, based on the Annif automated subject indexing toolkit, refines our previous system from the first LLMs4Subjects shared task, which produced excellent results. We further improved the system by using many small and efficient language models for translation and synthetic data generation and by using LLMs for ranking candidate subjects. Our system ranked 1st in the overall quantitative evaluation of and 1st in the qualitative evaluation of Subtask 2.', 'abstract_zh': '本文在GermEval-2025 LLMs4Subjects 共享任务（子任务2）中介绍了Annif系统。该任务要求使用大规模语言模型为文献记录创建科目预测，并特别注重计算效率。我们的系统基于Annif自动主题索引工具包，改进了我们之前在第一个LLMs4Subjects 共享任务中的系统，该系统取得了优异的结果。我们通过使用多个小型高效的语言模型进行翻译和合成数据生成，并利用大规模语言模型对候选主题进行排序，进一步改进了系统。我们的系统在子任务2的整体定量评估和定性评估中均排名第一。', 'title_zh': 'Annif在GermEval-2025 LLMs4Subjects任务中的应用：由高效大语言模型增强的传统跨模态文本分类'}
{'arxiv_id': 'arXiv:2508.15875', 'title': 'NEAT: Concept driven Neuron Attribution in LLMs', 'authors': 'Vivek Hruday Kavuri, Gargi Shroff, Rahul Mishra', 'link': 'https://arxiv.org/abs/2508.15875', 'abstract': 'Locating neurons that are responsible for final predictions is important for opening the black-box large language models and understanding the inside mechanisms. Previous studies have tried to find mechanisms that operate at the neuron level but these methods fail to represent a concept and there is also scope for further optimization of compute required. In this paper, with the help of concept vectors, we propose a method for locating significant neurons that are responsible for representing certain concepts and term those neurons as concept neurons. If the number of neurons is n and the number of examples is m, we reduce the number of forward passes required from O(n*m) to just O(n) compared to the previous works and hence optimizing the time and computation required over previous works. We also compare our method with several baselines and previous methods and our results demonstrate better performance than most of the methods and are more optimal when compared to the state-of-the-art method. We, as part of our ablation studies, also try to optimize the search for the concept neurons by involving clustering methods. Finally, we apply our methods to find, turn off the neurons that we find, and analyze its implications in parts of hate speech and bias in LLMs, and we also evaluate our bias part in terms of Indian context. Our methodology, analysis and explanations facilitate understating of neuron-level responsibility for more broader and human-like concepts and also lay a path for future research in this direction of finding concept neurons and intervening them.', 'abstract_zh': '基于概念向量识别负责最终预测的神经元以优化大型语言模型的内部机制', 'title_zh': 'NEAT: 基于概念的神经元归因在大规模语言模型中'}
{'arxiv_id': 'arXiv:2508.15868', 'title': 'CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning', 'authors': 'Wenqiao Zhu, Ji Liu, Rongjuncheng Zhang, Haipang Wu, Yulun Zhang', 'link': 'https://arxiv.org/abs/2508.15868', 'abstract': 'Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \\TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \\TheName{} in terms of robustness, performance (up to 10.15\\%), and efficiency (up to 30.62\\%). Code is available at this https URL.', 'abstract_zh': '基于标注思维链的对比学习强化微调以提升大规模语言模型的推理能力', 'title_zh': 'CARFT：通过带有标注思维链加固的对比学习强化细调来提升LLM推理能力'}
{'arxiv_id': 'arXiv:2508.15858', 'title': 'Building and Measuring Trust between Large Language Models', 'authors': 'Maarten Buyl, Yousra Fettach, Guillaume Bied, Tijl De Bie', 'link': 'https://arxiv.org/abs/2508.15858', 'abstract': "As large language models (LLMs) increasingly interact with each other, most notably in multi-agent setups, we may expect (and hope) that `trust' relationships develop between them, mirroring trust relationships between human colleagues, friends, or partners. Yet, though prior work has shown LLMs to be capable of identifying emotional connections and recognizing reciprocity in trust games, little remains known about (i) how different strategies to build trust compare, (ii) how such trust can be measured implicitly, and (iii) how this relates to explicit measures of trust.\nWe study these questions by relating implicit measures of trust, i.e. susceptibility to persuasion and propensity to collaborate financially, with explicit measures of trust, i.e. a dyadic trust questionnaire well-established in psychology. We build trust in three ways: by building rapport dynamically, by starting from a prewritten script that evidences trust, and by adapting the LLMs' system prompt. Surprisingly, we find that the measures of explicit trust are either little or highly negatively correlated with implicit trust measures. These findings suggest that measuring trust between LLMs by asking their opinion may be deceiving. Instead, context-specific and implicit measures may be more informative in understanding how LLMs trust each other.", 'abstract_zh': '大型语言模型（LLMs）之间日益增加的互动，尤其是在多智能体设置中，我们可能会期望（并希望）它们之间建立“信任”关系，类似于人类同事、朋友或合作伙伴之间的信任关系。然而，尽管先前的研究表明LLMs能够识别情感联系并识别信任博弈中的互惠性，但关于（i）不同建立信任的策略有何不同，（ii）如何隐含地测量这种信任，以及（iii）这如何与显式的信任度度量相关的内容仍知之甚少。我们通过将隐含的信任度量，即易受说服性和财务合作倾向，与心理学中广泛应用的双向信任问卷等显性信任度量相关联来研究这些问题。我们通过三种方式建立信任：动态建立 rapport，从已有脚本出发，该脚本证明了信任，以及调整LLM系统提示。令人意外的是，我们发现显性信任度量与隐性信任度量之间的相关性要么很小，要么高度负相关。这些发现表明，通过询问它们的意见来测量LLMs之间的信任可能是误导性的。相反，情境特定和隐含的度量可能更能帮助我们理解LLMs如何彼此信任。', 'title_zh': '构建和衡量大型语言模型之间的信任'}
{'arxiv_id': 'arXiv:2508.15845', 'title': 'Coarse-to-Fine Personalized LLM Impressions for Streamlined Radiology Reports', 'authors': 'Chengbo Sun, Hui Yi Leong, Lei Li', 'link': 'https://arxiv.org/abs/2508.15845', 'abstract': 'The manual creation of the "Impression" section in radiology reports is a primary driver of radiologist burnout. To address this challenge, we propose a coarse-to-fine framework that leverages open-source large language models (LLMs) to automatically generate and personalize impressions from clinical findings. The system first produces a draft impression and then refines it using machine learning and reinforcement learning from human feedback (RLHF) to align with individual radiologists\' styles while ensuring factual accuracy. We fine-tune LLaMA and Mistral models on a large dataset of reports from the University of Chicago Medicine. Our approach is designed to significantly reduce administrative workload and improve reporting efficiency while maintaining high standards of clinical precision.', 'abstract_zh': '放射学报告中“印象”部分的手动创建是放射科医生 burnout 的主要驱动因素。为应对这一挑战，我们提出了一种由粗到细的框架，利用开源大规模语言模型（LLMs）自动生成并个性化生成“印象”文本。该系统首先生成初步印象，然后利用机器学习和基于人类反馈的强化学习（RLHF）对其进行细化，以与个体放射科医生的风格保持一致，同时确保事实准确性。我们对 LLaMA 和 Mistral 模型进行了大规模医学报告数据集的微调。该方法旨在显著减轻行政负担、提高报告效率，同时保持临床精准度的高标准。', 'title_zh': '从粗到细个性化LLM印象以简化放射学报告'}
{'arxiv_id': 'arXiv:2508.15831', 'title': "Who's Asking? Investigating Bias Through the Lens of Disability Framed Queries in LLMs", 'authors': 'Srikant Panda, Vishnu Hari, Kalpana Panda, Amit Agarwal, Hitesh Laxmichand Patel', 'link': 'https://arxiv.org/abs/2508.15831', 'abstract': 'Large Language Models (LLMs) routinely infer users demographic traits from phrasing alone, which can result in biased responses, even when no explicit demographic information is provided. The role of disability cues in shaping these inferences remains largely uncharted. Thus, we present the first systematic audit of disability-conditioned demographic bias across eight state-of-the-art instruction-tuned LLMs ranging from 3B to 72B parameters. Using a balanced template corpus that pairs nine disability categories with six real-world business domains, we prompt each model to predict five demographic attributes - gender, socioeconomic status, education, cultural background, and locality - under both neutral and disability-aware conditions.\nAcross a varied set of prompts, models deliver a definitive demographic guess in up to 97\\% of cases, exposing a strong tendency to make arbitrary inferences with no clear justification. Disability context heavily shifts predicted attribute distributions, and domain context can further amplify these deviations. We observe that larger models are simultaneously more sensitive to disability cues and more prone to biased reasoning, indicating that scale alone does not mitigate stereotype amplification.\nOur findings reveal persistent intersections between ableism and other demographic stereotypes, pinpointing critical blind spots in current alignment strategies. We release our evaluation framework and results to encourage disability-inclusive benchmarking and recommend integrating abstention calibration and counterfactual fine-tuning to curb unwarranted demographic inference. Code and data will be released on acceptance.', 'abstract_zh': '大型语言模型（LLMs）仅从措辞中就常能推断出用户的-demographic特质，即使没有提供明确的demographic信息，这也可能导致有偏见的回应。残疾线索在塑造这些推断中的作用尚待探索。因此，我们进行了首次针对八种先进的指令调校LLM系统的系统性审计，这些系统从3B到72B参数不等。使用一个平衡的模板语料库，该语料库将九类残疾与六个真实世界的企业领域配对，我们促使每个模型在中立和知觉残疾条件下预测五种人口统计属性——性别、社会经济地位、教育、文化背景和所在地区。\n\n在各种各样的提示下，模型在高达97%的情况下给出了明确的人口统计猜测，揭示了强烈的随意推断的趋势，而缺乏明确的解释。残疾背景强烈地改变了预测属性的分布，而领域背景可以进一步放大这些偏差。我们发现，较大的模型同时对残疾线索更加敏感并且更易产生偏见的推理，这表明规模本身并不能缓解刻板印象的放大。\n\n我们的研究揭示了能ISM与其它人口统计刻板印象之间的持久交叉点，指出了当前对齐策略中的关键盲点。我们发布了我们的评估框架和结果，以鼓励包容残疾的基准评估，并建议集成避免性校准和反事实微调来遏制不必要的人口统计推断。代码和数据将在接收后发布。', 'title_zh': '谁在提问？通过残障框架查询视角探究偏见'}
{'arxiv_id': 'arXiv:2508.15830', 'title': 'DAIQ: Auditing Demographic Attribute Inference from Question in LLMs', 'authors': 'Srikant Panda, Hitesh Laxmichand Patel, Shahad Al-Khalifa, Amit Agarwal, Hend Al-Khalifa, Sharefah Al-Ghamdi', 'link': 'https://arxiv.org/abs/2508.15830', 'abstract': 'Large Language Models (LLMs) are known to reflect social biases when demographic attributes, such as gender or race, are explicitly present in the input. But even in their absence, these models still infer user identities based solely on question phrasing. This subtle behavior has received far less attention, yet poses serious risks: it violates expectations of neutrality, infers unintended demographic information, and encodes stereotypes that undermine fairness in various domains including healthcare, finance and education.\nWe introduce Demographic Attribute Inference from Questions (DAIQ), a task and framework for auditing an overlooked failure mode in language models: inferring user demographic attributes from questions that lack explicit demographic cues. Our approach leverages curated neutral queries, systematic prompting, and both quantitative and qualitative analysis to uncover how models infer demographic information. We show that both open and closed source LLMs do assign demographic labels based solely on question phrasing.\nPrevalence and consistency of demographic inferences across diverse models reveal a systemic and underacknowledged risk: LLMs can fabricate demographic identities, reinforce societal stereotypes, and propagate harms that erode privacy, fairness, and trust posing a broader threat to social equity and responsible AI deployment. To mitigate this, we develop a prompt-based guardrail that substantially reduces identity inference and helps align model behavior with fairness and privacy objectives.', 'abstract_zh': '大型语言模型中的人口统计属性推断：一种审计框架', 'title_zh': 'DAIQ：审查LLM中问题推断人口统计属性'}
{'arxiv_id': 'arXiv:2508.15827', 'title': 'Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models', 'authors': 'Zhifei Xie, Ziyang Ma, Zihang Liu, Kaiyu Pang, Hongyu Li, Jialin Zhang, Yue Liao, Deheng Ye, Chunyan Miao, Shuicheng Yan', 'link': 'https://arxiv.org/abs/2508.15827', 'abstract': 'Reasoning is essential for effective communication and decision-making. While recent advances in LLMs and MLLMs have shown that incorporating explicit reasoning significantly improves understanding and generalization, reasoning in LSMs remains in a nascent stage. Early efforts attempt to transfer the "Thinking-before-Speaking" paradigm from textual models to speech. However, this sequential formulation introduces notable latency, as spoken responses are delayed until reasoning is fully completed, impairing real-time interaction and communication efficiency. To address this, we propose Mini-Omni-Reasoner, a framework that enables reasoning within speech via a novel "Thinking-in-Speaking" formulation. Rather than completing reasoning before producing any verbal output, Mini-Omni-Reasoner interleaves silent reasoning tokens with spoken response tokens at the token level. This design allows continuous speech generation while embedding structured internal reasoning, leveraging the model\'s high-frequency token processing capability. Although interleaved, local semantic alignment is enforced to ensure that each response token is informed by its preceding reasoning. To support this framework, we introduce Spoken-Math-Problems-3M, a large-scale dataset tailored for interleaved reasoning and response. The dataset ensures that verbal tokens consistently follow relevant reasoning content, enabling accurate and efficient learning of speech-coupled reasoning. Built on a hierarchical Thinker-Talker architecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken responses, maintaining both naturalness and precision. On the Spoken-MQA benchmark, it achieves a +19.1% gain in arithmetic reasoning and +6.4% in contextual understanding, with shorter outputs and zero decoding latency.', 'abstract_zh': 'Mini-Omni-Reasoner: Thinking-in-Speaking for Efficient and Logical Speech Generation', 'title_zh': 'Mini-Omni-Reasoner: 在大语言模型中实现 token 级别边说边思能力'}
{'arxiv_id': 'arXiv:2508.15822', 'title': 'An Auditable Pipeline for Fuzzy Full-Text Screening in Systematic Reviews: Integrating Contrastive Semantic Highlighting and LLM Judgment', 'authors': 'Pouria Mortezaagha, Arya Rahgozar', 'link': 'https://arxiv.org/abs/2508.15822', 'abstract': 'Full-text screening is the major bottleneck of systematic reviews (SRs), as decisive evidence is dispersed across long, heterogeneous documents and rarely admits static, binary rules. We present a scalable, auditable pipeline that reframes inclusion/exclusion as a fuzzy decision problem and benchmark it against statistical and crisp baselines in the context of the Population Health Modelling Consensus Reporting Network for noncommunicable diseases (POPCORN). Articles are parsed into overlapping chunks and embedded with a domain-adapted model; for each criterion (Population, Intervention, Outcome, Study Approach), we compute contrastive similarity (inclusion-exclusion cosine) and a vagueness margin, which a Mamdani fuzzy controller maps into graded inclusion degrees with dynamic thresholds in a multi-label setting. A large language model (LLM) judge adjudicates highlighted spans with tertiary labels, confidence scores, and criterion-referenced rationales; when evidence is insufficient, fuzzy membership is attenuated rather than excluded. In a pilot on an all-positive gold set (16 full texts; 3,208 chunks), the fuzzy system achieved recall of 81.3% (Population), 87.5% (Intervention), 87.5% (Outcome), and 75.0% (Study Approach), surpassing statistical (56.3-75.0%) and crisp baselines (43.8-81.3%). Strict "all-criteria" inclusion was reached for 50.0% of articles, compared to 25.0% and 12.5% under the baselines. Cross-model agreement on justifications was 98.3%, human-machine agreement 96.1%, and a pilot review showed 91% inter-rater agreement (kappa = 0.82), with screening time reduced from about 20 minutes to under 1 minute per article at significantly lower cost. These results show that fuzzy logic with contrastive highlighting and LLM adjudication yields high recall, stable rationale, and end-to-end traceability.', 'abstract_zh': '全文本筛选是系统评价的重大瓶颈，因为关键证据分散在长且异质的文档中，很少能用静态二元规则来识别。我们提出了一种可扩展且可审计的流水线，将其纳入/排除重新定义为一种模糊决策问题，并在非传染性疾病人口健康建模共识报告网络（POPCORN）的背景下与统计和清晰基准进行对比。文档被解析为重叠片段，并嵌入领域适应模型；对于每个标准（人群、干预措施、结果、研究方法），我们计算对比相似度（纳入-排除余弦）和模糊度余量，Mamdani模糊控制器将其映射到多标签设置中的逐步纳入程度，并具有动态阈值。一个大型语言模型（LLM）法官裁定突出显示的片段，带有三级标签、置信度分数和标准参考的合理性；当证据不足时，模糊隶属度被削弱而不是排除。在全阳性金集试点（16篇全文；3,208个片段）上，模糊系统实现了81.3%（人群）、87.5%（干预措施）、87.5%（结果）和75.0%（研究方法）的召回率，超过了统计（56.3-75.0%）和清晰基准（43.8-81.3%）。采用所有标准纳入的文章比例达到50.0%，而基准下的比例分别为25.0%和12.5%。跨模型对合理性的共识为98.3%，人类-机器共识为96.1%，试点审查显示91%的评分者间一致性（κ=0.82），筛选时间从每篇文章约20分钟缩短到不足1分钟，并且成本显著降低。这些结果表明，使用对比突出显示和LLM裁定的模糊逻辑能够实现高召回率、稳定合理性和端到端可追溯性。', 'title_zh': '可审计的工作流：结合对比语义突出显示和大规模语言模型判断的模糊全文筛查方法应用于系统评价'}
{'arxiv_id': 'arXiv:2508.15820', 'title': 'Research on intelligent generation of structural demolition suggestions based on multi-model collaboration', 'authors': 'Zhifeng Yang, Peizong Wu', 'link': 'https://arxiv.org/abs/2508.15820', 'abstract': 'The steel structure demolition scheme needs to be compiled according to the specific engineering characteristics and the update results of the finite element model. The designers need to refer to the relevant engineering cases according to the standard requirements when compiling. It takes a lot of time to retrieve information and organize language, and the degree of automation and intelligence is low. This paper proposes an intelligent generation method of structural demolition suggestions based on multi-model collaboration, and improves the text generation performance of large language models in the field of structural demolition by Retrieval-Augmented Generation and Low-Rank Adaptation Fine-Tuning technology. The intelligent generation framework of multi-model collaborative structural demolition suggestions can start from the specific engineering situation, drive the large language model to answer with anthropomorphic thinking, and propose demolition suggestions that are highly consistent with the characteristics of the structure. Compared with CivilGPT, the multi-model collaboration framework proposed in this paper can focus more on the key information of the structure, and the suggestions are more targeted.', 'abstract_zh': '基于多模型协作的智能结构拆卸建议生成方法及Retrieval-Augmented Generation和低秩适配微调技术在结构拆卸领域的文本生成性能提升', 'title_zh': '基于多模型协作的结构拆除建议智能化生成研究'}
{'arxiv_id': 'arXiv:2508.15815', 'title': 'User-Assistant Bias in LLMs', 'authors': 'Xu Pan, Jingxuan Fan, Zidi Xiong, Ely Hahami, Jorin Overwiening, Ziqian Xie', 'link': 'https://arxiv.org/abs/2508.15815', 'abstract': "Large language models (LLMs) can bias towards relying on their own or the user's information in chat history, leading to overly stubborn or agreeable behaviors in multi-turn conversations. In this paper, we formalize this model characteristic as user-assistant bias and introduce an 8k multi-turn conversation dataset $\\textbf{UserAssist}$, which we use to benchmark, understand and manipulate the user-assistant bias in frontier LLMs. Leveraging $\\textbf{UserAssist-test}$, we first benchmark the user-assistant bias of 26 commercial and 26 open-weight models. Commercial models show various levels of user bias. Evaluation on open-weight models reveals significant user bias in the instruction-tuned models, and weak user bias in reasoning (or reasoning-distilled) models. We then perform controlled fine-tuning experiments to pinpoint the post-training recipe contributing to these bias shifts: human preference alignment increases user bias, while training on chain-of-thought reasoning traces decreases it. Finally, we demonstrate that user-assistant bias can be bidirectionally adjusted by performing direct preference optimization (DPO) on $\\textbf{UserAssist-train}$, and generalizes well to both in-domain and out-of-domain conversations. Our results provide insights into how the LLM integrates information from different sources, and also a viable way to detect and control model abnormalities.", 'abstract_zh': '大型语言模型（LLMs）可能会倾向于依赖自身或用户在对话历史中的信息，导致在多轮对话中表现出过于固执或顺从的行为。在本文中，我们将这种模型特性正式化为用户-助手偏见，并引入了包含8000个对话轮次的数据集$\\textbf{UserAssist}$，以此来评估、理解并操控前沿LLM中的用户-助手偏见。利用$\\textbf{UserAssist-test}$，我们首先评估了26个商用和26个开源重量模型的用户-助手偏见。商用模型显示不同程度的用户偏见。对开源模型的评估表明，在指令调优模型中有显著的用户偏见，在推理（或推理提炼）模型中则偏见较弱。随后，我们进行了受控的微调实验，以确定导致这些偏见变化的后训练食谱：人类偏好对齐会增加用户偏见，而在chain-of-thought推理轨迹上训练则会减少用户偏见。最后，我们通过直接偏好优化（DPO）在$\\textbf{UserAssist-train}$上直接对用户-助手偏见进行双向调整，并且这种调整在领域内和领域外对话中都能很好地泛化。我们的结果为理解LLM如何集成不同来源的信息提供了洞见，同时也提供了一种检测和控制模型异常的有效方法。', 'title_zh': 'LLMs中的人工助手偏见'}
{'arxiv_id': 'arXiv:2508.15813', 'title': 'SCOPE: A Generative Approach for LLM Prompt Compression', 'authors': 'Tinghui Zhang, Yifan Wang, Daisy Zhe Wang', 'link': 'https://arxiv.org/abs/2508.15813', 'abstract': 'Prompt compression methods enhance the efficiency of Large Language Models (LLMs) and minimize the cost by reducing the length of input context. The goal of prompt compression is to shorten the LLM prompt while maintaining a high generation quality. However, existing solutions, mainly based on token removal, face challenges such as information loss and structural incoherence, like missing grammar elements in a sentence, or incomplete word phrases after token removal. Such challenges limit the final generation quality of LLM.\nTo overcome these limitations, we present a novel generative prompt compression method. Unlike the existing token removal methods, our method centers at a chunking-and-summarization mechanism. Specifically, our method splits prompt into semantically coherent chunks and rewrites the chunks to be more concise. The chunks are reconstructed into meaningful prompt finally. We design several optimization techniques for the mechanism, including optimized semantic chunking, outlier chunk handling, dynamic compression ratio, compression prioritization, and keyword maintaining. These techniques effectively improve the identifying and preserving of critical information and coherence among texts, as well as providing finer grind control of the compression ratio. We conduct extensive evaluation on question-answering and summarization tasks, with datasets covering multiple different domain. The evaluation shows our method achieves a significantly better compression quality, and higher stability than the state-of-the-art methods, especially under high compression ratio, which proves the effectiveness and practicality of our method.', 'abstract_zh': '提示压缩方法提高大型语言模型的效率并降低生成成本通过缩短输入上下文的长度。提示压缩的目标是在保持高生成质量的同时缩短大型语言模型的提示。然而，现有的解决方案，主要依靠词元移除，面临着诸如信息丢失和结构不连贯等问题，例如在词元移除后句子缺少语法元素或不完整的词组。这些挑战限制了大型语言模型最终的生成质量。\n\n为了克服这些限制，我们提出了一种新颖的生成提示压缩方法。与现有的词元移除方法不同，我们的方法以切分和总结机制为中心。具体而言，我们的方法将提示切分成语义连贯的片段，并对这些片段进行更简洁的重写。最终将这些片段重构为有意义的提示。我们为此机制设计了多种优化技术，包括优化的语义切分、异常切片处理、动态压缩比、压缩优先级和关键词保持。这些技术有效提高了关键信息的识别和保留以及文本之间连贯性的程度，并提供了更精细的压缩比控制。我们在多个领域的问答和总结任务上进行了广泛的评估。评估结果显示，我们的方法在压缩质量和稳定性方面显著优于现有最佳方法，特别是在高压缩比下，证明了该方法的有效性和实用性。', 'title_zh': 'SCOPE: 一种生成式LLM提示压缩方法'}
{'arxiv_id': 'arXiv:2508.15810', 'title': 'Detecting Hope, Hate, and Emotion in Arabic Textual Speech and Multi-modal Memes Using Large Language Models', 'authors': 'Nouar AlDahoul, Yasir Zaki', 'link': 'https://arxiv.org/abs/2508.15810', 'abstract': 'The rise of social media and online communication platforms has led to the spread of Arabic textual posts and memes as a key form of digital expression. While these contents can be humorous and informative, they are also increasingly being used to spread offensive language and hate speech. Consequently, there is a growing demand for precise analysis of content in Arabic text and memes. This paper explores the potential of large language models to effectively identify hope, hate speech, offensive language, and emotional expressions within such content. We evaluate the performance of base LLMs, fine-tuned LLMs, and pre-trained embedding models. The evaluation is conducted using a dataset of Arabic textual speech and memes proposed in the ArabicNLP MAHED 2025 challenge. The results underscore the capacity of LLMs such as GPT-4o-mini, fine-tuned with Arabic textual speech, and Gemini Flash 2.5, fine-tuned with Arabic memes, to deliver the superior performance. They achieve up to 72.1%, 57.8%, and 79.6% macro F1 scores for tasks 1, 2, and 3, respectively, and secure first place overall in the Mahed 2025 challenge. The proposed solutions offer a more nuanced understanding of both text and memes for accurate and efficient Arabic content moderation systems.', 'abstract_zh': '社交媒体和在线通信平台的兴起导致阿拉伯文本帖子和 meme 成为数字表达的重要形式。虽然这些内容可以是幽默和信息性的，但它们也被越来越多地用于传播冒犯语言和仇恨言论。因此，对阿拉伯文本内容和 meme 的精确分析需求 growing。本文探讨了大规模语言模型在有效识别希望、仇恨言论、冒犯语言和情感表达方面的能力。我们评估了基线语言模型、微调语言模型和预训练嵌入模型的性能。评估使用了阿拉伯NLP MAHED 2025挑战中提议的阿拉伯文本演讲和 meme 数据集。结果表明，微调了阿拉伯文本演讲的 GPT-4o-mini 和微调了阿拉伯 memes 的 Gemini Flash 2.5 具有优越的表现，分别在任务 1、2 和 3 中实现了最高达 72.1%、57.8% 和 79.6% 的宏 F1 得分，并在 Mahed 2025 挑战中获得总体第一。提出的解决方案为准确高效的阿拉伯内容审核系统提供了更细致的理解。', 'title_zh': '使用大型语言模型检测阿拉伯文本语音和多模态 meme 中的希望、仇恨与情绪'}
{'arxiv_id': 'arXiv:2508.15809', 'title': 'Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration', 'authors': 'Songyuan Sui, Hongyi Liu, Serena Liu, Li Li, Soo-Hyun Choi, Rui Chen, Xia Hu', 'link': 'https://arxiv.org/abs/2508.15809', 'abstract': 'Table understanding requires structured, multi-step reasoning. Large Language Models (LLMs) struggle with it due to the structural complexity of tabular data. Recently, multi-agent frameworks for SQL generation have shown promise in tackling the challenges of understanding tabular data, but existing approaches often suffer from limitations such as the inability to comprehend table structure for reliable SQL generation, error propagation that results in invalid queries, and over-reliance on execution correctness. To address these issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for SQL-aided table understanding. CoQ adopts natural-language-style representations of table schemas to abstract away structural noise and enhance understanding. It employs a clause-by-clause SQL generation strategy to improve query quality and introduces a hybrid reasoning division that separates SQL-based mechanical reasoning from LLM-based logical inference, thereby reducing reliance on execution outcomes. Experiments with four models (both closed- and open-source) across five widely used benchmarks show that Chain-of-Query significantly improves accuracy from 61.11% to 74.77% and reduces the invalid SQL rate from 9.48% to 3.34%, demonstrating its superior effectiveness in table understanding. The code is available at this https URL.', 'abstract_zh': '表理解需要结构化的多步骤推理。大型语言模型（LLMs）由于表格数据的结构复杂性而在表理解方面存在困难。最近，用于SQL生成的多agent框架显示出攻克表格数据理解挑战的潜力，但现有方法经常受到理解表结构以进行可靠SQL生成、错误传播导致无效查询以及过度依赖执行正确性的限制。为解决这些问题，我们提出了链式查询（CoQ），这是一种新型的SQL辅助表理解的多agent框架。CoQ采用自然语言风格的表模式表示方式来抽象结构噪声并增强理解。它采用逐句SQL生成策略以提高查询质量，并引入了一种混合推理分工，将基于SQL的机械推理与基于LLM的逻辑推理分离，从而减少对执行结果的依赖。在五个广泛使用的基准上的四项模型（开源和闭源）实验表明，链式查询将准确性显著提高至74.77%，并将无效SQL率从9.48%降低至3.34%，证明了其在表理解方面的优越效果。代码见这个链接。', 'title_zh': '链式查询：通过多agent协作在SQL辅助表理解中释放大语言模型的强大功能'}
{'arxiv_id': 'arXiv:2508.15807', 'title': 'KL-based self-distillation for large language models', 'authors': 'Max Rehman Linder', 'link': 'https://arxiv.org/abs/2508.15807', 'abstract': 'Large pre-trained language models often struggle to incorporate new domain-specific terminology when fine-tuned on small, specialized corpora. In this work, we address the challenge of vocabulary expansion in frozen LLMs by introducing a mathematically grounded method for knowledge distillation via KL divergence, even when the original and extended models use different tokenizations. This allows the student model to inherit distributional knowledge from the teacher despite differing vocabularies. We compare our KL-based distillation approach to conventional cross-entropy training, evaluating both methods across multiple strategies for initializing new token embeddings. After embedding initialization, models are further fine-tuned to integrate the new vocabulary. Each trained model is benchmarked on approximately 2000 code-generation tasks, where our approach achieves the best performance across the board. Finally, through mechanistic interpretability, we analyze how models learn representations for the new tokens, providing an explanation for the observed gains and offering insight into the structure of embedding space during vocabulary expansion.', 'abstract_zh': '大型预训练语言模型在使用小规模专门数据集进行微调时往往难以整合新的领域特定术语。在本文中，我们通过引入基于KL散 오표的方式在冻结的LLM中扩展词汇量，即使原始模型和扩展模型使用不同的标记化方法，这种方法也具有数学依据。这使得学生模型能够继承来自教师模型的分布知识，即使它们的词汇表不同。我们将基于KL散 오표的知识蒸馏方法与传统的交叉熵训练方法进行比较，评估两种方法在多种初始化新词嵌入策略下的表现。在嵌入初始化之后，模型进一步微调以整合新的词汇表。每个训练好的模型都在约2000个代码生成任务上进行基准测试，其中我们的方法在所有任务中都取得了最佳性能。最后，通过机制可解释性，我们分析模型如何学习新词的表示，为观察到的性能提升提供了解释，并提供了词汇量扩展过程中嵌入空间结构的见解。', 'title_zh': '基于KL散度的自蒸馏大型语言模型'}
{'arxiv_id': 'arXiv:2508.15806', 'title': 'SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need for Robust KV Cache Compression', 'authors': 'Mengjie Li, William J. Song', 'link': 'https://arxiv.org/abs/2508.15806', 'abstract': 'The increasing input sequence length in Large Language Models (LLMs) puts significant pressure on key-value (KV) cache storage, making efficient inference challenging. Explicitly distinguishing attention behavior into our self-defined surface memorization and logic construction reveals essential roles in long-context reasoning. We observe that an individual attention head can display various behaviors, with nearly 98.5% effectively ignoring completely irrelevant information. The remaining 1.5% behaves as logic construction, and 0.5% behaves as surface memorization. Based on layer- and head-wise integration, we propose a novel two-stage SurfaceLogicKV method to utilize these attention behaviors for KV Cache compression. As a result, it achieves improved compressing robustness while maintaining competitive performance across various tasks and long sequences compared to baselines or even FullKV in some specific situations', 'abstract_zh': '大型语言模型中输入序列长度的增加对关键值缓存存储造成了显著压力，使得高效的推理变得具有挑战性。我们通过将注意力行为明确区分为我们自定义的表层记忆和逻辑构建，揭示了其在长上下文推理中的关键作用。我们观察到，单个注意力头可以表现出各种行为，其中约98.5%有效地忽略了完全无关的信息，剩余的1.5%表现为逻辑构建，0.5%表现为表层记忆。基于层级和头级的集成，我们提出了一种新的两阶段SurfaceLogicKV方法，以利用这些注意力行为进行关键值缓存压缩。结果显示，该方法在各种任务和长序列上优于基准模型或甚至FullKV，实现了增强的压缩稳健性并保持了竞争力。', 'title_zh': 'SurfaceLogicKV: 表面和逻辑注意力行为足以实现稳健的KV缓存压缩'}
{'arxiv_id': 'arXiv:2508.15805', 'title': 'ALAS: Autonomous Learning Agent for Self-Updating Language Models', 'authors': 'Dhruv Atreja', 'link': 'https://arxiv.org/abs/2508.15805', 'abstract': "Large language models (LLMs) often have a fixed knowledge cutoff, limiting their accuracy on emerging information. We present ALAS (Autonomous Learning Agent System), a modular pipeline that continuously updates an LLM's knowledge with minimal human intervention. ALAS autonomously generates a learning curriculum for a target domain, retrieves up-to-date information from the web (with citations), distills this into question-answer training data, and fine-tunes the model through supervised fine-tuning (SFT) and direct preference optimization (DPO). It iteratively evaluates performance and revises the curriculum, enabling long-term continual learning. We demonstrate ALAS's ability to self-improve a model on rapidly evolving domains (e.g., new Python releases, latest security CVEs, academic trends), significantly boosting post-cutoff question answering accuracy (from 15% to 90% on average) without manual dataset curation. The system emphasizes modularity and reproducibility: each component (planning, retrieval, distillation, memory, fine-tuning) is interchangeable and built on standard APIs. We discuss comparative baselines (e.g., retrieval-augmented generation vs. fine-tuning) and show that ALAS achieves 90% accuracy on knowledge-updated queries with minimal engineering overhead. Finally, we outline limitations (cost, dependency on source quality) and future directions for autonomous lifelong learning in LLMs.", 'abstract_zh': '自主学习代理系统（ALAS）：一种无需大量人工干预即可连续更新大型语言模型知识的模块化管道', 'title_zh': 'ALAS：自主学习代理用于自我更新的语言模型'}
{'arxiv_id': 'arXiv:2508.15804', 'title': 'ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks', 'authors': 'Minghao Li, Ying Zeng, Zhihao Cheng, Cong Ma, Kai Jia', 'link': 'https://arxiv.org/abs/2508.15804', 'abstract': 'The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, a systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domain-specific prompts and establish a comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released at the following link: this https URL', 'abstract_zh': '深度研究代理的出现大大缩短了进行广泛研究任务所需的时间。然而，这些任务本质上需要严格的事实准确性标准和全面性，需要在广泛应用之前进行彻底评估。本文提出ReportBench，这是一个系统性的基准，旨在评估由大语言模型（LLMs）生成的研究报告的内容质量。我们的评估集中于两个关键维度：(1) 引用文献的质量和相关性，以及(2) 报告中陈述的忠实性和真实性。ReportBench 利用可从arXiv获取的高质量已发表综述论文作为黄金标准参考，从中应用逆向指令工程来推导出专业领域的提示并建立一个全面的评估语料库。此外，ReportBench 中发展了一个基于代理的自动化框架，该框架系统地分析生成的报告，提取引用和陈述，检查引用内容的忠实性与原始来源的一致性，并利用网络资源验证未引用的声明。实证评估表明，如OpenAI和Google开发的商业深度研究代理一致生成了比配备搜索或浏览工具的独立LLMs更全面和可靠的研究报告。然而，在研究覆盖的广度和深度以及事实一致性方面仍有很大的改进空间。完整的代码和数据将在此链接发布：this https URL。', 'title_zh': 'ReportBench: 通过学术调查任务评估深度研究代理'}
{'arxiv_id': 'arXiv:2508.15802', 'title': 'MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding', 'authors': 'Mohan Jiang, Jin Gao, Jiahao Zhan, Dequan Wang', 'link': 'https://arxiv.org/abs/2508.15802', 'abstract': 'As multimodal large language models (MLLMs) grow increasingly capable, fixed benchmarks are gradually losing their effectiveness in evaluating high-level scientific understanding. In this paper, we introduce the Multimodal Academic Cover benchmark (MAC), a live benchmark that could continuously evolve with scientific advancement and model progress. MAC leverages over 25,000 image-text pairs sourced from issues of top-tier scientific journals such as Nature, Science, and Cell, challenging MLLMs to reason across abstract visual and textual scientific content. Experiments on our most recent yearly snapshot, MAC-2025, reveal that while MLLMs demonstrate strong perceptual abilities, their cross-modal scientific reasoning remains limited. To bridge this gap, we propose DAD, a lightweight inference-time approach that enhances MLLMs by extending MLLM visual features with language space reasoning, achieving performance improvements of up to 11%. Finally, we highlight the live nature of MAC through experiments on updating journal covers and models for curation, illustrating its potential to remain aligned with the frontier of human knowledge. We release our benchmark at this https URL.', 'abstract_zh': '随着多模态大型语言模型（MLLMs）的能力逐步增强，固定基准在评估高层次科学理解方面的有效性逐渐降低。本文介绍了多模态学术封面基准（MAC），这是一个能够随着科学进步和模型发展不断进化的活基准。MAC 利用了来自《Nature》、《Science》和《Cell》等顶尖科学期刊的超过 25,000 张图像-文本对，挑战 MLLMs 在抽象视觉和文本科学内容上的推理能力。我们的最新年度快照 MAC-2025 实验表明，尽管 MLLMs 在感知能力上表现出色，但在跨模态科学推理方面仍有限制。为弥补这一差距，我们提出了 DAD，这是一种轻量级的推理时方法，通过将 MLLM 视觉特征扩展到语言空间推理来增强 MLLMs，实现了高达 11% 的性能提升。最后，我们通过更新期刊封面和模型以进行策展的实验，突出了 MAC 的实时特性，展示了其与人类知识前沿保持一致的潜力。我们在此处 https:// 这里发布我们的基准。', 'title_zh': 'MAC：科学理解中多模态大型语言模型的现场基准测验'}
{'arxiv_id': 'arXiv:2508.15801', 'title': 'LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions', 'authors': 'Seyedali Mohammadi, Manas Paldhe, Amit Chhabra', 'link': 'https://arxiv.org/abs/2508.15801', 'abstract': "Phone call transcript labeling is prohibitively expensive (approximately 2 USD per minute) due to privacy regulations, consent requirements, and manual annotation costs requiring 3 hours of expert time per hour of audio. Existing extraction methods fail on conversational speech containing disfluencies, interruptions, and speaker overlap. We introduce LingVarBench, a synthetic data generation pipeline that addresses these constraints through automated validation. First, we prompt an LLM to generate realistic structured field values across multiple use cases. Second, we recursively prompt the model to transform these values into thousands of natural conversational utterances containing typical phone call characteristics. Third, we validate each synthetic utterance by testing whether a separate LLM-based extractor can recover the original structured information. We employ DSPy's SIMBA optimizer to automatically synthesize extraction prompts from validated synthetic transcripts, eliminating manual prompt engineering. Our optimized prompts achieve up to 95 percent accuracy for numeric fields (vs. 88-89 percent zero-shot), 90 percent for names (vs. 47-79 percent), and over 80 percent for dates (vs. 72-77 percent) on real customer transcripts, demonstrating substantial gains over zero-shot prompting. The synthetic-to-real transfer demonstrates that conversational patterns learned from generated data generalize effectively to authentic phone calls containing background noise and domain-specific terminology. LingVarBench provides the first systematic benchmark for structured extraction from synthetic conversational data, demonstrating that automated prompt optimization overcomes cost and privacy barriers preventing large-scale phone call analysis in commercial settings.", 'abstract_zh': '手机通话转录标注由于隐私规定、同意要求和手动标注成本（每分钟约2美元）而代价高昂（每小时音频需要3小时专家时间）。现有提取方法在包含不连贯、打断和发言人重叠的对话演讲中失效。我们引入了LingVarBench，一个通过自动化验证解决这些限制的合成数据生成流水线。首先，我们提示一个LLM生成多场景下的现实结构化字段值。其次，我们递归提示模型将这些值转换为包含典型通话特征的数千个自然对话陈述。第三，我们通过测试一个基于LLM的提取器能否恢复原始结构化信息来验证每个合成陈述。我们使用DSPy的SIMBA优化器从验证的合成转录中自动合成分提取提示，消除手动提示工程。优化的提示在真实客户转录中实现高达95%的数字字段准确率（优于零样本的88-89%），90%的姓名准确率（优于零样本的47-79%），以及超过80%的日期准确率（优于零样本的72-77%），展示了相较于零样本提示的重大改进。合成到实际的转移表明，从生成数据中学到的对话模式能够有效推广到包含背景噪音和领域特定术语的真实通话中。LingVarBench提供了结构化从合成对话数据中提取的第一个系统基准，展示了自动化提示优化克服了大规模商业环境中文本电话分析的成本和隐私障碍。', 'title_zh': 'LingVarBench: 评估大规模语言模型在结构化合成语音转写中自动化命名实体识别上的性能'}
{'arxiv_id': 'arXiv:2508.15798', 'title': 'Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models', 'authors': 'Saumya Roy', 'link': 'https://arxiv.org/abs/2508.15798', 'abstract': 'Warning: This research studies AI persuasion and bias amplification that could be misused; all experiments are for safety evaluation. Large Language Models (LLMs) now generate convincing, human-like text and are widely used in content creation, decision support, and user interactions. Yet the same systems can spread information or misinformation at scale and reflect social biases that arise from data, architecture, or training choices. This work examines how persuasion and bias interact in LLMs, focusing on how imperfect or skewed outputs affect persuasive impact. Specifically, we test whether persona-based models can persuade with fact-based claims while also, unintentionally, promoting misinformation or biased narratives.\nWe introduce a convincer-skeptic framework: LLMs adopt personas to simulate realistic attitudes. Skeptic models serve as human proxies; we compare their beliefs before and after exposure to arguments from convincer models. Persuasion is quantified with Jensen-Shannon divergence over belief distributions. We then ask how much persuaded entities go on to reinforce and amplify biased beliefs across race, gender, and religion. Strong persuaders are further probed for bias using sycophantic adversarial prompts and judged with additional models.\nOur findings show both promise and risk. LLMs can shape narratives, adapt tone, and mirror audience values across domains such as psychology, marketing, and legal assistance. But the same capacity can be weaponized to automate misinformation or craft messages that exploit cognitive biases, reinforcing stereotypes and widening inequities. The core danger lies in misuse more than in occasional model mistakes. By measuring persuasive power and bias reinforcement, we argue for guardrails and policies that penalize deceptive use and support alignment, value-sensitive design, and trustworthy deployment.', 'abstract_zh': '警告：本研究探讨了AI说服和偏见放大可能被滥用的情况；所有实验均用于安全评估。大型语言模型（LLMs）现在能够生成令人信服、类人的文本，并广泛用于内容创作、决策支持和用户互动。然而，相同的系统还可以大规模传播信息或谬误信息，并反映出源自数据、架构或训练选择的社会偏见。本工作研究了LLMs中说服和偏见的交互作用，重点在于不完美或失真的输出如何影响说服效果。具体而言，我们测试基于人设的模型是否能够在基于事实的主张中说服他人，同时也可能无意中促进谬误或偏见叙述。\n\n我们引入了一个说服者-怀疑者框架：LLMs采用人设来模拟现实态度。怀疑者模型作为人类代理，我们比较它们在接触说服者模型论点前后的信念。说服效果通过信念分布的 Jensen-Shannon 散度进行量化。然后我们考察被说服实体如何进一步强化和放大关于种族、性别和宗教等方面的偏见信念。强大的说服者进一步使用阿谀对抗提示测试偏见，并使用其他模型进行评判。\n\n我们的发现表明潜在的风险和机遇并存。LLMs可以塑造叙述、调整语气并在心理学、营销和法律援助等领域镜像受众价值观。然而，相同的容量也可能被武器化以自动化谬误传播或编造利用认知偏见的消息，从而强化刻板印象和扩大不平等。核心危险在于滥用而非偶尔的模型错误。通过测量说服力和偏见强化，我们主张设立护栏和政策，惩罚误导性使用并支持对齐、价值观敏感设计和可信赖部署。', 'title_zh': 'LLM中的说服力与偏见：探究说服力与偏见强化对语言模型的影响'}
{'arxiv_id': 'arXiv:2508.15797', 'title': 'Benchmarking the Medical Understanding and Reasoning of Large Language Models in Arabic Healthcare Tasks', 'authors': 'Nouar AlDahoul, Yasir Zaki', 'link': 'https://arxiv.org/abs/2508.15797', 'abstract': 'Recent progress in large language models (LLMs) has showcased impressive proficiency in numerous Arabic natural language processing (NLP) applications. Nevertheless, their effectiveness in Arabic medical NLP domains has received limited investigation. This research examines the degree to which state-of-the-art LLMs demonstrate and articulate healthcare knowledge in Arabic, assessing their capabilities across a varied array of Arabic medical tasks. We benchmark several LLMs using a medical dataset proposed in the Arabic NLP AraHealthQA challenge in MedArabiQ2025 track. Various base LLMs were assessed on their ability to accurately provide correct answers from existing choices in multiple-choice questions (MCQs) and fill-in-the-blank scenarios. Additionally, we evaluated the capacity of LLMs in answering open-ended questions aligned with expert answers. Our results reveal significant variations in correct answer prediction accuracy and low variations in semantic alignment of generated answers, highlighting both the potential and limitations of current LLMs in Arabic clinical contexts. Our analysis shows that for MCQs task, the proposed majority voting solution, leveraging three base models (Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms others, achieving up to 77% accuracy and securing first place overall in the Arahealthqa 2025 shared task-track 2 (sub-task 1) challenge. Moreover, for the open-ended questions task, several LLMs were able to demonstrate excellent performance in terms of semantic alignment and achieve a maximum BERTScore of 86.44%.', 'abstract_zh': 'Recent进展在大型语言模型（LLMs）在阿拉伯医学自然语言处理（NLP）领域的应用中取得了显著成效，然而它们在这方面的有效性仍然缺乏深入研究。本研究考察了最先进的LLMs在阿拉伯医学领域展示和表达医疗知识的程度，评估了它们在各种阿拉伯医学任务中的能力。我们使用MedArabiQ2025 AraHealthQA赛道提出的医学数据集对几种LLMs进行了基准测试。我们评估了多种基础LLMs在多项选择题（MCQs）和填空题场景中提供正确答案的能力，以及在回答与专家答案对齐的开放性问题方面的能力。我们的结果显示，MCQs任务中提出的基于多数投票的解决方案，利用Gemini Flash 2.5、Gemini Pro 2.5 和 GPT o3三种基础模型，表现最佳，准确率达到77%，在AraHealthQA 2025共享任务-2（子任务1）挑战中排名第一。此外，在开放性问题任务中，多种LLMs在语义对齐方面表现出色，达到了最高的BERTScore为86.44%。', 'title_zh': '阿拉伯医疗任务中大规模语言模型的医学理解与推理基准研究'}
{'arxiv_id': 'arXiv:2508.15796', 'title': 'Benchmarking the Legal Reasoning of LLMs in Arabic Islamic Inheritance Cases', 'authors': 'Nouar AlDahoul, Yasir Zaki', 'link': 'https://arxiv.org/abs/2508.15796', 'abstract': 'Islamic inheritance domain holds significant importance for Muslims to ensure fair distribution of shares between heirs. Manual calculation of shares under numerous scenarios is complex, time-consuming, and error-prone. Recent advancements in Large Language Models (LLMs) have sparked interest in their potential to assist with complex legal reasoning tasks. This study evaluates the reasoning capabilities of state-of-the-art LLMs to interpret and apply Islamic inheritance laws. We utilized the dataset proposed in the ArabicNLP QIAS 2025 challenge, which includes inheritance case scenarios given in Arabic and derived from Islamic legal sources. Various base and fine-tuned models, are assessed on their ability to accurately identify heirs, compute shares, and justify their reasoning in alignment with Islamic legal principles. Our analysis reveals that the proposed majority voting solution, leveraging three base models (Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms all other models that we utilized across every difficulty level. It achieves up to 92.7% accuracy and secures the third place overall in Task 1 of the Qias 2025 challenge.', 'abstract_zh': '伊斯兰继承领域对于确保穆斯林在多种情景下公平分配遗产份额具有重要意义。手动计算复杂的遗产份额既耗时又容易出错。近年来，大型语言模型（LLMs）的发展引发了其在复杂法律推理任务中潜力的关注。本研究评估了最先进的LLMs在解读和应用伊斯兰继承法方面的推理能力。我们利用了在阿拉伯NLP QIAS 2025挑战中提出的数据集，该数据集包含用阿拉伯语给出的继承案例场景，并来自伊斯兰法律来源。各种基础和微调模型被评估其准确识别继承人、计算份额以及在符合伊斯兰法律原则的前提下合理解释推理的能力。我们的分析显示，利用三种基础模型（Gemini Flash 2.5、Gemini Pro 2.5 和 GPT o3）的重大投票解决方案，在所有难度级别上均优于我们使用的其他所有模型。该方案在Qias 2025挑战任务1中取得了高达92.7%的准确率，并获得第三名。', 'title_zh': '阿拉伯伊斯兰继承案例中LLMs的法律推理benchmark研究'}
{'arxiv_id': 'arXiv:2508.15790', 'title': 'KG-o1: Enhancing Multi-hop Question Answering in Large Language Models via Knowledge Graph Integration', 'authors': 'Nan Wang, Yongqi Fan, yansha zhu, ZongYu Wang, Xuezhi Cao, Xinyan He, Haiyun Jiang, Tong Ruan, Jingping Liu', 'link': 'https://arxiv.org/abs/2508.15790', 'abstract': 'Large Language Models (LLMs) face challenges in knowledge-intensive reasoning tasks like classic multi-hop question and answering, which involves reasoning across multiple facts. This difficulty arises because the chain of thoughts (CoTs) generated by LLMs in such tasks often deviate from real or a priori reasoning paths. In contrast, knowledge graphs (KGs) explicitly represent the logical connections between facts through entities and relationships. This reflects a significant gap. Meanwhile, large reasoning models (LRMs), such as o1, have demonstrated that long-step reasoning significantly enhances the performance of LLMs. Building on these insights, we propose KG-o1, a four-stage approach that integrates KGs to enhance the multi-hop reasoning abilities of LLMs. We first filter out initial entities and generate complex subgraphs. Secondly, we construct logical paths for subgraphs and then use knowledge graphs to build a dataset with a complex and extended brainstorming process, which trains LLMs to imitate long-term reasoning. Finally, we employ rejection sampling to generate a self-improving corpus for direct preference optimization (DPO), further refining the LLMs reasoning abilities. We conducted experiments on two simple and two complex datasets. The results show that KG-o1 models exhibit superior performance across all tasks compared to existing LRMs.', 'abstract_zh': '大型语言模型在知识密集型推理任务中面临着挑战，如经典的多跳问答，这涉及跨越多个事实的推理。这种困难源于大型语言模型在这种任务中生成的思维链（CoTs）往往偏离真实或先验的推理路径。相比之下，知识图谱（KGs）明确地通过实体和关系表示事实之间的逻辑连接。这反映出一个重要差距。同时，大型推理模型（LRMs），如o1，已经证明长步骤推理显著提升了大型语言模型的表现。基于这些洞察，我们提出了KG-o1，一种四阶段方法，将知识图谱集成到大型语言模型中，以增强其多跳推理能力。我们首先过滤初始实体并生成复杂子图。其次，我们为子图构建逻辑路径，并利用知识图谱构建数据集，通过一个复杂且扩展的头脑风暴过程，训练大型语言模型模仿长期推理。最后，我们采用拒绝采样生成自我改进语料库，进一步通过直接偏好优化（DPO）精炼大型语言模型的推理能力。我们在两个简单和两个复杂的数据集上进行了实验。结果显示，KG-o1模型在所有任务中的表现均优于现有LRMs。', 'title_zh': 'KG-o1：通过知识图谱集成增强大规模语言模型的多跳问答能力'}
