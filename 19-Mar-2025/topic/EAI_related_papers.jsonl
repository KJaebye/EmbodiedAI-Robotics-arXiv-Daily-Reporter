{'arxiv_id': 'arXiv:2503.14352', 'title': 'Flying in Highly Dynamic Environments with End-to-end Learning Approach', 'authors': 'Xiyu Fan, Minghao Lu, Bowen Xu, Peng Lu', 'link': 'https://arxiv.org/abs/2503.14352', 'abstract': 'Obstacle avoidance for unmanned aerial vehicles like quadrotors is a popular research topic. Most existing research focuses only on static environments, and obstacle avoidance in environments with multiple dynamic obstacles remains challenging. This paper proposes a novel deep-reinforcement learning-based approach for the quadrotors to navigate through highly dynamic environments. We propose a lidar data encoder to extract obstacle information from the massive point cloud data from the lidar. Multi frames of historical scans will be compressed into a 2-dimension obstacle map while maintaining the obstacle features required. An end-to-end deep neural network is trained to extract the kinematics of dynamic and static obstacles from the obstacle map, and it will generate acceleration commands to the quadrotor to control it to avoid these obstacles. Our approach contains perception and navigating functions in a single neural network, which can change from a navigating state into a hovering state without mode switching. We also present simulations and real-world experiments to show the effectiveness of our approach while navigating in highly dynamic cluttered environments.', 'abstract_zh': '基于深度强化学习的四旋翼无人机在高动态环境中的避障方法', 'title_zh': '在端到端学习方法下高度动态环境中的飞行控制'}
{'arxiv_id': 'arXiv:2503.14268', 'title': 'Pushing Everything Everywhere All At Once: Probabilistic Prehensile Pushing', 'authors': 'Patrizio Perugini, Jens Lundell, Katharina Friedl, Danica Kragic', 'link': 'https://arxiv.org/abs/2503.14268', 'abstract': 'We address prehensile pushing, the problem of manipulating a grasped object by pushing against the environment. Our solution is an efficient nonlinear trajectory optimization problem relaxed from an exact mixed integer non-linear trajectory optimization formulation. The critical insight is recasting the external pushers (environment) as a discrete probability distribution instead of binary variables and minimizing the entropy of the distribution. The probabilistic reformulation allows all pushers to be used simultaneously, but at the optimum, the probability mass concentrates onto one due to the entropy minimization. We numerically compare our method against a state-of-the-art sampling-based baseline on a prehensile pushing task. The results demonstrate that our method finds trajectories 8 times faster and at a 20 times lower cost than the baseline. Finally, we demonstrate that a simulated and real Franka Panda robot can successfully manipulate different objects following the trajectories proposed by our method. Supplementary materials are available at this https URL.', 'abstract_zh': '我们解决了抓取物体并通过环境施加推力进行操作的prehensile pushing问题。我们的解决方案是从精确的混合整数非线性轨迹优化形式松驰而来的一个高效的非线性轨迹优化问题。关键洞察是将外部推力器（环境）重新定义为一个离散概率分布，而不是二进制变量，并且通过最小化分布的熵来实现。概率重构允许所有推力器同时使用，但在最优情况下，概率质量会集中在其中一个推力器上，这是由于熵最小化所致。我们对prehensile pushing任务与最先进的基于采样的基准方法进行了数值比较。结果表明，我们的方法比基准方法快8倍，并且成本低20倍。最后，我们展示了模拟和实际的Franka Panda机器人能够成功地按照我们的方法提出的轨迹操作不同物体。补充材料可在以下链接获取：这个 https URL。', 'title_zh': '把一切推送到每一个地方，同时进行：概率性的prehensile推举'}
{'arxiv_id': 'arXiv:2503.14254', 'title': 'CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented Robot Exploration', 'authors': 'Chunyu Yang, Shengben Bi, Yihui Xu, Xin Zhang', 'link': 'https://arxiv.org/abs/2503.14254', 'abstract': "With the increasing demand for efficient and flexible robotic exploration solutions, Reinforcement Learning (RL) is becoming a promising approach in the field of autonomous robotic exploration. However, current RL-based exploration algorithms often face limited environmental reasoning capabilities, slow convergence rates, and substantial challenges in Sim-To-Real (S2R) transfer. To address these issues, we propose a Curriculum Learning-based Transformer Reinforcement Learning Algorithm (CTSAC) aimed at improving both exploration efficiency and transfer performance. To enhance the robot's reasoning ability, a Transformer is integrated into the perception network of the Soft Actor-Critic (SAC) framework, leveraging historical information to improve the farsightedness of the strategy. A periodic review-based curriculum learning is proposed, which enhances training efficiency while mitigating catastrophic forgetting during curriculum transitions. Training is conducted on the ROS-Gazebo continuous robotic simulation platform, with LiDAR clustering optimization to further reduce the S2R gap. Experimental results demonstrate the CTSAC algorithm outperforms the state-of-the-art non-learning and learning-based algorithms in terms of success rate and success rate-weighted exploration time. Moreover, real-world experiments validate the strong S2R transfer capabilities of CTSAC.", 'abstract_zh': '基于 Curriculum 学习的 Transformer 强化学习算法（CTSAC）在自主机器人探索中的应用', 'title_zh': '基于课程学习的变压器软 actor-critic 算法用于目标导向的机器人探索'}
{'arxiv_id': 'arXiv:2503.14182', 'title': 'Bridging Past and Future: End-to-End Autonomous Driving with Historical Prediction and Planning', 'authors': 'Bozhou Zhang, Nan Song, Xin Jin, Li Zhang', 'link': 'https://arxiv.org/abs/2503.14182', 'abstract': "End-to-end autonomous driving unifies tasks in a differentiable framework, enabling planning-oriented optimization and attracting growing attention. Current methods aggregate historical information either through dense historical bird's-eye-view (BEV) features or by querying a sparse memory bank, following paradigms inherited from detection. However, we argue that these paradigms either omit historical information in motion planning or fail to align with its multi-step nature, which requires predicting or planning multiple future time steps. In line with the philosophy of future is a continuation of past, we propose BridgeAD, which reformulates motion and planning queries as multi-step queries to differentiate the queries for each future time step. This design enables the effective use of historical prediction and planning by applying them to the appropriate parts of the end-to-end system based on the time steps, which improves both perception and motion planning. Specifically, historical queries for the current frame are combined with perception, while queries for future frames are integrated with motion planning. In this way, we bridge the gap between past and future by aggregating historical insights at every time step, enhancing the overall coherence and accuracy of the end-to-end autonomous driving pipeline. Extensive experiments on the nuScenes dataset in both open-loop and closed-loop settings demonstrate that BridgeAD achieves state-of-the-art performance.", 'abstract_zh': '端到端自主驾驶统一差异可微框架中的任务，实现面向规划的优化并引起广泛关注', 'title_zh': 'past和未来接轨：基于历史预测与规划的端到端自动驾驶'}
{'arxiv_id': 'arXiv:2503.13934', 'title': 'COLSON: Controllable Learning-Based Social Navigation via Diffusion-Based Reinforcement Learning', 'authors': 'Yuki Tomita, Kohei Matsumoto, Yuki Hyodo, Ryo Kurazume', 'link': 'https://arxiv.org/abs/2503.13934', 'abstract': 'Mobile robot navigation in dynamic environments with pedestrian traffic is a key challenge in the development of autonomous mobile service robots. Recently, deep reinforcement learning-based methods have been actively studied and have outperformed traditional rule-based approaches owing to their optimization capabilities. Among these, methods that assume a continuous action space typically rely on a Gaussian distribution assumption, which limits the flexibility of generated actions. Meanwhile, the application of diffusion models to reinforcement learning has advanced, allowing for more flexible action distributions compared with Gaussian distribution-based approaches. In this study, we applied a diffusion-based reinforcement learning approach to social navigation and validated its effectiveness. Furthermore, by leveraging the characteristics of diffusion models, we propose an extension that enables post-training action smoothing and adaptation to static obstacle scenarios not considered during the training steps.', 'abstract_zh': '基于扩散模型的强化学习在动态环境中移动机器人社交导航的研究与拓展', 'title_zh': 'COLSON: 基于扩散强化学习的可控学习社会导航'}
{'arxiv_id': 'arXiv:2503.13716', 'title': '16 Ways to Gallop: Energetics and Body Dynamics of High-Speed Quadrupedal Gaits', 'authors': 'Yasser G. Alqaham, Jing Cheng, Zhenyu Gan', 'link': 'https://arxiv.org/abs/2503.13716', 'abstract': "Galloping is a common high-speed gait in both animals and quadrupedal robots, yet its energetic characteristics remain insufficiently explored. This study systematically analyzes a large number of possible galloping gaits by categorizing them based on the number of flight phases per stride and the phase relationships between the front and rear legs, following Hildebrand's framework for asymmetrical gaits. Using the A1 quadrupedal robot from Unitree, we model galloping dynamics as a hybrid dynamical system and employ trajectory optimization (TO) to minimize the cost of transport (CoT) across a range of speeds. Our results reveal that rotary and transverse gallop footfall sequences exhibit no fundamental energetic difference, despite variations in body yaw and roll motion. However, the number of flight phases significantly impacts energy efficiency: galloping with no flight phases is optimal at lower speeds, whereas galloping with two flight phases minimizes energy consumption at higher speeds. We validate these findings using a quadratic programming (QP)-based controller, developed in our previous work, in Gazebo simulations. These insights advance the understanding of quadrupedal locomotion energetics and may inform future legged robot designs for adaptive, energy-efficient gait transitions.", 'abstract_zh': '高尿苷核苷酸在动物和四足机器人中的高频运动普遍存在但其能量特性尚未充分探究。本研究基于Hildebrand框架系统分析了大量可能的galloping步态，并通过腿部飞行相数和前后腿相位关系对这些步态进行了分类。使用Unitree的A1四足机器人，我们将galloping动力学建模为混合动力系统，并采用轨迹优化（TO）方法，在不同速度范围内最小化运输成本（CoT）。研究结果表明，旋转和横越gallop足落下序列在能量特性上无根本差异，尽管存在身体旋倾运动的差异。然而，飞行相数显著影响能量效率：在较低速度下，无飞行相的galloping最优化，而在较高速度下，具有两个飞行相的galloping能量消耗最小。我们利用我们之前工作开发的基于二次规划（QP）的控制器，在Gazebo仿真中验证了这些发现。这些洞见推进了四足运动能量学的理解，并可能为未来适应性强、能量效率高的步态过渡的腿部机器人设计提供指导。', 'title_zh': '16种飞跃方式：高速四足运动的能量学与身体动力学'}
{'arxiv_id': 'arXiv:2503.13660', 'title': 'INPROVF: Leveraging Large Language Models to Repair High-level Robot Controllers from Assumption Violations', 'authors': 'Qian Meng, Jin Peng Zhou, Kilian Q. Weinberger, Hadas Kress-Gazit', 'link': 'https://arxiv.org/abs/2503.13660', 'abstract': 'This paper presents INPROVF, an automatic framework that combines large language models (LLMs) and formal methods to speed up the repair process of high-level robot controllers. Previous approaches based solely on formal methods are computationally expensive and cannot scale to large state spaces. In contrast, INPROVF uses LLMs to generate repair candidates, and formal methods to verify their correctness. To improve the quality of these candidates, our framework first translates the symbolic representations of the environment and controllers into natural language descriptions. If a candidate fails the verification, INPROVF provides feedback on potential unsafe behaviors or unsatisfied tasks, and iteratively prompts LLMs to generate improved solutions. We demonstrate the effectiveness of INPROVF through 12 violations with various workspaces, tasks, and state space sizes.', 'abstract_zh': '本论文提出了INPROVF，一种结合大型语言模型和形式方法的自动框架，用于加快高级机器人控制器修复过程的速度。与仅基于形式方法的先前方法相比，INPROVF使用大型语言模型生成修复候选方案，并使用形式方法验证其正确性。为了提高这些候选方案的质量，该框架首先将环境和控制器的符号表示翻译成自然语言描述。如果候选方案验证失败，INPROVF将提供潜在的不安全行为或未满足任务的反馈，并迭代地提示大型语言模型生成改进的解决方案。我们通过12个具有各种工作空间、任务和状态空间大小的违规案例展示了INPROVF的有效性。', 'title_zh': 'INPROVF：利用大规模语言模型从假设违反中修复高层机器人控制器'}
{'arxiv_id': 'arXiv:2503.13625', 'title': 'Does the Appearance of Autonomous Conversational Robots Affect User Spoken Behaviors in Real-World Conference Interactions?', 'authors': 'Zi Haur Pang, Yahui Fu, Divesh Lala, Mikey Elmers, Koji Inoue, Tatsuya Kawahara', 'link': 'https://arxiv.org/abs/2503.13625', 'abstract': "We investigate the impact of robot appearance on users' spoken behavior during real-world interactions by comparing a human-like android, ERICA, with a less anthropomorphic humanoid, TELECO. Analyzing data from 42 participants at SIGDIAL 2024, we extracted linguistic features such as disfluencies and syntactic complexity from conversation transcripts. The results showed moderate effect sizes, suggesting that participants produced fewer disfluencies and employed more complex syntax when interacting with ERICA. Further analysis involving training classification models like Naïve Bayes, which achieved an F1-score of 71.60\\%, and conducting feature importance analysis, highlighted the significant role of disfluencies and syntactic complexity in interactions with robots of varying human-like appearances. Discussing these findings within the frameworks of cognitive load and Communication Accommodation Theory, we conclude that designing robots to elicit more structured and fluent user speech can enhance their communicative alignment with humans.", 'abstract_zh': '我们通过对比人类相似的机器人ERICA和人类拟人性较低的机器人TELECO，研究机器人外观对用户在实际交流中口语行为的影响。基于SIGDIAL 2024的42名参与者的数据，我们从对话记录中提取了诸如非流畅性和句子复杂度等语言特征。结果表明有一定的效应大小，表明参与者在与ERICA互动时产生的非流畅性较少，使用的句法结构更为复杂。进一步分析包括使用朴素贝叶斯分类模型（F1分数为71.60%）训练并进行特征重要性分析，突出了不同拟人性机器人互动中非流畅性和句法复杂度的显著作用。在认知负荷理论和沟通调适理论的框架下讨论这些发现，我们得出结论，设计能够引发更结构化和流畅用户言语的机器人可以增强其与人类的沟通一致性。', 'title_zh': '自主对话机器人的出现是否会影响现实会议中用户的口语行为？'}
{'arxiv_id': 'arXiv:2503.13568', 'title': 'WMINet: A Wheel-Mounted Inertial Learning Approach For Mobile-Robot Positioning', 'authors': 'Gal Versano, Itzik Klein', 'link': 'https://arxiv.org/abs/2503.13568', 'abstract': "Autonomous mobile robots are widely used for navigation, transportation, and inspection tasks indoors and outdoors. In practical situations of limited satellite signals or poor lighting conditions, navigation depends only on inertial sensors. In such cases, the navigation solution rapidly drifts due to inertial measurement errors. In this work, we propose WMINet a wheel-mounted inertial deep learning approach to estimate the mobile robot's position based only on its inertial sensors. To that end, we merge two common practical methods to reduce inertial drift: a wheel-mounted approach and driving the mobile robot in periodic trajectories. Additionally, we enforce a wheelbase constraint to further improve positioning performance. To evaluate our proposed approach we recorded using the Rosbot-XL a wheel-mounted initial dataset totaling 190 minutes, which is made publicly available. Our approach demonstrated a 66\\% improvement over state-of-the-art approaches. As a consequence, our approach enables navigation in challenging environments and bridges the pure inertial gap. This enables seamless robot navigation using only inertial sensors for short periods.", 'abstract_zh': '自主移动机器人基于轮载惯性传感器的深度学习定位方法：WMINet及其在挑战性环境中的应用', 'title_zh': 'WMINet: 一种车载惯性学习方法用于移动机器人定位'}
{'arxiv_id': 'arXiv:2503.14492', 'title': 'Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control', 'authors': 'NVIDIA, Hassan Abu Alhaija, Jose Alvarez, Maciej Bala, Tiffany Cai, Tianshi Cao, Liz Cha, Joshua Chen, Mike Chen, Francesco Ferroni, Sanja Fidler, Dieter Fox, Yunhao Ge, Jinwei Gu, Ali Hassani, Michael Isaev, Pooya Jannaty, Shiyi Lan, Tobias Lasser, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Fabio Ramos, Xuanchi Ren, Tianchang Shen, Shitao Tang, Ting-Chun Wang, Jay Wu, Jiashu Xu, Stella Xu, Kevin Xie, Yuchong Ye, Xiaodong Yang, Xiaohui Zeng, Yu Zeng', 'link': 'https://arxiv.org/abs/2503.14492', 'abstract': 'We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at this https URL.', 'abstract_zh': '我们介绍了Cosmos-Transfer，这是一种基于多种模态的空间控制输入（如分割、深度和边缘）生成世界模拟的条件世界生成模型。在设计中，空间条件方案是适应性和可定制的，允许在不同的空间位置以不同的权重使用不同的条件输入。这使得世界生成具有高度可控性，并适用于各种世界到世界的传输应用场景，包括Sim2Real。我们进行了广泛的评估来分析所提出的模型，并展示了其在物理人工智能中的应用，包括机器人Sim2Real和自动驾驶车辆数据增强。我们还展示了实现实时世界生成的推理扩展策略，使用NVIDIA GB200 NVL72机架。为了加速该领域的研究开发，我们在此处公开了我们的模型和代码。', 'title_zh': 'Cosmos-Transfer1：基于自适应多模态控制的条件世界生成'}
{'arxiv_id': 'arXiv:2503.14259', 'title': 'Quantization-Free Autoregressive Action Transformer', 'authors': 'Ziyad Sheebaelhamd, Michael Tschannen, Michael Muehlebach, Claire Vernade', 'link': 'https://arxiv.org/abs/2503.14259', 'abstract': 'Current transformer-based imitation learning approaches introduce discrete action representations and train an autoregressive transformer decoder on the resulting latent code. However, the initial quantization breaks the continuous structure of the action space thereby limiting the capabilities of the generative model. We propose a quantization-free method instead that leverages Generative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous policy parametrization for autoregressive transformers. This simplifies the imitation learning pipeline while achieving state-of-the-art performance on a variety of popular simulated robotics tasks. We enhance our policy roll-outs by carefully studying sampling algorithms, further improving the results.', 'abstract_zh': '基于电流互感器的模拟学习方法引入离散的动作表示，并在结果的潜在代码上训练自回归变换器解码器。然而，初始的量化破坏了动作空间的连续结构，从而限制了生成模型的能力。我们提出了一种无量化的方法，而是利用生成无限词汇变换器（GIVT）作为自回归变换器直接的、连续的动作参数化。这种方法简化了模拟学习管道，并在多种流行的模拟机器人任务上实现了最新性能。我们通过仔细研究采样算法来进一步增强我们的策略roll-outs，进一步提高了结果。', 'title_zh': '无需量化自回归动作变换器'}
{'arxiv_id': 'arXiv:2503.14229', 'title': 'HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard', 'authors': 'Yifei Dong, Fengyi Wu, Qi He, Heng Li, Minghan Li, Zebang Cheng, Yuxuan Zhou, Jingdong Sun, Qi Dai, Zhi-Qi Cheng, Alexander G Hauptmann', 'link': 'https://arxiv.org/abs/2503.14229', 'abstract': 'Vision-and-Language Navigation (VLN) systems often focus on either discrete (panoramic) or continuous (free-motion) paradigms alone, overlooking the complexities of human-populated, dynamic environments. We introduce a unified Human-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit social-awareness constraints. Our contributions include: 1. A standardized task definition that balances discrete-continuous navigation with personal-space requirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded simulators capturing realistic multi-human interactions, outdoor contexts, and refined motion-language alignment; 3. Extensive benchmarking on 16,844 human-centric instructions, revealing how multi-human dynamics and partial observability pose substantial challenges for leading VLN agents; 4. Real-world robot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A public leaderboard supporting transparent comparisons across discrete and continuous tasks. Empirical results show improved navigation success and fewer collisions when social context is integrated, underscoring the need for human-centric design. By releasing all datasets, simulators, agent code, and evaluation tools, we aim to advance safer, more capable, and socially responsible VLN research.', 'abstract_zh': '具有明确社会意识约束的统一人类意识视觉-语言导航（HA-VLN）基准', 'title_zh': 'HA-VLN：离散-连续环境中动态多人交互的人工智能导航基准，现实世界验证及公开排行榜'}
{'arxiv_id': 'arXiv:2503.13966', 'title': 'FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation Tasks', 'authors': 'Siqi Zhang, Yanyuan Qiao, Qunbo Wang, Longteng Guo, Zhihua Wei, Jing Liu', 'link': 'https://arxiv.org/abs/2503.13966', 'abstract': 'The aspiration of the Vision-and-Language Navigation (VLN) task has long been to develop an embodied agent with robust adaptability, capable of seamlessly transferring its navigation capabilities across various tasks. Despite remarkable advancements in recent years, most methods necessitate dataset-specific training, thereby lacking the capability to generalize across diverse datasets encompassing distinct types of instructions. Large language models (LLMs) have demonstrated exceptional reasoning and generalization abilities, exhibiting immense potential in robot action planning. In this paper, we propose FlexVLN, an innovative hierarchical approach to VLN that integrates the fundamental navigation ability of a supervised-learning-based Instruction Follower with the robust generalization ability of the LLM Planner, enabling effective generalization across diverse VLN datasets. Moreover, a verification mechanism and a multi-model integration mechanism are proposed to mitigate potential hallucinations by the LLM Planner and enhance execution accuracy of the Instruction Follower. We take REVERIE, SOON, and CVDN-target as out-of-domain datasets for assessing generalization ability. The generalization performance of FlexVLN surpasses that of all the previous methods to a large extent.', 'abstract_zh': '视觉-语言导航任务的愿景一直是开发出具有强大适应性的实体化代理，使其能够无缝地在各种任务中转移其导航能力。尽管近年来取得了显著进展，大多数方法仍需要针对特定数据集进行训练，从而缺乏跨不同数据集泛化的能力，这些数据集包含不同类型的任务指令。大型语言模型（LLMs）展示了卓越的推理和泛化能力，显示出在机器人动作规划方面的巨大潜力。本文提出了一种名为FlexVLN的创新性分层方法，将基于监督学习的指令跟随器的基本导航能力与LLM规划器的强大泛化能力相结合，从而实现跨不同视觉-语言导航数据集的有效泛化。此外，本文还提出了一种验证机制和多模型集成机制，以减轻LLM规划器可能出现的幻觉并提高指令跟随器的执行准确性。为了评估泛化能力，我们使用了REVERIE、SOON和CVDN-target作为域外数据集。FlexVLN的泛化性能显著优于所有先前的方法。', 'title_zh': 'FlexVLN: 灵活性适配多样的视觉-语言导航任务'}
{'arxiv_id': 'arXiv:2503.13817', 'title': 'VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent Regularized Preferences', 'authors': 'Anukriti Singh, Amisha Bhaskar, Peihong Yu, Souradip Chakraborty, Ruthwik Dasyam, Amrit Bedi, Pratap Tokekar', 'link': 'https://arxiv.org/abs/2503.13817', 'abstract': "Designing reward functions for continuous-control robotics often leads to subtle misalignments or reward hacking, especially in complex tasks. Preference-based RL mitigates some of these pitfalls by learning rewards from comparative feedback rather than hand-crafted signals, yet scaling human annotations remains challenging. Recent work uses Vision-Language Models (VLMs) to automate preference labeling, but a single final-state image generally fails to capture the agent's full motion. In this paper, we present a two-part solution that both improves feedback accuracy and better aligns reward learning with the agent's policy. First, we overlay trajectory sketches on final observations to reveal the path taken, allowing VLMs to provide more reliable preferences-improving preference accuracy by approximately 15-20% in metaworld tasks. Second, we regularize reward learning by incorporating the agent's performance, ensuring that the reward model is optimized based on data generated by the current policy; this addition boosts episode returns by 20-30% in locomotion tasks. Empirical studies on metaworld demonstrate that our method achieves, for instance, around 70-80% success rate in all tasks, compared to below 50% for standard approaches. These results underscore the efficacy of combining richer visual representations with agent-aware reward regularization.", 'abstract_zh': '基于偏好的强化学习在连续控制机器人中的应用：结合丰富视觉表示与智能奖励正则化的方法', 'title_zh': 'VARP：带有代理正则化偏好的视觉-语言模型反馈强化学习'}
{'arxiv_id': 'arXiv:2503.14427', 'title': 'VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms', 'authors': 'Seungwon Lim, Sungwoong Kim, Jihwan Yu, Sungjae Lee, Jiwan Chung, Youngjae Yu', 'link': 'https://arxiv.org/abs/2503.14427', 'abstract': 'Escape rooms present a unique cognitive challenge that demands exploration-driven planning: players should actively search their environment, continuously update their knowledge based on new discoveries, and connect disparate clues to determine which elements are relevant to their objectives. Motivated by this, we introduce VisEscape, a benchmark of 20 virtual escape rooms specifically designed to evaluate AI models under these challenging conditions, where success depends not only on solving isolated puzzles but also on iteratively constructing and refining spatial-temporal knowledge of a dynamically changing environment. On VisEscape, we observed that even state-of-the-art multimodal models generally fail to escape the rooms, showing considerable variation in their levels of progress and trajectories. To address this issue, we propose VisEscaper, which effectively integrates Memory, Feedback, and ReAct modules, demonstrating significant improvements by performing 3.7 times more effectively and 5.0 times more efficiently on average.', 'abstract_zh': '密室逃脱提供了一种独特的认知挑战，要求玩家进行探索驱动的规划：玩家应该积极搜索环境，根据新发现不断更新知识，并将不相关的线索联系起来以确定哪些要素与目标相关。受此启发，我们提出了VisEscape，这是一个包含20个虚拟密室的基准测试，专门用于评估AI模型在这些挑战条件下的性能，成功不仅依赖于解决孤立的谜题，还依赖于不断构建和精炼动态变化环境的空间时间知识。在VisEscape上，我们观察到即使是最先进的多模态模型也通常无法成功逃脱密室，显示出他们在进展水平和轨迹上的显著差异。为了解决这一问题，我们提出了VisEscaper，该模型有效集成Memory、Feedback和ReAct模块，并在平均效果上提高了3.7倍，在效率上提高了5.0倍。', 'title_zh': 'VisEscape：评估虚拟逃脱房间中探索驱动决策的标准benchmark'}
{'arxiv_id': 'arXiv:2503.14250', 'title': 'A Parallel Hybrid Action Space Reinforcement Learning Model for Real-world Adaptive Traffic Signal Control', 'authors': 'Yuxuan Wang, Meng Long, Qiang Wu, Wei Liu, Jiatian Pi, Xinmin Yang', 'link': 'https://arxiv.org/abs/2503.14250', 'abstract': 'Adaptive traffic signal control (ATSC) can effectively reduce vehicle travel times by dynamically adjusting signal timings but poses a critical challenge in real-world scenarios due to the complexity of real-time decision-making in dynamic and uncertain traffic conditions. The burgeoning field of intelligent transportation systems, bolstered by artificial intelligence techniques and extensive data availability, offers new prospects for the implementation of ATSC. In this study, we introduce a parallel hybrid action space reinforcement learning model (PH-DDPG) that optimizes traffic signal phase and duration of traffic signals simultaneously, eliminating the need for sequential decision-making seen in traditional two-stage models. Our model features a task-specific parallel hybrid action space tailored for adaptive traffic control, which directly outputs discrete phase selections and their associated continuous duration parameters concurrently, thereby inherently addressing dynamic traffic adaptation through unified parametric optimization. %Our model features a unique parallel hybrid action space that allows for the simultaneous output of each action and its optimal parameters, streamlining the decision-making process. Furthermore, to ascertain the robustness and effectiveness of this approach, we executed ablation studies focusing on the utilization of a random action parameter mask within the critic network, which decouples the parameter space for individual actions, facilitating the use of preferable parameters for each action. The results from these studies confirm the efficacy of this method, distinctly enhancing real-world applicability', 'abstract_zh': '自适应交通信号控制（ATSC）可以通过动态调整信号时序有效减少车辆行驶时间，但在动态和不确定的交通条件下进行实时决策时面临重大挑战。智能交通系统领域的蓬勃发展，得益于人工智能技术和大数据的广泛应用，为ATSC的实施提供了新机遇。在本研究中，我们提出了一种并行混合动作空间强化学习模型（PH-DDPG），该模型同时优化交通信号相位和持续时间，消除了传统两阶段模型中序列决策的需求。我们的模型具有专门针对自适应交通控制的任务特定并行混合动作空间，可以直接同时输出离散的相位选择和其相关的连续持续时间参数，从而通过统一参数优化实现动态交通适应。初步研究表明，通过在评价网络中使用随机动作参数掩码来解耦动作参数空间，可以为每个动作使用更优的参数，确保方法的有效性和鲁棒性。', 'title_zh': '一种适用于实际道路交通信号控制的并行混合动作空间 reinforcement learning 模型'}
{'arxiv_id': 'arXiv:2503.14162', 'title': 'EIAD: Explainable Industrial Anomaly Detection Via Multi-Modal Large Language Models', 'authors': 'Zongyun Zhang, Jiacheng Ruan, Xian Gao, Ting Liu, Yuzhuo Fu', 'link': 'https://arxiv.org/abs/2503.14162', 'abstract': 'Industrial Anomaly Detection (IAD) is critical to ensure product quality during manufacturing. Although existing zero-shot defect segmentation and detection methods have shown effectiveness, they cannot provide detailed descriptions of the defects. Furthermore, the application of large multi-modal models in IAD remains in its infancy, facing challenges in balancing question-answering (QA) performance and mask-based grounding capabilities, often owing to overfitting during the fine-tuning process. To address these challenges, we propose a novel approach that introduces a dedicated multi-modal defect localization module to decouple the dialog functionality from the core feature extraction. This decoupling is achieved through independent optimization objectives and tailored learning strategies. Additionally, we contribute to the first multi-modal industrial anomaly detection training dataset, named Defect Detection Question Answering (DDQA), encompassing a wide range of defect types and industrial scenarios. Unlike conventional datasets that rely on GPT-generated data, DDQA ensures authenticity and reliability and offers a robust foundation for model training. Experimental results demonstrate that our proposed method, Explainable Industrial Anomaly Detection Assistant (EIAD), achieves outstanding performance in defect detection and localization tasks. It not only significantly enhances accuracy but also improves interpretability. These advancements highlight the potential of EIAD for practical applications in industrial settings.', 'abstract_zh': '工业异常检测中的可解释工业异常检测assistant (EIAD)', 'title_zh': 'EIAD: 通过多模态大语言模型实现可解释的工业异常检测'}
{'arxiv_id': 'arXiv:2503.13843', 'title': 'WebNav: An Intelligent Agent for Voice-Controlled Web Navigation', 'authors': 'Trisanth Srinivasan, Santosh Patapati', 'link': 'https://arxiv.org/abs/2503.13843', 'abstract': "The increasing reliance on web interfaces presents many challenges for visually impaired users, showcasing the need for more advanced assistive technologies. This paper introduces WebNav, a voice-controlled web navigation agent that leverages a ReAct-inspired architecture and generative AI to provide this framework. WebNav comprises of a hierarchical structure: a Digital Navigation Module (DIGNAV) for high-level strategic planning, an Assistant Module for translating abstract commands into executable actions, and an Inference Module for low-level interaction. A key component is a dynamic labeling engine, implemented as a browser extension, that generates real-time labels for interactive elements, creating mapping between voice commands and Document Object Model (DOM) components. Preliminary evaluations show that WebNav outperforms traditional screen readers in response time and task completion accuracy for the visually impaired. Future work will focus on extensive user evaluations, benchmark development, and refining the agent's adaptive capabilities for real-world deployment.", 'abstract_zh': '基于反应模型的生成人工智能的语音控制网页导航代理：WebNav的研究与应用', 'title_zh': 'WebNav：一种语音控制网页导航的智能代理'}
{'arxiv_id': 'arXiv:2503.14333', 'title': 'Revealing higher-order neural representations with generative artificial intelligence', 'authors': 'Hojjat Azimi Asrari, Megan A. K. Peters', 'link': 'https://arxiv.org/abs/2503.14333', 'abstract': 'Studies often aim to reveal how neural representations encode aspects of an observer\'s environment, such as its contents or structure. These are ``first-order" representations (FORs), because they\'re ``about" the external world. A less-common target is ``higher-order" representations (HORs), which are ``about" FORs -- their contents, stability, or uncertainty. HORs of uncertainty appear critically involved in adaptive behaviors including learning under uncertainty, influencing learning rates and internal model updating based on environmental feedback. However, HORs about uncertainty are unlikely to be direct ``read-outs" of FOR characteristics, instead reflecting estimation processes which may be lossy, bias-prone, or distortive and which may also incorporate estimates of distributions of uncertainty the observer is likely to experience. While some research has targeted neural representations of ``instantaneously" estimated uncertainty, how the brain represents \\textit{distributions} of expected uncertainty remains largely unexplored. Here, we propose a novel reinforcement learning (RL) based generative artificial intelligence (genAI) approach to explore neural representations of uncertainty distributions. We use existing functional magnetic resonance imaging data, where humans learned to `de-noise\' their brain states to achieve target neural patterns, to train denoising diffusion genAI models with RL algorithms to learn noise distributions similar to how humans might learn to do the same. We then explore these models\' learned noise-distribution HORs compared to control models trained with traditional backpropagation. Results reveal model-dependent differences in noise distribution representations -- with the RL-based model offering much higher explanatory power for human behavior -- offering an exciting path towards using genAI to explore neural noise-distribution HORs.', 'abstract_zh': '基于强化学习的生成人工智能探索神经不确定性分布的高级表示', 'title_zh': '揭示更高阶神经表示的生成人工智能'}
{'arxiv_id': 'arXiv:2503.13882', 'title': 'MoK-RAG: Mixture of Knowledge Paths Enhanced Retrieval-Augmented Generation for Embodied AI Environments', 'authors': 'Zhengsheng Guo, Linwei Zheng, Xinyang Chen, Xuefeng Bai, Kehai Chen, Min Zhang', 'link': 'https://arxiv.org/abs/2503.13882', 'abstract': 'While human cognition inherently retrieves information from diverse and specialized knowledge sources during decision-making processes, current Retrieval-Augmented Generation (RAG) systems typically operate through single-source knowledge retrieval, leading to a cognitive-algorithmic discrepancy. To bridge this gap, we introduce MoK-RAG, a novel multi-source RAG framework that implements a mixture of knowledge paths enhanced retrieval mechanism through functional partitioning of a large language model (LLM) corpus into distinct sections, enabling retrieval from multiple specialized knowledge paths. Applied to the generation of 3D simulated environments, our proposed MoK-RAG3D enhances this paradigm by partitioning 3D assets into distinct sections and organizing them based on a hierarchical knowledge tree structure. Different from previous methods that only use manual evaluation, we pioneered the introduction of automated evaluation methods for 3D scenes. Both automatic and human evaluations in our experiments demonstrate that MoK-RAG3D can assist Embodied AI agents in generating diverse scenes.', 'abstract_zh': '多源增强生成：面向3D模拟环境的MoK-RAG3D框架', 'title_zh': 'MoK-RAG: 结合知识路径混合的检索增强生成技术用于具身AI环境'}
{'arxiv_id': 'arXiv:2503.13861', 'title': 'RAD: Retrieval-Augmented Decision-Making of Meta-Actions with Vision-Language Models in Autonomous Driving', 'authors': 'Yujin Wang, Quanfeng Liu, Zhengxin Jiang, Tianyi Wang, Junfeng Jiao, Hongqing Chu, Bingzhao Gao, Hong Chen', 'link': 'https://arxiv.org/abs/2503.13861', 'abstract': "Accurately understanding and deciding high-level meta-actions is essential for ensuring reliable and safe autonomous driving systems. While vision-language models (VLMs) have shown significant potential in various autonomous driving tasks, they often suffer from limitations such as inadequate spatial perception and hallucination, reducing their effectiveness in complex autonomous driving scenarios. To address these challenges, we propose a retrieval-augmented decision-making (RAD) framework, a novel architecture designed to enhance VLMs' capabilities to reliably generate meta-actions in autonomous driving scenes. RAD leverages a retrieval-augmented generation (RAG) pipeline to dynamically improve decision accuracy through a three-stage process consisting of the embedding flow, retrieving flow, and generating flow. Additionally, we fine-tune VLMs on a specifically curated dataset derived from the NuScenes dataset to enhance their spatial perception and bird's-eye view image comprehension capabilities. Extensive experimental evaluations on the curated NuScenes-based dataset demonstrate that RAD outperforms baseline methods across key evaluation metrics, including match accuracy, and F1 score, and self-defined overall score, highlighting its effectiveness in improving meta-action decision-making for autonomous driving tasks.", 'abstract_zh': '准确理解与决策高层次元动作对于确保可靠和安全的自动驾驶系统至关重要。尽管视觉语言模型（VLMs）在各种自动驾驶任务中展现了显著的潜力，但它们往往受到空间感知不足和幻觉等问题的限制，这降低了其在复杂自动驾驶场景中的有效性。为了解决这些挑战，我们提出了一种检索增强决策（RAD）框架，这是一种新型架构，旨在增强VLMs在自动驾驶场景中可靠生成元动作的能力。RAD利用一种检索增强生成（RAG）流水线，通过一个包含嵌入流、检索流和生成流的三阶段过程动态提高决策准确度。此外，我们还在一个专门为自动驾驶任务精心策划的数据集上对VLMs进行了微调，以增强其空间感知和鸟瞰图图像理解能力。在精心策划的基于NuScenes的数据集上进行的广泛实验评估表明，RAD在关键评估指标，包括匹配准确度、F1分数和自定义总体评分上优于基线方法，强调了其在提高自动驾驶任务中元动作决策能力方面的有效性。', 'title_zh': 'RAD：基于视觉语言模型的自主驾驶中元动作检索增强决策'}
{'arxiv_id': 'arXiv:2503.13754', 'title': 'From Autonomous Agents to Integrated Systems, A New Paradigm: Orchestrated Distributed Intelligence', 'authors': 'Krti Tallam', 'link': 'https://arxiv.org/abs/2503.13754', 'abstract': 'The rapid evolution of artificial intelligence (AI) has ushered in a new era of integrated systems that merge computational prowess with human decision-making. In this paper, we introduce the concept of \\textbf{Orchestrated Distributed Intelligence (ODI)}, a novel paradigm that reconceptualizes AI not as isolated autonomous agents, but as cohesive, orchestrated networks that work in tandem with human expertise. ODI leverages advanced orchestration layers, multi-loop feedback mechanisms, and a high cognitive density framework to transform static, record-keeping systems into dynamic, action-oriented environments. Through a comprehensive review of multi-agent system literature, recent technological advances, and practical insights from industry forums, we argue that the future of AI lies in integrating distributed intelligence within human-centric workflows. This approach not only enhances operational efficiency and strategic agility but also addresses challenges related to scalability, transparency, and ethical decision-making. Our work outlines key theoretical implications and presents a practical roadmap for future research and enterprise innovation, aiming to pave the way for responsible and adaptive AI systems that drive sustainable innovation in human organizations.', 'abstract_zh': 'orchestrised 分布式智能 (ODI): 人工智能与人类决策的协同进化', 'title_zh': '从自主代理到集成系统：统筹分布intelligence的新范式'}
