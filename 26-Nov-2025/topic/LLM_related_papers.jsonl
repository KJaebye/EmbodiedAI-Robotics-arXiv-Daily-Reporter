{'arxiv_id': 'arXiv:2511.20627', 'title': 'Fighting AI with AI: Leveraging Foundation Models for Assuring AI-Enabled Safety-Critical Systems', 'authors': 'Anastasia Mavridou, Divya Gopinath, Corina S. Păsăreanu', 'link': 'https://arxiv.org/abs/2511.20627', 'abstract': 'The integration of AI components, particularly Deep Neural Networks (DNNs), into safety-critical systems such as aerospace and autonomous vehicles presents fundamental challenges for assurance. The opacity of AI systems, combined with the semantic gap between high-level requirements and low-level network representations, creates barriers to traditional verification approaches. These AI-specific challenges are amplified by longstanding issues in Requirements Engineering, including ambiguity in natural language specifications and scalability bottlenecks in formalization. We propose an approach that leverages AI itself to address these challenges through two complementary components. REACT (Requirements Engineering with AI for Consistency and Testing) employs Large Language Models (LLMs) to bridge the gap between informal natural language requirements and formal specifications, enabling early verification and validation. SemaLens (Semantic Analysis of Visual Perception using large Multi-modal models) utilizes Vision Language Models (VLMs) to reason about, test, and monitor DNN-based perception systems using human-understandable concepts. Together, these components provide a comprehensive pipeline from informal requirements to validated implementations.', 'abstract_zh': 'AI组件，特别是深度神经网络（DNNs），集成到航空和自主车辆等安全关键系统中，带来了确保性方面的基本挑战。AI系统的不透明性与高层要求和低层网络表示之间的语义差距，阻碍了传统验证方法的应用。这些AI特有的挑战加剧了长期存在于需求工程中的问题，包括自然语言规范的模糊性和形式化中的可扩展性瓶颈。我们提出了一种利用AI本身来应对这些挑战的方法，通过两个互补的组件来实现。REACT（基于AI的需求工程以确保一致性和测试）利用大型语言模型（LLMs）解决非正式自然语言需求与正式规范之间的差距，实现早期验证和验证。SemaLens（基于大型多模态模型的视觉感知语义分析）利用视觉语言模型（VLMs）使用人类可理解的概念来推理、测试和监测基于DNN的感知系统。这两个组件共同提供了一个从非正式需求到验证实现的综合流程。', 'title_zh': '用AI对抗AI：利用基础模型保证AI驱动的安全关键系统'}
{'arxiv_id': 'arXiv:2511.20623', 'title': 'Copyright Detection in Large Language Models: An Ethical Approach to Generative AI Development', 'authors': 'David Szczecina, Senan Gaffori, Edmond Li', 'link': 'https://arxiv.org/abs/2511.20623', 'abstract': 'The widespread use of Large Language Models (LLMs) raises critical concerns regarding the unauthorized inclusion of copyrighted content in training data. Existing detection frameworks, such as DE-COP, are computationally intensive, and largely inaccessible to independent creators. As legal scrutiny increases, there is a pressing need for a scalable, transparent, and user-friendly solution. This paper introduce an open-source copyright detection platform that enables content creators to verify whether their work was used in LLM training datasets. Our approach enhances existing methodologies by facilitating ease of use, improving similarity detection, optimizing dataset validation, and reducing computational overhead by 10-30% with efficient API calls. With an intuitive user interface and scalable backend, this framework contributes to increasing transparency in AI development and ethical compliance, facilitating the foundation for further research in responsible AI development and copyright enforcement.', 'abstract_zh': '大规模语言模型（LLMs）的广泛应用引发了对训练数据中未经授权包含版权内容的严重影响关切。现有的检测框架，如DE-COP，计算密集且对独立创作者来说较为难以获取。随着法律审查的加强，迫切需要一种可扩展、透明且用户友好的解决方案。本文介绍了一个开源版权检测平台，该平台使内容创作者能够验证其作品是否被用于LLM训练数据集。我们的方法通过简化使用流程、提高相似性检测、优化数据集验证以及通过高效API调用降低30%左右的计算 overhead，增强了现有方法。凭借直观的用户界面和可扩展的后台，该框架有助于增加AI开发过程中的透明度和道德合规性，为负责任的AI开发和版权执法奠定了基础。', 'title_zh': '大型语言模型中的版权检测：生成性AI开发的伦理途径'}
{'arxiv_id': 'arXiv:2511.20610', 'title': 'Building a Foundation Model for Trajectory from Scratch', 'authors': 'Gaspard Merten, Mahmoud Sakr, Gilles Dejaegere', 'link': 'https://arxiv.org/abs/2511.20610', 'abstract': "Foundation models are transformative in artificial intelligence, but building them from scratch, especially for mobility trajectories, is not yet clear or documented. This tutorial bridges this gap by demonstrating the steps and code of a minimal implementation of a trajectory-focused foundation model starting from GPT-2. Through a concise, step-by-step, code-driven process, we demonstrate adapting GPT-2 for spatiotemporal data. We then review and compare representative trajectory foundation models, such as TrajFM and TrajGPT, highlighting their architectural innovations and differences. Additionally, we introduce complementary techniques from related domains, like TimesFM's patching approach. Targeted at researchers and practitioners, this tutorial aims to explain the concepts and terminology of foundation models, at the implementation level. We find it timely and indispensable to create this educational material in order to support the SIGSPATIAL community in building and evaluating mobility foundation models, enhancing both research clarity and peer-review effectiveness in mobility AI.", 'abstract_zh': '基础模型在人工智能中具有变革性，但它们尤其是在移动轨迹方面的构建方法尚不清晰或未被充分记录。本教程通过展示从GPT-2出发构建一个面向移动轨迹的基础模型的步骤和代码，填补了这一空白。通过简洁的、逐步的代码驱动过程，我们演示了如何适应GPT-2以处理空间-时间数据。随后，我们回顾并比较了代表性移动轨迹基础模型，如TrajFM和TrajGPT，强调它们的架构创新和差异。此外，我们还介绍了来自相关领域（如TimesFM的修补方法）的互补技术。本教程旨在面向研究人员和实践者，解释基础模型的概念和术语，并在实施层面提供指导。我们发现创建此类教育资源非常及时且必不可少，旨在支持SIGSPATIAL社区在移动AI中的基础模型构建和评估，从而提高研究清晰度和同行评审效果。', 'title_zh': '从零构建轨迹基础模型'}
{'arxiv_id': 'arXiv:2511.20526', 'title': "Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam", 'authors': 'Xinran Wang, Boran Zhu, Shujuan Zhou, Ziwen Long, Dehua Zhou, Shu Zhang', 'link': 'https://arxiv.org/abs/2511.20526', 'abstract': "Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain this http URL China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.", 'abstract_zh': '背景：随着大型语言模型（LLMs）在数字健康教育和评估流程中的逐步集成，它们在支持高风险、专业领域认证任务方面的能力仍存争议。在中国，全国药师执照考试是衡量药师临床和理论能力的标准化基准。目标：本研究旨在比较两种LLM（ChatGPT-4o和DeepSeek-R1）在中国药师执照考试（2017-2021年）真实问题上的表现，并讨论这些性能差异对基于AI的形成性评估的影响。方法：共收集了2306道官方试题、培训材料和公共数据库中的单选题（仅文本格式），排除包含表格或图片的问题。每道题以原始中文格式输入，评估模型回复的准确度。使用皮尔逊卡方检验比较总体表现，使用费舍尔精确检验分析逐年单选题准确度。结果：DeepSeek-R1在总体准确度上显著优于ChatGPT-4o（90.0% vs. 76.1%，p < 0.001）。单元级别分析显示，DeepSeek-R1在基础和临床综合模块中持续占优势。虽然逐年单选题表现亦有利于DeepSeek-R1，但在任何特定单元-年份中，性能差距均未达到统计显著性（所有p > 0.05）。结论：DeepSeek-R1在结构和语义需求方面与药师执照考试表现出一致的契合度。这些发现表明，在专业领域模型方面需要进一步研究，同时也强调了在法律和伦理敏感的背景下需要人类监督的必要性。', 'title_zh': '评估大型语言模型的表现：来自中国药师考试的见解'}
{'arxiv_id': 'arXiv:2511.20471', 'title': 'Universe of Thoughts: Enabling Creative Reasoning with Large Language Models', 'authors': 'Yuto Suzuki, Farnoush Banaei-Kashani', 'link': 'https://arxiv.org/abs/2511.20471', 'abstract': "Reasoning based on Large Language Models (LLMs) has garnered increasing attention due to outstanding performance of these models in mathematical and complex logical tasks. Beginning with the Chain-of-Thought (CoT) prompting technique, numerous reasoning methods have emerged that decompose problems into smaller, sequential steps (or thoughts). However, existing reasoning models focus on conventional problem-solving and do not necessarily generate creative solutions by ``creative reasoning''. In domains where the solution space is expansive and conventional solutions are suboptimal, such as drug discovery or business strategization, creative reasoning to discover innovative solutions is crucial. To address this gap, first we introduce a computational framework for creative reasoning inspired by established cognitive science principles. With this framework, we propose three core creative reasoning paradigms, namely, \\textit{combinational}, \\textit{exploratory}, and \\textit{transformative} reasoning, where each offers specific directions for systematic exploration of the universe of thoughts to generate creative solutions. Next, to materialize this framework using LLMs, we introduce the \\textit{Universe of Thoughts} (or \\textit{UoT}, for short), a novel set of methods to implement the aforementioned three creative processes. Finally, we introduce three novel tasks that necessitate creative problem-solving, along with an evaluation benchmark to assess creativity from three orthogonal perspectives: feasibility as constraint, and utility and novelty as metrics. With a comparative analysis against the state-of-the-art (SOTA) reasoning techniques as well as representative commercial models with reasoning capability, we show that UoT demonstrates superior performance in creative reasoning.", 'abstract_zh': '基于大型语言模型的推理引起了越来越多的关注，这些模型在数学和复杂逻辑任务中的出色表现推动了这一领域的发展。从链式思维（Chain-of-Thought，CoT）提示技术开始，出现了多种分解问题为更小、顺序步骤（或思维）的方法。然而，现有的推理模型主要关注传统的问题解决方式，并不一定能通过“创造性推理”生成创意解决方案。在药物发现或企业管理等领域，解决方案空间庞大且传统解决方案可能并不理想时，需要通过创造性推理来发现创新解决方案。为解决这一缺口，我们首先引入了一个基于认知科学原理的计算框架，以此为基础，提出了三种核心的创造性推理范式，即组合性、探索性和转化性推理，分别提供了系统探索思维宇宙以生成创意解决方案的具体方向。接着，为了使用大型语言模型实现这一框架，我们引入了思维宇宙（UoT）这一新颖的一整套方法，以实施前述的三种创造性过程。最后，我们提出了三种需要创造性问题解决的新颖任务，并建立了评估创造性从三个不同维度的基准：可行性作为约束，以及效用和新颖性作为度量标准。通过与当前最先进的（SOTA）推理技术以及具有推理能力的代表性商业模型进行比较分析，我们证明了UoT在创造性推理方面表现更优。', 'title_zh': '思维宇宙：大规模语言模型促进创造性推理'}
{'arxiv_id': 'arXiv:2511.20468', 'title': 'DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs', 'authors': 'Yuanhao Li, Mingshan Liu, Hongbo Wang, Yiding Zhang, Yifei Ma, Wei Tan', 'link': 'https://arxiv.org/abs/2511.20468', 'abstract': "Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and this http URL works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic this http URL-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed", 'abstract_zh': '大规模语言模型（LLMs）在多步骤推理方面展示了 impressive 的能力，但这些方法通常依赖单次响应而缺乏推理探索的结构性多样性。本文提出了 DRAFT-RL，一个将 Draft 链推理（CoD）集成到多代理强化学习（RL）训练中的新框架。每个代理每次查询会产生多个草案，这些草案会由其他代理和学习到的奖励模型评估，以识别最有前途的发展轨迹。这些精选的草案用于通过演员-评论家方法进一步精炼推理策略。DRAFT-RL 通过明确的多路径探索、同伴引导的反思以及与奖励对齐的选择，提高了 LLM 代理行为的稳定性和可解释性。我们在代码合成、符号数学和知识密集型 QA 等复杂推理任务上评估了该方法，结果显示 DRAFT-RL 在准确性和收敛速度方面显著优于现有的反思性和基于 RL 的代理。', 'title_zh': '草案-RL：增强强化学习的多智能体链式推理方法'}
{'arxiv_id': 'arXiv:2511.20333', 'title': 'NNGPT: Rethinking AutoML with Large Language Models', 'authors': 'Roman Kochnev, Waleed Khalid, Tolgay Atinc Uzun, Xi Zhang, Yashkumar Sanjaybhai Dhameliya, Furui Qin, Chandini Vysyaraju, Raghuvir Duvvuri, Avi Goyal, Dmitry Ignatov, Radu Timofte', 'link': 'https://arxiv.org/abs/2511.20333', 'abstract': 'Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.', 'abstract_zh': '构建自我改进的AI系统仍然是AI领域的基本挑战。我们提出NNGPT，一个开源框架，将大规模语言模型（LLM）转化为用于神经网络开发的自改进AutoML引擎，主要针对计算机视觉领域。不同于之前的框架，NNGPT通过生成新模型来扩展神经网络的数据集，基于生成、评估和自我改进的闭环系统对LLM进行持续微调。NNGPT集成了一体化的五种协同工作的LLM管道：零shot架构合成、超参数优化（HPO）、代码感知的准确度/早期停止预测、检索增强的封闭范围PyTorch块合成（NN-RAG）和强化学习。基于经过审计的LEMMUR数据集作为可重复的指标库，NNGPT从单一提示出发，验证网络架构、预处理代码和超参数，端到端执行并从中学习。PyTorch适配器使NNGPT框架无关，增强了其性能：NN-RAG在1,289个目标上实现了73%的可执行性，三-shot提示增强了普通数据集上的准确性，基于哈希的去重节省了数百次运行。单一提示的预测与基于搜索的AutoML相当，减少了大量试验的需求。在LEMMUR上的HPO实现了RMSE 0.60，优于Optuna的0.64，而代码感知预测达到了RMSE 0.14，Pearson r值为0.78。该系统已生成超过5,000个验证模型，证明NNGPT为一个自主的AutoML引擎。在被接受后，代码、提示和检查点将对公众开放，以促进可重复性和社区使用。', 'title_zh': 'NNGPT：重新思考基于大规模语言模型的自动化机器学习'}
{'arxiv_id': 'arXiv:2511.20297', 'title': 'Improving Language Agents through BREW', 'authors': 'Shashank Kirtania, Param Biyani, Priyanshu Gupta, Yasharth Bajpai, Roshni Iyer, Sumit Gulwani, Gustavo Soares', 'link': 'https://arxiv.org/abs/2511.20297', 'abstract': 'Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $\\tau^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\\%$ improvement in task precision, $10-15\\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.', 'abstract_zh': '基于大型语言模型（LLM）的代理在数据操作、多步规划和计算机使用自动化等需要结构化推理、工具使用和环境适应的任务中 increasingly 应用。然而，尽管它们功能强大，当前用于模型权重优化方法（如 PPO 和 GRPO）的训练范式仍然相对不实用，因为它们具有较高的计算开销以实现展开收敛。此外，由此产生的代理策略难以解释、适应或逐步改进。为了解决这些问题，我们研究了通过知识库（KB）的构建与改进，从代理与其环境的经验学习中创建和优化结构化记忆作为代理优化的替代途径。我们介绍了 BREW（基于经验学习环境知识的自助）框架，该框架通过知识库构造和改进为下游任务实现代理优化。在我们的形式化中，我们引入了一种有效的方法来分区代理内存，以实现更高效的检索和改进。BREW 利用任务评分器和行为评标表学习见解，同时利用状态空间搜索以确保对自然语言中的噪声和非特异性进行稳健处理。在基于真实世界的领域特定基准（OSWorld、$\\tau^2$Bench 和 SpreadsheetBench）上进行的实验证明，BREW 能实现任务精度的 $10-20\\%$ 提高、API/工具调用的 $10-15\\%$ 减少，从而执行时间加快，同时保持与基模型相当的计算效率。不同于之前的工作将记忆视为静态上下文，我们确立了知识库作为模块化和可控的代理优化基础——一个明确的杠杆，以透明、可解释和可扩展的方式塑造行为。', 'title_zh': '通过BREW提升语言代理'}
{'arxiv_id': 'arXiv:2511.20200', 'title': 'Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025', 'authors': 'Yitian Huang, Yuxuan Lei, Jianxun Lian, Hao Liao', 'link': 'https://arxiv.org/abs/2511.20200', 'abstract': 'This report presents the solution and results of our team MSRA\\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at this https URL', 'abstract_zh': '本报告阐述了MSRA_SC团队在常识导向人物 ground 的对话挑战（CPDC 2025）中的解决方案和结果。我们提出了一种简单而有效的框架，统一了 GPU 轨道和 API 轨道的改进。我们的方法集中在两个关键组件上。首先，上下文工程通过动态工具剪枝和人物裁剪进行输入压缩，并结合参数归一化和函数合并等后处理技术。结合手动优化的提示，该设计提高了工具调用的稳定性、执行的可靠性以及角色扮演的指导。其次，在 GPU 轨道中，我们进一步采用 GRPO 训练，将监督微调替换为直接由奖励信号优化的强化学习。这减轻了小样本过拟合问题，并显著提高了任务导向对话的表现。在最终评估中，我们的团队在任务 2 API 中排名第一，在任务 1 API 中排名第二，在任务 3 API 和 GPU 轨道中分别排名第三，证明了我们方法的有效性。我们的代码可在以下网址公开访问。', 'title_zh': '基于LLMs的交互式AI角色：CPDC挑战赛2025技术报告'}
{'arxiv_id': 'arXiv:2511.20196', 'title': 'Towards Benign Memory Forgetting for Selective Multimodal Large Language Model Unlearning', 'authors': 'Zhen Zeng, Leijiang Gu, Zhangling Duan, Feng Li, Zenglin Shi, Cees G. M. Snoek, Meng Wang', 'link': 'https://arxiv.org/abs/2511.20196', 'abstract': "Multimodal Large Language Models (MLLMs) achieve remarkable capabilities but can inadvertently memorize privacy-sensitive information. Although existing unlearning methods can remove such knowledge, they fail to achieve benign forgetting because they often degrade the model's general image understanding performance. To address this, we propose the Sculpted Memory Forgetting Adapter (SMFA), which confines forgetting to targeted memory regions while preserving overall capabilities. SMFA first fine-tunes the model to replace sensitive responses with refusals, yielding a memory forgetting adapter, and then applies a retaining anchor-guided masking mechanism to prevent interference with unrelated knowledge and understanding ability. To systematically evaluate selective MLLM unlearning, we introduce S-MLLMUn Bench, the first benchmark designed to jointly assess the removal of sensitive knowledge and retention of general visual understanding. Extensive experiments show that, unlike prior methods, SMFA achieves precise and controllable unlearning while maintaining the model's foundational image understanding.", 'abstract_zh': '多模态大型语言模型的雕琢记忆遗忘适配器（Sculpted Memory Forgetting Adapter for MLLMs）实现精确可控的遗忘的同时保持模型的图像理解能力', 'title_zh': '面向选择性多模态大型语言模型遗忘的良性记忆遗忘方法'}
{'arxiv_id': 'arXiv:2511.20048', 'title': 'Reducing Latency of LLM Search Agent via Speculation-based Algorithm-System Co-Design', 'authors': 'Zixiao Huang, Wen Zeng, Tianyu Fu, Tengxuan Liu, Yizhou Sun, Ke Hong, Xinhao Yang, Chengchun Liu, Yan Li, Quanlu Zhang, Guohao Dai, Zhenhua Zhu, Yu Wang', 'link': 'https://arxiv.org/abs/2511.20048', 'abstract': 'LLM-based search agents achieve strong performance but suffer from severe latency, as each step requires serialized LLM reasoning followed by action of tool execution. We revisit this bottleneck through the lens of speculation. While traditional predict-verify speculation paradigm can break serial execution, its benefit remains limited, as it retains the full original workload and adds extra inference overhead. We observe that early agent steps often involve simple evidence-gathering, where correct actions can often be predicted without full reasoning. Building on these observations, we present SPAgent, an algorithm-system co-design framework that expands the role of speculation in search agents to reduce latency. Algorithmically, SPAgent introduces a two-phase adaptive speculation mechanism that selectively omits verification when safe. System-wise, a two-level scheduler regulates speculative requests based on engine load to ensure speculation remains beneficial. We implement SPAgent in real-world systems. Across extensive experimental settings, SPAgent achieves up to $1.65\\times$ end-to-end speedup while maintaining same or even achieving higher accuracy, enabling practical deployment of multi-step search agents.', 'abstract_zh': '基于LLM的搜索代理实现 strong performance 但遭受严重延迟问题，因为每一步骤都需要序列化的LLM推理随后是工具执行动作。我们通过推测的视角重新审视这一瓶颈。尽管传统的预测-验证推测范式可以打破串行执行，其收益仍然有限，因为它保留了原始全部工作负载并增加了额外的推理开销。我们观察到，在早期的代理步骤中，通常涉及简单的证据收集，正确的行动往往可以在不进行完全推理的情况下被预测。基于这些观察，我们提出了SPAgent，这是一种算法-系统协同设计框架，旨在扩大推测在搜索代理中的作用以减少延迟。从算法上看，SPAgent引入了一种双阶段自适应推测机制，在安全时可以选择跳过验证。从系统上看，双层调度器根据引擎负载调节推测请求，以确保推测仍有益处。我们在实际系统中实现了SPAgent。在广泛的实验设置中，SPAgent实现了高达1.65倍的端到端加速，同时保持相同甚至更高的准确率，从而使得多步搜索代理的实际部署成为可能。', 'title_zh': '基于 speculation 基础的算法-系统协同设计减少大模型搜索代理延迟'}
{'arxiv_id': 'arXiv:2511.19933', 'title': 'A System-Level Taxonomy of Failure Modes in Large Language Model Applications', 'authors': 'Vaishali Vinay', 'link': 'https://arxiv.org/abs/2511.19933', 'abstract': 'Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.', 'abstract_zh': '大型语言模型（LLMs）正迅速集成到决策支持工具、自动化工作流和AI驱动的软件系统中。然而，它们在生产环境中的行为仍然了解不足，其故障模式与传统机器学习模型有本质的不同。本文提出了一种系统级的分类法，涵盖了实际应用中出现的十五种隐藏故障模式，包括多步推理漂移、潜在不一致性、上下文边界退化、错误工具调用、版本漂移以及成本驱动的性能崩溃。使用这种分类法，我们分析了现有评估和监控实践日益扩大的差距：现有基准仅衡量知识或推理，但对于稳定性和可重复性、漂移或工作流集成几乎没有洞察力。我们进一步探讨了部署LLMs相关的生产挑战，包括可观测性限制、成本约束以及更新引起的回退，并概述了构建可靠、可维护且成本意识强的LLM系统的高层设计原则。最后，我们概述了构建可靠、可维护且成本意识强的基于LLM系统的高层设计原则。通过将LLM可靠性视为系统工程问题而非纯粹的模型中心问题，本文为未来的评估方法研究、AI系统鲁棒性和可依赖的LLM部署奠定了分析基础。', 'title_zh': '大型语言模型应用中故障模式的系统级分类'}
{'arxiv_id': 'arXiv:2511.19925', 'title': 'Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity', 'authors': 'Qiyao Wei, Edward Morrell, Lea Goetz, Mihaela van der Schaar', 'link': 'https://arxiv.org/abs/2511.19925', 'abstract': 'Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at this https URL and the dataset is available at this https URL.', 'abstract_zh': '评估大型语言模型（LLMs）生成的开放式文本响应通常需要测量响应与（人类生成的）参考答案的语义相似性。然而，目前的语义相似性方法可能存在捕获句法或词汇形式而非语义内容的情况。虽然存在语义等价基准，但它们往往由于依赖主观的人类判断而产生较高的生成成本，对于特定领域应用的可用性有限，且语义等价的定义不够清晰。本文介绍了一种新颖的方法，用于生成评估LLM输出语义相似性方法的基准，专门针对这些局限性。我们的方法利用知识图谱（KGs）生成语义相似或不相似的自然语言陈述对，将不相似的对分类为四种亚类型之一。我们生成了四个不同领域的基准数据集（通用知识、生物医学、金融、生物学），并对包括传统自然语言处理评分和LLM作为评委预测在内的语义相似性方法进行了比较研究。我们发现，语义变化的亚类型以及基准的领域都会影响语义相似性方法的性能，没有任何方法始终占优。研究结果提出了在使用LLM作为评委检测文本语义内容方面的意义。代码可供在此访问：this https URL，数据集可供在此访问：this https URL。', 'title_zh': '语义-KG：利用知识图谱构建测量语义相似度的基准'}
{'arxiv_id': 'arXiv:2511.19895', 'title': 'RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation', 'authors': 'Yuanyuan Lin, Xiangyu Ouyang, Teng Zhang, Kaixin Sui', 'link': 'https://arxiv.org/abs/2511.19895', 'abstract': 'Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.', 'abstract_zh': '基于树搜索的方法在增强大型语言模型的代码生成能力方面取得了显著进展。然而，由于难以有效评估中间算法步骤并及时定位和纠正错误步骤，这些方法往往生成错误代码并增加计算成本。为解决这些问题，我们提出RPM-MCTS，一种有效的基于蒙特卡洛树搜索和过程奖励模型的知识检索方法，用于评估中间算法步骤。通过利用知识库检索，RPM-MCTS避免了过程奖励模型的复杂训练。在扩展阶段，采用相似性过滤去除冗余节点，确保推理路径的多样性。此外，我们的方法利用沙盒执行反馈在生成过程中定位错误的算法步骤，实现及时和针对性的纠正。在四个公开的代码生成基准上的 extensive 实验表明，RPM-MCTS 在性能上优于当前最先进的方法，同时大约减少 15% 的 token 消耗。此外，使用 RPM-MCTS 构建的数据对基础模型进行全面微调显著增强了其代码能力。', 'title_zh': 'RPM-MCTS：基于过程奖励模型的 Monte Carlo 树搜索代码生成知识检索方法'}
{'arxiv_id': 'arXiv:2511.19872', 'title': 'Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy', 'authors': 'Daniel I Jackson, Emma L Jensen, Syed-Amad Hussain, Emre Sezgin', 'link': 'https://arxiv.org/abs/2511.19872', 'abstract': 'Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.', 'abstract_zh': '自我评估是可靠情报的关键方面，然而对大规模语言模型（LLMs）的评估主要集中在任务准确性上。我们改编了10项一般自我效能量表（GSES），从四个条件下十种LLM中 elicited 出模拟的自我评估：无任务条件、计算推理条件、社会推理条件和总结条件。GSES 回答在多次执行中表现出高度稳定，并且随机化的项目顺序也稳定。然而，模型在不同条件下的自我效能水平显著不同，汇总得分低于人类常态。所有模型在计算和社交问题上都达到了完美的准确性，而总结性能差异很大。自我评估并不能可靠地反映能力：一些低分模型表现准确，而一些高分模型则产生了较弱的总结。后续的信心提示导致了适度的、主要是向下修正，表明初次评估时可能存在轻微的高估。定性分析显示，更高的自我效能对应于更为自信且拟人化的推理风格，而较低的分数则反映了更为谨慎且去拟人化的解释。心理测量提示为理解LLM的交流行为提供了结构化的见解，但不能提供校准后的性能估计。', 'title_zh': '大型语言模型中的模拟自我评估：AI自我效能的 psychometric 方法'}
{'arxiv_id': 'arXiv:2511.19749', 'title': 'Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions', 'authors': 'Farzan Karimi-Malekabadi, Pooya Razavi, Sonya Powers', 'link': 'https://arxiv.org/abs/2511.19749', 'abstract': 'As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.', 'abstract_zh': '随着教育系统的发展，确保评估项目与内容标准保持一致对于维护公平性和教学相关性至关重要。传统的手动对齐审查虽然准确但效率低下且劳动密集，特别是在处理大规模项目库时。本研究探讨了大规模语言模型（LLMs）是否能够在不牺牲准确性的前提下加速这一过程。通过K-5年级的超过12,000个项目-技能对，我们测试了三种LLM（GPT-3.5 Turbo、GPT-4o-mini和GPT-4o）在三个模拟现实挑战的任务中的表现：识别不一致的项目、从完整标准集中选择正确的技能，以及在分类前缩小候选列表。在第一阶段，GPT-4o-mini正确识别对齐状态的情况约为83-94%，包括细微的不一致。在第二阶段，数学领域的表现强劲，但阅读领域表现较低，因为标准之间更具有语义重叠性。第三阶段显示，候选技能的预筛选显著提高了结果，正确技能出现在前五建议中的比例超过95%。这些发现表明，特别是在与候选技能过滤策略结合使用时，LLMs可以显著减轻手工项目审查的负担，同时保持对齐准确性。我们建议开发混合管线，结合基于LLM的筛选与人工审查，特别是在存在歧义的情况下，提供一种可扩展的持续项目验证和教学对齐解决方案。', 'title_zh': '使用大型语言模型扩展项目与标准的对齐：准确度、限制与解决方案'}
{'arxiv_id': 'arXiv:2511.19671', 'title': 'FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking', 'authors': 'Rishab Sharma, Iman Saberi, Elham Alipour, Jie JW Wu, Fatemeh Fard', 'link': 'https://arxiv.org/abs/2511.19671', 'abstract': 'Financial applications of large language models (LLMs) require factual reliability and computational efficiency, yet current systems often hallucinate details and depend on prohibitively large models. We propose FISCAL (Financial Synthetic Claim-Document Augmented Learning), a modular framework for generating synthetic data tailored to financial fact-checking. Using FISCAL, we generate a dataset called FISCAL-data and use it to train MiniCheck-FISCAL, a lightweight verifier for numerical financial claims. MiniCheck-FISCAL outperforms its baseline, surpasses GPT-3.5 Turbo and other open-source peers of similar size, and approaches the accuracy of much larger systems (20x), such as Mixtral-8x22B and Command R+. On external datasets FinDVer and Fin-Fact, it rivals GPT-4o and Claude-3.5 while outperforming Gemini-1.5 Flash. These results show that domain-specific synthetic data, combined with efficient fine-tuning, enables compact models to achieve state-of-the-art accuracy, robustness, and scalability for practical financial AI. The dataset and scripts are available in the project repository (link provided in the paper).', 'abstract_zh': '大型语言模型在金融领域的应用需要事实可靠性和计算效率，但当前系统往往会产生虚假细节并依赖于过于庞大的模型。我们提出了一种名为FISCAL（Financial Synthetic Claim-Document Augmented Learning）的模块化框架，用于生成适合金融事实核查的合成数据。利用FISCAL，我们生成了一个名为FISCAL-data的数据集，并使用该数据集训练了一个轻量级的数值金融声明验证器MiniCheck-FISCAL。MiniCheck-FISCAL在基准模型上表现出色，超越了GPT-3.5 Turbo和其他类似规模的开源竞争对手，并接近了更大规模系统的准确性（20倍），如Mixtral-8x22B和Command R+。在FinDVer和Fin-Fact外部数据集上，MiniCheck-FISCAL与GPT-4o和Claude-3.5竞争，同时超越了Gemini-1.5 Flash。这些结果表明，结合高效的微调，特定领域的合成数据能够使紧凑模型实现先进水平的准确度、稳健性和可扩展性，适用于实践中的金融AI。数据集和脚本已在项目存储库中提供（论文中提供链接）。', 'title_zh': '财政学：金融合成声明文档增强学习以实现高效的事实核查'}
{'arxiv_id': 'arXiv:2511.19663', 'title': 'Fara-7B: An Efficient Agentic Model for Computer Use', 'authors': 'Ahmed Awadallah, Yash Lara, Raghav Magazine, Hussein Mozannar, Akshay Nambi, Yash Pandya, Aravind Rajeswaran, Corby Rosset, Alexey Taymanov, Vibhav Vineet, Spencer Whitehead, Andrew Zhao', 'link': 'https://arxiv.org/abs/2511.19663', 'abstract': 'Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.', 'abstract_zh': '面向多步网络任务的新型合成数据生成系统FaraGen及其在CUA模型Fara-7B中的应用', 'title_zh': 'Fara-7B: 一种高效的代理型计算机使用模型'}
{'arxiv_id': 'arXiv:2511.19577', 'title': 'Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder', 'authors': 'Abhay Goyal, Navin Kumar, Kimberly DiMeola, Rafael Trujillo, Soorya Ram Shimgekar, Christian Poellabauer, Pi Zonooz, Ermonda Gjoni-Markaj, Declan Barry, Lynn Madden', 'link': 'https://arxiv.org/abs/2511.19577', 'abstract': 'Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.', 'abstract_zh': '慢性疼痛和阿片类药物使用障碍的共病及综合治疗研究：可穿戴设备与人工智能的应用探索', 'title_zh': '使用可穿戴设备改善阿片类药物使用障碍患者慢性疼痛治疗'}
{'arxiv_id': 'arXiv:2511.20639', 'title': 'Latent Collaboration in Multi-Agent Systems', 'authors': 'Jiaru Zou, Xiyuan Yang, Ruizhong Qiu, Gaotang Li, Katherine Tieu, Pan Lu, Ke Shen, Hanghang Tong, Yejin Choi, Jingrui He, James Zou, Mengdi Wang, Ling Yang', 'link': 'https://arxiv.org/abs/2511.20639', 'abstract': "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at this https URL.", 'abstract_zh': '多智能体系统（MAS）将大规模语言模型（LLMs）从独立单模型推理扩展到协调性的系统级智能。虽然现有的LLM代理依赖于基于文本的中介进行推理和通信，但我们通过使模型直接在连续的潜在空间中协作来迈出了一步。我们引入了LatentMAS，这是一种端到端的无需训练框架，能够在LLM代理之间实现纯粹的潜在协作。在LatentMAS中，每个代理首先通过最后一层隐藏嵌入生成自回归的潜在思想。共享的潜在工作记忆则保存并传输每个代理的内部表示，确保无损信息交换。我们提供了理论分析，证明LatentMAS在显著降低复杂性的情况下实现了更高的表达性和无损信息保存。此外，在涵盖数学和科学推理、常识理解和代码生成的9个综合基准上进行的实证评估显示，LatentMAS始终优于强大的单模型和基于文本的MAS基线，准确率提高了14.6%，输出令牌使用减少了70.8%-83.7%，并实现了4-4.3倍的端到端推理速度提升。这些结果表明，我们的新型潜在协作框架在提高系统级推理质量的同时提供了显著的效率提升，无需任何额外训练。相关代码和数据已完全开源。', 'title_zh': '多智能体系统中的潜在协作'}
{'arxiv_id': 'arXiv:2511.20626', 'title': 'ROOT: Robust Orthogonalized Optimizer for Neural Network Training', 'authors': 'Wei He, Kai Han, Hang Zhou, Hanting Chen, Zhicheng Liu, Xinghao Chen, Yunhe Wang', 'link': 'https://arxiv.org/abs/2511.20626', 'abstract': 'The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）的优化仍然是一个关键挑战，尤其是随着模型规模的扩大加剧了对算法精确度和训练稳定性的敏感性。最近在优化器方面的进展通过动量正交化提高了收敛效率，但仍然存在两个关键的鲁棒性限制：正交化精度的维数脆弱性和异常值诱导噪声的脆弱性。为了解决这些鲁棒性挑战，我们引入了ROOT（鲁棒正交化优化器），通过双重鲁棒性机制增强了训练稳定性。首先，我们开发了一种基于自适应牛顿迭代的鲁棒正交化方案，使用细粒度系数适应特定矩阵大小，确保在各种架构配置下保持一致的精度。其次，我们引入了一种通过近邻优化的优化鲁棒框架，抑制异常值噪声同时保留有意义的梯度方向。广泛的实验表明，与穆翁和基于Adam的优化器相比，ROOT在鲁棒性、收敛速度和最终性能方面均显著提高，特别是在嘈杂和非凸场景中。我们的工作建立了一种新的范式，用于开发能够处理现代大规模模型训练复杂性的鲁棒和精确优化器。相关代码将在此链接提供：https://xxxxxx。', 'title_zh': 'ROTA：稳健的正交优化器神经网络训练'}
{'arxiv_id': 'arXiv:2511.20621', 'title': 'DiFR: Inference Verification Despite Nondeterminism', 'authors': 'Adam Karvonen, Daniel Reuter, Roy Rinberg, Luke Marks, Adrià Garriga-Alonso, Keri Warr', 'link': 'https://arxiv.org/abs/2511.20621', 'abstract': 'As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.', 'abstract_zh': 'LLM推断验证方法：Token-DiFR与Activation-DiFR', 'title_zh': 'DiFR: 在非确定性环境下进行推理验证'}
{'arxiv_id': 'arXiv:2511.20613', 'title': 'Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning', 'authors': 'Panayiotis Danassis, Naman Goel', 'link': 'https://arxiv.org/abs/2511.20613', 'abstract': "The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios.", 'abstract_zh': '大型语言模型的迅速 proliferation 重塑了 AI 辅助代码生成。这种快速发展的 LLMs 超出了我们对其进行适当基准测试的能力。现存的基准测试主要强调单元测试通过率和语法正确性。这些指标低估了许多需要规划、优化和策略性交互的现实世界问题的难度。我们基于一个真实的物流优化问题（拍卖、取货和送货问题）引入了一种多Agent推理驱动的基准测试，该基准测试结合了竞争性拍卖与容量约束路由。基准测试要求构建能够（i）在不确定性下进行策略性出价和（ii）优化最大化利润的任务交付计划的Agent。我们对比了40个LLM编码的Agent（由多种最先进的LLM以多种提示方法，包括情绪编码下产生）与17个人编码的Agent（LLLMs出现之前开发），这些Agent在12场双循环赛和约4万个比赛中。结果显示：（i）人（研究生）编码Agent具有明显的优势：前五名位置始终被人编码Agent占据；（ii）大多数LLM编码Agent（40个中的33个）被非常简单的基线所击败；（iii）以最佳人类解决方案为输入并提示其改进时，表现最好的LLM反而使解决方案显著恶化而不是改进它。我们的结果突显了LLMs在生成能够有效应用于现实世界的代码方面的能力差距，并促使进行新的评估，重点关注在实际场景中基于推理的代码合成。', 'title_zh': 'Can Vibe Coding 击败graduate CS学生？一种基于市场的战略性规划的LLM与人类编码竞赛'}
{'arxiv_id': 'arXiv:2511.20604', 'title': 'On Evaluating LLM Alignment by Evaluating LLMs as Judges', 'authors': 'Yixin Liu, Pengfei Liu, Arman Cohan', 'link': 'https://arxiv.org/abs/2511.20604', 'abstract': "Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.", 'abstract_zh': 'LLMs的生成与评估能力在遵循人类偏好的关系研究：AlignEval基准的构建与应用', 'title_zh': '评估LLM对齐程度通过将LLM作为法官进行评估'}
{'arxiv_id': 'arXiv:2511.20507', 'title': 'The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models', 'authors': 'Nathan Roll, Jill Kries, Flora Jin, Catherine Wang, Ann Marie Finley, Meghan Sumner, Cory Shain, Laura Gwilliams', 'link': 'https://arxiv.org/abs/2511.20507', 'abstract': 'Large language models (LLMs) have emerged as a candidate "model organism" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB\'s design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen\'s kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.', 'abstract_zh': "大型语言模型（LLMs）作为人类语言的候选“模型生物”，为研究语言障碍如失语症的计算基础提供了前所未有的机会。然而，传统的临床评估方法不适合LLMs，因为它们假设了类似人类的语用压力，并探究了不在人工架构中固有的认知过程。我们引入了文本失语症电池（TAB），这是一种从快速失语症电池（QAB）改编而来的仅基于文本的标准，用于评估LLMs中的失语症样缺陷。TAB包含四个子测试：连贯文本、词汇理解、句法理解以及重复。本文详细介绍了TAB的设计、子测试以及评分标准。为了便于大规模使用，我们使用Gemini 2.5 Flash验证了自动评估协议，并实现了与专家人工评分者相当的可靠性（模型一致性同意的 prevalenceweighted Cohen's kappa = 0.255，人类-人类同意为0.286）。我们公开发布TAB，作为一种基于临床的、可扩展的框架，用于分析人工系统的语言缺陷。", 'title_zh': '文本失语症电池（TAB）：语言模型中失语症样缺陷的临床基准'}
{'arxiv_id': 'arXiv:2511.20459', 'title': "Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts", 'authors': 'Mosab Rezaei, Mina Rajaei Moghadam, Abdul Rahman Shaikh, Hamed Alhoori, Reva Freedman', 'link': 'https://arxiv.org/abs/2511.20459', 'abstract': "Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.", 'abstract_zh': '近期大型语言模型的发展为文体学研究带来了新机会，该研究旨在分析写作风格和作者身份。然而，两个挑战依然存在：在缺乏配对数据的情况下训练生成模型，以及在不依赖人类判断的情况下评估风格文本。本文提出了一种框架，用于生成和评估19世纪小说家风格的句子。通过使用最小的单词提示对大型语言模型进行微调，以产生诸如狄更斯、奥斯汀、泰文、奥尔科特和梅尔维尔等作者的声音文本。为了评估这些生成模型，我们使用一个基于转码器的检测器对其进行训练，将其用作分类器和风格解释工具。我们还使用句法比较和可解释AI方法，包括基于注意力和梯度的分析，以识别驱动风格模仿的语言线索。研究结果表明，生成的文本反映了作者的独特模式，且基于AI的评估提供了可靠的人类评估替代方案。本文的所有研究成果均已在线发布。', 'title_zh': '基于单令牌提示生成、评估和解释 novelist 的写作风格'}
{'arxiv_id': 'arXiv:2511.20403', 'title': 'LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework', 'authors': 'Andrea Lops, Fedelucio Narducci, Azzurra Ragone, Michelantonio Trizio, Claudio Barto', 'link': 'https://arxiv.org/abs/2511.20403', 'abstract': 'Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.', 'abstract_zh': '大型语言模型生成的Java单元测试的自动化评估框架：AgoneTest', 'title_zh': '基于Java的自动单元测试生成与评估的大型语言模型：AgoneTest框架'}
{'arxiv_id': 'arXiv:2511.20399', 'title': 'BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali', 'authors': 'Abdullah Al Sefat', 'link': 'https://arxiv.org/abs/2511.20399', 'abstract': 'Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.', 'abstract_zh': '大型语言模型在广泛多语言基准测试中表现出色，但在比喻性和文化 grounding 的推理能力，尤其是在低资源背景下，仍有待广泛评估。我们提出了 BengaliFig，这是一个面向孟加拉语这一广泛使用但资源匮乏语言的紧凑而丰富标注的挑战集。数据集包含源自孟加拉语口头和文学传统的 435 个独特谜语。每个条目按照推理类型、陷阱类型、文化深度、答案类别和难度五个正交维度进行标注，并通过一个基于约束的、AI 辅助的管道自动转换为多项选择格式。我们在多种主要提供商的八种最先进 LLM 下进行了零shot 和少shot 链式思考提示下的评估，揭示了其在隐喻性和文化特异性推理方面的持续弱点。因此，BengaliFig 既为评估 LLM 在低资源文化背景下的鲁棒性提供了诊断工具，也为促进包容性和文化遗产意识的 NLP 评估迈出了实质性的一步。', 'title_zh': 'BengaliFig：孟加拉语中具象化和文化 grounding 推理的低资源挑战'}
{'arxiv_id': 'arXiv:2511.20347', 'title': 'Soft Adaptive Policy Optimization', 'authors': 'Chang Gao, Chujie Zheng, Xiong-Hui Chen, Kai Dang, Shixuan Liu, Bowen Yu, An Yang, Shuai Bai, Jingren Zhou, Junyang Lin', 'link': 'https://arxiv.org/abs/2511.20347', 'abstract': 'Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.', 'abstract_zh': 'Soft Adaptive Policy Optimization for Enhancing Training Stability and Performance in Reinforcement Learning of Large Language Models', 'title_zh': '软自适应策略优化'}
{'arxiv_id': 'arXiv:2511.20315', 'title': 'Geometry of Decision Making in Language Models', 'authors': 'Abhinav Joshi, Divyanshu Bhatt, Ashutosh Modi', 'link': 'https://arxiv.org/abs/2511.20315', 'abstract': 'Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \\textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.', 'abstract_zh': '大型语言模型（LLMs）在多种任务上表现出强大的泛化能力，但其内部决策过程仍然不透明。本研究通过内在维度（ID）的研究视角，探讨LLMs中隐藏表示的几何结构，重点关注多选题回答（MCQA）任务中的决策动态。我们进行了大规模研究，涉及28个开放权重变压器模型，并使用多种估计器在各层上估计ID，同时量化各层在MCQA任务上的表现。我们的发现揭示了一个一致的ID模式：早期层在低维流形上操作，中间层扩展这一空间，而后层再次压缩它，最终收敛到与任务相关的表现。这些结果共同表明，LLMs隐式学习将语言输入投影到与任务特定决策相一致的结构化低维流形上，为语言模型中泛化和推理如何产生提供了新的几何洞察。', 'title_zh': '语言模型中决策几何学'}
{'arxiv_id': 'arXiv:2511.20284', 'title': 'Can LLMs Make (Personalized) Access Control Decisions?', 'authors': 'Friederike Groschupp, Daniele Lain, Aritra Dhar, Lara Magdalena Lazier, Srdjan Čapkun', 'link': 'https://arxiv.org/abs/2511.20284', 'abstract': "Precise access control decisions are crucial to the security of both traditional applications and emerging agent-based systems. Typically, these decisions are made by users during app installation or at runtime. Due to the increasing complexity and automation of systems, making these access control decisions can add a significant cognitive load on users, often overloading them and leading to suboptimal or even arbitrary access control decisions. To address this problem, we propose to leverage the processing and reasoning capabilities of large language models (LLMs) to make dynamic, context-aware decisions aligned with the user's security preferences. For this purpose, we conducted a user study, which resulted in a dataset of 307 natural-language privacy statements and 14,682 access control decisions made by users. We then compare these decisions against those made by two versions of LLMs: a general and a personalized one, for which we also gathered user feedback on 1,446 of its decisions.\nOur results show that in general, LLMs can reflect users' preferences well, achieving up to 86\\% accuracy when compared to the decision made by the majority of users. Our study also reveals a crucial trade-off in personalizing such a system: while providing user-specific privacy preferences to the LLM generally improves agreement with individual user decisions, adhering to those preferences can also violate some security best practices. Based on our findings, we discuss design and risk considerations for implementing a practical natural-language-based access control system that balances personalization, security, and utility.", 'abstract_zh': '精确的访问控制决策对于传统应用程序和新兴基于代理的系统安全至关重要。通常，这些决策是在应用程序安装时或运行时由用户作出的。由于系统的复杂性和自动化程度不断增加，这些访问控制决策可能会给用户带来显著的认知负担，导致用户决策过度负荷，从而导致次优甚至任意的访问控制决策。为解决这一问题，我们提出利用大型语言模型（LLMs）的处理和推理能力，动态地、上下文感知地作出与用户安全偏好相一致的决策。为此，我们进行了一项用户研究，收集了307个自然语言隐私声明和14,682个用户作出的访问控制决策。我们还将这些决策与两种版本的LLMs作出的决策进行了比较：一般版本和个性化版本，并收集了1,446个决策的用户反馈。我们的结果表明，总体而言，LLMs能够较好地反映用户偏好，与大多数用户作出的决策相比，准确率最高可达86%。我们的研究还揭示了一个关键的权衡：尽管将用户特定的隐私偏好提供给LLM通常能提高与个别用户决策的一致性，但遵循这些偏好也可能违反一些安全最佳实践。根据我们的发现，我们讨论了平衡个性化、安全性和实用性实施基于自然语言的访问控制系统的设计和风险考虑。', 'title_zh': 'LLM能够作出（个性化）访问控制决策吗？'}
{'arxiv_id': 'arXiv:2511.20172', 'title': 'Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management', 'authors': 'Xinjun Yang, Qingda Hu, Junru Li, Feifei Li, Yuqi Zhou, Yicong Zhu, Qiuru Lin, Jian Dai, Yang Kong, Jiayu Zhang, Guoqiang Xu, Qiang Liu', 'link': 'https://arxiv.org/abs/2511.20172', 'abstract': 'The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.', 'abstract_zh': 'Beluga：一种通过CXL交换机访问共享大规模内存池的GPU内存架构', 'title_zh': 'Beluga：一种基于CXL的可扩展高效的大语言模型KV缓存管理内存架构'}
{'arxiv_id': 'arXiv:2511.20162', 'title': 'While recognizing actions, LMMs struggle to detect core interaction events', 'authors': 'Daniel Harari, Michael Sidorov, Liel David, Chen Shterental, Abrham Kahsay Gebreselasie, Muhammad Haris Khan', 'link': 'https://arxiv.org/abs/2511.20162', 'abstract': "Large multi-modal models (LMMs) show increasing performance in realistic visual tasks for images and, more recently, for videos. For example, given a video sequence, such models are able to describe in detail objects, the surroundings and dynamic actions. In this study, we explored the extent to which these models ground their semantic understanding in the actual visual input. Specifically, given sequences of hands interacting with objects, we asked models when and where the interaction begins or ends. For this purpose, we introduce a first of its kind, large-scale dataset with more than 20K annotated interactions on videos from the Something-Something-V2 dataset. 250 AMTurk human annotators labeled core interaction events, particularly when and where objects and agents become attached ('contact') or detached ('release'). We asked two LMMs (Qwen-2.5VL and GPT-4o) to locate these events in short videos, each with a single event. The results show that although the models can reliably name the target objects, identify the action and provide coherent reasoning, they consistently fail to identify the frame where the interaction begins or ends and cannot localize the event within the scene. Our findings suggest that in struggling to pinpoint the moment and location of physical contact that defines the interaction, the models lack the perceptual grounding required for deeper understanding of dynamic scenes.", 'abstract_zh': '大型多模态模型在现实视觉任务中的语义理解程度：以手与物体交互视频为例', 'title_zh': '在识别动作时，LMMs难以检测核心交互事件。'}
{'arxiv_id': 'arXiv:2511.20120', 'title': '"When Data is Scarce, Prompt Smarter"... Approaches to Grammatical Error Correction in Low-Resource Settings', 'authors': 'Somsubhra De, Harsh Kumar, Arun Prakash A', 'link': 'https://arxiv.org/abs/2511.20120', 'abstract': 'Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.', 'abstract_zh': '基于提示的大语言模型在低资源印地语系语言语法错误修正中的应用', 'title_zh': '当数据稀缺时，让提示更智能……低资源环境中的语法错误纠正方法'}
{'arxiv_id': 'arXiv:2511.20104', 'title': 'The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs', 'authors': 'Craig Dickson', 'link': 'https://arxiv.org/abs/2511.20104', 'abstract': 'Prior work has shown that fine-tuning models on a narrow domain with misaligned data can lead to broad misalignment - a phenomenon termed "emergent misalignment" (Betley et al. 2025). While all tested models were susceptible to emergent misalignment, some models showed more resistance than others. Specifically the Qwen-2.5 family proved to be relatively resistant, while GPT-4o exhibited the strongest misalignment. In this paper we evaluate if current-generation open-weights models exhibit similar resistance to the Qwen-2.5 family and measure misalignment robustness over a range of model architectures and scales.\nWe replicate the effect across nine modern open-weights models (Gemma 3 and Qwen 3 families, 1B-32B parameters). Models fine-tuned on insecure code generation show a 0.68% misalignment rate (compared to 0.07% for base models), matching the lower end of prior open-model results but dramatically lower than GPT-4o\'s 20%.\nWe identify a critical format-dependent vulnerability: requiring JSON output doubles misalignment rates compared to natural language prompts (0.96% vs 0.42%). This suggests that structural constraints may bypass safety training by reducing the model\'s \'degrees of freedom\' to refuse. These findings confirm emergent misalignment as a reproducible phenomenon in modern open-weights models, with rates substantially lower than observed in proprietary systems.', 'abstract_zh': '新兴对齐失调在现代开放权重模型中的重现与分析', 'title_zh': '细节中的恶魔：开放权重LLM中 Emergent 不对齐、格式和连贯性问题'}
{'arxiv_id': 'arXiv:2511.20090', 'title': 'R3A: Reliable RTL Repair Framework with Multi-Agent Fault Localization and Stochastic Tree-of-Thoughts Patch Generation', 'authors': 'Zizhang Luo, Fan Cui, Kexing Zhou, Runlin Guo, Mile Xia, Hongyuan Hou, Yun Lian', 'link': 'https://arxiv.org/abs/2511.20090', 'abstract': 'Repairing RTL bugs is crucial for hardware design and verification. Traditional automatic program repair (APR) methods define dedicated search spaces to locate and fix bugs with program synthesis. However, they heavily rely on fixed templates and can only deal with limited bugs. As an alternative, Large Language Models with the ability to understand code semantics can be explored for RTL repair. However, they suffer from unreliable outcomes due to inherent randomness and long input contexts of RTL code and waveform. To address these challenges, we propose R3A, an LLM-based automatic RTL program repair framework upon the basic model to improve reliability. R3A proposes the stochastic Tree-Of-Thoughts method to control a patch generation agent to explore a validated solution for the bug. The algorithm samples search states according to a heuristic function to balance between exploration and exploitation for a reliable outcome. Besides, R3A proposes a multi-agent fault localization method to find fault candidates as the starting points for the patch generation agent, further increasing the reliability. Experiments show R3A can fix 90.6% of bugs in the RTL-repair dataset within a given time limit, which covers 45% more bugs than traditional methods and other LLM-based approaches, while achieving an 86.7% pass@5 rate on average, showing a high reliability.', 'abstract_zh': '基于大规模语言模型的可靠RTL自动程序修复框架R3A', 'title_zh': 'R3A：具有多代理故障定位和随机树状思考修补生成的可靠RTL修复框架'}
{'arxiv_id': 'arXiv:2511.20002', 'title': "On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation", 'authors': 'Changyue Li, Jiaying Li, Youliang Yuan, Jiaming He, Zhicong Huang, Pinjia He', 'link': 'https://arxiv.org/abs/2511.20002', 'abstract': 'Conventional adversarial attacks focus on manipulating a single decision of neural networks. However, real-world models often operate in a sequence of decisions, where an isolated mistake can be easily corrected, but cascading errors can lead to severe risks.\nThis paper reveals a novel threat: a single perturbation can hijack the whole decision chain. We demonstrate the feasibility of manipulating a model\'s outputs toward multiple, predefined outcomes, such as simultaneously misclassifying "non-motorized lane" signs as "motorized lane" and "pedestrian" as "plastic bag".\nTo expose this threat, we introduce Semantic-Aware Universal Perturbations (SAUPs), which induce varied outcomes based on the semantics of the inputs. We overcome optimization challenges by developing an effective algorithm, which searches for perturbations in normalized space with a semantic separation strategy. To evaluate the practical threat of SAUPs, we present RIST, a new real-world image dataset with fine-grained semantic annotations. Extensive experiments on three multimodal large language models demonstrate their vulnerability, achieving a 70% attack success rate when controlling five distinct targets using just an adversarial frame.', 'abstract_zh': '传统的对抗攻击主要针对神经网络单一决策的操纵。然而，现实世界的模型往往在一系列决策中运行，其中孤立的错误可能容易纠正，但级联错误可能导致严重风险。\n本文揭示了一种新型威胁：单一扰动可以劫持整个决策链。我们展示了操纵模型输出以实现多种预定义结果的可行性，例如同时错误地将“非机动车道”标志分类为“机动车道”并将“行人”分类为“塑料袋”。\n为了揭示这一威胁，我们引入了语义感知通用扰动（SAUPs），根据输入的语义诱导各种结果。我们通过开发一种有效的算法克服优化挑战，该算法使用语义分离策略在归一化空间中搜索扰动。为了评估SAUPs的实际威胁，我们提出了RIST，一个新的带细粒度语义标注的现实世界图像数据集。在三个多模态大语言模型上的广泛实验表明，仅使用一个对抗帧控制五个不同目标时，它们的成功攻击率可达70%。', 'title_zh': '通过对一个扰动即可劫持MLLMs决策链的可能性的研究'}
{'arxiv_id': 'arXiv:2511.19997', 'title': 'Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test', 'authors': 'Mihir Sahasrabudhe', 'link': 'https://arxiv.org/abs/2511.19997', 'abstract': 'Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a "reversal curse," and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.', 'abstract_zh': 'Transformer在理论上是反转不变的：其函数类不偏好从左到右的映射超过从右到左的映射。然而，自然语言的实证研究反复报道了“反转诅咒”，并且近期有关LLMs时间不对称性的研究指出，现实世界的数据集携带着自身的时间箭头。这留下了一个未解决的问题：方向性失败是源于语言统计数据，还是源于架构本身？我们通过一个完全合成的、熵控基准测试来消除这种模糊性，这个基准测试设计为方向性学习的洁净室压力测试。使用可调分支因子K的随机字符串映射，我们构建了零条件熵的正向任务和熵地板由解析方法确定的逆向任务。这些地板之上超出损失揭示即使是从零开始训练的GPT-2模型也表现出强烈且可重复的方向性优化差距（例如，在K=5时为1.16 nats），这远大于在同一数据上训练的MLP所表现出的差距。预训练初始化会改变优化行为但并不能消除这一差距，而LoRA在高熵逆映射上遇到了陡峭的能力墙。这些结果隔离了一个与因果Transformer训练固有相关的基本的方向性摩擦的最小化特征签名—即使移除了语言先验、标记频率和语料库级别的时间不对称性，该特征签名仍然存在。我们的基准测试提供了一种可控的工具来剖析现代序列模型中的方向性偏见，并激发对为什么逆变对Transformer来说依然根本上更难的更深入的机制性研究。', 'title_zh': 'Transformer中方向优化不对称性：一种合成压力测试'}
{'arxiv_id': 'arXiv:2511.19931', 'title': 'LLM-EDT: Large Language Model Enhanced Cross-domain Sequential Recommendation with Dual-phase Training', 'authors': 'Ziwei Liu, Qidong Liu, Wanyu Wang, Yejing Wang, Tong Xu, Wei Huang, Chong Chen, Peng Chuan, Xiangyu Zhao', 'link': 'https://arxiv.org/abs/2511.19931', 'abstract': "Cross-domain Sequential Recommendation (CDSR) has been proposed to enrich user-item interactions by incorporating information from various domains. Despite current progress, the imbalance issue and transition issue hinder further development of CDSR. The former one presents a phenomenon that the interactions in one domain dominate the entire behavior, leading to difficulty in capturing the domain-specific features in the other domain. The latter points to the difficulty in capturing users' cross-domain preferences within the mixed interaction sequence, resulting in poor next-item prediction performance for specific domains. With world knowledge and powerful reasoning ability, Large Language Models (LLMs) partially alleviate the above issues by performing as a generator and an encoder. However, current LLMs-enhanced CDSR methods are still under exploration, which fail to recognize the irrelevant noise and rough profiling problems. Thus, to make peace with the aforementioned challenges, we proposed an LLMs Enhanced Cross-domain Sequential Recommendation with Dual-phase Training ({LLM-EDT}). To address the imbalance issue while introducing less irrelevant noise, we first propose the transferable item augmenter to adaptively generate possible cross-domain behaviors for users. Then, to alleviate the transition issue, we introduce a dual-phase training strategy to empower the domain-specific thread with a domain-shared background. As for the rough profiling problem, we devise a domain-aware profiling module to summarize the user's preference in each domain and adaptively aggregate them to generate comprehensive user profiles. The experiments on three public datasets validate the effectiveness of our proposed LLM-EDT. To ease reproducibility, we have released the detailed code online at {this https URL}.", 'abstract_zh': 'Large Language Models Enhanced Cross-domain Sequential Recommendation with Dual-phase Training (LLM-EDT)', 'title_zh': 'LLM-EDT: 大型语言模型增强的跨域序列推荐与双阶段训练'}
{'arxiv_id': 'arXiv:2511.19875', 'title': "CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection", 'authors': 'Qingyu Zhang, Puzhuo Liu, Peng Di, Chenxiong Qian', 'link': 'https://arxiv.org/abs/2511.19875', 'abstract': 'Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level "purpose" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.', 'abstract_zh': 'CODEFUSE-COMMITEVAL：面向大型语言模型的消息-差异不一致性检测基准', 'title_zh': 'CodeFuse-CommitEval：面向提交信息和代码更改不一致检测能力基准测试的探索'}
{'arxiv_id': 'arXiv:2511.19874', 'title': 'Cross-LLM Generalization of Behavioral Backdoor Detection in AI Agent Supply Chains', 'authors': 'Arun Chowdary Sanna', 'link': 'https://arxiv.org/abs/2511.19874', 'abstract': 'As AI agents become integral to enterprise workflows, their reliance on shared tool libraries and pre-trained components creates significant supply chain vulnerabilities. While previous work has demonstrated behavioral backdoor detection within individual LLM architectures, the critical question of cross-LLM generalization remains unexplored, a gap with serious implications for organizations deploying multiple AI systems. We present the first systematic study of cross-LLM behavioral backdoor detection, evaluating generalization across six production LLMs (GPT-5.1, Claude Sonnet 4.5, Grok 4.1, Llama 4 Maverick, GPT-OSS 120B, and DeepSeek Chat V3.1). Through 1,198 execution traces and 36 cross-model experiments, we quantify a critical finding: single-model detectors achieve 92.7% accuracy within their training distribution but only 49.2% across different LLMs, a 43.4 percentage point generalization gap equivalent to random guessing. Our analysis reveals that this gap stems from model-specific behavioral signatures, particularly in temporal features (coefficient of variation > 0.8), while structural features remain stable across architectures. We show that model-aware detection incorporating model identity as an additional feature achieves 90.6% accuracy universally across all evaluated models. We release our multi-LLM trace dataset and detection framework to enable reproducible research.', 'abstract_zh': '随着AI代理成为企业工作流中不可或缺的部分，它们对共享工具库和预训练组件的依赖性创造了重大的供应链漏洞。虽然先前的工作已经展示了对单个大规模语言模型（LLM）架构中行为后门的检测，但跨LLM的一般化问题仍未得到探索，这在部署多个AI系统的企业中具有严重的影响。我们进行了首次系统性的跨LLM行为后门检测研究，评估了六大生产用大规模语言模型（GPT-5.1、Claude Sonnet 4.5、Grok 4.1、Llama 4 Maverick、GPT-OSS 120B、DeepSeek Chat V3.1）之间的泛化能力。通过1,198个执行踪迹和36个跨模型实验，我们量化了一个关键发现：单模型检测器在其训练分布内达到了92.7%的准确率，但在不同的LLM之间只有49.2%的准确率，这相当于43.4个百分点的泛化差距，相当于随机猜测。我们的分析表明，这一差距源于模型特有的行为特征，特别是在时间特征（变异系数＞0.8）上，而结构特征在架构之间保持稳定。我们展示了结合模型意识检测，将模型身份作为额外特征的检测方法，在所有评估模型中均达到了90.6%的准确率。我们发布了多LLM踪迹数据集和检测框架，以促进可再现的研究。', 'title_zh': 'AI代理供应链中行为后门检测的跨大语言模型泛化'}
{'arxiv_id': 'arXiv:2511.19858', 'title': 'A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction', 'authors': 'Farzad Ahmed, Joniel Augustine Jerome, Meliha Yetisgen, Özlem Uzuner', 'link': 'https://arxiv.org/abs/2511.19858', 'abstract': 'Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.\nMethods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.\nResults: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.\nConclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.', 'abstract_zh': '目标：临床记录中包含事实性、诊断性和管理性错误，这些错误可能会威胁患者安全。大型语言模型（LLMs）可能有助于检测和纠正这些错误，但不同提示策略下的行为尚不明确。我们评估了零-shot提示、静态提示配以随机示例（SPR）以及检索增强的动态提示（RDP）在医疗错误处理三个子任务中的表现：错误标识、错误句子检测和错误修正。', 'title_zh': '大型语言模型结合RAG启用的动态提示系统分析在医疗错误检测与修正中的应用'}
{'arxiv_id': 'arXiv:2511.19830', 'title': 'Beyond Relational: Semantic-Aware Multi-Modal Analytics with LLM-Native Query Optimization', 'authors': 'Junhao Zhu, Lu Chen, Xiangyu Ke, Ziquan Fang, Tianyi Li, Yunjun Gao, Christian S. Jensen', 'link': 'https://arxiv.org/abs/2511.19830', 'abstract': 'Multi-modal analytical processing has the potential to transform applications in e-commerce, healthcare, entertainment, and beyond. However, real-world adoption remains elusive due to the limited ability of traditional relational query operators to capture query semantics. The emergence of foundation models, particularly the large language models (LLMs), opens up new opportunities to develop flexible, semantic-aware data analytics systems that transcend the relational paradigm.\nWe present Nirvana, a multi-modal data analytics framework that incorporates programmable semantic operators while leveraging both logical and physical query optimization strategies, tailored for LLM-driven semantic query processing. Nirvana addresses two key challenges. First, it features an agentic logical optimizer that uses natural language-specified transformation rules and random-walk-based search to explore vast spaces of semantically equivalent query plans -- far beyond the capabilities of conventional optimizers. Second, it introduces a cost-aware physical optimizer that selects the most effective LLM backend for each operator using a novel improvement-score metric. To further enhance efficiency, Nirvana incorporates computation reuse and evaluation pushdown techniques guided by model capability hypotheses. Experimental evaluations on three real-world benchmarks demonstrate that Nirvana is able to reduce end-to-end runtime by 10%--85% and reduces system processing costs by 76% on average, outperforming state-of-the-art systems at both efficiency and scalability.', 'abstract_zh': '多模态数据分析处理有潜力在电子商务、医疗保健、娱乐等领域实现变革。然而，由于传统关系查询操作符捕获查询语义能力有限，实际应用仍然难以实现。基础模型，尤其是大型语言模型（LLMs）的出现为开发超越关系范式的灵活、语义感知的数据分析系统提供了新的机会。\n\n我们提出了Nirvana，一个集成了可编程语义操作符的多模态数据分析框架，同时利用了逻辑和物理查询优化策略，专为LLM驱动的语义查询处理设计。Nirvana解决两个关键挑战。首先，它采用了一个自主逻辑优化器，使用自然语言指定的转换规则和基于随机游走的搜索来探索广泛的语义等效查询计划空间，远超传统优化器的能力。其次，它引入了一种成本感知物理优化器，使用一种新颖的改进得分度量为每个操作符选择最有效的LLM后端。为了进一步提升效率，Nirvana采用了模型能力假设引导的计算复用和评估下推技术。在三个实际基准上的实验评估表明，Nirvana能够将端到端运行时间减少10%至85%，平均降低系统处理成本76%，在效率和可扩展性方面均优于现有的最先进的系统。', 'title_zh': '超越关系：基于语义的多模态分析与LLM原生查询优化'}
{'arxiv_id': 'arXiv:2511.19822', 'title': 'Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models', 'authors': 'Wentao Hu, Mingkuan Zhao, Shuangyong Song, Xiaoyan Zhu, Xin Lai, Jiayin Wang', 'link': 'https://arxiv.org/abs/2511.19822', 'abstract': 'Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static memory overhead, as all experts must be loaded into memory. Existing post-training pruning methods, while reducing model size, often derive their pruning criteria from a single, general-purpose corpus. This leads to a critical limitation: a catastrophic performance degradation when the pruned model is applied to other domains, necessitating a costly re-pruning for each new domain. To address this generalization gap, we introduce Mosaic Pruning (MoP). The core idea of MoP is to construct a functionally comprehensive set of experts through a structured ``cluster-then-select" process. This process leverages a similarity metric that captures expert performance across different task domains to functionally cluster the experts, and subsequently selects the most representative expert from each cluster based on our proposed Activation Variability Score. Unlike methods that optimize for a single corpus, our proposed Mosaic Pruning ensures that the pruned model retains a functionally complementary set of experts, much like the tiles of a mosaic that together form a complete picture of the original model\'s capabilities, enabling it to handle diverse downstream this http URL experiments on various MoE models demonstrate the superiority of our approach. MoP significantly outperforms prior work, achieving a 7.24\\% gain on general tasks and 8.92\\% on specialized tasks like math reasoning and code generation.', 'abstract_zh': '基于聚类选择的马赛克剪枝（MoP）方法：一种新的专家集合构建策略', 'title_zh': '拼图剪枝：Mixture-of-Experts模型的层次化可迁移剪枝框架'}
{'arxiv_id': 'arXiv:2511.19727', 'title': 'Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts', 'authors': 'Steven Peh', 'link': 'https://arxiv.org/abs/2511.19727', 'abstract': 'Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish between trusted instructions and untrusted content. While current LLMs lack native fence awareness, we demonstrate that simulated awareness through prompt instructions achieved complete prevention of injection attacks in our experiments, reducing success rates from 86.7% (260/300 successful attacks) to 0% (0/300 successful attacks) across 300 test cases with two leading LLM providers. We implement a proof-of-concept fence generation and verification pipeline with a total overhead of 0.224 seconds (0.130s for fence generation, 0.094s for validation) across 100 samples. Our approach is platform-agnostic and can be incrementally deployed as a security layer above existing LLM infrastructure, with the expectation that future models will be trained with native fence awareness for optimal security.', 'abstract_zh': '大型语言模型（LLMs）仍易受提示注入攻击的威胁，这是生产部署中最主要的安全风险。我们提出了提示防护（Prompt Fencing）这一新颖的架构方法，通过应用加密认证和数据架构原则，在LLM提示中建立明确的安全边界。该方法对提示片段添加加密签名的元数据，包括信任评级和内容类型，使LLM能够区分受信任指令和不受信任内容。尽管当前的LLM缺乏内置的防护意识，但我们的实验结果表明，通过提示指令模拟防护意识能够完全防止注入攻击，在两个主流LLM提供商的300个测试案例中，成功率从86.7%（260/300成功攻击）降至0%（0/300成功攻击）。我们实现了一个概念验证防护生成和验证管道，总开销为0.224秒（0.130秒用于生成防护，0.094秒用于验证），涵盖100个样本。我们的方法平台无关，并且可以作为现有LLM基础设施之上的安全层逐步部署，未来预计模型将通过内置防护意识优化安全性能。', 'title_zh': '提示围栏：在大规模语言模型提示中建立安全边界的一种密码学方法'}
{'arxiv_id': 'arXiv:2511.19699', 'title': 'A Layered Protocol Architecture for the Internet of Agents', 'authors': 'Charles Fleming, Vijoy Pandey, Ramana Kompella, Luca Muscariello', 'link': 'https://arxiv.org/abs/2511.19699', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance improvements and the ability to learn domain-specific languages (DSLs), including APIs and tool interfaces. This capability has enabled the creation of AI agents that can perform preliminary computations and act through tool calling, now being standardized via protocols like MCP. However, LLMs face fundamental limitations: their context windows cannot grow indefinitely, constraining their memory and computational capacity. Agent collaboration emerges as essential for solving increasingly complex problems, mirroring how computational systems rely on different types of memory to scale. The "Internet of Agents" (IoA) represents the communication stack that enables agents to scale by distributing computation across collaborating entities.\nCurrent network architectural stacks (OSI and TCP/IP) were designed for data delivery between hosts and processes, not for agent collaboration with semantic understanding. To address this gap, we propose two new layers: an \\textbf{Agent Communication Layer (L8)} and an \\textbf{Agent Semantic Negotiation Layer (L9)}. L8 formalizes the \\textit{structure} of communication, standardizing message envelopes, speech-act performatives (e.g., REQUEST, INFORM), and interaction patterns (e.g., request-reply, publish-subscribe), building on protocols like MCP. L9, which does not exist today, formalizes the \\textit{meaning} of communication, enabling agents to discover, negotiate, and lock a "Shared Context" -- a formal schema defining the concepts, tasks, and parameters relevant to their interaction. Together, these layers provide the foundation for scalable, distributed agent collaboration, enabling the next generation of multi-agentic systems.', 'abstract_zh': '大型语言模型(LLMs)在学习领域特定语言(DSLs)方面显示出显著的性能提升能力，包括API和工具接口。这一能力使创建能够执行初步计算并通过工具调用进行动作的AI代理成为可能，现在正通过MCP等协议标准化。然而，LLMs面临着根本性的限制：它们的上下文窗口无法无限扩大，限制了它们的内存和计算能力。代理之间的协作变得必不可少，以解决越来越复杂的问题，这与计算系统依赖不同类型的内存扩展相似。“代理互联网”(IoA)代表了使代理能够通过协作实体分配计算来实现扩展的通信栈。目前的网络架构栈（OSI和TCP/IP）是为在主机和进程之间传输数据而设计的，而不是为了具备语义理解的代理协作。为了解决这一差距，我们提出了两个新的层：一个**代理通信层(L8)**和一个**代理语义协商层(L9)**。L8实现了通信的**结构**，标准化了消息信封、言语行为（例如，REQUEST、INFORM）、以及交互模式（例如，请求-响应、发布-订阅），并在此基础上构建了类似于MCP的协议。L9是一个目前并不存在的层，它实现了通信的**意义**，使代理能够发现、协商并锁定一个“共享上下文”——一种形式化的架构，定义了与它们交互相关的概念、任务和参数。这两个层提供了可扩展的分布式代理协作的基础，使下一代多代理系统成为可能。', 'title_zh': '代理互联网的分层协议架构'}
{'arxiv_id': 'arXiv:2511.19654', 'title': 'Accuracy and Efficiency Trade-Offs in LLM-Based Malware Detection and Explanation: A Comparative Study of Parameter Tuning vs. Full Fine-Tuning', 'authors': 'Stephen C. Gravereaux, Sheikh Rabiul Islam', 'link': 'https://arxiv.org/abs/2511.19654', 'abstract': 'This study examines whether Low-Rank Adaptation (LoRA) fine-tuned Large Language Models (LLMs) can approximate the performance of fully fine-tuned models in generating human-interpretable decisions and explanations for malware classification. Achieving trustworthy malware detection, particularly when LLMs are involved, remains a significant challenge. We developed an evaluation framework using Bilingual Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), and Semantic Similarity Metrics to benchmark explanation quality across five LoRA configurations and a fully fine-tuned baseline. Results indicate that full fine-tuning achieves the highest overall scores, with BLEU and ROUGE improvements of up to 10% over LoRA variants. However, mid-range LoRA models deliver competitive performance exceeding full fine-tuning on two metrics while reducing model size by approximately 81% and training time by over 80% on a LoRA model with 15.5% trainable parameters. These findings demonstrate that LoRA offers a practical balance of interpretability and resource efficiency, enabling deployment in resource-constrained environments without sacrificing explanation quality. By providing feature-driven natural language explanations for malware classifications, this approach enhances transparency, analyst confidence, and operational scalability in malware detection systems.', 'abstract_zh': '本研究考察了低秩适应（LoRA）微调大型语言模型（LLMs）是否能够近似完全微调模型在生成可由人类理解的恶意软件分类决策和解释方面的性能。在大型语言模型参与的情况下，实现可信赖的恶意软件检测仍然是一个重大挑战。我们使用Bilingual Evaluation Understudy（BLEU）、Recall-Oriented Understudy for Gisting Evaluation（ROUGE）和语义相似度指标开发了一种评估框架，以衡量五种LoRA配置和一个完全微调基线的解释质量。结果表明，完全微调在总体评分上最高，BLEU和ROUGE得分比LoRA变体高出10%。然而，中等范围的LoRA模型在两个指标上表现出竞争力，同时将模型大小减少了约81%，并将训练时间减少了超过80%（在一个具有15.5%可训练参数的LoRA模型中）。这些发现表明，LoRA在可解释性和资源效率之间提供了实用的平衡，可以在资源受限的环境中部署，而不牺牲解释质量。通过为恶意软件分类提供特征驱动的自然语言解释，这种方法增强了恶意软件检测系统的透明度、分析师的信心和操作可扩展性。', 'title_zh': '基于LLM的恶意软件检测与解释中的准确性和效率权衡：参数调整与全程微调的比较研究'}
{'arxiv_id': 'arXiv:2511.19648', 'title': 'Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search', 'authors': 'Manil Shrestha, Edward Kim', 'link': 'https://arxiv.org/abs/2511.19648', 'abstract': 'Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.', 'abstract_zh': 'Knowledge Graph上的多跳问答由于潜在推理路径的组合爆炸依然具有计算上的挑战。最近的方法依赖昂贵的大语言模型（LLM）推理进行实体链接和路径排名，限制了其实用部署。此外，LLM生成的答案常常缺乏与结构化知识的验证性连接。我们提出了两种互补的混合算法来解决效率和可验证性问题：（1）LLM引导规划，通过单次LLM调用预测利用广度优先搜索执行的关系序列，实现近乎完美的准确性（微观F1 > 0.90），同时确保所有答案都基于知识图谱；（2）嵌入引导神经搜索，通过轻量级的6.7M参数边评分器融合文本和图嵌入，实现超过100倍的速度提升并保持竞争力的准确性，通过知识蒸馏，我们将规划能力压缩到一个4B参数模型，在零API成本的情况下匹配大模型性能。MetaQA上的评估表明，基于知识的推理始终优于非基线生成，结构化规划比直接答案生成更具有可迁移性。我们的结果表明，在推理时验证性的多跳推理并不需要大规模模型，而是需要结合符号结构与学习表示的正确架构偏置。', 'title_zh': '基于LLM规划与嵌入向量引导搜索的高效知识图谱多跳问答'}
{'arxiv_id': 'arXiv:2511.19550', 'title': 'The Semiotic Channel Principle: Measuring the Capacity for Meaning in LLM Communication', 'authors': 'Davide Picca', 'link': 'https://arxiv.org/abs/2511.19550', 'abstract': "This paper proposes a novel semiotic framework for analyzing Large Language Models (LLMs), conceptualizing them as stochastic semiotic engines whose outputs demand active, asymmetric human interpretation. We formalize the trade-off between expressive richness (semiotic breadth) and interpretive stability (decipherability) using information-theoretic tools. Breadth is quantified as source entropy, and decipherability as the mutual information between messages and human interpretations. We introduce a generative complexity parameter (lambda) that governs this trade-off, as both breadth and decipherability are functions of lambda. The core trade-off is modeled as an emergent property of their distinct responses to $\\lambda$. We define a semiotic channel, parameterized by audience and context, and posit a capacity constraint on meaning transmission, operationally defined as the maximum decipherability by optimizing lambda. This reframing shifts analysis from opaque model internals to observable textual artifacts, enabling empirical measurement of breadth and decipherability. We demonstrate the framework's utility across four key applications: (i) model profiling; (ii) optimizing prompt/context design; (iii) risk analysis based on ambiguity; and (iv) adaptive semiotic systems. We conclude that this capacity-based semiotic approach offers a rigorous, actionable toolkit for understanding, evaluating, and designing LLM-mediated communication.", 'abstract_zh': '一种新颖的符号框架：分析大型语言模型（LLMs），将其视为需要主动、不对称的人类解释的随机符号引擎', 'title_zh': '符号通道原理：测量LLM通信中的意义容量'}
{'arxiv_id': 'arXiv:2511.19537', 'title': 'Cross-Domain Generalization of Multimodal LLMs for Global Photovoltaic Assessment', 'authors': 'Muhao Guo, Yang Weng', 'link': 'https://arxiv.org/abs/2511.19537', 'abstract': 'The rapid expansion of distributed photovoltaic (PV) systems poses challenges for power grid management, as many installations remain undocumented. While satellite imagery provides global coverage, traditional computer vision (CV) models such as CNNs and U-Nets require extensive labeled data and fail to generalize across regions. This study investigates the cross-domain generalization of a multimodal large language model (LLM) for global PV assessment. By leveraging structured prompts and fine-tuning, the model integrates detection, localization, and quantification within a unified schema. Cross-regional evaluation using the $\\Delta$F1 metric demonstrates that the proposed model achieves the smallest performance degradation across unseen regions, outperforming conventional CV and transformer baselines. These results highlight the robustness of multimodal LLMs under domain shift and their potential for scalable, transferable, and interpretable global PV mapping.', 'abstract_zh': '分布式光伏系统快速扩展给电力网络管理带来了挑战，因为许多安装未被记录。尽管遥感影像提供了全球覆盖，传统计算机视觉模型如CNN和U-Nets需要大量标注数据，并且难以跨区域泛化。本研究探讨了多模态大型语言模型在全局光伏评估中的跨域泛化能力。通过利用结构化提示和微调，模型在统一框架中实现了检测、定位和量化。使用$\\Delta$F1指标进行跨区域评估表明，所提出模型在未见过的区域中表现出了最小的性能退化，优于传统计算机视觉和变压器基线。这些结果强调了多模态LLM在域迁移下的鲁棒性及其在可扩展、转移和可解释的全球光伏映射中的潜力。', 'title_zh': '跨域多模态大型语言模型在光伏全球评估中的泛化能力'}
{'arxiv_id': 'arXiv:2511.19536', 'title': 'AttackPilot: Autonomous Inference Attacks Against ML Services With LLM-Based Agents', 'authors': 'Yixin Wu, Rui Wen, Chi Cui, Michael Backes, Yang Zhang', 'link': 'https://arxiv.org/abs/2511.19536', 'abstract': 'Inference attacks have been widely studied and offer a systematic risk assessment of ML services; however, their implementation and the attack parameters for optimal estimation are challenging for non-experts. The emergence of advanced large language models presents a promising yet largely unexplored opportunity to develop autonomous agents as inference attack experts, helping address this challenge. In this paper, we propose AttackPilot, an autonomous agent capable of independently conducting inference attacks without human intervention. We evaluate it on 20 target services. The evaluation shows that our agent, using GPT-4o, achieves a 100.0% task completion rate and near-expert attack performance, with an average token cost of only $0.627 per run. The agent can also be powered by many other representative LLMs and can adaptively optimize its strategy under service constraints. We further perform trace analysis, demonstrating that design choices, such as a multi-agent framework and task-specific action spaces, effectively mitigate errors such as bad plans, inability to follow instructions, task context loss, and hallucinations. We anticipate that such agents could empower non-expert ML service providers, auditors, or regulators to systematically assess the risks of ML services without requiring deep domain expertise.', 'abstract_zh': '基于自主代理的推理攻击评估：AttackPilot及其应用', 'title_zh': 'AttackPilot: 基于LLM代理的自主推理攻击对抗ML服务'}
{'arxiv_id': 'arXiv:2511.19518', 'title': 'Towards Efficient VLMs: Information-Theoretic Driven Compression via Adaptive Structural Pruning', 'authors': 'Zhaoqi Xu, Yingying Zhang, Jian Li, Jianwei Guo, Qiannan Zhu, Hua Huang', 'link': 'https://arxiv.org/abs/2511.19518', 'abstract': 'Recent advances in vision-language models (VLMs) have shown remarkable performance across multimodal tasks, yet their ever-growing scale poses severe challenges for deployment and efficiency. Existing compression methods often rely on heuristic importance metrics or empirical pruning rules, lacking theoretical guarantees about information preservation. In this work, we propose InfoPrune, an information-theoretic framework for adaptive structural compression of VLMs. Grounded in the Information Bottleneck principle, we formulate pruning as a trade-off between retaining task-relevant semantics and discarding redundant dependencies. To quantify the contribution of each attention head, we introduce an entropy-based effective rank (eRank) and employ the Kolmogorov--Smirnov (KS) distance to measure the divergence between original and compressed structures. This yields a unified criterion that jointly considers structural sparsity and informational efficiency. Building on this foundation, we further design two complementary schemes: (1) a training-based head pruning guided by the proposed information loss objective, and (2) a training-free FFN compression via adaptive low-rank approximation. Extensive experiments on VQAv2, TextVQA, and GQA demonstrate that InfoPrune achieves up to 3.2x FLOP reduction and 1.8x acceleration with negligible performance degradation, establishing a theoretically grounded and practically effective step toward efficient multimodal large models.', 'abstract_zh': 'Recent advances in视觉语言模型(VLMs)已经在多模态任务中展现了显著性能，然而其日益扩大的规模给部署和效率带来了严重挑战。现有的压缩方法通常依赖于启发式重要性度量或经验剪枝规则，缺乏关于信息保留的理论保证。在此工作中，我们提出InfoPrune，这是一种基于信息理论的视觉语言模型自适应结构压缩框架。基于信息瓶颈原理，我们将剪枝视为保留任务相关语义与丢弃冗余依赖之间的权衡。为了量化每个注意力头的贡献，我们引入了一种基于熵的有效秩(eRank)并利用科莫洛哥夫-斯米尔诺夫(KS)距离衡量原始结构和压缩结构之间的偏差。这提供了一个同时考虑结构稀疏性和信息效率的统一标准。在此基础上，我们进一步设计了两种互补方案：(1) 由提出的信失量目标指导的训练基于头剪枝，以及 (2) 无需训练的基于自适应低秩逼近的FFN压缩。在VQAv2、TextVQA和GQA上的广泛实验表明，InfoPrune在几乎不降低性能的情况下实现了最高3.2倍的FLOP减少和1.8倍的加速，为高效的大规模多模态模型奠定了理论和实践有效的基础。', 'title_zh': '基于信息论的自适应结构剪枝驱动的有效VLM压缩'}
{'arxiv_id': 'arXiv:2511.19498', 'title': 'Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data', 'authors': 'Yi Zhang, Tianxiang Xu, Zijian Li, Chao Zhang, Kunyu Zhang, Zhan Gao, Meinuo Li, Xiaohan Zhang, Qichao Qi, Bing Chen', 'link': 'https://arxiv.org/abs/2511.19498', 'abstract': 'Large language models (LLMs) exhibit exceptional performance but pose substantial privacy risks due to training data memorization, particularly within healthcare contexts involving imperfect or privacy-sensitive patient information. We present a hierarchical dual-strategy framework for selective knowledge unlearning that precisely removes specialized knowledge while preserving fundamental medical competencies. Our approach synergistically integrates geometric-constrained gradient updates to selectively modulate target parameters with concept-aware token-level interventions that distinguish between preservation-critical and unlearning-targeted tokens via a unified four-level medical concept hierarchy. Comprehensive evaluations on the MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets demonstrate superior performance, achieving an 82.7% forgetting rate and 88.5% knowledge preservation. Notably, our framework maintains robust privacy guarantees while requiring modification of only 0.1% of parameters, addressing critical needs for regulatory compliance, auditability, and ethical standards in clinical research.', 'abstract_zh': '大型语言模型(Large Language Models)表现出色但存在显著的隐私风险，尤其是由于训练数据的 Memorization 特别是在涉及不完美或隐私敏感的患者信息的医疗情境中。我们提出了一种层次化的双重策略框架，用于选择性地遗忘专业知识，以精确移除专业性知识同时保留基本的医疗技能。该方法通过几何约束梯度更新和概念感知的token级干预协同整合，利用统一的四级医疗概念层次结构区分关键保留和遗忘目标的token。在MedMCQA（外科）和MHQA（焦虑、抑郁、创伤）数据集上的全面评估显示了卓越的性能，实现了82.7%的遗忘率和88.5%的知识保留。值得注意的是，该框架在仅修改0.1%参数的情况下维持了 robust 的隐私保障，解决了临床研究中至关重要的法规合规性、审计性及伦理标准需求。', 'title_zh': '基于不完美和隐私敏感医疗数据的分级双策略遗忘方法在生物医药和医疗智能中的应用'}
{'arxiv_id': 'arXiv:2511.19496', 'title': 'Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM', 'authors': 'Yang Liu, Xiaolong Zhong, Ling Jiang', 'link': 'https://arxiv.org/abs/2511.19496', 'abstract': 'Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present \\textbf{Xmodel-2.5}, a 1.3-billion-parameter small language model designed as a \\emph{drop-in agent core}. Training with maximal-update parameterization ($\\mu$P) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied \\emph{tie-word-embedding} architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that \\textbf{switching from AdamW to Muon during the decay phase} improves the 13-task reasoning average by 4.58\\,\\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.\\footnote{this https URL and this https URL (training checkpoints).} Training code and evaluation harness: this https URL.', 'abstract_zh': '大规模语言模型在推理和工具使用方面表现出色，但其计算需求使其不适合边缘或成本敏感的部署。我们提出了Xmodel-2.5，这是一个由最大更新参数化（$\\mu$P）训练的1.3亿参数的小型语言模型，设计为即插即用代理核心。通过参数绑定的tie-word-embedding架构，超参数可以从2000万参数的代理模型直接转移到完整模型。采用1.4万亿个 token 的升 warmup-稳定-衰减训练课程，并进一步表明，在衰减阶段从 AdamW 切换到 Muon 可以将13项任务的推理平均性能提高4.58%，同时保持其他所有超参数不变，验证了早期 AdamW 的稳定性可以与后期 Muon 的增强相结合以获得更好的下游性能。通过混合精度（FP8）训练平衡准确性和吞吐量。所有检查点、食谱和评估代码均在Apache-2.0许可证下发布。训练代码和评估框架：此链接。', 'title_zh': 'Xmodel-2.5: 1.3B数据高效的推理SLM'}
{'arxiv_id': 'arXiv:2511.19495', 'title': 'A Systematic Study of Compression Ordering for Large Language Models', 'authors': 'Shivansh Chhawri, Rahul Mahadik, Suparna Rooj', 'link': 'https://arxiv.org/abs/2511.19495', 'abstract': 'Large Language Models (LLMs) require substantial computational resources, making model compression essential for efficient deployment in constrained environments. Among the dominant compression techniques: knowledge distillation, structured pruning, and low-bit quantization, their individual effects are well studied, but their interactions and optimal sequencing remain unclear. This work systematically examines how these techniques perform both independently and in combination when applied to the Qwen2.5 3B model. We evaluate multiple compression pipelines, including single, and proposed three-technique sequences, using perplexity, G-Eval, clarity, prompt alignment, and compression ratio as metrics. Our experiments show that quantization provides the greatest standalone compression, while pruning introduces moderate quality degradation. Critically, the ordering of techniques significantly affects the final model quality: the sequence Pruning, Knowledge Distillation, Quantization (P-KD-Q) yields the best balance, achieving a 3.68x compression ratio while preserving strong instruction-following and language understanding capabilities. Conversely, pipelines applying quantization early suffer severe performance degradation due to irreversible information loss that impairs subsequent training. Overall, this study offers practical insight into designing effective, ordering-aware compression pipelines for deploying LLMs in resource-limited settings.', 'abstract_zh': '大型语言模型（LLMs）需要大量的计算资源，因此在受限环境中高效部署时，模型压缩变得至关重要。在这项工作中，我们系统地研究了知识蒸馏、结构剪枝和低比特量化这三种主流压缩技术单独应用和结合应用时在Qwen2.5 3B模型上的性能。我们使用困惑度、G-Eval、清晰度、指令对齐和压缩比等多个指标评估了多个压缩管道，包括单一技术和三种技术的组合序列。实验结果表明，量化提供了最大的单个压缩效果，而剪枝会带来一定程度的质量下降。关键的是，技术的顺序对最终模型质量有显著影响：剪枝、知识蒸馏、量化（P-KD-Q）的序列提供了最佳的平衡，实现了3.68倍的压缩比同时保持强大的指令跟随和语言理解能力。相反，早期应用量化的管道会因不可逆的信息损失而遭受严重的性能下降。总体而言，这项研究为在资源受限环境中设计有效的、顺序感知的压缩管道提供了实用的见解。', 'title_zh': '大规模语言模型的压缩排序系统研究'}
{'arxiv_id': 'arXiv:2511.19489', 'title': 'Evolution without an Oracle: Driving Effective Evolution with LLM Judges', 'authors': 'Zhe Zhao, Yuheng Yang, Haibin Wen, Xiaojie Qiu, Zaixi Zhang, Qingfu Zhang', 'link': 'https://arxiv.org/abs/2511.19489', 'abstract': 'The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through "Problem Specification." By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing "computable metrics" to "describable qualities," thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.', 'abstract_zh': '大规模语言模型与进化计算的集成开辟了科学研究的新前沿，但仍然受限于一个基本约束：对Oracle的依赖——一个客观的、机器可计算的适应度函数。本文突破了这一障碍，探讨：进化能在仅由LLM评审员治理的纯粹主观景观中繁荣吗？我们提出了MADE（多代理分解进化）框架，通过“问题规定”来驯服主观评估固有的噪音。通过将模糊指令分解为具体可验证的子需求，MADE将高变异的LLM反馈转化为稳定、精确的选择压力。结果具有变革性：在DevAI和InfoBench等复杂基准测试中，MADE在软件需求满足方面优于强基线超过50%（从39.9%提高到61.9%），并在复杂指令遵循方面达到了95%的完美通过率。这项工作验证了一个根本性的范式转变：从优化“可计算的指标”转向“可描述的品质”，从而为没有任何 ground truth 的巨大开放领域开启进化优化的可能性。', 'title_zh': '无需 Oracle 的进化：凭借 LLM 判官驱动有效的进化gorithm'}
{'arxiv_id': 'arXiv:2511.19488', 'title': 'Building Resilient Information Ecosystems: Large LLM-Generated Dataset of Persuasion Attacks', 'authors': 'Hsien-Te Kao, Aleksey Panasyuk, Peter Bautista, William Dupree, Gabriel Ganberg, Jeffrey M. Beaubien, Laura Cassani, Svitlana Volkova', 'link': 'https://arxiv.org/abs/2511.19488', 'abstract': "Organization's communication is essential for public trust, but the rise of generative AI models has introduced significant challenges by generating persuasive content that can form competing narratives with official messages from government and commercial organizations at speed and scale. This has left agencies in a reactive position, often unaware of how these models construct their persuasive strategies, making it more difficult to sustain communication effectiveness. In this paper, we introduce a large LLM-generated persuasion attack dataset, which includes 134,136 attacks generated by GPT-4, Gemma 2, and Llama 3.1 on agency news. These attacks span 23 persuasive techniques from SemEval 2023 Task 3, directed toward 972 press releases from ten agencies. The generated attacks come in two mediums, press release statements and social media posts, covering both long-form and short-form communication strategies. We analyzed the moral resonance of these persuasion attacks to understand their attack vectors. GPT-4's attacks mainly focus on Care, with Authority and Loyalty also playing a role. Gemma 2 emphasizes Care and Authority, while Llama 3.1 centers on Loyalty and Care. Analyzing LLM-generated persuasive attacks across models will enable proactive defense, allow to create the reputation armor for organizations, and propel the development of both effective and resilient communications in the information ecosystem.", 'abstract_zh': '组织沟通对于公众信任至关重要，但生成式AI模型的兴起通过快速大规模生成有说服力的内容，形成了与政府部门和商业组织官方信息竞争的叙事，使机构处于被动地位，难以维持沟通有效性。本文介绍了一个大型LLM生成的说服攻击数据集，包括134,136次由GPT-4、Gemma 2和Llama 3.1生成的针对 agencies 新闻的攻击，这些攻击覆盖了来自十个机构的972份新闻稿中的23种说服技术。生成的攻击有两种媒介形式：新闻稿声明和社交媒体帖子，涵盖了长格式和短格式的沟通策略。我们分析了这些说服攻击的道德 resonance，以理解其攻击向量。GPT-4 的攻击主要集中在关心（Care）方面，权威（Authority）和忠诚（Loyalty）也起到一定作用。Gemma 2 强调关心和权威，而 Llama 3.1 则集中在忠诚和关心。跨模型分析LLM生成的说服攻击将有助于主动防御，构建组织声誉盔甲，并推动信息生态系统的有效和稳健沟通的发展。', 'title_zh': '构建 resilent 信息生态系统：大型LLM生成的说服攻击数据集'}
{'arxiv_id': 'arXiv:2511.19486', 'title': 'Efficient Inference Using Large Language Models with Limited Human Data: Fine-Tuning then Rectification', 'authors': 'Lei Wang, Zikun Ye, Jinglong Zhao', 'link': 'https://arxiv.org/abs/2511.19486', 'abstract': 'Driven by recent advances in artificial intelligence (AI), a growing body of work demonstrates the potential of using large language models (LLMs) to generate human-like responses in market research and social science applications. Two primary approaches can be applied to improve the performance of LLMs: fine-tuning, which aligns LLM predictions more closely with human responses, and rectification, which corrects biases in LLM outputs. In this paper, we develop a framework that combines fine-tuning and rectification, and optimally allocates limited labeled samples across the two stages. Unlike the conventional objective that minimizes the mean squared prediction errors, we propose to minimize the variance of the prediction errors as the fine-tuning objective, which is optimal for the downstream rectification stage. Building on this insight, we leverage empirical scaling laws to develop a data-driven method for optimally splitting samples between the fine-tuning and rectification stages. Empirical analysis validates our framework, demonstrating improved estimation and inference performance compared to using either fine-tuning or rectification alone.', 'abstract_zh': '基于近年来人工智能（AI）的进展，越来越多的研究表明，可以通过大型语言模型（LLMs）在市场研究和社会科学应用中生成类人类的响应。有两种主要方法可以应用于提高LLMs的性能：微调，它可以更紧密地使LLMs的预测与人类响应一致；校正，它可以纠正LLMs输出中的偏差。在本文中，我们开发了一个结合微调和校正的框架，并在两个阶段中优化地分配有限的标注样本。不同于传统的最小化预测误差均方差的目标，我们提出以最小化预测误差的方差作为微调目标，这使得后续的校正阶段达到最优。基于这一洞察，我们利用经验标度律开发了一种数据驱动的方法，以最优地拆分样本在微调和校正阶段之间的分配。实证分析验证了我们的框架，证明了与仅使用微调或校正相比，我们的方法在估计和推断性能上的改进。', 'title_zh': '使用有限人力数据的大型语言模型高效推理：微调与校正'}
{'arxiv_id': 'arXiv:2511.19483', 'title': 'Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation', 'authors': 'Qingsong He, Jing Nan, Jiayu Jiao, Liangjie Tang, Xiaodong Xu, Mengmeng Sun, Qingyao Wang, Minghui Yan', 'link': 'https://arxiv.org/abs/2511.19483', 'abstract': "Large Language Models can break through knowledge and timeliness limitations by invoking external tools within the Model Context Protocol framework to achieve automated execution of complex tasks. However, with the rapid growth of enterprise-scale MCP services, efficiently and accurately matching target functionalities among thousands of heterogeneous tools has become a core challenge restricting system practicality. Existing approaches generally rely on full-prompt injection or static semantic retrieval, facing issues including semantic disconnection between user queries and tool descriptions, context inflation in LLM input, and high inference latency. To address these challenges, this paper proposes Z-Space, a data-generation-oriented multi-agent collaborative tool invocation framework Z-Space. The Z-Space framework establishes a multi-agent collaborative architecture and tool filtering algorithm: (1) A structured semantic understanding of user queries is achieved through an intent parsing model; (2) A tool filtering module (FSWW) based on fused subspace weighted algorithm realizes fine-grained semantic alignment between intents and tools without parameter tuning; (3) An inference execution agent is constructed to support dynamic planning and fault-tolerant execution for multi-step tasks. This framework has been deployed in the Eleme platform's technical division, serving large-scale test data generation scenarios across multiple business units including Taotian, Gaode, and Hema. Production data demonstrates that the system reduces average token consumption in tool inference by 96.26\\% while achieving a 92\\% tool invocation accuracy rate, significantly enhancing the efficiency and reliability of intelligent test data generation systems.", 'abstract_zh': '大型语言模型可以通过在模型上下文协议框架内调用外部工具来突破知识和时效限制，实现复杂任务的自动化执行。然而，随着企业级MCP服务的迅猛增长，高效准确地在成千上万种异构工具中匹配目标功能 became a 核心挑战，限制了系统的实用性。现有方法通常依赖于全提示注入或静态语义检索，面临着用户查询与工具描述语义脱节、LLM输入上下文膨胀以及高推理延迟的问题。为应对这些挑战，本文提出了一种以数据生成为导向的多代理协作工具调用框架Z-Space。Z-Space框架建立了一个多代理协作架构和工具筛选算法：（1）通过意图解析模型实现结构化的语义理解用户查询；（2）基于融合子空间加权算法的工具筛选模块（FSWW），实现意图与工具的细粒度语义对齐，无需参数调优；（3）构建推理执行代理，支持多步任务的动态规划和容错执行。该框架已在美团的技术部门部署，服务于包括淘宝、高德和盒马在内的多个业务单元的大规模测试数据生成场景。生产数据表明，该系统在工具推理中的平均词元消耗降低了96.26%，同时实现了92%的工具调用准确率，显著提升了智能测试数据生成系统的效率和可靠性。', 'title_zh': 'Z空间：面向企业级LLM自动化的人工智能代理工具编排框架'}
{'arxiv_id': 'arXiv:2511.19480', 'title': 'Exploiting the Experts: Unauthorized Compression in MoE-LLMs', 'authors': 'Pinaki Prasad Guha Neogi, Ahmad Mohammadshirazi, Dheeraj Kulshrestha, Rajiv Ramnath', 'link': 'https://arxiv.org/abs/2511.19480', 'abstract': 'Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning the remainder, effectively bypassing licensing and security constraints. In this paper, we systematically study the prunability of MoE-LLMs under task-specific usage. We first develop an expert attribution framework that identifies the subset of experts most responsible for a given task, then evaluate the performance trade-offs of pruning and re-aligning these experts using active learning-driven fine-tuning. Our findings reveal a critical knowledge loss--recovery trade-off: while certain experts can be isolated to retain task accuracy, significant degradation occurs without targeted re-alignment. Based on this analysis, we propose defense strategies that aim to make MoE models harder to compress and fine-tune without authorization, including entangled expert training and selective fine-tuning protocols that resist unauthorized adaptation. By positioning expert pruning as both a threat vector and a defense target, this work highlights the dual-use nature of MoE modularity and provides the first systematic evaluation framework for secure specialization of MoE-LLMs.', 'abstract_zh': 'Mixture-of-Experts (MoE) 架构在大型语言模型中的采用因其可扩展性和效率而日益增多。然而，其模块化结构引入了一种特有的脆弱性：攻击者可以通过修剪专家并廉价微调余下部分来尝试压缩或改变模型的功能，从而有效绕过许可和安全约束。在本文中，我们系统研究了特定任务使用下的MoE大型语言模型的可修剪性。我们首先开发了一种专家归属框架，以识别最负责某一任务的专家子集，然后评估通过主动学习驱动的微调重新对齐这些专家的性能权衡。我们的研究发现揭示了一个关键的知识损失-恢复权衡：虽然可以分离某些专家以保持任务准确性，但在缺乏目标重新对齐的情况下，会导致显著的性能下降。基于此分析，我们提出了旨在在未经授权的情况下使MoE模型更难压缩和微调的防御策略，包括缠结专家训练和选择性微调协议，以抵御未经授权的适应。通过将专家修剪定位为一种威胁向量和一种防御目标，本文突显了MoE模块化双重用途的本质，并提供了第一个系统评估框架，用于保障MoE大型语言模型的安全专业化。', 'title_zh': '利用专家力量：MoE-LLMs中的未授权压缩'}
{'arxiv_id': 'arXiv:2511.19473', 'title': 'WavefrontDiffusion: Dynamic Decoding Schedule or Improved Reasoning', 'authors': 'Haojin Yang, Rui Hu, Zequn Sun, Rui Zhou, Yujun Cai, Yiwei Wang', 'link': 'https://arxiv.org/abs/2511.19473', 'abstract': 'Diffusion Language Models (DLMs) have shown strong potential for text generation and are becoming a competitive alternative to autoregressive models. The denoising strategy plays an important role in determining the quality of their outputs. Mainstream denoising strategies include Standard Diffusion and BlockDiffusion. Standard Diffusion performs global denoising without restricting the update range, often finalizing incomplete context and causing premature end-of-sequence predictions. BlockDiffusion updates fixed-size blocks in a preset order, but its rigid structure can break apart coherent semantic units and disrupt reasoning. We present WavefrontDiffusion, a dynamic decoding approach that expands a wavefront of active tokens outward from finalized positions. This adaptive process follows the natural flow of semantic structure while keeping computational cost equal to block-based methods. Across four benchmarks in reasoning and code generation, WavefrontDiffusion achieves state-of-the-art performance while producing outputs with higher semantic fidelity, showing the value of adaptive scheduling for more coherent and efficient generation.', 'abstract_zh': '波前扩散语言模型（WavefrontDiffusion）：一种动态解码方法及其在推理和代码生成中的应用', 'title_zh': '波前扩散：动态解码调度或增强推理'}
