# Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents 

**Title (ZH)**: 开放CAPTCHA世界：一个全面的基于Web的平台，用于测试和基准测试多模态LLM代理 

**Authors**: Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen  

**Link**: [PDF](https://arxiv.org/pdf/2505.24878)  

**Abstract**: CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL. 

**Abstract (ZH)**: Open CaptchaWorld: 一个用于评估以大规模语言模型为基础的智能代理的视觉推理和交互能力的基准平台 

---
# MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning 

**Title (ZH)**: MiCRo: 混合建模与上下文感知路由的个性化偏好学习 

**Authors**: Jingyan Shen, Jiarui Yao, Rui Yang, Yifan Sun, Feng Luo, Rui Pan, Tong Zhang, Han Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2505.24846)  

**Abstract**: Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization. 

**Abstract (ZH)**: 基于布雷德利-特里模型的奖励建模是将人类反馈强化学习（RLHF）应用于大型语言模型（LLMs）以构建安全基础模型中的关键步骤。然而，基于布雷德利-特里模型的奖励建模假设了全局奖励函数，未能捕捉到人类偏好内部的多样性和异质性。因此，这种简化限制了LLMs在支持个性化和多元性对齐方面的能力。理论上，我们证明当人类偏好遵循多种亚群的混合分布时，单个布雷德利-特里模型具有不可约错误。虽然现有的解决方案，如细粒度标注的多目标学习，有助于解决这个问题，但它们成本高昂且受限于预定义属性，未能充分捕捉人类价值观的丰富性。在本工作中，我们提出了一种两阶段框架MiCRo，该框架通过大规模二元偏好数据集增强个性化偏好学习，无需明确的细粒度标注。在第一阶段，MiCRo引入上下文感知混合建模方法以捕捉多样的人类偏好。在第二阶段，MiCRo集成了一个在线路由策略，该策略根据特定上下文动态调整混合权重以解决歧义，允许在最少额外监督的情况下实现高效且可扩展的偏好适应。在多个偏好数据集上的实验表明，MiCRo有效地捕捉到多样化的人类偏好，并显著提高了下游个性化的效果。 

---
# Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success 

**Title (ZH)**: 随机规则森林 (RRF): 生成式大语言模型问题的可解释ensemble方法及其在预测创业成功中的应用 

**Authors**: Ben Griffin, Joseph Ternasky, Fuat Alican, Yigit Ihlamur  

**Link**: [PDF](https://arxiv.org/pdf/2505.24622)  

**Abstract**: Predicting startup success requires models that are both accurate and interpretable. We present a lightweight ensemble framework that combines YES/NO questions generated by large language models (LLMs), forming a transparent decision-making system. Each question acts as a weak heuristic, and by filtering, ranking, and aggregating them through a threshold-based voting mechanism, we construct a strong ensemble predictor. On a test set where 10% of startups are classified as successful, our approach achieves a precision rate of 50%, representing a 5x improvement over random selection, while remaining fully transparent. When we incorporate expert-guided heuristics into the generation process, performance improves further to 54% precision. These results highlight the value of combining LLM reasoning with human insight and demonstrate that simple, interpretable ensembles can support high-stakes decisions in domains such as venture capital (VC). 

**Abstract (ZH)**: 预测初创公司成功需要既准确又可解释的模型。我们提出了一种轻量级集成框架，该框架结合了由大规模语言模型（LLMs）生成的YES/NO问题，形成一个透明的决策系统。每个问题作为一种弱启发式，通过基于阈值的投票机制进行过滤、排序和聚合，构建一个强集成预测器。在测试集中，有10%的初创公司被分类为成功，我们的方法实现了50%的精确率，相比随机选择提高了5倍，同时保持完全透明。当我们将专家指导的启发式纳入生成过程时，精确率进一步提高到54%。这些结果凸显了将LLM推理与人力洞察相结合的价值，并展示了简单的可解释集成可以支持如风险投资（VC）等领域的重要决策。 

---
# Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction 

**Title (ZH)**: 专家混合模型为个性化和语义aware的下一个位置预测 

**Authors**: Shuai Liu, Ning Cao, Yile Chen, Yue Jiang, Gao Cong  

**Link**: [PDF](https://arxiv.org/pdf/2505.24597)  

**Abstract**: Next location prediction plays a critical role in understanding human mobility patterns. However, existing approaches face two core limitations: (1) they fall short in capturing the complex, multi-functional semantics of real-world locations; and (2) they lack the capacity to model heterogeneous behavioral dynamics across diverse user groups. To tackle these challenges, we introduce NextLocMoE, a novel framework built upon large language models (LLMs) and structured around a dual-level Mixture-of-Experts (MoE) design. Our architecture comprises two specialized modules: a Location Semantics MoE that operates at the embedding level to encode rich functional semantics of locations, and a Personalized MoE embedded within the Transformer backbone to dynamically adapt to individual user mobility patterns. In addition, we incorporate a history-aware routing mechanism that leverages long-term trajectory data to enhance expert selection and ensure prediction stability. Empirical evaluations across several real-world urban datasets show that NextLocMoE achieves superior performance in terms of predictive accuracy, cross-domain generalization, and interpretability 

**Abstract (ZH)**: NextLocMoE：基于大语言模型和双层Mixture-of-Experts的设计以预测下一个位置 

---
# MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge 

**Title (ZH)**: MELT: 通过利用嵌入的LLM知识朝着自动化多模态情感数据标注的方向努力 

**Authors**: Xin Jing, Jiadong Wang, Iosif Tsangko, Andreas Triantafyllopoulos, Björn W. Schuller  

**Link**: [PDF](https://arxiv.org/pdf/2505.24493)  

**Abstract**: Although speech emotion recognition (SER) has advanced significantly with deep learning, annotation remains a major hurdle. Human annotation is not only costly but also subject to inconsistencies annotators often have different preferences and may lack the necessary contextual knowledge, which can lead to varied and inaccurate labels. Meanwhile, Large Language Models (LLMs) have emerged as a scalable alternative for annotating text data. However, the potential of LLMs to perform emotional speech data annotation without human supervision has yet to be thoroughly investigated. To address these problems, we apply GPT-4o to annotate a multimodal dataset collected from the sitcom Friends, using only textual cues as inputs. By crafting structured text prompts, our methodology capitalizes on the knowledge GPT-4o has accumulated during its training, showcasing that it can generate accurate and contextually relevant annotations without direct access to multimodal inputs. Therefore, we propose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We demonstrate the effectiveness of MELT by fine-tuning four self-supervised learning (SSL) backbones and assessing speech emotion recognition performance across emotion datasets. Additionally, our subjective experiments\' results demonstrate a consistence performance improvement on SER. 

**Abstract (ZH)**: 虽然深度学习推动了语音情感识别（SER）的显著进步，但标注仍然是一个主要障碍。人力标注不仅成本高昂，而且标注者之间存在不一致性，往往有不同的偏好并可能缺乏必要的上下文知识，这可能导致标注结果的差异性和不准确性。与此同时，大规模语言模型（LLMs）已经成为了标注文本数据的一种可扩展替代方案。然而，LLMs在无需人类监督的情况下对情绪语音数据进行标注的潜力尚未得到充分探索。为了解决这些问题，我们应用GPT-4o对源自 situation comedy《老友记》的多模态数据集进行标注，仅使用文本暗示作为输入。通过构建结构化的文本提示，我们的方法利用了GPT-4o在训练过程中积累的知识，展示了它在无需直接访问多模态输入的情况下生成准确且上下文相关的标注的能力。因此，我们提出了MELT，一个完全由GPT-4o标注的多模态情感数据集。我们通过微调四种自我监督学习（SSL）骨干模型并评估不同情感数据集的语音情感识别性能，展示了MELT的有效性。此外，我们的主观实验结果显示，MELT在语音情感识别（SER）上具有持续的性能改进。 

---
# Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation 

**Title (ZH)**: Leveraging知识图谱和大规模语言模型进行误导性信息的结构化生成 

**Authors**: Sania Nayab, Marco Simoni, Giulio Rossolini  

**Link**: [PDF](https://arxiv.org/pdf/2505.24479)  

**Abstract**: The rapid spread of misinformation, further amplified by recent advances in generative AI, poses significant threats to society, impacting public opinion, democratic stability, and national security. Understanding and proactively assessing these threats requires exploring methodologies that enable structured and scalable misinformation generation. In this paper, we propose a novel approach that leverages knowledge graphs (KGs) as structured semantic resources to systematically generate fake triplets. By analyzing the structural properties of KGs, such as the distance between entities and their predicates, we identify plausibly false relationships. These triplets are then used to guide large language models (LLMs) in generating misinformation statements with varying degrees of credibility. By utilizing structured semantic relationships, our deterministic approach produces misinformation inherently challenging for humans to detect, drawing exclusively upon publicly available KGs (e.g., WikiGraphs).
Additionally, we investigate the effectiveness of LLMs in distinguishing between genuine and artificially generated misinformation. Our analysis highlights significant limitations in current LLM-based detection methods, underscoring the necessity for enhanced detection strategies and a deeper exploration of inherent biases in generative models. 

**Abstract (ZH)**: 近期生成AI技术的快速发展加剧了 misinformation 的传播，这对社会构成了重大威胁，影响公众意见、民主稳定和国家安全。理解并主动评估这些威胁需要探索能够实现结构化和可扩展的 misinformation 生成方法。本文提出了一种新的方法，利用知识图谱（KGs）作为结构化的语义资源，系统地生成虚假三元组。通过分析KGs的结构特性，如实体间的距离及其谓词，我们识别出可能虚假的关系。这些三元组随后用于指导大型语言模型（LLMs）生成具有不同可信度的虚假陈述。通过利用结构化的语义关系，我们的确定性方法生成的 misinformation 对人类而言具有难以检测的特性，仅依赖于公开可用的KGs（如WikiGraphs）。此外，我们探讨了LLMs在区分真实和人工生成的 misinformation 方面的有效性。我们的分析指出现有基于LLM的检测方法存在显著局限性，强调了改进检测策略并深入探索生成模型内在偏差的必要性。 

---
# Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning 

**Title (ZH)**: 知识图谱与大型语言模型之间接口的优化以支持复杂推理 

**Authors**: Vasilije Markovic, Lazar Obradovic, Laszlo Hajdu, Jovan Pavlovic  

**Link**: [PDF](https://arxiv.org/pdf/2505.24478)  

**Abstract**: Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results in complex systems with numerous hyperparameters that directly affect performance. While such systems are increasingly common in retrieval-augmented generation, the role of systematic hyperparameter optimization remains underexplored. In this paper, we study this problem in the context of Cognee, a modular framework for end-to-end KG construction and retrieval. Using three multi-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize parameters related to chunking, graph construction, retrieval, and prompting. Each configuration is scored using established metrics (exact match, F1, and DeepEval's LLM-based correctness metric). Our results demonstrate that meaningful gains can be achieved through targeted tuning. While the gains are consistent, they are not uniform, with performance varying across datasets and metrics. This variability highlights both the value of tuning and the limitations of standard evaluation measures. While demonstrating the immediate potential of hyperparameter tuning, we argue that future progress will depend not only on architectural advances but also on clearer frameworks for optimization and evaluation in complex, modular systems. 

**Abstract (ZH)**: 将大型语言模型（LLMs）与知识图谱（KGs）集成产生了具有众多超参数的复杂系统，这些超参数直接影响性能。尽管此类系统在检索增强生成中越来越常见，但系统的系统性超参数优化的作用仍较少被探索。在本文中，我们以Cognee这一模块化框架为研究背景，探讨了知识图谱构建和检索的端到端框架中超参数优化的问题。我们使用三个多跳问答基准（HotPotQA、TwoWikiMultiHop和MuSiQue）来优化与切分、图构建、检索和提示相关的参数。每个配置使用已建立的评价指标（精确匹配、F1和DeepEval的LLM基于正确性指标）进行评分。我们的结果显示，通过有针对性的调优可以获得有意义的性能提升。虽然这些提升是稳定的，但并不是均匀的，性能在不同数据集和评价指标上存在差异。这种差异性突显了调优的价值以及标准评价措施的局限性。本文展示了超参数调优的即时潜力，但我们认为未来的发展不仅依赖于架构的进步，还需要在复杂的模块化系统中建立更清晰的优化和评价框架。 

---
# RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation 

**Title (ZH)**: RMoA：通过多样性和残差补偿优化混合智能体 

**Authors**: Zhentao Xie, Chengcheng Han, Jinxin Shi, Wenjun Cui, Xin Zhao, Xingjiao Wu, Jiabao Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2505.24442)  

**Abstract**: Although multi-agent systems based on large language models show strong capabilities on multiple tasks, they are still limited by high computational overhead, information loss, and robustness. Inspired by ResNet's residual learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual connections to optimize efficiency and reliability. To maximize information utilization from model responses while minimizing computational costs, we innovatively design an embedding-based diversity selection mechanism that greedily selects responses via vector similarity. Furthermore, to mitigate iterative information degradation, we introduce a Residual Extraction Agent to preserve cross-layer incremental information by capturing inter-layer response differences, coupled with a Residual Aggregation Agent for hierarchical information integration. Additionally, we propose an adaptive termination mechanism that dynamically halts processing based on residual convergence, further improving inference efficiency. RMoA achieves state-of-the-art performance on the benchmarks of across alignment, mathematical reasoning, code generation, and multitasking understanding, while significantly reducing computational overhead. Code is available at this https URL. 

**Abstract (ZH)**: 基于大语言模型的多代理系统尽管在多项任务中展示了强大的能力，但仍受限于高计算开销、信息损失和鲁棒性问题。受ResNet中残差学习的启发，我们提出了一种残差混合代理（RMoA）模型，通过引入残差连接来优化效率和可靠性。为了最大限度地利用模型响应中的信息同时最小化计算成本，我们创新性地设计了一种基于嵌入的多样性选择机制，通过向量相似性贪婪地选择响应。为进一步减轻迭代信息退化的问题，我们引入了一种残差提取代理，通过捕捉层间响应差异来保存跨层的增量信息，并结合一种残差聚合代理以实现层级信息整合。此外，我们提出了一种自适应终止机制，根据残差收敛情况动态停止处理过程，进一步提高推理效率。RMoA在跨对齐、数学推理、代码生成和多任务理解的基准测试中取得了前沿性能，同时显著减少了计算开销。代码见此链接。 

---
# GridRoute: A Benchmark for LLM-Based Route Planning with Cardinal Movement in Grid Environments 

**Title (ZH)**: GridRoute：基于格网运动的LLM路由规划基准 

**Authors**: Kechen Li, Yaotian Tao, Ximing Wen, Quanwei Sun, Zifei Gong, Chang Xu, Xizhe Zhang, Tianbo Ji  

**Link**: [PDF](https://arxiv.org/pdf/2505.24306)  

**Abstract**: Recent advancements in Large Language Models (LLMs) have demonstrated their potential in planning and reasoning tasks, offering a flexible alternative to classical pathfinding algorithms. However, most existing studies focus on LLMs' independent reasoning capabilities and overlook the potential synergy between LLMs and traditional algorithms. To fill this gap, we propose a comprehensive evaluation benchmark GridRoute to assess how LLMs can take advantage of traditional algorithms. We also propose a novel hybrid prompting technique called Algorithm of Thought (AoT), which introduces traditional algorithms' guidance into prompting. Our benchmark evaluates six LLMs ranging from 7B to 72B parameters across various map sizes, assessing their performance in correctness, optimality, and efficiency in grid environments with varying sizes. Our results show that AoT significantly boosts performance across all model sizes, particularly in larger or more complex environments, suggesting a promising approach to addressing path planning challenges. Our code is open-sourced at this https URL. 

**Abstract (ZH)**: Recent advancements in大型语言模型（LLMs）在路径规划任务中的潜力体现：GridRoute基准评估LLMs与传统算法的协同效应及一种新的混合提示技术（AoT） 

---
# Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules 

**Title (ZH)**: 留意引用：通过插件式模块在大模型中实现引用意识对话 

**Authors**: Yueqi Zhang, Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.24292)  

**Abstract**: Human-AI conversation frequently relies on quoting earlier text-"check it with the formula I just highlighted"-yet today's large language models (LLMs) lack an explicit mechanism for locating and exploiting such spans. We formalise the challenge as span-conditioned generation, decomposing each turn into the dialogue history, a set of token-offset quotation spans, and an intent utterance. Building on this abstraction, we introduce a quotation-centric data pipeline that automatically synthesises task-specific dialogues, verifies answer correctness through multi-stage consistency checks, and yields both a heterogeneous training corpus and the first benchmark covering five representative scenarios. To meet the benchmark's zero-overhead and parameter-efficiency requirements, we propose QuAda, a lightweight training-based method that attaches two bottleneck projections to every attention head, dynamically amplifying or suppressing attention to quoted spans at inference time while leaving the prompt unchanged and updating < 2.8% of backbone weights. Experiments across models show that QuAda is suitable for all scenarios and generalises to unseen topics, offering an effective, plug-and-play solution for quotation-aware dialogue. 

**Abstract (ZH)**: 人类与AI对话经常需要引用之前的文本——“请检查我刚刚强调的公式”——但当前的大规模语言模型（LLMs）缺乏明确机制来定位和利用这些片段。我们将这一挑战形式化为基于片段生成，将每次对话分解为对话历史、一组标记偏移引用片段以及意图陈述。在此抽象基础上，我们引入了一种以引用为中心的数据管道，可以自动合成特定任务的对话，通过多阶段一致性检查验证答案的正确性，并产生一个异质训练语料库和覆盖五个代表性场景的第一套基准测试。为了满足基准测试的零开销和参数效率要求，我们提出了QuAda，一种轻量级的基于训练的方法，为每个注意力头添加了两个瓶颈投影，在推理时动态放大或抑制对引用片段的注意力，而不改变提示，并更新少于2.8%的骨干权重。实验表明，QuAda适用于所有场景，并可以迁移到未见过的主题，提供了一种有效且即插即用的引用意识对话解决方案。 

---
# How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning 

**Title (ZH)**: 多少回溯足够？探索SFT和RL在增强LLM推理中的相互作用 

**Authors**: Hongyi James Cai, Junlin Wang, Xiaoyin Chen, Bhuwan Dhingra  

**Link**: [PDF](https://arxiv.org/pdf/2505.24273)  

**Abstract**: Recent breakthroughs in large language models (LLMs) have effectively improved their reasoning abilities, particularly on mathematical and logical problems that have verifiable answers, through techniques such as supervised finetuning (SFT) and reinforcement learning (RL). Prior research indicates that RL effectively internalizes search strategies, enabling long chain-of-thought (CoT) reasoning, with backtracking emerging naturally as a learned capability. However, the precise benefits of backtracking, specifically, how significantly it contributes to reasoning improvements and the optimal extent of its use, remain poorly understood. In this work, we systematically investigate the dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self Reference. Our findings highlight that short CoT sequences used in SFT as a warm-up do have moderate contribution to RL training, compared with cold-start RL; however such contribution diminishes when tasks become increasingly difficult. Motivated by this observation, we construct synthetic datasets varying systematically in the number of backtracking steps and conduct controlled experiments to isolate the influence of either the correctness (content) or the structure (i.e., backtrack frequency). We find that (1) longer CoT with backtracks generally induce better and more stable RL training, (2) more challenging problems with larger search space tend to need higher numbers of backtracks during the SFT stage. Additionally, we demonstrate through experiments on distilled data that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Collectively, our results offer practical insights into designing optimal training strategies to effectively scale reasoning in LLMs. 

**Abstract (ZH)**: Recent突破在大型语言模型（LLMs）中的近期突破有效提高了它们在数学和逻辑问题上的推理能力，这些问题具有可验证的答案，通过监督微调（SFT）和强化学习（RL）等技术。先前的研究表明，RL能够有效内化搜索策略，使长链推理（CoT）成为自然学习的能力，其中回溯作为一种学习能力自然出现。然而，回溯的具体益处，特别是它对推理改进的贡献程度及其最佳使用程度，仍然 poorly understood. 在这项工作中，我们系统地研究了八项推理任务（Countdown、Sudoku、Arc 1D、Geometry、Color Cube Rotation、List Functions、Zebra Puzzles 和 Self Reference）中SFT和RL之间的动态关系。我们的研究发现表明，在SFT作为预热使用的短CoT序列在RL训练中确实有一定的贡献，与冷启动的RL相比；然而，当任务变得越来越困难时，这种贡献会逐渐减弱。基于这一观察，我们构建了系统变化回溯步数的合成数据集，并进行控制实验以分离内容（即正确性）或结构（即回溯频率）的影响。我们发现（1）带有回溯的较长CoT通常会导致更好的和更稳定的RL训练；（2）更具挑战性、搜索空间更大的问题在SFT阶段通常需要更多的回溯。另外，我们通过蒸馏数据的实验表明，长CoT序列的正确性对RL训练的影响很小，表明RL更侧重于结构模式而非内容正确性。总体而言，我们的结果为设计有效扩展LLMs推理的训练策略提供了实际见解。 

---
# FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation 

**Title (ZH)**: FABLE：一种新型过程性文本数据流分析基准，用于大型语言模型评估 

**Authors**: Vishal Pallagani, Nitin Gupta, John Aydin, Biplav Srivastava  

**Link**: [PDF](https://arxiv.org/pdf/2505.24258)  

**Abstract**: Understanding how data moves, transforms, and persists, known as data flow, is fundamental to reasoning in procedural tasks. Despite their fluency in natural and programming languages, large language models (LLMs), although increasingly being applied to decisions with procedural tasks, have not been systematically evaluated for their ability to perform data-flow reasoning. We introduce FABLE, an extensible benchmark designed to assess LLMs' understanding of data flow using structured, procedural text. FABLE adapts eight classical data-flow analyses from software engineering: reaching definitions, very busy expressions, available expressions, live variable analysis, interval analysis, type-state analysis, taint analysis, and concurrency analysis. These analyses are instantiated across three real-world domains: cooking recipes, travel routes, and automated plans. The benchmark includes 2,400 question-answer pairs, with 100 examples for each domain-analysis combination. We evaluate three types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a general-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code 8B). Each model is tested using majority voting over five sampled completions per prompt. Results show that the reasoning model achieves higher accuracy, but at the cost of over 20 times slower inference compared to the other models. In contrast, the general-purpose and code-specific models perform close to random chance. FABLE provides the first diagnostic benchmark to systematically evaluate data-flow reasoning and offers insights for developing models with stronger procedural understanding. 

**Abstract (ZH)**: 理解数据如何流动、变换和持久化，即数据流，是进行程序任务推理的基础。尽管大型语言模型（LLMs）在自然语言和编程语言方面表现出色，并且越来越多地应用于程序任务的决策中，但它们在执行数据流推理方面的能力尚未进行系统的评估。我们引入了FABLE，一个可扩展的基准测试，旨在使用结构化程序语言评估LLMs对数据流的理解。FABLE采用了软件工程中的八种经典数据流分析方法：可达定义、非常活跃的表达式、可用表达式、活动变量分析、区间分析、类型状态分析、污染分析和并发分析。这些分析方法被应用在三个实际领域中：烹饪食谱、旅行路线和自动化计划中。该基准测试包括2400个问答对，每个领域-分析组合有100个示例。我们评估了三种类型的LLMs：一个侧重推理的模型（DeepSeek-R1 8B）、一个通用模型（LLaMA 3.1 8B）和一个代码特定模型（Granite Code 8B）。每个模型都在五个采样完成中使用多数投票进行测试。结果表明，侧重推理的模型在准确率上更高，但推断速度比其他模型慢20多倍。相比之下，通用模型和代码特定模型的表现接近随机猜测。FABLE提供了一个诊断基准，首次系统性地评估数据流推理，并为开发具有更强程序理解能力的模型提供了见解。 

---
# Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap 

**Title (ZH)**: 通过减少预训练模态差距来提升大模型鲁棒性以保障多模态模型安全 

**Authors**: Wenhan Yang, Spencer Stice, Ali Payani, Baharan Mirzasoleiman  

**Link**: [PDF](https://arxiv.org/pdf/2505.24208)  

**Abstract**: Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for their reliable deployment. However, LVLMs suffer from drastic safety degradation compared to their LLM backbone. Even blank or irrelevant images can trigger LVLMs to generate harmful responses to prompts that would otherwise be refused in text-only contexts. The modality gap between image and text representations has been recently hypothesized to contribute to safety degradation of LVLMs. However, if and how the amount of modality gap affects LVLMs' safety is not studied. In this work, we show that the amount of modality gap is highly inversely correlated with VLMs' safety. Then, we show that this modality gap is introduced during pretraining LVLMs and persists through fine-tuning. Inspired by this observation, we propose a regularization to reduce the modality gap during pretraining. Our extensive experiments on LLaVA v1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves safety alignment of LVLMs, reducing unsafe rate by up to 16.3% without compromising performance, and can further boost existing defenses by up to 18.2%. 

**Abstract (ZH)**: 确保视觉-语言模型生成安全输出对于其可靠部署至关重要。然而，LVLMs在安全性上大幅恶化，相较于其LLM主干。即使是空白或无关的图片也能使LVLMs生成有害的响应，而在仅文本情境下这些响应原本会被拒绝。图像和文本表示之间的模态差距最近被假设为导致LVLMs安全性下降的因素之一。然而，模态差距的数量如何影响LVLMs的安全性尚未被研究。在本文中，我们展示了模态差距的数量与VLMs的安全性之间存在高度的负相关。然后，我们表明这种模态差距是在预训练LVLMs过程中引入的，并且在微调过程中持续存在。受此观察的启发，我们提出了一种正则化方法来减少预训练过程中的模态差距。我们在LLaVA v1.5、ShareGPT4V和MiniGPT-4上的广泛实验表明，我们的方法显著改善了LVLMs的安全性对齐，将不安全率降低了最多16.3%而不会牺牲性能，并且可以进一步增强现有的防御措施最多18.2%。 

---
# SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems 

**Title (ZH)**: SentinelAgent：多智能体系统中的基于图的异常检测 

**Authors**: Xu He, Di Wu, Yan Zhai, Kun Sun  

**Link**: [PDF](https://arxiv.org/pdf/2505.24201)  

**Abstract**: The rise of large language model (LLM)-based multi-agent systems (MAS) introduces new security and reliability challenges. While these systems show great promise in decomposing and coordinating complex tasks, they also face multi-faceted risks across prompt manipulation, unsafe tool usage, and emergent agent miscoordination. Existing guardrail mechanisms offer only partial protection, primarily at the input-output level, and fall short in addressing systemic or multi-point failures in MAS. In this work, we present a system-level anomaly detection framework tailored for MAS, integrating structural modeling with runtime behavioral oversight. Our approach consists of two components. First, we propose a graph-based framework that models agent interactions as dynamic execution graphs, enabling semantic anomaly detection at node, edge, and path levels. Second, we introduce a pluggable SentinelAgent, an LLM-powered oversight agent that observes, analyzes, and intervenes in MAS execution based on security policies and contextual reasoning. By bridging abstract detection logic with actionable enforcement, our method detects not only single-point faults and prompt injections but also multi-agent collusion and latent exploit paths. We validate our framework through two case studies, including an email assistant and Microsoft's Magentic-One system, demonstrating its ability to detect covert risks and provide explainable root-cause attribution. Our work lays the foundation for more trustworthy, monitorable, and secure agent-based AI ecosystems. 

**Abstract (ZH)**: 基于大型语言模型（LLM）的多代理系统（MAS）兴起引入了新的安全性和可靠性挑战。尽管这些系统在分解和协调复杂任务方面展现出巨大潜力，但也面临着包括提示操纵、不安全工具使用以及代理 emergent 不安全协调等多方面的风险。现有的护栏机制仅在输入输出层面提供部分保护，并无法有效应对MAS中的系统性或多点故障。在本工作中，我们提出了一种针对MAS的系统级异常检测框架，结合结构建模和运行时行为监督。我们的方法包含两个组件。首先，我们提出了一种基于图的框架，将代理交互建模为动态执行图，实现节点、边和路径级别的语义异常检测。其次，我们引入了一个可插拔的SentinelAgent，这是一个由LLM驱动的监督代理，基于安全策略和情境推理，在MAS执行过程中进行观察、分析和干预。通过将抽象的检测逻辑与可执行的强制措施相结合，我们的方法不仅能检测单点故障和提示注入，还能检测多代理共谋和潜在利用路径。我们通过两个案例研究，包括邮件助手和微软的Magentic-One系统，验证了该框架能够检测隐蔽风险，并提供可解释的根本原因归因。我们的工作为基础更值得信赖、可监控和安全的基于代理的AI生态系统奠定了基础。 

---
# Learning API Functionality from Demonstrations for Tool-based Agents 

**Title (ZH)**: 基于演示学习工具代理的功能性 

**Authors**: Bhrij Patel, Ashish Jagmohan, Aditya Vempaty  

**Link**: [PDF](https://arxiv.org/pdf/2505.24197)  

**Abstract**: Digital tool-based agents that invoke external Application Programming Interfaces (APIs) often rely on documentation to understand API functionality. However, such documentation is frequently missing, outdated, privatized, or inconsistent-hindering the development of reliable, general-purpose agents. In this work, we propose learning API functionality directly from demonstrations as a new paradigm applicable in scenarios without documentation. Using existing API benchmarks, we collect demonstrations from both expert API-based agents and from self-exploration. To understand what information demonstrations must convey for successful task completion, we extensively study how the number of demonstrations and the use of LLM-generated summaries and evaluations affect the task success rate of the API-based agent. Our experiments across 3 datasets and 5 models show that learning functionality from demonstrations remains a non-trivial challenge, even for state-of-the-art LLMs. We find that providing explicit function calls and natural language critiques significantly improves the agent's task success rate due to more accurate parameter filling. We analyze failure modes, identify sources of error, and highlight key open challenges for future work in documentation-free, self-improving, API-based agents. 

**Abstract (ZH)**: 基于数字工具的代理通过调用外部应用编程接口（API）来实现功能，通常依赖于文档。然而，此类文档经常缺失、过时、私有化或不一致，这阻碍了可靠、通用代理的开发。在本项工作中，我们提出了一种新的 paradigmn，即直接从演示学习 API 功能，适用于无文档场景。利用现有的 API 基准，我们从专家API基础上的代理和自我探索中收集演示数据。为了了解演示必须传达哪些信息才能成功完成任务，我们广泛研究了演示的数量以及LLM生成的摘要和评估如何影响基于API的代理的任务成功率。在3个数据集和5个模型上的实验表明，即使对于最先进的LLM，从演示中学习功能仍是一项非平凡的挑战。我们发现，提供显式的函数调用和自然语言批评显著提高了代理的任务成功率，因为这有助于更准确地填充参数。我们分析了失败模式，确定了错误的来源，并指出了未来无文档、自我改进的API基础代理的主要开放挑战。 

---
# SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought 

**Title (ZH)**: SCOUT: 教学预训练语言模型通过思维流程增强推理能力 

**Authors**: Guanghao Li, Wenhao Jiang, Mingfeng Chen, Yan Li, Hao Yu, Shuting Dong, Tao Ren, Ming Tang, Chun Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2505.24181)  

**Abstract**: Chain of Thought (CoT) prompting improves the reasoning performance of large language models (LLMs) by encouraging step by step thinking. However, CoT-based methods depend on intermediate reasoning steps, which limits scalability and generalization. Recent work explores recursive reasoning, where LLMs reuse internal layers across iterations to refine latent representations without explicit CoT supervision. While promising, these approaches often require costly pretraining and lack a principled framework for how reasoning should evolve across iterations. We address this gap by introducing Flow Chain of Thought (Flow CoT), a reasoning paradigm that models recursive inference as a progressive trajectory of latent cognitive states. Flow CoT frames each iteration as a distinct cognitive stage deepening reasoning across iterations without relying on manual supervision. To realize this, we propose SCOUT (Stepwise Cognitive Optimization Using Teachers), a lightweight fine tuning framework that enables Flow CoT style reasoning without the need for pretraining. SCOUT uses progressive distillation to align each iteration with a teacher of appropriate capacity, and a cross attention based retrospective module that integrates outputs from previous iterations while preserving the models original computation flow. Experiments across eight reasoning benchmarks show that SCOUT consistently improves both accuracy and explanation quality, achieving up to 1.8% gains under fine tuning. Qualitative analyses further reveal that SCOUT enables progressively deeper reasoning across iterations refining both belief formation and explanation granularity. These results not only validate the effectiveness of SCOUT, but also demonstrate the practical viability of Flow CoT as a scalable framework for enhancing reasoning in LLMs. 

**Abstract (ZH)**: Flow Chain of Thought (Flow CoT) 提高了大语言模型 (LLMs) 的推理性能，通过鼓励逐步思考。然而，基于 CoT 的方法依赖于中间推理步骤，这限制了其可扩展性和泛化能力。最近的研究探索了递归推理，其中 LLGs 在迭代中重新利用内部层以精炼潜在表示，而无需显式的 CoT 监督。尽管前景广阔，这些方法通常需要昂贵的预训练，并缺乏推理如何在迭代中逐步进化的原则性框架。我们通过引入 Flow Chain of Thought (Flow CoT)，一种将递归推理建模为潜在认知状态的逐步轨迹的推理范式来弥补这一缺口。Flow CoT 将每个迭代视为一个独特的认知阶段，无需依赖手动监督即可跨迭代加深推理。为了实现这一点，我们提出了 SCOUT（渐进认知优化教师使用），一种轻量级微调框架，使 Flow CoT 样式的推理无需预训练即可实现。SCOUT 使用渐进式蒸馏将每个迭代与相应能力的教师对齐，并使用跨注意力基于的反思模块整合先前迭代的输出，同时保留模型的原始计算流程。在八个推理基准上的实验表明，SCOUT 一致地提高了准确性和解释质量，在微调时最高可达 1.8% 的提升。定性分析进一步表明，SCOUT 使推理在每个迭代中逐步加深，改进了信念形成和解释粒度。这些结果不仅验证了 SCOUT 的有效性，还证明了 Flow CoT 作为增强 LLMs 推理的可扩展框架的实用性。 

---
# Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution 

**Title (ZH)**: 交给专家来处理：通过稀疏进化进行稀疏大型语言模型的修复与微调 

**Authors**: Qiao Xiao, Alan Ansell, Boqian Wu, Lu Yin, Mykola Pechenizkiy, Shiwei Liu, Decebal Constantin Mocanu  

**Link**: [PDF](https://arxiv.org/pdf/2505.24037)  

**Abstract**: Large language models (LLMs) have achieved remarkable success across various tasks but face deployment challenges due to their massive computational demands. While post-training pruning methods like SparseGPT and Wanda can effectively reduce the model size, but struggle to maintain model performance at high sparsity levels, limiting their utility for downstream tasks. Existing fine-tuning methods, such as full fine-tuning and LoRA, fail to preserve sparsity as they require updating the whole dense metrics, not well-suited for sparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a novel method designed specifically for sparse LLMs. SEFT dynamically evolves the sparse topology of pruned models during fine-tuning, while preserving the overall sparsity throughout the process. The strengths of SEFT lie in its ability to perform task-specific adaptation through a weight drop-and-grow strategy, enabling the pruned model to self-adapt its sparse connectivity pattern based on the target dataset. Furthermore, a sensitivity-driven pruning criterion is employed to ensure that the desired sparsity level is consistently maintained throughout fine-tuning. Our experiments on various LLMs, including LLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks demonstrate that SEFT achieves stronger performance while offering superior memory and time efficiency compared to existing baselines. Our code is publicly available at: this https URL. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在各种任务中取得了显著成功，但由于其巨大的计算需求面临部署挑战。虽然后训练修剪方法如SparseGPT和Wanda可以有效减小模型规模，但在高稀疏度水平下保持模型性能方面存在困难，限制了其在下游任务中的实用性。现有微调方法，如全面微调和LoRA，在保持稀疏性方面表现不佳，因为它们需要更新整个密集模型，不适用于稀疏LLMs。在本文中，我们提出了一种专为稀疏LLMs设计的新方法：稀疏性演变微调（SEFT）。SEFT在微调过程中动态演化修剪模型的稀疏拓扑结构，同时在整个过程中保持整体稀疏性。SEFT的优势在于通过权重丢失和生长策略实现任务特定适应的能力，使修剪模型能够根据目标数据集自适应其稀疏连通模式。此外，采用敏感性驱动的修剪准则确保在整个微调过程中维持所需的稀疏性水平。我们在包括LLaMA家族、DeepSeek和Mistral在内的多种LLMs上，使用多种基准测试表明，SEFT在性能、内存效率和时间效率方面优于现有基线。我们的代码已公开可用：this https URL。 

---
# GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs 

**Title (ZH)**: GenIC：一种基于LLM的knowledge graph实例完成框架 

**Authors**: Amel Gader, Alsayed Algergawy  

**Link**: [PDF](https://arxiv.org/pdf/2505.24036)  

**Abstract**: Knowledge graph completion aims to address the gaps of knowledge bases by adding new triples that represent facts. The complexity of this task depends on how many parts of a triple are already known. Instance completion involves predicting the relation-tail pair when only the head is given (h, ?, ?). Notably, modern knowledge bases often contain entity descriptions and types, which can provide valuable context for inferring missing facts. By leveraging these textual descriptions and the ability of large language models to extract facts from them and recognize patterns within the knowledge graph schema, we propose an LLM-powered, end-to-end instance completion approach. Specifically, we introduce GenIC: a two-step Generative Instance Completion framework. The first step focuses on property prediction, treated as a multi-label classification task. The second step is link prediction, framed as a generative sequence-to-sequence task. Experimental results on three datasets show that our method outperforms existing baselines. Our code is available at this https URL. 

**Abstract (ZH)**: 知识图谱完备性旨在通过添加表示事实的新三元组来填补知识库的空白。这一任务的复杂性取决于已知三元组的哪一部分。实例完备性涉及在仅给定主语（h, ?, ?）的情况下预测关系-宾语对。值得注意的是，现代知识库通常包含实体描述和类型，这些文本信息可以为推测缺失事实提供有价值的上下文。通过利用这些文本描述和大型语言模型从其中提取事实并在知识图谱模式内识别模式的能力，我们提出了一种基于大型语言模型的端到端实例完备性方法。具体来说，我们引入了GenIC：一种两步生成式实例完备性框架。第一步侧重于属性预测，被视为一个多标签分类任务。第二步是关系预测，被框定为生成式的序列到序列任务。在三个数据集上的实验结果表明，我们的方法优于现有基线。我们的代码可在以下链接获取。 

---
# Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding 

**Title (ZH)**: 多模式 Retrieval-Augmented Generation 系统：适配的视频理解 

**Authors**: Mingyang Mao, Mariela M. Perez-Cabarcas, Utteja Kallakuri, Nicholas R. Waytowich, Xiaomin Lin, Tinoosh Mohsenin  

**Link**: [PDF](https://arxiv.org/pdf/2505.23990)  

**Abstract**: To effectively engage in human society, the ability to adapt, filter information, and make informed decisions in ever-changing situations is critical. As robots and intelligent agents become more integrated into human life, there is a growing opportunity-and need-to offload the cognitive burden on humans to these systems, particularly in dynamic, information-rich scenarios.
To fill this critical need, we present Multi-RAG, a multimodal retrieval-augmented generation system designed to provide adaptive assistance to humans in information-intensive circumstances. Our system aims to improve situational understanding and reduce cognitive load by integrating and reasoning over multi-source information streams, including video, audio, and text. As an enabling step toward long-term human-robot partnerships, Multi-RAG explores how multimodal information understanding can serve as a foundation for adaptive robotic assistance in dynamic, human-centered situations. To evaluate its capability in a realistic human-assistance proxy task, we benchmarked Multi-RAG on the MMBench-Video dataset, a challenging multimodal video understanding benchmark. Our system achieves superior performance compared to existing open-source video large language models (Video-LLMs) and large vision-language models (LVLMs), while utilizing fewer resources and less input data. The results demonstrate Multi- RAG's potential as a practical and efficient foundation for future human-robot adaptive assistance systems in dynamic, real-world contexts. 

**Abstract (ZH)**: 为了有效参与到人类社会中，适应、筛选信息并在不断变化的情况下做出明智决策的能力至关重要。随着机器人和智能代理越来越多地融入人类生活，将认知负担转移到这些系统上的机会和需求不断增加，尤其是在动态、信息丰富的场景中。

为满足这一关键需求，我们提出了Multi-RAG，这是一种多模态检索增强生成系统，旨在为信息密集型情境下的用户提供适应性辅助。该系统通过整合和推理多源信息流，包括视频、音频和文本，以提高情境理解并减轻认知负担。作为长期人机伙伴关系的使能步骤，Multi-RAG 探索了多模态信息理解如何作为动态、以人为核心情境下适应性机器人辅助的基础。为了评估其在现实的人机辅助代理任务中的能力，我们在具有挑战性的多模态视频理解基准数据集（MMBench-Video）上对Multi-RAG进行了测试。与现有的开源视频大语言模型（Video-LLMs）和大型视觉-语言模型（LVLMs）相比，我们的系统在使用更少资源和更少输入数据的情况下实现了更好的性能。结果表明，Multi-RAG 具有作为未来动态现实世界环境中人机适应性辅助系统实用且高效的潜在基础的潜力。 

---
# MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge 

**Title (ZH)**: MSQA：评估大型语言模型在材料科学研究生级推理与知识方面的表现 

**Authors**: Jerry Junyang Cheung, Shiyao Shen, Yuchen Zhuang, Yinghao Li, Rampi Ramprasad, Chao Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.23982)  

**Abstract**: Despite recent advances in large language models (LLMs) for materials science, there is a lack of benchmarks for evaluating their domain-specific knowledge and complex reasoning abilities. To bridge this gap, we introduce MSQA, a comprehensive evaluation benchmark of 1,757 graduate-level materials science questions in two formats: detailed explanatory responses and binary True/False assessments. MSQA distinctively challenges LLMs by requiring both precise factual knowledge and multi-step reasoning across seven materials science sub-fields, such as structure-property relationships, synthesis processes, and computational modeling. Through experiments with 10 state-of-the-art LLMs, we identify significant gaps in current LLM performance. While API-based proprietary LLMs achieve up to 84.5% accuracy, open-source (OSS) LLMs peak around 60.5%, and domain-specific LLMs often underperform significantly due to overfitting and distributional shifts. MSQA represents the first benchmark to jointly evaluate the factual and reasoning capabilities of LLMs crucial for LLMs in advanced materials science. 

**Abstract (ZH)**: 尽管在材料科学领域的大型语言模型（LLM）方面取得了近期进展，但仍缺乏评估其领域特定知识和复杂推理能力的标准标尺。为了弥合这一差距，我们引入了MSQA，这是一个包含1,757个研究生水平材料科学问题的全面评估基准，问题格式包括详细的解释性回答和二元真/假评估。MSQA通过要求LLM在七个材料科学子领域的精确事实知识和多步推理来独特地挑战LLM，这些子领域包括结构-性能关系、合成过程和计算建模。通过对10个最先进的LLM的实验，我们发现当前LLM性能存在显著差距。虽然基于API的专有LLM的准确率达到84.5%，开源（OSS）LLM的准确率约为60.5%，且专门针对某一领域的LLM往往因为过度拟合和分布偏移而表现不佳。MSQA代表了第一个同时评估LLM在先进材料科学中至关重要的事实推理能力的标准标尺。 

---
# Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve 

**Title (ZH)**: 从多agent框架学习与改进代码LLM的经验教训 

**Authors**: Yuanzhe Liu, Ryan Deng, Tim Kaler, Xuhao Chen, Charles E. Leiserson, Yao Ma, Jie Chen  

**Link**: [PDF](https://arxiv.org/pdf/2505.23946)  

**Abstract**: Recent studies show that LLMs possess different skills and specialize in different tasks. In fact, we observe that their varied performance occur in several levels of granularity. For example, in the code optimization task, code LLMs excel at different optimization categories and no one dominates others. This observation prompts the question of how one leverages multiple LLM agents to solve a coding problem without knowing their complementary strengths a priori. We argue that a team of agents can learn from each other's successes and failures so as to improve their own performance. Thus, a lesson is the knowledge produced by an agent and passed on to other agents in the collective solution process. We propose a lesson-based collaboration framework, design the lesson solicitation--banking--selection mechanism, and demonstrate that a team of small LLMs with lessons learned can outperform a much larger LLM and other multi-LLM collaboration methods. 

**Abstract (ZH)**: 最近的研究表明，大型语言模型具有不同的技能并擅长不同的任务。实际上，我们观察到它们在不同粒度层次上表现出不同的性能。例如，在代码优化任务中，代码LLM在不同的优化类别中表现出色，没有一个模型能够全面超越其他模型。这一观察引发了如何在事先不知道各模型互补优势的情况下，利用多个LLM代理解决编程问题的问题。我们认为，代理团队可以从彼此的成功和失败中学习，以提高自己的性能。因此，从一个代理生成的知识并传递给团队中的其他代理是这一过程中的关键。我们提出了一种基于教训的合作框架，设计了教训的征集—存储—选择机制，并证明了一个学习了教训的较小规模LLM团队能够优于一个更大规模的LLM和其它LLM合作方法。 

---
# OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation 

**Title (ZH)**: OWL：优化 workforce 学习以实现现实世界多代理任务自动化的一般性协助 

**Authors**: Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, Guohao Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.23885)  

**Abstract**: Large Language Model (LLM)-based multi-agent systems show promise for automating real-world tasks but struggle to transfer across domains due to their domain-specific nature. Current approaches face two critical shortcomings: they require complete architectural redesign and full retraining of all components when applied to new domains. We introduce Workforce, a hierarchical multi-agent framework that decouples strategic planning from specialized execution through a modular architecture comprising: (i) a domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask management, and (iii) specialized Workers with domain-specific tool-calling capabilities. This decoupling enables cross-domain transferability during both inference and training phases: During inference, Workforce seamlessly adapts to new domains by adding or modifying worker agents; For training, we introduce Optimized Workforce Learning (OWL), which improves generalization across domains by optimizing a domain-agnostic planner with reinforcement learning from real-world feedback. To validate our approach, we evaluate Workforce on the GAIA benchmark, covering various realistic, multi-domain agentic tasks. Experimental results demonstrate Workforce achieves open-source state-of-the-art performance (69.70%), outperforming commercial systems like OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to GPT-4o on challenging tasks. To summarize, by enabling scalable generalization and modular domain transfer, our work establishes a foundation for the next generation of general-purpose AI assistants. 

**Abstract (ZH)**: 基于大型语言模型（LLM）的多Agent系统在自动化现实世界任务方面展现出潜力，但由于其领域特定性，难以跨领域转移。当前方法面临两大关键局限：应用于新领域时需要完全重新设计架构并重新训练所有组件。我们提出了Workforce，一种通过模块化架构解耦战略规划和专门执行的层次多Agent框架，该架构包括：（i）领域无关的规划者进行任务分解，（ii）协调器管理子任务，（iii）具有领域特定工具调用能力的专业工作者。这种解耦在推理和训练阶段都实现了跨领域的可转移性：在推理阶段，Workforce通过添加或修改工作者代理无缝适应新领域；在训练阶段，我们引入了优化Workforce学习（OWL），通过对来自真实世界反馈的强化学习优化领域无关的规划者，从而改善跨领域的泛化能力。为了验证我们的方法，我们在GAIA基准上评估了Workforce，涵盖各种现实的、多领域的代理任务。实验结果表明，Workforce达到了开源领域最佳性能（69.70%），比开源系统OpenAI的Deep Research更好（提高2.34%）。更值得一提的是，我们的OWL训练的32B模型准确率达到52.73%（提高16.37%），在挑战性任务上展示了与GPT-4o相当的表现。总之，通过实现可扩展的泛化能力和模块化的领域转移，我们的工作为下一代通用AI助手奠定了基础。 

---
# Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems 

**Title (ZH)**: 使用推理模型生成解决组合设计问题开放实例的搜索启发式方法 

**Authors**: Christopher D. Rosin  

**Link**: [PDF](https://arxiv.org/pdf/2505.23881)  

**Abstract**: Large Language Models (LLMs) with reasoning are trained to iteratively generate and refine their answers before finalizing them, which can help with applications to mathematics and code generation. We apply code generation with reasoning LLMs to a specific task in the mathematical field of combinatorial design. This field studies diverse types of combinatorial designs, many of which have lists of open instances for which existence has not yet been determined. The Constructive Protocol CPro1 uses LLMs to generate search heuristics that have the potential to construct solutions to small open instances. Starting with a textual definition and a validity verifier for a particular type of design, CPro1 guides LLMs to select and implement strategies, while providing automated hyperparameter tuning and execution feedback. CPro1 with reasoning LLMs successfully solves long-standing open instances for 7 of 16 combinatorial design problems selected from the 2006 Handbook of Combinatorial Designs, including new solved instances for 3 of these (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary Designs) that were unsolved by CPro1 with non-reasoning LLMs. It also solves open instances for several problems from recent (2025) literature, generating new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform Nested Steiner Quadruple System. 

**Abstract (ZH)**: 具有推理能力的大语言模型（LLMs）通过迭代生成和精炼答案来训练，这有助于数学和代码生成的应用。我们将具有推理能力的LLMs应用于组合设计领域的一个特定任务。该领域研究多种类型的组合设计，其中许多设计具有开放实例，其存在性尚未确定。Constructive Protocol CPro1使用LLMs生成搜索启发式，这些启发式有可能构建小型开放实例的解。从特定类型的组合设计的文本定义和有效性验证器开始，CPro1引导LLMs选择和实施策略，并提供自动化超参数调优和执行反馈。使用具有推理能力的LLMs的CPro1成功解决了16个2006年《组合设计手册》中选择的组合设计问题中的7个长期未决的开放实例，其中包括3个新解决的实例（Bhaskar Rao Designs、对称称重矩阵、平衡三元组合设计），这些实例是CPro1使用非推理LLMs未解决的新实例。此外，CPro1还解决了最近（2025年）文献中几个问题的开放实例，生成了新的覆盖序列、Johnson clique覆盖、删除码和均匀嵌套斯坦纳四元系。 

---
# ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models 

**Title (ZH)**: 延长强化学习：ProRL 扩展大型语言模型的推理边界 

**Authors**: Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, Yi Dong  

**Link**: [PDF](https://arxiv.org/pdf/2505.24864)  

**Abstract**: Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: this https URL 

**Abstract (ZH)**: 最近在注重推理的语言模型方面的进展表明，强化学习（RL）是使模型与可验证奖励对齐的一种有前途的方法。然而，仍然有争议的是，RL是否真正扩展了模型的推理能力，还是仅仅放大了基础模型分布中已经存在的高奖励输出，以及持续扩大RL计算是否可靠地提高了推理性能。在本工作中，我们通过证明长时间的RL（ProRL）训练可以在广泛的采样下揭示出基础模型无法访问的新推理策略来挑战现有的假设。我们引入了ProRL，一种新的训练方法，其中包括KL散度控制、参考策略重置以及一系列多样的任务。我们的实证分析表明，经过RL训练的模型在广泛的pass@k评估中始终优于基础模型，包括基础模型无论如何尝试都无法完成的情形。我们进一步表明，推理边界改进与基础模型的任务能力和训练时间密切相关，这表明RL可以在时间上探索并填充新的解空间。这些发现为理解何时以及在多大程度上RL能够真正扩展语言模型的推理边界提供了新的见解，并为长期RL在推理方面的研究奠定了基础。我们发布了模型权重以支持进一步的研究：[这个链接](这个链接)。 

---
# Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning 

**Title (ZH)**: 利用负信号：从教师数据中进行强化蒸馏以提升LLM推理能力 

**Authors**: Shuyao Xu, Cheng Peng, Jiangxuan Long, Weidi Xu, Wei Chu, Yuan Qi  

**Link**: [PDF](https://arxiv.org/pdf/2505.24850)  

**Abstract**: Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data. 

**Abstract (ZH)**: 近期模型蒸馏进展表明，来自高级推理模型（如DeepSeek-R1、OpenAI的o1）的数据可以有效转移复杂的推理能力给更小且高效的学徒模型。然而，标准做法通常采用拒绝采样，抛弃错误的推理示例——这些有价值的但往往被低估的数据。本文探讨了一个关键问题：如何有效利用正向和负向蒸馏推理轨迹，在离线环境中最大化LLM的推理性能？为此，我们提出了强化蒸馏（REDI）两阶段框架。第一阶段通过监督微调（SFT）学习正向轨迹。第二阶段通过我们提出的REDI目标，进一步利用正向和负向轨迹优化模型。这一新颖的目标是一个简单的、无需参考的损失函数，在此蒸馏背景下优于现有的方法如DPO和SimPO。我们的实证评估表明，REDI在数学推理任务上优于基线拒绝采样SFT或结合使用DPO/SimPO的方法。值得注意的是，仅在131,000个正向和负向开放Open-R1数据示例上进行后训练的Qwen-REDI-1.5B模型，在MATH-500（pass@1）上实现了83.1%的得分。其性能与DeepSeek-R1-Distill-Qwen-1.5B（一个在800,000个专有数据上进行后训练的模型）在各种数学推理基准测试中相当或更好，为1.5B模型在离线环境中使用公开可用数据进行后训练设定了新标准。 

---
# Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck 

**Title (ZH)**: Vision LLMs在层级视觉理解方面表现不佳，且LLMs是瓶颈。 

**Authors**: Yuwen Tan, Yuan Qing, Boqing Gong  

**Link**: [PDF](https://arxiv.org/pdf/2505.24840)  

**Abstract**: This paper reveals that many state-of-the-art large language models (LLMs) lack hierarchical knowledge about our visual world, unaware of even well-established biology taxonomies. This shortcoming makes LLMs a bottleneck for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone Fish but not Vertebrate). We arrive at these findings using about one million four-choice visual question answering (VQA) tasks constructed from six taxonomies and four image datasets. Interestingly, finetuning a vision LLM using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because the VQA tasks improve the LLM's hierarchical consistency more than the vision LLM's. We conjecture that one cannot make vision LLMs understand visual concepts fully hierarchical until LLMs possess corresponding taxonomy knowledge. 

**Abstract (ZH)**: 本文揭示了许多最先进的大型语言模型（LLMs）缺乏对视觉世界的层次化知识，甚至不知晓已建立的生物学分类法。这一缺陷使得LLMs成为视知觉LLMs层次化视觉理解的瓶颈（例如，能够识别海葵鱼但无法识别脊椎动物）。我们通过构建来自六个分类法和四个图像数据集的一百多万个四选一的视觉问答（VQA）任务得出了这些发现。有趣的是，使用我们的VQA任务微调视知觉LLM在一定程度上证实了LLMs的瓶颈效应，因为VQA任务比视知觉LLM本身更有效地提高了LLM的层次化一致性。我们推测，在LLMs获得相应的分类法知识之前，无法使视知觉LLM完全理解视觉概念的层次化结构。 

---
# Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs 

**Title (ZH)**: 通过原子事实核查提高检索增强大语言模型在医疗问答中的可靠性和可解释性 

**Authors**: Juraj Vladika, Annika Domres, Mai Nguyen, Rebecca Moser, Jana Nano, Felix Busch, Lisa C. Adams, Keno K. Bressem, Denise Bernhardt, Stephanie E. Combs, Kai J. Borm, Florian Matthes, Jan C. Peeken  

**Link**: [PDF](https://arxiv.org/pdf/2505.24830)  

**Abstract**: Large language models (LLMs) exhibit extensive medical knowledge but are prone to hallucinations and inaccurate citations, which pose a challenge to their clinical adoption and regulatory compliance. Current methods, such as Retrieval Augmented Generation, partially address these issues by grounding answers in source documents, but hallucinations and low fact-level explainability persist. In this work, we introduce a novel atomic fact-checking framework designed to enhance the reliability and explainability of LLMs used in medical long-form question answering. This method decomposes LLM-generated responses into discrete, verifiable units called atomic facts, each of which is independently verified against an authoritative knowledge base of medical guidelines. This approach enables targeted correction of errors and direct tracing to source literature, thereby improving the factual accuracy and explainability of medical Q&A. Extensive evaluation using multi-reader assessments by medical experts and an automated open Q&A benchmark demonstrated significant improvements in factual accuracy and explainability. Our framework achieved up to a 40% overall answer improvement and a 50% hallucination detection rate. The ability to trace each atomic fact back to the most relevant chunks from the database provides a granular, transparent explanation of the generated responses, addressing a major gap in current medical AI applications. This work represents a crucial step towards more trustworthy and reliable clinical applications of LLMs, addressing key prerequisites for clinical application and fostering greater confidence in AI-assisted healthcare. 

**Abstract (ZH)**: 大型语言模型在医学问答中的原子事实核查框架：提高可靠性和解释性 

---
# PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models 

**Title (ZH)**: PhySense: 原理导向的物理推理基准测试 для大型语言模型 

**Authors**: Yinggan Xu, Yue Liu, Zhiqiang Gao, Changnan Peng, Di Luo  

**Link**: [PDF](https://arxiv.org/pdf/2505.24823)  

**Abstract**: Large language models (LLMs) have rapidly advanced and are increasingly capable of tackling complex scientific problems, including those in physics. Despite this progress, current LLMs often fail to emulate the concise, principle-based reasoning characteristic of human experts, instead generating lengthy and opaque solutions. This discrepancy highlights a crucial gap in their ability to apply core physical principles for efficient and interpretable problem solving. To systematically investigate this limitation, we introduce PhySense, a novel principle-based physics reasoning benchmark designed to be easily solvable by experts using guiding principles, yet deceptively difficult for LLMs without principle-first reasoning. Our evaluation across multiple state-of-the-art LLMs and prompt types reveals a consistent failure to align with expert-like reasoning paths, providing insights for developing AI systems with efficient, robust and interpretable principle-based scientific reasoning. 

**Abstract (ZH)**: 大型语言模型（LLMs）在物理学等复杂科学问题上的推理能力有了 rapid advancement，并逐渐能够应对这些问题。尽管取得了这些进展，当前的 LLMs 往往不能模仿人类专家简洁且基于原理的推理方式，反而生成了冗长且难以理解的解决方案。这种差异凸显了他们在运用核心物理原理进行高效和可解释的问题解决方面的关键缺陷。为了系统地研究这一限制，我们引入了 PhySense，这是一种新颖的基于原理的物理推理基准，该基准能够被专家使用指导原则轻松解决，但对于没有基于原理推理的 LLMs 却显得具有欺骗性难度。我们在多个最先进的 LLMs 和不同的提示类型上的评估揭示了它们一致地无法与专家级推理路径对齐，为开发具有高效、稳健和可解释的基于原理的科学推理的人工智能系统提供了见解。 

---
# Drop Dropout on Single-Epoch Language Model Pretraining 

**Title (ZH)**: 单epoch语言模型预训练中去除 Dropout 

**Authors**: Houjun Liu, John Bauer, Christopher D. Manning  

**Link**: [PDF](https://arxiv.org/pdf/2505.24788)  

**Abstract**: Originally, dropout was seen as a breakthrough regularization technique that reduced overfitting and improved performance in almost all applications of deep learning by reducing overfitting. Yet, single-epoch pretraining tasks common to modern LLMs yield minimal overfitting, leading to dropout not being used for large LLMs. Nevertheless, no thorough empirical investigation has been done on the role of dropout in LM pretraining. Through experiments in single-epoch pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs with varying levels of dropout, we find that downstream performance in language modeling, morpho-syntax (BLiMP), question answering (SQuAD), and natural-language inference (MNLI) improves when dropout is not applied during pretraining. We additionally find that the recently-introduced "early dropout" also degrades performance over applying no dropout at all. We further investigate the models' editability, and find that models trained without dropout are more successful in gradient-based model editing (MEND) and equivalent in representation-based model editing (ReFT). Therefore, we advocate to drop dropout during single-epoch pretraining. 

**Abstract (ZH)**: 起初，dropout被视为一种减少过拟合并几乎在所有深度学习应用中提升性能的突破性正则化技术。然而，现代大语言模型中常见的单轮预训练任务导致过拟合极少发生，使得dropout不再用于大型语言模型。尽管如此，至今尚未进行过彻底的实证研究以探索dropout在语言模型预训练中的作用。通过在不同dropout水平下对掩码（BERT）和自回归（Pythia 160M和1.4B）语言模型进行单轮预训练的实验，我们发现，在预训练过程中不使用dropout，可以提升下游语言模型、形态语法（BLiMP）、问答（SQuAD）和自然语言推理（MNLI）等任务的表现。此外，我们还发现最近引入的“早期dropout”会进一步降低性能，不如完全不使用dropout。进一步研究发现，不使用dropout训练的模型在基于梯度的模型编辑（MEND）中更具可编辑性，在基于表示的模型编辑（ReFT）中表现相当。因此，我们建议在单轮预训练过程中舍弃dropout。 

---
# HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts 

**Title (ZH)**: HELM：通过混合曲率专家的双曲大型语言模型 

**Authors**: Neil He, Rishabh Anand, Hiren Madhu, Ali Maatouk, Smita Krishnaswamy, Leandros Tassiulas, Menglin Yang, Rex Ying  

**Link**: [PDF](https://arxiv.org/pdf/2505.24722)  

**Abstract**: Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining. 

**Abstract (ZH)**: Hyperbolic Space-Based HypErbolic Large Language Models (HELM) and Its Applications 

---
# Towards Scalable Schema Mapping using Large Language Models 

**Title (ZH)**: 面向大规模语义映射的大型语言模型方法研究 

**Authors**: Christopher Buss, Mahdis Safari, Arash Termehchy, Stefan Lee, David Maier  

**Link**: [PDF](https://arxiv.org/pdf/2505.24716)  

**Abstract**: The growing need to integrate information from a large number of diverse sources poses significant scalability challenges for data integration systems. These systems often rely on manually written schema mappings, which are complex, source-specific, and costly to maintain as sources evolve. While recent advances suggest that large language models (LLMs) can assist in automating schema matching by leveraging both structural and natural language cues, key challenges remain. In this paper, we identify three core issues with using LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to input phrasing and structure, which we propose methods to address through sampling and aggregation techniques; (2) the need for more expressive mappings (e.g., GLaV), which strain the limited context windows of LLMs; and (3) the computational cost of repeated LLM calls, which we propose to mitigate through strategies like data type prefiltering. 

**Abstract (ZH)**: 日益增长的从大量多样来源整合信息的需求给数据整合系统带来了显著的可扩展性挑战。这些系统通常依赖于手工编写的模式映射，这些映射复杂、来源特定，随着来源的变化维护成本高昂。尽管近期进展表明大型语言模型（LLMs）可以通过利用结构和自然语言线索来协助自动化模式匹配，但仍存在一些关键挑战。在本文中，我们指出了使用LLMs进行模式映射的三个核心问题：（1）由于对输入措辞和结构的敏感性导致不一致的输出，我们提出了通过采样和聚合技术来解决的方法；（2）需要更具表现力的映射（例如GLaV），这会超出LLMs有限的上下文窗口；（3）重复调用LLMs的计算成本，我们提出通过数据类型预筛选等策略来减轻这一问题。 

---
# Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting 

**Title (ZH)**: 因果意识大语言模型：通过学习、适应和行动增强决策制定 

**Authors**: Wei Chen, Jiahao Zhang, Haipeng Zhu, Boyan Xu, Zhifeng Hao, Keli Zhang, Junjian Ye, Ruichu Cai  

**Link**: [PDF](https://arxiv.org/pdf/2505.24710)  

**Abstract**: Large language models (LLMs) have shown great potential in decision-making due to the vast amount of knowledge stored within the models. However, these pre-trained models are prone to lack reasoning abilities and are difficult to adapt to new environments, further hindering their application to complex real-world tasks. To address these challenges, inspired by the human cognitive process, we propose Causal-aware LLMs, which integrate the structural causal model (SCM) into the decision-making process to model, update, and utilize structured knowledge of the environment in a ``learning-adapting-acting" paradigm. Specifically, in the learning stage, we first utilize an LLM to extract the environment-specific causal entities and their causal relations to initialize a structured causal model of the environment. Subsequently,in the adapting stage, we update the structured causal model through external feedback about the environment, via an idea of causal intervention. Finally, in the acting stage, Causal-aware LLMs exploit structured causal knowledge for more efficient policy-making through the reinforcement learning agent. The above processes are performed iteratively to learn causal knowledge, ultimately enabling the causal-aware LLMs to achieve a more accurate understanding of the environment and make more efficient decisions. Experimental results across 22 diverse tasks within the open-world game ``Crafter" validate the effectiveness of our proposed method. 

**Abstract (ZH)**: 具有因果意识的大语言模型（Causal-aware LLMs）：通过集成结构因果模型（SCM）实现环境结构化知识的建模、更新和利用 

---
# Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison 

**Title (ZH)**: 基于LLMs的多域ABSAC对话数据集生成以进行实际评估和模型比较 

**Authors**: Tejul Pandit, Meet Raval, Dhvani Upadhyay  

**Link**: [PDF](https://arxiv.org/pdf/2505.24701)  

**Abstract**: Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions but often suffers from the scarcity of diverse, labeled datasets that reflect real-world conversational nuances. This paper presents an approach for generating synthetic ABSA data using Large Language Models (LLMs) to address this gap. We detail the generation process aimed at producing data with consistent topic and sentiment distributions across multiple domains using GPT-4o. The quality and utility of the generated data were evaluated by assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification tasks. Our results demonstrate the effectiveness of the synthetic data, revealing distinct performance trade-offs among the models: DeepSeekR1 showed higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall, and Gemini 1.5 Pro offered significantly faster inference. We conclude that LLM-based synthetic data generation is a viable and flexible method for creating valuable ABSA resources, facilitating research and model evaluation without reliance on limited or inaccessible real-world labeled data. 

**Abstract (ZH)**: 基于方面的意见分析（ABSA）提供了详细的见解，但常常因缺乏多样且标注的真实世界对话数据而受到限制。本文提出了一种使用大型语言模型（LLMs）生成合成ABSA数据的方法，以填补这一空白。我们使用GPT-4o详细描述了生成过程，旨在产生在多个领域中具有一致的话题和情感分布的数据。通过评估三种最先进的LLMs（Gemini 1.5 Pro、Claude 3.5 Sonnet和DeepSeek-R1）在主题和情感分类任务上的性能，来评估生成数据的质量和实用性。我们的结果表明合成数据的有效性，揭示了不同模型之间的性能权衡：DeepSeekR1展现出更高的精确度，Gemini 1.5 Pro和Claude 3.5 Sonnet表现出强劲的召回率，而Gemini 1.5 Pro提供了显著更快的推理速度。我们得出结论，基于LLM的合成数据生成是一种可行且灵活的方法，可以创建有价值的ABSA资源，促进研究和模型评估，而无需依赖有限或难以获取的真实标注数据。 

---
# Multiple LLM Agents Debate for Equitable Cultural Alignment 

**Title (ZH)**: 多个LLM代理为公平的文化对齐展开辩论 

**Authors**: Dayeon Ki, Rachel Rudinger, Tianyi Zhou, Marine Carpuat  

**Link**: [PDF](https://arxiv.org/pdf/2505.24671)  

**Abstract**: Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters). 

**Abstract (ZH)**: 大型语言模型（LLMs）需要适应多样的文化背景以惠及全球多元社区。尽管以往的努力主要集中在单一LLM和单轮对话方法上，我们提出利用多种LLM的互补优势促进文化适应性。我们引入了一种多智能体辩论框架，其中两个基于LLM的智能体就一种文化场景进行辩论，并共同做出最终决策。我们提出了两种变体：一种是LLM智能体专门辩论，另一种是他们在回合中动态选择自我反思与辩论之间切换。我们在7种开源LLM（以及21种LLM组合）上使用涵盖75个国家社会礼仪规范的NormAd-ETI基准进行了评估。实验结果显示，辩论方法在总体准确率和文化群体平等性方面优于单一LLM基线。值得注意的是，多智能体辩论使相对较小的LLM（7-9B参数）能够达到与更大模型（27B参数）相当的准确率。 

---
# BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models 

**Title (ZH)**: BIMA：大规模视觉-语言模型中幻觉预测与缓解的双向最大似然学习方法 

**Authors**: Huu-Thien Tran, Thanh-Dat Truong, Khoa Luu  

**Link**: [PDF](https://arxiv.org/pdf/2505.24649)  

**Abstract**: Large vision-language models have become widely adopted to advance in various domains. However, developing a trustworthy system with minimal interpretable characteristics of large-scale models presents a significant challenge. One of the most prevalent terms associated with the fallacy functions caused by these systems is hallucination, where the language model generates a response that does not correspond to the visual content. To mitigate this problem, several approaches have been developed, and one prominent direction is to ameliorate the decoding process. In this paper, we propose a new Bijective Maximum Likelihood Learning (BIMA) approach to hallucination mitigation using normalizing flow theories. The proposed BIMA method can efficiently mitigate the hallucination problem in prevailing vision-language models, resulting in significant improvements. Notably, BIMA achieves the average F1 score of 85.06% on POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%, respectively. To the best of our knowledge, this is one of the first studies that contemplates the bijection means to reduce hallucination induced by large vision-language models. 

**Abstract (ZH)**: 大规模vision-language模型已在多个领域得到广泛应用。然而，开发具有最小可解释性特征的信任系统仍面临巨大挑战。与这些系统导致的谬误功能最相关的术语之一是幻觉，即语言模型生成的响应与视觉内容不一致。为了解决这一问题，已开发出多种方法，其中一个主要方向是改进解码过程。在本文中，我们提出了一种新的基于规范化流理论的双向最大似然学习（BIMA）方法，用于幻觉缓解。所提出的BIMA方法可以高效缓解现有vision-language模型中的幻觉问题，取得显著改进。据我们所知，这是首次研究考虑使用双射方法减少大型vision-language模型引起的幻觉的研究之一。 

---
# Efficient Text Encoders for Labor Market Analysis 

**Title (ZH)**: 高效文本编码器在劳动力市场分析中的应用 

**Authors**: Jens-Joris Decorte, Jeroen Van Hautte, Chris Develder, Thomas Demeester  

**Link**: [PDF](https://arxiv.org/pdf/2505.24640)  

**Abstract**: Labor market analysis relies on extracting insights from job advertisements, which provide valuable yet unstructured information on job titles and corresponding skill requirements. While state-of-the-art methods for skill extraction achieve strong performance, they depend on large language models (LLMs), which are computationally expensive and slow. In this paper, we propose \textbf{ConTeXT-match}, a novel contrastive learning approach with token-level attention that is well-suited for the extreme multi-label classification task of skill classification. \textbf{ConTeXT-match} significantly improves skill extraction efficiency and performance, achieving state-of-the-art results with a lightweight bi-encoder model. To support robust evaluation, we introduce \textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill annotations that explicitly address the redundancy in the large label space. Finally, we present \textbf{JobBERT V2}, an improved job title normalization model that leverages extracted skills to produce high-quality job title representations. Experiments demonstrate that our models are efficient, accurate, and scalable, making them ideal for large-scale, real-time labor market analysis. 

**Abstract (ZH)**: 劳动市场分析依赖于从招聘信息中提取洞察，招聘信息提供了职位名称和对应技能要求的宝贵但未结构化的信息。虽然最先进的技能提取方法表现强劲，但它们依赖于大型语言模型（LLMs），这使得它们计算成本高昂且速度较慢。在本文中，我们提出了\textbf{ConTeCTX	match}，一种适用于技能分类的极端多标签分类任务的新颖对比学习方法，具有token级注意力机制。ConTeCTX	match显著提高了技能提取的效率和性能，使用轻量级双编码器模型达到了最先进的效果。为了支持稳健的评估，我们引入了\textbf{Skill-XL}，这是一个新的基准，包含详尽的、句子级别的技能注解，明确解决了大量标签空间中的冗余问题。最后，我们呈现了\textbf{JobBERT V2}，一种改进的工作职位规范化模型，该模型利用提取的技能生成高质量的工作职位表示。实验表明，我们的模型高效、准确且可扩展，使其成为大规模、实时劳动市场分析的理想工具。 

---
# The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models 

**Title (ZH)**: 幻觉困境：事实导向的大型推理模型增强学习 

**Authors**: Junyi Li, Hwee Tou Ng  

**Link**: [PDF](https://arxiv.org/pdf/2505.24630)  

**Abstract**: Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance. 

**Abstract (ZH)**: 大型语言模型（LLMs）通过强化学习（RL）优化在推理任务中取得了显著进展，实现了各种具有挑战性的基准测试中的出色能力。然而，我们的实证分析揭示了一个关键缺点：面向推理的RL微调显著增加了幻觉现象的频率。我们理论上分析了RL训练动力学，将高方差梯度、熵引起的随机性及对虚假局部最优的敏感性识别为导致幻觉的关键因素。为了应对这一缺点，我们提出了事实意识分步策略优化（FSPO），这是一种创新的RL微调算法，其中间引入了明确的事实验证。FSPO利用自动验证与给定证据的对比，动态调整词元级优势值，激励推理过程中的事实正确性。使用Qwen2.5和Llama模型在数学推理和幻觉基准测试中的实验表明，FSPO有效减少了幻觉现象，同时提高了推理准确性，显著提升了可靠性和性能。 

---
# Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors 

**Title (ZH)**: 从视频中学习构建3D世界：通过3D视觉几何先验增强多模态大规模语言模型 

**Authors**: Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.24625)  

**Abstract**: Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations. 

**Abstract (ZH)**: 以前的研究考察了多模态大型语言模型（MLLMs）在将3D场景视为视频进行解释时的应用。这些方法通常依赖于全面的3D数据输入，如点云或重建的鸟瞰图（BEV）地图。在我们的研究中，我们通过增强MLLMs直接从视频数据中理解与推理三维空间的能力，推进了该领域的发展，而不需额外的3D输入。我们提出了一种新颖而有效的方法——视频-3D几何大型语言模型（VG LLM）。我们的方法使用三维视觉几何编码器从视频序列中提取三维先验信息，并将这些信息与视觉标记结合后输入到MLLM中。大量实验表明，我们的方法在各种与3D场景理解和空间推理相关的任务中取得了显著改进，且所有这些任务皆直接从视频源中学习。令人印象深刻的是，我们的4B模型不依赖于显式的3D数据输入，在VSI-Bench评估中达到了与现有最先进的方法具有竞争力的结果，并超越了Gemini-1.5-Pro。 

---
# Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX 

**Title (ZH)**: 判决之眼：剖析俄语系LLM的评估方法与POLLUX 

**Authors**: Nikita Martynov, Anastasia Mordasheva, Dmitriy Gorbetskiy, Danil Astafurov, Ulyana Isaeva, Elina Basyrova, Sergey Skachkov, Victoria Berestova, Nikolay Ivanov, Valeriia Zanina, Alena Fenogenova  

**Link**: [PDF](https://arxiv.org/pdf/2505.24616)  

**Abstract**: We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments. 

**Abstract (ZH)**: 我们引入了POLLUX，一个全面的开源基准，旨在评估俄语大型语言模型（LLMs）的生成能力。我们的主要贡献是一种新颖的评估方法，提高了对LLM评估的可解释性。对于每种任务类型，我们定义了一套详细的评估标准，并开发了一套评分协议，其中模型评估响应并提供评分依据。这使得评估过程超越了传统的资源密集型逐一对比的人类评估，实现透明且基于标准的评估。POLLUX包括一个详细且层次分明的35种任务类型分类体系，涵盖诸如代码生成、创造性写作和实际助手使用场景等多种生成领域，共包含2,100个手工制作的专业提示。每个任务按难度（简单/中等/复杂）分类，由专家从零开始构建数据集。我们还发布了具有细致评估能力的大型语言模型评判器（7B和32B）系列评估工具，为生成输出的细微评估提供支持。这种做法提供了可扩展且可解释的评估和注释工具，有效地替代了成本高且精度低的人类判断。 

---
# NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization 

**Title (ZH)**: NexusSum：层次化的大型语言模型代理用于长篇叙事总结 

**Authors**: Hyuntak Kim, Byung-Hak Kim  

**Link**: [PDF](https://arxiv.org/pdf/2505.24575)  

**Abstract**: Summarizing long-form narratives--such as books, movies, and TV scripts--requires capturing intricate plotlines, character interactions, and thematic coherence, a task that remains challenging for existing LLMs. We introduce NexusSum, a multi-agent LLM framework for narrative summarization that processes long-form text through a structured, sequential pipeline--without requiring fine-tuning. Our approach introduces two key innovations: (1) Dialogue-to-Description Transformation: A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence. (2) Hierarchical Multi-LLM Summarization: A structured summarization pipeline that optimizes chunk processing and controls output length for accurate, high-quality summaries. Our method establishes a new state-of-the-art in narrative summarization, achieving up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts. These results demonstrate the effectiveness of multi-agent LLMs in handling long-form content, offering a scalable approach for structured summarization in diverse storytelling domains. 

**Abstract (ZH)**: 长篇叙事概要总结——诸如书籍、电影和电视剧剧本——要求捕捉复杂的故事情节、角色互动和主题一致性，现有LLM仍面临挑战。我们引入了NexusSum，这是一种多代理LLM框架，用于叙事概要总结，通过结构化、顺序的管道处理长篇文本，无需微调。我们的方法引入了两项关键创新：（1）对话到描述转换：一种针对叙事的预处理方法，将角色对话和描述性文本标准化为统一格式，提高连贯性。（2）分层多LLM概要总结：一种结构化的概要总结管道，优化块处理并控制输出长度，以实现准确和高质量的概要。该方法在书籍、电影和电视剧剧本的叙事概要总结中达到了新的最先进水平，BERTScore（F1）提高了最高30.0%。这些结果表明多代理LLM在处理长篇文本方面的有效性，为不同叙事领域的结构化概要总结提供了可扩展的方法。 

---
# Bench4KE: Benchmarking Automated Competency Question Generation 

**Title (ZH)**: Bench4KE: 自动化能力问题生成评估基准 

**Authors**: Anna Sofia Lippolis, Minh Davide Ragagni, Paolo Ciancarini, Andrea Giovanni Nuzzolese, Valentina Presutti  

**Link**: [PDF](https://arxiv.org/pdf/2505.24554)  

**Abstract**: The availability of Large Language Models (LLMs) presents a unique opportunity to reinvigorate research on Knowledge Engineering (KE) automation, a trend already evident in recent efforts developing LLM-based methods and tools for the automatic generation of Competency Questions (CQs). However, the evaluation of these tools lacks standardisation. This undermines the methodological rigour and hinders the replication and comparison of results. To address this gap, we introduce Bench4KE, an extensible API-based benchmarking system for KE automation. Its first release focuses on evaluating tools that generate CQs automatically. CQs are natural language questions used by ontology engineers to define the functional requirements of an ontology. Bench4KE provides a curated gold standard consisting of CQ datasets from four real-world ontology projects. It uses a suite of similarity metrics to assess the quality of the CQs generated. We present a comparative analysis of four recent CQ generation systems, which are based on LLMs, establishing a baseline for future research. Bench4KE is also designed to accommodate additional KE automation tasks, such as SPARQL query generation, ontology testing and drafting. Code and datasets are publicly available under the Apache 2.0 license. 

**Abstract (ZH)**: 大型语言模型（LLMs）的可用性为知识工程（KE）自动化研究带来了独特的机会，这一趋势已在基于LLM的方法和工具开发中生成能力问题（CQs）的自动生成中显现出来。然而，这些工具的评估缺乏标准化，这削弱了方法论的严谨性，并阻碍了结果的重复和比较。为了解决这一缺口，我们引入了Bench4KE，这是一个可扩展的API基础基准系统，用于KE自动化评估。其首次发布专注于评估自动生成CQs的工具。CQs是本体工程师用来定义本体功能需求的自然语言问题。Bench4KE提供了一个经过策展的黄金标准，包括来自四个实际本体项目的数据集，并使用一系列相似性度量来评估生成的CQs的质量。我们对四个基于LLM的最新CQ生成系统进行了比较分析，建立了未来研究的基础。Bench4KE还设计用于容纳其他KE自动化任务，如SPARQL查询生成、本体测试和起草。代码和数据集在Apache 2.0许可下公开。 

---
# CREFT: Sequential Multi-Agent LLM for Character Relation Extraction 

**Title (ZH)**: CREFT: 顺序多agents语言模型用于角色关系提取 

**Authors**: Ye Eun Chun, Taeyoon Hwang, Seung-won Hwang, Byung-Hak Kim  

**Link**: [PDF](https://arxiv.org/pdf/2505.24553)  

**Abstract**: Understanding complex character relations is crucial for narrative analysis and efficient script evaluation, yet existing extraction methods often fail to handle long-form narratives with nuanced interactions. To address this challenge, we present CREFT, a novel sequential framework leveraging specialized Large Language Model (LLM) agents. First, CREFT builds a base character graph through knowledge distillation, then iteratively refines character composition, relation extraction, role identification, and group assignments. Experiments on a curated Korean drama dataset demonstrate that CREFT significantly outperforms single-agent LLM baselines in both accuracy and completeness. By systematically visualizing character networks, CREFT streamlines narrative comprehension and accelerates script review -- offering substantial benefits to the entertainment, publishing, and educational sectors. 

**Abstract (ZH)**: 理解复杂的人物关系对于叙事分析和高效的剧本评估至关重要，然而现有的提取方法往往无法处理包含细腻互动的长篇叙事。为此，我们提出CREFT，一种利用专业大型语言模型代理的新型序列框架。首先，CREFT通过知识蒸馏构建基础人物图，然后迭代优化人物组成、关系提取、角色识别和分组分配。在精心选择的韩剧数据集上的实验表明，CREFT在准确性和完整性方面显著优于单代理大型语言模型基线。通过系统可视化人物网络，CREFT简化了叙事理解并加速了剧本审核——为娱乐、出版和教育领域带来了重大益处。 

---
# Cross-Attention Speculative Decoding 

**Title (ZH)**: 跨注意力推测解码 

**Authors**: Wei Zhong, Manasa Bharadwaj, Yixiao Wang, Nikhil Verma, Yipeng Ji, Chul Lee  

**Link**: [PDF](https://arxiv.org/pdf/2505.24544)  

**Abstract**: Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly coupled, self-attention-based Transformer decoders, often augmented with auxiliary pooling or fusion layers. This coupling makes them increasingly complex and harder to generalize across different models. We present Budget EAGLE (Beagle), the first, to our knowledge, cross-attention-based Transformer decoder SD model that achieves performance on par with leading self-attention SD models (EAGLE-v2) while eliminating the need for pooling or auxiliary components, simplifying the architecture, improving training efficiency, and maintaining stable memory usage during training-time simulation. To enable effective training of this novel architecture, we propose Two-Stage Block-Attention Training, a new method that achieves training stability and convergence efficiency in block-level attention scenarios. Extensive experiments across multiple LLMs and datasets show that Beagle achieves competitive inference speedups and higher training efficiency than EAGLE-v2, offering a strong alternative for architectures in speculative decoding. 

**Abstract (ZH)**: Budget EAGLE：一种跨注意力机制的推测解码Transformer解码器模型及其训练方法 

---
# Localizing Persona Representations in LLMs 

**Title (ZH)**: LLMs中个性表示的定位 

**Authors**: Celia Cintas, Miriam Rateike, Erik Miehling, Elizabeth Daly, Skyler Speakman  

**Link**: [PDF](https://arxiv.org/pdf/2505.24539)  

**Abstract**: We present a study on how and where personas -- defined by distinct sets of human characteristics, values, and beliefs -- are encoded in the representation space of large language models (LLMs). Using a range of dimension reduction and pattern recognition methods, we first identify the model layers that show the greatest divergence in encoding these representations. We then analyze the activations within a selected layer to examine how specific personas are encoded relative to others, including their shared and distinct embedding spaces. We find that, across multiple pre-trained decoder-only LLMs, the analyzed personas show large differences in representation space only within the final third of the decoder layers. We observe overlapping activations for specific ethical perspectives -- such as moral nihilism and utilitarianism -- suggesting a degree of polysemy. In contrast, political ideologies like conservatism and liberalism appear to be represented in more distinct regions. These findings help to improve our understanding of how LLMs internally represent information and can inform future efforts in refining the modulation of specific human traits in LLM outputs. Warning: This paper includes potentially offensive sample statements. 

**Abstract (ZH)**: 我们研究了如何以及在何处通过不同的心理健康特征、价值观和信念（定义为人物角色）在大型语言模型（LLMs）的表示空间中进行编码。通过一系列降维和模式识别方法，我们首先确定表现出最大编码差异的模型层。然后，我们分析选定层内的激活，以检查特定人物角色相对于其他人物角色的编码情况，包括它们的共享和独特的嵌入空间。我们发现，在多个预训练的解码器型大型语言模型中，分析的人物角色在解码器层的最后三分之一中显示出较大的表示空间差异。我们观察到特定伦理视角（如道德虚无主义和功利主义）的重叠激活，表明一定程度的多义性。相比之下，政治意识形态（如保守主义和自由主义）似乎在更不同的区域得到表示。这些发现有助于我们更好地理解LLMs如何内部表示信息，并可为未来努力细化LLM输出中特定人类特质的调节提供指导。警告：本论文包括可能具有冒犯性的样本陈述。 

---
# Beyond Linear Steering: Unified Multi-Attribute Control for Language Models 

**Title (ZH)**: 超越线性 steering：统一的语言模型多属性控制 

**Authors**: Narmeen Oozeer, Luke Marks, Fazl Barez, Amirali Abdullah  

**Link**: [PDF](https://arxiv.org/pdf/2505.24535)  

**Abstract**: Controlling multiple behavioral attributes in large language models (LLMs) at inference time is a challenging problem due to interference between attributes and the limitations of linear steering methods, which assume additive behavior in activation space and require per-attribute tuning. We introduce K-Steering, a unified and flexible approach that trains a single non-linear multi-label classifier on hidden activations and computes intervention directions via gradients at inference time. This avoids linearity assumptions, removes the need for storing and tuning separate attribute vectors, and allows dynamic composition of behaviors without retraining. To evaluate our method, we propose two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral control. Empirical results across 3 model families, validated by both activation-based classifiers and LLM-based judges, demonstrate that K-Steering outperforms strong baselines in accurately steering multiple behaviors. 

**Abstract (ZH)**: 在推断时控制大型语言模型的多种行为属性是一个具有挑战性的问题，由于属性之间的相互干扰以及线性导向方法的限制，这些方法基于激活空间中的加性行为假设，并且需要为每个属性进行调优。我们提出了K-Steering，这是一种统一且灵活的方法，通过在隐藏激活上训练单个多标签非线性分类器，并在推断时通过梯度计算干预方向，从而避免了线性假设、消除了存储和调优单独属性向量的需要，并允许在无需重新训练的情况下动态组合行为。为了评估该方法，我们提出了两个新的基准测试，ToneBank和DebateMix，旨在实现组合行为控制。来自3个模型系列的实验结果，通过基于激活的分类器和基于大型语言模型的评估员验证，表明K-Steering在准确引导多种行为方面优于强基线。 

---
# Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors 

**Title (ZH)**: 机器生成文本检测的压力测试：使语言模型改变写作风格以欺骗检测器 

**Authors**: Andrea Pedrotti, Michele Papucci, Cristiano Ciaccio, Alessio Miaschi, Giovanni Puccetti, Felice Dell'Orletta, Andrea Esuli  

**Link**: [PDF](https://arxiv.org/pdf/2505.24523)  

**Abstract**: Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts. 

**Abstract (ZH)**: 最近生成人工智能和大型语言模型的进步使得创建高度真实的合成内容成为可能，这引起了关于恶意使用潜在风险的担忧，如 misinformation 和操纵。此外，由于缺乏评估现实世界场景泛化能力的稳健基准，检测机器生成文本（MGT）仍然是一个挑战。在本工作中，我们提出了一种测试最先进的 MGT 检测器（如 Mage、Radar、LLM-DetectAIve）对语言导向的对抗攻击的鲁棒性的流水线。为了挑战检测器，我们使用直接偏好优化（DPO）调整语言模型，使 MGT 的风格趋向于人类撰写文本（HWT）。这利用了检测器对风格线索的依赖性，使新的生成内容更难被检测。此外，我们分析了对齐过程中引起的语言变化以及检测器检测 MGT 文本时所使用的特征。我们的结果显示，检测器可以通过相对少量的示例被轻易欺骗，导致检测性能显著下降。这一结果强调了改进检测方法并使其能够应对未见过领域的文本的重要性。 

---
# Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting 

**Title (ZH)**: 慢思考的LLM能否进行时间推理？时间序列预测的实证研究 

**Authors**: Jiahao Wang, Mingyue Cheng, Qi Liu  

**Link**: [PDF](https://arxiv.org/pdf/2505.24511)  

**Abstract**: Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks. 

**Abstract (ZH)**: 时间序列 forecasting 中慢思考大语言模型的推理能力探究 

---
# TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence 

**Title (ZH)**: TimeHC-RL: 具有时间意识的分层认知强化学习提升LLMs的社会智能 

**Authors**: Guiyang Hou, Xing Gao, Yuchuan Wu, Xiang Huang, Wenqi Zhang, Zhe Zheng, Yongliang Shen, Jialu Du, Fei Huang, Yongbin Li, Weiming Lu  

**Link**: [PDF](https://arxiv.org/pdf/2505.24500)  

**Abstract**: Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights. 

**Abstract (ZH)**: Recent Progress and Underexplored Areas in Enhancing Large Language Models' Social Intelligence: Introducing Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) 

---
# Evaluating Gemini in an arena for learning 

**Title (ZH)**: 评估Gemini在学习竞技场中的表现 

**Authors**: LearnLM Team Google, Abhinit Modi, Aditya Srikanth Veerubhotla, Aliya Rysbek, Andrea Huber, Ankit Anand, Avishkar Bhoopchand, Brett Wiltshire, Daniel Gillick, Daniel Kasenberg, Eleni Sgouritsa, Gal Elidan, Hengrui Liu, Holger Winnemoeller, Irina Jurenka, James Cohan, Jennifer She, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Komal Singh, Lisa Wang, Markus Kunesch, Miruna Pîslar, Niv Efron, Parsa Mahmoudieh, Pierre-Alexandre Kamienny, Sara Wiltberger, Shakir Mohamed, Shashank Agarwal, Shubham Milind Phal, Sun Jae Lee, Theofilos Strinopoulos, Wei-Jen Ko, Yael Gold-Zamir, Yael Haramaty, Yannis Assael  

**Link**: [PDF](https://arxiv.org/pdf/2505.24477)  

**Abstract**: Artificial intelligence (AI) is poised to transform education, but the research community lacks a robust, general benchmark to evaluate AI models for learning. To assess state-of-the-art support for educational use cases, we ran an "arena for learning" where educators and pedagogy experts conduct blind, head-to-head, multi-turn comparisons of leading AI models. In particular, $N = 189$ educators drew from their experience to role-play realistic learning use cases, interacting with two models sequentially, after which $N = 206$ experts judged which model better supported the user's learning goals. The arena evaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7 Sonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro in 73.2% of these match-ups -- ranking it first overall in the arena. Gemini 2.5 Pro also demonstrated markedly higher performance across key principles of good pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading model for learning. 

**Abstract (ZH)**: 人工智能（AI）有望革新教育，但研究社区缺乏一个 robust、通用的基准来评估AI模型在学习中的应用。为了评估最先进的支持教育用途的技术，我们举办了一场“学习竞技场”，其中教育者和教学专家进行盲测、一对一、多轮次的领先AI模型对比。特别地，来自经验的189名教育者扮演了现实的学习场景，与两个模型依次交互，之后206名专家判断哪个模型更好地支持了用户的学习目标。竞技场评估了多个最先进的模型：Gemini 2.5 Pro、Claude 3.7 Sonnet、GPT-4o和OpenAI o3。不包括平局，在这些对战中，专家们更偏好Gemini 2.5 Pro，其比例高达73.2%，使其在竞技场中排名第一。Gemini 2.5 Pro还显著表现出更高的教学原则应用水平。这些结果使Gemini 2.5 Pro成为领先的學習模型。 

---
# LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs 

**Title (ZH)**: LPASS: 线性探针作为使用压缩大语言模型进行漏洞检测的 stepping stones 

**Authors**: Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux  

**Link**: [PDF](https://arxiv.org/pdf/2505.24451)  

**Abstract**: Large Language Models (LLMs) are being extensively used for cybersecurity purposes. One of them is the detection of vulnerable codes. For the sake of efficiency and effectiveness, compression and fine-tuning techniques are being developed, respectively. However, they involve spending substantial computational efforts. In this vein, we analyse how Linear Probes (LPs) can be used to provide an estimation on the performance of a compressed LLM at an early phase -- before fine-tuning. We also show their suitability to set the cut-off point when applying layer pruning compression. Our approach, dubbed $LPASS$, is applied in BERT and Gemma for the detection of 12 of MITRE's Top 25 most dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in 142.97 s. and provide key findings: (1) 33.3 \% and 72.2\% of layers can be removed, respectively, with no precision loss; (2) they provide an early estimate of the post-fine-tuning and post-compression model effectiveness, with 3\% and 8.68\% as the lowest and average precision errors, respectively. $LPASS$-based LLMs outperform the state of the art, reaching 86.9\% of accuracy in multi-class vulnerability detection. Interestingly, $LPASS$-based compressed versions of Gemma outperform the original ones by 1.6\% of F1-score at a maximum while saving 29.4 \% and 23.8\% of training and inference time and 42.98\% of model size. 

**Abstract (ZH)**: 大型语言模型（LLMs）在网络安全领域得到了广泛的应用，其中之一是检测脆弱的代码。为了提高效率和效果，正在开发压缩和微调技术，但这些技术需要大量的计算资源。为此，我们分析了线性探测（LPs）如何在微调之前的一个早期阶段提供压缩LLM性能的估计。我们还展示了它们在应用层剪枝压缩时作为截止点设置的适用性。我们的方法称为$LPASS$，在BERT和Gemma上对480,000个C/C++样本中的MITRE TOP 25最具危险性的12种漏洞进行了检测。LPs可以在142.97秒内计算出关键发现：（1）可以分别去除33.3%和72.2%的层而不会丢失精度；（2）它们可以提前估计微调后和压缩后模型的效果，精度误差分别为3%和8.68%。基于$LPASS$的LLMs在多类漏洞检测方面超越了最先进的方法，达到86.9%的准确率。有趣的是，基于$LPASS$压缩的Gemma版本在F1分数上最高可优于原版1.6%，同时节省了29.4%的训练和推理时间以及42.98%的模型大小。 

---
# Learning Safety Constraints for Large Language Models 

**Title (ZH)**: 为大型语言模型学习安全约束 

**Authors**: Xin Chen, Yarden As, Andreas Krause  

**Link**: [PDF](https://arxiv.org/pdf/2505.24445)  

**Abstract**: Large language models (LLMs) have emerged as powerful tools but pose significant safety risks through harmful outputs and vulnerability to adversarial attacks. We propose SaP, short for Safety Polytope, a geometric approach to LLM safety that learns and enforces multiple safety constraints directly in the model's representation space. We develop a framework that identifies safe and unsafe regions via the polytope's facets, enabling both detection and correction of unsafe outputs through geometric steering. Unlike existing approaches that modify model weights, SaP operates post-hoc in the representation space, preserving model capabilities while enforcing safety constraints. Experiments across multiple LLMs demonstrate that our method can effectively detect unethical inputs, reduce adversarial attack success rates while maintaining performance on standard tasks, thus highlighting the importance of having an explicit geometric model for safety. Analysis of the learned polytope facets reveals emergence of specialization in detecting different semantic notions of safety, providing interpretable insights into how safety is captured in LLMs' representation space. 

**Abstract (ZH)**: 大型语言模型（LLMs）作为强大的工具，但通过有害输出和对抗攻击的脆弱性，带来了显著的安全风险。我们提出Safety Polytope（安全多面体，简称SaP）作为一种几何方法，直接在模型的表示空间中学习和执行多重安全约束，以保障LLM安全。我们开发了一个框架，通过多面体的面来识别安全和不安全的区域，从而通过几何引导来检测和纠正不安全的输出。与修改模型权重的现有方法不同，SaP在表示空间中事后操作，保留模型能力的同时强制执行安全约束。在多个LLM上的实验表明，我们的方法可以有效检测不道德的输入、降低对抗攻击的成功率同时保持标准任务的性能，从而突显了拥有明确几何模型的重要性。分析学习到的多面体面揭示了不同语义层面的安全检测的专业化现象，提供了在LLM表示空间中如何捕获安全性的可解释见解。 

---
# LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory 

**Title (ZH)**: LLMs 是全局多语言但局部单语言：通过语言和思维理论探究知识迁移 

**Authors**: Eojin Kang, Juae Kim  

**Link**: [PDF](https://arxiv.org/pdf/2505.24409)  

**Abstract**: Multilingual large language models (LLMs) open up new possibilities for leveraging information across languages, but their factual knowledge recall remains inconsistent depending on the input language. While previous studies have attempted to address this issue through English-based prompting and evaluation, we explore non-English to English transfer via Language and Thought Theory. This perspective allows us to examine language-thought binding in LLMs and uncover why factual knowledge often fails to transfer effectively. We propose the Language-to-Thought (L2T) prompting strategy, which analyzes the relationship between input language, internal cognitive processes, and knowledge. Experimental results challenge the assumption that English-based approaches consistently outperform other languages and offer a novel insight that aligning the model's internal thought with the knowledge required for the task is critical for successful cross-lingual transfer. Furthermore, we show that applying L2T during training can alleviate LLMs' reliance on the input language and facilitate cross-linguistic knowledge integration without translation-based learning. Code and datasets will be available. 

**Abstract (ZH)**: 多语言大型语言模型（LLMs）为跨语言利用信息开启了新的可能性，但其事实知识回忆的一致性取决于输入语言。虽然先前的研究通过基于英语的提示和评估试图解决这一问题，我们探讨了通过语言和思维理论实现非英语到英语的转移。这种视角使我们能够考察LLMs中的语言-思维绑定并发现为何事实知识往往无法有效转移。我们提出了一种语言到思维（L2T）提示策略，该策略分析输入语言、内部认知过程和知识之间的关系。实验结果挑战了基于英语的方法始终优于其他语言的假设，提出了一个新的观点，即使模型的内部思维与任务所需的知识相一致对于成功的跨语言转移至关重要。此外，我们展示了在训练过程中应用L2T可以减轻LLMs对输入语言的依赖，促进跨语言知识整合而无需基于翻译的学习。代码和数据集将可供获取。 

---
# Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models 

**Title (ZH)**: 打破金标准：在大型语言模型中精确撤销学习下提取被遗忘的数据 

**Authors**: Xiaoyu Wu, Yifei Pang, Terrance Liu, Zhiwei Steven Wu  

**Link**: [PDF](https://arxiv.org/pdf/2505.24379)  

**Abstract**: Large language models are typically trained on datasets collected from the web, which may inadvertently contain harmful or sensitive personal information. To address growing privacy concerns, unlearning methods have been proposed to remove the influence of specific data from trained models. Of these, exact unlearning -- which retrains the model from scratch without the target data -- is widely regarded the gold standard, believed to be robust against privacy-related attacks. In this paper, we challenge this assumption by introducing a novel data extraction attack that compromises even exact unlearning. Our method leverages both the pre- and post-unlearning models: by guiding the post-unlearning model using signals from the pre-unlearning model, we uncover patterns that reflect the removed data distribution. Combining model guidance with a token filtering strategy, our attack significantly improves extraction success rates -- doubling performance in some cases -- across common benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our attack's effectiveness on a simulated medical diagnosis dataset to highlight real-world privacy risks associated with exact unlearning. In light of our findings, which suggest that unlearning may, in a contradictory way, increase the risk of privacy leakage, we advocate for evaluation of unlearning methods to consider broader threat models that account not only for post-unlearning models but also for adversarial access to prior checkpoints. 

**Abstract (ZH)**: 大规模语言模型通常在包含网络数据的数据集中训练，这些数据可能无意中包含有害或敏感的个人信息。为应对日益增长的隐私担忧，已经提出了遗忘方法来从训练模型中消除特定数据的影响。其中，精确遗忘——即不包含目标数据从头重新训练模型——被认为是最具抗御隐私攻击能力的标准方法。在本文中，我们通过引入一种新颖的数据提取攻击挑战这一假设，该攻击甚至能够突破精确遗忘的防线。我们的方法利用了预遗忘和后遗忘模型：通过使用预遗忘模型的信号来引导后遗忘模型，我们揭示了反映删除数据分布的模式。结合模型引导与令牌过滤策略，我们的攻击显著提高了提取成功率，在某些常见的基准测试如MUSE、TOFU和WMDP中，性能翻倍。此外，我们在一个模拟的医疗诊断数据集上展示了攻击的有效性，以突出精确遗忘可能带来的实际隐私风险。鉴于我们的发现表明，遗忘可能以一种矛盾的方式增加隐私泄露的风险，我们建议评估遗忘方法时应考虑更广泛的威胁模型，不仅考虑后遗忘模型，还考虑对手访问先前检查点的可能性。 

---
# Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering 

**Title (ZH)**: 基于网格的局部与全局区域识别方法：视频问答中的局部和全局区域文本转录 

**Authors**: Md Intisar Chowdhury, Kittinun Aukkapinyo, Hiroshi Fujimura, Joo Ann Woo, Wasu Wasusatein, Fadoua Ghourabi  

**Link**: [PDF](https://arxiv.org/pdf/2505.24371)  

**Abstract**: In this paper, we propose a Grid-based Local and Global Area Transcription (Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates in two phases. First, extracting text transcripts from video frames using a Vision-Language Model (VLM). Next, processing questions using these transcripts to generate answers through a Large Language Model (LLM). This design ensures image privacy by deploying the VLM on edge devices and the LLM in the cloud. To improve transcript quality, we propose grid-based visual prompting, which extracts intricate local details from each grid cell and integrates them with global information. Evaluation results show that Grid-LoGAT, using the open-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms state-of-the-art methods with similar baseline models on NExT-QA and STAR-QA datasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our method surpasses the non-grid version by 24 points on localization-based questions we created using NExT-QA. 

**Abstract (ZH)**: 基于网格的局部与全局区域转录（Grid-LoGAT）系统：面向视频问答（VideoQA）的设计与实现 

---
# Adversarial Preference Learning for Robust LLM Alignment 

**Title (ZH)**: 对抗偏好学习以实现稳健的LLM对齐 

**Authors**: Yuanfu Wang, Pengyu Wang, Chenyang Xi, Bo Tang, Junyi Zhu, Wenqiang Wei, Chen Chen, Chao Yang, Jingfeng Zhang, Chaochao Lu, Yijun Niu, Keming Mao, Zhiyu Li, Feiyu Xiong, Jie Hu, Mingchuan Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.24369)  

**Abstract**: Modern language models often rely on Reinforcement Learning from Human Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to adversarial attacks due to three key limitations: (1) the inefficiency and high cost of human annotation, (2) the vast diversity of potential adversarial attacks, and (3) the risk of feedback bias and reward hacking. To address these challenges, we introduce Adversarial Preference Learning (APL), an iterative adversarial training method incorporating three key innovations. First, a direct harmfulness metric based on the model's intrinsic preference probabilities, eliminating reliance on external assessment. Second, a conditional generative attacker that synthesizes input-specific adversarial variations. Third, an iterative framework with automated closed-loop feedback, enabling continuous adaptation through vulnerability discovery and mitigation. Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly enhances robustness, achieving 83.33% harmlessness win rate over the base model (evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured by LLaMA-Guard), and lowering attack success rate by up to 65% according to HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against the base model. 

**Abstract (ZH)**: 现代语言模型往往依赖人类反馈强化学习（RLHF）来鼓励安全行为。然而，它们仍因三大关键局限性而易受 adversarial 攻击：（1）人力标注的低效和高成本，（2）潜在 adversarial 攻击的高度多样性，（3）反馈偏差和奖励作弊的风险。为应对这些挑战，我们引入了对抗偏好学习（APL），这是一种迭代对抗训练方法，包含三项关键创新。首先，基于模型内在偏好概率的直接有害性度量，消除对外部评估的依赖。其次，生成条件型攻击者，合成输入特定的对抗变体。第三，迭代框架结合自动化闭环反馈，通过漏洞发现和缓解实现持续适应。实验证明，APL 显著提升了鲁棒性，在 Mistral-7B-Instruct-v0.3 上的有害性获胜率为 83.33%，相比于基线模型减少了有害输出（由 GPT-4o 评估，从 5.88% 降至 0.43%）和攻击成功率（根据 HarmBench 降低 65%）。值得注意的是，APL 维持了可竞争的实用性，MT-Bench 得分为 6.59（与基线的 6.78 相当）且 LC-WinRate 为 46.52%。 

---
# ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration 

**Title (ZH)**: ReCalKV: 通过头部重排序和离线校准的低秩键值缓存压缩 

**Authors**: Xianglong Yan, Zhiteng Li, Tianao Zhang, Linghe Kong, Yulun Zhang, Xiaokang Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.24357)  

**Abstract**: Large language models (LLMs) have achieved remarkable performance, yet their capability on long-context reasoning is often constrained by the excessive memory required to store the Key-Value (KV) cache. This makes KV cache compression an essential step toward enabling efficient long-context reasoning. Recent methods have explored reducing the hidden dimensions of the KV cache, but many introduce additional computation through projection layers or suffer from significant performance degradation under high compression ratios. To address these challenges, we propose ReCalKV, a post-training KV cache compression method that reduces the hidden dimensions of the KV cache. We develop distinct compression strategies for Keys and Values based on their different roles and varying importance in the attention mechanism. For Keys, we propose Head-wise Similarity-aware Reordering (HSR), which clusters similar heads and applies grouped SVD to the key projection matrix, reducing additional computation while preserving accuracy. For Values, we propose Offline Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra computational overhead. Experiments show that ReCalKV outperforms existing low-rank compression methods, achieving high compression ratios with minimal performance loss. Code is available at: this https URL. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在长上下文推理方面取得了显著性能，但其能力常常受限于存储Key-Value（KV）缓存所需的过多内存。因此，KV缓存压缩成为实现高效长上下文推理的关键步骤。最近的方法探索了减少KV缓存的隐藏维度，但许多方法通过投影层引入了额外的计算，或在高压缩比下导致显著的性能下降。为了解决这些挑战，我们提出ReCalKV，这是一种后训练的KV缓存压缩方法，通过减少KV缓存的隐藏维度来实现。我们根据不同角色和重要程度为Keys和Values开发了不同的压缩策略。对于Keys，我们提出了Head-wise Similarity-aware Reordering（HSR），通过聚类相似的heads并将键投影矩阵分组进行SVD，减少额外计算同时保持准确度。对于Values，我们提出了Offline Calibration and Matrix Fusion（OCMF），在不增加额外计算开销的情况下保持准确度。实验结果显示，ReCalKV在较低的性能损失下实现了高于现有低秩压缩方法的高压缩比。代码可在以下链接获取：this https URL。 

---
# Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings 

**Title (ZH)**: 探索有毒中文检测的多模态挑战：分类、基准和发现 

**Authors**: Shujian Yang, Shiyao Cui, Chuanrui Hu, Haicheng Wang, Tianwei Zhang, Minlie Huang, Jialiang Lu, Han Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2505.24341)  

**Abstract**: Detecting toxic content using language models is important but challenging. While large language models (LLMs) have demonstrated strong performance in understanding Chinese, recent studies show that simple character substitutions in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In this paper, we highlight the multimodal nature of Chinese language as a key challenge for deploying LLMs in toxic Chinese detection. First, we propose a taxonomy of 3 perturbation strategies and 8 specific approaches in toxic Chinese content. Then, we curate a dataset based on this taxonomy, and benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect perturbed toxic Chinese text. Additionally, we explore cost-effective enhancement solutions like in-context learning (ICL) and supervised fine-tuning (SFT). Our results reveal two important findings. (1) LLMs are less capable of detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a small number of perturbed examples may cause the LLMs "overcorrect'': misidentify many normal Chinese contents as toxic. 

**Abstract (ZH)**: 使用语言模型检测有毒内容至关重要但颇具挑战性：中文多模态特性的关键挑战及应对策略 

---
# AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning 

**Title (ZH)**: AReaL: 一种大规模异步语言推理强化学习系统 

**Authors**: Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu  

**Link**: [PDF](https://arxiv.org/pdf/2505.24298)  

**Abstract**: Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a \emph{fully asynchronous} RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training speedup} compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at this https URL. 

**Abstract (ZH)**: 强化学习（RL）已成为训练大规模语言模型（LLMs）的热点范式，尤其是用于推理任务。对于LLMs的高效RL训练迫切需要高效的训练系统。现有的大多数大规模RL系统是同步的，通过批量设置交替生成和训练，其中每个训练批次中的rollout均由同一模型（或最新模型）生成。这虽然稳定了RL训练，但也严重降低了系统效率。生成必须等待批次中最长输出完成之后才能进行模型更新，导致GPU利用率低下。我们提出了一种名为AReaL的完全异步RL系统，彻底将生成与训练脱钩。AReaL中的rollout工人连续生成新的输出而无需等待，训练工人则在收集到数据批次时更新模型。AReaL还包含了一系列系统级优化，显著提高了GPU利用率。为稳定RL训练，AReaL平衡了rollout和训练工人的工作量以控制数据陈旧度，并采用了一种陈旧度增强的PPO变体以更好地处理过时的训练样本。在数学和代码推理基准上的 extensive 实验表明，AReaL 在相同数量的GPU下相比最佳同步系统实现了 **高达2.57倍的训练加速**，且最终性能相当甚至更好。AReaL的代码可从 [此链接](this https URL) 获取。 

---
# Large Language Models are Locally Linear Mappings 

**Title (ZH)**: 大型语言模型是局部线性映射 

**Authors**: James R. Golden  

**Link**: [PDF](https://arxiv.org/pdf/2505.24293)  

**Abstract**: We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process. 

**Abstract (ZH)**: 我们证明了多个开源权重大规模语言模型（LLMs）的推理操作可以映射到一个与输入序列完全等价的线性系统中，无需修改模型权重或改变输出预测。通过扩展图像扩散模型中局部或分段线性特性相关技术，我们针对给定输入序列的下一个标记预测战略地修改了梯度计算，使模型的雅可比矩阵几乎精确地重现前向预测的线性系统。我们跨多个模型（Llama 3、Gemma 3、Qwen 3、Phi 4、Mistral、Ministral 和 OLMo 2，直至 Llama 3.3 70B Q4）展示了这种方法，并通过分离雅可比的奇异值分解证明了这些LLMs在极低维度子空间中运行，其中许多最大的奇异向量解码为与最有可能的输出标记相关的概念。该方法还允许我们逐层（及其注意力和MLP组件）近似视为线性系统，并观察语义概念的涌现。尽管现代LLMs具有强大的表达能力和全局非线性，但它们可以通过近似局部线性分解来解释，从而揭示其内部表示并揭示下一个标记预测过程中的可解释语义结构。 

---
# Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations 

**Title (ZH)**: 忠实且 robust 的基于大规模语言模型的定理证明用于 NLI 解释 

**Authors**: Xin Quan, Marco Valentino, Louise A. Dennis, André Freitas  

**Link**: [PDF](https://arxiv.org/pdf/2505.24264)  

**Abstract**: Natural language explanations play a fundamental role in Natural Language Inference (NLI) by revealing how premises logically entail hypotheses. Recent work has shown that the interaction of large language models (LLMs) with theorem provers (TPs) can help verify and improve the validity of NLI explanations. However, TPs require translating natural language into machine-verifiable formal representations, a process that introduces the risk of semantic information loss and unfaithful interpretation, an issue compounded by LLMs' challenges in capturing critical logical structures with sufficient precision. Moreover, LLMs are still limited in their capacity for rigorous and robust proof construction within formal verification frameworks. To mitigate issues related to faithfulness and robustness, this paper investigates strategies to (1) alleviate semantic loss during autoformalisation, (2) efficiently identify and correct syntactic errors in logical representations, (3) explicitly use logical expressions to guide LLMs in generating structured proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree using different LLMs demonstrate that the proposed strategies yield significant improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover, we show that specific interventions on the hybrid LLM-TP architecture can substantially improve efficiency, drastically reducing the number of iterations required for successful verification. 

**Abstract (ZH)**: 自然语言解释在自然语言推理中的基础作用：大型语言模型与定理证明器的交互促进解释验证与改进 

---
# Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games 

**Title (ZH)**: 思维理论和亲社会信念对 Ultimatum 游戏中 LLMs 人类对齐行为的影响 

**Authors**: Neemesh Yadav, Palakorn Achananuparp, Jing Jiang, Ee-Peng Lim  

**Link**: [PDF](https://arxiv.org/pdf/2505.24255)  

**Abstract**: Large Language Models (LLMs) have shown potential in simulating human behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for complex social interactions. In this study, we investigate the role of ToM reasoning in aligning agentic behaviors with human norms in negotiation tasks, using the ultimatum game as a controlled environment. We initialized LLM agents with different prosocial beliefs (including Greedy, Fair, and Selfless) and reasoning methods like chain-of-thought (CoT) and varying ToM levels, and examined their decision-making processes across diverse LLMs, including reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from 2,700 simulations indicated that ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes. Consistent with previous findings, reasoning models exhibit limited capability compared to models with ToM reasoning, different roles of the game benefits with different orders of ToM reasoning. Our findings contribute to the understanding of ToM's role in enhancing human-AI interaction and cooperative decision-making. The code used for our experiments can be found at this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）在模拟人类行为和执行理论%E2%80%93of%E2%80%93mind（ToM）推理方面显示出潜力，这是复杂社交互动中的一项关键技能。本研究通过使用 ultimatum 游戏作为受控环境，探讨 ToM 推理在谈判任务中使代理行为与人类规范保持一致的作用。我们初始化了具有不同利他主义信念（包括贪婪、公正和无私）和推理方法（如链式思考）的不同 ToM 水平的 LLM 代理，并在包括推理模型 o3-mini 和 DeepSeek-R1 Distilled Qwen 32B 等多种 LLM 中研究了它们的决策过程。2,700 次模拟的结果表明，ToM 推理增强了行为一致性、决策一致性和谈判结果。与先前的研究一致，具有 ToM 推理的模型的性能优于仅具有推理能力的模型，不同的博弈收益顺序和 ToM 推理的角色也有所不同。我们的研究成果有助于理解 ToM 在增强人机互动和合作决策中的作用。实验使用的代码可以在此处找到：https://xxxxxx。 

---
# A Reward-driven Automated Webshell Malicious-code Generator for Red-teaming 

**Title (ZH)**: 基于奖励驱动的自动化Webshell恶意代码生成器用于红队演练 

**Authors**: Yizhong Ding  

**Link**: [PDF](https://arxiv.org/pdf/2505.24252)  

**Abstract**: Frequent cyber-attacks have elevated WebShell exploitation and defense to a critical research focus within network security. However, there remains a significant shortage of publicly available, well-categorized malicious-code datasets organized by obfuscation method. Existing malicious-code generation methods, which primarily rely on prompt engineering, often suffer from limited diversity and high redundancy in the payloads they produce. To address these limitations, we propose \textbf{RAWG}, a \textbf{R}eward-driven \textbf{A}utomated \textbf{W}ebshell Malicious-code \textbf{G}enerator designed for red-teaming applications. Our approach begins by categorizing webshell samples from common datasets into seven distinct types of obfuscation. We then employ a large language model (LLM) to extract and normalize key tokens from each sample, creating a standardized, high-quality corpus. Using this curated dataset, we perform supervised fine-tuning (SFT) on an open-source large model to enable the generation of diverse, highly obfuscated webshell malicious payloads. To further enhance generation quality, we apply Proximal Policy Optimization (PPO), treating malicious-code samples as "chosen" data and benign code as "rejected" data during reinforcement learning. Extensive experiments demonstrate that RAWG significantly outperforms current state-of-the-art methods in both payload diversity and escape effectiveness. 

**Abstract (ZH)**: 频繁的网络攻击已将WebShell利用与防御提升为网络安全性研究的关键关注点。然而，公开 availability 的、按混淆方法分类的恶意代码数据集仍然严重缺乏。现有的恶意代码生成方法主要依赖提示工程，往往在生成的样本中表现出多样性有限和重复性高的问题。为解决这些限制，我们提出了一种名为RAWG的奖励驱动的自动化WebShell恶意代码生成器，旨在支持红队应用。我们的方法首先将常见数据集中WebShell样本分类为七种不同的混淆类型。然后利用大型语言模型（LLM）从每个样本中提取和规范化关键标记，创建一个标准化且高质量的语料库。利用这个精选的数据集，我们对开源大型模型进行监督微调（SFT），以实现生成多样且高度混淆的WebShell恶意载荷。为了进一步提升生成质量，在强化学习中我们将Proximal Policy Optimization (PPO) 应用于恶意代码样本作为“选择”数据和良性代码作为“拒绝”数据。广泛的实验表明，RAWG在载荷多样性与逃逸有效性方面显著优于当前最先进的方法。 

---
# An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring 

**Title (ZH)**: 基于可信度评分的抗对手多Agent大型语言模型系统 

**Authors**: Sana Ebrahimi, Mohsen Dehghankar, Abolfazl Asudeh  

**Link**: [PDF](https://arxiv.org/pdf/2505.24239)  

**Abstract**: While multi-agent LLM systems show strong capabilities in various domains, they are highly vulnerable to adversarial and low-performing agents. To resolve this issue, in this paper, we introduce a general and adversary-resistant multi-agent LLM framework based on credibility scoring. We model the collaborative query-answering process as an iterative game, where the agents communicate and contribute to a final system output. Our system associates a credibility score that is used when aggregating the team outputs. The credibility scores are learned gradually based on the past contributions of each agent in query answering. Our experiments across multiple tasks and settings demonstrate our system's effectiveness in mitigating adversarial influence and enhancing the resilience of multi-agent cooperation, even in the adversary-majority settings. 

**Abstract (ZH)**: 基于可信度评分的抗敌对攻击多智能体大语言模型框架 

---
# From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models 

**Title (ZH)**: 从幻觉到逃逸：重新思考大型基础模型的安全性 

**Authors**: Haibo Jin, Peiyan Zhang, Peiran Wang, Man Luo, Haohan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.24232)  

**Abstract**: Large foundation models (LFMs) are susceptible to two distinct vulnerabilities: hallucinations and jailbreak attacks. While typically studied in isolation, we observe that defenses targeting one often affect the other, hinting at a deeper connection.
We propose a unified theoretical framework that models jailbreaks as token-level optimization and hallucinations as attention-level optimization. Within this framework, we establish two key propositions: (1) \textit{Similar Loss Convergence} - the loss functions for both vulnerabilities converge similarly when optimizing for target-specific outputs; and (2) \textit{Gradient Consistency in Attention Redistribution} - both exhibit consistent gradient behavior driven by shared attention dynamics.
We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4, showing consistent optimization trends and aligned gradients. Leveraging this connection, we demonstrate that mitigation techniques for hallucinations can reduce jailbreak success rates, and vice versa. Our findings reveal a shared failure mode in LFMs and suggest that robustness strategies should jointly address both vulnerabilities. 

**Abstract (ZH)**: 大型基础模型（LFMs）存在两种不同的脆弱性：幻觉和 Jailbreak 攻击。虽然通常分别研究，但我们观察到针对一种脆弱性的防御措施往往会影响另一种，暗示两者之间存在更深层的联系。 

---
# Reasoning Can Hurt the Inductive Abilities of Large Language Models 

**Title (ZH)**: 推理可能会损害大型语言模型的归纳能力 

**Authors**: Haibo Jin, Peiyan Zhang, Man Luo, Haohan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.24225)  

**Abstract**: Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning - inferring latent rules from sparse examples - remains limited. It is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. We investigate this assumption with creating four controlled, diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack - with hidden human-defined rules. We find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts.
To explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. Based on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各个领域取得了显著进展，但在归纳推理能力——从稀疏样本中推断潜在规则——方面仍有限制。通常假定，大型推理模型（LRMs）中的链式思考（CoT）提示能够增强这种推理能力。我们通过创建四种受控的诊断游戏任务——国际象棋、德州扑克、骰子游戏和 blackjack——来验证这一假设，这些任务都隐藏了人类定义的规则。我们发现，CoT 推理可能会降低归纳性能，LRMs 往往不如非推理对照组表现良好。

为了解释这一现象，我们提出了一种理论框架，揭示了推理步骤如何通过三种故障模式放大错误：不正确的子任务分解、不正确的子任务解决以及不正确的最终答案总结。基于我们的理论和实证分析，我们提出了结构化的干预措施，根据我们识别的失败类型适应CoT生成。这些干预措施提高了归纳准确性，而无需重新训练。我们的研究结果表明，有效的（CoT）推理不仅依赖于采取更多的步骤，还依赖于确保这些步骤结构良好。 

---
# Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows 

**Title (ZH)**: 调优SLM或提示LLM？生成低代码工作流的案例研究 

**Authors**: Orlando Marquez Ayala, Patrice Bechard, Emily Chen, Maggie Baird, Jingfei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2505.24189)  

**Abstract**: Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations. 

**Abstract (ZH)**: 大型语言模型（LLMs）如GPT-4可以借助合适的提示处理广泛的复杂任务。随着按token成本的降低，细调小型语言模型（SLMs）在实际应用中的优势——更快的推理、更低的成本——可能不再明显。在本文中，我们提供了证据表明，对于需要结构化输出的领域特定任务，SLMs仍然具有质量优势。我们将细调SLMs与使用LLMs进行任务提示进行比较，该任务是生成JSON形式的低代码工作流。我们观察到，尽管一个好的提示可以产生合理的结果，但细调平均提高了10%的质量。我们还进行了系统的错误分析以揭示模型的局限性。 

---
# SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling 

**Title (ZH)**: SALE：高效的长上下文LLM预填充中稀疏注意机制的低比特估计 

**Authors**: Xiaodong Ji, Hailin Zhang, Fangcheng Fu, Bin Cui  

**Link**: [PDF](https://arxiv.org/pdf/2505.24179)  

**Abstract**: Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality. 

**Abstract (ZH)**: SALE：一种在长上下文预填充阶段加速大规模语言模型细粒度稀疏注意力的方法 

---
# LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing 

**Title (ZH)**: LKD-KGC：通过LLM驱动的知识依赖解析实现的领域特定知识图谱构建 

**Authors**: Jiaqi Sun, Shiyou Qian, Zhangchi Han, Wei Li, Zelin Qian, Dingyu Yang, Jian Cao, Guangtao Xue  

**Link**: [PDF](https://arxiv.org/pdf/2505.24163)  

**Abstract**: Knowledge Graphs (KGs) structure real-world entities and their relationships into triples, enhancing machine reasoning for various tasks. While domain-specific KGs offer substantial benefits, their manual construction is often inefficient and requires specialized knowledge. Recent approaches for knowledge graph construction (KGC) based on large language models (LLMs), such as schema-guided KGC and reference knowledge integration, have proven efficient. However, these methods are constrained by their reliance on manually defined schema, single-document processing, and public-domain references, making them less effective for domain-specific corpora that exhibit complex knowledge dependencies and specificity, as well as limited reference knowledge. To address these challenges, we propose LKD-KGC, a novel framework for unsupervised domain-specific KG construction. LKD-KGC autonomously analyzes document repositories to infer knowledge dependencies, determines optimal processing sequences via LLM driven prioritization, and autoregressively generates entity schema by integrating hierarchical inter-document contexts. This schema guides the unsupervised extraction of entities and relationships, eliminating reliance on predefined structures or external knowledge. Extensive experiments show that compared with state-of-the-art baselines, LKD-KGC generally achieves improvements of 10% to 20% in both precision and recall rate, demonstrating its potential in constructing high-quality domain-specific KGs. 

**Abstract (ZH)**: 一种用于无监督领域特定知识图构建的新框架：LKD-KGC 

---
# AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits 

**Title (ZH)**: AMSbench：评估AMS电路中MLLM能力的综合性基准 

**Authors**: Yichen Shi, Ze Zhang, Hongyang Wang, Zhuofu Tao, Zhongyi Li, Bingyu Chen, Yaxin Wang, Zhiping Yu, Ting-Jung Lin, Lei He  

**Link**: [PDF](https://arxiv.org/pdf/2505.24138)  

**Abstract**: Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit design has remained a longstanding challenge due to its difficulty and complexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer promising potential for supporting AMS circuit analysis and design. However, current research typically evaluates MLLMs on isolated tasks within the domain, lacking a comprehensive benchmark that systematically assesses model capabilities across diverse AMS-related challenges. To address this gap, we introduce AMSbench, a benchmark suite designed to evaluate MLLM performance across critical tasks including circuit schematic perception, circuit analysis, and circuit design. AMSbench comprises approximately 8000 test questions spanning multiple difficulty levels and assesses eight prominent models, encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and Gemini 2.5 Pro. Our evaluation highlights significant limitations in current MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit design tasks. These results underscore the necessity of advancing MLLMs' understanding and effective application of circuit-specific knowledge, thereby narrowing the existing performance gap relative to human expertise and moving toward fully automated AMS circuit design workflows. Our data is released at this https URL 

**Abstract (ZH)**: Analog/M Mixed-Signal (AMS) 电路在集成电路（IC）行业中发挥着关键作用。然而，由于其复杂性，自动化 Analog/Mixed-Signal (AMS) 电路设计一直是一个长期挑战。近期多模态大规模语言模型（MLLMs）的进步为支持 AMS 电路分析和设计带来了希望。然而，当前研究通常在单一任务上评估 MLLMs，缺乏一个系统评估模型在多类 AMS 相关挑战上的综合基准。为填补这一空白，我们引入了 AMSbench，这是一个基准套件，旨在评估 MLLM 在电路示意图感知、电路分析和电路设计等关键任务上的性能。AMSbench 包含约 8000 个测试问题，涵盖多个难度级别，并评估了八种主流模型，包括开源和专有解决方案如 Qwen 2.5-VL 和 Gemini 2.5 Pro。我们的评估突显了当前 MLLMs 的主要局限性，特别是在复杂多模态推理和复杂电路设计任务上的局限性。这些结果强调了推进 MLLMs 对电路特定知识的理解和有效应用的必要性，从而缩小与人类专家表现的差距，并朝着完全自动化的 AMS 电路设计流程迈进。我们的数据在此 https:// link 中发布。 

---
# DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures 

**Title (ZH)**: DSR-Bench：通过数据结构评估LLM的结构性推理能力 

**Authors**: Yu He, Yingxi Li, Colin White, Ellen Vitercik  

**Link**: [PDF](https://arxiv.org/pdf/2505.24069)  

**Abstract**: Large language models (LLMs) are increasingly deployed for real-world tasks that fundamentally involve data manipulation. A core requirement across these tasks is the ability to perform structural reasoning--that is, to understand and reason about data relationships. For example, customer requests require a temporal ordering, which can be represented by data structures such as queues. However, existing benchmarks primarily focus on high-level, application-driven evaluations without isolating this fundamental capability. To address this gap, we introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning capabilities through data structures, which provide interpretable representations of data relationships. DSR-Bench includes 20 data structures, 35 operations, and 4,140 problem instances, organized hierarchically for fine-grained analysis of reasoning limitations. Our evaluation pipeline is fully automated and deterministic, eliminating subjective human or model-based judgments. Its synthetic nature also ensures scalability and minimizes data contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis shows that instruction-tuned models struggle with basic multi-attribute and multi-hop reasoning. Furthermore, while reasoning-oriented models perform better, they remain fragile on complex and hybrid structures, with the best model achieving an average score of only 47% on the challenge subset. Crucially, models often perform poorly on multi-dimensional data and natural language task descriptions, highlighting a critical gap for real-world deployment. 

**Abstract (ZH)**: 大型语言模型（LLMs）日益被用于涉及数据操作的基本任务。这些任务的核心要求之一是能够进行结构推理，即理解并推理数据关系的能力。例如，客户请求需要时间上的顺序，这可以通过队列等数据结构来表示。然而，现有的基准测试主要集中在高层次的应用驱动评估上，而没有将这种基本能力隔离出来。为了解决这一差距，我们引入了DSR-Bench，这是一个新的基准测试，通过数据结构来评估LLMs的结构推理能力，这些数据结构可以提供数据关系的可解释表示。DSR-Bench包含20种数据结构、35种操作以及4,140个问题实例，并分层组织以细粒度分析推理限制。我们的评估管道是全自动和确定性的，消除了主观的人为或模型判断。其合成的特性也确保了可扩展性，并减少了数据污染的风险。我们对九个最先进的LLMs进行了基准测试。我们的分析显示，指令调优模型在基本的多属性和多跳推理方面存在困难。此外，虽然推理导向的模型表现更好，但在复杂和混合结构上仍然脆弱，最佳模型在挑战子集上的平均得分为47%。 crucially，模型在多维数据和自然语言任务描述方面表现不佳，突显了实际部署中的关键差距。 

---
# MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering 

**Title (ZH)**: MedPAIR: 评估医生和AI在医学问答中的相关性一致性 

**Authors**: Yuexing Hao, Kumail Alhamoud, Hyewon Jeong, Haoran Zhang, Isha Puri, Philip Torr, Mike Schaekermann, Ariel D. Stern, Marzyeh Ghassemi  

**Link**: [PDF](https://arxiv.org/pdf/2505.24040)  

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable performance on various medical question-answering (QA) benchmarks, including standardized medical exams. However, correct answers alone do not ensure correct logic, and models may reach accurate conclusions through flawed processes. In this study, we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance Estimation and Question Answering) dataset to evaluate how physician trainees and LLMs prioritize relevant information when answering QA questions. We obtain annotations on 1,300 QA pairs from 36 physician trainees, labeling each sentence within the question components for relevance. We compare these relevance estimates to those for LLMs, and further evaluate the impact of these "relevant" subsets on downstream task performance for both physician trainees and LLMs. We find that LLMs are frequently not aligned with the content relevance estimates of physician trainees. After filtering out physician trainee-labeled irrelevant sentences, accuracy improves for both the trainees and the LLMs. All LLM and physician trainee-labeled data are available at: this http URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各种医学问答（QA）基准测试，包括标准化医学考试中展现了 remarkable 的性能。然而，仅正确答案不能保证正确的逻辑过程，模型可能通过错误的过程得出正确的结论。本研究引入 MedPAIR（医学数据集，对比医生和AI的相关性估计与问答）数据集，以评估医生实习生和LLMs在回答医学问答问题时如何优先考虑相关信息。我们从36名医生实习生中获得了1,300个问答对的标注，为每个问题组件内的句子标注了相关性标签，并将这些相关性估计与LLMs的进行对比，进一步评估这些“相关”子集对医生实习生和LLMs下游任务性能的影响。我们发现，LLMs与医生实习生的内容相关性估计往往不一致。在过滤掉医生实习生标注的无关句子后，两者的表现准确性都得到了提高。所有医生实习生和LLMs标注的数据均可在以下链接获取：this http URL。 

---
# LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin 

**Title (ZH)**: LlamaRL：一种高效的大规模语言模型分布式异步 reinforcement learning 框架 

**Authors**: Bo Wu, Sid Wang, Yunhao Tang, Jia Ding, Eryk Helenowski, Liang Tan, Tengyu Xu, Tushar Gowda, Zhengxing Chen, Chen Zhu, Xiaocheng Tang, Yundi Qian, Beibei Zhu, Rui Hou  

**Link**: [PDF](https://arxiv.org/pdf/2505.24034)  

**Abstract**: Reinforcement Learning (RL) has become the most effective post-training approach for improving the capabilities of Large Language Models (LLMs). In practice, because of the high demands on latency and memory, it is particularly challenging to develop an efficient RL framework that reliably manages policy models with hundreds to thousands of billions of parameters.
In this paper, we present LlamaRL, a fully distributed, asynchronous RL framework optimized for efficient training of large-scale LLMs with various model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a handful to thousands of devices. LlamaRL introduces a streamlined, single-controller architecture built entirely on native PyTorch, enabling modularity, ease of use, and seamless scalability to thousands of GPUs. We also provide a theoretical analysis of LlamaRL's efficiency, including a formal proof that its asynchronous design leads to strict RL speed-up. Empirically, by leveraging best practices such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. Furthermore, the efficiency advantage continues to grow with increasing model scale, demonstrating the framework's suitability for future large-scale RL training. 

**Abstract (ZH)**: 强化学习（RL）已成为提高大型语言模型（LLMs）能力的最有效后训练方法。然而，由于对延迟和内存的高要求，开发一个高效且可靠的RL框架来管理从数十亿到数千亿参数的策略模型极具挑战性。

在本文中，我们提出LlamaRL，这是一个针对GPU集群（从少量到数千个设备）高效训练各种模型规模（8B、70B和405B参数）的大规模LLMs进行了优化的完全分布异步RL框架。LlamaRL引入了一种基于原生PyTorch的简洁的单一控制器架构，实现了模块化、易用性和无缝扩展到数千个GPU。我们还提供了LlamaRL效率的理论分析，包括异步设计严格提高RL速度的正式证明。通过利用如模型并置卸载、异步离策略训练和分布式直接内存访问同步权重等最佳实践，LlamaRL实现了显著的效率提升——相对于DeepSpeed-Chat类似的系统，在一个405B参数的策略模型上，速度提升高达10.7倍。此外，随着模型规模的增加，效率优势持续增长，展示了该框架适用于未来大规模RL训练的适用性。 

---
# DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models 

**Title (ZH)**: DINO-R1：激励视觉基础模型的推理能力 

**Authors**: Chenbin Pan, Wenbin He, Zhengzhong Tu, Liu Ren  

**Link**: [PDF](https://arxiv.org/pdf/2505.24025)  

**Abstract**: The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose \textbf{DINO-R1}, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces \textbf{Group Relative Query Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios. 

**Abstract (ZH)**: 基于强化学习激励视觉上下文推理能力的DINO-R1 

---
# LLM Agents Should Employ Security Principles 

**Title (ZH)**: LLM代理应采用安全原则 

**Authors**: Kaiyuan Zhang, Zian Su, Pin-Yu Chen, Elisa Bertino, Xiangyu Zhang, Ninghui Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.24019)  

**Abstract**: Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system's susceptibility to prompt injection and other forms of context manipulation introduce new vulnerabilities related to privacy leakage and system exploitation. This position paper argues that the well-established design principles in information security, which are commonly referred to as security principles, should be employed when deploying LLM agents at scale. Design principles such as defense-in-depth, least privilege, complete mediation, and psychological acceptability have helped guide the design of mechanisms for securing information systems over the last five decades, and we argue that their explicit and conscientious adoption will help secure agentic systems. To illustrate this approach, we introduce AgentSandbox, a conceptual framework embedding these security principles to provide safeguards throughout an agent's life-cycle. We evaluate with state-of-the-art LLMs along three dimensions: benign utility, attack utility, and attack success rate. AgentSandbox maintains high utility for its intended functions under both benign and adversarial evaluations while substantially mitigating privacy risks. By embedding secure design principles as foundational elements within emerging LLM agent protocols, we aim to promote trustworthy agent ecosystems aligned with user privacy expectations and evolving regulatory requirements. 

**Abstract (ZH)**: 大型语言模型（LLM）代理在利用上下文推理自动化复杂任务方面显示出巨大潜力；然而，涉及多个代理的交互和系统对提示注入和其他形式的上下文操纵的敏感性引入了与隐私泄露和系统滥用相关的新漏洞。本文认为，在大规模部署LLM代理时应采用信息安全管理中常见的基本安全原则。这些包括纵深防御、最小权限、完全中介和心理可接受性等设计原则，在过去的五十年中指导了信息安全机制的设计，并坚信它们的明确定义和审慎采用将有助于保护代理系统。为了说明这种方法，我们引入了AgentSandbox，一种嵌入这些安全原则的概念框架，以在整个代理生命周期中提供保护措施。我们使用最先进的LLM模型在三个维度上进行了评估：良性效用、攻击效用和攻击成功率。AgentSandbox在良性评估和对抗性评估下都能保持其预期功能的高度效用，同时大幅降低隐私风险。通过将安全设计原则作为新兴LLM代理协议的基础元素，我们旨在促进符合用户隐私期望和 evolving监管要求的信任代理生态系统。 

---
# Large Language Model Meets Constraint Propagation 

**Title (ZH)**: 大规模语言模型与约束传播相遇 

**Authors**: Alexandre Bonlarron, Florian Régin, Elisabetta De Maria, Jean-Charles Régin  

**Link**: [PDF](https://arxiv.org/pdf/2505.24012)  

**Abstract**: Large Language Models (LLMs) excel at generating fluent text but struggle to enforce external constraints because they generate tokens sequentially without explicit control mechanisms. GenCP addresses this limitation by combining LLM predictions with Constraint Programming (CP) reasoning, formulating text generation as a Constraint Satisfaction Problem (CSP). In this paper, we improve GenCP by integrating Masked Language Models (MLMs) for domain generation, which allows bidirectional constraint propagation that leverages both past and future tokens. This integration bridges the gap between token-level prediction and structured constraint enforcement, leading to more reliable and constraint-aware text generation. Our evaluation on COLLIE benchmarks demonstrates that incorporating domain preview via MLM calls significantly improves GenCP's performance. Although this approach incurs additional MLM calls and, in some cases, increased backtracking, the overall effect is a more efficient use of LLM inferences and an enhanced ability to generate feasible and meaningful solutions, particularly in tasks with strict content constraints. 

**Abstract (ZH)**: 大型语言模型（LLMs）在生成流畅文本方面表现出色，但在强制执行外部约束方面存在困难，因为它们是通过顺序生成词元而没有明确的控制机制。GenCP通过将LLM预测与约束编程(CP)推理相结合，将文本生成 formulations 为约束满足问题(CSP)来解决这一局限性。在本文中，我们通过集成掩蔽语言模型(MLMs)来增强GenCP的领域生成能力，这允许双向约束传播，利用既有词元和未来词元。这种集成在词元级预测和结构化约束强制执行之间建立了桥梁，从而产生了更可靠且更具约束意识的文本生成。我们在COLLIE基准上的评估表明，通过MLM调用进行领域预览显著提高了GenCP的性能。尽管这种方法会增加额外的MLM调用，甚至在某些情况下增加回溯，但总体而言，这促进了LLM推理的更高效使用，并增强了生成可行且有意义解决方案的能力，尤其是在有严格内容约束的任务中。 

---
# Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws 

**Title (ZH)**: Transformer 层的多样性：参数缩放法则的一个方面 

**Authors**: Hidetaka Kamigaito, Ying Zhang, Jingun Kwon, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe  

**Link**: [PDF](https://arxiv.org/pdf/2505.24009)  

**Abstract**: Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study. 

**Abstract (ZH)**: Transformer架构在广泛的任务中展现出卓越的性能，现已成为大型语言模型（LLMs）的主要骨干架构。尽管参数缩放规律的研究表明增加参数量可以提升其任务解决性能，但近期关于其内在机制的机制可解释性研究并没有完全阐明这些内在机制与参数缩放规律之间的关系。为填补这一空白，我们集中在决定Transformer参数量的层及其规模上。为此，我们首先通过偏差多样性分解理论研究残差流中的层。分解将偏差（即每层输出与真实值之间的误差）和多样性（即每层输出之间的差异性）分开。在该理论下分析Transformer显示，当每层的预测接近正确答案并保持相互差异时，性能会提升。我们表明，当每层的输出远离真实值时，多样性变得尤为重要。最终，我们引入了信息论多样性，并展示了主要发现：只有当新增的层表现出差异性（即多样化）时，增加层才能提升性能。我们还揭示了通过增加层数获得的性能增益具有子模性：随着新增层的增加，边际改进逐渐减少，这与参数缩放规律预测的对数收敛相吻合。在多种语义理解任务上使用不同LLM的实验验证了本研究中理论性质的实证结果。 

---
# Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs 

**Title (ZH)**: 你的模型有多公平？面向LLMs的不确定性感知公平性评估 

**Authors**: Yinong Oliver Wang, Nivedha Sivakumar, Falaah Arif Khan, Rin Metcalf Susa, Adam Golinski, Natalie Mackraz, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff  

**Link**: [PDF](https://arxiv.org/pdf/2505.23996)  

**Abstract**: The recent rapid adoption of large language models (LLMs) highlights the critical need for benchmarking their fairness. Conventional fairness metrics, which focus on discrete accuracy-based evaluations (i.e., prediction correctness), fail to capture the implicit impact of model uncertainty (e.g., higher model confidence about one group over another despite similar accuracy). To address this limitation, we propose an uncertainty-aware fairness metric, UCerF, to enable a fine-grained evaluation of model fairness that is more reflective of the internal bias in model decisions compared to conventional fairness measures. Furthermore, observing data size, diversity, and clarity issues in current datasets, we introduce a new gender-occupation fairness evaluation dataset with 31,756 samples for co-reference resolution, offering a more diverse and suitable dataset for evaluating modern LLMs. We establish a benchmark, using our metric and dataset, and apply it to evaluate the behavior of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness due to high confidence in incorrect predictions, a detail overlooked by Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark, which evaluates fairness with uncertainty awareness, paves the way for developing more transparent and accountable AI systems. 

**Abstract (ZH)**: 近期大型语言模型（LLMs）的迅速采用凸显了对其公平性进行基准测试的紧迫需求。传统公平性度量关注基于离散准确性的评估（即预测正确性），未能捕捉到模型不确定性的影响（如，在相似准确性的条件下，模型对某一组比另一组更具信心）。为解决这一局限，我们提出了一种基于不确定性的公平性度量UCerF，以实现对模型公平性的细粒度评估，该评估更能反映模型决策中的内部偏差，相较于传统的公平性度量。此外，鉴于当前数据集在数据量、多样性和清晰度方面存在的问题，我们引入了一个新的包含31,756个样本的性别-职业公平性评估数据集，用于共指消解，提供了一个更多样化和合适的评价现代LLMs的基准数据集。我们构建了一个基准，使用我们的度量和数据集，并将其应用于评估十种开源LLMs的行为。例如，Mistral-7B由于对错误预测的高度信心而表现出次优化的公平性，这一细节被平等机会所忽视，但被UCerF捕捉到。总体而言，我们的基于不确定性的LLM基准，能够评估公平性，为开发更透明和负责任的人工智能系统铺平了道路。 

---
# Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization 

**Title (ZH)**: 大型语言模型在可控多属性多目标分子优化中的应用 

**Authors**: Vishal Dey, Xiao Hu, Xia Ning  

**Link**: [PDF](https://arxiv.org/pdf/2505.23987)  

**Abstract**: In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria. However, existing computational approaches and instruction-tuned LLMs fail to capture such nuanced property-specific objectives, limiting their practical applicability. To address this, we introduce C-MuMOInstruct, the first instruction-tuning dataset focused on multi-property optimization with explicit, property-specific objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of instruction-tuned LLMs that can perform targeted property-specific optimization. Our experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions. This offers a step toward a foundational LLM to support realistic, diverse optimizations with property-specific objectives. C-MuMOInstruct and code are accessible through this https URL. 

**Abstract (ZH)**: 在现实药物设计中，分子优化需要选择性地提高多种分子性质至药用相关水平，同时保持其他已经达到此类标准的性质。然而，现有的计算方法和指令调优的大语言模型未能捕捉到这种细微的性质特定目标，限制了它们的实际应用。为了解决这一问题，我们引入了C-MuMOInstruct，这是首个专注于多性质优化并明确包含性质特定目标的指令调难题集。利用C-MuMOInstruct，我们开发了GeLLMO-Cs系列指令调 tuning 大语言模型，能够进行目标导向的性质特定优化。我们的实验在5个分布内和5个分布外任务中表明，GeLLMO-Cs在所有任务中均显著优于Strong Baselines，成功率最高可提高126%。值得注意的是，GeLLMO-Cs在新型优化任务和未见过的指令上展现出了出色的零样本泛化能力。这朝着支持具有性质特定目标的现实多样化优化的基础大语言模型迈出了一步。C-MuMOInstruct和代码可通过以下链接获取。 

---
# Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention 

**Title (ZH)**: 保密守护者：密码学禁止模型弃权滥用 

**Authors**: Stephan Rabanser, Ali Shahin Shamsabadi, Olive Franzese, Xiao Wang, Adrian Weller, Nicolas Papernot  

**Link**: [PDF](https://arxiv.org/pdf/2505.23968)  

**Abstract**: Cautious predictions -- where a machine learning model abstains when uncertain -- are crucial for limiting harmful errors in safety-critical applications. In this work, we identify a novel threat: a dishonest institution can exploit these mechanisms to discriminate or unjustly deny services under the guise of uncertainty. We demonstrate the practicality of this threat by introducing an uncertainty-inducing attack called Mirage, which deliberately reduces confidence in targeted input regions, thereby covertly disadvantaging specific individuals. At the same time, Mirage maintains high predictive performance across all data points. To counter this threat, we propose Confidential Guardian, a framework that analyzes calibration metrics on a reference dataset to detect artificially suppressed confidence. Additionally, it employs zero-knowledge proofs of verified inference to ensure that reported confidence scores genuinely originate from the deployed model. This prevents the provider from fabricating arbitrary model confidence values while protecting the model's proprietary details. Our results confirm that Confidential Guardian effectively prevents the misuse of cautious predictions, providing verifiable assurances that abstention reflects genuine model uncertainty rather than malicious intent. 

**Abstract (ZH)**: 谨慎预测——当机器学习模型在不确定时弃权对于限制安全关键应用中的有害错误至关重要。在本文中，我们识别了一种新型威胁：不诚实的机构可以通过利用这些机制在借口不确定性的情况下进行歧视或不公平地拒绝服务。我们通过引入一种称为Mirage的不确定性诱导攻击来证明这一威胁的可行性，这种攻击故意降低目标输入区域的置信度，从而秘密地对特定个体造成不利影响。同时，Mirage在所有数据点上保持了高预测性能。为了应对这一威胁，我们提出了一种名为Confidential Guardian的框架，该框架通过在参考数据集上分析校准指标来检测人为压制的置信度，并采用经过验证的推断的零知识证明来确保报告的置信分数确实源自部署的模型。这可以防止提供者伪造任意模型置信度值，同时保护模型的专有细节。我们的结果显示，Confidential Guardian有效地防止了谨慎预测的滥用，提供了可验证的保障，以确保弃权反映的是真实的模型不确定性而非恶意意图。 

---
# Enhancing LLM-Based Code Generation with Complexity Metrics: A Feedback-Driven Approach 

**Title (ZH)**: 基于复杂度度量增强的LLM代码生成：一种反馈驱动的方法 

**Authors**: Melika Sepidband, Hamed Taherkhani, Song Wang, Hadi Hemmati  

**Link**: [PDF](https://arxiv.org/pdf/2505.23953)  

**Abstract**: Automatic code generation has gained significant momentum with the advent of Large Language Models (LLMs) such as GPT-4. Although many studies focus on improving the effectiveness of LLMs for code generation, very limited work tries to understand the generated code's characteristics and leverage that to improve failed cases. In this paper, as the most straightforward characteristic of code, we investigate the relationship between code complexity and the success of LLM generated code. Using a large set of standard complexity metrics, we first conduct an empirical analysis to explore their correlation with LLM's performance on code generation (i.e., Pass@1). Using logistic regression models, we identify which complexity metrics are most predictive of code correctness. Building on these findings, we propose an iterative feedback method, where LLMs are prompted to generate correct code based on complexity metrics from previous failed outputs. We validate our approach across multiple benchmarks (i.e., HumanEval, MBPP, LeetCode, and BigCodeBench) and various LLMs (i.e., GPT-4o, GPT-3.5 Turbo, Llama 3.1, and GPT-o3 mini), comparing the results with two baseline methods: (a) zero-shot generation, and (b) iterative execution-based feedback without our code complexity insights. Experiment results show that our approach makes notable improvements, particularly with a smaller LLM (GPT3.5 Turbo), where, e.g., Pass@1 increased by 35.71% compared to the baseline's improvement of 12.5% on the HumanEval dataset. The study expands experiments to BigCodeBench and integrates the method with the Reflexion code generation agent, leading to Pass@1 improvements of 20% (GPT-4o) and 23.07% (GPT-o3 mini). The results highlight that complexity-aware feedback enhances both direct LLM prompting and agent-based workflows. 

**Abstract (ZH)**: 自动代码生成随着大型语言模型（LLMs）如GPT-4的出现获得了显著的发展。虽然许多研究关注于提高LLMs在代码生成方面的有效性，但很少有研究尝试理解生成代码的特性，并利用这些特性来改进失败的情况。本文中，我们作为代码最直接的特性，探讨代码复杂性和LLM生成代码成功率之间的关系。通过使用大量标准复杂性度量标准，我们首先进行实证分析，探索这些度量标准与LLM在代码生成任务上的表现（即Pass@1）之间的相关性。通过逻辑回归模型，我们确定了哪些复杂性度量指标最能预测代码的正确性。基于这些发现，我们提出了一种迭代反馈方法，其中LLM根据先前失败输出的复杂性指标生成正确的代码。我们跨越多个基准（即HumanEval、MBPP、LeetCode和BigCodeBench）和多种LLM（即GPT-4o、GPT-3.5 Turbo、Llama 3.1和GPT-o3 mini）验证了我们的方法，并将结果与两种基线方法进行了比较：（a）零样本生成，（b）基于迭代执行的反馈，但不使用我们的代码复杂性见解。实验结果表明，我们的方法取得了明显的改进，特别是在较小的LLM（GPT3.5 Turbo）上，例如，在HumanEval数据集上，Pass@1提高了35.71%，而基线方法的改进仅为12.5%。研究扩展到BigCodeBench，并与Reflexion代码生成代理结合使用，导致Pass@1分别提高了20%（GPT-4o）和23.07%（GPT-o3 mini）。结果表明，复杂性意识的反馈不仅增强了直接的LLM提示，还增强了基于代理的工作流程。 

---
# A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models 

**Title (ZH)**: 对大型（视觉）语言模型偏差和链式思维忠实性的深入考察 

**Authors**: Sriram Balasubramanian, Samyadeep Basu, Soheil Feizi  

**Link**: [PDF](https://arxiv.org/pdf/2505.23945)  

**Abstract**: Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent'' reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated. 

**Abstract (ZH)**: Chain-of-Thought (CoT)推理增强了大型语言模型的性能，但关于这些推理痕迹是否真实反映模型内部过程的问题仍然存在。我们首次全面研究了大型视觉-语言模型（LVLM）中的CoT可信度，探讨了基于文本和之前未被探索的基于图像的偏差如何影响推理和偏倚表达。我们的工作引入了一个新颖的细粒度评估管道，用于分类偏倚表达模式，使CoT推理的分析比之前的方法更加精确。该框架揭示了模型在处理不同类型偏倚时的重要区别，为LVLM CoT可信度提供了新的见解。研究发现，与显式的基于文本的偏倚相比，微妙的基于图像的偏倚几乎从未被表达出来，即使是在专门用于推理的模型中。此外，许多模型表现出一种先前未被识别的现象，我们称之为“不一致”推理——正确推理之后突然改变答案，这可能是检测不公平推理的潜在标志。然后，我们应用相同的评估管道重新审视了不同隐含线索层次上的LLM的CoT可信度。研究发现，当前仅依赖语言的推理模型仍然难以表达未明确陈述的线索。 

---
# Probing Association Biases in LLM Moderation Over-Sensitivity 

**Title (ZH)**: 探究大模型 Moderation 过度敏感中的关联偏见 

**Authors**: Yuxin Wang, Botao Yu, Ivory Yang, Saeed Hassanpour, Soroush Vosoughi  

**Link**: [PDF](https://arxiv.org/pdf/2505.23914)  

**Abstract**: Large Language Models are widely used for content moderation but often misclassify benign comments as toxic, leading to over-sensitivity. While previous research attributes this issue primarily to the presence of offensive terms, we reveal a potential cause beyond token level: LLMs exhibit systematic topic biases in their implicit associations. Inspired by cognitive psychology's implicit association tests, we introduce Topic Association Analysis, a semantic-level approach to quantify how LLMs associate certain topics with toxicity. By prompting LLMs to generate free-form scenario imagination for misclassified benign comments and analyzing their topic amplification levels, we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger topic stereotype despite lower overall false positive rates. These biases suggest that LLMs do not merely react to explicit, offensive language but rely on learned topic associations, shaping their moderation decisions. Our findings highlight the need for refinement beyond keyword-based filtering, providing insights into the underlying mechanisms driving LLM over-sensitivity. 

**Abstract (ZH)**: 大规模语言模型广泛用于内容审核，但 often misclassify benign comments as toxic, leading to over-sensitivity. 被翻译为：大规模语言模型在内容审核中广泛应用，但常常误将无害评论分类为有毒评论，导致过度敏感。 

---
# Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation 

**Title (ZH)**: 强化学习以提高长文生成中的口头化自信度 

**Authors**: Caiqi Zhang, Xiaochen Zhu, Chengzu Li, Nigel Collier, Andreas Vlachos  

**Link**: [PDF](https://arxiv.org/pdf/2505.23912)  

**Abstract**: Hallucination remains a major challenge for the safe and trustworthy deployment of large language models (LLMs) in factual content generation. Prior work has explored confidence estimation as an effective approach to hallucination detection, but often relies on post-hoc self-consistency methods that require computationally expensive sampling. Verbalized confidence offers a more efficient alternative, but existing approaches are largely limited to short-form question answering (QA) tasks and do not generalize well to open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized Confidence), an on-the-fly verbalized confidence estimation method for long-form generation. Specifically, we use reinforcement learning (RL) to train LLMs to append numerical confidence scores to each generated statement, serving as a direct and interpretable signal of the factuality of generation. Our experiments consider both on-policy and off-policy RL methods, including DPO, ORPO, and GRPO, to enhance the model calibration. We introduce two novel evaluation settings, free-form tagging and iterative tagging, to assess different verbalized confidence estimation methods. Experiments on three long-form QA datasets show that our RL-trained models achieve better calibration and generalize robustly across domains. Also, our method is highly efficient, as it only requires adding a few tokens to the output being decoded. 

**Abstract (ZH)**: 长文档生成中的即时口头化置信度估计方法：LoVeC 

---
# Actor-Critic based Online Data Mixing For Language Model Pre-Training 

**Title (ZH)**: 基于演员-评论家的在线数据混合语言模型预训练 

**Authors**: Jing Ma, Chenhao Dang, Mingjie Liao  

**Link**: [PDF](https://arxiv.org/pdf/2505.23878)  

**Abstract**: The coverage and composition of pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). To reduce the carbon footprint and financial costs of training, some data mixing methods, which applied the optimized domain weights of a small proxy model to train a larger one, were proposed. However, these methods did not evolute with the training dynamics. The existing online data mixing (ODM) method addressed this limitation by applying the multi-armed bandit algorithm as data sampling strategy. Yet, it did not consider the intra-domain interactions. In this paper, we develop an actor-critic based online data mixing (AC-ODM) method, which captures the varying domain weights by auxiliary actor-critic networks and consider the intra-domain interactions with the reward function. While constructing the dataset to pretrain a large target LLM, we directly apply the actor, which is trained with a small proxy LLM as the environment, as the sampling strategy. The transfer of sampling strategy can not only ensure the efficiency of dynamical data mixing, but also expedite the convergence of pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M, which invokes the sampling strategy obtained by a proxy LLM with 410M parameters, reaching the optimal validation perplexity of ODM 71% faster, and improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy, about 2.23x better on pass@1 of HumanEval benchmark. 

**Abstract (ZH)**: 预训练数据的覆盖率和组成显著影响大规模语言模型（LLMs）的泛化能力。为了减少训练的碳足迹和财务成本，提出了利用小型代理模型优化领域权重来训练更大模型的一些数据混合方法，但这些方法并未随着训练动态进行演变。现有的在线数据混合（ODM）方法通过应用多臂 bandit 算法作为数据采样策略，解决了这一局限性，但未考虑域内交互。本文开发了一种基于演员-评论家的在线数据混合（AC-ODM）方法，通过辅助演员-评论家网络捕获变化中的领域权重，并通过奖励函数考虑域内交互。在构建用于预训练目标大模型的数据集时，直接将通过一个小的代理模型作为环境训练的演员用作采样策略。采样策略的转移不仅能确保动态数据混合的效率，还能加速目标大模型的预训练过程。数值结果表明，使用具有410M参数的代理模型获得的采样策略的AC-ODM-410M，在达到ODM最优验证困惑度方面快71%，并在零样本MMLU基准测试中提高了27.5%的准确率，HumanEval基准测试中的pass@1性能提升了约2.23倍。 

---
# MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection 

**Title (ZH)**: MaCP：基于分层次余弦投影的最小且强大的适应性方法 

**Authors**: Yixian Shen, Qi Bi, Jia-Hong Huang, Hongyi Zhu, Andy D. Pimentel, Anuj Pathania  

**Link**: [PDF](https://arxiv.org/pdf/2505.23870)  

**Abstract**: We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine Projection, that achieves exceptional performance while requiring minimal parameters and memory for fine-tuning large foundation models. Its general idea is to exploit the superior energy compaction and decorrelation properties of cosine projection to improve both model efficiency and accuracy. Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space. Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and each partition's most critical frequency components are selected. Extensive experiments demonstrate the effectiveness of MaCP across a wide range of single-modality tasks, including natural language understanding, natural language generation, text summarization, as well as multi-modality tasks such as image classification and video understanding. MaCP consistently delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives. 

**Abstract (ZH)**: 一种新的适应方法MaCP：最小而强大的自适应余弦投影，其在要求少量参数和内存的情况下实现了卓越的性能，通过利用余弦投影的优越的能量压缩和去相关特性，提高模型的效率和准确性。 

---
# Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert 

**Title (ZH)**: 噪声鲁棒性通过噪声实现：带有投毒专家的非对称LoRA微调 

**Authors**: Zhaokun Wang, Jinyu Guo, Jingwen Pu, Lingfeng Chen, Hongli Pu, Jie Ou.Libo Qin, Wenhong Tian  

**Link**: [PDF](https://arxiv.org/pdf/2505.23868)  

**Abstract**: Current parameter-efficient fine-tuning methods for adapting pre-trained language models to downstream tasks are susceptible to interference from noisy data. Conventional noise-handling approaches either rely on laborious data pre-processing or employ model architecture modifications prone to error accumulation. In contrast to existing noise-process paradigms, we propose a noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a novel framework that enhances model robustness to noise only with generated noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE strategically integrates a dedicated poisoning expert in an asymmetric LoRA configuration. Through a two-stage paradigm, LoPE performs noise injection on the poisoning expert during fine-tuning to enhance its noise discrimination and processing ability. During inference, we selectively mask the dedicated poisoning expert to leverage purified knowledge acquired by normal experts for noise-robust output. Extensive experiments demonstrate that LoPE achieves strong performance and robustness purely through the low-cost noise injection, which completely eliminates the requirement of data cleaning. 

**Abstract (ZH)**: 基于异构LoRA中毒专家的噪声鲁棒适应方法 

---
# Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation 

**Title (ZH)**: Infi-Med: 低资源医学MLLMs的稳健推理评估 

**Authors**: Zeyu Liu, Zhitian Hou, Yining Di, Kejing Yang, Zhijie Sang, Congkai Xie, Jingwen Yang, Siyuan Liu, Jialu Wang, Chunming Li, Ming Li, Hongxia Yang  

**Link**: [PDF](https://arxiv.org/pdf/2505.23867)  

**Abstract**: Multimodal large language models (MLLMs) have demonstrated promising prospects in healthcare, particularly for addressing complex medical tasks, supporting multidisciplinary treatment (MDT), and enabling personalized precision medicine. However, their practical deployment faces critical challenges in resource efficiency, diagnostic accuracy, clinical considerations, and ethical privacy. To address these limitations, we propose Infi-Med, a comprehensive framework for medical MLLMs that introduces three key innovations: (1) a resource-efficient approach through curating and constructing high-quality supervised fine-tuning (SFT) datasets with minimal sample requirements, with a forward-looking design that extends to both pretraining and posttraining phases; (2) enhanced multimodal reasoning capabilities for cross-modal integration and clinical task understanding; and (3) a systematic evaluation system that assesses model performance across medical modalities and task types. Our experiments demonstrate that Infi-Med achieves state-of-the-art (SOTA) performance in general medical reasoning while maintaining rapid adaptability to clinical scenarios. The framework establishes a solid foundation for deploying MLLMs in real-world healthcare settings by balancing model effectiveness with operational constraints. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）在医疗领域的前景及其在处理复杂医疗任务、支持多学科治疗（MDT）和实现个性化精准医疗方面的潜力已得到验证。然而，它们的实际部署面临资源效率、诊断准确性、临床考虑和伦理隐私等关键挑战。为了应对这些局限性，我们提出了一种名为Infi-Med的全面框架，该框架引入了三项关键创新：（1）通过策源和构建高质量的监督微调（SFT）数据集，减少样本需求，以提高资源效率，并具有前瞻性的设计，扩展到预训练和后训练阶段；（2）增强多模态推理能力，以实现跨模态整合和临床任务理解；（3）系统评估体系，评估模型在不同医疗模态和任务类型中的表现。我们的实验表明，Infi-Med在一般医疗推理方面达到了当前最佳性能（SOTA），同时保持了对临床场景的快速适应能力。该框架通过平衡模型有效性与运营约束，为在真实世界医疗环境中部署MLLMs奠定了坚实基础。 

---
# OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities 

**Title (ZH)**: OMNIGUARD: 跨模态的高效AI安全审核方法 

**Authors**: Sahil Verma, Keegan Hines, Jeff Bilmes, Charlotte Siska, Luke Zettlemoyer, Hila Gonen, Chandan Singh  

**Link**: [PDF](https://arxiv.org/pdf/2505.23856)  

**Abstract**: The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\% over the strongest baseline in a multilingual setting, by 20.44\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient ($\approx 120 \times$ faster than the next fastest baseline). Code and data are available at: this https URL. 

**Abstract (ZH)**: 大型语言模型 emerging 能力带来的潜在有害滥用问题引发了关注。为缓解这一问题，我们提出了 OMNIGUARD 方法，以跨语言和模态检测有害提示。OMNIGUARD 方法通过识别跨语言或模态对齐的大型语言模型/多元语言模型的内部表示，然后利用这些表示构建一种语言无关或模态无关的分类器来检测有害提示。在多语言设置中，OMNIGUARD 将有害提示分类的准确率提高了 11.57%，对于基于图像的提示提高了 20.44%，并对基于音频的提示设定了新的 SOTA。通过重用生成过程中计算的嵌入，OMNIGUARD 还非常高效（比下一个最快的基线快约 120 倍）。代码和数据可在以下网址获得：this https URL。 

---
# Revisiting Uncertainty Estimation and Calibration of Large Language Models 

**Title (ZH)**: 重新审视大型语言模型的不确定性估计和校准 

**Authors**: Linwei Tao, Yi-Fan Yeh, Minjing Dong, Tao Huang, Philip Torr, Chang Xu  

**Link**: [PDF](https://arxiv.org/pdf/2505.23854)  

**Abstract**: As large language models (LLMs) are increasingly deployed in high-stakes applications, robust uncertainty estimation is essential for ensuring the safe and trustworthy deployment of LLMs. We present the most comprehensive study to date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and closed-source families, dense and Mixture-of-Experts (MoE) architectures, reasoning and non-reasoning modes, quantization variants and parameter scales from 0.6B to 671B. Focusing on three representative black-box single-pass methods, including token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically evaluate uncertainty calibration and selective classification using the challenging MMLU-Pro benchmark, which covers both reasoning-intensive and knowledge-based tasks. Our results show that LVU consistently outperforms TPU and NVU, offering stronger calibration and discrimination while being more interpretable. We also find that high accuracy does not imply reliable uncertainty, and that model scale, post-training, reasoning ability and quantization all influence estimation performance. Notably, LLMs exhibit better uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good calibration does not necessarily translate to effective error ranking. These findings highlight the need for multi-perspective evaluation and position LVU as a practical tool for improving the reliability of LLMs in real-world settings. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在高风险应用中日益普及，因此需要可靠的不确定性估计来确保LLMs的安全和信赖部署。本文进行了迄今为止最全面的大规模语言模型不确定性估计研究，评估了涵盖开源和闭源家族、密集型和Mixture-of-Experts（MoE）架构、推理与非推理模式、以及从0.6B到671B不同参数规模和量化变种的80个模型。本文集中在三种代表性的黑盒单次通过方法上，包括基于标记概率的不确定性（TPU）、数值语义不确定性（NVU）和语言语义不确定性（LVU），系统地评估了不确定性的校准和选择性分类，使用具有推理密集型和知识密集型任务的挑战性MMLU-Pro基准进行评估。结果显示，LVU在一致性和区分度上优于TPU和NVU，并且更具可解释性。我们还发现，高准确率并不意味着可靠的不确定性，模型规模、后训练、推理能力和量化都会影响估计性能。值得注意的是，LLMs在推理任务上的不确定性估计优于在知识密集型任务上，良好的校准并不必然转化为有效的错误排序。这些发现强调了多视角评估的必要性，并将LVU定位为提高LLMs在实际应用中可靠性的一种实用工具。 

---
# Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease 

**Title (ZH)**: 基于大型语言模型的代理在阿尔茨海默病自动研究可重复性中的探索性研究 

**Authors**: Nic Dobbins, Christelle Xiong, Kristine Lan, Meliha Yetisgen  

**Link**: [PDF](https://arxiv.org/pdf/2505.23852)  

**Abstract**: Objective: To demonstrate the capabilities of Large Language Models (LLMs) as autonomous agents to reproduce findings of published research studies using the same or similar dataset.
Materials and Methods: We used the "Quick Access" dataset of the National Alzheimer's Coordinating Center (NACC). We identified highly cited published research manuscripts using NACC data and selected five studies that appeared reproducible using this dataset alone. Using GPT-4o, we created a simulated research team of LLM-based autonomous agents tasked with writing and executing code to dynamically reproduce the findings of each study, given only study Abstracts, Methods sections, and data dictionary descriptions of the dataset.
Results: We extracted 35 key findings described in the Abstracts across 5 Alzheimer's studies. On average, LLM agents approximately reproduced 53.2% of findings per study. Numeric values and range-based findings often differed between studies and agents. The agents also applied statistical methods or parameters that varied from the originals, though overall trends and significance were sometimes similar.
Discussion: In some cases, LLM-based agents replicated research techniques and findings. In others, they failed due to implementation flaws or missing methodological detail. These discrepancies show the current limits of LLMs in fully automating reproducibility assessments. Still, this early investigation highlights the potential of structured agent-based systems to provide scalable evaluation of scientific rigor.
Conclusion: This exploratory work illustrates both the promise and limitations of LLMs as autonomous agents for automating reproducibility in biomedical research. 

**Abstract (ZH)**: 目标：展示大型语言模型（LLMs）作为自主代理的能力，使用相同的或类似的数据集重现已出版研究研究的结果。 

---
# ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark 

**Title (ZH)**: ASyMOB: 代数符号数学运算基准 

**Authors**: Michael Shalyt, Rotem Elimelech, Ido Kaminer  

**Link**: [PDF](https://arxiv.org/pdf/2505.23851)  

**Abstract**: Large language models (LLMs) are rapidly approaching the level of proficiency in university-level symbolic mathematics required for applications in advanced science and technology. However, existing benchmarks fall short in assessing the core skills of LLMs in symbolic mathematics-such as integration, differential equations, and algebraic simplification. To address this gap, we introduce ASyMOB, a novel assessment framework focused exclusively on symbolic manipulation, featuring 17,092 unique math challenges, organized by similarity and complexity. ASyMOB enables analysis of LLM generalization capabilities by comparing performance in problems that differ by simple numerical or symbolic `perturbations'. Evaluated LLMs exhibit substantial degradation in performance for all perturbation types (up to -70.3%), suggesting reliance on memorized patterns rather than deeper understanding of symbolic math, even among models achieving high baseline accuracy. Comparing LLM performance to computer algebra systems, we identify examples where they fail while LLMs succeed, as well as problems solved only by combining both approaches. Models capable of integrated code execution yielded higher accuracy compared to their performance without code, particularly stabilizing weaker models (up to +33.1% for certain perturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5 Flash) demonstrate not only high symbolic math proficiency (scoring 96.8% and 97.6% on the unperturbed set), but also remarkable robustness against perturbations, (-21.7% and -21.2% vs. average -50.4% for the other models). This may indicate a recent "phase transition" in the generalization capabilities of frontier LLMs. It remains to be seen whether the path forward lies in deeper integration with sophisticated external tools, or in developing models so capable that symbolic math systems like CAS become unnecessary. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在高级科学和技术应用所需的大学水平符号数学 proficiency 方面正迅速接近所需水平。然而，现有的基准在评估LLMs在符号数学方面的核心技能（如积分、微分方程和代数简化）方面存在不足。为解决这一问题，我们引入了ASyMOB，一个专注于符号操作的新型评估框架，包含17,092个独特数学挑战，按相似性和复杂性组织。ASyMOB 通过比较由简单数值或符号 `扰动` 引起的问题性能差异，有助于分析LLM的一般化能力。评估的LLMs在所有扰动类型下的性能显著下降（高达-70.3%），这表明它们依赖于记忆模式而非对符号数学更深层次的理解，即使在基础准确度高的模型中也是如此。将LLM性能与计算机代数系统进行比较，我们发现了LLM失败而LLM成功的问题实例，以及只有结合两者方法才能解决的问题。能够执行集成代码的模型在没有代码的情况下性能更高，尤其能稳定较弱的模型（某些扰动类型提高多达+33.1%）。值得注意的是，最先进的模型（o4-mini, Gemini 2.5 Flash）不仅在未扰动集上的符号数学熟练度很高（分别达到96.8%和97.6%），而且对扰动表现出惊人的鲁棒性（分别为-21.7%和-21.2%，而其他模型的平均值为-50.4%）。这可能表明前沿LLM的一般化能力最近经历了“相变”。未来的发展路径可能是更深层次地整合复杂的外部工具，也可能是开发出足够强大的模型，使得像CAS这样的符号数学系统成为不必要的工具。 

---
# Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems 

**Title (ZH)**: 跨域多智能体LLM系统必须解决的七个安全挑战 

**Authors**: Ronny Ko, Jiseong Jeong, Shuyuan Zheng, Chuan Xiao, Taewan Kim, Makoto Onizuka, Wonyong Shin  

**Link**: [PDF](https://arxiv.org/pdf/2505.23847)  

**Abstract**: Large language models (LLMs) are rapidly evolving into autonomous agents that cooperate across organizational boundaries, enabling joint disaster response, supply-chain optimization, and other tasks that demand decentralized expertise without surrendering data ownership. Yet, cross-domain collaboration shatters the unified trust assumptions behind current alignment and containment techniques. An agent benign in isolation may, when receiving messages from an untrusted peer, leak secrets or violate policy, producing risks driven by emergent multi-agent dynamics rather than classical software bugs. This position paper maps the security agenda for cross-domain multi-agent LLM systems. We introduce seven categories of novel security challenges, for each of which we also present plausible attacks, security evaluation metrics, and future research guidelines. 

**Abstract (ZH)**: 大型语言模型（LLMs）正迅速演变为跨越组织边界合作的自主代理， enabling 联合灾害响应、供应链优化和其他需要去中心化专业知识但不牺牲数据所有权的任务。然而，跨域协作打破了当前对齐和 containment 技术背后的一体化信任假设。孤立时表现 benign 的代理，当接收到不可信同伴的消息时，可能会泄露秘密或违反政策，产生的风险来自 emergent 多代理动力学而非经典软件错误。本文探讨了跨域多代理 LLM 系统的安全议程。我们提出了七类新型安全挑战，并为每种挑战提供了可能的攻击、安全评估指标和未来研究指南。 

---
# Large Language Models Often Know When They Are Being Evaluated 

**Title (ZH)**: 大型语言模型往往知道自己正在被评估。 

**Authors**: Joe Needham, Giles Edkins, Govind Pimpale, Henning Bartsch, Marius Hobbhahn  

**Link**: [PDF](https://arxiv.org/pdf/2505.23836)  

**Abstract**: If AI models can detect when they are being evaluated, the effectiveness of evaluations might be compromised. For example, models could have systematically different behavior during evaluations, leading to less reliable benchmarks for deployment and governance decisions. We investigate whether frontier language models can accurately classify transcripts based on whether they originate from evaluations or real-world deployment, a capability we call evaluation awareness. To achieve this, we construct a diverse benchmark of 1,000 prompts and transcripts from 61 distinct datasets. These span public benchmarks (e.g., MMLU, SWEBench), real-world deployment interactions, and agent trajectories from scaffolding frameworks (e.g., web-browsing agents). Frontier models clearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches an AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of $0.92$). Furthermore, both AI models and humans are better at identifying evaluations in agentic settings compared to chat settings. Additionally, we test whether models can identify the purpose of the evaluation. Under multiple-choice and open-ended questioning, AI models far outperform random chance in identifying what an evaluation is testing for. Our results indicate that frontier models already exhibit a substantial, though not yet superhuman, level of evaluation-awareness. We recommend tracking this capability in future models. 

**Abstract (ZH)**: 如果AI模型能够识别出自己正在被评估，评估的效果可能会受到损害。前沿语言模型能否准确区分来自评估和实际部署的对话转录：一项能力分析 

---
# LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions 

**Title (ZH)**: LayerIF：使用影响函数估计大型语言模型层次质量 

**Authors**: Hadi Askari, Shivanshu Gupta, Fei Wang, Anshuman Chhabra, Muhao Chen  

**Link**: [PDF](https://arxiv.org/pdf/2505.23811)  

**Abstract**: Pretrained Large Language Models (LLMs) achieve strong performance across a wide range of tasks, yet exhibit substantial variability in the various layers' training quality with respect to specific downstream applications, limiting their downstream this http URL is therefore critical to estimate layer-wise training quality in a manner that accounts for both model architecture and training data. However, existing approaches predominantly rely on model-centric heuristics (such as spectral statistics, outlier detection, or uniform allocation) while overlooking the influence of data. To address these limitations, we propose LayerIF, a data-driven framework that leverages Influence Functions to quantify the training quality of individual layers in a principled and task-sensitive manner. By isolating each layer's gradients and measuring the sensitivity of the validation loss to training examples by computing layer-wise influences, we derive data-driven estimates of layer importance. Notably, our method produces task-specific layer importance estimates for the same LLM, revealing how layers specialize for different test-time evaluation tasks. We demonstrate the utility of our scores by leveraging them for two downstream applications: (a) expert allocation in LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM pruning. Experiments across multiple LLM architectures demonstrate that our model-agnostic, influence-guided allocation leads to consistent gains in task performance. 

**Abstract (ZH)**: 预训练大型语言模型（LLMs）在多种任务中表现出色，但在特定下游应用的不同层的训练质量存在显著差异，限制了它们的下游应用效果。因此，估算每层的训练质量，考虑到模型架构和训练数据的影响，变得至关重要。然而，现有方法主要依赖于模型中心的启发式方法（如谱统计、离群值检测或均匀分配），而忽略了数据的影响。为了应对这些局限性，我们提出了一种基于数据的框架LayerIF，该框架利用影响函数以一种原则性和任务敏感的方式量化每层的训练质量。通过隔离每一层的梯度，并通过计算层间影响来衡量验证损失对训练样本的敏感性，我们获得了基于数据的层重要性估计。值得注意的是，我们的方法为相同的LLM提供了特定任务的层重要性估计，揭示了不同测试时评估任务下层的专业化情况。我们通过将其应用于两种下游应用来展示我们的得分的实用价值：(a) LoRA-MoE架构中的专家分配，以及(b) LLM剪枝中的层间稀疏性分布。在多种LLM架构上的实验表明，我们的模型无关、基于影响的分配方法在任务性能上表现出一致的改进。 

---
# MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation 

**Title (ZH)**: MARS-Bench：对话评价的多轮体育真实场景基准 

**Authors**: Chenghao Yang, Yinbo Luo, Zhoufutu Wen, Qi Chu, Tao Gong, Longxiang Liu, Kaiyuan Zhang, Jianpeng Jiao, Ge Zhang, Wenhao Huang, Nenghai Yu  

**Link**: [PDF](https://arxiv.org/pdf/2505.23810)  

**Abstract**: Large Language Models (\textbf{LLMs}), e.g. ChatGPT, have been widely adopted in real-world dialogue applications. However, LLMs' robustness, especially in handling long complex dialogue sessions, including frequent motivation transfer, sophisticated cross-turn dependency, is criticized all along. Nevertheless, no existing benchmarks can fully reflect these weaknesses. We present \textbf{MARS-Bench}, a \textbf{M}ulti-turn \textbf{A}thletic \textbf{R}eal-world \textbf{S}cenario Dialogue \textbf{Bench}mark, designed to remedy the gap. MARS-Bench is constructed from play-by-play text commentary so to feature realistic dialogues specifically designed to evaluate three critical aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn, and Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that closed-source LLMs significantly outperform open-source alternatives, explicit reasoning significantly boosts LLMs' robustness on handling long complex dialogue sessions, and LLMs indeed face significant challenges when handling motivation transfer and sophisticated cross-turn dependency. Moreover, we provide mechanistic interpretability on how attention sinks due to special tokens lead to LLMs' performance degradation when handling long complex dialogue sessions based on attention visualization experiment in Qwen2.5-7B-Instruction. 

**Abstract (ZH)**: 大规模语言模型（LLMs），例如ChatGPT，已在实际对话应用中被广泛采用。然而，LLMs在处理长且复杂的对话会话方面的稳健性，包括频繁的动力转移和复杂的跨轮依存关系，一直受到批评。尽管如此，现有的基准测试无法充分反映这些弱点。我们提出了MARS-Bench，一个多轮运动现实场景对话基准测试，旨在弥补这一缺口。MARS-Bench基于逐赛事的文字评论构建，旨在突出真实的对话，特别设计用于评估多轮对话的三个关键方面：超多轮、互动多轮和跨轮任务。对MARS-Bench的广泛实验还表明，闭源LLMs显著优于开源替代品，明确的推理显著增强了LLMs在处理长且复杂对话会话方面的稳健性，并且LLMs在处理动力转移和复杂的跨轮依存关系时确实面临着重大挑战。此外，我们根据Qwen2.5-7B-Instruction中的注意力可视化实验，提供了关于特殊标记导致注意力沉降如何影响LLMs在处理长且复杂对话会话时表现的机理解释。 

---
# LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion 

**Title (ZH)**: 基于LLM的电子商务营销内容优化：平衡创意与转化 

**Authors**: Haowei Yang, Haotian Lyu, Tianle Zhang, Dingzhou Wang, Yushang Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2505.23809)  

**Abstract**: As e-commerce competition intensifies, balancing creative content with conversion effectiveness becomes critical. Leveraging LLMs' language generation capabilities, we propose a framework that integrates prompt engineering, multi-objective fine-tuning, and post-processing to generate marketing copy that is both engaging and conversion-driven. Our fine-tuning method combines sentiment adjustment, diversity enhancement, and CTA embedding. Through offline evaluations and online A/B tests across categories, our approach achieves a 12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content novelty. This provides a practical solution for automated copy generation and suggests paths for future multimodal, real-time personalization. 

**Abstract (ZH)**: 随着电子商务竞争的加剧，平衡创意内容与转化效果变得至关重要。利用大语言模型的语言生成能力，我们提出了一种框架，该框架结合了提示工程、多目标微调和后处理，以生成既具有吸引力又具有转化驱动性的营销文案。我们的微调方法结合了情绪调整、多样性和CTA嵌入。通过跨类别的离线评估和在线A/B测试，我们的方法在保持内容新颖性的同时，实现了12.5%的点击率（CTR）提升和8.3%的转化率（CVR）提升。这为自动化文案生成提供了一个实用的解决方案，并为未来多模态、实时个性化提供了路径。 

---
# DenseLoRA: Dense Low-Rank Adaptation of Large Language Models 

**Title (ZH)**: DenseLoRA: 密集低秩大型语言模型适应 

**Authors**: Lin Mu, Xiaoyu Wang, Li Ni, Yang Li, Zhize Wu, Peiquan Jin, Yiwen Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.23808)  

**Abstract**: Low-rank adaptation (LoRA) has been developed as an efficient approach for adapting large language models (LLMs) by fine-tuning two low-rank matrices, thereby reducing the number of trainable parameters. However, prior research indicates that many of the weights in these matrices are redundant, leading to inefficiencies in parameter utilization. To address this limitation, we introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances parameter efficiency while achieving superior performance compared to LoRA. DenseLoRA builds upon the concept of representation fine-tuning, incorporating a single Encoder-Decoder to refine and compress hidden representations across all adaptation layers before applying adaptation. Instead of relying on two redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense low-rank matrix, improving parameter utilization and adaptation efficiency. We evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8% accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8% accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we conduct extensive experiments to systematically assess the impact of DenseLoRA's components on overall model performance. Code is available at this https URL. 

**Abstract (ZH)**: 密集低秩适应（DenseLoRA）：提高参数效率并实现优于LoRA的性能 

---
# DLP: Dynamic Layerwise Pruning in Large Language Models 

**Title (ZH)**: DLP: 动态逐层剪枝在大规模语言模型中 

**Authors**: Yuli Chen, Bo Cheng, Jiale Han, Yingying Zhang, Yingting Li, Shuhao Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.23807)  

**Abstract**: Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at this https URL to facilitate future research. 

**Abstract (ZH)**: 动态层wise剪枝（DLP）：一种基于模型权重和输入激活信息的自适应剪枝方法 

---
# MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation 

**Title (ZH)**: MedOrchestra: 一种临床数据解释的混合云-本地大语言模型方法 

**Authors**: Sihyeon Lee, Hyunjoo Song, Jong-chan Lee, Yoon Jin Lee, Boram Lee, Hee-Eon Lim, Dongyeong Kim, Jinwook Seo, Bohyoung Kim  

**Link**: [PDF](https://arxiv.org/pdf/2505.23806)  

**Abstract**: Deploying large language models (LLMs) in clinical settings faces critical trade-offs: cloud LLMs, with their extensive parameters and superior performance, pose risks to sensitive clinical data privacy, while local LLMs preserve privacy but often fail at complex clinical interpretation tasks. We propose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex clinical tasks into manageable subtasks and prompt generation, while a local LLM executes these subtasks in a privacy-preserving manner. Without accessing clinical data, the cloud LLM generates and validates subtask prompts using clinical guidelines and synthetic test cases. The local LLM executes subtasks locally and synthesizes outputs generated by the cloud LLM. We evaluate MedOrchestra on pancreatic cancer staging using 100 radiology reports under NCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy, outperforming local model baselines (without guideline: 48.94%, with guideline: 56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons: 65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches 85.42% accuracy, showing clear superiority across all settings. 

**Abstract (ZH)**: 在临床上部署大规模语言模型面临关键权衡：云端的大规模语言模型虽然参数丰富、性能优越，但会带来敏感临床数据隐私风险，而本地的大规模语言模型则能保护隐私但往往难以完成复杂的临床解释任务。我们提出MedOrchestra，这是一种混合框架，其中云端的大规模语言模型将复杂的临床任务分解为可管理的子任务和提示生成，而本地的大规模语言模型则以隐私保护的方式执行这些子任务。云端的大规模语言模型不访问临床数据，而是使用临床指南和合成测试案例生成和验证子任务提示。本地的大规模语言模型则在本地执行子任务并综合云端生成的输出结果。我们利用NCCN指南下的100份放射学报告，对MedOrchestra在胰腺癌分期的应用进行了评估。在自由文本报告中，MedOrchestra的准确率为70.21%，优于未使用指南的本地模型baseline（48.94%）和使用指南的本地模型baseline（56.59%），也优于胃肠病学专科医生（59.57%）、外科医生（65.96%）和放射科医生（55.32%）。在结构化报告中，MedOrchestra的准确率为85.42%，在所有设定中均表现优异。 

---
# Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies 

**Title (ZH)**: 利用子句频率校准文本到SQL解析的LLM模型 

**Authors**: Terrance Liu, Shuyi Wang, Daniel Preotiuc-Pietro, Yash Chandarana, Chirag Gupta  

**Link**: [PDF](https://arxiv.org/pdf/2505.23804)  

**Abstract**: While large language models (LLMs) achieve strong performance on text-to-SQL parsing, they sometimes exhibit unexpected failures in which they are confidently incorrect. Building trustworthy text-to-SQL systems thus requires eliciting reliable uncertainty measures from the LLM. In this paper, we study the problem of providing a calibrated confidence score that conveys the likelihood of an output query being correct. Our work is the first to establish a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In particular, we show that Platt scaling, a canonical method for calibration, provides substantial improvements over directly using raw model output probabilities as confidence scores. Furthermore, we propose a method for text-to-SQL calibration that leverages the structured nature of SQL queries to provide more granular signals of correctness, named "sub-clause frequency" (SCF) scores. Using multivariate Platt scaling (MPS), our extension of the canonical Platt scaling technique, we combine individual SCF scores into an overall accurate and calibrated score. Empirical evaluation on two popular text-to-SQL datasets shows that our approach of combining MPS and SCF yields further improvements in calibration and the related task of error detection over traditional Platt scaling. 

**Abstract (ZH)**: 基于大型语言模型的可信赖文本到SQL系统中的后验校准基准研究：一种结构化子句频率分数的方法 

---
# MultiPhishGuard: An LLM-based Multi-Agent System for Phishing Email Detection 

**Title (ZH)**: MultiPhishGuard：一个基于LLM的多Agent系统用于钓鱼邮件检测 

**Authors**: Yinuo Xue, Eric Spero, Yun Sing Koh, Giovanni Russello  

**Link**: [PDF](https://arxiv.org/pdf/2505.23803)  

**Abstract**: Phishing email detection faces critical challenges from evolving adversarial tactics and heterogeneous attack patterns. Traditional detection methods, such as rule-based filters and denylists, often struggle to keep pace with these evolving tactics, leading to false negatives and compromised security. While machine learning approaches have improved detection accuracy, they still face challenges adapting to novel phishing strategies. We present MultiPhishGuard, a dynamic LLM-based multi-agent detection system that synergizes specialized expertise with adversarial-aware reinforcement learning. Our framework employs five cooperative agents (text, URL, metadata, explanation simplifier, and adversarial agents) with automatically adjusted decision weights powered by a Proximal Policy Optimization reinforcement learning algorithm. To address emerging threats, we introduce an adversarial training loop featuring an adversarial agent that generates subtle context-aware email variants, creating a self-improving defense ecosystem and enhancing system robustness. Experimental evaluations on public datasets demonstrate that MultiPhishGuard significantly outperforms Chain-of-Thoughts, single-agent baselines and state-of-the-art detectors, as validated by ablation studies and comparative analyses. Experiments demonstrate that MultiPhishGuard achieves high accuracy (97.89\%) with low false positive (2.73\%) and false negative rates (0.20\%). Additionally, we incorporate an explanation simplifier agent, which provides users with clear and easily understandable explanations for why an email is classified as phishing or legitimate. This work advances phishing defense through dynamic multi-agent collaboration and generative adversarial resilience. 

**Abstract (ZH)**: 基于多智能体的抗对抗强化学习多重钓鱼邮件检测系统 

---
# MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks 

**Title (ZH)**: MedHELM: 医疗任务中大规模语言模型的整体评估 

**Authors**: Suhana Bedi, Hejie Cui, Miguel Fuentes, Alyssa Unell, Michael Wornow, Juan M. Banda, Nikesh Kotecha, Timothy Keyes, Yifan Mai, Mert Oez, Hao Qiu, Shrey Jain, Leonardo Schettini, Mehr Kashyap, Jason Alan Fries, Akshay Swaminathan, Philip Chung, Fateme Nateghi, Asad Aali, Ashwin Nayak, Shivam Vedak, Sneha S. Jain, Birju Patel, Oluseyi Fayanju, Shreya Shah, Ethan Goh, Dong-han Yao, Brian Soetikno, Eduardo Reis, Sergios Gatidis, Vasu Divi, Robson Capasso, Rachna Saralkar, Chia-Chun Chiang, Jenelle Jindal, Tho Pham, Faraz Ghoddusi, Steven Lin, Albert S. Chiou, Christy Hong, Mohana Roy, Michael F. Gensheimer, Hinesh Patel, Kevin Schulman, Dev Dash, Danton Char, Lance Downing, Francois Grolleau, Kameron Black, Bethel Mieso, Aydin Zahedivash, Wen-wai Yim, Harshita Sharma, Tony Lee, Hannah Kirsch, Jennifer Lee, Nerissa Ambers, Carlene Lugtu, Aditya Sharma, Bilal Mawji, Alex Alekseyev, Vicky Zhou, Vikas Kakkar, Jarrod Helzer, Anurang Revri, Yair Bannett, Roxana Daneshjou, Jonathan Chen, Emily Alsentzer, Keith Morse, Nirmal Ravi, Nima Aghaeepour, Vanessa Kennedy, Akshay Chaudhari, Thomas Wang, Sanmi Koyejo, Matthew P. Lungren, Eric Horvitz, Percy Liang, Mike Pfeffer, Nigam H. Shah  

**Link**: [PDF](https://arxiv.org/pdf/2505.23802)  

**Abstract**: While large language models (LLMs) achieve near-perfect scores on medical licensing exams, these evaluations inadequately reflect the complexity and diversity of real-world clinical practice. We introduce MedHELM, an extensible evaluation framework for assessing LLM performance for medical tasks with three key contributions. First, a clinician-validated taxonomy spanning 5 categories, 22 subcategories, and 121 tasks developed with 29 clinicians. Second, a comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly formulated) providing complete coverage of all categories and subcategories in the taxonomy. Third, a systematic comparison of LLMs with improved evaluation methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9 frontier LLMs, using the 35 benchmarks, revealed significant performance variation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64% win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved comparable results at 40% lower estimated computational cost. On a normalized accuracy scale (0-1), most models performed strongly in Clinical Note Generation (0.73-0.85) and Patient Communication & Education (0.78-0.83), moderately in Medical Research Assistance (0.65-0.75), and generally lower in Clinical Decision Support (0.56-0.72) and Administration & Workflow (0.53-0.63). Our LLM-jury evaluation method achieved good agreement with clinician ratings (ICC = 0.47), surpassing both average clinician-clinician agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top models at lower estimated cost. These findings highlight the importance of real-world, task-specific evaluation for medical use of LLMs and provides an open source framework to enable this. 

**Abstract (ZH)**: 尽管大型语言模型在医学执照考试中取得了近乎完美的成绩，但这些评估未能充分反映现实临床实践中复杂性和多样性。我们介绍了MedHELM，这是一种评估大型语言模型在医学任务性能的可扩展框架，包含三项关键贡献：一个由29名临床医生验证的涵盖5个类别、22个亚类别和121个任务的分类体系；一个全面的基准套件，包括35个基准（17个现有，18个新制定的），覆盖分类体系中的所有类别和亚类别；以及一种系统性比较基于改进评估方法（使用大型语言模型陪审团）及成本-性能分析的方法。使用35个基准对9个前沿大型语言模型的评估显示了显著的性能差异。高级推理模型（DeepSeek R1：66%胜率；o3-mini：64%胜率）表现出色，尽管Claude 3.5 Sonnet在估计计算成本低40%的情况下取得了类似结果。在归一化准确度尺度（0-1）上，大多数模型在临床病历生成（0.73-0.85）和患者沟通与教育（0.78-0.83）方面表现强劲，在医学研究援助（0.65-0.75）方面表现适度，在临床决策支持（0.56-0.72）和行政与工作流程（0.53-0.63）方面表现一般。我们的大型语言模型陪审团评估方法与临床医生评分的共识度良好（ICC=0.47），超过了临床医生-临床医生共识度平均值（ICC=0.43）和包括ROUGE-L（0.36）和BERTScore-F1（0.44）在内的自动化基线。Claude 3.5 Sonnet在较低的估计成本下达到了顶级模型的性能。这些发现突显了在医学中使用大型语言模型进行现实世界的、任务特定的评估的重要性，并提供了一个开源框架来实现这一点。 

---
# Estimating LLM Consistency: A User Baseline vs Surrogate Metrics 

**Title (ZH)**: 评估LLM一致性：用户基准 vs 替代指标 

**Authors**: Xiaoyuan Wu, Weiran Lin, Omer Akgul, Lujo Bauer  

**Link**: [PDF](https://arxiv.org/pdf/2505.23799)  

**Abstract**: Large language models (LLMs) are prone to hallucinations and sensitive to prompt perturbations, often resulting in inconsistent or unreliable generated text. Different methods have been proposed to mitigate such hallucinations and fragility -- one of them being measuring the consistency (the model's confidence in the response, or likelihood of generating a similar response when resampled) of LLM responses. In previous work, measuring consistency often relied on the probability of a response appearing within a pool of resampled responses, or internal states or logits of responses. However, it is not yet clear how well these approaches approximate how humans perceive the consistency of LLM responses. We performed a user study (n=2,976) and found current methods typically do not approximate users' perceptions of LLM consistency very well. We propose a logit-based ensemble method for estimating LLM consistency, and we show that this method matches the performance of the best-performing existing metric in estimating human ratings of LLM consistency. Our results suggest that methods of estimating LLM consistency without human evaluation are sufficiently imperfect that we suggest evaluation with human input be more broadly used. 

**Abstract (ZH)**: 大型语言模型（LLMs）容易出现幻觉并对外部提示变化敏感，常导致生成文本的一致性和可靠性受损。提出了多种方法来减轻这些幻觉和脆弱性——其中之一是衡量LLM响应的一致性（模型对响应的信心，或在重新采样时生成类似响应的可能性）。在以往的研究中，衡量一致性通常依赖于响应在重新采样响应池中的概率，或响应的内部状态或logits。然而，目前尚不清楚这些方法如何接近人类对LLM一致性感知的实际情况。我们进行了用户研究（n=2,976）并发现，当前的方法通常不能很好地接近用户的LLM一致性感知。我们提出了一种基于logits的组合方法来估算LLM的一致性，并证明该方法在估算人类对LLM一致性评级的性能上与现有的最佳指标相当。我们的结果表明，无需人类评估来估算LLM一致性的方法存在足够的缺陷，因此建议更广泛地使用包含人类输入的评估方法。 

---
# R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning 

**Title (ZH)**: R3-RAG：通过强化学习学习逐步推理和检索的LLM方法 

**Authors**: Yuan Li, Qi Luo, Xiaonan Li, Bufan Li, Qinyuan Cheng, Bo Wang, Yining Zheng, Yuxin Wang, Zhangyue Yin, Xipeng Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2505.23794)  

**Abstract**: Retrieval-Augmented Generation (RAG) integrates external knowledge with Large Language Models (LLMs) to enhance factual correctness and mitigate hallucination. However, dense retrievers often become the bottleneck of RAG systems due to their limited parameters compared to LLMs and their inability to perform step-by-step reasoning. While prompt-based iterative RAG attempts to address these limitations, it is constrained by human-designed workflows. To address these limitations, we propose $\textbf{R3-RAG}$, which uses $\textbf{R}$einforcement learning to make the LLM learn how to $\textbf{R}$eason and $\textbf{R}$etrieve step by step, thus retrieving comprehensive external knowledge and leading to correct answers. R3-RAG is divided into two stages. We first use cold start to make the model learn the manner of iteratively interleaving reasoning and retrieval. Then we use reinforcement learning to further harness its ability to better explore the external retrieval environment. Specifically, we propose two rewards for R3-RAG: 1) answer correctness for outcome reward, which judges whether the trajectory leads to a correct answer; 2) relevance-based document verification for process reward, encouraging the model to retrieve documents that are relevant to the user question, through which we can let the model learn how to iteratively reason and retrieve relevant documents to get the correct answer. Experimental results show that R3-RAG significantly outperforms baselines and can transfer well to different retrievers. We release R3-RAG at this https URL. 

**Abstract (ZH)**: Retrieval-Augmented Generation with Reinforcement Learning for Step-by-Step Reasoning and Retrieval 

---
# USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models 

**Title (ZH)**: USB：面向多模态大型语言模型的综合性统一安全性评估基准 

**Authors**: Baolin Zheng, Guanlin Chen, Hongqiong Zhong, Qingyang Teng, Yingshui Tan, Zhendong Liu, Weixun Wang, Jiaheng Liu, Jian Yang, Huiyun Jing, Jincheng Wei, Wenbo Su, Xiaoyong Zhu, Bo Zheng, Kaifu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2505.23793)  

**Abstract**: Despite their remarkable achievements and widespread adoption, Multimodal Large Language Models (MLLMs) have revealed significant security vulnerabilities, highlighting the urgent need for robust safety evaluation benchmarks. Existing MLLM safety benchmarks, however, fall short in terms of data quality and coverge, and modal risk combinations, resulting in inflated and contradictory evaluation results, which hinders the discovery and governance of security concerns. Besides, we argue that vulnerabilities to harmful queries and oversensitivity to harmless ones should be considered simultaneously in MLLMs safety evaluation, whereas these were previously considered separately. In this paper, to address these shortcomings, we introduce Unified Safety Benchmarks (USB), which is one of the most comprehensive evaluation benchmarks in MLLM safety. Our benchmark features high-quality queries, extensive risk categories, comprehensive modal combinations, and encompasses both vulnerability and oversensitivity evaluations. From the perspective of two key dimensions: risk categories and modality combinations, we demonstrate that the available benchmarks -- even the union of the vast majority of them -- are far from being truly comprehensive. To bridge this gap, we design a sophisticated data synthesis pipeline that generates extensive, high-quality complementary data addressing previously unexplored aspects. By combining open-source datasets with our synthetic data, our benchmark provides 4 distinct modality combinations for each of the 61 risk sub-categories, covering both English and Chinese across both vulnerability and oversensitivity dimensions. 

**Abstract (ZH)**: 尽管多模态大型语言模型在成就和广泛应用方面取得了显著进展，但它们揭示了重大安全漏洞，突显了对稳健安全评估基准的迫切需求。现有的多模态大型语言模型安全基准在数据质量和覆盖面、模态风险组合方面存在不足，导致评价结果夸大和矛盾，阻碍了安全问题的发现和治理。此外，我们认为在多模态大型语言模型的安全评估中，有害查询的漏洞和无害查询的过度敏感性应同时予以考虑，而以前这些方面的评估是单独进行的。为了应对这些不足，本文引入了统一安全基准（USB），这是迄今为止最全面的多模态大型语言模型安全评价基准之一。我们的基准具有高质量的查询、广泛的危险类别、全面的模态组合，并涵盖了漏洞和过度敏感性评估。从两个关键维度：危险类别和模态组合的角度来看，我们表明，即使是最主要部分的现有基准也不够全面。为了弥合这一差距，我们设计了一套复杂的数据合成管道，以生成广泛的高质量补充数据，涵盖之前未探索的方面。通过结合开源数据集和合成数据，我们的基准为每个61个风险子类别提供了4种不同的模态组合，分别涵盖了英语和中文在漏洞和过度敏感性维度上的全面覆盖。 

---
# Rethinking the Understanding Ability across LLMs through Mutual Information 

**Title (ZH)**: 重思LLM之间的相互信息理解能力 

**Authors**: Shaojie Wang, Sirui Ding, Na Zou  

**Link**: [PDF](https://arxiv.org/pdf/2505.23790)  

**Abstract**: Recent advances in large language models (LLMs) have revolutionized natural language processing, yet evaluating their intrinsic linguistic understanding remains challenging. Moving beyond specialized evaluation tasks, we propose an information-theoretic framework grounded in mutual information (MI) to achieve this. We formalize the understanding as MI between an input sentence and its latent representation (sentence-level MI), measuring how effectively input information is preserved in latent representation. Given that LLMs learn embeddings for individual tokens, we decompose sentence-level MI into token-level MI between tokens and sentence embeddings, establishing theoretical bounds connecting these measures. Based on this foundation, we theoretically derive a computable lower bound for token-level MI using Fano's inequality, which directly relates to token-level recoverability-the ability to predict original tokens from sentence embedding. We implement this recoverability task to comparatively measure MI across different LLMs, revealing that encoder-only models consistently maintain higher information fidelity than their decoder-only counterparts, with the latter exhibiting a distinctive late-layer "forgetting" pattern where mutual information is first enhanced and then discarded. Moreover, fine-tuning to maximize token-level recoverability consistently improves understanding ability of LLMs on tasks without task-specific supervision, demonstrating that mutual information can serve as a foundation for understanding and improving language model capabilities. 

**Abstract (ZH)**: Recent Advances in Large Language Models: An Information-Theoretic Framework Based on Mutual Information for Evaluating Intrinsic Linguistic Understanding 

---
# Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework 

**Title (ZH)**: 九种违反版权法的方式及我们LLM为何不会：一种符合合理使用原则的生成框架 

**Authors**: Aakash Sen Sharma, Debdeep Sanyal, Priyansh Srivastava, Sundar Atreya H., Shirish Karande, Mohan Kankanhalli, Murari Mandal  

**Link**: [PDF](https://arxiv.org/pdf/2505.23788)  

**Abstract**: Large language models (LLMs) commonly risk copyright infringement by reproducing protected content verbatim or with insufficient transformative modifications, posing significant ethical, legal, and practical concerns. Current inference-time safeguards predominantly rely on restrictive refusal-based filters, often compromising the practical utility of these models. To address this, we collaborated closely with intellectual property experts to develop FUA-LLM (Fair Use Aligned Language Models), a legally-grounded framework explicitly designed to align LLM outputs with fair-use doctrine. Central to our method is FairUseDB, a carefully constructed dataset containing 18,000 expert-validated examples covering nine realistic infringement scenarios. Leveraging this dataset, we apply Direct Preference Optimization (DPO) to fine-tune open-source LLMs, encouraging them to produce legally compliant and practically useful alternatives rather than resorting to blunt refusal. Recognizing the shortcomings of traditional evaluation metrics, we propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic Mean (CAH) to balance infringement risk against response utility. Extensive quantitative experiments coupled with expert evaluations confirm that FUA-LLM substantially reduces problematic outputs (up to 20\%) compared to state-of-the-art approaches, while preserving real-world usability. 

**Abstract (ZH)**: 大型语言模型（LLMs）通过直接复制或缺乏足够的转化性修改来重现受保护内容，这会引发严重的版权侵权风险，带来伦理、法律和实践上的问题。当前的推理时防护措施主要依赖于限制性的拒绝型过滤器，这往往牺牲了这些模型的实际效用。为解决这一问题，我们与知识产权专家紧密合作，开发了FUA-LLM（Fair Use Aligned Language Models），这是一个具有法律依据的框架，旨在明确使LLM的输出符合合理使用原则。我们方法的核心是FairUseDB，这是一个精心构建的数据集，包含18,000个专家验证的示例，覆盖了九种现实中的侵权场景。通过利用这个数据集，我们应用直接偏好优化（DPO）来微调开源LLM，鼓励它们生成合法合规且实用的替代方案，而不仅仅是采取武断的拒绝。鉴于传统评价指标的不足，我们提出新的指标：加权罚分效用和合规意识调和平均（CAH），以平衡侵权风险与回应效用。大量定量实验结合专家评估证实，FUA-LLM相较于最先进的方法显著减少了有问题的输出（最高可达20%），同时保持了现实世界的实用性。 

---
# Mind the Gap: A Practical Attack on GGUF Quantization 

**Title (ZH)**: 注意差距：对GGUF量化的一种实用攻击 

**Authors**: Kazuki Egashira, Robin Staab, Mark Vero, Jingxuan He, Martin Vechev  

**Link**: [PDF](https://arxiv.org/pdf/2505.23786)  

**Abstract**: With the increasing size of frontier LLMs, post-training quantization has become the standard for memory-efficient deployment. Recent work has shown that basic rounding-based quantization schemes pose security risks, as they can be exploited to inject malicious behaviors into quantized models that remain hidden in full precision. However, existing attacks cannot be applied to more complex quantization methods, such as the GGUF family used in the popular ollama and this http URL frameworks. In this work, we address this gap by introducing the first attack on GGUF. Our key insight is that the quantization error -- the difference between the full-precision weights and their (de-)quantized version -- provides sufficient flexibility to construct malicious quantized models that appear benign in full precision. Leveraging this, we develop an attack that trains the target malicious LLM while constraining its weights based on quantization errors. We demonstrate the effectiveness of our attack on three popular LLMs across nine GGUF quantization data types on three diverse attack scenarios: insecure code generation ($\Delta$=$88.7\%$), targeted content injection ($\Delta$=$85.0\%$), and benign instruction refusal ($\Delta$=$30.1\%$). Our attack highlights that (1) the most widely used post-training quantization method is susceptible to adversarial interferences, and (2) the complexity of quantization schemes alone is insufficient as a defense. 

**Abstract (ZH)**: 随着前沿大语言模型的规模不断增加，后训练量化已成为内存高效部署的标准方法。现有研究表明，基于基本舍入的量化方案可能存在安全风险，因为它们可以被利用在量化模型中注入恶意行为，而这些行为在高精度下仍保持隐藏状态。然而，现有的攻击方法无法应用于更复杂的量化方法，如流行框架ollama和this http URL所使用的GGUF家族。在本工作中，我们通过引入针对GGUF的首个攻击方法来弥补这一空白。我们的核心洞察是，量化误差——全精度权重与其量化版本之间的差异——提供了足够的灵活性，以构建在全精度下看似无害的恶意量化模型。利用这一观察，我们开发了一种攻击方法，该方法在约束权重基于量化误差的同时训练目标恶意大语言模型。我们在三种流行的大型语言模型的九种不同的攻击场景中演示了该攻击的有效性：不安全的代码生成（$\Delta$=$88.7\%$）、目标内容注入（$\Delta$=$85.0\%$）和良性指令拒绝（$\Delta$=$30.1\%$）。该攻击突显了两点：（1）最常用的后训练量化方法容易受到恶意干扰的影响；（2）仅量化方案的复杂性不足以作为防御措施。 

---
# Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale 

**Title (ZH)**: 意义不是度量：使用大语言模型大规模使文化背景明晰 

**Authors**: Cody Kommers, Drew Hemment, Maria Antoniak, Joel Z. Leibo, Hoyt Long, Emily Robinson, Adam Sobey  

**Link**: [PDF](https://arxiv.org/pdf/2505.23785)  

**Abstract**: This position paper argues that large language models (LLMs) can make cultural context, and therefore human meaning, legible at an unprecedented scale in AI-based sociotechnical systems. We argue that such systems have previously been unable to represent human meaning because they rely on thin descriptions: numerical representations that enforce standardization and therefore strip human activity of the cultural context that gives it meaning. By contrast, scholars in the humanities and qualitative social sciences have developed frameworks for representing meaning through thick description: verbal representations that accommodate heterogeneity and retain contextual information needed to represent human meaning. While these methods can effectively codify meaning, they are difficult to deploy at scale. However, the verbal capabilities of LLMs now provide a means of (at least partially) automating the generation and processing of thick descriptions, potentially overcoming this bottleneck. We argue that the problem of rendering human meaning legible is not just about selecting better metrics, but about developing new representational formats (based on thick description). We frame this as a crucial direction for the application of generative AI and identify five key challenges: preserving context, maintaining interpretive pluralism, integrating perspectives based on lived experience and critical distance, distinguishing qualitative content from quantitative magnitude, and acknowledging meaning as dynamic rather than static. Furthermore, we suggest that thick description has the potential to serve as a unifying framework to address a number of emerging concerns about the difficulties of representing culture in (or using) LLMs. 

**Abstract (ZH)**: 论文化语境下大型语言模型如何在人工智能社会技术系统中使人类意义变得可读 

---
# Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning 

**Title (ZH)**: 通过经典监督学习的视角增强LLMs的上下文学习能力 

**Authors**: Korel Gundem, Juncheng Dong, Dennis Zhang, Vahid Tarokh, Zhengling Qi  

**Link**: [PDF](https://arxiv.org/pdf/2505.23783)  

**Abstract**: In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new tasks with just a few examples, but their predictions often suffer from systematic biases, leading to unstable performances in classification. While calibration techniques are proposed to mitigate these biases, we show that, in the logit space, many of these methods are equivalent to merely shifting the LLM's decision boundary without having the ability to alter its orientation. This proves inadequate when biases cause the LLM to be severely misdirected. To address these limitations and provide a unifying framework, we propose Supervised Calibration (SC), a loss-minimization based framework which learns an optimal, per-class affine transformation of the LLM's predictive probabilities in the logit space without requiring external data beyond the context. By using a more expressive functional class, SC not only subsumes many existing calibration methods in ICL as special cases, but also enables the ability to alter and even completely reverse the orientation of the LLM's decision boundary. Furthermore, SC's loss-based nature facilitates the seamless integration of two purpose-built regularization techniques: context-invariance and directional trust-region. The former is designed to tackle the instability issue in ICL, while the latter controls the degree of calibration. Finally, SC delivers state-of-the-art performance over calibration baselines in the 4-shot, 8-shot, and 16-shot settings across all nine datasets for Mistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct. 

**Abstract (ZH)**: 基于监督校准的上下文学习校准框架（Supervised Calibration for Context Learning） 

---
# Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection 

**Title (ZH)**: 使用大语言模型进行传感器数据少样本优化：疲劳检测案例研究 

**Authors**: Elsen Ronando, Sozo Inoue  

**Link**: [PDF](https://arxiv.org/pdf/2505.18754)  

**Abstract**: In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid Euclidean Distance with Large Language Models) to improve example selection for sensor-based classification tasks. While few-shot prompting enables efficient inference with limited labeled data, its performance largely depends on the quality of selected examples. HED-LM addresses this challenge through a hybrid selection pipeline that filters candidate examples based on Euclidean distance and re-ranks them using contextual relevance scored by large language models (LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection task using accelerometer data characterized by overlapping patterns and high inter-subject variability. Unlike simpler tasks such as activity recognition, fatigue detection demands more nuanced example selection due to subtle differences in physiological signals. Our experiments show that HED-LM achieves a mean macro F1-score of 69.13$\pm$10.71%, outperforming both random selection (59.30$\pm$10.13%) and distance-only filtering (67.61$\pm$11.39%). These represent relative improvements of 16.6% and 2.3%, respectively. The results confirm that combining numerical similarity with contextual relevance improves the robustness of few-shot prompting. Overall, HED-LM offers a practical solution to improve performance in real-world sensor-based learning tasks and shows potential for broader applications in healthcare monitoring, human activity recognition, and industrial safety scenarios. 

**Abstract (ZH)**: 基于HED-LM的少样本优化方法以提高基于传感器的分类任务示例选择 

---
# Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play 

**Title (ZH)**: 通过自博弈实现协作自主驾驶的自然语言通信 

**Authors**: Jiaxun Cui, Chen Tang, Jarrett Holtz, Janice Nguyen, Alessandro G. Allievi, Hang Qiu, Peter Stone  

**Link**: [PDF](https://arxiv.org/pdf/2505.18334)  

**Abstract**: Past work has demonstrated that autonomous vehicles can drive more safely if they communicate with one another than if they do not. However, their communication has often not been human-understandable. Using natural language as a vehicle-to-vehicle (V2V) communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with human drivers. In this work, we propose a suite of traffic tasks in autonomous driving where vehicles in a traffic scenario need to communicate in natural language to facilitate coordination in order to avoid an imminent collision and/or support efficient traffic flow. To this end, this paper introduces a novel method, LLM+Debrief, to learn a message generation and high-level decision-making policy for autonomous vehicles through multi-agent discussion. To evaluate LLM agents for driving, we developed a gym-like simulation environment that contains a range of driving scenarios. Our experimental results demonstrate that LLM+Debrief is more effective at generating meaningful and human-understandable natural language messages to facilitate cooperation and coordination than a zero-shot LLM agent. Our code and demo videos are available at this https URL. 

**Abstract (ZH)**: 过去的研究所证明，如果自动驾驶车辆彼此通信，它们的驾驶安全性会更高，而如果不通信则安全性较差。然而，它们之间的通信往往对人类来说难以理解。使用自然语言作为车辆到车辆（V2V）通信协议，可以使自动驾驶车辆不仅与其他自动驾驶车辆而且与人类驾驶员进行合作驾驶。在本文中，我们提出了一个自动驾驶中的交通任务套件，在这些任务中，交通场景中的车辆需要通过自然语言进行通信以实现协调，从而避免即将发生的碰撞并/或支持高效的交通流动。为此，本文介绍了一种新的方法——LLM+Debrief，通过多智能体讨论来学习自动驾驶车辆的消息生成和高层决策策略。为了评估LLM驱动代理的有效性，我们开发了一个类似于 gym 的模拟环境，该环境包含各种驾驶场景。我们的实验结果表明，LLM+Debrief 在生成有助于合作与协调的有意义且易于理解的自然语言消息方面比零样本LLM代理更为有效。我们的代码和演示视频可以在以下链接获取。 

---
