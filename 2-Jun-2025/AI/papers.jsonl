{'arxiv_id': 'arXiv:2505.24878', 'title': 'Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents', 'authors': 'Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen', 'link': 'https://arxiv.org/abs/2505.24878', 'abstract': 'CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.', 'abstract_zh': 'Open CaptchaWorld: 一个用于评估以大规模语言模型为基础的智能代理的视觉推理和交互能力的基准平台', 'title_zh': '开放CAPTCHA世界：一个全面的基于Web的平台，用于测试和基准测试多模态LLM代理'}
{'arxiv_id': 'arXiv:2505.24846', 'title': 'MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning', 'authors': 'Jingyan Shen, Jiarui Yao, Rui Yang, Yifan Sun, Feng Luo, Rui Pan, Tong Zhang, Han Zhao', 'link': 'https://arxiv.org/abs/2505.24846', 'abstract': 'Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization.', 'abstract_zh': '基于布雷德利-特里模型的奖励建模是将人类反馈强化学习（RLHF）应用于大型语言模型（LLMs）以构建安全基础模型中的关键步骤。然而，基于布雷德利-特里模型的奖励建模假设了全局奖励函数，未能捕捉到人类偏好内部的多样性和异质性。因此，这种简化限制了LLMs在支持个性化和多元性对齐方面的能力。理论上，我们证明当人类偏好遵循多种亚群的混合分布时，单个布雷德利-特里模型具有不可约错误。虽然现有的解决方案，如细粒度标注的多目标学习，有助于解决这个问题，但它们成本高昂且受限于预定义属性，未能充分捕捉人类价值观的丰富性。在本工作中，我们提出了一种两阶段框架MiCRo，该框架通过大规模二元偏好数据集增强个性化偏好学习，无需明确的细粒度标注。在第一阶段，MiCRo引入上下文感知混合建模方法以捕捉多样的人类偏好。在第二阶段，MiCRo集成了一个在线路由策略，该策略根据特定上下文动态调整混合权重以解决歧义，允许在最少额外监督的情况下实现高效且可扩展的偏好适应。在多个偏好数据集上的实验表明，MiCRo有效地捕捉到多样化的人类偏好，并显著提高了下游个性化的效果。', 'title_zh': 'MiCRo: 混合建模与上下文感知路由的个性化偏好学习'}
{'arxiv_id': 'arXiv:2505.24785', 'title': 'EXP-Bench: Can AI Conduct AI Research Experiments?', 'authors': 'Patrick Tser Jern Kon, Jiachen Liu, Xinyi Zhu, Qiuyi Ding, Jingjia Peng, Jiarong Xing, Yibo Huang, Yiming Qiu, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Matei Zaharia, Ang Chen', 'link': 'https://arxiv.org/abs/2505.24785', 'abstract': 'Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading LLM-based agents, such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness occasionally reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments. EXP-Bench is open-sourced at this https URL.', 'abstract_zh': '自动化AI研究具有加速科学进步的巨大潜力，然而当前的AI代理在处理严格的端到端实验复杂性方面存在困难。我们引入了EXP-Bench，这是一种新颖的基准，旨在系统评估源自有影响力AI出版物的完整研究实验。给定研究问题和不完整的起始代码，EXP-Bench要求AI代理提出假设，设计并实施实验程序，执行它们并分析结果。为了创建如此复杂且真实的任务，并且具有高保真度，我们设计了一种半自主的工作流来从这些研究论文及其相关的开源代码中提取和结构化关键实验细节。借助该工作流，EXP-Bench收集了来自51篇顶级AI研究论文的461个AI研究任务。对EXP-Bench的领先LLM基代理，如OpenHands和IterativeAgent的评估显示了部分能力：虽然在设计或实现正确性等个别实验方面得分偶尔达到20-35%，但可执行的完整实验成功率仅为0.5%。通过识别这些瓶颈并提供现实的逐步实验程序，EXP-Bench已成为未来AI代理改进其进行AI研究实验能力的重要工具。EXP-Bench在此处开放源代码：this https URL。', 'title_zh': 'EXP-Bench: AI能否开展AI研究实验？'}
{'arxiv_id': 'arXiv:2505.24784', 'title': 'AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models', 'authors': 'Conor Heins, Toon Van de Maele, Alexander Tschantz, Hampus Linander, Dimitrije Markovic, Tommaso Salvatori, Corrado Pezzato, Ozan Catal, Ran Wei, Magnus Koudahl, Marco Perin, Karl Friston, Tim Verbelen, Christopher Buckley', 'link': 'https://arxiv.org/abs/2505.24784', 'abstract': 'Current deep reinforcement learning (DRL) approaches achieve state-of-the-art performance in various domains, but struggle with data efficiency compared to human learning, which leverages core priors about objects and their interactions. Active inference offers a principled framework for integrating sensory information with prior knowledge to learn a world model and quantify the uncertainty of its own beliefs and predictions. However, active inference models are usually crafted for a single task with bespoke knowledge, so they lack the domain flexibility typical of DRL approaches. To bridge this gap, we propose a novel architecture that integrates a minimal yet expressive set of core priors about object-centric dynamics and interactions to accelerate learning in low-data regimes. The resulting approach, which we call AXIOM, combines the usual data efficiency and interpretability of Bayesian approaches with the across-task generalization usually associated with DRL. AXIOM represents scenes as compositions of objects, whose dynamics are modeled as piecewise linear trajectories that capture sparse object-object interactions. The structure of the generative model is expanded online by growing and learning mixture models from single events and periodically refined through Bayesian model reduction to induce generalization. AXIOM masters various games within only 10,000 interaction steps, with both a small number of parameters compared to DRL, and without the computational expense of gradient-based optimization.', 'abstract_zh': '当前的深度强化学习方法在各种领域中取得了最先进的性能，但在数据效率方面与利用对象及其相互作用核心先验的人类学习相比存在不足。主动推断提供了一种原理性的框架，将感知识别信息与先验知识结合起来学习世界模型，并量化其信念和预测的不确定性。然而，主动推断模型通常仅为单个任务定制了特有的先验知识，因此缺乏深度强化学习方法典型的领域灵活性。为了弥合这一差距，我们提出了一种新的架构，该架构融合了一组关于对象中心动力学和相互作用的最少但表达丰富的核心先验知识，以加快在低数据量条件下的学习。我们称之为AXIOM的方法结合了贝叶斯方法通常的数据效率和可解释性，以及深度强化学习方法通常的任务泛化能力。AXIOM将场景表示为对象的组合，其动力学被建模为分段线性轨迹，以捕捉稀疏的对象-对象交互。生成模型的结构通过从单个事件中生长和学习混合模型并在期间通过贝叶斯模型约简定期优化在线扩展，以促进泛化。AXIOM仅在10,000次交互步骤内就能掌握多种游戏，参数量比深度强化学习方法少，且没有基于梯度的优化所需的计算开销。', 'title_zh': 'AXIOM: 在几分钟内通过扩展对象中心模型学习玩视频游戏'}
{'arxiv_id': 'arXiv:2505.24655', 'title': 'Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models', 'authors': 'Frederike Lübeck, Jonas Wildberger, Frederik Träuble, Maximilian Mordig, Sergios Gatidis, Andreas Krause, Bernhard Schölkopf', 'link': 'https://arxiv.org/abs/2505.24655', 'abstract': 'Cardiovascular disease (CVD) risk prediction models are essential for identifying high-risk individuals and guiding preventive actions. However, existing models struggle with the challenges of real-world clinical practice as they oversimplify patient profiles, rely on rigid input schemas, and are sensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk prediction framework built on large language models extensively fine-tuned on over half a million participants from the UK Biobank. In benchmark comparisons, AdaCVD surpasses established risk scores and standard machine learning approaches, achieving state-of-the-art performance. Crucially, for the first time, it addresses key clinical challenges across three dimensions: it flexibly incorporates comprehensive yet variable patient information; it seamlessly integrates both structured data and unstructured text; and it rapidly adapts to new patient populations using minimal additional data. In stratified analyses, it demonstrates robust performance across demographic, socioeconomic, and clinical subgroups, including underrepresented cohorts. AdaCVD offers a promising path toward more flexible, AI-driven clinical decision support tools suited to the realities of heterogeneous and dynamic healthcare environments.', 'abstract_zh': '心血管疾病(CVD)风险预测模型对于识别高风险个体并指导预防措施至关重要。然而，现有模型在实际临床实践中面临简化患者特征、依赖固定输入模式以及对分布偏移敏感等挑战。我们开发了AdaCVD，这是一种基于广泛 Fine-Tuned 大型语言模型的心血管疾病风险预测框架，这些模型是在超过五十万来自英国生物库的参与者数据上训练的。在基准比较中，AdaCVD 超越了现有风险评分和标准机器学习方法，实现了最先进的性能。最关键的是，它首次从三个维度上解决了关键的临床挑战：灵活整合全面而多变的患者信息；无缝整合结构化数据和非结构化文本；并使用最少的额外数据迅速适应新的患者群体。在分层分析中，它在人口统计学、社会经济和临床子群体中，包括未充分代表的群体中表现出稳健的性能。AdaCVD 为更灵活的、基于人工智能的临床决策支持工具的发展提供了一条有希望的道路，这些工具适合异质且动态的医疗保健环境。', 'title_zh': '使用大型语言模型从异质数据中实现心血管疾病风险的适应性预测'}
{'arxiv_id': 'arXiv:2505.24622', 'title': 'Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success', 'authors': 'Ben Griffin, Joseph Ternasky, Fuat Alican, Yigit Ihlamur', 'link': 'https://arxiv.org/abs/2505.24622', 'abstract': 'Predicting startup success requires models that are both accurate and interpretable. We present a lightweight ensemble framework that combines YES/NO questions generated by large language models (LLMs), forming a transparent decision-making system. Each question acts as a weak heuristic, and by filtering, ranking, and aggregating them through a threshold-based voting mechanism, we construct a strong ensemble predictor. On a test set where 10% of startups are classified as successful, our approach achieves a precision rate of 50%, representing a 5x improvement over random selection, while remaining fully transparent. When we incorporate expert-guided heuristics into the generation process, performance improves further to 54% precision. These results highlight the value of combining LLM reasoning with human insight and demonstrate that simple, interpretable ensembles can support high-stakes decisions in domains such as venture capital (VC).', 'abstract_zh': '预测初创公司成功需要既准确又可解释的模型。我们提出了一种轻量级集成框架，该框架结合了由大规模语言模型（LLMs）生成的YES/NO问题，形成一个透明的决策系统。每个问题作为一种弱启发式，通过基于阈值的投票机制进行过滤、排序和聚合，构建一个强集成预测器。在测试集中，有10%的初创公司被分类为成功，我们的方法实现了50%的精确率，相比随机选择提高了5倍，同时保持完全透明。当我们将专家指导的启发式纳入生成过程时，精确率进一步提高到54%。这些结果凸显了将LLM推理与人力洞察相结合的价值，并展示了简单的可解释集成可以支持如风险投资（VC）等领域的重要决策。', 'title_zh': '随机规则森林 (RRF): 生成式大语言模型问题的可解释ensemble方法及其在预测创业成功中的应用'}
{'arxiv_id': 'arXiv:2505.24601', 'title': 'Taxonomic Networks: A Representation for Neuro-Symbolic Pairing', 'authors': 'Zekun Wang, Ethan L. Haarer, Nicki Barari, Christopher J. MacLellan', 'link': 'https://arxiv.org/abs/2505.24601', 'abstract': 'We introduce the concept of a \\textbf{neuro-symbolic pair} -- neural and symbolic approaches that are linked through a common knowledge representation. Next, we present \\textbf{taxonomic networks}, a type of discrimination network in which nodes represent hierarchically organized taxonomic concepts. Using this representation, we construct a novel neuro-symbolic pair and evaluate its performance. We show that our symbolic method learns taxonomic nets more efficiently with less data and compute, while the neural method finds higher-accuracy taxonomic nets when provided with greater resources. As a neuro-symbolic pair, these approaches can be used interchangeably based on situational needs, with seamless translation between them when necessary. This work lays the foundation for future systems that more fundamentally integrate neural and symbolic computation.', 'abstract_zh': '我们引入了神经符号对（neuro-symbolic pair）的概念——通过共同的知识表示连接神经和符号方法。接着，我们提出了分类网络（taxonomic networks）这一类型的选择网络，其中节点表示层次组织的分类概念。利用这种表示，我们构建了一种新的神经符号对并评估了其性能。我们证明，我们的符号方法可以在较少数据和计算资源的情况下更有效地学习分类网，而神经方法在获得更多资源时可以发现更高准确性的分类网。作为神经符号对，这些方法可以根据具体情况互换使用，并在必要时无缝转换。这项工作为未来更根本地整合神经和符号计算的系统奠定了基础。', 'title_zh': '税onomic网络：一种神经符号配对表示方法'}
{'arxiv_id': 'arXiv:2505.24597', 'title': 'Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction', 'authors': 'Shuai Liu, Ning Cao, Yile Chen, Yue Jiang, Gao Cong', 'link': 'https://arxiv.org/abs/2505.24597', 'abstract': 'Next location prediction plays a critical role in understanding human mobility patterns. However, existing approaches face two core limitations: (1) they fall short in capturing the complex, multi-functional semantics of real-world locations; and (2) they lack the capacity to model heterogeneous behavioral dynamics across diverse user groups. To tackle these challenges, we introduce NextLocMoE, a novel framework built upon large language models (LLMs) and structured around a dual-level Mixture-of-Experts (MoE) design. Our architecture comprises two specialized modules: a Location Semantics MoE that operates at the embedding level to encode rich functional semantics of locations, and a Personalized MoE embedded within the Transformer backbone to dynamically adapt to individual user mobility patterns. In addition, we incorporate a history-aware routing mechanism that leverages long-term trajectory data to enhance expert selection and ensure prediction stability. Empirical evaluations across several real-world urban datasets show that NextLocMoE achieves superior performance in terms of predictive accuracy, cross-domain generalization, and interpretability', 'abstract_zh': 'NextLocMoE：基于大语言模型和双层Mixture-of-Experts的设计以预测下一个位置', 'title_zh': '专家混合模型为个性化和语义aware的下一个位置预测'}
{'arxiv_id': 'arXiv:2505.24493', 'title': 'MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge', 'authors': 'Xin Jing, Jiadong Wang, Iosif Tsangko, Andreas Triantafyllopoulos, Björn W. Schuller', 'link': 'https://arxiv.org/abs/2505.24493', 'abstract': "Although speech emotion recognition (SER) has advanced significantly with deep learning, annotation remains a major hurdle. Human annotation is not only costly but also subject to inconsistencies annotators often have different preferences and may lack the necessary contextual knowledge, which can lead to varied and inaccurate labels. Meanwhile, Large Language Models (LLMs) have emerged as a scalable alternative for annotating text data. However, the potential of LLMs to perform emotional speech data annotation without human supervision has yet to be thoroughly investigated. To address these problems, we apply GPT-4o to annotate a multimodal dataset collected from the sitcom Friends, using only textual cues as inputs. By crafting structured text prompts, our methodology capitalizes on the knowledge GPT-4o has accumulated during its training, showcasing that it can generate accurate and contextually relevant annotations without direct access to multimodal inputs. Therefore, we propose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We demonstrate the effectiveness of MELT by fine-tuning four self-supervised learning (SSL) backbones and assessing speech emotion recognition performance across emotion datasets. Additionally, our subjective experiments\\' results demonstrate a consistence performance improvement on SER.", 'abstract_zh': '虽然深度学习推动了语音情感识别（SER）的显著进步，但标注仍然是一个主要障碍。人力标注不仅成本高昂，而且标注者之间存在不一致性，往往有不同的偏好并可能缺乏必要的上下文知识，这可能导致标注结果的差异性和不准确性。与此同时，大规模语言模型（LLMs）已经成为了标注文本数据的一种可扩展替代方案。然而，LLMs在无需人类监督的情况下对情绪语音数据进行标注的潜力尚未得到充分探索。为了解决这些问题，我们应用GPT-4o对源自 situation comedy《老友记》的多模态数据集进行标注，仅使用文本暗示作为输入。通过构建结构化的文本提示，我们的方法利用了GPT-4o在训练过程中积累的知识，展示了它在无需直接访问多模态输入的情况下生成准确且上下文相关的标注的能力。因此，我们提出了MELT，一个完全由GPT-4o标注的多模态情感数据集。我们通过微调四种自我监督学习（SSL）骨干模型并评估不同情感数据集的语音情感识别性能，展示了MELT的有效性。此外，我们的主观实验结果显示，MELT在语音情感识别（SER）上具有持续的性能改进。', 'title_zh': 'MELT: 通过利用嵌入的LLM知识朝着自动化多模态情感数据标注的方向努力'}
{'arxiv_id': 'arXiv:2505.24479', 'title': 'Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation', 'authors': 'Sania Nayab, Marco Simoni, Giulio Rossolini', 'link': 'https://arxiv.org/abs/2505.24479', 'abstract': 'The rapid spread of misinformation, further amplified by recent advances in generative AI, poses significant threats to society, impacting public opinion, democratic stability, and national security. Understanding and proactively assessing these threats requires exploring methodologies that enable structured and scalable misinformation generation. In this paper, we propose a novel approach that leverages knowledge graphs (KGs) as structured semantic resources to systematically generate fake triplets. By analyzing the structural properties of KGs, such as the distance between entities and their predicates, we identify plausibly false relationships. These triplets are then used to guide large language models (LLMs) in generating misinformation statements with varying degrees of credibility. By utilizing structured semantic relationships, our deterministic approach produces misinformation inherently challenging for humans to detect, drawing exclusively upon publicly available KGs (e.g., WikiGraphs).\nAdditionally, we investigate the effectiveness of LLMs in distinguishing between genuine and artificially generated misinformation. Our analysis highlights significant limitations in current LLM-based detection methods, underscoring the necessity for enhanced detection strategies and a deeper exploration of inherent biases in generative models.', 'abstract_zh': '近期生成AI技术的快速发展加剧了 misinformation 的传播，这对社会构成了重大威胁，影响公众意见、民主稳定和国家安全。理解并主动评估这些威胁需要探索能够实现结构化和可扩展的 misinformation 生成方法。本文提出了一种新的方法，利用知识图谱（KGs）作为结构化的语义资源，系统地生成虚假三元组。通过分析KGs的结构特性，如实体间的距离及其谓词，我们识别出可能虚假的关系。这些三元组随后用于指导大型语言模型（LLMs）生成具有不同可信度的虚假陈述。通过利用结构化的语义关系，我们的确定性方法生成的 misinformation 对人类而言具有难以检测的特性，仅依赖于公开可用的KGs（如WikiGraphs）。此外，我们探讨了LLMs在区分真实和人工生成的 misinformation 方面的有效性。我们的分析指出现有基于LLM的检测方法存在显著局限性，强调了改进检测策略并深入探索生成模型内在偏差的必要性。', 'title_zh': 'Leveraging知识图谱和大规模语言模型进行误导性信息的结构化生成'}
{'arxiv_id': 'arXiv:2505.24478', 'title': 'Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning', 'authors': 'Vasilije Markovic, Lazar Obradovic, Laszlo Hajdu, Jovan Pavlovic', 'link': 'https://arxiv.org/abs/2505.24478', 'abstract': "Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results in complex systems with numerous hyperparameters that directly affect performance. While such systems are increasingly common in retrieval-augmented generation, the role of systematic hyperparameter optimization remains underexplored. In this paper, we study this problem in the context of Cognee, a modular framework for end-to-end KG construction and retrieval. Using three multi-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize parameters related to chunking, graph construction, retrieval, and prompting. Each configuration is scored using established metrics (exact match, F1, and DeepEval's LLM-based correctness metric). Our results demonstrate that meaningful gains can be achieved through targeted tuning. While the gains are consistent, they are not uniform, with performance varying across datasets and metrics. This variability highlights both the value of tuning and the limitations of standard evaluation measures. While demonstrating the immediate potential of hyperparameter tuning, we argue that future progress will depend not only on architectural advances but also on clearer frameworks for optimization and evaluation in complex, modular systems.", 'abstract_zh': '将大型语言模型（LLMs）与知识图谱（KGs）集成产生了具有众多超参数的复杂系统，这些超参数直接影响性能。尽管此类系统在检索增强生成中越来越常见，但系统的系统性超参数优化的作用仍较少被探索。在本文中，我们以Cognee这一模块化框架为研究背景，探讨了知识图谱构建和检索的端到端框架中超参数优化的问题。我们使用三个多跳问答基准（HotPotQA、TwoWikiMultiHop和MuSiQue）来优化与切分、图构建、检索和提示相关的参数。每个配置使用已建立的评价指标（精确匹配、F1和DeepEval的LLM基于正确性指标）进行评分。我们的结果显示，通过有针对性的调优可以获得有意义的性能提升。虽然这些提升是稳定的，但并不是均匀的，性能在不同数据集和评价指标上存在差异。这种差异性突显了调优的价值以及标准评价措施的局限性。本文展示了超参数调优的即时潜力，但我们认为未来的发展不仅依赖于架构的进步，还需要在复杂的模块化系统中建立更清晰的优化和评价框架。', 'title_zh': '知识图谱与大型语言模型之间接口的优化以支持复杂推理'}
{'arxiv_id': 'arXiv:2505.24458', 'title': 'SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering Behaviors', 'authors': 'Tianlong Yu, Chenghang Ye, Zheyu Yang, Ziyi Zhou, Cui Tang, Zui Tao, Jun Zhang, Kailong Wang, Liting Zhou, Yang Yang, Ting Bi', 'link': 'https://arxiv.org/abs/2505.24458', 'abstract': "The SEAR Dataset is a novel multimodal resource designed to study the emerging threat of social engineering (SE) attacks orchestrated through augmented reality (AR) and multimodal large language models (LLMs). This dataset captures 180 annotated conversations across 60 participants in simulated adversarial scenarios, including meetings, classes and networking events. It comprises synchronized AR-captured visual/audio cues (e.g., facial expressions, vocal tones), environmental context, and curated social media profiles, alongside subjective metrics such as trust ratings and susceptibility assessments. Key findings reveal SEAR's alarming efficacy in eliciting compliance (e.g., 93.3% phishing link clicks, 85% call acceptance) and hijacking trust (76.7% post-interaction trust surge). The dataset supports research in detecting AR-driven SE attacks, designing defensive frameworks, and understanding multimodal adversarial manipulation. Rigorous ethical safeguards, including anonymization and IRB compliance, ensure responsible use. The SEAR dataset is available at this https URL.", 'abstract_zh': 'SEAR数据集是为研究通过增强现实(AR)和多模态大规模语言模型(LLMs)策划的社会工程(SE)攻击新兴威胁而设计的一种新型多模态资源。该数据集在模拟 adversarial 场景中捕捉了60名参与者共计180条标注对话，包括会议、课堂和社交活动。它包含同步捕获的AR视觉/音频提示（如面部表情、语音语调）、环境背景以及定制的社会媒体资料，还包括主观度量，如信任评分和易受攻击性评估。关键发现表明，SEAR在诱导合规性（例如，93.3%的鱼叉式钓鱼链接点击率，85%的通话接受率）和劫持信任（例如，交互后信任度提升76.7%）方面具有令人担忧的有效性。该数据集支持研究AR驱动的SE攻击检测、设计防御框架以及理解多模态对抗操纵。严格的伦理保障措施，包括匿名化和IRB合规性，确保了负责任的使用。SEAR数据集可通过以下链接访问：this https URL。', 'title_zh': 'SEAR：一种用于分析AR-LLM驱动的社会工程行为的多模态数据集'}
{'arxiv_id': 'arXiv:2505.24442', 'title': 'RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation', 'authors': 'Zhentao Xie, Chengcheng Han, Jinxin Shi, Wenjun Cui, Xin Zhao, Xingjiao Wu, Jiabao Zhao', 'link': 'https://arxiv.org/abs/2505.24442', 'abstract': "Although multi-agent systems based on large language models show strong capabilities on multiple tasks, they are still limited by high computational overhead, information loss, and robustness. Inspired by ResNet's residual learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual connections to optimize efficiency and reliability. To maximize information utilization from model responses while minimizing computational costs, we innovatively design an embedding-based diversity selection mechanism that greedily selects responses via vector similarity. Furthermore, to mitigate iterative information degradation, we introduce a Residual Extraction Agent to preserve cross-layer incremental information by capturing inter-layer response differences, coupled with a Residual Aggregation Agent for hierarchical information integration. Additionally, we propose an adaptive termination mechanism that dynamically halts processing based on residual convergence, further improving inference efficiency. RMoA achieves state-of-the-art performance on the benchmarks of across alignment, mathematical reasoning, code generation, and multitasking understanding, while significantly reducing computational overhead. Code is available at this https URL.", 'abstract_zh': '基于大语言模型的多代理系统尽管在多项任务中展示了强大的能力，但仍受限于高计算开销、信息损失和鲁棒性问题。受ResNet中残差学习的启发，我们提出了一种残差混合代理（RMoA）模型，通过引入残差连接来优化效率和可靠性。为了最大限度地利用模型响应中的信息同时最小化计算成本，我们创新性地设计了一种基于嵌入的多样性选择机制，通过向量相似性贪婪地选择响应。为进一步减轻迭代信息退化的问题，我们引入了一种残差提取代理，通过捕捉层间响应差异来保存跨层的增量信息，并结合一种残差聚合代理以实现层级信息整合。此外，我们提出了一种自适应终止机制，根据残差收敛情况动态停止处理过程，进一步提高推理效率。RMoA在跨对齐、数学推理、代码生成和多任务理解的基准测试中取得了前沿性能，同时显著减少了计算开销。代码见此链接。', 'title_zh': 'RMoA：通过多样性和残差补偿优化混合智能体'}
{'arxiv_id': 'arXiv:2505.24426', 'title': 'P: A Universal Measure of Predictive Intelligence', 'authors': 'David Gamez', 'link': 'https://arxiv.org/abs/2505.24426', 'abstract': 'Over the last thirty years, considerable progress has been made with the development of systems that can drive cars, play games, predict protein folding and generate natural language. These systems are described as intelligent and there has been a great deal of talk about the rapid increase in artificial intelligence and its potential dangers. However, our theoretical understanding of intelligence and ability to measure it lag far behind our capacity for building systems that mimic intelligent human behaviour. There is no commonly agreed definition of the intelligence that AI systems are said to possess. No-one has developed a practical measure that would enable us to compare the intelligence of humans, animals and AIs on a single ratio scale.\nThis paper sets out a new universal measure of intelligence that is based on the hypothesis that prediction is the most important component of intelligence. As an agent interacts with its normal environment, the accuracy of its predictions is summed up and the complexity of its predictions and perceived environment is accounted for using Kolmogorov complexity. Two experiments were carried out to evaluate the practical feasibility of the algorithm. These demonstrated that it could measure the intelligence of an agent embodied in a virtual maze and an agent that makes predictions about time-series data. This universal measure could be the starting point for a new comparative science of intelligence that ranks humans, animals and AIs on a single ratio scale.', 'abstract_zh': '过去三十年间，随着能够驾驶汽车、玩游戏、预测蛋白质折叠和生成自然语言的系统的发展，取得了一定的进步。这些系统被描述为智能的，并且关于人工智能的迅速发展及其潜在危险的讨论很多。然而，对于智能的理论理解以及衡量智能的能力远远落后于构建模仿人类智能行为的系统的能力建立。目前尚无一致认可的人工智能系统所具备的智能的定义。也没有开发出有效的测量工具，以在单一比例尺度上比较人类、动物和人工智能的智能水平。本文提出了一种新的通用智能度量标准，基于预测是智能最重要组成部分的假设。通过一个代理与其正常环境的交互，其预测的准确性被汇总，预测的复杂性和感知环境的复杂性则使用柯尔莫哥洛夫复杂性来进行量化。进行了两个实验以评估该算法的实际可行性。结果显示，它可以测量在虚拟迷宫中行动的代理和预测时间序列数据的代理的智能水平。这种通用的度量标准可能是建立一个新的智能比较科学的基础，该科学能在单一比例尺度上对人类、动物和人工智能进行排名。', 'title_zh': 'P: 通用预测智能度量'}
{'arxiv_id': 'arXiv:2505.24422', 'title': 'Three Kinds of Negation in Knowledge and Their Mathematical Foundations', 'authors': 'Zhenghua Pan, Yong Wang', 'link': 'https://arxiv.org/abs/2505.24422', 'abstract': 'In the field of artificial intelligence, understanding, distinguishing, expressing, and computing the negation in knowledge is a fundamental issue in knowledge processing and research. In this paper, we examine and analyze the understanding and characteristics of negation in various fields such as philosophy, logic, and linguistics etc. Based on the distinction between the concepts of contradiction and opposition, we propose that there are three different types of negation in knowledge from a conceptual perspective: contradictory negation, opposite negation, and intermediary negation. To establish a mathematical foundation that fully reflects the intrinsic connections, properties, and laws of these different forms of negation, we introduce SCOI: sets with contradictory negation, opposite negation and intermediary negation, and LCOI: logic with contradictory negation, opposite negation and intermediary negation, and we proved the main operational properties of SCOI as well as the formal inference relations in LCOI.', 'abstract_zh': '在人工智能领域，理解、区分、表达和计算知识中的否定是知识处理和研究中的一个基本问题。本文从哲学、逻辑和语言学等多个领域考察和分析了否定的理解与特性，基于矛盾与对立概念的区别，提出了从概念上知识中有三种不同类型的否定：矛盾否定、对立否定和中间否定。为全面反映这些不同形式否定的内在联系、属性和规律，我们引入了SCOI：具有矛盾否定、对立否定和中间否定的集合，以及LCOI：具有矛盾否定、对立否定和中间否定的逻辑，并证明了SCOI的主要操作性质以及LCOI中的形式推理关系。', 'title_zh': '知识中的三种否定及其数学基础'}
{'arxiv_id': 'arXiv:2505.24306', 'title': 'GridRoute: A Benchmark for LLM-Based Route Planning with Cardinal Movement in Grid Environments', 'authors': 'Kechen Li, Yaotian Tao, Ximing Wen, Quanwei Sun, Zifei Gong, Chang Xu, Xizhe Zhang, Tianbo Ji', 'link': 'https://arxiv.org/abs/2505.24306', 'abstract': "Recent advancements in Large Language Models (LLMs) have demonstrated their potential in planning and reasoning tasks, offering a flexible alternative to classical pathfinding algorithms. However, most existing studies focus on LLMs' independent reasoning capabilities and overlook the potential synergy between LLMs and traditional algorithms. To fill this gap, we propose a comprehensive evaluation benchmark GridRoute to assess how LLMs can take advantage of traditional algorithms. We also propose a novel hybrid prompting technique called Algorithm of Thought (AoT), which introduces traditional algorithms' guidance into prompting. Our benchmark evaluates six LLMs ranging from 7B to 72B parameters across various map sizes, assessing their performance in correctness, optimality, and efficiency in grid environments with varying sizes. Our results show that AoT significantly boosts performance across all model sizes, particularly in larger or more complex environments, suggesting a promising approach to addressing path planning challenges. Our code is open-sourced at this https URL.", 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）在路径规划任务中的潜力体现：GridRoute基准评估LLMs与传统算法的协同效应及一种新的混合提示技术（AoT）', 'title_zh': 'GridRoute：基于格网运动的LLM路由规划基准'}
{'arxiv_id': 'arXiv:2505.24292', 'title': 'Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules', 'authors': 'Yueqi Zhang, Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li', 'link': 'https://arxiv.org/abs/2505.24292', 'abstract': 'Human-AI conversation frequently relies on quoting earlier text-"check it with the formula I just highlighted"-yet today\'s large language models (LLMs) lack an explicit mechanism for locating and exploiting such spans. We formalise the challenge as span-conditioned generation, decomposing each turn into the dialogue history, a set of token-offset quotation spans, and an intent utterance. Building on this abstraction, we introduce a quotation-centric data pipeline that automatically synthesises task-specific dialogues, verifies answer correctness through multi-stage consistency checks, and yields both a heterogeneous training corpus and the first benchmark covering five representative scenarios. To meet the benchmark\'s zero-overhead and parameter-efficiency requirements, we propose QuAda, a lightweight training-based method that attaches two bottleneck projections to every attention head, dynamically amplifying or suppressing attention to quoted spans at inference time while leaving the prompt unchanged and updating < 2.8% of backbone weights. Experiments across models show that QuAda is suitable for all scenarios and generalises to unseen topics, offering an effective, plug-and-play solution for quotation-aware dialogue.', 'abstract_zh': '人类与AI对话经常需要引用之前的文本——“请检查我刚刚强调的公式”——但当前的大规模语言模型（LLMs）缺乏明确机制来定位和利用这些片段。我们将这一挑战形式化为基于片段生成，将每次对话分解为对话历史、一组标记偏移引用片段以及意图陈述。在此抽象基础上，我们引入了一种以引用为中心的数据管道，可以自动合成特定任务的对话，通过多阶段一致性检查验证答案的正确性，并产生一个异质训练语料库和覆盖五个代表性场景的第一套基准测试。为了满足基准测试的零开销和参数效率要求，我们提出了QuAda，一种轻量级的基于训练的方法，为每个注意力头添加了两个瓶颈投影，在推理时动态放大或抑制对引用片段的注意力，而不改变提示，并更新少于2.8%的骨干权重。实验表明，QuAda适用于所有场景，并可以迁移到未见过的主题，提供了一种有效且即插即用的引用意识对话解决方案。', 'title_zh': '留意引用：通过插件式模块在大模型中实现引用意识对话'}
{'arxiv_id': 'arXiv:2505.24273', 'title': 'How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning', 'authors': 'Hongyi James Cai, Junlin Wang, Xiaoyin Chen, Bhuwan Dhingra', 'link': 'https://arxiv.org/abs/2505.24273', 'abstract': 'Recent breakthroughs in large language models (LLMs) have effectively improved their reasoning abilities, particularly on mathematical and logical problems that have verifiable answers, through techniques such as supervised finetuning (SFT) and reinforcement learning (RL). Prior research indicates that RL effectively internalizes search strategies, enabling long chain-of-thought (CoT) reasoning, with backtracking emerging naturally as a learned capability. However, the precise benefits of backtracking, specifically, how significantly it contributes to reasoning improvements and the optimal extent of its use, remain poorly understood. In this work, we systematically investigate the dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self Reference. Our findings highlight that short CoT sequences used in SFT as a warm-up do have moderate contribution to RL training, compared with cold-start RL; however such contribution diminishes when tasks become increasingly difficult. Motivated by this observation, we construct synthetic datasets varying systematically in the number of backtracking steps and conduct controlled experiments to isolate the influence of either the correctness (content) or the structure (i.e., backtrack frequency). We find that (1) longer CoT with backtracks generally induce better and more stable RL training, (2) more challenging problems with larger search space tend to need higher numbers of backtracks during the SFT stage. Additionally, we demonstrate through experiments on distilled data that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Collectively, our results offer practical insights into designing optimal training strategies to effectively scale reasoning in LLMs.', 'abstract_zh': 'Recent突破在大型语言模型（LLMs）中的近期突破有效提高了它们在数学和逻辑问题上的推理能力，这些问题具有可验证的答案，通过监督微调（SFT）和强化学习（RL）等技术。先前的研究表明，RL能够有效内化搜索策略，使长链推理（CoT）成为自然学习的能力，其中回溯作为一种学习能力自然出现。然而，回溯的具体益处，特别是它对推理改进的贡献程度及其最佳使用程度，仍然 poorly understood. 在这项工作中，我们系统地研究了八项推理任务（Countdown、Sudoku、Arc 1D、Geometry、Color Cube Rotation、List Functions、Zebra Puzzles 和 Self Reference）中SFT和RL之间的动态关系。我们的研究发现表明，在SFT作为预热使用的短CoT序列在RL训练中确实有一定的贡献，与冷启动的RL相比；然而，当任务变得越来越困难时，这种贡献会逐渐减弱。基于这一观察，我们构建了系统变化回溯步数的合成数据集，并进行控制实验以分离内容（即正确性）或结构（即回溯频率）的影响。我们发现（1）带有回溯的较长CoT通常会导致更好的和更稳定的RL训练；（2）更具挑战性、搜索空间更大的问题在SFT阶段通常需要更多的回溯。另外，我们通过蒸馏数据的实验表明，长CoT序列的正确性对RL训练的影响很小，表明RL更侧重于结构模式而非内容正确性。总体而言，我们的结果为设计有效扩展LLMs推理的训练策略提供了实际见解。', 'title_zh': '多少回溯足够？探索SFT和RL在增强LLM推理中的相互作用'}
{'arxiv_id': 'arXiv:2505.24260', 'title': 'Generative AI for Urban Design: A Stepwise Approach Integrating Human Expertise with Multimodal Diffusion Models', 'authors': 'Mingyi He, Yuebing Liang, Shenhao Wang, Yunhan Zheng, Qingyi Wang, Dingyi Zhuang, Li Tian, Jinhua Zhao', 'link': 'https://arxiv.org/abs/2505.24260', 'abstract': 'Urban design is a multifaceted process that demands careful consideration of site-specific constraints and collaboration among diverse professionals and stakeholders. The advent of generative artificial intelligence (GenAI) offers transformative potential by improving the efficiency of design generation and facilitating the communication of design ideas. However, most existing approaches are not well integrated with human design workflows. They often follow end-to-end pipelines with limited control, overlooking the iterative nature of real-world design. This study proposes a stepwise generative urban design framework that integrates multimodal diffusion models with human expertise to enable more adaptive and controllable design processes. Instead of generating design outcomes in a single end-to-end process, the framework divides the process into three key stages aligned with established urban design workflows: (1) road network and land use planning, (2) building layout planning, and (3) detailed planning and rendering. At each stage, multimodal diffusion models generate preliminary designs based on textual prompts and image-based constraints, which can then be reviewed and refined by human designers. We design an evaluation framework to assess the fidelity, compliance, and diversity of the generated designs. Experiments using data from Chicago and New York City demonstrate that our framework outperforms baseline models and end-to-end approaches across all three dimensions. This study underscores the benefits of multimodal diffusion models and stepwise generation in preserving human control and facilitating iterative refinements, laying the groundwork for human-AI interaction in urban design solutions.', 'abstract_zh': '基于多模态扩散模型的分步生成城市设计框架', 'title_zh': '基于多模态扩散模型融合人类 expertise 的城市设计生成式人工智能：逐步方法'}
{'arxiv_id': 'arXiv:2505.24258', 'title': 'FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation', 'authors': 'Vishal Pallagani, Nitin Gupta, John Aydin, Biplav Srivastava', 'link': 'https://arxiv.org/abs/2505.24258', 'abstract': "Understanding how data moves, transforms, and persists, known as data flow, is fundamental to reasoning in procedural tasks. Despite their fluency in natural and programming languages, large language models (LLMs), although increasingly being applied to decisions with procedural tasks, have not been systematically evaluated for their ability to perform data-flow reasoning. We introduce FABLE, an extensible benchmark designed to assess LLMs' understanding of data flow using structured, procedural text. FABLE adapts eight classical data-flow analyses from software engineering: reaching definitions, very busy expressions, available expressions, live variable analysis, interval analysis, type-state analysis, taint analysis, and concurrency analysis. These analyses are instantiated across three real-world domains: cooking recipes, travel routes, and automated plans. The benchmark includes 2,400 question-answer pairs, with 100 examples for each domain-analysis combination. We evaluate three types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a general-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code 8B). Each model is tested using majority voting over five sampled completions per prompt. Results show that the reasoning model achieves higher accuracy, but at the cost of over 20 times slower inference compared to the other models. In contrast, the general-purpose and code-specific models perform close to random chance. FABLE provides the first diagnostic benchmark to systematically evaluate data-flow reasoning and offers insights for developing models with stronger procedural understanding.", 'abstract_zh': '理解数据如何流动、变换和持久化，即数据流，是进行程序任务推理的基础。尽管大型语言模型（LLMs）在自然语言和编程语言方面表现出色，并且越来越多地应用于程序任务的决策中，但它们在执行数据流推理方面的能力尚未进行系统的评估。我们引入了FABLE，一个可扩展的基准测试，旨在使用结构化程序语言评估LLMs对数据流的理解。FABLE采用了软件工程中的八种经典数据流分析方法：可达定义、非常活跃的表达式、可用表达式、活动变量分析、区间分析、类型状态分析、污染分析和并发分析。这些分析方法被应用在三个实际领域中：烹饪食谱、旅行路线和自动化计划中。该基准测试包括2400个问答对，每个领域-分析组合有100个示例。我们评估了三种类型的LLMs：一个侧重推理的模型（DeepSeek-R1 8B）、一个通用模型（LLaMA 3.1 8B）和一个代码特定模型（Granite Code 8B）。每个模型都在五个采样完成中使用多数投票进行测试。结果表明，侧重推理的模型在准确率上更高，但推断速度比其他模型慢20多倍。相比之下，通用模型和代码特定模型的表现接近随机猜测。FABLE提供了一个诊断基准，首次系统性地评估数据流推理，并为开发具有更强程序理解能力的模型提供了见解。', 'title_zh': 'FABLE：一种新型过程性文本数据流分析基准，用于大型语言模型评估'}
{'arxiv_id': 'arXiv:2505.24230', 'title': 'ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction', 'authors': 'Murari Ambati', 'link': 'https://arxiv.org/abs/2505.24230', 'abstract': "We propose ProofNet++, a neuro-symbolic framework that enhances automated theorem proving by combining large language models (LLMs) with formal proof verification and self-correction mechanisms. Current LLM-based systems suffer from hallucinated logical steps and unverifiable reasoning. ProofNet++ mitigates these limitations by integrating symbolic proof tree supervision, a reinforcement learning loop using verifiers as reward functions, and an iterative self-correction module. Our experiments on miniF2F, Lean's mathlib, and HOL Light show that ProofNet++ significantly improves proof accuracy, correctness, and formal verifiability over prior models. We provide theoretical analysis of the convergence and stability of the verifier-guided RL framework and release our datasets and codebase for future research.", 'abstract_zh': 'ProofNet++：一种结合大型语言模型、形式证明验证和自纠错机制的神经符号框架', 'title_zh': 'ProofNet++：一种具备自校正能力的神经符号系统形式证明验证'}
{'arxiv_id': 'arXiv:2505.24226', 'title': 'E^2GraphRAG: Streamlining Graph-based RAG for High Efficiency and Effectiveness', 'authors': 'Yibo Zhao, Jiapeng Zhu, Ye Guo, Kangkang He, Xiang Li', 'link': 'https://arxiv.org/abs/2505.24226', 'abstract': 'Graph-based RAG methods like GraphRAG have shown promising global understanding of the knowledge base by constructing hierarchical entity graphs. However, they often suffer from inefficiency and rely on manually pre-defined query modes, limiting practical use. In this paper, we propose E^2GraphRAG, a streamlined graph-based RAG framework that improves both Efficiency and Effectiveness. During the indexing stage, E^2GraphRAG constructs a summary tree with large language models and an entity graph with SpaCy based on document chunks. We then construct bidirectional indexes between entities and chunks to capture their many-to-many relationships, enabling fast lookup during both local and global retrieval. For the retrieval stage, we design an adaptive retrieval strategy that leverages the graph structure to retrieve and select between local and global modes. Experiments show that E^2GraphRAG achieves up to 10 times faster indexing than GraphRAG and 100 times speedup over LightRAG in retrieval while maintaining competitive QA performance.', 'abstract_zh': '基于图的RAG方法如GraphRAG通过构建层次实体图展示了对知识库的全局理解潜力，但往往存在效率低下和依赖手动预定义查询模式的问题，限制了其实用性。本文提出了一种改进效率和效果的流线型基于图的RAG框架E^2GraphRAG。在索引阶段，E^2GraphRAG利用大型语言模型和基于文档片段的SpaCy构建摘要树和实体图。然后构建实体与片段之间的双向索引，以捕捉它们的多对多关系，在局部和全局检索中均能实现快速查找。在检索阶段，设计了一种适应性的检索策略，利用图结构在局部和全局模式之间进行检索和选择。实验结果显示，与GraphRAG相比，E^2GraphRAG的索引速度提高了10倍，与LightRAG相比检索速度提高了100倍，同时保持了竞争力的问答性能。', 'title_zh': 'E^2GraphRAG: 简化基于图的RAG以提高效率和效果'}
{'arxiv_id': 'arXiv:2505.24208', 'title': 'Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap', 'authors': 'Wenhan Yang, Spencer Stice, Ali Payani, Baharan Mirzasoleiman', 'link': 'https://arxiv.org/abs/2505.24208', 'abstract': "Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for their reliable deployment. However, LVLMs suffer from drastic safety degradation compared to their LLM backbone. Even blank or irrelevant images can trigger LVLMs to generate harmful responses to prompts that would otherwise be refused in text-only contexts. The modality gap between image and text representations has been recently hypothesized to contribute to safety degradation of LVLMs. However, if and how the amount of modality gap affects LVLMs' safety is not studied. In this work, we show that the amount of modality gap is highly inversely correlated with VLMs' safety. Then, we show that this modality gap is introduced during pretraining LVLMs and persists through fine-tuning. Inspired by this observation, we propose a regularization to reduce the modality gap during pretraining. Our extensive experiments on LLaVA v1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves safety alignment of LVLMs, reducing unsafe rate by up to 16.3% without compromising performance, and can further boost existing defenses by up to 18.2%.", 'abstract_zh': '确保视觉-语言模型生成安全输出对于其可靠部署至关重要。然而，LVLMs在安全性上大幅恶化，相较于其LLM主干。即使是空白或无关的图片也能使LVLMs生成有害的响应，而在仅文本情境下这些响应原本会被拒绝。图像和文本表示之间的模态差距最近被假设为导致LVLMs安全性下降的因素之一。然而，模态差距的数量如何影响LVLMs的安全性尚未被研究。在本文中，我们展示了模态差距的数量与VLMs的安全性之间存在高度的负相关。然后，我们表明这种模态差距是在预训练LVLMs过程中引入的，并且在微调过程中持续存在。受此观察的启发，我们提出了一种正则化方法来减少预训练过程中的模态差距。我们在LLaVA v1.5、ShareGPT4V和MiniGPT-4上的广泛实验表明，我们的方法显著改善了LVLMs的安全性对齐，将不安全率降低了最多16.3%而不会牺牲性能，并且可以进一步增强现有的防御措施最多18.2%。', 'title_zh': '通过减少预训练模态差距来提升大模型鲁棒性以保障多模态模型安全'}
{'arxiv_id': 'arXiv:2505.24201', 'title': 'SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems', 'authors': 'Xu He, Di Wu, Yan Zhai, Kun Sun', 'link': 'https://arxiv.org/abs/2505.24201', 'abstract': "The rise of large language model (LLM)-based multi-agent systems (MAS) introduces new security and reliability challenges. While these systems show great promise in decomposing and coordinating complex tasks, they also face multi-faceted risks across prompt manipulation, unsafe tool usage, and emergent agent miscoordination. Existing guardrail mechanisms offer only partial protection, primarily at the input-output level, and fall short in addressing systemic or multi-point failures in MAS. In this work, we present a system-level anomaly detection framework tailored for MAS, integrating structural modeling with runtime behavioral oversight. Our approach consists of two components. First, we propose a graph-based framework that models agent interactions as dynamic execution graphs, enabling semantic anomaly detection at node, edge, and path levels. Second, we introduce a pluggable SentinelAgent, an LLM-powered oversight agent that observes, analyzes, and intervenes in MAS execution based on security policies and contextual reasoning. By bridging abstract detection logic with actionable enforcement, our method detects not only single-point faults and prompt injections but also multi-agent collusion and latent exploit paths. We validate our framework through two case studies, including an email assistant and Microsoft's Magentic-One system, demonstrating its ability to detect covert risks and provide explainable root-cause attribution. Our work lays the foundation for more trustworthy, monitorable, and secure agent-based AI ecosystems.", 'abstract_zh': '基于大型语言模型（LLM）的多代理系统（MAS）兴起引入了新的安全性和可靠性挑战。尽管这些系统在分解和协调复杂任务方面展现出巨大潜力，但也面临着包括提示操纵、不安全工具使用以及代理 emergent 不安全协调等多方面的风险。现有的护栏机制仅在输入输出层面提供部分保护，并无法有效应对MAS中的系统性或多点故障。在本工作中，我们提出了一种针对MAS的系统级异常检测框架，结合结构建模和运行时行为监督。我们的方法包含两个组件。首先，我们提出了一种基于图的框架，将代理交互建模为动态执行图，实现节点、边和路径级别的语义异常检测。其次，我们引入了一个可插拔的SentinelAgent，这是一个由LLM驱动的监督代理，基于安全策略和情境推理，在MAS执行过程中进行观察、分析和干预。通过将抽象的检测逻辑与可执行的强制措施相结合，我们的方法不仅能检测单点故障和提示注入，还能检测多代理共谋和潜在利用路径。我们通过两个案例研究，包括邮件助手和微软的Magentic-One系统，验证了该框架能够检测隐蔽风险，并提供可解释的根本原因归因。我们的工作为基础更值得信赖、可监控和安全的基于代理的AI生态系统奠定了基础。', 'title_zh': 'SentinelAgent：多智能体系统中的基于图的异常检测'}
{'arxiv_id': 'arXiv:2505.24197', 'title': 'Learning API Functionality from Demonstrations for Tool-based Agents', 'authors': 'Bhrij Patel, Ashish Jagmohan, Aditya Vempaty', 'link': 'https://arxiv.org/abs/2505.24197', 'abstract': "Digital tool-based agents that invoke external Application Programming Interfaces (APIs) often rely on documentation to understand API functionality. However, such documentation is frequently missing, outdated, privatized, or inconsistent-hindering the development of reliable, general-purpose agents. In this work, we propose learning API functionality directly from demonstrations as a new paradigm applicable in scenarios without documentation. Using existing API benchmarks, we collect demonstrations from both expert API-based agents and from self-exploration. To understand what information demonstrations must convey for successful task completion, we extensively study how the number of demonstrations and the use of LLM-generated summaries and evaluations affect the task success rate of the API-based agent. Our experiments across 3 datasets and 5 models show that learning functionality from demonstrations remains a non-trivial challenge, even for state-of-the-art LLMs. We find that providing explicit function calls and natural language critiques significantly improves the agent's task success rate due to more accurate parameter filling. We analyze failure modes, identify sources of error, and highlight key open challenges for future work in documentation-free, self-improving, API-based agents.", 'abstract_zh': '基于数字工具的代理通过调用外部应用编程接口（API）来实现功能，通常依赖于文档。然而，此类文档经常缺失、过时、私有化或不一致，这阻碍了可靠、通用代理的开发。在本项工作中，我们提出了一种新的 paradigmn，即直接从演示学习 API 功能，适用于无文档场景。利用现有的 API 基准，我们从专家API基础上的代理和自我探索中收集演示数据。为了了解演示必须传达哪些信息才能成功完成任务，我们广泛研究了演示的数量以及LLM生成的摘要和评估如何影响基于API的代理的任务成功率。在3个数据集和5个模型上的实验表明，即使对于最先进的LLM，从演示中学习功能仍是一项非平凡的挑战。我们发现，提供显式的函数调用和自然语言批评显著提高了代理的任务成功率，因为这有助于更准确地填充参数。我们分析了失败模式，确定了错误的来源，并指出了未来无文档、自我改进的API基础代理的主要开放挑战。', 'title_zh': '基于演示学习工具代理的功能性'}
{'arxiv_id': 'arXiv:2505.24181', 'title': 'SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought', 'authors': 'Guanghao Li, Wenhao Jiang, Mingfeng Chen, Yan Li, Hao Yu, Shuting Dong, Tao Ren, Ming Tang, Chun Yuan', 'link': 'https://arxiv.org/abs/2505.24181', 'abstract': 'Chain of Thought (CoT) prompting improves the reasoning performance of large language models (LLMs) by encouraging step by step thinking. However, CoT-based methods depend on intermediate reasoning steps, which limits scalability and generalization. Recent work explores recursive reasoning, where LLMs reuse internal layers across iterations to refine latent representations without explicit CoT supervision. While promising, these approaches often require costly pretraining and lack a principled framework for how reasoning should evolve across iterations. We address this gap by introducing Flow Chain of Thought (Flow CoT), a reasoning paradigm that models recursive inference as a progressive trajectory of latent cognitive states. Flow CoT frames each iteration as a distinct cognitive stage deepening reasoning across iterations without relying on manual supervision. To realize this, we propose SCOUT (Stepwise Cognitive Optimization Using Teachers), a lightweight fine tuning framework that enables Flow CoT style reasoning without the need for pretraining. SCOUT uses progressive distillation to align each iteration with a teacher of appropriate capacity, and a cross attention based retrospective module that integrates outputs from previous iterations while preserving the models original computation flow. Experiments across eight reasoning benchmarks show that SCOUT consistently improves both accuracy and explanation quality, achieving up to 1.8% gains under fine tuning. Qualitative analyses further reveal that SCOUT enables progressively deeper reasoning across iterations refining both belief formation and explanation granularity. These results not only validate the effectiveness of SCOUT, but also demonstrate the practical viability of Flow CoT as a scalable framework for enhancing reasoning in LLMs.', 'abstract_zh': 'Flow Chain of Thought (Flow CoT) 提高了大语言模型 (LLMs) 的推理性能，通过鼓励逐步思考。然而，基于 CoT 的方法依赖于中间推理步骤，这限制了其可扩展性和泛化能力。最近的研究探索了递归推理，其中 LLGs 在迭代中重新利用内部层以精炼潜在表示，而无需显式的 CoT 监督。尽管前景广阔，这些方法通常需要昂贵的预训练，并缺乏推理如何在迭代中逐步进化的原则性框架。我们通过引入 Flow Chain of Thought (Flow CoT)，一种将递归推理建模为潜在认知状态的逐步轨迹的推理范式来弥补这一缺口。Flow CoT 将每个迭代视为一个独特的认知阶段，无需依赖手动监督即可跨迭代加深推理。为了实现这一点，我们提出了 SCOUT（渐进认知优化教师使用），一种轻量级微调框架，使 Flow CoT 样式的推理无需预训练即可实现。SCOUT 使用渐进式蒸馏将每个迭代与相应能力的教师对齐，并使用跨注意力基于的反思模块整合先前迭代的输出，同时保留模型的原始计算流程。在八个推理基准上的实验表明，SCOUT 一致地提高了准确性和解释质量，在微调时最高可达 1.8% 的提升。定性分析进一步表明，SCOUT 使推理在每个迭代中逐步加深，改进了信念形成和解释粒度。这些结果不仅验证了 SCOUT 的有效性，还证明了 Flow CoT 作为增强 LLMs 推理的可扩展框架的实用性。', 'title_zh': 'SCOUT: 教学预训练语言模型通过思维流程增强推理能力'}
{'arxiv_id': 'arXiv:2505.24073', 'title': 'mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation', 'authors': 'Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Zhengzhong Tu', 'link': 'https://arxiv.org/abs/2505.24073', 'abstract': 'Large Vision-Language Models (LVLMs) have made remarkable strides in multimodal tasks such as visual question answering, visual grounding, and complex reasoning. However, they remain limited by static training data, susceptibility to hallucinations, and inability to verify claims against up-to-date, external evidence, compromising their performance in dynamic real-world applications. Retrieval-Augmented Generation (RAG) offers a practical solution to mitigate these challenges by allowing the LVLMs to access large-scale knowledge databases via retrieval mechanisms, thereby grounding model outputs in factual, contextually relevant information. Here in this paper, we conduct the first systematic dissection of the multimodal RAG pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the modality configurations and retrieval strategies, (2) the re-ranking stage: on strategies to mitigate positional biases and improve the relevance of retrieved evidence, and (3) the generation phase: we further investigate how to best integrate retrieved candidates into the final generation process. Finally, we extend to explore a unified agentic framework that integrates re-ranking and generation through self-reflection, enabling LVLMs to select relevant evidence and suppress irrelevant context dynamically. Our full-stack exploration of RAG for LVLMs yields substantial insights, resulting in an average performance boost of 5% without any fine-tuning.', 'abstract_zh': '大型多模态视觉语言模型在多模态任务中的 Retrieval-Augmented Generation：首个系统剖析及自省框架探究', 'title_zh': 'mRAG：阐明多模态检索增强生成的设计空间'}
{'arxiv_id': 'arXiv:2505.24037', 'title': 'Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution', 'authors': 'Qiao Xiao, Alan Ansell, Boqian Wu, Lu Yin, Mykola Pechenizkiy, Shiwei Liu, Decebal Constantin Mocanu', 'link': 'https://arxiv.org/abs/2505.24037', 'abstract': 'Large language models (LLMs) have achieved remarkable success across various tasks but face deployment challenges due to their massive computational demands. While post-training pruning methods like SparseGPT and Wanda can effectively reduce the model size, but struggle to maintain model performance at high sparsity levels, limiting their utility for downstream tasks. Existing fine-tuning methods, such as full fine-tuning and LoRA, fail to preserve sparsity as they require updating the whole dense metrics, not well-suited for sparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a novel method designed specifically for sparse LLMs. SEFT dynamically evolves the sparse topology of pruned models during fine-tuning, while preserving the overall sparsity throughout the process. The strengths of SEFT lie in its ability to perform task-specific adaptation through a weight drop-and-grow strategy, enabling the pruned model to self-adapt its sparse connectivity pattern based on the target dataset. Furthermore, a sensitivity-driven pruning criterion is employed to ensure that the desired sparsity level is consistently maintained throughout fine-tuning. Our experiments on various LLMs, including LLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks demonstrate that SEFT achieves stronger performance while offering superior memory and time efficiency compared to existing baselines. Our code is publicly available at: this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在各种任务中取得了显著成功，但由于其巨大的计算需求面临部署挑战。虽然后训练修剪方法如SparseGPT和Wanda可以有效减小模型规模，但在高稀疏度水平下保持模型性能方面存在困难，限制了其在下游任务中的实用性。现有微调方法，如全面微调和LoRA，在保持稀疏性方面表现不佳，因为它们需要更新整个密集模型，不适用于稀疏LLMs。在本文中，我们提出了一种专为稀疏LLMs设计的新方法：稀疏性演变微调（SEFT）。SEFT在微调过程中动态演化修剪模型的稀疏拓扑结构，同时在整个过程中保持整体稀疏性。SEFT的优势在于通过权重丢失和生长策略实现任务特定适应的能力，使修剪模型能够根据目标数据集自适应其稀疏连通模式。此外，采用敏感性驱动的修剪准则确保在整个微调过程中维持所需的稀疏性水平。我们在包括LLaMA家族、DeepSeek和Mistral在内的多种LLMs上，使用多种基准测试表明，SEFT在性能、内存效率和时间效率方面优于现有基线。我们的代码已公开可用：this https URL。', 'title_zh': '交给专家来处理：通过稀疏进化进行稀疏大型语言模型的修复与微调'}
{'arxiv_id': 'arXiv:2505.24036', 'title': 'GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs', 'authors': 'Amel Gader, Alsayed Algergawy', 'link': 'https://arxiv.org/abs/2505.24036', 'abstract': 'Knowledge graph completion aims to address the gaps of knowledge bases by adding new triples that represent facts. The complexity of this task depends on how many parts of a triple are already known. Instance completion involves predicting the relation-tail pair when only the head is given (h, ?, ?). Notably, modern knowledge bases often contain entity descriptions and types, which can provide valuable context for inferring missing facts. By leveraging these textual descriptions and the ability of large language models to extract facts from them and recognize patterns within the knowledge graph schema, we propose an LLM-powered, end-to-end instance completion approach. Specifically, we introduce GenIC: a two-step Generative Instance Completion framework. The first step focuses on property prediction, treated as a multi-label classification task. The second step is link prediction, framed as a generative sequence-to-sequence task. Experimental results on three datasets show that our method outperforms existing baselines. Our code is available at this https URL.', 'abstract_zh': '知识图谱完备性旨在通过添加表示事实的新三元组来填补知识库的空白。这一任务的复杂性取决于已知三元组的哪一部分。实例完备性涉及在仅给定主语（h, ?, ?）的情况下预测关系-宾语对。值得注意的是，现代知识库通常包含实体描述和类型，这些文本信息可以为推测缺失事实提供有价值的上下文。通过利用这些文本描述和大型语言模型从其中提取事实并在知识图谱模式内识别模式的能力，我们提出了一种基于大型语言模型的端到端实例完备性方法。具体来说，我们引入了GenIC：一种两步生成式实例完备性框架。第一步侧重于属性预测，被视为一个多标签分类任务。第二步是关系预测，被框定为生成式的序列到序列任务。在三个数据集上的实验结果表明，我们的方法优于现有基线。我们的代码可在以下链接获取。', 'title_zh': 'GenIC：一种基于LLM的knowledge graph实例完成框架'}
{'arxiv_id': 'arXiv:2505.23990', 'title': 'Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding', 'authors': 'Mingyang Mao, Mariela M. Perez-Cabarcas, Utteja Kallakuri, Nicholas R. Waytowich, Xiaomin Lin, Tinoosh Mohsenin', 'link': 'https://arxiv.org/abs/2505.23990', 'abstract': "To effectively engage in human society, the ability to adapt, filter information, and make informed decisions in ever-changing situations is critical. As robots and intelligent agents become more integrated into human life, there is a growing opportunity-and need-to offload the cognitive burden on humans to these systems, particularly in dynamic, information-rich scenarios.\nTo fill this critical need, we present Multi-RAG, a multimodal retrieval-augmented generation system designed to provide adaptive assistance to humans in information-intensive circumstances. Our system aims to improve situational understanding and reduce cognitive load by integrating and reasoning over multi-source information streams, including video, audio, and text. As an enabling step toward long-term human-robot partnerships, Multi-RAG explores how multimodal information understanding can serve as a foundation for adaptive robotic assistance in dynamic, human-centered situations. To evaluate its capability in a realistic human-assistance proxy task, we benchmarked Multi-RAG on the MMBench-Video dataset, a challenging multimodal video understanding benchmark. Our system achieves superior performance compared to existing open-source video large language models (Video-LLMs) and large vision-language models (LVLMs), while utilizing fewer resources and less input data. The results demonstrate Multi- RAG's potential as a practical and efficient foundation for future human-robot adaptive assistance systems in dynamic, real-world contexts.", 'abstract_zh': '为了有效参与到人类社会中，适应、筛选信息并在不断变化的情况下做出明智决策的能力至关重要。随着机器人和智能代理越来越多地融入人类生活，将认知负担转移到这些系统上的机会和需求不断增加，尤其是在动态、信息丰富的场景中。\n\n为满足这一关键需求，我们提出了Multi-RAG，这是一种多模态检索增强生成系统，旨在为信息密集型情境下的用户提供适应性辅助。该系统通过整合和推理多源信息流，包括视频、音频和文本，以提高情境理解并减轻认知负担。作为长期人机伙伴关系的使能步骤，Multi-RAG 探索了多模态信息理解如何作为动态、以人为核心情境下适应性机器人辅助的基础。为了评估其在现实的人机辅助代理任务中的能力，我们在具有挑战性的多模态视频理解基准数据集（MMBench-Video）上对Multi-RAG进行了测试。与现有的开源视频大语言模型（Video-LLMs）和大型视觉-语言模型（LVLMs）相比，我们的系统在使用更少资源和更少输入数据的情况下实现了更好的性能。结果表明，Multi-RAG 具有作为未来动态现实世界环境中人机适应性辅助系统实用且高效的潜在基础的潜力。', 'title_zh': '多模式 Retrieval-Augmented Generation 系统：适配的视频理解'}
{'arxiv_id': 'arXiv:2505.23982', 'title': 'MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge', 'authors': 'Jerry Junyang Cheung, Shiyao Shen, Yuchen Zhuang, Yinghao Li, Rampi Ramprasad, Chao Zhang', 'link': 'https://arxiv.org/abs/2505.23982', 'abstract': 'Despite recent advances in large language models (LLMs) for materials science, there is a lack of benchmarks for evaluating their domain-specific knowledge and complex reasoning abilities. To bridge this gap, we introduce MSQA, a comprehensive evaluation benchmark of 1,757 graduate-level materials science questions in two formats: detailed explanatory responses and binary True/False assessments. MSQA distinctively challenges LLMs by requiring both precise factual knowledge and multi-step reasoning across seven materials science sub-fields, such as structure-property relationships, synthesis processes, and computational modeling. Through experiments with 10 state-of-the-art LLMs, we identify significant gaps in current LLM performance. While API-based proprietary LLMs achieve up to 84.5% accuracy, open-source (OSS) LLMs peak around 60.5%, and domain-specific LLMs often underperform significantly due to overfitting and distributional shifts. MSQA represents the first benchmark to jointly evaluate the factual and reasoning capabilities of LLMs crucial for LLMs in advanced materials science.', 'abstract_zh': '尽管在材料科学领域的大型语言模型（LLM）方面取得了近期进展，但仍缺乏评估其领域特定知识和复杂推理能力的标准标尺。为了弥合这一差距，我们引入了MSQA，这是一个包含1,757个研究生水平材料科学问题的全面评估基准，问题格式包括详细的解释性回答和二元真/假评估。MSQA通过要求LLM在七个材料科学子领域的精确事实知识和多步推理来独特地挑战LLM，这些子领域包括结构-性能关系、合成过程和计算建模。通过对10个最先进的LLM的实验，我们发现当前LLM性能存在显著差距。虽然基于API的专有LLM的准确率达到84.5%，开源（OSS）LLM的准确率约为60.5%，且专门针对某一领域的LLM往往因为过度拟合和分布偏移而表现不佳。MSQA代表了第一个同时评估LLM在先进材料科学中至关重要的事实推理能力的标准标尺。', 'title_zh': 'MSQA：评估大型语言模型在材料科学研究生级推理与知识方面的表现'}
{'arxiv_id': 'arXiv:2505.23950', 'title': 'InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback', 'authors': 'Boyuan Chen, Donghai Hong, Jiaming Ji, Jiacheng Zheng, Bowen Dong, Jiayi Zhou, Kaile Wang, Juntao Dai, Xuyao Wang, Wenqi Chen, Qirui Zheng, Wenxin Li, Sirui Han, Yike Guo, Yaodong Yang', 'link': 'https://arxiv.org/abs/2505.23950', 'abstract': 'As multimodal large models (MLLMs) continue to advance across challenging tasks, a key question emerges: What essential capabilities are still missing? A critical aspect of human learning is continuous interaction with the environment -- not limited to language, but also involving multimodal understanding and generation. To move closer to human-level intelligence, models must similarly support multi-turn, multimodal interaction. In particular, they should comprehend interleaved multimodal contexts and respond coherently in ongoing exchanges. In this work, we present an initial exploration through the InterMT -- the first preference dataset for multi-turn multimodal interaction, grounded in real human feedback. In this exploration, we particularly emphasize the importance of human oversight, introducing expert annotations to guide the process, motivated by the fact that current MLLMs lack such complex interactive capabilities. InterMT captures human preferences at both global and local levels into nine sub-dimensions, consists of 15.6k prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled preference pairs. To compensate for the lack of capability for multi-modal understanding and generation, we introduce an agentic workflow that leverages tool-augmented MLLMs to construct multi-turn QA instances. To further this goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting judges with multi-turn, multimodal tasks. We demonstrate the utility of \\InterMT through applications such as judge moderation and further reveal the multi-turn scaling law of judge model. We hope the open-source of our data can help facilitate further research on aligning current MLLMs to the next step. Our project website can be found at this https URL .', 'abstract_zh': '多模态大型模型在挑战性任务中不断进步，一个关键问题随之浮现：尚缺失哪些基本能力？人类学习的一个关键方面是对环境的持续互动——不仅限于语言，还包括多模态的理解和生成。为了接近人类级别的智能，模型必须同样支持多轮次、多模态的互动。特别是，它们应该理解交错的多模态上下文并在持续的交流中作出连贯的响应。在本文中，我们通过InterMT进行了初步探索——这是第一个多轮次多模态互动偏好数据集，基于真实人的反馈构建。在这一探索中，我们特别强调了人类监督的重要性，引入了专家注释来引导这一过程，原因是当前的多模态大型模型缺乏这样的复杂互动能力。InterMT在全局和局部两个层次上捕捉人类偏好，包含九个子维度，共有15600个提示，52600个多轮对话实例，以及32400个多轮对话实例的人类标注偏好配对。为了弥补多模态理解和生成能力的缺失，我们引入了一种自主的工作流程，利用工具增强的多模态大型模型构建多轮次问答实例。为了进一步实现这个目标，我们引入了InterMT-Bench来评估多模态大型模型在协助法官完成多轮次、多模态任务方面的能力。我们展示了InterMT在裁判调解等应用中的实用性，并进一步揭示了裁判模型的多轮次扩展规律。我们希望开源数据能帮助推动进一步研究，使其更好地与下一阶段的多模态大型模型对齐。更多内容，请访问此网址：this https URL。', 'title_zh': '多轮交错偏好对齐与人类反馈'}
{'arxiv_id': 'arXiv:2505.23946', 'title': 'Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve', 'authors': 'Yuanzhe Liu, Ryan Deng, Tim Kaler, Xuhao Chen, Charles E. Leiserson, Yao Ma, Jie Chen', 'link': 'https://arxiv.org/abs/2505.23946', 'abstract': "Recent studies show that LLMs possess different skills and specialize in different tasks. In fact, we observe that their varied performance occur in several levels of granularity. For example, in the code optimization task, code LLMs excel at different optimization categories and no one dominates others. This observation prompts the question of how one leverages multiple LLM agents to solve a coding problem without knowing their complementary strengths a priori. We argue that a team of agents can learn from each other's successes and failures so as to improve their own performance. Thus, a lesson is the knowledge produced by an agent and passed on to other agents in the collective solution process. We propose a lesson-based collaboration framework, design the lesson solicitation--banking--selection mechanism, and demonstrate that a team of small LLMs with lessons learned can outperform a much larger LLM and other multi-LLM collaboration methods.", 'abstract_zh': '最近的研究表明，大型语言模型具有不同的技能并擅长不同的任务。实际上，我们观察到它们在不同粒度层次上表现出不同的性能。例如，在代码优化任务中，代码LLM在不同的优化类别中表现出色，没有一个模型能够全面超越其他模型。这一观察引发了如何在事先不知道各模型互补优势的情况下，利用多个LLM代理解决编程问题的问题。我们认为，代理团队可以从彼此的成功和失败中学习，以提高自己的性能。因此，从一个代理生成的知识并传递给团队中的其他代理是这一过程中的关键。我们提出了一种基于教训的合作框架，设计了教训的征集—存储—选择机制，并证明了一个学习了教训的较小规模LLM团队能够优于一个更大规模的LLM和其它LLM合作方法。', 'title_zh': '从多agent框架学习与改进代码LLM的经验教训'}
{'arxiv_id': 'arXiv:2505.23885', 'title': 'OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation', 'authors': 'Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, Guohao Li', 'link': 'https://arxiv.org/abs/2505.23885', 'abstract': "Large Language Model (LLM)-based multi-agent systems show promise for automating real-world tasks but struggle to transfer across domains due to their domain-specific nature. Current approaches face two critical shortcomings: they require complete architectural redesign and full retraining of all components when applied to new domains. We introduce Workforce, a hierarchical multi-agent framework that decouples strategic planning from specialized execution through a modular architecture comprising: (i) a domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask management, and (iii) specialized Workers with domain-specific tool-calling capabilities. This decoupling enables cross-domain transferability during both inference and training phases: During inference, Workforce seamlessly adapts to new domains by adding or modifying worker agents; For training, we introduce Optimized Workforce Learning (OWL), which improves generalization across domains by optimizing a domain-agnostic planner with reinforcement learning from real-world feedback. To validate our approach, we evaluate Workforce on the GAIA benchmark, covering various realistic, multi-domain agentic tasks. Experimental results demonstrate Workforce achieves open-source state-of-the-art performance (69.70%), outperforming commercial systems like OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to GPT-4o on challenging tasks. To summarize, by enabling scalable generalization and modular domain transfer, our work establishes a foundation for the next generation of general-purpose AI assistants.", 'abstract_zh': '基于大型语言模型（LLM）的多Agent系统在自动化现实世界任务方面展现出潜力，但由于其领域特定性，难以跨领域转移。当前方法面临两大关键局限：应用于新领域时需要完全重新设计架构并重新训练所有组件。我们提出了Workforce，一种通过模块化架构解耦战略规划和专门执行的层次多Agent框架，该架构包括：（i）领域无关的规划者进行任务分解，（ii）协调器管理子任务，（iii）具有领域特定工具调用能力的专业工作者。这种解耦在推理和训练阶段都实现了跨领域的可转移性：在推理阶段，Workforce通过添加或修改工作者代理无缝适应新领域；在训练阶段，我们引入了优化Workforce学习（OWL），通过对来自真实世界反馈的强化学习优化领域无关的规划者，从而改善跨领域的泛化能力。为了验证我们的方法，我们在GAIA基准上评估了Workforce，涵盖各种现实的、多领域的代理任务。实验结果表明，Workforce达到了开源领域最佳性能（69.70%），比开源系统OpenAI的Deep Research更好（提高2.34%）。更值得一提的是，我们的OWL训练的32B模型准确率达到52.73%（提高16.37%），在挑战性任务上展示了与GPT-4o相当的表现。总之，通过实现可扩展的泛化能力和模块化的领域转移，我们的工作为下一代通用AI助手奠定了基础。', 'title_zh': 'OWL：优化 workforce 学习以实现现实世界多代理任务自动化的一般性协助'}
{'arxiv_id': 'arXiv:2505.23881', 'title': 'Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems', 'authors': 'Christopher D. Rosin', 'link': 'https://arxiv.org/abs/2505.23881', 'abstract': 'Large Language Models (LLMs) with reasoning are trained to iteratively generate and refine their answers before finalizing them, which can help with applications to mathematics and code generation. We apply code generation with reasoning LLMs to a specific task in the mathematical field of combinatorial design. This field studies diverse types of combinatorial designs, many of which have lists of open instances for which existence has not yet been determined. The Constructive Protocol CPro1 uses LLMs to generate search heuristics that have the potential to construct solutions to small open instances. Starting with a textual definition and a validity verifier for a particular type of design, CPro1 guides LLMs to select and implement strategies, while providing automated hyperparameter tuning and execution feedback. CPro1 with reasoning LLMs successfully solves long-standing open instances for 7 of 16 combinatorial design problems selected from the 2006 Handbook of Combinatorial Designs, including new solved instances for 3 of these (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary Designs) that were unsolved by CPro1 with non-reasoning LLMs. It also solves open instances for several problems from recent (2025) literature, generating new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform Nested Steiner Quadruple System.', 'abstract_zh': '具有推理能力的大语言模型（LLMs）通过迭代生成和精炼答案来训练，这有助于数学和代码生成的应用。我们将具有推理能力的LLMs应用于组合设计领域的一个特定任务。该领域研究多种类型的组合设计，其中许多设计具有开放实例，其存在性尚未确定。Constructive Protocol CPro1使用LLMs生成搜索启发式，这些启发式有可能构建小型开放实例的解。从特定类型的组合设计的文本定义和有效性验证器开始，CPro1引导LLMs选择和实施策略，并提供自动化超参数调优和执行反馈。使用具有推理能力的LLMs的CPro1成功解决了16个2006年《组合设计手册》中选择的组合设计问题中的7个长期未决的开放实例，其中包括3个新解决的实例（Bhaskar Rao Designs、对称称重矩阵、平衡三元组合设计），这些实例是CPro1使用非推理LLMs未解决的新实例。此外，CPro1还解决了最近（2025年）文献中几个问题的开放实例，生成了新的覆盖序列、Johnson clique覆盖、删除码和均匀嵌套斯坦纳四元系。', 'title_zh': '使用推理模型生成解决组合设计问题开放实例的搜索启发式方法'}
{'arxiv_id': 'arXiv:2505.24872', 'title': 'ProxyThinker: Test-Time Guidance through Small Visual Reasoners', 'authors': 'Zilin Xiao, Jaywon Koo, Siru Ouyang, Jefferson Hernandez, Yu Meng, Vicente Ordonez', 'link': 'https://arxiv.org/abs/2505.24872', 'abstract': 'Recent advancements in reinforcement learning with verifiable rewards have pushed the boundaries of the visual reasoning capabilities in large vision-language models (LVLMs). However, training LVLMs with reinforcement fine-tuning (RFT) is computationally expensive, posing a significant challenge to scaling model size. In this work, we propose ProxyThinker, an inference-time technique that enables large models to inherit the visual reasoning capabilities from small, slow-thinking visual reasoners without any training. By subtracting the output distributions of base models from those of RFT reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors such as self-verification and self-correction. ProxyThinker consistently boosts performance on challenging visual benchmarks on spatial, mathematical, and multi-disciplinary reasoning, enabling untuned base models to compete with the performance of their full-scale RFT counterparts. Furthermore, our implementation efficiently coordinates multiple language models with parallelism techniques and achieves up to 38 $\\times$ faster inference compared to previous decoding-time methods, paving the way for the practical deployment of ProxyThinker. Code is available at this https URL.', 'abstract_zh': '近期可验证奖励强化学习的进展推动了大规模视觉语言模型的视觉推理能力边界。然而，使用强化微调（RFT）训练LVLMs在计算上非常昂贵，给模型规模的扩展带来了显著挑战。在本文中，我们提出了ProxyThinker，一种推理时技术，使大模型能够继承小型慢思考视觉推理器的视觉推理能力，而无需任何训练。通过从RFT推理器的输出分布中减去基础模型的输出分布，ProxyThinker修改了解码动力学，并成功激发了由复杂行为（如自我验证和自我修正）表现出的慢思考推理。ProxyThinker在空间、数学和跨学科推理等具有挑战性的视觉基准测试中一致地提升了性能，使未微调的基础模型能够与全规模RFT对应物的性能相竞争。此外，我们的实现利用并行技术高效协调多个语言模型，并达到了比之前解码时方法快38倍的推理速度，为ProxyThinker的实际部署铺平了道路。代码可通过以下链接获得：this https URL。', 'title_zh': 'ProxyThinker：通过小型视觉推理器实现测试时指导'}
{'arxiv_id': 'arXiv:2505.24867', 'title': "Time Blindness: Why Video-Language Models Can't See What Humans Can?", 'authors': 'Ujjwal Upadhyay, Mukul Ranjan, Zhiqiang Shen, Mohamed Elhoseiny', 'link': 'https://arxiv.org/abs/2505.24867', 'abstract': 'Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce $\\textbf{SpookyBench}$, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: this https URL.', 'abstract_zh': 'Recent Advances in Vision-Language Models: Bridging the Gap between Human and Machine Video Understanding with SpookyBench', 'title_zh': '时间盲视：为什么视频语言模型看不到人类能看到的内容？'}
{'arxiv_id': 'arXiv:2505.24864', 'title': 'ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models', 'authors': 'Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, Yi Dong', 'link': 'https://arxiv.org/abs/2505.24864', 'abstract': "Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: this https URL", 'abstract_zh': '最近在注重推理的语言模型方面的进展表明，强化学习（RL）是使模型与可验证奖励对齐的一种有前途的方法。然而，仍然有争议的是，RL是否真正扩展了模型的推理能力，还是仅仅放大了基础模型分布中已经存在的高奖励输出，以及持续扩大RL计算是否可靠地提高了推理性能。在本工作中，我们通过证明长时间的RL（ProRL）训练可以在广泛的采样下揭示出基础模型无法访问的新推理策略来挑战现有的假设。我们引入了ProRL，一种新的训练方法，其中包括KL散度控制、参考策略重置以及一系列多样的任务。我们的实证分析表明，经过RL训练的模型在广泛的pass@k评估中始终优于基础模型，包括基础模型无论如何尝试都无法完成的情形。我们进一步表明，推理边界改进与基础模型的任务能力和训练时间密切相关，这表明RL可以在时间上探索并填充新的解空间。这些发现为理解何时以及在多大程度上RL能够真正扩展语言模型的推理边界提供了新的见解，并为长期RL在推理方面的研究奠定了基础。我们发布了模型权重以支持进一步的研究：[这个链接](这个链接)。', 'title_zh': '延长强化学习：ProRL 扩展大型语言模型的推理边界'}
{'arxiv_id': 'arXiv:2505.24853', 'title': 'DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation', 'authors': 'Zhao Mandi, Yifan Hou, Dieter Fox, Yashraj Narang, Ajay Mandlekar, Shuran Song', 'link': 'https://arxiv.org/abs/2505.24853', 'abstract': 'We study the problem of functional retargeting: learning dexterous manipulation policies to track object states from human hand-object demonstrations. We focus on long-horizon, bimanual tasks with articulated objects, which is challenging due to large action space, spatiotemporal discontinuities, and embodiment gap between human and robot hands. We propose DexMachina, a novel curriculum-based algorithm: the key idea is to use virtual object controllers with decaying strength: an object is first driven automatically towards its target states, such that the policy can gradually learn to take over under motion and contact guidance. We release a simulation benchmark with a diverse set of tasks and dexterous hands, and show that DexMachina significantly outperforms baseline methods. Our algorithm and benchmark enable a functional comparison for hardware designs, and we present key findings informed by quantitative and qualitative results. With the recent surge in dexterous hand development, we hope this work will provide a useful platform for identifying desirable hardware capabilities and lower the barrier for contributing to future research. Videos and more at this https URL', 'abstract_zh': '功能重定向问题研究：学习跟踪物体状态的灵巧操作策略，基于人类手-物体演示。我们关注长期任务和灵巧物体的双臂操作，由于动作空间庞大、时空连续性中断以及人手与机器人手的实体差距，这一任务具有挑战性。我们提出DexMachina，一种新颖的基于课程的学习算法：关键思想是使用衰减强度的虚拟物体控制器：首先自动驱动物体向目标状态移动，从而使策略能够在运动和接触引导下逐渐学习接管。我们发布了一个包含多种任务和灵巧手的模拟基准，并展示了DexMachina显著优于基线方法。我们的算法和基准为硬件设计的功能比较提供了可能，我们基于定量和定性结果介绍了关键发现。随着灵巧手开发的兴起，我们希望这项工作能为识别 desirable 硬件能力提供一个有用的平台，并降低未来研究的参与门槛。更多信息请访问此链接。', 'title_zh': 'DexMachina：双臂灵巧操作的功能重定标'}
{'arxiv_id': 'arXiv:2505.24850', 'title': 'Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning', 'authors': 'Shuyao Xu, Cheng Peng, Jiangxuan Long, Weidi Xu, Wei Chu, Yuan Qi', 'link': 'https://arxiv.org/abs/2505.24850', 'abstract': "Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data.", 'abstract_zh': '近期模型蒸馏进展表明，来自高级推理模型（如DeepSeek-R1、OpenAI的o1）的数据可以有效转移复杂的推理能力给更小且高效的学徒模型。然而，标准做法通常采用拒绝采样，抛弃错误的推理示例——这些有价值的但往往被低估的数据。本文探讨了一个关键问题：如何有效利用正向和负向蒸馏推理轨迹，在离线环境中最大化LLM的推理性能？为此，我们提出了强化蒸馏（REDI）两阶段框架。第一阶段通过监督微调（SFT）学习正向轨迹。第二阶段通过我们提出的REDI目标，进一步利用正向和负向轨迹优化模型。这一新颖的目标是一个简单的、无需参考的损失函数，在此蒸馏背景下优于现有的方法如DPO和SimPO。我们的实证评估表明，REDI在数学推理任务上优于基线拒绝采样SFT或结合使用DPO/SimPO的方法。值得注意的是，仅在131,000个正向和负向开放Open-R1数据示例上进行后训练的Qwen-REDI-1.5B模型，在MATH-500（pass@1）上实现了83.1%的得分。其性能与DeepSeek-R1-Distill-Qwen-1.5B（一个在800,000个专有数据上进行后训练的模型）在各种数学推理基准测试中相当或更好，为1.5B模型在离线环境中使用公开可用数据进行后训练设定了新标准。', 'title_zh': '利用负信号：从教师数据中进行强化蒸馏以提升LLM推理能力'}
{'arxiv_id': 'arXiv:2505.24840', 'title': 'Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck', 'authors': 'Yuwen Tan, Yuan Qing, Boqing Gong', 'link': 'https://arxiv.org/abs/2505.24840', 'abstract': "This paper reveals that many state-of-the-art large language models (LLMs) lack hierarchical knowledge about our visual world, unaware of even well-established biology taxonomies. This shortcoming makes LLMs a bottleneck for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone Fish but not Vertebrate). We arrive at these findings using about one million four-choice visual question answering (VQA) tasks constructed from six taxonomies and four image datasets. Interestingly, finetuning a vision LLM using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because the VQA tasks improve the LLM's hierarchical consistency more than the vision LLM's. We conjecture that one cannot make vision LLMs understand visual concepts fully hierarchical until LLMs possess corresponding taxonomy knowledge.", 'abstract_zh': '本文揭示了许多最先进的大型语言模型（LLMs）缺乏对视觉世界的层次化知识，甚至不知晓已建立的生物学分类法。这一缺陷使得LLMs成为视知觉LLMs层次化视觉理解的瓶颈（例如，能够识别海葵鱼但无法识别脊椎动物）。我们通过构建来自六个分类法和四个图像数据集的一百多万个四选一的视觉问答（VQA）任务得出了这些发现。有趣的是，使用我们的VQA任务微调视知觉LLM在一定程度上证实了LLMs的瓶颈效应，因为VQA任务比视知觉LLM本身更有效地提高了LLM的层次化一致性。我们推测，在LLMs获得相应的分类法知识之前，无法使视知觉LLM完全理解视觉概念的层次化结构。', 'title_zh': 'Vision LLMs在层级视觉理解方面表现不佳，且LLMs是瓶颈。'}
{'arxiv_id': 'arXiv:2505.24838', 'title': 'VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software', 'authors': 'Brandon Man, Ghadi Nehme, Md Ferdous Alam, Faez Ahmed', 'link': 'https://arxiv.org/abs/2505.24838', 'abstract': "Computer-Aided Design (CAD) is a time-consuming and complex process, requiring precise, long-horizon user interactions with intricate 3D interfaces. While recent advances in AI-driven user interface (UI) agents show promise, most existing datasets and methods focus on short, low-complexity tasks in mobile or web applications, failing to capture the demands of professional engineering tools. In this work, we introduce VideoCAD, the first attempt at engineering UI interaction learning for precision tasks. Specifically, VideoCAD is a large-scale synthetic dataset consisting of over 41K annotated video recordings of CAD operations, generated using an automated framework for collecting high-fidelity UI action data from human-made CAD designs. Compared to existing datasets, VideoCAD offers an order of magnitude higher complexity in UI interaction learning for real-world engineering tasks, having up to a 20x longer time horizon than other datasets. We show two important downstream applications of VideoCAD: learning UI interactions from professional precision 3D CAD tools and a visual question-answering (VQA) benchmark designed to evaluate multimodal large language models' (LLM) spatial reasoning and video understanding abilities. To learn the UI interactions, we propose VideoCADFormer - a state-of-the-art model in learning CAD interactions directly from video, which outperforms multiple behavior cloning baselines. Both VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key challenges in the current state of video-based UI understanding, including the need for precise action grounding, multi-modal and spatial reasoning, and long-horizon dependencies.", 'abstract_zh': '计算机辅助设计（CAD）的交互学习：VideoCAD的研究', 'title_zh': 'VideoCAD：一种用于学习UI交互和3D推理的大型视频数据集，源于CAD软件'}
{'arxiv_id': 'arXiv:2505.24830', 'title': 'Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs', 'authors': 'Juraj Vladika, Annika Domres, Mai Nguyen, Rebecca Moser, Jana Nano, Felix Busch, Lisa C. Adams, Keno K. Bressem, Denise Bernhardt, Stephanie E. Combs, Kai J. Borm, Florian Matthes, Jan C. Peeken', 'link': 'https://arxiv.org/abs/2505.24830', 'abstract': 'Large language models (LLMs) exhibit extensive medical knowledge but are prone to hallucinations and inaccurate citations, which pose a challenge to their clinical adoption and regulatory compliance. Current methods, such as Retrieval Augmented Generation, partially address these issues by grounding answers in source documents, but hallucinations and low fact-level explainability persist. In this work, we introduce a novel atomic fact-checking framework designed to enhance the reliability and explainability of LLMs used in medical long-form question answering. This method decomposes LLM-generated responses into discrete, verifiable units called atomic facts, each of which is independently verified against an authoritative knowledge base of medical guidelines. This approach enables targeted correction of errors and direct tracing to source literature, thereby improving the factual accuracy and explainability of medical Q&A. Extensive evaluation using multi-reader assessments by medical experts and an automated open Q&A benchmark demonstrated significant improvements in factual accuracy and explainability. Our framework achieved up to a 40% overall answer improvement and a 50% hallucination detection rate. The ability to trace each atomic fact back to the most relevant chunks from the database provides a granular, transparent explanation of the generated responses, addressing a major gap in current medical AI applications. This work represents a crucial step towards more trustworthy and reliable clinical applications of LLMs, addressing key prerequisites for clinical application and fostering greater confidence in AI-assisted healthcare.', 'abstract_zh': '大型语言模型在医学问答中的原子事实核查框架：提高可靠性和解释性', 'title_zh': '通过原子事实核查提高检索增强大语言模型在医疗问答中的可靠性和可解释性'}
{'arxiv_id': 'arXiv:2505.24823', 'title': 'PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models', 'authors': 'Yinggan Xu, Yue Liu, Zhiqiang Gao, Changnan Peng, Di Luo', 'link': 'https://arxiv.org/abs/2505.24823', 'abstract': 'Large language models (LLMs) have rapidly advanced and are increasingly capable of tackling complex scientific problems, including those in physics. Despite this progress, current LLMs often fail to emulate the concise, principle-based reasoning characteristic of human experts, instead generating lengthy and opaque solutions. This discrepancy highlights a crucial gap in their ability to apply core physical principles for efficient and interpretable problem solving. To systematically investigate this limitation, we introduce PhySense, a novel principle-based physics reasoning benchmark designed to be easily solvable by experts using guiding principles, yet deceptively difficult for LLMs without principle-first reasoning. Our evaluation across multiple state-of-the-art LLMs and prompt types reveals a consistent failure to align with expert-like reasoning paths, providing insights for developing AI systems with efficient, robust and interpretable principle-based scientific reasoning.', 'abstract_zh': '大型语言模型（LLMs）在物理学等复杂科学问题上的推理能力有了 rapid advancement，并逐渐能够应对这些问题。尽管取得了这些进展，当前的 LLMs 往往不能模仿人类专家简洁且基于原理的推理方式，反而生成了冗长且难以理解的解决方案。这种差异凸显了他们在运用核心物理原理进行高效和可解释的问题解决方面的关键缺陷。为了系统地研究这一限制，我们引入了 PhySense，这是一种新颖的基于原理的物理推理基准，该基准能够被专家使用指导原则轻松解决，但对于没有基于原理推理的 LLMs 却显得具有欺骗性难度。我们在多个最先进的 LLMs 和不同的提示类型上的评估揭示了它们一致地无法与专家级推理路径对齐，为开发具有高效、稳健和可解释的基于原理的科学推理的人工智能系统提供了见解。', 'title_zh': 'PhySense: 原理导向的物理推理基准测试 для大型语言模型'}
{'arxiv_id': 'arXiv:2505.24808', 'title': 'RealDrive: Retrieval-Augmented Driving with Diffusion Models', 'authors': 'Wenhao Ding, Sushant Veer, Yuxiao Chen, Yulong Cao, Chaowei Xiao, Marco Pavone', 'link': 'https://arxiv.org/abs/2505.24808', 'abstract': 'Learning-based planners generate natural human-like driving behaviors by learning to reason about nuanced interactions from data, overcoming the rigid behaviors that arise from rule-based planners. Nonetheless, data-driven approaches often struggle with rare, safety-critical scenarios and offer limited controllability over the generated trajectories. To address these challenges, we propose RealDrive, a Retrieval-Augmented Generation (RAG) framework that initializes a diffusion-based planning policy by retrieving the most relevant expert demonstrations from the training dataset. By interpolating between current observations and retrieved examples through a denoising process, our approach enables fine-grained control and safe behavior across diverse scenarios, leveraging the strong prior provided by the retrieved scenario. Another key insight we produce is that a task-relevant retrieval model trained with planning-based objectives results in superior planning performance in our framework compared to a task-agnostic retriever. Experimental results demonstrate improved generalization to long-tail events and enhanced trajectory diversity compared to standard learning-based planners -- we observe a 40% reduction in collision rate on the Waymo Open Motion dataset with RAG.', 'abstract_zh': '基于学习的规划器通过从数据中学习细微交互来生成自然的人类驾驶行为，克服了基于规则的规划器产生的刚性行为。然而，数据驱动的方法往往难以应对罕见的安全关键场景，并且在生成的轨迹控制上提供有限的可控性。为了解决这些挑战，我们提出了一种检索增强生成（RAG）框架——RealDrive，该框架通过从训练数据集中检索最具相关性的专家演示来初始化基于扩散的过程规划策略。通过在当前观测值和检索到的示例之间通过去噪过程进行插值，我们的方法能够在多样化的场景中实现精细的控制和安全行为，利用检索到的场景提供的强大先验知识。另一个关键洞见是我们发现，使用规划任务相关的检索模型训练，在我们的框架中比使用任务无关的检索器，能够获得更优的规划性能。实验结果表明，与标准的学习基于规划器相比，我们的RAG方法在实例泛化和轨迹多样性方面表现更好——我们在Waymo Open Motion数据集上观察到碰撞率降低了40%。', 'title_zh': '实驱：基于扩散模型的检索增强驾驶'}
{'arxiv_id': 'arXiv:2505.24791', 'title': 'Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding', 'authors': 'Jiaru Zhang, Juanwu Lu, Ziran Wang, Ruqi Zhang', 'link': 'https://arxiv.org/abs/2505.24791', 'abstract': "Normalizing flows are promising generative models with advantages such as theoretical rigor, analytical log-likelihood computation, and end-to-end training. However, the architectural constraints to ensure invertibility and tractable Jacobian computation limit their expressive power and practical usability. Recent advancements utilize autoregressive modeling, significantly enhancing expressive power and generation quality. However, such sequential modeling inherently restricts parallel computation during inference, leading to slow generation that impedes practical deployment. In this paper, we first identify that strict sequential dependency in inference is unnecessary to generate high-quality samples. We observe that patches in sequential modeling can also be approximated without strictly conditioning on all preceding patches. Moreover, the models tend to exhibit low dependency redundancy in the initial layer and higher redundancy in subsequent layers. Leveraging these observations, we propose a selective Jacobi decoding (SeJD) strategy that accelerates autoregressive inference through parallel iterative optimization. Theoretical analyses demonstrate the method's superlinear convergence rate and guarantee that the number of iterations required is no greater than the original sequential approach. Empirical evaluations across multiple datasets validate the generality and effectiveness of our acceleration technique. Experiments demonstrate substantial speed improvements up to 4.7 times faster inference while keeping the generation quality and fidelity.", 'abstract_zh': '严格顺序依赖性在推断中不是生成高质量样本所必需的：一种基于选择性雅可比解码的并行加速自回归推断方法', 'title_zh': '自回归规范流的选择性雅基解码加速推理'}
{'arxiv_id': 'arXiv:2505.24788', 'title': 'Drop Dropout on Single-Epoch Language Model Pretraining', 'authors': 'Houjun Liu, John Bauer, Christopher D. Manning', 'link': 'https://arxiv.org/abs/2505.24788', 'abstract': 'Originally, dropout was seen as a breakthrough regularization technique that reduced overfitting and improved performance in almost all applications of deep learning by reducing overfitting. Yet, single-epoch pretraining tasks common to modern LLMs yield minimal overfitting, leading to dropout not being used for large LLMs. Nevertheless, no thorough empirical investigation has been done on the role of dropout in LM pretraining. Through experiments in single-epoch pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs with varying levels of dropout, we find that downstream performance in language modeling, morpho-syntax (BLiMP), question answering (SQuAD), and natural-language inference (MNLI) improves when dropout is not applied during pretraining. We additionally find that the recently-introduced "early dropout" also degrades performance over applying no dropout at all. We further investigate the models\' editability, and find that models trained without dropout are more successful in gradient-based model editing (MEND) and equivalent in representation-based model editing (ReFT). Therefore, we advocate to drop dropout during single-epoch pretraining.', 'abstract_zh': '起初，dropout被视为一种减少过拟合并几乎在所有深度学习应用中提升性能的突破性正则化技术。然而，现代大语言模型中常见的单轮预训练任务导致过拟合极少发生，使得dropout不再用于大型语言模型。尽管如此，至今尚未进行过彻底的实证研究以探索dropout在语言模型预训练中的作用。通过在不同dropout水平下对掩码（BERT）和自回归（Pythia 160M和1.4B）语言模型进行单轮预训练的实验，我们发现，在预训练过程中不使用dropout，可以提升下游语言模型、形态语法（BLiMP）、问答（SQuAD）和自然语言推理（MNLI）等任务的表现。此外，我们还发现最近引入的“早期dropout”会进一步降低性能，不如完全不使用dropout。进一步研究发现，不使用dropout训练的模型在基于梯度的模型编辑（MEND）中更具可编辑性，在基于表示的模型编辑（ReFT）中表现相当。因此，我们建议在单轮预训练过程中舍弃dropout。', 'title_zh': '单epoch语言模型预训练中去除 Dropout'}
{'arxiv_id': 'arXiv:2505.24786', 'title': 'DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics', 'authors': 'Eran Bamani Beeri, Eden Nissinman, Avishai Sintov', 'link': 'https://arxiv.org/abs/2505.24786', 'abstract': 'Dynamic hand gestures play a pivotal role in assistive human-robot interaction (HRI), facilitating intuitive, non-verbal communication, particularly for individuals with mobility constraints or those operating robots remotely. Current gesture recognition methods are mostly limited to short-range interactions, reducing their utility in scenarios demanding robust assistive communication from afar. In this paper, we introduce a novel approach designed specifically for assistive robotics, enabling dynamic gesture recognition at extended distances of up to 30 meters, thereby significantly improving accessibility and quality of life. Our proposed Distance-aware Gesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable Alignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust processing and classification of gesture sequences captured under challenging conditions, including significant physical attenuation, reduced resolution, and dynamic gesture variations commonly experienced in real-world assistive environments. We further introduce the Radiometric Spatio-Temporal Depth Attenuation Loss (RSTDAL), shown to enhance learning and strengthen model robustness across varying distances. Our model demonstrates significant performance improvement over state-of-the-art gesture recognition frameworks, achieving a recognition accuracy of 97.3% on a diverse dataset with challenging hyper-range gestures. By effectively interpreting gestures from considerable distances, DiG-Net significantly enhances the usability of assistive robots in home healthcare, industrial safety, and remote assistance scenarios, enabling seamless and intuitive interactions for users regardless of physical limitations', 'abstract_zh': '动态手势在辅助人机交互（HRI）中扮演关键角色，促进直观的非言语通信，特别适用于行动受限的个体或远程操作机器人的人。当前的手势识别方法 mostly 限制在短距离交互，这在需要远程稳健辅助通信的场景中降低了实用性。本文提出了一种针对辅助机器人设计的新方法，能够在30米以上的距离实现动态手势识别，从而显著提高无障碍性和生活质量。我们提出的距离感知手势网络（DiG-Net）有效结合了深度条件变形对齐（DADA）模块和时空图模块，能够在包括显著物理衰减、降低分辨率和常见的动态手势变异等挑战条件下，实现稳健的手势序列处理和分类。此外，我们引入了辐射度时空深度衰减损失（RSTDAL），证明了其能够增强学习并提高模型在不同距离下的鲁棒性。实验结果表明，DiG-Net 在一个包含具有挑战性的超远程手势的多样数据集上的识别准确率达到97.3%，显著优于当前最先进的手势识别框架。通过有效解读远距离手势，DiG-Net 显著提升了在家用护理、工业安全和远程协助场景中辅助机器人的可用性，使用户能够实现无障碍的无缝和直观交互。', 'title_zh': 'DiG-Net: 通过超远距离动态手势识别提升辅助机器人服务质量'}
{'arxiv_id': 'arXiv:2505.24767', 'title': 'A survey of using EHR as real-world evidence for discovering and validating new drug indications', 'authors': 'Nabasmita Talukdar, Xiaodan Zhang, Shreya Paithankar, Hui Wang, Bin Chen', 'link': 'https://arxiv.org/abs/2505.24767', 'abstract': 'Electronic Health Records (EHRs) have been increasingly used as real-world evidence (RWE) to support the discovery and validation of new drug indications. This paper surveys current approaches to EHR-based drug repurposing, covering data sources, processing methodologies, and representation techniques. It discusses study designs and statistical frameworks for evaluating drug efficacy. Key challenges in validation are discussed, with emphasis on the role of large language models (LLMs) and target trial emulation. By synthesizing recent developments and methodological advances, this work provides a foundational resource for researchers aiming to translate real-world data into actionable drug-repurposing evidence.', 'abstract_zh': '电子健康记录（EHRs）在支持新药适应症发现和验证中的应用日益增加：基于EHR的药物再利用现状综述及其挑战', 'title_zh': 'EHR在发现和验证新药适应证中的实际世界证据综述'}
{'arxiv_id': 'arXiv:2505.24765', 'title': 'Supervised Quantum Machine Learning: A Future Outlook from Qubits to Enterprise Applications', 'authors': 'Srikanth Thudumu, Jason Fisher, Hung Du', 'link': 'https://arxiv.org/abs/2505.24765', 'abstract': 'Supervised Quantum Machine Learning (QML) represents an intersection of quantum computing and classical machine learning, aiming to use quantum resources to support model training and inference. This paper reviews recent developments in supervised QML, focusing on methods such as variational quantum circuits, quantum neural networks, and quantum kernel methods, along with hybrid quantum-classical workflows. We examine recent experimental studies that show partial indications of quantum advantage and describe current limitations including noise, barren plateaus, scalability issues, and the lack of formal proofs of performance improvement over classical methods. The main contribution is a ten-year outlook (2025-2035) that outlines possible developments in supervised QML, including a roadmap describing conditions under which QML may be used in applied research and enterprise systems over the next decade.', 'abstract_zh': '监督量子机器学习（QML）代表了量子计算与经典机器学习的交叉领域，旨在利用量子资源支持模型训练和推理。本文回顾了监督QML的 Recent Developments，重点关注变量子电路、量子神经网络和量子核方法等方法，以及混合量子-经典工作流。我们考察了最近的实验研究，这些研究展示了部分量子优势的迹象，并描述了当前的限制，包括噪声、荒原 plateau 问题、可扩展性问题以及对性能改进缺乏形式证明的问题。主要贡献是提出了 2025-2035 年的十年展望，概述了监督QML在未来十年可能的发展，包括可用于应用研究和企业系统的路线图条件。', 'title_zh': '监督量子机器学习：从量子位到企业应用的未来展望'}
{'arxiv_id': 'arXiv:2505.24760', 'title': 'REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards', 'authors': 'Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, Andreas Köpf', 'link': 'https://arxiv.org/abs/2505.24760', 'abstract': 'We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.', 'abstract_zh': '我们介绍了Reasoning Gym (RG)：一种增强学习的可验证奖励推理环境库，涵盖代数、算术、计算、认知、几何、图论、逻辑以及多种常见游戏等多个领域，具备生成可调节复杂度的几乎无限训练数据的能力，实验结果证明了RG在推理模型评估和训练中的有效性。', 'title_zh': '推理健身房：具有可验证奖励的强化学习推理环境'}
{'arxiv_id': 'arXiv:2505.24759', 'title': 'Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport', 'authors': 'Mu Qiao', 'link': 'https://arxiv.org/abs/2505.24759', 'abstract': 'Identifying evolutionary correspondences between cell types across species is a fundamental challenge in comparative genomics and evolutionary biology. Existing approaches often rely on either reference-based matching, which imposes asymmetry by designating one species as the reference, or projection-based matching, which may increase computational complexity and obscure biological interpretability at the cell-type level. Here, we present OT-MESH, an unsupervised computational framework leveraging entropy-regularized optimal transport (OT) to systematically determine cross-species cell type homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn (MESH) technique to refine the OT plan. It begins by selecting genes with high Signal-to-Noise Ratio (SNR) to capture the most informative features, from which a cost matrix is constructed using cosine distances between cell-type centroids. Importantly, the MESH procedure iteratively refines the cost matrix, leading to a transport plan with significantly enhanced sparsity and interpretability of the resulting correspondence matrices. Applied to retinal bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque, OT-MESH accurately recovers known evolutionary relationships and uncovers novel correspondences, one of which was independently validated experimentally. Thus, our framework offers a principled, scalable, symmetric, and interpretable solution for evolutionary cell type mapping, facilitating deeper insights into cellular specialization and conservation across species.', 'abstract_zh': '基于最优传输的熵正则化方法在物种间细胞类型同源性识别中的应用：OT-MESH框架', 'title_zh': '无监督进化细胞类型匹配：基于熵最小化最优传输的方法'}
{'arxiv_id': 'arXiv:2505.24754', 'title': "Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation", 'authors': 'Yingchaojie Feng, Yiqun Sun, Yandong Sun, Minfeng Zhu, Qiang Huang, Anthony K. H. Tung, Wei Chen', 'link': 'https://arxiv.org/abs/2505.24754', 'abstract': 'In this work, we investigate an important task named instruction-following text embedding, which generates dynamic text embeddings that adapt to user instructions, highlighting specific attributes of text. Despite recent advancements, existing approaches suffer from significant computational overhead, as they require re-encoding the entire corpus for each new instruction. To address this challenge, we propose GSTransform, a novel instruction-following text embedding framework based on Guided Space Transformation. Our key observation is that instruction-relevant information is inherently encoded in generic embeddings but remains underutilized. Instead of repeatedly encoding the corpus for each instruction, GSTransform is a lightweight transformation mechanism that adapts pre-computed embeddings in real time to align with user instructions, guided by a small amount of text data with instruction-focused label annotation. We conduct extensive experiments on three instruction-awareness downstream tasks across nine real-world datasets, demonstrating that GSTransform improves instruction-following text embedding quality over state-of-the-art methods while achieving dramatic speedups of 6~300x in real-time processing on large-scale datasets. The source code is available at this https URL.', 'abstract_zh': '在本工作中，我们调查了一个名为指令跟随文本嵌入的重要任务，该任务生成能够适应用户指令的动态文本嵌入，突出文本的特定属性。尽管近期取得了一定进展，现有的方法仍存在显著的计算开销问题，因为它们需要为每个新的指令重新编码整个语料库。为了解决这一挑战，我们提出了一种基于引导空间转化的新型指令跟随文本嵌入框架GSTransform。我们的关键观察是，与指令相关的信息已经在通用嵌入中隐式编码，但仍未得到充分利用。与为每个指令反复重新编码语料库不同，GSTransform 是一种轻量级的转化机制，能够实时调整预计算的嵌入，以与用户指令对齐，该过程仅需少量带有指令集中注释的文本数据的引导。我们在三个指令感知下游任务上的九个实际数据集上进行了广泛的实验，证明GSTransform 在保持嵌入质量的同时，能够在大规模数据集上实现6~300倍的实时处理速度提升。源代码可在以下链接获取。', 'title_zh': '不要重造轮子：基于引导空间转换的高效指令遵循文本嵌入'}
{'arxiv_id': 'arXiv:2505.24722', 'title': 'HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts', 'authors': 'Neil He, Rishabh Anand, Hiren Madhu, Ali Maatouk, Smita Krishnaswamy, Leandros Tassiulas, Menglin Yang, Rex Ying', 'link': 'https://arxiv.org/abs/2505.24722', 'abstract': 'Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.', 'abstract_zh': 'Hyperbolic Space-Based HypErbolic Large Language Models (HELM) and Its Applications', 'title_zh': 'HELM：通过混合曲率专家的双曲大型语言模型'}
{'arxiv_id': 'arXiv:2505.24716', 'title': 'Towards Scalable Schema Mapping using Large Language Models', 'authors': 'Christopher Buss, Mahdis Safari, Arash Termehchy, Stefan Lee, David Maier', 'link': 'https://arxiv.org/abs/2505.24716', 'abstract': 'The growing need to integrate information from a large number of diverse sources poses significant scalability challenges for data integration systems. These systems often rely on manually written schema mappings, which are complex, source-specific, and costly to maintain as sources evolve. While recent advances suggest that large language models (LLMs) can assist in automating schema matching by leveraging both structural and natural language cues, key challenges remain. In this paper, we identify three core issues with using LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to input phrasing and structure, which we propose methods to address through sampling and aggregation techniques; (2) the need for more expressive mappings (e.g., GLaV), which strain the limited context windows of LLMs; and (3) the computational cost of repeated LLM calls, which we propose to mitigate through strategies like data type prefiltering.', 'abstract_zh': '日益增长的从大量多样来源整合信息的需求给数据整合系统带来了显著的可扩展性挑战。这些系统通常依赖于手工编写的模式映射，这些映射复杂、来源特定，随着来源的变化维护成本高昂。尽管近期进展表明大型语言模型（LLMs）可以通过利用结构和自然语言线索来协助自动化模式匹配，但仍存在一些关键挑战。在本文中，我们指出了使用LLMs进行模式映射的三个核心问题：（1）由于对输入措辞和结构的敏感性导致不一致的输出，我们提出了通过采样和聚合技术来解决的方法；（2）需要更具表现力的映射（例如GLaV），这会超出LLMs有限的上下文窗口；（3）重复调用LLMs的计算成本，我们提出通过数据类型预筛选等策略来减轻这一问题。', 'title_zh': '面向大规模语义映射的大型语言模型方法研究'}
{'arxiv_id': 'arXiv:2505.24715', 'title': 'CoRet: Improved Retriever for Code Editing', 'authors': 'Fabio Fehr, Prabhu Teja Sivaprasad, Luca Franceschi, Giovanni Zappella', 'link': 'https://arxiv.org/abs/2505.24715', 'abstract': "In this paper, we introduce CoRet, a dense retrieval model designed for code-editing tasks that integrates code semantics, repository structure, and call graph dependencies. The model focuses on retrieving relevant portions of a code repository based on natural language queries such as requests to implement new features or fix bugs. These retrieved code chunks can then be presented to a user or to a second code-editing model or agent. To train CoRet, we propose a loss function explicitly designed for repository-level retrieval. On SWE-bench and Long Code Arena's bug localisation datasets, we show that our model substantially improves retrieval recall by at least 15 percentage points over existing models, and ablate the design choices to show their importance in achieving these results.", 'abstract_zh': '本文介绍了CoRet，一种用于代码编辑任务的密集检索模型，融合了代码语义、仓库结构和调用图依赖关系。该模型专注于根据诸如实现新功能或修复bug等自然语言查询检索代码仓库的相关部分。获取的代码片段可以呈现给用户或第二代码编辑模型或代理。为了训练CoRet，我们提出了一种显式针对仓库级别检索设计的损失函数。在SWE-bench和Long Code Arena的bug定位数据集中，我们展示了我们的模型在检索召回率上比现有模型提高了至少15个百分点，并消融了设计选择以证明其对实现这些结果的重要性。', 'title_zh': 'CoRet: 提升的代码编辑检索器'}
{'arxiv_id': 'arXiv:2505.24710', 'title': 'Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting', 'authors': 'Wei Chen, Jiahao Zhang, Haipeng Zhu, Boyan Xu, Zhifeng Hao, Keli Zhang, Junjian Ye, Ruichu Cai', 'link': 'https://arxiv.org/abs/2505.24710', 'abstract': 'Large language models (LLMs) have shown great potential in decision-making due to the vast amount of knowledge stored within the models. However, these pre-trained models are prone to lack reasoning abilities and are difficult to adapt to new environments, further hindering their application to complex real-world tasks. To address these challenges, inspired by the human cognitive process, we propose Causal-aware LLMs, which integrate the structural causal model (SCM) into the decision-making process to model, update, and utilize structured knowledge of the environment in a ``learning-adapting-acting" paradigm. Specifically, in the learning stage, we first utilize an LLM to extract the environment-specific causal entities and their causal relations to initialize a structured causal model of the environment. Subsequently,in the adapting stage, we update the structured causal model through external feedback about the environment, via an idea of causal intervention. Finally, in the acting stage, Causal-aware LLMs exploit structured causal knowledge for more efficient policy-making through the reinforcement learning agent. The above processes are performed iteratively to learn causal knowledge, ultimately enabling the causal-aware LLMs to achieve a more accurate understanding of the environment and make more efficient decisions. Experimental results across 22 diverse tasks within the open-world game ``Crafter" validate the effectiveness of our proposed method.', 'abstract_zh': '具有因果意识的大语言模型（Causal-aware LLMs）：通过集成结构因果模型（SCM）实现环境结构化知识的建模、更新和利用', 'title_zh': '因果意识大语言模型：通过学习、适应和行动增强决策制定'}
{'arxiv_id': 'arXiv:2505.24709', 'title': 'On Symmetric Losses for Robust Policy Optimization with Noisy Preferences', 'authors': 'Soichiro Nishimori, Yu-Jie Zhang, Thanawat Lodkaew, Masashi Sugiyama', 'link': 'https://arxiv.org/abs/2505.24709', 'abstract': 'Optimizing policies based on human preferences is key to aligning language models with human intent. This work focuses on reward modeling, a core component in reinforcement learning from human feedback (RLHF), and offline preference optimization, such as direct preference optimization. Conventional approaches typically assume accurate annotations. However, real-world preference data often contains noise due to human errors or biases. We propose a principled framework for robust policy optimization under noisy preferences, viewing reward modeling as a classification problem. This allows us to leverage symmetric losses, known for their robustness to label noise in classification, leading to our Symmetric Preference Optimization (SymPO) method. We prove that symmetric losses enable successful policy optimization even under noisy labels, as the resulting reward remains rank-preserving -- a property sufficient for policy improvement. Experiments on synthetic and real-world tasks demonstrate the effectiveness of SymPO.', 'abstract_zh': '基于人类偏好的政策优化是将语言模型与人类意图对齐的关键。本工作聚焦于奖励建模，这是人类反馈强化学习（RLHF）的核心组件之一，以及离线偏好优化，如直接偏好优化。传统方法通常假设标注准确。然而，实际的偏好数据由于人类错误或偏差往往包含噪声。我们提出了一个稳健的政策优化框架，将奖励建模视为分类问题，从而利用对标签噪声具有鲁棒性的对称损失，提出了一种对称偏好优化（SymPO）方法。我们证明了对称损失即使在噪声标签下也能成功进行政策优化，因为产生的奖励保持了排名不变——这是政策改进所需的属性。实验证明了SymPO的有效性。', 'title_zh': '关于稳健策略优化中无噪音偏好下的对称损失方法'}
{'arxiv_id': 'arXiv:2505.24701', 'title': 'Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison', 'authors': 'Tejul Pandit, Meet Raval, Dhvani Upadhyay', 'link': 'https://arxiv.org/abs/2505.24701', 'abstract': 'Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions but often suffers from the scarcity of diverse, labeled datasets that reflect real-world conversational nuances. This paper presents an approach for generating synthetic ABSA data using Large Language Models (LLMs) to address this gap. We detail the generation process aimed at producing data with consistent topic and sentiment distributions across multiple domains using GPT-4o. The quality and utility of the generated data were evaluated by assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification tasks. Our results demonstrate the effectiveness of the synthetic data, revealing distinct performance trade-offs among the models: DeepSeekR1 showed higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall, and Gemini 1.5 Pro offered significantly faster inference. We conclude that LLM-based synthetic data generation is a viable and flexible method for creating valuable ABSA resources, facilitating research and model evaluation without reliance on limited or inaccessible real-world labeled data.', 'abstract_zh': '基于方面的意见分析（ABSA）提供了详细的见解，但常常因缺乏多样且标注的真实世界对话数据而受到限制。本文提出了一种使用大型语言模型（LLMs）生成合成ABSA数据的方法，以填补这一空白。我们使用GPT-4o详细描述了生成过程，旨在产生在多个领域中具有一致的话题和情感分布的数据。通过评估三种最先进的LLMs（Gemini 1.5 Pro、Claude 3.5 Sonnet和DeepSeek-R1）在主题和情感分类任务上的性能，来评估生成数据的质量和实用性。我们的结果表明合成数据的有效性，揭示了不同模型之间的性能权衡：DeepSeekR1展现出更高的精确度，Gemini 1.5 Pro和Claude 3.5 Sonnet表现出强劲的召回率，而Gemini 1.5 Pro提供了显著更快的推理速度。我们得出结论，基于LLM的合成数据生成是一种可行且灵活的方法，可以创建有价值的ABSA资源，促进研究和模型评估，而无需依赖有限或难以获取的真实标注数据。', 'title_zh': '基于LLMs的多域ABSAC对话数据集生成以进行实际评估和模型比较'}
{'arxiv_id': 'arXiv:2505.24684', 'title': 'Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs', 'authors': 'Zihao Chen, Yu Xiang, Wenyong Wang', 'link': 'https://arxiv.org/abs/2505.24684', 'abstract': 'Despite the success in learning semantically meaningful, unsupervised disentangled representations, variational autoencoders (VAEs) and their variants face a fundamental theoretical challenge: substantial evidence indicates that unsupervised disentanglement is unattainable without implicit inductive bias, yet such bias remains elusive. In this work, we focus on exploring the implicit inductive bias that drive disentanglement in VAEs with factorization priors. By analyzing the total correlation in \\b{eta}-TCVAE, we uncover a crucial implicit inductive bias called disentangling granularity, which leads to the discovery of an interesting "V"-shaped optimal Evidence Lower Bound (ELBO) trajectory within the parameter space. This finding is validated through over 100K experiments using factorized VAEs and our newly proposed model, \\b{eta}-STCVAE. Notably, experimental results reveal that conventional factorized VAEs, constrained by fixed disentangling granularity, inherently tend to disentangle low-complexity feature. Whereas, appropriately tuning disentangling granularity, as enabled by \\b{eta}-STCVAE, broadens the range of disentangled representations, allowing for the disentanglement of high-complexity features. Our findings unveil that disentangling granularity as an implicit inductive bias in factorized VAEs influence both disentanglement performance and the inference of the ELBO, offering fresh insights into the interpretability and inherent biases of VAEs.', 'abstract_zh': '因子化先验下变分自编码器中隐含归纳偏置的探索：解缠粒度的影响', 'title_zh': '粒度分解：因子化VAEs中的隐式归纳偏见'}
{'arxiv_id': 'arXiv:2505.24683', 'title': 'Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation', 'authors': 'Dayeon Ki, Kevin Duh, Marine Carpuat', 'link': 'https://arxiv.org/abs/2505.24683', 'abstract': 'As people increasingly use AI systems in work and daily life, feedback mechanisms that help them use AI responsibly are urgently needed, particularly in settings where users are not equipped to assess the quality of AI predictions. We study a realistic Machine Translation (MT) scenario where monolingual users decide whether to share an MT output, first without and then with quality feedback. We compare four types of quality feedback: explicit feedback that directly give users an assessment of translation quality using 1) error highlights and 2) LLM explanations, and implicit feedback that helps users compare MT inputs and outputs through 3) backtranslation and 4) question-answer (QA) tables. We find that all feedback types, except error highlights, significantly improve both decision accuracy and appropriate reliance. Notably, implicit feedback, especially QA tables, yields significantly greater gains than explicit feedback in terms of decision accuracy, appropriate reliance, and user perceptions, receiving the highest ratings for helpfulness and trust, and the lowest for mental burden.', 'abstract_zh': '随着人们在工作和日常生活中 increasingly 使用 AI 系统，特别是在用户不具备评估 AI 预测质量能力的场景中，需要相应的反馈机制帮助他们负责任地使用 AI。我们研究了一个现实的机器翻译 (MT) 情景，其中独语用户在有和没有质量反馈的情况下决定是否分享 MT 输出。我们比较了四种类型的质量反馈：直接给出翻译质量评估的显式反馈（包括 1）错误高亮和 2）大模型解释），以及通过 3）回译和 4）问答（QA）表帮助用户比较 MT 输入和输出的隐式反馈。我们发现，除了错误高亮外，所有类型的反馈都显著提高了决策准确性和适当的依赖性。值得注意的是，隐式反馈，尤其是 QA 表，在决策准确性和适当依赖性方面以及用户感知方面带来了显著更大的增益，并收到了最高的有用性和信任度评分，以及最低的认知负担评分。', 'title_zh': '我应该分享这个翻译吗？基于机器翻译质量反馈的用户依赖评估'}
{'arxiv_id': 'arXiv:2505.24681', 'title': 'Generative Knowledge Production Pipeline Driven by Academic Influencers', 'authors': 'Katalin Feher, Marton Demeter', 'link': 'https://arxiv.org/abs/2505.24681', 'abstract': "Generative AI transforms knowledge production, validation, and dissemination, raising academic integrity and credibility concerns. This study examines 53 academic influencer videos that reached 5.3 million viewers to identify an emerging, structured, implementation-ready pipeline balancing originality, ethical compliance, and human-AI collaboration despite the disruptive impacts. Findings highlight generative AI's potential to automate publication workflows and democratize participation in knowledge production while challenging traditional scientific norms. Academic influencers emerge as key intermediaries in this paradigm shift, connecting bottom-up practices with institutional policies to improve adaptability. Accordingly, the study proposes a generative publication production pipeline and a policy framework for co-intelligence adaptation and reinforcing credibility-centered standards in AI-powered research. These insights support scholars, educators, and policymakers in understanding AI's transformative impact by advocating responsible and innovation-driven knowledge production. Additionally, they reveal pathways for automating best practices, optimizing scholarly workflows, and fostering creativity in academic research and publication.", 'abstract_zh': '生成式AI变革知识生产、验证和传播，引发学术诚信和可信度 Concerns. 本研究分析了53个学术影响力视频，观看人次达530万，旨在识别一种平衡原创性、伦理合规性和人类-AI协作的新兴结构化实施流程，尽管受到颠覆性影响。研究发现强调生成式AI有潜力自动化出版工作流程并使知识生产民主化，同时挑战传统科学规范。学术影响力者成为这一范式转变中的关键中介，连接自下而上的实践与机构政策，提高适应性。据此，本研究提出了一种生成式出版生产流程和一种促进AI驱动研究中协同智能适应及强化以可信度为中心的标准的政策框架。这些见解支持学者、教育者和政策制定者理解AI的变革影响，倡导负责任和创新驱动的知识生产。此外，它们揭示了自动化最佳实践、优化学者工作流程和激发学术研究与出版中创造力的途径。', 'title_zh': '由学术影响力驱动的生成性知识生产流水线'}
{'arxiv_id': 'arXiv:2505.24671', 'title': 'Multiple LLM Agents Debate for Equitable Cultural Alignment', 'authors': 'Dayeon Ki, Rachel Rudinger, Tianyi Zhou, Marine Carpuat', 'link': 'https://arxiv.org/abs/2505.24671', 'abstract': 'Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).', 'abstract_zh': '大型语言模型（LLMs）需要适应多样的文化背景以惠及全球多元社区。尽管以往的努力主要集中在单一LLM和单轮对话方法上，我们提出利用多种LLM的互补优势促进文化适应性。我们引入了一种多智能体辩论框架，其中两个基于LLM的智能体就一种文化场景进行辩论，并共同做出最终决策。我们提出了两种变体：一种是LLM智能体专门辩论，另一种是他们在回合中动态选择自我反思与辩论之间切换。我们在7种开源LLM（以及21种LLM组合）上使用涵盖75个国家社会礼仪规范的NormAd-ETI基准进行了评估。实验结果显示，辩论方法在总体准确率和文化群体平等性方面优于单一LLM基线。值得注意的是，多智能体辩论使相对较小的LLM（7-9B参数）能够达到与更大模型（27B参数）相当的准确率。', 'title_zh': '多个LLM代理为公平的文化对齐展开辩论'}
{'arxiv_id': 'arXiv:2505.24649', 'title': 'BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models', 'authors': 'Huu-Thien Tran, Thanh-Dat Truong, Khoa Luu', 'link': 'https://arxiv.org/abs/2505.24649', 'abstract': 'Large vision-language models have become widely adopted to advance in various domains. However, developing a trustworthy system with minimal interpretable characteristics of large-scale models presents a significant challenge. One of the most prevalent terms associated with the fallacy functions caused by these systems is hallucination, where the language model generates a response that does not correspond to the visual content. To mitigate this problem, several approaches have been developed, and one prominent direction is to ameliorate the decoding process. In this paper, we propose a new Bijective Maximum Likelihood Learning (BIMA) approach to hallucination mitigation using normalizing flow theories. The proposed BIMA method can efficiently mitigate the hallucination problem in prevailing vision-language models, resulting in significant improvements. Notably, BIMA achieves the average F1 score of 85.06% on POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%, respectively. To the best of our knowledge, this is one of the first studies that contemplates the bijection means to reduce hallucination induced by large vision-language models.', 'abstract_zh': '大规模vision-language模型已在多个领域得到广泛应用。然而，开发具有最小可解释性特征的信任系统仍面临巨大挑战。与这些系统导致的谬误功能最相关的术语之一是幻觉，即语言模型生成的响应与视觉内容不一致。为了解决这一问题，已开发出多种方法，其中一个主要方向是改进解码过程。在本文中，我们提出了一种新的基于规范化流理论的双向最大似然学习（BIMA）方法，用于幻觉缓解。所提出的BIMA方法可以高效缓解现有vision-language模型中的幻觉问题，取得显著改进。据我们所知，这是首次研究考虑使用双射方法减少大型vision-language模型引起的幻觉的研究之一。', 'title_zh': 'BIMA：大规模视觉-语言模型中幻觉预测与缓解的双向最大似然学习方法'}
{'arxiv_id': 'arXiv:2505.24640', 'title': 'Efficient Text Encoders for Labor Market Analysis', 'authors': 'Jens-Joris Decorte, Jeroen Van Hautte, Chris Develder, Thomas Demeester', 'link': 'https://arxiv.org/abs/2505.24640', 'abstract': 'Labor market analysis relies on extracting insights from job advertisements, which provide valuable yet unstructured information on job titles and corresponding skill requirements. While state-of-the-art methods for skill extraction achieve strong performance, they depend on large language models (LLMs), which are computationally expensive and slow. In this paper, we propose \\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level attention that is well-suited for the extreme multi-label classification task of skill classification. \\textbf{ConTeXT-match} significantly improves skill extraction efficiency and performance, achieving state-of-the-art results with a lightweight bi-encoder model. To support robust evaluation, we introduce \\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill annotations that explicitly address the redundancy in the large label space. Finally, we present \\textbf{JobBERT V2}, an improved job title normalization model that leverages extracted skills to produce high-quality job title representations. Experiments demonstrate that our models are efficient, accurate, and scalable, making them ideal for large-scale, real-time labor market analysis.', 'abstract_zh': '劳动市场分析依赖于从招聘信息中提取洞察，招聘信息提供了职位名称和对应技能要求的宝贵但未结构化的信息。虽然最先进的技能提取方法表现强劲，但它们依赖于大型语言模型（LLMs），这使得它们计算成本高昂且速度较慢。在本文中，我们提出了\\textbf{ConTeCTX\tmatch}，一种适用于技能分类的极端多标签分类任务的新颖对比学习方法，具有token级注意力机制。ConTeCTX\tmatch显著提高了技能提取的效率和性能，使用轻量级双编码器模型达到了最先进的效果。为了支持稳健的评估，我们引入了\\textbf{Skill-XL}，这是一个新的基准，包含详尽的、句子级别的技能注解，明确解决了大量标签空间中的冗余问题。最后，我们呈现了\\textbf{JobBERT V2}，一种改进的工作职位规范化模型，该模型利用提取的技能生成高质量的工作职位表示。实验表明，我们的模型高效、准确且可扩展，使其成为大规模、实时劳动市场分析的理想工具。', 'title_zh': '高效文本编码器在劳动力市场分析中的应用'}
{'arxiv_id': 'arXiv:2505.24638', 'title': 'Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models', 'authors': 'Zahid Hassan Tushar, Adeleke Ademakinwa, Jianwu Wang, Zhibo Zhang, Sanjay Purushotham', 'link': 'https://arxiv.org/abs/2505.24638', 'abstract': "Cloud Optical Thickness (COT) is a critical cloud property influencing Earth's climate, weather, and radiation budget. Satellite radiance measurements enable global COT retrieval, but challenges like 3D cloud effects, viewing angles, and atmospheric interference must be addressed to ensure accurate estimation. Traditionally, the Independent Pixel Approximation (IPA) method, which treats individual pixels independently, has been used for COT estimation. However, IPA introduces significant bias due to its simplified assumptions. Recently, deep learning-based models have shown improved performance over IPA but lack robustness, as they are sensitive to variations in radiance intensity, distortions, and cloud shadows. These models also introduce substantial errors in COT estimation under different solar and viewing zenith angles. To address these challenges, we propose a novel angle-invariant, attention-based deep model called Cloud-Attention-Net with Angle Coding (CAAC). Our model leverages attention mechanisms and angle embeddings to account for satellite viewing geometry and 3D radiative transfer effects, enabling more accurate retrieval of COT. Additionally, our multi-angle training strategy ensures angle invariance. Through comprehensive experiments, we demonstrate that CAAC significantly outperforms existing state-of-the-art deep learning models, reducing cloud property retrieval errors by at least a factor of nine.", 'abstract_zh': '基于角度Attention的云光厚度网络（Cloud-Attention-Net with Angle Coding, CAAC）：一种角度不变的深学习模型', 'title_zh': '基于角度不变注意机制的深度学习模型云光学厚度反演'}
{'arxiv_id': 'arXiv:2505.24630', 'title': 'The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models', 'authors': 'Junyi Li, Hwee Tou Ng', 'link': 'https://arxiv.org/abs/2505.24630', 'abstract': 'Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance.', 'abstract_zh': '大型语言模型（LLMs）通过强化学习（RL）优化在推理任务中取得了显著进展，实现了各种具有挑战性的基准测试中的出色能力。然而，我们的实证分析揭示了一个关键缺点：面向推理的RL微调显著增加了幻觉现象的频率。我们理论上分析了RL训练动力学，将高方差梯度、熵引起的随机性及对虚假局部最优的敏感性识别为导致幻觉的关键因素。为了应对这一缺点，我们提出了事实意识分步策略优化（FSPO），这是一种创新的RL微调算法，其中间引入了明确的事实验证。FSPO利用自动验证与给定证据的对比，动态调整词元级优势值，激励推理过程中的事实正确性。使用Qwen2.5和Llama模型在数学推理和幻觉基准测试中的实验表明，FSPO有效减少了幻觉现象，同时提高了推理准确性，显著提升了可靠性和性能。', 'title_zh': '幻觉困境：事实导向的大型推理模型增强学习'}
{'arxiv_id': 'arXiv:2505.24625', 'title': 'Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors', 'authors': 'Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang', 'link': 'https://arxiv.org/abs/2505.24625', 'abstract': "Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.", 'abstract_zh': '以前的研究考察了多模态大型语言模型（MLLMs）在将3D场景视为视频进行解释时的应用。这些方法通常依赖于全面的3D数据输入，如点云或重建的鸟瞰图（BEV）地图。在我们的研究中，我们通过增强MLLMs直接从视频数据中理解与推理三维空间的能力，推进了该领域的发展，而不需额外的3D输入。我们提出了一种新颖而有效的方法——视频-3D几何大型语言模型（VG LLM）。我们的方法使用三维视觉几何编码器从视频序列中提取三维先验信息，并将这些信息与视觉标记结合后输入到MLLM中。大量实验表明，我们的方法在各种与3D场景理解和空间推理相关的任务中取得了显著改进，且所有这些任务皆直接从视频源中学习。令人印象深刻的是，我们的4B模型不依赖于显式的3D数据输入，在VSI-Bench评估中达到了与现有最先进的方法具有竞争力的结果，并超越了Gemini-1.5-Pro。', 'title_zh': '从视频中学习构建3D世界：通过3D视觉几何先验增强多模态大规模语言模型'}
{'arxiv_id': 'arXiv:2505.24623', 'title': 'Hyperbolic Dataset Distillation', 'authors': 'Wenyuan Li, Guang Li, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama', 'link': 'https://arxiv.org/abs/2505.24623', 'abstract': 'To address the computational and storage challenges posed by large-scale datasets in deep learning, dataset distillation has been proposed to synthesize a compact dataset that replaces the original while maintaining comparable model performance. Unlike optimization-based approaches that require costly bi-level optimization, distribution matching (DM) methods improve efficiency by aligning the distributions of synthetic and original data, thereby eliminating nested optimization. DM achieves high computational efficiency and has emerged as a promising solution. However, existing DM methods, constrained to Euclidean space, treat data as independent and identically distributed points, overlooking complex geometric and hierarchical relationships. To overcome this limitation, we propose a novel hyperbolic dataset distillation method, termed HDD. Hyperbolic space, characterized by negative curvature and exponential volume growth with distance, naturally models hierarchical and tree-like structures. HDD embeds features extracted by a shallow network into the Lorentz hyperbolic space, where the discrepancy between synthetic and original data is measured by the hyperbolic (geodesic) distance between their centroids. By optimizing this distance, the hierarchical structure is explicitly integrated into the distillation process, guiding synthetic samples to gravitate towards the root-centric regions of the original data distribution while preserving their underlying geometric characteristics. Furthermore, we find that pruning in hyperbolic space requires only 20% of the distilled core set to retain model performance, while significantly improving training stability. Notably, HDD is seamlessly compatible with most existing DM methods, and extensive experiments on different datasets validate its effectiveness.', 'abstract_zh': '针对大规模数据集在深度学习中带来的计算和存储挑战，数据集蒸馏已被提出以合成一个紧凑的数据集来替代原始数据并维持相当的模型性能。不同于基于优化的方法需要昂贵的双层优化，分布匹配（DM）方法通过对齐合成数据和原始数据的分布来提高效率，从而消除嵌套优化。DM实现了高计算效率，并已成为一种有前途的解决方案。然而，现有的DM方法仅限于欧几里得空间，将数据视为独立同分布的点，忽略了复杂的几何和层次结构关系。为克服这一局限，我们提出了一种新颖的双曲空间数据集蒸馏方法，称为HDD。双曲空间以其负曲率和随距离呈指数增长的体积自然地建模了层次和树形结构。HDD将浅网络提取的特征嵌入洛伦兹双曲空间，其中合成数据与原始数据之间的差异由其质心之间的双曲（测地）距离衡量。通过优化这一距离，层次结构显式地整合到蒸馏过程中，引导合成样本向着原始数据分布的根中心区域聚集，同时保留其潜在的几何特性。此外，我们发现，在双曲空间中剪枝仅需蒸馏核心集的20%即可保持模型性能，同时显著提高训练稳定性。值得注意的是，HDD与大多数现有DM方法无缝兼容，不同数据集上的广泛实验验证了其有效性。', 'title_zh': '双曲数据集蒸馏'}
{'arxiv_id': 'arXiv:2505.24616', 'title': 'Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX', 'authors': 'Nikita Martynov, Anastasia Mordasheva, Dmitriy Gorbetskiy, Danil Astafurov, Ulyana Isaeva, Elina Basyrova, Sergey Skachkov, Victoria Berestova, Nikolay Ivanov, Valeriia Zanina, Alena Fenogenova', 'link': 'https://arxiv.org/abs/2505.24616', 'abstract': 'We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.', 'abstract_zh': '我们引入了POLLUX，一个全面的开源基准，旨在评估俄语大型语言模型（LLMs）的生成能力。我们的主要贡献是一种新颖的评估方法，提高了对LLM评估的可解释性。对于每种任务类型，我们定义了一套详细的评估标准，并开发了一套评分协议，其中模型评估响应并提供评分依据。这使得评估过程超越了传统的资源密集型逐一对比的人类评估，实现透明且基于标准的评估。POLLUX包括一个详细且层次分明的35种任务类型分类体系，涵盖诸如代码生成、创造性写作和实际助手使用场景等多种生成领域，共包含2,100个手工制作的专业提示。每个任务按难度（简单/中等/复杂）分类，由专家从零开始构建数据集。我们还发布了具有细致评估能力的大型语言模型评判器（7B和32B）系列评估工具，为生成输出的细微评估提供支持。这种做法提供了可扩展且可解释的评估和注释工具，有效地替代了成本高且精度低的人类判断。', 'title_zh': '判决之眼：剖析俄语系LLM的评估方法与POLLUX'}
{'arxiv_id': 'arXiv:2505.24595', 'title': 'Binary Cumulative Encoding meets Time Series Forecasting', 'authors': 'Andrei Chernov, Vitaliy Pozdnyakov, Ilya Makarov', 'link': 'https://arxiv.org/abs/2505.24595', 'abstract': 'Recent studies in time series forecasting have explored formulating regression via classification task. By discretizing the continuous target space into bins and predicting over a fixed set of classes, these approaches benefit from stable training, robust uncertainty modeling, and compatibility with modern deep learning architectures. However, most existing methods rely on one-hot encoding that ignores the inherent ordinal structure of the underlying values. As a result, they fail to provide information about the relative distance between predicted and true values during training. In this paper, we propose to address this limitation by introducing binary cumulative encoding (BCE), that represents scalar targets into monotonic binary vectors. This encoding implicitly preserves order and magnitude information, allowing the model to learn distance-aware representations while still operating within a classification framework. We propose a convolutional neural network architecture specifically designed for BCE, incorporating residual and dilated convolutions to enable fast and expressive temporal modeling. Through extensive experiments on benchmark forecasting datasets, we show that our approach outperforms widely used methods in both point and probabilistic forecasting, while requiring fewer parameters and enabling faster training.', 'abstract_zh': '最近时间序列预测的研究探索了通过分类任务进行回归的方法。通过将连续的目标空间离散化为区间，并在固定类别的集合上进行预测，这些方法可以从稳定的训练、鲁棒的不确定性建模以及与现代深度学习架构的兼容性中受益。然而，现有的大多数方法依赖于独热编码，这忽略了底层值的固有序数结构。因此，它们在训练过程中无法提供关于预测值与真实值之间相对距离的信息。在本文中，我们提出了一种通过引入二元累积编码（BCE）来解决这一局限性，该编码将标量目标表示为单调的二元向量。这种编码隐式地保留了顺序和幅度信息，使模型能够在分类框架中学习距离感知的表示。我们提出了一种专门为BCE设计的卷积神经网络架构，结合了残差连接和扩张卷积，以实现快速而富有表现力的时间建模。通过在基准预测数据集上的广泛实验，我们表明，与广泛使用的预测方法相比，我们的方法在点预测和概率预测方面表现更优，所需的参数更少，并能实现更快的训练。', 'title_zh': '二进制累积编码 meets 时间序列预测'}
{'arxiv_id': 'arXiv:2505.24593', 'title': 'Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis', 'authors': 'Junzhuo Li, Bo Wang, Xiuze Zhou, Peijie Jiang, Jia Liu, Xuming Hu', 'link': 'https://arxiv.org/abs/2505.24593', 'abstract': 'The interpretability of Mixture-of-Experts (MoE) models, especially those with heterogeneous designs, remains underexplored. Existing attribution methods for dense models fail to capture dynamic routing-expert interactions in sparse MoE architectures. To address this issue, we propose a cross-level attribution algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results show MoE models achieve 37% higher per-layer efficiency via a "mid-activation, late-amplification" pattern: early layers screen experts, while late layers refine knowledge collaboratively. Ablation studies reveal a "basic-refinement" framework--shared experts handle general tasks (entity recognition), while routed experts specialize in domain-specific processing (geographic attributes). Semantic-driven routing is evidenced by strong correlations between attention heads and experts (r=0.68), enabling task-aware coordination. Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10 experts) through shared expert redundancy, whereas shallow OLMoE suffers severe degradation (76% drop). Task sensitivity further guides design: core-sensitive tasks (geography) require concentrated expertise, while distributed-tolerant tasks (object attributes) leverage broader participation. These insights advance MoE interpretability, offering principles to balance efficiency, specialization, and robustness.', 'abstract_zh': 'MoE模型的可解释性，特别是异构设计的模型，仍处于未探索阶段。现有的密集模型归因方法无法捕捉稀疏MoE架构中的路由-专家动态交互。为解决这一问题，我们提出了一种跨级归因算法，用于分析稀疏MoE架构（Qwen 1.5-MoE, OLMoE, Mixtral-8x7B）与密集模型（Qwen 1.5-7B, Llama-7B, Mixtral-7B）之间的差异。结果表明，MoE模型通过“中间激活，后期放大”模式实现每层37%的效率提升：早期层筛选专家，而后期层协作细化知识。消融研究揭示了“基础-精炼”框架——共享专家处理通用任务（实体识别），而路由专家专门处理领域特定处理（地理属性）。以语义为导向的路由通过注意力头与专家之间强烈的相关性（r=0.68）凸显，实现任务感知的协调。值得注意的是，架构深度决定了鲁棒性：深度的Qwen 1.5-MoE通过共享专家冗余来缓解专家故障（例如，封锁顶级10个专家时地理任务的MRR下降43%），而浅层的OLMoE遭受严重降解（76%的下降）。任务敏感性进一步指导设计：核心敏感任务（地理）需要集中的专业知识，而分布式耐受任务（对象属性）利用更广泛的参与。这些洞见推进了MoE的可解释性，提供了平衡效率、专业化和鲁棒性的原则。', 'title_zh': '混合专家体系中知识归因的解码：基础-精炼协作与效率分析'}
{'arxiv_id': 'arXiv:2505.24592', 'title': 'A Flat Minima Perspective on Understanding Augmentations and Model Robustness', 'authors': 'Weebum Yoo, Sung Whan Yoon', 'link': 'https://arxiv.org/abs/2505.24592', 'abstract': "Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruption, adversarial attacks, and domain shifts. Data augmentation is one of the prevalent and effective ways to enhance robustness. Despite the great success of augmentations in different fields, a general theoretical understanding of their efficacy in improving model robustness is lacking. We offer a unified theoretical framework to clarify how augmentations can enhance model robustness through the lens of loss surface flatness and PAC generalization bound. Our work diverges from prior studies in that our analysis i) broadly encompasses much of the existing augmentation methods, and ii) is not limited to specific types of distribution shifts like adversarial attacks. We confirm our theories through simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and ImageNet datasets, as well as domain generalization benchmarks including PACS and OfficeHome.", 'abstract_zh': '模型稳健性表明模型在未预见的数据分布变化、数据腐蚀、对抗攻击和领域变换等方面具有良好泛化能力的能力。数据增强是提高稳健性的一种常见且有效的方法。尽管数据增强在不同领域取得了巨大成功，但在提高模型稳健性方面的有效性上缺乏普遍的理论理解。我们提供了一个统一的理论框架，通过损失面平坦度和PAC泛化界的角度阐明数据增强如何提高模型稳健性。我们的工作与先前研究不同，因为我们对数据增强的分析i) 广泛涵盖了现有的大部分增强方法，ii) 不局限于特定类型的分布变化，如对抗攻击。我们通过在CIFAR和ImageNet数据集的现有通用腐蚀和对抗稳健性基准以及包括PACS和OfficeHome在内的领域泛化基准上的模拟验证了我们的理论。', 'title_zh': '从平坦极小值的角度理解增强和模型稳健性'}
{'arxiv_id': 'arXiv:2505.24584', 'title': 'AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams', 'authors': 'Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana', 'link': 'https://arxiv.org/abs/2505.24584', 'abstract': 'Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials; however, transitioning these discoveries to industrial-scale production remains a critical bottleneck, as it requires the development of entirely new chemical manufacturing processes. Current AI methods cannot auto-generate PFDs or PIDs, despite their critical role in scaling chemical processes, while adhering to engineering constraints. We present a closed loop, physics aware framework for the automated generation of industrially viable PFDs and PIDs. The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure feasibility. To improve both runtime efficiency and model compactness, the framework incorporates advanced inference time optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test Time Inference Scaling and independently applies structural pruning techniques (width and depth) guided by importance heuristics to reduce model size with minimal accuracy loss. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R&D timelines from lab discovery to plant deployment.', 'abstract_zh': 'Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials; however, transitioning these discoveries to industrial-scale production remains a critical bottleneck, as it requires the development of entirely new chemical manufacturing processes. Current AI methods cannot auto-generate PFDs or PIDs, despite their critical role in scaling chemical processes, while adhering to engineering constraints. We present a closed loop, physics aware framework for the automated generation of industrially viable PFDs and PIDs. The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure feasibility. To improve both runtime efficiency and model compactness, the framework incorporates advanced inference time optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test Time Inference Scaling and independently applies structural pruning techniques (width and depth) guided by importance heuristics to reduce model size with minimal accuracy loss. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R&D timelines from lab discovery to plant deployment.\n\n标题：\n一种意识物理的闭环框架：用于工业可实现的PFD和PID的自动生成', 'title_zh': '自动生成化学工艺和仪器图的闭环、物理aware智能代理框架AutoChemSchematic AI'}
{'arxiv_id': 'arXiv:2505.24575', 'title': 'NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization', 'authors': 'Hyuntak Kim, Byung-Hak Kim', 'link': 'https://arxiv.org/abs/2505.24575', 'abstract': 'Summarizing long-form narratives--such as books, movies, and TV scripts--requires capturing intricate plotlines, character interactions, and thematic coherence, a task that remains challenging for existing LLMs. We introduce NexusSum, a multi-agent LLM framework for narrative summarization that processes long-form text through a structured, sequential pipeline--without requiring fine-tuning. Our approach introduces two key innovations: (1) Dialogue-to-Description Transformation: A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence. (2) Hierarchical Multi-LLM Summarization: A structured summarization pipeline that optimizes chunk processing and controls output length for accurate, high-quality summaries. Our method establishes a new state-of-the-art in narrative summarization, achieving up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts. These results demonstrate the effectiveness of multi-agent LLMs in handling long-form content, offering a scalable approach for structured summarization in diverse storytelling domains.', 'abstract_zh': '长篇叙事概要总结——诸如书籍、电影和电视剧剧本——要求捕捉复杂的故事情节、角色互动和主题一致性，现有LLM仍面临挑战。我们引入了NexusSum，这是一种多代理LLM框架，用于叙事概要总结，通过结构化、顺序的管道处理长篇文本，无需微调。我们的方法引入了两项关键创新：（1）对话到描述转换：一种针对叙事的预处理方法，将角色对话和描述性文本标准化为统一格式，提高连贯性。（2）分层多LLM概要总结：一种结构化的概要总结管道，优化块处理并控制输出长度，以实现准确和高质量的概要。该方法在书籍、电影和电视剧剧本的叙事概要总结中达到了新的最先进水平，BERTScore（F1）提高了最高30.0%。这些结果表明多代理LLM在处理长篇文本方面的有效性，为不同叙事领域的结构化概要总结提供了可扩展的方法。', 'title_zh': 'NexusSum：层次化的大型语言模型代理用于长篇叙事总结'}
{'arxiv_id': 'arXiv:2505.24554', 'title': 'Bench4KE: Benchmarking Automated Competency Question Generation', 'authors': 'Anna Sofia Lippolis, Minh Davide Ragagni, Paolo Ciancarini, Andrea Giovanni Nuzzolese, Valentina Presutti', 'link': 'https://arxiv.org/abs/2505.24554', 'abstract': 'The availability of Large Language Models (LLMs) presents a unique opportunity to reinvigorate research on Knowledge Engineering (KE) automation, a trend already evident in recent efforts developing LLM-based methods and tools for the automatic generation of Competency Questions (CQs). However, the evaluation of these tools lacks standardisation. This undermines the methodological rigour and hinders the replication and comparison of results. To address this gap, we introduce Bench4KE, an extensible API-based benchmarking system for KE automation. Its first release focuses on evaluating tools that generate CQs automatically. CQs are natural language questions used by ontology engineers to define the functional requirements of an ontology. Bench4KE provides a curated gold standard consisting of CQ datasets from four real-world ontology projects. It uses a suite of similarity metrics to assess the quality of the CQs generated. We present a comparative analysis of four recent CQ generation systems, which are based on LLMs, establishing a baseline for future research. Bench4KE is also designed to accommodate additional KE automation tasks, such as SPARQL query generation, ontology testing and drafting. Code and datasets are publicly available under the Apache 2.0 license.', 'abstract_zh': '大型语言模型（LLMs）的可用性为知识工程（KE）自动化研究带来了独特的机会，这一趋势已在基于LLM的方法和工具开发中生成能力问题（CQs）的自动生成中显现出来。然而，这些工具的评估缺乏标准化，这削弱了方法论的严谨性，并阻碍了结果的重复和比较。为了解决这一缺口，我们引入了Bench4KE，这是一个可扩展的API基础基准系统，用于KE自动化评估。其首次发布专注于评估自动生成CQs的工具。CQs是本体工程师用来定义本体功能需求的自然语言问题。Bench4KE提供了一个经过策展的黄金标准，包括来自四个实际本体项目的数据集，并使用一系列相似性度量来评估生成的CQs的质量。我们对四个基于LLM的最新CQ生成系统进行了比较分析，建立了未来研究的基础。Bench4KE还设计用于容纳其他KE自动化任务，如SPARQL查询生成、本体测试和起草。代码和数据集在Apache 2.0许可下公开。', 'title_zh': 'Bench4KE: 自动化能力问题生成评估基准'}
{'arxiv_id': 'arXiv:2505.24553', 'title': 'CREFT: Sequential Multi-Agent LLM for Character Relation Extraction', 'authors': 'Ye Eun Chun, Taeyoon Hwang, Seung-won Hwang, Byung-Hak Kim', 'link': 'https://arxiv.org/abs/2505.24553', 'abstract': 'Understanding complex character relations is crucial for narrative analysis and efficient script evaluation, yet existing extraction methods often fail to handle long-form narratives with nuanced interactions. To address this challenge, we present CREFT, a novel sequential framework leveraging specialized Large Language Model (LLM) agents. First, CREFT builds a base character graph through knowledge distillation, then iteratively refines character composition, relation extraction, role identification, and group assignments. Experiments on a curated Korean drama dataset demonstrate that CREFT significantly outperforms single-agent LLM baselines in both accuracy and completeness. By systematically visualizing character networks, CREFT streamlines narrative comprehension and accelerates script review -- offering substantial benefits to the entertainment, publishing, and educational sectors.', 'abstract_zh': '理解复杂的人物关系对于叙事分析和高效的剧本评估至关重要，然而现有的提取方法往往无法处理包含细腻互动的长篇叙事。为此，我们提出CREFT，一种利用专业大型语言模型代理的新型序列框架。首先，CREFT通过知识蒸馏构建基础人物图，然后迭代优化人物组成、关系提取、角色识别和分组分配。在精心选择的韩剧数据集上的实验表明，CREFT在准确性和完整性方面显著优于单代理大型语言模型基线。通过系统可视化人物网络，CREFT简化了叙事理解并加速了剧本审核——为娱乐、出版和教育领域带来了重大益处。', 'title_zh': 'CREFT: 顺序多agents语言模型用于角色关系提取'}
{'arxiv_id': 'arXiv:2505.24544', 'title': 'Cross-Attention Speculative Decoding', 'authors': 'Wei Zhong, Manasa Bharadwaj, Yixiao Wang, Nikhil Verma, Yipeng Ji, Chul Lee', 'link': 'https://arxiv.org/abs/2505.24544', 'abstract': 'Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly coupled, self-attention-based Transformer decoders, often augmented with auxiliary pooling or fusion layers. This coupling makes them increasingly complex and harder to generalize across different models. We present Budget EAGLE (Beagle), the first, to our knowledge, cross-attention-based Transformer decoder SD model that achieves performance on par with leading self-attention SD models (EAGLE-v2) while eliminating the need for pooling or auxiliary components, simplifying the architecture, improving training efficiency, and maintaining stable memory usage during training-time simulation. To enable effective training of this novel architecture, we propose Two-Stage Block-Attention Training, a new method that achieves training stability and convergence efficiency in block-level attention scenarios. Extensive experiments across multiple LLMs and datasets show that Beagle achieves competitive inference speedups and higher training efficiency than EAGLE-v2, offering a strong alternative for architectures in speculative decoding.', 'abstract_zh': 'Budget EAGLE：一种跨注意力机制的推测解码Transformer解码器模型及其训练方法', 'title_zh': '跨注意力推测解码'}
{'arxiv_id': 'arXiv:2505.24541', 'title': 'Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts', 'authors': 'Xin He, Xumeng Han, Longhui Wei, Lingxi Xie, Qi Tian', 'link': 'https://arxiv.org/abs/2505.24541', 'abstract': 'Multimodal large language models (MLLMs) require a nuanced interpretation of complex image information, typically leveraging a vision encoder to perceive various visual scenarios. However, relying solely on a single vision encoder to handle diverse task domains proves difficult and inevitably leads to conflicts. Recent work enhances data perception by directly integrating multiple domain-specific vision encoders, yet this structure adds complexity and limits the potential for joint optimization. In this paper, we introduce Mixpert, an efficient mixture-of-vision-experts architecture that inherits the joint learning advantages from a single vision encoder while being restructured into a multi-expert paradigm for task-specific fine-tuning across different visual tasks. Additionally, we design a dynamic routing mechanism that allocates input images to the most suitable visual expert. Mixpert effectively alleviates domain conflicts encountered by a single vision encoder in multi-task learning with minimal additional computational cost, making it more efficient than multiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM, with experimental results demonstrating substantial performance gains across various tasks.', 'abstract_zh': '多模态大型语言模型的混合视觉专家架构：Mixpert', 'title_zh': 'Mixpert: 以高效视觉专家混合方式缓解多模态学习冲突'}
{'arxiv_id': 'arXiv:2505.24539', 'title': 'Localizing Persona Representations in LLMs', 'authors': 'Celia Cintas, Miriam Rateike, Erik Miehling, Elizabeth Daly, Skyler Speakman', 'link': 'https://arxiv.org/abs/2505.24539', 'abstract': 'We present a study on how and where personas -- defined by distinct sets of human characteristics, values, and beliefs -- are encoded in the representation space of large language models (LLMs). Using a range of dimension reduction and pattern recognition methods, we first identify the model layers that show the greatest divergence in encoding these representations. We then analyze the activations within a selected layer to examine how specific personas are encoded relative to others, including their shared and distinct embedding spaces. We find that, across multiple pre-trained decoder-only LLMs, the analyzed personas show large differences in representation space only within the final third of the decoder layers. We observe overlapping activations for specific ethical perspectives -- such as moral nihilism and utilitarianism -- suggesting a degree of polysemy. In contrast, political ideologies like conservatism and liberalism appear to be represented in more distinct regions. These findings help to improve our understanding of how LLMs internally represent information and can inform future efforts in refining the modulation of specific human traits in LLM outputs. Warning: This paper includes potentially offensive sample statements.', 'abstract_zh': '我们研究了如何以及在何处通过不同的心理健康特征、价值观和信念（定义为人物角色）在大型语言模型（LLMs）的表示空间中进行编码。通过一系列降维和模式识别方法，我们首先确定表现出最大编码差异的模型层。然后，我们分析选定层内的激活，以检查特定人物角色相对于其他人物角色的编码情况，包括它们的共享和独特的嵌入空间。我们发现，在多个预训练的解码器型大型语言模型中，分析的人物角色在解码器层的最后三分之一中显示出较大的表示空间差异。我们观察到特定伦理视角（如道德虚无主义和功利主义）的重叠激活，表明一定程度的多义性。相比之下，政治意识形态（如保守主义和自由主义）似乎在更不同的区域得到表示。这些发现有助于我们更好地理解LLMs如何内部表示信息，并可为未来努力细化LLM输出中特定人类特质的调节提供指导。警告：本论文包括可能具有冒犯性的样本陈述。', 'title_zh': 'LLMs中个性表示的定位'}
{'arxiv_id': 'arXiv:2505.24536', 'title': 'CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control', 'authors': 'Chaohui Xu, Qi Cui, Chip-Hong Chang', 'link': 'https://arxiv.org/abs/2505.24536', 'abstract': 'The pervasion of large-scale Deep Neural Networks (DNNs) and their enormous training costs make their intellectual property (IP) protection of paramount importance. Recently introduced passport-based methods attempt to steer DNN watermarking towards strengthening ownership verification against ambiguity attacks by modulating the affine parameters of normalization layers. Unfortunately, neither watermarking nor passport-based methods provide a holistic protection with robust ownership proof, high fidelity, active usage authorization and user traceability for offline access distributed models and multi-user Machine-Learning as a Service (MLaaS) cloud model. In this paper, we propose a Chameleon Hash-based Irreversible Passport (CHIP) protection framework that utilizes the cryptographic chameleon hash function to achieve all these goals. The collision-resistant property of chameleon hash allows for strong model ownership claim upon IP infringement and liable user traceability, while the trapdoor-collision property enables hashing of multiple user passports and licensee certificates to the same immutable signature to realize active usage control. Using the owner passport as an oracle, multiple user-specific triplets, each contains a passport-aware user model, a user passport, and a licensee certificate can be created for secure offline distribution. The watermarked master model can also be deployed for MLaaS with usage permission verifiable by the provision of any trapdoor-colliding user passports. CHIP is extensively evaluated on four datasets and two architectures to demonstrate its protection versatility and robustness. Our code is released at this https URL.', 'abstract_zh': '基于可逆хи曼散列的护照保护框架：用于大规模深度神经网络的综合保护', 'title_zh': 'CHIP: 铳龙哈希基不可逆通行证用于 robust 深度模型所有权验证及主动使用控制'}
{'arxiv_id': 'arXiv:2505.24535', 'title': 'Beyond Linear Steering: Unified Multi-Attribute Control for Language Models', 'authors': 'Narmeen Oozeer, Luke Marks, Fazl Barez, Amirali Abdullah', 'link': 'https://arxiv.org/abs/2505.24535', 'abstract': 'Controlling multiple behavioral attributes in large language models (LLMs) at inference time is a challenging problem due to interference between attributes and the limitations of linear steering methods, which assume additive behavior in activation space and require per-attribute tuning. We introduce K-Steering, a unified and flexible approach that trains a single non-linear multi-label classifier on hidden activations and computes intervention directions via gradients at inference time. This avoids linearity assumptions, removes the need for storing and tuning separate attribute vectors, and allows dynamic composition of behaviors without retraining. To evaluate our method, we propose two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral control. Empirical results across 3 model families, validated by both activation-based classifiers and LLM-based judges, demonstrate that K-Steering outperforms strong baselines in accurately steering multiple behaviors.', 'abstract_zh': '在推断时控制大型语言模型的多种行为属性是一个具有挑战性的问题，由于属性之间的相互干扰以及线性导向方法的限制，这些方法基于激活空间中的加性行为假设，并且需要为每个属性进行调优。我们提出了K-Steering，这是一种统一且灵活的方法，通过在隐藏激活上训练单个多标签非线性分类器，并在推断时通过梯度计算干预方向，从而避免了线性假设、消除了存储和调优单独属性向量的需要，并允许在无需重新训练的情况下动态组合行为。为了评估该方法，我们提出了两个新的基准测试，ToneBank和DebateMix，旨在实现组合行为控制。来自3个模型系列的实验结果，通过基于激活的分类器和基于大型语言模型的评估员验证，表明K-Steering在准确引导多种行为方面优于强基线。', 'title_zh': '超越线性 steering：统一的语言模型多属性控制'}
{'arxiv_id': 'arXiv:2505.24533', 'title': 'Directional Non-Commutative Monoidal Structures with Interchange Law via Commutative Generators', 'authors': 'Mahesh Godavarti', 'link': 'https://arxiv.org/abs/2505.24533', 'abstract': 'We introduce a novel framework consisting of a class of algebraic structures that generalize one-dimensional monoidal systems into higher dimensions by defining per-axis composition operators subject to non-commutativity and a global interchange law. These structures, defined recursively from a base case of vector-matrix pairs, model directional composition in multiple dimensions while preserving structural coherence through commutative linear operators.\nWe show that the framework that unifies several well-known linear transforms in signal processing and data analysis. In this framework, data indices are embedded into a composite structure that decomposes into simpler components. We show that classic transforms such as the Discrete Fourier Transform (DFT), the Walsh transform, and the Hadamard transform are special cases of our algebraic structure. The framework provides a systematic way to derive these transforms by appropriately choosing vector and matrix pairs. By subsuming classical transforms within a common structure, the framework also enables the development of learnable transformations tailored to specific data modalities and tasks.', 'abstract_zh': '我们提出了一种新颖的框架，该框架包含一类代数结构，通过定义轴向组合操作并满足非交换性和全局交换定律，将一维单调系统推广到高维。这些结构从向量-矩阵对的基本情况进行递归定义，模型多维方向上的组合保持结构性上的连贯性，并通过交换的线性操作来实现。\n\n该框架统一了信号处理和数据分析中多种已知的线性变换。在此框架中，数据索引嵌入到可分解为更简单组件的复合结构中。我们证明经典的变换，如离散傅里叶变换（DFT）、沃尔什变换和哈达amard变换，都是我们代数结构的特殊情况。该框架提供了一种系统的方法，通过适当地选择向量和矩阵对来推导这些变换。通过在一个共同结构下包含经典变换，该框架还使针对特定数据模态和任务的可学习变换的发展成为可能。', 'title_zh': '基于交换生成元的方向非交换乘方结构与交換律'}
{'arxiv_id': 'arXiv:2505.24523', 'title': 'Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors', 'authors': "Andrea Pedrotti, Michele Papucci, Cristiano Ciaccio, Alessio Miaschi, Giovanni Puccetti, Felice Dell'Orletta, Andrea Esuli", 'link': 'https://arxiv.org/abs/2505.24523', 'abstract': "Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts.", 'abstract_zh': '最近生成人工智能和大型语言模型的进步使得创建高度真实的合成内容成为可能，这引起了关于恶意使用潜在风险的担忧，如 misinformation 和操纵。此外，由于缺乏评估现实世界场景泛化能力的稳健基准，检测机器生成文本（MGT）仍然是一个挑战。在本工作中，我们提出了一种测试最先进的 MGT 检测器（如 Mage、Radar、LLM-DetectAIve）对语言导向的对抗攻击的鲁棒性的流水线。为了挑战检测器，我们使用直接偏好优化（DPO）调整语言模型，使 MGT 的风格趋向于人类撰写文本（HWT）。这利用了检测器对风格线索的依赖性，使新的生成内容更难被检测。此外，我们分析了对齐过程中引起的语言变化以及检测器检测 MGT 文本时所使用的特征。我们的结果显示，检测器可以通过相对少量的示例被轻易欺骗，导致检测性能显著下降。这一结果强调了改进检测方法并使其能够应对未见过领域的文本的重要性。', 'title_zh': '机器生成文本检测的压力测试：使语言模型改变写作风格以欺骗检测器'}
{'arxiv_id': 'arXiv:2505.24511', 'title': 'Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting', 'authors': 'Jiahao Wang, Mingyue Cheng, Qi Liu', 'link': 'https://arxiv.org/abs/2505.24511', 'abstract': 'Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.', 'abstract_zh': '时间序列 forecasting 中慢思考大语言模型的推理能力探究', 'title_zh': '慢思考的LLM能否进行时间推理？时间序列预测的实证研究'}
{'arxiv_id': 'arXiv:2505.24503', 'title': 'Online Fair Division with Additional Information', 'authors': 'Tzeh Yuan Neoh, Jannik Peters, Nicholas Teh', 'link': 'https://arxiv.org/abs/2505.24503', 'abstract': "We study the problem of fairly allocating indivisible goods to agents in an online setting, where goods arrive sequentially and must be allocated irrevocably to agents. Focusing on the popular fairness notions of envy-freeness, proportionality, and maximin share fairness (and their approximate variants), we ask how the availability of information on future goods influences the existence and approximability of fair allocations. In the absence of any such information, we establish strong impossibility results, demonstrating the inherent difficulty of achieving even approximate fairness guarantees. In contrast, we demonstrate that knowledge of additional information -- such as aggregate of each agent's total valuations (equivalently, normalized valuations) or the multiset of future goods values (frequency predictions) -- would enable the design of fairer online algorithms. Given normalization information, we propose an algorithm that achieves stronger fairness guarantees than previously known results. Given frequency predictions, we introduce a meta-algorithm that leverages frequency predictions to match the best-known offline guarantees for a broad class of ''share-based'' fairness notions. Our complementary impossibility results in each setting underscore both the limitations imposed by uncertainty about future goods and the potential of leveraging structured information to achieve fairer outcomes in online fair division.", 'abstract_zh': '在线环境中不可分物品公平分配的问题：未来物品信息获取对公平分配存在性和近似性的影响', 'title_zh': '在线公平分割问题中的额外信息应用'}
{'arxiv_id': 'arXiv:2505.24500', 'title': "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence", 'authors': 'Guiyang Hou, Xing Gao, Yuchuan Wu, Xiang Huang, Wenqi Zhang, Zhe Zheng, Yongliang Shen, Jialu Du, Fei Huang, Yongbin Li, Weiming Lu', 'link': 'https://arxiv.org/abs/2505.24500', 'abstract': "Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.", 'abstract_zh': "Recent Progress and Underexplored Areas in Enhancing Large Language Models' Social Intelligence: Introducing Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL)", 'title_zh': 'TimeHC-RL: 具有时间意识的分层认知强化学习提升LLMs的社会智能'}
{'arxiv_id': 'arXiv:2505.24492', 'title': 'Object Centric Concept Bottlenecks', 'authors': 'David Steinmann, Wolfgang Stammer, Antonia Wüst, Kristian Kersting', 'link': 'https://arxiv.org/abs/2505.24492', 'abstract': 'Developing high-performing, yet interpretable models remains a critical challenge in modern AI. Concept-based models (CBMs) attempt to address this by extracting human-understandable concepts from a global encoding (e.g., image encoding) and then applying a linear classifier on the resulting concept activations, enabling transparent decision-making. However, their reliance on holistic image encodings limits their expressiveness in object-centric real-world settings and thus hinders their ability to solve complex vision tasks beyond single-label classification. To tackle these challenges, we introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines the strengths of CBMs and pre-trained object-centric foundation models, boosting performance and interpretability. We evaluate OCB on complex image datasets and conduct a comprehensive ablation study to analyze key components of the framework, such as strategies for aggregating object-concept encodings. The results show that OCB outperforms traditional CBMs and allows one to make interpretable decisions for complex visual tasks.', 'abstract_zh': '基于对象中心的概念瓶颈以提升性能与可解释性', 'title_zh': '以对象为中心的概念瓶颈'}
{'arxiv_id': 'arXiv:2505.24489', 'title': 'Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing', 'authors': 'Anasse Boutayeb, Iyad Lahsen-cherif, Ahmed El Khadimi', 'link': 'https://arxiv.org/abs/2505.24489', 'abstract': 'Object detection has recently seen an interesting trend in terms of the most innovative research work, this task being of particular importance in the field of remote sensing, given the consistency of these images in terms of geographical coverage and the objects present. Furthermore, Deep Learning (DL) models, in particular those based on Transformers, are especially relevant for visual computing tasks in general, and target detection in particular. Thus, the present work proposes an application of Deformable-DETR model, a specific architecture using deformable attention mechanisms, on remote sensing images in two different modes, especially optical and Synthetic Aperture Radar (SAR). To achieve this objective, two datasets are used, one optical, which is Pleiades Aircraft dataset, and the other SAR, in particular SAR Ship Detection Dataset (SSDD). The results of a 10-fold stratified validation showed that the proposed model performed particularly well, obtaining an F1 score of 95.12% for the optical dataset and 94.54% for SSDD, while comparing these results with several models detections, especially those based on CNNs and transformers, as well as those specifically designed to detect different object classes in remote sensing images.', 'abstract_zh': '遥感图像中基于变形Transformer的物体检测应用：光学与SAR模式下的性能评估', 'title_zh': '可变形注意力机制在遥感目标检测中的应用'}
{'arxiv_id': 'arXiv:2505.24486', 'title': 'Rehearsal with Auxiliary-Informed Sampling for Audio Deepfake Detection', 'authors': 'Falih Gozi Febrinanto, Kristen Moore, Chandra Thapa, Jiangang Ma, Vidya Saikrishna, Feng Xia', 'link': 'https://arxiv.org/abs/2505.24486', 'abstract': "The performance of existing audio deepfake detection frameworks degrades when confronted with new deepfake attacks. Rehearsal-based continual learning (CL), which updates models using a limited set of old data samples, helps preserve prior knowledge while incorporating new information. However, existing rehearsal techniques don't effectively capture the diversity of audio characteristics, introducing bias and increasing the risk of forgetting. To address this challenge, we propose Rehearsal with Auxiliary-Informed Sampling (RAIS), a rehearsal-based CL approach for audio deepfake detection. RAIS employs a label generation network to produce auxiliary labels, guiding diverse sample selection for the memory buffer. Extensive experiments show RAIS outperforms state-of-the-art methods, achieving an average Equal Error Rate (EER) of 1.953 % across five experiences. The code is available at: this https URL.", 'abstract_zh': '现有的音频深度伪造检测框架在面对新的深度伪造攻击时性能下降。基于回顾的持续学习（CL）通过使用有限的老数据样本更新模型，可以在保留先前知识的同时融入新信息。然而，现有的回顾技术无法有效地捕捉音频特征的多样性，引入了偏差并增加了遗忘的风险。为了解决这一挑战，我们提出了一种基于回顾的音频深度伪造检测持续学习方法——辅助指导采样回顾（RAIS）。RAIS 使用标签生成网络生成辅助标签，指导内存缓冲区中多样样本的选择。广泛实验表明，RAIS 在五个场景中均优于最新方法，平均等错误率（EER）为 1.953%。代码可在以下链接获取：this https URL。', 'title_zh': '基于辅助信息采样的音频深度假声检测复述方法'}
{'arxiv_id': 'arXiv:2505.24480', 'title': 'Towards Effective Code-Integrated Reasoning', 'authors': 'Fei Bai, Yingqian Min, Beichen Zhang, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2505.24480', 'abstract': "In this paper, we investigate code-integrated reasoning, where models generate code when necessary and integrate feedback by executing it through a code interpreter. To acquire this capability, models must learn when and how to use external code tools effectively, which is supported by tool-augmented reinforcement learning (RL) through interactive learning. Despite its benefits, tool-augmented RL can still suffer from potential instability in the learning dynamics. In light of this challenge, we present a systematic approach to improving the training effectiveness and stability of tool-augmented RL for code-integrated reasoning. Specifically, we develop enhanced training strategies that balance exploration and stability, progressively building tool-use capabilities while improving reasoning performance. Through extensive experiments on five mainstream mathematical reasoning benchmarks, our model demonstrates significant performance improvements over multiple competitive baselines. Furthermore, we conduct an in-depth analysis of the mechanism and effect of code-integrated reasoning, revealing several key insights, such as the extension of model's capability boundaries and the simultaneous improvement of reasoning efficiency through code integration. All data and code for reproducing this work are available at: this https URL.", 'abstract_zh': '在本文中，我们研究了代码集成推理，其中模型在必要时生成代码，并通过代码解释器执行代码以整合反馈。为了获得这一能力，模型必须学会何时以及如何有效地使用外部代码工具，这通过交互学习支持基于工具增强的强化学习（RL）。尽管这种方法有许多优势，但工具增强的RL仍然可能在学习动态中遭受潜在的不稳定问题。针对这一挑战，我们提出了一种系统方法，以提高工具增强的RL在代码集成推理中的训练效果和稳定性。具体地，我们开发了平衡探索与稳定性的增强训练策略，在逐步建立工具使用能力的同时提升推理性能。通过在五个主流数学推理基准上的广泛实验，我们的模型在多个竞争性基线上显示出显著的性能提升。此外，我们深入分析了代码集成推理的机制和效果，揭示了几个关键见解，如模型能力边界的扩展以及通过代码集成同时提高推理效率。所有再现本文工作的数据和代码可在此处获得：this https URL。', 'title_zh': '面向有效的代码集成推理'}
{'arxiv_id': 'arXiv:2505.24477', 'title': 'Evaluating Gemini in an arena for learning', 'authors': 'LearnLM Team Google, Abhinit Modi, Aditya Srikanth Veerubhotla, Aliya Rysbek, Andrea Huber, Ankit Anand, Avishkar Bhoopchand, Brett Wiltshire, Daniel Gillick, Daniel Kasenberg, Eleni Sgouritsa, Gal Elidan, Hengrui Liu, Holger Winnemoeller, Irina Jurenka, James Cohan, Jennifer She, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Komal Singh, Lisa Wang, Markus Kunesch, Miruna Pîslar, Niv Efron, Parsa Mahmoudieh, Pierre-Alexandre Kamienny, Sara Wiltberger, Shakir Mohamed, Shashank Agarwal, Shubham Milind Phal, Sun Jae Lee, Theofilos Strinopoulos, Wei-Jen Ko, Yael Gold-Zamir, Yael Haramaty, Yannis Assael', 'link': 'https://arxiv.org/abs/2505.24477', 'abstract': 'Artificial intelligence (AI) is poised to transform education, but the research community lacks a robust, general benchmark to evaluate AI models for learning. To assess state-of-the-art support for educational use cases, we ran an "arena for learning" where educators and pedagogy experts conduct blind, head-to-head, multi-turn comparisons of leading AI models. In particular, $N = 189$ educators drew from their experience to role-play realistic learning use cases, interacting with two models sequentially, after which $N = 206$ experts judged which model better supported the user\'s learning goals. The arena evaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7 Sonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro in 73.2% of these match-ups -- ranking it first overall in the arena. Gemini 2.5 Pro also demonstrated markedly higher performance across key principles of good pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading model for learning.', 'abstract_zh': '人工智能（AI）有望革新教育，但研究社区缺乏一个 robust、通用的基准来评估AI模型在学习中的应用。为了评估最先进的支持教育用途的技术，我们举办了一场“学习竞技场”，其中教育者和教学专家进行盲测、一对一、多轮次的领先AI模型对比。特别地，来自经验的189名教育者扮演了现实的学习场景，与两个模型依次交互，之后206名专家判断哪个模型更好地支持了用户的学习目标。竞技场评估了多个最先进的模型：Gemini 2.5 Pro、Claude 3.7 Sonnet、GPT-4o和OpenAI o3。不包括平局，在这些对战中，专家们更偏好Gemini 2.5 Pro，其比例高达73.2%，使其在竞技场中排名第一。Gemini 2.5 Pro还显著表现出更高的教学原则应用水平。这些结果使Gemini 2.5 Pro成为领先的學習模型。', 'title_zh': '评估Gemini在学习竞技场中的表现'}
{'arxiv_id': 'arXiv:2505.24473', 'title': 'Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy', 'authors': 'Nikita Balagansky, Yaroslav Aksenov, Daniil Laptev, Vadim Kurochkin, Gleb Gerasimov, Nikita Koryagin, Daniil Gavrilov', 'link': 'https://arxiv.org/abs/2505.24473', 'abstract': 'Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting neural networks by decomposing hidden representations into disentangled, interpretable features via sparsity constraints. However, conventional SAEs are constrained by the fixed sparsity level chosen during training; meeting different sparsity requirements therefore demands separate models and increases the computational footprint during both training and evaluation. We introduce a novel training objective, \\emph{HierarchicalTopK}, which trains a single SAE to optimise reconstructions across multiple sparsity levels simultaneously. Experiments with Gemma-2 2B demonstrate that our approach achieves Pareto-optimal trade-offs between sparsity and explained variance, outperforming traditional SAEs trained at individual sparsity levels. Further analysis shows that HierarchicalTopK preserves high interpretability scores even at higher sparsity. The proposed objective thus closes an important gap between flexibility and interpretability in SAE design.', 'abstract_zh': '层次TopK训练目标HierarchicalTopK：实现稀疏自编码器SAE在不同稀疏性水平下的最优 TRADE-OFF', 'title_zh': '在多个稀疏性预算下训练一个稀疏自编码器以保留可解释性和准确性'}
{'arxiv_id': 'arXiv:2505.24472', 'title': 'VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation', 'authors': 'Hieu Tran, Phuong-Anh Nguyen-Le, Huy Nghiem, Quang-Nhan Nguyen, Wei Ai, Marine Carpuat', 'link': 'https://arxiv.org/abs/2505.24472', 'abstract': "Machine translation systems fail when processing code-mixed inputs for low-resource languages. We address this challenge by curating VietMix, a parallel corpus of naturally occurring code-mixed Vietnamese text paired with expert English translations. Augmenting this resource, we developed a complementary synthetic data generation pipeline. This pipeline incorporates filtering mechanisms to ensure syntactic plausibility and pragmatic appropriateness in code-mixing patterns. Experimental validation shows our naturalistic and complementary synthetic data boost models' performance, measured by translation quality estimation scores, of up to 71.84 on COMETkiwi and 81.77 on XCOMET. Triangulating positive results with LLM-based assessments, augmented models are favored over seed fine-tuned counterparts in approximately 49% of judgments (54-56% excluding ties). VietMix and our augmentation methodology advance ecological validity in neural MT evaluations and establish a framework for addressing code-mixed translation challenges across other low-resource pairs.", 'abstract_zh': '低资源语言代码混用输入下机器翻译系统的失败：通过构建VietMix平行语料库及其辅助合成数据生成管道予以应对', 'title_zh': 'VietMix：一种迭代扩增的自然产生的越南语-英语代码混用语料库及其在机器翻译中的应用'}
{'arxiv_id': 'arXiv:2505.24451', 'title': 'LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs', 'authors': 'Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux', 'link': 'https://arxiv.org/abs/2505.24451', 'abstract': "Large Language Models (LLMs) are being extensively used for cybersecurity purposes. One of them is the detection of vulnerable codes. For the sake of efficiency and effectiveness, compression and fine-tuning techniques are being developed, respectively. However, they involve spending substantial computational efforts. In this vein, we analyse how Linear Probes (LPs) can be used to provide an estimation on the performance of a compressed LLM at an early phase -- before fine-tuning. We also show their suitability to set the cut-off point when applying layer pruning compression. Our approach, dubbed $LPASS$, is applied in BERT and Gemma for the detection of 12 of MITRE's Top 25 most dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in 142.97 s. and provide key findings: (1) 33.3 \\% and 72.2\\% of layers can be removed, respectively, with no precision loss; (2) they provide an early estimate of the post-fine-tuning and post-compression model effectiveness, with 3\\% and 8.68\\% as the lowest and average precision errors, respectively. $LPASS$-based LLMs outperform the state of the art, reaching 86.9\\% of accuracy in multi-class vulnerability detection. Interestingly, $LPASS$-based compressed versions of Gemma outperform the original ones by 1.6\\% of F1-score at a maximum while saving 29.4 \\% and 23.8\\% of training and inference time and 42.98\\% of model size.", 'abstract_zh': '大型语言模型（LLMs）在网络安全领域得到了广泛的应用，其中之一是检测脆弱的代码。为了提高效率和效果，正在开发压缩和微调技术，但这些技术需要大量的计算资源。为此，我们分析了线性探测（LPs）如何在微调之前的一个早期阶段提供压缩LLM性能的估计。我们还展示了它们在应用层剪枝压缩时作为截止点设置的适用性。我们的方法称为$LPASS$，在BERT和Gemma上对480,000个C/C++样本中的MITRE TOP 25最具危险性的12种漏洞进行了检测。LPs可以在142.97秒内计算出关键发现：（1）可以分别去除33.3%和72.2%的层而不会丢失精度；（2）它们可以提前估计微调后和压缩后模型的效果，精度误差分别为3%和8.68%。基于$LPASS$的LLMs在多类漏洞检测方面超越了最先进的方法，达到86.9%的准确率。有趣的是，基于$LPASS$压缩的Gemma版本在F1分数上最高可优于原版1.6%，同时节省了29.4%的训练和推理时间以及42.98%的模型大小。', 'title_zh': 'LPASS: 线性探针作为使用压缩大语言模型进行漏洞检测的 stepping stones'}
{'arxiv_id': 'arXiv:2505.24445', 'title': 'Learning Safety Constraints for Large Language Models', 'authors': 'Xin Chen, Yarden As, Andreas Krause', 'link': 'https://arxiv.org/abs/2505.24445', 'abstract': "Large language models (LLMs) have emerged as powerful tools but pose significant safety risks through harmful outputs and vulnerability to adversarial attacks. We propose SaP, short for Safety Polytope, a geometric approach to LLM safety that learns and enforces multiple safety constraints directly in the model's representation space. We develop a framework that identifies safe and unsafe regions via the polytope's facets, enabling both detection and correction of unsafe outputs through geometric steering. Unlike existing approaches that modify model weights, SaP operates post-hoc in the representation space, preserving model capabilities while enforcing safety constraints. Experiments across multiple LLMs demonstrate that our method can effectively detect unethical inputs, reduce adversarial attack success rates while maintaining performance on standard tasks, thus highlighting the importance of having an explicit geometric model for safety. Analysis of the learned polytope facets reveals emergence of specialization in detecting different semantic notions of safety, providing interpretable insights into how safety is captured in LLMs' representation space.", 'abstract_zh': '大型语言模型（LLMs）作为强大的工具，但通过有害输出和对抗攻击的脆弱性，带来了显著的安全风险。我们提出Safety Polytope（安全多面体，简称SaP）作为一种几何方法，直接在模型的表示空间中学习和执行多重安全约束，以保障LLM安全。我们开发了一个框架，通过多面体的面来识别安全和不安全的区域，从而通过几何引导来检测和纠正不安全的输出。与修改模型权重的现有方法不同，SaP在表示空间中事后操作，保留模型能力的同时强制执行安全约束。在多个LLM上的实验表明，我们的方法可以有效检测不道德的输入、降低对抗攻击的成功率同时保持标准任务的性能，从而突显了拥有明确几何模型的重要性。分析学习到的多面体面揭示了不同语义层面的安全检测的专业化现象，提供了在LLM表示空间中如何捕获安全性的可解释见解。', 'title_zh': '为大型语言模型学习安全约束'}
{'arxiv_id': 'arXiv:2505.24429', 'title': 'Deep Learning Weather Models for Subregional Ocean Forecasting: A Case Study on the Canary Current Upwelling System', 'authors': 'Giovanny C-Londoño, Javier Sánchez, Ángel Rodríguez-Santana', 'link': 'https://arxiv.org/abs/2505.24429', 'abstract': 'Oceanographic forecasting impacts various sectors of society by supporting environmental conservation and economic activities. Based on global circulation models, traditional forecasting methods are computationally expensive and slow, limiting their ability to provide rapid forecasts. Recent advances in deep learning offer faster and more accurate predictions, although these data-driven models are often trained with global data from numerical simulations, which may not reflect reality. The emergence of such models presents great potential for improving ocean prediction at a subregional domain. However, their ability to predict fine-scale ocean processes, like mesoscale structures, remains largely unknown. This work aims to adapt a graph neural network initially developed for global weather forecasting to improve subregional ocean prediction, specifically focusing on the Canary Current upwelling system. The model is trained with satellite data and compared to state-of-the-art physical ocean models to assess its performance in capturing ocean dynamics. Our results show that the deep learning model surpasses traditional methods in precision despite some challenges in upwelling areas. It demonstrated superior performance in reducing RMSE errors compared to ConvLSTM and the GLORYS reanalysis, particularly in regions with complex oceanic dynamics such as Cape Ghir, Cape Bojador, and Cape Blanc. The model achieved improvements of up to 26.5% relative to ConvLSTM and error reductions of up to 76% in 5-day forecasts compared to the GLORYS reanalysis at these critical locations, highlighting its enhanced capability to capture spatial variability and improve predictive accuracy in complex areas. These findings suggest the viability of adapting meteorological data-driven models for improving subregional medium-term ocean forecasting.', 'abstract_zh': '海洋学预报影响社会的各个领域，通过支持环境保护和经济活动。基于全球环流模型的传统预报方法计算成本高且速度慢，限制了其提供快速预报的能力。最近深度学习的发展提供了更快更准确的预测，尽管这些数据驱动的模型通常使用来自数值模拟的全球数据进行训练，这可能无法反映实际情况。这些模型的出现为改善区域海洋预报提供了巨大潜力。然而，它们在预测如中尺度结构等精细海洋过程的能力尚不清楚。本研究旨在将最初用于全球天气预报的图神经网络适应于改善区域海洋预报，特别聚焦于加那利洋流上升流系统。该模型使用卫星数据进行训练，并与最先进的物理海洋模型进行比较，以评估其在捕捉海洋动力学方面的性能。结果显示，深度学习模型在精度方面超过了传统方法，尽管在上升流地区存在一些挑战。它在减少RMSE误差方面优于ConvLSTM和GLORYS再分析，特别是在如盖希尔角、波多贾勒角和白朗角等海洋动力学复杂的区域表现尤为突出。模型在这些关键地点5天预报中的相对改进高达26.5%，GLORYS再分析误差减少了高达76%，表明其在复杂区域捕捉空间变异性和提高预测准确性的增强能力。这些发现表明，气象数据驱动模型的区域中期海洋预报能够实现可行的适应。', 'title_zh': '基于深度学习的区域海洋预报： Canary 流动上升系统案例研究'}
{'arxiv_id': 'arXiv:2505.24415', 'title': 'Boosting Automatic Exercise Evaluation Through Musculoskeletal Simulation-Based IMU Data Augmentation', 'authors': 'Andreas Spilz, Heiko Oppel, Michael Munz', 'link': 'https://arxiv.org/abs/2505.24415', 'abstract': 'Automated evaluation of movement quality holds significant potential for enhancing physiotherapeutic treatments and sports training by providing objective, real-time feedback. However, the effectiveness of deep learning models in assessing movements captured by inertial measurement units (IMUs) is often hampered by limited data availability, class imbalance, and label ambiguity. In this work, we present a novel data augmentation method that generates realistic IMU data using musculoskeletal simulations integrated with systematic modifications of movement trajectories. Crucially, our approach ensures biomechanical plausibility and allows for automatic, reliable labeling by combining inverse kinematic parameters with a knowledge-based evaluation strategy. Extensive evaluations demonstrate that augmented variants closely resembles real-world data, significantly improving the classification accuracy and generalization capability of neural network models. Additionally, we highlight the benefits of augmented data for patient-specific fine-tuning scenarios, particularly when only limited subject-specific training examples are available. Our findings underline the practicality and efficacy of this augmentation method in overcoming common challenges faced by deep learning applications in physiotherapeutic exercise evaluation.', 'abstract_zh': '自动评估运动质量在增强物理治疗和体育训练方面具有显著潜力，通过提供客观的实时反馈。然而，深度学习模型在评估惯性测量单元（IMU）捕捉的运动时，常常受限于数据不足、类别不平衡和标签模糊。在本工作中，我们提出了一种新颖的数据增强方法，该方法使用结合系统运动轨迹修改的肌肉骨骼模拟生成现实的IMU数据。最关键的是，我们的方法确保了生物力学的合理性，并通过将逆运动学参数与基于知识的评估策略结合，实现了自动可靠的标签生成。广泛的评估表明，增强的数据变体更接近真实世界的数据，显著提高了神经网络模型的分类准确性和泛化能力。此外，我们还强调了增强数据在患者特定微调场景中的优势，特别是在仅具有限的特定患者训练样本时。我们的研究结果强调了该增强方法在克服深度学习在物理治疗运动评估中面临的常见挑战方面的实用性和有效性。', 'title_zh': '基于 musculoskeletal 模拟的 IMU 数据增强以提升自动运动评估'}
{'arxiv_id': 'arXiv:2505.24409', 'title': 'LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory', 'authors': 'Eojin Kang, Juae Kim', 'link': 'https://arxiv.org/abs/2505.24409', 'abstract': "Multilingual large language models (LLMs) open up new possibilities for leveraging information across languages, but their factual knowledge recall remains inconsistent depending on the input language. While previous studies have attempted to address this issue through English-based prompting and evaluation, we explore non-English to English transfer via Language and Thought Theory. This perspective allows us to examine language-thought binding in LLMs and uncover why factual knowledge often fails to transfer effectively. We propose the Language-to-Thought (L2T) prompting strategy, which analyzes the relationship between input language, internal cognitive processes, and knowledge. Experimental results challenge the assumption that English-based approaches consistently outperform other languages and offer a novel insight that aligning the model's internal thought with the knowledge required for the task is critical for successful cross-lingual transfer. Furthermore, we show that applying L2T during training can alleviate LLMs' reliance on the input language and facilitate cross-linguistic knowledge integration without translation-based learning. Code and datasets will be available.", 'abstract_zh': '多语言大型语言模型（LLMs）为跨语言利用信息开启了新的可能性，但其事实知识回忆的一致性取决于输入语言。虽然先前的研究通过基于英语的提示和评估试图解决这一问题，我们探讨了通过语言和思维理论实现非英语到英语的转移。这种视角使我们能够考察LLMs中的语言-思维绑定并发现为何事实知识往往无法有效转移。我们提出了一种语言到思维（L2T）提示策略，该策略分析输入语言、内部认知过程和知识之间的关系。实验结果挑战了基于英语的方法始终优于其他语言的假设，提出了一个新的观点，即使模型的内部思维与任务所需的知识相一致对于成功的跨语言转移至关重要。此外，我们展示了在训练过程中应用L2T可以减轻LLMs对输入语言的依赖，促进跨语言知识整合而无需基于翻译的学习。代码和数据集将可供获取。', 'title_zh': 'LLMs 是全局多语言但局部单语言：通过语言和思维理论探究知识迁移'}
{'arxiv_id': 'arXiv:2505.24380', 'title': 'SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification', 'authors': 'Zheng Wang', 'link': 'https://arxiv.org/abs/2505.24380', 'abstract': "Fine-grained bird image classification (FBIC) is not only of great significance for ecological monitoring and species identification, but also holds broad research value in the fields of image recognition and fine-grained visual modeling. Compared with general image classification tasks, FBIC poses more formidable challenges: 1) the differences in species size and imaging distance result in the varying sizes of birds presented in the images; 2) complex natural habitats often introduce strong background interference; 3) and highly flexible poses such as flying, perching, or foraging result in substantial intra-class variability. These factors collectively make it difficult for traditional methods to stably extract discriminative features, thereby limiting the generalizability and interpretability of models in real-world applications. To address these challenges, this paper proposes a fine-grained bird classification framework based on strip-aware spatial perception, which aims to capture long-range spatial dependencies across entire rows or columns in bird images, thereby enhancing the model's robustness and interpretability. The proposed method incorporates two novel modules: extensional perception aggregator (EPA) and channel semantic weaving (CSW). Specifically, EPA integrates local texture details with global structural cues by aggregating information across horizontal and vertical spatial directions. CSW further refines the semantic representations by adaptively fusing long-range and short-range information along the channel dimension. Built upon a ResNet-50 backbone, the model enables jump-wise connection of extended structural features across the spatial domain. Experimental results on the CUB-200-2011 dataset demonstrate that our framework achieves significant performance improvements while maintaining architectural efficiency.", 'abstract_zh': '细粒度鸟类图像分类（FBIC）不仅对生态监测和物种识别具有重大意义，还在图像识别和细粒度视觉建模领域具有广泛的科研价值。与一般图像分类任务相比，FBIC提出了更大的挑战：1）不同物种的大小差异和成像距离导致鸟类在图像中的大小各不相同；2）复杂的自然环境常常引入强烈的背景干扰；3）飞行、栖息或觅食等高度灵活的姿态导致类内显著变异性。这些因素共同使得传统方法难以稳定地提取判别性特征，从而限制了模型在实际应用中的泛化能力和可解释性。为应对这些挑战，本文提出了一种基于条带感知的空间感知细粒度鸟类分类框架，旨在捕获鸟类图像中整行或整列的长范围空间依赖关系，从而增强模型的鲁棒性和可解释性。该方法结合了两个新型模块：扩展感知聚合器（EPA）和通道语义编织（CSW）。具体而言，EPA 通过在水平和垂直空间方向上整合信息，将局部纹理细节与全局结构线索相结合。CSW 进一步通过在通道维上适配地融合长范围和短范围信息来细化语义表示。该模型基于 ResNet-50 主干构建，在空间域中实现了扩展结构特征的跳跃式连接。在 CUB-200-2011 数据集上的实验结果表明，本文框架在保持架构效率的同时取得了显著的性能提升。', 'title_zh': 'SASP: 条状物感知的空间知觉在细粒度鸟类图像分类中的应用'}
{'arxiv_id': 'arXiv:2505.24379', 'title': 'Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models', 'authors': 'Xiaoyu Wu, Yifei Pang, Terrance Liu, Zhiwei Steven Wu', 'link': 'https://arxiv.org/abs/2505.24379', 'abstract': "Large language models are typically trained on datasets collected from the web, which may inadvertently contain harmful or sensitive personal information. To address growing privacy concerns, unlearning methods have been proposed to remove the influence of specific data from trained models. Of these, exact unlearning -- which retrains the model from scratch without the target data -- is widely regarded the gold standard, believed to be robust against privacy-related attacks. In this paper, we challenge this assumption by introducing a novel data extraction attack that compromises even exact unlearning. Our method leverages both the pre- and post-unlearning models: by guiding the post-unlearning model using signals from the pre-unlearning model, we uncover patterns that reflect the removed data distribution. Combining model guidance with a token filtering strategy, our attack significantly improves extraction success rates -- doubling performance in some cases -- across common benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our attack's effectiveness on a simulated medical diagnosis dataset to highlight real-world privacy risks associated with exact unlearning. In light of our findings, which suggest that unlearning may, in a contradictory way, increase the risk of privacy leakage, we advocate for evaluation of unlearning methods to consider broader threat models that account not only for post-unlearning models but also for adversarial access to prior checkpoints.", 'abstract_zh': '大规模语言模型通常在包含网络数据的数据集中训练，这些数据可能无意中包含有害或敏感的个人信息。为应对日益增长的隐私担忧，已经提出了遗忘方法来从训练模型中消除特定数据的影响。其中，精确遗忘——即不包含目标数据从头重新训练模型——被认为是最具抗御隐私攻击能力的标准方法。在本文中，我们通过引入一种新颖的数据提取攻击挑战这一假设，该攻击甚至能够突破精确遗忘的防线。我们的方法利用了预遗忘和后遗忘模型：通过使用预遗忘模型的信号来引导后遗忘模型，我们揭示了反映删除数据分布的模式。结合模型引导与令牌过滤策略，我们的攻击显著提高了提取成功率，在某些常见的基准测试如MUSE、TOFU和WMDP中，性能翻倍。此外，我们在一个模拟的医疗诊断数据集上展示了攻击的有效性，以突出精确遗忘可能带来的实际隐私风险。鉴于我们的发现表明，遗忘可能以一种矛盾的方式增加隐私泄露的风险，我们建议评估遗忘方法时应考虑更广泛的威胁模型，不仅考虑后遗忘模型，还考虑对手访问先前检查点的可能性。', 'title_zh': '打破金标准：在大型语言模型中精确撤销学习下提取被遗忘的数据'}
{'arxiv_id': 'arXiv:2505.24378', 'title': 'Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer', 'authors': 'Yilun Kong, Guozheng Ma, Qi Zhao, Haoyu Wang, Li Shen, Xueqian Wang, Dacheng Tao', 'link': 'https://arxiv.org/abs/2505.24378', 'abstract': "Despite recent advancements in offline multi-task reinforcement learning (MTRL) have harnessed the powerful capabilities of the Transformer architecture, most approaches focus on a limited number of tasks, with scaling to extremely massive tasks remaining a formidable challenge. In this paper, we first revisit the key impact of task numbers on current MTRL method, and further reveal that naively expanding the parameters proves insufficient to counteract the performance degradation as the number of tasks escalates. Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE) framework that tackles task scalability by further unlocking the model's parameter scalability. Specifically, we enhance both the architecture and the optimization of the agent, where we strengthen the Decision Transformer (DT) backbone with MoE to reduce task load on parameter subsets, and introduce a three-stage training mechanism to facilitate efficient training with optimal performance. Experimental results show that, by increasing the number of experts, M3DT not only consistently enhances its performance as model expansion on the fixed task numbers, but also exhibits remarkable task scalability, successfully extending to 160 tasks with superior performance.", 'abstract_zh': '尽管近期离线多任务强化学习（MTRL）的进步充分利用了Transformer架构的强大能力，大多数方法仍专注于少量任务，而将规模扩展到极大量任务仍然是一个艰巨的挑战。在本文中，我们首先重新审视当前MTRL方法中任务数量的关键影响，并进一步揭示简单扩展参数不能有效抵消任务数量增加引起的性能下降。基于这些见解，我们提出了M3DT，这是一种新颖的混合专家（MoE）框架，通过进一步解锁模型的参数规模来应对任务的可扩展性挑战。具体而言，我们增强了智能体的架构和优化，通过将决策Transformer（DT）骨干与MoE结合以减少参数子集上的任务负载，并引入三阶段训练机制以实现高效的训练和最优性能。实验结果表明，通过增加专家数量，M3DT不仅在固定任务数量上的模型扩展中一致提高了性能，而且还表现出显著的任务可扩展性，成功扩展到160个任务并保持了优异的性能。', 'title_zh': '通过专家混合决策变换器掌握大规模多任务强化学习'}
{'arxiv_id': 'arXiv:2505.24371', 'title': 'Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering', 'authors': 'Md Intisar Chowdhury, Kittinun Aukkapinyo, Hiroshi Fujimura, Joo Ann Woo, Wasu Wasusatein, Fadoua Ghourabi', 'link': 'https://arxiv.org/abs/2505.24371', 'abstract': 'In this paper, we propose a Grid-based Local and Global Area Transcription (Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates in two phases. First, extracting text transcripts from video frames using a Vision-Language Model (VLM). Next, processing questions using these transcripts to generate answers through a Large Language Model (LLM). This design ensures image privacy by deploying the VLM on edge devices and the LLM in the cloud. To improve transcript quality, we propose grid-based visual prompting, which extracts intricate local details from each grid cell and integrates them with global information. Evaluation results show that Grid-LoGAT, using the open-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms state-of-the-art methods with similar baseline models on NExT-QA and STAR-QA datasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our method surpasses the non-grid version by 24 points on localization-based questions we created using NExT-QA.', 'abstract_zh': '基于网格的局部与全局区域转录（Grid-LoGAT）系统：面向视频问答（VideoQA）的设计与实现', 'title_zh': '基于网格的局部与全局区域识别方法：视频问答中的局部和全局区域文本转录'}
{'arxiv_id': 'arXiv:2505.24369', 'title': 'Adversarial Preference Learning for Robust LLM Alignment', 'authors': 'Yuanfu Wang, Pengyu Wang, Chenyang Xi, Bo Tang, Junyi Zhu, Wenqiang Wei, Chen Chen, Chao Yang, Jingfeng Zhang, Chaochao Lu, Yijun Niu, Keming Mao, Zhiyu Li, Feiyu Xiong, Jie Hu, Mingchuan Yang', 'link': 'https://arxiv.org/abs/2505.24369', 'abstract': "Modern language models often rely on Reinforcement Learning from Human Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to adversarial attacks due to three key limitations: (1) the inefficiency and high cost of human annotation, (2) the vast diversity of potential adversarial attacks, and (3) the risk of feedback bias and reward hacking. To address these challenges, we introduce Adversarial Preference Learning (APL), an iterative adversarial training method incorporating three key innovations. First, a direct harmfulness metric based on the model's intrinsic preference probabilities, eliminating reliance on external assessment. Second, a conditional generative attacker that synthesizes input-specific adversarial variations. Third, an iterative framework with automated closed-loop feedback, enabling continuous adaptation through vulnerability discovery and mitigation. Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly enhances robustness, achieving 83.33% harmlessness win rate over the base model (evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured by LLaMA-Guard), and lowering attack success rate by up to 65% according to HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against the base model.", 'abstract_zh': '现代语言模型往往依赖人类反馈强化学习（RLHF）来鼓励安全行为。然而，它们仍因三大关键局限性而易受 adversarial 攻击：（1）人力标注的低效和高成本，（2）潜在 adversarial 攻击的高度多样性，（3）反馈偏差和奖励作弊的风险。为应对这些挑战，我们引入了对抗偏好学习（APL），这是一种迭代对抗训练方法，包含三项关键创新。首先，基于模型内在偏好概率的直接有害性度量，消除对外部评估的依赖。其次，生成条件型攻击者，合成输入特定的对抗变体。第三，迭代框架结合自动化闭环反馈，通过漏洞发现和缓解实现持续适应。实验证明，APL 显著提升了鲁棒性，在 Mistral-7B-Instruct-v0.3 上的有害性获胜率为 83.33%，相比于基线模型减少了有害输出（由 GPT-4o 评估，从 5.88% 降至 0.43%）和攻击成功率（根据 HarmBench 降低 65%）。值得注意的是，APL 维持了可竞争的实用性，MT-Bench 得分为 6.59（与基线的 6.78 相当）且 LC-WinRate 为 46.52%。', 'title_zh': '对抗偏好学习以实现稳健的LLM对齐'}
{'arxiv_id': 'arXiv:2505.24357', 'title': 'ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration', 'authors': 'Xianglong Yan, Zhiteng Li, Tianao Zhang, Linghe Kong, Yulun Zhang, Xiaokang Yang', 'link': 'https://arxiv.org/abs/2505.24357', 'abstract': 'Large language models (LLMs) have achieved remarkable performance, yet their capability on long-context reasoning is often constrained by the excessive memory required to store the Key-Value (KV) cache. This makes KV cache compression an essential step toward enabling efficient long-context reasoning. Recent methods have explored reducing the hidden dimensions of the KV cache, but many introduce additional computation through projection layers or suffer from significant performance degradation under high compression ratios. To address these challenges, we propose ReCalKV, a post-training KV cache compression method that reduces the hidden dimensions of the KV cache. We develop distinct compression strategies for Keys and Values based on their different roles and varying importance in the attention mechanism. For Keys, we propose Head-wise Similarity-aware Reordering (HSR), which clusters similar heads and applies grouped SVD to the key projection matrix, reducing additional computation while preserving accuracy. For Values, we propose Offline Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra computational overhead. Experiments show that ReCalKV outperforms existing low-rank compression methods, achieving high compression ratios with minimal performance loss. Code is available at: this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在长上下文推理方面取得了显著性能，但其能力常常受限于存储Key-Value（KV）缓存所需的过多内存。因此，KV缓存压缩成为实现高效长上下文推理的关键步骤。最近的方法探索了减少KV缓存的隐藏维度，但许多方法通过投影层引入了额外的计算，或在高压缩比下导致显著的性能下降。为了解决这些挑战，我们提出ReCalKV，这是一种后训练的KV缓存压缩方法，通过减少KV缓存的隐藏维度来实现。我们根据不同角色和重要程度为Keys和Values开发了不同的压缩策略。对于Keys，我们提出了Head-wise Similarity-aware Reordering（HSR），通过聚类相似的heads并将键投影矩阵分组进行SVD，减少额外计算同时保持准确度。对于Values，我们提出了Offline Calibration and Matrix Fusion（OCMF），在不增加额外计算开销的情况下保持准确度。实验结果显示，ReCalKV在较低的性能损失下实现了高于现有低秩压缩方法的高压缩比。代码可在以下链接获取：this https URL。', 'title_zh': 'ReCalKV: 通过头部重排序和离线校准的低秩键值缓存压缩'}
{'arxiv_id': 'arXiv:2505.24341', 'title': 'Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings', 'authors': 'Shujian Yang, Shiyao Cui, Chuanrui Hu, Haicheng Wang, Tianwei Zhang, Minlie Huang, Jialiang Lu, Han Qiu', 'link': 'https://arxiv.org/abs/2505.24341', 'abstract': 'Detecting toxic content using language models is important but challenging. While large language models (LLMs) have demonstrated strong performance in understanding Chinese, recent studies show that simple character substitutions in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In this paper, we highlight the multimodal nature of Chinese language as a key challenge for deploying LLMs in toxic Chinese detection. First, we propose a taxonomy of 3 perturbation strategies and 8 specific approaches in toxic Chinese content. Then, we curate a dataset based on this taxonomy, and benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect perturbed toxic Chinese text. Additionally, we explore cost-effective enhancement solutions like in-context learning (ICL) and supervised fine-tuning (SFT). Our results reveal two important findings. (1) LLMs are less capable of detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a small number of perturbed examples may cause the LLMs "overcorrect\'\': misidentify many normal Chinese contents as toxic.', 'abstract_zh': '使用语言模型检测有毒内容至关重要但颇具挑战性：中文多模态特性的关键挑战及应对策略', 'title_zh': '探索有毒中文检测的多模态挑战：分类、基准和发现'}
{'arxiv_id': 'arXiv:2505.24336', 'title': 'When Humans Growl and Birds Speak: High-Fidelity Voice Conversion from Human to Animal and Designed Sounds', 'authors': 'Minsu Kang, Seolhee Lee, Choonghyeon Lee, Namhyun Cho', 'link': 'https://arxiv.org/abs/2505.24336', 'abstract': 'Human to non-human voice conversion (H2NH-VC) transforms human speech into animal or designed vocalizations. Unlike prior studies focused on dog-sounds and 16 or 22.05kHz audio transformation, this work addresses a broader range of non-speech sounds, including natural sounds (lion-roars, birdsongs) and designed voice (synthetic growls). To accomodate generation of diverse non-speech sounds and 44.1kHz high-quality audio transformation, we introduce a preprocessing pipeline and an improved CVAE-based H2NH-VC model, both optimized for human and non-human voices. Experimental results showed that the proposed method outperformed baselines in quality, naturalness, and similarity MOS, achieving effective voice conversion across diverse non-human timbres. Demo samples are available at this https URL', 'abstract_zh': '人类到非人类声音转换（H2NH-VC）将人类语音转换为动物或设计的声音。', 'title_zh': '当人类低吼且鸟类说话：高保真从人类到动物及设计声音的语音转换'}
{'arxiv_id': 'arXiv:2505.24298', 'title': 'AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning', 'authors': 'Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu', 'link': 'https://arxiv.org/abs/2505.24298', 'abstract': 'Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a \\emph{fully asynchronous} RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves \\textbf{up to 2.57$\\times$ training speedup} compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at this https URL.', 'abstract_zh': '强化学习（RL）已成为训练大规模语言模型（LLMs）的热点范式，尤其是用于推理任务。对于LLMs的高效RL训练迫切需要高效的训练系统。现有的大多数大规模RL系统是同步的，通过批量设置交替生成和训练，其中每个训练批次中的rollout均由同一模型（或最新模型）生成。这虽然稳定了RL训练，但也严重降低了系统效率。生成必须等待批次中最长输出完成之后才能进行模型更新，导致GPU利用率低下。我们提出了一种名为AReaL的完全异步RL系统，彻底将生成与训练脱钩。AReaL中的rollout工人连续生成新的输出而无需等待，训练工人则在收集到数据批次时更新模型。AReaL还包含了一系列系统级优化，显著提高了GPU利用率。为稳定RL训练，AReaL平衡了rollout和训练工人的工作量以控制数据陈旧度，并采用了一种陈旧度增强的PPO变体以更好地处理过时的训练样本。在数学和代码推理基准上的 extensive 实验表明，AReaL 在相同数量的GPU下相比最佳同步系统实现了 **高达2.57倍的训练加速**，且最终性能相当甚至更好。AReaL的代码可从 [此链接](this https URL) 获取。', 'title_zh': 'AReaL: 一种大规模异步语言推理强化学习系统'}
{'arxiv_id': 'arXiv:2505.24293', 'title': 'Large Language Models are Locally Linear Mappings', 'authors': 'James R. Golden', 'link': 'https://arxiv.org/abs/2505.24293', 'abstract': 'We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.', 'abstract_zh': '我们证明了多个开源权重大规模语言模型（LLMs）的推理操作可以映射到一个与输入序列完全等价的线性系统中，无需修改模型权重或改变输出预测。通过扩展图像扩散模型中局部或分段线性特性相关技术，我们针对给定输入序列的下一个标记预测战略地修改了梯度计算，使模型的雅可比矩阵几乎精确地重现前向预测的线性系统。我们跨多个模型（Llama 3、Gemma 3、Qwen 3、Phi 4、Mistral、Ministral 和 OLMo 2，直至 Llama 3.3 70B Q4）展示了这种方法，并通过分离雅可比的奇异值分解证明了这些LLMs在极低维度子空间中运行，其中许多最大的奇异向量解码为与最有可能的输出标记相关的概念。该方法还允许我们逐层（及其注意力和MLP组件）近似视为线性系统，并观察语义概念的涌现。尽管现代LLMs具有强大的表达能力和全局非线性，但它们可以通过近似局部线性分解来解释，从而揭示其内部表示并揭示下一个标记预测过程中的可解释语义结构。', 'title_zh': '大型语言模型是局部线性映射'}
{'arxiv_id': 'arXiv:2505.24291', 'title': 'Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion', 'authors': 'Kaidi Wang, Wenhao Guan, Ziyue Jiang, Hukai Huang, Peijie Chen, Weijie Wu, Qingyang Hong, Lin Li', 'link': 'https://arxiv.org/abs/2505.24291', 'abstract': "Currently, zero-shot voice conversion systems are capable of synthesizing the voice of unseen speakers. However, most existing approaches struggle to accurately replicate the speaking style of the source speaker or mimic the distinctive speaking style of the target speaker, thereby limiting the controllability of voice conversion. In this work, we propose Discl-VC, a novel voice conversion framework that disentangles content and prosody information from self-supervised speech representations and synthesizes the target speaker's voice through in-context learning with a flow matching transformer. To enable precise control over the prosody of generated speech, we introduce a mask generative transformer that predicts discrete prosody tokens in a non-autoregressive manner based on prompts. Experimental results demonstrate the superior performance of Discl-VC in zero-shot voice conversion and its remarkable accuracy in prosody control for synthesized speech.", 'abstract_zh': '当前，零样本语音转换系统能够合成未见说话人的语音。然而，大多数现有方法在准确复制源说话人的语音风格或模仿目标说话人的独特语音风格方面存在困难，从而限制了语音转换的可控性。在这种情况下，我们提出了一种新颖的语音转换框架Discl-VC，该框架从自监督语音表示中分离出内容和语调信息，并通过上下文学习和流动匹配变换器合成目标说话人的语音。为了实现对生成语音语调的精确控制，我们引入了一种掩码生成变换器，该变换器基于提示以非自回归的方式预测离散的语调标记。实验结果表明，Discl-VC在零样本语音转换中的性能优越，并且在合成语音的语调控制方面具有显著的准确性。', 'title_zh': 'Discl-VC: 解耦离散令牌与上下文学习在可控零样本语音转换中的应用'}
{'arxiv_id': 'arXiv:2505.24269', 'title': 'INSIGHT: A Survey of In-Network Systems for Intelligent, High-Efficiency AI and Topology Optimization', 'authors': 'Aleksandr Algazinov, Joydeep Chandra, Matt Laing', 'link': 'https://arxiv.org/abs/2505.24269', 'abstract': 'In-network computation represents a transformative approach to addressing the escalating demands of Artificial Intelligence (AI) workloads on network infrastructure. By leveraging the processing capabilities of network devices such as switches, routers, and Network Interface Cards (NICs), this paradigm enables AI computations to be performed directly within the network fabric, significantly reducing latency, enhancing throughput, and optimizing resource utilization. This paper provides a comprehensive analysis of optimizing in-network computation for AI, exploring the evolution of programmable network architectures, such as Software-Defined Networking (SDN) and Programmable Data Planes (PDPs), and their convergence with AI. It examines methodologies for mapping AI models onto resource-constrained network devices, addressing challenges like limited memory and computational capabilities through efficient algorithm design and model compression techniques. The paper also highlights advancements in distributed learning, particularly in-network aggregation, and the potential of federated learning to enhance privacy and scalability. Frameworks like Planter and Quark are discussed for simplifying development, alongside key applications such as intelligent network monitoring, intrusion detection, traffic management, and Edge AI. Future research directions, including runtime programmability, standardized benchmarks, and new applications paradigms, are proposed to advance this rapidly evolving field. This survey underscores the potential of in-network AI to create intelligent, efficient, and responsive networks capable of meeting the demands of next-generation AI applications.', 'abstract_zh': '网络内计算代表了一种变革性的方法，用于应对不断增长的AI工作负载对网络基础设施的需求。通过利用交换机、路由器和网络接口卡（NIC）等网络设备的处理能力，这一范式能够在网络 fabric 中直接进行AI计算，显著降低延迟、提升吞吐量并优化资源利用率。本文对优化网络内计算以支持AI进行了全面分析，探索了可编程网络架构（如软件定义网络SDN和可编程数据平面PDP）的发展及其与AI的融合。研究了将AI模型映射到资源受限的网络设备的方法，通过高效算法设计和模型压缩技术解决内存和计算能力有限等挑战。本文还强调了分布式学习的进展，特别是网络内聚合，并探讨了联邦学习在增强隐私性和可扩展性方面的潜力。讨论了简化开发的框架如Planter和Quark，以及智能网络监控、入侵检测、流量管理和边缘AI等关键应用。提出了未来研究方向，包括运行时可编程性、标准化基准测试和新应用范式，以推进这一快速发展的领域。本文综述了网络内AI的潜力，使其能够创建智能、高效和响应迅速的网络，以满足下一代AI应用的需求。', 'title_zh': 'INSIGHT：面向智能高效AI和拓扑优化的网络内系统综述'}
{'arxiv_id': 'arXiv:2505.24264', 'title': 'Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations', 'authors': 'Xin Quan, Marco Valentino, Louise A. Dennis, André Freitas', 'link': 'https://arxiv.org/abs/2505.24264', 'abstract': "Natural language explanations play a fundamental role in Natural Language Inference (NLI) by revealing how premises logically entail hypotheses. Recent work has shown that the interaction of large language models (LLMs) with theorem provers (TPs) can help verify and improve the validity of NLI explanations. However, TPs require translating natural language into machine-verifiable formal representations, a process that introduces the risk of semantic information loss and unfaithful interpretation, an issue compounded by LLMs' challenges in capturing critical logical structures with sufficient precision. Moreover, LLMs are still limited in their capacity for rigorous and robust proof construction within formal verification frameworks. To mitigate issues related to faithfulness and robustness, this paper investigates strategies to (1) alleviate semantic loss during autoformalisation, (2) efficiently identify and correct syntactic errors in logical representations, (3) explicitly use logical expressions to guide LLMs in generating structured proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree using different LLMs demonstrate that the proposed strategies yield significant improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover, we show that specific interventions on the hybrid LLM-TP architecture can substantially improve efficiency, drastically reducing the number of iterations required for successful verification.", 'abstract_zh': '自然语言解释在自然语言推理中的基础作用：大型语言模型与定理证明器的交互促进解释验证与改进', 'title_zh': '忠实且 robust 的基于大规模语言模型的定理证明用于 NLI 解释'}
{'arxiv_id': 'arXiv:2505.24255', 'title': 'Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games', 'authors': 'Neemesh Yadav, Palakorn Achananuparp, Jing Jiang, Ee-Peng Lim', 'link': 'https://arxiv.org/abs/2505.24255', 'abstract': "Large Language Models (LLMs) have shown potential in simulating human behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for complex social interactions. In this study, we investigate the role of ToM reasoning in aligning agentic behaviors with human norms in negotiation tasks, using the ultimatum game as a controlled environment. We initialized LLM agents with different prosocial beliefs (including Greedy, Fair, and Selfless) and reasoning methods like chain-of-thought (CoT) and varying ToM levels, and examined their decision-making processes across diverse LLMs, including reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from 2,700 simulations indicated that ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes. Consistent with previous findings, reasoning models exhibit limited capability compared to models with ToM reasoning, different roles of the game benefits with different orders of ToM reasoning. Our findings contribute to the understanding of ToM's role in enhancing human-AI interaction and cooperative decision-making. The code used for our experiments can be found at this https URL.", 'abstract_zh': '大型语言模型（LLMs）在模拟人类行为和执行理论%E2%80%93of%E2%80%93mind（ToM）推理方面显示出潜力，这是复杂社交互动中的一项关键技能。本研究通过使用 ultimatum 游戏作为受控环境，探讨 ToM 推理在谈判任务中使代理行为与人类规范保持一致的作用。我们初始化了具有不同利他主义信念（包括贪婪、公正和无私）和推理方法（如链式思考）的不同 ToM 水平的 LLM 代理，并在包括推理模型 o3-mini 和 DeepSeek-R1 Distilled Qwen 32B 等多种 LLM 中研究了它们的决策过程。2,700 次模拟的结果表明，ToM 推理增强了行为一致性、决策一致性和谈判结果。与先前的研究一致，具有 ToM 推理的模型的性能优于仅具有推理能力的模型，不同的博弈收益顺序和 ToM 推理的角色也有所不同。我们的研究成果有助于理解 ToM 在增强人机互动和合作决策中的作用。实验使用的代码可以在此处找到：https://xxxxxx。', 'title_zh': '思维理论和亲社会信念对 Ultimatum 游戏中 LLMs 人类对齐行为的影响'}
{'arxiv_id': 'arXiv:2505.24253', 'title': 'Interactive Video Generation via Domain Adaptation', 'authors': 'Ishaan Rawal, Suryansh Kumar', 'link': 'https://arxiv.org/abs/2505.24253', 'abstract': 'Text-conditioned diffusion models have emerged as powerful tools for high-quality video generation. However, enabling Interactive Video Generation (IVG), where users control motion elements such as object trajectory, remains challenging. Recent training-free approaches introduce attention masking to guide trajectory, but this often degrades perceptual quality. We identify two key failure modes in these methods, both of which we interpret as domain shift problems, and propose solutions inspired by domain adaptation. First, we attribute the perceptual degradation to internal covariate shift induced by attention masking, as pretrained models are not trained to handle masked attention. To address this, we propose mask normalization, a pre-normalization layer designed to mitigate this shift via distribution matching. Second, we address initialization gap, where the randomly sampled initial noise does not align with IVG conditioning, by introducing a temporal intrinsic diffusion prior that enforces spatio-temporal consistency at each denoising step. Extensive qualitative and quantitative evaluations demonstrate that mask normalization and temporal intrinsic denoising improve both perceptual quality and trajectory control over the existing state-of-the-art IVG techniques.', 'abstract_zh': '基于文本条件的扩散模型已成为高质量视频生成的强大工具。然而，在用户能够控制如 объект轨迹等运动元素的交互视频生成（IVG）方面仍然颇具挑战性。近期的无训练方法引入了注意力屏蔽以引导轨迹，但往往会降低感知质量。我们识别出这些方法中的两种关键失败模式，我们将这两种模式解释为领域偏移问题，并提出了基于领域自适应的解决方案。首先，我们将感知质量下降归因于注意力屏蔽引起的内部协变量偏移，因为预训练模型并未被训练来处理屏蔽注意力。为此，我们提出了屏蔽归一化，这是一种预归一化层，设计用于通过分布匹配来缓解这种偏移。其次，我们通过引入时间内在去噪先验来解决初始化差距问题，该先验在每个去噪步骤中强制执行时空一致性。广泛的定性和定量评估表明，屏蔽归一化和时间内在去噪均能够提升感知质量和轨迹控制，从而超越现有最先进的IVG技术。', 'title_zh': '基于域适应的互动视频生成'}
{'arxiv_id': 'arXiv:2505.24252', 'title': 'A Reward-driven Automated Webshell Malicious-code Generator for Red-teaming', 'authors': 'Yizhong Ding', 'link': 'https://arxiv.org/abs/2505.24252', 'abstract': 'Frequent cyber-attacks have elevated WebShell exploitation and defense to a critical research focus within network security. However, there remains a significant shortage of publicly available, well-categorized malicious-code datasets organized by obfuscation method. Existing malicious-code generation methods, which primarily rely on prompt engineering, often suffer from limited diversity and high redundancy in the payloads they produce. To address these limitations, we propose \\textbf{RAWG}, a \\textbf{R}eward-driven \\textbf{A}utomated \\textbf{W}ebshell Malicious-code \\textbf{G}enerator designed for red-teaming applications. Our approach begins by categorizing webshell samples from common datasets into seven distinct types of obfuscation. We then employ a large language model (LLM) to extract and normalize key tokens from each sample, creating a standardized, high-quality corpus. Using this curated dataset, we perform supervised fine-tuning (SFT) on an open-source large model to enable the generation of diverse, highly obfuscated webshell malicious payloads. To further enhance generation quality, we apply Proximal Policy Optimization (PPO), treating malicious-code samples as "chosen" data and benign code as "rejected" data during reinforcement learning. Extensive experiments demonstrate that RAWG significantly outperforms current state-of-the-art methods in both payload diversity and escape effectiveness.', 'abstract_zh': '频繁的网络攻击已将WebShell利用与防御提升为网络安全性研究的关键关注点。然而，公开 availability 的、按混淆方法分类的恶意代码数据集仍然严重缺乏。现有的恶意代码生成方法主要依赖提示工程，往往在生成的样本中表现出多样性有限和重复性高的问题。为解决这些限制，我们提出了一种名为RAWG的奖励驱动的自动化WebShell恶意代码生成器，旨在支持红队应用。我们的方法首先将常见数据集中WebShell样本分类为七种不同的混淆类型。然后利用大型语言模型（LLM）从每个样本中提取和规范化关键标记，创建一个标准化且高质量的语料库。利用这个精选的数据集，我们对开源大型模型进行监督微调（SFT），以实现生成多样且高度混淆的WebShell恶意载荷。为了进一步提升生成质量，在强化学习中我们将Proximal Policy Optimization (PPO) 应用于恶意代码样本作为“选择”数据和良性代码作为“拒绝”数据。广泛的实验表明，RAWG在载荷多样性与逃逸有效性方面显著优于当前最先进的方法。', 'title_zh': '基于奖励驱动的自动化Webshell恶意代码生成器用于红队演练'}
{'arxiv_id': 'arXiv:2505.24245', 'title': 'LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework', 'authors': 'Xin Kang, Zihan Zheng, Lei Chu, Yue Gao, Jiahao Li, Hao Pan, Xuejin Chen, Yan Lu', 'link': 'https://arxiv.org/abs/2505.24245', 'abstract': 'We present LTM3D, a Latent Token space Modeling framework for conditional 3D shape generation that integrates the strengths of diffusion and auto-regressive (AR) models. While diffusion-based methods effectively model continuous latent spaces and AR models excel at capturing inter-token dependencies, combining these paradigms for 3D shape generation remains a challenge. To address this, LTM3D features a Conditional Distribution Modeling backbone, leveraging a masked autoencoder and a diffusion model to enhance token dependency learning. Additionally, we introduce Prefix Learning, which aligns condition tokens with shape latent tokens during generation, improving flexibility across modalities. We further propose a Latent Token Reconstruction module with Reconstruction-Guided Sampling to reduce uncertainty and enhance structural fidelity in generated shapes. Our approach operates in token space, enabling support for multiple 3D representations, including signed distance fields, point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on image- and text-conditioned shape generation tasks demonstrate that LTM3D outperforms existing methods in prompt fidelity and structural accuracy while offering a generalizable framework for multi-modal, multi-representation 3D generation.', 'abstract_zh': '我们提出LTM3D，这是一种结合扩散和自回归模型优势的潜在令牌空间建模框架，用于条件3D形状生成。虽然基于扩散的方法有效地建模了连续的潜在空间，而自回归模型在捕捉令牌间依赖性方面表现出色，但将这两种范式结合用于3D形状生成依然面临挑战。为此，LTM3D 包含一个条件分布建模骨干，利用掩码自编码器和扩散模型来增强令牌依赖性学习。另外，我们引入了前缀学习，该方法在生成过程中将条件令牌与形状的潜在令牌对齐，提高了不同模态下的灵活性。进一步地，我们提出了一种潜在令牌重建模块，通过重建引导采样来减少不确定性并提高生成形状的结构保真度。我们的方法在令牌空间中操作，支持包括隐式距离场、点云、网格和三维高斯点等多样的3D表示。在基于图像和文本条件的形状生成任务中的大量实验表明，LTM3D 在提示保真度和结构准确性方面优于现有方法，同时还提供了一个适用于多模态、多表示的三维生成的可泛化框架。', 'title_zh': 'LTM3D：自回归扩散框架下条件3D生成的_tokens_空间桥梁构建'}
{'arxiv_id': 'arXiv:2505.24239', 'title': 'An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring', 'authors': 'Sana Ebrahimi, Mohsen Dehghankar, Abolfazl Asudeh', 'link': 'https://arxiv.org/abs/2505.24239', 'abstract': "While multi-agent LLM systems show strong capabilities in various domains, they are highly vulnerable to adversarial and low-performing agents. To resolve this issue, in this paper, we introduce a general and adversary-resistant multi-agent LLM framework based on credibility scoring. We model the collaborative query-answering process as an iterative game, where the agents communicate and contribute to a final system output. Our system associates a credibility score that is used when aggregating the team outputs. The credibility scores are learned gradually based on the past contributions of each agent in query answering. Our experiments across multiple tasks and settings demonstrate our system's effectiveness in mitigating adversarial influence and enhancing the resilience of multi-agent cooperation, even in the adversary-majority settings.", 'abstract_zh': '基于可信度评分的抗敌对攻击多智能体大语言模型框架', 'title_zh': '基于可信度评分的抗对手多Agent大型语言模型系统'}
{'arxiv_id': 'arXiv:2505.24232', 'title': 'From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models', 'authors': 'Haibo Jin, Peiyan Zhang, Peiran Wang, Man Luo, Haohan Wang', 'link': 'https://arxiv.org/abs/2505.24232', 'abstract': 'Large foundation models (LFMs) are susceptible to two distinct vulnerabilities: hallucinations and jailbreak attacks. While typically studied in isolation, we observe that defenses targeting one often affect the other, hinting at a deeper connection.\nWe propose a unified theoretical framework that models jailbreaks as token-level optimization and hallucinations as attention-level optimization. Within this framework, we establish two key propositions: (1) \\textit{Similar Loss Convergence} - the loss functions for both vulnerabilities converge similarly when optimizing for target-specific outputs; and (2) \\textit{Gradient Consistency in Attention Redistribution} - both exhibit consistent gradient behavior driven by shared attention dynamics.\nWe validate these propositions empirically on LLaVA-1.5 and MiniGPT-4, showing consistent optimization trends and aligned gradients. Leveraging this connection, we demonstrate that mitigation techniques for hallucinations can reduce jailbreak success rates, and vice versa. Our findings reveal a shared failure mode in LFMs and suggest that robustness strategies should jointly address both vulnerabilities.', 'abstract_zh': '大型基础模型（LFMs）存在两种不同的脆弱性：幻觉和 Jailbreak 攻击。虽然通常分别研究，但我们观察到针对一种脆弱性的防御措施往往会影响另一种，暗示两者之间存在更深层的联系。', 'title_zh': '从幻觉到逃逸：重新思考大型基础模型的安全性'}
{'arxiv_id': 'arXiv:2505.24231', 'title': 'Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion', 'authors': 'Md Shahnawaz, Bishwajit Prasad Gond, Durga Prasad Mohapatra', 'link': 'https://arxiv.org/abs/2505.24231', 'abstract': 'Malware detection and classification remains a topic of concern for cybersecurity, since it is becoming common for attackers to use advanced obfuscation on their malware to stay undetected. Conventional static analysis is not effective against polymorphic and metamorphic malware as these change their appearance without modifying their behavior, thus defying the analysis by code structure alone. This makes it important to use dynamic detection that monitors malware behavior at runtime. In this paper, we present a dynamic malware categorization framework that extracts API argument calls at the runtime execution of Windows Portable Executable (PE) files. Extracting and encoding the dynamic features of API names, argument return values, and other relative features, we convert raw behavioral data to temporal patterns. To enhance feature portrayal, the generated patterns are subsequently converted into grayscale pictures using a magma colormap. These improved photos are used to teach a Convolutional Neural Network (CNN) model discriminative features, which allows for reliable and accurate malware classification. Results from experiments indicate that our method, with an average accuracy of 98.36% is effective in classifying different classes of malware and benign by integrating dynamic analysis and deep learning. It not only achieves high classification accuracy but also demonstrates significant resilience against typical evasion strategies.', 'abstract_zh': '恶意软件检测与分类仍然被认为是网络安全领域的重点问题，因为攻击者常常使用高级混淆技术来使其恶意软件逃避检测。传统的静态分析对变体和变种恶意软件无效，因为这些恶意软件在不改变其行为的情况下改变外观，仅仅依赖代码结构进行分析难以奏效。因此，使用运行时动态检测来监控恶意软件行为变得尤为重要。本文提出了一种动态恶意软件分类框架，在Windows可移植可执行（PE）文件的运行时执行过程中提取API参数调用。通过提取和编码API名称、参数返回值以及其他相关特征，将原始行为数据转换为时间序列模式。为了增强特征表述，生成的模式随后被转换成灰度图片，使用熔岩色图进行表示。这些改进后的图片被用来训练卷积神经网络（CNN）模型，以学习具有区分性的特征，从而使恶意软件分类既可靠又准确。实验结果表明，该方法通过结合动态分析和深度学习，在不同类别恶意软件和良性软件分类中的平均准确率达到98.36%，不仅实现了高分类准确性，还展示了对典型规避策略的强大抗性。', 'title_zh': '基于CNN和运行时API调用参数转换生成的灰度图像的Windows PE文件动态恶意软件分类'}
{'arxiv_id': 'arXiv:2505.24225', 'title': 'Reasoning Can Hurt the Inductive Abilities of Large Language Models', 'authors': 'Haibo Jin, Peiyan Zhang, Man Luo, Haohan Wang', 'link': 'https://arxiv.org/abs/2505.24225', 'abstract': "Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning - inferring latent rules from sparse examples - remains limited. It is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. We investigate this assumption with creating four controlled, diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack - with hidden human-defined rules. We find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts.\nTo explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. Based on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.", 'abstract_zh': '大型语言模型（LLMs）在各个领域取得了显著进展，但在归纳推理能力——从稀疏样本中推断潜在规则——方面仍有限制。通常假定，大型推理模型（LRMs）中的链式思考（CoT）提示能够增强这种推理能力。我们通过创建四种受控的诊断游戏任务——国际象棋、德州扑克、骰子游戏和 blackjack——来验证这一假设，这些任务都隐藏了人类定义的规则。我们发现，CoT 推理可能会降低归纳性能，LRMs 往往不如非推理对照组表现良好。\n\n为了解释这一现象，我们提出了一种理论框架，揭示了推理步骤如何通过三种故障模式放大错误：不正确的子任务分解、不正确的子任务解决以及不正确的最终答案总结。基于我们的理论和实证分析，我们提出了结构化的干预措施，根据我们识别的失败类型适应CoT生成。这些干预措施提高了归纳准确性，而无需重新训练。我们的研究结果表明，有效的（CoT）推理不仅依赖于采取更多的步骤，还依赖于确保这些步骤结构良好。', 'title_zh': '推理可能会损害大型语言模型的归纳能力'}
{'arxiv_id': 'arXiv:2505.24214', 'title': 'Benchmarking Foundation Models for Zero-Shot Biometric Tasks', 'authors': 'Redwan Sony, Parisa Farmanifard, Hamzeh Alzwairy, Nitish Shukla, Arun Ross', 'link': 'https://arxiv.org/abs/2505.24214', 'abstract': 'The advent of foundation models, particularly Vision-Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), has redefined the frontiers of artificial intelligence, enabling remarkable generalization across diverse tasks with minimal or no supervision. Yet, their potential in biometric recognition and analysis remains relatively underexplored. In this work, we introduce a comprehensive benchmark that evaluates the zero-shot and few-shot performance of state-of-the-art publicly available VLMs and MLLMs across six biometric tasks spanning the face and iris modalities: face verification, soft biometric attribute prediction (gender and race), iris recognition, presentation attack detection (PAD), and face manipulation detection (morphs and deepfakes). A total of 41 VLMs were used in this evaluation. Experiments show that embeddings from these foundation models can be used for diverse biometric tasks with varying degrees of success. For example, in the case of face verification, a True Match Rate (TMR) of 96.77 percent was obtained at a False Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW) dataset, without any fine-tuning. In the case of iris recognition, the TMR at 1 percent FMR on the IITD-R-Full dataset was 97.55 percent without any fine-tuning. Further, we show that applying a simple classifier head to these embeddings can help perform DeepFake detection for faces, Presentation Attack Detection (PAD) for irides, and extract soft biometric attributes like gender and ethnicity from faces with reasonably high accuracy. This work reiterates the potential of pretrained models in achieving the long-term vision of Artificial General Intelligence.', 'abstract_zh': '基础模型的兴起，特别是视觉-语言模型（VLMs）和多模态大型语言模型（MLLMs），已重新定义了人工智能的边界，使其在最少或无需监督的情况下能够实现跨多种任务的显著泛化。然而，它们在生物特征识别与分析领域的潜力仍相对未被充分探索。在本文中，我们介绍了一个全面基准，评估了六种生物特征任务（涵盖面部和虹膜模态）上最先进的公开可用VLMs和MLLMs的零-shot和少-shot性能：面部验证、软生物特征属性预测（性别和种族）、虹膜识别、欺骗性攻击检测（PAD）和面部操控检测（变形和深度假 face）。共使用了41种VLMs进行评估。实验结果显示，这些基础模型的嵌入可以在不同程度上用于多样的生物特征任务。例如，在面部验证任务中，通过在Labeled Face in the Wild（LFW）数据集上进行96.77%的正确匹配率（TMR）而同时保持1%的错误匹配率（FMR）时，无需微调即达到了这一结果。在虹膜识别任务中，通过在IITD-R-Full数据集上进行97.55%的正确匹配率（TMR）而同时保持1%的错误匹配率（FMR）时，同样无需微调。此外，我们展示，通过对这些嵌入施加一个简单的分类器头部，可以用于检测面部的深度假 face、虹膜的欺骗性攻击检测（PAD）以及从面部中提取性别和种族等软生物特征属性，其准确率相对较高。本文重申了预训练模型实现长期愿景——通用人工智能的潜力。', 'title_zh': '零样本生物特征任务的基础模型基准研究'}
{'arxiv_id': 'arXiv:2505.24189', 'title': 'Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows', 'authors': 'Orlando Marquez Ayala, Patrice Bechard, Emily Chen, Maggie Baird, Jingfei Chen', 'link': 'https://arxiv.org/abs/2505.24189', 'abstract': 'Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.', 'abstract_zh': '大型语言模型（LLMs）如GPT-4可以借助合适的提示处理广泛的复杂任务。随着按token成本的降低，细调小型语言模型（SLMs）在实际应用中的优势——更快的推理、更低的成本——可能不再明显。在本文中，我们提供了证据表明，对于需要结构化输出的领域特定任务，SLMs仍然具有质量优势。我们将细调SLMs与使用LLMs进行任务提示进行比较，该任务是生成JSON形式的低代码工作流。我们观察到，尽管一个好的提示可以产生合理的结果，但细调平均提高了10%的质量。我们还进行了系统的错误分析以揭示模型的局限性。', 'title_zh': '调优SLM或提示LLM？生成低代码工作流的案例研究'}
{'arxiv_id': 'arXiv:2505.24185', 'title': 'Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling', 'authors': 'Yipan Wei, Yuchen Zou, Yapeng Li, Bo Du', 'link': 'https://arxiv.org/abs/2505.24185', 'abstract': 'Federated Multi-Task Learning (FMTL) enables multiple clients performing heterogeneous tasks without exchanging their local data, offering broad potential for privacy preserving multi-task collaboration. However, most existing methods focus on building personalized models for each client and unable to support the aggregation of multiple heterogeneous tasks into a unified model. As a result, in real-world scenarios where task objectives, label spaces, and optimization paths vary significantly, conventional FMTL methods struggle to achieve effective joint training. To address this challenge, we propose FedDEA (Federated Decoupled Aggregation), an update-structure-aware aggregation method specifically designed for multi-task model integration. Our method dynamically identifies task-relevant dimensions based on the response strength of local updates and enhances their optimization effectiveness through rescaling. This mechanism effectively suppresses cross-task interference and enables task-level decoupled aggregation within a unified global model. FedDEA does not rely on task labels or architectural modifications, making it broadly applicable and deployment-friendly. Experimental results demonstrate that it can be easily integrated into various mainstream federated optimization algorithms and consistently delivers significant overall performance improvements on widely used NYUD-V2 and PASCAL-Context. These results validate the robustness and generalization capabilities of FedDEA under highly heterogeneous task settings.', 'abstract_zh': '联邦多任务学习（Federated Multi-Task Learning, FMTL）使得多个执行异构任务的客户端无需交换本地数据即可进行协作，提供了广泛的数据隐私保护多任务学习潜力。然而，现有大部分方法集中在为每个客户端构建个性化模型，无法支持将多个异构任务统合成一个统一模型。因此，在任务目标、标签空间和优化路径差异显著的现实场景中，传统FMTL方法难以实现有效联合训练。为解决这一挑战，我们提出了一种更新结构感知聚合方法FedDEA（Federated Decoupled Aggregation），专门设计用于多任务模型集成。该方法基于局部更新的响应强度动态识别任务相关维度，并通过重缩放增强其优化效果，有效抑制跨任务干扰，在统一全局模型中实现任务级解耦聚合。FedDEA 不依赖于任务标签或模型结构修改，使其具有广泛应用性和易于部署的特点。实验结果表明，它可轻松集成到各种主流联邦优化算法中，并在广泛使用的NYUD-V2和PASCAL-Context数据集上实现显著的整体性能提升。这些结果验证了FedDEA在高度异构任务设置下的稳健性和泛化能力。', 'title_zh': '统一建模在子空间解耦的联邦多任务学习中探讨'}
{'arxiv_id': 'arXiv:2505.24182', 'title': 'Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT', 'authors': 'Zhuobai Dong, Junchao Yi, Ziyuan Zheng, Haochen Han, Xiangxi Zheng, Alex Jinpeng Wang, Fangming Liu, Linjie Li', 'link': 'https://arxiv.org/abs/2505.24182', 'abstract': 'Understanding the physical world - governed by laws of motion, spatial relations, and causality - poses a fundamental challenge for multimodal large language models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o demonstrate impressive perceptual and reasoning capabilities, our investigation reveals these models struggle profoundly with visual physical reasoning, failing to grasp basic physical laws, spatial interactions, and causal effects in complex scenes. More importantly, they often fail to follow coherent reasoning chains grounded in visual evidence, especially when multiple steps are needed to arrive at the correct answer. To rigorously evaluate this capability, we introduce MVPBench, a curated benchmark designed to rigorously evaluate visual physical reasoning through the lens of visual chain-of-thought (CoT). Each example features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues. This setup mirrors how humans reason through real-world physical processes over time. To ensure fine-grained evaluation, we introduce a graph-based CoT consistency metric that verifies whether the reasoning path of model adheres to valid physical logic. Additionally, we minimize shortcut exploitation from text priors, encouraging models to rely on visual understanding. Experimental results reveal a concerning trend: even cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text alignment in physical domains. Surprisingly, RL-based post-training alignment - commonly believed to improve visual reasoning performance - often harms spatial reasoning, suggesting a need to rethink current fine-tuning practices.', 'abstract_zh': '理解由运动定律、空间关系和因果关系支配的物理世界，对多模态大型语言模型（MLLMs）构成了根本性挑战。尽管诸如OpenAI o3和GPT-4o等最近的进步展示了令人印象深刻的感知和推理能力，我们的研究发现，这些模型在视觉物理推理方面表现差强人意，难以掌握基本的物理定律、空间交互和因果效应，尤其是在复杂场景中。更重要的是，它们常常无法遵循基于视觉证据的连贯推理链，特别是在需要多步推理才能得出正确答案的情况下。为了严格评估这种能力，我们提出了MVPBench，这是一个精心设计的基准测试，通过视觉链式推理（CoT）的视角严格评估视觉物理推理。每个示例都包含交错的多幅图像输入，并不仅要求正确的最终答案，还要求连贯的、基于不断变化的视觉线索的逐步推理路径。这种设置类似于人类如何随着时间推移推理现实世界的物理过程。为了确保精细评估，我们引入了一种基于图的CoT一致性度量，验证模型的推理路径是否遵循有效的物理逻辑。此外，我们还减少了对文本先验的捷径利用，促使模型依赖于视觉理解。实验结果显示了一个令人担忧的趋势：即使是最先进的MLLMs在物理领域的视觉推理准确性和图像-文本对齐方面表现欠佳。令人惊讶的是，通常被认为能够提高视觉推理性能的基于RL的后训练对齐往往损害了空间推理，这表明需要重新审视当前的微调实践。', 'title_zh': '看见并不等同于推理：基于图的多路径视觉物理共情推理评估基准MVPBench'}
{'arxiv_id': 'arXiv:2505.24179', 'title': 'SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling', 'authors': 'Xiaodong Ji, Hailin Zhang, Fangcheng Fu, Bin Cui', 'link': 'https://arxiv.org/abs/2505.24179', 'abstract': 'Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality.', 'abstract_zh': 'SALE：一种在长上下文预填充阶段加速大规模语言模型细粒度稀疏注意力的方法', 'title_zh': 'SALE：高效的长上下文LLM预填充中稀疏注意机制的低比特估计'}
{'arxiv_id': 'arXiv:2505.24178', 'title': 'Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem', 'authors': 'Katherine Tieu, Dongqi Fu, Jun Wu, Jingrui He', 'link': 'https://arxiv.org/abs/2505.24178', 'abstract': 'In the era of foundation models, Out-of- Distribution (OOD) problems, i.e., the data discrepancy between the training environments and testing environments, hinder AI generalization. Further, relational data like graphs disobeying the Independent and Identically Distributed (IID) condition makes the problem more challenging, especially much harder when it is associated with time. Motivated by this, to realize the robust invariant learning over temporal graphs, we want to investigate what components in temporal graphs are most invariant and representative with respect to labels. With the Information Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector that can distinguish invariant components and variant components during the training process to make the deep learning model generalizable for different testing scenarios. Besides deriving a series of rigorous generalizable optimization functions, we also equip the training with task-specific loss functions, e.g., temporal link prediction, to make pretrained models solve real-world application tasks like citation recommendation and merchandise recommendation, as demonstrated in our experiments with state-of-the-art (SOTA) methods. Our code is available at this https URL.', 'abstract_zh': '在基础模型时代，离分布(OOD)问题，即训练环境与测试环境之间的数据差异，阻碍了AI的一般化。进一步地，不符合独立同分布(IID)条件的图数据关系使得问题更加棘手，尤其是在与时间相关的情况下更为困难。鉴于此，为了在时序图上实现鲁棒不变学习，我们旨在探究时序图中最不变且最具代表性的部分与标签之间的关系。借助信息瓶颈(IB)方法，我们提出了一种误差界不变链接选择器，该选择器在训练过程中能够区分不变部分和变化部分，从而使深度学习模型在不同测试场景下具有泛化能力。除了推导一系列严格的泛化优化函数之外，我们还为特定任务配备了损失函数，例如时序链接预测，以使预训练模型解决实际应用任务，如引用推荐和商品推荐，这已在实验中通过对比尖端方法得到证实。我们的代码可在以下链接获取：this https URL。', 'title_zh': '空间-时间域外问题的不变连接选择器'}
{'arxiv_id': 'arXiv:2505.24163', 'title': 'LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing', 'authors': 'Jiaqi Sun, Shiyou Qian, Zhangchi Han, Wei Li, Zelin Qian, Dingyu Yang, Jian Cao, Guangtao Xue', 'link': 'https://arxiv.org/abs/2505.24163', 'abstract': 'Knowledge Graphs (KGs) structure real-world entities and their relationships into triples, enhancing machine reasoning for various tasks. While domain-specific KGs offer substantial benefits, their manual construction is often inefficient and requires specialized knowledge. Recent approaches for knowledge graph construction (KGC) based on large language models (LLMs), such as schema-guided KGC and reference knowledge integration, have proven efficient. However, these methods are constrained by their reliance on manually defined schema, single-document processing, and public-domain references, making them less effective for domain-specific corpora that exhibit complex knowledge dependencies and specificity, as well as limited reference knowledge. To address these challenges, we propose LKD-KGC, a novel framework for unsupervised domain-specific KG construction. LKD-KGC autonomously analyzes document repositories to infer knowledge dependencies, determines optimal processing sequences via LLM driven prioritization, and autoregressively generates entity schema by integrating hierarchical inter-document contexts. This schema guides the unsupervised extraction of entities and relationships, eliminating reliance on predefined structures or external knowledge. Extensive experiments show that compared with state-of-the-art baselines, LKD-KGC generally achieves improvements of 10% to 20% in both precision and recall rate, demonstrating its potential in constructing high-quality domain-specific KGs.', 'abstract_zh': '一种用于无监督领域特定知识图构建的新框架：LKD-KGC', 'title_zh': 'LKD-KGC：通过LLM驱动的知识依赖解析实现的领域特定知识图谱构建'}
{'arxiv_id': 'arXiv:2505.24157', 'title': "Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents", 'authors': 'Seungjoon Lee, Suhwan Kim, Minhyeon Oh, Youngsik Yoon, Jungseul Ok', 'link': 'https://arxiv.org/abs/2505.24157', 'abstract': "Developing autonomous agents capable of mastering complex, multi-step tasks in unpredictable, interactive environments presents a significant challenge. While Large Language Models (LLMs) offer promise for planning, existing approaches often rely on problematic internal knowledge or make unrealistic environmental assumptions. Although recent work explores learning planning knowledge, they still retain limitations due to partial reliance on external knowledge or impractical setups. Indeed, prior research has largely overlooked developing agents capable of acquiring planning knowledge from scratch, directly in realistic settings. While realizing this capability is necessary, it presents significant challenges, primarily achieving robustness given the substantial risk of incorporating LLMs' inaccurate knowledge. Moreover, efficiency is crucial for practicality as learning can demand prohibitive exploration. In response, we introduce Robust and Efficient Planning for Open-world Agents (REPOA), a novel framework designed to tackle these issues. REPOA features three key components: adaptive dependency learning and fine-grained failure-aware operation memory to enhance robustness to knowledge inaccuracies, and difficulty-based exploration to improve learning efficiency. Our evaluation in two established open-world testbeds demonstrates REPOA's robust and efficient planning, showcasing its capability to successfully obtain challenging late-game items that were beyond the reach of prior approaches.", 'abstract_zh': '开发能够在不确定的交互环境中掌握复杂多步任务的自主代理面临重大挑战。虽然大型语言模型（LLMs）在规划方面展现出潜力，但现有方法往往依赖于有问题的内部知识或不切实际的环境假设。尽管近期工作探索了学习规划知识，但它们仍然受限于对外部知识的部分依赖或不实际的设置。实际上，早期研究大多忽略了开发能够在现实场景中从零开始学习规划知识的代理。虽然实现这一能力是必要的，但这也带来了重大挑战，主要在于如何在大量引入LLMs不准确的知识风险下实现鲁棒性。此外，提高学习效率对于实际应用至关重要，因为学习可能需要大量不可行的探索。为此，我们提出了一种名为Robust and Efficient Planning for Open-world Agents（REPOA）的新型框架，以应对上述问题。REPOA 包含三个关键组件：自适应依赖学习和细粒度失败感知操作记忆，以增强对知识不准确性的鲁棒性，以及基于难度的探索，以提高学习效率。在两个已建立的开放世界测试平台上的评估展示了REPOA 的鲁棒和高效规划能力，证明了其能够成功获取前人方法难以企及的后期游戏资源。', 'title_zh': '别只是跟随MLLM计划：开放世界代理的稳健高效规划'}
{'arxiv_id': 'arXiv:2505.24149', 'title': 'RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget', 'authors': 'Adam Piaseczny, Md Kamran Chowdhury Shisher, Shiqiang Wang, Christopher G. Brinton', 'link': 'https://arxiv.org/abs/2505.24149', 'abstract': 'Machine learning (ML) algorithms deployed in real-world environments are often faced with the challenge of adapting models to concept drift, where the task data distributions are shifting over time. The problem becomes even more difficult when model performance must be maintained under adherence to strict resource constraints. Existing solutions often depend on drift-detection methods that produce high computational overhead for resource-constrained environments, and fail to provide strict guarantees on resource usage or theoretical performance assurances. To address these shortcomings, we propose RCCDA: a dynamic model update policy that optimizes ML training dynamics while ensuring strict compliance to predefined resource constraints, utilizing only past loss information and a tunable drift threshold. In developing our policy, we analytically characterize the evolution of model loss under concept drift with arbitrary training update decisions. Integrating these results into a Lyapunov drift-plus-penalty framework produces a lightweight policy based on a measurable accumulated loss threshold that provably limits update frequency and cost. Experimental results on three domain generalization datasets demonstrate that our policy outperforms baseline methods in inference accuracy while adhering to strict resource constraints under several schedules of concept drift, making our solution uniquely suited for real-time ML deployments.', 'abstract_zh': '基于概念漂移的严格资源约束下动态模型更新策略：RCCDA', 'title_zh': 'RCCDA：在受限资源预算下概念漂移时的自适应模型更新'}
{'arxiv_id': 'arXiv:2505.24141', 'title': 'The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models', 'authors': 'Jiashuai Liu, Yingjia Shang, Yingkang Zhan, Di Zhang, Yi Niu, Dong Wei, Xian Wu, Zeyu Gao, Chen Li, Yefeng Zheng', 'link': 'https://arxiv.org/abs/2505.24141', 'abstract': 'With the widespread adoption of pathology foundation models in both research and clinical decision support systems, exploring their security has become a critical concern. However, despite their growing impact, the vulnerability of these models to adversarial attacks remains largely unexplored. In this work, we present the first systematic investigation into the security of pathology foundation models for whole slide image~(WSI) analysis against adversarial attacks. Specifically, we introduce the principle of \\textit{local perturbation with global impact} and propose a label-free attack framework that operates without requiring access to downstream task labels. Under this attack framework, we revise four classical white-box attack methods and redefine the perturbation budget based on the characteristics of WSI. We conduct comprehensive experiments on three representative pathology foundation models across five datasets and six downstream tasks. Despite modifying only 0.1\\% of patches per slide with imperceptible noise, our attack leads to downstream accuracy degradation that can reach up to 20\\% in the worst cases. Furthermore, we analyze key factors that influence attack success, explore the relationship between patch-level vulnerability and semantic content, and conduct a preliminary investigation into potential defence strategies. These findings lay the groundwork for future research on the adversarial robustness and reliable deployment of pathology foundation models. Our code is publicly available at: this https URL.', 'abstract_zh': '病理基础模型在全视野图像分析中对抗攻击的安全性系统研究', 'title_zh': '病理学中的蝴蝶效应：探究病理基础模型的安全性'}
{'arxiv_id': 'arXiv:2505.24139', 'title': 'S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation', 'authors': 'Yichen Xie, Runsheng Xu, Tong He, Jyh-Jing Hwang, Katie Luo, Jingwei Ji, Hubert Lin, Letian Chen, Yiren Lu, Zhaoqi Leng, Dragomir Anguelov, Mingxing Tan', 'link': 'https://arxiv.org/abs/2505.24139', 'abstract': 'The latest advancements in multi-modal large language models (MLLMs) have spurred a strong renewed interest in end-to-end motion planning approaches for autonomous driving. Many end-to-end approaches rely on human annotations to learn intermediate perception and prediction tasks, while purely self-supervised approaches--which directly learn from sensor inputs to generate planning trajectories without human annotations often underperform the state of the art. We observe a key gap in the input representation space: end-to-end approaches built on MLLMs are often pretrained with reasoning tasks in 2D image space rather than the native 3D space in which autonomous vehicles plan. To this end, we propose S4-Driver, a scalable self-supervised motion planning algorithm with spatio-temporal visual representation, based on the popular PaLI multimodal large language model. S4-Driver uses a novel sparse volume strategy to seamlessly transform the strong visual representation of MLLMs from perspective view to 3D space without the need to finetune the vision encoder. This representation aggregates multi-view and multi-frame visual inputs and enables better prediction of planning trajectories in 3D space. To validate our method, we run experiments on both nuScenes and Waymo Open Motion Dataset (with in-house camera data). Results show that S4-Driver performs favorably against existing supervised multi-task approaches while requiring no human annotations. It also demonstrates great scalability when pretrained on large volumes of unannotated driving logs.', 'abstract_zh': '多模态大型语言模型最新进展重新激发了自动驾驶端到端运动规划方法的研究兴趣', 'title_zh': 'S4-Driver：面向时空视觉表示的可扩展自监督驾驶多模态大语言模型'}
{'arxiv_id': 'arXiv:2505.24138', 'title': 'AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits', 'authors': 'Yichen Shi, Ze Zhang, Hongyang Wang, Zhuofu Tao, Zhongyi Li, Bingyu Chen, Yaxin Wang, Zhiping Yu, Ting-Jung Lin, Lei He', 'link': 'https://arxiv.org/abs/2505.24138', 'abstract': "Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit design has remained a longstanding challenge due to its difficulty and complexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer promising potential for supporting AMS circuit analysis and design. However, current research typically evaluates MLLMs on isolated tasks within the domain, lacking a comprehensive benchmark that systematically assesses model capabilities across diverse AMS-related challenges. To address this gap, we introduce AMSbench, a benchmark suite designed to evaluate MLLM performance across critical tasks including circuit schematic perception, circuit analysis, and circuit design. AMSbench comprises approximately 8000 test questions spanning multiple difficulty levels and assesses eight prominent models, encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and Gemini 2.5 Pro. Our evaluation highlights significant limitations in current MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit design tasks. These results underscore the necessity of advancing MLLMs' understanding and effective application of circuit-specific knowledge, thereby narrowing the existing performance gap relative to human expertise and moving toward fully automated AMS circuit design workflows. Our data is released at this https URL", 'abstract_zh': 'Analog/M Mixed-Signal (AMS) 电路在集成电路（IC）行业中发挥着关键作用。然而，由于其复杂性，自动化 Analog/Mixed-Signal (AMS) 电路设计一直是一个长期挑战。近期多模态大规模语言模型（MLLMs）的进步为支持 AMS 电路分析和设计带来了希望。然而，当前研究通常在单一任务上评估 MLLMs，缺乏一个系统评估模型在多类 AMS 相关挑战上的综合基准。为填补这一空白，我们引入了 AMSbench，这是一个基准套件，旨在评估 MLLM 在电路示意图感知、电路分析和电路设计等关键任务上的性能。AMSbench 包含约 8000 个测试问题，涵盖多个难度级别，并评估了八种主流模型，包括开源和专有解决方案如 Qwen 2.5-VL 和 Gemini 2.5 Pro。我们的评估突显了当前 MLLMs 的主要局限性，特别是在复杂多模态推理和复杂电路设计任务上的局限性。这些结果强调了推进 MLLMs 对电路特定知识的理解和有效应用的必要性，从而缩小与人类专家表现的差距，并朝着完全自动化的 AMS 电路设计流程迈进。我们的数据在此 https:// link 中发布。', 'title_zh': 'AMSbench：评估AMS电路中MLLM能力的综合性基准'}
{'arxiv_id': 'arXiv:2505.24136', 'title': 'Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction', 'authors': 'Yaşar Utku Alçalar, Mehmet Akçakaya', 'link': 'https://arxiv.org/abs/2505.24136', 'abstract': "Physics-driven deep learning (PD-DL) models have proven to be a powerful approach for improved reconstruction of rapid MRI scans. In order to train these models in scenarios where fully-sampled reference data is unavailable, self-supervised learning has gained prominence. However, its application at high acceleration rates frequently introduces artifacts, compromising image fidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL networks via carefully-designed perturbations. In particular, we enhance the k-space masking idea of conventional self-supervised learning with a novel consistency term that assesses the model's ability to accurately predict the added perturbations in a sparse domain, leading to more reliable and artifact-free reconstructions. The results obtained from the fastMRI knee and brain datasets show that the proposed training strategy effectively reduces aliasing artifacts and mitigates noise amplification at high acceleration rates, outperforming state-of-the-art self-supervised methods both visually and quantitatively.", 'abstract_zh': '基于物理驱动的深度学习模型的自监督训练新方法：克服高加速率下的伪影和噪声放大问题', 'title_zh': '驱动稀疏性并行成像一致性以改善自监督MRI重建'}
{'arxiv_id': 'arXiv:2505.24133', 'title': 'R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration', 'authors': 'Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu', 'link': 'https://arxiv.org/abs/2505.24133', 'abstract': 'Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.', 'abstract_zh': '冗余感知推理模型的KV缓存压缩方法（R-KV）', 'title_zh': 'R-KV: 训练无关的键值缓存压缩方法以加速推理模型，考虑冗余性'}
{'arxiv_id': 'arXiv:2505.24120', 'title': 'CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs', 'authors': 'Ai Jian, Weijie Qiu, Xiaokun Wang, Peiyu Wang, Yunzhuo Hao, Jiangbo Pei, Yichen Wei, Yi Peng, Xuchen Song', 'link': 'https://arxiv.org/abs/2505.24120', 'abstract': 'Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remains inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question this http URL benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex this http URL additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6\\% this http URL empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at this https URL.', 'abstract_zh': 'Vision-Language模型（VLMs）在多模态理解方面取得了显著进展，但在科学推理能力方面仍缺乏充分评估。当前的多模态基准主要评估通用图像理解或基于文本的推理，缺乏需要特定领域知识与视觉证据分析相结合的真实科学情境。为填补这一空白，我们提出了CSVQA这一专门用于评估基于领域知识的科学推理的诊断性多模态基准，网址为this http URL基准包含了1,378个精挑细选的问题-答案对，覆盖多个STEM学科，每个问题都要求应用领域知识、整合视觉证据并进行高层次推理。与以往的多模态基准相比，CSVQA更侧重于真实世界的科学内容和复杂推理过程。我们还提出了一套严格的评估协议，系统地评估模型预测是否基于经过筛选的中间推理步骤。对15种VLMs在该基准上的全面评估显示了显著的性能差异，即使排名第一的专有模型也只能达到49.6%的准确率。我们的实证研究强调了提升VLMs科学推理能力的重要需求。CSVQA基准已发布在this https URL。', 'title_zh': 'CSVQA: 一种评估VLMs在STEM推理能力上的中文多模态基准'}
{'arxiv_id': 'arXiv:2505.24099', 'title': 'Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning', 'authors': 'Mohammad Shah Alam, William Ott, Ilya Timofeyev', 'link': 'https://arxiv.org/abs/2505.24099', 'abstract': 'In this paper, we explore the predictive capabilities of echo state networks (ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal nonlinear PDE that exhibits spatiotemporal chaos. We introduce a novel methodology that integrates ESNs with transfer learning, aiming to enhance predictive performance across various parameter regimes of the gKS model. Our research focuses on predicting changes in long-term statistical patterns of the gKS model that result from varying the dispersion relation or the length of the spatial domain. We use transfer learning to adapt ESNs to different parameter settings and successfully capture changes in the underlying chaotic attractor.', 'abstract_zh': '本文探讨了回声状态网络(ESNs)对广义库朗托-西瓦辛斯基(gKS)方程的预测能力，该方程是一种典型的具有时空混沌特性的非线性偏微分方程。我们提出了一种新颖的方法，将ESNs与迁移学习相结合，旨在提高在gKS模型不同参数区域的预测性能。我们的研究重点是预测由改变dispersion关系或空间域长度引起的gKS模型长期内统计模式的变化。通过迁移学习，我们成功地使ESNs适应不同的参数设置，并捕获了潜在的混沌吸引子的变化。', 'title_zh': '基于回声状态网络和迁移学习的时空混沌动力系统吸引子学习'}
{'arxiv_id': 'arXiv:2505.24090', 'title': 'Searching Clinical Data Using Generative AI', 'authors': 'Karan Hanswadkar, Anika Kanchi, Shivani Tripathi, Shi Qiao, Rony Chatterjee, Alekh Jindal', 'link': 'https://arxiv.org/abs/2505.24090', 'abstract': 'Artificial Intelligence (AI) is making a major impact on healthcare, particularly through its application in natural language processing (NLP) and predictive analytics. The healthcare sector has increasingly adopted AI for tasks such as clinical data analysis and medical code assignment. However, searching for clinical information in large and often unorganized datasets remains a manual and error-prone process. Assisting this process with automations can help physicians improve their operational productivity significantly.\nIn this paper, we present a generative AI approach, coined SearchAI, to enhance the accuracy and efficiency of searching clinical data. Unlike traditional code assignment, which is a one-to-one problem, clinical data search is a one-to-many problem, i.e., a given search query can map to a family of codes. Healthcare professionals typically search for groups of related diseases, drugs, or conditions that map to many codes, and therefore, they need search tools that can handle keyword synonyms, semantic variants, and broad open-ended queries. SearchAI employs a hierarchical model that respects the coding hierarchy and improves the traversal of relationships from parent to child nodes. SearchAI navigates these hierarchies predictively and ensures that all paths are reachable without losing any relevant nodes.\nTo evaluate the effectiveness of SearchAI, we conducted a series of experiments using both public and production datasets. Our results show that SearchAI outperforms default hierarchical traversals across several metrics, including accuracy, robustness, performance, and scalability. SearchAI can help make clinical data more accessible, leading to streamlined workflows, reduced administrative burden, and enhanced coding and diagnostic accuracy.', 'abstract_zh': '人工智能（AI）在医疗领域产生了重大影响，特别是在自然语言处理（NLP）和预测分析方面的应用。医疗行业越来越多地采用AI进行临床数据分析和医疗编码。然而，在大型且往往无序的数据集中搜索临床信息仍是一个手动且容易出错的过程。通过自动化辅助这一过程，可以显著提高医生的操作生产力。\n\n在本文中，我们提出了一种生成式AI方法，名为SearchAI，以提高临床数据搜索的准确性和效率。与传统的编码分配问题是一对一的问题不同，临床数据搜索是多对一的问题，即给定的搜索查询可以映射到一组代码。医疗专业人员通常搜索一组相关疾病、药物或条件，这些组对应于众多代码，因此他们需要能够处理关键词同义词、语义变体和宽泛开放查询的搜索工具。SearchAI采用分层模型，尊重编码层次结构，并改进了从父节点到子节点的关系遍历。SearchAI预测性地导航这些层次结构，确保所有路径都是可达的，且不会丢失任何相关节点。\n\n为了评估SearchAI的有效性，我们使用公开和生产数据集进行了系列实验。结果显示，SearchAI在准确性、鲁棒性、性能和可扩展性等多方面指标上均优于默认的层级遍历方法。SearchAI有助于使临床数据更加易于访问，从而简化工作流程，减少行政负担，并提高编码和诊断准确性。', 'title_zh': '使用生成式AI搜索临床数据'}
{'arxiv_id': 'arXiv:2505.24069', 'title': 'DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures', 'authors': 'Yu He, Yingxi Li, Colin White, Ellen Vitercik', 'link': 'https://arxiv.org/abs/2505.24069', 'abstract': "Large language models (LLMs) are increasingly deployed for real-world tasks that fundamentally involve data manipulation. A core requirement across these tasks is the ability to perform structural reasoning--that is, to understand and reason about data relationships. For example, customer requests require a temporal ordering, which can be represented by data structures such as queues. However, existing benchmarks primarily focus on high-level, application-driven evaluations without isolating this fundamental capability. To address this gap, we introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning capabilities through data structures, which provide interpretable representations of data relationships. DSR-Bench includes 20 data structures, 35 operations, and 4,140 problem instances, organized hierarchically for fine-grained analysis of reasoning limitations. Our evaluation pipeline is fully automated and deterministic, eliminating subjective human or model-based judgments. Its synthetic nature also ensures scalability and minimizes data contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis shows that instruction-tuned models struggle with basic multi-attribute and multi-hop reasoning. Furthermore, while reasoning-oriented models perform better, they remain fragile on complex and hybrid structures, with the best model achieving an average score of only 47% on the challenge subset. Crucially, models often perform poorly on multi-dimensional data and natural language task descriptions, highlighting a critical gap for real-world deployment.", 'abstract_zh': '大型语言模型（LLMs）日益被用于涉及数据操作的基本任务。这些任务的核心要求之一是能够进行结构推理，即理解并推理数据关系的能力。例如，客户请求需要时间上的顺序，这可以通过队列等数据结构来表示。然而，现有的基准测试主要集中在高层次的应用驱动评估上，而没有将这种基本能力隔离出来。为了解决这一差距，我们引入了DSR-Bench，这是一个新的基准测试，通过数据结构来评估LLMs的结构推理能力，这些数据结构可以提供数据关系的可解释表示。DSR-Bench包含20种数据结构、35种操作以及4,140个问题实例，并分层组织以细粒度分析推理限制。我们的评估管道是全自动和确定性的，消除了主观的人为或模型判断。其合成的特性也确保了可扩展性，并减少了数据污染的风险。我们对九个最先进的LLMs进行了基准测试。我们的分析显示，指令调优模型在基本的多属性和多跳推理方面存在困难。此外，虽然推理导向的模型表现更好，但在复杂和混合结构上仍然脆弱，最佳模型在挑战子集上的平均得分为47%。 crucially，模型在多维数据和自然语言任务描述方面表现不佳，突显了实际部署中的关键差距。', 'title_zh': 'DSR-Bench：通过数据结构评估LLM的结构性推理能力'}
{'arxiv_id': 'arXiv:2505.24055', 'title': 'Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs', 'authors': 'Yilong Wang, Tianxiang Zhao, Zongyu Wu, Suhang Wang', 'link': 'https://arxiv.org/abs/2505.24055', 'abstract': "Graph neural networks (GNNs) have shown great ability for node classification on graphs. However, the success of GNNs relies on abundant labeled data, while obtaining high-quality labels is costly and challenging, especially for newly emerging domains. Hence, unsupervised domain adaptation (UDA), which trains a classifier on the labeled source graph and adapts it to the unlabeled target graph, is attracting increasing attention. Various approaches have been proposed to alleviate the distribution shift between the source and target graphs to facilitate the classifier adaptation. However, most of them simply adopt existing UDA techniques developed for independent and identically distributed data to gain domain-invariant node embeddings for graphs, which do not fully consider the graph structure and message-passing mechanism of GNNs during the adaptation and will fail when label distribution shift exists among domains. In this paper, we proposed a novel framework that adopts link prediction to connect nodes between source and target graphs, which can facilitate message-passing between the source and target graphs and augment the target nodes to have ``in-distribution'' neighborhoods with the source domain. This strategy modified the target graph on the input level to reduce its deviation from the source domain in the embedding space and is insensitive to disproportional label distributions across domains. To prevent the loss of discriminative information in the target graph, we further design a novel identity-preserving learning objective, which guides the learning of the edge insertion module together with reconstruction and adaptation losses. Experimental results on real-world datasets demonstrate the effectiveness of our framework.", 'abstract_zh': '基于链接预测的图神经网络无监督域适应框架', 'title_zh': '通过链接预测桥梁源领域和目标领域以实现图上的无监督领域适应'}
{'arxiv_id': 'arXiv:2505.24040', 'title': 'MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering', 'authors': 'Yuexing Hao, Kumail Alhamoud, Hyewon Jeong, Haoran Zhang, Isha Puri, Philip Torr, Mike Schaekermann, Ariel D. Stern, Marzyeh Ghassemi', 'link': 'https://arxiv.org/abs/2505.24040', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance on various medical question-answering (QA) benchmarks, including standardized medical exams. However, correct answers alone do not ensure correct logic, and models may reach accurate conclusions through flawed processes. In this study, we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance Estimation and Question Answering) dataset to evaluate how physician trainees and LLMs prioritize relevant information when answering QA questions. We obtain annotations on 1,300 QA pairs from 36 physician trainees, labeling each sentence within the question components for relevance. We compare these relevance estimates to those for LLMs, and further evaluate the impact of these "relevant" subsets on downstream task performance for both physician trainees and LLMs. We find that LLMs are frequently not aligned with the content relevance estimates of physician trainees. After filtering out physician trainee-labeled irrelevant sentences, accuracy improves for both the trainees and the LLMs. All LLM and physician trainee-labeled data are available at: this http URL.', 'abstract_zh': '大型语言模型（LLMs）在各种医学问答（QA）基准测试，包括标准化医学考试中展现了 remarkable 的性能。然而，仅正确答案不能保证正确的逻辑过程，模型可能通过错误的过程得出正确的结论。本研究引入 MedPAIR（医学数据集，对比医生和AI的相关性估计与问答）数据集，以评估医生实习生和LLMs在回答医学问答问题时如何优先考虑相关信息。我们从36名医生实习生中获得了1,300个问答对的标注，为每个问题组件内的句子标注了相关性标签，并将这些相关性估计与LLMs的进行对比，进一步评估这些“相关”子集对医生实习生和LLMs下游任务性能的影响。我们发现，LLMs与医生实习生的内容相关性估计往往不一致。在过滤掉医生实习生标注的无关句子后，两者的表现准确性都得到了提高。所有医生实习生和LLMs标注的数据均可在以下链接获取：this http URL。', 'title_zh': 'MedPAIR: 评估医生和AI在医学问答中的相关性一致性'}
{'arxiv_id': 'arXiv:2505.24034', 'title': 'LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin', 'authors': 'Bo Wu, Sid Wang, Yunhao Tang, Jia Ding, Eryk Helenowski, Liang Tan, Tengyu Xu, Tushar Gowda, Zhengxing Chen, Chen Zhu, Xiaocheng Tang, Yundi Qian, Beibei Zhu, Rui Hou', 'link': 'https://arxiv.org/abs/2505.24034', 'abstract': "Reinforcement Learning (RL) has become the most effective post-training approach for improving the capabilities of Large Language Models (LLMs). In practice, because of the high demands on latency and memory, it is particularly challenging to develop an efficient RL framework that reliably manages policy models with hundreds to thousands of billions of parameters.\nIn this paper, we present LlamaRL, a fully distributed, asynchronous RL framework optimized for efficient training of large-scale LLMs with various model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a handful to thousands of devices. LlamaRL introduces a streamlined, single-controller architecture built entirely on native PyTorch, enabling modularity, ease of use, and seamless scalability to thousands of GPUs. We also provide a theoretical analysis of LlamaRL's efficiency, including a formal proof that its asynchronous design leads to strict RL speed-up. Empirically, by leveraging best practices such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. Furthermore, the efficiency advantage continues to grow with increasing model scale, demonstrating the framework's suitability for future large-scale RL training.", 'abstract_zh': '强化学习（RL）已成为提高大型语言模型（LLMs）能力的最有效后训练方法。然而，由于对延迟和内存的高要求，开发一个高效且可靠的RL框架来管理从数十亿到数千亿参数的策略模型极具挑战性。\n\n在本文中，我们提出LlamaRL，这是一个针对GPU集群（从少量到数千个设备）高效训练各种模型规模（8B、70B和405B参数）的大规模LLMs进行了优化的完全分布异步RL框架。LlamaRL引入了一种基于原生PyTorch的简洁的单一控制器架构，实现了模块化、易用性和无缝扩展到数千个GPU。我们还提供了LlamaRL效率的理论分析，包括异步设计严格提高RL速度的正式证明。通过利用如模型并置卸载、异步离策略训练和分布式直接内存访问同步权重等最佳实践，LlamaRL实现了显著的效率提升——相对于DeepSpeed-Chat类似的系统，在一个405B参数的策略模型上，速度提升高达10.7倍。此外，随着模型规模的增加，效率优势持续增长，展示了该框架适用于未来大规模RL训练的适用性。', 'title_zh': 'LlamaRL：一种高效的大规模语言模型分布式异步 reinforcement learning 框架'}
{'arxiv_id': 'arXiv:2505.24030', 'title': 'From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?', 'authors': 'Ziming Zhao, ChengAo Shen, Hanghang Tong, Dongjin Song, Zhigang Deng, Qingsong Wen, Jingchao Ni', 'link': 'https://arxiv.org/abs/2505.24030', 'abstract': 'Transformer-based models have gained increasing attention in time series research, driving interest in Large Language Models (LLMs) and foundation models for time series analysis. As the field moves toward multi-modality, Large Vision Models (LVMs) are emerging as a promising direction. In the past, the effectiveness of Transformer and LLMs in time series has been debated. When it comes to LVMs, a similar question arises: are LVMs truely useful for time series analysis? To address it, we design and conduct the first principled study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across both high-level (classification) and low-level (forecasting) tasks, with extensive ablation analysis. Our findings indicate LVMs are indeed useful for time series classification but face challenges in forecasting. Although effective, the contemporary best LVM forecasters are limited to specific types of LVMs and imaging methods, exhibit a bias toward forecasting periods, and have limited ability to utilize long look-back windows. We hope our findings could serve as a cornerstone for future research on LVM- and multimodal-based solutions to different time series tasks.', 'abstract_zh': '基于Transformer的模型在时间序列研究中获得了越来越多的关注，推动了大型语言模型（LLMs）和基础模型在时间序列分析中的应用。随着领域向多模态发展，大型视觉模型（LVMs）正 emerge as a promising direction. In the past,有关Transformer和LLMs在时间序列中的有效性存在争议。至于LVMs，类似的问题出现了：LVMs真对时间序列分析有用吗？为了解答这一问题，我们设计并开展了一项第一性的原则研究，涉及4种LVMs、8种成像方法、18个数据集和26种基线，涵盖了高层（分类）和低层（预测）任务，并进行了广泛的消融分析。研究发现，LVMs确实适用于时间序列分类但在预测方面面临挑战。尽管有效，当前最佳的LVM预测器仅限于特定类型的LVMs和成像方法，偏向于预测时期，并且难以利用长回溯窗口。希望我们的发现能够为未来基于LVM和多模态的方法在不同类型时间序列任务中的研究奠定基石。', 'title_zh': '从图像到信号：大规模视觉模型对时间序列分析有用吗？'}
{'arxiv_id': 'arXiv:2505.24026', 'title': 'MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking', 'authors': 'Numair Nadeem, Muhammad Hamza Asad, Saeed Anwar, Abdul Bais', 'link': 'https://arxiv.org/abs/2505.24026', 'abstract': 'Semantic segmentation of crops and weeds is crucial for site-specific farm management; however, most existing methods depend on labor intensive pixel-level annotations. A further challenge arises when models trained on one field (source domain) fail to generalize to new fields (target domain) due to domain shifts, such as variations in lighting, camera setups, soil composition, and crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by enabling adaptation without target-domain labels, but current UDA methods struggle with occlusions and visual blending between crops and weeds, leading to misclassifications in real-world conditions. To overcome these limitations, we introduce MaskAdapt, a novel approach that enhances segmentation accuracy through multimodal contextual learning by integrating RGB images with features derived from depth data. By computing depth gradients from depth maps, our method captures spatial transitions that help resolve texture ambiguities. These gradients, through a cross-attention mechanism, refines RGB feature representations, resulting in sharper boundary delineation. In addition, we propose a geometry-aware masking strategy that applies horizontal, vertical, and stochastic masks during training. This encourages the model to focus on the broader spatial context for robust visual recognition. Evaluations on real agricultural datasets demonstrate that MaskAdapt consistently outperforms existing State-of-the-Art (SOTA) UDA methods, achieving improved segmentation mean Intersection over Union (mIOU) across diverse field conditions.', 'abstract_zh': '基于掩码适应的作物与杂草语义分割在特定位置农艺管理中至关重要；然而，大多数现有方法依赖于劳动密集型的像素级标注。当模型在一种农田（源领域）训练并在新农田（目标领域）泛化时，由于光照、相机设置、土壤组成和作物生长阶段的变化导致领域偏移，会进一步出现泛化能力不足的问题。无监督领域适应（UDA）通过在没有目标域标签的情况下进行适应来解决这一问题，但当前的UDA方法在遮挡和作物与杂草的视觉融合方面存在难题，导致在实际条件下的误分类。为克服这些限制，我们提出了MaskAdapt，一种通过将RGB图像与深度数据提取的特征结合的多模态上下文学习方法来提升分割准确性。通过从深度图计算深度梯度，我们的方法捕捉到有助于解决纹理歧义的空间转换。这些梯度通过跨注意力机制进一步细化RGB特征表示，从而实现更清晰的边界划定。此外，我们提出了一种几何感知的掩码策略，在训练过程中应用水平、垂直和随机掩码，这促使模型关注更大的空间上下文以增强视觉识别的鲁棒性。在实际农业数据集上的评估结果显示，MaskAdapt在各种农田条件下始终优于现有的最先进（SOTA）UDA方法，实现了更好的分割平均交并比（mIOU）。', 'title_zh': 'MaskAdapt：基于多模态上下文学习和RGB-Depth掩膜的无监督几何感知领域适应'}
{'arxiv_id': 'arXiv:2505.24025', 'title': 'DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models', 'authors': 'Chenbin Pan, Wenbin He, Zhengzhong Tu, Liu Ren', 'link': 'https://arxiv.org/abs/2505.24025', 'abstract': 'The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose \\textbf{DINO-R1}, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces \\textbf{Group Relative Query Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios.', 'abstract_zh': '基于强化学习激励视觉上下文推理能力的DINO-R1', 'title_zh': 'DINO-R1：激励视觉基础模型的推理能力'}
{'arxiv_id': 'arXiv:2505.24023', 'title': 'Multi-Group Proportional Representation for Text-to-Image Models', 'authors': 'Sangwon Jung, Alex Oesterling, Claudio Mayrink Verdun, Sajani Vithana, Taesup Moon, Flavio P. Calmon', 'link': 'https://arxiv.org/abs/2505.24023', 'abstract': 'Text-to-image (T2I) generative models can create vivid, realistic images from textual descriptions. As these models proliferate, they expose new concerns about their ability to represent diverse demographic groups, propagate stereotypes, and efface minority populations. Despite growing attention to the "safe" and "responsible" design of artificial intelligence (AI), there is no established methodology to systematically measure and control representational harms in image generation. This paper introduces a novel framework to measure the representation of intersectional groups in images generated by T2I models by applying the Multi-Group Proportional Representation (MPR) metric. MPR evaluates the worst-case deviation of representation statistics across given population groups in images produced by a generative model, allowing for flexible and context-specific measurements based on user requirements. We also develop an algorithm to optimize T2I models for this metric. Through experiments, we demonstrate that MPR can effectively measure representation statistics across multiple intersectional groups and, when used as a training objective, can guide models toward a more balanced generation across demographic groups while maintaining generation quality.', 'abstract_zh': '基于文本到图像生成模型的交叉群体表征测量框架', 'title_zh': '多组比例代表制文本到图像模型'}
{'arxiv_id': 'arXiv:2505.24019', 'title': 'LLM Agents Should Employ Security Principles', 'authors': 'Kaiyuan Zhang, Zian Su, Pin-Yu Chen, Elisa Bertino, Xiangyu Zhang, Ninghui Li', 'link': 'https://arxiv.org/abs/2505.24019', 'abstract': "Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system's susceptibility to prompt injection and other forms of context manipulation introduce new vulnerabilities related to privacy leakage and system exploitation. This position paper argues that the well-established design principles in information security, which are commonly referred to as security principles, should be employed when deploying LLM agents at scale. Design principles such as defense-in-depth, least privilege, complete mediation, and psychological acceptability have helped guide the design of mechanisms for securing information systems over the last five decades, and we argue that their explicit and conscientious adoption will help secure agentic systems. To illustrate this approach, we introduce AgentSandbox, a conceptual framework embedding these security principles to provide safeguards throughout an agent's life-cycle. We evaluate with state-of-the-art LLMs along three dimensions: benign utility, attack utility, and attack success rate. AgentSandbox maintains high utility for its intended functions under both benign and adversarial evaluations while substantially mitigating privacy risks. By embedding secure design principles as foundational elements within emerging LLM agent protocols, we aim to promote trustworthy agent ecosystems aligned with user privacy expectations and evolving regulatory requirements.", 'abstract_zh': '大型语言模型（LLM）代理在利用上下文推理自动化复杂任务方面显示出巨大潜力；然而，涉及多个代理的交互和系统对提示注入和其他形式的上下文操纵的敏感性引入了与隐私泄露和系统滥用相关的新漏洞。本文认为，在大规模部署LLM代理时应采用信息安全管理中常见的基本安全原则。这些包括纵深防御、最小权限、完全中介和心理可接受性等设计原则，在过去的五十年中指导了信息安全机制的设计，并坚信它们的明确定义和审慎采用将有助于保护代理系统。为了说明这种方法，我们引入了AgentSandbox，一种嵌入这些安全原则的概念框架，以在整个代理生命周期中提供保护措施。我们使用最先进的LLM模型在三个维度上进行了评估：良性效用、攻击效用和攻击成功率。AgentSandbox在良性评估和对抗性评估下都能保持其预期功能的高度效用，同时大幅降低隐私风险。通过将安全设计原则作为新兴LLM代理协议的基础元素，我们旨在促进符合用户隐私期望和 evolving监管要求的信任代理生态系统。', 'title_zh': 'LLM代理应采用安全原则'}
{'arxiv_id': 'arXiv:2505.24012', 'title': 'Large Language Model Meets Constraint Propagation', 'authors': 'Alexandre Bonlarron, Florian Régin, Elisabetta De Maria, Jean-Charles Régin', 'link': 'https://arxiv.org/abs/2505.24012', 'abstract': "Large Language Models (LLMs) excel at generating fluent text but struggle to enforce external constraints because they generate tokens sequentially without explicit control mechanisms. GenCP addresses this limitation by combining LLM predictions with Constraint Programming (CP) reasoning, formulating text generation as a Constraint Satisfaction Problem (CSP). In this paper, we improve GenCP by integrating Masked Language Models (MLMs) for domain generation, which allows bidirectional constraint propagation that leverages both past and future tokens. This integration bridges the gap between token-level prediction and structured constraint enforcement, leading to more reliable and constraint-aware text generation. Our evaluation on COLLIE benchmarks demonstrates that incorporating domain preview via MLM calls significantly improves GenCP's performance. Although this approach incurs additional MLM calls and, in some cases, increased backtracking, the overall effect is a more efficient use of LLM inferences and an enhanced ability to generate feasible and meaningful solutions, particularly in tasks with strict content constraints.", 'abstract_zh': '大型语言模型（LLMs）在生成流畅文本方面表现出色，但在强制执行外部约束方面存在困难，因为它们是通过顺序生成词元而没有明确的控制机制。GenCP通过将LLM预测与约束编程(CP)推理相结合，将文本生成 formulations 为约束满足问题(CSP)来解决这一局限性。在本文中，我们通过集成掩蔽语言模型(MLMs)来增强GenCP的领域生成能力，这允许双向约束传播，利用既有词元和未来词元。这种集成在词元级预测和结构化约束强制执行之间建立了桥梁，从而产生了更可靠且更具约束意识的文本生成。我们在COLLIE基准上的评估表明，通过MLM调用进行领域预览显著提高了GenCP的性能。尽管这种方法会增加额外的MLM调用，甚至在某些情况下增加回溯，但总体而言，这促进了LLM推理的更高效使用，并增强了生成可行且有意义解决方案的能力，尤其是在有严格内容约束的任务中。', 'title_zh': '大规模语言模型与约束传播相遇'}
{'arxiv_id': 'arXiv:2505.24009', 'title': 'Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws', 'authors': 'Hidetaka Kamigaito, Ying Zhang, Jingun Kwon, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe', 'link': 'https://arxiv.org/abs/2505.24009', 'abstract': "Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study.", 'abstract_zh': 'Transformer架构在广泛的任务中展现出卓越的性能，现已成为大型语言模型（LLMs）的主要骨干架构。尽管参数缩放规律的研究表明增加参数量可以提升其任务解决性能，但近期关于其内在机制的机制可解释性研究并没有完全阐明这些内在机制与参数缩放规律之间的关系。为填补这一空白，我们集中在决定Transformer参数量的层及其规模上。为此，我们首先通过偏差多样性分解理论研究残差流中的层。分解将偏差（即每层输出与真实值之间的误差）和多样性（即每层输出之间的差异性）分开。在该理论下分析Transformer显示，当每层的预测接近正确答案并保持相互差异时，性能会提升。我们表明，当每层的输出远离真实值时，多样性变得尤为重要。最终，我们引入了信息论多样性，并展示了主要发现：只有当新增的层表现出差异性（即多样化）时，增加层才能提升性能。我们还揭示了通过增加层数获得的性能增益具有子模性：随着新增层的增加，边际改进逐渐减少，这与参数缩放规律预测的对数收敛相吻合。在多种语义理解任务上使用不同LLM的实验验证了本研究中理论性质的实证结果。', 'title_zh': 'Transformer 层的多样性：参数缩放法则的一个方面'}
{'arxiv_id': 'arXiv:2505.24003', 'title': 'Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting', 'authors': 'ChengAo Shen, Wenchao Yu, Ziming Zhao, Dongjin Song, Wei Cheng, Haifeng Chen, Jingchao Ni', 'link': 'https://arxiv.org/abs/2505.24003', 'abstract': 'Time series, typically represented as numerical sequences, can also be transformed into images and texts, offering multi-modal views (MMVs) of the same underlying signal. These MMVs can reveal complementary patterns and enable the use of powerful pre-trained large models, such as large vision models (LVMs), for long-term time series forecasting (LTSF). However, as we identified in this work, applying LVMs to LTSF poses an inductive bias towards "forecasting periods". To harness this bias, we propose DMMV, a novel decomposition-based multi-modal view framework that leverages trend-seasonal decomposition and a novel backcast residual based adaptive decomposition to integrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art (SOTA) models across diverse datasets show that DMMV outperforms single-view and existing multi-modal baselines, achieving the best mean squared error (MSE) on 6 out of 8 benchmark datasets.', 'abstract_zh': '时间序列，通常表示为数值序列，也可以转换为图像和文本，提供同一基础信号的多模态视图（MMVs）。这些MMVs可以揭示互补模式，并启用强大的预训练大型模型，如大型视觉模型（LVMs），用于长期时间序列预测（LTSF）。然而，正如我们在本文中所发现的，将LVMs应用于LTSF会导致一种“预测期”的归纳偏见。为了利用这种偏见，我们提出了一种新颖的基于分解的多模态视图框架DMMV，该框架利用趋势-季节分解和新颖的基于回溯残差的自适应分解来整合MMVs进行LTSF。在多种数据集上与14个最先进的（SOTA）模型的对比评估表明，DMMV在6个基准数据集中实现了最低的均方误差（MSE），优于单视图和现有多模态基线。', 'title_zh': '多模态视角增强的大视觉模型在长期时间序列预测中的应用'}
{'arxiv_id': 'arXiv:2505.24001', 'title': 'Multi-output Classification using a Cross-talk Architecture for Compound Fault Diagnosis of Motors in Partially Labeled Condition', 'authors': 'Wonjun Yi, Wonho Jung, Kangmin Jang, Yong-Hwa Park', 'link': 'https://arxiv.org/abs/2505.24001', 'abstract': "The increasing complexity of rotating machinery and the diversity of operating conditions, such as rotating speed and varying torques, have amplified the challenges in fault diagnosis in scenarios requiring domain adaptation, particularly involving compound faults. This study addresses these challenges by introducing a novel multi-output classification (MOC) framework tailored for domain adaptation in partially labeled (PL) target datasets. Unlike conventional multi-class classification (MCC) approaches, the proposed MOC framework classifies the severity levels of compound faults simultaneously. Furthermore, we explore various single-task and multi-task architectures applicable to the MOC formulation-including shared trunk and cross-talk-based designs-for compound fault diagnosis under PL conditions. Based on this investigation, we propose a novel cross-talk layer structure that enables selective information sharing across diagnostic tasks, effectively enhancing classification performance in compound fault scenarios. In addition, frequency-layer normalization was incorporated to improve domain adaptation performance on motor vibration data. Compound fault conditions were implemented using a motor-based test setup, and the proposed model was evaluated across six domain adaptation scenarios. The experimental results demonstrate its superior macro F1 performance compared to baseline models. We further showed that the proposed mode's structural advantage is more pronounced in compound fault settings through a single-fault comparison. We also found that frequency-layer normalization fits the fault diagnosis task better than conventional methods. Lastly, we discuss that this improvement primarily stems from the model's structural ability to leverage inter-fault classification task interactions, rather than from a simple increase in model parameters.", 'abstract_zh': '旋转机械日益增加的复杂性和多变的运行条件，如旋转速度和变化的扭矩，加剧了在涉及领域适应的故障诊断中的挑战，尤其是涉及复合故障的情况。本研究通过引入一种针对部分标注目标数据集（PL）领域适应的新颖多输出分类（MOC）框架来应对这些挑战。与传统的多类分类（MCC）方法不同，所提出的工作框架同时对复合故障的严重程度进行分类。此外，我们探讨了适用于MOC表示的各种单任务和多任务架构，包括共享主干和基于互扰的设计，并在部分标注条件下对复合故障进行诊断。基于这项研究，我们提出了一种新颖的互扰层结构，能够选择性地在诊断任务之间共享信息，从而有效提高在复合故障场景中的分类性能。此外，我们引入了频率层归一化，以提高对电机振动数据的领域适应性能。使用基于电机的测试装置实现复合故障条件，并在六个领域适应场景中评估所提出的模型。实验结果表明，与基线模型相比，其宏观F1性能更优。进一步研究表明，在复合故障设置中，所提模型的结构优势尤为明显。我们还发现，频率层归一化比传统方法更适合故障诊断任务。最后，我们讨论了这种改进主要来自于模型结构利用跨故障分类任务交互的能力，而不仅仅是模型参数的简单增加。', 'title_zh': '在部分标注条件下使用交叉对话架构进行电机复合故障诊断的多输出分类'}
{'arxiv_id': 'arXiv:2505.23996', 'title': 'Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs', 'authors': 'Yinong Oliver Wang, Nivedha Sivakumar, Falaah Arif Khan, Rin Metcalf Susa, Adam Golinski, Natalie Mackraz, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff', 'link': 'https://arxiv.org/abs/2505.23996', 'abstract': 'The recent rapid adoption of large language models (LLMs) highlights the critical need for benchmarking their fairness. Conventional fairness metrics, which focus on discrete accuracy-based evaluations (i.e., prediction correctness), fail to capture the implicit impact of model uncertainty (e.g., higher model confidence about one group over another despite similar accuracy). To address this limitation, we propose an uncertainty-aware fairness metric, UCerF, to enable a fine-grained evaluation of model fairness that is more reflective of the internal bias in model decisions compared to conventional fairness measures. Furthermore, observing data size, diversity, and clarity issues in current datasets, we introduce a new gender-occupation fairness evaluation dataset with 31,756 samples for co-reference resolution, offering a more diverse and suitable dataset for evaluating modern LLMs. We establish a benchmark, using our metric and dataset, and apply it to evaluate the behavior of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness due to high confidence in incorrect predictions, a detail overlooked by Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark, which evaluates fairness with uncertainty awareness, paves the way for developing more transparent and accountable AI systems.', 'abstract_zh': '近期大型语言模型（LLMs）的迅速采用凸显了对其公平性进行基准测试的紧迫需求。传统公平性度量关注基于离散准确性的评估（即预测正确性），未能捕捉到模型不确定性的影响（如，在相似准确性的条件下，模型对某一组比另一组更具信心）。为解决这一局限，我们提出了一种基于不确定性的公平性度量UCerF，以实现对模型公平性的细粒度评估，该评估更能反映模型决策中的内部偏差，相较于传统的公平性度量。此外，鉴于当前数据集在数据量、多样性和清晰度方面存在的问题，我们引入了一个新的包含31,756个样本的性别-职业公平性评估数据集，用于共指消解，提供了一个更多样化和合适的评价现代LLMs的基准数据集。我们构建了一个基准，使用我们的度量和数据集，并将其应用于评估十种开源LLMs的行为。例如，Mistral-7B由于对错误预测的高度信心而表现出次优化的公平性，这一细节被平等机会所忽视，但被UCerF捕捉到。总体而言，我们的基于不确定性的LLM基准，能够评估公平性，为开发更透明和负责任的人工智能系统铺平了道路。', 'title_zh': '你的模型有多公平？面向LLMs的不确定性感知公平性评估'}
{'arxiv_id': 'arXiv:2505.23987', 'title': 'Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization', 'authors': 'Vishal Dey, Xiao Hu, Xia Ning', 'link': 'https://arxiv.org/abs/2505.23987', 'abstract': 'In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria. However, existing computational approaches and instruction-tuned LLMs fail to capture such nuanced property-specific objectives, limiting their practical applicability. To address this, we introduce C-MuMOInstruct, the first instruction-tuning dataset focused on multi-property optimization with explicit, property-specific objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of instruction-tuned LLMs that can perform targeted property-specific optimization. Our experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions. This offers a step toward a foundational LLM to support realistic, diverse optimizations with property-specific objectives. C-MuMOInstruct and code are accessible through this https URL.', 'abstract_zh': '在现实药物设计中，分子优化需要选择性地提高多种分子性质至药用相关水平，同时保持其他已经达到此类标准的性质。然而，现有的计算方法和指令调优的大语言模型未能捕捉到这种细微的性质特定目标，限制了它们的实际应用。为了解决这一问题，我们引入了C-MuMOInstruct，这是首个专注于多性质优化并明确包含性质特定目标的指令调难题集。利用C-MuMOInstruct，我们开发了GeLLMO-Cs系列指令调 tuning 大语言模型，能够进行目标导向的性质特定优化。我们的实验在5个分布内和5个分布外任务中表明，GeLLMO-Cs在所有任务中均显著优于Strong Baselines，成功率最高可提高126%。值得注意的是，GeLLMO-Cs在新型优化任务和未见过的指令上展现出了出色的零样本泛化能力。这朝着支持具有性质特定目标的现实多样化优化的基础大语言模型迈出了一步。C-MuMOInstruct和代码可通过以下链接获取。', 'title_zh': '大型语言模型在可控多属性多目标分子优化中的应用'}
{'arxiv_id': 'arXiv:2505.23977', 'title': 'VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL', 'authors': 'Yichen Feng, Zhangchen Xu, Fengqing Jiang, Yuetai Li, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran', 'link': 'https://arxiv.org/abs/2505.23977', 'abstract': 'Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.', 'abstract_zh': '视觉语言模型（VLMs）被预期能够实现有效的多模态推理并做出逻辑连贯的决策，这对于图示理解与空间问题求解等任务至关重要。然而，当前的VLM推理缺乏大规模且结构良好的训练数据集。为解决这一问题，我们提出VisualSphinx，这是一个首创的大规模合成视觉逻辑推理训练数据集。为应对基于答案的图像合成挑战，我们提出了一种规则到图像合成管道，该管道从种子问题中提取并扩展谜题规则，并生成用于谜题样本组装的接地合成图像生成代码。实验表明，使用VisualSphinx上的GRPO训练的VLM可以从我们数据集的逻辑连贯性和可读性中受益，并在逻辑推理任务中表现出改进的性能。从VisualSphinx中获得的增强推理能力也惠及其他推理任务，如代数推理、算术推理和几何推理。', 'title_zh': 'VisualSphinx: 大规模合成视觉逻辑谜题用于RL'}
{'arxiv_id': 'arXiv:2505.23968', 'title': 'Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention', 'authors': 'Stephan Rabanser, Ali Shahin Shamsabadi, Olive Franzese, Xiao Wang, Adrian Weller, Nicolas Papernot', 'link': 'https://arxiv.org/abs/2505.23968', 'abstract': "Cautious predictions -- where a machine learning model abstains when uncertain -- are crucial for limiting harmful errors in safety-critical applications. In this work, we identify a novel threat: a dishonest institution can exploit these mechanisms to discriminate or unjustly deny services under the guise of uncertainty. We demonstrate the practicality of this threat by introducing an uncertainty-inducing attack called Mirage, which deliberately reduces confidence in targeted input regions, thereby covertly disadvantaging specific individuals. At the same time, Mirage maintains high predictive performance across all data points. To counter this threat, we propose Confidential Guardian, a framework that analyzes calibration metrics on a reference dataset to detect artificially suppressed confidence. Additionally, it employs zero-knowledge proofs of verified inference to ensure that reported confidence scores genuinely originate from the deployed model. This prevents the provider from fabricating arbitrary model confidence values while protecting the model's proprietary details. Our results confirm that Confidential Guardian effectively prevents the misuse of cautious predictions, providing verifiable assurances that abstention reflects genuine model uncertainty rather than malicious intent.", 'abstract_zh': '谨慎预测——当机器学习模型在不确定时弃权对于限制安全关键应用中的有害错误至关重要。在本文中，我们识别了一种新型威胁：不诚实的机构可以通过利用这些机制在借口不确定性的情况下进行歧视或不公平地拒绝服务。我们通过引入一种称为Mirage的不确定性诱导攻击来证明这一威胁的可行性，这种攻击故意降低目标输入区域的置信度，从而秘密地对特定个体造成不利影响。同时，Mirage在所有数据点上保持了高预测性能。为了应对这一威胁，我们提出了一种名为Confidential Guardian的框架，该框架通过在参考数据集上分析校准指标来检测人为压制的置信度，并采用经过验证的推断的零知识证明来确保报告的置信分数确实源自部署的模型。这可以防止提供者伪造任意模型置信度值，同时保护模型的专有细节。我们的结果显示，Confidential Guardian有效地防止了谨慎预测的滥用，提供了可验证的保障，以确保弃权反映的是真实的模型不确定性而非恶意意图。', 'title_zh': '保密守护者：密码学禁止模型弃权滥用'}
{'arxiv_id': 'arXiv:2505.23960', 'title': 'Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation', 'authors': 'Henry Conklin', 'link': 'https://arxiv.org/abs/2505.23960', 'abstract': 'Despite the remarkable success of large large-scale neural networks, we still lack unified notation for thinking about and describing their representational spaces. We lack methods to reliably describe how their representations are structured, how that structure emerges over training, and what kinds of structures are desirable. This thesis introduces quantitative methods for identifying systematic structure in a mapping between spaces, and leverages them to understand how deep-learning models learn to represent information, what representational structures drive generalisation, and how design decisions condition the structures that emerge. To do this I identify structural primitives present in a mapping, along with information theoretic quantifications of each. These allow us to analyse learning, structure, and generalisation across multi-agent reinforcement learning models, sequence-to-sequence models trained on a single task, and Large Language Models. I also introduce a novel, performant, approach to estimating the entropy of vector space, that allows this analysis to be applied to models ranging in size from 1 million to 12 billion parameters.\nThe experiments here work to shed light on how large-scale distributed models of cognition learn, while allowing us to draw parallels between those systems and their human analogs. They show how the structures of language and the constraints that give rise to them in many ways parallel the kinds of structures that drive performance of contemporary neural networks.', 'abstract_zh': '尽管大规模神经网络取得了显著的成功，但我们仍然缺乏统一的表示空间思考和描述的符号体系。我们缺乏可靠的方法来描述其表示的结构，这些结构如何在训练中涌现，以及哪些结构是可取的。本论文引入了量化方法来识别映射空间之间系统结构，并利用这些方法来理解深度学习模型如何学习表示信息，哪些表示结构驱动泛化，以及设计决策如何影响涌现的结构。通过识别映射中存在的结构性质及其信息论量化的每个性质，我们能够分析多智能体强化学习模型、单任务训练的序列到序列模型以及大型语言模型中的学习、结构和泛化。我还提出了一个新的高效方法来估计向量空间的熵，从而使这一分析能够应用于从100万到120亿参数的模型。实验旨在揭示大规模分布式认知模型的学习机制，并将这些系统与其类比的人类系统进行比较。它们展示了语言结构及其在许多方面类似当代神经网络性能驱动结构的约束。', 'title_zh': '映射中的信息结构：一种学习、表示和泛化的Approach'}
{'arxiv_id': 'arXiv:2505.23953', 'title': 'Enhancing LLM-Based Code Generation with Complexity Metrics: A Feedback-Driven Approach', 'authors': 'Melika Sepidband, Hamed Taherkhani, Song Wang, Hadi Hemmati', 'link': 'https://arxiv.org/abs/2505.23953', 'abstract': "Automatic code generation has gained significant momentum with the advent of Large Language Models (LLMs) such as GPT-4. Although many studies focus on improving the effectiveness of LLMs for code generation, very limited work tries to understand the generated code's characteristics and leverage that to improve failed cases. In this paper, as the most straightforward characteristic of code, we investigate the relationship between code complexity and the success of LLM generated code. Using a large set of standard complexity metrics, we first conduct an empirical analysis to explore their correlation with LLM's performance on code generation (i.e., Pass@1). Using logistic regression models, we identify which complexity metrics are most predictive of code correctness. Building on these findings, we propose an iterative feedback method, where LLMs are prompted to generate correct code based on complexity metrics from previous failed outputs. We validate our approach across multiple benchmarks (i.e., HumanEval, MBPP, LeetCode, and BigCodeBench) and various LLMs (i.e., GPT-4o, GPT-3.5 Turbo, Llama 3.1, and GPT-o3 mini), comparing the results with two baseline methods: (a) zero-shot generation, and (b) iterative execution-based feedback without our code complexity insights. Experiment results show that our approach makes notable improvements, particularly with a smaller LLM (GPT3.5 Turbo), where, e.g., Pass@1 increased by 35.71% compared to the baseline's improvement of 12.5% on the HumanEval dataset. The study expands experiments to BigCodeBench and integrates the method with the Reflexion code generation agent, leading to Pass@1 improvements of 20% (GPT-4o) and 23.07% (GPT-o3 mini). The results highlight that complexity-aware feedback enhances both direct LLM prompting and agent-based workflows.", 'abstract_zh': '自动代码生成随着大型语言模型（LLMs）如GPT-4的出现获得了显著的发展。虽然许多研究关注于提高LLMs在代码生成方面的有效性，但很少有研究尝试理解生成代码的特性，并利用这些特性来改进失败的情况。本文中，我们作为代码最直接的特性，探讨代码复杂性和LLM生成代码成功率之间的关系。通过使用大量标准复杂性度量标准，我们首先进行实证分析，探索这些度量标准与LLM在代码生成任务上的表现（即Pass@1）之间的相关性。通过逻辑回归模型，我们确定了哪些复杂性度量指标最能预测代码的正确性。基于这些发现，我们提出了一种迭代反馈方法，其中LLM根据先前失败输出的复杂性指标生成正确的代码。我们跨越多个基准（即HumanEval、MBPP、LeetCode和BigCodeBench）和多种LLM（即GPT-4o、GPT-3.5 Turbo、Llama 3.1和GPT-o3 mini）验证了我们的方法，并将结果与两种基线方法进行了比较：（a）零样本生成，（b）基于迭代执行的反馈，但不使用我们的代码复杂性见解。实验结果表明，我们的方法取得了明显的改进，特别是在较小的LLM（GPT3.5 Turbo）上，例如，在HumanEval数据集上，Pass@1提高了35.71%，而基线方法的改进仅为12.5%。研究扩展到BigCodeBench，并与Reflexion代码生成代理结合使用，导致Pass@1分别提高了20%（GPT-4o）和23.07%（GPT-o3 mini）。结果表明，复杂性意识的反馈不仅增强了直接的LLM提示，还增强了基于代理的工作流程。', 'title_zh': '基于复杂度度量增强的LLM代码生成：一种反馈驱动的方法'}
{'arxiv_id': 'arXiv:2505.23949', 'title': 'TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks', 'authors': 'Xiang Meng, Mehdi Makni, Rahul Mazumder', 'link': 'https://arxiv.org/abs/2505.23949', 'abstract': "Network pruning reduces the computational requirements of large neural networks, with N:M sparsity -- retaining only N out of every M consecutive weights -- offering a compelling balance between compressed model quality and hardware acceleration. However, N:M sparsity only accelerates forward-pass computations, as N:M patterns are not preserved during matrix transposition, limiting efficiency during training where both passes are computationally intensive. While transposable N:M sparsity has been proposed to address this limitation, existing methods for finding transposable N:M sparse masks either fail to scale to large models or are restricted to M=4 which results in suboptimal compression-accuracy trade-off. We introduce an efficient solver for transposable N:M masks that scales to billion-parameter models. We formulate mask generation as optimal transport problems and solve through entropy regularization and Dykstra's algorithm, followed by a rounding procedure. Our tensor-based implementation exploits GPU parallelism, achieving up to 100x speedup with only 1-10% error compared to existing methods. Our approach can be integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT and ALPS to produce transposable N:M sparse models with arbitrary N:M values. Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains performance close to its standard N:M counterpart and outperforms standard 2:4 sparse model, showing the practical value of our approach.", 'abstract_zh': '可迁移的N:M稀疏性提高了大规模神经网络的计算效率，但矩阵转置期间N:M模式的丢失限制了其在训练过程中的效率。尽管已经提出了可迁移的N:M稀疏性来解决这一限制，现有方法要么无法扩展到大型模型，要么仅限于M=4，导致压缩-准确性权衡不佳。我们引入了一种可扩展到十亿参数模型的高效可迁移N:M稀疏性掩码求解器。我们将掩码生成公式化为最优传输问题，并通过熵正则化和迪克斯特拉算法结合舍入步骤进行求解。基于张量的实现利用了GPU并行性，在保持1-10%误差的情况下实现了高达100倍的加速。我们的方法可以与LayerWise N:M 神经网络剪枝框架（如Wanda、SparseGPT和ALPS）集成，生成具有任意N:M值的可迁移N:M稀疏模型。实验表明，具有可迁移16:32稀疏性的LLaMA3.2-8B在性能上接近其标准的N:M对等模型，并且优于标准的2:4稀疏模型，展示了我们方法的实际价值。', 'title_zh': 'TSENOR: 高效的寻找可移位N:M稀疏掩码算法'}
{'arxiv_id': 'arXiv:2505.23947', 'title': 'Position: The Future of Bayesian Prediction Is Prior-Fitted', 'authors': 'Samuel Müller, Arik Reuter, Noah Hollmann, David Rügamer, Frank Hutter', 'link': 'https://arxiv.org/abs/2505.23947', 'abstract': 'Training neural networks on randomly generated artificial datasets yields Bayesian models that capture the prior defined by the dataset-generating distribution. Prior-data Fitted Networks (PFNs) are a class of methods designed to leverage this insight. In an era of rapidly increasing computational resources for pre-training and a near stagnation in the generation of new real-world data in many applications, PFNs are poised to play a more important role across a wide range of applications. They enable the efficient allocation of pre-training compute to low-data scenarios. Originally applied to small Bayesian modeling tasks, the field of PFNs has significantly expanded to address more complex domains and larger datasets. This position paper argues that PFNs and other amortized inference approaches represent the future of Bayesian inference, leveraging amortized learning to tackle data-scarce problems. We thus believe they are a fruitful area of research. In this position paper, we explore their potential and directions to address their current limitations.', 'abstract_zh': '训练随机生成的仿制数据集的神经网络会产生捕捉数据集生成分布先验的贝叶斯模型。先验-数据拟合网络（PFNs）是一类旨在利用这一洞察的方法。在计算资源用于预训练迅速增加而许多应用中新现实数据生成近于停滞的时代，PFNs有望在广泛的应用中发挥更重要的作用。它们能使预训练计算资源更高效地分配到数据稀少场景中。最初应用于小型贝叶斯建模任务，PFN领域已经显著扩展以应对更复杂的领域和更大的数据集。本文认为，PFNs和其他拟合性推理方法是贝叶斯推理的未来，通过拟合性学习来应对数据稀少的问题。因此，我们认为这是一条富有成果的研究方向。在本文中，我们探讨了它们的潜在应用和改进方向以解决当前的局限。', 'title_zh': '位置：Bayesian预测的未来是先验匹配'}
{'arxiv_id': 'arXiv:2505.23945', 'title': 'A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models', 'authors': 'Sriram Balasubramanian, Samyadeep Basu, Soheil Feizi', 'link': 'https://arxiv.org/abs/2505.23945', 'abstract': "Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent'' reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.", 'abstract_zh': 'Chain-of-Thought (CoT)推理增强了大型语言模型的性能，但关于这些推理痕迹是否真实反映模型内部过程的问题仍然存在。我们首次全面研究了大型视觉-语言模型（LVLM）中的CoT可信度，探讨了基于文本和之前未被探索的基于图像的偏差如何影响推理和偏倚表达。我们的工作引入了一个新颖的细粒度评估管道，用于分类偏倚表达模式，使CoT推理的分析比之前的方法更加精确。该框架揭示了模型在处理不同类型偏倚时的重要区别，为LVLM CoT可信度提供了新的见解。研究发现，与显式的基于文本的偏倚相比，微妙的基于图像的偏倚几乎从未被表达出来，即使是在专门用于推理的模型中。此外，许多模型表现出一种先前未被识别的现象，我们称之为“不一致”推理——正确推理之后突然改变答案，这可能是检测不公平推理的潜在标志。然后，我们应用相同的评估管道重新审视了不同隐含线索层次上的LLM的CoT可信度。研究发现，当前仅依赖语言的推理模型仍然难以表达未明确陈述的线索。', 'title_zh': '对大型（视觉）语言模型偏差和链式思维忠实性的深入考察'}
{'arxiv_id': 'arXiv:2505.23942', 'title': 'SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations', 'authors': 'Gaurav Sarkar, Jay Gala, Subarna Tripathi', 'link': 'https://arxiv.org/abs/2505.23942', 'abstract': "The design of activation functions remains a pivotal component in optimizing deep neural networks. While prevailing choices like Swish and GELU demonstrate considerable efficacy, they often exhibit domain-specific optima. This work introduces SG-Blend, a novel activation function that blends our proposed SSwish, a first-order symmetric variant of Swish and the established GELU through dynamic interpolation. By adaptively blending these constituent functions via learnable parameters, SG-Blend aims to harness their complementary strengths: SSwish's controlled non-monotonicity and symmetry, and GELU's smooth, probabilistic profile, to achieve a more universally robust balance between model expressivity and gradient stability. We conduct comprehensive empirical evaluations across diverse modalities and architectures, showing performance improvements across all considered natural language and computer vision tasks and models. These results, achieved with negligible computational overhead, underscore SG-Blend's potential as a versatile, drop-in replacement that consistently outperforms strong contemporary baselines. The code is available at this https URL.", 'abstract_zh': '激活函数的设计仍然是优化深层神经网络的关键组件。虽然现有的选择如Swish和GELU表现出显著的效果，但它们往往在特定领域表现出最优性。本文引入了SG-Blend，这是一种新的激活函数，通过动态插值结合了我们提出的SSwish（Swish的一阶对称变体）和已有的GELU。通过使用可学习参数自适应地结合这些组成函数，SG-Blend旨在利用它们的互补优势：SSwish的可控非单调性和对称性以及GELU的平滑和概率特性，以实现模型表达能力和梯度稳定性之间的更通用稳健平衡。我们在多种模态和架构下进行了全面的经验评估，展示了在所有考虑的自然语言处理和计算机视觉任务和模型中的性能改进。这些结果表明，SG-Blend在几乎不增加计算开销的情况下，具有作为多功能且可直接替代的潜在能力，通常优于强大的当代基线。代码可在以下链接获取。', 'title_zh': 'SG-Blend: 学习改进Swish和GELU之间的插值以获得稳健的神经表示'}
{'arxiv_id': 'arXiv:2505.23933', 'title': 'BIRD: Behavior Induction via Representation-structure Distillation', 'authors': 'Galen Pogoncheff, Michael Beyeler', 'link': 'https://arxiv.org/abs/2505.23933', 'abstract': "Human-aligned deep learning models exhibit behaviors consistent with human values, such as robustness, fairness, and honesty. Transferring these behavioral properties to models trained on different tasks or data distributions remains challenging: aligned behavior is easily forgotten during fine-tuning, and collecting task-specific data that preserves this behavior can be prohibitively costly. We introduce BIRD (Behavior Induction via Representation-structure Distillation), a flexible framework for transferring aligned behavior by matching the internal representation structure of a student model to that of a teacher. Applied to out-of-distribution robustness in image classification, BIRD outperforms fine-tuning, transfer learning, and continual learning methods, improving robust accuracy by up to 16% over the next strongest baseline. It remains effective even when the teacher is trained on a much simpler dataset and is $25 \\times$ smaller than the student. In a large-scale study of over 400 teacher-student pairs, we show that three interpretable and computable properties of the teacher's representations (i.e., task relevance, behavioral relevance, and complementary knowledge) explain up to 85% of the variance in transfer success. These insights offer practical guidance for teacher selection and design. BIRD turns small, well-aligned models into scalable alignment seeds, removing a key bottleneck in deploying safe AI systems in the wild.", 'abstract_zh': '基于表示结构蒸馏的行为传递：BIRD框架', 'title_zh': 'BIRD: 基于表示结构蒸馏的行为诱导'}
{'arxiv_id': 'arXiv:2505.23931', 'title': 'Scaling up the think-aloud method', 'authors': 'Daniel Wurgaft, Ben Prystawski, Kanishk Gandhi, Cedegao E. Zhang, Joshua B. Tenenbaum, Noah D. Goodman', 'link': 'https://arxiv.org/abs/2505.23931', 'abstract': 'The think-aloud method, where participants voice their thoughts as they solve a task, is a valuable source of rich data about human reasoning processes. Yet, it has declined in popularity in contemporary cognitive science, largely because labor-intensive transcription and annotation preclude large sample sizes. Here, we develop methods to automate the transcription and annotation of verbal reports of reasoning using natural language processing tools, allowing for large-scale analysis of think-aloud data. In our study, 640 participants thought aloud while playing the Game of 24, a mathematical reasoning task. We automatically transcribed the recordings and coded the transcripts as search graphs, finding moderate inter-rater reliability with humans. We analyze these graphs and characterize consistency and variation in human reasoning traces. Our work demonstrates the value of think-aloud data at scale and serves as a proof of concept for the automated analysis of verbal reports.', 'abstract_zh': '自述法，即参与者在完成任务时口头表达其思考过程的方法，是研究人类推理过程的重要数据来源。然而，由于劳动密集型的转录和标注工作限制了样本量，该方法在当代认知科学中的应用已大大减少。本文开发了使用自然语言处理工具自动转录和标注推理口头报告的方法，以便大规模分析自述数据。在我们的研究中，640名参与者在玩24点游戏（一个数学推理任务）时进行了自述。我们自动转录了录音，并将转录文本编码为搜索图，与人工标注具有 Moderate 的一致可靠性。我们分析这些图，描述了人类推理痕迹的一致性和差异性。我们的研究证明了大规模自述数据的价值，并且展示了自动化分析口头报告的可行性。', 'title_zh': '扩大思维 aloud 方法的应用规模'}
{'arxiv_id': 'arXiv:2505.23930', 'title': 'Exploring Societal Concerns and Perceptions of AI: A Thematic Analysis through the Lens of Problem-Seeking', 'authors': 'Naomi Omeonga wa Kayembe', 'link': 'https://arxiv.org/abs/2505.23930', 'abstract': "This study introduces a novel conceptual framework distinguishing problem-seeking from problem-solving to clarify the unique features of human intelligence in contrast to AI. Problem-seeking refers to the embodied, emotionally grounded process by which humans identify and set goals, while problem-solving denotes the execution of strategies aimed at achieving such predefined objectives. The framework emphasizes that while AI excels at efficiency and optimization, it lacks the orientation derived from experiential grounding and the embodiment flexibility intrinsic to human cognition. To empirically explore this distinction, the research analyzes metadata from 157 YouTube videos discussing AI. Conducting a thematic analysis combining qualitative insights with keyword-based quantitative metrics, this mixed-methods approach uncovers recurring themes in public discourse, including privacy, job displacement, misinformation, optimism, and ethical concerns. The results reveal a dual sentiment: public fascination with AI's capabilities coexists with anxiety and skepticism about its societal implications. The discussion critiques the orthogonality thesis, which posits that intelligence is separable from goal content, and instead argues that human intelligence integrates goal-setting and goal-pursuit. It underscores the centrality of embodied cognition in human reasoning and highlights how AI's limitations come from its current reliance on computational processing. The study advocates for enhancing emotional and digital literacy to foster responsible AI engagement. It calls for reframing public discourse to recognize AI as a tool that augments -- rather than replaces -- human intelligence. By positioning problem seeking at the core of cognition and as a critical dimension of intelligence, this research offers new perspectives on ethically aligned and human-centered AI development.", 'abstract_zh': '本研究引入了一个新颖的概念框架，区分问题寻找与问题解决，以阐明人类智能与AI的独特特征。问题寻找是指人类识别并设定目标的具身、情感导向过程，而问题解决则指执行旨在达成这些预定义目标的策略。该框架强调，尽管AI在效率和优化方面表现优异，但它缺乏基于经验 grounding 和人类认知本质具身灵活性的方向性。为了实证探索这一区别，研究分析了157个YouTube视频的元数据，这些视频讨论了AI。运用结合定性洞见与基于关键词的定量指标的主题分析方法，这种混合方法揭示了公共话语中的重复主题，包括隐私、就业替代、虚假信息、乐观主义和伦理关切。研究结果揭示了一种双重情感：公众对AI能力的兴趣与对其社会影响的焦虑和怀疑共存。讨论批评了正交性论点，该论点认为智能可以独立于目标内容，而是认为人类智能将目标设定与目标追求紧密结合。研究强调了具身认知在人类推理中的核心地位，并指出了AI当前依赖于计算处理导致的局限性。研究倡导增强情感和数字素养，以促进负责任的AI参与。研究呼吁重新构架公众话语，承认AI作为一种增强——而非替代——人类智能的工具。通过将问题寻找置于认知的核心，并作为智能的关键维度，本研究为伦理对齐和以人类为中心的AI发展提供了新的视角。', 'title_zh': '探究关于AI的社会关注与感知：问题探讨视角的主题分析'}
{'arxiv_id': 'arXiv:2505.23923', 'title': 'ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents', 'authors': 'Feiteng Fang, Ting-En Lin, Yuchuan Wu, Xiong Liu, Xiang Huang, Dingwei Chen, Jing Ye, Haonan Zhang, Liang Zhu, Hamid Alinejad-Rokny, Min Yang, Fei Huang, Yongbin Li', 'link': 'https://arxiv.org/abs/2505.23923', 'abstract': 'Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic and engaging human-computer interactions. However, traditional reward models often struggle with scalability and adapting to subjective conversational preferences. We propose ChARM, a Character-based Act-adaptive Reward Model, addressing these challenges through two innovations: (1) an act-adaptive margin that significantly enhances learning efficiency and generalizability, and (2) a self-evolution mechanism leveraging large-scale unlabeled data to improve training coverage. Additionally, we introduce RoleplayPref, the first large-scale preference dataset specifically for RPLAs, featuring 1,108 characters, 13 subcategories, and 16,888 bilingual dialogues, alongside RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13% improvement over the conventional Bradley-Terry model in preference rankings. Furthermore, applying ChARM-generated rewards to preference learning techniques (e.g., direct preference optimization) achieves state-of-the-art results on CharacterEval and RoleplayEval. Code and dataset are available at this https URL.', 'abstract_zh': '基于角色的行动自适应奖励模型（ChARM）旨在模拟角色以实现真实且互动的人机对话。然而，传统的奖励模型往往在可扩展性和适应主观对话偏好方面存在困难。我们提出了一种基于角色的行动自适应奖励模型（ChARM），通过两项创新解决这些挑战：（1）行动自适应边际，显著提高了学习效率和普适性；（2）利用大规模未标注数据的自我演化机制，提高训练覆盖范围。此外，我们引入了首个专门针对角色扮演语言代理（RPLAs）的大规模偏好数据集——RoleplayPref，包含1,108个角色、13个子类别和16,888个双语对话，以及专门的评估基准——RoleplayEval。实验结果显示，ChARM在偏好排名上比传统的Bradley-Terry模型提高了13%。进一步将ChARM生成的奖励应用于偏好学习技术（例如直接偏好优化）在CharacterEval和RoleplayEval上达到了最先进的结果。代码和数据集可在以下链接获取。', 'title_zh': '基于字符的适应性奖励建模以优化高级角色扮演语言代理'}
{'arxiv_id': 'arXiv:2505.23917', 'title': 'Representational Difference Explanations', 'authors': 'Neehar Kondapaneni, Oisin Mac Aodha, Pietro Perona', 'link': 'https://arxiv.org/abs/2505.23917', 'abstract': 'We propose a method for discovering and visualizing the differences between two learned representations, enabling more direct and interpretable model comparisons. We validate our method, which we call Representational Differences Explanations (RDX), by using it to compare models with known conceptual differences and demonstrate that it recovers meaningful distinctions where existing explainable AI (XAI) techniques fail. Applied to state-of-the-art models on challenging subsets of the ImageNet and iNaturalist datasets, RDX reveals both insightful representational differences and subtle patterns in the data. Although comparison is a cornerstone of scientific analysis, current tools in machine learning, namely post hoc XAI methods, struggle to support model comparison effectively. Our work addresses this gap by introducing an effective and explainable tool for contrasting model representations.', 'abstract_zh': '我们提出了一种发现和可视化两种学习表示之间差异的方法，从而实现更直接和可解释的模型比较。我们通过使用该方法比较具有已知概念差异的模型，并证明它能够在现有可解释人工智能(XAI)技术失效的情况下恢复有意义的区分，验证了该方法的有效性。我们将其称为表示差异解释(RDX)。应用于ImageNet和iNaturalist数据集具有挑战性的子集中的最新模型，RDX揭示了有益的表示差异和数据中的细微模式。尽管比较是科学研究的核心，当前机器学习中的工具，特别是事后可解释性人工智能(XAI)方法，难以有效地支持模型比较。我们的工作通过引入一个有效的可解释工具来对比模型表示，填补了这一空白。', 'title_zh': '表征差异解释'}
{'arxiv_id': 'arXiv:2505.23914', 'title': 'Probing Association Biases in LLM Moderation Over-Sensitivity', 'authors': 'Yuxin Wang, Botao Yu, Ivory Yang, Saeed Hassanpour, Soroush Vosoughi', 'link': 'https://arxiv.org/abs/2505.23914', 'abstract': "Large Language Models are widely used for content moderation but often misclassify benign comments as toxic, leading to over-sensitivity. While previous research attributes this issue primarily to the presence of offensive terms, we reveal a potential cause beyond token level: LLMs exhibit systematic topic biases in their implicit associations. Inspired by cognitive psychology's implicit association tests, we introduce Topic Association Analysis, a semantic-level approach to quantify how LLMs associate certain topics with toxicity. By prompting LLMs to generate free-form scenario imagination for misclassified benign comments and analyzing their topic amplification levels, we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger topic stereotype despite lower overall false positive rates. These biases suggest that LLMs do not merely react to explicit, offensive language but rely on learned topic associations, shaping their moderation decisions. Our findings highlight the need for refinement beyond keyword-based filtering, providing insights into the underlying mechanisms driving LLM over-sensitivity.", 'abstract_zh': '大规模语言模型广泛用于内容审核，但 often misclassify benign comments as toxic, leading to over-sensitivity. 被翻译为：大规模语言模型在内容审核中广泛应用，但常常误将无害评论分类为有毒评论，导致过度敏感。', 'title_zh': '探究大模型 Moderation 过度敏感中的关联偏见'}
{'arxiv_id': 'arXiv:2505.23912', 'title': 'Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation', 'authors': 'Caiqi Zhang, Xiaochen Zhu, Chengzu Li, Nigel Collier, Andreas Vlachos', 'link': 'https://arxiv.org/abs/2505.23912', 'abstract': 'Hallucination remains a major challenge for the safe and trustworthy deployment of large language models (LLMs) in factual content generation. Prior work has explored confidence estimation as an effective approach to hallucination detection, but often relies on post-hoc self-consistency methods that require computationally expensive sampling. Verbalized confidence offers a more efficient alternative, but existing approaches are largely limited to short-form question answering (QA) tasks and do not generalize well to open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized Confidence), an on-the-fly verbalized confidence estimation method for long-form generation. Specifically, we use reinforcement learning (RL) to train LLMs to append numerical confidence scores to each generated statement, serving as a direct and interpretable signal of the factuality of generation. Our experiments consider both on-policy and off-policy RL methods, including DPO, ORPO, and GRPO, to enhance the model calibration. We introduce two novel evaluation settings, free-form tagging and iterative tagging, to assess different verbalized confidence estimation methods. Experiments on three long-form QA datasets show that our RL-trained models achieve better calibration and generalize robustly across domains. Also, our method is highly efficient, as it only requires adding a few tokens to the output being decoded.', 'abstract_zh': '长文档生成中的即时口头化置信度估计方法：LoVeC', 'title_zh': '强化学习以提高长文生成中的口头化自信度'}
{'arxiv_id': 'arXiv:2505.23879', 'title': 'CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data', 'authors': 'Caio Cheohen, Vinnícius M. S. Gomes, Manuela L. da Silva', 'link': 'https://arxiv.org/abs/2505.23879', 'abstract': 'The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need for accurate prediction of disease severity to optimize healthcare resource allocation and patient management. The spike protein, which facilitates viral entry into host cells, exhibits high mutation rates, particularly in the receptor-binding domain, influencing viral pathogenicity. Artificial intelligence approaches, such as deep learning, offer promising solutions for leveraging genomic and clinical data to predict disease outcomes. Objective: This study aimed to develop a hybrid CNN-LSTM deep learning model to predict COVID-19 severity using spike protein sequences and associated clinical metadata from South American patients. Methods: We retrieved 9,570 spike protein sequences from the GISAID database, of which 3,467 met inclusion criteria after standardization. The dataset included 2,313 severe and 1,154 mild cases. A feature engineering pipeline extracted features from sequences, while demographic and clinical variables were one-hot encoded. A hybrid CNN-LSTM architecture was trained, combining CNN layers for local pattern extraction and an LSTM layer for long-term dependency modeling. Results: The model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%, and recall of 82.85%, demonstrating robust classification performance. Training stabilized at 85% accuracy with minimal overfitting. The most prevalent lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional epidemiological trends, suggesting potential associations between viral genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model effectively predicted COVID-19 severity using spike protein sequences and clinical data, highlighting the utility of AI in genomic surveillance and precision public health. Despite limitations, this approach provides a framework for early severity prediction in future outbreaks.', 'abstract_zh': 'COVID-19疫情背景下基于刺突蛋白序列和临床元数据的混合CNN-LSTM深度学习模型在南美患者中预测疾病严重程度的研究', 'title_zh': '基于CNN-LSTM混合模型的AI驱动的新冠肺炎严重程度预测方法：从刺突序列和临床数据出发'}
{'arxiv_id': 'arXiv:2505.23878', 'title': 'Actor-Critic based Online Data Mixing For Language Model Pre-Training', 'authors': 'Jing Ma, Chenhao Dang, Mingjie Liao', 'link': 'https://arxiv.org/abs/2505.23878', 'abstract': 'The coverage and composition of pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). To reduce the carbon footprint and financial costs of training, some data mixing methods, which applied the optimized domain weights of a small proxy model to train a larger one, were proposed. However, these methods did not evolute with the training dynamics. The existing online data mixing (ODM) method addressed this limitation by applying the multi-armed bandit algorithm as data sampling strategy. Yet, it did not consider the intra-domain interactions. In this paper, we develop an actor-critic based online data mixing (AC-ODM) method, which captures the varying domain weights by auxiliary actor-critic networks and consider the intra-domain interactions with the reward function. While constructing the dataset to pretrain a large target LLM, we directly apply the actor, which is trained with a small proxy LLM as the environment, as the sampling strategy. The transfer of sampling strategy can not only ensure the efficiency of dynamical data mixing, but also expedite the convergence of pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M, which invokes the sampling strategy obtained by a proxy LLM with 410M parameters, reaching the optimal validation perplexity of ODM 71% faster, and improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy, about 2.23x better on pass@1 of HumanEval benchmark.', 'abstract_zh': '预训练数据的覆盖率和组成显著影响大规模语言模型（LLMs）的泛化能力。为了减少训练的碳足迹和财务成本，提出了利用小型代理模型优化领域权重来训练更大模型的一些数据混合方法，但这些方法并未随着训练动态进行演变。现有的在线数据混合（ODM）方法通过应用多臂 bandit 算法作为数据采样策略，解决了这一局限性，但未考虑域内交互。本文开发了一种基于演员-评论家的在线数据混合（AC-ODM）方法，通过辅助演员-评论家网络捕获变化中的领域权重，并通过奖励函数考虑域内交互。在构建用于预训练目标大模型的数据集时，直接将通过一个小的代理模型作为环境训练的演员用作采样策略。采样策略的转移不仅能确保动态数据混合的效率，还能加速目标大模型的预训练过程。数值结果表明，使用具有410M参数的代理模型获得的采样策略的AC-ODM-410M，在达到ODM最优验证困惑度方面快71%，并在零样本MMLU基准测试中提高了27.5%的准确率，HumanEval基准测试中的pass@1性能提升了约2.23倍。', 'title_zh': '基于演员-评论家的在线数据混合语言模型预训练'}
{'arxiv_id': 'arXiv:2505.23876', 'title': 'A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size', 'authors': 'Polad Geidarov', 'link': 'https://arxiv.org/abs/2505.23876', 'abstract': 'The paper discusses the capabilities of multilayer perceptron neural networks implementing metric recognition methods, for which the values of the weights are calculated analytically by formulas. Comparative experiments in training a neural network with pre-calculated weights and with random initialization of weights on different sizes of the MNIST training dataset are carried out. The results of the experiments show that a multilayer perceptron with pre-calculated weights can be trained much faster and is much more robust to the reduction of the training dataset.', 'abstract_zh': '具有预计算权重的多层感知器神经网络的度量识别能力研究：基于不同MNIST训练数据集规模的比较实验', 'title_zh': '基于训练数据集规模的计算权重神经网络与随机生成权重神经网络的比较分析'}
{'arxiv_id': 'arXiv:2505.23875', 'title': 'A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants', 'authors': 'Peter Samoaa, Marcus Vukojevic, Morteza Haghir Chehreghani, Antonio Longa', 'link': 'https://arxiv.org/abs/2505.23875', 'abstract': "Graph-level regression underpins many real-world applications, yet public benchmarks remain heavily skewed toward molecular graphs and citation networks. This limited diversity hinders progress on models that must generalize across both homogeneous and heterogeneous graph structures. We introduce RelSC, a new graph-regression dataset built from program graphs that combine syntactic and semantic information extracted from source code. Each graph is labelled with the execution-time cost of the corresponding program, providing a continuous target variable that differs markedly from those found in existing benchmarks. RelSC is released in two complementary variants. RelSC-H supplies rich node features under a single (homogeneous) edge type, while RelSC-M preserves the original multi-relational structure, connecting nodes through multiple edge types that encode distinct semantic relationships. Together, these variants let researchers probe how representation choice influences model behaviour. We evaluate a diverse set of graph neural network architectures on both variants of RelSC. The results reveal consistent performance differences between the homogeneous and multi-relational settings, emphasising the importance of structural representation. These findings demonstrate RelSC's value as a challenging and versatile benchmark for advancing graph regression methods.", 'abstract_zh': '基于图的回归在许多实际应用中占据重要地位，但公共基准数据集仍然过分集中在分子图和引用网络上。这种有限的多样性阻碍了模型在同质和异质图结构之间泛化的进展。我们介绍了RelSC，这是一个新的图回归数据集，它基于包含源代码语法规则和语义信息的程序图构建。每个图都标记有对应程序的执行时间成本，提供了一个与现有基准中存在的目标变量大相径庭的连续目标变量。RelSC提供两种互补的变体。RelSC-H提供单一边类型下的丰富节点特征，而RelSC-M保留了原始的多关系结构，通过多种边类型连接节点，这些边类型编码不同的语义关系。这两种变体使研究人员能够探究表示选择如何影响模型行为。我们使用一系列图神经网络架构评估了RelSC的两种变体。结果揭示了同质和多关系设置之间的一致性能差异，强调了结构表示的重要性。这些发现证明了RelSC作为挑战性和多功能基准数据集的价值，对于推进图回归方法具有重要意义。', 'title_zh': '同质化和多关系变体的图回归基准数据集'}
{'arxiv_id': 'arXiv:2505.23873', 'title': 'KGMark: A Diffusion Watermark for Knowledge Graphs', 'authors': 'Hongrui Peng, Haolang Lu, Yuanlong Yu, Weiye Fu, Kun Wang, Guoshun Nan', 'link': 'https://arxiv.org/abs/2505.23873', 'abstract': 'Knowledge graphs (KGs) are ubiquitous in numerous real-world applications, and watermarking facilitates protecting intellectual property and preventing potential harm from AI-generated content. Existing watermarking methods mainly focus on static plain text or image data, while they can hardly be applied to dynamic graphs due to spatial and temporal variations of structured data. This motivates us to propose KGMARK, the first graph watermarking framework that aims to generate robust, detectable, and transparent diffusion fingerprints for dynamic KG data. Specifically, we propose a novel clustering-based alignment method to adapt the watermark to spatial variations. Meanwhile, we present a redundant embedding strategy to harden the diffusion watermark against various attacks, facilitating the robustness of the watermark to the temporal variations. Additionally, we introduce a novel learnable mask matrix to improve the transparency of diffusion fingerprints. By doing so, our KGMARK properly tackles the variation challenges of structured data. Experiments on various public benchmarks show the effectiveness of our proposed KGMARK.', 'abstract_zh': '知识图谱水印框架KGMARK：针对动态知识图谱数据的鲁棒、可检测和透明的扩散指纹生成', 'title_zh': 'KGMark: 用于知识图谱的扩散水印'}
{'arxiv_id': 'arXiv:2505.23871', 'title': 'ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning', 'authors': 'Zeyuan Liu, Zhihe Yang, Jiawei Xu, Rui Yang, Jiafei Lyu, Baoxiang Wang, Yunjian Xu, Xiu Li', 'link': 'https://arxiv.org/abs/2505.23871', 'abstract': 'Real-world datasets collected from sensors or human inputs are prone to noise and errors, posing significant challenges for applying offline reinforcement learning (RL). While existing methods have made progress in addressing corrupted actions and rewards, they remain insufficient for handling corruption in high-dimensional state spaces and for cases where multiple elements in the dataset are corrupted simultaneously. Diffusion models, known for their strong denoising capabilities, offer a promising direction for this problem-but their tendency to overfit noisy samples limits their direct applicability. To overcome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a novel approach that pioneers the use of diffusion models to tackle data corruption in offline RL. First, we introduce Ambient Denoising Diffusion Probabilistic Models (DDPM) from approximated distributions, which enable learning on partially corrupted datasets with theoretical guarantees. Second, we use the noise-prediction property of Ambient DDPM to distinguish between clean and corrupted data, and then use the clean subset to train a standard DDPM. Third, we employ the trained standard DDPM to refine the previously identified corrupted data, enhancing data quality for subsequent offline RL training. A notable strength of ADG is its versatility-it can be seamlessly integrated with any offline RL algorithm. Experiments on a range of benchmarks, including MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively mitigates the impact of corrupted data and improves the robustness of offline RL under various noise settings, achieving state-of-the-art results.', 'abstract_zh': '基于去噪扩散模型的 offline 强化学习数据恢复方法', 'title_zh': 'ADG: 时空扩散引导的数据集恢复方法以实现鲁棒离线强化学习'}
{'arxiv_id': 'arXiv:2505.23870', 'title': 'MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection', 'authors': 'Yixian Shen, Qi Bi, Jia-Hong Huang, Hongyi Zhu, Andy D. Pimentel, Anuj Pathania', 'link': 'https://arxiv.org/abs/2505.23870', 'abstract': "We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine Projection, that achieves exceptional performance while requiring minimal parameters and memory for fine-tuning large foundation models. Its general idea is to exploit the superior energy compaction and decorrelation properties of cosine projection to improve both model efficiency and accuracy. Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space. Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and each partition's most critical frequency components are selected. Extensive experiments demonstrate the effectiveness of MaCP across a wide range of single-modality tasks, including natural language understanding, natural language generation, text summarization, as well as multi-modality tasks such as image classification and video understanding. MaCP consistently delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives.", 'abstract_zh': '一种新的适应方法MaCP：最小而强大的自适应余弦投影，其在要求少量参数和内存的情况下实现了卓越的性能，通过利用余弦投影的优越的能量压缩和去相关特性，提高模型的效率和准确性。', 'title_zh': 'MaCP：基于分层次余弦投影的最小且强大的适应性方法'}
{'arxiv_id': 'arXiv:2505.23868', 'title': 'Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert', 'authors': 'Zhaokun Wang, Jinyu Guo, Jingwen Pu, Lingfeng Chen, Hongli Pu, Jie Ou.Libo Qin, Wenhong Tian', 'link': 'https://arxiv.org/abs/2505.23868', 'abstract': 'Current parameter-efficient fine-tuning methods for adapting pre-trained language models to downstream tasks are susceptible to interference from noisy data. Conventional noise-handling approaches either rely on laborious data pre-processing or employ model architecture modifications prone to error accumulation. In contrast to existing noise-process paradigms, we propose a noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a novel framework that enhances model robustness to noise only with generated noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE strategically integrates a dedicated poisoning expert in an asymmetric LoRA configuration. Through a two-stage paradigm, LoPE performs noise injection on the poisoning expert during fine-tuning to enhance its noise discrimination and processing ability. During inference, we selectively mask the dedicated poisoning expert to leverage purified knowledge acquired by normal experts for noise-robust output. Extensive experiments demonstrate that LoPE achieves strong performance and robustness purely through the low-cost noise injection, which completely eliminates the requirement of data cleaning.', 'abstract_zh': '基于异构LoRA中毒专家的噪声鲁棒适应方法', 'title_zh': '噪声鲁棒性通过噪声实现：带有投毒专家的非对称LoRA微调'}
{'arxiv_id': 'arXiv:2505.23867', 'title': 'Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation', 'authors': 'Zeyu Liu, Zhitian Hou, Yining Di, Kejing Yang, Zhijie Sang, Congkai Xie, Jingwen Yang, Siyuan Liu, Jialu Wang, Chunming Li, Ming Li, Hongxia Yang', 'link': 'https://arxiv.org/abs/2505.23867', 'abstract': 'Multimodal large language models (MLLMs) have demonstrated promising prospects in healthcare, particularly for addressing complex medical tasks, supporting multidisciplinary treatment (MDT), and enabling personalized precision medicine. However, their practical deployment faces critical challenges in resource efficiency, diagnostic accuracy, clinical considerations, and ethical privacy. To address these limitations, we propose Infi-Med, a comprehensive framework for medical MLLMs that introduces three key innovations: (1) a resource-efficient approach through curating and constructing high-quality supervised fine-tuning (SFT) datasets with minimal sample requirements, with a forward-looking design that extends to both pretraining and posttraining phases; (2) enhanced multimodal reasoning capabilities for cross-modal integration and clinical task understanding; and (3) a systematic evaluation system that assesses model performance across medical modalities and task types. Our experiments demonstrate that Infi-Med achieves state-of-the-art (SOTA) performance in general medical reasoning while maintaining rapid adaptability to clinical scenarios. The framework establishes a solid foundation for deploying MLLMs in real-world healthcare settings by balancing model effectiveness with operational constraints.', 'abstract_zh': '多模态大型语言模型（MLLMs）在医疗领域的前景及其在处理复杂医疗任务、支持多学科治疗（MDT）和实现个性化精准医疗方面的潜力已得到验证。然而，它们的实际部署面临资源效率、诊断准确性、临床考虑和伦理隐私等关键挑战。为了应对这些局限性，我们提出了一种名为Infi-Med的全面框架，该框架引入了三项关键创新：（1）通过策源和构建高质量的监督微调（SFT）数据集，减少样本需求，以提高资源效率，并具有前瞻性的设计，扩展到预训练和后训练阶段；（2）增强多模态推理能力，以实现跨模态整合和临床任务理解；（3）系统评估体系，评估模型在不同医疗模态和任务类型中的表现。我们的实验表明，Infi-Med在一般医疗推理方面达到了当前最佳性能（SOTA），同时保持了对临床场景的快速适应能力。该框架通过平衡模型有效性与运营约束，为在真实世界医疗环境中部署MLLMs奠定了坚实基础。', 'title_zh': 'Infi-Med: 低资源医学MLLMs的稳健推理评估'}
{'arxiv_id': 'arXiv:2505.23866', 'title': 'Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization', 'authors': 'Chengli Tan, Yubo Zhou, Haishan Ye, Guang Dai, Junmin Liu, Zengjie Song, Jiangshe Zhang, Zixiang Zhao, Yunda Hao, Yong Xu', 'link': 'https://arxiv.org/abs/2505.23866', 'abstract': 'Deep neural networks have been increasingly used in safety-critical applications such as medical diagnosis and autonomous driving. However, many studies suggest that they are prone to being poorly calibrated and have a propensity for overconfidence, which may have disastrous consequences. In this paper, unlike standard training such as stochastic gradient descent, we show that the recently proposed sharpness-aware minimization (SAM) counteracts this tendency towards overconfidence. The theoretical analysis suggests that SAM allows us to learn models that are already well-calibrated by implicitly maximizing the entropy of the predictive distribution. Inspired by this finding, we further propose a variant of SAM, coined as CSAM, to ameliorate model calibration. Extensive experiments on various datasets, including ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error. Meanwhile, CSAM performs even better than SAM and consistently achieves lower calibration error than other approaches', 'abstract_zh': '深度神经网络在医疗诊断和自动驾驶等关键安全应用中越来越广泛。然而，许多研究指出，它们容易校准不当并且倾向于过度自信，这可能产生灾难性后果。本文中，与标准训练方法（如随机梯度下降）不同，我们证明了最近提出的锐度意识最小化（SAM）能够对抗这一过度自信的倾向。理论分析表明，SAM使得我们能够通过隐式最大化预测分布的熵来学习已经校准良好的模型。受这一发现的启发，我们进一步提出了一种SAM的变种，即CSAM，以改善模型校准。在包括ImageNet-1K在内的多个数据集上的广泛实验表明，SAM在减少校准误差方面具有优势。同时，CSAM表现更优，并且始终能够在校准误差上比其他方法取得更好的效果。', 'title_zh': '理解Sharpness-Aware Minimization的校准效益途径'}
{'arxiv_id': 'arXiv:2505.23865', 'title': 'Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration', 'authors': 'Emanuele Masiero, Vito Trianni, Giuseppe Vizzari, Dimitri Ognibene', 'link': 'https://arxiv.org/abs/2505.23865', 'abstract': 'Precision agriculture requires efficient autonomous systems for crop monitoring, where agents must explore large-scale environments while minimizing resource consumption. This work addresses the problem as an active exploration task in a grid environment representing an agricultural field. Each cell may contain targets (e.g., damaged crops) observable from nine predefined points of view (POVs). Agents must infer the number of targets per cell using partial, sequential observations.\nWe propose a two-stage deep learning framework. A pre-trained LSTM serves as a belief model, updating a probabilistic map of the environment and its associated entropy, which defines the expected information gain (IG). This allows agents to prioritize informative regions. A key contribution is the inclusion of a POV visibility mask in the input, preserving the Markov property under partial observability and avoiding revisits to already explored views.\nThree agent architectures were compared: an untrained IG-based agent selecting actions to maximize entropy reduction; a DQN agent using CNNs over local 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent with wider spatial context. Simulations on 20x20 maps showed that the untrained agent performs well despite its simplicity. The DQN agent matches this performance when the POV mask is included, while the Double-CNN agent consistently achieves superior exploration efficiency, especially in larger environments.\nResults show that uncertainty-aware policies leveraging entropy, belief states, and visibility tracking lead to robust and scalable exploration. Future work includes curriculum learning, multi-agent cooperation with shared rewards, transformer-based models, and intrinsic motivation mechanisms to further enhance learning efficiency and policy generalization.', 'abstract_zh': '精准农业需要高效的自主系统来进行作物监测，其中代理必须在尽量减少资源消耗的情况下探索大规模环境。本研究将该问题视为在代表农田的网格环境中进行的积极探索任务。每个网格单元可能包含从九个预定义视角（POVs）之一可观察的目标（如受损作物）。代理必须使用部分顺序观察来推断每个单元中的目标数量。\n我们提出了一种两阶段的深度学习框架。预先训练的LSTM作为信念模型，更新环境及其关联的熵概率地图，熵定义了预期的信息增益（IG），这使得代理可以优先考虑具有信息性的区域。一个关键贡献是在输入中包括POV可见性掩码，这在部分可观测性下保持了马尔可夫性质，并避免了对已探索视角的重复访问。\n三种代理架构进行了比较：一个未训练的基于熵减的代理，选择行动以最大化熵减少；一个使用局部3x3输入的CNNs的DQN代理，带有信念、熵和POV掩码；以及一个具有更宽空间上下文的双CNN DQN代理。在20x20的地图上进行的模拟显示，尽管未训练的代理很简单，但它表现良好。当包括POV掩码时，DQN代理达到了类似的性能，而双CNN代理则在更大的环境中始终实现更高效的探索效率。\n结果表明，利用熵、信念状态和可见性跟踪的不确定性感知策略能够实现稳健且可扩展的探索。未来的工作包括层级学习、具有共享奖励的多代理合作、基于变换器的模型以及内在动机机制，以进一步增强学习效率和策略的一般化。', 'title_zh': '结合深层架构进行信息增益估计与多agent领域探索的强化学习方法'}
{'arxiv_id': 'arXiv:2505.23864', 'title': 'Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections', 'authors': 'Wei Zhuo, Zhaohuan Zhan, Ziduo Yang, Han Yu', 'link': 'https://arxiv.org/abs/2505.23864', 'abstract': 'Federated learning (FL) on graph-structured data typically faces non-IID challenges, particularly in scenarios where each client holds a distinct subgraph sampled from a global graph. In this paper, we introduce Federated learning with Auxiliary projections (FedAux), a personalized subgraph FL framework that learns to align, compare, and aggregate heterogeneously distributed local models without sharing raw data or node embeddings. In FedAux, each client jointly trains (i) a local GNN and (ii) a learnable auxiliary projection vector (APV) that differentiably projects node embeddings onto a 1D space. A soft-sorting operation followed by a lightweight 1D convolution refines these embeddings in the ordered space, enabling the APV to effectively capture client-specific information. After local training, these APVs serve as compact signatures that the server uses to compute inter-client similarities and perform similarity-weighted parameter mixing, yielding personalized models while preserving cross-client knowledge transfer. Moreover, we provide rigorous theoretical analysis to establish the convergence and rationality of our design. Empirical evaluations across diverse graph benchmarks demonstrate that FedAux substantially outperforms existing baselines in both accuracy and personalization performance.', 'abstract_zh': '图结构数据上的 Federated learning (FL) 通常面临非IID挑战，特别是在每个客户端持有来自全局图的独立子图的场景中。本文介绍了Federated learning with Auxiliary projections (FedAux)，一种无需共享原始数据或节点嵌入的学习个性化局部模型的框架，该框架能够学习对齐、比较和聚合异质分布的局部模型。在FedAux中，每个客户端共同训练一个局部GNN和一个可学习的辅助投影向量（APV），该向量能够差异性地将节点嵌入投影到一维空间中。随后的软排序操作和轻量级的一维卷积在有序空间中进一步细化这些嵌入，使APV能够有效捕捉客户端特定的信息。经过本地训练后，这些APV作为客户端的紧凑签名，服务器利用它们计算客户端间的相似性，并进行加权参数混合，从而产生个性化模型，同时保持跨客户端的知识迁移。此外，我们提供了严格的理论分析，以确证我们设计的收敛性和合理性。在多种图基准测试上的实证评估表明，FedAux在准确性和个性化性能上显著优于现有基线。', 'title_zh': '个性化子图联邦学习与可微辅助投影'}
{'arxiv_id': 'arXiv:2505.23863', 'title': 'Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting', 'authors': 'Chang Liu, Bohao Zhao, Jingtao Ding, Huandong Wang, Yong Li', 'link': 'https://arxiv.org/abs/2505.23863', 'abstract': 'Long-term forecasting of chaotic systems from short-term observations remains a fundamental and underexplored challenge due to the intrinsic sensitivity to initial conditions and the complex geometry of strange attractors. Existing approaches often rely on long-term training data or focus on short-term sequence correlations, struggling to maintain predictive stability and dynamical coherence over extended horizons. We propose PhyxMamba, a novel framework that integrates a Mamba-based state-space model with physics-informed principles to capture the underlying dynamics of chaotic systems. By reconstructing the attractor manifold from brief observations using time-delay embeddings, PhyxMamba extracts global dynamical features essential for accurate forecasting. Our generative training scheme enables Mamba to replicate the physical process, augmented by multi-token prediction and attractor geometry regularization for physical constraints, enhancing prediction accuracy and preserving key statistical invariants. Extensive evaluations on diverse simulated and real-world chaotic systems demonstrate that PhyxMamba delivers superior long-term forecasting and faithfully captures essential dynamical invariants from short-term data. This framework opens new avenues for reliably predicting chaotic systems under observation-scarce conditions, with broad implications across climate science, neuroscience, epidemiology, and beyond. Our code is open-source at this https URL.', 'abstract_zh': '短时观测下混沌系统的长期预报仍然是一个因初始条件的高度敏感性和怪异吸引子的复杂几何结构而未充分探索的基本挑战。现有的方法往往依赖于长期训练数据或集中在短期序列相关性上，难以在较长的时间尺度上保持预测稳定性与动力学一致性。我们提出PhyxMamba框架，该框架将基于Mamba的状态空间模型与物理指导原则相结合，以捕捉混沌系统的内在动力学。通过使用时间延迟嵌入从短暂观测中重构吸引子流形，PhyxMamba提取了对于准确预报至关重要的全局动力学特征。我们生成性训练方案使Mamba能够模拟物理过程，并通过多令牌预测和吸引子几何结构正则化增加物理约束，从而提高预测准确性并保持关键统计不变量。在多元模拟和实际混沌系统的广泛评估中，PhyxMamba展示了其在长期预报方面的优越性能，并能从短时数据中忠实捕捉关键动力学不变量。该框架为在观测数据稀缺条件下可靠预测混沌系统开辟了新途径，在气候科学、神经科学、流行病学等领域具有广泛影响。我们的代码在以下网址公开：this https URL。', 'title_zh': 'Mamba 结合物理原理掌握长期混沌系统预测'}
{'arxiv_id': 'arXiv:2505.23862', 'title': 'A New Deep-learning-Based Approach For mRNA Optimization: High Fidelity, Computation Efficiency, and Multiple Optimization Factors', 'authors': 'Zheng Gong, Ziyi Jiang, Weihao Gao, Deng Zhuo, Lan Ma', 'link': 'https://arxiv.org/abs/2505.23862', 'abstract': "The mRNA optimization is critical for therapeutic and biotechnological applications, since sequence features directly govern protein expression levels and efficacy. However, current methods face significant challenges in simultaneously achieving three key objectives: (1) fidelity (preventing unintended amino acid changes), (2) computational efficiency (speed and scalability), and (3) the scope of optimization variables considered (multi-objective capability). Furthermore, existing methods often fall short of comprehensively incorporating the factors related to the mRNA lifecycle and translation process, including intrinsic mRNA sequence properties, secondary structure, translation elongation kinetics, and tRNA availability. To address these limitations, we introduce \\textbf{RNop}, a novel deep learning-based method for mRNA optimization. We collect a large-scale dataset containing over 3 million sequences and design four specialized loss functions, the GPLoss, CAILoss, tAILoss, and MFELoss, which simultaneously enable explicit control over sequence fidelity while optimizing species-specific codon adaptation, tRNA availability, and desirable mRNA secondary structure features. Then, we demonstrate RNop's effectiveness through extensive in silico and in vivo experiments. RNop ensures high sequence fidelity, achieves significant computational throughput up to 47.32 sequences/s, and yields optimized mRNA sequences resulting in a significant increase in protein expression for functional proteins compared to controls. RNop surpasses current methodologies in both quantitative metrics and experimental validation, enlightening a new dawn for efficient and effective mRNA design. Code and models will be available at this https URL.", 'abstract_zh': 'mRNA优化对于治疗和生物技术应用至关重要，因为序列特征直接控制蛋白质的表达水平和疗效。然而，当前方法在同时实现三个关键目标方面面临重大挑战：(1) 忠实性（防止非预期的氨基酸变化），(2) 计算效率（速度和可扩展性），以及(3) 考虑的优化变量范围（多目标能力）。此外，现有方法往往未能全面纳入与mRNA生命周期和翻译过程相关的因素，包括内在的mRNA序列属性、二级结构、翻译延伸动力学和tRNA可用性。为了解决这些局限性，我们引入了**RNop**，这是一种基于深度学习的mRNA优化新方法。我们收集了一个包含超过300万个序列的大规模数据集，并设计了四个专门的损失函数：GPLoss、CAILoss、tAILoss 和 MFELoss，这些函数能够在优化特定物种的密码子适应性、tRNA可用性和理想的mRNA二级结构特征的同时，明确控制序列忠实性。然后，我们通过广泛的体外和体内实验证明了RNop的有效性。RNop确保了高序列忠实性，实现了显著的计算吞吐量（高达47.32序列/秒），并且优化的mRNA序列相比对照组显著提高了功能性蛋白质的表达。RNop在定量指标和实验验证方面均超过了现有方法，为高效有效的mRNA设计开辟了新的前景。源代码和模型可在以下链接获取。', 'title_zh': '基于深度学习的新方法为mRNA优化：高保真度、计算效率及多重优化因素'}
{'arxiv_id': 'arXiv:2505.23861', 'title': 'BiBLDR: Bidirectional Behavior Learning for Drug Repositioning', 'authors': 'Renye Zhang, Mengyun Yang, Qichang Zhao, Jianxin Wang', 'link': 'https://arxiv.org/abs/2505.23861', 'abstract': 'Drug repositioning aims to identify potential new indications for existing drugs to reduce the time and financial costs associated with developing new drugs. Most existing deep learning-based drug repositioning methods predominantly utilize graph-based representations. However, graph-based drug repositioning methods struggle to perform effective inference in cold-start scenarios involving novel drugs because of the lack of association information with the diseases. Unlike traditional graph-based approaches, we propose a bidirectional behavior learning strategy for drug repositioning, known as BiBLDR. This innovative framework redefines drug repositioning as a behavior sequential learning task to capture drug-disease interaction patterns. First, we construct bidirectional behavioral sequences based on drug and disease sides. The consideration of bidirectional information ensures a more meticulous and rigorous characterization of the behavioral sequences. Subsequently, we propose a two-stage strategy for drug repositioning. In the first stage, we construct prototype spaces to characterize the representational attributes of drugs and diseases. In the second stage, these refined prototypes and bidirectional behavior sequence data are leveraged to predict potential drug-disease associations. Based on this learning approach, the model can more robustly and precisely capture the interactive relationships between drug and disease features from bidirectional behavioral sequences. Extensive experiments demonstrate that our method achieves state-of-the-art performance on benchmark datasets. Meanwhile, BiBLDR demonstrates significantly superior performance compared to previous methods in cold-start scenarios. Our code is published in this https URL.', 'abstract_zh': '药物再定位旨在识别现有药物的新潜在适应症，以减少开发新药物所需的时间和财务成本。现有的大多数基于深度学习的药物再定位方法主要利用图表示。然而，基于图的药物再定位方法在涉及新型药物的冷启动场景中难以进行有效的推理，因为缺乏与疾病的关联信息。不同于传统的基于图的方法，我们提出了一种双向行为学习策略，称为BiBLDR。该创新框架将药物再定位重新定义为行为序列学习任务，以捕获药物-疾病交互模式。首先，我们基于药物和疾病两侧构建双向行为序列。考虑双向信息确保了对行为序列的更细致和严谨的描述。随后，我们提出了一种双阶段的药物再定位策略。在第一阶段，我们构建原型空间以表征药物和疾病的表示属性。在第二阶段，利用这些细化的原型和双向行为序列数据来预测潜在的药物-疾病关联。基于此学习方法，模型能够更 robust 地和精确地从双向行为序列中捕捉药物和疾病特征的交互关系。大量实验表明，我们的方法在基准数据集中实现了最先进的性能。同时，BiBLDR 在冷启动场景中的性能明显优于之前的 方法。我们的代码发布在 https://github.com/alibaba/Qwen-BiBLDR 。', 'title_zh': 'BiBLDR：双向行为学习用于药物再定位'}
{'arxiv_id': 'arXiv:2505.23860', 'title': 'Quantum computing and artificial intelligence: status and perspectives', 'authors': 'Giovanni Acampora, Andris Ambainis, Natalia Ares, Leonardo Banchi, Pallavi Bhardwaj, Daniele Binosi, G. Andrew D. Briggs, Tommaso Calarco, Vedran Dunjko, Jens Eisert, Olivier Ezratty, Paul Erker, Federico Fedele, Elies Gil-Fuster, Martin Gärttner, Mats Granath, Markus Heyl, Iordanis Kerenidis, Matthias Klusch, Anton Frisk Kockum, Richard Kueng, Mario Krenn, Jörg Lässig, Antonio Macaluso, Sabrina Maniscalco, Florian Marquardt, Kristel Michielsen, Gorka Muñoz-Gil, Daniel Müssig, Hendrik Poulsen Nautrup, Evert van Nieuwenburg, Roman Orus, Jörg Schmiedmayer, Markus Schmitt, Philipp Slusallek, Filippo Vicentini, Christof Weitenberg, Frank K. Wilhelm', 'link': 'https://arxiv.org/abs/2505.23860', 'abstract': 'This white paper discusses and explores the various points of intersection between quantum computing and artificial intelligence (AI). It describes how quantum computing could support the development of innovative AI solutions. It also examines use cases of classical AI that can empower research and development in quantum technologies, with a focus on quantum computing and quantum sensing. The purpose of this white paper is to provide a long-term research agenda aimed at addressing foundational questions about how AI and quantum computing interact and benefit one another. It concludes with a set of recommendations and challenges, including how to orchestrate the proposed theoretical work, align quantum AI developments with quantum hardware roadmaps, estimate both classical and quantum resources - especially with the goal of mitigating and optimizing energy consumption - advance this emerging hybrid software engineering discipline, and enhance European industrial competitiveness while considering societal implications.', 'abstract_zh': 'this white paper探讨了量子计算与人工智能的各种交叉点，并探讨了量子计算如何支持创新人工智能解决方案的发展。它还研究了经典人工智能的应用案例，这些案例可以增强量子技术的研究和开发，重点关注量子计算和量子传感。本文的目的是提供一个长期研究议程，以解决人工智能和量子计算如何相互作用并从中受益的基本问题。最后，它提出了建议和挑战，包括如何协调拟议的理论工作、将量子人工智能的发展与量子硬件路线图对齐、估算从经典到量子的各种资源——特别是在减轻和优化能源消耗方面、推进这一新兴的混合软件工程学科、以及在考虑社会影响的同时增强欧洲工业竞争力。', 'title_zh': '量子计算与人工智能：现状与展望'}
{'arxiv_id': 'arXiv:2505.23859', 'title': 'Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration', 'authors': 'Wenju Sun, Qingyong Li, Wen Wang, Yang Liu, Yangli-ao Geng, Boyang Li', 'link': 'https://arxiv.org/abs/2505.23859', 'abstract': 'Multi-task model merging aims to consolidate knowledge from multiple fine-tuned task-specific experts into a unified model while minimizing performance degradation. Existing methods primarily approach this by minimizing differences between task-specific experts and the unified model, either from a parameter-level or a task-loss perspective. However, parameter-level methods exhibit a significant performance gap compared to the upper bound, while task-loss approaches entail costly secondary training procedures. In contrast, we observe that performance degradation closely correlates with feature drift, i.e., differences in feature representations of the same sample caused by model merging. Motivated by this observation, we propose Layer-wise Optimal Task Vector Merging (LOT Merging), a technique that explicitly minimizes feature drift between task-specific experts and the unified model in a layer-by-layer manner. LOT Merging can be formulated as a convex quadratic optimization problem, enabling us to analytically derive closed-form solutions for the parameters of linear and normalization layers. Consequently, LOT Merging achieves efficient model consolidation through basic matrix operations. Extensive experiments across vision and vision-language benchmarks demonstrate that LOT Merging significantly outperforms baseline methods, achieving improvements of up to 4.4% (ViT-B/32) over state-of-the-art approaches.', 'abstract_zh': '多层次最优任务向量融合以最小化特征漂移实现多任务模型合并', 'title_zh': '向最小化模型合并中的特征漂移靠拢：层-wise 任务向量融合实现自适应知识集成'}
{'arxiv_id': 'arXiv:2505.23857', 'title': 'DATD3: Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient For Model Free Reinforcement Learning Under Output Feedback Control', 'authors': 'Wuhao Wang, Zhiyong Chen', 'link': 'https://arxiv.org/abs/2505.23857', 'abstract': 'Reinforcement learning in real-world applications often involves output-feedback settings, where the agent receives only partial state information. To address this challenge, we propose the Output-Feedback Markov Decision Process (OPMDP), which extends the standard MDP formulation to accommodate decision-making based on observation histories. Building on this framework, we introduce Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient (DATD3), a novel actor-critic algorithm that employs depthwise separable convolution and multi-head attention to encode historical observations. DATD3 maintains policy expressiveness while avoiding the instability of recurrent models. Extensive experiments on continuous control tasks demonstrate that DATD3 outperforms existing memory-based and recurrent baselines under both partial and full observability.', 'abstract_zh': '输出反馈马尔可夫决策过程（OPMDP）在现实世界应用中的强化学习通常涉及部分状态信息反馈场景。为应对这一挑战，我们提出了输出反馈马尔可夫决策过程（OPMDP），将其标准MDP公式扩展以适应基于观测历史的决策。基于这一框架，我们引入了一种新颖的演员-评论家算法——深度可分离卷积与多头注意力深度确定性策略梯度（DATD3），该算法利用深度可分离卷积和多头注意力来编码历史观测。DATD3保持了策略的表达性，同时避免了循环模型的不稳定性。在连续控制任务上的广泛实验表明，DATD3在部分可观测性和完全可观测性情况下均优于现有的基于记忆和循环的基线方法。', 'title_zh': 'DATD3:深度可分离注意机制延迟双深确定性策略梯度在输出反馈控制下的模型无关强化学习'}
{'arxiv_id': 'arXiv:2505.23856', 'title': 'OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities', 'authors': 'Sahil Verma, Keegan Hines, Jeff Bilmes, Charlotte Siska, Luke Zettlemoyer, Hila Gonen, Chandan Singh', 'link': 'https://arxiv.org/abs/2505.23856', 'abstract': 'The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\% over the strongest baseline in a multilingual setting, by 20.44\\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient ($\\approx 120 \\times$ faster than the next fastest baseline). Code and data are available at: this https URL.', 'abstract_zh': '大型语言模型 emerging 能力带来的潜在有害滥用问题引发了关注。为缓解这一问题，我们提出了 OMNIGUARD 方法，以跨语言和模态检测有害提示。OMNIGUARD 方法通过识别跨语言或模态对齐的大型语言模型/多元语言模型的内部表示，然后利用这些表示构建一种语言无关或模态无关的分类器来检测有害提示。在多语言设置中，OMNIGUARD 将有害提示分类的准确率提高了 11.57%，对于基于图像的提示提高了 20.44%，并对基于音频的提示设定了新的 SOTA。通过重用生成过程中计算的嵌入，OMNIGUARD 还非常高效（比下一个最快的基线快约 120 倍）。代码和数据可在以下网址获得：this https URL。', 'title_zh': 'OMNIGUARD: 跨模态的高效AI安全审核方法'}
{'arxiv_id': 'arXiv:2505.23854', 'title': 'Revisiting Uncertainty Estimation and Calibration of Large Language Models', 'authors': 'Linwei Tao, Yi-Fan Yeh, Minjing Dong, Tao Huang, Philip Torr, Chang Xu', 'link': 'https://arxiv.org/abs/2505.23854', 'abstract': 'As large language models (LLMs) are increasingly deployed in high-stakes applications, robust uncertainty estimation is essential for ensuring the safe and trustworthy deployment of LLMs. We present the most comprehensive study to date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and closed-source families, dense and Mixture-of-Experts (MoE) architectures, reasoning and non-reasoning modes, quantization variants and parameter scales from 0.6B to 671B. Focusing on three representative black-box single-pass methods, including token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically evaluate uncertainty calibration and selective classification using the challenging MMLU-Pro benchmark, which covers both reasoning-intensive and knowledge-based tasks. Our results show that LVU consistently outperforms TPU and NVU, offering stronger calibration and discrimination while being more interpretable. We also find that high accuracy does not imply reliable uncertainty, and that model scale, post-training, reasoning ability and quantization all influence estimation performance. Notably, LLMs exhibit better uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good calibration does not necessarily translate to effective error ranking. These findings highlight the need for multi-perspective evaluation and position LVU as a practical tool for improving the reliability of LLMs in real-world settings.', 'abstract_zh': '大规模语言模型（LLMs）在高风险应用中日益普及，因此需要可靠的不确定性估计来确保LLMs的安全和信赖部署。本文进行了迄今为止最全面的大规模语言模型不确定性估计研究，评估了涵盖开源和闭源家族、密集型和Mixture-of-Experts（MoE）架构、推理与非推理模式、以及从0.6B到671B不同参数规模和量化变种的80个模型。本文集中在三种代表性的黑盒单次通过方法上，包括基于标记概率的不确定性（TPU）、数值语义不确定性（NVU）和语言语义不确定性（LVU），系统地评估了不确定性的校准和选择性分类，使用具有推理密集型和知识密集型任务的挑战性MMLU-Pro基准进行评估。结果显示，LVU在一致性和区分度上优于TPU和NVU，并且更具可解释性。我们还发现，高准确率并不意味着可靠的不确定性，模型规模、后训练、推理能力和量化都会影响估计性能。值得注意的是，LLMs在推理任务上的不确定性估计优于在知识密集型任务上，良好的校准并不必然转化为有效的错误排序。这些发现强调了多视角评估的必要性，并将LVU定位为提高LLMs在实际应用中可靠性的一种实用工具。', 'title_zh': '重新审视大型语言模型的不确定性估计和校准'}
{'arxiv_id': 'arXiv:2505.23852', 'title': "Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease", 'authors': 'Nic Dobbins, Christelle Xiong, Kristine Lan, Meliha Yetisgen', 'link': 'https://arxiv.org/abs/2505.23852', 'abstract': 'Objective: To demonstrate the capabilities of Large Language Models (LLMs) as autonomous agents to reproduce findings of published research studies using the same or similar dataset.\nMaterials and Methods: We used the "Quick Access" dataset of the National Alzheimer\'s Coordinating Center (NACC). We identified highly cited published research manuscripts using NACC data and selected five studies that appeared reproducible using this dataset alone. Using GPT-4o, we created a simulated research team of LLM-based autonomous agents tasked with writing and executing code to dynamically reproduce the findings of each study, given only study Abstracts, Methods sections, and data dictionary descriptions of the dataset.\nResults: We extracted 35 key findings described in the Abstracts across 5 Alzheimer\'s studies. On average, LLM agents approximately reproduced 53.2% of findings per study. Numeric values and range-based findings often differed between studies and agents. The agents also applied statistical methods or parameters that varied from the originals, though overall trends and significance were sometimes similar.\nDiscussion: In some cases, LLM-based agents replicated research techniques and findings. In others, they failed due to implementation flaws or missing methodological detail. These discrepancies show the current limits of LLMs in fully automating reproducibility assessments. Still, this early investigation highlights the potential of structured agent-based systems to provide scalable evaluation of scientific rigor.\nConclusion: This exploratory work illustrates both the promise and limitations of LLMs as autonomous agents for automating reproducibility in biomedical research.', 'abstract_zh': '目标：展示大型语言模型（LLMs）作为自主代理的能力，使用相同的或类似的数据集重现已出版研究研究的结果。', 'title_zh': '基于大型语言模型的代理在阿尔茨海默病自动研究可重复性中的探索性研究'}
{'arxiv_id': 'arXiv:2505.23851', 'title': 'ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark', 'authors': 'Michael Shalyt, Rotem Elimelech, Ido Kaminer', 'link': 'https://arxiv.org/abs/2505.23851', 'abstract': 'Large language models (LLMs) are rapidly approaching the level of proficiency in university-level symbolic mathematics required for applications in advanced science and technology. However, existing benchmarks fall short in assessing the core skills of LLMs in symbolic mathematics-such as integration, differential equations, and algebraic simplification. To address this gap, we introduce ASyMOB, a novel assessment framework focused exclusively on symbolic manipulation, featuring 17,092 unique math challenges, organized by similarity and complexity. ASyMOB enables analysis of LLM generalization capabilities by comparing performance in problems that differ by simple numerical or symbolic `perturbations\'. Evaluated LLMs exhibit substantial degradation in performance for all perturbation types (up to -70.3%), suggesting reliance on memorized patterns rather than deeper understanding of symbolic math, even among models achieving high baseline accuracy. Comparing LLM performance to computer algebra systems, we identify examples where they fail while LLMs succeed, as well as problems solved only by combining both approaches. Models capable of integrated code execution yielded higher accuracy compared to their performance without code, particularly stabilizing weaker models (up to +33.1% for certain perturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5 Flash) demonstrate not only high symbolic math proficiency (scoring 96.8% and 97.6% on the unperturbed set), but also remarkable robustness against perturbations, (-21.7% and -21.2% vs. average -50.4% for the other models). This may indicate a recent "phase transition" in the generalization capabilities of frontier LLMs. It remains to be seen whether the path forward lies in deeper integration with sophisticated external tools, or in developing models so capable that symbolic math systems like CAS become unnecessary.', 'abstract_zh': '大规模语言模型（LLMs）在高级科学和技术应用所需的大学水平符号数学 proficiency 方面正迅速接近所需水平。然而，现有的基准在评估LLMs在符号数学方面的核心技能（如积分、微分方程和代数简化）方面存在不足。为解决这一问题，我们引入了ASyMOB，一个专注于符号操作的新型评估框架，包含17,092个独特数学挑战，按相似性和复杂性组织。ASyMOB 通过比较由简单数值或符号 `扰动` 引起的问题性能差异，有助于分析LLM的一般化能力。评估的LLMs在所有扰动类型下的性能显著下降（高达-70.3%），这表明它们依赖于记忆模式而非对符号数学更深层次的理解，即使在基础准确度高的模型中也是如此。将LLM性能与计算机代数系统进行比较，我们发现了LLM失败而LLM成功的问题实例，以及只有结合两者方法才能解决的问题。能够执行集成代码的模型在没有代码的情况下性能更高，尤其能稳定较弱的模型（某些扰动类型提高多达+33.1%）。值得注意的是，最先进的模型（o4-mini, Gemini 2.5 Flash）不仅在未扰动集上的符号数学熟练度很高（分别达到96.8%和97.6%），而且对扰动表现出惊人的鲁棒性（分别为-21.7%和-21.2%，而其他模型的平均值为-50.4%）。这可能表明前沿LLM的一般化能力最近经历了“相变”。未来的发展路径可能是更深层次地整合复杂的外部工具，也可能是开发出足够强大的模型，使得像CAS这样的符号数学系统成为不必要的工具。', 'title_zh': 'ASyMOB: 代数符号数学运算基准'}
{'arxiv_id': 'arXiv:2505.23849', 'title': 'CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning', 'authors': 'Kaveen Hiniduma, Zilinghan Li, Aditya Sinha, Ravi Madduri, Suren Byna', 'link': 'https://arxiv.org/abs/2505.23849', 'abstract': "Privacy-Preserving Federated Learning (PPFL) is a decentralized machine learning approach where multiple clients train a model collaboratively. PPFL preserves privacy and security of the client's data by not exchanging it. However, ensuring that data at each client is of high quality and ready for federated learning (FL) is a challenge due to restricted data access. In this paper, we introduce CADRE (Customizable Assurance of Data REadiness) for FL, a novel framework that allows users to define custom data readiness (DR) standards, metrics, rules, and remedies tailored to specific FL tasks. Our framework generates comprehensive DR reports based on the user-defined metrics, rules, and remedies to ensure datasets are optimally prepared for FL while preserving privacy. We demonstrate the framework's practical application by integrating it into an existing PPFL framework. We conducted experiments across six diverse datasets, addressing seven different DR issues. The results illustrate the framework's versatility and effectiveness in ensuring DR across various dimensions, including data quality, privacy, and fairness. This approach enhances the performance and reliability of FL models as well as utilizes valuable resources by identifying and addressing data-related issues before the training phase.", 'abstract_zh': '隐私保护联邦学习（PPFL）中的数据准备保障（CADRE）框架：针对特定联邦学习任务自定义数据就绪标准与方法', 'title_zh': 'CADRE：隐私保护联邦学习中的可定制化数据就绪性保障'}
{'arxiv_id': 'arXiv:2505.23847', 'title': 'Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems', 'authors': 'Ronny Ko, Jiseong Jeong, Shuyuan Zheng, Chuan Xiao, Taewan Kim, Makoto Onizuka, Wonyong Shin', 'link': 'https://arxiv.org/abs/2505.23847', 'abstract': 'Large language models (LLMs) are rapidly evolving into autonomous agents that cooperate across organizational boundaries, enabling joint disaster response, supply-chain optimization, and other tasks that demand decentralized expertise without surrendering data ownership. Yet, cross-domain collaboration shatters the unified trust assumptions behind current alignment and containment techniques. An agent benign in isolation may, when receiving messages from an untrusted peer, leak secrets or violate policy, producing risks driven by emergent multi-agent dynamics rather than classical software bugs. This position paper maps the security agenda for cross-domain multi-agent LLM systems. We introduce seven categories of novel security challenges, for each of which we also present plausible attacks, security evaluation metrics, and future research guidelines.', 'abstract_zh': '大型语言模型（LLMs）正迅速演变为跨越组织边界合作的自主代理， enabling 联合灾害响应、供应链优化和其他需要去中心化专业知识但不牺牲数据所有权的任务。然而，跨域协作打破了当前对齐和 containment 技术背后的一体化信任假设。孤立时表现 benign 的代理，当接收到不可信同伴的消息时，可能会泄露秘密或违反政策，产生的风险来自 emergent 多代理动力学而非经典软件错误。本文探讨了跨域多代理 LLM 系统的安全议程。我们提出了七类新型安全挑战，并为每种挑战提供了可能的攻击、安全评估指标和未来研究指南。', 'title_zh': '跨域多智能体LLM系统必须解决的七个安全挑战'}
{'arxiv_id': 'arXiv:2505.23836', 'title': 'Large Language Models Often Know When They Are Being Evaluated', 'authors': 'Joe Needham, Giles Edkins, Govind Pimpale, Henning Bartsch, Marius Hobbhahn', 'link': 'https://arxiv.org/abs/2505.23836', 'abstract': 'If AI models can detect when they are being evaluated, the effectiveness of evaluations might be compromised. For example, models could have systematically different behavior during evaluations, leading to less reliable benchmarks for deployment and governance decisions. We investigate whether frontier language models can accurately classify transcripts based on whether they originate from evaluations or real-world deployment, a capability we call evaluation awareness. To achieve this, we construct a diverse benchmark of 1,000 prompts and transcripts from 61 distinct datasets. These span public benchmarks (e.g., MMLU, SWEBench), real-world deployment interactions, and agent trajectories from scaffolding frameworks (e.g., web-browsing agents). Frontier models clearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches an AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of $0.92$). Furthermore, both AI models and humans are better at identifying evaluations in agentic settings compared to chat settings. Additionally, we test whether models can identify the purpose of the evaluation. Under multiple-choice and open-ended questioning, AI models far outperform random chance in identifying what an evaluation is testing for. Our results indicate that frontier models already exhibit a substantial, though not yet superhuman, level of evaluation-awareness. We recommend tracking this capability in future models.', 'abstract_zh': '如果AI模型能够识别出自己正在被评估，评估的效果可能会受到损害。前沿语言模型能否准确区分来自评估和实际部署的对话转录：一项能力分析', 'title_zh': '大型语言模型往往知道自己正在被评估。'}
{'arxiv_id': 'arXiv:2505.23813', 'title': 'DP-RTFL: Differentially Private Resilient Temporal Federated Learning for Trustworthy AI in Regulated Industries', 'authors': 'Abhijit Talluri', 'link': 'https://arxiv.org/abs/2505.23813', 'abstract': 'Federated Learning (FL) has emerged as a critical paradigm for enabling privacy-preserving machine learning, particularly in regulated sectors such as finance and healthcare. However, standard FL strategies often encounter significant operational challenges related to fault tolerance, system resilience against concurrent client and server failures, and the provision of robust, verifiable privacy guarantees essential for handling sensitive data. These deficiencies can lead to training disruptions, data loss, compromised model integrity, and non-compliance with data protection regulations (e.g., GDPR, CCPA). This paper introduces Differentially Private Resilient Temporal Federated Learning (DP-RTFL), an advanced FL framework designed to ensure training continuity, precise state recovery, and strong data privacy. DP-RTFL integrates local Differential Privacy (LDP) at the client level with resilient temporal state management and integrity verification mechanisms, such as hash-based commitments (referred to as Zero-Knowledge Integrity Proofs or ZKIPs in this context). The framework is particularly suited for critical applications like credit risk assessment using sensitive financial data, aiming to be operationally robust, auditable, and scalable for enterprise AI deployments. The implementation of the DP-RTFL framework is available as open-source.', 'abstract_zh': '差异隐私 resilient 时间联邦学习（DP-RTFL）：确保训练连续性、精确状态恢复和强数据隐私的高级联邦学习框架', 'title_zh': '差分隐私 resilient 时空联邦学习：可信 AI 在受监管行业的应用'}
{'arxiv_id': 'arXiv:2505.23812', 'title': 'Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content', 'authors': 'Lata Pangtey, Mohammad Zia Ur Rehman, Prasad Chaudhari, Shubhi Bansal, Nagendra Kumar', 'link': 'https://arxiv.org/abs/2505.23812', 'abstract': "The rapid evolution of social media has generated an overwhelming volume of user-generated content, conveying implicit opinions and contributing to the spread of misinformation. The method aims to enhance the detection of stance where misinformation can polarize user opinions. Stance detection has emerged as a crucial approach to effectively analyze underlying biases in shared information and combating misinformation. This paper proposes a novel method for \\textbf{S}tance \\textbf{P}rediction through a \\textbf{L}abel-fused dual cross-\\textbf{A}ttentive \\textbf{E}motion-aware neural \\textbf{Net}work (SPLAENet) in misinformative social media user-generated content. The proposed method employs a dual cross-attention mechanism and a hierarchical attention network to capture inter and intra-relationships by focusing on the relevant parts of source text in the context of reply text and vice versa. We incorporate emotions to effectively distinguish between different stance categories by leveraging the emotional alignment or divergence between the texts. We also employ label fusion that uses distance-metric learning to align extracted features with stance labels, improving the method's ability to accurately distinguish between stances. Extensive experiments demonstrate the significant improvements achieved by SPLAENet over existing state-of-the-art methods. SPLAENet demonstrates an average gain of 8.92\\% in accuracy and 17.36\\% in F1-score on the RumourEval dataset. On the SemEval dataset, it achieves average gains of 7.02\\% in accuracy and 10.92\\% in F1-score. On the P-stance dataset, it demonstrates average gains of 10.03\\% in accuracy and 11.18\\% in F1-score. These results validate the effectiveness of the proposed method for stance detection in the context of misinformative social media content.", 'abstract_zh': '社交媒体中误导性内容中立场预测的SPLAENet标签融合双重交叉注意情感意识神经网络', 'title_zh': '具有标签融合的情感意识双重交叉注意神经网络在虚假社交媒体内容立场检测中的应用'}
{'arxiv_id': 'arXiv:2505.23811', 'title': 'LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions', 'authors': 'Hadi Askari, Shivanshu Gupta, Fei Wang, Anshuman Chhabra, Muhao Chen', 'link': 'https://arxiv.org/abs/2505.23811', 'abstract': "Pretrained Large Language Models (LLMs) achieve strong performance across a wide range of tasks, yet exhibit substantial variability in the various layers' training quality with respect to specific downstream applications, limiting their downstream this http URL is therefore critical to estimate layer-wise training quality in a manner that accounts for both model architecture and training data. However, existing approaches predominantly rely on model-centric heuristics (such as spectral statistics, outlier detection, or uniform allocation) while overlooking the influence of data. To address these limitations, we propose LayerIF, a data-driven framework that leverages Influence Functions to quantify the training quality of individual layers in a principled and task-sensitive manner. By isolating each layer's gradients and measuring the sensitivity of the validation loss to training examples by computing layer-wise influences, we derive data-driven estimates of layer importance. Notably, our method produces task-specific layer importance estimates for the same LLM, revealing how layers specialize for different test-time evaluation tasks. We demonstrate the utility of our scores by leveraging them for two downstream applications: (a) expert allocation in LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM pruning. Experiments across multiple LLM architectures demonstrate that our model-agnostic, influence-guided allocation leads to consistent gains in task performance.", 'abstract_zh': '预训练大型语言模型（LLMs）在多种任务中表现出色，但在特定下游应用的不同层的训练质量存在显著差异，限制了它们的下游应用效果。因此，估算每层的训练质量，考虑到模型架构和训练数据的影响，变得至关重要。然而，现有方法主要依赖于模型中心的启发式方法（如谱统计、离群值检测或均匀分配），而忽略了数据的影响。为了应对这些局限性，我们提出了一种基于数据的框架LayerIF，该框架利用影响函数以一种原则性和任务敏感的方式量化每层的训练质量。通过隔离每一层的梯度，并通过计算层间影响来衡量验证损失对训练样本的敏感性，我们获得了基于数据的层重要性估计。值得注意的是，我们的方法为相同的LLM提供了特定任务的层重要性估计，揭示了不同测试时评估任务下层的专业化情况。我们通过将其应用于两种下游应用来展示我们的得分的实用价值：(a) LoRA-MoE架构中的专家分配，以及(b) LLM剪枝中的层间稀疏性分布。在多种LLM架构上的实验表明，我们的模型无关、基于影响的分配方法在任务性能上表现出一致的改进。', 'title_zh': 'LayerIF：使用影响函数估计大型语言模型层次质量'}
{'arxiv_id': 'arXiv:2505.23810', 'title': 'MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation', 'authors': 'Chenghao Yang, Yinbo Luo, Zhoufutu Wen, Qi Chu, Tao Gong, Longxiang Liu, Kaiyuan Zhang, Jianpeng Jiao, Ge Zhang, Wenhao Huang, Nenghai Yu', 'link': 'https://arxiv.org/abs/2505.23810', 'abstract': "Large Language Models (\\textbf{LLMs}), e.g. ChatGPT, have been widely adopted in real-world dialogue applications. However, LLMs' robustness, especially in handling long complex dialogue sessions, including frequent motivation transfer, sophisticated cross-turn dependency, is criticized all along. Nevertheless, no existing benchmarks can fully reflect these weaknesses. We present \\textbf{MARS-Bench}, a \\textbf{M}ulti-turn \\textbf{A}thletic \\textbf{R}eal-world \\textbf{S}cenario Dialogue \\textbf{Bench}mark, designed to remedy the gap. MARS-Bench is constructed from play-by-play text commentary so to feature realistic dialogues specifically designed to evaluate three critical aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn, and Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that closed-source LLMs significantly outperform open-source alternatives, explicit reasoning significantly boosts LLMs' robustness on handling long complex dialogue sessions, and LLMs indeed face significant challenges when handling motivation transfer and sophisticated cross-turn dependency. Moreover, we provide mechanistic interpretability on how attention sinks due to special tokens lead to LLMs' performance degradation when handling long complex dialogue sessions based on attention visualization experiment in Qwen2.5-7B-Instruction.", 'abstract_zh': '大规模语言模型（LLMs），例如ChatGPT，已在实际对话应用中被广泛采用。然而，LLMs在处理长且复杂的对话会话方面的稳健性，包括频繁的动力转移和复杂的跨轮依存关系，一直受到批评。尽管如此，现有的基准测试无法充分反映这些弱点。我们提出了MARS-Bench，一个多轮运动现实场景对话基准测试，旨在弥补这一缺口。MARS-Bench基于逐赛事的文字评论构建，旨在突出真实的对话，特别设计用于评估多轮对话的三个关键方面：超多轮、互动多轮和跨轮任务。对MARS-Bench的广泛实验还表明，闭源LLMs显著优于开源替代品，明确的推理显著增强了LLMs在处理长且复杂对话会话方面的稳健性，并且LLMs在处理动力转移和复杂的跨轮依存关系时确实面临着重大挑战。此外，我们根据Qwen2.5-7B-Instruction中的注意力可视化实验，提供了关于特殊标记导致注意力沉降如何影响LLMs在处理长且复杂对话会话时表现的机理解释。', 'title_zh': 'MARS-Bench：对话评价的多轮体育真实场景基准'}
{'arxiv_id': 'arXiv:2505.23809', 'title': 'LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion', 'authors': 'Haowei Yang, Haotian Lyu, Tianle Zhang, Dingzhou Wang, Yushang Zhao', 'link': 'https://arxiv.org/abs/2505.23809', 'abstract': "As e-commerce competition intensifies, balancing creative content with conversion effectiveness becomes critical. Leveraging LLMs' language generation capabilities, we propose a framework that integrates prompt engineering, multi-objective fine-tuning, and post-processing to generate marketing copy that is both engaging and conversion-driven. Our fine-tuning method combines sentiment adjustment, diversity enhancement, and CTA embedding. Through offline evaluations and online A/B tests across categories, our approach achieves a 12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content novelty. This provides a practical solution for automated copy generation and suggests paths for future multimodal, real-time personalization.", 'abstract_zh': '随着电子商务竞争的加剧，平衡创意内容与转化效果变得至关重要。利用大语言模型的语言生成能力，我们提出了一种框架，该框架结合了提示工程、多目标微调和后处理，以生成既具有吸引力又具有转化驱动性的营销文案。我们的微调方法结合了情绪调整、多样性和CTA嵌入。通过跨类别的离线评估和在线A/B测试，我们的方法在保持内容新颖性的同时，实现了12.5%的点击率（CTR）提升和8.3%的转化率（CVR）提升。这为自动化文案生成提供了一个实用的解决方案，并为未来多模态、实时个性化提供了路径。', 'title_zh': '基于LLM的电子商务营销内容优化：平衡创意与转化'}
{'arxiv_id': 'arXiv:2505.23808', 'title': 'DenseLoRA: Dense Low-Rank Adaptation of Large Language Models', 'authors': 'Lin Mu, Xiaoyu Wang, Li Ni, Yang Li, Zhize Wu, Peiquan Jin, Yiwen Zhang', 'link': 'https://arxiv.org/abs/2505.23808', 'abstract': "Low-rank adaptation (LoRA) has been developed as an efficient approach for adapting large language models (LLMs) by fine-tuning two low-rank matrices, thereby reducing the number of trainable parameters. However, prior research indicates that many of the weights in these matrices are redundant, leading to inefficiencies in parameter utilization. To address this limitation, we introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances parameter efficiency while achieving superior performance compared to LoRA. DenseLoRA builds upon the concept of representation fine-tuning, incorporating a single Encoder-Decoder to refine and compress hidden representations across all adaptation layers before applying adaptation. Instead of relying on two redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense low-rank matrix, improving parameter utilization and adaptation efficiency. We evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8% accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8% accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we conduct extensive experiments to systematically assess the impact of DenseLoRA's components on overall model performance. Code is available at this https URL.", 'abstract_zh': '密集低秩适应（DenseLoRA）：提高参数效率并实现优于LoRA的性能', 'title_zh': 'DenseLoRA: 密集低秩大型语言模型适应'}
{'arxiv_id': 'arXiv:2505.23807', 'title': 'DLP: Dynamic Layerwise Pruning in Large Language Models', 'authors': 'Yuli Chen, Bo Cheng, Jiale Han, Yingying Zhang, Yingting Li, Shuhao Zhang', 'link': 'https://arxiv.org/abs/2505.23807', 'abstract': 'Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at this https URL to facilitate future research.', 'abstract_zh': '动态层wise剪枝（DLP）：一种基于模型权重和输入激活信息的自适应剪枝方法', 'title_zh': 'DLP: 动态逐层剪枝在大规模语言模型中'}
{'arxiv_id': 'arXiv:2505.23806', 'title': 'MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation', 'authors': 'Sihyeon Lee, Hyunjoo Song, Jong-chan Lee, Yoon Jin Lee, Boram Lee, Hee-Eon Lim, Dongyeong Kim, Jinwook Seo, Bohyoung Kim', 'link': 'https://arxiv.org/abs/2505.23806', 'abstract': 'Deploying large language models (LLMs) in clinical settings faces critical trade-offs: cloud LLMs, with their extensive parameters and superior performance, pose risks to sensitive clinical data privacy, while local LLMs preserve privacy but often fail at complex clinical interpretation tasks. We propose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex clinical tasks into manageable subtasks and prompt generation, while a local LLM executes these subtasks in a privacy-preserving manner. Without accessing clinical data, the cloud LLM generates and validates subtask prompts using clinical guidelines and synthetic test cases. The local LLM executes subtasks locally and synthesizes outputs generated by the cloud LLM. We evaluate MedOrchestra on pancreatic cancer staging using 100 radiology reports under NCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy, outperforming local model baselines (without guideline: 48.94%, with guideline: 56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons: 65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches 85.42% accuracy, showing clear superiority across all settings.', 'abstract_zh': '在临床上部署大规模语言模型面临关键权衡：云端的大规模语言模型虽然参数丰富、性能优越，但会带来敏感临床数据隐私风险，而本地的大规模语言模型则能保护隐私但往往难以完成复杂的临床解释任务。我们提出MedOrchestra，这是一种混合框架，其中云端的大规模语言模型将复杂的临床任务分解为可管理的子任务和提示生成，而本地的大规模语言模型则以隐私保护的方式执行这些子任务。云端的大规模语言模型不访问临床数据，而是使用临床指南和合成测试案例生成和验证子任务提示。本地的大规模语言模型则在本地执行子任务并综合云端生成的输出结果。我们利用NCCN指南下的100份放射学报告，对MedOrchestra在胰腺癌分期的应用进行了评估。在自由文本报告中，MedOrchestra的准确率为70.21%，优于未使用指南的本地模型baseline（48.94%）和使用指南的本地模型baseline（56.59%），也优于胃肠病学专科医生（59.57%）、外科医生（65.96%）和放射科医生（55.32%）。在结构化报告中，MedOrchestra的准确率为85.42%，在所有设定中均表现优异。', 'title_zh': 'MedOrchestra: 一种临床数据解释的混合云-本地大语言模型方法'}
{'arxiv_id': 'arXiv:2505.23805', 'title': 'ADA: Automated Moving Target Defense for AI Workloads via Ephemeral Infrastructure-Native Rotation in Kubernetes', 'authors': 'Akram Sheriff, Ken Huang, Zsolt Nemeth, Madjid Nakhjiri', 'link': 'https://arxiv.org/abs/2505.23805', 'abstract': "This paper introduces the Adaptive Defense Agent (ADA), an innovative Automated Moving Target Defense (AMTD) system designed to fundamentally enhance the security posture of AI workloads. ADA operates by continuously and automatically rotating these workloads at the infrastructure level, leveraging the inherent ephemerality of Kubernetes pods. This constant managed churn systematically invalidates attacker assumptions and disrupts potential kill chains by regularly destroying and respawning AI service instances. This methodology, applying principles of chaos engineering as a continuous, proactive defense, offers a paradigm shift from traditional static defenses that rely on complex and expensive confidential or trusted computing solutions to secure the underlying compute platforms, while at the same time agnostically supporting the latest advancements in agentic and nonagentic AI ecosystems and solutions such as agent-to-agent (A2A) communication frameworks or model context protocols (MCP). This AI-native infrastructure design, relying on the widely proliferated cloud-native Kubernetes technologies, facilitates easier deployment, simplifies maintenance through an inherent zero trust posture achieved by rotation, and promotes faster adoption. We posit that ADA's novel approach to AMTD provides a more robust, agile, and operationally efficient zero-trust model for AI services, achieving security through proactive environmental manipulation rather than reactive patching.", 'abstract_zh': 'Adaptive Defense Agent：一种基于混沌工程的自适应防御代理系统', 'title_zh': 'ADA：通过Kubernetes中短暂基础设施原生旋转实现的AI工作负载自动化移动目标防御'}
{'arxiv_id': 'arXiv:2505.23804', 'title': 'Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies', 'authors': 'Terrance Liu, Shuyi Wang, Daniel Preotiuc-Pietro, Yash Chandarana, Chirag Gupta', 'link': 'https://arxiv.org/abs/2505.23804', 'abstract': 'While large language models (LLMs) achieve strong performance on text-to-SQL parsing, they sometimes exhibit unexpected failures in which they are confidently incorrect. Building trustworthy text-to-SQL systems thus requires eliciting reliable uncertainty measures from the LLM. In this paper, we study the problem of providing a calibrated confidence score that conveys the likelihood of an output query being correct. Our work is the first to establish a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In particular, we show that Platt scaling, a canonical method for calibration, provides substantial improvements over directly using raw model output probabilities as confidence scores. Furthermore, we propose a method for text-to-SQL calibration that leverages the structured nature of SQL queries to provide more granular signals of correctness, named "sub-clause frequency" (SCF) scores. Using multivariate Platt scaling (MPS), our extension of the canonical Platt scaling technique, we combine individual SCF scores into an overall accurate and calibrated score. Empirical evaluation on two popular text-to-SQL datasets shows that our approach of combining MPS and SCF yields further improvements in calibration and the related task of error detection over traditional Platt scaling.', 'abstract_zh': '基于大型语言模型的可信赖文本到SQL系统中的后验校准基准研究：一种结构化子句频率分数的方法', 'title_zh': '利用子句频率校准文本到SQL解析的LLM模型'}
{'arxiv_id': 'arXiv:2505.23803', 'title': 'MultiPhishGuard: An LLM-based Multi-Agent System for Phishing Email Detection', 'authors': 'Yinuo Xue, Eric Spero, Yun Sing Koh, Giovanni Russello', 'link': 'https://arxiv.org/abs/2505.23803', 'abstract': 'Phishing email detection faces critical challenges from evolving adversarial tactics and heterogeneous attack patterns. Traditional detection methods, such as rule-based filters and denylists, often struggle to keep pace with these evolving tactics, leading to false negatives and compromised security. While machine learning approaches have improved detection accuracy, they still face challenges adapting to novel phishing strategies. We present MultiPhishGuard, a dynamic LLM-based multi-agent detection system that synergizes specialized expertise with adversarial-aware reinforcement learning. Our framework employs five cooperative agents (text, URL, metadata, explanation simplifier, and adversarial agents) with automatically adjusted decision weights powered by a Proximal Policy Optimization reinforcement learning algorithm. To address emerging threats, we introduce an adversarial training loop featuring an adversarial agent that generates subtle context-aware email variants, creating a self-improving defense ecosystem and enhancing system robustness. Experimental evaluations on public datasets demonstrate that MultiPhishGuard significantly outperforms Chain-of-Thoughts, single-agent baselines and state-of-the-art detectors, as validated by ablation studies and comparative analyses. Experiments demonstrate that MultiPhishGuard achieves high accuracy (97.89\\%) with low false positive (2.73\\%) and false negative rates (0.20\\%). Additionally, we incorporate an explanation simplifier agent, which provides users with clear and easily understandable explanations for why an email is classified as phishing or legitimate. This work advances phishing defense through dynamic multi-agent collaboration and generative adversarial resilience.', 'abstract_zh': '基于多智能体的抗对抗强化学习多重钓鱼邮件检测系统', 'title_zh': 'MultiPhishGuard：一个基于LLM的多Agent系统用于钓鱼邮件检测'}
{'arxiv_id': 'arXiv:2505.23802', 'title': 'MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks', 'authors': 'Suhana Bedi, Hejie Cui, Miguel Fuentes, Alyssa Unell, Michael Wornow, Juan M. Banda, Nikesh Kotecha, Timothy Keyes, Yifan Mai, Mert Oez, Hao Qiu, Shrey Jain, Leonardo Schettini, Mehr Kashyap, Jason Alan Fries, Akshay Swaminathan, Philip Chung, Fateme Nateghi, Asad Aali, Ashwin Nayak, Shivam Vedak, Sneha S. Jain, Birju Patel, Oluseyi Fayanju, Shreya Shah, Ethan Goh, Dong-han Yao, Brian Soetikno, Eduardo Reis, Sergios Gatidis, Vasu Divi, Robson Capasso, Rachna Saralkar, Chia-Chun Chiang, Jenelle Jindal, Tho Pham, Faraz Ghoddusi, Steven Lin, Albert S. Chiou, Christy Hong, Mohana Roy, Michael F. Gensheimer, Hinesh Patel, Kevin Schulman, Dev Dash, Danton Char, Lance Downing, Francois Grolleau, Kameron Black, Bethel Mieso, Aydin Zahedivash, Wen-wai Yim, Harshita Sharma, Tony Lee, Hannah Kirsch, Jennifer Lee, Nerissa Ambers, Carlene Lugtu, Aditya Sharma, Bilal Mawji, Alex Alekseyev, Vicky Zhou, Vikas Kakkar, Jarrod Helzer, Anurang Revri, Yair Bannett, Roxana Daneshjou, Jonathan Chen, Emily Alsentzer, Keith Morse, Nirmal Ravi, Nima Aghaeepour, Vanessa Kennedy, Akshay Chaudhari, Thomas Wang, Sanmi Koyejo, Matthew P. Lungren, Eric Horvitz, Percy Liang, Mike Pfeffer, Nigam H. Shah', 'link': 'https://arxiv.org/abs/2505.23802', 'abstract': 'While large language models (LLMs) achieve near-perfect scores on medical licensing exams, these evaluations inadequately reflect the complexity and diversity of real-world clinical practice. We introduce MedHELM, an extensible evaluation framework for assessing LLM performance for medical tasks with three key contributions. First, a clinician-validated taxonomy spanning 5 categories, 22 subcategories, and 121 tasks developed with 29 clinicians. Second, a comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly formulated) providing complete coverage of all categories and subcategories in the taxonomy. Third, a systematic comparison of LLMs with improved evaluation methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9 frontier LLMs, using the 35 benchmarks, revealed significant performance variation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64% win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved comparable results at 40% lower estimated computational cost. On a normalized accuracy scale (0-1), most models performed strongly in Clinical Note Generation (0.73-0.85) and Patient Communication & Education (0.78-0.83), moderately in Medical Research Assistance (0.65-0.75), and generally lower in Clinical Decision Support (0.56-0.72) and Administration & Workflow (0.53-0.63). Our LLM-jury evaluation method achieved good agreement with clinician ratings (ICC = 0.47), surpassing both average clinician-clinician agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top models at lower estimated cost. These findings highlight the importance of real-world, task-specific evaluation for medical use of LLMs and provides an open source framework to enable this.', 'abstract_zh': '尽管大型语言模型在医学执照考试中取得了近乎完美的成绩，但这些评估未能充分反映现实临床实践中复杂性和多样性。我们介绍了MedHELM，这是一种评估大型语言模型在医学任务性能的可扩展框架，包含三项关键贡献：一个由29名临床医生验证的涵盖5个类别、22个亚类别和121个任务的分类体系；一个全面的基准套件，包括35个基准（17个现有，18个新制定的），覆盖分类体系中的所有类别和亚类别；以及一种系统性比较基于改进评估方法（使用大型语言模型陪审团）及成本-性能分析的方法。使用35个基准对9个前沿大型语言模型的评估显示了显著的性能差异。高级推理模型（DeepSeek R1：66%胜率；o3-mini：64%胜率）表现出色，尽管Claude 3.5 Sonnet在估计计算成本低40%的情况下取得了类似结果。在归一化准确度尺度（0-1）上，大多数模型在临床病历生成（0.73-0.85）和患者沟通与教育（0.78-0.83）方面表现强劲，在医学研究援助（0.65-0.75）方面表现适度，在临床决策支持（0.56-0.72）和行政与工作流程（0.53-0.63）方面表现一般。我们的大型语言模型陪审团评估方法与临床医生评分的共识度良好（ICC=0.47），超过了临床医生-临床医生共识度平均值（ICC=0.43）和包括ROUGE-L（0.36）和BERTScore-F1（0.44）在内的自动化基线。Claude 3.5 Sonnet在较低的估计成本下达到了顶级模型的性能。这些发现突显了在医学中使用大型语言模型进行现实世界的、任务特定的评估的重要性，并提供了一个开源框架来实现这一点。', 'title_zh': 'MedHELM: 医疗任务中大规模语言模型的整体评估'}
{'arxiv_id': 'arXiv:2505.23801', 'title': 'SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks', 'authors': 'Sajid Hussain, Muhammad Sohail, Nauman Ali Khan', 'link': 'https://arxiv.org/abs/2505.23801', 'abstract': 'Background: Federated Learning (FL) has emerged as a promising paradigm for training machine learning models while preserving data privacy. However, applying FL to Natural Language Processing (NLP) tasks presents unique challenges due to semantic heterogeneity across clients, vocabulary mismatches, and varying resource constraints on edge devices. Objectives: This paper introduces SEMFED, a novel semantic-aware resource-efficient federated learning framework specifically designed for heterogeneous NLP tasks. Methods: SEMFED incorporates three key innovations: (1) a semantic-aware client selection mechanism that balances semantic diversity with resource constraints, (2) adaptive NLP-specific model architectures tailored to device capabilities while preserving semantic information, and (3) a communication-efficient semantic feature compression technique that significantly reduces bandwidth requirements. Results: Experimental results on various NLP classification tasks demonstrate that SEMFED achieves an 80.5% reduction in communication costs while maintaining model accuracy above 98%, outperforming state-of-the-art FL approaches. Conclusion: SEMFED effectively manages heterogeneous client environments with varying computational resources, network reliability, and semantic data distributions, making it particularly suitable for real-world federated NLP deployments.', 'abstract_zh': '背景：联邦学习（FL）作为一种在保护数据隐私的同时训练机器学习模型的有前途的范式已经出现。然而，将FL应用于自然语言处理（NLP）任务由于客户端之间语义异质性、词汇不匹配以及边缘设备上不同的资源约束带来了独特的挑战。目标：本文提出了一种名为SEMFED的新颖语义意识资源高效联邦学习框架，专门针对异构NLP任务。方法：SEMFED包含三个关键创新：（1）一种语义意识的客户端选择机制，平衡语义多样性和资源约束；（2）根据设备能力定制适应性的NLP特定模型架构，同时保留语义信息；（3）一种通信高效的语义特征压缩技术，显著减少了带宽要求。结果：在多种NLP分类任务上的实验结果表明，SEMFED在通信成本降低了80.5%的同时，模型准确性保持在98%以上，优于最先进的FL方法。结论：SEMFED有效地管理了具有不同计算资源、网络可靠性和语义数据分布的异构客户端环境，使其特别适合实际部署中的联邦NLP应用。', 'title_zh': 'SEMFED: 具有语义意识的资源高效联邦学习方法用于异构NLP任务'}
{'arxiv_id': 'arXiv:2505.23799', 'title': 'Estimating LLM Consistency: A User Baseline vs Surrogate Metrics', 'authors': 'Xiaoyuan Wu, Weiran Lin, Omer Akgul, Lujo Bauer', 'link': 'https://arxiv.org/abs/2505.23799', 'abstract': "Large language models (LLMs) are prone to hallucinations and sensitive to prompt perturbations, often resulting in inconsistent or unreliable generated text. Different methods have been proposed to mitigate such hallucinations and fragility -- one of them being measuring the consistency (the model's confidence in the response, or likelihood of generating a similar response when resampled) of LLM responses. In previous work, measuring consistency often relied on the probability of a response appearing within a pool of resampled responses, or internal states or logits of responses. However, it is not yet clear how well these approaches approximate how humans perceive the consistency of LLM responses. We performed a user study (n=2,976) and found current methods typically do not approximate users' perceptions of LLM consistency very well. We propose a logit-based ensemble method for estimating LLM consistency, and we show that this method matches the performance of the best-performing existing metric in estimating human ratings of LLM consistency. Our results suggest that methods of estimating LLM consistency without human evaluation are sufficiently imperfect that we suggest evaluation with human input be more broadly used.", 'abstract_zh': '大型语言模型（LLMs）容易出现幻觉并对外部提示变化敏感，常导致生成文本的一致性和可靠性受损。提出了多种方法来减轻这些幻觉和脆弱性——其中之一是衡量LLM响应的一致性（模型对响应的信心，或在重新采样时生成类似响应的可能性）。在以往的研究中，衡量一致性通常依赖于响应在重新采样响应池中的概率，或响应的内部状态或logits。然而，目前尚不清楚这些方法如何接近人类对LLM一致性感知的实际情况。我们进行了用户研究（n=2,976）并发现，当前的方法通常不能很好地接近用户的LLM一致性感知。我们提出了一种基于logits的组合方法来估算LLM的一致性，并证明该方法在估算人类对LLM一致性评级的性能上与现有的最佳指标相当。我们的结果表明，无需人类评估来估算LLM一致性的方法存在足够的缺陷，因此建议更广泛地使用包含人类输入的评估方法。', 'title_zh': '评估LLM一致性：用户基准 vs 替代指标'}
{'arxiv_id': 'arXiv:2505.23798', 'title': "My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals", 'authors': 'Jian Lan, Yifei Fu, Udo Schlegel, Gengyuan Zhang, Tanveer Hannan, Haokun Chen, Thomas Seidl', 'link': 'https://arxiv.org/abs/2505.23798', 'abstract': "Social bias is a critical issue in large vision-language models (VLMs), where fairness- and ethics-related problems harm certain groups of people in society. It is unknown to what extent VLMs yield social bias in generative responses. In this study, we focus on evaluating and mitigating social bias on both the model's response and probability distribution. To do so, we first evaluate four state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the multiple-choice selection task. Surprisingly, we find that models suffer from generating gender-biased or race-biased responses. We also observe that models are prone to stating their responses are fair, but indeed having mis-calibrated confidence levels towards particular social groups. While investigating why VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit substantial fluctuations in fairness levels. Meanwhile, residuals in each layer show mixed effects on fairness, with some contributing positively while some lead to increased bias. Based on these findings, we propose a post-hoc method for the inference stage to mitigate social bias, which is training-free and model-agnostic. We achieve this by ablating bias-associated residuals while amplifying fairness-associated residuals on model hidden layers during inference. We demonstrate that our post-hoc method outperforms the competing training strategies, helping VLMs have fairer responses and more reliable confidence levels.", 'abstract_zh': '大型视觉-语言模型中的社会偏见是关键问题，其中公平性和伦理相关的问题损害了社会上的某些群体。尚不清楚大型视觉-语言模型在生成响应时是否会表现出社会偏见。在本研究中，我们侧重于评估和缓解模型响应及其概率分布中的社会偏见。为此，我们首先使用多个选择任务在PAIRS和SocialCounterfactuals数据集上评估了四种最先进的视觉-语言模型。令人惊讶的是，我们发现模型在生成性别偏向或种族偏向的回答方面存在缺陷。我们还观察到，模型倾向于声称其回答是公平的，但实际上对于特定的社会群体具有误校准的信心水平。在研究为什么视觉-语言模型在此过程中表现出不公平时，我们发现其隐藏层在公平性水平上存在显著波动。同时，每一层的残差表现出混合效应，有些有利于公平性，而有些则会增加偏见。基于这些发现，我们提出了一种后处理方法，用于推理阶段以缓解社会偏见，这种方法无需训练且适用于任何模型。我们在推理过程中通过消除与偏见相关的残差并对与公平性相关的残差进行放大来实现这一点。我们证明了我们的后处理方法优于竞争性训练策略，有助于使视觉-语言模型的回答更加公平，并提高其信心水平。', 'title_zh': '我的答案不说是“公平”：通过公平与偏见残差缓解视觉-语言模型中的社会偏见'}
{'arxiv_id': 'arXiv:2505.23797', 'title': 'Detection of Suicidal Risk on Social Media: A Hybrid Model', 'authors': 'Zaihan Yang, Ryan Leonard, Hien Tran, Rory Driscoll, Chadbourne Davis', 'link': 'https://arxiv.org/abs/2505.23797', 'abstract': "Suicidal thoughts and behaviors are increasingly recognized as a critical societal concern, highlighting the urgent need for effective tools to enable early detection of suicidal risk. In this work, we develop robust machine learning models that leverage Reddit posts to automatically classify them into four distinct levels of suicide risk severity. We frame this as a multi-class classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa), a state-of-the-art deep learning transformer model, with the statistical term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy and reliability of suicide risk assessment. To address data imbalance and overfitting, we explore various data resampling techniques and data augmentation strategies to enhance model generalization. Additionally, we compare our model's performance against that of using RoBERTa only, the BERT model and other traditional machine learning classifiers. Experimental results demonstrate that the hybrid model can achieve improved performance, giving a best weighted $F_{1}$ score of 0.7512.", 'abstract_zh': '自杀念头和行为日益被认可为一个重要的社会问题，突显了早期检测自杀风险的有效工具的迫切需求。本文中，我们开发了稳健的机器学习模型，利用Reddit帖子自动将它们分类为四种不同的自杀风险严重程度级别。我们将此视为一个多分类分类任务，并提出了一种结合了Robustly Optimized BERT Approach（RoBERTa）的深度上下文嵌入、TF-IDF词频权重和PCA压缩的混合模型，以提高自杀风险评估的准确性和可靠性。为了应对数据不平衡和过拟合问题，我们探索了多种数据重采样技术和数据增强策略以提高模型的泛化能力。此外，我们将我们的模型性能与仅使用RoBERTa、BERT模型以及其他传统机器学习分类器进行比较。实验结果表明，混合模型可以实现更好的性能，获得最佳加权F1分数0.7512。', 'title_zh': '社交媒体中自杀风险的检测：一种混合模型'}
{'arxiv_id': 'arXiv:2505.23794', 'title': 'R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning', 'authors': 'Yuan Li, Qi Luo, Xiaonan Li, Bufan Li, Qinyuan Cheng, Bo Wang, Yining Zheng, Yuxin Wang, Zhangyue Yin, Xipeng Qiu', 'link': 'https://arxiv.org/abs/2505.23794', 'abstract': 'Retrieval-Augmented Generation (RAG) integrates external knowledge with Large Language Models (LLMs) to enhance factual correctness and mitigate hallucination. However, dense retrievers often become the bottleneck of RAG systems due to their limited parameters compared to LLMs and their inability to perform step-by-step reasoning. While prompt-based iterative RAG attempts to address these limitations, it is constrained by human-designed workflows. To address these limitations, we propose $\\textbf{R3-RAG}$, which uses $\\textbf{R}$einforcement learning to make the LLM learn how to $\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving comprehensive external knowledge and leading to correct answers. R3-RAG is divided into two stages. We first use cold start to make the model learn the manner of iteratively interleaving reasoning and retrieval. Then we use reinforcement learning to further harness its ability to better explore the external retrieval environment. Specifically, we propose two rewards for R3-RAG: 1) answer correctness for outcome reward, which judges whether the trajectory leads to a correct answer; 2) relevance-based document verification for process reward, encouraging the model to retrieve documents that are relevant to the user question, through which we can let the model learn how to iteratively reason and retrieve relevant documents to get the correct answer. Experimental results show that R3-RAG significantly outperforms baselines and can transfer well to different retrievers. We release R3-RAG at this https URL.', 'abstract_zh': 'Retrieval-Augmented Generation with Reinforcement Learning for Step-by-Step Reasoning and Retrieval', 'title_zh': 'R3-RAG：通过强化学习学习逐步推理和检索的LLM方法'}
{'arxiv_id': 'arXiv:2505.23793', 'title': 'USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models', 'authors': 'Baolin Zheng, Guanlin Chen, Hongqiong Zhong, Qingyang Teng, Yingshui Tan, Zhendong Liu, Weixun Wang, Jiaheng Liu, Jian Yang, Huiyun Jing, Jincheng Wei, Wenbo Su, Xiaoyong Zhu, Bo Zheng, Kaifu Zhang', 'link': 'https://arxiv.org/abs/2505.23793', 'abstract': 'Despite their remarkable achievements and widespread adoption, Multimodal Large Language Models (MLLMs) have revealed significant security vulnerabilities, highlighting the urgent need for robust safety evaluation benchmarks. Existing MLLM safety benchmarks, however, fall short in terms of data quality and coverge, and modal risk combinations, resulting in inflated and contradictory evaluation results, which hinders the discovery and governance of security concerns. Besides, we argue that vulnerabilities to harmful queries and oversensitivity to harmless ones should be considered simultaneously in MLLMs safety evaluation, whereas these were previously considered separately. In this paper, to address these shortcomings, we introduce Unified Safety Benchmarks (USB), which is one of the most comprehensive evaluation benchmarks in MLLM safety. Our benchmark features high-quality queries, extensive risk categories, comprehensive modal combinations, and encompasses both vulnerability and oversensitivity evaluations. From the perspective of two key dimensions: risk categories and modality combinations, we demonstrate that the available benchmarks -- even the union of the vast majority of them -- are far from being truly comprehensive. To bridge this gap, we design a sophisticated data synthesis pipeline that generates extensive, high-quality complementary data addressing previously unexplored aspects. By combining open-source datasets with our synthetic data, our benchmark provides 4 distinct modality combinations for each of the 61 risk sub-categories, covering both English and Chinese across both vulnerability and oversensitivity dimensions.', 'abstract_zh': '尽管多模态大型语言模型在成就和广泛应用方面取得了显著进展，但它们揭示了重大安全漏洞，突显了对稳健安全评估基准的迫切需求。现有的多模态大型语言模型安全基准在数据质量和覆盖面、模态风险组合方面存在不足，导致评价结果夸大和矛盾，阻碍了安全问题的发现和治理。此外，我们认为在多模态大型语言模型的安全评估中，有害查询的漏洞和无害查询的过度敏感性应同时予以考虑，而以前这些方面的评估是单独进行的。为了应对这些不足，本文引入了统一安全基准（USB），这是迄今为止最全面的多模态大型语言模型安全评价基准之一。我们的基准具有高质量的查询、广泛的危险类别、全面的模态组合，并涵盖了漏洞和过度敏感性评估。从两个关键维度：危险类别和模态组合的角度来看，我们表明，即使是最主要部分的现有基准也不够全面。为了弥合这一差距，我们设计了一套复杂的数据合成管道，以生成广泛的高质量补充数据，涵盖之前未探索的方面。通过结合开源数据集和合成数据，我们的基准为每个61个风险子类别提供了4种不同的模态组合，分别涵盖了英语和中文在漏洞和过度敏感性维度上的全面覆盖。', 'title_zh': 'USB：面向多模态大型语言模型的综合性统一安全性评估基准'}
{'arxiv_id': 'arXiv:2505.23792', 'title': 'Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things', 'authors': 'Kai Li, Conggai Li, Xin Yuan, Shenghong Li, Sai Zou, Syed Sohail Ahmed, Wei Ni, Dusit Niyato, Abbas Jamalipour, Falko Dressler, Ozgur B. Akan', 'link': 'https://arxiv.org/abs/2505.23792', 'abstract': 'This paper focuses on Zero-Trust Foundation Models (ZTFMs), a novel paradigm that embeds zero-trust security principles into the lifecycle of foundation models (FMs) for Internet of Things (IoT) systems. By integrating core tenets, such as continuous verification, least privilege access (LPA), data confidentiality, and behavioral analytics into the design, training, and deployment of FMs, ZTFMs can enable secure, privacy-preserving AI across distributed, heterogeneous, and potentially adversarial IoT environments. We present the first structured synthesis of ZTFMs, identifying their potential to transform conventional trust-based IoT architectures into resilient, self-defending ecosystems. Moreover, we propose a comprehensive technical framework, incorporating federated learning (FL), blockchain-based identity management, micro-segmentation, and trusted execution environments (TEEs) to support decentralized, verifiable intelligence at the network edge. In addition, we investigate emerging security threats unique to ZTFM-enabled systems and evaluate countermeasures, such as anomaly detection, adversarial training, and secure aggregation. Through this analysis, we highlight key open research challenges in terms of scalability, secure orchestration, interpretable threat attribution, and dynamic trust calibration. This survey lays a foundational roadmap for secure, intelligent, and trustworthy IoT infrastructures powered by FMs.', 'abstract_zh': '基于零信任原则的基金会模型：构建安全可靠的物联网系统的新范式', 'title_zh': '零信任基础模型：物联网安全协作人工智能的新范式'}
{'arxiv_id': 'arXiv:2505.23791', 'title': 'Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning', 'authors': 'Sayyed Farid Ahamed, Sandip Roy, Soumya Banerjee, Marc Vucovich, Kevin Choi, Abdul Rahman, Alison Hu, Edward Bowen, Sachin Shetty', 'link': 'https://arxiv.org/abs/2505.23791', 'abstract': "Federated Learning (FL) is a collaborative learning framework designed to protect client data, yet it remains highly vulnerable to Intellectual Property (IP) threats. Model extraction (ME) attacks pose a significant risk to Machine Learning as a Service (MLaaS) platforms, enabling attackers to replicate confidential models by querying black-box (without internal insight) APIs. Despite FL's privacy-preserving goals, its distributed nature makes it particularly susceptible to such attacks. This paper examines the vulnerability of FL-based victim models to two types of model extraction attacks. For various federated clients built under the NVFlare platform, we implemented ME attacks across two deep learning architectures and three image datasets. We evaluate the proposed ME attack performance using various metrics, including accuracy, fidelity, and KL divergence. The experiments show that for different FL clients, the accuracy and fidelity of the extracted model are closely related to the size of the attack query set. Additionally, we explore a transfer learning based approach where pretrained models serve as the starting point for the extraction process. The results indicate that the accuracy and fidelity of the fine-tuned pretrained extraction models are notably higher, particularly with smaller query sets, highlighting potential advantages for attackers.", 'abstract_zh': '联邦学习（FL）是一种旨在保护客户端数据的协作学习框架，但仍高度 Vulnerable 至知识产权（IP）威胁。模型提取（ME）攻击对作为服务的机器学习（MLaaS）平台构成了重大风险，使攻击者能够通过查询黑盒（无内部见解）API 复制保密模型。尽管联邦学习具备隐私保护目标，但其分布式特性使其特别容易遭受此类攻击。本文探讨了基于 FL 的受害模型对两种类型模型提取攻击的脆弱性。我们在 NVFlare 平台上实现了针对两种深度学习架构和三种图像数据集的 ME 攻击。我们使用准确性、保真度和 KL 分散度等多种指标评估提出的 ME 攻击性能。实验表明，对于不同的 FL 客户端，提取模型的准确性和保真度与其攻击查询集的大小密切相关。此外，我们还探索了一种迁移学习方法，其中预训练模型作为提取过程的起点。结果表明，调整后的预训练提取模型的准确性和保真度显著提高，尤其是在查询集较小的情况下，突显了潜在的攻击优势。', 'title_zh': '基于迁移学习的模型提取攻击在联邦学习中查询效率和准确性的评估'}
{'arxiv_id': 'arXiv:2505.23790', 'title': 'Rethinking the Understanding Ability across LLMs through Mutual Information', 'authors': 'Shaojie Wang, Sirui Ding, Na Zou', 'link': 'https://arxiv.org/abs/2505.23790', 'abstract': 'Recent advances in large language models (LLMs) have revolutionized natural language processing, yet evaluating their intrinsic linguistic understanding remains challenging. Moving beyond specialized evaluation tasks, we propose an information-theoretic framework grounded in mutual information (MI) to achieve this. We formalize the understanding as MI between an input sentence and its latent representation (sentence-level MI), measuring how effectively input information is preserved in latent representation. Given that LLMs learn embeddings for individual tokens, we decompose sentence-level MI into token-level MI between tokens and sentence embeddings, establishing theoretical bounds connecting these measures. Based on this foundation, we theoretically derive a computable lower bound for token-level MI using Fano\'s inequality, which directly relates to token-level recoverability-the ability to predict original tokens from sentence embedding. We implement this recoverability task to comparatively measure MI across different LLMs, revealing that encoder-only models consistently maintain higher information fidelity than their decoder-only counterparts, with the latter exhibiting a distinctive late-layer "forgetting" pattern where mutual information is first enhanced and then discarded. Moreover, fine-tuning to maximize token-level recoverability consistently improves understanding ability of LLMs on tasks without task-specific supervision, demonstrating that mutual information can serve as a foundation for understanding and improving language model capabilities.', 'abstract_zh': 'Recent Advances in Large Language Models: An Information-Theoretic Framework Based on Mutual Information for Evaluating Intrinsic Linguistic Understanding', 'title_zh': '重思LLM之间的相互信息理解能力'}
{'arxiv_id': 'arXiv:2505.23788', 'title': "Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework", 'authors': 'Aakash Sen Sharma, Debdeep Sanyal, Priyansh Srivastava, Sundar Atreya H., Shirish Karande, Mohan Kankanhalli, Murari Mandal', 'link': 'https://arxiv.org/abs/2505.23788', 'abstract': 'Large language models (LLMs) commonly risk copyright infringement by reproducing protected content verbatim or with insufficient transformative modifications, posing significant ethical, legal, and practical concerns. Current inference-time safeguards predominantly rely on restrictive refusal-based filters, often compromising the practical utility of these models. To address this, we collaborated closely with intellectual property experts to develop FUA-LLM (Fair Use Aligned Language Models), a legally-grounded framework explicitly designed to align LLM outputs with fair-use doctrine. Central to our method is FairUseDB, a carefully constructed dataset containing 18,000 expert-validated examples covering nine realistic infringement scenarios. Leveraging this dataset, we apply Direct Preference Optimization (DPO) to fine-tune open-source LLMs, encouraging them to produce legally compliant and practically useful alternatives rather than resorting to blunt refusal. Recognizing the shortcomings of traditional evaluation metrics, we propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic Mean (CAH) to balance infringement risk against response utility. Extensive quantitative experiments coupled with expert evaluations confirm that FUA-LLM substantially reduces problematic outputs (up to 20\\%) compared to state-of-the-art approaches, while preserving real-world usability.', 'abstract_zh': '大型语言模型（LLMs）通过直接复制或缺乏足够的转化性修改来重现受保护内容，这会引发严重的版权侵权风险，带来伦理、法律和实践上的问题。当前的推理时防护措施主要依赖于限制性的拒绝型过滤器，这往往牺牲了这些模型的实际效用。为解决这一问题，我们与知识产权专家紧密合作，开发了FUA-LLM（Fair Use Aligned Language Models），这是一个具有法律依据的框架，旨在明确使LLM的输出符合合理使用原则。我们方法的核心是FairUseDB，这是一个精心构建的数据集，包含18,000个专家验证的示例，覆盖了九种现实中的侵权场景。通过利用这个数据集，我们应用直接偏好优化（DPO）来微调开源LLM，鼓励它们生成合法合规且实用的替代方案，而不仅仅是采取武断的拒绝。鉴于传统评价指标的不足，我们提出新的指标：加权罚分效用和合规意识调和平均（CAH），以平衡侵权风险与回应效用。大量定量实验结合专家评估证实，FUA-LLM相较于最先进的方法显著减少了有问题的输出（最高可达20%），同时保持了现实世界的实用性。', 'title_zh': '九种违反版权法的方式及我们LLM为何不会：一种符合合理使用原则的生成框架'}
{'arxiv_id': 'arXiv:2505.23786', 'title': 'Mind the Gap: A Practical Attack on GGUF Quantization', 'authors': 'Kazuki Egashira, Robin Staab, Mark Vero, Jingxuan He, Martin Vechev', 'link': 'https://arxiv.org/abs/2505.23786', 'abstract': 'With the increasing size of frontier LLMs, post-training quantization has become the standard for memory-efficient deployment. Recent work has shown that basic rounding-based quantization schemes pose security risks, as they can be exploited to inject malicious behaviors into quantized models that remain hidden in full precision. However, existing attacks cannot be applied to more complex quantization methods, such as the GGUF family used in the popular ollama and this http URL frameworks. In this work, we address this gap by introducing the first attack on GGUF. Our key insight is that the quantization error -- the difference between the full-precision weights and their (de-)quantized version -- provides sufficient flexibility to construct malicious quantized models that appear benign in full precision. Leveraging this, we develop an attack that trains the target malicious LLM while constraining its weights based on quantization errors. We demonstrate the effectiveness of our attack on three popular LLMs across nine GGUF quantization data types on three diverse attack scenarios: insecure code generation ($\\Delta$=$88.7\\%$), targeted content injection ($\\Delta$=$85.0\\%$), and benign instruction refusal ($\\Delta$=$30.1\\%$). Our attack highlights that (1) the most widely used post-training quantization method is susceptible to adversarial interferences, and (2) the complexity of quantization schemes alone is insufficient as a defense.', 'abstract_zh': '随着前沿大语言模型的规模不断增加，后训练量化已成为内存高效部署的标准方法。现有研究表明，基于基本舍入的量化方案可能存在安全风险，因为它们可以被利用在量化模型中注入恶意行为，而这些行为在高精度下仍保持隐藏状态。然而，现有的攻击方法无法应用于更复杂的量化方法，如流行框架ollama和this http URL所使用的GGUF家族。在本工作中，我们通过引入针对GGUF的首个攻击方法来弥补这一空白。我们的核心洞察是，量化误差——全精度权重与其量化版本之间的差异——提供了足够的灵活性，以构建在全精度下看似无害的恶意量化模型。利用这一观察，我们开发了一种攻击方法，该方法在约束权重基于量化误差的同时训练目标恶意大语言模型。我们在三种流行的大型语言模型的九种不同的攻击场景中演示了该攻击的有效性：不安全的代码生成（$\\Delta$=$88.7\\%$）、目标内容注入（$\\Delta$=$85.0\\%$）和良性指令拒绝（$\\Delta$=$30.1\\%$）。该攻击突显了两点：（1）最常用的后训练量化方法容易受到恶意干扰的影响；（2）仅量化方案的复杂性不足以作为防御措施。', 'title_zh': '注意差距：对GGUF量化的一种实用攻击'}
{'arxiv_id': 'arXiv:2505.23785', 'title': 'Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale', 'authors': 'Cody Kommers, Drew Hemment, Maria Antoniak, Joel Z. Leibo, Hoyt Long, Emily Robinson, Adam Sobey', 'link': 'https://arxiv.org/abs/2505.23785', 'abstract': 'This position paper argues that large language models (LLMs) can make cultural context, and therefore human meaning, legible at an unprecedented scale in AI-based sociotechnical systems. We argue that such systems have previously been unable to represent human meaning because they rely on thin descriptions: numerical representations that enforce standardization and therefore strip human activity of the cultural context that gives it meaning. By contrast, scholars in the humanities and qualitative social sciences have developed frameworks for representing meaning through thick description: verbal representations that accommodate heterogeneity and retain contextual information needed to represent human meaning. While these methods can effectively codify meaning, they are difficult to deploy at scale. However, the verbal capabilities of LLMs now provide a means of (at least partially) automating the generation and processing of thick descriptions, potentially overcoming this bottleneck. We argue that the problem of rendering human meaning legible is not just about selecting better metrics, but about developing new representational formats (based on thick description). We frame this as a crucial direction for the application of generative AI and identify five key challenges: preserving context, maintaining interpretive pluralism, integrating perspectives based on lived experience and critical distance, distinguishing qualitative content from quantitative magnitude, and acknowledging meaning as dynamic rather than static. Furthermore, we suggest that thick description has the potential to serve as a unifying framework to address a number of emerging concerns about the difficulties of representing culture in (or using) LLMs.', 'abstract_zh': '论文化语境下大型语言模型如何在人工智能社会技术系统中使人类意义变得可读', 'title_zh': '意义不是度量：使用大语言模型大规模使文化背景明晰'}
{'arxiv_id': 'arXiv:2505.23783', 'title': 'Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning', 'authors': 'Korel Gundem, Juncheng Dong, Dennis Zhang, Vahid Tarokh, Zhengling Qi', 'link': 'https://arxiv.org/abs/2505.23783', 'abstract': "In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new tasks with just a few examples, but their predictions often suffer from systematic biases, leading to unstable performances in classification. While calibration techniques are proposed to mitigate these biases, we show that, in the logit space, many of these methods are equivalent to merely shifting the LLM's decision boundary without having the ability to alter its orientation. This proves inadequate when biases cause the LLM to be severely misdirected. To address these limitations and provide a unifying framework, we propose Supervised Calibration (SC), a loss-minimization based framework which learns an optimal, per-class affine transformation of the LLM's predictive probabilities in the logit space without requiring external data beyond the context. By using a more expressive functional class, SC not only subsumes many existing calibration methods in ICL as special cases, but also enables the ability to alter and even completely reverse the orientation of the LLM's decision boundary. Furthermore, SC's loss-based nature facilitates the seamless integration of two purpose-built regularization techniques: context-invariance and directional trust-region. The former is designed to tackle the instability issue in ICL, while the latter controls the degree of calibration. Finally, SC delivers state-of-the-art performance over calibration baselines in the 4-shot, 8-shot, and 16-shot settings across all nine datasets for Mistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct.", 'abstract_zh': '基于监督校准的上下文学习校准框架（Supervised Calibration for Context Learning）', 'title_zh': '通过经典监督学习的视角增强LLMs的上下文学习能力'}
{'arxiv_id': 'arXiv:2505.23782', 'title': '4,500 Seconds: Small Data Training Approaches for Deep UAV Audio Classification', 'authors': 'Andrew P. Berg, Qian Zhang, Mia Y. Wang', 'link': 'https://arxiv.org/abs/2505.23782', 'abstract': 'Unmanned aerial vehicle (UAV) usage is expected to surge in the coming decade, raising the need for heightened security measures to prevent airspace violations and security threats. This study investigates deep learning approaches to UAV classification focusing on the key issue of data scarcity. To investigate this we opted to train the models using a total of 4,500 seconds of audio samples, evenly distributed across a 9-class dataset. We leveraged parameter efficient fine-tuning (PEFT) and data augmentations to mitigate the data scarcity. This paper implements and compares the use of convolutional neural networks (CNNs) and attention-based transformers. Our results show that, CNNs outperform transformers by 1-2\\% accuracy, while still being more computationally efficient. These early findings, however, point to potential in using transformers models; suggesting that with more data and further optimizations they could outperform CNNs. Future works aims to upscale the dataset to better understand the trade-offs between these approaches.', 'abstract_zh': '无人飞行器(UAV)的使用预计将在未来十年大幅增长，从而提高了对 airspace违规和安全威胁预防的更高安全措施的需求。本研究探讨了深度学习方法在无人机分类中的应用，重点关注数据稀缺性这一关键问题。为研究这一问题，我们使用了总计4500秒的音频样本进行模型训练，这些样本被均匀分配在九类数据集中。我们利用参数高效微调(PEFT)和数据增强来缓解数据稀缺性。本文实现了并比较了卷积神经网络(CNNs)和基于注意力的变换器的使用。结果显示，卷积神经网络的准确率比变换器高1-2%，同时计算效率更高。然而，这些初步发现表明，变换器模型具有潜力，随着数据量的增加和进一步优化，它们可能会超越卷积神经网络。未来的工作旨在扩大数据集，以更好地理解这些方法之间的权衡。', 'title_zh': '4,500秒：面向深度无人机音频分类的小数据训练方法'}
{'arxiv_id': 'arXiv:2505.23780', 'title': 'More-than-Human Storytelling: Designing Longitudinal Narrative Engagements with Generative AI', 'authors': 'Émilie Fabre, Katie Seaborn, Shuta Koiwai, Mizuki Watanabe, Paul Riesch', 'link': 'https://arxiv.org/abs/2505.23780', 'abstract': 'Longitudinal engagement with generative AI (GenAI) storytelling agents is a timely but less charted domain. We explored multi-generational experiences with "Dreamsmithy," a daily dream-crafting app, where participants (N = 28) co-created stories with AI narrator "Makoto" every day. Reflections and interactions were captured through a two-week diary study. Reflexive thematic analysis revealed themes likes "oscillating ambivalence" and "socio-chronological bonding," highlighting the complex dynamics that emerged between individuals and the AI narrator over time. Findings suggest that while people appreciated the personal notes, opportunities for reflection, and AI creativity, limitations in narrative coherence and control occasionally caused frustration. The results underscore the potential of GenAI for longitudinal storytelling, but also raise critical questions about user agency and ethics. We contribute initial empirical insights and design considerations for developing adaptive, more-than-human storytelling systems.', 'abstract_zh': '长期参与生成式AI讲故事代理的互动是一个及时但未充分探索的领域。我们通过“Dreamsmithy”这一每日梦境创作应用探索了代际经验，参与者（N=28）每日与AI叙述者“Makoto”共同创作故事。通过两周的日记研究记录了反思和互动。反思性主题分析揭示了“波动的矛盾”和“社会时间纽带”等主题，突出了个体与AI叙述者之间随时间演变的复杂动态。研究发现，尽管人们赞赏个人笔记、反思机会和AI的创造力，但叙事连贯性和控制的限制偶尔也会引起挫败感。结果强调了生成式AI在 longitudinal 故事创作中的潜在价值，同时也引发了关于用户自主性和伦理的关键问题。我们提供了初始的实证见解和设计考虑，用于开发适应性和超越人类的故事讲述系统。', 'title_zh': '超越人类的故事讲述：基于生成性人工智能的 longitudinal 故事 Engagement 设计'}
{'arxiv_id': 'arXiv:2505.23770', 'title': 'A comprehensive survey of cybercrimes in India over the last decade', 'authors': 'Sudhanshu Sekhar Tripathy', 'link': 'https://arxiv.org/abs/2505.23770', 'abstract': 'Since the 1990s, the integration of technology into daily life has led to the creation of an extensive network of interconnected devices, transforming how individuals and organizations operate. However, this digital transformation has also spurred the rise of cybercrime, criminal activities perpetrated through networks or computer systems. Cybercrime has become a global concern, presenting significant challenges to security systems. Although advancements in digital technology have enhanced efficiency, they have also opened new avenues for exploitation by cybercriminals, highlighting the urgent need for advanced cybersecurity measures. The escalating number of cyberattacks and associated risks in the past decade highlights the critical importance of protecting sensitive data and safeguarding information systems. Cybercrimes range from financial fraud and phishing scams to identity theft and online harassment, posing substantial risks to both individuals and organizations. In response, governments, law enforcement agencies, and cybersecurity units have intensified their efforts to address these threats. In recent years, India has experienced a significant surge in cybercrime incidents, with a notable increase in cases involving ransomware, data breaches, and social engineering attacks. The growing penetration of internet services, the expansion of e-commerce, and the rapid adoption of digital payment systems have made individuals and organizations more vulnerable to cyber threats. Key areas affected include banking, healthcare, and government sectors, which are frequently targeted due to the sensitive nature of the data they handle. To combat these risks, there is an increasing focus on public awareness, cybersecurity education, and robust regulatory frameworks. This paper examines cybercrime, prevention strategies, security protocols, and terminology to safeguard digital infrastructure.', 'abstract_zh': '自20世纪90年代以来，技术融入日常生活导致了互联设备的广泛网络形成，改变了个人和组织的运作方式。然而，这一数字化转型也促生了网络安全犯罪，即通过网络或计算机系统实施的犯罪行为。网络安全犯罪已成为全球关注的问题，对安全系统提出了重大挑战。尽管数字技术的进步提高了效率，但同时也为网络安全犯罪分子提供了新的exploitation途径，突显了加强网络安全措施的急迫性。近年来，网络攻击案件数量的激增及其相关风险凸显了保护敏感数据和保障信息系统安全的重要性和紧迫性。网络安全犯罪范围从金融欺诈和网络钓鱼到身份盗窃和网络骚扰，对个人和组织均构成了重大风险。对此，政府、执法机构和网络安全单位已加大努力应对这些威胁。近年来，印度的网络安全犯罪事件显著增多，特别是勒索软件、数据泄露和社交工程攻击案例呈上升趋势。互联网服务渗透程度的加深、电子商务的扩展以及数字支付系统的快速采用，使个人和组织更容易受到网络威胁。受影响的关键领域包括银行、医疗保健和政府行业，这些领域由于处理的数据具有敏感性，经常成为目标。为应对这些风险，公众意识提升、网络安全教育和健全的监管框架成为重点。本文探讨了网络安全犯罪、预防策略、安全协议和相关术语，旨在保护数字基础设施。', 'title_zh': '近十年来印度网络犯罪的综合调查'}
{'arxiv_id': 'arXiv:2505.18754', 'title': 'Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection', 'authors': 'Elsen Ronando, Sozo Inoue', 'link': 'https://arxiv.org/abs/2505.18754', 'abstract': 'In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid Euclidean Distance with Large Language Models) to improve example selection for sensor-based classification tasks. While few-shot prompting enables efficient inference with limited labeled data, its performance largely depends on the quality of selected examples. HED-LM addresses this challenge through a hybrid selection pipeline that filters candidate examples based on Euclidean distance and re-ranks them using contextual relevance scored by large language models (LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection task using accelerometer data characterized by overlapping patterns and high inter-subject variability. Unlike simpler tasks such as activity recognition, fatigue detection demands more nuanced example selection due to subtle differences in physiological signals. Our experiments show that HED-LM achieves a mean macro F1-score of 69.13$\\pm$10.71%, outperforming both random selection (59.30$\\pm$10.13%) and distance-only filtering (67.61$\\pm$11.39%). These represent relative improvements of 16.6% and 2.3%, respectively. The results confirm that combining numerical similarity with contextual relevance improves the robustness of few-shot prompting. Overall, HED-LM offers a practical solution to improve performance in real-world sensor-based learning tasks and shows potential for broader applications in healthcare monitoring, human activity recognition, and industrial safety scenarios.', 'abstract_zh': '基于HED-LM的少样本优化方法以提高基于传感器的分类任务示例选择', 'title_zh': '使用大语言模型进行传感器数据少样本优化：疲劳检测案例研究'}
{'arxiv_id': 'arXiv:2505.18334', 'title': 'Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play', 'authors': 'Jiaxun Cui, Chen Tang, Jarrett Holtz, Janice Nguyen, Alessandro G. Allievi, Hang Qiu, Peter Stone', 'link': 'https://arxiv.org/abs/2505.18334', 'abstract': 'Past work has demonstrated that autonomous vehicles can drive more safely if they communicate with one another than if they do not. However, their communication has often not been human-understandable. Using natural language as a vehicle-to-vehicle (V2V) communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with human drivers. In this work, we propose a suite of traffic tasks in autonomous driving where vehicles in a traffic scenario need to communicate in natural language to facilitate coordination in order to avoid an imminent collision and/or support efficient traffic flow. To this end, this paper introduces a novel method, LLM+Debrief, to learn a message generation and high-level decision-making policy for autonomous vehicles through multi-agent discussion. To evaluate LLM agents for driving, we developed a gym-like simulation environment that contains a range of driving scenarios. Our experimental results demonstrate that LLM+Debrief is more effective at generating meaningful and human-understandable natural language messages to facilitate cooperation and coordination than a zero-shot LLM agent. Our code and demo videos are available at this https URL.', 'abstract_zh': '过去的研究所证明，如果自动驾驶车辆彼此通信，它们的驾驶安全性会更高，而如果不通信则安全性较差。然而，它们之间的通信往往对人类来说难以理解。使用自然语言作为车辆到车辆（V2V）通信协议，可以使自动驾驶车辆不仅与其他自动驾驶车辆而且与人类驾驶员进行合作驾驶。在本文中，我们提出了一个自动驾驶中的交通任务套件，在这些任务中，交通场景中的车辆需要通过自然语言进行通信以实现协调，从而避免即将发生的碰撞并/或支持高效的交通流动。为此，本文介绍了一种新的方法——LLM+Debrief，通过多智能体讨论来学习自动驾驶车辆的消息生成和高层决策策略。为了评估LLM驱动代理的有效性，我们开发了一个类似于 gym 的模拟环境，该环境包含各种驾驶场景。我们的实验结果表明，LLM+Debrief 在生成有助于合作与协调的有意义且易于理解的自然语言消息方面比零样本LLM代理更为有效。我们的代码和演示视频可以在以下链接获取。', 'title_zh': '通过自博弈实现协作自主驾驶的自然语言通信'}
