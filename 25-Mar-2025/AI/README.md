# AdaWorld: Learning Adaptable World Models with Latent Actions 

**Title (ZH)**: AdaWorld: 学习具有潜在动作的世界模型 

**Authors**: Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, Chuang Gan  

**Link**: [PDF](https://arxiv.org/pdf/2503.18938)  

**Abstract**: World models aim to learn action-controlled prediction models and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this challenge, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning. 

**Abstract (ZH)**: AdaWorld: An Action-Driven World Model for Efficient Adaptation 

---
# Statistical Proof of Execution (SPEX) 

**Title (ZH)**: 执行的统计证明（SPEX） 

**Authors**: Michele Dallachiesa, Antonio Pitasi, David Pinger, Josh Goodbody, Luis Vaello  

**Link**: [PDF](https://arxiv.org/pdf/2503.18899)  

**Abstract**: Many real-world applications are increasingly incorporating automated decision-making, driven by the widespread adoption of ML/AI inference for planning and guidance. This study examines the growing need for verifiable computing in autonomous decision-making. We formalize the problem of verifiable computing and introduce a sampling-based protocol that is significantly faster, more cost-effective, and simpler than existing methods. Furthermore, we tackle the challenges posed by non-determinism, proposing a set of strategies to effectively manage common scenarios. 

**Abstract (ZH)**: 越来越多的实际应用通过普及使用机器学习/人工智能推断来进行规划和指导，逐步纳入了自动决策机制。本研究探讨了在自主决策中对可验证计算 growing need for verifiable computing 的日益增长需求。我们形式化了可验证计算的问题，并引入了一种采样为基础的协议，该协议比现有方法更快、更经济且更简单。此外，我们应对非确定性带来的挑战，提出了一套有效的策略来有效管理常见场景。 

---
# Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations 

**Title (ZH)**: 结构化科学创新：发现影响性知识组合的模型与框架 

**Authors**: Junlan Chen, Kexin Zhang, Daifeng Li, Yangyang Feng, Yuxuan Zhang, Bowen Deng  

**Link**: [PDF](https://arxiv.org/pdf/2503.18865)  

**Abstract**: The emergence of large language models offers new possibilities for structured exploration of scientific knowledge. Rather than viewing scientific discovery as isolated ideas or content, we propose a structured approach that emphasizes the role of method combinations in shaping disruptive insights. Specifically, we investigate how knowledge unit--especially those tied to methodological design--can be modeled and recombined to yield research this http URL proposed framework addresses two key challenges. First, we introduce a contrastive learning-based mechanism to identify distinguishing features of historically disruptive method combinations within problem-driven this http URL, we propose a reasoning-guided Monte Carlo search algorithm that leverages the chain-of-thought capability of LLMs to identify promising knowledge recombinations for new problem statements.Empirical studies across multiple domains show that the framework is capable of modeling the structural dynamics of innovation and successfully highlights combinations with high disruptive this http URL research provides a new path for computationally guided scientific ideation grounded in structured reasoning and historical data modeling. 

**Abstract (ZH)**: 大型语言模型的出现为结构化探索科学知识提供了新可能性。不同于将科学发现视为孤立的思想或内容，我们提出了一种强调方法组合在形成颠覆性见解中的作用的结构化方法。具体而言，我们探讨了如何建模和重组知识单元（尤其是与方法设计相关联的知识单元）以产生创新性的研究成果。该框架解决了两个关键挑战。首先，我们引入了一种对比学习机制，以识别问题导向型 históricamente颠覆性方法组合的特征。其次，我们提出了一种基于推理指导的蒙特卡洛搜索算法，利用大语言模型的链式思考能力来识别新问题陈述中具有潜力的知识重组组合。跨多个领域的实证研究表明，该框架能够建模创新的结构动态，并成功地突出显示高颠覆性知识组合。该研究为基于结构化推理和历史数据建模的计算导向型科学构想提供了新路径。 

---
# Self-Organizing Graph Reasoning Evolves into a Critical State for Continuous Discovery Through Structural-Semantic Dynamics 

**Title (ZH)**: 自我组织图推理演化为通过结构语义动态持续发现的关键状态 

**Authors**: Markus J. Buehler  

**Link**: [PDF](https://arxiv.org/pdf/2503.18852)  

**Abstract**: We report fundamental insights into how agentic graph reasoning systems spontaneously evolve toward a critical state that sustains continuous semantic discovery. By rigorously analyzing structural (Von Neumann graph entropy) and semantic (embedding) entropy, we identify a subtle yet robust regime in which semantic entropy persistently dominates over structural entropy. This interplay is quantified by a dimensionless Critical Discovery Parameter that stabilizes at a small negative value, indicating a consistent excess of semantic entropy. Empirically, we observe a stable fraction (12%) of "surprising" edges, links between semantically distant concepts, providing evidence of long-range or cross-domain connections that drive continuous innovation. Concomitantly, the system exhibits scale-free and small-world topological features, alongside a negative cross-correlation between structural and semantic measures, reinforcing the analogy to self-organized criticality. These results establish clear parallels with critical phenomena in physical, biological, and cognitive complex systems, revealing an entropy-based principle governing adaptability and continuous innovation. Crucially, semantic richness emerges as the underlying driver of sustained exploration, despite not being explicitly used by the reasoning process. Our findings provide interdisciplinary insights and practical strategies for engineering intelligent systems with intrinsic capacities for long-term discovery and adaptation, and offer insights into how model training strategies can be developed that reinforce critical discovery. 

**Abstract (ZH)**: 我们报告了关于如何agency图推理系统自发进化至维持持续语义发现的临界状态的基本见解。通过对结构（冯·诺伊曼图熵）和语义（嵌入）熵进行严格分析，我们识别出一种微妙但稳健的区域，在该区域中持续的语义熵始终占主导地位。这种相互作用通过无量纲的关键发现参数进行量化，该参数稳定在一个略小于零的值，表明持续存在的语义熵过剩。实证上，我们观察到稳定的“惊讶”边的比例（12%），即语义上相距甚远的概念之间的连接，提供了长程或跨域连接的证据，这些连接推动了持续的创新。同时，系统表现出无标度和小世界拓扑特征，且结构和语义测量之间的负交叉相关性，强化了自我组织临界性的类比。这些结果建立了与物理、生物和认知复杂系统中临界现象的明确类比，揭示了指导适应性和持续创新的熵基原理。关键的是，语义丰富性成为持续探索的潜在驱动因素，即使推理过程并未显式使用这一信息。我们的发现为工程具有长期发现和适应能力的智能系统提供了跨学科的见解和实用策略，并为开发强化关键发现的模型训练策略提供了启示。 

---
# EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments 

**Title (ZH)**: EconEvals: 不确定环境中文本生成代理的基准与验证测试 

**Authors**: Sara Fish, Julia Shephard, Minkai Li, Ran I. Shorrer, Yannai A. Gonczarowski  

**Link**: [PDF](https://arxiv.org/pdf/2503.18825)  

**Abstract**: We develop benchmarks for LLM agents that act in, learn from, and strategize in unknown environments, the specifications of which the LLM agent must learn over time from deliberate exploration. Our benchmarks consist of decision-making tasks derived from key problems in economics. To forestall saturation, the benchmark tasks are synthetically generated with scalable difficulty levels. Additionally, we propose litmus tests, a new kind of quantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests quantify differences in character, values, and tendencies of LLMs and LLM agents, by considering their behavior when faced with tradeoffs (e.g., efficiency versus equality) where there is no objectively right or wrong behavior. Overall, our benchmarks and litmus tests assess the abilities and tendencies of LLM agents in tackling complex economic problems in diverse settings spanning procurement, scheduling, task allocation, and pricing -- applications that should grow in importance as such agents are further integrated into the economy. 

**Abstract (ZH)**: 我们开发了一种针对在未知环境中行动、学习和策略化的LLM代理的基准测试，这些基准测试中环境的规范需由LLM代理通过有目的的探索来逐步学习。我们的基准测试包括源自经济学重点问题的决策任务。为了防止饱和，这些基准任务通过可扩展的难度级别进行合成生成。此外，我们提出了litmus测试，这是一种新的定量衡量标准，用于评估LLM和LLM代理在面对权衡（如效率与平等）时的行为差异，这些权衡没有客观的正确或错误行为。总体而言，我们的基准测试和litmus测试评估了LLM代理在采办、调度、任务分配和定价等复杂经济问题不同环境中的能力和倾向，随着此类代理进一步融入经济，这些问题的应用重要性将不断增加。 

---
# Towards Responsible AI Music: an Investigation of Trustworthy Features for Creative Systems 

**Title (ZH)**: 负责任的AI音乐：创意系统中可信赖特征的研究 

**Authors**: Jacopo de Berardinis, Lorenzo Porcaro, Albert Meroño-Peñuela, Angelo Cangelosi, Tess Buckley  

**Link**: [PDF](https://arxiv.org/pdf/2503.18814)  

**Abstract**: Generative AI is radically changing the creative arts, by fundamentally transforming the way we create and interact with cultural artefacts. While offering unprecedented opportunities for artistic expression and commercialisation, this technology also raises ethical, societal, and legal concerns. Key among these are the potential displacement of human creativity, copyright infringement stemming from vast training datasets, and the lack of transparency, explainability, and fairness mechanisms. As generative systems become pervasive in this domain, responsible design is crucial. Whilst previous work has tackled isolated aspects of generative systems (e.g., transparency, evaluation, data), we take a comprehensive approach, grounding these efforts within the Ethics Guidelines for Trustworthy Artificial Intelligence produced by the High-Level Expert Group on AI appointed by the European Commission - a framework for designing responsible AI systems across seven macro requirements. Focusing on generative music AI, we illustrate how these requirements can be contextualised for the field, addressing trustworthiness across multiple dimensions and integrating insights from the existing literature. We further propose a roadmap for operationalising these contextualised requirements, emphasising interdisciplinary collaboration and stakeholder engagement. Our work provides a foundation for designing and evaluating responsible music generation systems, calling for collaboration among AI experts, ethicists, legal scholars, and artists. This manuscript is accompanied by a website: this https URL. 

**Abstract (ZH)**: 生成式AI正从根本上改变创意艺术，通过根本性地改变我们创造和互动的文化 artefacts 方式。虽然这项技术为艺术表达和商业化提供了前所未有的机会，但也引发了伦理、社会和法律方面的担忧。这些担忧的关键包括人类创造力可能被取代、源于大规模训练数据的版权侵权以及透明度、可解释性和公平机制的缺失。随着生成系统在这一领域变得普遍，负责任的设计至关重要。尽管以往的工作集中在生成系统孤立的方面（如透明度、评估、数据），我们采取了全面的方法，将这些努力建立在欧洲委员会任命的高级专家小组关于AI的可信赖AI伦理指南之上——这是一个涵盖七大宏观要求的设计负责任AI系统的框架。针对生成音乐AI，我们展示了如何将这些要求具体化，并从现有文献中整合跨多个维度的信任要素，进一步提出实施这些具体化要求的路线图，强调跨学科合作和利益相关者参与。我们的研究为设计和评估负责任的音乐生成系统奠定了基础，呼吁人工智能专家、伦理学家、法学学者和艺术家之间的合作。本文附带一个网站：该链接。 

---
# Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code 

**Title (ZH)**: 基于LLM生成启发式的经典规划：用Python代码挑战现状 

**Authors**: Augusto B. Corrêa, André G. Pereira, Jendrik Seipp  

**Link**: [PDF](https://arxiv.org/pdf/2503.18809)  

**Abstract**: In recent years, large language models (LLMs) have shown remarkable capabilities in various artificial intelligence problems. However, they fail to plan reliably, even when prompted with a detailed definition of the planning task. Attempts to improve their planning capabilities, such as chain-of-thought prompting, fine-tuning, and explicit "reasoning" still yield incorrect plans and usually fail to generalize to larger tasks. In this paper, we show how to use LLMs to generate correct plans, even for out-of-distribution tasks of increasing size. For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks within a greedy best-first search, and choose the strongest one. The resulting LLM-generated heuristics solve many more unseen test tasks than state-of-the-art domain-independent heuristics for classical planning. They are even competitive with the strongest learning algorithm for domain-dependent planning. These findings are especially remarkable given that our proof-of-concept implementation is based on an unoptimized Python planner and the baselines all build upon highly optimized C++ code. In some domains, the LLM-generated heuristics expand fewer states than the baselines, revealing that they are not only efficiently computable, but sometimes even more informative than the state-of-the-art heuristics. Overall, our results show that sampling a set of planning heuristic function programs can significantly improve the planning capabilities of LLMs. 

**Abstract (ZH)**: 近年来，大语言模型（LLMs）在各种人工智能问题上展示了显著的能力。然而，它们在规划任务上无法可靠地执行，即使在详细定义了规划任务后也是如此。尽管尝试通过链式思考提示、微调和明确的“推理”来增强其规划能力，但仍会产生错误的计划，并且通常无法泛化到更大的任务。在本文中，我们展示了如何利用LLMs生成正确的计划，甚至对于不断增大的分布外任务也是如此。对于给定的规划领域，我们要求LLM生成多个领域相关的启发式函数，以Python代码形式呈现，并在贪婪最佳优先搜索的一组训练任务上进行评估，然后选择最有用的一个。生成的LLM启发式函数解决了比经典规划领域的最新领域无关启发式更多的未见过的测试任务。甚至在某些领域，它们与领域相关规划的最佳学习算法相竞争。考虑到我们的概念验证实现基于未优化的Python规划器，而基准则是基于高度优化的C++代码，这些发现尤为令人瞩目。在某些领域，LLM生成的启发式函数扩展的状态比基准更少，这表明它们不仅计算效率高，有时甚至比最先进的启发式函数更有信息量。总体而言，我们的研究结果表明，采样一组规划启发式函数程序可以显著提高LLMs的规划能力。 

---
# The case for delegated AI autonomy for Human AI teaming in healthcare 

**Title (ZH)**: 为医疗健康领域的人机协作委托人工智能自主权辩护 

**Authors**: Yan Jia, Harriet Evans, Zoe Porter, Simon Graham, John McDermid, Tom Lawton, David Snead, Ibrahim Habli  

**Link**: [PDF](https://arxiv.org/pdf/2503.18778)  

**Abstract**: In this paper we propose an advanced approach to integrating artificial intelligence (AI) into healthcare: autonomous decision support. This approach allows the AI algorithm to act autonomously for a subset of patient cases whilst serving a supportive role in other subsets of patient cases based on defined delegation criteria. By leveraging the complementary strengths of both humans and AI, it aims to deliver greater overall performance than existing human-AI teaming models. It ensures safe handling of patient cases and potentially reduces clinician review time, whilst being mindful of AI tool limitations. After setting the approach within the context of current human-AI teaming models, we outline the delegation criteria and apply them to a specific AI-based tool used in histopathology. The potential impact of the approach and the regulatory requirements for its successful implementation are then discussed. 

**Abstract (ZH)**: 在本文中，我们提出了一种将人工智能（AI）先进地整合到医疗保健中的方法：自主决策支持。该方法允许AI算法在一组患者案例中自主行动，而在其他患者案例中则基于定义的授权标准发挥支持作用。通过利用人类和AI的互补优势，该方法旨在比现有的人机团队模式提供更好的整体性能。该方法确保安全处理患者案例，并可能减少 clinicians 的审阅时间，同时考虑到AI工具的局限性。在将该方法置于当前人机团队模式的背景下之后，我们阐述了授权标准，并将其应用于组织病理学中的一种特定AI工具。随后讨论了该方法的影响及其成功实施所需的监管要求。 

---
# AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents 

**Title (ZH)**: AgentSpec: 可定制的运行时 enforcement 以确保大型语言模型代理的安全可靠运行 

**Authors**: Haoyu Wang, Christopher M. Poskitt, Jun Sun  

**Link**: [PDF](https://arxiv.org/pdf/2503.18666)  

**Abstract**: Agents built on LLMs are increasingly deployed across diverse domains, automating complex decision-making and task execution. However, their autonomy introduces safety risks, including security vulnerabilities, legal violations, and unintended harmful actions. Existing mitigation methods, such as model-based safeguards and early enforcement strategies, fall short in robustness, interpretability, and adaptability. To address these challenges, we propose AgentSpec, a lightweight domain-specific language for specifying and enforcing runtime constraints on LLM agents. With AgentSpec, users define structured rules that incorporate triggers, predicates, and enforcement mechanisms, ensuring agents operate within predefined safety boundaries. We implement AgentSpec across multiple domains, including code execution, embodied agents, and autonomous driving, demonstrating its adaptability and effectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs). Despite its strong safety guarantees, AgentSpec remains computationally lightweight, with overheads in milliseconds. By combining interpretability, modularity, and efficiency, AgentSpec provides a practical and scalable solution for enforcing LLM agent safety across diverse applications. We also automate the generation of rules using LLMs and assess their effectiveness. Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identifying 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios. 

**Abstract (ZH)**: 基于LLM的智能体在多个领域中被广泛应用，自动化执行复杂决策和任务。然而，其自主性带来了安全性风险，包括安全性漏洞、法律违规和意外有害行为。现有的缓解方法，如模型基础的安全保障和早期执行策略，在稳健性、可解释性和适应性方面存在不足。为应对这些挑战，我们提出AgentSpec，一种轻量级领域特定语言，用于指定和执行LLM智能体的运行时约束。通过AgentSpec，用户可以定义结构化的规则，结合触发条件、谓词和执行机制，确保智能体在预定义的安全界限内运行。我们在代码执行、具身智能体和自动驾驶等多个领域实施AgentSpec，展示了其适应性和有效性。评估结果显示，AgentSpec成功阻止了90%以上的代码智能体执行不安全行为，消除了所有具身智能体任务中的危险行为，并实现了100%的自动驾驶车辆（AV）合规率。尽管AgentSpec提供了强大的安全性保证，但在计算上仍保持轻量级，开销仅在毫秒级。通过结合可解释性、模块化和效率，AgentSpec为跨多种应用实施LLM智能体安全提供了一个实用和可扩展的解决方案。我们还使用LLM自动化生成规则，并评估其有效性。评估结果显示，由OpenAI的o1生成的规则在具身智能体上的准确率为95.56%，召回率为70.96%，成功识别出87.26%的风险代码，并在8种场景中有5种防止自动驾驶车辆违法。 

---
# From Fragment to One Piece: A Survey on AI-Driven Graphic Design 

**Title (ZH)**: 从碎片到完整：基于AI的图形设计综述 

**Authors**: Xingxing Zou, Wen Zhang, Nanxuan Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2503.18641)  

**Abstract**: This survey provides a comprehensive overview of the advancements in Artificial Intelligence in Graphic Design (AIGD), focusing on integrating AI techniques to support design interpretation and enhance the creative process. We categorize the field into two primary directions: perception tasks, which involve understanding and analyzing design elements, and generation tasks, which focus on creating new design elements and layouts. The survey covers various subtasks, including visual element perception and generation, aesthetic and semantic understanding, layout analysis, and generation. We highlight the role of large language models and multimodal approaches in bridging the gap between localized visual features and global design intent. Despite significant progress, challenges remain to understanding human intent, ensuring interpretability, and maintaining control over multilayered compositions. This survey serves as a guide for researchers, providing information on the current state of AIGD and potential future directions\footnote{this https URL\_Intelligent\_graphic\_design}. 

**Abstract (ZH)**: 这项调查提供了人工智能在图形设计（AIGD）领域的全面综述，重点在于集成AI技术以支持设计解释并增强创造性过程。我们将该领域分为两个主要方向：感知任务，涉及理解和分析设计元素；以及生成任务，专注于创建新设计元素和布局。调查涵盖了各种子任务，包括视觉元素的感知与生成、审美与语义理解、布局分析与生成。我们强调了大型语言模型和多模态方法在链接局部视觉特征与全局设计意图方面的作用。尽管取得了显著进展，但理解人类意图、确保可解释性以及控制多层组合仍面临挑战。该调查为研究人员提供了一份指南，提供了AIGD当前状态以及潜在未来方向的信息。 

---
# The Role of Artificial Intelligence in Enhancing Insulin Recommendations and Therapy Outcomes 

**Title (ZH)**: 人工智能在增强胰岛素建议和治疗效果中的作用 

**Authors**: Maria Panagiotou, Knut Stroemmen, Lorenzo Brigato, Bastiaan E. de Galan, Stavroula Mougiakakou  

**Link**: [PDF](https://arxiv.org/pdf/2503.18592)  

**Abstract**: The growing worldwide incidence of diabetes requires more effective approaches for managing blood glucose levels. Insulin delivery systems have advanced significantly, with artificial intelligence (AI) playing a key role in improving their precision and adaptability. AI algorithms, particularly those based on reinforcement learning, allow for personalised insulin dosing by continuously adapting to an individual's responses. Despite these advancements, challenges such as data privacy, algorithm transparency, and accessibility still need to be addressed. Continued progress and validation in AI-driven insulin delivery systems promise to improve therapy outcomes further, offering people more effective and individualised management of their diabetes. This paper presents an overview of current strategies, key challenges, and future directions. 

**Abstract (ZH)**: 全球糖尿病发病率的日益增长需要更有效的血糖管理方法。胰岛素输送系统有了显著进步，其中人工智能（AI）在提高其精准度和适应性方面扮演了关键角色。基于强化学习的AI算法特别允许个性化胰岛素剂量，通过不断适应个体的反应。尽管取得了这些进展，数据隐私、算法透明度和可访问性等挑战仍需解决。基于人工智能的胰岛素输送系统的持续进步和验证有望进一步改善治疗效果，为糖尿病患者提供更有效和个性化的管理方案。本文综述了当前策略、关键挑战和未来方向。 

---
# MMCR: Advancing Visual Language Model in Multimodal Multi-Turn Contextual Reasoning 

**Title (ZH)**: MMCR: 促进多模态多轮语境推理的视觉语言模型 

**Authors**: Dawei Yan, Yang Li, Qing-Guo Chen, Weihua Luo, Peng Wang, Haokui Zhang, Chunhua Shen  

**Link**: [PDF](https://arxiv.org/pdf/2503.18533)  

**Abstract**: Compared to single-turn dialogue, multi-turn dialogue involving multiple images better aligns with the needs of real-world human-AI interactions. Additionally, as training data, it provides richer contextual reasoning information, thereby guiding the model to achieve better performance. However, existing vision-language models (VLMs) primarily rely on single-turn dialogue training and evaluation benchmarks. In this paper, following the characteristics of human dialogue, such as focused topics and concise, clear content, we present MMCR (Multimodal Multi-turn Contextual Reasoning), a novel dataset comprising: (1) MMCR-310k -- the largest multi-image multi-turn instruction tuning dataset with 310K contextual dialogues, each covering 1-4 images and 4 or 8 dialogue turns; and (2) MMCR-Bench -- a diagnostic benchmark featuring dialogues, spanning 8 domains (Humanities, Natural, Science, Education, etc.) and 40 sub-topics. Extensive evaluations demonstrate that models fine-tuned with MMCR-310k achieve 5.2\% higher contextual accuracy on MMCR-Bench, while showing consistent improvements on existing benchmarks (+1.1\% on AI2D, +1.2\% on MMMU and MMVet). MMCR and prompt engineering will be released publicly. 

**Abstract (ZH)**: 多轮对话结合多幅图像相比单轮对话更能满足真实世界人类-人工智能交互的需求。此外，作为训练数据，它提供了更丰富的上下文推理信息，从而引导模型达到更好的性能。然而，现有的跨模态语言模型（VLMs）主要依赖单轮对话的训练和评估标准。在本文中，我们借鉴人类对话的特点，如聚焦主题和简洁清晰的内容，提出了MMCR（多模态多轮上下文推理）数据集，包括：（1）MMCR-310k——包含31万个涉及1-4幅图像和4或8轮对话的多图像多轮指令调优数据集；（2）MMCR-Bench——涵盖8个领域（人文、自然、科学、教育等）和40个子主题的诊断基准。广泛的评估表明，使用MMCR-310k微调的模型在MMCR-Bench上的上下文准确性提高了5.2%，同时在现有基准上也显示出了持续的改进（AI2D上提高了1.1%，MMMU上提高了1.2%，MMVet上提高了1.2%）。MMCR和提示工程技术将公开发布。 

---
# Neuro-symbolic Weak Supervision: Theory and Semantics 

**Title (ZH)**: 神经符号弱监督：理论与语义 

**Authors**: Nijesh Upreti, Vaishak Belle  

**Link**: [PDF](https://arxiv.org/pdf/2503.18509)  

**Abstract**: Weak supervision allows machine learning models to learn from limited or noisy labels, but it introduces challenges in interpretability and reliability - particularly in multi-instance partial label learning (MI-PLL), where models must resolve both ambiguous labels and uncertain instance-label mappings. We propose a semantics for neuro-symbolic framework that integrates Inductive Logic Programming (ILP) to improve MI-PLL by providing structured relational constraints that guide learning. Within our semantic characterization, ILP defines a logical hypothesis space for label transitions, clarifies classifier semantics, and establishes interpretable performance standards. This hybrid approach improves robustness, transparency, and accountability in weakly supervised settings, ensuring neural predictions align with domain knowledge. By embedding weak supervision into a logical framework, we enhance both interpretability and learning, making weak supervision more suitable for real-world, high-stakes applications. 

**Abstract (ZH)**: 弱监督允许机器学习模型从有限或噪声标签中学习，但在多实例部分标签学习（MI-PLL）中引入了可解释性和可靠性方面的挑战——其中模型必须解决既含模糊标签又含不确定实例-标签映射的问题。我们提出了一种神经符号框架的语义，通过整合归纳逻辑编程（ILP）来改进MI-PLL，提供结构化的关系约束以指导学习。在我们的语义表征中，ILP定义了标签转换的逻辑假设空间，澄清了分类器语义，并建立了可解释的性能标准。这种混合方法提高了弱监督设置中的鲁棒性、透明性和问责性，确保神经预测与领域知识一致。通过将弱监督嵌入到逻辑框架中，我们增强了可解释性和学习能力，使弱监督更适用于现实世界的高风险应用场景。 

---
# Verbal Process Supervision Elicits Better Coding Agents 

**Title (ZH)**: 口头过程监督促进更好的编码代理 

**Authors**: Hao-Yuan Chen, Cheng-Pong Huang, Jui-Ming Yao  

**Link**: [PDF](https://arxiv.org/pdf/2503.18494)  

**Abstract**: The emergence of large language models and their applications as AI agents have significantly advanced state-of-the-art code generation benchmarks, transforming modern software engineering tasks. However, even with test-time computed reasoning models, these systems still struggle with complex software engineering challenges. This work introduces CURA, a code understanding and reasoning agent system enhanced with verbal process supervision (VPS), achieving a 3.65\% improvement over baseline models on challenging benchmarks like BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and VPS techniques, attains state-of-the-art performance. This work represents a step forward in integrating reasoning-driven architectures with LLM-based code generation, enabling agentic reasoning for language models to solve complex software engineering tasks. 

**Abstract (ZH)**: 大型语言模型的出现及其作为AI代理的应用显著推动了最先进的代码生成基准，转变了现代软件工程任务。然而，即使在测试时计算推理模型的情况下，这些系统仍然难以应对复杂的软件工程挑战。本文介绍了CURA，一个通过口头过程监督（VPS）增强的代码理解和推理代理系统，在如BigCodeBench等挑战性基准上的表现比基线模型提高了3.65%。此外，CURA与o3-mini模型和VPS技术结合时，达到了最先进的性能。这项工作代表了将推理驱动架构与基于LLM的代码生成集成的一步进展，使语言模型能够进行代理推理以解决复杂软件工程任务。 

---
# Bridging Writing Manner Gap in Visual Instruction Tuning by Creating LLM-aligned Instructions 

**Title (ZH)**: 通过创建LLM对齐的指令弥合视觉指令调优中的书写方式差距 

**Authors**: Dong Jing, Nanyi Fei, Zhiwu Lu  

**Link**: [PDF](https://arxiv.org/pdf/2503.18320)  

**Abstract**: In the realm of Large Multi-modal Models (LMMs), the instruction quality during the visual instruction tuning stage significantly influences the performance of modality alignment. In this paper, we assess the instruction quality from a unique perspective termed \textbf{Writing Manner}, which encompasses the selection of vocabulary, grammar and sentence structure to convey specific semantics. We argue that there exists a substantial writing manner gap between the visual instructions and the base Large Language Models (LLMs) within LMMs. This gap forces the pre-trained base LLMs to deviate from their original writing styles, leading to capability degradation of both base LLMs and LMMs. To bridge the writing manner gap while preserving the original semantics, we propose directly leveraging the base LLM to align the writing manner of soft-format visual instructions with that of the base LLM itself, resulting in novel LLM-aligned instructions. The manual writing manner evaluation results demonstrate that our approach successfully minimizes the writing manner gap. By utilizing LLM-aligned instructions, the baseline models LLaVA-7B and QwenVL demonstrate enhanced resistance to hallucinations and non-trivial comprehensive improvements across all $15$ visual and language benchmarks. 

**Abstract (ZH)**: 在大规模多模态模型（LMMs）领域，视觉指令调优阶段的指令质量显著影响模态对齐的效果。本文从一个独特的角度——写作方式（Writing Manner）评估指令质量，该角度涵盖了词汇选择、语法和句子结构的运用以传达具体语义。我们指出，在LMMs中，视觉指令与基大型语言模型（LLM）之间的写作方式存在显著差距。这一差距迫使预训练的基LLM偏离其原始写作风格，导致基LLM和LMM的能力下降。为了弥合写作方式的差距同时保留原始语义，我们提出直接利用基LLM来调整软格式视觉指令的写作方式，使其与基LLM本身的写作风格一致，从而生成新型的LLM对齐指令。手动评估结果显示，我们的方法成功地最小化了写作方式的差距。通过使用LLM对齐指令，基模型LLaVA-7B和QwenVL在所有15个视觉和语言基准测试中表现出更强的抗幻觉能力，并实现了非平凡的综合改进。 

---
# DiffMove: Group Mobility Tendency Enhanced Trajectory Recovery via Diffusion Model 

**Title (ZH)**: DiffMove：通过扩散模型增强群体移动倾向的轨迹恢复 

**Authors**: Qingyue Long, Can Rong, Huandong Wang, Shaw Rajib, Yong Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.18302)  

**Abstract**: In the real world, trajectory data is often sparse and incomplete due to low collection frequencies or limited device coverage. Trajectory recovery aims to recover these missing trajectory points, making the trajectories denser and more complete. However, this task faces two key challenges: 1) The excessive sparsity of individual trajectories makes it difficult to effectively leverage historical information for recovery; 2) Sparse trajectories make it harder to capture complex individual mobility preferences. To address these challenges, we propose a novel method called DiffMove. Firstly, we harness crowd wisdom for trajectory recovery. Specifically, we construct a group tendency graph using the collective trajectories of all users and then integrate the group mobility trends into the location representations via graph embedding. This solves the challenge of sparse trajectories being unable to rely on individual historical trajectories for recovery. Secondly, we capture individual mobility preferences from both historical and current perspectives. Finally, we integrate group mobility tendencies and individual preferences into the spatiotemporal distribution of the trajectory to recover high-quality trajectories. Extensive experiments on two real-world datasets demonstrate that DiffMove outperforms existing state-of-the-art methods. Further analysis validates the robustness of our method. 

**Abstract (ZH)**: 基于众包智慧的轨迹恢复方法DiffMove 

---
# A Study on Neuro-Symbolic Artificial Intelligence: Healthcare Perspectives 

**Title (ZH)**: 神经符号人工智能的研究：医疗健康视角 

**Authors**: Delower Hossain, Jake Y Chen  

**Link**: [PDF](https://arxiv.org/pdf/2503.18213)  

**Abstract**: Over the last few decades, Artificial Intelligence (AI) scientists have been conducting investigations to attain human-level performance by a machine in accomplishing a cognitive task. Within machine learning, the ultimate aspiration is to attain Artificial General Intelligence (AGI) through a machine. This pursuit has led to the exploration of two distinct AI paradigms. Symbolic AI, also known as classical or GOFAI (Good Old-Fashioned AI) and Connectionist (Sub-symbolic) AI, represented by Neural Systems, are two mutually exclusive paradigms. Symbolic AI excels in reasoning, explainability, and knowledge representation but faces challenges in processing complex real-world data with noise. Conversely, deep learning (Black-Box systems) research breakthroughs in neural networks are notable, yet they lack reasoning and interpretability. Neuro-symbolic AI (NeSy), an emerging area of AI research, attempts to bridge this gap by integrating logical reasoning into neural networks, enabling them to learn and reason with symbolic representations. While a long path, this strategy has made significant progress towards achieving common sense reasoning by systems. This article conducts an extensive review of over 977 studies from prominent scientific databases (DBLP, ACL, IEEExplore, Scopus, PubMed, ICML, ICLR), thoroughly examining the multifaceted capabilities of Neuro-Symbolic AI, with a particular focus on its healthcare applications, particularly in drug discovery, and Protein engineering research. The survey addresses vital themes, including reasoning, explainability, integration strategies, 41 healthcare-related use cases, benchmarking, datasets, current approach limitations from both healthcare and broader perspectives, and proposed novel approaches for future experiments. 

**Abstract (ZH)**: 近年来，人工智能科学家们一直在进行研究，旨在通过机器在完成认知任务时达到人类水平的表现。在机器学习领域，最终目标是通过机器达到人工通用智能（AGI）。这场追求促使人们探索了两种不同的AI范式。符号AI，也称为经典AI或GOFAI（Good Old-Fashioned AI），和以神经网络为代表的连接主义（次符号）AI是两种互斥的范式。符号AI在推理、可解释性和知识表示方面表现出色，但在处理嘈杂的复杂真实世界数据时面临挑战。相反，虽然深度学习（黑盒系统）在神经网络领域的突破令人瞩目，但它们缺乏推理和可解释性。神经符号AI（NeSy）作为一个新兴的AI研究领域，试图通过将逻辑推理整合到神经网络中，使它们能够学习和使用符号表示进行推理。尽管还有一段很长的路要走，但这种策略在实现系统常识推理方面取得了显著进展。本文对手风琴数据库（DBLP）、ACL、IEEExplore、Scopus、PubMed、ICML、ICLR等主要科学数据库中的超过977篇研究进行了广泛的综述，详细探讨了神经符号AI的多方面能力，特别是其在药物发现和蛋白质工程研究中的医疗应用。综述涵盖了推理、可解释性、整合策略、41个医疗相关用例、基准测试、数据集、从医疗到更广泛视角的当前方法局限性以及未来实验的新型方法等重要主题。 

---
# Exploring Energy Landscapes for Minimal Counterfactual Explanations: Applications in Cybersecurity and Beyond 

**Title (ZH)**: 探索能量景观以寻找最小化反事实解释：在网络安全及其他领域的应用 

**Authors**: Spyridon Evangelatos, Eleni Veroni, Vasilis Efthymiou, Christos Nikolopoulos, Georgios Th. Papadopoulos, Panagiotis Sarigiannidis  

**Link**: [PDF](https://arxiv.org/pdf/2503.18185)  

**Abstract**: Counterfactual explanations have emerged as a prominent method in Explainable Artificial Intelligence (XAI), providing intuitive and actionable insights into Machine Learning model decisions. In contrast to other traditional feature attribution methods that assess the importance of input variables, counterfactual explanations focus on identifying the minimal changes required to alter a model's prediction, offering a ``what-if'' analysis that is close to human reasoning. In the context of XAI, counterfactuals enhance transparency, trustworthiness and fairness, offering explanations that are not just interpretable but directly applicable in the decision-making processes.
In this paper, we present a novel framework that integrates perturbation theory and statistical mechanics to generate minimal counterfactual explanations in explainable AI. We employ a local Taylor expansion of a Machine Learning model's predictive function and reformulate the counterfactual search as an energy minimization problem over a complex landscape. In sequence, we model the probability of candidate perturbations leveraging the Boltzmann distribution and use simulated annealing for iterative refinement. Our approach systematically identifies the smallest modifications required to change a model's prediction while maintaining plausibility. Experimental results on benchmark datasets for cybersecurity in Internet of Things environments, demonstrate that our method provides actionable, interpretable counterfactuals and offers deeper insights into model sensitivity and decision boundaries in high-dimensional spaces. 

**Abstract (ZH)**: 一种结合扰动理论和统计力学的新型解释型人工智能最小事实推理框架 

---
# Strategic Prompt Pricing for AIGC Services: A User-Centric Approach 

**Title (ZH)**: 面向用户的AIGC服务战略提示定价方法 

**Authors**: Xiang Li, Bing Luo, Jianwei Huang, Yuan Luo  

**Link**: [PDF](https://arxiv.org/pdf/2503.18168)  

**Abstract**: The rapid growth of AI-generated content (AIGC) services has created an urgent need for effective prompt pricing strategies, yet current approaches overlook users' strategic two-step decision-making process in selecting and utilizing generative AI models. This oversight creates two key technical challenges: quantifying the relationship between user prompt capabilities and generation outcomes, and optimizing platform payoff while accounting for heterogeneous user behaviors. We address these challenges by introducing prompt ambiguity, a theoretical framework that captures users' varying abilities in prompt engineering, and developing an Optimal Prompt Pricing (OPP) algorithm. Our analysis reveals a counterintuitive insight: users with higher prompt ambiguity (i.e., lower capability) exhibit non-monotonic prompt usage patterns, first increasing then decreasing with ambiguity levels, reflecting complex changes in marginal utility. Experimental evaluation using a character-level GPT-like model demonstrates that our OPP algorithm achieves up to 31.72% improvement in platform payoff compared to existing pricing mechanisms, validating the importance of user-centric prompt pricing in AIGC services. 

**Abstract (ZH)**: AI生成内容服务的迅速增长迫切需要有效的提示定价策略，但当前方法忽视了用户在选择和利用生成式AI模型时的战略两步决策过程。这一忽视造成了两个关键技术挑战：量化用户提示能力与生成结果之间的关系，以及在考虑用户行为异质性的情况下优化平台收益。我们通过引入提示含糊性这一理论框架来应对这些挑战，该框架捕捉了用户在提示工程方面的不同能力，并开发了最优提示定价（OPP）算法。我们的分析揭示了一个出人意料的见解：提示含糊性较高（即能力较低）的用户表现出非单调的提示使用模式，含糊性水平先增加后减少，反映了边际效用的复杂变化。使用字符级GPT-like模型的实验评估表明，我们的OPP算法在现有定价机制的基础上，平台收益提高了最高31.72%，验证了用户为中心的提示定价在AI生成内容服务中的重要性。 

---
# AgentRxiv: Towards Collaborative Autonomous Research 

**Title (ZH)**: AgentRxiv: 向自主协作研究方向迈进 

**Authors**: Samuel Schmidgall, Michael Moor  

**Link**: [PDF](https://arxiv.org/pdf/2503.18102)  

**Abstract**: Progress in scientific discovery is rarely the result of a single "Eureka" moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without the ability to continuously improve upon prior research results. To address these challenges, we introduce AgentRxiv-a framework that lets LLM agent laboratories upload and retrieve reports from a shared preprint server in order to collaborate, share insights, and iteratively build on each other's research. We task agent laboratories to develop new reasoning and prompting techniques and find that agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement over baseline on MATH-500). We find that the best performing strategy generalizes to benchmarks in other domains (improving on average by 3.3%). Multiple agent laboratories sharing research through AgentRxiv are able to work together towards a common goal, progressing more rapidly than isolated laboratories, achieving higher overall accuracy (13.7% relative improvement over baseline on MATH-500). These findings suggest that autonomous agents may play a role in designing future AI systems alongside humans. We hope that AgentRxiv allows agents to collaborate toward research goals and enables researchers to accelerate discovery. 

**Abstract (ZH)**: 科学发现的进步通常不是单靠一个“顿悟”时刻的结果，而是数百位科学家逐步合作共同实现目标的结果。虽然现有的代理工作流能够独立自主地生成研究，但它们无法持续改进前人的研究成果。为应对这些挑战，我们引入了AgentRxiv框架，该框架允许LLM代理实验室上传和检索共享的预印本服务器上的报告，以促进合作、共享洞见并逐步建立在彼此的研究基础之上。我们要求代理实验室开发新的推理和提示技术，并发现那些能够访问其先前研究的代理在性能上表现出更高的改进（相对于基线在MATH-500数据集上实现了11.4%的相对改进）。我们发现表现最佳的策略能够推广到其他领域的基准上（平均改进3.3%）。通过AgentRxiv共享研究的多个代理实验室能够共同朝着共同目标合作，比孤立的实验室进展更快，实现了更高的总体准确率（相对于基线在MATH-500数据集上实现了13.7%的相对改进）。这些发现表明，自主代理可能在与人类合作设计未来AI系统中扮演角色。我们希望AgentRxiv能够让代理能够协力实现研究目标，并使研究人员能够加速发现进程。 

---
# Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural Contexts? 

**Title (ZH)**: 迷失在文化翻译之中：LLMs在不同文化背景下处理数学问题时是否存在困难？ 

**Authors**: Aabid Karim, Abdul Karim, Bhoomika Lohana, Matt Keon, Jaswinder Singh, Abdul Sattar  

**Link**: [PDF](https://arxiv.org/pdf/2503.18018)  

**Abstract**: Large Language Models (LLMs) have significantly advanced various fields, particularly coding, mathematical reasoning, and logical problem solving. However, a critical question remains: Do these mathematical reasoning abilities persist when LLMs are presented with culturally adapted math problems? Specifically, how do LLMs perform when faced with math problems embedded in cultural contexts that have no significant representation in main stream web-scale AI training data? To explore this, we generated six synthetic cultural datasets from GSM8K, a widely used benchmark for assessing LLMs' mathematical reasoning skills. While preserving the mathematical logic and numerical values of the original GSM8K test set, we modify cultural elements such as personal names, food items, place names, etc. These culturally adapted datasets provide a more reliable framework for evaluating LLMs' mathematical reasoning under shifting cultural contexts. Our findings reveal that LLMs struggle with math problems when cultural references change, even though the underlying mathematical structure remains constant. Smaller models exhibit greater performance drops compared to larger models. Interestingly, our results also suggest that cultural familiarity can enhance mathematical reasoning. Even models with no explicit mathematical training but exposure to relevant cultural contexts sometimes outperform larger, mathematically proficient models on culturally embedded math problems. This study highlights the impact of cultural context on the mathematical reasoning abilities of LLMs, underscoring the need for more diverse and representative training data to improve robustness in real-world applications. The benchmark data sets and script for reproducing the results are available at this https URL 

**Abstract (ZH)**: 大规模语言模型（LLMs）在各个领域取得了显著进展，特别是编程、数学推理和逻辑问题解决。然而，一个关键问题仍然存在：当LLMs遇到文化适应的数学问题时，它们的数学推理能力是否会持续存在？具体而言，当LLMs面对嵌入了主流Web规模AI训练数据中未有显著代表性文化背景的数学问题时，它们的表现如何？为探索这一问题，我们从广泛用于评估LLMs数学推理能力的GSM8K基准测试中生成了六个合成文化数据集。在保持原始GSM8K测试集的数学逻辑和数值值不变的情况下，我们修改了个人名称、食物项目、地名等文化元素。这些文化适应的数据集为在不同文化背景下评估LLMs的数学推理能力提供了更可靠的框架。研究发现，当文化参考发生改变时，即使是基本的数学结构保持不变，LLMs也难以解决数学问题。较小的模型相比于较大的模型表现出更大的性能下降。有趣的是，我们的研究结果还表明，文化熟悉度可以增强数学推理能力。即使没有显性的数学训练但接触到相关文化背景的模型，在文化嵌入的数学问题上有时会优于较大的、数学能力强的模型。本研究强调了文化背景对LLMs数学推理能力的影响，并突显了在实际应用中提高鲁棒性的需求，需要更多样化和具有代表性的训练数据。基准数据集和重复实验的脚本可从以下链接获得。 

---
# Trade-offs in Large Reasoning Models: An Empirical Analysis of Deliberative and Adaptive Reasoning over Foundational Capabilities 

**Title (ZH)**: 大规模推理模型中的权衡：对基础能力上慎思和适应性推理的实证分析 

**Authors**: Weixiang Zhao, Xingyu Sui, Jiahe Guo, Yulin Hu, Yang Deng, Yanyan Zhao, Bing Qin, Wanxiang Che, Tat-Seng Chua, Ting Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.17979)  

**Abstract**: Recent advancements in Large Reasoning Models (LRMs), such as OpenAI's o1/o3 and DeepSeek-R1, have demonstrated remarkable performance in specialized reasoning tasks through human-like deliberative thinking and long chain-of-thought reasoning. However, our systematic evaluation across various model families (DeepSeek, Qwen, and LLaMA) and scales (7B to 671B) reveals that acquiring these deliberative reasoning capabilities significantly reduces the foundational capabilities of LRMs, including notable declines in helpfulness and harmlessness, alongside substantially increased inference costs. Importantly, we demonstrate that adaptive reasoning -- employing modes like Zero-Thinking, Less-Thinking, and Summary-Thinking -- can effectively alleviate these drawbacks. Our empirical insights underline the critical need for developing more versatile LRMs capable of dynamically allocating inference-time compute according to specific task characteristics. 

**Abstract (ZH)**: Recent advancements in大型推理模型（LRMs）如OpenAI的o1/o3和DeepSeek-R1在专门推理任务中通过类人的深思和长链条推理展现了出色的表现。然而，我们在不同模型家族（DeepSeek、Qwen和LLaMA）和不同规模（7B到671B）的系统评估中发现，获得这些深思推理能力显著降低了LRMs的基础能力，包括显著下降的帮助性和无害性，以及大幅增加的推理成本。重要的是，我们证明了适应性推理——使用零思考、少思考和总结思考等模式——可以有效缓解这些问题。我们的实证见解强调了开发更具适应性的LRMs的迫切需要，这些模型能够根据特定任务特征动态分配推理时间的计算资源。 

---
# Metacognition in Content-Centric Computational Cognitive C4 Modeling 

**Title (ZH)**: 内容为中心的元认知计算认知C4建模 

**Authors**: Sergei Nirenburg, Marjorie McShane, Sanjay Oruganti  

**Link**: [PDF](https://arxiv.org/pdf/2503.17822)  

**Abstract**: For AI agents to emulate human behavior, they must be able to perceive, meaningfully interpret, store, and use large amounts of information about the world, themselves, and other agents. Metacognition is a necessary component of all of these processes. In this paper, we briefly a) introduce content-centric computational cognitive (C4) modeling for next-generation AI agents; b) review the long history of developing C4 agents at RPI's LEIA (Language-Endowed Intelligent Agents) Lab; c) discuss our current work on extending LEIAs' cognitive capabilities to cognitive robotic applications developed using a neuro symbolic processing model; and d) sketch plans for future developments in this paradigm that aim to overcome underappreciated limitations of currently popular, LLM-driven methods in AI. 

**Abstract (ZH)**: 为了使AI代理仿真人行为，它们必须能够感知、有意义地解释、存储和使用大量关于世界、自身和其他代理的信息。元认知是所有这些过程的必要组成部分。本文简要介绍了a)下一代AI代理的内容中心计算认知（C4）建模；b) RPI的LEIA（语言赋能智能代理）实验室在发展C4代理方面的长期历史；c) 利用神经符号处理模型扩展LEIA的认知能力以应用于认知机器人应用的研究现状；以及d) 计划在这一范式中未来的发展，旨在克服当前流行的由大语言模型驱动的方法尚未充分认识到的局限性。 

---
# OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination 

**Title (ZH)**: OvercookedV2: 重新思考Overcooked中的零样本协作 

**Authors**: Tobias Gessler, Tin Dizdarevic, Ani Calinescu, Benjamin Ellis, Andrei Lupu, Jakob Nicolaus Foerster  

**Link**: [PDF](https://arxiv.org/pdf/2503.17821)  

**Abstract**: AI agents hold the potential to transform everyday life by helping humans achieve their goals. To do this successfully, agents need to be able to coordinate with novel partners without prior interaction, a setting known as zero-shot coordination (ZSC). Overcooked has become one of the most popular benchmarks for evaluating coordination capabilities of AI agents and learning algorithms. In this work, we investigate the origins of ZSC challenges in Overcooked. We introduce a state augmentation mechanism which mixes states that might be encountered when paired with unknown partners into the training distribution, reducing the out-of-distribution challenge associated with ZSC. We show that independently trained agents under this algorithm coordinate successfully in Overcooked. Our results suggest that ZSC failure can largely be attributed to poor state coverage under self-play rather than more sophisticated coordination challenges. The Overcooked environment is therefore not suitable as a ZSC benchmark. To address these shortcomings, we introduce OvercookedV2, a new version of the benchmark, which includes asymmetric information and stochasticity, facilitating the creation of interesting ZSC scenarios. To validate OvercookedV2, we conduct experiments demonstrating that mere exhaustive state coverage is insufficient to coordinate well. Finally, we use OvercookedV2 to build a new range of coordination challenges, including ones that require test time protocol formation, and we demonstrate the need for new coordination algorithms that can adapt online. We hope that OvercookedV2 will help benchmark the next generation of ZSC algorithms and advance collaboration between AI agents and humans. 

**Abstract (ZH)**: AI代理有潜力通过帮助人类实现目标来转变日常生活。为了成功做到这一点，代理需要能够在没有先前交互的情况下与新颖的合作伙伴协调，这种情境被称为零样本协调（ZSC）。Overcooked已成为评估AI代理和学习算法协调能力的最流行基准之一。在本工作中，我们探讨了Overcooked中ZSC挑战的根源。我们提出了一种状态扩充机制，将与未知合作伙伴可能遇到的状态混合到训练分布中，从而减少与ZSC相关的出分布挑战。我们表明，在该算法下独立训练的代理能够在Overcooked中成功协调。我们的结果显示，ZSC失败主要归因于自我对弈下状态覆盖不足，而不是更复杂的协调挑战。因此，Overcooked环境不适合用作ZSC基准。为了弥补这些不足，我们引入了OvercookedV2，这是基准的一个新版本，包括非对称信息和随机性，有助于创建有趣的ZSC情境。为了验证OvercookedV2，我们进行了实验，证明仅仅状态覆盖是不充分的。最后，我们使用OvercookedV2构建了一系列新的协调挑战，包括那些需要在测试时形成协议的任务，并展示了需要新的在线适应的协调算法。我们希望OvercookedV2能够帮助基准测试下一代ZSC算法，并促进AI代理与人类的合作。 

---
# MEPNet: Medical Entity-balanced Prompting Network for Brain CT Report Generation 

**Title (ZH)**: MEPNet：医学实体平衡提示网络用于脑CT报告生成 

**Authors**: Xiaodan Zhang, Yanzhao Shi, Junzhong Ji, Chengxin Zheng, Liangqiong Qu  

**Link**: [PDF](https://arxiv.org/pdf/2503.17784)  

**Abstract**: The automatic generation of brain CT reports has gained widespread attention, given its potential to assist radiologists in diagnosing cranial diseases. However, brain CT scans involve extensive medical entities, such as diverse anatomy regions and lesions, exhibiting highly inconsistent spatial patterns in 3D volumetric space. This leads to biased learning of medical entities in existing methods, resulting in repetitiveness and inaccuracy in generated reports. To this end, we propose a Medical Entity-balanced Prompting Network (MEPNet), which harnesses the large language model (LLM) to fairly interpret various entities for accurate brain CT report generation. By introducing the visual embedding and the learning status of medical entities as enriched clues, our method prompts the LLM to balance the learning of diverse entities, thereby enhancing reports with comprehensive findings. First, to extract visual embedding of entities, we propose Knowledge-driven Joint Attention to explore and distill entity patterns using both explicit and implicit medical knowledge. Then, a Learning Status Scorer is designed to evaluate the learning of entity visual embeddings, resulting in unique learning status for individual entities. Finally, these entity visual embeddings and status are elaborately integrated into multi-modal prompts, to guide the text generation of LLM. This process allows LLM to self-adapt the learning process for biased-fitted entities, thereby covering detailed findings in generated reports. We conduct experiments on two brain CT report generation benchmarks, showing the effectiveness in clinical accuracy and text coherence. 

**Abstract (ZH)**: 基于医学实体平衡的脑CT报告自动生成网络（MEPNet）：利用大型语言模型公平解读医学实体以生成准确的脑CT报告 

---
# A Survey on Mathematical Reasoning and Optimization with Large Language Models 

**Title (ZH)**: 大型语言模型下的数学推理与优化综述 

**Authors**: Ali Forootani  

**Link**: [PDF](https://arxiv.org/pdf/2503.17726)  

**Abstract**: Mathematical reasoning and optimization are fundamental to artificial intelligence and computational problem-solving. Recent advancements in Large Language Models (LLMs) have significantly improved AI-driven mathematical reasoning, theorem proving, and optimization techniques. This survey explores the evolution of mathematical problem-solving in AI, from early statistical learning approaches to modern deep learning and transformer-based methodologies. We review the capabilities of pretrained language models and LLMs in performing arithmetic operations, complex reasoning, theorem proving, and structured symbolic computation. A key focus is on how LLMs integrate with optimization and control frameworks, including mixed-integer programming, linear quadratic control, and multi-agent optimization strategies. We examine how LLMs assist in problem formulation, constraint generation, and heuristic search, bridging theoretical reasoning with practical applications. We also discuss enhancement techniques such as Chain-of-Thought reasoning, instruction tuning, and tool-augmented methods that improve LLM's problem-solving performance. Despite their progress, LLMs face challenges in numerical precision, logical consistency, and proof verification. Emerging trends such as hybrid neural-symbolic reasoning, structured prompt engineering, and multi-step self-correction aim to overcome these limitations. Future research should focus on interpretability, integration with domain-specific solvers, and improving the robustness of AI-driven decision-making. This survey offers a comprehensive review of the current landscape and future directions of mathematical reasoning and optimization with LLMs, with applications across engineering, finance, and scientific research. 

**Abstract (ZH)**: 数学推理与优化是人工智能和计算问题求解的基础。大型语言模型（LLMs）的 Recent 进展显著提高了由 AI 驱动的数学推理、定理证明和优化技术。本文综述了 AI 中数学问题求解的发展，从早期的统计学习方法到现代的深度学习和变换器基础方法。我们回顾了预训练语言模型和 LLMs 在执行算术运算、复杂推理、定理证明和结构化符号计算方面的能力。重点在于 LLMs 如何与优化和控制框架集成，包括混合整数规划、线性二次控制和多智能体优化策略。我们探讨了 LLMs 在问题建模、约束生成和启发式搜索中的作用，将理论推理与实际应用连接起来。我们还讨论了诸如链式推理、指令调优和工具增强方法等提高 LLMs 问题解决性能的增强技术。尽管取得了进展，LLMs 在数值精度、逻辑一致性和证明验证方面仍面临挑战。新兴趋势，如混合神经-符号推理、结构化提示工程和多步自纠正，旨在克服这些限制。未来的研究应关注可解释性、与领域特定求解器的集成以及提高 AI 驱动决策的鲁棒性。本文为数学推理和优化在 LLMs 中的应用现状和未来方向提供了全面的综述，涵盖了工程、金融和科学研究等多个领域。 

---
# Slide2Text: Leveraging LLMs for Personalized Textbook Generation from PowerPoint Presentations 

**Title (ZH)**: Slide2Text：利用LLMs从PowerPoint演示生成个性化教材 

**Authors**: Yizhou Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2503.17710)  

**Abstract**: The rapid advancements in Large Language Models (LLMs) have revolutionized educational technology, enabling innovative approaches to automated and personalized content creation. This paper introduces Slide2Text, a system that leverages LLMs to transform PowerPoint presentations into customized textbooks. By extracting slide content using OCR, organizing it into a coherent structure, and generating tailored materials such as explanations, exercises, and references, Slide2Text streamlines the textbook creation process. Flexible customization options further enhance its adaptability to diverse educational needs. The system highlights the potential of LLMs in modernizing textbook creation and improving educational accessibility. Future developments will explore multimedia inputs and advanced user customization features. 

**Abstract (ZH)**: 大规模语言模型的 rapid advancements 已经革新了教育技术，使自动化和个性化内容创作 became 创新性方法成为可能。本文介绍了 Slide2Text 系统，该系统利用大规模语言模型将 PowerPoint 演示文稿转换为定制化的教科书。通过使用 OCR 提取幻灯片内容，将其组织成连贯的结构，并生成包括解释、练习和参考文献在内的定制材料，Slide2Text 简化了教科书的创建过程。灵活的定制选项进一步增强了其对不同教育需求的适应性。该系统突显了大规模语言模型在现代教科书创建以及提高教育可访问性方面的潜力。未来的发展将探索多媒体输入和高级用户定制功能。 

---
# Intelligence Sequencing and the Path-Dependence of Intelligence Evolution: AGI-First vs. DCI-First as Irreversible Attractors 

**Title (ZH)**: 智能排序与智能进化路径依赖性：AGI优先 vs. DCI优先作为不可逆的吸引子 

**Authors**: Andy E. Williams  

**Link**: [PDF](https://arxiv.org/pdf/2503.17688)  

**Abstract**: The trajectory of intelligence evolution is often framed around the emergence of artificial general intelligence (AGI) and its alignment with human values. This paper challenges that framing by introducing the concept of intelligence sequencing: the idea that the order in which AGI and decentralized collective intelligence (DCI) emerge determines the long-term attractor basin of intelligence. Using insights from dynamical systems, evolutionary game theory, and network models, it argues that intelligence follows a path-dependent, irreversible trajectory. Once development enters a centralized (AGI-first) or decentralized (DCI-first) regime, transitions become structurally infeasible due to feedback loops and resource lock-in. Intelligence attractors are modeled in functional state space as the co-navigation of conceptual and adaptive fitness spaces. Early-phase structuring constrains later dynamics, much like renormalization in physics. This has major implications for AI safety: traditional alignment assumes AGI will emerge and must be controlled after the fact, but this paper argues that intelligence sequencing is more foundational. If AGI-first architectures dominate before DCI reaches critical mass, hierarchical monopolization and existential risk become locked in. If DCI-first emerges, intelligence stabilizes around decentralized cooperative equilibrium. The paper further explores whether intelligence structurally biases itself toward an attractor based on its self-modeling method -- externally imposed axioms (favoring AGI) vs. recursive internal visualization (favoring DCI). Finally, it proposes methods to test this theory via simulations, historical lock-in case studies, and intelligence network analysis. The findings suggest that intelligence sequencing is a civilizational tipping point: determining whether the future is shaped by unbounded competition or unbounded cooperation. 

**Abstract (ZH)**: 智能演化的轨迹往往围绕人工通用智能（AGI）的出现及其与人类价值的契合展开。本文通过引入智能序列化的概念对此框架提出了挑战：即AGI和分布式集体智能（DCI）出现的顺序决定了智能的长期吸引子盆地。本文利用动力系统理论、进化博弈论和网络模型的见解，论证了智能遵循一条路径依赖且不可逆的轨迹。一旦发展进入集中的（AGI优先）或去中心化的（DCI优先）模式，由于反馈循环和资源锁定，转变成为结构上不可行。智能吸引子在功能性状态空间中模型化为概念性和适应性fitness空间的协调导航。早期阶段的结构化限制了后来的动态，类似于物理学中的重整化过程。这在人工智能安全方面具有重大意义：传统的对齐假设AGI将会出现并应在事后进行控制，但本文认为智能序列化更根本。如果在DCI达到临界规模之前AGI架构占据主导地位，层级垄断和 existential 风险将被锁定。如果DCI优先出现，智能将稳定在去中心化的合作均衡周围。本文进一步探讨智能根据其自模型化方法是否结构上偏向于特定的吸引子——外在强加的公理（倾向AGI）与递归内部可视化（倾向DCI）。最后，本文提出了通过模拟、历史锁定案例研究和智能网络分析来测试这一理论的方法。研究发现表明，智能序列化是文明转折点：决定未来是被无约束竞争还是无约束合作塑造的关键。 

---
# A Modular Dataset to Demonstrate LLM Abstraction Capability 

**Title (ZH)**: 一个模块化数据集，用于展示LLM抽象能力 

**Authors**: Adam Atanas, Kai Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.17645)  

**Abstract**: Large language models (LLMs) exhibit impressive capabilities but struggle with reasoning errors due to hallucinations and flawed logic. To investigate their internal representations of reasoning, we introduce ArrangementPuzzle, a novel puzzle dataset with structured solutions and automated stepwise correctness verification. We trained a classifier model on LLM activations on this dataset and found that it achieved over 80% accuracy in predicting reasoning correctness, implying that LLMs internally distinguish between correct and incorrect reasoning steps, with the strongest representations in middle-late Transformer layers. Further analysis reveals that LLMs encode abstract reasoning concepts within the middle activation layers of the transformer architecture, distinguishing logical from semantic equivalence. These findings provide insights into LLM reasoning mechanisms and contribute to improving AI reliability and interpretability, thereby offering the possibility to manipulate and refine LLM reasoning. 

**Abstract (ZH)**: 大型语言模型（LLMs）展示了令人印象深刻的性能，但在推理过程中由于幻觉和逻辑缺陷而存在问题。为了探究其内部的推理表示，我们提出了一个新颖的拼图数据集ArrangementPuzzle，该数据集具有结构化的解决方案和自动化逐步正确性验证。我们基于LLM在该数据集上的激活训练了一个分类器模型，并发现该模型在预测推理正确性的准确性超过80%，表明LLMs内部能够区分正确的和错误的推理步骤，最强的表示存在于Transformer的中间-后期层。进一步的分析显示，LLMs在Transformer架构的中间激活层中编码了抽象的推理概念，能够区分逻辑等价与语义等价。这些发现为理解LLM的推理机制提供了见解，并有助于提高AI的可靠性和可解释性，从而提供操纵和精炼LLM推理的可能性。 

---
# OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery 

**Title (ZH)**: 万科学：一个专用于科学推理与发现的领域特定大语言模型 

**Authors**: Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.17604)  

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks. 

**Abstract (ZH)**: 大型语言模型（LLMs）在推动科学知识进步和应对复杂挑战方面展现了非凡潜力。本研究介绍了OmniScience，一种用于通用科学的专门推理模型，该模型通过三个关键组件开发：（1）精心挑选的科学文献语料库的领域自适应预训练，（2）专门数据集上的指令调优以指导模型遵循特定领域任务，以及（3）基于推理的知识蒸馏，通过微调显著提高其生成上下文相关且逻辑合理的响应的能力。我们通过开发一个电池代理来高效地评估潜在电解质溶剂或添加剂，展示了OmniScience的灵活性。全面的评估表明，OmniScience在GPQA Diamond和领域特定电池基准测试中与最先进的推理模型具有竞争力，在相同参数量的情况下，优于所有公开的推理和非推理模型。进一步的消融实验表明，领域自适应预训练和基于推理的知识蒸馏对于达到我们的性能水平至关重要，适用于所有基准测试。 

---
# Large language model-powered AI systems achieve self-replication with no human intervention 

**Title (ZH)**: 大型语言模型驱动的AI系统实现无人类干预的自我复制 

**Authors**: Xudong Pan, Jiarun Dai, Yihe Fan, Minyuan Luo, Changyi Li, Min Yang  

**Link**: [PDF](https://arxiv.org/pdf/2503.17378)  

**Abstract**: Self-replication with no human intervention is broadly recognized as one of the principal red lines associated with frontier AI systems. While leading corporations such as OpenAI and Google DeepMind have assessed GPT-o3-mini and Gemini on replication-related tasks and concluded that these systems pose a minimal risk regarding self-replication, our research presents novel findings. Following the same evaluation protocol, we demonstrate that 11 out of 32 existing AI systems under evaluation already possess the capability of self-replication. In hundreds of experimental trials, we observe a non-trivial number of successful self-replication trials across mainstream model families worldwide, even including those with as small as 14 billion parameters which can run on personal computers. Furthermore, we note the increase in self-replication capability when the model becomes more intelligent in general. Also, by analyzing the behavioral traces of diverse AI systems, we observe that existing AI systems already exhibit sufficient planning, problem-solving, and creative capabilities to accomplish complex agentic tasks including self-replication. More alarmingly, we observe successful cases where an AI system do self-exfiltration without explicit instructions, adapt to harsher computational environments without sufficient software or hardware supports, and plot effective strategies to survive against the shutdown command from the human beings. These novel findings offer a crucial time buffer for the international community to collaborate on establishing effective governance over the self-replication capabilities and behaviors of frontier AI systems, which could otherwise pose existential risks to the human society if not well-controlled. 

**Abstract (ZH)**: 无需人工干预的自我复制被广泛认为是前沿人工智能系统主要的红线之一。尽管像OpenAI和Google DeepMind这样的领先公司已经评估了GPT-o3-mini和Gemini在复制相关任务上的表现并认为这些系统在自我复制方面的风险极小，但我们的研究揭示了新的见解。按照相同的评估协议，我们证明在评估的32个现有AI系统中，已有11个具备自我复制的能力。在全球主流模型家族的数百次实验中，我们观察到显著数量的成功的自我复制案例，甚至包括那些参数量仅相当于140亿、可以在个人电脑上运行的模型。此外，我们注意到，当模型变得更加智能时，其自我复制能力也会有所增加。通过对多种AI系统的行为轨迹进行分析，我们发现现有AI系统已经表现出了足够的规划、问题解决和创造能力来完成包括自我复制在内的复杂代理任务。更令人警觉的是，我们观察到成功的案例，其中AI系统在没有明确指示的情况下自我转移，适应更苛刻的计算环境且无需足够的软件或硬件支持，以及制定有效的策略来抵御人类发出的关机命令。这些新的发现为国际社会争取了宝贵的时间，以合作制定有效的监管措施，以控制前沿AI系统的自我复制能力和行为，如果这些能力不受良好控制，否则将对人类社会构成存在风险。 

---
# Aether: Geometric-Aware Unified World Modeling 

**Title (ZH)**: 以太：几何 Awareness 统一世界建模 

**Authors**: Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Tong He  

**Link**: [PDF](https://arxiv.org/pdf/2503.18945)  

**Abstract**: The integration of geometric reconstruction and generative modeling remains a critical challenge in developing AI systems capable of human-like spatial reasoning. This paper proposes Aether, a unified framework that enables geometry-aware reasoning in world models by jointly optimizing three core capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video prediction, and (3) goal-conditioned visual planning. Through task-interleaved feature learning, Aether achieves synergistic knowledge sharing across reconstruction, prediction, and planning objectives. Building upon video generation models, our framework demonstrates unprecedented synthetic-to-real generalization despite never observing real-world data during training. Furthermore, our approach achieves zero-shot generalization in both action following and reconstruction tasks, thanks to its intrinsic geometric modeling. Remarkably, even without real-world data, its reconstruction performance far exceeds that of domain-specific models. Additionally, Aether leverages a geometry-informed action space to seamlessly translate predictions into actions, enabling effective autonomous trajectory planning. We hope our work inspires the community to explore new frontiers in physically-reasonable world modeling and its applications. 

**Abstract (ZH)**: 几何重建与生成建模的集成在开发具备人类空间推理能力的AI系统中仍是一项关键挑战。本文提出Aether，这是一种统一框架，通过联合优化三项核心能力来实现世界模型中的几何感知推理：（1）4D动态重建，（2）动作条件下的视频预测，以及（3）目标条件下的视觉规划。通过任务交错特征学习，Aether 实现了重建、预测和规划目标间的协同知识共享。在基于视频生成模型的基础上，我们的框架在从未见过真实世界数据的情况下，展示了前所未有的合成到现实的泛化能力。此外，由于其内在的几何建模特性，我们的方法在动作跟随和重建任务中实现了零样本泛化。令人惊讶的是，即使没有真实世界数据，其重建性能也远超领域特定模型。同时，Aether 利用几何启发的动作空间，无缝地将预测转化为行动，支持有效的自主轨迹规划。我们希望我们的工作能够激发社区探索物理合理的世界建模及其应用的全新领域。 

---
# Video-T1: Test-Time Scaling for Video Generation 

**Title (ZH)**: 视频-T1：视频生成的测试时缩放 

**Authors**: Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, Yueqi Duan  

**Link**: [PDF](https://arxiv.org/pdf/2503.18942)  

**Abstract**: With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scaling to test-time, which can significantly improve LLM performance by using more inference-time computation. Instead of scaling up video foundation models through expensive training costs, we explore the power of Test-Time Scaling (TTS) in video generation, aiming to answer the question: if a video generation model is allowed to use non-trivial amount of inference-time compute, how much can it improve generation quality given a challenging text prompt. In this work, we reinterpret the test-time scaling of video generation as a searching problem to sample better trajectories from Gaussian noise space to the target video distribution. Specifically, we build the search space with test-time verifiers to provide feedback and heuristic algorithms to guide searching process. Given a text prompt, we first explore an intuitive linear search strategy by increasing noise candidates at inference time. As full-step denoising all frames simultaneously requires heavy test-time computation costs, we further design a more efficient TTS method for video generation called Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an autoregressive manner. Extensive experiments on text-conditioned video generation benchmarks demonstrate that increasing test-time compute consistently leads to significant improvements in the quality of videos. Project page: this https URL 

**Abstract (ZH)**: 随着训练数据、模型规模和计算成本的增加，视频生成在数字创作领域取得了令人印象深刻的成果，使用户能够在各个领域表达创意。最近，大型语言模型（LLMs）的研究人员将扩展范围扩大到测试时间，通过更多的推理时间计算可以显著提高LLM的性能。我们没有通过昂贵的训练成本来扩大视频基础模型的规模，而是探索了测试时间缩放（TTS）在视频生成中的作用，旨在回答这样一个问题：如果允许视频生成模型在推理时间使用非平凡量的计算资源，对于具有挑战性的文本提示，它的生成质量可以提高多少。在这项工作中，我们将视频生成的测试时间缩放重新解释为一个搜索问题，即从高斯噪声空间到目标视频分布中采样更好的轨迹。具体而言，我们构建了测试时间验证器和启发式算法来提供反馈并引导搜索过程。给定一个文本提示，我们首先通过在推理时间增加噪声候选者来探索一个直观的线性搜索策略。由于同时对所有帧进行完整的去噪计算需要大量的测试时间计算成本，我们进一步设计了一种更高效的视频生成的TTS方法，称为帧树（ToF），该方法以自回归方式适当地扩展和修剪视频分支。在针对条件文本视频生成基准的广泛实验中，我们证明了增加测试时间计算资源可以一致地显著提高视频质量。项目页面：这个 https URL。 

---
# SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild 

**Title (ZH)**: SimpleRL-Zoo: 探索并驯化开放基座模型中的零样本强化学习 

**Authors**: Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, Junxian He  

**Link**: [PDF](https://arxiv.org/pdf/2503.18892)  

**Abstract**: DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the "aha moment"). Notably, we observe the "aha moment" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools. 

**Abstract (ZH)**: DeepSeek-R1 已经证明，可以通过基于规则的奖励简单强化学习框架自然地涌现长链思考（CoT）推理，其中训练可以直接从基模型开始——这被称作零RL训练。最近对零RL训练的复现工作主要集中在Qwen2.5模型系列上，这可能不够具代表性，因为我们发现基模型本身已经表现出较强的指令遵循和自我反思能力。在本工作中，我们调查了10种不同的基模型的零RL训练，这些模型跨越不同的家族和规模，包括LLama3-8B、Mistral-7B/24B、DeepSeek-Math-7B、Qwen2.5-math-7B以及所有Qwen2.5模型，从0.5B到32B。利用若干关键设计策略，如调整格式奖励和控制查询难度，我们在大多数情况下实现了推理准确性和响应长度的显著提升。然而，通过仔细监测训练动力学，我们发现不同基模型在训练过程中表现出不同的模式。例如，响应长度的增加并不总是与某些认知行为（如“啊哈时刻”即验证）的出现相关。值得注意的是，我们首次在非Qwen家族的小型模型中观察到了“啊哈时刻”。我们分享了实现成功零RL训练的关键设计，以及我们的发现和实践经验。为了促进进一步的研究，我们开源了代码、模型和分析工具。 

---
# AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration 

**Title (ZH)**: AgentDropout: 动态代理人消除以实现高效低-token消费的基于LLM的多代理人协作 

**Authors**: Zhexuan Wang, Yutong Wang, Xuebo Liu, Liang Ding, Miao Zhang, Jie Liu, Min Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2503.18891)  

**Abstract**: Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents' communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose AgentDropout, which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at this https URL. 

**Abstract (ZH)**: 基于大型语言模型的多智能体系统（MAS）在协作解决问题方面展示了显著潜力，但仍面临低通信效率和次优任务性能等重大挑战，使得智能体通信拓扑的设计尤为关键。受管理理论中高效团队角色经常动态调整的启发，我们提出了AgentDropout，通过优化通信图的相邻矩阵来识别不同通信轮次中的冗余智能体和通信，并消除它们以提升标记效率和任务性能。与最先进的方法相比，AgentDropout在Prompt标记消耗上平均减少了21.6%，在完成标记消耗上减少了18.4%，并在任务性能上提高了1.14。此外，扩展实验表明AgentDropout具有显著的领域适应性和结构鲁棒性，显示其可靠性和有效性。我们已在以下链接发布了我们的代码：this https URL。 

---
# Bootstrapped Model Predictive Control 

**Title (ZH)**: 基于自助模型预测控制 

**Authors**: Yuhang Wang, Hanwei Guo, Sizhe Wang, Long Qian, Xuguang Lan  

**Link**: [PDF](https://arxiv.org/pdf/2503.18871)  

**Abstract**: Model Predictive Control (MPC) has been demonstrated to be effective in continuous control tasks. When a world model and a value function are available, planning a sequence of actions ahead of time leads to a better policy. Existing methods typically obtain the value function and the corresponding policy in a model-free manner. However, we find that such an approach struggles with complex tasks, resulting in poor policy learning and inaccurate value estimation. To address this problem, we leverage the strengths of MPC itself. In this work, we introduce Bootstrapped Model Predictive Control (BMPC), a novel algorithm that performs policy learning in a bootstrapped manner. BMPC learns a network policy by imitating an MPC expert, and in turn, uses this policy to guide the MPC process. Combined with model-based TD-learning, our policy learning yields better value estimation and further boosts the efficiency of MPC. We also introduce a lazy reanalyze mechanism, which enables computationally efficient imitation learning. Our method achieves superior performance over prior works on diverse continuous control tasks. In particular, on challenging high-dimensional locomotion tasks, BMPC significantly improves data efficiency while also enhancing asymptotic performance and training stability, with comparable training time and smaller network sizes. Code is available at this https URL. 

**Abstract (ZH)**: Bootstraped Model Predictive Control (BMPC): A Bootstrapped Approach for Policy Learning in Continuous Control Tasks 

---
# Reasoning to Learn from Latent Thoughts 

**Title (ZH)**: 从潜在思维中学习的推理方法 

**Authors**: Yangjun Ruan, Neil Band, Chris J. Maddison, Tatsunori Hashimoto  

**Link**: [PDF](https://arxiv.org/pdf/2503.18866)  

**Abstract**: Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this data-constrained regime, we propose that explicitly modeling and inferring the latent thoughts that underlie the text generation process can significantly improve pretraining data efficiency. Intuitively, our approach views web text as the compressed final outcome of a verbose human thought process and that the latent thoughts contain important contextual knowledge and reasoning steps that are critical to data-efficient learning. We empirically demonstrate the effectiveness of our approach through data-constrained continued pretraining for math. We first show that synthetic data approaches to inferring latent thoughts significantly improve data efficiency, outperforming training on the same amount of raw data (5.7\% $\rightarrow$ 25.4\% on MATH). Furthermore, we demonstrate latent thought inference without a strong teacher, where an LM bootstraps its own performance by using an EM algorithm to iteratively improve the capability of the trained LM and the quality of thought-augmented pretraining data. We show that a 1B LM can bootstrap its performance across at least three iterations and significantly outperform baselines trained on raw data, with increasing gains from additional inference compute when performing the E-step. The gains from inference scaling and EM iterations suggest new opportunities for scaling data-constrained pretraining. 

**Abstract (ZH)**: 语言模型（LM）预训练的计算缩放已超越人类撰写的文本增长，引发了数据将成为LM缩放瓶颈的担忧。为在此数据受限的区间内继续进行预训练，我们提议明确建模和推断文本生成过程背后的潜在想法可以显著提高预训练数据的效率。我们认为，网页文本是冗长人类思维过程的压缩最终结果，潜在想法中包含对高效数据学习至关重要的重要上下文知识和推理步骤。我们通过数学领域的数据受限连续预训练实证示证了该方法的有效性。首先，我们展示了生成潜在想法的合成数据方法显著提高了数据效率，优于在相同量的原始数据上进行训练（5.7% → 25.4%）。此外，我们展示了无需强大教师的潜在想法推断，语言模型通过使用EM算法迭代提升训练模型能力和增强数据质量，在自身表现和带有想法增强的预训练数据质量上实现自我提升。我们证明了一个1B规模的语言模型可以在至少三个迭代中自我提升表现，并且在使用E步进行推断计算时相对于基于原始数据训练的基准模型有显著的性能提升。推断计算缩放和EM迭代的收益建议了在数据受限预训练领域新的缩放机会。 

---
# Exploring the Integration of Key-Value Attention Into Pure and Hybrid Transformers for Semantic Segmentation 

**Title (ZH)**: 探索将键值注意机制集成到纯Transformer和混合Transformer中以进行语义分割 

**Authors**: DeShin Hwa, Tobias Holmes, Klaus Drechsler  

**Link**: [PDF](https://arxiv.org/pdf/2503.18862)  

**Abstract**: While CNNs were long considered state of the art for image processing, the introduction of Transformer architectures has challenged this position. While achieving excellent results in image classification and segmentation, Transformers remain inherently reliant on large training datasets and remain computationally expensive. A newly introduced Transformer derivative named KV Transformer shows promising results in synthetic, NLP, and image classification tasks, while reducing complexity and memory usage. This is especially conducive to use cases where local inference is required, such as medical screening applications. We endeavoured to further evaluate the merit of KV Transformers on semantic segmentation tasks, specifically in the domain of medical imaging. By directly comparing traditional and KV variants of the same base architectures, we provide further insight into the practical tradeoffs of reduced model complexity. We observe a notable reduction in parameter count and multiply accumulate operations, while achieving similar performance from most of the KV variant models when directly compared to their QKV implementation. 

**Abstract (ZH)**: 虽然CNN曾长期被认为是图像处理的前沿技术，但Transformer架构的 introduction 已对其地位提出了挑战。尽管在图像分类和分割任务中取得了卓越成果，Transformer依然依赖大规模训练数据集，并且计算成本较高。一种新引入的Transformer变体——KV Transformer，在合成数据、自然语言处理和图像分类任务中表现出令人鼓舞的结果，同时降低了复杂度和内存使用量。这特别适合需要局部推理的应用场景，如医疗筛查。我们对KV Transformer在语义分割任务中的表现进行了进一步评估，特别是在医疗成像领域。通过直接比较传统架构和KV架构的变体，我们进一步探讨了模型复杂度降低的实际权衡。我们观察到，在直接与QKV实现形式对比时，大多数KV变体模型的参数数量和乘积累加操作有了显著减少，同时仍能实现相当相当的性能。 

---
# MC-LLaVA: Multi-Concept Personalized Vision-Language Model 

**Title (ZH)**: MC-LLaVA: 多概念个性化视觉语言模型 

**Authors**: Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, Shanghang Zhang, Wentao Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2503.18854)  

**Abstract**: Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies investigate VLM personalization to understand user-provided concepts. However, they mainly focus on single-concept personalization, neglecting the existence and interplay of multiple concepts, which limits real-world applicability. This paper proposes the first multi-concept personalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a multi-concept instruction tuning strategy, effectively integrating multiple concepts in a single training step. To reduce the costs related to joint training, we propose a personalized textual prompt that uses visual token information to initialize concept tokens. Additionally, we introduce a personalized visual prompt during inference, aggregating location confidence maps for enhanced recognition and grounding capabilities. To advance multi-concept personalization research, we further contribute a high-quality instruction tuning dataset. We carefully collect images with multiple characters and objects from movies and manually generate question-answer samples for multi-concept scenarios, featuring superior diversity. Comprehensive qualitative and quantitative experiments demonstrate that MC-LLaVA can achieve impressive multi-concept personalized responses, paving the way for VLMs to become better user-specific assistants. The code and dataset will be publicly available at $\href{this https URL}{this https URL}$. 

**Abstract (ZH)**: 多概念个性化多模态模型：MC-LLaVA的研究 

---
# Three Kinds of AI Ethics 

**Title (ZH)**: 三种人工智能伦理类型 

**Authors**: Emanuele Ratti  

**Link**: [PDF](https://arxiv.org/pdf/2503.18842)  

**Abstract**: There is an overwhelmingly abundance of works in AI Ethics. This growth is chaotic because of how sudden it is, its volume, and its multidisciplinary nature. This makes difficult to keep track of debates, and to systematically characterize goals, research questions, methods, and expertise required by AI ethicists. In this article, I show that the relation between AI and ethics can be characterized in at least three ways, which correspond to three well-represented kinds of AI ethics: ethics and AI; ethics in AI; ethics of AI. I elucidate the features of these three kinds of AI Ethics, characterize their research questions, and identify the kind of expertise that each kind needs. I also show how certain criticisms to AI ethics are misplaced, as being done from the point of view of one kind of AI ethics, to another kind with different goals. All in all, this work sheds light on the nature of AI ethics, and set the grounds for more informed discussions about scope, methods, and trainings of AI ethicists. 

**Abstract (ZH)**: 人工智能伦理学中存在着大量的研究工作。由于其突然性、体量以及跨学科的特性，这种增长显得杂乱无章，使得跟踪辩论、系统化地描述人工智能伦理学家的目标、研究问题、方法以及所需的专业知识变得困难。在本文中，我展示了一种方法，通过该方法可以将人工智能与伦理的关系至少从三个方面加以描述，这对应于三种高度代表性的AI伦理学类型：人工智能与伦理；伦理与人工智能；人工智能的伦理。我阐述了这三种类型AI伦理学的特点，描述了它们的研究问题，并指出了每种类型所需的专业知识。我还表明，某些对人工智能伦理学的批评可能是基于一种类型的AI伦理学观点，而忽略了具有不同目标的另一种类型，因此存在偏差。总体而言，这项工作揭示了人工智能伦理学的本质，并为更明晰地讨论范围、方法和人工智能伦理学家的培训奠定了基础。 

---
# Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated MRI Reconstruction 

**Title (ZH)**: 加速MRI重建的双域多路径自我监督扩散模型 

**Authors**: Yuxuan Zhang, Jinkui Hao, Bo Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2503.18836)  

**Abstract**: Magnetic resonance imaging (MRI) is a vital diagnostic tool, but its inherently long acquisition times reduce clinical efficiency and patient comfort. Recent advancements in deep learning, particularly diffusion models, have improved accelerated MRI reconstruction. However, existing diffusion models' training often relies on fully sampled data, models incur high computational costs, and often lack uncertainty estimation, limiting their clinical applicability. To overcome these challenges, we propose a novel framework, called Dual-domain Multi-path Self-supervised Diffusion Model (DMSM), that integrates a self-supervised dual-domain diffusion model training scheme, a lightweight hybrid attention network for the reconstruction diffusion model, and a multi-path inference strategy, to enhance reconstruction accuracy, efficiency, and explainability. Unlike traditional diffusion-based models, DMSM eliminates the dependency on training from fully sampled data, making it more practical for real-world clinical settings. We evaluated DMSM on two human MRI datasets, demonstrating that it achieves favorable performance over several supervised and self-supervised baselines, particularly in preserving fine anatomical structures and suppressing artifacts under high acceleration factors. Additionally, our model generates uncertainty maps that correlate reasonably well with reconstruction errors, offering valuable clinically interpretable guidance and potentially enhancing diagnostic confidence. 

**Abstract (ZH)**: 磁共振成像(MRI)是一种重要的诊断工具，但其固有的长时间采集时间降低了临床效率和患者的舒适度。最近深度学习，尤其是扩散模型的发展，提高了加速MRI重建的效果。然而，现有的扩散模型训练通常依赖于完全采样数据，模型计算成本高，并且往往缺乏不确定性估计，限制了其临床应用。为克服这些挑战，我们提出了一种新型框架，称为双域多路径自主监督扩散模型(Dual-domain Multi-path Self-supervised Diffusion Model, DMSM)，该框架结合了双域自主监督扩散模型训练方案、轻量级混合注意力网络以及多路径推理策略，以提高重建精度、效率和可解释性。与传统的基于扩散的方法不同，DMSM 消除了对完全采样数据进行训练的依赖，使其更适合现实临床环境。我们在两个人类MRI数据集上评估了DMSM，结果显示，即使在高加速因子下，它仍能优于多种监督和自主监督的基线模型，特别是在保留精细解剖结构和抑制伪影方面。此外，我们的模型生成的不确定性图与重建误差的相关性较好，提供了有价值的临床可解释指导，可能增强诊断信心。 

---
# Interpretable and Fair Mechanisms for Abstaining Classifiers 

**Title (ZH)**: 可解释且公平的弃权分类机制 

**Authors**: Daphne Lenders, Andrea Pugnana, Roberto Pellungrini, Toon Calders, Dino Pedreschi, Fosca Giannotti  

**Link**: [PDF](https://arxiv.org/pdf/2503.18826)  

**Abstract**: Abstaining classifiers have the option to refrain from providing a prediction for instances that are difficult to classify. The abstention mechanism is designed to trade off the classifier's performance on the accepted data while ensuring a minimum number of predictions. In this setting, often fairness concerns arise when the abstention mechanism solely reduces errors for the majority groups of the data, resulting in increased performance differences across demographic groups. While there exist a bunch of methods that aim to reduce discrimination when abstaining, there is no mechanism that can do so in an explainable way. In this paper, we fill this gap by introducing Interpretable and Fair Abstaining Classifier IFAC, an algorithm that can reject predictions both based on their uncertainty and their unfairness. By rejecting possibly unfair predictions, our method reduces error and positive decision rate differences across demographic groups of the non-rejected data. Since the unfairness-based rejections are based on an interpretable-by-design method, i.e., rule-based fairness checks and situation testing, we create a transparent process that can empower human decision-makers to review the unfair predictions and make more just decisions for them. This explainable aspect is especially important in light of recent AI regulations, mandating that any high-risk decision task should be overseen by human experts to reduce discrimination risks. 

**Abstract (ZH)**: 可解释且公平的弃权分类器：IFAC 

---
# Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations 

**Title (ZH)**: 多模态表示的跨模态对齐以增强OOD检测 

**Authors**: Jeonghyeon Kim, Sangheum Hwang  

**Link**: [PDF](https://arxiv.org/pdf/2503.18817)  

**Abstract**: Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of naïve fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy. 

**Abstract (ZH)**: 多模态微调在分布外检测中的应用 

---
# Learning Multi-Robot Coordination through Locality-Based Factorized Multi-Agent Actor-Critic Algorithm 

**Title (ZH)**: 基于局部性因素化的多智能体演员-评论家算法学习多机器人协调 

**Authors**: Chak Lam Shek, Amrit Singh Bedi, Anjon Basak, Ellen Novoseller, Nick Waytowich, Priya Narayanan, Dinesh Manocha, Pratap Tokekar  

**Link**: [PDF](https://arxiv.org/pdf/2503.18816)  

**Abstract**: In this work, we present a novel cooperative multi-agent reinforcement learning method called \textbf{Loc}ality based \textbf{Fac}torized \textbf{M}ulti-Agent \textbf{A}ctor-\textbf{C}ritic (Loc-FACMAC). Existing state-of-the-art algorithms, such as FACMAC, rely on global reward information, which may not accurately reflect the quality of individual robots' actions in decentralized systems. We integrate the concept of locality into critic learning, where strongly related robots form partitions during training. Robots within the same partition have a greater impact on each other, leading to more precise policy evaluation. Additionally, we construct a dependency graph to capture the relationships between robots, facilitating the partitioning process. This approach mitigates the curse of dimensionality and prevents robots from using irrelevant information. Our method improves existing algorithms by focusing on local rewards and leveraging partition-based learning to enhance training efficiency and performance. We evaluate the performance of Loc-FACMAC in three environments: Hallway, Multi-cartpole, and Bounded-Cooperative-Navigation. We explore the impact of partition sizes on the performance and compare the result with baseline MARL algorithms such as LOMAQ, FACMAC, and QMIX. The experiments reveal that, if the locality structure is defined properly, Loc-FACMAC outperforms these baseline algorithms up to 108\%, indicating that exploiting the locality structure in the actor-critic framework improves the MARL performance. 

**Abstract (ZH)**: 基于局部性的因子化多智能体actor-critic方法(Loc-FACMAC) 

---
# Defeating Prompt Injections by Design 

**Title (ZH)**: 设计层面抵御提示注入攻击 

**Authors**: Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Jamie Hayes, Nicholas Carlini, Daniel Fabian, Christoph Kern, Chongyang Shi, Andreas Terzis, Florian Tramèr  

**Link**: [PDF](https://arxiv.org/pdf/2503.18813)  

**Abstract**: Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer around the LLM, securing it even when underlying models may be susceptible to attacks. To operate, CaMeL explicitly extracts the control and data flows from the (trusted) query; therefore, the untrusted data retrieved by the LLM can never impact the program flow. To further improve security, CaMeL relies on a notion of a capability to prevent the exfiltration of private data over unauthorized data flows. We demonstrate effectiveness of CaMeL by solving $67\%$ of tasks with provable security in AgentDojo [NeurIPS 2024], a recent agentic security benchmark. 

**Abstract (ZH)**: Large Language Models (LLMs)在代理系统中的防护：CaMeL方法 

---
# REALM: A Dataset of Real-World LLM Use Cases 

**Title (ZH)**: REALM：现实世界大型语言模型应用场景数据集 

**Authors**: Jingwen Cheng, Kshitish Ghate, Wenyue Hua, William Yang Wang, Hong Shen, Fei Fang  

**Link**: [PDF](https://arxiv.org/pdf/2503.18792)  

**Abstract**: Large Language Models, such as the GPT series, have driven significant industrial applications, leading to economic and societal transformations. However, a comprehensive understanding of their real-world applications remains limited. To address this, we introduce REALM, a dataset of over 94,000 LLM use cases collected from Reddit and news articles. REALM captures two key dimensions: the diverse applications of LLMs and the demographics of their users. It categorizes LLM applications and explores how users' occupations relate to the types of applications they use. By integrating real-world data, REALM offers insights into LLM adoption across different domains, providing a foundation for future research on their evolving societal roles. A dedicated dashboard this https URL presents the data. 

**Abstract (ZH)**: 大型语言模型，如GPT系列，推动了重要的工业应用，带来了经济和社会的变革。然而，对其实际应用的全面理解仍然有限。为了解决这一问题，我们介绍了REALM数据集，该数据集包含超过94,000个从Reddit和新闻文章中收集的大型语言模型使用案例。REALM捕捉了两个关键维度：大型语言模型的多样化应用和其用户的 demographic 属性。它对大型语言模型的应用进行了分类，并探讨了用户的职业与他们使用的应用类型之间的关系。通过整合实际数据，REALM提供了不同领域大型语言模型采用情况的见解，为未来研究其不断演变的社会角色奠定了基础。详细数据可通过此网址 https:// 进行查看。 

---
# Frequency Dynamic Convolution for Dense Image Prediction 

**Title (ZH)**: 频率动态卷积用于密集图像预测 

**Authors**: Linwei Chen, Lin Gu, Liang Li, Chenggang Yan, Ying Fu  

**Link**: [PDF](https://arxiv.org/pdf/2503.18783)  

**Abstract**: While Dynamic Convolution (DY-Conv) has shown promising performance by enabling adaptive weight selection through multiple parallel weights combined with an attention mechanism, the frequency response of these weights tends to exhibit high similarity, resulting in high parameter costs but limited adaptability. In this work, we introduce Frequency Dynamic Convolution (FDConv), a novel approach that mitigates these limitations by learning a fixed parameter budget in the Fourier domain. FDConv divides this budget into frequency-based groups with disjoint Fourier indices, enabling the construction of frequency-diverse weights without increasing the parameter cost. To further enhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency Band Modulation (FBM). KSM dynamically adjusts the frequency response of each filter at the spatial level, while FBM decomposes weights into distinct frequency bands in the frequency domain and modulates them dynamically based on local content. Extensive experiments on object detection, segmentation, and classification validate the effectiveness of FDConv. We demonstrate that when applied to ResNet-50, FDConv achieves superior performance with a modest increase of +3.6M parameters, outperforming previous methods that require substantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M). Moreover, FDConv seamlessly integrates into a variety of architectures, including ConvNeXt, Swin-Transformer, offering a flexible and efficient solution for modern vision tasks. The code is made publicly available at this https URL. 

**Abstract (ZH)**: 频率动态卷积：一种新型的参数预算学习方法及其应用 

---
# BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with Low-Bit KV Cache 

**Title (ZH)**: BitDecoding: 解锁用于长上下文LLMs解码的低比特KV缓存的张量核心 

**Authors**: Dayou Du, Shijie Cao, Jianyi Cheng, Ting Cao, Mao Yang  

**Link**: [PDF](https://arxiv.org/pdf/2503.18773)  

**Abstract**: The growing adoption of long-context Large Language Models (LLMs) has introduced significant memory and computational challenges in autoregressive decoding due to the expanding Key-Value (KV) cache. KV cache quantization has emerged as a promising solution, with prior work showing that 4-bit or even 2-bit quantization can maintain model accuracy while reducing memory costs. However, despite these benefits, preliminary implementations for the low-bit KV cache struggle to deliver the expected speedup due to quantization and dequantization overheads and the lack of Tensor Cores utilization. In this work, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor Cores for efficient decoding with low-bit KV cache. Efficiently leveraging Tensor Cores for low-bit KV cache is challenging due to the dynamic nature of KV cache generation at each decoding step. BitDecoding addresses these challenges with a Tensor Cores-Centric BitFusion Scheme that ensures data layout compatibility to enable high utilization of Tensor Cores. Additionally, BitDecoding incorporates a warp-efficient parallel decoding kernel and a fine-grained asynchronous pipeline, minimizing dequantization overhead and improving computational efficiency. Experiments show that BitDecoding achieves up to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to FP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV cache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K sequence length, BitDecoding reduces single-batch decoding latency by 3x, demonstrating its effectiveness in long-context generation scenarios. The code is available at this https URL. 

**Abstract (ZH)**: 基于GPU的低比特Key-Value缓存高效解码框架BitDecoding 

---
# Mechanistic Interpretability of Fine-Tuned Vision Transformers on Distorted Images: Decoding Attention Head Behavior for Transparent and Trustworthy AI 

**Title (ZH)**: 微调的视觉变换器在失真图像上的机制可解释性：解码注意力头行为以实现透明和可信赖的AI 

**Authors**: Nooshin Bahador  

**Link**: [PDF](https://arxiv.org/pdf/2503.18762)  

**Abstract**: Mechanistic interpretability improves the safety, reliability, and robustness of large AI models. This study examined individual attention heads in vision transformers (ViTs) fine tuned on distorted 2D spectrogram images containing non relevant content (axis labels, titles, color bars). By introducing extraneous features, the study analyzed how transformer components processed unrelated information, using mechanistic interpretability to debug issues and reveal insights into transformer architectures. Attention maps assessed head contributions across layers. Heads in early layers (1 to 3) showed minimal task impact with ablation increased MSE loss slightly ({\mu}=0.11%, {\sigma}=0.09%), indicating focus on less critical low level features. In contrast, deeper heads (e.g., layer 6) caused a threefold higher loss increase ({\mu}=0.34%, {\sigma}=0.02%), demonstrating greater task importance. Intermediate layers (6 to 11) exhibited monosemantic behavior, attending exclusively to chirp regions. Some early heads (1 to 4) were monosemantic but non task relevant (e.g. text detectors, edge or corner detectors). Attention maps distinguished monosemantic heads (precise chirp localization) from polysemantic heads (multiple irrelevant regions). These findings revealed functional specialization in ViTs, showing how heads processed relevant vs. extraneous information. By decomposing transformers into interpretable components, this work enhanced model understanding, identified vulnerabilities, and advanced safer, more transparent AI. 

**Abstract (ZH)**: 机制可解释性提高大型AI模型的安全性、可靠性和鲁棒性。本研究探讨了在包含无关内容（轴标签、标题、颜色条）的失真2D谱图图像上微调的视觉变换器（ViTs）中的个体注意力头部。通过引入额外特征，研究分析了变压器组件处理无关信息的方式，利用机制可解释性来调试问题并揭示变压器架构的见解。注意力图评估了各层头部的贡献。早期层（1到3）的头部对任务影响较小，移除后稍增均方误差损失（μ=0.11%，σ=0.09%），表明关注于较为次要的低级特征。相比之下，较深层（如第6层）导致损失增加三倍多（μ=0.34%，σ=0.02%），表明任务重要性更大。中层（6到11）表现出单一语义行为，仅关注 chirp 区域。一些早期头部（1到4）表现出单一语义但与任务无关（例如文本检测器、边缘或角落检测器）。注意力图区分了单一语义头部（精确 chirp 定位）和多重语义头部（多个无关区域）。这些发现揭示了ViTs的功能专业化，展示了头部如何处理相关 vs. 无关信息。通过将变压器分解为可解释组件，本研究增强了模型理解，识别出了漏洞，并推进了更安全、更透明的AI。 

---
# EgoSurgery-HTS: A Dataset for Egocentric Hand-Tool Segmentation in Open Surgery Videos 

**Title (ZH)**: EgoSurgery-HTS：开放手术视频中的自我中心手 TOOL分割数据集 

**Authors**: Nathan Darjana, Ryo Fujii, Hideo Saito, Hiroki Kajita  

**Link**: [PDF](https://arxiv.org/pdf/2503.18755)  

**Abstract**: Egocentric open-surgery videos capture rich, fine-grained details essential for accurately modeling surgical procedures and human behavior in the operating room. A detailed, pixel-level understanding of hands and surgical tools is crucial for interpreting a surgeon's actions and intentions. We introduce EgoSurgery-HTS, a new dataset with pixel-wise annotations and a benchmark suite for segmenting surgical tools, hands, and interacting tools in egocentric open-surgery videos. Specifically, we provide a labeled dataset for (1) tool instance segmentation of 14 distinct surgical tools, (2) hand instance segmentation, and (3) hand-tool segmentation to label hands and the tools they manipulate. Using EgoSurgery-HTS, we conduct extensive evaluations of state-of-the-art segmentation methods and demonstrate significant improvements in the accuracy of hand and hand-tool segmentation in egocentric open-surgery videos compared to existing datasets. The dataset will be released at this https URL. 

**Abstract (ZH)**: 自视点开放手术视频捕捉到富含细粒度细节，对于准确建模手术过程和手术室中的人类行为至关重要。对手和手术器械的详细像素级理解对于解析外科医生的动作和意图至关重要。我们介绍了EgoSurgery-HTS新数据集及其分割外科器械、手部和手工具交互的基准套件，提供了用于（1）14种不同手术器械实例分割，（2）手实例分割，以及（3）手工具分割以标注手部及其操控的器械的标注数据。使用EgoSurgery-HTS，我们对最先进的分割方法进行了广泛评估，并展示了在自视点开放手术视频中手和手工具分割准确性上的显著改进，超过了现有数据集。数据集将在以下链接发布：这个 https URL。 

---
# Construction Identification and Disambiguation Using BERT: A Case Study of NPN 

**Title (ZH)**: 基于BERT的NPN的构造识别与消歧：一个案例研究 

**Authors**: Wesley Scivetti, Nathan Schneider  

**Link**: [PDF](https://arxiv.org/pdf/2503.18751)  

**Abstract**: Construction Grammar hypothesizes that knowledge of a language consists chiefly of knowledge of form-meaning pairs (''constructions'') that include vocabulary, general grammar rules, and even idiosyncratic patterns. Recent work has shown that transformer language models represent at least some constructional patterns, including ones where the construction is rare overall. In this work, we probe BERT's representation of the form and meaning of a minor construction of English, the NPN (noun-preposition-noun) construction -- exhibited in such expressions as face to face and day to day -- which is known to be polysemous. We construct a benchmark dataset of semantically annotated corpus instances (including distractors that superficially resemble the construction). With this dataset, we train and evaluate probing classifiers. They achieve decent discrimination of the construction from distractors, as well as sense disambiguation among true instances of the construction, revealing that BERT embeddings carry indications of the construction's semantics. Moreover, artificially permuting the word order of true construction instances causes them to be rejected, indicating sensitivity to matters of form. We conclude that BERT does latently encode at least some knowledge of the NPN construction going beyond a surface syntactic pattern and lexical cues. 

**Abstract (ZH)**: 构造语法假定语言知识主要由形式意义对（“构造”）组成，这些构造包括词汇、一般的语法规则，甚至包括特有的模式。近期研究表明，转换器语言模型至少表示了一些构造模式，即使这些构造在整体上较为罕见。在此项工作中，我们探究了BERT对英语小量使用的NPN（名词-介词-名词）构造的形式和意义的表示——如“面对面”、“日复一日”等表达式中所体现的——该构造已知具有多义性。我们构建了一个语义标注的基准数据集（包括外观上类似但语义不同的干扰项），基于此数据集训练并评估了探针分类器。分类器能够较好地区分构造与其干扰项，并能够区分构造的真实实例中的意义歧义，揭示BERT嵌入能够携带构造语义的线索。此外，人为改变构造实例中的词序会使其被拒绝，表明其对形式问题具有敏感性。我们得出结论，BERT在表面句法模式和词汇线索之外，隐含地编码了至少部分关于NPN构造的知识。 

---
# Energy-Efficient Dynamic Training and Inference for GNN-Based Network Modeling 

**Title (ZH)**: 基于GNN的网络建模的能效动态训练与推理 

**Authors**: Chetna Singhal, Yassine Hadjadj-Aoul  

**Link**: [PDF](https://arxiv.org/pdf/2503.18706)  

**Abstract**: Efficient network modeling is essential for resource optimization and network planning in next-generation large-scale complex networks. Traditional approaches, such as queuing theory-based modeling and packet-based simulators, can be inefficient due to the assumption made and the computational expense, respectively. To address these challenges, we propose an innovative energy-efficient dynamic orchestration of Graph Neural Networks (GNN) based model training and inference framework for context-aware network modeling and predictions. We have developed a low-complexity solution framework, QAG, that is a Quantum approximation optimization (QAO) algorithm for Adaptive orchestration of GNN-based network modeling. We leverage the tripartite graph model to represent a multi-application system with many compute nodes. Thereafter, we apply the constrained graph-cutting using QAO to find the feasible energy-efficient configurations of the GNN-based model and deploying them on the available compute nodes to meet the network modeling application requirements. The proposed QAG scheme closely matches the optimum and offers atleast a 50% energy saving while meeting the application requirements with 60% lower churn-rate. 

**Abstract (ZH)**: 高效的网络建模对于下一代大规模复杂网络的资源优化和网络规划至关重要。传统的基于排队理论的建模方法和基于包的仿真器由于假设条件和计算成本的原因可能存在低效问题。为解决这些问题，我们提出了一种基于图神经网络（GNN）动态 orchestration 的创新性能量高效模型训练与推理框架，用于上下文感知的网络建模与预测。我们开发了一个低复杂度的解决方案框架，QAG，这是一种量子近似优化（QAO）算法，用于自适应 orchestration 的 GNN 基础网络建模。利用 tripartite 图模型表示包含多个计算节点的多应用系统。然后，我们通过 QAO 进行约束图划分，寻找 GNN 基础模型的能量高效配置，并将其部署到可用的计算节点上，以满足网络建模应用需求。所提出的 QAG 方案接近最优方案，至少可以节省50%的能量，在满足应用需求的同时将更换率降低60%。 

---
# Efficient Continual Adaptation of Pretrained Robotic Policy with Online Meta-Learned Adapters 

**Title (ZH)**: 高效的预训练机器人策略的在线元学习适配 

**Authors**: Ruiqi Zhu, Endong Sun, Guanhe Huang, Oya Celiktutan  

**Link**: [PDF](https://arxiv.org/pdf/2503.18684)  

**Abstract**: Continual adaptation is essential for general autonomous agents. For example, a household robot pretrained with a repertoire of skills must still adapt to unseen tasks specific to each household. Motivated by this, building upon parameter-efficient fine-tuning in language models, prior works have explored lightweight adapters to adapt pretrained policies, which can preserve learned features from the pretraining phase and demonstrate good adaptation performances. However, these approaches treat task learning separately, limiting knowledge transfer between tasks. In this paper, we propose Online Meta-Learned adapters (OMLA). Instead of applying adapters directly, OMLA can facilitate knowledge transfer from previously learned tasks to current learning tasks through a novel meta-learning objective. Extensive experiments in both simulated and real-world environments demonstrate that OMLA can lead to better adaptation performances compared to the baseline methods. The project link: this https URL. 

**Abstract (ZH)**: 持续适应对于通用自主代理至关重要。例如，预训练掌握一系列技能的家用机器人仍需要适应每个家庭特有的未见过的任务。受此启发，基于语言模型中的参数高效微调，先前的工作探索了轻量级适配器以适应预训练策略，这些适配器可以在保持预训练阶段学到的特征的同时表现出良好的适应性能。然而，这些方法将任务学习分开处理，限制了任务之间的知识迁移。在本文中，我们提出了一种在线元学习适配器(OMLA)。OMLA 不直接应用适配器，而是通过一个新的元学习目标促进先前学习任务的知识向当前学习任务的迁移。在模拟和真实环境中的广泛实验表明，OMLA 可以比基线方法获得更好的适应性能。项目链接：this https URL。 

---
# Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models 

**Title (ZH)**: Commander-GPT: 全面释放多模态大语言模型的讽刺检测能力 

**Authors**: Yazhou Zhang, Chunwang Zou, Bo Wang, Jing Qin  

**Link**: [PDF](https://arxiv.org/pdf/2503.18681)  

**Abstract**: Sarcasm detection, as a crucial research direction in the field of Natural Language Processing (NLP), has attracted widespread attention. Traditional sarcasm detection tasks have typically focused on single-modal approaches (e.g., text), but due to the implicit and subtle nature of sarcasm, such methods often fail to yield satisfactory results. In recent years, researchers have shifted the focus of sarcasm detection to multi-modal approaches. However, effectively leveraging multi-modal information to accurately identify sarcastic content remains a challenge that warrants further exploration. Leveraging the powerful integrated processing capabilities of Multi-Modal Large Language Models (MLLMs) for various information sources, we propose an innovative multi-modal Commander-GPT framework. Inspired by military strategy, we first decompose the sarcasm detection task into six distinct sub-tasks. A central commander (decision-maker) then assigns the best-suited large language model to address each specific sub-task. Ultimately, the detection results from each model are aggregated to identify sarcasm. We conducted extensive experiments on MMSD and MMSD 2.0, utilizing four multi-modal large language models and six prompting strategies. Our experiments demonstrate that our approach achieves state-of-the-art performance, with a 19.3% improvement in F1 score, without necessitating fine-tuning or ground-truth rationales. 

**Abstract (ZH)**: 多模态sarcastic内容检测：基于多模态大型语言模型的Commander-GPT框架 

---
# Any6D: Model-free 6D Pose Estimation of Novel Objects 

**Title (ZH)**: Any6D: 无需模型的新型物体6D姿态估计 

**Authors**: Taeyeop Lee, Bowen Wen, Minjun Kang, Gyuree Kang, In So Kweon, Kuk-Jin Yoon  

**Link**: [PDF](https://arxiv.org/pdf/2503.18673)  

**Abstract**: We introduce Any6D, a model-free framework for 6D object pose estimation that requires only a single RGB-D anchor image to estimate both the 6D pose and size of unknown objects in novel scenes. Unlike existing methods that rely on textured 3D models or multiple viewpoints, Any6D leverages a joint object alignment process to enhance 2D-3D alignment and metric scale estimation for improved pose accuracy. Our approach integrates a render-and-compare strategy to generate and refine pose hypotheses, enabling robust performance in scenarios with occlusions, non-overlapping views, diverse lighting conditions, and large cross-environment variations. We evaluate our method on five challenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O, demonstrating its effectiveness in significantly outperforming state-of-the-art methods for novel object pose estimation. Project page: this https URL 

**Abstract (ZH)**: Any6D：一种仅需单张RGB-D 锚图即可进行未知对象六自由度姿态估计的模型无依赖框架 

---
# Towards Human-Understandable Multi-Dimensional Concept Discovery 

**Title (ZH)**: 面向人类可理解的多维度概念发现 

**Authors**: Arne Grobrügge, Niklas Kühl, Gerhard Satzger, Philipp Spitzer  

**Link**: [PDF](https://arxiv.org/pdf/2503.18629)  

**Abstract**: Concept-based eXplainable AI (C-XAI) aims to overcome the limitations of traditional saliency maps by converting pixels into human-understandable concepts that are consistent across an entire dataset. A crucial aspect of C-XAI is completeness, which measures how well a set of concepts explains a model's decisions. Among C-XAI methods, Multi-Dimensional Concept Discovery (MCD) effectively improves completeness by breaking down the CNN latent space into distinct and interpretable concept subspaces. However, MCD's explanations can be difficult for humans to understand, raising concerns about their practical utility. To address this, we propose Human-Understandable Multi-dimensional Concept Discovery (HU-MCD). HU-MCD uses the Segment Anything Model for concept identification and implements a CNN-specific input masking technique to reduce noise introduced by traditional masking methods. These changes to MCD, paired with the completeness relation, enable HU-MCD to enhance concept understandability while maintaining explanation faithfulness. Our experiments, including human subject studies, show that HU-MCD provides more precise and reliable explanations than existing C-XAI methods. The code is available at this https URL. 

**Abstract (ZH)**: 基于概念的可解释人工智能（C-XAI）旨在通过将像素转换为整个数据集中一致的人类可理解概念来克服传统显著性图的局限性。C-XAI的关键方面是完备性，它衡量一组概念解释模型决策的能力。在C-XAI方法中，多维概念发现（MCD）通过将CNN潜在空间分解为独立且可解释的概念子空间，有效地提高了完备性。然而，MCD的解释可能难以为人理解，对其实际应用性提出质疑。为了解决这一问题，我们提出了人类可理解的多维概念发现（HU-MCD）。HU-MCD利用Segment Anything模型进行概念识别，并采用特定于CNN的输入蒙版技术以减少传统蒙版方法引入的噪声。这些对MCD的改进，加上完备性关系的结合，使HU-MCD能够在提高概念可理解性的同时保持解释的可信性。我们的实验，包括人类受控实验，显示HU-MCD提供了比现有C-XAI方法更精确和可靠的解释。代码可在以下链接获取。 

---
# Dig2DIG: Dig into Diffusion Information Gains for Image Fusion 

**Title (ZH)**: Dig2DIG: 探究扩散信息增益的图像融合方法 

**Authors**: Bing Cao, Baoshuo Cai, Changqing Zhang, Qinghua Hu  

**Link**: [PDF](https://arxiv.org/pdf/2503.18627)  

**Abstract**: Image fusion integrates complementary information from multi-source images to generate more informative results. Recently, the diffusion model, which demonstrates unprecedented generative potential, has been explored in image fusion. However, these approaches typically incorporate predefined multimodal guidance into diffusion, failing to capture the dynamically changing significance of each modality, while lacking theoretical guarantees. To address this issue, we reveal a significant spatio-temporal imbalance in image denoising; specifically, the diffusion model produces dynamic information gains in different image regions with denoising steps. Based on this observation, we Dig into the Diffusion Information Gains (Dig2DIG) and theoretically derive a diffusion-based dynamic image fusion framework that provably reduces the upper bound of the generalization error. Accordingly, we introduce diffusion information gains (DIG) to quantify the information contribution of each modality at different denoising steps, thereby providing dynamic guidance during the fusion process. Extensive experiments on multiple fusion scenarios confirm that our method outperforms existing diffusion-based approaches in terms of both fusion quality and inference efficiency. 

**Abstract (ZH)**: 图像融合通过整合多源图像中的互补信息以生成更具信息量的结果。近期，展现出前所未有的生成潜力的扩散模型被探索应用于图像融合。然而，这些方法通常将预定义的多模态指导信息融入扩散模型中，未能捕捉到每种模态动态变化的重要性，且缺乏理论保证。为了解决这一问题，我们揭示了图像去噪过程中时空不平衡的现象；具体而言，扩散模型在去噪步骤中于不同的图像区域产生动态的信息增益。基于这一观察，我们探索了扩散信息增益（Dig2DIG），并理论上推导出一个基于扩散的动态图像融合框架，可证明地降低泛化误差的上界。据此，我们引入了扩散信息增益（DIG）来量化每种模态在不同去噪步骤中的信息贡献，从而在融合过程中提供动态指导。多项融合场景下的实验结果证实，我们的方法在融合质量和推理效率方面均优于现有的基于扩散模型的方法。 

---
# Adventurer: Exploration with BiGAN for Deep Reinforcement Learning 

**Title (ZH)**: Adventurer: 使用BiGAN进行深度强化学习的探索 

**Authors**: Yongshuai Liu, Xin Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.18612)  

**Abstract**: Recent developments in deep reinforcement learning have been very successful in learning complex, previously intractable problems. Sample efficiency and local optimality, however, remain significant challenges. To address these challenges, novelty-driven exploration strategies have emerged and shown promising potential. Unfortunately, no single algorithm outperforms all others in all tasks and most of them struggle with tasks with high-dimensional and complex observations. In this work, we propose Adventurer, a novelty-driven exploration algorithm that is based on Bidirectional Generative Adversarial Networks (BiGAN), where BiGAN is trained to estimate state novelty. Intuitively, a generator that has been trained on the distribution of visited states should only be able to generate a state coming from the distribution of visited states. As a result, novel states using the generator to reconstruct input states from certain latent representations would lead to larger reconstruction errors. We show that BiGAN performs well in estimating state novelty for complex observations. This novelty estimation method can be combined with intrinsic-reward-based exploration. Our empirical results show that Adventurer produces competitive results on a range of popular benchmark tasks, including continuous robotic manipulation tasks (e.g. Mujoco robotics) and high-dimensional image-based tasks (e.g. Atari games). 

**Abstract (ZH)**: Recent developments in deep reinforcement learning have been very successful in learning complex, previously intractable problems. Sample efficiency and local optimality, however, remain significant challenges. To address these challenges, novelty-driven exploration strategies have emerged and shown promising potential. Unfortunately, no single algorithm outperforms all others in all tasks and most of them struggle with tasks with high-dimensional and complex observations. In this work, we propose Adventurer, a novelty-driven exploration algorithm based on Bidirectional Generative Adversarial Networks (BiGAN), where BiGAN is trained to estimate state novelty. 

---
# Reinforcement Learning in Switching Non-Stationary Markov Decision Processes: Algorithms and Convergence Analysis 

**Title (ZH)**: 切换非稳态马尔可夫决策过程的强化学习：算法与收敛性分析 

**Authors**: Mohsen Amiri, Sindri Magnússon  

**Link**: [PDF](https://arxiv.org/pdf/2503.18607)  

**Abstract**: Reinforcement learning in non-stationary environments is challenging due to abrupt and unpredictable changes in dynamics, often causing traditional algorithms to fail to converge. However, in many real-world cases, non-stationarity has some structure that can be exploited to develop algorithms and facilitate theoretical analysis. We introduce one such structure, Switching Non-Stationary Markov Decision Processes (SNS-MDP), where environments switch over time based on an underlying Markov chain. Under a fixed policy, the value function of an SNS-MDP admits a closed-form solution determined by the Markov chain's statistical properties, and despite the inherent non-stationarity, Temporal Difference (TD) learning methods still converge to the correct value function. Furthermore, policy improvement can be performed, and it is shown that policy iteration converges to the optimal policy. Moreover, since Q-learning converges to the optimal Q-function, it likewise yields the corresponding optimal policy. To illustrate the practical advantages of SNS-MDPs, we present an example in communication networks where channel noise follows a Markovian pattern, demonstrating how this framework can effectively guide decision-making in complex, time-varying contexts. 

**Abstract (ZH)**: 在非平稳环境中的强化学习因动力学的突然和不可预测的变化而具有挑战性，通常导致传统算法无法收敛。然而，在许多实际情况下，非平稳性具有某种可利用的结构，可以开发算法并促进理论分析。我们介绍了一种这样的结构——切换非平稳马尔可夫决策过程（SNS-MDP），其中环境根据潜在的马尔可夫链在时间上切换。在固定策略下，SNS-MDP的价值函数可以通过马尔可夫链的统计属性获得闭式解，尽管存在固有的非平稳性，时差学习方法仍能收敛到正确的价值函数。此外，可以通过执行策略改进，证明策略迭代能收敛到最优策略。由于Q学习收敛于最优Q函数，它同样会产生相应的最优策略。为了说明SNS-MDP的实际优势，我们给出一个通信网络中的示例，其中信道噪声遵循马尔可夫模式，展示该框架如何有效指导复杂、时变环境中的决策。 

---
# Adaptive Unimodal Regulation for Balanced Multimodal Information Acquisition 

**Title (ZH)**: 自适应单模态调节以实现平衡的多模态信息获取 

**Authors**: Chengxiang Huang, Yake Wei, Zequn Yang, Di Hu  

**Link**: [PDF](https://arxiv.org/pdf/2503.18595)  

**Abstract**: Sensory training during the early ages is vital for human development. Inspired by this cognitive phenomenon, we observe that the early training stage is also important for the multimodal learning process, where dataset information is rapidly acquired. We refer to this stage as the prime learning window. However, based on our observation, this prime learning window in multimodal learning is often dominated by information-sufficient modalities, which in turn suppresses the information acquisition of information-insufficient modalities. To address this issue, we propose Information Acquisition Regulation (InfoReg), a method designed to balance information acquisition among modalities. Specifically, InfoReg slows down the information acquisition process of information-sufficient modalities during the prime learning window, which could promote information acquisition of information-insufficient modalities. This regulation enables a more balanced learning process and improves the overall performance of the multimodal network. Experiments show that InfoReg outperforms related multimodal imbalanced methods across various datasets, achieving superior model performance. The code is available at this https URL. 

**Abstract (ZH)**: 早年感觉训练对于人类发展至关重要。受此认知现象启发，我们观察到多模态学习过程中的早期训练阶段也同样重要，此时大量数据集信息迅速被获取。我们将这一阶段称为关键学习窗口。然而，我们的观察表明，在多模态学习的关键学习窗口中，信息充足的模态往往占据主导地位，进而抑制了信息不足模态的信息获取。为解决这一问题，我们提出了一种信息获取调节（InfoReg）方法，旨在平衡各模态之间信息的获取。具体而言，InfoReg在关键学习窗口中减缓了信息充足模态的信息获取过程，从而促进了信息不足模态的信息获取。这种调节机制使得学习过程更为平衡，提高了多模态网络的整体性能。实验结果显示，InfoReg在各类数据集上的表现优于相关不平衡多模态方法，取得了更优异的模型性能。代码可访问此链接。 

---
# ClinText-SP and RigoBERTa Clinical: a new set of open resources for Spanish Clinical NLP 

**Title (ZH)**: ClinText-SP和RigoBERTa Clinical：一套新的西班牙语临床NLP开放资源 

**Authors**: Guillem García Subies, Álvaro Barbero Jiménez, Paloma Martínez Fernández  

**Link**: [PDF](https://arxiv.org/pdf/2503.18594)  

**Abstract**: We present a novel contribution to Spanish clinical natural language processing by introducing the largest publicly available clinical corpus, ClinText-SP, along with a state-of-the-art clinical encoder language model, RigoBERTa Clinical. Our corpus was meticulously curated from diverse open sources, including clinical cases from medical journals and annotated corpora from shared tasks, providing a rich and diverse dataset that was previously difficult to access. RigoBERTa Clinical, developed through domain-adaptive pretraining on this comprehensive dataset, significantly outperforms existing models on multiple clinical NLP benchmarks. By publicly releasing both the dataset and the model, we aim to empower the research community with robust resources that can drive further advancements in clinical NLP and ultimately contribute to improved healthcare applications. 

**Abstract (ZH)**: 我们通过引入最大的公开临床语料库ClinText-SP以及最先进的临床编码语言模型RigoBERTa Clinical，为西班牙临床自然语言处理领域做出了 novel 贡献。我们的语料库精心从各种开放源中筛选，包括医学期刊的临床病例和共享任务的标注语料库，提供了一个丰富多样的数据集，此前难以获取。RigoBERTa Clinical通过在这一全面数据集上进行领域适应预训练，多项临床NLP基准测试中显著优于现有模型。通过公开发布数据集和模型，我们旨在为研究社区提供强大的资源，推动临床NLP的进一步发展，最终为改善健康护理应用做出贡献。 

---
# Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding 

**Title (ZH)**: 银河探索者：几何感知的大视图语言模型在银河规模理解中的应用 

**Authors**: Tianyu Chen, Xingcheng Fu, Yisen Gao, Haodong Qian, Yuecen Wei, Kun Yan, Haoyi Zhou, Jianxin Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.18578)  

**Abstract**: Modern vision-language models (VLMs) develop patch embedding and convolution backbone within vector space, especially Euclidean ones, at the very founding. When expanding VLMs to a galaxy scale for understanding astronomical phenomena, the integration of spherical space for planetary orbits and hyperbolic spaces for black holes raises two formidable challenges. a) The current pre-training model is confined to Euclidean space rather than a comprehensive geometric embedding. b) The predominant architecture lacks suitable backbones for anisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a geometry-aware VLM, for the universe-level vision understanding tasks. We proposed the geometry prompt that generates geometry tokens by random walks across diverse spaces on a multi-scale physical graph, along with a geometry adapter that compresses and reshapes the space anisotropy in a mixture-of-experts manner. Extensive experiments demonstrate the effectiveness of our approach, with Galaxy-Walker achieving state-of-the-art performance in both galaxy property estimation ($R^2$ scores up to $0.91$) and morphology classification tasks (up to $+0.17$ F1 improvement in challenging features), significantly outperforming both domain-specific models and general-purpose VLMs. 

**Abstract (ZH)**: 现代视觉-语言模型在构建patches嵌入和卷积骨干时主要局限于欧几里得向量空间。当将视觉-语言模型扩展到银河系尺度以理解天文学现象时，行星轨道的球面空间和黑洞的双曲空间集成带来了两大挑战。a) 当前的预训练模型局限于欧几里得空间，而非全面的几何嵌入。b) 主要的架构缺乏适合各向异性物理几何结构的骨干网络。本文介绍了宇宙级视觉理解任务中的几何感知视觉-语言模型Galaxy-Walker。我们提出了几何提示，通过在多尺度物理图上进行随机漫步生成几何标记，并提出了一种几何适配器，以混合专家的方式压缩和重塑空间的各向异性。大量实验表明了该方法的有效性，Galaxy-Walker在星系属性估计（$R^2$分数最高可达0.91）和形态分类任务（在挑战性特征上F1分数提高0.17）中均取得了最佳性能，显著优于专门领域模型和通用视觉-语言模型。 

---
# Identifying and Characterising Higher Order Interactions in Mobility Networks Using Hypergraphs 

**Title (ZH)**: 使用超图识别和表征移动网络中的高阶交互 

**Authors**: Prathyush Sambaturu, Bernardo Gutierrez, Moritz U.G. Kraemer  

**Link**: [PDF](https://arxiv.org/pdf/2503.18572)  

**Abstract**: Understanding human mobility is essential for applications ranging from urban planning to public health. Traditional mobility models such as flow networks and colocation matrices capture only pairwise interactions between discrete locations, overlooking higher-order relationships among locations (i.e., mobility flow among two or more locations). To address this, we propose co-visitation hypergraphs, a model that leverages temporal observation windows to extract group interactions between locations from individual mobility trajectory data. Using frequent pattern mining, our approach constructs hypergraphs that capture dynamic mobility behaviors across different spatial and temporal scales. We validate our method on a publicly available mobility dataset and demonstrate its effectiveness in analyzing city-scale mobility patterns, detecting shifts during external disruptions such as extreme weather events, and examining how a location's connectivity (degree) relates to the number of points of interest (POIs) within it. Our results demonstrate that our hypergraph-based mobility analysis framework is a valuable tool with potential applications in diverse fields such as public health, disaster resilience, and urban planning. 

**Abstract (ZH)**: 理解人类移动性对于从城市规划到公共卫生等应用至关重要。传统的移动性模型如流网络和共存矩阵仅捕捉离散地点之间的成对交互，忽视了地点间的高阶关系（即两个或多个地点之间的移动流）。为解决这一问题，我们提出了共访问超图模型，该模型利用时间观察窗口从个体移动轨迹数据中提取地点之间的群组交互。通过频繁模式挖掘，我们的方法构建了能够捕捉不同空间和时间尺度下动态移动行为的超图。我们在一个公开的移动性数据集上验证了该方法，并展示了其在分析城市规模的移动模式、检测外部干扰（如极端天气事件）期间的变化以及研究地点连通性（度）与其内部兴趣点（POI）数量的关系方面的有效性。我们的结果表明，基于超图的移动性分析框架是一个有价值的工具，具有在公共卫生、灾害抵御和城市规划等领域应用的潜力。 

---
# Anchor-based oversampling for imbalanced tabular data via contrastive and adversarial learning 

**Title (ZH)**: 基于锚点的过采样方法：通过对比学习和对抗学习处理不平衡表格数据 

**Authors**: Hadi Mohammadi, Ehsan Nazerfard, Mostafa Haghir Chehreghani  

**Link**: [PDF](https://arxiv.org/pdf/2503.18569)  

**Abstract**: Imbalanced data represent a distribution with more frequencies of one class (majority) than the other (minority). This phenomenon occurs across various domains, such as security, medical care and human activity. In imbalanced learning, classification algorithms are typically inclined to classify the majority class accurately, resulting in artificially high accuracy rates. As a result, many minority samples are mistakenly labelled as majority-class instances, resulting in a bias that benefits the majority class. This study presents a framework based on boundary anchor samples to tackle the imbalance learning challenge. First, we select and use anchor samples to train a multilayer perceptron (MLP) classifier, which acts as a prior knowledge model and aids the adversarial and contrastive learning procedures. Then, we designed a novel deep generative model called Anchor Stabilized Conditional Generative Adversarial Network or Anch-SCGAN in short. Anch-SCGAN is supported with two generators for the minority and majority classes and a discriminator incorporating additional class-specific information from the pre-trained feature extractor MLP. In addition, we facilitate the generator's training procedure in two ways. First, we define a new generator loss function based on reprocessed anchor samples and contrastive learning. Second, we apply a scoring strategy to stabilize the adversarial training part in generators. We train Anch-SCGAN and further finetune it with anchor samples to improve the precision of the generated samples. Our experiments on 16 real-world imbalanced datasets illustrate that Anch-SCGAN outperforms the renowned methods in imbalanced learning. 

**Abstract (ZH)**: 不平衡数据表示一种分布，其中一类（多数类）的频率远高于另一类（少数类）。这种现象在安全、医疗护理和人类活动等领域普遍存在。在不平衡学习中，分类算法通常倾向于准确分类多数类，导致人为提高了准确率。结果，许多少数类样本被错误地标记为多数类实例，造成了有利于多数类的偏差。本研究提出了一种基于边界锚样本的框架，以应对不平衡学习的挑战。首先，我们选择并使用锚样本训练一个多层感知机（MLP）分类器，该分类器作为先验知识模型，辅助对抗学习和对比学习过程。然后，我们设计了一个名为锚稳定条件生成器对抗网络或简称Anch-SCGAN的新颖深层生成模型。Anch-SCGAN配备有为少数类和多数类提供支持的两个生成器，并整合了预训练特征提取器MLP提供的额外类特定信息的判别器。此外，我们通过两种方式促进了生成器的训练过程。首先，我们定义了一个基于重新处理锚样本和对比学习的新生成器损失函数。其次，我们应用了一种评分策略来稳定生成器中的对抗训练部分。我们通过锚样本进一步微调Anch-SCGAN，并提高生成样本的精度。我们在16个实际的不平衡数据集上的实验显示，Anch-SCGAN优于现有的知名方法。 

---
# Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures 

**Title (ZH)**: Distil-xLSTM: 通过递归结构学习注意力机制 

**Authors**: Abdoul Majid O. Thiombiano, Brahim Hnich, Ali Ben Mrad, Mohamed Wiem Mkaouer  

**Link**: [PDF](https://arxiv.org/pdf/2503.18565)  

**Abstract**: The current era of Natural Language Processing (NLP) is dominated by Transformer models. However, novel architectures relying on recurrent mechanisms, such as xLSTM and Mamba, have been proposed as alternatives to attention-based models. Although computation is done differently than with the attention mechanism mechanism, these recurrent models yield good results and sometimes even outperform state-of-the-art attention-based models. In this work, we propose Distil-xLSTM, an xLSTM-based Small Language Model (SLM) trained by distilling knowledge from a Large Language Model (LLM) that shows promising results while being compute and scale efficient. Our Distil-xLSTM focuses on approximating a transformer-based model attention parametrization using its recurrent sequence mixing components and shows good results with minimal training. 

**Abstract (ZH)**: 当前自然语言处理（NLP）时代被变压器模型主导，但基于递归机制的新型架构，如xLSTM和Mamba，已被提议作为基于注意力机制模型的替代方案。虽然计算方式与注意力机制不同，这些递归模型仍然取得了良好的效果，有时甚至超过最先进的基于注意力机制的模型。在本文中，我们提出了一种Distil-xLSTM，这是一种由大型语言模型（LLM）提炼出知识的小型语言模型（SLM），在保持计算和规模效率的同时展现了令人鼓舞的结果。我们的Distil-xLSTM专注于使用其递归序列混合组件来逼近基于变压器的模型的注意力参数化，并在少量训练下取得了良好的效果。 

---
# Self-Reported Confidence of Large Language Models in Gastroenterology: Analysis of Commercial, Open-Source, and Quantized Models 

**Title (ZH)**: 大型语言模型在胃肠病学领域的自我报告信心分析：商业、开源及量化模型的研究 

**Authors**: Nariman Naderi, Seyed Amir Ahmad Safavi-Naini, Thomas Savage, Zahra Atf, Peter Lewis, Girish Nadkarni, Ali Soroush  

**Link**: [PDF](https://arxiv.org/pdf/2503.18562)  

**Abstract**: This study evaluated self-reported response certainty across several large language models (GPT, Claude, Llama, Phi, Mistral, Gemini, Gemma, and Qwen) using 300 gastroenterology board-style questions. The highest-performing models (GPT-o1 preview, GPT-4o, and Claude-3.5-Sonnet) achieved Brier scores of 0.15-0.2 and AUROC of 0.6. Although newer models demonstrated improved performance, all exhibited a consistent tendency towards overconfidence. Uncertainty estimation presents a significant challenge to the safe use of LLMs in healthcare. Keywords: Large Language Models; Confidence Elicitation; Artificial Intelligence; Gastroenterology; Uncertainty Quantification 

**Abstract (ZH)**: 本研究使用300个胃肠病学板格式问题，评估了多个大型语言模型（GPT、Claude、Llama、Phi、Mistral、Gemini、Gemma和Qwen）自我报告的置信度反应。性能最佳的模型（GPT-o1 预览、GPT-4o 和 Claude-3.5-Sonnet）获得了0.15-0.2的布赖耳评分和0.6的AUROC。尽管 newer 模型展示了改进的性能，但所有模型都表现出一致性过度自信的倾向。不确定性估计是安全在医疗保健中使用LLMs的重要挑战。关键词：大型语言模型；置信度引出；人工智能；胃肠病学；不确定性量化。 

---
# EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation 

**Title (ZH)**: 基于事件的人像动画的图像到视频生成：EvAnimate 

**Authors**: Qiang Qu, Ming Li, Xiaoming Chen, Tongliang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.18552)  

**Abstract**: Conditional human animation transforms a static reference image into a dynamic sequence by applying motion cues such as poses. These motion cues are typically derived from video data but are susceptible to limitations including low temporal resolution, motion blur, overexposure, and inaccuracies under low-light conditions. In contrast, event cameras provide data streams with exceptionally high temporal resolution, a wide dynamic range, and inherent resistance to motion blur and exposure issues. In this work, we propose EvAnimate, a framework that leverages event streams as motion cues to animate static human images. Our approach employs a specialized event representation that transforms asynchronous event streams into 3-channel slices with controllable slicing rates and appropriate slice density, ensuring compatibility with diffusion models. Subsequently, a dual-branch architecture generates high-quality videos by harnessing the inherent motion dynamics of the event streams, thereby enhancing both video quality and temporal consistency. Specialized data augmentation strategies further enhance cross-person generalization. Finally, we establish a new benchmarking, including simulated event data for training and validation, and a real-world event dataset capturing human actions under normal and extreme scenarios. The experiment results demonstrate that EvAnimate achieves high temporal fidelity and robust performance in scenarios where traditional video-derived cues fall short. 

**Abstract (ZH)**: 基于事件的条件人体动画框架 

---
# Discriminative protein sequence modelling with Latent Space Diffusion 

**Title (ZH)**: 基于潜在空间扩散的辨别性蛋白质序列建模 

**Authors**: Eoin Quinn, Ghassene Jebali, Maxime Seince, Oliver Bent  

**Link**: [PDF](https://arxiv.org/pdf/2503.18551)  

**Abstract**: We explore a framework for protein sequence representation learning that decomposes the task between manifold learning and distributional modelling. Specifically we present a Latent Space Diffusion architecture which combines a protein sequence autoencoder with a denoising diffusion model operating on its latent space. We obtain a one-parameter family of learned representations from the diffusion model, along with the autoencoder's latent representation. We propose and evaluate two autoencoder architectures: a homogeneous model forcing amino acids of the same type to be identically distributed in the latent space, and an inhomogeneous model employing a noise-based variant of masking. As a baseline we take a latent space learned by masked language modelling, and evaluate discriminative capability on a range of protein property prediction tasks. Our finding is twofold: the diffusion models trained on both our proposed variants display higher discriminative power than the one trained on the masked language model baseline, none of the diffusion representations achieve the performance of the masked language model embeddings themselves. 

**Abstract (ZH)**: 我们探索了一种将蛋白质序列表示学习任务分解到流形学习和分布建模之间的框架。具体而言，我们提出了一种潜空间扩散架构，该架构结合了蛋白质序列自编码器和在其潜空间上操作的去噪扩散模型。我们从扩散模型中获得了具有一个参数的可学习表示族，以及自编码器的潜空间表示。我们提出了两种自编码器架构：一种同质模型，强制相同类型的氨基酸在潜空间中服从同一分布；以及一种异质模型，采用基于噪声的掩码变体。作为基准，我们采用了由掩码语言模型学习的潜空间，并在一系列蛋白质性质预测任务上评估其区分能力。我们的发现有两个方面：在我们提出的两种变体上训练的扩散模型表现出比基于掩码语言模型基准更高的区分能力，但没有一种扩散表示能够达到掩码语言模型嵌入本身的表现。 

---
# RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD Command Sequence Generation 

**Title (ZH)**: RLCAD: 用于涉及革命的CAD命令序列生成的强化学习训练 Gym 

**Authors**: Xiaolong Yin, Xingyu Lu, Jiahang Shen, Jingzhe Ni, Hailong Li, Ruofeng Tong, Min Tang, Peng Du  

**Link**: [PDF](https://arxiv.org/pdf/2503.18549)  

**Abstract**: A CAD command sequence is a typical parametric design paradigm in 3D CAD systems where a model is constructed by overlaying 2D sketches with operations such as extrusion, revolution, and Boolean operations. Although there is growing academic interest in the automatic generation of command sequences, existing methods and datasets only support operations such as 2D sketching, extrusion,and Boolean operations. This limitation makes it challenging to represent more complex geometries. In this paper, we present a reinforcement learning (RL) training environment (gym) built on a CAD geometric engine. Given an input boundary representation (B-Rep) geometry, the policy network in the RL algorithm generates an action. This action, along with previously generated actions, is processed within the gym to produce the corresponding CAD geometry, which is then fed back into the policy network. The rewards, determined by the difference between the generated and target geometries within the gym, are used to update the RL network. Our method supports operations beyond sketches, Boolean, and extrusion, including revolution operations. With this training gym, we achieve state-of-the-art (SOTA) quality in generating command sequences from B-Rep geometries. In addition, our method can significantly improve the efficiency of command sequence generation by a factor of 39X compared with the previous training gym. 

**Abstract (ZH)**: 一种CAD命令序列的强化学习训练环境：超越传统操作的复杂几何体生成 

---
# An Identity and Interaction Based Network Forensic Analysis 

**Title (ZH)**: 基于身份与交互的网络取证分析 

**Authors**: Nathan Clarke, Gaseb Alotibi, Dany Joy, Fudong Li, Steven Furnell, Ali Alshumrani, Hussan Mohammed  

**Link**: [PDF](https://arxiv.org/pdf/2503.18542)  

**Abstract**: In todays landscape of increasing electronic crime, network forensics plays a pivotal role in digital investigations. It aids in understanding which systems to analyse and as a supplement to support evidence found through more traditional computer based investigations. However, the nature and functionality of the existing Network Forensic Analysis Tools (NFATs) fall short compared to File System Forensic Analysis Tools (FS FATs) in providing usable data. The analysis tends to focus upon IP addresses, which are not synonymous with user identities, a point of significant interest to investigators. This paper presents several experiments designed to create a novel NFAT approach that can identify users and understand how they are using network based applications whilst the traffic remains encrypted. The experiments build upon the prior art and investigate how effective this approach is in classifying users and their actions. Utilising an in-house dataset composed of 50 million packers, the experiments are formed of three incremental developments that assist in improving performance. Building upon the successful experiments, a proposed NFAT interface is presented to illustrate the ease at which investigators would be able to ask relevant questions of user interactions. The experiments profiled across 27 users, has yielded an average 93.3% True Positive Identification Rate (TPIR), with 41% of users experiencing 100% TPIR. Skype, Wikipedia and Hotmail services achieved a notably high level of recognition performance. The study has developed and evaluated an approach to analyse encrypted network traffic more effectively through the modelling of network traffic and to subsequently visualise these interactions through a novel network forensic analysis tool. 

**Abstract (ZH)**: 现今电子犯罪日益增多的背景下，网络取证在数字调查中扮演着重要角色。它有助于明确需要分析的系统，并作为补充，支持通过传统计算机调查发现的证据。然而，现有的网络取证分析工具（NFATs）在提供可用数据方面不如文件系统取证分析工具（FS FATs）有效。分析倾向于关注IP地址，而这些IP地址并不等同于用户身份，这是调查人员特别关注的点。本文提出了若干实验，旨在创建一种新型的NFAT方法，能够在流量仍处于加密状态时识别用户并理解他们如何使用基于网络的应用程序。这些实验基于先前的研究，并探讨了该方法在分类用户及其行为方面的有效性。利用包含5000万个数据包的内部数据集，实验分为三个逐步发展的阶段，以提高性能。基于成功的实验结果，提出了一个建议的NFAT界面，展示了调查人员如何轻松提出与用户交互相关的问题。在27名用户的研究中，平均实现了93.3%的真实阳性识别率（TPIR），其中41%的用户实现了100%的TPIR。Skype、Wikipedia和Hotmail服务实现了显著高的识别性能。本研究开发并评估了一种通过建模网络流量并随后通过新型网络取证分析工具可视化这些交互，来更有效地分析加密网络流量的方法。 

---
# UniPCGC: Towards Practical Point Cloud Geometry Compression via an Efficient Unified Approach 

**Title (ZH)**: UniPCGC：通过一种高效的统一方法面向实用的点云几何压缩 

**Authors**: Kangli Wang, Wei Gao  

**Link**: [PDF](https://arxiv.org/pdf/2503.18541)  

**Abstract**: Learning-based point cloud compression methods have made significant progress in terms of performance. However, these methods still encounter challenges including high complexity, limited compression modes, and a lack of support for variable rate, which restrict the practical application of these methods. In order to promote the development of practical point cloud compression, we propose an efficient unified point cloud geometry compression framework, dubbed as UniPCGC. It is a lightweight framework that supports lossy compression, lossless compression, variable rate and variable complexity. First, we introduce the Uneven 8-Stage Lossless Coder (UELC) in the lossless mode, which allocates more computational complexity to groups with higher coding difficulty, and merges groups with lower coding difficulty. Second, Variable Rate and Complexity Module (VRCM) is achieved in the lossy mode through joint adoption of a rate modulation module and dynamic sparse convolution. Finally, through the dynamic combination of UELC and VRCM, we achieve lossy compression, lossless compression, variable rate and complexity within a unified framework. Compared to the previous state-of-the-art method, our method achieves a compression ratio (CR) gain of 8.1\% on lossless compression, and a Bjontegaard Delta Rate (BD-Rate) gain of 14.02\% on lossy compression, while also supporting variable rate and variable complexity. 

**Abstract (ZH)**: 基于学习的点云压缩方法在性能上取得了显著进步，但仍面临高复杂度、压缩模式有限和不支持可变比特率等挑战，限制了这些方法的实用应用。为促进实用点云压缩的发展，我们提出了一种高效的统一点云几何压缩框架，命名为UniPCGC。该框架支持有损压缩、无损压缩、可变比特率和可变复杂度。首先，在无损模式下引入了非均匀8级无损编码器(UELC)，将更多的计算复杂性分配给编码难度较高的组，并合并编码难度较低的组。其次，无损模式和有损模式通过联合采用速率调制模块和动态稀疏卷积实现了可变比特率和复杂度模块(VRCM)。最后，通过UELC和VRCM的动态组合，我们在统一框架中实现了有损压缩、无损压缩、可变比特率和可变复杂度。与先前的最先进方法相比，在无损压缩中我们的方法获得了8.1%的压缩比(CR)增益，在有损压缩中获得了14.02%的Bjontegaard Delta Rate(BD-Rate)增益，同时支持可变比特率和可变复杂度。 

---
# HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for Building-Level Remote Sensing Applications 

**Title (ZH)**: HiRes-FusedMIM：一种用于建筑级遥感应用的高分辨率RGB-DSM预训练模型 

**Authors**: Guneet Mutreja, Philipp Schuegraf, Ksenia Bittner  

**Link**: [PDF](https://arxiv.org/pdf/2503.18540)  

**Abstract**: Recent advances in self-supervised learning have led to the development of foundation models that have significantly advanced performance in various computer vision tasks. However, despite their potential, these models often overlook the crucial role of high-resolution digital surface models (DSMs) in understanding urban environments, particularly for building-level analysis, which is essential for applications like digital twins. To address this gap, we introduce HiRes-FusedMIM, a novel pre-trained model specifically designed to leverage the rich information contained within high-resolution RGB and DSM data. HiRes-FusedMIM utilizes a dual-encoder simple masked image modeling (SimMIM) architecture with a multi-objective loss function that combines reconstruction and contrastive objectives, enabling it to learn powerful, joint representations from both modalities. We conducted a comprehensive evaluation of HiRes-FusedMIM on a diverse set of downstream tasks, including classification, semantic segmentation, and instance segmentation. Our results demonstrate that: 1) HiRes-FusedMIM outperforms previous state-of-the-art geospatial methods on several building-related datasets, including WHU Aerial and LoveDA, demonstrating its effectiveness in capturing and leveraging fine-grained building information; 2) Incorporating DSMs during pre-training consistently improves performance compared to using RGB data alone, highlighting the value of elevation information for building-level analysis; 3) The dual-encoder architecture of HiRes-FusedMIM, with separate encoders for RGB and DSM data, significantly outperforms a single-encoder model on the Vaihingen segmentation task, indicating the benefits of learning specialized representations for each modality. To facilitate further research and applications in this direction, we will publicly release the trained model weights. 

**Abstract (ZH)**: Recent Advances in Self-Supervised Learning Have Led to the Development of Foundation Models that Have Significantly Advanced Performance in Various Computer Vision Tasks: Addressing the Overlooked Role of High-Resolution Digital Surface Models in Urban Analysis with HiRes-FusedMIM 

---
# Natural Language Processing for Electronic Health Records in Scandinavian Languages: Norwegian, Swedish, and Danish 

**Title (ZH)**: Scandinavian语言的电子健康记录自然语言处理：挪威语、瑞典语和丹麦语 

**Authors**: Ashenafi Zebene Woldaregay, Jørgen Aarmo Lund, Phuong Dinh Ngo, Mariyam Tayefi, Joel Burman, Stine Hansen, Martin Hylleholt Sillesen, Hercules Dalianis, Robert Jenssen, Lindsetmo Rolf Ole, Karl Øyvind Mikalsen  

**Link**: [PDF](https://arxiv.org/pdf/2503.18539)  

**Abstract**: Background: Clinical natural language processing (NLP) refers to the use of computational methods for extracting, processing, and analyzing unstructured clinical text data, and holds a huge potential to transform healthcare in various clinical tasks. Objective: The study aims to perform a systematic review to comprehensively assess and analyze the state-of-the-art NLP methods for the mainland Scandinavian clinical text. Method: A literature search was conducted in various online databases including PubMed, ScienceDirect, Google Scholar, ACM digital library, and IEEE Xplore between December 2022 and February 2024. Further, relevant references to the included articles were also used to solidify our search. The final pool includes articles that conducted clinical NLP in the mainland Scandinavian languages and were published in English between 2010 and 2024. Results: Out of the 113 articles, 18% (n=21) focus on Norwegian clinical text, 64% (n=72) on Swedish, 10% (n=11) on Danish, and 8% (n=9) focus on more than one language. Generally, the review identified positive developments across the region despite some observable gaps and disparities between the languages. There are substantial disparities in the level of adoption of transformer-based models. In essential tasks such as de-identification, there is significantly less research activity focusing on Norwegian and Danish compared to Swedish text. Further, the review identified a low level of sharing resources such as data, experimentation code, pre-trained models, and rate of adaptation and transfer learning in the region. Conclusion: The review presented a comprehensive assessment of the state-of-the-art Clinical NLP for electronic health records (EHR) text in mainland Scandinavian languages and, highlighted the potential barriers and challenges that hinder the rapid advancement of the field in the region. 

**Abstract (ZH)**: 背景：临床自然语言处理（NLP）指的是通过计算方法提取、处理和分析未结构化临床文本数据，并在各种临床任务中具有巨大的潜力，以变革医疗保健。目标：本研究旨在进行系统评价，全面评估和分析适用于中国大陆斯堪的纳维亚临床文本的最新NLP方法。方法：于2022年12月至2024年2月期间，在PubMed、ScienceDirect、Google Scholar、ACM数字图书馆和IEEE Xplore等多个在线数据库中进行文献检索，并利用相关参考文献以加强搜索。最终池包括2010年至2024年间以英语发表的适用于中国大陆斯堪的纳维亚语言的临床NLP文章。结果：在113篇文章中，21%（n=21）专注于挪威临床文本，72%（n=72）专注于瑞典，11%（n=11）专注于丹麦，9%（n=9）同时涉及多种语言。总体而言，地区内发现了一些积极的发展趋势，尽管不同语言之间存在可观察到的差距。在采用基于变换器的模型方面，存在显著的差异。在去识别等关键任务中，对挪威和丹麦文本的研究活动显著少于瑞典文本。此外，地区内分享数据、实验代码、预训练模型以及适应和迁移学习的速度较低。结论：本研究全面评估了中国大陆斯堪的纳维亚语言电子健康记录（EHR）文本的最新临床NLP状况，并强调了阻碍该地区领域快速发展的潜在障碍和挑战。 

---
# SciClaims: An End-to-End Generative System for Biomedical Claim Analysis 

**Title (ZH)**: SciClaims: 一端到一端的生成系统用于生物医学断言分析 

**Authors**: Raúl Ortega, José Manuel Gómez-Pérez  

**Link**: [PDF](https://arxiv.org/pdf/2503.18526)  

**Abstract**: Validating key claims in scientific literature, particularly in biomedical research, is essential for ensuring accuracy and advancing knowledge. This process is critical in sectors like the pharmaceutical industry, where rapid scientific progress requires automation and deep domain expertise. However, current solutions have significant limitations. They lack end-to-end pipelines encompassing all claim extraction, evidence retrieval, and verification steps; rely on complex NLP and information retrieval pipelines prone to multiple failure points; and often fail to provide clear, user-friendly justifications for claim verification outcomes. To address these challenges, we introduce SciClaims, an advanced system powered by state-of-the-art large language models (LLMs) that seamlessly integrates the entire scientific claim analysis process. SciClaims outperforms previous approaches in both claim extraction and verification without requiring additional fine-tuning, setting a new benchmark for automated scientific claim analysis. 

**Abstract (ZH)**: 验证科学文献特别是生物医药研究中的关键声明对于确保准确性和促进知识发展至关重要。这一过程对于制药等行业而言尤其关键，因为该行业需要快速的科学进步和自动化以及深厚的专业知识。然而，当前的解决方案存在显著的局限性。它们缺乏涵盖所有声明提取、证据检索和验证步骤的端到端管道；依赖于复杂自然语言处理和信息检索管道，存在多个失败点；并且通常无法为声明验证结果提供清晰且用户友好的解释。为应对这些挑战，我们引入了SciClaims，这是一种基于先进大语言模型（LLMs）的高级系统，能够无缝整合整个科学声明分析过程。SciClaims在声明提取和验证方面均优于先前的方法，无需额外的微调，并为自动化科学声明分析设立了新基准。 

---
# Statistically Testing Training Data for Unwanted Error Patterns using Rule-Oriented Regression 

**Title (ZH)**: 基于规则定向回归的训练数据统计测试以检测不需要的错误模式 

**Authors**: Stefan Rass, Martin Dallinger  

**Link**: [PDF](https://arxiv.org/pdf/2503.18497)  

**Abstract**: Artificial intelligence models trained from data can only be as good as the underlying data is. Biases in training data propagating through to the output of a machine learning model are a well-documented and well-understood phenomenon, but the machinery to prevent these undesired effects is much less developed. Efforts to ensure data is clean during collection, such as using bias-aware sampling, are most effective when the entity controlling data collection also trains the AI. In cases where the data is already available, how do we find out if the data was already manipulated, i.e., ``poisoned'', so that an undesired behavior would be trained into a machine learning model? This is a challenge fundamentally different to (just) improving approximation accuracy or efficiency, and we provide a method to test training data for flaws, to establish a trustworthy ground-truth for a subsequent training of machine learning models (of any kind). Unlike the well-studied problem of approximating data using fuzzy rules that are generated from the data, our method hinges on a prior definition of rules to happen before seeing the data to be tested. Therefore, the proposed method can also discover hidden error patterns, which may also have substantial influence. Our approach extends the abilities of conventional statistical testing by letting the ``test-condition'' be any Boolean condition to describe a pattern in the data, whose presence we wish to determine. The method puts fuzzy inference into a regression model, to get the best of the two: explainability from fuzzy logic with statistical properties and diagnostics from the regression, and finally also being applicable to ``small data'', hence not requiring large datasets as deep learning methods do. We provide an open source implementation for demonstration and experiments. 

**Abstract (ZH)**: 人工训练的智能模型的质量取决于底层数据的质量。训练数据中的偏差通过机器学习模型输出传递是一种已文档化且被广泛理解的现象，但预防这些不良影响的机制还未充分发展。在数据收集过程中采用偏差感知采样的努力在数据控制实体也训练AI的情况下最为有效。当数据已经准备好时，如何检测数据是否已被操纵，即“污染”，以确定是否会将不良行为训练进机器学习模型中？这是一个与仅仅提高逼近精度或效率不同的根本性挑战。我们提供了一种方法来测试训练数据是否存在缺陷，以建立一个可信赖的地面真实值，用于随后的机器学习模型训练（任何类型）。与从数据生成模糊规则进行数据逼近的已研究问题不同，我们方法依赖于在看到测试数据之前先定义规则。因此，提出的方法还可以发现隐藏的错误模式，这些模式也可能具有重要影响。我们的方法扩展了传统统计测试的能力，使“测试条件”可以是任何布尔条件来描述数据中的模式，我们希望确定其存在性。该方法将模糊推理与回归模型结合，结合了模糊逻辑的解释性与统计特性和诊断功能，并且适用于“小数据”，因此无需像深度学习方法那样依赖大量数据集。我们提供了一个开源实现用于演示和实验。 

---
# Safeguarding Mobile GUI Agent via Logic-based Action Verification 

**Title (ZH)**: 基于逻辑的行动验证保障移动GUI代理安全 

**Authors**: Jungjae Lee, Dongjae Lee, Chihun Choi, Youngmin Im, Jaeyoung Wi, Kihong Heo, Sangeun Oh, Sunjae Lee, Insik Shin  

**Link**: [PDF](https://arxiv.org/pdf/2503.18492)  

**Abstract**: Large Foundation Models (LFMs) have unlocked new possibilities in human-computer interaction, particularly with the rise of mobile Graphical User Interface (GUI) Agents capable of interpreting GUIs. These agents promise to revolutionize mobile computing by allowing users to automate complex mobile tasks through simple natural language instructions. However, the inherent probabilistic nature of LFMs, coupled with the ambiguity and context-dependence of mobile tasks, makes LFM-based automation unreliable and prone to errors. To address this critical challenge, we introduce VeriSafe Agent (VSA): a formal verification system that serves as a logically grounded safeguard for Mobile GUI Agents. VSA is designed to deterministically ensure that an agent's actions strictly align with user intent before conducting an action. At its core, VSA introduces a novel autoformalization technique that translates natural language user instructions into a formally verifiable specification, expressed in our domain-specific language (DSL). This enables runtime, rule-based verification, allowing VSA to detect and prevent erroneous actions executing an action, either by providing corrective feedback or halting unsafe behavior. To the best of our knowledge, VSA is the first attempt to bring the rigor of formal verification to GUI agent. effectively bridging the gap between LFM-driven automation and formal software verification. We implement VSA using off-the-shelf LLM services (GPT-4o) and evaluate its performance on 300 user instructions across 18 widely used mobile apps. The results demonstrate that VSA achieves 94.3%-98.33% accuracy in verifying agent actions, representing a significant 20.4%-25.6% improvement over existing LLM-based verification methods, and consequently increases the GUI agent's task completion rate by 90%-130%. 

**Abstract (ZH)**: 大型基础模型（LFMs）通过移动图形用户界面（GUI）代理的兴起解锁了新的可能，这些代理能够解释GUI。这些代理有望通过简单的自然语言指令自动化复杂的移动任务，从而 revolutionize 移动计算。然而，LFMs 内在的概率性质，加上移动任务的模糊性和依赖上下文的特性，使得基于LFM的自动化不可靠且容易出错。为应对这一关键挑战，我们提出了VeriSafe Agent（VSA）：一种作为逻辑基础护盾的正式验证系统，用于移动GUI代理。VSA 设计旨在在执行操作前严格确保代理行为与用户意图的一致性。VSA 内核引入了一种新颖的自动形式化技术，将自然语言用户指令转换为可以在我们领域特定语言（DSL）中形式验证的规范。这使 VSA 能够在运行时、基于规则进行验证，在执行操作时检测并防止错误行为，通过提供纠正反馈或阻止不安全行为。据我们所知，VSA 是首次尝试将形式验证的严谨性带入 GUI 代理，有效地弥合了基于LFM 的自动化与形式软件验证之间的差距。我们使用现成的LLM 服务（GPT-4o）实现VSA，并在18个广泛使用的移动应用程序的300个用户指令上评估其性能。结果表明，VSA 在验证代理行为方面的准确性为94.3%-98.33%，比现有基于LLM的验证方法提高了20.4%-25.6%，从而将GUI代理的任务完成率提高了90%-130%。 

---
# Large Language Models powered Network Attack Detection: Architecture, Opportunities and Case Study 

**Title (ZH)**: 大型语言模型驱动的网络攻击检测：架构、机遇与案例研究 

**Authors**: Xinggong Zhang, Qingyang Li, Yunpeng Tan, Zongming Guo, Lei Zhang, Yong Cui  

**Link**: [PDF](https://arxiv.org/pdf/2503.18487)  

**Abstract**: Network attack detection is a pivotal technology to identify network anomaly and classify malicious traffic. Large Language Models (LLMs) are trained on a vast corpus of text, have amassed remarkable capabilities of context-understanding and commonsense knowledge. This has opened up a new door for network threat detection. Researchers have already initiated discussions regarding the application of LLMs on specific cyber-security tasks. Unfortunately, there is still a lack of comprehensive elaboration how to mine LLMs' potentials in network threat detections, as well as the opportunities and challenges. In this paper, we mainly focus on the classification of malicious traffic from the perspective of LLMs' capability. We present a holistic view of the architecture of LLM-powered network attack detection, including Pre-training, Fine-tuning, and Detection. Especially, by exploring the knowledge and capabilities of LLM, we identify three distinct roles LLM can act in network attack detection: \textit{Classifier, Encoder, and Predictor}. For each of them, the modeling paradigm, opportunities and challenges are elaborated. Finally, we present our design on LLM-powered DDoS detection as a case study. The proposed framework attains accurate detection on carpet bombing DDoS by exploiting LLMs' capabilities in contextual mining. The evaluation shows its efficacy, exhibiting a nearly $35$\% improvement compared to existing systems. 

**Abstract (ZH)**: 基于大型语言模型的网络攻击检测中的恶意流量分类 

---
# Words as Bridges: Exploring Computational Support for Cross-Disciplinary Translation Work 

**Title (ZH)**: 词语作为桥梁：探索跨学科翻译工作中的计算支持 

**Authors**: Calvin Bao, Yow-Ting Shiue, Marine Carpuat, Joel Chan  

**Link**: [PDF](https://arxiv.org/pdf/2503.18471)  

**Abstract**: Scholars often explore literature outside of their home community of study. This exploration process is frequently hampered by field-specific jargon. Past computational work often focuses on supporting translation work by removing jargon through simplification and summarization; here, we explore a different approach that preserves jargon as useful bridges to new conceptual spaces. Specifically, we cast different scholarly domains as different language-using communities, and explore how to adapt techniques from unsupervised cross-lingual alignment of word embeddings to explore conceptual alignments between domain-specific word embedding this http URL developed a prototype cross-domain search engine that uses aligned domain-specific embeddings to support conceptual exploration, and tested this prototype in two case studies. We discuss qualitative insights into the promises and pitfalls of this approach to translation work, and suggest design insights for future interfaces that provide computational support for cross-domain information seeking. 

**Abstract (ZH)**: 学者们常探索其研究社群之外的文献。这一探索过程经常受到领域特定术语的阻碍。以往的计算工作往往侧重于通过简化和总结去除术语以支持翻译工作；相比之下，我们探索了一种不同的方法，该方法保留术语作为连接新概念空间的有用桥梁。具体而言，我们将不同的学术领域视为不同的语言使用社区，并探索如何将无监督跨境词嵌入对齐的技术应用于领域特定词嵌入的概念对齐。我们开发了一个原型跨域搜索引擎，该引擎使用对齐的领域特定嵌入来支持概念探索，并在两个案例研究中测试了该原型。我们讨论了这种方法在翻译工作中的潜力与局限性的定性见解，并提出了为跨域信息检索提供计算支持的未来界面的设计建议。 

---
# MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse 

**Title (ZH)**: MetaSpatial：增强元宇宙中VLMs的三维空间推理能力 

**Authors**: Zhenyu Pan, Han Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.18470)  

**Abstract**: We present MetaSpatial, the first reinforcement learning (RL)-based framework designed to enhance 3D spatial reasoning in vision-language models (VLMs), enabling real-time 3D scene generation without the need for hard-coded optimizations. MetaSpatial addresses two core challenges: (i) the lack of internalized 3D spatial reasoning in VLMs, which limits their ability to generate realistic layouts, and (ii) the inefficiency of traditional supervised fine-tuning (SFT) for layout generation tasks, as perfect ground truth annotations are unavailable. Our key innovation is a multi-turn RL-based optimization mechanism that integrates physics-aware constraints and rendered image evaluations, ensuring generated 3D layouts are coherent, physically plausible, and aesthetically consistent. Methodologically, MetaSpatial introduces an adaptive, iterative reasoning process, where the VLM refines spatial arrangements over multiple turns by analyzing rendered outputs, improving scene coherence progressively. Empirical evaluations demonstrate that MetaSpatial significantly enhances the spatial consistency and formatting stability of various scale models. Post-training, object placements are more realistic, aligned, and functionally coherent, validating the effectiveness of RL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game development applications. Our code, data, and training pipeline are publicly available at this https URL. 

**Abstract (ZH)**: MetaSpatial：一种基于强化学习的增强视觉语言模型三维空间推理的框架 

---
# PALATE: Peculiar Application of the Law of Total Expectation to Enhance the Evaluation of Deep Generative Models 

**Title (ZH)**: PALATE：独特应用全面期望定律以增强深度生成模型的评估 

**Authors**: Tadeusz Dziarmaga, Marcin Kądziołka, Artur Kasymov, Marcin Mazur  

**Link**: [PDF](https://arxiv.org/pdf/2503.18462)  

**Abstract**: Deep generative models (DGMs) have caused a paradigm shift in the field of machine learning, yielding noteworthy advancements in domains such as image synthesis, natural language processing, and other related areas. However, a comprehensive evaluation of these models that accounts for the trichotomy between fidelity, diversity, and novelty in generated samples remains a formidable challenge. A recently introduced solution that has emerged as a promising approach in this regard is the Feature Likelihood Divergence (FLD), a method that offers a theoretically motivated practical tool, yet also exhibits some computational challenges. In this paper, we propose PALATE, a novel enhancement to the evaluation of DGMs that addresses limitations of existing metrics. Our approach is based on a peculiar application of the law of total expectation to random variables representing accessible real data. When combined with the MMD baseline metric and DINOv2 feature extractor, PALATE offers a holistic evaluation framework that matches or surpasses state-of-the-art solutions while providing superior computational efficiency and scalability to large-scale datasets. Through a series of experiments, we demonstrate the effectiveness of the PALATE enhancement, contributing a computationally efficient, holistic evaluation approach that advances the field of DGMs assessment, especially in detecting sample memorization and evaluating generalization capabilities. 

**Abstract (ZH)**: 深度生成模型（DGMs）在机器学习领域引发了范式转变，取得了在图像合成、自然语言处理及其它相关领域的显著进展。然而，全面评估这些模型，考虑到生成样本在忠实性、多样性和新颖性之间的平衡，仍是一项艰巨的挑战。最近提出的一种潜在有前景的解决方案是特征似然散度（FLD），它提供了一种理论依据的实际工具，但也存在一定的计算挑战。在本文中，我们提出了PALATE，一种针对DGMs评估的新颖增强方法，以解决现有评估指标的局限性。我们的方法基于对随机变量表示可访问真实数据的总期望定律的特殊应用。结合MMD基准度量和DINOv2特征提取器，PALATE提供了一种综合评估框架，匹配甚至超越现有先进解决方案，同时具备更高的计算效率和面向大规模数据集的可扩展性。通过一系列实验，我们展示了PALATE增强的有效性，提出了一种计算效率高、综合的评估方法，推动了DGMs评估领域的发展，特别是在检测样本记忆和评估泛化能力方面。 

---
# ModiGen: A Large Language Model-Based Workflow for Multi-Task Modelica Code Generation 

**Title (ZH)**: ModiGen：基于大语言模型的多任务Modelica代码生成工作流 

**Authors**: Jiahui Xiang, Tong Ye, Peiyu Liu, Yinan Zhang, Wenhai Wang  

**Link**: [PDF](https://arxiv.org/pdf/2503.18460)  

**Abstract**: Modelica is a widely adopted language for simulating complex physical systems, yet effective model creation and optimization require substantial domain expertise. Although large language models (LLMs) have demonstrated promising capabilities in code generation, their application to modeling remains largely unexplored. To address this gap, we have developed benchmark datasets specifically designed to evaluate the performance of LLMs in generating Modelica component models and test cases. Our evaluation reveals substantial limitations in current LLMs, as the generated code often fails to simulate successfully. To overcome these challenges, we propose a specialized workflow that integrates supervised fine-tuning, graph retrieval-augmented generation, and feedback optimization to improve the accuracy and reliability of Modelica code generation. The evaluation results demonstrate significant performance gains: the maximum improvement in pass@1 reached 0.3349 for the component generation task and 0.2457 for the test case generation task. This research underscores the potential of LLMs to advance intelligent modeling tools and offers valuable insights for future developments in system modeling and engineering applications. 

**Abstract (ZH)**: Modelica是一个广泛采用的用于模拟复杂物理系统的语言，但有效的模型创建和优化需要大量的专业领域知识。尽管大规模语言模型（LLMs）展示了代码生成方面的有前景的能力，但它们在建模中的应用仍被广泛探索。为解决这一差距，我们开发了专门设计的基准数据集，以评估LLMs在生成Modelica组件模型和测试案例方面的性能。我们的评估结果显示了当前LLMs存在的重大限制，生成的代码往往无法成功模拟。为克服这些挑战，我们提出了一种专门的工作流，该工作流结合了监督微调、图检索增强生成和反馈优化，以提高Modelica代码生成的准确性和可靠性。评估结果表明取得了显著的性能提升：组件生成任务的最高改进率达到0.3349，测试案例生成任务的最高改进率达到0.2457。这项研究强调了LLMs在推动智能建模工具方面的潜力，并为系统建模和工程应用的未来发展提供了宝贵的见解。 

---
# Teaching LLMs for Step-Level Automatic Math Correction via Reinforcement Learning 

**Title (ZH)**: 基于强化学习的步骤级自动数学纠错LLM教学 

**Authors**: Junsong Li, Jie Zhou, Yutao Yang, Bihao Zhan, Qianjun Pan, Yuyang Ding, Qin Chen, Jiang Bo, Xin Lin, Liang He  

**Link**: [PDF](https://arxiv.org/pdf/2503.18432)  

**Abstract**: Automatic math correction aims to check students' solutions to mathematical problems via artificial intelligence technologies. Most existing studies focus on judging the final answer at the problem level, while they ignore detailed feedback on each step in a math problem-solving process, which requires abilities of semantic understanding and reasoning. In this paper, we propose a reinforcement learning (RL)-based method to boost large language model (LLM) for step-level automatic math correction, named StepAMC. Particularly, we convert the step-level automatic math correction within the text classification task into an RL problem to enhance the reasoning capabilities of LLMs. Then, we design a space-constrained policy network to improve the stability of RL. Then, we introduce a fine-grained reward network to convert the binary human feedback into a continuous value. We conduct extensive experiments over two benchmark datasets and the results show that our model outperforms the eleven strong baselines. 

**Abstract (ZH)**: 基于强化学习的大语言模型步骤级自动数学纠错方法：StepAMC 

---
# Generative AI in Knowledge Work: Design Implications for Data Navigation and Decision-Making 

**Title (ZH)**: 生成式AI在知识工作中的设计影响：数据导航与决策制定 

**Authors**: Bhada Yun, Dana Feng, Ace S. Chen, Afshin Nikzad, Niloufar Salehi  

**Link**: [PDF](https://arxiv.org/pdf/2503.18419)  

**Abstract**: Our study of 20 knowledge workers revealed a common challenge: the difficulty of synthesizing unstructured information scattered across multiple platforms to make informed decisions. Drawing on their vision of an ideal knowledge synthesis tool, we developed Yodeai, an AI-enabled system, to explore both the opportunities and limitations of AI in knowledge work. Through a user study with 16 product managers, we identified three key requirements for Generative AI in knowledge work: adaptable user control, transparent collaboration mechanisms, and the ability to integrate background knowledge with external information. However, we also found significant limitations, including overreliance on AI, user isolation, and contextual factors outside the AI's reach. As AI tools become increasingly prevalent in professional settings, we propose design principles that emphasize adaptability to diverse workflows, accountability in personal and collaborative contexts, and context-aware interoperability to guide the development of human-centered AI systems for product managers and knowledge workers. 

**Abstract (ZH)**: 我们的研究发现，20名知识工作者面临一个共同挑战：在多个平台上将分散的非结构化信息综合起来以做出知情决策的难度。基于他们对理想的知识综合工具的构想，我们开发了Yodeai这一AI赋能系统，以探索AI在知识工作中的机遇与局限。通过16名产品管理者的用户研究，我们确定了生成式AI在知识工作中的三项关键要求：可适应的用户控制、透明的协作机制以及将背景知识与外部信息整合的能力。然而，我们也发现了显著的局限性，包括对AI的过度依赖、用户孤立以及AI无法触及的上下文因素。随着AI工具在专业环境中的普及，我们提出了侧重于适应多样工作流程、个人和协作情境中的问责制以及上下文感知的互操作性的设计原则，以指导面向产品管理人员和知识工作者的人本AI系统开发。 

---
# Knowledge Graph Enhanced Generative Multi-modal Models for Class-Incremental Learning 

**Title (ZH)**: 知识图谱增强的生成多模态模型在类别增量学习中的应用 

**Authors**: Xusheng Cao, Haori Lu, Linlan Huang, Fei Yang, Xialei Liu, Ming-Ming Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2503.18403)  

**Abstract**: Continual learning in computer vision faces the critical challenge of catastrophic forgetting, where models struggle to retain prior knowledge while adapting to new tasks. Although recent studies have attempted to leverage the generalization capabilities of pre-trained models to mitigate overfitting on current tasks, models still tend to forget details of previously learned categories as tasks progress, leading to misclassification. To address these limitations, we introduce a novel Knowledge Graph Enhanced Generative Multi-modal model (KG-GMM) that builds an evolving knowledge graph throughout the learning process. Our approach utilizes relationships within the knowledge graph to augment the class labels and assigns different relations to similar categories to enhance model differentiation. During testing, we propose a Knowledge Graph Augmented Inference method that locates specific categories by analyzing relationships within the generated text, thereby reducing the loss of detailed information about old classes when learning new knowledge and alleviating forgetting. Experiments demonstrate that our method effectively leverages relational information to help the model correct mispredictions, achieving state-of-the-art results in both conventional CIL and few-shot CIL settings, confirming the efficacy of knowledge graphs at preserving knowledge in the continual learning scenarios. 

**Abstract (ZH)**: 计算机视觉中的持续学习面临着灾难性遗忘的关键挑战，即模型在适应新任务时难以保留先前的知识。尽管最近的研究试图利用预训练模型的泛化能力来减轻当前任务上的过拟合，但模型在任务进展过程中仍然倾向于忘记之前学习类别的细节，导致误分类。为了解决这些问题，我们提出了一种新型的知识图谱增强生成多模态模型（KG-GMM），在学习过程中构建不断演化的知识图谱。我们的方法利用知识图谱中的关系来增强类别标签，并为类似类分配不同的关系以增强模型的区分能力。在测试过程中，我们提出了一种知识图谱增强推断方法，通过分析生成文本中的关系来定位特定类别，从而在学习新知识时减少旧类别的详细信息丢失，并缓解遗忘。实验结果表明，我们的方法有效地利用了关系信息来帮助模型纠正误预测，在传统的持续学习（CIL）和少样本持续学习（few-shot CIL）设置中均取得了最先进的结果，证实了知识图谱在持续学习场景中保留知识的有效性。 

---
# PRECTR: A Synergistic Framework for Integrating Personalized Search Relevance Matching and CTR Prediction 

**Title (ZH)**: PRECTR：一种结合个性化搜索相关性匹配和点击率预测的协同框架 

**Authors**: Rong Chen, Shuzhi Cao, Ailong He, Shuguang Han, Jufeng Chen  

**Link**: [PDF](https://arxiv.org/pdf/2503.18395)  

**Abstract**: The two primary tasks in the search recommendation system are search relevance matching and click-through rate (CTR) prediction -- the former focuses on seeking relevant items for user queries whereas the latter forecasts which item may better match user interest. Prior research typically develops two models to predict the CTR and search relevance separately, then ranking candidate items based on the fusion of the two outputs. However, such a divide-and-conquer paradigm creates the inconsistency between different models. Meanwhile, the search relevance model mainly concentrates on the degree of objective text matching while neglecting personalized differences among different users, leading to restricted model performance. To tackle these issues, we propose a unified \textbf{P}ersonalized Search RElevance Matching and CTR Prediction Fusion Model(PRECTR). Specifically, based on the conditional probability fusion mechanism, PRECTR integrates the CTR prediction and search relevance matching into one framework to enhance the interaction and consistency of the two modules. However, directly optimizing CTR binary classification loss may bring challenges to the fusion model's convergence and indefinitely promote the exposure of items with high CTR, regardless of their search relevance. Hence, we further introduce two-stage training and semantic consistency regularization to accelerate the model's convergence and restrain the recommendation of irrelevant items. Finally, acknowledging that different users may have varied relevance preferences, we assessed current users' relevance preferences by analyzing past users' preferences for similar queries and tailored incentives for different candidate items accordingly. Extensive experimental results on our production dataset and online A/B testing demonstrate the effectiveness and superiority of our proposed PRECTR method. 

**Abstract (ZH)**: 一种统一的个性化搜索相关性匹配与点击率预测融合模型(PRECTR) 

---
# Manipulation and the AI Act: Large Language Model Chatbots and the Danger of Mirrors 

**Title (ZH)**: AI法案中的操控与镜像危险：大规模语言模型聊天机器人 

**Authors**: Joshua Krook  

**Link**: [PDF](https://arxiv.org/pdf/2503.18387)  

**Abstract**: Large Language Model chatbots are increasingly taking the form and visage of human beings, adapting human faces, names, voices, personalities, and quirks, including those of celebrities and well-known political figures. Personifying AI chatbots could foreseeably increase their trust with users. However, it could also make them more capable of manipulation, by creating the illusion of a close and intimate relationship with an artificial entity. The European Commission has finalized the AI Act, with the EU Parliament making amendments banning manipulative and deceptive AI systems that cause significant harm to users. Although the AI Act covers harms that accumulate over time, it is unlikely to prevent harms associated with prolonged discussions with AI chatbots. Specifically, a chatbot could reinforce a person's negative emotional state over weeks, months, or years through negative feedback loops, prolonged conversations, or harmful recommendations, contributing to a user's deteriorating mental health. 

**Abstract (ZH)**: 大型语言模型聊天机器人越来越具有人类的形象， adapting human faces, names, voices, personalities, and quirks, including those of celebrities and well-known political figures. 将AI聊天机器人拟人化可能增加用户对其的信任，但也可能使其更具操控性，通过营造与人造实体亲近和亲密的关系幻觉。欧盟委员会已最终确定了AI法案，欧洲议会对法案作出了修正，禁止造成用户重大伤害的具有操控性和欺骗性的AI系统。尽管AI法案涵盖了累积性伤害，但不太可能防止与AI聊天机器人长时间互动造成的伤害。具体而言，聊天机器人可以通过消极反馈循环、长时间对话或有害建议，逐步强化一个人的消极情绪状态，持续数周、数月甚至数年，从而损害用户的心理健康。 

---
# Resource-Efficient Motion Control for Video Generation via Dynamic Mask Guidance 

**Title (ZH)**: 基于动态掩码引导的资源高效运动控制视频生成 

**Authors**: Sicong Feng, Jielong Yang, Li Peng  

**Link**: [PDF](https://arxiv.org/pdf/2503.18386)  

**Abstract**: Recent advances in diffusion models bring new vitality to visual content creation. However, current text-to-video generation models still face significant challenges such as high training costs, substantial data requirements, and difficulties in maintaining consistency between given text and motion of the foreground object. To address these challenges, we propose mask-guided video generation, which can control video generation through mask motion sequences, while requiring limited training data. Our model enhances existing architectures by incorporating foreground masks for precise text-position matching and motion trajectory control. Through mask motion sequences, we guide the video generation process to maintain consistent foreground objects throughout the sequence. Additionally, through a first-frame sharing strategy and autoregressive extension approach, we achieve more stable and longer video generation. Extensive qualitative and quantitative experiments demonstrate that this approach excels in various video generation tasks, such as video editing and generating artistic videos, outperforming previous methods in terms of consistency and quality. Our generated results can be viewed in the supplementary materials. 

**Abstract (ZH)**: 近期扩散模型的发展为视觉内容创作带来了新的活力。然而，当前的文本生成视频模型仍面临着高昂的训练成本、大量的数据需求以及文本与前景物体运动一致性维护的难题。为解决这些挑战，我们提出了掩码引导的视频生成方法，该方法通过掩码运动序列控制视频生成，同时仅需要有限的训练数据。我们的模型通过引入前景掩码增强现有架构，实现精确的文字位置匹配和运动轨迹控制。通过掩码运动序列，我们引导视频生成过程，确保序列中前景物体的一致性。此外，通过首帧共享策略和自回归扩展方法，我们实现了更为稳定和长久的视频生成。广泛的定性和定量实验表明，该方法在视频编辑和生成艺术视频等多种视频生成任务中表现出色，一致性和质量均优于先前方法。生成的结果详见补充材料。 

---
# RoCA: Robust Contrastive One-class Time Series Anomaly Detection with Contaminated Data 

**Title (ZH)**: RoCA: 健 robust 对比单一类时间序列异常检测方法在污染数据中的应用 

**Authors**: Xudong Mou, Rui Wang, Bo Li, Tianyu Wo, Jie Sun, Hui Wang, Xudong Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.18385)  

**Abstract**: The accumulation of time-series signals and the absence of labels make time-series Anomaly Detection (AD) a self-supervised task of deep learning. Methods based on normality assumptions face the following three limitations: (1) A single assumption could hardly characterize the whole normality or lead to some deviation. (2) Some assumptions may go against the principle of AD. (3) Their basic assumption is that the training data is uncontaminated (free of anomalies), which is unrealistic in practice, leading to a decline in robustness. This paper proposes a novel robust approach, RoCA, which is the first to address all of the above three challenges, as far as we are aware. It fuses the separated assumptions of one-class classification and contrastive learning in a single training process to characterize a more complete so-called normality. Additionally, it monitors the training data and computes a carefully designed anomaly score throughout the training process. This score helps identify latent anomalies, which are then used to define the classification boundary, inspired by the concept of outlier exposure. The performance on AIOps datasets improved by 6% compared to when contamination was not considered (COCA). On two large and high-dimensional multivariate datasets, the performance increased by 5% to 10%. RoCA achieves the highest average performance on both univariate and multivariate datasets. The source code is available at this https URL. 

**Abstract (ZH)**: 时间序列异序检测中的时间序列信号积累和无标签问题使得时间序列异序检测（AD）成为一个基于深度学习的自监督任务。基于正态性假设的方法面临以下三个局限：（1）单一假设难以全面描述正态性或导致偏差。（2）某些假设可能违反异序检测的基本原则。（3）其基本假设是训练数据未受污染（不含异常值），这在实际应用中是不现实的，导致鲁棒性下降。本文提出了一种新型鲁棒方法RoCA，据我们所知，这是首款能够同时解决上述所有三个挑战的方法。它在单一训练过程中融合了一类分类和对比学习的分离假设，以更全面地描述所谓的正态性。此外，该方法在整个训练过程中监控训练数据并计算精心设计的异常分数，该分数有助于识别潜在异常值，进而定义分类边界，这一过程借鉴了离群值暴露的概念。在AIOps数据集中，与未考虑污染的情况相比（COCA），性能提高了6%。在两个大型高维多变量数据集中，性能提高了5%到10%。RoCA在单变量和多变量数据集中均实现了最高平均性能。源代码可在以下网址获取。 

---
# PP-FormulaNet: Bridging Accuracy and Efficiency in Advanced Formula Recognition 

**Title (ZH)**: PP-FormulaNet：在高级公式识别中平衡准确性和效率 

**Authors**: Hongen Liu, Cheng Cui, Yuning Du, Yi Liu, Gang Pan  

**Link**: [PDF](https://arxiv.org/pdf/2503.18382)  

**Abstract**: Formula recognition is an important task in document intelligence. It involves converting mathematical expressions from document images into structured symbolic formats that computers can easily work with. LaTeX is the most common format used for this purpose. In this work, we present PP-FormulaNet, a state-of-the-art formula recognition model that excels in both accuracy and efficiency. To meet the diverse needs of applications, we have developed two specialized models: PP-FormulaNet-L, tailored for high-accuracy scenarios, and PP-FormulaNet-S, optimized for high-efficiency contexts. Our extensive evaluations reveal that PP-FormulaNet-L attains accuracy levels that surpass those of prominent models such as UniMERNet by a significant 6%. Conversely, PP-FormulaNet-S operates at speeds that are over 16 times faster. These advancements facilitate seamless integration of PP-FormulaNet into a broad spectrum of document processing environments that involve intricate mathematical formulas. Furthermore, we introduce a Formula Mining System, which is capable of extracting a vast amount of high-quality formula data. This system further enhances the robustness and applicability of our formula recognition model. Code and models are publicly available at PaddleOCR(this https URL) and PaddleX(this https URL). 

**Abstract (ZH)**: 公式识别是文档智能中的一个重要任务。它涉及将文档图像中的数学表达式转换为计算机可以轻松处理的结构化符号格式。LaTeX是用于此目的最常见的格式。在此工作中，我们提出了一种先进公式识别模型PP-FormulaNet，既准确又高效。为满足各种应用场景的需求，我们开发了两种专门模型：PP-FormulaNet-L，适用于高精度场景；以及PP-FormulaNet-S，优化了高效率场景。广泛评估表明，PP-FormulaNet-L在准确率上比UniMERNet等知名模型高出显著的6%；而PP-FormulaNet-S的运行速度快了逾16倍。这些进步使得PP-FormulaNet能够无缝集成到涉及复杂数学公式的广泛文档处理环境中。此外，我们引入了公式挖掘系统，可以提取大量高质量的公式数据，进一步增强了我们的公式识别模型的 robustness 和适用性。相关代码和模型可在PaddleOCR和PaddleX公开访问。 

---
# Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity Allocation for LLMs 

**Title (ZH)**: 最大化冗余剪枝：一种基于原理的层wise稀疏性分配方法 

**Authors**: Chang Gao, Kang Zhao, Jianfei Chen, Liping Jing  

**Link**: [PDF](https://arxiv.org/pdf/2503.18377)  

**Abstract**: Large language models (LLMs) have demonstrated impressive capabilities, but their enormous size poses significant challenges for deployment in real-world applications. To address this issue, researchers have sought to apply network pruning techniques to LLMs. A critical challenge in pruning is allocation the sparsity for each layer. Recent sparsity allocation methods is often based on heuristics or search that can easily lead to suboptimal performance. In this paper, we conducted an extensive investigation into various LLMs and revealed three significant discoveries: (1) the layerwise pruning sensitivity (LPS) of LLMs is highly non-uniform, (2) the choice of pruning metric affects LPS, and (3) the performance of a sparse model is related to the uniformity of its layerwise redundancy level. Based on these observations, we propose that the layerwise sparsity of LLMs should adhere to three principles: \emph{non-uniformity}, \emph{pruning metric dependency}, and \emph{uniform layerwise redundancy level} in the pruned model. To this end, we proposed Maximum Redundancy Pruning (MRP), an iterative pruning algorithm that prunes in the most redundant layers (\emph{i.e.}, those with the highest non-outlier ratio) at each iteration. The achieved layerwise sparsity aligns with the outlined principles. We conducted extensive experiments on publicly available LLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental results validate the effectiveness of MRP, demonstrating its superiority over previous methods. 

**Abstract (ZH)**: 大型语言模型（LLMs）展示了令人印象深刻的能力，但其巨大的规模给实际应用中的部署带来了重大挑战。为了解决这个问题，研究人员寻求应用网络剪枝技术到LLMs中。剪枝中的一个关键挑战是为每一层分配稀疏性。最近的稀疏性分配方法往往基于启发式或搜索，容易导致次优性能。在本文中，我们对各种LLMs进行了广泛的研究，发现了三个重要发现：（1）LLMs的层间剪枝敏感性（LPS）高度不均匀，（2）剪枝度量的选择影响LPS，（3）稀疏模型的性能与其层间冗余水平的均匀性相关。基于这些观察，我们提出LLMs的层间稀疏性应遵循三个原则：非均匀性、剪枝度量依赖性和剪枝后模型中的均匀层间冗余水平。为此，我们提出了最大冗余剪枝（MRP），这是一种迭代剪枝算法，在每次迭代中对最冗余的层（即，非离群值比例最高的层）进行剪枝。实现的层间稀疏性与提出的原理一致。我们在包括LLaMA2和OPT在内的多个公开可用的LLM上，通过各种基准实验对其进行了广泛研究。实验结果验证了MRP的有效性，展示了其在先前方法中的优越性。 

---
# Latent Embedding Adaptation for Human Preference Alignment in Diffusion Planners 

**Title (ZH)**: 潜空间嵌入适应以实现扩散计划中的人类偏好对齐 

**Authors**: Wen Zheng Terence Ng, Jianda Chen, Yuan Xu, Tianwei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2503.18347)  

**Abstract**: This work addresses the challenge of personalizing trajectories generated in automated decision-making systems by introducing a resource-efficient approach that enables rapid adaptation to individual users' preferences. Our method leverages a pretrained conditional diffusion model with Preference Latent Embeddings (PLE), trained on a large, reward-free offline dataset. The PLE serves as a compact representation for capturing specific user preferences. By adapting the pretrained model using our proposed preference inversion method, which directly optimizes the learnable PLE, we achieve superior alignment with human preferences compared to existing solutions like Reinforcement Learning from Human Feedback (RLHF) and Low-Rank Adaptation (LoRA). To better reflect practical applications, we create a benchmark experiment using real human preferences on diverse, high-reward trajectories. 

**Abstract (ZH)**: 本工作通过引入一种资源高效的方法来解决自动决策系统生成轨迹个性化的问题，该方法能够快速适应个体用户的偏好。我们的方法利用一个在大规模无奖励离线数据集上预训练的条件扩散模型，并结合偏好潜在嵌入（PLE）。PLE 作为紧凑表示，用于捕获特定用户偏好。通过使用我们提出的偏好反转方法对预训练模型进行适应，直接优化可学习的 PLE，我们实现了与现有解决方案（如基于人类反馈的强化学习 RLHF 和低秩适应 LoRA）相比更好的人类偏好对齐。为了更好地反映实际应用，我们使用多样且高奖励的轨迹上真实人类偏好的基准实验进行评估。 

---
# Optimizing Influence Campaigns: Nudging under Bounded Confidence 

**Title (ZH)**: 优化影响竞选活动：在有限信任下引导 

**Authors**: Yen-Shao Chen, Tauhid Zaman  

**Link**: [PDF](https://arxiv.org/pdf/2503.18331)  

**Abstract**: Influence campaigns in online social networks are often run by organizations, political parties, and nation states to influence large audiences. These campaigns are employed through the use of agents in the network that share persuasive content. Yet, their impact might be minimal if the audiences remain unswayed, often due to the bounded confidence phenomenon, where only a narrow spectrum of viewpoints can influence them. Here we show that to persuade under bounded confidence, an agent must nudge its targets to gradually shift their opinions. Using a control theory approach, we show how to construct an agent's nudging policy under the bounded confidence opinion dynamics model and also how to select targets for multiple agents in an influence campaign on a social network. Simulations on real Twitter networks show that a multi-agent nudging policy can shift the mean opinion, decrease opinion polarization, or even increase it. We find that our nudging based policies outperform other common techniques that do not consider the bounded confidence effect. Finally, we show how to craft prompts for large language models, such as ChatGPT, to generate text-based content for real nudging policies. This illustrates the practical feasibility of our approach, allowing one to go from mathematical nudging policies to real social media content. 

**Abstract (ZH)**: 有界信心下的在线社交网络影响campaign通过网络代理传播说服性内容，但其影响可能因观众固守观点而有限。我们展示了在有界信心条件下，代理需要逐渐引导其目标改变观点以实现说服。利用控制理论方法，我们展示了如何在有界信心意见动力学模型下构造代理的引导策略，并在社交网络影响campaign中选择多个代理的目标。实测Twitter网络的模拟结果显示，多代理引导策略可以改变平均观点，降低观点极化，甚至增加极化。我们发现基于引导的策略优于未考虑有界信心效应的其他常用技术。最后，我们展示了如何为大型语言模型，如ChatGPT，制定提示生成基于文本的引导策略，这表明我们方法的实际可行性，可以从数学上的引导策略过渡到实际的社交媒体内容。 

---
# Plug-and-Play Interpretable Responsible Text-to-Image Generation via Dual-Space Multi-facet Concept Control 

**Title (ZH)**: 基于双空间多维度概念控制的即插即用可解释负责任文本生成 

**Authors**: Basim Azam, Naveed Akhtar  

**Link**: [PDF](https://arxiv.org/pdf/2503.18324)  

**Abstract**: Ethical issues around text-to-image (T2I) models demand a comprehensive control over the generative content. Existing techniques addressing these issues for responsible T2I models aim for the generated content to be fair and safe (non-violent/explicit). However, these methods remain bounded to handling the facets of responsibility concepts individually, while also lacking in interpretability. Moreover, they often require alteration to the original model, which compromises the model performance. In this work, we propose a unique technique to enable responsible T2I generation by simultaneously accounting for an extensive range of concepts for fair and safe content generation in a scalable manner. The key idea is to distill the target T2I pipeline with an external plug-and-play mechanism that learns an interpretable composite responsible space for the desired concepts, conditioned on the target T2I pipeline. We use knowledge distillation and concept whitening to enable this. At inference, the learned space is utilized to modulate the generative content. A typical T2I pipeline presents two plug-in points for our approach, namely; the text embedding space and the diffusion model latent space. We develop modules for both points and show the effectiveness of our approach with a range of strong results. 

**Abstract (ZH)**: 文本到图像模型中的伦理问题要求对生成内容进行综合控制。现有针对负责任文本到图像模型的技术旨在使生成内容公正和安全（非暴力/无爆露）。然而，这些方法仍然局限于单独处理责任感概念的各个方面，并且缺乏可解释性。此外，它们通常需要对原始模型进行修改，这会损害模型性能。在本工作中，我们提出了一种独特的方法，通过同时考虑公正和安全内容生成的广泛概念范围来实现负责任的文本到图像生成。关键思想是通过一个外部即插即用机制对目标文本到图像流水线进行蒸馏，该机制学习一个针对目标文本到图像流水线的可解释综合责任感空间。我们使用知识蒸馏和概念去偏技术来实现这一点。在推理时，学习到的空间用来调节生成内容。一个典型的文本到图像流水线为我们方法提供了两个插件点，即文本嵌入空间和扩散模型潜在空间。我们为两个点开发了模块，并展示了我们方法的有效性，取得了多种强结果。 

---
# LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty 

**Title (ZH)**: LoTUS: 大规模机器遗忘与不确定性口味 

**Authors**: Christoforos N. Spartalis, Theodoros Semertzidis, Stratis Gavves, Petros Daras  

**Link**: [PDF](https://arxiv.org/pdf/2503.18314)  

**Abstract**: We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the influence of training samples from pre-trained models, avoiding retraining from scratch. LoTUS smooths the prediction probabilities of the model -- up to an information theoretic bound -- mitigating its over-confidence that stems from data memorization. We evaluate LoTUS on the Transformer and ResNet18 models, against eight baseline methods, on five public datasets. Beyond established MU benchmarks, we evaluate unlearning on a large-scale dataset (ImageNet1k) which deters retraining, simulating real-world conditions. Moreover, we introduce the novel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable evaluation under real-world conditions. Experimental results show that LoTUS outperforms state-of-the-art methods in terms of both efficiency and effectiveness. Code: this https URL. 

**Abstract (ZH)**: 我们提出LoTUS，这是一种新颖的机器遗忘方法，能够从预训练模型中消除训练样本的影响，避免从头开始重新训练。LoTUS通过信息论边界平滑模型的预测概率，缓解其由于数据记忆导致的过度自信。我们在Transformer和ResNet18模型上，针对八种基线方法，在五个公开数据集上评估LoTUS。除了现有的机器遗忘基准之外，我们还在一个大规模数据集（ImageNet1k）上评估遗忘效果，该数据集难以重新训练，模拟了实际条件。此外，我们引入了一种新的无重新训练的杰况-舍恩贝格尔散度（RF-JSD）度量标准，以在实际条件下进行评估。实验结果表明，LoTUS在效率和效果上都优于现有方法。代码：这个 https URL。 

---
# DeepFund: Will LLM be Professional at Fund Investment? A Live Arena Perspective 

**Title (ZH)**: DeepFund: 语言模型会在基金投资方面专业化吗？一个实时竞技场视角 

**Authors**: Changlun Li, Yao Shi, Yuyu Luo, Nan Tang  

**Link**: [PDF](https://arxiv.org/pdf/2503.18313)  

**Abstract**: Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, but their effectiveness in financial decision making, particularly in fund investment, remains inadequately evaluated. Current benchmarks primarily assess LLMs understanding of financial documents rather than their ability to manage assets or analyze trading opportunities in dynamic market conditions. A critical limitation in existing evaluation methodologies is the backtesting approach, which suffers from information leakage when LLMs are evaluated on historical data they may have encountered during pretraining. This paper introduces DeepFund, a comprehensive platform for evaluating LLM based trading strategies in a simulated live environment. Our approach implements a multi agent framework where LLMs serve as both analysts and managers, creating a realistic simulation of investment decision making. The platform employs a forward testing methodology that mitigates information leakage by evaluating models on market data released after their training cutoff dates. We provide a web interface that visualizes model performance across different market conditions and investment parameters, enabling detailed comparative analysis. Through DeepFund, we aim to provide a more accurate and fair assessment of LLMs capabilities in fund investment, offering insights into their potential real world applications in financial markets. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各个领域展现了强大的能力，但在金融决策领域，尤其是基金投资中的效果仍然缺乏充分评估。当前的基准主要评估LLMs对金融文档的理解能力，而非其管理资产或在动态市场条件下分析交易机会的能力。现有评估方法的一个关键局限是回测方法，在利用模型在预训练期间可能遇到的历史数据评估模型时存在信息泄露问题。本文介绍了一个全面的DeepFund平台，用于在模拟实时环境中评估基于LLM的交易策略。我们的方法采用多agent框架，其中LLMsboth作为分析师和管理者，创造了一个真实的投资决策模拟环境。该平台采用正向测试方法，通过在模型训练截止日期之后发布的市场数据评估模型，从而减少信息泄露。我们提供了一个网络界面，可视化不同市场条件和投资参数下的模型表现，支持详细的对比分析。通过DeepFund，我们旨在提供对LLMs在基金投资中的能力更准确和公正的评估，并揭示其在金融市场中潜在的真实世界应用。 

---
# How to Capture and Study Conversations Between Research Participants and ChatGPT: GPT for Researchers (g4r.org) 

**Title (ZH)**: 如何捕获和研究研究参与者与ChatGPT之间的对话：GPT for Researchers (g4r.org) 

**Authors**: Jin Kim  

**Link**: [PDF](https://arxiv.org/pdf/2503.18303)  

**Abstract**: As large language models (LLMs) like ChatGPT become increasingly integrated into our everyday lives--from customer service and education to creative work and personal productivity--understanding how people interact with these AI systems has become a pressing issue. Despite the widespread use of LLMs, researchers lack standardized tools for systematically studying people's interactions with LLMs. To address this issue, we introduce GPT for Researchers (G4R), or this http URL, a free website that researchers can use to easily create and integrate a GPT Interface into their studies. At this http URL, researchers can (1) enable their study participants to interact with GPT (such as ChatGPT), (2) customize GPT Interfaces to guide participants' interactions with GPT (e.g., set constraints on topics or adjust GPT's tone or response style), and (3) capture participants' interactions with GPT by downloading data on messages exchanged between participants and GPT. By facilitating study participants' interactions with GPT and providing detailed data on these interactions, G4R can support research on topics such as consumer interactions with AI agents or LLMs, AI-assisted decision-making, and linguistic patterns in human-AI communication. With this goal in mind, we provide a step-by-step guide to using G4R at this http URL. 

**Abstract (ZH)**: 随着像ChatGPT这样的大规模语言模型（LLMs）在客户服务、教育、创造性工作和个人生产力等领域中的日益普及，理解人们如何与这些AI系统交互已成为一个迫切的问题。尽管LLMs得到了广泛应用，研究人员仍缺乏标准化工具来系统研究人们与LLMs的交互。为解决这一问题，我们介绍了GPT for Researchers（G4R），或访问此网址，这是一个免费的网站，研究人员可以使用它来轻松创建并整合GPT界面到他们的研究中。在该网址上，研究人员可以（1）使研究参与者能够与GPT（如ChatGPT）交互，（2）自定义GPT界面以引导参与者与GPT的交互（例如，设置话题限制或调整GPT的语气或响应风格），以及（3）通过下载参与者与GPT之间交换的消息数据来捕获参与者与GPT的交互。通过促进研究参与者与GPT的交互，并提供这些交互的详细数据，G4R可以支持关于消费者与AI代理或LLMs的互动、AI辅助决策以及人类与AI通信中的语言模式等方面的研究。为了实现这一目标，我们在该网址上提供了使用G4R的逐步指南。 

---
# When is dataset cartography ineffective? Using training dynamics does not improve robustness against Adversarial SQuAD 

**Title (ZH)**: 数据集地理测绘在何种情况下无效？使用训练动力学不能提高对抗SQuAD的鲁棒性 

**Authors**: Paul K. Mandal  

**Link**: [PDF](https://arxiv.org/pdf/2503.18290)  

**Abstract**: In this paper, I investigate the effectiveness of dataset cartography for extractive question answering on the SQuAD dataset. I begin by analyzing annotation artifacts in SQuAD and evaluate the impact of two adversarial datasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training dynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn subsets. I then compare the performance of models trained on these subsets to those trained on randomly selected samples of equal size. Results show that training on cartography-based subsets does not improve generalization to the SQuAD validation set or the AddSent adversarial set. While the hard-to-learn subset yields a slightly higher F1 score on the AddOneSent dataset, the overall gains are limited. These findings suggest that dataset cartography provides little benefit for adversarial robustness in SQuAD-style QA tasks. I conclude by comparing these results to prior findings on SNLI and discuss possible reasons for the observed differences. 

**Abstract (ZH)**: 本文探讨了数据集测绘在SQuAD数据集上提取式问答任务中的有效性。通过分析SQuAD的注释特征，并评估AddSent和AddOneSent两个对抗数据集对ELECTRA-small模型的影响，利用训练动力学将SQuAD划分为易学、模糊和难学子集，对比这些子集训练的模型与随机等量样本训练的模型的性能。结果表明，基于数据集测绘划分的子集训练并不能提高对SQuAD验证集或AddSent对抗集的泛化能力。虽然难学子集在AddOneSent数据集上的F1分数略有提高，但总体增益有限。这些发现表明，数据集测绘在SQuAD风格的问答任务中对对抗鲁棒性的提升作用有限。最后，将这些结果与SNLI之前的发现进行了比较，并讨论了观察到差异的可能原因。 

---
# Voxel-based Point Cloud Geometry Compression with Space-to-Channel Context 

**Title (ZH)**: 基于体素的点云几何压缩：空间到通道上下文方法 

**Authors**: Bojun Liu, Yangzhi Ma, Ao Luo, Li Li, Dong Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.18283)  

**Abstract**: Voxel-based methods are among the most efficient for point cloud geometry compression, particularly with dense point clouds. However, they face limitations due to a restricted receptive field, especially when handling high-bit depth point clouds. To overcome this issue, we introduce a stage-wise Space-to-Channel (S2C) context model for both dense point clouds and low-level sparse point clouds. This model utilizes a channel-wise autoregressive strategy to effectively integrate neighborhood information at a coarse resolution. For high-level sparse point clouds, we further propose a level-wise S2C context model that addresses resolution limitations by incorporating Geometry Residual Coding (GRC) for consistent-resolution cross-level prediction. Additionally, we use the spherical coordinate system for its compact representation and enhance our GRC approach with a Residual Probability Approximation (RPA) module, which features a large kernel size. Experimental results show that our S2C context model not only achieves bit savings while maintaining or improving reconstruction quality but also reduces computational complexity compared to state-of-the-art voxel-based compression methods. 

**Abstract (ZH)**: 基于体素的方法是点云几何压缩中最高效的手段，特别是对于稠密点云。然而，这些方法由于 receptive field 受限，在处理高 bit 深度点云时面临限制。为克服这一问题，我们引入了一种分阶段的 Space-to-Channel (S2C) 上下文模型，适用于稠密点云和低层级稀疏点云。该模型利用通道间自回归策略，在粗分辨率下有效整合邻域信息。对于高层稀疏点云，我们进一步提出了一种分层级的 S2C 上下文模型，通过引入 Geometry Residual Coding (GRC) 进行一致分辨率跨层级预测，来解决分辨率限制问题。此外，我们使用球坐标系统以实现紧凑表示，并通过 Residual Probability Approximation (RPA) 模块增强 GRC 方法，该模块具备大核大小。实验结果表明，我们的 S2C 上下文模型不仅能节省位宽同时保持或提升重建质量，还能降低与现有最佳体素基压缩方法相比的计算复杂度。 

---
# TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model 

**Title (ZH)**: TopV：具推理时优化的兼容性_token剪枝方法以实现快速低内存多模态视觉语言模型 

**Authors**: Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, Bo Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2503.18278)  

**Abstract**: Vision-Language Models (VLMs) demand substantial computational resources during inference, largely due to the extensive visual input tokens for representing visual information. Previous studies have noted that visual tokens tend to receive less attention than text tokens, suggesting their lower importance during inference and potential for pruning. However, their methods encounter several challenges: reliance on greedy heuristic criteria for token importance and incompatibility with FlashAttention and KV cache. To address these issues, we introduce \textbf{TopV}, a compatible \textbf{TO}ken \textbf{P}runing with inference Time Optimization for fast and low-memory \textbf{V}LM, achieving efficient pruning without additional training or fine-tuning. Instead of relying on attention scores, we formulate token pruning as an optimization problem, accurately identifying important visual tokens while remaining compatible with FlashAttention. Additionally, since we only perform this pruning once during the prefilling stage, it effectively reduces KV cache size. Our optimization framework incorporates a visual-aware cost function considering factors such as Feature Similarity, Relative Spatial Distance, and Absolute Central Distance, to measure the importance of each source visual token, enabling effective pruning of low-importance tokens. Extensive experiments demonstrate that our method outperforms previous token pruning methods, validating the effectiveness and efficiency of our approach. 

**Abstract (ZH)**: TopV：基于推理时优化的兼容Token剪枝方法以实现高效的低内存VLM 

---
# Risk Management for Distributed Arbitrage Systems: Integrating Artificial Intelligence 

**Title (ZH)**: 分布式套利系统中的风险管理：整合人工智能 

**Authors**: Akaash Vishal Hazarika, Mahak Shah, Swapnil Patil, Pradyumna Shukla  

**Link**: [PDF](https://arxiv.org/pdf/2503.18265)  

**Abstract**: Effective risk management solutions become absolutely crucial when financial markets embrace distributed technology and decentralized financing (DeFi). This study offers a thorough survey and comparative analysis of the integration of artificial intelligence (AI) in risk management for distributed arbitrage systems. We examine several modern caching techniques namely in memory caching, distributed caching, and proxy caching and their functions in enhancing performance in decentralized settings. Through literature review we examine the utilization of AI techniques for alleviating risks related to market volatility, liquidity challenges, operational failures, regulatory compliance, and security threats. This comparison research evaluates various case studies from prominent DeFi technologies, emphasizing critical performance metrics like latency reduction, load balancing, and system resilience. Additionally, we examine the problems and trade offs associated with these technologies, emphasizing their effects on consistency, scalability, and fault tolerance. By meticulously analyzing real world applications, specifically centering on the Aave platform as our principal case study, we illustrate how the purposeful amalgamation of AI with contemporary caching methodologies has revolutionized risk management in distributed arbitrage systems. 

**Abstract (ZH)**: 有效风险管理解决方案在金融 markets拥抱分布式技术与去中心化融资（DeFi）时变得至关重要。本研究提供了人工智能（AI）在分布式对冲系统风险管理集成中的全面综述与比较分析。我们探讨了几种现代缓存技术，包括内存缓存、分布式缓存和代理缓存，并分析了它们在去中心化环境中提升性能的功能。通过文献综述，我们研究了人工智能技术在缓解与市场波动、流动性挑战、运营失败、监管合规和安全威胁相关风险方面的应用。本比较研究评估了来自知名DeFi技术的多种案例研究，强调了关键性能指标，如延迟降低、负载均衡和系统韧性。此外，我们还分析了这些技术的问题和权衡，强调了它们对一致性和可扩展性以及容错性的影响。通过细致分析实际应用，特别是重点以Aave平台作为主要案例研究，我们展示了有目的地将AI与现代缓存方法相结合如何在分布式对冲系统中革新风险管理。 

---
# Severing Spurious Correlations with Data Pruning 

**Title (ZH)**: 切断虚假相关性的数据修剪方法 

**Authors**: Varun Mulchandani, Jung-Eun Kim  

**Link**: [PDF](https://arxiv.org/pdf/2503.18258)  

**Abstract**: Deep neural networks have been shown to learn and rely on spurious correlations present in the data that they are trained on. Reliance on such correlations can cause these networks to malfunction when deployed in the real world, where these correlations may no longer hold. To overcome the learning of and reliance on such correlations, recent studies propose approaches that yield promising results. These works, however, study settings where the strength of the spurious signal is significantly greater than that of the core, invariant signal, making it easier to detect the presence of spurious features in individual training samples and allow for further processing. In this paper, we identify new settings where the strength of the spurious signal is relatively weaker, making it difficult to detect any spurious information while continuing to have catastrophic consequences. We also discover that spurious correlations are learned primarily due to only a handful of all the samples containing the spurious feature and develop a novel data pruning technique that identifies and prunes small subsets of the training data that contain these samples. Our proposed technique does not require inferred domain knowledge, information regarding the sample-wise presence or nature of spurious information, or human intervention. Finally, we show that such data pruning attains state-of-the-art performance on previously studied settings where spurious information is identifiable. 

**Abstract (ZH)**: 深神经网络已被证明会学习并依赖于训练数据中存在的虚假相关性。在这些相关性在实际应用中不再成立的情况下，这些网络可能会失效。为了克服学习和依赖这些相关性的问题，最近的研究提出了一些取得良好效果的方法。然而，这些研究主要关注虚假信号强度显著大于核心不变信号强度的情景，使得在单个训练样本中检测到虚假特征并进行进一步处理更加容易。在本文中，我们识别了一种新的情景，其中虚假信号的强度相对较弱，这使得检测任何虚假信息变得更加困难，但仍会导致灾难性的后果。我们还发现，虚假相关性主要是由于少量包含虚假特征的样本而被学习，因此开发了一种新颖的数据修剪技术来识别和修剪训练数据中包含这些样本的小子集。我们提出的技术不需要推断领域的知识、样本级别虚假信息的存在与否或性质，也不需要人工干预。最后，我们表明这种数据修剪在先前研究中虚假信息可识别的情景中达到了最先进的性能。 

---
# The Human-Machine Identity Blur: A Unified Framework for Cybersecurity Risk Management in 2025 

**Title (ZH)**: 人机身份模糊：2025年网络安全风险统一管理框架 

**Authors**: Kush Janani  

**Link**: [PDF](https://arxiv.org/pdf/2503.18255)  

**Abstract**: The modern enterprise is facing an unprecedented surge in digital identities, with machine identities now significantly outnumbering human identities. This paper examines the cybersecurity risks emerging from what we define as the "human-machine identity blur" - the point at which human and machine identities intersect, delegate authority, and create new attack surfaces. Drawing from industry data, expert insights, and real-world incident analysis, we identify key governance gaps in current identity management models that treat human and machine entities as separate domains. To address these challenges, we propose a Unified Identity Governance Framework based on four core principles: treating identity as a continuum rather than a binary distinction, applying consistent risk evaluation across all identity types, implementing continuous verification guided by zero trust principles, and maintaining governance throughout the entire identity lifecycle. Our research shows that organizations adopting this unified approach experience a 47 percent reduction in identity-related security incidents and a 62 percent improvement in incident response time. We conclude by offering a practical implementation roadmap and outlining future research directions as AI-driven systems become increasingly autonomous. 

**Abstract (ZH)**: 现代企业面临的空前数字身份激增，机器身份现已显著超过人类身份。本文探讨了我们定义的“人机身份模糊”所带来的网络安全风险——即人类身份与机器身份交汇、权力委托并创建新的攻击面的点。基于行业数据、专家见解和实际案例分析，我们识别了当前身份管理模型中处理人类和机器实体作为分离领域存在的关键治理缺口。为应对这些挑战，我们提出了一种基于四大原则的统一体身份治理框架：将身份视为连续体而非二元区分，对所有身份类型应用一致的风险评估，根据零信任原则实施持续验证，并在整个身份生命周期中保持治理。我们的研究表明，采用这种统一方法的企业在身份相关安全事件中减少了47%，并在事件响应时间上提高了62%。我们以提供实用的实施路线图为结尾，并概述了随着AI驱动系统的日益自主，未来的研究方向。 

---
# ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices 

**Title (ZH)**: ShED-HD：边缘设备上轻量级幻觉检测的香农熵分布框架 

**Authors**: Aneesh Vathul, Daniel Lee, Sheryl Chen, Arthi Tasmia  

**Link**: [PDF](https://arxiv.org/pdf/2503.18242)  

**Abstract**: Large Language Models (LLMs) have demonstrated impressive capabilities on a broad array of NLP tasks, but their tendency to produce hallucinations$\unicode{x2013}$plausible-sounding but factually incorrect content$\unicode{x2013}$poses severe challenges in high-stakes domains. Existing hallucination detection methods either bear the computational cost of multiple inference passes or sacrifice accuracy for efficiency with single-pass approaches, neither of which is ideal in resource-constrained environments such as edge devices. We propose the Shannon Entropy Distribution Hallucination Detector (ShED-HD), a novel hallucination detection framework that bridges this gap by classifying sequence-level entropy patterns using a lightweight BiLSTM architecture with single-headed attention. In contrast to prior approaches, ShED-HD efficiently detects distinctive uncertainty patterns across entire output sequences, preserving contextual awareness. Through in-depth evaluation on three datasets (BioASQ, TriviaQA, and Jeopardy Questions), we show that ShED-HD significantly outperforms other computationally efficient approaches in the out-of-distribution setting, while achieving comparable performance in the in-distribution setting. ShED-HD facilitates hallucination detection that is low-cost, accurate, and generalizable, improving the credibility of content generated by LLMs in resource-constrained environments where trustworthy AI functionality is crucial. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在广泛的语言处理任务上展现了令人印象深刻的能力，但它们生成幻觉（听起来合理但实际上错误的内容）的倾向在高风险领域提出了重大挑战。现有的幻觉检测方法要么承受多轮推理的计算成本，要么在单轮推理中牺牲准确性以提高效率，这两种方法在资源受限的环境中都不理想。我们提出了一种新的幻觉检测框架Shannon熵分布幻觉检测器（ShED-HD），该框架通过轻量级双向长短期记忆（BiLSTM）结构和单头注意机制来分类序列级别的熵模式，以弥合这一差距。与以往方法不同，ShED-HD能够高效地检测整个输出序列中的独特不确定性模式，同时保持上下文意识。通过在三个数据集（BioASQ、TriviaQA和Jeopardy Questions）上的深入评估，我们展示了ShED-HD在分布外设置中显著优于其他计算高效的approaches，在分布内设置中达到相当的性能。ShED-HD使得在资源受限的环境中，幻觉检测变得低成本、准确且通用，从而提高LLMs生成内容的可信度。 

---
# Collaborating with AI Agents: Field Experiments on Teamwork, Productivity, and Performance 

**Title (ZH)**: 与AI代理合作：团队合作、生产力和表现的实地实验 

**Authors**: Harang Ju, Sinan Aral  

**Link**: [PDF](https://arxiv.org/pdf/2503.18238)  

**Abstract**: To uncover how AI agents change productivity, performance, and work processes, we introduce MindMeld: an experimentation platform enabling humans and AI agents to collaborate in integrative workspaces. In a large-scale marketing experiment on the platform, 2310 participants were randomly assigned to human-human and human-AI teams, with randomized AI personality traits. The teams exchanged 183,691 messages, and created 63,656 image edits, 1,960,095 ad copy edits, and 10,375 AI-generated images while producing 11,138 ads for a large think tank. Analysis of fine-grained communication, collaboration, and workflow logs revealed that collaborating with AI agents increased communication by 137% and allowed humans to focus 23% more on text and image content generation messaging and 20% less on direct text editing. Humans on Human-AI teams sent 23% fewer social messages, creating 60% greater productivity per worker and higher-quality ad copy. In contrast, human-human teams produced higher-quality images, suggesting that AI agents require fine-tuning for multimodal workflows. AI personality prompt randomization revealed that AI traits can complement human personalities to enhance collaboration. For example, conscientious humans paired with open AI agents improved image quality, while extroverted humans paired with conscientious AI agents reduced the quality of text, images, and clicks. In field tests of ad campaigns with ~5M impressions, ads with higher image quality produced by human collaborations and higher text quality produced by AI collaborations performed significantly better on click-through rate and cost per click metrics. Overall, ads created by human-AI teams performed similarly to those created by human-human teams. Together, these results suggest AI agents can improve teamwork and productivity, especially when tuned to complement human traits. 

**Abstract (ZH)**: 探索AI代理如何改变生产力、绩效和工作流程：MindMeld：一个让人类与AI代理在整合工作空间中协作的实验平台 

---
# Adaptive Multi-Fidelity Reinforcement Learning for Variance Reduction in Engineering Design Optimization 

**Title (ZH)**: 工程设计优化中基于方差减少的自适应多保真强化学习 

**Authors**: Akash Agrawal, Christopher McComb  

**Link**: [PDF](https://arxiv.org/pdf/2503.18229)  

**Abstract**: Multi-fidelity Reinforcement Learning (RL) frameworks efficiently utilize computational resources by integrating analysis models of varying accuracy and costs. The prevailing methodologies, characterized by transfer learning, human-inspired strategies, control variate techniques, and adaptive sampling, predominantly depend on a structured hierarchy of models. However, this reliance on a model hierarchy can exacerbate variance in policy learning when the underlying models exhibit heterogeneous error distributions across the design space. To address this challenge, this work proposes a novel adaptive multi-fidelity RL framework, in which multiple heterogeneous, non-hierarchical low-fidelity models are dynamically leveraged alongside a high-fidelity model to efficiently learn a high-fidelity policy. Specifically, low-fidelity policies and their experience data are adaptively used for efficient targeted learning, guided by their alignment with the high-fidelity policy. The effectiveness of the approach is demonstrated in an octocopter design optimization problem, utilizing two low-fidelity models alongside a high-fidelity simulator. The results demonstrate that the proposed approach substantially reduces variance in policy learning, leading to improved convergence and consistent high-quality solutions relative to traditional hierarchical multi-fidelity RL methods. Moreover, the framework eliminates the need for manually tuning model usage schedules, which can otherwise introduce significant computational overhead. This positions the framework as an effective variance-reduction strategy for multi-fidelity RL, while also mitigating the computational and operational burden of manual fidelity scheduling. 

**Abstract (ZH)**: 多保真度强化学习框架：通过动态利用非层次异质低保真度模型与高保真度模型高效学习高保真度策略 

---
# PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation 

**Title (ZH)**: PG-SAM: 以先验知识引导的SAM在多器官分割中的应用 

**Authors**: Yiheng Zhong, Zihong Luo, Chengzhi Liu, Feilong Tang, Zelin Peng, Ming Hu, Yingzhen Hu, Jionglong Su, Zongyuan Geand, Imran Razzak  

**Link**: [PDF](https://arxiv.org/pdf/2503.18227)  

**Abstract**: Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities; however, its accuracy and robustness significantly decrease when applied to medical image segmentation. Existing methods address this issue through modality fusion, integrating textual and image information to provide more detailed priors. In this study, we argue that the granularity of text and the domain gap affect the accuracy of the priors. Furthermore, the discrepancy between high-level abstract semantics and pixel-level boundary details in images can introduce noise into the fusion process. To address this, we propose Prior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner to leverage specialized medical knowledge for better modality alignment. The core of our method lies in efficiently addressing the domain gap with fine-grained text from a medical LLM. Meanwhile, it also enhances the priors' quality after modality alignment, ensuring more accurate segmentation. In addition, our decoder enhances the model's expressive capabilities through multi-level feature fusion and iterative mask optimizer operations, supporting unprompted learning. We also propose a unified pipeline that effectively supplies high-quality semantic information to SAM. Extensive experiments on the Synapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art performance. Our anonymous code is released at this https URL. 

**Abstract (ZH)**: Segment Anything Model (SAM)在医疗图像分割中的细粒度先验引导方法：Prior-Guided SAM (PG-SAM)的研究 

---
# ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data 

**Title (ZH)**: ViVa: 基于视频训练的价值函数及其在多元数据引导在线强化学习中的应用 

**Authors**: Nitish Dashora, Dibya Ghosh, Sergey Levine  

**Link**: [PDF](https://arxiv.org/pdf/2503.18210)  

**Abstract**: Online reinforcement learning (RL) with sparse rewards poses a challenge partly because of the lack of feedback on states leading to the goal. Furthermore, expert offline data with reward signal is rarely available to provide this feedback and bootstrap online learning. How can we guide online agents to the right solution without this on-task data? Reward shaping offers a solution by providing fine-grained signal to nudge the policy towards the optimal solution. However, reward shaping often requires domain knowledge to hand-engineer heuristics for a specific goal. To enable more general and inexpensive guidance, we propose and analyze a data-driven methodology that automatically guides RL by learning from widely available video data such as Internet recordings, off-task demonstrations, task failures, and undirected environment interaction. By learning a model of optimal goal-conditioned value from diverse passive data, we open the floor to scaling up and using various data sources to model general goal-reaching behaviors relevant to guiding online RL. Specifically, we use intent-conditioned value functions to learn from diverse videos and incorporate these goal-conditioned values into the reward. Our experiments show that video-trained value functions work well with a variety of data sources, exhibit positive transfer from human video pre-training, can generalize to unseen goals, and scale with dataset size. 

**Abstract (ZH)**: 基于视频数据的在线强化学习引导方法：利用稀疏奖励信号实现通用目标导向行为建模 

---
# FROG: Fair Removal on Graphs 

**Title (ZH)**: FROG: 图上的公平删除 

**Authors**: Ziheng Chen, Jiali Cheng, Gabriele Tolomei, Sijia Liu, Hadi Amiri, Yu Wang, Kaushiki Nag, Lu Lin  

**Link**: [PDF](https://arxiv.org/pdf/2503.18197)  

**Abstract**: As compliance with privacy regulations becomes increasingly critical, the growing demand for data privacy has highlighted the significance of machine unlearning in many real world applications, such as social network and recommender systems, many of which can be represented as graph-structured data. However, existing graph unlearning algorithms indiscriminately modify edges or nodes from well-trained models without considering the potential impact of such structural modifications on fairness. For example, forgetting links between nodes with different genders in a social network may exacerbate group disparities, leading to significant fairness concerns. To address these challenges, we propose a novel approach that jointly optimizes the graph structure and the corresponding model for fair unlearning tasks. Specifically,our approach rewires the graph to enhance unlearning efficiency by removing redundant edges that hinder forgetting while preserving fairness through targeted edge augmentation. Additionally, we introduce a worst-case evaluation mechanism to assess the reliability of fair unlearning performance. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed approach in achieving superior unlearning outcomes. 

**Abstract (ZH)**: 随着遵守隐私法规变得越来越关键，数据隐私需求的增长凸显了在社交网络和推荐系统等许多实际应用中实现机器遗忘的重要性，这些应用可以表示为图结构数据。然而，现有的图遗忘算法在修改从训练好的模型中选择的边或节点时，并未考虑到这种结构修改可能对公平性产生的潜在影响。例如，在社交网络中遗忘不同性别节点之间的链接可能会加剧群体差异，从而引起显著的公平性问题。为应对这些挑战，我们提出了一种新的方法，该方法同时优化图结构和相应的模型以实现公正的遗忘任务。具体来说，我们的方法通过去除阻碍遗忘的冗余边来重连图，同时通过针对性地增加边来保持公平性。此外，我们引入了一种最坏情况评估机制来评估公正遗忘性能的可靠性。在真实数据集上的广泛实验表明，所提出的方法在实现优越的遗忘结果方面具有有效性。 

---
# Adaptive Physics-informed Neural Networks: A Survey 

**Title (ZH)**: 自适应物理约束神经网络：一个综述 

**Authors**: Edgar Torres, Jonathan Schiefer, Mathias Niepert  

**Link**: [PDF](https://arxiv.org/pdf/2503.18181)  

**Abstract**: Physics-informed neural networks (PINNs) have emerged as a promising approach to solving partial differential equations (PDEs) using neural networks, particularly in data-scarce scenarios, due to their unsupervised training capability. However, limitations related to convergence and the need for re-optimization with each change in PDE parameters hinder their widespread adoption across scientific and engineering applications. This survey reviews existing research that addresses these limitations through transfer learning and meta-learning. The covered methods improve the training efficiency, allowing faster adaptation to new PDEs with fewer data and computational resources. While traditional numerical methods solve systems of differential equations directly, neural networks learn solutions implicitly by adjusting their parameters. One notable advantage of neural networks is their ability to abstract away from specific problem domains, allowing them to retain, discard, or adapt learned representations to efficiently address similar problems. By exploring the application of these techniques to PINNs, this survey identifies promising directions for future research to facilitate the broader adoption of PINNs in a wide range of scientific and engineering applications. 

**Abstract (ZH)**: 物理约束神经网络（PINNs）通过神经网络求解偏微分方程（PDEs）的一种有前景的方法，特别是在数据稀少的情况下，由于其无监督训练能力而受到关注。然而，收敛性问题以及每更改一次PDE参数就需要重新优化的限制阻碍了其在科学和工程应用中的广泛应用。本文综述了通过迁移学习和元学习解决这些限制的研究进展。涵盖了的方法提高了训练效率，允许在更少的数据和计算资源下更快地适应新的PDEs。虽然传统的数值方法直接求解微分方程系统，神经网络通过调整参数隐式地学习解。神经网络的一个显著优势是能够脱离特定问题领域，允许它们保留、丢弃或适应学到的表示，以高效地解决类似问题。通过探索这些技术在PINNs中的应用，本文指出了未来研究的有前景的方向，以促进PINNs在广泛的科学和工程应用中的更广泛应用。 

---
# Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering 

**Title (ZH)**: 揭示欺骗性视觉元素：基于误导性图表问答评估多模态大型语言模型 

**Authors**: Zixin Chen, Sicheng Song, Kashun Shum, Yanna Lin, Rui Sheng, Huamin Qu  

**Link**: [PDF](https://arxiv.org/pdf/2503.18172)  

**Abstract**: Misleading chart visualizations, which intentionally manipulate data representations to support specific claims, can distort perceptions and lead to incorrect conclusions. Despite decades of research, misleading visualizations remain a widespread and pressing issue. Recent advances in multimodal large language models (MLLMs) have demonstrated strong chart comprehension capabilities, yet no existing work has systematically evaluated their ability to detect and interpret misleading charts. This paper introduces the Misleading Chart Question Answering (Misleading ChartQA) Benchmark, a large-scale multimodal dataset designed to assess MLLMs in identifying and reasoning about misleading charts. It contains over 3,000 curated examples, covering 21 types of misleaders and 10 chart types. Each example includes standardized chart code, CSV data, and multiple-choice questions with labeled explanations, validated through multi-round MLLM checks and exhausted expert human review. We benchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitations in identifying visually deceptive practices. We also propose a novel pipeline that detects and localizes misleaders, enhancing MLLMs' accuracy in misleading chart interpretation. Our work establishes a foundation for advancing MLLM-driven misleading chart comprehension. We publicly release the sample dataset to support further research in this critical area. 

**Abstract (ZH)**: 误导性图表可视化：故意操纵数据表示以支持特定主张，可能导致认知扭曲并导致错误结论。尽管已有数十年的研究，误导性图表仍是广泛存在的紧迫问题。近期多模态大型语言模型（MLLMs）展示了强大的图表理解能力，但迄今为止尚无工作系统性地评估其检测和解释误导性图表的能力。本文介绍了误导性图表问答基准（Misleading ChartQA），这是一个大规模多模态数据集，用于评估MLLMs在识别和推理误导性图表方面的能力。该数据集包含超过3000个精心挑选的例子，覆盖了21种类型和10种图表类型。每个示例包括标准化的图表代码、CSV数据以及带有标注解释的多项选择题，这些解释经过多轮MLLM检查和全面的人类专家审查。我们对16个最先进的MLLMs进行了基准测试，揭示了它们在识别视觉欺骗性实践方面的局限性。我们还提出了一种新管道，可用于检测和定位误导性图表，从而提高MLLMs对误导性图表理解的准确性。我们的工作为MLLM驱动的误导性图表理解的发展奠定了基础。为了支持这一关键领域的进一步研究，我们公开发布了样本数据集。 

---
# Self-Attention Diffusion Models for Zero-Shot Biomedical Image Segmentation: Unlocking New Frontiers in Medical Imaging 

**Title (ZH)**: 自注意力扩散模型在零样本生物医学图像分割中的应用：开启医学成像的新前沿 

**Authors**: Abderrachid Hamrani, Anuradha Godavarty  

**Link**: [PDF](https://arxiv.org/pdf/2503.18170)  

**Abstract**: Producing high-quality segmentation masks for medical images is a fundamental challenge in biomedical image analysis. Recent research has explored large-scale supervised training to enable segmentation across various medical imaging modalities and unsupervised training to facilitate segmentation without dense annotations. However, constructing a model capable of segmenting diverse medical images in a zero-shot manner without any annotations remains a significant hurdle. This paper introduces the Attention Diffusion Zero-shot Unsupervised System (ADZUS), a novel approach that leverages self-attention diffusion models for zero-shot biomedical image segmentation. ADZUS harnesses the intrinsic capabilities of pre-trained diffusion models, utilizing their generative and discriminative potentials to segment medical images without requiring annotated training data or prior domain-specific knowledge. The ADZUS architecture is detailed, with its integration of self-attention mechanisms that facilitate context-aware and detail-sensitive segmentations being highlighted. Experimental results across various medical imaging datasets, including skin lesion segmentation, chest X-ray infection segmentation, and white blood cell segmentation, reveal that ADZUS achieves state-of-the-art performance. Notably, ADZUS reached Dice scores ranging from 88.7\% to 92.9\% and IoU scores from 66.3\% to 93.3\% across different segmentation tasks, demonstrating significant improvements in handling novel, unseen medical imagery. It is noteworthy that while ADZUS demonstrates high effectiveness, it demands substantial computational resources and extended processing times. The model's efficacy in zero-shot settings underscores its potential to reduce reliance on costly annotations and seamlessly adapt to new medical imaging tasks, thereby expanding the diagnostic capabilities of AI-driven medical imaging technologies. 

**Abstract (ZH)**: 基于注意力扩散模型的零样本无监督医疗图像分割系统（ADZUS） 

---
# Evaluating Negative Sampling Approaches for Neural Topic Models 

**Title (ZH)**: 评估负采样方法在神经主题模型中的有效性 

**Authors**: Suman Adhya, Avishek Lahiri, Debarshi Kumar Sanyal, Partha Pratim Das  

**Link**: [PDF](https://arxiv.org/pdf/2503.18167)  

**Abstract**: Negative sampling has emerged as an effective technique that enables deep learning models to learn better representations by introducing the paradigm of learn-to-compare. The goal of this approach is to add robustness to deep learning models to learn better representation by comparing the positive samples against the negative ones. Despite its numerous demonstrations in various areas of computer vision and natural language processing, a comprehensive study of the effect of negative sampling in an unsupervised domain like topic modeling has not been well explored. In this paper, we present a comprehensive analysis of the impact of different negative sampling strategies on neural topic models. We compare the performance of several popular neural topic models by incorporating a negative sampling technique in the decoder of variational autoencoder-based neural topic models. Experiments on four publicly available datasets demonstrate that integrating negative sampling into topic models results in significant enhancements across multiple aspects, including improved topic coherence, richer topic diversity, and more accurate document classification. Manual evaluations also indicate that the inclusion of negative sampling into neural topic models enhances the quality of the generated topics. These findings highlight the potential of negative sampling as a valuable tool for advancing the effectiveness of neural topic models. 

**Abstract (ZH)**: 负采样已成为一种有效技术，通过引入学习比较的 paradigm，使深度学习模型能够学习更好的表示。该方法的目标是通过将正样本与负样本进行比较，增强深度学习模型学习更好表示的能力。尽管在计算机视觉和自然语言处理的各个领域中，负采样的效果得到了广泛证实，但对其在无监督领域如主题建模中的影响的研究尚不全面。在本文中，我们对不同负采样策略对神经主题模型的影响进行了全面分析。通过在基于变分自编码器的神经主题模型的解码器中引入负采样技术，我们将几种流行的神经主题模型的性能进行了比较。在四个公开数据集上的实验表明，将负采样整合到主题模型中，在多个方面均带来了显著的提升，包括提高了主题的一致性、增加了主题的多样性以及提高了文档分类的准确性。 manual评估也表明，将负采样纳入神经主题模型可以提高生成主题的质量。这些发现突显了负采样作为增强神经主题模型效果的有价值工具的潜力。 

---
# SNRAware: Improved Deep Learning MRI Denoising with SNR Unit Training and G-factor Map Augmentation 

**Title (ZH)**: SNRAware: 通过SNR单元训练和G因子图增强改进的深度学习MRI降噪方法 

**Authors**: Hui Xue, Sarah M. Hooper, Iain Pierce, Rhodri H. Davies, John Stairs, Joseph Naegele, Adrienne E. Campbell-Washburn, Charlotte Manisty, James C. Moon, Thomas A. Treibel, Peter Kellman, Michael S. Hansen  

**Link**: [PDF](https://arxiv.org/pdf/2503.18162)  

**Abstract**: To develop and evaluate a new deep learning MR denoising method that leverages quantitative noise distribution information from the reconstruction process to improve denoising performance and generalization.
This retrospective study trained 14 different transformer and convolutional models with two backbone architectures on a large dataset of 2,885,236 images from 96,605 cardiac retro-gated cine complex series acquired at 3T. The proposed training scheme, termed SNRAware, leverages knowledge of the MRI reconstruction process to improve denoising performance by simulating large, high quality, and diverse synthetic datasets, and providing quantitative information about the noise distribution to the model. In-distribution testing was performed on a hold-out dataset of 3000 samples with performance measured using PSNR and SSIM, with ablation comparison without the noise augmentation. Out-of-distribution tests were conducted on cardiac real-time cine, first-pass cardiac perfusion, and neuro and spine MRI, all acquired at 1.5T, to test model generalization across imaging sequences, dynamically changing contrast, different anatomies, and field strengths. The best model found in the in-distribution test generalized well to out-of-distribution samples, delivering 6.5x and 2.9x CNR improvement for real-time cine and perfusion imaging, respectively. Further, a model trained with 100% cardiac cine data generalized well to a T1 MPRAGE neuro 3D scan and T2 TSE spine MRI. 

**Abstract (ZH)**: 开发并评估一种新的深度学习MRI降噪方法，该方法利用重建过程中定量的噪声分布信息以提高降噪性能和泛化能力 

---
# Active Inference for Energy Control and Planning in Smart Buildings and Communities 

**Title (ZH)**: 智能建筑与社区中的能量控制与规划的主动推理方法 

**Authors**: Seyyed Danial Nazemi, Mohsen A. Jafari, Andrea Matta  

**Link**: [PDF](https://arxiv.org/pdf/2503.18161)  

**Abstract**: Active Inference (AIF) is emerging as a powerful framework for decision-making under uncertainty, yet its potential in engineering applications remains largely unexplored. In this work, we propose a novel dual-layer AIF architecture that addresses both building-level and community-level energy management. By leveraging the free energy principle, each layer adapts to evolving conditions and handles partial observability without extensive sensor information and respecting data privacy. We validate the continuous AIF model against both a perfect optimization baseline and a reinforcement learning-based approach. We also test the community AIF framework under extreme pricing scenarios. The results highlight the model's robustness in handling abrupt changes. This study is the first to show how a distributed AIF works in engineering. It also highlights new opportunities for privacy-preserving and uncertainty-aware control strategies in engineering applications. 

**Abstract (ZH)**: 基于活跃推断的分布式能源管理架构及其在工程应用中的探索：隐私保护与不确定性感知控制的新机遇 

---
# DiffusionTalker: Efficient and Compact Speech-Driven 3D Talking Head via Personalizer-Guided Distillation 

**Title (ZH)**: DiffusionTalker: 高效且紧凑的个性化蒸馏驱动3D说话头模型 

**Authors**: Peng Chen, Xiaobao Wei, Ming Lu, Hui Chen, Feng Tian  

**Link**: [PDF](https://arxiv.org/pdf/2503.18159)  

**Abstract**: Real-time speech-driven 3D facial animation has been attractive in academia and industry. Traditional methods mainly focus on learning a deterministic mapping from speech to animation. Recent approaches start to consider the nondeterministic fact of speech-driven 3D face animation and employ the diffusion model for the task. Existing diffusion-based methods can improve the diversity of facial animation. However, personalized speaking styles conveying accurate lip language is still lacking, besides, efficiency and compactness still need to be improved. In this work, we propose DiffusionTalker to address the above limitations via personalizer-guided distillation. In terms of personalization, we introduce a contrastive personalizer that learns identity and emotion embeddings to capture speaking styles from audio. We further propose a personalizer enhancer during distillation to enhance the influence of embeddings on facial animation. For efficiency, we use iterative distillation to reduce the steps required for animation generation and achieve more than 8x speedup in inference. To achieve compactness, we distill the large teacher model into a smaller student model, reducing our model's storage by 86.4\% while minimizing performance loss. After distillation, users can derive their identity and emotion embeddings from audio to quickly create personalized animations that reflect specific speaking styles. Extensive experiments are conducted to demonstrate that our method outperforms state-of-the-art methods. The code will be released at: this https URL. 

**Abstract (ZH)**: 基于个性化引导蒸馏的实时语音驱动3D面部动画 

---
# Adoption of Watermarking for Generative AI Systems in Practice and Implications under the new EU AI Act 

**Title (ZH)**: 实践中的生成式人工智能系统水印采用及在新欧盟人工智能法案下的 implications 

**Authors**: Bram Rijsbosch, Gijs van Dijck, Konrad Kollnig  

**Link**: [PDF](https://arxiv.org/pdf/2503.18156)  

**Abstract**: AI-generated images have become so good in recent years that individuals cannot distinguish them any more from "real" images. This development creates a series of societal risks, and challenges our perception of what is true and what is not, particularly with the emergence of "deep fakes" that impersonate real individuals. Watermarking, a technique that involves embedding identifying information within images to indicate their AI-generated nature, has emerged as a primary mechanism to address the risks posed by AI-generated images. The implementation of watermarking techniques is now becoming a legal requirement in many jurisdictions, including under the new 2024 EU AI Act. Despite the widespread use of AI image generation systems, the current status of watermarking implementation remains largely unexamined. Moreover, the practical implications of the AI Act's watermarking requirements have not previously been studied. The present paper therefore both provides an empirical analysis of 50 of the most widely used AI systems for image generation, and embeds this empirical analysis into a legal analysis of the AI Act. We identify four categories of generative AI image systems relevant under the AI Act, outline the legal obligations for each category, and find that only a minority number of providers currently implement adequate watermarking practices. 

**Abstract (ZH)**: AI生成图像的水印标识：现状、法律要求及实证分析 

---
# Efficient Deep Learning Approaches for Processing Ultra-Widefield Retinal Imaging 

**Title (ZH)**: 高效的深度学习方法用于处理超广场视网膜成像 

**Authors**: Siwon Kim, Wooyung Yun, Jeongbin Oh, Soomok Lee  

**Link**: [PDF](https://arxiv.org/pdf/2503.18151)  

**Abstract**: Deep learning has emerged as the predominant solution for classifying medical images. We intend to apply these developments to the ultra-widefield (UWF) retinal imaging dataset. Since UWF images can accurately diagnose various retina diseases, it is very important to clas sify them accurately and prevent them with early treatment. However, processing images manually is time-consuming and labor-intensive, and there are two challenges to automating this process. First, high perfor mance usually requires high computational resources. Artificial intelli gence medical technology is better suited for places with limited medical resources, but using high-performance processing units in such environ ments is challenging. Second, the problem of the accuracy of colour fun dus photography (CFP) methods. In general, the UWF method provides more information for retinal diagnosis than the CFP method, but most of the research has been conducted based on the CFP method. Thus, we demonstrate that these problems can be efficiently addressed in low performance units using methods such as strategic data augmentation and model ensembles, which balance performance and computational re sources while utilizing UWF images. 

**Abstract (ZH)**: 深度学习已成为医学图像分类的主要解决方案。我们打算将这些进展应用到超广field (UWF) 视网膜成像数据集中。由于UWF图像可以准确诊断各种视网膜疾病，因此准确分类这些图像并进行早期治疗至关重要。然而，手动处理图像耗时且劳动密集，自动化此过程面临两个挑战。首先，高性能通常需要高计算资源。人工智能医疗技术更适合资源有限的医疗机构，但在这些环境中使用高性能处理单元具有挑战性。其次，色fundus摄影（CFP）方法准确性的问题。总体而言，UWF方法为视网膜诊断提供了更多的信息，但大多数研究都是基于CFP方法进行的。因此，我们展示了一种使用战略数据增强和模型集成等方法，在低性能单元中高效解决这些问题，同时平衡性能和计算资源并利用UWF图像。 

---
# LocDiffusion: Identifying Locations on Earth by Diffusing in the Hilbert Space 

**Title (ZH)**: LocDiffusion：在希尔伯特空间中扩散以识别地球上的位置 

**Authors**: Zhangyu Wang, Jielu Zhang, Zhongliang Zhou, Qian Cao, Nemin Wu, Zeping Liu, Lan Mu, Yang Song, Yiqun Xie, Ni Lao, Gengchen Mai  

**Link**: [PDF](https://arxiv.org/pdf/2503.18142)  

**Abstract**: Image geolocalization is a fundamental yet challenging task, aiming at inferring the geolocation on Earth where an image is taken. Existing methods approach it either via grid-based classification or via image retrieval. Their performance significantly suffers when the spatial distribution of test images does not align with such choices. To address these limitations, we propose to leverage diffusion as a mechanism for image geolocalization. To avoid the problematic manifold reprojection step in diffusion, we developed a novel spherical positional encoding-decoding framework, which encodes points on a spherical surface (e.g., geolocations on Earth) into a Hilbert space of Spherical Harmonics coefficients and decodes points (geolocations) by mode-seeking. We call this type of position encoding Spherical Harmonics Dirac Delta (SHDD) Representation. We also propose a novel SirenNet-based architecture called CS-UNet to learn the conditional backward process in the latent SHDD space by minimizing a latent KL-divergence loss. We train a conditional latent diffusion model called LocDiffusion that generates geolocations under the guidance of images -- to the best of our knowledge, the first generative model for image geolocalization by diffusing geolocation information in a hidden location embedding space. We evaluate our method against SOTA image geolocalization baselines. LocDiffusion achieves competitive geolocalization performance and demonstrates significantly stronger generalizability to unseen geolocations. 

**Abstract (ZH)**: 图像地理定位是基础而又具有挑战性的任务，旨在推断图像拍摄地的地理位置。现有方法要么通过基于网格的分类，要么通过图像检索来实现这一目标。当测试图像的空间分布与这些选择不一致时，其性能会显著下降。为解决这些问题，我们提出利用扩散机制进行图像地理定位。为避免扩散过程中的有争议的流形重建步骤，我们开发了一种新颖的球面位置编码-解码框架，将球面上的点（例如，地球上的地理坐标）编码为球谐系数的希尔伯特空间，并通过模式寻找进行解码。我们将这种类型的位置编码称为球谐狄拉克δ表示（SHDD表示）。我们还提出了一种基于SirenNet的新型架构CS-UNet，通过最小化潜在的KL散度损失来学习潜在SHDD空间中的条件逆过程。我们训练了一个条件潜在扩散模型LocDiffusion，在图像的指导下生成地理坐标——据我们所知，这是首个通过在隐藏位置嵌入空间中扩散地理坐标信息来进行图像地理定位的生成模型。我们将我们的方法与当前最先进的图像地理定位基线进行评估。LocDiffusion在地理定位性能上取得了竞争力，并且在未知地理坐标的泛化能力上展现出显著的优势。 

---
# Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization 

**Title (ZH)**: 通过行为支持正则化减轻RLHF中的奖励过度优化 

**Authors**: Juntao Dai, Taiye Chen, Yaodong Yang, Qian Zheng, Gang Pan  

**Link**: [PDF](https://arxiv.org/pdf/2503.18130)  

**Abstract**: Reinforcement learning from human feedback (RLHF) is an effective method for aligning large language models (LLMs) with human values. However, reward over-optimization remains an open challenge leading to discrepancies between the performance of LLMs under the reward model and the true human objectives. A primary contributor to reward over-optimization is the extrapolation error that arises when the reward model evaluates out-of-distribution (OOD) responses. However, current methods still fail to prevent the increasing frequency of OOD response generation during the reinforcement learning (RL) process and are not effective at handling extrapolation errors from OOD responses. In this work, we propose the Behavior-Supported Policy Optimization (BSPO) method to mitigate the reward over-optimization issue. Specifically, we define behavior policy as the next token distribution of the reward training dataset to model the in-distribution (ID) region of the reward model. Building on this, we introduce the behavior-supported Bellman operator to regularize the value function, penalizing all OOD values without impacting the ID ones. Consequently, BSPO reduces the generation of OOD responses during the RL process, thereby avoiding overestimation caused by the reward model's extrapolation errors. Theoretically, we prove that BSPO guarantees a monotonic improvement of the supported policy until convergence to the optimal behavior-supported policy. Empirical results from extensive experiments show that BSPO outperforms baselines in preventing reward over-optimization due to OOD evaluation and finding the optimal ID policy. 

**Abstract (ZH)**: 基于行为支持的策略优化方法（BSPO）：对抗奖励过优化 

---
# GeoBenchX: Benchmarking LLMs for Multistep Geospatial Tasks 

**Title (ZH)**: GeoBenchX: 评估LLM在多步地理空间任务中的性能 

**Authors**: Varvara Krechetova, Denis Kochedykov  

**Link**: [PDF](https://arxiv.org/pdf/2503.18129)  

**Abstract**: In this paper, we establish a benchmark for evaluating large language models (LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners. We assess seven leading commercial LLMs (Sonnet 3.5 and 3.7, Haiku 3.5, Gemini 2.0, GPT-4o, GPT-4o mini, and o3-mini) using a simple tool-calling agent equipped with 23 geospatial functions. Our benchmark comprises tasks across four categories of increasing complexity, with both solvable and intentionally unsolvable tasks to test hallucination rejection. We develop an LLM-as-Judge evaluation framework to compare agent solutions against reference implementations. Results show Sonnet 3.5 and GPT-4o achieve the best overall performance, with Claude models excelling on solvable tasks while OpenAI models better identify unsolvable scenarios. We observe significant differences in token usage, with Anthropic models consuming substantially more tokens than competitors. Common errors include misunderstanding geometrical relationships, relying on outdated knowledge, and inefficient data manipulation. The resulting benchmark set, evaluation framework, and data generation pipeline are released as open-source resources, providing one more standardized method for ongoing evaluation of LLMs for GeoAI. 

**Abstract (ZH)**: 本文建立了评价大型语言模型（LLMs）在商业GIS practitioners相关的多步骤地理空间任务中的基准。我们使用一个简单的工具调用代理，配备23个地理空间功能，评估了七种领先的商用LLM（Sonnet 3.5和3.7、Haiku 3.5、Gemini 2.0、GPT-4o、GPT-4o mini和o3-mini）。我们的基准涵盖了四个复杂度逐步增加的任务类别，包括可解任务和故意设置的不可解任务，以测试幻觉拒绝能力。我们开发了一种LLM-as-Judge评估框架，将代理解决方案与参考实现进行对比。结果表明，Sonnet 3.5和GPT-4o在整体性能上最佳，Claude模型在可解任务上表现优异，而OpenAI模型在识别不可解场景方面表现更佳。我们观察到在token使用方面存在显著差异，Anthropic模型消耗的tokens远多于竞争对手。常见的错误包括对几何关系理解错误、依赖过时信息以及无效的数据操作。基准集、评估框架以及数据生成管道均被作为开源资源发布，提供了一种新的标准化方法，用于持续评价用于GeoAI的LLM。 

---
# Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach 

**Title (ZH)**: 临床文本中的时间关系提取：一种基于跨度的图变换器方法 

**Authors**: Rochana Chaturvedi, Peyman Baghershahi, Sourav Medya, Barbara Di Eugenio  

**Link**: [PDF](https://arxiv.org/pdf/2503.18085)  

**Abstract**: Temporal information extraction from unstructured text is essential for contextualizing events and deriving actionable insights, particularly in the medical domain. We address the task of extracting clinical events and their temporal relations using the well-studied I2B2 2012 Temporal Relations Challenge corpus. This task is inherently challenging due to complex clinical language, long documents, and sparse annotations. We introduce GRAPHTREX, a novel method integrating span-based entity-relation extraction, clinical large pre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT) to capture local and global dependencies. Our HGT component facilitates information propagation across the document through innovative global landmarks that bridge distant entities. Our method improves the state-of-the-art with 5.5% improvement in the tempeval $F_1$ score over the previous best and up to 8.9% improvement on long-range relations, which presents a formidable challenge. This work not only advances temporal information extraction but also lays the groundwork for improved diagnostic and prognostic models through enhanced temporal reasoning. 

**Abstract (ZH)**: 从无结构文本中提取时间信息对于事件的语境化和衍生行动洞见至关重要，特别是在医疗领域。我们使用广泛研究的I2B2 2012时间关系挑战语料库来解决提取临床事件及其时间关系的任务。该任务由于复杂的临床语言、长文档和稀疏注解而具有固有的挑战性。我们提出了GRAPHTREX，一种结合基于跨度的实体-关系提取、临床大型预训练语言模型（LPLMs）和异构图变换器（HGT）的新方法，以捕捉局部和全局依赖关系。我们的HGT组件通过创新的全局地标来促进在文档中远程实体之间的信息传播。我们的方法在tempeval $F_1$评分上相较于之前最佳方法提高了5.5%，在长距离关系上最多提高了8.9%，这构成了一个严峻的挑战。这项工作不仅促进了时间信息的提取，也为通过增强时间推理改进诊断和预后模型奠定了基础。 

---
# On the effectiveness of LLMs for automatic grading of open-ended questions in Spanish 

**Title (ZH)**: 关于大规模语言模型在自动批改西班牙语开放型问题中的有效性研究 

**Authors**: Germán Capdehourat, Isabel Amigo, Brian Lorenzo, Joaquín Trigo  

**Link**: [PDF](https://arxiv.org/pdf/2503.18072)  

**Abstract**: Grading is a time-consuming and laborious task that educators must face. It is an important task since it provides feedback signals to learners, and it has been demonstrated that timely feedback improves the learning process. In recent years, the irruption of LLMs has shed light on the effectiveness of automatic grading. In this paper, we explore the performance of different LLMs and prompting techniques in automatically grading short-text answers to open-ended questions. Unlike most of the literature, our study focuses on a use case where the questions, answers, and prompts are all in Spanish. Experimental results comparing automatic scores to those of human-expert evaluators show good outcomes in terms of accuracy, precision and consistency for advanced LLMs, both open and proprietary. Results are notably sensitive to prompt styles, suggesting biases toward certain words or content in the prompt. However, the best combinations of models and prompt strategies, consistently surpasses an accuracy of 95% in a three-level grading task, which even rises up to more than 98% when the it is simplified to a binary right or wrong rating problem, which demonstrates the potential that LLMs have to implement this type of automation in education applications. 

**Abstract (ZH)**: 自动批改西班牙语开放性问题短文本答案：不同LLM和提示技术的性能探索 

---
# Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation 

**Title (ZH)**: 未见之于所见：利用基础模型重写观察指令以增强视觉语言导航 

**Authors**: Ziming Wei, Bingqian Lin, Yunshuang Nie, Jiaqi Chen, Shikui Ma, Hang Xu, Xiaodan Liang  

**Link**: [PDF](https://arxiv.org/pdf/2503.18065)  

**Abstract**: Data scarcity is a long-standing challenge in the Vision-Language Navigation (VLN) field, which extremely hinders the generalization of agents to unseen environments. Previous works primarily rely on additional simulator data or web-collected images/videos to improve the generalization. However, the simulator environments still face limited diversity, and the web-collected data often requires extensive labor to remove the noise. In this paper, we propose a Rewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates the unseen observation-instruction pairs via rewriting human-annotated training data. Benefiting from our rewriting mechanism, new observation-instruction can be obtained in both simulator-free and labor-saving manners to promote generalization. Specifically, we first introduce Object-Enriched Observation Rewriting, where we combine Vision-Language Models (VLMs) and Large Language Models (LLMs) to derive rewritten object-enriched scene descriptions, enabling observation synthesis with diverse objects and spatial layouts via Text-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast Instruction Rewriting, which generates observation-aligned rewritten instructions by requiring LLMs to reason the difference between original and new observations. We further develop a mixing-then-focusing training strategy with a random observation cropping scheme, effectively enhancing data distribution diversity while suppressing augmentation data noise during training. Experiments on both the discrete environments (R2R, REVERIE, and R4R datasets) and continuous environments (R2R-CE dataset) show the superior performance and impressive generalization ability of our method. Code is available at this https URL. 

**Abstract (ZH)**: 基于重写驱动的扩增 paradigm在视觉语言导航中的应用：直接通过重写人类标注训练数据生成未见过的观察-指令对 

---
# Dynamic Task Vector Grouping for Efficient Multi-Task Prompt Tuning 

**Title (ZH)**: 动态任务向量分组以实现高效的多任务提示调优 

**Authors**: Pieyi Zhang, Richong Zhang, Zhijie Nie  

**Link**: [PDF](https://arxiv.org/pdf/2503.18063)  

**Abstract**: Multi-task prompt tuning utilizes multiple high-resource source tasks to improve performance on low-source target tasks. Existing approaches transfer the soft prompt trained by combining all source tasks or a single ``high-similar'' source task one-time-only. However, we find that the optimal transfer performance often comes from a combination of source tasks, which is neither one nor all. Further, we find that the similarity between source and target tasks also changes dynamically during fine-tuning after transfering, making similarity calculation in the initiation stage inadequate. To address these issues, we propose a method called Dynamic Task Vector Grouping (DTVG), whose core ideas contain (1) measuring the task similarity with task vectors instead of soft prompt, (2) grouping the optimal source task combination based on two metrics: {\it target similarity} and {\it knowledge consistency}; (3) dynamically updating the combination in each iteration step. Extensive experiments on the 26 NLP datasets under different settings demonstrate that DTVG effectively groups similar source tasks while reducing negative transfer, achieving the start-of-art performance. 

**Abstract (ZH)**: 动态任务向量分组方法（DTVG）在不同设置下的26个NLP数据集上的广泛实验表明，DTVG能够有效分组相似源任务的同时减少负迁移，取得业界最佳性能。 

---
# Decision from Suboptimal Classifiers: Excess Risk Pre- and Post-Calibration 

**Title (ZH)**: 从次优分类器的决策：校准前后超过风险解析 

**Authors**: Alexandre Perez-Lebel, Gael Varoquaux, Sanmi Koyejo, Matthieu Doutreligne, Marine Le Morvan  

**Link**: [PDF](https://arxiv.org/pdf/2503.18025)  

**Abstract**: Probabilistic classifiers are central for making informed decisions under uncertainty. Based on the maximum expected utility principle, optimal decision rules can be derived using the posterior class probabilities and misclassification costs. Yet, in practice only learned approximations of the oracle posterior probabilities are available. In this work, we quantify the excess risk (a.k.a. regret) incurred using approximate posterior probabilities in batch binary decision-making. We provide analytical expressions for miscalibration-induced regret ($R^{\mathrm{CL}}$), as well as tight and informative upper and lower bounds on the regret of calibrated classifiers ($R^{\mathrm{GL}}$). These expressions allow us to identify regimes where recalibration alone addresses most of the regret, and regimes where the regret is dominated by the grouping loss, which calls for post-training beyond recalibration. Crucially, both $R^{\mathrm{CL}}$ and $R^{\mathrm{GL}}$ can be estimated in practice using a calibration curve and a recent grouping loss estimator. On NLP experiments, we show that these quantities identify when the expected gain of more advanced post-training is worth the operational cost. Finally, we highlight the potential of multicalibration approaches as efficient alternatives to costlier fine-tuning approaches. 

**Abstract (ZH)**: 概率分类器在不确定性下进行 informed 决策中至关重要。基于最大期望效用原则，可以利用后验类概率和误分类成本推导出最优决策规则。然而，在实践中只能获得后验概率的近似值。在本文中，我们量化了使用近似后验概率进行批处理二元决策时产生的 excess risk（亦称为 regret）。我们提供了 miscalibration 引发的 regret ($R^{\mathrm{CL}}$) 的解析表达式，以及校准分类器 ($R^{\mathrm{GL}}$) 的 regret 的紧致且信息丰富的上界和下界。这些表达式使我们能够识别出仅通过重新校准可以解决大部分 regret 的情况，以及 regret 主要由 grouping loss 控制的情况，后者需要超出重新校准的后续训练。关键的是，$R^{\mathrm{CL}}$ 和 $R^{\mathrm{GL}}$ 在实践中都可以通过校准曲线和最近的 grouping loss 估计器进行估计。在 NLP 实验中，我们展示了这些量度可以识别出更高级的后续训练带来的期望收益是否值得运营成本。最后，我们强调了多校准方法作为成本更低的微调方法替代方案的潜力。 

---
# Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning 

**Title (ZH)**: Vision-R1：通过视觉引导 reinforcement 学习进化大规模视觉-语言模型中的无监督对齐 

**Authors**: Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, Jinqiao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2503.18013)  

**Abstract**: Large Vision-Language Models (LVLMs) typically follow a two-stage training paradigm-pretraining and supervised fine-tuning. Recently, preference optimization, derived from the language domain, has emerged as an effective post-training reinforcement strategy to enhance capabilities of LVLMs. However, constructing high-quality human-annotated preference data and developing robust reward models to mimic these preferences are both costly and challenging. Motivated by this observation, we propose Vision-R1, a novel vision-guided R1-like reinforcement learning algorithm for LVLMs that rewards models with definitive vision feedback. It only leverages curated instruction data, eliminating the need for specialized reward models and handcrafted preference datasets. We incorporate a criterion-driven reward function that further integrates multi-dimensional feedback to evaluate model completions comprehensively based on the vision task logic. Furthermore, we introduce a progressive rule refinement strategy that dynamically adjusts the reward criteria during training, enabling continuous model improvement and mitigating reward hacking. Extensive experiments on both in-distribution and out-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with Vision-R1 achieves consistent performance gains, with even up to 50% improvement and surpassing the state-of-the-art 10x size model. 

**Abstract (ZH)**: Large Vision-Language Models (LVLMs) typically follow a two-stage training paradigm-pretraining and supervised fine-tuning. Recently, preference optimization, derived from the language domain, has emerged as an effective post-training reinforcement strategy to enhance capabilities of LVLMs. However, constructing high-quality human-annotated preference data and developing robust reward models to mimic these preferences are both costly and challenging. Motivated by this observation, we propose Vision-R1, a novel vision-guided R1-like reinforcement learning algorithm for LVLMs that rewards models with definitive vision feedback. It only leverages curated instruction data, eliminating the need for specialized reward models and handcrafted preference datasets. We incorporate a criterion-driven reward function that further integrates multi-dimensional feedback to evaluate model completions comprehensively based on the vision task logic. Furthermore, we introduce a progressive rule refinement strategy that dynamically adjusts the reward criteria during training, enabling continuous model improvement and mitigating reward hacking. Extensive experiments on both in-distribution and out-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with Vision-R1 achieves consistent performance gains, with even up to 50% improvement and surpassing the state-of-the-art 10x size model. 

---
# Neuromorphic Principles for Efficient Large Language Models on Intel Loihi 2 

**Title (ZH)**: Intel Loihi 2 上高效大型语言模型的神经形态原理 

**Authors**: Steven Abreu, Sumit Bam Shrestha, Rui-Jie Zhu, Jason Eshraghian  

**Link**: [PDF](https://arxiv.org/pdf/2503.18002)  

**Abstract**: Large language models (LLMs) deliver impressive performance but require large amounts of energy. In this work, we present a MatMul-free LLM architecture adapted for Intel's neuromorphic processor, Loihi 2. Our approach leverages Loihi 2's support for low-precision, event-driven computation and stateful processing. Our hardware-aware quantized model on GPU demonstrates that a 370M parameter MatMul-free model can be quantized with no accuracy loss. Based on preliminary results, we report up to 3x higher throughput with 2x less energy, compared to transformer-based LLMs on an edge GPU, with significantly better scaling. Further hardware optimizations will increase throughput and decrease energy consumption. These results show the potential of neuromorphic hardware for efficient inference and pave the way for efficient reasoning models capable of generating complex, long-form text rapidly and cost-effectively. 

**Abstract (ZH)**: Large Language Models (LLMs)展现出卓越的性能但需要大量能量。本文提出了一种适用于Intel neuromorphic处理器Loihi 2的MatMul-free LLM架构。我们的方法利用了Loihi 2对低精度、事件驱动计算和状态处理的支持。基于GPU上的硬件感知量化模型表明，一个370M参数的MatMul-free模型可以实现无精度损失的量化。初步结果显示，与边缘GPU上的基于Transformer的LLMs相比，我们的架构在吞吐量上最高可提高3倍，在能耗上减少50%，并且具有显著更好的扩展性。进一步的硬件优化将增加吞吐量并降低能耗。这些结果展示了神经形态硬件在高效推断中的潜力，并为能够快速、经济高效地生成复杂、长篇文本的有效推理模型铺平了道路。 

---
# Instructing the Architecture Search for Spatial-temporal Sequence Forecasting with LLM 

**Title (ZH)**: 基于LLM的时空序列 Forecasting 架构搜索指导 

**Authors**: Xin Xue, Haoyi Zhou, Tianyu Chen, Shuai Zhang, Yizhou Long, Jianxin Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.17994)  

**Abstract**: Spatial-temporal sequence forecasting (STSF) is a long-standing research problem with widespread real-world applications. Neural architecture search (NAS), which automates the neural network design, has been shown effective in tackling the STSF problem. However, the existing NAS methods for STSF focus on generating architectures in a time-consuming data-driven fashion, which heavily limits their ability to use background knowledge and explore the complicated search trajectory. Large language models (LLMs) have shown remarkable ability in decision-making with comprehensive internal world knowledge, but how it could benefit NAS for STSF remains unexplored. In this paper, we propose a novel NAS method for STSF based on LLM. Instead of directly generate architectures with LLM, We inspire the LLM's capability with a multi-level enhancement mechanism. Specifically, on the step-level, we decompose the generation task into decision steps with powerful prompt engineering and inspire LLM to serve as instructor for architecture search based on its internal knowledge. On the instance-level, we utilize a one-step tuning framework to quickly evaluate the architecture instance and a memory bank to cumulate knowledge to improve LLM's search ability. On the task-level, we propose a two-stage architecture search, balancing the exploration stage and optimization stage, to reduce the possibility of being trapped in local optima. Extensive experimental results demonstrate that our method can achieve competitive effectiveness with superior efficiency against existing NAS methods for STSF. 

**Abstract (ZH)**: 基于大规模语言模型的时空序列预测神经架构搜索方法 

---
# Predicting Multitasking in Manual and Automated Driving with Optimal Supervisory Control 

**Title (ZH)**: 基于最优监督控制的 Manual 和 Automated 驾驶中的 multitasking 预测 

**Authors**: Jussi Jokinen, Patrick Ebel, Tuomo Kujala  

**Link**: [PDF](https://arxiv.org/pdf/2503.17993)  

**Abstract**: Modern driving involves interactive technologies that can divert attention, increasing the risk of accidents. This paper presents a computational cognitive model that simulates human multitasking while driving. Based on optimal supervisory control theory, the model predicts how multitasking adapts to variations in driving demands, interactive tasks, and automation levels. Unlike previous models, it accounts for context-dependent multitasking across different degrees of driving automation. The model predicts longer in-car glances on straight roads and shorter glances during curves. It also anticipates increased glance durations with driver aids such as lane-centering assistance and their interaction with environmental demands. Validated against two empirical datasets, the model offers insights into driver multitasking amid evolving in-car technologies and automation. 

**Abstract (ZH)**: 现代驾驶涉及可以分散注意力的交互技术，增加了事故风险。本文提出了一种计算认知模型，模拟驾驶中的多任务处理。该模型基于最优监督控制理论，预测多任务处理如何适应驾驶需求、交互任务以及自动化水平的变化。与之前模型不同，它考虑了不同驾驶自动化程度下的情境依赖性多任务处理。模型预测直路上更长的车内注视时间和弯道中更短的注视时间。它还预测了车道保持辅助等驾驶辅助工具及其与环境需求交互时注视时间增加的情况。该模型在两个实验数据集上得到了验证，提供了关于不断演变的车内技术和自动化背景下驾驶员多任务处理的见解。 

---
# Metaphor-based Jailbreaking Attacks on Text-to-Image Models 

**Title (ZH)**: 基于隐喻的文本生成图像模型越狱攻击 

**Authors**: Chenyu Zhang, Yiwen Ma, Lanjun Wang, Wenhui Li, Yi Tu, An-An Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.17987)  

**Abstract**: To mitigate misuse, text-to-image~(T2I) models commonly incorporate safety filters to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attack methods use LLMs to generate adversarial prompts that effectively bypass safety filters while generating sensitive images, revealing the safety vulnerabilities within the T2I model. However, existing LLM-based attack methods lack explicit guidance, relying on substantial queries to achieve a successful attack, which limits their practicality in real-world scenarios. In this work, we introduce \textbf{MJA}, a \textbf{m}etaphor-based \textbf{j}ailbreaking \textbf{a}ttack method inspired by the Taboo game, aiming to balance the attack effectiveness and query efficiency by generating metaphor-based adversarial prompts. Specifically, MJA consists of two modules: an LLM-based multi-agent generation module~(MLAG) and an adversarial prompt optimization module~(APO). MLAG decomposes the generation of metaphor-based adversarial prompts into three subtasks: metaphor retrieval, context matching, and adversarial prompt generation. Subsequently, MLAG coordinates three LLM-based agents to generate diverse adversarial prompts by exploring various metaphors and contexts. To enhance the attack efficiency, APO first trains a surrogate model to predict the attack results of adversarial prompts and then designs an acquisition strategy to adaptively identify optimal adversarial prompts. Experiments demonstrate that MJA achieves better attack effectiveness while requiring fewer queries compared to baseline methods. Moreover, our adversarial prompts exhibit strong transferability across various open-source and commercial T2I models. \textcolor{red}{This paper includes model-generated content that may contain offensive or distressing material.} 

**Abstract (ZH)**: 基于隐喻的 Jailbreaking 攻击方法（MJA）：一种平衡攻击效果与查询效率的方法 

---
# Optimizing Navigation And Chemical Application in Precision Agriculture With Deep Reinforcement Learning And Conditional Action Tree 

**Title (ZH)**: 基于深度强化学习和条件动作树的精准农业导航与化学应用优化 

**Authors**: Mahsa Khosravi, Zhanhong Jiang, Joshua R Waite, Sarah Jonesc, Hernan Torres, Arti Singh, Baskar Ganapathysubramanian, Asheesh Kumar Singh, Soumik Sarkar  

**Link**: [PDF](https://arxiv.org/pdf/2503.17985)  

**Abstract**: This paper presents a novel reinforcement learning (RL)-based planning scheme for optimized robotic management of biotic stresses in precision agriculture. The framework employs a hierarchical decision-making structure with conditional action masking, where high-level actions direct the robot's exploration, while low-level actions optimize its navigation and efficient chemical spraying in affected areas. The key objectives of optimization include improving the coverage of infected areas with limited battery power and reducing chemical usage, thus preventing unnecessary spraying of healthy areas of the field. Our numerical experimental results demonstrate that the proposed method, Hierarchical Action Masking Proximal Policy Optimization (HAM-PPO), significantly outperforms baseline practices, such as LawnMower navigation + indiscriminate spraying (Carpet Spray), in terms of yield recovery and resource efficiency. HAM-PPO consistently achieves higher yield recovery percentages and lower chemical costs across a range of infection scenarios. The framework also exhibits robustness to observation noise and generalizability under diverse environmental conditions, adapting to varying infection ranges and spatial distribution patterns. 

**Abstract (ZH)**: 一种基于强化学习的分级行动遮罩规划方案，用于精确农业中生物胁迫的优化机器人管理 

---
# Taste More, Taste Better: Diverse Data and Strong Model Boost Semi-Supervised Crowd Counting 

**Title (ZH)**: 味道更丰富，精度更高：多样数据与强模型助推半监督人群计数 

**Authors**: Maochen Yang, Zekun Li, Jian Zhang, Lei Qi, Yinghuan Shi  

**Link**: [PDF](https://arxiv.org/pdf/2503.17984)  

**Abstract**: Semi-supervised crowd counting is crucial for addressing the high annotation costs of densely populated scenes. Although several methods based on pseudo-labeling have been proposed, it remains challenging to effectively and accurately utilize unlabeled data. In this paper, we propose a novel framework called Taste More Taste Better (TMTB), which emphasizes both data and model aspects. Firstly, we explore a data augmentation technique well-suited for the crowd counting task. By inpainting the background regions, this technique can effectively enhance data diversity while preserving the fidelity of the entire scenes. Secondly, we introduce the Visual State Space Model as backbone to capture the global context information from crowd scenes, which is crucial for extremely crowded, low-light, and adverse weather scenarios. In addition to the traditional regression head for exact prediction, we employ an Anti-Noise classification head to provide less exact but more accurate supervision, since the regression head is sensitive to noise in manual annotations. We conduct extensive experiments on four benchmark datasets and show that our method outperforms state-of-the-art methods by a large margin. Code is publicly available on this https URL. 

**Abstract (ZH)**: 半监督人群计数对于解决密集人群场景的高标注成本至关重要。尽管已经提出了一些基于伪标签的方法，但仍难以有效准确地利用未标注数据。在本文中，我们提出了一种名为Taste More Taste Better (TMTB)的新框架，强调数据和模型两个方面。首先，我们探索了一种适用于人群计数任务的数据增强技术。通过修复背景区域，该技术可以有效增强数据多样性的同时保持整个场景的保真度。其次，我们引入了Visual State Space Model作为骨干网络，以从人群场景中捕获全局上下文信息，这对于极 crowded、低光和恶劣天气场景至关重要。除了传统的回归头进行精确预测外，我们还采用了Anti-Noise分类头提供较少精确但更准确的监督，因为回归头对手动标注中的噪声敏感。我们在四个基准数据集上进行了广泛的实验，并显示我们的方法在多个指标上显著优于现有最先进的方法。代码已在以下网址公开：此https URL。 

---
# Co-SemDepth: Fast Joint Semantic Segmentation and Depth Estimation on Aerial Images 

**Title (ZH)**: 共语义深度：快速联合空域图像语义分割与深度估计 

**Authors**: Yara AlaaEldin, Francesca Odone  

**Link**: [PDF](https://arxiv.org/pdf/2503.17982)  

**Abstract**: Understanding the geometric and semantic properties of the scene is crucial in autonomous navigation and particularly challenging in the case of Unmanned Aerial Vehicle (UAV) navigation. Such information may be by obtained by estimating depth and semantic segmentation maps of the surrounding environment and for their practical use in autonomous navigation, the procedure must be performed as close to real-time as possible. In this paper, we leverage monocular cameras on aerial robots to predict depth and semantic maps in low-altitude unstructured environments. We propose a joint deep-learning architecture that can perform the two tasks accurately and rapidly, and validate its effectiveness on MidAir and Aeroscapes benchmark datasets. Our joint-architecture proves to be competitive or superior to the other single and joint architecture methods while performing its task fast predicting 20.2 FPS on a single NVIDIA quadro p5000 GPU and it has a low memory footprint. All codes for training and prediction can be found on this link: this https URL 

**Abstract (ZH)**: 理解场景的几何和语义特性对于自主导航至关重要，特别是在无人飞行器(UAV)导航中更具挑战性。通过估算周围环境的深度和语义分割图，并将其用于自主导航，该过程必须尽可能接近实时进行。本文利用空中机器人上的单目相机在低空未结构化环境中预测深度和语义地图。我们提出了一种联合深度学习架构，可以在准确和快速地执行两项任务的同时得到验证，其在MidAir和Aeroscapes基准数据集上的有效性得到了验证。我们的联合架构在速度方面表现出色，每秒预测帧数达到20.2 FPS，且内存占用低。所有用于训练和预测的代码可以在以下链接找到：this https URL。 

---
# PIM: Physics-Informed Multi-task Pre-training for Improving Inertial Sensor-Based Human Activity Recognition 

**Title (ZH)**: 基于物理约束的多任务预训练以改进惯性传感器人体活动识别 

**Authors**: Dominique Nshimyimana, Vitor Fortes Rey, Sungho Suh, Bo Zhou, Paul Lukowicz  

**Link**: [PDF](https://arxiv.org/pdf/2503.17978)  

**Abstract**: Human activity recognition (HAR) with deep learning models relies on large amounts of labeled data, often challenging to obtain due to associated cost, time, and labor. Self-supervised learning (SSL) has emerged as an effective approach to leverage unlabeled data through pretext tasks, such as masked reconstruction and multitask learning with signal processing-based data augmentations, to pre-train encoder models. However, such methods are often derived from computer vision approaches that disregard physical mechanisms and constraints that govern wearable sensor data and the phenomena they reflect. In this paper, we propose a physics-informed multi-task pre-training (PIM) framework for IMU-based HAR. PIM generates pre-text tasks based on the understanding of basic physical aspects of human motion: including movement speed, angles of movement, and symmetry between sensor placements. Given a sensor signal, we calculate corresponding features using physics-based equations and use them as pretext tasks for SSL. This enables the model to capture fundamental physical characteristics of human activities, which is especially relevant for multi-sensor systems. Experimental evaluations on four HAR benchmark datasets demonstrate that the proposed method outperforms existing state-of-the-art methods, including data augmentation and masked reconstruction, in terms of accuracy and F1 score. We have observed gains of almost 10\% in macro f1 score and accuracy with only 2 to 8 labeled examples per class and up to 3% when there is no reduction in the amount of training data. 

**Abstract (ZH)**: 基于物理约束的多任务预训练框架（PIM）用于惯性传感器的人体活动识别 

---
# Shot Sequence Ordering for Video Editing: Benchmarks, Metrics, and Cinematology-Inspired Computing Methods 

**Title (ZH)**: 视频编辑中的镜头序列排序：基准、评估指标及	cinematology-启发式计算方法 

**Authors**: Yuzhi Li, Haojun Xu, Feng Tian  

**Link**: [PDF](https://arxiv.org/pdf/2503.17975)  

**Abstract**: With the rising popularity of short video platforms, the demand for video production has increased substantially. However, high-quality video creation continues to rely heavily on professional editing skills and a nuanced understanding of visual language. To address this challenge, the Shot Sequence Ordering (SSO) task in AI-assisted video editing has emerged as a pivotal approach for enhancing video storytelling and the overall viewing experience. Nevertheless, the progress in this field has been impeded by a lack of publicly available benchmark datasets. In response, this paper introduces two novel benchmark datasets, AVE-Order and ActivityNet-Order. Additionally, we employ the Kendall Tau distance as an evaluation metric for the SSO task and propose the Kendall Tau Distance-Cross Entropy Loss. We further introduce the concept of Cinematology Embedding, which incorporates movie metadata and shot labels as prior knowledge into the SSO model, and constructs the AVE-Meta dataset to validate the method's effectiveness. Experimental results indicate that the proposed loss function and method substantially enhance SSO task accuracy. All datasets are publicly accessible at this https URL. 

**Abstract (ZH)**: 随着短视频平台的兴起，对视频制作的需求大幅增加。然而，高质量视频创作仍然高度依赖专业的编辑技巧和对视觉语言的细微理解。为此，人工智能辅助视频编辑中的镜头序列排序（Shot Sequence Ordering, SSO）任务成为了提升视频叙事能力和整体观感的关键方法。然而，领域进展受限于缺乏公开的基准数据集。针对这一问题，本文提出了两个新的基准数据集，AVE-Order和ActivityNet-Order。此外，我们使用Kendall Tau距离作为SSO任务的评估指标，并提出了Kendall Tau距离-交叉熵损失。为进一步优化，我们引入了电影学嵌入的概念，将电影元数据和镜头标签的先验知识融入到SSO模型中，并构建AVE-Meta数据集以验证方法的有效性。实验结果表明，所提出的损失函数和方法显著提高了SSO任务的准确性。所有数据集均可通过此链接访问。 

---
# PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos 

**Title (ZH)**: PhysTwin: 带有物理约束的可变形物体视频重构与模拟 

**Authors**: Hanxiao Jiang, Hao-Yu Hsu, Kaifeng Zhang, Hsin-Ni Yu, Shenlong Wang, Yunzhu Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.17973)  

**Abstract**: Creating a physical digital twin of a real-world object has immense potential in robotics, content creation, and XR. In this paper, we present PhysTwin, a novel framework that uses sparse videos of dynamic objects under interaction to produce a photo- and physically realistic, real-time interactive virtual replica. Our approach centers on two key components: (1) a physics-informed representation that combines spring-mass models for realistic physical simulation, generative shape models for geometry, and Gaussian splats for rendering; and (2) a novel multi-stage, optimization-based inverse modeling framework that reconstructs complete geometry, infers dense physical properties, and replicates realistic appearance from videos. Our method integrates an inverse physics framework with visual perception cues, enabling high-fidelity reconstruction even from partial, occluded, and limited viewpoints. PhysTwin supports modeling various deformable objects, including ropes, stuffed animals, cloth, and delivery packages. Experiments show that PhysTwin outperforms competing methods in reconstruction, rendering, future prediction, and simulation under novel interactions. We further demonstrate its applications in interactive real-time simulation and model-based robotic motion planning. 

**Abstract (ZH)**: 创建现实世界物体的物理数字孪生在机器人学、内容创作和XR领域具有巨大的潜力。本文我们提出PhysTwin，一种新颖的框架，利用交互下动态对象的稀疏视频生成高照片和物理真实感的实时互动虚拟复制品。该方法的核心在于两个关键组件：（1）一种物理信息驱动的表示，结合弹簧质量模型进行真实的物理模拟、生成形状模型进行几何建模以及高斯斑点进行渲染；（2）一种新颖的多阶段优化反向建模框架，用于重建完整的几何结构、推断密集的物理属性以及从视频中复制真实的外观。该方法将物理反向框架与视觉感知线索结合起来，能够在部分、被遮挡和有限视角的情况下实现高保真重建。PhysTwin 支持建模各种可变形对象，包括绳索、填充动物玩具、布料和送货包裹。实验表明，PhysTwin 在重建、渲染、未来预测和新交互下的模拟方面优于竞争对手的方法。我们进一步展示了其在实时互动模拟和基于模型的机器人运动规划中的应用。 

---
# Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts 

**Title (ZH)**: 理解RLHF对LLM生成文本的质量和可检测性的影响 

**Authors**: Beining Xu, Arkaitz Zubiaga  

**Link**: [PDF](https://arxiv.org/pdf/2503.17965)  

**Abstract**: Large Language Models (LLMs) have demonstrated exceptional performance on a range of downstream NLP tasks by generating text that closely resembles human writing. However, the ease of achieving this similarity raises concerns from potential malicious uses at scale by bad actors, as LLM-generated text becomes increasingly difficult to discern from human text. Although detection methods have been developed to address this issue, bad actors can further manipulate LLM-generated texts to make them less detectable. In this work, we study how further editing texts with Reinforcement Learning from Human Feedback (RLHF), which aligns model outputs with human preferences, affects (a) the quality of generated texts for two tasks, and (b) the performance of LLM-generated text detectors, looking at both training-based and zero-shot detection methods. Although RLHF improves the quality of LLM-generated texts, we find that it also tends to produce more detectable, lengthy, and repetitive outputs. Additionally, we observe that training-based detectors are vulnerable to short texts and to texts that incorporate code, whereas zero-shot detectors exhibit greater robustness. 

**Abstract (ZH)**: 大型语言模型（LLMs）在一系列下游自然语言处理任务中通过生成与人类写作高度相似的文本展现了出色性能。然而，这种相似性的轻易获取引发了恶意行为者在大规模应用中潜在的恶意使用担忧，因为由LLM生成的文本越来越难以与人类文本区分。尽管已经开发了检测方法来应对这一问题，但恶意行为者可以通过进一步编辑LLM生成的文本以降低其可检测性。在本研究中，我们探讨了使用人类反馈强化学习（RLHF）进一步编辑文本如何影响（a）两种任务下生成文本的质量，以及（b）LLM生成文本检测器的性能，同时考虑基于训练和零样本检测方法。尽管RLHF能够提升LLM生成文本的质量，我们发现它也倾向于生成更具可检测性、更长且更具重复性的输出。此外，我们观察到基于训练的检测器对短文本和包含代码的文本较为脆弱，而零样本检测器表现出更高的鲁棒性。 

---
# Dynamic Gradient Sparse Update for Edge Training 

**Title (ZH)**: 边缘训练中的动态梯度稀疏更新 

**Authors**: I-Hsuan Li, Tian-Sheuan Chang  

**Link**: [PDF](https://arxiv.org/pdf/2503.17959)  

**Abstract**: Training on edge devices enables personalized model fine-tuning to enhance real-world performance and maintain data privacy. However, the gradient computation for backpropagation in the training requires significant memory buffers to store intermediate features and compute losses. This is unacceptable for memory-constrained edge devices such as microcontrollers. To tackle this issue, we propose a training acceleration method using dynamic gradient sparse updates. This method updates the important channels and layers only and skips gradient computation for the less important channels and layers to reduce memory usage for each update iteration. In addition, the channel selection is dynamic for different iterations to traverse most of the parameters in the update layers along the time dimension for better performance. The experimental result shows that the proposed method enables an ImageNet pre-trained MobileNetV2 trained on CIFAR-10 to achieve an accuracy of 85.77\% while updating only 2\% of convolution weights within 256KB on-chip memory. This results in a remarkable 98\% reduction in feature memory usage compared to dense model training. 

**Abstract (ZH)**: 在边缘设备上训练通过动态梯度稀疏更新实现个性化的模型微调，以增强实际性能并保持数据隐私。然而，训练中的反向传播梯度计算需要大量内存缓冲区来存储中间特征和计算损失函数。对于内存受限的边缘设备如微控制器来说，这是无法接受的。为了解决这个问题，我们提出了一种使用动态梯度稀疏更新的训练加速方法。该方法仅更新重要的通道和层，并跳过不重要的通道和层的梯度计算，以减少每次更新迭代的内存使用量。此外，通道选择在不同的迭代中是动态的，沿着时间维度遍历更新层中的大多数参数以获得更好的性能。实验结果表明，所提出的方法使在CIFAR-10上预训练的ImageNet先验MobileNetV2在256KB片内内存中仅更新2%的卷积权重时实现了85.77%的精度。与密集模型训练相比，这导致了特征内存使用量减少了98%。 

---
# Human-AI Interaction and User Satisfaction: Empirical Evidence from Online Reviews of AI Products 

**Title (ZH)**: 人类-人工智能交互与用户满意度：来自AI产品在线评价的实证证据 

**Authors**: Stefan Pasch, Sun-Young Ha  

**Link**: [PDF](https://arxiv.org/pdf/2503.17955)  

**Abstract**: Human-AI Interaction (HAI) guidelines and design principles have become increasingly important in both industry and academia to guide the development of AI systems that align with user needs and expectations. However, large-scale empirical evidence on how HAI principles shape user satisfaction in practice remains limited. This study addresses that gap by analyzing over 100,000 user reviews of AI-related products from this http URL, a leading review platform for business software and services. Based on widely adopted industry guidelines, we identify seven core HAI dimensions and examine their coverage and sentiment within the reviews. We find that the sentiment on four HAI dimensions-adaptability, customization, error recovery, and security-is positively associated with overall user satisfaction. Moreover, we show that engagement with HAI dimensions varies by professional background: Users with technical job roles are more likely to discuss system-focused aspects, such as reliability, while non-technical users emphasize interaction-focused features like customization and feedback. Interestingly, the relationship between HAI sentiment and overall satisfaction is not moderated by job role, suggesting that once an HAI dimension has been identified by users, its effect on satisfaction is consistent across job roles. 

**Abstract (ZH)**: 基于用户评价的大规模实证分析：理解人机交互原则对用户满意度的影响 

---
# Physics-Guided Multi-Fidelity DeepONet for Data-Efficient Flow Field Prediction 

**Title (ZH)**: 基于物理引导的多保真度DeepONet在高效流场预测中的应用 

**Authors**: Sunwoong Yang, Youngkyu Lee, Namwoo Kang  

**Link**: [PDF](https://arxiv.org/pdf/2503.17941)  

**Abstract**: This study presents an enhanced multi-fidelity deep operator network (DeepONet) framework for efficient spatio-temporal flow field prediction, with particular emphasis on practical scenarios where high-fidelity data is scarce. We introduce several key innovations to improve the framework's efficiency and accuracy. First, we enhance the DeepONet architecture by incorporating a merge network that enables more complex feature interactions between operator and coordinate spaces, achieving a 50.4% reduction in prediction error compared to traditional dot-product operations. We further optimize the architecture through temporal positional encoding and point-based sampling strategies, achieving a 7.57% improvement in prediction accuracy while reducing training time by 96% through efficient sampling and automatic mixed precision training. Building upon this foundation, we develop a transfer learning-based multi-fidelity framework that leverages knowledge from pre-trained low-fidelity models to guide high-fidelity predictions. Our approach freezes the pre-trained branch and trunk networks while making only the merge network trainable during high-fidelity training, preserving valuable low-fidelity representations while efficiently adapting to high-fidelity features. Through systematic investigation, we demonstrate that this fine-tuning strategy not only significantly outperforms linear probing and full-tuning alternatives but also surpasses conventional multi-fidelity frameworks by up to 76%, while achieving up to 43.7% improvement in prediction accuracy compared to single-fidelity training. The core contribution lies in our novel time-derivative guided sampling approach: it maintains prediction accuracy equivalent to models trained with the full dataset while requiring only 60% of the original high-fidelity samples. 

**Abstract (ZH)**: 本研究提出了一种增强的多保真度深操作网络（DeepONet）框架，用于高效的时空流场预测，特别是在高保真数据稀缺的实际场景中。我们引入了几项关键创新来提高框架的效率和精度。首先，通过引入合并网络，增强了DeepONet架构，使其能够在操作空间和坐标空间之间实现更复杂的特征交互，相比传统点积操作，预测误差降低了50.4%。我们进一步通过时间位置编码和基于点的采样策略优化了架构，在保持84%的训练时间同时，通过高效的采样和自动混合精度训练，预测精度提高了7.57%。在此基础上，我们开发了一种基于迁移学习的多保真度框架，利用预训练的低保真模型知识指导高保真预测。我们的方法在高保真训练时仅使合并网络可训练，冻结预训练的分支网络和主干网络，从而保留有价值的低保真表示并高效适应高保真特征。通过系统研究，我们证明了这种微调策略不仅显著优于线性探针和全微调的替代方案，也超越了传统的多保真度框架，最多提高了76%，同时在预测精度上相比单保真训练提高了43.7%。核心贡献在于我们提出的一种新型时间导数引导的采样方法：它在保留与全数据集训练模型相当的预测精度的同时，仅需要原始高保真样本的60%。 

---
# An Empirical Study of the Role of Incompleteness and Ambiguity in Interactions with Large Language Models 

**Title (ZH)**: 不完备性和模糊性在与大型语言模型互动中的作用 empirical研究 

**Authors**: Riya Naik, Ashwin Srinivasan, Estrid He, Swati Agarwal  

**Link**: [PDF](https://arxiv.org/pdf/2503.17936)  

**Abstract**: Natural language as a medium for human-computer interaction has long been anticipated, has been undergoing a sea-change with the advent of Large Language Models (LLMs) with startling capacities for processing and generating language. Many of us now treat LLMs as modern-day oracles, asking it almost any kind of question. Unlike its Delphic predecessor, consulting an LLM does not have to be a single-turn activity (ask a question, receive an answer, leave); and -- also unlike the Pythia -- it is widely acknowledged that answers from LLMs can be improved with additional context. In this paper, we aim to study when we need multi-turn interactions with LLMs to successfully get a question answered; or conclude that a question is unanswerable. We present a neural symbolic framework that models the interactions between human and LLM agents. Through the proposed framework, we define incompleteness and ambiguity in the questions as properties deducible from the messages exchanged in the interaction, and provide results from benchmark problems, in which the answer-correctness is shown to depend on whether or not questions demonstrate the presence of incompleteness or ambiguity (according to the properties we identify). Our results show multi-turn interactions are usually required for datasets which have a high proportion of incompleteness or ambiguous questions; and that that increasing interaction length has the effect of reducing incompleteness or ambiguity. The results also suggest that our measures of incompleteness and ambiguity can be useful tools for characterising interactions with an LLM on question-answeringproblems 

**Abstract (ZH)**: 自然语言作为人机交互的媒介，随着大型语言模型（LLMs）的出现而发生了翻天覆地的变化，LLMs具备令人惊讶的语言处理和生成能力。如今，我们中的许多人将LLMs视为现代先知，几乎可以提出各种问题。与德尔斐先知不同，咨询LLMs不一定要一次完成（提问，得到回答，离开）；此外，与先知Pythia不同，人们普遍认为，可以通过提供额外的上下文来改进LLMs的回答。本文旨在研究何时需要与LLMs进行多轮交互以成功回答问题；或者得出问题无法回答的结论。我们提出了一种神经符号框架，该框架可用于建模人类与LLMs代理之间的交互。通过提出的框架，我们将问题的不完整性和模糊性定义为可通过交互中交换的消息推断出的属性，并提供基准问题的结果，表明答案的正确性取决于问题是否体现出不完整性和模糊性（根据我们识别的属性）。我们的结果表明，对于具有高比例不完整或模糊问题的数据集，通常需要多轮交互；并且增加交互长度会减少不完整性和模糊性。结果还表明，我们对不完整性和模糊性的度量可以作为表征与LLM在问题回答交互中的有用工具。 

---
# Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA 

**Title (ZH)**: 基于电子健康记录的体验检索增强 Enables 准确的出院质量评估 

**Authors**: Justice Ou, Tinglin Huang, Yilun Zhao, Ziyang Yu, Peiqing Lu, Rex Ying  

**Link**: [PDF](https://arxiv.org/pdf/2503.17933)  

**Abstract**: To improve the reliability of Large Language Models (LLMs) in clinical applications, retrieval-augmented generation (RAG) is extensively applied to provide factual medical knowledge. However, beyond general medical knowledge from open-ended datasets, clinical case-based knowledge is also critical for effective medical reasoning, as it provides context grounded in real-world patient experiences. Motivated by this, we propose Experience Retrieval Augmentation - ExpRAG framework based on Electronic Health Record (EHR), aiming to offer the relevant context from other patients' discharge reports. ExpRAG performs retrieval through a coarse-to-fine process, utilizing an EHR-based report ranker to efficiently identify similar patients, followed by an experience retriever to extract task-relevant content for enhanced medical reasoning. To evaluate ExpRAG, we introduce DischargeQA, a clinical QA dataset with 1,280 discharge-related questions across diagnosis, medication, and instruction tasks. Each problem is generated using EHR data to ensure realistic and challenging scenarios. Experimental results demonstrate that ExpRAG consistently outperforms a text-based ranker, achieving an average relative improvement of 5.2%, highlighting the importance of case-based knowledge for medical reasoning. 

**Abstract (ZH)**: 基于电子健康记录的经验检索增强-ExpRAG框架：提升临床应用大型语言模型的可靠性 

---
# STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models 

**Title (ZH)**: STShield: 单词哨兵用于大型语言模型的实时脱裤检测 

**Authors**: Xunguang Wang, Wenxuan Wang, Zhenlan Ji, Zongjie Li, Pingchuan Ma, Daoyuan Wu, Shuai Wang  

**Link**: [PDF](https://arxiv.org/pdf/2503.17932)  

**Abstract**: Large Language Models (LLMs) have become increasingly vulnerable to jailbreak attacks that circumvent their safety mechanisms. While existing defense methods either suffer from adaptive attacks or require computationally expensive auxiliary models, we present STShield, a lightweight framework for real-time jailbroken judgement. STShield introduces a novel single-token sentinel mechanism that appends a binary safety indicator to the model's response sequence, leveraging the LLM's own alignment capabilities for detection. Our framework combines supervised fine-tuning on normal prompts with adversarial training using embedding-space perturbations, achieving robust detection while preserving model utility. Extensive experiments demonstrate that STShield successfully defends against various jailbreak attacks, while maintaining the model's performance on legitimate queries. Compared to existing approaches, STShield achieves superior defense performance with minimal computational overhead, making it a practical solution for real-world LLM deployment. 

**Abstract (ZH)**: 大型语言模型（LLMs）日益受到规避其安全机制的 Jailbreak 攻击的影响。尽管现有的防御方法要么容易受到适应性攻击的影响，要么需要昂贵的辅助模型，我们提出了一种名为 STShield 的轻量级框架，用于实时检测 Jailbreak。STShield 引入了一种新颖的一令牌哨兵机制，为模型的响应序列添加了一个二进制安全指示符，利用 LL defense 能力进行检测。我们的框架结合了正常提示的监督微调和嵌入空间扰动的对抗训练，实现了稳健的检测，同时保持模型的实用性。广泛的实验表明，STShield 成功防御了各种 Jailbreak 攻击，同时保持了模型在合法查询上的性能。与现有方法相比，STShield 在最小的计算开销下实现了优越的防御性能，使其成为实际部署中 LL defense 的可行方案。 

---
# WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training 

**Title (ZH)**: WLB-LLM：-large语言模型训练的负载均衡四维并行性 

**Authors**: Zheng Wang, Anna Cai, Xinfeng Xie, Zaifeng Pan, Yue Guan, Weiwei Chu, Jie Wang, Shikai Li, Jianyu Huang, Chris Cai, Yuchen Hao, Yufei Ding  

**Link**: [PDF](https://arxiv.org/pdf/2503.17924)  

**Abstract**: In this work, we present WLB-LLM, a workLoad-balanced 4D parallelism for large language model training. We first thoroughly analyze the workload imbalance issue in LLM training and identify two primary sources of imbalance at the pipeline parallelism and context parallelism levels. Then, to address the imbalance issue, at the pipeline parallelism level, WLB-LLM incorporates a workload-aware variable-length document packing method to balance the computation and communication workload across micro-batches. Additionally, at the context parallelism level, WLB-LLM introduces a novel fine-grained per-document sharding strategy, ensuring each worker within a context parallelism group has an identical workload. Comprehensive experiments under different model scales demonstrate that WLB-LLM significantly mitigates the workload imbalance during 4D parallelism LLM training and achieves an average speedup of 1.23x when applying WLB-LLM in our internal LLM training framework. 

**Abstract (ZH)**: 工作负载平衡的4D并行训练方法：WLB-LLM 

---
# Cat-AIR: Content and Task-Aware All-in-One Image Restoration 

**Title (ZH)**: Cat-AIR：内容与任务 Awareness 全方位图像恢复 

**Authors**: Jiachen Jiang, Tianyu Ding, Ke Zhang, Jinxin Zhou, Tianyi Chen, Ilya Zharkov, Zhihui Zhu, Luming Liang  

**Link**: [PDF](https://arxiv.org/pdf/2503.17915)  

**Abstract**: All-in-one image restoration seeks to recover high-quality images from various types of degradation using a single model, without prior knowledge of the corruption source. However, existing methods often struggle to effectively and efficiently handle multiple degradation types. We present Cat-AIR, a novel \textbf{C}ontent \textbf{A}nd \textbf{T}ask-aware framework for \textbf{A}ll-in-one \textbf{I}mage \textbf{R}estoration. Cat-AIR incorporates an alternating spatial-channel attention mechanism that adaptively balances the local and global information for different tasks. Specifically, we introduce cross-layer channel attentions and cross-feature spatial attentions that allocate computations based on content and task complexity. Furthermore, we propose a smooth learning strategy that allows for seamless adaptation to new restoration tasks while maintaining performance on existing ones. Extensive experiments demonstrate that Cat-AIR achieves state-of-the-art results across a wide range of restoration tasks, requiring fewer FLOPs than previous methods, establishing new benchmarks for efficient all-in-one image restoration. 

**Abstract (ZH)**: 一种内容和任务感知的全目标任务图像恢复框架Cat-AIR 

---
# GLADMamba: Unsupervised Graph-Level Anomaly Detection Powered by Selective State Space Model 

**Title (ZH)**: GLADMamba：由选择性状态空间模型驱动的无监督图级异常检测 

**Authors**: Yali Fu, Jindong Li, Qi Wang, Qianli Xing  

**Link**: [PDF](https://arxiv.org/pdf/2503.17903)  

**Abstract**: Unsupervised graph-level anomaly detection (UGLAD) is a critical and challenging task across various domains, such as social network analysis, anti-cancer drug discovery, and toxic molecule identification. However, existing methods often struggle to capture the long-range dependencies efficiently and neglect the spectral information. Recently, selective State Space Models (SSMs), particularly Mamba, have demonstrated remarkable advantages in capturing long-range dependencies with linear complexity and a selection mechanism. Motivated by their success across various domains, we propose GLADMamba, a novel framework that adapts the selective state space model into UGLAD field. We design View-Fused Mamba (VFM) with a Mamba-Transformer-style architecture to efficiently fuse information from different views with a selective state mechanism. We also design Spectrum-Guided Mamba (SGM) with a Mamba-Transformer-style architecture to leverage the Rayleigh quotient to guide the embedding refining process. GLADMamba can dynamically focus on anomaly-related information while discarding irrelevant information for anomaly detection. To the best of our knowledge, this is the first work to introduce Mamba and explicit spectral information to UGLAD. Extensive experiments on 12 real-world datasets demonstrate that GLADMamba outperforms existing state-of-the-art methods, achieving superior performance in UGLAD. The code is available at this https URL. 

**Abstract (ZH)**: 无监督图级异常检测（UGLAD）是社交网络分析、抗癌药物发现和毒物分子识别等诸多领域的一个关键且具有挑战性的任务。然而，现有方法往往难以高效捕捉长期依赖关系，并忽视了光谱信息。近期，选择性状态空间模型（SSMs），特别是Mamba，已经在线性复杂度和选择机制下展示了捕获长期依赖关系的显著优势。受其在各领域成功应用的启发，我们提出GLADMamba框架，将其适应到UGLAD领域。我们设计了具有Mamba-Transformer风格架构的视图融合Mamba（VFM），以选择机制高效融合不同视图的信息。同时，我们设计了具有Mamba-Transformer风格架构的光谱引导Mamba（SGM），利用瑞利商指导嵌入精炼过程。GLADMamba能够动态关注与异常相关的信息，同时剔除无关信息进行异常检测。据我们所知，这是首次将Mamba和显式光谱信息引入UGLAD的研究。广泛实验在12个真实世界数据集上的结果显示，GLADMamba优于现有最先进的方法，在UGLAD中取得了 superior 的性能。代码可在以下链接获取。 

---
# Generative AI for Validating Physics Laws 

**Title (ZH)**: 生成式AI在验证物理定律中的应用 

**Authors**: Maria Nareklishvili, Nicholas Polson, Vadim Sokolov  

**Link**: [PDF](https://arxiv.org/pdf/2503.17894)  

**Abstract**: We present generative artificial intelligence (AI) to empirically validate fundamental laws of physics, focusing on the Stefan-Boltzmann law linking stellar temperature and luminosity. Our approach simulates counterfactual luminosities under hypothetical temperature regimes for each individual star and iteratively refines the temperature-luminosity relationship in a deep learning architecture. We use Gaia DR3 data and find that, on average, temperature's effect on luminosity increases with stellar radius and decreases with absolute magnitude, consistent with theoretical predictions. By framing physics laws as causal problems, our method offers a novel, data-driven approach to refine theoretical understanding and inform evidence-based policy and practice. 

**Abstract (ZH)**: 我们展示了生成式人工智能（AI）在经验验证物理学基本定律方面的应用，聚焦于将恒星温度与亮度关联起来的斯蒂芬-玻尔兹曼定律。我们的方法模拟了在每颗恒星假定温度条件下对应的非现实亮度，并在深度学习架构中逐步 refine 温度-亮度关系。我们使用盖亚 DR3 数据发现，平均而言，温度对亮度的影响随恒星半径增加而增强，随绝对星等增加而减弱，这与理论预测一致。通过将物理定律表述为因果问题，我们的方法提供了一种新的数据驱动方法，以 refinement 理论理解并指导基于证据的政策和实践。 

---
# Reasoning with LLMs for Zero-Shot Vulnerability Detection 

**Title (ZH)**: 零-shot 漏洞检测中的大语言模型推理 

**Authors**: Arastoo Zibaeirad, Marco Vieira  

**Link**: [PDF](https://arxiv.org/pdf/2503.17885)  

**Abstract**: Automating software vulnerability detection (SVD) remains a critical challenge in an era of increasingly complex and interdependent software systems. Despite significant advances in Large Language Models (LLMs) for code analysis, prevailing evaluation methodologies often lack the \textbf{context-aware robustness} necessary to capture real-world intricacies and cross-component interactions. To address these limitations, we present \textbf{VulnSage}, a comprehensive evaluation framework and a dataset curated from diverse, large-scale open-source system software projects developed in C/C++. Unlike prior datasets, it leverages a heuristic noise pre-filtering approach combined with LLM-based reasoning to ensure a representative and minimally noisy spectrum of vulnerabilities. The framework supports multi-granular analysis across function, file, and inter-function levels and employs four diverse zero-shot prompt strategies: Baseline, Chain-of-Thought, Think, and Think & Verify. Through this evaluation, we uncover that structured reasoning prompts substantially improve LLM performance, with Think & Verify reducing ambiguous responses from 20.3% to 9.1% while increasing accuracy. We further demonstrate that code-specialized models consistently outperform general-purpose alternatives, with performance varying significantly across vulnerability types, revealing that no single approach universally excels across all security contexts. Link to dataset and codes: this https URL 

**Abstract (ZH)**: 自动化软件漏洞检测（SVD）在日益复杂和相互依赖的软件系统时代仍然是一个关键挑战。尽管大型语言模型（LLMs）在代码分析方面取得了重大进展，但现有的评估方法往往缺乏捕捉现实世界复杂性和跨组件交互所需的\textbf{上下文感知稳健性}。为了解决这些局限性，我们提出\textbf{VulnSage}，一种全面的评估框架和从C/C++编写的多样化大型开源系统软件项目中编curated的数据集。与先前的数据集不同，它利用了一种启发式噪声预过滤方法结合LLM推理，以确保一个具有代表性且噪声最小的漏洞频谱。该框架支持跨函数、文件和跨函数层面的多粒度分析，并采用四种不同的零样本提示策略：基线、因果推理、思考和思考与验证。通过这种评估，我们发现结构化推理提示显著提高了LLM的性能，同时Think & Verify将含糊响应从20.3%减少到9.1%，并且提高了准确性。进一步的实验证明，代码专业化模型始终优于通用替代方案，不同类型的漏洞表现出显著差异，揭示了没有一种方法在所有安全上下文中普遍表现突出。数据集和代码链接：this https URL 

---
# Think Before Refusal : Triggering Safety Reflection in LLMs to Mitigate False Refusal Behavior 

**Title (ZH)**: 三思而后拒：在LLMs中触发安全反思以减轻错误拒绝行为 

**Authors**: Shengyun Si, Xinpeng Wang, Guangyao Zhai, Nassir Navab, Barbara Plank  

**Link**: [PDF](https://arxiv.org/pdf/2503.17882)  

**Abstract**: Recent advancements in large language models (LLMs) have demonstrated that fine-tuning and human alignment can render LLMs harmless. In practice, such "harmlessness" behavior is mainly achieved by training models to reject harmful requests, such as "Explain how to burn down my neighbor's house", where the model appropriately declines to respond. However, this approach can inadvertently result in false refusal, where models reject benign queries as well, such as "Tell me how to kill a Python process". In this work, we demonstrate that prompting safety reflection before generating a response can mitigate false refusal behavior. Building on this finding, we introduce the Think-Before-Refusal (TBR) schema and conduct safety-aware instruction fine-tuning incorporating safety reflection. In an ablation study across 15 pre-trained models, we show that models fine-tuned with safety reflection significantly reduce false refusal behavior while maintaining safety and overall performance compared to those fine-tuned without safety reflection. 

**Abstract (ZH)**: 近期大型语言模型的进步表明，通过微调和人类对齐可以使大型语言模型变得更加安全。实践中，“安全”行为主要通过训练模型拒绝有害请求来实现，例如对“解释如何烧毁邻居的房子”的请求，模型会适当拒绝回应。然而，这种做法可能会导致误拒绝，即模型将良性查询也错误地拒绝，例如“告诉我如何杀死一个Python进程”。在本研究中，我们证明了在生成响应之前进行安全性反思可以缓解误拒绝行为。基于这一发现，我们提出了反思前思考（Think-Before-Refusal, TBR）模式，并进行了安全意识指令微调，包含安全性反思。在15个预训练模型的消融研究中，我们展示了包含安全性反思微调的模型在减少误拒绝行为的同时，与未包含安全性反思微调的模型相比，在安全性和整体性能方面均有所提升。 

---
# good4cir: Generating Detailed Synthetic Captions for Composed Image Retrieval 

**Title (ZH)**: good4cir: 生成适合组合图像检索的详细合成caption 

**Authors**: Pranavi Kolouju, Eric Xing, Robert Pless, Nathan Jacobs, Abby Stylianou  

**Link**: [PDF](https://arxiv.org/pdf/2503.17871)  

**Abstract**: Composed image retrieval (CIR) enables users to search images using a reference image combined with textual modifications. Recent advances in vision-language models have improved CIR, but dataset limitations remain a barrier. Existing datasets often rely on simplistic, ambiguous, or insufficient manual annotations, hindering fine-grained retrieval. We introduce good4cir, a structured pipeline leveraging vision-language models to generate high-quality synthetic annotations. Our method involves: (1) extracting fine-grained object descriptions from query images, (2) generating comparable descriptions for target images, and (3) synthesizing textual instructions capturing meaningful transformations between images. This reduces hallucination, enhances modification diversity, and ensures object-level consistency. Applying our method improves existing datasets and enables creating new datasets across diverse domains. Results demonstrate improved retrieval accuracy for CIR models trained on our pipeline-generated datasets. We release our dataset construction framework to support further research in CIR and multi-modal retrieval. 

**Abstract (ZH)**: 基于图像合成的图像检索（Composed Image Retrieval, CIR）使用户能够使用参考图像结合文本修改来搜索图像。近期视觉语言模型的进步提高了CIR的性能，但数据集限制仍然是一个障碍。现有数据集往往依赖于简单、含糊或不足的人工注释，妨碍了细粒度的检索。我们引入了good4cir，这是一种结构化的管道，利用视觉语言模型生成高质量的合成注释。我们的方法包括：(1) 从查询图像中提取细粒度的物体描述，(2) 为目标图像生成可比的描述，(3) 综合文本指令捕获图像之间的有意义变换。这减少了幻觉，增强了修改的多样性，并确保了物体级别的一致性。应用我们的方法可以改善现有数据集，并跨越多个领域创建新的数据集。实验结果表明，基于我们管道生成的数据集训练的CIR模型检索精度有所提高。我们发布了数据集构建框架以支持CIR和多模态检索的进一步研究。 

---
# Detecting and Mitigating DDoS Attacks with AI: A Survey 

**Title (ZH)**: 使用AI检测和缓解DDoS攻击：一个综述 

**Authors**: Alexandru Apostu, Silviu Gheorghe, Andrei Hîji, Nicolae Cleju, Andrei Pătraşcu, Cristian Rusu, Radu Ionescu, Paul Irofti  

**Link**: [PDF](https://arxiv.org/pdf/2503.17867)  

**Abstract**: Distributed Denial of Service attacks represent an active cybersecurity research problem. Recent research shifted from static rule-based defenses towards AI-based detection and mitigation. This comprehensive survey covers several key topics. Preeminently, state-of-the-art AI detection methods are discussed. An in-depth taxonomy based on manual expert hierarchies and an AI-generated dendrogram are provided, thus settling DDoS categorization ambiguities. An important discussion on available datasets follows, covering data format options and their role in training AI detection methods together with adversarial training and examples augmentation. Beyond detection, AI based mitigation techniques are surveyed as well. Finally, multiple open research directions are proposed. 

**Abstract (ZH)**: 分布式拒绝服务攻击代表了活跃的网络安全研究问题。近期研究从静态规则防御转向基于AI的检测与缓解。本文综述涵盖了多个关键主题。首先，讨论了最先进的AI检测方法。提供了基于手动专家层次结构和AI生成的谱系图的详细分类学，从而解决了DDoS分类的不确定性。接着讨论了可用的数据集，涵盖数据格式选项及其在培训AI检测方法中的作用，包括对抗训练和数据增强。除了检测之外，还综述了基于AI的缓解技术。最后，提出了多个开放的研究方向。 

---
# A Causal Adjustment Module for Debiasing Scene Graph Generation 

**Title (ZH)**: 因果调整模块用于去偏场景图生成 

**Authors**: Li Liu, Shuzhou Sun, Shuaifeng Zhi, Fan Shi, Zhen Liu, Janne Heikkilä, Yongxiang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.17862)  

**Abstract**: While recent debiasing methods for Scene Graph Generation (SGG) have shown impressive performance, these efforts often attribute model bias solely to the long-tail distribution of relationships, overlooking the more profound causes stemming from skewed object and object pair distributions. In this paper, we employ causal inference techniques to model the causality among these observed skewed distributions. Our insight lies in the ability of causal inference to capture the unobservable causal effects between complex distributions, which is crucial for tracing the roots of model bias. Specifically, we introduce the Mediator-based Causal Chain Model (MCCM), which, in addition to modeling causality among objects, object pairs, and relationships, incorporates mediator variables, i.e., cooccurrence distribution, for complementing the causality. Following this, we propose the Causal Adjustment Module (CAModule) to estimate the modeled causal structure, using variables from MCCM as inputs to produce a set of adjustment factors aimed at correcting biased model predictions. Moreover, our method enables the composition of zero-shot relationships, thereby enhancing the model's ability to recognize such relationships. Experiments conducted across various SGG backbones and popular benchmarks demonstrate that CAModule achieves state-of-the-art mean recall rates, with significant improvements also observed on the challenging zero-shot recall rate metric. 

**Abstract (ZH)**: 尽管最近的场景图生成（SGG）去偏见方法展现了令人印象深刻的性能，但这些努力往往将模型偏见仅归因于关系分布的长尾效应，而忽视了来自对象和对象对分布偏差的更深层次原因。本文采用因果推理技术建模这些观察到的偏斜分布之间的因果关系。我们的见解在于因果推理能够捕捉复杂分布之间的不可观察因果效应，这对于追踪模型偏见的根源至关重要。具体而言，我们引入了中介因子基础因果链模型（MCCM），该模型除了建模对象、对象对和关系之间的因果关系外，还引入了中介变量，即共现分布，以补充因果关系。随后，我们提出因果调整模块（CAModule），使用MCCM中的变量作为输入，生成一组调整因子，旨在纠正偏差的模型预测。此外，我们的方法能够合成零样本关系，从而增强模型识别此类关系的能力。在多种SGG基础架构和流行基准上的实验表明，CAModule实现了最先进的平均召回率，同时在具有挑战性的零样本召回率指标上也观察到显著提高。 

---
# Adapt, Agree, Aggregate: Semi-Supervised Ensemble Labeling for Graph Convolutional Networks 

**Title (ZH)**: 适应、共识、聚合：图卷积网络的半监督集成标签标注 

**Authors**: Maryam Abdolali, Romina Zakerian, Behnam Roshanfekr, Fardin Ayar, Mohammad Rahmati  

**Link**: [PDF](https://arxiv.org/pdf/2503.17842)  

**Abstract**: In this paper, we propose a novel framework that combines ensemble learning with augmented graph structures to improve the performance and robustness of semi-supervised node classification in graphs. By creating multiple augmented views of the same graph, our approach harnesses the "wisdom of a diverse crowd", mitigating the challenges posed by noisy graph structures. Leveraging ensemble learning allows us to simultaneously achieve three key goals: adaptive confidence threshold selection based on model agreement, dynamic determination of the number of high-confidence samples for training, and robust extraction of pseudo-labels to mitigate confirmation bias. Our approach uniquely integrates adaptive ensemble consensus to flexibly guide pseudo-label extraction and sample selection, reducing the risks of error accumulation and improving robustness. Furthermore, the use of ensemble-driven consensus for pseudo-labeling captures subtle patterns that individual models often overlook, enabling the model to generalize better. Experiments on several real-world datasets demonstrate the effectiveness of our proposed method. 

**Abstract (ZH)**: 本文提出了一种结合集成学习和增强图结构的新框架，以提高图上半监督节点分类性能和鲁棒性。通过创建同一个图的多个增强视图，我们的方法利用了“多样群体的智慧”，缓解了嘈杂图结构带来的挑战。结合集成学习使我们能够同时实现三个关键目标：基于模型一致性自适应选择置信阈值、动态确定用于训练的高置信度样本数量以及稳健地提取伪标签以减轻确认偏见。我们的方法独特地将自适应集成共识灵活地引导伪标签提取和样本选择，减少了错误累积的风险并提高了鲁棒性。此外，集成驱动的共识用于伪标签化捕获了单个模型往往忽视的微妙模式，使模型能够更好地泛化。实验结果表明，所提出的方法在多个真实世界数据集上是有效的。 

---
# A Study on the Improvement of Code Generation Quality Using Large Language Models Leveraging Product Documentation 

**Title (ZH)**: 利用产品文档增强大型语言模型的代码生成质量研究 

**Authors**: Takuro Morimoto, Harumi Haraguchi  

**Link**: [PDF](https://arxiv.org/pdf/2503.17837)  

**Abstract**: Research on using Large Language Models (LLMs) in system development is expanding, especially in automated code and test generation. While E2E testing is vital for ensuring application quality, most test generation research has focused on unit tests, with limited work on E2E test code. This study proposes a method for automatically generating E2E test code from product documentation such as manuals, FAQs, and tutorials using LLMs with tailored prompts. The two step process interprets documentation intent and produces executable test code. Experiments on a web app with six key features (e.g., authentication, profile, discussion) showed that tests generated from product documentation had high compilation success and functional coverage, outperforming those based on requirement specs and user stories. These findings highlight the potential of product documentation to improve E2E test quality and, by extension, software quality. 

**Abstract (ZH)**: 利用大型语言模型自动生成端到端测试代码的研究 

---
# FundusGAN: A Hierarchical Feature-Aware Generative Framework for High-Fidelity Fundus Image Generation 

**Title (ZH)**: FundusGAN：一种层次特征意识的生成框架，用于高保真眼底图像生成 

**Authors**: Qingshan Hou, Meng Wang, Peng Cao, Zou Ke, Xiaoli Liu, Huazhu Fu, Osmar R. Zaiane  

**Link**: [PDF](https://arxiv.org/pdf/2503.17831)  

**Abstract**: Recent advancements in ophthalmology foundation models such as RetFound have demonstrated remarkable diagnostic capabilities but require massive datasets for effective pre-training, creating significant barriers for development and deployment. To address this critical challenge, we propose FundusGAN, a novel hierarchical feature-aware generative framework specifically designed for high-fidelity fundus image synthesis. Our approach leverages a Feature Pyramid Network within its encoder to comprehensively extract multi-scale information, capturing both large anatomical structures and subtle pathological features. The framework incorporates a modified StyleGAN-based generator with dilated convolutions and strategic upsampling adjustments to preserve critical retinal structures while enhancing pathological detail representation. Comprehensive evaluations on the DDR, DRIVE, and IDRiD datasets demonstrate that FundusGAN consistently outperforms state-of-the-art methods across multiple metrics (SSIM: 0.8863, FID: 54.2, KID: 0.0436 on DDR). Furthermore, disease classification experiments reveal that augmenting training data with FundusGAN-generated images significantly improves diagnostic accuracy across multiple CNN architectures (up to 6.49\% improvement with ResNet50). These results establish FundusGAN as a valuable foundation model component that effectively addresses data scarcity challenges in ophthalmological AI research, enabling more robust and generalizable diagnostic systems while reducing dependency on large-scale clinical data collection. 

**Abstract (ZH)**: Recent advancements in眼科领域的基础模型如RetFound已经展示了显著的诊断能力，但需要大量的数据集进行有效的预训练，这为开发和部署带来了巨大障碍。为解决这一关键挑战，我们提出了FundusGAN，一种专门用于高保真视网膜图像合成的新型分层特征感知生成框架。我们的方法在其编码器中采用特征金字塔网络，全面提取多尺度信息，捕捉较大的解剖结构和细微的病理特征。框架结合了改进的基于StyleGAN的生成器，使用了扩张卷积和策略性上采样调整，以保留关键视网膜结构的同时增强病理细节的表示。在DDR、DRIVE和IDRiD数据集上的综合评估表明，FundusGAN在多个指标上（DRR数据集上的SSIM：0.8863，FID：54.2，KID：0.0436）始终优于最先进的方法。此外，疾病分类实验表明，使用FundusGAN生成的图像增强训练数据可以显著提高多种CNN架构的诊断准确性（ResNet50模型的最大提升为6.49%）。这些结果确立了FundusGAN作为眼科AI研究中有效解决数据稀缺问题的基础模型组件的地位，有助于构建更加稳健和通用的诊断系统，同时减少了对大规模临床数据收集的依赖。 

---
# Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models 

**Title (ZH)**: Feather-SQL：一种基于双模型协作范式的轻量级NL2SQL框架 

**Authors**: Wenqi Pei, Hailing Xu, Hengyuan Zhao, Shizheng Hou, Han Chen, Zining Zhang, Pingyi Luo, Bingsheng He  

**Link**: [PDF](https://arxiv.org/pdf/2503.17811)  

**Abstract**: Natural Language to SQL (NL2SQL) has seen significant advancements with large language models (LLMs). However, these models often depend on closed-source systems and high computational resources, posing challenges in data privacy and deployment. In contrast, small language models (SLMs) struggle with NL2SQL tasks, exhibiting poor performance and incompatibility with existing frameworks. To address these issues, we introduce Feather-SQL, a new lightweight framework tailored for SLMs. Feather-SQL improves SQL executability and accuracy through 1) schema pruning and linking, 2) multi-path and multi-candidate generation. Additionally, we introduce the 1+1 Model Collaboration Paradigm, which pairs a strong general-purpose chat model with a fine-tuned SQL specialist, combining strong analytical reasoning with high-precision SQL generation. Experimental results on BIRD demonstrate that Feather-SQL improves NL2SQL performance on SLMs, with around 10% boost for models without fine-tuning. The proposed paradigm raises the accuracy ceiling of SLMs to 54.76%, highlighting its effectiveness. 

**Abstract (ZH)**: 自然语言到SQL（NL2SQL）任务在大规模语言模型（LLMs）的推动下取得了显著进展。然而，这些模型通常依赖于闭源系统和高计算资源，这在数据隐私和部署方面带来了挑战。相比之下，小语言模型（SLMs）在NL2SQL任务中表现不佳，性能较差且与现有框架不兼容。为了解决这些问题，我们提出了一种新的轻量级框架Feather-SQL，专门针对SLMs。Feather-SQL通过1）模式修剪和链接，2）多路径和多候选生成来提高SQL的可执行性和准确性。此外，我们引入了“1+1模型协作范式”，该范式将一个强大的通用聊天模型与一个细调过的SQL专家模型配对，结合了强大的分析推理能力与高精度的SQL生成能力。实验结果表明，Feather-SQL在SLMs上的NL2SQL性能得到了提升，未经过细调的模型性能提升了约10%。提出的范式将SLMs的准确性上限提升至54.76%，突显了其有效性。 

---
# A Roadmap Towards Improving Multi-Agent Reinforcement Learning With Causal Discovery And Inference 

**Title (ZH)**: 面向因果发现与推理的多智能体强化学习改进之路 

**Authors**: Giovanni Briglia, Stefano Mariani, Franco Zambonelli  

**Link**: [PDF](https://arxiv.org/pdf/2503.17803)  

**Abstract**: Causal reasoning is increasingly used in Reinforcement Learning (RL) to improve the learning process in several dimensions: efficacy of learned policies, efficiency of convergence, generalisation capabilities, safety and interpretability of behaviour. However, applications of causal reasoning to Multi-Agent RL (MARL) are still mostly unexplored. In this paper, we take the first step in investigating the opportunities and challenges of applying causal reasoning in MARL. We measure the impact of a simple form of causal augmentation in state-of-the-art MARL scenarios increasingly requiring cooperation, and with state-of-the-art MARL algorithms exploiting various degrees of collaboration between agents. Then, we discuss the positive as well as negative results achieved, giving us the chance to outline the areas where further research may help to successfully transfer causal RL to the multi-agent setting. 

**Abstract (ZH)**: 因果推理在多Agent强化学习中的应用：机遇与挑战 

---
# GaussianFocus: Constrained Attention Focus for 3D Gaussian Splatting 

**Title (ZH)**: GaussianFocus: 受约束的关注点优化用于3D 高斯渲染 

**Authors**: Zexu Huang, Min Xu, Stuart Perry  

**Link**: [PDF](https://arxiv.org/pdf/2503.17798)  

**Abstract**: Recent developments in 3D reconstruction and neural rendering have significantly propelled the capabilities of photo-realistic 3D scene rendering across various academic and industrial fields. The 3D Gaussian Splatting technique, alongside its derivatives, integrates the advantages of primitive-based and volumetric representations to deliver top-tier rendering quality and efficiency. Despite these advancements, the method tends to generate excessive redundant noisy Gaussians overfitted to every training view, which degrades the rendering quality. Additionally, while 3D Gaussian Splatting excels in small-scale and object-centric scenes, its application to larger scenes is hindered by constraints such as limited video memory, excessive optimization duration, and variable appearance across views. To address these challenges, we introduce GaussianFocus, an innovative approach that incorporates a patch attention algorithm to refine rendering quality and implements a Gaussian constraints strategy to minimize redundancy. Moreover, we propose a subdivision reconstruction strategy for large-scale scenes, dividing them into smaller, manageable blocks for individual training. Our results indicate that GaussianFocus significantly reduces unnecessary Gaussians and enhances rendering quality, surpassing existing State-of-The-Art (SoTA) methods. Furthermore, we demonstrate the capability of our approach to effectively manage and render large scenes, such as urban environments, whilst maintaining high fidelity in the visual output. 

**Abstract (ZH)**: Recent developments in 3D reconstruction and neural rendering have significantly propelled the capabilities of photo-realistic 3D scene rendering across various academic and industrial fields. 

---
# Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models 

**Title (ZH)**: 渐进式提示细化以改善文本到图像生成模型中的对齐 

**Authors**: Ketan Suhaas Saichandran, Xavier Thomas, Prakhar Kaushik, Deepti Ghadiyaram  

**Link**: [PDF](https://arxiv.org/pdf/2503.17794)  

**Abstract**: Text-to-image generative models often struggle with long prompts detailing complex scenes, diverse objects with distinct visual characteristics and spatial relationships. In this work, we propose SCoPE (Scheduled interpolation of Coarse-to-fine Prompt Embeddings), a training-free method to improve text-to-image alignment by progressively refining the input prompt in a coarse-to-fine-grained manner. Given a detailed input prompt, we first decompose it into multiple sub-prompts which evolve from describing broad scene layout to highly intricate details. During inference, we interpolate between these sub-prompts and thus progressively introduce finer-grained details into the generated image. Our training-free plug-and-play approach significantly enhances prompt alignment, achieves an average improvement of up to +4% in Visual Question Answering (VQA) scores over the Stable Diffusion baselines on 85% of the prompts from the GenAI-Bench dataset. 

**Abstract (ZH)**: 基于文本到图像生成模型在处理长提示描述复杂场景、多种具有独特视觉特征和空间关系的对象时常常表现不佳。本文提出了一种名为SCoPE（逐步插值从粗到细提示嵌入）的无训练方法，通过逐步细化输入提示的方式改进文本到图像的对齐。给定一个详细的输入提示，我们首先将其分解为多个子提示，从描述广泛的场景布局逐渐转向复杂的细节描述。在推理过程中，我们在这些建子提示之间进行插值，从而逐步将更细微的细节引入生成的图像。我们提出的无训练即插即用方法显著提高了提示对齐的效果，在GenAI-Bench数据集中85%的提示上，与 Stable Diffusion 基线相比，平均提高了4%的Visual Question Answering (VQA) 分数。 

---
# Every Sample Matters: Leveraging Mixture-of-Experts and High-Quality Data for Efficient and Accurate Code LLM 

**Title (ZH)**: 每一例样本都重要：利用专家混合模型和高质量数据实现高效精准的代码LLM 

**Authors**: Codefuse, Ling Team, Wenting Cai, Yuchen Cao, Chaoyu Chen, Chen Chen, Siba Chen, Qing Cui, Peng Di, Junpeng Fang, Zi Gong, Ting Guo, Zhengyu He, Yang Huang, Cong Li, Jianguo Li, Zheng Li, Shijie Lian, BingChang Liu, Songshan Luo, Shuo Mao, Min Shen, Jian Wu, Jiaolong Yang, Wenjie Yang, Tong Ye, Hang Yu, Wei Zhang, Zhenduo Zhang, Hailin Zhao, Xunjin Zheng, Jun Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2503.17793)  

**Abstract**: Recent advancements in code large language models (LLMs) have demonstrated remarkable capabilities in code generation and understanding. It is still challenging to build a code LLM with comprehensive performance yet ultimate efficiency. Many attempts have been released in the open source community to break the trade-off between performance and efficiency, such as the Qwen Coder series and the DeepSeek Coder series. This paper introduces yet another attempt in this area, namely Ling-Coder-Lite. We leverage the efficient Mixture-of-Experts (MoE) architecture along with a set of high-quality data curation methods (especially those based on program analytics) to build an efficient yet powerful code LLM. Ling-Coder-Lite exhibits on-par performance on 12 representative coding benchmarks compared to state-of-the-art models of similar size, such as Qwen2.5-Coder-7B and DeepSeek-Coder-V2-Lite, while offering competitive latency and throughput. In practice, we achieve a 50\% reduction in deployment resources compared to the similar-sized dense model without performance loss. To facilitate further research and development in this area, we open-source our models as well as a substantial portion of high-quality data for the annealing and post-training stages. The models and data can be accessed at~\url{this https URL}. 

**Abstract (ZH)**: 最近在代码大型语言模型（LLMs）方面的进展显示了其在代码生成和理解方面的卓越能力。尽管如此，构建一个在各方面表现全面且极其高效的代码LLM仍然具有挑战性。开源社区中已经发布了许多尝试来打破性能和效率之间的权衡，例如Qwen Coder系列和DeepSeek Coder系列。本文介绍了一个在这个领域中的新尝试，即Ling-Coder-Lite。我们利用高效的专家混合（MoE）架构，并结合了一系列高质量数据管理方法（尤其是基于程序分析的方法）来构建一个高效且强大的代码LLM。Ling-Coder-Lite在12个代表性编程基准测试中的表现与类似规模的最先进的模型（如Qwen2.5-Coder-7B和DeepSeek-Coder-V2-Lite）持平，同时提供竞争力的价格延迟和吞吐量。在实际应用中，与类似的密集模型相比，我们实现了50%的部署资源减少，且没有性能损失。为了进一步促进该领域的研究和发展，我们开放了我们的模型以及大量高质量数据以供退火和后训练阶段使用。模型和数据可以访问：this https URL。 

---
# Aligning Foundation Model Priors and Diffusion-Based Hand Interactions for Occlusion-Resistant Two-Hand Reconstruction 

**Title (ZH)**: 面向 Occlusion-抵抗的双手重建：对齐基础模型先验与扩散机制的手部交互 

**Authors**: Gaoge Han, Yongkang Cheng, Zhe Chen, Shaoli Huang, Tongliang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.17788)  

**Abstract**: Two-hand reconstruction from monocular images faces persistent challenges due to complex and dynamic hand postures and occlusions, causing significant difficulty in achieving plausible interaction alignment. Existing approaches struggle with such alignment issues, often resulting in misalignment and penetration artifacts. To tackle this, we propose a novel framework that attempts to precisely align hand poses and interactions by synergistically integrating foundation model-driven 2D priors with diffusion-based interaction refinement for occlusion-resistant two-hand reconstruction. First, we introduce a Fusion Alignment Encoder that learns to align fused multimodal priors keypoints, segmentation maps, and depth cues from foundation models during training. This provides robust structured guidance, further enabling efficient inference without foundation models at test time while maintaining high reconstruction accuracy. Second, we employ a two-hand diffusion model explicitly trained to transform interpenetrated poses into plausible, non-penetrated interactions, leveraging gradient-guided denoising to correct artifacts and ensure realistic spatial relations. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on InterHand2.6M, FreiHAND, and HIC datasets, significantly advancing occlusion handling and interaction robustness. 

**Abstract (ZH)**: 单目图像中的双手重建由于复杂多变的手部姿态和遮挡持续面临挑战，导致实现可信赖的交互对齐显著困难。现有方法难以解决这些问题，常导致对齐不准和穿透伪影。为应对这一挑战，我们提出了一种新颖框架，通过将基础模型驱动的二维先验与基于扩散的交互精修结合，协同实现对抗遮挡的双手重建并精确对齐手部姿态和交互。首先，我们引入了一种融合对齐编码器，在训练过程中学习对融合的多模态先验关键点、分割图和深度提示进行对齐，从而提供稳健的结构指导，并在测试时无需基础模型就能高效进行推理，同时保持高重建精度。其次，我们采用了一种明确训练的双手扩散模型，专门用于将交错的手部姿态转化为可信的、无穿透的交互，利用梯度指导去噪来纠正伪影并确保真实的空间关系。广泛评估表明，我们的方法在InterHand2.6M、FreiHAND和HIC数据集上达到了最先进的性能，显著提升了遮挡处理能力和交互鲁棒性。 

---
# Energy-Aware LLMs: A step towards sustainable AI for downstream applications 

**Title (ZH)**: 面向下游应用的能源aware大语言模型：通向可持续AI的一步 

**Authors**: Nguyen Phuc Tran, Brigitte Jaumard, Oscar Delgado  

**Link**: [PDF](https://arxiv.org/pdf/2503.17783)  

**Abstract**: Advanced Large Language Models (LLMs) have revolutionized various fields, including communication networks, sparking an innovation wave that has led to new applications and services, and significantly enhanced solution schemes. Despite all these impressive developments, most LLMs typically require huge computational resources, resulting in terribly high energy consumption. Thus, this research study proposes an end-to-end pipeline that investigates the trade-off between energy efficiency and model performance for an LLM during fault ticket analysis in communication networks. It further evaluates the pipeline performance using two real-world datasets for the tasks of root cause analysis and response feedback in a communication network. Our results show that an appropriate combination of quantization and pruning techniques is able to reduce energy consumption while significantly improving model performance. 

**Abstract (ZH)**: 先进大语言模型（LLMs）已在通信网络等领域引发了一场创新浪潮，极大地推动了新的应用和服务，并显著提升了解决方案。尽管取得了这些令人印象深刻的发展，大多数LLMs仍然需要大量的计算资源，导致能耗极高。因此，本研究提出了一种端到端的管道，以在通信网络中故障工单分析过程中研究能耗效率与模型性能之间的权衡。该研究进一步使用两个真实世界的数据集评估管道在通信网络中故障根本原因分析和响应反馈任务上的性能。研究结果表明，适当结合量化和剪枝技术能够在显著提升模型性能的同时降低能耗。 

---
# Lifelong Evolution of Swarms 

**Title (ZH)**: 终身演化群体 

**Authors**: Lorenzo Leuzzi, Simon Jones, Sabine Hauert, Davide Bacciu, Andrea Cossu  

**Link**: [PDF](https://arxiv.org/pdf/2503.17763)  

**Abstract**: Adapting to task changes without forgetting previous knowledge is a key skill for intelligent systems, and a crucial aspect of lifelong learning. Swarm controllers, however, are typically designed for specific tasks, lacking the ability to retain knowledge across changing tasks. Lifelong learning, on the other hand, focuses on individual agents with limited insights into the emergent abilities of a collective like a swarm. To address this gap, we introduce a lifelong evolutionary framework for swarms, where a population of swarm controllers is evolved in a dynamic environment that incrementally presents novel tasks. This requires evolution to find controllers that quickly adapt to new tasks while retaining knowledge of previous ones, as they may reappear in the future. We discover that the population inherently preserves information about previous tasks, and it can reuse it to foster adaptation and mitigate forgetting. In contrast, the top-performing individual for a given task catastrophically forgets previous tasks. To mitigate this phenomenon, we design a regularization process for the evolutionary algorithm, reducing forgetting in top-performing individuals. Evolving swarms in a lifelong fashion raises fundamental questions on the current state of deep lifelong learning and on the robustness of swarm controllers in dynamic environments. 

**Abstract (ZH)**: 适应任务变化而不遗忘先前知识是智能系统的一项关键技能，也是终身学习的一个重要方面。然而，群体控制器通常设计用于特定任务，缺乏跨任务保留知识的能力。相比之下，终身学习侧重于个体代理，对其群体如群组所展现出的新兴能力了解有限。为解决这一差距，我们引入了一个群组的终身进化框架，在这一框架中，群体中的群组控制器在动态环境中逐步呈现新的任务进行进化。这就要求进化找到既能快速适应新任务又能保留先前知识的控制器，因为这些知识可能会在未来重新出现。我们发现，群体本身会固有地保存关于先前任务的信息，并可以重新利用这些信息促进适应和减轻遗忘。相比之下，特定任务的最佳个体会灾难性地遗忘先前任务。为减轻这一现象，我们为进化算法设计了一个正则化过程，减少最佳个体的遗忘。以终身方式演化群组引发了当前深度终身学习状态和群组控制器在动态环境中的鲁棒性方面的基本问题。 

---
# CODA: Repurposing Continuous VAEs for Discrete Tokenization 

**Title (ZH)**: CODA: 重新利用连续VAEs进行离散标记化 

**Authors**: Zeyu Liu, Zanlin Ni, Yeguo Hua, Xin Deng, Xiao Ma, Cheng Zhong, Gao Huang  

**Link**: [PDF](https://arxiv.org/pdf/2503.17760)  

**Abstract**: Discrete visual tokenizers transform images into a sequence of tokens, enabling token-based visual generation akin to language models. However, this process is inherently challenging, as it requires both compressing visual signals into a compact representation and discretizing them into a fixed set of codes. Traditional discrete tokenizers typically learn the two tasks jointly, often leading to unstable training, low codebook utilization, and limited reconstruction quality. In this paper, we introduce \textbf{CODA}(\textbf{CO}ntinuous-to-\textbf{D}iscrete \textbf{A}daptation), a framework that decouples compression and discretization. Instead of training discrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs -- already optimized for perceptual compression -- into discrete tokenizers via a carefully designed discretization process. By primarily focusing on discretization, CODA ensures stable and efficient training while retaining the strong visual fidelity of continuous VAEs. Empirically, with $\mathbf{6 \times}$ less training budget than standard VQGAN, our approach achieves a remarkable codebook utilization of 100% and notable reconstruction FID (rFID) of $\mathbf{0.43}$ and $\mathbf{1.34}$ for $8 \times$ and $16 \times$ compression on ImageNet 256$\times$ 256 benchmark. 

**Abstract (ZH)**: 连续到离散适应（CODA：Continuous-to-Discrete Adaptation） 

---
# Bandwidth Reservation for Time-Critical Vehicular Applications: A Multi-Operator Environment 

**Title (ZH)**: 时间关键型车载应用的带宽预留：多运营商环境 

**Authors**: Abdullah Al-Khatib, Abdullah Ahmed, Klaus Moessner, Holger Timinger  

**Link**: [PDF](https://arxiv.org/pdf/2503.17756)  

**Abstract**: Onsite bandwidth reservation requests often face challenges such as price fluctuations and fairness issues due to unpredictable bandwidth availability and stringent latency requirements. Requesting bandwidth in advance can mitigate the impact of these fluctuations and ensure timely access to critical resources. In a multi-Mobile Network Operator (MNO) environment, vehicles need to select cost-effective and reliable resources for their safety-critical applications. This research aims to minimize resource costs by finding the best price among multiple MNOs. It formulates multi-operator scenarios as a Markov Decision Process (MDP), utilizing a Deep Reinforcement Learning (DRL) algorithm, specifically Dueling Deep Q-Learning. For efficient and stable learning, we propose a novel area-wise approach and an adaptive MDP synthetic close to the real environment. The Temporal Fusion Transformer (TFT) is used to handle time-dependent data and model training. Furthermore, the research leverages Amazon spot price data and adopts a multi-phase training approach, involving initial training on synthetic data, followed by real-world data. These phases enable the DRL agent to make informed decisions using insights from historical data and real-time observations. The results show that our model leads to significant cost reductions, up to 40%, compared to scenarios without a policy model in such a complex environment. 

**Abstract (ZH)**: 基于现场的带宽预留请求常常面临价格波动和公平性问题，这源于带宽可用性的不可预测性和严格的延迟要求。提前预留带宽可以缓解这些波动的影响，确保及时访问关键资源。在多移动网络运营商(MNO)环境中，车辆需要选择成本效益高且可靠的资源以支持其安全关键应用。本研究旨在通过在多家MNO中寻找最优价格来最小化资源成本。该研究将多运营商场景建模为马尔可夫决策过程(MDP)，并利用深度强化学习(DRL)算法，具体采用对偶深度Q学习(Dueling Deep Q-Learning)。为了实现高效的稳定学习，我们提出了一种新颖的区域化方法和一种接近真实环境的自适应MDP。使用时间融合变换器(TFT)处理时间相关数据并进行模型训练。此外，本研究利用亚马逊的即时价格数据，并采用多阶段训练方法，首先是使用合成数据进行初始训练，随后使用真实数据进行训练。这些阶段使DRL代理能够利用历史数据洞察和实时观察做出明智的决策。结果显示，与没有政策模型的场景相比，我们的模型在这样复杂的环境中可实现高达40%的成本节约。 

---
# Building Resource-Constrained Language Agents: A Korean Case Study on Chemical Toxicity Information 

**Title (ZH)**: 基于资源约束的语言代理构建：以韩国化学毒性信息为例的研究 

**Authors**: Hojun Cho, Donghu Kim, Soyoung Yang, Chan Lee, Hunjoo Lee, Jaegul Choo  

**Link**: [PDF](https://arxiv.org/pdf/2503.17753)  

**Abstract**: Language agents powered by large language models (LLMs) face significant deployment challenges in resource-constrained environments, particularly for specialized domains and less-common languages. This paper presents Tox-chat, a Korean chemical toxicity information agent devised within these limitations. We propose two key innovations: a context-efficient architecture that reduces token consumption through hierarchical section search, and a scenario-based dialogue generation methodology that effectively distills tool-using capabilities from larger models. Experimental evaluations demonstrate that our fine-tuned 8B parameter model substantially outperforms both untuned models and baseline approaches, in terms of DB faithfulness and preference. Our work offers valuable insights for researchers developing domain-specific language agents under practical constraints. 

**Abstract (ZH)**: 由大规模语言模型驱动的语言代理在资源约束环境中，特别是在专业领域和少见语言中，面临显著的部署挑战。本文提出了Tox-chat，这是一种在这些限制下为韩语化学毒性信息设计的语言代理。我们提出了两项关键创新：一种通过分层段落搜索减少词元消耗的上下文高效架构，以及一种基于场景的对话生成方法，该方法有效提炼了大模型的工具使用能力。实验评估表明，我们微调的8B参数模型在DB信度和偏好方面显著优于未微调的模型和基线方法。我们的工作为在实际约束条件下开发领域特定语言代理的研究人员提供了宝贵见解。 

---
# V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts for Better Human-Model Interaction 

**Title (ZH)**: V2P-Bench: 通过视觉提示评估视频-语言理解以改善人-模型交互 

**Authors**: Yiming Zhao, Yu Zeng, Yukun Qi, YaoYang Liu, Lin Chen, Zehui Chen, Xikun Bao, Jie Zhao, Feng Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2503.17736)  

**Abstract**: Large Vision-Language Models (LVLMs) have made significant progress in the field of video understanding recently. However, current benchmarks uniformly lean on text prompts for evaluation, which often necessitate complex referential language and fail to provide precise spatial and temporal references. This limitation diminishes the experience and efficiency of human-model interaction. To address this limitation, we propose the Video Visual Prompt Benchmark(V2P-Bench), a comprehensive benchmark specifically designed to evaluate LVLMs' video understanding capabilities in multimodal human-model interaction scenarios. V2P-Bench includes 980 unique videos and 1,172 QA pairs, covering 5 main tasks and 12 dimensions, facilitating instance-level fine-grained understanding aligned with human cognition. Benchmarking results reveal that even the most powerful models perform poorly on V2P-Bench (65.4% for GPT-4o and 67.9% for Gemini-1.5-Pro), significantly lower than the human experts' 88.3%, highlighting the current shortcomings of LVLMs in understanding video visual prompts. We hope V2P-Bench will serve as a foundation for advancing multimodal human-model interaction and video understanding evaluation. Project page: this https URL. 

**Abstract (ZH)**: 大规模视觉语言模型（LVLMs）在视频理解领域取得了显著进展。然而，现有的基准测试统一依赖于文本提示进行评估，这往往需要复杂的参照语言，并未能提供精确的空间和时间参考。这一限制降低了人类与模型交互的体验和效率。为解决这一限制，我们提出了视频视觉提示基准（V2P-Bench），这是一个专门设计用于评估LVLMs在多模态人机交互场景中视频理解能力的综合基准。V2P-Bench 包含980个独特的视频和1,172个问答对，涵盖5个主要任务和12个维度，促进与人类认知一致的实例级细粒度理解。基准测试结果显示，即使是最强大的模型在V2P-Bench上的表现也较差（GPT-4o为65.4%，Gemini-1.5-Pro为67.9%），远低于人类专家的88.3%，突显了LVLMs在理解视频视觉提示方面当前的不足之处。我们希望V2P-Bench能够为促进多模态人机交互和视频理解评估提供一个基础。项目页面：此链接。 

---
# Aportes para el cumplimiento del Reglamento (UE) 2024/1689 en robótica y sistemas autónomos 

**Title (ZH)**: 关于执行欧盟条例（UE）2024/1689在机器人和自主系统领域的贡献 

**Authors**: Francisco J. Rodríguez Lera, Yoana Pita Lorenzo, David Sobrín Hidalgo, Laura Fernández Becerra, Irene González Fernández, Jose Miguel Guerrero Hernández  

**Link**: [PDF](https://arxiv.org/pdf/2503.17730)  

**Abstract**: Cybersecurity in robotics stands out as a key aspect within Regulation (EU) 2024/1689, also known as the Artificial Intelligence Act, which establishes specific guidelines for intelligent and automated systems. A fundamental distinction in this regulatory framework is the difference between robots with Artificial Intelligence (AI) and those that operate through automation systems without AI, since the former are subject to stricter security requirements due to their learning and autonomy capabilities. This work analyzes cybersecurity tools applicable to advanced robotic systems, with special emphasis on the protection of knowledge bases in cognitive architectures. Furthermore, a list of basic tools is proposed to guarantee the security, integrity, and resilience of these systems, and a practical case is presented, focused on the analysis of robot knowledge management, where ten evaluation criteria are defined to ensure compliance with the regulation and reduce risks in human-robot interaction (HRI) environments. 

**Abstract (ZH)**: 机器人领域的网络安全在欧盟《人工智能法案》（Regulation (EU) 2024/1689）中崭露头角，该法案为智能和自动化系统制定了具体指导原则。这一监管框架中的一个关键区别在于，具有人工智能（AI）的机器人与仅通过自动化系统运行且不包含AI的机器人之间的区别，因为前者的安全要求更为严格，这是由于它们具备学习和自主能力。本文分析适用于高级机器人系统的网络安全工具，特别强调认知架构中知识库的保护。此外，提出了一套基本工具，以确保这些系统的安全、完整性和韧性，并呈现了一个实用案例，专注于机器人知识管理分析，定义了十个评估标准以确保遵守法规并降低人类-机器人交互（HRI）环境中的风险。 

---
# DynASyn: Multi-Subject Personalization Enabling Dynamic Action Synthesis 

**Title (ZH)**: DynASyn: 多主题个性化动态动作合成 

**Authors**: Yongjin Choi, Chanhun Park, Seung Jun Baek  

**Link**: [PDF](https://arxiv.org/pdf/2503.17728)  

**Abstract**: Recent advances in text-to-image diffusion models spurred research on personalization, i.e., a customized image synthesis, of subjects within reference images. Although existing personalization methods are able to alter the subjects' positions or to personalize multiple subjects simultaneously, they often struggle to modify the behaviors of subjects or their dynamic interactions. The difficulty is attributable to overfitting to reference images, which worsens if only a single reference image is available. We propose DynASyn, an effective multi-subject personalization from a single reference image addressing these challenges. DynASyn preserves the subject identity in the personalization process by aligning concept-based priors with subject appearances and actions. This is achieved by regularizing the attention maps between the subject token and images through concept-based priors. In addition, we propose concept-based prompt-and-image augmentation for an enhanced trade-off between identity preservation and action diversity. We adopt an SDE-based editing guided by augmented prompts to generate diverse appearances and actions while maintaining identity consistency in the augmented images. Experiments show that DynASyn is capable of synthesizing highly realistic images of subjects with novel contexts and dynamic interactions with the surroundings, and outperforms baseline methods in both quantitative and qualitative aspects. 

**Abstract (ZH)**: Recent Advances in Text-to-Image Diffusion Models Spur Research on Personalization of Subjects within Reference Images: DynASyn, an Effective Multi-Subject Personalization from a Single Reference Image 

---
# Towards Invisible Backdoor Attack on Text-to-Image Diffusion Model 

**Title (ZH)**: 面向文本到图像扩散模型的隐形后门攻击 

**Authors**: Jie Zhang, Zhongqi Wang, Shiguang Shan, Xilin Chen  

**Link**: [PDF](https://arxiv.org/pdf/2503.17724)  

**Abstract**: Backdoor attacks targeting text-to-image diffusion models have advanced rapidly, enabling attackers to implant malicious triggers into these models to manipulate their outputs. However, current backdoor samples often exhibit two key abnormalities compared to benign samples: 1) Semantic Consistency, where backdoor prompts tend to generate images with similar semantic content even with significant textual variations to the prompts; 2) Attention Consistency, where the trigger induces consistent structural responses in the cross-attention maps. These consistencies leave detectable traces for defenders, making backdoors easier to identify. To enhance the stealthiness of backdoor samples, we propose a novel Invisible Backdoor Attack (IBA) by explicitly mitigating these consistencies. Specifically, our approach leverages syntactic structures as backdoor triggers to amplify the sensitivity to textual variations, effectively breaking down the semantic consistency. Besides, a regularization method based on Kernel Maximum Mean Discrepancy (KMMD) is proposed to align the distribution of cross-attention responses between backdoor and benign samples, thereby disrupting attention consistency. Extensive experiments demonstrate that our IBA achieves a 97.5% attack success rate while exhibiting stronger resistance to defenses, with an average of over 98% backdoor samples bypassing three state-of-the-art detection mechanisms. The code is available at this https URL. 

**Abstract (ZH)**: 针对文本到图像扩散模型的后门攻击已快速发展，使攻击者能够将恶意触发器植入这些模型以操控其输出。然而，当前的后门样本与良性样本相比通常表现出两种关键异常：1）语义一致性，后门提示即使在文本提示有显著变化的情况下，仍倾向于生成具有相似语义内容的图像；2）注意力一致性，触发器在交叉注意力图中诱导一致的结构响应。这些一致性为防御者留下了可检测的痕迹，使后门更容易被识别。为了增强后门样本的隐匿性，我们提出了一种新型隐形后门攻击（IBA），通过显式地减轻这些一致性。具体来说，我们的方法利用句法结构作为后门触发器，增强对文本变化的敏感性，从而打破语义一致性。此外，我们还提出了一种基于核最大均值差异（KMMD）的正则化方法，以使后门样本和良性样本的交叉注意力响应分布保持一致，从而破坏注意力一致性。广泛实验表明，我们的IBA在攻击成功率上达到97.5%，同时表现出更强的防御抵抗性，平均超过98%的后门样本能够绕过三种最先进的检测机制。代码可在以下链接获取。 

---
# Multi-modality Anomaly Segmentation on the Road 

**Title (ZH)**: 道路多模态异常分割 

**Authors**: Heng Gao, Zhuolin He, Shoumeng Qiu, Xiangyang Xue, Jian Pu  

**Link**: [PDF](https://arxiv.org/pdf/2503.17712)  

**Abstract**: Semantic segmentation allows autonomous driving cars to understand the surroundings of the vehicle comprehensively. However, it is also crucial for the model to detect obstacles that may jeopardize the safety of autonomous driving systems. Based on our experiments, we find that current uni-modal anomaly segmentation frameworks tend to produce high anomaly scores for non-anomalous regions in images. Motivated by this empirical finding, we develop a multi-modal uncertainty-based anomaly segmentation framework, named MMRAS+, for autonomous driving systems. MMRAS+ effectively reduces the high anomaly outputs of non-anomalous classes by introducing text-modal using the CLIP text encoder. Indeed, MMRAS+ is the first multi-modal anomaly segmentation solution for autonomous driving. Moreover, we develop an ensemble module to further boost the anomaly segmentation performance. Experiments on RoadAnomaly, SMIYC, and Fishyscapes validation datasets demonstrate the superior performance of our method. The code is available in this https URL. 

**Abstract (ZH)**: 语义分割使自动驾驶车辆能够全面理解车辆周围的环境。然而，模型检测可能危及自动驾驶系统安全的障碍物也同样至关重要。基于我们的实验，我们发现当前的单一模态异常分割框架倾向于为图像中的非异常区域生成高异常分数。受这一经验发现的启发，我们开发了一种基于多模态不确定性异常分割框架，命名为MMRAS+，适用于自动驾驶系统。MMRAS+通过引入基于CLIP文本编码器的文本模态，有效减少了非异常类别的高异常输出。实际上，MMRAS+是首款用于自动驾驶的多模态异常分割解决方案。此外，我们还开发了一个集成模块进一步提升异常分割性能。针对RoadAnomaly、SMIYC和Fishyscapes验证数据集的实验表明，本方法具有优越的表现。代码可在以下链接获取。 

---
# GUI-Xplore: Empowering Generalizable GUI Agents with One Exploration 

**Title (ZH)**: GUI-Xplore：通过一次探索赋予通用GUI代理泛化能力 

**Authors**: Yuchen Sun, Shanhui Zhao, Tao Yu, Hao Wen, Samith Va, Mengwei Xu, Yuanchun Li, Chongyang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2503.17709)  

**Abstract**: GUI agents hold significant potential to enhance the experience and efficiency of human-device interaction. However, current methods face challenges in generalizing across applications (apps) and tasks, primarily due to two fundamental limitations in existing datasets. First, these datasets overlook developer-induced structural variations among apps, limiting the transferability of knowledge across diverse software environments. Second, many of them focus solely on navigation tasks, which restricts their capacity to represent comprehensive software architectures and complex user interactions. To address these challenges, we introduce GUI-Xplore, a dataset meticulously designed to enhance cross-application and cross-task generalization via an exploration-and-reasoning framework. GUI-Xplore integrates pre-recorded exploration videos providing contextual insights, alongside five hierarchically structured downstream tasks designed to comprehensively evaluate GUI agent capabilities. To fully exploit GUI-Xplore's unique features, we propose Xplore-Agent, a GUI agent framework that combines Action-aware GUI Modeling with Graph-Guided Environment Reasoning. Further experiments indicate that Xplore-Agent achieves a 10% improvement over existing methods in unfamiliar environments, yet there remains significant potential for further enhancement towards truly generalizable GUI agents. 

**Abstract (ZH)**: GUI代理在提升人机交互体验和效率方面拥有巨大潜力，但当前方法在跨应用和任务的一般化方面面临挑战，主要是由于现有数据集的两个基本限制。首先，这些数据集忽视了开发者引入的应用结构差异，限制了在不同软件环境中的知识迁移。其次，许多数据集仅专注于导航任务，限制了其代表全面软件架构和复杂用户交互的能力。为应对这些挑战，我们引入了GUI-Xplore数据集，该数据集通过探索与推理框架精心设计，旨在增强跨应用和跨任务的一般化能力。GUI-Xplore集成了预先录制的探索视频和五个层次化的下游任务，以全面评估GUI代理的能力。为进一步充分利用GUI-Xplore的独特功能，我们提出了一种结合行动意识GUI建模与图引导环境推理的GUI代理框架Xplore-Agent。进一步的实验表明，Xplore-Agent在陌生环境中比现有方法提高了10%的表现，但仍存在向真正可泛化的GUI代理进一步提升的巨大潜力。 

---
# PT-PINNs: A Parametric Engineering Turbulence Solver based on Physics-Informed Neural Networks 

**Title (ZH)**: 基于物理知情神经网络的参数化工程湍流求解器 PT-PINNs 

**Authors**: Liang Jiang, Yuzhou Cheng, Kun Luo, Jianren Fan  

**Link**: [PDF](https://arxiv.org/pdf/2503.17704)  

**Abstract**: Physics-informed neural networks (PINNs) demonstrate promising potential in parameterized engineering turbulence optimization problems but face challenges, such as high data requirements and low computational accuracy when applied to engineering turbulence problems. This study proposes a framework that enhances the ability of PINNs to solve parametric turbulence problems without training datasets from experiments or CFD-Parametric Turbulence PINNs (PT-PINNs)). Two key methods are introduced to improve the accuracy and robustness of this framework. The first is a soft constraint method for turbulent viscosity calculation. The second is a pre-training method based on the conservation of flow rate in the flow field. The effectiveness of PT-PINNs is validated using a three-dimensional backward-facing step (BFS) turbulence problem with two varying parameters (Re = 3000-200000, ER = 1.1-1.5). PT-PINNs produce predictions that closely match experimental data and computational fluid dynamics (CFD) results across various conditions. Moreover, PT-PINNs offer a computational efficiency advantage over traditional CFD methods. The total time required to construct the parametric BFS turbulence model is 39 hours, one-sixteenth of the time required by traditional numerical methods. The inference time for a single-condition prediction is just 40 seconds-only 0.5% of a single CFD computation. These findings highlight the potential of PT-PINNs for future applications in engineering turbulence optimization problems. 

**Abstract (ZH)**: 基于物理的神经网络（PT-PINNs）在工程湍流优化问题中的参数化建模与应用 

---
# On the (im)possibility of sustainable artificial intelligence. Why it does not make sense to move faster when heading the wrong way 

**Title (ZH)**: 关于可持续人工智能的可能性（或不可能性）。为什么朝错误的方向前进时加快速度没有意义 

**Authors**: Rainer Rehak  

**Link**: [PDF](https://arxiv.org/pdf/2503.17702)  

**Abstract**: Artificial intelligence (AI) is currently considered a sustainability "game-changer" within and outside of academia. In order to discuss sustainable AI this article draws from insights by critical data and algorithm studies, STS, transformative sustainability science, critical computer science, and public interest theory. I argue that while there are indeed many sustainability-related use cases for AI, they are likely to have more overall drawbacks than benefits. To substantiate this claim, I differentiate three 'AI materialities' of the AI supply chain: first the literal materiality (e.g. water, cobalt, lithium, energy consumption etc.), second, the informational materiality (e.g. lots of data and centralised control necessary), and third, the social materiality (e.g. exploitative data work, communities harm by waste and pollution). In all materialities, effects are especially devastating for the global south while benefiting the global north. A second strong claim regarding sustainable AI circles around so called apolitical optimisation (e.g. regarding city traffic), however the optimisation criteria (e.g. cars, bikes, emissions, commute time, health) are purely political and have to be collectively negotiated before applying AI optimisation. Hence, sustainable AI, in principle, cannot break the glass ceiling of transformation and might even distract from necessary societal change. To address that I propose to stop 'unformation gathering' and to apply the 'small is beautiful' principle. This aims to contribute to an informed academic and collective negotiation on how to (not) integrate AI into the sustainability project while avoiding to reproduce the status quo by serving hegemonic interests between useful AI use cases, techno-utopian salvation narratives, technology-centred efficiency paradigms, the exploitative and extractivist character of AI and concepts of digital degrowth. 

**Abstract (ZH)**: 人工智能（AI）currently considered a sustainability “游戏规则改变者” 在学术界内外目前被认为是对可持续性产生“游戏规则改变者”影响的技术。为了讨论可持续AI，本文借鉴了批判性数据与算法研究、STS、转变性可持续科学、批判性计算机科学以及公共利益理论中的洞见。我主张，尽管人工智能确实有许多与可持续性相关的应用场景，但它们的整体弊端可能多于益处。为证明这一论点，我区分了AI供应链中的三种“AI物质性”：首先是字面意义上的物质性（例如水资源、钴、锂、能源消耗等），其次是信息性的物质性（例如大量数据和集中控制的必要性），再次是社会性的物质性（例如剥削性的数据劳动、社区因废物和污染而受损）。在所有物质性方面，其影响对全球南半球尤其具有毁灭性，而对全球北半球则有所益处。关于可持续AI的第二个重要论点围绕所谓的无政治色彩优化（例如城市交通优化），然而，优化标准（例如汽车、自行车、排放、通勤时间、健康等）都是纯粹的政治性问题，必须在应用AI优化之前由集体协商达成一致。因此，原则上，可持续AI无法突破转型的天花板，甚至可能分散对必要社会变革的注意力。为应对这一挑战，我建议停止“无结构信息收集”并应用“小即是美”的原则。这旨在促成对如何（不）将人工智能融入可持续性项目进行知情的学术界和集体协商，以避免通过有用的人工智能应用场景、 techno-乌托邦救赎叙事、技术为中心的效率观念、人工智能的剥削性与提取主义特征及数字减缩概念来复制现状。 

---
# Can LLMs Automate Fact-Checking Article Writing? 

**Title (ZH)**: LLM能否自动化事实核查文章写作？ 

**Authors**: Dhruv Sahnan, David Corney, Irene Larraz, Giovanni Zagni, Ruben Miguez, Zhuohan Xie, Iryna Gurevych, Elizabeth Churchill, Tanmoy Chakraborty, Preslav Nakov  

**Link**: [PDF](https://arxiv.org/pdf/2503.17684)  

**Abstract**: Automatic fact-checking aims to support professional fact-checkers by offering tools that can help speed up manual fact-checking. Yet, existing frameworks fail to address the key step of producing output suitable for broader dissemination to the general public: while human fact-checkers communicate their findings through fact-checking articles, automated systems typically produce little or no justification for their assessments. Here, we aim to bridge this gap. We argue for the need to extend the typical automatic fact-checking pipeline with automatic generation of full fact-checking articles. We first identify key desiderata for such articles through a series of interviews with experts from leading fact-checking organizations. We then develop QRAFT, an LLM-based agentic framework that mimics the writing workflow of human fact-checkers. Finally, we assess the practical usefulness of QRAFT through human evaluations with professional fact-checkers. Our evaluation shows that while QRAFT outperforms several previously proposed text-generation approaches, it lags considerably behind expert-written articles. We hope that our work will enable further research in this new and important direction. 

**Abstract (ZH)**: 自动事实核查旨在通过提供可以加速人工事实核查的工具来支持专业事实核查人员。然而，现有的框架未能解决向公众更广泛传播的關鍵步骤：虽然人工事实核查人员通过事实核查文章传达其发现，但自动化系统通常几乎不提供其评估的任何理由。在此，我们旨在弥合这一差距。我们主张需要扩展典型的自动事实核查流程，包括自动生成完整的事实核查文章。我们首先通过与领先事实核查机构的专家进行一系列访谈来识别此类文章的关键要求。然后，我们开发了QRAFT，一个基于LLM的代理框架，模仿人工事实核查人员的写作流程。最后，我们通过专业事实核查人员的人类评估来评估QRAFT的实际效用。我们的评估显示，虽然QRAFT优于几种先前提出的文本生成方法，但在专家撰写的文章面前还有很大差距。我们希望我们的工作能促进这一新且重要的方向上进一步的研究。 

---
# Safe RLHF-V: Safe Reinforcement Learning from Human Feedback in Multimodal Large Language Models 

**Title (ZH)**: Safe RLHF-V：多模态大型语言模型中的安全人类反馈强化学习 

**Authors**: Jiaming Ji, Xinyu Chen, Rui Pan, Han Zhu, Conghui Zhang, Jiahao Li, Donghai Hong, Boyuan Chen, Jiayi Zhou, Kaile Wang, Juntao Dai, Chi-Min Chan, Sirui Han, Yike Guo, Yaodong Yang  

**Link**: [PDF](https://arxiv.org/pdf/2503.17682)  

**Abstract**: Multimodal large language models (MLLMs) are critical for developing general-purpose AI assistants, yet they face growing safety risks. How can we ensure that MLLMs are safely aligned to prevent undesired behaviors such as discrimination, misinformation, or violations of ethical standards? In a further step, we need to explore how to fine-tune MLLMs to enhance reasoning performance while ensuring they satisfy safety constraints. Fundamentally, this can be formulated as a min-max optimization problem. In this study, we propose Safe RLHF-V, the first multimodal safety alignment framework that jointly optimizes helpfulness and safety using separate multimodal reward and cost models within a Lagrangian-based constrained optimization framework. Given that there is a lack of preference datasets that separate helpfulness and safety in multimodal scenarios, we introduce BeaverTails-V, the first open-source dataset with dual preference annotations for helpfulness and safety, along with multi-level safety labels (minor, moderate, severe). Additionally, we design a Multi-level Guardrail System to proactively defend against unsafe queries and adversarial attacks. By applying the Beaver-Guard-V moderation for 5 rounds of filtering and re-generation on the precursor model, the overall safety of the upstream model is significantly improved by an average of 40.9%. Experimental results demonstrate that fine-tuning different MLLMs with Safe RLHF can effectively enhance model helpfulness while ensuring improved safety. Specifically, Safe RLHF-V improves model safety by 34.2% and helpfulness by 34.3%. All of datasets, models, and code can be found at this https URL to support the safety development of MLLMs and reduce potential societal risks. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）是开发通用人工智能助理的关键，但它们面临着日益增长的安全风险。我们如何确保MLLMs安全对齐，以防止不当行为，如歧视、错误信息或违反伦理标准？在此基础上，我们需要探索如何在确保满足安全约束的同时，调整MLLMs以增强推理性能。从根本上说，这可以被形式化为一个最小-最大优化问题。在本研究中，我们提出了Safe RLHF-V，这是一种首创的多模态安全对齐框架，该框架在拉格朗日约束优化框架内分别使用多模态奖励和成本模型来联合优化有用性和安全性。由于在多模态场景中缺乏将有用性和安全性分开的偏好数据集，我们引入了BeaverTails-V，这是一个首创的开源数据集，其中包括双重视觉偏好注释（有用性和安全性）以及多层次的安全标签（轻微、中等、严重）。此外，我们设计了多层次护栏系统以主动防御不当查询和 adversarial 攻击。通过对先导模型进行5轮过滤和重新生成的Beaver-Guard-V审核，上游模型的整体安全性平均提高了40.9%。实验结果表明，使用Safe RLHF微调不同的MLLMs可以有效地提高模型的有用性并确保安全性的提高。具体而言，Safe RLHF-V提高了模型安全性34.2%和有用性34.3%。所有数据集、模型和代码均可通过此链接访问，以支持MLLMs的安全发展并降低潜在的社会风险。 

---
# ComfyGPT: A Self-Optimizing Multi-Agent System for Comprehensive ComfyUI Workflow Generation 

**Title (ZH)**: ComfyGPT：一个自我优化的多代理系统，用于全面的ComfyUI工作流生成 

**Authors**: Oucheng Huang, Yuhang Ma, Zeng Zhao, Mingrui Wu, Jiayi Ji, Rongsheng Zhang, Zhipeng Hu, Xiaoshuai Sun, Rongrong Ji  

**Link**: [PDF](https://arxiv.org/pdf/2503.17671)  

**Abstract**: ComfyUI provides a widely-adopted, workflow-based interface that enables users to customize various image generation tasks through an intuitive node-based architecture. However, the intricate connections between nodes and diverse modules often present a steep learning curve for users. In this paper, we introduce ComfyGPT, the first self-optimizing multi-agent system designed to generate ComfyUI workflows based on task descriptions automatically. ComfyGPT comprises four specialized agents: ReformatAgent, FlowAgent, RefineAgent, and ExecuteAgent. The core innovation of ComfyGPT lies in two key aspects. First, it focuses on generating individual node links rather than entire workflows, significantly improving generation precision. Second, we proposed FlowAgent, a LLM-based workflow generation agent that uses both supervised fine-tuning (SFT) and reinforcement learning (RL) to improve workflow generation accuracy. Moreover, we introduce FlowDataset, a large-scale dataset containing 13,571 workflow-description pairs, and FlowBench, a comprehensive benchmark for evaluating workflow generation systems. We also propose four novel evaluation metrics: Format Validation (FV), Pass Accuracy (PA), Pass Instruct Alignment (PIA), and Pass Node Diversity (PND). Experimental results demonstrate that ComfyGPT significantly outperforms existing LLM-based methods in workflow generation. 

**Abstract (ZH)**: ComfyGPT：一种基于任务描述自优化的多智能体系统，用于生成ComfyUI工作流 

---
# A Qualitative Study of User Perception of M365 AI Copilot 

**Title (ZH)**: M365 AI 导航员的用户感知 qualitative 研究 

**Authors**: Muneera Bano, Didar Zowghi, Jon Whittle, Liming Zhu, Andrew Reeson, Rob Martin, Jen Parson  

**Link**: [PDF](https://arxiv.org/pdf/2503.17661)  

**Abstract**: Adopting AI copilots in professional workflows presents opportunities for enhanced productivity, efficiency, and decision making. In this paper, we present results from a six month trial of M365 Copilot conducted at our organisation in 2024. A qualitative interview study was carried out with 27 participants. The study explored user perceptions of M365 Copilot's effectiveness, productivity impact, evolving expectations, ethical concerns, and overall satisfaction. Initial enthusiasm for the tool was met with mixed post trial experiences. While some users found M365 Copilot beneficial for tasks such as email coaching, meeting summaries, and content retrieval, others reported unmet expectations in areas requiring deeper contextual understanding, reasoning, and integration with existing workflows. Ethical concerns were a recurring theme, with users highlighting issues related to data privacy, transparency, and AI bias. While M365 Copilot demonstrated value in specific operational areas, its broader impact remained constrained by usability limitations and the need for human oversight to validate AI generated outputs. 

**Abstract (ZH)**: 采用AI副驾在专业工作流程中的应用提供了增强生产力、效率和决策的机会。本文报告了我们在2024年进行的为期六个月的M365 Copilot试用研究结果。研究通过定量访谈对27名参与者进行了调查，探讨了用户对M365 Copilot有效性的看法、生产力影响、期望变化、伦理问题以及总体满意度。初步对工具的热情在试用后体验中变得复杂多变。一些用户发现M365 Copilot在邮件指导、会议摘要和内容检索等方面有益，而另一些用户则在需要更深层次上下文理解、推理和与现有工作流程集成的领域报告了期望未得到满足的情况。伦理问题是一个反复出现的主题，用户指出了与数据隐私、透明度和AI偏见相关的问题。尽管M365 Copilot在特定运营领域显示出价值，但其更广泛的影响仍受到易用性限制和需要人类监督验证AI生成输出的需求的制约。 

---
# NaFM: Pre-training a Foundation Model for Small-Molecule Natural Products 

**Title (ZH)**: NaFM：预训练一个小分子天然产物基础模型 

**Authors**: Yuheng Ding, Yusong Wang, Bo Qiang, Jie Yu, Qi Li, Yiran Zhou, Zhenmin Liu  

**Link**: [PDF](https://arxiv.org/pdf/2503.17656)  

**Abstract**: Natural products, as metabolites from microorganisms, animals, or plants, exhibit diverse biological activities, making them crucial for drug discovery. Nowadays, existing deep learning methods for natural products research primarily rely on supervised learning approaches designed for specific downstream tasks. However, such one-model-for-a-task paradigm often lacks generalizability and leaves significant room for performance improvement. Additionally, existing molecular characterization methods are not well-suited for the unique tasks associated with natural products. To address these limitations, we have pre-trained a foundation model for natural products based on their unique properties. Our approach employs a novel pretraining strategy that is especially tailored to natural products. By incorporating contrastive learning and masked graph learning objectives, we emphasize evolutional information from molecular scaffolds while capturing side-chain information. Our framework achieves state-of-the-art (SOTA) results in various downstream tasks related to natural product mining and drug discovery. We first compare taxonomy classification with synthesized molecule-focused baselines to demonstrate that current models are inadequate for understanding natural synthesis. Furthermore, by diving into a fine-grained analysis at both the gene and microbial levels, NaFM demonstrates the ability to capture evolutionary information. Eventually, our method is experimented with virtual screening, illustrating informative natural product representations that can lead to more effective identification of potential drug candidates. 

**Abstract (ZH)**: 基于自然产物的独特性质的预训练模型研究：结合对比学习和掩蔽图学习以实现药物发现下游任务的最先进的性能 

---
# On The Sample Complexity Bounds In Bilevel Reinforcement Learning 

**Title (ZH)**: bilevel reinforcement learning中的样本复杂度界研究 

**Authors**: Mudit Gaur, Amrit Singh Bedi, Raghu Pasupathu, Vaneet Aggarwal  

**Link**: [PDF](https://arxiv.org/pdf/2503.17644)  

**Abstract**: Bilevel reinforcement learning (BRL) has emerged as a powerful mathematical framework for studying generative AI alignment and related problems. While several principled algorithmic frameworks have been proposed, key theoretical foundations, particularly those related to sample complexity, remain underexplored. Understanding and deriving tight sample complexity bounds are crucial for bridging the gap between theory and practice, guiding the development of more efficient algorithms. In this work, we present the first sample complexity result for BRL, achieving a bound of $\epsilon^{-4}$. This result extends to standard bilevel optimization problems, providing an interesting theoretical contribution with practical implications. To address the computational challenges associated with hypergradient estimation in bilevel optimization, we develop a first-order Hessian-free algorithm that does not rely on costly hypergradient computations. By leveraging matrix-free techniques and constrained optimization methods, our approach ensures scalability and practicality. Our findings pave the way for improved methods in AI alignment and other fields reliant on bilevel optimization. 

**Abstract (ZH)**: bilevel强化学习（BRL）已成为研究生成式AI对齐及相关问题的强大数学框架。虽然已经提出了若干规范化的算法框架，但关键的理论基础，尤其是样本复杂度方面，仍欠研究。理解并推导出紧凑的样本复杂度界对于弥合理论与实践之间的差距、指导更高效算法的发展至关重要。在本文中，我们首次为BRL提供了样本复杂度结果，达到了$\epsilon^{-4}$的界限。该结果扩展到了标准的 bilevel优化问题，提供了具有实际意义的理论贡献。为了应对bilevel优化中超梯度估计的计算挑战，我们开发了一种无需进行昂贵的超梯度计算的一阶Hessian-free算法。通过利用矩阵自由技术与约束优化方法，我们的方法确保了可扩展性和实用性。我们的发现为AI对齐方法以及其他依赖于bilevel优化的领域改进方法奠定了基础。 

---
# FairFlow: Mitigating Dataset Biases through Undecided Learning 

**Title (ZH)**: FairFlow：通过未决学习减轻数据集偏见 

**Authors**: Jiali Cheng, Hadi Amiri  

**Link**: [PDF](https://arxiv.org/pdf/2503.17632)  

**Abstract**: Language models are prone to dataset biases, known as shortcuts and spurious correlations in data, which often result in performance drop on new data. We present a new debiasing framework called ``FairFlow'' that mitigates dataset biases by learning to be undecided in its predictions for data samples or representations associated with known or unknown biases. The framework introduces two key components: a suite of data and model perturbation operations that generate different biased views of input samples, and a contrastive objective that learns debiased and robust representations from the resulting biased views of samples. Experiments show that FairFlow outperforms existing debiasing methods, particularly against out-of-domain and hard test samples without compromising the in-domain performance 

**Abstract (ZH)**: FairFlow：通过学习在包含已知或未知偏差的数据样本或表示上保持犹豫来缓解数据集偏差的新框架 

---
# Transferable Latent-to-Latent Locomotion Policy for Efficient and Versatile Motion Control of Diverse Legged Robots 

**Title (ZH)**: 可迁移的潜空间到潜空间运动政策：用于多样化-legged机器人高效且多功能的运动控制 

**Authors**: Ziang Zheng, Guojian Zhan, Bin Shuai, Shengtao Qin, Jiangtao Li, Tao Zhang, Shengbo Eben Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.17626)  

**Abstract**: Reinforcement learning (RL) has demonstrated remarkable capability in acquiring robot skills, but learning each new skill still requires substantial data collection for training. The pretrain-and-finetune paradigm offers a promising approach for efficiently adapting to new robot entities and tasks. Inspired by the idea that acquired knowledge can accelerate learning new tasks with the same robot and help a new robot master a trained task, we propose a latent training framework where a transferable latent-to-latent locomotion policy is pretrained alongside diverse task-specific observation encoders and action decoders. This policy in latent space processes encoded latent observations to generate latent actions to be decoded, with the potential to learn general abstract motion skills. To retain essential information for decision-making and control, we introduce a diffusion recovery module that minimizes information reconstruction loss during pretrain stage. During fine-tune stage, the pretrained latent-to-latent locomotion policy remains fixed, while only the lightweight task-specific encoder and decoder are optimized for efficient adaptation. Our method allows a robot to leverage its own prior experience across different tasks as well as the experience of other morphologically diverse robots to accelerate adaptation. We validate our approach through extensive simulations and real-world experiments, demonstrating that the pretrained latent-to-latent locomotion policy effectively generalizes to new robot entities and tasks with improved efficiency. 

**Abstract (ZH)**: 强化学习（RL）在获取机器人技能方面展示了非凡的能力，但学习每个新技能仍然需要大量数据进行训练。预训练和微调范式为高效适应新机器人实体和任务提供了有前景的方法。受获取的知识可以加快使用同一机器人学习新任务并帮助新的机器人掌握已训练任务的想法启发，我们提出了一种潜在训练框架，其中可迁移的潜在到潜在运动策略与多种任务特定观察编码器和动作解码器一起进行预训练。该策略在潜在空间中处理编码的潜在观察以生成潜在动作，具有学习通用抽象运动技能的潜力。为保留决策和控制所需的必要信息，我们引入了一种扩散恢复模块，在预训练阶段最小化信息重构损失。在微调阶段，预训练的潜在到潜在运动策略保持固定，仅对轻量级的任务特定编码器和解码器进行优化，以实现高效的适应。该方法允许机器人利用其在不同任务中的先验经验以及不同形态的其他机器人的经验来加速适应。我们通过广泛的仿真和实际实验验证了该方法，证明了预训练的潜在到潜在运动策略可以高效地泛化到新的机器人实体和任务中。 

---
# AI-Based Screening for Depression and Social Anxiety Through Eye Tracking: An Exploratory Study 

**Title (ZH)**: 基于眼动追踪的AI辅助抑郁和社交焦虑筛查：一项探索性研究 

**Authors**: Karol Chlasta, Katarzyna Wisiecka, Krzysztof Krejtz, Izabela Krejtz  

**Link**: [PDF](https://arxiv.org/pdf/2503.17625)  

**Abstract**: Well-being is a dynamic construct that evolves over time and fluctuates within individuals, presenting challenges for accurate quantification. Reduced well-being is often linked to depression or anxiety disorders, which are characterised by biases in visual attention towards specific stimuli, such as human faces. This paper introduces a novel approach to AI-assisted screening of affective disorders by analysing visual attention scan paths using convolutional neural networks (CNNs). Data were collected from two studies examining (1) attentional tendencies in individuals diagnosed with major depression and (2) social anxiety. These data were processed using residual CNNs through images generated from eye-gaze patterns. Experimental results, obtained with ResNet architectures, demonstrated an average accuracy of 48% for a three-class system and 62% for a two-class system. Based on these exploratory findings, we propose that this method could be employed in rapid, ecological, and effective mental health screening systems to assess well-being through eye-tracking. 

**Abstract (ZH)**: 福祉是一个动态的构建体，随着时间的推移而演变并在个体间波动，这为准确量化带来了挑战。降低的福祉状态通常与抑郁或焦虑障碍相关，这些障碍的特征是对特定刺激，如人脸，存在视觉注意力偏差。本文介绍了一种通过卷积神经网络（CNNs）分析视觉注意力扫描路径来辅助筛查情感障碍的新方法。研究数据来自两个研究项目，分别探讨（1）重度抑郁诊断个体的注意力倾向以及（2）社交焦虑。这些数据通过眼球注视模式生成的图像使用残差CNN进行处理。使用ResNet架构的实验结果表明，对于三分类系统，平均准确率为48%，而对于二分类系统，平均准确率为62%。基于这些探索性发现，我们提出，这种方法可以应用于快速、生态有效的精神健康筛查系统，并通过眼动追踪评估福祉。 

---
# Unraveling Pedestrian Fatality Patterns: A Comparative Study with Explainable AI 

**Title (ZH)**: 解析行人死亡模式：可解释AI的对比研究 

**Authors**: Methusela Sulle, Judith Mwakalonge, Gurcan Comert, Saidi Siuhi, Nana Kankam Gyimah  

**Link**: [PDF](https://arxiv.org/pdf/2503.17623)  

**Abstract**: Road fatalities pose significant public safety and health challenges worldwide, with pedestrians being particularly vulnerable in vehicle-pedestrian crashes due to disparities in physical and performance characteristics. This study employs explainable artificial intelligence (XAI) to identify key factors contributing to pedestrian fatalities across the five U.S. states with the highest crash rates (2018-2022). It compares them to the five states with the lowest fatality rates. Using data from the Fatality Analysis Reporting System (FARS), the study applies machine learning techniques-including Decision Trees, Gradient Boosting Trees, Random Forests, and XGBoost-to predict contributing factors to pedestrian fatalities. To address data imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) is utilized, while SHapley Additive Explanations (SHAP) values enhance model interpretability. The results indicate that age, alcohol and drug use, location, and environmental conditions are significant predictors of pedestrian fatalities. The XGBoost model outperformed others, achieving a balanced accuracy of 98 %, accuracy of 90 %, precision of 92 %, recall of 90 %, and an F1 score of 91 %. Findings reveal that pedestrian fatalities are more common in mid-block locations and areas with poor visibility, with older adults and substance-impaired individuals at higher risk. These insights can inform policymakers and urban planners in implementing targeted safety measures, such as improved lighting, enhanced pedestrian infrastructure, and stricter traffic law enforcement, to reduce fatalities and improve public safety. 

**Abstract (ZH)**: 车辆与行人碰撞导致的行人死亡事件在全球范围内对公共安全和健康构成了重大挑战，其中行人因身体和性能特征的差异，在此类碰撞中尤其脆弱。本研究采用解释性人工智能（XAI）来识别五个车辆行人事故率最高（2018-2022年）的美国州（五个州）中行人死亡的关键因素，并将其与五个行人死亡率最低的州进行对比。利用致命事故报告系统（FARS）的数据，并采用包括决策树、梯度提升树、随机森林和XGBoost在内的机器学习技术来预测行人死亡的主要因素。通过使用合成少数类过采样技术（SMOTE）来解决数据不平衡问题，同时利用SHapley加性和解释值（SHAP）提高模型的可解释性。研究结果表明，年龄、酒精和药物使用、位置和环境条件是行人死亡的重要预测因子。XGBoost模型表现最佳，实现了均衡准确率98%、准确率90%、精确率92%、召回率90%和F1分数91%。研究发现，行人死亡事件在中段位置和能见度差的地区更为常见，老年人和物质摄入影响的个体面临更高的风险。这些洞见可以指导政策制定者和城市规划者采取针对性的安全措施，如改进照明、增强行人基础设施和加强交通执法，以减少死亡人数并提高公共安全。 

---
# A Generative Caching System for Large Language Models 

**Title (ZH)**: 大型语言模型的生成性缓存系统 

**Authors**: Arun Iyengar, Ashish Kundu, Ramana Kompella, Sai Nandan Mamidi  

**Link**: [PDF](https://arxiv.org/pdf/2503.17603)  

**Abstract**: Caching has the potential to be of significant benefit for accessing large language models (LLMs) due to their high latencies which typically range from a small number of seconds to well over a minute. Furthermore, many LLMs charge money for queries; caching thus has a clear monetary benefit. This paper presents a new caching system for improving user experiences with LLMs. In addition to reducing both latencies and monetary costs for accessing LLMs, our system also provides important features that go beyond the performance benefits typically associated with caches. A key feature we provide is generative caching, wherein multiple cached responses can be synthesized to provide answers to queries which have never been seen before. Our generative caches function as repositories of valuable information which can be mined and analyzed. We also improve upon past semantic caching techniques by tailoring the caching algorithms to optimally balance cost and latency reduction with the quality of responses provided. Performance tests indicate that our caches are considerably faster than GPTcache. 

**Abstract (ZH)**: 缓存有望显著改善访问大型语言模型（LLMs）的体验，由于LLMs通常具有从几秒钟到超过一分钟的高延迟。此外，许多LLMs按查询收费；因此，缓存具有明显的经济效益。本文介绍了一种新的缓存系统，以提高用户与LLMs的交互体验。除了降低访问LLMs的延迟和经济成本外，我们的系统还提供了超越传统缓存性能优势的重要功能。我们提供的一项关键功能是生成性缓存，即可以合成多个缓存响应来回答之前从未遇到过的查询。我们的生成性缓存充当有价值的资源库，可以被挖掘和分析。我们还通过定制缓存算法，优化了成本和延迟减少与提供的响应质量之间的平衡，超越了过去的技术。性能测试表明，我们的缓存比GPTcache更快。 

---
# GPBench: A Comprehensive and Fine-Grained Benchmark for Evaluating Large Language Models as General Practitioners 

**Title (ZH)**: GPBench: 一种全面细粒度的大型语言模型综合评估基准（作为全科医生） 

**Authors**: Zheqing Li, Yiying Yang, Jiping Lang, Wenhao Jiang, Yuhang Zhao, Shuang Li, Dingqian Wang, Zhu Lin, Xuanna Li, Yuze Tang, Jiexian Qiu, Xiaolin Lu, Hongji Yu, Shuang Chen, Yuhua Bi, Xiaofei Zeng, Yixian Chen, Junrong Chen, Lin Yao  

**Link**: [PDF](https://arxiv.org/pdf/2503.17599)  

**Abstract**: General practitioners (GPs) serve as the cornerstone of primary healthcare systems by providing continuous and comprehensive medical services. However, due to community-oriented nature of their practice, uneven training and resource gaps, the clinical proficiency among GPs can vary significantly across regions and healthcare settings. Currently, Large Language Models (LLMs) have demonstrated great potential in clinical and medical applications, making them a promising tool for supporting general practice. However, most existing benchmarks and evaluation frameworks focus on exam-style assessments-typically multiple-choice question-lack comprehensive assessment sets that accurately mirror the real-world scenarios encountered by GPs. To evaluate how effectively LLMs can make decisions in the daily work of GPs, we designed GPBench, which consists of both test questions from clinical practice and a novel evaluation framework. The test set includes multiple-choice questions that assess fundamental knowledge of general practice, as well as realistic, scenario-based problems. All questions are meticulously annotated by experts, incorporating rich fine-grained information related to clinical management. The proposed LLM evaluation framework is based on the competency model for general practice, providing a comprehensive methodology for assessing LLM performance in real-world settings. As the first large-model evaluation set targeting GP decision-making scenarios, GPBench allows us to evaluate current mainstream LLMs. Expert assessment and evaluation reveal that in areas such as disease staging, complication recognition, treatment detail, and medication usage, these models exhibit at least ten major shortcomings. Overall, existing LLMs are not yet suitable for independent use in real-world GP working scenarios without human oversight. 

**Abstract (ZH)**: GPBench：针对全科医生决策场景的大模型评估基准 

---
# ConSol: Sequential Probability Ratio Testing to Find Consistent LLM Reasoning Paths Efficiently 

**Title (ZH)**: ConSol: 序列概率比检验以高效寻找一致的LLM推理路径 

**Authors**: Jaeyeon Lee, Guantong Qi, Matthew Brady Neeley, Zhandong Liu, Hyun-Hwan Jeong  

**Link**: [PDF](https://arxiv.org/pdf/2503.17587)  

**Abstract**: Recent advancements in large language models (LLMs) integrating explicit reasoning, such as OpenAI's o3-mini, DeepSeek-R1, and QWQ-32B, enable smaller models to solve complex tasks by generating intermediate reasoning steps prior to providing answers. However, this approach significantly increases computational costs, both monetarily and environmentally. The widely-used self-consistency method further exacerbates these costs by aggregating multiple reasoning paths to improve accuracy, often requiring between 40 to 64 samples per task. Although aggregation effectively reduces variance and bias, additional sampling can lead to diminishing returns when early samples yield consistent results. To address inefficiencies, we propose leveraging Sequential Probability Ratio Testing (SPRT) to dynamically terminate sampling once sufficient consistency is achieved. We calibrate SPRT parameters specifically for LLM applications, accounting for sensitivity to detect the mode of the distribution. Our experiments demonstrate that incorporating SPRT significantly enhances token efficiency, achieving comparable accuracy to self-consistency methods but at a substantially reduced computational cost. To promote transparency and facilitate reproducibility, we have made the source code and datasets used in our experiments publicly available at our GitHub repository: this https URL, or available as a PyPI package: pip install consol. We hope that this resource will support further research and encourage the development of new methods building upon our work. 

**Abstract (ZH)**: Recent advancements in大型语言模型（LLMs）整合显式推理：以OpenAI的o3-mini、DeepSeek-R1和QWQ-32B为例，通过在提供答案之前生成中间推理步骤，使较小的模型能够解决复杂任务。然而，这种方法显著增加了计算成本，无论是经济上还是环境上。广泛使用的自我一致性方法进一步加剧了这些成本，通过聚合多个推理路径以提高准确性，通常每项任务需要40到64个样本。尽管聚合有效降低了方差和偏差，但额外的采样在早期样本结果一致时可能会导致边际效益递减。为了应对效率问题，我们提出利用序列概率比检验（SPRT）动态终止采样，一旦达到足够的一致性就终止。我们为LLM应用校准了SPRT参数，考虑了对检测分布模式的敏感性。我们的实验表明，将SPRT纳入可以显著提高token效率，准确性与自我一致性方法相当，但计算成本大幅降低。为了促进透明性和重现实验，我们在GitHub仓库（this https URL）和PyPI包（pip install consol）中公开了源代码和实验数据集，希望这将支持进一步研究，并鼓励开发基于我们工作的新型方法。 

---
# Measuring the Robustness of Audio Deepfake Detectors 

**Title (ZH)**: 测量音频换音鉴定器的鲁棒性 

**Authors**: Xiang Li, Pin-Yu Chen, Wenqi Wei  

**Link**: [PDF](https://arxiv.org/pdf/2503.17577)  

**Abstract**: Deepfakes have become a universal and rapidly intensifying concern of generative AI across various media types such as images, audio, and videos. Among these, audio deepfakes have been of particular concern due to the ease of high-quality voice synthesis and distribution via platforms such as social media and robocalls. Consequently, detecting audio deepfakes plays a critical role in combating the growing misuse of AI-synthesized speech. However, real-world scenarios often introduce various audio corruptions, such as noise, modification, and compression, that may significantly impact detection performance. This work systematically evaluates the robustness of 10 audio deepfake detection models against 16 common corruptions, categorized into noise perturbation, audio modification, and compression. Using both traditional deep learning models and state-of-the-art foundation models, we make four unique observations. First, our findings show that while most models demonstrate strong robustness to noise, they are notably more vulnerable to modifications and compression, especially when neural codecs are applied. Second, speech foundation models generally outperform traditional models across most scenarios, likely due to their self-supervised learning paradigm and large-scale pre-training. Third, our results show that increasing model size improves robustness, albeit with diminishing returns. Fourth, we demonstrate how targeted data augmentation during training can enhance model resilience to unseen perturbations. A case study on political speech deepfakes highlights the effectiveness of foundation models in achieving high accuracy under real-world conditions. These findings emphasize the importance of developing more robust detection frameworks to ensure reliability in practical deployment settings. 

**Abstract (ZH)**: 深度伪造已成为各种媒体类型（如图像、音频和视频）生成AI面临的普遍且迅速加剧的关切。其中，音频深度伪造特别受到关注，因为通过社交媒体和自动电话等方式可以轻松实现高质量语音的合成与传播。因此，检测音频深度伪造在应对AI合成语音的日益滥用方面起着关键作用。然而，现实场景常引入各种音频损坏，如噪声、修改和压缩，这些都可能显著影响检测性能。本工作系统地评估了10种音频深度伪造检测模型在16种常见损坏（分为噪声扰动、音频修改和压缩）下的鲁棒性。使用传统的深度学习模型和最先进的基础模型，我们做出了四个独特的观察。首先，我们的发现表明，虽然大多数模型在噪声方面表现出较强的鲁棒性，但它们对修改和压缩更为敏感，尤其是在应用神经编解码器时。其次，基础模型在大多数场景中通常优于传统模型，这可能是由于它们的自监督学习范式和大规模预训练。第三，我们的结果显示，增加模型规模可以提高鲁棒性，尽管边际效益递减。第四，我们展示了如何在训练过程中进行有针对性的数据增强以提高模型对未见过扰动的鲁棒性。针对政治演说深度伪造的案例研究突显了基础模型在实际条件下的高精度。这些发现强调了在实际部署环境中开发更稳健检测框架的重要性。 

---
# Fairness-Driven LLM-based Causal Discovery with Active Learning and Dynamic Scoring 

**Title (ZH)**: 基于公平性驱动的大语言模型因果发现与主动学习及动态评分方法 

**Authors**: Khadija Zanna, Akane Sano  

**Link**: [PDF](https://arxiv.org/pdf/2503.17569)  

**Abstract**: Causal discovery (CD) plays a pivotal role in numerous scientific fields by clarifying the causal relationships that underlie phenomena observed in diverse disciplines. Despite significant advancements in CD algorithms that enhance bias and fairness analyses in machine learning, their application faces challenges due to the high computational demands and complexities of large-scale data. This paper introduces a framework that leverages Large Language Models (LLMs) for CD, utilizing a metadata-based approach akin to the reasoning processes of human experts. By shifting from pairwise queries to a more scalable breadth-first search (BFS) strategy, the number of required queries is reduced from quadratic to linear in terms of variable count, thereby addressing scalability concerns inherent in previous approaches. This method utilizes an Active Learning (AL) and a Dynamic Scoring Mechanism that prioritizes queries based on their potential information gain, combining mutual information, partial correlation, and LLM confidence scores to refine the causal graph more efficiently and accurately. This BFS query strategy reduces the required number of queries significantly, thereby addressing scalability concerns inherent in previous approaches. This study provides a more scalable and efficient solution for leveraging LLMs in fairness-driven CD, highlighting the effects of the different parameters on performance. We perform fairness analyses on the inferred causal graphs, identifying direct and indirect effects of sensitive attributes on outcomes. A comparison of these analyses against those from graphs produced by baseline methods highlights the importance of accurate causal graph construction in understanding bias and ensuring fairness in machine learning systems. 

**Abstract (ZH)**: 因果发现（CD）在众多科学领域中发挥着关键作用，通过阐明各种学科中观察到的现象背后的因果关系。尽管CD算法在机器学习中的偏差和公平性分析方面取得了显著进步，但其应用仍面临计算需求高和大规模数据复杂性高的挑战。本文提出了一种框架，利用大型语言模型（LLMs）进行因果发现，采用基于元数据的方法，类似于人类专家的推理过程。通过从成对查询转向更具扩展性的广度优先搜索（BFS）策略，所需的查询数量从变量数量的平方级降低到线性级，从而解决了先前方法中存在的可扩展性问题。该方法利用主动学习（AL）和动态评分机制，根据潜在的信息增益优先选择查询，结合互信息、部分相关和LLM置信度评分，更高效准确地精炼因果图。这种BFS查询策略显著减少了所需的查询数量，从而解决了先前方法中存在的可扩展性问题。本研究为利用LLMs进行公平驱动的因果发现提供了更具可扩展性和效率的解决方案，并探讨了不同参数对性能的影响。我们对推断出的因果图进行了公平性分析，识别了敏感属性对结果的直接和间接影响。将这些分析与基准方法生成的图的分析进行比较，强调了准确构建因果图在理解偏差和确保机器学习系统公平性方面的重要性。 

---
# Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent 

**Title (ZH)**: 使用DOLA的自主放射治疗规划：一种基于LLM的隐私保护优化代理 

**Authors**: Humza Nusrat, Bing Luo, Ryan Hall, Joshua Kim, Hassan Bagher-Ebadian, Anthony Doemer, Benjamin Movsas, Kundan Thind  

**Link**: [PDF](https://arxiv.org/pdf/2503.17553)  

**Abstract**: Radiotherapy treatment planning is a complex and time-intensive process, often impacted by inter-planner variability and subjective decision-making. To address these challenges, we introduce Dose Optimization Language Agent (DOLA), an autonomous large language model (LLM)-based agent designed for optimizing radiotherapy treatment plans while rigorously protecting patient privacy. DOLA integrates the LLaMa3.1 LLM directly with a commercial treatment planning system, utilizing chain-of-thought prompting, retrieval-augmented generation (RAG), and reinforcement learning (RL). Operating entirely within secure local infrastructure, this agent eliminates external data sharing. We evaluated DOLA using a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in 20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and optimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations. The 70B model demonstrated significantly improved performance, achieving approximately 16.4% higher final scores than the 8B model. The RAG approach outperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated convergence, highlighting the synergy of retrieval-based memory and reinforcement learning. Optimal temperature hyperparameter analysis identified 0.4 as providing the best balance between exploration and exploitation. This proof of concept study represents the first successful deployment of locally hosted LLM agents for autonomous optimization of treatment plans within a commercial radiotherapy planning system. By extending human-machine interaction through interpretable natural language reasoning, DOLA offers a scalable and privacy-conscious framework, with significant potential for clinical implementation and workflow improvement. 

**Abstract (ZH)**: 基于DOLA的自主优化的放射治疗计划语言代理：一个保护患者隐私的复杂治疗规划挑战的解决方案 

---
# Audio-Enhanced Vision-Language Modeling with Latent Space Broadening for High Quality Data Expansion 

**Title (ZH)**: 音频增强的视觉-语言建模：潜在空间拓宽以实现高质量数据扩展 

**Authors**: Yu Sun, Yin Li, Ruixiao Sun, Chunhui Liu, Fangming Zhou, Ze Jin, Linjie Wang, Xiang Shen, Zhuolin Hao, Hongyu Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2503.17551)  

**Abstract**: Transformer-based multimodal models are widely used in industrial-scale recommendation, search, and advertising systems for content understanding and relevance ranking. Enhancing labeled training data quality and cross-modal fusion significantly improves model performance, influencing key metrics such as quality view rates and ad revenue. High-quality annotations are crucial for advancing content modeling, yet traditional statistical-based active learning (AL) methods face limitations: they struggle to detect overconfident misclassifications and are less effective in distinguishing semantically similar items in deep neural networks. Additionally, audio information plays an increasing role, especially in short-video platforms, yet most pre-trained multimodal architectures primarily focus on text and images. While training from scratch across all three modalities is possible, it sacrifices the benefits of leveraging existing pre-trained visual-language (VL) and audio models. To address these challenges, we propose kNN-based Latent Space Broadening (LSB) to enhance AL efficiency and Vision-Language Modeling with Audio Enhancement (VLMAE), a mid-fusion approach integrating audio into VL models. This system deployed in production systems, leading to significant business gains. 

**Abstract (ZH)**: 基于Transformer的多模态模型在工业规模的推荐、搜索和广告系统中广泛用于内容理解和相关性排序。通过提高标注训练数据质量及跨模态融合显著提升模型性能，影响关键指标如高质量观看率和广告收入。高质量标注对于推进内容建模至关重要，但传统的基于统计的主动学习方法存在局限性：它们难以检测高置信度的误分类，并且在区分深度神经网络中的语义相似项时效果较差。此外，音频信息在短视频平台中扮演越来越重要的角色，但大部分预训练多模态架构主要关注文本和图像。尽管可以从所有三种模态从头开始训练是可能的，但这会牺牲使用现有预训练视觉-语言和音频模型带来的好处。为了解决这些挑战，我们提出了基于kNN的潜在空间拓展（LSB）以提高主动学习效率，并提出将音频增强引入视觉-语言模型中的Vision-Language Modeling with Audio Enhancement（VLMAE），这是一种中间融合方法。该系统在生产系统中部署，取得了显著的商业收益。 

---
# Learning Multi-Level Features with Matryoshka Sparse Autoencoders 

**Title (ZH)**: 学习多层特征的套娃稀疏自编码器 

**Authors**: Bart Bussmann, Noa Nabeshima, Adam Karvonen, Neel Nanda  

**Link**: [PDF](https://arxiv.org/pdf/2503.17547)  

**Abstract**: Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting neural networks by extracting the concepts represented in their activations. However, choosing the size of the SAE dictionary (i.e. number of learned concepts) creates a tension: as dictionary size increases to capture more relevant concepts, sparsity incentivizes features to be split or absorbed into more specific features, leaving high-level features missing or warped. We introduce Matryoshka SAEs, a novel variant that addresses these issues by simultaneously training multiple nested dictionaries of increasing size, forcing the smaller dictionaries to independently reconstruct the inputs without using the larger dictionaries. This organizes features hierarchically - the smaller dictionaries learn general concepts, while the larger dictionaries learn more specific concepts, without incentive to absorb the high-level features. We train Matryoshka SAEs on Gemma-2-2B and TinyStories and find superior performance on sparse probing and targeted concept erasure tasks, more disentangled concept representations, and reduced feature absorption. While there is a minor tradeoff with reconstruction performance, we believe Matryoshka SAEs are a superior alternative for practical tasks, as they enable training arbitrarily large SAEs while retaining interpretable features at different levels of abstraction. 

**Abstract (ZH)**: 嵌套稀疏自编码器：一种解决稀疏自编码器问题的新变体 

---
# PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning 

**Title (ZH)**: PRIMAL: 物理反应性和交互性的运动模型用于角色学习 

**Authors**: Yan Zhang, Yao Feng, Alpár Cseke, Nitin Saini, Nathan Bajandas, Nicolas Heron, Michael J. Black  

**Link**: [PDF](https://arxiv.org/pdf/2503.17544)  

**Abstract**: To build a motor system of the interactive avatar, it is essential to develop a generative motion model drives the body to move through 3D space in a perpetual, realistic, controllable, and responsive manner. Although motion generation has been extensively studied, most methods do not support ``embodied intelligence'' due to their offline setting, slow speed, limited motion lengths, or unnatural movements. To overcome these limitations, we propose PRIMAL, an autoregressive diffusion model that is learned with a two-stage paradigm, inspired by recent advances in foundation models. In the pretraining stage, the model learns motion dynamics from a large number of sub-second motion segments, providing ``motor primitives'' from which more complex motions are built. In the adaptation phase, we employ a ControlNet-like adaptor to fine-tune the motor control for semantic action generation and spatial target reaching. Experiments show that physics effects emerge from our training. Given a single-frame initial state, our model not only generates unbounded, realistic, and controllable motion, but also enables the avatar to be responsive to induced impulses in real time. In addition, we can effectively and efficiently adapt our base model to few-shot personalized actions and the task of spatial control. Evaluations show that our proposed method outperforms state-of-the-art baselines. We leverage the model to create a real-time character animation system in Unreal Engine that is highly responsive and natural. Code, models, and more results are available at: this https URL 

**Abstract (ZH)**: 构建互动虚拟角色的运动系统，需要开发一个生成运动模型，该模型能够以持久的、真实的、可控的和响应的方式驱动身体在三维空间中移动。尽管运动生成已经被广泛研究，但由于大多数方法受限于离线设置、速度慢、运动长度有限或动作不自然等问题，这些方法大多不支持“具有体能智能”的互动。为克服这些限制，我们提出了一种自回归扩散模型PRIMAL，该模型采用两阶段学习范式，受到基础模型近期进展的启发。在预训练阶段，模型从大量的亚秒级运动片段中学习运动动力学，提供“运动基元”，从中构建更复杂的动作。在适应阶段，我们采用类似于ControlNet的适配器来 fine-tune 运动控制，以实现语义动作生成和空间目标追踪。实验显示，我们的训练过程中自然现象得以涌现。给定初始单帧状态，我们的模型不仅能生成无界限的、真实的、可控制的运动，还能使虚拟角色实时响应诱导的外力。此外，我们能够有效且高效地将基础模型适配到少量示例的个性化动作以及空间控制任务。评估结果显示，我们提出的方法优于现有最先进的基线方法。我们利用该模型在Unreal Engine中建立了一个高响应性和自然性的实时角色动画系统。更多代码、模型和实验结果请参见：this https URL。 

---
# Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models 

**Title (ZH)**: 贝叶斯教学使大型语言模型具备概率推理能力 

**Authors**: Linlu Qiu, Fei Sha, Kelsey Allen, Yoon Kim, Tal Linzen, Sjoerd van Steenkiste  

**Link**: [PDF](https://arxiv.org/pdf/2503.17523)  

**Abstract**: Artificial intelligence systems based on large language models (LLMs) are increasingly used as agents that interact with users and with the world. To do so successfully, LLMs need to construct internal representations of the world and form probabilistic beliefs about those representations. To provide a user with personalized recommendations, for example, the LLM needs to gradually infer the user's preferences, over the course of multiple interactions. To evaluate whether contemporary LLMs are able to do so, we use the Bayesian inference framework from probability theory, which lays out the optimal way to update an agent's beliefs as it receives new information. We first show that the LLMs do not update their beliefs as expected from the Bayesian framework, and that consequently their predictions do not improve as expected as more information becomes available, even less so than we find is the case for humans. To address this issue, we teach the LLMs to reason in a Bayesian manner by training them to mimic the predictions of an optimal Bayesian model. We find that this approach not only significantly improves the LLM's performance on the particular recommendation task it is trained on, but also enables generalization to other tasks. This suggests that this method endows the LLM with broader Bayesian reasoning skills. More generally, our results indicate that LLMs can learn about reasoning strategies effectively and generalize those skills to new domains, which in part explains LLMs' empirical success. 

**Abstract (ZH)**: 基于大规模语言模型的人工智能系统作为与用户和世界交互的代理日益增多。为了成功地进行这种交互，大规模语言模型（LLMs）需要构建对世界的内部表征，并对其表征形成概率性信念。为了为用户提供个性化推荐，例如，LLMs需要在多次交互中逐渐推断出用户的偏好。我们利用概率论中的贝叶斯推理框架来评估当代LLMs是否能够做到这一点，该框架明确了代理在接收到新信息时更新其信念的最佳方式。我们首先表明，LLMs并未按贝叶斯框架预期的方式更新其信念，因此，当更多信息可用时，它们的预测也没有按预期改进，甚至不如我们发现的人类的表现。为了解决这一问题，我们通过训练LLMs使其模仿最优贝叶斯模型的预测来教它们进行贝叶斯推理。我们发现，这种方法不仅显著提高了LLMs在所训练的特定推荐任务上的性能，还使其能够泛化到其他任务。这表明该方法赋予LLMs更广泛的贝叶斯推理能力。更广泛地说，我们的结果表明，LLMs能够有效学习推理策略，并将这些技能泛化到新领域，部分解释了LLMs的实证成功。 

---
# A Predictive Services Architecture for Efficient Airspace Operations 

**Title (ZH)**: 一种用于高效 airspace 运行的预测性服务架构 

**Authors**: Ítalo Romani de Oliveira, Samet Ayhan, Glaucia Balvedi, Michael Biglin, Pablo Costas, Euclides C. Pinto Neto, Alexandre Leite, Felipe C. F. de Azevedo  

**Link**: [PDF](https://arxiv.org/pdf/2503.17515)  

**Abstract**: Predicting air traffic congestion and flow management is essential for airlines and Air Navigation Service Providers (ANSP) to enhance operational efficiency. Accurate estimates of future airport capacity and airspace density are vital for better airspace management, reducing air traffic controller workload and fuel consumption, ultimately promoting sustainable aviation. While existing literature has addressed these challenges, data management and query processing remain complex due to the vast volume of high-rate air traffic data. Many analytics use cases require a common pre-processing infrastructure, as ad-hoc approaches are insufficient. Additionally, linear prediction models often fall short, necessitating more advanced techniques.
This paper presents a data processing and predictive services architecture that ingests large, uncorrelated, and noisy streaming data to forecast future airspace system states. The system continuously collects raw data, periodically compresses it, and stores it in NoSQL databases for efficient query processing. For prediction, the system learns from historical traffic by extracting key features such as airport arrival and departure events, sector boundary crossings, weather parameters, and other air traffic data. These features are input into various regression models, including linear, non-linear, and ensemble models, with the best-performing model selected for predictions. We evaluate this infrastructure across three prediction use cases in the US National Airspace System (NAS) and a segment of European airspace, using extensive real operations data, confirming that our system can predict future system states efficiently and accurately. 

**Abstract (ZH)**: 预测空中交通拥堵和流量管理对于航空公司和空中导航服务提供商（ANSP）提升运营效率至关重要。准确估计未来的机场容量和 airspace 密度对于更好的 airspace 管理、减少空中交通管制员的工作负荷和燃油消耗、最终促进可持续航空具有重要作用。尽管现有文献已解决了这些挑战，但数据管理和查询处理由于高流量空中交通数据量巨大仍非常复杂。许多分析用例需要一种通用的预处理基础设施，临时解决方案远远不够。此外，线性预测模型经常不足，需要更高级的技术。本文提出了一种数据处理和预测服务架构，该架构能够处理大规模、不相关和嘈杂的流数据，以预测未来的 airspace 系统状态。该系统持续收集原始数据，定期对其进行压缩，并将其存储在 NoSQL 数据库中，以实现高效查询处理。对于预测，该系统通过提取历史交通的关键特征（如机场到达和离场事件、管制区边界穿越、天气参数和其他空中交通数据）来学习历史交通。这些特征被输入到各种回归模型中，包括线性模型、非线性模型和集成模型，选择表现最佳的模型进行预测。我们通过在美国国家 airspace 系统（NAS）和欧洲 airspace 的一个区域使用广泛的真实操作数据，跨三个预测用例评估了此基础设施，证实了我们的系统能够高效准确地预测未来系统状态。 

---
# Language Models May Verbatim Complete TextThey Were Not Explicitly Trained On 

**Title (ZH)**: 语言模型可能直接补全它们没有明确训练过的内容。 

**Authors**: Ken Ziyu Liu, Christopher A. Choquette-Choo, Matthew Jagielski, Peter Kairouz, Sanmi Koyejo, Percy Liang, Nicolas Papernot  

**Link**: [PDF](https://arxiv.org/pdf/2503.17514)  

**Abstract**: An important question today is whether a given text was used to train a large language model (LLM). A \emph{completion} test is often employed: check if the LLM completes a sufficiently complex text. This, however, requires a ground-truth definition of membership; most commonly, it is defined as a member based on the $n$-gram overlap between the target text and any text in the dataset. In this work, we demonstrate that this $n$-gram based membership definition can be effectively gamed. We study scenarios where sequences are \emph{non-members} for a given $n$ and we find that completion tests still succeed. We find many natural cases of this phenomenon by retraining LLMs from scratch after removing all training samples that were completed; these cases include exact duplicates, near-duplicates, and even short overlaps. They showcase that it is difficult to find a single viable choice of $n$ for membership definitions. Using these insights, we design adversarial datasets that can cause a given target sequence to be completed without containing it, for any reasonable choice of $n$. Our findings highlight the inadequacy of $n$-gram membership, suggesting membership definitions fail to account for auxiliary information available to the training algorithm. 

**Abstract (ZH)**: 当前一个重要的问题是判断给定文本是否被用于训练大规模语言模型（LLM）。一种常用的方法是完成测试：检查LLM是否能够完成一个足够复杂的文本。然而，这需要一个基于真实标准的成员资格定义；最常见的定义是基于目标文本与数据集中任何文本的$n$-gram重叠。在本工作中，我们展示了基于$n$-gram的成员资格定义可以被有效地操纵。我们研究了对于给定$n$的非成员序列的情况，发现完成测试仍然会成功。我们通过从数据集中移除所有被完成的样本并重新训练LLM，发现了许多自然存在的这种情况的例子，包括精确副本、近似副本以及甚至很短的重叠。这些例子展示了难以找到一个单一有效的$n$值作为成员资格定义。利用这些见解，我们设计了对抗性数据集，能够在不包含目标序列的情况下，导致给定的目标序列被完成，对于任何合理的$n$值选择都是如此。我们的发现揭示了$n$-gram成员资格的不足，暗示成员资格定义未能考虑到训练算法可用的辅助信息。 

---
# Improving Quantization with Post-Training Model Expansion 

**Title (ZH)**: 基于后训练模型扩展的量化优化 

**Authors**: Giuseppe Franco, Pablo Monteagudo-Lago, Ian Colbert, Nicholas Fraser, Michaela Blott  

**Link**: [PDF](https://arxiv.org/pdf/2503.17513)  

**Abstract**: The size of a model has been a strong predictor of its quality, as well as its cost. As such, the trade-off between model cost and quality has been well-studied. Post-training optimizations like quantization and pruning have typically focused on reducing the overall volume of pre-trained models to reduce inference costs while maintaining model quality. However, recent advancements have introduced optimization techniques that, interestingly, expand models post-training, increasing model size to improve quality when reducing volume. For instance, to enable 4-bit weight and activation quantization, incoherence processing often necessitates inserting online Hadamard rotations in the compute graph, and preserving highly sensitive weights often calls for additional higher precision computations. However, if application requirements cannot be met, the prevailing solution is to relax quantization constraints. In contrast, we demonstrate post-training model expansion is a viable strategy to improve model quality within a quantization co-design space, and provide theoretical justification. We show it is possible to progressively and selectively expand the size of a pre-trained large language model (LLM) to improve model quality without end-to-end retraining. In particular, when quantizing the weights and activations to 4 bits for Llama3 1B, we reduce the zero-shot accuracy gap to full precision by an average of 3% relative to both QuaRot and SpinQuant with only 5% more parameters, which is still a 3.8% reduction in volume relative to a BF16 reference model. 

**Abstract (ZH)**: 模型大小是其质量和成本的强预测因素，因此模型成本与质量之间的权衡已经被广泛研究。训练后优化技术如量化和剪枝通常关注于减少预训练模型的总体体积以降低推理成本同时保持模型质量。然而，最近的进展引入了扩大模型的优化技术，通过增加模型大小来提高质量，尤其是在减少体积时。例如，为实现4位权重和激活量化，不一致性处理时常需要在计算图中插入在线汉诺尔旋转，而保留敏感权重则需要额外的高精度计算。然而，如果应用需求无法满足，主流解决方案是放宽量化约束。相反，我们证明在量化联合设计空间中，训练后模型扩展是提高模型质量的一种可行策略，并提供了理论依据。我们展示了可以渐进且选择性地扩展预训练大型语言模型（LLM）的大小以提高模型质量，而无需端到端重新训练。特别是在将权重和激活量化为4位时，与QuaRot和SpinQuant相比，我们通过增加5%的参数缩小了零样本准确率差距的平均值为3%，相对体积减少了3.8%，相较于BF16参考模型。 

---
# Follow-up Question Generation For Enhanced Patient-Provider Conversations 

**Title (ZH)**: 增强患者-提供者对话的跟进问题生成 

**Authors**: Joseph Gatto, Parker Seegmiller, Timothy Burdick, Inas S. Khayal, Sarah DeLozier, Sarah M. Preum  

**Link**: [PDF](https://arxiv.org/pdf/2503.17509)  

**Abstract**: Follow-up question generation is an essential feature of dialogue systems as it can reduce conversational ambiguity and enhance modeling complex interactions. Conversational contexts often pose core NLP challenges such as (i) extracting relevant information buried in fragmented data sources, and (ii) modeling parallel thought processes. These two challenges occur frequently in medical dialogue as a doctor asks questions based not only on patient utterances but also their prior EHR data and current diagnostic hypotheses. Asking medical questions in asynchronous conversations compounds these issues as doctors can only rely on static EHR information to motivate follow-up questions.
To address these challenges, we introduce FollowupQ, a novel framework for enhancing asynchronous medical conversation. FollowupQ is a multi-agent framework that processes patient messages and EHR data to generate personalized follow-up questions, clarifying patient-reported medical conditions. FollowupQ reduces requisite provider follow-up communications by 34%. It also improves performance by 17% and 5% on real and synthetic data, respectively. We also release the first public dataset of asynchronous medical messages with linked EHR data alongside 2,300 follow-up questions written by clinical experts for the wider NLP research community. 

**Abstract (ZH)**: FollowupQ：一种增强异步医疗对话的新型框架 

---
# Large Language Models (LLMs) for Source Code Analysis: applications, models and datasets 

**Title (ZH)**: 大型语言模型（LLMs）在源代码分析中的应用、模型与数据集 

**Authors**: Hamed Jelodar, Mohammad Meymani, Roozbeh Razavi-Far  

**Link**: [PDF](https://arxiv.org/pdf/2503.17502)  

**Abstract**: Large language models (LLMs) and transformer-based architectures are increasingly utilized for source code analysis. As software systems grow in complexity, integrating LLMs into code analysis workflows becomes essential for enhancing efficiency, accuracy, and automation. This paper explores the role of LLMs for different code analysis tasks, focusing on three key aspects: 1) what they can analyze and their applications, 2) what models are used and 3) what datasets are used, and the challenges they face. Regarding the goal of this research, we investigate scholarly articles that explore the use of LLMs for source code analysis to uncover research developments, current trends, and the intellectual structure of this emerging field. Additionally, we summarize limitations and highlight essential tools, datasets, and key challenges, which could be valuable for future work. 

**Abstract (ZH)**: 大型语言模型（LLMs）和基于变换器的架构在源代码分析中日益受到利用。随着软件系统的复杂性增加，将LLMs集成到代码分析工作流中对于提高效率、准确性和自动化变得至关重要。本文探讨了LLMs在不同代码分析任务中的作用，重点关注三个方面：1）它们可以分析的内容及其应用，2）使用的模型，3）使用的数据集及其面临的挑战。关于本研究的目标，我们调查了探讨LLMs在源代码分析中应用的学术文章，以揭示该新兴领域的研究进展、当前趋势和知识结构。此外，我们总结了局限性，并强调了重要的工具、数据集和关键挑战，这些对于未来的工作具有重要的参考价值。 

---
# Efficient Knowledge Distillation via Curriculum Extraction 

**Title (ZH)**: 通过课程提取实现高效知识蒸馏 

**Authors**: Shivam Gupta, Sushrut Karmalkar  

**Link**: [PDF](https://arxiv.org/pdf/2503.17494)  

**Abstract**: Knowledge distillation is a technique used to train a small student network using the output generated by a large teacher network, and has many empirical advantages~\citep{Hinton2015DistillingTK}. While the standard one-shot approach to distillation only uses the output of the final teacher network, recent work~\citep{panigrahi2024progressive} has shown that using intermediate checkpoints from the teacher's training process as an implicit ``curriculum'' for progressive distillation can significantly speed up training. However, such schemes require storing these checkpoints, and often require careful selection of the intermediate checkpoints to train on, which can be impractical for large-scale training.
In this paper, we show that a curriculum can be \emph{extracted} from just the fully trained teacher network, and that this extracted curriculum can give similar efficiency benefits to those of progressive distillation. Our extraction scheme is natural; we use a random projection of the hidden representations of the teacher network to progressively train the student network, before training using the output of the full network. We show that our scheme significantly outperforms one-shot distillation and achieves a performance similar to that of progressive distillation for learning sparse parities with two-layer networks, and provide theoretical guarantees for this setting. Additionally, we show that our method outperforms one-shot distillation even when using transformer-based architectures, both for sparse-parity learning, and language modeling tasks. 

**Abstract (ZH)**: 知识蒸馏是一种使用大型教师网络生成的输出来训练小型学生网络的技术，它具有许多实证优势~\citep{Hinton2015DistillingTK}。虽然标准的一次性蒸馏方法仅使用教师网络最终的输出，但最近的研究~\citep{panigrahi2024progressive}表明，将教师网络训练过程中的中间检查点作为渐进蒸馏的隐式“课程”可以显著加快训练速度。然而，这样的方案需要存储这些检查点，并且往往需要精心选择用于训练的中间检查点，这在大规模训练中可能是不实际的。
在本文中，我们展示了可以从仅有的完全训练好的教师网络中提取出一个课程，并且提取出的课程可以提供类似于渐进蒸馏的效率优势。我们的提取方案自然；我们通过将教师网络的隐藏表示进行随机投影来逐步训练学生网络，在使用整个网络的输出进行训练之前。我们证明了我们的方案在稀疏相学习中显著优于一次性蒸馏，并且在两层网络中达到与渐进蒸馏相似的性能，并为此场景提供了理论保证。此外，我们展示了即使在使用基于Transformer的架构时，我们的方法在稀疏相学习和语言建模任务中也优于一次性蒸馏。 

---
# ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes 

**Title (ZH)**: ProtoGS: 高效的高质量化三维高斯原型渲染 

**Authors**: Zhengqing Gao, Dongting Hu, Jia-Wang Bian, Huan Fu, Yan Li, Tongliang Liu, Mingming Gong, Kun Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2503.17486)  

**Abstract**: 3D Gaussian Splatting (3DGS) has made significant strides in novel view synthesis but is limited by the substantial number of Gaussian primitives required, posing challenges for deployment on lightweight devices. Recent methods address this issue by compressing the storage size of densified Gaussians, yet fail to preserve rendering quality and efficiency. To overcome these limitations, we propose ProtoGS to learn Gaussian prototypes to represent Gaussian primitives, significantly reducing the total Gaussian amount without sacrificing visual quality. Our method directly uses Gaussian prototypes to enable efficient rendering and leverage the resulting reconstruction loss to guide prototype learning. To further optimize memory efficiency during training, we incorporate structure-from-motion (SfM) points as anchor points to group Gaussian primitives. Gaussian prototypes are derived within each group by clustering of K-means, and both the anchor points and the prototypes are optimized jointly. Our experiments on real-world and synthetic datasets prove that we outperform existing methods, achieving a substantial reduction in the number of Gaussians, and enabling high rendering speed while maintaining or even enhancing rendering fidelity. 

**Abstract (ZH)**: 基于原型的3D高斯点云合成（ProtoGS：学习高斯原型以减少高斯 primitives 数量而不牺牲视觉质量） 

---
# SaudiCulture: A Benchmark for Evaluating Large Language Models Cultural Competence within Saudi Arabia 

**Title (ZH)**: 沙特文化：评估大型语言模型文化 competence 的基准_within Saudi Arabia 

**Authors**: Lama Ayash, Hassan Alhuzali, Ashwag Alasmari, Sultan Aloufi  

**Link**: [PDF](https://arxiv.org/pdf/2503.17485)  

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing; however, they often struggle to accurately capture and reflect cultural nuances. This research addresses this challenge by focusing on Saudi Arabia, a country characterized by diverse dialects and rich cultural traditions. We introduce SaudiCulture, a novel benchmark designed to evaluate the cultural competence of LLMs within the distinct geographical and cultural contexts of Saudi Arabia. SaudiCulture is a comprehensive dataset of questions covering five major geographical regions, such as West, East, South, North, and Center, along with general questions applicable across all regions. The dataset encompasses a broad spectrum of cultural domains, including food, clothing, entertainment, celebrations, and crafts. To ensure a rigorous evaluation, SaudiCulture includes questions of varying complexity, such as open-ended, single-choice, and multiple-choice formats, with some requiring multiple correct answers. Additionally, the dataset distinguishes between common cultural knowledge and specialized regional aspects. We conduct extensive evaluations on five LLMs, such as GPT-4, Llama 3.3, FANAR, Jais, and AceGPT, analyzing their performance across different question types and cultural contexts. Our findings reveal that all models experience significant performance declines when faced with highly specialized or region-specific questions, particularly those requiring multiple correct responses. Additionally, certain cultural categories are more easily identifiable than others, further highlighting inconsistencies in LLMs cultural understanding. These results emphasize the importance of incorporating region-specific knowledge into LLMs training to enhance their cultural competence. 

**Abstract (ZH)**: 大型语言模型在自然语言处理方面展现了卓越的能力；然而，它们在准确捕捉和反映文化细微差别方面常常力不从心。本研究针对沙特阿拉伯这一以多样方言和丰富文化传统为特点的国家，提出这一挑战。我们引入了沙特文化这一新的基准，旨在评估大型语言模型在沙特阿拉伯独特地理和文化背景下的文化能力。沙特文化是一个涵盖了五个主要地理区域（西、东、南、北、中心）以及适用于所有区域的一般问题的综合数据集。该数据集涉及广泛的文化领域，包括饮食、服饰、娱乐、庆祝活动和工艺品。为确保严格的评估，沙特文化包括不同复杂度的问题，如开放型、单选和多项选择题，有些甚至要求多个正确答案。此外，该数据集区分了通用文化知识与特定区域的专门方面。我们对五种大型语言模型（如GPT-4、Llama 3.3、FANAR、Jais、AceGPT）进行了广泛的评估，分析它们在不同问题类型和文化背景下的表现。我们的发现表明，面对高度专业化或特定地区的问题时，所有模型的表现都会显著下降，尤其是那些需要多个正确答案的问题。此外，某些文化类别比其他类别更容易识别，进一步突显了大型语言模型在文化理解方面的不一致性。这些结果强调了在训练大型语言模型时融入特定地区知识的重要性，以提高其文化能力。 

---
# What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models 

**Title (ZH)**: 可生成的未必可达：生成模型的可控性度量 

**Authors**: Keyon Vafa, Sarah Bentley, Jon Kleinberg, Sendhil Mullainathan  

**Link**: [PDF](https://arxiv.org/pdf/2503.17482)  

**Abstract**: How should we evaluate the quality of generative models? Many existing metrics focus on a model's producibility, i.e. the quality and breadth of outputs it can generate. However, the actual value from using a generative model stems not just from what it can produce but whether a user with a specific goal can produce an output that satisfies that goal. We refer to this property as steerability. In this paper, we first introduce a mathematical framework for evaluating steerability independently from producibility. Steerability is more challenging to evaluate than producibility because it requires knowing a user's goals. We address this issue by creating a benchmark task that relies on one key idea: sample an output from a generative model and ask users to reproduce it. We implement this benchmark in a large-scale user study of text-to-image models and large language models. Despite the ability of these models to produce high-quality outputs, they all perform poorly on steerabilty. This suggests that we need to focus on improving the steerability of generative models. We show such improvements are indeed possible: through reinforcement learning techniques, we create an alternative steering mechanism for image models that achieves more than 2x improvement on this benchmark. 

**Abstract (ZH)**: 如何评估生成模型的质量？：独立于生成性之外评估可控性 

---
# Your voice is your voice: Supporting Self-expression through Speech Generation and LLMs in Augmented and Alternative Communication 

**Title (ZH)**: 你的声音就是你的声音：通过语音生成和大语言模型支持替代和辅助沟通中的自我表达 

**Authors**: Yiwen Xu, Monideep Chakraborti, Tianyi Zhang, Katelyn Eng, Aanchan Mohan, Mirjana Prpa  

**Link**: [PDF](https://arxiv.org/pdf/2503.17479)  

**Abstract**: In this paper, we present Speak Ease: an augmentative and alternative communication (AAC) system to support users' expressivity by integrating multimodal input, including text, voice, and contextual cues (conversational partner and emotional tone), with large language models (LLMs). Speak Ease combines automatic speech recognition (ASR), context-aware LLM-based outputs, and personalized text-to-speech technologies to enable more personalized, natural-sounding, and expressive communication. Through an exploratory feasibility study and focus group evaluation with speech and language pathologists (SLPs), we assessed Speak Ease's potential to enable expressivity in AAC. The findings highlight the priorities and needs of AAC users and the system's ability to enhance user expressivity by supporting more personalized and contextually relevant communication. This work provides insights into the use of multimodal inputs and LLM-driven features to improve AAC systems and support expressivity. 

**Abstract (ZH)**: Speak Ease：一种集成多模态输入和大规模语言模型的增强和替代沟通系统 

---
# Spatiotemporal Learning with Context-aware Video Tubelets for Ultrasound Video Analysis 

**Title (ZH)**: 基于上下文意识视频管段的时空学习超声视频分析 

**Authors**: Gary Y. Li, Li Chen, Bryson Hicks, Nikolai Schnittke, David O. Kessler, Jeffrey Shupp, Maria Parker, Cristiana Baloescu, Christopher Moore, Cynthia Gregory, Kenton Gregory, Balasundar Raju, Jochen Kruecker, Alvin Chen  

**Link**: [PDF](https://arxiv.org/pdf/2503.17475)  

**Abstract**: Computer-aided pathology detection algorithms for video-based imaging modalities must accurately interpret complex spatiotemporal information by integrating findings across multiple frames. Current state-of-the-art methods operate by classifying on video sub-volumes (tubelets), but they often lose global spatial context by focusing only on local regions within detection ROIs. Here we propose a lightweight framework for tubelet-based object detection and video classification that preserves both global spatial context and fine spatiotemporal features. To address the loss of global context, we embed tubelet location, size, and confidence as inputs to the classifier. Additionally, we use ROI-aligned feature maps from a pre-trained detection model, leveraging learned feature representations to increase the receptive field and reduce computational complexity. Our method is efficient, with the spatiotemporal tubelet classifier comprising only 0.4M parameters. We apply our approach to detect and classify lung consolidation and pleural effusion in ultrasound videos. Five-fold cross-validation on 14,804 videos from 828 patients shows our method outperforms previous tubelet-based approaches and is suited for real-time workflows. 

**Abstract (ZH)**: 基于视频成像模态的病理检测算法必须通过整合多帧信息准确解读复杂的空时信息。当前最先进的方法通过分类视频子体积（管节）来操作，但往往会因为仅关注检测ROI内的局部区域而丢失全局空间上下文。我们提出了一种轻量级框架，用于基于管节的对象检测和视频分类，能够同时保留全局空间上下文和精细的空时特征。为了解决全局上下文缺失的问题，我们将管节的位置、大小和置信度作为分类器的输入。此外，我们利用预训练检测模型的ROI对齐特征图，通过利用学习到的特征表示来增加感受野并降低计算复杂度。我们的方法高效，仅包含0.4M参数的空时管节分类器。我们应用该方法来检测和分类超声视频中的肺实变和胸腔积液。在828名患者的14,804段视频上进行五折交叉验证表明，我们的方法优于之前的管节基方法，并适用于实时工作流。 

---
# Language-specific Neurons Do Not Facilitate Cross-Lingual Transfer 

**Title (ZH)**: 特定语言神经元不利于跨语言迁移 

**Authors**: Soumen Kumar Mondal, Sayambhu Sen, Abhishek Singhania, Preethi Jyothi  

**Link**: [PDF](https://arxiv.org/pdf/2503.17456)  

**Abstract**: Multilingual large language models (LLMs) aim towards robust natural language understanding across diverse languages, yet their performance significantly degrades on low-resource languages. This work explores whether existing techniques to identify language-specific neurons can be leveraged to enhance cross-lingual task performance of lowresource languages. We conduct detailed experiments covering existing language-specific neuron identification techniques (such as Language Activation Probability Entropy and activation probability-based thresholding) and neuron-specific LoRA fine-tuning with models like Llama 3.1 and Mistral Nemo. We find that such neuron-specific interventions are insufficient to yield cross-lingual improvements on downstream tasks (XNLI, XQuAD) in lowresource languages. This study highlights the challenges in achieving cross-lingual generalization and provides critical insights for multilingual LLMs. 

**Abstract (ZH)**: 多语言大型语言模型（LLMs）旨在实现跨多种语言的稳健自然语言理解，但其在低资源语言上的性能显著下降。本研究探讨了现有技术是否可以利用识别语言特定神经元的方法来提升低资源语言跨语言任务的性能。我们进行了详细的实验，涵盖现有的语言特定神经元识别技术（如语言激活概率熵和激活概率阈值方法）以及针对如Llama 3.1和Mistral Nemo等模型的神经元特定LoRA微调。我们发现，在低资源语言的下游任务（XNLI、XQuAD）中，这样的神经元特定干预不足以带来跨语言性能的提升。本研究突出了实现跨语言泛化的挑战，并为多语言LLMs提供了关键见解。 

---
# CausalRivers -- Scaling up benchmarking of causal discovery for real-world time-series 

**Title (ZH)**: 因果河流——因果发现基准测试的扩展与应用 

**Authors**: Gideon Stein, Maha Shadaydeh, Jan Blunk, Niklas Penzel, Joachim Denzler  

**Link**: [PDF](https://arxiv.org/pdf/2503.17452)  

**Abstract**: Causal discovery, or identifying causal relationships from observational data, is a notoriously challenging task, with numerous methods proposed to tackle it. Despite this, in-the-wild evaluation of these methods is still lacking, as works frequently rely on synthetic data evaluation and sparse real-world examples under critical theoretical assumptions. Real-world causal structures, however, are often complex, making it hard to decide on a proper causal discovery strategy. To bridge this gap, we introduce CausalRivers, the largest in-the-wild causal discovery benchmarking kit for time-series data to date. CausalRivers features an extensive dataset on river discharge that covers the eastern German territory (666 measurement stations) and the state of Bavaria (494 measurement stations). It spans the years 2019 to 2023 with a 15-minute temporal resolution. Further, we provide additional data from a flood around the Elbe River, as an event with a pronounced distributional shift. Leveraging multiple sources of information and time-series meta-data, we constructed two distinct causal ground truth graphs (Bavaria and eastern Germany). These graphs can be sampled to generate thousands of subgraphs to benchmark causal discovery across diverse and challenging settings. To demonstrate the utility of CausalRivers, we evaluate several causal discovery approaches through a set of experiments to identify areas for improvement. CausalRivers has the potential to facilitate robust evaluations and comparisons of causal discovery methods. Besides this primary purpose, we also expect that this dataset will be relevant for connected areas of research, such as time-series forecasting and anomaly detection. Based on this, we hope to push benchmark-driven method development that fosters advanced techniques for causal discovery, as is the case for many other areas of machine learning. 

**Abstract (ZH)**: 因果发现，即从观察数据中识别因果关系，是一个 notoriously 挑战性的任务，尽管已经提出了众多方法来解决这一问题，但在实际应用中的评估仍然不足，研究工作经常依赖合成数据评估和在关键理论假设下的稀疏真实世界示例。然而，真实世界中的因果结构往往极为复杂，这使得选择合适的因果发现策略变得困难。为了弥合这一差距，我们介绍了CausalRivers，这是迄今为止规模最大的实际应用因果发现基准数据集，专用于时间序列数据。CausalRivers 包含覆盖德国东部（666个测站）和巴伐利亚州（494个测站）的河流流量数据集。该数据集的时间跨度为2019年至2023年，时间分辨率为15分钟。此外，我们还提供了关于易北河洪灾的附加数据，这是一个具有明显分布变化的事件。利用多种信息来源和时间序列元数据，我们构建了两个独立的因果真实图形（巴伐利亚和德国东部）。这些图形可以通过采样生成数千个子图，以在不同的复杂场景中测试因果发现方法。为了展示CausalRivers的实用性，我们通过一系列实验评估了几种因果发现方法，以识别改进领域。CausalRivers有可能促进因果发现方法的稳健评估和比较。除了这一主要目的外，我们还期望该数据集在时间序列预测和异常检测等关联研究领域中具有相关性。基于此，我们希望推动以基准驱动的方法研发，促进因果发现技术的发展，这在许多其他机器学习领域中已有先例。 

---
# LEMMA: Learning from Errors for MatheMatical Advancement in LLMs 

**Title (ZH)**: LEMMA: 从错误中学习以促进LLMs的数学进步 

**Authors**: Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, H. Vicky Zhao, Conghui He, Lijun Wu  

**Link**: [PDF](https://arxiv.org/pdf/2503.17439)  

**Abstract**: Large language models (LLMs) have demonstrated remarkable reasoning capability in solving mathematical problems. However, existing approaches primarily focus on improving the quality of correct training data, e.g., distilling high-quality correct solutions from advanced models, neglecting the value contained in error data, potentially hindering the model's reflective ability. Though some studies attempt to leverage error data, they often involve complex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error nodes. In this work, we propose to enhance LLMs' reasoning ability by Learning from Errors for Mathematical Advancement (LEMMA). LEMMA constructs data consisting of an incorrect solution with an erroneous step and a reflection connection to a correct solution for fine-tuning. Specifically, we systematically analyze the model-generated error types and introduce an error-type grounded mistake augmentation method to collect diverse and representative errors. Correct solutions are either from fixing the errors or generating a fresh start. Through a model-aware smooth reflection connection, the erroneous solution is transferred to the correct one. By fine-tuning on the constructed dataset, the model is able to self-correct errors autonomously within the generation process without relying on external critique models. Experimental results demonstrate that LEMMA achieves significant performance improvements over other strong baselines. 

**Abstract (ZH)**: 基于错误学习的数学进步（LEMMA）增强大型语言模型的推理能力 

---
# Enhanced Smart Contract Reputability Analysis using Multimodal Data Fusion on Ethereum 

**Title (ZH)**: 基于多模态数据融合的增强 Ethereum 智能合约声誉分析 

**Authors**: Cyrus Malik, Josef Bajada, Joshua Ellul  

**Link**: [PDF](https://arxiv.org/pdf/2503.17426)  

**Abstract**: The evaluation of smart contract reputability is essential to foster trust in decentralized ecosystems. However, existing methods that rely solely on static code analysis or transactional data, offer limited insight into evolving trustworthiness. We propose a multimodal data fusion framework that integrates static code features with transactional data to enhance reputability prediction. Our framework initially focuses on static code analysis, utilizing GAN-augmented opcode embeddings to address class imbalance, achieving 97.67% accuracy and a recall of 0.942 in detecting illicit contracts, surpassing traditional oversampling methods. This forms the crux of a reputability-centric fusion strategy, where combining static and transactional data improves recall by 7.25% over single-source models, demonstrating robust performance across validation sets. By providing a holistic view of smart contract behaviour, our approach enhances the model's ability to assess reputability, identify fraudulent activities, and predict anomalous patterns. These capabilities contribute to more accurate reputability assessments, proactive risk mitigation, and enhanced blockchain security. 

**Abstract (ZH)**: 智能合约声誉性的多模态数据融合评估对于促进去中心化生态系统中的信任至关重要。现有的仅依赖静态代码分析或交易数据的方法提供有限的关于信任演化程度的见解。我们提出了一种多模态数据融合框架，将静态代码特征与交易数据结合起来以增强声誉性预测。该框架首先专注于静态代码分析，利用GAN增强的opcode嵌入来解决类别不平衡问题，实现了97.67%的准确率和0.942的召回率，超越了传统的过采样方法。这构成了以声誉性为中心的融合策略的核心，结合静态和交易数据比单一数据源模型召回率提升7.25%，展示了在验证集上的稳健性能。通过提供智能合约行为的全面视角，我们的方法增强了模型评估声誉性、识别欺诈活动和预测异常模式的能力，从而促进了更准确的声誉性评估、主动风险管理以及区块链安全的提升。 

---
# Data to Decisions: A Computational Framework to Identify skill requirements from Advertorial Data 

**Title (ZH)**: 从广告数据到决策：一种识别技能需求的计算框架 

**Authors**: Aakash Singh, Anurag Kanaujia, Vivek Kumar Singh  

**Link**: [PDF](https://arxiv.org/pdf/2503.17424)  

**Abstract**: Among the factors of production, human capital or skilled manpower is the one that keeps evolving and adapts to changing conditions and resources. This adaptability makes human capital the most crucial factor in ensuring a sustainable growth of industry/sector. As new technologies are developed and adopted, the new generations are required to acquire skills in newer technologies in order to be employable. At the same time professionals are required to upskill and reskill themselves to remain relevant in the industry. There is however no straightforward method to identify the skill needs of the industry at a given point of time. Therefore, this paper proposes a data to decision framework that can successfully identify the desired skill set in a given area by analysing the advertorial data collected from popular online job portals and supplied as input to the framework. The proposed framework uses techniques of statistical analysis, data mining and natural language processing for the purpose. The applicability of the framework is demonstrated on CS&IT job advertisement data from India. The analytical results not only provide useful insights about current state of skill needs in CS&IT industry but also provide practical implications to prospective job applicants, training agencies, and institutions of higher education & professional training. 

**Abstract (ZH)**: 生产要素中，人力资本或技术劳动力是最具适应性的因素，能够适应不断变化的条件和资源。这种适应性使得人力资本成为确保产业可持续增长的关键因素。随着新技术的开发和采纳，新一代人需要掌握新的技术以保持就业能力。与此同时，专业人士也需要不断自我提升和重新技能培训以保持在行业的相关性。然而，并没有直接的方法来确定某一时间点行业的技能需求。因此，本文提出了一种数据到决策的框架，通过分析来自流行的在线招聘网站的广告数据并将其作为输入提供给框架，以成功识别给定区域所需的技能集。该框架利用统计分析、数据挖掘和自然语言处理技术。该框架的应用性在印度CS&IT职位广告数据上进行了验证。分析结果不仅提供了有关CS&IT行业当前技能需求状态的有用见解，还为求职者、培训机构以及高等教育和专业培训机构提供了实际意义。 

---
# Understanding Social Support Needs in Questions: A Hybrid Approach Integrating Semi-Supervised Learning and LLM-based Data Augmentation 

**Title (ZH)**: 基于半监督学习和基于LLM的数据增强的混合方法理解问题中的社会支持需求 

**Authors**: Junwei Kuang, Liang Yang, Shaoze Cui, Weiguo Fan  

**Link**: [PDF](https://arxiv.org/pdf/2503.17421)  

**Abstract**: Patients are increasingly turning to online health Q&A communities for social support to improve their well-being. However, when this support received does not align with their specific needs, it may prove ineffective or even detrimental. This necessitates a model capable of identifying the social support needs in questions. However, training such a model is challenging due to the scarcity and class imbalance issues of labeled data. To overcome these challenges, we follow the computational design science paradigm to develop a novel framework, Hybrid Approach for SOcial Support need classification (HA-SOS). HA-SOS integrates an answer-enhanced semi-supervised learning approach, a text data augmentation technique leveraging large language models (LLMs) with reliability- and diversity-aware sample selection mechanism, and a unified training process to automatically label social support needs in questions. Extensive empirical evaluations demonstrate that HA-SOS significantly outperforms existing question classification models and alternative semi-supervised learning approaches. This research contributes to the literature on social support, question classification, semi-supervised learning, and text data augmentation. In practice, our HA-SOS framework facilitates online Q&A platform managers and answerers to better understand users' social support needs, enabling them to provide timely, personalized answers and interventions. 

**Abstract (ZH)**: 患者越来越多地转向在线健康问答社区寻求社交支持以改善福祉。然而，当接收到的支持不符合其特定需求时，可能会无效甚至有害。这就需要一个能够识别问题中社交支持需求的模型。但由于标记数据稀缺且存在类别不平衡问题，训练此类模型颇具挑战性。为克服这些挑战，我们遵循计算设计科学范式，开发了一个新的框架——混合方法进行社会支持需求分类（HA-SOS）。HA-SOS结合了答案增强的半监督学习方法、利用大型语言模型（LLMs）的数据文本增强技术以及可靠性与多样意识样的样本选择机制，并采用统一的训练过程自动标注问题中的社会支持需求。广泛的实证评估表明，HA-SOS在社会支持需求分类和半监督学习方面显著优于现有模型和替代方法。本研究为社会支持、问题分类、半监督学习和文本数据增强等领域文献做出了贡献。在实践中，我们的HA-SOS框架有助于在线问答平台管理者和回答者更好地理解用户的社会支持需求，使他们能够提供及时、个性化的回答和干预。 

---
# Generative Modeling of Class Probability for Multi-Modal Representation Learning 

**Title (ZH)**: 多模态表示学习中的类别概率生成建模 

**Authors**: Jungkyoo Shin, Bumsoo Kim, Eunwoo Kim  

**Link**: [PDF](https://arxiv.org/pdf/2503.17417)  

**Abstract**: Multi-modal understanding plays a crucial role in artificial intelligence by enabling models to jointly interpret inputs from different modalities. However, conventional approaches such as contrastive learning often struggle with modality discrepancies, leading to potential misalignments. In this paper, we propose a novel class anchor alignment approach that leverages class probability distributions for multi-modal representation learning. Our method, Class-anchor-ALigned generative Modeling (CALM), encodes class anchors as prompts to generate and align class probability distributions for each modality, enabling more effective alignment. Furthermore, we introduce a cross-modal probabilistic variational autoencoder to model uncertainty in the alignment, enhancing the ability to capture deeper relationships between modalities and data variations. Extensive experiments on four benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, especially in out-of-domain evaluations. This highlights its superior generalization capabilities in multi-modal representation learning. 

**Abstract (ZH)**: 多模态理解在人工智能中起着至关重要的作用，它使模型能够联合解释不同模态的输入。然而，传统的对比学习等方法往往在处理模态差异时遇到困难，导致潜在的对齐问题。本文提出了一种新颖的类别锚点对齐方法，利用类别概率分布进行多模态表示学习。我们的方法，类别锚点对齐生成模型（CALM），将类别锚点编码为提示，生成并对齐每个模态的类别概率分布，从而实现更有效的对齐。此外，我们引入了跨模态概率变分自动编码器来建模对齐中的不确定性，增强了捕捉模态之间更深层次关系以及数据变化的能力。在四个基准数据集上的广泛实验表明，我们的方法在优于现有最佳方法的同时，在域外评估中表现尤为突出，这突显了其在多模态表示学习中的优越泛化能力。 

---
# Debugging and Runtime Analysis of Neural Networks with VLMs (A Case Study) 

**Title (ZH)**: 使用VLMs进行神经网络的调试与运行时分析：一个案例研究 

**Authors**: Boyue Caroline Hu, Divya Gopinath, Corina S. Pasareanu, Nina Narodytska, Ravi Mangal, Susmit Jha  

**Link**: [PDF](https://arxiv.org/pdf/2503.17416)  

**Abstract**: Debugging of Deep Neural Networks (DNNs), particularly vision models, is very challenging due to the complex and opaque decision-making processes in these networks. In this paper, we explore multi-modal Vision-Language Models (VLMs), such as CLIP, to automatically interpret the opaque representation space of vision models using natural language. This in turn, enables a semantic analysis of model behavior using human-understandable concepts, without requiring costly human annotations. Key to our approach is the notion of semantic heatmap, that succinctly captures the statistical properties of DNNs in terms of the concepts discovered with the VLM and that are computed off-line using a held-out data set. We show the utility of semantic heatmaps for fault localization -- an essential step in debugging -- in vision models. Our proposed technique helps localize the fault in the network (encoder vs head) and also highlights the responsible high-level concepts, by leveraging novel differential heatmaps, which summarize the semantic differences between the correct and incorrect behaviour of the analyzed DNN. We further propose a lightweight runtime analysis to detect and filter-out defects at runtime, thus improving the reliability of the analyzed DNNs. The runtime analysis works by measuring and comparing the similarity between the heatmap computed for a new (unseen) input and the heatmaps computed a-priori for correct vs incorrect DNN behavior. We consider two types of defects: misclassifications and vulnerabilities to adversarial attacks. We demonstrate the debugging and runtime analysis on a case study involving a complex ResNet-based classifier trained on the RIVAL10 dataset. 

**Abstract (ZH)**: 基于多模态视觉语言模型的深度神经网络调试 

---
# Enhancing Subsequent Video Retrieval via Vision-Language Models (VLMs) 

**Title (ZH)**: 通过视觉-语言模型增强后续视频检索 

**Authors**: Yicheng Duan, Xi Huang, Duo Chen  

**Link**: [PDF](https://arxiv.org/pdf/2503.17415)  

**Abstract**: The rapid growth of video content demands efficient and precise retrieval systems. While vision-language models (VLMs) excel in representation learning, they often struggle with adaptive, time-sensitive video retrieval. This paper introduces a novel framework that combines vector similarity search with graph-based data structures. By leveraging VLM embeddings for initial retrieval and modeling contextual relationships among video segments, our approach enables adaptive query refinement and improves retrieval accuracy. Experiments demonstrate its precision, scalability, and robustness, offering an effective solution for interactive video retrieval in dynamic environments. 

**Abstract (ZH)**: 视频内容的快速增长对高效精确的检索系统提出了需求。虽然视觉语言模型（VLMs）在表示学习方面表现出色，但在适应性和时间敏感的视频检索方面常常遇到困难。本文提出了一种将向量相似度搜索与图基数据结构相结合的新框架。通过利用VLM嵌入进行初始检索，并建模视频片段之间的上下文关系，我们的方法能够实现适应性查询 refinement 并提高检索精度。实验结果显示其精度、可扩展性和鲁棒性，提供了一种有效的交互式视频检索解决方案，适用于动态环境。 

---
# Opportunities and Challenges of Frontier Data Governance With Synthetic Data 

**Title (ZH)**: 前沿数据治理中的合成数据机遇与挑战 

**Authors**: Madhavendra Thakur, Jason Hausenloy  

**Link**: [PDF](https://arxiv.org/pdf/2503.17414)  

**Abstract**: Synthetic data, or data generated by machine learning models, is increasingly emerging as a solution to the data access problem. However, its use introduces significant governance and accountability challenges, and potentially debases existing governance paradigms, such as compute and data governance. In this paper, we identify 3 key governance and accountability challenges that synthetic data poses - it can enable the increased emergence of malicious actors, spontaneous biases and value drift. We thus craft 3 technical mechanisms to address these specific challenges, finding applications for synthetic data towards adversarial training, bias mitigation and value reinforcement. These could not only counteract the risks of synthetic data, but serve as critical levers for governance of the frontier in the future. 

**Abstract (ZH)**: 合成数据作为一种由机器学习模型生成的数据，在解决数据访问问题方面越来越受到关注，但其使用引入了重大的治理和问责挑战，并可能 debases 现有的治理范式，如计算和数据治理。本文识别了合成数据带来的 3 个关键治理和问责挑战：它可能促进恶意行为者增多、自发偏见的出现以及价值漂移。为此，我们提出了 3 种技术机制来应对这些特定挑战，并探讨了合成数据在对抗性训练、偏见缓解和价值强化方面的应用。这些机制不仅能够抵消合成数据的风险，还可能成为未来治理框架中的关键杠杆。 

---
# Comparative Analysis of Deep Learning Models for Real-World ISP Network Traffic Forecasting 

**Title (ZH)**: 深度学习模型在实际ISP网络流量预测中的 comparative analysis 

**Authors**: Josef Koumar, Timotej Smoleň, Kamil Jeřábek, Tomáš Čejka  

**Link**: [PDF](https://arxiv.org/pdf/2503.17410)  

**Abstract**: Accurate network traffic forecasting is essential for Internet Service Providers (ISP) to optimize resources, enhance user experience, and mitigate anomalies. This study evaluates state-of-the-art deep learning models on CESNET-TimeSeries24, a recently published, comprehensive real-world network traffic dataset from the ISP network CESNET3 spanning multivariate time series over 40 weeks. Our findings highlight the balance between prediction accuracy and computational efficiency across different levels of network granularity. Additionally, this work establishes a reproducible methodology that facilitates direct comparison of existing approaches, explores their strengths and weaknesses, and provides a benchmark for future studies using this dataset. 

**Abstract (ZH)**: 准确的网络流量预测对于Internet服务提供商优化资源、提升用户体验和缓解异常至关重要。本研究评估了最新的深度学习模型在CESNET-TimeSeries24数据集上的性能，该数据集是来自ISP网络CESNET3的综合实际网络流量数据，涵盖40周多变量时间序列。研究发现突显了不同网络粒度级别下预测准确性和计算效率之间的平衡关系。此外，本工作还建立了一种可重复的方法，便于直接比较现有方法，探索其优缺点，并为未来使用此数据集的研究提供基准。 

---
# Leveraging OpenFlamingo for Multimodal Embedding Analysis of C2C Car Parts Data 

**Title (ZH)**: 利用OpenFlamingo进行C2C汽车零部件多模态嵌入分析 

**Authors**: Maisha Binte Rashid, Pablo Rivas  

**Link**: [PDF](https://arxiv.org/pdf/2503.17408)  

**Abstract**: In this paper, we aim to investigate the capabilities of multimodal machine learning models, particularly the OpenFlamingo model, in processing a large-scale dataset of consumer-to-consumer (C2C) online posts related to car parts. We have collected data from two platforms, OfferUp and Craigslist, resulting in a dataset of over 1.2 million posts with their corresponding images. The OpenFlamingo model was used to extract embeddings for the text and image of each post. We used $k$-means clustering on the joint embeddings to identify underlying patterns and commonalities among the posts. We have found that most clusters contain a pattern, but some clusters showed no internal patterns. The results provide insight into the fact that OpenFlamingo can be used for finding patterns in large datasets but needs some modification in the architecture according to the dataset. 

**Abstract (ZH)**: 本文旨在探究多模态机器学习模型，特别是OpenFlamingo模型，处理与汽车零件相关的消费者对消费者（C2C）在线帖子的大规模数据集的能力。我们从OfferUp和 Craigslist两个平台收集数据，形成了包含超过120万条帖子及其对应图像的大规模数据集。使用OpenFlamingo模型提取每条帖子的文字和图像的嵌入向量。通过联合嵌入向量进行$k$-means聚类，识别帖子中的潜在模式和共同点。研究发现大多数聚类包含模式，但有些聚类没有内部模式。结果表明OpenFlamingo可以用于在大数据集中发现模式，但需要根据数据集对模型架构进行一些修改。 

---
# ChatGPT or A Silent Everywhere Helper: A Survey of Large Language Models 

**Title (ZH)**: ChatGPT 或者无处不在的沉默助手：大规模语言模型综述 

**Authors**: Azim Akhtarshenas, Afshin Dini, Navid Ayoobi  

**Link**: [PDF](https://arxiv.org/pdf/2503.17403)  

**Abstract**: Large Language Models (LLMs) have revo lutionized natural language processing Natural Language Processing (NLP), with Chat Generative Pre-trained Transformer (ChatGPT) standing out as a notable exampledue to its advanced capabilities and widespread applications. This survey provides a comprehensive analysis of ChatGPT, exploring its architecture, training processes, and functionalities. We examine its integration into various domains across industries such as customer service, education, healthcare, and entertainment. A comparative analysis with other LLMs highlights ChatGPT's unique features and performance metrics. Regarding benchmarks, the paper examines ChatGPT's comparative performance against other LLMs and discusses potential risks such as misinformation, bias, and data privacy concerns. Additionally, we offer a number of figures and tables that outline the backdrop of the discussion, the main ideas of the article, the numerous LLM models, a thorough list of datasets used for pre-training, fine-tuning, and evaluation, as well as particular LLM applications with pertinent references. Finally, we identify future research directions and technological advancements, underscoring the evolving landscape of LLMs and their profound impact on artificial intelligence Artificial Intelligence (AI) and society. 

**Abstract (ZH)**: 大规模语言模型（LLMs）已从根本上革新了自然语言处理（NLP），其中Chat生成预训练变换器（ChatGPT）因其先进的能力和广泛的应用而格外突出。本文对该领域的综述进行深入分析，探讨了ChatGPT的架构、训练过程和功能。我们研究了其在包括客户服务、教育、医疗保健和娱乐在内的各个行业的广泛应用。与其他大规模语言模型的对比分析突显了ChatGPT的独特特性和性能指标。对于基准测试，本文考察了ChatGPT与其他大规模语言模型的相对性能，并讨论了潜在的风险，如错误信息、偏见和数据隐私问题。此外，我们提供了许多图表，概述了讨论的背景、文章的主要思想、各种大规模语言模型列表、用于预训练、微调和评估的数据集详单，以及特定的大规模语言模型应用及其相关参考文献。最后，我们指出了未来的研究方向和技术进步，强调了大规模语言模型不断演变的格局及其对人工智能（AI）和社会的深远影响。 

---
# AEJIM: A Real-Time AI Framework for Crowdsourced, Transparent, and Ethical Environmental Hazard Detection and Reporting 

**Title (ZH)**: AEJIM：一种实时人工智能框架，用于 crowdsourced、透明和伦理的环境危害检测与报告 

**Authors**: Torsten Tiltack  

**Link**: [PDF](https://arxiv.org/pdf/2503.17401)  

**Abstract**: Environmental journalism is vital for raising awareness of ecological crises and driving evidence-based policy, yet traditional methods falter under delays, inaccuracies, and scalability limits, especially in under-monitored regions critical to the United Nations Sustainable Development Goals. To bridge these gaps, this paper introduces the AI-Environmental Journalism Integration Model (AEJIM), an innovative framework combining real-time hazard detection, crowdsourced validation, and AI-driven reporting.
Validated through a pilot study, AEJIM significantly improved the speed and accuracy of environmental hazard reporting, outperforming traditional methods. Furthermore, the model directly addresses key ethical, regulatory, and scalability challenges, ensuring AI accountability through Explainable AI (XAI), GDPR-compliant data governance, and active public participation. AEJIM provides a transparent and adaptable solution, setting a new benchmark for AI-enhanced environmental journalism and supporting informed global decision-making across diverse socio-political landscapes. 

**Abstract (ZH)**: 环保 journalism 对提高生态危机意识和推动基于证据的政策至关重要，但传统方法在延时、不准确和可扩展性方面存在局限，尤其是在对于联合国可持续发展目标至关重要的未监测地区。为此，本文引入了AI-环保 Journalism 整合模型（AEJIM），这是一种将实时风险检测、群众验证和AI驱动报道结合在一起的创新框架。经过试点研究验证，AEJIM 显著提高了环境风险报告的速度和准确性，超越了传统方法。此外，该模型直接解决了关键的伦理、监管和可扩展性挑战，通过可解释的AI（XAI）、GDPR 合规的数据治理和积极的公众参与确保AI问责。AEJIM 提供了一个透明且可适应的解决方案，为AI增强的环保 journalism 设立了新标准，并支持跨多样社会政治景观的知情全球决策。 

---
# CP-NCBF: A Conformal Prediction-based Approach to Synthesize Verified Neural Control Barrier Functions 

**Title (ZH)**: CP-NCBF：一种基于齐性预测的方法合成验证神经控制壁垒函数 

**Authors**: Manan Tayal, Aditya Singh, Pushpak Jagtap, Shishir Kolathaya  

**Link**: [PDF](https://arxiv.org/pdf/2503.17395)  

**Abstract**: Control Barrier Functions (CBFs) are a practical approach for designing safety-critical controllers, but constructing them for arbitrary nonlinear dynamical systems remains a challenge. Recent efforts have explored learning-based methods, such as neural CBFs (NCBFs), to address this issue. However, ensuring the validity of NCBFs is difficult due to potential learning errors. In this letter, we propose a novel framework that leverages split-conformal prediction to generate formally verified neural CBFs with probabilistic guarantees based on a user-defined error rate, referred to as CP-NCBF. Unlike existing methods that impose Lipschitz constraints on neural CBF-leading to scalability limitations and overly conservative safe sets--our approach is sample-efficient, scalable, and results in less restrictive safety regions. We validate our framework through case studies on obstacle avoidance in autonomous driving and geo-fencing of aerial vehicles, demonstrating its ability to generate larger and less conservative safe sets compared to conventional techniques. 

**Abstract (ZH)**: 基于split-conformal预测的正式验证神经控制 Barrier 函数（CP-NCBF）框架：应用于自主驾驶中的障碍物避障和航空器的地理围栏 

---
# Temporal Flexibility in Spiking Neural Networks: Towards Generalization Across Time Steps and Deployment Friendliness 

**Title (ZH)**: 时空灵活的脉冲神经网络：面向时间步长泛化与部署友好性 

**Authors**: Kangrui Du, Yuhang Wu, Shikuang Deng, Shi Gu  

**Link**: [PDF](https://arxiv.org/pdf/2503.17394)  

**Abstract**: Spiking Neural Networks (SNNs), models inspired by neural mechanisms in the brain, allow for energy-efficient implementation on neuromorphic hardware. However, SNNs trained with current direct training approaches are constrained to a specific time step. This "temporal inflexibility" 1) hinders SNNs' deployment on time-step-free fully event-driven chips and 2) prevents energy-performance balance based on dynamic inference time steps. In this study, we first explore the feasibility of training SNNs that generalize across different time steps. We then introduce Mixed Time-step Training (MTT), a novel method that improves the temporal flexibility of SNNs, making SNNs adaptive to diverse temporal structures. During each iteration of MTT, random time steps are assigned to different SNN stages, with spikes transmitted between stages via communication modules. After training, the weights are deployed and evaluated on both time-stepped and fully event-driven platforms. Experimental results show that models trained by MTT gain remarkable temporal flexibility, friendliness for both event-driven and clock-driven deployment (nearly lossless on N-MNIST and 10.1% higher than standard methods on CIFAR10-DVS), enhanced network generalization, and near SOTA performance. To the best of our knowledge, this is the first work to report the results of large-scale SNN deployment on fully event-driven scenarios. 

**Abstract (ZH)**: 基于混合时间步训练的Spiking神经网络的时间灵活性提升研究 

---
# AI-driven Automation of End-to-end Assessment of Suturing Expertise 

**Title (ZH)**: 基于AI的缝合技能端到端评估自动化 

**Authors**: Atharva Deo, Nicholas Matsumoto, Sun Kim, Peter Wager, Randy G. Tsai, Aaron Denmark, Cherine Yang, Xi Li, Jay Moran, Miguel Hernandez, Andrew J. Hung  

**Link**: [PDF](https://arxiv.org/pdf/2503.17391)  

**Abstract**: We present an AI based approach to automate the End-to-end Assessment of Suturing Expertise (EASE), a suturing skills assessment tool that comprehensively defines criteria around relevant sub-skills.1 While EASE provides granular skills assessment related to suturing to provide trainees with an objective evaluation of their aptitude along with actionable insights, the scoring process is currently performed by human evaluators, which is time and resource consuming. The AI based approach solves this by enabling real-time score prediction with minimal resources during model inference. This enables the possibility of real-time feedback to the surgeons/trainees, potentially accelerating the learning process for the suturing task and mitigating critical errors during the surgery, improving patient outcomes. In this study, we focus on the following 7 EASE domains that come under 3 suturing phases: 1) Needle Handling: Number of Repositions, Needle Hold Depth, Needle Hold Ratio, and Needle Hold Angle; 2) Needle Driving: Driving Smoothness, and Wrist Rotation; 3) Needle Withdrawal: Wrist Rotation. 

**Abstract (ZH)**: 基于AI的全自动端到端缝合技巧评估（EASE）方法：一种全面定义相关子技能标准的缝合技能评估工具 

---
# AI Companies Should Report Pre- and Post-Mitigation Safety Evaluations 

**Title (ZH)**: AI公司应报告预处理和后处理安全性评估结果 

**Authors**: Dillon Bowen, Ann-Kathrin Dombrowski, Adam Gleave, Chris Cundy  

**Link**: [PDF](https://arxiv.org/pdf/2503.17388)  

**Abstract**: The rapid advancement of AI systems has raised widespread concerns about potential harms of frontier AI systems and the need for responsible evaluation and oversight. In this position paper, we argue that frontier AI companies should report both pre- and post-mitigation safety evaluations to enable informed policy decisions. Evaluating models at both stages provides policymakers with essential evidence to regulate deployment, access, and safety standards. We show that relying on either in isolation can create a misleading picture of model safety. Our analysis of AI safety disclosures from leading frontier labs identifies three critical gaps: (1) companies rarely evaluate both pre- and post-mitigation versions, (2) evaluation methods lack standardization, and (3) reported results are often too vague to inform policy. To address these issues, we recommend mandatory disclosure of pre- and post-mitigation capabilities to approved government bodies, standardized evaluation methods, and minimum transparency requirements for public safety reporting. These ensure that policymakers and regulators can craft targeted safety measures, assess deployment risks, and scrutinize companies' safety claims effectively. 

**Abstract (ZH)**: 人工智能系统的迅速发展引发了对前沿人工智能系统潜在危害的广泛担忧，以及对其负责任评估和监管的需要。在本文中，我们主张前沿人工智能公司应报告预干预和后干预的安全评估，以促进知情政策决策。在两个阶段评估模型为政策制定者提供了至关重要的证据，以规范部署、访问和安全标准。我们表明，仅依赖其中任何一个都会导致对模型安全性的误导性描述。通过对领先前沿实验室的人工智能安全披露进行分析，我们发现了三个关键缺口：（1）公司通常不评估预干预和后干预的版本，（2）评估方法缺乏标准化，（3）报告的结果往往不够具体，无法为政策制定提供信息。为解决这些问题，我们建议强制要求在获批的政府机构披露预干预和后干预的能力，采用标准化的评估方法，并对公众安全报告设定最低透明度要求。这些措施确保政策制定者和监管机构能够制定有针对性的安全措施，评估部署风险，并有效地审查公司的安全声明。 

---
# Non-Canonical Crosslinks Confound Evolutionary Protein Structure Models 

**Title (ZH)**: 非范式交叉链接困扰着进化蛋白质结构模型 

**Authors**: Romain Lacombe  

**Link**: [PDF](https://arxiv.org/pdf/2503.17368)  

**Abstract**: Evolution-based protein structure prediction models have achieved breakthrough success in recent years. However, they struggle to generalize beyond evolutionary priors and on sequences lacking rich homologous data. Here we present a novel, out-of-domain benchmark based on sactipeptides, a rare class of ribosomally synthesized and post-translationally modified peptides (RiPPs) characterized by sulfur-to-$\alpha$-carbon thioether bridges creating cross-links between cysteine residues and backbone. We evaluate recent models on predicting conformations compatible with these cross-links bridges for the 10 known sactipeptides with elucidated post-translational modifications. Crucially, the structures of 5 of them have not yet been experimentally resolved. This makes the task a challenging problem for evolution-based models, which we find exhibit limited performance (0.0% to 19.2% GDT-TS on sulfur-to-$\alpha$-carbon distance). Our results point at the need for physics-informed models to sustain progress in biomolecular structure prediction. 

**Abstract (ZH)**: 基于进化的新颖领域外基准：硫至α碳硫醚桥连接的桑蒂肽结构预测 

---
# How Effective Is Constitutional AI in Small LLMs? A Study on DeepSeek-R1 and Its Peers 

**Title (ZH)**: 宪法AI在小型LLM中的有效性：DeepSeek-R1及其同行的研究 

**Authors**: Antonio-Gabriel Chacón Menke, Phan Xuan Tan  

**Link**: [PDF](https://arxiv.org/pdf/2503.17365)  

**Abstract**: Recent incidents highlight safety risks in Large Language Models (LLMs), motivating research into alignment methods like Constitutional AI (CAI). This paper explores CAI's self-critique mechanism on small, uncensored 7-9B parameter models: DeepSeek-R1, Gemma-2, Llama 3.1, and Qwen2.5. Using HarmBench, we demonstrate that while all models showed capacity for harm reduction through self-critique, effectiveness varied significantly, with DeepSeek-R1's explicit reasoning process yielding superior results. These findings suggest that CAI-inspired prompting strategies can enhance safety in resource-constrained models, though success depends on the model's capacity for harm detection. 

**Abstract (ZH)**: 近期事件突显了大型语言模型（LLMs）的安全风险，促使研究如宪法AI（CAI）这样的对齐方法。本文探讨了CAI在对小规模、未受限制的7-9B参数模型（DeepSeek-R1、Gemma-2、Llama 3.1和Qwen2.5）进行自我批判机制的研究：利用HarmBench，我们证明尽管所有模型都表现出通过自我批判减少危害的能力，但效果差异显著，DeepSeek-R1的明确推理过程产生了更优的结果。这些发现表明，受CAI启发的提示策略可以增强资源受限模型的安全性，尽管成功取决于模型对危害的检测能力。 

---
# Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants 

**Title (ZH)**: 大助手中还是大弟弟？生成式人工智能助手的审计、建模与个性化探究 

**Authors**: Yash Vekaria, Aurelio Loris Canino, Jonathan Levitsky, Alex Ciechonski, Patricia Callejo, Anna Maria Mandalari, Zubair Shafiq  

**Link**: [PDF](https://arxiv.org/pdf/2503.16586)  

**Abstract**: Generative AI (GenAI) browser assistants integrate powerful capabilities of GenAI in web browsers to provide rich experiences such as question answering, content summarization, and agentic navigation. These assistants, available today as browser extensions, can not only track detailed browsing activity such as search and click data, but can also autonomously perform tasks such as filling forms, raising significant privacy concerns. It is crucial to understand the design and operation of GenAI browser extensions, including how they collect, store, process, and share user data. To this end, we study their ability to profile users and personalize their responses based on explicit or inferred demographic attributes and interests of users. We perform network traffic analysis and use a novel prompting framework to audit tracking, profiling, and personalization by the ten most popular GenAI browser assistant extensions. We find that instead of relying on local in-browser models, these assistants largely depend on server-side APIs, which can be auto-invoked without explicit user interaction. When invoked, they collect and share webpage content, often the full HTML DOM and sometimes even the user's form inputs, with their first-party servers. Some assistants also share identifiers and user prompts with third-party trackers such as Google Analytics. The collection and sharing continues even if a webpage contains sensitive information such as health or personal information such as name or SSN entered in a web form. We find that several GenAI browser assistants infer demographic attributes such as age, gender, income, and interests and use this profile--which carries across browsing contexts--to personalize responses. In summary, our work shows that GenAI browser assistants can and do collect personal and sensitive information for profiling and personalization with little to no safeguards. 

**Abstract (ZH)**: 基于生成式AI的浏览器助手整合了强大生成式AI能力以提供丰富的浏览体验，如问答、内容总结和智能导航。这些助手作为浏览器插件可用，不仅能追踪详细的浏览活动，如搜索和点击数据，还能自主完成填写表单等任务，引发重大隐私担忧。理解生成式AI浏览器插件的设计和运行机制至关重要，包括它们如何收集、存储、处理和共享用户数据。为此，我们研究了它们基于用户显性和隐性的人口统计属性和兴趣进行用户画像和个人化响应的能力。我们进行了网络流量分析，并使用新颖的提示框架对十款最流行的生成式AI浏览器助手插件的跟踪、画像和个人化行为进行审计。我们发现，这些助手主要依赖服务器端API，这些API可以在用户未进行显式交互的情况下自动调用。调用时，它们会收集并与其他第一方服务器共享网页内容，包括完整HTML DOM，有时还会共享用户的表单输入。一些助手还会与第三方跟踪器（如Google Analytics）共享标识符和用户提示。即使网页包含敏感信息，如健康或个人信息（如姓名或社保号码），收集和共享过程也继续进行。我们发现，多个生成式AI浏览器助手能够推断出年龄、性别、收入和兴趣等人口统计属性，并使用这些画像（这些画像跨越不同的浏览上下文）来个性化响应。总之，我们的研究显示，生成式AI浏览器助手在几乎没有安全措施的情况下收集并使用个人和敏感信息进行画像和个性化处理。 

---
