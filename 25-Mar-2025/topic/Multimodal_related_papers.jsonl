{'arxiv_id': 'arXiv:2503.18533', 'title': 'MMCR: Advancing Visual Language Model in Multimodal Multi-Turn Contextual Reasoning', 'authors': 'Dawei Yan, Yang Li, Qing-Guo Chen, Weihua Luo, Peng Wang, Haokui Zhang, Chunhua Shen', 'link': 'https://arxiv.org/abs/2503.18533', 'abstract': 'Compared to single-turn dialogue, multi-turn dialogue involving multiple images better aligns with the needs of real-world human-AI interactions. Additionally, as training data, it provides richer contextual reasoning information, thereby guiding the model to achieve better performance. However, existing vision-language models (VLMs) primarily rely on single-turn dialogue training and evaluation benchmarks. In this paper, following the characteristics of human dialogue, such as focused topics and concise, clear content, we present MMCR (Multimodal Multi-turn Contextual Reasoning), a novel dataset comprising: (1) MMCR-310k -- the largest multi-image multi-turn instruction tuning dataset with 310K contextual dialogues, each covering 1-4 images and 4 or 8 dialogue turns; and (2) MMCR-Bench -- a diagnostic benchmark featuring dialogues, spanning 8 domains (Humanities, Natural, Science, Education, etc.) and 40 sub-topics. Extensive evaluations demonstrate that models fine-tuned with MMCR-310k achieve 5.2\\% higher contextual accuracy on MMCR-Bench, while showing consistent improvements on existing benchmarks (+1.1\\% on AI2D, +1.2\\% on MMMU and MMVet). MMCR and prompt engineering will be released publicly.', 'abstract_zh': '多轮对话结合多幅图像相比单轮对话更能满足真实世界人类-人工智能交互的需求。此外，作为训练数据，它提供了更丰富的上下文推理信息，从而引导模型达到更好的性能。然而，现有的跨模态语言模型（VLMs）主要依赖单轮对话的训练和评估标准。在本文中，我们借鉴人类对话的特点，如聚焦主题和简洁清晰的内容，提出了MMCR（多模态多轮上下文推理）数据集，包括：（1）MMCR-310k——包含31万个涉及1-4幅图像和4或8轮对话的多图像多轮指令调优数据集；（2）MMCR-Bench——涵盖8个领域（人文、自然、科学、教育等）和40个子主题的诊断基准。广泛的评估表明，使用MMCR-310k微调的模型在MMCR-Bench上的上下文准确性提高了5.2%，同时在现有基准上也显示出了持续的改进（AI2D上提高了1.1%，MMMU上提高了1.2%，MMVet上提高了1.2%）。MMCR和提示工程技术将公开发布。', 'title_zh': 'MMCR: 促进多模态多轮语境推理的视觉语言模型'}
{'arxiv_id': 'arXiv:2503.18817', 'title': 'Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations', 'authors': 'Jeonghyeon Kim, Sangheum Hwang', 'link': 'https://arxiv.org/abs/2503.18817', 'abstract': 'Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of naïve fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy.', 'abstract_zh': '多模态微调在分布外检测中的应用', 'title_zh': '多模态表示的跨模态对齐以增强OOD检测'}
{'arxiv_id': 'arXiv:2503.18595', 'title': 'Adaptive Unimodal Regulation for Balanced Multimodal Information Acquisition', 'authors': 'Chengxiang Huang, Yake Wei, Zequn Yang, Di Hu', 'link': 'https://arxiv.org/abs/2503.18595', 'abstract': 'Sensory training during the early ages is vital for human development. Inspired by this cognitive phenomenon, we observe that the early training stage is also important for the multimodal learning process, where dataset information is rapidly acquired. We refer to this stage as the prime learning window. However, based on our observation, this prime learning window in multimodal learning is often dominated by information-sufficient modalities, which in turn suppresses the information acquisition of information-insufficient modalities. To address this issue, we propose Information Acquisition Regulation (InfoReg), a method designed to balance information acquisition among modalities. Specifically, InfoReg slows down the information acquisition process of information-sufficient modalities during the prime learning window, which could promote information acquisition of information-insufficient modalities. This regulation enables a more balanced learning process and improves the overall performance of the multimodal network. Experiments show that InfoReg outperforms related multimodal imbalanced methods across various datasets, achieving superior model performance. The code is available at this https URL.', 'abstract_zh': '早年感觉训练对于人类发展至关重要。受此认知现象启发，我们观察到多模态学习过程中的早期训练阶段也同样重要，此时大量数据集信息迅速被获取。我们将这一阶段称为关键学习窗口。然而，我们的观察表明，在多模态学习的关键学习窗口中，信息充足的模态往往占据主导地位，进而抑制了信息不足模态的信息获取。为解决这一问题，我们提出了一种信息获取调节（InfoReg）方法，旨在平衡各模态之间信息的获取。具体而言，InfoReg在关键学习窗口中减缓了信息充足模态的信息获取过程，从而促进了信息不足模态的信息获取。这种调节机制使得学习过程更为平衡，提高了多模态网络的整体性能。实验结果显示，InfoReg在各类数据集上的表现优于相关不平衡多模态方法，取得了更优异的模型性能。代码可访问此链接。', 'title_zh': '自适应单模态调节以实现平衡的多模态信息获取'}
{'arxiv_id': 'arXiv:2503.18403', 'title': 'Knowledge Graph Enhanced Generative Multi-modal Models for Class-Incremental Learning', 'authors': 'Xusheng Cao, Haori Lu, Linlan Huang, Fei Yang, Xialei Liu, Ming-Ming Cheng', 'link': 'https://arxiv.org/abs/2503.18403', 'abstract': 'Continual learning in computer vision faces the critical challenge of catastrophic forgetting, where models struggle to retain prior knowledge while adapting to new tasks. Although recent studies have attempted to leverage the generalization capabilities of pre-trained models to mitigate overfitting on current tasks, models still tend to forget details of previously learned categories as tasks progress, leading to misclassification. To address these limitations, we introduce a novel Knowledge Graph Enhanced Generative Multi-modal model (KG-GMM) that builds an evolving knowledge graph throughout the learning process. Our approach utilizes relationships within the knowledge graph to augment the class labels and assigns different relations to similar categories to enhance model differentiation. During testing, we propose a Knowledge Graph Augmented Inference method that locates specific categories by analyzing relationships within the generated text, thereby reducing the loss of detailed information about old classes when learning new knowledge and alleviating forgetting. Experiments demonstrate that our method effectively leverages relational information to help the model correct mispredictions, achieving state-of-the-art results in both conventional CIL and few-shot CIL settings, confirming the efficacy of knowledge graphs at preserving knowledge in the continual learning scenarios.', 'abstract_zh': '计算机视觉中的持续学习面临着灾难性遗忘的关键挑战，即模型在适应新任务时难以保留先前的知识。尽管最近的研究试图利用预训练模型的泛化能力来减轻当前任务上的过拟合，但模型在任务进展过程中仍然倾向于忘记之前学习类别的细节，导致误分类。为了解决这些问题，我们提出了一种新型的知识图谱增强生成多模态模型（KG-GMM），在学习过程中构建不断演化的知识图谱。我们的方法利用知识图谱中的关系来增强类别标签，并为类似类分配不同的关系以增强模型的区分能力。在测试过程中，我们提出了一种知识图谱增强推断方法，通过分析生成文本中的关系来定位特定类别，从而在学习新知识时减少旧类别的详细信息丢失，并缓解遗忘。实验结果表明，我们的方法有效地利用了关系信息来帮助模型纠正误预测，在传统的持续学习（CIL）和少样本持续学习（few-shot CIL）设置中均取得了最先进的结果，证实了知识图谱在持续学习场景中保留知识的有效性。', 'title_zh': '知识图谱增强的生成多模态模型在类别增量学习中的应用'}
{'arxiv_id': 'arXiv:2503.18324', 'title': 'Plug-and-Play Interpretable Responsible Text-to-Image Generation via Dual-Space Multi-facet Concept Control', 'authors': 'Basim Azam, Naveed Akhtar', 'link': 'https://arxiv.org/abs/2503.18324', 'abstract': 'Ethical issues around text-to-image (T2I) models demand a comprehensive control over the generative content. Existing techniques addressing these issues for responsible T2I models aim for the generated content to be fair and safe (non-violent/explicit). However, these methods remain bounded to handling the facets of responsibility concepts individually, while also lacking in interpretability. Moreover, they often require alteration to the original model, which compromises the model performance. In this work, we propose a unique technique to enable responsible T2I generation by simultaneously accounting for an extensive range of concepts for fair and safe content generation in a scalable manner. The key idea is to distill the target T2I pipeline with an external plug-and-play mechanism that learns an interpretable composite responsible space for the desired concepts, conditioned on the target T2I pipeline. We use knowledge distillation and concept whitening to enable this. At inference, the learned space is utilized to modulate the generative content. A typical T2I pipeline presents two plug-in points for our approach, namely; the text embedding space and the diffusion model latent space. We develop modules for both points and show the effectiveness of our approach with a range of strong results.', 'abstract_zh': '文本到图像模型中的伦理问题要求对生成内容进行综合控制。现有针对负责任文本到图像模型的技术旨在使生成内容公正和安全（非暴力/无爆露）。然而，这些方法仍然局限于单独处理责任感概念的各个方面，并且缺乏可解释性。此外，它们通常需要对原始模型进行修改，这会损害模型性能。在本工作中，我们提出了一种独特的方法，通过同时考虑公正和安全内容生成的广泛概念范围来实现负责任的文本到图像生成。关键思想是通过一个外部即插即用机制对目标文本到图像流水线进行蒸馏，该机制学习一个针对目标文本到图像流水线的可解释综合责任感空间。我们使用知识蒸馏和概念去偏技术来实现这一点。在推理时，学习到的空间用来调节生成内容。一个典型的文本到图像流水线为我们方法提供了两个插件点，即文本嵌入空间和扩散模型潜在空间。我们为两个点开发了模块，并展示了我们方法的有效性，取得了多种强结果。', 'title_zh': '基于双空间多维度概念控制的即插即用可解释负责任文本生成'}
{'arxiv_id': 'arXiv:2503.18278', 'title': 'TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model', 'authors': 'Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, Bo Yuan', 'link': 'https://arxiv.org/abs/2503.18278', 'abstract': 'Vision-Language Models (VLMs) demand substantial computational resources during inference, largely due to the extensive visual input tokens for representing visual information. Previous studies have noted that visual tokens tend to receive less attention than text tokens, suggesting their lower importance during inference and potential for pruning. However, their methods encounter several challenges: reliance on greedy heuristic criteria for token importance and incompatibility with FlashAttention and KV cache. To address these issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken \\textbf{P}runing with inference Time Optimization for fast and low-memory \\textbf{V}LM, achieving efficient pruning without additional training or fine-tuning. Instead of relying on attention scores, we formulate token pruning as an optimization problem, accurately identifying important visual tokens while remaining compatible with FlashAttention. Additionally, since we only perform this pruning once during the prefilling stage, it effectively reduces KV cache size. Our optimization framework incorporates a visual-aware cost function considering factors such as Feature Similarity, Relative Spatial Distance, and Absolute Central Distance, to measure the importance of each source visual token, enabling effective pruning of low-importance tokens. Extensive experiments demonstrate that our method outperforms previous token pruning methods, validating the effectiveness and efficiency of our approach.', 'abstract_zh': 'TopV：基于推理时优化的兼容Token剪枝方法以实现高效的低内存VLM', 'title_zh': 'TopV：具推理时优化的兼容性_token剪枝方法以实现快速低内存多模态视觉语言模型'}
{'arxiv_id': 'arXiv:2503.18172', 'title': 'Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering', 'authors': 'Zixin Chen, Sicheng Song, Kashun Shum, Yanna Lin, Rui Sheng, Huamin Qu', 'link': 'https://arxiv.org/abs/2503.18172', 'abstract': "Misleading chart visualizations, which intentionally manipulate data representations to support specific claims, can distort perceptions and lead to incorrect conclusions. Despite decades of research, misleading visualizations remain a widespread and pressing issue. Recent advances in multimodal large language models (MLLMs) have demonstrated strong chart comprehension capabilities, yet no existing work has systematically evaluated their ability to detect and interpret misleading charts. This paper introduces the Misleading Chart Question Answering (Misleading ChartQA) Benchmark, a large-scale multimodal dataset designed to assess MLLMs in identifying and reasoning about misleading charts. It contains over 3,000 curated examples, covering 21 types of misleaders and 10 chart types. Each example includes standardized chart code, CSV data, and multiple-choice questions with labeled explanations, validated through multi-round MLLM checks and exhausted expert human review. We benchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitations in identifying visually deceptive practices. We also propose a novel pipeline that detects and localizes misleaders, enhancing MLLMs' accuracy in misleading chart interpretation. Our work establishes a foundation for advancing MLLM-driven misleading chart comprehension. We publicly release the sample dataset to support further research in this critical area.", 'abstract_zh': '误导性图表可视化：故意操纵数据表示以支持特定主张，可能导致认知扭曲并导致错误结论。尽管已有数十年的研究，误导性图表仍是广泛存在的紧迫问题。近期多模态大型语言模型（MLLMs）展示了强大的图表理解能力，但迄今为止尚无工作系统性地评估其检测和解释误导性图表的能力。本文介绍了误导性图表问答基准（Misleading ChartQA），这是一个大规模多模态数据集，用于评估MLLMs在识别和推理误导性图表方面的能力。该数据集包含超过3000个精心挑选的例子，覆盖了21种类型和10种图表类型。每个示例包括标准化的图表代码、CSV数据以及带有标注解释的多项选择题，这些解释经过多轮MLLM检查和全面的人类专家审查。我们对16个最先进的MLLMs进行了基准测试，揭示了它们在识别视觉欺骗性实践方面的局限性。我们还提出了一种新管道，可用于检测和定位误导性图表，从而提高MLLMs对误导性图表理解的准确性。我们的工作为MLLM驱动的误导性图表理解的发展奠定了基础。为了支持这一关键领域的进一步研究，我们公开发布了样本数据集。', 'title_zh': '揭示欺骗性视觉元素：基于误导性图表问答评估多模态大型语言模型'}
{'arxiv_id': 'arXiv:2503.17736', 'title': 'V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts for Better Human-Model Interaction', 'authors': 'Yiming Zhao, Yu Zeng, Yukun Qi, YaoYang Liu, Lin Chen, Zehui Chen, Xikun Bao, Jie Zhao, Feng Zhao', 'link': 'https://arxiv.org/abs/2503.17736', 'abstract': "Large Vision-Language Models (LVLMs) have made significant progress in the field of video understanding recently. However, current benchmarks uniformly lean on text prompts for evaluation, which often necessitate complex referential language and fail to provide precise spatial and temporal references. This limitation diminishes the experience and efficiency of human-model interaction. To address this limitation, we propose the Video Visual Prompt Benchmark(V2P-Bench), a comprehensive benchmark specifically designed to evaluate LVLMs' video understanding capabilities in multimodal human-model interaction scenarios. V2P-Bench includes 980 unique videos and 1,172 QA pairs, covering 5 main tasks and 12 dimensions, facilitating instance-level fine-grained understanding aligned with human cognition. Benchmarking results reveal that even the most powerful models perform poorly on V2P-Bench (65.4% for GPT-4o and 67.9% for Gemini-1.5-Pro), significantly lower than the human experts' 88.3%, highlighting the current shortcomings of LVLMs in understanding video visual prompts. We hope V2P-Bench will serve as a foundation for advancing multimodal human-model interaction and video understanding evaluation. Project page: this https URL.", 'abstract_zh': '大规模视觉语言模型（LVLMs）在视频理解领域取得了显著进展。然而，现有的基准测试统一依赖于文本提示进行评估，这往往需要复杂的参照语言，并未能提供精确的空间和时间参考。这一限制降低了人类与模型交互的体验和效率。为解决这一限制，我们提出了视频视觉提示基准（V2P-Bench），这是一个专门设计用于评估LVLMs在多模态人机交互场景中视频理解能力的综合基准。V2P-Bench 包含980个独特的视频和1,172个问答对，涵盖5个主要任务和12个维度，促进与人类认知一致的实例级细粒度理解。基准测试结果显示，即使是最强大的模型在V2P-Bench上的表现也较差（GPT-4o为65.4%，Gemini-1.5-Pro为67.9%），远低于人类专家的88.3%，突显了LVLMs在理解视频视觉提示方面当前的不足之处。我们希望V2P-Bench能够为促进多模态人机交互和视频理解评估提供一个基础。项目页面：此链接。', 'title_zh': 'V2P-Bench: 通过视觉提示评估视频-语言理解以改善人-模型交互'}
{'arxiv_id': 'arXiv:2503.17712', 'title': 'Multi-modality Anomaly Segmentation on the Road', 'authors': 'Heng Gao, Zhuolin He, Shoumeng Qiu, Xiangyang Xue, Jian Pu', 'link': 'https://arxiv.org/abs/2503.17712', 'abstract': 'Semantic segmentation allows autonomous driving cars to understand the surroundings of the vehicle comprehensively. However, it is also crucial for the model to detect obstacles that may jeopardize the safety of autonomous driving systems. Based on our experiments, we find that current uni-modal anomaly segmentation frameworks tend to produce high anomaly scores for non-anomalous regions in images. Motivated by this empirical finding, we develop a multi-modal uncertainty-based anomaly segmentation framework, named MMRAS+, for autonomous driving systems. MMRAS+ effectively reduces the high anomaly outputs of non-anomalous classes by introducing text-modal using the CLIP text encoder. Indeed, MMRAS+ is the first multi-modal anomaly segmentation solution for autonomous driving. Moreover, we develop an ensemble module to further boost the anomaly segmentation performance. Experiments on RoadAnomaly, SMIYC, and Fishyscapes validation datasets demonstrate the superior performance of our method. The code is available in this https URL.', 'abstract_zh': '语义分割使自动驾驶车辆能够全面理解车辆周围的环境。然而，模型检测可能危及自动驾驶系统安全的障碍物也同样至关重要。基于我们的实验，我们发现当前的单一模态异常分割框架倾向于为图像中的非异常区域生成高异常分数。受这一经验发现的启发，我们开发了一种基于多模态不确定性异常分割框架，命名为MMRAS+，适用于自动驾驶系统。MMRAS+通过引入基于CLIP文本编码器的文本模态，有效减少了非异常类别的高异常输出。实际上，MMRAS+是首款用于自动驾驶的多模态异常分割解决方案。此外，我们还开发了一个集成模块进一步提升异常分割性能。针对RoadAnomaly、SMIYC和Fishyscapes验证数据集的实验表明，本方法具有优越的表现。代码可在以下链接获取。', 'title_zh': '道路多模态异常分割'}
{'arxiv_id': 'arXiv:2503.17551', 'title': 'Audio-Enhanced Vision-Language Modeling with Latent Space Broadening for High Quality Data Expansion', 'authors': 'Yu Sun, Yin Li, Ruixiao Sun, Chunhui Liu, Fangming Zhou, Ze Jin, Linjie Wang, Xiang Shen, Zhuolin Hao, Hongyu Xiong', 'link': 'https://arxiv.org/abs/2503.17551', 'abstract': 'Transformer-based multimodal models are widely used in industrial-scale recommendation, search, and advertising systems for content understanding and relevance ranking. Enhancing labeled training data quality and cross-modal fusion significantly improves model performance, influencing key metrics such as quality view rates and ad revenue. High-quality annotations are crucial for advancing content modeling, yet traditional statistical-based active learning (AL) methods face limitations: they struggle to detect overconfident misclassifications and are less effective in distinguishing semantically similar items in deep neural networks. Additionally, audio information plays an increasing role, especially in short-video platforms, yet most pre-trained multimodal architectures primarily focus on text and images. While training from scratch across all three modalities is possible, it sacrifices the benefits of leveraging existing pre-trained visual-language (VL) and audio models. To address these challenges, we propose kNN-based Latent Space Broadening (LSB) to enhance AL efficiency and Vision-Language Modeling with Audio Enhancement (VLMAE), a mid-fusion approach integrating audio into VL models. This system deployed in production systems, leading to significant business gains.', 'abstract_zh': '基于Transformer的多模态模型在工业规模的推荐、搜索和广告系统中广泛用于内容理解和相关性排序。通过提高标注训练数据质量及跨模态融合显著提升模型性能，影响关键指标如高质量观看率和广告收入。高质量标注对于推进内容建模至关重要，但传统的基于统计的主动学习方法存在局限性：它们难以检测高置信度的误分类，并且在区分深度神经网络中的语义相似项时效果较差。此外，音频信息在短视频平台中扮演越来越重要的角色，但大部分预训练多模态架构主要关注文本和图像。尽管可以从所有三种模态从头开始训练是可能的，但这会牺牲使用现有预训练视觉-语言和音频模型带来的好处。为了解决这些挑战，我们提出了基于kNN的潜在空间拓展（LSB）以提高主动学习效率，并提出将音频增强引入视觉-语言模型中的Vision-Language Modeling with Audio Enhancement（VLMAE），这是一种中间融合方法。该系统在生产系统中部署，取得了显著的商业收益。', 'title_zh': '音频增强的视觉-语言建模：潜在空间拓宽以实现高质量数据扩展'}
{'arxiv_id': 'arXiv:2503.17417', 'title': 'Generative Modeling of Class Probability for Multi-Modal Representation Learning', 'authors': 'Jungkyoo Shin, Bumsoo Kim, Eunwoo Kim', 'link': 'https://arxiv.org/abs/2503.17417', 'abstract': 'Multi-modal understanding plays a crucial role in artificial intelligence by enabling models to jointly interpret inputs from different modalities. However, conventional approaches such as contrastive learning often struggle with modality discrepancies, leading to potential misalignments. In this paper, we propose a novel class anchor alignment approach that leverages class probability distributions for multi-modal representation learning. Our method, Class-anchor-ALigned generative Modeling (CALM), encodes class anchors as prompts to generate and align class probability distributions for each modality, enabling more effective alignment. Furthermore, we introduce a cross-modal probabilistic variational autoencoder to model uncertainty in the alignment, enhancing the ability to capture deeper relationships between modalities and data variations. Extensive experiments on four benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, especially in out-of-domain evaluations. This highlights its superior generalization capabilities in multi-modal representation learning.', 'abstract_zh': '多模态理解在人工智能中起着至关重要的作用，它使模型能够联合解释不同模态的输入。然而，传统的对比学习等方法往往在处理模态差异时遇到困难，导致潜在的对齐问题。本文提出了一种新颖的类别锚点对齐方法，利用类别概率分布进行多模态表示学习。我们的方法，类别锚点对齐生成模型（CALM），将类别锚点编码为提示，生成并对齐每个模态的类别概率分布，从而实现更有效的对齐。此外，我们引入了跨模态概率变分自动编码器来建模对齐中的不确定性，增强了捕捉模态之间更深层次关系以及数据变化的能力。在四个基准数据集上的广泛实验表明，我们的方法在优于现有最佳方法的同时，在域外评估中表现尤为突出，这突显了其在多模态表示学习中的优越泛化能力。', 'title_zh': '多模态表示学习中的类别概率生成建模'}
{'arxiv_id': 'arXiv:2503.17408', 'title': 'Leveraging OpenFlamingo for Multimodal Embedding Analysis of C2C Car Parts Data', 'authors': 'Maisha Binte Rashid, Pablo Rivas', 'link': 'https://arxiv.org/abs/2503.17408', 'abstract': 'In this paper, we aim to investigate the capabilities of multimodal machine learning models, particularly the OpenFlamingo model, in processing a large-scale dataset of consumer-to-consumer (C2C) online posts related to car parts. We have collected data from two platforms, OfferUp and Craigslist, resulting in a dataset of over 1.2 million posts with their corresponding images. The OpenFlamingo model was used to extract embeddings for the text and image of each post. We used $k$-means clustering on the joint embeddings to identify underlying patterns and commonalities among the posts. We have found that most clusters contain a pattern, but some clusters showed no internal patterns. The results provide insight into the fact that OpenFlamingo can be used for finding patterns in large datasets but needs some modification in the architecture according to the dataset.', 'abstract_zh': '本文旨在探究多模态机器学习模型，特别是OpenFlamingo模型，处理与汽车零件相关的消费者对消费者（C2C）在线帖子的大规模数据集的能力。我们从OfferUp和 Craigslist两个平台收集数据，形成了包含超过120万条帖子及其对应图像的大规模数据集。使用OpenFlamingo模型提取每条帖子的文字和图像的嵌入向量。通过联合嵌入向量进行$k$-means聚类，识别帖子中的潜在模式和共同点。研究发现大多数聚类包含模式，但有些聚类没有内部模式。结果表明OpenFlamingo可以用于在大数据集中发现模式，但需要根据数据集对模型架构进行一些修改。', 'title_zh': '利用OpenFlamingo进行C2C汽车零部件多模态嵌入分析'}
