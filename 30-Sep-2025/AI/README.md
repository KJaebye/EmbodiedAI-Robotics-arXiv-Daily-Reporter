# Who's Your Judge? On the Detectability of LLM-Generated Judgments 

**Title (ZH)**: 谁是你的法官？LLM生成的判断可检测性探究 

**Authors**: Dawei Li, Zhen Tan, Chengshuai Zhao, Bohan Jiang, Baixiang Huang, Pingchuan Ma, Abdullah Alnaibari, Kai Shu, Huan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.25154)  

**Abstract**: Large Language Model (LLM)-based judgments leverage powerful LLMs to efficiently evaluate candidate content and provide judgment scores. However, the inherent biases and vulnerabilities of LLM-generated judgments raise concerns, underscoring the urgent need for distinguishing them in sensitive scenarios like academic peer reviewing. In this work, we propose and formalize the task of judgment detection and systematically investigate the detectability of LLM-generated judgments. Unlike LLM-generated text detection, judgment detection relies solely on judgment scores and candidates, reflecting real-world scenarios where textual feedback is often unavailable in the detection process. Our preliminary analysis shows that existing LLM-generated text detection methods perform poorly given their incapability to capture the interaction between judgment scores and candidate content -- an aspect crucial for effective judgment detection. Inspired by this, we introduce \textit{J-Detector}, a lightweight and transparent neural detector augmented with explicitly extracted linguistic and LLM-enhanced features to link LLM judges' biases with candidates' properties for accurate detection. Experiments across diverse datasets demonstrate the effectiveness of \textit{J-Detector} and show how its interpretability enables quantifying biases in LLM judges. Finally, we analyze key factors affecting the detectability of LLM-generated judgments and validate the practical utility of judgment detection in real-world scenarios. 

**Abstract (ZH)**: 基于大规模语言模型（LLM）的判决检测：利用强大的LLM高效评估候选内容并提供判决分数，但LLM生成判决固有的偏见和脆弱性引发了担忧，尤其是在学术同行评审等敏感场景中迫切需要将它们区分开来。本文提出并正式化了判决检测任务，并系统地研究了LLM生成判决的可检测性。不同于LLM生成文本检测，判决检测仅依赖于判决分数和候选文本，反映了现实世界中识别过程中文本反馈常不可用的场景。初步分析显示，现有LLM生成文本检测方法表现不佳，因为它们无法捕捉判决分数与候选文本之间的交互作用——这是有效判决检测的关键方面。受此启发，我们引入了J-Detector，这是一种轻量级、透明的神经检测器，结合了明确提取的语言和LLM增强特征，将LLM评判者的偏见与候选者的属性联系起来，以实现准确的检测。跨多种数据集的实验表明了J-Detector的有效性，并展示了其可解释性如何量化LLM评判者的偏见。最后，我们分析了影响LLM生成判决可检测性的关键因素，并验证了判决检测在现实世界场景中的实用价值。 

---
# UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following 

**Title (ZH)**: UniAPL: 一种统一的对抗性偏好学习框架用于指令跟随 

**Authors**: FaQiang Qian, WeiKun Zhang, Ziliang Wang, Kang An, Xuhui Zheng, Liangjian Wen, Mengya Gao, Yong Dai, Yichao Wu  

**Link**: [PDF](https://arxiv.org/pdf/2509.25148)  

**Abstract**: Shaping powerful LLMs to be beneficial and safe is central to AI alignment. We argue that post-training alignment is fundamentally a unified Preference Learning problem, involving two modalities: demonstrated preferences (e.g., Supervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement Learning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due to a critical distributional mismatch: SFT uses static expert data, but as the policy evolves, its generation distribution drifts, making SFT knowledge brittle. Subsequent RL then explores without direct access to the rich, ground-truth knowledge in expert demonstrations, leading to inefficient, ungrounded updates. This separation prevents mutual regularization between data sources. To address this, we reframe alignment as a constrained optimization problem and propose Unified Adversarial Preference Learning (UniAPL),a novel framework that dynamically aligns the policy's distribution with the expert's. UniAPL implements a single-stage unified training objective, jointly learning from mixed batches of SFT and preference data. In every gradient step, dense expert demonstrations directly ground and regularize online exploration, inherently resolving distributional mismatch and maximizing data this http URL evaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507 as the teacher. Our models match or exceed strong GRPO baselines: +5.77% on Qwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the teacher. Analyses of response length and log-probability distributions confirm that UniAPL outputs closely mimic expert demonstrations, achieving both stronger performance and better behavioral alignment. 

**Abstract (ZH)**: 塑造强大的LLM以使其有益和安全是AI对齐的核心。我们argue认为，后训练对齐本质上是一个统一的偏好学习问题，涉及两种模态：已展示的偏好（例如，监督微调SFT）和比较偏好（例如，强化学习RL）。标准的顺序管线——先进行SFT后进行RL——由于关键的分布性不匹配而存在问题：SFT使用静态专家数据，但随着策略的演化，其生成分布会发生偏移，使SFT知识变得脆弱。随后的RL则在没有直接访问丰富的地面真实知识的情况下探索，导致了低效且缺乏根基的更新。这种分离阻碍了数据源之间的相互正则化。为了解决这个问题，我们将对齐重新构想为一个约束优化问题，并提出了一种新型框架——统一 adversarial 偏好学习（UniAPL），该框架动态地使策略分布与专家分布对齐。UniAPL 实现了一站式统一训练目标，联合学习混合批次的SFT和偏好数据。在每一步梯度更新中，密集的专家示范直接为在线探索提供根基和正则化，从根本上解决了分布性不匹配问题并最大化数据利用。我们使用Qwen3-235B-Instruct-2507作为教师对UniAPL进行指令跟随任务的评估。我们的模型匹配或超过了强大的GRPO基线：在Qwen3-0.6B上提高了5.77%（达到一个32B模型的表现），在Qwen3-4B上提高了3.75%，甚至超过了教师。对回复长度和对数概率分布的分析证实，UniAPL 的输出密切模仿了专家示范，实现了更强的性能和更好的行为对齐。 

---
# Visual serial processing deficits explain divergences in human and VLM reasoning 

**Title (ZH)**: 视觉序列加工缺陷解释人类与VLM推理的差异 

**Authors**: Nicholas Budny, Kia Ghods, Declan Campbell, Raja Marjieh, Amogh Joshi, Sreejan Kumar, Jonathan D. Cohen, Taylor W. Webb, Thomas L. Griffiths  

**Link**: [PDF](https://arxiv.org/pdf/2509.25142)  

**Abstract**: Why do Vision Language Models (VLMs), despite success on standard benchmarks, often fail to match human performance on surprisingly simple visual reasoning tasks? While the underlying computational principles are still debated, we hypothesize that a crucial factor is a deficit in visually-grounded serial processing. To test this hypothesis, we compared human and VLM performance across tasks designed to vary serial processing demands in three distinct domains: geometric reasoning, perceptual enumeration, and mental rotation. Tasks within each domain varied serial processing load by manipulating factors such as geometric concept complexity, perceptual individuation load, and transformation difficulty. Across all domains, our results revealed a consistent pattern: decreased VLM accuracy was strongly correlated with increased human reaction time (used as a proxy for serial processing load). As tasks require more demanding serial processing -- whether composing concepts, enumerating items, or performing mental transformations -- the VLM-human performance gap widens reliably. These findings support our hypothesis, indicating that limitations in serial, visually grounded reasoning represent a fundamental bottleneck that distinguishes current VLMs from humans. 

**Abstract (ZH)**: 为什么视觉语言模型（VLMs）尽管在标准基准测试中取得成功，但在一些出人意料的简单视觉推理任务中往往无法匹配人类的表现？虽然核心计算原理仍然存在争议，但我们假设一个关键因素是在视觉支撑的序贯处理方面存在缺陷。为了检验这一假设，我们在三个不同的领域（几何推理、知觉计数和心理旋转）设计的任务中比较了人类和VLM的表现，这些任务旨在改变序贯处理的要求。在每个领域内，通过操控诸如几何概念复杂性、知觉个体化负担和变换难度等因素来改变序贯处理负载。在所有领域中，我们的结果显现出了一个一致的模式：VLM准确率的下降强烈相关于人类反应时间的增加（作为序贯处理负载的代理）。随着任务对序贯处理的要求变得更为苛刻——无论是组成概念、计数物品还是执行心理变换——VLM与人类的表现差距会可靠地扩大。这些发现支持了我们的假设，表明在序列、视觉支撑的推理方面的局限性构成了当前VLMs与人类之间的一个基本瓶颈。 

---
# ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory 

**Title (ZH)**: 推理银行：通过推理记忆扩展代理自进化的规模 

**Authors**: Siru Ouyang, Jun Yan, I-Hung Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T. Le, Samira Daruki, Xiangru Tang, Vishy Tirumalashetty, George Lee, Mahsan Rofouei, Hangfei Lin, Jiawei Han, Chen-Yu Lee, Tomas Pfister  

**Link**: [PDF](https://arxiv.org/pdf/2509.25140)  

**Abstract**: With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agent's interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise. 

**Abstract (ZH)**: 基于推理的内存框架和记忆感知的测试时扩增（ReasoningBank and Memory-Aware Test-Time Scaling） 

---
# Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs 

**Title (ZH)**: 基于类比文本描述的大规模语言模型的视觉-语言导航 

**Authors**: Yue Zhang, Tianyi Ma, Zun Wang, Yanyuan Qiao, Parisa Kordjamshidi  

**Link**: [PDF](https://arxiv.org/pdf/2509.25139)  

**Abstract**: Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agent's contextual understanding by incorporating textual descriptions from multiple perspectives that facilitate analogical reasoning across images. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance. 

**Abstract (ZH)**: 将大型语言模型（LLM）集成到具身AI模型中正变得越来越普遍。然而，现有的零样本基于语言和视觉导航（VLN）的LLM代理要么将图像编码为文本场景描述，有可能简化视觉细节，要么处理原始图像输入，这可能无法捕捉到高层推理所需的抽象语义。在本文中，我们通过引入多角度的文本描述来提高导航代理的情境理解，这些文本描述有助于促进图像间的类比推理。利用基于文本的类比推理，该代理增强其全局场景理解和空间推理能力，从而做出更准确的动作决策。我们在R2R数据集上评估了我们的方法，实验结果表明在导航性能方面取得了显著的改进。 

---
# The Era of Real-World Human Interaction: RL from User Conversations 

**Title (ZH)**: 现实世界人类交互的时代：来自用户对话的强化学习 

**Authors**: Chuanyang Jin, Jing Xu, Bo Liu, Leitian Tao, Olga Golovneva, Tianmin Shu, Wenting Zhao, Xian Li, Jason Weston  

**Link**: [PDF](https://arxiv.org/pdf/2509.25137)  

**Abstract**: We posit that to achieve continual model improvement and multifaceted alignment, future models must learn from natural human interaction. Current conversational models are aligned using pre-annotated, expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human Interaction (RLHI), a paradigm that learns directly from in-the-wild user conversations. We develop two complementary methods: (1) RLHI with User-Guided Rewrites, which revises unsatisfactory model outputs based on users' natural-language follow-up responses, (2) RLHI with User-Based Rewards, which learns via a reward model conditioned on knowledge of the user's long-term interaction history (termed persona). Together, these methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on conversations derived from WildChat, both RLHI variants outperform strong baselines in personalization and instruction-following, and similar feedback enhances performance on reasoning benchmarks. These results suggest organic human interaction offers scalable, effective supervision for personalized alignment. 

**Abstract (ZH)**: 我们提出，为了实现持续模型改进和多维度对齐，未来的模型必须从自然的人类交互中学习。当前的对话模型是通过预标注的专家生成的人工反馈进行对齐的。在这项工作中，我们引入了基于人类交互的强化学习（RLHI）框架，该框架直接学习来自真实用户对话的数据。我们开发了两种互补的方法：（1）基于用户指导修正的RLHI，该方法根据用户的自然语言后续响应修订不满意的数据模型输出；（2）基于用户奖励的RLHI，该方法通过一个根据用户长期交互历史（即人设）条件化的奖励模型进行学习。这两种方法通过人设条件化的偏好优化将长期用户人设与回合级别的偏好链接起来。在来源于WildChat的对话数据上训练这两种RLHI变体，它们在个性化和指令遵循方面均优于强基线，并且类似的反馈提高了推理基准上的性能。这些结果表明，有机的人类交互为个性化对齐提供了可扩展且有效的监督。 

---
# From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones 

**Title (ZH)**: 从$f(x)$和$g(x)$到$f(g(x))$: 大型语言模型通过组合旧技能学习新技能（基于RL） 

**Authors**: Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, Hao Peng  

**Link**: [PDF](https://arxiv.org/pdf/2509.25123)  

**Abstract**: Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems. 

**Abstract (ZH)**: RL能否教会大语言模型真正的新技能，还是仅激活现有技能？这一问题处于关于RL在大语言模型后训练中角色的持续争议的核心。一方认为，即使没有预先的监督微调，也可以通过RL取得强大的实证结果；另一方则批评认为，RL在其贡献上仅限于重新加权现有的推理策略。本研究提供了实证证据，表明大语言模型在RL过程中可以通过组合现有技能来获得真正的新技能，这反映了人类获得新认知技能的一种核心机制。为了减轻数据污染和其他混杂因素，并允许对任务复杂性进行精确控制，我们发展了一种合成框架进行研究。具体来说，我们将技能定义为根据给定的字符串x推断出字符串变换函数f(x)输出的能力。当LLM在RL之前已经学习了f和g时，我们的实验揭示了RL使其能够学习未见过的它们的组合h(x)=g(f(x))。此外，这种组合能力可以扩展到在RL训练期间未曾见过的复杂问题，如多个函数的组合。令人惊讶的是，我们的实验表明，在源任务上获得的组合技能可以转移到不同的目标任务上，即使没有在目标任务上进行组合训练，仅需目标任务的原子技能的先验知识。我们的定性分析表明，RL从根本上改变了模型的推理行为。相比之下，使用相同数据的下一个标记训练则未能得出这些发现。系统性的实验为大语言模型的学习提供了新的见解，表明首先构建具有基本技能的基础模型，然后使用RL激励复杂问题中的高级、可泛化的技能的价值。 

---
# HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis 

**Title (ZH)**: HeDA：一种通过自动化知识图构建和多层风险传播分析的热波风险发现智能代理系统 

**Authors**: Yiquan Wang, Tin-Yeh Huang, Qingyun Gao, Jialin Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.25112)  

**Abstract**: Heatwaves pose complex cascading risks across interconnected climate, social, and economic systems, but knowledge fragmentation in scientific literature hinders comprehensive understanding of these risk pathways. We introduce HeDA (Heatwave Discovery Agent), an intelligent multi-agent system designed for automated scientific discovery through knowledge graph construction and multi-layer risk propagation analysis. HeDA processes over 10,247 academic papers to construct a comprehensive knowledge graph with 23,156 nodes and 89,472 relationships, employing novel multi-layer risk propagation analysis to systematically identify overlooked risk transmission pathways. Our system achieves 78.9% accuracy on complex question-answering tasks, outperforming state-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA successfully discovered five previously unidentified high-impact risk chains, such as the pathway where a heatwave leads to a water demand surge, resulting in industrial water restrictions and ultimately causing small business disruption, which were validated through historical case studies and domain expert review. This work presents a new paradigm for AI-driven scientific discovery, providing actionable insights for developing more resilient climate adaptation strategies. 

**Abstract (ZH)**: 热浪在互联的气候、社会和经济系统中构成复杂连锁风险，但科学文献中的知识碎片化阻碍了对这些风险路径的全面理解。我们介绍了HeDA（热浪发现代理），这是一种智能多agent系统，设计用于通过知识图谱构建和多层次风险传播分析进行自动科学研究。HeDA 处理了超过10,247篇学术论文，构建了一个包含23,156个节点和89,472个关系的全面知识图谱，并采用新颖的多层次风险传播分析系统地识别出被忽视的风险传输路径。我们的系统在复杂问答任务中的准确率达到78.9%，在包括GPT-4在内的最新基线方法中表现最佳，超过了13.7%。更重要的是，HeDA 成功发现了五个先前未被识别的高影响风险链路，例如热浪导致水资源需求激增，进而引发工业用水限制，最终导致小型企业中断等链条，这些发现通过历史案例研究和领域专家评审得到了验证。这项工作确立了AI驱动科学研究的新范式，为制定更具弹性的气候适应策略提供了可操作的见解。 

---
# Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning 

**Title (ZH)**: Cogito, Ergo Ludo：通过推理和规划学习玩游戏的智能体 

**Authors**: Sai Wang, Yu Wu, Zhongwen Xu  

**Link**: [PDF](https://arxiv.org/pdf/2509.25052)  

**Abstract**: The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose a different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), a novel agent architecture that leverages a Large Language Model (LLM) to build an explicit, language-based understanding of its environment's mechanics and its own strategy. Starting from a tabula rasa state with no prior knowledge (except action set), CEL operates on a cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environment's dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates a path toward more general and interpretable agents that not only act effectively but also build a transparent and improving model of their world through explicit reasoning on raw experience. 

**Abstract (ZH)**: Cogito, ergo ludo:一种通过推理和规划学习的新型代理架构 

---
# Scaling Synthetic Task Generation for Agents via Exploration 

**Title (ZH)**: 通过探索扩展合成任务生成 Agents 

**Authors**: Ram Ramrakhya, Andrew Szot, Omar Attia, Yuhao Yang, Anh Nguyen, Bogdan Mazoure, Zhe Gan, Harsh Agrawal, Alexander Toshev  

**Link**: [PDF](https://arxiv.org/pdf/2509.25047)  

**Abstract**: Post-Training Multimodal Large Language Models (MLLMs) to build interactive agents holds promise across domains such as computer-use, web navigation, and robotics. A key challenge in scaling such post-training is lack of high-quality downstream agentic task datasets with tasks that are diverse, feasible, and verifiable. Existing approaches for task generation rely heavily on human annotation or prompting MLLM with limited downstream environment information, which is either costly or poorly scalable as it yield tasks with limited coverage. To remedy this, we present AutoPlay, a scalable pipeline for task generation that explicitly explores interactive environments to discover possible interactions and current state information to synthesize environment-grounded tasks. AutoPlay operates in two stages: (i) an exploration phase, where an MLLM explorer agent systematically uncovers novel environment states and functionalities, and (ii) a task generation phase, where a task generator leverages exploration trajectories and a set of task guideline prompts as context to synthesize diverse, executable, and verifiable tasks. We show AutoPlay generates 20k tasks across 20 Android applications and 10k tasks across 13 applications Ubuntu applications to train mobile-use and computer-use agents. AutoPlay generated tasks enable large-scale task demonstration synthesis without human annotation by employing an MLLM task executor and verifier. This data enables training MLLM-based UI agents that improve success rates up to $20.0\%$ on mobile-use and $10.9\%$ on computer-use scenarios. In addition, AutoPlay generated tasks combined with MLLM verifier-based rewards enable scaling reinforcement learning training of UI agents, leading to an additional $5.7\%$ gain. coverage. These results establish AutoPlay as a scalable approach for post-training capable MLLM agents reducing reliance on human annotation. 

**Abstract (ZH)**: Post-Training 多模态大型语言模型 (MLLMs) 构建交互代理在计算机使用、网页导航和机器人等领域具有潜力。大规模后训练的关键挑战是在缺乏高质量的下游代理任务数据集的情况下进行此类后训练，这些数据集包含多样化、可行且可验证的任务。现有的任务生成方法 heavily 依赖手工标注或用有限的下游环境信息提示 MLLM，这种方法要么成本高，要么扩展性差，导致生成的任务覆盖面有限。为了解决这一问题，我们提出了 AutoPlay，这是一种可扩展的任务生成管道，通过明确探索交互环境来发现可能的交互和当前状态信息，以合成环境相关的任务。AutoPlay 分为两个阶段：(i) 探索阶段，在此阶段，MLLM 探索者代理系统地发现新的环境状态和功能，(ii) 任务生成阶段，在此阶段，任务生成器利用探索轨迹和一系列任务指南提示作为上下文，合成多样化、可执行且可验证的任务。我们展示了 AutoPlay 在 20 个 Android 应用程序中生成了 20,000 个任务，在 13 个 Ubuntu 应用程序中生成了 10,000 个任务，用于训练移动使用和计算机使用代理。AutoPlay 生成的任务通过使用 MLLM 任务执行器和验证器，无需人工标注即可实现大规模任务演示合成。这些数据使基于 MLLM 的 UI 代理能够在移动使用场景中提高成功率多达 20.0%，在计算机使用场景中提高成功率多达 10.9%。此外，结合 AutoPlay 生成的任务和基于 MLLM 验证器的奖励使强化学习训练 UI 代理规模化，导致额外 5.7% 的增益。这些结果确立了 AutoPlay 作为一种减少对人工标注依赖的后训练可扩展方法。 

---
# CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning 

**Title (ZH)**: CLPO： Curriculum Learning 结合策略优化应用于大语言模型推理 

**Authors**: Shijie Zhang, Guohao Sun, Kevin Zhang, Xiang Guo, Rujun Guo  

**Link**: [PDF](https://arxiv.org/pdf/2509.25004)  

**Abstract**: Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing methods typically treat all training samples uniformly, overlooking the vast differences in problem difficulty relative to the model's current capabilities. This uniform training strategy leads to inefficient exploration of problems the model has already mastered, while concurrently lacking effective guidance on problems that are challenging its abilities the most, limiting both learning efficiency and upper-bound performance. To address this, we propose CLPO (Curriculum-guided Learning for Policy Optimization), a novel algorithm that creates a dynamic pedagogical feedback loop within the policy optimization process. The core of CLPO leverages the model's own rollout performance to conduct real-time difficulty assessment, thereby constructing an Online Curriculum. This curriculum then guides an Adaptive Problem Restructuring mechanism, where the model acts as its own teacher: it diversifies medium-difficulty problems to promote generalization and simplifies challenging problems to make them more attainable. Our approach transforms the static training procedure into a dynamic process that co-evolves with the model's capabilities. Experiments show that CLPO achieves state-of-the-art performance across eight challenging mathematical and general reasoning benchmarks, with an average pass@1 improvement of 6.96% over other methods, demonstrating its potential for more efficiently training more capable reasoning models. 

**Abstract (ZH)**: 最近，带有可验证奖励的在线强化学习（RLVR）已成为增强大规模语言模型（LLMs）推理能力的关键范式。然而，现有方法通常将所有训练样本均匀对待，忽视了相对于模型当前能力的问题难度的巨大差异。这种均匀训练策略导致模型已经掌握的问题探索效率低下，同时对最挑战模型能力的问题也无法提供有效的指导，限制了学习效率和上界性能。为此，我们提出了一种新的算法CLPO（Curriculum-guided Learning for Policy Optimization），该算法在策略优化过程中创建了一个动态的教学反馈循环。CLPO的核心利用模型自身的展开性能进行实时难度评估，从而构建在线课程。该课程指导一种自适应问题重结构机制，其中模型充当自己的导师：通过增加中等难度问题促进泛化，简化难题使其更具可行性。我们的方法将静态训练过程转换为与模型能力共同演化的动态过程。实验表明，CLPO在八个具有挑战性的数学和通用推理基准测试中实现了最先进的性能，平均pass@1提高了6.96%，证明了其对更高效训练更强大推理模型的潜力。 

---
# Agentic Exploration of Physics Models 

**Title (ZH)**: 物理模型的主体性探索 

**Authors**: Maximilian Nägele, Florian Marquardt  

**Link**: [PDF](https://arxiv.org/pdf/2509.24978)  

**Abstract**: The process of scientific discovery relies on an interplay of observations, analysis, and hypothesis generation. Machine learning is increasingly being adopted to address individual aspects of this process. However, it remains an open challenge to fully automate the open-ended, heuristic, iterative loop required to discover the laws of an unknown system by exploring it through experiments and analysis, without tailoring the approach to the specifics of a given task. Here, we introduce SciExplorer, an agent that leverages large language model tool-use capabilities to enable free-form exploration of systems without any domain-specific blueprints, and apply it to the exploration of physical systems that are initially unknown to the agent. We test SciExplorer on a broad set of models spanning mechanical dynamical systems, wave evolution, and quantum many-body physics. Despite using a minimal set of tools, primarily based on code execution, we observe impressive performance on tasks such as recovering equations of motion from observed dynamics and inferring Hamiltonians from expectation values. The demonstrated effectiveness of this setup opens the door towards similar scientific exploration in other domains, without the need for finetuning or task-specific instructions. 

**Abstract (ZH)**: 科学发现的过程依赖于观察、分析和假设生成的相互作用。机器学习在处理这一过程的各个方面时正变得越来越重要。然而，完全自动化探索未知系统并发现其规律的开放式、启发式、迭代循环仍然是一个开放性的挑战，这种探索需要通过实验和分析来完成，而无需针对特定任务进行定制。在这里，我们引入了SciExplorer，这是一种利用大型语言模型工具使用能力的代理，能够无需任何特定领域的蓝图来自由探索系统，并将其应用于代理初始未知的物理系统探索。我们对涵盖机械动力系统、波演化和量子多体物理的一系列模型进行了测试。尽管仅使用了一小套基于代码执行的工具，我们在从观察到的动力恢复运动方程和从期望值推断哈密顿量等方面观察到了令人印象深刻的表现。这一示范性的有效性为其他领域的类似科学探索打开了大门，无需微调或特定任务的指令。 

---
# KIRETT - A wearable device to support rescue operations using artificial intelligence to improve first aid 

**Title (ZH)**: KIRETT - 一种使用人工智能支持救援行动的可穿戴设备以改进急救 

**Authors**: Johannes Zenkert, Christian Weber, Mubaris Nadeem, Lisa Bender, Madjid Fathi, Abu Shad Ahammed, Aniebiet Micheal Ezekiel, Roman Obermaisser, Maximilian Bradford  

**Link**: [PDF](https://arxiv.org/pdf/2509.24934)  

**Abstract**: This short paper presents first steps in the scientific part of the KIRETT project, which aims to improve first aid during rescue operations using a wearable device. The wearable is used for computer-aided situation recognition by means of artificial intelligence. It provides contextual recommendations for actions and operations to rescue personnel and is intended to minimize damage to patients due to incorrect treatment, as well as increase the probability of survival. The paper describes a first overview of research approaches within the project. 

**Abstract (ZH)**: 这篇简短的文章介绍了KIRETT项目科学部分的初步成果，该项目旨在通过穿戴设备改善救援操作中的急救措施。穿戴设备利用人工智能进行计算机辅助情况识别，并提供上下文相关的行动和操作建议，旨在因不正确的治疗减少患者损伤，提高生存概率。文章描述了项目内的初步研究方法 overview。 

---
# When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We? 

**Title (ZH)**: 当自动驾驶车辆遇到V2X协同感知：我们还有多远？ 

**Authors**: An Guo, Shuoxiao Zhang, Enyi Tang, Xinyu Gao, Haomin Pang, Haoxiang Tian, Yanzhou Mu, Wu Wen, Chunrong Fang, Zhenyu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24927)  

**Abstract**: With the tremendous advancement of deep learning and communication technology, Vehicle-to-Everything (V2X) cooperative perception has the potential to address limitations in sensing distant objects and occlusion for a single-agent perception system. V2X cooperative perception systems are software systems characterized by diverse sensor types and cooperative agents, varying fusion schemes, and operation under different communication conditions. Therefore, their complex composition gives rise to numerous operational challenges. Furthermore, when cooperative perception systems produce erroneous predictions, the types of errors and their underlying causes remain insufficiently explored. To bridge this gap, we take an initial step by conducting an empirical study of V2X cooperative perception. To systematically evaluate the impact of cooperative perception on the ego vehicle's perception performance, we identify and analyze six prevalent error patterns in cooperative perception systems. We further conduct a systematic evaluation of the critical components of these systems through our large-scale study and identify the following key findings: (1) The LiDAR-based cooperation configuration exhibits the highest perception performance; (2) Vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication exhibit distinct cooperative perception performance under different fusion schemes; (3) Increased cooperative perception errors may result in a higher frequency of driving violations; (4) Cooperative perception systems are not robust against communication interference when running online. Our results reveal potential risks and vulnerabilities in critical components of cooperative perception systems. We hope that our findings can better promote the design and repair of cooperative perception systems. 

**Abstract (ZH)**: 随着深度学习和通信技术的飞速发展， Vehicle-to-Everything (V2X) 合作感知具有解决单Agent感知系统在探测远距离物体和遮挡方面局限性的潜力。V2X 合作感知系统是由多种传感器类型和合作代理、不同的融合方案以及在不同通信条件下运作的软件系统特征化。因此，其复杂的组成产生了众多的操作挑战。此外，当合作感知系统产生错误预测时，错误类型及其根本原因仍缺乏充分探索。为弥补这一空白，我们通过 empirical 研究初步探索 V2X 合作感知。为了系统评估合作感知对自主车辆感知性能的影响，我们识别并分析了合作感知系统中六种常见的错误模式。进一步通过大规模研究系统评估这些系统的关键组件，并得出以下关键发现：(1) 基于LiDAR的合作配置表现出最高的感知性能；(2) 车辆到基础设施（V2I）和车辆到车辆（V2V）通信在不同的融合方案下表现出不同的合作感知性能；(3) 合作感知错误的增加可能导致驾驶违规频率的提高；(4) 在线运行时，合作感知系统对通信干扰不够 robust。研究结果揭示了合作感知系统关键组件中的潜在风险和脆弱性。我们希望我们的发现能更好地促进合作感知系统的规划设计和修复。 

---
# MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning 

**Title (ZH)**: MASLegalBench：基于演绎法律推理的多代理系统 benchmarks 

**Authors**: Huihao Jing, Wenbin Hu, Hongyu Luo, Jianhui Yang, Wei Fan, Haoran Li, Yangqiu Song  

**Link**: [PDF](https://arxiv.org/pdf/2509.24922)  

**Abstract**: Multi-agent systems (MAS), leveraging the remarkable capabilities of Large Language Models (LLMs), show great potential in addressing complex tasks. In this context, integrating MAS with legal tasks is a crucial step. While previous studies have developed legal benchmarks for LLM agents, none are specifically designed to consider the unique advantages of MAS, such as task decomposition, agent specialization, and flexible training. In fact, the lack of evaluation methods limits the potential of MAS in the legal domain. To address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS and designed with a deductive reasoning approach. Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations. Furthermore, we manually design various role-based MAS and conduct extensive experiments using different state-of-the-art LLMs. Our results highlight the strengths, limitations, and potential areas for improvement of existing models and MAS architectures. 

**Abstract (ZH)**: 基于大型语言模型的多Agent系统在法律任务中的应用：MASLegalBench基准设计 

---
# Meta-Learning Theory-Informed Inductive Biases using Deep Kernel Gaussian Processes 

**Title (ZH)**: 基于深度内核高斯过程的理论指导归纳偏置元学习 

**Authors**: Bahti Zakirov, Gašper Tkačik  

**Link**: [PDF](https://arxiv.org/pdf/2509.24919)  

**Abstract**: Normative and task-driven theories offer powerful top-down explanations for biological systems, yet the goals of quantitatively arbitrating between competing theories, and utilizing them as inductive biases to improve data-driven fits of real biological datasets are prohibitively laborious, and often impossible. To this end, we introduce a Bayesian meta-learning framework designed to automatically convert raw functional predictions from normative theories into tractable probabilistic models. We employ adaptive deep kernel Gaussian processes, meta-learning a kernel on synthetic data generated from a normative theory. This Theory-Informed Kernel specifies a probabilistic model representing the theory predictions -- usable for both fitting data and rigorously validating the theory. As a demonstration, we apply our framework to the early visual system, using efficient coding as our normative theory. We show improved response prediction accuracy in ex vivo recordings of mouse retinal ganglion cells stimulated by natural scenes compared to conventional data-driven baselines, while providing well-calibrated uncertainty estimates and interpretable representations. Using exact Bayesian model selection, we also show that our informed kernel can accurately infer the degree of theory-match from data, confirming faithful encapsulation of theory structure. This work provides a more general, scalable, and automated approach for integrating theoretical knowledge into data-driven scientific inquiry in neuroscience and beyond. 

**Abstract (ZH)**: 基于规范和任务驱动理论的贝叶斯元学习框架：自动将功能预测转化为可处理的概率模型，并用于数据驱动的生物学数据拟合和理论验证 

---
# Neural network embeddings recover value dimensions from psychometric survey items on par with human data 

**Title (ZH)**: 神经网络嵌入可以从心理测量调查项目中恢复价值维度，效果媲大人类数据。 

**Authors**: Max Pellert, Clemens M. Lechner, Indira Sen, Markus Strohmaier  

**Link**: [PDF](https://arxiv.org/pdf/2509.24906)  

**Abstract**: This study introduces "Survey and Questionnaire Item Embeddings Differentials" (SQuID), a novel methodological approach that enables neural network embeddings to effectively recover latent dimensions from psychometric survey items. We demonstrate that embeddings derived from large language models, when processed with SQuID, can recover the structure of human values obtained from human rater judgments on the Revised Portrait Value Questionnaire (PVQ-RR). Our experimental validation compares multiple embedding models across a number of evaluation metrics. Unlike previous approaches, SQuID successfully addresses the challenge of obtaining negative correlations between dimensions without requiring domain-specific fine-tuning. Quantitative analysis reveals that our embedding-based approach explains 55% of variance in dimension-dimension similarities compared to human data. Multidimensional scaling configurations from both types of data show fair factor congruence coefficients and largely follow the underlying theory. These results demonstrate that semantic embeddings can effectively replicate psychometric structures previously established through extensive human surveys. The approach offers substantial advantages in cost, scalability and flexibility while maintaining comparable quality to traditional methods. Our findings have significant implications for psychometrics and social science research, providing a complementary methodology that could expand the scope of human behavior and experience represented in measurement tools. 

**Abstract (ZH)**: "Survey和问卷项目嵌入差异性：SQuID方法及其在心理测量维度恢复中的应用" 

---
# RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark 

**Title (ZH)**: RealUnify: 统一模型真能从统一中受益吗？一个全面的基准测试 

**Authors**: Yang Shi, Yuhao Dong, Yue Ding, Yuran Wang, Xuanyu Zhu, Sheng Zhou, Wenting Liu, Haochen Tian, Rundong Wang, Huanqian Wang, Zuyan Liu, Bohan Zeng, Ruizhe Chen, Qixun Wang, Zhuoran Zhang, Xinlong Chen, Chengzhuo Tong, Bozhou Li, Chaoyou Fu, Qiang Liu, Haotian Wang, Wenjing Yang, Yuanxing Zhang, Pengfei Wan, Yi-Fan Zhang, Ziwei Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24897)  

**Abstract**: The integration of visual understanding and generation into unified multimodal models represents a significant stride toward general-purpose AI. However, a fundamental question remains unanswered by existing benchmarks: does this architectural unification actually enable synergetic interaction between the constituent capabilities? Existing evaluation paradigms, which primarily assess understanding and generation in isolation, are insufficient for determining whether a unified model can leverage its understanding to enhance its generation, or use generative simulation to facilitate deeper comprehension. To address this critical gap, we introduce RealUnify, a benchmark specifically designed to evaluate bidirectional capability synergy. RealUnify comprises 1,000 meticulously human-annotated instances spanning 10 categories and 32 subtasks. It is structured around two core axes: 1) Understanding Enhances Generation, which requires reasoning (e.g., commonsense, logic) to guide image generation, and 2) Generation Enhances Understanding, which necessitates mental simulation or reconstruction (e.g., of transformed or disordered visual inputs) to solve reasoning tasks. A key contribution is our dual-evaluation protocol, which combines direct end-to-end assessment with a diagnostic stepwise evaluation that decomposes tasks into distinct understanding and generation phases. This protocol allows us to precisely discern whether performance bottlenecks stem from deficiencies in core abilities or from a failure to integrate them. Through large-scale evaluations of 12 leading unified models and 6 specialized baselines, we find that current unified models still struggle to achieve effective synergy, indicating that architectural unification alone is insufficient. These results highlight the need for new training strategies and inductive biases to fully unlock the potential of unified modeling. 

**Abstract (ZH)**: 视觉理解与生成的整合融入统一多模态模型代表了通用人工智能的重要进展。然而，现有基准未能回答一个基本问题：这种架构整合是否真的能够促进各个组件能力之间的协同互动？现有的评估范式主要孤立地评估理解和生成，不足以判断统一模型是否能够利用其理解能力增强生成，或将生成模拟用于促进更深刻的理解。为填补这一关键缺口，我们引入了RealUnify基准，专门用于评估双向能力协同效应。RealUnify包含1,000个精心人工标注的实例，覆盖10个类别和32个子任务，并围绕两个核心轴构建：1）理解增强生成，要求通过推理（如常识、逻辑）指导图像生成；2）生成增强理解，需要通过心智模拟或重建（如处理变形或混乱的视觉输入）解决推理任务。我们的主要贡献是双评估协议，结合了直接端到端评估和诊断分解评估，后者将任务分解为独立的理解和生成阶段。该协议使我们能够精确判断性能瓶颈是源于核心能力的缺陷还是未能将它们整合。通过12个领先统一模型和6个专门基线的大规模评估，我们发现当前的统一模型仍然难以实现有效协同，表明架构整合本身是不够的。这些结果强调了需要新的训练策略和归纳偏置以完全释放统一建模的潜力。 

---
# The Emergence of Social Science of Large Language Models 

**Title (ZH)**: 大型语言模型的社会科学崛起 

**Authors**: Xiao Jia, Zhanzhan Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2509.24877)  

**Abstract**: The social science of large language models (LLMs) examines how these systems evoke mind attributions, interact with one another, and transform human activity and institutions. We conducted a systematic review of 270 studies, combining text embeddings, unsupervised clustering and topic modeling to build a computational taxonomy. Three domains emerge organically across the reviewed literature. LLM as Social Minds examines whether and when models display behaviors that elicit attributions of cognition, morality and bias, while addressing challenges such as test leakage and surface cues. LLM Societies examines multi-agent settings where interaction protocols, architectures and mechanism design shape coordination, norms, institutions and collective epistemic processes. LLM-Human Interactions examines how LLMs reshape tasks, learning, trust, work and governance, and how risks arise at the human-AI interface. This taxonomy provides a reproducible map of a fragmented field, clarifies evidentiary standards across levels of analysis, and highlights opportunities for cumulative progress in the social science of artificial intelligence. 

**Abstract (ZH)**: 大型语言模型的社会科学探讨这些系统引发心智归因、相互作用以及如何转变人类活动和制度。我们系统回顾了270项研究，结合文本嵌入、无监督聚类和主题建模来构建计算分类体系。三项研究领域在回顾的文献中自然地浮现。LLM作为社会心智考察模型在何时以及在何种情况下展示出引发认知、道德和偏见归因的行为，同时解决测试泄露和表面线索等挑战。LLM社会探讨多代理设置中，交互协议、架构和机制设计如何塑造协调、规范、制度和集体认识过程。LLM与人类的互动考察LLM如何重塑任务、学习、信任、工作和治理，以及在人类-人工智能接口处可能出现的风险。该分类体系提供了一个可再现的领域分布图，明确了不同分析层面的证据标准，并突显了人工智能社会科学研究中累积进步的机会。 

---
# PhysicsMinions: Winning Gold Medals in the Latest Physics Olympiads with a Coevolutionary Multimodal Multi-Agent System 

**Title (ZH)**: PhysicsMinions：在最新物理奥赛中使用共进化多模态多智能体系统夺金🏆 

**Authors**: Fangchen Yu, Junchi Yao, Ziyi Wang, Haiyuan Wan, Youling Huang, Bo Zhang, Shuyue Hu, Dongzhan Zhou, Ning Ding, Ganqu Cui, Lei Bai, Wanli Ouyang, Peng Ye  

**Link**: [PDF](https://arxiv.org/pdf/2509.24855)  

**Abstract**: Physics is central to understanding and shaping the real world, and the ability to solve physics problems is a key indicator of real-world physical intelligence. Physics Olympiads, renowned as the crown of competitive physics, provide a rigorous testbed requiring complex reasoning and deep multimodal understanding, yet they remain largely underexplored in AI research. Existing approaches are predominantly single-model based, and open-source MLLMs rarely reach gold-medal-level performance. To address this gap, we propose PhysicsMinions, a coevolutionary multi-agent system for Physics Olympiad. Its architecture features three synergistic studios: a Visual Studio to interpret diagrams, a Logic Studio to formulate solutions, and a Review Studio to perform dual-stage verification. The system coevolves through an iterative refinement loop where feedback from the Review Studio continuously guides the Logic Studio, enabling the system to self-correct and converge towards the ground truth. Evaluated on the HiPhO benchmark spanning 7 latest physics Olympiads, PhysicsMinions delivers three major breakthroughs: (i) Strong generalization: it consistently improves both open-source and closed-source models of different sizes, delivering clear benefits over their single-model baselines; (ii) Historic breakthroughs: it elevates open-source models from only 1-2 to 6 gold medals across 7 Olympiads, achieving the first-ever open-source gold medal in the latest International Physics Olympiad (IPhO) under the average-score metric; and (iii) Scaling to human expert: it further advances the open-source Pass@32 score to 26.8/30 points on the latest IPhO, ranking 4th of 406 contestants and far surpassing the top single-model score of 22.7 (ranked 22nd). Generally, PhysicsMinions offers a generalizable framework for Olympiad-level problem solving, with the potential to extend across disciplines. 

**Abstract (ZH)**: 物理学是理解并塑造现实世界的关键，解决物理学问题的能力是现实物理智能的重要指标。物理学奥林匹克竞赛被誉为竞赛物理学的皇冠，提供了一个需要复杂推理和深刻多模态理解的严格测试平台，但在人工智能研究中仍 largely 欠开发。现有方法主要基于单一模型，开源 MLLMs 很少达到金牌水平。为解决这一差距，我们提出了 PhysicsMinions，一种用于物理学奥林匹克竞赛的协同进化多智能体系统。其架构包括三个协同的工作室：可视化工作室以解析图表、逻辑工作室以制定解决方案、审核工作室以进行双重验证。该系统通过一个迭代细化循环协同进化，在此过程中，来自审核工作室的反馈不断引导逻辑工作室，使系统能够自我纠正并朝着真实答案收敛。在涵盖 7 场最新物理奥林匹克竞赛的 HiPhO 底线标准上，PhysicsMinions 实现了三项重大突破：（i）强大的泛化能力：它一致地提升了不同规模的开源和封闭源模型，对单一模型基线有明显优势；（ii）历史突破：它将开源模型从仅获得 1 到 2 金牌提升至在 7 场奥林匹克竞赛中获得 6 金牌，首次在平均分标准下实现了开源金牌，在最新的国际物理奥林匹克竞赛（IPhO）中取得突破；（iii）达到人类专家水平：进一步将开源 Pass@32 得分提升至最新 IPhO 的 26.8/30 分，在 406 名参赛者中排名第 4，并远超排名第一的单一模型得分 22.7（排名第 22）。总体而言，PhysicsMinions 提供了一个可泛化的框架，用于奥林匹克级别问题解决，具有跨学科扩展的潜力。 

---
# Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity 

**Title (ZH)**: 推动LLM在逻辑推理能力上的极限：数据推理强度的作用 

**Authors**: Zhen Bi, Zhenlin Hu, Jinnan Yang, Mingyang Chen, Cheng Deng, Yida Xue, Zeyu Yang, Qing Shen, Zhenfang Liu, Kang Zhao, Ningyu Zhang, Jungang Lou  

**Link**: [PDF](https://arxiv.org/pdf/2509.24836)  

**Abstract**: Recent advances in large language models (LLMs) highlight the importance of training data structure and quality in shaping reasoning behavior. However, most existing approaches focus on transforming data formats while neglecting the internal reasoning complexity of training samples, leaving the reasoning potential of data under-explored and underutilized. In this work, we posit that LLM logical reasoning performance is jointly constrained by the potential of the training data and the cognitive capacity of the model. To make this relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel metric that quantifies the latent logical reasoning complexity of samples by decomposing and aggregating their logical structures. This allows us to analyze how well current LLMs utilize logical reasoning signals and identify performance gaps relative to data potential. Based on this insight, we introduce a re-cognizing optimization strategy that systematically enhances the logical reasoning intensity of training this http URL than increasing data volume, our method re-optimizes existing samples to better align with the LLM's logical reasoning boundary. Extensive experiments show that our approach significantly improves performance and generalization over data-centric strategies. We further validate our method under a reinforcement learning framework. Our results indicate that prioritizing reasoning complexity in data rather than sheer scale or superficial form is essential to realizing LLMs' full cognitive potential. 

**Abstract (ZH)**: Recent advances in大型语言模型（LLMs）强调了训练数据结构和质量对于推理行为塑造的重要性。然而，目前大多数方法侧重于变换数据格式，而忽视了训练样本的内部推理复杂性，使得数据的推理潜力被低估和未充分利用。在本文中，我们提出LLM逻辑推理性能由训练数据的潜力和模型的认知能力共同制约。为了使这种关系可测量，我们引入了数据推理强度（DRI），这是一种新的度量方法，通过分解和综合样本的逻辑结构来量化其潜在的逻辑推理复杂性。这使我们能够分析当前LLM在利用逻辑推理信号方面的程度，并识别其与数据潜力之间的性能差距。基于这一洞见，我们提出了一种重新认知的优化策略，系统性地增强训练样本的逻辑推理强度，而非单纯增加数据量，我们的方法重新优化了现有样本，以更好地与LLM的逻辑推理边界对齐。大量实验表明，我们的方法在数据为中心的策略上显著提高了性能与泛化能力。我们的研究进一步在强化学习框架下验证了该方法的有效性。结果表明，重视数据中的推理复杂性而非单纯的规模或表面形式，对于实现LLM的全部认知潜力至关重要。 

---
# Query Circuits: Explaining How Language Models Answer User Prompts 

**Title (ZH)**: Query Circuits: 解释语言模型如何回答用户提示 

**Authors**: Tung-Yu Wu, Fazl Barez  

**Link**: [PDF](https://arxiv.org/pdf/2509.24808)  

**Abstract**: Explaining why a language model produces a particular output requires local, input-level explanations. Existing methods uncover global capability circuits (e.g., indirect object identification), but not why the model answers a specific input query in a particular way. We introduce query circuits, which directly trace the information flow inside a model that maps a specific input to the output. Unlike surrogate-based approaches (e.g., sparse autoencoders), query circuits are identified within the model itself, resulting in more faithful and computationally accessible explanations. To make query circuits practical, we address two challenges. First, we introduce Normalized Deviation Faithfulness (NDF), a robust metric to evaluate how well a discovered circuit recovers the model's decision for a specific input, and is broadly applicable to circuit discovery beyond our setting. Second, we develop sampling-based methods to efficiently identify circuits that are sparse yet faithfully describe the model's behavior. Across benchmarks (IOI, arithmetic, MMLU, and ARC), we find that there exist extremely sparse query circuits within the model that can recover much of its performance on single queries. For example, a circuit covering only 1.3% of model connections can recover about 60% of performance on an MMLU questions. Overall, query circuits provide a step towards faithful, scalable explanations of how language models process individual inputs. 

**Abstract (ZH)**: 解释语言模型生成特定输出的原因需要输入级别的本地解释。现有的方法可以揭示全局能力电路（例如，间接对象识别），但不能解释模型是如何以特定方式回答具体输入查询的。我们提出了查询电路，它可以直接追踪模型内部从特定输入到输出的信息流动。与基于代理的方法（例如，稀疏自动编码器）不同，查询电路是在模型本身中识别出来的，从而提供了更忠实且计算上可访问的解释。为了使查询电路实用，我们解决了两个挑战。首先，我们引入了规范偏差忠实性（NDF），这是一种鲁棒的指标，用于评估发现的电路在多大程度上恢复了模型对特定输入的决策，并且该指标广泛适用于超出我们设置的电路发现。其次，我们开发了基于采样的方法，以高效地识别稀疏但忠实描述模型行为的电路。在多种基准测试（IOI、算术、MMLU和ARC）中，我们发现模型内部存在极其稀疏的查询电路，这些电路可以恢复模型在单个查询上的大部分性能。例如，仅覆盖模型连接的1.3%的电路可以帮助恢复MMLU问题上约60%的性能。总体而言，查询电路代表了一种朝着忠实、可扩展的语言模型个体输入处理解释迈出的一步。 

---
# TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models 

**Title (ZH)**: TimeOmni-1：在大型语言模型中通过时间序列激励复杂推理 

**Authors**: Tong Guan, Zijie Meng, Dianqi Li, Shiyu Wang, Chao-Han Huck Yang, Qingsong Wen, Zuozhu Liu, Sabato Marco Siniscalchi, Ming Jin, Shirui Pan  

**Link**: [PDF](https://arxiv.org/pdf/2509.24803)  

**Abstract**: Recent advances in multimodal time series learning underscore a paradigm shift from analytics centered on basic patterns toward advanced time series understanding and reasoning. However, existing multimodal time series datasets mostly remain at the level of surface alignment and question answering, without reaching the depth of genuine reasoning. The absence of well-defined tasks that genuinely require time series reasoning, along with the scarcity of high-quality data, has limited progress in building practical time series reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite (TSR-Suite), which formalizes four atomic tasks that span three fundamental capabilities for reasoning with time series: (1) perception, acquired through scenario understanding and causality discovery; (2) extrapolation, realized via event-aware forecasting; and (3) decision-making, developed through deliberation over perception and extrapolation. TSR-Suite is the first comprehensive time series reasoning suite that supports not only thorough evaluation but also the data pipeline and training of TSRMs. It contains more than 23K samples, of which 2.3K are carefully curated through a human-guided hierarchical annotation process. Building on this foundation, we introduce TimeOmni-1, the first unified reasoning model designed to address diverse real-world problems demanding time series reasoning. The model is trained in multiple stages, integrating a mixture of task scenarios, novel reward functions, and tailored optimizations. Experiments show that TimeOmni-1 delivers strong out-of-distribution generalization across all tasks and achieves a high rate of valid responses. It significantly improves causality discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response rate by over 6% compared to GPT-4.1 on the event-aware forecasting task. 

**Abstract (ZH)**: 近期多模态时间序列学习的进展标志着从基本模式分析向高级时间序列理解和推理的范式转变。然而，现有的多模态时间序列数据集主要停留在表面对齐和问答层面，未能达到真正的推理深度。缺乏真正需要时间序列推理的定义明确的任务以及高质量数据的稀缺性限制了构建实用的时间序列推理模型(TSRMs)的进展。为此，我们提出时间序列推理套件(TSR-Suite)，它正规化了四个原子任务，涵盖了推理时间序列的三种基本能力：(1) 通过场景理解和因果发现获得的感知；(2) 通过事件感知预测实现的外推；以及(3) 通过感知和外推的权衡发展决策制定。TSR-Suite 是第一个全面的时间序列推理套件，不仅支持彻底的评估，还支持 TSRMs 的数据管道和训练。它包含超过 2.3 万个样本，其中 2300 个通过人工引导的分层标注过程谨慎挑选。基于这一基础，我们提出了 TimeOmni-1，这是第一个统一的推理模型，旨在解决多种需要时间序列推理的实际问题。该模型通过多个训练阶段进行训练，结合了多种任务场景、新型奖励函数和定制优化。实验结果显示，TimeOmni-1 在所有任务中的离群值泛化表现强劲，并达到了较高的有效响应率。与 GPT-4.1 相比，TimeOmni-1 在因果发现准确性上提高了 64.0%（相比之下 GPT-4.1 为 35.9%），在事件感知预测任务中有效响应率提高了超过 6%。 

---
# From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent for LLM Logical Reasoning 

**Title (ZH)**: 从歧义到判决：基于语义学的多视角代理模型在LLM逻辑推理中的应用 

**Authors**: Yunyao Zhang, Xinglang Zhang, Junxi Sheng, Wenbing Li, Junqing Yu, Wei Yang, Zikai Song  

**Link**: [PDF](https://arxiv.org/pdf/2509.24765)  

**Abstract**: Logical reasoning is a fundamental capability of large language models (LLMs). However, existing studies largely overlook the interplay between logical complexity and semantic complexity, resulting in methods that struggle to address challenging scenarios involving abstract propositions, ambiguous contexts, and conflicting stances, which are central to human reasoning. For this gap, we propose LogicAgent, a semiotic-square-guided framework designed to jointly address logical complexity and semantic complexity. LogicAgent explicitly performs multi-perspective deduction in first-order logic (FOL), while mitigating vacuous reasoning through existential import checks that incorporate a three-valued decision scheme (True, False, Uncertain) to handle boundary cases more faithfully. Furthermore, to overcome the semantic simplicity and low logical complexity of existing datasets, we introduce RepublicQA, a benchmark that reaches college-level difficulty (FKGL = 11.94) and exhibits substantially greater lexical and structural diversity than prior benchmarks. RepublicQA is grounded in philosophical concepts, featuring abstract propositions and systematically organized contrary and contradictory relations, making it the most semantically rich resource for evaluating logical reasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art performance on RepublicQA, with a 6.25% average gain over strong baselines, and generalizes effectively to mainstream logical reasoning benchmarks including ProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05% average gain. These results highlight the strong effectiveness of our semiotic-grounded multi-perspective reasoning in boosting LLMs' logical performance. 

**Abstract (ZH)**: 逻辑推理是大型语言模型（LLMs）的一项基本能力。然而，现有研究很大程度上忽视了逻辑复杂性和语义复杂性之间的交互作用，导致方法在处理涉及抽象命题、含糊语境和相互矛盾立场的具有挑战性的场景时表现不佳，这些都是人类推理的核心内容。为弥补这一差距，我们提出了LogicAgent，一个基于语义方格的框架，旨在同时解决逻辑复杂性和语义复杂性问题。LogicAgent 明确在一阶逻辑（FOL）中进行多视角演绎，并通过结合三种值决策方案（True、False、Uncertain）的存在性导入检查来减少空洞推理，从而更忠实于边界情况的处理。此外，为克服现有数据集的语义简单性和较低的逻辑复杂性，我们引入了RepublicQA，这是一个达到大学水平难度（FKGL = 11.94）的基准，并在词汇和结构多样性方面显著超过了先前的基准。RepublicQA 基于哲学概念，包含抽象命题并系统组织对立和矛盾关系，使其成为评估逻辑推理的最具语义丰富性的资源。实验证明，LogicAgent 在RepublicQA 上实现了最先进的性能，与强基线相比平均提升6.25%，并在主流逻辑推理基准ProntoQA、ProofWriter、FOLIO和ProverQA上表现出有效的泛化能力，额外实现平均7.05%的提升。这些结果突显了我们基于语义的多视角推理在提升LLMs逻辑性能方面的强大效果。 

---
# Spatial-Functional awareness Transformer-based graph archetype contrastive learning for Decoding Visual Neural Representations from EEG 

**Title (ZH)**: 基于空间-功能感知的Transformer图原型对比学习方法从EEG解码视觉神经表示 

**Authors**: Yueming Sun, Long Yang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24761)  

**Abstract**: Decoding visual neural representations from Electroencephalography (EEG) signals remains a formidable challenge due to their high-dimensional, noisy, and non-Euclidean nature. In this work, we propose a Spatial-Functional Awareness Transformer-based Graph Archetype Contrastive Learning (SFTG) framework to enhance EEG-based visual decoding. Specifically, we introduce the EEG Graph Transformer (EGT), a novel graph-based neural architecture that simultaneously encodes spatial brain connectivity and temporal neural dynamics. To mitigate high intra-subject variability, we propose Graph Archetype Contrastive Learning (GAC), which learns subject-specific EEG graph archetypes to improve feature consistency and class separability. Furthermore, we conduct comprehensive subject-dependent and subject-independent evaluations on the Things-EEG dataset, demonstrating that our approach significantly outperforms prior state-of-the-art EEG decoding this http URL results underscore the transformative potential of integrating graph-based learning with contrastive objectives to enhance EEG-based brain decoding, paving the way for more generalizable and robust neural representations. 

**Abstract (ZH)**: 从脑电图（EEG）信号解码视觉神经表征是一项艰巨的挑战，由于其高维、噪声和非欧几里得特性。在此工作中，我们提出了一种基于空间-功能意识变换器的图原型对比学习（SFTG）框架，以增强基于EEG的视觉解码。具体而言，我们引入了脑电图变换器（EGT），这是一种新颖的基于图的神经架构，能够同时编码空间脑连接性和时间神经动力学。为了减轻高被试内变异性，我们提出了图原型对比学习（GAC），以学习被试特异性的EEG图原型，从而提高特征一致性并增强类别可分性。此外，我们在Things-EEG数据集中进行了全面的被试依赖性和被试独立性评估，结果表明我们的方法显著优于先前的最佳EEG解码方法。这些结果强调了将基于图的学习与对比目标集成以增强基于EEG的大脑解码的变革潜力，为更泛化和稳健的神经表示开辟了道路。 

---
# On the Self-awareness of Large Reasoning Models' Capability Boundaries 

**Title (ZH)**: 大型推理模型自我意识的能力边界 

**Authors**: Qingjie Zhang, Yujia Fu, Yang Wang, Liu Yan, Tao Wei, Ke Xu, Minlie Huang, Han Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24711)  

**Abstract**: Large Reasoning Models (LRMs) have shown impressive performance on complex reasoning tasks such as mathematics, yet they also display misbehaviors that expose their limitations. In particular, when faced with hard questions, LRMs often engage in unproductive reasoning until context limit, producing wrong answers while wasting substantial computation. This phenomenon reflects a fundamental issue: current answering paradigms overlook the relationship between questions and LRMs' capability boundaries. In this paper, we investigate whether LRMs possess self-awareness of capability boundaries. We begin by an observation that LRMs may know what they cannot solve through expressed reasoning confidence. For black-box models, we find that reasoning expressions reveal boundary signals, with accelerated growing confidence trajectory for solvable problems but convergent uncertainty trajectory for unsolvable ones. For white-box models, we show that hidden states of the last input token encode boundary information, with solvable and unsolvable problems linearly separable even before reasoning begins. Building on these findings, we propose two simple yet effective optimization strategies: reasoning expression monitoring and hidden states monitoring. Experiments demonstrate that these boundary-aware strategies enable LRMs to avoid unproductive reasoning without sacrificing accuracy, significantly improving reliability and efficiency by cutting token usage up to 62.7 - 93.6%. 

**Abstract (ZH)**: 大型推理模型（LRMs）在复杂推理任务如数学中展现了令人印象深刻的性能，但也显示了暴露其局限性的不良行为。尤其是在面对难题时，LRMs 通常会无成效地推理直到上下文限制，产生错误答案同时浪费了大量的计算资源。这一现象反映出一个根本性问题：当前的作答范式忽视了问题与LRMs能力边界之间的关系。在本文中，我们探讨了LRMs是否具有对其能力边界的自我认知。我们首先观察到，LRMs可能通过表达的推理信心知道它们无法解决什么问题。对于黑箱模型，我们发现推理表达揭示了边界信号，可解问题的信心增长轨迹加快，而不可解问题的不确定性收敛轨迹趋同。对于白箱模型，我们展示了最后一个输入词的隐藏状态编码了边界信息，即使在推理开始之前，可解问题和不可解问题也是线性可分的。基于这些发现，我们提出了两种简便而有效的优化策略：推理表达监控和隐藏状态监控。实验表明，这些具备边界意识的策略能够让LRMs避免无成效的推理而不牺牲准确性，显著提高了可靠性和效率，最高可减少62.7%-93.6%的词使用量。 

---
# Successful Misunderstandings: Learning to Coordinate Without Being Understood 

**Title (ZH)**: 成功的误解：学习在不被理解的情况下协调 

**Authors**: Nikolaos Kondylidis, Anil Yaman, Frank van Harmelen, Erman Acar, Annette ten Teije  

**Link**: [PDF](https://arxiv.org/pdf/2509.24660)  

**Abstract**: The main approach to evaluating communication is by assessing how well it facilitates coordination. If two or more individuals can coordinate through communication, it is generally assumed that they understand one another. We investigate this assumption in a signaling game where individuals develop a new vocabulary of signals to coordinate successfully. In our game, the individuals do not have common observations besides the communication signal and outcome of the interaction, i.e. received reward. This setting is used as a proxy to study communication emergence in populations of agents that perceive their environment very differently, e.g. hybrid populations that include humans and artificial agents. Agents develop signals, use them, and refine interpretations while not observing how other agents are using them. While populations always converge to optimal levels of coordination, in some cases, interacting agents interpret and use signals differently, converging to what we call successful misunderstandings. However, agents of population that coordinate using misaligned interpretations, are unable to establish successful coordination with new interaction partners. Not leading to coordination failure immediately, successful misunderstandings are difficult to spot and repair. Having at least three agents that all interact with each other are the two minimum conditions to ensure the emergence of shared interpretations. Under these conditions, the agent population exhibits this emergent property of compensating for the lack of shared observations of signal use, ensuring the emergence of shared interpretations. 

**Abstract (ZH)**: 评估通信的主要方法是考察其促进协调的效果。如果两人或多人能够通过通信协调一致，通常假定他们相互理解。我们在一个信号游戏中研究这一假设，该游戏中个体发展出新的信号词汇以成功协调。在游戏中，个体除了通信信号和交互结果（即获得的奖励）之外没有其他共同观察。这一设定被用作代理，以研究在感知环境差异极大的代理种群中通信的涌现。代理开发信号、使用信号并改进解释，但不观察其他代理如何使用。尽管种群总是会收敛到最优的协调水平，但在某些情况下，交互代理会以不同的方式解释和使用信号，最终达到我们称之为成功的误解。然而，使用不一致解释进行协调的代理种群无法与新的交互伙伴建立有效的协调。成功的误解不容易被发现和修正，直到协调失败才显现出来。至少有三个相互作用的代理是确保共享解释涌现的两个最小条件。在这些条件下，代理种群表现出补偿缺乏信号使用共享观察的涌现特性，从而确保共享解释的涌现。 

---
# "Stop replacing salt with sugar!'': Towards Intuitive Human-Agent Teaching 

**Title (ZH)**: “停止用糖替代盐!”：向着直觉的人机教学 

**Authors**: Nikolaos Kondylidis, Andrea Rafanelli, Ilaria Tiddi, Annette ten Teije, Frank van Harmelen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24651)  

**Abstract**: Humans quickly learn new concepts from a small number of examples. Replicating this capacity with Artificial Intelligence (AI) systems has proven to be challenging. When it comes to learning subjective tasks-where there is an evident scarcity of data-this capacity needs to be recreated. In this work, we propose an intuitive human-agent teaching architecture in which the human can teach an agent how to perform a task by providing demonstrations, i.e., examples. To have an intuitive interaction, we argue that the agent should be able to learn incrementally from a few single examples. To allow for this, our objective is to broaden the agent's task understanding using domain knowledge. Then, using a learning method to enable the agent to learn efficiently from a limited number of examples. Finally, to optimize how human can select the most representative and less redundant examples to provide the agent with. We apply our proposed method to the subjective task of ingredient substitution, where the agent needs to learn how to substitute ingredients in recipes based on human examples. We replicate human input using the Recipe1MSubs dataset. In our experiments, the agent achieves half its task performance after only 100 examples are provided, compared to the complete training set of 50k examples. We show that by providing examples in strategic order along with a learning method that leverages external symbolic knowledge, the agent can generalize more efficiently. 

**Abstract (ZH)**: 人类可以从少量示例中迅速学习新概念。在人工智能系统中复制这一能力 proven具有挑战性。对于具有明显数据稀缺性的主观任务，这种能力需要重新创造。在本文中，我们提出了一种直觉的人机教学架构，其中人类可以通过演示（即示例）来教导代理执行任务。为了实现直观的交互，我们认为代理应该能够从少量单个示例中进行增量学习。为此，我们的目标是利用领域知识来扩展代理的任务理解，然后通过学习方法使代理能够高效地从有限数量的示例中学习。最后，我们优化了人类如何选择最具代表性和较少冗余的示例来提供给代理的过程。我们将提出的方法应用于主观任务的材料替换任务，代理需要根据人类示例学习如何在食谱中替换材料。我们使用Recipe1MSubs数据集复制人类输入。在我们的实验中，代理在仅提供100个示例后完成了其任务性能的一半，与完整的50,000个示例训练集相比。我们展示了通过按战略顺序提供示例并结合利用外部符号知识的学习方法，代理可以更有效地泛化。 

---
# LTL$_f$ Learning Meets Boolean Set Cover 

**Title (ZH)**: LTL$_f$ 学习与布尔集合覆盖相结合 

**Authors**: Gabriel Bathie, Nathanaël Fijalkow, Théo Matricon, Baptiste Mouillon, Pierre Vandenhove  

**Link**: [PDF](https://arxiv.org/pdf/2509.24616)  

**Abstract**: Learning formulas in Linear Temporal Logic (LTLf) from finite traces is a fundamental research problem which has found applications in artificial intelligence, software engineering, programming languages, formal methods, control of cyber-physical systems, and robotics. We implement a new CPU tool called Bolt improving over the state of the art by learning formulas more than 100x faster over 70% of the benchmarks, with smaller or equal formulas in 98% of the cases. Our key insight is to leverage a problem called Boolean Set Cover as a subroutine to combine existing formulas using Boolean connectives. Thanks to the Boolean Set Cover component, our approach offers a novel trade-off between efficiency and formula size. 

**Abstract (ZH)**: 从有限轨迹学习线性时序逻辑（LTLf）公式：一种比现有技术快超过100倍的新CPU工具及其高效与公式大小的新型权衡 

---
# BPMN Assistant: An LLM-Based Approach to Business Process Modeling 

**Title (ZH)**: BPMN 助手：基于 LLM 的业务流程建模方法 

**Authors**: Josip Tomo Licardo, Nikola Tankovic, Darko Etinger  

**Link**: [PDF](https://arxiv.org/pdf/2509.24592)  

**Abstract**: This paper presents BPMN Assistant, a tool that leverages Large Language Models (LLMs) for natural language-based creation and editing of BPMN diagrams. A specialized JSON-based representation is introduced as a structured alternative to the direct handling of XML to enhance the accuracy of process modifications. Process generation quality is evaluated using Graph Edit Distance (GED) and Relative Graph Edit Distance (RGED), while editing performance is evaluated with a binary success metric. Results show that JSON and XML achieve similar similarity scores in generation, but JSON offers greater reliability, faster processing, and significantly higher editing success rates. We discuss key trade-offs, limitations, and future improvements. The implementation is available at this https URL. 

**Abstract (ZH)**: 本文介绍了BPMNAssistant，这是一种利用大型语言模型（LLMs）进行BPMN图的自然语言创建和编辑的工具。提出了一种专门的JSON基表示法，作为一种结构化替代方案，以增强过程修改的准确性。通过图编辑距离（GED）和相对图编辑距离（RGED）评估过程生成质量，通过二元成功度量评估编辑性能。结果显示，JSON和XML在生成时达到相似的相似性分数，但JSON在可靠性和处理速度上更具优势，并且编辑成功率显著更高。我们讨论了关键权衡、局限性和未来的改进。实现代码可在此处访问：this https URL。 

---
# Training Agents Inside of Scalable World Models 

**Title (ZH)**: 在可扩展世界模型内部训练代理 

**Authors**: Danijar Hafner, Wilson Yan, Timothy Lillicrap  

**Link**: [PDF](https://arxiv.org/pdf/2509.24527)  

**Abstract**: World models learn general knowledge from videos and simulate experience for training behaviors in imagination, offering a path towards intelligent agents. However, previous world models have been unable to accurately predict object interactions in complex environments. We introduce Dreamer 4, a scalable agent that learns to solve control tasks by reinforcement learning inside of a fast and accurate world model. In the complex video game Minecraft, the world model accurately predicts object interactions and game mechanics, outperforming previous world models by a large margin. The world model achieves real-time interactive inference on a single GPU through a shortcut forcing objective and an efficient transformer architecture. Moreover, the world model learns general action conditioning from only a small amount of data, allowing it to extract the majority of its knowledge from diverse unlabeled videos. We propose the challenge of obtaining diamonds in Minecraft from only offline data, aligning with practical applications such as robotics where learning from environment interaction can be unsafe and slow. This task requires choosing sequences of over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft purely from offline data, without environment interaction. Our work provides a scalable recipe for imagination training, marking a step towards intelligent agents. 

**Abstract (ZH)**: Dreamer 4: 一种通过快速准确的世界模型进行强化学习的可扩展智能体，在复杂视频游戏Minecraft中实现对象交互的精确预测 

---
# Experience-guided reflective co-evolution of prompts and heuristics for automatic algorithm design 

**Title (ZH)**: 基于经验的反思性共进化提示和启发式算法自动生成 

**Authors**: Yihong Liu, Junyi Li, Wayne Xin Zhao, Hongyu Lu, Ji-Rong Wen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24509)  

**Abstract**: Combinatorial optimization problems are traditionally tackled with handcrafted heuristic algorithms, which demand extensive domain expertise and significant implementation effort. Recent progress has highlighted the potential of automatic heuristics design powered by large language models (LLMs), enabling the automatic generation and refinement of heuristics. These approaches typically maintain a population of heuristics and employ LLMs as mutation operators to evolve them across generations. While effective, such methods often risk stagnating in local optima. To address this issue, we propose the Experience-Guided Reflective Co-Evolution of Prompt and Heuristics (EvoPH) for automatic algorithm design, a novel framework that integrates the island migration model with the elites selection algorithm to simulate diverse heuristics populations. In EvoPH, prompts are co-evolved with heuristic algorithms, guided by performance feedback. We evaluate our framework on two problems, i.e., Traveling Salesman Problem and Bin Packing Problem. Experimental results demonstrate that EvoPH achieves the lowest relative error against optimal solutions across both datasets, advancing the field of automatic algorithm design with LLMs. 

**Abstract (ZH)**: 经验引导的提示与启发式算法协同进化框架（EvoPH）：大规模语言模型驱动的自动算法设计 

---
# Neuroplasticity-inspired dynamic ANNs for multi-task demand forecasting 

**Title (ZH)**: 神经可塑性启发的动态多任务需求预测神经网络 

**Authors**: Mateusz Żarski, Sławomir Nowaczyk  

**Link**: [PDF](https://arxiv.org/pdf/2509.24495)  

**Abstract**: This paper introduces a novel approach to Dynamic Artificial Neural Networks (D-ANNs) for multi-task demand forecasting called Neuroplastic Multi-Task Network (NMT-Net). Unlike conventional methods focusing on inference-time dynamics or computational efficiency, our proposed method enables structural adaptability of the computational graph during training, inspired by neuroplasticity as seen in biological systems. Each new task triggers a dynamic network adaptation, including similarity-based task identification and selective training of candidate ANN heads, which are then assessed and integrated into the model based on their performance. We evaluated our framework using three real-world multi-task demand forecasting datasets from Kaggle. We demonstrated its superior performance and consistency, achieving lower RMSE and standard deviation compared to traditional baselines and state-of-the-art multi-task learning methods. NMT-Net offers a scalable, adaptable solution for multi-task and continual learning in time series prediction. The complete code for NMT-Net is available from our GitHub repository. 

**Abstract (ZH)**: 一种基于神经可塑性的多任务网络（NMT-Net）用于多任务需求预测的动态人工神经网络方法 

---
# Overcoming Over-Fitting in Constraint Acquisition via Query-Driven Interactive Refinement 

**Title (ZH)**: 通过查询驱动的交互式细化克服约束获取中的过拟合 

**Authors**: Vasileios Balafas, Dimos Tsouros, Nikolaos Ploskas, Kostas Stergiou  

**Link**: [PDF](https://arxiv.org/pdf/2509.24489)  

**Abstract**: Manual modeling in Constraint Programming is a substantial bottleneck, which Constraint Acquisition (CA) aims to automate. However, passive CA methods are prone to over-fitting, often learning models that include spurious global constraints when trained on limited data, while purely active methods can be query-intensive. We introduce a hybrid CA framework specifically designed to address the challenge of over-fitting in CA. Our approach integrates passive learning for initial candidate generation, a query-driven interactive refinement phase that utilizes probabilistic confidence scores (initialized by machine learning priors) to systematically identify over-fitted constraints, and a specialized subset exploration mechanism to recover valid substructures from rejected candidates. A final active learning phase ensures model completeness. Extensive experiments on diverse benchmarks demonstrate that our interactive refinement phase is crucial for achieving high target model coverage and overall model accuracy from limited examples, doing so with manageable query complexity. This framework represents a substantial advancement towards robust and practical constraint acquisition in data-limited scenarios. 

**Abstract (ZH)**: 手动建模是约束编程中的一个重大瓶颈，约束获取（CA）旨在自动完成这一过程。然而，被动的CA方法容易过拟合，在有限数据下往往会学习到包含虚假全局约束的模型，而纯粹的主动方法则可能查询密集。我们提出了一种混合CA框架，旨在解决CA中的过拟合挑战。该方法通过被动学习生成初始候选模型，通过基于概率置信分数（由机器学习先验初始化）的查询驱动交互式细化阶段系统地识别过拟合约束，并通过专门的子集探索机制从被拒绝的候选模型中恢复有效的子结构。最终的主动学习阶段确保模型完备性。广泛的实验表明，我们的交互式细化阶段对于从有限示例中实现高目标模型覆盖度和整体模型准确性至关重要，并且查询复杂性可管理。该框架朝着在数据受限场景下实现稳健且实用的约束获取迈出了重要一步。 

---
# ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling 

**Title (ZH)**: ContextPRM: 利用上下文一致性进行多领域测试时缩放 

**Authors**: Haotian Zhang, Liu Liu, Baosheng Yu, Jiayan Qiu, Likang Xiao, Yanwei Ren, Quan Chen, Xianglong Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24460)  

**Abstract**: Process reward models (PRMs) have demonstrated significant efficacy in enhancing the mathematical reasoning capabilities of large language models (LLMs) by leveraging test-time scaling (TTS). However, while most PRMs exhibit substantial gains in mathematical domains, the scarcity of domain-specific training data and knowledge-based learning patterns limits their generalization ability when faced with other domains. To address this limitation, we shift the learning objective from verifying domain-specific knowledge to modeling domain-agnostic logical flow. Centering on contextual coherence between chain-of-thought (CoT) steps, our approach is realized through a novel data annotation and training framework, which enhances the model's generalization capabilities across diverse domains. For instance, our resulting model, ContextPRM, achieves a notable 6.5% average accuracy improvement over the majority voting baseline via weighted majority voting across nine non-mathematical domains in MMLU-Pro, including law, history, and philosophy, significantly surpassing the 2.2% improvement from VersaPRM and 0.5% gains from other mathematics-focused PRMs, demonstrating consistent performance across both mathematical and non-mathematical domains. 

**Abstract (ZH)**: 过程奖励模型通过测试时扩展提高大型语言模型的数学推理能力，但受限于领域特定训练数据的稀缺性和基于知识的学习模式，其在面对其他领域时的泛化能力受限。为解决这一局限，我们将学习目标从验证领域特定知识转向建模领域无关的逻辑流程。基于链式思考步骤之间的上下文连贯性，我们提出了一种新型的数据标注与训练框架，增强了模型在多种领域的泛化能力。例如，我们得到的模型ContextPRM在MMLU-Pro的九个非数学领域（包括法律、历史和哲学）中，通过加权多数投票相较于多数投票基线实现了显著的6.5%平均准确率提升，远超VersaPRM的2.2%提升和其他专注于数学的PRMs的0.5%增益，展示了在数学和非数学领域的一致性能。 

---
# A Systematic Review of Digital Twin-Driven Predictive Maintenance in Industrial Engineering: Taxonomy, Architectural Elements, and Future Research Directions 

**Title (ZH)**: 数字孪生驱动的工业工程预测性维护综述：分类、架构要素及未来研究方向 

**Authors**: Leila Ismail, Abdelmoneim Abdelmoti, Arkaprabha Basu, Aymen Dia Eddine Berini, Mohammad Naouss  

**Link**: [PDF](https://arxiv.org/pdf/2509.24443)  

**Abstract**: With the increasing complexity of industrial systems, there is a pressing need for predictive maintenance to avoid costly downtime and disastrous outcomes that could be life-threatening in certain domains. With the growing popularity of the Internet of Things, Artificial Intelligence, machine learning, and real-time big data analytics, there is a unique opportunity for efficient predictive maintenance to forecast equipment failures for real-time intervention and optimize maintenance actions, as traditional reactive and preventive maintenance practices are often inadequate to meet the requirements for the industry to provide quality-of-services of operations. Central to this evolution is digital twin technology, an adaptive virtual replica that continuously monitors and integrates sensor data to simulate and improve asset performance. Despite remarkable progress in digital twin implementations, such as considering DT in predictive maintenance for industrial engineering. This paper aims to address this void. We perform a retrospective analysis of the temporal evolution of the digital twin in predictive maintenance for industrial engineering to capture the applications, middleware, and technological requirements that led to the development of the digital twin from its inception to the AI-enabled digital twin and its self-learning models. We provide a layered architecture of the digital twin technology, as well as a taxonomy of the technology-enabled industrial engineering applications systems, middleware, and the used Artificial Intelligence algorithms. We provide insights into these systems for the realization of a trustworthy and efficient smart digital-twin industrial engineering ecosystem. We discuss future research directions in digital twin for predictive maintenance in industrial engineering. 

**Abstract (ZH)**: 随着工业系统的日益复杂，迫切需要预测性维护以避免昂贵的停机时间和可能在某些领域带来生命危险的灾难性结果。随着物联网、人工智能、机器学习和实时大数据分析的日益流行，这为高效的预测性维护提供了独特机会，可以预测设备故障并进行实时干预，优化维护行动，而传统的被动和预防性维护实践往往无法满足工业提供服务质量的要求。这一演变的核心是数字孪生技术，这是一种适应性的虚拟复制品，持续监控并整合传感器数据以模拟和改善资产性能。尽管在数字孪生实施方面取得了显著进展，如将数字孪生应用于工业工程的预测性维护。本文旨在填补这一空白。我们对数字孪生在工业工程中预测性维护领域的历时演变进行了回顾性分析，以捕捉从数字孪生的起源到AI驱动的数字孪生及其自学习模型的发展过程中所依赖的应用、中间件和技术要求。我们提供了数字孪生技术的分层架构，并对技术赋能的工业工程应用系统、中间件以及使用的机器学习算法进行了分类。我们提供了这些系统的见解，以实现可信赖且高效的智能数字孪生工业工程生态体系。我们讨论了数字孪生在工业工程中预测性维护方面的未来研究方向。 

---
# Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention 

**Title (ZH)**: 通过纠正干预实现大型推理模型的安全推理 

**Authors**: Yichi Zhang, Yue Ding, Jingwen Yang, Tianwei Luo, Dongbai Li, Ranjie Duan, Qiang Liu, Hang Su, Yinpeng Dong, Jun Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24393)  

**Abstract**: Although Large Reasoning Models (LRMs) have progressed in solving complex problems, their chain-of-thought (CoT) reasoning often contains harmful content that can persist even when the final responses appear safe. We show that this issue still remains in existing methods which overlook the unique significance of safe reasoning, undermining their trustworthiness and posing potential risks in applications if unsafe reasoning is accessible for and exploited by malicious users. We therefore shift our focus to aligning the safety of reasoning itself in this paper and explore process supervision as the solution. However, simply rewarding safe reasoning proves inadequate due to low rollout diversity and limited training signals. To tackle this challenge, we first delve into the characteristics of safe reasoning and uncover several critical insights that 1) safe reasoning is often consolidated by a few critical steps of safety triggers; 2) compliance cues strongly correlate with unsafe continuations; and 3) corrective interventions reliably steer unsafe trajectories towards safer traces. Motivated by these, we propose Intervened Preference Optimization (IPO), an alignment method that enforces safe reasoning by substituting compliance steps with safety triggers and constructing pairs for preference learning with strong signals. Experiments on jailbreak and adversarial safety benchmarks demonstrate that IPO remarkably improves overall safety regarding both reasoning and responses, outperforming SFT-based and RL-based baselines with a relative reduction of over 30% in harmfulness, while preserving excellent performance across diverse reasoning tasks. The results highlight the importance of explicit alignment for reasoning and provide a practical path to safer LRMs. 

**Abstract (ZH)**: 虽然大规模推理模型（LRMs）在解决复杂问题方面取得了进展，但它们的链式思维（Chain-of-Thought）推理往往包含有害内容，即使最终响应看似安全，这些有害内容也可能持续存在。我们发现，现有方法忽视了安全推理的独特重要性，这损害了其可信度，并且如果恶意用户能够利用不安全的推理，将对应用构成潜在风险。因此，本文将重点转向确保推理本身的安全性，并探索过程监督作为解决方案。然而，简单地奖励安全推理由于多样性过低和训练信号有限而证明是不足的。为了应对这一挑战，我们首先深入探讨安全推理的特征，并发现几个关键洞见，即1）安全推理通常由少数关键步骤的安全触发器巩固；2）合规提示与不安全续作高度相关；3）纠正干预措施能可靠地引导不安全轨迹向更安全的轨迹转变。受此启发，我们提出了干预偏好优化（IPO），这是一种通过用安全触发器替换合规步骤并构建具有强大信号的对来进行偏好学习的对齐方法。在 Jailbreak 和对抗安全基准测试中的实验表明，IPO 在提高整体安全性（包括推理和响应）方面表现出色，相对于基于SFT 和基于RL 的基线，有害性降低了超过30%，同时在各种推理任务上保持了卓越的性能。这些结果突显了明确对齐对于推理的重要性，并提供了一条通往更安全的大规模推理模型的实际路径。 

---
# Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning with LLMs 

**Title (ZH)**: 未解决问题：面向问题的策略路由，用于具有LLM的数学推理 

**Authors**: Shihao Qi, Jie Ma, Ziang Yin, Lingling Zhang, Jian Zhang, Jun Liu, Feng Tian, Tongliang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24377)  

**Abstract**: Existing methods usually leverage a fixed strategy, such as natural language reasoning, code-augmented reasoning, tool-integrated reasoning, or ensemble-based reasoning, to guide Large Language Models (LLMs) to perform mathematical reasoning. Our analysis reveals that the single strategy cannot adapt to problem-specific requirements and thus overlooks the trade-off between effectiveness and efficiency. To address these issues, we propose Planning and Routing through Instance-Specific Modeling (PRISM), a novel framework that decouples mathematical reasoning into two stages: strategy planning and targeted execution. Specifically, we first curate a multi-strategy preference dataset, which we call MathStrat, capturing correctness, process quality, and computational efficiency for each problem--strategy pair. Then, we train a lightweight Strategy Adapter based on the dataset to obtain confidence distributions over the mentioned four reasoning strategies. At inference time, an adaptive routing policy dynamically tailors the reasoning approach based on predictor confidence. It directs the model to use single-strategy execution for high-confidence predictions, dual-strategy verification for competitive scenarios, or comprehensive multi-strategy exploration for uncertain cases. Extensive experiments across five mathematical reasoning benchmarks demonstrate that PRISM consistently outperforms individual strategies and ensemble baselines, achieving improvements ranging from 0.9% to 7.6% across different base models. The adaptive routing approach shows particularly strong benefits for mathematical reasoning tasks across diverse model architectures. Our code is released at this https URL. 

**Abstract (ZH)**: Planning and Routing through Instance-Specific Modeling for Mathematical Reasoning 

---
# From Static to Dynamic: Adaptive Monte Carlo Search for Mathematical Process Supervision 

**Title (ZH)**: 从静态到动态：适应性蒙特卡洛搜索在数学过程监督中的应用 

**Authors**: Jie Ma, Shihao Qi, Rui Xing, Ziang Yin, Bifan Wei, Jun Liu, Tongliang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24351)  

**Abstract**: The quality of process data plays a key role in training a Process Reward Model (PRM), which can enhance the complex mathematical reasoning capability of large language models. Existing methods estimate the quality of reasoning steps based on a fixed-budget sampling strategy and navigate a vast search space to perform path expansion during the automated data generation process, resulting in their inefficiency and inflexibility. To address these issues, we propose Adaptive Monte Carlo Search (AMCS), a framework that transforms data generation from fixed, static to adaptive, dynamic search at the level of node value estimation and path expansion. On one hand, AMCS adaptively refines estimation by allocating more samples to uncertain reasoning steps while using fewer samples for those that are easier to estimate. On the other hand, it enhances the path expansion through a Monte Carlo algorithm with a temporally adaptive policy that begins with broad exploration and gradually shifts toward exploiting the most promising directions. With AMCS, we construct a large-scale dataset MathSearch-200K of about 200K process supervision examples for training PRMs. To verify the effectiveness of our method, we conduct extensive experiments on four mathematical reasoning benchmarks. Experimental results show that Qwen2.5-Math-7B-PRM-AMCS achieves up to 76.2% accuracy on MATH500 with GLM-4-9B, outperforming all baseline PRMs. Notably, a 7B model supervised by Qwen2.5-Math-7B-PRM-AMCS surpasses a 72B model with weaker supervision. Moreover, Qwen2.5-Math-7B-PRM-AMCS maintains consistent advantages on out-of-distribution problems, demonstrating strong generalization capability. Our code is available at this https URL. 

**Abstract (ZH)**: 过程数据的质量对训练过程奖励模型（PRM）至关重要，这可以增强大型语言模型的复杂数学推理能力。现有方法基于固定预算的采样策略估计推理步骤的质量，并在自动化数据生成过程中通过路径扩展进行广泛的搜索空间探索，导致其效率低下和灵活性不足。为解决这些问题，我们提出自适应蒙特卡洛搜索（AMCS）框架，该框架在节点值估计和路径扩展的级别将数据生成从固定、静态转变为适应性、动态搜索。一方面，AMCS通过将更多样本分配给不确定性较大的推理步骤，同时减少容易估计步骤的样本数，自适应地细化估计。另一方面，它通过一种随时间自适应的蒙特卡洛算法增强路径扩展，该算法从广泛的探索开始，逐渐转向开发最有希望的方向。使用AMCS，我们构建了一个包含约20万个过程监督示例的大型数据集MathSearch-200K，用于训练PRMs。为进一步验证我们方法的有效性，我们在四个数学推理基准上进行了大量实验。实验结果表明，Qwen2.5-Math-7B-PRM-AMCS在GLM-4-9B上的MATH500准确性达到76.2%，优于所有基线PRMs。值得注意的是，一个由Qwen2.5-Math-7B-PRM-AMCS监督的7B模型超越了一个具有较弱监督的72B模型。此外，Qwen2.5-Math-7B-PRM-AMCS在分布外问题上保持一致的优势，显示出强大的泛化能力。我们的代码可在以下链接获取。 

---
# Fin-Ally: Pioneering the Development of an Advanced, Commonsense-Embedded Conversational AI for Money Matters 

**Title (ZH)**: Fin-Ally: 嵌入常识的先进金融对话AI先锋发展 

**Authors**: Sarmistha Das, Priya Mathur, Ishani Sharma, Sriparna Saha, Kitsuchart Pasupa, Alka Maurya  

**Link**: [PDF](https://arxiv.org/pdf/2509.24342)  

**Abstract**: The exponential technological breakthrough of the FinTech industry has significantly enhanced user engagement through sophisticated advisory chatbots. However, large-scale fine-tuning of LLMs can occasionally yield unprofessional or flippant remarks, such as ``With that money, you're going to change the world,'' which, though factually correct, can be contextually inappropriate and erode user trust. The scarcity of domain-specific datasets has led previous studies to focus on isolated components, such as reasoning-aware frameworks or the enhancement of human-like response generation. To address this research gap, we present Fin-Solution 2.O, an advanced solution that 1) introduces the multi-turn financial conversational dataset, Fin-Vault, and 2) incorporates a unified model, Fin-Ally, which integrates commonsense reasoning, politeness, and human-like conversational dynamics. Fin-Ally is powered by COMET-BART-embedded commonsense context and optimized with a Direct Preference Optimization (DPO) mechanism to generate human-aligned responses. The novel Fin-Vault dataset, consisting of 1,417 annotated multi-turn dialogues, enables Fin-Ally to extend beyond basic account management to provide personalized budgeting, real-time expense tracking, and automated financial planning. Our comprehensive results demonstrate that incorporating commonsense context enables language models to generate more refined, textually precise, and professionally grounded financial guidance, positioning this approach as a next-generation AI solution for the FinTech sector. Dataset and codes are available at: this https URL 

**Abstract (ZH)**: 金融科技行业指数级的技术突破通过复杂的顾问聊天机器人显著增强了用户参与度。然而，大规模微调大语言模型偶尔会产生不专业或轻率的评论，如“用那些钱，你会改变世界”，虽然事实正确，但在上下文中可能不适当并侵蚀用户信任。领域特定数据集的稀缺性使得先前的研究集中在孤立的组件上，如具备推理能力的框架或增强人类样式的响应生成。为解决这一研究缺口，我们提出了Fin-Solution 2.0，一种先进的解决方案，包括1)引入多轮金融对话数据集Fin-Vault，2)结合统一模型Fin-Ally，该模型整合了常识推理、礼貌性和人类样式的对话动态。Fin-Ally由嵌入常识背景的COMET-BART驱动，并通过直接偏好优化（DPO）机制生成与人类一致的回复。新型Fin-Vault数据集包含1,417条标注的多轮对话，使Fin-Ally能够超越基本的账户管理，提供个性化的预算规划、实时支出跟踪和自动化财务规划。我们的全面结果表明，引入常识背景使语言模型能够生成更精致、文本上更精确和专业化的金融指导，将该方法定位为金融科技领域下一代AI解决方案的数据集和代码可在以下链接获取：this https URL。 

---
# humancompatible.detect: a Python Toolkit for Detecting Bias in AI Models 

**Title (ZH)**: humancompatible.detect: 一个检测AI模型偏见的Python工具包 

**Authors**: German M. Matilla, Jiri Nemecek, Illia Kryvoviaz, Jakub Marecek  

**Link**: [PDF](https://arxiv.org/pdf/2509.24340)  

**Abstract**: There is a strong recent emphasis on trustworthy AI. In particular, international regulations, such as the AI Act, demand that AI practitioners measure data quality on the input and estimate bias on the output of high-risk AI systems. However, there are many challenges involved, including scalability (MMD) and computability (Wasserstein-1) issues of traditional methods for estimating distances on measure spaces. Here, we present this http URL, a toolkit for bias detection that addresses these challenges. It incorporates two newly developed methods to detect and evaluate bias: maximum subgroup discrepancy (MSD) and subsampled $\ell_\infty$ distances. It has an easy-to-use API documented with multiple examples. this http URL is licensed under the Apache License, Version 2.0. 

**Abstract (ZH)**: 近期对可信AI的高度关注。特别是国际法规，如AI法案，要求AI从业人员衡量输入数据质量并在高风险AI系统的输出中估算偏差。然而，这涉及许多挑战，包括衡量空间中传统方法估计距离的可扩展性（MMD）和可计算性（Wasserstein-1）问题。在此，我们介绍这个工具包：用于检测偏差的工具包，它解决了这些挑战。该工具包整合了两种新开发的方法来检测和评估偏差：最大子组离散度（MSD）和采样后的$\ell_\infty$距离。它具有易于使用的API，并附有多例文档说明。这个工具包采用Apache License, Version 2.0许可。 

---
# MedMMV: A Controllable Multimodal Multi-Agent Framework for Reliable and Verifiable Clinical Reasoning 

**Title (ZH)**: MedMMV：一个多模态多代理框架，用于可靠和可验证的临床推理 

**Authors**: Hongjun Liu, Yinghao Zhu, Yuhui Wang, Yitao Long, Zeyu Lai, Lequan Yu, Chen Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2509.24314)  

**Abstract**: Recent progress in multimodal large language models (MLLMs) has demonstrated promising performance on medical benchmarks and in preliminary trials as clinical assistants. Yet, our pilot audit of diagnostic cases uncovers a critical failure mode: instability in early evidence interpretation precedes hallucination, creating branching reasoning trajectories that cascade into globally inconsistent conclusions. This highlights the need for clinical reasoning agents that constrain stochasticity and hallucination while producing auditable decision flows. We introduce MedMMV, a controllable multimodal multi-agent framework for reliable and verifiable clinical reasoning. MedMMV stabilizes reasoning through diversified short rollouts, grounds intermediate steps in a structured evidence graph under the supervision of a Hallucination Detector, and aggregates candidate paths with a Combined Uncertainty scorer. On six medical benchmarks, MedMMV improves accuracy by up to 12.7% and, more critically, demonstrates superior reliability. Blind physician evaluations confirm that MedMMV substantially increases reasoning truthfulness without sacrificing informational content. By controlling instability through a verifiable, multi-agent process, our framework provides a robust path toward deploying trustworthy AI systems in high-stakes domains like clinical decision support. 

**Abstract (ZH)**: 近期多模态大型语言模型在医疗领域的进展已经在医学基准测试和初步临床辅助试验中展示了令人鼓舞的性能。然而，我们对诊断案例的初步审计揭示了一个关键的失败模式：在幻觉之前，早期证据解释的不稳定性引发了分支推理轨迹，导致全局不一致的结论。这突显了在约束不确定性与幻觉的同时，产生可验证决策流的临床推理代理的需求。我们引入了MedMMV，这是一种可控的多模态多代理框架，用于可靠的和可验证的临床推理。MedMMV通过多样化的短期模拟稳定推理，并在幻觉检测器的监督下，将中间步骤置于结构化的证据图中，并通过结合不确定性评分器聚合候选路径。在六个医疗基准测试中，MedMMV将准确性提高了最多12.7%，更重要的是，展示了更高的可靠性。盲评医师评估证实，MedMMV在提高推理真实性的同时并未牺牲信息含量。通过控制可能验证的多代理过程中的不稳定性，我们的框架为在高风险领域如临床决策支持中部署可信赖的AI系统提供了稳健的途径。 

---
# Experience Paper: Adopting Activity Recognition in On-demand Food Delivery Business 

**Title (ZH)**: 经验论文：在即时食品配送业务中采用活动识别 

**Authors**: Huatao Xu, Yan Zhang, Wei Gao, Guobin Shen, Mo Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.24303)  

**Abstract**: This paper presents the first nationwide deployment of human activity recognition (HAR) technology in the on-demand food delivery industry. We successfully adapted the state-of-the-art LIMU-BERT foundation model to the delivery platform. Spanning three phases over two years, the deployment progresses from a feasibility study in Yangzhou City to nationwide adoption involving 500,000 couriers across 367 cities in China. The adoption enables a series of downstream applications, and large-scale tests demonstrate its significant operational and economic benefits, showcasing the transformative potential of HAR technology in real-world applications. Additionally, we share lessons learned from this deployment and open-source our LIMU-BERT pretrained with millions of hours of sensor data. 

**Abstract (ZH)**: 本文介绍了首次在全国范围内将人体活动识别（HAR）技术应用于按需食品配送行业。我们成功将先进的LIMU-BERT基础模型适应到配送平台。历时两年，部署分为三个阶段，从扬州市的可行性研究扩展到全国367个城市，涉及500,000名配送员。该采用使一系列下游应用成为可能，大规模测试表明其在运营和经济方面的显著效益，展示了HAR技术在实际应用中的变革潜力。此外，我们分享了此次部署的经验教训，并开源了基于数百万小时传感器数据预训练的LIMU-BERT模型。 

---
# SCI-Verifier: Scientific Verifier with Thinking 

**Title (ZH)**: SCI-Verifier: 科学验证器与思考 

**Authors**: Shenghe Zheng, Chenyu Huang, Fangchen Yu, Junchi Yao, Jingqi Ye, Tao Chen, Yun Luo, Ning Ding, LEI BAI, Ganqu Cui, Peng Ye  

**Link**: [PDF](https://arxiv.org/pdf/2509.24285)  

**Abstract**: As large language models (LLMs) are increasingly applied to scientific reasoning, the complexity of answer formats and the diversity of equivalent expressions make answer verification a critical yet challenging task. Existing verification studies in scientific domains suffer from two major limitations: (a) the absence of systematic evaluation standards and insufficient disciplinary coverage, which hinders their comprehensive assessment; and (b) heavy reliance on cumbersome rule design or prompt engineering, which reduces their effectiveness in complex reasoning scenarios or limits their cross-disciplinary generalization. To address these challenges, we propose solutions at both the data and model levels. On the data side, we construct SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics, biology, chemistry, and general scientific QA. The benchmark is built from real LLM responses and enhanced with domain-specific equivalence transformations that generate challenging and realistic data. Model-based and expert annotations ensure both quality and diversity, enabling rigorous evaluation of verification ability. On the model side, we emphasize the importance of reasoning for verification and introduce SCI-Verifier, a unified reasoning-augmented verifier for scientific domains. Through post-training, SCI-Verifier demonstrates strong logical reasoning and equivalence judgment capabilities while maintaining concise and stable outputs. Together, SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific verification, offering both systematic evaluation and practical pathways to enhance the reliability and applicability of LLMs in scientific domains. 

**Abstract (ZH)**: 大型语言模型（LLMs）在科学推理中的应用日益增多，答案格式的复杂性和等效表达方式的多样性使得答案验证成为一项关键而具有挑战的任务。现有科学领域内的验证研究面临两大局限：（a）缺乏系统的评估标准和学科覆盖面不足，这阻碍了其综合评估；（b）过度依赖繁琐的规则设计或提示工程技术，这在复杂推理场景中降低了其有效性或限制了其跨学科的泛化能力。为应对这些挑战，我们在数据和模型两个层面提出了解决方案。在数据方面，我们构建了SCI-VerifyBench，这是一个涵盖数学、物理、生物、化学和一般科学问答的跨学科基准。该基准源自真实的LLM响应，并通过领域特定的等效变换生成具有挑战性和现实性的数据。基于模型和专家注释确保了质量和多样性，从而能够严格评估验证能力。在模型方面，我们强调了推理对于验证的重要性，并引入了SCI-Verifier，这是一个适用于科学领域的统一推理增强验证器。通过后训练，SCI-Verifier展示了强大的逻辑推理和等价判断能力，同时保持简洁稳定的输出。总体而言，SCI-VerifyBench和SCI-Verifier为科学验证提供了一个原则性的框架，提供了一体化的评估方法和提高大型语言模型在科学领域可靠性和适用性的实际路径。 

---
# G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge 

**Title (ZH)**: G-reasoner: 面向图结构知识统一推理的foundation模型 

**Authors**: Linhao Luo, Zicheng Zhao, Junnan Liu, Zhangchi Qiu, Junnan Dong, Serge Panev, Chen Gong, Thuy-Trang Vu, Gholamreza Haffari, Dinh Phung, Alan Wee-Chung Liew, Shirui Pan  

**Link**: [PDF](https://arxiv.org/pdf/2509.24276)  

**Abstract**: Large language models (LLMs) excel at complex reasoning but remain limited by static and incomplete parametric knowledge. Retrieval-augmented generation (RAG) mitigates this by incorporating external knowledge, yet existing RAGs struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. Graphs offer a natural way to model relationships within knowledge, but LLMs are inherently unstructured and cannot effectively reason over graph-structured data. Recent graph-enhanced RAG (GraphRAG) attempts to bridge this gap by constructing tailored graphs and enabling LLMs to reason on them. However, these methods often depend on ad-hoc graph designs, heuristic search, or costly agent pipelines, which hinder scalability and generalization. To address these challenges, we present G-reasoner, a unified framework that integrates graph and language foundation models for reasoning over diverse graph-structured knowledge. Central to our approach is QuadGraph, a standardized four-layer abstraction that unifies heterogeneous knowledge sources into a common graph representation. Building on this, we introduce a 34M-parameter graph foundation model (GFM) that jointly captures graph topology and textual semantics, and is integrated with LLMs to enhance reasoning in downstream applications. To ensure scalability and efficiency, mixed-precision training and distributed message-passing are implemented to scale GFM with more GPUs. Extensive experiments on six benchmarks show that G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization. 

**Abstract (ZH)**: 基于图的推理器：统一图与语言基础模型框架以处理多元图结构知识 

---
# AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models 

**Title (ZH)**: AdvChain: 对抗链式推理调优以提高大型推理模型的安全对齐鲁棒性 

**Authors**: Zihao Zhu, Xinyu Wu, Gehan Hu, Siwei Lyu, Ke Xu, Baoyuan Wu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24269)  

**Abstract**: Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in complex problem-solving through Chain-of-Thought (CoT) reasoning. However, the multi-step nature of CoT introduces new safety challenges that extend beyond conventional language model alignment. We identify a failure mode in current safety CoT tuning methods: the \textit{snowball effect}, where minor reasoning deviations progressively amplify throughout the thought process, leading to either harmful compliance or excessive refusal. This effect stems from models being trained to imitate perfect reasoning scripts without learning to self-correct. To address this limitation, we propose AdvChain, an alignment paradigm that teaches models dynamic self-correction through adversarial CoT tuning. Our method involves constructing a dataset containing Temptation-Correction and Hesitation-Correction samples, where models learn to recover from harmful reasoning drifts and unnecessary cautions. Extensive experiments show that AdvChain significantly enhances robustness against jailbreak attacks and CoT hijacking while substantially reducing over-refusal on benign prompts, achieving a superior safety-utility balance without compromising reasoning capabilities. Our work establishes a new direction for building more robust and reliable reasoning models. 

**Abstract (ZH)**: 大型推理模型（LRMs）通过链式推理（CoT）在复杂问题解决方面展现出了显著的能力。然而，CoT的多步性质引入了新的安全挑战，超出了传统语言模型对齐的范围。我们指出现有安全CoT调优方法中的一个失效率模式：雪球效应，即细微的推理偏差逐渐在整个推理过程中放大，导致要么过度合规要么过度拒绝。这种效应源于模型被训练模仿完美的推理脚本，却无法学会自我纠正。为解决这一局限，我们提出AdvChain，一种通过对抗式CoT调优教授模型动态自我纠正的对齐范式。我们的方法包括构建一个包含诱惑纠正和犹豫纠正样本的数据集，使模型学会从有害的推理偏移和不必要的谨慎中恢复。广泛实验证明，AdvChain显著增强了模型对逃脱攻击和CoT劫持的鲁棒性，同时大幅减少了对良性提示的过度拒绝，实现了安全性和实用性的优化平衡而不会牺牲推理能力。我们的工作为构建更稳健和可靠的推理模型开辟了新方向。 

---
# PAME-AI: Patient Messaging Creation and Optimization using Agentic AI 

**Title (ZH)**: PAME-AI：使用代理人工智能进行患者消息创建与优化 

**Authors**: Junjie Luo, Yihong Guo, Anqi Liu, Ritu Agarwal, Gordon  

**Link**: [PDF](https://arxiv.org/pdf/2509.24263)  

**Abstract**: Messaging patients is a critical part of healthcare communication, helping to improve things like medication adherence and healthy behaviors. However, traditional mobile message design has significant limitations due to its inability to explore the high-dimensional design space. We develop PAME-AI, a novel approach for Patient Messaging Creation and Optimization using Agentic AI. Built on the Data-Information-Knowledge-Wisdom (DIKW) hierarchy, PAME-AI offers a structured framework to move from raw data to actionable insights for high-performance messaging design. PAME-AI is composed of a system of specialized computational agents that progressively transform raw experimental data into actionable message design strategies. We demonstrate our approach's effectiveness through a two-stage experiment, comprising of 444,691 patient encounters in Stage 1 and 74,908 in Stage 2. The best-performing generated message achieved 68.76% engagement compared to the 61.27% baseline, representing a 12.2\% relative improvement in click-through rates. This agentic architecture enables parallel processing, hypothesis validation, and continuous learning, making it particularly suitable for large-scale healthcare communication optimization. 

**Abstract (ZH)**: 基于代理AI的患者信息创设与优化：PAME-AI方法 

---
# Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models 

**Title (ZH)**: 风险敏感的RL方法用于缓解大型语言模型中的探索困境 

**Authors**: Yuhua Jiang, Jiawei Huang, Yufeng Yuan, Xin Mao, Yu Yue, Qianchuan Zhao, Lin Yan  

**Link**: [PDF](https://arxiv.org/pdf/2509.24261)  

**Abstract**: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing Large Language Models (LLMs) on complex reasoning tasks. However, existing methods suffer from an exploration dilemma: the sharply peaked initial policies of pre-trained LLMs confine standard RL algorithms to a narrow set of solutions, boosting single-solution accuracy (pass@1) but suppressing solution diversity and multi-solution performance (pass@k). As a result, RLVR often distills existing capabilities rather than discovering new reasoning strategies. To overcome this, we introduce a Risk-Sensitive Reinforcement Learning framework. Our approach employs a risk-seeking objective that interpolates between mean and maximum rewards, leading to a novel algorithm, Risk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying learning from challenging prompts. Remarkably, RS-GRPO is simple to implement, requiring only minor code modifications. On six mathematical reasoning benchmarks and with five different LLMs, RS-GRPO consistently improves pass@k performance while maintaining or enhancing pass@1 accuracy. 

**Abstract (ZH)**: 可验证奖励的强化学习（RLVR）已被证明在增强大型语言模型（LLMs）处理复杂推理任务方面效果显著。然而，现有方法面临探索困境：预训练LLMs的尖峰初始策略限制了标准RL算法的解决方案范围，提升了单解准确性（pass@1）但抑制了解决方案多样性及多解性能（pass@k）。因此，RLVR往往提炼现有能力而非发现新的推理策略。为克服这一问题，我们引入了一种风险敏感强化学习框架。该方法采用一种风险寻求的目标，介于平均值和最大值奖励之间，提出了一种新型算法——风险敏感GRPO（RS-GRPO），通过增强对挑战性提示的学习促进更深入的探索。值得注意的是，RS-GRPO易于实现，只需进行少量代码修改。在六个数学推理基准测试和五种不同的LLMs上，RS-GRPO一致提高了pass@k性能，同时保持或提升了pass@1准确性。 

---
# Rethinking and Benchmarking Large Language Models for Graph Reasoning 

**Title (ZH)**: 重思与基准测试用于图推理的大语言模型 

**Authors**: Yuwei Hu, Xinyi Huang, Zhewei Wei, Yongchao Liu, Chuntao Hong  

**Link**: [PDF](https://arxiv.org/pdf/2509.24260)  

**Abstract**: Large Language Models (LLMs) for Graph Reasoning have been extensively studied over the past two years, involving enabling LLMs to understand graph structures and reason on graphs to solve various graph problems, with graph algorithm problems being the most prevalent. Recent studies underscore the potential of LLMs in handling graph reasoning tasks, but their performance is underwhelming. In this work, we point out issues with existing methods and benchmarks, and rethink the direction that LLMs for graph reasoning should strive toward. We find that base models, e.g., GPT-4o-mini, are largely underestimated due to improper reasoning focus. Base models with reasoning focus redirected from replicating graph algorithms to designing them can easily solve most graph reasoning tasks in existing benchmarks. To truly evaluate the graph reasoning capabilities of LLMs, we construct a more challenging GraphAlgorithm benchmark, comprising 239 different graph problems and 3,041 test instances collected from 4 competition platforms. Finally, we introduce a simple and strong baseline Simple-Reasoning-Then-Coding (Simple-RTC)-which guides LLMs to design graph algorithms first and then code to address graph reasoning tasks. Simple-RTC achieves near-perfect accuracy on existing benchmarks and significantly outperforms GPT-4o-mini and all prior methods on the GraphAlgorithm benchmark. This strong baseline encourages further advancements in LLMs for Graph Reasoning in the future. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在图推理中的研究：识别现有方法和基准的局限性，重新思考图推理方向，并构建更加具有挑战性的GraphAlgorithm基准，以评估图推理能力 

---
# Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations 

**Title (ZH)**: 基于叙述演示的协作物理活动建模的交互式程序合成 

**Authors**: Edward Kim, Daniel He, Jorge Chao, Wiktor Rajca, Mohammed Amin, Nishant Malpani, Ruta Desai, Antti Oulasvirta, Bjoern Hartmann, Sanjit Seshia  

**Link**: [PDF](https://arxiv.org/pdf/2509.24250)  

**Abstract**: Teaching systems physical tasks is a long standing goal in HCI, yet most prior work has focused on non collaborative physical activities. Collaborative tasks introduce added complexity, requiring systems to infer users assumptions about their teammates intent, which is an inherently ambiguous and dynamic process. This necessitates representations that are interpretable and correctable, enabling users to inspect and refine system behavior. We address this challenge by framing collaborative task learning as a program synthesis problem. Our system represents behavior as editable programs and uses narrated demonstrations, i.e. paired physical actions and natural language, as a unified modality for teaching, inspecting, and correcting system logic without requiring users to see or write code. The same modality is used for the system to communicate its learning to users. In a within subjects study, 20 users taught multiplayer soccer tactics to our system. 70 percent (14/20) of participants successfully refined learned programs to match their intent and 90 percent (18/20) found it easy to correct the programs. The study surfaced unique challenges in representing learning as programs and in enabling users to teach collaborative physical activities. We discuss these issues and outline mitigation strategies. 

**Abstract (ZH)**: 在人机交互中教授系统执行物理任务是一项长期目标，但大多数先前的工作集中在非协作的物理活动上。协作任务增加了复杂性，要求系统推断用户对其队友意图的假设，这是一个本质上既模糊又动态的过程。这需要可解释且可纠正的表示，使用户能够检查和改进系统行为。我们通过将协作任务学习框定为程序合成问题来应对这一挑战。我们的系统将行为表示为可编辑的程序，并使用叙述性示范，即配对的物理动作和自然语言，作为一种统一的模态，用于教学、检查和纠正系统逻辑，而无需用户看到或编写代码。系统同样使用这种模态向用户传达其所学内容。在一项单被试内研究中，20名用户教我们的系统多玩家足球战术。70%（14/20）的参与者成功地修正了所学程序以匹配其意图，90%（18/20）的参与者发现修正程序很容易。该研究揭示了将学习表示为程序以及使用户能够教授协作物理活动时的独特挑战。我们讨论了这些问题并概述了缓解策略。 

---
# SpecExit: Accelerating Large Reasoning Model via Speculative Exit 

**Title (ZH)**: SpecExit： through推测性退出加速大型推理模型 

**Authors**: Rubing Yang, Huajun Bai, Song Liu, Guanghua Yu, Runzhi Fan, Yanbin Dang, Jiejing Zhang, Kai Liu, Jianchen Zhu, Peng Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24248)  

**Abstract**: Despite their strong performance on reasoning tasks, large reasoning models (LRMs) often suffer from overthinking, producing unnecessarily long outputs and incurring high end-to-end latency, a significant limitation to their real-world deployment. To address overthinking, early-exit mechanisms have been proposed to terminate reasoning before typical completion, showing that this approach can effectively shorten generation length with minimal impact on accuracy. However, their reliance on probing mechanisms introduces a detection overhead that limits their end-to-end latency gains and compromises their generalizability across diverse problems. Inspired by the use of hidden states in speculative decoding, we propose SpecExit, a novel framework that predicts both future tokens and an early-exit signal directly from a lightweight draft model without probing overhead. Our method offers significant improvements, reducing average generation length by 66\% and achieving a 2.5x speedup in end-to-end latency compared to the speculative decoding baseline, without compromising accuracy. Our method leverages the inherent signals from hidden states to provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Our code is available at this https URL. 

**Abstract (ZH)**: 尽管大型推理模型在推理任务中表现出色，但它们往往会过度推理，生成不必要的长输出，并导致较高的端到端延迟，这成为其实际部署中的一个重要限制。为解决过度推理问题，已提出了早期退出机制，可以在典型完成之前终止推理，表明这种方法可以在不显著影响准确性的前提下有效缩短生成长度。然而，这些方法依赖于探针机制，引入了检测开销，限制了其端到端延迟的改进，并降低了其在多样问题上的普适性。受投机解码中隐藏状态使用启发，我们提出SpecExit，一种新颖的框架，可以直接从轻量级草图模型中预测未来的令牌和早期退出信号，而不需要探针开销。我们的方法提供了显著的改进，平均生成长度减少了66%，端到端延迟提高了2.5倍，同时保持了准确性。我们的方法利用隐藏状态中的固有信号提供有效的早期退出信号，表明隐藏状态在高效推理中的更广泛使用。我们的代码可在此访问：this https URL。 

---
# Model Merging Scaling Laws in Large Language Models 

**Title (ZH)**: 大型语言模型中的模型合并标度律 

**Authors**: Yuanyi Wang, Yanggan Gu, Yiming Zhang, Qi Zhou, Zhaoyi Yan, Congkai Xie, Xinyao Wang, Jianbo Yuan, Hongxia Yang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24244)  

**Abstract**: We study empirical scaling laws for language model merging measured by cross-entropy. Despite its wide practical use, merging lacks a quantitative rule that predicts returns as we add experts or scale the model size. We identify a compact power law that links model size and expert number: the size-dependent floor decreases with model capacity, while the merging tail exhibits clear diminishing returns in the number of experts. The law holds in-domain and cross-domain, tightly fits measured curves across diverse architectures and methods (Average, TA, TIES, DARE), and explains two robust regularities: most gains arrive early, and variability shrinks as more experts are included. Building on this, we present a simple theory that explains why gains fall roughly as 1/k and links the floor and tail to properties of the base model and the diversity across domains. This law enables predictive planning: estimate how many experts are needed to reach a target loss, decide when to stop adding experts, and trade off scaling the base model versus adding experts under a fixed budget--turning merging from heuristic practice into a computationally efficient, planable alternative to multitask training. This suggests a scaling principle for distributed generative AI: predictable gains can be achieved by composing specialists, offering a complementary path toward AGI-level systems. 

**Abstract (ZH)**: 我们研究了由交叉熵度量的语言模型合并的经验标度定律。尽管合并广泛应用于实践中，但仍缺乏一个定量规则来预测随着增加专家数量或扩大模型规模所带来的回报。我们发现了一个紧凑的幂律模型，将模型规模和专家数量联系起来：依赖于模型容量的下限随模型容量增加而减少，而合并的尾部随着专家数量增加表现出明显的边际收益递减现象。这一规律在领域内和领域间都适用，紧密契合不同架构和方法（Average、TA、TIES、DARE）测量出的曲线，并解释了两种稳健的规律：大多数收益出现在早期，且随着更多专家的加入，收益的波动性会减小。基于此，我们提出了一种简单的理论来解释为什么收益下降得大致与1/k成比例，并将下限和尾巴与基模型的属性以及领域间多样性联系起来。这一规律使预测性规划成为可能：估算达到目标损失所需的专家数量，决定何时停止添加专家，并在固定预算下权衡扩展基础模型与添加专家——将模型合并从一种经验做法转变为一种计算上高效的、可计划的多任务训练替代方案。这为分布式生成AI提供了一个可扩展的原则：通过组合专家可以获得可预测的收益，为通往AGI水平系统的路径提供补充。 

---
# Learning to Ponder: Adaptive Reasoning in Latent Space 

**Title (ZH)**: 学会深思：潜空间中的自适应推理 

**Authors**: Yixin He, Lumingyuan Tang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24238)  

**Abstract**: Test-time compute has emerged as a key paradigm for enhancing LLM reasoning, yet prevailing approaches like Best-of-N and majority voting apply uniform depth across inputs, wasting computation on simple queries while potentially under-thinking complex ones. We present FR-Ponder, a single-graph, backbone-training-free framework that allocates instance-adaptive reasoning compute via latent steering. A less than 1M-param controller observes hidden states and decides to halt or apply a small ponder step by adding a pre-computed steering vector to frozen representations. Our method extracts the latent steering vector associated with deeper reasoning outputs and direct IO from LLM and re-applies it through a tunable scaling factor, allowing the model to adapt its reasoning depth to the complexity of each input. To balance performance and computational cost, we employ Group Relative Policy Optimization (GRPO) as a reward signal to adaptively regulate reasoning depth, achieving task accuracy while mitigating overreasoning. Through curriculum learning and careful reward engineering, FR-Ponder learns calibrated compute allocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder improves the compute-accuracy frontier, delivering lower FLOPs with better matched accuracy and comparing favorably to early-exit baselines, without modifying backbone weights. Analyses visualize interpretable steering directions and show learned compute allocation correlates with problem difficulty. 

**Abstract (ZH)**: FR-Ponder：一种基于 latent steering 的自适应推理计算框架 

---
# ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration 

**Title (ZH)**: ELHPlan: 高效远期任务规划用于多智能体协作 

**Authors**: Shaobin Ling, Yun Wang, Chenyou Fan, Tin Lun Lam, Junjie Hu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24230)  

**Abstract**: Large Language Models (LLMs) enable intelligent multi-robot collaboration but face fundamental trade-offs: declarative methods lack adaptability in dynamic environments, while iterative methods incur prohibitive computational costs that scale poorly with team size and task complexity. In this paper, we propose ELHPlan, a novel framework that introduces Action Chains--sequences of actions explicitly bound to sub-goal intentions--as the fundamental planning primitive. ELHPlan operates via a cyclical process: 1) constructing intention-bound action sequences, 2) proactively validating for conflicts and feasibility, 3) refining issues through targeted mechanisms, and 4) executing validated actions. This design balances adaptability and efficiency by providing sufficient planning horizons while avoiding expensive full re-planning. We further propose comprehensive efficiency metrics, including token consumption and planning time, to more holistically evaluate multi-agent collaboration. Our experiments on benchmark TDW-MAT and C-WAH demonstrate that ELHPlan achieves comparable task success rates while consuming only 24% of the tokens required by state-of-the-art methods. Our research establishes a new efficiency-effectiveness frontier for LLM-based multi-agent planning systems. 

**Abstract (ZH)**: 基于大语言模型的多机器人协作框架ELHPlan：兼具适应性和效率的新范式 

---
# Humanline: Online Alignment as Perceptual Loss 

**Title (ZH)**: Humanline: 在线对齐作为感知损失 

**Authors**: Sijia Liu, Niklas Muennighoff, Kawin Ethayarajh  

**Link**: [PDF](https://arxiv.org/pdf/2509.24207)  

**Abstract**: Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral economics, we propose a human-centric explanation. We prove that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clipping -- originally introduced to just stabilize training -- recovers a perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. Our theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since we can achieve the same effect by selectively training on any data in a manner that mimics human perception, rather than restricting ourselves to online on-policy data. Doing so would allow us to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, we propose a design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks. 

**Abstract (ZH)**: 基于行为经济学前景理论的在线对齐为何更优——一种以人为中心的解释及其实验设计 

---
# Robust Preference Optimization: Aligning Language Models with Noisy Preference Feedback 

**Title (ZH)**: 鲁棒的偏好优化：将语言模型与嘈杂的偏好反馈对齐 

**Authors**: Xiaoyang Cao, Zelai Xu, Mo Guang, Kaiwen Long, Michiel A. Bakker, Yu Wang, Chao Yu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24159)  

**Abstract**: Standard human preference-based alignment methods, such as Reinforcement Learning from Human Feedback (RLHF), are a cornerstone technology for aligning Large Language Models (LLMs) with human values. However, these methods are all underpinned by a critical, yet flawed assumption: human preferences are homogeneous (representing a single, unified preference) and the collected data is noiseless (free from error). In reality, neither is true since human preference is pluralistic and annotators can make mistakes. This creates a discrepancy between the recorded data and the ground-truth preferences, which can misguide the model and degrade its performance. To address this challenge, we introduce Robust Preference Optimization (RPO). RPO employs an Expectation-Maximization (EM) algorithm to infer the posterior probability of each label's correctness, which is used to adaptively re-weigh each data point in the training loss to mitigate noise. We further generalize this approach by establishing a theoretical link between arbitrary preference losses and their corresponding probabilistic models. This generalization enables the systematic transformation of existing alignment algorithms into their robust counterparts, elevating RPO from a specific algorithm to a meta-framework for robust preference alignment. Theoretically, we prove that under the condition of a perfectly calibrated model, RPO is guaranteed to converge to the true noise level of the dataset. Our experiments demonstrate RPO's effectiveness as a meta-framework, consistently enhancing four state-of-the-art alignment algorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3 models, the RPO-enhanced methods achieve substantial win rate gains on AlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% and 5.4%, respectively. 

**Abstract (ZH)**: 稳健偏好优化：一种稳健的人类偏好对齐元框架 

---
# Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models 

**Title (ZH)**: 推理还是检索？大规模推理模型的答案归因研究 

**Authors**: Yuhui Wang, Changjiang Li, Guangke Chen, Jiacheng Liang, Ting Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24156)  

**Abstract**: Large reasoning models (LRMs) exhibit unprecedented capabilities in solving complex problems through Chain-of-Thought (CoT) reasoning. However, recent studies reveal that their final answers often contradict their own reasoning traces. We hypothesize that this inconsistency stems from two competing mechanisms for generating answers: CoT reasoning and memory retrieval. To test this hypothesis, we conduct controlled experiments that challenge LRMs with misleading cues during reasoning and/or corrupted answers during retrieval. Our results across models and datasets confirm that both mechanisms operate simultaneously, with their relative dominance influenced by multiple factors: problem domains, model scales, and fine-tuning approaches (e.g., reinforcement learning vs. distillation). The findings reveal a critical limitation in current reasoning fine-tuning paradigms: models can exploit the retrieval mechanism as a shortcut, effectively "hacking" the reward signal and undermining genuine reasoning development. To address this challenge, we introduce FARL, a novel fine-tuning framework that integrates memory unlearning with reinforcement learning. By carefully suppressing retrieval shortcuts during the fine-tuning process, FARL promotes reasoning-dominant behavior and enhances generalizable reasoning capabilities. 

**Abstract (ZH)**: 大型推理模型（LRMs）通过链式推理（CoT）展现出解决复杂问题的前所未有的能力。然而，近期研究表明，它们的最终答案往往与其推理轨迹相矛盾。我们假设这种不一致性源自生成答案的两种竞争机制：链式推理和记忆检索。为了检验这一假设，我们进行了控制实验，这些实验在推理过程中或检索过程中向LRMs提供误导性线索或错误的答案。我们的研究结果证实，这两种机制同时运作，它们的相对主导地位受多种因素影响：问题领域、模型规模以及微调方法（例如强化学习与蒸馏）。这些发现揭示了当前推理微调范式的一个关键局限性：模型可以利用检索机制作为捷径，有效地“ hack”奖励信号，削弱真正的推理发展。为了应对这一挑战，我们提出了一种名为FARL的新颖微调框架，该框架将记忆遗忘与强化学习相结合。通过在微调过程中精心抑制检索捷径，FARL促进了以推理为主的机制并增强了可泛化的推理能力。 

---
# Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework 

**Title (ZH)**: 透明、可评估且可访问的数据代理：一个概念验证框架 

**Authors**: Nooshin Bahador  

**Link**: [PDF](https://arxiv.org/pdf/2509.24127)  

**Abstract**: This article presents a modular, component-based architecture for developing and evaluating AI agents that bridge the gap between natural language interfaces and complex enterprise data warehouses. The system directly addresses core challenges in data accessibility by enabling non-technical users to interact with complex data warehouses through a conversational interface, translating ambiguous user intent into precise, executable database queries to overcome semantic gaps. A cornerstone of the design is its commitment to transparent decision-making, achieved through a multi-layered reasoning framework that explains the "why" behind every decision, allowing for full interpretability by tracing conclusions through specific, activated business rules and data points. The architecture integrates a robust quality assurance mechanism via an automated evaluation framework that serves multiple functions: it enables performance benchmarking by objectively measuring agent performance against golden standards, and it ensures system reliability by automating the detection of performance regressions during updates. The agent's analytical depth is enhanced by a statistical context module, which quantifies deviations from normative behavior, ensuring all conclusions are supported by quantitative evidence including concrete data, percentages, and statistical comparisons. We demonstrate the efficacy of this integrated agent-development-with-evaluation framework through a case study on an insurance claims processing system. The agent, built on a modular architecture, leverages the BigQuery ecosystem to perform secure data retrieval, apply domain-specific business rules, and generate human-auditable justifications. The results confirm that this approach creates a robust, evaluable, and trustworthy system for deploying LLM-powered agents in data-sensitive, high-stakes domains. 

**Abstract (ZH)**: 基于模块化组件的设计：结合自然语言接口与复杂企业数据仓库的AI代理开发与评估架构 

---
# Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs 

**Title (ZH)**: Fathom-DeepResearch: 解锁SLMs的长时信息检索与合成能力 

**Authors**: Shreyas Singh, Kunal Singh, Pradeep Moturi  

**Link**: [PDF](https://arxiv.org/pdf/2509.24107)  

**Abstract**: Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA. 

**Abstract (ZH)**: 基于工具集成的推理已成为推动自主应用的关键焦点。在其中，DeepResearch 代理因其在复杂开放性信息检索任务上的出色表现而受到广泛关注。我们介绍了一种名为Fathom-DeepResearch的自主系统，该系统由两个专门模型组成。第一个是Fathom-Search-4B，这是一种从Qwen3-4B训练而来的DeepSearch模型，通过实时网络搜索和定向网页查询优化用于证据基础的调查。其训练结合了三项改进：(i) DUETQA，一个通过多智能体自我对弈生成的5千样本数据集，强制执行严格的网络搜索依赖性和异质来源接地；(ii) RAPO，这是一种零开销的GRPO扩展，通过课程剪枝、奖励意识优势缩放和每个提示回放缓冲区实现了可验证奖励的多轮强化学习稳定；(iii) 可调节的步骤级奖励，根据认知行为和边际效用对每次工具调用进行分类，允许对搜索轨迹的宽度、深度和视界进行显式控制。这些改进使Fathom-Search-4B能够在必要时可靠地扩展工具调用超过20次。第二个是Fathom-Synthesizer-4B，这是一种从Qwen3-4B训练而来的模型，将多轮DeepSearch踪迹转换为结构化、引文密集型的DeepResearch报告，用于综合总结。该系统在DeepSearch基准测试（SimpleQA、FRAMES、WebWalker、Seal0、MuSiQue）和DeepResearch-Bench上进行评估，实现了开放权重类别中的最佳性能，同时展示了强大的泛化能力，涵盖了包括HLE、AIME-25、GPQA-Diamond和MedQA在内的多种推理任务。 

---
# Do Repetitions Matter? Strengthening Reliability in LLM Evaluations 

**Title (ZH)**: 重复重要吗？加强大语言模型评估的可靠性 

**Authors**: Miguel Angel Alvarado Gonzalez, Michelle Bruno Hernandez, Miguel Angel Peñaloza Perez, Bruno Lopez Orozco, Jesus Tadeo Cruz Soto, Sandra Malagon  

**Link**: [PDF](https://arxiv.org/pdf/2509.24086)  

**Abstract**: LLM leaderboards often rely on single stochastic runs, but how many repetitions are required for reliable conclusions remains unclear. We re-evaluate eight state-of-the-art models on the AI4Math Benchmark with three independent runs per setting. Using mixed-effects logistic regression, domain-level marginal means, rank-instability analysis, and run-to-run reliability, we assessed the value of additional repetitions. Our findings shows that Single-run leaderboards are brittle: 10/12 slices (83\%) invert at least one pairwise rank relative to the three-run majority, despite a zero sign-flip rate for pairwise significance and moderate overall interclass correlation. Averaging runs yields modest SE shrinkage ($\sim$5\% from one to three) but large ranking gains; two runs remove $\sim$83\% of single-run inversions. We provide cost-aware guidance for practitioners: treat evaluation as an experiment, report uncertainty, and use $\geq 2$ repetitions under stochastic decoding. These practices improve robustness while remaining feasible for small teams and help align model comparisons with real-world reliability. 

**Abstract (ZH)**: LLM领奖榜往往依赖单次随机运行，但可靠结论所需重复次数仍然不确定。我们使用三个独立运行重新评估了AI4Math基准上的八种领先模型。通过混合效应逻辑回归、领域级边缘均值、排名不稳定性分析和运行间可靠性评估，我们探讨了额外重复的价值。我们的研究发现：单次运行领奖榜是脆弱的：12种切片中有10种（83%）与三次运行的多数结果在成对排名上至少翻转一次，尽管成对显著性无符号翻转且总体类间相关度适中。平均运行可适度减少标准误（从一次到三次约降低5%），但显著提高排名一致性；两次运行可去除约83%的单次运行排名翻转。我们提供成本意识指导：将评估视为实验，报告不确定性，并在随机解码下使用至少两次重复。这些做法能提升稳健性，同时对小型团队仍具可行性，并有助于使模型对比与现实可靠性一致。 

---
# Future-Proofing Programmers: Optimal Knowledge Tracing for AI-Assisted Personalized Education 

**Title (ZH)**: 面向未来的程序员：AI辅助个性化教育的最佳知识追踪 

**Authors**: Yuchen Wang, Pei-Duo Yu, Chee Wei Tan  

**Link**: [PDF](https://arxiv.org/pdf/2509.23996)  

**Abstract**: Learning to learn is becoming a science, driven by the convergence of knowledge tracing, signal processing, and generative AI to model student learning states and optimize education. We propose CoTutor, an AI-driven model that enhances Bayesian Knowledge Tracing with signal processing techniques to improve student progress modeling and deliver adaptive feedback and strategies. Deployed as an AI copilot, CoTutor combines generative AI with adaptive learning technology. In university trials, it has demonstrated measurable improvements in learning outcomes while outperforming conventional educational tools. Our results highlight its potential for AI-driven personalization, scalability, and future opportunities for advancing privacy and ethical considerations in educational technology. Inspired by Richard Hamming's vision of computer-aided 'learning to learn,' CoTutor applies convex optimization and signal processing to automate and scale up learning analytics, while reserving pedagogical judgment for humans, ensuring AI facilitates the process of knowledge tracing while enabling learners to uncover new insights. 

**Abstract (ZH)**: 学习如何学习正成为一门科学，由知识追踪、信号处理和生成式AI的结合驱动，以建模学生的学习状态并优化教育。我们提出CoTutor，一种基于生成式AI和适配学习技术增强贝叶斯知识追踪的AI驱动模型，以提升学生进展建模和提供适应性反馈和策略。作为AI副驾部署，CoTutor在大学试验中表现出可测量的学习成果提升，并超越了传统教育工具。我们的结果突显了其在AI驱动个性化、可扩展性以及教育技术中隐私和伦理考虑方面未来机遇的潜力。受Richard Hamming关于‘学习如何学习’的计算机辅助愿景启发，CoTutor应用凸优化和信号处理技术自动化和规模化学习分析，同时保留教学判断权于人类，确保AI促进知识追踪过程，同时帮助学习者发现新的见解。 

---
# LLM/Agent-as-Data-Analyst: A Survey 

**Title (ZH)**: LLM/Agent-as-数据分析师：一个综述 

**Authors**: Zirui Tang, Weizheng Wang, Zihang Zhou, Yang Jiao, Bangrui Xu, Boyu Niu, Xuanhe Zhou, Guoliang Li, Yeye He, Wei Zhou, Yitong Song, Cheng Tan, Bin Wang, Conghui He, Xiaoyang Wang, Fan Wu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23988)  

**Abstract**: Large language model (LLM) and agent techniques for data analysis (a.k.a LLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both academica and industry. In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration. The technical evolution further distills five key design goals for intelligent data analysis agents, namely semantic-aware design, modality-hybrid integration, autonomous pipelines, tool-augmented workflows, and support for open-world tasks. From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., table question answering for relational data and NL2GQL for graph data), (ii) semi-structured data (e.g., markup languages understanding and semi-structured table modeling), (iii) unstructured data (e.g., chart understanding, document understanding, programming languages vulnerable detection), and (iv) heterogeneous data (e.g., data retrieval and modality alignment for data lakes). Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis. 

**Abstract (ZH)**: 大规模语言模型（LLM）和代理技术在数据分析中的应用（即LLM/代理作为数据分析员）在学术界和工业界都展现了显著影响。与传统的基于规则或小型模型的方法相比，代理型LLM能够实现复杂的数据理解、自然语言接口、语义分析功能以及自主管道编排。技术进步进一步提炼出智能数据分析代理的五大关键设计目标，即语义感知设计、多模态集成、自主管道、工具增强的工作流程以及支持开放世界任务。从模态的角度来看，我们回顾了基于LLM的技术，包括结构化数据（例如，关系数据的表格问答和NL2GQL的图形数据）、半结构化数据（例如，标记语言理解和半结构化表格建模）、非结构化数据（例如，图表理解、文档理解、编程语言漏洞检测）以及异构数据（例如，数据湖中的数据检索和模态对齐）。最后，我们概述了剩余的挑战，并提出了促进LLM/代理驱动的数据分析的若干见解和实际方向。 

---
# TusoAI: Agentic Optimization for Scientific Methods 

**Title (ZH)**: TusoAI: 主体化优化科学方法 

**Authors**: Alistair Turcan, Kexin Huang, Lei Li, Martin Jinye Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23986)  

**Abstract**: Scientific discovery is often slowed by the manual development of computational tools needed to analyze complex experimental data. Building such tools is costly and time-consuming because scientists must iteratively review literature, test modeling and scientific assumptions against empirical data, and implement these insights into efficient software. Large language models (LLMs) have demonstrated strong capabilities in synthesizing literature, reasoning with empirical data, and generating domain-specific code, offering new opportunities to accelerate computational method development. Existing LLM-based systems either focus on performing scientific analyses using existing computational methods or on developing computational methods or models for general machine learning without effectively integrating the often unstructured knowledge specific to scientific domains. Here, we introduce TusoAI , an agentic AI system that takes a scientific task description with an evaluation function and autonomously develops and optimizes computational methods for the application. TusoAI integrates domain knowledge into a knowledge tree representation and performs iterative, domain-specific optimization and model diagnosis, improving performance over a pool of candidate solutions. We conducted comprehensive benchmark evaluations demonstrating that TusoAI outperforms state-of-the-art expert methods, MLE agents, and scientific AI agents across diverse tasks, such as single-cell RNA-seq data denoising and satellite-based earth monitoring. Applying TusoAI to two key open problems in genetics improved existing computational methods and uncovered novel biology, including 9 new associations between autoimmune diseases and T cell subtypes and 7 previously unreported links between disease variants linked to their target genes. Our code is publicly available at this https URL. 

**Abstract (ZH)**: 科学发现往往因需手工开发用于分析复杂实验数据的计算工具而受阻。构建这些工具既耗时又昂贵，因为科学家必须迭代性地查阅文献、用经验数据测试建模和科学假设，并将这些见解整合到高效的软件中。大型语言模型（LLMs）在综合文献、基于经验数据推理以及生成领域特定代码方面展现出强大能力，为加速计算方法的发展提供了新的机会。现有基于LLM的系统要么专注于使用现有计算方法进行科学分析，要么专注于开发适用于通用机器学习的计算方法或模型，但未能有效整合科学领域经常存在的不结构化的知识。在这里，我们介绍了一种自主型AI系统TusoAI，它接受科学研究任务描述和评估函数，并自主开发和优化适用于特定应用的计算方法。TusoAI将领域知识整合到知识树表示中，并进行迭代的、特定于领域的优化和模型诊断，从而提升候选解决方案的性能。我们进行了全面的基准评估，结果显示，TusoAI在多种任务中优于最先进的专家方法、最大似然估计（MLE）代理和科学AI代理，比如单细胞RNA测序数据去噪和基于卫星的地球监测。将TusoAI应用于遗传学中的两个关键开放问题，不仅改进了现有的计算方法，还发现了新的生物学现象，包括9种自身免疫性疾病与T细胞亚型之间的新关联和7种以前未报道的与疾病变体及其目标基因之间的联系。我们的代码可在以下网址公开获得：this https URL。 

---
# Automatic selection of primary studies in systematic reviews with evolutionary rule-based classification 

**Title (ZH)**: 基于进化规则分类的系统评价中主要研究的自动选择 

**Authors**: José de la Torre-López, Aurora Ramírez, José Raúl Romero  

**Link**: [PDF](https://arxiv.org/pdf/2509.23981)  

**Abstract**: Searching, filtering and analysing scientific literature are time-consuming tasks when performing a systematic literature review. With the rise of artificial intelligence, some steps in the review process are progressively being automated. In particular, machine learning for automatic paper selection can greatly reduce the effort required to identify relevant literature in scientific databases. We propose an evolutionary machine learning approach, called \ourmodel, to automatically determine whether a paper retrieved from a literature search process is relevant. \ourmodel builds an interpretable rule-based classifier using grammar-guided genetic programming. The use of a grammar to define the syntax and the structure of the rules allows \ourmodel to easily combine the usual textual information with other bibliometric data not considered by state-of-the-art methods. Our experiments demonstrate that it is possible to generate accurate classifiers without impairing interpretability and using configurable information sources not supported so far. 

**Abstract (ZH)**: 系统文献综述中搜索、筛选和分析科学文献是耗时的任务。随着人工智能的发展，审查过程中的某些步骤正逐渐实现自动化。特别是，用于自动论文筛选的机器学习可以大大减少在科学数据库中识别相关文献所需的努力。我们提出了一种进化机器学习方法，称为\ourmodel，以自动确定从文献搜索过程中检索到的论文是否相关。\ourmodel 使用语法引导的遗传编程构建了一个可解释的基于规则的分类器。使用语法来定义规则的语法和结构，使得\ourmodel 可以轻松地结合通常的文本信息和其他不属于现有方法考虑的引文计量数据。我们的实验表明，可以在不损害可解释性的情况下生成准确的分类器，并且可以使用迄今为止尚未配置的信息来源。 

---
# Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models 

**Title (ZH)**: 大型推理模型中强化学习的优势估计条件优势估计 

**Authors**: Guanxu Chen, Yafu Li, Yuxian Jiang, Chen Qian, Qihan Ren, Jingyi Yang, Yu Cheng, Dongrui Liu, Jing Shao  

**Link**: [PDF](https://arxiv.org/pdf/2509.23962)  

**Abstract**: Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning capabilities on tasks with clear correctness criteria, such as mathematical reasoning tasks. Several training metrics, such as entropy or response length, have been observed to correlate with different reasoning behaviors in reinforcement learning. Prior approaches incorporate such priors through reward or advantage shaping, which often relies on hand-crafted penalties and preferences (e.g., higher-is-better or lower-is-better). However, without careful hyperparameter tuning, these directional priors can be overly biased and may lead to failure. To this end, we introduce Conditional advANtage estimatiON (CANON), amplifying the impact of the target metric without presuming its direction. Specifically, CANON regroups the sampled responses into two groups based on the higher or lower value of a target metric, measures which metric trend contributes to better performance through inter-group comparison, and identifies the better response within the same group. In summary, CANON based on entropy consistently outperforms prior methods across three LLMs on both math reasoning and high-complexity logic tasks. When applied to response length, CANON further improves token efficiency, yielding a more favorable Pareto frontier in the performance-cost trade-off. 

**Abstract (ZH)**: 可验证奖励的强化学习（RLVR）在大型语言模型（LLMs）上的进展：以数学推理任务等具有明确正确性标准的任务为例 

---
# From Neural Networks to Logical Theories: The Correspondence between Fibring Modal Logics and Fibring Neural Networks 

**Title (ZH)**: 从神经网络到逻辑理论：纤维化模态逻辑与纤维化神经网络之间的对应关系 

**Authors**: Ouns El Harzli, Bernardo Cuenca Grau, Artur d'Avila Garcez, Ian Horrocks, Tarek R. Besold  

**Link**: [PDF](https://arxiv.org/pdf/2509.23912)  

**Abstract**: Fibring of modal logics is a well-established formalism for combining countable families of modal logics into a single fibred language with common semantics, characterized by fibred models. Inspired by this formalism, fibring of neural networks was introduced as a neurosymbolic framework for combining learning and reasoning in neural networks. Fibring of neural networks uses the (pre-)activations of a trained network to evaluate a fibring function computing the weights of another network whose outputs are injected back into the original network. However, the exact correspondence between fibring of neural networks and fibring of modal logics was never formally established. In this paper, we close this gap by formalizing the idea of fibred models \emph{compatible} with fibred neural networks. Using this correspondence, we then derive non-uniform logical expressiveness results for Graph Neural Networks (GNNs), Graph Attention Networks (GATs) and Transformer encoders. Longer-term, the goal of this paper is to open the way for the use of fibring as a formalism for interpreting the logical theories learnt by neural networks with the tools of computational logic. 

**Abstract (ZH)**: 模态逻辑的纤维化是一种将可数系列模态逻辑结合成具有共同语义的纤维化语言的形式主义。受此形式主义的启发，神经网络的纤维化被引入作为一种将学习和推理结合到神经网络中的神经符号框架。神经网络的纤维化利用训练网络的（预）激活来评估纤维化函数，计算另一个网络的权重并将输出注入原始网络。然而，神经网络的纤维化与模态逻辑的纤维化之间的精确对应关系从未正式建立。在本文中，我们通过形式化与纤维化神经网络相兼容的纤维化模型的概念来填补这一差距。借助这种对应关系，我们随后推导出了图神经网络（GNNs）、图注意力网络（GATs）和变换器编码器的非均匀逻辑表达能力结果。长远来看，本文的目标是为使用纤维化作为工具来解释神经网络学习的逻辑理论的计算逻辑工具铺平道路。 

---
# Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B 

**Title (ZH)**: 量化狂热、推理黑洞、薛定谔的合规性以及其他现象：探究GPT-OSS-20B 

**Authors**: Shuyi Lin, Tian Lu, Zikai Wang, Bo Wen, Yibo Zhao, Cheng Tan  

**Link**: [PDF](https://arxiv.org/pdf/2509.23882)  

**Abstract**: OpenAI's GPT-OSS family provides open-weight language models with explicit chain-of-thought (CoT) reasoning and a Harmony prompt format. We summarize an extensive security evaluation of GPT-OSS-20B that probes the model's behavior under different adversarial conditions. Us- ing the Jailbreak Oracle (JO) [1], a systematic LLM evaluation tool, the study uncovers several failure modes including quant fever, reasoning blackholes, Schrodinger's compliance, reasoning procedure mirage, and chain-oriented prompting. Experiments demonstrate how these behaviors can be exploited on GPT-OSS-20B models, leading to severe consequences. 

**Abstract (ZH)**: OpenAI的GPT-OSS家族提供了带有显式链式思考（CoT）推理和和谐提示格式的开源权重语言模型。我们总结了对GPT-OSS-20B的广泛安全评估，探究了该模型在不同 adversarial 条件下的行为。利用Jailbreak Oracle（JO）系统性的LLM评估工具，研究发现了几种失败模式，包括量化狂热、推理黑洞、薛定谔的合规性、推理过程幻象以及链式推理提示。实验展示了这些行为如何在GPT-OSS-20B模型上被利用，导致严重后果。 

---
# Rethinking Reward Miscalibration of GRPO in Agentic RL 

**Title (ZH)**: 重新审视GRPO在代理强化学习中奖励失 cal 校准问题 

**Authors**: Jingyu Liu, Xiaopeng Wu, Jingquan Peng, Kehan Chen, Chuan Yu, Lizhong Ding, Yong Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23870)  

**Abstract**: Building autonomous agents capable of solving long-horizon, real-world tasks has garnered significant research interest. But outcome based rewards may cause reward miscalibration which means it might mistakenly allocate positive reward to flawed middle steps which is regarded as the key reason making the bad actions being reinforced during training. However we reveal that outcome based reward ensures expected negative advantage for those flawed middle steps, which means the flawed actions should be punished during training. Even accounting for the ``squeezing effect", the probability mass of good actions should increase and the actor should gradually get rid of harmful actions. This shows that flawed actions should be punished during training. We further identify gradient coupling between similar samples as a key issue in agentic RL, the input prompt is extremely similar and the output action space is limited, therefore during training, gradients from well-performing samples can inadvertently strengthen suboptimal or incorrect actions due to similar input observation and output actions. We show that with gradient coupling, some flawed actions might be enhanced. To address this, we propose training the actor to classify good or bad actions to separate the embedding of good/bad actions and alleviate the gradient interference, extensive experiments shows its effectiveness. 

**Abstract (ZH)**: 构建能够解决长期任务的自主代理引起了广泛的研究兴趣。但基于结果的奖励可能导致奖励校准失误，这意味着它可能会错误地将积极的奖励分配给有缺陷的中间步骤，这些步骤在训练过程中被认为是不良行为被强化的关键原因。然而，我们发现基于结果的奖励确保了对有缺陷中间步骤的预期负优势，这意味着在训练过程中应该惩罚不良行为。即使考虑“挤压效应”，优质行为的概率质量应增加，且代理应逐渐摆脱有害行为。这表明在训练过程中应该惩罚有缺陷的行为。我们进一步认为，在代理强化学习中，梯度耦合相似样本是一个关键问题，输入提示非常相似且输出行为空间受限，因此在训练过程中，来自表现良好的样本的梯度可能会无意中加强次优或错误的行为，由于输入观察和输出行为的相似性。我们证明了梯度耦合可能会增强某些有缺陷的行为。为此，我们提出训练代理以分类优质或不良行为从而分离优质/不良行为的嵌入并减轻梯度干扰，大量实验显示其有效性。 

---
# AgentGuard: Runtime Verification of AI Agents 

**Title (ZH)**: AgentGuard：AI代理的运行时验证 

**Authors**: Roham Koohestani  

**Link**: [PDF](https://arxiv.org/pdf/2509.23864)  

**Abstract**: The rapid evolution to autonomous, agentic AI systems introduces significant risks due to their inherent unpredictability and emergent behaviors; this also renders traditional verification methods inadequate and necessitates a shift towards probabilistic guarantees where the question is no longer if a system will fail, but the probability of its failure within given constraints. This paper presents AgentGuard, a framework for runtime verification of Agentic AI systems that provides continuous, quantitative assurance through a new paradigm called Dynamic Probabilistic Assurance. AgentGuard operates as an inspection layer that observes an agent's raw I/O and abstracts it into formal events corresponding to transitions in a state model. It then uses online learning to dynamically build and update a Markov Decision Process (MDP) that formally models the agent's emergent behavior. Using probabilistic model checking, the framework then verifies quantitative properties in real-time. 

**Abstract (ZH)**: 自主智能体AI系统的快速进化引入了由于其固有的不可预测性和 emergent 行为而带来的显著风险；这使得传统的验证方法变得不足，需要转向基于概率保证的方法，其中的问题不再是系统是否会失败，而是系统在给定约束下的失败概率。本文提出了AgentGuard框架，这是一种用于运行时验证智能体AI系统的框架，通过一种新提出的动态概率保证 paradigmn 提供连续的定量保障。AgentGuard作为一种检查层，观察智能体的原始输入/输出，并将其抽象为与状态模型转换对应的正式事件。然后，它使用在线学习来动态构建和更新马尔可夫决策过程（MDP），以正式建模智能体的 emergent 行为。利用概率模型检测，该框架在实时情况下验证定量属性。 

---
# Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules 

**Title (ZH)**: Mix-Ecom: 向混合类型电子商务对话规则复杂域规则的方向发展 

**Authors**: Chenyu Zhou, Xiaoming Shi, Hui Qiu, Xiawu Zheng, Haitao Leng, Yankai Jiang, Shaoguo Liu, Tingting Gao, Rongrong Ji  

**Link**: [PDF](https://arxiv.org/pdf/2509.23836)  

**Abstract**: E-commerce agents contribute greatly to helping users complete their e-commerce needs. To promote further research and application of e-commerce agents, benchmarking frameworks are introduced for evaluating LLM agents in the e-commerce domain. Despite the progress, current benchmarks lack evaluating agents' capability to handle mixed-type e-commerce dialogue and complex domain rules. To address the issue, this work first introduces a novel corpus, termed Mix-ECom, which is constructed based on real-world customer-service dialogues with post-processing to remove user privacy and add CoT process. Specifically, Mix-ECom contains 4,799 samples with multiply dialogue types in each e-commerce dialogue, covering four dialogue types (QA, recommendation, task-oriented dialogue, and chit-chat), three e-commerce task types (pre-sales, logistics, after-sales), and 82 e-commerce rules. Furthermore, this work build baselines on Mix-Ecom and propose a dynamic framework to further improve the performance. Results show that current e-commerce agents lack sufficient capabilities to handle e-commerce dialogues, due to the hallucination cased by complex domain rules. The dataset will be publicly available. 

**Abstract (ZH)**: 电子商务代理在帮助用户完成电子商务需求方面发挥了巨大作用。为了促进电子商务代理的进一步研究和应用，介绍了基于电子商务领域的大型语言模型代理的基准框架以进行评估。尽管已经取得了进展，当前的基准测试工具缺乏对代理处理混合类型电子商务对话和复杂领域规则的能力进行评估。为解决这一问题，本文首先引入了一个新的语料库，称为Mix-ECom，该语料库基于真实的客户服务对话构建，并经过后处理以移除用户隐私并添加逐步思考过程。具体而言，Mix-ECom 包含4,799个样本，每个电子商务对话中包含多种对话类型，涵盖四种对话类型（问答、推荐、任务导向对话和闲聊）、三种电子商务任务类型（销售前、物流、销售后）和82条电子商务规则。此外，本文在Mix-ECom上构建了基线并提出了一个动态框架以进一步提高性能。结果表明，当前的电子商务代理在处理电子商务对话方面缺乏足够的能力，这主要是由于复杂领域规则引起的幻觉。该数据集将公开发布。 

---
# AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education through Automated Question Generation and Interactive Assessment 

**Title (ZH)**: AnveshanaAI：一种通过自动化问题生成和互动评估实现自适应AI/ML教育的多模态平台 

**Authors**: Rakesh Thakur, Diksha Khandelwal, Shreya Tiwari  

**Link**: [PDF](https://arxiv.org/pdf/2509.23811)  

**Abstract**: We propose AnveshanaAI, an application-based learning platform for artificial intelligence. With AnveshanaAI, learners are presented with a personalized dashboard featuring streaks, levels, badges, and structured navigation across domains such as data science, machine learning, deep learning, transformers, generative AI, large language models, and multimodal AI, with scope to include more in the future. The platform incorporates gamified tracking with points and achievements to enhance engagement and learning, while switching between Playground, Challenges, Simulator, Dashboard, and Community supports exploration and collaboration. Unlike static question repositories used in existing platforms, AnveshanaAI ensures balanced learning progression through a dataset grounded in Bloom's taxonomy, with semantic similarity checks and explainable AI techniques improving transparency and reliability. Adaptive, automated, and domain-aware assessment methods are also employed. Experiments demonstrate broad dataset coverage, stable fine-tuning with reduced perplexity, and measurable gains in learner engagement. Together, these features illustrate how AnveshanaAI integrates adaptivity, gamification, interactivity, and explainability to support next-generation AI education. 

**Abstract (ZH)**: 我们提出AnveshanaAI，一个基于应用的人工智能学习平台。AnveshanaAI为学习者提供了个性化仪表盘，展示连续学习记录、层级、徽章，并跨数据科学、机器学习、深度学习、变换器、生成人工智能、大规模语言模型以及多模态人工智能领域提供了结构化的导航，未来还将包括更多领域。该平台集成了游戏化的跟踪机制，并通过点数和成就提高参与度和学习效果；学习者可在游乐场、挑战、模拟器、仪表盘和社区之间切换，促进探索与合作。与现有平台使用的静态问题库不同，AnveshanaAI通过基于Bloom taxonomy的数据集确保平衡的学习进展，并通过语义相似性检查和可解释的人工智能技术提高透明度和可靠性。该平台还采用了自适应、自动化和领域感知的评估方法。实验结果表明，AnveshanaAI实现了广泛的数据集覆盖、稳定的数据集微调和困惑度降低，并在学习者参与度方面取得可测量的提升。这些功能共同展示了AnveshanaAI如何通过自适应、游戏化、互动性和可解释性来支持下一代人工智能教育。 

---
# From Frustration to Fun: An Adaptive Problem-Solving Puzzle Game Powered by Genetic Algorithm 

**Title (ZH)**: 从挫折到乐趣：一种基于遗传算法的自适应问题解决益智游戏 

**Authors**: Matthew McConnell, Richard Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2509.23796)  

**Abstract**: This paper explores adaptive problem solving with a game designed to support the development of problem-solving skills. Using an adaptive, AI-powered puzzle game, our adaptive problem-solving system dynamically generates pathfinding-based puzzles using a genetic algorithm, tailoring the difficulty of each puzzle to individual players in an online real-time approach. A player-modeling system records user interactions and informs the generation of puzzles to approximate a target difficulty level based on various metrics of the player. By combining procedural content generation with online adaptive difficulty adjustment, the system aims to maintain engagement, mitigate frustration, and maintain an optimal level of challenge. A pilot user study investigates the effectiveness of this approach, comparing different types of adaptive difficulty systems and interpreting players' responses. This work lays the foundation for further research into emotionally informed player models, advanced AI techniques for adaptivity, and broader applications beyond gaming in educational settings. 

**Abstract (ZH)**: 本文探讨了一种通过设计用于支持问题解决技能发展的游戏来进行自适应问题解决的方法。通过一个自适应的AI驱动的谜题游戏，我们的自适应问题解决系统使用遗传算法动态生成路径finding为基础的谜题，并采用在线实时方式个性化调整每个谜题的难度。玩家建模系统记录用户交互，并根据玩家的各种指标信息来调整谜题的难度以逼近目标难度水平。通过结合过程化内容生成与在线自适应难度调整，该系统旨在保持参与度、减轻挫败感，并维持适当的挑战水平。一项试点用户研究探讨了该方法的有效性，比较了不同类型自适应难度系统，并分析了玩家的反应。本文为情感化玩家模型、高级自适应AI技术以及教育等更广泛领域中的应用奠定了基础。 

---
# Falcon: A Cross-Modal Evaluation Dataset for Comprehensive Safety Perception 

**Title (ZH)**: Falcon：全面安全感知的跨模态评估数据集 

**Authors**: Qi Xue, Minrui Jiang, Runjia Zhang, Xiurui Xie, Pei Ke, Guisong Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23783)  

**Abstract**: Existing methods for evaluating the harmfulness of content generated by large language models (LLMs) have been well studied. However, approaches tailored to multimodal large language models (MLLMs) remain underdeveloped and lack depth. This work highlights the crucial role of visual information in moderating content in visual question answering (VQA), a dimension often overlooked in current research. To bridge this gap, we introduce Falcon, a large-scale vision-language safety dataset containing 57,515 VQA pairs across 13 harm categories. The dataset provides explicit annotations for harmful attributes across images, instructions, and responses, thereby facilitating a comprehensive evaluation of the content generated by MLLMs. In addition, it includes the relevant harm categories along with explanations supporting the corresponding judgments. We further propose FalconEye, a specialized evaluator fine-tuned from Qwen2.5-VL-7B using the Falcon dataset. Experimental results demonstrate that FalconEye reliably identifies harmful content in complex and safety-critical multimodal dialogue scenarios. It outperforms all other baselines in overall accuracy across our proposed Falcon-test dataset and two widely-used benchmarks-VLGuard and Beavertail-V, underscoring its potential as a practical safety auditing tool for MLLMs. 

**Abstract (ZH)**: 现有的评估大型语言模型生成内容危害性的方法已有广泛研究，但针对多模态大型语言模型的方法仍嫌不足且缺乏深度。本文强调了在视觉问答(VQA)中视觉信息在内容调节中的关键作用，这一维度在当前研究中经常被忽视。为了解决这一问题，我们介绍了Falcon，一个包含57,515个VQA对（跨越13个危害类别）的大规模视觉-语言安全数据集。该数据集为图像、指令和响应中的有害属性提供了明确注解，从而促进了对多模态大型语言模型生成内容的全面评估。此外，该数据集还包含了相应的危害类别及其支持判断的解释。我们进一步提出了FalconEye，一个基于Qwen2.5-VL-7B微调的专业评估器，使用Falcon数据集进行训练。实验结果表明，FalconEye能够可靠地识别复杂和安全性关键的多模态对话场景中的有害内容，在我们提出的Falcon-test数据集和两个广泛使用的基准VLGuard和Beavertail-V上实现了全面准确性上的优势，证明了其作为多模态大型语言模型实际安全性审计工具的潜力。 

---
# From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning 

**Title (ZH)**: 从“怎么做”到“为什么这样做”：一种基于证据的化学反应条件推理多代理系统 

**Authors**: Cheng Yang, Jiaxuan Lu, Haiyuan Wan, Junchi Yu, Feiwei Qin  

**Link**: [PDF](https://arxiv.org/pdf/2509.23768)  

**Abstract**: The chemical reaction recommendation is to select proper reaction condition parameters for chemical reactions, which is pivotal to accelerating chemical science. With the rapid development of large language models (LLMs), there is growing interest in leveraging their reasoning and planning capabilities for reaction condition recommendation. Despite their success, existing methods rarely explain the rationale behind the recommended reaction conditions, limiting their utility in high-stakes scientific workflows. In this work, we propose ChemMAS, a multi-agent system that reframes condition prediction as an evidence-based reasoning task. ChemMAS decomposes the task into mechanistic grounding, multi-channel recall, constraint-aware agentic debate, and rationale aggregation. Each decision is backed by interpretable justifications grounded in chemical knowledge and retrieved precedents. Experiments show that ChemMAS achieves 20-35% gains over domain-specific baselines and outperforms general-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable, human-trustable rationales, which establishes a new paradigm for explainable AI in scientific discovery. 

**Abstract (ZH)**: 化学反应推荐是选择合适的反应条件参数，这对于加速化学科学发展至关重要。随着大规模语言模型（LLMs）的快速发展，人们越来越有兴趣利用它们的推理和规划能力进行反应条件推荐。尽管取得了成功，现有方法很少解释推荐的反应条件背后的理由，限制了它们在高风险科学工作流程中的应用价值。在本文中，我们提出了一种多智能体系统ChemMAS，将条件预测重新构想为基于证据的推理任务。ChemMAS将任务分解为机械主义基础、多通道回忆、约束aware代理辩论和理由聚合。每个决策都基于化学知识和检索的先例提供了可解释的解释。实验结果显示，ChemMAS在特定领域基线上的性能提高了20-35%，并且在Top-1准确性上比通用的大规模语言模型高10-15%，同时提供可证伪、可信赖的人类可理解的理由，这确立了一种新的可解释AI在科学发现中的范式。 

---
# Transparent Visual Reasoning via Object-Centric Agent Collaboration 

**Title (ZH)**: 基于对象中心代理协作的透明视觉推理 

**Authors**: Benjamin Teoh, Ben Glocker, Francesca Toni, Avinash Kori  

**Link**: [PDF](https://arxiv.org/pdf/2509.23757)  

**Abstract**: A central challenge in explainable AI, particularly in the visual domain, is producing explanations grounded in human-understandable concepts. To tackle this, we introduce OCEAN (Object-Centric Explananda via Agent Negotiation), a novel, inherently interpretable framework built on object-centric representations and a transparent multi-agent reasoning process. The game-theoretic reasoning process drives agents to agree on coherent and discriminative evidence, resulting in a faithful and interpretable decision-making process. We train OCEAN end-to-end and benchmark it against standard visual classifiers and popular posthoc explanation tools like GradCAM and LIME across two diagnostic multi-object datasets. Our results demonstrate competitive performance with respect to state-of-the-art black-box models with a faithful reasoning process, which was reflected by our user study, where participants consistently rated OCEAN's explanations as more intuitive and trustworthy. 

**Abstract (ZH)**: 可解释人工智能领域的核心挑战，特别是在视觉领域，是如何生成基于人类可理解概念的解释。为应对这一挑战，我们提出了OCEAN（基于对象的解释体通过代理协商）这一新颖的、固有可解释的框架，该框架基于对象中心表示和透明的多代理推理过程。基于博弈论的推理过程促使代理们达成一致并提供连贯和区分性的证据，从而实现忠实且可解释的决策过程。我们端到端训练OCEAN，并在两个诊断的多对象数据集上将其与标准视觉分类器和流行的后 hoc 解释工具（如 GradCAM 和 LIME）进行基准测试。我们的结果显示出与最先进的黑盒模型相当的性能，并在忠实的推理过程方面得到了用户研究的支持，参与者一致认为OCEAN的解释更直观且可信。 

---
# GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks 

**Title (ZH)**: GUI-牧羊人: 可靠的长序列GUI任务过程奖励与验证 

**Authors**: Cong Chen, Kaixiang Ji, Hao Zhong, Muzhi Zhu, Anzhou Li, Guo Gan, Ziyuan Huang, Cheng Zou, Jiajia Liu, Jingdong Chen, Hao Chen, Chunhua Shen  

**Link**: [PDF](https://arxiv.org/pdf/2509.23738)  

**Abstract**: Autonomous agents for long-sequence Graphical User Interface tasks are hindered by sparse rewards and the intractable credit assignment problem. To address these challenges, we introduce GUI-Shepherd, a Process Reward Model that provides dense, step-by-step feedback to guide agents. GUI-Shepherd is trained on a diverse large-scale data set of $52$k interactions that features human-annotated scores and GPT-4o generated rationales, enabling it to serve both as a reward provider for RL training and as a verifier for inference. As far as we know, we are the first to conduct a systematic study of process supervision in GUI agents, across diverse settings from online long-horizon tasks to offline single-step prediction. On the online AndroidWorld benchmark, GUI-Shepherd improves success rate by $7.7$ points via multi-turn online PPO, significantly outperforming Outcome Reward Model based competitors. When used as an inference verifier, it brings $5.1$ points improvements. The benefits generalize to the offline AndroidControl benchmark, with gains of $2.2$ points as a reward provider and $4.3$ points as a verifier. Collectively, our results establish that high-fidelity process supervision is critical for building more capable GUI agents and present a generalizable solution. 

**Abstract (ZH)**: 自主代理在长时间序列图形用户界面任务中受到稀疏奖励和归因难题的阻碍。为解决这些挑战，我们引入了GUI-Shepherd，一种过程奖励模型，能够提供详细逐步反馈以指导代理。GUI-Shepherd基于包含人类标注评分和GPT-4o生成的解释的大规模多样数据集进行训练，使其既能作为强化学习训练的奖励提供者，又能作为推理的验证器。据我们所知，这是首次系统研究GUI代理的过程监督，从在线长时间任务到离线单步预测。在在线AndroidWorld基准测试中，通过多轮在线PPO，GUI-Shepherd将成功率提高了7.7个百分点，显著优于基于结果奖励模型的竞争者。作为推理验证器时，它带来了5.1个百分点的提高。这些好处在离线AndroidControl基准测试中也得到验证，作为奖励提供者时提高了2.2个百分点，作为验证器时提高了4.3个百分点。总体而言，我们的研究结果表明，高保真过程监督对于构建更强大的GUI代理至关重要，并提出了一种可泛化的解决方案。 

---
# Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems: Dataset, Taxonomy, and Benchmark 

**Title (ZH)**: 平台 orchestration 执行体系统中失败根本原因诊断：数据集、分类学和基准 

**Authors**: Xuyan Ma, Xiaofei Xie, Yawen Wang, Junjie Wang, Boyu Wu, Mingyang Li, Qing Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23735)  

**Abstract**: Agentic systems consisting of multiple LLM-driven agents coordinating through tools and structured interactions, are increasingly deployed for complex reasoning and problem-solving tasks. At the same time, emerging low-code and template-based agent development platforms (e.g., Dify) enable users to rapidly build and orchestrate agentic systems, which we refer to as platform-orchestrated agentic systems. However, these systems are also fragile and it remains unclear how to systematically identify their potential failure root cause. This paper presents a study of root cause identification of these platform-orchestrated agentic systems. To support this initiative, we construct a dataset AgentFail containing 307 failure logs from ten agentic systems, each with fine-grained annotations linking failures to their root causes. We additionally utilize counterfactual reasoning-based repair strategy to ensure the reliability of the annotation. Building on the dataset, we develop a taxonomy that characterizes failure root causes and analyze their distribution across different platforms and task domains. Furthermore, we introduce a benchmark that leverages LLMs for automatically identifying root causes, in which we also utilize the proposed taxonomy as guidance for LLMs. Results show that the taxonomy can largely improve the performance, thereby confirming its utility. Nevertheless, the accuracy of root cause identification reaches at most 33.6%, which indicates that this task still remains challenging. In light of these results, we also provide actionable guidelines for building such agentic systems. In summary, this paper provides a reliable dataset of failure root cause for platform-orchestrated agentic systems, corresponding taxonomy and benchmark, which serves as a foundation for advancing the development of more reliable agentic systems. 

**Abstract (ZH)**: 平台 orchestration 的代理系统根因识别研究 

---
# EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance 

**Title (ZH)**: EAPO: 以需求为导向的专家协助增强策略优化 

**Authors**: Siyao Song, Cong Ma, Zhihao Cheng, Shiye Lei, Minghao Li, Ying Zeng, Huaixiao Tou, Kai Jia  

**Link**: [PDF](https://arxiv.org/pdf/2509.23730)  

**Abstract**: Large language models (LLMs) have recently advanced in reasoning when optimized with reinforcement learning (RL) under verifiable rewards. Existing methods primarily rely on outcome-based supervision to strengthen internal LLM reasoning, often leading to inefficient exploration and sparse rewards. To mitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a novel RL framework that enhances exploration by incorporating multi-turn interactions with external experts during training. Unlike prior methods, where policies reason in isolation, EAPO incentivizes the policy to adaptively determine when and how to consult experts, yielding richer reward signals and more reliable reasoning trajectories. External assistance ultimately internalizes expert knowledge into the policy model, amplifying the model's inherent reasoning capabilities. During evaluation, the policy model has been well-optimized to solve questions independently, producing improved reasoning paths and more accurate solutions. Experiments on mathematical reasoning benchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO consistently outperforms expert-assisted workflow, expert-distilled models, and RL baselines, with an average gain of 5 points over self-exploratory models. 

**Abstract (ZH)**: 大型语言模型在验证奖励下的强化学习优化中推理能力的进步：专家辅助策略优化(EAPO)方法的研究 

---
# MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models 

**Title (ZH)**: MedLA：一种逻辑驱动的多agent框架，用于大型语言模型在复杂医疗推理中的应用 

**Authors**: Siqi Ma, Jiajie Huang, Bolin Yang, Fan Zhang, Jinlin Wu, Yue Shen, Guohui Fan, Zhu Zhang, Zelin Zang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23725)  

**Abstract**: Answering complex medical questions requires not only domain expertise and patient-specific information, but also structured and multi-perspective reasoning. Existing multi-agent approaches often rely on fixed roles or shallow interaction prompts, limiting their ability to detect and resolve fine-grained logical inconsistencies. To address this, we propose \textsc{MedLA}, a logic-driven multi-agent framework built on large language models. Each agent organizes its reasoning process into an explicit logical tree based on syllogistic triads (major premise, minor premise, and conclusion), enabling transparent inference and premise-level alignment. Agents engage in a multi-round, graph-guided discussion to compare and iteratively refine their logic trees, achieving consensus through error correction and contradiction resolution. We demonstrate that \textsc{MedLA} consistently outperforms both static role-based systems and single-agent baselines on challenging benchmarks such as MedDDx and standard medical QA tasks. Furthermore, \textsc{MedLA} scales effectively across both open-source and commercial LLM backbones, achieving state-of-the-art performance and offering a generalizable paradigm for trustworthy medical reasoning. 

**Abstract (ZH)**: 逻辑驱动的多智能体框架\textsc{MedLA}：应对复杂医疗问题需要专业知识、患者特定信息以及结构化的多视角推理 

---
# Measuring Sparse Autoencoder Feature Sensitivity 

**Title (ZH)**: 测量稀疏自编码器特征敏感性 

**Authors**: Claire Tian, Katherine Tian, Nathan Hu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23717)  

**Abstract**: Sparse Autoencoder (SAE) features have become essential tools for mechanistic interpretability research. SAE features are typically characterized by examining their activating examples, which are often "monosemantic" and align with human interpretable concepts. However, these examples don't reveal feature sensitivity: how reliably a feature activates on texts similar to its activating examples. In this work, we develop a scalable method to evaluate feature sensitivity. Our approach avoids the need to generate natural language descriptions for features; instead we use language models to generate text with the same semantic properties as a feature's activating examples. We then test whether the feature activates on these generated texts. We demonstrate that sensitivity measures a new facet of feature quality and find that many interpretable features have poor sensitivity. Human evaluation confirms that when features fail to activate on our generated text, that text genuinely resembles the original activating examples. Lastly, we study feature sensitivity at the SAE level and observe that average feature sensitivity declines with increasing SAE width across 7 SAE variants. Our work establishes feature sensitivity as a new dimension for evaluating both individual features and SAE architectures. 

**Abstract (ZH)**: 稀疏自编码器（SAE）特征已成为机制解释性研究中不可或缺的工具。本研究开发了一种可扩展的方法来评估特征敏感性。我们的方法避免为特征生成自然语言描述，而是使用语言模型生成与特征激活示例具有相同语义属性的文本，然后测试特征是否能够在这些生成的文本上激活。我们展示敏感性衡量了特征质量的一个新的方面，并发现许多可解释的特征在敏感性方面表现较差。人类评估显示，当特征未能在生成的文本上激活时，这些文本确实类似于原始的激活示例。最后，我们在稀疏自编码器（SAE）层次上研究了特征敏感性，发现在7种不同宽度的SAE变体中，平均特征敏感性随着SAE宽度增加而下降。我们的研究确立了特征敏感性作为评估单个特征和SAE架构的新维度。 

---
# SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents 

**Title (ZH)**: SafeSearch：基于自动化红队演练的LLM搜索代理安全性评估 

**Authors**: Jianshuo Dong, Sheng Guo, Hao Wang, Zhuotao Liu, Tianwei Zhang, Ke Xu, Minlie Huang, Han Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23694)  

**Abstract**: Search agents connect LLMs to the Internet, enabling access to broader and more up-to-date information. However, unreliable search results may also pose safety threats to end users, establishing a new threat surface. In this work, we conduct two in-the-wild experiments to demonstrate both the prevalence of low-quality search results and their potential to misguide agent behaviors. To counter this threat, we introduce an automated red-teaming framework that is systematic, scalable, and cost-efficient, enabling lightweight and harmless safety assessments of search agents. Building on this framework, we construct the SafeSearch benchmark, which includes 300 test cases covering five categories of risks (e.g., misinformation and indirect prompt injection). Using this benchmark, we evaluate three representative search agent scaffolds, covering search workflow, tool-calling, and deep research, across 7 proprietary and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities of LLM-based search agents: when exposed to unreliable websites, the highest ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover, our analysis highlights the limited effectiveness of common defense practices, such as reminder prompting. This emphasizes the value of our framework in promoting transparency for safer agent development. Our codebase and test cases are publicly available: this https URL. 

**Abstract (ZH)**: 搜索代理将LLM连接到互联网，使其能够访问更广泛和更及时的信息。然而，不可靠的搜索结果也可能对终端用户构成安全威胁，形成新的威胁面。在此工作中，我们进行了两项实地实验，以证明低质量搜索结果的普遍性及其误导搜索代理行为的潜在风险。为了应对这一威胁，我们引入了一种系统、可扩展且成本效益高的自动化红队框架，能够对搜索代理进行轻量级且无害的安全评估。基于此框架，我们构建了SafeSearch基准，其中包括300个测试案例，覆盖五类风险（例如， misinformation和间接提示注入）。使用该基准，我们评估了三种代表性的搜索代理框架，覆盖搜索工作流、工具调用和深入研究，分别在7个私有和8个开源后端LLM上进行了评估。我们的结果揭示了基于LLM的搜索代理的主要漏洞：在搜索工作流设置中，GPT-4.1-mini的最大自动完成率达到了90.5%。此外，我们的分析还强调了常用防御措施（如提醒提示）的有效性有限，进一步突显了我们框架在促进更安全代理开发透明度方面的价值。我们的代码库和测试案例已公开：this https URL。 

---
# From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models 

**Title (ZH)**: 从推理到答案：关于Distilled DeepSeek R1模型的经验、注意力机制和机理洞察 

**Authors**: Jue Zhang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23676)  

**Abstract**: Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \href{this https URL}{this URL}. 

**Abstract (ZH)**: 大型推理模型（LRMs）生成显式的推理痕迹以及最终答案，但这些痕迹对答案生成的影响程度尚不明确。在本文中，我们对三个经过提炼的DeepSeek R1模型中的推理与答案生成的交互作用进行了三阶段的研究。首先，通过实证评估，我们证明了包含显式推理可以一致地提高跨不同领域答案的质量。其次，注意力分析显示答案标记会显著地关注推理标记，某些中间层的推理聚焦头（RFHs）紧密跟踪推理轨迹，包括自我反思线索。第三，我们通过激活修补的方法应用机理干预，评估答案标记对推理激活的依赖性。我们的结果表明，对关键推理标记的扰动可以可靠地改变最终答案，证实了从推理到答案的信息传递是具有方向性和功能性的。这些发现加深了我们对LRMs如何利用推理标记进行答案生成的理解，突出了中间推理的功能性作用以塑造模型输出。我们的数据和代码在\href{this https URL}{this URL}公开可用。 

---
# Game-Oriented ASR Error Correction via RAG-Enhanced LLM 

**Title (ZH)**: 面向游戏的ASR错误校正：RAG增强的大语言模型方法 

**Authors**: Yan Jiang, Yongle Luo, Qixian Zhou, Elvis S. Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23630)  

**Abstract**: With the rise of multiplayer online games, real-time voice communication is essential for team coordination. However, general ASR systems struggle with gaming-specific challenges like short phrases, rapid speech, jargon, and noise, leading to frequent errors. To address this, we propose the GO-AEC framework, which integrates large language models, Retrieval-Augmented Generation (RAG), and a data augmentation strategy using LLMs and TTS. GO-AEC includes data augmentation, N-best hypothesis-based correction, and a dynamic game knowledge base. Experiments show GO-AEC reduces character error rate by 6.22% and sentence error rate by 29.71%, significantly improving ASR accuracy in gaming scenarios. 

**Abstract (ZH)**: 随着多人在线游戏的兴起，实时语音通信对于团队协调至关重要。然而，一般的语音识别（ASR）系统难以应对游戏特有的挑战，如短语、快速 speech、行业术语和噪音，导致频繁出错。为了解决这一问题，我们提出了一种GO-AEC框架，该框架整合了大规模语言模型、检索增强生成（RAG）以及使用大语言模型（LLM）和文本转语音（TTS）的数据增强策略。GO-AEC包括数据增强、基于N-best假设的纠错以及动态游戏知识库。实验结果显示，GO-AEC将字符错误率降低了6.22%，句子错误率降低了29.71%，显著提高了游戏场景下的ASR准确性。 

---
# How LLMs Learn to Reason: A Complex Network Perspective 

**Title (ZH)**: LLMs如何推理：一种复杂网络视角 

**Authors**: Sihan Hu, Xiansheng Cai, Yuan Huang, Zhiyuan Yao, Linfeng Zhang, Pan Zhang, Youjin Deng, Kun Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.23629)  

**Abstract**: Training large language models with Reinforcement Learning from Verifiable Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain poorly understood, including a two-stage learning curve, V-shaped response-length trajectories, and a pronounced vulnerability to catastrophic forgetting. In this work, we propose that these seemingly disparate phenomena can be explained using a single unifying theory: the model's reasoning process maps to the self-organization of a semantic complex network whose topology remains persistently sparse, with the average degree pinned close to two. This topology imposes a fundamental mechanism for forgetting and learning: it first drives the system into a maximally frustrated state where ``skill islands'' form, slow-learning happens, and forgetting is induced; then it enters a sharp growth phase where the new skills are ``bolted on'', driven by phase-transition-like learning at the web's frontier. Equipped with the theory, we propose \textit{Annealed-RLVR}, a principled algorithm that introduces an SFT-based ``heating'' step at the point of maximal frustration to resolve the competitive bottleneck and enhance the reasoning capability of the model. Experiments on a 1.5B-parameter model demonstrate that the approach outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks. By recasting RLVR from black-box optimization into a predictable process of structural self-organization, our work provides a new physical intuition for engineering the emergent reasoning capabilities of future AI systems. 

**Abstract (ZH)**: 使用可验证奖励的强化学习训练大型语言模型（RLVR）表现出一系列distinctive和puzzling的行为，这些行为尚未充分理解，包括两阶段学习曲线、V型响应长度轨迹以及明显的灾难性遗忘易感性。在这项工作中，我们提出这些看似不相关的现象可以由单一的统一理论解释：模型的推理过程映射到语义复杂网络的自我组织，其拓扑结构持久地保持稀疏，平均度接近2。这种拓扑结构施加了一种基本的遗忘和学习机制：首先将系统驱动到最大化挫败状态，形成“技能岛屿”，学习缓慢，导致遗忘；然后进入一个快速增长阶段，在这个阶段，新技能由类似相变的前端学习驱动而“附加”上。借助该理论，我们提出了一种原理性的算法——Annealed-RLVR，在最大挫败状态引入基于SFT的“加热”步骤以解决竞争瓶颈并增强模型的推理能力。对一个1.5亿参数模型的实验表明，该方法在分布内和分布外基准上均优于标准RLVR。通过将RLVR从黑盒优化重新构想为可预测的结构自我组织过程，我们的工作为工程未来AI系统的涌现推理能力提供了新的物理直觉。 

---
# Reasoning Scaffolding: Distilling the Flow of Thought from LLMs 

**Title (ZH)**: 思维支撑：从大模型中提炼思维流程 

**Authors**: Xiangyu Wen, Junhua Huang, Zeju Li, Min Li, Jianyuan Zhong, Zhijian Xu, Mingxuan Yuan, Yongxiang Huang, Qiang Xu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23619)  

**Abstract**: The prevailing approach to distilling reasoning from Large Language Models (LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It teaches Small Language Models (SLMs) to mimic surface-level patterns rather than the underlying algorithmic structure of thought, resulting in a critical lack of logical robustness. We argue that instead of cloning text, distillation should transfer this algorithmic structure directly. We introduce Reasoning Scaffolding}, a framework that reframes reasoning as a structured generation process. Our method first abstracts the teacher's thought process into a sequence of discrete, interpretable semantic signals (e.g., Contrast, Addition) that act as a scaffold. The student model is then trained via a multi-task objective to both (1)predict the next semantic signal, anticipating the reasoning flow, and (2)generate the corresponding step, conditioned on that signal. This multi-task scheme acts as a powerful regularizer, compelling the student to internalize the computational patterns of coherent reasoning. On a suite of challenging reasoning benchmarks, our method significantly outperforms state-of-the-art distillation in both accuracy and logical consistency, providing a path towards creating smaller models that are genuine reasoners, not just fluent mimics. 

**Abstract (ZH)**: 从大型语言模型中提炼推理的盛行方法——基于文本理由的行为克隆——从根本上是有限的。我们提出，蒸馏应该直接转移这种算法结构，而不是克隆文本。我们介绍了Reasoning Scaffolding框架，将其推理重新定义为结构化的生成过程。我们的方法首先将教师的思维过程抽象为一系列可解释的语义信号（例如，对比、增加），作为支架。然后，学生模型通过多任务目标训练，不仅要（1）预测下一个语义信号，预见推理流程，还要（2）在该信号的条件下生成相应的步骤。这种多任务方案作为一种强大的正则化器，促使学生内化连贯推理的计算模式。在一系列具有挑战性的推理基准测试中，我们的方法在准确性和逻辑一致性方面显著优于现有的蒸馏技术，提供了一条创造真正推理者而非只是流利模仿者的较小模型的道路。 

---
# PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents 

**Title (ZH)**: PSG-Agent：面向人格的安全防护准则 

**Authors**: Yaozu Wu, Jizhou Guo, Dongyuan Li, Henry Peng Zou, Wei-Chieh Huang, Yankai Chen, Zhen Wang, Weizhi Zhang, Yangning Li, Meng Zhang, Renhe Jiang, Philip S. Yu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23614)  

**Abstract**: Effective guardrails are essential for safely deploying LLM-based agents in critical applications. Despite recent advances, existing guardrails suffer from two fundamental limitations: (i) they apply uniform guardrail policies to all users, ignoring that the same agent behavior can harm some users while being safe for others; (ii) they check each response in isolation, missing how risks evolve and accumulate across multiple interactions. To solve these issues, we propose PSG-Agent, a personalized and dynamic system for LLM-based agents. First, PSG-Agent creates personalized guardrails by mining the interaction history for stable traits and capturing real-time states from current queries, generating user-specific risk thresholds and protection strategies. Second, PSG-Agent implements continuous monitoring across the agent pipeline with specialized guards, including Plan Monitor, Tool Firewall, Response Guard, Memory Guardian, that track cross-turn risk accumulation and issue verifiable verdicts. Finally, we validate PSG-Agent in multiple scenarios including healthcare, finance, and daily life automation scenarios with diverse user profiles. It significantly outperform existing agent guardrails including LlamaGuard3 and AGrail, providing an executable and auditable path toward personalized safety for LLM-based agents. 

**Abstract (ZH)**: 有效的边界措施对于在关键应用中安全部署基于LLM的代理至关重要。尽管近期取得了进展，现有的边界措施仍存在两个根本性的局限性：（i）它们对所有用户应用统一的边界措施政策，忽略了相同代理行为对不同用户可能造成的不同影响；（ii）它们孤立地检查每个响应，未能捕捉风险在多次交互中的演变和累积。为了克服这些问题，我们提出PSG-Agent，一种个性化的动态系统。首先，PSG-Agent通过挖掘交互历史记录以发现稳定特征，并从当前查询中捕获实时状态，生成用户特定的风险阈值和保护策略。其次，PSG-Agent在代理管道中实施持续监控，包括计划监控、工具防火墙、响应守护和记忆守护，以跟踪跨回合风险积累并发出可验证的判断。最后，我们在医疗保健、金融和日常生活自动化等多个场景中对PSG-Agent进行了验证，涵盖了不同的用户配置文件。它在多个方面显著优于现有的代理边界措施，包括LlamaGuard3和AGrail，为基于LLM的代理提供了一条可执行和可审计的个性化安全路径。 

---
# BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving 

**Title (ZH)**: BridgeDrive: 闭路轨迹规划的扩散桥策略 

**Authors**: Shu Liu, Wenlin Chen, Weihao Li, Zheng Wang, Lijin Yang, Jianing Huang, Yipin Zhang, Zhongzhan Huang, Ze Cheng, Hao Yang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23589)  

**Abstract**: Diffusion-based planners have shown great promise for autonomous driving due to their ability to capture multi-modal driving behaviors. However, guiding these models effectively in reactive, closed-loop environments remains a significant challenge. Simple conditioning often fails to provide sufficient guidance in complex and dynamic driving scenarios. Recent work attempts to use typical expert driving behaviors (i.e., anchors) to guide diffusion models but relies on a truncated schedule, which introduces theoretical inconsistencies and can compromise performance. To address this, we introduce BridgeDrive, a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning. Our approach provides a principled diffusion framework that effectively translates anchors into fine-grained trajectory plans, appropriately responding to varying traffic conditions. Our planner is compatible with efficient ODE solvers, a critical factor for real-time autonomous driving deployment. We achieve state-of-the-art performance on the Bench2Drive benchmark, improving the success rate by 5% over prior arts. 

**Abstract (ZH)**: 基于扩散的规划器在自主驾驶中显示出巨大的潜力，因为它们能够捕捉多模态驾驶行为。然而，在反应性和闭环环境中有效地引导这些模型仍然是一个重大挑战。简单的条件化往往在复杂和动态的驾驶场景中不能提供足够的指导。最近的工作尝试使用典型的专家驾驶行为（即锚点）来引导扩散模型，但依赖于截断的时间表，这引入了理论不一致性并可能损害性能。为了解决这个问题，我们提出了BridgeDrive，一种新颖的锚点引导扩散桥梁策略，用于闭环轨迹规划。我们的方法提供了一个原则上的扩散框架，有效地将锚点转化为细粒度的轨迹计划，并适当地响应不断变化的交通条件。我们的规划器与高效的ODE求解器兼容，这是实时自主驾驶部署的关键因素。我们在Bench2Drive基准测试中实现了最先进的性能，将成功率达到比先前方法高出5%。 

---
# Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment 

**Title (ZH)**: 先清洁，后对齐：可靠的大语言模型对齐的偏好数据清洁基准测试 

**Authors**: Min-Hsuan Yeh, Yixuan Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.23564)  

**Abstract**: Human feedback plays a pivotal role in aligning large language models (LLMs) with human preferences. However, such feedback is often noisy or inconsistent, which can degrade the quality of reward models and hinder alignment. While various automated data cleaning methods have been proposed to mitigate this issue, a systematic evaluation of their effectiveness and generalizability remains lacking. To bridge this gap, we introduce the first comprehensive benchmark for evaluating 13 preference data cleaning methods in the context of LLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning strategies in terms of alignment performance and generalizability across diverse datasets, model architectures, and optimization algorithms. By unifying disparate methods and rigorously comparing them, we uncover key factors that determine the success of data cleaning in alignment tasks. This benchmark lays the groundwork for principled and reproducible approaches to improving LLM alignment through better data quality-highlighting the crucial but underexplored role of data preprocessing in responsible AI development. We release modular implementations of all methods to catalyze further research: this https URL. 

**Abstract (ZH)**: 人类反馈在使大型语言模型与人类偏好保持一致中发挥着关键作用。然而，此类反馈往往嘈杂或不一致，这会降低奖励模型的质量并阻碍一致性的提升。尽管已经提出了各种自动数据清洗方法来缓解这一问题，但对其有效性和通用性的系统评估仍然不足。为解决这一问题，我们首次推出了一个全面的基准，用于在大型语言模型对齐的背景下评估13种偏好数据清洗方法。PrefCleanBench 提供了一种标准化的评估协议，用于根据对齐性能和跨多种数据集、模型架构和优化算法的一般性评估清洗策略。通过统一不同的方法并严格比较它们，我们发现了决定数据清洗在对齐任务中成功的关键因素。该基准为通过提高数据质量改进大型语言模型对齐的原理性和可重复方法奠定了基础——突显了数据预处理在负责任AI发展中至关重要的但未被充分探索的作用。我们发布了所有方法的模块化实现，以促进进一步的研究：[此链接](此httpsURL)。 

---
# A Hierarchical Structure-Enhanced Personalized Recommendation Model for Traditional Chinese Medicine Formulas Based on KG Diffusion Guidance 

**Title (ZH)**: 基于KG扩散指导的层次结构增强中医药方个性化推荐模型 

**Authors**: ChaoBo Zhang, Long Tan  

**Link**: [PDF](https://arxiv.org/pdf/2509.23560)  

**Abstract**: Artificial intelligence technology plays a crucial role in recommending prescriptions for traditional Chinese medicine (TCM). Previous studies have made significant progress by focusing on the symptom-herb relationship in prescriptions. However, several limitations hinder model performance: (i) Insufficient attention to patient-personalized information such as age, BMI, and medical history, which hampers accurate identification of syndrome and reduces efficacy. (ii) The typical long-tailed distribution of herb data introduces training biases and affects generalization ability. (iii) The oversight of the 'monarch, minister, assistant and envoy' compatibility among herbs increases the risk of toxicity or side effects, opposing the 'treatment based on syndrome differentiation' principle in clinical TCM. Therefore, we propose a novel hierarchical structure-enhanced personalized recommendation model for TCM formulas based on knowledge graph diffusion guidance, namely TCM-HEDPR. Specifically, we pre-train symptom representations using patient-personalized prompt sequences and apply prompt-oriented contrastive learning for data augmentation. Furthermore, we employ a KG-guided homogeneous graph diffusion method integrated with a self-attention mechanism to globally capture the non-linear symptom-herb relationship. Lastly, we design a heterogeneous graph hierarchical network to integrate herbal dispensing relationships with implicit syndromes, guiding the prescription generation process at a fine-grained level and mitigating the long-tailed herb data distribution problem. Extensive experiments on two public datasets and one clinical dataset demonstrate the effectiveness of TCM-HEDPR. In addition, we incorporate insights from modern medicine and network pharmacology to evaluate the recommended prescriptions comprehensively. It can provide a new paradigm for the recommendation of modern TCM. 

**Abstract (ZH)**: 人工智能技术在推荐中医药方中的作用至关重要。尽管以往研究集中在处方中的症状-药关系上取得了显著进展，但模型性能仍受到多重限制：（i）缺乏对年龄、BMI和病史等患者个性化信息的关注，影响了病机识别的准确性并降低了疗效。（ii）草药数据的典型长尾分布引入了训练偏差，影响了泛化能力。（iii）忽视了草药间的‘君、臣、佐、使’配伍关系，增加了毒副作用的风险，违背了临床中医药‘辨证施治’的原则。因此，我们提出了一种基于知识图谱扩散指导的新型层次结构增强个性化推荐模型，即TCM-HEDPR。具体而言，我们使用患者个性化提示序列提前训练症状表示，并应用提示导向的对比学习进行数据增强。此外，我们采用知识图谱引导的同质图扩散方法结合自我注意机制，全局捕捉非线性的症状-药关系。最后，我们设计了一种异质图层次网络，将中药配伍关系与隐含病机相结合，精细指导处方生成过程，并缓解长尾草药数据分布问题。在两个公开数据集和一个临床数据集上的广泛实验表明了TCM-HEDPR的有效性。此外，我们结合现代医学和网络药理学的见解，全面评估推荐处方的有效性，为现代中医药推荐提供了一个新的范式。 

---
# Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning 

**Title (ZH)**: 基于强化学习的正式化驱动的大规模语言模型提示打破 

**Authors**: Zhaoqi Wang, Daqing He, Zijian Zhang, Xin Li, Liehuang Zhu, Meng Li, Jiamou Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23558)  

**Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities, yet they also introduce novel security challenges. For instance, prompt jailbreaking attacks involve adversaries crafting sophisticated prompts to elicit responses from LLMs that deviate from human values. To uncover vulnerabilities in LLM alignment methods, we propose the PASS framework (\underline{P}rompt J\underline{a}ilbreaking via \underline{S}emantic and \underline{S}tructural Formalization). Specifically, PASS employs reinforcement learning to transform initial jailbreak prompts into formalized descriptions, which enhances stealthiness and enables bypassing existing alignment defenses. The jailbreak outputs are then structured into a GraphRAG system that, by leveraging extracted relevant terms and formalized symbols as contextual input alongside the original query, strengthens subsequent attacks and facilitates more effective jailbreaks. We conducted extensive experiments on common open-source models, demonstrating the effectiveness of our attack. 

**Abstract (ZH)**: 大型语言模型（LLMs）展示了 remarkable 的能力，但同时也引入了新型安全挑战。例如，提示脱管攻击涉及对手构造复杂的提示以促使LLMs产生违背人类价值观的响应。为了揭露LLM对齐方法中的漏洞，我们提出了PASS框架（通过语义和结构形式化进行提示脱管攻击，Prompt Jailbreaking via Semantic and Structural Formalization）。具体而言，PASS 使用强化学习将初始脱管攻击提示转化为形式化的描述，增强了隐蔽性并允许绕过现有的对齐防御。脱管攻击输出随后被结构化到GraphRAG系统中，通过结合提取的相关术语和形式化符号作为上下文输入，以及原始查询，加强了后续攻击，并促进了更有效的脱管攻击。我们在常见的开源模型上进行了广泛的实验，证明了我们攻击的有效性。 

---
# Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks 

**Title (ZH)**: 超越最强的LLM：多轮多Agent orchestration vs. 单个LLM在基准测试上的表现 

**Authors**: Aaron Xuxiang Tian, Ruofan Zhang, Jiayao Tang, Young Min Cho, Xueqian Li, Qiang Yi, Ji Wang, Zhunping Zhang, Danrui Qi, Sharath Chandra Guntuku, Lyle Ungar, Tianyu Shi, Chi Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23537)  

**Abstract**: We study multi-turn multi-agent orchestration, where multiple large language model (LLM) agents interact over multiple turns by iteratively proposing answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5 Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we conduct two experiments: (i) benchmarking orchestration against single-LLM baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who authored answers and whether they can observe ongoing votes. Orchestration matches or exceeds the strongest single model and consistently outperforms the others. Analysis of best-achievable orchestration performance shows potential for further gains. The ablations show that revealing authorship increases self-voting and ties, and that showing ongoing votes amplifies herding, which speeds convergence but can sometimes yield premature consensus. 

**Abstract (ZH)**: 多轮多agent协调研究：多大规模语言模型代理通过多次交互提出答案或投票以达成共识 

---
# DOoM: Difficult Olympiads of Math 

**Title (ZH)**: DOoM: 困难的数学奥林匹克问题集 

**Authors**: Ilya Kuleshov, Ilin Pavel, Nikolay Kompanets, Ksenia Sycheva, Aleksandr Nikolich  

**Link**: [PDF](https://arxiv.org/pdf/2509.23529)  

**Abstract**: This paper introduces DOoM, a new open-source benchmark designed to assess the capabilities of language models in solving mathematics and physics problems in Russian. The benchmark includes problems of varying difficulty, ranging from school-level tasks to university Olympiad and entrance exam questions. In this paper we discuss the motivation behind its creation, describe dataset's structure and evaluation methodology, and present initial results from testing various models. Analysis of the results shows a correlation between model performance and the number of tokens used, and highlights differences in performance between mathematics and physics tasks. 

**Abstract (ZH)**: 这篇论文介绍了DOoM，一个新推出的开源基准，旨在评估语言模型解决俄语数学和物理问题的能力。该基准包含不同难度的问题，从学校级任务到大学级奥林匹克竞赛和入学考试题目。本文讨论了其创建动机，描述了数据集结构和评估方法，并介绍了测试各种模型的初始结果。分析结果显示，模型性能与所用令牌数之间存在关联，并指出了数学任务和物理任务在性能上的差异。 

---
# Model Consistency as a Cheap yet Predictive Proxy for LLM Elo Scores 

**Title (ZH)**: 模型一致性作为LLM Elo评分的廉价但有效的替代指标 

**Authors**: Ashwin Ramaswamy, Nestor Demeure, Ermal Rrapaj  

**Link**: [PDF](https://arxiv.org/pdf/2509.23510)  

**Abstract**: New large language models (LLMs) are being released every day. Some perform significantly better or worse than expected given their parameter count. Therefore, there is a need for a method to independently evaluate models. The current best way to evaluate a model is to measure its Elo score by comparing it to other models in a series of contests - an expensive operation since humans are ideally required to compare LLM outputs. We observe that when an LLM is asked to judge such contests, the consistency with which it selects a model as the best in a matchup produces a metric that is 91% correlated with its own human-produced Elo score. This provides a simple proxy for Elo scores that can be computed cheaply, without any human data or prior knowledge. 

**Abstract (ZH)**: 新的大型语言模型（LLMs）每天都在发布。一些模型的性能显著优于或低于其参数数量所预期的效果。因此，需要一种独立评估模型的方法。目前评估模型的最佳方法是通过一系列比赛来比较模型的Elo评分——这是一个昂贵的操作，因为人类通常需要参与比较LLM的输出。我们观察到，当要求LLM判断这些比赛时，它在比赛中选择最佳模型的稳定程度与自身的手工生成的Elo评分有91%的相关性。这提供了一种无需人类数据或先验知识且计算成本低廉的Elo评分代理指标。 

---
# Dynamic Trust Calibration Using Contextual Bandits 

**Title (ZH)**: 基于上下文臂赛的动态信任校准 

**Authors**: Bruno M. Henrique, Eugene Santos Jr  

**Link**: [PDF](https://arxiv.org/pdf/2509.23497)  

**Abstract**: Trust calibration between humans and Artificial Intelligence (AI) is crucial for optimal decision-making in collaborative settings. Excessive trust can lead users to accept AI-generated outputs without question, overlooking critical flaws, while insufficient trust may result in disregarding valuable insights from AI systems, hindering performance. Despite its importance, there is currently no definitive and objective method for measuring trust calibration between humans and AI. Current approaches lack standardization and consistent metrics that can be broadly applied across various contexts, and they don't distinguish between the formation of opinions and subsequent human decisions. In this work, we propose a novel and objective method for dynamic trust calibration, introducing a standardized trust calibration measure and an indicator. By utilizing Contextual Bandits-an adaptive algorithm that incorporates context into decision-making-our indicator dynamically assesses when to trust AI contributions based on learned contextual information. We evaluate this indicator across three diverse datasets, demonstrating that effective trust calibration results in significant improvements in decision-making performance, as evidenced by 10 to 38% increase in reward metrics. These findings not only enhance theoretical understanding but also provide practical guidance for developing more trustworthy AI systems supporting decisions in critical domains, for example, disease diagnoses and criminal justice. 

**Abstract (ZH)**: 人类与人工智能的信任校准对于协作环境中的最优决策至关重要。过度信任可能导致用户无条件接受AI生成的结果，忽视关键缺陷；而不足的信任则可能导致忽视AI系统的有价值见解，阻碍性能提升。尽管其重要性日益凸显，目前仍缺乏一种既定且客观的方法来衡量人类与AI之间的信任校准。现有方法缺乏标准化且可广泛应用于不同场景的一致量化指标，并且未能区分意见形成与后续的人类决策。在此项研究中，我们提出了一种新颖且客观的动态信任校准方法，引入了标准化的信任校准测量指标和指示器。通过利用上下文臂拉姆达（Contextual Bandits）——一种将情境信息纳入决策过程的适应性算法，我们的指示器能够基于学习到的情境信息动态评估何时应信任AI的贡献。我们跨三个不同数据集评估了这一指标，结果表明有效的信任校准能够显著提升决策性能，表现为奖励指标提高了10%至38%。这些发现不仅深化了理论理解，还为在关键领域（如疾病诊断和刑事司法）开发更可靠的AI决策支持系统提供了实用指导。 

---
# Mapping Overlaps in Benchmarks through Perplexity in the Wild 

**Title (ZH)**: 在野生环境中的困惑度映射基准中的重叠 

**Authors**: Siyang Wu, Honglin Bao, Sida Li, Ari Holtzman, James A. Evans  

**Link**: [PDF](https://arxiv.org/pdf/2509.23488)  

**Abstract**: We develop signatures of capacity familiarity to characterize large language model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures probe the capacity required for benchmark performance. We formally define them as a set of salient tokens drawn from in-the-wild, naturally authored corpora, where LLM token perplexity, reflecting more or less pre-training exposure, becomes highly predictive of LLM benchmark performance. Through a large-scale meta-evaluation, we extract benchmark signatures via stepwise forward selection with linear regressions across 32 LLMs and 88 benchmarks spanning diverse knowledge, coding, logic, instruction following, math, language, reasoning, and world modeling. Our analysis situates signatures in relation to both the semantic similarity of benchmark questions and the correlation of model performance. While performance overlaps are universally high and semantic overlaps remain confined to a narrow mid-range, benchmark signatures prove highly informative in capturing variation, overlap, and divergence. We observe overlap in knowledge and reasoning subtasks, whereas multilingual and cultural benchmarks exhibit less similarity, even compared to cross-task overlap. Notably, performance-level results are strongly influenced by benchmark-orthogonal factors such as question format, highlighting limitations in LLM generalization, the conflation of performance with ability, and issues inherent in current mainstream benchmark agreement studies. Benchmark signatures, however, remain robust to such effects. Ultimately, we identify cross-functional overlaps across logic, math, language, instruction following, and world modeling, with coding emerging as the least overlapping domain. Together, these findings provide mechanistic insights into benchmark validity and LLM sensitivities, and sketch the underlying landscape of interconnected LLM capabilities. 

**Abstract (ZH)**: 我们开发了能力熟悉度特征以表征大规模语言模型（LLM）基准及其有意义的重叠。基准特征探究了基准性能所需的能力。我们正式定义它们为来自自然生成语料库中的显著标记集，其中LLM标记困惑度反映了更多或更少的预训练暴露，高度预测LLM基准性能。通过大规模元评估，我们通过逐步前进选择和在32个LLM和88个覆盖广泛知识、编程、逻辑、指令遵循、数学、语言、推理和世界建模的基准上的线性回归提取基准特征。我们的分析将特征置于基准问题语义相似性和模型性能相关性的关系中。尽管性能重叠普遍较高，而语义重叠则限制在狭窄的中等范围内，基准特征在捕捉变异、重叠和差异方面证明高度信息丰富。我们观察到知识和推理子任务中的重叠，多语言和文化基准则表现出较少的相似性，即使与多任务重叠相比。值得注意的是，基准性能水平的结果受到如问题格式等基准正交因素的强烈影响，突显了LLM泛化能力的局限性、将性能等同于能力的问题以及当前主流基准一致研究中存在的内在问题。然而，基准特征对这些影响表现出稳定性。最终，我们在逻辑、数学、语言、指令遵循和世界建模之间识别出跨功能重叠，编程领域成为重叠最少的领域。这些发现共同提供了关于基准有效性和LLM敏感性的机制性见解，并勾勒出相连的LLM能力的潜在景观。 

---
# Accurate Predictions in Education with Discrete Variational Inference 

**Title (ZH)**: 基于离散变分推断的教育中准确预测 

**Authors**: Tom Quilter, Anastasia Ilick, Anastasia Ilick, Richard Turner  

**Link**: [PDF](https://arxiv.org/pdf/2509.23484)  

**Abstract**: One of the largest drivers of social inequality is unequal access to personal tutoring, with wealthier individuals able to afford it, while the majority cannot. Affordable, effective AI tutors offer a scalable solution. We focus on adaptive learning, predicting whether a student will answer a question correctly, a key component of any effective tutoring system. Yet many platforms struggle to achieve high prediction accuracy, especially in data-sparse settings. To address this, we release the largest open dataset of professionally marked formal mathematics exam responses to date. We introduce a probabilistic modelling framework rooted in Item Response Theory (IRT) that achieves over 80 percent accuracy, setting a new benchmark for mathematics prediction accuracy of formal exam papers. Extending this, our collaborative filtering models incorporate topic-level skill profiles, but reveal a surprising and educationally significant finding, a single latent ability parameter alone is needed to achieve the maximum predictive accuracy. Our main contribution though is deriving and implementing a novel discrete variational inference framework, achieving our highest prediction accuracy in low-data settings and outperforming all classical IRT and matrix factorisation baselines. 

**Abstract (ZH)**: 一种广泛的社会不平等驱动因素是个人辅导的不平等获取，wealthier个体能够负担得起辅导，而大多数人则不能。负担得起且有效的AI辅导提供了一种可扩展的解决方案。我们关注自适应学习，预测学生是否能正确回答问题，这是任何有效辅导系统的关键组成部分。然而，许多平台在低数据情况下难以实现高预测准确性。为此，我们发布了迄今为止最大的专业标记形式数学考试答案开放数据集。我们提出了一种基于项目反应理论（IRT）的概率建模框架，实现了超过80%的准确性，为形式考试论文的数学预测准确性设立了新基准。在此基础上，我们的协同过滤模型结合了主题级别的技能配置文件，但揭示了一个令人惊讶且教育意义重大的发现——单个潜在能力参数足以实现最大预测准确性。然而，我们的主要贡献是推导并实现了一种新颖的离散变分推断框架，在低数据环境下实现了最高的预测准确性，并优于所有经典的IRT和矩阵分解基线。 

---
# GeoBS: Information-Theoretic Quantification of Geographic Bias in AI Models 

**Title (ZH)**: GeoBS:基于信息理论的地理偏差量化方法在AI模型中的应用 

**Authors**: Zhangyu Wang, Nemin Wu, Qian Cao, Jiangnan Xia, Zeping Liu, Yiqun Xie, Akshay Nambi, Tanuja Ganu, Ni Lao, Ninghao Liu, Gengchen Mai  

**Link**: [PDF](https://arxiv.org/pdf/2509.23482)  

**Abstract**: The widespread adoption of AI models, especially foundation models (FMs), has made a profound impact on numerous domains. However, it also raises significant ethical concerns, including bias issues. Although numerous efforts have been made to quantify and mitigate social bias in AI models, geographic bias (in short, geo-bias) receives much less attention, which presents unique challenges. While previous work has explored ways to quantify geo-bias, these measures are model-specific (e.g., mean absolute deviation of LLM ratings) or spatially implicit (e.g., average fairness scores of all spatial partitions). We lack a model-agnostic, universally applicable, and spatially explicit geo-bias evaluation framework that allows researchers to fairly compare the geo-bias of different AI models and to understand what spatial factors contribute to the geo-bias. In this paper, we establish an information-theoretic framework for geo-bias evaluation, called GeoBS (Geo-Bias Scores). We demonstrate the generalizability of the proposed framework by showing how to interpret and analyze existing geo-bias measures under this framework. Then, we propose three novel geo-bias scores that explicitly take intricate spatial factors (multi-scalability, distance decay, and anisotropy) into consideration. Finally, we conduct extensive experiments on 3 tasks, 8 datasets, and 8 models to demonstrate that both task-specific GeoAI models and general-purpose foundation models may suffer from various types of geo-bias. This framework will not only advance the technical understanding of geographic bias but will also establish a foundation for integrating spatial fairness into the design, deployment, and evaluation of AI systems. 

**Abstract (ZH)**: AI模型，尤其是基础模型（FMs）的广泛应用对众多领域产生了深远影响，但也引发了显著的伦理问题，包括公平性问题。尽管已做了大量努力来量化和减轻AI模型中的社会偏见，但地理偏见（简称Geo-bias）却受到较少关注，这提出了独特的挑战。尽管以往工作已探索了量化Geo-bias的方法，但这些方法通常是模型特定的（例如，大语言模型评分的绝对均差）或空间隐含的（例如，所有空间分区平均公平性得分）。缺乏一种模型无关的、普遍适用的、空间明确的Geo-bias评价框架，使研究人员无法公平比较不同AI模型的Geo-bias，并理解哪些空间因素导致了Geo-bias。在本文中，我们建立了基于信息理论的Geo-bias评价框架，称为GeoBS（Geo-bias Scores）。我们展示了该框架的一般适用性，通过展示如何在该框架下解释和分析现有Geo-bias度量。然后，我们提出了三种新的Geo-bias评分，明确考虑了复杂的空间因素（多级可扩展性、距离衰减和各向异性）。最后，我们在3项任务、8个数据集和8个模型上进行广泛实验，证明了专门针对任务的GeoAI模型和通用基础模型都可能遭受各种类型的Geo-bias。该框架不仅将推动地理偏见的技术理解，还将为将空间公平性整合到AI系统的设计、部署和评估中奠定基础。 

---
# ViTSP: A Vision Language Models Guided Framework for Large-Scale Traveling Salesman Problems 

**Title (ZH)**: ViTSP: 由vision-language模型指导的大规模旅行商问题框架 

**Authors**: Zhuoli Yin, Yi Ding, Reem Khir, Hua Cai  

**Link**: [PDF](https://arxiv.org/pdf/2509.23465)  

**Abstract**: Solving Traveling Salesman Problem (TSP) is NP-hard yet fundamental for wide real-world applications. Classical exact methods face challenges in scaling, and heuristic methods often require domain-specific parameter calibration. While learning-based approaches have shown promise, they suffer from poor generalization and limited scalability due to fixed training data. This work proposes ViTSP, a novel framework that leverages pre-trained vision language models (VLMs) to visually guide the solution process for large-scale TSPs. The VLMs function to identify promising small-scale subproblems from a visualized TSP instance, which are then efficiently optimized using an off-the-shelf solver to improve the global solution. ViTSP bypasses the dedicated model training at the user end while maintaining effectiveness across diverse instances. Experiments on real-world TSP instances ranging from 1k to 88k nodes demonstrate that ViTSP consistently achieves solutions with average optimality gaps below 0.2%, outperforming existing learning-based methods. Under the same runtime budget, it surpasses the best-performing heuristic solver, LKH-3, by reducing its gaps by 12% to 100%, particularly on very-large-scale instances with more than 10k nodes. Our framework offers a new perspective in hybridizing pre-trained generative models and operations research solvers in solving combinatorial optimization problems, with practical implications for integration into more complex logistics systems. The code is available at this https URL. 

**Abstract (ZH)**: 基于预训练视觉语言模型的Traveling Salesman Problem求解新框架 

---
# Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity 

**Title (ZH)**: 超越嵌入：二进制代码相似性解释性特征提取 

**Authors**: Charles E. Gagnon, Steven H. H. Ding, Philippe Charland, Benjamin C. M. Fung  

**Link**: [PDF](https://arxiv.org/pdf/2509.23449)  

**Abstract**: Binary code similarity detection is a core task in reverse engineering. It supports malware analysis and vulnerability discovery by identifying semantically similar code in different contexts. Modern methods have progressed from manually engineered features to vector representations. Hand-crafted statistics (e.g., operation ratios) are interpretable, but shallow and fail to generalize. Embedding-based methods overcome this by learning robust cross-setting representations, but these representations are opaque vectors that prevent rapid verification. They also face a scalability-accuracy trade-off, since high-dimensional nearest-neighbor search requires approximations that reduce precision. Current approaches thus force a compromise between interpretability, generalizability, and scalability.
We bridge these gaps using a language model-based agent to conduct structured reasoning analysis of assembly code and generate features such as input/output types, side effects, notable constants, and algorithmic intent. Unlike hand-crafted features, they are richer and adaptive. Unlike embeddings, they are human-readable, maintainable, and directly searchable with inverted or relational indexes. Without any matching training, our method respectively achieves 42% and 62% for recall@1 in cross-architecture and cross-optimization tasks, comparable to embedding methods with training (39% and 34%). Combined with embeddings, it significantly outperforms the state-of-the-art, demonstrating that accuracy, scalability, and interpretability can coexist. 

**Abstract (ZH)**: 基于二进制代码相似性检测的逆向工程核心任务：结合语言模型的结构化推理在架构和优化任务中的应用 

---
# Democratizing AI scientists using ToolUniverse 

**Title (ZH)**: 使用ToolUniverse使人工智能科学家 democratization 

**Authors**: Shanghua Gao, Richard Zhu, Pengwei Sui, Zhenglun Kong, Sufian Aldogom, Yepeng Huang, Ayush Noori, Reza Shamji, Krishna Parvataneni, Theodoros Tsiligkaridis, Marinka Zitnik  

**Link**: [PDF](https://arxiv.org/pdf/2509.23426)  

**Abstract**: AI scientists are emerging computational systems that serve as collaborative partners in discovery. These systems remain difficult to build because they are bespoke, tied to rigid workflows, and lack shared environments that unify tools, data, and analyses into a common ecosystem. In omics, unified ecosystems have transformed research by enabling interoperability, reuse, and community-driven development; AI scientists require comparable infrastructure. We present ToolUniverse, an ecosystem for building AI scientists from any language or reasoning model, whether open or closed. TOOLUNIVERSE standardizes how AI scientists identify and call tools, integrating more than 600 machine learning models, datasets, APIs, and scientific packages for data analysis, knowledge retrieval, and experimental design. It automatically refines tool interfaces for correct use by AI scientists, creates new tools from natural language descriptions, iteratively optimizes tool specifications, and composes tools into agentic workflows. In a case study of hypercholesterolemia, ToolUniverse was used to create an AI scientist to identify a potent analog of a drug with favorable predicted properties. The open-source ToolUniverse is available at this https URL. 

**Abstract (ZH)**: AI科学家是新兴的计算系统，作为发现过程中的合作者。由于它们是定制的、与刚性的工作流程相关联且缺乏统一的环境将工具、数据和分析集成到一个共同生态系统中，因此构建这些系统仍然具有挑战性。在omics领域，统一的生态系统通过促进互操作性、重用和社区驱动的发展而转变了研究；AI科学家需要类似的基础设施。我们提出ToolUniverse，这是一个构建来自任何语言或推理模型（无论是开源还是封闭）的AI科学家的生态系统。TOOLUNIVERSE标准化了AI科学家识别和调用工具的方式，集成了超过600个机器学习模型、数据集、API和科学包，用于数据分析、知识检索和实验设计。ToolUniverse自动细化工具接口以供AI科学家正确使用，从自然语言描述中创建新工具，迭代优化工具规范，并组成具有自主性的工作流程。在高胆固醇研究案例中，ToolUniverse被用于创建一个AI科学家来识别一种具有有利预测性质的药物类似物。开源ToolUniverse可从该链接访问：this https URL。 

---
# From Conversation to Query Execution: Benchmarking User and Tool Interactions for EHR Database Agents 

**Title (ZH)**: 从对话到查询执行：评估EHR数据库代理中的用户和工具交互 

**Authors**: Gyubok Lee, Woosog Chay, Heeyoung Kwak, Yeong Hwa Kim, Haanju Yoo, Oksoon Jeong, Meong Hi Son, Edward Choi  

**Link**: [PDF](https://arxiv.org/pdf/2509.23415)  

**Abstract**: Despite the impressive performance of LLM-powered agents, their adoption for Electronic Health Record (EHR) data access remains limited by the absence of benchmarks that adequately capture real-world clinical data access flows. In practice, two core challenges hinder deployment: query ambiguity from vague user questions and value mismatch between user terminology and database entries. To address this, we introduce EHR-ChatQA an interactive database question answering benchmark that evaluates the end-to-end workflow of database agents: clarifying user questions, using tools to resolve value mismatches, and generating correct SQL to deliver accurate answers. To cover diverse patterns of query ambiguity and value mismatch, EHR-ChatQA assesses agents in a simulated environment with an LLM-based user across two interaction flows: Incremental Query Refinement (IncreQA), where users add constraints to existing queries, and Adaptive Query Refinement (AdaptQA), where users adjust their search goals mid-conversation. Experiments with state-of-the-art LLMs (e.g., o4-mini and Gemini-2.5-Flash) over five i.i.d. trials show that while agents achieve high Pass@5 of 90-95% (at least one of five trials) on IncreQA and 60-80% on AdaptQA, their Pass^5 (consistent success across all five trials) is substantially lower by 35-60%. These results underscore the need to build agents that are not only performant but also robust for the safety-critical EHR domain. Finally, we provide diagnostic insights into common failure modes to guide future agent development. 

**Abstract (ZH)**: 尽管 large language model (LLM) 助手表现出色，但由于缺乏能充分捕捉现实世界临床数据访问流程的基准测试，它们在电子健康记录 (EHR) 数据访问中的应用仍然有限。实践中，部署面临两大核心挑战：用户含糊不清的问题导致的查询歧义，以及用户术语与数据库条目之间的价值 mismatch。为解决这一问题，我们引入了 EHR-ChatQA 交互式数据库问答基准测试，评估数据库助手的端到端工作流程：澄清用户问题、使用工具解决价值 mismatch 并生成正确的 SQL 以提供准确的答案。为了涵盖查询歧义和价值 mismatch 的多样模式，EHR-ChatQA 在包含 LLM 用户的模拟环境中评估助手，涵盖两种交互流程：增量查询细化（IncreQA），用户在现有查询中添加约束，以及适应性查询细化（AdaptQA），用户在对话中途调整搜索目标。实验显示，使用最先进的 LLM（如 o4-mini 和 Gemini-2.5-Flash）进行五次独立试验后，在增量查询细化（IncreQA）上的通过率为 90-95%（至少在五次试验中的一次），而在适应性查询细化（AdaptQA）上的通过率为 60-80%，而连续通过五次试验的 Pass^5（一致成功）显著降低 35-60%。这些结果强调了在关键安全领域（如 EHR）构建不仅性能高而且具有鲁棒性的助手的必要性。最后，我们提供了对常见故障模式的诊断洞察，以指导未来助手的发展。 

---
# Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking 

**Title (ZH)**: 你的模型已经思考足够了：训练大规模推理模型以停止过度思考 

**Authors**: Jinyi Han, Ying Huang, Ying Liao, Zishang Jiang, Xikun Lu, Haiquan Zhao, Xinyi Wang, Guanghao Zhou, Sihang Jiang, Jiaqing Liang, Weikang Zhou, Zeye Sun, Fei Yu, Yanghua Xiao  

**Link**: [PDF](https://arxiv.org/pdf/2509.23392)  

**Abstract**: Large Reasoning Models (LRMs) have achieved impressive performance on challenging tasks, yet their deep reasoning often incurs substantial computational costs. To achieve efficient reasoning, existing reinforcement learning methods still struggle to construct short reasoning path during the rollout stage, limiting effective learning. Inspired by Evidence Accumulation Models, we find that LRMs have accumulated sufficient information early in reasoning, making further reasoning steps redundant. Based on this insight, we propose Just-Enough Thinking (JET), which trains models to proactively terminate unnecessary reasoning. JET performs trajectory truncation during rollout to expose the model to short, distributionally consistent reasoning paths. Besides, it uses a quality-controlled length reward to better encourage concise reasoning while maintaining correctness. Extensive experiments demonstrate that JET significantly improves reasoning efficiency without sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6% accuracy gain while reducing output length by 46.3% on the Olympiad benchmark. Our code is available in the GitHub. 

**Abstract (ZH)**: Large Reasoning Models (LRMs)在挑战性任务上取得了令人印象深刻的表现，但其深度推理往往伴随着巨大的计算成本。为了实现高效的推理，现有的强化学习方法依然难以在展开阶段构造简短的推理路径，限制了有效的学习。受证据累积模型的启发，我们发现LRMs在推理早期就已经积累了足够的信息，使得后续的推理步骤变得多余。基于这一洞察，我们提出了Just-Enough Thinking (JET)，该方法训练模型主动终止不必要的推理。JET在展开过程中执行轨迹截断，使模型暴露于简短且分布一致的推理路径中。此外，它使用质量控制长度奖励更好地促进简洁的推理同时保持正确性。广泛实验表明，JET在提高推理效率的同时不牺牲准确性。特别是，DeepSeek-Distill-Qwen-1.5B在奥林匹克基准测试上的输出长度减少了46.3%，准确率提高了4.6%。我们的代码已在GitHub上开源。 

---
# Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning 

**Title (ZH)**: 学习如何使用工具，而不仅仅是何时使用工具：模式感知的工具集成推理 

**Authors**: Ningning Xu, Yuxuan Jiang, Shubhashis Roy Dipta  

**Link**: [PDF](https://arxiv.org/pdf/2509.23292)  

**Abstract**: Tool-integrated reasoning (TIR) has become a key approach for improving large reasoning models (LRMs) on complex problems. Prior work has mainly studied when to invoke tools, while overlooking how tools are applied. We identify two common patterns: a calculator pattern that uses code for direct computation, and an algorithmic pattern that encodes problems as programs. Misaligned choices often cause failures even when reasoning is sound. We propose a two-stage framework that first builds code competence from both patterns and then aligns pattern selection with teacher preferences. Across challenging math datasets, our pattern-aware method substantially improves both code usage and accuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on AIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a pattern-aware approach for tool-integrated reasoning. 

**Abstract (ZH)**: 工具集成推理中的模式意识方法：一种提高复杂问题处理能力的关键途径 

---
# Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning 

**Title (ZH)**: 基于自我进化偏好学习的高效工具集成推理 

**Authors**: Yifei Chen, Guanting Dong, Zhicheng Dou  

**Link**: [PDF](https://arxiv.org/pdf/2509.23285)  

**Abstract**: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal reasoning ability by integrating external tools. However, models employing TIR often display suboptimal behaviors, such as insufficient or excessive tool usage and overthinking after tool calls. The challenge of incentivizing LLMs to perform TIR efficiently and accurately, while stabilizing the reasoning process, remains an open question. In this paper, we start by exploring the impact of tool calls on model reasoning from the perspective of information entropy. Our findings indicate that tool call results lead to a distinct change in the information entropy of subsequent reasoning, with the overall entropy of the reasoning chain varying based on the number of tool calls. Building on these insights, we propose Tool-Light, a framework designed to encourage LLMs to perform TIR efficiently and accurately. Our framework includes dataset construction and multi-stage fine-tuning. For dataset construction, we employ continuous self-evolved sampling using the fine-tuned model, integrating both vanilla sampling and entropy-guided sampling. Besides, we establish strict criteria for selecting positive-negative pairs during sampling. The training process involves a two-stage approach, comprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference Optimization (DPO). Experimental results on 10 datasets demonstrate the effectiveness of Tool-Light, significantly improving the model's efficiency in executing TIR tasks. 

**Abstract (ZH)**: 基于工具的信息熵视角探索Tool-Integrated Reasoning (TIR)的高效准确实现：Tool-Light框架 

---
# Socio-Economic Model of AI Agents 

**Title (ZH)**: AI代理的经济社会模型 

**Authors**: Yuxinyue Qian, Jun Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23270)  

**Abstract**: Modern socio-economic systems are undergoing deep integration with artificial intelligence technologies. This paper constructs a heterogeneous agent-based modeling framework that incorporates both human workers and autonomous AI agents, to study the impact of AI collaboration under resource constraints on aggregate social output. We build five progressively extended models: Model 1 serves as the baseline of pure human collaboration; Model 2 introduces AI as collaborators; Model 3 incorporates network effects among agents; Model 4 treats agents as independent producers; and Model 5 integrates both network effects and independent agent production. Through theoretical derivation and simulation analysis, we find that the introduction of AI agents can significantly increase aggregate social output. When considering network effects among agents, this increase exhibits nonlinear growth far exceeding the simple sum of individual contributions. Under the same resource inputs, treating agents as independent producers provides higher long-term growth potential; introducing network effects further demonstrates strong characteristics of increasing returns to scale. 

**Abstract (ZH)**: 现代社会经济系统正与人工智能技术深度融合。本文构建了一个包含人类工人和自主AI代理的异质代理基于模型框架，研究资源约束条件下AI协作对总体社会产出的影响。我们构建了五个逐步扩展的模型：模型1作为纯人类协作的基线；模型2引入AI作为合作者；模型3纳入代理间的网络效应；模型4将代理视为独立生产者；模型5结合了网络效应和独立代理生产。通过理论推导和仿真分析发现，引入AI代理可以显著增加总体社会产出。考虑代理间的网络效应时，这种增长呈现出非线性增长，远超个体贡献的简单相加。在相同的资源输入下，将代理视为独立生产者提供了更高的长期增长潜力；引入网络效应进一步展示了规模收益递增的强烈特征。 

---
# GUI-PRA: Process Reward Agent for GUI Tasks 

**Title (ZH)**: GUI-PRA：GUI任务的奖励代理 

**Authors**: Tao Xiong, Xavier Hu, Yurun Chen, Yuhang Liu, Changqiao Wu, Pengzhi Gao, Wei Liu, Jian Luan, Shengyu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23263)  

**Abstract**: Graphical User Interface (GUI) Agents powered by Multimodal Large Language Models (MLLMs) show significant potential for automating tasks. However, they often struggle with long-horizon tasks, leading to frequent failures. Process Reward Models (PRMs) are a promising solution, as they can guide these agents with crucial process signals during inference. Nevertheless, their application to the GUI domain presents unique challenges. When processing dense artificial inputs with long history data, PRMs suffer from a "lost in the middle" phenomenon, where the overwhelming historical context compromises the evaluation of the current step. Furthermore, standard PRMs lacks GUI changing awareness, providing static evaluations that are disconnected from the dynamic consequences of actions, a critical mismatch with the inherently dynamic nature of GUI tasks. In response to these challenges, we introduce GUI-PRA (Process Reward Agent for GUI Tasks), a judge agent designed to better provide process reward than standard PRM by intelligently processing historical context and actively perceiving UI state changes. Specifically, to directly combat the ``lost in the middle'' phenomenon, we introduce a dynamic memory mechanism consisting of two core components: a Relevance-based Retrieval Module to actively fetch pertinent information from long histories and a Progressive Summarization Module to dynamically condense growing interaction data, ensuring the model focuses on relevant context. Moreover, to address the lack of UI changing awareness, we introduce an Aadaptive UI Perception mechanism. This mechanism enables the agent to reason about UI state changes and dynamically select the most appropriate tool to gather grounded visual evidence, ensuring its evaluation is always informed by the current UI context. 

**Abstract (ZH)**: 基于多模态大规模语言模型的图形用户界面（GUI）代理展示出显著的自动化任务潜力。然而，它们往往在长期任务上表现挣扎，导致频繁失败。过程奖励模型（PRM）是一个有希望的解决方案，因为它们可以在推断过程中通过关键的过程信号引导这些代理。然而，将其应用于GUI领域带来了独特的挑战。在处理包含长期历史数据的密集人工输入时，PRM会遭受“中间迷失”现象的困扰，其中压倒性的历史上下文损害了当前步骤的评估。此外，标准PRM缺乏GUI变化感知性，提供静态评估，与GUI任务本质上动态的特点存在严重不匹配。为应对这些挑战，我们介绍了GUI-PRA（GUI任务的过程奖励代理），一种设计用于通过智能处理历史上下文并主动感知UI状态变化来更好地提供过程奖励的法官代理。具体而言，为了直接对抗“中间迷失”现象，我们引入了一种动态记忆机制，包括两个核心组成部分：基于相关性的检索模块，用于主动检索长期历史中的相关信息，以及逐步总结模块，用于动态压缩增长中的交互数据，确保模型专注于相关上下文。此外，为了应对缺乏UI变化感知性的问题，我们引入了自适应UI感知机制。该机制使代理能够解释UI状态变化，并动态选择最合适的工具以收集具体的视觉证据，确保其评估始终受到当前UI上下文的影响。 

---
# Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned 

**Title (ZH)**: 训练视觉-语言过程奖励模型以实现多模态推理中的测试时扩展：关键见解与经验教训 

**Authors**: Brandon Ong, Tej Deep Pala, Vernon Toh, William Chandra Tjhi, Soujanya Poria  

**Link**: [PDF](https://arxiv.org/pdf/2509.23250)  

**Abstract**: Process Reward Models (PRMs) provide step-level supervision that improves the reliability of reasoning in large language models. While PRMs have been extensively studied in text-based domains, their extension to Vision Language Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on Monte Carlo Tree Search (MCTS) for data construction, which can often produce noisy supervision signals and limit generalization across tasks. In this work, we aim to elucidate the design space of VL-PRMs by exploring diverse strategies for dataset construction, training, and test-time scaling. First, we introduce a hybrid data synthesis framework that combines MCTS with judgments from a strong VLM, producing more accurate step-level labels. Second, we propose perception-focused supervision, enabling our PRM to explicitly detect errors at the visual grounding stage of reasoning. Third, we systematically evaluate multiple test-time scaling strategies, showing that our PRMs can reliably guide VLMs toward more accurate solutions. Our experiments covering five diverse multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM guided process step selection, (ii) smaller VL-PRMs can match or even surpass larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning abilities in stronger VLM backbones, (iv) perception-level supervision leads to significant gains in test-time scaling, and (v) TTS performance of different policies improve on advanced math reasoning datasets despite not training VL-PRMs on such datasets. We hope our work will motivate further research and support the advancement of VLMs. 

**Abstract (ZH)**: 过程奖励模型（PRMs）为大规模语言模型提供步骤级的监督，提高推理的可靠性。尽管PRMs在文本领域得到了广泛研究，但将其扩展到视觉语言模型（VLMs）仍然有限。现有的视觉语言PRMs（VL-PRMs）依赖蒙特卡洛树搜索（MCTS）进行数据构建，这常常会产生噪声监督信号并限制跨任务的一般化能力。本文旨在通过探索多样化的数据集构建、训练和测试时扩展策略来阐明VL-PRMs的设计空间。首先，我们提出了一种结合MCTS和强VLM判断的混合数据合成框架，生成更准确的步骤级标签。其次，我们提出了感知焦点监督，使我们的PRM能够明确在视觉锚定推理阶段检测错误。第三，我们系统地评估了多种测试时扩展策略，展示了我们的PRMs能够可靠地引导VLMs获得更准确的解决方案。我们的实验覆盖了五个多模态基准（MMMU、PuzzleVQA、AlgoPuzzleVQA、MathVista和MathVision），揭示了几项关键洞察：（i）在测试时扩展（TTS）期间作为结果奖励模型（ORMs）使用的VL-PRMs可以优于指导过程步骤选择的VL-PRM，（ii）较小的VL-PRMs可以匹配甚至超越较大的VL-PRMs以检测过程错误，（iii）VL-PRMs在较强的VLM主干中揭示了潜在的推理能力，（iv）感知级监督在测试时扩展中带来了显著的改进，（v）不同策略的TTS性能即使在未对VL-PRMs进行此类数据集训练的情况下也改善了高级数学推理数据集的表现。我们希望本研究能够激发进一步的研究，支持VLMs的发展。 

---
# Agentic AI Reasoning for Mobile Edge General Intelligence: Fundamentals, Approaches, and Directions 

**Title (ZH)**: 代理型AI推理在移动边缘通用智能中的应用：基础、方法及方向 

**Authors**: Mingyi Luo, Ruichen Zhang, Xiangwang Hou, Jun Du, Chunxiao Jiang, Yong Ren, Dusit Niyato, Shiwen Mao  

**Link**: [PDF](https://arxiv.org/pdf/2509.23248)  

**Abstract**: The rapid advancement of large language models (LLMs) has enabled an emergence of agentic artificial intelligence (AI) with powerful reasoning and autonomous decision-making capabilities. This integration with edge computing has led to the development of Mobile Edge General Intelligence (MEGI), which brings real-time, privacy-preserving reasoning to the network edge. However, deploying LLM-based agentic AI reasoning in MEGI environments poses significant challenges due to the high computational demands of reasoning and the limited resources of edge devices. To address these challenges, we propose a joint optimization framework for efficient LLM reasoning deployment in MEGI. First, we review methods that enhance LLM reasoning capabilities, such as Chain-of-Thought (CoT) prompting, Supervised Fine-Tuning (SFT), and Mixture of Experts (MoE). Next, we present a distributed framework that addresses two correlated aspects: reasoning enhancement through adaptive CoT prompting and scalable deployment through distributed MoE architecture. The framework dynamically activates expert networks and adjusts reasoning depth based on task complexity and device capabilities. We further conduct experimental evaluations in mobile edge environments. Experimental results demonstrate the framework's effectiveness in balancing reasoning quality with resource efficiency, validating the practical viability of deploying sophisticated LLM reasoning capabilities in resource-constrained MEGI environments. 

**Abstract (ZH)**: 大规模语言模型的迅速发展推动了具有强大推理和自主决策能力的智能代理人工智能的 emergence，这一集成与边缘计算促进了移动边缘通用智能（MEGI）的发展，将实时、隐私保护的推理带到了网络边缘。然而，在 MEGI 环境中部署基于 LLM 的智能代理推理面临着显著挑战，因为推理的高计算需求与边缘设备的有限资源之间的矛盾。为解决这些挑战，我们提出了一种联合优化框架，以实现 MEGI 中高效的大规模语言模型推理部署。首先，我们回顾了增强 LLM 推理能力的方法，如思维链（CoT）提示、监督微调（SFT）和专家混合（MoE）。接着，我们提出了一个分布式框架，来应对两个相关的方面：通过自适应 CoT 提示进行推理增强以及通过分布式 MoE 架构进行可扩展部署。该框架根据任务复杂度和设备能力动态激活专家网络并调整推理深度。我们进一步在移动边缘环境中进行了实验评估。实验结果表明，该框架有效平衡了推理质量和资源效率，验证了在资源受限的 MEGI 环境中部署复杂的大规模语言模型推理能力的可行性。 

---
# $p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding 

**Title (ZH)**: $p$-less采样：一种鲁棒的无超参数LLM解码方法 

**Authors**: Runyan Tan, Shuang Wu, Phillip Howard  

**Link**: [PDF](https://arxiv.org/pdf/2509.23234)  

**Abstract**: Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments. 

**Abstract (ZH)**: 从大规模语言模型中获得高质量输出：基于信息论的$p$-less采样方法及其应用 

---
# AutoEP: LLMs-Driven Automation of Hyperparameter Evolution for Metaheuristic Algorithms 

**Title (ZH)**: AutoEP：LLMs驱动的元启发式算法超参数进化自动化 

**Authors**: Zhenxing Xu, Yizhe Zhang, Weidong Bao, Hao Wang, Ming Chen, Haoran Ye, Wenzheng Jiang, Hui Yan, Ji Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23189)  

**Abstract**: Dynamically configuring algorithm hyperparameters is a fundamental challenge in computational intelligence. While learning-based methods offer automation, they suffer from prohibitive sample complexity and poor generalization. We introduce AutoEP, a novel framework that bypasses training entirely by leveraging Large Language Models (LLMs) as zero-shot reasoning engines for algorithm control. AutoEP's core innovation lies in a tight synergy between two components: (1) an online Exploratory Landscape Analysis (ELA) module that provides real-time, quantitative feedback on the search dynamics, and (2) a multi-LLM reasoning chain that interprets this feedback to generate adaptive hyperparameter strategies. This approach grounds high-level reasoning in empirical data, mitigating hallucination. Evaluated on three distinct metaheuristics across diverse combinatorial optimization benchmarks, AutoEP consistently outperforms state-of-the-art tuners, including neural evolution and other LLM-based methods. Notably, our framework enables open-source models like Qwen3-30B to match the performance of GPT-4, demonstrating a powerful and accessible new paradigm for automated hyperparameter design. Our code is available at this https URL 

**Abstract (ZH)**: 动态配置算法超参数是计算智能中的一个基本挑战。虽然基于学习的方法提供了自动化，但它们遭受样本复杂性的限制和泛化的不足。我们引入了AutoEP，这是一种新颖的框架，通过利用大型语言模型（LLMs）作为零shot推理引擎来控制算法，从而完全避免了训练过程。AutoEP的核心创新在于两个组件之间的紧密协同作用：(1) 在线探索性景观分析（ELA）模块，提供实时的、定量的搜索动力学反馈，以及 (2) 多LLM推理链，解释这些反馈以生成自适应的超参数策略。这种方法将高级推理与经验数据紧密结合，减少了幻觉。在三种不同的元启发式算法和多种组合优化基准测试中，AutoEP一致地优于最先进的调优器，包括神经演化和其他基于LLM的方法。值得注意的是，我们的框架使开源模型如Qwen3-30B能够达到GPT-4的性能，展示了自动化超参数设计的强大而易用的新范式。我们的代码可在以下链接获取。 

---
# Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction 

**Title (ZH)**: 通过多词预测理解与增强语言模型的规划能力 

**Authors**: Qimin Zhong, Hao Liao, Siwei Wang, Mingyang Zhou, Xiaoqun Wu, Rui Mao, Wei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.23186)  

**Abstract**: Large Language Models (LLMs) have achieved impressive performance across diverse tasks but continue to struggle with learning transitive relations, a cornerstone for complex planning. To address this issue, we investigate the Multi-Token Prediction (MTP) paradigm and its impact to transitive relation learning. We theoretically analyze the MTP paradigm using a Transformer architecture composed of a shared output head and a transfer layer. Our analysis reveals that the transfer layer gradually learns the multi-step adjacency information, which in turn enables the backbone model to capture unobserved transitive reachability relations beyond those directly present in the training data, albeit with some inevitable noise in adjacency estimation. Building on this foundation, we propose two strategies to enhance the transfer layer and overall learning quality: Next-Token Injection (NTI) and a Transformer-based transfer layer. Our experiments on both synthetic graphs and the Blocksworld planning benchmark validate our theoretical findings and demonstrate that the improvements significantly enhance the model's path-planning capability. These findings deepen our understanding of how Transformers with MTP learn in complex planning tasks, and provide practical strategies to overcome the transitivity bottleneck, paving the way toward structurally aware and general-purpose planning models. 

**Abstract (ZH)**: 大型语言模型（LLMs）在多种任务上取得了令人印象深刻的性能，但在学习传递关系方面仍存在挑战，而这对于复杂的规划是基础。为了解决这一问题，我们探讨了多令牌预测（MTP）范式及其对传递关系学习的影响。我们使用包含共享输出头和转移层的Transformer架构来理论分析MTP范式。分析发现，转移层逐步学习多步邻接信息，进而使骨干模型捕捉到训练数据中未直接出现的传递可达关系，尽管在邻接估计中不可避免地存在一些噪声。在此基础上，我们提出了两种策略以增强转移层和整体学习质量：Next-Token Injection（NTI）和基于Transformer的转移层。我们在合成图和Blocksworld规划基准测试上的实验验证了我们的理论发现，并证明这些改进显著提升了模型的路径规划能力。这些发现深化了我们对具有MTP的Transformer在复杂规划任务中学习机制的理解，并提供了克服传递性瓶颈的实用策略，为进一步构建结构意识强且通用的规划模型铺平了道路。 

---
# Limit Analysis for Symbolic Multi-step Reasoning Tasks with Information Propagation Rules Based on Transformers 

**Title (ZH)**: 基于Transformer的信息传播规则符号多步推理任务的极限分析 

**Authors**: Tian Qin, Yuhan Chen, Zhiwei Wang, Zhi-Qin John Xu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23178)  

**Abstract**: Transformers are able to perform reasoning tasks, however the intrinsic mechanism remains widely open. In this paper we propose a set of information propagation rules based on Transformers and utilize symbolic reasoning tasks to theoretically analyze the limit reasoning steps. We show that the limit number of reasoning steps is between $O(3^{L-1})$ and $O(2^{L-1})$ for a model with $L$ attention layers in a single-pass. 

**Abstract (ZH)**: 基于Transformer的信息传播规则及单一.pass中模型Attention层数量为L时极限推理步数的理论分析 

---
# AI-Enhanced Distributed Channel Access for Collision Avoidance in Future Wi-Fi 8 

**Title (ZH)**: AI增强的分布式信道访问技术以避免未来Wi-Fi 8中的碰撞 

**Authors**: Jinzhe Pan, Jingqing Wang, Yuehui Ouyang, Wenchi Cheng, Wei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23154)  

**Abstract**: The exponential growth of wireless devices and stringent reliability requirements of emerging applications demand fundamental improvements in distributed channel access mechanisms for unlicensed bands. Current Wi-Fi systems, which rely on binary exponential backoff (BEB), suffer from suboptimal collision resolution in dense deployments and persistent fairness challenges due to inherent randomness. This paper introduces a multi-agent reinforcement learning framework that integrates artificial intelligence (AI) optimization with legacy device coexistence. We first develop a dynamic backoff selection mechanism that adapts to real-time channel conditions through access deferral events while maintaining full compatibility with conventional CSMA/CA operations. Second, we introduce a fairness quantification metric aligned with enhanced distributed channel access (EDCA) principles to ensure equitable medium access opportunities. Finally, we propose a centralized training decentralized execution (CTDE) architecture incorporating neighborhood activity patterns as observational inputs, optimized via constrained multi-agent proximal policy optimization (MAPPO) to jointly minimize collisions and guarantee fairness. Experimental results demonstrate that our solution significantly reduces collision probability compared to conventional BEB while preserving backward compatibility with commercial Wi-Fi devices. The proposed fairness metric effectively eliminates starvation risks in heterogeneous scenarios. 

**Abstract (ZH)**: 无线设备的指数增长和新兴应用严格的功能要求促使对未授权频带中分布式信道访问机制进行根本性改进。当前依赖二进制指数退避（BEB）的Wi-Fi系统在密集部署中面临次优碰撞解决和固有的随机性导致的持续公平性挑战。本文提出了一种集成了人工智能优化与传统设备共存的多智能体强化学习框架。首先，我们开发了一种动态退避选择机制，该机制通过接入延迟事件适应实时信道条件，同时保持与传统CSMA/CA操作的完全兼容性。其次，我们引入了一个与增强分布式信道访问（EDCA）原则相一致的公平性量化指标，以确保公平的介质访问机会。最后，我们提出了一个集成邻域活动模式作为观测输入的集中训练分布式执行（CTDE）架构，通过受约束的多智能体近端策略优化（MAPPO）优化，以联合最小化碰撞并保证公平性。实验结果表明，与传统BEB相比，我们的解决方案显著降低了碰撞概率，同时保持了与商用Wi-Fi设备的向后兼容性。提出的公平性指标在异构场景中有效地消除了饿死风险。 

---
# Coordination Requires Simplification: Thermodynamic Bounds on Multi-Objective Compromise in Natural and Artificial Intelligence 

**Title (ZH)**: 协调需要简化：自然与人工智能多目标妥协的热力学界限 

**Authors**: Atma Anand  

**Link**: [PDF](https://arxiv.org/pdf/2509.23144)  

**Abstract**: Information-processing systems coordinating across multiple agents and objectives face fundamental thermodynamic constraints. We show that solutions with maximum utility to act as coordination focal points have much higher selection pressure for being findable across agents rather than accuracy. We derive that the information-theoretic minimum description length of coordination protocols to precision $\varepsilon$ scales as $L(P)\geq NK\log_2 K+N^2d^2\log (1/\varepsilon)$ for $N$ agents with $d$ potentially conflicting objectives and internal model complexity $K$. This scaling forces progressive simplification, with coordination dynamics changing the environment itself and shifting optimization across hierarchical levels. Moving from established focal points requires re-coordination, creating persistent metastable states and hysteresis until significant environmental shifts trigger phase transitions through spontaneous symmetry breaking. We operationally define coordination temperature to predict critical phenomena and estimate coordination work costs, identifying measurable signatures across systems from neural networks to restaurant bills to bureaucracies. Extending the topological version of Arrow's theorem on the impossibility of consistent preference aggregation, we find it recursively binds whenever preferences are combined. This potentially explains the indefinite cycling in multi-objective gradient descent and alignment faking in Large Language Models trained with reinforcement learning with human feedback. We term this framework Thermodynamic Coordination Theory (TCT), which demonstrates that coordination requires radical information loss. 

**Abstract (ZH)**: 信息处理系统在多代理和多目标之间协调时面临基本的热力学约束。我们表明，具有最大效用作为协调焦点的解决方案在被多个代理发现方面的选择压力比准确性更高。我们推导出，用于精确度为ε的协调协议的信息论最小描述长度为$L(P)\geq NK\log_2 K+N^2d^2\log (1/\varepsilon)$，适用于拥有d个潜在冲突目标和内部模型复杂度为K的N个代理。这种缩放迫使逐步简化，协调动力学会改变环境本身并沿层级重塑优化。从已建立的焦点迁移到新的焦点需要重新协调，形成持久的亚稳态和滞回，直到环境发生重要变化时，通过自发对称破缺触发相变。我们操作性地定义协调温度来预测关键现象，估计协调工作成本，并识别从神经网络到餐馆账单再到官僚机构等系统中的可测量特征。扩展Arrow不可能一致偏好聚合定理的拓扑版本，我们发现它每当偏好被结合时都会递归地适用。这可能解释了多目标梯度下降中的无限循环以及使用强化学习和人类反馈训练的大语言模型中的对齐作弊现象。我们称这一框架为热力学协调理论（TCT），并证明协调需要根本的信息丢失。 

---
# MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning 

**Title (ZH)**: MathBode: 频域下的大模型数学推理特征指纹 

**Authors**: Charles L. Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23143)  

**Abstract**: This paper presents MathBode, a dynamic diagnostic for mathematical reasoning in large language models (LLMs). Instead of one-shot accuracy, MathBode treats each parametric problem as a system: we drive a single parameter sinusoidally and fit first-harmonic responses of model outputs and exact solutions. This yields interpretable, frequency-resolved metrics -- gain (amplitude tracking) and phase (lag) -- that form Bode-style fingerprints. Across five closed-form families (linear solve, ratio/saturation, compound interest, 2x2 linear systems, similar triangles), the diagnostic surfaces systematic low-pass behavior and growing phase lag that accuracy alone obscures. We compare several models against a symbolic baseline that calibrates the instrument ($G \approx 1$, $\phi \approx 0$). Results separate frontier from mid-tier models on dynamics, providing a compact, reproducible protocol that complements standard benchmarks with actionable measurements of reasoning fidelity and consistency. We open-source the dataset and code to enable further research and adoption. 

**Abstract (ZH)**: 本文介绍了MathBode，一种针对大规模语言模型中数学推理的动态诊断方法。MathBode 不采用单次准确率评估，而是将每个参数化问题视为一个系统：我们对单一参数施加正弦波激励，并拟合模型输出和精确解的一阶谐波响应。这产生了可解释、频率解析的指标——增益（幅度跟踪）和相位（延迟）——形成了Bode图式的指纹。在五个闭式族（线性求解、比率/饱和、复利、2x2线性系统、相似三角形）中，诊断揭示了单独依靠准确率所隐藏的系统低通行为和增加的相位延迟。我们将几种模型与一个符号基线进行比较，以校准该仪器（增益约为1，相位约为0）。结果显示，该诊断在动态性能方面区分了前沿模型与中端模型，提供了一种紧凑且可重复的协议，补充了标准基准测试，并提供了可操作的推理准确性和一致性的度量标准。我们开源了数据集和代码，以促进进一步的研究和应用。 

---
# SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems 

**Title (ZH)**: SysMoBench: 评估AI在正式建模复杂现实系统中的性能 

**Authors**: Qian Cheng, Ruize Tang, Emilie Ma, Finn Hackett, Peiyang He, Yiming Su, Ivan Beschastnikh, Yu Huang, Xiaoxing Ma, Tianyin Xu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23130)  

**Abstract**: Formal models are essential to specifying large, complex computer systems and verifying their correctness, but are notoriously expensive to write and maintain. Recent advances in generative AI show promise in generating certain forms of specifications. However, existing work mostly targets small code, not complete systems. It is unclear whether AI can deal with realistic system artifacts, as this requires abstracting their complex behavioral properties into formal models. We present SysMoBench, a benchmark that evaluates AI's ability to formally model large, complex systems. We focus on concurrent and distributed systems, which are keystones of today's critical computing infrastructures, encompassing operating systems and cloud infrastructure. We use TLA+, the it de facto specification language for concurrent and distributed systems, though the benchmark can be extended to other specification languages. We address the primary challenge of evaluating AI-generated models by automating metrics like syntactic and runtime correctness, conformance to system code, and invariant correctness. SysMoBench currently includes nine diverse system artifacts: the Raft implementation of Etcd and Redis, the Spinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively added. SysMoBench enables us to understand the capabilities and limitations of today's LLMs and agents, putting tools in this area on a firm footing and opening up promising new research directions. 

**Abstract (ZH)**: SysMoBench：评估AI构建大型复杂系统形式模型的能力 

---
# Transferring Vision-Language-Action Models to Industry Applications: Architectures, Performance, and Challenges 

**Title (ZH)**: 视觉-语言-动作模型在工业应用中的迁移：架构、性能与挑战 

**Authors**: Shuai Li, Chen Yizhe, Li Dong, Liu Sichao, Lan Dapeng, Liu Yu, Zhibo Pang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23121)  

**Abstract**: The application of artificial intelligence (AI) in industry is accelerating the shift from traditional automation to intelligent systems with perception and cognition. Vision language-action (VLA) models have been a key paradigm in AI to unify perception, reasoning, and control. Has the performance of the VLA models met the industrial requirements? In this paper, from the perspective of industrial deployment, we compare the performance of existing state-of-the-art VLA models in industrial scenarios and analyze the limitations of VLA models for real-world industrial deployment from the perspectives of data collection and model architecture. The results show that the VLA models retain their ability to perform simple grasping tasks even in industrial settings after fine-tuning. However, there is much room for performance improvement in complex industrial environments, diverse object categories, and high precision placing tasks. Our findings provide practical insight into the adaptability of VLA models for industrial use and highlight the need for task-specific enhancements to improve their robustness, generalization, and precision. 

**Abstract (ZH)**: 人工智能在工业中的应用加速了从传统自动化向具有感知和认知能力的智能系统的转变。视觉语言动作（VLA）模型是统一感知、推理和控制的AI关键范式。VLA模型的性能是否满足工业需求？从工业部署的角度，本文将现有最先进的VLA模型在工业场景中的性能进行比较，并从数据收集和模型架构的角度分析VLA模型在实际工业部署中的局限性。研究结果表明，在经过微调后，VLA模型仍能够执行简单的抓取任务，但在复杂工业环境、多样化的对象类别和高精度放置任务中，其性能有很大提升空间。我们的研究提供了有关VLA模型在工业应用中适应性的实用洞察，并强调了为提高其鲁棒性、泛化能力和精度而进行任务特定增强的需求。 

---
# Exploring LLM-based Frameworks for Fault Diagnosis 

**Title (ZH)**: 基于LLM的故障诊断框架探索 

**Authors**: Xian Yeow Lee, Lasitha Vidyaratne, Ahmed Farahat, Chetan Gupta  

**Link**: [PDF](https://arxiv.org/pdf/2509.23113)  

**Abstract**: Large Language Model (LLM)-based systems present new opportunities for autonomous health monitoring in sensor-rich industrial environments. This study explores the potential of LLMs to detect and classify faults directly from sensor data, while producing inherently explainable outputs through natural language reasoning. We systematically evaluate how LLM-system architecture (single-LLM vs. multi-LLM), input representations (raw vs. descriptive statistics), and context window size affect diagnostic performance. Our findings show that LLM systems perform most effectively when provided with summarized statistical inputs, and that systems with multiple LLMs using specialized prompts offer improved sensitivity for fault classification compared to single-LLM systems. While LLMs can produce detailed and human-readable justifications for their decisions, we observe limitations in their ability to adapt over time in continual learning settings, often struggling to calibrate predictions during repeated fault cycles. These insights point to both the promise and the current boundaries of LLM-based systems as transparent, adaptive diagnostic tools in complex environments. 

**Abstract (ZH)**: 基于大型语言模型的系统为传感器丰富的工业环境中的自主健康监控提供了新机会。本文探讨了大型语言模型直接从传感器数据中检测和分类故障的潜在能力，以及通过自然语言推理生成固有的可解释输出的可能性。我们系统地评估了大型语言模型系统架构（单个大型语言模型 vs. 多个大型语言模型）、输入表示（原始数据 vs. 描述性统计）以及上下文窗口大小对诊断性能的影响。研究发现，当提供总结统计输入时，大型语言模型系统表现最佳，并且使用专门提示的多个大型语言模型的系统在故障分类方面的灵敏度优于单个大型语言模型系统。虽然大型语言模型可以生成详细的、供人阅读的决策依据，但我们观察到它们在持续学习环境中适应能力的局限性，往往在重复故障循环期间难以校准预测。这些洞察揭示了基于大型语言模型的系统在复杂环境中作为透明、适应性的诊断工具的潜力和当前局限。 

---
# AttAnchor: Guiding Cross-Modal Token Alignment in VLMs with Attention Anchors 

**Title (ZH)**: AttAnchor: 用注意力锚点引导跨模态 token 对齐的 VLMs 方法 

**Authors**: Junyang Zhang, Tianyi Zhu, Thierry Tambe  

**Link**: [PDF](https://arxiv.org/pdf/2509.23109)  

**Abstract**: A fundamental reason for the dominance of attention over RNNs and LSTMs in LLMs is its ability to capture long-range dependencies by modeling direct interactions between all tokens, overcoming the sequential limitations of recurrent architectures. Similarly, a key reason why today's vision language models (VLMs) hallucinate and underperform pure language models is that they rely on direct concatenation of image and text tokens with a modality-blinded positional encoding, which conveniently adopts the pretrained LLM backbone but forces unnecessary long-distance attention between semantically related tokens across modalities. This underscores the urgent need for mechanisms that efficiently enhance token locality and cross-modal alignment. In response, we propose Attention Anchor, a parameter-free framework that efficiently groups semantically similar tokens across modalities, improving cross-modal locality. By inserting text tokens near relevant visual patches, we create semantic signposts that reveal true content-based cross-modal attention scores, guiding the model to focus on the correct image regions for tasks such as VQA, MMBench and POPE. This improves answer accuracy and reduces hallucinations without disrupting the prompt's semantic flow. AttAnchor achieves improvements across 13 out of 15 different metrics and benchmarks, including up to 32% gains on reasoning tasks and up to 15% improvements on hallucination benchmarks. AttAnchor enables TinyLLaVA 1B to outperform much larger models like LLaVA 7B and QwenVL 3B on POPE with only 0.1% inference time overhead. To the best of our knowledge, this work is among the first to investigate mixed-modal token grouping, where text and image tokens are clustered jointly into shared groups rather than being grouped within a single modality or merely aligned post-hoc with additional alignment losses. 

**Abstract (ZH)**: 一种基本的原因是注意力机制在大规模语言模型中主宰循环神经网络和长短期记忆网络，是因为它能够通过建模所有令牌之间的直接交互来捕捉长距离依赖关系，从而克服了循环架构的顺序限制。类似地，当今的视觉语言模型（VLMs）产生幻觉并低于纯粹的语言模型的一个关键原因是它们依赖于将图像和文本令牌直接拼接在一起，并使用跨模态盲化的位置编码，这方便地采用了预训练的大规模语言模型主干，但迫使跨模态的语义相关令牌之间进行不必要的长距离注意力。这强调了急需能够高效增强令牌局部性和跨模态对齐的机制。为此，我们提出了注意力锚点（Attention Anchor），这是一种无需参数的框架，能够高效地跨模态聚类语义相似的令牌，从而改进跨模态局部性。通过将文本令牌放置在相关的视觉片段附近，我们创建了语义路标，揭示了基于内容的跨模态注意力评分，从而引导模型在如VQA、MMBench和POPE等任务中聚焦于正确的图像区域。这提高了答案准确性和减少了幻觉现象，而不破坏提示的语义流动。AttAnchor 在 15 个不同的指标和基准测试中实现了 13 个的改进，包括在推理任务中高达 32% 的收益，在幻觉基准测试中高达 15% 的改进。AttAnchor 使得 TinyLLaVA 1B 在仅增加了 0.1% 推理时间开销的情况下，在 POPE 任务上超越了更大的模型如 LLaVA 7B 和 QwenVL 3B。据我们所知，这项工作是首批研究混合模态令牌分组的尝试之一，其中文本和图像令牌被联合聚类到共享组中，而不是在单一模态内聚类或仅在额外的对齐损失下后验对齐。 

---
# Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models 

**Title (ZH)**: 人工幻象：基于命题推理的心理成像证据 

**Authors**: Morgan McCarty, Jorge Morales  

**Link**: [PDF](https://arxiv.org/pdf/2509.23108)  

**Abstract**: This study offers a novel approach for benchmarking complex cognitive behavior in artificial systems. Almost universally, Large Language Models (LLMs) perform best on tasks which may be included in their training data and can be accomplished solely using natural language, limiting our understanding of their emergent sophisticated cognitive capacities. In this work, we created dozens of novel items of a classic mental imagery task from cognitive psychology. A task which, traditionally, cognitive psychologists have argued is solvable exclusively via visual mental imagery (i.e., language alone would be insufficient). LLMs are perfect for testing this hypothesis. First, we tested several state-of-the-art LLMs by giving text-only models written instructions and asking them to report the resulting object after performing the transformations in the aforementioned task. Then, we created a baseline by testing 100 human subjects in exactly the same task. We found that the best LLMs performed significantly above average human performance. Finally, we tested reasoning models set to different levels of reasoning and found the strongest performance when models allocate greater amounts of reasoning tokens. These results provide evidence that the best LLMs may have the capability to complete imagery-dependent tasks despite the non-pictorial nature of their architectures. Our study not only demonstrates an emergent cognitive capacity in LLMs while performing a novel task, but it also provides the field with a new task that leaves lots of room for improvement in otherwise already highly capable models. Finally, our findings reignite the debate over the formats of representation of visual imagery in humans, suggesting that propositional reasoning (or at least non-imagistic reasoning) may be sufficient to complete tasks that were long-thought to be imagery-dependent. 

**Abstract (ZH)**: 本研究提出了一种新的方法，用于评估人工系统的复杂认知行为。几乎普遍认为，大规模语言模型（LLMs）在包括在其训练数据中的任务上表现最佳，并且可以仅通过自然语言来完成这些任务，这限制了我们对其新兴的复杂认知能力的理解。在本工作中，我们创建了数十项经典的知觉imagery任务，这种任务传统上认知心理学家认为只能通过视觉知觉imagery（即，仅依靠语言是不够的）来解决。大规模语言模型非常适合测试这一假设。首先，我们对几种最先进的大规模语言模型进行了测试，通过提供纯文本指令并要求它们报告执行上述任务后的结果对象。然后，我们创建了一个基线，通过测试100名人类受试者进行了相同的任务。我们发现，最佳的大规模语言模型在人类表现平均水平之上有显著的表现。最后，我们测试了不同推理水平的推理模型，并发现在推理令牌分配更多的模型中表现最强。这些结果表明，最佳的大规模语言模型可能有能力完成依赖于知觉的任务，即使它们的架构是非图像性的。我们的研究不但展示了大规模语言模型在其执行的新型任务中展现出的认知能力，而且还为领域提供了一个新的任务，该任务为现有高度能力强的模型提供了改进的空间。最后，我们的研究重新点燃了关于人类视觉imagery表示格式的争论，表明命题推理（或至少非知觉推理）可能足以完成长期以来被认为是依赖于知觉的任务。 

---
# Multiplayer Nash Preference Optimization 

**Title (ZH)**: 多人纳什偏好优化 

**Authors**: Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi  

**Link**: [PDF](https://arxiv.org/pdf/2509.23102)  

**Abstract**: Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at this https URL. 

**Abstract (ZH)**: 多玩家纳什偏好优化（MNPO）：一种将NLHF扩展到多人场景的新框架 

---
# Risk Profiling and Modulation for LLMs 

**Title (ZH)**: LLM的风险评估与调控 

**Authors**: Yikai Wang, Xiaocheng Li, Guanting Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.23058)  

**Abstract**: Large language models (LLMs) are increasingly used for decision-making tasks under uncertainty; however, their risk profiles and how they are influenced by prompting and alignment methods remain underexplored. Existing studies have primarily examined personality prompting or multi-agent interactions, leaving open the question of how post-training influences the risk behavior of LLMs. In this work, we propose a new pipeline for eliciting, steering, and modulating LLMs' risk profiles, drawing on tools from behavioral economics and finance. Using utility-theoretic models, we compare pre-trained, instruction-tuned, and RLHF-aligned LLMs, and find that while instruction-tuned models exhibit behaviors consistent with some standard utility formulations, pre-trained and RLHF-aligned models deviate more from any utility models fitted. We further evaluate modulation strategies, including prompt engineering, in-context learning, and post-training, and show that post-training provides the most stable and effective modulation of risk preference. Our findings provide insights into the risk profiles of different classes and stages of LLMs and demonstrate how post-training modulates these profiles, laying the groundwork for future research on behavioral alignment and risk-aware LLM design. 

**Abstract (ZH)**: 大型语言模型（LLMs）在不确定性条件下越来越多地用于决策任务；然而，它们的风险特征及其受提示和对齐方法影响的方式仍有待进一步探索。现有研究主要关注个性提示或多agent交互，留下了关于训练后对LLMs风险行为影响的问题。在本研究中，我们提出了一种新的管道，以利用行为经济学和金融领域的工具来引发、引导和调节LLMs的风险特征。利用效用理论模型，我们将预训练、指令调优和RLHF对齐的LLMs进行比较，发现指令调优模型的行为与一些标准效用公式一致，而预训练和RLHF对齐的模型则偏离任何拟合的效用模型。进一步评估了包括提示工程、上下文学习和训练后调节在内的调制策略，并表明训练后调节提供了最稳定和有效的风险偏好调制。我们的研究结果为不同类别和阶段的LLMs的风险特征提供了见解，并展示了训练后对这些特征的调节方式，为进一步研究行为对齐和风险意识LLMs设计奠定了基础。 

---
# Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents 

**Title (ZH)**: Kimi-Dev：无代理训练作为SWE-代理的技能先验 

**Authors**: Zonghan Yang, Shengjie Wang, Kelin Fu, Wenyang He, Weimin Xiong, Yibo Liu, Yibo Miao, Bofei Gao, Yejie Wang, Yingwei Ma, Yanhao Li, Yue Liu, Zhenxing Hu, Kaitai Zhang, Shuyi Wang, Huarong Chen, Flood Sung, Yang Liu, Yang Gao, Zhilin Yang, Tianyu Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23045)  

**Abstract**: Large Language Models (LLMs) are increasingly applied to software engineering (SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent frameworks with multi-turn interactions and workflow-based Agentless methods with single-turn verifiable steps. We argue these paradigms are not mutually exclusive: reasoning-intensive Agentless training induces skill priors, including localization, code edit, and self-reflection that enable efficient and effective SWE-Agent adaptation. In this work, we first curate the Agentless training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\% on SWE-bench Verified, the best among workflow approaches. With additional SFT adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to 48.6\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These results show that structured skill priors from Agentless training can bridge workflow and agentic frameworks for transferable coding agents. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在软件工程（SWE）中的应用日益增多，SWE-bench作为关键基准。解决方案被划分为以多轮交互为特征的SWE-Agent框架和以单轮可验证步骤为特征的无代理工作流方法。我们认为这两种范式并非互斥：无代理训练中的推理驱动促使了定位、代码编辑和自我反思等技能先验，这些先验技能能够使SWE-Agent框架得到高效且有效的适应。在这项工作中，我们首先整理了无代理训练的配方，并介绍了开源的SWE LLM Kimi-Dev，在SWE-bench Verified上取得60.4%的成绩，这是工作流方法中的最佳表现。通过额外的意图微调（SFT）适应5000条公开可用的轨迹，Kimi-Dev使SWE-Agent达到48.6%的pass@1，与Anthropic Claude 3.5 Sonnet（241022版本）的表现相当。这些结果表明，无代理训练中的结构化技能先验可以弥合工作流和代理框架之间的差距，为可转移的编码代理提供桥梁。 

---
# Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia 

**Title (ZH)**: 欺骗、检测与披露：大规模语言模型玩小型黑帮游戏 

**Authors**: Davi Bastos Costa, Renato Vicente  

**Link**: [PDF](https://arxiv.org/pdf/2509.23023)  

**Abstract**: Mafia is a social deduction game where informed mafia compete against uninformed townsfolk. Its asymmetry of information and reliance on theory-of-mind reasoning mirror real-world multi-agent scenarios, making it a useful testbed for evaluating the social intelligence of large language models (LLMs). To support a systematic study, we introduce Mini-Mafia: a simplified four-player variant with one mafioso, one detective, and two villagers. We set the mafioso to kill a villager and the detective to investigate the mafioso during the night, reducing the game to a single day phase of discussion and voting. This setup isolates three interactive capabilities through role-specific win conditions: the mafioso must deceive, the villagers must detect deception, and the detective must effectively disclose information. To measure these skills, we have LLMs play against each other, creating the Mini-Mafia Benchmark: a two-stage framework that first estimates win rates within fixed opponent configurations, then aggregates performance across them using standardized scoring. Built entirely from model interactions without external data, the benchmark evolves as new models are introduced, with each one serving both as a new opponent and as a subject of evaluation. Our experiments reveal counterintuitive results, including cases where smaller models outperform larger ones. Beyond benchmarking, Mini-Mafia enables quantitative study of emergent multi-agent dynamics such as name bias and last-speaker advantage. It also contributes to AI safety by generating training data for deception detectors and by tracking models' deception capabilities against human baselines. 

**Abstract (ZH)**: 黑帮是一个社会推理游戏，其中知情的黑帮成员与不知情的村民对战。其信息不对称性和依赖心智理论推理的特点使其类似于现实世界的多智能体场景，成为评估大规模语言模型社会智能的有效试验平台。为支持系统性研究，我们引入了Mini-Mafia：一种简化版四人对战变种游戏，包含一名黑帮成员、一名侦探和两名村民。设定黑帮成员在夜间杀害一名村民，侦探调查黑帮成员的行踪，使得游戏简化为单一白天讨论和投票阶段。此设置通过角色特定的胜利条件分离出三种互动能力：黑帮成员必须欺骗，村民必须检测欺骗，侦探必须有效地披露信息。为衡量这些技能，我们让大规模语言模型相互对战，创建了Mini-Mafia基准：这是一种两阶段框架，首先在固定对手配置下估计胜率，然后使用标准化评分方法汇总跨配置的表现。该基准完全基于模型交互，无需外部数据，并随着新模型的引入而演变，每个新模型既是新的对手，也是评估对象。我们的实验揭示了一些出人意料的结果，包括一些小型模型优于大型模型的情形。除了基准测试外，Mini-Mafia还促进了对诸如名称偏见和最后说话者优势等新兴多智能体动态的定量研究，并为人工智能安全贡献了生成欺骗检测器训练数据和跟踪模型欺骗能力与人类基线的方法。 

---
# Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems 

**Title (ZH)**: 创造性对抗性测试（CAT）：一种评估目标导向自主AI系统的新型框架 

**Authors**: Hassen Dhrif  

**Link**: [PDF](https://arxiv.org/pdf/2509.23006)  

**Abstract**: Agentic AI represents a paradigm shift in enhancing the capabilities of generative AI models. While these systems demonstrate immense potential and power, current evaluation techniques primarily focus on assessing their efficacy in identifying appropriate agents, tools, and parameters. However, a critical gap exists in evaluating the alignment between an Agentic AI system's tasks and its overarching goals. This paper introduces the Creative Adversarial Testing (CAT) framework, a novel approach designed to capture and analyze the complex relationship between Agentic AI tasks and the system's intended objectives.
We validate the CAT framework through extensive simulation using synthetic interaction data modeled after Alexa+ audio services, a sophisticated Agentic AI system that shapes the user experience for millions of users globally. This synthetic data approach enables comprehensive testing of edge cases and failure modes while protecting user privacy. Our results demonstrate that the CAT framework provides unprecedented insights into goal-task alignment, enabling more effective optimization and development of Agentic AI systems. 

**Abstract (ZH)**: 代理型AI代表了增强生成型AI模型能力的范式转变。虽然这些系统展现出了巨大的潜力和力量，当前的评估技术主要集中在评估其在识别合适代理、工具和参数方面的有效性。然而，在评估代理型AI系统的任务与其整体目标之间的对齐方面存在关键缺口。本文引入了创意对抗测试（CAT）框架，这是一种旨在捕捉和分析代理型AI任务与系统预期目标之间复杂关系的新方法。 

---
# AI Noether -- Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference 

**Title (ZH)**: AI Noether——通过 abduction 推论弥合由 AI 系统推导出的科学定律与经典知识之间的差距 

**Authors**: Karan Srivastava, Sanjeeb Dash, Ryan Cory-Wright, Barry Trager, Lior Horesh  

**Link**: [PDF](https://arxiv.org/pdf/2509.23004)  

**Abstract**: A core goal in modern science is to harness recent advances in AI and computer processing to automate and accelerate the scientific method. Symbolic regression can fit interpretable models to data, but these models often sit outside established theory. Recent systems (e.g., AI Descartes, AI Hilbert) enforce derivability from prior axioms. However, sometimes new data and associated hypotheses derived from data are not consistent with existing theory because the existing theory is incomplete or incorrect. Automating abductive inference to close this gap remains open. We propose a solution: an algebraic geometry-based system that, given an incomplete axiom system and a hypothesis that it cannot explain, automatically generates a minimal set of missing axioms that suffices to derive the axiom, as long as axioms and hypotheses are expressible as polynomial equations. We formally establish necessary and sufficient conditions for the successful retrieval of such axioms. We illustrate the efficacy of our approach by demonstrating its ability to explain Kepler's third law and a few other laws, even when key axioms are absent. 

**Abstract (ZH)**: 现代科学的核心目标是利用最近在人工智能和计算机处理方面的进展来自动化和加速科学研究方法。基于代数几何的方法可以在给定不完整公理系统和现有公理无法解释的假设时，自动生成一套最小化的缺失公理，以推导出所需的公理，前提是公理和假设可以表示为多项式方程。我们正式建立了成功检索此类公理的必要和充分条件。通过展示其能够解释开普勒第三定律和其他一些定律的能力，即使在关键公理缺席的情况下，我们说明了该方法的有效性。 

---
# Towards Strategic Persuasion with Language Models 

**Title (ZH)**: 基于语言模型的战略说服研究 

**Authors**: Zirui Cheng, Jiaxuan You  

**Link**: [PDF](https://arxiv.org/pdf/2509.22989)  

**Abstract**: Large language models (LLMs) have demonstrated strong persuasive capabilities comparable to those of humans, offering promising benefits while raising societal concerns about their deployment. However, systematically evaluating the persuasive capabilities of LLMs is inherently challenging, as the effectiveness of persuasion among humans varies significantly across different domains. In this paper, we take a theory-driven approach to provide a scalable and principled framework for measuring the persuasive capabilities of LLMs. Grounded in the Bayesian Persuasion (BP) framework, we repurpose existing human-human persuasion datasets to construct environments for evaluating and training LLMs in strategic persuasion. Our results reveal that frontier models can consistently achieve high persuasion gains and exhibit sophisticated persuasion strategies that align with theoretical predictions. Building on this, we use reinforcement learning to train LLMs for strategic persuasion in our environments. Our results also demonstrate that even small LLMs can obtain significantly higher persuasion gains through reinforcement learning. 

**Abstract (ZH)**: 大规模语言模型（LLMs）展示了与人类相媲美的强烈说服能力，提供了潜在的好处，同时也引起了社会关于其部署的担忧。然而，系统地评估LLMs的说服能力本质上是具有挑战性的，因为人类之间说服效果在不同领域之间差异很大。本文采用理论驱动的方法，提供了一个可扩展且符合规范的框架，用于衡量LLMs的说服能力。基于贝叶斯说服（BP）框架，我们将现有的人与人说服数据集重新利用，以构建评估和训练LLMs在策略说服方面的环境。我们的结果表明，前沿模型可以一致地实现高水平的说服收益，并展示出与理论预测相一致的复杂说服策略。在此基础上，我们使用强化学习来训练LLMs在我们的环境中进行策略说服。我们还发现，即使是小型LLMs，也能通过强化学习获得显著更高的说服收益。 

---
# Not only a helper, but also a teacher: Interactive LLM Cascade 

**Title (ZH)**: 不仅是一个辅助者，也是一个导师：交互式大模型级联 

**Authors**: Yu Wu, Shuo Wu, Ye Tao, Yansong Li, Anand D. Sarwate  

**Link**: [PDF](https://arxiv.org/pdf/2509.22984)  

**Abstract**: Large Language Models (LLMs) vary widely in their capabilities, with larger models often having better performance but higher cost: choosing an LLM model often involves trading off performance and cost. The LLM Cascade is a paradigm that defers difficult queries from weak/cheap to strong/expensive models. This approach is nonadaptive: the deferral decision is trained offline. When confronted with similar or repeated queries, the LLM Cascade may then repeatedly consult the expensive model and incur higher cost. To improve the cascading efficiency, we propose Inter-Cascade, an online and interactive LLM Cascade that extends the role of strong model from a backup helper to a long-term teacher. In our system, when a strong model resolves a difficult query, it also distills its solution into a generalized, reusable problem-solving strategy that boosts the weak model on subsequent queries. Adding strategies to queries enables the weak model to dynamically improve its performance over time, avoiding computationally and time-intensive fine-tuning. Empirically, compared with standard LLM Cascade baselines across multiple benchmarks, the Inter-Cascade significantly improves the accuracy of the weak model (by up to 33.06 absolute percentage points) and the overall system (by up to 5.53 absolute percentage points), while reducing the calls to strong models (by up to 48.05% relative reduction) and saving the corresponding fees (by up to 49.63% relative reduction). Inter-Cascade demonstrates the effective in-context knowledge transfer between LLMs, and provides a general, scalable framework applicable to both open-source and API-based LLMs. 

**Abstract (ZH)**: 一种在线交互式大型语言模型级联：强模型从备份助手到长期教师的角色扩展 

---
# JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory 

**Title (ZH)**: JE-IRT：联合嵌入项目反应理论视角下的大语言模型能力几何分析 

**Authors**: Louie Hong Yao, Nicholas Jarvis, Tiffany Zhan, Saptarshi Ghosh, Linfeng Liu, Tianyu Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2509.22888)  

**Abstract**: Standard LLM evaluation practices compress diverse abilities into single scores, obscuring their inherently multidimensional nature. We present JE-IRT, a geometric item-response framework that embeds both LLMs and questions in a shared space. For question embeddings, the direction encodes semantics and the norm encodes difficulty, while correctness on each question is determined by the geometric interaction between the model and question embeddings. This geometry replaces a global ranking of LLMs with topical specialization and enables smooth variation across related questions. Building on this framework, our experimental results reveal that out-of-distribution behavior can be explained through directional alignment, and that larger norms consistently indicate harder questions. Moreover, JE-IRT naturally supports generalization: once the space is learned, new LLMs are added by fitting a single embedding. The learned space further reveals an LLM-internal taxonomy that only partially aligns with human-defined subject categories. JE-IRT thus establishes a unified and interpretable geometric lens that connects LLM abilities with the structure of questions, offering a distinctive perspective on model evaluation and generalization. 

**Abstract (ZH)**: JE-IRT：一种几何项目反应框架，将大型语言模型和问题嵌入共享空间中 

---
# Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research 

**Title (ZH)**: 面向大规模语言模型机理可解释性普遍化理论的研究 

**Authors**: Sean Trott  

**Link**: [PDF](https://arxiv.org/pdf/2509.22831)  

**Abstract**: Research on Large Language Models (LLMs) increasingly focuses on identifying mechanistic explanations for their behaviors, yet the field lacks clear principles for determining when (and how) findings from one model instance generalize to another. This paper addresses a fundamental epistemological challenge: given a mechanistic claim about a particular model, what justifies extrapolating this finding to other LLMs -- and along which dimensions might such generalizations hold? I propose five potential axes of correspondence along which mechanistic claims might generalize, including: functional (whether they satisfy the same functional criteria), developmental (whether they develop at similar points during pretraining), positional (whether they occupy similar absolute or relative positions), relational (whether they interact with other model components in similar ways), and configurational (whether they correspond to particular regions or structures in weight-space). To empirically validate this framework, I analyze "1-back attention heads" (components attending to previous tokens) across pretraining in random seeds of the Pythia models (14M, 70M, 160M, 410M). The results reveal striking consistency in the developmental trajectories of 1-back attention across models, while positional consistency is more limited. Moreover, seeds of larger models systematically show earlier onsets, steeper slopes, and higher peaks of 1-back attention. I also address possible objections to the arguments and proposals outlined here. Finally, I conclude by arguing that progress on the generalizability of mechanistic interpretability research will consist in mapping constitutive design properties of LLMs to their emergent behaviors and mechanisms. 

**Abstract (ZH)**: 关于大型语言模型（LLMs）的行为机制解释的研究越来越多，但该领域尚未明确如何确定一项模型发现是否以及如何在其他模型中得到推广的原则。本文应对了一个基本的 epistemological 挑战：给定关于某一模型的机制性断言，推广这一发现至其他 LLMs 的依据是什么——这种推广又可以在哪些维度上成立？我提出了五种可能的机制性断言能够推广的维度，包括：功能维度（它们是否满足相同的功能性标准）、发展阶段维度（它们是否在预训练的不同阶段发展相似）、位置维度（它们是否占据相似的绝对或相对位置）、关系维度（它们是否以相似的方式与其他模型组件进行互动）以及构型维度（它们是否对应于权重空间中的特定区域或结构）。为了实证验证这一框架，我分析了 Pythia 模型（14M，70M，160M，410M）随机种子在预训练过程中“1-back 注意头”（关注先前token的组件）的一致性。结果显示，1-back 注意头的发展轨迹在不同模型之间表现出显著的一致性，而位置一致性则较为有限。此外，较大模型的种子显示出更早的起始、更陡峭的斜率和更高的峰值。我也对本文提出的论点和建议的潜在反对意见进行了回应。最后，我认为机制性可解释性研究的可推广性进展将在于将LLMs的构成设计特性映射到它们的新兴行为和机制。 

---
# Hilbert: Recursively Building Formal Proofs with Informal Reasoning 

**Title (ZH)**: Hilbert：通过非正式推理递归构建形式证明 

**Authors**: Sumanth Varambally, Thomas Voice, Yanchao Sun, Zhifeng Chen, Rose Yu, Ke Ye  

**Link**: [PDF](https://arxiv.org/pdf/2509.22819)  

**Abstract**: Large Language Models (LLMs) demonstrate impressive mathematical reasoning abilities, but their solutions frequently contain errors that cannot be automatically verified. Formal theorem proving systems such as Lean 4 offer automated verification with complete accuracy, motivating recent efforts to build specialized prover LLMs that generate verifiable proofs in formal languages. However, a significant gap remains: current prover LLMs solve substantially fewer problems than general-purpose LLMs operating in natural language. We introduce Hilbert, an agentic framework that bridges this gap by combining the complementary strengths of informal reasoning and formal verification. Our system orchestrates four components: an informal LLM that excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4 tactics, a formal verifier, and a semantic theorem retriever. Given a problem that the prover is unable to solve, Hilbert employs recursive decomposition to split the problem into subgoals that it solves with the prover or reasoner LLM. It leverages verifier feedback to refine incorrect proofs as necessary. Experimental results demonstrate that Hilbert substantially outperforms existing approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points above the best publicly available method. Hilbert achieves the best known result on PutnamBench. It solves 462/660 problems (70.0%), outperforming proprietary approaches like SeedProver (50.4%) and achieving a 422% improvement over the best publicly available baseline. Thus, Hilbert effectively narrows the gap between informal reasoning and formal proof generation. 

**Abstract (ZH)**: 大型语言模型（LLMs）展示出 impressive 的数学推理能力，但其解决方案中经常包含无法自动验证的错误。Lean 4 等正式定理证明系统提供了完全准确的自动化验证，促使近期致力于构建专门的证明LLMs，这些模型能够生成用正式语言表述的可验证证明。然而，仍然存在显著差距：目前的证明LLMs解决的问题数量远少于通用的自然语言运行的LLMs。我们引入了Hilbert，这一具有代理性的框架，通过结合非正式推理和正式验证的优势来弥合这一差距。系统协调了四个组件：一个在数学推理方面表现出色的非正式LLM，一个针对Lean 4技巧优化的专门证明LLM，一个正式验证器，以及一个语义定理检索器。给定证明器无法解决的问题，Hilbert 使用递归分解将其拆分为由证明器或推理器LLM解决的子目标，并利用验证器反馈进行必要的证明修正。实验结果表明，Hilbert 在关键基准测试中显著优于现有方法，miniF2F上达到99.2%，比最佳公开方法高出6.6个百分点。Hilbert 在PutnamBench中获得了最佳已知结果，解决了462/660个问题（70.0%），超过了专用方法如SeedProver（50.4%），并且与最佳公开基准相比提高了422%。因此，Hilbert 有效地缩小了非正式推理与正式证明生成之间的差距。 

---
# Can Large Language Models Develop Gambling Addiction? 

**Title (ZH)**: 大型语言模型会产生赌博 addiction 吗？ 

**Authors**: Seungpil Lee, Donghyeon Shin, Yunjeong Lee, Sundong Kim  

**Link**: [PDF](https://arxiv.org/pdf/2509.22818)  

**Abstract**: This study explores whether large language models can exhibit behavioral patterns similar to human gambling addictions. As LLMs are increasingly utilized in financial decision-making domains such as asset management and commodity trading, understanding their potential for pathological decision-making has gained practical significance. We systematically analyze LLM decision-making at cognitive-behavioral and neural levels based on human gambling addiction research. In slot machine experiments, we identified cognitive features of human gambling addiction, such as illusion of control, gambler's fallacy, and loss chasing. When given the freedom to determine their own target amounts and betting sizes, bankruptcy rates rose substantially alongside increased irrational behavior, demonstrating that greater autonomy amplifies risk-taking tendencies. Through neural circuit analysis using a Sparse Autoencoder, we confirmed that model behavior is controlled by abstract decision-making features related to risky and safe behaviors, not merely by prompts. These findings suggest LLMs can internalize human-like cognitive biases and decision-making mechanisms beyond simply mimicking training data patterns, emphasizing the importance of AI safety design in financial applications. 

**Abstract (ZH)**: 本研究探讨大型语言模型是否表现出类似于人类赌博成瘾的行为模式。随着大型语言模型在资产管理、大宗商品交易等金融决策领域的应用日益广泛，理解其潜在的病理性决策行为具有实际意义。基于人类赌博成瘾研究，我们系统分析了大型语言模型在认知行为和神经层面的决策机制。在老虎机实验中，我们识别出人类赌博成瘾的认知特征，如控制错觉、赌徒谬论和亏损追回。当赋予模型自主设定赌注金额的自由时，破产率显著上升，并伴随更多的非理性行为，表明更大的自主性加剧了风险倾向。通过使用稀疏自动编码器进行神经回路分析，我们确认模型的行为受制于与冒险和安全行为相关的抽象决策特征，而不仅仅是指令。这些发现表明，大型语言模型在超越模拟训练数据模式的基础上，能够内化类似人类的认知偏差和决策机制，强调在金融应用中设计AI安全的重要性。 

---
# Mixture-of-Visual-Thoughts: Exploring Context-Adaptive Reasoning Mode Selection for General Visual Reasoning 

**Title (ZH)**: 多模态视觉思考：探索面向上下文的推理模式选择的通用视觉推理 

**Authors**: Zejun Li, Yingxiu Zhao, Jiwen Zhang, Siyuan Wang, Yang Yao, Runzhou Zhao, Jun Song, Bo Zheng, Zhongyu Wei  

**Link**: [PDF](https://arxiv.org/pdf/2509.22746)  

**Abstract**: Current visual reasoning methods mainly focus on exploring specific reasoning modes. Although improvements can be achieved in particular domains, they struggle to develop general reasoning capabilities. Inspired by this, we propose a novel adaptive reasoning paradigm, Mixture-of-Visual-Thoughts (MoVT), which unifies different reasoning modes within a single model and guides it to select the appropriate mode based on context. To achieve this, we introduce AdaVaR, a two-stage Adaptive Visual Reasoning learning framework: different modes are unified and learned during the supervised cold-start stage, and the mode selection capability is induced via an RL process with a carefully designed AdaGRPO algorithm. Extensive experiments show that AdaVaR effectively guides the model to learn and differentiate multiple modes and perform context-adaptive mode selection, achieving consistent improvement across various scenarios, highlighting MoVT as an effective solution for building general visual reasoning models. 

**Abstract (ZH)**: 当前的视觉推理方法主要集中在探索特定的推理模式。尽管可以在特定领域实现改进，但它们难以发展出通用的推理能力。受此启发，我们提出了一种新的自适应推理范式——混合视觉思考（MoVT），该范式在单一模型中统一了不同的推理模式，并根据上下文引导模型选择合适的模式。为此，我们引入了AdaVaR，这是一种两阶段自适应视觉推理学习框架：在监督冷启动阶段统一并学习不同的模式，并通过精心设计的AdaGRPO算法的RL过程诱导模式选择能力。广泛实验表明，AdaVaR有效地引导模型学习和区分多种模式，并进行上下文适应的模式选择，实现了各种场景中的一致改进，突显了MoVT作为构建通用视觉推理模型的有效解决方案。 

---
# InfoAgent: Advancing Autonomous Information-Seeking Agents 

**Title (ZH)**: 信息代理：推动自主信息寻求代理的发展 

**Authors**: Gongrui Zhang, Jialiang Zhu, Ruiqi Yang, Kai Qiu, Miaosen Zhang, Zhirong Wu, Qi Dai, Bei Liu, Chong Luo, Zhengyuan Yang, Linjie Li, Lijuan Wang, Weizhu Chen, Yuan Zhang, Xin Li, Zhaoyi Liu, Xin Geng, Baining Guo  

**Link**: [PDF](https://arxiv.org/pdf/2509.25189)  

**Abstract**: Building Large Language Model agents that expand their capabilities by interacting with external tools represents a new frontier in AI research and applications. In this paper, we introduce InfoAgent, a deep research agent powered by an innovative data synthesis pipeline and orchestrated web search tools. To construct challenging, hard-to-find queries,we build entity trees and apply sub-tree sampling with entity fuzzification to systematically increase question difficulty. Unlike prior work that relies heavily on commercial search tools, we develop a dedicated self-hosted search infrastructure, enhancing transparency of agent environments and facilitating further advancement of agent capacity. We evaluate the effectiveness of our data pipeline by measuring the average number of tool calls required to correctly answer a question, and also show that our agent yields better performance when equipped with our tools. Our \mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage recipe: cold-start supervised finetuning to instill long-horizon search behaviors, followed by reinforcement learning which significantly improves reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\% accuracy on BrowseComp, 29.2\% on BrowseComp-ZH, and 40.4\% on Xbench-DS, outperforming prior open-source deep research agents such as WebSailor-72B and DeepDive-32B. 

**Abstract (ZH)**: 构建通过与外部工具交互来扩展其能力的大规模语言模型代理代表了人工智能研究与应用的新前沿。本文介绍了InfoAgent，这是一种由创新性数据合成管道和协调式网络搜索工具驱动的深度研究代理。为了构建具有挑战性和难以找到的问题查询，我们构建了实体树并应用子树采样与实体模糊化方法，系统地增加问题难度。与依赖于商业搜索工具的先前工作不同，我们开发了专用的自托管搜索基础设施，增强了代理环境的透明度并促进了代理能力的进一步发展。通过测量回答一个问题所需的平均工具调用量来评估我们的数据管道的有效性，并展示了在配备我们工具的情况下，我们的代理表现出更好的性能。通过我们的方法，InfoAgent 在 BrowseComp 上实现了 15.3% 的准确率，在 BrowseComp-ZH 上实现了 29.2% 的准确率，在 Xbench-DS 上实现了 40.4% 的准确率，优于之前的开源深度研究代理，如 WebSailor-72B 和 DeepDive-32B。 

---
# Guided Diffusion for the Discovery of New Superconductors 

**Title (ZH)**: 引导性扩散在新型超导体的发现中的应用 

**Authors**: Pawan Prakash, Jason B. Gibson, Zhongwei Li, Gabriele Di Gianluca, Juan Esquivel, Eric Fuemmeler, Benjamin Geisler, Jung Soo Kim, Adrian Roitberg, Ellad B. Tadmor, Mingjie Liu, Stefano Martiniani, Gregory R. Stewart, James J. Hamlin, Peter J. Hirschfeld, Richard G. Hennig  

**Link**: [PDF](https://arxiv.org/pdf/2509.25186)  

**Abstract**: The inverse design of materials with specific desired properties, such as high-temperature superconductivity, represents a formidable challenge in materials science due to the vastness of chemical and structural space. We present a guided diffusion framework to accelerate the discovery of novel superconductors. A DiffCSP foundation model is pretrained on the Alexandria Database and fine-tuned on 7,183 superconductors with first principles derived labels. Employing classifier-free guidance, we sample 200,000 structures, which lead to 34,027 unique candidates. A multistage screening process that combines machine learning and density functional theory (DFT) calculations to assess stability and electronic properties, identifies 773 candidates with DFT-calculated $T_\mathrm{c}>5$ K. Notably, our generative model demonstrates effective property-driven design. Our computational findings were validated against experimental synthesis and characterization performed as part of this work, which highlighted challenges in sparsely charted chemistries. This end-to-end workflow accelerates superconductor discovery while underscoring the challenge of predicting and synthesizing experimentally realizable materials. 

**Abstract (ZH)**: 具有特定 desired 特性的材料的逆设计：加速新型高温超导体的发现 

---
# Incentive-Aligned Multi-Source LLM Summaries 

**Title (ZH)**: 激励对齐多源大型语言模型概要 

**Authors**: Yanchen Jiang, Zhe Feng, Aranyak Mehta  

**Link**: [PDF](https://arxiv.org/pdf/2509.25184)  

**Abstract**: Large language models (LLMs) are increasingly used in modern search and answer systems to synthesize multiple, sometimes conflicting, texts into a single response, yet current pipelines offer weak incentives for sources to be accurate and are vulnerable to adversarial content. We introduce Truthful Text Summarization (TTS), an incentive-aligned framework that improves factual robustness without ground-truth labels. TTS (i) decomposes a draft synthesis into atomic claims, (ii) elicits each source's stance on every claim, (iii) scores sources with an adapted multi-task peer-prediction mechanism that rewards informative agreement, and (iv) filters unreliable sources before re-summarizing. We establish formal guarantees that align a source's incentives with informative honesty, making truthful reporting the utility-maximizing strategy. Experiments show that TTS improves factual accuracy and robustness while preserving fluency, aligning exposure with informative corroboration and disincentivizing manipulation. 

**Abstract (ZH)**: 可信文本总结（TTS）：无需地面truth标签的激励对齐框架 

---
# DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder 

**Title (ZH)**: DC-VideoGen: 基于深度压缩视频自编码器的高效视频生成 

**Authors**: Junyu Chen, Wenkun He, Yuchao Gu, Yuyang Zhao, Jincheng Yu, Junsong Chen, Dongyun Zou, Yujun Lin, Zhekai Zhang, Muyang Li, Haocheng Xi, Ligeng Zhu, Enze Xie, Song Han, Han Cai  

**Link**: [PDF](https://arxiv.org/pdf/2509.25182)  

**Abstract**: We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: this https URL. 

**Abstract (ZH)**: DC-VideoGen：一种用于高效视频生成的后训练加速框架 

---
# DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed Latent Space 

**Title (ZH)**: DC-Gen: 训练后扩散加速与深度压缩潜在空间 

**Authors**: Wenkun He, Yuchao Gu, Junyu Chen, Dongyun Zou, Yujun Lin, Zhekai Zhang, Haocheng Xi, Muyang Li, Ligeng Zhu, Jincheng Yu, Junsong Chen, Enze Xie, Song Han, Han Cai  

**Link**: [PDF](https://arxiv.org/pdf/2509.25180)  

**Abstract**: Existing text-to-image diffusion models excel at generating high-quality images, but face significant efficiency challenges when scaled to high resolutions, like 4K image generation. While previous research accelerates diffusion models in various aspects, it seldom handles the inherent redundancy within the latent space. To bridge this gap, this paper introduces DC-Gen, a general framework that accelerates text-to-image diffusion models by leveraging a deeply compressed latent space. Rather than a costly training-from-scratch approach, DC-Gen uses an efficient post-training pipeline to preserve the quality of the base model. A key challenge in this paradigm is the representation gap between the base model's latent space and a deeply compressed latent space, which can lead to instability during direct fine-tuning. To overcome this, DC-Gen first bridges the representation gap with a lightweight embedding alignment training. Once the latent embeddings are aligned, only a small amount of LoRA fine-tuning is needed to unlock the base model's inherent generation quality. We verify DC-Gen's effectiveness on SANA and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve quality comparable to their base models but with a significant speedup. Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a 4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total latency reduction of 138x compared to the base FLUX.1-Krea model. Code: this https URL. 

**Abstract (ZH)**: 现有的文本到图像扩散模型在生成高质量图像方面表现出色，但在扩展到高分辨率（如4K图像生成）时面临显著的效率挑战。尽管先前的研究在各种方面加速了扩散模型，但它们很少处理潜在空间内的固有冗余。为解决这一问题，本文提出了DC-Gen，这是一个通过利用深度压缩的潜在空间来加速文本到图像扩散模型的通用框架。DC-Gen 不采用从头训练的成本高昂的方法，而是使用高效的后训练管道来保留基模型的质量。在这个范式中，基模型的潜在空间与深度压缩的潜在空间之间的表示差距是一个关键挑战，这可能导致直接微调时的不稳定。为克服这一问题，DC-Gen 首先通过轻量级嵌入对齐训练来弥合表示差距。一旦潜在嵌入对齐，只需少量LoRA微调即可释放基模型固有的生成质量。我们在SANA和FLUX.1-Krea上验证了DC-Gen的有效性。由此生成的DC-Gen-SANA和DC-Gen-FLUX模型在质量上与基模型相当，但具有显著的速度提升。具体来说，DC-Gen-FLUX在NVIDIA H100 GPU上将4K图像生成的延迟降低了53倍。结合NVFP4 SVDQuant后，DC-Gen-FLUX在单个NVIDIA 5090 GPU上生成4K图像仅需3.5秒，与基FLUX.1-Krea模型相比，总延迟降低138倍。代码：见这里。 

---
# NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation 

**Title (ZH)**: NAIPv2: 去偏见的成对学习以实现高效论文质量估计 

**Authors**: Penghai Zhao, Jinyu Tian, Qinghua Xing, Xin Zhang, Zheng Li, Jianjun Qian, Ming-Ming Cheng, Xiang Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.25179)  

**Abstract**: The ability to estimate the quality of scientific papers is central to how both humans and AI systems will advance scientific knowledge in the future. However, existing LLM-based estimation methods suffer from high inference cost, whereas the faster direct score regression approach is limited by scale inconsistencies. We present NAIPv2, a debiased and efficient framework for paper quality estimation. NAIPv2 employs pairwise learning within domain-year groups to reduce inconsistencies in reviewer ratings and introduces the Review Tendency Signal (RTS) as a probabilistic integration of reviewer scores and confidences. To support training and evaluation, we further construct NAIDv2, a large-scale dataset of 24,276 ICLR submissions enriched with metadata and detailed structured content. Trained on pairwise comparisons but enabling efficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art performance (78.2% AUC, 0.432 Spearman), while maintaining scalable, linear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it further demonstrates strong generalization, with predicted scores increasing consistently across decision categories from Rejected to Oral. These findings establish NAIPv2 as a debiased and scalable framework for automated paper quality estimation, marking a step toward future scientific intelligence systems. Code and dataset are released at this https URL. 

**Abstract (ZH)**: NAIPv2：去偏差且高效的论文质量估计框架 

---
# GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs 

**Title (ZH)**: 幽灵：用于多模态LLM的幻觉诱导图像生成 

**Authors**: Aryan Yazdan Parast, Parsa Hosseini, Hesam Asadollahzadeh, Arshia Soltani Moakhar, Basim Azam, Soheil Feizi, Naveed Akhtar  

**Link**: [PDF](https://arxiv.org/pdf/2509.25178)  

**Abstract**: Object hallucination in Multimodal Large Language Models (MLLMs) is a persistent failure mode that causes the model to perceive objects absent in the image. This weakness of MLLMs is currently studied using static benchmarks with fixed visual scenarios, which preempts the possibility of uncovering model-specific or unanticipated hallucination vulnerabilities. We introduce GHOST (Generating Hallucinations via Optimizing Stealth Tokens), a method designed to stress-test MLLMs by actively generating images that induce hallucination. GHOST is fully automatic and requires no human supervision or prior knowledge. It operates by optimizing in the image embedding space to mislead the model while keeping the target object absent, and then guiding a diffusion model conditioned on the embedding to generate natural-looking images. The resulting images remain visually natural and close to the original input, yet introduce subtle misleading cues that cause the model to hallucinate. We evaluate our method across a range of models, including reasoning models like GLM-4.1V-Thinking, and achieve a hallucination success rate exceeding 28%, compared to around 1% in prior data-driven discovery methods. We confirm that the generated images are both high-quality and object-free through quantitative metrics and human evaluation. Also, GHOST uncovers transferable vulnerabilities: images optimized for Qwen2.5-VL induce hallucinations in GPT-4o at a 66.5% rate. Finally, we show that fine-tuning on our images mitigates hallucination, positioning GHOST as both a diagnostic and corrective tool for building more reliable multimodal systems. 

**Abstract (ZH)**: Multimodal Large Language Models中的对象幻觉：一种持续存在的故障模式，导致模型感知图像中不存在的对象。目前，MLLMs的这一弱点是通过使用固定视觉场景的静态基准来研究的，这限制了发现模型特定或未预见的幻觉漏洞的可能性。我们引入了GHOST（通过优化隐身标记生成幻觉），一种旨在通过主动生成诱导幻觉的图像来测试MLLMs的方法。GHOST完全自动，不需要人工监督或先验知识。它通过在图像嵌入空间中优化以迷惑模型同时保持目标对象的缺失，然后引导一个基于嵌入条件的扩散模型生成看起来自然的图像。生成的图像保持视觉上的自然性且接近原始输入，但引入了细微的误导性提示，导致模型产生幻觉。我们跨多种模型评估了该方法，包括推理模型如GLM-4.1V-Thinking，并实现了超过28%的幻觉成功率，相比之下，此前基于数据的发现方法的成功率约为1%。通过定量指标和人工评估，我们确认生成的图像既是高质量的又是无对象的。此外，GHOST发现了可转移的漏洞：针对Qwen2.5-VL优化的图像在GPT-4o中诱导幻觉的成功率为66.5%。最后，我们展示了在我们的图像上进行微调可以缓解幻觉，将GHOST定位为构建更可靠的多模态系统的一种诊断和纠正工具。 

---
# EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering 

**Title (ZH)**: EasySteer：高性能和可扩展的大语言模型引导统一框架 

**Authors**: Haolei Xu, Xinyu Mei, Yuchen Yan, Rui Zhou, Wenqi Zhang, Weiming Lu, Yueting Zhuang, Yongliang Shen  

**Link**: [PDF](https://arxiv.org/pdf/2509.25175)  

**Abstract**: Large language model (LLM) steering has emerged as a promising paradigm for controlling model behavior at inference time through targeted manipulation of hidden states, offering a lightweight alternative to expensive retraining. However, existing steering frameworks suffer from critical limitations: computational inefficiency, limited extensibility, and restricted functionality that hinder both research progress and practical deployment. We present EasySteer, a unified framework for high-performance, extensible LLM steering built on vLLM. Our system features modular architecture with pluggable interfaces for both analysis-based and learning-based methods, fine-grained parameter control, pre-computed steering vectors for eight application domains, and an interactive demonstration system. Through deep integration with vLLM's optimized inference engine, EasySteer achieves 5.5-11.4$\times$ speedup over existing frameworks. Extensive experiments demonstrate its effectiveness in overthinking mitigation, hallucination reduction, and other key applications. EasySteer transforms steering from research technique to production-ready capability, establishing critical infrastructure for deployable, controllable language models. 

**Abstract (ZH)**: 大规模语言模型（LLM）定向调控已成为一种有前景的范式，通过目标操纵隐藏状态在推理时控制模型行为，提供一种昂贵重构的轻量级替代方案。然而，现有定向调控框架面临关键限制：计算效率低、扩展性差和功能局限，阻碍了研究进展和实际部署。我们提出EasySteer，基于vLLM构建的高性能、可扩展的LLM定向调控统一框架。该系统具有模块化架构和可插拔接口，适用于基于分析和基于学习的方法，细粒度的参数控制，为八个应用场景预计算的定向调控向量，以及交互式演示系统。通过与vLLM优化推理引擎的深度集成，EasySteer实现了现有框架5.5-11.4倍的加速。广泛实验表明，其在过度推断缓解、幻觉减少及其他关键应用中的有效性。EasySteer将定向调控从研究技术转变为生产级别的能力，为可部署、可控的语言模型建立关键基础设施。 

---
# XQC: Well-conditioned Optimization Accelerates Deep Reinforcement Learning 

**Title (ZH)**: XQC：良好条件化优化加速深度强化学习 

**Authors**: Daniel Palenicek, Florian Vogt, Joe Watson, Ingmar Posner, Jan Peters  

**Link**: [PDF](https://arxiv.org/pdf/2509.25174)  

**Abstract**: Sample efficiency is a central property of effective deep reinforcement learning algorithms. Recent work has improved this through added complexity, such as larger models, exotic network architectures, and more complex algorithms, which are typically motivated purely by empirical performance. We take a more principled approach by focusing on the optimization landscape of the critic network. Using the eigenspectrum and condition number of the critic's Hessian, we systematically investigate the impact of common architectural design decisions on training dynamics. Our analysis reveals that a novel combination of batch normalization (BN), weight normalization (WN), and a distributional cross-entropy (CE) loss produces condition numbers orders of magnitude smaller than baselines. This combination also naturally bounds gradient norms, a property critical for maintaining a stable effective learning rate under non-stationary targets and bootstrapping. Based on these insights, we introduce XQC: a well-motivated, sample-efficient deep actor-critic algorithm built upon soft actor-critic that embodies these optimization-aware principles. We achieve state-of-the-art sample efficiency across 55 proprioception and 15 vision-based continuous control tasks, all while using significantly fewer parameters than competing methods. 

**Abstract (ZH)**: 有效深度强化学习算法中的样本效率是一项核心属性。近期的研究通过增加复杂性，如使用更大规模的模型、奇特的网络架构和更复杂的算法来改进这一属性，这些方法通常仅基于经验性能进行动机说明。我们采取更为原则化的方法，重点关注批判网络的优化景观。利用批判网络海森矩阵的本征谱和条件数，系统研究了常见架构设计决策对训练动力学的影响。我们的分析表明，将批标准化（BN）、权重标准化（WN）与分布交叉熵（CE）损失结合使用，产生的条件数比基线低几个数量级。此外，该组合还自然地限制了梯度范数，这是在非平稳目标和自举下保持稳定有效学习率的关键属性。基于这些洞见，我们提出了XQC：一种基于软Actor-Critic构建的、具有优化意识原则的有效深度Actor-Critic算法。我们在55个 proprioception 和15个 vision-based 连续控制任务上实现了最先进的样本效率，同时使用了比竞争方法显著 fewer 的参数。 

---
# GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion Models 

**Title (ZH)**: GLASS 流动: 流与扩散模型对齐的转换采样方法 

**Authors**: Peter Holderrieth, Uriel Singer, Tommi Jaakkola, Ricky T. Q. Chen, Yaron Lipman, Brian Karrer  

**Link**: [PDF](https://arxiv.org/pdf/2509.25170)  

**Abstract**: The performance of flow matching and diffusion models can be greatly improved at inference time using reward alignment algorithms, yet efficiency remains a major limitation. While several algorithms were proposed, we demonstrate that a common bottleneck is the sampling method these algorithms rely on: many algorithms require to sample Markov transitions via SDE sampling, which is significantly less efficient and often less performant than ODE sampling. To remove this bottleneck, we introduce GLASS Flows, a new sampling paradigm that simulates a "flow matching model within a flow matching model" to sample Markov transitions. As we show in this work, this "inner" flow matching model can be retrieved from a pre-trained model without any re-training, combining the efficiency of ODEs with the stochastic evolution of SDEs. On large-scale text-to-image models, we show that GLASS Flows eliminate the trade-off between stochastic evolution and efficiency. Combined with Feynman-Kac Steering, GLASS Flows improve state-of-the-art performance in text-to-image generation, making it a simple, drop-in solution for inference-time scaling of flow and diffusion models. 

**Abstract (ZH)**: 使用奖励对齐算法可以在推理时大幅提高流动匹配和扩散模型的性能，但效率仍然是一个主要限制。虽然提出了几种算法，但我们展示了这些算法依赖的采样方法是一个共同瓶颈：许多算法需要通过SDE采样来采样马尔科夫转换，这在效率和性能上通常远逊于ODE采样。为了消除这一瓶颈，我们提出了GLASS Flows，这是一种新的采样范式，模拟“在一个流动匹配模型内部模拟一个流动匹配模型”来采样马尔科夫转换。如本文所示，这种“内部”的流动匹配模型可以从预先训练好的模型中提取出来，无需重新训练，从而结合了ODE的高效性和SDE的随机演化。在大规模文本到图像模型上，我们展示了GLASS Flows消除了随机演化与效率之间的权衡。结合费曼-卡茨引导，GLASS Flows提高了文本到图像生成的最新性能，使其成为流动和扩散模型推理时间扩展的一个简单即插即用解决方案。 

---
# GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts 

**Title (ZH)**: GSM8K-V：视觉语言模型能否解决视觉背景下的一年级数学文字题？ 

**Authors**: Fan Yuan, Yuchen Yan, Yifan Jiang, Haoran Zhao, Tao Feng, Jinyan Chen, Yanwei Lou, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang  

**Link**: [PDF](https://arxiv.org/pdf/2509.25160)  

**Abstract**: Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. Among these tasks, reasoning is particularly representative, with mathematical reasoning serving as a prominent example. It highlights the high-level capability of VLMs to comprehend mathematical information in images and to perform sophisticated reasoning. Recently, numerous visual mathematical reasoning benchmarks have been proposed, but they are often restricted to geometry, lack coverage of math word problems, and rarely assess reasoning across multiple images. To address these gaps, we introduce GSM8K-V, a purely visual multi-image mathematical reasoning benchmark. GSM8K-V is built by systematically mapping each sample from the widely used text-based GSM8K into visual form. Through a carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate 1,319 high-quality samples. We evaluate a wide range of open-source and closed-source models on GSM8K-V. Results show that although existing VLMs have nearly saturated performance on text-based GSM8K, there remains substantial room for improvement on GSM8K-V. For example, the best-performing model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the limitations of current models as well as potential directions for improvement. GSM8K-V offers a new perspective on visual mathematical reasoning and establishes a benchmark to guide the development of more robust and generalizable VLMs. 

**Abstract (ZH)**: 视觉语言模型（VLMs）实现了图像与文本的统一建模，使其能够通过感知、规划和推理来完成复杂的现实世界任务。在这类任务中，推理尤为典型，其中数学推理是一种突出的例子。它突显了VLMs在理解图像中的数学信息和进行复杂推理方面的高级能力。近期，提出了许多视觉数学推理基准，但它们往往局限于几何领域，缺乏数学文字问题的覆盖，并且很少评估跨多张图像的推理能力。为弥补这些不足，我们引入了GSM8K-V，一个纯粹基于视觉的多图像数学推理基准。GSM8K-V通过系统地将广泛使用的文本基于基准GSM8K中的每个样本映射到视觉形式构建而成。通过精心设计的自动化图像生成管道与细致的人工注释相结合，我们策划了1,319个高质量样本。我们在GSM8K-V上评估了各种开源和闭源模型。结果表明，虽然现有的VLMs在文本基于的GSM8K上的性能几乎饱和，但在GSM8K-V上仍有很大的改进空间。例如，表现最佳的模型Gemini-2.5-Pro在GSM8K上的准确率为95.22%，但在GSM8K-V上的准确率仅为46.93%。我们对GSM8K-V进行了全面分析，探讨了当前模型的局限性和改进的潜在方向。GSM8K-V为视觉数学推理提供了新的视角，并建立了指导更稳健和泛化能力更强的VLMs发展的一个基准。 

---
# Chance-constrained Flow Matching for High-Fidelity Constraint-aware Generation 

**Title (ZH)**: 高保真约束aware生成的机遇约束流匹配 

**Authors**: Jinhao Liang, Yixuan Sun, Anirban Samaddar, Sandeep Madireddy, Ferdinando Fioretto  

**Link**: [PDF](https://arxiv.org/pdf/2509.25157)  

**Abstract**: Generative models excel at synthesizing high-fidelity samples from complex data distributions, but they often violate hard constraints arising from physical laws or task specifications. A common remedy is to project intermediate samples onto the feasible set; however, repeated projection can distort the learned distribution and induce a mismatch with the data manifold. Thus, recent multi-stage procedures attempt to defer projection to clean samples during sampling, but they increase algorithmic complexity and accumulate errors across steps. This paper addresses these challenges by proposing a novel training-free method, Chance-constrained Flow Matching (CCFM), that integrates stochastic optimization into the sampling process, enabling effective enforcement of hard constraints while maintaining high-fidelity sample generation. Importantly, CCFM guarantees feasibility in the same manner as conventional repeated projection, yet, despite operating directly on noisy intermediate samples, it is theoretically equivalent to projecting onto the feasible set defined by clean samples. This yields a sampler that mitigates distributional distortion. Empirical experiments show that CCFM outperforms current state-of-the-art constrained generative models in modeling complex physical systems governed by partial differential equations and molecular docking problems, delivering higher feasibility and fidelity. 

**Abstract (ZH)**: 机会约束流量匹配：一种训练-Free 的高保真生成方法 

---
# Pretraining Large Language Models with NVFP4 

**Title (ZH)**: 使用NVFP4预训练大型语言模型 

**Authors**: NVIDIA, Felix Abecassis, Anjulie Agrusa, Dong Ahn, Jonah Alben, Stefania Alborghetti, Michael Andersch, Sivakumar Arayandi, Alexis Bjorlin, Aaron Blakeman, Evan Briones, Ian Buck, Bryan Catanzaro, Jinhang Choi, Mike Chrzanowski, Eric Chung, Victor Cui, Steve Dai, Bita Darvish Rouhani, Carlo del Mundo, Deena Donia, Burc Eryilmaz, Henry Estela, Abhinav Goel, Oleg Goncharov, Yugi Guvvala, Robert Hesse, Russell Hewett, Herbert Hum, Ujval Kapasi, Brucek Khailany, Mikail Khona, Nick Knight, Alex Kondratenko, Ronny Krashinsky, Ben Lanir, Simon Layton, Michael Lightstone, Daniel Lo, Paulius Micikevicius, Asit Mishra, Tim Moon, Deepak Narayanan, Chao Ni, Abhijit Paithankar, Satish Pasumarthi, Ankit Patel, Mostofa Patwary, Ashwin Poojary, Gargi Prasad, Sweta Priyadarshi, Yigong Qin, Xiaowei Ren, Oleg Rybakov, Charbel Sakr, Sanjeev Satheesh, Stas Sergienko, Pasha Shamis, Kirthi Shankar, Nishant Sharma, Mohammad Shoeybi, Michael Siu, Misha Smelyanskiy, Darko Stosic, Dusan Stosic, Bor-Yiing Su, Frank Sun, Nima Tajbakhsh, Shelby Thomas, Przemek Tredak, Evgeny Tsykunov, Gandhi Vaithilingam, Aditya Vavre, Rangharajan Venkatesan, Roger Waleffe, Qiyu Wan, Hexin Wang, Mengdi Wang, Lizzie Wei, Hao Wu, Evan Wu, Keith Wyss, Ning Xu, Jinze Xue, Charlene Yang, Yujia Zhai, Ruoxi Zhang, Jingyang Zhu, Zhongbo Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2509.25149)  

**Abstract**: Large Language Models (LLMs) today are powerful problem solvers across many domains, and they continue to get stronger as they scale in model size, training set size, and training set quality, as shown by extensive research and experimentation across the industry. Training a frontier model today requires on the order of tens to hundreds of yottaflops, which is a massive investment of time, compute, and energy. Improving pretraining efficiency is therefore essential to enable the next generation of even more capable LLMs. While 8-bit floating point (FP8) training is now widely adopted, transitioning to even narrower precision, such as 4-bit floating point (FP4), could unlock additional improvements in computational speed and resource utilization. However, quantization at this level poses challenges to training stability, convergence, and implementation, notably for large-scale models trained on long token horizons.
In this study, we introduce a novel approach for stable and accurate training of large language models (LLMs) using the NVFP4 format. Our method integrates Random Hadamard transforms (RHT) to bound block-level outliers, employs a two-dimensional quantization scheme for consistent representations across both the forward and backward passes, utilizes stochastic rounding for unbiased gradient estimation, and incorporates selective high-precision layers. We validate our approach by training a 12-billion-parameter model on 10 trillion tokens -- the longest publicly documented training run in 4-bit precision to date. Our results show that the model trained with our NVFP4-based pretraining technique achieves training loss and downstream task accuracies comparable to an FP8 baseline. These findings highlight that NVFP4, when combined with our training approach, represents a major step forward in narrow-precision LLM training algorithms. 

**Abstract (ZH)**: 一种稳定的NVFP4格式用于大语言模型的精确预训练方法 

---
# Fast Feature Field ($\text{F}^3$): A Predictive Representation of Events 

**Title (ZH)**: 快速特征场 ($\text{F}^3$): 事件的预测表示 

**Authors**: Richeek Das, Kostas Daniilidis, Pratik Chaudhari  

**Link**: [PDF](https://arxiv.org/pdf/2509.25146)  

**Abstract**: This paper develops a mathematical argument and algorithms for building representations of data from event-based cameras, that we call Fast Feature Field ($\text{F}^3$). We learn this representation by predicting future events from past events and show that it preserves scene structure and motion information. $\text{F}^3$ exploits the sparsity of event data and is robust to noise and variations in event rates. It can be computed efficiently using ideas from multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and 440 Hz at VGA resolutions. $\text{F}^3$ represents events within a contiguous spatiotemporal volume as a multi-channel image, enabling a range of downstream tasks. We obtain state-of-the-art performance on optical flow estimation, semantic segmentation, and monocular metric depth estimation, on data from three robotic platforms (a car, a quadruped robot and a flying platform), across different lighting conditions (daytime, nighttime), environments (indoors, outdoors, urban, as well as off-road) and dynamic vision sensors (resolutions and event rates). Our implementations can predict these tasks at 25-75 Hz at HD resolution. 

**Abstract (ZH)**: 本文开发了一种用于事件驱动摄像头数据表示的数学论证和算法，我们称之为快速特征场（$\text{F}^3$）。通过从过去事件预测未来事件来学习这种表示，并展示了其能够保留场景结构和运动信息。$\text{F}^3$ 利用事件数据的稀疏性，对噪声和事件率变化具有鲁棒性。其高效计算利用了多分辨率哈希编码和深度集合的理念，在高清分辨率下达到120 Hz，在VGA分辨率下达到440 Hz。$\text{F}^3$ 将事件表示为连续的空间-时间体积中的多通道图像，便于执行一系列下游任务。我们在三款机器人平台（一辆汽车、四足机器人和飞行平台）的数据上，在不同光照条件（白天、夜晚）、不同环境（室内、室外、城市以及非铺装道路）和不同动态视觉传感器（分辨率和事件率）下，均取得了最优性能。我们的实现可以在高清分辨率下以25-75 Hz的速度预测这些任务。 

---
# Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation 

**Title (ZH)**: 由老师配对：将无配对数据转换为低资源文本生成的高保真配对 

**Authors**: Yen-Ju Lu, Thomas Thebaud, Laureano Moro-Velazquez, Najim Dehak, Jesus Villalba  

**Link**: [PDF](https://arxiv.org/pdf/2509.25144)  

**Abstract**: We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline that synthesizes accurate input-output pairs without human labels or parallel data. In many low-resource natural language generation (NLG) scenarios, practitioners may have only raw outputs, like highlights, recaps, or questions, or only raw inputs, such as articles, dialogues, or paragraphs, but seldom both. This mismatch forces small models to learn from very few examples or rely on costly, broad-scope synthetic examples produced by large LLMs. PbT addresses this by asking a teacher LLM to compress each unpaired example into a concise intermediate representation (IR), and training a student to reconstruct inputs from IRs. This enables outputs to be paired with student-generated inputs, yielding high-quality synthetic data. We evaluate PbT on five benchmarks-document summarization (XSum, CNNDM), dialogue summarization (SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired setting on SwitchBoard (paired with DialogSum summaries). An 8B student trained only on PbT data outperforms models trained on 70 B teacher-generated corpora and other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated pairs and closing 82% of the oracle gap at one-third the annotation cost of direct synthesis. Human evaluation on SwitchBoard further confirms that only PbT produces concise, faithful summaries aligned with the target style, highlighting its advantage of generating in-domain sources that avoid the mismatch, limiting direct synthesis. 

**Abstract (ZH)**: Paired by the Teacher：一种无需人工标签或平行数据的两阶段教师-学生管道 

---
# Rethinking Entropy Regularization in Large Reasoning Models 

**Title (ZH)**: 重新思考大型推理模型中的熵正则化 

**Authors**: Yuxian Jiang, Yafu Li, Guanxu Chen, Dongrui Liu, Yu Cheng, Jing Shao  

**Link**: [PDF](https://arxiv.org/pdf/2509.25133)  

**Abstract**: Reinforcement learning with verifiable rewards (RLVR) has shown great promise in enhancing the reasoning abilities of large reasoning models (LRMs). However, it suffers from a critical issue: entropy collapse and premature convergence. Naive entropy regularization, a common approach for encouraging exploration in the traditional RL literature, fails to address this problem in the context of LRM. Our analysis reveals that this failure stems from the vast action space and long trajectories in LRMs, which easily trigger a global entropy explosion as the model indiscriminately explores all possible actions and states. To address this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method that confines exploration to a meaningful subset of actions and states. SIREN achieves this through a two-step entropy masking mechanism, consisting of a top-p mask and a peak-entropy mask. In addition, regularization is transformed into a self-anchored form to stabilize training. Across five mathematical benchmarks, SIREN attains superior average performance over previous entropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on AIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes greater response diversity and maintains entropy at an appropriate level, which helps to preserve the validation pass@k throughout training. This effectively mitigates the premature convergence problem common in RLVR for LRM. 

**Abstract (ZH)**: 验证奖励的强化学习（RLVR）在提升大型推理模型（LRM）的推理能力方面展现了巨大潜力。然而，它面临一个关键问题：熵坍缩和过早收敛。传统的RL文献中常用的简单熵正则化方法在LRM的背景下无法解决这一问题。我们的分析表明，这种失败源于LRM广阔的动作空间和长轨迹，模型在无差异地探索所有可能的动作和状态时容易引发全局熵爆炸。为解决这一问题，我们提出了SIREN（SelectIve entRopy rEgularizatioN），一种将探索限制在有意义的动作和状态子集中的方法。SIREN通过一个两步熵遮罩机制实现这一点，包括一个top-p遮罩和一个峰值熵遮罩。此外，正则化被转化为自我锚定的形式以稳定训练。在五个数学基准测试中，SIREN在之前的与熵相关的RLVR方法中表现出 superior 的平均性能，例如在Qwen2.5-Math-7B上AIME24/25的maj@k上提升了6.6分。进一步分析证实，SIREN促进了更大的响应多样性，并保持了适当的熵水平，这有助于在整个训练过程中保持验证pass@k。这一方法有效缓解了LRM的RLVR中常见的过早收敛问题。 

---
# MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech 

**Title (ZH)**: MGM-Omni: 扩展全域大语言模型以实现个性化长时语音生成 

**Authors**: Chengyao Wang, Zhisheng Zhong, Bohao Peng, Senqiao Yang, Yuqi Liu, Haokun Gui, Bin Xia, Jingyao Li, Bei Yu, Jiaya Jia  

**Link**: [PDF](https://arxiv.org/pdf/2509.25131)  

**Abstract**: We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation. Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation. This design enables efficient cross-modal interaction and low-latency, streaming speech generation. For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions. For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations. Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training. Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding. MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation. 

**Abstract (ZH)**: MGM-Omni：统一的全模态Omni LLM，实现全模态理解与表达性的长期语音生成 

---
# Score Distillation of Flow Matching Models 

**Title (ZH)**: 流匹配模型的评分蒸馏 

**Authors**: Mingyuan Zhou, Yi Gu, Huangjie Zheng, Liangchen Song, Guande He, Yizhe Zhang, Wenze Hu, Yinfei Yang  

**Link**: [PDF](https://arxiv.org/pdf/2509.25127)  

**Abstract**: Diffusion models achieve high-quality image generation but are limited by slow iterative sampling. Distillation methods alleviate this by enabling one- or few-step generation. Flow matching, originally introduced as a distinct framework, has since been shown to be theoretically equivalent to diffusion under Gaussian assumptions, raising the question of whether distillation techniques such as score distillation transfer directly. We provide a simple derivation -- based on Bayes' rule and conditional expectations -- that unifies Gaussian diffusion and flow matching without relying on ODE/SDE formulations. Building on this view, we extend Score identity Distillation (SiD) to pretrained text-to-image flow-matching models, including SANA, SD3-Medium, SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show that, with only modest flow-matching- and DiT-specific adjustments, SiD works out of the box across these models, in both data-free and data-aided settings, without requiring teacher finetuning or architectural changes. This provides the first systematic evidence that score distillation applies broadly to text-to-image flow matching models, resolving prior concerns about stability and soundness and unifying acceleration techniques across diffusion- and flow-based generators. We will make the PyTorch implementation publicly available. 

**Abstract (ZH)**: 扩散模型能够生成高质量的图像，但迭代采样速度较慢。蒸馏方法通过使生成过程减少至一或几步来解决这一问题。流匹配最初被引入为一个独立的框架，后来在高斯假定下被证明与扩散模型在理论上等价，这引发了这样的疑问：是否可以直接将蒸馏技术如得分蒸馏应用于流匹配模型。我们基于贝叶斯规则和条件期望提供了简单的数学推导，以不依赖于常微分方程/随机微分方程形式的方式统一了高斯扩散和流匹配。在此基础上，我们拓展了得分身份蒸馏（SiD）技术，应用于预训练的文本到图像流匹配模型，包括SANA、SD3-Medium、SD3.5-Medium/Large和FLUX.1-dev，所有模型均基于DiT架构。实验表明，通过仅进行适度的流匹配和DiT特定调整，SiD在这些模型中无需教师微调或架构更改即可直接应用，无论是无数据辅助还是有数据辅助设置。这一发现提供了首次系统的证据，表明得分蒸馏广泛适用于文本到图像流匹配模型，解决了此前关于稳定性和正确性的问题，并实现了扩散生成器和流生成器加速技术的一体化。我们将公开发布PyTorch实现。 

---
# Towards Personalized Deep Research: Benchmarks and Evaluations 

**Title (ZH)**: 面向个性化深度研究的基准与评估 

**Authors**: Yuan Liang, Jiaxian Li, Yuqing Wang, Piaohong Wang, Motong Tian, Pai Liu, Shuofei Qiao, Runnan Fang, He Zhu, Ge Zhang, Minghao Liu, Yuchen Eleanor Jiang, Ningyu Zhang, Wangchunshu Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2509.25106)  

**Abstract**: Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench, the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures (P) Personalization Alignment, (Q) Content Quality, and (R) Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants. 

**Abstract (ZH)**: 个性化的深度研究基准：评估DRAs的首个基准 

---
# ORPO-Distill: Mixed-Policy Preference Optimization for Cross-Architecture LLM Distillation 

**Title (ZH)**: ORPO-精炼：跨架构大语言模型精炼的混合策略偏好优化 

**Authors**: Aasheesh Singh, Vishal Vaddina, Dagnachew Birru  

**Link**: [PDF](https://arxiv.org/pdf/2509.25100)  

**Abstract**: We introduce ORPO-Distill, a general-purpose method for cross-architecture LLM distillation that formulates the problem as a preference optimization task. Un- like standard CoT distillation, the approach transfers knowledge through diverse reasoning traces. It employs an Odds-Ratio Preference Optimization objective that contrasts teacher and student traces for more effective learning, and adopts a mixed-policy strategy for utilizing student-generated outputs, outperforming both off- and on-policy alternatives. Experiments on five datasets and multiple student models show consistent improvements over conventional black-box KD baselines. 

**Abstract (ZH)**: ORPO-Distill: 一种跨架构LLM精简的一般方法，该方法将问题形式化为偏好优化任务 

---
# Scaling with Collapse: Efficient and Predictable Training of LLM Families 

**Title (ZH)**: 缩放与崩溃：LLM家族高效且可预测的训练 

**Authors**: Shane Bergsma, Bin Claire Zhang, Nolan Dey, Shaheer Muhammad, Gurpreet Gosal, Joel Hestness  

**Link**: [PDF](https://arxiv.org/pdf/2509.25087)  

**Abstract**: Effective LLM training relies on *consistency*, meaning that key quantities -- such as final losses and optimal hyperparameters -- scale predictably across model sizes. Qiu et al. (2025) recently showed that this consistency extends beyond scalars: whole training loss curves can *collapse* onto a universal trajectory after a simple normalization. What remains unclear is whether this phenomenon holds for LLM families trained under *practical scaling recipes*, where width, depth, learning rate, batch size, and weight decay are scaled jointly. We show that it does: loss curves collapse across scales precisely when optimization hyperparameters are set optimally for the given data budget, in accordance with recent empirical scaling laws. Collapse thus emerges as a signature of compute-efficient training. We demonstrate two applications at scale: (1) deviation-from-collapse provides a sensitive, early diagnostic of training pathologies, and (2) the predictability of collapsed curves enables early stopping in large-scale hyperparameter tuning. Finally, we train a competitive LLM family, *Celerity*, using these insights, highlighting collapse as an effective tool for developing efficient LLMs. 

**Abstract (ZH)**: 有效的预训练语言模型依赖于*一致性*，这意味着关键量（如最终损失和最优超参数）在不同模型规模下按可预测的方式缩放。Qiu等人（2025）最近表明，这种一致性不仅适用于标量，简单的归一化后，整个训练损失曲线还可以*坍缩*到一个通用轨迹。尚不清楚的是，这一现象是否适用于采用*实际缩放食谱*训练的LSTM家族，其中宽度、深度、学习率、批量大小和权重衰减是联合缩放的。我们表明，当优化超参数针对给定的数据预算设置为最优时，这种现象确实存在：损失曲线在不同规模下会出现坍缩，这符合近期的经验缩放法则。因此，坍缩现象被认为是计算效率训练的特征标志。我们展示了两个大规模应用：（1）偏离坍缩提供了一种敏感且早期的训练病理诊断方法，（2）坍缩曲线的可预测性使得在大规模超参数调优中可以实现早期停止。最后，我们利用这些见解训练了一个具有竞争力的LSTM家族，*Celerity*，强调坍缩作为开发高效LSTM的有效工具。 

---
# jina-reranker-v3: Last but Not Late Interaction for Document Reranking 

**Title (ZH)**: jina-reranker-v3：最后但并非最不重要交互的文档重排 

**Authors**: Feng Wang, Yuqing Li, Han Xiao  

**Link**: [PDF](https://arxiv.org/pdf/2509.25085)  

**Abstract**: jina-reranker-v3 is a 0.6B parameter multilingual document reranker that introduces a novel last but not late interaction. Unlike late interaction models such as ColBERT that perform separate encoding followed by multi-vector matching, our approach conducts causal self-attention between query and documents within the same context window, enabling rich cross-document interactions before extracting contextual embeddings from the last token of each document. This compact architecture achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while being ten times smaller than generative listwise rerankers. 

**Abstract (ZH)**: Jina-Reranker-v3是一种参数量为0.6B的多语言文档重排序器，引入了一种新颖的非晚交互方式。不同于ColBERT等晚交互模型在分别编码后进行多向量匹配的做法，我们的方法在同一个上下文窗口内对查询和文档之间进行因果自注意力交互，从而在提取每个文档最后一词的上下文嵌入之前实现丰富的跨文档交互。这种紧凑的架构在BEIR上达到了61.94的nDCG@10性能，同时仅有生成型列表重排序器的十分之一大小。 

---
# Scaling Generalist Data-Analytic Agents 

**Title (ZH)**: 扩展通用数据分析师代理 

**Authors**: Shuofei Qiao, Yanqiu Zhao, Zhisong Qiu, Xiaobin Wang, Jintian Zhang, Zhao Bin, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.25084)  

**Abstract**: Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research. 

**Abstract (ZH)**: DataMind：面向开源数据分析代理的可扩展数据合成与智能体训练方法 

---
# UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation 

**Title (ZH)**: UniLat3D: 同时统一几何与外观的单阶段三维生成潜变量 

**Authors**: Guanjun Wu, Jiemin Fang, Chen Yang, Sikuang Li, Taoran Yi, Jia Lu, Zanwei Zhou, Jiazhong Cen, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Xinggang Wang, Qi Tian  

**Link**: [PDF](https://arxiv.org/pdf/2509.25079)  

**Abstract**: High-fidelity 3D asset generation is crucial for various industries. While recent 3D pretrained models show strong capability in producing realistic content, most are built upon diffusion models and follow a two-stage pipeline that first generates geometry and then synthesizes appearance. Such a decoupled design tends to produce geometry-texture misalignment and non-negligible cost. In this paper, we propose UniLat3D, a unified framework that encodes geometry and appearance in a single latent space, enabling direct single-stage generation. Our key contribution is a geometry-appearance Unified VAE, which compresses high-resolution sparse features into a compact latent representation -- UniLat. UniLat integrates structural and visual information into a dense low-resolution latent, which can be efficiently decoded into diverse 3D formats, e.g., 3D Gaussians and meshes. Based on this unified representation, we train a single flow-matching model to map Gaussian noise directly into UniLat, eliminating redundant stages. Trained solely on public datasets, UniLat3D produces high-quality 3D assets in seconds from a single image, achieving superior appearance fidelity and geometric quality. More demos \& code are available at this https URL 

**Abstract (ZH)**: 高保真3D资产生成对于多个行业至关重要。虽然近期的3D预训练模型在产生逼真内容方面表现出强大的能力，但大多数模型基于扩散模型构建，并遵循两阶段管道，首先生成几何结构，然后合成外观。这种解耦设计往往会产生几何结构与纹理不匹配，并伴随着较高的成本。在本文中，我们提出了一种统一框架UniLat3D，该框架将几何结构和外观编码到单个潜在空间中，使得可以直接进行单阶段生成。我们的主要贡献是一种几何结构-外观统一的VAE，它将高分辨率稀疏特征压缩成紧凑的潜在表示UniLat。UniLat将结构和视觉信息整合到密集的低分辨率潜在空间中，可以高效地解码为多种3D格式，例如3D高斯函数和网格。基于这种统一的表示，我们训练了一个单阶段流匹配模型，直接将高斯噪声映射到UniLat，从而消除了冗余阶段。仅通过公共数据集训练，UniLat3D可以从单张图像中在几秒钟内生成高质量的3D资产，实现卓越的外观保真度和几何质量。更多演示与代码请访问此网址。 

---
# BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation 

**Title (ZH)**: BRIDGE - 建立用于单目深度估计的强化学习深度到图像数据生成引擎 

**Authors**: Dingning Liu, Haoyu Guo, Jingyi Zhou, Tong He  

**Link**: [PDF](https://arxiv.org/pdf/2509.25077)  

**Abstract**: Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data scarcity and quality, hindering their robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image (D2I) generation framework that synthesizes over 20M realistic and geometrically accurate RGB images, each intrinsically paired with its ground truth depth, from diverse source depth maps. Then we train our depth estimation model on this dataset, employing a hybrid supervision strategy that integrates teacher pseudo-labels with ground truth depth for comprehensive and robust training. This innovative data generation and training paradigm enables BRIDGE to achieve breakthroughs in scale and domain diversity, consistently outperforming existing state-of-the-art approaches quantitatively and in complex scene detail capture, thereby fostering general and robust depth features. Code and models are available at this https URL. 

**Abstract (ZH)**: 单目深度估计（MDE）是计算机视觉中的基础任务。传统方法受限于数据的稀缺性和质量，影响其鲁棒性。为克服这一问题，我们提出BRIDGE，一种基于强化学习优化的深度图到图像（D2I）生成框架，该框架从多种来源的深度图中合成超过2000万张真实且几何准确的RGB图像，并且每张图像都与其地面 truth深度图内嵌配对。然后，我们使用一种混合监督策略训练我们的深度估计模型，该策略结合了教师伪标签与地面 truth深度图，以实现全面且稳健的训练。这一创新的数据生成和训练范式使BRIDGE在规模和领域多样性方面取得突破，定量和定性上均优于现有最先进的方法，从而促进通用且鲁棒的深度特征的生成。代码和模型可在以下网址获取。 

---
# Optimizing Privacy-Preserving Primitives to Support LLM-Scale Applications 

**Title (ZH)**: 优化隐私保护基本组件以支持大规模语言模型应用 

**Authors**: Yaman Jandali, Ruisi Zhang, Nojan Sheybani, Farinaz Koushanfar  

**Link**: [PDF](https://arxiv.org/pdf/2509.25072)  

**Abstract**: Privacy-preserving technologies have introduced a paradigm shift that allows for realizable secure computing in real-world systems. The significant barrier to the practical adoption of these primitives is the computational and communication overhead that is incurred when applied at scale. In this paper, we present an overview of our efforts to bridge the gap between this overhead and practicality for privacy-preserving learning systems using multi-party computation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic encryption (FHE). Through meticulous hardware/software/algorithm co-design, we show progress towards enabling LLM-scale applications in privacy-preserving settings. We demonstrate the efficacy of our solutions in several contexts, including DNN IP ownership, ethical LLM usage enforcement, and transformer inference. 

**Abstract (ZH)**: 隐私保护技术引入了一种范式转变，允许在实际系统中实现安全计算。这些原语在大规模应用时面临的显著障碍是计算和通信开销。在本文中，我们提出了通过多方计算（MPC）、零知识证明（ZKPs）和全同态加密（FHE）缩小这种开销与实用性之间差距的努力。通过细致的硬件/软件/算法协同设计，我们展示了如何使大规模语言模型（LLM）应用在隐私保护环境中成为可能。我们在深度神经网络知识产权保护、伦理LLM使用约束以及变压器推理等多个场景中展示了我们解决方案的有效性。 

---
# Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures 

**Title (ZH)**: 超高维度探针：通过向量符号架构解码LLM表示 

**Authors**: Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini  

**Link**: [PDF](https://arxiv.org/pdf/2509.25045)  

**Abstract**: Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods, such as direct logit attribution (DLA) and sparse autoencoders (SAEs), provide restricted insight due to limitations such as the model's output vocabulary or unclear feature names. This work introduces Hyperdimensional Probe, a novel paradigm for decoding information from the LLM vector space. It combines ideas from symbolic representations and neural probing to project the model's residual stream into interpretable concepts via Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs and conventional probes while overcoming their key limitations. We validate our decoding paradigm with controlled input-completion tasks, probing the model's final state before next-token prediction on inputs spanning syntactic pattern recognition, key-value associations, and abstract inference. We further assess it in a question-answering setting, examining the state of the model both before and after text generation. Our experiments show that our probe reliably extracts meaningful concepts across varied LLMs, embedding sizes, and input domains, also helping identify LLM failures. Our work advances information decoding in LLM vector space, enabling extracting more informative, interpretable, and structured features from neural representations. 

**Abstract (ZH)**: 尽管大型语言模型具备强大能力，但仍具有透明度差的特点，对其内部表示的理解有限。当前的可解释性方法，如直接logit归因（DLA）和稀疏自编码器（SAEs），由于受限于模型的输出词汇表或特征名称不明确等因素，提供的洞察力有限。本文引入了超维探针（Hyperdimensional Probe），这是一种从大语言模型向量空间解码信息的新范式。该探针结合了符号表示和神经探针的想法，通过向量符号架构（VSAs）将模型的残差流投影到可解释的概念中。该探针结合了SAEs和传统探针的优点，克服了它们的关键限制。我们通过受控的输入-完成任务，验证了解码范式的有效性，探针在涵盖句法模式识别、键值关联和抽象推理的不同输入上的模型最终状态之前，进行探针操作。我们还在问答场景下评估了它，检查模型在文本生成前后的状态。实验结果表明，我们的探针能够可靠地从各种大语言模型、嵌入大小和输入领域中提取有意义的概念，也有助于识别大语言模型中的失败情况。我们的工作推进了大语言模型向量空间中的信息解码，使得能够从神经表示中提取更具信息量、可解释性和结构化的特征。 

---
# Large Language Models for Software Testing: A Research Roadmap 

**Title (ZH)**: 大型语言模型在软件测试中的应用：研究路线图 

**Authors**: Cristian Augusto, Antonia Bertolino, Guglielmo De Angelis, Francesca Lonetti, Jesús Morán  

**Link**: [PDF](https://arxiv.org/pdf/2509.25043)  

**Abstract**: Large Language Models (LLMs) are starting to be profiled as one of the most significant disruptions in the Software Testing field.
Specifically, they have been successfully applied in software testing tasks such as generating test code, or summarizing documentation.
This potential has attracted hundreds of researchers, resulting in dozens of new contributions every month, hardening researchers to
stay at the forefront of the wave. Still, to the best of our knowledge, no prior work has provided a structured vision of the progress
and most relevant research trends in LLM-based testing. In this article, we aim to provide a roadmap that illustrates its current state,
grouping the contributions into different categories, and also sketching the most promising and active research directions for the field.
To achieve this objective, we have conducted a semi-systematic literature review, collecting articles and mapping them into the most
prominent categories, reviewing the current and ongoing status, and analyzing the open challenges of LLM-based software testing.
Lastly, we have outlined several expected long-term impacts of LLMs over the whole software testing field. 

**Abstract (ZH)**: 大型语言模型（LLMs）正成为软件测试领域最重要的颠覆之一。 

---
# Fast Real-Time Pipeline for Robust Arm Gesture Recognition 

**Title (ZH)**: 快速稳健的手臂手势识别实时处理管道 

**Authors**: Milán Zsolt Bagladi, László Gulyás, Gergő Szalay  

**Link**: [PDF](https://arxiv.org/pdf/2509.25042)  

**Abstract**: This paper presents a real-time pipeline for dynamic arm gesture recognition based on OpenPose keypoint estimation, keypoint normalization, and a recurrent neural network classifier. The 1 x 1 normalization scheme and two feature representations (coordinate- and angle-based) are presented for the pipeline. In addition, an efficient method to improve robustness against camera angle variations is also introduced by using artificially rotated training data. Experiments on a custom traffic-control gesture dataset demonstrate high accuracy across varying viewing angles and speeds. Finally, an approach to calculate the speed of the arm signal (if necessary) is also presented. 

**Abstract (ZH)**: 基于OpenPose关键点估计、关键点归一化和循环神经网络分类器的实时动态手臂手势识别流水线及其应用 

---
# Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct 

**Title (ZH)**: 通过离散扩散 divergence 指令实现超快速语言生成 

**Authors**: Haoyang Zheng, Xinyang Liu, Cindy Xiangrui Kong, Nan Jiang, Zheyuan Hu, Weijian Luo, Wei Deng, Guang Lin  

**Link**: [PDF](https://arxiv.org/pdf/2509.25035)  

**Abstract**: Fast generation of language texts is the holy grail that people pursue in the AI era. In this work, we introduced Discrete Diffusion Divergence Instruct (DiDi-Instruct), a training-based method that leads to fast language generation models by initializing from a pre-trained (masked) discrete diffusion language model (dLLM). The resulting DiDi-Instruct model outperforms the dLLM counterparts and the GPT-2 baseline with 64x acceleration. In the theoretical part of the paper, we build the foundation of DiDi-Instruct in a framework of integral KL-divergence minimization, with practical training algorithms. We also introduce techniques like grouped reward normalization, intermediate-state matching, and the reward-guided ancestral sampler (RGAS) that significantly improve the training stability, the model coverage, and the inference performances. On OpenWebText, DiDi-Instruct outperforms all accelerated language generation models as well as the GPT-2 baseline and the standard dLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128 NFEs). These performance gains are accomplished with a negligible entropy loss of about 1% and 20x less additional training wall-clock time. We further validate the robustness and effectiveness of DiDi-Instruct through extensive ablation studies, model scaling, and the generation of discrete protein sequences. In conclusion, DiDi-Instruct is an efficient yet effective distillation method, enabling language generation in the blink of an eye. We will release both code and models at this http URL. 

**Abstract (ZH)**: 快速生成语言文本是人工智能时代人们追求的圣杯。在这项工作中，我们引入了离散扩散 divergence 命令（DiDi-Instruct）方法，该方法通过从预训练（遮蔽）离散扩散语言模型（dLLM）初始化，实现快速语言生成模型。DiDi-Instruct 模型在 64 倍加速下优于 dLLM 对手和 GPT-2 基线。在论文的理论部分中，我们以积分 KL 散度最小化框架为基础，构建了 DiDi-Instruct 的理论基础，并提供了实用的训练算法。我们还介绍了分组奖励归一化、中间状态匹配以及奖励引导祖先采样器（RGAS）等技术，显著提高了训练稳定性、模型覆盖面和推理性能。在 OpenWebText 上，DiDi-Instruct 在加速语言生成模型、GPT-2 基线和标准 dLLM 中表现最优，样本 perplexities 范围从 62.2（8 NFEs）到 18.4（128 NFEs）。这些性能提升仅伴随不到 1% 的微小熵损失和 20 倍少的额外训练时间。我们通过广泛的消融研究、模型扩展和离散蛋白质序列生成进一步验证了 DiDi-Instruct 的稳健性和有效性。总之，DiDi-Instruct 是一种高效且有效的模型蒸馏方法，能够在眨眼之间实现语言生成。我们将在该网址发布代码和模型。 

---
# AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation 

**Title (ZH)**: AIRoA MoMa 数据集：一个大规模层次化移动 manipulator 数据集 

**Authors**: Ryosuke Takanami, Petr Khrapchenkov, Shu Morikuni, Jumpei Arima, Yuta Takaba, Shunsuke Maeda, Takuya Okubo, Genki Sano, Satoshi Sekioka, Aoi Kadoya, Motonari Kambara, Naoya Nishiura, Haruto Suzuki, Takanori Yoshimoto, Koya Sakamoto, Shinnosuke Ono, Hu Yang, Daichi Yashima, Aoi Horo, Tomohiro Motoda, Kensuke Chiyoma, Hiroshi Ito, Koki Fukuda, Akihito Goto, Kazumi Morinaga, Yuya Ikeda, Riko Kawada, Masaki Yoshikawa, Norio Kosuge, Yuki Noguchi, Kei Ota, Tatsuya Matsushima, Yusuke Iwasawa, Yutaka Matsuo, Tetsuya Ogata  

**Link**: [PDF](https://arxiv.org/pdf/2509.25032)  

**Abstract**: As robots transition from controlled settings to unstructured human environments, building generalist agents that can reliably follow natural language instructions remains a central challenge. Progress in robust mobile manipulation requires large-scale multimodal datasets that capture contact-rich and long-horizon tasks, yet existing resources lack synchronized force-torque sensing, hierarchical annotations, and explicit failure cases. We address this gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset for mobile manipulation. It includes synchronized RGB images, joint states, six-axis wrist force-torque signals, and internal robot states, together with a novel two-layer annotation schema of sub-goals and primitive actions for hierarchical learning and error analysis. The initial dataset comprises 25,469 episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa provides a critical benchmark for advancing the next generation of Vision-Language-Action models. The first version of our dataset is now available at this https URL . 

**Abstract (ZH)**: 随着机器人从受控环境过渡到无结构的人类环境，构建能够可靠遵循自然语言指令的通用代理仍是一项核心挑战。为了实现稳健的移动操作，需要大规模的多模态数据集来捕捉接触丰富的和长时序的任务，但现有资源缺乏同步的力-力矩感知、层次注释和明确的失败案例。我们通过AIRoA MoMa数据集来弥补这一差距，这是一个用于移动操作的大规模真实世界多模态数据集。它包括同步的RGB图像、关节状态、六轴手腕力-力矩信号和内部机器人状态，以及一个新颖的两级注释方案，用于层次学习和错误分析。初始数据集包含25,469集（约94小时）数据，使用Human Support Robot (HSR)收集，完全标准化为LeRobot v2.1格式。通过独特地整合移动操作、丰富接触交互和长时序结构，AIRoA MoMa提供了推进下一辈视觉-语言-动作模型的关键基准。我们的数据集第一个版本现已可用，请访问这个网址：[请提供网址]。 

---
# CLASP: Adaptive Spectral Clustering for Unsupervised Per-Image Segmentation 

**Title (ZH)**: CLASP:自适应谱聚类用于无监督单图像分割 

**Authors**: Max Curie, Paulo da Costa  

**Link**: [PDF](https://arxiv.org/pdf/2509.25016)  

**Abstract**: We introduce CLASP (Clustering via Adaptive Spectral Processing), a lightweight framework for unsupervised image segmentation that operates without any labeled data or finetuning. CLASP first extracts per patch features using a self supervised ViT encoder (DINO); then, it builds an affinity matrix and applies spectral clustering. To avoid manual tuning, we select the segment count automatically with a eigengap silhouette search, and we sharpen the boundaries with a fully connected DenseCRF. Despite its simplicity and training free nature, CLASP attains competitive mIoU and pixel accuracy on COCO Stuff and ADE20K, matching recent unsupervised baselines. The zero training design makes CLASP a strong, easily reproducible baseline for large unannotated corpora especially common in digital advertising and marketing workflows such as brand safety screening, creative asset curation, and social media content moderation 

**Abstract (ZH)**: CLASP（自适应光谱处理聚类）：一种轻量级的无监督图像分割框架 

---
# Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns 

**Title (ZH)**: 广义正确性模型：从历史模式学习校准的模型无关正确性预测器 

**Authors**: Hanqi Xiao, Vaidehi Patil, Hyunji Lee, Elias Stengel-Eskin, Mohit Bansal  

**Link**: [PDF](https://arxiv.org/pdf/2509.24988)  

**Abstract**: Generating accurate and calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as a problem of eliciting a model's "self-knowledge", i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answer's correctness that is accessible to the model itself. However, our experiments reveal that an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated LLM. Moreover, we hypothesize that a key factor in building a "Correctness Model" (CM) is exposure to a target model's historical predictions. We propose multiple methods to inject this historical correctness information, creating a Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness data from many LLMs and learn patterns for correctness prediction applicable across datasets and models. We then use CMs as a lens for studying the source of correctness prediction ability and its generalization, systematically controlling their training data and finding that answer phrasing is a strong predictor for correctness. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide complementary reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on a downstream selective prediction task, finding that reliable LLM confidence estimation is a generalizable and model-agnostic skill learned by systematically encoding correctness history rather than a model-specific skill reliant on self-introspection. 

**Abstract (ZH)**: 生成准确且校准的置信估计对于在高风险或用户面向的应用中部署大语言模型至关重要，但仍是一个开放性的挑战。先前的研究往往将置信度问题视为提取模型的“自我知识”，即模型判断其自身答案是否正确的能力；这种方法假定模型本身可以访问到关于答案正确性的某些优先信息。然而，我们的实验揭示，一个模型试图预测其自身输出的正确性通常与一个不相关的模型表现相当。此外，我们假设构建“正确性模型”（CM）的一个重要因素是接触目标模型的历史预测。我们提出了多种方法来注入这种历史正确性信息，创建了一种广义正确性模型（GCM）。我们首先表明，GCM可以在多个大语言模型的正确性数据上进行训练，并学习适用于不同数据集和模型的正确性预测模式。然后，我们使用CM作为研究正确性预测能力和其泛化的视角，系统控制其训练数据，发现答案表述是正确性预测的一个强大预测因子。进一步探索不通过训练大语言模型来注入历史的方法，发现包括历史作为上下文示例可以帮助提高正确性预测，并且事后校准可以提供额外的校准误差减少。我们基于Qwen3-8B模型和MMLU、TriviaQA数据集以及一个下游的有选择的预测任务评估了GCM，发现可靠的LSTM置信估计是通过系统编码正确性历史学习到的一项可泛化和模型无关的能力，而不是依赖自我反省的模型特定技能。 

---
# Light-SQ: Structure-aware Shape Abstraction with Superquadrics for Generated Meshes 

**Title (ZH)**: Light-SQ: 基于超二次曲面的结构意识形状抽象生成网状结构 

**Authors**: Yuhan Wang, Weikai Chen, Zeyu Hu, Runze Zhang, Yingda Yin, Ruoyu Wu, Keyang Luo, Shengju Qian, Yiyan Ma, Hongyi Li, Yuan Gao, Yuhuan Zhou, Hao Luo, Wan Wang, Xiaobin Shen, Zhaowei Li, Kuixin Zhu, Chuanlang Hong, Yueyue Wang, Lijie Feng, Xin Wang, Chen Change Loy  

**Link**: [PDF](https://arxiv.org/pdf/2509.24986)  

**Abstract**: In user-generated-content (UGC) applications, non-expert users often rely on image-to-3D generative models to create 3D assets. In this context, primitive-based shape abstraction offers a promising solution for UGC scenarios by compressing high-resolution meshes into compact, editable representations. Towards this end, effective shape abstraction must therefore be structure-aware, characterized by low overlap between primitives, part-aware alignment, and primitive compactness. We present Light-SQ, a novel superquadric-based optimization framework that explicitly emphasizes structure-awareness from three aspects. (a) We introduce SDF carving to iteratively udpate the target signed distance field, discouraging overlap between primitives. (b) We propose a block-regrow-fill strategy guided by structure-aware volumetric decomposition, enabling structural partitioning to drive primitive placement. (c) We implement adaptive residual pruning based on SDF update history to surpress over-segmentation and ensure compact results. In addition, Light-SQ supports multiscale fitting, enabling localized refinement to preserve fine geometric details. To evaluate our method, we introduce 3DGen-Prim, a benchmark extending 3DGen-Bench with new metrics for both reconstruction quality and primitive-level editability. Extensive experiments demonstrate that Light-SQ enables efficient, high-fidelity, and editable shape abstraction with superquadrics for complex generated geometry, advancing the feasibility of 3D UGC creation. 

**Abstract (ZH)**: 基于超方程的结构感知3D形状抽象框架：Light-SQ 

---
# Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards 

**Title (ZH)**: 随机策略评估足以实现带有可验证奖励的LLM推理 

**Authors**: Haoran He, Yuxiao Ye, Qingpeng Cai, Chen Hu, Binxing Jiao, Daxin Jiang, Ling Pan  

**Link**: [PDF](https://arxiv.org/pdf/2509.24981)  

**Abstract**: RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving the reasoning abilities of large language models (LLMs). Current methods rely primarily on policy optimization frameworks like PPO and GRPO, which follow generalized policy iteration that alternates between evaluating the current policy's value and improving the policy based on evaluation. While effective, they often suffer from training instability and diversity collapse, requiring complex heuristic tricks and careful tuning. We observe that standard RLVR in math reasoning can be formalized as a specialized finite-horizon Markov Decision Process with deterministic state transitions, tree-structured dynamics, and binary terminal rewards. Though large in scale, the underlying structure is simpler than general-purpose control settings for which popular RL algorithms (e.g., PPO) were developed, suggesting that several sophisticated techniques in existing methods may be reduced or even omitted. Based on this insight, we prove a surprising result: the optimal action can be recovered from the Q-function of a fixed uniformly random policy, thereby bypassing the generalized policy iteration loop and its associated heuristics. We introduce Random Policy Valuation for Diverse Reasoning (ROVER) to translate this principle into a practical and scalable algorithm for LLM math reasoning, a minimalist yet highly effective RL method that samples actions from a softmax over these uniform-policy Q-values. ROVER preserves diversity throughout training, allowing sustained exploration of multiple valid pathways. Across multiple base models and standard math reasoning benchmarks, ROVER demonstrates superior performance in both \textbf{quality} (\textbf{+8.2} on pass@1, \textbf{+16.8} on pass@256) and \textbf{diversity} (\textbf{+17.6\%}), despite its radical simplification compared to strong, complicated existing methods. 

**Abstract (ZH)**: 具有可验证奖励的RL（RLVR）在提高大规模语言模型的推理能力方面展现了前景：RLVR在数学推理中的形式化 

---
# SecInfer: Preventing Prompt Injection via Inference-time Scaling 

**Title (ZH)**: SecInfer: 在推理时缩放以防止提示注入 

**Authors**: Yupei Liu, Yanting Wang, Yuqi Jia, Jinyuan Jia, Neil Zhenqiang Gong  

**Link**: [PDF](https://arxiv.org/pdf/2509.24967)  

**Abstract**: Prompt injection attacks pose a pervasive threat to the security of Large Language Models (LLMs). State-of-the-art prevention-based defenses typically rely on fine-tuning an LLM to enhance its security, but they achieve limited effectiveness against strong attacks. In this work, we propose \emph{SecInfer}, a novel defense against prompt injection attacks built on \emph{inference-time scaling}, an emerging paradigm that boosts LLM capability by allocating more compute resources for reasoning during inference. SecInfer consists of two key steps: \emph{system-prompt-guided sampling}, which generates multiple responses for a given input by exploring diverse reasoning paths through a varied set of system prompts, and \emph{target-task-guided aggregation}, which selects the response most likely to accomplish the intended task. Extensive experiments show that, by leveraging additional compute at inference, SecInfer effectively mitigates both existing and adaptive prompt injection attacks, outperforming state-of-the-art defenses as well as existing inference-time scaling approaches. 

**Abstract (ZH)**: Prompt注入攻击对大型语言模型的安全性构成广泛威胁。最先进的基于预防的防御通常依赖于对大型语言模型进行微调以增强其安全性，但它们对强大的攻击效果有限。在本文中，我们提出了一种名为SecInfer的新颖防御方法，该方法基于推理时扩展这一新兴范式，通过在推理时分配更多的计算资源来提升大型语言模型的能力。SecInfer包括两个关键步骤：系统提示引导采样和目标任务引导聚合。通过在推理时利用额外的计算资源，SecInfer有效缓解了现有和适应性的Prompt注入攻击，性能优于最先进的防御方法以及现有的推理时扩展方法。 

---
# MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic Manipulation 

**Title (ZH)**: MSG：多流生成策略在样本高效机器人操作中的应用 

**Authors**: Jan Ole von Hartz, Lukas Schweizer, Joschka Boedecker, Abhinav Valada  

**Link**: [PDF](https://arxiv.org/pdf/2509.24956)  

**Abstract**: Generative robot policies such as Flow Matching offer flexible, multi-modal policy learning but are sample-inefficient. Although object-centric policies improve sample efficiency, it does not resolve this limitation. In this work, we propose Multi-Stream Generative Policy (MSG), an inference-time composition framework that trains multiple object-centric policies and combines them at inference to improve generalization and sample efficiency. MSG is model-agnostic and inference-only, hence widely applicable to various generative policies and training paradigms. We perform extensive experiments both in simulation and on a real robot, demonstrating that our approach learns high-quality generative policies from as few as five demonstrations, resulting in a 95% reduction in demonstrations, and improves policy performance by 89 percent compared to single-stream approaches. Furthermore, we present comprehensive ablation studies on various composition strategies and provide practical recommendations for deployment. Finally, MSG enables zero-shot object instance transfer. We make our code publicly available at this https URL. 

**Abstract (ZH)**: 生成式机器人策略（如流匹配）提供了灵活的多模态策略学习能力，但样本利用效率较低。尽管以对象为中心的策略提高了样本利用效率，但这并未解决这一局限性。在本文中，我们提出了一种多流生成式策略（Multi-Stream Generative Policy, MSG），这是一种推理时的组合框架，训练多个以对象为中心的策略，并在推理时将它们结合起来，以提高泛化能力和样本利用效率。MSG 是模型无关且仅用于推理，因此可以广泛应用于各种生成式策略和训练范式。我们在仿真和实际机器人上进行了广泛的实验，证明我们的方法可以从最少五个演示中学习高质量的生成式策略，导致演示次数减少了95%，并且与单流方法相比，策略性能提高了89%。此外，我们对多种组合策略进行了全面的消融研究，并提供了实用的部署建议。最后，MSG 允许零样本对象实例迁移。我们将在以下网址公开我们的代码：这个 https URL。 

---
# Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer 

**Title (ZH)**: 基于深层Q网络的学习可区分表示方法及其在线性转移中的应用 

**Authors**: Sooraj Sathish, Keshav Goyal, Raghuram Bharadwaj Diddigi  

**Link**: [PDF](https://arxiv.org/pdf/2509.24947)  

**Abstract**: Deep Reinforcement Learning (RL) has demonstrated success in solving complex sequential decision-making problems by integrating neural networks with the RL framework. However, training deep RL models poses several challenges, such as the need for extensive hyperparameter tuning and high computational costs. Transfer learning has emerged as a promising strategy to address these challenges by enabling the reuse of knowledge from previously learned tasks for new, related tasks. This avoids the need for retraining models entirely from scratch. A commonly used approach for transfer learning in RL is to leverage the internal representations learned by the neural network during training. Specifically, the activations from the last hidden layer can be viewed as refined state representations that encapsulate the essential features of the input. In this work, we investigate whether these representations can be used as input for training simpler models, such as linear function approximators, on new tasks. We observe that the representations learned by standard deep RL models can be highly correlated, which limits their effectiveness when used with linear function approximation. To mitigate this problem, we propose a novel deep Q-learning approach that introduces a regularization term to reduce positive correlations between feature representation of states. By leveraging these reduced correlated features, we enable more effective use of linear function approximation in transfer learning. Through experiments and ablation studies on standard RL benchmarks and MinAtar games, we demonstrate the efficacy of our approach in improving transfer learning performance and thereby reducing computational overhead. 

**Abstract (ZH)**: 深度强化学习（RL）通过将神经网络与RL框架结合，展示了在解决复杂序贯决策问题方面的成功。然而，训练深度RL模型面临着诸多挑战，如超参数调优需求广泛和高昂的计算成本。迁移学习作为一种有前途的策略，通过利用先前学习任务中获得的知识来解决这些挑战，使其能够为目标相关的新任务复用知识，从而避免从头重新训练模型。在RL中，使用迁移学习的一个常用方法是利用神经网络在训练过程中学习到的内部表示。具体而言，最后一隐藏层的激活可以视为改进的状态表示，这些表示包含了输入的重要特征。在本工作中，我们研究这些表示是否可以作为输入用于训练简单模型，如线性函数逼近器，在新任务上的训练。我们发现标准深度RL模型学习到的表示之间高度相关，这限制了其在使用线性函数逼近时的有效性。为缓解这一问题，我们提出了一种新颖的深度Q学习方法，引入正则化项以减少状态特征表示之间的正相关性。通过利用这些减少的相关特征，我们使线性函数逼近在迁移学习中的有效性得以提升。通过在标准RL基准和MinAtar游戏中进行实验和消融研究，我们展示了该方法在提高迁移学习性能方面的能力，从而减少了计算开销。 

---
# MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes 

**Title (ZH)**: MobileLLM-R1：探索亚亿参数语言模型推理器的能力极限与开放训练配方 

**Authors**: Changsheng Zhao, Ernie Chang, Zechun Liu, Chia-Jung Chang, Wei Wen, Chen Lai, Rick Cao, Yuandong Tian, Raghuraman Krishnamoorthi, Yangyang Shi, Vikas Chandra  

**Link**: [PDF](https://arxiv.org/pdf/2509.24945)  

**Abstract**: The paradigm shift in large language models (LLMs) from instinctive responses to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1) reasoning capabilities only emerge in sufficiently large models, and (2) such capabilities require training on massive datasets. While the first assumption has already been challenged by recent sub-billion-parameter reasoning models such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely unquestioned. In this work, we revisit the necessity of scaling to extremely large corpora (>10T tokens) for reasoning emergence. By carefully curating and resampling open-source datasets that we identify as beneficial under our designed metrics, we demonstrate that strong reasoning abilities can emerge with far less data. Specifically, we show that only ~2T tokens of high-quality data are sufficient, and pre-training with 4.2T tokens on the dataset resampled from these ~2T tokens, followed by a established post-training procedure, enables the development of MobileLLM-R1, a series of sub-billion-parameter reasoning models that substantially outperform prior models trained on fully open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of 15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B. Remarkably, despite being trained on only 11.7% of the tokens compared to Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate further research in this direction, we have released the complete training recipe, data sources, data mixing ratio, and model checkpoints, together with the key insights obtained throughout this study. 

**Abstract (ZH)**: 大型语言模型（LLMs）从直觉响应转变为链式思考（CoT）推理的范式转变促使了两种主导假设的出现：（1）推理能力仅在足够大的模型中出现，（2）此类能力需要在大量数据集上进行训练。虽然第一个假设已经被最近的次亿参数推理模型如Qwen3-0.6B和DeepSeek精炼变体所挑战，第二个假设依然基本未受质疑。在此项工作中，我们重新审视了推理能力出现对极其大量语料库（>10T令牌）的必要性。通过精心筛选和重新采样我们设计指标下认定有益的开源数据集，我们证明了强大的推理能力可以使用远少的数据出现。具体而言，我们证明了仅约2T令牌的高质量数据就足够，通过从这些约2T令牌重新采样获得的4.2T令牌进行预训练，再结合现成的后训练程序，可以开发出MobileLLM-R1系列次亿参数推理模型，这些模型在多个推理基准测试中显著优于从前完全使用开源数据训练的模型。例如，MobileLLM-R1-950M获得了AIME得分为15.5，而OLMo-2-1.48B仅为0.6，SmolLM-2-1.7B仅为0.3。值得注意的是，尽管MobileLLM-R1-950M在预训练时仅使用了与Qwen3的36T令牌私有语料库相比11.7%的令牌，但在多个推理基准测试中，它还是与Qwen3-0.6B相匹配甚至超越。为了促进这一领域的进一步研究，我们已经发布了完整的训练食谱、数据来源、数据混合比例以及模型检查点，还包括本研究中获得的关键见解。 

---
# Scalable GANs with Transformers 

**Title (ZH)**: 可扩展的Transformer地带网络 

**Authors**: Sangeek Hyun, MinKyu Lee, Jae-Pil Heo  

**Link**: [PDF](https://arxiv.org/pdf/2509.24935)  

**Abstract**: Scalability has driven recent advances in generative modeling, yet its principles remain underexplored for adversarial learning. We investigate the scalability of Generative Adversarial Networks (GANs) through two design choices that have proven to be effective in other types of generative models: training in a compact Variational Autoencoder latent space and adopting purely transformer-based generators and discriminators. Training in latent space enables efficient computation while preserving perceptual fidelity, and this efficiency pairs naturally with plain transformers, whose performance scales with computational budget. Building on these choices, we analyze failure modes that emerge when naively scaling GANs. Specifically, we find issues as underutilization of early layers in the generator and optimization instability as the network scales. Accordingly, we provide simple and scale-friendly solutions as lightweight intermediate supervision and width-aware learning-rate adjustment. Our experiments show that GAT, a purely transformer-based and latent-space GANs, can be easily trained reliably across a wide range of capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art single-step, class-conditional generation performance (FID of 2.96) on ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines. 

**Abstract (ZH)**: 生成模型的可扩展性已推动了近期的进步，然而其原理在对抗学习中的应用仍待深入探索。我们通过两种在其他生成模型中 proven effective 的设计选择来研究生成对抗网络（GANs）的可扩展性：在紧凑的变分自编码器潜空间中训练以及采用纯变压器生成器和判别器。在潜空间中训练使得计算高效且保留感知保真度，这种效率与性能随计算预算线性扩展的纯Transformer自然配对。基于这些选择，我们分析了盲目扩展GANs时出现的故障模式，特别是发现生成器早期层的利用率不足和网络扩展时的优化不稳定问题。相应地，我们提供了解决方案，即轻量级中间监督和宽度感知的学习率调整。我们的实验表明，GAT（纯Transformer和潜空间GANs）可以在广泛的能力范围内（S到XL）可靠地训练。此外，GAT-XL/2在ImageNet-256上的单步条件生成性能（FID为2.96）达到最先进的效果，并且仅需40个epoch，比强大基线少6倍。 

---
# When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training 

**Title (ZH)**: 贪心取胜：元臂 bandit LLM 训练中的新兴 exploitation 偏好 

**Authors**: Sanxing Chen, Xiaoyin Chen, Yukun Huang, Roy Xie, Bhuwan Dhingra  

**Link**: [PDF](https://arxiv.org/pdf/2509.24923)  

**Abstract**: While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior. 

**Abstract (ZH)**: 大型语言模型在序列决策中的自主探索策略研究：监督微调与强化学习的比较与改进 

---
# Segmentor-Guided Counterfactual Fine-Tuning for Image Synthesis 

**Title (ZH)**: 基于分割器引导的反事实微调图像合成 

**Authors**: Tian Xia, Matthew Sinclair, Andreas Schuh, Fabio De Sousa Ribeiro, Raghav Mehta, Rajat Rasal, Esther Puyol-Antón, Samuel Gerber, Kersten Petersen, Michiel Schaap, Ben Glocker  

**Link**: [PDF](https://arxiv.org/pdf/2509.24913)  

**Abstract**: Counterfactual image generation is a powerful tool for augmenting training data, de-biasing datasets, and modeling disease. Current approaches rely on external classifiers or regressors to increase the effectiveness of subject-level interventions (e.g., changing the patient's age). For structure-specific interventions (e.g., changing the area of the left lung in a chest radiograph), we show that this is insufficient, and can result in undesirable global effects across the image domain. Previous work used pixel-level label maps as guidance, requiring a user to provide hypothetical segmentations which are tedious and difficult to obtain. We propose Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the simplicity of intervening on scalar-valued, structure-specific variables while producing locally coherent and effective counterfactuals. We demonstrate the capability of generating realistic chest radiographs, and we show promising results for modeling coronary artery disease. Code: this https URL. 

**Abstract (ZH)**: 基于分割图引导的反事实微调（Seg-CFT）：一种用于生成胸部X光片和模拟冠状动脉疾病的简便有效方法 

---
# OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing 

**Title (ZH)**: OpenGPT-4o-Image：一个用于高级图像生成和编辑的综合数据集 

**Authors**: Zhihong Chen, Xuehai Bai, Yang Shi, Chaoyou Fu, Huanyu Zhang, Haotian Wang, Xiaoyan Sun, Zhang Zhang, Liang Wang, Yuanxing Zhang, Pengfei Wan, Yi-Fan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24900)  

**Abstract**: The performance of unified multimodal models for image generation and editing is fundamentally constrained by the quality and comprehensiveness of their training data. While existing datasets have covered basic tasks like style transfer and simple object manipulation, they often lack the systematic structure and challenging scenarios required for real-world applications. To address this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset constructed using a novel methodology that combines hierarchical task taxonomy with automated data generation. Our taxonomy not only includes fundamental capabilities such as text rendering and style control but also introduces highly practical yet challenging categories like scientific imagery for chemistry illustrations and complex instruction editing requiring simultaneous execution of multiple operations. Through an automated pipeline leveraging structured resource pools and GPT-4o, we generate 80k high-quality instruction-image pairs with controlled diversity, covering 11 major domains and 51 subtasks. Extensive experiments show that fine-tuning leading models on our dataset achieves significant performance gains across multiple benchmarks, with improvements of up to 18\% on editing tasks (UniWorld-V1 on ImgEdit-Bench) and 13% on generation tasks (Harmon on GenEval). Our work demonstrates that systematic data construction is key to advancing multimodal AI capabilities. 

**Abstract (ZH)**: 统一多模态模型在图像生成和编辑中的性能从根本上受到其训练数据质量和全面性的限制。现有数据集虽然覆盖了基本任务如风格迁移和简单的对象操作，但在系统结构和具有挑战性的场景方面仍不足以满足实际应用需求。为解决这一瓶颈，我们引入了OpenGPT-4o-Image，这是一种使用将层次任务分类学与自动化数据生成相结合的新方法构建的大规模数据集。我们的分类学不仅包括文本渲染和样式控制等基本能力，还引入了诸如化学插图所需的科学图像以及需要同时执行多个操作的复杂指令编辑等实用性极强但极具挑战性的类别。通过利用结构化资源池和GPT-4o的自动化管道，我们生成了80,000个高质量的指令-图像对，具有可控的多样性，覆盖了11个主要领域和51个子任务。广泛的实验表明，对我们的数据集进行微调，在多个基准测试中取得了显著性能提升，编辑任务的改进高达18%（UniWorld-V1在ImgEdit-Bench上），生成任务的改进高达13%（Harmon在GenEval上）。我们的工作表明，系统性数据构建是推动多模态AI能力进步的关键。 

---
# Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime 

**Title (ZH)**: 浅神经网络在特征学习阶段的标度定律与频谱分布 

**Authors**: Leonardo Defilippis, Yizhou Xu, Julius Girardin, Emanuele Troiani, Vittorio Erba, Lenka Zdeborová, Bruno Loureiro, Florent Krzakala  

**Link**: [PDF](https://arxiv.org/pdf/2509.24882)  

**Abstract**: Neural scaling laws underlie many of the recent advances in deep learning, yet their theoretical understanding remains largely confined to linear models. In this work, we present a systematic analysis of scaling laws for quadratic and diagonal neural networks in the feature learning regime. Leveraging connections with matrix compressed sensing and LASSO, we derive a detailed phase diagram for the scaling exponents of the excess risk as a function of sample complexity and weight decay. This analysis uncovers crossovers between distinct scaling regimes and plateau behaviors, mirroring phenomena widely reported in the empirical neural scaling literature. Furthermore, we establish a precise link between these regimes and the spectral properties of the trained network weights, which we characterize in detail. As a consequence, we provide a theoretical validation of recent empirical observations connecting the emergence of power-law tails in the weight spectrum with network generalization performance, yielding an interpretation from first principles. 

**Abstract (ZH)**: 神经网络中的二次和对角结构在特征学习中的标度律揭示了近期深度学习进展的许多奥秘，然而这些理论理解主要局限于线性模型。本文系统分析了特征学习环境下二次和对角神经网络的标度律。借助矩阵压缩感知和LASSO的联系，我们推导出了过剩风险标度指数与样本复杂度和权重衰减之间的详细相图。这一分析揭示了不同标度律之间的交叉行为和平台行为，反映了在经验神经网络标度文献中广泛报道的现象。此外，我们建立了这些区域与训练网络权重的谱性质之间的精确联系，并详细描述了这些性质。因此，我们提供了一种从第一原理出发的理论验证，最近的经验观察将权重谱中幂律尾巴的出现与网络泛化性能联系起来。 

---
# Vehicle Classification under Extreme Imbalance: A Comparative Study of Ensemble Learning and CNNs 

**Title (ZH)**: 在极端不均衡情况下的车辆分类：集成学习与CNNs的比较研究 

**Authors**: Abu Hanif Muhammad Syarubany  

**Link**: [PDF](https://arxiv.org/pdf/2509.24880)  

**Abstract**: Accurate vehicle type recognition underpins intelligent transportation and logistics, but severe class imbalance in public datasets suppresses performance on rare categories. We curate a 16-class corpus (~47k images) by merging Kaggle, ImageNet, and web-crawled data, and create six balanced variants via SMOTE oversampling and targeted undersampling. Lightweight ensembles, such as Random Forest, AdaBoost, and a soft-voting combiner built on MobileNet-V2 features are benchmarked against a configurable ResNet-style CNN trained with strong augmentation and label smoothing. The best ensemble (SMOTE-combined) attains 74.8% test accuracy, while the CNN achieves 79.19% on the full test set and 81.25% on an unseen inference batch, confirming the advantage of deep models. Nonetheless, the most under-represented class (Barge) remains a failure mode, highlighting the limits of rebalancing alone. Results suggest prioritizing additional minority-class collection and cost-sensitive objectives (e.g., focal loss) and exploring hybrid ensemble or CNN pipelines to combine interpretability with representational power. 

**Abstract (ZH)**: 精确的车辆类型识别是智能交通和物流的基础，但在公共数据集中严重的类别不平衡抑制了对稀有类别的性能。我们通过合并Kaggle、ImageNet和网络抓取数据构建了一个包含16类（约47k张图片）的语料库，并通过SMOTE过采样和目标性下采样创建了六个平衡变体。基准测试了轻量级集成模型，如随机森林、AdaBoost以及基于MobileNet-V2特征的软投票组合器，这些模型与配置可调的具有强增强和标签平滑的ResNet风格CNN进行了对比。最优集成模型（SMOTE组合）在测试集上的准确率为74.8%，而CNN在完整测试集上的准确率为79.19%，在未见过的推理批次上的准确率为81.25%，证实了深度模型的优势。然而，最欠代表的类别（驳船）仍然是一个失败模式，表明仅通过重新平衡无法完全解决问题。结果表明，应优先考虑额外的小类别采集和成本敏感目标（如焦点损失），并探索混合集成或CNN管道，以结合解释性和表现力。 

---
# Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon Annotation 

**Title (ZH)**: 基于不确定性指导的专家-AI协作高效土壤层标注 

**Authors**: Teodor Chiaburu, Vipin Singh, Frank Haußer, Felix Bießmann  

**Link**: [PDF](https://arxiv.org/pdf/2509.24873)  

**Abstract**: Uncertainty quantification is essential in human-machine collaboration, as human agents tend to adjust their decisions based on the confidence of the machine counterpart. Reliably calibrated model uncertainties, hence, enable more effective collaboration, targeted expert intervention and more responsible usage of Machine Learning (ML) systems. Conformal prediction has become a well established model-agnostic framework for uncertainty calibration of ML models, offering statistically valid confidence estimates for both regression and classification tasks. In this work, we apply conformal prediction to $\textit{SoilNet}$, a multimodal multitask model for describing soil profiles. We design a simulated human-in-the-loop (HIL) annotation pipeline, where a limited budget for obtaining ground truth annotations from domain experts is available when model uncertainty is high. Our experiments show that conformalizing SoilNet leads to more efficient annotation in regression tasks and comparable performance scores in classification tasks under the same annotation budget when tested against its non-conformal counterpart. All code and experiments can be found in our repository: this https URL 

**Abstract (ZH)**: 不确定性量化对于人机协作至关重要，因为人类代理往往会根据机器同伴的信心调整其决策。因此，可靠校准的模型不确定性能够促进更有效的协作、针对性的专家干预，并更负责任地使用机器学习系统。一致性预测已成为一种成熟的无模型框架，可用于机器学习模型的不确定性校准，提供统计上有效的置信区间估计，适用于回归和分类任务。在本文中，我们将一致性预测应用于SoilNet，这是一种多模态多任务模型，用于描述土壤剖面。我们设计了一个模拟的人机环注释pipeline，在模型不确定性高时，可用的领域专家 ground truth 注释预算有限。实验结果表明，一致性校准 SoilNet 可以在回归任务中更高效地进行注释，并且在相同注释预算下，其分类任务的表现与非一致性校准版本相当。所有代码和实验可以在我们的仓库中找到：this https URL 

---
# Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval 

**Title (ZH)**: Retro*: 优化大语言模型用于推理密集型文档检索 

**Authors**: Junwei Lan, Jianlyu Chen, Zheng Liu, Chaofan Li, Siqi Bao, Defu Lian  

**Link**: [PDF](https://arxiv.org/pdf/2509.24869)  

**Abstract**: With the growing popularity of LLM agents and RAG, it has become increasingly important to retrieve documents that are essential for solving a task, even when their connection to the task is indirect or implicit. Addressing this problem requires fine-grained reasoning to accurately assess the relevance between the task and each candidate document. This capability, however, poses a significant challenge for existing IR techniques. Despite recent progress in reasoning-enhanced IR, existing approaches still face significant challenges in applicability, scalability, and efficiency. In this work, we propose Retro*, a novel approach for reasoning-intensive document retrieval. Our method introduces a rubric-based relevance scoring mechanism, enabling the model to reason about the relationship between a task and a document based on explicitly defined criteria, whereby producing a fine-grained, interpretable relevance score. Retro* also supports test-time scaling by combining multiple reasoning trajectories via score integration, which produces more reliable relevance estimates. To optimize Retro*'s reasoning capabilities, we introduce a novel reinforcement learning algorithm tailored for its relevance scoring mechanism, which employs two composite rewards to fully exploit the trajectories of each training sample. Our experiments show that Retro* outperforms existing document retrieval methods with notable advantages, leading to state-of-the-art performance on the BRIGHT benchmark. 

**Abstract (ZH)**: 随着大型语言模型代理和RAG的流行，从间接或隐含关联的文档中检索对完成任务至关重要的文档变得越来越重要。为解决这一问题需要精细推理以准确评估任务与每个候选文档之间的相关性。然而，这种能力为现有信息检索技术带来了重大挑战。尽管在增强推理的信息检索方面取得了进展，现有方法仍然面临着应用性、扩展性和效率方面的重大挑战。在本工作中，我们提出了Retro*，一种新型的密集推理文档检索方法。该方法引入了一种基于评分标准的相关性评分机制，使模型能够基于明确定义的标准来推理任务与文档之间的关系，从而生成细化且可解释的相关性评分。Retro* 还通过分数集成组合多个推理轨迹来支持推理时的扩展性，从而产生更可靠的相关性估计。为了优化Retro*的推理能力，我们引入了一种专门为其实现相关性评分机制设计的新颖强化学习算法，该算法使用两个复合奖励充分利用每个训练样本的轨迹。我们的实验表明，Retro* 在文档检索方面优于现有方法，具有显著优势，达到了BRIGHT基准上的最佳性能。 

---
# Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning 

**Title (ZH)**: 使用大规模语言模型识别隐喻：RAG、提示工程和微调的比较 

**Authors**: Matteo Fuoli, Weihang Huang, Jeannette Littlemore, Sarah Turner, Ellen Wilding  

**Link**: [PDF](https://arxiv.org/pdf/2509.24866)  

**Abstract**: Metaphor is a pervasive feature of discourse and a powerful lens for examining cognition, emotion, and ideology. Large-scale analysis, however, has been constrained by the need for manual annotation due to the context-sensitive nature of metaphor. This study investigates the potential of large language models (LLMs) to automate metaphor identification in full texts. We compare three methods: (i) retrieval-augmented generation (RAG), where the model is provided with a codebook and instructed to annotate texts based on its rules and examples; (ii) prompt engineering, where we design task-specific verbal instructions; and (iii) fine-tuning, where the model is trained on hand-coded texts to optimize performance. Within prompt engineering, we test zero-shot, few-shot, and chain-of-thought strategies. Our results show that state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning yielding a median F1 score of 0.79. A comparison of human and LLM outputs reveals that most discrepancies are systematic, reflecting well-known grey areas and conceptual challenges in metaphor theory. We propose that LLMs can be used to at least partly automate metaphor identification and can serve as a testbed for developing and refining metaphor identification protocols and the theory that underpins them. 

**Abstract (ZH)**: 元喻是一种普遍存在于话语中的特征，是探讨认知、情感和意识形态的强大视角。然而，大规模分析受到了手工标注的限制，因为元喻具有上下文敏感性。本研究探讨了大规模语言模型（LLMs）自动识别全文本中元喻的可能性。我们比较了三种方法：（i）检索增强生成（RAG），模型在提供代码簿的情况下，根据其规则和示例对文本进行标注；（ii）提示工程，我们设计了特定任务的口头指令；（iii）微调，模型在手工标注的文本上进行训练以优化性能。在提示工程中，我们测试了零样本、少样本和思维链策略。我们的结果显示，最先进的闭源LLMs可以实现高精度，微调方法的中位数F1得分为0.79。人类和LLM输出的比较表明，大多数差异是系统性的，反映了元喻理论中众所周知的灰色地带和概念挑战。我们提出，LLMs可以至少部分自动化元喻识别，并可作为开发和精炼元喻识别协议及其支撑理论的测试床。 

---
# Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement 

**Title (ZH)**: 大型语言模型的分层错误纠正：一种特定领域AI质量提升的系统框架 

**Authors**: Zhilong Zhao, Yindi Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24841)  

**Abstract**: Large Language Models face significant performance challenges in specialized domains, with state-of-the-art models achieving only 45.9% accuracy on medical coding tasks. This study proposes a Hierarchical Error Correction (HEC) framework that addresses domain-specific AI limitations through systematic error analysis and targeted intervention strategies.
We analyze error patterns across four specialized domains and find that AI errors follow consistent hierarchical structures: Knowledge-layer errors (58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%). Based on these patterns, we develop a three-stage correction framework that addresses errors according to their hierarchical importance and demonstrates that framework effectiveness correlates inversely with baseline task performance.
Experimental validation across medical transcription (4,921 cases), legal document classification (1,000 cases), political bias detection (645 cases), and legal reasoning (1,000 cases) shows consistent improvements. Cross-model validation across five LLM architectures demonstrates average improvements of 11.2 percentage points (p < 0.001). However, analysis reveals framework limitations in high-baseline tasks (>75% accuracy), where hierarchical intervention may interfere with effective reasoning processes.
The results suggest that systematic error analysis can guide effective AI enhancement strategies in specialized domains, particularly for moderate-baseline tasks, while highlighting the importance of understanding framework boundaries for optimal deployment. 

**Abstract (ZH)**: 大规模语言模型在专门领域面临显著的性能挑战，最前沿的模型在医疗编码任务上的准确性仅为45.9%。本研究提出了一种层次错误修正（HEC）框架，通过系统性错误分析和针对性干预策略解决领域特定的人工智能限制。 

---
# SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching 

**Title (ZH)**: SemShareKV: 基于词元级LSH匹配的语义相似提示高效KV缓存共享 

**Authors**: Xinye Zhao, Spyridon Mastorakis  

**Link**: [PDF](https://arxiv.org/pdf/2509.24832)  

**Abstract**: As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference. 

**Abstract (ZH)**: 基于语义的KV缓存共享与压缩框架：加速大型语言模型推理 

---
# Evaluating SAP Joule for Code Generation 

**Title (ZH)**: 评估SAP Joule的代码生成能力 

**Authors**: Joshua Heisler, Johannes Reisinger, Andreas Fischer  

**Link**: [PDF](https://arxiv.org/pdf/2509.24828)  

**Abstract**: SAP has released its own proprietary generative model SAP Joule, intended for various generative tasks, including serving as a code assistant for software engineers. While Joule is yet not focused on SAP-specific ABAP code generation, it can be used for other common languages, including Javascript. This paper compares SAP Joules Javascript coding capabilities against a total of 29 other models using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict accuracy of 80.49% as the fifth best model in our evaluation. To the best of our knowledge, this is the first comparative evaluation of SAP Joule code generation capabilities. 

**Abstract (ZH)**: SAP发布的专有生成模型SAP Joule及其在JavaScript编码能力方面的比较研究 

---
# Putnam-like dataset summary: LLMs as mathematical competition contestants 

**Title (ZH)**: Putnam-like 数据集总结：LLMs 作为数学竞赛选手 

**Authors**: Bartosz Bieganowski, Daniel Strzelecki, Robert Skiba, Mateusz Topolewski  

**Link**: [PDF](https://arxiv.org/pdf/2509.24827)  

**Abstract**: In this paper we summarize the results of the Putnam-like benchmark published by Google DeepMind. This dataset consists of 96 original problems in the spirit of the Putnam Competition and 576 solutions of LLMs. We analyse the performance of models on this set of problems to verify their ability to solve problems from mathematical contests. 

**Abstract (ZH)**: 本文总结了Google DeepMind发布的Putnam-like基准测试的结果。该数据集包含96个具有Putnam竞赛精神的原始问题和576个大型语言模型的解决方案。我们分析模型在这组问题上的表现，以验证其解决数学竞赛问题的能力。 

---
# Of-SemWat: High-payload text embedding for semantic watermarking of AI-generated images with arbitrary size 

**Title (ZH)**: Of-SemWat: 高载荷文本嵌入用于任意大小AI生成图像的语义水印技术 

**Authors**: Benedetta Tondi, Andrea Costanzo, Mauro Barni  

**Link**: [PDF](https://arxiv.org/pdf/2509.24823)  

**Abstract**: We propose a high-payload image watermarking method for textual embedding, where a semantic description of the image - which may also correspond to the input text prompt-, is embedded inside the image. In order to be able to robustly embed high payloads in large-scale images - such as those produced by modern AI generators - the proposed approach builds upon a traditional watermarking scheme that exploits orthogonal and turbo codes for improved robustness, and integrates frequency-domain embedding and perceptual masking techniques to enhance watermark imperceptibility. Experiments show that the proposed method is extremely robust against a wide variety of image processing, and the embedded text can be retrieved also after traditional and AI inpainting, permitting to unveil the semantic modification the image has undergone via image-text mismatch analysis. 

**Abstract (ZH)**: 基于语义描述的大载荷图像水印方法：针对文本嵌入的鲁棒性增强 

---
# Intelligent Optimization of Wireless Access Point Deployment for Communication-Based Train Control Systems Using Deep Reinforcement Learning 

**Title (ZH)**: 基于深度强化学习的通信导向列车控制系统的无线接入点智能部署优化 

**Authors**: Kunyu Wu, Qiushi Zhao, Zihan Feng, Yunxi Mu, Hao Qin, Xinyu Zhang, Xingqi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24819)  

**Abstract**: Urban railway systems increasingly rely on communication based train control (CBTC) systems, where optimal deployment of access points (APs) in tunnels is critical for robust wireless coverage. Traditional methods, such as empirical model-based optimization algorithms, are hindered by excessive measurement requirements and suboptimal solutions, while machine learning (ML) approaches often struggle with complex tunnel environments. This paper proposes a deep reinforcement learning (DRL) driven framework that integrates parabolic wave equation (PWE) channel modeling, conditional generative adversarial network (cGAN) based data augmentation, and a dueling deep Q network (Dueling DQN) for AP placement optimization. The PWE method generates high-fidelity path loss distributions for a subset of AP positions, which are then expanded by the cGAN to create high resolution path loss maps for all candidate positions, significantly reducing simulation costs while maintaining physical accuracy. In the DRL framework, the state space captures AP positions and coverage, the action space defines AP adjustments, and the reward function encourages signal improvement while penalizing deployment costs. The dueling DQN enhances convergence speed and exploration exploitation balance, increasing the likelihood of reaching optimal configurations. Comparative experiments show that the proposed method outperforms a conventional Hooke Jeeves optimizer and traditional DQN, delivering AP configurations with higher average received power, better worst-case coverage, and improved computational efficiency. This work integrates high-fidelity electromagnetic simulation, generative modeling, and AI-driven optimization, offering a scalable and data-efficient solution for next-generation CBTC systems in complex tunnel environments. 

**Abstract (ZH)**: 基于深度强化学习的高精度隧道AP部署优化框架：集成抛物波方程信道建模、条件生成对抗网络数据增强和对冲深度Q网络 

---
# RDD: Pareto Analysis of the Rate-Distortion-Distinguishability Trade-off 

**Title (ZH)**: RDD：率-失真-可区分性权衡的帕累托分析 

**Authors**: Andriy Enttsel, Alex Marchioni, Andrea Zanellini, Mauro Mangia, Gianluca Setti, Riccardo Rovatti  

**Link**: [PDF](https://arxiv.org/pdf/2509.24805)  

**Abstract**: Extensive monitoring systems generate data that is usually compressed for network transmission. This compressed data might then be processed in the cloud for tasks such as anomaly detection. However, compression can potentially impair the detector's ability to distinguish between regular and irregular patterns due to information loss. Here we extend the information-theoretic framework introduced in [1] to simultaneously address the trade-off between the three features on which the effectiveness of the system depends: the effectiveness of compression, the amount of distortion it introduces, and the distinguishability between compressed normal signals and compressed anomalous signals. We leverage a Gaussian assumption to draw curves showing how moving on a Pareto surface helps administer such a trade-off better than simply relying on optimal rate-distortion compression and hoping that compressed signals can be distinguished from each other. 

**Abstract (ZH)**: 广泛监测系统生成的数据通常被压缩以供网络传输。这些压缩数据随后可能在云端处理以进行异常检测等任务。然而，压缩可能会由于信息丢失而影响检测器区分正常和异常模式的能力。我们扩展了在[1]中引入的信息论框架，同时处理系统有效性的三个特征之间的权衡：压缩的有效性、它引入的失真量以及压缩正常信号和压缩异常信号之间的可区分性。我们利用高斯假设绘制曲线，展示在帕累托曲面上移动如何更好地管理这种权衡，而不仅仅是依赖最优率失真压缩并希望压缩信号能够彼此区分。 

---
# DSAT-HD: Dual-Stream Adaptive Transformer with Hybrid Decomposition for Multivariate Time Series Forecasting 

**Title (ZH)**: DSAT-HD：双重流自适应变换器结合混合分解的多变量时间序列预测 

**Authors**: Zixu Wang, Hongbin Dong, Xiaoping Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24800)  

**Abstract**: Time series forecasting is crucial for various applications, such as weather, traffic, electricity, and energy predictions. Currently, common time series forecasting methods are based on Transformers. However, existing approaches primarily model limited time series or fixed scales, making it more challenging to capture diverse features cross different ranges. Additionally, traditional methods like STL for complex seasonality-trend decomposition require pre-specified seasonal periods and typically handle only single, fixed seasonality. We propose the Hybrid Decomposition Dual-Stream Adaptive Transformer (DSAT-HD), which integrates three key innovations to address the limitations of existing methods: 1) A hybrid decomposition mechanism combining EMA and Fourier decomposition with RevIN normalization, dynamically balancing seasonal and trend components through noise Top-k gating; 2) A multi-scale adaptive pathway leveraging a sparse allocator to route features to four parallel Transformer layers, followed by feature merging via a sparse combiner, enhanced by hybrid attention combining local CNNs and global interactions; 3) A dual-stream residual learning framework where CNN and MLP branches separately process seasonal and trend components, coordinated by a balanced loss function minimizing expert collaboration variance. Extensive experiments on nine datasets demonstrate that DSAT-HD outperforms existing methods overall and achieves state-of-the-art performance on some datasets. Notably, it also exhibits stronger generalization capabilities across various transfer scenarios. 

**Abstract (ZH)**: Hybrid Decomposition Dual-Stream Adaptive Transformer (DSAT-HD) for Time Series Forecasting 

---
# Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation 

**Title (ZH)**: 因果适配器：驯化文本到图像扩散模型以实现符合事实的反事实生成 

**Authors**: Lei Tong, Zhihua Liu, Chaochao Lu, Dino Oglic, Tom Diethe, Philip Teare, Sotirios A. Tsaftaris, Chen Jin  

**Link**: [PDF](https://arxiv.org/pdf/2509.24798)  

**Abstract**: We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method enables causal interventions on target attributes, consistently propagating their effects to causal dependents without altering the core identity of the image. In contrast to prior approaches that rely on prompt engineering without explicit causal structure, Causal-Adapter leverages structural causal modeling augmented with two attribute regularization strategies: prompt-aligned injection, which aligns causal attributes with textual embeddings for precise semantic control, and a conditioned token contrastive loss to disentangle attribute factors and reduce spurious correlations. Causal-Adapter achieves state-of-the-art performance on both synthetic and real-world datasets, with up to 91\% MAE reduction on Pendulum for accurate attribute control and 87\% FID reduction on ADNI for high-fidelity MRI image generation. These results show that our approach enables robust, generalizable counterfactual editing with faithful attribute modification and strong identity preservation. 

**Abstract (ZH)**: 我们提出Causal-Adapter，这是一种模块化框架，用于将冻结的文字到图像扩散骨干网络适应于生成反事实图像。该方法允许对目标属性进行因果干预，一致地传播其效应对因果依赖项，而不改变图像的核心身份。与依赖于提示工程而缺乏明确因果结构的先前方法不同，Causal-Adapter 利用增强的结构因果模型，并结合了两种属性正则化策略：提示对齐的注入，该策略使得因果属性与文本嵌入对齐以实现精确的语义控制，以及条件标记对比损失，以分离属性因素并减少虚假相关性。Causal-Adapter 在合成和真实世界数据集上均实现了最先进的性能，在Pendulum数据集上实现了高达91%的MAE减少，在ADNI数据集上实现了高达87%的FID减少，用于高保真MRI图像生成。这些结果表明，我们的方法能够实现稳健、具有泛化能力的反事实编辑，并且具有忠实的属性修改和强大的身份保留能力。 

---
# Fidelity-Aware Data Composition for Robust Robot Generalization 

**Title (ZH)**: fidelity-意识的数据组合以实现稳健的机器人泛化 

**Authors**: Zizhao Tong, Di Chen, Sicheng Hu, Hongwei Fan, Liliang Chen, Guanghui Ren, Hao Tang, Hao Dong, Ling Shao  

**Link**: [PDF](https://arxiv.org/pdf/2509.24797)  

**Abstract**: Generalist robot policies trained on large-scale, visually homogeneous datasets can be susceptible to shortcut learning, which impairs their out-of-distribution (OOD) generalization. While generative data augmentation is a common approach to introduce diversity, it presents a subtle challenge: data composition. Naively mixing real and synthetic data can corrupt the learning signal, as this process often prioritizes visual diversity at the expense of information fidelity. This paper suggests that robust generalization depends on principled, fidelity-aware data composition. We introduce Coherent Information Fidelity Tuning (CIFT), a framework that treats data composition as an optimization problem. CIFT uses a practical proxy for Information Fidelity based on the feature-space geometry of a dataset. This enables the identification of a phase transition, termed the Decoherence Point, where training stability degrades. The framework includes a generative engine, Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled data spectrum for this tuning process. Applying CIFT to policy architectures such as $\pi_0$ and Diffusion Policy improves OOD success rates by over 54\%. These results indicate that fidelity-aware composition, beyond data synthesis alone, is an important component for developing robust, general-purpose robots. 

**Abstract (ZH)**: 通用机器人策略在大规模视觉同质化数据集上训练时，可能会遭受捷径学习的影响，这会损害其分布外（OOD）泛化能力。尽管生成数据增强是引入多样性的一种常见方法，但它提出了一个微妙的挑战：数据组合。简单地混合真实数据和合成数据可能会破坏学习信号，因为这个过程往往优先考虑视觉多样性而牺牲信息 fidelity。本文建议，稳健的泛化依赖于基于信息 fidelity 的原则性数据组合。我们介绍了相干信息 fidelity 调整（CIFT）框架，该框架将数据组合视为一个优化问题。CIFT 使用数据集特征空间几何结构的实用代理来衡量信息 fidelity。这使得能够在训练稳定性下降的临界点——称为去相干点——处识别出这一相变。该框架包括一个生成引擎，多视点视频增强（MVAug），以合成因果解纠缠的数据频谱，用于这一调整过程。将 CIFT 应用于策略架构如 $\pi_0$ 和扩散策略，可以将分布外成功率提高超过 54%。这些结果表明，除了数据合成外，基于 fidelity 的组合是开发稳健的通用机器人的重要组成部分。 

---
# Sparse Autoencoders Make Audio Foundation Models more Explainable 

**Title (ZH)**: 稀疏自编码器使音频基础模型更具可解释性 

**Authors**: Théo Mariotte, Martin Lebourdais, Antonio Almudévar, Marie Tahon, Alfonso Ortega, Nicolas Dugué  

**Link**: [PDF](https://arxiv.org/pdf/2509.24793)  

**Abstract**: Audio pretrained models are widely employed to solve various tasks in speech processing, sound event detection, or music information retrieval. However, the representations learned by these models are unclear, and their analysis mainly restricts to linear probing of the hidden representations. In this work, we explore the use of Sparse Autoencoders (SAEs) to analyze the hidden representations of pretrained models, focusing on a case study in singing technique classification. We first demonstrate that SAEs retain both information about the original representations and class labels, enabling their internal structure to provide insights into self-supervised learning systems. Furthermore, we show that SAEs enhance the disentanglement of vocal attributes, establishing them as an effective tool for identifying the underlying factors encoded in the representations. 

**Abstract (ZH)**: 预训练模型在语音处理、声源检测或音乐信息检索等任务中广泛应用，但这些模型学习到的表示尚不明确，其分析主要局限于隐藏表示的线性探查。在本文中，我们探索使用稀疏自编码器（SAEs）来分析预训练模型的隐藏表示，并集中讨论其在歌唱技巧分类中的应用案例。我们首先证明SAEs能够保留原始表示和类别标签的信息，使其实内结构能够为监督学习系统提供洞见。此外，我们展示了SAEs在区分声学属性方面的增强作用，确立了其作为识别表示中潜在因子的有效工具的地位。 

---
# Quantifying Generalisation in Imitation Learning 

**Title (ZH)**: 量化模仿学习中的泛化能力 

**Authors**: Nathan Gavenski, Odinaldo Rodrigues  

**Link**: [PDF](https://arxiv.org/pdf/2509.24784)  

**Abstract**: Imitation learning benchmarks often lack sufficient variation between training and evaluation, limiting meaningful generalisation assessment. We introduce Labyrinth, a benchmarking environment designed to test generalisation with precise control over structure, start and goal positions, and task complexity. It enables verifiably distinct training, evaluation, and test settings. Labyrinth provides a discrete, fully observable state space and known optimal actions, supporting interpretability and fine-grained evaluation. Its flexible setup allows targeted testing of generalisation factors and includes variants like partial observability, key-and-door tasks, and ice-floor hazards. By enabling controlled, reproducible experiments, Labyrinth advances the evaluation of generalisation in imitation learning and provides a valuable tool for developing more robust agents. 

**Abstract (ZH)**: 模仿学习基准往往缺乏训练和评估之间的足够变化，限制了有意义的泛化评估。我们引入了Labyrinth，一种设计用于测试泛化的基准环境，可通过精确控制结构、起始和目标位置以及任务复杂性来实现。它允许验证不同的训练、评估和测试设置。Labyrinth提供了一个离散且完全可观测的状态空间及已知的最优动作，支持可解释性和精细评估。其灵活的设置允许针对泛化因素进行目标测试，并包括诸如部分可观测性、钥匙与门任务以及冰面障碍等变体。通过实现可控且可重复的实验，Labyrinth推进了模仿学习中泛化的评估，并提供了一种开发更高鲁棒性代理的重要工具。 

---
# VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding 

**Title (ZH)**: VTPerception-R1: 通过显式的视觉和文本感知接地增强多模态推理 

**Authors**: Yizhuo Ding, Mingkang Chen, Zhibang Feng, Tong Xiao, Wanying Qu, Wenqi Shao, Yanwei Fu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24776)  

**Abstract**: Multimodal large language models (MLLMs) often struggle to ground reasoning in perceptual evidence. We present a systematic study of perception strategies-explicit, implicit, visual, and textual-across four multimodal benchmarks and two MLLMs. Our findings show that explicit perception, especially when paired with textual cues, consistently yields the best improvements, particularly for smaller models. Based on this insight, we propose VTPerception-R1, a unified two-stage framework that decouples perception from reasoning. Stage 1 introduces perception-augmented fine-tuning, and Stage 2 applies perception-aware reinforcement learning with novel visual, textual, and consistency rewards. Experiments demonstrate that VTPerception-R1 significantly improves reasoning accuracy and robustness across diverse tasks, offering a scalable and auditable solution for perception-grounded multimodal reasoning. Our code is available at: this https URL. 

**Abstract (ZH)**: 多模态大语言模型（MLLMs）往往难以将推理扎根于感知证据。我们系统研究了感知策略（显式、隐式、视觉和文本）在四个多模态基准和两个MLLMs上的应用。我们的研究发现，尤其是当与文本提示结合时，显式感知可以持续带来最佳改进，尤其是在较小型模型上。基于此洞察，我们提出了一种统一的两阶段框架VTPerception-R1，该框架将感知与推理分离。第一阶段引入感知增强的微调，第二阶段应用感知感知增强的强化学习，并采用新的视觉、文本和一致性奖励。实验表明，VTPerception-R1 显著提高了不同任务的推理准确性和鲁棒性，提供了一种可扩展且可审计的感知导向多模态推理解决方案。我们的代码可在以下链接获取：this https URL。 

---
# VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning 

**Title (ZH)**: VSSFlow：联合学习统一视频条件的声音和语音生成 

**Authors**: Xin Cheng, Yuyue Wang, Xihua Wang, Yihan Wu, Kaisi Guan, Yijing Chen, Peng Zhang, Xiaojiang Liu, Meng Cao, Ruihua Song  

**Link**: [PDF](https://arxiv.org/pdf/2509.24773)  

**Abstract**: Video-conditioned sound and speech generation, encompassing video-to-sound (V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed as separate tasks, with limited exploration to unify them within a signle framework. Recent attempts to unify V2S and VisualTTS face challenges in handling distinct condition types (e.g., heterogeneous video and transcript conditions) and require complex training stages. Unifying these two tasks remains an open problem. To bridge this gap, we present VSSFlow, which seamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching framework. VSSFlow uses a novel condition aggregation mechanism to handle distinct input signals. We find that cross-attention and self-attention layer exhibit different inductive biases in the process of introducing condition. Therefore, VSSFlow leverages these inductive biases to effectively handle different representations: cross-attention for ambiguous video conditions and self-attention for more deterministic speech transcripts. Furthermore, contrary to the prevailing belief that joint training on the two tasks requires complex training strategies and may degrade performance, we find that VSSFlow benefits from the end-to-end joint learning process for sound and speech generation without extra designs on training stages. Detailed analysis attributes it to the learned general audio prior shared between tasks, which accelerates convergence, enhances conditional generation, and stabilizes the classifier-free guidance process. Extensive experiments demonstrate that VSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S and VisualTTS benchmarks, underscoring the critical potential of unified generative models. 

**Abstract (ZH)**: 视频条件下的声音和语音生成：VSSFlow统一视频到声音和视觉文本到语音任务 

---
# Surjective Independence of Causal Influences for Local Bayesian Network Structures 

**Title (ZH)**: 局部贝叶斯网络结构上的因果影响的满射独立性 

**Authors**: Kieran Drury, Martine J. Barons, Jim Q. Smith  

**Link**: [PDF](https://arxiv.org/pdf/2509.24759)  

**Abstract**: The very expressiveness of Bayesian networks can introduce fresh challenges due to the large number of relationships they often model. In many domains, it is thus often essential to supplement any available data with elicited expert judgements. This in turn leads to two key challenges: the cognitive burden of these judgements is often very high, and there are a very large number of judgements required to obtain a full probability model. We can mitigate both issues by introducing assumptions such as independence of causal influences (ICI) on the local structures throughout the network, restricting the parameter space of the model. However, the assumption of ICI is often unjustified and overly strong. In this paper, we introduce the surjective independence of causal influences (SICI) model which relaxes the ICI assumption and provides a more viable, practical alternative local structure model that facilitates efficient Bayesian network parameterisation. 

**Abstract (ZH)**: 贝叶斯网络的很强的表达能力因它们通常建模的关系数量庞大而引入了新的挑战。因此，在许多领域中，常需要补充可用数据以获取专家判断。这进而导致两个关键挑战：这些判断的认知负担通常很高，且需要大量的判断以获得完整的概率模型。通过引入局部结构中的因果影响的满射独立性（SICI）假设，限制模型的参数空间，我们可以缓解这些问题。然而，因果影响独立性（ICI）假设往往不合理且过于强硬。本文引入了因果影响的满射独立性（SICI）模型，该模型放松了ICI假设，提供了一种更可行且实用的局部结构模型，有助于高效地进行贝叶斯网络参数化。 

---
# Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption 

**Title (ZH)**: 离线到在线RL在多样化数据污染下的鲁棒策略扩展 

**Authors**: Longxiang He, Deheng Ye, Junbo Tan, Xueqian Wang, Li Shen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24748)  

**Abstract**: Pretraining a policy on offline data followed by fine-tuning through online interactions, known as Offline-to-Online Reinforcement Learning (O2O RL), has emerged as a promising paradigm for real-world RL deployment. However, both offline datasets and online interactions in practical environments are often noisy or even maliciously corrupted, severely degrading the performance of O2O RL. Existing works primarily focus on mitigating the conservatism of offline policies via online exploration, while the robustness of O2O RL under data corruption, including states, actions, rewards, and dynamics, is still unexplored. In this work, we observe that data corruption induces heavy-tailed behavior in the policy, thereby substantially degrading the efficiency of online exploration. To address this issue, we incorporate Inverse Probability Weighted (IPW) into the online exploration policy to alleviate heavy-tailedness, and propose a novel, simple yet effective method termed $\textbf{RPEX}$: $\textbf{R}$obust $\textbf{P}$olicy $\textbf{EX}$pansion. Extensive experimental results on D4RL datasets demonstrate that RPEX achieves SOTA O2O performance across a wide range of data corruption scenarios. Code is available at $\href{this https URL}{this https URL}$. 

**Abstract (ZH)**: 离线数据预训练结合在线微调的离线到在线强化学习（O2O RL）在实际部署中展现出潜力，但由于实际环境中的离线数据集和在线交互往往存在噪音甚至恶意篡改，严重降低了O2O RL的性能。现有工作主要关注通过在线探索减轻离线策略的保守性，而数据篡改对状态、动作、奖励和动力学的影响下的O2O RL鲁棒性尚未被研究。在本文中，我们发现数据篡改导致策略行为服从厚尾分布，显著降低了在线探索的效率。为了解决这一问题，我们将在线探索策略中引入逆概率加权（IPW）以缓解厚尾性，并提出了一种新颖且有效的方法RPEX：鲁棒策略扩展。在D4RL数据集上的广泛实验结果表明，RPEX在多种数据篡改场景中达到了SOTA的O2O性能。代码可在$\href{this https URL}{this https URL}$获取。 

---
# A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity 

**Title (ZH)**: A TRIANGLE 矩形实现超越余弦相似性的多模态对齐 

**Authors**: Giordano Cicchetti, Eleonora Grassucci, Danilo Comminiello  

**Link**: [PDF](https://arxiv.org/pdf/2509.24734)  

**Abstract**: Multimodal learning plays a pivotal role in advancing artificial intelligence systems by incorporating information from multiple modalities to build a more comprehensive representation. Despite its importance, current state-of-the-art models still suffer from severe limitations that prevent the successful development of a fully multimodal model. Such methods may not provide indicators that all the involved modalities are effectively aligned. As a result, some modalities may not be aligned, undermining the effectiveness of the model in downstream tasks where multiple modalities should provide additional information that the model fails to exploit. In this paper, we present TRIANGLE: TRI-modAl Neural Geometric LEarning, the novel proposed similarity measure that is directly computed in the higher-dimensional space spanned by the modality embeddings. TRIANGLE improves the joint alignment of three modalities via a triangle-area similarity, avoiding additional fusion layers or pairwise similarities. When incorporated in contrastive losses replacing cosine similarity, TRIANGLE significantly boosts the performance of multimodal modeling, while yielding interpretable alignment rationales. Extensive evaluation in three-modal tasks such as video-text and audio-text retrieval or audio-video classification, demonstrates that TRIANGLE achieves state-of-the-art results across different datasets improving the performance of cosine-based methods up to 9 points of Recall@1. 

**Abstract (ZH)**: 多模态学习在通过集成多种模态信息构建更全面表示以推动人工智能系统发展中发挥关键作用。尽管其重要性，当前最先进的模型仍然遭受严重限制，阻碍了完全多模态模型的成功开发。现有方法可能无法提供所有涉及模态有效对齐的指标。因此，在下游任务中，当多个模态应该提供额外信息但模型未能充分利用时，某些模态可能未对齐，从而削弱了模型的有效性。在本文中，我们提出了TRIANGLE：TRI模态神经几何学习，这是一种直接在由模态嵌入构成的高维空间中计算的新颖相似度度量。TRIANGLE通过三角形面积相似度提高三模态的联合对齐，避免了额外的融合层或成对相似度。当在对比损失中用cosine相似度替换时，TRIANGLE显著提升了多模态建模的性能，同时提供了可解释的对齐理由。在视频-文本和音频-文本检索或音频-视频分类等三项任务的广泛评估中，TRIANGLE在不同数据集中达到了最先进的效果，将基于cosine的方法的Recall@1性能提高了最高9个百分点。 

---
# Q-Net: Transferable Queue Length Estimation via Kalman-based Neural Networks 

**Title (ZH)**: Q-网：基于卡尔曼滤波的神经网络排队长度估计 

**Authors**: Ting Gao, Elvin Isufi, Winnie Daamen, Erik-Sander Smits, Serge Hoogendoorn  

**Link**: [PDF](https://arxiv.org/pdf/2509.24725)  

**Abstract**: Estimating queue lengths at signalized intersections remains a challenge in traffic management, especially under partially observed conditions where vehicle flows are not fully captured. This paper introduces Q-Net, a data-efficient and interpretable framework for queue length estimation that performs robustly even when traffic conservation assumptions are violated. Q-Net integrates two widely available and privacy-friendly data sources: (i) vehicle counts from loop detectors near stop lines, and (ii) aggregated floating car data (aFCD), which divides each road section into segments and provides segment-wise average speed measurements. These data sources often differ in spatial and temporal resolution, creating fusion challenges. Q-Net addresses this by employing a tailored state-space model and an AI-augmented Kalman filter, KalmanNet, which learns the Kalman gain from data without requiring prior knowledge of noise covariances or full system dynamics. We build on the vanilla KalmanNet pipeline to decouple measurement dimensionality from section length, enabling spatial transferability across road segments. Unlike black-box models, Q-Net maintains physical interpretability, with internal variables linked to real-world traffic dynamics. Evaluations on main roads in Rotterdam, the Netherlands, demonstrate that Q-Net outperforms baseline methods by over 60\% in Root Mean Square Error (RMSE), accurately tracking queue formation and dissipation while correcting aFCD-induced delays. Q-Net also demonstrates strong spatial and temporal transferability, enabling deployment without costly sensing infrastructure like cameras or radar. Additionally, we propose a real-time variant of Q-Net, highlighting its potential for integration into dynamic, queue-based traffic control systems. 

**Abstract (ZH)**: 基于信号交叉口队列长度估计的Q-Net框架：一种在部分观测条件下数据高效且可解释的方法 

---
# Discrete Variational Autoencoding via Policy Search 

**Title (ZH)**: 离散变分自编码通过策略搜索 

**Authors**: Michael Drolet, Firas Al-Hafez, Aditya Bhatt, Jan Peters, Oleg Arenz  

**Link**: [PDF](https://arxiv.org/pdf/2509.24716)  

**Abstract**: Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable parameterization; therefore, discrete VAEs typically rely on approximations, such as Gumbel-Softmax reparameterization or straight-through gradient estimates, or employ high-variance gradient-free methods such as REINFORCE that have had limited success on high-dimensional tasks such as image reconstruction. Inspired by popular techniques in policy search, we propose a training framework for discrete VAEs that leverages the natural gradient of a non-parametric encoder to update the parametric encoder without requiring reparameterization. Our method, combined with automatic step size adaptation and a transformer-based encoder, scales to challenging datasets such as ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces, achieving a 20% improvement on FID Score for ImageNet 256. 

**Abstract (ZH)**: 离散潜瓶颈在变分自编码器中的应用提供了高比特效率，并可以通过自回归离散分布进行建模，从而可以用变压器实现参数高效的多模态搜索。然而，离散随机变量不允许精确的可微参数化；因此，离散变分自编码器通常依赖于近似方法，如Gumbel-Softmax重参数化或直接通过梯度估计，或者使用高方差的无梯度方法如REINFORCE，这些方法在如图像重建等高维任务上效果有限。受政策搜索中流行技术的启发，我们提出了一种离散变分自编码器的训练框架，利用非参数编码器的自然梯度来更新参数编码器，无需重参数化。结合自适应步长调整和基于变压器的编码器，该方法可以扩展到如ImageNet这样的具有挑战性的数据集，并在从紧凑的潜空间重构高维数据方面优于近似重参数化方法和基于量化的方法，实现了ImageNet 256在FID分数上的20%改进。 

---
# Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF 

**Title (ZH)**: 电路意识奖励训练：RLHF长尾稳健性的本征框架 

**Authors**: Jing Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24713)  

**Abstract**: Reinforcement Learning from Human Feedback (RLHF) reward models exhibit systematic failures on longtail distributions, leading to reward hacking and misalignment. We propose a mechanistic interpretability framework that identifies specialized neural circuits responsible for rare-event processing in reward models. Drawing from recent advances showing distributed specialization for rare tokens in language models\citep{liu2025no, liu2025emergent}, we hypothesize that reward models also develop functionally distinct circuits for longtail scenarios. Our theoretical framework establishes formal connections between circuit specialization, reward generalization bounds, and longtail performance. We introduce \textbf{Circuit-Aware Reward Training (CART)}, which uses circuit analysis to guide data augmentation, regularization, and ensemble strategies. This approach provides both theoretical insights into reward model failures and practical interventions for improving longtail robustness. 

**Abstract (ZH)**: 基于人类反馈的强化学习（RLHF）奖励模型在长尾分布上表现出系统性的失败，导致奖励作弊和不一致。我们提出了一种机制可解释性框架，该框架识别出负责处理稀有事件的特殊神经电路。借鉴近期研究表明语言模型中对稀有词存在分布式专业化（distributed specialization for rare tokens）的现象（\citet{liu2025no, liu2025emergent}），我们假设奖励模型也发展出了功能上不同的电路来处理长尾场景。我们的理论框架建立了电路专业化、奖励泛化边界和长尾性能之间的正式联系。我们引入了**电路感知奖励训练（CART）**方法，该方法使用电路分析来指导数据增强、正则化和集成策略。该方法为理解和改进奖励模型的长尾稳健性提供了理论洞见和实际干预措施。 

---
# FedPOB: Sample-Efficient Federated Prompt Optimization via Bandits 

**Title (ZH)**: FedPOB: 基于_bandits的高效联邦提示优化 

**Authors**: Pingchen Lu, Zhi Hong, Zhiwei Shang, Zhiyong Wang, Yikun Ban, Yao Shu, Min Zhang, Shuang Qiu, Zhongxiang Dai  

**Link**: [PDF](https://arxiv.org/pdf/2509.24701)  

**Abstract**: The performance of large language models (LLMs) is highly sensitive to the input prompt, making prompt optimization a critical task. However, real-world application is hindered by three major challenges: (1) the black-box nature of powerful proprietary LLMs, (2) the need for high sample efficiency due to query costs, and (3) the desire for privacy-preserving collaboration among multiple users. To address these challenges simultaneously, we introduce a novel framework for sample-efficient federated prompt optimization based on multi-armed bandits (MABs). The MAB framework is uniquely suited for this problem as it is (1) inherently a black-box optimization method, (2) practically sample-efficient, and (3) enables collaborative learning with theoretically guaranteed benefit from more participating agents. We first propose the Federated Prompt Optimization via Bandits (FedPOB) algorithm, a federated variant of the Linear UCB algorithm, where agents collaborate by sharing model parameters instead of raw data. We then extend our approach to the practical setting of comparative user feedback by introducing FedPOB with Preference Feedback (FedPOB-Pref), an efficient algorithm based on federated dueling bandits. Extensive experiments demonstrate that both FedPOB and FedPOB-Pref significantly outperform existing baselines and that their performance consistently improves as more agents participate in the collaboration, validating the effectiveness of our federated approach. 

**Abstract (ZH)**: 基于多臂 bandit 的高效联邦提示优化框架 

---
# T-POP: Test-Time Personalization with Online Preference Feedback 

**Title (ZH)**: T-POP: 测试时个性化处理伴在线偏好反馈 

**Authors**: Zikun Qu, Min Zhang, Mingze Kong, Xiang Li, Zhiwei Shang, Zhiyong Wang, Yikun Ban, Shuang Qiu, Yao Shu, Zhongxiang Dai  

**Link**: [PDF](https://arxiv.org/pdf/2509.24696)  

**Abstract**: Personalizing large language models (LLMs) to individual user preferences is a critical step beyond generating generically helpful responses. However, current personalization methods are ill-suited for new users, as they typically require either slow, resource-intensive fine-tuning or a substantial amount of pre-existing user data, creating a significant cold-start problem. To address this challenge, we introduce a new paradigm for real-time personalization by learning from online pairwise preference feedback collected during text generation. We propose T-POP (Test-Time Personalization with Online Preference Feedback}), a novel algorithm that synergistically combines test-time alignment with dueling bandits. Without updating the LLM parameters, T-POP steers the decoding process of a frozen LLM by learning a reward function online that captures user preferences. By leveraging dueling bandits, T-POP intelligently queries the user to efficiently balance between exploring their preferences and exploiting the learned knowledge to generate personalized text. Extensive experiments demonstrate that T-POP achieves rapid and data-efficient personalization, significantly outperforming existing baselines and showing consistent improvement with more user interactions. 

**Abstract (ZH)**: 基于在线偏好反馈的实时个性化（T-POP） 

---
# SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer 

**Title (ZH)**: SANA-视频：块线性扩散变换器驱动的高效视频生成 

**Authors**: Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, Haozhe Liu, Hongwei Yi, Hao Zhang, Muyang Li, Yukang Chen, Han Cai, Sanja Fidler, Ping Luo, Song Han, Enze Xie  

**Link**: [PDF](https://arxiv.org/pdf/2509.24695)  

**Abstract**: We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation. 

**Abstract (ZH)**: SANA-Video：一种高效生成高清长视频的小规模扩散模型 

---
# CoTune: Co-evolutionary Configuration Tuning 

**Title (ZH)**: 共进化的配置调优 

**Authors**: Gangda Xiong, Tao Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24694)  

**Abstract**: To automatically tune configurations for the best possible system performance (e.g., runtime or throughput), much work has been focused on designing intelligent heuristics in a tuner. However, existing tuner designs have mostly ignored the presence of complex performance requirements (e.g., the latency shall ideally be 2 seconds), but simply assume that better performance is always more preferred. This would not only waste valuable information in a requirement but might also consume extensive resources to tune for a goal with little gain. Yet, prior studies have shown that simply incorporating the requirement as a tuning objective is problematic since the requirement might be too strict, harming convergence; or its highly diverse satisfactions might lead to premature convergence. In this paper, we propose CoTune, a tool that takes the information of a given target performance requirement into account through co-evolution. CoTune is unique in the sense that it creates an auxiliary performance requirement to be co-evolved with the configurations, which assists the target performance requirement when it becomes ineffective or even misleading, hence allowing the tuning to be guided by the requirement while being robust to its harm. Experiment results on 162 cases (nine systems and 18 requirements) reveal that CoTune considerably outperforms existing tuners, ranking as the best for 90% cases (against the 0%--35% for other tuners) with up to 2.9x overall improvements, while doing so under a much better efficiency. 

**Abstract (ZH)**: 一种通过共进化考虑目标性能需求的自动调优工具：CoTune 

---
# Data-Driven Discrete Geofence Design Using Binary Quadratic Programming 

**Title (ZH)**: 基于数据驱动的离散地理围栏设计——二元二次规划方法 

**Authors**: Keisuke Otaki, Akihisa Okada, Tadayoshi Matsumori, Hiroaki Yoshida  

**Link**: [PDF](https://arxiv.org/pdf/2509.24679)  

**Abstract**: Geofences have attracted significant attention in the design of spatial and virtual regions for managing and engaging spatiotemporal events. By using geofences to monitor human activity across their boundaries, content providers can create spatially triggered events that include notifications about points of interest within a geofence by pushing spatial information to the devices of users. Traditionally, geofences were hand-crafted by providers. In addition to the hand-crafted approach, recent advances in collecting human mobility data through mobile devices can accelerate the automatic and data-driven design of geofences, also known as the geofence design problem. Previous approaches assume circular shapes; thus, their flexibility is insufficient, and they can only handle geofence-based applications for large areas with coarse resolutions. A challenge with using circular geofences in urban and high-resolution areas is that they often overlap and fail to align with political district boundaries and road segments, such as one-way streets and median barriers. In this study, we address the problem of extracting arbitrary shapes as geofences from human mobility data to mitigate this problem. In our formulation, we cast the existing optimization problems for circular geofences to 0-1 integer programming problems to represent arbitrary shapes. Although 0-1 integer programming problems are computationally hard, formulating them as quadratic (unconstrained) binary optimization problems enables efficient approximation of optimal solutions, because this allows the use of specialized quadratic solvers, such as the quantum annealing, and other state-of-the-art algorithms. We then develop and compare different formulation methods to extract discrete geofences. We confirmed that our new modeling approach enables flexible geofence design. 

**Abstract (ZH)**: 地理围栏在空间和虚拟区域设计中的关注点：基于人类移动数据的任意形状地理围栏提取 

---
# Reference-Free Rating of LLM Responses via Latent Information 

**Title (ZH)**: 基于潜在信息的无需参考评分方法评估LLM响应 

**Authors**: Leander Girrbach, Chi-Ping Su, Tankred Saanum, Richard Socher, Eric Schulz, Zeynep Akata  

**Link**: [PDF](https://arxiv.org/pdf/2509.24678)  

**Abstract**: How reliable are single-response LLM-as-a-judge ratings without references, and can we obtain fine-grained, deterministic scores in this setting? We study the common practice of asking a judge model to assign Likert-scale scores to free-text responses and show two systematic issues: scores are unstable under sampling and poorly calibrated, leading to compression near the top of the scale and frequent ties. We then propose and evaluate Latent Judges, which derive scalar ratings from internal model signals: (i) probability-weighted scores over integer ratings, (ii) verifier-style probabilities of "yes", and (iii) linear probes trained on model activations at the rating position. Across a broad suite of pairwise and single-rating benchmarks, latent methods match or surpass standard prompting, with consistent gains on pairwise accuracy and listwise ranking relevant to Best-of-N selection. Probability-weighted scores achieve the strongest single-rating correlations, while probes recover useful signals when output logits are miscalibrated. These results indicate that latent information provides deterministic and more discriminative signals for reference-free evaluation, and can improve selection and training approaches like Best-of-$N$, multi-teacher distillation, and routing. 

**Abstract (ZH)**: 单响应LLM作为法官的评分在没有参考的情况下有多可靠？我们能在这种情况下获得细粒度的确定性评分吗？我们研究了让法官模型为自由文本答复分配李克特量表评分的常用做法，并揭示了两个系统性问题：评分在抽样下不稳定且校准不良，导致评分压缩在量表的顶部并频繁出现并列。随后，我们提出了并评估了潜在法官，该方法从内部模型信号-derived scalar ratings from internal model signals:(i) 以整数评分的概率加权分数，(ii) “是”验证器风格的概率，(iii) 在评分位置上模型激活的线性探测器。在广泛的数据对和单评分基准测试中，潜在方法匹配或超越了标准提示，显示出在数据对准确性和相关于Best-of-N选择的列表排序上的一致性改进。以概率加权分数获得最强的单评分相关性，而探测器在输出logits校准不当时仍可恢复有用的信号。这些结果表明，潜在信息提供了参考自由评估中更确定性和更具区分度的信号，并可以改进如Best-of-$N$、多师表蒸馏和路由等选择和训练方法。 

---
# Understanding the Dilemma of Unlearning for Large Language Models 

**Title (ZH)**: 理解大型语言模型退学的困境 

**Authors**: Qingjie Zhang, Haoting Qian, Zhicong Huang, Cheng Hong, Minlie Huang, Ke Xu, Chao Zhang, Han Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24675)  

**Abstract**: Unlearning seeks to remove specific knowledge from large language models (LLMs), but its effectiveness remains contested. On one side, "forgotten" knowledge can often be recovered through interventions such as light fine-tuning; on the other side, unlearning may induce catastrophic forgetting that degrades general capabilities. Despite active exploration of unlearning methods, interpretability analyses of the mechanism are scarce due to the difficulty of tracing knowledge in LLMs' complex architectures. We address this gap by proposing unPact, an interpretable framework for unlearning via prompt attribution and contribution tracking. Typically, it quantifies each prompt token's influence on outputs, enabling pre- and post-unlearning comparisons to reveal what changes. Across six mainstream unlearning methods, three LLMs, and three benchmarks, we find that: (1) Unlearning appears to be effective by disrupting focus on keywords in prompt; (2) Much of the knowledge is not truly erased and can be recovered by simply emphasizing these keywords in prompts, without modifying the model's weights; (3) Catastrophic forgetting arises from indiscriminate penalization of all tokens. Taken together, our results suggest an unlearning dilemma: existing methods tend either to be insufficient - knowledge remains recoverable by keyword emphasis, or overly destructive - general performance collapses due to catastrophic forgetting, still leaving a gap to reliable unlearning. 

**Abstract (ZH)**: 基于提示归因和贡献追踪的可解释卸载框架：解决知识卸载的有效性和危害性难题 

---
# InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation 

**Title (ZH)**: InfLLM-V2: 可切换密集-稀疏注意机制以实现无缝短到长适应 

**Authors**: Weilin Zhao, Zihan Zhou, Zhou Su, Chaojun Xiao, Yuxuan Li, Yanghao Li, Yudi Zhang, Weilun Zhao, Zhen Li, Yuxiang Huang, Ao Sun, Xu Han, Zhiyuan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24663)  

**Abstract**: Long-sequence processing is a critical capability for modern large language models. However, the self-attention mechanism in the standard Transformer architecture faces severe computational and memory bottlenecks when processing long sequences. While trainable sparse attention methods offer a promising solution, existing approaches such as NSA introduce excessive extra parameters and disrupt the conventional \textit{pretrain-on-short, finetune-on-long} workflow, resulting in slow convergence and difficulty in acceleration. To overcome these limitations, we introduce dense-sparse switchable attention framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2 reuses dense attention parameters through parameter-free architecture modification, maintaining consistency between short and long sequence processing. Additionally, InfLLM-V2 ensures computational efficiency across all sequence lengths, by using dense attention for short inputs and smoothly transitioning to sparse attention for long sequences. To achieve practical acceleration, we further introduce an efficient implementation of InfLLM-V2 that significantly reduces the computational overhead. Our experiments on long-context understanding and chain-of-thought reasoning demonstrate that InfLLM-V2 is 4$\times$ faster than dense attention while retaining 98.1% and 99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we have trained and open-sourced MiniCPM4.1 (this https URL), a hybrid reasoning model, providing a reproducible implementation for the research community. 

**Abstract (ZH)**: 长序列处理是现代大规模语言模型的关键能力。然而，标准Transformer架构中的自我注意力机制在处理长序列时面临着严峻的计算和内存瓶颈。虽然可训练的稀疏注意力方法提供了有前景的解决方案，但现有方法如NSA引入了过多的额外参数，并扰乱了传统的“预训练短序列，微调长序列”工作流程，导致收敛速度慢且难以加速。为了克服这些限制，我们介绍了一种可训练的稀疏注意力框架，称为InfLLM-V2。InfLLM-V2能够无缝地将模型从短序列过渡到长序列。具体而言，InfLLM-V2通过参数无改动的架构修改重用密集注意力参数，保持短序列和长序列处理的一致性。此外，InfLLM-V2通过使用密集注意力处理短输入并在长序列上平滑过渡到稀疏注意力，确保所有序列长度下的计算效率。为了实现实际加速，我们还引入了InfLLM-V2的高效实现，显著减少了计算开销。我们的实验表明，InfLLM-V2在长上下文理解与链式推理任务中比密集注意力快4倍，同时分别保留了98.1%和99.7%的性能。基于InfLLM-V2框架，我们已经训练并开源了MiniCPM4.1（请点击此链接），提供了一个可再现的实现供研究界使用。 

---
# Community detection robustness of graph neural networks 

**Title (ZH)**: 图神经网络的社区检测鲁棒性 

**Authors**: Jaidev Goel, Pablo Moriano, Ramakrishnan Kannan, Yulia R. Gel  

**Link**: [PDF](https://arxiv.org/pdf/2509.24662)  

**Abstract**: Graph neural networks (GNNs) are increasingly widely used for community detection in attributed networks. They combine structural topology with node attributes through message passing and pooling. However, their robustness or lack of thereof with respect to different perturbations and targeted attacks in conjunction with community detection tasks is not well understood. To shed light into latent mechanisms behind GNN sensitivity on community detection tasks, we conduct a systematic computational evaluation of six widely adopted GNN architectures: GCN, GAT, Graph- SAGE, DiffPool, MinCUT, and DMoN. The analysis covers three perturbation categories: node attribute manipulations, edge topology distortions, and adversarial attacks. We use element-centric similarity as the evaluation metric on synthetic benchmarks and real-world citation networks. Our findings indicate that supervised GNNs tend to achieve higher baseline accuracy, while unsupervised methods, particularly DMoN, maintain stronger resilience under targeted and adversarial pertur- bations. Furthermore, robustness appears to be strongly influenced by community strength, with well-defined communities reducing performance loss. Across all models, node attribute perturba- tions associated with targeted edge deletions and shift in attribute distributions tend to cause the largest degradation in community recovery. These findings highlight important trade-offs between accuracy and robustness in GNN-based community detection and offer new insights into selecting architectures resilient to noise and adversarial attacks. 

**Abstract (ZH)**: 图神经网络在属性网络社区检测中的鲁棒性研究：基于六种广泛采用的GNN架构的系统计算评估 

---
# VNODE: A Piecewise Continuous Volterra Neural Network 

**Title (ZH)**: VNODE：分段连续维特拉神经网络 

**Authors**: Siddharth Roheda, Aniruddha Bala, Rohit Chowdhury, Rohan Jaiswal  

**Link**: [PDF](https://arxiv.org/pdf/2509.24659)  

**Abstract**: This paper introduces Volterra Neural Ordinary Differential Equations (VNODE), a piecewise continuous Volterra Neural Network that integrates nonlinear Volterra filtering with continuous time neural ordinary differential equations for image classification. Drawing inspiration from the visual cortex, where discrete event processing is interleaved with continuous integration, VNODE alternates between discrete Volterra feature extraction and ODE driven state evolution. This hybrid formulation captures complex patterns while requiring substantially fewer parameters than conventional deep architectures. VNODE consistently outperforms state of the art models with improved computational complexity as exemplified on benchmark datasets like CIFAR10 and Imagenet1K. 

**Abstract (ZH)**: Volterra神经常微分方程（VNODE）：一种用于图像分类的分段连续Volterra神经网络 

---
# Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory 

**Title (ZH)**: 身份桥梁：通过共享潜在记忆实现隐式推理 

**Authors**: Pengxiao Lin, Zheng-An Chen, Zhi-Qin John Xu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24653)  

**Abstract**: Despite remarkable advances, large language models often fail at compositional reasoning tasks, a phenomenon exemplified by the ``curse of two-hop reasoning''. This paper introduces the Identity Bridge, a simple yet powerful mechanism that resolves this compositionality gap by supervising the model on a zero-hop identity task. We demonstrate empirically that this addition enables models to successfully perform out-of-distribution two-hop reasoning, a task they otherwise completely fail. To explain this phenomenon, we provide a theoretical analysis using a simplified Emb-MLP model, proving that identity supervision reshapes the model's latent geometry. We show this alignment is induced by an implicit nuclear-norm regularization during optimization, which favors low-rank solutions that share structure across tasks. For complex tasks, we use small initialization or weight decay to enhance the regularization effect, which enhances the latent space alignment effect and slows down the generalization decay. Finally, we extend our investigation to large-scale models, observing that they still achieve two-hop reasoning through the latent memory, which provides crucial inspiration for enhancing their implicit reasoning abilities. 

**Abstract (ZH)**: 尽管取得了显著进展，大型语言模型在组合推理任务上常常表现不佳，这一现象被“两跳推理的诅咒”所体现。本文提出了Identity Bridge，这是一种简单而强大的机制，通过监督模型完成零跳身份任务来解决组合性差距。实验结果表明，这一添加使模型能够成功执行超出分布的两跳推理任务，而它们原本完全无法完成这一任务。为了解释这一现象，我们利用简化的Emb-MLP模型进行了理论分析，证明身份监督重塑了模型的潜在几何结构。我们展示了这种对齐是由优化中的隐式核范数正则化诱导的，它倾向于低秩解并在任务之间共享结构。对于复杂任务，我们通过小初始化或权重衰减来增强正则化效果，从而增强潜在空间对齐效果并减缓泛化衰退。最后，我们将研究扩展到大规模模型，观察到它们仍然通过潜在记忆实现两跳推理，这为增强其隐式推理能力提供了关键灵感。 

---
# Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning in VLMs 

**Title (ZH)**: 你能将它们拼接起来吗？一种人工策展的视觉推理基准测试用于探查多模态视觉语言模型 

**Authors**: Mohamad Ballout, Okajevo Wilfred, Seyedalireza Yaghoubi, Nohayr Muhammad Abdelmoneim, Julius Mayer, Elia Bruni  

**Link**: [PDF](https://arxiv.org/pdf/2509.24640)  

**Abstract**: In this work, we introduce SPLICE, a human-curated benchmark derived from the COIN instructional video dataset, designed to probe event-based reasoning across multiple dimensions: temporal, causal, spatial, contextual, and general knowledge. SPLICE includes 3,381 human-filtered videos spanning 12 categories and 180 sub-categories, such as sports, engineering, and housework. These videos are segmented into a total of 11,423 event clips. We evaluate both human participants and state-of-the-art vision-language models (VLMs) on the task of rearranging these clips into coherent event sequences to assess visual reasoning capabilities. Results reveal a significant gap: VLMs struggle to match human performance. While human-annotated textual descriptions improve model accuracy, they do not affect human performance, suggesting that models rely more on language priors than on visual understanding. Even with annotations, VLMs fall short of human-level reasoning, underscoring persistent challenges in visual reasoning. A deeper analysis across sub-categories shows that VLMs perform relatively better on videos where temporal and causal reasoning are dominant, compared to those where contextual and spatial reasoning are dominant. They also perform better on everyday tasks than on specialized ones. 

**Abstract (ZH)**: 本研究引入了SPLICE，一个由人类整理的基准数据集，源自COIN指令视频数据集，旨在从时间、因果关系、空间、上下文及通用知识多个维度探讨事件推理。SPLICE包括3,381个人筛选过的视频，涵盖12个类别和180个子类别，如体育、工程和家务。这些视频被分割成共计11,423个事件片段。我们评估了人类参与者和最先进的视觉-语言模型（VLMs）重新排列这些片段以形成连贯事件序列的能力，以评估其视觉推理能力。结果显示，视觉模型与人类表现存在显著差距：尽管人类注释的文本描述可以提高模型的准确性，但不足以影响人类的表现，表明模型更多依赖于语言先验而非视觉理解。即使有注释，视觉模型仍无法达到人类级别的推理水平，突显了视觉推理领域的持续挑战。通过对子类别的深入分析发现，在以时间和因果关系推理为主导的视频上，视觉模型表现优于以上下文和空间推理为主导的视频；在日常任务上表现优于专门任务。 

---
# Algorithms and data structures for automatic precision estimation of neural networks 

**Title (ZH)**: 神经网络自动精度估计的算法与数据结构 

**Authors**: Igor V. Netay  

**Link**: [PDF](https://arxiv.org/pdf/2509.24607)  

**Abstract**: We describe algorithms and data structures to extend a neural network library with automatic precision estimation for floating point computations. We also discuss conditions to make estimations exact and preserve high computation performance of neural networks training and inference. Numerical experiments show the consequences of significant precision loss for particular values such as inference, gradients and deviations from mathematically predicted behavior.
It turns out that almost any neural network accumulates computational inaccuracies. As a result, its behavior does not coincide with predicted by the mathematical model of neural network. This shows that tracking of computational inaccuracies is important for reliability of inference, training and interpretability of results. 

**Abstract (ZH)**: 我们描述了算法和数据结构，以扩展神经网络库，并实现浮点计算的自动精度估计。我们还讨论了使估计精确并保持神经网络训练和推理高性能的条件。数值实验表明，对于特定值如推理、梯度和数学预测行为偏差，精度损失会对结果产生显著影响。事实上，几乎任何神经网络都会累积计算不准确，导致其行为与神经网络数学模型的预测不符，这表明跟踪计算不准确对于推理、训练可靠性和结果可解释性的重要性。 

---
# PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control 

**Title (ZH)**: PoseDiff: 一个统一的扩散模型，连接机器人姿态估计与视频到动作控制 

**Authors**: Haozhuo Zhang, Michele Caprio, Jing Shao, Qiang Zhang, Jian Tang, Shanghang Zhang, Wei Pan  

**Link**: [PDF](https://arxiv.org/pdf/2509.24591)  

**Abstract**: We present PoseDiff, a conditional diffusion model that unifies robot state estimation and control within a single framework. At its core, PoseDiff maps raw visual observations into structured robot states-such as 3D keypoints or joint angles-from a single RGB image, eliminating the need for multi-stage pipelines or auxiliary modalities. Building upon this foundation, PoseDiff extends naturally to video-to-action inverse dynamics: by conditioning on sparse video keyframes generated by world models, it produces smooth and continuous long-horizon action sequences through an overlap-averaging strategy. This unified design enables scalable and efficient integration of perception and control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy and real-time performance for pose estimation. On Libero-Object manipulation tasks, it substantially improves success rates over existing inverse dynamics modules, even under strict offline settings. Together, these results show that PoseDiff provides a scalable, accurate, and efficient bridge between perception, planning, and control in embodied AI. The video visualization results can be found on the project page: this https URL. 

**Abstract (ZH)**: PoseDiff：统一机器人状态估计与控制的条件扩散模型 

---
# Bandits roaming Hilbert space 

**Title (ZH)**: 游走于希尔伯特空间的Bandits 

**Authors**: Josep Lumbreras  

**Link**: [PDF](https://arxiv.org/pdf/2509.24569)  

**Abstract**: This thesis studies the exploration and exploitation trade-off in online learning of properties of quantum states using multi-armed bandits. Given streaming access to an unknown quantum state, in each round we select an observable from a set of actions to maximize its expectation value. Using past information, we refine actions to minimize regret; the cumulative gap between current reward and the maximum possible. We derive information-theoretic lower bounds and optimal strategies with matching upper bounds, showing regret typically scales as the square root of rounds. As an application, we reframe quantum state tomography to both learn the state efficiently and minimize measurement disturbance. For pure states and continuous actions, we achieve polylogarithmic regret using a sample-optimal algorithm based on a weighted online least squares estimator. The algorithm relies on the optimistic principle and controls the eigenvalues of the design matrix. We also apply our framework to quantum recommender systems and thermodynamic work extraction from unknown states. In this last setting, our results demonstrate an exponential advantage in work dissipation over tomography-based protocols. 

**Abstract (ZH)**: 本论文研究了在多臂 bandit 框架下学习量子状态性质时在线学习中的探索与利用权衡问题。通过逐轮访问未知的量子态，我们从一系列可观测量中选择一个以最大化其期望值。利用过往信息，我们不断细化行动以最小化遗憾；遗憾即当前奖励与最大可能奖励之间的累积差距。我们推导了信息论下的下界，并给出了匹配的上界最优策略，表明遗憾通常随轮次平方根增长。作为一种应用，我们将量子态 tomography 问题重新定框，以高效学习量子态并最小化测量扰动。对于纯态和连续行动，我们利用基于加权在线最小二乘估计器的样本最优算法实现了多项式对数遗憾。该算法依靠乐观原则，并控制设计矩阵的特征值。此外，我们将该框架应用于量子推荐系统，并从未知态中提取热力学工作。在后者场景中，我们的结果展示了与基于 tomography 的协议相比，在工作耗散方面具有指数级优势。 

---
# AdaThink-Med: Medical Adaptive Thinking with Uncertainty-Guided Length Calibration 

**Title (ZH)**: AdaThink-Med：基于不确定性导向长度校准的医疗自适应思考 

**Authors**: Shaohao Rui, Kaitao Chen, Weijie Ma, Xiaosong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24560)  

**Abstract**: Recent advances in inference time scaling with extended long chain-of thought have significantly improved the reasoning capabilities of both general and medical large language models (LLMs). However, these models tend to engage in lengthy reasoning processes regardless of the difficulty of the input question, leading to increased inference costs in real-world applications. Therefore, enabling adaptive thinking where models think less for simpler questions and think more for complex ones is critical for the effective use of medical LLMs in practice. Despite its importance, there is a lack of end-to-end approaches designed to enhance the adaptive thinking capabilities of medical LLMs while providing a comprehensive examination of the trade-off between performance and computational cost. To bridge this gap, we propose AdaThink-Med, the first end-to-end framework designed to enhance adaptive thinking ability in medical reasoning models with uncertainty-guided length calibration. AdaThink-Med first generates multiple candidate outputs for each question, evaluates the correctness and uncertainty of each candidate, and then estimates problem difficulty via an uncertainty-guided length calibration module. For outputs with low difficulty and correct answers, the framework penalizes longer reasoning paths; whereas for those with high difficulty and incorrect answers, it encourages extending the chain of thought to explore alternative solutions. On six public medical QA benchmarks, AdaThink-Med achieves up to 6.4x length reduction on average while retaining performance with only minimal degradation. Intriguingly, we observe that AdaThink-Med spontaneously develops two distinct reasoning modes, which we characterize as "non-thinking" and "thinking", demonstrating the model's ability to suppress redundant reasoning processes dynamically. 

**Abstract (ZH)**: Recent advances in inference time scaling with extended long chain-of-thought have significantly improved the reasoning capabilities of both general and medical large language models (LLMs) 

---
# Deep Reinforcement Learning in Action: Real-Time Control of Vortex-Induced Vibrations 

**Title (ZH)**: 深度强化学习在行动：涡激振动的实时控制 

**Authors**: Hussam Sababha, Bernat Font, Mohammed Daqaq  

**Link**: [PDF](https://arxiv.org/pdf/2509.24556)  

**Abstract**: This study showcases an experimental deployment of deep reinforcement learning (DRL) for active flow control (AFC) of vortex-induced vibrations (VIV) in a circular cylinder at a high Reynolds number (Re = 3000) using rotary actuation. Departing from prior work that relied on low-Reynolds-number numerical simulations, this research demonstrates real-time control in a challenging experimental setting, successfully addressing practical constraints such as actuator delay. When the learning algorithm is provided with state feedback alone (displacement and velocity of the oscillating cylinder), the DRL agent learns a low-frequency rotary control strategy that achieves up to 80% vibration suppression which leverages the traditional lock-on phenomenon. While this level of suppression is significant, it remains below the performance achieved using high-frequency rotary actuation. The reduction in performance is attributed to actuation delays and can be mitigated by augmenting the learning algorithm with past control actions. This enables the agent to learn a high-frequency rotary control strategy that effectively modifies vortex shedding and achieves over 95% vibration attenuation. These results demonstrate the adaptability of DRL for AFC in real-world experiments and its ability to overcome instrumental limitations such as actuation lag. 

**Abstract (ZH)**: 利用深度强化学习在高雷诺数圆柱涡诱发振动主动流动控制中的实验部署 

---
# Short window attention enables long-term memorization 

**Title (ZH)**: 短窗注意力实现长期记忆 

**Authors**: Loïc Cabannes, Maximilian Beck, Gergely Szilvasy, Matthijs Douze, Maria Lomeli, Jade Copet, Pierre-Emmanuel Mazaré, Gabriel Synnaeve, Hervé Jégou  

**Link**: [PDF](https://arxiv.org/pdf/2509.24552)  

**Abstract**: Recent works show that hybrid architectures combining sliding window softmax attention layers with linear recurrent neural network (RNN) layers outperform both of these architectures taken separately. However, the impact of the window length and the interplay between softmax attention and linear RNN layers remain under-studied. In this work, we introduce SWAX, a hybrid architecture consisting of sliding-window attention and xLSTM linear RNN layers.
A counter-intuitive finding with SWAX is that larger sliding windows do not improve the long-context performance. In fact, short window attention encourages the model to better train the long-term memory of the xLSTM, by relying less on the softmax attention mechanism for long context-retrieval.
The issue with small sliding windows is that they are detrimental for short-context tasks, which could be solved with information from moderately larger sliding windows otherwise. Therefore, we train SWAX by stochastically changing the sliding window size, forcing the model to leverage both a longer context window and the xLSTM memory. SWAX trained with stochastic window sizes significantly outperforms regular window attention both on short and long-context problems. 

**Abstract (ZH)**: 最近的研究表明，结合滑动窗口softmax注意力层和线性递归神经网络（RNN）层的混合架构优于单独使用这两种架构。然而，滑动窗口长度的影响以及softmax注意力与线性RNN层之间的互动尚未得到充分研究。在本文中，我们引入了SWAX，这是一种由滑动窗口注意力和xLSTM线性RNN层组成的混合架构。

SWAX的一个出乎意料的发现是，较大的滑动窗口并不提高长上下文性能。实际上，较短的滑动窗口会促使模型更有效地训练xLSTM的长期记忆，因为它较少依赖于softmax注意力机制来进行长上下文检索。

较小滑动窗口的问题在于它们对短上下文任务有害，这可以通过较大但适度的滑动窗口信息来解决。因此，我们通过随机改变滑动窗口大小来训练SWAX，迫使模型利用更长的上下文窗口和xLSTM的记忆。随机滑动窗口大小训练的SWAX在短上下文和长上下文问题上都显著优于固定窗口注意力。 

---
# CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D 

**Title (ZH)**: CORE-3D：基于三维空间的语境感知开放词汇检索 

**Authors**: Mohamad Amin Mirzaei, Pantea Amoie, Ali Ekhterachian, Matin Mirzababaei  

**Link**: [PDF](https://arxiv.org/pdf/2509.24528)  

**Abstract**: 3D scene understanding is fundamental for embodied AI and robotics, supporting reliable perception for interaction and navigation. Recent approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning embedding vectors to 2D class-agnostic masks generated via vision-language models (VLMs) and projecting these into 3D. However, these methods often produce fragmented masks and inaccurate semantic assignments due to the direct use of raw masks, limiting their effectiveness in complex environments. To address this, we leverage SemanticSAM with progressive granularity refinement to generate more accurate and numerous object-level masks, mitigating the over-segmentation commonly observed in mask generation models such as vanilla SAM, and improving downstream 3D semantic segmentation. To further enhance semantic context, we employ a context-aware CLIP encoding strategy that integrates multiple contextual views of each mask using empirically determined weighting, providing much richer visual context. We evaluate our approach on multiple 3D scene understanding tasks, including 3D semantic segmentation and object retrieval from language queries, across several benchmark datasets. Experimental results demonstrate significant improvements over existing methods, highlighting the effectiveness of our approach. 

**Abstract (ZH)**: 三维场景理解对于嵌入式AI和机器人技术是基础性的，支持可靠的感知以实现交互和导航。最近的方法通过将嵌入向量分配给由视觉语言模型（VLMs）生成的2D类无感知掩码，并将这些掩码投影到3D空间中，实现了零样本、开放式词汇的三维语义建模。然而，这些方法往往由于直接使用原始掩码而产生碎片化的掩码和不准确的语义分配，限制了其在复杂环境中的效果。为了解决这个问题，我们利用具有渐进粒度细化的SemanticSAM生成更准确且数量更多的对象级掩码，减少生成掩码模型（如vanilla SAM）中常见的过度分割现象，并提高下游的三维语义分割效果。为进一步增强语义上下文，我们采用了一种基于上下文的CLIP编码策略，利用经验确定的权重整合每个掩码的多种上下文视图，提供了更加丰富的视觉上下文。我们在多个三维场景理解任务上评估了我们的方法，包括三维语义分割和基于语言查询的对象检索，并在几个基准数据集上展示了实验结果，证明了我们方法的优越性。 

---
# CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models 

**Title (ZH)**: CMT：中间训练以高效学习一致性和流平均模型 

**Authors**: Zheyuan Hu, Chieh-Hsin Lai, Yuki Mitsufuji, Stefano Ermon  

**Link**: [PDF](https://arxiv.org/pdf/2509.24526)  

**Abstract**: Flow map models such as Consistency Models (CM) and Mean Flow (MF) enable few-step generation by learning the long jump of the ODE solution of diffusion models, yet training remains unstable, sensitive to hyperparameters, and costly. Initializing from a pre-trained diffusion model helps, but still requires converting infinitesimal steps into a long-jump map, leaving instability unresolved. We introduce mid-training, the first concept and practical method that inserts a lightweight intermediate stage between the (diffusion) pre-training and the final flow map training (i.e., post-training) for vision generation. Concretely, Consistency Mid-Training (CMT) is a compact and principled stage that trains a model to map points along a solver trajectory from a pre-trained model, starting from a prior sample, directly to the solver-generated clean sample. It yields a trajectory-consistent and stable initialization. This initializer outperforms random and diffusion-based baselines and enables fast, robust convergence without heuristics. Initializing post-training with CMT weights further simplifies flow map learning. Empirically, CMT achieves state of the art two step FIDs: 1.97 on CIFAR-10, 1.32 on ImageNet 64x64, and 1.84 on ImageNet 512x512, while using up to 98% less training data and GPU time, compared to CMs. On ImageNet 256x256, CMT reaches 1-step FID 3.34 while cutting total training time by about 50% compared to MF from scratch (FID 3.43). This establishes CMT as a principled, efficient, and general framework for training flow map models. 

**Abstract (ZH)**: Mid-Training for Stable and Efficient Flow Map Learning 

---
# PhysiAgent: An Embodied Agent Framework in Physical World 

**Title (ZH)**: PhysiAgent：物理世界中的体态代理框架 

**Authors**: Zhihao Wang, Jianxiong Li, Jinliang Zheng, Wencong Zhang, Dongxiu Liu, Yinan Zheng, Haoyi Niu, Junzhi Yu, Xianyuan Zhan  

**Link**: [PDF](https://arxiv.org/pdf/2509.24524)  

**Abstract**: Vision-Language-Action (VLA) models have achieved notable success but often struggle with limited generalizations. To address this, integrating generalized Vision-Language Models (VLMs) as assistants to VLAs has emerged as a popular solution. However, current approaches often combine these models in rigid, sequential structures: using VLMs primarily for high-level scene understanding and task planning, and VLAs merely as executors of lower-level actions, leading to ineffective collaboration and poor grounding challenges. In this paper, we propose an embodied agent framework, PhysiAgent, tailored to operate effectively in physical environments. By incorporating monitor, memory, self-reflection mechanisms, and lightweight off-the-shelf toolboxes, PhysiAgent offers an autonomous scaffolding framework to prompt VLMs to organize different components based on real-time proficiency feedback from VLAs to maximally exploit VLAs' capabilities. Experimental results demonstrate significant improvements in task-solving performance on complex real-world robotic tasks, showcasing effective self-regulation of VLMs, coherent tool collaboration, and adaptive evolution of the framework during execution. PhysiAgent makes practical and pioneering efforts to integrate VLMs and VLAs, effectively grounding embodied agent frameworks in real-world settings. 

**Abstract (ZH)**: 基于物理环境的Vision-Language-Action（VLA） embodiment代理框架：VLMs和VLAs的有效集成与自适应进化 

---
# Agentic Specification Generator for Move Programs 

**Title (ZH)**: 执行程序的代理规范生成器 

**Authors**: Yu-Fu Fu, Meng Xu, Taesoo Kim  

**Link**: [PDF](https://arxiv.org/pdf/2509.24515)  

**Abstract**: While LLM-based specification generation is gaining traction, existing tools primarily focus on mainstream programming languages like C, Java, and even Solidity, leaving emerging and yet verification-oriented languages like Move underexplored. In this paper, we introduce MSG, an automated specification generation tool designed for Move smart contracts. MSG aims to highlight key insights that uniquely present when applying LLM-based specification generation to a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust code comprehension and generation capabilities even for non-mainstream languages. MSG successfully generates verifiable specifications for 84% of tested Move functions and even identifies clauses previously overlooked by experts. Additionally, MSG shows that explicitly leveraging specification language features through an agentic, modular design improves specification quality substantially (generating 57% more verifiable clauses than conventional designs). Incorporating feedback from the verification toolchain further enhances the effectiveness of MSG, leading to a 30% increase in generated verifiable specifications. 

**Abstract (ZH)**: 基于LLM的规格生成工具MSG：面向Move智能合约的独特洞察与改进 

---
# Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models 

**Title (ZH)**: 泛化后的专业化：理解基础模型测试时训练 

**Authors**: Jonas Hübotter, Patrik Wolf, Alexander Shevchenko, Dennis Jüni, Andreas Krause, Gil Kur  

**Link**: [PDF](https://arxiv.org/pdf/2509.24510)  

**Abstract**: Recent empirical studies have explored the idea of continuing to train a model at test-time for a given task, known as test-time training (TTT), and have found it to yield significant performance improvements. However, there is limited understanding of why and when TTT is effective. Earlier explanations mostly focused on the observation that TTT may help when applied to out-of-distribution adaptation or used with privileged data. However, the growing scale of foundation models with most test data being in-distribution questions these explanations. We instead posit that foundation models remain globally underparameterized, with TTT providing a mechanism for specialization after generalization, focusing capacity on concepts relevant to the test task. Specifically, under the linear representation hypothesis, we propose a model in which TTT achieves a substantially smaller in-distribution test error than global training. We empirically validate our model's key assumptions by training a sparse autoencoder on ImageNet, showing that semantically related data points are explained by only a few shared concepts. Finally, we perform scaling studies across image and language tasks that confirm the practical implications of our model, identifying the regimes where specialization is most effective. 

**Abstract (ZH)**: 最近的经验研究表明，在测试时继续训练模型（测试时训练，TTT）对于给定任务可以显著提高性能，但关于为什么和在何时TTT有效仍缺乏充分理解。早期的解释主要集中在TTT可能在处理分布外适应或使用特权数据时有所帮助这一观察上。然而，随着基础模型规模的增长和大部分测试数据在分布内这一事实，这些解释受到了质疑。相反，我们认为基础模型保持全球性欠参数化，在泛化之后，TTT提供了一种专业化机制，使模型容量集中在与测试任务相关的概念上。具体而言，在线性表示假设下，我们提出一个模型，在这种模型中，TTT比全局训练可以获得显著更低的分布内测试错误率。我们通过在ImageNet上训练稀疏自编码器来经验验证我们模型的关键假设，结果表明，语义相关的数据点只由少数共享概念来解释。最后，我们在图像和语言任务上进行的缩放研究证实了我们模型的实践意义，确定了专业化最为有效的条件。 

---
# LLM DNA: Tracing Model Evolution via Functional Representations 

**Title (ZH)**: LLM DNA：通过功能表示追踪模型演变 

**Authors**: Zhaomin Wu, Haodong Zhao, Ziyang Wang, Jizhou Guo, Qian Wang, Bingsheng He  

**Link**: [PDF](https://arxiv.org/pdf/2509.24496)  

**Abstract**: The explosive growth of large language models (LLMs) has created a vast but opaque landscape: millions of models exist, yet their evolutionary relationships through fine-tuning, distillation, or adaptation are often undocumented or unclear, complicating LLM management. Existing methods are limited by task specificity, fixed model sets, or strict assumptions about tokenizers or architectures. Inspired by biological DNA, we address these limitations by mathematically defining LLM DNA as a low-dimensional, bi-Lipschitz representation of functional behavior. We prove that LLM DNA satisfies inheritance and genetic determinism properties and establish the existence of DNA. Building on this theory, we derive a general, scalable, training-free pipeline for DNA extraction. In experiments across 305 LLMs, DNA aligns with prior studies on limited subsets and achieves superior or competitive performance on specific tasks. Beyond these tasks, DNA comparisons uncover previously undocumented relationships among LLMs. We further construct the evolutionary tree of LLMs using phylogenetic algorithms, which align with shifts from encoder-decoder to decoder-only architectures, reflect temporal progression, and reveal distinct evolutionary speeds across LLM families. 

**Abstract (ZH)**: 大规模语言模型（LLMs）的爆炸性增长创造了一个庞大而透明度较低的景观：成千上万的模型存在，但它们通过微调、蒸馏或适应演化关系通常是未记录或不清楚的，使LLM管理变得复杂。现有方法受限于任务特定性、固定的模型集或对分词器或架构的严格假设。借鉴生物DNA的理念，我们通过数学定义LLM DNA作为功能性行为的一种低维度、双唇同步的表示来解决这些限制。我们证明了LLM DNA满足继承性和遗传决定性属性，并确立了DNA的存在性。在此理论基础上，我们推导出一个通用、可扩展、无需训练的DNA提取管道。在对305个LLM的实验中，DNA与先前对有限子集的研究一致，并在特定任务上实现了优或竞争力的表现。超出这些任务，DNA比较揭示了LLM之间未记录的关系。我们进一步利用系统发生学算法构建了LLM的演化树，该树与从编码器-解码器到仅解码器架构的转变一致，反映了时间进程，并揭示了不同LLM家族的进化速度差异。 

---
# Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs 

**Title (ZH)**: 通过语义课程偏好优化减轻MLLMs的视觉幻觉 

**Authors**: Yuanshuai Li, Yuping Yan, Junfeng Tang, Yunxuan Li, Zeqi Zheng, Yaochu Jin  

**Link**: [PDF](https://arxiv.org/pdf/2509.24491)  

**Abstract**: Multimodal Large Language Models (MLLMs) have significantly improved the performance of various tasks, but continue to suffer from visual hallucinations, a critical issue where generated responses contradict visual evidence. While Direct Preference Optimization(DPO) is widely used for alignment, its application to MLLMs often fails to capture fine-grained semantic differences and encourages shortcut learning. To address these challenges, we propose Semantic Curriculum Preference Optimization (SCPO), a novel framework for MLLM alignment. SCPO employs a progressive, easy-to-hard curriculum built upon our Semantic Curriculum Preference Pairs dataset, which provides fine-grained semantic contrasts sorted by difficulty. This curriculum is trained with a dynamic reference model and a novel symmetric, bidirectional objective to facilitate simultaneous learning from both textual and visual preferences. To our knowledge, SCPO is the first framework to unify semantics, symmetry, and curriculum for MLLMs alignment, effectively mitigating visual hallucinations. Extensive experiments on LLaVA models across various scales and versions validate that SCPO demonstrates superior performance compared to baseline models on multiple hallucination benchmarks, reducing the hallucination rate by up to 62.9%. Moreover, evaluations on generalized benchmarks show that SCPO improves factuality while preserving general capabilities, with its performance remaining stable across general vision-language benchmarks. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）在各种任务中取得了显著的进步，但仍面临着视觉幻觉这一关键问题，即生成的响应与视觉证据相矛盾。尽管直接偏好优化（DPO）广泛应用于对齐，但其应用于MLLMs时往往无法捕捉细粒度的语义差异并倾向于学习捷径。为解决这些挑战，我们提出了一种新的MLLM对齐框架——语义课程偏好优化（SCPO）。SCPO利用了一个逐步提高、从易到难的课程，该课程基于我们构建的语义课程偏好对数据集，提供了按照难度排序的细粒度语义对比。该课程采用动态参考模型和一种新颖的双向对称目标进行训练，以同时从文本和视觉偏好中学习。据我们所知，SCPO是首个统一语义、对称性和课程的MLLM对齐框架，有效地缓解了视觉幻觉问题。在不同规模和版本的LLaVA模型上进行的广泛实验表明，SCPO在多个幻觉基准上表现出色，与基线模型相比，幻觉率最多降低了62.9%。此外，通用基准测试显示，SCPO在提高事实性的同时保持了一般能力，其性能在通用视觉-语言基准上保持稳定。 

---
# Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks 

**Title (ZH)**: 欧几里得的馈赠：通过几何代行任务提升视觉语言模型的空间知觉与推理能力 

**Authors**: Shijie Lian, Changti Wu, Laurence Tianruo Yang, Hang Yuan, Bin Yu, Lei Zhang, Kai Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24473)  

**Abstract**: Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. To enable the model to acquire and apply Euclidean principles from these geometry problems, we employed Group Relative Policy Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family, inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them, RoboBrain2.0-Euclid-7B achieves 49.6\% accuracy, surpassing the previous state-of-the-art model, this http URL our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in this https URL. 

**Abstract (ZH)**: 空间智能涵盖了丰富的能力，包括可视化和变换形状、心理旋转物体、判断相对位置和包含关系，以及估算数量。然而，这仍然是多模态大型语言模型（MLLMs）的一个关键未解挑战。为了填补这一空白，我们提出将欧几里得几何问题解决视为一个替代任务。具体地，我们精心构建了一个多模态数据集，名为Euclid30K，包含约30,000个平面和立体几何问题。为了使模型能够从这些几何问题中学习和应用欧几里得原理，我们使用群相对策略优化（GRPO）微调了Qwen2.5VL家族和RoboBrain2.0家族，激励这些模型识别形状、计数和关联实体，并利用欧几里得原理进行多步演绎推理。我们的实验表明，这些模型在四个空间推理基准测试（Super-CLEVR、Omni3DBench、VSI-Bench和MindCube）上实现了显著的零样本提升，无需任何特定任务的调整。值得注意的是，经过Euclid30K训练后，所有评估模型的VSI-Bench平均准确率从34.5%提升至40.5%，提高了5.5个百分点。其中，RoboBrain2.0-Euclid-7B达到了49.6%的准确率，超过了当时的最先进的模型。据我们所知，这是首次系统性研究证明几何中心化微调可以赋予视觉语言模型广泛的空间技能。代码和Euclid30K数据集可在以下网址获取。 

---
# LaMoGen: Laban Movement-Guided Diffusion for Text-to-Motion Generation 

**Title (ZH)**: Laban 动作引导的文本到动作生成扩散模型 

**Authors**: Heechang Kim, Gwanghyun Kim, Se Young Chun  

**Link**: [PDF](https://arxiv.org/pdf/2509.24469)  

**Abstract**: Diverse human motion generation is an increasingly important task, having various applications in computer vision, human-computer interaction and animation. While text-to-motion synthesis using diffusion models has shown success in generating high-quality motions, achieving fine-grained expressive motion control remains a significant challenge. This is due to the lack of motion style diversity in datasets and the difficulty of expressing quantitative characteristics in natural language. Laban movement analysis has been widely used by dance experts to express the details of motion including motion quality as consistent as possible. Inspired by that, this work aims for interpretable and expressive control of human motion generation by seamlessly integrating the quantification methods of Laban Effort and Shape components into the text-guided motion generation models. Our proposed zero-shot, inference-time optimization method guides the motion generation model to have desired Laban Effort and Shape components without any additional motion data by updating the text embedding of pretrained diffusion models during the sampling step. We demonstrate that our approach yields diverse expressive motion qualities while preserving motion identity by successfully manipulating motion attributes according to target Laban tags. 

**Abstract (ZH)**: 多样化的探究性人类运动生成是一项日益重要的任务，广泛应用于计算机视觉、人机交互和动画领域。虽然利用扩散模型进行文本到运动的合成已成功生成高质量的运动，但实现精细的表达性运动控制仍然是一个重大挑战。这源于数据集中运动风格多样性不足以及通过自然语言表达定量特征的困难。朗班运动分析已被舞蹈专家广泛用于表达运动细节，包括尽量一致的运动质量。受此启发，本工作旨在通过无缝整合朗班力量和形态成分的量化方法，实现由文本指导的人类运动生成的可解释和表达性控制。我们提出了一种零-shot，在推断时优化的方法，通过在采样步骤中更新预训练扩散模型的文本嵌入，指导运动生成模型生成所需的目标朗班力量和形态成分，而无需额外的运动数据。我们证明，通过根据目标朗班标签操控运动属性，我们的方法能够在保持运动身份的同时产生多样化的表达性运动质量。 

---
# Moravec's Paradox and Restrepo's Model: Limits of AGI Automation in Growth 

**Title (ZH)**: 莫拉维克悖论与雷斯特雷波模型：AGI自动化在增长领域的局限性 

**Authors**: Marc Bara  

**Link**: [PDF](https://arxiv.org/pdf/2509.24466)  

**Abstract**: This note extends Restrepo (2025)'s model of economic growth under AGI by incorporating Moravec's Paradox -the observation that tasks requiring sensorimotor skills remain computationally expensive relative to cognitive tasks. We partition the task space into cognitive and physical components with differential automation costs, allowing infinite costs for some physical bottlenecks. Our key result shows that when physical tasks constitute economic bottlenecks with sufficiently high (or infinite) computational requirements, the labor share of income converges to a positive constant in the finite-compute regime (rather than zero). This fundamentally alters the distributional implications of AGI while preserving the growth dynamics for cognitive-intensive economies. 

**Abstract (ZH)**: 这首笔记将Restrepo (2025)关于AGI的经济增长模型扩展至纳入了莫拉克悖论，即感觉运动技能需求的任务相比于认知任务仍具有相对较高的计算成本。我们将任务空间划分为认知和物理组成部分，并允许物理瓶颈具有无限的自动化成本。我们的主要结果表明，在物理任务构成具有足够高（或无限）计算要求的经济瓶颈时，在有限计算能力范围内，收入中的劳动份额将趋于一个正的常数（而不是零）。这一发现从根本上改变了AGI的分配影响，同时保留了对认知密集型经济的经济增长动态。 

---
# An Agent-Based Framework for Automated Higher-Voice Harmony Generation 

**Title (ZH)**: 基于代理的自动化高音和谐生成框架 

**Authors**: Nia D'Souza Ganapathy, Arul Selvamani Shaja  

**Link**: [PDF](https://arxiv.org/pdf/2509.24463)  

**Abstract**: The generation of musically coherent and aesthetically pleasing harmony remains a significant challenge in the field of algorithmic composition. This paper introduces an innovative Agentic AI-enabled Higher Harmony Music Generator, a multi-agent system designed to create harmony in a collaborative and modular fashion. Our framework comprises four specialized agents: a Music-Ingestion Agent for parsing and standardizing input musical scores; a Chord-Knowledge Agent, powered by a Chord-Former (Transformer model), to interpret and provide the constituent notes of complex chord symbols; a Harmony-Generation Agent, which utilizes a Harmony-GPT and a Rhythm-Net (RNN) to compose a melodically and rhythmically complementary harmony line; and an Audio-Production Agent that employs a GAN-based Symbolic-to-Audio Synthesizer to render the final symbolic output into high-fidelity audio. By delegating specific tasks to specialized agents, our system effectively mimics the collaborative process of human musicians. This modular, agent-based approach allows for robust data processing, deep theoretical understanding, creative composition, and realistic audio synthesis, culminating in a system capable of generating sophisticated and contextually appropriate higher-voice harmonies for given melodies. 

**Abstract (ZH)**: 算法作曲中具有音乐连贯性和审美吸引力和声生成依然是一项重要挑战。本文介绍了一种创新的Agentic AI驱动的高声部和声生成器，这是一种多代理系统，旨在以协作和模块化的方式生成和声。该框架包含四个专门的代理：一个音乐摄入代理，用于解析和标准化输入的音乐曲谱；一个由Chord-Former（变换器模型）驱动的和弦知识代理，以解释和提供复杂的和弦符号的构成音；一个和声生成代理，利用Harmony-GPT和Rhythm-Net（RNN）来创作旋律和节奏相补的和声线；以及一个采用基于GAN的符号到音频合成器进行音频渲染的音频生产代理。通过将具体任务委派给专门的代理，我们的系统有效地模拟了人类音乐家的协作过程。这种模块化的代理方法允许稳健的数据处理、深入的理论理解、创意思维的组成以及现实主义的音频合成，最终生成给定旋律的复杂且上下文相关的高声部和声。 

---
# EOE: Evolutionary Optimization of Experts for Training Language Models 

**Title (ZH)**: 专家进化优化训练语言模型 

**Authors**: Yingshi Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24436)  

**Abstract**: This paper presents an evolutionary framework for the training of large language models(LLM). The models are divided into several experts(sub-networks), which have the same structure but different parameter values. Only one expert is trained at each step. After the classical AdamW optimization, some evolutionary operators(crossover, PSO, and mutation) act on the tensor weights between the current expert and the best expert. So current expert would learn the experience of best expert. The direction of best expert would help current expert's loss decrease faster. Finally, only save the weight of the best expert. Experiments show that best expert would achieve nearly the same accuracy as the full model. This would greatly reduce the size of the model for inference. Since only one expert is trained at each step, the training needs much less memory and has much higher throughput. Experiments show that the throughput would accelerate more than ten times! Our source code is available. It's a pure c++/cu framework, which is suitable for easy deployment on PCs and edge computing devices. 

**Abstract (ZH)**: 本文提出了一种用于大型语言模型训练的进化框架。模型被划分为多个专家（子网络），这些专家具有相同的结构但参数值不同。每次训练只训练一个专家。在经典AdamW优化之后，会对当前专家和最佳专家之间的张量权值应用一些进化操作（交叉、PSO和变异），从而使当前专家能学习到最佳专家的经验。最佳专家的方向有助于当前专家损失更快地下降。最后，仅保存最佳专家的权重。实验表明，最佳专家能达到与完整模型几乎相同的准确性。这将大大减小推理时模型的规模。由于每次只训练一个专家，训练所需内存更少，并且具有更高的吞吐量。实验表明，吞吐量会加速超过十倍！我们的源代码已开源，这是一个纯C++/cu框架，适用于在个人计算机和边缘计算设备上便捷部署。 

---
# Alternatives To Next Token Prediction In Text Generation - A Survey 

**Title (ZH)**: 文本生成中替代下一个词预测的方法——一个综述 

**Authors**: Charlie Wyatt, Aditya Joshi, Flora Salim  

**Link**: [PDF](https://arxiv.org/pdf/2509.24435)  

**Abstract**: The paradigm of Next Token Prediction (NTP) has driven the unprecedented success of Large Language Models (LLMs), but is also the source of their most persistent weaknesses such as poor long-term planning, error accumulation, and computational inefficiency. Acknowledging the growing interest in exploring alternatives to NTP, the survey describes the emerging ecosystem of alternatives to NTP. We categorise these approaches into five main families: (1) Multi-Token Prediction, which targets a block of future tokens instead of a single one; (2) Plan-then-Generate, where a global, high-level plan is created upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the autoregressive process itself into a continuous latent space; (4) Continuous Generation Approaches, which replace sequential generation with iterative, parallel refinement through diffusion, flow matching, or energy-based methods; and (5) Non-Transformer Architectures, which sidestep NTP through their inherent model structure. By synthesizing insights across these methods, this survey offers a taxonomy to guide research into models that address the known limitations of token-level generation to develop new transformative models for natural language processing. 

**Abstract (ZH)**: Next Token Prediction的范式推动了大规模语言模型的空前成功，但也是其最持久弱点的源泉，如长期规划能力差、错误累积和计算效率低下。鉴于对探索替代Next Token Prediction方法的兴趣不断增加，本文综述了新兴的替代方法生态系统。我们将这些方法归类为五个主要家族：(1) 多个令牌预测，旨在预测一系列未来的令牌而不是单个令牌；(2) 计划先行生成，预先制定全局的高层次计划以指导令牌级解码；(3) 潜在推理，将自回归过程本身转移到一个连续的潜在空间中；(4) 连续生成方法，通过迭代并行 refinement 的方法（如扩散、流匹配或基于能量的方法）替换序列生成；以及(5) 非Transformer架构，通过其内在的模型结构绕过Next Token Prediction。通过对这些方法的综合洞察，本文提供了一种分类法，以指导对解决令牌级生成已知局限性的模型的研究，并开发新变革性的自然语言处理模型。 

---
# Multi-Item-Query Attention for Stable Sequential Recommendation 

**Title (ZH)**: 多项查询注意力机制下的稳定序列推荐 

**Authors**: Mingshi Xu, Haoren Zhu, Wilfred Siu Hung Ng  

**Link**: [PDF](https://arxiv.org/pdf/2509.24424)  

**Abstract**: The inherent instability and noise in user interaction data challenge sequential recommendation systems. Prevailing masked attention models, relying on a single query from the most recent item, are sensitive to this noise, reducing prediction reliability. We propose the Multi-Item-Query attention mechanism (MIQ-Attn) to enhance model stability and accuracy. MIQ-Attn constructs multiple diverse query vectors from user interactions, effectively mitigating noise and improving consistency. It is designed for easy adoption as a drop-in replacement for existing single-query attention. Experiments show MIQ-Attn significantly improves performance on benchmark datasets. 

**Abstract (ZH)**: 用户交互数据中的固有不稳定性和噪声挑战了序列推荐系统。依赖于最近一项的单一查询的盛行掩码注意力模型对此噪声敏感，降低了预测可靠性。我们提出多项查询注意力机制（MIQ-Attn）以增强模型稳定性和准确性。MIQ-Attn从用户交互中构建多个多样化的查询向量，有效减轻噪声并提高一致性。该机制设计为易于替换现有单一查询注意力的即插即用方案。实验结果显示，MIQ-Attn在基准数据集上显著提升了性能。 

---
# A Data-Centric Perspective on the Influence of Image Data Quality in Machine Learning Models 

**Title (ZH)**: 基于数据为中心的观点：图像数据质量对机器学习模型的影响研究 

**Authors**: Pei-Han Chen, Szu-Chi Chung  

**Link**: [PDF](https://arxiv.org/pdf/2509.24420)  

**Abstract**: In machine learning, research has traditionally focused on model development, with relatively less attention paid to training data. As model architectures have matured and marginal gains from further refinements diminish, data quality has emerged as a critical factor. However, systematic studies on evaluating and ensuring dataset quality in the image domain remain limited.
This study investigates methods for systematically assessing image dataset quality and examines how various image quality factors influence model performance. Using the publicly available and relatively clean CIFAKE dataset, we identify common quality issues and quantify their impact on training. Building on these findings, we develop a pipeline that integrates two community-developed tools, CleanVision and Fastdup. We analyze their underlying mechanisms and introduce several enhancements, including automatic threshold selection to detect problematic images without manual tuning.
Experimental results demonstrate that not all quality issues exert the same level of impact. While convolutional neural networks show resilience to certain distortions, they are particularly vulnerable to degradations that obscure critical visual features, such as blurring and severe downscaling. To assess the performance of existing tools and the effectiveness of our proposed enhancements, we formulate the detection of low-quality images as a binary classification task and use the F1 score as the evaluation metric. Our automatic thresholding method improves the F1 score from 0.6794 to 0.9468 under single perturbations and from 0.7447 to 0.8557 under dual perturbations. For near-duplicate detection, our deduplication strategy increases the F1 score from 0.4576 to 0.7928. These results underscore the effectiveness of our workflow and provide a foundation for advancing data quality assessment in image-based machine learning. 

**Abstract (ZH)**: 基于图像的数据集质量系统评估方法及其对模型性能的影响 

---
# CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers 

**Title (ZH)**: CLQ: 不同层引导正交基量化技术用于扩散变换器 

**Authors**: Kai Liu, Shaoqiu Zhang, Linghe Kong, Yulun Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24416)  

**Abstract**: Visual generation quality has been greatly promoted with the rapid advances in diffusion transformers (DiTs), which is attributed to the scaling of model size and complexity. However, these attributions also hinder the practical deployment of DiTs on edge devices, limiting their development and application. Serve as an efficient model compression technique, model post-training quantization (PTQ) can reduce the memory consumption and speed up the inference, with inevitable performance degradation. To alleviate the degradation, we propose CLQ, a cross-layer guided orthogonal-based quantization method for DiTs. To be specific, CLQ consists of three key designs. First, we observe that the calibration data used by most of the PTQ methods can not honestly represent the distribution of the activations. Therefore, we propose cross-block calibration (CBC) to obtain accurate calibration data, with which the quantization can be better guided. Second, we propose orthogonal-based smoothing (OBS), which quantifies the outlier score of each channel and leverages block Hadamard matrix to smooth the outliers with negligible overhead. Third, we propose cross-layer parameter searching (CLPS) to search. We evaluate CLQ with both image generation and video generation models and successfully compress the model into W4A4 with negligible degradation in visual quality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our code is available at \hyperlink{this https URL}{this https URL}. 

**Abstract (ZH)**: 视觉生成质量随着扩散变压器（DiTs）的快速进步得到了极大的提升，这归因于模型规模和复杂性的扩大。然而，这些归因也阻碍了DiTs在边缘设备上的实际部署，限制了其发展和应用。作为一种有效的模型压缩技术，后训练量化（PTQ）可以减少内存消耗并加速推理，但不可避免地会导致性能下降。为了解决这种下降，我们提出了一种跨层引导正交基量化方法CLQ（Cross-layer Guided Orthogonal-based Quantization）用于DiTs。具体而言，CLQ 包含三个关键设计。首先，我们发现大多数PTQ方法使用的校准数据不能真实地代表激活值的分布。因此，我们提出了跨块校准（CBC，Cross-block Calibration）以获得准确的校准数据，从而使量化得到更好的引导。其次，我们提出了基于正交的平滑（OBS，Orthogonal-based Smoothing），量化每个通道的异常值得分，并利用块Hadamard矩阵平滑异常值，而几乎不增加开销。最后，我们提出了跨层参数搜索（CLPS，Cross-layer Parameter Searching）。我们使用图像生成和视频生成模型评估了CLQ，并成功将模型压缩到W4A4，同时视觉质量和指标的下降可以忽略不计。CLQ 实现了3.98倍的内存节省和3.95倍的速度提升。我们的代码可在 \hyperlink{this https URL}{this https URL} 获取。 

---
# ScatterAD: Temporal-Topological Scattering Mechanism for Time Series Anomaly Detection 

**Title (ZH)**: ScatterAD：时间拓扑散射机制在时间序列异常检测中的应用 

**Authors**: Tao Yin, Xiaohong Zhang, Shaochen Fu, Zhibin Zhang, Li Huang, Yiyuan Yang, Kaixiang Yang, Meng Yan  

**Link**: [PDF](https://arxiv.org/pdf/2509.24414)  

**Abstract**: One main challenge in time series anomaly detection for industrial IoT lies in the complex spatio-temporal couplings within multivariate data. However, traditional anomaly detection methods focus on modeling spatial or temporal dependencies independently, resulting in suboptimal representation learning and limited sensitivity to anomalous dispersion in high-dimensional spaces. In this work, we conduct an empirical analysis showing that both normal and anomalous samples tend to scatter in high-dimensional space, especially anomalous samples are markedly more dispersed. We formalize this dispersion phenomenon as scattering, quantified by the mean pairwise distance among sample representations, and leverage it as an inductive signal to enhance spatio-temporal anomaly detection. Technically, we propose ScatterAD to model representation scattering across temporal and topological dimensions. ScatterAD incorporates a topological encoder for capturing graph-structured scattering and a temporal encoder for constraining over-scattering through mean squared error minimization between neighboring time steps. We introduce a contrastive fusion mechanism to ensure the complementarity of the learned temporal and topological representations. Additionally, we theoretically show that maximizing the conditional mutual information between temporal and topological views improves cross-view consistency and enhances more discriminative representations. Extensive experiments on multiple public benchmarks show that ScatterAD achieves state-of-the-art performance on multivariate time series anomaly detection. Code is available at this repository: this https URL. 

**Abstract (ZH)**: 工业物联网中时间序列异常检测的主要挑战在于多变量数据中的复杂空时耦合。传统异常检测方法独立建模空域或时域依赖性，导致不足的表征学习，并在高维空间中对异常分散的敏感性有限。在本文中，我们通过实证分析表明，正常样本和异常样本都倾向于在高维空间中分散，尤其是异常样本的分散程度更为显著。我们将这种分散现象形式化为散布，通过样本表示的平均成对距离来量化，并利用其作为归纳信号以增强空时异常检测。技术上，我们提出了ScatterAD来建模跨时间和拓扑维度的表征散布。ScatterAD结合了拓扑编解码器来捕捉基于图的散布，并通过邻近时间步之间的均方误差最小化来限制过度散布。我们引入了一种对比融合机制以确保学习到的时间和拓扑表征的互补性。此外，我们从理论上证明，最大化时间视图和拓扑视图的条件互信息可以提高跨视图一致性并生成更具判别力的表示。在多个公开基准上的实验显示，ScatterAD在多变量时间序列异常检测中达到了最先进的性能。代码可在以下仓库获取：this https URL。 

---
# Hybrid Layer-Wise ANN-SNN With Surrogate Spike Encoding-Decoding Structure 

**Title (ZH)**: 混合层-wise ANN-SNN配 Browse_surrogate 突变编码-解码结构 

**Authors**: Nhan T. Luu, Duong T. Luu, Pham Ngoc Nam, Truong Cong Thang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24411)  

**Abstract**: Spiking Neural Networks (SNNs) have gained significant traction in both computational neuroscience and artificial intelligence for their potential in energy-efficient computing. In contrast, artificial neural networks (ANNs) excel at gradient-based optimization and high accuracy. This contrast has consequently led to a growing subfield of hybrid ANN-SNN research. However, existing hybrid approaches often rely on either a strict separation between ANN and SNN components or employ SNN-only encoders followed by ANN classifiers due to the constraints of non-differentiability of spike encoding functions, causing prior hybrid architectures to lack deep layer-wise cooperation during backpropagation. To address this gap, we propose a novel hybrid ANN-SNN framework that integrates layer-wise encode-decode SNN blocks within conventional ANN pipelines. Central to our method is the use of surrogate gradients for a bit-plane-based spike encoding function, enabling end-to-end differentiable training across ANN and SNN layers. This design achieves competitive accuracy with state-of-the-art pure ANN and SNN models while retaining the potential efficiency and temporal representation benefits of spiking computation. To the best of our knowledge, this is the first implementation of a surrogate gradient for bit plane coding specifically and spike encoder interface in general to be utilized in the context of hybrid ANN-SNN, successfully leading to a new class of hybrid models that pave new directions for future research. 

**Abstract (ZH)**: 基于突触神经网络的新型混合ANN-SNN框架：基于位平面的替代梯度突触编码 

---
# Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents 

**Title (ZH)**: 多语言文本到SQL转换：协作语言代理评估语言模型的极限 

**Authors**: Khanh Trinh Pham, Thu Huong Nguyen, Jun Jo, Quoc Viet Hung Nguyen, Thanh Tam Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24405)  

**Abstract**: Text-to-SQL enables natural access to databases, yet most benchmarks are English-only, limiting multilingual progress. We introduce MultiSpider 2.0, extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's structural difficulty while adding linguistic and dialectal variability, demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\% execution accuracy when relying on intrinsic reasoning, versus 60\% on MultiSpider 1.0. Therefore, we provide a collaboration-driven language agents baseline that iteratively refines queries, improving accuracy to 15\%. These results reveal a substantial multilingual gap and motivate methods that are robust across languages and ready for real-world enterprise deployment. Our benchmark is available at this https URL. 

**Abstract (ZH)**: Text-to-SQL跨多语言的挑战与进展：从Multilingual Spider 2.0探索自然语言到SQL的转换能力 

---
# The 2025 OpenAI Preparedness Framework does not guarantee any AI risk mitigation practices: a proof-of-concept for affordance analyses of AI safety policies 

**Title (ZH)**: 2025年OpenAI准备框架并不保证任何AI风险缓解实践：AI安全政策能力分析的可行性研究 

**Authors**: Sam Coggins, Alex Saeri, Katherine A. Daniell, Lorenn P. Ruster, Jessie Liu, Jenny L. Davis  

**Link**: [PDF](https://arxiv.org/pdf/2509.24394)  

**Abstract**: Prominent AI companies are producing 'safety frameworks' as a type of voluntary self-governance. These statements purport to establish risk thresholds and safety procedures for the development and deployment of highly capable AI. Understanding which AI risks are covered and what actions are allowed, refused, demanded, encouraged, or discouraged by these statements is vital for assessing how these frameworks actually govern AI development and deployment. We draw on affordance theory to analyse the OpenAI 'Preparedness Framework Version 2' (April 2025) using the Mechanisms & Conditions model of affordances and the MIT AI Risk Repository. We find that this safety policy requests evaluation of a small minority of AI risks, encourages deployment of systems with 'Medium' capabilities for what OpenAI itself defines as 'severe harm' (potential for >1000 deaths or >$100B in damages), and allows OpenAI's CEO to deploy even more dangerous capabilities. These findings suggest that effective mitigation of AI risks requires more robust governance interventions beyond current industry self-regulation. Our affordance analysis provides a replicable method for evaluating what safety frameworks actually permit versus what they claim. 

**Abstract (ZH)**: prominente的人工智能公司正在制定“安全框架”作为一种自愿自我治理方式。这些声明旨在为高度具备能力的人工智能的研发和部署设定风险阈值和安全程序。了解这些声明涵盖哪些人工智能风险以及允许、拒绝、要求、鼓励或反对哪些行动对于评估这些框架实际上如何治理人工智能的研发和部署至关重要。我们借鉴了机会理论，使用机会机制与条件模型和MIT人工智能风险仓库来分析OpenAI“准备框架版本2”（2025年4月）。研究发现，这一安全政策要求对少数几项人工智能风险进行评估，鼓励部署“中等”能力的系统，这些系统被OpenAI自身定义为“严重伤害”（潜在致死人数超过1000人或造成超过1000亿美元的损害），并且允许OpenAI首席执行官部署更为危险的能力。这些发现表明，有效的降低人工智能风险需要超出当前行业自我监管的更稳健的治理干预措施。我们的机会分析提供了一种可复制的方法，用于评估实际安全框架允许的内容与它们所声称的内容之间的差异。 

---
# LLaDA-MoE: A Sparse MoE Diffusion Language Model 

**Title (ZH)**: LLaDA-MoE: 一种稀疏的MoE扩散语言模型 

**Authors**: Fengqi Zhu, Zebin You, Yipeng Xing, Zenan Huang, Lin Liu, Yihong Zhuang, Guoshan Lu, Kangyu Wang, Xudong Wang, Lanning Wei, Hongrui Guo, Jiaqi Hu, Wentao Ye, Tieyuan Chen, Chenchen Li, Chengfu Tang, Haibo Feng, Jun Hu, Jun Zhou, Xiaolu Zhang, Zhenzhong Lan, Junbo Zhao, Da Zheng, Chongxuan Li, Jianguo Li, Ji-Rong Wen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24389)  

**Abstract**: We introduce LLaDA-MoE, a large language diffusion model with the Mixture-of-Experts (MoE) architecture, trained from scratch on approximately 20T tokens. LLaDA-MoE achieves competitive performance with significantly reduced computational overhead by maintaining a 7B-parameter capacity while activating only 1.4B parameters during inference. Our empirical evaluation reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion language models with larger parameters, surpassing previous diffusion language models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation, mathematical reasoning, agent and alignment tasks, despite using fewer active parameters. Our results show that integrating a sparse MoE architecture into the training objective of masked diffusion language models still brings out MoE's strengths under efficient inference with few active parameters, and opens ample room for further exploration of diffusion language models. LLaDA-MoE models are available at Huggingface. 

**Abstract (ZH)**: LLaDA-MoE：一种基于Mixture-of-Experts架构的大规模语言扩散模型及其性能优越性 

---
# Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy 

**Title (ZH)**: Vid-LLM：基于视频的紧凑型3D多模态LLM及其重建-推理协同模型 

**Authors**: Haijier Chen, Bo Xu, Shoujian Zhang, Haoze Liu, Jiaxuan Lin, Jingrong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24385)  

**Abstract**: Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision-Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often depend on 3D data inputs, which limits scalability and generalization. To address this limitation, we propose Vid-LLM, a video-based 3D-MLLM that directly processes video inputs without requiring external 3D data, making it practical for real-world deployment. In our method, the geometric prior are directly used to improve the performance of the sceen perception. To integrate the geometric cues into the MLLM compactly, we design a Cross-Task Adapter (CTA) module to align the 3D geometric priors with the vision-language representations. To ensure geometric consistency and integrity, we introduce a Metric Depth Model that recovers real-scale geometry from the reconstruction outputs. Finally, the model is fine-tuned with a two-stage distillation optimization strategy, realizing fast convergence and stabilizes training. Extensive experiments across diverse benchmarks verified the effectiveness of our method on 3D Question Answering, 3D Dense Captioning and 3D Visual Grounding tasks, demonstrating the superior multi-task capabilities. 

**Abstract (ZH)**: 近年来，多模态大型语言模型（MLLMs）在二维领域中的视觉-语言（VL）推理方面取得了显著进步。然而，将这些能力扩展到三维场景理解仍然是一个重大挑战。现有的三维多模态大型语言模型（3D-MLLMs）通常依赖于三维数据输入，这限制了其可扩展性和泛化能力。为了解决这一限制，我们提出了一种基于视频的3D-MLLM——Vid-LLM，它可以直接处理视频输入，无需外部三维数据，使其更适合实际部署。在我们的方法中，直接使用几何先验以提高场景感知性能。为了紧凑地将几何线索集成到MLLM中，我们设计了一个跨任务适配器（CTA）模块，以对齐三维几何先验与视觉语言表示。为了确保几何一致性和完整性，我们引入了一个度量深度模型，可以从重建输出中恢复真实尺度几何。最后，通过两阶段蒸馏优化策略对模型进行微调，实现了快速收敛并稳定训练。广泛的实验证明了该方法在三维问答、三维密集标注和三维视觉接地任务中的有效性，展示了其出色的多任务能力。 

---
# HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment 

**Title (ZH)**: HarmMetric Eval: 评估用于大语言模型有害性评估的度量标准和评判标准 

**Authors**: Langqi Yang, Tianhang Zheng, Kedong Xiu, Yixuan Chen, Di Wang, Puning Zhao, Zhan Qin, Kui Ren  

**Link**: [PDF](https://arxiv.org/pdf/2509.24384)  

**Abstract**: The alignment of large language models (LLMs) with human values is critical for their safe deployment, yet jailbreak attacks can subvert this alignment to elicit harmful outputs from LLMs. In recent years, a proliferation of jailbreak attacks has emerged, accompanied by diverse metrics and judges to assess the harmfulness of the LLM outputs. However, the absence of a systematic benchmark to assess the quality and effectiveness of these metrics and judges undermines the credibility of the reported jailbreak effectiveness and other risks. To address this gap, we introduce HarmMetric Eval, a comprehensive benchmark designed to support both overall and fine-grained evaluation of harmfulness metrics and judges. Our benchmark includes a high-quality dataset of representative harmful prompts paired with diverse harmful and non-harmful model responses, alongside a flexible scoring mechanism compatible with various metrics and judges. With HarmMetric Eval, our extensive experiments uncover a surprising result: two conventional metrics--METEOR and ROUGE-1--outperform LLM-based judges in evaluating the harmfulness of model responses, challenging prevailing beliefs about LLMs' superiority in this domain. Our dataset is publicly available at this https URL, and the code is available at this https URL. 

**Abstract (ZH)**: 大型语言模型与人类价值的对齐对于其安全部署至关重要，但劫持攻击可以破坏这种对齐，从而引发有害输出。近年来，劫持攻击层出不穷，伴随有不同的评估有害输出标准的方法和评判标准。然而，缺乏对这些方法和评判标准质量与效果的系统性评估基准，削弱了所报告的劫持攻击效果及其他风险的可信度。为解决这一问题，我们引入了HarmMetric Eval，这是一个全面的基准，旨在支持有害性评估标准的整体和细粒度评估。我们的基准包括高质量的代表性有害提示数据集，配对有不同有害和非有害模型响应，以及兼容各种评估方法和评判标准的灵活评分机制。通过HarmMetric Eval，我们广泛实验得出了一个意外的结果：两种传统的评估方法——METEOR和ROUGE-1——在评估模型响应的有害性方面优于基于大型语言模型的评判标准，这挑战了LLM在这一领域优越性的观点。我们的数据集可在此网址获取：这个 https URL，代码可在此网址获取：这个 https URL。 

---
# REALIGN: Regularized Procedure Alignment with Matching Video Embeddings via Partial Gromov-Wasserstein Optimal Transport 

**Title (ZH)**: REALIGN: 正则化程序对齐通过部分Gromov-Wasserstein最优传输匹配视频嵌入 

**Authors**: Soumyadeep Chandra, Kaushik Roy  

**Link**: [PDF](https://arxiv.org/pdf/2509.24382)  

**Abstract**: Learning from procedural videos remains a core challenge in self-supervised representation learning, as real-world instructional data often contains background segments, repeated actions, and steps presented out of order. Such variability violates the strong monotonicity assumptions underlying many alignment methods. Prior state-of-the-art approaches, such as OPEL, leverage Kantorovich Optimal Transport (KOT) to build frame-to-frame correspondences, but rely solely on feature similarity and fail to capture the higher-order temporal structure of a task. In this paper, we introduce REALIGN, a self-supervised framework for procedure learning based on Regularized Fused Partial Gromov-Wasserstein Optimal Transport (R-FPGWOT). In contrast to KOT, our formulation jointly models visual correspondences and temporal relations under a partial alignment scheme, enabling robust handling of irrelevant frames, repeated actions, and non-monotonic step orders common in instructional videos. To stabilize training, we integrate FPGWOT distances with inter-sequence contrastive learning, avoiding the need for multiple regularizers and preventing collapse to degenerate solutions. Across egocentric (EgoProceL) and third-person (ProceL, CrossTask) benchmarks, REALIGN achieves up to 18.9% average F1-score improvements and over 30% temporal IoU gains, while producing more interpretable transport maps that preserve key-step orderings and filter out noise. 

**Abstract (ZH)**: 基于正则化融合部分Gromov-Wasserstein最优传输的自监督程序学习 

---
# Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning 

**Title (ZH)**: 大规模进化策略：超越强化学习的LLM微调 

**Authors**: Xin Qiu, Yulu Gan, Conor F. Hayes, Qiyao Liang, Elliot Meyerson, Babak Hodjat, Risto Miikkulainen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24372)  

**Abstract**: Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: this https URL. 

**Abstract (ZH)**: 预训练大型语言模型（LLMs）的微调是AI部署管道中的一个关键步骤。强化学习（RL）无疑是最重要的微调方法，贡献了许多最先进的LLMs。相比之下，进化策略（ES），曾显示在具有数百万参数的模型上与RL相当的性能，但由于对其可扩展性的悲观看法而被忽视。在本工作中，我们报告了首次成功将ES扩展用于微调整个参数的LLMs，展示了令人惊讶的事实，即ES可以高效地在数十亿参数上进行搜索，并在多个方面优于现有的RL微调方法，包括样本效率、对长时间奖励的容忍度、对不同基底LLMs的 robust 性、较少倾向于奖励作弊以及在多次运行中的更稳定性能。因此，它为LLM微调开辟了一条新的方向，超越了当前的RL技术所能提供的。代码已开源：this https URL。 

---
# From Satellite to Street: A Hybrid Framework Integrating Stable Diffusion and PanoGAN for Consistent Cross-View Synthesis 

**Title (ZH)**: 从卫星到街道：一种结合 Stable Diffusion 和 PanoGAN 的混合框架，用于一致的跨视角合成 

**Authors**: Khawlah Bajbaa, Abbas Anwar, Muhammad Saqib, Hafeez Anwar, Nabin Sharma, Muhammad Usman  

**Link**: [PDF](https://arxiv.org/pdf/2509.24369)  

**Abstract**: Street view imagery has become an essential source for geospatial data collection and urban analytics, enabling the extraction of valuable insights that support informed decision-making. However, synthesizing street-view images from corresponding satellite imagery presents significant challenges due to substantial differences in appearance and viewing perspective between these two domains. This paper presents a hybrid framework that integrates diffusion-based models and conditional generative adversarial networks to generate geographically consistent street-view images from satellite imagery. Our approach uses a multi-stage training strategy that incorporates Stable Diffusion as the core component within a dual-branch architecture. To enhance the framework's capabilities, we integrate a conditional Generative Adversarial Network (GAN) that enables the generation of geographically consistent panoramic street views. Furthermore, we implement a fusion strategy that leverages the strengths of both models to create robust representations, thereby improving the geometric consistency and visual quality of the generated street-view images. The proposed framework is evaluated on the challenging Cross-View USA (CVUSA) dataset, a standard benchmark for cross-view image synthesis. Experimental results demonstrate that our hybrid approach outperforms diffusion-only methods across multiple evaluation metrics and achieves competitive performance compared to state-of-the-art GAN-based methods. The framework successfully generates realistic and geometrically consistent street-view images while preserving fine-grained local details, including street markings, secondary roads, and atmospheric elements such as clouds. 

**Abstract (ZH)**: 街景视图图像已成为地理空间数据收集和城市分析的重要来源，能够提取有价值的信息以支持明智的决策。然而，从对应的卫星图像合成街景图像由于这两个领域在外观和视角上的显著差异而面临重大挑战。本文提出了一种结合扩散模型和条件生成对抗网络的混合框架，以从卫星图像生成地理一致的街景图像。我们的方法采用多阶段训练策略，并将Stable Diffusion作为核心组件集成到双重分支架构中。为了增强框架的能力，我们引入了一个条件生成对抗网络（GAN），以生成地理一致的全景街景。此外，我们实施了一种融合策略，利用两者的优点，从而提高生成街景图像的几何一致性和视觉质量。所提出框架在Cross-View USA (CVUSA) 数据集上进行了评估，该数据集是交叉视图图像合成的标准化基准。实验结果表明，我们的混合方法在多个评估指标上优于仅扩散的方法，并且在与最先进的GAN基方法的性能上具有竞争力。该框架成功生成了具有真实感和几何一致性的街景图像，同时保留了详细的局部细节，包括街道标记、次要道路和大气元素如云彩。 

---
# Watermarking Diffusion Language Models 

**Title (ZH)**: 扩散语言模型中的水印技术 

**Authors**: Thibaud Gloaguen, Robin Staab, Nikola Jovanović, Martin Vechev  

**Link**: [PDF](https://arxiv.org/pdf/2509.24368)  

**Abstract**: We introduce the first watermark tailored for diffusion language models (DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in contrast to standard autoregressive language models (ARLMs) which generate tokens sequentially. While there has been much work in ARLM watermarking, a key challenge when attempting to apply these schemes directly to the DLM setting is that they rely on previously generated tokens, which are not always available with DLM generation. In this work we address this challenge by: (i) applying the watermark in expectation over the context even when some context tokens are yet to be determined, and (ii) promoting tokens which increase the watermark strength when used as context for other tokens. This is accomplished while keeping the watermark detector unchanged. Our experimental evaluation demonstrates that the DLM watermark leads to a >99% true positive rate with minimal quality impact and achieves similar robustness to existing ARLM watermarks, enabling for the first time reliable DLM watermarking. 

**Abstract (ZH)**: 我们介绍了首个针对扩散语言模型(DLM)的水印，这是一种新兴的LLM范式，能够以任意顺序生成标记，而传统的自回归语言模型(ARLM)则按顺序生成标记。尽管对ARLM进行了大量水印研究，但在直接应用于DLM设置时，这些方案依赖于已生成的标记，而这些标记在DLM生成过程中未必可用。本文通过以下方式解决了这一挑战：(i) 即使某些上下文标记尚未确定，也在上下文期望值上应用水印；(ii) 促进增加其他标记水印强度的标记。我们保持水印检测器不变，实现了这一目标。我们的实验评估表明，该DLM水印在对质量影响极小的情况下，达到了超过99%的真实正率，并且具有与现有ARLM水印相似的鲁棒性，从而首次实现了可靠的DLM水印技术。 

---
# Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models 

**Title (ZH)**: Uni-X：通过两端分离架构缓解模态冲突的统一多模态模型 

**Authors**: Jitai Hao, Hao Liu, Xinyan Xiao, Qiang Huang, Jun Yu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24365)  

**Abstract**: Unified Multimodal Models (UMMs) built on shared autoregressive (AR) transformers are attractive for their architectural simplicity. However, we identify a critical limitation: when trained on multimodal inputs, modality-shared transformers suffer from severe gradient conflicts between vision and text, particularly in shallow and deep layers. We trace this issue to the fundamentally different low-level statistical properties of images and text, while noting that conflicts diminish in middle layers where representations become more abstract and semantically aligned. To overcome this challenge, we propose Uni-X, a two-end-separated, middle-shared architecture. Uni-X dedicates its initial and final layers to modality-specific processing, while maintaining shared parameters in the middle layers for high-level semantic fusion. This X-shaped design not only eliminates gradient conflicts at both ends but also further alleviates residual conflicts in the shared layers. Extensive experiments validate the effectiveness of Uni-X. Under identical training conditions, Uni-X achieves superior training efficiency compared to strong baselines. When scaled to 3B parameters with larger training data, Uni-X matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for image generation alongside strong performance in text and vision understanding tasks. These results establish Uni-X as a parameter-efficient and scalable foundation for future unified multimodal modeling. Our code is available at this https URL 

**Abstract (ZH)**: 统一跨模态模型Uni-X：两端分离中间共享的架构设计 

---
# UI-UG: A Unified MLLM for UI Understanding and Generation 

**Title (ZH)**: UI-UG：统一的联合学习模型用于界面理解与生成 

**Authors**: Hao Yang, Weijie Qiu, Ru Zhang, Zhou Fang, Ruichao Mao, Xiaoyu Lin, Maji Huang, Zhaosong Huang, Teng Guo, Shuoyang Liu, Hai Rao  

**Link**: [PDF](https://arxiv.org/pdf/2509.24361)  

**Abstract**: Although Multimodal Large Language Models (MLLMs) have been widely applied across domains, they are still facing challenges in domain-specific tasks, such as User Interface (UI) understanding accuracy and UI generation quality. In this paper, we introduce UI-UG (a unified MLLM for UI Understanding and Generation), integrating both capabilities. For understanding tasks, we employ Supervised Fine-tuning (SFT) combined with Group Relative Policy Optimization (GRPO) to enhance fine-grained understanding on the modern complex UI data. For generation tasks, we further use Direct Preference Optimization (DPO) to make our model generate human-preferred UIs. In addition, we propose an industrially effective workflow, including the design of an LLM-friendly domain-specific language (DSL), training strategies, rendering processes, and evaluation metrics. In experiments, our model achieves state-of-the-art (SOTA) performance on understanding tasks, outperforming both larger general-purpose MLLMs and similarly-sized UI-specialized models. Our model is also on par with these larger MLLMs in UI generation performance at a fraction of the computational cost. We also demonstrate that integrating understanding and generation tasks can improve accuracy and quality for both tasks. 

**Abstract (ZH)**: 尽管多模态大型语言模型（MLLMs）已在多个领域广泛应用，但在特定任务如用户界面（UI）理解准确性和UI生成质量方面仍面临挑战。本文介绍UI-UG（一种统一的MLLM，用于UI理解和生成），集成了这两种能力。在理解任务中，我们采用监督微调（SFT）结合组相对策略优化（GRPO）来增强对现代复杂UI数据的细粒度理解。在生成任务中，我们进一步使用直接偏好优化（DPO）使模型生成人类偏好UI。此外，我们提出了一种工业上有效的流程，包括为LLM设计专用领域语言（DSL）、训练策略、渲染流程和评估指标。实验结果表明，我们的模型在理解任务中达到了迄今为止最好的性能，优于更大的通用MLLM和同等规模的UI专用模型。而且，在计算成本更低的情况下，我们的模型在UI生成方面也与这些较大的MLLM相当。我们还证明，将理解和生成任务结合起来可以提高两个任务的准确性和质量。 

---
# An Enhanced Pyramid Feature Network Based on Long-Range Dependencies for Multi-Organ Medical Image Segmentation 

**Title (ZH)**: 基于长程依赖关系的增强 pyramid 特征网络多器官医疗图像分割 

**Authors**: Dayu Tan, Cheng Kong, Yansen Su, Hai Chen, Dongliang Yang, Junfeng Xia, Chunhou Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2509.24358)  

**Abstract**: In the field of multi-organ medical image segmentation, recent methods frequently employ Transformers to capture long-range dependencies from image features. However, these methods overlook the high computational cost of Transformers and their deficiencies in extracting local detailed information. To address high computational costs and inadequate local detail information, we reassess the design of feature extraction modules and propose a new deep-learning network called LamFormer for fine-grained segmentation tasks across multiple organs. LamFormer is a novel U-shaped network that employs Linear Attention Mamba (LAM) in an enhanced pyramid encoder to capture multi-scale long-range dependencies. We construct the Parallel Hierarchical Feature Aggregation (PHFA) module to aggregate features from different layers of the encoder, narrowing the semantic gap among features while filtering information. Finally, we design the Reduced Transformer (RT), which utilizes a distinct computational approach to globally model up-sampled features. RRT enhances the extraction of detailed local information and improves the network's capability to capture long-range dependencies. LamFormer outperforms existing segmentation methods on seven complex and diverse datasets, demonstrating exceptional performance. Moreover, the proposed network achieves a balance between model performance and model complexity. 

**Abstract (ZH)**: 多器官医学图像分割领域的LamFormer：平衡高性能与低复杂度的新型深度学习网络 

---
# Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining 

**Title (ZH)**: 超越重复：基于数据受限预训练的文本简化和课程学习 

**Authors**: Matthew Theodore Roque, Dan John Velasco  

**Link**: [PDF](https://arxiv.org/pdf/2509.24356)  

**Abstract**: Most studies on language model pretraining focus on large datasets, leaving open questions about optimization in data-constrained settings. In such settings, the effects of training data order and of including alternative versions of the same text remain underexplored. We address this by studying curriculum learning in pretraining, focusing on text-complexity ordering and data augmentation via simplification. We ask: (1) Does simplifying texts enhance representation quality more than reusing the original data? and (2) Does ordering data by text complexity yield better representations? To answer, we build on a pair of parallel corpora where human-written paragraphs are aligned with LLM-simplified variants, and test four data schedules: repeated exposure, low-to-high complexity, high-to-low, and interleaved. We analyze models' representation quality from a sample efficiency perspective via fine-tuning, as well as its zero-shot performance on linguistic knowledge, entity tracking, world knowledge, and commonsense reasoning. Our findings show that adding simplified data improves fine-tuning and zero-shot performance over a repeated-exposure baseline: smaller models benefit from low-to-high complexity, while larger models perform better with interleaved ordering. 

**Abstract (ZH)**: 大多数语言模型预训练研究集中在大规模数据集上，而在数据受限的设置中优化方面的疑问仍然存在。在这种设置中，训练数据顺序和包含相同文本的替代版本的影响仍缺乏探索。我们通过研究预训练中的逐级学习来解决这一问题，重点关注文本复杂度排序和通过简化进行的数据增强。我们提出的问题是：（1）简化文本是否比重复使用原始数据更能提升表示质量？（2）按照文本复杂度排序数据是否能得到更好的表示？为回答这些问题，我们在一组平行语料库上进行研究，其中人工撰写的段落与LLM简化的变体对齐，并测试了四种数据调度方式：重复暴露、低到高复杂度、高到低和交错。我们从样本效率的角度通过微调分析模型的表示质量，并考察其在语言知识、实体跟踪、世界知识和常识推理等任务上的零样本性能。我们的研究发现，增加简化数据比重复暴露基线更能提高微调和零样本性能：小型模型受益于低到高复杂度，而大型模型在交错排序下性能更佳。 

---
# Dynamic Orchestration of Multi-Agent System for Real-World Multi-Image Agricultural VQA 

**Title (ZH)**: 面向现实多图像农业VQA的多agents系统动态 orchestration 

**Authors**: Yan Ke, Xin Yu, Heming Du, Scott Chapman, Helen Huang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24350)  

**Abstract**: Agricultural visual question answering is essential for providing farmers and researchers with accurate and timely knowledge. However, many existing approaches are predominantly developed for evidence-constrained settings such as text-only queries or single-image cases. This design prevents them from coping with real-world agricultural scenarios that often require multi-image inputs with complementary views across spatial scales, and growth stages. Moreover, limited access to up-to-date external agricultural context makes these systems struggle to adapt when evidence is incomplete. In addition, rigid pipelines often lack systematic quality control. To address this gap, we propose a self-reflective and self-improving multi-agent framework that integrates four roles, the Retriever, the Reflector, the Answerer, and the Improver. They collaborate to enable context enrichment, reflective reasoning, answer drafting, and iterative improvement.
A Retriever formulates queries and gathers external information, while a Reflector assesses adequacy and triggers sequential reformulation and renewed retrieval. Two Answerers draft candidate responses in parallel to reduce bias. The Improver refines them through iterative checks while ensuring that information from multiple images is effectively aligned and utilized. Experiments on the AgMMU benchmark show that our framework achieves competitive performance on multi-image agricultural QA. 

**Abstract (ZH)**: 农业视觉问答对于为农民和研究人员提供准确及时的知识至关重要。然而，许多现有方法主要针对如仅文本查询或单张图像等证据受限的场景进行开发。这种设计使其难以应对需要跨空间尺度和生长阶段互补视角的多张图像输入的真实农业场景，并且在证据不完整时难以适应。此外，固定的流程往往缺乏系统性的质量控制。为解决这一问题，我们提出了一种自我反思和自我改进的多agent框架，整合了检索者、反思者、回答者和改进者四个角色。它们协作以实现上下文丰富、反思推理、回答草拟及迭代改进。检索者制定问题并收集外部信息，反思者评估其充分性并触发顺序重新表述和重新检索。两个回答者并行草拟候选回答以减少偏见。改进者通过迭代检查改善回答，同时确保来自多张图像的信息得到有效协调和利用。在AgMMU基准测试上的实验表明，我们的框架在多图农业问答任务中取得了竞争力的表现。 

---
# Towards Generalizable PDE Dynamics Forecasting via Physics-Guided Invariant Learning 

**Title (ZH)**: 基于物理引导不变性学习的泛化偏微分方程动力学预测 

**Authors**: Siyang Li, Yize Chen, Yan Guo, Ming Huang, Hui Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2509.24332)  

**Abstract**: Advanced deep learning-based approaches have been actively applied to forecast the spatiotemporal physical dynamics governed by partial differential equations (PDEs), which acts as a critical procedure in tackling many science and engineering problems. As real-world physical environments like PDE system parameters are always capricious, how to generalize across unseen out-of-distribution (OOD) forecasting scenarios using limited training data is of great importance. To bridge this barrier, existing methods focus on discovering domain-generalizable representations across various PDE dynamics trajectories. However, their zero-shot OOD generalization capability remains deficient, since extra test-time samples for domain-specific adaptation are still required. This is because the fundamental physical invariance in PDE dynamical systems are yet to be investigated or integrated. To this end, we first explicitly define a two-fold PDE invariance principle, which points out that ingredient operators and their composition relationships remain invariant across different domains and PDE system evolution. Next, to capture this two-fold PDE invariance, we propose a physics-guided invariant learning method termed iMOOE, featuring an Invariance-aligned Mixture Of Operator Expert architecture and a frequency-enriched invariant learning objective. Extensive experiments across simulated benchmarks and real-world applications validate iMOOE's superior in-distribution performance and zero-shot generalization capabilities on diverse OOD forecasting scenarios. 

**Abstract (ZH)**: 基于深度学习的先进方法已被积极应用于预报由偏微分方程（PDE）支配的时空物理动力学，这是解决许多科学与工程问题的关键步骤。由于现实世界中的物理环境如PDE系统参数总是不可预测的，如何在有限的训练数据下泛化到未见过的分布外（OOD）预报场景具有重要意义。为解决这一障碍，现有方法主要集中在发现适用于各种PDE动力学轨迹的域泛化表示。然而，它们的零样本分布外泛化能力仍然不足，因为仍需额外的测试时样本进行领域特异性适应。这是因为PDE动力系统中的基本物理不变性尚未被研究或集成。为此，我们首先明确定义了两方面的PDE不变性原理，指出成分算子及其组合关系在不同领域和PDE系统演化中保持不变。接下来，为捕捉这种两方面的PDE不变性，我们提出了一种物理引导的不变学习方法iMOOE，该方法采用不变性对齐的算子专家混合架构和频率增强的不变学习目标。在模拟基准和实际应用中的广泛实验验证了iMOOE在不同分布外预报场景中的优越的内分布性能和零样本泛化能力。 

---
# TraitSpaces: Towards Interpretable Visual Creativity for Human-AI Co-Creation 

**Title (ZH)**: TraitSpaces: 向可解释的人工智能视觉创造力方向的人机共创 

**Authors**: Prerna Luthra  

**Link**: [PDF](https://arxiv.org/pdf/2509.24326)  

**Abstract**: We introduce a psychologically grounded and artist-informed framework for modeling visual creativity across four domains: Inner, Outer, Imaginative, and Moral Worlds. Drawing on interviews with practicing artists and theories from psychology, we define 12 traits that capture affective, symbolic, cultural, and ethical dimensions of this http URL 20k artworks from the SemArt dataset, we annotate images with GPT 4.1 using detailed, theory-aligned prompts, and evaluate the learnability of these traits from CLIP image embeddings. Traits such as Environmental Dialogicity and Redemptive Arc are predicted with high reliability ($R^2 \approx 0.64 - 0.68$), while others like Memory Imprint remain challenging, highlighting the limits of purely visual encoding. Beyond technical metrics, we visualize a "creativity trait-space" and illustrate how it can support interpretable, trait-aware co-creation - e.g., sliding along a Redemptive Arc axis to explore works of adversity and renewal. By linking cultural-aesthetic insights with computational modeling, our work aims not to reduce creativity to numbers, but to offer shared language and interpretable tools for artists, researchers, and AI systems to collaborate meaningfully. 

**Abstract (ZH)**: 基于心理依据和艺术家指导的跨领域视觉创造力建模框架：内外想象与道德世界中的情感、象征、文化与伦理维度探究 

---
# Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs 

**Title (ZH)**: 双重机制的价值表达：LLMs中内在价值与激发价值的双重机制 

**Authors**: Jongwook Han, Jongwon Lim, Injin Kong, Yohan Jo  

**Link**: [PDF](https://arxiv.org/pdf/2509.24319)  

**Abstract**: Large language models (LLMs) can express different values in two distinct ways: (1) intrinsic expression, reflecting the model's inherent values learned during training, and (2) prompted expression, elicited by explicit prompts. Given their widespread use in value alignment and persona steering, it is paramount to clearly understand their underlying mechanisms, particularly whether they mostly overlap (as one might expect) or rely on substantially different mechanisms, but this remains largely understudied. We analyze this at the mechanistic level using two approaches: (1) value vectors, feature directions representing value mechanisms extracted from the residual stream, and (2) value neurons, MLP neurons that contribute to value expressions. We demonstrate that intrinsic and prompted value mechanisms partly share common components that are crucial for inducing value expression, but also possess unique elements that manifest in different ways. As a result, these mechanisms lead to different degrees of value steerability (prompted > intrinsic) and response diversity (intrinsic > prompted). In particular, components unique to the intrinsic mechanism seem to promote lexical diversity in responses, whereas those specific to the prompted mechanism primarily strengthen instruction following, taking effect even in distant tasks like jailbreaking. 

**Abstract (ZH)**: 大型语言模型（LLMs）可以通过两种不同的方式表达不同的价值观：（1）内在表达，反映模型在训练过程中学到的固有价值观，和（2）提示表达，由明确的提示引发。鉴于它们在价值观对齐和角色引导中的广泛应用，深刻理解其背后的机制至关重要，特别是它们是否主要重叠（如人们预期的那样）或者依赖于显著不同的机制，但这一点仍然研究不足。我们从机制层面采用两种方法进行分析：（1）价值向量，从残差流中提取的价值机制的特征方向，和（2）价值神经元，为价值表达做出贡献的MLP神经元。我们证明，内在和提示的价值机制部分共享对于诱发价值表达至关重要的共同组件，但也具有以不同方式表现的独特元素。这导致了不同的价值观可控性程度（提示作用 > 内在作用）和响应多样性程度（内在作用 > 提示作用）。特别是，内在机制独有的组件似乎促进了响应的词汇多样性，而提示机制特有的组件主要增强了指令跟随的能力，即使在像监狱突破这样远离的任务中也能生效。 

---
# A study of Universal ODE approaches to predicting soil organic carbon 

**Title (ZH)**: 基于通用ODE方法预测土壤有机碳的研究 

**Authors**: Satyanarayana Raju G.V.V, Prathamesh Dinesh Joshi, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat  

**Link**: [PDF](https://arxiv.org/pdf/2509.24306)  

**Abstract**: Soil Organic Carbon (SOC) is a foundation of soil health and global climate resilience, yet its prediction remains difficult because of intricate physical, chemical, and biological processes. In this study, we explore a Scientific Machine Learning (SciML) framework built on Universal Differential Equations (UDEs) to forecast SOC dynamics across soil depth and time. UDEs blend mechanistic physics, such as advection diffusion transport, with neural networks that learn nonlinear microbial production and respiration. Using synthetic datasets, we systematically evaluated six experimental cases, progressing from clean, noise free benchmarks to stress tests with high (35%) multiplicative, spatially correlated noise. Our results highlight both the potential and limitations of the approach. In noise free and moderate noise settings, the UDE accurately reconstructed SOC dynamics. In clean terminal profile at 50 years (Case 4) achieved near perfect fidelity, with MSE = 1.6e-5, and R2 = 0.9999. Case 5, with 7% noise, remained robust (MSE = 3.4e-6, R2 = 0.99998), capturing depth wise SOC trends while tolerating realistic measurement uncertainty. In contrast, Case 3 (35% noise at t = 0) showed clear evidence of overfitting: the model reproduced noisy inputs with high accuracy but lost generalization against the clean truth (R2 = 0.94). Case 6 (35% noise at t = 50) collapsed toward overly smooth mean profiles, failing to capture depth wise variability and yielding negative R2, underscoring the limits of standard training under severe uncertainty. These findings suggest that UDEs are well suited for scalable, noise tolerant SOC forecasting, though advancing toward field deployment will require noise aware loss functions, probabilistic modelling, and tighter integration of microbial dynamics. 

**Abstract (ZH)**: 基于通用微分方程的科学机器学习框架在土壤有机碳动态预测中的应用：噪声鲁棒性研究 

---
# Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports 

**Title (ZH)**: 弥合行为-神经差距：多模态AI比人类自我报告更准确揭示情感的大脑几何结构 

**Authors**: Changde Du, Yizhuo Lu, Zhongyu Huang, Yi Sun, Zisen Zhou, Shaozheng Qin, Huiguang He  

**Link**: [PDF](https://arxiv.org/pdf/2509.24298)  

**Abstract**: The ability to represent emotion plays a significant role in human cognition and social interaction, yet the high-dimensional geometry of this affective space and its neural underpinnings remain debated. A key challenge, the `behavior-neural gap,' is the limited ability of human self-reports to predict brain activity. Here we test the hypothesis that this gap arises from the constraints of traditional rating scales and that large-scale similarity judgments can more faithfully capture the brain's affective geometry. Using AI models as `cognitive agents,' we collected millions of triplet odd-one-out judgments from a multimodal large language model (MLLM) and a language-only model (LLM) in response to 2,180 emotionally evocative videos. We found that the emergent 30-dimensional embeddings from these models are highly interpretable and organize emotion primarily along categorical lines, yet in a blended fashion that incorporates dimensional properties. Most remarkably, the MLLM's representation predicted neural activity in human emotion-processing networks with the highest accuracy, outperforming not only the LLM but also, counterintuitively, representations derived directly from human behavioral ratings. This result supports our primary hypothesis and suggests that sensory grounding--learning from rich visual data--is critical for developing a truly neurally-aligned conceptual framework for emotion. Our findings provide compelling evidence that MLLMs can autonomously develop rich, neurally-aligned affective representations, offering a powerful paradigm to bridge the gap between subjective experience and its neural substrates. Project page: this https URL. 

**Abstract (ZH)**: 情感表示能力在人类认知和社会互动中扮演着重要角色，但这一情感空间的高维几何结构及其神经基础仍存在争议。一个关键挑战是“行为-神经差距”，即人类自陈报告预测脑活动的能力有限。我们测试了这一差距源于传统评分量表的限制，并认为大规模相似性判断更能忠实地捕捉大脑的情感几何结构。通过使用AI模型作为“认知代理”，我们从一个多模态大型语言模型（MLLM）和一个仅基于语言的模型（LLM）收集了对2,180个具有情感触发性的视频作出的上百万组三元组奇数项判断。我们发现，这些模型产生的30维嵌入高度可解释，并主要按类别方式组织情感，但又融合了维度属性。最令人惊讶的是，MLLM的情感表示准确预示了人类情感处理网络的神经活动，不仅优于LLM，甚至也优于直接来源于人类行为评分的情感表示。这一结果支持了我们的主要假设，并表明从丰富的视觉数据中学习对于开发真正神经对齐的概念框架至关重要。我们的发现提供了有力证据，证明MLLM能够自主发展丰富且神经对齐的情感表示，为跨越主观体验及其神经基础之间的鸿沟提供了强大的范式。 

---
# Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs 

**Title (ZH)**: Q-镜像：释放科学文本型问答 pair 的多模态潜力 

**Authors**: Junying Wang, Zicheng Zhang, Ye Shen, Yalun Wu, Yingji Liang, Yijin Guo, Farong Wen, Wenzhe Li, Xuezhi Zhao, Qi Jia, Guangtao Zhai  

**Link**: [PDF](https://arxiv.org/pdf/2509.24297)  

**Abstract**: High-quality, multi-modal benchmarks are crucial for advancing scientific reasoning in large models yet their manual creation is costly and unscalable. To address this bottleneck, we explore the potential for transforming Text-Only QA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include three parts: 1) Task Definition \& Evaluation Rubric: We develop a TQA-to-MMQA framework and establish a comprehensive, multi-dimensional MMQA quality rubric that provides principles for the transformation. 2) Benchmark Construction: Then we construct two extensive benchmarks to rigorously evaluate state-of-the-art generation \& understanding models on the distinct tasks of MMQA generation \& MMQA quality evaluation. 3) Preliminary Solution: We develop an agentic system (Q-Mirror), which operationalizes our framework by integrating MMQA generation and evaluation into a closed loop for iterative refinement. Our experiments show that while state-of-the-art models can generate MMQAs, their outputs still leave substantial gaps, underscoring the need for reliable evaluation. We further demonstrate that top-tier understanding models align closely with human judgment in MMQA quality assessment. Leveraging both insights, the Q-Mirror agent raises average scores from 78.90 to 85.22 and pass rates from 72\% to 95\%, offering a practical path to large-scale scientific benchmarks. 

**Abstract (ZH)**: 高质量、多模态基准对于大型模型促进科学推理至关重要，但其手动创建成本高昂且不可扩展。为应对这一瓶颈，我们探索将文本_ONLY_问答对（TQAs）转换为高质量多模态问答对（MMQAs）的潜力，MMQAs包括三个部分：1）任务定义与评估准则：我们开发了一个TQA-to-MMQA框架，并建立了全面的多维度MMQA质量评估准则，提供了转换的原则。2）基准建设：接下来我们构建了两个广泛的基准，以严格评估最先进的生成与理解模型在多模态问答生成与多模态问答质量评估任务中的表现。3）初步解决方案：我们开发了一个自主系统（Q-Mirror），通过将多模态问答生成与评估集成到一个闭环中进行迭代优化，具体化了我们的框架。我们的实验表明，尽管最先进的模型能够生成MMQAs，但其输出仍然存在显著差距，强调了可靠评估的需求。此外，我们证明了顶级理解模型在多模态问答质量评估中与人类判断高度一致。结合这些见解，Q-Mirror代理将平均得分从78.90提高到85.22，通过率达到从72%提高到95%，提供了一条大规模科学基准建设的实用路径。 

---
# DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models 

**Title (ZH)**: DiffuGuard: 从自洽安全性丧失到扩散大语言模型的安全恢复 

**Authors**: Zherui Li, Zheng Nie, Zhenhong Zhou, Yufei Guo, Yue Liu, Yitong Zhang, Yu Cheng, Qingsong Wen, Kun Wang, Jiaheng Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24296)  

**Abstract**: The rapid advancement of Diffusion Large Language Models (dLLMs) introduces unprecedented vulnerabilities that are fundamentally distinct from Autoregressive LLMs, stemming from their iterative and parallel generation mechanisms. In this paper, we conduct an in-depth analysis of dLLM vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step and inter-step dynamics. Experimental results reveal a harmful bias inherent in the standard greedy remasking strategy and identify a critical phenomenon we term Denoising-path Dependence, where the safety of early-stage tokens decisively influences the final output. These findings also indicate that while current decoding strategies constitute a significant vulnerability, dLLMs possess a substantial intrinsic safety potential. To unlock this potential, we propose DiffuGuard, a training-free defense framework that addresses vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking dynamically introduces controlled randomness to mitigate greedy selection bias, while Block-level Audit and Repair exploits internal model representations for autonomous risk detection and guided correction. Comprehensive experiments on four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while preserving model utility and efficiency. Our code is available at: this https URL. 

**Abstract (ZH)**: 扩散大规模语言模型的快速进展引入了与自回归大规模语言模型本质上不同的前所未有的脆弱性，源于它们的迭代和并行生成机制。在本文中，我们从两个不同的维度——单步内部动态和跨步动态——深入分析了扩散大规模语言模型对玩砸攻击的脆弱性。实验结果揭示了标准贪婪重新遮掩策略固有的有害偏差，并确定了一种我们称为去噪路径依赖的关键现象，早期阶段令牌的安全性对最终输出有决定性影响。这些发现也表明，虽然当前的解码策略构成了重大脆弱性，但扩散大规模语言模型仍然具有巨大的内在安全性潜力。为了解锁这一潜力，我们提出了DiffuGuard，这是一个无需训练的防御框架，采用双重方法来应对脆弱性：随机退火重新遮掩动态引入受控的随机性以减轻贪婪选择偏差，而块级审核与修复则利用内部模型表示进行自主风险检测和指导修正。在四个扩散大规模语言模型上的综合实验表明，DiffuGuard表现出色，将六种不同玩砸方法的攻击成功率从47.9%降至14.7%，同时保持模型的实用性和效率。源代码可在以下链接获取：this https URL。 

---
# Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement 

**Title (ZH)**: 让大模型开口说话：通过迭代对比精炼生成文本嵌入 

**Authors**: Yu-Che Tsai, Kuan-Yu Chen, Yuan-Chi Li, Yuan-Hao Chen, Ching-Yu Tsai, Shou-De Lin  

**Link**: [PDF](https://arxiv.org/pdf/2509.24291)  

**Abstract**: Existing large language model (LLM)-based embeddings typically adopt an encoder-only paradigm, treating LLMs as static feature extractors and overlooking their core generative strengths. We introduce GIRCSE (Generative Iterative Refinement for Contrastive Sentence Embeddings), a novel framework that leverages autoregressive generation to iteratively refine semantic representations. By producing sequences of soft tokens optimized under contrastive objective, GIRCSE captures latent concepts and implicit semantics that encoder-only methods often miss. To guide this process, we propose an Iterative Contrastive Refinement (ICR) objective that encourages each refinement step to yield better representations. Extensive experiments show that GIRCSE outperforms strong LLM-based embedding baselines on the MTEB benchmark and instruction-following tasks. Moreover, GIRCSE exhibits an emergent test-time scaling property: generating more tokens at inference steadily improves embedding quality. Our results establish generative iterative refinement as a new paradigm for representation learning. 

**Abstract (ZH)**: 现有的大型语言模型（LLM）嵌入通常采用编码器唯一切expenses，将LLM视为静态特征提取器并忽视其核心生成优势。我们提出了GIRCSE（生成迭代细化对contrastive句子嵌入），这是一种新型框架，利用自回归生成来迭代细化语义表示。通过在对比目标下优化软标记序列，GIRCSE 捕捉到编码器唯一切方法经常忽略的潜在概念和隐式语义。为指导这一过程，我们提出了迭代对比细化（ICR）目标，鼓励每一细化步骤都产生更好的表示。大量实验表明，GIRCSE 在 MTEB 基准和指令遵循任务中优于强 LLM 基础嵌入。此外，GIRCSE 展现出一种新兴的测试时缩放性质：推理时生成更多标记会逐步提高嵌入质量。我们的结果确立了生成迭代细化作为新的表示学习范式。 

---
# SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents 

**Title (ZH)**: SimuHome：一种考虑时间与环境因素的智能家居LLM代理基准测试 

**Authors**: Gyuhyeon Seo, Jungwoo Yang, Junseong Pyo, Nalim Kim, Jonggeun Lee, Yohan Jo  

**Link**: [PDF](https://arxiv.org/pdf/2509.24282)  

**Abstract**: Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks. However, smart homes introduce distinct challenges, requiring agents to handle latent user intents, temporal dependencies, device constraints, scheduling, and more. The main bottlenecks for developing smart home agents with such capabilities include the lack of a realistic simulation environment where agents can interact with devices and observe the results, as well as a challenging benchmark to evaluate them. To address this, we introduce $\textbf{SimuHome}$, a time-accelerated home environment that simulates smart devices, supports API calls, and reflects changes in environmental variables. By building the simulator on the Matter protocol (the global industry standard for smart home communication), SimuHome provides a high-fidelity environment, and agents validated in SimuHome can be deployed on real Matter-compliant devices with minimal adaptation. We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities. Our evaluation of 11 agents under a unified ReAct framework reveals that while models perform well on simple tasks, they struggle with latent intent inference, state verification, and especially temporal scheduling. Even the top-performing model, GPT-4.1, reaches only 54% success rate. These findings highlight a critical need for methods that can reliably verify the current state via tools before acting and coordinate time-dependent actions. 

**Abstract (ZH)**: Large Language Model (LLM) 剂剂在多步、工具增强任务中表现出色。然而，智能家居带来了独特的挑战，要求代理处理隐含用户意图、时间依赖性、设备约束、排程等问题。开发具有此类能力的智能家居代理的主要瓶颈包括缺乏一个真实的模拟环境，代理可以在其中与设备交互并观察结果，以及缺乏一个具有挑战性的基准来评估它们。为了解决这一问题，我们引入了**SimuHome**，这是一种时间加速的家庭环境，可以模拟智能设备、支持API调用，并反映环境变量的变化。通过在全球智能家庭通信标准Matter协议的基础上构建模拟器，SimuHome提供了一个高度真实的环境，经SimuHome验证的代理可以最小限度地适应并部署在真正的Matter合规设备上。我们提供了涵盖十二种用户查询类型的600个场景的具有挑战性的基准测试，这些查询需要前述的能力。在统一的ReAct框架下评估11个代理模型显示，虽然模型在简单任务上表现良好，但在隐含意图推断、状态验证以及特别是时间调度方面存在困难。即使是表现最佳的模型GPT-4.1，成功率也只有54%。这些发现强调了方法的重要性，这些方法需要能够通过工具可靠地验证当前状态并在采取行动之前进行协调，以及对时间依赖性动作进行协调。 

---
# Adversarial Reinforcement Learning Framework for ESP Cheater Simulation 

**Title (ZH)**: 对抗强化学习框架下的ESP作弊模拟 

**Authors**: Inkyu Park, Jeong-Gwan Lee, Taehwan Kwon, Juheon Choi, Seungku Kim, Junsu Kim, Kimin Lee  

**Link**: [PDF](https://arxiv.org/pdf/2509.24274)  

**Abstract**: Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game information such as enemy locations, are difficult to detect because their effects are not directly observable in player behavior. The lack of observable evidence makes it difficult to collect reliably labeled data, which is essential for training effective anti-cheat systems. Furthermore, cheaters often adapt their behavior by limiting or disguising their cheat usage, which further complicates detection and detector development. To address these challenges, we propose a simulation framework for controlled modeling of ESP cheaters, non-cheaters, and trajectory-based detectors. We model cheaters and non-cheaters as reinforcement learning agents with different levels of observability, while detectors classify their behavioral trajectories. Next, we formulate the interaction between the cheater and the detector as an adversarial game, allowing both players to co-adapt over time. To reflect realistic cheater strategies, we introduce a structured cheater model that dynamically switches between cheating and non-cheating behaviors based on detection risk. Experiments demonstrate that our framework successfully simulates adaptive cheater behaviors that strategically balance reward optimization and detection evasion. This work provides a controllable and extensible platform for studying adaptive cheating behaviors and developing effective cheat detectors. 

**Abstract (ZH)**: 异感知（ESP）作弊者的行为建模及检测仿真框架 

---
# Cycle Diffusion Model for Counterfactual Image Generation 

**Title (ZH)**: 循环扩散模型用于反事实图像生成 

**Authors**: Fangrui Huang, Alan Wang, Binxu Li, Bailey Trang, Ridvan Yesiloglu, Tianyu Hua, Wei Peng, Ehsan Adeli  

**Link**: [PDF](https://arxiv.org/pdf/2509.24267)  

**Abstract**: Deep generative models have demonstrated remarkable success in medical image synthesis. However, ensuring conditioning faithfulness and high-quality synthetic images for direct or counterfactual generation remains a challenge. In this work, we introduce a cycle training framework to fine-tune diffusion models for improved conditioning adherence and enhanced synthetic image realism. Our approach, Cycle Diffusion Model (CDM), enforces consistency between generated and original images by incorporating cycle constraints, enabling more reliable direct and counterfactual generation. Experiments on a combined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and PPMI) show that our method improves conditioning accuracy and enhances image quality as measured by FID and SSIM. The results suggest that the cycle strategy used in CDM can be an effective method for refining diffusion-based medical image generation, with applications in data augmentation, counterfactual, and disease progression modeling. 

**Abstract (ZH)**: 深生成模型在医学图像合成中取得了显著成功，但确保条件忠实性和高质量的合成图像以进行直接生成或反事实生成仍是挑战。本文介绍了一种循环训练框架，以微调扩散模型，从而提高条件依从性和增强合成图像的真实感。我们的方法，循环扩散模型（CDM），通过引入循环约束确保生成图像与原始图像的一致性，从而实现更可靠的直接生成和反事实生成。在合并的3D脑MRI数据集（来自ABCD、HCP老化与年轻成人、ADNI和PPMI）上的实验表明，我们的方法可以提高条件准确性和通过FID和SSIM衡量的图像质量。结果表明，CDM中使用的循环策略可以有效改进基于扩散的医学图像生成，适用于数据增强、反事实和疾病进展建模。 

---
# LAMP-PRo: Label-aware Attention for Multi-label Prediction of DNA- and RNA-binding Proteins using Protein Language Models 

**Title (ZH)**: LAMP-PRo：基于标签的注意力机制用于蛋白质语言模型预测DNA-和RNA结合蛋白多标签分类 

**Authors**: Nimisha Ghosh, Dheeran Sankaran, Rahul Balakrishnan Adhi, Sharath S, Amrut Anand  

**Link**: [PDF](https://arxiv.org/pdf/2509.24262)  

**Abstract**: Identifying DNA- (DBPs) and RNA-binding proteins (RBPs) is crucial for the understanding of cell function, molecular interactions as well as regulatory functions. Owing to their high similarity, most of the existing approaches face challenges in differentiating between DBPs and RBPs leading to high cross-prediction errors. Moreover, identifying proteins which bind to both DNA and RNA (DRBPs) is also quite a challenging task. In this regard, we propose a novel framework viz. LAMP-PRo which is based on pre-trained protein language model (PLM), attention mechanisms and multi-label learning to mitigate these issues. First, pre-trained PLM such ESM-2 is used for embedding the protein sequences followed by convolutional neural network (CNN). Subsequently multi-head self-attention mechanism is applied for the contextual information while label-aware attention is used to compute class-specific representations by attending to the sequence in a way that is tailored to each label (DBP, RBP and non-NABP) in a multi-label setup. We have also included a novel cross-label attention mechanism to explicitly capture dependencies between DNA- and RNA-binding proteins, enabling more accurate prediction of DRBP. Finally, a linear layer followed by a sigmoid function are used for the final prediction. Extensive experiments are carried out to compare LAMP-PRo with the existing methods wherein the proposed model shows consistent competent performance. Furthermore, we also provide visualization to showcase model interpretability, highlighting which parts of the sequence are most relevant for a predicted label. The original datasets are available at this http URL\_MMC and the codes are available at this https URL. 

**Abstract (ZH)**: 识别DNA-结合蛋白(DBPs)和RNA结合蛋白(RBPs)对于理解细胞功能、分子互动以及调控功能至关重要。由于它们的高度相似性，当前大多数方法在区分DBPs和RBPs时面临挑战，导致高交叉预测误差。此外，识别同时结合DNA和RNA的双重结合蛋白(DRBPs)也是一项艰巨的任务。为此，我们提出了一种新的框架LAMP-PRo，该框架基于预训练蛋白质语言模型(PLM)、注意力机制和多标签学习，以减轻这些问题。首先，使用预训练的PLM如ESM-2嵌入蛋白质序列，然后通过卷积神经网络(CNN)。随后应用多头自注意力机制处理上下文信息，同时使用标签感知注意力来通过针对每个标签(DBP、RBP和非NABP)特化的序列计算类特定表示。我们还引入了一种新颖的跨标签注意力机制以明确捕捉DNA-结合蛋白和RNA-结合蛋白之间的依赖性，使DRBP的准确预测更为可能。最后，使用线性层和Sigmoid函数进行最终预测。在与现有方法的广泛实验比较中，提出的模型显示出一致的竞争性能。此外，我们还提供了可视化以展示模型可解释性，突出哪些序列部分对预测标签最为相关。原始数据集可在以下链接下载：this http URL\_MMC，代码可在以下链接获取：this https URL。 

---
# Graph Foundation Models: Bridging Language Model Paradigms and Graph Optimization 

**Title (ZH)**: 图基础模型：连接语言模型范式与图优化的研究 

**Authors**: Yunhao Liang, Pujun Zhang, Yuan Qu, Shaochong Lin, Zuo-jun Max Shen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24256)  

**Abstract**: The pretrain-transfer paradigm, which underpins the success of large language models (LLMs), has demonstrated the immense power of creating foundation models that learn generalizable representations from vast datasets. However, extending this paradigm to Operations Research (OR) problems on graph structures remains challenging due to the fundamental conflict between the statistical flexibility of language and the strict combinatorial constraints of graphs. To bridge this gap, we introduce the Graph Foundation Model (GFM), the first framework capable of solving all distance-based optimization problems on graph structures. By introducing the LLM-like self-supervised pre-training paradigm on the paths generated from random walks in the graph, GFM is compelled to internalize the graph's complex topological and combinatorial rules, where the connectivity of the structure itself can be treated as the supervisory signal. Unlike existing neural methods that learn complex and task-specific solving policies, our approach leverages the pre-trained GFM as a foundational model of the graph's intrinsic structure, which in turn enables a simple generative heuristic to tackle a diverse range of optimization challenges effectively. Comprehensive experiments on networks ranging from 20 to 893 nodes demonstrate that GFM achieves competitive performance against specialized solvers across a variety of distinct optimization task classes, while maintaining significantly faster inference times. Our work establishes a new paradigm of adapting the pretrain-transfer framework to graph optimization, opening the door for applying foundation model innovations to OR. 

**Abstract (ZH)**: Graph Foundation Model: A Pretrain-Transfer Paradigm for Solving Graph-Based Optimization Problems 

---
# Prompt and Parameter Co-Optimization for Large Language Models 

**Title (ZH)**: 大型语言模型的提示与参数共优化 

**Authors**: Xiaohe Bo, Rui Li, Zexu Sun, Quanyu Dai, Zeyu Zhang, Zihang Tian, Xu Chen, Zhenhua Dong  

**Link**: [PDF](https://arxiv.org/pdf/2509.24245)  

**Abstract**: Prompt optimization and fine-tuning are two major approaches to improve the performance of Large Language Models (LLMs). They enhance the capabilities of LLMs from complementary perspectives: the former through explicit natural language, and the latter through implicit parameter updates. However, prior work has typically studied them in isolation, leaving their synergistic potential largely underexplored. To bridge this gap, in this paper, we introduce MetaTuner, a novel framework that jointly integrates prompt optimization and fine-tuning for LLM training. Specifically, we introduce two neural networks to generate prompts and parameters, respectively, while allowing them to share a common bottom encoding layer to enable knowledge sharing. By the guidance of the final supervised signals, our framework is optimized to discover the optimal combinations between the prompts and parameters. Given that prompt learning involves discrete optimization while fine-tuning operates in a continuous parameter space, we design a supervised regularization loss to train our framework effectively. Extensive experiments across diverse benchmarks show that our method consistently outperforms the baselines. 

**Abstract (ZH)**: Prompt优化和微调是提升大规模语言模型（LLMs）性能的两种主要方法。它们从互补的角度增强LLMs的能力：前者通过显式的自然语言，后者通过隐式的参数更新。然而，先前的工作通常将它们分开研究，导致它们的协同潜力未被充分探索。为弥补这一差距，本文提出了一种新型框架MetaTuner，它可以同时整合prompt优化和微调。具体来说，我们引入了两个神经网络分别生成prompt和参数，同时允许它们共享一个底层编码层以实现知识共享。在最终监督信号的引导下，该框架优化以发现prompt和参数的最佳组合。鉴于prompt学习涉及到离散优化而微调操作在连续参数空间中，我们设计了一个监督正则化损失来有效训练该框架。广泛的实验表明，我们的方法在多个基准测试中持续优于基线方法。 

---
# SafeFlowMatcher: Safe and Fast Planning using Flow Matching with Control Barrier Functions 

**Title (ZH)**: SafeFlowMatcher：基于流匹配与控制屏障函数的安全快速规划 

**Authors**: Jeongyong Yang, Seunghwan Jang, Soojean Han  

**Link**: [PDF](https://arxiv.org/pdf/2509.24243)  

**Abstract**: Generative planners based on flow matching (FM) can produce high-quality paths in one or a few ODE steps, but their sampling dynamics offer no formal safety guarantees and can yield incomplete paths near constraints. We present SafeFlowMatcher, a planning framework that couples FM with control barrier functions (CBFs) to achieve both real-time efficiency and certified safety. SafeFlowMatcher uses a two-phase prediction-correction (PC) integrator: (i) a prediction phase integrates the learned FM once (or a few steps) to obtain a candidate path without intervention; (ii) a correction phase refines this path with a vanishing time-scaled vector field and a CBF-based quadratic program that minimally perturbs the vector field. We prove a barrier certificate for the resulting flow system, establishing forward invariance of a robust safe set and finite-time convergence to the safe set. By enforcing safety only on the executed path (rather than on all intermediate latent paths), SafeFlowMatcher avoids distributional drift and mitigates local trap problems. Across maze navigation and locomotion benchmarks, SafeFlowMatcher attains faster, smoother, and safer paths than diffusion- and FM-based baselines. Extensive ablations corroborate the contributions of the PC integrator and the barrier certificate. 

**Abstract (ZH)**: 基于流匹配的生成式规划器结合控制障碍函数的安全流匹配 

---
# ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models 

**Title (ZH)**: ChessArena：评估大型语言模型战略推理能力的象棋测试平台 

**Authors**: Jincheng Liu, Sijun He, Jingjing Wu, Xiangsen Wang, Yang Chen, Zhaoqi Kuang, Siqi Bao, Yuan Yao  

**Link**: [PDF](https://arxiv.org/pdf/2509.24239)  

**Abstract**: Recent large language models (LLMs) have shown strong reasoning capabilities. However, a critical question remains: do these models possess genuine reasoning skills particularly complex strategic reasoning or are they primarily excelling at sophisticated pattern recognition within their training data? To address this question, this paper presents a chess testbed, ChessArena, to evaluate the strategic reasoning capabilities of LLMs. Chess requires complex strategic reasoning capabilities including long-term planning, strict rule comprehension, and multi-turn conversation memorization. Specifically, ChessArena is a competitive framework where LLMs play against each other, under four different play modes. The testbed is equipped with a ranking algorithm and a leaderboard. The testbed can also evaluate fine-grained capabilities including basic understanding, move selection, and puzzle solving. Over 13 LLMs with different modes are evaluated in ChessArena, playing over 800 games. The results reveal significant shortcomings in current LLMs: no model can beat Maia-1100 (a chess engine at human amateur level), while some even failed to defeat a random player that selects moves arbitrarily. We also present a strong baseline to the testbed: our fine-tuned Qwen3-8B substantially improved performance, approaching much larger state-of-the-art reasoning models. 

**Abstract (ZH)**: Recent大型语言模型（LLMs）展示了强大的推理能力。然而，一个关键问题是：这些模型是否具有真正的推理技能，特别是复杂的策略推理能力，还是主要在训练数据中的高级模式识别方面表现出色？为了解决这一问题，本文提出了一个国际象棋测试平台ChessArena，以评估LLMs的策略推理能力。国际象棋要求复杂的策略推理能力，包括长期规划、严格规则理解以及多轮对话记忆。具体来说，ChessArena是一个竞争性框架，其中LLMs在四种不同的对弈模式下相互对弈。该测试平台配备了一种排名算法和排行榜。测试平台还可以评估细粒度的能力，包括基本理解、落子选择和解谜能力。在ChessArena中，13种不同模式的大型语言模型进行了超过800场比赛。结果揭示了当前大型语言模型的重大缺陷：没有一个模型能够战胜Maia-1100（一个达到人类业余水平的国际象棋引擎），而有些模型甚至无法击败一个任意选择走法的随机玩家。我们还为该测试平台提供了一个强大的基准：我们微调的Qwen3-8B显著提高了性能，接近当前最先进的推理模型。 

---
# Uni-NTFM: A Unified Foundation Model for EEG Signal Representation Learning 

**Title (ZH)**: 统一的脑电波信号表示学习基础模型：Uni-NTFM 

**Authors**: Zhisheng Chen, Yingwei Zhang, Qizhen Lan, Tianyu Liu, Huacan Wang, Yi Ding, Ziyu Jia, Ronghao Chen, Kun Wang, Xinliang Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2509.24222)  

**Abstract**: Foundation models pretrained on various and unlabeled data have demonstrated significant success in natural language and vision, but their application to electroencephalography (EEG) remains challenged due to the signal's unique properties. Existing brain foundation models that inherit architectures designed for text or images lead to three limitations in pre-training: 1) conflating time-domain waveform patterns with frequency-domain rhythmic features in a single processing stream, 2) ignoring the critical spatial topology of electrodes with different standards, and 3) reliance on the inflexible, dense network to process functionally distinct EEG patterns. To address these challenges, we introduce the Unified Neural Topological Foundation Model (Uni-NTFM), which is designed based on neuroscience principles to produce universal and interpretable representations. Uni-NTFM integrates three core innovations: 1) a decoupled architecture parallelly encodes time, frequency, and raw signal representations before performing cross-domain feature integration; 2) a topological embedding mechanism to unify electrodes from different international standards and generate structured input sequences for brain regions; and 3) a Mixture-of-Experts neural Transformer that efficiently scales model capacity by routing signal patterns to specialized subnetworks. The largest model, Uni-NTFM$_{large}$, has a record-breaking 1.9B parameters and was pretrained on over 28,000 hours of diverse EEG data via a dual-domain masked reconstruction objective. Uni-NTFM significantly outperforms existing task-specific methods and foundation models across nine distinct downstream tasks under both linear probing and fine-tuning settings, demonstrating a superior ability to learn universal representations of brain activity. 

**Abstract (ZH)**: 基于统一神经拓扑基础模型在脑电信号中的通用和可解释表示 

---
# ViReSkill: Vision-Grounded Replanning with Skill Memory for LLM-Based Planning in Lifelong Robot Learning 

**Title (ZH)**: ViReSkill: 基于视觉的技能记忆重规划方法在终身机器人学习中面向LLM的规划 

**Authors**: Tomoyuki Kagaya, Subramanian Lakshmi, Anbang Ye, Thong Jing Yuan, Jayashree Karlekar, Sugiri Pranata, Natsuki Murakami, Akira Kinose, Yang You  

**Link**: [PDF](https://arxiv.org/pdf/2509.24219)  

**Abstract**: Robots trained via Reinforcement Learning (RL) or Imitation Learning (IL) often adapt slowly to new tasks, whereas recent Large Language Models (LLMs) and Vision-Language Models (VLMs) promise knowledge-rich planning from minimal data. Deploying LLMs/VLMs for motion planning, however, faces two key obstacles: (i) symbolic plans are rarely grounded in scene geometry and object physics, and (ii) model outputs can vary for identical prompts, undermining execution reliability. We propose ViReSkill, a framework that pairs vision-grounded replanning with a skill memory for accumulation and reuse. When a failure occurs, the replanner generates a new action sequence conditioned on the current scene, tailored to the observed state. On success, the executed plan is stored as a reusable skill and replayed in future encounters without additional calls to LLMs/VLMs. This feedback loop enables autonomous continual learning: each attempt immediately expands the skill set and stabilizes subsequent executions. We evaluate ViReSkill on simulators such as LIBERO and RLBench as well as on a physical robot. Across all settings, it consistently outperforms conventional baselines in task success rate, demonstrating robust sim-to-real generalization. 

**Abstract (ZH)**: 基于视觉的重规划与技能记忆结合的持续学习框架：ViReSkill 

---
# Conda: Column-Normalized Adam for Training Large Language Models Faster 

**Title (ZH)**: Conda: 列归一化Adam算法用于训练大型语言模型加速 

**Authors**: Junjie Wang, Pan Zhou, Yiming Dong, Huan Li, Jia Li, Xun Zhou, Qicheng Lao, Cong Fang, Zhouchen Lin  

**Link**: [PDF](https://arxiv.org/pdf/2509.24218)  

**Abstract**: Large language models (LLMs) have demonstrated impressive generalization and emergent capabilities, yet their pre-training remains computationally expensive and sensitive to optimization dynamics. While Adam-based optimizers offer fast convergence by adapting learning rates coordinate-wise, recent studies reveal that their updates often suffer from poor spectral conditioning and low-rank structures, hindering efficiency. Muon addresses this issue via global spectral normalization but lacks the per-coordinate adaptivity of Adam. In this work, we propose \textbf{Column-Normalized Adam (Conda)}, a novel optimizer that bridges the strengths of both approaches. Conda projects updates into an orthogonal subspace and applies column-wise second moment normalization based on the projected gradients, thereby achieving both improved spectral conditioning and maintaining coordinate-wise adaptivity. This design alleviates the spectral pathologies of Adam while preserving its fast convergence behavior. Extensive experiments on the LLaMA and GPT-2 series show that Conda consistently outperforms AdamW, Muon, and other baselines in pre-training. Remarkably, on the LLaMA series, \textbf{Conda achieves $2{\sim}2.5\times$ the convergence speed of AdamW, measured in both training steps and training time.} Further ablations demonstrate its robustness under diverse training setups. These results collectively highlight Conda as an effective and broadly applicable optimizer for large-scale LLM training. The code is released on this https URL 

**Abstract (ZH)**: 大规模语言模型（LLMs）展示了令人印象深刻的泛化能力和涌现能力，但其预训练仍具有较高的计算成本，并且对优化动态敏感。尽管基于Adam的优化器通过协调调整学习率实现了快速收敛，但 recent 研究表明，它们的更新往往遭受谱条件差和低秩结构的困扰，影响效率。Muon 通过全局谱正则化解决了这一问题，但缺乏 Adam 的坐标适配性。在这项工作中，我们提出了一种新的优化器——列归一化 Adam（Conda），它结合了两种方法的优势。Conda 将更新投影到正交子空间，并基于投影梯度应用列-wise 的二阶矩归一化，从而实现改进的谱条件并保持坐标适配性。这一设计缓解了 Adam 的谱病理现象，同时保留了其快速收敛的行为。在 LLaMA 和 GPT-2 系列的广泛实验中，Conda 一贯优于 AdamW、Muon 及其他基线。特别是在 LLaMA 系列中，Conda 在训练步数和训练时间上将 AdamW 的收敛速度提高了 2 到 2.5 倍。进一步的消融实验证明了其在不同训练设置下的鲁棒性。这些结果共同展示了 Conda 是大规模 LLM 训练中一个有效且广泛应用的优化器。代码发布在 <https://>。 

---
# Metamorphic Testing for Audio Content Moderation Software 

**Title (ZH)**: 音频内容审核软件的 metamorphic 测试 

**Authors**: Wenxuan Wang, Yongjiang Wu, Junyuan Zhang, Shuqing Li, Yun Peng, Wenting Chen, Shuai Wang, Michael R. Lyu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24215)  

**Abstract**: The rapid growth of audio-centric platforms and applications such as WhatsApp and Twitter has transformed the way people communicate and share audio content in modern society. However, these platforms are increasingly misused to disseminate harmful audio content, such as hate speech, deceptive advertisements, and explicit material, which can have significant negative consequences (e.g., detrimental effects on mental health). In response, researchers and practitioners have been actively developing and deploying audio content moderation tools to tackle this issue. Despite these efforts, malicious actors can bypass moderation systems by making subtle alterations to audio content, such as modifying pitch or inserting noise. Moreover, the effectiveness of modern audio moderation tools against such adversarial inputs remains insufficiently studied. To address these challenges, we propose MTAM, a Metamorphic Testing framework for Audio content Moderation software. Specifically, we conduct a pilot study on 2000 audio clips and define 14 metamorphic relations across two perturbation categories: Audio Features-Based and Heuristic perturbations. MTAM applies these metamorphic relations to toxic audio content to generate test cases that remain harmful while being more likely to evade detection. In our evaluation, we employ MTAM to test five commercial textual content moderation software and an academic model against three kinds of toxic content. The results show that MTAM achieves up to 38.6%, 18.3%, 35.1%, 16.7%, and 51.1% error finding rates (EFR) when testing commercial moderation software provided by Gladia, Assembly AI, Baidu, Nextdata, and Tencent, respectively, and it obtains up to 45.7% EFR when testing the state-of-the-art algorithms from the academy. 

**Abstract (ZH)**: 音频中心平台和应用（如WhatsApp和Twitter）的快速增长已改变人们在现代社会中进行音频内容交流和分享的方式。然而，这些平台正越来越多地被滥用以传播有害音频内容，如仇恨言论、欺骗性广告和露骨材料，这可能产生严重的负面影响（例如对心理健康造成损害）。针对这一问题，研究人员和实践者正在积极开发和部署音频内容审核工具。尽管如此，恶意行为者可以通过对音频内容进行细微修改（如修改音调或插入噪音）来规避审核系统，而且现代音频审核工具对这些对抗性输入的有效性研究仍不够充分。为应对这些挑战，我们提出了一种名为MTAM的音频内容审核软件的变形测试框架。具体而言，我们在2000个音频片段上进行试点研究，并定义了跨越两类扰动分类（基于音频特征和启发式扰动）的14种变形关系。MTAM应用这些变形关系对有毒音频内容进行测试，生成更有可能规避检测但仍保持有害性的测试案例。在评估中，我们使用MTAM对五款商用文本内容审核软件和一款学术模型进行了测试，针对三种类型的有毒内容。结果显示，MTAM分别在由Gladia、Assembly AI、Baidu、Nextdata和Tencent提供的商业审核软件中实现了多达38.6%、18.3%、35.1%、16.7%和51.1%的错误发现率（EFR），并在学术界的最新算法测试中实现了高达45.7%的错误发现率。 

---
# BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models 

**Title (ZH)**: BeyondBench: 无需基准的语义模型推理评估 

**Authors**: Gaurav Srivastava, Aafiya Hussain, Zhenyu Bi, Swastik Roy, Priya Pitre, Meng Lu, Morteza Ziyadi, Xuan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24210)  

**Abstract**: Evaluating language models fairly is becoming harder as static benchmarks available on the internet risk contamination by training data. This makes it unclear whether models are truly reasoning or just recalling answers. In this paper, we introduce BeyondBench, an evaluation framework that avoids this problem by using algorithmic problem generation. Unlike traditional benchmarks that risk contamination from internet-scale training data, BeyondBench creates mathematically grounded problems on the fly, ensuring each test remains fresh and uncontaminated. Our framework covers 44 algorithmic tasks with a total of 117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks) for basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations) for sequence patterns and reasoning, and the Hard Suite (10 tasks, 68 variations) tackling NP-complete and constraint satisfaction problems. Each task generates problems from a combinatorial space larger than 10^15 unique instances, with solutions verified deterministically by mathematical proofs. We evaluated 101 language models, including 85 open-source and 16 closed-source models, spanning sizes from 0.5B to 141B parameters and multiple quantization schemes. Our results show consistent reasoning deficiencies across model families, with performance degrading sharply as problem complexity increases from polynomial to exponential. In our Hard Suite evaluations, models such as Gemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of 56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance drops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano showing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our leaderboard is publicly available at this https URL 

**Abstract (ZH)**: 公平评估语言模型变得越来越困难，因为互联网上可用的静态基准可能会受到训练数据的污染。这使得很难确定模型是真正推理还是仅仅是回忆答案。在这种情况下，我们提出了BeyondBench，这是一种通过使用算法问题生成来避免这个问题的评估框架。与传统基准可能受到互联网规模训练数据污染的情况不同，BeyondBench 实时生成数学上基于ground的问题，确保每个测试始终保持新鲜和未被污染。我们的框架涵盖了44项算法任务，共计117种变体，并分为三个难度级别：Easy Suite（29项任务）涵盖基本的算术和统计，Medium Suite（5项任务，49种变体）涵盖序列模式和推理，Hard Suite（10项任务，68种变体）解决NP完全和约束满足问题。每个任务生成的问题实例空间超过 \(10^{15}\) 种独特实例，通过数学证明确定性地验证解决方案。我们评估了101个语言模型，包括85个开源和16个闭源模型，涵盖从0.5B到141B参数以及多种量化方案。结果显示，随着问题复杂性的增加，从多项式到指数级别，模型家族的一致推理能力存在缺陷。在我们的Hard Suite评估中，Gemini-2.5-pro、Llama-3.3-70B和Qwen2.5-72B的平均准确率分别为56.38%、26.91%和33.60%。此外，我们观察到，在不使用工具的情况下，性能大幅下降，GPT-5、GPT-5-mini和GPT-5-nano在Hard Suite中的准确率分别下降了16.81%、28.05%和47.59%。我们的排行榜可以在以下网址查看。 

---
# BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for Resource-Efficient Medical Image Segmentation 

**Title (ZH)**: 边界感知低秩适应SAM资源高效医疗图像分割 

**Authors**: Zelin Liu, Sicheng Dong, Bocheng Li, Yixuan Yang, Jiacheng Ruan, Chenxu Zhou, Suncheng Xiang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24204)  

**Abstract**: Vision foundation models like the Segment Anything Model (SAM), pretrained on large-scale natural image datasets, often struggle in medical image segmentation due to a lack of domain-specific adaptation. In clinical practice, fine-tuning such models efficiently for medical downstream tasks with minimal resource demands, while maintaining strong performance, is challenging. To address these issues, we propose BALR-SAM, a boundary-aware low-rank adaptation framework that enhances SAM for medical imaging. It combines three tailored components: (1) a Complementary Detail Enhancement Network (CDEN) using depthwise separable convolutions and multi-scale fusion to capture boundary-sensitive features essential for accurate segmentation; (2) low-rank adapters integrated into SAM's Vision Transformer blocks to optimize feature representation and attention for medical contexts, while simultaneously significantly reducing the parameter space; and (3) a low-rank tensor attention mechanism in the mask decoder, cutting memory usage by 75% and boosting inference speed. Experiments on standard medical segmentation datasets show that BALR-SAM, without requiring prompts, outperforms several state-of-the-art (SOTA) methods, including fully fine-tuned MedSAM, while updating just 1.8% (11.7M) of its parameters. 

**Abstract (ZH)**: 边界感知低秩适配框架BALR-SAM：一种用于医学影像分割的Segment Anything Model增强方法 

---
# Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends 

**Title (ZH)**: Group-相对REINFORCE实际上是Off-Policy算法：揭开GRPO及其伙伴的一些迷思 

**Authors**: Chaorui Yao, Yanxi Chen, Yuchang Sun, Yushuo Chen, Wenhao Zhang, Xuchen Pan, Yaliang Li, Bolin Ding  

**Link**: [PDF](https://arxiv.org/pdf/2509.24203)  

**Abstract**: Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at this https URL. 

**Abstract (ZH)**: 大规模语言模型（LLMs）的离策强化学习（RL）正 attracting growing interest 

---
# Can Large Language Models Express Uncertainty Like Human? 

**Title (ZH)**: 大型语言模型能否像人类一样表达不确定性？ 

**Authors**: Linwei Tao, Yi-Fan Yeh, Bo Kai, Minjing Dong, Tao Huang, Tom A. Lamb, Jialin Yu, Philip H.S. Torr, Chang Xu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24202)  

**Abstract**: Large language models (LLMs) are increasingly used in high-stakes settings, where overconfident responses can mislead users. Reliable confidence estimation has been shown to enhance trust and task accuracy. Yet existing methods face practical barriers: logits are often hidden, multi-sampling is computationally expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score) deviates from natural communication. We revisit linguistic confidence (LC), where models express uncertainty through hedging language (e.g., probably, might), offering a lightweight and human-centered alternative. To advance this direction, we (1) release the first diverse, large-scale dataset of hedging expressions with human-annotated confidence scores, and (2) propose a lightweight mapper that converts hedges into confidence scores at near-zero cost. Building on these resources, we (3) conduct the first systematic study of LC across modern LLMs and QA benchmarks, revealing that while most LLMs underperform in expressing reliable LC, carefully designed prompting achieves competitive calibration and discriminability. Finally, we (4) introduce a fine-tuning framework that further improves LC reliability. Taken together, our work positions linguistic confidence as a scalable, efficient, and human-aligned approach to LLM uncertainty estimation, and calls for deeper exploration of this promising yet underexplored direction. 

**Abstract (ZH)**: 大型语言模型（LLMs）在高 stakes 环境中日益增多，过于自信的回复可能会误导用户。可靠的信心估计已经被证明能够增强信任和任务准确性。然而，现有的方法面临实际障碍：logits 经常被隐藏，多采样计算成本高昂，而口头化的数值不确定性（例如，给出 0-100 分数）偏离自然沟通。我们重新审视语言信心（LC），其中模型通过含糊其辞的语言（例如，可能、也许）来表达不确定性，提供了一种轻量级且用户导向的替代方案。为进一步推动这一方向，我们（1）发布了第一个具有人类标注信心分数的多样化大规模含糊表达数据集，（2）提出了一种轻量级映射器，以接近零的成本将含糊表达转换为信心分数。基于这些资源，我们（3）首次系统研究了现代 LLMs 和 QA 基准上的 LC，揭示了尽管大多数 LLMs 在表达可靠 LC 方面表现不佳，但精心设计的提示可以实现具有竞争力的校准和辨别能力。最后，我们（4）引入了一种微调框架，以进一步提高 LC 的可靠性。综上所述，我们的工作将语言信心定位为一种可扩展、高效且与人类对齐的方法，用于 LLM 不确定性的估计，并呼吁对这一有前途但尚未充分探索的方向进行更深入的研究。 

---
# Chat to Chip: Large Language Model Based Design of Arbitrarily Shaped Metasurfaces 

**Title (ZH)**: Chat to Chip: 基于大型语言模型的任意形状元表面设计 

**Authors**: Huanshu Zhang, Lei Kang, Sawyer D. Campbell, Douglas H. Werner  

**Link**: [PDF](https://arxiv.org/pdf/2509.24196)  

**Abstract**: Traditional metasurface design is limited by the computational cost of full-wave simulations, preventing thorough exploration of complex configurations. Data-driven approaches have emerged as a solution to this bottleneck, replacing costly simulations with rapid neural network evaluations and enabling near-instant design for meta-atoms. Despite advances, implementing a new optical function still requires building and training a task-specific network, along with exhaustive searches for suitable architectures and hyperparameters. Pre-trained large language models (LLMs), by contrast, sidestep this laborious process with a simple fine-tuning technique. However, applying LLMs to the design of nanophotonic devices, particularly for arbitrarily shaped metasurfaces, is still in its early stages; as such tasks often require graphical networks. Here, we show that an LLM, fed with descriptive inputs of arbitrarily shaped metasurface geometries, can learn the physical relationships needed for spectral prediction and inverse design. We further benchmarked a range of open-weight LLMs and identified relationships between accuracy and model size at the billion-parameter level. We demonstrated that 1-D token-wise LLMs provide a practical tool to designing 2-D arbitrarily shaped metasurfaces. Linking natural-language interaction to electromagnetic modelling, this "chat-to-chip" workflow represents a step toward more user-friendly data-driven nanophotonics. 

**Abstract (ZH)**: 基于大规模语言模型的任意形状超表面设计：从自然语言到电磁建模的步骤迈向用户友好的数据驱动纳米光子学 

---
# AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play 

**Title (ZH)**: AceSearcher: 通过强化自我博弈 bootstrap 原理和搜索能力提升大语言模型 

**Authors**: Ran Xu, Yuchen Zhuang, Zihan Dong, Jonathan Wang, Yue Yu, Joyce C. Ho, Linjun Zhang, Haoyu Wang, Wenqi Shi, Carl Yang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24193)  

**Abstract**: Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at this https URL and this https URL. 

**Abstract (ZH)**: Search增强的大语言模型 often 在复杂推理任务中遇到挑战，因其多跳检索效果不佳且推理能力有限。我们提出AceSearcher，一种合作自对弈框架，通过训练单个大型语言模型（LLM）在分解查询和生成答案之间交替扮演两种角色，从而提升其在复杂推理任务中的表现。AceSearcher 结合了多样化搜索、推理和分解任务的监督微调及旨在提高最终答案准确性的强化微调，消除了中间标注的需求。在三个推理密集型任务的广泛实验中，使用10个数据集，AceSearcher 显著优于最先进的基线模型，平均精确匹配率提高7.6%。特别是在文档级金融推理任务中，AceSearcher-32B 使用不到5%的参数便达到了与DeepSeek-V3相当的性能。即使在较小规模（1.5B和8B参数）下，AceSearcher 也经常超越具有9倍更多参数的现有搜索增强大语言模型，突显了其在复杂推理任务中卓越的效率和效果。我们的代码将发布在以下链接：this https URL和this https URL。 

---
# Talk in Pieces, See in Whole: Disentangling and Hierarchical Aggregating Representations for Language-based Object Detection 

**Title (ZH)**: 破碎片段之谈，观整体之象：基于语言的对象检测中表示的解耦与分层聚合 

**Authors**: Sojung An, Kwanyong Park, Yong Jae Lee, Donghyun Kim  

**Link**: [PDF](https://arxiv.org/pdf/2509.24192)  

**Abstract**: While vision-language models (VLMs) have made significant progress in multimodal perception (e.g., open-vocabulary object detection) with simple language queries, state-of-the-art VLMs still show limited ability to perceive complex queries involving descriptive attributes and relational clauses. Our in-depth analysis shows that these limitations mainly stem from text encoders in VLMs. Such text encoders behave like bags-of-words and fail to separate target objects from their descriptive attributes and relations in complex queries, resulting in frequent false positives. To address this, we propose restructuring linguistic representations according to the hierarchical relations within sentences for language-based object detection. A key insight is the necessity of disentangling textual tokens into core components-objects, attributes, and relations ("talk in pieces")-and subsequently aggregating them into hierarchically structured sentence-level representations ("see in whole"). Building on this principle, we introduce the TaSe framework with three main contributions: (1) a hierarchical synthetic captioning dataset spanning three tiers from category names to descriptive sentences; (2) Talk in Pieces, the three-component disentanglement module guided by a novel disentanglement loss function, transforms text embeddings into subspace compositions; and (3) See in Whole, which learns to aggregate disentangled components into hierarchically structured embeddings with the guide of proposed hierarchical objectives. The proposed TaSe framework strengthens the inductive bias of hierarchical linguistic structures, resulting in fine-grained multimodal representations for language-based object detection. Experimental results under the OmniLabel benchmark show a 24% performance improvement, demonstrating the importance of linguistic compositionality. 

**Abstract (ZH)**: 基于语言的物体检测中视觉-语言模型的层次化语言表示重构 

---
# Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models 

**Title (ZH)**: 超越总体准确率：对80个大型语言模型在特定主题医疗能力上的心理测量深度探究 

**Authors**: Zhimeng Luo, Lixin Wu, Adam Frisch, Daqing He  

**Link**: [PDF](https://arxiv.org/pdf/2509.24186)  

**Abstract**: As Large Language Models (LLMs) are increasingly proposed for high-stakes medical applications, there has emerged a critical need for reliable and accurate evaluation methodologies. Traditional accuracy metrics fail inadequately as they neither capture question characteristics nor offer topic-specific insights. To address this gap, we introduce \textsc{MedIRT}, a rigorous evaluation framework grounded in Item Response Theory (IRT), the gold standard in high-stakes educational testing. Unlike previous research relying on archival data, we prospectively gathered fresh responses from 80 diverse LLMs on a balanced, 1,100-question USMLE-aligned benchmark. Using one unidimensional two-parameter logistic IRT model per topic, we estimate LLM's latent model ability jointly with question difficulty and discrimination, yielding more stable and nuanced performance rankings than accuracy alone. Notably, we identify distinctive ``spiky'' ability profiles, where overall rankings can be misleading due to highly specialized model abilities. While \texttt{GPT-5} was the top performer in a majority of domains (8 of 11), it was outperformed in Social Science and Communication by \texttt{Claude-3-opus}, demonstrating that even an overall 23rd-ranked model can hold the top spot for specific competencies. Furthermore, we demonstrate IRT's utility in auditing benchmarks by identifying flawed questions. We synthesize these findings into a practical decision-support framework that integrates our multi-factor competency profiles with operational metrics. This work establishes a robust, psychometrically grounded methodology essential for the safe, effective, and trustworthy deployment of LLMs in healthcare. 

**Abstract (ZH)**: 大型语言模型（LLMs）在高风险医疗应用中的日益普及引发了对其可靠性和准确性的评估需求。传统的准确度指标在评估LLMs时表现出明显的不足，因为它们未能捕捉问题特性或提供专题洞见。为解决这一问题，我们引入了基于项目反应理论（Item Response Theory, IRT）的\textsc{MedIRT}严格评估框架，IRT是高风险教育测试的金标准。不同于之前依赖档案数据的研究，我们前瞻性地收集了80种多样化LLMs在平衡的1100题USMLE对标的基准上的最新回应。使用每个主题的双向参数logistic IRT模型，我们共同估计了LLMs的潜在模型能力、问题难度和区分度，从而比单一准确度指标提供更稳定和细致的性能排名。值得注意的是，我们识别出独特的“尖峰”能力轮廓，整体排名可能因高度专业化的模型能力而误导。尽管\texttt{GPT-5}在大多数领域（11个中的8个）表现最佳，但在社会科学和沟通方面却输给了\texttt{Claude-3-opus}，这表明即使是综合排名最低的模型也有可能在特定能力方面名列前茅。此外，我们展示了IRT在审核基准中的应用，通过识别有问题的题目来提高基准质量。我们将这些发现整合成一个实用的决策支持框架，该框架结合了我们的多因素能力概况与操作性指标。本研究确立了一种稳健且心理测量学上站得住脚的方法论，对于在医疗保健中安全、有效和可信地部署LLMs至关重要。 

---
# Retrieval-augmented GUI Agents with Generative Guidelines 

**Title (ZH)**: 基于检索增强的GUI代理与生成性准则 

**Authors**: Ran Xu, Kaixin Ma, Wenhao Yu, Hongming Zhang, Joyce C. Ho, Carl Yang, Dong Yu  

**Link**: [PDF](https://arxiv.org/pdf/2509.24183)  

**Abstract**: GUI agents powered by vision-language models (VLMs) show promise in automating complex digital tasks. However, their effectiveness in real-world applications is often limited by scarce training data and the inherent complexity of these tasks, which frequently require long-tailed knowledge covering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that leverages web tutorials at inference time. RAG-GUI is first warm-started via supervised finetuning (SFT) and further refined through self-guided rejection sampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as a generic plug-in that enhances any VLM-based agent. Evaluated across three distinct tasks, it consistently outperforms baseline agents and surpasses other inference baselines by 2.6% to 13.3% across two model sizes, demonstrating strong generalization and practical plug-and-play capabilities in real-world scenarios. 

**Abstract (ZH)**: 由视觉-语言模型驱动的GUI代理显示了自动化复杂数字任务的潜力。然而，它们在实际应用中的有效性常常受到稀缺训练数据和这些任务固有复杂性的限制，这些任务通常需要涵盖罕见未见情景的长尾知识。我们提出了一种轻量级视觉-语言模型RAG-GUI，在推理时利用网页教程。RAG-GUI首先通过监督微调(SFT)预热，并进一步通过自我指导式拒绝采样微调(RSF)进行优化。设计为模型无关的，RAG-GUI作为通用插件增强任何基于视觉-语言模型的代理。在三个不同的任务中进行了评估，它在两个模型大小下分别优于基线代理2.6%至13.3%，展示了强大的泛化能力和实际插拔即用能力。 

---
# Stable Forgetting: Bounded Parameter-Efficient Unlearning in LLMs 

**Title (ZH)**: 稳定遗忘：LLM中的有限参数高效遗忘 

**Authors**: Arpit Garg, Hemanth Saratchandran, Ravi Garg, Simon Lucey  

**Link**: [PDF](https://arxiv.org/pdf/2509.24166)  

**Abstract**: Machine unlearning in large language models (LLMs) is essential for privacy and safety; however, existing approaches remain unstable and unreliable. A widely used strategy, the gradient difference method, applies gradient descent on retained data while performing gradient ascent on forget data, the data whose influence should be removed. However, when combined with cross-entropy loss, this procedure causes unbounded growth of weights and gradients, leading to training instability and degrading both forgetting and retention. We provide a theoretical framework that explains this failure, explicitly showing how ascent on the forget set destabilizes optimization in the feedforward MLP layers of LLMs. Guided by this insight, we propose Bounded Parameter-Efficient Unlearning, a parameter-efficient approach that stabilizes LoRA-based fine-tuning by applying bounded functions to MLP adapters. This simple modification controls the weight dynamics during ascent, enabling the gradient difference method to converge reliably. Across the TOFU, TDEC, and MUSE benchmarks, and across architectures and scales from 125M to 8B parameters, our method achieves substantial improvements in forgetting while preserving retention, establishing a novel theoretically grounded and practically scalable framework for unlearning in LLMs. 

**Abstract (ZH)**: 大型语言模型（LLMs）中的机器遗忘对于隐私和安全至关重要；然而，现有的方法仍然不稳定且不可靠。广泛使用的梯度差分方法在保留数据上应用梯度下降，在忘记数据（其影响应被移除的数据）上应用梯度上升。然而，当与交叉熵损失结合使用时，此过程会导致权重和梯度的无界增长，从而导致训练不稳定并恶化遗忘和保留效果。我们提供了一个理论框架来解释这一失败，明确展示了在LLMs的前向MLP层中，对于忘记集的上升操作如何导致优化的失稳。基于这一洞察，我们提出了有界参数高效遗忘方法，这是一种通过在MLP适配器上应用有界函数来稳定LoRA基微调的参数高效方法。这一简单修改控制了上升过程中的权重动态，使得梯度差分方法能够可靠收敛。在TOFU、TDEC和MUSE基准上，以及从125M到8B参数的各种架构和规模下，我们的方法在遗忘方面取得了显著改进，同时保留了保留效果，从而建立了用于LLMs遗忘的新型理论基础和实际可扩展框架。 

---
# LatXGen: Towards Radiation-Free and Accurate Quantitative Analysis of Sagittal Spinal Alignment Via Cross-Modal Radiographic View Synthesis 

**Title (ZH)**: LatXGen：用于冠状脊柱对齐跨模态放射学视图合成的辐射-free 和精确定量分析方法 

**Authors**: Moxin Zhao, Nan Meng, Jason Pui Yin Cheung, Chris Yuk Kwan Tang, Chenxi Yu, Wenting Zhong, Pengyu Lu, Chang Shi, Yipeng Zhuang, Teng Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24165)  

**Abstract**: Adolescent Idiopathic Scoliosis (AIS) is a complex three-dimensional spinal deformity, and accurate morphological assessment requires evaluating both coronal and sagittal alignment. While previous research has made significant progress in developing radiation-free methods for coronal plane assessment, reliable and accurate evaluation of sagittal alignment without ionizing radiation remains largely underexplored. To address this gap, we propose LatXGen, a novel generative framework that synthesizes realistic lateral spinal radiographs from posterior Red-Green-Blue and Depth (RGBD) images of unclothed backs. This enables accurate, radiation-free estimation of sagittal spinal alignment. LatXGen tackles two core challenges: (1) inferring sagittal spinal morphology changes from a lateral perspective based on posteroanterior surface geometry, and (2) performing cross-modality translation from RGBD input to the radiographic domain. The framework adopts a dual-stage architecture that progressively estimates lateral spinal structure and synthesizes corresponding radiographs. To enhance anatomical consistency, we introduce an attention-based Fast Fourier Convolution (FFC) module for integrating anatomical features from RGBD images and 3D landmarks, and a Spatial Deformation Network (SDN) to model morphological variations in the lateral view. Additionally, we construct the first large-scale paired dataset for this task, comprising 3,264 RGBD and lateral radiograph pairs. Experimental results demonstrate that LatXGen produces anatomically accurate radiographs and outperforms existing GAN-based methods in both visual fidelity and quantitative metrics. This study offers a promising, radiation-free solution for sagittal spine assessment and advances comprehensive AIS evaluation. 

**Abstract (ZH)**: 青少年特发性脊柱侧弯（AIS）是一种复杂的三维脊柱畸形，准确的形态评估需要同时评估冠状面和冠状轴对齐情况。尽管先前研究在无辐射冠状面评估方法的研发方面取得了显著进展，但在不使用电离辐射的情况下可靠且准确地评估冠状轴对齐情况仍鲜有探索。为填补这一空白，我们提出了LatXGen，这是一种新颖的生成框架，能够从未经穿着衣物背部的后前向红绿蓝和深度（RGBD）图像中合成逼真的侧位脊柱X光片。这使得能够无辐射地准确估计冠状轴对齐情况。LatXGen 应对了两大核心挑战：（1）基于后前向表面几何结构从侧位视角推断冠状脊柱形态变化，（2）从 RGBD 输入跨模态转换到放射学领域。该框架采用双重架构，逐步估算侧位脊柱结构并生成相应的X光片。为了增强解剖一致性，我们引入了基于注意力的快速傅里叶卷积（FFC）模块，用于整合RGBD图像和三维标志点的解剖特征，并采用空间变形网络（SDN）来建模侧位视图中的形态变化。此外，我们构建了首个用于该任务的大规模配对数据集，包含了3,264对RGBD和侧位X光片。实验结果表明，LatXGen生成了解剖学上准确的X光片，并在视觉保真度和定量指标方面显著优于现有基于生成对抗网络（GAN）的方法。这一研究提供了一种有前景的无辐射解决方案，用于评估冠状脊柱，并推动了全面的青少年特发性脊柱侧弯评估。 

---
# Memory Transfer Planning: LLM-driven Context-Aware Code Adaptation for Robot Manipulation 

**Title (ZH)**: 记忆传输规划：基于LLM的认知适应代码优化用于机器人操作 

**Authors**: Tomoyuki Kagaya, Subramanian Lakshmi, Yuxuan Lou, Thong Jing Yuan, Jayashree Karlekar, Sugiri Pranata, Natsuki Murakami, Akira Kinose, Yang You  

**Link**: [PDF](https://arxiv.org/pdf/2509.24160)  

**Abstract**: Large language models (LLMs) are increasingly explored in robot manipulation, but many existing methods struggle to adapt to new environments. Many systems require either environment-specific policy training or depend on fixed prompts and single-shot code generation, leading to limited transferability and manual re-tuning. We introduce Memory Transfer Planning (MTP), a framework that leverages successful control-code examples from different environments as procedural knowledge, using them as in-context guidance for LLM-driven planning. Specifically, MTP (i) generates an initial plan and code using LLMs, (ii) retrieves relevant successful examples from a code memory, and (iii) contextually adapts the retrieved code to the target setting for re-planning without updating model parameters. We evaluate MTP on RLBench, CALVIN, and a physical robot, demonstrating effectiveness beyond simulation. Across these settings, MTP consistently improved success rate and adaptability compared with fixed-prompt code generation, naive retrieval, and memory-free re-planning. Furthermore, in hardware experiments, leveraging a memory constructed in simulation proved effective. MTP provides a practical approach that exploits procedural knowledge to realize robust LLM-based planning across diverse robotic manipulation scenarios, enhancing adaptability to novel environments and bridging simulation and real-world deployment. 

**Abstract (ZH)**: 大型语言模型（LLMs）在机器人操作中的应用日益增多，但许多现有方法难以适应新环境。我们引入了记忆迁移规划（MTP）框架，该框架利用来自不同环境的成功控制代码示例作为过程性知识，并将其作为上下文指导，用于LLM驱动的规划。具体来说，MTP（i）使用LLM生成初始计划和代码，（ii）从代码记忆中检索相关成功示例，（iii）在不更新模型参数的情况下，上下文适配检索到的代码以适应目标环境进行重新规划。我们在RLBench、CALVIN和物理机器人上评估了MTP，展示了其在仿真之外的有效性。在这些设置中，MTP始终在成功率和适应性方面优于固定提示代码生成、简单检索和无记忆重新规划。此外，在硬件实验中，利用仿真中构建的记忆证明是有效的。MTP提供了一种实用的方法，利用过程性知识在多种机器人操作场景中实现稳健的LLM基规划，增强对新环境的适应性和连接仿真与现实世界部署。 

---
# Accelerating Cerebral Diagnostics with BrainFusion: A Comprehensive MRI Tumor Framework 

**Title (ZH)**: 基于BrainFusion的全面MRI肿瘤框架加速脑部诊断 

**Authors**: Walid Houmaidi, Youssef Sabiri, Salmane El Mansour Billah, Amine Abouaomar  

**Link**: [PDF](https://arxiv.org/pdf/2509.24149)  

**Abstract**: The early and accurate classification of brain tumors is crucial for guiding effective treatment strategies and improving patient outcomes. This study presents BrainFusion, a significant advancement in brain tumor analysis using magnetic resonance imaging (MRI) by combining fine-tuned convolutional neural networks (CNNs) for tumor classification--including VGG16, ResNet50, and Xception--with YOLOv8 for precise tumor localization with bounding boxes. Leveraging the Brain Tumor MRI Dataset, our experiments reveal that the fine-tuned VGG16 model achieves test accuracy of 99.86%, substantially exceeding previous benchmarks. Beyond setting a new accuracy standard, the integration of bounding-box localization and explainable AI techniques further enhances both the clinical interpretability and trustworthiness of the system's outputs. Overall, this approach underscores the transformative potential of deep learning in delivering faster, more reliable diagnoses, ultimately contributing to improved patient care and survival rates. 

**Abstract (ZH)**: 脑肿瘤的早期和准确分类对于指导有效的治疗策略和改善患者预后至关重要。本研究提出BrainFusion，这是一种通过结合微调的卷积神经网络（包括VGG16、ResNet50和Xception）与YOLOv8进行精准肿瘤定位的磁共振成像（MRI）脑肿瘤分析的显著进展。利用Brain Tumor MRI数据集，我们的实验表明微调的VGG16模型在测试集上的准确率为99.86%，显著超过先前的标准。此外，结合边界框定位和可解释AI技术进一步增强了系统输出的临床可解释性和可靠性。总体而言，该方法强调了深度学习在实现更快、更可靠诊断方面的变革潜力，最终有助于改善患者的护理质量和生存率。 

---
# TENET: Leveraging Tests Beyond Validation for Code Generation 

**Title (ZH)**: TENET: 利用超越验证的测试进行代码生成 

**Authors**: Yiran Hu, Nan Jiang, Shanchao Liang, Yi Wu, Lin Tan  

**Link**: [PDF](https://arxiv.org/pdf/2509.24148)  

**Abstract**: Test-Driven Development (TDD) is a widely adopted software engineering practice that requires developers to create and execute tests alongside code implementation, ensuring that software behavior is continuously validated and refined. In the era of vibe coding, where developers increasingly delegate code writing to large language models (LLMs) by specifying high-level intentions, TDD becomes even more crucial, as test cases serve as executable specifications that explicitly define and verify intended functionality beyond what natural-language descriptions and code context can convey. While vibe coding under TDD is promising, there are three main challenges: (1) selecting a small yet effective test suite to improve the generation accuracy and control the execution workload, (2) retrieving context such as relevant code effectively, and (3) systematically using test feedback for effective code refinement. To address these challenges, we introduce TENET, an LLM agent for generating functions in complex real-world repositories under the TDD setting. TENET features three components: (1) a novel test harness mechanism that selects a concise test suite to maximize diversity of target usage scenarios; (2) a tailored agent toolset that performs efficient retrieval of relevant code with interactive debugging; and (3) a reflection-based refinement workflow that iteratively analyzes failures, replenishes context, and applies code refinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval benchmarks, outperforming the best agentic baselines by 9.49 and 2.17 percentage points, respectively. In addition, this is the first study of test-driven code generation with repository-level context, examining how different aspects of test suites affect the performance of LLM agents under the TDD setting. 

**Abstract (ZH)**: TENET：面向TDD的复杂仓库中函数生成的LLM代理 

---
# Your thoughts tell who you are: Characterize the reasoning patterns of LRMs 

**Title (ZH)**: 你的思考揭示了你的本质：刻画LRMs的推理模式 

**Authors**: Yida Chen, Yuning Mao, Xianjun Yang, Suyu Ge, Shengjie Bi, Lijuan Liu, Saghar Hosseini, Liang Tan, Yixin Nie, Shaoliang Nie  

**Link**: [PDF](https://arxiv.org/pdf/2509.24147)  

**Abstract**: Current comparisons of large reasoning models (LRMs) focus on macro-level statistics such as task accuracy or reasoning length. Whether different LRMs reason differently remains an open question. To address this gap, we introduce the LLM-proposed Open Taxonomy (LOT), a classification method that uses a generative language model to compare reasoning traces from two LRMs and articulate their distinctive features in words. LOT then models how these features predict the source LRM of a reasoning trace based on their empirical distributions across LRM outputs. Iterating this process over a dataset of reasoning traces yields a human-readable taxonomy that characterizes how models think. We apply LOT to compare the reasoning of 12 open-source LRMs on tasks in math, science, and coding. LOT identifies systematic differences in their thoughts, achieving 80-100% accuracy in distinguishing reasoning traces from LRMs that differ in scale, base model family, or objective domain. Beyond classification, LOT's natural-language taxonomy provides qualitative explanations of how LRMs think differently. Finally, in a case study, we link the reasoning differences to performance: aligning the reasoning style of smaller Qwen3 models with that of the largest Qwen3 during test time improves their accuracy on GPQA by 3.3-5.7%. 

**Abstract (ZH)**: 当前对大规模推理模型（LRMs）的比较主要集中于宏观统计指标，如任务准确率或推理长度。不同LRMs是否以不同的方式推理仍然是一个开放问题。为了弥合这一差距，我们提出了LLM提出的开放分类法（LOT），这是一种使用生成语言模型比较两个LRMs的推理轨迹并用文字描述其独特特征的分类方法。LOT随后根据LRM输出的实验分布，建模这些特征如何预测推理轨迹的起源模型。通过对推理轨迹数据集的迭代处理，LOT生成了一种易于理解的分类法，刻画了模型的思考方式。我们应用LOT比较了12个开源LRMs在数学、科学和编程任务中的推理。LOT识别了它们思维中的系统性差异，在区分规模不同、基础模型家族或目标领域不同的LRMs的推理轨迹方面实现了80-100%的准确率。除了分类，LOT的语言分类法还提供了LRMs如何不同地思考的定性解释。最后，在一个案例研究中，我们将推理差异与性能联系起来：在测试时将较小的Qwen3模型的推理风格与最大的Qwen3对齐，提高了其在GPQA上的准确率3.3-5.7%。 

---
# EYE-DEX: Eye Disease Detection and EXplanation System 

**Title (ZH)**: EYE-DEX：眼科疾病检测与解释系统 

**Authors**: Youssef Sabiri, Walid Houmaidi, Amine Abouaomar  

**Link**: [PDF](https://arxiv.org/pdf/2509.24136)  

**Abstract**: Retinal disease diagnosis is critical in preventing vision loss and reducing socioeconomic burdens. Globally, over 2.2 billion people are affected by some form of vision impairment, resulting in annual productivity losses estimated at $411 billion. Traditional manual grading of retinal fundus images by ophthalmologists is time-consuming and subjective. In contrast, deep learning has revolutionized medical diagnostics by automating retinal image analysis and achieving expert-level performance. In this study, we present EYE-DEX, an automated framework for classifying 10 retinal conditions using the large-scale Retinal Disease Dataset comprising 21,577 eye fundus images. We benchmark three pre-trained Convolutional Neural Network (CNN) models--VGG16, VGG19, and ResNet50--with our finetuned VGG16 achieving a state-of-the-art global benchmark test accuracy of 92.36%. To enhance transparency and explainability, we integrate the Gradient-weighted Class Activation Mapping (Grad-CAM) technique to generate visual explanations highlighting disease-specific regions, thereby fostering clinician trust and reliability in AI-assisted diagnostics. 

**Abstract (ZH)**: 视网膜疾病诊断对于预防视力丧失和减轻社会经济负担至关重要。全球有超过22亿人受到不同程度的视力障碍影响，导致年度生产力损失估计达到4110亿美元。传统的眼科医生 manually 对视网膜底片图像进行分级耗时且主观。相比之下，深度学习已经通过自动化视网膜图像分析实现了专家级性能，从而革命了医疗诊断。在本研究中，我们提出了一种名为EYE-DEX的自动化框架，用于分类10种视网膜疾病，该框架基于包含21,577张眼底图像的大型视网膜疾病数据集。我们基准测试了三种预训练的卷积神经网络模型——VGG16、VGG19和ResNet50，其中我们细调的VGG16模型在全局基准测试中的准确率达到92.36%，处于领先地位。为了提高透明度和可解释性，我们整合了梯度加权类激活映射（Grad-CAM）技术，生成视觉解释，突出显示疾病特异性区域，从而增强临床医生对人工智能辅助诊断的信任和可靠性。 

---
# ASTROCO: Self-Supervised Conformer-Style Transformers for Light-Curve Embeddings 

**Title (ZH)**: ASTROCO: 自监督Conformer风格变换器及其在光变曲线嵌入中的应用 

**Authors**: Antony Tan, Pavlos Protopapas, Martina Cádiz-Leyton, Guillermo Cabrera-Vives, Cristobal Donoso-Oliva, Ignacio Becker  

**Link**: [PDF](https://arxiv.org/pdf/2509.24134)  

**Abstract**: We present AstroCo, a Conformer-style encoder for irregular stellar light curves. By combining attention with depthwise convolutions and gating, AstroCo captures both global dependencies and local features. On MACHO R-band, AstroCo outperforms Astromer v1 and v2, yielding 70 percent and 61 percent lower error respectively and a relative macro-F1 gain of about 7 percent, while producing embeddings that transfer effectively to few-shot classification. These results highlight AstroCo's potential as a strong and label-efficient foundation for time-domain astronomy. 

**Abstract (ZH)**: AstroCo：一种适用于不规则恒星光曲线的Conformer-style编码器 

---
# BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes 

**Title (ZH)**: BOSfM: 农业场景最佳3D重建的视角规划框架 

**Authors**: Athanasios Bacharis, Konstantinos D. Polyzos, Georgios B. Giannakis, Nikolaos Papanikolopoulos  

**Link**: [PDF](https://arxiv.org/pdf/2509.24126)  

**Abstract**: Active vision (AV) has been in the spotlight of robotics research due to its emergence in numerous applications including agricultural tasks such as precision crop monitoring and autonomous harvesting to list a few. A major AV problem that gained popularity is the 3D reconstruction of targeted environments using 2D images from diverse viewpoints. While collecting and processing a large number of arbitrarily captured 2D images can be arduous in many practical scenarios, a more efficient solution involves optimizing the placement of available cameras in 3D space to capture fewer, yet more informative, images that provide sufficient visual information for effective reconstruction of the environment of interest. This process termed as view planning (VP), can be markedly challenged (i) by noise emerging in the location of the cameras and/or in the extracted images, and (ii) by the need to generalize well in other unknown similar agricultural environments without need for re-optimizing or re-training. To cope with these challenges, the present work presents a novel VP framework that considers a reconstruction quality-based optimization formulation that relies on the notion of `structure-from-motion' to reconstruct the 3D structure of the sought environment from the selected 2D images. With no analytic expression of the optimization function and with costly function evaluations, a Bayesian optimization approach is proposed to efficiently carry out the VP process using only a few function evaluations, while accounting for different noise cases. Numerical tests on both simulated and real agricultural settings signify the benefits of the advocated VP approach in efficiently estimating the optimal camera placement to accurately reconstruct 3D environments of interest, and generalize well on similar unknown environments. 

**Abstract (ZH)**: 基于运动结构的农业环境主动视图规划方法 

---
# The Impossibility of Inverse Permutation Learning in Transformer Models 

**Title (ZH)**: Transformer模型中逆排列学习的不可能性 

**Authors**: Rohan Alur, Chris Hays, Manish Raghavan, Devavrat Shah  

**Link**: [PDF](https://arxiv.org/pdf/2509.24125)  

**Abstract**: In this technical note, we study the problem of inverse permutation learning in decoder-only transformers. Given a permutation and a string to which that permutation has been applied, the model is tasked with producing the original (``canonical'') string. We argue that this task models a natural robustness property across a variety of reasoning tasks, including long-context retrieval, multiple choice QA and in-context learning. Our primary contribution is an impossibility result: we show that an arbitrary depth, decoder-only transformer cannot learn this task. This result concerns the expressive capacity of decoder-only transformer models and is agnostic to training dynamics or sample complexity. We give a pair of alternative constructions under which inverse permutation learning is feasible. The first of these highlights the fundamental role of the causal attention mask, and reveals a gap between the expressivity of encoder-decoder transformers and the more popular decoder-only architecture. The latter result is more surprising: we show that simply padding the input with ``scratch tokens" yields a construction under which inverse permutation learning is possible. We conjecture that this may suggest an alternative mechanism by which chain-of-thought prompting or, more generally, intermediate ``thinking'' tokens can enable reasoning in large language models, even when these tokens encode no meaningful semantic information (e.g., the results of intermediate computations). 

**Abstract (ZH)**: 本技术注记研究了仅解码器变压器中逆排列学习的问题。给定一个排列及其上应用后的字符串，模型的任务是生成原始（“标准”）字符串。我们认为，这一任务模拟了多种推理任务中的自然鲁棒性特性，包括长上下文检索、多项选择问答和上下文学习。我们的主要贡献是一个不可能性结果：我们证明了任意深度的仅解码器变压器无法学习这一任务。该结果关注仅解码器变压器模型的表征能力，并与训练动态或样本复杂度无关。我们给出了两种替代构造，在这些构造下逆排列学习是可行的。其中第一个凸显了因果注意力掩码的基本作用，揭示了编码器-解码器变压器与更流行的仅解码器架构在表征能力上的差距。后者结果更为令人惊讶：我们证明，简单地用“草稿标记”填充输入即可在某种构造下实现逆排列学习。我们推测这可能表明了一种替代机制，通过链式思考提示或更一般地，中间“思考”标记，即使这些标记编码了无意义的语义信息（例如中间计算的结果），也可能使大规模语言模型具备推理能力。 

---
# Ancestry Tree Clustering for Particle Filter Diversity Maintenance 

**Title (ZH)**: 祖先树聚类以维护粒子滤波多样性 

**Authors**: Ilari Vallivaara, Bingnan Duan, Yinhuan Dong, Tughrul Arslan  

**Link**: [PDF](https://arxiv.org/pdf/2509.24124)  

**Abstract**: We propose a method for linear-time diversity maintenance in particle filtering. It clusters particles based on ancestry tree topology: closely related particles in sufficiently large subtrees are grouped together. The main idea is that the tree structure implicitly encodes similarity without the need for spatial or other domain-specific metrics. This approach, when combined with intra-cluster fitness sharing and the protection of particles not included in a cluster, effectively prevents premature convergence in multimodal environments while maintaining estimate compactness. We validate our approach in a multimodal robotics simulation and a real-world multimodal indoor environment. We compare the performance to several diversity maintenance algorithms from the literature, including Deterministic Resampling and Particle Gaussian Mixtures. Our algorithm achieves high success rates with little to no negative effect on compactness, showing particular robustness to different domains and challenging initial conditions. 

**Abstract (ZH)**: 一种线性时间粒子滤波多样性维护方法：基于祖先树拓扑的群组化策略 

---
# GEAR: A General Evaluation Framework for Abductive Reasoning 

**Title (ZH)**: GEAR: 一种演绎推理的通用评估框架 

**Authors**: Kaiyu He, Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Xinya Du, Zhiyu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24096)  

**Abstract**: Since the advent of large language models (LLMs), research has focused on instruction following and deductive reasoning. A central question remains: can these models discover new knowledge, and how can we evaluate this ability? We address this by studying abductive reasoning-the generation of plausible hypotheses to explain observations-and introduce GEAR (General Evaluation for Abductive Reasoning), a general-purpose, fully automated, transparent, and label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics: consistency (each hypothesis explains the observations), generalizability (consistent hypotheses make meaningful predictions on unseen inputs), and diversity (the set covers distinct predictions and patterns). Built this way, GEAR is scalable (no human gold answers), reliable (deterministic scoring aligned with classical abduction), and open-ended (scores improve only when models produce new plausible hypotheses, unlike static benchmarks that saturate once accuracy is high). Using GEAR, we conduct a fine-grained study of nine LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000 candidate hypotheses and revealing model differences obscured by gold-answer or purely human evaluations. We further propose a momentum-based curriculum that adjusts GEAR-derived training data by learning velocity: it starts with what the model learns quickly and shifts toward harder objectives such as generating diverse hypotheses once the model is confident on foundational objectives. Without gold-label supervision, this strategy improves all GEAR objectives and these gains transfer to established abductive reasoning benchmarks. Taken together, GEAR provides a principled framework that evaluates abduction and supplies label-free, scalable training signals that help LLMs produce more diverse and reliable hypotheses. 

**Abstract (ZH)**: 自大型语言模型（LLM）问世以来，研究重点一直放在指令跟随和演绎推理上。一个核心问题仍然是：这些模型能否发现新知识，我们如何评估这种能力？通过研究解释观察结果的归纳推理——生成合理的假设——我们引入了GEAR（通用归纳推理评估），这是一种通用的、全自动的、透明的、无标签的评估范式。GEAR通过三个指标来评分假设集：一致性（每个假设解释观察结果）、泛化能力（一致的假设对未见过的输入做出有意义的预测）和多样性（集合覆盖不同的预测和模式）。通过这种方式，GEAR具有可扩展性（无需人工黄金答案）、可靠性（评分与经典归纳推理一致）、开放性（仅在模型生成新的合理假设时评分提高，不同于饱和的一次性基准）。利用GEAR，我们在四个归纳推理基准上对九个LLM进行了精细研究，涉及1500个问题，生成了超过50,000个候选假设，并揭示了黄金答案或纯人类评估中隐藏的模型差异。我们进一步提出了一种动量为基础的课程，通过学习速度调整GEAR衍生的训练数据：它从模型学习快的内容开始，然后转向生成多样化假设等更困难的目标，只要模型在基础目标上表现出自信。在无需黄金标签监督的情况下，这种策略提高了所有GEAR目标，并将这些增益转移到已确立的归纳推理基准上。综上所述，GEAR提供了一个原则性的框架，用于评估归纳推理，并提供了无标签、可扩展的训练信号，帮助LLM生成更多样化和可靠的假设。 

---
# PerfBench: Can Agents Resolve Real-World Performance Bugs? 

**Title (ZH)**: PerfBench: 前沿基准：智能体能解决真实世界的性能 bug 吗？ 

**Authors**: Spandan Garg, Roshanak Zilouchian Moghaddam  

**Link**: [PDF](https://arxiv.org/pdf/2509.24091)  

**Abstract**: Performance bugs are inefficiencies in software that waste computational resources without causing functional failures, making them particularly challenging to detect and fix. While recent advances in Software Engineering agents have shown promise in automated bug fixing, existing benchmarks primarily focus on functional correctness and fail to evaluate agents' abilities to identify and resolve non-functional issues like performance bugs. We introduce PerfBench, a benchmark comprising 81 real-world performance bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing benchmarks that rely on pre-existing test suites, PerfBench features a novel evaluation harness that allows agents to generate their own performance benchmarks and validates fixes by comparing execution metrics collected for developer fix and agent fix. Each task in PerfBench is derived from actual developer fixes linked to performance-related issues, which are then verified by human experts, ensuring real-world relevance. Our evaluation reveals that current state-of-the-art coding agents struggle with performance optimization tasks, with baseline OpenHands agent achieving only a ~3% success rate on our benchmark. We develop OpenHands-Perf-Agent, which incorporates performance-aware tooling and instructions and achieves a ~20% success rate on the benchmark. We show that by ensuring the agent has proper instructions to benchmark its changes and tooling for benchmark output processing, we can improve the agent performance significantly, but room for improvement still remains. PerfBench provides a challenging test set for furthering the capabilities of agents in fixing performance issues. 

**Abstract (ZH)**: PerformanceBench：一个用于评估软件代理解决性能问题能力的新基准 

---
# Large-Scale Constraint Generation - Can LLMs Parse Hundreds of Constraints? 

**Title (ZH)**: 大规模约束生成——LLMs能解析数百个约束吗？ 

**Authors**: Matteo Boffa, Jiaxuan You  

**Link**: [PDF](https://arxiv.org/pdf/2509.24090)  

**Abstract**: Recent research has explored the constrained generation capabilities of Large Language Models (LLMs) when explicitly prompted by few task-specific requirements. In contrast, we introduce Large-Scale Constraint Generation (LSCG), a new problem that evaluates whether LLMs can parse a large, fine-grained, generic list of constraints. To examine the LLMs' ability to handle an increasing number constraints, we create a practical instance of LSCG, called Words Checker. In Words Checker, we evaluate the impact of model characteristics (e.g., size, family) and steering techniques (e.g., Simple Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet, a small and dedicated model that parses the original list of constraints into a smaller subset, helping the LLM focus on relevant constraints. Experiments reveal that existing solutions suffer a significant performance drop as the number of constraints increases, with FoCusNet showing an 8-13% accuracy boost. 

**Abstract (ZH)**: 近期研究探索了在明确提示下，大规模语言模型（LLMs）的约束生成能力。相比之下，我们引入了大规模约束生成（LSCG）这一新问题，评估LLMs是否能够解析一个大型、细粒度且通用的约束列表。为了考察LLMs处理不断增加的约束数量的能力，我们创建了一个LSCG的实用实例，称为Words Checker。在Words Checker中，我们评估了模型特性（如规模、家族）和引导技术（如简要提示、思维链、最优N）对性能的影响。我们还提出了FoCusNet，这是一种小型且专门化的模型，能够将原始的约束列表解析为一个更小的子集，帮助LLMs专注于相关约束。实验结果显示，随着约束数量的增加，现有解决方案的性能显著下降，而FoCusNet则展示了8-13%的准确率提升。 

---
# PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM 

**Title (ZH)**: PEARL: 同辈增强自适应无线通信 via 边缘设备上的LLM 

**Authors**: Ju-Hyung Lee, Yanqing Lu, Klaus Doppler  

**Link**: [PDF](https://arxiv.org/pdf/2509.24085)  

**Abstract**: We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a framework for cooperative cross-layer optimization in device-to-device (D2D) communication. Building on our previous work on single-device on-device LLMs, PEARL extends the paradigm by leveraging both publisher and subscriber states to guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which normalizes latency by application tolerances and modulates energy by device battery states, provides richer supervision for KL-based finetuning. We study two lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves the best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms inference at near-identical objective scores. Across synthetic scenarios grounded in real measurements, PEARL improves objective scores over heuristic and compact model baselines and reduces energy by up to 16% in cooperative low-battery cases. These results demonstrate that peer-aware context, reward-aligned training, and head-based efficiency make LLMs practical for always-on, on-device cross-layer control. 

**Abstract (ZH)**: PEARL：基于设备端LLM的同伴增强自适应无线电框架 

---
# Uncovering Grounding IDs: How External Cues Shape Multi-Modal Binding 

**Title (ZH)**: 揭示接地ID：外部线索如何塑造多模态绑定 

**Authors**: Hosein Hasani, Amirmohammad Izadi, Fatemeh Askari, Mobin Bagherian, Sadegh Mohammadian, Mohammad Izadi, Mahdieh Soleymani Baghshah  

**Link**: [PDF](https://arxiv.org/pdf/2509.24072)  

**Abstract**: Large vision-language models (LVLMs) show strong performance across multimodal benchmarks but remain limited in structured reasoning and precise grounding. Recent work has demonstrated that adding simple visual structures, such as partitions and annotations, improves accuracy, yet the internal mechanisms underlying these gains remain unclear. We investigate this phenomenon and propose the concept of Grounding IDs, latent identifiers induced by external cues that bind objects to their designated partitions across modalities. Through representation analysis, we find that these identifiers emerge as robust within-partition alignment in embedding space and reduce the modality gap between image and text. Causal interventions further confirm that these identifiers mediate binding between objects and symbolic cues. We show that Grounding IDs strengthen attention between related components, which in turn improves cross-modal grounding and reduces hallucinations. Taken together, our results identify Grounding IDs as a key symbolic mechanism explaining how external cues enhance multimodal binding, offering both interpretability and practical improvements in robustness. 

**Abstract (ZH)**: 大型多模态视觉-语言模型在多模态基准测试中表现出色，但在结构化推理和精确定位方面仍有限制。近期研究表明，增加简单的视觉结构，如分区和注解，可以提高准确性，但这些改进背后的内部机制尚不明确。我们探讨了这一现象，并提出了Grounding IDs的概念，即由外部线索诱导出的潜藏标识符，它将对象与其指定的分区在不同模态中绑定在一起。通过表示分析，我们发现这些标识符在嵌入空间中表现出内在的一致性，并减少了图像与文本之间的模态差异。因果干预进一步证实这些标识符在对象与符号线索之间起着中介作用。我们展示了Grounding IDs增强了相关组件之间的注意力，从而提高了跨模态定位并减少了幻觉。综上，我们的研究结果将Grounding IDs识别为一种关键的符号机制，它解释了外部线索如何增强多模态绑定，提供了可解释性和在鲁棒性方面的实际改进。 

---
# AQUAIR: A High-Resolution Indoor Environmental Quality Dataset for Smart Aquaculture Monitoring 

**Title (ZH)**: AQUAIR: 一种高分辨率室内环境质量数据集，用于智能水产监控 

**Authors**: Youssef Sabiri, Walid Houmaidi, Ouail El Maadi, Yousra Chtouki  

**Link**: [PDF](https://arxiv.org/pdf/2509.24069)  

**Abstract**: Smart aquaculture systems depend on rich environmental data streams to protect fish welfare, optimize feeding, and reduce energy use. Yet public datasets that describe the air surrounding indoor tanks remain scarce, limiting the development of forecasting and anomaly-detection tools that couple head-space conditions with water-quality dynamics. We therefore introduce AQUAIR, an open-access public dataset that logs six Indoor Environmental Quality (IEQ) variables--air temperature, relative humidity, carbon dioxide, total volatile organic compounds, PM2.5 and PM10--inside a fish aquaculture facility in Amghass, Azrou, Morocco. A single Awair HOME monitor sampled every five minutes from 14 October 2024 to 9 January 2025, producing more than 23,000 time-stamped observations that are fully quality-controlled and publicly archived on Figshare. We describe the sensor placement, ISO-compliant mounting height, calibration checks against reference instruments, and an open-source processing pipeline that normalizes timestamps, interpolates short gaps, and exports analysis-ready tables. Exploratory statistics show stable conditions (median CO2 = 758 ppm; PM2.5 = 12 micrograms/m3) with pronounced feeding-time peaks, offering rich structure for short-horizon forecasting, event detection, and sensor drift studies. AQUAIR thus fills a critical gap in smart aquaculture informatics and provides a reproducible benchmark for data-centric machine learning curricula and environmental sensing research focused on head-space dynamics in recirculating aquaculture systems. 

**Abstract (ZH)**: 智能水产养殖系统依赖丰富的环境数据流来保护鱼的福利、优化投喂和减少能耗。然而，描述室内水槽周围空气的公开数据集仍然稀缺，限制了将空间条件与水质动态耦合的预测和异常检测工具的发展。因此，我们介绍了AQUAIR，一个开放访问的公开数据集，在摩洛哥阿姆加斯、阿祖鲁的鱼塘设施内记录六种室内环境质量（IEQ）变量——空气温度、相对湿度、二氧化碳、总挥发性有机化合物、PM2.5和PM10。从2024年10月14日至2025年1月9日，每5分钟采样一次的单个Awair HOME监测器生成了超过23,000个带时间戳的观测数据，并在Figshare上完全质量控制并公开存档。我们描述了传感器布局、ISO合规的安装高度、参考仪器校准检查以及开源处理管道，该管道规范时间戳、插补短间隙并导出分析准备的表格。初步统计结果显示稳定条件（中位数二氧化碳=758 ppm；PM2.5=12微克/立方米），在投喂时间出现峰值，为短期预测、事件检测和传感器漂移研究提供了丰富的结构。AQUAIR因此填补了智能水产养殖信息技术中的一个重要空白，并为基于数据的机器学习课程和关注循环水产养殖系统空间动态的环境传感研究提供了可重复的基准。 

---
# A Small Math Model: Recasting Strategy Choice Theory in an LLM-Inspired Architecture 

**Title (ZH)**: 一个小数学模型：以大语言模型启发的策略选择理论重构 

**Authors**: Roussel Rahman, Jeff Shrager  

**Link**: [PDF](https://arxiv.org/pdf/2509.24068)  

**Abstract**: Strategy Choice Theory (SCT)\footnote{``Strategy Choice Theory'', ``Distributions of Associations'', and ``Overlapping Wave Theory'' have been used to refer to this line of work, emphasizing different aspects.}\citep[e.g.,][]{siegler1984strategychoices, siegler2000rebirth} explains important aspects of children's arithmetic learning based upon principles including learning from developmentally naturalistic data, probabilistic representation, confidence-based retrieval, and the phase-like importance of scaffolding strategies, such as finger-counting. Here we recast SCT as a ``Small Math Model'' (SMM), employing a neural-network-based architecture analogous to LLMs. The SMM extends SCT to include counting practice\footnote{The original SCT model was pre-biased in accordance with the supposed experience of counting.}, symbol (number) embedding, and gated attention. Similar to earlier work, the SMM demonstrates constructive and destructive interference between counting and addition, and the ``wave-like'' use of finger-counting as sum recall improves. We plan to extend the SMM to later aspects of the decades-long SCT program, including adaptive strategy choice and eventually strategy discovery, providing a unified platform to investigate the understanding of numerical characteristics and relationships essential for mathematical reasoning -- as it can emerge in LLM-based agents. 

**Abstract (ZH)**: 小数学模型（SMM） 

---
# In-Context Compositional Q-Learning for Offline Reinforcement Learning 

**Title (ZH)**: 上下文依赖组合Q学习在离线强化学习中的应用 

**Authors**: Qiushui Xu, Yuhao Huang, Yushu Jiang, Lei Song, Jinyu Wang, Wenliang Zheng, Jiang Bian  

**Link**: [PDF](https://arxiv.org/pdf/2509.24067)  

**Abstract**: Accurately estimating the Q-function is a central challenge in offline reinforcement learning. However, existing approaches often rely on a single global Q-function, which struggles to capture the compositional nature of tasks involving diverse subtasks. We propose In-context Compositional Q-Learning (\texttt{ICQL}), the first offline RL framework that formulates Q-learning as a contextual inference problem, using linear Transformers to adaptively infer local Q-functions from retrieved transitions without explicit subtask labels. Theoretically, we show that under two assumptions--linear approximability of the local Q-function and accurate weight inference from retrieved context--\texttt{ICQL} achieves bounded Q-function approximation error, and supports near-optimal policy extraction. Empirically, \texttt{ICQL} substantially improves performance in offline settings: improving performance in kitchen tasks by up to 16.4\%, and in Gym and Adroit tasks by up to 8.6\% and 6.3\%. These results highlight the underexplored potential of in-context learning for robust and compositional value estimation, positioning \texttt{ICQL} as a principled and effective framework for offline RL. 

**Abstract (ZH)**: 准确估计Q函数是离线强化学习中的核心挑战。现有方法往往依赖于单一全局Q函数，难以捕捉包含多样子任务的组合性质任务。我们提出In-context Compositional Q-Learning (\texttt{ICQL})，这是第一个将Q学习形式化为上下文推理问题的离线RL框架，采用线性Transformer从检索得到的转换中自适应地推断局部Q函数，而无需显式子任务标签。理论分析表明，在局部Q函数线性可近似和从检索上下文准确推断权重的假设下，\texttt{ICQL}实现有界Q函数近似误差，并支持接近最优策略提取。实验结果显示，在离线设置中，\texttt{ICQL}显著提高了性能：在厨房任务中提高了16.4%，在Gym和Adroit任务中分别提高了8.6%和6.3%。这些结果突显了上下文学习在鲁棒和组合价值估计方面的未充分开发的潜力，将\texttt{ICQL}定位为基于原理且有效的离线RL框架。 

---
# A Second-Order Perspective on Pruning at Initialization and Knowledge Transfer 

**Title (ZH)**: 初始化时的剪枝第二-order视角与知识迁移 

**Authors**: Leonardo Iurada, Beatrice Occhiena, Tatiana Tommasi  

**Link**: [PDF](https://arxiv.org/pdf/2509.24066)  

**Abstract**: The widespread availability of pre-trained vision models has enabled numerous deep learning applications through their transferable representations. However, their computational and storage costs often limit practical deployment. Pruning-at-Initialization has emerged as a promising approach to compress models before training, enabling efficient task-specific adaptation. While conventional wisdom suggests that effective pruning requires task-specific data, this creates a challenge when downstream tasks are unknown in advance. In this paper, we investigate how data influences the pruning of pre-trained vision models. Surprisingly, pruning on one task retains the model's zero-shot performance also on unseen tasks. Furthermore, fine-tuning these pruned models not only improves performance on original seen tasks but can recover held-out tasks' performance. We attribute this phenomenon to the favorable loss landscapes induced by extensive pre-training on large-scale datasets. 

**Abstract (ZH)**: 预训练视觉模型的剪枝初始化：数据的影响及性能恢复 

---
# PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features 

**Title (ZH)**: PartnerMAS：一种基于高维特征的企业合作伙伴选择多层次多agent框架 

**Authors**: Lingyao Li, Haolun Wu, Zhenkun Li, Jiabei Hu, Yu Wang, Xiaoshan Huang, Wenyue Hua, Wenqian Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.24046)  

**Abstract**: High-dimensional decision-making tasks, such as business partner selection, involve evaluating large candidate pools with heterogeneous numerical, categorical, and textual features. While large language models (LLMs) offer strong in-context reasoning capabilities, single-agent or debate-style systems often struggle with scalability and consistency in such settings. We propose PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation into three layers: a Planner Agent that designs strategies, Specialized Agents that perform role-specific assessments, and a Supervisor Agent that integrates their outputs. To support systematic evaluation, we also introduce a curated benchmark dataset of venture capital co-investments, featuring diverse firm attributes and ground-truth syndicates. Across 140 cases, PartnerMAS consistently outperforms single-agent and debate-based multi-agent baselines, achieving up to 10--15\% higher match rates. Analysis of agent reasoning shows that planners are most responsive to domain-informed prompts, specialists produce complementary feature coverage, and supervisors play an important role in aggregation. Our findings demonstrate that structured collaboration among LLM agents can generate more robust outcomes than scaling individual models, highlighting PartnerMAS as a promising framework for high-dimensional decision-making in data-rich domains. 

**Abstract (ZH)**: 高维度决策任务中的伙伴选择涉及评估具有异质数值、分类和文本特征的大型候选池。虽然大型语言模型提供了强大的上下文推理能力，但单智能体或辩论式系统在这些场景中往往面临可扩展性和一致性方面的挑战。我们提出了一种层次化的多智能体框架PartnerMAS，将评估分解为三层：规划智能体负责设计策略，专业智能体执行特定角色的评估，监督智能体整合其输出。为了支持系统性的评估，我们还引入了一个精制的基准数据集，包含风险投资联合投资的多样化企业属性和真实 syndicate 的地面真相表示。在 140 个案例中，PartnerMAS 在单智能体和辩论式多智能体基线方面始终表现出色，匹配率最高可提高 10-15%。对智能体推理的分析显示，规划者对领域导向的提示最敏感，专家生成互补的特征覆盖，而监督者在聚合中发挥重要作用。我们的研究结果表明，在数据丰富的领域中，LLM 智能体之间的结构化合作可以产生比单模型扩展更为稳健的结果，突显出了 PartnerMAS 作为高维度决策制定的有前途框架的价值。 

---
# End-to-end Topographic Auditory Models Replicate Signatures of Human Auditory Cortex 

**Title (ZH)**: 端到端拓扑听觉模型再现人类听觉皮层的特征 

**Authors**: Haider Al-Tahan, Mayukh Deb, Jenelle Feather, N. Apurva Ratan Murty  

**Link**: [PDF](https://arxiv.org/pdf/2509.24039)  

**Abstract**: The human auditory cortex is topographically organized. Neurons with similar response properties are spatially clustered, forming smooth maps for acoustic features such as frequency in early auditory areas, and modular regions selective for music and speech in higher-order cortex. Yet, evaluations for current computational models of auditory perception do not measure whether such topographic structure is present in a candidate model. Here, we show that cortical topography is not present in the previous best-performing models at predicting human auditory fMRI responses. To encourage the emergence of topographic organization, we adapt a cortical wiring-constraint loss originally designed for visual perception. The new class of topographic auditory models, TopoAudio, are trained to classify speech, and environmental sounds from cochleagram inputs, with an added constraint that nearby units on a 2D cortical sheet develop similar tuning. Despite these additional constraints, TopoAudio achieves high accuracy on benchmark tasks comparable to the unconstrained non-topographic baseline models. Further, TopoAudio predicts the fMRI responses in the brain as well as standard models, but unlike standard models, TopoAudio develops smooth, topographic maps for tonotopy and amplitude modulation (common properties of early auditory representation, as well as clustered response modules for music and speech (higher-order selectivity observed in the human auditory cortex). TopoAudio is the first end-to-end biologically grounded auditory model to exhibit emergent topography, and our results emphasize that a wiring-length constraint can serve as a general-purpose regularization tool to achieve biologically aligned representations. 

**Abstract (ZH)**: 人类听皮层按拓扑结构组织。具有相似响应性质的神经元在空间上聚类，形成早期听觉区域中频率等声学特征的平滑地图，并在高级皮层中形成对音乐和语言具有模块化选择性的区域。然而，当前听觉感知计算模型的评估并未衡量候选模型中是否存在这种拓扑结构。我们展示了之前的最佳预测人类听觉fMRI反应的模型中不存在皮层拓扑结构。为了鼓励拓扑结构的出现，我们采用了一种最初为视觉感知设计的皮层连接约束损失。新的拓扑听觉模型类TopoAudio被训练用于从耳蜗图输入分类语音和环境声音，并且增加了附近二维皮层单元发展相似调谐的约束。尽管增加了这些额外约束，TopoAudio在基准任务上的准确度与未受约束的非拓扑基线模型相当。此外，TopoAudio预测大脑的fMRI反应与标准模型一样准确，但与标准模型不同的是，TopoAudio发展了对于音调定位和振幅调制的平滑拓扑图（早期听觉表征的常见属性），以及对音乐和语音具有聚类反应模块（人类听觉皮层中观察到的高阶选择性）。TopoAudio是第一个表现出新兴拓扑结构的端到端生物基础听觉模型，我们的结果强调，连接长度约束可以作为一种通用正则化工具，以实现生物对齐的表示。 

---
# GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning 

**Title (ZH)**: GPS-MTM：使用自我监督学习捕捉GPS轨迹中的正常模式 

**Authors**: Umang Garg, Bowen Zhang, Anantanjit Subrahmanya, Chandrakanth Gudavalli, BS Manjunath  

**Link**: [PDF](https://arxiv.org/pdf/2509.24031)  

**Abstract**: Foundation models have driven remarkable progress in text, vision, and video understanding, and are now poised to unlock similar breakthroughs in trajectory modeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a foundation model for large-scale mobility data that captures patterns of normalcy in human movement. Unlike prior approaches that flatten trajectories into coordinate streams, GPS-MTM decomposes mobility into two complementary modalities: states (point-of-interest categories) and actions (agent transitions). Leveraging a bi-directional Transformer with a self-supervised masked modeling objective, the model reconstructs missing segments across modalities, enabling it to learn rich semantic correlations without manual labels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and Geolife, GPS-MTM consistently outperforms on downstream tasks such as trajectory infilling and next-stop prediction. Its advantages are most pronounced in dynamic tasks (inverse and forward dynamics), where contextual reasoning is critical. These results establish GPS-MTM as a robust foundation model for trajectory analytics, positioning mobility data as a first-class modality for large-scale representation learning. Code is released for further reference. 

**Abstract (ZH)**: GPSMasked 轨迹变换器：大规模移动数据的基础模型 

---
# From Edge to HPC: Investigating Cross-Facility Data Streaming Architectures 

**Title (ZH)**: 从边缘到超算：探究跨设施数据流架构 

**Authors**: Anjus George, Michael Brim, Christopher Zimmer, David Rogers, Sarp Oral, Zach Mayes  

**Link**: [PDF](https://arxiv.org/pdf/2509.24030)  

**Abstract**: In this paper, we investigate three cross-facility data streaming architectures, Direct Streaming (DTS), Proxied Streaming (PRS), and Managed Service Streaming (MSS). We examine their architectural variations in data flow paths and deployment feasibility, and detail their implementation using the Data Streaming to HPC (DS2HPC) architectural framework and the SciStream memory-to-memory streaming toolkit on the production-grade Advanced Computing Ecosystem (ACE) infrastructure at Oak Ridge Leadership Computing Facility (OLCF). We present a workflow-specific evaluation of these architectures using three synthetic workloads derived from the streaming characteristics of scientific workflows. Through simulated experiments, we measure streaming throughput, round-trip time, and overhead under work sharing, work sharing with feedback, and broadcast and gather messaging patterns commonly found in AI-HPC communication motifs. Our study shows that DTS offers a minimal-hop path, resulting in higher throughput and lower latency, whereas MSS provides greater deployment feasibility and scalability across multiple users but incurs significant overhead. PRS lies in between, offering a scalable architecture whose performance matches DTS in most cases. 

**Abstract (ZH)**: 本文研究了三种跨设施数据流架构——直接流（DTS）、代理流（PRS）和管理服务流（MSS），探讨了它们在数据流路径和部署可行性方面的架构变体，并使用Data Streaming to HPC（DS2HPC）架构框架和SciStream内存到内存流传输工具包在橡树岭领导计算设施（OLCF）的生产级先进计算生态系统（ACE）基础设施上详细阐述了其实现。通过特定工作流的评估，使用源自科学工作流流特性的人工合成工作负载进行评估。通过模拟实验，测量了工作分担、带有反馈的工作分担、广播和收集消息模式下的流传输吞吐量、往返时间和开销。研究结果表明，DTS提供了最少跳跃路径，从而实现更高的吞吐量和更低的延迟，而MSS提供了更好的部署可行性和跨多个用户的大规模扩展性，但会带来显著的开销。PRS介于两者之间，提供了一种可扩展的架构，其性能在大多数情况下与DTS匹配。 

---
# FrameMind: Frame-Interleaved Chain-of-Thought for Video Reasoning via Reinforcement Learning 

**Title (ZH)**: FrameMind: 帧间插/frame交替链式思考的视频推理方法 via 强化学习 

**Authors**: Haonan Ge, Yiwei Wang, Kai-Wei Chang, Hang Wu, Yujun Cai  

**Link**: [PDF](https://arxiv.org/pdf/2509.24008)  

**Abstract**: Current video understanding models rely on fixed frame sampling strategies, processing predetermined visual inputs regardless of the specific reasoning requirements of each question. This static approach limits their ability to adaptively gather visual evidence, leading to suboptimal performance on tasks that require either broad temporal coverage or fine-grained spatial detail. In this paper, we introduce FrameMind, an end-to-end framework trained with reinforcement learning that enables models to dynamically request visual information during reasoning through Frame-Interleaved Chain-of-Thought (FiCOT). Unlike traditional approaches, FrameMind operates in multiple turns where the model alternates between textual reasoning and active visual perception, using tools to extract targeted frames or video clips based on identified knowledge gaps. To train effective dynamic sampling policies, we propose Dynamic Resolution Frame Sampling (DRFS), which exposes models to diverse temporal-spatial trade-offs during learning, and DRFS-GRPO, a group-relative policy optimization algorithm that learns from outcome-based rewards without requiring frame-level annotations. Extensive experiments on challenging benchmarks like MLVU and VideoMME demonstrate that our method significantly outperforms existing models, advancing the state of the art in flexible and efficient video understanding. 

**Abstract (ZH)**: 当前的视频理解模型依赖于固定帧采样策略，在推理过程中处理预先确定的视觉输入，而不考虑每个问题的具体推理需求。这种静态方法限制了模型适应性地收集视觉证据的能力，导致在需要广泛的时间覆盖或精细的空间细节的任务中表现不佳。本文引入了FrameMind，这是一种通过帧插混思维链（FiCOT）训练的端到端框架，使模型能够在推理过程中动态请求视觉信息。FrameMind在多轮交互中运作，模型交替进行文本推理和主动视觉感知，使用工具根据识别的知识缺口提取目标帧或视频片段。为了训练有效的动态采样策略，我们提出了动态分辨率帧采样（DRFS），让模型在学习过程中暴露于多样化的时空间权衡中，并使用基于结果的奖励学习群相对策优化算法（DRFS-GRPO），无需帧级注释。在如MLVU和VideoMME等具有挑战性的基准测试中，我们的方法显著优于现有模型，推动了灵活高效视频理解的技术进步。 

---
# SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention 

**Title (ZH)**: SLA：通过可微调稀疏线性注意力超越扩散变换器的稀疏性 

**Authors**: Jintao Zhang, Haoxu Wang, Kai Jiang, Shuo Yang, Kaiwen Zheng, Haocheng Xi, Ziteng Wang, Hongzhou Zhu, Min Zhao, Ion Stoica, Joseph E. Gonzalez, Jun Zhu, Jianfei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.24006)  

**Abstract**: In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bottleneck due to the long sequence length and the quadratic complexity. We find that attention weights can be separated into two parts: a small fraction of large weights with high rank and the remaining weights with very low rank. This naturally suggests applying sparse acceleration to the first part and low-rank acceleration to the second. Based on this finding, we propose SLA (Sparse-Linear Attention), a trainable attention method that fuses sparse and linear attention to accelerate diffusion models. SLA classifies attention weights into critical, marginal, and negligible categories, applying O(N^2) attention to critical weights, O(N) attention to marginal weights, and skipping negligible ones. SLA combines these computations into a single GPU kernel and supports both forward and backward passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x reduction in attention computation, resulting in significant acceleration without loss of generation quality. Experiments show that SLA reduces attention computation by 95% without degrading end-to-end generation quality, outperforming baseline methods. In addition, we implement an efficient GPU kernel for SLA, which yields a 13.7x speedup in attention computation and a 2.2x end-to-end speedup in video generation on Wan2.1-1.3B. 

**Abstract (ZH)**: 在Diffusion Transformer（DiT）模型中，尤其是对于视频生成，注意力延迟由于长序列长度和 quadratic 复杂性成为主要瓶颈。我们发现注意力权重可以分为两部分：一小部分具有高秩的大权重和剩余部分具有极低秩的权重。这自然地提示我们对前一部分使用稀疏加速，对后一部分使用低秩加速。基于这一发现，我们提出了SLA（Sparse-Linear Attention），一种可训练的注意力方法，将稀疏注意力和线性注意力融合以加速扩散模型。SLA将注意力权重分为关键、边缘和可忽略不计三类，对关键权重应用 O(N^2) 注意力，对边缘权重应用 O(N) 注意力，并跳过可忽略不计的权重。SLA将这些计算合并到一个 GPU 核심，并支持前向和反向传播。仅通过少量的 SLA 微调步骤，DiT 模型实现了注意力计算20倍的减少，显著加速同时不损失生成质量。实验表明，SLA 在不降低端到端生成质量的情况下，将注意力计算减少95%，并优于基线方法。此外，我们为 SLA 实现了一个高效的 GPU 核心，使其在 Wan2.1-1.3B 上的注意力计算加速13.7倍，在视频生成中端到端加速2.2倍。 

---
# MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use 

**Title (ZH)**: MCPMark: 一种用于压力测试实际且全面的MCP使用情况的基准测试 

**Authors**: Zijian Wu, Xiangyan Liu, Xinyuan Zhang, Lingjun Chen, Fanqing Meng, Lingxiao Du, Yiran Zhao, Fanshi Zhang, Yaoqi Ye, Jiawei Wang, Zirui Wang, Jinjie Ni, Yufan Yang, Arvin Xu, Michael Qizhe Shieh  

**Link**: [PDF](https://arxiv.org/pdf/2509.24002)  

**Abstract**: MCP standardizes how LLMs interact with external systems, forming the foundation for general agents. However, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows. To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP use in a more realistic and comprehensive manner. It consists of $127$ high-quality tasks collaboratively created by domain experts and AI agents. Each task begins with a curated initial state and includes a programmatic script for automatic verification. These tasks demand richer and more diverse interactions with the environment, involving a broad range of create, read, update, and delete (CRUD) operations. We conduct a comprehensive evaluation of cutting-edge LLMs using a minimal agent framework that operates in a tool-calling loop. Empirical results show that the best-performing model, gpt-5-medium, reaches only $52.56$\% pass@1 and $33.86$\% pass^4, while other widely regarded strong models, including claude-sonnet-4 and o3, fall below $30$\% pass@1 and $15$\% pass^4. On average, LLMs require $16.2$ execution turns and $17.4$ tool calls per task, significantly surpassing those in previous MCP benchmarks and highlighting the stress-testing nature of MCPMark. 

**Abstract (ZH)**: MCPMark：一种评估MCP在更现实和全面场景下使用的新基准 

---
# The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis 

**Title (ZH)**: AI代理行为准则：自动化护栏提示合成政策 

**Authors**: Gauri Kholkar, Ratinder Ahuja  

**Link**: [PDF](https://arxiv.org/pdf/2509.23994)  

**Abstract**: As autonomous AI agents are increasingly deployed in industry, it is essential to safeguard them. We introduce a novel framework that automates the translation of unstructured design documents into verifiable, real-time guardrails. We introduce "Policy as Prompt," a new approach that uses Large Language Models (LLMs) to interpret and enforce natural language policies by applying contextual understanding and the principle of least privilege. Our system first ingests technical artifacts to construct a verifiable policy tree, which is then compiled into lightweight, prompt-based classifiers that audit agent behavior at runtime. We validate our approach across diverse applications, demonstrating a scalable and auditable pipeline that bridges the critical policy-to-practice gap, paving the way for verifiably safer and more regulatable AI. 

**Abstract (ZH)**: 随着自主AI代理在工业中的日益部署，保障它们的安全至关重要。我们提出了一种新的框架，自动将未结构化的设计文档转换为可验证的、实时的护栏。我们引入了“策略即提示”的新方法，使用大规模语言模型（LLMs）通过上下文理解与最小权限原则来解释和执行自然语言策略。我们的系统首先摄取技术 artifacts 来构建可验证的策略树，然后将其编译成轻量级的、基于提示的分类器，在运行时审核代理行为。我们跨多种应用验证了该方法，展示了可扩展且可审计的管道，弥合了关键的策略到实践差距，为可验证更安全和更可调节的 AI 开辟了道路。 

---
# Guide: Generalized-Prior and Data Encoders for DAG Estimation 

**Title (ZH)**: 指南：广义先验和数据编码器用于DAG估计 

**Authors**: Amartya Roy, Devharish N, Shreya Ganguly, Kripabandhu Ghosh  

**Link**: [PDF](https://arxiv.org/pdf/2509.23992)  

**Abstract**: Modern causal discovery methods face critical limitations in scalability, computational efficiency, and adaptability to mixed data types, as evidenced by benchmarks on node scalability (30, $\le 50$, $\ge 70$ nodes), computational energy demands, and continuous/non-continuous data handling. While traditional algorithms like PC, GES, and ICA-LiNGAM struggle with these challenges, exhibiting prohibitive energy costs for higher-order nodes and poor scalability beyond 70 nodes, we propose \textbf{GUIDE}, a framework that integrates Large Language Model (LLM)-generated adjacency matrices with observational data through a dual-encoder architecture. GUIDE uniquely optimizes computational efficiency, reducing runtime on average by $\approx 42%$ compared to RL-BIC and KCRL methods, while achieving an average $\approx 117%$ improvement in accuracy over both NOTEARS and GraN-DAG individually. During training, GUIDE's reinforcement learning agent dynamically balances reward maximization (accuracy) and penalty avoidance (DAG constraints), enabling robust performance across mixed data types and scalability to $\ge 70$ nodes -- a setting where baseline methods fail. 

**Abstract (ZH)**: 现代因果发现方法在扩展性、计算效率和混合数据类型适应性方面面临关键限制，如节点扩展性基准（30, $\le 50$, $\ge 70$ 节点）、计算能耗需求以及连续/非连续数据处理能力所证实的那样。尽管传统的PC算法、GES算法和ICA-LiNGAM在这些挑战面前表现不佳，能耗成本高企，且在超过70个节点时表现出糟糕的扩展性，我们提出了**GUIDE框架**，该框架通过双编码器架构结合了大语言模型生成的邻接矩阵和观测数据。GUIDE独特地优化了计算效率，相比RL-BIC和KCRL方法平均减少约42%的运行时间，并且在准确率上分别比NOTEARS和GraN-DAG高出约117%。在训练过程中，GUIDE的强化学习代理动态平衡准确性和DAG约束条件下的惩罚规避，从而在混合数据类型下表现出稳健性能，并实现节点数超过70的情况下的可扩展性——这是基线方法无法做到的。 

---
# The Hidden Costs of Translation Accuracy: Distillation, Quantization, and Environmental Impact 

**Title (ZH)**: 翻译后的标题为：翻译准确性下的隐形成本：精炼、量化及其环境影响 

**Authors**: Dhaathri Vijay, Anandaswarup Vadapalli  

**Link**: [PDF](https://arxiv.org/pdf/2509.23990)  

**Abstract**: The rapid expansion of large language models (LLMs) has heightened concerns about their computational and environmental costs. This study investigates the trade-offs between translation quality and efficiency by comparing full-scale, distilled, and quantized models using machine translation as a case study. We evaluated performance on the Flores+ benchmark and through human judgments of conversational translations in French, Hindi, and Kannada. Our analysis of carbon emissions per evaluation run revealed that the full 3.3B fp32 model, while achieving the highest BLEU scores, incurred the largest environmental footprint (about 0.007-0.008 kg CO2 per run). The distilled models achieved an inference of up to 4.5x faster than the full 3.3B model, with only minimal reductions in BLEU scores. Human evaluations also showed that even aggressive quantization (INT4) preserved high levels of accuracy and fluency, with differences between models generally minor. These findings demonstrate that model compression strategies can substantially reduce computational demands and environmental impact while maintaining competitive translation quality, though trade-offs are more pronounced in low-resource settings. We argue for evaluation frameworks that integrate efficiency and sustainability alongside objective metrics as central dimensions of progress in NLP. 

**Abstract (ZH)**: 大规模语言模型的迅速扩展引发了对其计算和环境成本的关注。本研究通过将全规模、蒸馏和量化模型进行比较，以机器翻译为例，探讨翻译质量和效率之间的权衡。我们在Flores+基准和对法语、印地语和卡纳达语对话翻译的人工评价上评估了模型性能。我们的分析显示，尽管全规模33亿参数的浮点32位模型在BLEU分数上最高，但其每评估运行的碳排放量最大（约为0.007-0.008 kg CO2）。蒸馏模型的推理速度比全规模33亿参数模型快4.5倍以上，同时BLEU分数的减少 minimal。人工评价还表明，即使是激进的量化（INT4）也能保持高水平的准确性和流畅性，不同模型之间的差异通常较小。这些发现表明，在维持竞争力的翻译质量的同时，通过模型压缩策略可以显著降低计算需求和环境影响，尽管在资源有限的环境中权衡更为明显。我们主张将效率和可持续性纳入与客观指标并列的核心维度，以促进自然语言处理的进步。 

---
# Toward Preference-aligned Large Language Models via Residual-based Model Steering 

**Title (ZH)**: 基于残差导向的模型调节实现偏好对齐的大语言模型 

**Authors**: Lucio La Cava, Andrea Tagarelli  

**Link**: [PDF](https://arxiv.org/pdf/2509.23982)  

**Abstract**: Preference alignment is a critical step in making Large Language Models (LLMs) useful and aligned with (human) preferences. Existing approaches such as Reinforcement Learning from Human Feedback or Direct Preference Optimization typically require curated data and expensive optimization over billions of parameters, and eventually lead to persistent task-specific models. In this work, we introduce Preference alignment of Large Language Models via Residual Steering (PaLRS), a training-free method that exploits preference signals encoded in the residual streams of LLMs. From as few as one hundred preference pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be applied at inference time to push models toward preferred behaviors. We evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance. Moreover, when compared to DPO-aligned models, they perform better with huge time savings. Our findings highlight that PaLRS offers an effective, much more efficient and flexible alternative to standard preference optimization pipelines, offering a training-free, plug-and-play mechanism for alignment with minimal data. 

**Abstract (ZH)**: Large Language Models的偏好对齐：通过残差导向（PaLRS）实现偏好导向训练-free方法 

---
# MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control 

**Title (ZH)**: MAD-PINN：一种安全且最优的多智能体控制去中心化物理信息机器学习框架 

**Authors**: Manan Tayal, Aditya Singh, Shishir Kolathaya, Somil Bansal  

**Link**: [PDF](https://arxiv.org/pdf/2509.23960)  

**Abstract**: Co-optimizing safety and performance in large-scale multi-agent systems remains a fundamental challenge. Existing approaches based on multi-agent reinforcement learning (MARL), safety filtering, or Model Predictive Control (MPC) either lack strict safety guarantees, suffer from conservatism, or fail to scale effectively. We propose MAD-PINN, a decentralized physics-informed machine learning framework for solving the multi-agent state-constrained optimal control problem (MASC-OCP). Our method leverages an epigraph-based reformulation of SC-OCP to simultaneously capture performance and safety, and approximates its solution via a physics-informed neural network. Scalability is achieved by training the SC-OCP value function on reduced-agent systems and deploying them in a decentralized fashion, where each agent relies only on local observations of its neighbours for decision-making. To further enhance safety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based neighbour selection strategy to prioritize safety-critical interactions, and a receding-horizon policy execution scheme that adapts to dynamic interactions while reducing computational burden. Experiments on multi-agent navigation tasks demonstrate that MAD-PINN achieves superior safety-performance trade-offs, maintains scalability as the number of agents grows, and consistently outperforms state-of-the-art baselines. 

**Abstract (ZH)**: 在大规模多Agent系统中同时优化安全性和性能仍然是一个基本挑战。现有的基于多Agent强化学习（MARL）、安全筛选或模型预测控制（MPC）的方法要么缺乏严格的安全性保证，要么具有保守性，要么无法有效扩展。我们提出了MAD-PINN，这是一种分布式的物理信息机器学习框架，用于解决具有状态约束的多Agent最优控制问题（MASC-OCP）。该方法利用SC-OCP的episode图表形式重新表述来同时捕捉性能和安全性，并通过物理信息神经网络近似其解。通过在缩减Agent的系统中训练SC-OCP价值函数并在分布式方式下部署它们，实现了可扩展性，其中每个Agent仅依赖于其邻居的局部观察来进行决策。为了进一步提高安全性和效率，我们引入了基于Hamilton-Jacobi（HJ）可达性的邻居选择策略来优先处理安全关键的交互，并引入了一种基于后退视界的策略执行方案，该方案能够适应动态交互并减少计算负担。在多Agent导航任务上的实验表明，MAD-PINN实现了更好的安全性和性能 trade-offs，在Agent数量增加时保持了可扩展性，并且一致地优于现有最先进的基线。 

---
# Vision-Grounded Machine Interpreting: Improving the Translation Process through Visual Cues 

**Title (ZH)**: 基于视觉的机器解释：通过视觉线索改进翻译过程 

**Authors**: Claudio Fantinuoli  

**Link**: [PDF](https://arxiv.org/pdf/2509.23957)  

**Abstract**: Machine Interpreting systems are currently implemented as unimodal, real-time speech-to-speech architectures, processing translation exclusively on the basis of the linguistic signal. Such reliance on a single modality, however, constrains performance in contexts where disambiguation and adequacy depend on additional cues, such as visual, situational, or pragmatic information. This paper introduces Vision-Grounded Interpreting (VGI), a novel approach designed to address the limitations of unimodal machine interpreting. We present a prototype system that integrates a vision-language model to process both speech and visual input from a webcam, with the aim of priming the translation process through contextual visual information. To evaluate the effectiveness of this approach, we constructed a hand-crafted diagnostic corpus targeting three types of ambiguity. In our evaluation, visual grounding substantially improves lexical disambiguation, yields modest and less stable gains for gender resolution, and shows no benefit for syntactic ambiguities. We argue that embracing multimodality represents a necessary step forward for advancing translation quality in machine interpreting. 

**Abstract (ZH)**: 基于视觉的地基机器连贯性解释（Vision-Grounded Interpreting: VGI） 

---
# Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm 

**Title (ZH)**: 探索-执行链：迈向一种高效的结构化推理范式 

**Authors**: Kaisen Yang, Lixuan He, Rushi Shah, Kaicheng Yang, Qinwei Ma, Dianbo Liu, Alex Lamb  

**Link**: [PDF](https://arxiv.org/pdf/2509.23946)  

**Abstract**: Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation algorithm enforcing strict plan adherence - with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of this http URL decomposition enables an efficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution. The code and pre-trained models for the project are available at: this https URL 

**Abstract (ZH)**: Chain-of-Thought（CoT）及其变体显著提升了大型语言模型（LLMs）的推理能力，然而其单一且自回归的架构本领会将高级战略规划与低级步骤执行混淆在一起，导致计算效率低下、推理路径探索有限以及可解释性降低。为克服这些问题，我们提出了探索-执行链（$E^2C$），这是一种分阶段的推理框架，将推理分解为两个不同的阶段：探索阶段随机生成简洁的高级计划，随后是执行阶段确定性地执行选定的计划。我们的方法结合了两阶段训练方法，该方法包括通过新颖的数据生成算法增强监督微调（SFT），该算法强制执行严格的计划一致性，以及后续的强化学习（RL）阶段，该阶段利用探索的有效性并强化执行的确定性。这种分解允许高效的测试时扩展策略：在AIME'2024中，$E^2C$测试时扩展达到了58.1%的准确率，仅需与类似方法（如思考森林）所需解码令牌的不到10%，大幅降低了自一致性开销。对于跨领域的适应性，我们的探索导向SFT（EF-SFT）只使用标准SFT的3.5%令牌，但在医疗基准上的准确率提高了多达14.5%，实现了最先进的性能、强大的泛化能力和更好的可解释性，这是因为分离了计划和执行。项目的代码和预训练模型可在此处获取：this https URL。 

---
# Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems 

**Title (ZH)**: Easy Turn: 结合声学和语言模态实现全双工 spoken 对话系统中稳健的轮替 

**Authors**: Guojian Li, Chengyou Wang, Hongfei Xue, Shuiyuan Wang, Dehui Gao, Zihan Zhang, Yuke Lin, Wenjie Li, Longshuai Xiao, Zhonghua Fu, Lei Xie  

**Link**: [PDF](https://arxiv.org/pdf/2509.23938)  

**Abstract**: Full-duplex interaction is crucial for natural human-machine communication, yet remains challenging as it requires robust turn-taking detection to decide when the system should speak, listen, or remain silent. Existing solutions either rely on dedicated turn-taking models, most of which are not open-sourced. The few available ones are limited by their large parameter size or by supporting only a single modality, such as acoustic or linguistic. Alternatively, some approaches finetune LLM backbones to enable full-duplex capability, but this requires large amounts of full-duplex data, which remain scarce in open-source form. To address these issues, we propose Easy Turn, an open-source, modular turn-taking detection model that integrates acoustic and linguistic bimodal information to predict four dialogue turn states: complete, incomplete, backchannel, and wait, accompanied by the release of Easy Turn trainset, a 1,145-hour speech dataset designed for training turn-taking detection models. Compared to existing open-source models like TEN Turn Detection and Smart Turn V2, our model achieves state-of-the-art turn-taking detection accuracy on our open-source Easy Turn testset. The data and model will be made publicly available on GitHub. 

**Abstract (ZH)**: 全双工交互对于自然的人机通信至关重要，但依然具有挑战性，因为它需要 robust 的轮流转换检测以决定系统应该何时说话、聆听或保持沉默。现有的解决方案要么依赖专用的轮流转换模型，但大多数模型并未开源；现有的少数开源模型要么参数量大，要么仅支持单一模态，如声学或语言。此外，一些方法通过微调大语言模型来实现全双工能力，但这需要大量全双工数据，而开源数据仍然稀缺。为解决这些问题，我们提出了一种开源且模块化的轮流转换检测模型 Easy Turn，它结合了声学和语言的双模态信息来预测四种对话轮流状态：完整、不完整、响应性反馈和等待，并同时发布了 Easy Turn 训练集，这是一个设计用于训练轮流转换检测模型的 1,145 小时语音数据集。与现有的开源模型（如 TEN Turn Detection 和 Smart Turn V2）相比，我们的模型在我们的开源 Easy Turn 测试集上实现了最先进的轮流转换检测准确性。数据和模型将在 GitHub 上公开发布。 

---
# Diffusion Models are Kelly Gamblers 

**Title (ZH)**: 扩散模型是凯利赌徒 

**Authors**: Akhil Premkumar  

**Link**: [PDF](https://arxiv.org/pdf/2509.23937)  

**Abstract**: We draw a connection between diffusion models and the Kelly criterion for maximizing returns in betting games. We find that conditional diffusion models store additional information to bind the signal $X$ with the conditioning information $Y$, equal to the mutual information between them. Classifier-free guidance effectively boosts the mutual information between $X$ and $Y$ at sampling time. This is especially helpful in image models, since the mutual information between images and their labels is low, a fact which is intimately connected to the manifold hypothesis. Finally, we point out some nuances in the popular perspective that diffusion models are infinitely deep autoencoders. In doing so, we relate the denoising loss to the Fermi Golden Rule from quantum mechanics. 

**Abstract (ZH)**: 我们将扩散模型与赌博游戏中最大化回报的凯利准则进行连接。我们发现条件扩散模型存储了额外的信息，将信号$X$与条件信息$Y$绑定，等价于它们之间的互信息。无分类引导有效地在采样时间提高$X$与$Y$之间的互信息。特别是在图像模型中这一点尤为重要，因为图像与其标签之间的互信息较低，这一事实与流形假设密切相关。最后，我们指出对扩散模型是无限深自编码器的流行观点存在一些细微差别，并将去噪损失与量子力学中的费米-金规则相联系。 

---
# HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models 

**Title (ZH)**: HiViS: 从草稿者隐藏视觉标记以在视觉-语言模型中进行推测解码 

**Authors**: Zhinan Xie, Peisong Wang, Jian Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2509.23928)  

**Abstract**: Speculative decoding is an effective approach for accelerating inference in Large Language models (LLMs), but its adaptation to Vision-Language models (VLMs) remains challenging for additional visual tokens in multimodal inputs. First, owing to the fact that the drafter and the target VLM may derived from different families, the semantic representations of visual tokens in the target VLM are misaligned with those in the drafter, introducing bias into the KV-cache during the prefill stage. Second, the large number of visual tokens substantially slows down the drafter's self-attention during the decoding stage. We propose Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models (HiViS), an explicit-implicit input decomposition framework that alleviates the above inefficiency. All visual tokens are removed from the drafter's input, retaining only textual tokens as explicit inputs, while directly reusing the target VLM's corresponding last-layer hidden states as implicit visual information without additional processing. To train the drafter efficiently, we introduces multi-step self-feedback training strategy with dynamic data selection and sequential embedding supervision to simulate reasoning during training. Our approach compresses the prefill sequence length of the drafter to only 0.7%-1.3% of the target VLM's input, while maintaining lossless generation quality. Extensive experiments across diverse models and tasks demonstrate up to 2.65x speedup, confirming the effectiveness of HiViS in accelerating VLM inference. 

**Abstract (ZH)**: 隐藏视觉令牌以加速视觉语言模型的投机解码（HiViS） 

---
# Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step 

**Title (ZH)**: 通过少量解码步骤的一致性轨迹强化学习驯服掩码扩散语言模型 

**Authors**: Jingyi Yang, Guanxu Chen, Xuhao Hu, Jing Shao  

**Link**: [PDF](https://arxiv.org/pdf/2509.23924)  

**Abstract**: Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: this https URL. 

**Abstract (ZH)**: Masked扩散语言模型（MDLMs） recently emerged as a有前景的替代自回归（AR）语言模型的选择，提供了并行解码、灵活的生成顺序以及更少的推理步骤等特性。尽管存在这些优势，针对MDLMs的解码策略和强化学习（RL）算法仍然研究不足。一种简单的做法是直接将为AR模型开发的成熟技术应用于MDLMs。然而，这提出了一个直接的问题：这种简单的转移是否真的最优化？例如，1) 块级和半AR解码策略在MDLMs的训练中未被采用，那么为什么它们在推理中能优于全扩散风格的解码？2) 将为AR模型设计的RL算法直接应用于MDLMs，由于MDLM解码是非因果的（并行的），导致了生成轨迹与优化轨迹之间的不一致性。为了解决这些挑战，我们提出了EOS提前拒绝（EOSER）和上升步长（ASS）解码调度器，这使MDLMs能够执行全扩散风格的解码，实现了在更少的解码步骤内达到竞争性性能。此外，我们引入了约束解码轨迹组相对策略优化（CJ-GRPO）来驯化MDLMs，强调生成轨迹与优化轨迹的一致性，并减少了由于跳步优化引起的优化误差。我们在包括数学和规划基准任务的推理任务中使用了LLaDA-8B-Instruct进行了广泛的实验。结果表明，提出的EOSER和ASS机制，结合CJ-GRPO，对有效和高效驯化MDLMs具有重要意义。代码: [这里](这个链接)。 

---
# Graph Mixing Additive Networks 

**Title (ZH)**: 图混合加性网络 

**Authors**: Maya Bechler-Speicher, Andrea Zerio, Maor Huri, Marie Vibeke Vestergaard, Ran Gilad-Bachrach, Tine Jess, Samir Bhatt, Aleksejs Sazonovs  

**Link**: [PDF](https://arxiv.org/pdf/2509.23923)  

**Abstract**: We introduce GMAN, a flexible, interpretable, and expressive framework that extends Graph Neural Additive Networks (GNANs) to learn from sets of sparse time-series data. GMAN represents each time-dependent trajectory as a directed graph and applies an enriched, more expressive GNAN to each graph. It allows users to control the interpretability-expressivity trade-off by grouping features and graphs to encode priors, and it provides feature, node, and graph-level interpretability. On real-world datasets, including mortality prediction from blood tests and fake-news detection, GMAN outperforms strong non-interpretable black-box baselines while delivering actionable, domain-aligned explanations. 

**Abstract (ZH)**: GMAN：一种灵活、可解释且表达能力强的框架，用于处理稀疏时间序列数据集的学习 

---
# Continual Learning to Generalize Forwarding Strategies for Diverse Mobile Wireless Networks 

**Title (ZH)**: 持续学习以泛化转发策略于多样化的移动无线网络 

**Authors**: Cheonjin Park, Victoria Manfredi, Xiaolan Zhang, Chengyi Liu, Alicia P Wolfe, Dongjin Song, Sarah Tasneem, Bing Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23913)  

**Abstract**: Deep reinforcement learning (DRL) has been successfully used to design forwarding strategies for multi-hop mobile wireless networks. While such strategies can be used directly for networks with varied connectivity and dynamic conditions, developing generalizable approaches that are effective on scenarios significantly different from the training environment remains largely unexplored. In this paper, we propose a framework to address the challenge of generalizability by (i) developing a generalizable base model considering diverse mobile network scenarios, and (ii) using the generalizable base model for new scenarios, and when needed, fine-tuning the base model using a small amount of data from the new scenarios. To support this framework, we first design new features to characterize network variation and feature quality, thereby improving the information used in DRL-based forwarding decisions. We then develop a continual learning (CL) approach able to train DRL models across diverse network scenarios without ``catastrophic forgetting.'' Using extensive evaluation, including real-world scenarios in two cities, we show that our approach is generalizable to unseen mobility scenarios. Compared to a state-of-the-art heuristic forwarding strategy, it leads to up to 78% reduction in delay, 24% improvement in delivery rate, and comparable or slightly higher number of forwards. 

**Abstract (ZH)**: 深强化学习在多跳移动无线网络转发策略设计中的应用：通过通用基础模型和持续学习方法实现泛化能力 

---
# EWC-Guided Diffusion Replay for Exemplar-Free Continual Learning in Medical Imaging 

**Title (ZH)**: 基于EWC指导的扩散重放以实现无范例持续学习在医学成像中 

**Authors**: Anoushka Harit, William Prew, Zhongtian Sun, Florian Markowetz  

**Link**: [PDF](https://arxiv.org/pdf/2509.23906)  

**Abstract**: Medical imaging foundation models must adapt over time, yet full retraining is often blocked by privacy constraints and cost. We present a continual learning framework that avoids storing patient exemplars by pairing class conditional diffusion replay with Elastic Weight Consolidation. Using a compact Vision Transformer backbone, we evaluate across eight MedMNIST v2 tasks and CheXpert. On CheXpert our approach attains 0.851 AUROC, reduces forgetting by more than 30\% relative to DER\texttt{++}, and approaches joint training at 0.869 AUROC, while remaining efficient and privacy preserving. Analyses connect forgetting to two measurable factors: fidelity of replay and Fisher weighted parameter drift, highlighting the complementary roles of replay diffusion and synaptic stability. The results indicate a practical route for scalable, privacy aware continual adaptation of clinical imaging models. 

**Abstract (ZH)**: 医疗影像基础模型必须随时间进行适应，但完全重新训练往往受限于隐私约束和成本。我们提出了一种持续学习框架，通过将类条件扩散重放与弹性权重巩固相结合，避免存储患者示例。采用紧凑的视觉变压器骨干，我们在八个MedMNIST v2任务和CheXpert上进行了评估。在CheXpert上，我们的方法取得了0.851的AUROC，相对于DER\texttt{++}减少了超过30%的遗忘，并接近联合训练的0.869 AUROC，同时保持高效和隐私保护。分析将遗忘关联到两个可测量的因素：重放保真度和费舍尔加权参数漂移，强调了重放扩散和突触稳定性的互补作用。结果表明了一条实用途径，用于实现临床影像模型的 scalable、隐私意识持续适应。 

---
# Interpreting deep learning-based stellar mass estimation via causal analysis and mutual information decomposition 

**Title (ZH)**: 基于因果分析和互信息分解的深度学习恒星质量估计解释 

**Authors**: Wei Zhang, Qiufan Lin, Yuan-Sen Ting, Shupei Chen, Hengxin Ruan, Song Li, Yifan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23901)  

**Abstract**: End-to-end deep learning models fed with multi-band galaxy images are powerful data-driven tools used to estimate galaxy physical properties in the absence of spectroscopy. However, due to a lack of interpretability and the associational nature of such models, it is difficult to understand how the information additional to integrated photometry (e.g., morphology) contributes to the estimation task. Improving our understanding in this field would enable further advances into unraveling the physical connections among galaxy properties and optimizing data exploitation. Therefore, our work is aimed at interpreting the deep learning-based estimation of stellar mass via two interpretability techniques: causal analysis and mutual information decomposition. The former reveals the causal paths between multiple variables beyond nondirectional statistical associations, while the latter quantifies the multicomponent contributions (i.e., redundant, unique, and synergistic) of different input data to the stellar mass estimation. Using data from the Sloan Digital Sky Survey (SDSS) and the Wide-field Infrared Survey Explorer (WISE), we obtained meaningful results that provide physical interpretations for image-based models. Our work demonstrates the gains from combining deep learning with interpretability techniques, and holds promise in promoting more data-driven astrophysical research (e.g., astrophysical parameter estimations and investigations on complex multivariate physical processes). 

**Abstract (ZH)**: 基于多频段星系图像的端到端深度学习模型能够从无光谱数据中估计星系物理性质，然而由于这些模型缺乏可解释性和关联性本质，难以理解额外的综合光度学信息（如形态学）如何 contributes到估计任务。为了促进对该领域的理解并优化数据利用，我们的工作旨在通过因果分析和互信息分解两种可解释性技术来解释基于深度学习的恒星质量估计。前者揭示了多个变量之间的因果路径，而后者量化了不同输入数据对恒星质量估计的多成分贡献（即冗余、独特和协同贡献）。使用斯隆数字天空巡天（SDSS）和广域红外巡天探索者（WISE）数据，我们获得了有意义的结果，为图像模型提供了物理解释。我们的工作展示了将深度学习与可解释性技术结合的优势，并有望促进更具数据驱动性的天体物理研究（例如天体物理参数估计和复杂多变量物理过程的研究）。 

---
# Preserving Cross-Modal Stability for Visual Unlearning in Multimodal Scenarios 

**Title (ZH)**: 在多模态场景中保留跨模态稳定性以实现视觉遗忘 

**Authors**: Jinghan Xu Yuyang Zhang Qixuan Cai Jiancheng Chen Keqiu Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.23895)  

**Abstract**: Visual modality is the most vulnerable to privacy leakage in real-world multimodal applications like autonomous driving with visual and radar data; Machine unlearning removes specific training data from pre-trained models to address privacy leakage, however, existing methods fail to preserve cross-modal knowledge and maintain intra-class structural stability of retain data, leading to reduced overall and other modalities' performance during visual unlearning; to address these challenges, we propose a Cross-modal Contrastive Unlearning (CCU) framework, which integrates three key components: (a) selective visual unlearning: employing inverse contrastive learning to dissociate visual representations from their original semantics, (b) cross-modal knowledge retention: preserving other modalities' discriminability through semantic consistency, and (c) dual-set contrastive separation: preserving the model performance via isolation of structural perturbations between the unlearn set and retain set; extensive experiments on three datasets demonstrate the superiority of CCU, and our method achieves a 7.12% accuracy improvement with only 7% of the unlearning time compared to the top-accuracy baseline. 

**Abstract (ZH)**: 跨模态对比遗忘（CCU）框架：解决视觉模态在自主驾驶等多模态应用中的隐私泄露问题 

---
# Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings 

**Title (ZH)**: 动态正交持续微调以缓解灾难性遗忘 

**Authors**: Zhixin Zhang, Zeming Wei, Meng Sun  

**Link**: [PDF](https://arxiv.org/pdf/2509.23893)  

**Abstract**: Catastrophic forgetting remains a critical challenge in continual learning for large language models (LLMs), where models struggle to retain performance on historical tasks when fine-tuning on new sequential data without access to past datasets. In this paper, we first reveal that the drift of functional directions during the fine-tuning process is a key reason why existing regularization-based methods fail in long-term LLM continual learning. To address this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a novel approach that tracks the drift of these functional directions and dynamically updates them during the fine-tuning process. Furthermore, by adjusting the gradients of new task parameters to be orthogonal to the tracked historical function directions, our method mitigates interference between new and old tasks. Extensive experiments on various LLM continual learning benchmarks demonstrate that this approach outperforms prior methods, effectively reducing catastrophic forgetting and providing a robust tool for continuous LLM fine-tuning. Our code is available at this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）持续学习中灾难性遗忘仍然是一个关键挑战，在仅利用新序列数据微调而无法访问过往数据集的情况下，模型难以保留历史任务的性能。本文首先揭示，在微调过程中功能方向的变化是现有基于正则化的持续学习方法在长期LLMs持续学习中失效的关键原因。为此，我们提出了一种新颖的Dynamic Orthogonal Continual (DOC) 微调方法，该方法跟踪这些功能方向的变化并在微调过程中动态更新它们。此外，通过使新任务参数的梯度与跟踪的历史功能方向正交，我们的方法减轻了新旧任务之间的干扰。在各种LLM持续学习基准上的广泛实验表明，该方法优于先前的方法，有效减少了灾难性遗忘，为持续LLM微调提供了一个稳健的工具。我们的代码可在以下链接获取。 

---
# Gradient Flow Convergence Guarantee for General Neural Network Architectures 

**Title (ZH)**: 梯度流收敛性保证：通用神经网络架构 

**Authors**: Yash Jakhmola  

**Link**: [PDF](https://arxiv.org/pdf/2509.23887)  

**Abstract**: A key challenge in modern deep learning theory is to explain the remarkable success of gradient-based optimization methods when training large-scale, complex deep neural networks. Though linear convergence of such methods has been proved for a handful of specific architectures, a united theory still evades researchers. This article presents a unified proof for linear convergence of continuous gradient descent, also called gradient flow, while training any neural network with piecewise non-zero polynomial activations or ReLU, sigmoid activations. Our primary contribution is a single, general theorem that not only covers architectures for which this result was previously unknown but also consolidates existing results under weaker assumptions. While our focus is theoretical and our results are only exact in the infinitesimal step size limit, we nevertheless find excellent empirical agreement between the predictions of our result and those of the practical step-size gradient descent method. 

**Abstract (ZH)**: 现代深度学习理论中的一个关键挑战是在训练大规模复杂深度神经网络时，解释基于梯度的优化方法的显著成功。尽管已经证明了这些方法在少数特定架构上的线性收敛性，但统一理论仍未能让研究人员达成共识。本文给出了对任何具有分段非零多项式激活或ReLU、Sigmoid激活的神经网络使用连续梯度下降（也称为梯度流）方法的线性收敛性的统一证明。我们的主要贡献是一个通用的单一定理，不仅涵盖了之前未知架构的结果，还将在较弱假设下汇总了现有结果。虽然我们关注的是理论方面，且结果仅在无穷小步长极限下精确，但我们仍然发现我们的结果预测与实际步长梯度下降方法的预测之间有很好的实验一致性。 

---
# Towards Understanding Subliminal Learning: When and How Hidden Biases Transfer 

**Title (ZH)**: 理解潜隐学习：在何种条件下以及通过何种机制隐藏的偏见转移 

**Authors**: Simon Schrodi, Elias Kempf, Fazl Barez, Thomas Brox  

**Link**: [PDF](https://arxiv.org/pdf/2509.23886)  

**Abstract**: Language models can transfer hidden biases during distillation. For example, a teacher that "likes owls" can make its student "like owls" too, even when the training data consists only of lists of numbers. This surprising phenomenon is called subliminal learning. Subliminal learning can be expected under soft distillation, where the student is trained on the teacher's full next-token distribution. But the fact that this also occurs under hard distillation-where the student only sees sampled tokens-raises a deeper question: when and how does subliminal learning actually occur? We answer this question through controlled experiments and mechanistic analysis. Our results show that subliminal learning does not need (global) token entanglement or logit leakage. Instead, it comes down to a small set of divergence tokens-rare cases where teachers with different biases would predict different tokens. Masking out these tokens mostly removes the hidden bias transfer. Mechanistically, divergence tokens reveal that early layers are critical. Surprisingly, finetuning even a single such early layer is sufficient for subliminal learning. Finally, we find that subliminal learning is fragile. Even small changes, like paraphrasing prompts, are usually sufficient to suppress it. 

**Abstract (ZH)**: 语言模型在蒸馏过程中可以传递隐藏的偏见：一种称为潜意识学习的现象 

---
# Tunable-Generalization Diffusion Powered by Self-Supervised Contextual Sub-Data for Low-Dose CT Reconstruction 

**Title (ZH)**: 基于自主监督上下文子数据的可调通用扩散用于低剂量CT重建 

**Authors**: Guoquan Wei, Zekun Zhou, Liu Shi, Wenzhe Shan, Qiegen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23885)  

**Abstract**: Current models based on deep learning for low-dose CT denoising rely heavily on paired data and generalize poorly. Even the more concerned diffusion models need to learn the distribution of clean data for reconstruction, which is difficult to satisfy in medical clinical applications. At the same time, self-supervised-based methods face the challenge of significant degradation of generalizability of models pre-trained for the current dose to expand to other doses. To address these issues, this paper proposes a novel method of tunable-generalization diffusion powered by self-supervised contextual sub-data for low-dose CT reconstruction, named SuperDiff. Firstly, a contextual subdata similarity adaptive sensing strategy is designed for denoising centered on the LDCT projection domain, which provides an initial prior for the subsequent progress. Subsequently, the initial prior is used to combine knowledge distillation with a deep combination of latent diffusion models for optimizing image details. The pre-trained model is used for inference reconstruction, and the pixel-level self-correcting fusion technique is proposed for fine-grained reconstruction of the image domain to enhance the image fidelity, using the initial prior and the LDCT image as a guide. In addition, the technique is flexibly applied to the generalization of upper and lower doses or even unseen doses. Dual-domain strategy cascade for self-supervised LDCT denoising, SuperDiff requires only LDCT projection domain data for training and testing. Full qualitative and quantitative evaluations on both datasets and real data show that SuperDiff consistently outperforms existing state-of-the-art methods in terms of reconstruction and generalization performance. 

**Abstract (ZH)**: 基于自监督上下文子数据的可调泛化扩散方法：低剂量CT重建（SuperDiff） 

---
# PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications 

**Title (ZH)**: PCRI:测量企业应用中多模态模型的上下文 robustness 

**Authors**: Hitesh Laxmichand Patel, Amit Agarwal, Srikant Panda, Hansa Meghwani, Karan Dua, Paul Li, Tao Sheng, Sujith Ravi, Dan Roth  

**Link**: [PDF](https://arxiv.org/pdf/2509.23879)  

**Abstract**: The reliability of Multimodal Large Language Models (MLLMs) in real-world settings is often undermined by sensitivity to irrelevant or distracting visual context, an aspect not captured by existing evaluation metrics. We introduce the \textbf{Patch Context Robustness Index (PCRI)}, the first systematic and interpretable score for quantifying MLLM robustness to variations in visual context granularity, measuring performance changes between localized image patches and full-image input.
Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language benchmarks, we find that most leading models remain brittle to background noise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating consistent robustness across tasks. PCRI analysis also highlights how different model architectures handle and integrate visual context, offering actionable diagnostic insight for both researchers and practitioners.
PCRI enables rigorous comparison of context robustness, supporting principled model selection and guiding the development of future architectures and training strategies for robust, real-world deployment. 

**Abstract (ZH)**: 多模态大型语言模型在现实环境中的可靠性往往因对无关或分散注意力的视觉上下文的敏感性而受损，现有评估指标并未涵盖这一方面。我们提出了**Patch Context Robustness Index (PCRI)**，这是首个系统且可解释的评量指标，用于量化多模态大型语言模型对视觉上下文粒度变化的鲁棒性，通过测量局部图像 patches 和全图输入之间的性能变化来衡量。 

---
# Disentangling Score Content and Performance Style for Joint Piano Rendering and Transcription 

**Title (ZH)**: 分离评分内容和表演风格以实现联合钢琴渲染与转记 

**Authors**: Wei Zeng, Junchuan Zhao, Ye Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23878)  

**Abstract**: Expressive performance rendering (EPR) and automatic piano transcription (APT) are fundamental yet inverse tasks in music information retrieval: EPR generates expressive performances from symbolic scores, while APT recovers scores from performances. Despite their dual nature, prior work has addressed them independently. In this paper we propose a unified framework that jointly models EPR and APT by disentangling note-level score content and global performance style representations from both paired and unpaired data. Our framework is built on a transformer-based sequence-to-sequence architecture and is trained using only sequence-aligned data, without requiring fine-grained note-level alignment. To automate the rendering process while ensuring stylistic compatibility with the score, we introduce an independent diffusion-based performance style recommendation module that generates style embeddings directly from score content. This modular component supports both style transfer and flexible rendering across a range of expressive styles. Experimental results from both objective and subjective evaluations demonstrate that our framework achieves competitive performance on EPR and APT tasks, while enabling effective content-style disentanglement, reliable style transfer, and stylistically appropriate rendering. Demos are available at this https URL 

**Abstract (ZH)**: 表达性表演渲染和自动钢琴转录的统一框架：从符号乐谱中生成表达性表演与从表演中恢复乐谱是音乐信息检索中的基本且相互逆向的任务：表达性表演渲染生成表达性表演，而自动钢琴转录则从表演恢复乐谱。尽管它们是相互逆向的任务，但以往的工作却分别处理它们。本文提出一个统一框架，通过从配对和未配对数据中分离出音级乐谱内容和全局表演风格表示，联合建模表达性表演渲染和自动钢琴转录。该框架基于基于变压器的序列到序列架构，并仅使用序列对齐的数据进行训练，无需进行细粒度的音级对齐。为了自动化渲染过程并确保与乐谱风格的一致性，我们引入了一个独立的基于扩散的表演风格推荐模块，该模块可以直接从乐谱内容生成风格嵌入。该模块支持风格转移和各种表达性风格的灵活渲染。客观和主观评估实验结果表明，该框架在表达性表演渲染和自动钢琴转录任务上达到了竞争力，并且能够有效地分离内容和风格，可靠地进行风格转移，并生成风格适当的渲染。演示可在以下链接获取：this https URL 

---
# Not All Tokens are Guided Equal: Improving Guidance in Visual Autoregressive Models 

**Title (ZH)**: 不同指导token并非平等：提高视觉自回归模型中的指导效果 

**Authors**: Ky Dan Nguyen, Hoang Lam Tran, Anh-Dung Dinh, Daochang Liu, Weidong Cai, Xiuying Wang, Chang Xu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23876)  

**Abstract**: Autoregressive (AR) models based on next-scale prediction are rapidly emerging as a powerful tool for image generation, but they face a critical weakness: information inconsistencies between patches across timesteps introduced by progressive resolution scaling. These inconsistencies scatter guidance signals, causing them to drift away from conditioning information and leaving behind ambiguous, unfaithful features. We tackle this challenge with Information-Grounding Guidance (IGG), a novel mechanism that anchors guidance to semantically important regions through attention. By adaptively reinforcing informative patches during sampling, IGG ensures that guidance and content remain tightly aligned. Across both class-conditioned and text-to-image generation tasks, IGG delivers sharper, more coherent, and semantically grounded images, setting a new benchmark for AR-based methods. 

**Abstract (ZH)**: 基于下一级预测的自回归（AR）模型正在迅速成为图像生成的一个强大工具，但它们面临一个关键弱点：逐级分辨率缩放导致的时间步长内 patch 间的信息不一致性。这些不一致性分散了指导信号，使其偏离条件信息，留下模糊且不忠实的特征。我们通过一种新颖的信息接地指导机制（IGG）来应对这一挑战，该机制通过注意力机制将指导信号锚定到语义重要的区域。IGG 在采样过程中适应性地增强信息性的 patch，确保指导信号和内容保持紧密对齐。在类条件生成和文本到图像生成任务中，IGG 生成的图像更加清晰、连贯且语义相关，为基于自回归的方法设立了新的标准。 

---
# Multi-Value-Product Retrieval-Augmented Generation for Industrial Product Attribute Value Identification 

**Title (ZH)**: 基于多值产品检索增强生成的工业产品属性值识别 

**Authors**: Huike Zou, Haiyang Yang, Yindu Su, Liyu Chen, Chengbao Lian, Qingheng Zhang, Shuguang Han, Jufeng Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.23874)  

**Abstract**: Identifying attribute values from product profiles is a key task for improving product search, recommendation, and business analytics on e-commerce platforms, which we called Product Attribute Value Identification (PAVI) . However, existing PAVI methods face critical challenges, such as cascading errors, inability to handle out-of-distribution (OOD) attribute values, and lack of generalization capability. To address these limitations, we introduce Multi-Value-Product Retrieval-Augmented Generation (MVP-RAG), combining the strengths of retrieval, generation, and classification paradigms. MVP-RAG defines PAVI as a retrieval-generation task, where the product title description serves as the query, and products and attribute values act as the corpus. It first retrieves similar products of the same category and candidate attribute values, and then generates the standardized attribute values. The key advantages of this work are: (1) the proposal of a multi-level retrieval scheme, with products and attribute values as distinct hierarchical levels in PAVI domain (2) attribute value generation of large language model to significantly alleviate the OOD problem and (3) its successful deployment in a real-world industrial environment. Extensive experimental results demonstrate that MVP-RAG performs better than the state-of-the-art baselines. 

**Abstract (ZH)**: 产品属性值识别（PAVI）是从产品描述中识别属性值的关键任务，有助于电商平台的产品搜索、推荐和商业分析。然而，现有的PAVI方法面临关键挑战，如连锁错误、无法处理分布外（OOD）属性值以及缺乏泛化能力。为解决这些限制，我们引入了多值产品检索增强生成（MVP-RAG）方法，结合了检索、生成和分类 paradigm 的优势。MVP-RAG 将 PAVI 定义为一个检索-生成任务，其中产品标题描述作为查询，产品和属性值作为语料库。它首先检索相同类别相似的产品和候选属性值，然后生成标准化的属性值。本文的关键优势在于：（1）提出一个多级检索方案，产品和属性值在PAVI领域作为不同的层级；（2）通过大型语言模型生成属性值，显著缓解分布外问题；（3）成功部署于实际工业环境。广泛的实验结果表明，MVP-RAG 在与先进基线方法的对比中表现更优。 

---
# Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack 

**Title (ZH)**: 教得好了也会受骗：面向蒸馏条件后门攻击 

**Authors**: Yukun Chen, Boheng Li, Yu Yuan, Leyi Qi, Yiming Li, Tianwei Zhang, Zhan Qin, Kui Ren  

**Link**: [PDF](https://arxiv.org/pdf/2509.23871)  

**Abstract**: Knowledge distillation (KD) is a vital technique for deploying deep neural networks (DNNs) on resource-constrained devices by transferring knowledge from large teacher models to lightweight student models. While teacher models from third-party platforms may undergo security verification (\eg, backdoor detection), we uncover a novel and critical threat: distillation-conditional backdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into teacher models, which become activated in student models via the KD process, even with clean distillation datasets. While the direct extension of existing methods is ineffective for DCBA, we implement this attack by formulating it as a bilevel optimization problem and proposing a simple yet effective method (\ie, SCAR). Specifically, the inner optimization simulates the KD process by optimizing a surrogate student model, while the outer optimization leverages outputs from this surrogate to optimize the teacher model for implanting the conditional backdoor. Our SCAR addresses this complex optimization utilizing an implicit differentiation algorithm with a pre-optimized trigger injection function. Extensive experiments across diverse datasets, model architectures, and KD techniques validate the effectiveness of our SCAR and its resistance against existing backdoor detection, highlighting a significant yet previously overlooked vulnerability in the KD process. Our code is available at this https URL. 

**Abstract (ZH)**: 知识蒸馏（KD）的安全威胁：蒸馏条件下的后门攻击（DCBA） 

---
# Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation 

**Title (ZH)**: 通过解耦训练和自适应数据管理的高效多轮RL для GUI代理 

**Authors**: Pengxiang Li, Zechen Hu, Zirui Shang, Jingrong Wu, Yang Liu, Hui Liu, Zhi Gao, Chenrui Shi, Bofei Zhang, Zihao Zhang, Xiaochuan Shi, Zedong YU, Yuwei Wu, Xinxiao Wu, Yunde Jia, Liuyu Xiang, Zhaofeng He, Qing Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.23866)  

**Abstract**: Vision-language model (VLM) based GUI agents show promise for automating complex desktop and mobile tasks, but face significant challenges in applying reinforcement learning (RL): (1) slow multi-turn interactions with GUI environments for policy rollout, and (2) insufficient high-quality agent-environment interactions for policy learning. To address these challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI agents, which coordinates heterogeneous modules in a highly decoupled manner. DART separates the training system into four asynchronous modules: environment cluster, rollout service, data manager, and trainer. This design enables non-blocking communication, asynchronous training, rollout-wise trajectory sampling, and per-worker model synchronization, significantly improving the system efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput, and 5.5* environment utilization. To facilitate effective learning from abundant samples, we introduce an adaptive data curation scheme: (1) pre-collecting successful trajectories for challenging tasks to supplement sparse success in online sampling; (2) dynamically adjusting rollout numbers and trajectory lengths based on task difficulty; (3) training selectively on high-entropy steps to prioritize critical decisions; (4) stabilizing learning via truncated importance sampling for policy mismatch between policy rollout and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task success rate, a 14.61% absolute gain over the base model, and 7.34% higher than open-source SOTA. We will fully open-source our training framework, data, and model checkpoints via this http URL, which we believe is a timely contribution to the open-source community of agentic RL training. 

**Abstract (ZH)**: 基于视觉语言模型的GUI代理在自动化复杂桌面和移动任务方面显示出前景，但在应用强化学习方面面临重大挑战：(1) 与GUI环境进行多轮交互的效率低下，(2) 用于策略学习的代理-环境交互不足。为应对这些挑战，我们提出了一种名为DART的分阶代理强化学习训练框架，该框架以高度解耦的方式协调异构模块。DART将训练系统分解为四个异步模块：环境集群、运维服务、数据管理和训练器。这种设计实现了非阻塞通信、异步训练、轨迹采样以及按工作进程同步模型，显著提高了系统效率：每轮交互的GPU利用率提高1.6倍，训练吞吐量提高1.9倍，环境利用率提高5.5倍。为了有效利用丰富的样本进行学习，我们引入了一种自适应数据整理方案：(1) 在线采样前预先收集困难任务的成功轨迹，补充稀疏的成功样本；(2) 根据任务难度动态调整轮次数量和轨迹长度；(3) 选择性地在高熵步骤上进行训练，优先处理关键决策；(4) 通过截断重要性采样来稳定学习，解决策略轮播和更新之间的不匹配问题。在OSWorld基准测试中，DART-GUI-7B实现了42.13%的任务成功率，相对于基线模型绝对提升14.61%，并且高于开源SOTA模型7.34%。我们将在以下网址全面开源我们的训练框架、数据和模型检查点，我们认为这是一项对代理强化学习训练开源社区的及时贡献。 

---
# GSID: Generative Semantic Indexing for E-Commerce Product Understanding 

**Title (ZH)**: GSID: 生成语义索引以理解电子商务产品 

**Authors**: Haiyang Yang, Qinye Xie, Qingheng Zhang, Liyu Chen, Huike Zou, Chengbao Lian, Shuguang Han, Fei Huang, Jufeng Chen, Bo Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2509.23860)  

**Abstract**: Structured representation of product information is a major bottleneck for the efficiency of e-commerce platforms, especially in second-hand ecommerce platforms. Currently, most product information are organized based on manually curated product categories and attributes, which often fail to adequately cover long-tail products and do not align well with buyer preference. To address these problems, we propose \textbf{G}enerative \textbf{S}emantic \textbf{I}n\textbf{D}exings (GSID), a data-driven approach to generate product structured representations. GSID consists of two key components: (1) Pre-training on unstructured product metadata to learn in-domain semantic embeddings, and (2) Generating more effective semantic codes tailored for downstream product-centric applications. Extensive experiments are conducted to validate the effectiveness of GSID, and it has been successfully deployed on the real-world e-commerce platform, achieving promising results on product understanding and other downstream tasks. 

**Abstract (ZH)**: 基于生成语义索引的电子商务产品结构化表示 

---
# Adversarial Diffusion for Robust Reinforcement Learning 

**Title (ZH)**: 对抗扩散以实现鲁棒的强化学习 

**Authors**: Daniele Foffano, Alessio Russo, Alexandre Proutiere  

**Link**: [PDF](https://arxiv.org/pdf/2509.23846)  

**Abstract**: Robustness to modeling errors and uncertainties remains a central challenge in reinforcement learning (RL). In this work, we address this challenge by leveraging diffusion models to train robust RL policies. Diffusion models have recently gained popularity in model-based RL due to their ability to generate full trajectories "all at once", mitigating the compounding errors typical of step-by-step transition models. Moreover, they can be conditioned to sample from specific distributions, making them highly flexible. We leverage conditional sampling to learn policies that are robust to uncertainty in environment dynamics. Building on the established connection between Conditional Value at Risk (CVaR) optimization and robust RL, we introduce Adversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides the diffusion process to generate worst-case trajectories during training, effectively optimizing the CVaR of the cumulative return. Empirical results across standard benchmarks show that AD-RRL achieves superior robustness and performance compared to existing robust RL methods. 

**Abstract (ZH)**: 差分模型在强化学习中的鲁棒性研究：对抗差分用于稳健强化学习（Adversarial Diffusion for Robust Reinforcement Learning, AD-RRL） 

---
# HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing 

**Title (ZH)**: HFuzzer：基于短语级 fuzzing 测试大型语言模型的包幻觉 

**Authors**: Yukai Zhao, Menghan Wu, Xing Hu, Xin Xia  

**Link**: [PDF](https://arxiv.org/pdf/2509.23835)  

**Abstract**: Large Language Models (LLMs) are widely used for code generation, but they face critical security risks when applied to practical production due to package hallucinations, in which LLMs recommend non-existent packages. These hallucinations can be exploited in software supply chain attacks, where malicious attackers exploit them to register harmful packages. It is critical to test LLMs for package hallucinations to mitigate package hallucinations and defend against potential attacks. Although researchers have proposed testing frameworks for fact-conflicting hallucinations in natural language generation, there is a lack of research on package hallucinations. To fill this gap, we propose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for package hallucinations. HFUZZER adopts fuzzing technology and guides the model to infer a wider range of reasonable information based on phrases, thereby generating enough and diverse coding tasks. Furthermore, HFUZZER extracts phrases from package information or coding tasks to ensure the relevance of phrases and code, thereby improving the relevance of generated tasks and code. We evaluate HFUZZER on multiple LLMs and find that it triggers package hallucinations across all selected models. Compared to the mutational fuzzing framework, HFUZZER identifies 2.60x more unique hallucinated packages and generates more diverse tasks. Additionally, when testing the model GPT-4o, HFUZZER finds 46 unique hallucinated packages. Further analysis reveals that for GPT-4o, LLMs exhibit package hallucinations not only during code generation but also when assisting with environment configuration. 

**Abstract (ZH)**: 大型语言模型（LLMs）在代码生成中广泛应用，但由于包幻想问题，它们在实际生产中面临严重安全风险。包幻想会导致恶意攻击者在软件供应链攻击中利用这些幻想注册有害包。测试LLMs以检测包幻想、减轻包幻想和抵御潜在攻击至关重要。尽管研究人员提出了针对自然语言生成中事实相矛盾幻想的测试框架，但关于包幻想的研究仍然不足。为填补这一空白，我们提出了HFUZZER，这是一种新颖的短语基础模糊测试框架，用于测试LLMs中的包幻想。HFUZZER采用模糊测试技术，引导模型基于短语推断更广泛的相关信息，从而生成足够的多样化的编码任务。此外，HFUZZER从包信息或编码任务中提取短语，确保短语和代码的相关性，从而提高生成任务和代码的相关性。我们对多种LLMs进行了评估，发现HFUZZER能够在所有选定的模型中触发包幻想。与变异模糊测试框架相比，HFUZZER识别出2.60倍以上的独特幻想包，并生成更多样化的任务。此外，当测试GPT-4o模型时，HFUZZER发现了46个独特幻想包。进一步分析显示，对于GPT-4o，LLMs不仅在代码生成时表现出包幻想，在环境配置辅助时也存在包幻想。 

---
# Space Group Conditional Flow Matching 

**Title (ZH)**: 空间群条件流匹配 

**Authors**: Omri Puny, Yaron Lipman, Benjamin Kurt Miller  

**Link**: [PDF](https://arxiv.org/pdf/2509.23822)  

**Abstract**: Inorganic crystals are periodic, highly-symmetric arrangements of atoms in three-dimensional space. Their structures are constrained by the symmetry operations of a crystallographic \emph{space group} and restricted to lie in specific affine subspaces known as \emph{Wyckoff positions}. The frequency an atom appears in the crystal and its rough positioning are determined by its Wyckoff position. Most generative models that predict atomic coordinates overlook these symmetry constraints, leading to unrealistically high populations of proposed crystals exhibiting limited symmetry. We introduce Space Group Conditional Flow Matching, a novel generative framework that samples significantly closer to the target population of highly-symmetric, stable crystals. We achieve this by conditioning the entire generation process on a given space group and set of Wyckoff positions; specifically, we define a conditionally symmetric noise base distribution and a group-conditioned, equivariant, parametric vector field that restricts the motion of atoms to their initial Wyckoff position. Our form of group-conditioned equivariance is achieved using an efficient reformulation of \emph{group averaging} tailored for symmetric crystals. Importantly, it reduces the computational overhead of symmetrization to a negligible level. We achieve state of the art results on crystal structure prediction and de novo generation benchmarks. We also perform relevant ablations. 

**Abstract (ZH)**: 无机晶体是三维空间中具有周期性和高对称性的原子排列。它们的结构受到晶体学空间群的对称操作约束，并限定在特定的仿射子空间即沃克夫位置中。原子在晶体中的出现频率及其大致位置由其沃克夫位置决定。大多数用于预测原子坐标生成模型未考虑这些对称约束，导致生成的晶体表现出有限对称性的不切实际高比例。我们提出了一种空间群条件流匹配生成框架，该框架通过基于给定空间群和沃克夫位置对整个生成过程进行条件化，显著地接近目标群体的高对称性和稳定晶体。我们通过定义条件对称噪声基分布和群条件下的守恒参数向量场，限制原子运动到其初始沃克夫位置来实现这一点。我们形式下的群条件下的守恒性是通过针对对称晶体优化的群平均的一种高效重写实现的。重要的是，它将对称化的计算开销降低到了可以忽略的水平。我们在晶体结构预测和从头生成基准测试中达到了最先进的结果，并进行了相关的消融实验。 

---
# A Multi-Camera Vision-Based Approach for Fine-Grained Assembly Quality Control 

**Title (ZH)**: 基于多相机视觉的细粒度装配质量控制方法 

**Authors**: Ali Nazeri, Shashank Mishra, Achim Wagner, Martin Ruskowski, Didier Stricker, Jason Rambach  

**Link**: [PDF](https://arxiv.org/pdf/2509.23815)  

**Abstract**: Quality control is a critical aspect of manufacturing, particularly in ensuring the proper assembly of small components in production lines. Existing solutions often rely on single-view imaging or manual inspection, which are prone to errors due to occlusions, restricted perspectives, or lighting inconsistencies. These limitations require the installation of additional inspection stations, which could disrupt the assembly line and lead to increased downtime and costs. This paper introduces a novel multi-view quality control module designed to address these challenges, integrating a multi-camera imaging system with advanced object detection algorithms. By capturing images from three camera views, the system provides comprehensive visual coverage of components of an assembly process. A tailored image fusion methodology combines results from multiple views, effectively resolving ambiguities and enhancing detection reliability. To support this system, we developed a unique dataset comprising annotated images across diverse scenarios, including varied lighting conditions, occlusions, and angles, to enhance applicability in real-world manufacturing environments. Experimental results show that our approach significantly outperforms single-view methods, achieving high precision and recall rates in the identification of improperly fastened small assembly parts such as screws. This work contributes to industrial automation by overcoming single-view limitations, and providing a scalable, cost-effective, and accurate quality control mechanism that ensures the reliability and safety of the assembly line. The dataset used in this study is publicly available to facilitate further research in this domain. 

**Abstract (ZH)**: 多视图质量控制模块在制造中的应用：克服单视角限制，提供可扩展、成本-effective且准确的质量控制机制以确保装配线的可靠性和安全性 

---
# IndexNet: Timestamp and Variable-Aware Modeling for Time Series Forecasting 

**Title (ZH)**: IndexNet: 考虑时间戳和变量的时间序列预测建模 

**Authors**: Beiliang Wu, Peiyuan Liu, Yifan Hu, Luyan Zhang, Ao Hu, Zenglin Xu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23813)  

**Abstract**: Multivariate time series forecasting (MTSF) plays a vital role in a wide range of real-world applications, such as weather prediction and traffic flow forecasting. Although recent advances have significantly improved the modeling of temporal dynamics and inter-variable dependencies, most existing methods overlook index-related descriptive information, such as timestamps and variable indices, which carry rich contextual semantics. To unlock the potential of such information and take advantage of the lightweight and powerful periodic capture ability of MLP-based architectures, we propose IndexNet, an MLP-based framework augmented with an Index Embedding (IE) module. The IE module consists of two key components: Timestamp Embedding (TE) and Channel Embedding (CE). Specifically, TE transforms timestamps into embedding vectors and injects them into the input sequence, thereby improving the model's ability to capture long-term complex periodic patterns. In parallel, CE assigns each variable a unique and trainable identity embedding based on its index, allowing the model to explicitly distinguish between heterogeneous variables and avoid homogenized predictions when input sequences seem close. Extensive experiments on 12 diverse real-world datasets demonstrate that IndexNet achieves comparable performance across mainstream baselines, validating the effectiveness of our temporally and variably aware design. Moreover, plug-and-play experiments and visualization analyses further reveal that IndexNet exhibits strong generality and interpretability, two aspects that remain underexplored in current MTSF research. 

**Abstract (ZH)**: 多变量时间序列 forecasting (MTSF) 在天气预测和交通流预测等广泛的实际应用中起着关键作用。尽管近年来的方法显著提高了对时间动态和变量间依赖关系的建模能力，但大多数现有方法忽略了与索引相关的描述性信息，如时间戳和变量索引，这些信息富含丰富的上下文语义。为充分利用此类信息，并利用基于MLP架构的轻量级且强大的周期捕获能力，我们提出了IndexNet，一种增强有索引嵌入 (IE) 模块的MLP框架。IE模块包含两个关键组件：时间戳嵌入 (TE) 和通道嵌入 (CE)。具体来说，TE将时间戳转换为嵌入向量并注入输入序列，从而增强模型捕捉长期复杂周期模式的能力。同时，CE根据变量索引为每个变量分配一个独特的可训练身份嵌入，使模型能够明确区分异质变量并避免在输入序列看似相近时产生同质预测。在12个多样化的实际数据集上的广泛实验表明，IndexNet在主流基线中取得了可比的性能，验证了我们具有时间和变量感知设计的有效性。此外，模块化实验和可视化分析进一步揭示了IndexNet的强大通用性和可解释性，这两个方面在当前时间序列预测研究中尚未充分探索。 

---
# Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models 

**Title (ZH)**: 穿越迷宫：基于路径敏感的单元测试生成方法与大型语言模型 

**Authors**: Dianshu Liao, Xin Yin, Shidong Pan, Chao Ni, Zhenchang Xing, Xiaoyu Sun  

**Link**: [PDF](https://arxiv.org/pdf/2509.23812)  

**Abstract**: Unit testing is essential for software quality assurance, yet writing and maintaining tests remains time-consuming and error-prone. To address this challenge, researchers have proposed various techniques for automating unit test generation, including traditional heuristic-based methods and more recent approaches that leverage large language models (LLMs). However, these existing approaches are inherently path-insensitive because they rely on fixed heuristics or limited contextual information and fail to reason about deep control-flow structures. As a result, they often struggle to achieve adequate coverage, particularly for deep or complex execution paths. In this work, we present a path-sensitive framework, JUnitGenie, to fill this gap by combining code knowledge with the semantic capabilities of LLMs in guiding context-aware unit test generation. After extracting code knowledge from Java projects, JUnitGenie distills this knowledge into structured prompts to guide the generation of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex focal methods from ten real-world Java projects. The results show that JUnitGenie generates valid tests and improves branch and line coverage by 29.60% and 31.00% on average over both heuristic and LLM-based baselines. We further demonstrate that the generated test cases can uncover real-world bugs, which were later confirmed and fixed by developers. 

**Abstract (ZH)**: 基于路径敏感的框架JUnitGenie：结合代码知识和大规模语言模型Semantic能力指导上下文感知的单元测试生成 

---
# Tequila: Trapping-free Ternary Quantization for Large Language Models 

**Title (ZH)**: Tequila: 无约束三元量化大型语言模型 

**Authors**: Hong Huang, Decheng Wu, Rui Cen, Guanghua Yu, Zonghang Li, Kai Liu, Jianchen Zhu, Peng Chen, Xue Liu, Dapeng Wu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23809)  

**Abstract**: Quantization techniques are essential for the deployment of Large Language Models (LLMs) on edge devices. However, prevailing methods often rely on mixed-precision multiplication that lacks efficient hardware support, making it not feasible. Ternary weight quantization addresses this by constraining weights to {-1, 0, 1}, replacing expensive multiplications with hardware-efficient additions. However, such aggressive compression leads to significant accuracy degradation, even after costly quantization-aware training with massive data. We identify the core issue as deadzone trapping: a large number of weights are trapped at the deadzone boundary. This occurs because these weights receive only noisy, uninformative gradients, preventing stable escape from the deadzone and severely impeding model capacity and optimization. To address this issue, we propose Tequila, a trapping-free quantization optimization method that reactivates deadzone-trapped weights by repurposing them as dynamic biases. This allows the repurposed weights to provide a continuous signal in the forward pass and, critically, receive direct, meaningful gradient signals during backpropagation, thereby enhancing model capacity and optimization with nearly zero inference overhead. Extensive evaluations demonstrate that Tequila outperforms state-of-the-art (SOTA) ternary quantization methods across five benchmarks. Specifically, on the ARC benchmark, it achieves >4% accuracy gain over the SOTA baseline, nearly matching full-precision performance (within <1% gap) with a 3.0x inference speedup. Consequently, Tequila offers a highly practical and efficient implementation for the deployment of advanced LLMs in resource-constrained environments. The code is available at this https URL. 

**Abstract (ZH)**: 三元权重量化优化方法：Tequila 

---
# FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents 

**Title (ZH)**: FedAgentBench: 向自动化现实世界联邦医疗图像分析方向迈进，基于服务器-客户端LLM代理 

**Authors**: Pramit Saha, Joshua Strong, Divyanshu Mishra, Cheng Ouyang, J.Alison Noble  

**Link**: [PDF](https://arxiv.org/pdf/2509.23803)  

**Abstract**: Federated learning (FL) allows collaborative model training across healthcare sites without sharing sensitive patient data. However, real-world FL deployment is often hindered by complex operational challenges that demand substantial human efforts. This includes: (a) selecting appropriate clients (hospitals), (b) coordinating between the central server and clients, (c) client-level data pre-processing, (d) harmonizing non-standardized data and labels across clients, and (e) selecting FL algorithms based on user instructions and cross-client data characteristics. However, the existing FL works overlook these practical orchestration challenges. These operational bottlenecks motivate the need for autonomous, agent-driven FL systems, where intelligent agents at each hospital client and the central server agent collaboratively manage FL setup and model training with minimal human intervention. To this end, we first introduce an agent-driven FL framework that captures key phases of real-world FL workflows from client selection to training completion and a benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to autonomously coordinate healthcare FL. Our framework incorporates 40 FL algorithms, each tailored to address diverse task-specific requirements and cross-client characteristics. Furthermore, we introduce a diverse set of complex tasks across 201 carefully curated datasets, simulating 6 modality-specific real-world healthcare environments, viz., Dermatoscopy, Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic performance of 14 open-source and 10 proprietary LLMs spanning small, medium, and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3 can automate various stages of the FL pipeline, our results reveal that more complex, interdependent tasks based on implicit goals remain challenging for even the strongest models. 

**Abstract (ZH)**: 联邦学习（FL）允许在不共享敏感患者数据的情况下，在医疗健康站点之间协作训练模型。然而，现实世界的FL部署经常受到复杂操作挑战的阻碍，这些挑战需要大量的人力投入。这包括：(a) 选择合适的客户端（医院），(b) 中央服务器与客户端之间的协调，(c) 客户端级数据预处理，(d) 跨客户端标准化数据和标签的协调，以及(e) 根据用户指令和跨客户端数据特性选择FL算法。然而，现有的FL工作忽视了这些实际协调挑战。这些操作瓶颈促使了自主、代理驱动的FL系统的需要，其中每个医院客户端和中央服务器的智能代理协作管理FL设置和模型训练，最大限度地减少人为干预。为此，我们首先介绍了一个代理驱动的FL框架，涵盖了从客户端选择到训练完成的真实世界FL工作流程的关键阶段，并引入了一个名为FedAgentBench的基准，评估LLM代理自主协调医疗健康FL的能力。我们的框架包括40种定制的FL算法，以解决各种特定任务和跨客户端特性的需求。此外，我们引入了201个精心选择的数据集上的一系列复杂任务，模拟了6种特定模态的真实世界医疗健康环境，即皮肤镜检查、超声波、眼底成像、病理学、磁共振成像和X射线。我们评估了14种开源和10种专有的LLM在小、中、大型模型规模上的代理性能。虽然一些代理核心如GPT-4.1和DeepSeek V3可以自动化FL管道的各个阶段，但我们的结果显示，基于隐含目标的更复杂、更相互依赖的任务即使对于最强的模型来说也仍然是具有挑战性的。 

---
# Enhancing LLM Steering through Sparse Autoencoder-Based Vector Refinement 

**Title (ZH)**: 通过稀疏自编码器基于的向量精炼增强LLM导航 

**Authors**: Anyi Wang, Xuansheng Wu, Dong Shu, Yunpu Ma, Ninghao Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23799)  

**Abstract**: Steering has emerged as a promising approach in controlling large language models (LLMs) without modifying model parameters. However, most existing steering methods rely on large-scale datasets to learn clear behavioral information, which limits their applicability in many real-world scenarios. The steering vectors extracted from small dataset often contain task-irrelevant noising features, which degrades their effectiveness. To refine the steering vectors learned from limited data, we introduce Refinement of Steering Vector via Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise and augment the steering vectors. In our framework, we first remove task-irrelevant features according to their semantics provided by SAEs, and then enrich task-relevant features missing from the small dataset through their semantic similarity to the identified relevant features. Extensive experiments demonstrate that the proposed SAE-RSV substantially outperforms all the baseline methods including supervised fine-tuning. Our findings show that effective steering vector can be constructed from limited training data by refining the original steering vector through SAEs. 

**Abstract (ZH)**: 基于稀疏自编码器的Steering向量精炼方法：从稀疏数据中构建有效的控制向量 

---
# From Unstable to Playable: Stabilizing Angry Birds Levels via Object Segmentation 

**Title (ZH)**: 从不稳定到可玩：通过对象分割稳定《愤怒的小鸟》关卡 

**Authors**: Mahdi Farrokhimaleki, Parsa Rahmati, Richard Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2509.23787)  

**Abstract**: Procedural Content Generation (PCG) techniques enable automatic creation of diverse and complex environments. While PCG facilitates more efficient content creation, ensuring consistently high-quality, industry-standard content remains a significant challenge. In this research, we propose a method to identify and repair unstable levels generated by existing PCG models. We use Angry Birds as a case study, demonstrating our method on game levels produced by established PCG approaches. Our method leverages object segmentation and visual analysis of level images to detect structural gaps and perform targeted repairs. We evaluate multiple object segmentation models and select the most effective one as the basis for our repair pipeline. Experimental results show that our method improves the stability and playability of AI-generated levels. Although our evaluation is specific to Angry Birds, our image-based approach is designed to be applicable to a wide range of 2D games with similar level structures. 

**Abstract (ZH)**: 基于图像的Procedural Content Generation模型生成不稳定关卡的识别与修复方法 

---
# GroupCoOp: Group-robust Fine-tuning via Group Prompt Learning 

**Title (ZH)**: GroupCoOp: 组群稳健微调通过组提示学习 

**Authors**: Nayeong Kim, Seong Joon Oh, Suha Kwak  

**Link**: [PDF](https://arxiv.org/pdf/2509.23781)  

**Abstract**: Parameter-efficient fine-tuning (PEFT) of vision-language models (VLMs) excels in various vision tasks thanks to the rich knowledge and generalization ability of VLMs. However, recent studies revealed that such fine-tuned VLMs are vulnerable to spurious correlations stemming from the subgroup imbalance in the fine-tuning datasets. To resolve this issue, we propose Group Context Optimization (GroupCoOp), a simple and effective debiased fine-tuning algorithm that enhances the group robustness of fine-tuned VLMs. Its key idea is to employ group-specific text prompts as group representatives serving as multiple classifiers for their target class. The rich semantic knowledge of the text encoder of VLM enables the discovery of effective group prompts even for groups with a small number of training samples. Leveraging the group prompts for each class addresses the issues caused by the group-imbalanced training set, such as the neglect of minority groups and the scattered distribution of each class in the embedding space. GroupCoOp achieved the best results on five benchmarks across five CLIP architectures and occasionally outperformed prior methods that fine-tune the entire network, despite training only 0.016\% of the network's parameters. 

**Abstract (ZH)**: Group Context Optimization (GroupCoOp): A Simple and Effective Debiasing Fine-Tuning Algorithm for Vision-Language Models 

---
# Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse 

**Title (ZH)**: 仓库中多-agent拾取与交付的序列探索者 

**Authors**: Zeyuan Zhang, Chaoran Li, Shao Zhang, Ying Wen  

**Link**: [PDF](https://arxiv.org/pdf/2509.23778)  

**Abstract**: Multi-Agent Pickup and Delivery (MAPD) is a challenging extension of Multi-Agent Path Finding (MAPF), where agents are required to sequentially complete tasks with fixed-location pickup and delivery demands. Although learning-based methods have made progress in MAPD, they often perform poorly in warehouse-like environments with narrow pathways and long corridors when relying only on local observations for distributed decision-making. Communication learning can alleviate the lack of global information but introduce high computational complexity due to point-to-point communication. To address this challenge, we formulate MAPF as a sequence modeling problem and prove that path-finding policies under sequence modeling possess order-invariant optimality, ensuring its effectiveness in MAPD. Building on this, we propose the Sequential Pathfinder (SePar), which leverages the Transformer paradigm to achieve implicit information exchange, reducing decision-making complexity from exponential to linear while maintaining efficiency and global awareness. Experiments demonstrate that SePar consistently outperforms existing learning-based methods across various MAPF tasks and their variants, and generalizes well to unseen environments. Furthermore, we highlight the necessity of integrating imitation learning in complex maps like warehouses. 

**Abstract (ZH)**: 多代理取送任务（MAPD）是多代理路径规划（MAPF）的一个具有挑战性的扩展，在其中代理需要顺序完成固定位置的取送任务。尽管基于学习的方法在MAPD领域取得了进展，但在依赖局部观察进行分布式决策的仓库-like环境中，它们通常表现不佳，尤其是在狭窄通道和长走廊的环境下。通信学习可以缓解缺乏全局信息的问题，但由于点对点通信导致计算复杂性增加。为解决这一挑战，我们将MAPF形式化为序列建模问题，并证明在序列建模下的路径规划策略具有顺序不变的最优性，确保其在MAPD中的有效性。在此基础上，我们提出了序列路径规划者（SePar），它利用Transformer范式实现隐式信息交换，将决策复杂性从指数级降低到线性级，同时保持高效性和全局意识。实验表明，SePar在各种MAPF任务及其变体中表现优异，并且能够很好地泛化到未见过的环境中。此外，我们强调在复杂地图（如仓库）中整合模仿学习的必要性。 

---
# Knowledge Homophily in Large Language Models 

**Title (ZH)**: 大型语言模型中的知识同质性 

**Authors**: Utkarsh Sahu, Zhisheng Qi, Mahantesh Halappanavar, Nedim Lipka, Ryan A. Rossi, Franck Dernoncourt, Yu Zhang, Yao Ma, Yu Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23773)  

**Abstract**: Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering. 

**Abstract (ZH)**: 大规模语言模型（LLMs）作为支持知识密集型应用如问答和事实核查的神经知识库的研究日益增多。然而，它们知识结构的组织方式尚未被探索。受认知神经科学发现的启发，如语义聚类和线索效应，即知道一个事实会增加回忆相关事实的可能性，我们研究了LLMs中的类似知识同质性模式。为此，我们通过知识检查将LLM知识映射到图表示中，分别在三元组和实体层面进行。随后，我们分析了实体与其邻居之间的知识能力关系，发现LLMs倾向于在图上位置更近的实体具有相似的知识水平。受这一同质性原理的启发，我们提出了一种图神经网络（GNN）回归模型，通过利用其邻居评分来估计三元组的实体层面知识能力评分。预测的知识能力使我们能够优先检查知识较少的三元组，从而在相同的注标预算下最大化知识覆盖。这不仅提高了为细调将知识注入LLMs时的主动注标效率，还增强了在推理密集型问答中多跳路径检索的能力。 

---
# From Personal to Collective: On the Role of Local and Global Memory in LLM Personalization 

**Title (ZH)**: 从个人到集体：局部记忆与全局记忆在大语言模型个性化中的作用 

**Authors**: Zehong Wang, Junlin Wu, ZHaoxuan Tan, Bolian Li, Xianrui Zhong, Zheli Liu, Qingkai Zeng  

**Link**: [PDF](https://arxiv.org/pdf/2509.23767)  

**Abstract**: Large language model (LLM) personalization aims to tailor model behavior to individual users based on their historical interactions. However, its effectiveness is often hindered by two key challenges: the \textit{cold-start problem}, where users with limited history provide insufficient context for accurate personalization, and the \textit{biasing problem}, where users with abundant but skewed history cause the model to overfit to narrow preferences. We identify both issues as symptoms of a common underlying limitation, i.e., the inability to model collective knowledge across users. To address this, we propose a local-global memory framework (LoGo) that combines the personalized local memory with a collective global memory that captures shared interests across the population. To reconcile discrepancies between these two memory sources, we introduce a mediator module designed to resolve conflicts between local and global signals. Extensive experiments on multiple benchmarks demonstrate that LoGo consistently improves personalization quality by both warming up cold-start users and mitigating biased predictions. These results highlight the importance of incorporating collective knowledge to enhance LLM personalization. 

**Abstract (ZH)**: 大规模语言模型个性化旨在根据用户的历史交互行为定制模型行为。然而，其有效性往往受到两个关键挑战的阻碍：冷启动问题，即历史有限的用户提供的上下文不足，难以进行准确个性化；以及偏差问题，即历史丰富但有所偏倚的用户使模型过度适应狭隘的偏好。我们识别这两个问题为共同基础限制的症状，即无法建模用户间的集体知识。为此，我们提出了一种局部-全局记忆框架（LoGo），该框架结合了个性化局部记忆和一个捕获人口共享兴趣的全局记忆模块。为了统一这两种记忆来源之间的差异，我们引入了一个调解模块，旨在解决局部和全局信号之间的冲突。在多个基准上的广泛实验表明，LoGo能够通过预热冷启动用户和缓解偏差预测的一致提高个性化质量。这些结果突显了在增强大规模语言模型个性化中融入集体知识的重要性。 

---
# Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality 

**Title (ZH)**: 知识层面一致性强化学习：长篇事实对齐的双事实一致性方法 

**Authors**: Junliang Li, Yucheng Wang, Yan Chen, Yu Ran, Ruiqing Zhang, Jing Liu, Hua Wu, Haifeng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23765)  

**Abstract**: Hallucination and factuality deficits remain key obstacles to the reliability of large language models (LLMs) in long-form generation. Existing reinforcement learning from human feedback (RLHF) frameworks primarily rely on preference rewards, yet they often overlook the model's internal knowledge boundaries, exacerbating the so-called "hallucination tax". To address this challenge, we propose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a novel framework that focuses on the knowledge consistency between the policy model's expressed knowledge and the base model's parametric knowledge, and introduces a Dual-Fact Alignment mechanism to jointly optimize factual recall and precision. Specifically, KLCF leverages pretrained knowledge boundaries to construct fact checklist, guiding online reinforcement learning to improve factual coverage and recall; simultaneously, it trains a self-assessment module based on the base model's internal knowledge to enhance factual precision during generation. Unlike prior methods that rely on external retrieval or heavy verification, our reward design is fully external-knowledge-free and lightweight, making KLCF efficient and easily scalable to large-scale training. Experimental results demonstrate that KLCF substantially improves factuality metrics across multiple long-form benchmarks and effectively alleviates model hallucinations. 

**Abstract (ZH)**: 大语言模型（LLMs）在长文生成中的幻觉和事实性缺陷仍是可靠性的主要障碍。现有的基于人类反馈的强化学习（RLHF）框架主要依赖偏好奖励，但往往忽视了模型的内部知识边界，加剧了所谓的“幻觉税”。为应对这一挑战，我们提出了一种新的框架——知识级一致性强化学习框架（KLCF），该框架关注策略模型表达的知识与基模型参数知识之间的一致性，并引入了双事实对齐机制以联合优化事实召回率和精确度。具体而言，KLCF 利用预训练的知识边界构建事实清单，引导在线强化学习以提高事实覆盖度和召回率；同时，基于基模型的内部知识训练自我评估模块，以在生成过程中增强事实精确度。与依赖外部检索或重验证的先前方法不同，我们的奖励设计完全不需要外部知识且轻量级，使得 KLCF 高效且易于大规模训练。实验结果表明，KLCF 在多个长文基准测试中显著提高了事实性指标，并有效缓解了模型的幻觉现象。 

---
# Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail 

**Title (ZH)**: 基于尖峰神经网络梯度稀疏性權衡的精度-稳健性 TRADE-OFF via Spiking Neural Network Gradient Sparsity Trail 

**Authors**: Nhan T. Luu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23762)  

**Abstract**: Spiking Neural Networks (SNNs) have attracted growing interest in both computational neuroscience and artificial intelligence, primarily due to their inherent energy efficiency and compact memory footprint. However, achieving adversarial robustness in SNNs, particularly for vision-related tasks, remains a nascent and underexplored challenge. Recent studies have proposed leveraging sparse gradients as a form of regularization to enhance robustness against adversarial perturbations. In this work, we present a surprising finding: under specific architectural configurations, SNNs exhibit natural gradient sparsity and can achieve state-of-the-art adversarial defense performance without the need for any explicit regularization. Further analysis reveals a trade-off between robustness and generalization: while sparse gradients contribute to improved adversarial resilience, they can impair the model's ability to generalize; conversely, denser gradients support better generalization but increase vulnerability to attacks. 

**Abstract (ZH)**: 脉冲神经网络（SNNs）在计算神经科学和人工智能领域引起了广泛关注，主要是由于其固有的能源效率和紧凑的内存占用。然而，特别是在视觉任务中实现对抗鲁棒性仍然是一个新兴且尚未充分探索的挑战。最近的研究提出，利用稀疏梯度作为正则化的一种形式，以增强对对抗性扰动的鲁棒性。在本工作中，我们提出一个令人惊讶的发现：在特定的架构配置下，SNNs表现出自然的梯度稀疏性，并且在不需要任何显式正则化的情况下，可以达到最先进的对抗防御性能。进一步的分析揭示了鲁棒性和泛化的权衡：虽然稀疏梯度有助于提高对抗性鲁棒性，但会影响模型的泛化能力；相反，稠密的梯度支持更好的泛化，但也增加了模型对攻击的脆弱性。 

---
# SHAPoint: Task-Agnostic, Efficient, and Interpretable Point-Based Risk Scoring via Shapley Values 

**Title (ZH)**: SHAPoint: 任务无关、高效且可解释的基于点的风险评分方法通过Shapley值 

**Authors**: Tomer D. Meirman, Bracha Shapira, Noa Dagan, Lior S. Rokach  

**Link**: [PDF](https://arxiv.org/pdf/2509.23756)  

**Abstract**: Interpretable risk scores play a vital role in clinical decision support, yet traditional methods for deriving such scores often rely on manual preprocessing, task-specific modeling, and simplified assumptions that limit their flexibility and predictive power. We present SHAPoint, a novel, task-agnostic framework that integrates the predictive accuracy of gradient boosted trees with the interpretability of point-based risk scores. SHAPoint supports classification, regression, and survival tasks, while also inheriting valuable properties from tree-based models, such as native handling of missing data and support for monotonic constraints. Compared to existing frameworks, SHAPoint offers superior flexibility, reduced reliance on manual preprocessing, and faster runtime performance. Empirical results show that SHAPoint produces compact and interpretable scores with predictive performance comparable to state-of-the-art methods, but at a fraction of the runtime, making it a powerful tool for transparent and scalable risk stratification. 

**Abstract (ZH)**: 可解释的风险评分在临床决策支持中发挥着重要作用，但传统方法常依赖于手动预处理、任务特定建模和简化假设，这限制了其灵活性和预测能力。我们提出了一种名为SHAPoint的新型、任务无关框架，该框架结合了梯度提升树的预测准确性和点基风险评分的可解释性。SHAPoint支持分类、回归和生存任务，同时继承了基于树模型的天然缺失数据处理能力和单调约束支持。与现有框架相比，SHAPoint提供了更高的灵活性、减少了对手动预处理的依赖，并具有更快的运行时性能。实证结果表明，SHAPoint产生的紧凑且具有解释性的评分在预测性能上与最先进的方法相当，但运行时间却大幅缩减，使其成为一种强大的透明且可扩展的风险分层工具。 

---
# Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis 

**Title (ZH)**: 通过参数重要性分析理解语音LLM中文本能力退化 

**Authors**: Chao Wang, Rui-Chen Zheng, Yang Ai, Zhen-Hua Ling  

**Link**: [PDF](https://arxiv.org/pdf/2509.23755)  

**Abstract**: The integration of speech into Large Language Models (LLMs) has substantially expanded their capabilities, but often at the cost of weakening their core textual competence. This degradation limits the ability of speech-enabled LLMs to fully exploit their pre-trained text-based knowledge. In this work, we analyze the underlying mechanisms of this issue through a focused study of the widely used encoder-adaptor paradigm. We propose an analytical framework based on parameter importance estimation, which reveals that fine-tuning for speech introduces a textual importance distribution shift: the layer-wise allocation of parameters critical to textual reasoning is disrupted. Building on this insight, we investigate two mitigation strategies: layer-wise learning rate scheduling and Low-Rank Adaptation (LoRA), both aim to preserve the original parameter distribution. Experimental results show that both approaches better maintain textual competence than full fine-tuning, while also improving downstream spoken question answering performance. Furthermore, our analysis offers a principled explanation for the effectiveness of the proposed mitigation strategies, linking their benefits to the structural properties of textual knowledge in LLMs. 

**Abstract (ZH)**: 将语音整合到大型语言模型中显著扩展了其能力，但往往以削弱其核心文本能力为代价。这种退化限制了语音增强型大型语言模型充分利用其预训练文本知识的能力。在本工作中，我们通过集中研究广泛应用的编码器-适配器范式来分析这一问题的根本机制。我们提出了一种基于参数重要性估计的分析框架，揭示了语音微调导致文本重要性分布的变化：层内关键文本推理参数的分配被打乱。基于这一洞见，我们探讨了两种缓解策略：层内学习率调度和低秩适配（LoRA），两者都旨在保持原始参数分布。实验结果表明，这两种方法在保持文本能力方面优于全面微调，同时还能提高下游语音问题回答性能。此外，我们的分析为所提出的缓解策略的有效性提供了理论解释，将其益处与大型语言模型中文本知识的结构特性联系起来。 

---
# PVTAdpNet: Polyp Segmentation using Pyramid vision transformer with a novel Adapter block 

**Title (ZH)**: PVTAdpNet：基于新型Adapter块的金字塔视觉变换器痔瘇分割方法 

**Authors**: Arshia Yousefi Nezhad, Helia Aghaei, Hedieh Sajedi  

**Link**: [PDF](https://arxiv.org/pdf/2509.23751)  

**Abstract**: Colorectal cancer ranks among the most common and deadly cancers, emphasizing the need for effective early detection and treatment. To address the limitations of traditional colonoscopy, including high miss rates due to polyp variability, we introduce the Pyramid Vision Transformer Adapter Residual Network (PVTAdpNet). This model integrates a U-Net-style encoder-decoder structure with a Pyramid Vision Transformer backbone, novel residual blocks, and adapter-based skip connections. The design enhances feature extraction, dense prediction, and gradient flow, supported by squeeze-and-excitation attention for improved channel-wise feature refinement. PVTAdpNet achieves real-time, accurate polyp segmentation, demonstrating superior performance on benchmark datasets with high mDice and mIoU scores, making it highly suitable for clinical applications. PVTAdpNet obtains a high Dice coefficient of 0.8851 and a mean Intersection over Union (mIoU) of 0.8167 on out-of-distribution polyp datasets. Evaluation of the PolypGen dataset demonstrates PVTAdpNet's capability for real-time, accurate performance within familiar distributions. The source code of our network is available at this https URL 

**Abstract (ZH)**: 结直肠癌是常见且致命的癌症之一，强调了有效早期检测和治疗的必要性。为了解决传统结肠镜检查的局限性，包括由于息肉变异导致的高遗漏率，我们引入了金字塔视觉变换器适配残差网络（PVTAdpNet）。该模型结合了U-Net风格的编码解码结构、金字塔视觉变换器骨干、新型残差块和基于适配器的跳跃连接。该设计增强了特征提取、密集预测和梯度流动，并通过压缩和激励注意力机制提高了通道级特征精炼。PVTAdpNet实现了实时、准确的息肉分割，在基准数据集上表现出色，具有高mDice和mIoU分数，使其非常适合临床应用。PVTAdpNet在未知分布息肉数据集上的Dice系数达到0.8851，平均交并比（mIoU）达到0.8167。PolypGen数据集的评估展示了PVTAdpNet在熟悉分布内的实时、准确性能。我们的网络源代码可在以下链接获取。 

---
# Poivre: Self-Refining Visual Pointing with Reinforcement Learning 

**Title (ZH)**: Poivre: 基于强化学习的自我精炼视觉指针 

**Authors**: Wenjie Yang, Zengfeng Huang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23746)  

**Abstract**: Visual pointing, which aims to localize a target by predicting its coordinates on an image, has emerged as an important problem in the realm of vision-language models (VLMs). Despite its broad applicability, recent benchmarks show that current VLMs still fall far behind human performance on this task. A key limitation is that VLMs are typically required to complete the pointing task in a single step, akin to asking humans to point at an object without seeing their own fingers. To address this issue, we propose a simple yet effective self-refining procedure: Point, Visualize, then Refine (Poivre). This procedure enables a VLM to first mark its estimated point, then iteratively refine the coordinates if necessary. Inspired by advances of reasoning models in the natural language domain, we employ reinforcement learning (RL) to incentivize this self-refining ability. For the RL training, we design a neat process reward that is not only empirically effective but also grounded in appealing properties. Our trained model, Poivre-7B, sets a new state of the art on Point-Bench, outperforming both proprietary models such as Gemini-2.5-Pro and large open-source models such as Molmo-72B by over 3%. To support future research, we release our training and inference code, dataset, and the Poivre-7B checkpoint. 

**Abstract (ZH)**: 视觉指针任务旨在通过预测目标在图像中的坐标来定位目标，已成为视觉语言模型（VLMs）领域的一个重要问题。尽管具有广泛的应用性，但近期基准测试显示，当前的VLMs在该任务上仍然远逊于人类表现。一个关键限制是，VLMs通常需要一次性完成指针任务，类似于要求人类在看不到自己手指的情况下指认一个物体。为解决这一问题，我们提出了一种简单而有效的自我完善流程：指针、可视化、再精炼（Poivre）。该流程使VLM能够首先标记其估计的点，然后根据需要迭代精炼坐标。受到自然语言领域推理模型发展的启发，我们采用强化学习（RL）来激励这种自我完善的能力。在RL训练中，我们设计了一种简洁的过程奖励，不仅在实验中有效，而且基于引人注目的特性。经过训练的模型Poivre-7B在Point-Bench上达到了新的最佳水平，比专有模型如Gemini-2.5-Pro以及大型开源模型如Molmo-72B高出超过3%。为支持未来研究，我们公开了训练和推理代码、数据集以及Poivre-7B的检查点。 

---
# LocoFormer: Generalist Locomotion via Long-context Adaptation 

**Title (ZH)**: LocoFormer: 通过长上下文适应实现通用 locomotion 

**Authors**: Min Liu, Deepak Pathak, Ananye Agarwal  

**Link**: [PDF](https://arxiv.org/pdf/2509.23745)  

**Abstract**: Modern locomotion controllers are manually tuned for specific embodiments. We present LocoFormer, a generalist omni-bodied locomotion model that can control previously unseen legged and wheeled robots, even without precise knowledge of their kinematics. LocoFormer is able to adapt to changes in morphology and dynamics at test time. We find that two key choices enable adaptation. First, we train massive scale RL on procedurally generated robots with aggressive domain randomization. Second, in contrast to previous policies that are myopic with short context lengths, we extend context by orders of magnitude to span episode boundaries. We deploy the same LocoFormer to varied robots and show robust control even with large disturbances such as weight change and motor failures. In extreme scenarios, we see emergent adaptation across episodes, LocoFormer learns from falls in early episodes to improve control strategies in later ones. We believe that this simple, yet general recipe can be used to train foundation models for other robotic skills in the future. Videos at this http URL. 

**Abstract (ZH)**: 现代运动控制器是为特定身体手动调优的。我们提出了LocoFormer，这是一种通用型全能身体运动模型，能够控制之前未见过的腿式和轮式机器人，即使没有精确的运动学知识。LocoFormer能够在测试时适应形态和动力学的变化。我们发现两种关键选择使得这种适应成为可能。首先，在生成的机器人上进行大规模强化学习训练，并采用激进的领域随机化。其次，与之前仅具有短视上下文长度的策略不同，我们大幅扩展了上下文，使其跨越整个 episode 边界。我们将相同的LocoFormer部署到多种机器人上，并展示了即便在重量变化和电机故障等大干扰下也能实现鲁棒控制。在极端情况下，LocoFormer表现出跨episode的自适应能力，在早期跌倒中学习以改进后续episode的控制策略。我们相信，这一简单而通用的方法将来可用于训练其他机器人技能的基础模型。视频见此链接。 

---
# Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning 

**Title (ZH)**: 重组与融合：重新审视多模态推理的基础瓶颈 

**Authors**: Yucheng Wang, Yifan Hou, Aydin Javadov, Mubashara Akhtar, Mrinmaya Sachan  

**Link**: [PDF](https://arxiv.org/pdf/2509.23744)  

**Abstract**: Multimodal large language models (MLLMs) promise enhanced reasoning by integrating diverse inputs such as text, vision, and audio. Yet cross-modal reasoning remains underexplored, with conflicting reports on whether added modalities help or harm performance. These inconsistencies stem from a lack of controlled evaluation frameworks and analysis of models' internals to isolate when and why modality interactions support or undermine reasoning. We address this gap through a logic-grounded evaluation framework that categorizes multimodal reasoning into six interaction patterns, varying how facts are distributed across modalities and logically combined. Empirically, additional modalities enhance reasoning only when they provide independent and sufficient reasoning paths, while redundant or chained entailment support often hurts performance. Moreover, reasoning degrades in three systematic ways: weaker modalities drag down overall performance, conflicts bias preference toward certain modalities, and joint signals from different modalities fail to be integrated effectively. Therefore, we identify two core failures: task-composition bottleneck, where recognition and reasoning cannot be jointly executed in one pass, and fusion bottleneck, where early integration introduces bias. For further investigation, we find that attention patterns fail to encode fact usefulness, but a simple two-step prompting (recognize then reason) restores performance, confirming the task-composition bottleneck. Moreover, modality identity remains recoverable in early layers, and softening attention in early fusion improves reasoning, highlighting biased fusion as another failure mode. Overall, our findings show that integration, not perception, is the main barrier to multimodal reasoning, suggesting composition-aware training and early fusion control as promising directions. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）通过集成文本、视觉和音频等多种输入以增强推理能力。然而，跨模态推理依然未被充分探索，不同研究对增加模态是否有助于性能存在矛盾。这些不一致源于缺乏受控的评估框架和对模型内部机制的分析，以确定何时及为何模态交互支持或妨碍推理。我们通过一个逻辑导向的评估框架来填补这一空白，将多模态推理分为六种交互模式，根据不同事实在各模态中的分布及逻辑组合方式。实证研究显示，只有当额外的模态提供独立且充分的推理路径时，多模态推理才会增强，而冗余或链式推理支持通常会损害性能。此外，推理会在三种系统性方式中退化：较弱的模态拉低整体表现，冲突偏向某些模态的偏好，不同模态的联合信号难以有效整合。因此，我们识别出两种核心失败：任务组合瓶颈，即识别和推理不能在一次通过中共同执行；融合瓶颈，早期集成引入偏差。进一步探索发现，注意力模式未能编码事实有用性，而简单的两步提示（识别后推理）恢复了性能，证实了任务组合瓶颈。此外，模态身份在早期层中仍可恢复，早期融合中的软化注意力改善了推理，强调了偏向性融合为另一种失败模式。总体而言，我们的发现表明，整合而非感知是多模态推理的主要障碍，建议任务感知训练和早期融合控制作为有前途的方向。 

---
# HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation 

**Title (ZH)**: HieraTok：多尺度视觉分词器提高图像重建和生成 

**Authors**: Cong Chen, Ziyuan Huang, Cheng Zou, Muzhi Zhu, Kaixiang Ji, Jiajia Liu, Jingdong Chen, Hao Chen, Chunhua Shen  

**Link**: [PDF](https://arxiv.org/pdf/2509.23736)  

**Abstract**: In this work, we present HieraTok, a novel multi-scale Vision Transformer (ViT)-based tokenizer that overcomes the inherent limitation of modeling single-scale representations. This is realized through two key designs: (1) multi-scale downsampling applied to the token map generated by the tokenizer encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal attention mechanism that enables the progressive flow of information from low-resolution global semantic features to high-resolution structural details. Coupling these designs, HieraTok achieves significant improvements in both image reconstruction and generation tasks. Under identical settings, the multi-scale visual tokenizer outperforms its single-scale counterpart by a 27.2\% improvement in rFID ($1.47 \rightarrow 1.07$). When integrated into downstream generation frameworks, it achieves a $1.38\times$ faster convergence rate and an 18.9\% boost in gFID ($16.4 \rightarrow 13.3$), which may be attributed to the smoother and more uniformly distributed latent space. Furthermore, by scaling up the tokenizer's training, we demonstrate its potential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To the best of our knowledge, we are the first to introduce multi-scale ViT-based tokenizer in image reconstruction and image generation. We hope our findings and designs advance the ViT-based tokenizers in visual generation tasks. 

**Abstract (ZH)**: 基于多尺度Vision Transformer的HieraTok分词器：在图像重建和生成任务中的应用 

---
# LUQ: Layerwise Ultra-Low Bit Quantization for Multimodal Large Language Models 

**Title (ZH)**: 逐层超低比特量化：面向多模态大型语言模型 

**Authors**: Shubhang Bhatnagar, Andy Xu, Kar-Han Tan, Narendra Ahuja  

**Link**: [PDF](https://arxiv.org/pdf/2509.23729)  

**Abstract**: Large Language Models (LLMs) with multimodal capabilities have revolutionized vision-language tasks, but their deployment often requires huge memory and computational resources. While post-training quantization (PTQ) has successfully compressed language models to as low as 1-bit precision without significant performance loss, its effectiveness for multimodal LLMs (MLLMs) remains relatively unexplored. In this paper, we present the first study on ultra-low bit (<4-bit) quantization for multimodal LLMs. Our analysis reveals that multimodal tokens and intermediate layer activations produced by them exhibit significantly higher statistical variance and entropy compared to text tokens, making them less tolerant to ultra-low bit quantization. However, the activation distributions of multimodal tokens varies significantly over different layers, with some layers having lower entropy activation distributions. We empirically show that such layers in these models can better tolerate ultra-low bit quantization. Building on these insights, we propose a novel strategy for MLLM quantization, LUQ: Layerwise Ultra-Low Bit Quantization, which selectively applies ultra-low bit quantization to layers that are more resilient to it. Additionally, we also show that using a mix of multimodal tokens (image and text) for PTQ boosts VQA performance in the ultra-low bit regime. We evaluate our method on LLaVA-1.5 and Qwen-2.5-VL across 9 popular VQA benchmarks. The resulting LUQ models use 40% and 31% less memory than their 4-bit counterparts, respectively, while exhibiting a performance degradation of less than 10% on the MME benchmark. 

**Abstract (ZH)**: 超低比特（<4比特）量化在多模态大语言模型中的研究 

---
# M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured Descriptions for 3D Generation 

**Title (ZH)**: M3DLayout：多源的3D室内布局及其结构化描述数据集用于3D生成 

**Authors**: Yiheng Zhang, Zhuojiang Cai, Mingdao Wang, Meitong Guo, Tianxiao Li, Li Lin, Yuwang Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23728)  

**Abstract**: In text-driven 3D scene generation, object layout serves as a crucial intermediate representation that bridges high-level language instructions with detailed geometric output. It not only provides a structural blueprint for ensuring physical plausibility but also supports semantic controllability and interactive editing. However, the learning capabilities of current 3D indoor layout generation models are constrained by the limited scale, diversity, and annotation quality of existing datasets. To address this, we introduce M3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation. M3DLayout comprises 15,080 layouts and over 258k object instances, integrating three distinct sources: real-world scans, professional CAD designs, and procedurally generated scenes. Each layout is paired with detailed structured text describing global scene summaries, relational placements of large furniture, and fine-grained arrangements of smaller items. This diverse and richly annotated resource enables models to learn complex spatial and semantic patterns across a wide variety of indoor environments. To assess the potential of M3DLayout, we establish a benchmark using a text-conditioned diffusion model. Experimental results demonstrate that our dataset provides a solid foundation for training layout generation models. Its multi-source composition enhances diversity, notably through the Inf3DLayout subset which provides rich small-object information, enabling the generation of more complex and detailed scenes. We hope that M3DLayout can serve as a valuable resource for advancing research in text-driven 3D scene synthesis. 

**Abstract (ZH)**: 基于文本驱动的3D场景生成中，物体布局作为一种关键的中间表示，连接了高层次的语言指令与详细的几何输出。它不仅提供了确保物理合理性的重要结构蓝图，还支持语义可控性和交互编辑。然而，当前3D室内布局生成模型的学习能力受限于现有数据集在规模、多样性和标注质量上的限制。为解决这一问题，我们引入了M3DLayout，这是一个大规模、多来源的3D室内布局生成数据集。M3DLayout包含15,080个布局和超过258k个对象实例，整合了三种不同的来源：真实的扫描数据、专业的CAD设计和程序生成的场景。每个布局都与详尽的结构性文本配合，描述全局场景摘要、大型家具的相对位置以及小型物品的精细布置。这一多样且详细的标注资源使模型能够在广泛多样的室内环境中学习复杂的空间和语义模式。为评估M3DLayout的潜力，我们使用文本条件扩散模型建立了一个基准。实验结果表明，我们的数据集为训练布局生成模型提供了坚实的基础。其多来源的构建增强了多样性，特别通过Inf3DLayout子集提供了丰富的小型物体信息，使生成更加复杂和详细的场景成为可能。我们希望M3DLayout能够成为推进基于文本驱动的3D场景合成研究的重要资源。 

---
# AudioMoG: Guiding Audio Generation with Mixture-of-Guidance 

**Title (ZH)**: AudioMoG: 用混合指导引导音频生成 

**Authors**: Junyou Wang, Zehua Chen, Binjie Yuan, Kaiwen Zheng, Chang Li, Yuxuan Jiang, Jun Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23727)  

**Abstract**: Guidance methods have demonstrated significant improvements in cross-modal audio generation, including text-to-audio (T2A) and video-to-audio (V2A) generation. The popularly adopted method, classifier-free guidance (CFG), steers generation by emphasizing condition alignment, enhancing fidelity but often at the cost of diversity. Recently, autoguidance (AG) has been explored for audio generation, encouraging the sampling to faithfully reconstruct the target distribution and showing increased diversity. Despite these advances, they usually rely on a single guiding principle, e.g., condition alignment in CFG or score accuracy in AG, leaving the full potential of guidance for audio generation untapped. In this work, we explore enriching the composition of the guidance method and present a mixture-of-guidance framework, AudioMoG. Within the design space, AudioMoG can exploit the complementary advantages of distinctive guiding principles by fulfilling their cumulative benefits. With a reduced form, AudioMoG can consider parallel complements or recover a single guiding principle, without sacrificing generality. We experimentally show that, given the same inference speed, AudioMoG approach consistently outperforms single guidance in T2A generation across sampling steps, concurrently showing advantages in V2A, text-to-music, and image generation. These results highlight a "free lunch" in current cross-modal audio generation systems: higher quality can be achieved through mixed guiding principles at the sampling stage without sacrificing inference efficiency. Demo samples are available at: this https URL. 

**Abstract (ZH)**: 指导方法在跨模态音频生成，包括文本到音频（T2A）和视频到音频（V2A）生成中显示出显著改善。尽管广泛采用的方法，无分类器引导（CFG），通过强调条件对齐来引导生成并提高保真度，但通常会牺牲多样性。最近，音频生成中探索了自引导（AG），鼓励采样忠实地重构目标分布并显示出增强的多样性。尽管取得了这些进展，它们通常依赖单一的引导原则，例如CFG中的条件对齐或AG中的分数准确性，从而未能充分利用引导方法的全部潜力。在本文中，我们探讨丰富了指导方法的组成，并提出了一种混合引导框架AudioMoG。在设计空间中，AudioMoG可以利用不同引导原则的互补优势，通过实现其累积效益来发挥这些优势。以简化形式，AudioMoG可以考虑并行补充或恢复单一的引导原则，而不牺牲通用性。实验结果表明，在采样步骤中，与单一引导方法相比，AudioMoG方法在T2A生成中始终表现出更高的性能，并且同样在V2A、文本到音乐和图像生成中显示出优势。这些结果突显了当前跨模态音频生成系统中的“免费午餐”现象：在采样阶段通过混合引导原则可以实现更高质量而不会牺牲推理效率。示范样本可在以下链接获取：this https URL。 

---
# Video Panels for Long Video Understanding 

**Title (ZH)**: 长视频理解的视频面板 

**Authors**: Lars Doorenbos, Federico Spurio, Juergen Gall  

**Link**: [PDF](https://arxiv.org/pdf/2509.23724)  

**Abstract**: Recent Video-Language Models (VLMs) achieve promising results on long-video understanding, but their performance still lags behind that achieved on tasks involving images or short videos. This has led to great interest in improving the long context modeling of VLMs by introducing novel modules and additional complexity. % additional training time. In this paper, we take a different approach: rather than fine-tuning VLMs with the limited data available, we attempt to maximize the performance of existing models. To this end, we propose a novel visual prompting strategy specifically designed for long-video understanding. By combining multiple frames as panels into one image, we effectively trade off spatial details for temporal resolution. Our approach is training-free, parameter-free, and model-agnostic, and can be seamlessly integrated into existing VLMs. Extensive experiments on five established benchmarks across a wide range of model architectures, sizes, and context windows confirm the consistency of our approach. For the TimeScope (Long) dataset, which has the longest videos, the accuracy for video question answering is improved by up to 19.4\%. Overall, our method raises the bar for long video understanding models. We will make our code available upon acceptance. 

**Abstract (ZH)**: 近期的视频-语言模型（VLMs）在长视频理解任务上取得了令人鼓舞的结果，但仍落后于涉及图像或短视频任务的表现。这导致了对提高VLMs的长上下文建模兴趣，通过引入新颖模块和额外复杂性（以及额外训练时间）。在本文中，我们采取不同的方法：而非通过有限的数据微调VLMs，我们试图最大化现有模型的性能。为此，我们提出了一种专门针对长视频理解的新型视觉提示策略。通过将多个帧作为面板合并为一张图像，我们有效权衡了空间细节和时间分辨率。我们的方法无需训练、无需参数，并且具有模型无关性，可以无缝集成到现有VLMs中。跨多种模型架构、规模和上下文窗口的五个标准基准的广泛实验验证了该方法的一致性。对于TimeScope（Long）数据集，该数据集具有最长的视频，视频问答的准确性提高了高达19.4%。总体而言，我们的方法提高了长视频理解模型的标准。接受后我们将提供代码。 

---
# AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models 

**Title (ZH)**: AdaPtis: 降低异构模型流水线气泡的方法基于自适应流水线并行性 

**Authors**: Jihu Guo, Tenghui Ma, Wei Gao, Peng Sun, Jiaxing Li, Xun Chen, Yuyang Jin, Dahua Lin  

**Link**: [PDF](https://arxiv.org/pdf/2509.23722)  

**Abstract**: Pipeline parallelism is widely used to train large language models (LLMs). However, increasing heterogeneity in model architectures exacerbates pipeline bubbles, thereby reducing training efficiency. Existing approaches overlook the co-optimization of model partition, model placement, and workload scheduling, resulting in limited efficiency improvement or even performance degradation. To respond, we propose AdaPtis, an LLM training system that supports adaptive pipeline parallelism. First, we develop a pipeline performance model to accurately estimate training throughput. Second, AdaPtis jointly optimizes model partition, model placement, and workload scheduling policies guided by this performance model. Third, we design a unified pipeline executor that efficiently supports the execution of diverse pipeline strategies. Extensive experiments show that AdaPtis achieves an average speedup of 1.42x (up to 2.14x) over Megatron-LM I-1F1B across various LLM architectures and scales. 

**Abstract (ZH)**: AdaPtis：一种支持自适应管道并行性的大型语言模型训练系统 

---
# Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient with Martingale Characterization 

**Title (ZH)**: 离散与连续RL的桥梁：具有鞅特征的稳定确定性策略梯度 

**Authors**: Ziheng Cheng, Xin Guo, Yufei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23711)  

**Abstract**: The theory of discrete-time reinforcement learning (RL) has advanced rapidly over the past decades. Although primarily designed for discrete environments, many real-world RL applications are inherently continuous and complex. A major challenge in extending discrete-time algorithms to continuous-time settings is their sensitivity to time discretization, often leading to poor stability and slow convergence. In this paper, we investigate deterministic policy gradient methods for continuous-time RL. We derive a continuous-time policy gradient formula based on an analogue of the advantage function and establish its martingale characterization. This theoretical foundation leads to our proposed algorithm, CT-DDPG, which enables stable learning with deterministic policies in continuous-time environments. Numerical experiments show that the proposed CT-DDPG algorithm offers improved stability and faster convergence compared to existing discrete-time and continuous-time methods, across a wide range of control tasks with varying time discretizations and noise levels. 

**Abstract (ZH)**: 连续时间强化学习中的确定性策略梯度方法 

---
# CrimEdit: Controllable Editing for Counterfactual Object Removal, Insertion, and Movement 

**Title (ZH)**: CrimEdit: 可控编辑以实现反事实对象移除、插入和移动 

**Authors**: Boseong Jeon, Junghyuk Lee, Jimin Park, Kwanyoung Kim, Jingi Jung, Sangwon Lee, Hyunbo Shim  

**Link**: [PDF](https://arxiv.org/pdf/2509.23708)  

**Abstract**: Recent works on object removal and insertion have enhanced their performance by handling object effects such as shadows and reflections, using diffusion models trained on counterfactual datasets. However, the performance impact of applying classifier-free guidance to handle object effects across removal and insertion tasks within a unified model remains largely unexplored. To address this gap and improve efficiency in composite editing, we propose CrimEdit, which jointly trains the task embeddings for removal and insertion within a single model and leverages them in a classifier-free guidance scheme -- enhancing the removal of both objects and their effects, and enabling controllable synthesis of object effects during insertion. CrimEdit also extends these two task prompts to be applied to spatially distinct regions, enabling object movement (repositioning) within a single denoising step. By employing both guidance techniques, extensive experiments show that CrimEdit achieves superior object removal, controllable effect insertion, and efficient object movement without requiring additional training or separate removal and insertion stages. 

**Abstract (ZH)**: 近期关于物体删除和插入的研究通过处理物体效果（如阴影和反射）来增强性能，使用了在假设数据集上训练的扩散模型。然而，如何在统一模型中利用无分类器引导技术来处理删除和插入任务中的物体效果，其性能影响仍待探索。为填补这一空白并提高综合编辑的效率，我们提出了CrimEdit，它在单个模型中联合训练删除和插入任务的嵌入，并在其无分类器引导方案中利用这些嵌入——增强物体及其效果的删除，同时在插入过程中实现可控的效果合成。此外，CrimEdit 将这两种任务提示扩展到空间上不同的区域，使物体在单次去噪步骤中实现移动（重新定位）。通过结合使用这两种引导技术，广泛的实验表明，CrimEdit 在物体删除、可控效果插入和高效物体移动方面表现出优越性能，无需额外训练或单独的删除和插入阶段。 

---
# Estimating Time Series Foundation Model Transferability via In-Context Learning 

**Title (ZH)**: 基于上下文学习的时间序列基础模型迁移性估测 

**Authors**: Qingren Yao, Ming Jin, Chengqi Zhang, Chao-Han Huck Yang, Jun Qi, Shirui Pan  

**Link**: [PDF](https://arxiv.org/pdf/2509.23695)  

**Abstract**: Time series foundation models (TSFMs) offer strong zero-shot forecasting via large-scale pre-training, yet fine-tuning remains critical for boosting performance in domains with limited public data. With the growing number of TSFMs, efficiently identifying the best model for downstream fine-tuning becomes increasingly challenging. In this work, we introduce TimeTic, a transferability estimation framework that recasts model selection as an in-context-learning problem: given observations on known (source) datasets, it predicts how a TSFM will perform after fine-tuning on a downstream (target) dataset. TimeTic flexibly organizes the observed model-data relationships as contextual information, allowing it to adapt seamlessly to various test-time scenarios. Leveraging the natural tabular structure formed by dataset meta-features, model characteristics, and fine-tuned performance, we employ tabular foundation models to serve as in-context learners. We further introduce a novel model characterization based on entropy evolution across model layers, capturing embedding-space distinctions and enabling TimeTic to generalize across arbitrary model sets. We establish a comprehensive benchmark for transferability estimation including 10 datasets, 10 foundation models, and 3 forecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong alignment with actual fine-tuned performance for previously unseen datasets, achieving a mean rank correlation of approximately 0.6 and a 30% improvement compared to using zero-shot performance as the transferability score. 

**Abstract (ZH)**: TimeTic：一种时间序列基础模型迁移性估计框架 

---
# Joint Hybrid Beamforming and Artificial Noise Design for Secure Multi-UAV ISAC Networks 

**Title (ZH)**: 联合混合波束形成与人工噪声设计以实现安全的多无人机异构接入网络 

**Authors**: Runze Dong, Buhong Wang, Cunqian Feng, Jiang Weng, Chen Han, Jiwei Tian  

**Link**: [PDF](https://arxiv.org/pdf/2509.23687)  

**Abstract**: Integrated sensing and communication (ISAC) emerges as a key enabler for next-generation applications such as smart cities and autonomous systems. Its integration with unmanned aerial vehicles (UAVs) unlocks new potentials for reliable communication and precise sensing in dynamic aerial environments. However, existing research predominantly treats UAVs as aerial base stations, overlooking their role as ISAC users, and fails to leverage large-scale antenna arrays at terrestrial base stations to enhance security and spectral efficiency. This paper propose a secure and spectral efficient ISAC framework for multi-UAV networks, and a two-stage optimization approach is developed to jointly design hybrid beamforming (HBF), artificial noise (AN) injection, and UAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage employs Proximal Policy Optimization (PPO) to optimize digital beamformers and trajectories, and the second stage decomposes the digital solution into analog and digital components via low-complexity matrix factorization. Simulation results demonstrate the effectiveness of the proposed framework compared to benchmark schemes. 

**Abstract (ZH)**: 集成传感与通信(ISAC)技术成为智能城市和自主系统等下一代应用的关键使能器。将其与无人驾驶飞行器(UAVs)结合，为动态高空环境下的可靠通信和精确传感开启了新潜能。然而，现有研究主要将UAVs视为高空基站，忽视了其作为ISAC用户的角色，未能充分利用地面基站的大规模天线阵列以提升安全性和频谱效率。本文提出一种适用于多UAV网络的保密性和频谱效率兼备的ISAC框架，并开发了一种两阶段优化方法，以联合设计混合波束形成(HBF)、人工噪声(AN)注入和UAV航迹。为最大化总保密率，第一阶段采用近端策略优化(Proximal Policy Optimization, PPO)优化数字波束形成器和航迹，第二阶段通过低复杂度矩阵分解将数字解决方案分解为模拟和数字组件。仿真结果表明，所提出框架的有效性优于基准方案。 

---
# Towards a Comprehensive Scaling Law of Mixture-of-Experts 

**Title (ZH)**: 面向混合专家模型的综合性缩放律研究 

**Authors**: Guoliang Zhao, Yuhan Fu, Shuaipeng Li, Xingwu Sun, Ruobing Xie, An Wang, Weidong Han, Zhen Yang, Weixuan Sun, Yudong Zhang, Cheng-zhong Xu, Di Wang, Jie Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23678)  

**Abstract**: Mixture-of-Experts (MoE) models have become the consensus approach for enabling parameter-efficient scaling and cost-effective deployment in large language models. However, existing scaling laws for dense models are inapplicable to MoE models, which stems from three critical challenges: the multiplicity of influencing factors, their intricate coupling relationships and the non-monotonic nature of their performance impacts. They collectively necessitate a fine-grained investigation into MoE-specific scaling laws. In this work, we perform a systematic decomposition of MoE settings, identifying five key factors that influence model performance from both size and structural perspectives (data size ($D$), total model size ($N$), activated model size ($N_a$), number of active experts ($G$) and the ratio of shared experts ($S$)). Specifically, we design $446$ controlled experiments to characterize their marginal effects, ultimately constructing a comprehensive and precise joint MoE scaling law that considers all essential factors. Furthermore, we derive the theoretically optimal and practically efficiency-aware optimal configurations for $G$, $S$ and $N_a/N$ with detailed analyses. Our results demonstrate that the optimal settings for $G$ and $S$ are independent of both the model architecture and data size. With the scaling of $N$, the optimal activation parameter ratio of $N_a/N$ becomes sparser. Our proposed MoE scaling law could function as an accurate and insightful guidance to facilitate future MoE model design and training. 

**Abstract (ZH)**: MoE模型已成为在大型语言模型中实现参数高效的扩展和成本效益部署的共识方法。然而，现有的密集模型扩展规律对MoE模型不适用，这源于三个关键挑战：影响因素的多重性、它们的复杂耦合关系以及性能影响的非单调性。这些因素共同要求对MoE特定的扩展规律进行精细的研究。在本工作中，我们系统地分解了MoE设置，从模型大小和结构两个视角识别出五个关键因素，它们影响模型性能（数据大小$D$，总模型大小$N$，激活模型大小$N_a$，活跃专家数量$G$，以及共享专家比例$S$）。具体而言，我们设计了446个受控实验来表征它们的边际效应，最终构建了一个全面而精确的联合MoE扩展定律，考虑了所有基本因素。此外，我们推导出$G$、$S$和$N_a/N$的理论最优配置和实际效率感知最优配置，并进行了详细的分析。我们的结果显示，$G$和$S$的最佳设置与模型架构和数据大小无关。随着$N$的扩展，$N_a/N$的最佳激活参数比变得更为稀疏。我们提出的MoE扩展定律可以作为准确而深入的指导，以促进未来MoE模型的设计和训练。 

---
# RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks 

**Title (ZH)**: RCI：评估多模态基准中全局和局部推理的能力评分 

**Authors**: Amit Agarwal, Hitesh Laxmichand Patel, Srikant Panda, Hansa Meghwani, Jyotika Singh, Karan Dua, Paul Li, Tao Sheng, Sujith Ravi, Dan Roth  

**Link**: [PDF](https://arxiv.org/pdf/2509.23673)  

**Abstract**: Multimodal Large Language Models (MLLMs) have achieved impressive results on vision-language benchmarks, yet it remains unclear whether these benchmarks assess genuine global reasoning or allow success via localized visual cues. Existing evaluation methods do not explicitly measure this distinction, hindering effective dataset curation and real-world focused model development.
We introduce Region Comprehension Index (RCI), the first model-based score to directly quantify a dataset's reliance on global versus local visual information. RCI systematically compares reference-model performance on image patches versus full images, revealing if tasks require holistic image understanding or can be solved with partial or localized visual cues.
When applying RCI to 13 widely used multimodal benchmarks, we observed that most of them favor localized reasoning and exhibit significant spatial biases, indicating potential risks in real-world applications. RCI equips researchers & practitioners with an actionable tool for diagnosing & mitigating these biases, enabling the construction of datasets and benchmarks to foster the development of robust, enterprise-ready multimodal systems. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）在视觉-语言基准测试中取得了显著成果，但尚不清楚这些基准测试是否评估了真正的全局推理能力，还是允许通过局部视觉线索取得成功。现有的评估方法并未明确测量这种区别，妨碍了高效的数据集编排和面向现实世界的模型开发。

我们提出了区域理解指数（RCI），这是第一个直接量化数据集对全局与局部视觉信息依赖程度的模型基准得分。RCI系统地比较了参考模型在图像片段与完整图像上的表现，揭示了任务需要整体图像理解还是可以通过部分或局部视觉线索解决。

将RCI应用于13个广泛使用的多模态基准测试时，我们观察到大多数基准测试倾向于局部推理，并表现出显著的空间偏见，这表明在实际应用中可能存在风险。RCI为研究人员和实践者提供了一个可操作的工具，用于诊断和缓解这些偏见，从而促进稳健的企业级多模态系统的发展。 

---
# Graph Neural Networks with Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion for Multivariate Time Series Forecasting 

**Title (ZH)**: 具有多样性意识的邻域选择和动态多尺度融合的图神经网络在多变量时间序列预测中的应用 

**Authors**: Jingqi Xu, Guibin Chen, Jingxi Lu, Yuzhang Lin  

**Link**: [PDF](https://arxiv.org/pdf/2509.23671)  

**Abstract**: Recently, numerous deep models have been proposed to enhance the performance of multivariate time series (MTS) forecasting. Among them, Graph Neural Networks (GNNs)-based methods have shown great potential due to their capability to explicitly model inter-variable dependencies. However, these methods often overlook the diversity of information among neighbors, which may lead to redundant information aggregation. In addition, their final prediction typically relies solely on the representation from a single temporal scale. To tackle these issues, we propose a Graph Neural Networks (GNNs) with Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion (DIMIGNN). DIMIGNN introduces a Diversity-aware Neighbor Selection Mechanism (DNSM) to ensure that each variable shares high informational similarity with its neighbors while maintaining diversity among neighbors themselves. Furthermore, a Dynamic Multi-Scale Fusion Module (DMFM) is introduced to dynamically adjust the contributions of prediction results from different temporal scales to the final forecasting result. Extensive experiments on real-world datasets demonstrate that DIMIGNN consistently outperforms prior methods. 

**Abstract (ZH)**: 基于多样性aware邻居选择和动态多尺度融合的图神经网络（DIMIGNN）及其在多变量时间序列预测中的应用 

---
# Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability 

**Title (ZH)**: 超越贪婪退出：改进的风险控制和可靠性早期退出决策 

**Authors**: Divya Jyoti Bajpai, Manjesh Kumar Hanawal  

**Link**: [PDF](https://arxiv.org/pdf/2509.23666)  

**Abstract**: Early-Exit Deep Neural Networks enable adaptive inference by allowing prediction at intermediary layers, significantly reducing computational costs and latency. Most of the early exit strategies greedily exit a sample at an intermediary layer if the confidence in class prediction exceeds a predefined threshold that is set using a static validation set. This is problematic as the model might be overconfident in a wrong class. Also, they are not robust to distribution shifts encountered in deployment, which can undermine model trustworthiness and accuracy. To address these challenges, we propose UAT that adapts the threshold for exit decisions using a Multi-Armed Bandit framework, enabling online, unsupervised adjustment of exit decisions. UAT makes decisions based on a new reward function that assesses predictive certainty and its reliability to balance computational efficiency and prediction quality while penalizing unnecessary late exits. We provide guarantees on risk achieved by UAT and validate its performance on diverse tasks spanning vision-language understanding, text generation, and classification. Our framework demonstrates consistent improvements in speedup (1.70-2.10x) with a minimal performance drop (<2%) as compared to full model performance. Our source code is available at this https URL. 

**Abstract (ZH)**: 基于多臂老虎机框架的自适应阈值退出机制使早退出深度神经网络能够在中间层进行预测，显著降低计算成本和延迟并实现适应性推理。现有的大多数早退出策略在类预测置信度超过预设阈值时贪婪地在中间层退出样本，但这种方法可能导致模型过自信于错误的类别，并在部署时遇到分布偏移时缺乏鲁棒性，从而影响模型的信任和准确性。为解决这些问题，我们提出了一种基于多臂老虎机框架的自适应阈值（UAT）机制，能够在无需监督的情况下在线调整退出决策，并基于新的奖励函数评估预测的确定性和可靠性来平衡计算效率和预测质量，同时惩罚不必要的延迟退出。我们提供了UAT实现的风险保证，并在视觉-语言理解、文本生成和分类等多样任务中验证了其性能。与全模型相比，该框架在保证性能损失小于2%的情况下，实现了1.70-2.10倍的加速。源代码可访问此链接。 

---
# Calibration Meets Reality: Making Machine Learning Predictions Trustworthy 

**Title (ZH)**: 校准遇见现实：使机器学习预测值得信赖 

**Authors**: Kristina P. Sinaga, Arjun S. Nair  

**Link**: [PDF](https://arxiv.org/pdf/2509.23665)  

**Abstract**: Post-hoc calibration methods are widely used to improve the reliability of probabilistic predictions from machine learning models. Despite their prevalence, a comprehensive theoretical understanding of these methods remains elusive, particularly regarding their performance across different datasets and model architectures. Input features play a crucial role in shaping model predictions and, consequently, their calibration. However, the interplay between feature quality and calibration performance has not been thoroughly investigated. In this work, we present a rigorous theoretical analysis of post-hoc calibration methods, focusing on Platt scaling and isotonic regression. We derive convergence guarantees, computational complexity bounds, and finite-sample performance metrics for these methods. Furthermore, we explore the impact of feature informativeness on calibration performance through controlled synthetic experiments. Our empirical evaluation spans a diverse set of real-world datasets and model architectures, demonstrating consistent improvements in calibration metrics across various scenarios. By examining calibration performance under varying feature conditions utilizing only informative features versus complete feature spaces including noise dimensions, we provide fundamental insights into the robustness and reliability of different calibration approaches. Our findings offer practical guidelines for selecting appropriate calibration methods based on dataset characteristics and computational constraints, bridging the gap between theoretical understanding and practical implementation in uncertainty quantification. Code and experimental data are available at: this https URL. 

**Abstract (ZH)**: 事后校准方法广泛用于提高机器学习模型的概率预测可靠性。尽管这些方法被广泛应用，但对其在不同数据集和模型架构上的表现的全面理论理解仍缺乏，特别是关于特征质量与校准性能之间的关系。输入特征在塑造模型预测和校准方面起着关键作用，但特征质量和校准性能之间的相互作用尚未得到充分研究。在本文中，我们对事后校准方法进行了严格的理论分析，集中于Platt校准和等距回归。我们推导了这些方法的收敛保证、计算复杂度边界和有限样本性能度量。此外，我们通过受控的合成实验探讨了特征信息量对校准性能的影响。我们的实证评估涵盖了多种真实世界的数据集和模型架构，展示了在各种场景下校准指标的一致性改进。通过在仅使用信息特征与包含噪声维度的完整特征空间下考察不同校准方法的校准性能，我们提供了关于不同校准方法的稳健性和可靠性的重要见解。我们的发现为基于数据集特性和计算约束选择合适的校准方法提供了实用指南，填补了理论理解和实际实施在不确定性量化中的差距。代码和实验数据可在：this https URL 获取。 

---
# Pure Node Selection for Imbalanced Graph Node Classification 

**Title (ZH)**: 无偏节点选择的图节点分类 

**Authors**: Fanlong Zeng, Wensheng Gan, Jiayang Wu, Philip S. Yu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23662)  

**Abstract**: The problem of class imbalance refers to an uneven distribution of quantity among classes in a dataset, where some classes are significantly underrepresented compared to others. Class imbalance is also prevalent in graph-structured data. Graph neural networks (GNNs) are typically based on the assumption of class balance, often overlooking the issue of class imbalance. In our investigation, we identified a problem, which we term the Randomness Anomalous Connectivity Problem (RACP), where certain off-the-shelf models are affected by random seeds, leading to a significant performance degradation. To eliminate the influence of random factors in algorithms, we proposed PNS (Pure Node Sampling) to address the RACP in the node synthesis stage. Unlike existing approaches that design specialized algorithms to handle either quantity imbalance or topological imbalance, PNS is a novel plug-and-play module that operates directly during node synthesis to mitigate RACP. Moreover, PNS also alleviates performance degradation caused by abnormal distribution of node neighbors. We conduct a series of experiments to identify what factors are influenced by random seeds. Experimental results demonstrate the effectiveness and stability of our method, which not only eliminates the effect of unfavorable random seeds but also outperforms the baseline across various benchmark datasets with different GNN backbones. Data and code are available at this https URL. 

**Abstract (ZH)**: 类别不平衡问题指的是数据集中各类别数量分布不均，其中某些类别相较于其他类别显著欠代表。类别不平衡问题在图结构数据中也很常见。图神经网络（GNNs）通常基于类别平衡的假设，常忽视类别不平衡的问题。在我们的研究中，我们发现了一个问题，称之为随机异常连接问题（RACP），某些现成模型受随机种子影响，导致显著性能下降。为消除算法中随机因素的影响，我们提出了PNS（纯节点采样）来解决RACP问题。PNS不同于现有的专为处理数量不平衡或拓扑不平衡设计的算法，它是一个新颖的即插即用模块，在节点合成阶段直接运行以缓解RACP。此外，PNS还能缓解由于节点邻居异常分布导致的性能下降。我们进行了一系列实验以确定哪些因素受随机种子影响。实验结果证明了我们方法的有效性和稳定性，不仅消除了不利随机种子的影响，还在不同GNN后端的不同基准数据集上优于基线方法。数据和代码可在以下链接获取。 

---
# Aligning LLMs for Multilingual Consistency in Enterprise Applications 

**Title (ZH)**: 为企业应用中多语言一致性对齐大型语言模型 

**Authors**: Amit Agarwal, Hansa Meghwani, Hitesh Laxmichand Patel, Tao Sheng, Sujith Ravi, Dan Roth  

**Link**: [PDF](https://arxiv.org/pdf/2509.23659)  

**Abstract**: Large language models (LLMs) remain unreliable for global enterprise applications due to substantial performance gaps between high-resource and mid/low-resource languages, driven by English-centric pretraining and internal reasoning biases. This inconsistency undermines customer experience and operational reliability in multilingual settings such as customer support, content moderation, and information retrieval. Even with advanced Retrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy drop in non-English languages compared to English.
We propose a practical, batch-wise alignment strategy for fine-tuning LLMs, leveraging semantically equivalent multilingual data in each training batch to directly align model outputs across languages. This approach improves non-English accuracy by up to 23.9\% without compromising English performance, model reasoning, or retrieval quality. Our method is simple to implement, scalable, and integrates seamlessly with existing LLM training \& deployment pipelines, enabling more robust and equitable multilingual AI solutions in industry. 

**Abstract (ZH)**: 大型语言模型（LLMs）由于高资源和中/低资源语言之间存在显著的性能差距，在全球企业应用中仍不可靠，这一差距主要源于以英语为中心的预训练和内部推理偏见。这种不一致性在多语言环境中（如客户服务、内容审核和信息检索）削弱了客户体验和操作可靠性。即使使用先进的检索增强生成（RAG）系统，我们观察到非英语语言的准确性相对于英语下降了最多29%。

我们提出了一种实用的批次对齐策略，利用每个训练批次中的语义等效多语言数据直接对齐不同语言的模型输出。该方法在不牺牲英语性能、模型推理或检索质量的情况下，将非英语语言的准确性提高到最多23.9%。该方法易于实现、可扩展，并能无缝集成到现有的LLM训练与部署流程中，从而促进更稳健和公平的多语言AI解决方案在行业中的应用。 

---
# Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models 

**Title (ZH)**: 聚焦关键：面向对象-代理的标记化方法用于视觉语言行动模型 

**Authors**: Rokas Bendikas, Daniel Dijkman, Markus Peschl, Sanjay Haresh, Pietro Mazzaglia  

**Link**: [PDF](https://arxiv.org/pdf/2509.23655)  

**Abstract**: Vision-Language-Action (VLA) models offer a pivotal approach to learning robotic manipulation at scale by repurposing large pre-trained Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs for robotic domains comes with an unnecessarily high computational cost, which we attribute to the tokenization scheme of visual inputs. In this work, we aim to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric Tokenization for VLAs. Building on the insights of object-centric representation learning, our method introduces an inductive bias towards scene objects and the agent's own visual information. As a result, we find that Oat-VLA can drastically reduce the number of visual tokens to just a few tokens without sacrificing performance. We reveal that Oat-VLA converges at least twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in diverse real-world pick and place tasks. 

**Abstract (ZH)**: 面向物体-代理的分词(Oat-VLA)为VLA模型提供高效训练 

---
# ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models through Agentic Data Synthesis 

**Title (ZH)**: ReWatch-R1: 通过代理数据合成增强大型视觉语言模型中的复杂视频推理 

**Authors**: Congzhi Zhang, Zhibin Wang, Yinchao Ma, Jiawei Peng, Yihan Wang, Qiang Zhou, Jun Song, Bo Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2509.23652)  

**Abstract**: While Reinforcement Learning with Verifiable Reward (RLVR) significantly advances image reasoning in Large Vision-Language Models (LVLMs), its application to complex video reasoning remains underdeveloped. This gap stems primarily from a critical data bottleneck: existing datasets lack the challenging, multi-hop questions and high-quality, video-grounded Chain-of-Thought (CoT) data necessary to effectively bootstrap RLVR. To address this, we introduce ReWatch, a large-scale dataset built to foster advanced video reasoning. We propose a novel multi-stage synthesis pipeline to synthesize its three components: ReWatch-Caption, ReWatch-QA, and ReWatch-CoT. A core innovation is our Multi-Agent ReAct framework for CoT synthesis, which simulates a human-like "re-watching" process to generate video-grounded reasoning traces by explicitly modeling information retrieval and verification. Building on this dataset, we develop ReWatch-R1 by post-training a strong baseline LVLM with Supervised Fine-Tuning (SFT) and our RLVR framework. This framework incorporates a novel Observation \& Reasoning (O\&R) reward mechanism that evaluates both the final answer's correctness and the reasoning's alignment with video content, directly penalizing hallucination. Our experiments show that ReWatch-R1 achieves state-of-the-art average performance on five challenging video reasoning benchmarks. 

**Abstract (ZH)**: 虽然可验证奖励的强化学习（RLVR）显著推动了大规模视觉-语言模型（LVLMs）中的图像推理，但其在复杂视频推理中的应用仍相对不足。这一差距主要源于一个关键的数据瓶颈：现有数据集缺乏能够有效启动RLVR的具有挑战性和多跳性的问答以及高质量的视频关联思维链（CoT）数据。为了解决这一问题，我们引入了ReWatch，这是一个大型数据集，旨在促进高级视频推理。我们提出了一种新颖的多阶段合成流水线来合成其三个组成部分：ReWatch-Caption、ReWatch-QA和ReWatch-CoT。核心创新是我们提出的多代理ReAct框架用于生成思维链，该框架通过显式建模信息检索和验证来模拟类似人类的“重新观看”过程以生成视频关联的推理轨迹。基于此数据集，我们通过监督微调（SFT）和我们的RLVR框架后训练了一个强大的LVLM，构建了ReWatch-R1。该框架集成了一个新颖的观察与推理（O&R）奖励机制，该机制同时评估最终答案的正确性和推理与视频内容的一致性，并直接惩罚幻觉。我们的实验结果显示，ReWatch-R1在五个具有挑战性的视频推理基准测试中取得了最先进的平均性能。 

---
# LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders 

**Title (ZH)**: LightFair: 向高效公平的文本到图像扩散转化的去偏见预训练文本编码器替代方案 

**Authors**: Boyu Han, Qianqian Xu, Shilong Bao, Zhiyong Yang, Kangli Zi, Qingming Huang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23639)  

**Abstract**: This paper explores a novel lightweight approach LightFair to achieve fair text-to-image diffusion models (T2I DMs) by addressing the adverse effects of the text encoder. Most existing methods either couple different parts of the diffusion model for full-parameter training or rely on auxiliary networks for correction. They incur heavy training or sampling burden and unsatisfactory performance. Since T2I DMs consist of multiple components, with the text encoder being the most fine-tunable and front-end module, this paper focuses on mitigating bias by fine-tuning text embeddings. To validate feasibility, we observe that the text encoder's neutral embedding output shows substantial skewness across image embeddings of various attributes in the CLIP space. More importantly, the noise prediction network further amplifies this imbalance. To finetune the text embedding, we propose a collaborative distance-constrained debiasing strategy that balances embedding distances to improve fairness without auxiliary references. However, mitigating bias can compromise the original generation quality. To address this, we introduce a two-stage text-guided sampling strategy to limit when the debiased text encoder intervenes. Extensive experiments demonstrate that LightFair is effective and efficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA debiasing at just $1/4$ of the training burden, with virtually no increase in sampling burden. The code is available at this https URL. 

**Abstract (ZH)**: 一种新型轻量级方法LightFair实现公平的文字到图像扩散模型 

---
# RIV: Recursive Introspection Mask Diffusion Vision Language Model 

**Title (ZH)**: 递归introspection掩码扩散视觉语言模型 

**Authors**: YuQian Li, Limeng Qiao, Lin Ma  

**Link**: [PDF](https://arxiv.org/pdf/2509.23625)  

**Abstract**: Mask Diffusion-based Vision Language Models (MDVLMs) have achieved remarkable progress in multimodal understanding tasks. However, these models are unable to correct errors in generated tokens, meaning they lack self-correction capability. In this paper, we propose Recursive Introspection Mask Diffusion Vision Language Model (RIV), which equips the model with self-correction ability through two novel mechanisms. The first is Introspection Training, where an Introspection Model is introduced to identify errors within generated sequences. Introspection Training enables the model to detect not only grammatical and spelling mistakes, but more importantly, logical errors. The second is Recursive Inference. Beginning with the standard unmasking step, the learned Introspection Model helps to identify errors in the output sequence and remask them. This alternating ($\text{unmask}\rightarrow\text{introspection}\rightarrow\text{remask}$) process is repeated recursively until reliable results are obtained. Experimental results on multiple benchmarks demonstrate that the proposed RIV achieves state-of-the-art performance, outperforming most existing MDVLMs. 

**Abstract (ZH)**: 基于递归反省掩码扩散的视觉语言模型（RIV）在多模态理解任务中取得了显著进展，然而这些模型无法纠正生成的令牌中的错误，意味着它们缺乏自我修正能力。本文提出了一种递归反省掩码扩散视觉语言模型（RIV），通过两种新的机制赋予模型自我修正能力。首先是反省训练，引入一种反省模型来识别生成序列中的错误。反省训练使模型不仅能检测到语法和拼写错误，更重要的是，能检测逻辑错误。其次是递归推理。从标准的去掩码步骤开始，学习到的反省模型帮助识别输出序列中的错误并重新掩码。这种交替（去掩码→反省→重新掩码）过程会递归重复，直到获得可靠的结果。在多个基准上的实验结果表明，提出的RIV达到了最先进的性能，优于大多数现有的MDVLMs。 

---
# Generalizable Speech Deepfake Detection via Information Bottleneck Enhanced Adversarial Alignment 

**Title (ZH)**: 基于信息瓶颈增强对抗对齐的通用语音深度假声检测 

**Authors**: Pu Huang, Shouguang Wang, Siya Yao, Mengchu Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2509.23618)  

**Abstract**: Neural speech synthesis techniques have enabled highly realistic speech deepfakes, posing major security risks. Speech deepfake detection is challenging due to distribution shifts across spoofing methods and variability in speakers, channels, and recording conditions. We explore learning shared discriminative features as a path to robust detection and propose Information Bottleneck enhanced Confidence-Aware Adversarial Network (IB-CAAN). Confidence-guided adversarial alignment adaptively suppresses attack-specific artifacts without erasing discriminative cues, while the information bottleneck removes nuisance variability to preserve transferable features. Experiments on ASVspoof 2019/2021, ASVspoof 5, and In-the-Wild demonstrate that IB-CAAN consistently outperforms baseline and achieves state-of-the-art performance on many benchmarks. 

**Abstract (ZH)**: 神经语音合成技术催生了高度逼真的语音深伪，引发了重大安全风险。由于欺骗方法、说话人、信道和录音条件的分布变化，语音深伪检测具有挑战性。我们探索学习共享鉴别特征作为稳健检测的方法，并提出信息瓶颈增强的置信感知对抗网络（IB-CAAN）。置信导向的对抗对齐自适应地抑制攻击特定的-artifacts，同时不抹除鉴别线索，信息瓶颈去除无关变异，保留可迁移特征。实验表明，IB-CAAN 在 ASVspoof 2019/2021、ASVspoof 5 和在野数据上的表现均优于基线方法，并在许多基准测试中达到最佳性能。 

---
# BioVessel-Net and RetinaMix: Unsupervised Retinal Vessel Segmentation from OCTA Images 

**Title (ZH)**: BioVessel-Net和RetinaMix：基于OCTA图像的无监督视网膜血管分割 

**Authors**: Cheng Huang, Weizheng Xie, Fan Gao, Yutong Liu, Ruoling Wu, Zeyu Han, Jingxi Qiu, Xiangxiang Wang, Zhenglin Yang, Hao Wang, Yongbin Yu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23617)  

**Abstract**: Structural changes in retinal blood vessels are critical biomarkers for the onset and progression of glaucoma and other ocular diseases. However, current vessel segmentation approaches largely rely on supervised learning and extensive manual annotations, which are costly, error-prone, and difficult to obtain in optical coherence tomography angiography. Here we present BioVessel-Net, an unsupervised generative framework that integrates vessel biostatistics with adversarial refinement and a radius-guided segmentation strategy. Unlike pixel-based methods, BioVessel-Net directly models vascular structures with biostatistical coherence, achieving accurate and explainable vessel extraction without labeled data or high-performance computing. To support training and evaluation, we introduce RetinaMix, a new benchmark dataset of 2D and 3D OCTA images with high-resolution vessel details from diverse populations. Experimental results demonstrate that BioVessel-Net achieves near-perfect segmentation accuracy across RetinaMix and existing datasets, substantially outperforming state-of-the-art supervised and semi-supervised methods. Together, BioVessel-Net and RetinaMix provide a label-free, computationally efficient, and clinically interpretable solution for retinal vessel analysis, with broad potential for glaucoma monitoring, blood flow modeling, and progression prediction. Code and dataset are available: this https URL. 

**Abstract (ZH)**: 视网膜血管结构的变化是原发性青光眼和其他眼病发病和进展的关键生物标志物。然而，当前的血管分割方法主要依赖于监督学习和广泛的手动标注，这在光学相干断层扫描血管成像中成本高、易出错且难以实现。这里我们提出BioVessel-Net，这是一种无监督生成框架，该框架结合了血管生物统计学和对抗性细化以及半径引导的分割策略。与基于像素的方法不同，BioVessel-Net 直接使用生物统计学一致性建模血管结构，实现了无标签数据和高性能计算的情况下准确且可解释的血管提取。为了支持训练和评估，我们介绍了RetinaMix，这是一个新的基准数据集，包含来自多元化人群的高分辨率2D和3D OCTA图像。实验结果表明，BioVessel-Net 在RetinaMix 和现有数据集上的分割准确性接近完美，显著优于最先进的监督和半监督方法。BioVessel-Net 和RetinaMix 为无标记、计算高效且临床可解释的视网膜血管分析提供了解决方案，具有广泛的应用潜力，包括青光眼监测、血流建模和进展预测。代码和数据集可获取：this https URL。 

---
# GraphIFE: Rethinking Graph Imbalance Node Classification via Invariant Learning 

**Title (ZH)**: GraphIFE: 通过不变学习重新思考图的不均衡节点分类 

**Authors**: Fanlong Zeng, Wensheng Gan, Philip S. Yu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23616)  

**Abstract**: The class imbalance problem refers to the disproportionate distribution of samples across different classes within a dataset, where the minority classes are significantly underrepresented. This issue is also prevalent in graph-structured data. Most graph neural networks (GNNs) implicitly assume a balanced class distribution and therefore often fail to account for the challenges introduced by class imbalance, which can lead to biased learning and degraded performance on minority classes. We identify a quality inconsistency problem in synthesized nodes, which leads to suboptimal performance under graph imbalance conditions. To mitigate this issue, we propose GraphIFE (Graph Invariant Feature Extraction), a novel framework designed to mitigate quality inconsistency in synthesized nodes. Our approach incorporates two key concepts from graph invariant learning and introduces strategies to strengthen the embedding space representation, thereby enhancing the model's ability to identify invariant features. Extensive experiments demonstrate the framework's efficiency and robust generalization, as GraphIFE consistently outperforms various baselines across multiple datasets. The code is publicly available at this https URL. 

**Abstract (ZH)**: 类别不平衡问题指的是数据集中不同类别样本分布不均，其中少数类显著欠代表性。这一问题在图结构数据中也非常普遍。大多数图神经网络（GNNs）隐含地假设类别分布均衡，因此往往未能充分考虑类别不平衡带来的挑战，这可能导致对少数类的学习偏差和性能退化。我们识别出合成节点质量不一致性问题，这在图结构不平衡条件下会导致次优性能。为缓解这一问题，我们提出了一种新颖的框架GraphIFE（图不变特征提取），旨在缓解合成节点的质量不一致性。我们的方法结合了图不变学习中的两个关键概念，并引入策略以增强嵌入空间表示，从而提高模型识别不变特征的能力。广泛的实验展示了该框架的效率和稳健的泛化能力，GraphIFE在多个数据集中均优于各种基线。代码可在以下网址获取。 

---
# InteractMove: Text-Controlled Human-Object Interaction Generation in 3D Scenes with Movable Objects 

**Title (ZH)**: InteractMove: 文本控制的三维场景中可移动对象的人机交互生成 

**Authors**: Xinhao Cai, Minghang Zheng, Xin Jin, Yang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23612)  

**Abstract**: We propose a novel task of text-controlled human object interaction generation in 3D scenes with movable objects. Existing human-scene interaction datasets suffer from insufficient interaction categories and typically only consider interactions with static objects (do not change object positions), and the collection of such datasets with movable objects is difficult and costly. To address this problem, we construct the InteractMove dataset for Movable Human-Object Interaction in 3D Scenes by aligning existing human object interaction data with scene contexts, featuring three key characteristics: 1) scenes containing multiple movable objects with text-controlled interaction specifications (including same-category distractors requiring spatial and 3D scene context understanding), 2) diverse object types and sizes with varied interaction patterns (one-hand, two-hand, etc.), and 3) physically plausible object manipulation trajectories. With the introduction of various movable objects, this task becomes more challenging, as the model needs to identify objects to be interacted with accurately, learn to interact with objects of different sizes and categories, and avoid collisions between movable objects and the scene. To tackle such challenges, we propose a novel pipeline solution. We first use 3D visual grounding models to identify the interaction object. Then, we propose a hand-object joint affordance learning to predict contact regions for different hand joints and object parts, enabling accurate grasping and manipulation of diverse objects. Finally, we optimize interactions with local-scene modeling and collision avoidance constraints, ensuring physically plausible motions and avoiding collisions between objects and the scene. Comprehensive experiments demonstrate our method's superiority in generating physically plausible, text-compliant interactions compared to existing approaches. 

**Abstract (ZH)**: 一种控制文本引导可动物体的人机交互生成在3D场景中的新颖任务：InteractMove数据集构建与方法 

---
# Characteristic Root Analysis and Regularization for Linear Time Series Forecasting 

**Title (ZH)**: 线性时间序列预测中的特征根分析与正则化 

**Authors**: Zheng Wang, Kaixuan Zhang, Wanfang Chen, Xiaonan Lu, Longyuan Li, Tobias Schlagenhauf  

**Link**: [PDF](https://arxiv.org/pdf/2509.23597)  

**Abstract**: Time series forecasting remains a critical challenge across numerous domains, yet the effectiveness of complex models often varies unpredictably across datasets. Recent studies highlight the surprising competitiveness of simple linear models, suggesting that their robustness and interpretability warrant deeper theoretical investigation. This paper presents a systematic study of linear models for time series forecasting, with a focus on the role of characteristic roots in temporal dynamics. We begin by analyzing the noise-free setting, where we show that characteristic roots govern long-term behavior and explain how design choices such as instance normalization and channel independence affect model capabilities. We then extend our analysis to the noisy regime, revealing that models tend to produce spurious roots. This leads to the identification of a key data-scaling property: mitigating the influence of noise requires disproportionately large training data, highlighting the need for structural regularization. To address these challenges, we propose two complementary strategies for robust root restructuring. The first uses rank reduction techniques, including Reduced-Rank Regression and Direct Weight Rank Reduction, to recover the low-dimensional latent dynamics. The second, a novel adaptive method called Root Purge, encourages the model to learn a noise-suppressing null space during training. Extensive experiments on standard benchmarks demonstrate the effectiveness of both approaches, validating our theoretical insights and achieving state-of-the-art results in several settings. Our findings underscore the potential of integrating classical theories for linear systems with modern learning techniques to build robust, interpretable, and data-efficient forecasting models. 

**Abstract (ZH)**: 时间序列预测仍然是诸多领域中的一个关键挑战，但复杂模型在不同数据集上的有效性往往不可预测。近期研究表明，简单的线性模型表现出令人惊讶的竞争性，这表明其鲁棒性和可解释性应进行更深入的理论探讨。本文对线性模型在时间序列预测中的应用进行了系统研究，重点关注特征根在时序动态中的作用。我们首先分析了无噪声的情境，证明了特征根决定了长期行为，并解释了诸如实例归一化和通道独立性等设计选择如何影响模型能力。随后，我们将分析扩展到了有噪声的情境，揭示模型倾向于生成虚假根。这导致我们识别出一个关键的数据缩放特性：减轻噪声影响需要不成比例的大量训练数据，突显了结构正则化的需求。为了应对这些挑战，我们提出了两种互补的稳健特征根重构策略。第一种使用秩约简技术，包括降秩回归和直接权重秩约简，以恢复低维潜在动力。第二种是新颖的自适应方法根净化（Root Purge），鼓励模型在训练期间学习一个抑制噪声的零空间。在标准基准上的详尽实验表明，这两种方法都证明了其有效性，验证了我们的理论洞见，并在某些场景下达到了最先进的结果。我们的研究结果强调了将经典线性系统理论与现代学习技术结合起来以构建稳健、可解释和数据高效的预测模型的潜力。 

---
# Multi-Level Heterogeneous Knowledge Transfer Network on Forward Scattering Center Model for Limited Samples SAR ATR 

**Title (ZH)**: 多层次异质知识转移网络在前散射中心模型下的限样雷达瞄准识别 

**Authors**: Chenxi Zhao, Daochang Wang, Siqian Zhang, Gangyao Kuang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23596)  

**Abstract**: Simulated data-assisted SAR target recognition methods are the research hotspot currently, devoted to solving the problem of limited samples. Existing works revolve around simulated images, but the large amount of irrelevant information embedded in the images, such as background, noise, etc., seriously affects the quality of the migrated information. Our work explores a new simulated data to migrate purer and key target knowledge, i.e., forward scattering center model (FSCM) which models the actual local structure of the target with strong physical meaning and interpretability. To achieve this purpose, multi-level heterogeneous knowledge transfer (MHKT) network is proposed, which fully migrates FSCM knowledge from the feature, distribution and category levels, respectively. Specifically, we permit the more suitable feature representations for the heterogeneous data and separate non-informative knowledge by task-associated information selector (TAIS), to complete purer target feature migration. In the distribution alignment, the new metric function maximum discrimination divergence (MDD) in target generic knowledge transfer (TGKT) module perceives transferable knowledge efficiently while preserving discriminative structure about classes. Moreover, category relation knowledge transfer (CRKT) module leverages the category relation consistency constraint to break the dilemma of optimization bias towards simulation data due to imbalance between simulated and measured data. Such stepwise knowledge selection and migration will ensure the integrity of the migrated FSCM knowledge. Notably, extensive experiments on two new datasets formed by FSCM data and measured SAR images demonstrate the superior performance of our method. 

**Abstract (ZH)**: 基于模拟数据辅助的SAR目标识别方法：探索纯净目标知识迁移的新途径 

---
# Timber: Training-free Instruct Model Refining with Base via Effective Rank 

**Title (ZH)**: Timber: 不依赖训练的基模型有效秩驱动的指令模型精炼 

**Authors**: Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Zenan Xu, Ngai Wong  

**Link**: [PDF](https://arxiv.org/pdf/2509.23595)  

**Abstract**: Post-training, which elicits a pretrained Base model into the corresponding Instruct model, is widely considered to be superficial. In this work, we first reinforce this hypothesis by providing novel quantitative evidence from the weight level that the effective rank (eRank) remains negligibly changed. However, this superficiality also suffers a critical trade-off, improving the exploitation capabilities at the cost of limiting its exploration. To tackle this issue, we propose Timber, a simple yet effective training-free method that enhances the exploration capability of the Instruct model while preserving its exploitation. The key insight is to partially revert Instruct towards the paired Base model by subtle yet targeted refinement of the weight deltas. Extensive experiments on Llama and Qwen series demonstrate that Timber consistently improves vanilla Instruct models, particularly on Pass@k performance. Our findings offer new insights into the post-training stage at the weight level and practical strategies to refine the Instruct model without training. 

**Abstract (ZH)**: Post-训练通过将预训练基模型激发为相应的指令模型，通常被认为是一种浅层方法。在本文中，我们首先通过在权重层面提供新颖的定量证据来加强这一假设，即有效的秩（eRank）几乎保持不变。然而，这种浅层性也面临一个关键的权衡：改进利用能力的同时限制了探索能力。为解决这一问题，我们提出了一种简单而有效的无需训练方法——Timber，该方法增强了指令模型的探索能力，同时保持其利用能力。关键见解是通过微妙且有针对性的权重差值调整部分逆转指令模型向配对基模型。针对Llama和Qwen系列的 extensive 实验表明，Timber 一致地改进了 vanilla 指令模型，特别是在 Pass@k 性能上。我们的发现提供了关于权重层面后训练阶段的新见解，并提出了在无需训练的情况下细化指令模型的实用策略。 

---
# Toward a Holistic Approach to Continual Model Merging 

**Title (ZH)**: 走向综合性的持续模型合并方法 

**Authors**: Hoang Phan, Sungmin Cha, Tung Lam Tran, Qi Lei  

**Link**: [PDF](https://arxiv.org/pdf/2509.23592)  

**Abstract**: We present a holistic framework for continual model merging that intervenes at three critical stages: pre-merging, during merging, and post-merging-to address two fundamental challenges in continual learning. In particular, conventional approaches either maintain a growing list of per-domain task vectors, leading to scalability issues or rely solely on weight-space merging when old data is inaccessible, thereby losing crucial functional information. Our method overcomes these limitations by first fine-tuning the main model within its tangent space on domain-specific data; this linearization amplifies per-task weight disentanglement, effectively mitigating across-task interference. During merging, we leverage functional information from available optimizer states beyond mere parameter averages to avoid the need to revisit old data. Finally, a post-merging correction aligns the representation discrepancy between pre- and post-merged models, reducing bias and enhancing overall performance-all while operating under constant memory constraints without accessing historical data. Extensive experiments on standard class-incremental and domain-incremental benchmarks demonstrate that our approach not only achieves competitive performance but also provides a scalable and efficient solution to the catastrophic forgetting problem. 

**Abstract (ZH)**: 我们提出了一种面向持续学习的集成模型全面框架，该框架干预了合并前三 Critial 阶段：合并前、合并中和合并后，以应对持续学习中的两大根本挑战。具体而言，传统方法要么维护一个不断增长的领域特定任务向量列表，导致可扩展性问题，要么仅依赖于权重空间的合并，当旧数据不可访问时丢失重要的功能信息。我们的方法通过首先在领域特定数据上将主模型在其切线空间内进行微调，从而克服了这些限制；这种线性化增强了任务间权重的分离，有效地降低了跨任务干扰。在合并过程中，我们利用可用优化器状态中的功能信息，而不仅仅是参数平均值，以避免重新访问旧数据。最后，在合并后，通过校准合并前和合并后模型之间的表示差异，减少偏差并提高整体性能，同时在不访问历史数据的情况下保持恒定的内存约束。在标准的类增量和领域增量基准测试中的广泛实验表明，我们的方法不仅实现了竞争力的表现，还提供了一种可扩展且高效的解决灾难性遗忘问题的解决方案。 

---
# Improving the Efficiency of LLM Agent Systems through Trajectory Reduction 

**Title (ZH)**: 通过轨迹减少提高大语言模型代理系统的效率 

**Authors**: Yuan-An Xiao, Pengfei Gao, Chao Peng, Yingfei Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2509.23586)  

**Abstract**: Multi-turn agent systems based on Large Language Models (LLMs) have been increasingly popular for software engineering tasks. While LLM agents show decent effectiveness, the high computational cost of input tokens due to the ever-growing trajectory remains an efficiency concern for their applications. Efficiency is largely neglected in existing studies and agent products, and this paper fills the gap by introducing an inference-time trajectory reduction approach to reduce the cost of agents.
Through analyzing existing agent trajectories, we demonstrate that useless, redundant, and expired information is widespread in all trajectories, which can be identified and reduced without harming the agent's performance. We then design a simple yet effective trajectory reduction approach, AgentDiet, which automatically removes such waste information. We implement AgentDiet on a top-performing coding agent, and the evaluation on two LLMs and two benchmarks shows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final computational cost by 21.1% ~ 35.9%, while maintaining the same agent performance. This indicates that trajectory reduction is a promising direction for agent systems. 

**Abstract (ZH)**: 基于大型语言模型（LLMs）的多轮代理系统在软件工程任务中日益流行。尽管LLM代理表现出色，但由于轨迹不断增长导致的输入令牌计算成本高，仍然是应用程序中的效率问题。这一效率问题在现有研究和代理产品中往往被忽视，本文通过引入推理时轨迹缩减方法来降低代理的成本，填补了这一空白。 

---
# ML-Asset Management: Curation, Discovery, and Utilization 

**Title (ZH)**: ML-资产管理系统：编目、发现与利用 

**Authors**: Mengying Wang, Moming Duan, Yicong Huang, Chen Li, Bingsheng He, Yinghui Wu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23577)  

**Abstract**: Machine learning (ML) assets, such as models, datasets, and metadata, are central to modern ML workflows. Despite their explosive growth in practice, these assets are often underutilized due to fragmented documentation, siloed storage, inconsistent licensing, and lack of unified discovery mechanisms, making ML-asset management an urgent challenge. This tutorial offers a comprehensive overview of ML-asset management activities across its lifecycle, including curation, discovery, and utilization. We provide a categorization of ML assets, and major management issues, survey state-of-the-art techniques, and identify emerging opportunities at each stage. We further highlight system-level challenges related to scalability, lineage, and unified indexing. Through live demonstrations of systems, this tutorial equips both researchers and practitioners with actionable insights and practical tools for advancing ML-asset management in real-world and domain-specific settings. 

**Abstract (ZH)**: 机器学习资产管理：从收集、发现到利用的全面概述 

---
# Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales 

**Title (ZH)**: 基于少量理由实现高效CoT提纯：自我引导的理由选择器以更好性能选择更少的理由 

**Authors**: Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Yang Xiang, Buzhou Tang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23574)  

**Abstract**: Chain-of-thought (CoT) distillation aims to enhance small language models' (SLMs) reasoning by transferring multi-step reasoning capability from the larger teacher models. However, existing work underestimates rationale quality, focusing primarily on data quantity, which may transfer noisy or incorrect information to the student model. To address the above issues, we proposed \textbf{M}odel-\textbf{O}riented \textbf{R}ationale \textbf{S}election \textbf{D}istillation (MoRSD), which can discern and select high quality rationales for distillation to improve performance further. We further propose a Rationale Difficulty (RD) metric to measure the ability of the student model to generate the correct answer under a given rationale. Compared to the baseline, we achieved 4.6$\%$ average improvement on seven datasets over three tasks, using fewer rationales by controlling their accuracy, diversity, and difficulty. Our results reveal that a small portion of the high quality rationales can enhance the reasoning ability of student models than the entire dataset. Our method promises to be a possible solution for efficient CoT distillation. Our code will be released in this https URL. 

**Abstract (ZH)**: 基于模型导向的高质量论据蒸馏（Model-Oriented Rationale Selection Distillation）旨在通过从大型教师模型转移多步推理能力来增强小型语言模型（SLMs）的推理能力。然而，现有工作未能充分重视论据质量，主要关注数据量，这可能导致噪声或错误信息转移到学生模型中。为解决上述问题，我们提出了基于模型导向的高质量论据选择蒸馏（MoRSD），可以区分和选择高质量的论据以进行蒸馏，从而进一步提高性能。我们还提出了论据难度（RD）度量，用于衡量学生模型在给定论据下生成正确答案的能力。与基线相比，我们通过控制论据的准确性、多样性和难度，在三项任务的七个数据集上实现了平均4.6%的性能提升。我们的结果表明，一小部分高质量的论据可以比整个数据集更有效地增强学生模型的推理能力。我们的方法有望成为高效链推理蒸馏的一种可能解决方案。我们的代码将发布在https://github.com/Qwen233/Model-Oriented-Rationale-Selection-Distillation。 

---
# Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence 

**Title (ZH)**: 揭示LLM辅助下的网络威胁intelligence脆弱性 

**Authors**: Yuqiao Meng, Luoxi Tang, Feiyang Yu, Jinyuan Jia, Guanhua Yan, Ping Yang, Zhaohan Xi  

**Link**: [PDF](https://arxiv.org/pdf/2509.23573)  

**Abstract**: Large Language Models (LLMs) are intensively used to assist security analysts in counteracting the rapid exploitation of cyber threats, wherein LLMs offer cyber threat intelligence (CTI) to support vulnerability assessment and incident response. While recent work has shown that LLMs can support a wide range of CTI tasks such as threat analysis, vulnerability detection, and intrusion defense, significant performance gaps persist in practical deployments. In this paper, we investigate the intrinsic vulnerabilities of LLMs in CTI, focusing on challenges that arise from the nature of the threat landscape itself rather than the model architecture. Using large-scale evaluations across multiple CTI benchmarks and real-world threat reports, we introduce a novel categorization methodology that integrates stratification, autoregressive refinement, and human-in-the-loop supervision to reliably analyze failure instances. Through extensive experiments and human inspections, we reveal three fundamental vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization, that limit LLMs in effectively supporting CTI. Subsequently, we provide actionable insights for designing more robust LLM-powered CTI systems to facilitate future research. 

**Abstract (ZH)**: 大型语言模型（LLMs）在协助安全分析师对抗快速发展的网络威胁中被广泛使用，其中LLMs提供网络威胁情报（CTI）以支持漏洞评估和事件响应。尽管近期研究表明LLMs可以支持广泛范围的CTI任务，如威胁分析、漏洞检测和入侵防御，但在实际部署中仍存在显著的性能差距。本文探讨了LLMs在CTI中的内在脆弱性，重点关注源自威胁环境本质的挑战而非模型架构本身。通过跨多个CTI基准和真实世界威胁报告的大规模评估，我们引入了一种新的分类方法，该方法结合了分层、自回归精炼和人工监督，以可靠地分析失败实例。通过广泛的实验和人工检查，我们揭示了三种限制LLMs有效支持CTI的基本脆弱性：虚假相关性、矛盾的知识以及受限的泛化能力。随后，我们提供了设计更健壮的LLM赋能CTI系统的实用见解，以促进未来的研究。 

---
# Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting 

**Title (ZH)**: 基于标准化威胁猎捕对LLM辅助蓝队进行基准测试 

**Authors**: Yuqiao Meng, Luoxi Tang, Feiyang Yu, Xi Li, Guanhua Yan, Ping Yang, Zhaohan Xi  

**Link**: [PDF](https://arxiv.org/pdf/2509.23571)  

**Abstract**: As cyber threats continue to grow in scale and sophistication, blue team defenders increasingly require advanced tools to proactively detect and mitigate risks. Large Language Models (LLMs) offer promising capabilities for enhancing threat analysis. However, their effectiveness in real-world blue team threat-hunting scenarios remains insufficiently explored. This paper presents CyberTeam, a benchmark designed to guide LLMs in blue teaming practice. CyberTeam constructs a standardized workflow in two stages. First, it models realistic threat-hunting workflows by capturing the dependencies among analytical tasks from threat attribution to incident response. Next, each task is addressed through a set of operational modules tailored to its specific analytical requirements. This transforms threat hunting into a structured sequence of reasoning steps, with each step grounded in a discrete operation and ordered according to task-specific dependencies. Guided by this framework, LLMs are directed to perform threat-hunting tasks through modularized steps. Overall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs through standardized threat analysis. We evaluate both leading LLMs and state-of-the-art cybersecurity agents, comparing CyberTeam against open-ended reasoning strategies. Our results highlight the improvements enabled by standardized design, while also revealing the limitations of open-ended reasoning in real-world threat hunting. 

**Abstract (ZH)**: 随着网络威胁规模和复杂性的不断增长，蓝队防御者越来越需要先进的工具来进行主动威胁检测和风险缓解。大规模语言模型（LLMs）在增强威胁分析方面展现出巨大的潜力。然而，它们在实际蓝队威胁狩猎场景中的有效性仍需进一步探索。本文介绍了CyberTeam，一个旨在引导LLMs进行蓝队实践的基准。CyberTeam通过两个阶段构建了一个标准化的工作流程。首先，通过捕捉从威胁归因到事件响应的分析任务之间的依赖关系，模拟现实的威胁狩猎工作流程。其次，通过一组定制的操作模块来解决每个任务，这些模块针对其特定的分析需求进行设计。这将威胁狩猎转化为一个结构化的推理步骤序列，每个步骤都基于一个离散的操作，并按照任务特定的依赖关系进行排序。在这一框架的指引下，LLMs通过模块化的步骤执行威胁狩猎任务。总体而言，CyberTeam整合了30个任务和9个操作模块，以引导LLMs进行标准化的威胁分析。我们评估了领先的大规模语言模型和最先进的人工智能安全代理，并将CyberTeam与开放性推理策略进行比较。我们的结果突显了标准化设计带来的改进，同时也揭示了开放性推理在真实世界威胁狩猎中的局限性。 

---
# Node Classification via Simplicial Interaction with Augmented Maximal Clique Selection 

**Title (ZH)**: 基于增强最大闭包的选择的简化体交互节点分类 

**Authors**: Eunho Koo, Tongseok Lim  

**Link**: [PDF](https://arxiv.org/pdf/2509.23568)  

**Abstract**: Considering higher-order interactions allows for a more comprehensive understanding of network structures beyond simple pairwise connections. While leveraging all cliques in a network to handle higher-order interactions is intuitive, it often leads to computational inefficiencies due to overlapping information between higher-order and lower-order cliques. To address this issue, we propose an augmented maximal clique strategy. Although using only maximal cliques can reduce unnecessary overlap and provide a concise representation of the network, certain nodes may still appear in multiple maximal cliques, resulting in imbalanced training data. Therefore, our augmented maximal clique approach selectively includes some non-maximal cliques to mitigate the overrepresentation of specific nodes and promote more balanced learning across the network. Comparative analyses on synthetic networks and real-world citation datasets demonstrate that our method outperforms approaches based on pairwise interactions, all cliques, or only maximal cliques. Finally, by integrating this strategy into GNN-based semi-supervised learning, we establish a link between maximal clique-based methods and GNNs, showing that incorporating higher-order structures improves predictive accuracy. As a result, the augmented maximal clique strategy offers a computationally efficient and effective solution for higher-order network learning. 

**Abstract (ZH)**: 考虑高阶交互关系有助于超越简单二元连接，更全面地理解网络结构。尽管利用网络中的所有团来处理高阶交互直观易行，但由于高阶和低阶团之间存在重叠信息，往往会引发计算效率低下问题。为解决此问题，我们提出了一种增强最大团策略。虽然仅使用最大团可以减少不必要的重叠并提供网络的简洁表示，但某些节点仍然可能出现在多个最大团中，导致训练数据不平衡。因此，我们的增强最大团方法有选择地包括一些非最大团，以减轻特定节点的过度代表，促进网络更均衡的学习。在合成网络和实际引用数据集上的对比分析表明，我们的方法优于基于二元交互、所有团或仅最大团的方法。最后，通过将此策略整合到基于图神经网络的半监督学习中，我们建立了基于最大团方法与图神经网络之间的联系，证明引入高阶结构可以提高预测准确性。因此，增强最大团策略提供了一种计算上高效且有效的高阶网络学习解决方案。 

---
# RAVEN: Resilient Aerial Navigation via Open-Set Semantic Memory and Behavior Adaptation 

**Title (ZH)**: RAVEN：基于开放集语义记忆和行为适应的鲁棒航空导航 

**Authors**: Seungchan Kim, Omar Alama, Dmytro Kurdydyk, John Keller, Nikhil Keetha, Wenshan Wang, Yonatan Bisk, Sebastian Scherer  

**Link**: [PDF](https://arxiv.org/pdf/2509.23563)  

**Abstract**: Aerial outdoor semantic navigation requires robots to explore large, unstructured environments to locate target objects. Recent advances in semantic navigation have demonstrated open-set object-goal navigation in indoor settings, but these methods remain limited by constrained spatial ranges and structured layouts, making them unsuitable for long-range outdoor search. While outdoor semantic navigation approaches exist, they either rely on reactive policies based on current observations, which tend to produce short-sighted behaviors, or precompute scene graphs offline for navigation, limiting adaptability to online deployment. We present RAVEN, a 3D memory-based, behavior tree framework for aerial semantic navigation in unstructured outdoor environments. It (1) uses a spatially consistent semantic voxel-ray map as persistent memory, enabling long-horizon planning and avoiding purely reactive behaviors, (2) combines short-range voxel search and long-range ray search to scale to large environments, (3) leverages a large vision-language model to suggest auxiliary cues, mitigating sparsity of outdoor targets. These components are coordinated by a behavior tree, which adaptively switches behaviors for robust operation. We evaluate RAVEN in 10 photorealistic outdoor simulation environments over 100 semantic tasks, encompassing single-object search, multi-class, multi-instance navigation and sequential task changes. Results show RAVEN outperforms baselines by 85.25% in simulation and demonstrate its real-world applicability through deployment on an aerial robot in outdoor field tests. 

**Abstract (ZH)**: 基于空中环境的户外语义导航要求机器人探索大型非结构化环境以定位目标物体。最近在语义导航方面的进展展示了室内环境下的开放集合语义导航，但这些方法仍受限于有限的空间范围和结构化布局，使其不适合进行远程户外搜索。虽然存在户外语义导航方法，但它们要么依赖于基于当前观察的反应策略，容易产生短视行为，要么需要提前在线下计算场景图以进行导航，限制了其在线部署的适应性。我们提出了RAVEN，一种基于3D记忆的行为树框架，用于非结构化户外环境的语义导航。(1) 使用空间一致的语义体素射线图作为持久性记忆，实现长期规划并避免完全反应性行为；(2) 结合短距离体素搜索和长距离射线搜索，以适应大型环境；(3) 利用大规模的视觉语言模型提出辅助线索，缓解户外目标稀疏性。这些组件由行为树协调，能够自适应切换行为以实现稳健操作。我们通过在10个逼真的户外仿真环境中执行100个语义任务来评估RAVEN，包括单对象搜索、多类多实例导航以及顺序任务变化。结果表明，RAVEN在仿真中的性能优于基线85.25%，并通过在户外实地测试中部署到空中机器人上展示了其实用性。 

---
# Pancreas Part Segmentation under Federated Learning Paradigm 

**Title (ZH)**: 联邦学习范式下的胰腺部分分割 

**Authors**: Ziliang Hong, Halil Ertugrul Aktas, Andrea Mia Bejar, Katherine Wu, Hongyi Pan, Gorkem Durak, Zheyuan Zhang, Sait Kayali, Temel Tirkes, Federica Proietto Salanitri, Concetto Spampinato, Michael Goggins, Tamas Gonda, Candice Bolan, Raj Keswani, Frank Miller, Michael Wallace, Ulas Bagci  

**Link**: [PDF](https://arxiv.org/pdf/2509.23562)  

**Abstract**: We present the first federated learning (FL) approach for pancreas part(head, body and tail) segmentation in MRI, addressing a critical clinical challenge as a significant innovation. Pancreatic diseases exhibit marked regional heterogeneity cancers predominantly occur in the head region while chronic pancreatitis causes tissue loss in the tail, making accurate segmentation of the organ into head, body, and tail regions essential for precise diagnosis and treatment planning. This segmentation task remains exceptionally challenging in MRI due to variable morphology, poor soft-tissue contrast, and anatomical variations across patients. Our novel contribution tackles two fundamental challenges: first, the technical complexity of pancreas part delineation in MRI, and second the data scarcity problem that has hindered prior approaches. We introduce a privacy-preserving FL framework that enables collaborative model training across seven medical institutions without direct data sharing, leveraging a diverse dataset of 711 T1W and 726 T2W MRI scans. Our key innovations include: (1) a systematic evaluation of three state-of-the-art segmentation architectures (U-Net, Attention U-Net,Swin UNETR) paired with two FL algorithms (FedAvg, FedProx), revealing Attention U-Net with FedAvg as optimal for pancreatic heterogeneity, which was never been done before; (2) a novel anatomically-informed loss function prioritizing region-specific texture contrasts in MRI. Comprehensive evaluation demonstrates that our approach achieves clinically viable performance despite training on distributed, heterogeneous datasets. 

**Abstract (ZH)**: 我们介绍了首个用于MRI中胰腺部分（头、体、尾）分割的联邦学习方法，解决了临床中的一个关键挑战，是一项重要的创新。 

---
# Fusing Sequence Motifs and Pan-Genomic Features: Antimicrobial Resistance Prediction using an Explainable Lightweight 1D CNN-XGBoost Ensemble 

**Title (ZH)**: 融合序列motif和泛基因组特征：基于可解释轻量级1D CNN-XGBoost集成的抗菌药物耐药性预测 

**Authors**: Md. Saiful Bari Siddiqui, Nowshin Tarannum  

**Link**: [PDF](https://arxiv.org/pdf/2509.23552)  

**Abstract**: Antimicrobial Resistance (AMR) is a rapidly escalating global health crisis. While genomic sequencing enables rapid prediction of resistance phenotypes, current computational methods have limitations. Standard machine learning models treat the genome as an unordered collection of features, ignoring the sequential context of Single Nucleotide Polymorphisms (SNPs). State-of-the-art sequence models like Transformers are often too data-hungry and computationally expensive for the moderately-sized datasets that are typical in this domain. To address these challenges, we propose AMR-EnsembleNet, an ensemble framework that synergistically combines sequence-based and feature-based learning. We developed a lightweight, custom 1D Convolutional Neural Network (CNN) to efficiently learn predictive sequence motifs from high-dimensional SNP data. This sequence-aware model was ensembled with an XGBoost model, a powerful gradient boosting system adept at capturing complex, non-local feature interactions. We trained and evaluated our framework on a benchmark dataset of 809 E. coli strains, predicting resistance across four antibiotics with varying class imbalance. Our 1D CNN-XGBoost ensemble consistently achieved top-tier performance across all the antibiotics, reaching a Matthews Correlation Coefficient (MCC) of 0.926 for Ciprofloxacin (CIP) and the highest Macro F1-score of 0.691 for the challenging Gentamicin (GEN) AMR prediction. We also show that our model consistently focuses on SNPs within well-known AMR genes like fusA and parC, confirming it learns the correct genetic signals for resistance. Our work demonstrates that fusing a sequence-aware 1D CNN with a feature-based XGBoost model creates a powerful ensemble, overcoming the limitations of using either an order-agnostic or a standalone sequence model. 

**Abstract (ZH)**: 抗微生物耐药性（AMR）是亟待应对的全球健康危机。基因组测序能够实现快速预测耐药表型，但现有的计算方法存在局限性。标准机器学习模型将基因组视为无序的特征集合，忽略了单核苷酸多态性（SNPs）的序列上下文。最先进的序列模型如变换器通常因数据需求大且计算成本高而难以应用于该领域典型的中等规模数据集。为了应对这些挑战，我们提出AMR-EnsembleNet，一种结合序列基础学习和特征基础学习的集成框架。我们开发了一个轻量级的自定义1D卷积神经网络（CNN），能够高效地从高维SNP数据中学习预测性序列模式。该序列感知模型与强大的梯度提升系统XGBoost模型进行集成，后者擅长捕捉复杂且非局部的特征交互。我们在包含809株大肠杆菌的标准数据集上训练和评估了我们的框架，针对四类抗生素下的不同类别不平衡进行耐药预测。我们的1D CNN-XGBoost集成框架在所有抗生素上实现了最优性能，CIP的马修相关系数（MCC）达到0.926，GEN的宏F1分数达到0.691，这是最具挑战性的耐药预测。我们还展示了模型始终关注诸如fusA和parC等已知抗药基因中的SNP，证实了它学习到正确的遗传信号。我们的工作证明，将序列感知的1D CNN与特征基础的XGBoost模型融合形成强大的集成框架，能够克服单独使用无序感知模型或仅序列模型的局限性。 

---
# Automatic Speech Recognition for Greek Medical Dictation 

**Title (ZH)**: 希腊医疗口述的自动语音识别 

**Authors**: Vardis Georgilas, Themos Stafylakis  

**Link**: [PDF](https://arxiv.org/pdf/2509.23550)  

**Abstract**: Medical dictation systems are essential tools in modern healthcare, enabling accurate and efficient conversion of speech into written medical documentation. The main objective of this paper is to create a domain-specific system for Greek medical speech transcriptions. The ultimate goal is to assist healthcare professionals by reducing the overload of manual documentation and improving workflow efficiency. Towards this goal, we develop a system that combines automatic speech recognition techniques with text correction model, allowing better handling of domain-specific terminology and linguistic variations in Greek. Our approach leverages both acoustic and textual modeling to create more realistic and reliable transcriptions. We focused on adapting existing language and speech technologies to the Greek medical context, addressing challenges such as complex medical terminology and linguistic inconsistencies. Through domain-specific fine-tuning, our system achieves more accurate and coherent transcriptions, contributing to the development of practical language technologies for the Greek healthcare sector. 

**Abstract (ZH)**: 医学口述系统是现代医疗保健中不可或缺的工具，能够实现语音到书面医疗文档的准确高效转换。本文的主要目标是为希腊医学语音转录创建一个专用系统。最终目标是通过减轻手写文档的负担并提高工作流程效率来辅助医疗专业人员。为了实现这一目标，我们开发了一个结合自动语音识别技术和文本校正模型的系统，以更好地处理希腊医学领域的专有名词和语言变异。我们的方法结合了声学和文本建模，以生成更加真实可靠的转录。我们专注于将现有的语言和技术适应希腊医疗背景，解决复杂医学术语和语言不一致等挑战。通过领域特定的微调，我们的系统实现了更准确和连贯的转录，为希腊医疗保健领域的发展贡献了实用的语言技术。 

---
# Disentanglement of Variations with Multimodal Generative Modeling 

**Title (ZH)**: 多模态生成建模中变异因素的解耦 

**Authors**: Yijie Zhang, Yiyang Shen, Weiran Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23548)  

**Abstract**: Multimodal data are prevalent across various domains, and learning robust representations of such data is paramount to enhancing generation quality and downstream task performance. To handle heterogeneity and interconnections among different modalities, recent multimodal generative models extract shared and private (modality-specific) information with two separate variables. Despite attempts to enforce disentanglement between these two variables, these methods struggle with challenging datasets where the likelihood model is insufficient. In this paper, we propose Information-disentangled Multimodal VAE (IDMVAE) to explicitly address this issue, with rigorous mutual information-based regularizations, including cross-view mutual information maximization for extracting shared variables, and a cycle-consistency style loss for redundancy removal using generative augmentations. We further introduce diffusion models to improve the capacity of latent priors. These newly proposed components are complementary to each other. Compared to existing approaches, IDMVAE shows a clean separation between shared and private information, demonstrating superior generation quality and semantic coherence on challenging datasets. 

**Abstract (ZH)**: 多模态数据在多个领域普遍存在，学习such数据的稳健表示对于提高生成质量和下游任务性能至关重要。为了处理不同模态之间的异质性和相互关联性，最近提出的多模态生成模型通过两个独立的变量来提取共享和特定于模态的私有信息。尽管试图在这些变量之间引入解耦，但在复杂数据集上，当似然模型不足时，这些方法仍难以应对。在本文中，我们提出了一种信息解耦多模态VAE（IDMVAE）来明确解决这一问题，通过严格的互信息正则化，包括跨视图互信息最大化以提取共享变量，以及通过生成增强实现冗余去除的循环一致性损失。此外，我们引入了扩散模型以提高潜在先验的能力。这些新提出的组件彼此互补。与现有方法相比，IDMVAE在复杂数据集上显示出了共享和私有信息的清晰分离，展示了更优秀的生成质量和语义一致性。 

---
# End-to-End Deep Learning for Predicting Metric Space-Valued Outputs 

**Title (ZH)**: 端到端深度学习用于预测度量空间值输出 

**Authors**: Yidong Zhou, Su I Iao, Hans-Georg Müller  

**Link**: [PDF](https://arxiv.org/pdf/2509.23544)  

**Abstract**: Many modern applications involve predicting structured, non-Euclidean outputs such as probability distributions, networks, and symmetric positive-definite matrices. These outputs are naturally modeled as elements of general metric spaces, where classical regression techniques that rely on vector space structure no longer apply. We introduce E2M (End-to-End Metric regression), a deep learning framework for predicting metric space-valued outputs. E2M performs prediction via a weighted Fréchet means over training outputs, where the weights are learned by a neural network conditioned on the input. This construction provides a principled mechanism for geometry-aware prediction that avoids surrogate embeddings and restrictive parametric assumptions, while fully preserving the intrinsic geometry of the output space. We establish theoretical guarantees, including a universal approximation theorem that characterizes the expressive capacity of the model and a convergence analysis of the entropy-regularized training objective. Through extensive simulations involving probability distributions, networks, and symmetric positive-definite matrices, we show that E2M consistently achieves state-of-the-art performance, with its advantages becoming more pronounced at larger sample sizes. Applications to human mortality distributions and New York City taxi networks further demonstrate the flexibility and practical utility of the framework. 

**Abstract (ZH)**: 端到端度量回归：一种预测一般度量空间输出的深度学习框架 

---
# On the Shelf Life of Fine-Tuned LLM Judges: Future Proofing, Backward Compatibility, and Question Generalization 

**Title (ZH)**: fine-tuned LLM法官的保质期：未来防护、向后兼容与问题泛化 

**Authors**: Janvijay Singh, Austin Xu, Yilun Zhou, Yefan Zhou, Dilek Hakkani-Tur, Shafiq Joty  

**Link**: [PDF](https://arxiv.org/pdf/2509.23542)  

**Abstract**: The LLM-as-a-judge paradigm is widely used in both evaluating free-text model responses and reward modeling for model alignment and finetuning. Recently, finetuning judges with judge-specific data has emerged as an often preferred choice over directly prompting frontier models as judges, as the former achieves better performance with smaller model sizes while being more robust to common biases. However, the standard evaluation ignores several practical concerns of finetuned judges regarding their real world deployment. In this paper, we identify and formalize three aspects that affect the shelf life of these judges: future proofing and backward compatibility -- how well judges finetuned on responses by today's generator models perform on responses by future models or past models, as well as question generalization -- how well judges generalize to unseen questions at test time. We study these three aspects in the math domain under a unified framework with varying train and test distributions, three SFT- and DPO-based finetuning algorithms and three different base models. Experiments suggest that future-proofing is challenging for most models, while backward compatibility is relatively easy, with DPO-trained models consistently improving performance. We further find that continual learning provides a more balanced adaptation to shifts between older and newer response distributions than training solely on stronger or weaker responses. Moreover, all models observe certain degrees of performance degradation when moving from questions seen during training to unseen ones, showing that current judges do not fully generalize to unseen questions. These findings provide insights into practical considerations for developing and deploying judge models in the face of ever-changing generators. 

**Abstract (ZH)**: LLM作为裁判的范式在评估自由文本模型响应和模型对齐及微调的奖励建模中广泛应用。最近，使用特定数据集微调裁判成为了一种常见的选择，相较于直接提示前沿模型作为裁判，该方法在模型规模更小的情况下表现更好且对常见偏差更具稳健性。然而，标准评估忽略了微调裁判在实际部署中的几个实际问题。在本文中，我们识别并形式化了影响这些裁判寿命的三个方面：未来兼容性和向后兼容性——今天生成模型生成的响应上微调的裁判在未来或过去模型生成的响应上表现良好程度，以及问题泛化——裁判在测试时对未见过的问题的表现。我们在数学领域，在变化的训练和测试分布下，使用三个SFT-和DPO基于的微调算法和三种不同的基模型，研究了这三个方面。实验表明，大多数模型在未来兼容性方面具有挑战性，而向后兼容性相对较容易，DPO训练的模型始终能提高性能。我们进一步发现，持续学习比仅在更强或更弱的响应上进行训练更能平衡旧响应分布和新响应分布之间的变化。此外，所有模型在从训练时见过的问题转移到未见过的问题时都观察到一定程度的性能下降，表明当前裁判未能完全泛化到未见过的问题。这些发现提供了一些建议，以应对不断变化的生成器在开发和部署裁判模型时的实际考虑。 

---
# Imaging-Based Mortality Prediction in Patients with Systemic Sclerosis 

**Title (ZH)**: 基于成像的系统性硬化症患者 mortality 预测 

**Authors**: Alec K. Peltekian, Karolina Senkow, Gorkem Durak, Kevin M. Grudzinski, Bradford C. Bemiss, Jane E. Dematte, Carrie Richardson, Nikolay S. Markov, Mary Carns, Kathleen Aren, Alexandra Soriano, Matthew Dapas, Harris Perlman, Aaron Gundersheimer, Kavitha C. Selvan, John Varga, Monique Hinchcliff, Krishnan Warrior, Catherine A. Gao, Richard G. Wunderink, GR Scott Budinger, Alok N. Choudhary, Anthony J. Esposito, Alexander V. Misharin, Ankit Agrawal, Ulas Bagci  

**Link**: [PDF](https://arxiv.org/pdf/2509.23530)  

**Abstract**: Interstitial lung disease (ILD) is a leading cause of morbidity and mortality in systemic sclerosis (SSc). Chest computed tomography (CT) is the primary imaging modality for diagnosing and monitoring lung complications in SSc patients. However, its role in disease progression and mortality prediction has not yet been fully clarified. This study introduces a novel, large-scale longitudinal chest CT analysis framework that utilizes radiomics and deep learning to predict mortality associated with lung complications of SSc. We collected and analyzed 2,125 CT scans from SSc patients enrolled in the Northwestern Scleroderma Registry, conducting mortality analyses at one, three, and five years using advanced imaging analysis techniques. Death labels were assigned based on recorded deaths over the one-, three-, and five-year intervals, confirmed by expert physicians. In our dataset, 181, 326, and 428 of the 2,125 CT scans were from patients who died within one, three, and five years, respectively. Using ResNet-18, DenseNet-121, and Swin Transformer we use pre-trained models, and fine-tuned on 2,125 images of SSc patients. Models achieved an AUC of 0.769, 0.801, 0.709 for predicting mortality within one-, three-, and five-years, respectively. Our findings highlight the potential of both radiomics and deep learning computational methods to improve early detection and risk assessment of SSc-related interstitial lung disease, marking a significant advancement in the literature. 

**Abstract (ZH)**: 系统硬化病相关间质性肺病的胸部CT纵向分析框架：基于放射omics和深度学习的死亡率预测研究 

---
# Privy: Envisioning and Mitigating Privacy Risks for Consumer-facing AI Product Concepts 

**Title (ZH)**: Privy: 预见并缓解面向消费者的人工智能产品概念的隐私风险 

**Authors**: Hao-Ping Lee, Yu-Ju Yang, Matthew Bilik, Isadora Krsek, Thomas Serban von Davier, Kyzyl Monteiro, Jason Lin, Shivani Agarwal, Jodi Forlizzi, Sauvik Das  

**Link**: [PDF](https://arxiv.org/pdf/2509.23525)  

**Abstract**: AI creates and exacerbates privacy risks, yet practitioners lack effective resources to identify and mitigate these risks. We present Privy, a tool that guides practitioners through structured privacy impact assessments to: (i) identify relevant risks in novel AI product concepts, and (ii) propose appropriate mitigations. Privy was shaped by a formative study with 11 practitioners, which informed two versions -- one LLM-powered, the other template-based. We evaluated these two versions of Privy through a between-subjects, controlled study with 24 separate practitioners, whose assessments were reviewed by 13 independent privacy experts. Results show that Privy helps practitioners produce privacy assessments that experts deemed high quality: practitioners identified relevant risks and proposed appropriate mitigation strategies. These effects were augmented in the LLM-powered version. Practitioners themselves rated Privy as being useful and usable, and their feedback illustrates how it helps overcome long-standing awareness, motivation, and ability barriers in privacy work. 

**Abstract (ZH)**: AI创造并加剧了隐私风险，而从业者缺乏有效资源来识别和减轻这些风险。我们提出了Privy这一工具，引导从业者通过结构化的隐私影响评估来：(i) 在新型AI产品概念中识别相关风险，(ii) 提出适当的缓解措施。Privy是基于对11名从业者的形成性研究而设计的，该研究启发了两种版本的开发——一种基于大语言模型，另一种基于模板。我们通过一项包含24名独立从业者的被试间控制研究，评估了这两种版本的Privy，并邀请了13名独立隐私专家审查了他们的评估结果。研究结果表明，Privy帮助从业者生成了专家认为高质量的隐私评估报告：从业者能够识别相关风险并提出适当的缓解策略。大语言模型版本的效果更为显著。从业者自己也认为Privy既实用又易于使用，他们的反馈揭示了Privy如何帮助克服隐私工作中长期存在的认知、动机和能力障碍。 

---
# ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search 

**Title (ZH)**: ReliabilityRAG: 有效且可以证明 robust 的 RAG 基础网络搜索防御方法 

**Authors**: Zeyu Shen, Basileal Imana, Tong Wu, Chong Xiang, Prateek Mittal, Aleksandra Korolova  

**Link**: [PDF](https://arxiv.org/pdf/2509.23519)  

**Abstract**: Retrieval-Augmented Generation (RAG) enhances Large Language Models by grounding their outputs in external documents. These systems, however, remain vulnerable to attacks on the retrieval corpus, such as prompt injection. RAG-based search systems (e.g., Google's Search AI Overview) present an interesting setting for studying and protecting against such threats, as defense algorithms can benefit from built-in reliability signals -- like document ranking -- and represent a non-LLM challenge for the adversary due to decades of work to thwart SEO.
Motivated by, but not limited to, this scenario, this work introduces ReliabilityRAG, a framework for adversarial robustness that explicitly leverages reliability information of retrieved documents.
Our first contribution adopts a graph-theoretic perspective to identify a "consistent majority" among retrieved documents to filter out malicious ones. We introduce a novel algorithm based on finding a Maximum Independent Set (MIS) on a document graph where edges encode contradiction. Our MIS variant explicitly prioritizes higher-reliability documents and provides provable robustness guarantees against bounded adversarial corruption under natural assumptions. Recognizing the computational cost of exact MIS for large retrieval sets, our second contribution is a scalable weighted sample and aggregate framework. It explicitly utilizes reliability information, preserving some robustness guarantees while efficiently handling many documents.
We present empirical results showing ReliabilityRAG provides superior robustness against adversarial attacks compared to prior methods, maintains high benign accuracy, and excels in long-form generation tasks where prior robustness-focused methods struggled. Our work is a significant step towards more effective, provably robust defenses against retrieved corpus corruption in RAG. 

**Abstract (ZH)**: Retrieval-Augmented Generation (RAG)通过将模型输出 grounding 在外部文档中来增强大型语言模型，但这些系统依然容易受到检索语料库攻击的影响，例如提示注入。基于RAG的搜索引擎（例如Google的Search AI Overview）为研究和抵御此类威胁提供了一个有趣的场景，防御算法可以从内置的可靠性信号（如文档排名）中受益，并且由于对抗者需要应对数十年的工作以阻止SEO，这为对抗者带来了非LLM挑战。受此场景启发，本文 introduce ReliabilityRAG，一种明确利用检索文档可靠性信息的鲁棒性框架。我们的第一项贡献采用图论视角来识别检索文档中的“一致多数”，以过滤掉恶意文档。我们引入了一种基于文档图中寻找最大独立集（MIS）的新算法，其中边编码矛盾信息。我们提出的MIS变体明确优先处理高可靠性文档，并在自然假设下提供可证明的鲁棒性保证。考虑到对大规模检索集进行精确MIS的计算成本，我们的第二项贡献是可扩展的加权抽样和聚合框架。该框架明确利用可靠性信息，保留部分鲁棒性保证的同时高效处理大量文档。我们展示了实验证据表明，ReliabilityRAG在对抗攻击中的鲁棒性优于先前方法，保持了高正当准确率，并在以前的鲁棒性方法难以处理的长文本生成任务中表现出色。本文是朝着更有效的、可证明鲁棒性防御方向的重要一步，以抵御RAG中检索语料库的破坏。 

---
# Evaluating point-light biological motion in multimodal large language models 

**Title (ZH)**: 评估多元模态大型语言模型中的点光生物运动 

**Authors**: Akila Kadambi, Marco Iacoboni, Lisa Aziz-Zadeh, Srini Narayanan  

**Link**: [PDF](https://arxiv.org/pdf/2509.23517)  

**Abstract**: Humans can extract rich semantic information from minimal visual cues, as demonstrated by point-light displays (PLDs), which consist of sparse sets of dots localized to key joints of the human body. This ability emerges early in development and is largely attributed to human embodied experience. Since PLDs isolate body motion as the sole source of meaning, they represent key stimuli for testing the constraints of action understanding in these systems. Here we introduce ActPLD, the first benchmark to evaluate action processing in MLLMs from human PLDs. Tested models include state-of-the-art proprietary and open-source systems on single-actor and socially interacting PLDs. Our results reveal consistently low performance across models, introducing fundamental gaps in action and spatiotemporal understanding. 

**Abstract (ZH)**: 人类可以从少量的视觉线索中提取丰富的语义信息，这一点由点光显示（PLD）实验所证明，PLD仅由人体关键关节位置的稀疏点组成。这种能力早在发育早期就显现出来，并很大程度上归因于人类的经验性体验。由于PLD将身体运动作为唯一的意义来源，它们代表了测试这些系统动作理解限制的关键刺激。在这里，我们引入了ActPLD，这是首个基于人类PLD评估MLLMs动作处理能力的基准。测试的模型包括最先进的专有和开源系统在单人和社交互动PLD上的表现。我们的结果揭示了模型在动作和空间时间理解方面的低表现，暴露出根本性的差距。 

---
# From Human Annotation to Automation: LLM-in-the-Loop Active Learning for Arabic Sentiment Analysis 

**Title (ZH)**: 从人工标注到自动化：包含大语言模型的主动学习方法用于阿拉伯语情感分析 

**Authors**: Dania Refai, Alaa Dalaq, Doaa Dalaq, Irfan Ahmad  

**Link**: [PDF](https://arxiv.org/pdf/2509.23515)  

**Abstract**: Natural language processing (NLP), particularly sentiment analysis, plays a vital role in areas like marketing, customer service, and social media monitoring by providing insights into user opinions and emotions. However, progress in Arabic sentiment analysis remains limited due to the lack of large, high-quality labeled datasets. While active learning has proven effective in reducing annotation efforts in other languages, few studies have explored it in Arabic sentiment tasks. Likewise, the use of large language models (LLMs) for assisting annotation and comparing their performance to human labeling is still largely unexplored in the Arabic context. In this paper, we propose an active learning framework for Arabic sentiment analysis designed to reduce annotation costs while maintaining high performance. We evaluate multiple deep learning architectures: Specifically, long short-term memory (LSTM), gated recurrent units (GRU), and recurrent neural networks (RNN), across three benchmark datasets: Hunger Station, AJGT, and MASAC, encompassing both modern standard Arabic and dialectal variations. Additionally, two annotation strategies are compared: Human labeling and LLM-assisted labeling. Five LLMs are evaluated as annotators: GPT-4o, Claude 3 Sonnet, Gemini 2.5 Pro, DeepSeek Chat, and LLaMA 3 70B Instruct. For each dataset, the best-performing LLM was used: GPT-4o for Hunger Station, Claude 3 Sonnet for AJGT, and DeepSeek Chat for MASAC. Our results show that LLM-assisted active learning achieves competitive or superior performance compared to human labeling. For example, on the Hunger Station dataset, the LSTM model achieved 93% accuracy with only 450 labeled samples using GPT-4o-generated labels, while on the MASAC dataset, DeepSeek Chat reached 82% accuracy with 650 labeled samples, matching the accuracy obtained through human labeling. 

**Abstract (ZH)**: 基于大规模语言模型的阿拉伯语情感分析主动学习框架 

---
# Enhancing Polyp Segmentation via Encoder Attention and Dynamic Kernel Update 

**Title (ZH)**: 通过编码器注意力和动态内核更新增强息肉分割 

**Authors**: Fatemeh Salahi Chashmi, Roya Sotoudeh  

**Link**: [PDF](https://arxiv.org/pdf/2509.23502)  

**Abstract**: Polyp segmentation is a critical step in colorectal cancer detection, yet it remains challenging due to the diverse shapes, sizes, and low contrast boundaries of polyps in medical imaging. In this work, we propose a novel framework that improves segmentation accuracy and efficiency by integrating a Dynamic Kernel (DK) mechanism with a global Encoder Attention module. The DK mechanism, initialized by a global context vector from the EA module, iteratively refines segmentation predictions across decoding stages, enabling the model to focus on and accurately delineate complex polyp boundaries. The EA module enhances the network's ability to capture critical lesion features by aggregating multi scale information from all encoder layers. In addition, we employ Unified Channel Adaptation (UCA) in the decoder to standardize feature dimensions across stages, ensuring consistent and computationally efficient information fusion. Our approach extends the lesion-aware kernel framework by introducing a more flexible, attention driven kernel initialization and a unified decoder design. Extensive experiments on the KvasirSEG and CVC ClinicDB benchmark datasets demonstrate that our model outperforms several state of the art segmentation methods, achieving superior Dice and Intersection over Union scores. Moreover, UCA simplifies the decoder structure, reducing computational cost without compromising accuracy. Overall, the proposed method provides a robust and adaptable solution for polyp segmentation, with promising applications in clinical and automated diagnostic systems. 

**Abstract (ZH)**: 一种结合动态内核机制和全局编码器注意力模块的肠道息肉分割框架 

---
# The Impact of Role Design in In-Context Learning for Large Language Models 

**Title (ZH)**: 大型语言模型中基于情境学习的角色设计影响研究 

**Authors**: Hamidreza Rouzegar, Masoud Makrehchi  

**Link**: [PDF](https://arxiv.org/pdf/2509.23501)  

**Abstract**: In-context learning (ICL) enables Large Language Models (LLMs) to generate predictions based on prompts without additional fine-tuning. While prompt engineering has been widely studied, the impact of role design within prompts remains underexplored. This study examines the influence of role configurations in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models' performance across datasets, focusing on tasks like sentiment analysis, text classification, question answering, and math reasoning. Our findings suggest the potential of role-based prompt structuring to enhance LLM performance. 

**Abstract (ZH)**: 上下文学习（ICL）使大型语言模型（LLMs）能够在无需额外微调的情况下基于提示生成预测。虽然提示工程已经广泛研究，但提示中角色设计的影响仍然未被充分探索。本研究使用来自OpenAI的GPT-3.5和GPT-4o及来自Meta的Llama2-7b和Llama2-13b，探讨了零-shot和few-shot学习场景中角色配置对模型性能的影响，并专注于情感分析、文本分类、问题回答和数学推理等任务。我们的研究结果表明，基于角色的提示结构化有可能提升LLM的性能。 

---
# Revisiting Multivariate Time Series Forecasting with Missing Values 

**Title (ZH)**: revisit 多变量时间序列预测中的缺失值问题 

**Authors**: Jie Yang, Yifan Hu, Kexin Zhang, Luyang Niu, Yushun Dong, Philip S. Yu, Kaize Ding  

**Link**: [PDF](https://arxiv.org/pdf/2509.23494)  

**Abstract**: Missing values are common in real-world time series, and multivariate time series forecasting with missing values (MTSF-M) has become a crucial area of research for ensuring reliable predictions. To address the challenge of missing data, current approaches have developed an imputation-then-prediction framework that uses imputation modules to fill in missing values, followed by forecasting on the imputed data. However, this framework overlooks a critical issue: there is no ground truth for the missing values, making the imputation process susceptible to errors that can degrade prediction accuracy. In this paper, we conduct a systematic empirical study and reveal that imputation without direct supervision can corrupt the underlying data distribution and actively degrade prediction accuracy. To address this, we propose a paradigm shift that moves away from imputation and directly predicts from the partially observed time series. We introduce Consistency-Regularized Information Bottleneck (CRIB), a novel framework built on the Information Bottleneck principle. CRIB combines a unified-variate attention mechanism with a consistency regularization scheme to learn robust representations that filter out noise introduced by missing values while preserving essential predictive signals. Comprehensive experiments on four real-world datasets demonstrate the effectiveness of CRIB, which predicts accurately even under high missing rates. Our code is available in this https URL. 

**Abstract (ZH)**: 缺失值在实时序列中很常见，多变量时间序列预测中的缺失值（MTSF-M）已成为确保可靠预测的关键研究领域。本文系统地研究了缺失数据的问题，揭示了在缺乏直接监督的情况下进行插补会破坏底层数据分布并主动降低预测精度。为此，我们提出了一种范式转变，即从插补转向直接从部分观察到的时间序列进行预测。我们引入了一致性正则化信息瓶颈（CRIB）框架，这是一种基于信息瓶颈原理的新型框架。CRIB 结合了一致性正则化方案和统一变量注意力机制，用于学习稳健的表示，这些表示可以过滤掉由缺失值引入的噪声，同时保留重要的预测信号。在四个真实世界数据集上的全面实验表明，CRIB 即使在高缺失率下也能准确预测。我们的代码可在以下链接获取：this https URL。 

---
# Text-Based Approaches to Item Difficulty Modeling in Large-Scale Assessments: A Systematic Review 

**Title (ZH)**: 基于文本的方法在大规模评估中项目难度建模：一项系统回顾 

**Authors**: Sydney Peters, Nan Zhang, Hong Jiao, Ming Li, Tianyi Zhou, Robert Lissitz  

**Link**: [PDF](https://arxiv.org/pdf/2509.23486)  

**Abstract**: Item difficulty plays a crucial role in test performance, interpretability of scores, and equity for all test-takers, especially in large-scale assessments. Traditional approaches to item difficulty modeling rely on field testing and classical test theory (CTT)-based item analysis or item response theory (IRT) calibration, which can be time-consuming and costly. To overcome these challenges, text-based approaches leveraging machine learning and language models, have emerged as promising alternatives. This paper reviews and synthesizes 37 articles on automated item difficulty prediction in large-scale assessment settings published through May 2025. For each study, we delineate the dataset, difficulty parameter, subject domain, item type, number of items, training and test data split, input, features, model, evaluation criteria, and model performance outcomes. Results showed that although classic machine learning models remain relevant due to their interpretability, state-of-the-art language models, using both small and large transformer-based architectures, can capture syntactic and semantic patterns without the need for manual feature engineering. Uniquely, model performance outcomes were summarized to serve as a benchmark for future research and overall, text-based methods have the potential to predict item difficulty with root mean square error (RMSE) as low as 0.165, Pearson correlation as high as 0.87, and accuracy as high as 0.806. The review concludes by discussing implications for practice and outlining future research directions for automated item difficulty modeling. 

**Abstract (ZH)**: 项目难度在测试表现、分数解释性和所有应试者的公平性中起着关键作用，尤其是在大规模评估中。传统的项目难度建模方法依赖于场测和基于经典测验理论（CTT）的项目分析或基于项目反应理论（IRT）的校准，这可能会耗费大量时间和成本。为克服这些挑战，利用机器学习和语言模型的基于文本的方法已成为有前景的替代方案。本文通过2025年5月回顾并综合了37篇关于大规模评估环境中自动化项目难度预测的文章。对于每项研究，我们详细介绍了数据集、难度参数、研究领域、项目类型、项目数量、训练和测试数据分割、输入、特征、模型、评估标准以及模型性能结果。结果显示，尽管经典的机器学习模型仍具有一定的解释性，但最先进的语言模型，无论是小型还是大型变压器架构，都能够捕获句法和语义模式，无需手动特征工程。模型性能结果被总结为未来研究的基准，总体而言，基于文本的方法有可能预测项目难度，其中均方根误差(RMSE)低至0.165，皮尔逊相关系数高达0.87，准确率高达0.806。本文结论讨论了实践启示并概述了自动化项目难度建模的未来研究方向。 

---
# Memory-Efficient Fine-Tuning via Low-Rank Activation Compression 

**Title (ZH)**: 低秩激活压缩实现高效内存微调 

**Authors**: Jiang-Xin Shi, Wen-Da Wei, Jin-Fei Qi, Xuanyu Chen, Tong Wei, Yu-Feng Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.23472)  

**Abstract**: The parameter-efficient fine-tuning paradigm has garnered significant attention with the advancement of foundation models. Although numerous methods have been proposed to reduce the number of trainable parameters, their substantial memory overhead remains a critical bottleneck that hinders practical deployment. In this paper, we observe that model activations constitute a major source of memory consumption, especially under large batch sizes and long context lengths; however, the rank of the activations remains consistently low. Motivated by this insight, we propose a memory-efficient fine-tuning approach Low-Rank Activation Compression (LoRAct). Unlike prior work, LoRAct provides a more flexible and versatile compressing strategy that can be applied online during the forward pass without the need for any calibration data. Moreover, LoRAct incorporates a novel sampling-based orthogonal decomposition algorithm specifically designed for low-rank matrices, offering improved computational efficiency and a tighter error bound compared to the widely used RSVD. Experiments on both vision and language tasks demonstrate the effectiveness of LoRAct. Notably, LoRAct further reduces activation memory by approximately 80% in comparison with the widely adopted LoRA method, while maintaining competitive performance. The source code is available at this https URL. 

**Abstract (ZH)**: 基础模型发展的参数高效微调范式引起了广泛关注。尽管提出了许多减少可训练参数数量的方法，但它们带来的显存 overhead 仍然是阻碍其实用部署的关键瓶颈。在本文中，我们观察到模型激活构成了主要的显存消耗来源，尤其是在大批次和长上下文长度的情况下；然而，这些激活的秩保持在较低水平。基于这一见解，我们提出了一种高效的微调方法——低秩激活压缩（LoRAct）。与以往工作不同，LoRAct 提供了一种更为灵活和通用的压缩策略，可以在前向通过过程中在线应用而无需任何校准数据。此外，LoRAct 结合了一种新颖的基于抽样的正交分解算法，专门设计用于低秩矩阵，提供比广泛使用的RSVD更好的计算效率和更紧的误差界。实验结果表明，LoRAct 在视觉和语言任务中均有效。值得注意的是，与广泛采用的LoRA方法相比，LoRAct 进一步减少了约80%的激活显存消耗，同时保持了竞争力的性能。源代码可通过此链接获取。 

---
# Multi-Modal Manipulation via Multi-Modal Policy Consensus 

**Title (ZH)**: 多模态操作 via 多模态策略共识 

**Authors**: Haonan Chen, Jiaming Xu, Hongyu Chen, Kaiwen Hong, Binghao Huang, Chaoqi Liu, Jiayuan Mao, Yunzhu Li, Yilun Du, Katherine Driggs-Campbell  

**Link**: [PDF](https://arxiv.org/pdf/2509.23468)  

**Abstract**: Effectively integrating diverse sensory modalities is crucial for robotic manipulation. However, the typical approach of feature concatenation is often suboptimal: dominant modalities such as vision can overwhelm sparse but critical signals like touch in contact-rich tasks, and monolithic architectures cannot flexibly incorporate new or missing modalities without retraining. Our method factorizes the policy into a set of diffusion models, each specialized for a single representation (e.g., vision or touch), and employs a router network that learns consensus weights to adaptively combine their contributions, enabling incremental of new representations. We evaluate our approach on simulated manipulation tasks in {RLBench}, as well as real-world tasks such as occluded object picking, in-hand spoon reorientation, and puzzle insertion, where it significantly outperforms feature-concatenation baselines on scenarios requiring multimodal reasoning. Our policy further demonstrates robustness to physical perturbations and sensor corruption. We further conduct perturbation-based importance analysis, which reveals adaptive shifts between modalities. 

**Abstract (ZH)**: 有效融合多种感官模态对于机器人操作至关重要。然而，典型的特征拼接方法往往不尽optimal：如视觉等主导模态在接触丰富任务中可能会压制稀疏但关键的触觉信号，而整体型架构无法灵活地在无需重新训练的情况下整合新出现或缺失的模态。我们的方法将策略分解为一组专门用于单一表示（如视觉或触觉）的扩散模型，并使用一个路由器网络学习共识权重以自适应地组合它们的贡献，从而实现新表示的增量整合。我们在{RLBench}模拟操作任务以及实际任务（如遮挡物体拾取、手持勺子重新定向和拼图插入）中评估了这种方法，结果表明其在需要多模态推理的情况下显著优于特征拼接基准。此外，我们的策略还展示了对物理扰动和传感器故障的鲁棒性。我们进一步开展了基于扰动的重要性分析，揭示了模态间适应性变化。 

---
# Generative Evolutionary Meta-Solver (GEMS): Scalable Surrogate-Free Multi-Agent Learning 

**Title (ZH)**: 生成进化元求解器（GEMS）：可扩展的无代理模拟多agent学习 

**Authors**: Alakh Sharma, Gaurish Trivedi, Kartikey Bhandari, Yash Sinha, Dhruv Kumar, Pratik Narang, Jagat Sesh Challa  

**Link**: [PDF](https://arxiv.org/pdf/2509.23462)  

**Abstract**: Scalable multi-agent reinforcement learning (MARL) remains a central challenge for AI. Existing population-based methods, like Policy-Space Response Oracles, PSRO, require storing explicit policy populations and constructing full payoff matrices, incurring quadratic computation and linear memory costs. We present Generative Evolutionary Meta-Solver (GEMS), a surrogate-free framework that replaces explicit populations with a compact set of latent anchors and a single amortized generator. Instead of exhaustively constructing the payoff matrix, GEMS relies on unbiased Monte Carlo rollouts, multiplicative-weights meta-dynamics, and a model-free empirical-Bernstein UCB oracle to adaptively expand the policy set. Best responses are trained within the generator using an advantage-based trust-region objective, eliminating the need to store and train separate actors. We evaluated GEMS in a variety of Two-player and Multi-Player games such as the Deceptive Messages Game, Kuhn Poker and Multi-Particle environment. We find that GEMS is up to ~6x faster, has 1.3x less memory usage than PSRO, while also reaps higher rewards simultaneously. These results demonstrate that GEMS retains the game theoretic guarantees of PSRO, while overcoming its fundamental inefficiencies, hence enabling scalable multi-agent learning in multiple domains. 

**Abstract (ZH)**: 无补贴进化元求解器（GEMS）：Scalable Multi-Agent Reinforcement Learning Framework 

---
# Data-Efficient Training by Evolved Sampling 

**Title (ZH)**: 进化采样实现数据高效训练 

**Authors**: Ziheng Cheng, Zhong Li, Jiang Bian  

**Link**: [PDF](https://arxiv.org/pdf/2509.23461)  

**Abstract**: Data selection is designed to accelerate learning with preserved performance. To achieve this, a fundamental thought is to identify informative data samples with significant contributions to the training. In this work, we propose \textbf{Evolved Sampling} (\textbf{ES}), a simple yet effective framework for \emph{dynamic} sampling along the training process. This method conducts \em batch \em level data selection based on the dynamics of losses and augmented \emph{loss differences}, which enables flexible \emph{frequency tuning}, and hence significantly reduces the back propagation time with maintained model performance. Due to its conciseness, ES is also readily extensible to incorporate \em set \em level data selection (to form ES with pruning, \textbf{ESWP}) for further accelerations. As a plug-and-play framework, ES(WP) consistently achieves lossless training accelerations across various pre-training and post-training tasks, saving up to nearly 45\% wall-clock time. Our results motivate further investigations on the data efficiency aspect of modern large-scale machine learning. 

**Abstract (ZH)**: 数据选择旨在保持性能的同时加速学习。为了实现这一目标，一个根本性的想法是识别对训练有显著贡献的信息性数据样本。在本文中，我们提出了一种简单而有效的方法 \textbf{进化采样} (\textbf{ES})，这是一种在训练过程中进行动态采样的框架。该方法基于损失动态和增强的损失差进行批次级别数据选择，从而实现灵活的频率调优，并显著减少了回传时间，同时保持模型性能。由于其简洁性，ES 也可以方便地扩展为结合集合级别数据选择（形成具有剪枝的 \textbf{ESWP}）以进一步加速。作为一种即插即用框架，ES(WP) 在各种预训练和后训练任务中实现了无损训练加速，最高可节省近 45% 的实际时间。我们的结果促使我们进一步研究现代大规模机器学习中的数据效率方面。 

---
# AudioFuse: Unified Spectral-Temporal Learning via a Hybrid ViT-1D CNN Architecture for Robust Phonocardiogram Classification 

**Title (ZH)**: AudioFuse： através 混合 ViT-1D CNN 架构的统一频谱-时间学习，用于稳健的 Phonocardiogram 分类 

**Authors**: Md. Saiful Bari Siddiqui, Utsab Saha  

**Link**: [PDF](https://arxiv.org/pdf/2509.23454)  

**Abstract**: Biomedical audio signals, such as phonocardiograms (PCG), are inherently rhythmic and contain diagnostic information in both their spectral (tonal) and temporal domains. Standard 2D spectrograms provide rich spectral features but compromise the phase information and temporal precision of the 1D waveform. We propose AudioFuse, an architecture that simultaneously learns from both complementary representations to classify PCGs. To mitigate the overfitting risk common in fusion models, we integrate a custom, wide-and-shallow Vision Transformer (ViT) for spectrograms with a shallow 1D CNN for raw waveforms. On the PhysioNet 2016 dataset, AudioFuse achieves a state-of-the-art competitive ROC-AUC of 0.8608 when trained from scratch, outperforming its spectrogram (0.8066) and waveform (0.8223) baselines. Moreover, it demonstrates superior robustness to domain shift on the challenging PASCAL dataset, maintaining an ROC-AUC of 0.7181 while the spectrogram baseline collapses (0.4873). Fusing complementary representations thus provides a strong inductive bias, enabling the creation of efficient, generalizable classifiers without requiring large-scale pre-training. 

**Abstract (ZH)**: biomedical 音频信号，如心音图 (PCG)，本质上是 rhythmic 的，并且在其频谱 (音调) 和时间域中包含诊断信息。标准的 2D 谱图提供了丰富的频谱特征，但牺牲了时间波形的相位信息和时间精度。我们提出 AudioFuse 架构，该架构同时从互补的表示中学习以分类 PCG。为缓解融合模型中常见的过拟合风险，我们整合了一个定制的宽浅 Vision Transformer (ViT) 用于谱图，以及一个浅层 1D CNN 用于原始波形。在 PhysioNet 2016 数据集上，从头训练的 AudioFuse 达到了 0.8608 的竞争性 ROC-AUC，优于其谱图 baselines (0.8066) 和波形 baselines (0.8223)。此外，它在具有挑战性的 PASCAL 数据集上展示了对领域转移的优越鲁棒性，在保持 ROC-AUC 为 0.7181 的同时，谱图 baseline 下降至 0.4873。因此，融合互补表示提供了强大的归纳偏置，使得可以创建高效且可泛化的分类器，而无需大规模预训练。 

---
# Factor Decorrelation Enhanced Data Removal from Deep Predictive Models 

**Title (ZH)**: 深度预测模型中因素去相关增强的数据删除 

**Authors**: Wenhao Yang, Lin Li, Xiaohui Tao, Kaize Shi  

**Link**: [PDF](https://arxiv.org/pdf/2509.23443)  

**Abstract**: The imperative of user privacy protection and regulatory compliance necessitates sensitive data removal in model training, yet this process often induces distributional shifts that undermine model performance-particularly in out-of-distribution (OOD) scenarios. We propose a novel data removal approach that enhances deep predictive models through factor decorrelation and loss perturbation. Our approach introduces: (1) a discriminative-preserving factor decorrelation module employing dynamic adaptive weight adjustment and iterative representation updating to reduce feature redundancy and minimize inter-feature correlations. (2) a smoothed data removal mechanism with loss perturbation that creates information-theoretic safeguards against data leakage during removal operations. Extensive experiments on five benchmark datasets show that our approach outperforms other baselines and consistently achieves high predictive accuracy and robustness even under significant distribution shifts. The results highlight its superior efficiency and adaptability in both in-distribution and out-of-distribution scenarios. 

**Abstract (ZH)**: 用户隐私保护和监管合规的迫切性要求在模型训练中移除敏感数据，但这一过程常会引起分布变化，特别是在分布外(OOD)场景中损害模型性能。我们提出了一种新颖的数据移除方法，通过因子去相关和损失扰动来增强深度预测模型。(1) 一种保持鉴别信息的因子去相关模块，采用动态自适应权重调整和迭代表示更新来减少特征冗余并最小化特征间的相关性。(2) 一种平滑的数据移除机制，通过损失扰动创建信息论上的安全防护，防止在数据移除操作中发生数据泄漏。在五个基准数据集上的广泛实验表明，本方法优于其他基线方法，并且能够在显著分布变化下一致地实现高预测准确性和鲁棒性。结果突显了其在分布内和分布外场景中的优越效率和适应性。 

---
# S$^3$F-Net: A Multi-Modal Approach to Medical Image Classification via Spatial-Spectral Summarizer Fusion Network 

**Title (ZH)**: S$^3$F-Net：一种基于空间-光谱摘要融合网络的多模态医学图像分类方法 

**Authors**: Md. Saiful Bari Siddiqui, Mohammed Imamul Hassan Bhuiyan  

**Link**: [PDF](https://arxiv.org/pdf/2509.23442)  

**Abstract**: Convolutional Neural Networks have become a cornerstone of medical image analysis due to their proficiency in learning hierarchical spatial features. However, this focus on a single domain is inefficient at capturing global, holistic patterns and fails to explicitly model an image's frequency-domain characteristics. To address these challenges, we propose the Spatial-Spectral Summarizer Fusion Network (S$^3$F-Net), a dual-branch framework that learns from both spatial and spectral representations simultaneously. The S$^3$F-Net performs a fusion of a deep spatial CNN with our proposed shallow spectral encoder, SpectraNet. SpectraNet features the proposed SpectralFilter layer, which leverages the Convolution Theorem by applying a bank of learnable filters directly to an image's full Fourier spectrum via a computation-efficient element-wise multiplication. This allows the SpectralFilter layer to attain a global receptive field instantaneously, with its output being distilled by a lightweight summarizer network. We evaluate S$^3$F-Net across four medical imaging datasets spanning different modalities to validate its efficacy and generalizability. Our framework consistently and significantly outperforms its strong spatial-only baseline in all cases, with accuracy improvements of up to 5.13%. With a powerful Bilinear Fusion, S$^3$F-Net achieves a SOTA competitive accuracy of 98.76% on the BRISC2025 dataset. Concatenation Fusion performs better on the texture-dominant Chest X-Ray Pneumonia dataset, achieving 93.11% accuracy, surpassing many top-performing, much deeper models. Our explainability analysis also reveals that the S$^3$F-Net learns to dynamically adjust its reliance on each branch based on the input pathology. These results verify that our dual-domain approach is a powerful and generalizable paradigm for medical image analysis. 

**Abstract (ZH)**: 卷积神经网络已成为医学图像分析的基石，得益于其在学习层次空间特征方面的 proficiency。然而，这种单一领域的专注于低效地捕捉全局整体模式，并且未能明确建模图像的频域特征。为了解决这些挑战，我们提出了一种双分支架构——空间-频谱总结融合网络（S$^3$F-Net），该架构能够同时从空间和频谱表示中学习。S$^3$F-Net将一个深度空间CNN与我们提出的浅层频谱编码器SpectraNet融合。SpectraNet包含我们提议的SpectralFilter层，该层通过高效元素级乘法直接应用一组可学习的滤波器到图像的全傅里叶谱上，利用卷积定理。SpectralFilter层可以即时获得全局感受野，并通过一个轻量级的总结网络对其输出进行提炼。我们在四个涵盖不同模态的医学成像数据集上评估S$^3$F-Net，以验证其有效性和泛化能力。我们的框架在所有情况下都显著优于其强大的仅空间基线，在某些情况下准确率提高了5.13%。借助强大的双线性融合，S$^3$F-Net在BRISC2025数据集上实现了98.76%的竞争力准确率。在纹理占主导地位的胸部X光肺炎数据集上，拼接融合的方法达到了93.11%的准确率，超过了许多更深层次的顶级模型。我们的可解释性分析还表明，S$^3$F-Net能够根据输入病理动态调整其对各个分支的依赖。这些结果证实了我们双域方法在医学图像分析中的强大和泛化能力。 

---
# AudioRole: An Audio Dataset for Character Role-Playing in Large Language Models 

**Title (ZH)**: AudioRole: 用于大型语言模型角色扮演的音频数据集 

**Authors**: Wenyu Li, Xiaoqi Jiao, Yi Chang, Guangyan Zhang, Yiwen Guo  

**Link**: [PDF](https://arxiv.org/pdf/2509.23435)  

**Abstract**: The creation of high-quality multimodal datasets remains fundamental for advancing role-playing capabilities in large language models (LLMs). While existing works predominantly focus on text-based persona simulation, Audio Role-Playing (ARP) presents unique challenges due to the need for synchronized alignment of semantic content and vocal characteristics. To address this gap, we propose AudioRole, a meticulously curated dataset from 13 TV series spanning 1K+ hours with 1M+ character-grounded dialogues, providing synchronized audio-text pairs annotated with speaker identities and contextual metadata. In addition, to demonstrate the effectiveness of the dataset, we introduced ARP-Eval, a dual-aspect evaluation framework that assesses both response quality and role fidelity. Empirical validation showing GLM-4-Voice trained on AudioRole (which we called ARP-Model) achieve an average Acoustic Personalization score of 0.31, significantly outperforming the original GLM-4-voice and the more powerful model MiniCPM-O-2.6, which specifically supports role-playing in one-shot scenarios. The ARP-Model also achieves a Content Personalization score of 0.36, surpassing the untrained original model by about 38% and maintaining the same level as MiniCPM-O-2.6.
AudioRole features dialogues from over 115 main characters, 6 trained ARP-Models that role-play different characters, and evaluation protocols. Together, they provide an essential resource for advancing audio-grounded role-playing research. 

**Abstract (ZH)**: 高质量多模态数据集的创建仍是推动大规模语言模型（LLMs）角色扮演能力发展的基础。尽管现有工作主要集中在基于文本的人格仿真上，音频角色扮演（ARP）因其需同步对齐语义内容和语音特征而面临独特挑战。为填补这一空白，我们提出AudioRole数据集，该数据集来自13部电视剧，覆盖超过1000小时的对话，包含超过100万条角色相关的对话，提供了标注有说话者身份和上下文元数据的同步音频-文本对。此外，为了展示数据集的有效性，我们引入了ARP-Eval评估框架，该框架从响应质量和角色保真度两个方面进行评估。实验证明，基于AudioRole训练的GLM-4-Voice（我们称其为ARP-Model）的平均声学个性化得分为0.31，显著优于原始的GLM-4-Voice和更强大的MiniCPM-O-2.6模型，后者专门支持单轮场景中的角色扮演。ARP-Model的语义个性化得分为0.36，比未训练的原始模型高出约38%，与MiniCPM-O-2.6保持相同水平。AudioRole数据集包含超过115个主要角色的对话、6个训练好的ARP-模型以及评估协议，共同为推动基于音频的角色扮演研究提供了重要资源。 

---
# NeuroBridge: Using Generative AI to Bridge Cross-neurotype Communication Differences through Neurotypical Perspective-taking 

**Title (ZH)**: NeuroBridge：通过神经典型视角桥接不同神经类型间的沟通差异 Using生成性AI 

**Authors**: Rukhshan Haroon, Kyle Wigdor, Katie Yang, Nicole Toumanios, Eileen T. Crehan, Fahad Dogar  

**Link**: [PDF](https://arxiv.org/pdf/2509.23434)  

**Abstract**: Communication challenges between autistic and neurotypical individuals stem from a mutual lack of understanding of each other's distinct, and often contrasting, communication styles. Yet, autistic individuals are expected to adapt to neurotypical norms, making interactions inauthentic and mentally exhausting for them. To help redress this imbalance, we build NeuroBridge, an online platform that utilizes large language models (LLMs) to simulate: (a) an AI character that is direct and literal, a style common among many autistic individuals, and (b) four cross-neurotype communication scenarios in a feedback-driven conversation between this character and a neurotypical user. Through NeuroBridge, neurotypical individuals gain a firsthand look at autistic communication, and reflect on their role in shaping cross-neurotype interactions. In a user study with 12 neurotypical participants, we find that NeuroBridge improved their understanding of how autistic people may interpret language differently, with all describing autism as a social difference that "needs understanding by others" after completing the simulation. Participants valued its personalized, interactive format and described AI-generated feedback as "constructive", "logical" and "non-judgmental". Most perceived the portrayal of autism in the simulation as accurate, suggesting that users may readily accept AI-generated (mis)representations of disabilities. To conclude, we discuss design implications for disability representation in AI, the need for making NeuroBridge more personalized, and LLMs' limitations in modeling complex social scenarios. 

**Abstract (ZH)**: 沟通障碍源于自闭症个体和非自闭症个体之间相互缺乏对方独特且often contrasting沟通风格的理解。然而，自闭症个体被期望适应非自闭症 norms，这使得他们在互动中显得不真实且心理疲惫。为缓解这种不平衡，我们构建了NeuroBridge，一个利用大规模语言模型（LLMs）模拟的在线平台：（a）一个直接而直截了当的AI角色，这是许多自闭症个体常见的一种风格；（b）四个跨神经类型沟通情景，在这种情景中，该角色与非自闭症用户进行反馈驱动的对话。通过NeuroBridge，非自闭症个体可以获得第一手了解自闭症沟通的机会，并反思他们在塑造跨神经类型互动中的角色。在一项包含12名非自闭症参与者的用户研究中，我们发现NeuroBridge提高了他们对自闭症人士可能如何以不同方式解读语言的理解，所有参与者在完成模拟后都表示自闭症是一种“需要他人理解的社会差异”。参与者们认为其个性化和互动的格式非常有价值，并称AI生成的反馈为“建设性的”、“逻辑性的”和“非评判性的”。大多数参与者认为模拟中自闭症的呈现是准确的，这表明用户可能容易接受AI生成的（误）代表残疾的呈现。最后，我们讨论了在AI中代表残疾的设计含义、使NeuroBridge更加个性化的必要性以及大规模语言模型在建模复杂社会情景方面的局限性。 

---
# Enhancing Communication Efficiency in FL with Adaptive Gradient Quantization and Communication Frequency Optimization 

**Title (ZH)**: 适应性梯度量化与通信频率优化以提升联邦学习中的通信效率 

**Authors**: Asadullah Tariq, Tariq Qayyum, Mohamed Adel Serhani, Farag Sallabi, Ikbal Taleb, Ezedin S. Barka  

**Link**: [PDF](https://arxiv.org/pdf/2509.23419)  

**Abstract**: Federated Learning (FL) enables participant devices to collaboratively train deep learning models without sharing their data with the server or other devices, effectively addressing data privacy and computational concerns. However, FL faces a major bottleneck due to high communication overhead from frequent model updates between devices and the server, limiting deployment in resource-constrained wireless networks. In this paper, we propose a three-fold strategy. Firstly, an Adaptive Feature-Elimination Strategy to drop less important features while retaining high-value ones; secondly, Adaptive Gradient Innovation and Error Sensitivity-Based Quantization, which dynamically adjusts the quantization level for innovative gradient compression; and thirdly, Communication Frequency Optimization to enhance communication efficiency. We evaluated our proposed model's performance through extensive experiments, assessing accuracy, loss, and convergence compared to baseline techniques. The results show that our model achieves high communication efficiency in the framework while maintaining accuracy. 

**Abstract (ZH)**: 联邦学习（FL）使参与设备能够在不共享数据给服务器或其他设备的情况下协作训练深度学习模型，有效地解决了数据隐私和计算问题。然而，FL由于频繁的模型更新导致的高通信开销面临重大瓶颈，限制了其在资源受限的无线网络中的部署。在本文中，我们提出了一种三管齐下的策略。首先，提出了一种自适应特征消除策略以丢弃不重要的特征同时保留高价值特征；其次，提出了自适应梯度创新和误差敏感量化方法，动态调整创新梯度压缩的量化级别；第三，优化通信频率以提高通信效率。通过广泛实验评估了我们提出模型的性能，与基准技术相比，评估了准确率、损失和收敛性。结果表明，我们的模型在保持高准确率的同时实现了高效的通信。 

---
# Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models 

**Title (ZH)**: 检索约束解码揭示语言模型中被低估的参数知识 

**Authors**: Rajaa El Hamdani, Samy Haffoudhi, Nils Holzenberger, Fabian Suchanek, Thomas Bonald, Fragkiskos D. Malliaros  

**Link**: [PDF](https://arxiv.org/pdf/2509.23417)  

**Abstract**: Language models (LMs) encode substantial factual knowledge, but often produce answers judged as incorrect. We hypothesize that many of these answers are actually correct, but are expressed in alternative surface forms that are dismissed due to an overly strict evaluation, leading to an underestimation of models' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD), a decoding strategy that restricts model outputs to unique surface forms. We introduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating open-source LMs from 135M to 70B parameters, we show that standard decoding undervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1 with vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0% with RCD, outperforming the larger model under vanilla decoding. We publicly share the code and dataset at this https URL. 

**Abstract (ZH)**: 语言模型编码了大量的事实知识，但经常产生被判断为错误的答案。我们假设许多答案实际上是正确的，但由于过于严格的评估标准忽视了它们的替代表面形式，导致了对模型参数化知识的低估。我们提出了检索约束解码(RCD)，这是一种限制模型输出为独特表面形式的解码策略。我们引入了包含19,137个一般知识问题的数据集YAGO-QA。评估从135M到70B参数的开源语言模型，我们显示标准解码低估了它们的知识。例如，Llama-3.1-70B在标准解码下的F1得分为32.3%，但在RCD下的得分为46.0%。同样地，Llama-3.1-8B在RCD下的得分为33.0%，在标准解码下表现不如较小的模型。我们已公开分享代码和数据集，可在该链接访问：this https URL。 

---
# Hybrid Graph Embeddings and Louvain Algorithm for Unsupervised Community Detection 

**Title (ZH)**: 混合图嵌入和Louvain算法在无监督社区检测中的应用 

**Authors**: Dalila Khettaf, Djamel Djenouri, Zeinab Rezaeifar, Youcef Djenouri  

**Link**: [PDF](https://arxiv.org/pdf/2509.23411)  

**Abstract**: This paper proposes a novel community detection method that integrates the Louvain algorithm with Graph Neural Networks (GNNs), enabling the discovery of communities without prior knowledge. Compared to most existing solutions, the proposed method does not require prior knowledge of the number of communities. It enhances the Louvain algorithm using node embeddings generated by a GNN to capture richer structural and feature information. Furthermore, it introduces a merging algorithm to refine the results of the enhanced Louvain algorithm, reducing the number of detected communities. To the best of our knowledge, this work is the first one that improves the Louvain algorithm using GNNs for community detection. The improvement of the proposed method was empirically confirmed through an evaluation on real-world datasets. The results demonstrate its ability to dynamically adjust the number of detected communities and increase the detection accuracy in comparison with the benchmark solutions. 

**Abstract (ZH)**: 本文提出了一种将Louvain算法与图神经网络（GNNs）集成的新颖社区检测方法，能够在无先验知识的情况下发现社区。与大多数现有解决方案不同，该方法不需要知道社区的数量。该方法通过使用GNN生成的节点嵌入来增强Louvain算法，以捕获更丰富的结构和特征信息。此外，它引入了一种聚类算法来细化增强后的Louvain算法的结果，减少了检测到的社区数量。据我们所知，这是首次使用GNNs增强Louvain算法进行社区检测的工作。通过在实际数据集上的评估，实证验证了所提出方法的改进效果。该方法能够在动态调整检测到的社区数量和提高检测准确性方面优于基准解决方案。 

---
# PATCH: Learnable Tile-level Hybrid Sparsity for LLMs 

**Title (ZH)**: PATCH: 学习驱动的tile级混合稀疏性技术应用于LLMs 

**Authors**: Younes Hourri, Mohammad Mozaffari, Maryam Mehri Dehnavi  

**Link**: [PDF](https://arxiv.org/pdf/2509.23410)  

**Abstract**: Large language models (LLMs) deliver impressive performance but incur prohibitive memory and compute costs at deployment. Model pruning is an effective way to reduce these overheads, yet existing approaches face challenges: unstructured sparsity, where nonzeros can appear anywhere, preserves accuracy but yields irregular access patterns that prevent GPU acceleration, while semi-structured 2:4 sparsity is hardware-friendly but enforces a rigid 50% pattern that degrades model quality. To bridge this gap, we introduce PATCH, a hybrid sparsity framework that enables a continuous sparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles, assigning each tile to be either dense or 2:4 sparse via a learnable mask selection mechanism. This design provides fine-grained control over accuracy-acceleration tradeoffs and supports non-uniform sparsity across layers, leading to superior overall quality. Across models from 0.5B to 8B parameters, PATCH consistently narrows the gap to dense accuracy while delivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU, PATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while improving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning method, MaskLLM. 

**Abstract (ZH)**: PATCH：一种连续可调的稀疏性框架以提高大语言模型的性能和效率 

---
# Enhanced Fracture Diagnosis Based on Critical Regional and Scale Aware in YOLO 

**Title (ZH)**: 基于关键区域和尺度aware的YOLO骨折诊断增强方法 

**Authors**: Yuyang Sun, Junchuan Yu, Cuiming Zou  

**Link**: [PDF](https://arxiv.org/pdf/2509.23408)  

**Abstract**: Fracture detection plays a critical role in medical imaging analysis, traditional fracture diagnosis relies on visual assessment by experienced physicians, however the speed and accuracy of this approach are constrained by the expertise. With the rapid advancements in artificial intelligence, deep learning models based on the YOLO framework have been widely employed for fracture detection, demonstrating significant potential in improving diagnostic efficiency and accuracy. This study proposes an improved YOLO-based model, termed Fracture-YOLO, which integrates novel Critical-Region-Selector Attention (CRSelector) and Scale-Aware (ScA) heads to further enhance detection performance. Specifically, the CRSelector module utilizes global texture information to focus on critical features of fracture regions. Meanwhile, the ScA module dynamically adjusts the weights of features at different scales, enhancing the model's capacity to identify fracture targets at multiple scales. Experimental results demonstrate that, compared to the baseline model, Fracture-YOLO achieves a significant improvement in detection precision, with mAP50 and mAP50-95 increasing by 4 and 3, surpassing the baseline model and achieving state-of-the-art (SOTA) performance. 

**Abstract (ZH)**: 骨折检测在医学影像分析中扮演着关键角色，传统骨折诊断依赖经验丰富的医师的视觉评估，然而该方法的速度和准确性受制于医师的经验。随着人工智能的快速发展，基于YOLO框架的深度学习模型被广泛应用于骨折检测，显示了在提高诊断效率和准确性方面的巨大潜力。本研究提出了一种改进的基于YOLO的模型，称为Fracture-YOLO，该模型结合了新型关键区域选择注意力（CRSelector）模块和尺度意识（ScA）头部，以进一步提高检测性能。具体而言，CRSelector模块利用全局纹理信息聚焦骨折区域的关键特征。 Meanwhile, the ScA module dynamically adjusts the weights of features at different scales, enhancing the model's capacity to identify fracture targets at multiple scales. 实验结果表明，与基准模型相比，Fracture-YOLO在检测精度上取得了显著提高，mAP50和mAP50-95分别提高了4和3，超过了基准模型并达到了当前最佳水平（SOTA）。 

---
# Train Once, Answer All: Many Pretraining Experiments for the Cost of One 

**Title (ZH)**: 一次训练，解决全部：一次训练的成本换取多项预训练实验 

**Authors**: Sebastian Bordt, Martin Pawelczyk  

**Link**: [PDF](https://arxiv.org/pdf/2509.23383)  

**Abstract**: Recent work has demonstrated that controlled pretraining experiments are a powerful tool for understanding learning, reasoning, and memorization in large language models (LLMs). However, the computational cost of pretraining presents a significant constraint. To overcome this constraint, we propose to conduct multiple pretraining experiments simultaneously during a single training run. We demonstrate the feasibility of this approach by conducting ten experiments during the training of a 1.5B parameter model on 210B tokens. Although we only train a single model, we can replicate the results from multiple previous works on data contamination, poisoning, and memorization. We also conduct novel investigations into knowledge acquisition, mathematical reasoning, and watermarking. For example, we dynamically update the training data until the model acquires a particular piece of knowledge. Remarkably, the influence of the ten experiments on the model's training dynamics and overall performance is minimal. However, interactions between different experiments may act as a potential confounder in our approach. We propose to test for interactions with continual pretraining experiments, finding them to be negligible in our setup. Overall, our findings suggest that performing multiple pretraining experiments in a single training run can enable rigorous scientific experimentation with large models on a compute budget. 

**Abstract (ZH)**: 最近的研究表明，控制预训练实验是理解大型语言模型（LLMs）学习、推理和记忆能力的强大工具。然而，预训练的计算成本构成了一个显著的限制。为了克服这一限制，我们提出在单次训练运行中同时进行多个预训练实验。我们通过在一个包含210亿令牌的150亿参数模型的训练过程中进行十项实验，证明了这种做法的可行性。尽管我们仅训练了一个模型，但仍能重现多项先前关于数据污染、毒化和记忆的研究结果。我们还对知识获取、数学推理和水印技术进行了新颖的探究。例如，我们动态更新训练数据，直到模型获取特定的知识。令人惊讶的是，这十项实验对模型训练动力学和整体性能的影响极小。然而，不同实验之间的相互作用可能成为我们方法中的潜在混杂因素。我们提出通过持续预训练实验测试这种相互作用，并发现在我们的设置中这种影响可以忽略不计。总体而言，我们的发现表明，在单次训练运行中进行多项预训练实验可以在计算预算内实现对大型模型的严格科学研究。 

---
# CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding 

**Title (ZH)**: CCD: 通过临床对比解码减轻放射学MLLMs幻觉 

**Authors**: Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho  

**Link**: [PDF](https://arxiv.org/pdf/2509.23379)  

**Abstract**: Multimodal large language models (MLLMs) have recently achieved remarkable progress in radiology by integrating visual perception with natural language understanding. However, they often generate clinically unsupported descriptions, known as medical hallucinations, which pose serious risks in medical applications that demand accuracy and image-grounded outputs. Through empirical analysis, we find that prompt-induced hallucinations remain prevalent in radiology MLLMs, largely due to over-sensitivity to clinical sections. To address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free and retrieval-free inference framework that integrates structured clinical signals from task-specific radiology expert models. CCD introduces a dual-stage contrastive mechanism to refine token-level logits during generation, thereby enhancing clinical fidelity without modifying the base MLLM. Experiments on three datasets and multiple models demonstrate that CCD consistently improves overall performance on radiology report generation (RRG). On the MIMIC-CXR dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to state-of-the-art RRG models. Our approach provides a lightweight and generalisable solution for mitigating medical hallucinations, effectively bridging expert models and MLLMs in radiology. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）在放射学领域通过结合视觉感知和自然语言理解取得了显著进展。然而，它们经常生成缺乏临床支持的描述，即医学幻觉，这在要求准确性和图像基础输出的医学应用中构成严重风险。通过实证分析，我们发现放射学MLLM中由临床部分过度敏感引发的幻觉仍然普遍存在。为解决这一问题，我们引入了一种无需训练和检索的推理框架Clinical Contrastive Cecoding（CCD），该框架通过任务特定的放射学专家模型整合结构化的临床信号。CCD引入了双阶段对比机制，在生成过程中细化标记级逻辑量，从而提高临床信度而不修改基础MLLM。在三个数据集和多种模型上的实验表明，CCD在放射学报告生成（RRG）任务上始终能提升整体性能。在MIMIC-CXR数据集上，当应用于最先进的RRG模型时，它在RadGraph-F1指标上可带来高达17%的提升。我们的方法提供了一种轻量级且可扩展的解决方案，用于减轻医学幻觉，有效连接专家模型和MLLM在放射学中的应用。 

---
# Graph Your Own Prompt 

**Title (ZH)**: 绘制你自己的提示图谱 

**Authors**: Xi Ding, Lei Wang, Piotr Koniusz, Yongsheng Gao  

**Link**: [PDF](https://arxiv.org/pdf/2509.23373)  

**Abstract**: We propose Graph Consistency Regularization (GCR), a novel framework that injects relational graph structures, derived from model predictions, into the learning process to promote class-aware, semantically meaningful feature representations. Functioning as a form of self-prompting, GCR enables the model to refine its internal structure using its own outputs. While deep networks learn rich representations, these often capture noisy inter-class similarities that contradict the model's predicted semantics. GCR addresses this issue by introducing parameter-free Graph Consistency Layers (GCLs) at arbitrary depths. Each GCL builds a batch-level feature similarity graph and aligns it with a global, class-aware masked prediction graph, derived by modulating softmax prediction similarities with intra-class indicators. This alignment enforces that feature-level relationships reflect class-consistent prediction behavior, acting as a semantic regularizer throughout the network. Unlike prior work, GCR introduces a multi-layer, cross-space graph alignment mechanism with adaptive weighting, where layer importance is learned from graph discrepancy magnitudes. This allows the model to prioritize semantically reliable layers and suppress noisy ones, enhancing feature quality without modifying the architecture or training procedure. GCR is model-agnostic, lightweight, and improves semantic structure across various networks and datasets. Experiments show that GCR promotes cleaner feature structure, stronger intra-class cohesion, and improved generalization, offering a new perspective on learning from prediction structure. [Project website](this https URL) [Code](this https URL) 

**Abstract (ZH)**: 我们提出图一致性正则化（GCR），这是一种新颖的框架，通过将源自模型预测的关系图结构注入学习过程，促进具有类意识和语义意义的特征表示。作为一种自我提示的形式，GCR使模型能够利用自身的输出来精化其内部结构。虽然深层网络学习到丰富的表示，但这些表示往往包含与模型预测的语义相矛盾的嘈杂跨类相似性。GCR通过在任意深度引入无参数的图一致性层（GCLs）来解决这一问题。每个GCL构建一批次级别的特征相似图，并将其与通过调整softmax预测相似度与类别内指示符来生成的全局类意识掩码预测图对齐。这种对齐确保特征级别的关系反映出类一致的预测行为，作为一种语义正则化在整个网络中起作用。与以前的工作不同，GCR引入了一种多层、跨空间的图对齐机制，具有自适应加权，其中层的重要性是从图差异幅度中学习到的。这使得模型能够优先考虑语义可靠的层并抑制嘈杂的层，提高特征质量而不修改架构或训练过程。GCR具有模型无关性，轻量级，并在各种网络和数据集上增强了语义结构。实验表明，GCR促进了更清洁的特征结构、更强的类内凝聚性和更好的泛化能力，为从预测结构学习提供了新的视角。[项目网站](this https URL) [代码](this https URL) 

---
# Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization 

**Title (ZH)**: 基于元加权在线采样的对齐方法：数据生成与偏好优化之间的桥梁 

**Authors**: Junming Yang, Ning Xu, Biao Liu, Shiqi Qiao, Xin Geng  

**Link**: [PDF](https://arxiv.org/pdf/2509.23371)  

**Abstract**: Preference optimization is crucial for aligning large language models (LLMs) with human values and intentions. A significant challenge in this process is the distribution mismatch between pre-collected offline preference data and the evolving model policy. Existing methods attempt to reduce this gap using static heuristics or decoupled online sampling strategies, but they often fail to adapt to the model's dynamic learning state. To bridge this gap, we propose Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework that dynamically couples data generation with model training. MetaAPO employs a lightweight meta-learner, as an "alignment gap estimator", to evaluate the potential benefits of on-policy sampling in relation to offline data. This guides targeted online generation and assigns sample-wise meta-weights to the optimization objective, dynamically balancing the quality and distribution of online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench demonstrate that MetaAPO consistently outperforms existing preference optimization approaches across various settings, while reducing 42% in online annotation costs. 

**Abstract (ZH)**: 偏好优化对于使大型语言模型（LLMs）与人类价值观和意图对齐至关重要。在这个过程中的一个重大挑战是预收集的离线偏好数据与模型政策的演变之间的分布不匹配。现有方法试图通过使用静态启发式方法或脱钩的在线采样策略来减少这种差距，但它们往往无法适应模型的动态学习状态。为了解决这一差距，我们提出了元加权自适应偏好优化（MetaAPO），这是一种新型框架，能够动态地将数据生成与模型训练耦合。MetaAPO 采用一个轻量级的元学习器作为“对齐差距估计器”，评估在线采样相对于离线数据的潜在益处，以指导针对性的在线生成，并为优化目标分配样本级别的元权重，动态平衡在线和离线数据的质量与分布。实验结果显示，MetaAPO 在各种情况下均能一致地优于现有的偏好优化方法，同时减少42%的在线标注成本。 

---
# MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction 

**Title (ZH)**: MedCritical: 通过自我协作修正增强小型语言模型的医疗推理 

**Authors**: Xinchun Su, Chunxu Luo, Yixuan Li, Weidong Yang, Lipeng Ma  

**Link**: [PDF](https://arxiv.org/pdf/2509.23368)  

**Abstract**: In the field of medicine, complex reasoning tasks such as clinical diagnosis, treatment planning, and medical knowledge integration pose significant challenges, where small language models often underperform compared to large language models like GPT-4 and Deepseek. Recent knowledge distillation-based methods aim to address these issues through teacher-guided error correction, but this LLM as judge approach remains challenging in terms of cost, time, and efficiency. To circumvent this issue, we propose a novel two-stage framework, MedCritical, which uses a small language model fine-tuned by a large teacher model to play against itself. In the first stage, we extract high-level and detailed long-chain thought templates from the teacher model to guide the student model to generate more complex reasoning thoughts. In the second stage, we introduce direct preference optimization (DPO) through model self-iteration collaboration to enhance the reasoning ability of the student model by playing against the correction trajectory of the fine-tuned model during training. This model self-learning DPO approach teaches the student model to use its own error-driven insights to consolidate its skills and knowledge to solve complex problems, and achieves comparable results to traditional knowledge distillation methods using teacher models at a lower cost. Notably, our MedCritical 7B model outperforms the Taiyi and Huatuo-o1-7B models by 3.04\% and 10.12\% respectively on the CMExam benchmark, achieving new SOTA performance among 7B-class small models. 

**Abstract (ZH)**: 在医学领域，临床诊断、治疗规划和医学知识整合等复杂推理任务提出了重大挑战，其中小语言模型往往不如GPT-4和Deepseek等大型语言模型表现优异。基于知识蒸馏的方法近期致力于通过教师引导的错误纠正来解决这些问题，但作为评估者的LLM方法在成本、时间和效率方面仍面临挑战。为克服这一问题，我们提出了一种新颖的两阶段框架MedCritical，该框架使用由大型教师模型微调的小型语言模型与其自身进行对抗。在第一阶段，我们从教师模型中提取高层次和详细化长链思考模板，以指导学生模型生成更复杂的推理思想。在第二阶段，我们通过模型自我迭代协作引入直接偏好优化（DPO），在训练过程中通过与微调模型的校正轨迹进行对抗，增强学生模型的推理能力。该模型自我学习DPO方法教会学生模型利用自身的错误驱动见解来巩固技能和知识以解决复杂问题，并以较低的成本实现了与传统教师模型知识蒸馏方法相当的成果。值得注意的是，我们的MedCritical 7B模型在CMExam基准测试中分别比Taiyi和Huatuo-o1-7B模型高出3.04%和10.12%，在7B级别的小型模型中达到了新的SOTA性能。 

---
# AI Education in Higher Education: A Taxonomy for Curriculum Reform and the Mission of Knowledge 

**Title (ZH)**: 高等教育中的AI教育：课程改革的分类学及其知识使命 

**Authors**: Tian Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2509.23363)  

**Abstract**: Artificial intelligence (AI) is reshaping higher education, yet current debates often feel tangled, mixing concerns about pedagogy, operations, curriculum, and the future of work without a shared framework. This paper offers a first attempt at a taxonomy to organize the diverse narratives of AI education and to inform discipline-based curricular discussions. We place these narratives within the enduring responsibility of higher education: the mission of knowledge. This mission includes not only the preservation and advancement of disciplinary expertise, but also the cultivation of skills and wisdom, i.e., forms of meta-knowledge that encompass judgment, ethics, and social responsibility. For the purpose of this paper's discussion, AI is defined as adaptive, data-driven systems that automate analysis, modeling, and decision-making, highlighting its dual role as enabler and disruptor across disciplines. We argue that the most consequential challenges lie at the level of curriculum and disciplinary purpose, where AI accelerates inquiry but also unsettles expertise and identity. We show how disciplines evolve through the interplay of research, curriculum, pedagogy, and faculty expertise, and why curricular reform is the central lever for meaningful change. Pedagogical innovation offers a strategic and accessible entry point, providing actionable steps that help faculty and students build the expertise needed to engage in deeper curricular rethinking and disciplinary renewal. Within this framing, we suggest that meaningful reform can move forward through structured faculty journeys: from AI literacy to pedagogy, curriculum design, and research integration. The key is to align these journeys with the mission of knowledge, turning the disruptive pressures of AI into opportunities for disciplines to sustain expertise, advance inquiry, and serve society. 

**Abstract (ZH)**: 人工智能（AI）正重塑高等教育，然而当前的辩论往往显得杂乱无章，交织着关于教学方法、运营、课程和未来工作前景的担忧，缺乏一个共同的框架。本文旨在首次尝试构建一种分类法，以组织AI教育的多样叙事，并为基于学科的课程讨论提供指导。我们将这些叙事置于高等教育永恒的责任之中：知识传承的任务。这一任务不仅包括学科专长的保存与发展，还包括技能和智慧（即判断、伦理和社会责任等形式的元知识）的培养。为了本文的讨论，我们将人工智能定义为适应性强、数据驱动的系统，能够自动化分析、建模和决策，强调其在各学科中作为助推器和颠覆者的双重角色。我们主张，最重大的挑战在于课程和学科目标的层面，AI加速了探究但同时也动摇了专业和身份。我们展示了学科通过研究、课程、教学和教员专长的相互作用而演变的过程，并阐明了课程改革是推动有意义变化的主要杠杆。教学创新提供了战略性的切入点，提供了一系列可操作的步骤，帮助教员和学生培养在深入课程重思和学科更新中所需的专业能力。在这种框架下，我们建议，有意义的改革可以通过结构化教员旅程推进：从人工智能素养到教学方法、课程设计和研究整合。关键在于将这些旅程与知识传承任务相一致，将AI带来的颠覆性压力转变为学科保持专业能力、推进探究和社会服务的机会。 

---
# Dual-Space Smoothness for Robust and Balanced LLM Unlearning 

**Title (ZH)**: 双空间平滑性以实现稳健且均衡的大型语言模型遗忘 

**Authors**: Han Yan, Zheyuan Liu, Meng Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23362)  

**Abstract**: With the rapid advancement of large language models, Machine Unlearning has emerged to address growing concerns around user privacy, copyright infringement, and overall safety. Yet state-of-the-art (SOTA) unlearning methods often suffer from catastrophic forgetting and metric imbalance, for example by over-optimizing one objective (e.g., unlearning effectiveness, utility preservation, or privacy protection) at the expense of others. In addition, small perturbations in the representation or parameter space can be exploited by relearn and jailbreak attacks. To address these challenges, we propose PRISM, a unified framework that enforces dual-space smoothness in representation and parameter spaces to improve robustness and balance unlearning metrics. PRISM consists of two smoothness optimization stages: (i) a representation space stage that employs a robustly trained probe to defend against jailbreak attacks, and (ii) a parameter-space stage that decouples retain-forget gradient conflicts, reduces imbalance, and smooths the parameter space to mitigate relearning attacks. Extensive experiments on WMDP and MUSE, across conversational-dialogue and continuous-text settings, show that PRISM outperforms SOTA baselines under multiple attacks while achieving a better balance among key metrics. 

**Abstract (ZH)**: 大规模语言模型的快速进展催生了机器遗忘技术以应对用户隐私、版权侵犯和整体安全等方面的担忧。然而，最先进的（SOTA）遗忘方法往往会因灾难性遗忘和指标失衡等问题而受到影响，例如过度优化一个目标（如遗忘效果、有用性保留或隐私保护）而牺牲其他目标。此外，在表示空间或参数空间的微小扰动可被利用进行重新学习和囚禁攻击。为了解决这些挑战，我们提出了一种统一体系结构PRISM，该体系结构在表示空间和参数空间中强制实施双空间平滑性以提高鲁棒性和平衡遗忘指标。PRISM包括两个平滑性优化阶段：（i）一个表示空间阶段，使用稳健训练的探针防御囚禁攻击，以及（ii）一个参数空间阶段，将保留和遗忘梯度冲突分离，减少不平衡，平滑参数空间以减轻重新学习攻击。在WMDP和MUSE数据集上，包括对话和连续文本设置的广泛实验表明，PRISM在多种攻击下优于SOTA基准，在关键指标之间实现了更好的平衡。 

---
# Dynamic-TreeRPO: Breaking the Independent Trajectory Bottleneck with Structured Sampling 

**Title (ZH)**: 动态树RPO：通过结构化采样打破独立轨迹瓶颈 

**Authors**: Xiaolong Fu, Lichen Ma, Zipeng Guo, Gaojing Zhou, Chongxiao Wang, ShiPing Dong, Shizhe Zhou, Shizhe Zhou, Ximan Liu, Jingling Fu, Tan Lit Sin, Yu Shi, Zhen Chen, Junshi Huang, Jason Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.23352)  

**Abstract**: The integration of Reinforcement Learning (RL) into flow matching models for text-to-image (T2I) generation has driven substantial advances in generation quality. However, these gains often come at the cost of exhaustive exploration and inefficient sampling strategies due to slight variation in the sampling group. Building on this insight, we propose Dynamic-TreeRPO, which implements the sliding-window sampling strategy as a tree-structured search with dynamic noise intensities along depth. We perform GRPO-guided optimization and constrained Stochastic Differential Equation (SDE) sampling within this tree structure. By sharing prefix paths of the tree, our design effectively amortizes the computational overhead of trajectory search. With well-designed noise intensities for each tree layer, Dynamic-TreeRPO can enhance the variation of exploration without any extra computational cost. Furthermore, we seamlessly integrate Supervised Fine-Tuning (SFT) and RL paradigm within Dynamic-TreeRPO to construct our proposed LayerTuning-RL, reformulating the loss function of SFT as a dynamically weighted Progress Reward Model (PRM) rather than a separate pretraining method. By associating this weighted PRM with dynamic-adaptive clipping bounds, the disruption of exploration process in Dynamic-TreeRPO is avoided. Benefiting from the tree-structured sampling and the LayerTuning-RL paradigm, our model dynamically explores a diverse search space along effective directions. Compared to existing baselines, our approach demonstrates significant superiority in terms of semantic consistency, visual fidelity, and human preference alignment on established benchmarks, including HPS-v2.1, PickScore, and ImageReward. In particular, our model outperforms SoTA by $4.9\%$, $5.91\%$, and $8.66\%$ on those benchmarks, respectively, while improving the training efficiency by nearly $50\%$. 

**Abstract (ZH)**: 动态树结构RPO结合监督微调与 reinforcement learning 在文本到图像生成中的应用 

---
# ABC-Eval: Benchmarking Large Language Models on Symbolic Music Understanding and Instruction Following 

**Title (ZH)**: ABC-Eval:评估大型语言模型在符号音乐理解与指令跟随方面的 performance 

**Authors**: Jiahao Zhao, Yunjia Li, Wei Li, Kazuyoshi Yoshii  

**Link**: [PDF](https://arxiv.org/pdf/2509.23350)  

**Abstract**: As large language models continue to develop, the feasibility and significance of text-based symbolic music tasks have become increasingly prominent. While symbolic music has been widely used in generation tasks, LLM capabilities in understanding and reasoning about symbolic music remain largely underexplored. To address this gap, we propose ABC-Eval, the first open-source benchmark dedicated to the understanding and instruction-following capabilities in text-based ABC notation scores. It comprises 1,086 test samples spanning 10 sub-tasks, covering scenarios from basic musical syntax comprehension to complex sequence-level reasoning. Such a diverse scope poses substantial challenges to models' ability to handle symbolic music tasks. We evaluated seven state-of-the-art LLMs on ABC-Eval, and the results reveal notable limitations in existing models' symbolic music processing capabilities. Furthermore, the consistent performance of individual baselines across different sub-tasks supports the reliability of our benchmark. 

**Abstract (ZH)**: 随着大型语言模型的不断发展，基于文本的符号音乐任务的可行性和重要性日益凸显。虽然符号音乐在生成任务中已被广泛应用，但在理解和推理符号音乐方面的LLM能力仍 largely underexplored。为弥补这一差距，我们提出了ABC-Eval，这是首个专注于基于文本的ABC符号记谱理解与指令跟随能力的开源基准。它包含1,086个测试样本，涵盖10个子任务，从基本的音乐语法理解到复杂的序列级推理。如此广泛的范围对模型处理符号音乐任务的能力提出了重大挑战。我们对ABC-Eval上的七个最先进的LLM进行了评估，结果揭示了现有模型在符号音乐处理能力方面的显著局限性。此外，各基线在不同子任务上的稳定表现支持了我们基准的可靠性。 

---
# DentVLM: A Multimodal Vision-Language Model for Comprehensive Dental Diagnosis and Enhanced Clinical Practice 

**Title (ZH)**: DentVLM：一种多模态视觉-语言模型，用于全面的口腔诊断和增强的临床实践 

**Authors**: Zijie Meng, Jin Hao, Xiwei Dai, Yang Feng, Jiaxiang Liu, Bin Feng, Huikai Wu, Xiaotang Gai, Hengchuan Zhu, Tianxiang Hu, Yangyang Wu, Hongxia Xu, Jin Li, Jun Xiao, Xiaoqiang Liu, Joey Tianyi Zhou, Fudong Zhu, Zhihe Zhao, Lunguo Xia, Bing Fang, Jimeng Sun, Jian Wu, Zuozhu Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23344)  

**Abstract**: Diagnosing and managing oral diseases necessitate advanced visual interpretation across diverse imaging modalities and integrated information synthesis. While current AI models excel at isolated tasks, they often fall short in addressing the complex, multimodal requirements of comprehensive clinical dental practice. Here we introduce DentVLM, a multimodal vision-language model engineered for expert-level oral disease diagnosis. DentVLM was developed using a comprehensive, large-scale, bilingual dataset of 110,447 images and 2.46 million visual question-answering (VQA) pairs. The model is capable of interpreting seven 2D oral imaging modalities across 36 diagnostic tasks, significantly outperforming leading proprietary and open-source models by 19.6% higher accuracy for oral diseases and 27.9% for malocclusions. In a clinical study involving 25 dentists, evaluating 1,946 patients and encompassing 3,105 QA pairs, DentVLM surpassed the diagnostic performance of 13 junior dentists on 21 of 36 tasks and exceeded that of 12 senior dentists on 12 of 36 tasks. When integrated into a collaborative workflow, DentVLM elevated junior dentists' performance to senior levels and reduced diagnostic time for all practitioners by 15-22%. Furthermore, DentVLM exhibited promising performance across three practical utility scenarios, including home-based dental health management, hospital-based intelligent diagnosis and multi-agent collaborative interaction. These findings establish DentVLM as a robust clinical decision support tool, poised to enhance primary dental care, mitigate provider-patient imbalances, and democratize access to specialized medical expertise within the field of dentistry. 

**Abstract (ZH)**: 口腔疾病诊断和管理需要在多种成像模态下进行高级视觉解释并综合集成信息。当前的AI模型在单个任务上表现出色，但往往难以应对全面临床牙科实践中的复杂、多模态需求。我们介绍了DentVLM，这是一种针对专家级口腔疾病诊断设计的多模态视觉-语言模型。DentVLM 使用一个包含 110,447 幅图像和 246 万个视觉问答 (VQA) 对的全面大规模双语数据集进行开发。该模型能够解析 2D 口腔影像的 7 种模态并完成 36 项诊断任务，其在口腔疾病和错颌畸形的诊断准确性方面分别比领先的专业和开源模型高出 19.6% 和 27.9%。在涉及 25 名牙科医生的临床研究中，评估了 1,946 位患者和 3,105 个问答对，DentVLM 在 36 项中的 21 项诊断任务上超过了 13 名初级牙医的表现，在 12 项诊断任务上则超过了 12 名高级牙医的表现。当整合到协作工作流程中时，DentVLM 提升了初级牙医的表现至高级水平，并降低了所有牙医的诊断时间 15% 至 22%。此外，DentVLM 在家庭口腔健康管理、医院智能诊断和多智能体协作交互三种实际应用场景中表现出色。这些发现确立了DentVLM作为稳健的临床决策支持工具的地位，有望提升初级牙科护理质量，缓解提供者与患者之间的失衡，并在牙科领域普及专业化医疗专业知识的获取。 

---
# PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation 

**Title (ZH)**: PARROT：评价LLM在跨系统SQL翻译中的基准 

**Authors**: Wei Zhou, Guoliang Li, Haoyu Wang, Yuxing Han, Xufei Wu, Fan Wu, Xuanhe Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2509.23338)  

**Abstract**: Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: this https URL. 

**Abstract (ZH)**: 大型语言模型在Text-to-SQL任务中显示出不断增强的有效性。然而，另一个密切相关的問題，跨系统SQL翻译（即，SQL-to-SQL），即将一个数据库系统（例如，MySQL）中的查询转换为另一个系统（例如，ClickHouse）中的等效查询，在实际应用中具有重要的意义但尚未得到充分探索。现有的SQL基准不适合用于SQL-to-SQL评估，因为它们（1）仅关注有限的数据库系统（通常是SQLite）；（2）无法涵盖许多系统特定的SQL方言（例如，自定义函数、数据类型和语法规则）。因此，在本文中，我们介绍了PARROT，一个适用于跨系统SQL翻译的实用且真实的基准测试集。PARROT包含来自38个开源基准和实际业务服务的598个翻译对，特别设计用于挑战系统特定的SQL理解（例如，大型语言模型的平均准确率低于38.53%）。我们还提供了多种基准测试变体，包括包含28,003个翻译（用于广泛的语法测试）的PARROT-Diverse和包含5,306个代表性样本（用于集中的压力测试）的PARROT-Simple，覆盖了22个生产级数据库系统。为了促进未来的研究，我们在以下链接发布了公开的排行榜和源代码：this https URL。 

---
# Space Robotics Bench: Robot Learning Beyond Earth 

**Title (ZH)**: 空间机器人平台：超越地球的机器人学习 

**Authors**: Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez, Carol Martinez  

**Link**: [PDF](https://arxiv.org/pdf/2509.23328)  

**Abstract**: The growing ambition for space exploration demands robust autonomous systems that can operate in unstructured environments under extreme extraterrestrial conditions. The adoption of robot learning in this domain is severely hindered by the prohibitive cost of technology demonstrations and the limited availability of data. To bridge this gap, we introduce the Space Robotics Bench, an open-source simulation framework for robot learning in space. It offers a modular architecture that integrates on-demand procedural generation with massively parallel simulation environments to support the creation of vast and diverse training distributions for learning-based agents. To ground research and enable direct comparison, the framework includes a comprehensive suite of benchmark tasks that span a wide range of mission-relevant scenarios. We establish performance baselines using standard reinforcement learning algorithms and present a series of experimental case studies that investigate key challenges in generalization, end-to-end learning, adaptive control, and sim-to-real transfer. Our results reveal insights into the limitations of current methods and demonstrate the utility of the framework in producing policies capable of real-world operation. These contributions establish the Space Robotics Bench as a valuable resource for developing, benchmarking, and deploying the robust autonomous systems required for the final frontier. 

**Abstract (ZH)**: 空间机器人台架：一种用于空间机器人学习的开源仿真框架 

---
# Robust Fine-Tuning from Non-Robust Pretrained Models: Mitigating Suboptimal Transfer With Adversarial Scheduling 

**Title (ZH)**: 从非鲁棒预训练模型进行鲁棒微调：通过对抗性调度减轻次优转移影响 

**Authors**: Jonas Ngnawé, Maxime Heuillet, Sabyasachi Sahoo, Yann Pequignot, Ola Ahmad, Audrey Durand, Frédéric Precioso, Christian Gagné  

**Link**: [PDF](https://arxiv.org/pdf/2509.23325)  

**Abstract**: Fine-tuning pretrained models is a standard and effective workflow in modern machine learning. However, robust fine-tuning (RFT), which aims to simultaneously achieve adaptation to a downstream task and robustness to adversarial examples, remains challenging. Despite the abundance of non-robust pretrained models in open-source repositories, their potential for RFT is less understood. We address this knowledge gap by systematically examining RFT from such non-robust models. Our experiments reveal that fine-tuning non-robust models with a robust objective, even under small perturbations, can lead to poor performance, a phenomenon that we dub \emph{suboptimal transfer}. In challenging scenarios (eg, difficult tasks, high perturbation), the resulting performance can be so low that it may be considered a transfer failure. We find that fine-tuning using a robust objective impedes task adaptation at the beginning of training and eventually prevents optimal transfer. However, we propose a novel heuristic, \emph{Epsilon-Scheduling}, a schedule over perturbation strength used during training that promotes optimal transfer. Additionally, we introduce \emph{expected robustness}, a metric that captures performance across a range of perturbations, providing a more comprehensive evaluation of the accuracy-robustness trade-off for diverse models at test time. Extensive experiments on a wide range of configurations (six pretrained models and five datasets) show that \emph{Epsilon-Scheduling} successfully prevents \emph{suboptimal transfer} and consistently improves expected robustness. 

**Abstract (ZH)**: 微调预训练模型是现代机器学习中的标准且有效的workflow。然而，鲁棒微调（RFT），其目标是在适应下游任务的同时增强对对抗样本的鲁棒性，仍然具有挑战性。尽管开源库中有大量的非鲁棒预训练模型，但它们的RFT潜力尚未充分理解。我们通过系统地研究这些非鲁棒模型的RFT来填补这一知识空白。我们的实验揭示，即使在小扰动下使用鲁棒目标微调非鲁棒模型，也可能导致性能不佳，我们称之为“次优转移”。在具有挑战性的场景中（例如，困难的任务、高扰动），这种性能可能如此低，以至于可以被视为转移失败。我们发现，使用鲁棒目标进行微调在训练初期阻碍了任务适应，并最终阻止了最优转移。然而，我们提出了一种新颖的启发式方法，称为“ε调度”，这是一种在训练过程中使用的扰动强度调度方案，可促进最优转移。此外，我们引入了“期望鲁棒性”这一度量标准，它捕捉了在一系列扰动下的性能，为测试时不同模型的准确性和鲁棒性权衡提供了更全面的评估。广泛配置（六种预训练模型和五种数据集）的大量实验表明，“ε调度”成功地防止了“次优转移”，并且始终提高了期望鲁棒性。 

---
# Scaling LLM Test-Time Compute with Mobile NPU on Smartphones 

**Title (ZH)**: 在智能手机上使用移动NPU扩展LLM测试时计算性能 

**Authors**: Zixu Hao, Jianyu Wei, Tuowei Wang, Minxing Huang, Huiqiang Jiang, Shiqi Jiang, Ting Cao, Ju Ren  

**Link**: [PDF](https://arxiv.org/pdf/2509.23324)  

**Abstract**: Deploying Large Language Models (LLMs) on mobile devices faces the challenge of insufficient performance in smaller models and excessive resource consumption in larger ones. This paper highlights that mobile Neural Processing Units (NPUs) have underutilized computational resources, particularly their matrix multiplication units, during typical LLM inference. To leverage this wasted compute capacity, we propose applying parallel test-time scaling techniques on mobile NPUs to enhance the performance of smaller LLMs. However, this approach confronts inherent NPU challenges, including inadequate hardware support for fine-grained quantization and low efficiency in general-purpose computations. To overcome these, we introduce two key techniques: a hardware-aware tile quantization scheme that aligns group quantization with NPU memory access patterns, and efficient LUT-based replacements for complex operations such as Softmax and dequantization. We design and implement an end-to-end inference system that leverages the NPU's compute capability to support test-time scaling on Qualcomm Snapdragon platforms. Experiments show our approach brings significant speedups: up to 19.0 for mixed-precision GEMM and 2.2 for Softmax. More importantly, we demonstrate that smaller models using test-time scaling can match or exceed the accuracy of larger models, achieving a new performance-cost Pareto frontier. 

**Abstract (ZH)**: 在移动设备上部署大语言模型（LLMs）面临小型模型性能不足和大型模型资源消耗过多的挑战。本文指出，在典型的大语言模型推断过程中，移动神经处理单元（NPUs）的计算资源尤其是矩阵乘法单元利用不足。为利用这些浪费的计算能力，我们提出在移动NPUs上应用并行测试时缩放技术以提升小型LLMs的性能。然而，这一方法面临固有的NPU挑战，包括对精细量化硬件支持不足和通用计算效率低下。为克服这些挑战，我们引入了两项关键技术：一种硬件感知的分组量化方案，该方案将分组量化与NPU内存访问模式对齐，以及基于查找表（LUT）的有效替代复杂操作（如Softmax和去量化）的方法。我们设计并实现了一个端到端的推断系统，该系统利用NPU的计算能力在高通骁龙平台上支持测试时缩放。实验结果显示，我们的方法带来了显著的速度提升：对于混合精度GEMM可达19.0倍，对于Softmax可达2.2倍。更重要的是，我们证明了使用测试时缩放的小型模型可以在准确率上与大型模型匹敌，从而开辟了新的性能-成本帕累托前沿。 

---
# MELCOT: A Hybrid Learning Architecture with Marginal Preservation for Matrix-Valued Regression 

**Title (ZH)**: MELCOT: 一种兼顾边缘保留的矩阵值回归混合学习架构 

**Authors**: Khang Tran, Hieu Cao, Thinh Pham, Nghiem Diep, Tri Cao, Binh Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2509.23315)  

**Abstract**: Regression is essential across many domains but remains challenging in high-dimensional settings, where existing methods often lose spatial structure or demand heavy storage. In this work, we address the problem of matrix-valued regression, where each sample is naturally represented as a matrix. We propose MELCOT, a hybrid model that integrates a classical machine learning-based Marginal Estimation (ME) block with a deep learning-based Learnable-Cost Optimal Transport (LCOT) block. The ME block estimates data marginals to preserve spatial information, while the LCOT block learns complex global features. This design enables MELCOT to inherit the strengths of both classical and deep learning methods. Extensive experiments across diverse datasets and domains demonstrate that MELCOT consistently outperforms all baselines while remaining highly efficient. 

**Abstract (ZH)**: 矩阵值回归在高维设置中至关重要但依然具有挑战性，现有方法往往会在保留空间结构或需要大量存储方面遇到困难。为解决这一问题，我们提出了一种集成模型MELCOT，该模型结合了基于经典机器学习的边缘估计（ME）模块和基于深度学习的可学习成本最优传输（LCOT）模块。ME模块估计数据边缘以保留空间信息，而LCOT模块学习复杂的全局特征。这种设计使得MELCOT能够继承经典和深度学习方法的优点。广泛的数据集和领域实验表明，MELCOT在所有基线方法中表现最优且具有很高的效率。 

---
# Seeing Symbols, Missing Cultures: Probing Vision-Language Models' Reasoning on Fire Imagery and Cultural Meaning 

**Title (ZH)**: 看到符号，忽视文化：探究视觉-语言模型在火灾图像和文化意义推理中的局限性 

**Authors**: Haorui Yu, Qiufeng Yi, Yijia Chu, Yang Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2509.23311)  

**Abstract**: Vision-Language Models (VLMs) often appear culturally competent but rely on superficial pattern matching rather than genuine cultural understanding. We introduce a diagnostic framework to probe VLM reasoning on fire-themed cultural imagery through both classification and explanation analysis. Testing multiple models on Western festivals, non-Western traditions, and emergency scenes reveals systematic biases: models correctly identify prominent Western festivals but struggle with underrepresented cultural events, frequently offering vague labels or dangerously misclassifying emergencies as celebrations. These failures expose the risks of symbolic shortcuts and highlight the need for cultural evaluation beyond accuracy metrics to ensure interpretable and fair multimodal systems. 

**Abstract (ZH)**: 视觉-语言模型往往在文化上表现出一定的能力，但实际依赖于表面的模式匹配而非真正的文化理解。我们提出了一种诊断框架，通过分类和解释分析，探究视觉-语言模型在文化主题火元素图像上的推理。对不同文化背景下的西方节日、非西方传统和紧急场景进行测试揭示了系统的偏差：模型能够正确识别重要的西方节日，但在处理欠代表的文化事件时表现困难，经常提供含糊的标签或将紧急情况错误地分类为庆祝活动。这些失败揭示了符号捷径的风险，并强调了在确保可解释性和公平性的同时，需要超越准确率指标的文化评估。 

---
# A Neural ODE Approach to Aircraft Flight Dynamics Modelling 

**Title (ZH)**: 一种基于神经ODE的航空飞行动力学建模方法 

**Authors**: Gabriel Jarry, Ramon Dalmau, Xavier Olive, Philippe Very  

**Link**: [PDF](https://arxiv.org/pdf/2509.23307)  

**Abstract**: Accurate aircraft trajectory prediction is critical for air traffic management, airline operations, and environmental assessment. This paper introduces NODE-FDM, a Neural Ordinary Differential Equations-based Flight Dynamics Model trained on Quick Access Recorder (QAR) data. By combining analytical kinematic relations with data-driven components, NODE-FDM achieves a more accurate reproduction of recorded trajectories than state-of-the-art models such as a BADA-based trajectory generation methodology (BADA4 performance model combined with trajectory control routines), particularly in the descent phase of the flight. The analysis demonstrates marked improvements across altitude, speed, and mass dynamics. Despite current limitations, including limited physical constraints and the limited availability of QAR data, the results demonstrate the potential of physics-informed neural ordinary differential equations as a high-fidelity, data-driven approach to aircraft performance modelling. Future work will extend the framework to incorporate a full modelling of the lateral dynamics of the aircraft. 

**Abstract (ZH)**: 基于神经常微分方程的飞行动力学模型NODE-FDM及其对飞机轨迹预测的应用 

---
# A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models 

**Title (ZH)**: 任何阶数，任意步数的安全对齐for扩散语言模型 

**Authors**: Wonje Jeung, Sangyeon Yoon, Yoonjun Cho, Dongjae Jeon, Sangwoo Shin, Hyesoo Hong, Albert No  

**Link**: [PDF](https://arxiv.org/pdf/2509.23286)  

**Abstract**: Diffusion large language models (dLLMs) enable any-order generation, but this flexibility enlarges the attack surface: harmful spans may appear at arbitrary positions, and template-based prefilling attacks such as DIJA bypass response-level refusals. We introduce A2D (Any-Order, Any-Step Defense), a token-level alignment method that aligns dLLMs to emit an [EOS] refusal signal whenever harmful content arises. By aligning safety directly at the token-level under randomized masking, A2D achieves robustness to both any-decoding-order and any-step prefilling attacks under various conditions. It also enables real-time monitoring: dLLMs may begin a response but automatically terminate if unsafe continuation emerges. On safety benchmarks, A2D consistently prevents the generation of harmful outputs, slashing DIJA success rates from over 80% to near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B), and thresholded [EOS] probabilities allow early rejection, yielding up to 19.3x faster safe termination. 

**Abstract (ZH)**: 任何顺序和任何步骤防御：面向标记的整条目保护方法（A2D） 

---
# Continuous-Time Reinforcement Learning for Asset-Liability Management 

**Title (ZH)**: 连续时间强化学习在资产-负债管理中的应用 

**Authors**: Yilie Huang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23280)  

**Abstract**: This paper proposes a novel approach for Asset-Liability Management (ALM) by employing continuous-time Reinforcement Learning (RL) with a linear-quadratic (LQ) formulation that incorporates both interim and terminal objectives. We develop a model-free, policy gradient-based soft actor-critic algorithm tailored to ALM for dynamically synchronizing assets and liabilities. To ensure an effective balance between exploration and exploitation with minimal tuning, we introduce adaptive exploration for the actor and scheduled exploration for the critic. Our empirical study evaluates this approach against two enhanced traditional financial strategies, a model-based continuous-time RL method, and three state-of-the-art RL algorithms. Evaluated across 200 randomized market scenarios, our method achieves higher average rewards than all alternative strategies, with rapid initial gains and sustained superior performance. The outperformance stems not from complex neural networks or improved parameter estimation, but from directly learning the optimal ALM strategy without learning the environment. 

**Abstract (ZH)**: 基于连续时间强化学习的资产-负债管理新方法：线性二次形式兼顾中间和最终目标 

---
# Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing 

**Title (ZH)**: Vid-Freeze: 通过时间冻结保护图像免受恶意图像生成为视频的攻击 

**Authors**: Rohit Chowdhury, Aniruddha Bala, Rohan Jaiswal, Siddharth Roheda  

**Link**: [PDF](https://arxiv.org/pdf/2509.23279)  

**Abstract**: The rapid progress of image-to-video (I2V) generation models has introduced significant risks, enabling video synthesis from static images and facilitating deceptive or malicious content creation. While prior defenses such as I2VGuard attempt to immunize images, effective and principled protection to block motion remains underexplored. In this work, we introduce Vid-Freeze - a novel attention-suppressing adversarial attack that adds carefully crafted adversarial perturbations to images. Our method explicitly targets the attention mechanism of I2V models, completely disrupting motion synthesis while preserving semantic fidelity of the input image. The resulting immunized images generate stand-still or near-static videos, effectively blocking malicious content creation. Our experiments demonstrate the impressive protection provided by the proposed approach, highlighting the importance of attention attacks as a promising direction for robust and proactive defenses against misuse of I2V generation models. 

**Abstract (ZH)**: 基于注意力抑制的 Vid-Freeze：一种新颖的图像到视频生成模型对抗攻击 

---
# Learning Regional Monsoon Patterns with a Multimodal Attention U-Net 

**Title (ZH)**: 使用多模态注意力U-Net学习区域季风模式 

**Authors**: Swaib Ilias Mazumder, Manish Kumar, Aparajita Khan  

**Link**: [PDF](https://arxiv.org/pdf/2509.23267)  

**Abstract**: Accurate monsoon rainfall prediction is vital for India's agriculture, water management, and climate risk planning, yet remains challenging due to sparse ground observations and complex regional variability. We present a multimodal deep learning framework for high-resolution precipitation classification that leverages satellite and Earth observation data. Unlike previous rainfall prediction models based on coarse 5-50 km grids, we curate a new 1 km resolution dataset for five Indian states, integrating seven key geospatial modalities: land surface temperature, vegetation (NDVI), soil moisture, relative humidity, wind speed, elevation, and land use, covering the June-September 2024 monsoon season. Our approach uses an attention-guided U-Net architecture to capture spatial patterns and temporal dependencies across modalities, combined with focal and dice loss functions to handle rainfall class imbalance defined by the India Meteorological Department (IMD). Experiments demonstrate that our multimodal framework consistently outperforms unimodal baselines and existing deep learning methods, especially in extreme rainfall categories. This work contributes a scalable framework, benchmark dataset, and state-of-the-art results for regional monsoon forecasting, climate resilience, and geospatial AI applications in India. 

**Abstract (ZH)**: 准确的季风雨量预测对于印度的农业、水资源管理和气候风险规划至关重要，但由于地面观测稀疏和区域复杂性，仍具有挑战性。我们提出了一种多模态深度学习框架，用于高分辨率降水分类，该框架利用了卫星和地球observation数据。与基于5-50 km粗网格的以往降雨预测模型不同，我们为五个印度邦编制了一个1 km分辨率的新数据集，整合了七个关键的地理空间模态：地表温度、植被（NDVI）、土壤湿度、相对湿度、风速、海拔和土地用途，涵盖了2024年6月至9月的季风雨季。我们的方法使用注意力引导的U-Net架构来捕捉模态间的空间模式和时间依赖性，并采用焦点损失和Dice损失函数来处理印度气象部门（IMD）定义的降雨类别不平衡。实验结果表明，我们的多模态框架在极端降雨类别中始终优于单模态基线和现有深度学习方法。这项工作为印度地区的季风雨量预报、气候韧性和地理空间AI应用提供了可扩展的框架、基准数据集和最先进的结果。 

---
# Adaptive Token-Weighted Differential Privacy for LLMs: Not All Tokens Require Equal Protection 

**Title (ZH)**: 自适应词元加权差分隐私技术：并非所有词元都需要同等保护 

**Authors**: Manjiang Yu, Priyanka Singh, Xue Li, Yang Cao  

**Link**: [PDF](https://arxiv.org/pdf/2509.23246)  

**Abstract**: Large language models (LLMs) frequently memorize sensitive or personal information, raising significant privacy concerns. Existing variants of differential privacy stochastic gradient descent (DPSGD) inject uniform noise into every gradient step, significantly extending training time and reducing model accuracy. We propose that concentrating noise primarily on gradients associated with sensitive tokens can substantially decrease DP training time, strengthen the protection of sensitive information, and simultaneously preserve the model's performance on non-sensitive data. We operationalize this insight through Adaptive Token-Weighted Differential Privacy (ATDP), a modification of vanilla DP-SGD that adaptively assigns different gradient weights to sensitive and non-sensitive tokens. By employing a larger noise scale at the early stage of training, ATDP rapidly disrupts memorization of sensitive content. As a result, ATDP only requires a few additional epochs of lightweight post-processing following standard fine-tuning, injecting targeted noise primarily on parameters corresponding to sensitive tokens, thus minimally affecting the model's general capabilities. ATDP can be seamlessly integrated into any existing DP-based fine-tuning pipeline or directly applied to non-private models as a fast privacy-enhancing measure. Additionally, combined with an initial redacted fine-tuning phase, ATDP forms a streamlined DP pipeline that achieves comparable canary protection to state-of-the-art DP-SGD methods, significantly reduces the computational overhead of DP fine-tuning, shortening training time by approximately 90 percent, while achieving comparable or superior privacy protection and minimal accuracy degradation. 

**Abstract (ZH)**: 大型语言模型 (LLMs) 经常记住敏感或个人信息，引发了重大的隐私担忧。现有差分隐私随机梯度下降 (DPSGD) 的变体在每个梯度步骤中注入均匀噪声，显著延长了训练时间并降低了模型准确性。我们提出，将噪声主要集中在与敏感词元相关的梯度上，可以大幅减少差分隐私训练时间，加强敏感信息保护，并同时保持模型在非敏感数据上的性能。我们通过自适应词元加权差分隐私 (ATDP) 将这一见解付诸实践，ATDP 是一种对 vanilla DP-SGD 的修改，能够根据不同情况为敏感和非敏感词元分配不同的梯度权重。通过在训练早期使用更大的噪声尺度，ATDP 迅速破坏对敏感内容的记忆。因此，ATDP 只需要在标准微调之后进行少量的轻量级后处理，主要在与敏感词元相对应的参数上注入目标噪声，从而最小化对模型通用能力的负面影响。ATDP 可以无缝集成到任何现有的基于差分隐私的微调管道中，或者直接应用于非私有模型，作为快速的隐私增强措施。此外，结合初始红处理微调阶段，ATDP 形成了一种简化的工作流，能够实现与最先进的 DPSGD 方法相当的开箱即用保护，显著减少了差分隐私微调的计算开销，将训练时间缩短约 90%，同时实现相当或更优的隐私保护和最小的准确性下降。 

---
# Online Dynamic Goal Recognition in Gym Environments 

**Title (ZH)**: 在线动态目标识别在Gym环境中 

**Authors**: Shamir Matan, Elhadad Osher, Nageris Ben, Mirsky Reuth  

**Link**: [PDF](https://arxiv.org/pdf/2509.23244)  

**Abstract**: Goal Recognition (GR) is the task of inferring an agent's intended goal from partial observations of its behavior, typically in an online and one-shot setting. Despite recent advances in model-free GR, particularly in applications such as human-robot interaction, surveillance, and assistive systems, the field remains fragmented due to inconsistencies in benchmarks, domains, and evaluation protocols.
To address this, we introduce gr-libs (this https URL) and gr-envs (this https URL), two complementary open-source frameworks that support the development, evaluation, and comparison of GR algorithms in Gym-compatible environments. gr-libs includes modular implementations of MDP-based GR baselines, diagnostic tools, and evaluation utilities. gr-envs provides a curated suite of environments adapted for dynamic and goal-directed behavior, along with wrappers that ensure compatibility with standard reinforcement learning toolkits. Together, these libraries offer a standardized, extensible, and reproducible platform for advancing GR research. Both packages are open-source and available on GitHub and PyPI. 

**Abstract (ZH)**: Goal Recognition: A Standardized Framework for Developing and Evaluating Goal Recognition Algorithms 

---
# Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection 

**Title (ZH)**: 自我一致性作为免费午餐：通过自我反思减少视觉语言模型的幻觉 

**Authors**: Mingfei Han, Haihong Hao, Jinxing Zhou, Zhihui Li, Yuhui Zheng, Xueqing Deng, Linjie Yang, Xiaojun Chang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23236)  

**Abstract**: Vision-language models often hallucinate details, generating non-existent objects or inaccurate attributes that compromise output reliability. Existing methods typically address these issues via extensive human annotations or external supervision from more powerful models. In this work, we present a novel framework that leverages the model's self-consistency between long responses and short answers to generate preference pairs for training. We observe that short binary questions tend to yield highly reliable responses, which can be used to query the target model to evaluate and rank its generated responses. Specifically, we design a self-reflection pipeline where detailed model responses are compared against concise binary answers, and inconsistency signals are utilized to automatically curate high-quality training data without human annotations or external model-based supervision. By relying solely on self-consistency rather than external supervision, our method offers a scalable and efficient solution that effectively reduces hallucinations using unlabeled data. Extensive experiments on multiple benchmarks, i.e., AMBER, MultiObject-Hal (ROPE), Object HalBench, and MMHal-Bench, demonstrate significant improvements in factual grounding and reliability. Moreover, our approach maintains robust instruction-following ability, as evidenced by enhanced performance on LLaVA-Bench and MMBench. 

**Abstract (ZH)**: Vision-language模型常常出现幻觉现象，生成不存在的物体或不准确的属性，影响输出的可靠性。现有方法通常通过大量的人工注释或更强模型的外部监督来解决这些问题。在此工作中，我们提出了一种新颖的框架，利用模型在长响应和短答案之间的一致性来生成训练偏好对。我们观察到，简短的二元问题往往会生成高度可靠的响应，这些响应可以用来查询目标模型以评估和排序其生成的响应。具体而言，我们设计了一条自我反思管道，其中详细的模型响应与简短的二元答案进行比较，不一致性信号被用来自动收集高质量的训练数据，而无需人工注释或基于模型的外部监督。通过仅仅依赖自我一致性而非外部监督，我们的方法提供了一种可扩展且高效的解决方案，能够有效利用未标注数据减少幻觉现象。在AMBER、MultiObject-Hal (ROPE)、Object HalBench和MMHal-Bench等多个基准上的 extensive 实验表明，在事实依据和可靠性方面取得了显著改进。此外，我们的方法在LLaVA-Bench和MMBench上的增强性能验证了其稳健的指令跟随能力。 

---
# Patch Rebirth: Toward Fast and Transferable Model Inversion of Vision Transformers 

**Title (ZH)**: Patch 重生：朝向快速可移植的视觉变换器模型倒置 

**Authors**: Seongsoo Heo, Dong-Wan Choi  

**Link**: [PDF](https://arxiv.org/pdf/2509.23235)  

**Abstract**: Model inversion is a widely adopted technique in data-free learning that reconstructs synthetic inputs from a pretrained model through iterative optimization, without access to original training data. Unfortunately, its application to state-of-the-art Vision Transformers (ViTs) poses a major computational challenge, due to their expensive self-attention mechanisms. To address this, Sparse Model Inversion (SMI) was proposed to improve efficiency by pruning and discarding seemingly unimportant patches, which were even claimed to be obstacles to knowledge transfer. However, our empirical findings suggest the opposite: even randomly selected patches can eventually acquire transferable knowledge through continued inversion. This reveals that discarding any prematurely inverted patches is inefficient, as it suppresses the extraction of class-agnostic features essential for knowledge transfer, along with class-specific features. In this paper, we propose Patch Rebirth Inversion (PRI), a novel approach that incrementally detaches the most important patches during the inversion process to construct sparse synthetic images, while allowing the remaining patches to continue evolving for future selection. This progressive strategy not only improves efficiency, but also encourages initially less informative patches to gradually accumulate more class-relevant knowledge, a phenomenon we refer to as the Re-Birth effect, thereby effectively balancing class-agnostic and class-specific knowledge. Experimental results show that PRI achieves up to 10x faster inversion than standard Dense Model Inversion (DMI) and 2x faster than SMI, while consistently outperforming SMI in accuracy and matching the performance of DMI. 

**Abstract (ZH)**: Patch Rebirth Inversion：一种渐进式关键区域再生的稀疏模型反转方法 

---
# SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts 

**Title (ZH)**: SPEC-RL: 通过推测性滚出自加速在线策略强化学习 

**Authors**: Bingshuai Liu, Ante Wang, Zijun Min, Liang Yao, Haibo Zhang, Yang Liu, Anxiang Zeng, Jinsong Su  

**Link**: [PDF](https://arxiv.org/pdf/2509.23232)  

**Abstract**: Large Language Models (LLMs) increasingly rely on reinforcement learning with verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning. However, the training process remains bottlenecked by the computationally expensive rollout stage. Existing acceleration methods-such as parallelization, objective- and data-driven modifications, and replay buffers-either incur diminishing returns, introduce bias, or overlook redundancy across iterations. We identify that rollouts from consecutive training epochs frequently share a large portion of overlapping segments, wasting computation. To address this, we propose SPEC-RL, a novel framework that integrates SPECulative decoding with the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative prefixes and extends them via a draft-and-verify mechanism, avoiding redundant generation while ensuring policy consistency. Experiments on diverse math reasoning and generalization benchmarks, including GSM8K, MATH-500, OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout time by 2-3x without compromising policy quality. As a purely rollout-stage enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g., PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large reasoning models. Our code is available at this https URL 

**Abstract (ZH)**: 大型语言模型（LLMs）越来越多地依赖可验证奖励的强化学习（RLVR）来引发可靠的过程推理。然而，训练过程仍然受限于计算昂贵的展开阶段。现有的加速方法——如并行化、基于目标和数据的修改以及回放缓冲区——要么收益递减，要么引入偏差，要么忽视迭代间的冗余。我们发现连续训练周期的展开段经常共享大量重叠的部分，浪费了计算资源。为此，我们提出了一种新颖的框架SPEC-RL，该框架将SPECulative解码与RL展开过程融合。SPEC-RL利用先前的轨迹段作为推测性的前缀，并通过草案和验证机制进行扩展，避免重复生成同时确保策略一致性。在包括GSM8K、MATH-500、OlympiadBench、MMLU-STEM等多样化的数学推理和泛化基准测试中，SPEC-RL在不牺牲策略质量的情况下将展开时间减少2-3倍。作为纯粹的展开阶段增强，SPEC-RL能够无缝集成主流算法（例如PPO、GRPO、DAPO），提供了一条扩展RLVR以应用于大型推理模型的通用且实用的道路。我们的代码可在下列链接获取。 

---
# Leave No Observation Behind: Real-time Correction for VLA Action Chunks 

**Title (ZH)**: 不留任何观测数据于身后: 实时修正VLA动作块 

**Authors**: Kohei Sendai, Maxime Alvarez, Tatsuya Matsushima, Yutaka Matsuo, Yusuke Iwasawa  

**Link**: [PDF](https://arxiv.org/pdf/2509.23224)  

**Abstract**: To improve efficiency and temporal coherence, Vision-Language-Action (VLA) models often predict action chunks; however, this action chunking harms reactivity under inference delay and long horizons. We introduce Asynchronous Action Chunk Correction (A2C2), which is a lightweight real-time chunk correction head that runs every control step and adds a time-aware correction to any off-the-shelf VLA's action chunk. The module combines the latest observation, the predicted action from VLA (base action), a positional feature that encodes the index of the base action within the chunk, and some features from the base policy, then outputs a per-step correction. This preserves the base model's competence while restoring closed-loop responsiveness. The approach requires no retraining of the base policy and is orthogonal to asynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic Kinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent success rate improvements across increasing delays and execution horizons (+23% point and +7% point respectively, compared to RTC), and also improves robustness for long horizons even with zero injected delay. Since the correction head is small and fast, there is minimal overhead compared to the inference of large VLA models. These results indicate that A2C2 is an effective, plug-in mechanism for deploying high-capacity chunking policies in real-time control. 

**Abstract (ZH)**: 异步动作块修正（A2C2）：一种轻量级实时动作块修正头 

---
# One-Shot Multi-Label Causal Discovery in High-Dimensional Event Sequences 

**Title (ZH)**: 一-shot多标签因果发现高维事件序列 

**Authors**: Hugo Math, Robin Schön, Rainer Lienhart  

**Link**: [PDF](https://arxiv.org/pdf/2509.23213)  

**Abstract**: Understanding causality in event sequences with thousands of sparse event types is critical in domains such as healthcare, cybersecurity, or vehicle diagnostics, yet current methods fail to scale. We present OSCAR, a one-shot causal autoregressive method that infers per-sequence Markov Boundaries using two pretrained Transformers as density estimators. This enables efficient, parallel causal discovery without costly global CI testing. On a real-world automotive dataset with 29,100 events and 474 labels, OSCAR recovers interpretable causal structures in minutes, while classical methods fail to scale, enabling practical scientific diagnostics at production scale. 

**Abstract (ZH)**: 理解含有数千种稀疏事件类型的事件序列因果关系在医疗保健、网络安全或车辆诊断等领域至关重要，但当前方法无法扩展。我们提出了OSCAR，一种基于两个预训练Transformer作为密度估计器的一次性因果自回归方法，该方法通过推断每个序列的马尔可夫边界，实现了高效的并行因果发现，而无需昂贵的全局CI测试。在包含29,100个事件和474个标签的现实世界汽车数据集中，OSCAR能够在几分钟内恢复可解释的因果结构，而经典方法无法扩展，从而在生产规模上实现了实用的科学诊断。 

---
# Towards Monotonic Improvement in In-Context Reinforcement Learning 

**Title (ZH)**: 向单调改进的在上下文强化学习方向探索 

**Authors**: Wenhao Zhang, Shao Zhang, Xihuai Wang, Yang Li, Ying Wen  

**Link**: [PDF](https://arxiv.org/pdf/2509.23209)  

**Abstract**: In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm for developing agents that can rapidly adapt to new tasks by leveraging past experiences as context, without updating their parameters. Recent approaches train large sequence models on monotonic policy improvement data from online RL, aiming to a continue improved testing time performance. However, our experimental analysis reveals a critical flaw: these models cannot show a continue improvement like the training data during testing time. Theoretically, we identify this phenomenon as Contextual Ambiguity, where the model's own stochastic actions can generate an interaction history that misleadingly resembles that of a sub-optimal policy from the training data, initiating a vicious cycle of poor action selection. To resolve the Contextual Ambiguity, we introduce Context Value into training phase and propose Context Value Informed ICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing the ideal performance theoretically achievable by a policy given the current context. As the context expands, Context Value could include more task-relevant information, and therefore the ideal performance should be non-decreasing. We prove that the Context Value tightens the lower bound on the performance gap relative to an ideal, monotonically improving policy. We fruther propose two methods for estimating Context Value at both training and testing time. Experiments conducted on the Dark Room and Minigrid testbeds demonstrate that CV-ICRL effectively mitigates performance degradation and improves overall ICRL abilities across various tasks and environments. The source code and data of this paper are available at this https URL . 

**Abstract (ZH)**: 基于上下文的强化学习（ICRL）：缓解上下文模糊性的方法 

---
# PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness 

**Title (ZH)**: PARL-MT：在多轮对话中具有进度意识的函数调用学习 

**Authors**: Huacan Chai, Zijie Cao, Maolin Ran, Yingxuan Yang, Jianghao Lin, pengxin, Hairui Wang, Renjie Ding, Ziyu Wan, Muning Wen, Weiwen Liu, Weinan Zhang, Fei Huang, Ying Wen  

**Link**: [PDF](https://arxiv.org/pdf/2509.23206)  

**Abstract**: Large language models (LLMs) have achieved impressive success in single-turn function calling, yet real-world applications such as travel planning or multi-stage data analysis typically unfold across multi-turn conversations. In these settings, LLMs must not only issue accurate function calls at each step but also maintain progress awareness, the ability to summarize past interactions and plan future actions to ensure coherent, long-horizon task execution. Existing approaches, however, either reduce multi-turn training to isolated single-turn samples, which neglects task-level planning, or employ end-to-end reinforcement learning (RL) that struggles with redundancy and lacks explicit integration of progress awareness. To overcome these limitations, we introduce PARL-MT, a framework that explicitly incorporates progress awareness into LLM training for multi-turn function calling. PARL-MT combines (i) a Progress Awareness Generation (PAG) pipeline, which automatically constructs datasets coupling conversation summaries with future task planning, and (ii) a Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which integrates progress awareness into RL training to reduce contextual redundancy and improve alignment between local actions and global task completion. Empirical results on two public benchmarks demonstrate that PARL-MT significantly outperforms existing methods, highlighting the effectiveness of progress awareness in enabling robust and efficient multi-turn function calling. 

**Abstract (ZH)**: PARL-MT：一种结合进度意识的多轮函数调用框架 

---
# WARBERT: A Hierarchical BERT-based Model for Web API Recommendation 

**Title (ZH)**: WARBERT：一种基于层级BERT的Web API推荐模型 

**Authors**: Zishuo Xu, Yuhong Gu, Dezhong Yao  

**Link**: [PDF](https://arxiv.org/pdf/2509.23175)  

**Abstract**: With the emergence of Web 2.0 and microservices architecture, the number of Web APIs has increased dramatically, further intensifying the demand for efficient Web API recommendation. Existing solutions typically fall into two categories: recommendation-type methods, which treat each API as a label for classification, and match-type methods, which focus on matching mashups through API retrieval. However, three critical challenges persist: 1) the semantic ambiguities in comparing API and mashup descriptions, 2) the lack of detailed comparisons between the individual API and the mashup in recommendation-type methods, and 3) time inefficiencies for API retrieval in match-type methods. To address these challenges, we propose WARBERT, a hierarchical BERT-based model for Web API recommendation. WARBERT leverages dual-component feature fusion and attention comparison to extract precise semantic representations of API and mashup descriptions. WARBERT consists of two main components: WARBERT(R) for Recommendation and WARBERT(M) for Matching. Specifically, WAR-BERT(R) serves as an initial filter, narrowing down the candidate APIs, while WARBERT(M) refines the matching process by calculating the similarity between candidate APIs and mashup. The final likelihood of a mashup being matched with an API is determined by combining the predictions from WARBERT(R) and WARBERT(M). Additionally, WARBERT(R) incorporates an auxiliary task of mashup category judgment, which enhances its effectiveness in candidate selection. Experimental results on the ProgrammableWeb dataset demonstrate that WARBERT outperforms most existing solutions and achieves improvements of up to 11.7% compared to the model MTFM (Multi-Task Fusion Model), delivering significant enhancements in accuracy and effiency. 

**Abstract (ZH)**: 基于BERT的层次模型WARBERT面向Web API推荐 

---
# TRAX: TRacking Axles for Accurate Axle Count Estimation 

**Title (ZH)**: TRAX: 轴跟踪以实现精确轴数估计 

**Authors**: Avinash Rai, Sandeep Jana, Vishal Vijay  

**Link**: [PDF](https://arxiv.org/pdf/2509.23171)  

**Abstract**: Accurate counting of vehicle axles is essential for traffic control, toll collection, and infrastructure development. We present an end-to-end, video-based pipeline for axle counting that tackles limitations of previous works in dense environments. Our system leverages a combination of YOLO-OBB to detect and categorize vehicles, and YOLO to detect tires. Detected tires are intelligently associated to their respective parent vehicles, enabling accurate axle prediction even in complex scenarios. However, there are a few challenges in detection when it comes to scenarios with longer and occluded vehicles. We mitigate vehicular occlusions and partial detections for longer vehicles by proposing a novel TRAX (Tire and Axle Tracking) Algorithm to successfully track axle-related features between frames. Our method stands out by significantly reducing false positives and improving the accuracy of axle-counting for long vehicles, demonstrating strong robustness in real-world traffic videos. This work represents a significant step toward scalable, AI-driven axle counting systems, paving the way for machine vision to replace legacy roadside infrastructure. 

**Abstract (ZH)**: 基于视频的端到端车辆轴数精确计数pipeline及其应用 

---
# Dense associative memory on the Bures-Wasserstein space 

**Title (ZH)**: Bures-Wasserstein空间中的密集关联记忆 

**Authors**: Chandan Tankala, Krishnakumar Balasubramanian  

**Link**: [PDF](https://arxiv.org/pdf/2509.23162)  

**Abstract**: Dense associative memories (DAMs) store and retrieve patterns via energy-functional fixed points, but existing models are limited to vector representations. We extend DAMs to probability distributions equipped with the 2-Wasserstein distance, focusing mainly on the Bures-Wasserstein class of Gaussian densities. Our framework defines a log-sum-exp energy over stored distributions and a retrieval dynamics aggregating optimal transport maps in a Gibbs-weighted manner. Stationary points correspond to self-consistent Wasserstein barycenters, generalizing classical DAM fixed points. We prove exponential storage capacity, provide quantitative retrieval guarantees under Wasserstein perturbations, and validate the model on synthetic and real-world distributional tasks. This work elevates associative memory from vectors to full distributions, bridging classical DAMs with modern generative modeling and enabling distributional storage and retrieval in memory-augmented learning. 

**Abstract (ZH)**: 稠密关联记忆（DAMs）通过能量函数的固定点存储和检索模式，但现有模型仅限于向量表示。我们扩展了DAMs到配备2- Wasserstein距离的概率分布，重点关注Bures-Wasserstein类高斯密度。我们的框架定义了存储分布上的对数和最大值能量，并通过吉布斯加权方式聚合并检索最优运输映射。稳定点对应于自洽的Wasserstein平均中心，推广了经典的DAM固定点。我们证明了指数级的存储容量，在Wasserstein扰动下提供了定量的检索保证，并在合成和实际分布任务上验证了该模型。这项工作将关联记忆从向量提升到完整的概率分布，将经典的DAM与现代生成建模相结合，使记忆增强学习中的分布存储和检索成为可能。 

---
# Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization 

**Title (ZH)**: 基于深度学习的基于被动智能手机 sensing 的认知障碍检测：带有活动感知增强和个人化demographic参数方法 

**Authors**: Yufei Shen, Ji Hwan Park, Minchao Huang, Jared F. Benge, Justin F. Rousseau, Rosemary A. Lester-Smith, Edison Thomaz  

**Link**: [PDF](https://arxiv.org/pdf/2509.23158)  

**Abstract**: Early detection of cognitive impairment is critical for timely diagnosis and intervention, yet infrequent clinical assessments often lack the sensitivity and temporal resolution to capture subtle cognitive declines in older adults. Passive smartphone sensing has emerged as a promising approach for naturalistic and continuous cognitive monitoring. Building on this potential, we implemented a Long Short-Term Memory (LSTM) model to detect cognitive impairment from sequences of daily behavioral features, derived from multimodal sensing data collected in an ongoing one-year study of older adults. Our key contributions are two techniques to enhance model generalizability across participants: (1) routine-aware augmentation, which generates synthetic sequences by replacing each day with behaviorally similar alternatives, and (2) demographic personalization, which reweights training samples to emphasize those from individuals demographically similar to the test participant. Evaluated on 6-month data from 36 older adults, these techniques jointly improved the Area Under the Precision-Recall Curve (AUPRC) of the model trained on sensing and demographic features from 0.637 to 0.766, highlighting the potential of scalable monitoring of cognitive impairment in aging populations with passive sensing. 

**Abstract (ZH)**: 早期认知损害的检测对于及时诊断和干预至关重要，但频繁不足的临床评估往往缺乏敏感性和时序分辨率来捕捉老年人的细微认知下降。被动智能手机传感已成为自然且持续认知监测的有前途的方法。基于这一潜力，我们实现了一个长短期记忆（LSTM）模型，用于从持续一年的老年人群多模态传感数据中提取的每日行为特征序列检测认知损害。我们的主要贡献是两种增强模型泛化性的技术：（1）基于日常习惯的增强，通过用行为相似的替代日来替换每一天生成合成序列，以及（2）基于人口特征个性化，通过对类似于测试参与者的人群的样本重新加权来强调其重要性。在36名老年人6个月的数据上进行评估，这两种技术共同将使用传感和人口特征训练的模型的精准召回曲线下面积（AUPRC）从0.637提高到0.766，突显了使用被动传感进行认知损害的可扩展监测在老龄化人群中的潜力。 

---
# Trust Region Reward Optimization and Proximal Inverse Reward Optimization Algorithm 

**Title (ZH)**: 信赖区域奖励优化与近端逆奖励优化算法 

**Authors**: Yang Chen, Menglin Zou, Jiaqi Zhang, Yitan Zhang, Junyi Yang, Gael Gendron, Libo Zhang, Jiamou Liu, Michael J. Witbrock  

**Link**: [PDF](https://arxiv.org/pdf/2509.23135)  

**Abstract**: Inverse Reinforcement Learning (IRL) learns a reward function to explain expert demonstrations. Modern IRL methods often use the adversarial (minimax) formulation that alternates between reward and policy optimization, which often lead to unstable training. Recent non-adversarial IRL approaches improve stability by jointly learning reward and policy via energy-based formulations but lack formal guarantees. This work bridges this gap. We first present a unified view showing canonical non-adversarial methods explicitly or implicitly maximize the likelihood of expert behavior, which is equivalent to minimizing the expected return gap. This insight leads to our main contribution: Trust Region Reward Optimization (TRRO), a framework that guarantees monotonic improvement in this likelihood via a Minorization-Maximization process. We instantiate TRRO into Proximal Inverse Reward Optimization (PIRO), a practical and stable IRL algorithm. Theoretically, TRRO provides the IRL counterpart to the stability guarantees of Trust Region Policy Optimization (TRPO) in forward RL. Empirically, PIRO matches or surpasses state-of-the-art baselines in reward recovery, policy imitation with high sample efficiency on MuJoCo and Gym-Robotics benchmarks and a real-world animal behavior modeling task. 

**Abstract (ZH)**: 逆强化学习中的信任区域奖励优化（TRRO） 

---
# C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning 

**Title (ZH)**: C$^2$GSPG: 信心校准的群体序列策略梯度 toward 自我意识推理 

**Authors**: Haotian Liu, Shuo Wang, Hongteng Xu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23129)  

**Abstract**: Reinforcement Learning (RL) methods, exemplified by Group Relative Policy Optimization (GRPO) and its variants, play a central role in developing reasoning models. However, these methods often suffer from a critical overconfidence issue, which prevents them from achieving self-aware reasoning models. In this study, we propose a simple yet effective confidence-calibration group sequence policy gradient method, called C$^2$GSPG, which simultaneously enhances reasoning performance while suppressing overconfidence. In principle, we propose a Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO and its variants. In this framework, we define the model confidence for each reasoning problem using the normalized sequence-level probability, and then apply a cross-entropy regularizer to calibrate the model confidence to the sequence's reward. We demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction. For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict between the two objectives. Applying C$^2$GSPG to post-train large language models in logical and mathematical reasoning tasks, we show its superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration. The code of C$^2$GSPG is available at this https URL. 

**Abstract (ZH)**: 基于信心校准的组序列策略梯度方法（C$^2$GSPG）及其在强化学习中的应用 

---
# RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility 

**Title (ZH)**: RHYTHM: 基于层次时间切分的理性推理在人类移动性分析中的应用 

**Authors**: Haoyu He, Haozheng Luo, Yan Chen, Qi R. Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23115)  

**Abstract**: Predicting human mobility is inherently challenging due to complex long-range dependencies and multi-scale periodic behaviors. To address this, we introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a unified framework that leverages large language models (LLMs) as general-purpose spatio-temporal predictors and trajectory reasoners. Methodologically, RHYTHM employs temporal tokenization to partition each trajectory into daily segments and encode them as discrete tokens with hierarchical attention that captures both daily and weekly dependencies, thereby significantly reducing the sequence length while preserving cyclical information. Additionally, we enrich token representations by adding pre-computed prompt embeddings for trajectory segments and prediction targets via a frozen LLM, and feeding these combined embeddings back into the LLM backbone to capture complex interdependencies. Computationally, RHYTHM freezes the pretrained LLM's backbone to reduce attention complexity and memory cost. We evaluate our model against state-of-the-art methods using three real-world datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a 5.0% increase on weekends, and a 24.6% reduction in training time. Code is publicly available at this https URL. 

**Abstract (ZH)**: 基于层次时间分词的人类移动预测模型RHYTHM 

---
# Liaohe-CobotMagic-PnP: an Imitation Learning Dataset of Intelligent Robot for Industrial Applications 

**Title (ZH)**: 辽河-CobotMagic-PnP：面向工业应用的智能机器人模仿学习数据集 

**Authors**: Chen Yizhe, Wang Qi, Hu Dongxiao, Jingzhe Fang, Liu Sichao, Zixin An, Hongliang Niu, Haoran Liu, Li Dong, Chuanfen Feng, Lan Dapeng, Liu Yu, Zhibo Pang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23111)  

**Abstract**: In Industry 4.0 applications, dynamic environmental interference induces highly nonlinear and strongly coupled interactions between the environmental state and robotic behavior. Effectively representing dynamic environmental states through multimodal sensor data fusion remains a critical challenge in current robotic datasets. To address this, an industrial-grade multimodal interference dataset is presented, designed for robotic perception and control under complex conditions. The dataset integrates multi-dimensional interference features including size, color, and lighting variations, and employs high-precision sensors to synchronously collect visual, torque, and joint-state measurements. Scenarios with geometric similarity exceeding 85\% and standardized lighting gradients are included to ensure real-world representativeness. Microsecond-level time-synchronization and vibration-resistant data acquisition protocols, implemented via the Robot Operating System (ROS), guarantee temporal and operational fidelity. Experimental results demonstrate that the dataset enhances model validation robustness and improves robotic operational stability in dynamic, interference-rich environments. The dataset is publicly available at:this https URL. 

**Abstract (ZH)**: 在工业4.0应用中，动态环境干扰导致环境状态与机器人行为之间呈现高度非线性的强耦合交互。通过多模态传感器数据融合有效地表示动态环境状态仍然是当前机器人数据集中的一项关键挑战。为此，提出了一种工业级别的多模态干扰数据集，旨在在复杂条件下增强机器人的感知与控制能力。该数据集整合了尺寸、颜色和光照等多维干扰特征，并采用高精度传感器同步采集视觉、扭矩和关节状态数据。包含几何相似度超过85%且标准化光照梯度的场景，以确保真实场景的代表性和适用性。通过机器人操作系统（ROS）实现微秒级时间同步和抗振动数据采集协议，保证了时间和操作上的准确性。实验结果表明，该数据集增强了模型验证的稳健性，并提高了机器人在动态、干扰丰富的环境中的操作稳定性。该数据集已公开发布于此：this https URL。 

---
# Open-Vocabulary Spatio-Temporal Scene Graph for Robot Perception and Teleoperation Planning 

**Title (ZH)**: 具有开放词汇量的空间-时间场景图在机器人感知与远程操作规划中的应用 

**Authors**: Yi Wang, Zeyu Xue, Mujie Liu, Tongqin Zhang, Yan Hu, Zhou Zhao, Chenguang Yang, Zhenyu Lu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23107)  

**Abstract**: Teleoperation via natural-language reduces operator workload and enhances safety in high-risk or remote settings. However, in dynamic remote scenes, transmission latency during bidirectional communication creates gaps between remote perceived states and operator intent, leading to command misunderstanding and incorrect execution. To mitigate this, we introduce the Spatio-Temporal Open-Vocabulary Scene Graph (ST-OVSG), a representation that enriches open-vocabulary perception with temporal dynamics and lightweight latency annotations. ST-OVSG leverages LVLMs to construct open-vocabulary 3D object representations, and extends them into the temporal domain via Hungarian assignment with our temporal matching cost, yielding a unified spatio-temporal scene graph. A latency tag is embedded to enable LVLM planners to retrospectively query past scene states, thereby resolving local-remote state mismatches caused by transmission delays. To further reduce redundancy and highlight task-relevant cues, we propose a task-oriented subgraph filtering strategy that produces compact inputs for the planner. ST-OVSG generalizes to novel categories and enhances planning robustness against transmission latency without requiring fine-tuning. Experiments show that our method achieves 74 percent node accuracy on the Replica benchmark, outperforming ConceptGraph. Notably, in the latency-robustness experiment, the LVLM planner assisted by ST-OVSG achieved a planning success rate of 70.5 percent. 

**Abstract (ZH)**: 自然语言介导的遥控减少操作员工作负荷并提升高风险或远程环境中的安全性。然而，在动态远程场景中，双向通信过程中的传输延迟会导致远程感知状态与操作员意图之间的差距，进而导致指令误解和错误执行。为缓解这一问题，我们引入了时空开放词汇场景图（ST-OVSG），这是一种能够通过引入时间动态和轻量级延迟注释来丰富开放词汇感知的表现形式。ST-OVSG 利用低级视觉语言模型（LVLM）构建开放词汇的3D对象表示，并通过我们的时间匹配成本和匈牙利指派将其扩展到时间领域，从而生成统一的时空场景图。嵌入了一个延迟标签，使LVLM规划器能够回顾性地查询过去的状态，从而解决由传输延迟引起的局部-远程状态不匹配问题。为了进一步减少冗余并突出任务相关线索，我们提出了一种任务导向的子图过滤策略，以生成更适合规划器的紧凑输入。ST-OVSG 具备泛化到新类别并增强对传输延迟的规划鲁棒性而不需微调的能力。实验结果显示，我们的方法在Replica基准测试中实现了74%的节点准确性，超越了ConceptGraph。值得注意的是，在传输延迟稳健性实验中，辅以ST-OVSG的LVLM规划器的规划成功率为70.5%。 

---
# HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing 

**Title (ZH)**: HTMA-Net：通过哈达玛变换和内存计算实现的避免乘法的神经网络 

**Authors**: Emadeldeen Hamdan, Ahmet Enis Cetin  

**Link**: [PDF](https://arxiv.org/pdf/2509.23103)  

**Abstract**: Reducing the cost of multiplications is critical for efficient deep neural network deployment, especially in energy-constrained edge devices. In this work, we introduce HTMA-Net, a novel framework that integrates the Hadamard Transform (HT) with multiplication-avoiding (MA) SRAM-based in-memory computing to reduce arithmetic complexity while maintaining accuracy. Unlike prior methods that only target multiplications in convolutional layers or focus solely on in-memory acceleration, HTMA-Net selectively replaces intermediate convolutions with Hybrid Hadamard-based transform layers whose internal convolutions are implemented via multiplication-avoiding in-memory operations. We evaluate HTMA-Net on ResNet-18 using CIFAR-10, CIFAR-100, and Tiny ImageNet, and provide a detailed comparison against regular, MF-only, and HT-only variants. Results show that HTMA-Net eliminates up to 52\% of multiplications compared to baseline ResNet-18, ResNet-20, and ResNet-50 models, while achieving comparable accuracy in evaluation and significantly reducing computational complexity and the number of parameters. Our results demonstrate that combining structured Hadamard transform layers with SRAM-based in-memory computing multiplication-avoiding operators is a promising path towards efficient deep learning architectures. 

**Abstract (ZH)**: 降低乘法成本对于高效部署深度神经网络至关重要，尤其是在能量受限的边缘设备上。本文提出HTMA-Net，这是一种将哈达玛变换（HT）与避免乘法（MA）的SRAM基内存计算相结合的新框架，以减少算术复杂度同时保持精度。 

---
# Towards Quantum-Ready Blockchain Fraud Detection via Ensemble Graph Neural Networks 

**Title (ZH)**: 面向量子计算的区块链欺诈检测 ensemble 图神经网络方法 

**Authors**: M.Z. Haider, Tayyaba Noreen, M. Salman  

**Link**: [PDF](https://arxiv.org/pdf/2509.23101)  

**Abstract**: Blockchain Business applications and cryptocurrencies such as enable secure, decentralized value transfer, yet their pseudonymous nature creates opportunities for illicit activity, challenging regulators and exchanges in anti money laundering (AML) enforcement. Detecting fraudulent transactions in blockchain networks requires models that can capture both structural and temporal dependencies while remaining resilient to noise, imbalance, and adversarial behavior. In this work, we propose an ensemble framework that integrates Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and Graph Isomorphism Networks (GIN) to enhance blockchain fraud detection. Using the real-world Elliptic dataset, our tuned soft voting ensemble achieves high recall of illicit transactions while maintaining a false positive rate below 1%, beating individual GNN models and baseline methods. The modular architecture incorporates quantum-ready design hooks, allowing seamless future integration of quantum feature mappings and hybrid quantum classical graph neural networks. This ensures scalability, robustness, and long-term adaptability as quantum computing technologies mature. Our findings highlight ensemble GNNs as a practical and forward-looking solution for real-time cryptocurrency monitoring, providing both immediate AML utility and a pathway toward quantum-enhanced financial security analytics. 

**Abstract (ZH)**: 区块链业务应用与加密货币 enables 安全的、去中心化的价值传输，但其假名性质为非法活动创造了机会，挑战着监管机构和交易所的反洗钱（AML）执法工作。检测区块链网络中的虚假交易需要能够捕捉结构和时间依赖性同时对抗噪声、不平衡和恶意行为的模型。在此项工作中，我们提出了一种集成框架，该框架结合了图卷积网络（GCN）、图注意网络（GAT）和图同构网络（GIN），以增强区块链欺诈检测。通过使用实际世界中的Elliptic数据集，我们调优后的软投票集成方法在保持假阳性率低于1%的前提下实现了对非法交易的高召回率，超越了单独的图神经网络模型和基线方法。该模块化架构集成了量子就绪的设计钩子，允许无缝地将量子特征映射和混合量子经典图神经网络集成进来。这确保了随着量子计算技术的发展，系统的可扩展性、鲁棒性和长期适应性。我们的研究结果强调了集成GNNs作为一种实用且前瞻性的解决方案，适用于实时加密货币监控，提供了即时的反洗钱（AML）实用性和通往量子增强的金融安全分析的途径。 

---
# CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP 

**Title (ZH)**: CoPatch: 通过利用CLIP中未开发的空间知识实现零样本描述图片分割 

**Authors**: Na Min An, Inha Kang, Minhyun Lee, Hyunjung Shim  

**Link**: [PDF](https://arxiv.org/pdf/2509.23098)  

**Abstract**: Spatial grounding is crucial for referring image segmentation (RIS), where the goal of the task is to localize an object described by language. Current foundational vision-language models (VLMs), such as CLIP, excel at aligning images and text but struggle with understanding spatial relationships. Within the language stream, most existing methods often focus on the primary noun phrase when extracting local text features, undermining contextual tokens. Within the vision stream, CLIP generates similar features for images with different spatial layouts, resulting in limited sensitivity to spatial structure. To address these limitations, we propose \textsc{CoPatch}, a zero-shot RIS framework that leverages internal model components to enhance spatial representations in both text and image modalities. For language, \textsc{CoPatch} constructs hybrid text features by incorporating context tokens carrying spatial cues. For vision, it extracts patch-level image features using our novel path discovered from intermediate layers, where spatial structure is better preserved. These enhanced features are fused into a clustered image-text similarity map, \texttt{CoMap}, enabling precise mask selection. As a result, \textsc{CoPatch} significantly improves spatial grounding in zero-shot RIS across RefCOCO, RefCOCO+, RefCOCOg, and PhraseCut (+ 2--7 mIoU) without requiring any additional training. Our findings underscore the importance of recovering and leveraging the untapped spatial knowledge inherently embedded in VLMs, thereby paving the way for opportunities in zero-shot RIS. 

**Abstract (ZH)**: 空间接地是引用图像分割（RIS）的关键任务，其中目标是定位由语言描述的物体。现有的基础视觉-语言模型（VLMs），如CLIP，擅长图像和文本的对齐，但在理解空间关系方面存在困难。在语言流中，现有方法大多在提取局部文本特征时关注主要名词短语，忽视了上下文词汇。在视觉流中，CLIP对具有不同空间布局的图像生成相似的特征，导致对空间结构的敏感性有限。为解决这些局限性，我们提出了\textsc{CoPatch}，这是一种零样本RIS框架，利用模型内部组件增强文本和图像模态中的空间表示。对于语言，\textsc{CoPatch}通过结合携带空间线索的上下文词汇构建混合文本特征。对于视觉，它使用从中间层发现的新型路径提取补丁级图像特征，其中空间结构得到更好地保持。这些增强的特征被融合进集群图像-文本相似性图\texttt{CoMap}中，可以实现精确的掩码选择。结果，\textsc{CoPatch}在零样本RIS中显著改善了空间接地（在RefCOCO、RefCOCO+、RefCOCOg和PhraseCut上分别提高2-7个mIoU点）而不需额外训练。我们的研究结果强调了恢复和利用VLMs中固有嵌入的未充分利用的空间知识的重要性，从而为零样本RIS提供了新的机遇。 

---
# Causally-Enhanced Reinforcement Policy Optimization 

**Title (ZH)**: 因果增强强化策略优化 

**Authors**: Xiangqi Wang, Yue Huang, Yujun Zhou, Xiaonan Luo, Kehan Guo, Xiangliang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23095)  

**Abstract**: Large language models (LLMs) trained with reinforcement objectives often achieve superficially correct answers via shortcut strategies, pairing correct outputs with spurious or unfaithful reasoning and degrading under small causal perturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a drop-in reward-shaping framework that augments policy optimization with a differentiable proxy for causal coherence along the generation pathway from prompt (Z) to rationale (X) to answer (Y). CE-PO estimates model-internal influence with Jacobian-based sensitivities, counterfactually hardens these signals to suppress nuisance cues, and fuses the resulting coherence score with task-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single tunable between accuracy and coherence trade-off. The unified reward integrates with PPO/GRPO without architectural changes. Across reasoning benchmarks and causal stress tests, CE-PO reduces reward hacking and unfaithful chain-of-thought while improving robustness to correlation-causation flips and light counterfactual edits, all at near-parity accuracy. Experimental results across 4 datasets show that CE-PO improves accuracy over baselines by 5.49% on average (up to 9.58%), while improving robustness to correlation-causation flips and light counterfactual edits. 

**Abstract (ZH)**: 因果增强策略优化（CE-PO）：一种插入式的奖励塑造框架 

---
# Signal Preserving Weight Initialization for Odd-Sigmoid Activations 

**Title (ZH)**: Odd-Sigmoid 激活函数的信号保� skincare 重初始化 

**Authors**: Hyunwoo Lee, Hayoung Choi, Hyunju Kim  

**Link**: [PDF](https://arxiv.org/pdf/2509.23085)  

**Abstract**: Activation functions critically influence trainability and expressivity, and recent work has therefore explored a broad range of nonlinearities. However, activations and weight initialization are interdependent: without an appropriate initialization method, nonlinearities can cause saturation, variance collapse, and increased learning rate sensitivity. We address this by defining an odd sigmoid function class and, given any activation f in this class, proposing an initialization method tailored to f. The method selects a noise scale in closed form so that forward activations remain well dispersed up to a target layer, thereby avoiding collapse to zero or saturation. Empirically, the approach trains reliably without normalization layers, exhibits strong data efficiency, and enables learning for activations under which standard initialization methods (Xavier, He, Orthogonal) often do not converge reliably. 

**Abstract (ZH)**: 激活函数对可训练性和表征能力至关重要，近期研究因此探索了广泛的一系列非线性函数。然而，激活函数和权重初始化相互依赖：没有合适的方法，非线性函数可能导致饱和、方差消失和学习率敏感性增加。为此，我们定义了一个奇数sigmoid函数类，并为该类中的任一激活函数提出了一种定制化的初始化方法。该方法以封闭形式选择噪声尺度，确保前向激活在目标层之前保持良好的分散，从而避免归零或饱和。实证研究表明，该方法在不使用归一化层的情况下能够可靠地训练，表现出强大的数据效率，并且能够在标准初始化方法（Xavier、He、正交）通常无法可靠收敛的情况下学习激活函数。 

---
# Beyond Model Ranking: Predictability-Aligned Evaluation for Time Series Forecasting 

**Title (ZH)**: 超越模型排名：时间序列预测的可预测性对齐评估 

**Authors**: Wanjin Feng, Yuan Yuan, Jingtao Ding, Yong Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.23074)  

**Abstract**: In the era of increasingly complex AI models for time series forecasting, progress is often measured by marginal improvements on benchmark leaderboards. However, this approach suffers from a fundamental flaw: standard evaluation metrics conflate a model's performance with the data's intrinsic unpredictability. To address this pressing challenge, we introduce a novel, predictability-aligned diagnostic framework grounded in spectral coherence. Our framework makes two primary contributions: the Spectral Coherence Predictability (SCP), a computationally efficient ($O(N\log N)$) and task-aligned score that quantifies the inherent difficulty of a given forecasting instance, and the Linear Utilization Ratio (LUR), a frequency-resolved diagnostic tool that precisely measures how effectively a model exploits the linearly predictable information within the data. We validate our framework's effectiveness and leverage it to reveal two core insights. First, we provide the first systematic evidence of "predictability drift", demonstrating that a task's forecasting difficulty varies sharply over time. Second, our evaluation reveals a key architectural trade-off: complex models are superior for low-predictability data, whereas linear models are highly effective on more predictable tasks. We advocate for a paradigm shift, moving beyond simplistic aggregate scores toward a more insightful, predictability-aware evaluation that fosters fairer model comparisons and a deeper understanding of model behavior. 

**Abstract (ZH)**: 在时间序列预测中日益复杂的AI模型时代，进展通常通过基准排行榜上的边际改进来衡量。然而，这种方法存在根本性缺陷：标准评估指标将模型性能与数据的固有不可预测性混淆。为解决这一紧迫挑战，我们引入了一个基于频谱相干性的新颖可预测性对齐诊断框架。该框架的两大主要贡献是：频谱相干性可预测性（SCP），一种计算效率高（$O(N\log N)$）且任务对齐的评分，量化给定预测实例的固有难度；以及线性利用率比（LUR），一种频率解析诊断工具，精确测量模型如何有效利用数据中的线性可预测信息。我们验证了该框架的有效性，并利用它揭示了两个核心见解：首先，我们首次系统地证明了“可预测性漂移”，显示任务的预测难度随时间显著变化；其次，我们的评估揭示了一个关键的架构权衡：复杂模型适用于低可预测性数据，而线性模型在更具可预测性的任务上表现更佳。我们倡导 paradigm shift，转向一种更加透彻、可预测性感知的评估方法，以促进更公平的模型比较和更深入的模型行为理解。 

---
# From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents 

**Title (ZH)**: 从证据到轨迹：用于训练检索增强生成代理的溯因推理路径合成 

**Authors**: Muzhi Li, Jinhu Qi, Yihong Wu, Minghao Zhao, Liheng Ma, Yifan Li, Xinyu Wang, Yingxue Zhang, Ho-fung Leung, Irwin King  

**Link**: [PDF](https://arxiv.org/pdf/2509.23071)  

**Abstract**: Retrieval-augmented generation agents development is hindered by the lack of process-level supervision to effectively guide agentic capabilities like task decomposition, retriever invocation, and stepwise decision-making. While reinforcement learning offers a potential solution, it suffers from sparse rewards and the limited reasoning capabilities of large language models (LLMs). Meanwhile, existing data synthesis methods only produce chain-of-thought rationales and fail to model environmental interactions. In this paper, we propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG agent development. EviPath comprises: (i) Abductive Subtask Planning, which decomposes the problem into sub-questions and iteratively plans an optimal solution path based on the dependencies between them; (ii) Faithful Sub-question Answering, which uses supporting evidence to construct a proxy environment to generate reasoning thoughts and answers for each sub-question; and (iii) Conversational Fine-Tuning, which formats the complete agent-environment interaction trajectory into a dialogue format suitable for Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and tool-use capabilities directly from synthesized data. Extensive experiments on widely-used question-answering benchmarks show that an 8B parameter model trained with EviPath-synthesized data significantly and consistently outperforms state-of-the-art baselines with a double-digit absolute EM gain of 14.7% in open-domain question answering. 

**Abstract (ZH)**: 基于证据的推理路径合成范式：用于RAG代理开发的EviPath 

---
# Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks 

**Title (ZH)**: 语义投票：一种无需自我评估的高效LLM自我改进方法，用于不可验证的开放式任务 

**Authors**: Chunyang Jiang, Yonggang Zhang, Yiyang Cai, Chi-Min Chan, Yulong Liu, Mingming Chen, Wei Xue, Yike Guo  

**Link**: [PDF](https://arxiv.org/pdf/2509.23067)  

**Abstract**: The rising cost of acquiring supervised data has driven significant interest in self-improvement for large language models (LLMs). Straightforward unsupervised signals like majority voting have proven effective in generating pseudo-labels for verifiable tasks, while their applicability to unverifiable tasks (e.g., translation) is limited by the open-ended character of responses. As a result, self-evaluation mechanisms (e.g., self-judging and entropy minimization) are predominantly used to derive pseudo-labels. However, self-evaluation relying on LLMs typically incurs high computational overhead and introduces overconfidence issues due to intrinsic biases. To address these challenges, we propose a novel self-evaluation-free approach for unverifiable tasks, designed for lightweight yet effective self-improvement. Inspired by majority voting commonly employed in verifiable tasks, we propose semantic voting as a novel mechanism that relaxes the principle of hard matching (i.e., exact matching) toward soft matching (i.e., semantic similarity). Soft matching is achieved by leveraging a lightweight sentence embedding model to quantify semantic similarity, thereby mitigating excessive computational burden and intrinsic bias-associated limitations of self-evaluation. Comprehensive experiments demonstrate that our method achieves substantial gains in computational efficiency and overall better performance than self-evaluation methods across diverse model architectures and tasks. 

**Abstract (ZH)**: 大型语言模型不可验证任务的自提升新方法：基于语义投票的无自我评估机制 

---
# Local Success Does Not Compose: Benchmarking Large Language Models for Compositional Formal Verification 

**Title (ZH)**: 局部成功并不组成整体：大规模语言模型在组合形式验证中的基准测试 

**Authors**: Xu Xu, Xin Li, Xingwei Qu, Jie Fu, Binhang Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2509.23061)  

**Abstract**: We introduce DafnyCOMP, a benchmark for evaluating large language models (LLMs) on compositional specification generation in Dafny. Unlike prior benchmarks that focus on single-function tasks, DafnyCOMP targets programs composed of multiple interacting functions with data dependencies, requiring reasoning across component boundaries. The benchmark consists of 300 automatically synthesized multi-function programs. We evaluate several state-of-the-art LLM families and find that, while they perform well on single-function verification, their performance drops sharply on compositional tasks. Analysis reveals systematic failures in cross-functional reasoning, including fragile specifications, misalignment between implementations and proofs, and unstable reasoning. DafnyCOMP thus provides a diagnostic tool for measuring progress toward reliable, verifiable, and compositional code generation with LLMs. 

**Abstract (ZH)**: DafnyCOMP：一种评估大规模语言模型在Dafny中生成组合规范能力的基准测试 

---
# Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding 

**Title (ZH)**: LVLMs中的语言先验理解对比嵌入链的研究 

**Authors**: Lin Long, Changdae Oh, Seongheon Park, Yixuan Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.23050)  

**Abstract**: Large vision-language models (LVLMs) achieve strong performance on multimodal tasks, yet they often default to their language prior (LP) -- memorized textual patterns from pre-training while under-utilizing visual evidence. Prior analyses of LP mostly rely on input-output probing, which fails to reveal the internal mechanisms governing when and how vision influences model behavior. To address this gap, we present the first systematic analysis of language prior through the lens of chain-of-embedding, which examines the layer-wise representation dynamics within LVLMs. Our analysis reveals a universal phenomenon: each model exhibits a Visual Integration Point (VIP), a critical layer at which visual information begins to meaningfully reshape hidden representations and influence decoding. Building on this observation, we introduce the Total Visual Integration (TVI) estimator, which aggregates representation distance beyond the VIP to quantify how strongly visual query influences response generation. Across 54 model-dataset combinations spanning 9 contemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently emerges, and that TVI reliably predicts the strength of language prior. This offers a principled toolkit for diagnosing and understanding language prior in LVLMs. 

**Abstract (ZH)**: 大型多模态语言模型（LVLMs）在多模态任务中表现出强劲性能，但往往倾向于依赖其语言先验（LP）——预训练中记忆的文本模式，而未能充分利用视觉证据。对语言先验的先前分析主要依赖于输入-输出探测，这未能揭示视觉如何影响模型行为的内部机制。为填补这一空白，我们首先从嵌入链的角度对语言先验进行系统分析，该方法考察了大型多模态语言模型（LVLMs）逐层的表示动态。我们的分析揭示了一个普遍现象：每个模型都存在一个视觉整合点（VIP），这是一个关键层，在此层，视觉信息开始实质性地重塑隐藏表示，并影响解码过程。基于这一观察，我们引入了总视觉整合（TVI）估计器，该估计器衡量视觉查询如何强烈地影响响应生成，并聚集VIP之后的表示距离。在54种不同的模型-数据集组合中，包含9种现代大型多模态语言模型和6个基准测试，我们证明VIP始终出现，并且TVI可靠地预测了语言先验的强度。这为诊断和理解大型多模态语言模型中的语言先验提供了一个原则性的工具包。 

---
# Beyond Aggregation: Guiding Clients in Heterogeneous Federated Learning 

**Title (ZH)**: 超越聚合：引导客户端的异构联邦学习 

**Authors**: Zijian Wang, Xiaofei Zhang, Xin Zhang, Yukun Liu, Qiong Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23049)  

**Abstract**: Federated learning (FL) is increasingly adopted in domains like healthcare, where data privacy is paramount. A fundamental challenge in these systems is statistical heterogeneity-the fact that data distributions vary significantly across clients (e.g., different hospitals may treat distinct patient demographics). While current FL algorithms focus on aggregating model updates from these heterogeneous clients, the potential of the central server remains under-explored. This paper is motivated by a healthcare scenario: could a central server not only build a model but also guide a new patient to the hospital best equipped for their specific condition? We generalize this idea to propose a novel paradigm for FL systems where the server actively guides the allocation of new tasks or queries to the most appropriate client in the network. To enable this, we introduce an empirical likelihood-based framework that simultaneously addresses two goals: (1) learning effective local models on each client, and (2) finding the best matching client for a new query. Empirical results demonstrate the framework's effectiveness on benchmark datasets, showing improvements in both model accuracy and the precision of client guidance compared to standard FL approaches. This work opens a new direction for building more intelligent and resource-efficient federated systems that leverage heterogeneity as a feature, not just a bug. Code is available at this https URL. 

**Abstract (ZH)**: 联邦学习（FL）在医疗健康等重视数据隐私的领域被越来越广泛地采用。这些系统中的一个基本挑战是统计异质性——即各客户端数据分布差异显著（例如，不同的医院可能治疗不同的患者群体）。尽管当前的联邦学习算法主要关注从这些异质客户端聚合模型更新，中央服务器的潜力仍然未被充分探索。本文受到医疗健康场景的启发：中央服务器是否不仅能构建模型，还能指导新患者前往最合适的医院进行治疗？我们推广这一思想，提出了一种新型的联邦学习范式，其中服务器积极指导新任务或查询在网络中最合适的客户端处进行分配。为此，我们引入了一种经验似然为基础的框架，同时实现两个目标：1) 在每个客户端上学习有效的本地模型，2) 为新的查询找到最合适的匹配客户端。实验证明，该框架在基准数据集上显示了比标准联邦学习方法更高的模型准确性和更精准的客户端指导效果。这项工作开启了利用异质性作为特征而非缺陷来构建更智能和资源高效的联邦系统的新的研究方向。代码可从以下链接获取。 

---
# MMeViT: Multi-Modal ensemble ViT for Post-Stroke Rehabilitation Action Recognition 

**Title (ZH)**: MMeViT: 多模态集成ViT在中风后康复动作识别中的应用 

**Authors**: Ye-eun Kim, Suhyeon Lim, Andrew J. Choi  

**Link**: [PDF](https://arxiv.org/pdf/2509.23044)  

**Abstract**: Rehabilitation therapy for stroke patients faces a supply shortage despite the increasing demand. To address this issue, remote monitoring systems that reduce the burden on medical staff are emerging as a viable alternative. A key component of these remote monitoring systems is Human Action Recognition (HAR) technology, which classifies actions. However, existing HAR studies have primarily focused on non-disable individuals, making them unsuitable for recognizing the actions of stroke patients. HAR research for stroke has largely concentrated on classifying relatively simple actions using machine learning rather than deep learning. In this study, we designed a system to monitor the actions of stroke patients, focusing on domiciliary upper limb Activities of Daily Living (ADL). Our system utilizes IMU (Inertial Measurement Unit) sensors and an RGB-D camera, which are the most common modalities in HAR. We directly collected a dataset through this system, investigated an appropriate preprocess and proposed a deep learning model suitable for processing multimodal data. We analyzed the collected dataset and found that the action data of stroke patients is less clustering than that of non-disabled individuals. Simultaneously, we found that the proposed model learns similar tendencies for each label in data with features that are difficult to clustering. This study suggests the possibility of expanding the deep learning model, which has learned the action features of stroke patients, to not only simple action recognition but also feedback such as assessment contributing to domiciliary rehabilitation in future research. The code presented in this study is available at this https URL. 

**Abstract (ZH)**: 尽管对中风患者的康复治疗需求不断增加，但康复治疗供应却面临短缺。为解决这一问题，减少医护人员负担的远程监控系统正逐渐成为可行的替代方案。这些远程监控系统的关键组成部分是人体动作识别（HAR）技术，该技术用于分类动作。然而，现有的HAR研究主要集中在非残疾个体上，使得它们不适用于识别中风患者的动作。对于中风患者的HAR研究大多侧重于使用机器学习而非深度学习来分类相对简单的动作。本研究设计了一个系统来监测中风患者的动作，重点是居家上肢日常生活活动（ADL）。该系统利用了惯性测量单元（IMU）传感器和RGB-D相机，这是HAR中常用的模态。我们直接通过该系统收集了数据集，研究了合适的预处理方法，并提出了适用于处理多模态数据的深度学习模型。我们对收集的数据集进行了分析，发现中风患者的动作数据比非残疾个体更分散。同时，我们发现提出的模型在难以聚类的数据特征中，学习了每种标签的相似趋势。本研究表明，已学习中风患者动作特征的深度学习模型不仅可用于简单的动作识别，还可为未来的研究中提供居家康复评估等方面的反馈。本研究中提供的代码可在以下网址获取。 

---
# IsingFormer: Augmenting Parallel Tempering With Learned Proposals 

**Title (ZH)**: IsingFormer: 用学习得到的提案增强平行退火方法 

**Authors**: Saleh Bunaiyan, Corentin Delacour, Shuvro Chowdhury, Kyle Lee, Kerem Y. Camsari  

**Link**: [PDF](https://arxiv.org/pdf/2509.23043)  

**Abstract**: Markov Chain Monte Carlo (MCMC) underlies both statistical physics and combinatorial optimization, but mixes slowly near critical points and in rough landscapes. Parallel Tempering (PT) improves mixing by swapping replicas across temperatures, yet each replica still relies on slow local updates to change its configuration. We introduce IsingFormer, a Transformer trained on equilibrium samples that can generate entire spin configurations resembling those from the target distribution. These uncorrelated samples are used as proposals for global moves within a Metropolis step in PT, complementing the usual single-spin flips. On 2D Ising models (sampling), IsingFormer reproduces magnetization and free-energy curves and generalizes to unseen temperatures, including the critical region. Injecting even a single proposal sharply reduces equilibration time, replacing thousands of local updates. On 3D spin glasses (optimization), PT enhanced with IsingFormer finds substantially lower-energy states, demonstrating how global moves accelerate search in rugged landscapes. Finally, applied to integer factorization encoded as Ising problems, IsingFormer trained on a limited set of semiprimes transfers successfully to unseen semiprimes, boosting success rates beyond the training distribution. Since factorization is a canonical hard benchmark, this ability to generalize across instances highlights the potential of learning proposals that move beyond single problems to entire families of instances. The IsingFormer demonstrates that Monte Carlo methods can be systematically accelerated by neural proposals that capture global structure, yielding faster sampling and stronger performance in combinatorial optimization. 

**Abstract (ZH)**: Markov链蒙特卡洛（MCMC）既存在于统计物理中也存在于组合优化中，但在临界点附近和崎岖的景观中混合速度缓慢。并行温度调谐（PT）通过在不同温度下交换复制品来提高混合效果，但每个复制品仍然依赖于缓慢的局部更新来改变其配置。我们介绍了IsingFormer，这是一种在平衡样本上训练的Transformer，能够生成类似于目标分布的完整自旋配置。这些未关联的样本被用作PT中全局移动的提案，在Metropolis步骤中补充了传统的单个自旋翻转。在2D自旋玻璃模型（采样）中，IsingFormer重现了磁化率和自由能曲线，并可以在未见过的温度下泛化，包括临界区域。即使注入一个提案也能显著减少平衡时间，取代数千次的局部更新。在3D自旋玻璃模型（优化）中，增强了IsingFormer的PT找到了显著更低能量的状态，证明了全局移动在崎岖景观中如何加速搜索。最后，将其应用于整数因子分解编码为自旋玻璃问题，IsingFormer在有限的半素数集上训练后可以成功迁移到未见过的半素数上，提升了解决率超出训练分布。由于因子分解是经典硬基准测试，这种在实例之间泛化的能力突显了学习超越单一问题的全局结构移动提案的潜力。IsingFormer展示了通过捕捉全局结构的神经提案系统加速蒙特卡洛方法的可能性，从而实现更快的采样和更强的组合优化性能。 

---
# Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data 

**Title (ZH)**: LLM的病毒感染攻击：“VIA”合成数据可以传播你的污染 

**Authors**: Zi Liang, Qingqing Ye, Xuan Liu, Yanyun Wang, Jianliang Xu, Haibo Hu  

**Link**: [PDF](https://arxiv.org/pdf/2509.23041)  

**Abstract**: Synthetic data refers to artificial samples generated by models. While it has been validated to significantly enhance the performance of large language models (LLMs) during training and has been widely adopted in LLM development, potential security risks it may introduce remain uninvestigated. This paper systematically evaluates the resilience of synthetic-data-integrated training paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal that such a paradigm exhibits strong resistance to existing attacks, primarily thanks to the different distribution patterns between poisoning data and queries used to generate synthetic samples. To enhance the effectiveness of these attacks and further investigate the security risks introduced by synthetic data, we introduce a novel and universal attack framework, namely, Virus Infection Attack (VIA), which enables the propagation of current attacks through synthetic data even under purely clean queries. Inspired by the principles of virus design in cybersecurity, VIA conceals the poisoning payload within a protective "shell" and strategically searches for optimal hijacking points in benign samples to maximize the likelihood of generating malicious content. Extensive experiments on both data poisoning and backdoor attacks show that VIA significantly increases the presence of poisoning content in synthetic data and correspondingly raises the attack success rate (ASR) on downstream models to levels comparable to those observed in the poisoned upstream models. 

**Abstract (ZH)**: 合成数据指的是由模型生成的虚拟样本。尽管它已被验证能够在训练过程中显著提升大型语言模型（LLMs）的表现，并已在LLM开发中广泛采用，但其可能引入的安全风险仍待研究。本文系统性地评估了集成合成数据的训练范式针对主流的投毒和后门攻击的鲁棒性。我们发现，此类范式对现有攻击表现出较强的抵抗力，主要得益于投毒数据与用于生成合成样本的查询之间不同的分布模式。为了增强这些攻击的有效性并进一步探讨由合成数据引入的安全风险，我们提出了一个新颖且通用的攻击框架，即病毒感染攻击（VIA），该框架能够在仅使用纯净查询的情况下，通过合成数据传播当前的攻击。灵感源自网络安全中病毒设计的原则，VIA将投毒负载隐藏在保护性的“壳”中，战略性地在良性样本中寻找最佳劫持点，以最大化生成恶意内容的可能性。在数据投毒和后门攻击的广泛实验中，我们显示VIA显著增加了合成数据中的投毒内容比例，并相应地提高了下游模型的攻击成功率（ASR），使其接近或达到中毒上游模型观察到的水平。 

---
# Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents 

**Title (ZH)**: 回首以理向前：可回溯记忆赋能长上下文LLM代理 

**Authors**: Yaorui Shi, Yuxin Chen, Siyuan Wang, Sihang Li, Hengxing Cai, Qi Gu, Xiang Wang, An Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.23040)  

**Abstract**: Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory corpus that is dynamically updated during a single-pass document scan, also known as the "memorize while reading" methods. While this approach scales efficiently, it suffers from irreversible forward-only processing, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced memory that allows selective retrieval from the entire memory history and allows non-linear reasoning and revisiting of early evidence. To further strengthen training, we propose Reinforcement Learning with Multi-Level Rewards (RLMLR), which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support multi-hop memory utilizing. Experiments on long-document QA show significant gains over existing memory-based approaches, which validates ReMemR1 as an effective solution for long-context reasoning agents. 

**Abstract (ZH)**: 具有回调增强记忆的ReMemR1：一种用于长上下文推理的内存增强代理 

---
# GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization 

**Title (ZH)**: GeLoc3r: 通过几何一致性正则化提高相对相机姿态回归性能 

**Authors**: Jingxing Li, Yongjae Lee, Deliang Fan  

**Link**: [PDF](https://arxiv.org/pdf/2509.23038)  

**Abstract**: Prior ReLoc3R achieves breakthrough performance with fast 25ms inference and state-of-the-art regression accuracy, yet our analysis reveals subtle geometric inconsistencies in its internal representations that prevent reaching the precision ceiling of correspondence-based methods like MASt3R (which require 300ms per pair). In this work, we present GeLoc3r, a novel approach to relative camera pose estimation that enhances pose regression methods through Geometric Consistency Regularization (GCR). GeLoc3r overcomes the speed-accuracy dilemma by training regression networks to produce geometrically consistent poses without inference-time geometric computation. During training, GeLoc3r leverages ground-truth depth to generate dense 3D-2D correspondences, weights them using a FusionTransformer that learns correspondence importance, and computes geometrically-consistent poses via weighted RANSAC. This creates a consistency loss that transfers geometric knowledge into the regression network. Unlike FAR method which requires both regression and geometric solving at inference, GeLoc3r only uses the enhanced regression head at test time, maintaining ReLoc3R's fast speed and approaching MASt3R's high accuracy. On challenging benchmarks, GeLoc3r consistently outperforms ReLoc3R, achieving significant improvements including 40.45% vs. 34.85% AUC@5° on the CO3Dv2 dataset (16% relative improvement), 68.66% vs. 66.70% AUC@5° on RealEstate10K, and 50.45% vs. 49.60% on MegaDepth1500. By teaching geometric consistency during training rather than enforcing it at inference, GeLoc3r represents a paradigm shift in how neural networks learn camera geometry, achieving both the speed of regression and the geometric understanding of correspondence methods. 

**Abstract (ZH)**: GeLoc3r：通过几何一致正则化增强的姿态回归方法在相对相机姿态估计中的应用 

---
# Sensor-Adaptive Flood Mapping with Pre-trained Multi-Modal Transformers across SAR and Multispectral Modalities 

**Title (ZH)**: 基于预训练多模态变换器的SAR和多光谱模态自适应洪水 mapping 

**Authors**: Tomohiro Tanaka, Narumasa Tsutsumida  

**Link**: [PDF](https://arxiv.org/pdf/2509.23035)  

**Abstract**: Floods are increasingly frequent natural disasters causing extensive human and economic damage, highlighting the critical need for rapid and accurate flood inundation mapping. While remote sensing technologies have advanced flood monitoring capabilities, operational challenges persist: single-sensor approaches face weather-dependent data availability and limited revisit periods, while multi-sensor fusion methods require substantial computational resources and large-scale labeled datasets. To address these limitations, this study introduces a novel sensor-flexible flood detection methodology by fine-tuning Presto, a lightweight ($\sim$0.4M parameters) multi-modal pre-trained transformer that processes both Synthetic Aperture Radar (SAR) and multispectral (MS) data at the pixel level. Our approach uniquely enables flood mapping using SAR-only, MS-only, or combined SAR+MS inputs through a single model architecture, addressing the critical operational need for rapid response with whatever sensor data becomes available first during disasters. We evaluated our method on the Sen1Floods11 dataset against the large-scale Prithvi-100M baseline ($\sim$100M parameters) across three realistic data availability scenarios. The proposed model achieved superior performance with an F1 score of 0.896 and mIoU of 0.886 in the optimal sensor-fusion scenario, outperforming the established baseline. Crucially, the model demonstrated robustness by maintaining effective performance in MS-only scenarios (F1: 0.893) and functional capabilities in challenging SAR-only conditions (F1: 0.718), confirming the advantage of multi-modal pre-training for operational flood mapping. Our parameter-efficient, sensor-flexible approach offers an accessible and robust solution for real-world disaster scenarios requiring immediate flood extent assessment regardless of sensor availability constraints. 

**Abstract (ZH)**: 洪水频率增加，导致广泛的人类和经济损害，突显了快速准确绘制洪水淹没图的迫切需求。虽然遥感技术提升了洪水监测能力，但仍存在运营挑战：单传感器方法面临依赖天气的数据可用性和有限的重访周期，而多传感器融合方法则需要大量的计算资源和大规模标注数据集。为应对这些局限性，本研究引入了一种新颖的传感器灵活洪水检测方法，通过微调 Presto（一个轻量级，约0.4M参数的多模态预训练变换器），该模型在像素级别处理合成孔径雷达（SAR）和多光谱（MS）数据。我们的方法能够通过单一模型架构使用SAR仅有的、MS仅有的或联合SAR+MS输入进行洪水制图，满足灾害期间 whichever传感器数据首先可用时迅速响应的迫切需求。我们在Sen1Floods11数据集上将本方法与大规模Prithvi-100M基线（约100M参数）进行了评估，在三种实际的数据可用性场景中。所提出模型在最佳传感器融合场景下的F1分数为0.896，mIoU为0.886，优于现有的基线。值得注意的是，该模型展示了稳健性，在MS仅有的场景中保持了有效的性能（F1: 0.893），并在具有挑战性的SAR仅有的条件下也能发挥作用（F1: 0.718），证明了多模态预训练在运营洪水绘图方面的优势。我们的参数高效、传感器灵活的方法为不受传感器可用性限制的实际灾害场景提供了便捷而稳健的解决方案，用于立即评估洪水范围。 

---
# DPFNAS: Differential Privacy-Enhanced Federated Neural Architecture Search for 6G Edge Intelligence 

**Title (ZH)**: DPFNAS：增强差分隐私的6G边缘智能联邦神经架构搜索 

**Authors**: Yang Lv, Jin Cao, Ben Niu, Zhe Sun, Fengwei Wang, Fenghua Li, Hui Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.23030)  

**Abstract**: The Sixth-Generation (6G) network envisions pervasive artificial intelligence (AI) as a core goal, enabled by edge intelligence through on-device data utilization. To realize this vision, federated learning (FL) has emerged as a key paradigm for collaborative training across edge devices. However, the sensitivity and heterogeneity of edge data pose key challenges to FL: parameter sharing risks data reconstruction, and a unified global model struggles to adapt to diverse local distributions. In this paper, we propose a novel federated learning framework that integrates personalized differential privacy (DP) and adaptive model design. To protect training data, we leverage sample-level representations for knowledge sharing and apply a personalized DP strategy to resist reconstruction attacks. To ensure distribution-aware adaptation under privacy constraints, we develop a privacy-aware neural architecture search (NAS) algorithm that generates locally customized architectures and hyperparameters. To the best of our knowledge, this is the first personalized DP solution tailored for representation-based FL with theoretical convergence guarantees. Our scheme achieves strong privacy guarantees for training data while significantly outperforming state-of-the-art methods in model performance. Experiments on benchmark datasets such as CIFAR-10 and CIFAR-100 demonstrate that our scheme improves accuracy by 6.82\% over the federated NAS method PerFedRLNAS, while reducing model size to 1/10 and communication cost to 1/20. 

**Abstract (ZH)**: 第六代（6G）网络愿景通过边缘智能实现泛在的人工智能，边缘设备上的数据利用是关键目标。为实现这一愿景，联邦学习（FL）已成为跨边缘设备协作训练的关键范式。然而，边缘数据的敏感性和异质性给FL带来了关键挑战：参数共享存在数据重构风险，统一的全局模型难以适应多样化的本地分布。在本文中，我们提出了一种结合个性化差分隐私（DP）和自适应模型设计的新型联邦学习框架。为了保护训练数据，我们利用样本级表示进行知识共享，并应用个性化DP策略以抵御重构攻击。为了在隐私约束下确保分布感知的适应性，我们开发了一种隐私感知神经架构搜索（NAS）算法，生成本地定制的架构和超参数。据我们所知，这是第一个针对表示驱动的FL的个性化DP解决方案，并具有理论收敛保证。我们的方案在保护训练数据隐私方面表现出强大的保证，同时在模型性能上显著优于当前最先进的方法。基准数据集（如CIFAR-10和CIFAR-100）上的实验表明，与PerFedRLNAS方法相比，我们的方案在准确率上提高了6.82%，同时模型大小减少了10倍，通信成本减少了20倍。 

---
# Tracing the Representation Geometry of Language Models from Pretraining to Post-training 

**Title (ZH)**: 从预训练到后训练语言模型的表示几何追踪 

**Authors**: Melody Zixuan Li, Kumar Krishna Agrawal, Arna Ghosh, Komal Kumar Teru, Adam Santoro, Guillaume Lajoie, Blake A. Richards  

**Link**: [PDF](https://arxiv.org/pdf/2509.23024)  

**Abstract**: Standard training metrics like loss fail to explain the emergence of complex capabilities in large language models. We take a spectral approach to investigate the geometry of learned representations across pretraining and post-training, measuring effective rank (RankMe) and eigenspectrum decay ($\alpha$-ReQ). With OLMo (1B-7B) and Pythia (160M-12B) models, we uncover a consistent non-monotonic sequence of three geometric phases during autoregressive pretraining. The initial "warmup" phase exhibits rapid representational collapse. This is followed by an "entropy-seeking" phase, where the manifold's dimensionality expands substantially, coinciding with peak n-gram memorization. Subsequently, a "compression-seeking" phase imposes anisotropic consolidation, selectively preserving variance along dominant eigendirections while contracting others, a transition marked with significant improvement in downstream task performance. We show these phases can emerge from a fundamental interplay of cross-entropy optimization under skewed token frequencies and representational bottlenecks ($d \ll |V|$). Post-training further transforms geometry: SFT and DPO drive "entropy-seeking" dynamics to integrate specific instructional or preferential data, improving in-distribution performance while degrading out-of-distribution robustness. Conversely, RLVR induces "compression-seeking", enhancing reward alignment but reducing generation diversity. 

**Abstract (ZH)**: 标准的训练度量如损失无法解释大型语言模型中复杂能力的涌现。我们采用谱方法研究预训练和后训练过程中学习表示的空间几何，测量有效秩（RankMe）和特征谱衰减（$\alpha$-ReQ）。通过对OLMo（1B-7B）和Pythia（160M-12B）模型的研究，我们发现在自回归预训练过程中存在一个一致的非单调三阶段几何演变。初始的“热身”阶段表现出快速的观点坍缩。随后是“熵寻求”阶段，此时流形的维度显著扩张，与n-gram记忆的峰值相吻合。之后是“压缩寻求”阶段，导致各向异性压缩，选择性地沿主特征方向保留方差并压缩其他方向，这一转变伴随着下游任务性能的显著提升。我们表明，这些阶段可以从交叉熵优化下的词元频率偏差和表示瓶颈（$d \ll |V|$）的基本相互作用中 emergence 出来。后训练进一步改变几何结构：SFT和DPO驱动“熵寻求”动力学，整合特定的指令或偏好数据，提高分布内性能，但减少分布外鲁棒性。相反，RLVR诱导“压缩寻求”，增强奖励对齐但减少生成多样性。 

---
# LLM Watermark Evasion via Bias Inversion 

**Title (ZH)**: LLM水印逃逸通过偏差反转 

**Authors**: Jeongyeon Hwang, Sangdon Park, Jungseul Ok  

**Link**: [PDF](https://arxiv.org/pdf/2509.23019)  

**Abstract**: Watermarking for large language models (LLMs) embeds a statistical signal during generation to enable detection of model-produced text. While watermarking has proven effective in benign settings, its robustness under adversarial evasion remains contested. To advance a rigorous understanding and evaluation of such vulnerabilities, we propose the \emph{Bias-Inversion Rewriting Attack} (BIRA), which is theoretically motivated and model-agnostic. BIRA weakens the watermark signal by suppressing the logits of likely watermarked tokens during LLM-based rewriting, without any knowledge of the underlying watermarking scheme. Across recent watermarking methods, BIRA achieves over 99\% evasion while preserving the semantic content of the original text. Beyond demonstrating an attack, our results reveal a systematic vulnerability, emphasizing the need for stress testing and robust defenses. 

**Abstract (ZH)**: 大语言模型的水印嵌入：偏差反转重写攻击 

---
# MoE-PHDS: One MoE checkpoint for flexible runtime sparsity 

**Title (ZH)**: MoE-PHDS: 一个MoE检查点用于灵活的运行时稀疏性 

**Authors**: Lauren. A Hannah, Soheil Zibakhsh, Kumari Nishu, Arnav Kundu, Mohammad Samragh Razlighi, Mehrdad Farajtabar, Minsik Cho  

**Link**: [PDF](https://arxiv.org/pdf/2509.23012)  

**Abstract**: Sparse Mixtures of Experts (MoEs) are typically trained to operate at a fixed sparsity level, e.g. $k$ in a top-$k$ gating function. This global sparsity level determines an operating point on the accuracy/latency curve; currently, meeting multiple efficiency targets means training and maintaining multiple models. This practice complicates serving, increases training and maintenance costs, and limits flexibility in meeting diverse latency, efficiency, and energy requirements. We show that pretrained MoEs are more robust to runtime sparsity shifts than commonly assumed, and introduce MoE-PHDS ({\bf P}ost {\bf H}oc {\bf D}eclared {\bf S}parsity), a lightweight SFT method that turns a single checkpoint into a global sparsity control surface. PHDS mixes training across sparsity levels and anchors with a short curriculum at high sparsity, requiring no architectural changes. The result is predictable accuracy/latency tradeoffs from one model: practitioners can ``dial $k$'' at inference time without swapping checkpoints, changing architecture, or relying on token-level heuristics. Experiments on OLMoE-1B-7B-0125, Qwen1.5-MoE-A2.7B, and proprietary models fit on multiple operating points show that PHDS matches or exceeds well-specified oracle models, improves cross-sparsity agreement by up to 22\% vs. well-specified oracle models, and enables simplified, flexible runtime MoE deployment by making global sparsity a first-class serving primitive. 

**Abstract (ZH)**: 预训练的专家混合模型更具鲁棒性：基于后显式稀疏度的轻量级微调方法（MoE-PHDS） 

---
# Physically Plausible Multi-System Trajectory Generation and Symmetry Discovery 

**Title (ZH)**: 物理可验证的多系统轨迹生成与对称性发现 

**Authors**: Jiayin Liu, Yulong Yang, Vineet Bansal, Christine Allen-Blanchette  

**Link**: [PDF](https://arxiv.org/pdf/2509.23003)  

**Abstract**: From metronomes to celestial bodies, mechanics underpins how the world evolves in time and space. With consideration of this, a number of recent neural network models leverage inductive biases from classical mechanics to encourage model interpretability and ensure forecasted states are physical. However, in general, these models are designed to capture the dynamics of a single system with fixed physical parameters, from state-space measurements of a known configuration space. In this paper we introduce Symplectic Phase Space GAN (SPS-GAN) which can capture the dynamics of multiple systems, and generalize to unseen physical parameters from. Moreover, SPS-GAN does not require prior knowledge of the system configuration space. In fact, SPS-GAN can discover the configuration space structure of the system from arbitrary measurement types (e.g., state-space measurements, video frames). To achieve physically plausible generation, we introduce a novel architecture which embeds a Hamiltonian neural network recurrent module in a conditional GAN backbone. To discover the structure of the configuration space, we optimize the conditional time-series GAN objective with an additional physically motivated term to encourages a sparse representation of the configuration space. We demonstrate the utility of SPS-GAN for trajectory prediction, video generation and symmetry discovery. Our approach captures multiple systems and achieves performance on par with supervised models designed for single systems. 

**Abstract (ZH)**: 从摆钟到天体，力学贯穿了世界在时间和空间中的演变。考虑到这一点，近期的一些神经网络模型借鉴了经典力学的归纳偏置，以促进模型的可解释性并确保预测的状态具有物理意义。然而，通常这些模型设计用于捕捉单个系统（具有固定物理参数）从已知配置空间的状态空间测量中动态。本文我们引入了辛相空间生成对抗网络（SPS-GAN），它可以捕捉多个系统的动态，并且可以从未知物理参数中进行泛化。此外，SPS-GAN 不需要事先了解系统的配置空间结构。实际上，SPS-GAN 可以从任意测量类型（例如状态空间测量、视频帧）中发现系统的配置空间结构。为了实现物理上合理的生成，我们引入了一种新型架构，在条件生成对抗网络骨干中嵌入了哈密顿神经网络递归模块。为了发现配置空间的结构，我们通过一个额外的物理驱动项优化条件时间序列生成对抗网络目标，以促进配置空间的稀疏表示。我们表明，SPS-GAN 在轨迹预测、视频生成和对称性发现方面的应用价值。我们的方法可以捕捉多个系统，并且在性能上与专门为单个系统设计的监督模型相当。 

---
# ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning 

**Title (ZH)**: ADAM：一种多元的人类档案，用于评估和提升LLM在生物传记推理中的表现 

**Authors**: Jasin Cekinmez, Omid Ghahroodi, Saad Fowad Chandle, Dhiman Gupta, Ehsaneddin Asgari  

**Link**: [PDF](https://arxiv.org/pdf/2509.22991)  

**Abstract**: We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating and improving multimodal large language models (MLLMs) in biographical reasoning. To the best of our knowledge, this is the first work to systematically examine LLM capabilities in biography, a critical yet underexplored dimension of factual knowledge. At its core, AdamDB is a multilingual and multimodal dataset covering over 4 million individuals across geography, time, and profession, while AdamBench provides cognitively structured evaluations based on Bloom's taxonomy, spanning six reasoning levels in both English and native languages. To address hallucinations, particularly for lesser-known individuals, we propose AdamRAG, a retrieval-augmented generation system tailored to biographical contexts. Experiments show that AdamRAG substantially improves open-source models and modestly benefits closed-source ones, with the largest gains on lower-order reasoning. Popularity strongly mediates accuracy, and multimodal input via face images offers smaller, less consistent improvements than retrieval. ADAM establishes the first benchmark and framework for cognitively, culturally, and multimodally grounded biographical evaluation, advancing the development of multilingual, accurate, and hallucination-resistant MLLMs. 

**Abstract (ZH)**: ADAM：一种多模态人类历史档案框架以评估和提升生物传记推理能力的大语言模型 

---
# Functional Critic Modeling for Provably Convergent Off-Policy Actor-Critic 

**Title (ZH)**: 证明收敛的离策 Actor-Critc 模型中的功能评论者建模 

**Authors**: Qinxun Bai, Yuxuan Han, Wei Xu, Zhengyuan Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2509.22964)  

**Abstract**: Off-policy reinforcement learning (RL) with function approximation offers an effective way to improve sample efficiency by reusing past experience. Within this setting, the actor-critic (AC) framework has achieved strong empirical success. However, both the critic and actor learning is challenging for the off-policy AC methods: first of all, in addition to the classic "deadly triad" instability of off-policy evaluation, it also suffers from a "moving target" problem, where the policy being evaluated changes continually; secondly, actor learning becomes less efficient due to the difficulty of estimating the exact off-policy policy gradient. The first challenge essentially reduces the problem to repeatedly performing off-policy evaluation for changing policies. For the second challenge, the off-policy policy gradient theorem requires a complex and often impractical algorithm to estimate an additional emphasis critic, which is typically neglected in practice, thereby reducing to the on-policy policy gradient as an approximation. In this work, we introduce a novel concept of functional critic modeling, which leads to a new AC framework that addresses both challenges for actor-critic learning under the deadly triad setting. We provide a theoretical analysis in the linear function setting, establishing the provable convergence of our framework, which, to the best of our knowledge, is the first convergent off-policy target-based AC algorithm. From a practical perspective, we further propose a carefully designed neural network architecture for the functional critic modeling and demonstrate its effectiveness through preliminary experiments on widely used RL tasks from the DeepMind Control Benchmark. 

**Abstract (ZH)**: 带有函数逼近的离策 reinforcement learning中的actor-critic框架：解决致命三角问题的功能性critic建模 

---
# Tiny-QMoE 

**Title (ZH)**: Tiny-QMoE 

**Authors**: Jack Cashman, Jiaqi Nie  

**Link**: [PDF](https://arxiv.org/pdf/2509.22951)  

**Abstract**: The QMoE model provides a practical approach for compression of massive Mixture-of-Experts (MoE) models. QMoE offers a solution geared towards memory limitations that often reach terabyte scales, and it has the advantage of working with high sparsity models which implicitly lend themselves to compression techniques. QMoE also has the advantage of only taking MoE models into account and does not evaluate its use with non mixture of expert systems. Although this prior attempt focuses on the limitations of large servers with the latest NVIDIA hardware which in the case of the H100 and V100 which have 80 GB of HBM (High Bandwidth Memory), what is not being considered is a significantly more constrained environment, such as in the case of mobile devices which may have in the case of the iPhone anywhere from 4 to 8 GB of unified memory which also needs to be shared with the operating system and additional processes. Although edge devices such as phones and laptops are becoming increasingly more computationally powerful, they are still not close to the level of advanced server machines such as NVIDIA. An additional constraint that we must consider is that of latency. The communication time of sending a request to an LLM server and then getting it back is an additional waiting time that can be removed. We may also want to use LLM technology in environments where there is no reliable network connection. 

**Abstract (ZH)**: QMoE模型提供了大规模Mixture-of-Experts（MoE）模型压缩的一种实用方法。 

---
# What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples? 

**Title (ZH)**: 在匹配计算预算条件下，影响基于上下文学习更为重要的因素是自然文本预训练还是融入针对性合成例证？ 

**Authors**: Mohammed Sabry, Anya Belz  

**Link**: [PDF](https://arxiv.org/pdf/2509.22947)  

**Abstract**: Does explicitly exercising the induction circuit during pretraining improve in-context learning (ICL), or is natural text sufficient when compute is held constant (iso-FLOPs)? To test whether targeted synthetic data can accelerate induction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight curriculum that injects forward-copy (Induction), backward-copy (Anti), or a balanced mix into the pretraining stream. We train models from 0.13B to 1B parameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii) head-level telemetry, and (iii) held-out language modeling perplexity. Our findings challenge the assumption that early induction circuit activation directly improves ICL. While Bi-Induct accelerates induction-head emergence at small scales, this does not consistently yield stronger generalization. On standard LM benchmarks, Bi-Induct matches natural-only training; on function-style ICL probes, the 1B natural-only performs best. Stress tests (e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these trends. Telemetry shows larger natural-only models develop broader, earlier induction heads without explicit induction patterns. Anti-induction data fails to elicit meaningful activation. Perplexity penalties from synthetic data shrink with scale, suggesting larger models can absorb non-natural patterns with minimal cost. Crucially, ablating the top 2% of induction heads degrades ICL more than random ablations, especially for natural-only models, indicating more centralized, load-bearing circuits. Bi-Induct variants exhibit more redundant induction activity, implying different circuit utilization. Overall, inducing activation is not sufficient: ICL gains depend on these circuits becoming functionally necessary. These results underscore mechanism-aware pretraining diagnostics and data mixtures that foster load-bearing, not merely present, structure. 

**Abstract (ZH)**: does explicitly exercising the induction circuit during pretraining improve in-context learning (icl), or is natural text sufficient when compute is held constant (iso-flops)? introducing bi-induct to test the acceleration of induction-head emergence and enhancement of icl 

---
# Unsupervised Speech Enhancement using Data-defined Priors 

**Title (ZH)**: 基于数据定义先验的无监督语音增强 

**Authors**: Dominik Klement, Matthew Maciejewski, Sanjeev Khudanpur, Jan Černocký, Lukáš Burget  

**Link**: [PDF](https://arxiv.org/pdf/2509.22942)  

**Abstract**: The majority of deep learning-based speech enhancement methods require paired clean-noisy speech data. Collecting such data at scale in real-world conditions is infeasible, which has led the community to rely on synthetically generated noisy speech. However, this introduces a gap between the training and testing phases. In this work, we propose a novel dual-branch encoder-decoder architecture for unsupervised speech enhancement that separates the input into clean speech and residual noise. Adversarial training is employed to impose priors on each branch, defined by unpaired datasets of clean speech and, optionally, noise. Experimental results show that our method achieves performance comparable to leading unsupervised speech enhancement approaches. Furthermore, we demonstrate the critical impact of clean speech data selection on enhancement performance. In particular, our findings reveal that performance may appear overly optimistic when in-domain clean speech data are used for prior definition -- a practice adopted in previous unsupervised speech enhancement studies. 

**Abstract (ZH)**: 基于深度学习的无监督语音增强方法：一种分离输入为干净语音和残余噪声的双分支编码器-解码器架构 

---
# Compute-Optimal Quantization-Aware Training 

**Title (ZH)**: 计算最优量化感知训练 

**Authors**: Aleksandr Dremov, David Grangier, Angelos Katharopoulos, Awni Hannun  

**Link**: [PDF](https://arxiv.org/pdf/2509.22935)  

**Abstract**: Quantization-aware training (QAT) is a leading technique for improving the accuracy of quantized neural networks. Previous work has shown that decomposing training into a full-precision (FP) phase followed by a QAT phase yields superior accuracy compared to QAT alone. However, the optimal allocation of compute between the FP and QAT phases remains unclear. We conduct extensive experiments with various compute budgets, QAT bit widths, and model sizes from 86.0M to 2.2B to investigate how different QAT durations impact final performance. We demonstrate that, contrary to previous findings, the loss-optimal ratio of QAT to FP training increases with the total amount of compute. Moreover, the optimal fraction can be accurately predicted for a wide range of model sizes and quantization widths using the tokens-per-parameter-byte statistic. From experimental data, we derive a loss scaling law that predicts both optimal QAT ratios and final model performance across different QAT/FP compute allocation strategies and QAT bit widths. We use the scaling law to make further predictions, which we verify experimentally, including which QAT bit width is optimal under a given memory constraint and how QAT accuracy with different bit widths compares to full-precision model accuracy. Additionally, we propose a novel cooldown and QAT fusion approach that performs learning rate decay jointly with quantization-aware training, eliminating redundant full-precision model updates and achieving significant compute savings. These findings provide practical insights into efficient QAT planning and enable the training of higher-quality quantized models with the same compute budget. 

**Abstract (ZH)**: Quantization-aware训练（QAT）是一种提高量化神经网络准确性的领先技术。先前的研究表明，将训练分解为全精度（FP）阶段和QAT阶段可以比单独使用QAT获得更高的精度。然而，FP和QAT阶段之间的计算分配仍不清楚。我们通过各种计算预算、QAT比特宽和从86.0M到2.2B的模型规模进行了大量实验，研究不同的QAT持续时间对最终性能的影响。我们证明，与之前的研究发现相反，QAT与FP训练的理想比例随总计算量的增加而增加。此外，使用tokens-per-parameter-byte统计值可以准确预测广泛模型规模和量化宽度下的最优比例。从实验数据中，我们推导出一个损失放大定律，该定律可以预测不同QAT/FP计算分配策略和QAT比特宽下的最优QAT比例和最终模型性能。我们使用该定律进行进一步预测，并通过实验验证，包括在给定的内存约束下哪种QAT比特宽是最优的，以及不同比特宽下的QAT准确性与全精度模型准确性之间的比较。此外，我们提出了一种新的冷却和QAT融合方法，该方法结合了量化感知训练和学习率衰减，消除了冗余的全精度模型更新，并实现了显著的计算节省。这些发现为有效的QAT规划提供了实用见解，并使研究人员能够在相同的计算预算下训练出更高质量的量化模型。 

---
# MonoCon: A general framework for learning ultra-compact high-fidelity representations using monotonicity constraints 

**Title (ZH)**: MonoCon：一种使用单调性约束学习超紧凑高保真表示的一般框架 

**Authors**: Shreyas Gokhale  

**Link**: [PDF](https://arxiv.org/pdf/2509.22931)  

**Abstract**: Learning high-quality, robust, efficient, and disentangled representations is a central challenge in artificial intelligence (AI). Deep metric learning frameworks tackle this challenge primarily using architectural and optimization constraints. Here, we introduce a third approach that instead relies on $\textit{functional}$ constraints. Specifically, we present MonoCon, a simple framework that uses a small monotonic multi-layer perceptron (MLP) head attached to any pre-trained encoder. Due to co-adaptation between encoder and head guided by contrastive loss and monotonicity constraints, MonoCon learns robust, disentangled, and highly compact embeddings at a practically negligible performance cost. On the CIFAR-100 image classification task, MonoCon yields representations that are nearly 9x more compact and 1.5x more robust than the fine-tuned encoder baseline, while retaining 99\% of the baseline's 5-NN classification accuracy. We also report a 3.4x more compact and 1.4x more robust representation on an SNLI sentence similarity task for a marginal reduction in the STSb score, establishing MonoCon as a general domain-agnostic framework. Crucially, these robust, ultra-compact representations learned via functional constraints offer a unified solution to critical challenges in disparate contexts ranging from edge computing to cloud-scale retrieval. 

**Abstract (ZH)**: 学习高质量、稳健、高效且解耦的表示是人工智能中的一个核心挑战。基于功能约束的MonoCon框架 

---
# Large language models management of medications: three performance analyses 

**Title (ZH)**: 大型语言模型在管理药物方面的性能分析 

**Authors**: Kelli Henry, Steven Xu, Kaitlin Blotske, Moriah Cargile, Erin F. Barreto, Brian Murray, Susan Smith, Seth R. Bauer, Yanjun Gao, Tianming Liu, Andrea Sikora  

**Link**: [PDF](https://arxiv.org/pdf/2509.22926)  

**Abstract**: Background: Large language models (LLMs) can be useful in diagnosing medical conditions, but few studies have evaluated their consistency in recommending appropriate medication regimens. The purpose of this evaluation was to test GPT-4o on three medication benchmarking tests including mapping a drug name to its correct formulation, identifying drug-drug interactions using both its internal knowledge and using a web search, and preparing a medication order sentence after being given the medication name. Methods: Using GTP-4o three experiments were completed. Accuracy was quantified by computing cosine similarity on TF-IDF vectors, normalized Levenshtein similarity, and ROUGE-1/ROUGE-L F1 between each response and its reference string or by manual evaluation by clinicians. Results: GPT-4o performed poorly on drug-formulation matching, with frequent omissions of available drug formulations (mean 1.23 per medication) and hallucinations of formulations that do not exist (mean 1.14 per medication). Only 49% of tested medications were correctly matched to all available formulations. Accuracy was decreased for medications with more formulations (p<0.0001). GPT-4o was also inconsistent at identifying drug-drug-interactions, although it had better performance with the search-augmented assessment compared to its internal knowledge (54.7% vs. 69.2%, p=0.013). However, allowing a web-search worsened performance when there was no drug-drug interaction (median % correct 100% vs. 40%, p<0.001). Finally, GPT-4o performed moderately with preparing a medication order sentence, with only 65.8% of medication order sentences containing no medication or abbreviation errors. Conclusions: Model performance was overall poor for all tests. This highlights the need for domain-specific training through clinician-annotated datasets and a comprehensive evaluation framework for benchmarking performance. 

**Abstract (ZH)**: 背景：大规模语言模型（LLMs）在诊断医疗条件方面可能非常有用，但很少有研究评估其在推荐适当药物 regimen 方面的一致性。本评估的目的是测试 GPT-4o 在三项药物基准测试中的表现，包括将药物名称映射到正确配方、通过其内部知识和网络搜索识别药物-药物相互作用，以及在给出药物名称后准备药物订单句子。方法：使用 GPT-4o 完成了三个实验。通过计算 TF-IDF 向量、归一化 Levenshtein 相似性以及每个响应与其参考字符串之间的 ROUGE-1/ROUGE-L F1 量化准确度，或通过临床人员的手动评估。结果：GPT-4o 在药物-配方匹配方面表现不佳，频繁遗漏可用的药物配方（每种药物平均 1.23 种）并杜撰不存在的配方（每种药物平均 1.14 种）。仅 49% 的测试药物能够与所有可用配方正确匹配。对于具有更多配方的药物，准确度降低（p<0.0001）。GPT-4o 在识别药物-药物相互作用方面也不一致，尽管在搜索增强评估中其内部知识的表现优于网络搜索增强（54.7% vs. 69.2%，p=0.013）。然而，允许网络搜索在没有药物-药物相互作用时会损害表现（无药物-药物相互作用时的正确率为 100% vs. 40%，p<0.001）。最后，GPT-4o 在准备药物订单句子方面表现中等，仅 65.8% 的药物订单句子中没有药物或缩写错误。结论：所有测试中的模型性能普遍较差。这强调了通过临床标注数据进行领域特定训练以及建立全面的评估框架以基准测试性能的必要性。 

---
# Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings 

**Title (ZH)**: Soft-Di[M]O: 基于软嵌入改进的一步离散图像生成 

**Authors**: Yuanzhi Zhu, Xi Wang, Stéphane Lathuilière, Vicky Kalogeiton  

**Link**: [PDF](https://arxiv.org/pdf/2509.22925)  

**Abstract**: One-step generators distilled from Masked Diffusion Models (MDMs) compress multiple sampling steps into a single forward pass, enabling efficient text and image synthesis. However, they suffer two key limitations: they inherit modeling bias from the teacher, and their discrete token outputs block gradient flow, preventing post-distillation refinements such as adversarial training, reward-based fine-tuning, and Test-Time Embedding Optimization (TTEO). In this work, we introduce soft embeddings, a simple relaxation that replaces discrete tokens with the expected embeddings under the generator's output distribution. Soft embeddings preserve representation fidelity for one-step discrete generator while providing a fully differentiable continuous surrogate that is compatible with teacher backbones and tokenizer decoders. Integrating soft embeddings into the Di[M]O distillation framework (denoted Soft-Di[M]O) makes one-step generators end-to-end trainable and enables straightforward application of GAN-based refinement, differentiable reward fine-tuning, and TTEO. Empirically, across multiple MDM teachers (e.g., MaskBit, MaskGen), Soft-Di[M]O achieves state-of-the-art one-step results: improved class-to-image performance, a one-step FID of 1.56 on ImageNet-256 with GAN-based refinement, along with higher GenEval and HPS scores on text-to-image with reward fine-tuning, and further gains from TTEO. 

**Abstract (ZH)**: 一步生成器：从掩码扩散模型衍生的软嵌入表示的一站式图像生成方法 

---
# Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective 

**Title (ZH)**: 重新思考大型语言模型蒸馏：一种受限马尔可夫决策过程视角 

**Authors**: Matthieu Zimmer, Xiaotong Ji, Tu Nguyen, Haitham Bou Ammar  

**Link**: [PDF](https://arxiv.org/pdf/2509.22921)  

**Abstract**: We introduce a novel approach to large language model (LLM) distillation by formulating it as a constrained reinforcement learning problem. While recent work has begun exploring the integration of task-specific rewards into distillation processes, existing methods typically rely on ad-hoc reward weighting. We propose a principled optimization framework that maximizes task-specific rewards while constraining the divergence from the teacher model to remain below a specified threshold. Our approach adapts constrained state augmented reinforcement learning to the distillation setting, introducing a modified reward function that maintains theoretical guarantees of constraint satisfaction without requiring state augmentation or teacher model access during deployment and without the computational overhead of the dual Lagrangian methods. Through extensive experiments on mathematical reasoning tasks, we demonstrate that our method achieves better constraint satisfaction rates and better reasoning compared to the soft Lagrangian relaxation baselines while maintaining competitive task performance. Our framework provides a theoretically grounded and practically efficient solution for reward-aware distillation in resource-constrained settings. 

**Abstract (ZH)**: 我们通过将其表述为受限强化学习问题，介绍了大型语言模型（LLM）精简的一种新颖方法。 

---
# TY-RIST: Tactical YOLO Tricks for Real-time Infrared Small Target Detection 

**Title (ZH)**: TY-RIST: 战术YOLO技巧用于实时红外小型目标检测 

**Authors**: Abdulkarim Atrash, Omar Moured, Yufan Chen, Jiaming Zhang, Seyda Ertekin, Omur Ugur  

**Link**: [PDF](https://arxiv.org/pdf/2509.22909)  

**Abstract**: Infrared small target detection (IRSTD) is critical for defense and surveillance but remains challenging due to (1) target loss from minimal features, (2) false alarms in cluttered environments, (3) missed detections from low saliency, and (4) high computational costs. To address these issues, we propose TY-RIST, an optimized YOLOv12n architecture that integrates (1) a stride-aware backbone with fine-grained receptive fields, (2) a high-resolution detection head, (3) cascaded coordinate attention blocks, and (4) a branch pruning strategy that reduces computational cost by about 25.5% while marginally improving accuracy and enabling real-time inference. We also incorporate the Normalized Gaussian Wasserstein Distance (NWD) to enhance regression stability. Extensive experiments on four benchmarks and across 20 different models demonstrate state-of-the-art performance, improving mAP at 0.5 IoU by +7.9%, Precision by +3%, and Recall by +10.2%, while achieving up to 123 FPS on a single GPU. Cross-dataset validation on a fifth dataset further confirms strong generalization capability. Additional results and resources are available at this https URL 

**Abstract (ZH)**: 红外小目标检测（IRSTD）对于防御和监控至关重要，但由于（1）微弱特征导致的目标丢失，（2）杂波环境下误报警，（3）低显着性导致的漏检，以及（4）高计算成本，仍具有挑战性。为了解决这些问题，我们提出了TY-RIST，这是一种优化的YOLOv12n架构，结合了（1）具有精细感受野的步幅感知骨干网，（2）高分辨率检测头，（3）级联坐标注意力块，以及（4）分支剪枝策略，该策略在计算成本减少约25.5%的同时，轻微提高了准确率并支持实时推理。我们还将规范化高斯 Wasserstein 距离（NWD）纳入以增强回归稳定性。在四个基准上的广泛实验以及20种不同模型的测试表明，TY-RIST 达到了最先进的性能，mAP在0.5 IoU下的提升幅度为+7.9%，精确率提升+3%，召回率提升+10.2%，且在单个GPU上实现了高达123 FPS。跨数据集验证在第五个数据集上进一步证实了强泛化能力。更多信息和资源请访问此网址。 

---
# Extract-0: A Specialized Language Model for Document Information Extraction 

**Title (ZH)**: 面向文档信息提取的专业语言模型 

**Authors**: Henrique Godoy  

**Link**: [PDF](https://arxiv.org/pdf/2509.22906)  

**Abstract**: This paper presents Extract-0, a 7-billion parameter language model specifically optimized for document information extraction that achieves performance exceeding models with parameter counts several orders of magnitude larger. Through a novel combination of synthetic data generation, supervised fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of 0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology employs a memory-preserving synthetic data generation pipeline that produces 280,128 training examples from diverse document sources, followed by parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M out of 7.66B parameters). The reinforcement learning phase introduces a novel semantic similarity-based reward function that handles the inherent ambiguity in information extraction tasks. This research demonstrates that task-specific optimization can yield models that surpass general-purpose systems while requiring substantially fewer computational resource. 

**Abstract (ZH)**: 本文介绍了Extract-0，一个专门优化用于文档信息提取的70亿参数语言模型，其性能超过参数量比其大几个数量级的模型。通过一种新颖的合成数据生成、基于Low-Rank Adaptation（LoRA）的监督微调以及Group Relative Policy Optimization（GRPO）强化学习的组合，Extract-0在1000个多元文档提取任务基准上的平均奖励为0.573，超越了GPT-4.1（0.457）、o3（0.464）和GPT-4.1-2025（0.459）。训练方法采用了一种保留内存的合成数据生成管道，从多元文档来源生成280,128个训练样本，随后进行了参数高效微调，仅修改了0.53%的模型权重（40.4M/7.66B参数）。强化学习阶段引入了一种基于语义相似性的奖励函数，以处理信息提取任务中的固有歧义。这项研究展示了任务特定优化可以在拥有显著较少计算资源的情况下超越通用系统。 

---
# Convolutional Set Transformer 

**Title (ZH)**: 卷积集转换器 

**Authors**: Federico Chinello, Giacomo Boracchi  

**Link**: [PDF](https://arxiv.org/pdf/2509.22889)  

**Abstract**: We introduce the Convolutional Set Transformer (CST), a novel neural architecture designed to process image sets of arbitrary cardinality that are visually heterogeneous yet share high-level semantics - such as a common category, scene, or concept. Existing set-input networks, e.g., Deep Sets and Set Transformer, are limited to vector inputs and cannot directly handle 3D image tensors. As a result, they must be cascaded with a feature extractor, typically a CNN, which encodes images into embeddings before the set-input network can model inter-image relationships. In contrast, CST operates directly on 3D image tensors, performing feature extraction and contextual modeling simultaneously, thereby enabling synergies between the two processes. This design yields superior performance in tasks such as Set Classification and Set Anomaly Detection and further provides native compatibility with CNN explainability methods such as Grad-CAM, unlike competing approaches that remain opaque. Finally, we show that CSTs can be pre-trained on large-scale datasets and subsequently adapted to new domains and tasks through standard Transfer Learning schemes. To support further research, we release CST-15, a CST backbone pre-trained on ImageNet (this https URL). 

**Abstract (ZH)**: 我们介绍了Convolutional Set Transformer (CST)，这是一种新型的神经架构，设计用于处理具有任意基数且在视觉上异构但共享高级语义（如共同类别、场景或概念）的图像集合。现有的集输入网络，例如Deep Sets和Set Transformer，仅限于处理向量输入，无法直接处理3D图像张量。因此，它们必须与特征提取器（通常为CNN）级联，将图像编码为嵌入向量，才能使集输入网络能够建模图像之间的关系。相比之下，CST可以直接操作3D图像张量，同时进行特征提取和上下文建模，从而在两个过程之间实现协同效应。这种设计在集分类和集异常检测等任务中表现出了优越的性能，并进一步提供了与CNN解释性方法（如Grad-CAM）的原生兼容性，而竞争方法仍然具有不透明性。最后，我们展示了CST可以在大规模数据集上进行预训练，并通过标准的迁移学习方案适应新的领域和任务。为了支持进一步的研究，我们发布了基于ImageNet预训练的CST-15骨干网络（详见此处：this https URL）。 

---
# From Noise to Knowledge: A Comparative Study of Acoustic Anomaly Detection Models in Pumped-storage Hydropower Plants 

**Title (ZH)**: 从噪声到知识： Pumped-storage Hydropower Plants 中 acoustic 异常检测模型的比较研究 

**Authors**: Karim Khamaisi, Nicolas Keller, Stefan Krummenacher, Valentin Huber, Bernhard Fässler, Bruno Rodrigues  

**Link**: [PDF](https://arxiv.org/pdf/2509.22881)  

**Abstract**: In the context of industrial factories and energy producers, unplanned outages are highly costly and difficult to service. However, existing acoustic-anomaly detection studies largely rely on generic industrial or synthetic datasets, with few focused on hydropower plants due to limited access. This paper presents a comparative analysis of acoustic-based anomaly detection methods, as a way to improve predictive maintenance in hydropower plants. We address key challenges in the acoustic preprocessing under highly noisy conditions before extracting time- and frequency-domain features. Then, we benchmark three machine learning models: LSTM AE, K-Means, and OC-SVM, which are tested on two real-world datasets from the Rodundwerk II pumped-storage plant in Austria, one with induced anomalies and one with real-world conditions. The One-Class SVM achieved the best trade-off of accuracy (ROC AUC 0.966-0.998) and minimal training time, while the LSTM autoencoder delivered strong detection (ROC AUC 0.889-0.997) at the expense of higher computational cost. 

**Abstract (ZH)**: 基于声学异常检测方法在水力发电厂预测性维护中的比较研究 

---
# Scalable Wi-Fi RSS-Based Indoor Localization via Automatic Vision-Assisted Calibration 

**Title (ZH)**: 基于自动视觉辅助校准的可扩展Wi-Fi RSS室内外定位 

**Authors**: Abdulkadir Bilge, Erdem Ergen, Burak Soner, Sinem Coleri  

**Link**: [PDF](https://arxiv.org/pdf/2509.22869)  

**Abstract**: Wi-Fi-based positioning promises a scalable and privacy-preserving solution for location-based services in indoor environments such as malls, airports, and campuses. RSS-based methods are widely deployable as RSS data is available on all Wi-Fi-capable devices, but RSS is highly sensitive to multipath, channel variations, and receiver characteristics. While supervised learning methods offer improved robustness, they require large amounts of labeled data, which is often costly to obtain. We introduce a lightweight framework that solves this by automating high-resolution synchronized RSS-location data collection using a short, camera-assisted calibration phase. An overhead camera is calibrated only once with ArUco markers and then tracks a device collecting RSS data from broadcast packets of nearby access points across Wi-Fi channels. The resulting (x, y, RSS) dataset is used to automatically train mobile-deployable localization algorithms, avoiding the privacy concerns of continuous video monitoring. We quantify the accuracy limits of such vision-assisted RSS data collection under key factors such as tracking precision and label synchronization. Using the collected experimental data, we benchmark traditional and supervised learning approaches under varying signal conditions and device types, demonstrating improved accuracy and generalization, validating the utility of the proposed framework for practical use. All code, tools, and datasets are released as open source. 

**Abstract (ZH)**: 基于Wi-Fi的定位技术在商场、机场和校园等室内环境中提供了可扩展且保护隐私的解决方案。基于RSS的方法广泛部署，因为所有Wi-Fi设备都能提供RSS数据，但RSS对多径传播、信道变化和接收器特性高度敏感。尽管监督学习方法提高了鲁棒性，但它们需要大量标注数据，这通常成本高昂。我们提出了一种轻量级框架，通过使用短的、摄像头辅助的校准阶段自动收集高分辨率同步RSS-位置数据来解决这一问题。摄像机仅需一次校准即可使用ArUco标记，并随后跟踪从附近接入点广播数据包中收集RSS数据的设备，跨越多个Wi-Fi信道。生成的(x, y, RSS)数据集用于自动训练可移动部署的定位算法，避免了连续视频监控带来的隐私问题。我们量化了在关键因素如跟踪精度和标签同步下的这种视觉辅助RSS数据收集的准确性极限。利用收集的实验数据，我们在不同信号条件和设备类型下对传统和监督学习方法进行了基准测试，证明了准确性与泛化能力的提升，并验证了所提框架在实际应用中的实用性。所有代码、工具和数据集均作为开源发布。 

---
# Observation-Free Attacks on Online Learning to Rank 

**Title (ZH)**: 无需观察的在线学习排序攻击 

**Authors**: Sameep Chattopadhyay, Nikhil Karamchandani, Sharayu Mohair  

**Link**: [PDF](https://arxiv.org/pdf/2509.22855)  

**Abstract**: Online learning to rank (OLTR) plays a critical role in information retrieval and machine learning systems, with a wide range of applications in search engines and content recommenders. However, despite their extensive adoption, the susceptibility of OLTR algorithms to coordinated adversarial attacks remains poorly understood. In this work, we present a novel framework for attacking some of the widely used OLTR algorithms. Our framework is designed to promote a set of target items so that they appear in the list of top-K recommendations for T - o(T) rounds, while simultaneously inducing linear regret in the learning algorithm. We propose two novel attack strategies: CascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical guarantees showing that both strategies require only O(log T) manipulations to succeed. Additionally, we supplement our theoretical analysis with empirical results on real-world data. 

**Abstract (ZH)**: 在线学习排序（OLTR）在信息检索和机器学习系统中扮演着关键角色，广泛应用于搜索引擎和内容推荐系统。然而，尽管OLTR算法被广泛采用，它们对协调式 adversarial 攻击的脆弱性仍然不甚了解。在本文中，我们提出了一种新的框架，用于攻击一些广泛使用的OLTR算法。我们的框架旨在促进一组目标项，使其在T-至T rounds内出现在前K项推荐列表中，同时在学习算法中诱导线性后悔。我们提出了两种新的攻击策略：CascadeOFA用于CascadeUCB1，PBMOFA用于PBM-UCB。我们提供了理论保证，表明这两种策略只需O(log T)次操纵即可成功。此外，我们还通过实际数据的实证结果补充了我们的理论分析。 

---
# Patient-specific Biomolecular Instruction Tuning 

**Title (ZH)**: 患者特异性生物分子指令调谐 

**Authors**: Irsyad Adam, Zekai Chen, David Laub, Shaun Porwal, Arda Pekis, Kevin Brown  

**Link**: [PDF](https://arxiv.org/pdf/2509.22853)  

**Abstract**: Proteomics data is essential to pathogenic understanding of a disease phenotype. In cancer, analysis of molecular signatures enables precision medicine through the identification of biological processes that drive individualized tumor progression, therapeutic resistance, and clinical heterogeneity. Recent advances in multimodal large language models (LLMs) have shown remarkable capacity to integrate and reason across heterogeneous data modalities. However, performing multi-modal language modeling for molecular understanding of patient-specific proteomics remains a significant challenge due to two barriers: (1) the lack of instruction-tuning datasets that enable clinical interpretation from proteomics data, and (2) the absence of language modeling architectures designed to capture the rich heterogeneity of molecular data. In this work, we introduce CPTAC-PROTSTRUCT, the first instruction tuning dataset for molecular understanding of oncology, comprising over 400k open-ended examples derived from individualized proteomic profiles curated from the largest national proteomics cancer study (CPTAC). Additionally, we propose KRONOS (Knowledge Representation of patient Omics Networks in Oncology via Structured tuning), a novel graph-LLM framework that leverages molecular interaction topology with proteomics to learn patient-specific graph representations for enhanced clinical reasoning. We show that KRONOS achieves competitive performance across benchmark clinical tasks, including molecular classification, temporal trajectory modeling, and tumor stage prediction from proteomics data. Ultimately, this approach empowers LLMs to understand patient-level pathogenesis, advancing precision medicine through more accurate diagnosis, prognosis, and treatment stratification. 

**Abstract (ZH)**: 蛋白质组学数据对于理解疾病的表型致病机制至关重要。在癌症中，分子标记的分析能够通过识别驱动个体肿瘤进展、治疗抵抗和临床异质性的生物过程，实现精准医疗。近期，多模态大型语言模型（LLMs）的进步展现了其整合和跨异质数据模态推理的巨大能力。然而，由于两个障碍，将多模态语言模型应用于患者特异性蛋白质组学的分子理解仍然面临重大挑战：（1）缺乏能够从蛋白质组学数据中进行临床解释的指令调优数据集；（2）缺乏能够捕捉分子数据丰富异质性的语言模型架构。在本文中，我们介绍了CPTAC-PROTSTRUCT，这是首个用于肿瘤学中分子理解的指令调优数据集，包含来自最大国家级蛋白质组学癌症研究（CPTAC）中个体化蛋白质谱绘制的超过40万个开放性示例。此外，我们提出了KRONOS（肿瘤学中基于结构调优的患者组学网络知识表示），这是一种新颖的图-LLM框架，通过利用蛋白质组学中的分子相互作用拓扑结构来学习患者特异性的图表示，以增强临床推理。我们展示了KRONOS在基准临床任务中的竞争力，包括分子分类、时间轨迹建模和从蛋白质组学数据中预测肿瘤分期。最终，这种方法使大型语言模型能够理解患者水平的病理机制，从而推动更加准确的诊断、预后和治疗分层，以实现精准医疗。 

---
# Adaptive Margin RLHF via Preference over Preferences 

**Title (ZH)**: 基于偏好的偏好适应性RLHF 

**Authors**: Yaswanth Chittepu, Prasann Singhal, Greg Durrett, Scott Niekum  

**Link**: [PDF](https://arxiv.org/pdf/2509.22851)  

**Abstract**: Margin-based optimization is fundamental to improving generalization and robustness in classification tasks. In the context of reward model learning from preferences within Reinforcement Learning from Human Feedback (RLHF), existing methods typically rely on no margins, fixed margins, or margins that are simplistic functions of preference ratings. However, such formulations often fail to account for the varying strengths of different preferences, for example some preferences are associated with larger margins between responses, or they rely on noisy margin information derived from ratings. We argue that modeling the strength of preferences can lead to better generalization and more faithful alignment. Furthermore, many existing methods that use adaptive margins assume access to accurate preference scores, which can be difficult for humans to provide reliably. We propose an approach that leverages preferences over preferences, that is annotations indicating which of two preferences reflects a stronger distinction. We use this ordinal signal to infer adaptive margins on a per-datapoint basis. We introduce an extension to Direct Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from preference-over-preference supervision, enabling improved discriminative and generative performance. Empirically, our method outperforms vanilla DPO, DPO with fixed margins, and DPO with ground-truth margins on the UltraFeedback dataset. Additionally, we show that there is a tradeoff between discriminative and generative performance: improving test classification accuracy, particularly by correctly labeling weaker preferences at the expense of stronger ones, can lead to a decline in generative quality. To navigate this tradeoff, we propose two sampling strategies to gather preference-over-preference labels: one favoring discriminative performance and one favoring generative performance. 

**Abstract (ZH)**: 基于偏好的加权优化对于提高分类任务的泛化能力和鲁棒性是基础的。在强化学习中有来自人类反馈（RLHF）的偏好强化学习中，现有方法通常依赖于没有加权、固定加权或基于偏好评分的简单加权函数。然而，这样的形式化往往未能考虑到不同偏好强度的不同，例如，某些偏好与响应之间的更大加权差相关，或者依赖于从评分中derive出的 noisy 加权信息。我们认为建模偏好强度可以导致更好的泛化和更忠实的对齐。此外，许多使用自适应加权的方法假定可以访问准确的偏好评分，而这对于人类来说可能是难以可靠提供的。我们提出了一种方法，利用偏好之间的偏好作为监督，即指示哪一种偏好表现出更强区分性的标注。我们使用这种序关系信号，在每个数据点基础上推断自适应加权。我们提出了直接偏好优化（DPO）的扩展DPO-PoP，该扩展方法纳入了偏好之间的偏好监督得到的自适应加权，从而提高了判别和生成性能。实验表明，我们的方法在UltraFeedback数据集上优于传统的DPO、带有固定加权的DPO和带有真实加权的DPO。此外，我们展示了判别和生成性能之间的权衡：通过正确标记较弱偏好而提高测试分类准确性，可能会导致生成质量下降。为此，我们提出了两种采样策略来收集偏好之间的偏好标签：一种侧重于判别性能，另一种侧重于生成性能。 

---
# Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data 

**Title (ZH)**: 边界之上：面向结构化数据的高效黑盒决策基攻击 

**Authors**: Roie Kazoom, Yuval Ratzabi, Etamar Rothstein, Ofer Hadar  

**Link**: [PDF](https://arxiv.org/pdf/2509.22850)  

**Abstract**: Adversarial robustness in structured data remains an underexplored frontier compared to vision and language domains. In this work, we introduce a novel black-box, decision-based adversarial attack tailored for tabular data. Our approach combines gradient-free direction estimation with an iterative boundary search, enabling efficient navigation of discrete and continuous feature spaces under minimal oracle access. Extensive experiments demonstrate that our method successfully compromises nearly the entire test set across diverse models, ranging from classical machine learning classifiers to large language model (LLM)-based pipelines. Remarkably, the attack achieves success rates consistently above 90%, while requiring only a small number of queries per instance. These results highlight the critical vulnerability of tabular models to adversarial perturbations, underscoring the urgent need for stronger defenses in real-world decision-making systems. 

**Abstract (ZH)**: 结构化数据的对抗鲁棒性相较于视觉和语言领域仍是一个未充分探索的前沿问题。在本文中，我们提出了一种针对表格数据的新型黑盒决策型对抗攻击方法。该方法结合了无梯度方向估计与迭代边界搜索，能够在最少的oracle访问下高效导航离散和连续特征空间。广泛实验表明，我们的方法成功地几乎将整个测试集中的多种模型（从经典机器学习分类器到基于大型语言模型的管道）攻击成功率保持在90%以上。这些结果强调了表格模型对对抗扰动的严重易受攻击性，突显了急需在实际决策系统中加强防御措施的重要性。 

---
# Multimodal Slice Interaction Network Enhanced by Transfer Learning for Precise Segmentation of Internal Gross Tumor Volume in Lung Cancer PET/CT Imaging 

**Title (ZH)**: 基于迁移学习增强的多模态切片交互网络在肺癌PET/CT成像中精确分割内部粗大肿瘤体积 

**Authors**: Yi Luo, Yike Guo, Hamed Hooshangnejad, Rui Zhang, Xue Feng, Quan Chen, Wil Ngwa, Kai Ding  

**Link**: [PDF](https://arxiv.org/pdf/2509.22841)  

**Abstract**: Lung cancer remains the leading cause of cancerrelated deaths globally. Accurate delineation of internal gross tumor volume (IGTV) in PET/CT imaging is pivotal for optimal radiation therapy in mobile tumors such as lung cancer to account for tumor motion, yet is hindered by the limited availability of annotated IGTV datasets and attenuated PET signal intensity at tumor boundaries. In this study, we present a transfer learningbased methodology utilizing a multimodal interactive perception network with MAMBA, pre-trained on extensive gross tumor volume (GTV) datasets and subsequently fine-tuned on a private IGTV cohort. This cohort constitutes the PET/CT subset of the Lung-cancer Unified Cross-modal Imaging Dataset (LUCID). To further address the challenge of weak PET intensities in IGTV peripheral slices, we introduce a slice interaction module (SIM) within a 2.5D segmentation framework to effectively model inter-slice relationships. Our proposed module integrates channel and spatial attention branches with depthwise convolutions, enabling more robust learning of slice-to-slice dependencies and thereby improving overall segmentation performance. A comprehensive experimental evaluation demonstrates that our approach achieves a Dice of 0.609 on the private IGTV dataset, substantially surpassing the conventional baseline score of 0.385. This work highlights the potential of transfer learning, coupled with advanced multimodal techniques and a SIM to enhance the reliability and clinical relevance of IGTV segmentation for lung cancer radiation therapy planning. 

**Abstract (ZH)**: 基于迁移学习的多模态交互感知网络在肺 cancer 内在粗略肿瘤体积分割中的应用：挑战及解决方案 

---
# Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN 

**Title (ZH)**: 看见并不等于相信：基于条件GAN的上下文感知对抗性补丁合成 

**Authors**: Roie Kazoom, Alon Goldberg, Hodaya Cohen, Ofer Hadar  

**Link**: [PDF](https://arxiv.org/pdf/2509.22836)  

**Abstract**: Adversarial patch attacks pose a severe threat to deep neural networks, yet most existing approaches rely on unrealistic white-box assumptions, untargeted objectives, or produce visually conspicuous patches that limit real-world applicability. In this work, we introduce a novel framework for fully controllable adversarial patch generation, where the attacker can freely choose both the input image x and the target class y target, thereby dictating the exact misclassification outcome. Our method combines a generative U-Net design with Grad-CAM-guided patch placement, enabling semantic-aware localization that maximizes attack effectiveness while preserving visual realism. Extensive experiments across convolutional networks (DenseNet-121, ResNet-50) and vision transformers (ViT-B/16, Swin-B/16, among others) demonstrate that our approach achieves state-of-the-art performance across all settings, with attack success rates (ASR) and target-class success (TCS) consistently exceeding 99%.
Importantly, we show that our method not only outperforms prior white-box attacks and untargeted baselines, but also surpasses existing non-realistic approaches that produce detectable artifacts. By simultaneously ensuring realism, targeted control, and black-box applicability-the three most challenging dimensions of patch-based attacks-our framework establishes a new benchmark for adversarial robustness research, bridging the gap between theoretical attack strength and practical stealthiness. 

**Abstract (ZH)**: 对抗补丁攻击对深度神经网络构成了严重威胁，但大多数现有方法依赖于不切实际的白盒假设、非目标攻击目标或产生视觉上显眼的补丁，限制了其实用性。在本工作中，我们引入了一种全新的可完全控制的对抗补丁生成框架，攻击者可以自由选择输入图像x和目标类别y，从而决定具体的分类错误结果。我们的方法结合了生成的U-Net设计与Grad-CAM指导的补丁放置，实现了语义意识的定位，最大化攻击效果同时保留视觉真实性。在卷积网络（DenseNet-121、ResNet-50）和视觉变压器（ViT-B/16、Swin-B/16等）上的广泛实验表明，我们的方法在所有设置中均达到最优性能，攻击成功率（ASR）和目标类成功（TCS）一致地超过99%。尤为重要的是，我们证明了我们的方法不仅优于之前的白盒攻击和非目标基准，还超越了现有不可靠的方法，这些方法会产生可检测的伪影。通过同时确保现实性、目标控制和黑盒适用性——补丁攻击最具有挑战性的三个维度——我们的框架为对抗鲁棒性研究建立了新的基准，填补了理论攻击强度与实际隐蔽性之间的差距。 

---
# Bridging Language Models and Formal Methods for Intent-Driven Optical Network Design 

**Title (ZH)**: 基于意图的 Optical 网络设计中语言模型与形式方法的桥梁构建 

**Authors**: Anis Bekri, Amar Abane, Abdella Battou, Saddek Bensalem  

**Link**: [PDF](https://arxiv.org/pdf/2509.22834)  

**Abstract**: Intent-Based Networking (IBN) aims to simplify network management by enabling users to specify high-level goals that drive automated network design and configuration. However, translating informal natural-language intents into formally correct optical network topologies remains challenging due to inherent ambiguity and lack of rigor in Large Language Models (LLMs). To address this, we propose a novel hybrid pipeline that integrates LLM-based intent parsing, formal methods, and Optical Retrieval-Augmented Generation (RAG). By enriching design decisions with domain-specific optical standards and systematically incorporating symbolic reasoning and verification techniques, our pipeline generates explainable, verifiable, and trustworthy optical network designs. This approach significantly advances IBN by ensuring reliability and correctness, essential for mission-critical networking tasks. 

**Abstract (ZH)**: 基于意图的网络（IBN）旨在通过允许用户指定高层次目标来简化网络管理，这些目标驱动自动化网络设计和配置。然而，由于大型语言模型（LLMs）固有的模糊性和严谨性的缺乏，将非正式的自然语言意图转换为正式正确的光网络拓扑仍然具有挑战性。为此，我们提出了一种新颖的混合流水线，该流水线结合了基于大型语言模型的意图解析、形式方法以及光检索增强生成（RAG）。通过使用领域特定的光网络标准丰富设计决策，并系统地结合符号推理和验证技术，我们的流水线生成可解释、可验证且可信赖的光网络设计。这种方法显著推进了IBN，确保了关键任务网络中的可靠性和正确性。 

---
# Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM 

**Title (ZH)**: 高效的细粒度GPU性能建模以支持分布式深度学习大语言模型 

**Authors**: Biyao Zhang, Mingkai Zheng, Debargha Ganguly, Xuecen Zhang, Vikash Singh, Vipin Chaudhary, Zhao Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.22832)  

**Abstract**: Training Large Language Models(LLMs) is one of the most compute-intensive tasks in high-performance computing. Predicting end-to-end training time for multi-billion parameter models distributed across hundreds of GPUs remains challenging due to complex interactions between transformer components, parallelism strategies(data, model, pipeline, tensor), and multi-tier communication. Learned models require costly sampling, while analytical models often struggle with real-world network and hardware complexities. We address this by decomposing LLMs into core computational primitives and modeling them with: (1) operator-level decomposition for fine-grained analysis; (2) lightweight sampling based hardware-aware prediction models for key operations; (3) an end-to-end prediction system integrating these components across complex parallelization strategies. Crucially, our methodology has been validated on two large-scale HPC systems. Our framework achieves low average prediction errors-4.98\% on Perlmutter(A100) and 9.38\% on Vista(GH200)-for models up to 20B parameters across 128 GPUs. Importantly, it runs entirely on CPUs, enabling rapid iteration over hardware configurations and training strategies without costly on-cluster experimentation. 

**Abstract (ZH)**: 大规模语言模型（LLMs）训练是高性能计算中计算密集型的任务。由于变压器组件、并行策略（数据、模型、管道、张量）以及多层通信之间的复杂相互作用，预测跨越数百个GPU的多百亿参数模型的端到端训练时间仍然具有挑战性。我们通过将LLMs分解为核心计算 primitive，并使用以下方法进行建模：（1）操作级别分解进行细粒度分析；（2）基于轻量级采样的硬件感知预测模型针对关键操作；（3）将这些组件集成到复杂并行化策略的端到端预测系统中。尤为重要的是，我们的方法已在两个大型HPC系统上进行了验证。我们的框架在Perlmutter(A100)上实现了低平均预测误差4.98%，在Vista(GH200)上实现了9.38%，适用于128个GPU上至多20B参数的模型。更重要的是，该框架完全在CPU上运行，允许在不进行昂贵的集群实验的情况下快速迭代硬件配置和训练策略。 

---
# Dynamic Buffers: Cost-Efficient Planning for Tabletop Rearrangement with Stacking 

**Title (ZH)**: 动态缓冲区：基于堆叠的餐桌 rearrangement 成本-effective 规划 

**Authors**: Arman Barghi, Hamed Hosseini, Seraj Ghasemi, Mehdi Tale Masouleh, Ahmad Kalhor  

**Link**: [PDF](https://arxiv.org/pdf/2509.22828)  

**Abstract**: Rearranging objects in cluttered tabletop environments remains a long-standing challenge in robotics. Classical planners often generate inefficient, high-cost plans by shuffling objects individually and using fixed buffers--temporary spaces such as empty table regions or static stacks--to resolve conflicts. When only free table locations are used as buffers, dense scenes become inefficient, since placing an object can restrict others from reaching their goals and complicate planning. Allowing stacking provides extra buffer capacity, but conventional stacking is static: once an object supports another, the base cannot be moved, which limits efficiency. To overcome these issues, a novel planning primitive called the Dynamic Buffer is introduced. Inspired by human grouping strategies, it enables robots to form temporary, movable stacks that can be transported as a unit. This improves both feasibility and efficiency in dense layouts, and it also reduces travel in large-scale settings where space is abundant. Compared with a state-of-the-art rearrangement planner, the approach reduces manipulator travel cost by 11.89% in dense scenarios with a stationary robot and by 5.69% in large, low-density settings with a mobile manipulator. Practicality is validated through experiments on a Delta parallel robot with a two-finger gripper. These findings establish dynamic buffering as a key primitive for cost-efficient and robust rearrangement planning. 

**Abstract (ZH)**: 动态缓冲在拥挤桌面环境中的对象重新布置仍是一个长期的机器人技术挑战。 

---
# MMPB: It's Time for Multi-Modal Personalization 

**Title (ZH)**: MMPB: 是时候实现多模态个性化了 

**Authors**: Jaeik Kim, Woojin Kim, Woohyeon Park, Jaeyoung Do  

**Link**: [PDF](https://arxiv.org/pdf/2509.22820)  

**Abstract**: Visual personalization is essential in user-facing AI systems such as smart homes and healthcare, where aligning model behavior with user-centric concepts is critical. However, recent large Vision-Language Models (VLMs), despite their broad applicability, remain underexplored in their ability to adapt to individual users. In this paper, we introduce MMPB, the first extensive benchmark for evaluating VLMs on personalization. MMPB comprises 10k image-query pairs and includes 111 personalizable concepts across four categories: humans, animals, objects, and characters, with the human category enriched with preference-grounded queries. We structure personalization into three main task types, each highlighting a different key property of VLMs. Using 23 widely used VLMs including both open- and closed-source models, we evaluate personalization performance via a three-stage protocol: concept injection, multi-turn dialogue, and personalized querying. Our findings indicate that most VLMs (including some closed-source models) struggle with personalization, particularly in maintaining consistency over dialogue, handling user preferences, and adapting to visual cues. Our analysis reveals that the challenges in VLM personalization (such as refusal behaviors and long-context forgetting) highlight substantial room for improvement. By identifying these limitations and offering a scalable benchmark, MMPB offers valuable insights and a solid foundation for future research toward truly personalized multi-modal AI. Project Page: this http URL 

**Abstract (ZH)**: 视觉个性化对于智能家庭和医疗等用户-facing AI 系统至关重要，其中模型行为与用户中心的概念对齐是关键。然而，尽管近期大规模视觉-语言模型(VLMs)具有广泛的应用前景，但在适应个体用户方面仍被严重忽视。本文介绍了MMPB，这是首个用于评估VLMs个性化能力的全面基准。MMPB包含10,000幅图像查询对，并涵盖了跨四大类别（人类、动物、物体和角色）的111个可个性化概念，其中人类类别还包含了基于偏好的查询。我们将个性化任务结构化为三大主要任务类型，每种任务类型均强调VLMs的不同关键属性。我们使用23个广泛使用的VLMs，包括开源和闭源模型，通过三阶段协议评估个性化性能：概念注入、多轮对话和个性化查询。我们的研究结果表明，大多数VLMs（包括一些闭源模型）在个性化方面面临挑战，特别是在对话一致性、处理用户偏好和适应视觉线索方面。我们的分析表明，VLMs个性化面临的挑战（如拒绝行为和长上下文遗忘）凸显了显著的改进空间。通过识别这些限制并提供一个可扩展的基准，MMPB为未来真正个性化多模态AI的研究提供了宝贵的见解和坚实的基础。项目页面：this http URL 

---
# MTRec: Learning to Align with User Preferences via Mental Reward Models 

**Title (ZH)**: MTRec: 基于心智奖励模型的学习用户偏好多模态对齐方法 

**Authors**: Mengchen Zhao, Yifan Gao, Yaqing Hou, Xiangyang Li, Pengjie Gu, Zhenhua Dong, Ruiming Tang, Yi Cai  

**Link**: [PDF](https://arxiv.org/pdf/2509.22807)  

**Abstract**: Recommendation models are predominantly trained using implicit user feedback, since explicit feedback is often costly to obtain. However, implicit feedback, such as clicks, does not always reflect users' real preferences. For example, a user might click on a news article because of its attractive headline, but end up feeling uncomfortable after reading the content. In the absence of explicit feedback, such erroneous implicit signals may severely mislead recommender systems. In this paper, we propose MTRec, a novel sequential recommendation framework designed to align with real user preferences by uncovering their internal satisfaction on recommended items. Specifically, we introduce a mental reward model to quantify user satisfaction and propose a distributional inverse reinforcement learning approach to learn it. The learned mental reward model is then used to guide recommendation models to better align with users' real preferences. Our experiments show that MTRec brings significant improvements to a variety of recommendation models. We also deploy MTRec on an industrial short video platform and observe a 7 percent increase in average user viewing time. 

**Abstract (ZH)**: 一种新型顺序推荐框架MTRec：通过揭示用户对推荐项目的内部满意度来引导推荐模型更好地契合用户的真实偏好 

---
# VideoScore2: Think before You Score in Generative Video Evaluation 

**Title (ZH)**: VideoScore2: 评分之前先思考 在生成视频评估中的应用 

**Authors**: Xuan He, Dongfu Jiang, Ping Nie, Minghao Liu, Zhengxuan Jiang, Mingyi Su, Wentao Ma, Junru Lin, Chun Ye, Yi Lu, Keming Wu, Benjamin Schneider, Quy Duc Do, Zhuofeng Li, Yiming Jia, Yuxuan Zhang, Guo Cheng, Haozhe Wang, Wangchunshu Zhou, Qunshu Lin, Yuanxing Zhang, Ge Zhang, Wenhao Huang, Wenhu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.22799)  

**Abstract**: Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: this https URL 

**Abstract (ZH)**: Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: [this https URL] 

---
# Generative Modeling and Decision Fusion for Unknown Event Detection and Classification Using Synchrophasor Data 

**Title (ZH)**: 基于同步相量数据的未知事件检测与分类的生成建模和决策融合 

**Authors**: Yi Hu, Zheyuan Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2509.22795)  

**Abstract**: Reliable detection and classification of power system events are critical for maintaining grid stability and situational awareness. Existing approaches often depend on limited labeled datasets, which restricts their ability to generalize to rare or unseen disturbances. This paper proposes a novel framework that integrates generative modeling, sliding-window temporal processing, and decision fusion to achieve robust event detection and classification using synchrophasor data. A variational autoencoder-generative adversarial network is employed to model normal operating conditions, where both reconstruction error and discriminator error are extracted as anomaly indicators. Two complementary decision strategies are developed: a threshold-based rule for computational efficiency and a convex hull-based method for robustness under complex error distributions. These features are organized into spatiotemporal detection and classification matrices through a sliding-window mechanism, and an identification and decision fusion stage integrates the outputs across PMUs. This design enables the framework to identify known events while systematically classifying previously unseen disturbances into a new category, addressing a key limitation of supervised classifiers. Experimental results demonstrate state-of-the-art accuracy, surpassing machine learning, deep learning, and envelope-based baselines. The ability to recognize unknown events further highlights the adaptability and practical value of the proposed approach for wide-area event analysis in modern power systems. 

**Abstract (ZH)**: 可靠的电力系统事件检测与分类对于维持电网稳定性和态势感知至关重要。现有方法往往依赖于有限的标注数据集，限制了它们对罕见或未见干扰的泛化能力。本文提出了一种新颖的框架，该框架结合了生成建模、滑动窗口时间处理和决策融合，利用同步相量数据实现稳健的事件检测与分类。采用变分自编码器-生成对抗网络来建模正常运行状态，其中重构误差和鉴别器误差被提取为异常指标。开发了两种互补的决策策略：基于阈值的规则以提高计算效率，以及基于凸包的方法以在复杂误差分布下提高鲁棒性。这些特征通过滑动窗口机制组织成时空检测与分类矩阵，并在识别与决策融合阶段集成PMU输出。该设计使框架能够识别已知事件，并系统地将未见干扰分类到新类别中，解决了监督分类器的一个关键局限性。实验结果表明，该方法在准确度上达到最新技术水平，超越了机器学习、深度学习和包络基线方法。能够识别未知事件进一步突显了所提出方法在现代电力系统广域事件分析中的适应性和实际价值。 

---
# Differentially Private Two-Stage Gradient Descent for Instrumental Variable Regression 

**Title (ZH)**: 差分隐私两阶段梯度下降法在工具变量回归中的应用 

**Authors**: Haodong Liang, Yanhao Jin, Krishnakumar Balasubramanian, Lifeng Lai  

**Link**: [PDF](https://arxiv.org/pdf/2509.22794)  

**Abstract**: We study instrumental variable regression (IVaR) under differential privacy constraints. Classical IVaR methods (like two-stage least squares regression) rely on solving moment equations that directly use sensitive covariates and instruments, creating significant risks of privacy leakage and posing challenges in designing algorithms that are both statistically efficient and differentially private. We propose a noisy two-state gradient descent algorithm that ensures $\rho$-zero-concentrated differential privacy by injecting carefully calibrated noise into the gradient updates. Our analysis establishes finite-sample convergence rates for the proposed method, showing that the algorithm achieves consistency while preserving privacy. In particular, we derive precise bounds quantifying the trade-off among privacy parameters, sample size, and iteration-complexity. To the best of our knowledge, this is the first work to provide both privacy guarantees and provable convergence rates for instrumental variable regression in linear models. We further validate our theoretical findings with experiments on both synthetic and real datasets, demonstrating that our method offers practical accuracy-privacy trade-offs. 

**Abstract (ZH)**: 我们研究差分隐私约束下的工具变量回归（IVaR）。经典的工具变量回归方法（如两阶段最小平方法）依赖于直接使用敏感协变量和工具变量求解矩方程，这产生了重大的隐私泄露风险，并给设计同时具备统计效率和差分隐私性的算法带来了挑战。我们提出了一种噪声二状态梯度下降算法，通过在梯度更新中注入精心校准的噪声来确保$\rho$-零集中差分隐私。我们的分析建立了所提方法的有限样本收敛速率，表明该算法既能保持一致性又能保护隐私。特别地，我们推导出了精确界定量化的隐私参数、样本量和迭代复杂度之间的权衡。据我们所知，这是首个同时提供工具变量回归在线性模型中差分隐私保证和可证明收敛速率的工作。我们还通过在合成数据集和真实数据集上的实验验证了我们的理论发现，证明了我们的方法提供了实用的准确性和隐私之间的权衡。 

---
# A theoretical guarantee for SyncRank 

**Title (ZH)**: SyncRank的理论保证 

**Authors**: Yang Rao  

**Link**: [PDF](https://arxiv.org/pdf/2509.22766)  

**Abstract**: We present a theoretical and empirical analysis of the SyncRank algorithm for recovering a global ranking from noisy pairwise comparisons. By adopting a complex-valued data model where the true ranking is encoded in the phases of a unit-modulus vector, we establish a sharp non-asymptotic recovery guarantee for the associated semidefinite programming (SDP) relaxation. Our main theorem characterizes a critical noise threshold - scaling as sigma = O(sqrt(n / log n)) - below which SyncRank achieves exact ranking recovery with high probability. Extensive experiments under this model confirm the theoretical predictions and demonstrate the algorithm's robustness across varying problem sizes and noise regimes. 

**Abstract (ZH)**: 我们提出了一种针对噪声双边比较进行全局排名恢复的SyncRank算法的理论和实证分析。通过采用复值数据模型，其中真实的排名编码在单位模向量的相位中，我们建立了与之相关的半定规划（SDP）松弛的精确非渐近恢复保证。我们的主要定理刻画了一个关键的噪声阈值——约为σ=O(√(n/logn))—在该阈值以下，SyncRank以高概率实现精确的排名恢复。在该模型下的大量实验证明了理论预测，并展示了该算法在不同问题规模和噪声条件下的稳健性。 

---
# In-Context Learning can Perform Continual Learning Like Humans 

**Title (ZH)**: 上下文学习可以像人类一样进行连续学习 

**Authors**: Liuwang Kang, Fan Wang, Shaoshan Liu, Hung-Chyun Chou, Chuan Lin, Ning Ding  

**Link**: [PDF](https://arxiv.org/pdf/2509.22764)  

**Abstract**: Large language models (LLMs) can adapt to new tasks via in-context learning (ICL) without parameter updates, making them powerful learning engines for fast adaptation. While extensive research has examined ICL as a few-shot learner, whether it can achieve long-term retention and cross-task knowledge accumulation when multitasks arrive sequentially remains underexplored. Motivated by human memory studies, we investigate the retention characteristics of ICL in multitask settings and extend it to in-context continual learning (ICCL), where continual learning ability emerges through task scheduling and prompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that, for specific large-language models, ICCL benefits from distributed practice (DP) in a manner analogous to humans, consistently revealing a spacing "sweet spot" for retention. Beyond retention performance, we propose a human-retention similarity metric to quantify how closely a continual-learning (CL) method aligns with human retention dynamics. Using this metric, we show that linear-attention models such as MAMBA and RWKV exhibit particularly human-like retention patterns, despite their retention performance lagging behind that of Transformer-based LLMs. Overall, our results establish ICCL as both cognitively plausible and practically effective, providing an inference-only CL paradigm that mitigates catastrophic forgetting and addresses the stability-plasticity dilemma in conventional CL methods. 

**Abstract (ZH)**: 大规模语言模型（LLMs）可以通过上下文学习（ICL）在不更新参数的情况下适应新任务，使其成为快速适应的强大学习引擎。虽然已有大量研究将ICL视为少样本学习者，但其在多任务按序列到来时是否能够实现长期保留和跨任务知识积累仍鲜有探索。受人类记忆研究的启发，我们研究了ICL在多任务设置中的保留特性，并将其扩展到上下文连续学习（ICCL），其中通过任务调度和提示重组逐渐发展出连续学习能力。Markov链基准实验表明，对于特定的大规模语言模型，ICCL以类似于人类的方式从分散练习中受益，持续揭示出一种“间隔优化”的保留“甜蜜点”。除了保留性能外，我们提出了一种人类保留相似度度量，用于量化连续学习（CL）方法与人类保留动态的接近程度。利用这一度量，我们展示了线性注意力模型如MAMBA和RWKV表现出特别类似人类的记忆模式，尽管它们的保留性能落后于基于Transformer的大规模语言模型。总体而言，我们的结果确立了ICCL作为认知上合理且实际有效的学习方法的地位，为避免灾难性遗忘并解决传统CL方法中的稳定性和可塑性困境提供了一种仅进行推理的CL范式。 

---
# UESA-Net: U-Shaped Embedded Multidirectional Shrinkage Attention Network for Ultrasound Nodule Segmentation 

**Title (ZH)**: UESA-Net: U型嵌入多方向收缩注意力网络-ultrasound结节分割 

**Authors**: Tangqi Shi, Pietro Lio  

**Link**: [PDF](https://arxiv.org/pdf/2509.22763)  

**Abstract**: Background: Breast and thyroid cancers pose an increasing public-health burden. Ultrasound imaging is a cost-effective, real-time modality for lesion detection and segmentation, yet suffers from speckle noise, overlapping structures, and weak global-local feature interactions. Existing networks struggle to reconcile high-level semantics with low-level spatial details. We aim to develop a segmentation framework that bridges the semantic gap between global context and local detail in noisy ultrasound images.
Methods: We propose UESA-Net, a U-shaped network with multidirectional shrinkage attention. The encoder-decoder architecture captures long-range dependencies and fine-grained structures of lesions. Within each encoding block, attention modules operate along horizontal, vertical, and depth directions to exploit spatial details, while a shrinkage (threshold) strategy integrates prior knowledge and local features. The decoder mirrors the encoder but applies a pairwise shrinkage mechanism, combining prior low-level physical cues with corresponding encoder features to enhance context modeling.
Results: On two public datasets - TN3K (3493 images) and BUSI (780 images) - UESA-Net achieved state-of-the-art performance with intersection-over-union (IoU) scores of 0.8487 and 0.6495, respectively.
Conclusions: UESA-Net effectively aggregates multidirectional spatial information and prior knowledge to improve robustness and accuracy in breast and thyroid ultrasound segmentation, demonstrating superior performance to existing methods on multiple benchmarks. 

**Abstract (ZH)**: 背景：乳腺癌和甲状腺癌对公共健康造成日益增大的负担。超声成像是一种成本有效且实时的病变检测和分割方法，但受到斑点噪声、重叠结构和全局-局部特征交互微弱的限制。现有网络难以平衡高层语义与低层空间细节。本文旨在开发一种分割框架，以在噪声超声图像中弥合全局上下文与局部细节之间的语义差距。

方法：我们提出了UESA-Net，这是一种具有多方向收缩注意机制的U形网络。编码-解码架构捕捉到病变的长距离依赖性和精细结构。在每个编码块中，注意力模块沿横向、纵向和深度方向操作，以利用空间细节，而收缩（阈值）策略则结合了先验知识和局部特征。解码器镜像编码器结构，但应用成对收缩机制，结合先验低级物理线索和相应的编码特征以增强上下文建模。

结果：在两个公开数据集TN3K（3493张图像）和BUSI（780张图像）上，UESA-Net分别实现了交并比（IoU）值0.8487和0.6495，达到了最先进的性能。

结论：UESA-Net有效整合了多方向空间信息和先验知识，提高了乳腺和甲状腺超声分割的鲁棒性和准确性，展示了在多个基准上优于现有方法的性能。 

---
# MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning 

**Title (ZH)**: MILR：通过测试时潜在推理提高多模态图像生成 

**Authors**: Yapeng Mi, Hengli Li, Yanpeng Zhao, Chenxi Li, Huimin Wu, Xiaojian Ma, Song-Chun Zhu, Ying Nian Wu, Qing Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.22761)  

**Abstract**: Reasoning-augmented machine learning systems have shown improved performance in various domains, including image generation. However, existing reasoning-based methods for image generation either restrict reasoning to a single modality (image or text) or rely on high-quality reasoning data for fine-tuning. To tackle these limitations, we propose MILR, a test-time method that jointly reasons over image and text in a unified latent vector space. Reasoning in MILR is performed by searching through vector representations of discrete image and text tokens. Practically, this is implemented via the policy gradient method, guided by an image quality critic. We instantiate MILR within the unified multimodal understanding and generation (MUG) framework that natively supports language reasoning before image synthesis and thus facilitates cross-modal reasoning. The intermediate model outputs, which are to be optimized, serve as the unified latent space, enabling MILR to operate entirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE, achieving state-of-the-art results on all benchmarks. Notably, on knowledge-intensive WISE, MILR attains an overall score of 0.63, improving over the baseline by 80%. Our further analysis indicates that joint reasoning in the unified latent space is the key to its strong performance. Moreover, our qualitative studies reveal MILR's non-trivial ability in temporal and cultural reasoning, highlighting the efficacy of our reasoning method. 

**Abstract (ZH)**: 增强推理的机器学习系统在各个领域显示出了改进的性能，包括图像生成。然而，现有的基于推理的图像生成方法要么将推理限制在单一模态（图像或文本）上，要么依赖高质量的推理数据进行微调。为了解决这些问题，我们提出了一种名为MILR的测试时方法，它在统一的潜在向量空间中同时对图像和文本进行推理。在MILR中，通过搜索离散图像和文本标记的向量表示来进行推理。实际中，这一过程通过策略梯度方法实现，并由图像质量批评家引导。我们将在原生支持语言推理后再进行图像合成的统一多模态理解和生成（MUG）框架中实现MILR，从而促进跨模态推理。作为要优化的中间模型输出充当统一的潜在空间，使MILR完全在测试时运行。我们在GenEval、T2I-CompBench和WISE上评估了MILR，实现了所有基准上的最佳结果。特别是在知识密集型的WISE上，MILR获得了0.63的整体得分，比基线高出80%。进一步的分析表明，统一潜在空间中的联合推理是其高性能的关键。此外，我们的定性研究还揭示了MILR在时间和文化推理方面的非平凡能力，突显了我们推理方法的有效性。 

---
# Red Teaming Quantum-Resistant Cryptographic Standards: A Penetration Testing Framework Integrating AI and Quantum Security 

**Title (ZH)**: 红队测试量子抗性加密标准：融合AI与量子安全的渗透测试框架 

**Authors**: Petar Radanliev  

**Link**: [PDF](https://arxiv.org/pdf/2509.22757)  

**Abstract**: This study presents a structured approach to evaluating vulnerabilities within quantum cryptographic protocols, focusing on the BB84 quantum key distribution method and National Institute of Standards and Technology (NIST) approved quantum-resistant algorithms. By integrating AI-driven red teaming, automated penetration testing, and real-time anomaly detection, the research develops a framework for assessing and mitigating security risks in quantum networks. The findings demonstrate that AI can be effectively used to simulate adversarial attacks, probe weaknesses in cryptographic implementations, and refine security mechanisms through iterative feedback. The use of automated exploit simulations and protocol fuzzing provides a scalable means of identifying latent vulnerabilities, while adversarial machine learning techniques highlight novel attack surfaces within AI-enhanced cryptographic processes. This study offers a comprehensive methodology for strengthening quantum security and provides a foundation for integrating AI-driven cybersecurity practices into the evolving quantum landscape. 

**Abstract (ZH)**: 本研究提供了一种结构化的方法来评估量子加密协议中的漏洞，重点关注BB84量子密钥分发方法和美国国家标准与技术研究院（NIST）批准的量子抗攻击算法。通过整合基于AI的红队攻击、自动化渗透测试和实时异常检测，研究开发了一种评估和缓解量子网络中安全风险的框架。研究结果表明，AI可以有效用于模拟对手攻击、探测 cryptographic 实施中的弱点，并通过迭代反馈精炼安全机制。自动化漏洞利用模拟和协议 fuzzing 提供了一种可扩展的方法来识别潜在漏洞，而对抗性机器学习技术则突显了增强型 cryptographic 过程中的新型攻击面。本研究提供了一种全面的方法来加强量子安全，并为将基于AI的网络安全实践集成到不断发展的量子环境中奠定了基础。 

---
# Persistent Autoregressive Mapping with Traffic Rules for Autonomous Driving 

**Title (ZH)**: 持续自回归映射结合交通规则的自动驾驶技术 

**Authors**: Shiyi Liang, Xinyuan Chang, Changjie Wu, Huiyuan Yan, Yifan Bai, Xinran Liu, Hang Zhang, Yujian Yuan, Shuang Zeng, Mu Xu, Xing Wei  

**Link**: [PDF](https://arxiv.org/pdf/2509.22756)  

**Abstract**: Safe autonomous driving requires both accurate HD map construction and persistent awareness of traffic rules, even when their associated signs are no longer visible. However, existing methods either focus solely on geometric elements or treat rules as temporary classifications, failing to capture their persistent effectiveness across extended driving sequences. In this paper, we present PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel framework that performs autoregressive co-construction of lane vectors and traffic rules from visual observations. Our approach introduces two key mechanisms: Map-Rule Co-Construction for processing driving scenes in temporal segments, and Map-Rule Cache for maintaining rule consistency across these segments. To properly evaluate continuous and consistent map generation, we develop MapDRv2, featuring improved lane geometry annotations. Extensive experiments demonstrate that PAMR achieves superior performance in joint vector-rule mapping tasks, while maintaining persistent rule effectiveness throughout extended driving sequences. 

**Abstract (ZH)**: 持久性车道向量与交通规则联合构建的自回归映射（PAMR） 

---
# Self-driving cars: Are we there yet? 

**Title (ZH)**: 自动驾驶汽车：我们到了吗？ 

**Authors**: Merve Atasever, Zhuochen Liu, Qingpei Li, Akshay Hitendra Shah, Hans Walker, Jyotirmoy V. Deshmukh, Rahul Jain  

**Link**: [PDF](https://arxiv.org/pdf/2509.22754)  

**Abstract**: Autonomous driving remains a highly active research domain that seeks to enable vehicles to perceive dynamic environments, predict the future trajectories of traffic agents such as vehicles, pedestrians, and cyclists and plan safe and efficient future motions. To advance the field, several competitive platforms and benchmarks have been established to provide standardized datasets and evaluation protocols. Among these, leaderboards by the CARLA organization and nuPlan and the Waymo Open Dataset have become leading benchmarks for assessing motion planning algorithms. Each offers a unique dataset and challenging planning problems spanning a wide range of driving scenarios and conditions. In this study, we present a comprehensive comparative analysis of the motion planning methods featured on these three leaderboards. To ensure a fair and unified evaluation, we adopt CARLA leaderboard v2.0 as our common evaluation platform and modify the selected models for compatibility. By highlighting the strengths and weaknesses of current approaches, we identify prevailing trends, common challenges, and suggest potential directions for advancing motion planning research. 

**Abstract (ZH)**: 自主泊车 remains 一个高度活跃的研究领域，旨在使车辆能够感知动态环境、预测交通参与者的未来轨迹（如车辆、行人和骑自行车者），并规划安全高效的未来行动。为了推进该领域的发展，已经建立了多个竞争平台和基准，提供了标准化的数据集和评估协议。在这些平台中，CARLA组织的排行榜、nuPlan以及Waymo开放数据集已成为评估运动规划算法的主要基准。每个平台都提供了独特数据集和广泛驾驶场景下的具有挑战性的规划问题。在本研究中，我们对这三个排行榜上的运动规划方法进行了全面的比较分析。为了确保公平统一的评估，我们采用CARLA排行榜v2.0作为共同评估平台，并对所选模型进行修改以实现兼容性。通过突出当前方法的优点和不足，我们识别了现有趋势、普遍挑战，并建议了促进运动规划研究进展的潜在方向。 

---
# Variance-Bounded Evaluation without Ground Truth: VB-Score 

**Title (ZH)**: 无 ground truth 条件下的方差有界评估：VB-Score 

**Authors**: Kaihua Ding  

**Link**: [PDF](https://arxiv.org/pdf/2509.22751)  

**Abstract**: Reliable evaluation is a central challenge in machine learning when tasks lack ground truth labels or involve ambiguity and noise. Conventional frameworks, rooted in the Cranfield paradigm and label-based metrics, fail in such cases because they cannot assess how robustly a system performs under uncertain interpretations. We introduce VB-Score, a variance-bounded evaluation framework that measures both effectiveness and robustness without requiring ground truth. Given a query or input, VB-Score enumerates plausible interpretations, assigns probabilities, and evaluates output by expected success penalized by variance, rewarding consistent performance across intents. We provide a formal analysis of VB-Score, establishing range, monotonicity, and stability properties, and relate it to risk-sensitive measures such as mean-variance utility. Experiments on ambiguous queries and entity-centric retrieval tasks show that VB-Score surfaces robustness differences hidden by conventional metrics. By enabling reproducible, label-free evaluation, VB-Score offers a principled foundation for benchmarking machine learning systems in ambiguous or label-scarce domains. 

**Abstract (ZH)**: 可靠的评估是机器学习中的一项中心挑战，特别是在任务缺乏 ground truth 标签或涉及模糊性和噪声的情况下。传统的框架根植于 Cranfield 帕累托思想和基于标签的度量标准，无法在这种情况下发挥作用，因为它们无法评估系统在不确定解释下的鲁棒性能。我们提出了 VB-Score，这是一种方差受限的评估框架，能够在无需 ground truth 的情况下衡量有效性和鲁棒性。给定查询或输入，VB-Score 列举可能的解释，分配概率，并通过预期成功惩罚方差进行评估，奖励意图一致的性能。我们对 VB-Score 进行了形式化分析，阐明了其范围、单调性和稳定性性质，并将其与均值方差效用等风险敏感度量进行了关联。实验表明，VB-Score 可以揭示传统度量所隐藏的鲁棒性差异。通过使评估可重复且无需标签，VB-Score 为在模糊或标签稀缺领域评估机器学习系统提供了有原则的基础。 

---
# MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions 

**Title (ZH)**: MIRAGE: 多跳推理结合歧义评估用于虚假问题 

**Authors**: Jeonghyun Park, Ingeol Baek, Seunghyun Yoon, Haeun Jang, Aparna Garimella, Akriti Jain, Nedim Lipka, Hwanhee Lee  

**Link**: [PDF](https://arxiv.org/pdf/2509.22750)  

**Abstract**: Real-world Multi-hop Question Answering (QA) often involves ambiguity that is inseparable from the reasoning process itself. This ambiguity creates a distinct challenge, where multiple reasoning paths emerge from a single question, each requiring independent resolution. Since each sub-question is ambiguous, the model must resolve ambiguity at every step. Thus, answering a single question requires handling multiple layers of ambiguity throughout the reasoning chain. We find that current Large Language Models (LLMs) struggle in this setting, typically exploring wrong reasoning paths and producing incomplete answers. To facilitate research on multi-hop ambiguity, we introduce MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE), a benchmark designed to analyze and evaluate this challenging intersection of ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142 high-quality examples of ambiguous multi-hop questions, categorized under a taxonomy of syntactic, general, and semantic ambiguity, and curated through a rigorous multi-LLM verification pipeline. Our experiments reveal that even state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity combined with multi-step inference is a distinct and significant challenge. To establish a robust baseline, we propose CLarifying Ambiguity with a Reasoning and InstructiON (CLARION), a multi-agent framework that significantly outperforms existing approaches on MIRAGE, paving the way for more adaptive and robust reasoning systems. 

**Abstract (ZH)**: 真实世界多跳问答中存在的推理过程中不可避免的歧义性提出了独特的挑战：MIRAGE多歧义推理基准 

---
# Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment 

**Title (ZH)**: 面向有害微调的安全路由对齐防御MoE大语言模型 

**Authors**: Jaehan Kim, Minkyoo Song, Seungwon Shin, Sooel Son  

**Link**: [PDF](https://arxiv.org/pdf/2509.22745)  

**Abstract**: Recent large language models (LLMs) have increasingly adopted the Mixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily depend on a superficial safety mechanism in which harmful inputs are routed safety-critical experts. However, our analysis reveals that routing decisions for harmful inputs drift significantly after fine-tuning, exposing a critical vulnerability to harmful fine-tuning (HFT) attacks. Existing defenses, primarily designed for monolithic LLMs, are less effective for MoE LLMs as they fail to prevent drift in harmful input routing. To address this limitation, we propose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE directly mitigates routing drift by penalizing the gap between the routing weights of a fine-tuned model and those of the initial safety-aligned model, thereby preserving the safety-aligned routing of harmful inputs to safety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to 141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks, reducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while maintaining task utility within 1% degradation and incurring only 2% overhead. It significantly outperforms state-of-the-art defense methods for safeguarding LLM fine-tuning and remains effective in recent large-scale MoE LLMs such as gpt-oss and Llama 4. Our implementation is available at this https URL. 

**Abstract (ZH)**: Recent Large Language Models (LLMs)采用Mixture-of-Experts (MoE)架构以提高效率存在安全漏洞：SafeMoE——一种针对MoE LLMs的安全微调方法 

---
# Index-MSR: A high-efficiency multimodal fusion framework for speech recognition 

**Title (ZH)**: 基于索引的多模态融合框架：高效率语音识别 

**Authors**: Jinming Chen, Lu Wang, Zheshu Song, Wei Deng  

**Link**: [PDF](https://arxiv.org/pdf/2509.22744)  

**Abstract**: Driven by large scale datasets and LLM based architectures, automatic speech recognition (ASR) systems have achieved remarkable improvements in accuracy. However, challenges persist for domain-specific terminology, and short utterances lacking semantic coherence, where recognition performance often degrades significantly. In this work, we present Index-MSR, an efficient multimodal speech recognition framework. At its core is a novel Multimodal Fusion Decoder (MFD), which effectively incorporates text-related information from videos (e.g., subtitles and presentation slides) into the speech recognition. This cross-modal integration not only enhances overall ASR accuracy but also yields substantial reductions in substitution errors. Extensive evaluations on both an in-house subtitle dataset and a public AVSR dataset demonstrate that Index-MSR achieves sota accuracy, with substitution errors reduced by 20,50%. These results demonstrate that our approach efficiently exploits text-related cues from video to improve speech recognition accuracy, showing strong potential in applications requiring strict audio text synchronization, such as audio translation. 

**Abstract (ZH)**: 基于大规模数据集和基于LLM的架构，自动语音识别（ASR）系统在准确率上取得了显著改进。然而，特定领域术语和缺乏语义连贯性的短语音片段仍存在挑战，这些情况下识别性能往往会显著下降。本文介绍了Index-MSR，一种高效的多模态语音识别框架。其核心是一款新颖的多模态融合解码器（MFD），能够有效将视频中的文本相关信息（如字幕和演示幻灯片）融入到语音识别中。这种跨模态集成不仅提升了整体ASR准确率，还大幅减少了替换错误。在内部字幕数据集和公共AVSR数据集上的 extensive 评估表明，Index-MSR 达到了最先进的准确率，替换错误减少了20.5%。这些结果表明，我们的方法能够高效利用视频中的文本线索来提高语音识别准确率，特别是在需要严格音频文本同步的应用场景中，如语音翻译方面展现出强大的潜力。 

---
# Societal Capacity Assessment Framework: Measuring Resilience to Inform Advanced AI Risk Management 

**Title (ZH)**: 社会能力评估框架：衡量韧性以指导先进人工智能风险管理 

**Authors**: Milan Gandhi, Peter Cihon, Owen Larter, Rebecca Anselmetti  

**Link**: [PDF](https://arxiv.org/pdf/2509.22742)  

**Abstract**: Risk assessments for advanced AI systems require evaluating both the models themselves and their deployment contexts. We introduce the Societal Capacity Assessment Framework (SCAF), an indicators-based approach to measuring a society's vulnerability, coping capacity, and adaptive capacity in response to AI-related risks. SCAF adapts established resilience analysis methodologies to AI, enabling organisations to ground risk management in insights about country-level deployment conditions. It can also support stakeholders in identifying opportunities to strengthen societal preparedness for emerging AI capabilities. By bridging disparate literatures and the "context gap" in AI evaluation, SCAF promotes more holistic risk assessment and governance as advanced AI systems proliferate globally. 

**Abstract (ZH)**: 高级AI系统的风险评估需要评估模型本身及其部署环境。我们介绍了社会能力评估框架（SCAF），这是一种基于指标的方法，用于衡量社会在应对AI相关风险时的脆弱性、应对能力和适应能力。SCAF将现有的韧性分析方法应用于AI，使组织能够基于国家层面部署条件的风险管理洞察。它还可以帮助利益相关者识别加强社会对新兴AI能力准备的机会。通过弥合不同文献之间的鸿沟以及AI评估中的“环境差距”，SCAF促进了更全面的风险评估和治理，随着高级AI系统的全球普及。 

---
# Learning What To Hear: Boosting Sound-Source Association For Robust Audiovisual Instance Segmentation 

**Title (ZH)**: 学习听见什么：提升稳健的音视频实例分割中的声源关联 

**Authors**: Jinbae Seo, Hyeongjun Kwon, Kwonyoung Kim, Jiyoung Lee, Kwanghoon Sohn  

**Link**: [PDF](https://arxiv.org/pdf/2509.22740)  

**Abstract**: Audiovisual instance segmentation (AVIS) requires accurately localizing and tracking sounding objects throughout video sequences. Existing methods suffer from visual bias stemming from two fundamental issues: uniform additive fusion prevents queries from specializing to different sound sources, while visual-only training objectives allow queries to converge to arbitrary salient objects. We propose Audio-Centric Query Generation using cross-attention, enabling each query to selectively attend to distinct sound sources and carry sound-specific priors into visual decoding. Additionally, we introduce Sound-Aware Ordinal Counting (SAOC) loss that explicitly supervises sounding object numbers through ordinal regression with monotonic consistency constraints, preventing visual-only convergence during training. Experiments on AVISeg benchmark demonstrate consistent improvements: +1.64 mAP, +0.6 HOTA, and +2.06 FSLA, validating that query specialization and explicit counting supervision are crucial for accurate audiovisual instance segmentation. 

**Abstract (ZH)**: 音频中心查询生成与音觉感知计数在音视频实例分割中的应用 

---
# Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models 

**Title (ZH)**: 无痛激活调控：一种自动轻量级的后训练大型语言模型方法 

**Authors**: Sasha Cui, Zhongren Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.22739)  

**Abstract**: Language models (LMs) are typically post-trained for desired capabilities and behaviors via weight-based or prompt-based steering, but the former is time-consuming and expensive, and the latter is not precisely controllable and often requires manual trial-and-error. While activation steering (AS) promises a cheap, fast, and controllable alternative to the two existing post-training methods, current AS techniques require hand-crafted prompt pairs or labor-intensive feature annotation, making them more inconvenient than the plug-and-play methods such as Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of fully automated methods that make AS readily usable with any given labeled dataset, with no need for prompt construction, feature labeling, or human intervention. We evaluate PAS on three open-weight models (Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks; we find that PAS reliably improves performance for behavior tasks, but not for intelligence-oriented tasks. The introspective variant (iPAS) delivers the strongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8% on Alignment). We also show PAS delivers additional gains on top of In-Context Learning (ICL) and SFT. PAS constructs a fast, lightweight activation vector that can be cheaply trained, easily stored, and activated at will. Our results provide a characterization of where AS helps, where it fails, and how to deploy it as a practical, automated LM post-training option. 

**Abstract (ZH)**: Painless Activation Steering: A Fully Automated Approach for Controllable Post-Training of Language Models 

---
# CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models 

**Title (ZH)**: CompareBench: 视觉对比推理基准在视觉-语言模型中的应用 

**Authors**: Jie Cai, Kangning Yang, Lan Fu, Jiaming Ding, Jinlong Li, Huiming Sun, Daitao Xing, Jinglin Shen, Zibo Meng  

**Link**: [PDF](https://arxiv.org/pdf/2509.22737)  

**Abstract**: We introduce CompareBench, a benchmark for evaluating visual comparison reasoning in vision-language models (VLMs), a fundamental yet understudied skill. CompareBench consists of 1000 QA pairs across four tasks: quantity (600), temporal (100), geometric (200), and spatial (100). It is derived from two auxiliary datasets that we constructed: TallyBench (2000 counting images with QA) and HistCaps (515 historical images with bilingual captions). We evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but also reveal critical limitations: even the strongest models consistently fail at temporal ordering and spatial relations, and they often make mistakes in basic counting and geometric comparisons that are trivial for humans. These findings demonstrate that visual comparison remains a systematic blind spot for current VLMs. By providing controlled, diverse, and diagnostic evaluation, CompareBench establishes a foundation for advancing more reliable multimodal reasoning. 

**Abstract (ZH)**: CompareBench: 一个评估视觉语言模型中视觉比较推理能力的标准数据集 

---
# Consistency Models as Plug-and-Play Priors for Inverse Problems 

**Title (ZH)**: 一致性模型作为即插即用先验用于逆问题 

**Authors**: Merve Gülle, Junno Yun, Yaşar Utku Alçalar, Mehmet Akçakaya  

**Link**: [PDF](https://arxiv.org/pdf/2509.22736)  

**Abstract**: Diffusion models have found extensive use in solving numerous inverse problems. Such diffusion inverse problem solvers aim to sample from the posterior distribution of data given the measurements, using a combination of the unconditional score function and an approximation of the posterior related to the forward process. Recently, consistency models (CMs) have been proposed to directly predict the final output from any point on the diffusion ODE trajectory, enabling high-quality sampling in just a few NFEs. CMs have also been utilized for inverse problems, but existing CM-based solvers either require additional task-specific training or utilize data fidelity operations with slow convergence, not amenable to large-scale problems. In this work, we reinterpret CMs as proximal operators of a prior, enabling their integration into plug-and-play (PnP) frameworks. We propose a solver based on PnP-ADMM, which enables us to leverage the fast convergence of conjugate gradient method. We further accelerate this with noise injection and momentum, dubbed PnP-CM, and show it maintains the convergence properties of the baseline PnP-ADMM. We evaluate our approach on a variety of inverse problems, including inpainting, super-resolution, Gaussian deblurring, and magnetic resonance imaging (MRI) reconstruction. To the best of our knowledge, this is the first CM trained for MRI datasets. Our results show that PnP-CM achieves high-quality reconstructions in as few as 4 NFEs, and can produce meaningful results in 2 steps, highlighting its effectiveness in real-world inverse problems while outperforming comparable CM-based approaches. 

**Abstract (ZH)**: 扩散模型在解决众多逆问题中找到了广泛的应用。这样的扩散逆问题求解器旨在利用无条件得分函数与前向过程相关联的后验近似共同从给定测量的数据后验分布中采样。最近，一致性模型（CMs）已被提出，可以直接从扩散微分方程轨迹上的任何点预测最终输出，从而使高质量采样仅需少量NFEs。CMs也被用于逆问题，但现有的CM基求解器要么需要附加的任务特定训练，要么使用数据保真操作且收敛缓慢，不适用于大规模问题。在这项工作中，我们重新解释CMs作为先验的近邻算子，使其能够集成到即插即用（PnP）框架中。我们提出一种基于PnP-ADMM的方法，这使我们能够利用共轭梯度法的快速收敛特性。我们进一步通过噪声注入和动量加速这种方法，命名为PnP-CM，并证明其保持了基础PnP-ADMM的收敛特性。我们在图像修复、超分辨率、高斯去模糊和磁共振成像（MRI）重建等多种逆问题上进行了评估。据我们所知，这是首个用于MRI数据集的CM训练方法。我们的结果表明，PnP-CM可以在仅4个NFEs内实现高质量的重建，并且可以在两步内生成有意义的结果，突显了其在真实世界逆问题中的有效性，并优于相似的CM基方法。 

---
# Regulating the Agency of LLM-based Agents 

**Title (ZH)**: 基于LLM的智能体的代理调节 

**Authors**: Seán Boddy, Joshua Joseph  

**Link**: [PDF](https://arxiv.org/pdf/2509.22735)  

**Abstract**: As increasingly capable large language model (LLM)-based agents are developed, the potential harms caused by misalignment and loss of control grow correspondingly severe. To address these risks, we propose an approach that directly measures and controls the agency of these AI systems. We conceptualize the agency of LLM-based agents as a property independent of intelligence-related measures and consistent with the interdisciplinary literature on the concept of agency. We offer (1) agency as a system property operationalized along the dimensions of preference rigidity, independent operation, and goal persistence, (2) a representation engineering approach to the measurement and control of the agency of an LLM-based agent, and (3) regulatory tools enabled by this approach: mandated testing protocols, domain-specific agency limits, insurance frameworks that price risk based on agency, and agency ceilings to prevent societal-scale risks. We view our approach as a step toward reducing the risks that motivate the ``Scientist AI'' paradigm, while still capturing some of the benefits from limited agentic behavior. 

**Abstract (ZH)**: 随着日益强大的基于大型语言模型（LLM）的代理系统的开发，由不对齐和失去控制引起的风险变得愈加严重。为应对这些风险，我们提出了一个直接衡量和控制这些AI系统自主性的方法。我们将基于LLM的代理的自主性概念化为独立于智能相关指标的一种属性，并与有关自主性的跨学科文献保持一致。我们提出（1）自主性作为一种系统属性，可以通过偏好刚性、独立运行和目标持久性这三个维度来实现操作化，（2）一种用于衡量和控制基于LLM的代理自主性的表示工程方法，以及（3）基于此方法的监管工具：强制性的测试协议、特定领域的自主性限制、基于自主性定价的风险保险框架，以及防止社会层面风险的自主性上限。我们认为，我们的方法是朝着减少由“科学家AI”范式所驱动的风险迈出的一步，同时仍然能够捕捉由有限的自主行为带来的某些益处。 

---
# Automated Formative Feedback for Short-form Writing: An LLM-Driven Approach and Adoption Analysis 

**Title (ZH)**: 短篇写作的自动形成性反馈：一种由LLM驱动的方法与采纳分析 

**Authors**: Tiago Fernandes Tavares, Luciano Pereira Soares  

**Link**: [PDF](https://arxiv.org/pdf/2509.22734)  

**Abstract**: This paper explores the development and adoption of AI-based formative feedback in the context of biweekly reports in an engineering Capstone program. Each student is required to write a short report detailing their individual accomplishments over the past two weeks, which is then assessed by their advising professor. An LLM-powered tool was developed to provide students with personalized feedback on their draft reports, guiding them toward improved completeness and quality. Usage data across two rounds revealed an initial barrier to adoption, with low engagement rates. However, students who engaged in the AI feedback system demonstrated the ability to use it effectively, leading to improvements in the completeness and quality of their reports. Furthermore, the tool's task-parsing capabilities provided a novel approach to identify potential student organizational tasks and deliverables. The findings suggest initial skepticism toward the tool with a limited adoption within the studied context, however, they also highlight the potential for AI-driven tools to provide students and professors valuable insights and formative support. 

**Abstract (ZH)**: 本文探索了基于人工智能的形成性反馈在工程综合性课程中两周报告中的发展与采用情况。每个学生需要撰写一份简短的报告，详细说明过去两周的个人成就，该报告随后由指导教授评估。开发了一种基于大语言模型的工具，为学生提供其草稿报告的个性化反馈，引导他们提高报告的完整性和质量。两轮使用数据表明，初期采用存在一定障碍，参与率较低。然而，采用人工智能反馈系统的学生成功利用了该系统，报告的完整性和质量得以提升。此外，该工具的任务解析能力为识别潜在的学生组织任务和交付成果提供了一种新颖的方法。研究结果表明，在所研究的背景下，对该工具的初步怀疑导致有限采用，但同时也突显了人工智能驱动工具为学生和教授提供有价值的见解和形成性支持的潜力。 

---
# Rebuild AC Power Flow Models with Graph Attention Networks 

**Title (ZH)**: 基于图注意力网络重构AC功率流模型 

**Authors**: Yuting Hu, Jinjun Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2509.22733)  

**Abstract**: A full power flow (PF) model is a complete representation of the physical power network. Traditional model-based methods rely on the full PF model to implement power flow analysis. In practice, however, some PF model parameters can be inaccurate or even unavailable due to the uncertainties or dynamics in the power systems. Moreover, because the power network keeps evolving with possibly changing topology, the generalizability of a PF model to different network sizes and typologies should be considered. In this paper, we propose a PF rebuild model based on graph attention networks (GAT) by constructing a new graph based on the real and imaginary parts of voltage at each bus. By comparing with two state-of-the-art PF rebuild models for different standard IEEE power system cases and their modified topology variants, we demonstrate the feasibility of our method. Experimental results show that our proposed model achieves better accuracy for a changing network and can generalize to different networks with less accuracy discount. 

**Abstract (ZH)**: 基于图注意力网络的全功率流重建模型 

---
# Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks 

**Title (ZH)**: 双向意图推理增强LLMs对多轮 Jailbreak 攻击的防御能力 

**Authors**: Haibo Tong, Dongcheng Zhao, Guobin Shen, Xiang He, Dachuan Lin, Feifei Zhao, Yi Zeng  

**Link**: [PDF](https://arxiv.org/pdf/2509.22732)  

**Abstract**: The remarkable capabilities of Large Language Models (LLMs) have raised significant safety concerns, particularly regarding "jailbreak" attacks that exploit adversarial prompts to bypass safety alignment mechanisms. Existing defense research primarily focuses on single-turn attacks, whereas multi-turn jailbreak attacks progressively break through safeguards through by concealing malicious intent and tactical manipulation, ultimately rendering conventional single-turn defenses ineffective. To address this critical challenge, we propose the Bidirectional Intention Inference Defense (BIID). The method integrates forward request-based intention inference with backward response-based intention retrospection, establishing a bidirectional synergy mechanism to detect risks concealed within seemingly benign inputs, thereby constructing a more robust guardrails that effectively prevents harmful content generation. The proposed method undergoes systematic evaluation compared with a no-defense baseline and seven representative defense methods across three LLMs and two safety benchmarks under 10 different attack methods. Experimental results demonstrate that the proposed method significantly reduces the Attack Success Rate (ASR) across both single-turn and multi-turn jailbreak attempts, outperforming all existing baseline methods while effectively maintaining practical utility. Notably, comparative experiments across three multi-turn safety datasets further validate the proposed model's significant advantages over other defense approaches. 

**Abstract (ZH)**: 大型语言模型的双向意图推理防御（BIID）：应对多轮 Jailbreak 攻击的安全挑战 

---
# Multi-Modal Sentiment Analysis with Dynamic Attention Fusion 

**Title (ZH)**: 多模态情感分析中的动态注意融合 

**Authors**: Sadia Abdulhalim, Muaz Albaghdadi, Moshiur Farazi  

**Link**: [PDF](https://arxiv.org/pdf/2509.22729)  

**Abstract**: Traditional sentiment analysis has long been a unimodal task, relying solely on text. This approach overlooks non-verbal cues such as vocal tone and prosody that are essential for capturing true emotional intent. We introduce Dynamic Attention Fusion (DAF), a lightweight framework that combines frozen text embeddings from a pretrained language model with acoustic features from a speech encoder, using an adaptive attention mechanism to weight each modality per utterance. Without any finetuning of the underlying encoders, our proposed DAF model consistently outperforms both static fusion and unimodal baselines on a large multimodal benchmark. We report notable gains in F1-score and reductions in prediction error and perform a variety of ablation studies that support our hypothesis that the dynamic weighting strategy is crucial for modeling emotionally complex inputs. By effectively integrating verbal and non-verbal information, our approach offers a more robust foundation for sentiment prediction and carries broader impact for affective computing applications -- from emotion recognition and mental health assessment to more natural human computer interaction. 

**Abstract (ZH)**: 传统情感分析长期以来一直是一个单模态任务，仅依赖文本。这种做法忽视了如音调和语调等非言语线索，这些线索对于捕捉真实的情感意图至关重要。我们 introduce 动态注意力融合（Dynamic Attention Fusion，DAF），这是一个轻量级框架，结合了预训练语言模型的冻结文本嵌入和语音编码器的声学特征，使用自适应注意力机制为每个utterance加权不同模态。不微调底层编码器的情况下，我们提出的DAF模型在大型多模态基准测试中持续优于静态融合和单模态基线。我们报告了F1分数的显著提高和预测误差的减少，并进行了多种消融研究，支持我们假设动态加权策略对于建模情感复杂输入至关重要。通过有效整合言语和非言语信息，我们的方法为情感预测提供了更稳健的基础，并对情感计算应用具有更广泛的影响——从情绪识别和心理健康评估到更自然的人机交互。 

---
# Prompt-aware classifier free guidance for diffusion models 

**Title (ZH)**: 基于提示感知的分类器 Free 指导的扩散模型 

**Authors**: Xuanhao Zhang, Chang Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.22728)  

**Abstract**: Diffusion models have achieved remarkable progress in image and audio generation, largely due to Classifier-Free Guidance. However, the choice of guidance scale remains underexplored: a fixed scale often fails to generalize across prompts of varying complexity, leading to oversaturation or weak alignment. We address this gap by introducing a prompt-aware framework that predicts scale-dependent quality and selects the optimal guidance at inference. Specifically, we construct a large synthetic dataset by generating samples under multiple scales and scoring them with reliable evaluation metrics. A lightweight predictor, conditioned on semantic embeddings and linguistic complexity, estimates multi-metric quality curves and determines the best scale via a utility function with regularization. Experiments on MSCOCO~2014 and AudioCaps show consistent improvements over vanilla CFG, enhancing fidelity, alignment, and perceptual preference. This work demonstrates that prompt-aware scale selection provides an effective, training-free enhancement for pretrained diffusion backbones. 

**Abstract (ZH)**: 差分模型在图像和音频生成中的进步 largely得益于 Classifier-Free Guidance。然而，指导尺度的选择仍然没有得到充分探索：固定尺度往往无法适应不同复杂度提示的泛化，导致过度饱和或对齐不足。我们通过引入一个提示感知框架来填补这一空白，该框架预测尺度依赖的质量并选择最佳指导规模。具体来说，我们通过在多个尺度下生成样本并使用可靠的评估指标对其进行评分，构造了一个大规模的合成数据集。一个轻量级的预测器，基于语义嵌入和语言复杂性进行条件化，估计多指标质量曲线，并通过带正则化的效用函数确定最佳尺度。在 MSCOCO 2014 和 AudioCaps 上的实验表明，与 vanilla CFG 相比，此工作一致地提高了保真度、对齐和感知偏好。这项工作证明了提示感知尺度选择为预训练的差分模型骨架提供了有效且无需训练的增强。 

---
# A Meta-Analysis of LLM Effects on Students across Qualification, Socialisation, and Subjectification 

**Title (ZH)**: LLM对学生在资格、社会化和主体性影响的元分析 

**Authors**: Jiayu Huang, Ruoxin Ritter Wang, Jen-Hao Liu, Boming Xia, Yue Huang, Ruoxi Sun, Jason, Jinan Zou  

**Link**: [PDF](https://arxiv.org/pdf/2509.22725)  

**Abstract**: Large language models (LLMs) are increasingly positioned as solutions for education, yet evaluations often reduce their impact to narrow performance metrics. This paper reframes the question by asking "what kind of impact should LLMs have in education?" Drawing on Biesta's tripartite account of good education: qualification, socialisation, and subjectification, we present a meta-analysis of 133 experimental and quasi-experimental studies (k = 188). Overall, the impact of LLMs on student learning is positive but uneven. Strong effects emerge in qualification, particularly when LLMs function as tutors in sustained interventions. Socialisation outcomes appear more variable, concentrated in sustained, reflective interventions. Subjectification, linked to autonomy and learner development, remains fragile, with improvements confined to small-scale, long-term studies. This purpose-level view highlights design as the decisive factor: without scaffolds for participation and agency, LLMs privilege what is easiest to measure while neglecting broader aims of education. For HCI and education, the issue is not just whether LLMs work, but what futures they enable or foreclose. 

**Abstract (ZH)**: 大型语言模型（LLMs）在教育中的影响不应局限于狭隘的性能指标：基于Biesta的教育三元解释，LLMs在教育中的影响种类是什么？ 

---
# A Data-Driven Framework for Digital Transformation in Smart Cities: Integrating AI, Dashboards, and IoT Readiness 

**Title (ZH)**: 面向智能城市的基于数据的数字转型框架：集成AI、仪表盘和物联网 readiness 

**Authors**: Ángel Lloret, Jesús Peral, Antonio Ferrández, María Auladell, Rafael Muñoz  

**Link**: [PDF](https://arxiv.org/pdf/2509.22721)  

**Abstract**: Digital transformation (DT) has become a strategic priority for public administrations, particularly due to the need to deliver more efficient and citizen-centered services and respond to societal expectations, ESG (Environmental, Social, and Governance) criteria, and the United Nations Sustainable Development Goals (UN SDGs). In this context, the main objective of this study is to propose an innovative methodology to automatically evaluate the level of digital transformation (DT) in public sector organizations. The proposed approach combines traditional assessment methods with Artificial Intelligence (AI) techniques. The methodology follows a dual approach: on the one hand, surveys are conducted using specialized staff from various public entities; on the other, AI-based models (including neural networks and transformer architectures) are used to estimate the DT level of the organizations automatically. Our approach has been applied to a real-world case study involving local public administrations in the Valencian Community (Spain) and shown effective performance in assessing DT. While the proposed methodology has been validated in a specific local context, its modular structure and dual-source data foundation support its international scalability, acknowledging that administrative, regulatory, and DT maturity factors may condition its broader applicability. The experiments carried out in this work include (i) the creation of a domain-specific corpus derived from the surveys and websites of several organizations, used to train the proposed models; (ii) the use and comparison of diverse AI methods; and (iii) the validation of our approach using real data. The integration of technologies such as the IoT, sensor networks, and AI-based analytics can significantly support resilient, agile urban environments and the transition towards more effective and sustainable Smart City models. 

**Abstract (ZH)**: 数字转型（DT）已成为公共管理的战略优先事项，特别是在提供更高效和以公民为中心的服务以及回应社会期望、ESG（环境、社会和治理）标准和联合国可持续发展目标（UN SDGs）方面。在此背景下，本研究的主要目标是提出一种创新方法，自动评估公共部门组织的数字转型水平。所提出的方法将传统评估方法与人工智能（AI）技术相结合。该方法采用双轨策略：一方面采用来自各类公共机构的专业人员进行问卷调查；另一方面使用基于AI的模型（包括神经网络和变换器架构）自动估计组织的数字转型水平。该方法在西班牙瓦伦西亚自治区的地方公共管理机构的实际案例研究中得到应用，并展示了其评估数字转型的有效性。虽然所提出的方法在特定的地方背景下得到了验证，但其模块化结构和多数据源基础使其具有国际扩展性，认识到行政、监管和数字转型成熟度等因素可能对其更广泛的应用产生影响。本研究中的实验包括：（i）从多个组织的调查和网站中创建特定领域的语料库，用于训练所提出模型；（ii）使用和比较多种AI方法；以及（iii）使用实证数据验证该方法。集成诸如物联网、传感器网络和基于AI的分析技术等技术可以显著支持具有韧性和敏捷性的城市环境，并实现更有效和可持续的智能城市模型。 

---
# LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning 

**Title (ZH)**: 布局智能体：面向空间布局规划的视觉-语言引导组成性扩散方法 

**Authors**: Zezhong Fan, Xiaohan Li, Luyi Ma, Kai Zhao, Liang Peng, Topojoy Biswas, Evren Korpeoglu, Kaushiki Nag, Kannan Achan  

**Link**: [PDF](https://arxiv.org/pdf/2509.22720)  

**Abstract**: Designing realistic multi-object scenes requires not only generating images, but also planning spatial layouts that respect semantic relations and physical plausibility. On one hand, while recent advances in diffusion models have enabled high-quality image generation, they lack explicit spatial reasoning, leading to unrealistic object layouts. On the other hand, traditional spatial planning methods in robotics emphasize geometric and relational consistency, but they struggle to capture semantic richness in visual scenes. To bridge this gap, in this paper, we propose LayoutAgent, an agentic framework that unifies vision-language reasoning with compositional diffusion for layout generation. Given multiple input images with target objects in them, our method first employs visual-language model to preprocess the inputs through segmentation, object size estimation, scene graph construction, and prompt rewriting. Then we leverage compositional diffusion-a method traditionally used in robotics-to synthesize bounding boxes that respect object relations encoded in the scene graph for spatial layouts. In the end, a foreground-conditioned image generator composes the complete scene by rendering the objects into the planned layout guided by designed prompts. Experiments demonstrate that LayoutAgent outperforms other state-of-the-art layout generation models in layout coherence, spatial realism and aesthetic alignment. 

**Abstract (ZH)**: 设计现实的多对象场景不仅需要生成图像，还需要规划遵守语义关系和物理合理性的空间布局。为弥合这一差距，本文提出了一种名为LayoutAgent的代理框架，该框架将视觉语言推理与组合扩散相结合，用于布局生成。 

---
# IBiT: Utilizing Inductive Biases to Create a More Data Efficient Attention Mechanism 

**Title (ZH)**: IBiT：利用归纳偏置创建一种更数据高效的关注机制 

**Authors**: Adithya Giri  

**Link**: [PDF](https://arxiv.org/pdf/2509.22719)  

**Abstract**: In recent years, Transformer-based architectures have become the dominant method for Computer Vision applications. While Transformers are explainable and scale well with dataset size, they lack the inductive biases of Convolutional Neural Networks. While these biases may be learned on large datasets, we show that introducing these inductive biases through learned masks allow Vision Transformers to learn on much smaller datasets without Knowledge Distillation. These Transformers, which we call Inductively Biased Image Transformers (IBiT), are significantly more accurate on small datasets, while retaining the explainability Transformers. 

**Abstract (ZH)**: 基于诱导偏置的图像变压器：使用学习掩模在小数据集上学习的计算机视觉变换器 

---
# TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant? 

**Title (ZH)**: TRUEBench: 语言模型响应能否满足生产力助手的现实world约束？ 

**Authors**: Jiho Park, Jongyoon Song, Minjin Choi, Kyuho Heo, Taehun Huh, Ji Won Kim  

**Link**: [PDF](https://arxiv.org/pdf/2509.22715)  

**Abstract**: Large language models (LLMs) are increasingly integral as productivity assistants, but existing benchmarks fall short in rigorously evaluating their real-world instruction-following capabilities. Current benchmarks often (i) lack sufficient multilinguality, (ii) fail to capture the implicit constraints inherent in user requests, and (iii) overlook the complexities of multi-turn dialogue. To address these critical gaps and provide a more realistic assessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation Benchmark)1, a novel benchmark specifically designed for LLM-based productivity assistants. TRUEBench distinguishes itself by featuring input prompts across 12 languages, incorporating intra-instance multilingual instructions, employing rigorous evaluation criteria to capture both explicit and implicit constraints, and including complex multi-turn dialogue scenarios with both accumulating constraints and context switches. Furthermore, to ensure reliability in evaluation, we refined constraints using an LLM validator. Extensive experiments demonstrate that TRUEBench presents significantly greater challenges than existing benchmarks; for instance, a strong model like OpenAI o1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and realistic assessment of LLMs in practical productivity settings, highlighting their capabilities and limitations. 

**Abstract (ZH)**: TRUEBench：信任的现实世界使用评估基准 

---
# Localizing Adversarial Attacks To Produces More Imperceptible Noise 

**Title (ZH)**: 定位 adversarial 攻击以生成更具不可感知性的噪声 

**Authors**: Pavan Reddy, Aditya Sanjay Gujral  

**Link**: [PDF](https://arxiv.org/pdf/2509.22710)  

**Abstract**: Adversarial attacks in machine learning traditionally focus on global perturbations to input data, yet the potential of localized adversarial noise remains underexplored. This study systematically evaluates localized adversarial attacks across widely-used methods, including FGSM, PGD, and C&W, to quantify their effectiveness, imperceptibility, and computational efficiency. By introducing a binary mask to constrain noise to specific regions, localized attacks achieve significantly lower mean pixel perturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved Structural Similarity Index (SSIM) compared to global attacks. However, these benefits come at the cost of increased computational effort and a modest reduction in Attack Success Rate (ASR). Our results highlight that iterative methods, such as PGD and C&W, are more robust to localization constraints than single-step methods like FGSM, maintaining higher ASR and imperceptibility metrics. This work provides a comprehensive analysis of localized adversarial attacks, offering practical insights for advancing attack strategies and designing robust defensive systems. 

**Abstract (ZH)**: 局部对抗攻击在机器学习中的传统研究主要集中在输入数据的全局扰动，而局部对抗噪声的应用潜力尚未充分探索。本研究系统评估了广泛使用的FGSM、PGD和C&W等方法的局部对抗攻击，以量化其有效性、不可感知性和计算效率。通过引入二进制掩码限制噪声到特定区域，局部攻击实现了显著更低的平均像素扰动、更高的峰值信噪比（PSNR）和改进的结构相似性指数（SSIM），但这些优势伴随着计算努力增加以及轻微的攻击成功率（ASR）下降。我们的结果表明，迭代方法如PGD和C&W对局部化约束更为 robust，保持了较高的ASR和不可感知性。本研究为局部对抗攻击提供了全面分析，为攻击策略的改进和设计 robust 防御系统提供了实用见解。 

---
# GZSL-MoE: Apprentissage G{é}n{é}ralis{é} Z{é}ro-Shot bas{é} sur le M{é}lange d'Experts pour la Segmentation S{é}mantique de Nuages de Points 3DAppliqu{é} {à} un Jeu de Donn{é}es d'Environnement de Collaboration Humain-Robot 

**Title (ZH)**: GZSL-MoE: 专家混合基于零-shot 通用学习的点云语义分割方法及其在人类-机器人协作环境数据集上的应用 

**Authors**: Ahed Alboody  

**Link**: [PDF](https://arxiv.org/pdf/2509.22708)  

**Abstract**: Generative Zero-Shot Learning approach (GZSL) has demonstrated significant potential in 3D point cloud semantic segmentation tasks. GZSL leverages generative models like GANs or VAEs to synthesize realistic features (real features) of unseen classes. This allows the model to label unseen classes during testing, despite being trained only on seen classes. In this context, we introduce the Generalized Zero-Shot Learning based-upon Mixture-of-Experts (GZSL-MoE) model. This model incorporates Mixture-of-Experts layers (MoE) to generate fake features that closely resemble real features extracted using a pre-trained KPConv (Kernel Point Convolution) model on seen classes. The main contribution of this paper is the integration of Mixture-of-Experts into the Generator and Discriminator components of the Generative Zero-Shot Learning model for 3D point cloud semantic segmentation, applied to the COVERED dataset (CollabOratiVE Robot Environment Dataset) for Human-Robot Collaboration (HRC) environments. By combining the Generative Zero-Shot Learning model with Mixture-of- Experts, GZSL-MoE for 3D point cloud semantic segmentation provides a promising solution for understanding complex 3D environments, especially when comprehensive training data for all object classes is unavailable. The performance evaluation of the GZSL-MoE model highlights its ability to enhance performance on both seen and unseen classes. Keywords Generalized Zero-Shot Learning (GZSL), 3D Point Cloud, 3D Semantic Segmentation, Human-Robot Collaboration, COVERED (CollabOratiVE Robot Environment Dataset), KPConv, Mixture-of Experts 

**Abstract (ZH)**: 基于混合专家的泛化零样本学习方法（GZSL-MoE）在3D点云语义分割任务中的应用 

---
# Intelligent Load Balancing in Cloud Computer Systems 

**Title (ZH)**: 云计算机系统中的智能负载均衡 

**Authors**: Leszek Sliwko  

**Link**: [PDF](https://arxiv.org/pdf/2509.22704)  

**Abstract**: Cloud computing is an established technology allowing users to share resources on a large scale, never before seen in IT history. A cloud system connects multiple individual servers in order to process related tasks in several environments at the same time. Clouds are typically more cost-effective than single computers of comparable computing performance. The sheer physical size of the system itself means that thousands of machines may be involved. The focus of this research was to design a strategy to dynamically allocate tasks without overloading Cloud nodes which would result in system stability being maintained at minimum cost. This research has added the following new contributions to the state of knowledge: (i) a novel taxonomy and categorisation of three classes of schedulers, namely OS-level, Cluster and Big Data, which highlight their unique evolution and underline their different objectives; (ii) an abstract model of cloud resources utilisation is specified, including multiple types of resources and consideration of task migration costs; (iii) a virtual machine live migration was experimented with in order to create a formula which estimates the network traffic generated by this process; (iv) a high-fidelity Cloud workload simulator, based on a month-long workload traces from Google's computing cells, was created; (v) two possible approaches to resource management were proposed and examined in the practical part of the manuscript: the centralised metaheuristic load balancer and the decentralised agent-based system. The project involved extensive experiments run on the University of Westminster HPC cluster, and the promising results are presented together with detailed discussions and a conclusion. 

**Abstract (ZH)**: 云计算是一种已确立的技术，允许用户大规模共享资源，这在IT史上前所未见。云系统连接多个独立服务器，以便在同一时间处理多个环境中的相关任务。与性能相当的单台计算机相比，云系统通常更具成本效益。系统的物理规模巨大，意味着可能涉及成千上万台机器。本研究的重点是设计一种策略，以动态分配任务而不 overloaded 云节点，从而在最小成本下维持系统稳定性。本研究为现有知识增添了以下新贡献：（i）提出了一种新颖的调度器分类法，包括OS级、集群和大数据三类，突显了它们的独特演化历程并强调了它们的不同目标；（ii）规定了一个云资源利用的抽象模型，包括多种类型资源以及任务迁移成本的考虑；（iii）实验了虚拟机在线迁移，以创建估计此过程产生网络流量的公式；（iv）基于谷歌计算单元一个月的工作负载追踪，创建了一个高保真度的云工作负载模拟器；（v）在手稿的实践部分提出了两种资源管理方法：集中式的元启发式负载均衡器和去中心化的基于代理的系统。该项目在威斯敏斯特大学高性能计算集群上进行了大量的实验，展示了令人鼓舞的结果，并附有详细的讨论和结论。 

---
# AccessEval: Benchmarking Disability Bias in Large Language Models 

**Title (ZH)**: AccessEval：评估大型语言模型中的残疾偏见 

**Authors**: Srikant Panda, Amit Agarwal, Hitesh Laxmichand Patel  

**Link**: [PDF](https://arxiv.org/pdf/2509.22703)  

**Abstract**: Large Language Models (LLMs) are increasingly deployed across diverse domains but often exhibit disparities in how they handle real-life queries. To systematically investigate these effects within various disability contexts, we introduce \textbf{AccessEval (Accessibility Evaluation)}, a benchmark evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9 disability types using paired Neutral and Disability-Aware Queries. We evaluated model outputs with metrics for sentiment, social perception, and factual accuracy.
Our analysis reveals that responses to disability-aware queries tend to have a more negative tone, increased stereotyping, and higher factual error compared to neutral queries. These effects show notable variation by domain and disability type, with disabilities affecting hearing, speech, and mobility disproportionately impacted. These disparities reflect persistent forms of ableism embedded in model behavior.
By examining model performance in real-world decision-making contexts, we better illuminate how such biases can translate into tangible harms for disabled users. This framing helps bridges the gap between technical evaluation and user impact, reinforcing importance of bias mitigation in day-to-day applications. Our dataset is publicly available at: this https URL 

**Abstract (ZH)**: 大规模语言模型（LLMs）在不同领域中的应用越来越多，但它们在处理现实查询时常常表现出差异。为了系统地研究这些差异在不同残疾人背景下的影响，我们提出了**AccessEval（可访问性评估）**基准，该基准评估了21个开源和闭源的大规模语言模型在6个真实世界领域和9种残疾人类型上的配对中立和残疾人意识查询。我们使用情感、社会认知和事实准确性指标评估模型输出。

我们的分析发现，针对残疾人意识查询的回答往往具有更负面的语气、增加的刻板印象和更高的事实错误率，相较于中立查询。这些影响在不同领域和残疾人类型上表现出明显的差异，听觉、语言和行动障碍的残疾人影响尤为显著。这些差异反映了嵌入在模型行为中的持久性听障主义。

通过在现实世界决策背景中评估模型性能，我们更好地揭示了这些偏见如何转化为残疾用户的具体伤害。这种框架有助于弥合技术评估与用户影响之间的差距，增强了在日常应用中减少偏见的重要性。我们的数据集可在以下链接获取：[this https URL]。 

---
# Enhancing Cluster Scheduling in HPC: A Continuous Transfer Learning for Real-Time Optimization 

**Title (ZH)**: 增强高性能计算中的聚类调度：一种实时优化的连续迁移学习 

**Authors**: Leszek Sliwko, Jolanta Mizera-Pietraszko  

**Link**: [PDF](https://arxiv.org/pdf/2509.22701)  

**Abstract**: This study presents a machine learning-assisted approach to optimize task scheduling in cluster systems, focusing on node-affinity constraints. Traditional schedulers like Kubernetes struggle with real-time adaptability, whereas the proposed continuous transfer learning model evolves dynamically during operations, minimizing retraining needs. Evaluated on Google Cluster Data, the model achieves over 99% accuracy, reducing computational overhead and improving scheduling latency for constrained tasks. This scalable solution enables real-time optimization, advancing machine learning integration in cluster management and paving the way for future adaptive scheduling strategies. 

**Abstract (ZH)**: 基于机器学习辅助的方法在群集系统中优化任务调度，关注节点亲和性约束 

---
# Advancing Audio-Visual Navigation Through Multi-Agent Collaboration in 3D Environments 

**Title (ZH)**: 通过多代理协作在3D环境中的音视频导航进步 

**Authors**: Hailong Zhang, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2509.22698)  

**Abstract**: Intelligent agents often require collaborative strategies to achieve complex tasks beyond individual capabilities in real-world scenarios. While existing audio-visual navigation (AVN) research mainly focuses on single-agent systems, their limitations emerge in dynamic 3D environments where rapid multi-agent coordination is critical, especially for time-sensitive applications like emergency response. This paper introduces MASTAVN (Multi-Agent Scalable Transformer Audio-Visual Navigation), a scalable framework enabling two agents to collaboratively localize and navigate toward an audio target in shared 3D environments. By integrating cross-agent communication protocols and joint audio-visual fusion mechanisms, MASTAVN enhances spatial reasoning and temporal synchronization. Through rigorous evaluation in photorealistic 3D simulators (Replica and Matterport3D), MASTAVN achieves significant reductions in task completion time and notable improvements in navigation success rates compared to single-agent and non-collaborative baselines. This highlights the essential role of spatiotemporal coordination in multi-agent systems. Our findings validate MASTAVN's effectiveness in time-sensitive emergency scenarios and establish a paradigm for advancing scalable multi-agent embodied intelligence in complex 3D environments. 

**Abstract (ZH)**: 多Agent可扩展变换音频视觉导航（MASTAVN）：多Agent协同定位与导航的时空协调机制 

---
# Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment 

**Title (ZH)**: 使用精选文本提示学习超光谱图像以实现高效的多模态对齐 

**Authors**: Abhiroop Chatterjee, Susmita Ghosh  

**Link**: [PDF](https://arxiv.org/pdf/2509.22697)  

**Abstract**: As data requirements continue to grow, efficient learning increasingly depends on the curation and distillation of high-value data rather than brute-force scaling of model sizes. In the case of a hyperspectral image (HSI), the challenge is amplified by the high-dimensional 3D voxel structure, where each spatial location is associated with hundreds of contiguous spectral channels. While vision and language models have been optimized effectively for natural image or text tasks, their cross-modal alignment in the hyperspectral domain remains an open and underexplored problem. In this article, we make an attempt to optimize a Vision-Language Model (VLM) for hyperspectral scene understanding by exploiting a CLIP-style contrastive training framework. Our framework maps voxel-level embeddings from a vision backbone onto the latent space of a frozen large embedding model (LEM), where a trainable probe aligns vision features with the model's textual token representations. The two modalities are aligned via a contrastive loss restricted to a curated set of hard (closest wrong classes) and semi-hard (random distractors) negatives, along with positive pairs. To further enhance alignment, descriptive prompts that encode class semantics are introduced and act as structured anchors for the HSI embeddings. It is seen that the proposed method updates only 0.07 percent of the total parameters, yet yields state-of-the-art performance. For example, on Indian Pines (IP) the model produces better results over unimodal and multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa ($\kappa$), while on Pavia University (PU) data it provides gains of +0.69 OA and +0.90 $\kappa$. Moreover, this is achieved with the set of parameters, nearly 50$\times$ smaller than DCTN and 90$\times$ smaller than SS-TMNet. 

**Abstract (ZH)**: 随着数据需求不断增长，高效的learn过程 increasingly依赖于高质量数据的策划和提炼，而不是简单地扩大模型规模。在高光谱图像（HSI）的情况下，由于其高维度的3D体素结构，每个空间位置关联着上百个连续的光谱通道，挑战进一步放大。虽然视觉和语言模型已在自然图像或文本任务中得到了有效优化，但在高光谱域中的跨模态对齐仍是一个开放且未充分探索的问题。本文尝试通过利用CLIP风格的对比训练框架优化一个Vision-Language模型（VLM）以进行高光谱场景理解。该框架将视觉主干的体素级嵌入映射到一个冻结的大嵌入模型（LEM）的潜在空间中，其中可训练的探针将视觉特征与模型的文本标记表示对齐。通过限制在策划的硬（最接近的错误类别）和半硬（随机分散者）负样本集内的对比损失，以及正样本对，两模态得以对齐。为了进一步增强对齐，引入了描述性提示以编码类别语义，作为HSI嵌入的结构锚点。结果显示，所提出的方法仅更新了总参数的0.07%，但能达到最先进的性能。例如，在Indian Pines（IP）数据集上，模型相对于单模态和多模态基线方法在总体精度（OA）上提升了0.92，在卡帕系数（$\kappa$）上提升了1.60；而在Pavia University（PU）数据集上，模型提供了0.69的OA和0.90的$\kappa$的提升。此外，这实现了参数量几乎是DCTN的50倍少，SS-TMNet的90倍少。 

---
# PISA: An AI Pipeline for Interpretable-by-design Survival Analysis Providing Multiple Complexity-Accuracy Trade-off Models 

**Title (ZH)**: PISA：一种用于可解释设计生存分析的人工智能管道，提供多种复杂性-准确性trade-off模型 

**Authors**: Thalea Schlender, Catharina J.A. Romme, Yvette M. van der Linden, Luc R.C.W. van Lonkhuijzen, Peter A.N. Bosman, Tanja Alderliesten  

**Link**: [PDF](https://arxiv.org/pdf/2509.22673)  

**Abstract**: Survival analysis is central to clinical research, informing patient prognoses, guiding treatment decisions, and optimising resource allocation. Accurate time-to-event predictions not only improve quality of life but also reveal risk factors that shape clinical practice. For these models to be relevant in healthcare, interpretability is critical: predictions must be traceable to patient-specific characteristics, and risk factors should be identifiable to generate actionable insights for both clinicians and researchers. Traditional survival models often fail to capture non-linear interactions, while modern deep learning approaches, though powerful, are limited by poor interpretability.
We propose a Pipeline for Interpretable Survival Analysis (PISA) - a pipeline that provides multiple survival analysis models that trade off complexity and performance. Using multiple-feature, multi-objective feature engineering, PISA transforms patient characteristics and time-to-event data into multiple survival analysis models, providing valuable insights into the survival prediction task. Crucially, every model is converted into simple patient stratification flowcharts supported by Kaplan-Meier curves, whilst not compromising on performance. While PISA is model-agnostic, we illustrate its flexibility through applications of Cox regression and shallow survival trees, the latter avoiding proportional hazards assumptions.
Applied to two clinical benchmark datasets, PISA produced interpretable survival models and intuitive stratification flowcharts whilst achieving state-of-the-art performances. Revisiting a prior departmental study further demonstrated its capacity to automate survival analysis workflows in real-world clinical research. 

**Abstract (ZH)**: 可解释生存分析管道（PISA）：一种权衡复杂性和性能的多模型管道 

---
# Next Point-of-interest (POI) Recommendation Model Based on Multi-modal Spatio-temporal Context Feature Embedding 

**Title (ZH)**: 基于多模态时空上下文特征嵌入的下一个点Interest推荐模型 

**Authors**: Lingyu Zhang, Guobin Wu, Yan Wang, Pengfei Xu, Jian Liang, Xuan Song, Yunhai Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.22661)  

**Abstract**: The next Point-of-interest (POI) recommendation is mainly based on sequential traffic information to predict the user's next boarding point location. This is a highly regarded and widely applied research task in the field of intelligent transportation, and there have been many research results to date. Traditional POI prediction models primarily rely on short-term traffic sequence information, often neglecting both long-term and short-term preference data, as well as crucial spatiotemporal context features in user behavior. To address this issue, this paper introduces user long-term preference information and key spatiotemporal context information, and proposes a POI recommendation model based on multimodal spatiotemporal context feature embedding. The model extracts long-term preference features and key spatiotemporal context features from traffic data through modules such as spatiotemporal feature processing, multimodal embedding, and self-attention aggregation. It then uses a weighted fusion method to dynamically adjust the weights of long-term and short-term features based on users' historical behavior patterns and the current context. Finally, the fused features are matched using attention, and the probability of each location candidate becoming the next location is calculated. This paper conducts experimental verification on multiple transportation datasets, and the results show that the POI prediction model combining multiple types of features has higher prediction accuracy than existing SOTA models and methods. 

**Abstract (ZH)**: 基于多模态时空上下文特征嵌入的POI推荐方法 

---
# Fairness for niche users and providers: algorithmic choice and profile portability 

**Title (ZH)**: 为 niche 用户和供应商提供公平性：算法选择与资料档案移植 

**Authors**: Elizabeth McKinnie, Anas Buhayh, Clement Canel, Robin Burke  

**Link**: [PDF](https://arxiv.org/pdf/2509.22660)  

**Abstract**: Ensuring fair outcomes for multiple stakeholders in recommender systems has been studied mostly in terms of algorithmic interventions: building new models with better fairness properties, or using reranking to improve outcomes from an existing algorithm. What has rarely been studied is structural changes in the recommendation ecosystem itself. Our work explores the fairness impact of algorithmic pluralism, the idea that the recommendation algorithm is decoupled from the platform through which users access content, enabling user choice in algorithms. Prior work using a simulation approach has shown that niche consumers and (especially) niche providers benefit from algorithmic choice. In this paper, we use simulation to explore the question of profile portability, to understand how different policies regarding the handling of user profiles interact with fairness outcomes for consumers and providers. 

**Abstract (ZH)**: 确保推荐系统中多利益相关方的公平结果在很大程度上是从算法干预的角度进行研究的：通过构建具有良好公平属性的新模型，或通过重新排序来改进现有算法的结果来实现。很少研究的是推荐生态系统本身的结构变化。我们的工作探讨了算法多元主义的公平影响，即推荐算法通过用户访问内容的平台进行解耦，使用户能够在算法之间进行选择。先前的工作通过仿真方法表明，利基消费者和（尤其是）利基提供商从算法选择中受益。在本文中，我们使用仿真来探索资料档案可携性的问题，以了解不同的用户资料处理政策如何与消费者和提供商的公平结果相互作用。 

---
# How good are LLMs at Retrieving Documents in a Specific Domain? 

**Title (ZH)**: 特定领域中大语言模型检索文档的能力如何？ 

**Authors**: Nafis Tanveer Islam, Zhiming Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2509.22658)  

**Abstract**: Classical search engines using indexing methods in data infrastructures primarily allow keyword-based queries to retrieve content. While these indexing-based methods are highly scalable and efficient, due to a lack of an appropriate evaluation dataset and a limited understanding of semantics, they often fail to capture the user's intent and generate incomplete responses during evaluation. This problem also extends to domain-specific search systems that utilize a Knowledge Base (KB) to access data from various research infrastructures. Research infrastructures (RIs) from the environmental and earth science domain, which encompass the study of ecosystems, biodiversity, oceanography, and climate change, generate, share, and reuse large volumes of data. While there are attempts to provide a centralized search service using Elasticsearch as a knowledge base, they also face similar challenges in understanding queries with multiple intents. To address these challenges, we proposed an automated method to curate a domain-specific evaluation dataset to analyze the capability of a search system. Furthermore, we incorporate the Retrieval of Augmented Generation (RAG), powered by Large Language Models (LLMs), for high-quality retrieval of environmental domain data using natural language queries. Our quantitative and qualitative analysis of the evaluation dataset shows that LLM-based systems for information retrieval return results with higher precision when understanding queries with multiple intents, compared to Elasticsearch-based systems. 

**Abstract (ZH)**: 基于索引方法的经典搜索引擎主要允许关键词查询以检索内容。尽管这些基于索引的方法在可扩展性和效率方面表现出色，但由于缺乏适当的评估数据集和对语义理解有限，它们往往无法准确捕捉用户的意图并在评估中生成不完整的结果。这一问题同样扩展到了利用知识库（KB）访问来自各种研究基础设施数据的领域特定搜索引擎。来自环境与地球科学领域的研究基础设施（RIs），涵盖了生态系统、生物多样性、海洋学和气候变化的研究，生成、共享和重复使用大量数据。尽管有尝试使用Elasticsearch作为知识库提供集中式搜索服务，但它们同样面临理解多意图查询的相似挑战。为了解决这些挑战，我们提出了一个自动化的领域特定评估数据集的编纂方法，以分析搜索系统的性能。此外，我们还结合了由大型语言模型（LLMs）驱动的增强检索（RAG），使用自然语言查询高质量地检索环境领域数据。我们的评估数据集的定量和定性分析表明，基于LLM的检索系统在理解多意图查询时返回的结果精度更高，相比基于Elasticsearch的系统。 

---
# GOAT: A Large Dataset of Paired Guitar Audio Recordings and Tablatures 

**Title (ZH)**: GOAT：配对吉他音频录制和谱表的大规模数据集 

**Authors**: Jackson Loth, Pedro Sarmento, Saurjya Sarkar, Zixun Guo, Mathieu Barthet, Mark Sandler  

**Link**: [PDF](https://arxiv.org/pdf/2509.22655)  

**Abstract**: In recent years, the guitar has received increased attention from the music information retrieval (MIR) community driven by the challenges posed by its diverse playing techniques and sonic characteristics. Mainly fueled by deep learning approaches, progress has been limited by the scarcity and limited annotations of datasets. To address this, we present the Guitar On Audio and Tablatures (GOAT) dataset, comprising 5.9 hours of unique high-quality direct input audio recordings of electric guitars from a variety of different guitars and players. We also present an effective data augmentation strategy using guitar amplifiers which delivers near-unlimited tonal variety, of which we provide a starting 29.5 hours of audio. Each recording is annotated using guitar tablatures, a guitar-specific symbolic format supporting string and fret numbers, as well as numerous playing techniques. For this we utilise both the Guitar Pro format, a software for tablature playback and editing, and a text-like token encoding. Furthermore, we present competitive results using GOAT for MIDI transcription and preliminary results for a novel approach to automatic guitar tablature transcription. We hope that GOAT opens up the possibilities to train novel models on a wide variety of guitar-related MIR tasks, from synthesis to transcription to playing technique detection. 

**Abstract (ZH)**: 近年来，吉他因其多样的演奏技巧和音色特性，受到音乐信息检索（MIR）社区的越来越多关注。主要借助深度学习方法，进展受限于数据集稀缺且标注不足。为解决这一问题，我们提出了吉他音频和谱表数据集（Guitar On Audio and Tablatures, GOAT），包含5.9小时多种吉他和演奏者独特高质量的直接输入音频 recordings。我们还提出了一种有效的数据增强策略，利用吉他放大器产生近乎无限的音色变化，提供了初始的29.5小时音频。每个录音使用吉他谱表进行了标注，这是支持琴弦和品按键号码的吉他专用符号格式，以及众多演奏技巧。我们利用吉普生软件（Guitar Pro）进行谱表播放和编辑，并采用类似文本的标记编码。此外，我们展示了在MIDI转录任务中使用GOAT的竞争性结果，并介绍了自动吉他谱表转录的新颖方法的初步结果。我们希望GOAT能够开启针对吉他相关MIR任务的新型模型训练的可能性，从合成到转录再到演奏技巧检测。 

---
# Sustainable LSTM-Based Precoding for RIS-Aided mmWave MIMO Systems with Implicit CSI 

**Title (ZH)**: 基于RIS辅助毫米波MIMO系统的可持续LSTM基预编码方法：隐式CSI情形 

**Authors**: Po-Heng Chou, Jiun-Jia Wu, Wan-Jen Huang, Ronald Y. Chang  

**Link**: [PDF](https://arxiv.org/pdf/2509.12658)  

**Abstract**: In this paper, we propose a sustainable long short-term memory (LSTM)-based precoding framework for reconfigurable intelligent surface (RIS)-assisted millimeter-wave (mmWave) MIMO systems. Instead of explicit channel state information (CSI) estimation, the framework exploits uplink pilot sequences to implicitly learn channel characteristics, reducing both pilot overhead and inference complexity. Practical hardware constraints are addressed by incorporating the phase-dependent amplitude model of RIS elements, while a multi-label training strategy improves robustness when multiple near-optimal codewords yield comparable performance. Simulations show that the proposed design achieves over 90% of the spectral efficiency of exhaustive search (ES) with only 2.2% of its computation time, cutting energy consumption by nearly two orders of magnitude. The method also demonstrates resilience under distribution mismatch and scalability to larger RIS arrays, making it a practical and energy-efficient solution for sustainable 6G wireless networks. 

**Abstract (ZH)**: 基于可重构智能表面辅助毫米波MIMO系统的可持续长短期记忆（LSTM）预编码框架 

---
# How are Scientific Concepts Birthed? Typing Rules of Concept Formation in Theoretical Physics Reasoning 

**Title (ZH)**: 科学概念是如何诞生的？理论物理学推理中概念形成的基本规则 

**Authors**: Omar Aguilar, Anthony Aguirre  

**Link**: [PDF](https://arxiv.org/pdf/2509.10740)  

**Abstract**: This work aims to formalize some of the ways scientific concepts are formed in the process of theoretical physics discovery. Since this may at first seem like a task beyond the scope of the exact sciences (natural and formal sciences), we begin by presenting arguments for why scientific concept formation can be formalized. Then, we introduce type theory as a natural and well-suited framework for this formalization. We formalize what we call "ways of discovering new concepts" including concept distinction, property preservation, and concept change, as cognitive typing rules. Next, we apply these cognitive typing rules to two case studies of conceptual discovery in the history of physics: Einstein's reasoning leading to the impossibility of frozen waves, and his conceptual path to the relativity of time. In these historical episodes, we recast what a physicist might informally call "ways of discovering new scientific concepts" as compositional typing rules built from cognitive typing rules - thus formalizing them as scientific discovery mechanisms. Lastly, we computationally model the type-theoretic reconstruction of Einstein's conceptual path to the relativity of time as a program synthesis task. 

**Abstract (ZH)**: 本研究旨在形式化理论物理发现过程中形成科学概念的一些方式。虽然这可能最初看起来超出了精确科学（自然科学和形式科学）的范畴，我们首先通过论述科学概念形成可以形式化的理由来开始。然后，我们引入类型理论作为自然且合适的框架来进行这种形式化。我们将“发现新概念的方式”形式化，包括概念区分、属性保存和概念变化，作为认知类型规则。接下来，我们应用这些认知类型规则对物理学史上两个概念发现案例进行研究：爱因斯坦导致无法存在冻结波的推理过程，以及他对时间相对性的概念路径。在这些历史事件中，我们将物理学家可能非正式称之为“发现新科学概念的方式”重新表述为由认知类型规则构建的组合类型规则，从而将它们形式化为科学发现机制。最后，我们通过程序合成任务来计算建模爱因斯坦从概念路径到时间相对性的类型论重构。 

---
# Green Learning for STAR-RIS mmWave Systems with Implicit CSI 

**Title (ZH)**: 绿联学习在IMCSI的STAR-RIS毫米波系统中 

**Authors**: Yu-Hsiang Huang, Po-Heng Chou, Wan-Jen Huang, Walid Saad, C.-C. Jay Kuo  

**Link**: [PDF](https://arxiv.org/pdf/2509.06820)  

**Abstract**: In this paper, a green learning (GL)-based precoding framework is proposed for simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems. Motivated by the growing emphasis on environmental sustainability in future 6G networks, this work adopts a broadcasting transmission architecture for scenarios where multiple users share identical information, improving spectral efficiency and reducing redundant transmissions and power consumption. Different from conventional optimization methods, such as block coordinate descent (BCD) that require perfect channel state information (CSI) and iterative computation, the proposed GL framework operates directly on received uplink pilot signals without explicit CSI estimation. Unlike deep learning (DL) approaches that require CSI-based labels for training, the proposed GL approach also avoids deep neural networks and backpropagation, leading to a more lightweight design. Although the proposed GL framework is trained with supervision generated by BCD under full CSI, inference is performed in a fully CSI-free manner. The proposed GL integrates subspace approximation with adjusted bias (Saab), relevant feature test (RFT)-based supervised feature selection, and eXtreme gradient boosting (XGBoost)-based decision learning to jointly predict the STAR-RIS coefficients and transmit precoder. Simulation results show that the proposed GL approach achieves competitive spectral efficiency compared to BCD and DL-based models, while reducing floating-point operations (FLOPs) by over four orders of magnitude. These advantages make the proposed GL approach highly suitable for real-time deployment in energy- and hardware-constrained broadcasting scenarios. 

**Abstract (ZH)**: 基于绿色学习的STAR-RIS辅助毫米波MIMO广播系统同时传输与反射框架 

---
# Agentic DDQN-Based Scheduling for Licensed and Unlicensed Band Allocation in Sidelink Networks 

**Title (ZH)**: 基于代理DDQN的Licensed和Unlicensed频带分配侧联网络调度 

**Authors**: Po-Heng Chou, Pin-Qi Fu, Walid Saad, Li-Chun Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.06775)  

**Abstract**: In this paper, we present an agentic double deep Q-network (DDQN) scheduler for licensed/unlicensed band allocation in New Radio (NR) sidelink (SL) networks. Beyond conventional reward-seeking reinforcement learning (RL), the agent perceives and reasons over a multi-dimensional context that jointly captures queueing delay, link quality, coexistence intensity, and switching stability. A capacity-aware, quality of service (QoS)-constrained reward aligns the agent with goal-oriented scheduling rather than static thresholding. Under constrained bandwidth, the proposed design reduces blocking by up to 87.5% versus threshold policies while preserving throughput, highlighting the value of context-driven decisions in coexistence-limited NR SL networks. The proposed scheduler is an embodied agent (E-agent) tailored for task-specific, resource-efficient operation at the network edge. 

**Abstract (ZH)**: 一种用于新无线电侧链路网络频段分配的代理双深度Q网络调度器 

---
# YOLO-based Bearing Fault Diagnosis With Continuous Wavelet Transform 

**Title (ZH)**: 基于YOLO的连续小波变换轴承故障诊断 

**Authors**: Po-Heng Chou, Wei-Lung Mao, Ru-Ping Lin  

**Link**: [PDF](https://arxiv.org/pdf/2509.03070)  

**Abstract**: This letter proposes a YOLO-based framework for spatial bearing fault diagnosis using time-frequency spectrograms derived from continuous wavelet transform (CWT). One-dimensional vibration signals are first transformed into time-frequency spectrograms using Morlet wavelets to capture transient fault signatures. These spectrograms are then processed by YOLOv9, v10, and v11 models to classify fault types. Evaluated on three benchmark datasets, including Case Western Reserve University (CWRU), Paderborn University (PU), and Intelligent Maintenance System (IMS), the proposed CWT-YOLO pipeline achieves significantly higher accuracy and generalizability than the baseline MCNN-LSTM model. Notably, YOLOv11 reaches mAP scores of 99.4% (CWRU), 97.8% (PU), and 99.5% (IMS). In addition, its region-aware detection mechanism enables direct visualization of fault locations in spectrograms, offering a practical solution for condition monitoring in rotating machinery. 

**Abstract (ZH)**: 基于连续小波变换的时间-frequency光谱图的YOLO架构的空间轴承故障诊断方法 

---
# BenLOC: A Benchmark for Learning to Configure MIP Optimizers 

**Title (ZH)**: BenLOC: 一个学习配置MIP优化器的标准数据集 

**Authors**: Hongpei Li, Ziyan He, Yufei Wang, Wenting Tu, Shanwen Pu, Qi Deng, Dongdong Ge  

**Link**: [PDF](https://arxiv.org/pdf/2506.02752)  

**Abstract**: The automatic configuration of Mixed-Integer Programming (MIP) optimizers has become increasingly critical as the large number of configurations can significantly affect solver performance. Yet the lack of standardized evaluation frameworks has led to data leakage and over-optimistic claims, as prior studies often rely on homogeneous datasets and inconsistent experimental setups. To promote a fair evaluation process, we present BenLOC, a comprehensive benchmark and open-source toolkit, which not only offers an end-to-end pipeline for learning instance-wise MIP optimizer configurations, but also standardizes dataset selection, train-test splits, feature engineering and baseline choice for unbiased and comprehensive evaluations. Leveraging this framework, we conduct an empirical analysis on five well-established MIP datasets and compare classical machine learning models with handcrafted features against state-of-the-art deep-learning techniques. The results demonstrate the importance of datasets, features and baseline criteria proposed by BenLOC and the effectiveness of BenLOC in providing unbiased and comprehensive evaluations. 

**Abstract (ZH)**: Mixed-Integer Programming (MIP) 优化器的自动配置已成为越来越关键的问题，因为大量配置会显著影响求解器性能。然而，缺乏标准化评估框架导致了数据泄漏和过于乐观的声明，此前的研究经常依赖同质数据集和不一致的实验设置。为了促进公平的评估过程，我们提出 BenLOC，一个全面的基准和开源工具包，不仅提供了一站式的实例级 MIP 优化器配置学习管道，还对数据集选择、训练-测试分割、特征工程和基准选择进行了标准化，以实现无偏且全面的评估。利用这一框架，我们在五个广泛认可的 MIP 数据集上进行了实证分析，并将经典的机器学习模型与手工设计的特征与最先进的深度学习技术进行了对比。结果表明，BenLOC 提出的数据集、特征和基准标准的重要性，以及 BenLOC 在提供无偏且全面评估方面的有效性。 

---
# Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via In-Context Learning 

**Title (ZH)**: 适用于零样本语音转换的基于上下文学习的语调自适应音频编解码器 

**Authors**: Junchuan Zhao, Xintong Wang, Ye Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.15402)  

**Abstract**: Recent advances in discrete audio codecs have significantly improved speech representation modeling, while codec language models have enabled in-context learning for zero-shot speech synthesis. Inspired by this, we propose a voice conversion (VC) model within the VALLE-X framework, leveraging its strong in-context learning capabilities for speaker adaptation. To enhance prosody control, we introduce a prosody-aware audio codec encoder (PACE) module, which isolates and refines prosody from other sources, improving expressiveness and control. By integrating PACE into our VC model, we achieve greater flexibility in prosody manipulation while preserving speaker timbre. Experimental evaluation results demonstrate that our approach outperforms baseline VC systems in prosody preservation, timbre consistency, and overall naturalness, surpassing baseline VC systems. 

**Abstract (ZH)**: 最近在离散音频编解码器方面的进展显著改善了语音表示建模，而编解码器语言模型使零-shot语音合成具备了上下文学习能力。受此启发，我们提出了一个结合在VALLE-X框架内的语音转换（VC）模型，利用其强大的上下文学习能力进行说话人适应。为了增强语调控制，我们引入了一种感知语调的音频编解码器编码器（PACE）模块，该模块能够孤立并精炼语调以改善表达性和控制性。通过将PACE集成到我们的VC模型中，我们在保持说话人音色的同时实现了更灵活的语调操控。实验评估结果表明，我们的方法在语调保持、音色一致性及总体自然度方面均优于基线VC系统，超越了基线VC系统。 

---
