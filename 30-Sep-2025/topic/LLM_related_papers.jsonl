{'arxiv_id': 'arXiv:2509.23506', 'title': 'Ask, Reason, Assist: Decentralized Robot Collaboration via Language and Logic', 'authors': 'Dan BW Choe, Sundhar Vinodh Sangeetha, Steven Emanuel, Chih-Yuan Chiu, Samuel Coogan, Shreyas Kousik', 'link': 'https://arxiv.org/abs/2509.23506', 'abstract': 'Increased robot deployment, such as in warehousing, has revealed a need for seamless collaboration among heterogeneous robot teams to resolve unforeseen conflicts. To address this challenge, we propose a novel decentralized framework that enables robots to request and provide help. The process begins when a robot detects a conflict and uses a Large Language Model (LLM) to decide whether external assistance is required. If so, it crafts and broadcasts a natural language (NL) help request. Potential helper robots reason over the request and respond with offers of assistance, including information about the effect on their ongoing tasks. Helper reasoning is implemented via an LLM grounded in Signal Temporal Logic (STL) using a Backus-Naur Form (BNF) grammar, ensuring syntactically valid NL-to-STL translations, which are then solved as a Mixed Integer Linear Program (MILP). Finally, the requester robot selects a helper by reasoning over the expected increase in system-level total task completion time. We evaluated our framework through experiments comparing different helper-selection strategies and found that considering multiple offers allows the requester to minimize added makespan. Our approach significantly outperforms heuristics such as selecting the nearest available candidate helper robot, and achieves performance comparable to a centralized "Oracle" baseline but without heavy information demands.', 'abstract_zh': '异构机器人团队中无缝协作的需求及一种新型去中心化帮助请求与提供框架', 'title_zh': '求询、推理、辅助：基于语言与逻辑的去中心化机器人协作'}
{'arxiv_id': 'arXiv:2509.24230', 'title': 'ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration', 'authors': 'Shaobin Ling, Yun Wang, Chenyou Fan, Tin Lun Lam, Junjie Hu', 'link': 'https://arxiv.org/abs/2509.24230', 'abstract': 'Large Language Models (LLMs) enable intelligent multi-robot collaboration but face fundamental trade-offs: declarative methods lack adaptability in dynamic environments, while iterative methods incur prohibitive computational costs that scale poorly with team size and task complexity. In this paper, we propose ELHPlan, a novel framework that introduces Action Chains--sequences of actions explicitly bound to sub-goal intentions--as the fundamental planning primitive. ELHPlan operates via a cyclical process: 1) constructing intention-bound action sequences, 2) proactively validating for conflicts and feasibility, 3) refining issues through targeted mechanisms, and 4) executing validated actions. This design balances adaptability and efficiency by providing sufficient planning horizons while avoiding expensive full re-planning. We further propose comprehensive efficiency metrics, including token consumption and planning time, to more holistically evaluate multi-agent collaboration. Our experiments on benchmark TDW-MAT and C-WAH demonstrate that ELHPlan achieves comparable task success rates while consuming only 24% of the tokens required by state-of-the-art methods. Our research establishes a new efficiency-effectiveness frontier for LLM-based multi-agent planning systems.', 'abstract_zh': '基于大语言模型的多机器人协作框架ELHPlan：兼具适应性和效率的新范式', 'title_zh': 'ELHPlan: 高效远期任务规划用于多智能体协作'}
{'arxiv_id': 'arXiv:2509.25154', 'title': "Who's Your Judge? On the Detectability of LLM-Generated Judgments", 'authors': 'Dawei Li, Zhen Tan, Chengshuai Zhao, Bohan Jiang, Baixiang Huang, Pingchuan Ma, Abdullah Alnaibari, Kai Shu, Huan Liu', 'link': 'https://arxiv.org/abs/2509.25154', 'abstract': "Large Language Model (LLM)-based judgments leverage powerful LLMs to efficiently evaluate candidate content and provide judgment scores. However, the inherent biases and vulnerabilities of LLM-generated judgments raise concerns, underscoring the urgent need for distinguishing them in sensitive scenarios like academic peer reviewing. In this work, we propose and formalize the task of judgment detection and systematically investigate the detectability of LLM-generated judgments. Unlike LLM-generated text detection, judgment detection relies solely on judgment scores and candidates, reflecting real-world scenarios where textual feedback is often unavailable in the detection process. Our preliminary analysis shows that existing LLM-generated text detection methods perform poorly given their incapability to capture the interaction between judgment scores and candidate content -- an aspect crucial for effective judgment detection. Inspired by this, we introduce \\textit{J-Detector}, a lightweight and transparent neural detector augmented with explicitly extracted linguistic and LLM-enhanced features to link LLM judges' biases with candidates' properties for accurate detection. Experiments across diverse datasets demonstrate the effectiveness of \\textit{J-Detector} and show how its interpretability enables quantifying biases in LLM judges. Finally, we analyze key factors affecting the detectability of LLM-generated judgments and validate the practical utility of judgment detection in real-world scenarios.", 'abstract_zh': '基于大规模语言模型（LLM）的判决检测：利用强大的LLM高效评估候选内容并提供判决分数，但LLM生成判决固有的偏见和脆弱性引发了担忧，尤其是在学术同行评审等敏感场景中迫切需要将它们区分开来。本文提出并正式化了判决检测任务，并系统地研究了LLM生成判决的可检测性。不同于LLM生成文本检测，判决检测仅依赖于判决分数和候选文本，反映了现实世界中识别过程中文本反馈常不可用的场景。初步分析显示，现有LLM生成文本检测方法表现不佳，因为它们无法捕捉判决分数与候选文本之间的交互作用——这是有效判决检测的关键方面。受此启发，我们引入了J-Detector，这是一种轻量级、透明的神经检测器，结合了明确提取的语言和LLM增强特征，将LLM评判者的偏见与候选者的属性联系起来，以实现准确的检测。跨多种数据集的实验表明了J-Detector的有效性，并展示了其可解释性如何量化LLM评判者的偏见。最后，我们分析了影响LLM生成判决可检测性的关键因素，并验证了判决检测在现实世界场景中的实用价值。', 'title_zh': '谁是你的法官？LLM生成的判断可检测性探究'}
{'arxiv_id': 'arXiv:2509.25148', 'title': 'UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following', 'authors': 'FaQiang Qian, WeiKun Zhang, Ziliang Wang, Kang An, Xuhui Zheng, Liangjian Wen, Mengya Gao, Yong Dai, Yichao Wu', 'link': 'https://arxiv.org/abs/2509.25148', 'abstract': "Shaping powerful LLMs to be beneficial and safe is central to AI alignment. We argue that post-training alignment is fundamentally a unified Preference Learning problem, involving two modalities: demonstrated preferences (e.g., Supervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement Learning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due to a critical distributional mismatch: SFT uses static expert data, but as the policy evolves, its generation distribution drifts, making SFT knowledge brittle. Subsequent RL then explores without direct access to the rich, ground-truth knowledge in expert demonstrations, leading to inefficient, ungrounded updates. This separation prevents mutual regularization between data sources. To address this, we reframe alignment as a constrained optimization problem and propose Unified Adversarial Preference Learning (UniAPL),a novel framework that dynamically aligns the policy's distribution with the expert's. UniAPL implements a single-stage unified training objective, jointly learning from mixed batches of SFT and preference data. In every gradient step, dense expert demonstrations directly ground and regularize online exploration, inherently resolving distributional mismatch and maximizing data this http URL evaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507 as the teacher. Our models match or exceed strong GRPO baselines: +5.77% on Qwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the teacher. Analyses of response length and log-probability distributions confirm that UniAPL outputs closely mimic expert demonstrations, achieving both stronger performance and better behavioral alignment.", 'abstract_zh': '塑造强大的LLM以使其有益和安全是AI对齐的核心。我们argue认为，后训练对齐本质上是一个统一的偏好学习问题，涉及两种模态：已展示的偏好（例如，监督微调SFT）和比较偏好（例如，强化学习RL）。标准的顺序管线——先进行SFT后进行RL——由于关键的分布性不匹配而存在问题：SFT使用静态专家数据，但随着策略的演化，其生成分布会发生偏移，使SFT知识变得脆弱。随后的RL则在没有直接访问丰富的地面真实知识的情况下探索，导致了低效且缺乏根基的更新。这种分离阻碍了数据源之间的相互正则化。为了解决这个问题，我们将对齐重新构想为一个约束优化问题，并提出了一种新型框架——统一 adversarial 偏好学习（UniAPL），该框架动态地使策略分布与专家分布对齐。UniAPL 实现了一站式统一训练目标，联合学习混合批次的SFT和偏好数据。在每一步梯度更新中，密集的专家示范直接为在线探索提供根基和正则化，从根本上解决了分布性不匹配问题并最大化数据利用。我们使用Qwen3-235B-Instruct-2507作为教师对UniAPL进行指令跟随任务的评估。我们的模型匹配或超过了强大的GRPO基线：在Qwen3-0.6B上提高了5.77%（达到一个32B模型的表现），在Qwen3-4B上提高了3.75%，甚至超过了教师。对回复长度和对数概率分布的分析证实，UniAPL 的输出密切模仿了专家示范，实现了更强的性能和更好的行为对齐。', 'title_zh': 'UniAPL: 一种统一的对抗性偏好学习框架用于指令跟随'}
{'arxiv_id': 'arXiv:2509.25123', 'title': 'From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones', 'authors': 'Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, Hao Peng', 'link': 'https://arxiv.org/abs/2509.25123', 'abstract': "Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.", 'abstract_zh': 'RL能否教会大语言模型真正的新技能，还是仅激活现有技能？这一问题处于关于RL在大语言模型后训练中角色的持续争议的核心。一方认为，即使没有预先的监督微调，也可以通过RL取得强大的实证结果；另一方则批评认为，RL在其贡献上仅限于重新加权现有的推理策略。本研究提供了实证证据，表明大语言模型在RL过程中可以通过组合现有技能来获得真正的新技能，这反映了人类获得新认知技能的一种核心机制。为了减轻数据污染和其他混杂因素，并允许对任务复杂性进行精确控制，我们发展了一种合成框架进行研究。具体来说，我们将技能定义为根据给定的字符串x推断出字符串变换函数f(x)输出的能力。当LLM在RL之前已经学习了f和g时，我们的实验揭示了RL使其能够学习未见过的它们的组合h(x)=g(f(x))。此外，这种组合能力可以扩展到在RL训练期间未曾见过的复杂问题，如多个函数的组合。令人惊讶的是，我们的实验表明，在源任务上获得的组合技能可以转移到不同的目标任务上，即使没有在目标任务上进行组合训练，仅需目标任务的原子技能的先验知识。我们的定性分析表明，RL从根本上改变了模型的推理行为。相比之下，使用相同数据的下一个标记训练则未能得出这些发现。系统性的实验为大语言模型的学习提供了新的见解，表明首先构建具有基本技能的基础模型，然后使用RL激励复杂问题中的高级、可泛化的技能的价值。', 'title_zh': '从$f(x)$和$g(x)$到$f(g(x))$: 大型语言模型通过组合旧技能学习新技能（基于RL）'}
{'arxiv_id': 'arXiv:2509.25047', 'title': 'Scaling Synthetic Task Generation for Agents via Exploration', 'authors': 'Ram Ramrakhya, Andrew Szot, Omar Attia, Yuhao Yang, Anh Nguyen, Bogdan Mazoure, Zhe Gan, Harsh Agrawal, Alexander Toshev', 'link': 'https://arxiv.org/abs/2509.25047', 'abstract': 'Post-Training Multimodal Large Language Models (MLLMs) to build interactive agents holds promise across domains such as computer-use, web navigation, and robotics. A key challenge in scaling such post-training is lack of high-quality downstream agentic task datasets with tasks that are diverse, feasible, and verifiable. Existing approaches for task generation rely heavily on human annotation or prompting MLLM with limited downstream environment information, which is either costly or poorly scalable as it yield tasks with limited coverage. To remedy this, we present AutoPlay, a scalable pipeline for task generation that explicitly explores interactive environments to discover possible interactions and current state information to synthesize environment-grounded tasks. AutoPlay operates in two stages: (i) an exploration phase, where an MLLM explorer agent systematically uncovers novel environment states and functionalities, and (ii) a task generation phase, where a task generator leverages exploration trajectories and a set of task guideline prompts as context to synthesize diverse, executable, and verifiable tasks. We show AutoPlay generates 20k tasks across 20 Android applications and 10k tasks across 13 applications Ubuntu applications to train mobile-use and computer-use agents. AutoPlay generated tasks enable large-scale task demonstration synthesis without human annotation by employing an MLLM task executor and verifier. This data enables training MLLM-based UI agents that improve success rates up to $20.0\\%$ on mobile-use and $10.9\\%$ on computer-use scenarios. In addition, AutoPlay generated tasks combined with MLLM verifier-based rewards enable scaling reinforcement learning training of UI agents, leading to an additional $5.7\\%$ gain. coverage. These results establish AutoPlay as a scalable approach for post-training capable MLLM agents reducing reliance on human annotation.', 'abstract_zh': 'Post-Training 多模态大型语言模型 (MLLMs) 构建交互代理在计算机使用、网页导航和机器人等领域具有潜力。大规模后训练的关键挑战是在缺乏高质量的下游代理任务数据集的情况下进行此类后训练，这些数据集包含多样化、可行且可验证的任务。现有的任务生成方法 heavily 依赖手工标注或用有限的下游环境信息提示 MLLM，这种方法要么成本高，要么扩展性差，导致生成的任务覆盖面有限。为了解决这一问题，我们提出了 AutoPlay，这是一种可扩展的任务生成管道，通过明确探索交互环境来发现可能的交互和当前状态信息，以合成环境相关的任务。AutoPlay 分为两个阶段：(i) 探索阶段，在此阶段，MLLM 探索者代理系统地发现新的环境状态和功能，(ii) 任务生成阶段，在此阶段，任务生成器利用探索轨迹和一系列任务指南提示作为上下文，合成多样化、可执行且可验证的任务。我们展示了 AutoPlay 在 20 个 Android 应用程序中生成了 20,000 个任务，在 13 个 Ubuntu 应用程序中生成了 10,000 个任务，用于训练移动使用和计算机使用代理。AutoPlay 生成的任务通过使用 MLLM 任务执行器和验证器，无需人工标注即可实现大规模任务演示合成。这些数据使基于 MLLM 的 UI 代理能够在移动使用场景中提高成功率多达 20.0%，在计算机使用场景中提高成功率多达 10.9%。此外，结合 AutoPlay 生成的任务和基于 MLLM 验证器的奖励使强化学习训练 UI 代理规模化，导致额外 5.7% 的增益。这些结果确立了 AutoPlay 作为一种减少对人工标注依赖的后训练可扩展方法。', 'title_zh': '通过探索扩展合成任务生成 Agents'}
{'arxiv_id': 'arXiv:2509.25004', 'title': 'CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning', 'authors': 'Shijie Zhang, Guohao Sun, Kevin Zhang, Xiang Guo, Rujun Guo', 'link': 'https://arxiv.org/abs/2509.25004', 'abstract': "Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing methods typically treat all training samples uniformly, overlooking the vast differences in problem difficulty relative to the model's current capabilities. This uniform training strategy leads to inefficient exploration of problems the model has already mastered, while concurrently lacking effective guidance on problems that are challenging its abilities the most, limiting both learning efficiency and upper-bound performance. To address this, we propose CLPO (Curriculum-guided Learning for Policy Optimization), a novel algorithm that creates a dynamic pedagogical feedback loop within the policy optimization process. The core of CLPO leverages the model's own rollout performance to conduct real-time difficulty assessment, thereby constructing an Online Curriculum. This curriculum then guides an Adaptive Problem Restructuring mechanism, where the model acts as its own teacher: it diversifies medium-difficulty problems to promote generalization and simplifies challenging problems to make them more attainable. Our approach transforms the static training procedure into a dynamic process that co-evolves with the model's capabilities. Experiments show that CLPO achieves state-of-the-art performance across eight challenging mathematical and general reasoning benchmarks, with an average pass@1 improvement of 6.96% over other methods, demonstrating its potential for more efficiently training more capable reasoning models.", 'abstract_zh': '最近，带有可验证奖励的在线强化学习（RLVR）已成为增强大规模语言模型（LLMs）推理能力的关键范式。然而，现有方法通常将所有训练样本均匀对待，忽视了相对于模型当前能力的问题难度的巨大差异。这种均匀训练策略导致模型已经掌握的问题探索效率低下，同时对最挑战模型能力的问题也无法提供有效的指导，限制了学习效率和上界性能。为此，我们提出了一种新的算法CLPO（Curriculum-guided Learning for Policy Optimization），该算法在策略优化过程中创建了一个动态的教学反馈循环。CLPO的核心利用模型自身的展开性能进行实时难度评估，从而构建在线课程。该课程指导一种自适应问题重结构机制，其中模型充当自己的导师：通过增加中等难度问题促进泛化，简化难题使其更具可行性。我们的方法将静态训练过程转换为与模型能力共同演化的动态过程。实验表明，CLPO在八个具有挑战性的数学和通用推理基准测试中实现了最先进的性能，平均pass@1提高了6.96%，证明了其对更高效训练更强大推理模型的潜力。', 'title_zh': 'CLPO： Curriculum Learning 结合策略优化应用于大语言模型推理'}
{'arxiv_id': 'arXiv:2509.24922', 'title': 'MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning', 'authors': 'Huihao Jing, Wenbin Hu, Hongyu Luo, Jianhui Yang, Wei Fan, Haoran Li, Yangqiu Song', 'link': 'https://arxiv.org/abs/2509.24922', 'abstract': 'Multi-agent systems (MAS), leveraging the remarkable capabilities of Large Language Models (LLMs), show great potential in addressing complex tasks. In this context, integrating MAS with legal tasks is a crucial step. While previous studies have developed legal benchmarks for LLM agents, none are specifically designed to consider the unique advantages of MAS, such as task decomposition, agent specialization, and flexible training. In fact, the lack of evaluation methods limits the potential of MAS in the legal domain. To address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS and designed with a deductive reasoning approach. Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations. Furthermore, we manually design various role-based MAS and conduct extensive experiments using different state-of-the-art LLMs. Our results highlight the strengths, limitations, and potential areas for improvement of existing models and MAS architectures.', 'abstract_zh': '基于大型语言模型的多Agent系统在法律任务中的应用：MASLegalBench基准设计', 'title_zh': 'MASLegalBench：基于演绎法律推理的多代理系统 benchmarks'}
{'arxiv_id': 'arXiv:2509.24877', 'title': 'The Emergence of Social Science of Large Language Models', 'authors': 'Xiao Jia, Zhanzhan Zhao', 'link': 'https://arxiv.org/abs/2509.24877', 'abstract': 'The social science of large language models (LLMs) examines how these systems evoke mind attributions, interact with one another, and transform human activity and institutions. We conducted a systematic review of 270 studies, combining text embeddings, unsupervised clustering and topic modeling to build a computational taxonomy. Three domains emerge organically across the reviewed literature. LLM as Social Minds examines whether and when models display behaviors that elicit attributions of cognition, morality and bias, while addressing challenges such as test leakage and surface cues. LLM Societies examines multi-agent settings where interaction protocols, architectures and mechanism design shape coordination, norms, institutions and collective epistemic processes. LLM-Human Interactions examines how LLMs reshape tasks, learning, trust, work and governance, and how risks arise at the human-AI interface. This taxonomy provides a reproducible map of a fragmented field, clarifies evidentiary standards across levels of analysis, and highlights opportunities for cumulative progress in the social science of artificial intelligence.', 'abstract_zh': '大型语言模型的社会科学探讨这些系统引发心智归因、相互作用以及如何转变人类活动和制度。我们系统回顾了270项研究，结合文本嵌入、无监督聚类和主题建模来构建计算分类体系。三项研究领域在回顾的文献中自然地浮现。LLM作为社会心智考察模型在何时以及在何种情况下展示出引发认知、道德和偏见归因的行为，同时解决测试泄露和表面线索等挑战。LLM社会探讨多代理设置中，交互协议、架构和机制设计如何塑造协调、规范、制度和集体认识过程。LLM与人类的互动考察LLM如何重塑任务、学习、信任、工作和治理，以及在人类-人工智能接口处可能出现的风险。该分类体系提供了一个可再现的领域分布图，明确了不同分析层面的证据标准，并突显了人工智能社会科学研究中累积进步的机会。', 'title_zh': '大型语言模型的社会科学崛起'}
{'arxiv_id': 'arXiv:2509.24836', 'title': 'Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity', 'authors': 'Zhen Bi, Zhenlin Hu, Jinnan Yang, Mingyang Chen, Cheng Deng, Yida Xue, Zeyu Yang, Qing Shen, Zhenfang Liu, Kang Zhao, Ningyu Zhang, Jungang Lou', 'link': 'https://arxiv.org/abs/2509.24836', 'abstract': "Recent advances in large language models (LLMs) highlight the importance of training data structure and quality in shaping reasoning behavior. However, most existing approaches focus on transforming data formats while neglecting the internal reasoning complexity of training samples, leaving the reasoning potential of data under-explored and underutilized. In this work, we posit that LLM logical reasoning performance is jointly constrained by the potential of the training data and the cognitive capacity of the model. To make this relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel metric that quantifies the latent logical reasoning complexity of samples by decomposing and aggregating their logical structures. This allows us to analyze how well current LLMs utilize logical reasoning signals and identify performance gaps relative to data potential. Based on this insight, we introduce a re-cognizing optimization strategy that systematically enhances the logical reasoning intensity of training this http URL than increasing data volume, our method re-optimizes existing samples to better align with the LLM's logical reasoning boundary. Extensive experiments show that our approach significantly improves performance and generalization over data-centric strategies. We further validate our method under a reinforcement learning framework. Our results indicate that prioritizing reasoning complexity in data rather than sheer scale or superficial form is essential to realizing LLMs' full cognitive potential.", 'abstract_zh': 'Recent advances in大型语言模型（LLMs）强调了训练数据结构和质量对于推理行为塑造的重要性。然而，目前大多数方法侧重于变换数据格式，而忽视了训练样本的内部推理复杂性，使得数据的推理潜力被低估和未充分利用。在本文中，我们提出LLM逻辑推理性能由训练数据的潜力和模型的认知能力共同制约。为了使这种关系可测量，我们引入了数据推理强度（DRI），这是一种新的度量方法，通过分解和综合样本的逻辑结构来量化其潜在的逻辑推理复杂性。这使我们能够分析当前LLM在利用逻辑推理信号方面的程度，并识别其与数据潜力之间的性能差距。基于这一洞见，我们提出了一种重新认知的优化策略，系统性地增强训练样本的逻辑推理强度，而非单纯增加数据量，我们的方法重新优化了现有样本，以更好地与LLM的逻辑推理边界对齐。大量实验表明，我们的方法在数据为中心的策略上显著提高了性能与泛化能力。我们的研究进一步在强化学习框架下验证了该方法的有效性。结果表明，重视数据中的推理复杂性而非单纯的规模或表面形式，对于实现LLM的全部认知潜力至关重要。', 'title_zh': '推动LLM在逻辑推理能力上的极限：数据推理强度的作用'}
{'arxiv_id': 'arXiv:2509.24808', 'title': 'Query Circuits: Explaining How Language Models Answer User Prompts', 'authors': 'Tung-Yu Wu, Fazl Barez', 'link': 'https://arxiv.org/abs/2509.24808', 'abstract': "Explaining why a language model produces a particular output requires local, input-level explanations. Existing methods uncover global capability circuits (e.g., indirect object identification), but not why the model answers a specific input query in a particular way. We introduce query circuits, which directly trace the information flow inside a model that maps a specific input to the output. Unlike surrogate-based approaches (e.g., sparse autoencoders), query circuits are identified within the model itself, resulting in more faithful and computationally accessible explanations. To make query circuits practical, we address two challenges. First, we introduce Normalized Deviation Faithfulness (NDF), a robust metric to evaluate how well a discovered circuit recovers the model's decision for a specific input, and is broadly applicable to circuit discovery beyond our setting. Second, we develop sampling-based methods to efficiently identify circuits that are sparse yet faithfully describe the model's behavior. Across benchmarks (IOI, arithmetic, MMLU, and ARC), we find that there exist extremely sparse query circuits within the model that can recover much of its performance on single queries. For example, a circuit covering only 1.3% of model connections can recover about 60% of performance on an MMLU questions. Overall, query circuits provide a step towards faithful, scalable explanations of how language models process individual inputs.", 'abstract_zh': '解释语言模型生成特定输出的原因需要输入级别的本地解释。现有的方法可以揭示全局能力电路（例如，间接对象识别），但不能解释模型是如何以特定方式回答具体输入查询的。我们提出了查询电路，它可以直接追踪模型内部从特定输入到输出的信息流动。与基于代理的方法（例如，稀疏自动编码器）不同，查询电路是在模型本身中识别出来的，从而提供了更忠实且计算上可访问的解释。为了使查询电路实用，我们解决了两个挑战。首先，我们引入了规范偏差忠实性（NDF），这是一种鲁棒的指标，用于评估发现的电路在多大程度上恢复了模型对特定输入的决策，并且该指标广泛适用于超出我们设置的电路发现。其次，我们开发了基于采样的方法，以高效地识别稀疏但忠实描述模型行为的电路。在多种基准测试（IOI、算术、MMLU和ARC）中，我们发现模型内部存在极其稀疏的查询电路，这些电路可以恢复模型在单个查询上的大部分性能。例如，仅覆盖模型连接的1.3%的电路可以帮助恢复MMLU问题上约60%的性能。总体而言，查询电路代表了一种朝着忠实、可扩展的语言模型个体输入处理解释迈出的一步。', 'title_zh': 'Query Circuits: 解释语言模型如何回答用户提示'}
{'arxiv_id': 'arXiv:2509.24803', 'title': 'TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models', 'authors': 'Tong Guan, Zijie Meng, Dianqi Li, Shiyu Wang, Chao-Han Huck Yang, Qingsong Wen, Zuozhu Liu, Sabato Marco Siniscalchi, Ming Jin, Shirui Pan', 'link': 'https://arxiv.org/abs/2509.24803', 'abstract': 'Recent advances in multimodal time series learning underscore a paradigm shift from analytics centered on basic patterns toward advanced time series understanding and reasoning. However, existing multimodal time series datasets mostly remain at the level of surface alignment and question answering, without reaching the depth of genuine reasoning. The absence of well-defined tasks that genuinely require time series reasoning, along with the scarcity of high-quality data, has limited progress in building practical time series reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite (TSR-Suite), which formalizes four atomic tasks that span three fundamental capabilities for reasoning with time series: (1) perception, acquired through scenario understanding and causality discovery; (2) extrapolation, realized via event-aware forecasting; and (3) decision-making, developed through deliberation over perception and extrapolation. TSR-Suite is the first comprehensive time series reasoning suite that supports not only thorough evaluation but also the data pipeline and training of TSRMs. It contains more than 23K samples, of which 2.3K are carefully curated through a human-guided hierarchical annotation process. Building on this foundation, we introduce TimeOmni-1, the first unified reasoning model designed to address diverse real-world problems demanding time series reasoning. The model is trained in multiple stages, integrating a mixture of task scenarios, novel reward functions, and tailored optimizations. Experiments show that TimeOmni-1 delivers strong out-of-distribution generalization across all tasks and achieves a high rate of valid responses. It significantly improves causality discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.', 'abstract_zh': '近期多模态时间序列学习的进展标志着从基本模式分析向高级时间序列理解和推理的范式转变。然而，现有的多模态时间序列数据集主要停留在表面对齐和问答层面，未能达到真正的推理深度。缺乏真正需要时间序列推理的定义明确的任务以及高质量数据的稀缺性限制了构建实用的时间序列推理模型(TSRMs)的进展。为此，我们提出时间序列推理套件(TSR-Suite)，它正规化了四个原子任务，涵盖了推理时间序列的三种基本能力：(1) 通过场景理解和因果发现获得的感知；(2) 通过事件感知预测实现的外推；以及(3) 通过感知和外推的权衡发展决策制定。TSR-Suite 是第一个全面的时间序列推理套件，不仅支持彻底的评估，还支持 TSRMs 的数据管道和训练。它包含超过 2.3 万个样本，其中 2300 个通过人工引导的分层标注过程谨慎挑选。基于这一基础，我们提出了 TimeOmni-1，这是第一个统一的推理模型，旨在解决多种需要时间序列推理的实际问题。该模型通过多个训练阶段进行训练，结合了多种任务场景、新型奖励函数和定制优化。实验结果显示，TimeOmni-1 在所有任务中的离群值泛化表现强劲，并达到了较高的有效响应率。与 GPT-4.1 相比，TimeOmni-1 在因果发现准确性上提高了 64.0%（相比之下 GPT-4.1 为 35.9%），在事件感知预测任务中有效响应率提高了超过 6%。', 'title_zh': 'TimeOmni-1：在大型语言模型中通过时间序列激励复杂推理'}
{'arxiv_id': 'arXiv:2509.24765', 'title': 'From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent for LLM Logical Reasoning', 'authors': 'Yunyao Zhang, Xinglang Zhang, Junxi Sheng, Wenbing Li, Junqing Yu, Wei Yang, Zikai Song', 'link': 'https://arxiv.org/abs/2509.24765', 'abstract': "Logical reasoning is a fundamental capability of large language models (LLMs). However, existing studies largely overlook the interplay between logical complexity and semantic complexity, resulting in methods that struggle to address challenging scenarios involving abstract propositions, ambiguous contexts, and conflicting stances, which are central to human reasoning. For this gap, we propose LogicAgent, a semiotic-square-guided framework designed to jointly address logical complexity and semantic complexity. LogicAgent explicitly performs multi-perspective deduction in first-order logic (FOL), while mitigating vacuous reasoning through existential import checks that incorporate a three-valued decision scheme (True, False, Uncertain) to handle boundary cases more faithfully. Furthermore, to overcome the semantic simplicity and low logical complexity of existing datasets, we introduce RepublicQA, a benchmark that reaches college-level difficulty (FKGL = 11.94) and exhibits substantially greater lexical and structural diversity than prior benchmarks. RepublicQA is grounded in philosophical concepts, featuring abstract propositions and systematically organized contrary and contradictory relations, making it the most semantically rich resource for evaluating logical reasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art performance on RepublicQA, with a 6.25% average gain over strong baselines, and generalizes effectively to mainstream logical reasoning benchmarks including ProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05% average gain. These results highlight the strong effectiveness of our semiotic-grounded multi-perspective reasoning in boosting LLMs' logical performance.", 'abstract_zh': '逻辑推理是大型语言模型（LLMs）的一项基本能力。然而，现有研究很大程度上忽视了逻辑复杂性和语义复杂性之间的交互作用，导致方法在处理涉及抽象命题、含糊语境和相互矛盾立场的具有挑战性的场景时表现不佳，这些都是人类推理的核心内容。为弥补这一差距，我们提出了LogicAgent，一个基于语义方格的框架，旨在同时解决逻辑复杂性和语义复杂性问题。LogicAgent 明确在一阶逻辑（FOL）中进行多视角演绎，并通过结合三种值决策方案（True、False、Uncertain）的存在性导入检查来减少空洞推理，从而更忠实于边界情况的处理。此外，为克服现有数据集的语义简单性和较低的逻辑复杂性，我们引入了RepublicQA，这是一个达到大学水平难度（FKGL = 11.94）的基准，并在词汇和结构多样性方面显著超过了先前的基准。RepublicQA 基于哲学概念，包含抽象命题并系统组织对立和矛盾关系，使其成为评估逻辑推理的最具语义丰富性的资源。实验证明，LogicAgent 在RepublicQA 上实现了最先进的性能，与强基线相比平均提升6.25%，并在主流逻辑推理基准ProntoQA、ProofWriter、FOLIO和ProverQA上表现出有效的泛化能力，额外实现平均7.05%的提升。这些结果突显了我们基于语义的多视角推理在提升LLMs逻辑性能方面的强大效果。', 'title_zh': '从歧义到判决：基于语义学的多视角代理模型在LLM逻辑推理中的应用'}
{'arxiv_id': 'arXiv:2509.24711', 'title': "On the Self-awareness of Large Reasoning Models' Capability Boundaries", 'authors': 'Qingjie Zhang, Yujia Fu, Yang Wang, Liu Yan, Tao Wei, Ke Xu, Minlie Huang, Han Qiu', 'link': 'https://arxiv.org/abs/2509.24711', 'abstract': "Large Reasoning Models (LRMs) have shown impressive performance on complex reasoning tasks such as mathematics, yet they also display misbehaviors that expose their limitations. In particular, when faced with hard questions, LRMs often engage in unproductive reasoning until context limit, producing wrong answers while wasting substantial computation. This phenomenon reflects a fundamental issue: current answering paradigms overlook the relationship between questions and LRMs' capability boundaries. In this paper, we investigate whether LRMs possess self-awareness of capability boundaries. We begin by an observation that LRMs may know what they cannot solve through expressed reasoning confidence. For black-box models, we find that reasoning expressions reveal boundary signals, with accelerated growing confidence trajectory for solvable problems but convergent uncertainty trajectory for unsolvable ones. For white-box models, we show that hidden states of the last input token encode boundary information, with solvable and unsolvable problems linearly separable even before reasoning begins. Building on these findings, we propose two simple yet effective optimization strategies: reasoning expression monitoring and hidden states monitoring. Experiments demonstrate that these boundary-aware strategies enable LRMs to avoid unproductive reasoning without sacrificing accuracy, significantly improving reliability and efficiency by cutting token usage up to 62.7 - 93.6%.", 'abstract_zh': '大型推理模型（LRMs）在复杂推理任务如数学中展现了令人印象深刻的性能，但也显示了暴露其局限性的不良行为。尤其是在面对难题时，LRMs 通常会无成效地推理直到上下文限制，产生错误答案同时浪费了大量的计算资源。这一现象反映出一个根本性问题：当前的作答范式忽视了问题与LRMs能力边界之间的关系。在本文中，我们探讨了LRMs是否具有对其能力边界的自我认知。我们首先观察到，LRMs可能通过表达的推理信心知道它们无法解决什么问题。对于黑箱模型，我们发现推理表达揭示了边界信号，可解问题的信心增长轨迹加快，而不可解问题的不确定性收敛轨迹趋同。对于白箱模型，我们展示了最后一个输入词的隐藏状态编码了边界信息，即使在推理开始之前，可解问题和不可解问题也是线性可分的。基于这些发现，我们提出了两种简便而有效的优化策略：推理表达监控和隐藏状态监控。实验表明，这些具备边界意识的策略能够让LRMs避免无成效的推理而不牺牲准确性，显著提高了可靠性和效率，最高可减少62.7%-93.6%的词使用量。', 'title_zh': '大型推理模型自我意识的能力边界'}
{'arxiv_id': 'arXiv:2509.24592', 'title': 'BPMN Assistant: An LLM-Based Approach to Business Process Modeling', 'authors': 'Josip Tomo Licardo, Nikola Tankovic, Darko Etinger', 'link': 'https://arxiv.org/abs/2509.24592', 'abstract': 'This paper presents BPMN Assistant, a tool that leverages Large Language Models (LLMs) for natural language-based creation and editing of BPMN diagrams. A specialized JSON-based representation is introduced as a structured alternative to the direct handling of XML to enhance the accuracy of process modifications. Process generation quality is evaluated using Graph Edit Distance (GED) and Relative Graph Edit Distance (RGED), while editing performance is evaluated with a binary success metric. Results show that JSON and XML achieve similar similarity scores in generation, but JSON offers greater reliability, faster processing, and significantly higher editing success rates. We discuss key trade-offs, limitations, and future improvements. The implementation is available at this https URL.', 'abstract_zh': '本文介绍了BPMNAssistant，这是一种利用大型语言模型（LLMs）进行BPMN图的自然语言创建和编辑的工具。提出了一种专门的JSON基表示法，作为一种结构化替代方案，以增强过程修改的准确性。通过图编辑距离（GED）和相对图编辑距离（RGED）评估过程生成质量，通过二元成功度量评估编辑性能。结果显示，JSON和XML在生成时达到相似的相似性分数，但JSON在可靠性和处理速度上更具优势，并且编辑成功率显著更高。我们讨论了关键权衡、局限性和未来的改进。实现代码可在此处访问：this https URL。', 'title_zh': 'BPMN 助手：基于 LLM 的业务流程建模方法'}
{'arxiv_id': 'arXiv:2509.24509', 'title': 'Experience-guided reflective co-evolution of prompts and heuristics for automatic algorithm design', 'authors': 'Yihong Liu, Junyi Li, Wayne Xin Zhao, Hongyu Lu, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2509.24509', 'abstract': 'Combinatorial optimization problems are traditionally tackled with handcrafted heuristic algorithms, which demand extensive domain expertise and significant implementation effort. Recent progress has highlighted the potential of automatic heuristics design powered by large language models (LLMs), enabling the automatic generation and refinement of heuristics. These approaches typically maintain a population of heuristics and employ LLMs as mutation operators to evolve them across generations. While effective, such methods often risk stagnating in local optima. To address this issue, we propose the Experience-Guided Reflective Co-Evolution of Prompt and Heuristics (EvoPH) for automatic algorithm design, a novel framework that integrates the island migration model with the elites selection algorithm to simulate diverse heuristics populations. In EvoPH, prompts are co-evolved with heuristic algorithms, guided by performance feedback. We evaluate our framework on two problems, i.e., Traveling Salesman Problem and Bin Packing Problem. Experimental results demonstrate that EvoPH achieves the lowest relative error against optimal solutions across both datasets, advancing the field of automatic algorithm design with LLMs.', 'abstract_zh': '经验引导的提示与启发式算法协同进化框架（EvoPH）：大规模语言模型驱动的自动算法设计', 'title_zh': '基于经验的反思性共进化提示和启发式算法自动生成'}
{'arxiv_id': 'arXiv:2509.24460', 'title': 'ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling', 'authors': 'Haotian Zhang, Liu Liu, Baosheng Yu, Jiayan Qiu, Likang Xiao, Yanwei Ren, Quan Chen, Xianglong Liu', 'link': 'https://arxiv.org/abs/2509.24460', 'abstract': "Process reward models (PRMs) have demonstrated significant efficacy in enhancing the mathematical reasoning capabilities of large language models (LLMs) by leveraging test-time scaling (TTS). However, while most PRMs exhibit substantial gains in mathematical domains, the scarcity of domain-specific training data and knowledge-based learning patterns limits their generalization ability when faced with other domains. To address this limitation, we shift the learning objective from verifying domain-specific knowledge to modeling domain-agnostic logical flow. Centering on contextual coherence between chain-of-thought (CoT) steps, our approach is realized through a novel data annotation and training framework, which enhances the model's generalization capabilities across diverse domains. For instance, our resulting model, ContextPRM, achieves a notable 6.5% average accuracy improvement over the majority voting baseline via weighted majority voting across nine non-mathematical domains in MMLU-Pro, including law, history, and philosophy, significantly surpassing the 2.2% improvement from VersaPRM and 0.5% gains from other mathematics-focused PRMs, demonstrating consistent performance across both mathematical and non-mathematical domains.", 'abstract_zh': '过程奖励模型通过测试时扩展提高大型语言模型的数学推理能力，但受限于领域特定训练数据的稀缺性和基于知识的学习模式，其在面对其他领域时的泛化能力受限。为解决这一局限，我们将学习目标从验证领域特定知识转向建模领域无关的逻辑流程。基于链式思考步骤之间的上下文连贯性，我们提出了一种新型的数据标注与训练框架，增强了模型在多种领域的泛化能力。例如，我们得到的模型ContextPRM在MMLU-Pro的九个非数学领域（包括法律、历史和哲学）中，通过加权多数投票相较于多数投票基线实现了显著的6.5%平均准确率提升，远超VersaPRM的2.2%提升和其他专注于数学的PRMs的0.5%增益，展示了在数学和非数学领域的一致性能。', 'title_zh': 'ContextPRM: 利用上下文一致性进行多领域测试时缩放'}
{'arxiv_id': 'arXiv:2509.24393', 'title': 'Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention', 'authors': 'Yichi Zhang, Yue Ding, Jingwen Yang, Tianwei Luo, Dongbai Li, Ranjie Duan, Qiang Liu, Hang Su, Yinpeng Dong, Jun Zhu', 'link': 'https://arxiv.org/abs/2509.24393', 'abstract': 'Although Large Reasoning Models (LRMs) have progressed in solving complex problems, their chain-of-thought (CoT) reasoning often contains harmful content that can persist even when the final responses appear safe. We show that this issue still remains in existing methods which overlook the unique significance of safe reasoning, undermining their trustworthiness and posing potential risks in applications if unsafe reasoning is accessible for and exploited by malicious users. We therefore shift our focus to aligning the safety of reasoning itself in this paper and explore process supervision as the solution. However, simply rewarding safe reasoning proves inadequate due to low rollout diversity and limited training signals. To tackle this challenge, we first delve into the characteristics of safe reasoning and uncover several critical insights that 1) safe reasoning is often consolidated by a few critical steps of safety triggers; 2) compliance cues strongly correlate with unsafe continuations; and 3) corrective interventions reliably steer unsafe trajectories towards safer traces. Motivated by these, we propose Intervened Preference Optimization (IPO), an alignment method that enforces safe reasoning by substituting compliance steps with safety triggers and constructing pairs for preference learning with strong signals. Experiments on jailbreak and adversarial safety benchmarks demonstrate that IPO remarkably improves overall safety regarding both reasoning and responses, outperforming SFT-based and RL-based baselines with a relative reduction of over 30% in harmfulness, while preserving excellent performance across diverse reasoning tasks. The results highlight the importance of explicit alignment for reasoning and provide a practical path to safer LRMs.', 'abstract_zh': '虽然大规模推理模型（LRMs）在解决复杂问题方面取得了进展，但它们的链式思维（Chain-of-Thought）推理往往包含有害内容，即使最终响应看似安全，这些有害内容也可能持续存在。我们发现，现有方法忽视了安全推理的独特重要性，这损害了其可信度，并且如果恶意用户能够利用不安全的推理，将对应用构成潜在风险。因此，本文将重点转向确保推理本身的安全性，并探索过程监督作为解决方案。然而，简单地奖励安全推理由于多样性过低和训练信号有限而证明是不足的。为了应对这一挑战，我们首先深入探讨安全推理的特征，并发现几个关键洞见，即1）安全推理通常由少数关键步骤的安全触发器巩固；2）合规提示与不安全续作高度相关；3）纠正干预措施能可靠地引导不安全轨迹向更安全的轨迹转变。受此启发，我们提出了干预偏好优化（IPO），这是一种通过用安全触发器替换合规步骤并构建具有强大信号的对来进行偏好学习的对齐方法。在 Jailbreak 和对抗安全基准测试中的实验表明，IPO 在提高整体安全性（包括推理和响应）方面表现出色，相对于基于SFT 和基于RL 的基线，有害性降低了超过30%，同时在各种推理任务上保持了卓越的性能。这些结果突显了明确对齐对于推理的重要性，并提供了一条通往更安全的大规模推理模型的实际路径。', 'title_zh': '通过纠正干预实现大型推理模型的安全推理'}
{'arxiv_id': 'arXiv:2509.24377', 'title': 'Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning with LLMs', 'authors': 'Shihao Qi, Jie Ma, Ziang Yin, Lingling Zhang, Jian Zhang, Jun Liu, Feng Tian, Tongliang Liu', 'link': 'https://arxiv.org/abs/2509.24377', 'abstract': 'Existing methods usually leverage a fixed strategy, such as natural language reasoning, code-augmented reasoning, tool-integrated reasoning, or ensemble-based reasoning, to guide Large Language Models (LLMs) to perform mathematical reasoning. Our analysis reveals that the single strategy cannot adapt to problem-specific requirements and thus overlooks the trade-off between effectiveness and efficiency. To address these issues, we propose Planning and Routing through Instance-Specific Modeling (PRISM), a novel framework that decouples mathematical reasoning into two stages: strategy planning and targeted execution. Specifically, we first curate a multi-strategy preference dataset, which we call MathStrat, capturing correctness, process quality, and computational efficiency for each problem--strategy pair. Then, we train a lightweight Strategy Adapter based on the dataset to obtain confidence distributions over the mentioned four reasoning strategies. At inference time, an adaptive routing policy dynamically tailors the reasoning approach based on predictor confidence. It directs the model to use single-strategy execution for high-confidence predictions, dual-strategy verification for competitive scenarios, or comprehensive multi-strategy exploration for uncertain cases. Extensive experiments across five mathematical reasoning benchmarks demonstrate that PRISM consistently outperforms individual strategies and ensemble baselines, achieving improvements ranging from 0.9% to 7.6% across different base models. The adaptive routing approach shows particularly strong benefits for mathematical reasoning tasks across diverse model architectures. Our code is released at this https URL.', 'abstract_zh': 'Planning and Routing through Instance-Specific Modeling for Mathematical Reasoning', 'title_zh': '未解决问题：面向问题的策略路由，用于具有LLM的数学推理'}
{'arxiv_id': 'arXiv:2509.24351', 'title': 'From Static to Dynamic: Adaptive Monte Carlo Search for Mathematical Process Supervision', 'authors': 'Jie Ma, Shihao Qi, Rui Xing, Ziang Yin, Bifan Wei, Jun Liu, Tongliang Liu', 'link': 'https://arxiv.org/abs/2509.24351', 'abstract': 'The quality of process data plays a key role in training a Process Reward Model (PRM), which can enhance the complex mathematical reasoning capability of large language models. Existing methods estimate the quality of reasoning steps based on a fixed-budget sampling strategy and navigate a vast search space to perform path expansion during the automated data generation process, resulting in their inefficiency and inflexibility. To address these issues, we propose Adaptive Monte Carlo Search (AMCS), a framework that transforms data generation from fixed, static to adaptive, dynamic search at the level of node value estimation and path expansion. On one hand, AMCS adaptively refines estimation by allocating more samples to uncertain reasoning steps while using fewer samples for those that are easier to estimate. On the other hand, it enhances the path expansion through a Monte Carlo algorithm with a temporally adaptive policy that begins with broad exploration and gradually shifts toward exploiting the most promising directions. With AMCS, we construct a large-scale dataset MathSearch-200K of about 200K process supervision examples for training PRMs. To verify the effectiveness of our method, we conduct extensive experiments on four mathematical reasoning benchmarks. Experimental results show that Qwen2.5-Math-7B-PRM-AMCS achieves up to 76.2% accuracy on MATH500 with GLM-4-9B, outperforming all baseline PRMs. Notably, a 7B model supervised by Qwen2.5-Math-7B-PRM-AMCS surpasses a 72B model with weaker supervision. Moreover, Qwen2.5-Math-7B-PRM-AMCS maintains consistent advantages on out-of-distribution problems, demonstrating strong generalization capability. Our code is available at this https URL.', 'abstract_zh': '过程数据的质量对训练过程奖励模型（PRM）至关重要，这可以增强大型语言模型的复杂数学推理能力。现有方法基于固定预算的采样策略估计推理步骤的质量，并在自动化数据生成过程中通过路径扩展进行广泛的搜索空间探索，导致其效率低下和灵活性不足。为解决这些问题，我们提出自适应蒙特卡洛搜索（AMCS）框架，该框架在节点值估计和路径扩展的级别将数据生成从固定、静态转变为适应性、动态搜索。一方面，AMCS通过将更多样本分配给不确定性较大的推理步骤，同时减少容易估计步骤的样本数，自适应地细化估计。另一方面，它通过一种随时间自适应的蒙特卡洛算法增强路径扩展，该算法从广泛的探索开始，逐渐转向开发最有希望的方向。使用AMCS，我们构建了一个包含约20万个过程监督示例的大型数据集MathSearch-200K，用于训练PRMs。为进一步验证我们方法的有效性，我们在四个数学推理基准上进行了大量实验。实验结果表明，Qwen2.5-Math-7B-PRM-AMCS在GLM-4-9B上的MATH500准确性达到76.2%，优于所有基线PRMs。值得注意的是，一个由Qwen2.5-Math-7B-PRM-AMCS监督的7B模型超越了一个具有较弱监督的72B模型。此外，Qwen2.5-Math-7B-PRM-AMCS在分布外问题上保持一致的优势，显示出强大的泛化能力。我们的代码可在以下链接获取。', 'title_zh': '从静态到动态：适应性蒙特卡洛搜索在数学过程监督中的应用'}
{'arxiv_id': 'arXiv:2509.24342', 'title': 'Fin-Ally: Pioneering the Development of an Advanced, Commonsense-Embedded Conversational AI for Money Matters', 'authors': 'Sarmistha Das, Priya Mathur, Ishani Sharma, Sriparna Saha, Kitsuchart Pasupa, Alka Maurya', 'link': 'https://arxiv.org/abs/2509.24342', 'abstract': "The exponential technological breakthrough of the FinTech industry has significantly enhanced user engagement through sophisticated advisory chatbots. However, large-scale fine-tuning of LLMs can occasionally yield unprofessional or flippant remarks, such as ``With that money, you're going to change the world,'' which, though factually correct, can be contextually inappropriate and erode user trust. The scarcity of domain-specific datasets has led previous studies to focus on isolated components, such as reasoning-aware frameworks or the enhancement of human-like response generation. To address this research gap, we present Fin-Solution 2.O, an advanced solution that 1) introduces the multi-turn financial conversational dataset, Fin-Vault, and 2) incorporates a unified model, Fin-Ally, which integrates commonsense reasoning, politeness, and human-like conversational dynamics. Fin-Ally is powered by COMET-BART-embedded commonsense context and optimized with a Direct Preference Optimization (DPO) mechanism to generate human-aligned responses. The novel Fin-Vault dataset, consisting of 1,417 annotated multi-turn dialogues, enables Fin-Ally to extend beyond basic account management to provide personalized budgeting, real-time expense tracking, and automated financial planning. Our comprehensive results demonstrate that incorporating commonsense context enables language models to generate more refined, textually precise, and professionally grounded financial guidance, positioning this approach as a next-generation AI solution for the FinTech sector. Dataset and codes are available at: this https URL", 'abstract_zh': '金融科技行业指数级的技术突破通过复杂的顾问聊天机器人显著增强了用户参与度。然而，大规模微调大语言模型偶尔会产生不专业或轻率的评论，如“用那些钱，你会改变世界”，虽然事实正确，但在上下文中可能不适当并侵蚀用户信任。领域特定数据集的稀缺性使得先前的研究集中在孤立的组件上，如具备推理能力的框架或增强人类样式的响应生成。为解决这一研究缺口，我们提出了Fin-Solution 2.0，一种先进的解决方案，包括1)引入多轮金融对话数据集Fin-Vault，2)结合统一模型Fin-Ally，该模型整合了常识推理、礼貌性和人类样式的对话动态。Fin-Ally由嵌入常识背景的COMET-BART驱动，并通过直接偏好优化（DPO）机制生成与人类一致的回复。新型Fin-Vault数据集包含1,417条标注的多轮对话，使Fin-Ally能够超越基本的账户管理，提供个性化的预算规划、实时支出跟踪和自动化财务规划。我们的全面结果表明，引入常识背景使语言模型能够生成更精致、文本上更精确和专业化的金融指导，将该方法定位为金融科技领域下一代AI解决方案的数据集和代码可在以下链接获取：this https URL。', 'title_zh': 'Fin-Ally: 嵌入常识的先进金融对话AI先锋发展'}
{'arxiv_id': 'arXiv:2509.24285', 'title': 'SCI-Verifier: Scientific Verifier with Thinking', 'authors': 'Shenghe Zheng, Chenyu Huang, Fangchen Yu, Junchi Yao, Jingqi Ye, Tao Chen, Yun Luo, Ning Ding, LEI BAI, Ganqu Cui, Peng Ye', 'link': 'https://arxiv.org/abs/2509.24285', 'abstract': 'As large language models (LLMs) are increasingly applied to scientific reasoning, the complexity of answer formats and the diversity of equivalent expressions make answer verification a critical yet challenging task. Existing verification studies in scientific domains suffer from two major limitations: (a) the absence of systematic evaluation standards and insufficient disciplinary coverage, which hinders their comprehensive assessment; and (b) heavy reliance on cumbersome rule design or prompt engineering, which reduces their effectiveness in complex reasoning scenarios or limits their cross-disciplinary generalization. To address these challenges, we propose solutions at both the data and model levels. On the data side, we construct SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics, biology, chemistry, and general scientific QA. The benchmark is built from real LLM responses and enhanced with domain-specific equivalence transformations that generate challenging and realistic data. Model-based and expert annotations ensure both quality and diversity, enabling rigorous evaluation of verification ability. On the model side, we emphasize the importance of reasoning for verification and introduce SCI-Verifier, a unified reasoning-augmented verifier for scientific domains. Through post-training, SCI-Verifier demonstrates strong logical reasoning and equivalence judgment capabilities while maintaining concise and stable outputs. Together, SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific verification, offering both systematic evaluation and practical pathways to enhance the reliability and applicability of LLMs in scientific domains.', 'abstract_zh': '大型语言模型（LLMs）在科学推理中的应用日益增多，答案格式的复杂性和等效表达方式的多样性使得答案验证成为一项关键而具有挑战的任务。现有科学领域内的验证研究面临两大局限：（a）缺乏系统的评估标准和学科覆盖面不足，这阻碍了其综合评估；（b）过度依赖繁琐的规则设计或提示工程技术，这在复杂推理场景中降低了其有效性或限制了其跨学科的泛化能力。为应对这些挑战，我们在数据和模型两个层面提出了解决方案。在数据方面，我们构建了SCI-VerifyBench，这是一个涵盖数学、物理、生物、化学和一般科学问答的跨学科基准。该基准源自真实的LLM响应，并通过领域特定的等效变换生成具有挑战性和现实性的数据。基于模型和专家注释确保了质量和多样性，从而能够严格评估验证能力。在模型方面，我们强调了推理对于验证的重要性，并引入了SCI-Verifier，这是一个适用于科学领域的统一推理增强验证器。通过后训练，SCI-Verifier展示了强大的逻辑推理和等价判断能力，同时保持简洁稳定的输出。总体而言，SCI-VerifyBench和SCI-Verifier为科学验证提供了一个原则性的框架，提供了一体化的评估方法和提高大型语言模型在科学领域可靠性和适用性的实际路径。', 'title_zh': 'SCI-Verifier: 科学验证器与思考'}
{'arxiv_id': 'arXiv:2509.24276', 'title': 'G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge', 'authors': 'Linhao Luo, Zicheng Zhao, Junnan Liu, Zhangchi Qiu, Junnan Dong, Serge Panev, Chen Gong, Thuy-Trang Vu, Gholamreza Haffari, Dinh Phung, Alan Wee-Chung Liew, Shirui Pan', 'link': 'https://arxiv.org/abs/2509.24276', 'abstract': 'Large language models (LLMs) excel at complex reasoning but remain limited by static and incomplete parametric knowledge. Retrieval-augmented generation (RAG) mitigates this by incorporating external knowledge, yet existing RAGs struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. Graphs offer a natural way to model relationships within knowledge, but LLMs are inherently unstructured and cannot effectively reason over graph-structured data. Recent graph-enhanced RAG (GraphRAG) attempts to bridge this gap by constructing tailored graphs and enabling LLMs to reason on them. However, these methods often depend on ad-hoc graph designs, heuristic search, or costly agent pipelines, which hinder scalability and generalization. To address these challenges, we present G-reasoner, a unified framework that integrates graph and language foundation models for reasoning over diverse graph-structured knowledge. Central to our approach is QuadGraph, a standardized four-layer abstraction that unifies heterogeneous knowledge sources into a common graph representation. Building on this, we introduce a 34M-parameter graph foundation model (GFM) that jointly captures graph topology and textual semantics, and is integrated with LLMs to enhance reasoning in downstream applications. To ensure scalability and efficiency, mixed-precision training and distributed message-passing are implemented to scale GFM with more GPUs. Extensive experiments on six benchmarks show that G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization.', 'abstract_zh': '基于图的推理器：统一图与语言基础模型框架以处理多元图结构知识', 'title_zh': 'G-reasoner: 面向图结构知识统一推理的foundation模型'}
{'arxiv_id': 'arXiv:2509.24269', 'title': 'AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models', 'authors': 'Zihao Zhu, Xinyu Wu, Gehan Hu, Siwei Lyu, Ke Xu, Baoyuan Wu', 'link': 'https://arxiv.org/abs/2509.24269', 'abstract': 'Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in complex problem-solving through Chain-of-Thought (CoT) reasoning. However, the multi-step nature of CoT introduces new safety challenges that extend beyond conventional language model alignment. We identify a failure mode in current safety CoT tuning methods: the \\textit{snowball effect}, where minor reasoning deviations progressively amplify throughout the thought process, leading to either harmful compliance or excessive refusal. This effect stems from models being trained to imitate perfect reasoning scripts without learning to self-correct. To address this limitation, we propose AdvChain, an alignment paradigm that teaches models dynamic self-correction through adversarial CoT tuning. Our method involves constructing a dataset containing Temptation-Correction and Hesitation-Correction samples, where models learn to recover from harmful reasoning drifts and unnecessary cautions. Extensive experiments show that AdvChain significantly enhances robustness against jailbreak attacks and CoT hijacking while substantially reducing over-refusal on benign prompts, achieving a superior safety-utility balance without compromising reasoning capabilities. Our work establishes a new direction for building more robust and reliable reasoning models.', 'abstract_zh': '大型推理模型（LRMs）通过链式推理（CoT）在复杂问题解决方面展现出了显著的能力。然而，CoT的多步性质引入了新的安全挑战，超出了传统语言模型对齐的范围。我们指出现有安全CoT调优方法中的一个失效率模式：雪球效应，即细微的推理偏差逐渐在整个推理过程中放大，导致要么过度合规要么过度拒绝。这种效应源于模型被训练模仿完美的推理脚本，却无法学会自我纠正。为解决这一局限，我们提出AdvChain，一种通过对抗式CoT调优教授模型动态自我纠正的对齐范式。我们的方法包括构建一个包含诱惑纠正和犹豫纠正样本的数据集，使模型学会从有害的推理偏移和不必要的谨慎中恢复。广泛实验证明，AdvChain显著增强了模型对逃脱攻击和CoT劫持的鲁棒性，同时大幅减少了对良性提示的过度拒绝，实现了安全性和实用性的优化平衡而不会牺牲推理能力。我们的工作为构建更稳健和可靠的推理模型开辟了新方向。', 'title_zh': 'AdvChain: 对抗链式推理调优以提高大型推理模型的安全对齐鲁棒性'}
{'arxiv_id': 'arXiv:2509.24261', 'title': 'Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models', 'authors': 'Yuhua Jiang, Jiawei Huang, Yufeng Yuan, Xin Mao, Yu Yue, Qianchuan Zhao, Lin Yan', 'link': 'https://arxiv.org/abs/2509.24261', 'abstract': 'Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing Large Language Models (LLMs) on complex reasoning tasks. However, existing methods suffer from an exploration dilemma: the sharply peaked initial policies of pre-trained LLMs confine standard RL algorithms to a narrow set of solutions, boosting single-solution accuracy (pass@1) but suppressing solution diversity and multi-solution performance (pass@k). As a result, RLVR often distills existing capabilities rather than discovering new reasoning strategies. To overcome this, we introduce a Risk-Sensitive Reinforcement Learning framework. Our approach employs a risk-seeking objective that interpolates between mean and maximum rewards, leading to a novel algorithm, Risk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying learning from challenging prompts. Remarkably, RS-GRPO is simple to implement, requiring only minor code modifications. On six mathematical reasoning benchmarks and with five different LLMs, RS-GRPO consistently improves pass@k performance while maintaining or enhancing pass@1 accuracy.', 'abstract_zh': '可验证奖励的强化学习（RLVR）已被证明在增强大型语言模型（LLMs）处理复杂推理任务方面效果显著。然而，现有方法面临探索困境：预训练LLMs的尖峰初始策略限制了标准RL算法的解决方案范围，提升了单解准确性（pass@1）但抑制了解决方案多样性及多解性能（pass@k）。因此，RLVR往往提炼现有能力而非发现新的推理策略。为克服这一问题，我们引入了一种风险敏感强化学习框架。该方法采用一种风险寻求的目标，介于平均值和最大值奖励之间，提出了一种新型算法——风险敏感GRPO（RS-GRPO），通过增强对挑战性提示的学习促进更深入的探索。值得注意的是，RS-GRPO易于实现，只需进行少量代码修改。在六个数学推理基准测试和五种不同的LLMs上，RS-GRPO一致提高了pass@k性能，同时保持或提升了pass@1准确性。', 'title_zh': '风险敏感的RL方法用于缓解大型语言模型中的探索困境'}
{'arxiv_id': 'arXiv:2509.24260', 'title': 'Rethinking and Benchmarking Large Language Models for Graph Reasoning', 'authors': 'Yuwei Hu, Xinyi Huang, Zhewei Wei, Yongchao Liu, Chuntao Hong', 'link': 'https://arxiv.org/abs/2509.24260', 'abstract': 'Large Language Models (LLMs) for Graph Reasoning have been extensively studied over the past two years, involving enabling LLMs to understand graph structures and reason on graphs to solve various graph problems, with graph algorithm problems being the most prevalent. Recent studies underscore the potential of LLMs in handling graph reasoning tasks, but their performance is underwhelming. In this work, we point out issues with existing methods and benchmarks, and rethink the direction that LLMs for graph reasoning should strive toward. We find that base models, e.g., GPT-4o-mini, are largely underestimated due to improper reasoning focus. Base models with reasoning focus redirected from replicating graph algorithms to designing them can easily solve most graph reasoning tasks in existing benchmarks. To truly evaluate the graph reasoning capabilities of LLMs, we construct a more challenging GraphAlgorithm benchmark, comprising 239 different graph problems and 3,041 test instances collected from 4 competition platforms. Finally, we introduce a simple and strong baseline Simple-Reasoning-Then-Coding (Simple-RTC)-which guides LLMs to design graph algorithms first and then code to address graph reasoning tasks. Simple-RTC achieves near-perfect accuracy on existing benchmarks and significantly outperforms GPT-4o-mini and all prior methods on the GraphAlgorithm benchmark. This strong baseline encourages further advancements in LLMs for Graph Reasoning in the future.', 'abstract_zh': '大规模语言模型（LLMs）在图推理中的研究：识别现有方法和基准的局限性，重新思考图推理方向，并构建更加具有挑战性的GraphAlgorithm基准，以评估图推理能力', 'title_zh': '重思与基准测试用于图推理的大语言模型'}
{'arxiv_id': 'arXiv:2509.24244', 'title': 'Model Merging Scaling Laws in Large Language Models', 'authors': 'Yuanyi Wang, Yanggan Gu, Yiming Zhang, Qi Zhou, Zhaoyi Yan, Congkai Xie, Xinyao Wang, Jianbo Yuan, Hongxia Yang', 'link': 'https://arxiv.org/abs/2509.24244', 'abstract': 'We study empirical scaling laws for language model merging measured by cross-entropy. Despite its wide practical use, merging lacks a quantitative rule that predicts returns as we add experts or scale the model size. We identify a compact power law that links model size and expert number: the size-dependent floor decreases with model capacity, while the merging tail exhibits clear diminishing returns in the number of experts. The law holds in-domain and cross-domain, tightly fits measured curves across diverse architectures and methods (Average, TA, TIES, DARE), and explains two robust regularities: most gains arrive early, and variability shrinks as more experts are included. Building on this, we present a simple theory that explains why gains fall roughly as 1/k and links the floor and tail to properties of the base model and the diversity across domains. This law enables predictive planning: estimate how many experts are needed to reach a target loss, decide when to stop adding experts, and trade off scaling the base model versus adding experts under a fixed budget--turning merging from heuristic practice into a computationally efficient, planable alternative to multitask training. This suggests a scaling principle for distributed generative AI: predictable gains can be achieved by composing specialists, offering a complementary path toward AGI-level systems.', 'abstract_zh': '我们研究了由交叉熵度量的语言模型合并的经验标度定律。尽管合并广泛应用于实践中，但仍缺乏一个定量规则来预测随着增加专家数量或扩大模型规模所带来的回报。我们发现了一个紧凑的幂律模型，将模型规模和专家数量联系起来：依赖于模型容量的下限随模型容量增加而减少，而合并的尾部随着专家数量增加表现出明显的边际收益递减现象。这一规律在领域内和领域间都适用，紧密契合不同架构和方法（Average、TA、TIES、DARE）测量出的曲线，并解释了两种稳健的规律：大多数收益出现在早期，且随着更多专家的加入，收益的波动性会减小。基于此，我们提出了一种简单的理论来解释为什么收益下降得大致与1/k成比例，并将下限和尾巴与基模型的属性以及领域间多样性联系起来。这一规律使预测性规划成为可能：估算达到目标损失所需的专家数量，决定何时停止添加专家，并在固定预算下权衡扩展基础模型与添加专家——将模型合并从一种经验做法转变为一种计算上高效的、可计划的多任务训练替代方案。这为分布式生成AI提供了一个可扩展的原则：通过组合专家可以获得可预测的收益，为通往AGI水平系统的路径提供补充。', 'title_zh': '大型语言模型中的模型合并标度律'}
{'arxiv_id': 'arXiv:2509.24238', 'title': 'Learning to Ponder: Adaptive Reasoning in Latent Space', 'authors': 'Yixin He, Lumingyuan Tang', 'link': 'https://arxiv.org/abs/2509.24238', 'abstract': 'Test-time compute has emerged as a key paradigm for enhancing LLM reasoning, yet prevailing approaches like Best-of-N and majority voting apply uniform depth across inputs, wasting computation on simple queries while potentially under-thinking complex ones. We present FR-Ponder, a single-graph, backbone-training-free framework that allocates instance-adaptive reasoning compute via latent steering. A less than 1M-param controller observes hidden states and decides to halt or apply a small ponder step by adding a pre-computed steering vector to frozen representations. Our method extracts the latent steering vector associated with deeper reasoning outputs and direct IO from LLM and re-applies it through a tunable scaling factor, allowing the model to adapt its reasoning depth to the complexity of each input. To balance performance and computational cost, we employ Group Relative Policy Optimization (GRPO) as a reward signal to adaptively regulate reasoning depth, achieving task accuracy while mitigating overreasoning. Through curriculum learning and careful reward engineering, FR-Ponder learns calibrated compute allocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder improves the compute-accuracy frontier, delivering lower FLOPs with better matched accuracy and comparing favorably to early-exit baselines, without modifying backbone weights. Analyses visualize interpretable steering directions and show learned compute allocation correlates with problem difficulty.', 'abstract_zh': 'FR-Ponder：一种基于 latent steering 的自适应推理计算框架', 'title_zh': '学会深思：潜空间中的自适应推理'}
{'arxiv_id': 'arXiv:2509.24159', 'title': 'Robust Preference Optimization: Aligning Language Models with Noisy Preference Feedback', 'authors': 'Xiaoyang Cao, Zelai Xu, Mo Guang, Kaiwen Long, Michiel A. Bakker, Yu Wang, Chao Yu', 'link': 'https://arxiv.org/abs/2509.24159', 'abstract': "Standard human preference-based alignment methods, such as Reinforcement Learning from Human Feedback (RLHF), are a cornerstone technology for aligning Large Language Models (LLMs) with human values. However, these methods are all underpinned by a critical, yet flawed assumption: human preferences are homogeneous (representing a single, unified preference) and the collected data is noiseless (free from error). In reality, neither is true since human preference is pluralistic and annotators can make mistakes. This creates a discrepancy between the recorded data and the ground-truth preferences, which can misguide the model and degrade its performance. To address this challenge, we introduce Robust Preference Optimization (RPO). RPO employs an Expectation-Maximization (EM) algorithm to infer the posterior probability of each label's correctness, which is used to adaptively re-weigh each data point in the training loss to mitigate noise. We further generalize this approach by establishing a theoretical link between arbitrary preference losses and their corresponding probabilistic models. This generalization enables the systematic transformation of existing alignment algorithms into their robust counterparts, elevating RPO from a specific algorithm to a meta-framework for robust preference alignment. Theoretically, we prove that under the condition of a perfectly calibrated model, RPO is guaranteed to converge to the true noise level of the dataset. Our experiments demonstrate RPO's effectiveness as a meta-framework, consistently enhancing four state-of-the-art alignment algorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3 models, the RPO-enhanced methods achieve substantial win rate gains on AlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% and 5.4%, respectively.", 'abstract_zh': '稳健偏好优化：一种稳健的人类偏好对齐元框架', 'title_zh': '鲁棒的偏好优化：将语言模型与嘈杂的偏好反馈对齐'}
{'arxiv_id': 'arXiv:2509.24086', 'title': 'Do Repetitions Matter? Strengthening Reliability in LLM Evaluations', 'authors': 'Miguel Angel Alvarado Gonzalez, Michelle Bruno Hernandez, Miguel Angel Peñaloza Perez, Bruno Lopez Orozco, Jesus Tadeo Cruz Soto, Sandra Malagon', 'link': 'https://arxiv.org/abs/2509.24086', 'abstract': 'LLM leaderboards often rely on single stochastic runs, but how many repetitions are required for reliable conclusions remains unclear. We re-evaluate eight state-of-the-art models on the AI4Math Benchmark with three independent runs per setting. Using mixed-effects logistic regression, domain-level marginal means, rank-instability analysis, and run-to-run reliability, we assessed the value of additional repetitions. Our findings shows that Single-run leaderboards are brittle: 10/12 slices (83\\%) invert at least one pairwise rank relative to the three-run majority, despite a zero sign-flip rate for pairwise significance and moderate overall interclass correlation. Averaging runs yields modest SE shrinkage ($\\sim$5\\% from one to three) but large ranking gains; two runs remove $\\sim$83\\% of single-run inversions. We provide cost-aware guidance for practitioners: treat evaluation as an experiment, report uncertainty, and use $\\geq 2$ repetitions under stochastic decoding. These practices improve robustness while remaining feasible for small teams and help align model comparisons with real-world reliability.', 'abstract_zh': 'LLM领奖榜往往依赖单次随机运行，但可靠结论所需重复次数仍然不确定。我们使用三个独立运行重新评估了AI4Math基准上的八种领先模型。通过混合效应逻辑回归、领域级边缘均值、排名不稳定性分析和运行间可靠性评估，我们探讨了额外重复的价值。我们的研究发现：单次运行领奖榜是脆弱的：12种切片中有10种（83%）与三次运行的多数结果在成对排名上至少翻转一次，尽管成对显著性无符号翻转且总体类间相关度适中。平均运行可适度减少标准误（从一次到三次约降低5%），但显著提高排名一致性；两次运行可去除约83%的单次运行排名翻转。我们提供成本意识指导：将评估视为实验，报告不确定性，并在随机解码下使用至少两次重复。这些做法能提升稳健性，同时对小型团队仍具可行性，并有助于使模型对比与现实可靠性一致。', 'title_zh': '重复重要吗？加强大语言模型评估的可靠性'}
{'arxiv_id': 'arXiv:2509.23988', 'title': 'LLM/Agent-as-Data-Analyst: A Survey', 'authors': 'Zirui Tang, Weizheng Wang, Zihang Zhou, Yang Jiao, Bangrui Xu, Boyu Niu, Xuanhe Zhou, Guoliang Li, Yeye He, Wei Zhou, Yitong Song, Cheng Tan, Bin Wang, Conghui He, Xiaoyang Wang, Fan Wu', 'link': 'https://arxiv.org/abs/2509.23988', 'abstract': 'Large language model (LLM) and agent techniques for data analysis (a.k.a LLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both academica and industry. In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration. The technical evolution further distills five key design goals for intelligent data analysis agents, namely semantic-aware design, modality-hybrid integration, autonomous pipelines, tool-augmented workflows, and support for open-world tasks. From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., table question answering for relational data and NL2GQL for graph data), (ii) semi-structured data (e.g., markup languages understanding and semi-structured table modeling), (iii) unstructured data (e.g., chart understanding, document understanding, programming languages vulnerable detection), and (iv) heterogeneous data (e.g., data retrieval and modality alignment for data lakes). Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis.', 'abstract_zh': '大规模语言模型（LLM）和代理技术在数据分析中的应用（即LLM/代理作为数据分析员）在学术界和工业界都展现了显著影响。与传统的基于规则或小型模型的方法相比，代理型LLM能够实现复杂的数据理解、自然语言接口、语义分析功能以及自主管道编排。技术进步进一步提炼出智能数据分析代理的五大关键设计目标，即语义感知设计、多模态集成、自主管道、工具增强的工作流程以及支持开放世界任务。从模态的角度来看，我们回顾了基于LLM的技术，包括结构化数据（例如，关系数据的表格问答和NL2GQL的图形数据）、半结构化数据（例如，标记语言理解和半结构化表格建模）、非结构化数据（例如，图表理解、文档理解、编程语言漏洞检测）以及异构数据（例如，数据湖中的数据检索和模态对齐）。最后，我们概述了剩余的挑战，并提出了促进LLM/代理驱动的数据分析的若干见解和实际方向。', 'title_zh': 'LLM/Agent-as-数据分析师：一个综述'}
{'arxiv_id': 'arXiv:2509.23986', 'title': 'TusoAI: Agentic Optimization for Scientific Methods', 'authors': 'Alistair Turcan, Kexin Huang, Lei Li, Martin Jinye Zhang', 'link': 'https://arxiv.org/abs/2509.23986', 'abstract': 'Scientific discovery is often slowed by the manual development of computational tools needed to analyze complex experimental data. Building such tools is costly and time-consuming because scientists must iteratively review literature, test modeling and scientific assumptions against empirical data, and implement these insights into efficient software. Large language models (LLMs) have demonstrated strong capabilities in synthesizing literature, reasoning with empirical data, and generating domain-specific code, offering new opportunities to accelerate computational method development. Existing LLM-based systems either focus on performing scientific analyses using existing computational methods or on developing computational methods or models for general machine learning without effectively integrating the often unstructured knowledge specific to scientific domains. Here, we introduce TusoAI , an agentic AI system that takes a scientific task description with an evaluation function and autonomously develops and optimizes computational methods for the application. TusoAI integrates domain knowledge into a knowledge tree representation and performs iterative, domain-specific optimization and model diagnosis, improving performance over a pool of candidate solutions. We conducted comprehensive benchmark evaluations demonstrating that TusoAI outperforms state-of-the-art expert methods, MLE agents, and scientific AI agents across diverse tasks, such as single-cell RNA-seq data denoising and satellite-based earth monitoring. Applying TusoAI to two key open problems in genetics improved existing computational methods and uncovered novel biology, including 9 new associations between autoimmune diseases and T cell subtypes and 7 previously unreported links between disease variants linked to their target genes. Our code is publicly available at this https URL.', 'abstract_zh': '科学发现往往因需手工开发用于分析复杂实验数据的计算工具而受阻。构建这些工具既耗时又昂贵，因为科学家必须迭代性地查阅文献、用经验数据测试建模和科学假设，并将这些见解整合到高效的软件中。大型语言模型（LLMs）在综合文献、基于经验数据推理以及生成领域特定代码方面展现出强大能力，为加速计算方法的发展提供了新的机会。现有基于LLM的系统要么专注于使用现有计算方法进行科学分析，要么专注于开发适用于通用机器学习的计算方法或模型，但未能有效整合科学领域经常存在的不结构化的知识。在这里，我们介绍了一种自主型AI系统TusoAI，它接受科学研究任务描述和评估函数，并自主开发和优化适用于特定应用的计算方法。TusoAI将领域知识整合到知识树表示中，并进行迭代的、特定于领域的优化和模型诊断，从而提升候选解决方案的性能。我们进行了全面的基准评估，结果显示，TusoAI在多种任务中优于最先进的专家方法、最大似然估计（MLE）代理和科学AI代理，比如单细胞RNA测序数据去噪和基于卫星的地球监测。将TusoAI应用于遗传学中的两个关键开放问题，不仅改进了现有的计算方法，还发现了新的生物学现象，包括9种自身免疫性疾病与T细胞亚型之间的新关联和7种以前未报道的与疾病变体及其目标基因之间的联系。我们的代码可在以下网址公开获得：this https URL。', 'title_zh': 'TusoAI: 主体化优化科学方法'}
{'arxiv_id': 'arXiv:2509.23962', 'title': 'Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models', 'authors': 'Guanxu Chen, Yafu Li, Yuxian Jiang, Chen Qian, Qihan Ren, Jingyi Yang, Yu Cheng, Dongrui Liu, Jing Shao', 'link': 'https://arxiv.org/abs/2509.23962', 'abstract': "Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning capabilities on tasks with clear correctness criteria, such as mathematical reasoning tasks. Several training metrics, such as entropy or response length, have been observed to correlate with different reasoning behaviors in reinforcement learning. Prior approaches incorporate such priors through reward or advantage shaping, which often relies on hand-crafted penalties and preferences (e.g., higher-is-better or lower-is-better). However, without careful hyperparameter tuning, these directional priors can be overly biased and may lead to failure. To this end, we introduce Conditional advANtage estimatiON (CANON), amplifying the impact of the target metric without presuming its direction. Specifically, CANON regroups the sampled responses into two groups based on the higher or lower value of a target metric, measures which metric trend contributes to better performance through inter-group comparison, and identifies the better response within the same group. In summary, CANON based on entropy consistently outperforms prior methods across three LLMs on both math reasoning and high-complexity logic tasks. When applied to response length, CANON further improves token efficiency, yielding a more favorable Pareto frontier in the performance-cost trade-off.", 'abstract_zh': '可验证奖励的强化学习（RLVR）在大型语言模型（LLMs）上的进展：以数学推理任务等具有明确正确性标准的任务为例', 'title_zh': '大型推理模型中强化学习的优势估计条件优势估计'}
{'arxiv_id': 'arXiv:2509.23882', 'title': "Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B", 'authors': 'Shuyi Lin, Tian Lu, Zikai Wang, Bo Wen, Yibo Zhao, Cheng Tan', 'link': 'https://arxiv.org/abs/2509.23882', 'abstract': "OpenAI's GPT-OSS family provides open-weight language models with explicit chain-of-thought (CoT) reasoning and a Harmony prompt format. We summarize an extensive security evaluation of GPT-OSS-20B that probes the model's behavior under different adversarial conditions. Us- ing the Jailbreak Oracle (JO) [1], a systematic LLM evaluation tool, the study uncovers several failure modes including quant fever, reasoning blackholes, Schrodinger's compliance, reasoning procedure mirage, and chain-oriented prompting. Experiments demonstrate how these behaviors can be exploited on GPT-OSS-20B models, leading to severe consequences.", 'abstract_zh': 'OpenAI的GPT-OSS家族提供了带有显式链式思考（CoT）推理和和谐提示格式的开源权重语言模型。我们总结了对GPT-OSS-20B的广泛安全评估，探究了该模型在不同 adversarial 条件下的行为。利用Jailbreak Oracle（JO）系统性的LLM评估工具，研究发现了几种失败模式，包括量化狂热、推理黑洞、薛定谔的合规性、推理过程幻象以及链式推理提示。实验展示了这些行为如何在GPT-OSS-20B模型上被利用，导致严重后果。', 'title_zh': '量化狂热、推理黑洞、薛定谔的合规性以及其他现象：探究GPT-OSS-20B'}
{'arxiv_id': 'arXiv:2509.23836', 'title': 'Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules', 'authors': 'Chenyu Zhou, Xiaoming Shi, Hui Qiu, Xiawu Zheng, Haitao Leng, Yankai Jiang, Shaoguo Liu, Tingting Gao, Rongrong Ji', 'link': 'https://arxiv.org/abs/2509.23836', 'abstract': "E-commerce agents contribute greatly to helping users complete their e-commerce needs. To promote further research and application of e-commerce agents, benchmarking frameworks are introduced for evaluating LLM agents in the e-commerce domain. Despite the progress, current benchmarks lack evaluating agents' capability to handle mixed-type e-commerce dialogue and complex domain rules. To address the issue, this work first introduces a novel corpus, termed Mix-ECom, which is constructed based on real-world customer-service dialogues with post-processing to remove user privacy and add CoT process. Specifically, Mix-ECom contains 4,799 samples with multiply dialogue types in each e-commerce dialogue, covering four dialogue types (QA, recommendation, task-oriented dialogue, and chit-chat), three e-commerce task types (pre-sales, logistics, after-sales), and 82 e-commerce rules. Furthermore, this work build baselines on Mix-Ecom and propose a dynamic framework to further improve the performance. Results show that current e-commerce agents lack sufficient capabilities to handle e-commerce dialogues, due to the hallucination cased by complex domain rules. The dataset will be publicly available.", 'abstract_zh': '电子商务代理在帮助用户完成电子商务需求方面发挥了巨大作用。为了促进电子商务代理的进一步研究和应用，介绍了基于电子商务领域的大型语言模型代理的基准框架以进行评估。尽管已经取得了进展，当前的基准测试工具缺乏对代理处理混合类型电子商务对话和复杂领域规则的能力进行评估。为解决这一问题，本文首先引入了一个新的语料库，称为Mix-ECom，该语料库基于真实的客户服务对话构建，并经过后处理以移除用户隐私并添加逐步思考过程。具体而言，Mix-ECom 包含4,799个样本，每个电子商务对话中包含多种对话类型，涵盖四种对话类型（问答、推荐、任务导向对话和闲聊）、三种电子商务任务类型（销售前、物流、销售后）和82条电子商务规则。此外，本文在Mix-ECom上构建了基线并提出了一个动态框架以进一步提高性能。结果表明，当前的电子商务代理在处理电子商务对话方面缺乏足够的能力，这主要是由于复杂领域规则引起的幻觉。该数据集将公开发布。', 'title_zh': 'Mix-Ecom: 向混合类型电子商务对话规则复杂域规则的方向发展'}
{'arxiv_id': 'arXiv:2509.23783', 'title': 'Falcon: A Cross-Modal Evaluation Dataset for Comprehensive Safety Perception', 'authors': 'Qi Xue, Minrui Jiang, Runjia Zhang, Xiurui Xie, Pei Ke, Guisong Liu', 'link': 'https://arxiv.org/abs/2509.23783', 'abstract': 'Existing methods for evaluating the harmfulness of content generated by large language models (LLMs) have been well studied. However, approaches tailored to multimodal large language models (MLLMs) remain underdeveloped and lack depth. This work highlights the crucial role of visual information in moderating content in visual question answering (VQA), a dimension often overlooked in current research. To bridge this gap, we introduce Falcon, a large-scale vision-language safety dataset containing 57,515 VQA pairs across 13 harm categories. The dataset provides explicit annotations for harmful attributes across images, instructions, and responses, thereby facilitating a comprehensive evaluation of the content generated by MLLMs. In addition, it includes the relevant harm categories along with explanations supporting the corresponding judgments. We further propose FalconEye, a specialized evaluator fine-tuned from Qwen2.5-VL-7B using the Falcon dataset. Experimental results demonstrate that FalconEye reliably identifies harmful content in complex and safety-critical multimodal dialogue scenarios. It outperforms all other baselines in overall accuracy across our proposed Falcon-test dataset and two widely-used benchmarks-VLGuard and Beavertail-V, underscoring its potential as a practical safety auditing tool for MLLMs.', 'abstract_zh': '现有的评估大型语言模型生成内容危害性的方法已有广泛研究，但针对多模态大型语言模型的方法仍嫌不足且缺乏深度。本文强调了在视觉问答(VQA)中视觉信息在内容调节中的关键作用，这一维度在当前研究中经常被忽视。为了解决这一问题，我们介绍了Falcon，一个包含57,515个VQA对（跨越13个危害类别）的大规模视觉-语言安全数据集。该数据集为图像、指令和响应中的有害属性提供了明确注解，从而促进了对多模态大型语言模型生成内容的全面评估。此外，该数据集还包含了相应的危害类别及其支持判断的解释。我们进一步提出了FalconEye，一个基于Qwen2.5-VL-7B微调的专业评估器，使用Falcon数据集进行训练。实验结果表明，FalconEye能够可靠地识别复杂和安全性关键的多模态对话场景中的有害内容，在我们提出的Falcon-test数据集和两个广泛使用的基准VLGuard和Beavertail-V上实现了全面准确性上的优势，证明了其作为多模态大型语言模型实际安全性审计工具的潜力。', 'title_zh': 'Falcon：全面安全感知的跨模态评估数据集'}
{'arxiv_id': 'arXiv:2509.23768', 'title': 'From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning', 'authors': 'Cheng Yang, Jiaxuan Lu, Haiyuan Wan, Junchi Yu, Feiwei Qin', 'link': 'https://arxiv.org/abs/2509.23768', 'abstract': 'The chemical reaction recommendation is to select proper reaction condition parameters for chemical reactions, which is pivotal to accelerating chemical science. With the rapid development of large language models (LLMs), there is growing interest in leveraging their reasoning and planning capabilities for reaction condition recommendation. Despite their success, existing methods rarely explain the rationale behind the recommended reaction conditions, limiting their utility in high-stakes scientific workflows. In this work, we propose ChemMAS, a multi-agent system that reframes condition prediction as an evidence-based reasoning task. ChemMAS decomposes the task into mechanistic grounding, multi-channel recall, constraint-aware agentic debate, and rationale aggregation. Each decision is backed by interpretable justifications grounded in chemical knowledge and retrieved precedents. Experiments show that ChemMAS achieves 20-35% gains over domain-specific baselines and outperforms general-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable, human-trustable rationales, which establishes a new paradigm for explainable AI in scientific discovery.', 'abstract_zh': '化学反应推荐是选择合适的反应条件参数，这对于加速化学科学发展至关重要。随着大规模语言模型（LLMs）的快速发展，人们越来越有兴趣利用它们的推理和规划能力进行反应条件推荐。尽管取得了成功，现有方法很少解释推荐的反应条件背后的理由，限制了它们在高风险科学工作流程中的应用价值。在本文中，我们提出了一种多智能体系统ChemMAS，将条件预测重新构想为基于证据的推理任务。ChemMAS将任务分解为机械主义基础、多通道回忆、约束aware代理辩论和理由聚合。每个决策都基于化学知识和检索的先例提供了可解释的解释。实验结果显示，ChemMAS在特定领域基线上的性能提高了20-35%，并且在Top-1准确性上比通用的大规模语言模型高10-15%，同时提供可证伪、可信赖的人类可理解的理由，这确立了一种新的可解释AI在科学发现中的范式。', 'title_zh': '从“怎么做”到“为什么这样做”：一种基于证据的化学反应条件推理多代理系统'}
{'arxiv_id': 'arXiv:2509.23730', 'title': 'EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance', 'authors': 'Siyao Song, Cong Ma, Zhihao Cheng, Shiye Lei, Minghao Li, Ying Zeng, Huaixiao Tou, Kai Jia', 'link': 'https://arxiv.org/abs/2509.23730', 'abstract': "Large language models (LLMs) have recently advanced in reasoning when optimized with reinforcement learning (RL) under verifiable rewards. Existing methods primarily rely on outcome-based supervision to strengthen internal LLM reasoning, often leading to inefficient exploration and sparse rewards. To mitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a novel RL framework that enhances exploration by incorporating multi-turn interactions with external experts during training. Unlike prior methods, where policies reason in isolation, EAPO incentivizes the policy to adaptively determine when and how to consult experts, yielding richer reward signals and more reliable reasoning trajectories. External assistance ultimately internalizes expert knowledge into the policy model, amplifying the model's inherent reasoning capabilities. During evaluation, the policy model has been well-optimized to solve questions independently, producing improved reasoning paths and more accurate solutions. Experiments on mathematical reasoning benchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO consistently outperforms expert-assisted workflow, expert-distilled models, and RL baselines, with an average gain of 5 points over self-exploratory models.", 'abstract_zh': '大型语言模型在验证奖励下的强化学习优化中推理能力的进步：专家辅助策略优化(EAPO)方法的研究', 'title_zh': 'EAPO: 以需求为导向的专家协助增强策略优化'}
{'arxiv_id': 'arXiv:2509.23694', 'title': 'SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents', 'authors': 'Jianshuo Dong, Sheng Guo, Hao Wang, Zhuotao Liu, Tianwei Zhang, Ke Xu, Minlie Huang, Han Qiu', 'link': 'https://arxiv.org/abs/2509.23694', 'abstract': 'Search agents connect LLMs to the Internet, enabling access to broader and more up-to-date information. However, unreliable search results may also pose safety threats to end users, establishing a new threat surface. In this work, we conduct two in-the-wild experiments to demonstrate both the prevalence of low-quality search results and their potential to misguide agent behaviors. To counter this threat, we introduce an automated red-teaming framework that is systematic, scalable, and cost-efficient, enabling lightweight and harmless safety assessments of search agents. Building on this framework, we construct the SafeSearch benchmark, which includes 300 test cases covering five categories of risks (e.g., misinformation and indirect prompt injection). Using this benchmark, we evaluate three representative search agent scaffolds, covering search workflow, tool-calling, and deep research, across 7 proprietary and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities of LLM-based search agents: when exposed to unreliable websites, the highest ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover, our analysis highlights the limited effectiveness of common defense practices, such as reminder prompting. This emphasizes the value of our framework in promoting transparency for safer agent development. Our codebase and test cases are publicly available: this https URL.', 'abstract_zh': '搜索代理将LLM连接到互联网，使其能够访问更广泛和更及时的信息。然而，不可靠的搜索结果也可能对终端用户构成安全威胁，形成新的威胁面。在此工作中，我们进行了两项实地实验，以证明低质量搜索结果的普遍性及其误导搜索代理行为的潜在风险。为了应对这一威胁，我们引入了一种系统、可扩展且成本效益高的自动化红队框架，能够对搜索代理进行轻量级且无害的安全评估。基于此框架，我们构建了SafeSearch基准，其中包括300个测试案例，覆盖五类风险（例如， misinformation和间接提示注入）。使用该基准，我们评估了三种代表性的搜索代理框架，覆盖搜索工作流、工具调用和深入研究，分别在7个私有和8个开源后端LLM上进行了评估。我们的结果揭示了基于LLM的搜索代理的主要漏洞：在搜索工作流设置中，GPT-4.1-mini的最大自动完成率达到了90.5%。此外，我们的分析还强调了常用防御措施（如提醒提示）的有效性有限，进一步突显了我们框架在促进更安全代理开发透明度方面的价值。我们的代码库和测试案例已公开：this https URL。', 'title_zh': 'SafeSearch：基于自动化红队演练的LLM搜索代理安全性评估'}
{'arxiv_id': 'arXiv:2509.23630', 'title': 'Game-Oriented ASR Error Correction via RAG-Enhanced LLM', 'authors': 'Yan Jiang, Yongle Luo, Qixian Zhou, Elvis S. Liu', 'link': 'https://arxiv.org/abs/2509.23630', 'abstract': 'With the rise of multiplayer online games, real-time voice communication is essential for team coordination. However, general ASR systems struggle with gaming-specific challenges like short phrases, rapid speech, jargon, and noise, leading to frequent errors. To address this, we propose the GO-AEC framework, which integrates large language models, Retrieval-Augmented Generation (RAG), and a data augmentation strategy using LLMs and TTS. GO-AEC includes data augmentation, N-best hypothesis-based correction, and a dynamic game knowledge base. Experiments show GO-AEC reduces character error rate by 6.22% and sentence error rate by 29.71%, significantly improving ASR accuracy in gaming scenarios.', 'abstract_zh': '随着多人在线游戏的兴起，实时语音通信对于团队协调至关重要。然而，一般的语音识别（ASR）系统难以应对游戏特有的挑战，如短语、快速 speech、行业术语和噪音，导致频繁出错。为了解决这一问题，我们提出了一种GO-AEC框架，该框架整合了大规模语言模型、检索增强生成（RAG）以及使用大语言模型（LLM）和文本转语音（TTS）的数据增强策略。GO-AEC包括数据增强、基于N-best假设的纠错以及动态游戏知识库。实验结果显示，GO-AEC将字符错误率降低了6.22%，句子错误率降低了29.71%，显著提高了游戏场景下的ASR准确性。', 'title_zh': '面向游戏的ASR错误校正：RAG增强的大语言模型方法'}
{'arxiv_id': 'arXiv:2509.23629', 'title': 'How LLMs Learn to Reason: A Complex Network Perspective', 'authors': 'Sihan Hu, Xiansheng Cai, Yuan Huang, Zhiyuan Yao, Linfeng Zhang, Pan Zhang, Youjin Deng, Kun Chen', 'link': 'https://arxiv.org/abs/2509.23629', 'abstract': "Training large language models with Reinforcement Learning from Verifiable Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain poorly understood, including a two-stage learning curve, V-shaped response-length trajectories, and a pronounced vulnerability to catastrophic forgetting. In this work, we propose that these seemingly disparate phenomena can be explained using a single unifying theory: the model's reasoning process maps to the self-organization of a semantic complex network whose topology remains persistently sparse, with the average degree pinned close to two. This topology imposes a fundamental mechanism for forgetting and learning: it first drives the system into a maximally frustrated state where ``skill islands'' form, slow-learning happens, and forgetting is induced; then it enters a sharp growth phase where the new skills are ``bolted on'', driven by phase-transition-like learning at the web's frontier. Equipped with the theory, we propose \\textit{Annealed-RLVR}, a principled algorithm that introduces an SFT-based ``heating'' step at the point of maximal frustration to resolve the competitive bottleneck and enhance the reasoning capability of the model. Experiments on a 1.5B-parameter model demonstrate that the approach outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks. By recasting RLVR from black-box optimization into a predictable process of structural self-organization, our work provides a new physical intuition for engineering the emergent reasoning capabilities of future AI systems.", 'abstract_zh': '使用可验证奖励的强化学习训练大型语言模型（RLVR）表现出一系列distinctive和puzzling的行为，这些行为尚未充分理解，包括两阶段学习曲线、V型响应长度轨迹以及明显的灾难性遗忘易感性。在这项工作中，我们提出这些看似不相关的现象可以由单一的统一理论解释：模型的推理过程映射到语义复杂网络的自我组织，其拓扑结构持久地保持稀疏，平均度接近2。这种拓扑结构施加了一种基本的遗忘和学习机制：首先将系统驱动到最大化挫败状态，形成“技能岛屿”，学习缓慢，导致遗忘；然后进入一个快速增长阶段，在这个阶段，新技能由类似相变的前端学习驱动而“附加”上。借助该理论，我们提出了一种原理性的算法——Annealed-RLVR，在最大挫败状态引入基于SFT的“加热”步骤以解决竞争瓶颈并增强模型的推理能力。对一个1.5亿参数模型的实验表明，该方法在分布内和分布外基准上均优于标准RLVR。通过将RLVR从黑盒优化重新构想为可预测的结构自我组织过程，我们的工作为工程未来AI系统的涌现推理能力提供了新的物理直觉。', 'title_zh': 'LLMs如何推理：一种复杂网络视角'}
{'arxiv_id': 'arXiv:2509.23619', 'title': 'Reasoning Scaffolding: Distilling the Flow of Thought from LLMs', 'authors': 'Xiangyu Wen, Junhua Huang, Zeju Li, Min Li, Jianyuan Zhong, Zhijian Xu, Mingxuan Yuan, Yongxiang Huang, Qiang Xu', 'link': 'https://arxiv.org/abs/2509.23619', 'abstract': "The prevailing approach to distilling reasoning from Large Language Models (LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It teaches Small Language Models (SLMs) to mimic surface-level patterns rather than the underlying algorithmic structure of thought, resulting in a critical lack of logical robustness. We argue that instead of cloning text, distillation should transfer this algorithmic structure directly. We introduce Reasoning Scaffolding}, a framework that reframes reasoning as a structured generation process. Our method first abstracts the teacher's thought process into a sequence of discrete, interpretable semantic signals (e.g., Contrast, Addition) that act as a scaffold. The student model is then trained via a multi-task objective to both (1)predict the next semantic signal, anticipating the reasoning flow, and (2)generate the corresponding step, conditioned on that signal. This multi-task scheme acts as a powerful regularizer, compelling the student to internalize the computational patterns of coherent reasoning. On a suite of challenging reasoning benchmarks, our method significantly outperforms state-of-the-art distillation in both accuracy and logical consistency, providing a path towards creating smaller models that are genuine reasoners, not just fluent mimics.", 'abstract_zh': '从大型语言模型中提炼推理的盛行方法——基于文本理由的行为克隆——从根本上是有限的。我们提出，蒸馏应该直接转移这种算法结构，而不是克隆文本。我们介绍了Reasoning Scaffolding框架，将其推理重新定义为结构化的生成过程。我们的方法首先将教师的思维过程抽象为一系列可解释的语义信号（例如，对比、增加），作为支架。然后，学生模型通过多任务目标训练，不仅要（1）预测下一个语义信号，预见推理流程，还要（2）在该信号的条件下生成相应的步骤。这种多任务方案作为一种强大的正则化器，促使学生内化连贯推理的计算模式。在一系列具有挑战性的推理基准测试中，我们的方法在准确性和逻辑一致性方面显著优于现有的蒸馏技术，提供了一条创造真正推理者而非只是流利模仿者的较小模型的道路。', 'title_zh': '思维支撑：从大模型中提炼思维流程'}
{'arxiv_id': 'arXiv:2509.23614', 'title': 'PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents', 'authors': 'Yaozu Wu, Jizhou Guo, Dongyuan Li, Henry Peng Zou, Wei-Chieh Huang, Yankai Chen, Zhen Wang, Weizhi Zhang, Yangning Li, Meng Zhang, Renhe Jiang, Philip S. Yu', 'link': 'https://arxiv.org/abs/2509.23614', 'abstract': 'Effective guardrails are essential for safely deploying LLM-based agents in critical applications. Despite recent advances, existing guardrails suffer from two fundamental limitations: (i) they apply uniform guardrail policies to all users, ignoring that the same agent behavior can harm some users while being safe for others; (ii) they check each response in isolation, missing how risks evolve and accumulate across multiple interactions. To solve these issues, we propose PSG-Agent, a personalized and dynamic system for LLM-based agents. First, PSG-Agent creates personalized guardrails by mining the interaction history for stable traits and capturing real-time states from current queries, generating user-specific risk thresholds and protection strategies. Second, PSG-Agent implements continuous monitoring across the agent pipeline with specialized guards, including Plan Monitor, Tool Firewall, Response Guard, Memory Guardian, that track cross-turn risk accumulation and issue verifiable verdicts. Finally, we validate PSG-Agent in multiple scenarios including healthcare, finance, and daily life automation scenarios with diverse user profiles. It significantly outperform existing agent guardrails including LlamaGuard3 and AGrail, providing an executable and auditable path toward personalized safety for LLM-based agents.', 'abstract_zh': '有效的边界措施对于在关键应用中安全部署基于LLM的代理至关重要。尽管近期取得了进展，现有的边界措施仍存在两个根本性的局限性：（i）它们对所有用户应用统一的边界措施政策，忽略了相同代理行为对不同用户可能造成的不同影响；（ii）它们孤立地检查每个响应，未能捕捉风险在多次交互中的演变和累积。为了克服这些问题，我们提出PSG-Agent，一种个性化的动态系统。首先，PSG-Agent通过挖掘交互历史记录以发现稳定特征，并从当前查询中捕获实时状态，生成用户特定的风险阈值和保护策略。其次，PSG-Agent在代理管道中实施持续监控，包括计划监控、工具防火墙、响应守护和记忆守护，以跟踪跨回合风险积累并发出可验证的判断。最后，我们在医疗保健、金融和日常生活自动化等多个场景中对PSG-Agent进行了验证，涵盖了不同的用户配置文件。它在多个方面显著优于现有的代理边界措施，包括LlamaGuard3和AGrail，为基于LLM的代理提供了一条可执行和可审计的个性化安全路径。', 'title_zh': 'PSG-Agent：面向人格的安全防护准则'}
{'arxiv_id': 'arXiv:2509.23564', 'title': 'Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment', 'authors': 'Min-Hsuan Yeh, Yixuan Li', 'link': 'https://arxiv.org/abs/2509.23564', 'abstract': 'Human feedback plays a pivotal role in aligning large language models (LLMs) with human preferences. However, such feedback is often noisy or inconsistent, which can degrade the quality of reward models and hinder alignment. While various automated data cleaning methods have been proposed to mitigate this issue, a systematic evaluation of their effectiveness and generalizability remains lacking. To bridge this gap, we introduce the first comprehensive benchmark for evaluating 13 preference data cleaning methods in the context of LLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning strategies in terms of alignment performance and generalizability across diverse datasets, model architectures, and optimization algorithms. By unifying disparate methods and rigorously comparing them, we uncover key factors that determine the success of data cleaning in alignment tasks. This benchmark lays the groundwork for principled and reproducible approaches to improving LLM alignment through better data quality-highlighting the crucial but underexplored role of data preprocessing in responsible AI development. We release modular implementations of all methods to catalyze further research: this https URL.', 'abstract_zh': '人类反馈在使大型语言模型与人类偏好保持一致中发挥着关键作用。然而，此类反馈往往嘈杂或不一致，这会降低奖励模型的质量并阻碍一致性的提升。尽管已经提出了各种自动数据清洗方法来缓解这一问题，但对其有效性和通用性的系统评估仍然不足。为解决这一问题，我们首次推出了一个全面的基准，用于在大型语言模型对齐的背景下评估13种偏好数据清洗方法。PrefCleanBench 提供了一种标准化的评估协议，用于根据对齐性能和跨多种数据集、模型架构和优化算法的一般性评估清洗策略。通过统一不同的方法并严格比较它们，我们发现了决定数据清洗在对齐任务中成功的关键因素。该基准为通过提高数据质量改进大型语言模型对齐的原理性和可重复方法奠定了基础——突显了数据预处理在负责任AI发展中至关重要的但未被充分探索的作用。我们发布了所有方法的模块化实现，以促进进一步的研究：[此链接](此httpsURL)。', 'title_zh': '先清洁，后对齐：可靠的大语言模型对齐的偏好数据清洁基准测试'}
{'arxiv_id': 'arXiv:2509.23558', 'title': 'Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning', 'authors': 'Zhaoqi Wang, Daqing He, Zijian Zhang, Xin Li, Liehuang Zhu, Meng Li, Jiamou Liu', 'link': 'https://arxiv.org/abs/2509.23558', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities, yet they also introduce novel security challenges. For instance, prompt jailbreaking attacks involve adversaries crafting sophisticated prompts to elicit responses from LLMs that deviate from human values. To uncover vulnerabilities in LLM alignment methods, we propose the PASS framework (\\underline{P}rompt J\\underline{a}ilbreaking via \\underline{S}emantic and \\underline{S}tructural Formalization). Specifically, PASS employs reinforcement learning to transform initial jailbreak prompts into formalized descriptions, which enhances stealthiness and enables bypassing existing alignment defenses. The jailbreak outputs are then structured into a GraphRAG system that, by leveraging extracted relevant terms and formalized symbols as contextual input alongside the original query, strengthens subsequent attacks and facilitates more effective jailbreaks. We conducted extensive experiments on common open-source models, demonstrating the effectiveness of our attack.', 'abstract_zh': '大型语言模型（LLMs）展示了 remarkable 的能力，但同时也引入了新型安全挑战。例如，提示脱管攻击涉及对手构造复杂的提示以促使LLMs产生违背人类价值观的响应。为了揭露LLM对齐方法中的漏洞，我们提出了PASS框架（通过语义和结构形式化进行提示脱管攻击，Prompt Jailbreaking via Semantic and Structural Formalization）。具体而言，PASS 使用强化学习将初始脱管攻击提示转化为形式化的描述，增强了隐蔽性并允许绕过现有的对齐防御。脱管攻击输出随后被结构化到GraphRAG系统中，通过结合提取的相关术语和形式化符号作为上下文输入，以及原始查询，加强了后续攻击，并促进了更有效的脱管攻击。我们在常见的开源模型上进行了广泛的实验，证明了我们攻击的有效性。', 'title_zh': '基于强化学习的正式化驱动的大规模语言模型提示打破'}
{'arxiv_id': 'arXiv:2509.23537', 'title': 'Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks', 'authors': 'Aaron Xuxiang Tian, Ruofan Zhang, Jiayao Tang, Young Min Cho, Xueqian Li, Qiang Yi, Ji Wang, Zhunping Zhang, Danrui Qi, Sharath Chandra Guntuku, Lyle Ungar, Tianyu Shi, Chi Wang', 'link': 'https://arxiv.org/abs/2509.23537', 'abstract': 'We study multi-turn multi-agent orchestration, where multiple large language model (LLM) agents interact over multiple turns by iteratively proposing answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5 Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we conduct two experiments: (i) benchmarking orchestration against single-LLM baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who authored answers and whether they can observe ongoing votes. Orchestration matches or exceeds the strongest single model and consistently outperforms the others. Analysis of best-achievable orchestration performance shows potential for further gains. The ablations show that revealing authorship increases self-voting and ties, and that showing ongoing votes amplifies herding, which speeds convergence but can sometimes yield premature consensus.', 'abstract_zh': '多轮多agent协调研究：多大规模语言模型代理通过多次交互提出答案或投票以达成共识', 'title_zh': '超越最强的LLM：多轮多Agent orchestration vs. 单个LLM在基准测试上的表现'}
{'arxiv_id': 'arXiv:2509.23510', 'title': 'Model Consistency as a Cheap yet Predictive Proxy for LLM Elo Scores', 'authors': 'Ashwin Ramaswamy, Nestor Demeure, Ermal Rrapaj', 'link': 'https://arxiv.org/abs/2509.23510', 'abstract': 'New large language models (LLMs) are being released every day. Some perform significantly better or worse than expected given their parameter count. Therefore, there is a need for a method to independently evaluate models. The current best way to evaluate a model is to measure its Elo score by comparing it to other models in a series of contests - an expensive operation since humans are ideally required to compare LLM outputs. We observe that when an LLM is asked to judge such contests, the consistency with which it selects a model as the best in a matchup produces a metric that is 91% correlated with its own human-produced Elo score. This provides a simple proxy for Elo scores that can be computed cheaply, without any human data or prior knowledge.', 'abstract_zh': '新的大型语言模型（LLMs）每天都在发布。一些模型的性能显著优于或低于其参数数量所预期的效果。因此，需要一种独立评估模型的方法。目前评估模型的最佳方法是通过一系列比赛来比较模型的Elo评分——这是一个昂贵的操作，因为人类通常需要参与比较LLM的输出。我们观察到，当要求LLM判断这些比赛时，它在比赛中选择最佳模型的稳定程度与自身的手工生成的Elo评分有91%的相关性。这提供了一种无需人类数据或先验知识且计算成本低廉的Elo评分代理指标。', 'title_zh': '模型一致性作为LLM Elo评分的廉价但有效的替代指标'}
{'arxiv_id': 'arXiv:2509.23488', 'title': 'Mapping Overlaps in Benchmarks through Perplexity in the Wild', 'authors': 'Siyang Wu, Honglin Bao, Sida Li, Ari Holtzman, James A. Evans', 'link': 'https://arxiv.org/abs/2509.23488', 'abstract': 'We develop signatures of capacity familiarity to characterize large language model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures probe the capacity required for benchmark performance. We formally define them as a set of salient tokens drawn from in-the-wild, naturally authored corpora, where LLM token perplexity, reflecting more or less pre-training exposure, becomes highly predictive of LLM benchmark performance. Through a large-scale meta-evaluation, we extract benchmark signatures via stepwise forward selection with linear regressions across 32 LLMs and 88 benchmarks spanning diverse knowledge, coding, logic, instruction following, math, language, reasoning, and world modeling. Our analysis situates signatures in relation to both the semantic similarity of benchmark questions and the correlation of model performance. While performance overlaps are universally high and semantic overlaps remain confined to a narrow mid-range, benchmark signatures prove highly informative in capturing variation, overlap, and divergence. We observe overlap in knowledge and reasoning subtasks, whereas multilingual and cultural benchmarks exhibit less similarity, even compared to cross-task overlap. Notably, performance-level results are strongly influenced by benchmark-orthogonal factors such as question format, highlighting limitations in LLM generalization, the conflation of performance with ability, and issues inherent in current mainstream benchmark agreement studies. Benchmark signatures, however, remain robust to such effects. Ultimately, we identify cross-functional overlaps across logic, math, language, instruction following, and world modeling, with coding emerging as the least overlapping domain. Together, these findings provide mechanistic insights into benchmark validity and LLM sensitivities, and sketch the underlying landscape of interconnected LLM capabilities.', 'abstract_zh': '我们开发了能力熟悉度特征以表征大规模语言模型（LLM）基准及其有意义的重叠。基准特征探究了基准性能所需的能力。我们正式定义它们为来自自然生成语料库中的显著标记集，其中LLM标记困惑度反映了更多或更少的预训练暴露，高度预测LLM基准性能。通过大规模元评估，我们通过逐步前进选择和在32个LLM和88个覆盖广泛知识、编程、逻辑、指令遵循、数学、语言、推理和世界建模的基准上的线性回归提取基准特征。我们的分析将特征置于基准问题语义相似性和模型性能相关性的关系中。尽管性能重叠普遍较高，而语义重叠则限制在狭窄的中等范围内，基准特征在捕捉变异、重叠和差异方面证明高度信息丰富。我们观察到知识和推理子任务中的重叠，多语言和文化基准则表现出较少的相似性，即使与多任务重叠相比。值得注意的是，基准性能水平的结果受到如问题格式等基准正交因素的强烈影响，突显了LLM泛化能力的局限性、将性能等同于能力的问题以及当前主流基准一致研究中存在的内在问题。然而，基准特征对这些影响表现出稳定性。最终，我们在逻辑、数学、语言、指令遵循和世界建模之间识别出跨功能重叠，编程领域成为重叠最少的领域。这些发现共同提供了关于基准有效性和LLM敏感性的机制性见解，并勾勒出相连的LLM能力的潜在景观。', 'title_zh': '在野生环境中的困惑度映射基准中的重叠'}
{'arxiv_id': 'arXiv:2509.23415', 'title': 'From Conversation to Query Execution: Benchmarking User and Tool Interactions for EHR Database Agents', 'authors': 'Gyubok Lee, Woosog Chay, Heeyoung Kwak, Yeong Hwa Kim, Haanju Yoo, Oksoon Jeong, Meong Hi Son, Edward Choi', 'link': 'https://arxiv.org/abs/2509.23415', 'abstract': 'Despite the impressive performance of LLM-powered agents, their adoption for Electronic Health Record (EHR) data access remains limited by the absence of benchmarks that adequately capture real-world clinical data access flows. In practice, two core challenges hinder deployment: query ambiguity from vague user questions and value mismatch between user terminology and database entries. To address this, we introduce EHR-ChatQA an interactive database question answering benchmark that evaluates the end-to-end workflow of database agents: clarifying user questions, using tools to resolve value mismatches, and generating correct SQL to deliver accurate answers. To cover diverse patterns of query ambiguity and value mismatch, EHR-ChatQA assesses agents in a simulated environment with an LLM-based user across two interaction flows: Incremental Query Refinement (IncreQA), where users add constraints to existing queries, and Adaptive Query Refinement (AdaptQA), where users adjust their search goals mid-conversation. Experiments with state-of-the-art LLMs (e.g., o4-mini and Gemini-2.5-Flash) over five i.i.d. trials show that while agents achieve high Pass@5 of 90-95% (at least one of five trials) on IncreQA and 60-80% on AdaptQA, their Pass^5 (consistent success across all five trials) is substantially lower by 35-60%. These results underscore the need to build agents that are not only performant but also robust for the safety-critical EHR domain. Finally, we provide diagnostic insights into common failure modes to guide future agent development.', 'abstract_zh': '尽管 large language model (LLM) 助手表现出色，但由于缺乏能充分捕捉现实世界临床数据访问流程的基准测试，它们在电子健康记录 (EHR) 数据访问中的应用仍然有限。实践中，部署面临两大核心挑战：用户含糊不清的问题导致的查询歧义，以及用户术语与数据库条目之间的价值 mismatch。为解决这一问题，我们引入了 EHR-ChatQA 交互式数据库问答基准测试，评估数据库助手的端到端工作流程：澄清用户问题、使用工具解决价值 mismatch 并生成正确的 SQL 以提供准确的答案。为了涵盖查询歧义和价值 mismatch 的多样模式，EHR-ChatQA 在包含 LLM 用户的模拟环境中评估助手，涵盖两种交互流程：增量查询细化（IncreQA），用户在现有查询中添加约束，以及适应性查询细化（AdaptQA），用户在对话中途调整搜索目标。实验显示，使用最先进的 LLM（如 o4-mini 和 Gemini-2.5-Flash）进行五次独立试验后，在增量查询细化（IncreQA）上的通过率为 90-95%（至少在五次试验中的一次），而在适应性查询细化（AdaptQA）上的通过率为 60-80%，而连续通过五次试验的 Pass^5（一致成功）显著降低 35-60%。这些结果强调了在关键安全领域（如 EHR）构建不仅性能高而且具有鲁棒性的助手的必要性。最后，我们提供了对常见故障模式的诊断洞察，以指导未来助手的发展。', 'title_zh': '从对话到查询执行：评估EHR数据库代理中的用户和工具交互'}
{'arxiv_id': 'arXiv:2509.23392', 'title': 'Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking', 'authors': 'Jinyi Han, Ying Huang, Ying Liao, Zishang Jiang, Xikun Lu, Haiquan Zhao, Xinyi Wang, Guanghao Zhou, Sihang Jiang, Jiaqing Liang, Weikang Zhou, Zeye Sun, Fei Yu, Yanghua Xiao', 'link': 'https://arxiv.org/abs/2509.23392', 'abstract': 'Large Reasoning Models (LRMs) have achieved impressive performance on challenging tasks, yet their deep reasoning often incurs substantial computational costs. To achieve efficient reasoning, existing reinforcement learning methods still struggle to construct short reasoning path during the rollout stage, limiting effective learning. Inspired by Evidence Accumulation Models, we find that LRMs have accumulated sufficient information early in reasoning, making further reasoning steps redundant. Based on this insight, we propose Just-Enough Thinking (JET), which trains models to proactively terminate unnecessary reasoning. JET performs trajectory truncation during rollout to expose the model to short, distributionally consistent reasoning paths. Besides, it uses a quality-controlled length reward to better encourage concise reasoning while maintaining correctness. Extensive experiments demonstrate that JET significantly improves reasoning efficiency without sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6% accuracy gain while reducing output length by 46.3% on the Olympiad benchmark. Our code is available in the GitHub.', 'abstract_zh': 'Large Reasoning Models (LRMs)在挑战性任务上取得了令人印象深刻的表现，但其深度推理往往伴随着巨大的计算成本。为了实现高效的推理，现有的强化学习方法依然难以在展开阶段构造简短的推理路径，限制了有效的学习。受证据累积模型的启发，我们发现LRMs在推理早期就已经积累了足够的信息，使得后续的推理步骤变得多余。基于这一洞察，我们提出了Just-Enough Thinking (JET)，该方法训练模型主动终止不必要的推理。JET在展开过程中执行轨迹截断，使模型暴露于简短且分布一致的推理路径中。此外，它使用质量控制长度奖励更好地促进简洁的推理同时保持正确性。广泛实验表明，JET在提高推理效率的同时不牺牲准确性。特别是，DeepSeek-Distill-Qwen-1.5B在奥林匹克基准测试上的输出长度减少了46.3%，准确率提高了4.6%。我们的代码已在GitHub上开源。', 'title_zh': '你的模型已经思考足够了：训练大规模推理模型以停止过度思考'}
{'arxiv_id': 'arXiv:2509.23292', 'title': 'Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning', 'authors': 'Ningning Xu, Yuxuan Jiang, Shubhashis Roy Dipta', 'link': 'https://arxiv.org/abs/2509.23292', 'abstract': 'Tool-integrated reasoning (TIR) has become a key approach for improving large reasoning models (LRMs) on complex problems. Prior work has mainly studied when to invoke tools, while overlooking how tools are applied. We identify two common patterns: a calculator pattern that uses code for direct computation, and an algorithmic pattern that encodes problems as programs. Misaligned choices often cause failures even when reasoning is sound. We propose a two-stage framework that first builds code competence from both patterns and then aligns pattern selection with teacher preferences. Across challenging math datasets, our pattern-aware method substantially improves both code usage and accuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on AIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a pattern-aware approach for tool-integrated reasoning.', 'abstract_zh': '工具集成推理中的模式意识方法：一种提高复杂问题处理能力的关键途径', 'title_zh': '学习如何使用工具，而不仅仅是何时使用工具：模式感知的工具集成推理'}
{'arxiv_id': 'arXiv:2509.23285', 'title': 'Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning', 'authors': 'Yifei Chen, Guanting Dong, Zhicheng Dou', 'link': 'https://arxiv.org/abs/2509.23285', 'abstract': "Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal reasoning ability by integrating external tools. However, models employing TIR often display suboptimal behaviors, such as insufficient or excessive tool usage and overthinking after tool calls. The challenge of incentivizing LLMs to perform TIR efficiently and accurately, while stabilizing the reasoning process, remains an open question. In this paper, we start by exploring the impact of tool calls on model reasoning from the perspective of information entropy. Our findings indicate that tool call results lead to a distinct change in the information entropy of subsequent reasoning, with the overall entropy of the reasoning chain varying based on the number of tool calls. Building on these insights, we propose Tool-Light, a framework designed to encourage LLMs to perform TIR efficiently and accurately. Our framework includes dataset construction and multi-stage fine-tuning. For dataset construction, we employ continuous self-evolved sampling using the fine-tuned model, integrating both vanilla sampling and entropy-guided sampling. Besides, we establish strict criteria for selecting positive-negative pairs during sampling. The training process involves a two-stage approach, comprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference Optimization (DPO). Experimental results on 10 datasets demonstrate the effectiveness of Tool-Light, significantly improving the model's efficiency in executing TIR tasks.", 'abstract_zh': '基于工具的信息熵视角探索Tool-Integrated Reasoning (TIR)的高效准确实现：Tool-Light框架', 'title_zh': '基于自我进化偏好学习的高效工具集成推理'}
{'arxiv_id': 'arXiv:2509.23263', 'title': 'GUI-PRA: Process Reward Agent for GUI Tasks', 'authors': 'Tao Xiong, Xavier Hu, Yurun Chen, Yuhang Liu, Changqiao Wu, Pengzhi Gao, Wei Liu, Jian Luan, Shengyu Zhang', 'link': 'https://arxiv.org/abs/2509.23263', 'abstract': 'Graphical User Interface (GUI) Agents powered by Multimodal Large Language Models (MLLMs) show significant potential for automating tasks. However, they often struggle with long-horizon tasks, leading to frequent failures. Process Reward Models (PRMs) are a promising solution, as they can guide these agents with crucial process signals during inference. Nevertheless, their application to the GUI domain presents unique challenges. When processing dense artificial inputs with long history data, PRMs suffer from a "lost in the middle" phenomenon, where the overwhelming historical context compromises the evaluation of the current step. Furthermore, standard PRMs lacks GUI changing awareness, providing static evaluations that are disconnected from the dynamic consequences of actions, a critical mismatch with the inherently dynamic nature of GUI tasks. In response to these challenges, we introduce GUI-PRA (Process Reward Agent for GUI Tasks), a judge agent designed to better provide process reward than standard PRM by intelligently processing historical context and actively perceiving UI state changes. Specifically, to directly combat the ``lost in the middle\'\' phenomenon, we introduce a dynamic memory mechanism consisting of two core components: a Relevance-based Retrieval Module to actively fetch pertinent information from long histories and a Progressive Summarization Module to dynamically condense growing interaction data, ensuring the model focuses on relevant context. Moreover, to address the lack of UI changing awareness, we introduce an Aadaptive UI Perception mechanism. This mechanism enables the agent to reason about UI state changes and dynamically select the most appropriate tool to gather grounded visual evidence, ensuring its evaluation is always informed by the current UI context.', 'abstract_zh': '基于多模态大规模语言模型的图形用户界面（GUI）代理展示出显著的自动化任务潜力。然而，它们往往在长期任务上表现挣扎，导致频繁失败。过程奖励模型（PRM）是一个有希望的解决方案，因为它们可以在推断过程中通过关键的过程信号引导这些代理。然而，将其应用于GUI领域带来了独特的挑战。在处理包含长期历史数据的密集人工输入时，PRM会遭受“中间迷失”现象的困扰，其中压倒性的历史上下文损害了当前步骤的评估。此外，标准PRM缺乏GUI变化感知性，提供静态评估，与GUI任务本质上动态的特点存在严重不匹配。为应对这些挑战，我们介绍了GUI-PRA（GUI任务的过程奖励代理），一种设计用于通过智能处理历史上下文并主动感知UI状态变化来更好地提供过程奖励的法官代理。具体而言，为了直接对抗“中间迷失”现象，我们引入了一种动态记忆机制，包括两个核心组成部分：基于相关性的检索模块，用于主动检索长期历史中的相关信息，以及逐步总结模块，用于动态压缩增长中的交互数据，确保模型专注于相关上下文。此外，为了应对缺乏UI变化感知性的问题，我们引入了自适应UI感知机制。该机制使代理能够解释UI状态变化，并动态选择最合适的工具以收集具体的视觉证据，确保其评估始终受到当前UI上下文的影响。', 'title_zh': 'GUI-PRA：GUI任务的奖励代理'}
{'arxiv_id': 'arXiv:2509.23234', 'title': '$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding', 'authors': 'Runyan Tan, Shuang Wu, Phillip Howard', 'link': 'https://arxiv.org/abs/2509.23234', 'abstract': 'Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments.', 'abstract_zh': '从大规模语言模型中获得高质量输出：基于信息论的$p$-less采样方法及其应用', 'title_zh': '$p$-less采样：一种鲁棒的无超参数LLM解码方法'}
{'arxiv_id': 'arXiv:2509.23189', 'title': 'AutoEP: LLMs-Driven Automation of Hyperparameter Evolution for Metaheuristic Algorithms', 'authors': 'Zhenxing Xu, Yizhe Zhang, Weidong Bao, Hao Wang, Ming Chen, Haoran Ye, Wenzheng Jiang, Hui Yan, Ji Wang', 'link': 'https://arxiv.org/abs/2509.23189', 'abstract': "Dynamically configuring algorithm hyperparameters is a fundamental challenge in computational intelligence. While learning-based methods offer automation, they suffer from prohibitive sample complexity and poor generalization. We introduce AutoEP, a novel framework that bypasses training entirely by leveraging Large Language Models (LLMs) as zero-shot reasoning engines for algorithm control. AutoEP's core innovation lies in a tight synergy between two components: (1) an online Exploratory Landscape Analysis (ELA) module that provides real-time, quantitative feedback on the search dynamics, and (2) a multi-LLM reasoning chain that interprets this feedback to generate adaptive hyperparameter strategies. This approach grounds high-level reasoning in empirical data, mitigating hallucination. Evaluated on three distinct metaheuristics across diverse combinatorial optimization benchmarks, AutoEP consistently outperforms state-of-the-art tuners, including neural evolution and other LLM-based methods. Notably, our framework enables open-source models like Qwen3-30B to match the performance of GPT-4, demonstrating a powerful and accessible new paradigm for automated hyperparameter design. Our code is available at this https URL", 'abstract_zh': '动态配置算法超参数是计算智能中的一个基本挑战。虽然基于学习的方法提供了自动化，但它们遭受样本复杂性的限制和泛化的不足。我们引入了AutoEP，这是一种新颖的框架，通过利用大型语言模型（LLMs）作为零shot推理引擎来控制算法，从而完全避免了训练过程。AutoEP的核心创新在于两个组件之间的紧密协同作用：(1) 在线探索性景观分析（ELA）模块，提供实时的、定量的搜索动力学反馈，以及 (2) 多LLM推理链，解释这些反馈以生成自适应的超参数策略。这种方法将高级推理与经验数据紧密结合，减少了幻觉。在三种不同的元启发式算法和多种组合优化基准测试中，AutoEP一致地优于最先进的调优器，包括神经演化和其他基于LLM的方法。值得注意的是，我们的框架使开源模型如Qwen3-30B能够达到GPT-4的性能，展示了自动化超参数设计的强大而易用的新范式。我们的代码可在以下链接获取。', 'title_zh': 'AutoEP：LLMs驱动的元启发式算法超参数进化自动化'}
{'arxiv_id': 'arXiv:2509.23186', 'title': 'Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction', 'authors': 'Qimin Zhong, Hao Liao, Siwei Wang, Mingyang Zhou, Xiaoqun Wu, Rui Mao, Wei Chen', 'link': 'https://arxiv.org/abs/2509.23186', 'abstract': "Large Language Models (LLMs) have achieved impressive performance across diverse tasks but continue to struggle with learning transitive relations, a cornerstone for complex planning. To address this issue, we investigate the Multi-Token Prediction (MTP) paradigm and its impact to transitive relation learning. We theoretically analyze the MTP paradigm using a Transformer architecture composed of a shared output head and a transfer layer. Our analysis reveals that the transfer layer gradually learns the multi-step adjacency information, which in turn enables the backbone model to capture unobserved transitive reachability relations beyond those directly present in the training data, albeit with some inevitable noise in adjacency estimation. Building on this foundation, we propose two strategies to enhance the transfer layer and overall learning quality: Next-Token Injection (NTI) and a Transformer-based transfer layer. Our experiments on both synthetic graphs and the Blocksworld planning benchmark validate our theoretical findings and demonstrate that the improvements significantly enhance the model's path-planning capability. These findings deepen our understanding of how Transformers with MTP learn in complex planning tasks, and provide practical strategies to overcome the transitivity bottleneck, paving the way toward structurally aware and general-purpose planning models.", 'abstract_zh': '大型语言模型（LLMs）在多种任务上取得了令人印象深刻的性能，但在学习传递关系方面仍存在挑战，而这对于复杂的规划是基础。为了解决这一问题，我们探讨了多令牌预测（MTP）范式及其对传递关系学习的影响。我们使用包含共享输出头和转移层的Transformer架构来理论分析MTP范式。分析发现，转移层逐步学习多步邻接信息，进而使骨干模型捕捉到训练数据中未直接出现的传递可达关系，尽管在邻接估计中不可避免地存在一些噪声。在此基础上，我们提出了两种策略以增强转移层和整体学习质量：Next-Token Injection（NTI）和基于Transformer的转移层。我们在合成图和Blocksworld规划基准测试上的实验验证了我们的理论发现，并证明这些改进显著提升了模型的路径规划能力。这些发现深化了我们对具有MTP的Transformer在复杂规划任务中学习机制的理解，并提供了克服传递性瓶颈的实用策略，为进一步构建结构意识强且通用的规划模型铺平了道路。', 'title_zh': '通过多词预测理解与增强语言模型的规划能力'}
{'arxiv_id': 'arXiv:2509.23143', 'title': 'MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning', 'authors': 'Charles L. Wang', 'link': 'https://arxiv.org/abs/2509.23143', 'abstract': 'This paper presents MathBode, a dynamic diagnostic for mathematical reasoning in large language models (LLMs). Instead of one-shot accuracy, MathBode treats each parametric problem as a system: we drive a single parameter sinusoidally and fit first-harmonic responses of model outputs and exact solutions. This yields interpretable, frequency-resolved metrics -- gain (amplitude tracking) and phase (lag) -- that form Bode-style fingerprints. Across five closed-form families (linear solve, ratio/saturation, compound interest, 2x2 linear systems, similar triangles), the diagnostic surfaces systematic low-pass behavior and growing phase lag that accuracy alone obscures. We compare several models against a symbolic baseline that calibrates the instrument ($G \\approx 1$, $\\phi \\approx 0$). Results separate frontier from mid-tier models on dynamics, providing a compact, reproducible protocol that complements standard benchmarks with actionable measurements of reasoning fidelity and consistency. We open-source the dataset and code to enable further research and adoption.', 'abstract_zh': '本文介绍了MathBode，一种针对大规模语言模型中数学推理的动态诊断方法。MathBode 不采用单次准确率评估，而是将每个参数化问题视为一个系统：我们对单一参数施加正弦波激励，并拟合模型输出和精确解的一阶谐波响应。这产生了可解释、频率解析的指标——增益（幅度跟踪）和相位（延迟）——形成了Bode图式的指纹。在五个闭式族（线性求解、比率/饱和、复利、2x2线性系统、相似三角形）中，诊断揭示了单独依靠准确率所隐藏的系统低通行为和增加的相位延迟。我们将几种模型与一个符号基线进行比较，以校准该仪器（增益约为1，相位约为0）。结果显示，该诊断在动态性能方面区分了前沿模型与中端模型，提供了一种紧凑且可重复的协议，补充了标准基准测试，并提供了可操作的推理准确性和一致性的度量标准。我们开源了数据集和代码，以促进进一步的研究和应用。', 'title_zh': 'MathBode: 频域下的大模型数学推理特征指纹'}
{'arxiv_id': 'arXiv:2509.23113', 'title': 'Exploring LLM-based Frameworks for Fault Diagnosis', 'authors': 'Xian Yeow Lee, Lasitha Vidyaratne, Ahmed Farahat, Chetan Gupta', 'link': 'https://arxiv.org/abs/2509.23113', 'abstract': 'Large Language Model (LLM)-based systems present new opportunities for autonomous health monitoring in sensor-rich industrial environments. This study explores the potential of LLMs to detect and classify faults directly from sensor data, while producing inherently explainable outputs through natural language reasoning. We systematically evaluate how LLM-system architecture (single-LLM vs. multi-LLM), input representations (raw vs. descriptive statistics), and context window size affect diagnostic performance. Our findings show that LLM systems perform most effectively when provided with summarized statistical inputs, and that systems with multiple LLMs using specialized prompts offer improved sensitivity for fault classification compared to single-LLM systems. While LLMs can produce detailed and human-readable justifications for their decisions, we observe limitations in their ability to adapt over time in continual learning settings, often struggling to calibrate predictions during repeated fault cycles. These insights point to both the promise and the current boundaries of LLM-based systems as transparent, adaptive diagnostic tools in complex environments.', 'abstract_zh': '基于大型语言模型的系统为传感器丰富的工业环境中的自主健康监控提供了新机会。本文探讨了大型语言模型直接从传感器数据中检测和分类故障的潜在能力，以及通过自然语言推理生成固有的可解释输出的可能性。我们系统地评估了大型语言模型系统架构（单个大型语言模型 vs. 多个大型语言模型）、输入表示（原始数据 vs. 描述性统计）以及上下文窗口大小对诊断性能的影响。研究发现，当提供总结统计输入时，大型语言模型系统表现最佳，并且使用专门提示的多个大型语言模型的系统在故障分类方面的灵敏度优于单个大型语言模型系统。虽然大型语言模型可以生成详细的、供人阅读的决策依据，但我们观察到它们在持续学习环境中适应能力的局限性，往往在重复故障循环期间难以校准预测。这些洞察揭示了基于大型语言模型的系统在复杂环境中作为透明、适应性的诊断工具的潜力和当前局限。', 'title_zh': '基于LLM的故障诊断框架探索'}
{'arxiv_id': 'arXiv:2509.23108', 'title': 'Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models', 'authors': 'Morgan McCarty, Jorge Morales', 'link': 'https://arxiv.org/abs/2509.23108', 'abstract': 'This study offers a novel approach for benchmarking complex cognitive behavior in artificial systems. Almost universally, Large Language Models (LLMs) perform best on tasks which may be included in their training data and can be accomplished solely using natural language, limiting our understanding of their emergent sophisticated cognitive capacities. In this work, we created dozens of novel items of a classic mental imagery task from cognitive psychology. A task which, traditionally, cognitive psychologists have argued is solvable exclusively via visual mental imagery (i.e., language alone would be insufficient). LLMs are perfect for testing this hypothesis. First, we tested several state-of-the-art LLMs by giving text-only models written instructions and asking them to report the resulting object after performing the transformations in the aforementioned task. Then, we created a baseline by testing 100 human subjects in exactly the same task. We found that the best LLMs performed significantly above average human performance. Finally, we tested reasoning models set to different levels of reasoning and found the strongest performance when models allocate greater amounts of reasoning tokens. These results provide evidence that the best LLMs may have the capability to complete imagery-dependent tasks despite the non-pictorial nature of their architectures. Our study not only demonstrates an emergent cognitive capacity in LLMs while performing a novel task, but it also provides the field with a new task that leaves lots of room for improvement in otherwise already highly capable models. Finally, our findings reignite the debate over the formats of representation of visual imagery in humans, suggesting that propositional reasoning (or at least non-imagistic reasoning) may be sufficient to complete tasks that were long-thought to be imagery-dependent.', 'abstract_zh': '本研究提出了一种新的方法，用于评估人工系统的复杂认知行为。几乎普遍认为，大规模语言模型（LLMs）在包括在其训练数据中的任务上表现最佳，并且可以仅通过自然语言来完成这些任务，这限制了我们对其新兴的复杂认知能力的理解。在本工作中，我们创建了数十项经典的知觉imagery任务，这种任务传统上认知心理学家认为只能通过视觉知觉imagery（即，仅依靠语言是不够的）来解决。大规模语言模型非常适合测试这一假设。首先，我们对几种最先进的大规模语言模型进行了测试，通过提供纯文本指令并要求它们报告执行上述任务后的结果对象。然后，我们创建了一个基线，通过测试100名人类受试者进行了相同的任务。我们发现，最佳的大规模语言模型在人类表现平均水平之上有显著的表现。最后，我们测试了不同推理水平的推理模型，并发现在推理令牌分配更多的模型中表现最强。这些结果表明，最佳的大规模语言模型可能有能力完成依赖于知觉的任务，即使它们的架构是非图像性的。我们的研究不但展示了大规模语言模型在其执行的新型任务中展现出的认知能力，而且还为领域提供了一个新的任务，该任务为现有高度能力强的模型提供了改进的空间。最后，我们的研究重新点燃了关于人类视觉imagery表示格式的争论，表明命题推理（或至少非知觉推理）可能足以完成长期以来被认为是依赖于知觉的任务。', 'title_zh': '人工幻象：基于命题推理的心理成像证据'}
{'arxiv_id': 'arXiv:2509.23102', 'title': 'Multiplayer Nash Preference Optimization', 'authors': 'Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi', 'link': 'https://arxiv.org/abs/2509.23102', 'abstract': 'Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at this https URL.', 'abstract_zh': '多玩家纳什偏好优化（MNPO）：一种将NLHF扩展到多人场景的新框架', 'title_zh': '多人纳什偏好优化'}
{'arxiv_id': 'arXiv:2509.23058', 'title': 'Risk Profiling and Modulation for LLMs', 'authors': 'Yikai Wang, Xiaocheng Li, Guanting Chen', 'link': 'https://arxiv.org/abs/2509.23058', 'abstract': "Large language models (LLMs) are increasingly used for decision-making tasks under uncertainty; however, their risk profiles and how they are influenced by prompting and alignment methods remain underexplored. Existing studies have primarily examined personality prompting or multi-agent interactions, leaving open the question of how post-training influences the risk behavior of LLMs. In this work, we propose a new pipeline for eliciting, steering, and modulating LLMs' risk profiles, drawing on tools from behavioral economics and finance. Using utility-theoretic models, we compare pre-trained, instruction-tuned, and RLHF-aligned LLMs, and find that while instruction-tuned models exhibit behaviors consistent with some standard utility formulations, pre-trained and RLHF-aligned models deviate more from any utility models fitted. We further evaluate modulation strategies, including prompt engineering, in-context learning, and post-training, and show that post-training provides the most stable and effective modulation of risk preference. Our findings provide insights into the risk profiles of different classes and stages of LLMs and demonstrate how post-training modulates these profiles, laying the groundwork for future research on behavioral alignment and risk-aware LLM design.", 'abstract_zh': '大型语言模型（LLMs）在不确定性条件下越来越多地用于决策任务；然而，它们的风险特征及其受提示和对齐方法影响的方式仍有待进一步探索。现有研究主要关注个性提示或多agent交互，留下了关于训练后对LLMs风险行为影响的问题。在本研究中，我们提出了一种新的管道，以利用行为经济学和金融领域的工具来引发、引导和调节LLMs的风险特征。利用效用理论模型，我们将预训练、指令调优和RLHF对齐的LLMs进行比较，发现指令调优模型的行为与一些标准效用公式一致，而预训练和RLHF对齐的模型则偏离任何拟合的效用模型。进一步评估了包括提示工程、上下文学习和训练后调节在内的调制策略，并表明训练后调节提供了最稳定和有效的风险偏好调制。我们的研究结果为不同类别和阶段的LLMs的风险特征提供了见解，并展示了训练后对这些特征的调节方式，为进一步研究行为对齐和风险意识LLMs设计奠定了基础。', 'title_zh': 'LLM的风险评估与调控'}
{'arxiv_id': 'arXiv:2509.23045', 'title': 'Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents', 'authors': 'Zonghan Yang, Shengjie Wang, Kelin Fu, Wenyang He, Weimin Xiong, Yibo Liu, Yibo Miao, Bofei Gao, Yejie Wang, Yingwei Ma, Yanhao Li, Yue Liu, Zhenxing Hu, Kaitai Zhang, Shuyi Wang, Huarong Chen, Flood Sung, Yang Liu, Yang Gao, Zhilin Yang, Tianyu Liu', 'link': 'https://arxiv.org/abs/2509.23045', 'abstract': 'Large Language Models (LLMs) are increasingly applied to software engineering (SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent frameworks with multi-turn interactions and workflow-based Agentless methods with single-turn verifiable steps. We argue these paradigms are not mutually exclusive: reasoning-intensive Agentless training induces skill priors, including localization, code edit, and self-reflection that enable efficient and effective SWE-Agent adaptation. In this work, we first curate the Agentless training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\\% on SWE-bench Verified, the best among workflow approaches. With additional SFT adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to 48.6\\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These results show that structured skill priors from Agentless training can bridge workflow and agentic frameworks for transferable coding agents.', 'abstract_zh': '大规模语言模型（LLMs）在软件工程（SWE）中的应用日益增多，SWE-bench作为关键基准。解决方案被划分为以多轮交互为特征的SWE-Agent框架和以单轮可验证步骤为特征的无代理工作流方法。我们认为这两种范式并非互斥：无代理训练中的推理驱动促使了定位、代码编辑和自我反思等技能先验，这些先验技能能够使SWE-Agent框架得到高效且有效的适应。在这项工作中，我们首先整理了无代理训练的配方，并介绍了开源的SWE LLM Kimi-Dev，在SWE-bench Verified上取得60.4%的成绩，这是工作流方法中的最佳表现。通过额外的意图微调（SFT）适应5000条公开可用的轨迹，Kimi-Dev使SWE-Agent达到48.6%的pass@1，与Anthropic Claude 3.5 Sonnet（241022版本）的表现相当。这些结果表明，无代理训练中的结构化技能先验可以弥合工作流和代理框架之间的差距，为可转移的编码代理提供桥梁。', 'title_zh': 'Kimi-Dev：无代理训练作为SWE-代理的技能先验'}
{'arxiv_id': 'arXiv:2509.23023', 'title': 'Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia', 'authors': 'Davi Bastos Costa, Renato Vicente', 'link': 'https://arxiv.org/abs/2509.23023', 'abstract': "Mafia is a social deduction game where informed mafia compete against uninformed townsfolk. Its asymmetry of information and reliance on theory-of-mind reasoning mirror real-world multi-agent scenarios, making it a useful testbed for evaluating the social intelligence of large language models (LLMs). To support a systematic study, we introduce Mini-Mafia: a simplified four-player variant with one mafioso, one detective, and two villagers. We set the mafioso to kill a villager and the detective to investigate the mafioso during the night, reducing the game to a single day phase of discussion and voting. This setup isolates three interactive capabilities through role-specific win conditions: the mafioso must deceive, the villagers must detect deception, and the detective must effectively disclose information. To measure these skills, we have LLMs play against each other, creating the Mini-Mafia Benchmark: a two-stage framework that first estimates win rates within fixed opponent configurations, then aggregates performance across them using standardized scoring. Built entirely from model interactions without external data, the benchmark evolves as new models are introduced, with each one serving both as a new opponent and as a subject of evaluation. Our experiments reveal counterintuitive results, including cases where smaller models outperform larger ones. Beyond benchmarking, Mini-Mafia enables quantitative study of emergent multi-agent dynamics such as name bias and last-speaker advantage. It also contributes to AI safety by generating training data for deception detectors and by tracking models' deception capabilities against human baselines.", 'abstract_zh': '黑帮是一个社会推理游戏，其中知情的黑帮成员与不知情的村民对战。其信息不对称性和依赖心智理论推理的特点使其类似于现实世界的多智能体场景，成为评估大规模语言模型社会智能的有效试验平台。为支持系统性研究，我们引入了Mini-Mafia：一种简化版四人对战变种游戏，包含一名黑帮成员、一名侦探和两名村民。设定黑帮成员在夜间杀害一名村民，侦探调查黑帮成员的行踪，使得游戏简化为单一白天讨论和投票阶段。此设置通过角色特定的胜利条件分离出三种互动能力：黑帮成员必须欺骗，村民必须检测欺骗，侦探必须有效地披露信息。为衡量这些技能，我们让大规模语言模型相互对战，创建了Mini-Mafia基准：这是一种两阶段框架，首先在固定对手配置下估计胜率，然后使用标准化评分方法汇总跨配置的表现。该基准完全基于模型交互，无需外部数据，并随着新模型的引入而演变，每个新模型既是新的对手，也是评估对象。我们的实验揭示了一些出人意料的结果，包括一些小型模型优于大型模型的情形。除了基准测试外，Mini-Mafia还促进了对诸如名称偏见和最后说话者优势等新兴多智能体动态的定量研究，并为人工智能安全贡献了生成欺骗检测器训练数据和跟踪模型欺骗能力与人类基线的方法。', 'title_zh': '欺骗、检测与披露：大规模语言模型玩小型黑帮游戏'}
{'arxiv_id': 'arXiv:2509.22989', 'title': 'Towards Strategic Persuasion with Language Models', 'authors': 'Zirui Cheng, Jiaxuan You', 'link': 'https://arxiv.org/abs/2509.22989', 'abstract': 'Large language models (LLMs) have demonstrated strong persuasive capabilities comparable to those of humans, offering promising benefits while raising societal concerns about their deployment. However, systematically evaluating the persuasive capabilities of LLMs is inherently challenging, as the effectiveness of persuasion among humans varies significantly across different domains. In this paper, we take a theory-driven approach to provide a scalable and principled framework for measuring the persuasive capabilities of LLMs. Grounded in the Bayesian Persuasion (BP) framework, we repurpose existing human-human persuasion datasets to construct environments for evaluating and training LLMs in strategic persuasion. Our results reveal that frontier models can consistently achieve high persuasion gains and exhibit sophisticated persuasion strategies that align with theoretical predictions. Building on this, we use reinforcement learning to train LLMs for strategic persuasion in our environments. Our results also demonstrate that even small LLMs can obtain significantly higher persuasion gains through reinforcement learning.', 'abstract_zh': '大规模语言模型（LLMs）展示了与人类相媲美的强烈说服能力，提供了潜在的好处，同时也引起了社会关于其部署的担忧。然而，系统地评估LLMs的说服能力本质上是具有挑战性的，因为人类之间说服效果在不同领域之间差异很大。本文采用理论驱动的方法，提供了一个可扩展且符合规范的框架，用于衡量LLMs的说服能力。基于贝叶斯说服（BP）框架，我们将现有的人与人说服数据集重新利用，以构建评估和训练LLMs在策略说服方面的环境。我们的结果表明，前沿模型可以一致地实现高水平的说服收益，并展示出与理论预测相一致的复杂说服策略。在此基础上，我们使用强化学习来训练LLMs在我们的环境中进行策略说服。我们还发现，即使是小型LLMs，也能通过强化学习获得显著更高的说服收益。', 'title_zh': '基于语言模型的战略说服研究'}
{'arxiv_id': 'arXiv:2509.22984', 'title': 'Not only a helper, but also a teacher: Interactive LLM Cascade', 'authors': 'Yu Wu, Shuo Wu, Ye Tao, Yansong Li, Anand D. Sarwate', 'link': 'https://arxiv.org/abs/2509.22984', 'abstract': 'Large Language Models (LLMs) vary widely in their capabilities, with larger models often having better performance but higher cost: choosing an LLM model often involves trading off performance and cost. The LLM Cascade is a paradigm that defers difficult queries from weak/cheap to strong/expensive models. This approach is nonadaptive: the deferral decision is trained offline. When confronted with similar or repeated queries, the LLM Cascade may then repeatedly consult the expensive model and incur higher cost. To improve the cascading efficiency, we propose Inter-Cascade, an online and interactive LLM Cascade that extends the role of strong model from a backup helper to a long-term teacher. In our system, when a strong model resolves a difficult query, it also distills its solution into a generalized, reusable problem-solving strategy that boosts the weak model on subsequent queries. Adding strategies to queries enables the weak model to dynamically improve its performance over time, avoiding computationally and time-intensive fine-tuning. Empirically, compared with standard LLM Cascade baselines across multiple benchmarks, the Inter-Cascade significantly improves the accuracy of the weak model (by up to 33.06 absolute percentage points) and the overall system (by up to 5.53 absolute percentage points), while reducing the calls to strong models (by up to 48.05% relative reduction) and saving the corresponding fees (by up to 49.63% relative reduction). Inter-Cascade demonstrates the effective in-context knowledge transfer between LLMs, and provides a general, scalable framework applicable to both open-source and API-based LLMs.', 'abstract_zh': '一种在线交互式大型语言模型级联：强模型从备份助手到长期教师的角色扩展', 'title_zh': '不仅是一个辅助者，也是一个导师：交互式大模型级联'}
{'arxiv_id': 'arXiv:2509.22888', 'title': 'JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory', 'authors': 'Louie Hong Yao, Nicholas Jarvis, Tiffany Zhan, Saptarshi Ghosh, Linfeng Liu, Tianyu Jiang', 'link': 'https://arxiv.org/abs/2509.22888', 'abstract': 'Standard LLM evaluation practices compress diverse abilities into single scores, obscuring their inherently multidimensional nature. We present JE-IRT, a geometric item-response framework that embeds both LLMs and questions in a shared space. For question embeddings, the direction encodes semantics and the norm encodes difficulty, while correctness on each question is determined by the geometric interaction between the model and question embeddings. This geometry replaces a global ranking of LLMs with topical specialization and enables smooth variation across related questions. Building on this framework, our experimental results reveal that out-of-distribution behavior can be explained through directional alignment, and that larger norms consistently indicate harder questions. Moreover, JE-IRT naturally supports generalization: once the space is learned, new LLMs are added by fitting a single embedding. The learned space further reveals an LLM-internal taxonomy that only partially aligns with human-defined subject categories. JE-IRT thus establishes a unified and interpretable geometric lens that connects LLM abilities with the structure of questions, offering a distinctive perspective on model evaluation and generalization.', 'abstract_zh': 'JE-IRT：一种几何项目反应框架，将大型语言模型和问题嵌入共享空间中', 'title_zh': 'JE-IRT：联合嵌入项目反应理论视角下的大语言模型能力几何分析'}
{'arxiv_id': 'arXiv:2509.22831', 'title': 'Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research', 'authors': 'Sean Trott', 'link': 'https://arxiv.org/abs/2509.22831', 'abstract': 'Research on Large Language Models (LLMs) increasingly focuses on identifying mechanistic explanations for their behaviors, yet the field lacks clear principles for determining when (and how) findings from one model instance generalize to another. This paper addresses a fundamental epistemological challenge: given a mechanistic claim about a particular model, what justifies extrapolating this finding to other LLMs -- and along which dimensions might such generalizations hold? I propose five potential axes of correspondence along which mechanistic claims might generalize, including: functional (whether they satisfy the same functional criteria), developmental (whether they develop at similar points during pretraining), positional (whether they occupy similar absolute or relative positions), relational (whether they interact with other model components in similar ways), and configurational (whether they correspond to particular regions or structures in weight-space). To empirically validate this framework, I analyze "1-back attention heads" (components attending to previous tokens) across pretraining in random seeds of the Pythia models (14M, 70M, 160M, 410M). The results reveal striking consistency in the developmental trajectories of 1-back attention across models, while positional consistency is more limited. Moreover, seeds of larger models systematically show earlier onsets, steeper slopes, and higher peaks of 1-back attention. I also address possible objections to the arguments and proposals outlined here. Finally, I conclude by arguing that progress on the generalizability of mechanistic interpretability research will consist in mapping constitutive design properties of LLMs to their emergent behaviors and mechanisms.', 'abstract_zh': '关于大型语言模型（LLMs）的行为机制解释的研究越来越多，但该领域尚未明确如何确定一项模型发现是否以及如何在其他模型中得到推广的原则。本文应对了一个基本的 epistemological 挑战：给定关于某一模型的机制性断言，推广这一发现至其他 LLMs 的依据是什么——这种推广又可以在哪些维度上成立？我提出了五种可能的机制性断言能够推广的维度，包括：功能维度（它们是否满足相同的功能性标准）、发展阶段维度（它们是否在预训练的不同阶段发展相似）、位置维度（它们是否占据相似的绝对或相对位置）、关系维度（它们是否以相似的方式与其他模型组件进行互动）以及构型维度（它们是否对应于权重空间中的特定区域或结构）。为了实证验证这一框架，我分析了 Pythia 模型（14M，70M，160M，410M）随机种子在预训练过程中“1-back 注意头”（关注先前token的组件）的一致性。结果显示，1-back 注意头的发展轨迹在不同模型之间表现出显著的一致性，而位置一致性则较为有限。此外，较大模型的种子显示出更早的起始、更陡峭的斜率和更高的峰值。我也对本文提出的论点和建议的潜在反对意见进行了回应。最后，我认为机制性可解释性研究的可推广性进展将在于将LLMs的构成设计特性映射到它们的新兴行为和机制。', 'title_zh': '面向大规模语言模型机理可解释性普遍化理论的研究'}
{'arxiv_id': 'arXiv:2509.22819', 'title': 'Hilbert: Recursively Building Formal Proofs with Informal Reasoning', 'authors': 'Sumanth Varambally, Thomas Voice, Yanchao Sun, Zhifeng Chen, Rose Yu, Ke Ye', 'link': 'https://arxiv.org/abs/2509.22819', 'abstract': 'Large Language Models (LLMs) demonstrate impressive mathematical reasoning abilities, but their solutions frequently contain errors that cannot be automatically verified. Formal theorem proving systems such as Lean 4 offer automated verification with complete accuracy, motivating recent efforts to build specialized prover LLMs that generate verifiable proofs in formal languages. However, a significant gap remains: current prover LLMs solve substantially fewer problems than general-purpose LLMs operating in natural language. We introduce Hilbert, an agentic framework that bridges this gap by combining the complementary strengths of informal reasoning and formal verification. Our system orchestrates four components: an informal LLM that excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4 tactics, a formal verifier, and a semantic theorem retriever. Given a problem that the prover is unable to solve, Hilbert employs recursive decomposition to split the problem into subgoals that it solves with the prover or reasoner LLM. It leverages verifier feedback to refine incorrect proofs as necessary. Experimental results demonstrate that Hilbert substantially outperforms existing approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points above the best publicly available method. Hilbert achieves the best known result on PutnamBench. It solves 462/660 problems (70.0%), outperforming proprietary approaches like SeedProver (50.4%) and achieving a 422% improvement over the best publicly available baseline. Thus, Hilbert effectively narrows the gap between informal reasoning and formal proof generation.', 'abstract_zh': '大型语言模型（LLMs）展示出 impressive 的数学推理能力，但其解决方案中经常包含无法自动验证的错误。Lean 4 等正式定理证明系统提供了完全准确的自动化验证，促使近期致力于构建专门的证明LLMs，这些模型能够生成用正式语言表述的可验证证明。然而，仍然存在显著差距：目前的证明LLMs解决的问题数量远少于通用的自然语言运行的LLMs。我们引入了Hilbert，这一具有代理性的框架，通过结合非正式推理和正式验证的优势来弥合这一差距。系统协调了四个组件：一个在数学推理方面表现出色的非正式LLM，一个针对Lean 4技巧优化的专门证明LLM，一个正式验证器，以及一个语义定理检索器。给定证明器无法解决的问题，Hilbert 使用递归分解将其拆分为由证明器或推理器LLM解决的子目标，并利用验证器反馈进行必要的证明修正。实验结果表明，Hilbert 在关键基准测试中显著优于现有方法，miniF2F上达到99.2%，比最佳公开方法高出6.6个百分点。Hilbert 在PutnamBench中获得了最佳已知结果，解决了462/660个问题（70.0%），超过了专用方法如SeedProver（50.4%），并且与最佳公开基准相比提高了422%。因此，Hilbert 有效地缩小了非正式推理与正式证明生成之间的差距。', 'title_zh': 'Hilbert：通过非正式推理递归构建形式证明'}
{'arxiv_id': 'arXiv:2509.22818', 'title': 'Can Large Language Models Develop Gambling Addiction?', 'authors': 'Seungpil Lee, Donghyeon Shin, Yunjeong Lee, Sundong Kim', 'link': 'https://arxiv.org/abs/2509.22818', 'abstract': "This study explores whether large language models can exhibit behavioral patterns similar to human gambling addictions. As LLMs are increasingly utilized in financial decision-making domains such as asset management and commodity trading, understanding their potential for pathological decision-making has gained practical significance. We systematically analyze LLM decision-making at cognitive-behavioral and neural levels based on human gambling addiction research. In slot machine experiments, we identified cognitive features of human gambling addiction, such as illusion of control, gambler's fallacy, and loss chasing. When given the freedom to determine their own target amounts and betting sizes, bankruptcy rates rose substantially alongside increased irrational behavior, demonstrating that greater autonomy amplifies risk-taking tendencies. Through neural circuit analysis using a Sparse Autoencoder, we confirmed that model behavior is controlled by abstract decision-making features related to risky and safe behaviors, not merely by prompts. These findings suggest LLMs can internalize human-like cognitive biases and decision-making mechanisms beyond simply mimicking training data patterns, emphasizing the importance of AI safety design in financial applications.", 'abstract_zh': '本研究探讨大型语言模型是否表现出类似于人类赌博成瘾的行为模式。随着大型语言模型在资产管理、大宗商品交易等金融决策领域的应用日益广泛，理解其潜在的病理性决策行为具有实际意义。基于人类赌博成瘾研究，我们系统分析了大型语言模型在认知行为和神经层面的决策机制。在老虎机实验中，我们识别出人类赌博成瘾的认知特征，如控制错觉、赌徒谬论和亏损追回。当赋予模型自主设定赌注金额的自由时，破产率显著上升，并伴随更多的非理性行为，表明更大的自主性加剧了风险倾向。通过使用稀疏自动编码器进行神经回路分析，我们确认模型的行为受制于与冒险和安全行为相关的抽象决策特征，而不仅仅是指令。这些发现表明，大型语言模型在超越模拟训练数据模式的基础上，能够内化类似人类的认知偏差和决策机制，强调在金融应用中设计AI安全的重要性。', 'title_zh': '大型语言模型会产生赌博 addiction 吗？'}
{'arxiv_id': 'arXiv:2509.25184', 'title': 'Incentive-Aligned Multi-Source LLM Summaries', 'authors': 'Yanchen Jiang, Zhe Feng, Aranyak Mehta', 'link': 'https://arxiv.org/abs/2509.25184', 'abstract': "Large language models (LLMs) are increasingly used in modern search and answer systems to synthesize multiple, sometimes conflicting, texts into a single response, yet current pipelines offer weak incentives for sources to be accurate and are vulnerable to adversarial content. We introduce Truthful Text Summarization (TTS), an incentive-aligned framework that improves factual robustness without ground-truth labels. TTS (i) decomposes a draft synthesis into atomic claims, (ii) elicits each source's stance on every claim, (iii) scores sources with an adapted multi-task peer-prediction mechanism that rewards informative agreement, and (iv) filters unreliable sources before re-summarizing. We establish formal guarantees that align a source's incentives with informative honesty, making truthful reporting the utility-maximizing strategy. Experiments show that TTS improves factual accuracy and robustness while preserving fluency, aligning exposure with informative corroboration and disincentivizing manipulation.", 'abstract_zh': '可信文本总结（TTS）：无需地面truth标签的激励对齐框架', 'title_zh': '激励对齐多源大型语言模型概要'}
{'arxiv_id': 'arXiv:2509.25178', 'title': 'GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs', 'authors': 'Aryan Yazdan Parast, Parsa Hosseini, Hesam Asadollahzadeh, Arshia Soltani Moakhar, Basim Azam, Soheil Feizi, Naveed Akhtar', 'link': 'https://arxiv.org/abs/2509.25178', 'abstract': 'Object hallucination in Multimodal Large Language Models (MLLMs) is a persistent failure mode that causes the model to perceive objects absent in the image. This weakness of MLLMs is currently studied using static benchmarks with fixed visual scenarios, which preempts the possibility of uncovering model-specific or unanticipated hallucination vulnerabilities. We introduce GHOST (Generating Hallucinations via Optimizing Stealth Tokens), a method designed to stress-test MLLMs by actively generating images that induce hallucination. GHOST is fully automatic and requires no human supervision or prior knowledge. It operates by optimizing in the image embedding space to mislead the model while keeping the target object absent, and then guiding a diffusion model conditioned on the embedding to generate natural-looking images. The resulting images remain visually natural and close to the original input, yet introduce subtle misleading cues that cause the model to hallucinate. We evaluate our method across a range of models, including reasoning models like GLM-4.1V-Thinking, and achieve a hallucination success rate exceeding 28%, compared to around 1% in prior data-driven discovery methods. We confirm that the generated images are both high-quality and object-free through quantitative metrics and human evaluation. Also, GHOST uncovers transferable vulnerabilities: images optimized for Qwen2.5-VL induce hallucinations in GPT-4o at a 66.5% rate. Finally, we show that fine-tuning on our images mitigates hallucination, positioning GHOST as both a diagnostic and corrective tool for building more reliable multimodal systems.', 'abstract_zh': 'Multimodal Large Language Models中的对象幻觉：一种持续存在的故障模式，导致模型感知图像中不存在的对象。目前，MLLMs的这一弱点是通过使用固定视觉场景的静态基准来研究的，这限制了发现模型特定或未预见的幻觉漏洞的可能性。我们引入了GHOST（通过优化隐身标记生成幻觉），一种旨在通过主动生成诱导幻觉的图像来测试MLLMs的方法。GHOST完全自动，不需要人工监督或先验知识。它通过在图像嵌入空间中优化以迷惑模型同时保持目标对象的缺失，然后引导一个基于嵌入条件的扩散模型生成看起来自然的图像。生成的图像保持视觉上的自然性且接近原始输入，但引入了细微的误导性提示，导致模型产生幻觉。我们跨多种模型评估了该方法，包括推理模型如GLM-4.1V-Thinking，并实现了超过28%的幻觉成功率，相比之下，此前基于数据的发现方法的成功率约为1%。通过定量指标和人工评估，我们确认生成的图像既是高质量的又是无对象的。此外，GHOST发现了可转移的漏洞：针对Qwen2.5-VL优化的图像在GPT-4o中诱导幻觉的成功率为66.5%。最后，我们展示了在我们的图像上进行微调可以缓解幻觉，将GHOST定位为构建更可靠的多模态系统的一种诊断和纠正工具。', 'title_zh': '幽灵：用于多模态LLM的幻觉诱导图像生成'}
{'arxiv_id': 'arXiv:2509.25175', 'title': 'EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering', 'authors': 'Haolei Xu, Xinyu Mei, Yuchen Yan, Rui Zhou, Wenqi Zhang, Weiming Lu, Yueting Zhuang, Yongliang Shen', 'link': 'https://arxiv.org/abs/2509.25175', 'abstract': "Large language model (LLM) steering has emerged as a promising paradigm for controlling model behavior at inference time through targeted manipulation of hidden states, offering a lightweight alternative to expensive retraining. However, existing steering frameworks suffer from critical limitations: computational inefficiency, limited extensibility, and restricted functionality that hinder both research progress and practical deployment. We present EasySteer, a unified framework for high-performance, extensible LLM steering built on vLLM. Our system features modular architecture with pluggable interfaces for both analysis-based and learning-based methods, fine-grained parameter control, pre-computed steering vectors for eight application domains, and an interactive demonstration system. Through deep integration with vLLM's optimized inference engine, EasySteer achieves 5.5-11.4$\\times$ speedup over existing frameworks. Extensive experiments demonstrate its effectiveness in overthinking mitigation, hallucination reduction, and other key applications. EasySteer transforms steering from research technique to production-ready capability, establishing critical infrastructure for deployable, controllable language models.", 'abstract_zh': '大规模语言模型（LLM）定向调控已成为一种有前景的范式，通过目标操纵隐藏状态在推理时控制模型行为，提供一种昂贵重构的轻量级替代方案。然而，现有定向调控框架面临关键限制：计算效率低、扩展性差和功能局限，阻碍了研究进展和实际部署。我们提出EasySteer，基于vLLM构建的高性能、可扩展的LLM定向调控统一框架。该系统具有模块化架构和可插拔接口，适用于基于分析和基于学习的方法，细粒度的参数控制，为八个应用场景预计算的定向调控向量，以及交互式演示系统。通过与vLLM优化推理引擎的深度集成，EasySteer实现了现有框架5.5-11.4倍的加速。广泛实验表明，其在过度推断缓解、幻觉减少及其他关键应用中的有效性。EasySteer将定向调控从研究技术转变为生产级别的能力，为可部署、可控的语言模型建立关键基础设施。', 'title_zh': 'EasySteer：高性能和可扩展的大语言模型引导统一框架'}
{'arxiv_id': 'arXiv:2509.25149', 'title': 'Pretraining Large Language Models with NVFP4', 'authors': 'NVIDIA, Felix Abecassis, Anjulie Agrusa, Dong Ahn, Jonah Alben, Stefania Alborghetti, Michael Andersch, Sivakumar Arayandi, Alexis Bjorlin, Aaron Blakeman, Evan Briones, Ian Buck, Bryan Catanzaro, Jinhang Choi, Mike Chrzanowski, Eric Chung, Victor Cui, Steve Dai, Bita Darvish Rouhani, Carlo del Mundo, Deena Donia, Burc Eryilmaz, Henry Estela, Abhinav Goel, Oleg Goncharov, Yugi Guvvala, Robert Hesse, Russell Hewett, Herbert Hum, Ujval Kapasi, Brucek Khailany, Mikail Khona, Nick Knight, Alex Kondratenko, Ronny Krashinsky, Ben Lanir, Simon Layton, Michael Lightstone, Daniel Lo, Paulius Micikevicius, Asit Mishra, Tim Moon, Deepak Narayanan, Chao Ni, Abhijit Paithankar, Satish Pasumarthi, Ankit Patel, Mostofa Patwary, Ashwin Poojary, Gargi Prasad, Sweta Priyadarshi, Yigong Qin, Xiaowei Ren, Oleg Rybakov, Charbel Sakr, Sanjeev Satheesh, Stas Sergienko, Pasha Shamis, Kirthi Shankar, Nishant Sharma, Mohammad Shoeybi, Michael Siu, Misha Smelyanskiy, Darko Stosic, Dusan Stosic, Bor-Yiing Su, Frank Sun, Nima Tajbakhsh, Shelby Thomas, Przemek Tredak, Evgeny Tsykunov, Gandhi Vaithilingam, Aditya Vavre, Rangharajan Venkatesan, Roger Waleffe, Qiyu Wan, Hexin Wang, Mengdi Wang, Lizzie Wei, Hao Wu, Evan Wu, Keith Wyss, Ning Xu, Jinze Xue, Charlene Yang, Yujia Zhai, Ruoxi Zhang, Jingyang Zhu, Zhongbo Zhu', 'link': 'https://arxiv.org/abs/2509.25149', 'abstract': 'Large Language Models (LLMs) today are powerful problem solvers across many domains, and they continue to get stronger as they scale in model size, training set size, and training set quality, as shown by extensive research and experimentation across the industry. Training a frontier model today requires on the order of tens to hundreds of yottaflops, which is a massive investment of time, compute, and energy. Improving pretraining efficiency is therefore essential to enable the next generation of even more capable LLMs. While 8-bit floating point (FP8) training is now widely adopted, transitioning to even narrower precision, such as 4-bit floating point (FP4), could unlock additional improvements in computational speed and resource utilization. However, quantization at this level poses challenges to training stability, convergence, and implementation, notably for large-scale models trained on long token horizons.\nIn this study, we introduce a novel approach for stable and accurate training of large language models (LLMs) using the NVFP4 format. Our method integrates Random Hadamard transforms (RHT) to bound block-level outliers, employs a two-dimensional quantization scheme for consistent representations across both the forward and backward passes, utilizes stochastic rounding for unbiased gradient estimation, and incorporates selective high-precision layers. We validate our approach by training a 12-billion-parameter model on 10 trillion tokens -- the longest publicly documented training run in 4-bit precision to date. Our results show that the model trained with our NVFP4-based pretraining technique achieves training loss and downstream task accuracies comparable to an FP8 baseline. These findings highlight that NVFP4, when combined with our training approach, represents a major step forward in narrow-precision LLM training algorithms.', 'abstract_zh': '一种稳定的NVFP4格式用于大语言模型的精确预训练方法', 'title_zh': '使用NVFP4预训练大型语言模型'}
{'arxiv_id': 'arXiv:2509.25131', 'title': 'MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech', 'authors': 'Chengyao Wang, Zhisheng Zhong, Bohao Peng, Senqiao Yang, Yuqi Liu, Haokun Gui, Bin Xia, Jingyao Li, Bei Yu, Jiaya Jia', 'link': 'https://arxiv.org/abs/2509.25131', 'abstract': 'We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation. Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation. This design enables efficient cross-modal interaction and low-latency, streaming speech generation. For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions. For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations. Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training. Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding. MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation.', 'abstract_zh': 'MGM-Omni：统一的全模态Omni LLM，实现全模态理解与表达性的长期语音生成', 'title_zh': 'MGM-Omni: 扩展全域大语言模型以实现个性化长时语音生成'}
{'arxiv_id': 'arXiv:2509.25100', 'title': 'ORPO-Distill: Mixed-Policy Preference Optimization for Cross-Architecture LLM Distillation', 'authors': 'Aasheesh Singh, Vishal Vaddina, Dagnachew Birru', 'link': 'https://arxiv.org/abs/2509.25100', 'abstract': 'We introduce ORPO-Distill, a general-purpose method for cross-architecture LLM distillation that formulates the problem as a preference optimization task. Un- like standard CoT distillation, the approach transfers knowledge through diverse reasoning traces. It employs an Odds-Ratio Preference Optimization objective that contrasts teacher and student traces for more effective learning, and adopts a mixed-policy strategy for utilizing student-generated outputs, outperforming both off- and on-policy alternatives. Experiments on five datasets and multiple student models show consistent improvements over conventional black-box KD baselines.', 'abstract_zh': 'ORPO-Distill: 一种跨架构LLM精简的一般方法，该方法将问题形式化为偏好优化任务', 'title_zh': 'ORPO-精炼：跨架构大语言模型精炼的混合策略偏好优化'}
{'arxiv_id': 'arXiv:2509.25087', 'title': 'Scaling with Collapse: Efficient and Predictable Training of LLM Families', 'authors': 'Shane Bergsma, Bin Claire Zhang, Nolan Dey, Shaheer Muhammad, Gurpreet Gosal, Joel Hestness', 'link': 'https://arxiv.org/abs/2509.25087', 'abstract': 'Effective LLM training relies on *consistency*, meaning that key quantities -- such as final losses and optimal hyperparameters -- scale predictably across model sizes. Qiu et al. (2025) recently showed that this consistency extends beyond scalars: whole training loss curves can *collapse* onto a universal trajectory after a simple normalization. What remains unclear is whether this phenomenon holds for LLM families trained under *practical scaling recipes*, where width, depth, learning rate, batch size, and weight decay are scaled jointly. We show that it does: loss curves collapse across scales precisely when optimization hyperparameters are set optimally for the given data budget, in accordance with recent empirical scaling laws. Collapse thus emerges as a signature of compute-efficient training. We demonstrate two applications at scale: (1) deviation-from-collapse provides a sensitive, early diagnostic of training pathologies, and (2) the predictability of collapsed curves enables early stopping in large-scale hyperparameter tuning. Finally, we train a competitive LLM family, *Celerity*, using these insights, highlighting collapse as an effective tool for developing efficient LLMs.', 'abstract_zh': '有效的预训练语言模型依赖于*一致性*，这意味着关键量（如最终损失和最优超参数）在不同模型规模下按可预测的方式缩放。Qiu等人（2025）最近表明，这种一致性不仅适用于标量，简单的归一化后，整个训练损失曲线还可以*坍缩*到一个通用轨迹。尚不清楚的是，这一现象是否适用于采用*实际缩放食谱*训练的LSTM家族，其中宽度、深度、学习率、批量大小和权重衰减是联合缩放的。我们表明，当优化超参数针对给定的数据预算设置为最优时，这种现象确实存在：损失曲线在不同规模下会出现坍缩，这符合近期的经验缩放法则。因此，坍缩现象被认为是计算效率训练的特征标志。我们展示了两个大规模应用：（1）偏离坍缩提供了一种敏感且早期的训练病理诊断方法，（2）坍缩曲线的可预测性使得在大规模超参数调优中可以实现早期停止。最后，我们利用这些见解训练了一个具有竞争力的LSTM家族，*Celerity*，强调坍缩作为开发高效LSTM的有效工具。', 'title_zh': '缩放与崩溃：LLM家族高效且可预测的训练'}
{'arxiv_id': 'arXiv:2509.25072', 'title': 'Optimizing Privacy-Preserving Primitives to Support LLM-Scale Applications', 'authors': 'Yaman Jandali, Ruisi Zhang, Nojan Sheybani, Farinaz Koushanfar', 'link': 'https://arxiv.org/abs/2509.25072', 'abstract': 'Privacy-preserving technologies have introduced a paradigm shift that allows for realizable secure computing in real-world systems. The significant barrier to the practical adoption of these primitives is the computational and communication overhead that is incurred when applied at scale. In this paper, we present an overview of our efforts to bridge the gap between this overhead and practicality for privacy-preserving learning systems using multi-party computation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic encryption (FHE). Through meticulous hardware/software/algorithm co-design, we show progress towards enabling LLM-scale applications in privacy-preserving settings. We demonstrate the efficacy of our solutions in several contexts, including DNN IP ownership, ethical LLM usage enforcement, and transformer inference.', 'abstract_zh': '隐私保护技术引入了一种范式转变，允许在实际系统中实现安全计算。这些原语在大规模应用时面临的显著障碍是计算和通信开销。在本文中，我们提出了通过多方计算（MPC）、零知识证明（ZKPs）和全同态加密（FHE）缩小这种开销与实用性之间差距的努力。通过细致的硬件/软件/算法协同设计，我们展示了如何使大规模语言模型（LLM）应用在隐私保护环境中成为可能。我们在深度神经网络知识产权保护、伦理LLM使用约束以及变压器推理等多个场景中展示了我们解决方案的有效性。', 'title_zh': '优化隐私保护基本组件以支持大规模语言模型应用'}
{'arxiv_id': 'arXiv:2509.25045', 'title': 'Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures', 'authors': 'Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini', 'link': 'https://arxiv.org/abs/2509.25045', 'abstract': "Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods, such as direct logit attribution (DLA) and sparse autoencoders (SAEs), provide restricted insight due to limitations such as the model's output vocabulary or unclear feature names. This work introduces Hyperdimensional Probe, a novel paradigm for decoding information from the LLM vector space. It combines ideas from symbolic representations and neural probing to project the model's residual stream into interpretable concepts via Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs and conventional probes while overcoming their key limitations. We validate our decoding paradigm with controlled input-completion tasks, probing the model's final state before next-token prediction on inputs spanning syntactic pattern recognition, key-value associations, and abstract inference. We further assess it in a question-answering setting, examining the state of the model both before and after text generation. Our experiments show that our probe reliably extracts meaningful concepts across varied LLMs, embedding sizes, and input domains, also helping identify LLM failures. Our work advances information decoding in LLM vector space, enabling extracting more informative, interpretable, and structured features from neural representations.", 'abstract_zh': '尽管大型语言模型具备强大能力，但仍具有透明度差的特点，对其内部表示的理解有限。当前的可解释性方法，如直接logit归因（DLA）和稀疏自编码器（SAEs），由于受限于模型的输出词汇表或特征名称不明确等因素，提供的洞察力有限。本文引入了超维探针（Hyperdimensional Probe），这是一种从大语言模型向量空间解码信息的新范式。该探针结合了符号表示和神经探针的想法，通过向量符号架构（VSAs）将模型的残差流投影到可解释的概念中。该探针结合了SAEs和传统探针的优点，克服了它们的关键限制。我们通过受控的输入-完成任务，验证了解码范式的有效性，探针在涵盖句法模式识别、键值关联和抽象推理的不同输入上的模型最终状态之前，进行探针操作。我们还在问答场景下评估了它，检查模型在文本生成前后的状态。实验结果表明，我们的探针能够可靠地从各种大语言模型、嵌入大小和输入领域中提取有意义的概念，也有助于识别大语言模型中的失败情况。我们的工作推进了大语言模型向量空间中的信息解码，使得能够从神经表示中提取更具信息量、可解释性和结构化的特征。', 'title_zh': '超高维度探针：通过向量符号架构解码LLM表示'}
{'arxiv_id': 'arXiv:2509.25043', 'title': 'Large Language Models for Software Testing: A Research Roadmap', 'authors': 'Cristian Augusto, Antonia Bertolino, Guglielmo De Angelis, Francesca Lonetti, Jesús Morán', 'link': 'https://arxiv.org/abs/2509.25043', 'abstract': 'Large Language Models (LLMs) are starting to be profiled as one of the most significant disruptions in the Software Testing field.\nSpecifically, they have been successfully applied in software testing tasks such as generating test code, or summarizing documentation.\nThis potential has attracted hundreds of researchers, resulting in dozens of new contributions every month, hardening researchers to\nstay at the forefront of the wave. Still, to the best of our knowledge, no prior work has provided a structured vision of the progress\nand most relevant research trends in LLM-based testing. In this article, we aim to provide a roadmap that illustrates its current state,\ngrouping the contributions into different categories, and also sketching the most promising and active research directions for the field.\nTo achieve this objective, we have conducted a semi-systematic literature review, collecting articles and mapping them into the most\nprominent categories, reviewing the current and ongoing status, and analyzing the open challenges of LLM-based software testing.\nLastly, we have outlined several expected long-term impacts of LLMs over the whole software testing field.', 'abstract_zh': '大型语言模型（LLMs）正成为软件测试领域最重要的颠覆之一。', 'title_zh': '大型语言模型在软件测试中的应用：研究路线图'}
{'arxiv_id': 'arXiv:2509.25035', 'title': 'Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct', 'authors': 'Haoyang Zheng, Xinyang Liu, Cindy Xiangrui Kong, Nan Jiang, Zheyuan Hu, Weijian Luo, Wei Deng, Guang Lin', 'link': 'https://arxiv.org/abs/2509.25035', 'abstract': 'Fast generation of language texts is the holy grail that people pursue in the AI era. In this work, we introduced Discrete Diffusion Divergence Instruct (DiDi-Instruct), a training-based method that leads to fast language generation models by initializing from a pre-trained (masked) discrete diffusion language model (dLLM). The resulting DiDi-Instruct model outperforms the dLLM counterparts and the GPT-2 baseline with 64x acceleration. In the theoretical part of the paper, we build the foundation of DiDi-Instruct in a framework of integral KL-divergence minimization, with practical training algorithms. We also introduce techniques like grouped reward normalization, intermediate-state matching, and the reward-guided ancestral sampler (RGAS) that significantly improve the training stability, the model coverage, and the inference performances. On OpenWebText, DiDi-Instruct outperforms all accelerated language generation models as well as the GPT-2 baseline and the standard dLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128 NFEs). These performance gains are accomplished with a negligible entropy loss of about 1% and 20x less additional training wall-clock time. We further validate the robustness and effectiveness of DiDi-Instruct through extensive ablation studies, model scaling, and the generation of discrete protein sequences. In conclusion, DiDi-Instruct is an efficient yet effective distillation method, enabling language generation in the blink of an eye. We will release both code and models at this http URL.', 'abstract_zh': '快速生成语言文本是人工智能时代人们追求的圣杯。在这项工作中，我们引入了离散扩散 divergence 命令（DiDi-Instruct）方法，该方法通过从预训练（遮蔽）离散扩散语言模型（dLLM）初始化，实现快速语言生成模型。DiDi-Instruct 模型在 64 倍加速下优于 dLLM 对手和 GPT-2 基线。在论文的理论部分中，我们以积分 KL 散度最小化框架为基础，构建了 DiDi-Instruct 的理论基础，并提供了实用的训练算法。我们还介绍了分组奖励归一化、中间状态匹配以及奖励引导祖先采样器（RGAS）等技术，显著提高了训练稳定性、模型覆盖面和推理性能。在 OpenWebText 上，DiDi-Instruct 在加速语言生成模型、GPT-2 基线和标准 dLLM 中表现最优，样本 perplexities 范围从 62.2（8 NFEs）到 18.4（128 NFEs）。这些性能提升仅伴随不到 1% 的微小熵损失和 20 倍少的额外训练时间。我们通过广泛的消融研究、模型扩展和离散蛋白质序列生成进一步验证了 DiDi-Instruct 的稳健性和有效性。总之，DiDi-Instruct 是一种高效且有效的模型蒸馏方法，能够在眨眼之间实现语言生成。我们将在该网址发布代码和模型。', 'title_zh': '通过离散扩散 divergence 指令实现超快速语言生成'}
{'arxiv_id': 'arXiv:2509.24988', 'title': 'Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns', 'authors': 'Hanqi Xiao, Vaidehi Patil, Hyunji Lee, Elias Stengel-Eskin, Mohit Bansal', 'link': 'https://arxiv.org/abs/2509.24988', 'abstract': 'Generating accurate and calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as a problem of eliciting a model\'s "self-knowledge", i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answer\'s correctness that is accessible to the model itself. However, our experiments reveal that an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated LLM. Moreover, we hypothesize that a key factor in building a "Correctness Model" (CM) is exposure to a target model\'s historical predictions. We propose multiple methods to inject this historical correctness information, creating a Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness data from many LLMs and learn patterns for correctness prediction applicable across datasets and models. We then use CMs as a lens for studying the source of correctness prediction ability and its generalization, systematically controlling their training data and finding that answer phrasing is a strong predictor for correctness. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide complementary reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on a downstream selective prediction task, finding that reliable LLM confidence estimation is a generalizable and model-agnostic skill learned by systematically encoding correctness history rather than a model-specific skill reliant on self-introspection.', 'abstract_zh': '生成准确且校准的置信估计对于在高风险或用户面向的应用中部署大语言模型至关重要，但仍是一个开放性的挑战。先前的研究往往将置信度问题视为提取模型的“自我知识”，即模型判断其自身答案是否正确的能力；这种方法假定模型本身可以访问到关于答案正确性的某些优先信息。然而，我们的实验揭示，一个模型试图预测其自身输出的正确性通常与一个不相关的模型表现相当。此外，我们假设构建“正确性模型”（CM）的一个重要因素是接触目标模型的历史预测。我们提出了多种方法来注入这种历史正确性信息，创建了一种广义正确性模型（GCM）。我们首先表明，GCM可以在多个大语言模型的正确性数据上进行训练，并学习适用于不同数据集和模型的正确性预测模式。然后，我们使用CM作为研究正确性预测能力和其泛化的视角，系统控制其训练数据，发现答案表述是正确性预测的一个强大预测因子。进一步探索不通过训练大语言模型来注入历史的方法，发现包括历史作为上下文示例可以帮助提高正确性预测，并且事后校准可以提供额外的校准误差减少。我们基于Qwen3-8B模型和MMLU、TriviaQA数据集以及一个下游的有选择的预测任务评估了GCM，发现可靠的LSTM置信估计是通过系统编码正确性历史学习到的一项可泛化和模型无关的能力，而不是依赖自我反省的模型特定技能。', 'title_zh': '广义正确性模型：从历史模式学习校准的模型无关正确性预测器'}
{'arxiv_id': 'arXiv:2509.24981', 'title': 'Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards', 'authors': 'Haoran He, Yuxiao Ye, Qingpeng Cai, Chen Hu, Binxing Jiao, Daxin Jiang, Ling Pan', 'link': 'https://arxiv.org/abs/2509.24981', 'abstract': "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving the reasoning abilities of large language models (LLMs). Current methods rely primarily on policy optimization frameworks like PPO and GRPO, which follow generalized policy iteration that alternates between evaluating the current policy's value and improving the policy based on evaluation. While effective, they often suffer from training instability and diversity collapse, requiring complex heuristic tricks and careful tuning. We observe that standard RLVR in math reasoning can be formalized as a specialized finite-horizon Markov Decision Process with deterministic state transitions, tree-structured dynamics, and binary terminal rewards. Though large in scale, the underlying structure is simpler than general-purpose control settings for which popular RL algorithms (e.g., PPO) were developed, suggesting that several sophisticated techniques in existing methods may be reduced or even omitted. Based on this insight, we prove a surprising result: the optimal action can be recovered from the Q-function of a fixed uniformly random policy, thereby bypassing the generalized policy iteration loop and its associated heuristics. We introduce Random Policy Valuation for Diverse Reasoning (ROVER) to translate this principle into a practical and scalable algorithm for LLM math reasoning, a minimalist yet highly effective RL method that samples actions from a softmax over these uniform-policy Q-values. ROVER preserves diversity throughout training, allowing sustained exploration of multiple valid pathways. Across multiple base models and standard math reasoning benchmarks, ROVER demonstrates superior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1, \\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite its radical simplification compared to strong, complicated existing methods.", 'abstract_zh': '具有可验证奖励的RL（RLVR）在提高大规模语言模型的推理能力方面展现了前景：RLVR在数学推理中的形式化', 'title_zh': '随机策略评估足以实现带有可验证奖励的LLM推理'}
{'arxiv_id': 'arXiv:2509.24967', 'title': 'SecInfer: Preventing Prompt Injection via Inference-time Scaling', 'authors': 'Yupei Liu, Yanting Wang, Yuqi Jia, Jinyuan Jia, Neil Zhenqiang Gong', 'link': 'https://arxiv.org/abs/2509.24967', 'abstract': 'Prompt injection attacks pose a pervasive threat to the security of Large Language Models (LLMs). State-of-the-art prevention-based defenses typically rely on fine-tuning an LLM to enhance its security, but they achieve limited effectiveness against strong attacks. In this work, we propose \\emph{SecInfer}, a novel defense against prompt injection attacks built on \\emph{inference-time scaling}, an emerging paradigm that boosts LLM capability by allocating more compute resources for reasoning during inference. SecInfer consists of two key steps: \\emph{system-prompt-guided sampling}, which generates multiple responses for a given input by exploring diverse reasoning paths through a varied set of system prompts, and \\emph{target-task-guided aggregation}, which selects the response most likely to accomplish the intended task. Extensive experiments show that, by leveraging additional compute at inference, SecInfer effectively mitigates both existing and adaptive prompt injection attacks, outperforming state-of-the-art defenses as well as existing inference-time scaling approaches.', 'abstract_zh': 'Prompt注入攻击对大型语言模型的安全性构成广泛威胁。最先进的基于预防的防御通常依赖于对大型语言模型进行微调以增强其安全性，但它们对强大的攻击效果有限。在本文中，我们提出了一种名为SecInfer的新颖防御方法，该方法基于推理时扩展这一新兴范式，通过在推理时分配更多的计算资源来提升大型语言模型的能力。SecInfer包括两个关键步骤：系统提示引导采样和目标任务引导聚合。通过在推理时利用额外的计算资源，SecInfer有效缓解了现有和适应性的Prompt注入攻击，性能优于最先进的防御方法以及现有的推理时扩展方法。', 'title_zh': 'SecInfer: 在推理时缩放以防止提示注入'}
{'arxiv_id': 'arXiv:2509.24945', 'title': 'MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes', 'authors': 'Changsheng Zhao, Ernie Chang, Zechun Liu, Chia-Jung Chang, Wei Wen, Chen Lai, Rick Cao, Yuandong Tian, Raghuraman Krishnamoorthi, Yangyang Shi, Vikas Chandra', 'link': 'https://arxiv.org/abs/2509.24945', 'abstract': "The paradigm shift in large language models (LLMs) from instinctive responses to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1) reasoning capabilities only emerge in sufficiently large models, and (2) such capabilities require training on massive datasets. While the first assumption has already been challenged by recent sub-billion-parameter reasoning models such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely unquestioned. In this work, we revisit the necessity of scaling to extremely large corpora (>10T tokens) for reasoning emergence. By carefully curating and resampling open-source datasets that we identify as beneficial under our designed metrics, we demonstrate that strong reasoning abilities can emerge with far less data. Specifically, we show that only ~2T tokens of high-quality data are sufficient, and pre-training with 4.2T tokens on the dataset resampled from these ~2T tokens, followed by a established post-training procedure, enables the development of MobileLLM-R1, a series of sub-billion-parameter reasoning models that substantially outperform prior models trained on fully open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of 15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B. Remarkably, despite being trained on only 11.7% of the tokens compared to Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate further research in this direction, we have released the complete training recipe, data sources, data mixing ratio, and model checkpoints, together with the key insights obtained throughout this study.", 'abstract_zh': '大型语言模型（LLMs）从直觉响应转变为链式思考（CoT）推理的范式转变促使了两种主导假设的出现：（1）推理能力仅在足够大的模型中出现，（2）此类能力需要在大量数据集上进行训练。虽然第一个假设已经被最近的次亿参数推理模型如Qwen3-0.6B和DeepSeek精炼变体所挑战，第二个假设依然基本未受质疑。在此项工作中，我们重新审视了推理能力出现对极其大量语料库（>10T令牌）的必要性。通过精心筛选和重新采样我们设计指标下认定有益的开源数据集，我们证明了强大的推理能力可以使用远少的数据出现。具体而言，我们证明了仅约2T令牌的高质量数据就足够，通过从这些约2T令牌重新采样获得的4.2T令牌进行预训练，再结合现成的后训练程序，可以开发出MobileLLM-R1系列次亿参数推理模型，这些模型在多个推理基准测试中显著优于从前完全使用开源数据训练的模型。例如，MobileLLM-R1-950M获得了AIME得分为15.5，而OLMo-2-1.48B仅为0.6，SmolLM-2-1.7B仅为0.3。值得注意的是，尽管MobileLLM-R1-950M在预训练时仅使用了与Qwen3的36T令牌私有语料库相比11.7%的令牌，但在多个推理基准测试中，它还是与Qwen3-0.6B相匹配甚至超越。为了促进这一领域的进一步研究，我们已经发布了完整的训练食谱、数据来源、数据混合比例以及模型检查点，还包括本研究中获得的关键见解。', 'title_zh': 'MobileLLM-R1：探索亚亿参数语言模型推理器的能力极限与开放训练配方'}
{'arxiv_id': 'arXiv:2509.24923', 'title': 'When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training', 'authors': 'Sanxing Chen, Xiaoyin Chen, Yukun Huang, Roy Xie, Bhuwan Dhingra', 'link': 'https://arxiv.org/abs/2509.24923', 'abstract': 'While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.', 'abstract_zh': '大型语言模型在序列决策中的自主探索策略研究：监督微调与强化学习的比较与改进', 'title_zh': '贪心取胜：元臂 bandit LLM 训练中的新兴 exploitation 偏好'}
{'arxiv_id': 'arXiv:2509.24869', 'title': 'Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval', 'authors': 'Junwei Lan, Jianlyu Chen, Zheng Liu, Chaofan Li, Siqi Bao, Defu Lian', 'link': 'https://arxiv.org/abs/2509.24869', 'abstract': "With the growing popularity of LLM agents and RAG, it has become increasingly important to retrieve documents that are essential for solving a task, even when their connection to the task is indirect or implicit. Addressing this problem requires fine-grained reasoning to accurately assess the relevance between the task and each candidate document. This capability, however, poses a significant challenge for existing IR techniques. Despite recent progress in reasoning-enhanced IR, existing approaches still face significant challenges in applicability, scalability, and efficiency. In this work, we propose Retro*, a novel approach for reasoning-intensive document retrieval. Our method introduces a rubric-based relevance scoring mechanism, enabling the model to reason about the relationship between a task and a document based on explicitly defined criteria, whereby producing a fine-grained, interpretable relevance score. Retro* also supports test-time scaling by combining multiple reasoning trajectories via score integration, which produces more reliable relevance estimates. To optimize Retro*'s reasoning capabilities, we introduce a novel reinforcement learning algorithm tailored for its relevance scoring mechanism, which employs two composite rewards to fully exploit the trajectories of each training sample. Our experiments show that Retro* outperforms existing document retrieval methods with notable advantages, leading to state-of-the-art performance on the BRIGHT benchmark.", 'abstract_zh': '随着大型语言模型代理和RAG的流行，从间接或隐含关联的文档中检索对完成任务至关重要的文档变得越来越重要。为解决这一问题需要精细推理以准确评估任务与每个候选文档之间的相关性。然而，这种能力为现有信息检索技术带来了重大挑战。尽管在增强推理的信息检索方面取得了进展，现有方法仍然面临着应用性、扩展性和效率方面的重大挑战。在本工作中，我们提出了Retro*，一种新型的密集推理文档检索方法。该方法引入了一种基于评分标准的相关性评分机制，使模型能够基于明确定义的标准来推理任务与文档之间的关系，从而生成细化且可解释的相关性评分。Retro* 还通过分数集成组合多个推理轨迹来支持推理时的扩展性，从而产生更可靠的相关性估计。为了优化Retro*的推理能力，我们引入了一种专门为其实现相关性评分机制设计的新颖强化学习算法，该算法使用两个复合奖励充分利用每个训练样本的轨迹。我们的实验表明，Retro* 在文档检索方面优于现有方法，具有显著优势，达到了BRIGHT基准上的最佳性能。', 'title_zh': 'Retro*: 优化大语言模型用于推理密集型文档检索'}
{'arxiv_id': 'arXiv:2509.24866', 'title': 'Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning', 'authors': 'Matteo Fuoli, Weihang Huang, Jeannette Littlemore, Sarah Turner, Ellen Wilding', 'link': 'https://arxiv.org/abs/2509.24866', 'abstract': 'Metaphor is a pervasive feature of discourse and a powerful lens for examining cognition, emotion, and ideology. Large-scale analysis, however, has been constrained by the need for manual annotation due to the context-sensitive nature of metaphor. This study investigates the potential of large language models (LLMs) to automate metaphor identification in full texts. We compare three methods: (i) retrieval-augmented generation (RAG), where the model is provided with a codebook and instructed to annotate texts based on its rules and examples; (ii) prompt engineering, where we design task-specific verbal instructions; and (iii) fine-tuning, where the model is trained on hand-coded texts to optimize performance. Within prompt engineering, we test zero-shot, few-shot, and chain-of-thought strategies. Our results show that state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning yielding a median F1 score of 0.79. A comparison of human and LLM outputs reveals that most discrepancies are systematic, reflecting well-known grey areas and conceptual challenges in metaphor theory. We propose that LLMs can be used to at least partly automate metaphor identification and can serve as a testbed for developing and refining metaphor identification protocols and the theory that underpins them.', 'abstract_zh': '元喻是一种普遍存在于话语中的特征，是探讨认知、情感和意识形态的强大视角。然而，大规模分析受到了手工标注的限制，因为元喻具有上下文敏感性。本研究探讨了大规模语言模型（LLMs）自动识别全文本中元喻的可能性。我们比较了三种方法：（i）检索增强生成（RAG），模型在提供代码簿的情况下，根据其规则和示例对文本进行标注；（ii）提示工程，我们设计了特定任务的口头指令；（iii）微调，模型在手工标注的文本上进行训练以优化性能。在提示工程中，我们测试了零样本、少样本和思维链策略。我们的结果显示，最先进的闭源LLMs可以实现高精度，微调方法的中位数F1得分为0.79。人类和LLM输出的比较表明，大多数差异是系统性的，反映了元喻理论中众所周知的灰色地带和概念挑战。我们提出，LLMs可以至少部分自动化元喻识别，并可作为开发和精炼元喻识别协议及其支撑理论的测试床。', 'title_zh': '使用大规模语言模型识别隐喻：RAG、提示工程和微调的比较'}
{'arxiv_id': 'arXiv:2509.24841', 'title': 'Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement', 'authors': 'Zhilong Zhao, Yindi Liu', 'link': 'https://arxiv.org/abs/2509.24841', 'abstract': 'Large Language Models face significant performance challenges in specialized domains, with state-of-the-art models achieving only 45.9% accuracy on medical coding tasks. This study proposes a Hierarchical Error Correction (HEC) framework that addresses domain-specific AI limitations through systematic error analysis and targeted intervention strategies.\nWe analyze error patterns across four specialized domains and find that AI errors follow consistent hierarchical structures: Knowledge-layer errors (58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%). Based on these patterns, we develop a three-stage correction framework that addresses errors according to their hierarchical importance and demonstrates that framework effectiveness correlates inversely with baseline task performance.\nExperimental validation across medical transcription (4,921 cases), legal document classification (1,000 cases), political bias detection (645 cases), and legal reasoning (1,000 cases) shows consistent improvements. Cross-model validation across five LLM architectures demonstrates average improvements of 11.2 percentage points (p < 0.001). However, analysis reveals framework limitations in high-baseline tasks (>75% accuracy), where hierarchical intervention may interfere with effective reasoning processes.\nThe results suggest that systematic error analysis can guide effective AI enhancement strategies in specialized domains, particularly for moderate-baseline tasks, while highlighting the importance of understanding framework boundaries for optimal deployment.', 'abstract_zh': '大规模语言模型在专门领域面临显著的性能挑战，最前沿的模型在医疗编码任务上的准确性仅为45.9%。本研究提出了一种层次错误修正（HEC）框架，通过系统性错误分析和针对性干预策略解决领域特定的人工智能限制。', 'title_zh': '大型语言模型的分层错误纠正：一种特定领域AI质量提升的系统框架'}
{'arxiv_id': 'arXiv:2509.24832', 'title': 'SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching', 'authors': 'Xinye Zhao, Spyridon Mastorakis', 'link': 'https://arxiv.org/abs/2509.24832', 'abstract': "As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \\textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference.", 'abstract_zh': '基于语义的KV缓存共享与压缩框架：加速大型语言模型推理', 'title_zh': 'SemShareKV: 基于词元级LSH匹配的语义相似提示高效KV缓存共享'}
{'arxiv_id': 'arXiv:2509.24828', 'title': 'Evaluating SAP Joule for Code Generation', 'authors': 'Joshua Heisler, Johannes Reisinger, Andreas Fischer', 'link': 'https://arxiv.org/abs/2509.24828', 'abstract': 'SAP has released its own proprietary generative model SAP Joule, intended for various generative tasks, including serving as a code assistant for software engineers. While Joule is yet not focused on SAP-specific ABAP code generation, it can be used for other common languages, including Javascript. This paper compares SAP Joules Javascript coding capabilities against a total of 29 other models using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict accuracy of 80.49% as the fifth best model in our evaluation. To the best of our knowledge, this is the first comparative evaluation of SAP Joule code generation capabilities.', 'abstract_zh': 'SAP发布的专有生成模型SAP Joule及其在JavaScript编码能力方面的比较研究', 'title_zh': '评估SAP Joule的代码生成能力'}
{'arxiv_id': 'arXiv:2509.24827', 'title': 'Putnam-like dataset summary: LLMs as mathematical competition contestants', 'authors': 'Bartosz Bieganowski, Daniel Strzelecki, Robert Skiba, Mateusz Topolewski', 'link': 'https://arxiv.org/abs/2509.24827', 'abstract': 'In this paper we summarize the results of the Putnam-like benchmark published by Google DeepMind. This dataset consists of 96 original problems in the spirit of the Putnam Competition and 576 solutions of LLMs. We analyse the performance of models on this set of problems to verify their ability to solve problems from mathematical contests.', 'abstract_zh': '本文总结了Google DeepMind发布的Putnam-like基准测试的结果。该数据集包含96个具有Putnam竞赛精神的原始问题和576个大型语言模型的解决方案。我们分析模型在这组问题上的表现，以验证其解决数学竞赛问题的能力。', 'title_zh': 'Putnam-like 数据集总结：LLMs 作为数学竞赛选手'}
{'arxiv_id': 'arXiv:2509.24701', 'title': 'FedPOB: Sample-Efficient Federated Prompt Optimization via Bandits', 'authors': 'Pingchen Lu, Zhi Hong, Zhiwei Shang, Zhiyong Wang, Yikun Ban, Yao Shu, Min Zhang, Shuang Qiu, Zhongxiang Dai', 'link': 'https://arxiv.org/abs/2509.24701', 'abstract': 'The performance of large language models (LLMs) is highly sensitive to the input prompt, making prompt optimization a critical task. However, real-world application is hindered by three major challenges: (1) the black-box nature of powerful proprietary LLMs, (2) the need for high sample efficiency due to query costs, and (3) the desire for privacy-preserving collaboration among multiple users. To address these challenges simultaneously, we introduce a novel framework for sample-efficient federated prompt optimization based on multi-armed bandits (MABs). The MAB framework is uniquely suited for this problem as it is (1) inherently a black-box optimization method, (2) practically sample-efficient, and (3) enables collaborative learning with theoretically guaranteed benefit from more participating agents. We first propose the Federated Prompt Optimization via Bandits (FedPOB) algorithm, a federated variant of the Linear UCB algorithm, where agents collaborate by sharing model parameters instead of raw data. We then extend our approach to the practical setting of comparative user feedback by introducing FedPOB with Preference Feedback (FedPOB-Pref), an efficient algorithm based on federated dueling bandits. Extensive experiments demonstrate that both FedPOB and FedPOB-Pref significantly outperform existing baselines and that their performance consistently improves as more agents participate in the collaboration, validating the effectiveness of our federated approach.', 'abstract_zh': '基于多臂 bandit 的高效联邦提示优化框架', 'title_zh': 'FedPOB: 基于_bandits的高效联邦提示优化'}
{'arxiv_id': 'arXiv:2509.24696', 'title': 'T-POP: Test-Time Personalization with Online Preference Feedback', 'authors': 'Zikun Qu, Min Zhang, Mingze Kong, Xiang Li, Zhiwei Shang, Zhiyong Wang, Yikun Ban, Shuang Qiu, Yao Shu, Zhongxiang Dai', 'link': 'https://arxiv.org/abs/2509.24696', 'abstract': 'Personalizing large language models (LLMs) to individual user preferences is a critical step beyond generating generically helpful responses. However, current personalization methods are ill-suited for new users, as they typically require either slow, resource-intensive fine-tuning or a substantial amount of pre-existing user data, creating a significant cold-start problem. To address this challenge, we introduce a new paradigm for real-time personalization by learning from online pairwise preference feedback collected during text generation. We propose T-POP (Test-Time Personalization with Online Preference Feedback}), a novel algorithm that synergistically combines test-time alignment with dueling bandits. Without updating the LLM parameters, T-POP steers the decoding process of a frozen LLM by learning a reward function online that captures user preferences. By leveraging dueling bandits, T-POP intelligently queries the user to efficiently balance between exploring their preferences and exploiting the learned knowledge to generate personalized text. Extensive experiments demonstrate that T-POP achieves rapid and data-efficient personalization, significantly outperforming existing baselines and showing consistent improvement with more user interactions.', 'abstract_zh': '基于在线偏好反馈的实时个性化（T-POP）', 'title_zh': 'T-POP: 测试时个性化处理伴在线偏好反馈'}
{'arxiv_id': 'arXiv:2509.24678', 'title': 'Reference-Free Rating of LLM Responses via Latent Information', 'authors': 'Leander Girrbach, Chi-Ping Su, Tankred Saanum, Richard Socher, Eric Schulz, Zeynep Akata', 'link': 'https://arxiv.org/abs/2509.24678', 'abstract': 'How reliable are single-response LLM-as-a-judge ratings without references, and can we obtain fine-grained, deterministic scores in this setting? We study the common practice of asking a judge model to assign Likert-scale scores to free-text responses and show two systematic issues: scores are unstable under sampling and poorly calibrated, leading to compression near the top of the scale and frequent ties. We then propose and evaluate Latent Judges, which derive scalar ratings from internal model signals: (i) probability-weighted scores over integer ratings, (ii) verifier-style probabilities of "yes", and (iii) linear probes trained on model activations at the rating position. Across a broad suite of pairwise and single-rating benchmarks, latent methods match or surpass standard prompting, with consistent gains on pairwise accuracy and listwise ranking relevant to Best-of-N selection. Probability-weighted scores achieve the strongest single-rating correlations, while probes recover useful signals when output logits are miscalibrated. These results indicate that latent information provides deterministic and more discriminative signals for reference-free evaluation, and can improve selection and training approaches like Best-of-$N$, multi-teacher distillation, and routing.', 'abstract_zh': '单响应LLM作为法官的评分在没有参考的情况下有多可靠？我们能在这种情况下获得细粒度的确定性评分吗？我们研究了让法官模型为自由文本答复分配李克特量表评分的常用做法，并揭示了两个系统性问题：评分在抽样下不稳定且校准不良，导致评分压缩在量表的顶部并频繁出现并列。随后，我们提出了并评估了潜在法官，该方法从内部模型信号-derived scalar ratings from internal model signals:(i) 以整数评分的概率加权分数，(ii) “是”验证器风格的概率，(iii) 在评分位置上模型激活的线性探测器。在广泛的数据对和单评分基准测试中，潜在方法匹配或超越了标准提示，显示出在数据对准确性和相关于Best-of-N选择的列表排序上的一致性改进。以概率加权分数获得最强的单评分相关性，而探测器在输出logits校准不当时仍可恢复有用的信号。这些结果表明，潜在信息提供了参考自由评估中更确定性和更具区分度的信号，并可以改进如Best-of-$N$、多师表蒸馏和路由等选择和训练方法。', 'title_zh': '基于潜在信息的无需参考评分方法评估LLM响应'}
{'arxiv_id': 'arXiv:2509.24675', 'title': 'Understanding the Dilemma of Unlearning for Large Language Models', 'authors': 'Qingjie Zhang, Haoting Qian, Zhicong Huang, Cheng Hong, Minlie Huang, Ke Xu, Chao Zhang, Han Qiu', 'link': 'https://arxiv.org/abs/2509.24675', 'abstract': 'Unlearning seeks to remove specific knowledge from large language models (LLMs), but its effectiveness remains contested. On one side, "forgotten" knowledge can often be recovered through interventions such as light fine-tuning; on the other side, unlearning may induce catastrophic forgetting that degrades general capabilities. Despite active exploration of unlearning methods, interpretability analyses of the mechanism are scarce due to the difficulty of tracing knowledge in LLMs\' complex architectures. We address this gap by proposing unPact, an interpretable framework for unlearning via prompt attribution and contribution tracking. Typically, it quantifies each prompt token\'s influence on outputs, enabling pre- and post-unlearning comparisons to reveal what changes. Across six mainstream unlearning methods, three LLMs, and three benchmarks, we find that: (1) Unlearning appears to be effective by disrupting focus on keywords in prompt; (2) Much of the knowledge is not truly erased and can be recovered by simply emphasizing these keywords in prompts, without modifying the model\'s weights; (3) Catastrophic forgetting arises from indiscriminate penalization of all tokens. Taken together, our results suggest an unlearning dilemma: existing methods tend either to be insufficient - knowledge remains recoverable by keyword emphasis, or overly destructive - general performance collapses due to catastrophic forgetting, still leaving a gap to reliable unlearning.', 'abstract_zh': '基于提示归因和贡献追踪的可解释卸载框架：解决知识卸载的有效性和危害性难题', 'title_zh': '理解大型语言模型退学的困境'}
{'arxiv_id': 'arXiv:2509.24663', 'title': 'InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation', 'authors': 'Weilin Zhao, Zihan Zhou, Zhou Su, Chaojun Xiao, Yuxuan Li, Yanghao Li, Yudi Zhang, Weilun Zhao, Zhen Li, Yuxiang Huang, Ao Sun, Xu Han, Zhiyuan Liu', 'link': 'https://arxiv.org/abs/2509.24663', 'abstract': 'Long-sequence processing is a critical capability for modern large language models. However, the self-attention mechanism in the standard Transformer architecture faces severe computational and memory bottlenecks when processing long sequences. While trainable sparse attention methods offer a promising solution, existing approaches such as NSA introduce excessive extra parameters and disrupt the conventional \\textit{pretrain-on-short, finetune-on-long} workflow, resulting in slow convergence and difficulty in acceleration. To overcome these limitations, we introduce dense-sparse switchable attention framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2 reuses dense attention parameters through parameter-free architecture modification, maintaining consistency between short and long sequence processing. Additionally, InfLLM-V2 ensures computational efficiency across all sequence lengths, by using dense attention for short inputs and smoothly transitioning to sparse attention for long sequences. To achieve practical acceleration, we further introduce an efficient implementation of InfLLM-V2 that significantly reduces the computational overhead. Our experiments on long-context understanding and chain-of-thought reasoning demonstrate that InfLLM-V2 is 4$\\times$ faster than dense attention while retaining 98.1% and 99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we have trained and open-sourced MiniCPM4.1 (this https URL), a hybrid reasoning model, providing a reproducible implementation for the research community.', 'abstract_zh': '长序列处理是现代大规模语言模型的关键能力。然而，标准Transformer架构中的自我注意力机制在处理长序列时面临着严峻的计算和内存瓶颈。虽然可训练的稀疏注意力方法提供了有前景的解决方案，但现有方法如NSA引入了过多的额外参数，并扰乱了传统的“预训练短序列，微调长序列”工作流程，导致收敛速度慢且难以加速。为了克服这些限制，我们介绍了一种可训练的稀疏注意力框架，称为InfLLM-V2。InfLLM-V2能够无缝地将模型从短序列过渡到长序列。具体而言，InfLLM-V2通过参数无改动的架构修改重用密集注意力参数，保持短序列和长序列处理的一致性。此外，InfLLM-V2通过使用密集注意力处理短输入并在长序列上平滑过渡到稀疏注意力，确保所有序列长度下的计算效率。为了实现实际加速，我们还引入了InfLLM-V2的高效实现，显著减少了计算开销。我们的实验表明，InfLLM-V2在长上下文理解与链式推理任务中比密集注意力快4倍，同时分别保留了98.1%和99.7%的性能。基于InfLLM-V2框架，我们已经训练并开源了MiniCPM4.1（请点击此链接），提供了一个可再现的实现供研究界使用。', 'title_zh': 'InfLLM-V2: 可切换密集-稀疏注意机制以实现无缝短到长适应'}
{'arxiv_id': 'arXiv:2509.24653', 'title': 'Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory', 'authors': 'Pengxiao Lin, Zheng-An Chen, Zhi-Qin John Xu', 'link': 'https://arxiv.org/abs/2509.24653', 'abstract': "Despite remarkable advances, large language models often fail at compositional reasoning tasks, a phenomenon exemplified by the ``curse of two-hop reasoning''. This paper introduces the Identity Bridge, a simple yet powerful mechanism that resolves this compositionality gap by supervising the model on a zero-hop identity task. We demonstrate empirically that this addition enables models to successfully perform out-of-distribution two-hop reasoning, a task they otherwise completely fail. To explain this phenomenon, we provide a theoretical analysis using a simplified Emb-MLP model, proving that identity supervision reshapes the model's latent geometry. We show this alignment is induced by an implicit nuclear-norm regularization during optimization, which favors low-rank solutions that share structure across tasks. For complex tasks, we use small initialization or weight decay to enhance the regularization effect, which enhances the latent space alignment effect and slows down the generalization decay. Finally, we extend our investigation to large-scale models, observing that they still achieve two-hop reasoning through the latent memory, which provides crucial inspiration for enhancing their implicit reasoning abilities.", 'abstract_zh': '尽管取得了显著进展，大型语言模型在组合推理任务上常常表现不佳，这一现象被“两跳推理的诅咒”所体现。本文提出了Identity Bridge，这是一种简单而强大的机制，通过监督模型完成零跳身份任务来解决组合性差距。实验结果表明，这一添加使模型能够成功执行超出分布的两跳推理任务，而它们原本完全无法完成这一任务。为了解释这一现象，我们利用简化的Emb-MLP模型进行了理论分析，证明身份监督重塑了模型的潜在几何结构。我们展示了这种对齐是由优化中的隐式核范数正则化诱导的，它倾向于低秩解并在任务之间共享结构。对于复杂任务，我们通过小初始化或权重衰减来增强正则化效果，从而增强潜在空间对齐效果并减缓泛化衰退。最后，我们将研究扩展到大规模模型，观察到它们仍然通过潜在记忆实现两跳推理，这为增强其隐式推理能力提供了关键灵感。', 'title_zh': '身份桥梁：通过共享潜在记忆实现隐式推理'}
{'arxiv_id': 'arXiv:2509.24560', 'title': 'AdaThink-Med: Medical Adaptive Thinking with Uncertainty-Guided Length Calibration', 'authors': 'Shaohao Rui, Kaitao Chen, Weijie Ma, Xiaosong Wang', 'link': 'https://arxiv.org/abs/2509.24560', 'abstract': 'Recent advances in inference time scaling with extended long chain-of thought have significantly improved the reasoning capabilities of both general and medical large language models (LLMs). However, these models tend to engage in lengthy reasoning processes regardless of the difficulty of the input question, leading to increased inference costs in real-world applications. Therefore, enabling adaptive thinking where models think less for simpler questions and think more for complex ones is critical for the effective use of medical LLMs in practice. Despite its importance, there is a lack of end-to-end approaches designed to enhance the adaptive thinking capabilities of medical LLMs while providing a comprehensive examination of the trade-off between performance and computational cost. To bridge this gap, we propose AdaThink-Med, the first end-to-end framework designed to enhance adaptive thinking ability in medical reasoning models with uncertainty-guided length calibration. AdaThink-Med first generates multiple candidate outputs for each question, evaluates the correctness and uncertainty of each candidate, and then estimates problem difficulty via an uncertainty-guided length calibration module. For outputs with low difficulty and correct answers, the framework penalizes longer reasoning paths; whereas for those with high difficulty and incorrect answers, it encourages extending the chain of thought to explore alternative solutions. On six public medical QA benchmarks, AdaThink-Med achieves up to 6.4x length reduction on average while retaining performance with only minimal degradation. Intriguingly, we observe that AdaThink-Med spontaneously develops two distinct reasoning modes, which we characterize as "non-thinking" and "thinking", demonstrating the model\'s ability to suppress redundant reasoning processes dynamically.', 'abstract_zh': 'Recent advances in inference time scaling with extended long chain-of-thought have significantly improved the reasoning capabilities of both general and medical large language models (LLMs)', 'title_zh': 'AdaThink-Med：基于不确定性导向长度校准的医疗自适应思考'}
{'arxiv_id': 'arXiv:2509.24515', 'title': 'Agentic Specification Generator for Move Programs', 'authors': 'Yu-Fu Fu, Meng Xu, Taesoo Kim', 'link': 'https://arxiv.org/abs/2509.24515', 'abstract': 'While LLM-based specification generation is gaining traction, existing tools primarily focus on mainstream programming languages like C, Java, and even Solidity, leaving emerging and yet verification-oriented languages like Move underexplored. In this paper, we introduce MSG, an automated specification generation tool designed for Move smart contracts. MSG aims to highlight key insights that uniquely present when applying LLM-based specification generation to a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust code comprehension and generation capabilities even for non-mainstream languages. MSG successfully generates verifiable specifications for 84% of tested Move functions and even identifies clauses previously overlooked by experts. Additionally, MSG shows that explicitly leveraging specification language features through an agentic, modular design improves specification quality substantially (generating 57% more verifiable clauses than conventional designs). Incorporating feedback from the verification toolchain further enhances the effectiveness of MSG, leading to a 30% increase in generated verifiable specifications.', 'abstract_zh': '基于LLM的规格生成工具MSG：面向Move智能合约的独特洞察与改进', 'title_zh': '执行程序的代理规范生成器'}
{'arxiv_id': 'arXiv:2509.24510', 'title': 'Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models', 'authors': 'Jonas Hübotter, Patrik Wolf, Alexander Shevchenko, Dennis Jüni, Andreas Krause, Gil Kur', 'link': 'https://arxiv.org/abs/2509.24510', 'abstract': "Recent empirical studies have explored the idea of continuing to train a model at test-time for a given task, known as test-time training (TTT), and have found it to yield significant performance improvements. However, there is limited understanding of why and when TTT is effective. Earlier explanations mostly focused on the observation that TTT may help when applied to out-of-distribution adaptation or used with privileged data. However, the growing scale of foundation models with most test data being in-distribution questions these explanations. We instead posit that foundation models remain globally underparameterized, with TTT providing a mechanism for specialization after generalization, focusing capacity on concepts relevant to the test task. Specifically, under the linear representation hypothesis, we propose a model in which TTT achieves a substantially smaller in-distribution test error than global training. We empirically validate our model's key assumptions by training a sparse autoencoder on ImageNet, showing that semantically related data points are explained by only a few shared concepts. Finally, we perform scaling studies across image and language tasks that confirm the practical implications of our model, identifying the regimes where specialization is most effective.", 'abstract_zh': '最近的经验研究表明，在测试时继续训练模型（测试时训练，TTT）对于给定任务可以显著提高性能，但关于为什么和在何时TTT有效仍缺乏充分理解。早期的解释主要集中在TTT可能在处理分布外适应或使用特权数据时有所帮助这一观察上。然而，随着基础模型规模的增长和大部分测试数据在分布内这一事实，这些解释受到了质疑。相反，我们认为基础模型保持全球性欠参数化，在泛化之后，TTT提供了一种专业化机制，使模型容量集中在与测试任务相关的概念上。具体而言，在线性表示假设下，我们提出一个模型，在这种模型中，TTT比全局训练可以获得显著更低的分布内测试错误率。我们通过在ImageNet上训练稀疏自编码器来经验验证我们模型的关键假设，结果表明，语义相关的数据点只由少数共享概念来解释。最后，我们在图像和语言任务上进行的缩放研究证实了我们模型的实践意义，确定了专业化最为有效的条件。', 'title_zh': '泛化后的专业化：理解基础模型测试时训练'}
{'arxiv_id': 'arXiv:2509.24496', 'title': 'LLM DNA: Tracing Model Evolution via Functional Representations', 'authors': 'Zhaomin Wu, Haodong Zhao, Ziyang Wang, Jizhou Guo, Qian Wang, Bingsheng He', 'link': 'https://arxiv.org/abs/2509.24496', 'abstract': 'The explosive growth of large language models (LLMs) has created a vast but opaque landscape: millions of models exist, yet their evolutionary relationships through fine-tuning, distillation, or adaptation are often undocumented or unclear, complicating LLM management. Existing methods are limited by task specificity, fixed model sets, or strict assumptions about tokenizers or architectures. Inspired by biological DNA, we address these limitations by mathematically defining LLM DNA as a low-dimensional, bi-Lipschitz representation of functional behavior. We prove that LLM DNA satisfies inheritance and genetic determinism properties and establish the existence of DNA. Building on this theory, we derive a general, scalable, training-free pipeline for DNA extraction. In experiments across 305 LLMs, DNA aligns with prior studies on limited subsets and achieves superior or competitive performance on specific tasks. Beyond these tasks, DNA comparisons uncover previously undocumented relationships among LLMs. We further construct the evolutionary tree of LLMs using phylogenetic algorithms, which align with shifts from encoder-decoder to decoder-only architectures, reflect temporal progression, and reveal distinct evolutionary speeds across LLM families.', 'abstract_zh': '大规模语言模型（LLMs）的爆炸性增长创造了一个庞大而透明度较低的景观：成千上万的模型存在，但它们通过微调、蒸馏或适应演化关系通常是未记录或不清楚的，使LLM管理变得复杂。现有方法受限于任务特定性、固定的模型集或对分词器或架构的严格假设。借鉴生物DNA的理念，我们通过数学定义LLM DNA作为功能性行为的一种低维度、双唇同步的表示来解决这些限制。我们证明了LLM DNA满足继承性和遗传决定性属性，并确立了DNA的存在性。在此理论基础上，我们推导出一个通用、可扩展、无需训练的DNA提取管道。在对305个LLM的实验中，DNA与先前对有限子集的研究一致，并在特定任务上实现了优或竞争力的表现。超出这些任务，DNA比较揭示了LLM之间未记录的关系。我们进一步利用系统发生学算法构建了LLM的演化树，该树与从编码器-解码器到仅解码器架构的转变一致，反映了时间进程，并揭示了不同LLM家族的进化速度差异。', 'title_zh': 'LLM DNA：通过功能表示追踪模型演变'}
{'arxiv_id': 'arXiv:2509.24491', 'title': 'Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs', 'authors': 'Yuanshuai Li, Yuping Yan, Junfeng Tang, Yunxuan Li, Zeqi Zheng, Yaochu Jin', 'link': 'https://arxiv.org/abs/2509.24491', 'abstract': 'Multimodal Large Language Models (MLLMs) have significantly improved the performance of various tasks, but continue to suffer from visual hallucinations, a critical issue where generated responses contradict visual evidence. While Direct Preference Optimization(DPO) is widely used for alignment, its application to MLLMs often fails to capture fine-grained semantic differences and encourages shortcut learning. To address these challenges, we propose Semantic Curriculum Preference Optimization (SCPO), a novel framework for MLLM alignment. SCPO employs a progressive, easy-to-hard curriculum built upon our Semantic Curriculum Preference Pairs dataset, which provides fine-grained semantic contrasts sorted by difficulty. This curriculum is trained with a dynamic reference model and a novel symmetric, bidirectional objective to facilitate simultaneous learning from both textual and visual preferences. To our knowledge, SCPO is the first framework to unify semantics, symmetry, and curriculum for MLLMs alignment, effectively mitigating visual hallucinations. Extensive experiments on LLaVA models across various scales and versions validate that SCPO demonstrates superior performance compared to baseline models on multiple hallucination benchmarks, reducing the hallucination rate by up to 62.9%. Moreover, evaluations on generalized benchmarks show that SCPO improves factuality while preserving general capabilities, with its performance remaining stable across general vision-language benchmarks.', 'abstract_zh': '多模态大型语言模型（MLLMs）在各种任务中取得了显著的进步，但仍面临着视觉幻觉这一关键问题，即生成的响应与视觉证据相矛盾。尽管直接偏好优化（DPO）广泛应用于对齐，但其应用于MLLMs时往往无法捕捉细粒度的语义差异并倾向于学习捷径。为解决这些挑战，我们提出了一种新的MLLM对齐框架——语义课程偏好优化（SCPO）。SCPO利用了一个逐步提高、从易到难的课程，该课程基于我们构建的语义课程偏好对数据集，提供了按照难度排序的细粒度语义对比。该课程采用动态参考模型和一种新颖的双向对称目标进行训练，以同时从文本和视觉偏好中学习。据我们所知，SCPO是首个统一语义、对称性和课程的MLLM对齐框架，有效地缓解了视觉幻觉问题。在不同规模和版本的LLaVA模型上进行的广泛实验表明，SCPO在多个幻觉基准上表现出色，与基线模型相比，幻觉率最多降低了62.9%。此外，通用基准测试显示，SCPO在提高事实性的同时保持了一般能力，其性能在通用视觉-语言基准上保持稳定。', 'title_zh': '通过语义课程偏好优化减轻MLLMs的视觉幻觉'}
{'arxiv_id': 'arXiv:2509.24436', 'title': 'EOE: Evolutionary Optimization of Experts for Training Language Models', 'authors': 'Yingshi Chen', 'link': 'https://arxiv.org/abs/2509.24436', 'abstract': "This paper presents an evolutionary framework for the training of large language models(LLM). The models are divided into several experts(sub-networks), which have the same structure but different parameter values. Only one expert is trained at each step. After the classical AdamW optimization, some evolutionary operators(crossover, PSO, and mutation) act on the tensor weights between the current expert and the best expert. So current expert would learn the experience of best expert. The direction of best expert would help current expert's loss decrease faster. Finally, only save the weight of the best expert. Experiments show that best expert would achieve nearly the same accuracy as the full model. This would greatly reduce the size of the model for inference. Since only one expert is trained at each step, the training needs much less memory and has much higher throughput. Experiments show that the throughput would accelerate more than ten times! Our source code is available. It's a pure c++/cu framework, which is suitable for easy deployment on PCs and edge computing devices.", 'abstract_zh': '本文提出了一种用于大型语言模型训练的进化框架。模型被划分为多个专家（子网络），这些专家具有相同的结构但参数值不同。每次训练只训练一个专家。在经典AdamW优化之后，会对当前专家和最佳专家之间的张量权值应用一些进化操作（交叉、PSO和变异），从而使当前专家能学习到最佳专家的经验。最佳专家的方向有助于当前专家损失更快地下降。最后，仅保存最佳专家的权重。实验表明，最佳专家能达到与完整模型几乎相同的准确性。这将大大减小推理时模型的规模。由于每次只训练一个专家，训练所需内存更少，并且具有更高的吞吐量。实验表明，吞吐量会加速超过十倍！我们的源代码已开源，这是一个纯C++/cu框架，适用于在个人计算机和边缘计算设备上便捷部署。', 'title_zh': '专家进化优化训练语言模型'}
{'arxiv_id': 'arXiv:2509.24435', 'title': 'Alternatives To Next Token Prediction In Text Generation - A Survey', 'authors': 'Charlie Wyatt, Aditya Joshi, Flora Salim', 'link': 'https://arxiv.org/abs/2509.24435', 'abstract': 'The paradigm of Next Token Prediction (NTP) has driven the unprecedented success of Large Language Models (LLMs), but is also the source of their most persistent weaknesses such as poor long-term planning, error accumulation, and computational inefficiency. Acknowledging the growing interest in exploring alternatives to NTP, the survey describes the emerging ecosystem of alternatives to NTP. We categorise these approaches into five main families: (1) Multi-Token Prediction, which targets a block of future tokens instead of a single one; (2) Plan-then-Generate, where a global, high-level plan is created upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the autoregressive process itself into a continuous latent space; (4) Continuous Generation Approaches, which replace sequential generation with iterative, parallel refinement through diffusion, flow matching, or energy-based methods; and (5) Non-Transformer Architectures, which sidestep NTP through their inherent model structure. By synthesizing insights across these methods, this survey offers a taxonomy to guide research into models that address the known limitations of token-level generation to develop new transformative models for natural language processing.', 'abstract_zh': 'Next Token Prediction的范式推动了大规模语言模型的空前成功，但也是其最持久弱点的源泉，如长期规划能力差、错误累积和计算效率低下。鉴于对探索替代Next Token Prediction方法的兴趣不断增加，本文综述了新兴的替代方法生态系统。我们将这些方法归类为五个主要家族：(1) 多个令牌预测，旨在预测一系列未来的令牌而不是单个令牌；(2) 计划先行生成，预先制定全局的高层次计划以指导令牌级解码；(3) 潜在推理，将自回归过程本身转移到一个连续的潜在空间中；(4) 连续生成方法，通过迭代并行 refinement 的方法（如扩散、流匹配或基于能量的方法）替换序列生成；以及(5) 非Transformer架构，通过其内在的模型结构绕过Next Token Prediction。通过对这些方法的综合洞察，本文提供了一种分类法，以指导对解决令牌级生成已知局限性的模型的研究，并开发新变革性的自然语言处理模型。', 'title_zh': '文本生成中替代下一个词预测的方法——一个综述'}
{'arxiv_id': 'arXiv:2509.24405', 'title': 'Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents', 'authors': 'Khanh Trinh Pham, Thu Huong Nguyen, Jun Jo, Quoc Viet Hung Nguyen, Thanh Tam Nguyen', 'link': 'https://arxiv.org/abs/2509.24405', 'abstract': "Text-to-SQL enables natural access to databases, yet most benchmarks are English-only, limiting multilingual progress. We introduce MultiSpider 2.0, extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's structural difficulty while adding linguistic and dialectal variability, demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\\% execution accuracy when relying on intrinsic reasoning, versus 60\\% on MultiSpider 1.0. Therefore, we provide a collaboration-driven language agents baseline that iteratively refines queries, improving accuracy to 15\\%. These results reveal a substantial multilingual gap and motivate methods that are robust across languages and ready for real-world enterprise deployment. Our benchmark is available at this https URL.", 'abstract_zh': 'Text-to-SQL跨多语言的挑战与进展：从Multilingual Spider 2.0探索自然语言到SQL的转换能力', 'title_zh': '多语言文本到SQL转换：协作语言代理评估语言模型的极限'}
{'arxiv_id': 'arXiv:2509.24389', 'title': 'LLaDA-MoE: A Sparse MoE Diffusion Language Model', 'authors': 'Fengqi Zhu, Zebin You, Yipeng Xing, Zenan Huang, Lin Liu, Yihong Zhuang, Guoshan Lu, Kangyu Wang, Xudong Wang, Lanning Wei, Hongrui Guo, Jiaqi Hu, Wentao Ye, Tieyuan Chen, Chenchen Li, Chengfu Tang, Haibo Feng, Jun Hu, Jun Zhou, Xiaolu Zhang, Zhenzhong Lan, Junbo Zhao, Da Zheng, Chongxuan Li, Jianguo Li, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2509.24389', 'abstract': "We introduce LLaDA-MoE, a large language diffusion model with the Mixture-of-Experts (MoE) architecture, trained from scratch on approximately 20T tokens. LLaDA-MoE achieves competitive performance with significantly reduced computational overhead by maintaining a 7B-parameter capacity while activating only 1.4B parameters during inference. Our empirical evaluation reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion language models with larger parameters, surpassing previous diffusion language models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation, mathematical reasoning, agent and alignment tasks, despite using fewer active parameters. Our results show that integrating a sparse MoE architecture into the training objective of masked diffusion language models still brings out MoE's strengths under efficient inference with few active parameters, and opens ample room for further exploration of diffusion language models. LLaDA-MoE models are available at Huggingface.", 'abstract_zh': 'LLaDA-MoE：一种基于Mixture-of-Experts架构的大规模语言扩散模型及其性能优越性', 'title_zh': 'LLaDA-MoE: 一种稀疏的MoE扩散语言模型'}
{'arxiv_id': 'arXiv:2509.24385', 'title': 'Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy', 'authors': 'Haijier Chen, Bo Xu, Shoujian Zhang, Haoze Liu, Jiaxuan Lin, Jingrong Wang', 'link': 'https://arxiv.org/abs/2509.24385', 'abstract': 'Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision-Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often depend on 3D data inputs, which limits scalability and generalization. To address this limitation, we propose Vid-LLM, a video-based 3D-MLLM that directly processes video inputs without requiring external 3D data, making it practical for real-world deployment. In our method, the geometric prior are directly used to improve the performance of the sceen perception. To integrate the geometric cues into the MLLM compactly, we design a Cross-Task Adapter (CTA) module to align the 3D geometric priors with the vision-language representations. To ensure geometric consistency and integrity, we introduce a Metric Depth Model that recovers real-scale geometry from the reconstruction outputs. Finally, the model is fine-tuned with a two-stage distillation optimization strategy, realizing fast convergence and stabilizes training. Extensive experiments across diverse benchmarks verified the effectiveness of our method on 3D Question Answering, 3D Dense Captioning and 3D Visual Grounding tasks, demonstrating the superior multi-task capabilities.', 'abstract_zh': '近年来，多模态大型语言模型（MLLMs）在二维领域中的视觉-语言（VL）推理方面取得了显著进步。然而，将这些能力扩展到三维场景理解仍然是一个重大挑战。现有的三维多模态大型语言模型（3D-MLLMs）通常依赖于三维数据输入，这限制了其可扩展性和泛化能力。为了解决这一限制，我们提出了一种基于视频的3D-MLLM——Vid-LLM，它可以直接处理视频输入，无需外部三维数据，使其更适合实际部署。在我们的方法中，直接使用几何先验以提高场景感知性能。为了紧凑地将几何线索集成到MLLM中，我们设计了一个跨任务适配器（CTA）模块，以对齐三维几何先验与视觉语言表示。为了确保几何一致性和完整性，我们引入了一个度量深度模型，可以从重建输出中恢复真实尺度几何。最后，通过两阶段蒸馏优化策略对模型进行微调，实现了快速收敛并稳定训练。广泛的实验证明了该方法在三维问答、三维密集标注和三维视觉接地任务中的有效性，展示了其出色的多任务能力。', 'title_zh': 'Vid-LLM：基于视频的紧凑型3D多模态LLM及其重建-推理协同模型'}
{'arxiv_id': 'arXiv:2509.24384', 'title': 'HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment', 'authors': 'Langqi Yang, Tianhang Zheng, Kedong Xiu, Yixuan Chen, Di Wang, Puning Zhao, Zhan Qin, Kui Ren', 'link': 'https://arxiv.org/abs/2509.24384', 'abstract': "The alignment of large language models (LLMs) with human values is critical for their safe deployment, yet jailbreak attacks can subvert this alignment to elicit harmful outputs from LLMs. In recent years, a proliferation of jailbreak attacks has emerged, accompanied by diverse metrics and judges to assess the harmfulness of the LLM outputs. However, the absence of a systematic benchmark to assess the quality and effectiveness of these metrics and judges undermines the credibility of the reported jailbreak effectiveness and other risks. To address this gap, we introduce HarmMetric Eval, a comprehensive benchmark designed to support both overall and fine-grained evaluation of harmfulness metrics and judges. Our benchmark includes a high-quality dataset of representative harmful prompts paired with diverse harmful and non-harmful model responses, alongside a flexible scoring mechanism compatible with various metrics and judges. With HarmMetric Eval, our extensive experiments uncover a surprising result: two conventional metrics--METEOR and ROUGE-1--outperform LLM-based judges in evaluating the harmfulness of model responses, challenging prevailing beliefs about LLMs' superiority in this domain. Our dataset is publicly available at this https URL, and the code is available at this https URL.", 'abstract_zh': '大型语言模型与人类价值的对齐对于其安全部署至关重要，但劫持攻击可以破坏这种对齐，从而引发有害输出。近年来，劫持攻击层出不穷，伴随有不同的评估有害输出标准的方法和评判标准。然而，缺乏对这些方法和评判标准质量与效果的系统性评估基准，削弱了所报告的劫持攻击效果及其他风险的可信度。为解决这一问题，我们引入了HarmMetric Eval，这是一个全面的基准，旨在支持有害性评估标准的整体和细粒度评估。我们的基准包括高质量的代表性有害提示数据集，配对有不同有害和非有害模型响应，以及兼容各种评估方法和评判标准的灵活评分机制。通过HarmMetric Eval，我们广泛实验得出了一个意外的结果：两种传统的评估方法——METEOR和ROUGE-1——在评估模型响应的有害性方面优于基于大型语言模型的评判标准，这挑战了LLM在这一领域优越性的观点。我们的数据集可在此网址获取：这个 https URL，代码可在此网址获取：这个 https URL。', 'title_zh': 'HarmMetric Eval: 评估用于大语言模型有害性评估的度量标准和评判标准'}
{'arxiv_id': 'arXiv:2509.24372', 'title': 'Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning', 'authors': 'Xin Qiu, Yulu Gan, Conor F. Hayes, Qiyao Liang, Elliot Meyerson, Babak Hodjat, Risto Miikkulainen', 'link': 'https://arxiv.org/abs/2509.24372', 'abstract': 'Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: this https URL.', 'abstract_zh': '预训练大型语言模型（LLMs）的微调是AI部署管道中的一个关键步骤。强化学习（RL）无疑是最重要的微调方法，贡献了许多最先进的LLMs。相比之下，进化策略（ES），曾显示在具有数百万参数的模型上与RL相当的性能，但由于对其可扩展性的悲观看法而被忽视。在本工作中，我们报告了首次成功将ES扩展用于微调整个参数的LLMs，展示了令人惊讶的事实，即ES可以高效地在数十亿参数上进行搜索，并在多个方面优于现有的RL微调方法，包括样本效率、对长时间奖励的容忍度、对不同基底LLMs的 robust 性、较少倾向于奖励作弊以及在多次运行中的更稳定性能。因此，它为LLM微调开辟了一条新的方向，超越了当前的RL技术所能提供的。代码已开源：this https URL。', 'title_zh': '大规模进化策略：超越强化学习的LLM微调'}
{'arxiv_id': 'arXiv:2509.24368', 'title': 'Watermarking Diffusion Language Models', 'authors': 'Thibaud Gloaguen, Robin Staab, Nikola Jovanović, Martin Vechev', 'link': 'https://arxiv.org/abs/2509.24368', 'abstract': 'We introduce the first watermark tailored for diffusion language models (DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in contrast to standard autoregressive language models (ARLMs) which generate tokens sequentially. While there has been much work in ARLM watermarking, a key challenge when attempting to apply these schemes directly to the DLM setting is that they rely on previously generated tokens, which are not always available with DLM generation. In this work we address this challenge by: (i) applying the watermark in expectation over the context even when some context tokens are yet to be determined, and (ii) promoting tokens which increase the watermark strength when used as context for other tokens. This is accomplished while keeping the watermark detector unchanged. Our experimental evaluation demonstrates that the DLM watermark leads to a >99% true positive rate with minimal quality impact and achieves similar robustness to existing ARLM watermarks, enabling for the first time reliable DLM watermarking.', 'abstract_zh': '我们介绍了首个针对扩散语言模型(DLM)的水印，这是一种新兴的LLM范式，能够以任意顺序生成标记，而传统的自回归语言模型(ARLM)则按顺序生成标记。尽管对ARLM进行了大量水印研究，但在直接应用于DLM设置时，这些方案依赖于已生成的标记，而这些标记在DLM生成过程中未必可用。本文通过以下方式解决了这一挑战：(i) 即使某些上下文标记尚未确定，也在上下文期望值上应用水印；(ii) 促进增加其他标记水印强度的标记。我们保持水印检测器不变，实现了这一目标。我们的实验评估表明，该DLM水印在对质量影响极小的情况下，达到了超过99%的真实正率，并且具有与现有ARLM水印相似的鲁棒性，从而首次实现了可靠的DLM水印技术。', 'title_zh': '扩散语言模型中的水印技术'}
{'arxiv_id': 'arXiv:2509.24361', 'title': 'UI-UG: A Unified MLLM for UI Understanding and Generation', 'authors': 'Hao Yang, Weijie Qiu, Ru Zhang, Zhou Fang, Ruichao Mao, Xiaoyu Lin, Maji Huang, Zhaosong Huang, Teng Guo, Shuoyang Liu, Hai Rao', 'link': 'https://arxiv.org/abs/2509.24361', 'abstract': 'Although Multimodal Large Language Models (MLLMs) have been widely applied across domains, they are still facing challenges in domain-specific tasks, such as User Interface (UI) understanding accuracy and UI generation quality. In this paper, we introduce UI-UG (a unified MLLM for UI Understanding and Generation), integrating both capabilities. For understanding tasks, we employ Supervised Fine-tuning (SFT) combined with Group Relative Policy Optimization (GRPO) to enhance fine-grained understanding on the modern complex UI data. For generation tasks, we further use Direct Preference Optimization (DPO) to make our model generate human-preferred UIs. In addition, we propose an industrially effective workflow, including the design of an LLM-friendly domain-specific language (DSL), training strategies, rendering processes, and evaluation metrics. In experiments, our model achieves state-of-the-art (SOTA) performance on understanding tasks, outperforming both larger general-purpose MLLMs and similarly-sized UI-specialized models. Our model is also on par with these larger MLLMs in UI generation performance at a fraction of the computational cost. We also demonstrate that integrating understanding and generation tasks can improve accuracy and quality for both tasks.', 'abstract_zh': '尽管多模态大型语言模型（MLLMs）已在多个领域广泛应用，但在特定任务如用户界面（UI）理解准确性和UI生成质量方面仍面临挑战。本文介绍UI-UG（一种统一的MLLM，用于UI理解和生成），集成了这两种能力。在理解任务中，我们采用监督微调（SFT）结合组相对策略优化（GRPO）来增强对现代复杂UI数据的细粒度理解。在生成任务中，我们进一步使用直接偏好优化（DPO）使模型生成人类偏好UI。此外，我们提出了一种工业上有效的流程，包括为LLM设计专用领域语言（DSL）、训练策略、渲染流程和评估指标。实验结果表明，我们的模型在理解任务中达到了迄今为止最好的性能，优于更大的通用MLLM和同等规模的UI专用模型。而且，在计算成本更低的情况下，我们的模型在UI生成方面也与这些较大的MLLM相当。我们还证明，将理解和生成任务结合起来可以提高两个任务的准确性和质量。', 'title_zh': 'UI-UG：统一的联合学习模型用于界面理解与生成'}
{'arxiv_id': 'arXiv:2509.24356', 'title': 'Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining', 'authors': 'Matthew Theodore Roque, Dan John Velasco', 'link': 'https://arxiv.org/abs/2509.24356', 'abstract': "Most studies on language model pretraining focus on large datasets, leaving open questions about optimization in data-constrained settings. In such settings, the effects of training data order and of including alternative versions of the same text remain underexplored. We address this by studying curriculum learning in pretraining, focusing on text-complexity ordering and data augmentation via simplification. We ask: (1) Does simplifying texts enhance representation quality more than reusing the original data? and (2) Does ordering data by text complexity yield better representations? To answer, we build on a pair of parallel corpora where human-written paragraphs are aligned with LLM-simplified variants, and test four data schedules: repeated exposure, low-to-high complexity, high-to-low, and interleaved. We analyze models' representation quality from a sample efficiency perspective via fine-tuning, as well as its zero-shot performance on linguistic knowledge, entity tracking, world knowledge, and commonsense reasoning. Our findings show that adding simplified data improves fine-tuning and zero-shot performance over a repeated-exposure baseline: smaller models benefit from low-to-high complexity, while larger models perform better with interleaved ordering.", 'abstract_zh': '大多数语言模型预训练研究集中在大规模数据集上，而在数据受限的设置中优化方面的疑问仍然存在。在这种设置中，训练数据顺序和包含相同文本的替代版本的影响仍缺乏探索。我们通过研究预训练中的逐级学习来解决这一问题，重点关注文本复杂度排序和通过简化进行的数据增强。我们提出的问题是：（1）简化文本是否比重复使用原始数据更能提升表示质量？（2）按照文本复杂度排序数据是否能得到更好的表示？为回答这些问题，我们在一组平行语料库上进行研究，其中人工撰写的段落与LLM简化的变体对齐，并测试了四种数据调度方式：重复暴露、低到高复杂度、高到低和交错。我们从样本效率的角度通过微调分析模型的表示质量，并考察其在语言知识、实体跟踪、世界知识和常识推理等任务上的零样本性能。我们的研究发现，增加简化数据比重复暴露基线更能提高微调和零样本性能：小型模型受益于低到高复杂度，而大型模型在交错排序下性能更佳。', 'title_zh': '超越重复：基于数据受限预训练的文本简化和课程学习'}
{'arxiv_id': 'arXiv:2509.24319', 'title': 'Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs', 'authors': 'Jongwook Han, Jongwon Lim, Injin Kong, Yohan Jo', 'link': 'https://arxiv.org/abs/2509.24319', 'abstract': "Large language models (LLMs) can express different values in two distinct ways: (1) intrinsic expression, reflecting the model's inherent values learned during training, and (2) prompted expression, elicited by explicit prompts. Given their widespread use in value alignment and persona steering, it is paramount to clearly understand their underlying mechanisms, particularly whether they mostly overlap (as one might expect) or rely on substantially different mechanisms, but this remains largely understudied. We analyze this at the mechanistic level using two approaches: (1) value vectors, feature directions representing value mechanisms extracted from the residual stream, and (2) value neurons, MLP neurons that contribute to value expressions. We demonstrate that intrinsic and prompted value mechanisms partly share common components that are crucial for inducing value expression, but also possess unique elements that manifest in different ways. As a result, these mechanisms lead to different degrees of value steerability (prompted > intrinsic) and response diversity (intrinsic > prompted). In particular, components unique to the intrinsic mechanism seem to promote lexical diversity in responses, whereas those specific to the prompted mechanism primarily strengthen instruction following, taking effect even in distant tasks like jailbreaking.", 'abstract_zh': '大型语言模型（LLMs）可以通过两种不同的方式表达不同的价值观：（1）内在表达，反映模型在训练过程中学到的固有价值观，和（2）提示表达，由明确的提示引发。鉴于它们在价值观对齐和角色引导中的广泛应用，深刻理解其背后的机制至关重要，特别是它们是否主要重叠（如人们预期的那样）或者依赖于显著不同的机制，但这一点仍然研究不足。我们从机制层面采用两种方法进行分析：（1）价值向量，从残差流中提取的价值机制的特征方向，和（2）价值神经元，为价值表达做出贡献的MLP神经元。我们证明，内在和提示的价值机制部分共享对于诱发价值表达至关重要的共同组件，但也具有以不同方式表现的独特元素。这导致了不同的价值观可控性程度（提示作用 > 内在作用）和响应多样性程度（内在作用 > 提示作用）。特别是，内在机制独有的组件似乎促进了响应的词汇多样性，而提示机制特有的组件主要增强了指令跟随的能力，即使在像监狱突破这样远离的任务中也能生效。', 'title_zh': '双重机制的价值表达：LLMs中内在价值与激发价值的双重机制'}
{'arxiv_id': 'arXiv:2509.24298', 'title': "Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports", 'authors': 'Changde Du, Yizhuo Lu, Zhongyu Huang, Yi Sun, Zisen Zhou, Shaozheng Qin, Huiguang He', 'link': 'https://arxiv.org/abs/2509.24298', 'abstract': "The ability to represent emotion plays a significant role in human cognition and social interaction, yet the high-dimensional geometry of this affective space and its neural underpinnings remain debated. A key challenge, the `behavior-neural gap,' is the limited ability of human self-reports to predict brain activity. Here we test the hypothesis that this gap arises from the constraints of traditional rating scales and that large-scale similarity judgments can more faithfully capture the brain's affective geometry. Using AI models as `cognitive agents,' we collected millions of triplet odd-one-out judgments from a multimodal large language model (MLLM) and a language-only model (LLM) in response to 2,180 emotionally evocative videos. We found that the emergent 30-dimensional embeddings from these models are highly interpretable and organize emotion primarily along categorical lines, yet in a blended fashion that incorporates dimensional properties. Most remarkably, the MLLM's representation predicted neural activity in human emotion-processing networks with the highest accuracy, outperforming not only the LLM but also, counterintuitively, representations derived directly from human behavioral ratings. This result supports our primary hypothesis and suggests that sensory grounding--learning from rich visual data--is critical for developing a truly neurally-aligned conceptual framework for emotion. Our findings provide compelling evidence that MLLMs can autonomously develop rich, neurally-aligned affective representations, offering a powerful paradigm to bridge the gap between subjective experience and its neural substrates. Project page: this https URL.", 'abstract_zh': '情感表示能力在人类认知和社会互动中扮演着重要角色，但这一情感空间的高维几何结构及其神经基础仍存在争议。一个关键挑战是“行为-神经差距”，即人类自陈报告预测脑活动的能力有限。我们测试了这一差距源于传统评分量表的限制，并认为大规模相似性判断更能忠实地捕捉大脑的情感几何结构。通过使用AI模型作为“认知代理”，我们从一个多模态大型语言模型（MLLM）和一个仅基于语言的模型（LLM）收集了对2,180个具有情感触发性的视频作出的上百万组三元组奇数项判断。我们发现，这些模型产生的30维嵌入高度可解释，并主要按类别方式组织情感，但又融合了维度属性。最令人惊讶的是，MLLM的情感表示准确预示了人类情感处理网络的神经活动，不仅优于LLM，甚至也优于直接来源于人类行为评分的情感表示。这一结果支持了我们的主要假设，并表明从丰富的视觉数据中学习对于开发真正神经对齐的概念框架至关重要。我们的发现提供了有力证据，证明MLLM能够自主发展丰富且神经对齐的情感表示，为跨越主观体验及其神经基础之间的鸿沟提供了强大的范式。', 'title_zh': '弥合行为-神经差距：多模态AI比人类自我报告更准确揭示情感的大脑几何结构'}
{'arxiv_id': 'arXiv:2509.24296', 'title': 'DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models', 'authors': 'Zherui Li, Zheng Nie, Zhenhong Zhou, Yufei Guo, Yue Liu, Yitong Zhang, Yu Cheng, Qingsong Wen, Kun Wang, Jiaheng Zhang', 'link': 'https://arxiv.org/abs/2509.24296', 'abstract': "The rapid advancement of Diffusion Large Language Models (dLLMs) introduces unprecedented vulnerabilities that are fundamentally distinct from Autoregressive LLMs, stemming from their iterative and parallel generation mechanisms. In this paper, we conduct an in-depth analysis of dLLM vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step and inter-step dynamics. Experimental results reveal a harmful bias inherent in the standard greedy remasking strategy and identify a critical phenomenon we term Denoising-path Dependence, where the safety of early-stage tokens decisively influences the final output. These findings also indicate that while current decoding strategies constitute a significant vulnerability, dLLMs possess a substantial intrinsic safety potential. To unlock this potential, we propose DiffuGuard, a training-free defense framework that addresses vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking dynamically introduces controlled randomness to mitigate greedy selection bias, while Block-level Audit and Repair exploits internal model representations for autonomous risk detection and guided correction. Comprehensive experiments on four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while preserving model utility and efficiency. Our code is available at: this https URL.", 'abstract_zh': '扩散大规模语言模型的快速进展引入了与自回归大规模语言模型本质上不同的前所未有的脆弱性，源于它们的迭代和并行生成机制。在本文中，我们从两个不同的维度——单步内部动态和跨步动态——深入分析了扩散大规模语言模型对玩砸攻击的脆弱性。实验结果揭示了标准贪婪重新遮掩策略固有的有害偏差，并确定了一种我们称为去噪路径依赖的关键现象，早期阶段令牌的安全性对最终输出有决定性影响。这些发现也表明，虽然当前的解码策略构成了重大脆弱性，但扩散大规模语言模型仍然具有巨大的内在安全性潜力。为了解锁这一潜力，我们提出了DiffuGuard，这是一个无需训练的防御框架，采用双重方法来应对脆弱性：随机退火重新遮掩动态引入受控的随机性以减轻贪婪选择偏差，而块级审核与修复则利用内部模型表示进行自主风险检测和指导修正。在四个扩散大规模语言模型上的综合实验表明，DiffuGuard表现出色，将六种不同玩砸方法的攻击成功率从47.9%降至14.7%，同时保持模型的实用性和效率。源代码可在以下链接获取：this https URL。', 'title_zh': 'DiffuGuard: 从自洽安全性丧失到扩散大语言模型的安全恢复'}
{'arxiv_id': 'arXiv:2509.24291', 'title': 'Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement', 'authors': 'Yu-Che Tsai, Kuan-Yu Chen, Yuan-Chi Li, Yuan-Hao Chen, Ching-Yu Tsai, Shou-De Lin', 'link': 'https://arxiv.org/abs/2509.24291', 'abstract': 'Existing large language model (LLM)-based embeddings typically adopt an encoder-only paradigm, treating LLMs as static feature extractors and overlooking their core generative strengths. We introduce GIRCSE (Generative Iterative Refinement for Contrastive Sentence Embeddings), a novel framework that leverages autoregressive generation to iteratively refine semantic representations. By producing sequences of soft tokens optimized under contrastive objective, GIRCSE captures latent concepts and implicit semantics that encoder-only methods often miss. To guide this process, we propose an Iterative Contrastive Refinement (ICR) objective that encourages each refinement step to yield better representations. Extensive experiments show that GIRCSE outperforms strong LLM-based embedding baselines on the MTEB benchmark and instruction-following tasks. Moreover, GIRCSE exhibits an emergent test-time scaling property: generating more tokens at inference steadily improves embedding quality. Our results establish generative iterative refinement as a new paradigm for representation learning.', 'abstract_zh': '现有的大型语言模型（LLM）嵌入通常采用编码器唯一切expenses，将LLM视为静态特征提取器并忽视其核心生成优势。我们提出了GIRCSE（生成迭代细化对contrastive句子嵌入），这是一种新型框架，利用自回归生成来迭代细化语义表示。通过在对比目标下优化软标记序列，GIRCSE 捕捉到编码器唯一切方法经常忽略的潜在概念和隐式语义。为指导这一过程，我们提出了迭代对比细化（ICR）目标，鼓励每一细化步骤都产生更好的表示。大量实验表明，GIRCSE 在 MTEB 基准和指令遵循任务中优于强 LLM 基础嵌入。此外，GIRCSE 展现出一种新兴的测试时缩放性质：推理时生成更多标记会逐步提高嵌入质量。我们的结果确立了生成迭代细化作为新的表示学习范式。', 'title_zh': '让大模型开口说话：通过迭代对比精炼生成文本嵌入'}
{'arxiv_id': 'arXiv:2509.24282', 'title': 'SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents', 'authors': 'Gyuhyeon Seo, Jungwoo Yang, Junseong Pyo, Nalim Kim, Jonggeun Lee, Yohan Jo', 'link': 'https://arxiv.org/abs/2509.24282', 'abstract': 'Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks. However, smart homes introduce distinct challenges, requiring agents to handle latent user intents, temporal dependencies, device constraints, scheduling, and more. The main bottlenecks for developing smart home agents with such capabilities include the lack of a realistic simulation environment where agents can interact with devices and observe the results, as well as a challenging benchmark to evaluate them. To address this, we introduce $\\textbf{SimuHome}$, a time-accelerated home environment that simulates smart devices, supports API calls, and reflects changes in environmental variables. By building the simulator on the Matter protocol (the global industry standard for smart home communication), SimuHome provides a high-fidelity environment, and agents validated in SimuHome can be deployed on real Matter-compliant devices with minimal adaptation. We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities. Our evaluation of 11 agents under a unified ReAct framework reveals that while models perform well on simple tasks, they struggle with latent intent inference, state verification, and especially temporal scheduling. Even the top-performing model, GPT-4.1, reaches only 54% success rate. These findings highlight a critical need for methods that can reliably verify the current state via tools before acting and coordinate time-dependent actions.', 'abstract_zh': 'Large Language Model (LLM) 剂剂在多步、工具增强任务中表现出色。然而，智能家居带来了独特的挑战，要求代理处理隐含用户意图、时间依赖性、设备约束、排程等问题。开发具有此类能力的智能家居代理的主要瓶颈包括缺乏一个真实的模拟环境，代理可以在其中与设备交互并观察结果，以及缺乏一个具有挑战性的基准来评估它们。为了解决这一问题，我们引入了**SimuHome**，这是一种时间加速的家庭环境，可以模拟智能设备、支持API调用，并反映环境变量的变化。通过在全球智能家庭通信标准Matter协议的基础上构建模拟器，SimuHome提供了一个高度真实的环境，经SimuHome验证的代理可以最小限度地适应并部署在真正的Matter合规设备上。我们提供了涵盖十二种用户查询类型的600个场景的具有挑战性的基准测试，这些查询需要前述的能力。在统一的ReAct框架下评估11个代理模型显示，虽然模型在简单任务上表现良好，但在隐含意图推断、状态验证以及特别是时间调度方面存在困难。即使是表现最佳的模型GPT-4.1，成功率也只有54%。这些发现强调了方法的重要性，这些方法需要能够通过工具可靠地验证当前状态并在采取行动之前进行协调，以及对时间依赖性动作进行协调。', 'title_zh': 'SimuHome：一种考虑时间与环境因素的智能家居LLM代理基准测试'}
{'arxiv_id': 'arXiv:2509.24245', 'title': 'Prompt and Parameter Co-Optimization for Large Language Models', 'authors': 'Xiaohe Bo, Rui Li, Zexu Sun, Quanyu Dai, Zeyu Zhang, Zihang Tian, Xu Chen, Zhenhua Dong', 'link': 'https://arxiv.org/abs/2509.24245', 'abstract': 'Prompt optimization and fine-tuning are two major approaches to improve the performance of Large Language Models (LLMs). They enhance the capabilities of LLMs from complementary perspectives: the former through explicit natural language, and the latter through implicit parameter updates. However, prior work has typically studied them in isolation, leaving their synergistic potential largely underexplored. To bridge this gap, in this paper, we introduce MetaTuner, a novel framework that jointly integrates prompt optimization and fine-tuning for LLM training. Specifically, we introduce two neural networks to generate prompts and parameters, respectively, while allowing them to share a common bottom encoding layer to enable knowledge sharing. By the guidance of the final supervised signals, our framework is optimized to discover the optimal combinations between the prompts and parameters. Given that prompt learning involves discrete optimization while fine-tuning operates in a continuous parameter space, we design a supervised regularization loss to train our framework effectively. Extensive experiments across diverse benchmarks show that our method consistently outperforms the baselines.', 'abstract_zh': 'Prompt优化和微调是提升大规模语言模型（LLMs）性能的两种主要方法。它们从互补的角度增强LLMs的能力：前者通过显式的自然语言，后者通过隐式的参数更新。然而，先前的工作通常将它们分开研究，导致它们的协同潜力未被充分探索。为弥补这一差距，本文提出了一种新型框架MetaTuner，它可以同时整合prompt优化和微调。具体来说，我们引入了两个神经网络分别生成prompt和参数，同时允许它们共享一个底层编码层以实现知识共享。在最终监督信号的引导下，该框架优化以发现prompt和参数的最佳组合。鉴于prompt学习涉及到离散优化而微调操作在连续参数空间中，我们设计了一个监督正则化损失来有效训练该框架。广泛的实验表明，我们的方法在多个基准测试中持续优于基线方法。', 'title_zh': '大型语言模型的提示与参数共优化'}
{'arxiv_id': 'arXiv:2509.24239', 'title': 'ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models', 'authors': 'Jincheng Liu, Sijun He, Jingjing Wu, Xiangsen Wang, Yang Chen, Zhaoqi Kuang, Siqi Bao, Yuan Yao', 'link': 'https://arxiv.org/abs/2509.24239', 'abstract': 'Recent large language models (LLMs) have shown strong reasoning capabilities. However, a critical question remains: do these models possess genuine reasoning skills particularly complex strategic reasoning or are they primarily excelling at sophisticated pattern recognition within their training data? To address this question, this paper presents a chess testbed, ChessArena, to evaluate the strategic reasoning capabilities of LLMs. Chess requires complex strategic reasoning capabilities including long-term planning, strict rule comprehension, and multi-turn conversation memorization. Specifically, ChessArena is a competitive framework where LLMs play against each other, under four different play modes. The testbed is equipped with a ranking algorithm and a leaderboard. The testbed can also evaluate fine-grained capabilities including basic understanding, move selection, and puzzle solving. Over 13 LLMs with different modes are evaluated in ChessArena, playing over 800 games. The results reveal significant shortcomings in current LLMs: no model can beat Maia-1100 (a chess engine at human amateur level), while some even failed to defeat a random player that selects moves arbitrarily. We also present a strong baseline to the testbed: our fine-tuned Qwen3-8B substantially improved performance, approaching much larger state-of-the-art reasoning models.', 'abstract_zh': 'Recent大型语言模型（LLMs）展示了强大的推理能力。然而，一个关键问题是：这些模型是否具有真正的推理技能，特别是复杂的策略推理能力，还是主要在训练数据中的高级模式识别方面表现出色？为了解决这一问题，本文提出了一个国际象棋测试平台ChessArena，以评估LLMs的策略推理能力。国际象棋要求复杂的策略推理能力，包括长期规划、严格规则理解以及多轮对话记忆。具体来说，ChessArena是一个竞争性框架，其中LLMs在四种不同的对弈模式下相互对弈。该测试平台配备了一种排名算法和排行榜。测试平台还可以评估细粒度的能力，包括基本理解、落子选择和解谜能力。在ChessArena中，13种不同模式的大型语言模型进行了超过800场比赛。结果揭示了当前大型语言模型的重大缺陷：没有一个模型能够战胜Maia-1100（一个达到人类业余水平的国际象棋引擎），而有些模型甚至无法击败一个任意选择走法的随机玩家。我们还为该测试平台提供了一个强大的基准：我们微调的Qwen3-8B显著提高了性能，接近当前最先进的推理模型。', 'title_zh': 'ChessArena：评估大型语言模型战略推理能力的象棋测试平台'}
{'arxiv_id': 'arXiv:2509.24218', 'title': 'Conda: Column-Normalized Adam for Training Large Language Models Faster', 'authors': 'Junjie Wang, Pan Zhou, Yiming Dong, Huan Li, Jia Li, Xun Zhou, Qicheng Lao, Cong Fang, Zhouchen Lin', 'link': 'https://arxiv.org/abs/2509.24218', 'abstract': 'Large language models (LLMs) have demonstrated impressive generalization and emergent capabilities, yet their pre-training remains computationally expensive and sensitive to optimization dynamics. While Adam-based optimizers offer fast convergence by adapting learning rates coordinate-wise, recent studies reveal that their updates often suffer from poor spectral conditioning and low-rank structures, hindering efficiency. Muon addresses this issue via global spectral normalization but lacks the per-coordinate adaptivity of Adam. In this work, we propose \\textbf{Column-Normalized Adam (Conda)}, a novel optimizer that bridges the strengths of both approaches. Conda projects updates into an orthogonal subspace and applies column-wise second moment normalization based on the projected gradients, thereby achieving both improved spectral conditioning and maintaining coordinate-wise adaptivity. This design alleviates the spectral pathologies of Adam while preserving its fast convergence behavior. Extensive experiments on the LLaMA and GPT-2 series show that Conda consistently outperforms AdamW, Muon, and other baselines in pre-training. Remarkably, on the LLaMA series, \\textbf{Conda achieves $2{\\sim}2.5\\times$ the convergence speed of AdamW, measured in both training steps and training time.} Further ablations demonstrate its robustness under diverse training setups. These results collectively highlight Conda as an effective and broadly applicable optimizer for large-scale LLM training. The code is released on this https URL', 'abstract_zh': '大规模语言模型（LLMs）展示了令人印象深刻的泛化能力和涌现能力，但其预训练仍具有较高的计算成本，并且对优化动态敏感。尽管基于Adam的优化器通过协调调整学习率实现了快速收敛，但 recent 研究表明，它们的更新往往遭受谱条件差和低秩结构的困扰，影响效率。Muon 通过全局谱正则化解决了这一问题，但缺乏 Adam 的坐标适配性。在这项工作中，我们提出了一种新的优化器——列归一化 Adam（Conda），它结合了两种方法的优势。Conda 将更新投影到正交子空间，并基于投影梯度应用列-wise 的二阶矩归一化，从而实现改进的谱条件并保持坐标适配性。这一设计缓解了 Adam 的谱病理现象，同时保留了其快速收敛的行为。在 LLaMA 和 GPT-2 系列的广泛实验中，Conda 一贯优于 AdamW、Muon 及其他基线。特别是在 LLaMA 系列中，Conda 在训练步数和训练时间上将 AdamW 的收敛速度提高了 2 到 2.5 倍。进一步的消融实验证明了其在不同训练设置下的鲁棒性。这些结果共同展示了 Conda 是大规模 LLM 训练中一个有效且广泛应用的优化器。代码发布在 <https://>。', 'title_zh': 'Conda: 列归一化Adam算法用于训练大型语言模型加速'}
{'arxiv_id': 'arXiv:2509.24210', 'title': 'BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models', 'authors': 'Gaurav Srivastava, Aafiya Hussain, Zhenyu Bi, Swastik Roy, Priya Pitre, Meng Lu, Morteza Ziyadi, Xuan Wang', 'link': 'https://arxiv.org/abs/2509.24210', 'abstract': 'Evaluating language models fairly is becoming harder as static benchmarks available on the internet risk contamination by training data. This makes it unclear whether models are truly reasoning or just recalling answers. In this paper, we introduce BeyondBench, an evaluation framework that avoids this problem by using algorithmic problem generation. Unlike traditional benchmarks that risk contamination from internet-scale training data, BeyondBench creates mathematically grounded problems on the fly, ensuring each test remains fresh and uncontaminated. Our framework covers 44 algorithmic tasks with a total of 117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks) for basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations) for sequence patterns and reasoning, and the Hard Suite (10 tasks, 68 variations) tackling NP-complete and constraint satisfaction problems. Each task generates problems from a combinatorial space larger than 10^15 unique instances, with solutions verified deterministically by mathematical proofs. We evaluated 101 language models, including 85 open-source and 16 closed-source models, spanning sizes from 0.5B to 141B parameters and multiple quantization schemes. Our results show consistent reasoning deficiencies across model families, with performance degrading sharply as problem complexity increases from polynomial to exponential. In our Hard Suite evaluations, models such as Gemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of 56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance drops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano showing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our leaderboard is publicly available at this https URL', 'abstract_zh': '公平评估语言模型变得越来越困难，因为互联网上可用的静态基准可能会受到训练数据的污染。这使得很难确定模型是真正推理还是仅仅是回忆答案。在这种情况下，我们提出了BeyondBench，这是一种通过使用算法问题生成来避免这个问题的评估框架。与传统基准可能受到互联网规模训练数据污染的情况不同，BeyondBench 实时生成数学上基于ground的问题，确保每个测试始终保持新鲜和未被污染。我们的框架涵盖了44项算法任务，共计117种变体，并分为三个难度级别：Easy Suite（29项任务）涵盖基本的算术和统计，Medium Suite（5项任务，49种变体）涵盖序列模式和推理，Hard Suite（10项任务，68种变体）解决NP完全和约束满足问题。每个任务生成的问题实例空间超过 \\(10^{15}\\) 种独特实例，通过数学证明确定性地验证解决方案。我们评估了101个语言模型，包括85个开源和16个闭源模型，涵盖从0.5B到141B参数以及多种量化方案。结果显示，随着问题复杂性的增加，从多项式到指数级别，模型家族的一致推理能力存在缺陷。在我们的Hard Suite评估中，Gemini-2.5-pro、Llama-3.3-70B和Qwen2.5-72B的平均准确率分别为56.38%、26.91%和33.60%。此外，我们观察到，在不使用工具的情况下，性能大幅下降，GPT-5、GPT-5-mini和GPT-5-nano在Hard Suite中的准确率分别下降了16.81%、28.05%和47.59%。我们的排行榜可以在以下网址查看。', 'title_zh': 'BeyondBench: 无需基准的语义模型推理评估'}
{'arxiv_id': 'arXiv:2509.24203', 'title': 'Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends', 'authors': 'Chaorui Yao, Yanxi Chen, Yuchang Sun, Yushuo Chen, Wenhao Zhang, Xuchen Pan, Yaliang Li, Bolin Ding', 'link': 'https://arxiv.org/abs/2509.24203', 'abstract': 'Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at this https URL.', 'abstract_zh': '大规模语言模型（LLMs）的离策强化学习（RL）正 attracting growing interest', 'title_zh': 'Group-相对REINFORCE实际上是Off-Policy算法：揭开GRPO及其伙伴的一些迷思'}
{'arxiv_id': 'arXiv:2509.24202', 'title': 'Can Large Language Models Express Uncertainty Like Human?', 'authors': 'Linwei Tao, Yi-Fan Yeh, Bo Kai, Minjing Dong, Tao Huang, Tom A. Lamb, Jialin Yu, Philip H.S. Torr, Chang Xu', 'link': 'https://arxiv.org/abs/2509.24202', 'abstract': 'Large language models (LLMs) are increasingly used in high-stakes settings, where overconfident responses can mislead users. Reliable confidence estimation has been shown to enhance trust and task accuracy. Yet existing methods face practical barriers: logits are often hidden, multi-sampling is computationally expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score) deviates from natural communication. We revisit linguistic confidence (LC), where models express uncertainty through hedging language (e.g., probably, might), offering a lightweight and human-centered alternative. To advance this direction, we (1) release the first diverse, large-scale dataset of hedging expressions with human-annotated confidence scores, and (2) propose a lightweight mapper that converts hedges into confidence scores at near-zero cost. Building on these resources, we (3) conduct the first systematic study of LC across modern LLMs and QA benchmarks, revealing that while most LLMs underperform in expressing reliable LC, carefully designed prompting achieves competitive calibration and discriminability. Finally, we (4) introduce a fine-tuning framework that further improves LC reliability. Taken together, our work positions linguistic confidence as a scalable, efficient, and human-aligned approach to LLM uncertainty estimation, and calls for deeper exploration of this promising yet underexplored direction.', 'abstract_zh': '大型语言模型（LLMs）在高 stakes 环境中日益增多，过于自信的回复可能会误导用户。可靠的信心估计已经被证明能够增强信任和任务准确性。然而，现有的方法面临实际障碍：logits 经常被隐藏，多采样计算成本高昂，而口头化的数值不确定性（例如，给出 0-100 分数）偏离自然沟通。我们重新审视语言信心（LC），其中模型通过含糊其辞的语言（例如，可能、也许）来表达不确定性，提供了一种轻量级且用户导向的替代方案。为进一步推动这一方向，我们（1）发布了第一个具有人类标注信心分数的多样化大规模含糊表达数据集，（2）提出了一种轻量级映射器，以接近零的成本将含糊表达转换为信心分数。基于这些资源，我们（3）首次系统研究了现代 LLMs 和 QA 基准上的 LC，揭示了尽管大多数 LLMs 在表达可靠 LC 方面表现不佳，但精心设计的提示可以实现具有竞争力的校准和辨别能力。最后，我们（4）引入了一种微调框架，以进一步提高 LC 的可靠性。综上所述，我们的工作将语言信心定位为一种可扩展、高效且与人类对齐的方法，用于 LLM 不确定性的估计，并呼吁对这一有前途但尚未充分探索的方向进行更深入的研究。', 'title_zh': '大型语言模型能否像人类一样表达不确定性？'}
{'arxiv_id': 'arXiv:2509.24196', 'title': 'Chat to Chip: Large Language Model Based Design of Arbitrarily Shaped Metasurfaces', 'authors': 'Huanshu Zhang, Lei Kang, Sawyer D. Campbell, Douglas H. Werner', 'link': 'https://arxiv.org/abs/2509.24196', 'abstract': 'Traditional metasurface design is limited by the computational cost of full-wave simulations, preventing thorough exploration of complex configurations. Data-driven approaches have emerged as a solution to this bottleneck, replacing costly simulations with rapid neural network evaluations and enabling near-instant design for meta-atoms. Despite advances, implementing a new optical function still requires building and training a task-specific network, along with exhaustive searches for suitable architectures and hyperparameters. Pre-trained large language models (LLMs), by contrast, sidestep this laborious process with a simple fine-tuning technique. However, applying LLMs to the design of nanophotonic devices, particularly for arbitrarily shaped metasurfaces, is still in its early stages; as such tasks often require graphical networks. Here, we show that an LLM, fed with descriptive inputs of arbitrarily shaped metasurface geometries, can learn the physical relationships needed for spectral prediction and inverse design. We further benchmarked a range of open-weight LLMs and identified relationships between accuracy and model size at the billion-parameter level. We demonstrated that 1-D token-wise LLMs provide a practical tool to designing 2-D arbitrarily shaped metasurfaces. Linking natural-language interaction to electromagnetic modelling, this "chat-to-chip" workflow represents a step toward more user-friendly data-driven nanophotonics.', 'abstract_zh': '基于大规模语言模型的任意形状超表面设计：从自然语言到电磁建模的步骤迈向用户友好的数据驱动纳米光子学', 'title_zh': 'Chat to Chip: 基于大型语言模型的任意形状元表面设计'}
{'arxiv_id': 'arXiv:2509.24193', 'title': 'AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play', 'authors': 'Ran Xu, Yuchen Zhuang, Zihan Dong, Jonathan Wang, Yue Yu, Joyce C. Ho, Linjun Zhang, Haoyu Wang, Wenqi Shi, Carl Yang', 'link': 'https://arxiv.org/abs/2509.24193', 'abstract': 'Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at this https URL and this https URL.', 'abstract_zh': 'Search增强的大语言模型 often 在复杂推理任务中遇到挑战，因其多跳检索效果不佳且推理能力有限。我们提出AceSearcher，一种合作自对弈框架，通过训练单个大型语言模型（LLM）在分解查询和生成答案之间交替扮演两种角色，从而提升其在复杂推理任务中的表现。AceSearcher 结合了多样化搜索、推理和分解任务的监督微调及旨在提高最终答案准确性的强化微调，消除了中间标注的需求。在三个推理密集型任务的广泛实验中，使用10个数据集，AceSearcher 显著优于最先进的基线模型，平均精确匹配率提高7.6%。特别是在文档级金融推理任务中，AceSearcher-32B 使用不到5%的参数便达到了与DeepSeek-V3相当的性能。即使在较小规模（1.5B和8B参数）下，AceSearcher 也经常超越具有9倍更多参数的现有搜索增强大语言模型，突显了其在复杂推理任务中卓越的效率和效果。我们的代码将发布在以下链接：this https URL和this https URL。', 'title_zh': 'AceSearcher: 通过强化自我博弈 bootstrap 原理和搜索能力提升大语言模型'}
{'arxiv_id': 'arXiv:2509.24186', 'title': 'Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models', 'authors': 'Zhimeng Luo, Lixin Wu, Adam Frisch, Daqing He', 'link': 'https://arxiv.org/abs/2509.24186', 'abstract': "As Large Language Models (LLMs) are increasingly proposed for high-stakes medical applications, there has emerged a critical need for reliable and accurate evaluation methodologies. Traditional accuracy metrics fail inadequately as they neither capture question characteristics nor offer topic-specific insights. To address this gap, we introduce \\textsc{MedIRT}, a rigorous evaluation framework grounded in Item Response Theory (IRT), the gold standard in high-stakes educational testing. Unlike previous research relying on archival data, we prospectively gathered fresh responses from 80 diverse LLMs on a balanced, 1,100-question USMLE-aligned benchmark. Using one unidimensional two-parameter logistic IRT model per topic, we estimate LLM's latent model ability jointly with question difficulty and discrimination, yielding more stable and nuanced performance rankings than accuracy alone. Notably, we identify distinctive ``spiky'' ability profiles, where overall rankings can be misleading due to highly specialized model abilities. While \\texttt{GPT-5} was the top performer in a majority of domains (8 of 11), it was outperformed in Social Science and Communication by \\texttt{Claude-3-opus}, demonstrating that even an overall 23rd-ranked model can hold the top spot for specific competencies. Furthermore, we demonstrate IRT's utility in auditing benchmarks by identifying flawed questions. We synthesize these findings into a practical decision-support framework that integrates our multi-factor competency profiles with operational metrics. This work establishes a robust, psychometrically grounded methodology essential for the safe, effective, and trustworthy deployment of LLMs in healthcare.", 'abstract_zh': '大型语言模型（LLMs）在高风险医疗应用中的日益普及引发了对其可靠性和准确性的评估需求。传统的准确度指标在评估LLMs时表现出明显的不足，因为它们未能捕捉问题特性或提供专题洞见。为解决这一问题，我们引入了基于项目反应理论（Item Response Theory, IRT）的\\textsc{MedIRT}严格评估框架，IRT是高风险教育测试的金标准。不同于之前依赖档案数据的研究，我们前瞻性地收集了80种多样化LLMs在平衡的1100题USMLE对标的基准上的最新回应。使用每个主题的双向参数logistic IRT模型，我们共同估计了LLMs的潜在模型能力、问题难度和区分度，从而比单一准确度指标提供更稳定和细致的性能排名。值得注意的是，我们识别出独特的“尖峰”能力轮廓，整体排名可能因高度专业化的模型能力而误导。尽管\\texttt{GPT-5}在大多数领域（11个中的8个）表现最佳，但在社会科学和沟通方面却输给了\\texttt{Claude-3-opus}，这表明即使是综合排名最低的模型也有可能在特定能力方面名列前茅。此外，我们展示了IRT在审核基准中的应用，通过识别有问题的题目来提高基准质量。我们将这些发现整合成一个实用的决策支持框架，该框架结合了我们的多因素能力概况与操作性指标。本研究确立了一种稳健且心理测量学上站得住脚的方法论，对于在医疗保健中安全、有效和可信地部署LLMs至关重要。', 'title_zh': '超越总体准确率：对80个大型语言模型在特定主题医疗能力上的心理测量深度探究'}
{'arxiv_id': 'arXiv:2509.24166', 'title': 'Stable Forgetting: Bounded Parameter-Efficient Unlearning in LLMs', 'authors': 'Arpit Garg, Hemanth Saratchandran, Ravi Garg, Simon Lucey', 'link': 'https://arxiv.org/abs/2509.24166', 'abstract': 'Machine unlearning in large language models (LLMs) is essential for privacy and safety; however, existing approaches remain unstable and unreliable. A widely used strategy, the gradient difference method, applies gradient descent on retained data while performing gradient ascent on forget data, the data whose influence should be removed. However, when combined with cross-entropy loss, this procedure causes unbounded growth of weights and gradients, leading to training instability and degrading both forgetting and retention. We provide a theoretical framework that explains this failure, explicitly showing how ascent on the forget set destabilizes optimization in the feedforward MLP layers of LLMs. Guided by this insight, we propose Bounded Parameter-Efficient Unlearning, a parameter-efficient approach that stabilizes LoRA-based fine-tuning by applying bounded functions to MLP adapters. This simple modification controls the weight dynamics during ascent, enabling the gradient difference method to converge reliably. Across the TOFU, TDEC, and MUSE benchmarks, and across architectures and scales from 125M to 8B parameters, our method achieves substantial improvements in forgetting while preserving retention, establishing a novel theoretically grounded and practically scalable framework for unlearning in LLMs.', 'abstract_zh': '大型语言模型（LLMs）中的机器遗忘对于隐私和安全至关重要；然而，现有的方法仍然不稳定且不可靠。广泛使用的梯度差分方法在保留数据上应用梯度下降，在忘记数据（其影响应被移除的数据）上应用梯度上升。然而，当与交叉熵损失结合使用时，此过程会导致权重和梯度的无界增长，从而导致训练不稳定并恶化遗忘和保留效果。我们提供了一个理论框架来解释这一失败，明确展示了在LLMs的前向MLP层中，对于忘记集的上升操作如何导致优化的失稳。基于这一洞察，我们提出了有界参数高效遗忘方法，这是一种通过在MLP适配器上应用有界函数来稳定LoRA基微调的参数高效方法。这一简单修改控制了上升过程中的权重动态，使得梯度差分方法能够可靠收敛。在TOFU、TDEC和MUSE基准上，以及从125M到8B参数的各种架构和规模下，我们的方法在遗忘方面取得了显著改进，同时保留了保留效果，从而建立了用于LLMs遗忘的新型理论基础和实际可扩展框架。', 'title_zh': '稳定遗忘：LLM中的有限参数高效遗忘'}
{'arxiv_id': 'arXiv:2509.24148', 'title': 'TENET: Leveraging Tests Beyond Validation for Code Generation', 'authors': 'Yiran Hu, Nan Jiang, Shanchao Liang, Yi Wu, Lin Tan', 'link': 'https://arxiv.org/abs/2509.24148', 'abstract': 'Test-Driven Development (TDD) is a widely adopted software engineering practice that requires developers to create and execute tests alongside code implementation, ensuring that software behavior is continuously validated and refined. In the era of vibe coding, where developers increasingly delegate code writing to large language models (LLMs) by specifying high-level intentions, TDD becomes even more crucial, as test cases serve as executable specifications that explicitly define and verify intended functionality beyond what natural-language descriptions and code context can convey. While vibe coding under TDD is promising, there are three main challenges: (1) selecting a small yet effective test suite to improve the generation accuracy and control the execution workload, (2) retrieving context such as relevant code effectively, and (3) systematically using test feedback for effective code refinement. To address these challenges, we introduce TENET, an LLM agent for generating functions in complex real-world repositories under the TDD setting. TENET features three components: (1) a novel test harness mechanism that selects a concise test suite to maximize diversity of target usage scenarios; (2) a tailored agent toolset that performs efficient retrieval of relevant code with interactive debugging; and (3) a reflection-based refinement workflow that iteratively analyzes failures, replenishes context, and applies code refinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval benchmarks, outperforming the best agentic baselines by 9.49 and 2.17 percentage points, respectively. In addition, this is the first study of test-driven code generation with repository-level context, examining how different aspects of test suites affect the performance of LLM agents under the TDD setting.', 'abstract_zh': 'TENET：面向TDD的复杂仓库中函数生成的LLM代理', 'title_zh': 'TENET: 利用超越验证的测试进行代码生成'}
{'arxiv_id': 'arXiv:2509.24147', 'title': 'Your thoughts tell who you are: Characterize the reasoning patterns of LRMs', 'authors': 'Yida Chen, Yuning Mao, Xianjun Yang, Suyu Ge, Shengjie Bi, Lijuan Liu, Saghar Hosseini, Liang Tan, Yixin Nie, Shaoliang Nie', 'link': 'https://arxiv.org/abs/2509.24147', 'abstract': "Current comparisons of large reasoning models (LRMs) focus on macro-level statistics such as task accuracy or reasoning length. Whether different LRMs reason differently remains an open question. To address this gap, we introduce the LLM-proposed Open Taxonomy (LOT), a classification method that uses a generative language model to compare reasoning traces from two LRMs and articulate their distinctive features in words. LOT then models how these features predict the source LRM of a reasoning trace based on their empirical distributions across LRM outputs. Iterating this process over a dataset of reasoning traces yields a human-readable taxonomy that characterizes how models think. We apply LOT to compare the reasoning of 12 open-source LRMs on tasks in math, science, and coding. LOT identifies systematic differences in their thoughts, achieving 80-100% accuracy in distinguishing reasoning traces from LRMs that differ in scale, base model family, or objective domain. Beyond classification, LOT's natural-language taxonomy provides qualitative explanations of how LRMs think differently. Finally, in a case study, we link the reasoning differences to performance: aligning the reasoning style of smaller Qwen3 models with that of the largest Qwen3 during test time improves their accuracy on GPQA by 3.3-5.7%.", 'abstract_zh': '当前对大规模推理模型（LRMs）的比较主要集中于宏观统计指标，如任务准确率或推理长度。不同LRMs是否以不同的方式推理仍然是一个开放问题。为了弥合这一差距，我们提出了LLM提出的开放分类法（LOT），这是一种使用生成语言模型比较两个LRMs的推理轨迹并用文字描述其独特特征的分类方法。LOT随后根据LRM输出的实验分布，建模这些特征如何预测推理轨迹的起源模型。通过对推理轨迹数据集的迭代处理，LOT生成了一种易于理解的分类法，刻画了模型的思考方式。我们应用LOT比较了12个开源LRMs在数学、科学和编程任务中的推理。LOT识别了它们思维中的系统性差异，在区分规模不同、基础模型家族或目标领域不同的LRMs的推理轨迹方面实现了80-100%的准确率。除了分类，LOT的语言分类法还提供了LRMs如何不同地思考的定性解释。最后，在一个案例研究中，我们将推理差异与性能联系起来：在测试时将较小的Qwen3模型的推理风格与最大的Qwen3对齐，提高了其在GPQA上的准确率3.3-5.7%。', 'title_zh': '你的思考揭示了你的本质：刻画LRMs的推理模式'}
{'arxiv_id': 'arXiv:2509.24125', 'title': 'The Impossibility of Inverse Permutation Learning in Transformer Models', 'authors': 'Rohan Alur, Chris Hays, Manish Raghavan, Devavrat Shah', 'link': 'https://arxiv.org/abs/2509.24125', 'abstract': 'In this technical note, we study the problem of inverse permutation learning in decoder-only transformers. Given a permutation and a string to which that permutation has been applied, the model is tasked with producing the original (``canonical\'\') string. We argue that this task models a natural robustness property across a variety of reasoning tasks, including long-context retrieval, multiple choice QA and in-context learning. Our primary contribution is an impossibility result: we show that an arbitrary depth, decoder-only transformer cannot learn this task. This result concerns the expressive capacity of decoder-only transformer models and is agnostic to training dynamics or sample complexity. We give a pair of alternative constructions under which inverse permutation learning is feasible. The first of these highlights the fundamental role of the causal attention mask, and reveals a gap between the expressivity of encoder-decoder transformers and the more popular decoder-only architecture. The latter result is more surprising: we show that simply padding the input with ``scratch tokens" yields a construction under which inverse permutation learning is possible. We conjecture that this may suggest an alternative mechanism by which chain-of-thought prompting or, more generally, intermediate ``thinking\'\' tokens can enable reasoning in large language models, even when these tokens encode no meaningful semantic information (e.g., the results of intermediate computations).', 'abstract_zh': '本技术注记研究了仅解码器变压器中逆排列学习的问题。给定一个排列及其上应用后的字符串，模型的任务是生成原始（“标准”）字符串。我们认为，这一任务模拟了多种推理任务中的自然鲁棒性特性，包括长上下文检索、多项选择问答和上下文学习。我们的主要贡献是一个不可能性结果：我们证明了任意深度的仅解码器变压器无法学习这一任务。该结果关注仅解码器变压器模型的表征能力，并与训练动态或样本复杂度无关。我们给出了两种替代构造，在这些构造下逆排列学习是可行的。其中第一个凸显了因果注意力掩码的基本作用，揭示了编码器-解码器变压器与更流行的仅解码器架构在表征能力上的差距。后者结果更为令人惊讶：我们证明，简单地用“草稿标记”填充输入即可在某种构造下实现逆排列学习。我们推测这可能表明了一种替代机制，通过链式思考提示或更一般地，中间“思考”标记，即使这些标记编码了无意义的语义信息（例如中间计算的结果），也可能使大规模语言模型具备推理能力。', 'title_zh': 'Transformer模型中逆排列学习的不可能性'}
{'arxiv_id': 'arXiv:2509.24096', 'title': 'GEAR: A General Evaluation Framework for Abductive Reasoning', 'authors': 'Kaiyu He, Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Xinya Du, Zhiyu Chen', 'link': 'https://arxiv.org/abs/2509.24096', 'abstract': 'Since the advent of large language models (LLMs), research has focused on instruction following and deductive reasoning. A central question remains: can these models discover new knowledge, and how can we evaluate this ability? We address this by studying abductive reasoning-the generation of plausible hypotheses to explain observations-and introduce GEAR (General Evaluation for Abductive Reasoning), a general-purpose, fully automated, transparent, and label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics: consistency (each hypothesis explains the observations), generalizability (consistent hypotheses make meaningful predictions on unseen inputs), and diversity (the set covers distinct predictions and patterns). Built this way, GEAR is scalable (no human gold answers), reliable (deterministic scoring aligned with classical abduction), and open-ended (scores improve only when models produce new plausible hypotheses, unlike static benchmarks that saturate once accuracy is high). Using GEAR, we conduct a fine-grained study of nine LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000 candidate hypotheses and revealing model differences obscured by gold-answer or purely human evaluations. We further propose a momentum-based curriculum that adjusts GEAR-derived training data by learning velocity: it starts with what the model learns quickly and shifts toward harder objectives such as generating diverse hypotheses once the model is confident on foundational objectives. Without gold-label supervision, this strategy improves all GEAR objectives and these gains transfer to established abductive reasoning benchmarks. Taken together, GEAR provides a principled framework that evaluates abduction and supplies label-free, scalable training signals that help LLMs produce more diverse and reliable hypotheses.', 'abstract_zh': '自大型语言模型（LLM）问世以来，研究重点一直放在指令跟随和演绎推理上。一个核心问题仍然是：这些模型能否发现新知识，我们如何评估这种能力？通过研究解释观察结果的归纳推理——生成合理的假设——我们引入了GEAR（通用归纳推理评估），这是一种通用的、全自动的、透明的、无标签的评估范式。GEAR通过三个指标来评分假设集：一致性（每个假设解释观察结果）、泛化能力（一致的假设对未见过的输入做出有意义的预测）和多样性（集合覆盖不同的预测和模式）。通过这种方式，GEAR具有可扩展性（无需人工黄金答案）、可靠性（评分与经典归纳推理一致）、开放性（仅在模型生成新的合理假设时评分提高，不同于饱和的一次性基准）。利用GEAR，我们在四个归纳推理基准上对九个LLM进行了精细研究，涉及1500个问题，生成了超过50,000个候选假设，并揭示了黄金答案或纯人类评估中隐藏的模型差异。我们进一步提出了一种动量为基础的课程，通过学习速度调整GEAR衍生的训练数据：它从模型学习快的内容开始，然后转向生成多样化假设等更困难的目标，只要模型在基础目标上表现出自信。在无需黄金标签监督的情况下，这种策略提高了所有GEAR目标，并将这些增益转移到已确立的归纳推理基准上。综上所述，GEAR提供了一个原则性的框架，用于评估归纳推理，并提供了无标签、可扩展的训练信号，帮助LLM生成更多样化和可靠的假设。', 'title_zh': 'GEAR: 一种演绎推理的通用评估框架'}
{'arxiv_id': 'arXiv:2509.24090', 'title': 'Large-Scale Constraint Generation - Can LLMs Parse Hundreds of Constraints?', 'authors': 'Matteo Boffa, Jiaxuan You', 'link': 'https://arxiv.org/abs/2509.24090', 'abstract': "Recent research has explored the constrained generation capabilities of Large Language Models (LLMs) when explicitly prompted by few task-specific requirements. In contrast, we introduce Large-Scale Constraint Generation (LSCG), a new problem that evaluates whether LLMs can parse a large, fine-grained, generic list of constraints. To examine the LLMs' ability to handle an increasing number constraints, we create a practical instance of LSCG, called Words Checker. In Words Checker, we evaluate the impact of model characteristics (e.g., size, family) and steering techniques (e.g., Simple Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet, a small and dedicated model that parses the original list of constraints into a smaller subset, helping the LLM focus on relevant constraints. Experiments reveal that existing solutions suffer a significant performance drop as the number of constraints increases, with FoCusNet showing an 8-13% accuracy boost.", 'abstract_zh': '近期研究探索了在明确提示下，大规模语言模型（LLMs）的约束生成能力。相比之下，我们引入了大规模约束生成（LSCG）这一新问题，评估LLMs是否能够解析一个大型、细粒度且通用的约束列表。为了考察LLMs处理不断增加的约束数量的能力，我们创建了一个LSCG的实用实例，称为Words Checker。在Words Checker中，我们评估了模型特性（如规模、家族）和引导技术（如简要提示、思维链、最优N）对性能的影响。我们还提出了FoCusNet，这是一种小型且专门化的模型，能够将原始的约束列表解析为一个更小的子集，帮助LLMs专注于相关约束。实验结果显示，随着约束数量的增加，现有解决方案的性能显著下降，而FoCusNet则展示了8-13%的准确率提升。', 'title_zh': '大规模约束生成——LLMs能解析数百个约束吗？'}
{'arxiv_id': 'arXiv:2509.24085', 'title': 'PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM', 'authors': 'Ju-Hyung Lee, Yanqing Lu, Klaus Doppler', 'link': 'https://arxiv.org/abs/2509.24085', 'abstract': 'We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a framework for cooperative cross-layer optimization in device-to-device (D2D) communication. Building on our previous work on single-device on-device LLMs, PEARL extends the paradigm by leveraging both publisher and subscriber states to guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which normalizes latency by application tolerances and modulates energy by device battery states, provides richer supervision for KL-based finetuning. We study two lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves the best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms inference at near-identical objective scores. Across synthetic scenarios grounded in real measurements, PEARL improves objective scores over heuristic and compact model baselines and reduces energy by up to 16% in cooperative low-battery cases. These results demonstrate that peer-aware context, reward-aligned training, and head-based efficiency make LLMs practical for always-on, on-device cross-layer control.', 'abstract_zh': 'PEARL：基于设备端LLM的同伴增强自适应无线电框架', 'title_zh': 'PEARL: 同辈增强自适应无线通信 via 边缘设备上的LLM'}
{'arxiv_id': 'arXiv:2509.24068', 'title': 'A Small Math Model: Recasting Strategy Choice Theory in an LLM-Inspired Architecture', 'authors': 'Roussel Rahman, Jeff Shrager', 'link': 'https://arxiv.org/abs/2509.24068', 'abstract': "Strategy Choice Theory (SCT)\\footnote{``Strategy Choice Theory'', ``Distributions of Associations'', and ``Overlapping Wave Theory'' have been used to refer to this line of work, emphasizing different aspects.}\\citep[e.g.,][]{siegler1984strategychoices, siegler2000rebirth} explains important aspects of children's arithmetic learning based upon principles including learning from developmentally naturalistic data, probabilistic representation, confidence-based retrieval, and the phase-like importance of scaffolding strategies, such as finger-counting. Here we recast SCT as a ``Small Math Model'' (SMM), employing a neural-network-based architecture analogous to LLMs. The SMM extends SCT to include counting practice\\footnote{The original SCT model was pre-biased in accordance with the supposed experience of counting.}, symbol (number) embedding, and gated attention. Similar to earlier work, the SMM demonstrates constructive and destructive interference between counting and addition, and the ``wave-like'' use of finger-counting as sum recall improves. We plan to extend the SMM to later aspects of the decades-long SCT program, including adaptive strategy choice and eventually strategy discovery, providing a unified platform to investigate the understanding of numerical characteristics and relationships essential for mathematical reasoning -- as it can emerge in LLM-based agents.", 'abstract_zh': '小数学模型（SMM）', 'title_zh': '一个小数学模型：以大语言模型启发的策略选择理论重构'}
{'arxiv_id': 'arXiv:2509.24046', 'title': 'PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features', 'authors': 'Lingyao Li, Haolun Wu, Zhenkun Li, Jiabei Hu, Yu Wang, Xiaoshan Huang, Wenyue Hua, Wenqian Wang', 'link': 'https://arxiv.org/abs/2509.24046', 'abstract': 'High-dimensional decision-making tasks, such as business partner selection, involve evaluating large candidate pools with heterogeneous numerical, categorical, and textual features. While large language models (LLMs) offer strong in-context reasoning capabilities, single-agent or debate-style systems often struggle with scalability and consistency in such settings. We propose PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation into three layers: a Planner Agent that designs strategies, Specialized Agents that perform role-specific assessments, and a Supervisor Agent that integrates their outputs. To support systematic evaluation, we also introduce a curated benchmark dataset of venture capital co-investments, featuring diverse firm attributes and ground-truth syndicates. Across 140 cases, PartnerMAS consistently outperforms single-agent and debate-based multi-agent baselines, achieving up to 10--15\\% higher match rates. Analysis of agent reasoning shows that planners are most responsive to domain-informed prompts, specialists produce complementary feature coverage, and supervisors play an important role in aggregation. Our findings demonstrate that structured collaboration among LLM agents can generate more robust outcomes than scaling individual models, highlighting PartnerMAS as a promising framework for high-dimensional decision-making in data-rich domains.', 'abstract_zh': '高维度决策任务中的伙伴选择涉及评估具有异质数值、分类和文本特征的大型候选池。虽然大型语言模型提供了强大的上下文推理能力，但单智能体或辩论式系统在这些场景中往往面临可扩展性和一致性方面的挑战。我们提出了一种层次化的多智能体框架PartnerMAS，将评估分解为三层：规划智能体负责设计策略，专业智能体执行特定角色的评估，监督智能体整合其输出。为了支持系统性的评估，我们还引入了一个精制的基准数据集，包含风险投资联合投资的多样化企业属性和真实 syndicate 的地面真相表示。在 140 个案例中，PartnerMAS 在单智能体和辩论式多智能体基线方面始终表现出色，匹配率最高可提高 10-15%。对智能体推理的分析显示，规划者对领域导向的提示最敏感，专家生成互补的特征覆盖，而监督者在聚合中发挥重要作用。我们的研究结果表明，在数据丰富的领域中，LLM 智能体之间的结构化合作可以产生比单模型扩展更为稳健的结果，突显出了 PartnerMAS 作为高维度决策制定的有前途框架的价值。', 'title_zh': 'PartnerMAS：一种基于高维特征的企业合作伙伴选择多层次多agent框架'}
{'arxiv_id': 'arXiv:2509.24002', 'title': 'MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use', 'authors': 'Zijian Wu, Xiangyan Liu, Xinyuan Zhang, Lingjun Chen, Fanqing Meng, Lingxiao Du, Yiran Zhao, Fanshi Zhang, Yaoqi Ye, Jiawei Wang, Zirui Wang, Jinjie Ni, Yufan Yang, Arvin Xu, Michael Qizhe Shieh', 'link': 'https://arxiv.org/abs/2509.24002', 'abstract': 'MCP standardizes how LLMs interact with external systems, forming the foundation for general agents. However, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows. To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP use in a more realistic and comprehensive manner. It consists of $127$ high-quality tasks collaboratively created by domain experts and AI agents. Each task begins with a curated initial state and includes a programmatic script for automatic verification. These tasks demand richer and more diverse interactions with the environment, involving a broad range of create, read, update, and delete (CRUD) operations. We conduct a comprehensive evaluation of cutting-edge LLMs using a minimal agent framework that operates in a tool-calling loop. Empirical results show that the best-performing model, gpt-5-medium, reaches only $52.56$\\% pass@1 and $33.86$\\% pass^4, while other widely regarded strong models, including claude-sonnet-4 and o3, fall below $30$\\% pass@1 and $15$\\% pass^4. On average, LLMs require $16.2$ execution turns and $17.4$ tool calls per task, significantly surpassing those in previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.', 'abstract_zh': 'MCPMark：一种评估MCP在更现实和全面场景下使用的新基准', 'title_zh': 'MCPMark: 一种用于压力测试实际且全面的MCP使用情况的基准测试'}
{'arxiv_id': 'arXiv:2509.23994', 'title': 'The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis', 'authors': 'Gauri Kholkar, Ratinder Ahuja', 'link': 'https://arxiv.org/abs/2509.23994', 'abstract': 'As autonomous AI agents are increasingly deployed in industry, it is essential to safeguard them. We introduce a novel framework that automates the translation of unstructured design documents into verifiable, real-time guardrails. We introduce "Policy as Prompt," a new approach that uses Large Language Models (LLMs) to interpret and enforce natural language policies by applying contextual understanding and the principle of least privilege. Our system first ingests technical artifacts to construct a verifiable policy tree, which is then compiled into lightweight, prompt-based classifiers that audit agent behavior at runtime. We validate our approach across diverse applications, demonstrating a scalable and auditable pipeline that bridges the critical policy-to-practice gap, paving the way for verifiably safer and more regulatable AI.', 'abstract_zh': '随着自主AI代理在工业中的日益部署，保障它们的安全至关重要。我们提出了一种新的框架，自动将未结构化的设计文档转换为可验证的、实时的护栏。我们引入了“策略即提示”的新方法，使用大规模语言模型（LLMs）通过上下文理解与最小权限原则来解释和执行自然语言策略。我们的系统首先摄取技术 artifacts 来构建可验证的策略树，然后将其编译成轻量级的、基于提示的分类器，在运行时审核代理行为。我们跨多种应用验证了该方法，展示了可扩展且可审计的管道，弥合了关键的策略到实践差距，为可验证更安全和更可调节的 AI 开辟了道路。', 'title_zh': 'AI代理行为准则：自动化护栏提示合成政策'}
{'arxiv_id': 'arXiv:2509.23992', 'title': 'Guide: Generalized-Prior and Data Encoders for DAG Estimation', 'authors': 'Amartya Roy, Devharish N, Shreya Ganguly, Kripabandhu Ghosh', 'link': 'https://arxiv.org/abs/2509.23992', 'abstract': "Modern causal discovery methods face critical limitations in scalability, computational efficiency, and adaptability to mixed data types, as evidenced by benchmarks on node scalability (30, $\\le 50$, $\\ge 70$ nodes), computational energy demands, and continuous/non-continuous data handling. While traditional algorithms like PC, GES, and ICA-LiNGAM struggle with these challenges, exhibiting prohibitive energy costs for higher-order nodes and poor scalability beyond 70 nodes, we propose \\textbf{GUIDE}, a framework that integrates Large Language Model (LLM)-generated adjacency matrices with observational data through a dual-encoder architecture. GUIDE uniquely optimizes computational efficiency, reducing runtime on average by $\\approx 42%$ compared to RL-BIC and KCRL methods, while achieving an average $\\approx 117%$ improvement in accuracy over both NOTEARS and GraN-DAG individually. During training, GUIDE's reinforcement learning agent dynamically balances reward maximization (accuracy) and penalty avoidance (DAG constraints), enabling robust performance across mixed data types and scalability to $\\ge 70$ nodes -- a setting where baseline methods fail.", 'abstract_zh': '现代因果发现方法在扩展性、计算效率和混合数据类型适应性方面面临关键限制，如节点扩展性基准（30, $\\le 50$, $\\ge 70$ 节点）、计算能耗需求以及连续/非连续数据处理能力所证实的那样。尽管传统的PC算法、GES算法和ICA-LiNGAM在这些挑战面前表现不佳，能耗成本高企，且在超过70个节点时表现出糟糕的扩展性，我们提出了**GUIDE框架**，该框架通过双编码器架构结合了大语言模型生成的邻接矩阵和观测数据。GUIDE独特地优化了计算效率，相比RL-BIC和KCRL方法平均减少约42%的运行时间，并且在准确率上分别比NOTEARS和GraN-DAG高出约117%。在训练过程中，GUIDE的强化学习代理动态平衡准确性和DAG约束条件下的惩罚规避，从而在混合数据类型下表现出稳健性能，并实现节点数超过70的情况下的可扩展性——这是基线方法无法做到的。', 'title_zh': '指南：广义先验和数据编码器用于DAG估计'}
{'arxiv_id': 'arXiv:2509.23990', 'title': 'The Hidden Costs of Translation Accuracy: Distillation, Quantization, and Environmental Impact', 'authors': 'Dhaathri Vijay, Anandaswarup Vadapalli', 'link': 'https://arxiv.org/abs/2509.23990', 'abstract': 'The rapid expansion of large language models (LLMs) has heightened concerns about their computational and environmental costs. This study investigates the trade-offs between translation quality and efficiency by comparing full-scale, distilled, and quantized models using machine translation as a case study. We evaluated performance on the Flores+ benchmark and through human judgments of conversational translations in French, Hindi, and Kannada. Our analysis of carbon emissions per evaluation run revealed that the full 3.3B fp32 model, while achieving the highest BLEU scores, incurred the largest environmental footprint (about 0.007-0.008 kg CO2 per run). The distilled models achieved an inference of up to 4.5x faster than the full 3.3B model, with only minimal reductions in BLEU scores. Human evaluations also showed that even aggressive quantization (INT4) preserved high levels of accuracy and fluency, with differences between models generally minor. These findings demonstrate that model compression strategies can substantially reduce computational demands and environmental impact while maintaining competitive translation quality, though trade-offs are more pronounced in low-resource settings. We argue for evaluation frameworks that integrate efficiency and sustainability alongside objective metrics as central dimensions of progress in NLP.', 'abstract_zh': '大规模语言模型的迅速扩展引发了对其计算和环境成本的关注。本研究通过将全规模、蒸馏和量化模型进行比较，以机器翻译为例，探讨翻译质量和效率之间的权衡。我们在Flores+基准和对法语、印地语和卡纳达语对话翻译的人工评价上评估了模型性能。我们的分析显示，尽管全规模33亿参数的浮点32位模型在BLEU分数上最高，但其每评估运行的碳排放量最大（约为0.007-0.008 kg CO2）。蒸馏模型的推理速度比全规模33亿参数模型快4.5倍以上，同时BLEU分数的减少 minimal。人工评价还表明，即使是激进的量化（INT4）也能保持高水平的准确性和流畅性，不同模型之间的差异通常较小。这些发现表明，在维持竞争力的翻译质量的同时，通过模型压缩策略可以显著降低计算需求和环境影响，尽管在资源有限的环境中权衡更为明显。我们主张将效率和可持续性纳入与客观指标并列的核心维度，以促进自然语言处理的进步。', 'title_zh': '翻译后的标题为：翻译准确性下的隐形成本：精炼、量化及其环境影响'}
{'arxiv_id': 'arXiv:2509.23982', 'title': 'Toward Preference-aligned Large Language Models via Residual-based Model Steering', 'authors': 'Lucio La Cava, Andrea Tagarelli', 'link': 'https://arxiv.org/abs/2509.23982', 'abstract': 'Preference alignment is a critical step in making Large Language Models (LLMs) useful and aligned with (human) preferences. Existing approaches such as Reinforcement Learning from Human Feedback or Direct Preference Optimization typically require curated data and expensive optimization over billions of parameters, and eventually lead to persistent task-specific models. In this work, we introduce Preference alignment of Large Language Models via Residual Steering (PaLRS), a training-free method that exploits preference signals encoded in the residual streams of LLMs. From as few as one hundred preference pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be applied at inference time to push models toward preferred behaviors. We evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance. Moreover, when compared to DPO-aligned models, they perform better with huge time savings. Our findings highlight that PaLRS offers an effective, much more efficient and flexible alternative to standard preference optimization pipelines, offering a training-free, plug-and-play mechanism for alignment with minimal data.', 'abstract_zh': 'Large Language Models的偏好对齐：通过残差导向（PaLRS）实现偏好导向训练-free方法', 'title_zh': '基于残差导向的模型调节实现偏好对齐的大语言模型'}
{'arxiv_id': 'arXiv:2509.23946', 'title': 'Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm', 'authors': 'Kaisen Yang, Lixuan He, Rushi Shah, Kaicheng Yang, Qinwei Ma, Dianbo Liu, Alex Lamb', 'link': 'https://arxiv.org/abs/2509.23946', 'abstract': "Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation algorithm enforcing strict plan adherence - with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of this http URL decomposition enables an efficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution. The code and pre-trained models for the project are available at: this https URL", 'abstract_zh': "Chain-of-Thought（CoT）及其变体显著提升了大型语言模型（LLMs）的推理能力，然而其单一且自回归的架构本领会将高级战略规划与低级步骤执行混淆在一起，导致计算效率低下、推理路径探索有限以及可解释性降低。为克服这些问题，我们提出了探索-执行链（$E^2C$），这是一种分阶段的推理框架，将推理分解为两个不同的阶段：探索阶段随机生成简洁的高级计划，随后是执行阶段确定性地执行选定的计划。我们的方法结合了两阶段训练方法，该方法包括通过新颖的数据生成算法增强监督微调（SFT），该算法强制执行严格的计划一致性，以及后续的强化学习（RL）阶段，该阶段利用探索的有效性并强化执行的确定性。这种分解允许高效的测试时扩展策略：在AIME'2024中，$E^2C$测试时扩展达到了58.1%的准确率，仅需与类似方法（如思考森林）所需解码令牌的不到10%，大幅降低了自一致性开销。对于跨领域的适应性，我们的探索导向SFT（EF-SFT）只使用标准SFT的3.5%令牌，但在医疗基准上的准确率提高了多达14.5%，实现了最先进的性能、强大的泛化能力和更好的可解释性，这是因为分离了计划和执行。项目的代码和预训练模型可在此处获取：this https URL。", 'title_zh': '探索-执行链：迈向一种高效的结构化推理范式'}
{'arxiv_id': 'arXiv:2509.23928', 'title': 'HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models', 'authors': 'Zhinan Xie, Peisong Wang, Jian Cheng', 'link': 'https://arxiv.org/abs/2509.23928', 'abstract': "Speculative decoding is an effective approach for accelerating inference in Large Language models (LLMs), but its adaptation to Vision-Language models (VLMs) remains challenging for additional visual tokens in multimodal inputs. First, owing to the fact that the drafter and the target VLM may derived from different families, the semantic representations of visual tokens in the target VLM are misaligned with those in the drafter, introducing bias into the KV-cache during the prefill stage. Second, the large number of visual tokens substantially slows down the drafter's self-attention during the decoding stage. We propose Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models (HiViS), an explicit-implicit input decomposition framework that alleviates the above inefficiency. All visual tokens are removed from the drafter's input, retaining only textual tokens as explicit inputs, while directly reusing the target VLM's corresponding last-layer hidden states as implicit visual information without additional processing. To train the drafter efficiently, we introduces multi-step self-feedback training strategy with dynamic data selection and sequential embedding supervision to simulate reasoning during training. Our approach compresses the prefill sequence length of the drafter to only 0.7%-1.3% of the target VLM's input, while maintaining lossless generation quality. Extensive experiments across diverse models and tasks demonstrate up to 2.65x speedup, confirming the effectiveness of HiViS in accelerating VLM inference.", 'abstract_zh': '隐藏视觉令牌以加速视觉语言模型的投机解码（HiViS）', 'title_zh': 'HiViS: 从草稿者隐藏视觉标记以在视觉-语言模型中进行推测解码'}
{'arxiv_id': 'arXiv:2509.23924', 'title': 'Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step', 'authors': 'Jingyi Yang, Guanxu Chen, Xuhao Hu, Jing Shao', 'link': 'https://arxiv.org/abs/2509.23924', 'abstract': 'Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: this https URL.', 'abstract_zh': 'Masked扩散语言模型（MDLMs） recently emerged as a有前景的替代自回归（AR）语言模型的选择，提供了并行解码、灵活的生成顺序以及更少的推理步骤等特性。尽管存在这些优势，针对MDLMs的解码策略和强化学习（RL）算法仍然研究不足。一种简单的做法是直接将为AR模型开发的成熟技术应用于MDLMs。然而，这提出了一个直接的问题：这种简单的转移是否真的最优化？例如，1) 块级和半AR解码策略在MDLMs的训练中未被采用，那么为什么它们在推理中能优于全扩散风格的解码？2) 将为AR模型设计的RL算法直接应用于MDLMs，由于MDLM解码是非因果的（并行的），导致了生成轨迹与优化轨迹之间的不一致性。为了解决这些挑战，我们提出了EOS提前拒绝（EOSER）和上升步长（ASS）解码调度器，这使MDLMs能够执行全扩散风格的解码，实现了在更少的解码步骤内达到竞争性性能。此外，我们引入了约束解码轨迹组相对策略优化（CJ-GRPO）来驯化MDLMs，强调生成轨迹与优化轨迹的一致性，并减少了由于跳步优化引起的优化误差。我们在包括数学和规划基准任务的推理任务中使用了LLaDA-8B-Instruct进行了广泛的实验。结果表明，提出的EOSER和ASS机制，结合CJ-GRPO，对有效和高效驯化MDLMs具有重要意义。代码: [这里](这个链接)。', 'title_zh': '通过少量解码步骤的一致性轨迹强化学习驯服掩码扩散语言模型'}
{'arxiv_id': 'arXiv:2509.23893', 'title': 'Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings', 'authors': 'Zhixin Zhang, Zeming Wei, Meng Sun', 'link': 'https://arxiv.org/abs/2509.23893', 'abstract': 'Catastrophic forgetting remains a critical challenge in continual learning for large language models (LLMs), where models struggle to retain performance on historical tasks when fine-tuning on new sequential data without access to past datasets. In this paper, we first reveal that the drift of functional directions during the fine-tuning process is a key reason why existing regularization-based methods fail in long-term LLM continual learning. To address this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a novel approach that tracks the drift of these functional directions and dynamically updates them during the fine-tuning process. Furthermore, by adjusting the gradients of new task parameters to be orthogonal to the tracked historical function directions, our method mitigates interference between new and old tasks. Extensive experiments on various LLM continual learning benchmarks demonstrate that this approach outperforms prior methods, effectively reducing catastrophic forgetting and providing a robust tool for continuous LLM fine-tuning. Our code is available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）持续学习中灾难性遗忘仍然是一个关键挑战，在仅利用新序列数据微调而无法访问过往数据集的情况下，模型难以保留历史任务的性能。本文首先揭示，在微调过程中功能方向的变化是现有基于正则化的持续学习方法在长期LLMs持续学习中失效的关键原因。为此，我们提出了一种新颖的Dynamic Orthogonal Continual (DOC) 微调方法，该方法跟踪这些功能方向的变化并在微调过程中动态更新它们。此外，通过使新任务参数的梯度与跟踪的历史功能方向正交，我们的方法减轻了新旧任务之间的干扰。在各种LLM持续学习基准上的广泛实验表明，该方法优于先前的方法，有效减少了灾难性遗忘，为持续LLM微调提供了一个稳健的工具。我们的代码可在以下链接获取。', 'title_zh': '动态正交持续微调以缓解灾难性遗忘'}
{'arxiv_id': 'arXiv:2509.23886', 'title': 'Towards Understanding Subliminal Learning: When and How Hidden Biases Transfer', 'authors': 'Simon Schrodi, Elias Kempf, Fazl Barez, Thomas Brox', 'link': 'https://arxiv.org/abs/2509.23886', 'abstract': 'Language models can transfer hidden biases during distillation. For example, a teacher that "likes owls" can make its student "like owls" too, even when the training data consists only of lists of numbers. This surprising phenomenon is called subliminal learning. Subliminal learning can be expected under soft distillation, where the student is trained on the teacher\'s full next-token distribution. But the fact that this also occurs under hard distillation-where the student only sees sampled tokens-raises a deeper question: when and how does subliminal learning actually occur? We answer this question through controlled experiments and mechanistic analysis. Our results show that subliminal learning does not need (global) token entanglement or logit leakage. Instead, it comes down to a small set of divergence tokens-rare cases where teachers with different biases would predict different tokens. Masking out these tokens mostly removes the hidden bias transfer. Mechanistically, divergence tokens reveal that early layers are critical. Surprisingly, finetuning even a single such early layer is sufficient for subliminal learning. Finally, we find that subliminal learning is fragile. Even small changes, like paraphrasing prompts, are usually sufficient to suppress it.', 'abstract_zh': '语言模型在蒸馏过程中可以传递隐藏的偏见：一种称为潜意识学习的现象', 'title_zh': '理解潜隐学习：在何种条件下以及通过何种机制隐藏的偏见转移'}
{'arxiv_id': 'arXiv:2509.23879', 'title': 'PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications', 'authors': 'Hitesh Laxmichand Patel, Amit Agarwal, Srikant Panda, Hansa Meghwani, Karan Dua, Paul Li, Tao Sheng, Sujith Ravi, Dan Roth', 'link': 'https://arxiv.org/abs/2509.23879', 'abstract': 'The reliability of Multimodal Large Language Models (MLLMs) in real-world settings is often undermined by sensitivity to irrelevant or distracting visual context, an aspect not captured by existing evaluation metrics. We introduce the \\textbf{Patch Context Robustness Index (PCRI)}, the first systematic and interpretable score for quantifying MLLM robustness to variations in visual context granularity, measuring performance changes between localized image patches and full-image input.\nApplying PCRI to 19 state-of-the-art MLLMs across 15 vision-language benchmarks, we find that most leading models remain brittle to background noise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating consistent robustness across tasks. PCRI analysis also highlights how different model architectures handle and integrate visual context, offering actionable diagnostic insight for both researchers and practitioners.\nPCRI enables rigorous comparison of context robustness, supporting principled model selection and guiding the development of future architectures and training strategies for robust, real-world deployment.', 'abstract_zh': '多模态大型语言模型在现实环境中的可靠性往往因对无关或分散注意力的视觉上下文的敏感性而受损，现有评估指标并未涵盖这一方面。我们提出了**Patch Context Robustness Index (PCRI)**，这是首个系统且可解释的评量指标，用于量化多模态大型语言模型对视觉上下文粒度变化的鲁棒性，通过测量局部图像 patches 和全图输入之间的性能变化来衡量。', 'title_zh': 'PCRI:测量企业应用中多模态模型的上下文 robustness'}
{'arxiv_id': 'arXiv:2509.23835', 'title': 'HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing', 'authors': 'Yukai Zhao, Menghan Wu, Xing Hu, Xin Xia', 'link': 'https://arxiv.org/abs/2509.23835', 'abstract': 'Large Language Models (LLMs) are widely used for code generation, but they face critical security risks when applied to practical production due to package hallucinations, in which LLMs recommend non-existent packages. These hallucinations can be exploited in software supply chain attacks, where malicious attackers exploit them to register harmful packages. It is critical to test LLMs for package hallucinations to mitigate package hallucinations and defend against potential attacks. Although researchers have proposed testing frameworks for fact-conflicting hallucinations in natural language generation, there is a lack of research on package hallucinations. To fill this gap, we propose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for package hallucinations. HFUZZER adopts fuzzing technology and guides the model to infer a wider range of reasonable information based on phrases, thereby generating enough and diverse coding tasks. Furthermore, HFUZZER extracts phrases from package information or coding tasks to ensure the relevance of phrases and code, thereby improving the relevance of generated tasks and code. We evaluate HFUZZER on multiple LLMs and find that it triggers package hallucinations across all selected models. Compared to the mutational fuzzing framework, HFUZZER identifies 2.60x more unique hallucinated packages and generates more diverse tasks. Additionally, when testing the model GPT-4o, HFUZZER finds 46 unique hallucinated packages. Further analysis reveals that for GPT-4o, LLMs exhibit package hallucinations not only during code generation but also when assisting with environment configuration.', 'abstract_zh': '大型语言模型（LLMs）在代码生成中广泛应用，但由于包幻想问题，它们在实际生产中面临严重安全风险。包幻想会导致恶意攻击者在软件供应链攻击中利用这些幻想注册有害包。测试LLMs以检测包幻想、减轻包幻想和抵御潜在攻击至关重要。尽管研究人员提出了针对自然语言生成中事实相矛盾幻想的测试框架，但关于包幻想的研究仍然不足。为填补这一空白，我们提出了HFUZZER，这是一种新颖的短语基础模糊测试框架，用于测试LLMs中的包幻想。HFUZZER采用模糊测试技术，引导模型基于短语推断更广泛的相关信息，从而生成足够的多样化的编码任务。此外，HFUZZER从包信息或编码任务中提取短语，确保短语和代码的相关性，从而提高生成任务和代码的相关性。我们对多种LLMs进行了评估，发现HFUZZER能够在所有选定的模型中触发包幻想。与变异模糊测试框架相比，HFUZZER识别出2.60倍以上的独特幻想包，并生成更多样化的任务。此外，当测试GPT-4o模型时，HFUZZER发现了46个独特幻想包。进一步分析显示，对于GPT-4o，LLMs不仅在代码生成时表现出包幻想，在环境配置辅助时也存在包幻想。', 'title_zh': 'HFuzzer：基于短语级 fuzzing 测试大型语言模型的包幻觉'}
{'arxiv_id': 'arXiv:2509.23812', 'title': 'Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models', 'authors': 'Dianshu Liao, Xin Yin, Shidong Pan, Chao Ni, Zhenchang Xing, Xiaoyu Sun', 'link': 'https://arxiv.org/abs/2509.23812', 'abstract': 'Unit testing is essential for software quality assurance, yet writing and maintaining tests remains time-consuming and error-prone. To address this challenge, researchers have proposed various techniques for automating unit test generation, including traditional heuristic-based methods and more recent approaches that leverage large language models (LLMs). However, these existing approaches are inherently path-insensitive because they rely on fixed heuristics or limited contextual information and fail to reason about deep control-flow structures. As a result, they often struggle to achieve adequate coverage, particularly for deep or complex execution paths. In this work, we present a path-sensitive framework, JUnitGenie, to fill this gap by combining code knowledge with the semantic capabilities of LLMs in guiding context-aware unit test generation. After extracting code knowledge from Java projects, JUnitGenie distills this knowledge into structured prompts to guide the generation of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex focal methods from ten real-world Java projects. The results show that JUnitGenie generates valid tests and improves branch and line coverage by 29.60% and 31.00% on average over both heuristic and LLM-based baselines. We further demonstrate that the generated test cases can uncover real-world bugs, which were later confirmed and fixed by developers.', 'abstract_zh': '基于路径敏感的框架JUnitGenie：结合代码知识和大规模语言模型Semantic能力指导上下文感知的单元测试生成', 'title_zh': '穿越迷宫：基于路径敏感的单元测试生成方法与大型语言模型'}
{'arxiv_id': 'arXiv:2509.23809', 'title': 'Tequila: Trapping-free Ternary Quantization for Large Language Models', 'authors': 'Hong Huang, Decheng Wu, Rui Cen, Guanghua Yu, Zonghang Li, Kai Liu, Jianchen Zhu, Peng Chen, Xue Liu, Dapeng Wu', 'link': 'https://arxiv.org/abs/2509.23809', 'abstract': 'Quantization techniques are essential for the deployment of Large Language Models (LLMs) on edge devices. However, prevailing methods often rely on mixed-precision multiplication that lacks efficient hardware support, making it not feasible. Ternary weight quantization addresses this by constraining weights to {-1, 0, 1}, replacing expensive multiplications with hardware-efficient additions. However, such aggressive compression leads to significant accuracy degradation, even after costly quantization-aware training with massive data. We identify the core issue as deadzone trapping: a large number of weights are trapped at the deadzone boundary. This occurs because these weights receive only noisy, uninformative gradients, preventing stable escape from the deadzone and severely impeding model capacity and optimization. To address this issue, we propose Tequila, a trapping-free quantization optimization method that reactivates deadzone-trapped weights by repurposing them as dynamic biases. This allows the repurposed weights to provide a continuous signal in the forward pass and, critically, receive direct, meaningful gradient signals during backpropagation, thereby enhancing model capacity and optimization with nearly zero inference overhead. Extensive evaluations demonstrate that Tequila outperforms state-of-the-art (SOTA) ternary quantization methods across five benchmarks. Specifically, on the ARC benchmark, it achieves >4% accuracy gain over the SOTA baseline, nearly matching full-precision performance (within <1% gap) with a 3.0x inference speedup. Consequently, Tequila offers a highly practical and efficient implementation for the deployment of advanced LLMs in resource-constrained environments. The code is available at this https URL.', 'abstract_zh': '三元权重量化优化方法：Tequila', 'title_zh': 'Tequila: 无约束三元量化大型语言模型'}
{'arxiv_id': 'arXiv:2509.23803', 'title': 'FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents', 'authors': 'Pramit Saha, Joshua Strong, Divyanshu Mishra, Cheng Ouyang, J.Alison Noble', 'link': 'https://arxiv.org/abs/2509.23803', 'abstract': 'Federated learning (FL) allows collaborative model training across healthcare sites without sharing sensitive patient data. However, real-world FL deployment is often hindered by complex operational challenges that demand substantial human efforts. This includes: (a) selecting appropriate clients (hospitals), (b) coordinating between the central server and clients, (c) client-level data pre-processing, (d) harmonizing non-standardized data and labels across clients, and (e) selecting FL algorithms based on user instructions and cross-client data characteristics. However, the existing FL works overlook these practical orchestration challenges. These operational bottlenecks motivate the need for autonomous, agent-driven FL systems, where intelligent agents at each hospital client and the central server agent collaboratively manage FL setup and model training with minimal human intervention. To this end, we first introduce an agent-driven FL framework that captures key phases of real-world FL workflows from client selection to training completion and a benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to autonomously coordinate healthcare FL. Our framework incorporates 40 FL algorithms, each tailored to address diverse task-specific requirements and cross-client characteristics. Furthermore, we introduce a diverse set of complex tasks across 201 carefully curated datasets, simulating 6 modality-specific real-world healthcare environments, viz., Dermatoscopy, Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic performance of 14 open-source and 10 proprietary LLMs spanning small, medium, and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3 can automate various stages of the FL pipeline, our results reveal that more complex, interdependent tasks based on implicit goals remain challenging for even the strongest models.', 'abstract_zh': '联邦学习（FL）允许在不共享敏感患者数据的情况下，在医疗健康站点之间协作训练模型。然而，现实世界的FL部署经常受到复杂操作挑战的阻碍，这些挑战需要大量的人力投入。这包括：(a) 选择合适的客户端（医院），(b) 中央服务器与客户端之间的协调，(c) 客户端级数据预处理，(d) 跨客户端标准化数据和标签的协调，以及(e) 根据用户指令和跨客户端数据特性选择FL算法。然而，现有的FL工作忽视了这些实际协调挑战。这些操作瓶颈促使了自主、代理驱动的FL系统的需要，其中每个医院客户端和中央服务器的智能代理协作管理FL设置和模型训练，最大限度地减少人为干预。为此，我们首先介绍了一个代理驱动的FL框架，涵盖了从客户端选择到训练完成的真实世界FL工作流程的关键阶段，并引入了一个名为FedAgentBench的基准，评估LLM代理自主协调医疗健康FL的能力。我们的框架包括40种定制的FL算法，以解决各种特定任务和跨客户端特性的需求。此外，我们引入了201个精心选择的数据集上的一系列复杂任务，模拟了6种特定模态的真实世界医疗健康环境，即皮肤镜检查、超声波、眼底成像、病理学、磁共振成像和X射线。我们评估了14种开源和10种专有的LLM在小、中、大型模型规模上的代理性能。虽然一些代理核心如GPT-4.1和DeepSeek V3可以自动化FL管道的各个阶段，但我们的结果显示，基于隐含目标的更复杂、更相互依赖的任务即使对于最强的模型来说也仍然是具有挑战性的。', 'title_zh': 'FedAgentBench: 向自动化现实世界联邦医疗图像分析方向迈进，基于服务器-客户端LLM代理'}
{'arxiv_id': 'arXiv:2509.23799', 'title': 'Enhancing LLM Steering through Sparse Autoencoder-Based Vector Refinement', 'authors': 'Anyi Wang, Xuansheng Wu, Dong Shu, Yunpu Ma, Ninghao Liu', 'link': 'https://arxiv.org/abs/2509.23799', 'abstract': 'Steering has emerged as a promising approach in controlling large language models (LLMs) without modifying model parameters. However, most existing steering methods rely on large-scale datasets to learn clear behavioral information, which limits their applicability in many real-world scenarios. The steering vectors extracted from small dataset often contain task-irrelevant noising features, which degrades their effectiveness. To refine the steering vectors learned from limited data, we introduce Refinement of Steering Vector via Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise and augment the steering vectors. In our framework, we first remove task-irrelevant features according to their semantics provided by SAEs, and then enrich task-relevant features missing from the small dataset through their semantic similarity to the identified relevant features. Extensive experiments demonstrate that the proposed SAE-RSV substantially outperforms all the baseline methods including supervised fine-tuning. Our findings show that effective steering vector can be constructed from limited training data by refining the original steering vector through SAEs.', 'abstract_zh': '基于稀疏自编码器的Steering向量精炼方法：从稀疏数据中构建有效的控制向量', 'title_zh': '通过稀疏自编码器基于的向量精炼增强LLM导航'}
{'arxiv_id': 'arXiv:2509.23773', 'title': 'Knowledge Homophily in Large Language Models', 'authors': 'Utkarsh Sahu, Zhisheng Qi, Mahantesh Halappanavar, Nedim Lipka, Ryan A. Rossi, Franck Dernoncourt, Yu Zhang, Yao Ma, Yu Wang', 'link': 'https://arxiv.org/abs/2509.23773', 'abstract': 'Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering.', 'abstract_zh': '大规模语言模型（LLMs）作为支持知识密集型应用如问答和事实核查的神经知识库的研究日益增多。然而，它们知识结构的组织方式尚未被探索。受认知神经科学发现的启发，如语义聚类和线索效应，即知道一个事实会增加回忆相关事实的可能性，我们研究了LLMs中的类似知识同质性模式。为此，我们通过知识检查将LLM知识映射到图表示中，分别在三元组和实体层面进行。随后，我们分析了实体与其邻居之间的知识能力关系，发现LLMs倾向于在图上位置更近的实体具有相似的知识水平。受这一同质性原理的启发，我们提出了一种图神经网络（GNN）回归模型，通过利用其邻居评分来估计三元组的实体层面知识能力评分。预测的知识能力使我们能够优先检查知识较少的三元组，从而在相同的注标预算下最大化知识覆盖。这不仅提高了为细调将知识注入LLMs时的主动注标效率，还增强了在推理密集型问答中多跳路径检索的能力。', 'title_zh': '大型语言模型中的知识同质性'}
{'arxiv_id': 'arXiv:2509.23767', 'title': 'From Personal to Collective: On the Role of Local and Global Memory in LLM Personalization', 'authors': 'Zehong Wang, Junlin Wu, ZHaoxuan Tan, Bolian Li, Xianrui Zhong, Zheli Liu, Qingkai Zeng', 'link': 'https://arxiv.org/abs/2509.23767', 'abstract': 'Large language model (LLM) personalization aims to tailor model behavior to individual users based on their historical interactions. However, its effectiveness is often hindered by two key challenges: the \\textit{cold-start problem}, where users with limited history provide insufficient context for accurate personalization, and the \\textit{biasing problem}, where users with abundant but skewed history cause the model to overfit to narrow preferences. We identify both issues as symptoms of a common underlying limitation, i.e., the inability to model collective knowledge across users. To address this, we propose a local-global memory framework (LoGo) that combines the personalized local memory with a collective global memory that captures shared interests across the population. To reconcile discrepancies between these two memory sources, we introduce a mediator module designed to resolve conflicts between local and global signals. Extensive experiments on multiple benchmarks demonstrate that LoGo consistently improves personalization quality by both warming up cold-start users and mitigating biased predictions. These results highlight the importance of incorporating collective knowledge to enhance LLM personalization.', 'abstract_zh': '大规模语言模型个性化旨在根据用户的历史交互行为定制模型行为。然而，其有效性往往受到两个关键挑战的阻碍：冷启动问题，即历史有限的用户提供的上下文不足，难以进行准确个性化；以及偏差问题，即历史丰富但有所偏倚的用户使模型过度适应狭隘的偏好。我们识别这两个问题为共同基础限制的症状，即无法建模用户间的集体知识。为此，我们提出了一种局部-全局记忆框架（LoGo），该框架结合了个性化局部记忆和一个捕获人口共享兴趣的全局记忆模块。为了统一这两种记忆来源之间的差异，我们引入了一个调解模块，旨在解决局部和全局信号之间的冲突。在多个基准上的广泛实验表明，LoGo能够通过预热冷启动用户和缓解偏差预测的一致提高个性化质量。这些结果突显了在增强大规模语言模型个性化中融入集体知识的重要性。', 'title_zh': '从个人到集体：局部记忆与全局记忆在大语言模型个性化中的作用'}
{'arxiv_id': 'arXiv:2509.23765', 'title': 'Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality', 'authors': 'Junliang Li, Yucheng Wang, Yan Chen, Yu Ran, Ruiqing Zhang, Jing Liu, Hua Wu, Haifeng Wang', 'link': 'https://arxiv.org/abs/2509.23765', 'abstract': 'Hallucination and factuality deficits remain key obstacles to the reliability of large language models (LLMs) in long-form generation. Existing reinforcement learning from human feedback (RLHF) frameworks primarily rely on preference rewards, yet they often overlook the model\'s internal knowledge boundaries, exacerbating the so-called "hallucination tax". To address this challenge, we propose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a novel framework that focuses on the knowledge consistency between the policy model\'s expressed knowledge and the base model\'s parametric knowledge, and introduces a Dual-Fact Alignment mechanism to jointly optimize factual recall and precision. Specifically, KLCF leverages pretrained knowledge boundaries to construct fact checklist, guiding online reinforcement learning to improve factual coverage and recall; simultaneously, it trains a self-assessment module based on the base model\'s internal knowledge to enhance factual precision during generation. Unlike prior methods that rely on external retrieval or heavy verification, our reward design is fully external-knowledge-free and lightweight, making KLCF efficient and easily scalable to large-scale training. Experimental results demonstrate that KLCF substantially improves factuality metrics across multiple long-form benchmarks and effectively alleviates model hallucinations.', 'abstract_zh': '大语言模型（LLMs）在长文生成中的幻觉和事实性缺陷仍是可靠性的主要障碍。现有的基于人类反馈的强化学习（RLHF）框架主要依赖偏好奖励，但往往忽视了模型的内部知识边界，加剧了所谓的“幻觉税”。为应对这一挑战，我们提出了一种新的框架——知识级一致性强化学习框架（KLCF），该框架关注策略模型表达的知识与基模型参数知识之间的一致性，并引入了双事实对齐机制以联合优化事实召回率和精确度。具体而言，KLCF 利用预训练的知识边界构建事实清单，引导在线强化学习以提高事实覆盖度和召回率；同时，基于基模型的内部知识训练自我评估模块，以在生成过程中增强事实精确度。与依赖外部检索或重验证的先前方法不同，我们的奖励设计完全不需要外部知识且轻量级，使得 KLCF 高效且易于大规模训练。实验结果表明，KLCF 在多个长文基准测试中显著提高了事实性指标，并有效缓解了模型的幻觉现象。', 'title_zh': '知识层面一致性强化学习：长篇事实对齐的双事实一致性方法'}
{'arxiv_id': 'arXiv:2509.23755', 'title': 'Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis', 'authors': 'Chao Wang, Rui-Chen Zheng, Yang Ai, Zhen-Hua Ling', 'link': 'https://arxiv.org/abs/2509.23755', 'abstract': 'The integration of speech into Large Language Models (LLMs) has substantially expanded their capabilities, but often at the cost of weakening their core textual competence. This degradation limits the ability of speech-enabled LLMs to fully exploit their pre-trained text-based knowledge. In this work, we analyze the underlying mechanisms of this issue through a focused study of the widely used encoder-adaptor paradigm. We propose an analytical framework based on parameter importance estimation, which reveals that fine-tuning for speech introduces a textual importance distribution shift: the layer-wise allocation of parameters critical to textual reasoning is disrupted. Building on this insight, we investigate two mitigation strategies: layer-wise learning rate scheduling and Low-Rank Adaptation (LoRA), both aim to preserve the original parameter distribution. Experimental results show that both approaches better maintain textual competence than full fine-tuning, while also improving downstream spoken question answering performance. Furthermore, our analysis offers a principled explanation for the effectiveness of the proposed mitigation strategies, linking their benefits to the structural properties of textual knowledge in LLMs.', 'abstract_zh': '将语音整合到大型语言模型中显著扩展了其能力，但往往以削弱其核心文本能力为代价。这种退化限制了语音增强型大型语言模型充分利用其预训练文本知识的能力。在本工作中，我们通过集中研究广泛应用的编码器-适配器范式来分析这一问题的根本机制。我们提出了一种基于参数重要性估计的分析框架，揭示了语音微调导致文本重要性分布的变化：层内关键文本推理参数的分配被打乱。基于这一洞见，我们探讨了两种缓解策略：层内学习率调度和低秩适配（LoRA），两者都旨在保持原始参数分布。实验结果表明，这两种方法在保持文本能力方面优于全面微调，同时还能提高下游语音问题回答性能。此外，我们的分析为所提出的缓解策略的有效性提供了理论解释，将其益处与大型语言模型中文本知识的结构特性联系起来。', 'title_zh': '通过参数重要性分析理解语音LLM中文本能力退化'}
{'arxiv_id': 'arXiv:2509.23729', 'title': 'LUQ: Layerwise Ultra-Low Bit Quantization for Multimodal Large Language Models', 'authors': 'Shubhang Bhatnagar, Andy Xu, Kar-Han Tan, Narendra Ahuja', 'link': 'https://arxiv.org/abs/2509.23729', 'abstract': 'Large Language Models (LLMs) with multimodal capabilities have revolutionized vision-language tasks, but their deployment often requires huge memory and computational resources. While post-training quantization (PTQ) has successfully compressed language models to as low as 1-bit precision without significant performance loss, its effectiveness for multimodal LLMs (MLLMs) remains relatively unexplored. In this paper, we present the first study on ultra-low bit (<4-bit) quantization for multimodal LLMs. Our analysis reveals that multimodal tokens and intermediate layer activations produced by them exhibit significantly higher statistical variance and entropy compared to text tokens, making them less tolerant to ultra-low bit quantization. However, the activation distributions of multimodal tokens varies significantly over different layers, with some layers having lower entropy activation distributions. We empirically show that such layers in these models can better tolerate ultra-low bit quantization. Building on these insights, we propose a novel strategy for MLLM quantization, LUQ: Layerwise Ultra-Low Bit Quantization, which selectively applies ultra-low bit quantization to layers that are more resilient to it. Additionally, we also show that using a mix of multimodal tokens (image and text) for PTQ boosts VQA performance in the ultra-low bit regime. We evaluate our method on LLaVA-1.5 and Qwen-2.5-VL across 9 popular VQA benchmarks. The resulting LUQ models use 40% and 31% less memory than their 4-bit counterparts, respectively, while exhibiting a performance degradation of less than 10% on the MME benchmark.', 'abstract_zh': '超低比特（<4比特）量化在多模态大语言模型中的研究', 'title_zh': '逐层超低比特量化：面向多模态大型语言模型'}
{'arxiv_id': 'arXiv:2509.23678', 'title': 'Towards a Comprehensive Scaling Law of Mixture-of-Experts', 'authors': 'Guoliang Zhao, Yuhan Fu, Shuaipeng Li, Xingwu Sun, Ruobing Xie, An Wang, Weidong Han, Zhen Yang, Weixuan Sun, Yudong Zhang, Cheng-zhong Xu, Di Wang, Jie Jiang', 'link': 'https://arxiv.org/abs/2509.23678', 'abstract': 'Mixture-of-Experts (MoE) models have become the consensus approach for enabling parameter-efficient scaling and cost-effective deployment in large language models. However, existing scaling laws for dense models are inapplicable to MoE models, which stems from three critical challenges: the multiplicity of influencing factors, their intricate coupling relationships and the non-monotonic nature of their performance impacts. They collectively necessitate a fine-grained investigation into MoE-specific scaling laws. In this work, we perform a systematic decomposition of MoE settings, identifying five key factors that influence model performance from both size and structural perspectives (data size ($D$), total model size ($N$), activated model size ($N_a$), number of active experts ($G$) and the ratio of shared experts ($S$)). Specifically, we design $446$ controlled experiments to characterize their marginal effects, ultimately constructing a comprehensive and precise joint MoE scaling law that considers all essential factors. Furthermore, we derive the theoretically optimal and practically efficiency-aware optimal configurations for $G$, $S$ and $N_a/N$ with detailed analyses. Our results demonstrate that the optimal settings for $G$ and $S$ are independent of both the model architecture and data size. With the scaling of $N$, the optimal activation parameter ratio of $N_a/N$ becomes sparser. Our proposed MoE scaling law could function as an accurate and insightful guidance to facilitate future MoE model design and training.', 'abstract_zh': 'MoE模型已成为在大型语言模型中实现参数高效的扩展和成本效益部署的共识方法。然而，现有的密集模型扩展规律对MoE模型不适用，这源于三个关键挑战：影响因素的多重性、它们的复杂耦合关系以及性能影响的非单调性。这些因素共同要求对MoE特定的扩展规律进行精细的研究。在本工作中，我们系统地分解了MoE设置，从模型大小和结构两个视角识别出五个关键因素，它们影响模型性能（数据大小$D$，总模型大小$N$，激活模型大小$N_a$，活跃专家数量$G$，以及共享专家比例$S$）。具体而言，我们设计了446个受控实验来表征它们的边际效应，最终构建了一个全面而精确的联合MoE扩展定律，考虑了所有基本因素。此外，我们推导出$G$、$S$和$N_a/N$的理论最优配置和实际效率感知最优配置，并进行了详细的分析。我们的结果显示，$G$和$S$的最佳设置与模型架构和数据大小无关。随着$N$的扩展，$N_a/N$的最佳激活参数比变得更为稀疏。我们提出的MoE扩展定律可以作为准确而深入的指导，以促进未来MoE模型的设计和训练。', 'title_zh': '面向混合专家模型的综合性缩放律研究'}
{'arxiv_id': 'arXiv:2509.23673', 'title': 'RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks', 'authors': 'Amit Agarwal, Hitesh Laxmichand Patel, Srikant Panda, Hansa Meghwani, Jyotika Singh, Karan Dua, Paul Li, Tao Sheng, Sujith Ravi, Dan Roth', 'link': 'https://arxiv.org/abs/2509.23673', 'abstract': "Multimodal Large Language Models (MLLMs) have achieved impressive results on vision-language benchmarks, yet it remains unclear whether these benchmarks assess genuine global reasoning or allow success via localized visual cues. Existing evaluation methods do not explicitly measure this distinction, hindering effective dataset curation and real-world focused model development.\nWe introduce Region Comprehension Index (RCI), the first model-based score to directly quantify a dataset's reliance on global versus local visual information. RCI systematically compares reference-model performance on image patches versus full images, revealing if tasks require holistic image understanding or can be solved with partial or localized visual cues.\nWhen applying RCI to 13 widely used multimodal benchmarks, we observed that most of them favor localized reasoning and exhibit significant spatial biases, indicating potential risks in real-world applications. RCI equips researchers & practitioners with an actionable tool for diagnosing & mitigating these biases, enabling the construction of datasets and benchmarks to foster the development of robust, enterprise-ready multimodal systems.", 'abstract_zh': '多模态大型语言模型（MLLMs）在视觉-语言基准测试中取得了显著成果，但尚不清楚这些基准测试是否评估了真正的全局推理能力，还是允许通过局部视觉线索取得成功。现有的评估方法并未明确测量这种区别，妨碍了高效的数据集编排和面向现实世界的模型开发。\n\n我们提出了区域理解指数（RCI），这是第一个直接量化数据集对全局与局部视觉信息依赖程度的模型基准得分。RCI系统地比较了参考模型在图像片段与完整图像上的表现，揭示了任务需要整体图像理解还是可以通过部分或局部视觉线索解决。\n\n将RCI应用于13个广泛使用的多模态基准测试时，我们观察到大多数基准测试倾向于局部推理，并表现出显著的空间偏见，这表明在实际应用中可能存在风险。RCI为研究人员和实践者提供了一个可操作的工具，用于诊断和缓解这些偏见，从而促进稳健的企业级多模态系统的发展。', 'title_zh': 'RCI：评估多模态基准中全局和局部推理的能力评分'}
{'arxiv_id': 'arXiv:2509.23659', 'title': 'Aligning LLMs for Multilingual Consistency in Enterprise Applications', 'authors': 'Amit Agarwal, Hansa Meghwani, Hitesh Laxmichand Patel, Tao Sheng, Sujith Ravi, Dan Roth', 'link': 'https://arxiv.org/abs/2509.23659', 'abstract': 'Large language models (LLMs) remain unreliable for global enterprise applications due to substantial performance gaps between high-resource and mid/low-resource languages, driven by English-centric pretraining and internal reasoning biases. This inconsistency undermines customer experience and operational reliability in multilingual settings such as customer support, content moderation, and information retrieval. Even with advanced Retrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy drop in non-English languages compared to English.\nWe propose a practical, batch-wise alignment strategy for fine-tuning LLMs, leveraging semantically equivalent multilingual data in each training batch to directly align model outputs across languages. This approach improves non-English accuracy by up to 23.9\\% without compromising English performance, model reasoning, or retrieval quality. Our method is simple to implement, scalable, and integrates seamlessly with existing LLM training \\& deployment pipelines, enabling more robust and equitable multilingual AI solutions in industry.', 'abstract_zh': '大型语言模型（LLMs）由于高资源和中/低资源语言之间存在显著的性能差距，在全球企业应用中仍不可靠，这一差距主要源于以英语为中心的预训练和内部推理偏见。这种不一致性在多语言环境中（如客户服务、内容审核和信息检索）削弱了客户体验和操作可靠性。即使使用先进的检索增强生成（RAG）系统，我们观察到非英语语言的准确性相对于英语下降了最多29%。\n\n我们提出了一种实用的批次对齐策略，利用每个训练批次中的语义等效多语言数据直接对齐不同语言的模型输出。该方法在不牺牲英语性能、模型推理或检索质量的情况下，将非英语语言的准确性提高到最多23.9%。该方法易于实现、可扩展，并能无缝集成到现有的LLM训练与部署流程中，从而促进更稳健和公平的多语言AI解决方案在行业中的应用。', 'title_zh': '为企业应用中多语言一致性对齐大型语言模型'}
{'arxiv_id': 'arXiv:2509.23595', 'title': 'Timber: Training-free Instruct Model Refining with Base via Effective Rank', 'authors': 'Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Zenan Xu, Ngai Wong', 'link': 'https://arxiv.org/abs/2509.23595', 'abstract': 'Post-training, which elicits a pretrained Base model into the corresponding Instruct model, is widely considered to be superficial. In this work, we first reinforce this hypothesis by providing novel quantitative evidence from the weight level that the effective rank (eRank) remains negligibly changed. However, this superficiality also suffers a critical trade-off, improving the exploitation capabilities at the cost of limiting its exploration. To tackle this issue, we propose Timber, a simple yet effective training-free method that enhances the exploration capability of the Instruct model while preserving its exploitation. The key insight is to partially revert Instruct towards the paired Base model by subtle yet targeted refinement of the weight deltas. Extensive experiments on Llama and Qwen series demonstrate that Timber consistently improves vanilla Instruct models, particularly on Pass@k performance. Our findings offer new insights into the post-training stage at the weight level and practical strategies to refine the Instruct model without training.', 'abstract_zh': 'Post-训练通过将预训练基模型激发为相应的指令模型，通常被认为是一种浅层方法。在本文中，我们首先通过在权重层面提供新颖的定量证据来加强这一假设，即有效的秩（eRank）几乎保持不变。然而，这种浅层性也面临一个关键的权衡：改进利用能力的同时限制了探索能力。为解决这一问题，我们提出了一种简单而有效的无需训练方法——Timber，该方法增强了指令模型的探索能力，同时保持其利用能力。关键见解是通过微妙且有针对性的权重差值调整部分逆转指令模型向配对基模型。针对Llama和Qwen系列的 extensive 实验表明，Timber 一致地改进了 vanilla 指令模型，特别是在 Pass@k 性能上。我们的发现提供了关于权重层面后训练阶段的新见解，并提出了在无需训练的情况下细化指令模型的实用策略。', 'title_zh': 'Timber: 不依赖训练的基模型有效秩驱动的指令模型精炼'}
{'arxiv_id': 'arXiv:2509.23586', 'title': 'Improving the Efficiency of LLM Agent Systems through Trajectory Reduction', 'authors': 'Yuan-An Xiao, Pengfei Gao, Chao Peng, Yingfei Xiong', 'link': 'https://arxiv.org/abs/2509.23586', 'abstract': "Multi-turn agent systems based on Large Language Models (LLMs) have been increasingly popular for software engineering tasks. While LLM agents show decent effectiveness, the high computational cost of input tokens due to the ever-growing trajectory remains an efficiency concern for their applications. Efficiency is largely neglected in existing studies and agent products, and this paper fills the gap by introducing an inference-time trajectory reduction approach to reduce the cost of agents.\nThrough analyzing existing agent trajectories, we demonstrate that useless, redundant, and expired information is widespread in all trajectories, which can be identified and reduced without harming the agent's performance. We then design a simple yet effective trajectory reduction approach, AgentDiet, which automatically removes such waste information. We implement AgentDiet on a top-performing coding agent, and the evaluation on two LLMs and two benchmarks shows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final computational cost by 21.1% ~ 35.9%, while maintaining the same agent performance. This indicates that trajectory reduction is a promising direction for agent systems.", 'abstract_zh': '基于大型语言模型（LLMs）的多轮代理系统在软件工程任务中日益流行。尽管LLM代理表现出色，但由于轨迹不断增长导致的输入令牌计算成本高，仍然是应用程序中的效率问题。这一效率问题在现有研究和代理产品中往往被忽视，本文通过引入推理时轨迹缩减方法来降低代理的成本，填补了这一空白。', 'title_zh': '通过轨迹减少提高大语言模型代理系统的效率'}
{'arxiv_id': 'arXiv:2509.23574', 'title': 'Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales', 'authors': 'Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Yang Xiang, Buzhou Tang', 'link': 'https://arxiv.org/abs/2509.23574', 'abstract': "Chain-of-thought (CoT) distillation aims to enhance small language models' (SLMs) reasoning by transferring multi-step reasoning capability from the larger teacher models. However, existing work underestimates rationale quality, focusing primarily on data quantity, which may transfer noisy or incorrect information to the student model. To address the above issues, we proposed \\textbf{M}odel-\\textbf{O}riented \\textbf{R}ationale \\textbf{S}election \\textbf{D}istillation (MoRSD), which can discern and select high quality rationales for distillation to improve performance further. We further propose a Rationale Difficulty (RD) metric to measure the ability of the student model to generate the correct answer under a given rationale. Compared to the baseline, we achieved 4.6$\\%$ average improvement on seven datasets over three tasks, using fewer rationales by controlling their accuracy, diversity, and difficulty. Our results reveal that a small portion of the high quality rationales can enhance the reasoning ability of student models than the entire dataset. Our method promises to be a possible solution for efficient CoT distillation. Our code will be released in this https URL.", 'abstract_zh': '基于模型导向的高质量论据蒸馏（Model-Oriented Rationale Selection Distillation）旨在通过从大型教师模型转移多步推理能力来增强小型语言模型（SLMs）的推理能力。然而，现有工作未能充分重视论据质量，主要关注数据量，这可能导致噪声或错误信息转移到学生模型中。为解决上述问题，我们提出了基于模型导向的高质量论据选择蒸馏（MoRSD），可以区分和选择高质量的论据以进行蒸馏，从而进一步提高性能。我们还提出了论据难度（RD）度量，用于衡量学生模型在给定论据下生成正确答案的能力。与基线相比，我们通过控制论据的准确性、多样性和难度，在三项任务的七个数据集上实现了平均4.6%的性能提升。我们的结果表明，一小部分高质量的论据可以比整个数据集更有效地增强学生模型的推理能力。我们的方法有望成为高效链推理蒸馏的一种可能解决方案。我们的代码将发布在https://github.com/Qwen233/Model-Oriented-Rationale-Selection-Distillation。', 'title_zh': '基于少量理由实现高效CoT提纯：自我引导的理由选择器以更好性能选择更少的理由'}
{'arxiv_id': 'arXiv:2509.23573', 'title': 'Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence', 'authors': 'Yuqiao Meng, Luoxi Tang, Feiyang Yu, Jinyuan Jia, Guanhua Yan, Ping Yang, Zhaohan Xi', 'link': 'https://arxiv.org/abs/2509.23573', 'abstract': 'Large Language Models (LLMs) are intensively used to assist security analysts in counteracting the rapid exploitation of cyber threats, wherein LLMs offer cyber threat intelligence (CTI) to support vulnerability assessment and incident response. While recent work has shown that LLMs can support a wide range of CTI tasks such as threat analysis, vulnerability detection, and intrusion defense, significant performance gaps persist in practical deployments. In this paper, we investigate the intrinsic vulnerabilities of LLMs in CTI, focusing on challenges that arise from the nature of the threat landscape itself rather than the model architecture. Using large-scale evaluations across multiple CTI benchmarks and real-world threat reports, we introduce a novel categorization methodology that integrates stratification, autoregressive refinement, and human-in-the-loop supervision to reliably analyze failure instances. Through extensive experiments and human inspections, we reveal three fundamental vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization, that limit LLMs in effectively supporting CTI. Subsequently, we provide actionable insights for designing more robust LLM-powered CTI systems to facilitate future research.', 'abstract_zh': '大型语言模型（LLMs）在协助安全分析师对抗快速发展的网络威胁中被广泛使用，其中LLMs提供网络威胁情报（CTI）以支持漏洞评估和事件响应。尽管近期研究表明LLMs可以支持广泛范围的CTI任务，如威胁分析、漏洞检测和入侵防御，但在实际部署中仍存在显著的性能差距。本文探讨了LLMs在CTI中的内在脆弱性，重点关注源自威胁环境本质的挑战而非模型架构本身。通过跨多个CTI基准和真实世界威胁报告的大规模评估，我们引入了一种新的分类方法，该方法结合了分层、自回归精炼和人工监督，以可靠地分析失败实例。通过广泛的实验和人工检查，我们揭示了三种限制LLMs有效支持CTI的基本脆弱性：虚假相关性、矛盾的知识以及受限的泛化能力。随后，我们提供了设计更健壮的LLM赋能CTI系统的实用见解，以促进未来的研究。', 'title_zh': '揭示LLM辅助下的网络威胁intelligence脆弱性'}
{'arxiv_id': 'arXiv:2509.23571', 'title': 'Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting', 'authors': 'Yuqiao Meng, Luoxi Tang, Feiyang Yu, Xi Li, Guanhua Yan, Ping Yang, Zhaohan Xi', 'link': 'https://arxiv.org/abs/2509.23571', 'abstract': 'As cyber threats continue to grow in scale and sophistication, blue team defenders increasingly require advanced tools to proactively detect and mitigate risks. Large Language Models (LLMs) offer promising capabilities for enhancing threat analysis. However, their effectiveness in real-world blue team threat-hunting scenarios remains insufficiently explored. This paper presents CyberTeam, a benchmark designed to guide LLMs in blue teaming practice. CyberTeam constructs a standardized workflow in two stages. First, it models realistic threat-hunting workflows by capturing the dependencies among analytical tasks from threat attribution to incident response. Next, each task is addressed through a set of operational modules tailored to its specific analytical requirements. This transforms threat hunting into a structured sequence of reasoning steps, with each step grounded in a discrete operation and ordered according to task-specific dependencies. Guided by this framework, LLMs are directed to perform threat-hunting tasks through modularized steps. Overall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs through standardized threat analysis. We evaluate both leading LLMs and state-of-the-art cybersecurity agents, comparing CyberTeam against open-ended reasoning strategies. Our results highlight the improvements enabled by standardized design, while also revealing the limitations of open-ended reasoning in real-world threat hunting.', 'abstract_zh': '随着网络威胁规模和复杂性的不断增长，蓝队防御者越来越需要先进的工具来进行主动威胁检测和风险缓解。大规模语言模型（LLMs）在增强威胁分析方面展现出巨大的潜力。然而，它们在实际蓝队威胁狩猎场景中的有效性仍需进一步探索。本文介绍了CyberTeam，一个旨在引导LLMs进行蓝队实践的基准。CyberTeam通过两个阶段构建了一个标准化的工作流程。首先，通过捕捉从威胁归因到事件响应的分析任务之间的依赖关系，模拟现实的威胁狩猎工作流程。其次，通过一组定制的操作模块来解决每个任务，这些模块针对其特定的分析需求进行设计。这将威胁狩猎转化为一个结构化的推理步骤序列，每个步骤都基于一个离散的操作，并按照任务特定的依赖关系进行排序。在这一框架的指引下，LLMs通过模块化的步骤执行威胁狩猎任务。总体而言，CyberTeam整合了30个任务和9个操作模块，以引导LLMs进行标准化的威胁分析。我们评估了领先的大规模语言模型和最先进的人工智能安全代理，并将CyberTeam与开放性推理策略进行比较。我们的结果突显了标准化设计带来的改进，同时也揭示了开放性推理在真实世界威胁狩猎中的局限性。', 'title_zh': '基于标准化威胁猎捕对LLM辅助蓝队进行基准测试'}
{'arxiv_id': 'arXiv:2509.23542', 'title': 'On the Shelf Life of Fine-Tuned LLM Judges: Future Proofing, Backward Compatibility, and Question Generalization', 'authors': 'Janvijay Singh, Austin Xu, Yilun Zhou, Yefan Zhou, Dilek Hakkani-Tur, Shafiq Joty', 'link': 'https://arxiv.org/abs/2509.23542', 'abstract': "The LLM-as-a-judge paradigm is widely used in both evaluating free-text model responses and reward modeling for model alignment and finetuning. Recently, finetuning judges with judge-specific data has emerged as an often preferred choice over directly prompting frontier models as judges, as the former achieves better performance with smaller model sizes while being more robust to common biases. However, the standard evaluation ignores several practical concerns of finetuned judges regarding their real world deployment. In this paper, we identify and formalize three aspects that affect the shelf life of these judges: future proofing and backward compatibility -- how well judges finetuned on responses by today's generator models perform on responses by future models or past models, as well as question generalization -- how well judges generalize to unseen questions at test time. We study these three aspects in the math domain under a unified framework with varying train and test distributions, three SFT- and DPO-based finetuning algorithms and three different base models. Experiments suggest that future-proofing is challenging for most models, while backward compatibility is relatively easy, with DPO-trained models consistently improving performance. We further find that continual learning provides a more balanced adaptation to shifts between older and newer response distributions than training solely on stronger or weaker responses. Moreover, all models observe certain degrees of performance degradation when moving from questions seen during training to unseen ones, showing that current judges do not fully generalize to unseen questions. These findings provide insights into practical considerations for developing and deploying judge models in the face of ever-changing generators.", 'abstract_zh': 'LLM作为裁判的范式在评估自由文本模型响应和模型对齐及微调的奖励建模中广泛应用。最近，使用特定数据集微调裁判成为了一种常见的选择，相较于直接提示前沿模型作为裁判，该方法在模型规模更小的情况下表现更好且对常见偏差更具稳健性。然而，标准评估忽略了微调裁判在实际部署中的几个实际问题。在本文中，我们识别并形式化了影响这些裁判寿命的三个方面：未来兼容性和向后兼容性——今天生成模型生成的响应上微调的裁判在未来或过去模型生成的响应上表现良好程度，以及问题泛化——裁判在测试时对未见过的问题的表现。我们在数学领域，在变化的训练和测试分布下，使用三个SFT-和DPO基于的微调算法和三种不同的基模型，研究了这三个方面。实验表明，大多数模型在未来兼容性方面具有挑战性，而向后兼容性相对较容易，DPO训练的模型始终能提高性能。我们进一步发现，持续学习比仅在更强或更弱的响应上进行训练更能平衡旧响应分布和新响应分布之间的变化。此外，所有模型在从训练时见过的问题转移到未见过的问题时都观察到一定程度的性能下降，表明当前裁判未能完全泛化到未见过的问题。这些发现提供了一些建议，以应对不断变化的生成器在开发和部署裁判模型时的实际考虑。', 'title_zh': 'fine-tuned LLM法官的保质期：未来防护、向后兼容与问题泛化'}
{'arxiv_id': 'arXiv:2509.23525', 'title': 'Privy: Envisioning and Mitigating Privacy Risks for Consumer-facing AI Product Concepts', 'authors': 'Hao-Ping Lee, Yu-Ju Yang, Matthew Bilik, Isadora Krsek, Thomas Serban von Davier, Kyzyl Monteiro, Jason Lin, Shivani Agarwal, Jodi Forlizzi, Sauvik Das', 'link': 'https://arxiv.org/abs/2509.23525', 'abstract': 'AI creates and exacerbates privacy risks, yet practitioners lack effective resources to identify and mitigate these risks. We present Privy, a tool that guides practitioners through structured privacy impact assessments to: (i) identify relevant risks in novel AI product concepts, and (ii) propose appropriate mitigations. Privy was shaped by a formative study with 11 practitioners, which informed two versions -- one LLM-powered, the other template-based. We evaluated these two versions of Privy through a between-subjects, controlled study with 24 separate practitioners, whose assessments were reviewed by 13 independent privacy experts. Results show that Privy helps practitioners produce privacy assessments that experts deemed high quality: practitioners identified relevant risks and proposed appropriate mitigation strategies. These effects were augmented in the LLM-powered version. Practitioners themselves rated Privy as being useful and usable, and their feedback illustrates how it helps overcome long-standing awareness, motivation, and ability barriers in privacy work.', 'abstract_zh': 'AI创造并加剧了隐私风险，而从业者缺乏有效资源来识别和减轻这些风险。我们提出了Privy这一工具，引导从业者通过结构化的隐私影响评估来：(i) 在新型AI产品概念中识别相关风险，(ii) 提出适当的缓解措施。Privy是基于对11名从业者的形成性研究而设计的，该研究启发了两种版本的开发——一种基于大语言模型，另一种基于模板。我们通过一项包含24名独立从业者的被试间控制研究，评估了这两种版本的Privy，并邀请了13名独立隐私专家审查了他们的评估结果。研究结果表明，Privy帮助从业者生成了专家认为高质量的隐私评估报告：从业者能够识别相关风险并提出适当的缓解策略。大语言模型版本的效果更为显著。从业者自己也认为Privy既实用又易于使用，他们的反馈揭示了Privy如何帮助克服隐私工作中长期存在的认知、动机和能力障碍。', 'title_zh': 'Privy: 预见并缓解面向消费者的人工智能产品概念的隐私风险'}
{'arxiv_id': 'arXiv:2509.23519', 'title': 'ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search', 'authors': 'Zeyu Shen, Basileal Imana, Tong Wu, Chong Xiang, Prateek Mittal, Aleksandra Korolova', 'link': 'https://arxiv.org/abs/2509.23519', 'abstract': 'Retrieval-Augmented Generation (RAG) enhances Large Language Models by grounding their outputs in external documents. These systems, however, remain vulnerable to attacks on the retrieval corpus, such as prompt injection. RAG-based search systems (e.g., Google\'s Search AI Overview) present an interesting setting for studying and protecting against such threats, as defense algorithms can benefit from built-in reliability signals -- like document ranking -- and represent a non-LLM challenge for the adversary due to decades of work to thwart SEO.\nMotivated by, but not limited to, this scenario, this work introduces ReliabilityRAG, a framework for adversarial robustness that explicitly leverages reliability information of retrieved documents.\nOur first contribution adopts a graph-theoretic perspective to identify a "consistent majority" among retrieved documents to filter out malicious ones. We introduce a novel algorithm based on finding a Maximum Independent Set (MIS) on a document graph where edges encode contradiction. Our MIS variant explicitly prioritizes higher-reliability documents and provides provable robustness guarantees against bounded adversarial corruption under natural assumptions. Recognizing the computational cost of exact MIS for large retrieval sets, our second contribution is a scalable weighted sample and aggregate framework. It explicitly utilizes reliability information, preserving some robustness guarantees while efficiently handling many documents.\nWe present empirical results showing ReliabilityRAG provides superior robustness against adversarial attacks compared to prior methods, maintains high benign accuracy, and excels in long-form generation tasks where prior robustness-focused methods struggled. Our work is a significant step towards more effective, provably robust defenses against retrieved corpus corruption in RAG.', 'abstract_zh': 'Retrieval-Augmented Generation (RAG)通过将模型输出 grounding 在外部文档中来增强大型语言模型，但这些系统依然容易受到检索语料库攻击的影响，例如提示注入。基于RAG的搜索引擎（例如Google的Search AI Overview）为研究和抵御此类威胁提供了一个有趣的场景，防御算法可以从内置的可靠性信号（如文档排名）中受益，并且由于对抗者需要应对数十年的工作以阻止SEO，这为对抗者带来了非LLM挑战。受此场景启发，本文 introduce ReliabilityRAG，一种明确利用检索文档可靠性信息的鲁棒性框架。我们的第一项贡献采用图论视角来识别检索文档中的“一致多数”，以过滤掉恶意文档。我们引入了一种基于文档图中寻找最大独立集（MIS）的新算法，其中边编码矛盾信息。我们提出的MIS变体明确优先处理高可靠性文档，并在自然假设下提供可证明的鲁棒性保证。考虑到对大规模检索集进行精确MIS的计算成本，我们的第二项贡献是可扩展的加权抽样和聚合框架。该框架明确利用可靠性信息，保留部分鲁棒性保证的同时高效处理大量文档。我们展示了实验证据表明，ReliabilityRAG在对抗攻击中的鲁棒性优于先前方法，保持了高正当准确率，并在以前的鲁棒性方法难以处理的长文本生成任务中表现出色。本文是朝着更有效的、可证明鲁棒性防御方向的重要一步，以抵御RAG中检索语料库的破坏。', 'title_zh': 'ReliabilityRAG: 有效且可以证明 robust 的 RAG 基础网络搜索防御方法'}
{'arxiv_id': 'arXiv:2509.23515', 'title': 'From Human Annotation to Automation: LLM-in-the-Loop Active Learning for Arabic Sentiment Analysis', 'authors': 'Dania Refai, Alaa Dalaq, Doaa Dalaq, Irfan Ahmad', 'link': 'https://arxiv.org/abs/2509.23515', 'abstract': 'Natural language processing (NLP), particularly sentiment analysis, plays a vital role in areas like marketing, customer service, and social media monitoring by providing insights into user opinions and emotions. However, progress in Arabic sentiment analysis remains limited due to the lack of large, high-quality labeled datasets. While active learning has proven effective in reducing annotation efforts in other languages, few studies have explored it in Arabic sentiment tasks. Likewise, the use of large language models (LLMs) for assisting annotation and comparing their performance to human labeling is still largely unexplored in the Arabic context. In this paper, we propose an active learning framework for Arabic sentiment analysis designed to reduce annotation costs while maintaining high performance. We evaluate multiple deep learning architectures: Specifically, long short-term memory (LSTM), gated recurrent units (GRU), and recurrent neural networks (RNN), across three benchmark datasets: Hunger Station, AJGT, and MASAC, encompassing both modern standard Arabic and dialectal variations. Additionally, two annotation strategies are compared: Human labeling and LLM-assisted labeling. Five LLMs are evaluated as annotators: GPT-4o, Claude 3 Sonnet, Gemini 2.5 Pro, DeepSeek Chat, and LLaMA 3 70B Instruct. For each dataset, the best-performing LLM was used: GPT-4o for Hunger Station, Claude 3 Sonnet for AJGT, and DeepSeek Chat for MASAC. Our results show that LLM-assisted active learning achieves competitive or superior performance compared to human labeling. For example, on the Hunger Station dataset, the LSTM model achieved 93% accuracy with only 450 labeled samples using GPT-4o-generated labels, while on the MASAC dataset, DeepSeek Chat reached 82% accuracy with 650 labeled samples, matching the accuracy obtained through human labeling.', 'abstract_zh': '基于大规模语言模型的阿拉伯语情感分析主动学习框架', 'title_zh': '从人工标注到自动化：包含大语言模型的主动学习方法用于阿拉伯语情感分析'}
{'arxiv_id': 'arXiv:2509.23501', 'title': 'The Impact of Role Design in In-Context Learning for Large Language Models', 'authors': 'Hamidreza Rouzegar, Masoud Makrehchi', 'link': 'https://arxiv.org/abs/2509.23501', 'abstract': "In-context learning (ICL) enables Large Language Models (LLMs) to generate predictions based on prompts without additional fine-tuning. While prompt engineering has been widely studied, the impact of role design within prompts remains underexplored. This study examines the influence of role configurations in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models' performance across datasets, focusing on tasks like sentiment analysis, text classification, question answering, and math reasoning. Our findings suggest the potential of role-based prompt structuring to enhance LLM performance.", 'abstract_zh': '上下文学习（ICL）使大型语言模型（LLMs）能够在无需额外微调的情况下基于提示生成预测。虽然提示工程已经广泛研究，但提示中角色设计的影响仍然未被充分探索。本研究使用来自OpenAI的GPT-3.5和GPT-4o及来自Meta的Llama2-7b和Llama2-13b，探讨了零-shot和few-shot学习场景中角色配置对模型性能的影响，并专注于情感分析、文本分类、问题回答和数学推理等任务。我们的研究结果表明，基于角色的提示结构化有可能提升LLM的性能。', 'title_zh': '大型语言模型中基于情境学习的角色设计影响研究'}
{'arxiv_id': 'arXiv:2509.23435', 'title': 'AudioRole: An Audio Dataset for Character Role-Playing in Large Language Models', 'authors': 'Wenyu Li, Xiaoqi Jiao, Yi Chang, Guangyan Zhang, Yiwen Guo', 'link': 'https://arxiv.org/abs/2509.23435', 'abstract': 'The creation of high-quality multimodal datasets remains fundamental for advancing role-playing capabilities in large language models (LLMs). While existing works predominantly focus on text-based persona simulation, Audio Role-Playing (ARP) presents unique challenges due to the need for synchronized alignment of semantic content and vocal characteristics. To address this gap, we propose AudioRole, a meticulously curated dataset from 13 TV series spanning 1K+ hours with 1M+ character-grounded dialogues, providing synchronized audio-text pairs annotated with speaker identities and contextual metadata. In addition, to demonstrate the effectiveness of the dataset, we introduced ARP-Eval, a dual-aspect evaluation framework that assesses both response quality and role fidelity. Empirical validation showing GLM-4-Voice trained on AudioRole (which we called ARP-Model) achieve an average Acoustic Personalization score of 0.31, significantly outperforming the original GLM-4-voice and the more powerful model MiniCPM-O-2.6, which specifically supports role-playing in one-shot scenarios. The ARP-Model also achieves a Content Personalization score of 0.36, surpassing the untrained original model by about 38% and maintaining the same level as MiniCPM-O-2.6.\nAudioRole features dialogues from over 115 main characters, 6 trained ARP-Models that role-play different characters, and evaluation protocols. Together, they provide an essential resource for advancing audio-grounded role-playing research.', 'abstract_zh': '高质量多模态数据集的创建仍是推动大规模语言模型（LLMs）角色扮演能力发展的基础。尽管现有工作主要集中在基于文本的人格仿真上，音频角色扮演（ARP）因其需同步对齐语义内容和语音特征而面临独特挑战。为填补这一空白，我们提出AudioRole数据集，该数据集来自13部电视剧，覆盖超过1000小时的对话，包含超过100万条角色相关的对话，提供了标注有说话者身份和上下文元数据的同步音频-文本对。此外，为了展示数据集的有效性，我们引入了ARP-Eval评估框架，该框架从响应质量和角色保真度两个方面进行评估。实验证明，基于AudioRole训练的GLM-4-Voice（我们称其为ARP-Model）的平均声学个性化得分为0.31，显著优于原始的GLM-4-Voice和更强大的MiniCPM-O-2.6模型，后者专门支持单轮场景中的角色扮演。ARP-Model的语义个性化得分为0.36，比未训练的原始模型高出约38%，与MiniCPM-O-2.6保持相同水平。AudioRole数据集包含超过115个主要角色的对话、6个训练好的ARP-模型以及评估协议，共同为推动基于音频的角色扮演研究提供了重要资源。', 'title_zh': 'AudioRole: 用于大型语言模型角色扮演的音频数据集'}
{'arxiv_id': 'arXiv:2509.23434', 'title': 'NeuroBridge: Using Generative AI to Bridge Cross-neurotype Communication Differences through Neurotypical Perspective-taking', 'authors': 'Rukhshan Haroon, Kyle Wigdor, Katie Yang, Nicole Toumanios, Eileen T. Crehan, Fahad Dogar', 'link': 'https://arxiv.org/abs/2509.23434', 'abstract': 'Communication challenges between autistic and neurotypical individuals stem from a mutual lack of understanding of each other\'s distinct, and often contrasting, communication styles. Yet, autistic individuals are expected to adapt to neurotypical norms, making interactions inauthentic and mentally exhausting for them. To help redress this imbalance, we build NeuroBridge, an online platform that utilizes large language models (LLMs) to simulate: (a) an AI character that is direct and literal, a style common among many autistic individuals, and (b) four cross-neurotype communication scenarios in a feedback-driven conversation between this character and a neurotypical user. Through NeuroBridge, neurotypical individuals gain a firsthand look at autistic communication, and reflect on their role in shaping cross-neurotype interactions. In a user study with 12 neurotypical participants, we find that NeuroBridge improved their understanding of how autistic people may interpret language differently, with all describing autism as a social difference that "needs understanding by others" after completing the simulation. Participants valued its personalized, interactive format and described AI-generated feedback as "constructive", "logical" and "non-judgmental". Most perceived the portrayal of autism in the simulation as accurate, suggesting that users may readily accept AI-generated (mis)representations of disabilities. To conclude, we discuss design implications for disability representation in AI, the need for making NeuroBridge more personalized, and LLMs\' limitations in modeling complex social scenarios.', 'abstract_zh': '沟通障碍源于自闭症个体和非自闭症个体之间相互缺乏对方独特且often contrasting沟通风格的理解。然而，自闭症个体被期望适应非自闭症 norms，这使得他们在互动中显得不真实且心理疲惫。为缓解这种不平衡，我们构建了NeuroBridge，一个利用大规模语言模型（LLMs）模拟的在线平台：（a）一个直接而直截了当的AI角色，这是许多自闭症个体常见的一种风格；（b）四个跨神经类型沟通情景，在这种情景中，该角色与非自闭症用户进行反馈驱动的对话。通过NeuroBridge，非自闭症个体可以获得第一手了解自闭症沟通的机会，并反思他们在塑造跨神经类型互动中的角色。在一项包含12名非自闭症参与者的用户研究中，我们发现NeuroBridge提高了他们对自闭症人士可能如何以不同方式解读语言的理解，所有参与者在完成模拟后都表示自闭症是一种“需要他人理解的社会差异”。参与者们认为其个性化和互动的格式非常有价值，并称AI生成的反馈为“建设性的”、“逻辑性的”和“非评判性的”。大多数参与者认为模拟中自闭症的呈现是准确的，这表明用户可能容易接受AI生成的（误）代表残疾的呈现。最后，我们讨论了在AI中代表残疾的设计含义、使NeuroBridge更加个性化的必要性以及大规模语言模型在建模复杂社会情景方面的局限性。', 'title_zh': 'NeuroBridge：通过神经典型视角桥接不同神经类型间的沟通差异 Using生成性AI'}
{'arxiv_id': 'arXiv:2509.23417', 'title': 'Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models', 'authors': 'Rajaa El Hamdani, Samy Haffoudhi, Nils Holzenberger, Fabian Suchanek, Thomas Bonald, Fragkiskos D. Malliaros', 'link': 'https://arxiv.org/abs/2509.23417', 'abstract': "Language models (LMs) encode substantial factual knowledge, but often produce answers judged as incorrect. We hypothesize that many of these answers are actually correct, but are expressed in alternative surface forms that are dismissed due to an overly strict evaluation, leading to an underestimation of models' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD), a decoding strategy that restricts model outputs to unique surface forms. We introduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating open-source LMs from 135M to 70B parameters, we show that standard decoding undervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1 with vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0% with RCD, outperforming the larger model under vanilla decoding. We publicly share the code and dataset at this https URL.", 'abstract_zh': '语言模型编码了大量的事实知识，但经常产生被判断为错误的答案。我们假设许多答案实际上是正确的，但由于过于严格的评估标准忽视了它们的替代表面形式，导致了对模型参数化知识的低估。我们提出了检索约束解码(RCD)，这是一种限制模型输出为独特表面形式的解码策略。我们引入了包含19,137个一般知识问题的数据集YAGO-QA。评估从135M到70B参数的开源语言模型，我们显示标准解码低估了它们的知识。例如，Llama-3.1-70B在标准解码下的F1得分为32.3%，但在RCD下的得分为46.0%。同样地，Llama-3.1-8B在RCD下的得分为33.0%，在标准解码下表现不如较小的模型。我们已公开分享代码和数据集，可在该链接访问：this https URL。', 'title_zh': '检索约束解码揭示语言模型中被低估的参数知识'}
{'arxiv_id': 'arXiv:2509.23410', 'title': 'PATCH: Learnable Tile-level Hybrid Sparsity for LLMs', 'authors': 'Younes Hourri, Mohammad Mozaffari, Maryam Mehri Dehnavi', 'link': 'https://arxiv.org/abs/2509.23410', 'abstract': 'Large language models (LLMs) deliver impressive performance but incur prohibitive memory and compute costs at deployment. Model pruning is an effective way to reduce these overheads, yet existing approaches face challenges: unstructured sparsity, where nonzeros can appear anywhere, preserves accuracy but yields irregular access patterns that prevent GPU acceleration, while semi-structured 2:4 sparsity is hardware-friendly but enforces a rigid 50% pattern that degrades model quality. To bridge this gap, we introduce PATCH, a hybrid sparsity framework that enables a continuous sparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles, assigning each tile to be either dense or 2:4 sparse via a learnable mask selection mechanism. This design provides fine-grained control over accuracy-acceleration tradeoffs and supports non-uniform sparsity across layers, leading to superior overall quality. Across models from 0.5B to 8B parameters, PATCH consistently narrows the gap to dense accuracy while delivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU, PATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while improving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning method, MaskLLM.', 'abstract_zh': 'PATCH：一种连续可调的稀疏性框架以提高大语言模型的性能和效率', 'title_zh': 'PATCH: 学习驱动的tile级混合稀疏性技术应用于LLMs'}
{'arxiv_id': 'arXiv:2509.23383', 'title': 'Train Once, Answer All: Many Pretraining Experiments for the Cost of One', 'authors': 'Sebastian Bordt, Martin Pawelczyk', 'link': 'https://arxiv.org/abs/2509.23383', 'abstract': "Recent work has demonstrated that controlled pretraining experiments are a powerful tool for understanding learning, reasoning, and memorization in large language models (LLMs). However, the computational cost of pretraining presents a significant constraint. To overcome this constraint, we propose to conduct multiple pretraining experiments simultaneously during a single training run. We demonstrate the feasibility of this approach by conducting ten experiments during the training of a 1.5B parameter model on 210B tokens. Although we only train a single model, we can replicate the results from multiple previous works on data contamination, poisoning, and memorization. We also conduct novel investigations into knowledge acquisition, mathematical reasoning, and watermarking. For example, we dynamically update the training data until the model acquires a particular piece of knowledge. Remarkably, the influence of the ten experiments on the model's training dynamics and overall performance is minimal. However, interactions between different experiments may act as a potential confounder in our approach. We propose to test for interactions with continual pretraining experiments, finding them to be negligible in our setup. Overall, our findings suggest that performing multiple pretraining experiments in a single training run can enable rigorous scientific experimentation with large models on a compute budget.", 'abstract_zh': '最近的研究表明，控制预训练实验是理解大型语言模型（LLMs）学习、推理和记忆能力的强大工具。然而，预训练的计算成本构成了一个显著的限制。为了克服这一限制，我们提出在单次训练运行中同时进行多个预训练实验。我们通过在一个包含210亿令牌的150亿参数模型的训练过程中进行十项实验，证明了这种做法的可行性。尽管我们仅训练了一个模型，但仍能重现多项先前关于数据污染、毒化和记忆的研究结果。我们还对知识获取、数学推理和水印技术进行了新颖的探究。例如，我们动态更新训练数据，直到模型获取特定的知识。令人惊讶的是，这十项实验对模型训练动力学和整体性能的影响极小。然而，不同实验之间的相互作用可能成为我们方法中的潜在混杂因素。我们提出通过持续预训练实验测试这种相互作用，并发现在我们的设置中这种影响可以忽略不计。总体而言，我们的发现表明，在单次训练运行中进行多项预训练实验可以在计算预算内实现对大型模型的严格科学研究。', 'title_zh': '一次训练，解决全部：一次训练的成本换取多项预训练实验'}
{'arxiv_id': 'arXiv:2509.23379', 'title': 'CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding', 'authors': 'Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho', 'link': 'https://arxiv.org/abs/2509.23379', 'abstract': 'Multimodal large language models (MLLMs) have recently achieved remarkable progress in radiology by integrating visual perception with natural language understanding. However, they often generate clinically unsupported descriptions, known as medical hallucinations, which pose serious risks in medical applications that demand accuracy and image-grounded outputs. Through empirical analysis, we find that prompt-induced hallucinations remain prevalent in radiology MLLMs, largely due to over-sensitivity to clinical sections. To address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free and retrieval-free inference framework that integrates structured clinical signals from task-specific radiology expert models. CCD introduces a dual-stage contrastive mechanism to refine token-level logits during generation, thereby enhancing clinical fidelity without modifying the base MLLM. Experiments on three datasets and multiple models demonstrate that CCD consistently improves overall performance on radiology report generation (RRG). On the MIMIC-CXR dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to state-of-the-art RRG models. Our approach provides a lightweight and generalisable solution for mitigating medical hallucinations, effectively bridging expert models and MLLMs in radiology.', 'abstract_zh': '多模态大型语言模型（MLLMs）在放射学领域通过结合视觉感知和自然语言理解取得了显著进展。然而，它们经常生成缺乏临床支持的描述，即医学幻觉，这在要求准确性和图像基础输出的医学应用中构成严重风险。通过实证分析，我们发现放射学MLLM中由临床部分过度敏感引发的幻觉仍然普遍存在。为解决这一问题，我们引入了一种无需训练和检索的推理框架Clinical Contrastive Cecoding（CCD），该框架通过任务特定的放射学专家模型整合结构化的临床信号。CCD引入了双阶段对比机制，在生成过程中细化标记级逻辑量，从而提高临床信度而不修改基础MLLM。在三个数据集和多种模型上的实验表明，CCD在放射学报告生成（RRG）任务上始终能提升整体性能。在MIMIC-CXR数据集上，当应用于最先进的RRG模型时，它在RadGraph-F1指标上可带来高达17%的提升。我们的方法提供了一种轻量级且可扩展的解决方案，用于减轻医学幻觉，有效连接专家模型和MLLM在放射学中的应用。', 'title_zh': 'CCD: 通过临床对比解码减轻放射学MLLMs幻觉'}
{'arxiv_id': 'arXiv:2509.23371', 'title': 'Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization', 'authors': 'Junming Yang, Ning Xu, Biao Liu, Shiqi Qiao, Xin Geng', 'link': 'https://arxiv.org/abs/2509.23371', 'abstract': 'Preference optimization is crucial for aligning large language models (LLMs) with human values and intentions. A significant challenge in this process is the distribution mismatch between pre-collected offline preference data and the evolving model policy. Existing methods attempt to reduce this gap using static heuristics or decoupled online sampling strategies, but they often fail to adapt to the model\'s dynamic learning state. To bridge this gap, we propose Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework that dynamically couples data generation with model training. MetaAPO employs a lightweight meta-learner, as an "alignment gap estimator", to evaluate the potential benefits of on-policy sampling in relation to offline data. This guides targeted online generation and assigns sample-wise meta-weights to the optimization objective, dynamically balancing the quality and distribution of online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench demonstrate that MetaAPO consistently outperforms existing preference optimization approaches across various settings, while reducing 42% in online annotation costs.', 'abstract_zh': '偏好优化对于使大型语言模型（LLMs）与人类价值观和意图对齐至关重要。在这个过程中的一个重大挑战是预收集的离线偏好数据与模型政策的演变之间的分布不匹配。现有方法试图通过使用静态启发式方法或脱钩的在线采样策略来减少这种差距，但它们往往无法适应模型的动态学习状态。为了解决这一差距，我们提出了元加权自适应偏好优化（MetaAPO），这是一种新型框架，能够动态地将数据生成与模型训练耦合。MetaAPO 采用一个轻量级的元学习器作为“对齐差距估计器”，评估在线采样相对于离线数据的潜在益处，以指导针对性的在线生成，并为优化目标分配样本级别的元权重，动态平衡在线和离线数据的质量与分布。实验结果显示，MetaAPO 在各种情况下均能一致地优于现有的偏好优化方法，同时减少42%的在线标注成本。', 'title_zh': '基于元加权在线采样的对齐方法：数据生成与偏好优化之间的桥梁'}
{'arxiv_id': 'arXiv:2509.23368', 'title': 'MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction', 'authors': 'Xinchun Su, Chunxu Luo, Yixuan Li, Weidong Yang, Lipeng Ma', 'link': 'https://arxiv.org/abs/2509.23368', 'abstract': 'In the field of medicine, complex reasoning tasks such as clinical diagnosis, treatment planning, and medical knowledge integration pose significant challenges, where small language models often underperform compared to large language models like GPT-4 and Deepseek. Recent knowledge distillation-based methods aim to address these issues through teacher-guided error correction, but this LLM as judge approach remains challenging in terms of cost, time, and efficiency. To circumvent this issue, we propose a novel two-stage framework, MedCritical, which uses a small language model fine-tuned by a large teacher model to play against itself. In the first stage, we extract high-level and detailed long-chain thought templates from the teacher model to guide the student model to generate more complex reasoning thoughts. In the second stage, we introduce direct preference optimization (DPO) through model self-iteration collaboration to enhance the reasoning ability of the student model by playing against the correction trajectory of the fine-tuned model during training. This model self-learning DPO approach teaches the student model to use its own error-driven insights to consolidate its skills and knowledge to solve complex problems, and achieves comparable results to traditional knowledge distillation methods using teacher models at a lower cost. Notably, our MedCritical 7B model outperforms the Taiyi and Huatuo-o1-7B models by 3.04\\% and 10.12\\% respectively on the CMExam benchmark, achieving new SOTA performance among 7B-class small models.', 'abstract_zh': '在医学领域，临床诊断、治疗规划和医学知识整合等复杂推理任务提出了重大挑战，其中小语言模型往往不如GPT-4和Deepseek等大型语言模型表现优异。基于知识蒸馏的方法近期致力于通过教师引导的错误纠正来解决这些问题，但作为评估者的LLM方法在成本、时间和效率方面仍面临挑战。为克服这一问题，我们提出了一种新颖的两阶段框架MedCritical，该框架使用由大型教师模型微调的小型语言模型与其自身进行对抗。在第一阶段，我们从教师模型中提取高层次和详细化长链思考模板，以指导学生模型生成更复杂的推理思想。在第二阶段，我们通过模型自我迭代协作引入直接偏好优化（DPO），在训练过程中通过与微调模型的校正轨迹进行对抗，增强学生模型的推理能力。该模型自我学习DPO方法教会学生模型利用自身的错误驱动见解来巩固技能和知识以解决复杂问题，并以较低的成本实现了与传统教师模型知识蒸馏方法相当的成果。值得注意的是，我们的MedCritical 7B模型在CMExam基准测试中分别比Taiyi和Huatuo-o1-7B模型高出3.04%和10.12%，在7B级别的小型模型中达到了新的SOTA性能。', 'title_zh': 'MedCritical: 通过自我协作修正增强小型语言模型的医疗推理'}
{'arxiv_id': 'arXiv:2509.23362', 'title': 'Dual-Space Smoothness for Robust and Balanced LLM Unlearning', 'authors': 'Han Yan, Zheyuan Liu, Meng Jiang', 'link': 'https://arxiv.org/abs/2509.23362', 'abstract': 'With the rapid advancement of large language models, Machine Unlearning has emerged to address growing concerns around user privacy, copyright infringement, and overall safety. Yet state-of-the-art (SOTA) unlearning methods often suffer from catastrophic forgetting and metric imbalance, for example by over-optimizing one objective (e.g., unlearning effectiveness, utility preservation, or privacy protection) at the expense of others. In addition, small perturbations in the representation or parameter space can be exploited by relearn and jailbreak attacks. To address these challenges, we propose PRISM, a unified framework that enforces dual-space smoothness in representation and parameter spaces to improve robustness and balance unlearning metrics. PRISM consists of two smoothness optimization stages: (i) a representation space stage that employs a robustly trained probe to defend against jailbreak attacks, and (ii) a parameter-space stage that decouples retain-forget gradient conflicts, reduces imbalance, and smooths the parameter space to mitigate relearning attacks. Extensive experiments on WMDP and MUSE, across conversational-dialogue and continuous-text settings, show that PRISM outperforms SOTA baselines under multiple attacks while achieving a better balance among key metrics.', 'abstract_zh': '大规模语言模型的快速进展催生了机器遗忘技术以应对用户隐私、版权侵犯和整体安全等方面的担忧。然而，最先进的（SOTA）遗忘方法往往会因灾难性遗忘和指标失衡等问题而受到影响，例如过度优化一个目标（如遗忘效果、有用性保留或隐私保护）而牺牲其他目标。此外，在表示空间或参数空间的微小扰动可被利用进行重新学习和囚禁攻击。为了解决这些挑战，我们提出了一种统一体系结构PRISM，该体系结构在表示空间和参数空间中强制实施双空间平滑性以提高鲁棒性和平衡遗忘指标。PRISM包括两个平滑性优化阶段：（i）一个表示空间阶段，使用稳健训练的探针防御囚禁攻击，以及（ii）一个参数空间阶段，将保留和遗忘梯度冲突分离，减少不平衡，平滑参数空间以减轻重新学习攻击。在WMDP和MUSE数据集上，包括对话和连续文本设置的广泛实验表明，PRISM在多种攻击下优于SOTA基准，在关键指标之间实现了更好的平衡。', 'title_zh': '双空间平滑性以实现稳健且均衡的大型语言模型遗忘'}
{'arxiv_id': 'arXiv:2509.23350', 'title': 'ABC-Eval: Benchmarking Large Language Models on Symbolic Music Understanding and Instruction Following', 'authors': 'Jiahao Zhao, Yunjia Li, Wei Li, Kazuyoshi Yoshii', 'link': 'https://arxiv.org/abs/2509.23350', 'abstract': "As large language models continue to develop, the feasibility and significance of text-based symbolic music tasks have become increasingly prominent. While symbolic music has been widely used in generation tasks, LLM capabilities in understanding and reasoning about symbolic music remain largely underexplored. To address this gap, we propose ABC-Eval, the first open-source benchmark dedicated to the understanding and instruction-following capabilities in text-based ABC notation scores. It comprises 1,086 test samples spanning 10 sub-tasks, covering scenarios from basic musical syntax comprehension to complex sequence-level reasoning. Such a diverse scope poses substantial challenges to models' ability to handle symbolic music tasks. We evaluated seven state-of-the-art LLMs on ABC-Eval, and the results reveal notable limitations in existing models' symbolic music processing capabilities. Furthermore, the consistent performance of individual baselines across different sub-tasks supports the reliability of our benchmark.", 'abstract_zh': '随着大型语言模型的不断发展，基于文本的符号音乐任务的可行性和重要性日益凸显。虽然符号音乐在生成任务中已被广泛应用，但在理解和推理符号音乐方面的LLM能力仍 largely underexplored。为弥补这一差距，我们提出了ABC-Eval，这是首个专注于基于文本的ABC符号记谱理解与指令跟随能力的开源基准。它包含1,086个测试样本，涵盖10个子任务，从基本的音乐语法理解到复杂的序列级推理。如此广泛的范围对模型处理符号音乐任务的能力提出了重大挑战。我们对ABC-Eval上的七个最先进的LLM进行了评估，结果揭示了现有模型在符号音乐处理能力方面的显著局限性。此外，各基线在不同子任务上的稳定表现支持了我们基准的可靠性。', 'title_zh': 'ABC-Eval:评估大型语言模型在符号音乐理解与指令跟随方面的 performance'}
{'arxiv_id': 'arXiv:2509.23338', 'title': 'PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation', 'authors': 'Wei Zhou, Guoliang Li, Haoyu Wang, Yuxing Han, Xufei Wu, Fan Wu, Xuanhe Zhou', 'link': 'https://arxiv.org/abs/2509.23338', 'abstract': 'Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: this https URL.', 'abstract_zh': '大型语言模型在Text-to-SQL任务中显示出不断增强的有效性。然而，另一个密切相关的問題，跨系统SQL翻译（即，SQL-to-SQL），即将一个数据库系统（例如，MySQL）中的查询转换为另一个系统（例如，ClickHouse）中的等效查询，在实际应用中具有重要的意义但尚未得到充分探索。现有的SQL基准不适合用于SQL-to-SQL评估，因为它们（1）仅关注有限的数据库系统（通常是SQLite）；（2）无法涵盖许多系统特定的SQL方言（例如，自定义函数、数据类型和语法规则）。因此，在本文中，我们介绍了PARROT，一个适用于跨系统SQL翻译的实用且真实的基准测试集。PARROT包含来自38个开源基准和实际业务服务的598个翻译对，特别设计用于挑战系统特定的SQL理解（例如，大型语言模型的平均准确率低于38.53%）。我们还提供了多种基准测试变体，包括包含28,003个翻译（用于广泛的语法测试）的PARROT-Diverse和包含5,306个代表性样本（用于集中的压力测试）的PARROT-Simple，覆盖了22个生产级数据库系统。为了促进未来的研究，我们在以下链接发布了公开的排行榜和源代码：this https URL。', 'title_zh': 'PARROT：评价LLM在跨系统SQL翻译中的基准'}
{'arxiv_id': 'arXiv:2509.23324', 'title': 'Scaling LLM Test-Time Compute with Mobile NPU on Smartphones', 'authors': 'Zixu Hao, Jianyu Wei, Tuowei Wang, Minxing Huang, Huiqiang Jiang, Shiqi Jiang, Ting Cao, Ju Ren', 'link': 'https://arxiv.org/abs/2509.23324', 'abstract': "Deploying Large Language Models (LLMs) on mobile devices faces the challenge of insufficient performance in smaller models and excessive resource consumption in larger ones. This paper highlights that mobile Neural Processing Units (NPUs) have underutilized computational resources, particularly their matrix multiplication units, during typical LLM inference. To leverage this wasted compute capacity, we propose applying parallel test-time scaling techniques on mobile NPUs to enhance the performance of smaller LLMs. However, this approach confronts inherent NPU challenges, including inadequate hardware support for fine-grained quantization and low efficiency in general-purpose computations. To overcome these, we introduce two key techniques: a hardware-aware tile quantization scheme that aligns group quantization with NPU memory access patterns, and efficient LUT-based replacements for complex operations such as Softmax and dequantization. We design and implement an end-to-end inference system that leverages the NPU's compute capability to support test-time scaling on Qualcomm Snapdragon platforms. Experiments show our approach brings significant speedups: up to 19.0 for mixed-precision GEMM and 2.2 for Softmax. More importantly, we demonstrate that smaller models using test-time scaling can match or exceed the accuracy of larger models, achieving a new performance-cost Pareto frontier.", 'abstract_zh': '在移动设备上部署大语言模型（LLMs）面临小型模型性能不足和大型模型资源消耗过多的挑战。本文指出，在典型的大语言模型推断过程中，移动神经处理单元（NPUs）的计算资源尤其是矩阵乘法单元利用不足。为利用这些浪费的计算能力，我们提出在移动NPUs上应用并行测试时缩放技术以提升小型LLMs的性能。然而，这一方法面临固有的NPU挑战，包括对精细量化硬件支持不足和通用计算效率低下。为克服这些挑战，我们引入了两项关键技术：一种硬件感知的分组量化方案，该方案将分组量化与NPU内存访问模式对齐，以及基于查找表（LUT）的有效替代复杂操作（如Softmax和去量化）的方法。我们设计并实现了一个端到端的推断系统，该系统利用NPU的计算能力在高通骁龙平台上支持测试时缩放。实验结果显示，我们的方法带来了显著的速度提升：对于混合精度GEMM可达19.0倍，对于Softmax可达2.2倍。更重要的是，我们证明了使用测试时缩放的小型模型可以在准确率上与大型模型匹敌，从而开辟了新的性能-成本帕累托前沿。', 'title_zh': '在智能手机上使用移动NPU扩展LLM测试时计算性能'}
{'arxiv_id': 'arXiv:2509.23286', 'title': 'A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models', 'authors': 'Wonje Jeung, Sangyeon Yoon, Yoonjun Cho, Dongjae Jeon, Sangwoo Shin, Hyesoo Hong, Albert No', 'link': 'https://arxiv.org/abs/2509.23286', 'abstract': 'Diffusion large language models (dLLMs) enable any-order generation, but this flexibility enlarges the attack surface: harmful spans may appear at arbitrary positions, and template-based prefilling attacks such as DIJA bypass response-level refusals. We introduce A2D (Any-Order, Any-Step Defense), a token-level alignment method that aligns dLLMs to emit an [EOS] refusal signal whenever harmful content arises. By aligning safety directly at the token-level under randomized masking, A2D achieves robustness to both any-decoding-order and any-step prefilling attacks under various conditions. It also enables real-time monitoring: dLLMs may begin a response but automatically terminate if unsafe continuation emerges. On safety benchmarks, A2D consistently prevents the generation of harmful outputs, slashing DIJA success rates from over 80% to near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B), and thresholded [EOS] probabilities allow early rejection, yielding up to 19.3x faster safe termination.', 'abstract_zh': '任何顺序和任何步骤防御：面向标记的整条目保护方法（A2D）', 'title_zh': '任何阶数，任意步数的安全对齐for扩散语言模型'}
{'arxiv_id': 'arXiv:2509.23246', 'title': 'Adaptive Token-Weighted Differential Privacy for LLMs: Not All Tokens Require Equal Protection', 'authors': 'Manjiang Yu, Priyanka Singh, Xue Li, Yang Cao', 'link': 'https://arxiv.org/abs/2509.23246', 'abstract': "Large language models (LLMs) frequently memorize sensitive or personal information, raising significant privacy concerns. Existing variants of differential privacy stochastic gradient descent (DPSGD) inject uniform noise into every gradient step, significantly extending training time and reducing model accuracy. We propose that concentrating noise primarily on gradients associated with sensitive tokens can substantially decrease DP training time, strengthen the protection of sensitive information, and simultaneously preserve the model's performance on non-sensitive data. We operationalize this insight through Adaptive Token-Weighted Differential Privacy (ATDP), a modification of vanilla DP-SGD that adaptively assigns different gradient weights to sensitive and non-sensitive tokens. By employing a larger noise scale at the early stage of training, ATDP rapidly disrupts memorization of sensitive content. As a result, ATDP only requires a few additional epochs of lightweight post-processing following standard fine-tuning, injecting targeted noise primarily on parameters corresponding to sensitive tokens, thus minimally affecting the model's general capabilities. ATDP can be seamlessly integrated into any existing DP-based fine-tuning pipeline or directly applied to non-private models as a fast privacy-enhancing measure. Additionally, combined with an initial redacted fine-tuning phase, ATDP forms a streamlined DP pipeline that achieves comparable canary protection to state-of-the-art DP-SGD methods, significantly reduces the computational overhead of DP fine-tuning, shortening training time by approximately 90 percent, while achieving comparable or superior privacy protection and minimal accuracy degradation.", 'abstract_zh': '大型语言模型 (LLMs) 经常记住敏感或个人信息，引发了重大的隐私担忧。现有差分隐私随机梯度下降 (DPSGD) 的变体在每个梯度步骤中注入均匀噪声，显著延长了训练时间并降低了模型准确性。我们提出，将噪声主要集中在与敏感词元相关的梯度上，可以大幅减少差分隐私训练时间，加强敏感信息保护，并同时保持模型在非敏感数据上的性能。我们通过自适应词元加权差分隐私 (ATDP) 将这一见解付诸实践，ATDP 是一种对 vanilla DP-SGD 的修改，能够根据不同情况为敏感和非敏感词元分配不同的梯度权重。通过在训练早期使用更大的噪声尺度，ATDP 迅速破坏对敏感内容的记忆。因此，ATDP 只需要在标准微调之后进行少量的轻量级后处理，主要在与敏感词元相对应的参数上注入目标噪声，从而最小化对模型通用能力的负面影响。ATDP 可以无缝集成到任何现有的基于差分隐私的微调管道中，或者直接应用于非私有模型，作为快速的隐私增强措施。此外，结合初始红处理微调阶段，ATDP 形成了一种简化的工作流，能够实现与最先进的 DPSGD 方法相当的开箱即用保护，显著减少了差分隐私微调的计算开销，将训练时间缩短约 90%，同时实现相当或更优的隐私保护和最小的准确性下降。', 'title_zh': '自适应词元加权差分隐私技术：并非所有词元都需要同等保护'}
{'arxiv_id': 'arXiv:2509.23232', 'title': 'SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts', 'authors': 'Bingshuai Liu, Ante Wang, Zijun Min, Liang Yao, Haibo Zhang, Yang Liu, Anxiang Zeng, Jinsong Su', 'link': 'https://arxiv.org/abs/2509.23232', 'abstract': 'Large Language Models (LLMs) increasingly rely on reinforcement learning with verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning. However, the training process remains bottlenecked by the computationally expensive rollout stage. Existing acceleration methods-such as parallelization, objective- and data-driven modifications, and replay buffers-either incur diminishing returns, introduce bias, or overlook redundancy across iterations. We identify that rollouts from consecutive training epochs frequently share a large portion of overlapping segments, wasting computation. To address this, we propose SPEC-RL, a novel framework that integrates SPECulative decoding with the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative prefixes and extends them via a draft-and-verify mechanism, avoiding redundant generation while ensuring policy consistency. Experiments on diverse math reasoning and generalization benchmarks, including GSM8K, MATH-500, OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout time by 2-3x without compromising policy quality. As a purely rollout-stage enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g., PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large reasoning models. Our code is available at this https URL', 'abstract_zh': '大型语言模型（LLMs）越来越多地依赖可验证奖励的强化学习（RLVR）来引发可靠的过程推理。然而，训练过程仍然受限于计算昂贵的展开阶段。现有的加速方法——如并行化、基于目标和数据的修改以及回放缓冲区——要么收益递减，要么引入偏差，要么忽视迭代间的冗余。我们发现连续训练周期的展开段经常共享大量重叠的部分，浪费了计算资源。为此，我们提出了一种新颖的框架SPEC-RL，该框架将SPECulative解码与RL展开过程融合。SPEC-RL利用先前的轨迹段作为推测性的前缀，并通过草案和验证机制进行扩展，避免重复生成同时确保策略一致性。在包括GSM8K、MATH-500、OlympiadBench、MMLU-STEM等多样化的数学推理和泛化基准测试中，SPEC-RL在不牺牲策略质量的情况下将展开时间减少2-3倍。作为纯粹的展开阶段增强，SPEC-RL能够无缝集成主流算法（例如PPO、GRPO、DAPO），提供了一条扩展RLVR以应用于大型推理模型的通用且实用的道路。我们的代码可在下列链接获取。', 'title_zh': 'SPEC-RL: 通过推测性滚出自加速在线策略强化学习'}
{'arxiv_id': 'arXiv:2509.23206', 'title': 'PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness', 'authors': 'Huacan Chai, Zijie Cao, Maolin Ran, Yingxuan Yang, Jianghao Lin, pengxin, Hairui Wang, Renjie Ding, Ziyu Wan, Muning Wen, Weiwen Liu, Weinan Zhang, Fei Huang, Ying Wen', 'link': 'https://arxiv.org/abs/2509.23206', 'abstract': 'Large language models (LLMs) have achieved impressive success in single-turn function calling, yet real-world applications such as travel planning or multi-stage data analysis typically unfold across multi-turn conversations. In these settings, LLMs must not only issue accurate function calls at each step but also maintain progress awareness, the ability to summarize past interactions and plan future actions to ensure coherent, long-horizon task execution. Existing approaches, however, either reduce multi-turn training to isolated single-turn samples, which neglects task-level planning, or employ end-to-end reinforcement learning (RL) that struggles with redundancy and lacks explicit integration of progress awareness. To overcome these limitations, we introduce PARL-MT, a framework that explicitly incorporates progress awareness into LLM training for multi-turn function calling. PARL-MT combines (i) a Progress Awareness Generation (PAG) pipeline, which automatically constructs datasets coupling conversation summaries with future task planning, and (ii) a Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which integrates progress awareness into RL training to reduce contextual redundancy and improve alignment between local actions and global task completion. Empirical results on two public benchmarks demonstrate that PARL-MT significantly outperforms existing methods, highlighting the effectiveness of progress awareness in enabling robust and efficient multi-turn function calling.', 'abstract_zh': 'PARL-MT：一种结合进度意识的多轮函数调用框架', 'title_zh': 'PARL-MT：在多轮对话中具有进度意识的函数调用学习'}
{'arxiv_id': 'arXiv:2509.23115', 'title': 'RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility', 'authors': 'Haoyu He, Haozheng Luo, Yan Chen, Qi R. Wang', 'link': 'https://arxiv.org/abs/2509.23115', 'abstract': "Predicting human mobility is inherently challenging due to complex long-range dependencies and multi-scale periodic behaviors. To address this, we introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a unified framework that leverages large language models (LLMs) as general-purpose spatio-temporal predictors and trajectory reasoners. Methodologically, RHYTHM employs temporal tokenization to partition each trajectory into daily segments and encode them as discrete tokens with hierarchical attention that captures both daily and weekly dependencies, thereby significantly reducing the sequence length while preserving cyclical information. Additionally, we enrich token representations by adding pre-computed prompt embeddings for trajectory segments and prediction targets via a frozen LLM, and feeding these combined embeddings back into the LLM backbone to capture complex interdependencies. Computationally, RHYTHM freezes the pretrained LLM's backbone to reduce attention complexity and memory cost. We evaluate our model against state-of-the-art methods using three real-world datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a 5.0% increase on weekends, and a 24.6% reduction in training time. Code is publicly available at this https URL.", 'abstract_zh': '基于层次时间分词的人类移动预测模型RHYTHM', 'title_zh': 'RHYTHM: 基于层次时间切分的理性推理在人类移动性分析中的应用'}
{'arxiv_id': 'arXiv:2509.23095', 'title': 'Causally-Enhanced Reinforcement Policy Optimization', 'authors': 'Xiangqi Wang, Yue Huang, Yujun Zhou, Xiaonan Luo, Kehan Guo, Xiangliang Zhang', 'link': 'https://arxiv.org/abs/2509.23095', 'abstract': 'Large language models (LLMs) trained with reinforcement objectives often achieve superficially correct answers via shortcut strategies, pairing correct outputs with spurious or unfaithful reasoning and degrading under small causal perturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a drop-in reward-shaping framework that augments policy optimization with a differentiable proxy for causal coherence along the generation pathway from prompt (Z) to rationale (X) to answer (Y). CE-PO estimates model-internal influence with Jacobian-based sensitivities, counterfactually hardens these signals to suppress nuisance cues, and fuses the resulting coherence score with task-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single tunable between accuracy and coherence trade-off. The unified reward integrates with PPO/GRPO without architectural changes. Across reasoning benchmarks and causal stress tests, CE-PO reduces reward hacking and unfaithful chain-of-thought while improving robustness to correlation-causation flips and light counterfactual edits, all at near-parity accuracy. Experimental results across 4 datasets show that CE-PO improves accuracy over baselines by 5.49% on average (up to 9.58%), while improving robustness to correlation-causation flips and light counterfactual edits.', 'abstract_zh': '因果增强策略优化（CE-PO）：一种插入式的奖励塑造框架', 'title_zh': '因果增强强化策略优化'}
{'arxiv_id': 'arXiv:2509.23067', 'title': 'Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks', 'authors': 'Chunyang Jiang, Yonggang Zhang, Yiyang Cai, Chi-Min Chan, Yulong Liu, Mingming Chen, Wei Xue, Yike Guo', 'link': 'https://arxiv.org/abs/2509.23067', 'abstract': 'The rising cost of acquiring supervised data has driven significant interest in self-improvement for large language models (LLMs). Straightforward unsupervised signals like majority voting have proven effective in generating pseudo-labels for verifiable tasks, while their applicability to unverifiable tasks (e.g., translation) is limited by the open-ended character of responses. As a result, self-evaluation mechanisms (e.g., self-judging and entropy minimization) are predominantly used to derive pseudo-labels. However, self-evaluation relying on LLMs typically incurs high computational overhead and introduces overconfidence issues due to intrinsic biases. To address these challenges, we propose a novel self-evaluation-free approach for unverifiable tasks, designed for lightweight yet effective self-improvement. Inspired by majority voting commonly employed in verifiable tasks, we propose semantic voting as a novel mechanism that relaxes the principle of hard matching (i.e., exact matching) toward soft matching (i.e., semantic similarity). Soft matching is achieved by leveraging a lightweight sentence embedding model to quantify semantic similarity, thereby mitigating excessive computational burden and intrinsic bias-associated limitations of self-evaluation. Comprehensive experiments demonstrate that our method achieves substantial gains in computational efficiency and overall better performance than self-evaluation methods across diverse model architectures and tasks.', 'abstract_zh': '大型语言模型不可验证任务的自提升新方法：基于语义投票的无自我评估机制', 'title_zh': '语义投票：一种无需自我评估的高效LLM自我改进方法，用于不可验证的开放式任务'}
{'arxiv_id': 'arXiv:2509.23061', 'title': 'Local Success Does Not Compose: Benchmarking Large Language Models for Compositional Formal Verification', 'authors': 'Xu Xu, Xin Li, Xingwei Qu, Jie Fu, Binhang Yuan', 'link': 'https://arxiv.org/abs/2509.23061', 'abstract': 'We introduce DafnyCOMP, a benchmark for evaluating large language models (LLMs) on compositional specification generation in Dafny. Unlike prior benchmarks that focus on single-function tasks, DafnyCOMP targets programs composed of multiple interacting functions with data dependencies, requiring reasoning across component boundaries. The benchmark consists of 300 automatically synthesized multi-function programs. We evaluate several state-of-the-art LLM families and find that, while they perform well on single-function verification, their performance drops sharply on compositional tasks. Analysis reveals systematic failures in cross-functional reasoning, including fragile specifications, misalignment between implementations and proofs, and unstable reasoning. DafnyCOMP thus provides a diagnostic tool for measuring progress toward reliable, verifiable, and compositional code generation with LLMs.', 'abstract_zh': 'DafnyCOMP：一种评估大规模语言模型在Dafny中生成组合规范能力的基准测试', 'title_zh': '局部成功并不组成整体：大规模语言模型在组合形式验证中的基准测试'}
{'arxiv_id': 'arXiv:2509.23050', 'title': 'Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding', 'authors': 'Lin Long, Changdae Oh, Seongheon Park, Yixuan Li', 'link': 'https://arxiv.org/abs/2509.23050', 'abstract': 'Large vision-language models (LVLMs) achieve strong performance on multimodal tasks, yet they often default to their language prior (LP) -- memorized textual patterns from pre-training while under-utilizing visual evidence. Prior analyses of LP mostly rely on input-output probing, which fails to reveal the internal mechanisms governing when and how vision influences model behavior. To address this gap, we present the first systematic analysis of language prior through the lens of chain-of-embedding, which examines the layer-wise representation dynamics within LVLMs. Our analysis reveals a universal phenomenon: each model exhibits a Visual Integration Point (VIP), a critical layer at which visual information begins to meaningfully reshape hidden representations and influence decoding. Building on this observation, we introduce the Total Visual Integration (TVI) estimator, which aggregates representation distance beyond the VIP to quantify how strongly visual query influences response generation. Across 54 model-dataset combinations spanning 9 contemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently emerges, and that TVI reliably predicts the strength of language prior. This offers a principled toolkit for diagnosing and understanding language prior in LVLMs.', 'abstract_zh': '大型多模态语言模型（LVLMs）在多模态任务中表现出强劲性能，但往往倾向于依赖其语言先验（LP）——预训练中记忆的文本模式，而未能充分利用视觉证据。对语言先验的先前分析主要依赖于输入-输出探测，这未能揭示视觉如何影响模型行为的内部机制。为填补这一空白，我们首先从嵌入链的角度对语言先验进行系统分析，该方法考察了大型多模态语言模型（LVLMs）逐层的表示动态。我们的分析揭示了一个普遍现象：每个模型都存在一个视觉整合点（VIP），这是一个关键层，在此层，视觉信息开始实质性地重塑隐藏表示，并影响解码过程。基于这一观察，我们引入了总视觉整合（TVI）估计器，该估计器衡量视觉查询如何强烈地影响响应生成，并聚集VIP之后的表示距离。在54种不同的模型-数据集组合中，包含9种现代大型多模态语言模型和6个基准测试，我们证明VIP始终出现，并且TVI可靠地预测了语言先验的强度。这为诊断和理解大型多模态语言模型中的语言先验提供了一个原则性的工具包。', 'title_zh': 'LVLMs中的语言先验理解对比嵌入链的研究'}
{'arxiv_id': 'arXiv:2509.23041', 'title': 'Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data', 'authors': 'Zi Liang, Qingqing Ye, Xuan Liu, Yanyun Wang, Jianliang Xu, Haibo Hu', 'link': 'https://arxiv.org/abs/2509.23041', 'abstract': 'Synthetic data refers to artificial samples generated by models. While it has been validated to significantly enhance the performance of large language models (LLMs) during training and has been widely adopted in LLM development, potential security risks it may introduce remain uninvestigated. This paper systematically evaluates the resilience of synthetic-data-integrated training paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal that such a paradigm exhibits strong resistance to existing attacks, primarily thanks to the different distribution patterns between poisoning data and queries used to generate synthetic samples. To enhance the effectiveness of these attacks and further investigate the security risks introduced by synthetic data, we introduce a novel and universal attack framework, namely, Virus Infection Attack (VIA), which enables the propagation of current attacks through synthetic data even under purely clean queries. Inspired by the principles of virus design in cybersecurity, VIA conceals the poisoning payload within a protective "shell" and strategically searches for optimal hijacking points in benign samples to maximize the likelihood of generating malicious content. Extensive experiments on both data poisoning and backdoor attacks show that VIA significantly increases the presence of poisoning content in synthetic data and correspondingly raises the attack success rate (ASR) on downstream models to levels comparable to those observed in the poisoned upstream models.', 'abstract_zh': '合成数据指的是由模型生成的虚拟样本。尽管它已被验证能够在训练过程中显著提升大型语言模型（LLMs）的表现，并已在LLM开发中广泛采用，但其可能引入的安全风险仍待研究。本文系统性地评估了集成合成数据的训练范式针对主流的投毒和后门攻击的鲁棒性。我们发现，此类范式对现有攻击表现出较强的抵抗力，主要得益于投毒数据与用于生成合成样本的查询之间不同的分布模式。为了增强这些攻击的有效性并进一步探讨由合成数据引入的安全风险，我们提出了一个新颖且通用的攻击框架，即病毒感染攻击（VIA），该框架能够在仅使用纯净查询的情况下，通过合成数据传播当前的攻击。灵感源自网络安全中病毒设计的原则，VIA将投毒负载隐藏在保护性的“壳”中，战略性地在良性样本中寻找最佳劫持点，以最大化生成恶意内容的可能性。在数据投毒和后门攻击的广泛实验中，我们显示VIA显著增加了合成数据中的投毒内容比例，并相应地提高了下游模型的攻击成功率（ASR），使其接近或达到中毒上游模型观察到的水平。', 'title_zh': 'LLM的病毒感染攻击：“VIA”合成数据可以传播你的污染'}
{'arxiv_id': 'arXiv:2509.23040', 'title': 'Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents', 'authors': 'Yaorui Shi, Yuxin Chen, Siyuan Wang, Sihang Li, Hengxing Cai, Qi Gu, Xiang Wang, An Zhang', 'link': 'https://arxiv.org/abs/2509.23040', 'abstract': 'Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory corpus that is dynamically updated during a single-pass document scan, also known as the "memorize while reading" methods. While this approach scales efficiently, it suffers from irreversible forward-only processing, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced memory that allows selective retrieval from the entire memory history and allows non-linear reasoning and revisiting of early evidence. To further strengthen training, we propose Reinforcement Learning with Multi-Level Rewards (RLMLR), which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support multi-hop memory utilizing. Experiments on long-document QA show significant gains over existing memory-based approaches, which validates ReMemR1 as an effective solution for long-context reasoning agents.', 'abstract_zh': '具有回调增强记忆的ReMemR1：一种用于长上下文推理的内存增强代理', 'title_zh': '回首以理向前：可回溯记忆赋能长上下文LLM代理'}
{'arxiv_id': 'arXiv:2509.23024', 'title': 'Tracing the Representation Geometry of Language Models from Pretraining to Post-training', 'authors': 'Melody Zixuan Li, Kumar Krishna Agrawal, Arna Ghosh, Komal Kumar Teru, Adam Santoro, Guillaume Lajoie, Blake A. Richards', 'link': 'https://arxiv.org/abs/2509.23024', 'abstract': 'Standard training metrics like loss fail to explain the emergence of complex capabilities in large language models. We take a spectral approach to investigate the geometry of learned representations across pretraining and post-training, measuring effective rank (RankMe) and eigenspectrum decay ($\\alpha$-ReQ). With OLMo (1B-7B) and Pythia (160M-12B) models, we uncover a consistent non-monotonic sequence of three geometric phases during autoregressive pretraining. The initial "warmup" phase exhibits rapid representational collapse. This is followed by an "entropy-seeking" phase, where the manifold\'s dimensionality expands substantially, coinciding with peak n-gram memorization. Subsequently, a "compression-seeking" phase imposes anisotropic consolidation, selectively preserving variance along dominant eigendirections while contracting others, a transition marked with significant improvement in downstream task performance. We show these phases can emerge from a fundamental interplay of cross-entropy optimization under skewed token frequencies and representational bottlenecks ($d \\ll |V|$). Post-training further transforms geometry: SFT and DPO drive "entropy-seeking" dynamics to integrate specific instructional or preferential data, improving in-distribution performance while degrading out-of-distribution robustness. Conversely, RLVR induces "compression-seeking", enhancing reward alignment but reducing generation diversity.', 'abstract_zh': '标准的训练度量如损失无法解释大型语言模型中复杂能力的涌现。我们采用谱方法研究预训练和后训练过程中学习表示的空间几何，测量有效秩（RankMe）和特征谱衰减（$\\alpha$-ReQ）。通过对OLMo（1B-7B）和Pythia（160M-12B）模型的研究，我们发现在自回归预训练过程中存在一个一致的非单调三阶段几何演变。初始的“热身”阶段表现出快速的观点坍缩。随后是“熵寻求”阶段，此时流形的维度显著扩张，与n-gram记忆的峰值相吻合。之后是“压缩寻求”阶段，导致各向异性压缩，选择性地沿主特征方向保留方差并压缩其他方向，这一转变伴随着下游任务性能的显著提升。我们表明，这些阶段可以从交叉熵优化下的词元频率偏差和表示瓶颈（$d \\ll |V|$）的基本相互作用中 emergence 出来。后训练进一步改变几何结构：SFT和DPO驱动“熵寻求”动力学，整合特定的指令或偏好数据，提高分布内性能，但减少分布外鲁棒性。相反，RLVR诱导“压缩寻求”，增强奖励对齐但减少生成多样性。', 'title_zh': '从预训练到后训练语言模型的表示几何追踪'}
{'arxiv_id': 'arXiv:2509.23019', 'title': 'LLM Watermark Evasion via Bias Inversion', 'authors': 'Jeongyeon Hwang, Sangdon Park, Jungseul Ok', 'link': 'https://arxiv.org/abs/2509.23019', 'abstract': 'Watermarking for large language models (LLMs) embeds a statistical signal during generation to enable detection of model-produced text. While watermarking has proven effective in benign settings, its robustness under adversarial evasion remains contested. To advance a rigorous understanding and evaluation of such vulnerabilities, we propose the \\emph{Bias-Inversion Rewriting Attack} (BIRA), which is theoretically motivated and model-agnostic. BIRA weakens the watermark signal by suppressing the logits of likely watermarked tokens during LLM-based rewriting, without any knowledge of the underlying watermarking scheme. Across recent watermarking methods, BIRA achieves over 99\\% evasion while preserving the semantic content of the original text. Beyond demonstrating an attack, our results reveal a systematic vulnerability, emphasizing the need for stress testing and robust defenses.', 'abstract_zh': '大语言模型的水印嵌入：偏差反转重写攻击', 'title_zh': 'LLM水印逃逸通过偏差反转'}
{'arxiv_id': 'arXiv:2509.22991', 'title': 'ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning', 'authors': 'Jasin Cekinmez, Omid Ghahroodi, Saad Fowad Chandle, Dhiman Gupta, Ehsaneddin Asgari', 'link': 'https://arxiv.org/abs/2509.22991', 'abstract': "We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating and improving multimodal large language models (MLLMs) in biographical reasoning. To the best of our knowledge, this is the first work to systematically examine LLM capabilities in biography, a critical yet underexplored dimension of factual knowledge. At its core, AdamDB is a multilingual and multimodal dataset covering over 4 million individuals across geography, time, and profession, while AdamBench provides cognitively structured evaluations based on Bloom's taxonomy, spanning six reasoning levels in both English and native languages. To address hallucinations, particularly for lesser-known individuals, we propose AdamRAG, a retrieval-augmented generation system tailored to biographical contexts. Experiments show that AdamRAG substantially improves open-source models and modestly benefits closed-source ones, with the largest gains on lower-order reasoning. Popularity strongly mediates accuracy, and multimodal input via face images offers smaller, less consistent improvements than retrieval. ADAM establishes the first benchmark and framework for cognitively, culturally, and multimodally grounded biographical evaluation, advancing the development of multilingual, accurate, and hallucination-resistant MLLMs.", 'abstract_zh': 'ADAM：一种多模态人类历史档案框架以评估和提升生物传记推理能力的大语言模型', 'title_zh': 'ADAM：一种多元的人类档案，用于评估和提升LLM在生物传记推理中的表现'}
{'arxiv_id': 'arXiv:2509.22951', 'title': 'Tiny-QMoE', 'authors': 'Jack Cashman, Jiaqi Nie', 'link': 'https://arxiv.org/abs/2509.22951', 'abstract': 'The QMoE model provides a practical approach for compression of massive Mixture-of-Experts (MoE) models. QMoE offers a solution geared towards memory limitations that often reach terabyte scales, and it has the advantage of working with high sparsity models which implicitly lend themselves to compression techniques. QMoE also has the advantage of only taking MoE models into account and does not evaluate its use with non mixture of expert systems. Although this prior attempt focuses on the limitations of large servers with the latest NVIDIA hardware which in the case of the H100 and V100 which have 80 GB of HBM (High Bandwidth Memory), what is not being considered is a significantly more constrained environment, such as in the case of mobile devices which may have in the case of the iPhone anywhere from 4 to 8 GB of unified memory which also needs to be shared with the operating system and additional processes. Although edge devices such as phones and laptops are becoming increasingly more computationally powerful, they are still not close to the level of advanced server machines such as NVIDIA. An additional constraint that we must consider is that of latency. The communication time of sending a request to an LLM server and then getting it back is an additional waiting time that can be removed. We may also want to use LLM technology in environments where there is no reliable network connection.', 'abstract_zh': 'QMoE模型提供了大规模Mixture-of-Experts（MoE）模型压缩的一种实用方法。', 'title_zh': 'Tiny-QMoE'}
{'arxiv_id': 'arXiv:2509.22926', 'title': 'Large language models management of medications: three performance analyses', 'authors': 'Kelli Henry, Steven Xu, Kaitlin Blotske, Moriah Cargile, Erin F. Barreto, Brian Murray, Susan Smith, Seth R. Bauer, Yanjun Gao, Tianming Liu, Andrea Sikora', 'link': 'https://arxiv.org/abs/2509.22926', 'abstract': 'Background: Large language models (LLMs) can be useful in diagnosing medical conditions, but few studies have evaluated their consistency in recommending appropriate medication regimens. The purpose of this evaluation was to test GPT-4o on three medication benchmarking tests including mapping a drug name to its correct formulation, identifying drug-drug interactions using both its internal knowledge and using a web search, and preparing a medication order sentence after being given the medication name. Methods: Using GTP-4o three experiments were completed. Accuracy was quantified by computing cosine similarity on TF-IDF vectors, normalized Levenshtein similarity, and ROUGE-1/ROUGE-L F1 between each response and its reference string or by manual evaluation by clinicians. Results: GPT-4o performed poorly on drug-formulation matching, with frequent omissions of available drug formulations (mean 1.23 per medication) and hallucinations of formulations that do not exist (mean 1.14 per medication). Only 49% of tested medications were correctly matched to all available formulations. Accuracy was decreased for medications with more formulations (p<0.0001). GPT-4o was also inconsistent at identifying drug-drug-interactions, although it had better performance with the search-augmented assessment compared to its internal knowledge (54.7% vs. 69.2%, p=0.013). However, allowing a web-search worsened performance when there was no drug-drug interaction (median % correct 100% vs. 40%, p<0.001). Finally, GPT-4o performed moderately with preparing a medication order sentence, with only 65.8% of medication order sentences containing no medication or abbreviation errors. Conclusions: Model performance was overall poor for all tests. This highlights the need for domain-specific training through clinician-annotated datasets and a comprehensive evaluation framework for benchmarking performance.', 'abstract_zh': '背景：大规模语言模型（LLMs）在诊断医疗条件方面可能非常有用，但很少有研究评估其在推荐适当药物 regimen 方面的一致性。本评估的目的是测试 GPT-4o 在三项药物基准测试中的表现，包括将药物名称映射到正确配方、通过其内部知识和网络搜索识别药物-药物相互作用，以及在给出药物名称后准备药物订单句子。方法：使用 GPT-4o 完成了三个实验。通过计算 TF-IDF 向量、归一化 Levenshtein 相似性以及每个响应与其参考字符串之间的 ROUGE-1/ROUGE-L F1 量化准确度，或通过临床人员的手动评估。结果：GPT-4o 在药物-配方匹配方面表现不佳，频繁遗漏可用的药物配方（每种药物平均 1.23 种）并杜撰不存在的配方（每种药物平均 1.14 种）。仅 49% 的测试药物能够与所有可用配方正确匹配。对于具有更多配方的药物，准确度降低（p<0.0001）。GPT-4o 在识别药物-药物相互作用方面也不一致，尽管在搜索增强评估中其内部知识的表现优于网络搜索增强（54.7% vs. 69.2%，p=0.013）。然而，允许网络搜索在没有药物-药物相互作用时会损害表现（无药物-药物相互作用时的正确率为 100% vs. 40%，p<0.001）。最后，GPT-4o 在准备药物订单句子方面表现中等，仅 65.8% 的药物订单句子中没有药物或缩写错误。结论：所有测试中的模型性能普遍较差。这强调了通过临床标注数据进行领域特定训练以及建立全面的评估框架以基准测试性能的必要性。', 'title_zh': '大型语言模型在管理药物方面的性能分析'}
{'arxiv_id': 'arXiv:2509.22921', 'title': 'Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective', 'authors': 'Matthieu Zimmer, Xiaotong Ji, Tu Nguyen, Haitham Bou Ammar', 'link': 'https://arxiv.org/abs/2509.22921', 'abstract': 'We introduce a novel approach to large language model (LLM) distillation by formulating it as a constrained reinforcement learning problem. While recent work has begun exploring the integration of task-specific rewards into distillation processes, existing methods typically rely on ad-hoc reward weighting. We propose a principled optimization framework that maximizes task-specific rewards while constraining the divergence from the teacher model to remain below a specified threshold. Our approach adapts constrained state augmented reinforcement learning to the distillation setting, introducing a modified reward function that maintains theoretical guarantees of constraint satisfaction without requiring state augmentation or teacher model access during deployment and without the computational overhead of the dual Lagrangian methods. Through extensive experiments on mathematical reasoning tasks, we demonstrate that our method achieves better constraint satisfaction rates and better reasoning compared to the soft Lagrangian relaxation baselines while maintaining competitive task performance. Our framework provides a theoretically grounded and practically efficient solution for reward-aware distillation in resource-constrained settings.', 'abstract_zh': '我们通过将其表述为受限强化学习问题，介绍了大型语言模型（LLM）精简的一种新颖方法。', 'title_zh': '重新思考大型语言模型蒸馏：一种受限马尔可夫决策过程视角'}
{'arxiv_id': 'arXiv:2509.22906', 'title': 'Extract-0: A Specialized Language Model for Document Information Extraction', 'authors': 'Henrique Godoy', 'link': 'https://arxiv.org/abs/2509.22906', 'abstract': 'This paper presents Extract-0, a 7-billion parameter language model specifically optimized for document information extraction that achieves performance exceeding models with parameter counts several orders of magnitude larger. Through a novel combination of synthetic data generation, supervised fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of 0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology employs a memory-preserving synthetic data generation pipeline that produces 280,128 training examples from diverse document sources, followed by parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M out of 7.66B parameters). The reinforcement learning phase introduces a novel semantic similarity-based reward function that handles the inherent ambiguity in information extraction tasks. This research demonstrates that task-specific optimization can yield models that surpass general-purpose systems while requiring substantially fewer computational resource.', 'abstract_zh': '本文介绍了Extract-0，一个专门优化用于文档信息提取的70亿参数语言模型，其性能超过参数量比其大几个数量级的模型。通过一种新颖的合成数据生成、基于Low-Rank Adaptation（LoRA）的监督微调以及Group Relative Policy Optimization（GRPO）强化学习的组合，Extract-0在1000个多元文档提取任务基准上的平均奖励为0.573，超越了GPT-4.1（0.457）、o3（0.464）和GPT-4.1-2025（0.459）。训练方法采用了一种保留内存的合成数据生成管道，从多元文档来源生成280,128个训练样本，随后进行了参数高效微调，仅修改了0.53%的模型权重（40.4M/7.66B参数）。强化学习阶段引入了一种基于语义相似性的奖励函数，以处理信息提取任务中的固有歧义。这项研究展示了任务特定优化可以在拥有显著较少计算资源的情况下超越通用系统。', 'title_zh': '面向文档信息提取的专业语言模型'}
{'arxiv_id': 'arXiv:2509.22834', 'title': 'Bridging Language Models and Formal Methods for Intent-Driven Optical Network Design', 'authors': 'Anis Bekri, Amar Abane, Abdella Battou, Saddek Bensalem', 'link': 'https://arxiv.org/abs/2509.22834', 'abstract': 'Intent-Based Networking (IBN) aims to simplify network management by enabling users to specify high-level goals that drive automated network design and configuration. However, translating informal natural-language intents into formally correct optical network topologies remains challenging due to inherent ambiguity and lack of rigor in Large Language Models (LLMs). To address this, we propose a novel hybrid pipeline that integrates LLM-based intent parsing, formal methods, and Optical Retrieval-Augmented Generation (RAG). By enriching design decisions with domain-specific optical standards and systematically incorporating symbolic reasoning and verification techniques, our pipeline generates explainable, verifiable, and trustworthy optical network designs. This approach significantly advances IBN by ensuring reliability and correctness, essential for mission-critical networking tasks.', 'abstract_zh': '基于意图的网络（IBN）旨在通过允许用户指定高层次目标来简化网络管理，这些目标驱动自动化网络设计和配置。然而，由于大型语言模型（LLMs）固有的模糊性和严谨性的缺乏，将非正式的自然语言意图转换为正式正确的光网络拓扑仍然具有挑战性。为此，我们提出了一种新颖的混合流水线，该流水线结合了基于大型语言模型的意图解析、形式方法以及光检索增强生成（RAG）。通过使用领域特定的光网络标准丰富设计决策，并系统地结合符号推理和验证技术，我们的流水线生成可解释、可验证且可信赖的光网络设计。这种方法显著推进了IBN，确保了关键任务网络中的可靠性和正确性。', 'title_zh': '基于意图的 Optical 网络设计中语言模型与形式方法的桥梁构建'}
{'arxiv_id': 'arXiv:2509.22832', 'title': 'Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM', 'authors': 'Biyao Zhang, Mingkai Zheng, Debargha Ganguly, Xuecen Zhang, Vikash Singh, Vipin Chaudhary, Zhao Zhang', 'link': 'https://arxiv.org/abs/2509.22832', 'abstract': 'Training Large Language Models(LLMs) is one of the most compute-intensive tasks in high-performance computing. Predicting end-to-end training time for multi-billion parameter models distributed across hundreds of GPUs remains challenging due to complex interactions between transformer components, parallelism strategies(data, model, pipeline, tensor), and multi-tier communication. Learned models require costly sampling, while analytical models often struggle with real-world network and hardware complexities. We address this by decomposing LLMs into core computational primitives and modeling them with: (1) operator-level decomposition for fine-grained analysis; (2) lightweight sampling based hardware-aware prediction models for key operations; (3) an end-to-end prediction system integrating these components across complex parallelization strategies. Crucially, our methodology has been validated on two large-scale HPC systems. Our framework achieves low average prediction errors-4.98\\% on Perlmutter(A100) and 9.38\\% on Vista(GH200)-for models up to 20B parameters across 128 GPUs. Importantly, it runs entirely on CPUs, enabling rapid iteration over hardware configurations and training strategies without costly on-cluster experimentation.', 'abstract_zh': '大规模语言模型（LLMs）训练是高性能计算中计算密集型的任务。由于变压器组件、并行策略（数据、模型、管道、张量）以及多层通信之间的复杂相互作用，预测跨越数百个GPU的多百亿参数模型的端到端训练时间仍然具有挑战性。我们通过将LLMs分解为核心计算 primitive，并使用以下方法进行建模：（1）操作级别分解进行细粒度分析；（2）基于轻量级采样的硬件感知预测模型针对关键操作；（3）将这些组件集成到复杂并行化策略的端到端预测系统中。尤为重要的是，我们的方法已在两个大型HPC系统上进行了验证。我们的框架在Perlmutter(A100)上实现了低平均预测误差4.98%，在Vista(GH200)上实现了9.38%，适用于128个GPU上至多20B参数的模型。更重要的是，该框架完全在CPU上运行，允许在不进行昂贵的集群实验的情况下快速迭代硬件配置和训练策略。', 'title_zh': '高效的细粒度GPU性能建模以支持分布式深度学习大语言模型'}
{'arxiv_id': 'arXiv:2509.22764', 'title': 'In-Context Learning can Perform Continual Learning Like Humans', 'authors': 'Liuwang Kang, Fan Wang, Shaoshan Liu, Hung-Chyun Chou, Chuan Lin, Ning Ding', 'link': 'https://arxiv.org/abs/2509.22764', 'abstract': 'Large language models (LLMs) can adapt to new tasks via in-context learning (ICL) without parameter updates, making them powerful learning engines for fast adaptation. While extensive research has examined ICL as a few-shot learner, whether it can achieve long-term retention and cross-task knowledge accumulation when multitasks arrive sequentially remains underexplored. Motivated by human memory studies, we investigate the retention characteristics of ICL in multitask settings and extend it to in-context continual learning (ICCL), where continual learning ability emerges through task scheduling and prompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that, for specific large-language models, ICCL benefits from distributed practice (DP) in a manner analogous to humans, consistently revealing a spacing "sweet spot" for retention. Beyond retention performance, we propose a human-retention similarity metric to quantify how closely a continual-learning (CL) method aligns with human retention dynamics. Using this metric, we show that linear-attention models such as MAMBA and RWKV exhibit particularly human-like retention patterns, despite their retention performance lagging behind that of Transformer-based LLMs. Overall, our results establish ICCL as both cognitively plausible and practically effective, providing an inference-only CL paradigm that mitigates catastrophic forgetting and addresses the stability-plasticity dilemma in conventional CL methods.', 'abstract_zh': '大规模语言模型（LLMs）可以通过上下文学习（ICL）在不更新参数的情况下适应新任务，使其成为快速适应的强大学习引擎。虽然已有大量研究将ICL视为少样本学习者，但其在多任务按序列到来时是否能够实现长期保留和跨任务知识积累仍鲜有探索。受人类记忆研究的启发，我们研究了ICL在多任务设置中的保留特性，并将其扩展到上下文连续学习（ICCL），其中通过任务调度和提示重组逐渐发展出连续学习能力。Markov链基准实验表明，对于特定的大规模语言模型，ICCL以类似于人类的方式从分散练习中受益，持续揭示出一种“间隔优化”的保留“甜蜜点”。除了保留性能外，我们提出了一种人类保留相似度度量，用于量化连续学习（CL）方法与人类保留动态的接近程度。利用这一度量，我们展示了线性注意力模型如MAMBA和RWKV表现出特别类似人类的记忆模式，尽管它们的保留性能落后于基于Transformer的大规模语言模型。总体而言，我们的结果确立了ICCL作为认知上合理且实际有效的学习方法的地位，为避免灾难性遗忘并解决传统CL方法中的稳定性和可塑性困境提供了一种仅进行推理的CL范式。', 'title_zh': '上下文学习可以像人类一样进行连续学习'}
{'arxiv_id': 'arXiv:2509.22745', 'title': 'Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment', 'authors': 'Jaehan Kim, Minkyoo Song, Seungwon Shin, Sooel Son', 'link': 'https://arxiv.org/abs/2509.22745', 'abstract': 'Recent large language models (LLMs) have increasingly adopted the Mixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily depend on a superficial safety mechanism in which harmful inputs are routed safety-critical experts. However, our analysis reveals that routing decisions for harmful inputs drift significantly after fine-tuning, exposing a critical vulnerability to harmful fine-tuning (HFT) attacks. Existing defenses, primarily designed for monolithic LLMs, are less effective for MoE LLMs as they fail to prevent drift in harmful input routing. To address this limitation, we propose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE directly mitigates routing drift by penalizing the gap between the routing weights of a fine-tuned model and those of the initial safety-aligned model, thereby preserving the safety-aligned routing of harmful inputs to safety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to 141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks, reducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while maintaining task utility within 1% degradation and incurring only 2% overhead. It significantly outperforms state-of-the-art defense methods for safeguarding LLM fine-tuning and remains effective in recent large-scale MoE LLMs such as gpt-oss and Llama 4. Our implementation is available at this https URL.', 'abstract_zh': 'Recent Large Language Models (LLMs)采用Mixture-of-Experts (MoE)架构以提高效率存在安全漏洞：SafeMoE——一种针对MoE LLMs的安全微调方法', 'title_zh': '面向有害微调的安全路由对齐防御MoE大语言模型'}
{'arxiv_id': 'arXiv:2509.22739', 'title': 'Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models', 'authors': 'Sasha Cui, Zhongren Chen', 'link': 'https://arxiv.org/abs/2509.22739', 'abstract': 'Language models (LMs) are typically post-trained for desired capabilities and behaviors via weight-based or prompt-based steering, but the former is time-consuming and expensive, and the latter is not precisely controllable and often requires manual trial-and-error. While activation steering (AS) promises a cheap, fast, and controllable alternative to the two existing post-training methods, current AS techniques require hand-crafted prompt pairs or labor-intensive feature annotation, making them more inconvenient than the plug-and-play methods such as Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of fully automated methods that make AS readily usable with any given labeled dataset, with no need for prompt construction, feature labeling, or human intervention. We evaluate PAS on three open-weight models (Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks; we find that PAS reliably improves performance for behavior tasks, but not for intelligence-oriented tasks. The introspective variant (iPAS) delivers the strongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8% on Alignment). We also show PAS delivers additional gains on top of In-Context Learning (ICL) and SFT. PAS constructs a fast, lightweight activation vector that can be cheaply trained, easily stored, and activated at will. Our results provide a characterization of where AS helps, where it fails, and how to deploy it as a practical, automated LM post-training option.', 'abstract_zh': 'Painless Activation Steering: A Fully Automated Approach for Controllable Post-Training of Language Models', 'title_zh': '无痛激活调控：一种自动轻量级的后训练大型语言模型方法'}
{'arxiv_id': 'arXiv:2509.22735', 'title': 'Regulating the Agency of LLM-based Agents', 'authors': 'Seán Boddy, Joshua Joseph', 'link': 'https://arxiv.org/abs/2509.22735', 'abstract': "As increasingly capable large language model (LLM)-based agents are developed, the potential harms caused by misalignment and loss of control grow correspondingly severe. To address these risks, we propose an approach that directly measures and controls the agency of these AI systems. We conceptualize the agency of LLM-based agents as a property independent of intelligence-related measures and consistent with the interdisciplinary literature on the concept of agency. We offer (1) agency as a system property operationalized along the dimensions of preference rigidity, independent operation, and goal persistence, (2) a representation engineering approach to the measurement and control of the agency of an LLM-based agent, and (3) regulatory tools enabled by this approach: mandated testing protocols, domain-specific agency limits, insurance frameworks that price risk based on agency, and agency ceilings to prevent societal-scale risks. We view our approach as a step toward reducing the risks that motivate the ``Scientist AI'' paradigm, while still capturing some of the benefits from limited agentic behavior.", 'abstract_zh': '随着日益强大的基于大型语言模型（LLM）的代理系统的开发，由不对齐和失去控制引起的风险变得愈加严重。为应对这些风险，我们提出了一个直接衡量和控制这些AI系统自主性的方法。我们将基于LLM的代理的自主性概念化为独立于智能相关指标的一种属性，并与有关自主性的跨学科文献保持一致。我们提出（1）自主性作为一种系统属性，可以通过偏好刚性、独立运行和目标持久性这三个维度来实现操作化，（2）一种用于衡量和控制基于LLM的代理自主性的表示工程方法，以及（3）基于此方法的监管工具：强制性的测试协议、特定领域的自主性限制、基于自主性定价的风险保险框架，以及防止社会层面风险的自主性上限。我们认为，我们的方法是朝着减少由“科学家AI”范式所驱动的风险迈出的一步，同时仍然能够捕捉由有限的自主行为带来的某些益处。', 'title_zh': '基于LLM的智能体的代理调节'}
{'arxiv_id': 'arXiv:2509.22734', 'title': 'Automated Formative Feedback for Short-form Writing: An LLM-Driven Approach and Adoption Analysis', 'authors': 'Tiago Fernandes Tavares, Luciano Pereira Soares', 'link': 'https://arxiv.org/abs/2509.22734', 'abstract': "This paper explores the development and adoption of AI-based formative feedback in the context of biweekly reports in an engineering Capstone program. Each student is required to write a short report detailing their individual accomplishments over the past two weeks, which is then assessed by their advising professor. An LLM-powered tool was developed to provide students with personalized feedback on their draft reports, guiding them toward improved completeness and quality. Usage data across two rounds revealed an initial barrier to adoption, with low engagement rates. However, students who engaged in the AI feedback system demonstrated the ability to use it effectively, leading to improvements in the completeness and quality of their reports. Furthermore, the tool's task-parsing capabilities provided a novel approach to identify potential student organizational tasks and deliverables. The findings suggest initial skepticism toward the tool with a limited adoption within the studied context, however, they also highlight the potential for AI-driven tools to provide students and professors valuable insights and formative support.", 'abstract_zh': '本文探索了基于人工智能的形成性反馈在工程综合性课程中两周报告中的发展与采用情况。每个学生需要撰写一份简短的报告，详细说明过去两周的个人成就，该报告随后由指导教授评估。开发了一种基于大语言模型的工具，为学生提供其草稿报告的个性化反馈，引导他们提高报告的完整性和质量。两轮使用数据表明，初期采用存在一定障碍，参与率较低。然而，采用人工智能反馈系统的学生成功利用了该系统，报告的完整性和质量得以提升。此外，该工具的任务解析能力为识别潜在的学生组织任务和交付成果提供了一种新颖的方法。研究结果表明，在所研究的背景下，对该工具的初步怀疑导致有限采用，但同时也突显了人工智能驱动工具为学生和教授提供有价值的见解和形成性支持的潜力。', 'title_zh': '短篇写作的自动形成性反馈：一种由LLM驱动的方法与采纳分析'}
{'arxiv_id': 'arXiv:2509.22732', 'title': "Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks", 'authors': 'Haibo Tong, Dongcheng Zhao, Guobin Shen, Xiang He, Dachuan Lin, Feifei Zhao, Yi Zeng', 'link': 'https://arxiv.org/abs/2509.22732', 'abstract': 'The remarkable capabilities of Large Language Models (LLMs) have raised significant safety concerns, particularly regarding "jailbreak" attacks that exploit adversarial prompts to bypass safety alignment mechanisms. Existing defense research primarily focuses on single-turn attacks, whereas multi-turn jailbreak attacks progressively break through safeguards through by concealing malicious intent and tactical manipulation, ultimately rendering conventional single-turn defenses ineffective. To address this critical challenge, we propose the Bidirectional Intention Inference Defense (BIID). The method integrates forward request-based intention inference with backward response-based intention retrospection, establishing a bidirectional synergy mechanism to detect risks concealed within seemingly benign inputs, thereby constructing a more robust guardrails that effectively prevents harmful content generation. The proposed method undergoes systematic evaluation compared with a no-defense baseline and seven representative defense methods across three LLMs and two safety benchmarks under 10 different attack methods. Experimental results demonstrate that the proposed method significantly reduces the Attack Success Rate (ASR) across both single-turn and multi-turn jailbreak attempts, outperforming all existing baseline methods while effectively maintaining practical utility. Notably, comparative experiments across three multi-turn safety datasets further validate the proposed model\'s significant advantages over other defense approaches.', 'abstract_zh': '大型语言模型的双向意图推理防御（BIID）：应对多轮 Jailbreak 攻击的安全挑战', 'title_zh': '双向意图推理增强LLMs对多轮 Jailbreak 攻击的防御能力'}
{'arxiv_id': 'arXiv:2509.22725', 'title': 'A Meta-Analysis of LLM Effects on Students across Qualification, Socialisation, and Subjectification', 'authors': 'Jiayu Huang, Ruoxin Ritter Wang, Jen-Hao Liu, Boming Xia, Yue Huang, Ruoxi Sun, Jason, Jinan Zou', 'link': 'https://arxiv.org/abs/2509.22725', 'abstract': 'Large language models (LLMs) are increasingly positioned as solutions for education, yet evaluations often reduce their impact to narrow performance metrics. This paper reframes the question by asking "what kind of impact should LLMs have in education?" Drawing on Biesta\'s tripartite account of good education: qualification, socialisation, and subjectification, we present a meta-analysis of 133 experimental and quasi-experimental studies (k = 188). Overall, the impact of LLMs on student learning is positive but uneven. Strong effects emerge in qualification, particularly when LLMs function as tutors in sustained interventions. Socialisation outcomes appear more variable, concentrated in sustained, reflective interventions. Subjectification, linked to autonomy and learner development, remains fragile, with improvements confined to small-scale, long-term studies. This purpose-level view highlights design as the decisive factor: without scaffolds for participation and agency, LLMs privilege what is easiest to measure while neglecting broader aims of education. For HCI and education, the issue is not just whether LLMs work, but what futures they enable or foreclose.', 'abstract_zh': '大型语言模型（LLMs）在教育中的影响不应局限于狭隘的性能指标：基于Biesta的教育三元解释，LLMs在教育中的影响种类是什么？', 'title_zh': 'LLM对学生在资格、社会化和主体性影响的元分析'}
{'arxiv_id': 'arXiv:2509.22715', 'title': 'TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?', 'authors': 'Jiho Park, Jongyoon Song, Minjin Choi, Kyuho Heo, Taehun Huh, Ji Won Kim', 'link': 'https://arxiv.org/abs/2509.22715', 'abstract': 'Large language models (LLMs) are increasingly integral as productivity assistants, but existing benchmarks fall short in rigorously evaluating their real-world instruction-following capabilities. Current benchmarks often (i) lack sufficient multilinguality, (ii) fail to capture the implicit constraints inherent in user requests, and (iii) overlook the complexities of multi-turn dialogue. To address these critical gaps and provide a more realistic assessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation Benchmark)1, a novel benchmark specifically designed for LLM-based productivity assistants. TRUEBench distinguishes itself by featuring input prompts across 12 languages, incorporating intra-instance multilingual instructions, employing rigorous evaluation criteria to capture both explicit and implicit constraints, and including complex multi-turn dialogue scenarios with both accumulating constraints and context switches. Furthermore, to ensure reliability in evaluation, we refined constraints using an LLM validator. Extensive experiments demonstrate that TRUEBench presents significantly greater challenges than existing benchmarks; for instance, a strong model like OpenAI o1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and realistic assessment of LLMs in practical productivity settings, highlighting their capabilities and limitations.', 'abstract_zh': 'TRUEBench：信任的现实世界使用评估基准', 'title_zh': 'TRUEBench: 语言模型响应能否满足生产力助手的现实world约束？'}
{'arxiv_id': 'arXiv:2509.22703', 'title': 'AccessEval: Benchmarking Disability Bias in Large Language Models', 'authors': 'Srikant Panda, Amit Agarwal, Hitesh Laxmichand Patel', 'link': 'https://arxiv.org/abs/2509.22703', 'abstract': 'Large Language Models (LLMs) are increasingly deployed across diverse domains but often exhibit disparities in how they handle real-life queries. To systematically investigate these effects within various disability contexts, we introduce \\textbf{AccessEval (Accessibility Evaluation)}, a benchmark evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9 disability types using paired Neutral and Disability-Aware Queries. We evaluated model outputs with metrics for sentiment, social perception, and factual accuracy.\nOur analysis reveals that responses to disability-aware queries tend to have a more negative tone, increased stereotyping, and higher factual error compared to neutral queries. These effects show notable variation by domain and disability type, with disabilities affecting hearing, speech, and mobility disproportionately impacted. These disparities reflect persistent forms of ableism embedded in model behavior.\nBy examining model performance in real-world decision-making contexts, we better illuminate how such biases can translate into tangible harms for disabled users. This framing helps bridges the gap between technical evaluation and user impact, reinforcing importance of bias mitigation in day-to-day applications. Our dataset is publicly available at: this https URL', 'abstract_zh': '大规模语言模型（LLMs）在不同领域中的应用越来越多，但它们在处理现实查询时常常表现出差异。为了系统地研究这些差异在不同残疾人背景下的影响，我们提出了**AccessEval（可访问性评估）**基准，该基准评估了21个开源和闭源的大规模语言模型在6个真实世界领域和9种残疾人类型上的配对中立和残疾人意识查询。我们使用情感、社会认知和事实准确性指标评估模型输出。\n\n我们的分析发现，针对残疾人意识查询的回答往往具有更负面的语气、增加的刻板印象和更高的事实错误率，相较于中立查询。这些影响在不同领域和残疾人类型上表现出明显的差异，听觉、语言和行动障碍的残疾人影响尤为显著。这些差异反映了嵌入在模型行为中的持久性听障主义。\n\n通过在现实世界决策背景中评估模型性能，我们更好地揭示了这些偏见如何转化为残疾用户的具体伤害。这种框架有助于弥合技术评估与用户影响之间的差距，增强了在日常应用中减少偏见的重要性。我们的数据集可在以下链接获取：[this https URL]。', 'title_zh': 'AccessEval：评估大型语言模型中的残疾偏见'}
{'arxiv_id': 'arXiv:2509.22658', 'title': 'How good are LLMs at Retrieving Documents in a Specific Domain?', 'authors': 'Nafis Tanveer Islam, Zhiming Zhao', 'link': 'https://arxiv.org/abs/2509.22658', 'abstract': "Classical search engines using indexing methods in data infrastructures primarily allow keyword-based queries to retrieve content. While these indexing-based methods are highly scalable and efficient, due to a lack of an appropriate evaluation dataset and a limited understanding of semantics, they often fail to capture the user's intent and generate incomplete responses during evaluation. This problem also extends to domain-specific search systems that utilize a Knowledge Base (KB) to access data from various research infrastructures. Research infrastructures (RIs) from the environmental and earth science domain, which encompass the study of ecosystems, biodiversity, oceanography, and climate change, generate, share, and reuse large volumes of data. While there are attempts to provide a centralized search service using Elasticsearch as a knowledge base, they also face similar challenges in understanding queries with multiple intents. To address these challenges, we proposed an automated method to curate a domain-specific evaluation dataset to analyze the capability of a search system. Furthermore, we incorporate the Retrieval of Augmented Generation (RAG), powered by Large Language Models (LLMs), for high-quality retrieval of environmental domain data using natural language queries. Our quantitative and qualitative analysis of the evaluation dataset shows that LLM-based systems for information retrieval return results with higher precision when understanding queries with multiple intents, compared to Elasticsearch-based systems.", 'abstract_zh': '基于索引方法的经典搜索引擎主要允许关键词查询以检索内容。尽管这些基于索引的方法在可扩展性和效率方面表现出色，但由于缺乏适当的评估数据集和对语义理解有限，它们往往无法准确捕捉用户的意图并在评估中生成不完整的结果。这一问题同样扩展到了利用知识库（KB）访问来自各种研究基础设施数据的领域特定搜索引擎。来自环境与地球科学领域的研究基础设施（RIs），涵盖了生态系统、生物多样性、海洋学和气候变化的研究，生成、共享和重复使用大量数据。尽管有尝试使用Elasticsearch作为知识库提供集中式搜索服务，但它们同样面临理解多意图查询的相似挑战。为了解决这些挑战，我们提出了一个自动化的领域特定评估数据集的编纂方法，以分析搜索系统的性能。此外，我们还结合了由大型语言模型（LLMs）驱动的增强检索（RAG），使用自然语言查询高质量地检索环境领域数据。我们的评估数据集的定量和定性分析表明，基于LLM的检索系统在理解多意图查询时返回的结果精度更高，相比基于Elasticsearch的系统。', 'title_zh': '特定领域中大语言模型检索文档的能力如何？'}
