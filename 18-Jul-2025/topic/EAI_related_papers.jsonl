{'arxiv_id': 'arXiv:2507.13340', 'title': 'Latent Policy Steering with Embodiment-Agnostic Pretrained World Models', 'authors': 'Yiqi Wang, Mrinal Verghese, Jeff Schneider', 'link': 'https://arxiv.org/abs/2507.13340', 'abstract': 'Learning visuomotor policies via imitation has proven effective across a wide range of robotic domains. However, the performance of these policies is heavily dependent on the number of training demonstrations, which requires expensive data collection in the real world. In this work, we aim to reduce data collection efforts when learning visuomotor robot policies by leveraging existing or cost-effective data from a wide range of embodiments, such as public robot datasets and the datasets of humans playing with objects (human data from play). Our approach leverages two key insights. First, we use optic flow as an embodiment-agnostic action representation to train a World Model (WM) across multi-embodiment datasets, and finetune it on a small amount of robot data from the target embodiment. Second, we develop a method, Latent Policy Steering (LPS), to improve the output of a behavior-cloned policy by searching in the latent space of the WM for better action sequences. In real world experiments, we observe significant improvements in the performance of policies trained with a small amount of data (over 50% relative improvement with 30 demonstrations and over 20% relative improvement with 50 demonstrations) by combining the policy with a WM pretrained on two thousand episodes sampled from the existing Open X-embodiment dataset across different robots or a cost-effective human dataset from play.', 'abstract_zh': '通过利用现有或低成本的多体模数据进行类目仿真的视觉运动机器人策略学习：减少数据收集努力的方法', 'title_zh': '基于体感无关先验世界模型的潜在策略引导'}
{'arxiv_id': 'arXiv:2507.13277', 'title': 'Evaluating Reinforcement Learning Algorithms for Navigation in Simulated Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour', 'authors': 'Emma M. A. Harrison', 'link': 'https://arxiv.org/abs/2507.13277', 'abstract': "Robots are increasingly integrated across industries, particularly in healthcare. However, many valuable applications for quadrupedal robots remain overlooked. This research explores the effectiveness of three reinforcement learning algorithms in training a simulated quadruped robot for autonomous navigation and obstacle avoidance. The goal is to develop a robotic guide dog simulation capable of path following and obstacle avoidance, with long-term potential for real-world assistance to guide dogs and visually impaired individuals. It also seeks to expand research into medical 'pets', including robotic guide and alert dogs.\nA comparative analysis of thirteen related research papers shaped key evaluation criteria, including collision detection, pathfinding algorithms, sensor usage, robot type, and simulation platforms. The study focuses on sensor inputs, collision frequency, reward signals, and learning progression to determine which algorithm best supports robotic navigation in complex environments.\nCustom-made environments were used to ensure fair evaluation of all three algorithms under controlled conditions, allowing consistent data collection. Results show that Proximal Policy Optimization (PPO) outperformed Deep Q-Network (DQN) and Q-learning across all metrics, particularly in average and median steps to goal per episode.\nBy analysing these results, this study contributes to robotic navigation, AI and medical robotics, offering insights into the feasibility of AI-driven quadruped mobility and its role in assistive robotics.", 'abstract_zh': '四足机器人在医疗领域的潜在应用探索：强化学习算法在自主导航和障碍避让中的效果比较', 'title_zh': '基于导盲犬行为的模拟四足机器人导航中强化学习算法的比较研究'}
{'arxiv_id': 'arXiv:2507.13200', 'title': 'Few-shot transfer of tool-use skills using human demonstrations with proximity and tactile sensing', 'authors': 'Marina Y. Aoyama, Sethu Vijayakumar, Tetsuya Narita', 'link': 'https://arxiv.org/abs/2507.13200', 'abstract': 'Tools extend the manipulation abilities of robots, much like they do for humans. Despite human expertise in tool manipulation, teaching robots these skills faces challenges. The complexity arises from the interplay of two simultaneous points of contact: one between the robot and the tool, and another between the tool and the environment. Tactile and proximity sensors play a crucial role in identifying these complex contacts. However, learning tool manipulation using these sensors remains challenging due to limited real-world data and the large sim-to-real gap. To address this, we propose a few-shot tool-use skill transfer framework using multimodal sensing. The framework involves pre-training the base policy to capture contact states common in tool-use skills in simulation and fine-tuning it with human demonstrations collected in the real-world target domain to bridge the domain gap. We validate that this framework enables teaching surface-following tasks using tools with diverse physical and geometric properties with a small number of demonstrations on the Franka Emika robot arm. Our analysis suggests that the robot acquires new tool-use skills by transferring the ability to recognise tool-environment contact relationships from pre-trained to fine-tuned policies. Additionally, combining proximity and tactile sensors enhances the identification of contact states and environmental geometry.', 'abstract_zh': '工具扩展了机器人的操作能力，如同扩展了人类的能力。尽管人类在工具操作方面具有专业知识，但教会机器人这些技能仍然面临挑战。这种复杂性源于两个同时接触点的交互：一个是机器人与工具之间的接触，另一个是工具与环境之间的接触。触觉和接近传感器在识别这些复杂的接触中扮演着关键角色。然而，使用这些传感器学习工具操作仍然具有挑战性，原因在于实际数据有限以及模拟与现实之间的巨大差距。为了解决这一问题，我们提出了一种基于多模态 sensing 的小样本工具使用技能转移框架。该框架通过在模拟中预训练基础策略以捕捉工具使用技能中常见的接触状态，并利用在真实目标域中收集的人类演示进行微调，从而弥合领域差距。我们验证了该框架能够使用少量演示教会 Franka Emika 机器人臂执行具有不同物理和几何特性的表面跟随任务。我们的分析表明，机器人通过将预训练到微调策略中的识别工具-环境接触关系的能力转移到新的任务中来获取新的工具使用技能。此外，结合接近传感器和触觉传感器增强了接触状态和环境几何形状的识别。', 'title_zh': '基于近距离和触觉感知的人类示范 few-shot 工具使用技能迁移'}
{'arxiv_id': 'arXiv:2507.13171', 'title': 'Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback', 'authors': 'Suzie Kim, Hye-Bin Shin, Seong-Whan Lee', 'link': 'https://arxiv.org/abs/2507.13171', 'abstract': 'Conventional reinforcement learning (RL) ap proaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, rein forcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, en abling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.', 'abstract_zh': '基于隐式人类反馈的强化学习框架（RLIHF）：利用脑电误差相关潜在成分实现可扩展的人机-aligned强化学习', 'title_zh': '通过隐式人类反馈强化学习实现人类与机器人对齐'}
{'arxiv_id': 'arXiv:2507.13019', 'title': 'Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities', 'authors': 'Liuyi Wang, Xinyuan Xia, Hui Zhao, Hanqing Wang, Tai Wang, Yilun Chen, Chengju Liu, Qijun Chen, Jiangmiao Pang', 'link': 'https://arxiv.org/abs/2507.13019', 'abstract': "Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at this https URL.", 'abstract_zh': '最近的Vision-and-Language Navigation (VLN)进展充满 promise，但其对机器人运动和控制的理想化假设未能反映实际身体化部署中的挑战。为了缩小这一差距，我们引入了VLN-PE，这是一个物理现实的VLN平台，支持人形、四足和轮式机器人。首次系统地评估了几种以自我为中心的VLN方法在不同技术管道下的物理机器人环境中，包括用于单步离散动作预测的分类模型、用于密集航点预测的扩散模型，以及基于路径规划的大语言模型（LLM），无需训练。我们的结果揭示了由于有限的机器人观察空间、环境光照变化以及碰撞和跌倒等物理挑战导致的重大性能下降。这还暴露了复杂环境中腿部机器人的运动限制。VLN-PE具有高度可扩展性，允许无缝集成超越MP3D的新场景，从而实现更全面的VLN评估。尽管当前模型在物理部署中的泛化能力较弱，但VLN-PE为提高跨实体适应性提供了新的途径。我们希望我们的发现和工具能激励社区重新思考VLN的局限性，并推动稳健、实用的VLN模型的发展。代码可在以下链接获取。', 'title_zh': '重新审视视觉-语言导航中的身体差距：一种全面研究物理与视觉差异的探究'}
{'arxiv_id': 'arXiv:2507.12977', 'title': 'Non-differentiable Reward Optimization for Diffusion-based Autonomous Motion Planning', 'authors': 'Giwon Lee, Daehee Park, Jaewoo Jeong, Kuk-Jin Yoon', 'link': 'https://arxiv.org/abs/2507.12977', 'abstract': 'Safe and effective motion planning is crucial for autonomous robots. Diffusion models excel at capturing complex agent interactions, a fundamental aspect of decision-making in dynamic environments. Recent studies have successfully applied diffusion models to motion planning, demonstrating their competence in handling complex scenarios and accurately predicting multi-modal future trajectories. Despite their effectiveness, diffusion models have limitations in training objectives, as they approximate data distributions rather than explicitly capturing the underlying decision-making dynamics. However, the crux of motion planning lies in non-differentiable downstream objectives, such as safety (collision avoidance) and effectiveness (goal-reaching), which conventional learning algorithms cannot directly optimize. In this paper, we propose a reinforcement learning-based training scheme for diffusion motion planning models, enabling them to effectively learn non-differentiable objectives that explicitly measure safety and effectiveness. Specifically, we introduce a reward-weighted dynamic thresholding algorithm to shape a dense reward signal, facilitating more effective training and outperforming models trained with differentiable objectives. State-of-the-art performance on pedestrian datasets (CrowdNav, ETH-UCY) compared to various baselines demonstrates the versatility of our approach for safe and effective motion planning.', 'abstract_zh': '安全且有效的运动规划对于自主机器人至关重要。扩散模型在捕捉复杂代理交互方面表现出色，这是动态环境决策制定的基本方面。最近的研究成功将扩散模型应用于运动规划，证明了它们在处理复杂场景和准确预测多模态未来轨迹方面的能力。尽管这些模型有效，但在训练目标方面仍有限制，因为它们近似数据分布而非明确捕捉底层的决策动态。然而，运动规划的核心在于非可微的下游目标，如安全性（碰撞避免）和有效性（目标达成），这些目标 conventional 学习算法无法直接优化。在本文中，我们提出了一种基于强化学习的训练方案，使扩散运动规划模型能够有效地学习非可微的目标，这些目标能够明确衡量安全性和有效性。具体地，我们引入了一种奖励加权动态阈值算法来塑造密集的奖励信号，从而提高训练效果，并优于使用可微目标训练的模型。与各种基线在行人数据集（CrowdNav, ETH-UCY）上的先进性能表明了我们方法在安全且有效运动规划中的适用性。', 'title_zh': '基于扩散的自主运动规划中的非可微奖励优化'}
{'arxiv_id': 'arXiv:2507.12911', 'title': 'LaViPlan : Language-Guided Visual Path Planning with RLVR', 'authors': 'Hayeon Oh', 'link': 'https://arxiv.org/abs/2507.12911', 'abstract': 'Out-of-distribution (OOD) scenarios in autonomous driving refer to situations that deviate from the training domain, often leading to unexpected and potentially hazardous behavior from planners that lack prior exposure to such cases. Recently, Vision-Language Models (VLMs) have been introduced into autonomous driving research for their promising generalization capabilities in OOD settings. Early studies demonstrated that VLMs could recognize OOD scenarios and generate user-level decisions such as "go straight" or "turn right." However, a new challenge has emerged due to the misalignment between the VLM\'s high-level decisions or visual reasoning expressed in language, and the low-level predicted trajectories interpreted as actions. In this paper, we propose LaViPlan, a framework that leverages Reinforcement Learning with Verifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics. This approach addresses the vision-language-action misalignment observed in existing VLMs fine-tuned via supervised learning, which can recognize driving scenarios but often produce context-unaware decisions. Experimental results demonstrate that our method improves situational awareness and decision-making under OOD conditions, highlighting its potential to mitigate the misalignment issue. This work introduces a promising post-training paradigm for VLM agents in the context of autonomous driving.', 'abstract_zh': '自主驾驶中离分布(out-of-distribution, OOD)场景指那些偏离训练域的情况，往往会导致规划器产生未预期且可能具有安全隐患的行为。近年来，视觉-语言模型(Vision-Language Models, VLMs)被引入到自主驾驶研究中，因其在OOD环境下的泛化能力表现突出。早期研究表明，VLMs能够识别OOD场景并生成诸如“直行”或“右转”的用户级决策。然而，由于VLM在高层决策或语言表达的视觉推理与低层预测轨迹作为行动解释之间存在不匹配，一个新的挑战由此出现。本文提出了一种命名为LaViPlan的框架，该框架利用可验证奖励的强化学习(Reinforcement Learning with Verifiable Rewards, RLVR)来使用面向规划的指标优化VLMs。该方法解决了现有通过监督学习微调的VLMs中存在的视觉-语言-行动不匹配问题，这些问题能够识别驾驶场景，但经常产生缺乏情境意识的决策。实验结果表明，我们的方法在OOD条件下提高了情景意识和决策能力，凸显了其缓解不匹配问题的潜力。本文为视觉-语言-行动代理在自主驾驶领域的后训练范式引入了一个有前景的框架。', 'title_zh': 'LaViPlan：基于语言导向的视觉路径规划与RLVR'}
{'arxiv_id': 'arXiv:2507.12855', 'title': 'DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task Demonstration Learning', 'authors': 'Rahel Rickenbach, Bruce Lee, René Zurbrügg, Carmen Amo Alonso, Melanie N. Zeilinger', 'link': 'https://arxiv.org/abs/2507.12855', 'abstract': 'The integration of large language models (LLMs) with control systems has demonstrated significant potential in various settings, such as task completion with a robotic manipulator. A main reason for this success is the ability of LLMs to perform in-context learning, which, however, strongly relies on the design of task examples, closely related to the target tasks. Consequently, employing LLMs to formulate optimal control problems often requires task examples that contain explicit mathematical expressions, designed by trained engineers. Furthermore, there is often no principled way to evaluate for hallucination before task execution. To address these challenges, we propose DEMONSTRATE, a novel methodology that avoids the use of LLMs for complex optimization problem generations, and instead only relies on the embedding representations of task descriptions. To do this, we leverage tools from inverse optimal control to replace in-context prompt examples with task demonstrations, as well as the concept of multitask learning, which ensures target and example task similarity by construction. Given the fact that hardware demonstrations can easily be collected using teleoperation or guidance of the robot, our approach significantly reduces the reliance on engineering expertise for designing in-context examples. Furthermore, the enforced multitask structure enables learning from few demonstrations and assessment of hallucinations prior to task execution. We demonstrate the effectiveness of our method through simulation and hardware experiments involving a robotic arm tasked with tabletop manipulation.', 'abstract_zh': '使用任务示范的LLM与控制系统的集成方法：DEMONSTRATE', 'title_zh': 'DEMONSTRATE：通过多任务示范学习实现零样本语言到机器人控制的转换'}
{'arxiv_id': 'arXiv:2507.12846', 'title': 'Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering', 'authors': 'Muhammad Fadhil Ginting, Dong-Ki Kim, Xiangyun Meng, Andrzej Reinke, Bandi Jai Krishna, Navid Kayhani, Oriana Peltzer, David D. Fan, Amirreza Shaban, Sung-Kyun Kim, Mykel J. Kochenderfer, Ali-akbar Agha-mohammadi, Shayegan Omidshafiei', 'link': 'https://arxiv.org/abs/2507.12846', 'abstract': 'As robots become increasingly capable of operating over extended periods -- spanning days, weeks, and even months -- they are expected to accumulate knowledge of their environments and leverage this experience to assist humans more effectively. This paper studies the problem of Long-term Active Embodied Question Answering (LA-EQA), a new task in which a robot must both recall past experiences and actively explore its environment to answer complex, temporally-grounded questions. Unlike traditional EQA settings, which typically focus either on understanding the present environment alone or on recalling a single past observation, LA-EQA challenges an agent to reason over past, present, and possible future states, deciding when to explore, when to consult its memory, and when to stop gathering observations and provide a final answer. Standard EQA approaches based on large models struggle in this setting due to limited context windows, absence of persistent memory, and an inability to combine memory recall with active exploration. To address this, we propose a structured memory system for robots, inspired by the mind palace method from cognitive science. Our method encodes episodic experiences as scene-graph-based world instances, forming a reasoning and planning algorithm that enables targeted memory retrieval and guided navigation. To balance the exploration-recall trade-off, we introduce value-of-information-based stopping criteria that determines when the agent has gathered sufficient information. We evaluate our method on real-world experiments and introduce a new benchmark that spans popular simulation environments and actual industrial sites. Our approach significantly outperforms state-of-the-art baselines, yielding substantial gains in both answer accuracy and exploration efficiency.', 'abstract_zh': '长期主动体 WEST 迷你问答：记忆系统在机器人中的应用', 'title_zh': '进入思维宝宫：长期主动体态问答中的推理与规划'}
{'arxiv_id': 'arXiv:2507.12753', 'title': 'osmAG-LLM: Zero-Shot Open-Vocabulary Object Navigation via Semantic Maps and Large Language Models Reasoning', 'authors': 'Fujing Xie, Sören Schwertfeger, Hermann Blum', 'link': 'https://arxiv.org/abs/2507.12753', 'abstract': 'Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features, achieving a high level of detail and guiding robots to find objects specified by open-vocabulary language queries. While the issue of scalability for such approaches has received some attention, another fundamental problem is that high-detail object mapping quickly becomes outdated, as objects get moved around a lot. In this work, we develop a mapping and navigation system for object-goal navigation that, from the ground up, considers the possibilities that a queried object can have moved, or may not be mapped at all. Instead of striving for high-fidelity mapping detail, we consider that the main purpose of a map is to provide environment grounding and context, which we combine with the semantic priors of LLMs to reason about object locations and deploy an active, online approach to navigate to the objects. Through simulated and real-world experiments we find that our approach tends to have higher retrieval success at shorter path lengths for static objects and by far outperforms prior approaches in cases of dynamic or unmapped object queries. We provide our code and dataset at: this https URL.', 'abstract_zh': '基于开放词汇的机器人.mapping方法通过预训练的视觉-语言特征丰富密集几何地图，实现高细节水平并指导机器人找到由开放词汇语言查询指定的对象。尽管此类方法的可扩展性问题已受到一定关注，但另一个基本问题是高细节度的目标建图会因对象频繁移动而迅速过时。在本工作中，我们开发了一种面向对象目标导航的建图与导航系统，从头开始就考虑查询对象可能已移动或根本未被建图的可能性。我们不追求高保真度的建图细节，而是认为地图的主要目的是提供环境基础和背景，并结合LLM的语义先验来推理对象位置，采用主动的、在线的方法导航至对象。通过模拟和真实世界的实验，我们发现我们的方法在静态对象较短路径长度的检索成功率上更高，并在动态或未建图对象查询的情况下显著优于先前方法。我们提供了代码和数据集：this https URL。', 'title_zh': 'osmAG-LLM: 基于语义地图和大规模语言模型推理的零样本开放词汇对象导航'}
{'arxiv_id': 'arXiv:2507.12751', 'title': 'Refining Motion for Peak Performance: Identifying Optimal Gait Parameters for Energy-Efficient Quadrupedal Bounding', 'authors': 'Yasser G. Alqaham, Jing Cheng, Zhenyu Gan', 'link': 'https://arxiv.org/abs/2507.12751', 'abstract': 'Energy efficiency is a critical factor in the performance and autonomy of quadrupedal robots. While previous research has focused on mechanical design and actuation improvements, the impact of gait parameters on energetics has been less explored. In this paper, we hypothesize that gait parameters, specifically duty factor, phase shift, and stride duration, are key determinants of energy consumption in quadrupedal locomotion. To test this hypothesis, we modeled the Unitree A1 quadrupedal robot and developed a locomotion controller capable of independently adjusting these gait parameters. Simulations of bounding gaits were conducted in Gazebo across a range of gait parameters at three different speeds: low, medium, and high. Experimental tests were also performed to validate the simulation results. The findings demonstrate that optimizing gait parameters can lead to significant reductions in energy consumption, enhancing the overall efficiency of quadrupedal locomotion. This work contributes to the advancement of energy-efficient control strategies for legged robots, offering insights directly applicable to commercially available platforms.', 'abstract_zh': '四足机器人性能和自主性的关键因素是能效。虽然以往的研究主要关注于机械设计和驱动装置的改进，但步态参数对能耗的影响尚缺乏探索。在本文中，我们假设步态参数，特别是步距因子、相位偏移和步长，是四足行走过程中能耗的关键决定因素。为了验证这一假设，我们使用Unitree A1四足机器人建立了模型，并开发了一个能够独立调整这些步态参数的运动控制器。在Gazebo中，我们在低、中、高三种速度下，对跳跃步态进行了步态参数范围内的仿真实验。实际实验也被用于验证仿真结果。研究发现，优化步态参数可以显著降低能耗，提高四足行走的整体效率。这项工作为腿部机器人高效控制策略的发展做出了贡献，提供了直接适用于商用平台的见解。', 'title_zh': '提高运动效率以达到峰值性能：识别能量高效的四足 bounding 最优步态参数'}
{'arxiv_id': 'arXiv:2507.12644', 'title': 'VLMgineer: Vision Language Models as Robotic Toolsmiths', 'authors': 'George Jiayuan Gao, Tianyu Li, Junyao Shi, Yihan Li, Zizhe Zhang, Nadia Figueroa, Dinesh Jayaraman', 'link': 'https://arxiv.org/abs/2507.12644', 'abstract': "Tool design and use reflect the ability to understand and manipulate the physical world through creativity, planning, and foresight. As such, these capabilities are often regarded as measurable indicators of intelligence across biological species. While much of today's research on robotic intelligence focuses on generating better controllers, inventing smarter tools offers a complementary form of physical intelligence: shifting the onus of problem-solving onto the tool's design. Given the vast and impressive common-sense, reasoning, and creative capabilities of today's foundation models, we investigate whether these models can provide useful priors to automatically design and effectively wield such tools? We present VLMgineer, a framework that harnesses the code generation abilities of vision language models (VLMs) together with evolutionary search to iteratively co-design physical tools and the action plans that operate them to perform a task. We evaluate VLMgineer on a diverse new benchmark of everyday manipulation scenarios that demand creative tool design and use. Across this suite, VLMgineer consistently discovers tools and policies that solve tasks more effectively and innovatively, transforming challenging robotics problems into straightforward executions. It also outperforms VLM-generated designs from human specifications and existing human-crafted tools for everyday tasks. To facilitate future research on automated tool invention, we will release our benchmark and code.", 'abstract_zh': '基于视觉语言模型的工具设计与使用框架：探索基础模型在自动化工具发明中的潜力', 'title_zh': 'VLMgineer: 视觉语言模型作为机器人工具师'}
{'arxiv_id': 'arXiv:2507.12499', 'title': 'ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving', 'authors': 'Yuhang Lu, Jiadong Tu, Yuexin Ma, Xinge Zhu', 'link': 'https://arxiv.org/abs/2507.12499', 'abstract': 'End-to-end autonomous driving has emerged as a promising approach to unify perception, prediction, and planning within a single framework, reducing information loss and improving adaptability. However, existing methods often rely on fixed and sparse trajectory supervision, limiting their ability to capture the hierarchical reasoning process that human drivers naturally employ. To bridge this gap, we propose ReAL-AD, a Reasoning-Augmented Learning framework that structures decision-making in autonomous driving based on the three-tier human cognitive model: Driving Strategy, Driving Decision, and Driving Operation, where Vision-Language Models (VLMs) are incorporated to enhance situational awareness and structured reasoning across these levels. Specifically, we introduce: (1) the Strategic Reasoning Injector, which formulates high-level driving strategies by interpreting complex traffic contexts from VLM-generated insights; (2) the Tactical Reasoning Integrator, which refines strategic intent into interpretable tactical choices such as lane changes, overtaking, and speed adjustments; and (3) the Hierarchical Trajectory Decoder, which progressively translates tactical decisions into precise control actions for smooth and human-like trajectory execution. Extensive evaluations show that integrating our framework improves planning accuracy and safety by over 30%, making end-to-end autonomous driving more interpretable and aligned with human-like hierarchical reasoning. The project page can be found at: \\href{this https URL}{\\texttt{this http URL\\_page/realad}}', 'abstract_zh': '端到端自主驾驶：一种增强学习框架，基于人类认知三层次模型实现决策优化，提高规划准确性和安全性', 'title_zh': 'ReAL-AD: 朝向人类级推理的端到端自主驾驶'}
{'arxiv_id': 'arXiv:2507.12496', 'title': 'FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making', 'authors': 'Yucen Wang, Rui Yu, Shenghua Wan, Le Gan, De-Chuan Zhan', 'link': 'https://arxiv.org/abs/2507.12496', 'abstract': "Foundation Models (FMs) and World Models (WMs) offer complementary strengths in task generalization at different levels. In this work, we propose FOUNDER, a framework that integrates the generalizable knowledge embedded in FMs with the dynamic modeling capabilities of WMs to enable open-ended task solving in embodied environments in a reward-free manner. We learn a mapping function that grounds FM representations in the WM state space, effectively inferring the agent's physical states in the world simulator from external observations. This mapping enables the learning of a goal-conditioned policy through imagination during behavior learning, with the mapped task serving as the goal state. Our method leverages the predicted temporal distance to the goal state as an informative reward signal. FOUNDER demonstrates superior performance on various multi-task offline visual control benchmarks, excelling in capturing the deep-level semantics of tasks specified by text or videos, particularly in scenarios involving complex observations or domain gaps where prior methods struggle. The consistency of our learned reward function with the ground-truth reward is also empirically validated. Our project website is this https URL.", 'abstract_zh': 'Foundation模型（FMs）和世界模型（WMs）在不同层面上为任务泛化提供了互补的优势。在本工作中，我们提出了一种名为FOUNDER的框架，该框架将FMs中嵌入的一般性知识与WMs的动态建模能力相结合，以在奖励免费的方式下实现开放性的任务解决，应用于具身环境。我们学习了一个映射函数，将FM表示嵌入到WM状态空间中，从而有效地从外部观察中推断出代理在世界仿真的物理状态。该映射使代理能够在行为学习过程中通过想象学习带有目标条件的策略，映射的任务作为目标状态。我们的方法利用预测到目标状态的时间距离作为信息性的奖励信号。FOUNDER在各种多任务离线视觉控制基准测试中展示了优越的性能，特别擅长捕捉由文本或视频指定的任务的深层语义，特别是在复杂观察或领域差距等场景中，优于先前的方法。同时，我们的学习奖励函数与真实奖励的一致性也得到了实证验证。我们的项目网站是这个 https URL。', 'title_zh': 'FOUNDER：将基础模型 grounding 在世界模型中以实现开放-ended 体感决策'}
{'arxiv_id': 'arXiv:2507.13231', 'title': 'VITA: Vision-to-Action Flow Matching Policy', 'authors': 'Dechen Gao, Boqi Zhao, Andrew Lee, Ian Chuang, Hanchu Zhou, Hang Wang, Zhe Zhao, Junshan Zhang, Iman Soltani', 'link': 'https://arxiv.org/abs/2507.13231', 'abstract': 'We present VITA, a Vision-To-Action flow matching policy that evolves latent visual representations into latent actions for visuomotor control. Traditional flow matching and diffusion policies sample from standard source distributions (e.g., Gaussian noise) and require additional conditioning mechanisms like cross-attention to condition action generation on visual information, creating time and space overheads. VITA proposes a novel paradigm that treats latent images as the flow source, learning an inherent mapping from vision to action while eliminating separate conditioning modules and preserving generative modeling capabilities. Learning flows between fundamentally different modalities like vision and action is challenging due to sparse action data lacking semantic structures and dimensional mismatches between high-dimensional visual representations and raw actions. We address this by creating a structured action latent space via an autoencoder as the flow matching target, up-sampling raw actions to match visual representation shapes. Crucially, we supervise flow matching with both encoder targets and final action outputs through flow latent decoding, which backpropagates action reconstruction loss through sequential flow matching ODE solving steps for effective end-to-end learning. Implemented as simple MLP layers, VITA is evaluated on challenging bi-manual manipulation tasks on the ALOHA platform, including 5 simulation and 2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or matches state-of-the-art generative policies while reducing inference latency by 50-130% compared to conventional flow matching policies requiring different conditioning mechanisms or complex architectures. To our knowledge, VITA is the first MLP-only flow matching policy capable of solving complex bi-manual manipulation tasks like those in ALOHA benchmarks.', 'abstract_zh': 'VITA：一种将视觉表示演化为潜在动作的视觉到行动流匹配策略', 'title_zh': 'VITA：视觉到动作流动策略匹配'}
{'arxiv_id': 'arXiv:2507.13152', 'title': 'SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models', 'authors': 'Xiangyu Dong, Haoran Zhao, Jiang Gao, Haozhou Li, Xiaoguang Ma, Yaoming Zhou, Fuhai Chen, Juan Liu', 'link': 'https://arxiv.org/abs/2507.13152', 'abstract': 'Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.', 'abstract_zh': '最近视知觉语言导航(VLN)领域的进展主要归因于新兴的大规模语言模型(LLMs)。这些方法在指令理解和任务推理方面表现出了出色的泛化能力。然而，它们受限于LLMs固定的知识基础和推理能力，无法充分整合经验知识，从而导致缺乏高效的进化能力。为了解决这一问题，我们从自然界智能体的进化能力中汲取灵感，提出了一种自进化的VLN框架(SE-VLN)，以赋予VLN代理在测试过程中持续进化的 ability。据我们所知，这是首次提出一种由多模态LLM驱动的自进化的VLN框架。具体而言，SE-VLN 包含三个核心模块：层次化记忆模块用于将成功和失败案例转化为可重复使用的知识，检索增强的思想推理模块用于检索经验并支持多步决策，以及反思模块用于实现持续进化。全面的测试表明，SE-VLN 在未见过的环境中分别实现了57%和35.2%的导航成功率，相较于R2R和REVERSE数据集上的当前最先进的方法，分别有23.9%和15.0%的绝对性能提升。此外，SE-VLN 的性能随经验库的增加而提高，说明其作为VLN领域自进化的代理框架具有巨大的潜力。', 'title_zh': 'SE-VLN：基于多模态大型语言模型的自我进化视觉-语言导航框架'}
{'arxiv_id': 'arXiv:2507.13052', 'title': 'Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication', 'authors': 'Tianyu Song, Feng Li, Yuan Bi, Angelos Karlas, Amir Yousefi, Daniela Branzan, Zhongliang Jiang, Ulrich Eck, Nassir Navab', 'link': 'https://arxiv.org/abs/2507.13052', 'abstract': 'The advancement and maturity of large language models (LLMs) and robotics have unlocked vast potential for human-computer interaction, particularly in the field of robotic ultrasound. While existing research primarily focuses on either patient-robot or physician-robot interaction, the role of an intelligent virtual sonographer (IVS) bridging physician-robot-patient communication remains underexplored. This work introduces a conversational virtual agent in Extended Reality (XR) that facilitates real-time interaction between physicians, a robotic ultrasound system(RUS), and patients. The IVS agent communicates with physicians in a professional manner while offering empathetic explanations and reassurance to patients. Furthermore, it actively controls the RUS by executing physician commands and transparently relays these actions to the patient. By integrating LLM-powered dialogue with speech-to-text, text-to-speech, and robotic control, our system enhances the efficiency, clarity, and accessibility of robotic ultrasound acquisition. This work constitutes a first step toward understanding how IVS can bridge communication gaps in physician-robot-patient interaction, providing more control and therefore trust into physician-robot interaction while improving patient experience and acceptance of robotic ultrasound.', 'abstract_zh': '大型语言模型和机器人技术的进步及其成熟性为人类与计算机交互，特别是在机器人超声领域，开辟了巨大的潜力。虽然现有研究主要集中在患者-机器人或医生-机器人交互上，但智能虚拟超声技师（IVS）在医生-机器人-患者沟通中的作用仍然尚未充分探索。本文介绍了一个在扩展现实（XR）中实现的对话虚拟代理，该代理促进了医生、机器人超声系统（RUS）和患者之间的实时交互。IVS代理以专业的方式与医生交流，并向患者提供同情心解释和支持。此外，它能够主动控制RUS执行医生命令，并透明地向患者传达这些行动。通过将基于LLM的对话与语音转文本、文本转语音和机器人控制相结合，我们的系统增强了机器人超声获取的效率、清晰度和可访问性。本工作标志着理解IVS如何在医生-机器人-患者交互中弥合沟通缺口的第一步，提供了更多的控制和信任，从而改善患者体验并提高对机器人超声的接受度。', 'title_zh': '智能虚拟超声检查员（IVS）：增强医师-机器人-患者沟通'}
{'arxiv_id': 'arXiv:2507.12898', 'title': 'Generalist Bimanual Manipulation via Foundation Video Diffusion Models', 'authors': 'Yao Feng, Hengkai Tan, Xinyi Mao, Guodong Liu, Shuhe Huang, Chendong Xiang, Hang Su, Jun Zhu', 'link': 'https://arxiv.org/abs/2507.12898', 'abstract': 'Bimanual robotic manipulation, which involves the coordinated control of two robotic arms, is foundational for solving challenging tasks. Despite recent progress in general-purpose manipulation, data scarcity and embodiment heterogeneity remain serious obstacles to further scaling up in bimanual settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning (VIDAR), a two-stage framework that leverages large-scale, diffusion-based video pre-training and a novel masked inverse dynamics model for action prediction. We pre-train the video diffusion model on 750K multi-view videos from three real-world bimanual robot platforms, utilizing a unified observation space that encodes robot, camera, task, and scene contexts. Our masked inverse dynamics model learns masks to extract action-relevant information from generated trajectories without requiring pixel-level labels, and the masks can effectively generalize to unseen backgrounds. Our experiments demonstrate that with only 20 minutes of human demonstrations on an unseen robot platform (only 1% of typical data requirements), VIDAR generalizes to unseen tasks and backgrounds with strong semantic understanding, surpassing state-of-the-art methods. Our findings highlight the potential of video foundation models, coupled with masked action prediction, to enable scalable and generalizable robotic manipulation in diverse real-world settings.', 'abstract_zh': '双臂机器人操纵：一种基于视频扩散的行动推理框架', 'title_zh': '基于基础视频扩散模型的通用双臂操作'}
{'arxiv_id': 'arXiv:2507.12768', 'title': 'AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation', 'authors': 'Hengkai Tan, Yao Feng, Xinyi Mao, Shuhe Huang, Guodong Liu, Zhongkai Hao, Hang Su, Jun Zhu', 'link': 'https://arxiv.org/abs/2507.12768', 'abstract': 'Vision-language-action (VLA) models have shown promise on task-conditioned control in complex settings such as bimanual manipulation. However, the heavy reliance on task-specific human demonstrations limits their generalization and incurs high data acquisition costs. In this work, we present a new notion of task-agnostic action paradigm that decouples action execution from task-specific conditioning, enhancing scalability, efficiency, and cost-effectiveness. To address the data collection challenges posed by this paradigm -- such as low coverage density, behavioral redundancy, and safety risks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a scalable self-supervised framework that accelerates collection by over $ 30\\times $ compared to human teleoperation. To further enable effective learning from task-agnostic data, which often suffers from distribution mismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics model equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder (DAD). We additionally integrate a video-conditioned action validation module to verify the feasibility of learned policies across diverse manipulation tasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51% improvement in test accuracy and achieves 30-40% higher success rates in downstream tasks such as lifting, pick-and-place, and clicking, using replay-based video validation. Project Page: this https URL', 'abstract_zh': 'Vision-语言-动作（VLA）模型在双臂操作等复杂环境下的任务条件控制中展示了潜力。然而，对任务特定的人类演示的过度依赖限制了它们的泛化能力并增加了数据获取成本。在这种工作中，我们提出了一种新的任务无关的动作范式，将动作执行与任务特定的条件解耦，增强了可扩展性、效率和成本效益。为了解决由此范式带来的数据采集挑战，如低覆盖率密度、行为冗余和安全风险，我们引入了ATARA（自动化任务无关随机动作）——一个可扩展的自监督框架，与人工远程操作相比，其数据采集速度提高了超过30倍。为了进一步从任务无关的数据中有效学习，这些数据往往存在分布不匹配和无关轨迹的问题，我们提出了AnyPos，这是一种配备臂解耦估计和方向感知解码器（DAD）的逆动力学模型。此外，我们还集成了一个基于视频的动作验证模块，以验证所学习策略在多样化的操作任务中的可行性。广泛的实验表明，AnyPos-ATARA流水线在测试准确性上提高了51%，并在拾取放置、点击等下游任务中实现了30-40%更高的成功率，使用基于回放缓冲区的视频验证。项目页面: [this https URL]', 'title_zh': 'AnyPos: 自动化任务无关双臂操作'}
{'arxiv_id': 'arXiv:2507.12741', 'title': 'Public Evaluation on Potential Social Impacts of Fully Autonomous Cybernetic Avatars for Physical Support in Daily-Life Environments: Large-Scale Demonstration and Survey at Avatar Land', 'authors': 'Lotfi El Hafi, Kazuma Onishi, Shoichi Hasegawa, Akira Oyama, Tomochika Ishikawa, Masashi Osada, Carl Tornberg, Ryoma Kado, Kento Murata, Saki Hashimoto, Sebastian Carrera Villalobos, Akira Taniguchi, Gustavo Alfonso Garcia Ricardez, Yoshinobu Hagiwara, Tatsuya Aoki, Kensuke Iwata, Takato Horii, Yukiko Horikawa, Takahiro Miyashita, Tadahiro Taniguchi, Hiroshi Ishiguro', 'link': 'https://arxiv.org/abs/2507.12741', 'abstract': 'Cybernetic avatars (CAs) are key components of an avatar-symbiotic society, enabling individuals to overcome physical limitations through virtual agents and robotic assistants. While semi-autonomous CAs intermittently require human teleoperation and supervision, the deployment of fully autonomous CAs remains a challenge. This study evaluates public perception and potential social impacts of fully autonomous CAs for physical support in daily life. To this end, we conducted a large-scale demonstration and survey during Avatar Land, a 19-day public event in Osaka, Japan, where fully autonomous robotic CAs, alongside semi-autonomous CAs, performed daily object retrieval tasks. Specifically, we analyzed responses from 2,285 visitors who engaged with various CAs, including a subset of 333 participants who interacted with fully autonomous CAs and shared their perceptions and concerns through a survey questionnaire. The survey results indicate interest in CAs for physical support in daily life and at work. However, concerns were raised regarding task execution reliability. In contrast, cost and human-like interaction were not dominant concerns. Project page: this https URL.', 'abstract_zh': '基于控制论的代理（CAs）是共生型代理社会的关键组成部分，通过虚拟代理和机器人助手使个体超越物理限制。虽然半自主的CAs间歇性地需要人类远程操作和监督，但完全自主的CAs的部署仍具挑战性。本研究评估了公众对日常生活中提供物理支持的完全自主CAs的感知及其潜在社会影响。为此，我们在日本大阪举行的为期19天的公共活动Avatar Land期间，进行了大规模展示和调查，其中完全自主机器人CAs和半自主CAs共同执行日常取物任务。具体而言，我们分析了2,285名与各种CAs互动的访客的反馈，其中333名参与者与完全自主CAs互动并通过调查问卷分享了他们的感知和担忧。调查结果显示，公众对日常生活中和工作中使用CAs以提供物理支持表现出兴趣，但对任务执行可靠性提出了质疑。相比之下，成本和拟人化交互并非主要关注点。项目页面：请点击此处。', 'title_zh': '面向日常生活环境的全自主网络替身物理支持潜在社会影响的公众评价：大规模示范与调查研究在替身之地'}
{'arxiv_id': 'arXiv:2507.12508', 'title': 'MindJourney: Test-Time Scaling with World Models for Spatial Reasoning', 'authors': 'Yuncong Yang, Jiageng Liu, Zheyuan Zhang, Siyuan Zhou, Reuben Tan, Jianwei Yang, Yilun Du, Chuang Gan', 'link': 'https://arxiv.org/abs/2507.12508', 'abstract': 'Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.', 'abstract_zh': '三维空间中的空间推理是人类认知的核心，并且对于导航和操作等具身任务至关重要。然而，最先进的视觉-语言模型（VLMs）在预测主体运动后场景会是什么样子这样简单的工作上经常遇到困难：它们只能感知二维图像，而缺乏对三维动态的内部模型。因此，我们提出了MindJourney，这是一种测试时缩放框架，通过将VLM与基于视频扩散的可控世界模型耦合，赋予其缺失的这种能力。VLM迭代地勾勒出简要的摄像机轨迹，而世界模型则在每一步生成相应的视角。VLM随后对在交互式探索过程中收集到的多视角证据进行推理。无需任何微调，我们的MindJourney在代表性的空间推理基准SAT上平均实现了超过8%的性能提升，表明将VLM与世界模型结合进行测试时缩放提供了一种简单且即插即用的途径，以实现稳健的三维推理。同时，我们的方法也改进了通过强化学习训练的测试时推理VLM，这表明了我们方法利用世界模型进行测试时缩放的潜力。', 'title_zh': 'MindJourney：基于世界模型的测试时扩展方法的空间推理'}
{'arxiv_id': 'arXiv:2507.12821', 'title': 'Assessing adaptive world models in machines with novel games', 'authors': 'Lance Ying, Katherine M. Collins, Prafull Sharma, Cedric Colas, Kaiya Ivy Zhao, Adrian Weller, Zenna Tavares, Phillip Isola, Samuel J. Gershman, Jacob D. Andreas, Thomas L. Griffiths, Francois Chollet, Kelsey R. Allen, Joshua B. Tenenbaum', 'link': 'https://arxiv.org/abs/2507.12821', 'abstract': "Human intelligence exhibits a remarkable capacity for rapid adaptation and effective problem-solving in novel and unfamiliar contexts. We argue that this profound adaptability is fundamentally linked to the efficient construction and refinement of internal representations of the environment, commonly referred to as world models, and we refer to this adaptation mechanism as world model induction. However, current understanding and evaluation of world models in artificial intelligence (AI) remains narrow, often focusing on static representations learned from training on a massive corpora of data, instead of the efficiency and efficacy of models in learning these representations through interaction and exploration within a novel environment. In this Perspective, we provide a view of world model induction drawing on decades of research in cognitive science on how humans learn and adapt so efficiently; we then call for a new evaluation framework for assessing adaptive world models in AI. Concretely, we propose a new benchmarking paradigm based on suites of carefully designed games with genuine, deep and continually refreshing novelty in the underlying game structures -- we refer to this kind of games as novel games. We detail key desiderata for constructing these games and propose appropriate metrics to explicitly challenge and evaluate the agent's ability for rapid world model induction. We hope that this new evaluation framework will inspire future evaluation efforts on world models in AI and provide a crucial step towards developing AI systems capable of the human-like rapid adaptation and robust generalization -- a critical component of artificial general intelligence.", 'abstract_zh': '人类智能展现出在新颖和陌生环境中快速适应和有效解决问题的非凡能力。我们认为，这种深远的适应能力本质上与高效构建和精炼环境的内部表示密切相关，我们称之为世界模型，并将这种适应机制称为世界模型归纳。然而，当前对人工智能（AI）中世界模型的理解和评估仍相对狭隘，常常侧重于从大规模数据集训练中学习到的静态表示，而忽视了模型通过与新环境的互动和探索学习建立这些表示的效率和有效性。在此视角中，我们借鉴认知科学数十年的研究成果，探讨人类如何高效学习和适应；然后呼吁为在AI中评估适应性强的世界模型建立新的评估框架。具体地，我们提出了一种新的基准测试 paradigm，基于一系列精心设计的游戏，这些游戏中底层的游戏结构具有真正的、深刻的、不断更新的新颖性——我们称这类游戏为新颖游戏。我们详细阐述了构建这些游戏的关键需求，并提出了适当的度量标准，以明确挑战和评估智能体快速世界模型归纳的能力。我们希望这一新的评估框架能够启发未来在AI中对世界模型的评估努力，并为开发能够实现人类般快速适应和稳健泛化的AI系统提供关键步骤——这是通用人工智能的关键组成部分。', 'title_zh': '评估机器中的适应性世界模型新型游戏方法'}
{'arxiv_id': 'arXiv:2507.12801', 'title': 'Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning', 'authors': 'Sosui Moribe, Taketoshi Ushiama', 'link': 'https://arxiv.org/abs/2507.12801', 'abstract': "In recent years, peer learning has gained attention as a method that promotes spontaneous thinking among learners, and its effectiveness has been confirmed by numerous studies. This study aims to develop an AI Agent as a learning companion that enables peer learning anytime and anywhere. However, peer learning between humans has various limitations, and it is not always effective. Effective peer learning requires companions at the same proficiency levels. In this study, we assume that a learner's peers with the same proficiency level as the learner make the same mistakes as the learner does and focus on English composition as a specific example to validate this approach.", 'abstract_zh': '近年来，同伴学习作为一种促进学习者自发思考的方法受到了关注，其有效性得到了许多研究的证实。本研究旨在开发一个AI代理作为学习伴侣，使同伴学习随时随地成为可能。然而，人类之间的同伴学习存在各种局限性，并不一定总是有效。有效的同伴学习要求同伴具有与学习者相同的能力水平。本研究假设与学习者能力水平相同的同伴学习者会犯与学习者相同的错误，并以英语作文为例来验证这一方法。', 'title_zh': '在线同伴学习中学习同伴AI代理模仿错误的研究'}
{'arxiv_id': 'arXiv:2507.12666', 'title': 'Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models', 'authors': 'Alex Zook, Josef Spjut, Jonathan Tremblay', 'link': 'https://arxiv.org/abs/2507.12666', 'abstract': "Game design hinges on understanding how static rules and content translate into dynamic player behavior - something modern generative systems that inspect only a game's code or assets struggle to capture. We present an automated design iteration framework that closes this gap by pairing a reinforcement learning (RL) agent, which playtests the game, with a large multimodal model (LMM), which revises the game based on what the agent does. In each loop the RL player completes several episodes, producing (i) numerical play metrics and/or (ii) a compact image strip summarising recent video frames. The LMM designer receives a gameplay goal and the current game configuration, analyses the play traces, and edits the configuration to steer future behaviour toward the goal. We demonstrate results that LMMs can reason over behavioral traces supplied by RL agents to iteratively refine game mechanics, pointing toward practical, scalable tools for AI-assisted game design.", 'abstract_zh': '游戏设计依赖于理解静态规则和内容如何转化为动态玩家行为——这是现代仅检查游戏代码或资产的生成系统难以捕捉到的。我们提出了一种自动化设计迭代框架，通过将强化学习（RL）代理与大型多模态模型（LMM）配对来弥补这一差距，该代理通过游戏测试，LMM则根据代理的行为进行游戏修订。在每个循环中，RL玩家完成多个回合，生成（i）数值游戏指标或（ii）总结最近视频帧的紧凑图像条带。LMM设计师接收游戏目标和当前游戏配置，分析游戏轨迹，并编辑配置以引导未来行为朝向目标。我们展示了LMM能够利用RL代理提供的行为轨迹进行迭代细化游戏机制的结果，指出了面向实际、可扩展的AI辅助游戏设计工具的可能性。', 'title_zh': '飞、失败、修复：基于强化学习和大型多模态模型的迭代游戏修复'}
{'arxiv_id': 'arXiv:2507.12484', 'title': 'AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education', 'authors': 'Jarosław A. Chudziak, Adam Kostka', 'link': 'https://arxiv.org/abs/2507.12484', 'abstract': 'The growing ubiquity of artificial intelligence (AI), in particular large language models (LLMs), has profoundly altered the way in which learners gain knowledge and interact with learning material, with many claiming that AI positively influences their learning achievements. Despite this advancement, current AI tutoring systems face limitations associated with their reactive nature, often providing direct answers without encouraging deep reflection or incorporating structured pedagogical tools and strategies. This limitation is most apparent in the field of mathematics, in which AI tutoring systems remain underdeveloped. This research addresses the question: How can AI tutoring systems move beyond providing reactive assistance to enable structured, individualized, and tool-assisted learning experiences? We introduce a novel multi-agent AI tutoring platform that combines adaptive and personalized feedback, structured course generation, and textbook knowledge retrieval to enable modular, tool-assisted learning processes. This system allows students to learn new topics while identifying and targeting their weaknesses, revise for exams effectively, and practice on an unlimited number of personalized exercises. This article contributes to the field of artificial intelligence in education by introducing a novel platform that brings together pedagogical agents and AI-driven components, augmenting the field with modular and effective systems for teaching mathematics.', 'abstract_zh': '人工智能（尤其是大语言模型）日益普及对学习者获取知识和互动方式产生了深远影响，许多人认为这积极地促进了他们的学习成就。尽管这项进展令人振奋，当前的人工智能辅导系统仍受到其反应性特征的限制，通常直接提供答案，而不鼓励深入反思或整合结构化的教学工具和策略。这种限制在数学领域尤为明显，人工智能辅导系统在这方面的开发仍显不足。本文探讨的问题是：如何使人工智能辅导系统超越提供被动帮助，以促进结构化、个性化和工具辅助的学习体验？我们提出了一种新颖的多智能体人工智能辅导平台，该平台结合了自适应和个性化反馈、结构化课程生成以及教科书知识检索，以促进模块化和工具辅助的学习过程。该系统使学生能够学习新主题、识别和集中解决弱点、有效复习考试，并无限制地进行个性化练习。本文通过引入一种结合了教学代理和人工智能驱动组件的新平台，为数学教育领域带来了模块化和有效的教学系统，从而推动了教育人工智能领域的发展。', 'title_zh': '基于AI的数学辅导平台：个性化和自适应教育heits'}
{'arxiv_id': 'arXiv:2507.13314', 'title': 'Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark', 'authors': 'Junsu Kim, Naeun Kim, Jaeho Lee, Incheol Park, Dongyoon Han, Seungryul Baek', 'link': 'https://arxiv.org/abs/2507.13314', 'abstract': 'The reasoning-based pose estimation (RPE) benchmark has emerged as a widely adopted evaluation standard for pose-aware multimodal large language models (MLLMs). Despite its significance, we identified critical reproducibility and benchmark-quality issues that hinder fair and consistent quantitative evaluations. Most notably, the benchmark utilizes different image indices from those of the original 3DPW dataset, forcing researchers into tedious and error-prone manual matching processes to obtain accurate ground-truth (GT) annotations for quantitative metrics (\\eg, MPJPE, PA-MPJPE). Furthermore, our analysis reveals several inherent benchmark-quality limitations, including significant image redundancy, scenario imbalance, overly simplistic poses, and ambiguous textual descriptions, collectively undermining reliable evaluations across diverse scenarios. To alleviate manual effort and enhance reproducibility, we carefully refined the GT annotations through meticulous visual matching and publicly release these refined annotations as an open-source resource, thereby promoting consistent quantitative evaluations and facilitating future advancements in human pose-aware multimodal reasoning.', 'abstract_zh': '基于推理的姿态估计基准（RPE）已成为评估姿态感知多模态大型语言模型（MLLMs）的广泛采用评估标准。尽管具有重要意义，但我们发现其实现可再现性及基准质量存在关键问题，阻碍了公平和一致的定量评估。最显著的是，该基准使用了与原始3DPW数据集不同的图像索引，迫使研究人员进行繁琐且易出错的手动匹配过程，以获取准确的地面真相（GT）注释以用于定量指标（例如，MPJPE，PA-MPJPE）。此外，我们的分析揭示了基准固有的几个质量限制，包括显著的图像冗余、场景失衡、过于简化的姿态以及模棱两可的文本描述，这些共同削弱了在不同场景下可靠评估的能力。为了减轻人工努力并增强可再现性，我们通过仔细的视觉匹配仔细精炼了GT注释，并公开发布了这些精炼注释作为开源资源，从而促进一致的定量评估并促进未来在姿态感知多模态推理方面的进步。', 'title_zh': '基于推理的 pose 估计基准中的可靠性重探'}
{'arxiv_id': 'arXiv:2507.13162', 'title': 'Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models', 'authors': 'Arian Mousakhan, Sudhanshu Mittal, Silvio Galesso, Karim Farid, Thomas Brox', 'link': 'https://arxiv.org/abs/2507.13162', 'abstract': 'Existing world models for autonomous driving struggle with long-horizon generation and generalization to challenging scenarios. In this work, we develop a model using simple design choices, and without additional supervision or sensors, such as maps, depth, or multiple cameras. We show that our model yields state-of-the-art performance, despite having only 469M parameters and being trained on 280h of video data. It particularly stands out in difficult scenarios like turning maneuvers and urban traffic. We test whether discrete token models possibly have advantages over continuous models based on flow matching. To this end, we set up a hybrid tokenizer that is compatible with both approaches and allows for a side-by-side comparison. Our study concludes in favor of the continuous autoregressive model, which is less brittle on individual design choices and more powerful than the model built on discrete tokens. Code, models and qualitative results are publicly available at this https URL.', 'abstract_zh': '现有的自动驾驶世界模型在长时_horizon生成和应对挑战性场景时表现不佳。在本工作中，我们采用简单的设计选择，无需额外的监督或传感器，如地图、深度信息或多摄像头。实验表明，尽管参数量仅为469M且训练数据量为280小时的视频数据，我们的模型仍能达到最先进的性能，特别在转弯操作和城市交通等困难场景中表现出色。我们测试了离散词元模型与连续模型在流匹配基础上的优劣，通过设置兼容两种方法的混合词元器进行对比分析。研究结果倾向于支持连续自回归模型，这种模型在单一设计选择上较少表现出脆弱性，并且比基于离散词元的模型更为强大。相关代码、模型及定性结果已公开。', 'title_zh': 'Orbis: 克服驾驶世界模型长期预测挑战'}
{'arxiv_id': 'arXiv:2507.13097', 'title': 'GraspGen: A Diffusion-based Framework for 6-DOF Grasping with On-Generator Training', 'authors': 'Adithyavairavan Murali, Balakumar Sundaralingam, Yu-Wei Chao, Wentao Yuan, Jun Yamada, Mark Carlson, Fabio Ramos, Stan Birchfield, Dieter Fox, Clemens Eppner', 'link': 'https://arxiv.org/abs/2507.13097', 'abstract': 'Grasping is a fundamental robot skill, yet despite significant research advancements, learning-based 6-DOF grasping approaches are still not turnkey and struggle to generalize across different embodiments and in-the-wild settings. We build upon the recent success on modeling the object-centric grasp generation process as an iterative diffusion process. Our proposed framework, GraspGen, consists of a DiffusionTransformer architecture that enhances grasp generation, paired with an efficient discriminator to score and filter sampled grasps. We introduce a novel and performant on-generator training recipe for the discriminator. To scale GraspGen to both objects and grippers, we release a new simulated dataset consisting of over 53 million grasps. We demonstrate that GraspGen outperforms prior methods in simulations with singulated objects across different grippers, achieves state-of-the-art performance on the FetchBench grasping benchmark, and performs well on a real robot with noisy visual observations.', 'abstract_zh': '基于学习的6-DOF抓取方法仍然是现成的解决方案，并且难以在不同的身体构型和真实环境设置中泛化。我们构建了最近在将对象中心的抓取生成过程建模为迭代扩散过程方面取得成功的基础。我们提出的框架GraspGen包括一个增强抓取生成的扩散变换器架构，并配有一个高效鉴别器用于评分和筛选采样的抓取。我们引入了一种新的高性能的生成器内训练策略以训练鉴别器。为了将GraspGen扩展到对象和抓手，我们发布了包含超过5300万个抓取的新模拟数据集。我们展示了GraspGen在不同抓手的分离物体仿真中优于先前的方法，在FetchBench抓取基准上的性能达到最新水平，并且在具有噪声视觉观测的实际机器人上表现良好。', 'title_zh': 'GraspGen：一种基于扩散的六自由度抓取框架及其生成器上的训练方法'}
{'arxiv_id': 'arXiv:2507.12916', 'title': 'Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding With Large Language Models', 'authors': 'Yifan Xu, Chao Zhang, Hanqi Jiang, Xiaoyan Wang, Ruifei Ma, Yiwei Li, Zihao Wu, Zeju Li, Xiangde Liu', 'link': 'https://arxiv.org/abs/2507.12916', 'abstract': 'Advancements in foundation models have made it possible to conduct applications in various downstream tasks. Especially, the new era has witnessed a remarkable capability to extend Large Language Models (LLMs) for tackling tasks of 3D scene understanding. Current methods rely heavily on 3D point clouds, but the 3D point cloud reconstruction of an indoor scene often results in information loss. Some textureless planes or repetitive patterns are prone to omission and manifest as voids within the reconstructed 3D point clouds. Besides, objects with complex structures tend to introduce distortion of details caused by misalignments between the captured images and the dense reconstructed point clouds. 2D multi-view images present visual consistency with 3D point clouds and provide more detailed representations of scene components, which can naturally compensate for these deficiencies. Based on these insights, we propose Argus, a novel 3D multimodal framework that leverages multi-view images for enhanced 3D scene understanding with LLMs. In general, Argus can be treated as a 3D Large Multimodal Foundation Model (3D-LMM) since it takes various modalities as input(text instructions, 2D multi-view images, and 3D point clouds) and expands the capability of LLMs to tackle 3D tasks. Argus involves fusing and integrating multi-view images and camera poses into view-as-scene features, which interact with the 3D features to create comprehensive and detailed 3D-aware scene embeddings. Our approach compensates for the information loss while reconstructing 3D point clouds and helps LLMs better understand the 3D world. Extensive experiments demonstrate that our method outperforms existing 3D-LMMs in various downstream tasks.', 'abstract_zh': 'Advancements in 基础模型使各种下游任务的应用成为可能。特别是，新的时代见证了大型语言模型（LLMs）扩展以解决3D场景理解任务的能力。当前的方法主要依赖于3D点云，但室内场景的3D点云重建往往会丢失信息。一些无纹理的平面或重复的模式容易被遗漏，表现为重建3D点云中的空洞。此外，结构复杂的物体由于捕获的图像与密集重建的点云之间的对齐错误，往往会引入细节失真。2D多视图图像与3D点云视觉上具有一致性，提供了场景组件的更详细表示，可以自然地弥补这些缺陷。基于这些见解，我们提出Argus，一种新颖的3D多模态框架，利用多视图图像增强LLMs的3D场景理解能力。总体来说，Argus可以被视为一个3D大型多模态基础模型（3D-LMM），因为它接受多种模态（文本指令、2D多视图图像和3D点云）作为输入，并扩展了LLMs解决3D任务的能力。Argus涉及将多视图图像和相机姿态融合成视图即场景特征，并与3D特征交互，以创建综合且详细的3D感知场景嵌入。我们的方法在重建3D点云时补偿信息损失，并帮助LLMs更好地理解3D世界。广泛的实验表明，我们的方法在各种下游任务中优于现有的3D-LMM。', 'title_zh': 'Argus: 利用多视角图像提升大型语言模型在三维场景理解中的能力'}
{'arxiv_id': 'arXiv:2507.12547', 'title': 'Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models', 'authors': "Lionel Wong, Katherine M. Collins, Lance Ying, Cedegao E. Zhang, Adrian Weller, Tobias Gersternberg, Timothy O'Donnell, Alexander K. Lew, Jacob D. Andreas, Joshua B. Tenenbaum, Tyler Brooke-Wilson", 'link': 'https://arxiv.org/abs/2507.12547', 'abstract': "When faced with novel situations, people are able to marshal relevant considerations from a wide range of background knowledge and put these to use in inferences and predictions. What permits us to draw in globally relevant information and reason over it coherently? Here, we explore the hypothesis that people use a combination of distributed and symbolic representations to construct bespoke mental models tailored to novel situations. We propose a computational implementation of this idea -- a ``Model Synthesis Architecture'' (MSA) -- using language models to implement global relevance-based retrieval and model synthesis and probabilistic programs to implement bespoke, coherent world models. We evaluate our MSA as a model of human judgments on a novel reasoning dataset. The dataset -- built around a `Model Olympics` domain of sports vignettes -- tests models' capacity for human-like, open-ended reasoning by requiring (i) judgments about novel causal structures described in language; (ii) drawing on large bodies of background knowledge; and (iii) doing both in light of observations that introduce arbitrary novel variables. Our MSA approach captures human judgments better than language model-only baselines, under both direct and chain-of-thought generations from the LM that supports model synthesis. These results suggest that MSAs can be implemented in a way that mirrors people's ability to deliver locally coherent reasoning over globally relevant variables, offering a path to understanding and replicating human reasoning in open-ended domains.", 'abstract_zh': '当面临新颖情境时，人们能够调动广泛背景知识中的相关考虑，并将其用于推理和预测。是什么使得我们能够整合全局相关的信息并一致地进行推理？在这里，我们探索了一个假说，即人们使用分布式表示和符号表示的结合来构建针对新颖情境量身定制的心理模型。我们提出了一种计算实现这一想法的方法——“模型合成架构”（MSA），使用语言模型进行全局相关性检索和模型合成，使用概率程序实现定制的、一致的世界模型。我们以一个基于“模型奥林匹克”领域体育情境的新型推理数据集为基础，评估MSA作为人类判断的模型的有效性。该数据集要求模型不仅判断语言中描述的新颖因果结构，还调动广泛背景知识，并在引入任意新颖变量时结合观察进行推理，测试模型进行类似人类开放性推理的能力。我们的MSA方法在直接生成和链式思考生成语言模型支持的模型合成中均比仅使用语言模型的基线更准确地捕捉了人类判断，这表明MSA可以模仿人们以局部一致的方式对全局变量进行推理的能力，为我们理解并复制人类在开放性领域中的推理提供了一条路径。', 'title_zh': '开放世界认知的即需即用的概率模型合成'}
