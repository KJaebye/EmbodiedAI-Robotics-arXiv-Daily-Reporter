{'arxiv_id': 'arXiv:2507.13340', 'title': 'Latent Policy Steering with Embodiment-Agnostic Pretrained World Models', 'authors': 'Yiqi Wang, Mrinal Verghese, Jeff Schneider', 'link': 'https://arxiv.org/abs/2507.13340', 'abstract': 'Learning visuomotor policies via imitation has proven effective across a wide range of robotic domains. However, the performance of these policies is heavily dependent on the number of training demonstrations, which requires expensive data collection in the real world. In this work, we aim to reduce data collection efforts when learning visuomotor robot policies by leveraging existing or cost-effective data from a wide range of embodiments, such as public robot datasets and the datasets of humans playing with objects (human data from play). Our approach leverages two key insights. First, we use optic flow as an embodiment-agnostic action representation to train a World Model (WM) across multi-embodiment datasets, and finetune it on a small amount of robot data from the target embodiment. Second, we develop a method, Latent Policy Steering (LPS), to improve the output of a behavior-cloned policy by searching in the latent space of the WM for better action sequences. In real world experiments, we observe significant improvements in the performance of policies trained with a small amount of data (over 50% relative improvement with 30 demonstrations and over 20% relative improvement with 50 demonstrations) by combining the policy with a WM pretrained on two thousand episodes sampled from the existing Open X-embodiment dataset across different robots or a cost-effective human dataset from play.', 'abstract_zh': '通过利用现有或成本效益高的数据来减少机器人视觉运动策略学习的数据采集努力：一种基于光学流的世界模型及潜在策略导向的方法', 'title_zh': '基于体型无关预训练世界模型的潜在策略引导'}
{'arxiv_id': 'arXiv:2507.13277', 'title': 'Evaluating Reinforcement Learning Algorithms for Navigation in Simulated Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour', 'authors': 'Emma M. A. Harrison', 'link': 'https://arxiv.org/abs/2507.13277', 'abstract': "Robots are increasingly integrated across industries, particularly in healthcare. However, many valuable applications for quadrupedal robots remain overlooked. This research explores the effectiveness of three reinforcement learning algorithms in training a simulated quadruped robot for autonomous navigation and obstacle avoidance. The goal is to develop a robotic guide dog simulation capable of path following and obstacle avoidance, with long-term potential for real-world assistance to guide dogs and visually impaired individuals. It also seeks to expand research into medical 'pets', including robotic guide and alert dogs.\nA comparative analysis of thirteen related research papers shaped key evaluation criteria, including collision detection, pathfinding algorithms, sensor usage, robot type, and simulation platforms. The study focuses on sensor inputs, collision frequency, reward signals, and learning progression to determine which algorithm best supports robotic navigation in complex environments.\nCustom-made environments were used to ensure fair evaluation of all three algorithms under controlled conditions, allowing consistent data collection. Results show that Proximal Policy Optimization (PPO) outperformed Deep Q-Network (DQN) and Q-learning across all metrics, particularly in average and median steps to goal per episode.\nBy analysing these results, this study contributes to robotic navigation, AI and medical robotics, offering insights into the feasibility of AI-driven quadruped mobility and its role in assistive robotics.", 'abstract_zh': 'quadrupedal机器狗在自主导航和避障应用中的强化学习算法比较研究：以模拟导盲犬为例', 'title_zh': '模拟四足机器人导航中强化学习算法评估：受导盲犬行为启发的比较研究'}
{'arxiv_id': 'arXiv:2507.13225', 'title': 'Signal Temporal Logic Compliant Co-design of Planning and Control', 'authors': 'Manas Sashank Juvvi, Tushar Dilip Kurne, Vaishnavi J, Shishir Kolathaya, Pushpak Jagtap', 'link': 'https://arxiv.org/abs/2507.13225', 'abstract': 'This work presents a novel co-design strategy that integrates trajectory planning and control to handle STL-based tasks in autonomous robots. The method consists of two phases: $(i)$ learning spatio-temporal motion primitives to encapsulate the inherent robot-specific constraints and $(ii)$ constructing an STL-compliant motion plan from these primitives. Initially, we employ reinforcement learning to construct a library of control policies that perform trajectories described by the motion primitives. Then, we map motion primitives to spatio-temporal characteristics. Subsequently, we present a sampling-based STL-compliant motion planning strategy tailored to meet the STL specification. The proposed model-free approach, which generates feasible STL-compliant motion plans across various environments, is validated on differential-drive and quadruped robots across various STL specifications. Demonstration videos are available at this https URL.', 'abstract_zh': '本研究提出了一种新型联合设计策略，将轨迹规划与控制相结合，以处理基于STL的任务自主机器人。该方法包含两个阶段：(i) 学习时空运动基元以封装固有的机器人特定约束；(ii) 从这些基元构建STL兼容的运动计划。首先，我们使用强化学习构建一个由运动基元描述轨迹的控制策略库。然后，我们将运动基元映射到时空特性。随后，我们提出了一种基于采样的STL兼容运动规划策略，以满足STL规范。提出的无模型方法在不同环境下生成可行的STL兼容运动计划，并已在差动驱动和四足机器人上针对不同的STL规范进行了验证。有关示范视频请参见此链接：此 https URL。', 'title_zh': '信号时序逻辑符合性的规划与控制协同设计'}
{'arxiv_id': 'arXiv:2507.13200', 'title': 'Few-shot transfer of tool-use skills using human demonstrations with proximity and tactile sensing', 'authors': 'Marina Y. Aoyama, Sethu Vijayakumar, Tetsuya Narita', 'link': 'https://arxiv.org/abs/2507.13200', 'abstract': 'Tools extend the manipulation abilities of robots, much like they do for humans. Despite human expertise in tool manipulation, teaching robots these skills faces challenges. The complexity arises from the interplay of two simultaneous points of contact: one between the robot and the tool, and another between the tool and the environment. Tactile and proximity sensors play a crucial role in identifying these complex contacts. However, learning tool manipulation using these sensors remains challenging due to limited real-world data and the large sim-to-real gap. To address this, we propose a few-shot tool-use skill transfer framework using multimodal sensing. The framework involves pre-training the base policy to capture contact states common in tool-use skills in simulation and fine-tuning it with human demonstrations collected in the real-world target domain to bridge the domain gap. We validate that this framework enables teaching surface-following tasks using tools with diverse physical and geometric properties with a small number of demonstrations on the Franka Emika robot arm. Our analysis suggests that the robot acquires new tool-use skills by transferring the ability to recognise tool-environment contact relationships from pre-trained to fine-tuned policies. Additionally, combining proximity and tactile sensors enhances the identification of contact states and environmental geometry.', 'abstract_zh': '工具扩展了机器人的操作能力，如同扩展了人类的能力。尽管人类在工具操作方面具有专业知识，但教会机器人这些技能仍然面临挑战。这种复杂性源于两个同时接触点的交互：一个是机器人与工具之间的接触，另一个是工具与环境之间的接触。触觉和接近传感器在识别这些复杂的接触中扮演着关键角色。然而，使用这些传感器学习工具操作仍然具有挑战性，原因在于实际数据有限以及模拟与现实之间的巨大差距。为了解决这一问题，我们提出了一种基于多模态 sensing 的小样本工具使用技能转移框架。该框架通过在模拟中预训练基础策略以捕捉工具使用技能中常见的接触状态，并利用在真实目标域中收集的人类演示进行微调，从而弥合领域差距。我们验证了该框架能够使用少量演示教会 Franka Emika 机器人臂执行具有不同物理和几何特性的表面跟随任务。我们的分析表明，机器人通过将预训练到微调策略中的识别工具-环境接触关系的能力转移到新的任务中来获取新的工具使用技能。此外，结合接近传感器和触觉传感器增强了接触状态和环境几何形状的识别。', 'title_zh': '基于近距离和触觉感知的人类示范 few-shot 工具使用技能迁移'}
{'arxiv_id': 'arXiv:2507.13171', 'title': 'Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback', 'authors': 'Suzie Kim, Hye-Bin Shin, Seong-Whan Lee', 'link': 'https://arxiv.org/abs/2507.13171', 'abstract': 'Conventional reinforcement learning (RL) ap proaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, rein forcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, en abling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.', 'abstract_zh': '基于隐式人类反馈的增强学习框架（利用EEG信号的RLIHF）：用于交互式机器人技术的可扩展且符合人类偏好的强化学习', 'title_zh': '通过隐含人类反馈的强化学习对齐人类和机器人'}
{'arxiv_id': 'arXiv:2507.13097', 'title': 'GraspGen: A Diffusion-based Framework for 6-DOF Grasping with On-Generator Training', 'authors': 'Adithyavairavan Murali, Balakumar Sundaralingam, Yu-Wei Chao, Wentao Yuan, Jun Yamada, Mark Carlson, Fabio Ramos, Stan Birchfield, Dieter Fox, Clemens Eppner', 'link': 'https://arxiv.org/abs/2507.13097', 'abstract': 'Grasping is a fundamental robot skill, yet despite significant research advancements, learning-based 6-DOF grasping approaches are still not turnkey and struggle to generalize across different embodiments and in-the-wild settings. We build upon the recent success on modeling the object-centric grasp generation process as an iterative diffusion process. Our proposed framework, GraspGen, consists of a DiffusionTransformer architecture that enhances grasp generation, paired with an efficient discriminator to score and filter sampled grasps. We introduce a novel and performant on-generator training recipe for the discriminator. To scale GraspGen to both objects and grippers, we release a new simulated dataset consisting of over 53 million grasps. We demonstrate that GraspGen outperforms prior methods in simulations with singulated objects across different grippers, achieves state-of-the-art performance on the FetchBench grasping benchmark, and performs well on a real robot with noisy visual observations.', 'abstract_zh': '基于学习的六自由度抓取方法仍然不够成熟，难以在不同实体和真实环境场景中进行泛化。我们在此基础上，将物体中心的抓取生成过程建模为迭代扩散过程。我们提出的框架GraspGen包括一个增强抓取生成的DiffusionTransformer架构，并配以高效辨别器对采样的抓取进行评分和筛选。我们引入了一种新颖且高效的生成器内训练配方来训练辨别器。为了使GraspGen适用于各种物体和抓取器，我们发布了包含超过5300万个抓取的新型模拟数据集。我们证明，GraspGen在不同抓取器上的独立物体模拟环境中优于先前方法，在FetchBench抓取基准测试中达到最先进的性能，并且在具有嘈杂视觉观测的真实机器人上表现良好。', 'title_zh': 'GraspGen：一种基于扩散的六自由度抓取框架及生成器上训练方法'}
{'arxiv_id': 'arXiv:2507.13088', 'title': 'ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning', 'authors': 'Rahel Rickenbach, Alan A. Lahoud, Erik Schaffernicht, Melanie N. Zeilinger, Johannes A. Stork', 'link': 'https://arxiv.org/abs/2507.13088', 'abstract': 'The computational burden of model predictive control (MPC) limits its application on real-time systems, such as robots, and often requires the use of short prediction horizons. This not only affects the control performance, but also increases the difficulty of designing MPC cost functions that reflect the desired long-term objective. This paper proposes ZipMPC, a method that imitates a long-horizon MPC behaviour by learning a compressed and context-dependent cost function for a short-horizon MPC. It improves performance over alternative methods, such as approximate explicit MPC and automatic cost parameter tuning, in particular in terms of i) optimizing the long term objective; ii) maintaining computational costs comparable to a short-horizon MPC; iii) ensuring constraint satisfaction; and iv) generalizing control behaviour to environments not observed during training. For this purpose, ZipMPC leverages the concept of differentiable MPC with neural networks to propagate gradients of the imitation loss through the MPC optimization. We validate our proposed method in simulation and real-world experiments on autonomous racing. ZipMPC consistently completes laps faster than selected baselines, achieving lap times close to the long-horizon MPC baseline. In challenging scenarios where the short-horizon MPC baseline fails to complete a lap, ZipMPC is able to do so. In particular, these performance gains are also observed on tracks unseen during training.', 'abstract_zh': '基于压缩成本函数的ZipMPC：一种模仿长期优化行为的模型预测控制方法', 'title_zh': 'ZipMPC: 压缩依赖上下文的MPC成本通过模仿学习'}
{'arxiv_id': 'arXiv:2507.13053', 'title': 'Efficient Online Learning and Adaptive Planning for Robotic Information Gathering Based on Streaming Data', 'authors': 'Sanjeev Ramkumar Sudha, Joel Jose, Erlend M. Coates', 'link': 'https://arxiv.org/abs/2507.13053', 'abstract': 'Robotic information gathering (RIG) techniques refer to methods where mobile robots are used to acquire data about the physical environment with a suite of sensors. Informative planning is an important part of RIG where the goal is to find sequences of actions or paths that maximize efficiency or the quality of information collected. Many existing solutions solve this problem by assuming that the environment is known in advance. However, real environments could be unknown or time-varying, and adaptive informative planning remains an active area of research. Adaptive planning and incremental online mapping are required for mapping initially unknown or varying spatial fields. Gaussian process (GP) regression is a widely used technique in RIG for mapping continuous spatial fields. However, it falls short in many applications as its real-time performance does not scale well to large datasets. To address these challenges, this paper proposes an efficient adaptive informative planning approach for mapping continuous scalar fields with GPs with streaming sparse GPs. Simulation experiments are performed with a synthetic dataset and compared against existing benchmarks. Finally, it is also verified with a real-world dataset to further validate the efficacy of the proposed method. Results show that our method achieves similar mapping accuracy to the baselines while reducing computational complexity for longer missions.', 'abstract_zh': '基于流式稀疏高斯过程的连续标量场自适应信息采集规划方法', 'title_zh': '基于流数据的机器人信息收集的高效在线学习与自适应规划'}
{'arxiv_id': 'arXiv:2507.13041', 'title': 'What Can Robots Teach Us About Trust and Reliance? An interdisciplinary dialogue between Social Sciences and Social Robotics', 'authors': 'Julien Wacquez, Elisabetta Zibetti, Joffrey Becker, Lorenzo Aloe, Fabio Amadio, Salvatore Anzalone, Lola Cañamero, Serena Ivaldi', 'link': 'https://arxiv.org/abs/2507.13041', 'abstract': 'As robots find their way into more and more aspects of everyday life, questions around trust are becoming increasingly important. What does it mean to trust a robot? And how should we think about trust in relationships that involve both humans and non-human agents? While the field of Human-Robot Interaction (HRI) has made trust a central topic, the concept is often approached in fragmented ways. At the same time, established work in sociology, where trust has long been a key theme, is rarely brought into conversation with developments in robotics. This article argues that we need a more interdisciplinary approach. By drawing on insights from both social sciences and social robotics, we explore how trust is shaped, tested and made visible. Our goal is to open up a dialogue between disciplines and help build a more grounded and adaptable framework for understanding trust in the evolving world of human-robot interaction.', 'abstract_zh': '随着机器人在日常生活的方方面面找到自己的位置，信任相关的问题变得越来越重要。我们如何定义对机器人的信任？在涉及人类和非人类代理的关系中，我们又应该如何思考信任？尽管人机交互（HRI）领域已经将信任作为核心议题，该概念经常以零散的方式进行探讨。同时，社会学领域中关于信任长期以来的主要主题，与机器人技术的发展鲜有交集。本文主张我们需要采取一种更加跨学科的方法。通过结合社会科学和社会机器人学的洞见，我们探讨信任如何被塑造、测试和展现。我们的目标是促进不同学科之间的对话，并帮助构建一个更加坚实且灵活的框架，以理解不断演变的人机交互中的信任。', 'title_zh': '机器人能教给我们关于信任和依赖的什么？社会科学与社会机器人学的跨学科对话'}
{'arxiv_id': 'arXiv:2507.13019', 'title': 'Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities', 'authors': 'Liuyi Wang, Xinyuan Xia, Hui Zhao, Hanqing Wang, Tai Wang, Yilun Chen, Chengju Liu, Qijun Chen, Jiangmiao Pang', 'link': 'https://arxiv.org/abs/2507.13019', 'abstract': "Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at this https URL.", 'abstract_zh': '近期的视觉-语言导航（VLN）进展充满 promise，但其对机器人移动和控制的理想化假设未能反映物理化的部署挑战。为弥合这一差距，我们提出了 VLN-PE，一个支持类人、四足和轮式机器人的物理现实 VLN 平台。首次系统性地在物理机器人环境中，基于不同的技术管道评估了几种自中心 VLN 方法，包括用于单步离散动作预测的分类模型、用于密集路径点预测的扩散模型，以及一种基于地图的大语言模型（LLM），与路径规划集成。我们的研究结果揭示了由于有限的机器人观测空间、环境光照变化以及碰撞和跌落等物理挑战导致的性能显著下降，这也揭示了复杂环境下腿足机器人的运动限制。VLN-PE 高度扩展，允许无缝集成新的场景，超越 MP3D，从而促进更全面的 VLN 评估。尽管当前模型在物理部署中的泛化能力较弱，但 VLN-PE 为我们提供了一条提高跨体适应性的新途径。我们希望我们的研究成果和工具能够启发社区重新思考 VLN 限制，并推动更 robust 和实用的 VLN 模型的发展。代码可在以下链接获取。', 'title_zh': '重新思考视觉语言导航中的体感差距：物理与视觉差异的综合性研究'}
{'arxiv_id': 'arXiv:2507.12986', 'title': 'Robustness Requirement Coverage using a Situation Coverage Approach for Vision-based AI Systems', 'authors': 'Sepeedeh Shahbeigi, Nawshin Mannan Proma, Victoria Hodge, Richard Hawkins, Boda Li, Valentina Donzella', 'link': 'https://arxiv.org/abs/2507.12986', 'abstract': 'AI-based robots and vehicles are expected to operate safely in complex and dynamic environments, even in the presence of component degradation. In such systems, perception relies on sensors such as cameras to capture environmental data, which is then processed by AI models to support decision-making. However, degradation in sensor performance directly impacts input data quality and can impair AI inference. Specifying safety requirements for all possible sensor degradation scenarios leads to unmanageable complexity and inevitable gaps. In this position paper, we present a novel framework that integrates camera noise factor identification with situation coverage analysis to systematically elicit robustness-related safety requirements for AI-based perception systems. We focus specifically on camera degradation in the automotive domain. Building on an existing framework for identifying degradation modes, we propose involving domain, sensor, and safety experts, and incorporating Operational Design Domain specifications to extend the degradation model by incorporating noise factors relevant to AI performance. Situation coverage analysis is then applied to identify representative operational contexts. This work marks an initial step toward integrating noise factor analysis and situational coverage to support principled formulation and completeness assessment of robustness requirements for camera-based AI perception.', 'abstract_zh': '基于AI的机器人和车辆预计能够在复杂且动态的环境中安全运行，即使在传感器性能下降的情况下也是如此。在这些系统中，感知依赖于如摄像头之类的传感器捕获环境数据，然后通过AI模型处理以支持决策制定。然而，传感器性能下降直接影响输入数据质量，并可能导致AI推理受损。为所有可能的传感器衰退场景明确规定安全需求会产生不可管理的复杂性和不可避免的缺口。在本文中，我们提出了一种新的框架，将摄像头噪声因素识别与情况覆盖分析相结合，系统地引出基于AI感知系统的鲁棒性相关安全需求。我们特别关注汽车领域中的摄像头退化。基于现有的退化模式识别框架，我们提出涉及领域、传感器和安全专家，并结合Operation Design Domain（ODD）规范来扩展退化模型，以纳入对AI性能相关噪声因素。然后应用情况覆盖分析来识别代表性操作上下文。这项工作标志着将噪声因素分析与情况覆盖分析综合起来支持鲁棒性需求的原则性表述与完整性评估的初步尝试。', 'title_zh': '基于情境覆盖的方法满足视觉导向AI系统的鲁棒性需求覆盖'}
{'arxiv_id': 'arXiv:2507.12977', 'title': 'Non-differentiable Reward Optimization for Diffusion-based Autonomous Motion Planning', 'authors': 'Giwon Lee, Daehee Park, Jaewoo Jeong, Kuk-Jin Yoon', 'link': 'https://arxiv.org/abs/2507.12977', 'abstract': 'Safe and effective motion planning is crucial for autonomous robots. Diffusion models excel at capturing complex agent interactions, a fundamental aspect of decision-making in dynamic environments. Recent studies have successfully applied diffusion models to motion planning, demonstrating their competence in handling complex scenarios and accurately predicting multi-modal future trajectories. Despite their effectiveness, diffusion models have limitations in training objectives, as they approximate data distributions rather than explicitly capturing the underlying decision-making dynamics. However, the crux of motion planning lies in non-differentiable downstream objectives, such as safety (collision avoidance) and effectiveness (goal-reaching), which conventional learning algorithms cannot directly optimize. In this paper, we propose a reinforcement learning-based training scheme for diffusion motion planning models, enabling them to effectively learn non-differentiable objectives that explicitly measure safety and effectiveness. Specifically, we introduce a reward-weighted dynamic thresholding algorithm to shape a dense reward signal, facilitating more effective training and outperforming models trained with differentiable objectives. State-of-the-art performance on pedestrian datasets (CrowdNav, ETH-UCY) compared to various baselines demonstrates the versatility of our approach for safe and effective motion planning.', 'abstract_zh': '安全且有效的运动规划对于自主机器人至关重要。扩散模型在捕捉复杂代理交互方面表现出色，这是动态环境决策制定的基本方面。最近的研究成功将扩散模型应用于运动规划，证明了它们在处理复杂场景和准确预测多模态未来轨迹方面的能力。尽管这些模型有效，但在训练目标方面仍有限制，因为它们近似数据分布而非明确捕捉底层的决策动态。然而，运动规划的核心在于非可微的下游目标，如安全性（碰撞避免）和有效性（目标达成），这些目标 conventional 学习算法无法直接优化。在本文中，我们提出了一种基于强化学习的训练方案，使扩散运动规划模型能够有效地学习非可微的目标，这些目标能够明确衡量安全性和有效性。具体地，我们引入了一种奖励加权动态阈值算法来塑造密集的奖励信号，从而提高训练效果，并优于使用可微目标训练的模型。与各种基线在行人数据集（CrowdNav, ETH-UCY）上的先进性能表明了我们方法在安全且有效运动规划中的适用性。', 'title_zh': '基于扩散的自主运动规划中的非可微奖励优化'}
{'arxiv_id': 'arXiv:2507.12920', 'title': 'MoCap2GT: A High-Precision Ground Truth Estimator for SLAM Benchmarking Based on Motion Capture and IMU Fusion', 'authors': 'Zichao Shu, Shitao Bei, Jicheng Dai, Lijun Li, Zetao Chen', 'link': 'https://arxiv.org/abs/2507.12920', 'abstract': 'Marker-based optical motion capture (MoCap) systems are widely used to provide ground truth (GT) trajectories for benchmarking SLAM algorithms. However, the accuracy of MoCap-based GT trajectories is mainly affected by two factors: spatiotemporal calibration errors between the MoCap system and the device under test (DUT), and inherent MoCap jitter. Consequently, existing benchmarks focus primarily on absolute translation error, as accurate assessment of rotation and inter-frame errors remains challenging, hindering thorough SLAM evaluation. This paper proposes MoCap2GT, a joint optimization approach that integrates MoCap data and inertial measurement unit (IMU) measurements from the DUT for generating high-precision GT trajectories. MoCap2GT includes a robust state initializer to ensure global convergence, introduces a higher-order B-spline pose parameterization on the SE(3) manifold with variable time offset to effectively model MoCap factors, and employs a degeneracy-aware measurement rejection strategy to enhance estimation accuracy. Experimental results demonstrate that MoCap2GT outperforms existing methods and significantly contributes to precise SLAM benchmarking. The source code is available at this https URL (temporarily hosted anonymously for double-blind review).', 'abstract_zh': '基于标记的光学动作捕捉(MoCap)系统广泛用于提供地面 truth (GT) 轨迹以评估SLAM算法。然而，MoCap-based GT 轨迹的准确性主要受两方面因素影响：MoCap系统与待测设备(DUT)之间的时空校准误差以及动作捕捉固有的抖动。因此，现有基准主要关注绝对平移误差，而准确评估旋转误差和帧间误差仍然具有挑战性，阻碍了SLAM算法的全面评估。本文提出MoCap2GT，这是一种结合MoCap数据和DUT的惯性测量单元(IMU)测量的联合优化方法，用于生成高精度GT轨迹。MoCap2GT包括一个稳健的状态初始化器以确保全局收敛，引入了SE(3)流形上的具有可变时间偏移的高阶B样条姿态参数化以有效建模MoCap因子，并采用了退化感知测量拒绝策略以提高估计精度。实验结果表明，MoCap2GT优于现有方法，并显著促进了精确的SLAM基准测试。源代码可在此链接访问（暂时匿名托管以供双盲评审）。', 'title_zh': 'MoCap2GT：基于运动捕捉和IMU融合的高精度地面truth估计器用于SLAM基准测试'}
{'arxiv_id': 'arXiv:2507.12911', 'title': 'LaViPlan : Language-Guided Visual Path Planning with RLVR', 'authors': 'Hayeon Oh', 'link': 'https://arxiv.org/abs/2507.12911', 'abstract': 'Out-of-distribution (OOD) scenarios in autonomous driving refer to situations that deviate from the training domain, often leading to unexpected and potentially hazardous behavior from planners that lack prior exposure to such cases. Recently, Vision-Language Models (VLMs) have been introduced into autonomous driving research for their promising generalization capabilities in OOD settings. Early studies demonstrated that VLMs could recognize OOD scenarios and generate user-level decisions such as "go straight" or "turn right." However, a new challenge has emerged due to the misalignment between the VLM\'s high-level decisions or visual reasoning expressed in language, and the low-level predicted trajectories interpreted as actions. In this paper, we propose LaViPlan, a framework that leverages Reinforcement Learning with Verifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics. This approach addresses the vision-language-action misalignment observed in existing VLMs fine-tuned via supervised learning, which can recognize driving scenarios but often produce context-unaware decisions. Experimental results demonstrate that our method improves situational awareness and decision-making under OOD conditions, highlighting its potential to mitigate the misalignment issue. This work introduces a promising post-training paradigm for VLM agents in the context of autonomous driving.', 'abstract_zh': '自主驾驶中离分布(out-of-distribution, OOD)场景指那些偏离训练域的情况，往往会导致规划器产生未预期且可能具有安全隐患的行为。近年来，视觉-语言模型(Vision-Language Models, VLMs)被引入到自主驾驶研究中，因其在OOD环境下的泛化能力表现突出。早期研究表明，VLMs能够识别OOD场景并生成诸如“直行”或“右转”的用户级决策。然而，由于VLM在高层决策或语言表达的视觉推理与低层预测轨迹作为行动解释之间存在不匹配，一个新的挑战由此出现。本文提出了一种命名为LaViPlan的框架，该框架利用可验证奖励的强化学习(Reinforcement Learning with Verifiable Rewards, RLVR)来使用面向规划的指标优化VLMs。该方法解决了现有通过监督学习微调的VLMs中存在的视觉-语言-行动不匹配问题，这些问题能够识别驾驶场景，但经常产生缺乏情境意识的决策。实验结果表明，我们的方法在OOD条件下提高了情景意识和决策能力，凸显了其缓解不匹配问题的潜力。本文为视觉-语言-行动代理在自主驾驶领域的后训练范式引入了一个有前景的框架。', 'title_zh': 'LaViPlan：基于语言导向的视觉路径规划与RLVR'}
{'arxiv_id': 'arXiv:2507.12855', 'title': 'DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task Demonstration Learning', 'authors': 'Rahel Rickenbach, Bruce Lee, René Zurbrügg, Carmen Amo Alonso, Melanie N. Zeilinger', 'link': 'https://arxiv.org/abs/2507.12855', 'abstract': 'The integration of large language models (LLMs) with control systems has demonstrated significant potential in various settings, such as task completion with a robotic manipulator. A main reason for this success is the ability of LLMs to perform in-context learning, which, however, strongly relies on the design of task examples, closely related to the target tasks. Consequently, employing LLMs to formulate optimal control problems often requires task examples that contain explicit mathematical expressions, designed by trained engineers. Furthermore, there is often no principled way to evaluate for hallucination before task execution. To address these challenges, we propose DEMONSTRATE, a novel methodology that avoids the use of LLMs for complex optimization problem generations, and instead only relies on the embedding representations of task descriptions. To do this, we leverage tools from inverse optimal control to replace in-context prompt examples with task demonstrations, as well as the concept of multitask learning, which ensures target and example task similarity by construction. Given the fact that hardware demonstrations can easily be collected using teleoperation or guidance of the robot, our approach significantly reduces the reliance on engineering expertise for designing in-context examples. Furthermore, the enforced multitask structure enables learning from few demonstrations and assessment of hallucinations prior to task execution. We demonstrate the effectiveness of our method through simulation and hardware experiments involving a robotic arm tasked with tabletop manipulation.', 'abstract_zh': '使用任务示范的LLM与控制系统的集成方法：DEMONSTRATE', 'title_zh': 'DEMONSTRATE：通过多任务示范学习实现零样本语言到机器人控制的转换'}
{'arxiv_id': 'arXiv:2507.12846', 'title': 'Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering', 'authors': 'Muhammad Fadhil Ginting, Dong-Ki Kim, Xiangyun Meng, Andrzej Reinke, Bandi Jai Krishna, Navid Kayhani, Oriana Peltzer, David D. Fan, Amirreza Shaban, Sung-Kyun Kim, Mykel J. Kochenderfer, Ali-akbar Agha-mohammadi, Shayegan Omidshafiei', 'link': 'https://arxiv.org/abs/2507.12846', 'abstract': 'As robots become increasingly capable of operating over extended periods -- spanning days, weeks, and even months -- they are expected to accumulate knowledge of their environments and leverage this experience to assist humans more effectively. This paper studies the problem of Long-term Active Embodied Question Answering (LA-EQA), a new task in which a robot must both recall past experiences and actively explore its environment to answer complex, temporally-grounded questions. Unlike traditional EQA settings, which typically focus either on understanding the present environment alone or on recalling a single past observation, LA-EQA challenges an agent to reason over past, present, and possible future states, deciding when to explore, when to consult its memory, and when to stop gathering observations and provide a final answer. Standard EQA approaches based on large models struggle in this setting due to limited context windows, absence of persistent memory, and an inability to combine memory recall with active exploration. To address this, we propose a structured memory system for robots, inspired by the mind palace method from cognitive science. Our method encodes episodic experiences as scene-graph-based world instances, forming a reasoning and planning algorithm that enables targeted memory retrieval and guided navigation. To balance the exploration-recall trade-off, we introduce value-of-information-based stopping criteria that determines when the agent has gathered sufficient information. We evaluate our method on real-world experiments and introduce a new benchmark that spans popular simulation environments and actual industrial sites. Our approach significantly outperforms state-of-the-art baselines, yielding substantial gains in both answer accuracy and exploration efficiency.', 'abstract_zh': '长期主动体态问答：一种新的机器人记忆与探索任务', 'title_zh': '进入思维宫殿：长期主动体态问答中的推理与规划'}
{'arxiv_id': 'arXiv:2507.12800', 'title': 'FFI-VTR: Lightweight and Robust Visual Teach and Repeat Navigation based on Feature Flow Indicator and Probabilistic Motion Planning', 'authors': 'Jikai Wang, Yunqi Cheng, Zonghai Chen', 'link': 'https://arxiv.org/abs/2507.12800', 'abstract': "Though visual and repeat navigation is a convenient solution for mobile robot self-navigation, achieving balance between efficiency and robustness in task environment still remains challenges. In this paper, we propose a novel visual and repeat robotic autonomous navigation method that requires no accurate localization and dense reconstruction modules, which makes our system featured by lightweight and robustness. Firstly, feature flow is introduced and we develop a qualitative mapping between feature flow and robot's motion, in which feature flow is defined as pixel location bias between matched features. Based on the mapping model, the map outputted by the teaching phase is represented as a keyframe graph, in which the feature flow on the edge encodes the relative motion between adjacent keyframes. Secondly, the visual repeating navigation is essentially modeled as a feature flow minimization problem between current observation and the map keyframe. To drive the robot to consistently reduce the feature flow between current frame and map keyframes without accurate localization, a probabilistic motion planning is developed based on our qualitative feature flow-motion mapping indicator. Extensive experiments using our mobile platform demonstrates that our proposed method is lightweight, robust, and superior to baselines. The source code has been made public at this https URL to benefit the community.", 'abstract_zh': '尽管视觉和重复导航是移动机器人自主导航的一种方便解决方案，但在任务环境中实现高效性和鲁棒性的平衡仍然存在挑战。本文提出了一种无需精确定位和密集重建模块的新型视觉和重复机器人自主导航方法，使得系统兼具轻量化和鲁棒性。首先，引入了特征流，并开发了特征流与机器人运动之间的定性映射，其中特征流定义为匹配特征之间的像素位置偏差。基于该映射模型，教学阶段输出的地图表示为关键帧图，特征流在边上的编码相邻关键帧之间的相对运动。其次，视觉重复导航本质上被建模为特征流在当前观测与地图关键帧之间的最小化问题。为了在无需精确定位的情况下驱动机器人一致地减少当前帧与地图关键帧之间的特征流，我们基于定性的特征流-运动映射指标开发了概率性运动规划。我们的移动平台上的广泛实验表明，所提出的方法具有轻量化、鲁棒性，并优于基线方法。源代码已在此httpsURL公开，以惠及社区。', 'title_zh': 'FFI-VTR：基于特征流指示器和概率运动规划的轻量级稳健视觉教vertising导航'}
{'arxiv_id': 'arXiv:2507.12753', 'title': 'osmAG-LLM: Zero-Shot Open-Vocabulary Object Navigation via Semantic Maps and Large Language Models Reasoning', 'authors': 'Fujing Xie, Sören Schwertfeger, Hermann Blum', 'link': 'https://arxiv.org/abs/2507.12753', 'abstract': 'Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features, achieving a high level of detail and guiding robots to find objects specified by open-vocabulary language queries. While the issue of scalability for such approaches has received some attention, another fundamental problem is that high-detail object mapping quickly becomes outdated, as objects get moved around a lot. In this work, we develop a mapping and navigation system for object-goal navigation that, from the ground up, considers the possibilities that a queried object can have moved, or may not be mapped at all. Instead of striving for high-fidelity mapping detail, we consider that the main purpose of a map is to provide environment grounding and context, which we combine with the semantic priors of LLMs to reason about object locations and deploy an active, online approach to navigate to the objects. Through simulated and real-world experiments we find that our approach tends to have higher retrieval success at shorter path lengths for static objects and by far outperforms prior approaches in cases of dynamic or unmapped object queries. We provide our code and dataset at: this https URL.', 'abstract_zh': '基于开放词汇的机器人.mapping方法通过预训练的视觉-语言特征丰富密集几何地图，实现高细节水平并指导机器人找到由开放词汇语言查询指定的对象。尽管此类方法的可扩展性问题已受到一定关注，但另一个基本问题是高细节度的目标建图会因对象频繁移动而迅速过时。在本工作中，我们开发了一种面向对象目标导航的建图与导航系统，从头开始就考虑查询对象可能已移动或根本未被建图的可能性。我们不追求高保真度的建图细节，而是认为地图的主要目的是提供环境基础和背景，并结合LLM的语义先验来推理对象位置，采用主动的、在线的方法导航至对象。通过模拟和真实世界的实验，我们发现我们的方法在静态对象较短路径长度的检索成功率上更高，并在动态或未建图对象查询的情况下显著优于先前方法。我们提供了代码和数据集：this https URL。', 'title_zh': 'osmAG-LLM: 基于语义地图和大规模语言模型推理的零样本开放词汇对象导航'}
{'arxiv_id': 'arXiv:2507.12751', 'title': 'Refining Motion for Peak Performance: Identifying Optimal Gait Parameters for Energy-Efficient Quadrupedal Bounding', 'authors': 'Yasser G. Alqaham, Jing Cheng, Zhenyu Gan', 'link': 'https://arxiv.org/abs/2507.12751', 'abstract': 'Energy efficiency is a critical factor in the performance and autonomy of quadrupedal robots. While previous research has focused on mechanical design and actuation improvements, the impact of gait parameters on energetics has been less explored. In this paper, we hypothesize that gait parameters, specifically duty factor, phase shift, and stride duration, are key determinants of energy consumption in quadrupedal locomotion. To test this hypothesis, we modeled the Unitree A1 quadrupedal robot and developed a locomotion controller capable of independently adjusting these gait parameters. Simulations of bounding gaits were conducted in Gazebo across a range of gait parameters at three different speeds: low, medium, and high. Experimental tests were also performed to validate the simulation results. The findings demonstrate that optimizing gait parameters can lead to significant reductions in energy consumption, enhancing the overall efficiency of quadrupedal locomotion. This work contributes to the advancement of energy-efficient control strategies for legged robots, offering insights directly applicable to commercially available platforms.', 'abstract_zh': '四足机器人性能和自主性的关键因素是能效。虽然以往的研究主要关注于机械设计和驱动装置的改进，但步态参数对能耗的影响尚缺乏探索。在本文中，我们假设步态参数，特别是步距因子、相位偏移和步长，是四足行走过程中能耗的关键决定因素。为了验证这一假设，我们使用Unitree A1四足机器人建立了模型，并开发了一个能够独立调整这些步态参数的运动控制器。在Gazebo中，我们在低、中、高三种速度下，对跳跃步态进行了步态参数范围内的仿真实验。实际实验也被用于验证仿真结果。研究发现，优化步态参数可以显著降低能耗，提高四足行走的整体效率。这项工作为腿部机器人高效控制策略的发展做出了贡献，提供了直接适用于商用平台的见解。', 'title_zh': '提高运动效率以达到峰值性能：识别能量高效的四足 bounding 最优步态参数'}
{'arxiv_id': 'arXiv:2507.12744', 'title': 'ASC-SW: Atrous strip convolution network with sliding windows for visual-assisted map navigation', 'authors': 'Cheng Liu, Fan Zhu, Yaoyu Zhuang Zhinan Chen Jiefeng Tang', 'link': 'https://arxiv.org/abs/2507.12744', 'abstract': 'With the rapid development of lightweight visual neural network architectures, traditional high-performance vision models have undergone significant compression, greatly improving their computational efficiency and energy consumption ratio. This makes them feasible for deployment on resource-constrained edge computing devices. We propose a visual-assisted navigation framework called Atrous Strip Convolution-Sliding Window (ASC-SW), which leverages a depth camera and a lightweight visual neural network to assist map-based mobile robot navigation. This framework compensates for the inability of traditional light detection and range (LiDAR) sensors to detect ground-level obstacles such as ground-level wires. We introduce a lightweight and efficient segmentation model, Atrous Strip Convolution Network (ASCnet), for detecting deformable linear objects (DLOs). MobileNetV2 is used as the backbone network, and Atrous Strip Convolution Spatial Pyramid Pooling (ASCSPP) is designed to extract DLO features more effectively. Atrous Strip Convolution is integrated into ASCSPP to accurately identify the linear structure of DLOs with low computational cost. Additionally, a Sliding Window (SW) post-processing module is proposed to denoise the output in complex environments, improving recognition accuracy. Our method strikes a balance between inference speed and segmentation performance. It achieves a mean Intersection over Union (Miou) score of 75.3% on a self-built dataset and reaches 9.3 FPS inference speed on the Jetson Orin Nano edge device. Overall, our approach outperforms existing DLO detection models and has been successfully validated on a physical robotic platform.', 'abstract_zh': '基于视觉辅助的空洞条状卷积滑动窗口导航框架：检测可变形线性对象', 'title_zh': 'ASC-SW：滑动窗口下的稀疏 atrous 卷积网络用于视觉辅助地图导航'}
{'arxiv_id': 'arXiv:2507.12731', 'title': 'Learning to Predict Mobile Robot Stability in Off-Road Environments', 'authors': 'Nathaniel Rose, Arif Ahmed, Emanuel Gutierrez-Cornejo, Parikshit Maini', 'link': 'https://arxiv.org/abs/2507.12731', 'abstract': "Navigating in off-road environments for wheeled mobile robots is challenging due to dynamic and rugged terrain. Traditional physics-based stability metrics, such as Static Stability Margin (SSM) or Zero Moment Point (ZMP) require knowledge of contact forces, terrain geometry, and the robot's precise center-of-mass that are difficult to measure accurately in real-world field conditions. In this work, we propose a learning-based approach to estimate robot platform stability directly from proprioceptive data using a lightweight neural network, IMUnet. Our method enables data-driven inference of robot stability without requiring an explicit terrain model or force sensing.\nWe also develop a novel vision-based ArUco tracking method to compute a scalar score to quantify robot platform stability called C3 score. The score captures image-space perturbations over time as a proxy for physical instability and is used as a training signal for the neural network based model. As a pilot study, we evaluate our approach on data collected across multiple terrain types and speeds and demonstrate generalization to previously unseen conditions. These initial results highlight the potential of using IMU and robot velocity as inputs to estimate platform stability. The proposed method finds application in gating robot tasks such as precision actuation and sensing, especially for mobile manipulation tasks in agricultural and space applications. Our learning method also provides a supervision mechanism for perception based traversability estimation and planning.", 'abstract_zh': '基于惯性测量单元和机器人速度的离-road环境轮式移动机器人平台稳定性估计方法', 'title_zh': '学习在非路面环境中预测移动机器人稳定性'}
{'arxiv_id': 'arXiv:2507.12716', 'title': 'MoistureMapper: An Autonomous Mobile Robot for High-Resolution Soil Moisture Mapping at Scale', 'authors': 'Nathaniel Rose, Hannah Chuang, Manuel A Andrade-Rodriguez, Rishi Parashar, Dani Or, Parikshit Maini', 'link': 'https://arxiv.org/abs/2507.12716', 'abstract': 'Soil moisture is a quantity of interest in many application areas including agriculture and climate modeling. Existing methods are not suitable for scale applications due to large deployment costs in high-resolution sensing applications such as for variable irrigation. In this work, we design, build and field deploy an autonomous mobile robot, MoistureMapper, for soil moisture sensing. The robot is equipped with Time Domain Reflectometry (TDR) sensors and a direct push drill mechanism for deploying the sensor to measure volumetric water content in the soil. Additionally, we implement and evaluate multiple adaptive sampling strategies based on a Gaussian Process based modeling to build a spatial mapping of moisture distribution in the soil. We present results from large scale computational simulations and proof-of-concept deployment on the field. The adaptive sampling approach outperforms a greedy benchmark approach and results in up to 30\\% reduction in travel distance and 5\\% reduction in variance in the reconstructed moisture maps. Link to video showing field experiments: this https URL', 'abstract_zh': '土壤湿度是农业和气候 modeling 等多个应用领域的关注量。现有方法由于在高分辨率感知识别变量灌溉等应用场景中部署成本较大而不适于大规模应用。本文中，我们设计、构建并现场部署了一台自主移动机器人 MoistureMapper，用于土壤湿度感应。该机器人配备有时域反射仪 (TDR) 传感器和直接压入钻机机制以将传感器部署到土壤中测量土壤体积含水量。此外，我们基于高斯过程模型实施并评估了多种自适应采样策略，以构建土壤内湿度分布的空间图谱。我们展示了大规模计算仿真结果和现场概念验证部署的结果。自适应采样方法优于贪婪基准方法，可减少高达 30% 的行驶距离并降低约 5% 的重建湿度图方差。视频展示现场试验：this https URL。', 'title_zh': 'MoistureMapper：一种用于高分辨率土壤水分大规模测绘的自主移动机器人'}
{'arxiv_id': 'arXiv:2507.12644', 'title': 'VLMgineer: Vision Language Models as Robotic Toolsmiths', 'authors': 'George Jiayuan Gao, Tianyu Li, Junyao Shi, Yihan Li, Zizhe Zhang, Nadia Figueroa, Dinesh Jayaraman', 'link': 'https://arxiv.org/abs/2507.12644', 'abstract': "Tool design and use reflect the ability to understand and manipulate the physical world through creativity, planning, and foresight. As such, these capabilities are often regarded as measurable indicators of intelligence across biological species. While much of today's research on robotic intelligence focuses on generating better controllers, inventing smarter tools offers a complementary form of physical intelligence: shifting the onus of problem-solving onto the tool's design. Given the vast and impressive common-sense, reasoning, and creative capabilities of today's foundation models, we investigate whether these models can provide useful priors to automatically design and effectively wield such tools? We present VLMgineer, a framework that harnesses the code generation abilities of vision language models (VLMs) together with evolutionary search to iteratively co-design physical tools and the action plans that operate them to perform a task. We evaluate VLMgineer on a diverse new benchmark of everyday manipulation scenarios that demand creative tool design and use. Across this suite, VLMgineer consistently discovers tools and policies that solve tasks more effectively and innovatively, transforming challenging robotics problems into straightforward executions. It also outperforms VLM-generated designs from human specifications and existing human-crafted tools for everyday tasks. To facilitate future research on automated tool invention, we will release our benchmark and code.", 'abstract_zh': '工具的设计与使用反映了通过创造力、规划和预见性理解并操控物理世界的能力，这些能力常被视为衡量智能的指标。因此，发明更智能的工具为物理智能提供了一种互补形式：将解决问题的负担转移到工具设计上。鉴于今天基础模型的广泛且令人印象深刻的常识、推理和创造能力，我们探讨这些模型是否能够提供有用的先验知识，自动设计并有效地使用这类工具？我们提出了VLMgineer框架，该框架结合了视觉语言模型（VLMs）的代码生成能力和进化搜索，以迭代的方式共同设计物理工具及其操作计划，以完成任务。我们利用一个包含多种日常生活操作场景的新基准评估VLMgineer，这些场景要求创造性地设计和使用工具。在这一系列测试中，VLMgineer一致地发现能更有效地和创新性地解决任务的工具和策略，将复杂的机器人问题转化为简单的执行。它还优于基于人类规范生成的VLM设计和现有的手工设计工具。为了促进自动化工具发明的未来研究，我们将发布我们的基准和代码。', 'title_zh': 'VLMgineer: 视觉语言模型作为机器人工具师'}
{'arxiv_id': 'arXiv:2507.12499', 'title': 'ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving', 'authors': 'Yuhang Lu, Jiadong Tu, Yuexin Ma, Xinge Zhu', 'link': 'https://arxiv.org/abs/2507.12499', 'abstract': 'End-to-end autonomous driving has emerged as a promising approach to unify perception, prediction, and planning within a single framework, reducing information loss and improving adaptability. However, existing methods often rely on fixed and sparse trajectory supervision, limiting their ability to capture the hierarchical reasoning process that human drivers naturally employ. To bridge this gap, we propose ReAL-AD, a Reasoning-Augmented Learning framework that structures decision-making in autonomous driving based on the three-tier human cognitive model: Driving Strategy, Driving Decision, and Driving Operation, where Vision-Language Models (VLMs) are incorporated to enhance situational awareness and structured reasoning across these levels. Specifically, we introduce: (1) the Strategic Reasoning Injector, which formulates high-level driving strategies by interpreting complex traffic contexts from VLM-generated insights; (2) the Tactical Reasoning Integrator, which refines strategic intent into interpretable tactical choices such as lane changes, overtaking, and speed adjustments; and (3) the Hierarchical Trajectory Decoder, which progressively translates tactical decisions into precise control actions for smooth and human-like trajectory execution. Extensive evaluations show that integrating our framework improves planning accuracy and safety by over 30%, making end-to-end autonomous driving more interpretable and aligned with human-like hierarchical reasoning. The project page can be found at: \\href{this https URL}{\\texttt{this http URL\\_page/realad}}', 'abstract_zh': '端到端自主驾驶：一种增强学习框架，基于人类认知三层次模型实现决策优化，提高规划准确性和安全性', 'title_zh': 'ReAL-AD: 朝向人类级推理的端到端自主驾驶'}
{'arxiv_id': 'arXiv:2507.12496', 'title': 'FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making', 'authors': 'Yucen Wang, Rui Yu, Shenghua Wan, Le Gan, De-Chuan Zhan', 'link': 'https://arxiv.org/abs/2507.12496', 'abstract': "Foundation Models (FMs) and World Models (WMs) offer complementary strengths in task generalization at different levels. In this work, we propose FOUNDER, a framework that integrates the generalizable knowledge embedded in FMs with the dynamic modeling capabilities of WMs to enable open-ended task solving in embodied environments in a reward-free manner. We learn a mapping function that grounds FM representations in the WM state space, effectively inferring the agent's physical states in the world simulator from external observations. This mapping enables the learning of a goal-conditioned policy through imagination during behavior learning, with the mapped task serving as the goal state. Our method leverages the predicted temporal distance to the goal state as an informative reward signal. FOUNDER demonstrates superior performance on various multi-task offline visual control benchmarks, excelling in capturing the deep-level semantics of tasks specified by text or videos, particularly in scenarios involving complex observations or domain gaps where prior methods struggle. The consistency of our learned reward function with the ground-truth reward is also empirically validated. Our project website is this https URL.", 'abstract_zh': 'Foundation Models (FMs)和World Models (WMs)在不同层次上为任务泛化提供了互补的优势。本文提出了一种称为FOUNDER的框架，该框架将FMs中嵌入的可泛化知识与WMs的动态建模能力结合起来，以在奖励免费的方式下解决开放性环境中的任务。我们学习了一个映射函数，将FMs的表示嵌入到WM的状态空间中，从而有效地从外部观察中推断出代理在世界模拟器中的物理状态。这种映射使代理在行为学习过程中通过想象来学习一个基于目标的策略，映射任务作为目标状态。我们的方法利用预测的目标状态的时间距离作为有信息性的奖励信号。FOUNDER在各种多任务离线视觉控制基准测试中表现优异，特别擅长捕捉由文本或视频指定的任务的深层次语义，特别是在涉及复杂观察或领域差距的场景中，前期方法难以应对。我们还实验验证了所学奖励函数与真实奖励函数的一致性。我们的项目网站是这个https URL。', 'title_zh': 'FOUNDER：将基础模型-grounding-于世界模型以实现开放性体态决策'}
{'arxiv_id': 'arXiv:2507.12489', 'title': 'Physically Based Neural LiDAR Resimulation', 'authors': 'Richard Marcus, Marc Stamminger', 'link': 'https://arxiv.org/abs/2507.12489', 'abstract': 'Methods for Novel View Synthesis (NVS) have recently found traction in the field of LiDAR simulation and large-scale 3D scene reconstruction. While solutions for faster rendering or handling dynamic scenes have been proposed, LiDAR specific effects remain insufficiently addressed. By explicitly modeling sensor characteristics such as rolling shutter, laser power variations, and intensity falloff, our method achieves more accurate LiDAR simulation compared to existing techniques. We demonstrate the effectiveness of our approach through quantitative and qualitative comparisons with state-of-the-art methods, as well as ablation studies that highlight the importance of each sensor model component. Beyond that, we show that our approach exhibits advanced resimulation capabilities, such as generating high resolution LiDAR scans in the camera perspective.\nOur code and the resulting dataset are available at this https URL.', 'abstract_zh': 'LiDAR特定效果的新型视图synthesize方法在LiDAR模拟和大规模3D场景重建中的应用', 'title_zh': '基于物理的神经LiDAR仿真'}
{'arxiv_id': 'arXiv:2507.13231', 'title': 'VITA: Vision-to-Action Flow Matching Policy', 'authors': 'Dechen Gao, Boqi Zhao, Andrew Lee, Ian Chuang, Hanchu Zhou, Hang Wang, Zhe Zhao, Junshan Zhang, Iman Soltani', 'link': 'https://arxiv.org/abs/2507.13231', 'abstract': 'We present VITA, a Vision-To-Action flow matching policy that evolves latent visual representations into latent actions for visuomotor control. Traditional flow matching and diffusion policies sample from standard source distributions (e.g., Gaussian noise) and require additional conditioning mechanisms like cross-attention to condition action generation on visual information, creating time and space overheads. VITA proposes a novel paradigm that treats latent images as the flow source, learning an inherent mapping from vision to action while eliminating separate conditioning modules and preserving generative modeling capabilities. Learning flows between fundamentally different modalities like vision and action is challenging due to sparse action data lacking semantic structures and dimensional mismatches between high-dimensional visual representations and raw actions. We address this by creating a structured action latent space via an autoencoder as the flow matching target, up-sampling raw actions to match visual representation shapes. Crucially, we supervise flow matching with both encoder targets and final action outputs through flow latent decoding, which backpropagates action reconstruction loss through sequential flow matching ODE solving steps for effective end-to-end learning. Implemented as simple MLP layers, VITA is evaluated on challenging bi-manual manipulation tasks on the ALOHA platform, including 5 simulation and 2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or matches state-of-the-art generative policies while reducing inference latency by 50-130% compared to conventional flow matching policies requiring different conditioning mechanisms or complex architectures. To our knowledge, VITA is the first MLP-only flow matching policy capable of solving complex bi-manual manipulation tasks like those in ALOHA benchmarks.', 'abstract_zh': 'Vision-To-Action 流匹配策略：一种将潜在视觉表示演化为潜在动作以实现视听运动控制的方法', 'title_zh': 'VITA: 视觉到动作的流匹配策略'}
{'arxiv_id': 'arXiv:2507.13229', 'title': '$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation', 'authors': 'Junhong Min, Youngpil Jeon, Jimin Kim, Minyong Choi', 'link': 'https://arxiv.org/abs/2507.13229', 'abstract': 'The pursuit of a generalizable stereo matching model, capable of performing across varying resolutions and disparity ranges without dataset-specific fine-tuning, has revealed a fundamental trade-off. Iterative local search methods achieve high scores on constrained benchmarks, but their core mechanism inherently limits the global consistency required for true generalization. On the other hand, global matching architectures, while theoretically more robust, have been historically rendered infeasible by prohibitive computational and memory costs. We resolve this dilemma with $S^2M^2$: a global matching architecture that achieves both state-of-the-art accuracy and high efficiency without relying on cost volume filtering or deep refinement stacks. Our design integrates a multi-resolution transformer for robust long-range correspondence, trained with a novel loss function that concentrates probability on feasible matches. This approach enables a more robust joint estimation of disparity, occlusion, and confidence. $S^2M^2$ establishes a new state of the art on the Middlebury v3 and ETH3D benchmarks, significantly outperforming prior methods across most metrics while reconstructing high-quality details with competitive efficiency.', 'abstract_zh': '一种能够在不同分辨率和视差范围内泛化的立体匹配模型的探索揭示了基本的trade-off。迭代局部搜索方法在受约束的基准测试中能够获得高分，但其核心机制内在地限制了真正泛化所需的全局一致性。另一方面，全局匹配架构虽然理论上更具鲁棒性，但由于计算和内存成本的限制，历史上一直难以实现。我们通过S²M²解决了这一困境：这是一种实现最先进的准确性和高效率的全局匹配架构，无需依赖代价体筛选或深层细化堆栈。我们的设计集成了多分辨率变换器，用于稳健的大范围对应关系，并通过一种新型损失函数进行训练，该函数将概率集中在可行的匹配上。这种方法能够更稳健地联合估计视差、遮挡和置信度。S²M²在Middlebury v3和ETH3D基准测试中建立了新的最先进的水平，大多数指标上显著优于先前的方法，同时以竞争性的效率重建高质量的细节。', 'title_zh': '$S^2M^2$: 可扩展的立体匹配模型以实现可靠的深度估计'}
{'arxiv_id': 'arXiv:2507.13152', 'title': 'SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models', 'authors': 'Xiangyu Dong, Haoran Zhao, Jiang Gao, Haozhou Li, Xiaoguang Ma, Yaoming Zhou, Fuhai Chen, Juan Liu', 'link': 'https://arxiv.org/abs/2507.13152', 'abstract': 'Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.', 'abstract_zh': 'Recent Advances in Vision-Language Navigation (VLN): From Large Language Models to a Self-Evolving VLN Framework (SE-VLN)', 'title_zh': 'SE-VLN：基于多模态大型语言模型的自我演化视觉-语言导航框架'}
{'arxiv_id': 'arXiv:2507.13145', 'title': 'DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model', 'authors': 'Maulana Bisyir Azhari, David Hyunchul Shim', 'link': 'https://arxiv.org/abs/2507.13145', 'abstract': "Learning-based monocular visual odometry (VO) poses robustness, generalization, and efficiency challenges in robotics. Recent advances in visual foundation models, such as DINOv2, have improved robustness and generalization in various vision tasks, yet their integration in VO remains limited due to coarse feature granularity. In this paper, we present DINO-VO, a feature-based VO system leveraging DINOv2 visual foundation model for its sparse feature matching. To address the integration challenge, we propose a salient keypoints detector tailored to DINOv2's coarse features. Furthermore, we complement DINOv2's robust-semantic features with fine-grained geometric features, resulting in more localizable representations. Finally, a transformer-based matcher and differentiable pose estimation layer enable precise camera motion estimation by learning good matches. Against prior detector-descriptor networks like SuperPoint, DINO-VO demonstrates greater robustness in challenging environments. Furthermore, we show superior accuracy and generalization of the proposed feature descriptors against standalone DINOv2 coarse features. DINO-VO outperforms prior frame-to-frame VO methods on the TartanAir and KITTI datasets and is competitive on EuRoC dataset, while running efficiently at 72 FPS with less than 1GB of memory usage on a single GPU. Moreover, it performs competitively against Visual SLAM systems on outdoor driving scenarios, showcasing its generalization capabilities.", 'abstract_zh': '基于学习的一维视觉里程计（VO）在机器人领域面临鲁棒性、泛化能力和效率的挑战。视觉基础模型如DINOv2的最近进展在各种视觉任务中提高了鲁棒性和泛化能力，但由于特征粒度粗糙，其在VO中的集成仍然有限。在本文中，我们提出DINO-VO，这是一种基于特征的VO系统，利用DINOv2视觉基础模型进行稀疏特征匹配。为了解决集成挑战，我们提出了一种专为DINOv2粗特征设计的显著关键点检测器。此外，我们通过补充细粒度几何特征，增强了DINOv2的鲁棒语义特征，从而获得更局部化的表示。最后，基于变换器的匹配器和可微分姿态估计层通过学习好的匹配来实现精确的相机运动估计。与如SuperPoint等先检测-描述网络相比，DINO-VO在具有挑战性的环境中具有更高的鲁棒性。此外，我们展示了所提出特征描述子的优越准确性和泛化能力，超过独立的DINOv2粗特征。DINO-VO在TartanAir和KITTI数据集上优于先前的帧到帧VO方法，在EuRoC数据集上具有竞争力，同时在单个GPU上以低于1GB的内存使用和72 FPS的效率运行。此外，DINO-VO在室外驾驶场景中与视觉SLAM系统具有竞争力，展示了其泛化能力。', 'title_zh': 'DINO-VO：一种基于特征的视觉里程计，利用视觉基础模型'}
{'arxiv_id': 'arXiv:2507.13052', 'title': 'Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication', 'authors': 'Tianyu Song, Feng Li, Yuan Bi, Angelos Karlas, Amir Yousefi, Daniela Branzan, Zhongliang Jiang, Ulrich Eck, Nassir Navab', 'link': 'https://arxiv.org/abs/2507.13052', 'abstract': 'The advancement and maturity of large language models (LLMs) and robotics have unlocked vast potential for human-computer interaction, particularly in the field of robotic ultrasound. While existing research primarily focuses on either patient-robot or physician-robot interaction, the role of an intelligent virtual sonographer (IVS) bridging physician-robot-patient communication remains underexplored. This work introduces a conversational virtual agent in Extended Reality (XR) that facilitates real-time interaction between physicians, a robotic ultrasound system(RUS), and patients. The IVS agent communicates with physicians in a professional manner while offering empathetic explanations and reassurance to patients. Furthermore, it actively controls the RUS by executing physician commands and transparently relays these actions to the patient. By integrating LLM-powered dialogue with speech-to-text, text-to-speech, and robotic control, our system enhances the efficiency, clarity, and accessibility of robotic ultrasound acquisition. This work constitutes a first step toward understanding how IVS can bridge communication gaps in physician-robot-patient interaction, providing more control and therefore trust into physician-robot interaction while improving patient experience and acceptance of robotic ultrasound.', 'abstract_zh': '大型语言模型和机器人技术的进步及其成熟性为人类与计算机交互，特别是在机器人超声领域，开辟了巨大的潜力。虽然现有研究主要集中在患者-机器人或医生-机器人交互上，但智能虚拟超声技师（IVS）在医生-机器人-患者沟通中的作用仍然尚未充分探索。本文介绍了一个在扩展现实（XR）中实现的对话虚拟代理，该代理促进了医生、机器人超声系统（RUS）和患者之间的实时交互。IVS代理以专业的方式与医生交流，并向患者提供同情心解释和支持。此外，它能够主动控制RUS执行医生命令，并透明地向患者传达这些行动。通过将基于LLM的对话与语音转文本、文本转语音和机器人控制相结合，我们的系统增强了机器人超声获取的效率、清晰度和可访问性。本工作标志着理解IVS如何在医生-机器人-患者交互中弥合沟通缺口的第一步，提供了更多的控制和信任，从而改善患者体验并提高对机器人超声的接受度。', 'title_zh': '智能虚拟超声检查员（IVS）：增强医师-机器人-患者沟通'}
{'arxiv_id': 'arXiv:2507.13017', 'title': 'CubeSat Orbit Insertion Maneuvering Using J2 Perturbation', 'authors': 'M. Amin Alandihallaj, M. Reza Emami', 'link': 'https://arxiv.org/abs/2507.13017', 'abstract': "The precise insertion of CubeSats into designated orbits is a complex task, primarily due to the limited propulsion capabilities and constrained fuel reserves onboard, which severely restrict the scope for large orbital corrections. This limitation necessitates the development of more efficient maneuvering techniques to ensure mission success. In this paper, we propose a maneuvering sequence that exploits the natural J2 perturbation caused by the Earth's oblateness. By utilizing the secular effects of this perturbation, it is possible to passively influence key orbital parameters such as the argument of perigee and the right ascension of the ascending node, thereby reducing the need for extensive propulsion-based corrections. The approach is designed to optimize the CubeSat's orbital insertion and minimize the total fuel required for trajectory adjustments, making it particularly suitable for fuel-constrained missions. The proposed methodology is validated through comprehensive numerical simulations that examine different initial orbital conditions and perturbation environments. Case studies are presented to demonstrate the effectiveness of the J2-augmented strategy in achieving accurate orbital insertion, showing a major reduction in fuel consumption compared to traditional methods. The results underscore the potential of this approach to extend the operational life and capabilities of CubeSats, offering a viable solution for future low-Earth orbit missions.", 'abstract_zh': 'CubeSats入轨到指定轨道的精确姿态设计：基于地球扁球性自然扰动的高效机动策略', 'title_zh': 'CubeSat 轨道插入 maneuvers 利用 J2 扰动'}
{'arxiv_id': 'arXiv:2507.12898', 'title': 'Generalist Bimanual Manipulation via Foundation Video Diffusion Models', 'authors': 'Yao Feng, Hengkai Tan, Xinyi Mao, Guodong Liu, Shuhe Huang, Chendong Xiang, Hang Su, Jun Zhu', 'link': 'https://arxiv.org/abs/2507.12898', 'abstract': 'Bimanual robotic manipulation, which involves the coordinated control of two robotic arms, is foundational for solving challenging tasks. Despite recent progress in general-purpose manipulation, data scarcity and embodiment heterogeneity remain serious obstacles to further scaling up in bimanual settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning (VIDAR), a two-stage framework that leverages large-scale, diffusion-based video pre-training and a novel masked inverse dynamics model for action prediction. We pre-train the video diffusion model on 750K multi-view videos from three real-world bimanual robot platforms, utilizing a unified observation space that encodes robot, camera, task, and scene contexts. Our masked inverse dynamics model learns masks to extract action-relevant information from generated trajectories without requiring pixel-level labels, and the masks can effectively generalize to unseen backgrounds. Our experiments demonstrate that with only 20 minutes of human demonstrations on an unseen robot platform (only 1% of typical data requirements), VIDAR generalizes to unseen tasks and backgrounds with strong semantic understanding, surpassing state-of-the-art methods. Our findings highlight the potential of video foundation models, coupled with masked action prediction, to enable scalable and generalizable robotic manipulation in diverse real-world settings.', 'abstract_zh': '双臂机器人操纵：一种基于视频扩散的行动推理框架', 'title_zh': '基于基础视频扩散模型的通用双臂操作'}
{'arxiv_id': 'arXiv:2507.12768', 'title': 'AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation', 'authors': 'Hengkai Tan, Yao Feng, Xinyi Mao, Shuhe Huang, Guodong Liu, Zhongkai Hao, Hang Su, Jun Zhu', 'link': 'https://arxiv.org/abs/2507.12768', 'abstract': 'Vision-language-action (VLA) models have shown promise on task-conditioned control in complex settings such as bimanual manipulation. However, the heavy reliance on task-specific human demonstrations limits their generalization and incurs high data acquisition costs. In this work, we present a new notion of task-agnostic action paradigm that decouples action execution from task-specific conditioning, enhancing scalability, efficiency, and cost-effectiveness. To address the data collection challenges posed by this paradigm -- such as low coverage density, behavioral redundancy, and safety risks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a scalable self-supervised framework that accelerates collection by over $ 30\\times $ compared to human teleoperation. To further enable effective learning from task-agnostic data, which often suffers from distribution mismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics model equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder (DAD). We additionally integrate a video-conditioned action validation module to verify the feasibility of learned policies across diverse manipulation tasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51% improvement in test accuracy and achieves 30-40% higher success rates in downstream tasks such as lifting, pick-and-place, and clicking, using replay-based video validation. Project Page: this https URL', 'abstract_zh': 'Vision-语言-动作（VLA）模型在双臂操作等复杂环境下的任务条件控制中展示了潜力。然而，对任务特定的人类演示的过度依赖限制了它们的泛化能力并增加了数据获取成本。在这种工作中，我们提出了一种新的任务无关的动作范式，将动作执行与任务特定的条件解耦，增强了可扩展性、效率和成本效益。为了解决由此范式带来的数据采集挑战，如低覆盖率密度、行为冗余和安全风险，我们引入了ATARA（自动化任务无关随机动作）——一个可扩展的自监督框架，与人工远程操作相比，其数据采集速度提高了超过30倍。为了进一步从任务无关的数据中有效学习，这些数据往往存在分布不匹配和无关轨迹的问题，我们提出了AnyPos，这是一种配备臂解耦估计和方向感知解码器（DAD）的逆动力学模型。此外，我们还集成了一个基于视频的动作验证模块，以验证所学习策略在多样化的操作任务中的可行性。广泛的实验表明，AnyPos-ATARA流水线在测试准确性上提高了51%，并在拾取放置、点击等下游任务中实现了30-40%更高的成功率，使用基于回放缓冲区的视频验证。项目页面: [this https URL]', 'title_zh': 'AnyPos: 自动化任务无关双臂操作'}
{'arxiv_id': 'arXiv:2507.12763', 'title': 'Continuous Marine Tracking via Autonomous UAV Handoff', 'authors': 'Heegyeong Kim, Alice James, Avishkar Seth, Endrowednes Kuantama, Jane Williamson, Yimeng Feng, Richard Han', 'link': 'https://arxiv.org/abs/2507.12763', 'abstract': 'This paper introduces an autonomous UAV vision system for continuous, real-time tracking of marine animals, specifically sharks, in dynamic marine environments. The system integrates an onboard computer with a stabilised RGB-D camera and a custom-trained OSTrack pipeline, enabling visual identification under challenging lighting, occlusion, and sea-state conditions. A key innovation is the inter-UAV handoff protocol, which enables seamless transfer of tracking responsibilities between drones, extending operational coverage beyond single-drone battery limitations. Performance is evaluated on a curated shark dataset of 5,200 frames, achieving a tracking success rate of 81.9\\% during real-time flight control at 100 Hz, and robustness to occlusion, illumination variation, and background clutter. We present a seamless UAV handoff framework, where target transfer is attempted via high-confidence feature matching, achieving 82.9\\% target coverage. These results confirm the viability of coordinated UAV operations for extended marine tracking and lay the groundwork for scalable, autonomous monitoring.', 'abstract_zh': '一种用于动态海洋环境连续实时追踪鲨鱼等海洋动物的自主无人机视觉系统', 'title_zh': '连续海洋跟踪通过自主无人机切换'}
{'arxiv_id': 'arXiv:2507.12741', 'title': 'Public Evaluation on Potential Social Impacts of Fully Autonomous Cybernetic Avatars for Physical Support in Daily-Life Environments: Large-Scale Demonstration and Survey at Avatar Land', 'authors': 'Lotfi El Hafi, Kazuma Onishi, Shoichi Hasegawa, Akira Oyama, Tomochika Ishikawa, Masashi Osada, Carl Tornberg, Ryoma Kado, Kento Murata, Saki Hashimoto, Sebastian Carrera Villalobos, Akira Taniguchi, Gustavo Alfonso Garcia Ricardez, Yoshinobu Hagiwara, Tatsuya Aoki, Kensuke Iwata, Takato Horii, Yukiko Horikawa, Takahiro Miyashita, Tadahiro Taniguchi, Hiroshi Ishiguro', 'link': 'https://arxiv.org/abs/2507.12741', 'abstract': 'Cybernetic avatars (CAs) are key components of an avatar-symbiotic society, enabling individuals to overcome physical limitations through virtual agents and robotic assistants. While semi-autonomous CAs intermittently require human teleoperation and supervision, the deployment of fully autonomous CAs remains a challenge. This study evaluates public perception and potential social impacts of fully autonomous CAs for physical support in daily life. To this end, we conducted a large-scale demonstration and survey during Avatar Land, a 19-day public event in Osaka, Japan, where fully autonomous robotic CAs, alongside semi-autonomous CAs, performed daily object retrieval tasks. Specifically, we analyzed responses from 2,285 visitors who engaged with various CAs, including a subset of 333 participants who interacted with fully autonomous CAs and shared their perceptions and concerns through a survey questionnaire. The survey results indicate interest in CAs for physical support in daily life and at work. However, concerns were raised regarding task execution reliability. In contrast, cost and human-like interaction were not dominant concerns. Project page: this https URL.', 'abstract_zh': '基于控制论的代理（CAs）是共生型代理社会的关键组成部分，通过虚拟代理和机器人助手使个体超越物理限制。虽然半自主的CAs间歇性地需要人类远程操作和监督，但完全自主的CAs的部署仍具挑战性。本研究评估了公众对日常生活中提供物理支持的完全自主CAs的感知及其潜在社会影响。为此，我们在日本大阪举行的为期19天的公共活动Avatar Land期间，进行了大规模展示和调查，其中完全自主机器人CAs和半自主CAs共同执行日常取物任务。具体而言，我们分析了2,285名与各种CAs互动的访客的反馈，其中333名参与者与完全自主CAs互动并通过调查问卷分享了他们的感知和担忧。调查结果显示，公众对日常生活中和工作中使用CAs以提供物理支持表现出兴趣，但对任务执行可靠性提出了质疑。相比之下，成本和拟人化交互并非主要关注点。项目页面：请点击此处。', 'title_zh': '面向日常生活环境的全自主网络替身物理支持潜在社会影响的公众评价：大规模示范与调查研究在替身之地'}
{'arxiv_id': 'arXiv:2507.12578', 'title': 'Deep Bilinear Koopman Model for Real-Time Vehicle Control in Frenet Frame', 'authors': 'Mohammad Abtahi, Farhang Motallebi Araghi, Navid Mojahed, Shima Nazari', 'link': 'https://arxiv.org/abs/2507.12578', 'abstract': 'Accurate modeling and control of autonomous vehicles remain a fundamental challenge due to the nonlinear and coupled nature of vehicle dynamics. While Koopman operator theory offers a framework for deploying powerful linear control techniques, learning a finite-dimensional invariant subspace for high-fidelity modeling continues to be an open problem. This paper presents a deep Koopman approach for modeling and control of vehicle dynamics within the curvilinear Frenet frame. The proposed framework uses a deep neural network architecture to simultaneously learn the Koopman operator and its associated invariant subspace from the data. Input-state bilinear interactions are captured by the algorithm while preserving convexity, which makes it suitable for real-time model predictive control (MPC) application. A multi-step prediction loss is utilized during training to ensure long-horizon prediction capability. To further enhance real-time trajectory tracking performance, the model is integrated with a cumulative error regulator (CER) module, which compensates for model mismatch by mitigating accumulated prediction errors. Closed-loop performance is evaluated through hardware-in-the-loop (HIL) experiments using a CarSim RT model as the target plant, with real-time validation conducted on a dSPACE SCALEXIO system. The proposed controller achieved significant reductions in tracking error relative to baseline controllers, confirming its suitability for real-time implementation in embedded autonomous vehicle systems.', 'abstract_zh': '基于曲率Frenet框架的深度Koopman方法在自主车辆动力学建模与控制中的应用', 'title_zh': '基于Frenet坐标系的深度双线性Koopman模型用于实时车辆控制'}
{'arxiv_id': 'arXiv:2507.12508', 'title': 'MindJourney: Test-Time Scaling with World Models for Spatial Reasoning', 'authors': 'Yuncong Yang, Jiageng Liu, Zheyuan Zhang, Siyuan Zhou, Reuben Tan, Jianwei Yang, Yilun Du, Chuang Gan', 'link': 'https://arxiv.org/abs/2507.12508', 'abstract': 'Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.', 'abstract_zh': '3D空间中的空间推理是人类认知的核心，对于导航和操作等体内任务不可或缺。然而，最先进的视觉-语言模型（VLMs）在预测主观运动后场景将如何变化等简单任务上常表现出色。它们只能感知2D图像，缺乏3D动态的内部模型。因此，我们提出MindJourney，一种测试时扩展框架，通过将VLM耦合到基于视频扩散的可控世界模型，赋予其缺乏的这种能力。VLM迭代地勾画出简要的相机轨迹，而世界模型在每一步合成相应的视图。VLM随后在交互式探索过程中收集的多视角证据上进行推理。在无需微调的情况下，我们的MindJourney在代表性的空间推理基准SAT上平均实现了超过8%的性能提升，表明将VLM与世界模型结合用于测试时扩展提供了实现稳健3D推理的简单、即插即用的方法。同时，我们的方法也改善了通过强化学习训练的测试时推理VLM，展示了我们方法利用世界模型进行测试时扩展的潜力。', 'title_zh': 'MindJourney：面向时空推理的测试时期世界模型缩放方法'}
{'arxiv_id': 'arXiv:2507.12494', 'title': 'MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents', 'authors': "Dustin Holley, Jovin D'sa, Hossein Nourkhiz Mahjoub, Gibran Ali", 'link': 'https://arxiv.org/abs/2507.12494', 'abstract': 'Enhancing simulation environments to replicate real-world driver behavior, i.e., more humanlike sim agents, is essential for developing autonomous vehicle technology. In the context of highway merging, previous works have studied the operational-level yielding dynamics of lag vehicles in response to a merging car at highway on-ramps. Other works focusing on tactical decision modeling generally consider limited action sets or utilize payoff functions with large parameter sets and limited payoff bounds. In this work, we aim to improve the simulation of the highway merge scenario by targeting a game theoretic model for tactical decision-making with improved payoff functions and lag actions. We couple this with an underlying dynamics model to have a unified decision and dynamics model that can capture merging interactions and simulate more realistic interactions in an explainable and interpretable fashion. The proposed model demonstrated good reproducibility of complex interactions when validated on a real-world dataset. The model was finally integrated into a high fidelity simulation environment and confirmed to have adequate computation time efficiency for use in large-scale simulations to support autonomous vehicle development.', 'abstract_zh': '增强仿真环境以更真实地模拟驾驶员行为，即更具人性化的仿真代理，对于开发自动驾驶车辆技术至关重要。在高速公路合并 context 下，以往研究已经探讨了后方车辆应对高速公路入口汇入车辆的操作级让行动态。其他专注于战术决策建模的工作通常考虑有限的动作集或使用具有较大参数集和有限支付范围的收益函数。在本工作中，我们旨在通过采用改进的收益函数和后方行动，以及游戏 theoretic 模型来改进高速公路合并场景的模拟。我们将此与一个底层动力学模型结合，从而实现统一的决策和动力学模型，能够捕捉合并交互并以可解释和可解析的方式模拟更真实的交互。所提出的模型在基于真实数据集进行验证时展示了对复杂交互的良好再现性。该模型最终被整合到高保真仿真环境中，并被证实具有在大规模仿真中所需的足够计算时间效率，以支持自动驾驶车辆的发展。', 'title_zh': 'MR-LDM — 合并反应纵向决策模型：基于博弈论的人类决策建模以用于交互式仿真实体'}
