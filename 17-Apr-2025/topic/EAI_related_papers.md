# EmoACT: a Framework to Embed Emotions into Artificial Agents Based on Affect Control Theory 

**Title (ZH)**: EmoACT：基于情感控制理论的情感嵌入人工代理框架 

**Authors**: Francesca Corrao, Alice Nardelli, Jennifer Renoux, Carmine Tommaso Recchiuto  

**Link**: [PDF](https://arxiv.org/pdf/2504.12125)  

**Abstract**: As robots and artificial agents become increasingly integrated into daily life, enhancing their ability to interact with humans is essential. Emotions, which play a crucial role in human interactions, can improve the naturalness and transparency of human-robot interactions (HRI) when embodied in artificial agents. This study aims to employ Affect Control Theory (ACT), a psychological model of emotions deeply rooted in interaction, for the generation of synthetic emotions. A platform-agnostic framework inspired by ACT was developed and implemented in a humanoid robot to assess its impact on human perception. Results show that the frequency of emotional displays impacts how users perceive the robot. Moreover, appropriate emotional expressions seem to enhance the robot's perceived emotional and cognitive agency. The findings suggest that ACT can be successfully employed to embed synthetic emotions into robots, resulting in effective human-robot interactions, where the robot is perceived more as a social agent than merely a machine. 

**Abstract (ZH)**: 随着机器人和人工代理越来越多地融入日常生活，提高它们与人类互动的能力变得越来越重要。将情绪体现在人工代理中可以增强人类与机器人互动（HRI）的自然性和透明度。本研究旨在利用情绪控制理论（ACT），一种深深植根于互动的心理学情绪模型，生成合成情绪。受ACT启发的一个平台无关的框架已在人形机器人中开发和实现，以评估其对人类感知的影响。研究结果表明，情绪展示的频率会影响用户对机器人的感知。此外，适当的的情绪表达似乎增强了机器人在感知中的情感和认知代理能力。研究发现表明，ACT可以成功应用于将合成情绪嵌入机器人中，从而在人机互动中实现有效的交互，使机器人被更多地视为社会代理而非 merely 机器。 

---
# A Graph-Based Reinforcement Learning Approach with Frontier Potential Based Reward for Safe Cluttered Environment Exploration 

**Title (ZH)**: 基于图形的强化学习方法：结合前端潜力奖励的安全拥挤环境探索 

**Authors**: Gabriele Calzolari, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos  

**Link**: [PDF](https://arxiv.org/pdf/2504.11907)  

**Abstract**: Autonomous exploration of cluttered environments requires efficient exploration strategies that guarantee safety against potential collisions with unknown random obstacles. This paper presents a novel approach combining a graph neural network-based exploration greedy policy with a safety shield to ensure safe navigation goal selection. The network is trained using reinforcement learning and the proximal policy optimization algorithm to maximize exploration efficiency while reducing the safety shield interventions. However, if the policy selects an infeasible action, the safety shield intervenes to choose the best feasible alternative, ensuring system consistency. Moreover, this paper proposes a reward function that includes a potential field based on the agent's proximity to unexplored regions and the expected information gain from reaching them. Overall, the approach investigated in this paper merges the benefits of the adaptability of reinforcement learning-driven exploration policies and the guarantee ensured by explicit safety mechanisms. Extensive evaluations in simulated environments demonstrate that the approach enables efficient and safe exploration in cluttered environments. 

**Abstract (ZH)**: 基于图神经网络的高效安全探索策略在拥挤环境中的自主探索 

---
# Causality-enhanced Decision-Making for Autonomous Mobile Robots in Dynamic Environments 

**Title (ZH)**: 因果关系增强的自主移动机器人在动态环境中的决策制定 

**Authors**: Luca Castri, Gloria Beraldo, Nicola Bellotto  

**Link**: [PDF](https://arxiv.org/pdf/2504.11901)  

**Abstract**: The growing integration of robots in shared environments -- such as warehouses, shopping centres, and hospitals -- demands a deep understanding of the underlying dynamics and human behaviours, including how, when, and where individuals engage in various activities and interactions. This knowledge goes beyond simple correlation studies and requires a more comprehensive causal analysis. By leveraging causal inference to model cause-and-effect relationships, we can better anticipate critical environmental factors and enable autonomous robots to plan and execute tasks more effectively. To this end, we propose a novel causality-based decision-making framework that reasons over a learned causal model to predict battery usage and human obstructions, understanding how these factors could influence robot task execution. Such reasoning framework assists the robot in deciding when and how to complete a given task. To achieve this, we developed also PeopleFlow, a new Gazebo-based simulator designed to model context-sensitive human-robot spatial interactions in shared workspaces. PeopleFlow features realistic human and robot trajectories influenced by contextual factors such as time, environment layout, and robot state, and can simulate a large number of agents. While the simulator is general-purpose, in this paper we focus on a warehouse-like environment as a case study, where we conduct an extensive evaluation benchmarking our causal approach against a non-causal baseline. Our findings demonstrate the efficacy of the proposed solutions, highlighting how causal reasoning enables autonomous robots to operate more efficiently and safely in dynamic environments shared with humans. 

**Abstract (ZH)**: 机器人在共享环境中的日益集成——如仓库、购物中心和医院等——需要深入理解其背后的动力学和人类行为，包括个体在何时、何地以及如何参与各种活动和互动。这一知识远远超出了简单的相关性研究，需要更全面的因果分析。通过利用因果推断来建模因果关系，我们能够更好地预见关键的环境因素，使自主机器人能够更有效地规划和执行任务。为此，我们提出了一种基于因果关系的决策框架，该框架基于学习到的因果模型来预测电池使用和人类干扰，理解这些因素如何影响机器人任务执行。这种推理框架帮助机器人决定何时以及如何完成给定任务。为此，我们还开发了PeopleFlow，这是一种新的基于Gazebo的模拟器，用于建模共享工作空间中上下文敏感的人机空间交互。PeopleFlow 功能包括受时间、环境布局和机器人状态等上下文因素影响的真实人类和机器人轨迹，并且可以模拟大量代理。虽然模拟器具有通用性，但在本文中，我们以类似仓库的环境为案例研究，进行了广泛的评估，将我们的因果方法与非因果基准进行了比较。我们的研究结果表明，所提出解决方案的有效性，突显了因果推理如何使自主机器人更高效、更安全地在与人类共享的动态环境中运行。 

---
# Towards Forceful Robotic Foundation Models: a Literature Survey 

**Title (ZH)**: 面向力ful机器人基础模型：文献综述 

**Authors**: William Xie, Nikolaus Correll  

**Link**: [PDF](https://arxiv.org/pdf/2504.11827)  

**Abstract**: This article reviews contemporary methods for integrating force, including both proprioception and tactile sensing, in robot manipulation policy learning. We conduct a comparative analysis on various approaches for sensing force, data collection, behavior cloning, tactile representation learning, and low-level robot control. From our analysis, we articulate when and why forces are needed, and highlight opportunities to improve learning of contact-rich, generalist robot policies on the path toward highly capable touch-based robot foundation models. We generally find that while there are few tasks such as pouring, peg-in-hole insertion, and handling delicate objects, the performance of imitation learning models is not at a level of dynamics where force truly matters. Also, force and touch are abstract quantities that can be inferred through a wide range of modalities and are often measured and controlled implicitly. We hope that juxtaposing the different approaches currently in use will help the reader to gain a systemic understanding and help inspire the next generation of robot foundation models. 

**Abstract (ZH)**: 本文回顾了结合本体感受和触觉感知的当代机器人 manipulaton 力控制方法。我们对各种力感知方法、数据收集、行为克隆、触觉表示学习以及低级机器人控制的策略进行了比较分析。通过分析，我们阐述了何时以及为何需要力，强调了在通向基于触觉的高能力机器人基础模型过程中改进接触丰富的一般机器人策略的机会。我们发现，虽然有些任务如倒液体、孔内插入钉子和处理精细物体需要力，但模仿学习模型的性能还未达到足够细致的动力学特性，使得力的实际作用不显著。此外，力和触觉是可以通过多种模态推断出的抽象量，通常会隐式地进行测量和控制。我们希望通过对比当前使用的不同方法，帮助读者获得系统的理解，并启发下一代机器人基础模型的研发。 

---
# Inversion of biological strategies in engineering technology: in case underwater soft robot 

**Title (ZH)**: 生物策略在工程技术中的逆向应用：以水下软体机器人为例 

**Authors**: Siqing Chen, He Xua, Xueyu Zhang, Zhen Ma  

**Link**: [PDF](https://arxiv.org/pdf/2504.11722)  

**Abstract**: This paper proposes a biomimetic design framework based on biological strategy inversion, aiming to systematically map solutions evolved in nature to the engineering field. By constructing a "Function-Behavior-Feature-Environment" (F-B-Cs in E) knowledge model, combined with natural language processing (NLP) and multi-criteria decision-making methods, it achieves efficient conversion from biological strategies to engineering solutions. Using underwater soft robot design as a case study, the effectiveness of the framework in optimizing drive mechanisms, power distribution, and motion pattern design is verified. This research provides scalable methodological support for interdisciplinary biomimetic innovation. 

**Abstract (ZH)**: 基于生物策略反转的生物仿生设计框架：从自然演化方案到工程解决方案的系统映射及其在水下软体机器人设计中的应用验证 

---
# GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision 

**Title (ZH)**: GrabS: 生成式具身代理用于无场景监督的3D物体分割 

**Authors**: Zihui Zhang, Yafei Yang, Hongtao Wen, Bo Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.11754)  

**Abstract**: We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually limited to identifying simple objects like cars or their segmented objects are often inferior due to the lack of objectness in pretrained features. In this paper, we propose a new two-stage pipeline called GrabS. The core concept of our method is to learn generative and discriminative object-centric priors as a foundation from object datasets in the first stage, and then design an embodied agent to learn to discover multiple objects by querying against the pretrained generative priors in the second stage. We extensively evaluate our method on two real-world datasets and a newly created synthetic dataset, demonstrating remarkable segmentation performance, clearly surpassing all existing unsupervised methods. 

**Abstract (ZH)**: 我们研究在无需3D场景人工标签监督的情况下复杂点云中的3D物体分割难题。通过依赖预训练的2D特征相似性或外部信号（如运动）来分组3D点为物体，现有的无监督方法通常仅限于识别简单的物体，如汽车，或者由于预训练特征缺乏物体性，分割出的物体质量往往较差。本文提出了一种名为GrabS的新两阶段管道。我们方法的核心概念是在第一阶段从物体数据集中学习生成性和判别性物体中心先验，然后在第二阶段设计一个具身代理，通过查询预训练的生成性先验来学习发现多个物体。我们详细评估了该方法在两个真实世界数据集和一个新创建的合成数据集上的性能，展示了显著的分割性能，明显优于所有现有的无监督方法。 

---
# Cross-cultural Deployment of Autonomous Vehicles Using Data-light Inverse Reinforcement Learning 

**Title (ZH)**: 基于数据轻量级逆强化学习的跨文化自动驾驶车辆部署 

**Authors**: Hongliang Lu, Shuqi Shen, Junjie Yang, Chao Lu, Xinhu Zheng, Hai Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.11506)  

**Abstract**: More than the adherence to specific traffic regulations, driving culture touches upon a more implicit part - an informal, conventional, collective behavioral pattern followed by drivers - that varies across countries, regions, and even cities. Such cultural divergence has become one of the biggest challenges in deploying autonomous vehicles (AVs) across diverse regions today. The current emergence of data-driven methods has shown a potential solution to enable culture-compatible driving through learning from data, but what if some underdeveloped regions cannot provide sufficient local data to inform driving culture? This issue is particularly significant for a broader global AV market. Here, we propose a cross-cultural deployment scheme for AVs, called data-light inverse reinforcement learning, designed to re-calibrate culture-specific AVs and assimilate them into other cultures. First, we report the divergence in driving cultures through a comprehensive comparative analysis of naturalistic driving datasets on highways from three countries: Germany, China, and the USA. Then, we demonstrate the effectiveness of our scheme by testing the expeditious cross-cultural deployment across these three countries, with cumulative testing mileage of over 56084 km. The performance is particularly advantageous when cross-cultural deployment is carried out without affluent local data. Results show that we can reduce the dependence on local data by a margin of 98.67% at best. This study is expected to bring a broader, fairer AV global market, particularly in those regions that lack enough local data to develop culture-compatible AVs. 

**Abstract (ZH)**: 跨文化自主驾驶车辆部署方案：基于数据稀疏逆强化学习的方法 

---
# Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection 

**Title (ZH)**: 选择性演示检索以提高隐含仇恨言论检测 

**Authors**: Yumin Kim, Hwanhee Lee  

**Link**: [PDF](https://arxiv.org/pdf/2504.12082)  

**Abstract**: Hate speech detection is a crucial area of research in natural language processing, essential for ensuring online community safety. However, detecting implicit hate speech, where harmful intent is conveyed in subtle or indirect ways, remains a major challenge. Unlike explicit hate speech, implicit expressions often depend on context, cultural subtleties, and hidden biases, making them more challenging to identify consistently. Additionally, the interpretation of such speech is influenced by external knowledge and demographic biases, resulting in varied detection results across different language models. Furthermore, Large Language Models often show heightened sensitivity to toxic language and references to vulnerable groups, which can lead to misclassifications. This over-sensitivity results in false positives (incorrectly identifying harmless statements as hateful) and false negatives (failing to detect genuinely harmful content). Addressing these issues requires methods that not only improve detection precision but also reduce model biases and enhance robustness. To address these challenges, we propose a novel method, which utilizes in-context learning without requiring model fine-tuning. By adaptively retrieving demonstrations that focus on similar groups or those with the highest similarity scores, our approach enhances contextual comprehension. Experimental results show that our method outperforms current state-of-the-art techniques. Implementation details and code are available at TBD. 

**Abstract (ZH)**: 隐含仇恨言论检测是自然语言处理领域的一项关键研究，对于确保在线社区安全至关重要。然而，识别通过含蓄或间接的方式传达有害意图的隐含仇恨言论仍是一项重大挑战。与明确的仇恨言论不同，隐含表达往往依赖于上下文、文化细微差别和隐藏的偏见，使其更难一致地识别。此外，这种言论的解读受到外部知识和人口统计学偏见的影响，导致不同语言模型的检测结果存在差异。同时，大型语言模型往往对有毒语言和对脆弱群体的提及表现出过度敏感，这可能导致误分类。这种过度敏感性会导致假阳性（错误地将无害陈述识别为仇恨言论）和假阴性（未能检测到真正有害的内容）。为了解决这些问题，需要不仅能提高检测精度，还能减少模型偏见和增强鲁棒性的方法。为此，我们提出了一种新方法，该方法利用上下文学习而无需对模型进行微调。通过适应性检索重点相似群体或具有最高相似度评分的演示文稿，我们的方法增强了上下文理解。实验结果显示，我们的方法优于当前最先进的技术。实施细节和代码可在TBD获取。 

---
# VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning 

**Title (ZH)**: VIPO: 基于值函数不一致性惩罚的离线强化学习 

**Authors**: Xuyang Chen, Guojian Wang, Keyu Yan, Lin Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2504.11944)  

**Abstract**: Offline reinforcement learning (RL) learns effective policies from pre-collected datasets, offering a practical solution for applications where online interactions are risky or costly. Model-based approaches are particularly advantageous for offline RL, owing to their data efficiency and generalizability. However, due to inherent model errors, model-based methods often artificially introduce conservatism guided by heuristic uncertainty estimation, which can be unreliable. In this paper, we introduce VIPO, a novel model-based offline RL algorithm that incorporates self-supervised feedback from value estimation to enhance model training. Specifically, the model is learned by additionally minimizing the inconsistency between the value learned directly from the offline data and the one estimated from the model. We perform comprehensive evaluations from multiple perspectives to show that VIPO can learn a highly accurate model efficiently and consistently outperform existing methods. It offers a general framework that can be readily integrated into existing model-based offline RL algorithms to systematically enhance model accuracy. As a result, VIPO achieves state-of-the-art performance on almost all tasks in both D4RL and NeoRL benchmarks. 

**Abstract (ZH)**: 基于模型的离线强化学习（RL）通过预收集的数据集学习有效的策略，为在线交互具有风险或成本的应用提供了一种实际解决方案。基于模型的方法特别适用于离线RL，这得益于其数据效率和泛化能力。然而，由于固有的模型误差，基于模型的方法通常会根据启发式的不确定性估计人为引入保守性，这可能是不可靠的。在这篇文章中，我们引入了VIPO，这是一种新颖的基于模型的离线RL算法，通过价值估计的自我监督反馈来增强模型训练。具体而言，该模型通过额外最小化直接从离线数据学习的价值与从模型估计的价值之间的一致性差异来学习。我们从多个角度进行综合评估，以证明VIPO能够高效且一致地学习高度准确的模型，并且能够系统地超越现有方法。它提供了一个通用框架，可以轻松集成到现有的基于模型的离线RL算法中，以系统地提高模型准确度。结果，VIPO在D4RL和NeoRL基准测试中的几乎所有任务中都实现了最先进的性能。 

---
# Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification 

**Title (ZH)**: 选择性注意联邦学习：提高临床文本分类的隐私性和效率 

**Authors**: Yue Li, Lihong Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2504.11793)  

**Abstract**: Federated Learning (FL) faces major challenges regarding communication overhead and model privacy when training large language models (LLMs), especially in healthcare applications. To address these, we introduce Selective Attention Federated Learning (SAFL), a novel approach that dynamically fine-tunes only those transformer layers identified as attention-critical. By employing attention patterns to determine layer importance, SAFL significantly reduces communication bandwidth and enhances differential privacy resilience. Evaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and MIMIC-III discharge summaries) demonstrate that SAFL achieves competitive performance with centralized models while substantially improving communication efficiency and privacy preservation. 

**Abstract (ZH)**: 选择性注意力联邦学习（SAFL）：一种用于大型语言模型训练的新方法 

---
# Perceptions of Agentic AI in Organizations: Implications for Responsible AI and ROI 

**Title (ZH)**: 组织中代理型人工智能的感知：负责任的人工智能与投资回报的影响 

**Authors**: Lee Ackerman  

**Link**: [PDF](https://arxiv.org/pdf/2504.11564)  

**Abstract**: As artificial intelligence (AI) systems rapidly gain autonomy, the need for robust responsible AI frameworks becomes paramount. This paper investigates how organizations perceive and adapt such frameworks amidst the emerging landscape of increasingly sophisticated agentic AI. Employing an interpretive qualitative approach, the study explores the lived experiences of AI professionals. Findings highlight that the inherent complexity of agentic AI systems and their responsible implementation, rooted in the intricate interconnectedness of responsible AI dimensions and the thematic framework (an analytical structure developed from the data), combined with the novelty of agentic AI, contribute to significant challenges in organizational adaptation, characterized by knowledge gaps, a limited emphasis on stakeholder engagement, and a strong focus on control. These factors, by hindering effective adaptation and implementation, ultimately compromise the potential for responsible AI and the realization of ROI. 

**Abstract (ZH)**: 随着人工智能（AI）系统迅速获得自主性，建立 robust 责任 AI 框架的需求变得至关重要。本文考察了组织在日益复杂的具身AI新兴景观中如何看待和适应这些框架的方式。通过一种解释性的定性方法，研究探讨了AI专业人员的 lived experiences。研究发现，具身AI系统的固有复杂性及其负责任实施的挑战，源于负责任AI维度和主题框架（从数据中开发的分析结构）的复杂交织关系，以及具身AI的新颖性，导致了组织适应中的显著挑战，包括知识空白、对利益相关者参与的重视不足以及对控制的强烈关注。这些因素通过阻碍有效的适应和实施，最终损害了责任AI及其回报的实现潜力。 

---
