# An Online Adaptation Method for Robust Depth Estimation and Visual Odometry in the Open World 

**Title (ZH)**: 开放世界中的稳健深度估计和视觉里程杆在线适应方法 

**Authors**: Xingwu Ji, Haochen Niu, Dexin Duan, Rendong Ying, Fei Wen, Peilin Liu  

**Link**: [PDF](https://arxiv.org/pdf/2504.11698)  

**Abstract**: Recently, learning-based robotic navigation systems have gained extensive research attention and made significant progress. However, the diversity of open-world scenarios poses a major challenge for the generalization of such systems to practical scenarios. Specifically, learned systems for scene measurement and state estimation tend to degrade when the application scenarios deviate from the training data, resulting to unreliable depth and pose estimation. Toward addressing this problem, this work aims to develop a visual odometry system that can fast adapt to diverse novel environments in an online manner. To this end, we construct a self-supervised online adaptation framework for monocular visual odometry aided by an online-updated depth estimation module. Firstly, we design a monocular depth estimation network with lightweight refiner modules, which enables efficient online adaptation. Then, we construct an objective for self-supervised learning of the depth estimation module based on the output of the visual odometry system and the contextual semantic information of the scene. Specifically, a sparse depth densification module and a dynamic consistency enhancement module are proposed to leverage camera poses and contextual semantics to generate pseudo-depths and valid masks for the online adaptation. Finally, we demonstrate the robustness and generalization capability of the proposed method in comparison with state-of-the-art learning-based approaches on urban, in-house datasets and a robot platform. Code is publicly available at: this https URL. 

**Abstract (ZH)**: 近期，基于学习的机器人导航系统获得了广泛的研究关注并取得了显著进展。然而，开放世界场景的多样性对这类系统在实际场景中的泛化能力构成了重大挑战。具体而言，用于场景测量和状态估计的学到的系统在应用场景偏离训练数据时往往会退化，导致不稳定的深度和姿态估计。为解决这一问题，本工作旨在开发一种能够在线快速适应多样化新型环境的视觉里程计系统。为此，我们构建了一个由在线更新的深度估计模块辅助的半监督在线适应框架。首先，我们设计了一种配有轻量级细化模块的单目深度估计网络，以实现高效的在线适应。然后，我们基于视觉里程计系统输出和场景的上下文语义信息构建了一个半监督学习目标。具体来说，我们提出了稀疏深度稠密化模块和动态一致性增强模块，利用相机姿态和上下文语义生成伪深度和有效的掩码以实现在线适应。最后，我们通过在城市、室内数据集和机器人平台上与现有最先进的学习方法进行比较，展示了所提出方法的鲁棒性和泛化能力。相关代码已公开于此：this https URL。 

---
# DM-OSVP++: One-Shot View Planning Using 3D Diffusion Models for Active RGB-Based Object Reconstruction 

**Title (ZH)**: DM-OSVP++: 基于3D扩散模型的一次成像视图规划方法用于基于RGB的对象重建 

**Authors**: Sicong Pan, Liren Jin, Xuying Huang, Cyrill Stachniss, Marija Popović, Maren Bennewitz  

**Link**: [PDF](https://arxiv.org/pdf/2504.11674)  

**Abstract**: Active object reconstruction is crucial for many robotic applications. A key aspect in these scenarios is generating object-specific view configurations to obtain informative measurements for reconstruction. One-shot view planning enables efficient data collection by predicting all views at once, eliminating the need for time-consuming online replanning. Our primary insight is to leverage the generative power of 3D diffusion models as valuable prior information. By conditioning on initial multi-view images, we exploit the priors from the 3D diffusion model to generate an approximate object model, serving as the foundation for our view planning. Our novel approach integrates the geometric and textural distributions of the object model into the view planning process, generating views that focus on the complex parts of the object to be reconstructed. We validate the proposed active object reconstruction system through both simulation and real-world experiments, demonstrating the effectiveness of using 3D diffusion priors for one-shot view planning. 

**Abstract (ZH)**: 基于3D扩散模型的主动物体重建中的单次视图规划关键技术 

---
# Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions 

**Title (ZH)**: 保障天空安全：无人机反制方法综述、基准测试及未来方向 

**Authors**: Yifei Dong, Fengyi Wu, Sanjian Zhang, Guangyu Chen, Yuzhi Hu, Masumi Yano, Jingdong Sun, Siyu Huang, Feng Liu, Qi Dai, Zhi-Qi Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2504.11967)  

**Abstract**: Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure inspection, surveillance, and related tasks, yet they also introduce critical security challenges. This survey provides a wide-ranging examination of the anti-UAV domain, centering on three core objectives-classification, detection, and tracking-while detailing emerging methodologies such as diffusion-based data synthesis, multi-modal fusion, vision-language modeling, self-supervised learning, and reinforcement learning. We systematically evaluate state-of-the-art solutions across both single-modality and multi-sensor pipelines (spanning RGB, infrared, audio, radar, and RF) and discuss large-scale as well as adversarially oriented benchmarks. Our analysis reveals persistent gaps in real-time performance, stealth detection, and swarm-based scenarios, underscoring pressing needs for robust, adaptive anti-UAV systems. By highlighting open research directions, we aim to foster innovation and guide the development of next-generation defense strategies in an era marked by the extensive use of UAVs. 

**Abstract (ZH)**: 无人驾驶航空器（UAVs）在基础设施检查、监控及相关任务中不可或缺，但同时也引入了关键的安全挑战。本文综述了反UAV领域，围绕分类、检测和跟踪三大核心目标进行广泛考察，并详细介绍了诸如基于扩散的数据合成、多模态融合、视觉-语言建模、半监督学习和强化学习等新兴方法。我们系统地评估了现有解决方案在单模态和多传感器管道（涵盖RGB、红外、音频、雷达和RF）中的性能，并讨论了大型及对抗性导向的基准测试。我们的分析揭示了实时性能、隐身检测和集群场景中的持续差距，突显了对稳健、自适应反UAV系统的迫切需求。通过强调开放的研究方向，旨在促进创新并指导在广泛使用UAV的时代下新一代防御策略的发展。 

---
# snnTrans-DHZ: A Lightweight Spiking Neural Network Architecture for Underwater Image Dehazing 

**Title (ZH)**: snnTrans-DHZ：一种用于水下图像去雾的轻量级脉冲神经网络架构 

**Authors**: Vidya Sudevan, Fakhreddine Zayer, Rizwana Kausar, Sajid Javed, Hamad Karki, Giulia De Masi, Jorge Dias  

**Link**: [PDF](https://arxiv.org/pdf/2504.11482)  

**Abstract**: Underwater image dehazing is critical for vision-based marine operations because light scattering and absorption can severely reduce visibility. This paper introduces snnTrans-DHZ, a lightweight Spiking Neural Network (SNN) specifically designed for underwater dehazing. By leveraging the temporal dynamics of SNNs, snnTrans-DHZ efficiently processes time-dependent raw image sequences while maintaining low power consumption. Static underwater images are first converted into time-dependent sequences by repeatedly inputting the same image over user-defined timesteps. These RGB sequences are then transformed into LAB color space representations and processed concurrently. The architecture features three key modules: (i) a K estimator that extracts features from multiple color space representations; (ii) a Background Light Estimator that jointly infers the background light component from the RGB-LAB images; and (iii) a soft image reconstruction module that produces haze-free, visibility-enhanced outputs. The snnTrans-DHZ model is directly trained using a surrogate gradient-based backpropagation through time (BPTT) strategy alongside a novel combined loss function. Evaluated on the UIEB benchmark, snnTrans-DHZ achieves a PSNR of 21.68 dB and an SSIM of 0.8795, and on the EUVP dataset, it yields a PSNR of 23.46 dB and an SSIM of 0.8439. With only 0.5670 million network parameters, and requiring just 7.42 GSOPs and 0.0151 J of energy, the algorithm significantly outperforms existing state-of-the-art methods in terms of efficiency. These features make snnTrans-DHZ highly suitable for deployment in underwater robotics, marine exploration, and environmental monitoring. 

**Abstract (ZH)**: 基于突触神经网络的snnTrans-DHZ水下去雾方法 

---
# Adapting a World Model for Trajectory Following in a 3D Game 

**Title (ZH)**: 适配于3D游戏轨迹跟随的世界模型 

**Authors**: Marko Tot, Shu Ishida, Abdelhak Lemkhenter, David Bignell, Pallavi Choudhury, Chris Lovett, Luis França, Matheus Ribeiro Furtado de Mendonça, Tarun Gupta, Darren Gehring, Sam Devlin, Sergio Valcarcel Macua, Raluca Georgescu  

**Link**: [PDF](https://arxiv.org/pdf/2504.12299)  

**Abstract**: Imitation learning is a powerful tool for training agents by leveraging expert knowledge, and being able to replicate a given trajectory is an integral part of it. In complex environments, like modern 3D video games, distribution shift and stochasticity necessitate robust approaches beyond simple action replay. In this study, we apply Inverse Dynamics Models (IDM) with different encoders and policy heads to trajectory following in a modern 3D video game -- Bleeding Edge. Additionally, we investigate several future alignment strategies that address the distribution shift caused by the aleatoric uncertainty and imperfections of the agent. We measure both the trajectory deviation distance and the first significant deviation point between the reference and the agent's trajectory and show that the optimal configuration depends on the chosen setting. Our results show that in a diverse data setting, a GPT-style policy head with an encoder trained from scratch performs the best, DINOv2 encoder with the GPT-style policy head gives the best results in the low data regime, and both GPT-style and MLP-style policy heads had comparable results when pre-trained on a diverse setting and fine-tuned for a specific behaviour setting. 

**Abstract (ZH)**: 模仿学习是一种利用专家知识训练代理的强大工具，复制给定轨迹是其核心组成部分。在现代3D视频游戏等复杂环境中，分布偏移和随机性需要超越简单的动作回放的稳健方法。在此研究中，我们应用不同编码器和策略头的逆动力学模型（IDM）在现代3D视频游戏——Bleeding Edge中进行轨迹跟随，并探讨了几种未来对齐策略，以解决由偶然不确定性引起的分布偏移和代理的不完美性。我们测量参考轨迹和代理轨迹之间的轨迹偏差距离及首次显著偏差点，并表明最优配置取决于所选设置。结果显示，在多样数据集设置中，从头训练的编码器与GPT风格的策略头相结合的配置效果最好；在数据有限的情况下，DINOv2编码器与GPT风格的策略头的效果最佳；当在多样数据集上预训练并在特定行为设置下微调时，GPT风格和MLP风格的策略头表现出类似的效果。 

---
# SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians 

**Title (ZH)**: SHeaP: 自监督头部几何预测器，通过2D高斯学习获得 

**Authors**: Liam Schoneveld, Zhe Chen, Davide Davoli, Jiapeng Tang, Saimon Terazawa, Ko Nishino, Matthias Nießner  

**Link**: [PDF](https://arxiv.org/pdf/2504.12292)  

**Abstract**: Accurate, real-time 3D reconstruction of human heads from monocular images and videos underlies numerous visual applications. As 3D ground truth data is hard to come by at scale, previous methods have sought to learn from abundant 2D videos in a self-supervised manner. Typically, this involves the use of differentiable mesh rendering, which is effective but faces limitations. To improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor Learned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a set of Gaussians that are rigged to this mesh. We then reanimate this rigged head avatar to match a target frame, and backpropagate photometric losses to both the 3DMM and Gaussian prediction networks. We find that using Gaussians for rendering substantially improves the effectiveness of this self-supervised approach. Training solely on 2D data, our method surpasses existing self-supervised approaches in geometric evaluations on the NoW benchmark for neutral faces and a new benchmark for non-neutral expressions. Our method also produces highly expressive meshes, outperforming state-of-the-art in emotion classification. 

**Abstract (ZH)**: 单目图像和视频中自监督的人头准确实时三维重建及其应用 

---
# How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions 

**Title (ZH)**: 如何做到这一点？合成日常交互中的3D手部运动和接触 

**Authors**: Aditya Prakash, Benjamin Lundell, Dmitry Andreychuk, David Forsyth, Saurabh Gupta, Harpreet Sawhney  

**Link**: [PDF](https://arxiv.org/pdf/2504.12284)  

**Abstract**: We tackle the novel problem of predicting 3D hand motion and contact maps (or Interaction Trajectories) given a single RGB view, action text, and a 3D contact point on the object as input. Our approach consists of (1) Interaction Codebook: a VQVAE model to learn a latent codebook of hand poses and contact points, effectively tokenizing interaction trajectories, (2) Interaction Predictor: a transformer-decoder module to predict the interaction trajectory from test time inputs by using an indexer module to retrieve a latent affordance from the learned codebook. To train our model, we develop a data engine that extracts 3D hand poses and contact trajectories from the diverse HoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger than existing works, in terms of diversity of objects and interactions observed, and test for generalization of the model across object categories, action categories, tasks, and scenes. Experimental results show the effectiveness of our approach over transformer & diffusion baselines across all settings. 

**Abstract (ZH)**: 基于单个RGB视图、动作文本和物体上的3D接触点预测3D手部运动和接触图（或交互轨迹）的问题研究：一种交互码本和交互预测器的方法及其应用 

---
# Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing 

**Title (ZH)**: 基于 Anatomy-Aware 后处理的不确定性引导从粗到细肿瘤分割 

**Authors**: Ilkin Sevgi Isler, David Mohaisen, Curtis Lisle, Damla Turgut, Ulas Bagci  

**Link**: [PDF](https://arxiv.org/pdf/2504.12215)  

**Abstract**: Reliable tumor segmentation in thoracic computed tomography (CT) remains challenging due to boundary ambiguity, class imbalance, and anatomical variability. We propose an uncertainty-guided, coarse-to-fine segmentation framework that combines full-volume tumor localization with refined region-of-interest (ROI) segmentation, enhanced by anatomically aware post-processing. The first-stage model generates a coarse prediction, followed by anatomically informed filtering based on lung overlap, proximity to lung surfaces, and component size. The resulting ROIs are segmented by a second-stage model trained with uncertainty-aware loss functions to improve accuracy and boundary calibration in ambiguous regions. Experiments on private and public datasets demonstrate improvements in Dice and Hausdorff scores, with fewer false positives and enhanced spatial interpretability. These results highlight the value of combining uncertainty modeling and anatomical priors in cascaded segmentation pipelines for robust and clinically meaningful tumor delineation. On the Orlando dataset, our framework improved Swin UNETR Dice from 0.4690 to 0.6447. Reduction in spurious components was strongly correlated with segmentation gains, underscoring the value of anatomically informed post-processing. 

**Abstract (ZH)**: 可靠的胸部分割在胸部计算机断层扫描（CT）中仍然具有挑战性，由于边界模糊、类别不平衡和解剖变异。我们提出了一种基于不确定性、从粗到细的分割框架，结合全体积肿瘤定位与细化区域of兴趣（ROI）分割，并通过解剖学意识的后续处理加以增强。首先，模型生成粗略预测，然后基于肺重叠、靠近肺表面的距离以及组件大小进行解剖学指导下的过滤。结果提取的ROIs由一个第二阶段模型进行分割，该模型使用不确定性感知损失函数进行训练，以提高在模糊区域的准确性和边界校准。在私有和公共数据集上的实验结果显示，Dice和Hausdorff分数有所提高，减少了假阳性，并增强了空间可 interpretability。这些结果突显了在级联分割管道中结合不确定性建模和解剖先验的价值，以实现稳健且临床上有意义的肿瘤界定。在Orlando数据集中，我们的框架将Swin UNETR的Dice分数从0.4690提高到0.6447。假分数量的减少与分割改进显著相关，突显了解剖学指导后续处理的价值。 

---
# Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement 

**Title (ZH)**: 学习物理知情的颜色感知变换以增强低光照图像 

**Authors**: Xingxing Yang, Jie Chen, Zaifeng Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.11896)  

**Abstract**: Image decomposition offers deep insights into the imaging factors of visual data and significantly enhances various advanced computer vision tasks. In this work, we introduce a novel approach to low-light image enhancement based on decomposed physics-informed priors. Existing methods that directly map low-light to normal-light images in the sRGB color space suffer from inconsistent color predictions and high sensitivity to spectral power distribution (SPD) variations, resulting in unstable performance under diverse lighting conditions. To address these challenges, we introduce a Physics-informed Color-aware Transform (PiCat), a learning-based framework that converts low-light images from the sRGB color space into deep illumination-invariant descriptors via our proposed Color-aware Transform (CAT). This transformation enables robust handling of complex lighting and SPD variations. Complementing this, we propose the Content-Noise Decomposition Network (CNDN), which refines the descriptor distributions to better align with well-lit conditions by mitigating noise and other distortions, thereby effectively restoring content representations to low-light images. The CAT and the CNDN collectively act as a physical prior, guiding the transformation process from low-light to normal-light domains. Our proposed PiCat framework demonstrates superior performance compared to state-of-the-art methods across five benchmark datasets. 

**Abstract (ZH)**: 基于分解物理先验的低光图像增强方法 

---
# Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting 

**Title (ZH)**: 基于结构不确定性建模和不准确GT深度拟合的现实世界深度恢复 

**Authors**: Delong Suzhang, Meng Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.11820)  

**Abstract**: The low-quality structure in raw depth maps is prevalent in real-world RGB-D datasets, which makes real-world depth recovery a critical task in recent years. However, the lack of paired raw-ground truth (raw-GT) data in the real world poses challenges for generalized depth recovery. Existing methods insufficiently consider the diversity of structure misalignment in raw depth maps, which leads to poor generalization in real-world depth recovery. Notably, random structure misalignments are not limited to raw depth data but also affect GT depth in real-world datasets. In the proposed method, we tackle the generalization problem from both input and output perspectives. For input, we enrich the diversity of structure misalignment in raw depth maps by designing a new raw depth generation pipeline, which helps the network avoid overfitting to a specific condition. Furthermore, a structure uncertainty module is designed to explicitly identify the misaligned structure for input raw depth maps to better generalize in unseen scenarios. Notably the well-trained depth foundation model (DFM) can help the structure uncertainty module estimate the structure uncertainty better. For output, a robust feature alignment module is designed to precisely align with the accurate structure of RGB images avoiding the interference of inaccurate GT depth. Extensive experiments on multiple datasets demonstrate the proposed method achieves competitive accuracy and generalization capabilities across various challenging raw depth maps. 

**Abstract (ZH)**: 原始深度图中的低质量结构在现实世界的RGB-D数据集中普遍存在，使得现实世界的深度恢复成为近年来的关键任务。然而，现实世界中缺乏配对的原始真实数据对（raw-GT）数据为通用深度恢复带来了挑战。现有方法未能充分考虑原始深度图中结构错位的多样性，导致在现实世界的深度恢复中泛化能力不足。值得注意的是，随机的结构错位不仅影响原始深度数据，还影响现实世界数据集中的真实深度数据。在所提出的方法中，我们从输入和输出两个方面解决泛化问题。对于输入，我们通过设计新的原始深度生成管道来丰富原始深度图中的结构错位多样性，帮助网络避免过度拟合特定条件。此外，我们设计了一种结构不确定性模块，明确识别输入原始深度图中的错位结构，从而更好地区分在未见过的场景中泛化。值得注意的是，经过充分训练的深度基础模型（DFM）可以帮助结构不确定性模块更好地估计结构不确定性。对于输出，我们设计了一种鲁棒的特征对齐模块，精确对齐RGB图像的准确结构，避免了不准确的真实深度信息的干扰。在多个数据集上的广泛实验表明，所提出的方法在各种挑战性的原始深度图中实现了竞争性的准确性和泛化能力。 

---
# GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision 

**Title (ZH)**: GrabS: 生成式 embodied 代理用于无场景监督的 3D 物体分割 

**Authors**: Zihui Zhang, Yafei Yang, Hongtao Wen, Bo Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.11754)  

**Abstract**: We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually limited to identifying simple objects like cars or their segmented objects are often inferior due to the lack of objectness in pretrained features. In this paper, we propose a new two-stage pipeline called GrabS. The core concept of our method is to learn generative and discriminative object-centric priors as a foundation from object datasets in the first stage, and then design an embodied agent to learn to discover multiple objects by querying against the pretrained generative priors in the second stage. We extensively evaluate our method on two real-world datasets and a newly created synthetic dataset, demonstrating remarkable segmentation performance, clearly surpassing all existing unsupervised methods. 

**Abstract (ZH)**: 我们研究在复杂点云中无需3D场景人工标注即进行3D物体分割的hard问题。通过依赖预训练2D特征的相似性或外部信号如运动来对3D点进行分组以形成物体，现有的无监督方法通常局限于识别简单的物体（如汽车），或者由于预训练特征缺乏物体特性而导致分割效果欠佳。在本文中，我们提出了一种新的两阶段管道方法称为GrabS。该方法的核心概念是在第一阶段从物体数据集中学习生成性和判别性的物体中心先验知识作为基础，并在第二阶段设计一种具身代理，通过查询预训练生成性先验知识来学习发现多个物体。我们在两个真实世界数据集和一个新创建的合成数据集上进行了广泛评估，展示了显著的分割性能，明显优于所有现有的无监督方法。 

---
# TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification 

**Title (ZH)**: TransitReID：具有遮挡抵抗的动态乘客再识别的公交OD数据采集 

**Authors**: Kaicong Huang, Talha Azfar, Jack Reilly, Ruimin Ke  

**Link**: [PDF](https://arxiv.org/pdf/2504.11500)  

**Abstract**: Transit Origin-Destination (OD) data are essential for transit planning, particularly in route optimization and demand-responsive paratransit systems. Traditional methods, such as manual surveys, are costly and inefficient, while Bluetooth and WiFi-based approaches require passengers to carry specific devices, limiting data coverage. On the other hand, most transit vehicles are equipped with onboard cameras for surveillance, offering an opportunity to repurpose them for edge-based OD data collection through visual person re-identification (ReID). However, such approaches face significant challenges, including severe occlusion and viewpoint variations in transit environments, which greatly reduce matching accuracy and hinder their adoption. Moreover, designing effective algorithms that can operate efficiently on edge devices remains an open challenge. To address these challenges, we propose TransitReID, a novel framework for individual-level transit OD data collection. TransitReID consists of two key components: (1) An occlusion-robust ReID algorithm featuring a variational autoencoder guided region-attention mechanism that adaptively focuses on visible body regions through reconstruction loss-optimized weight allocation; and (2) a Hierarchical Storage and Dynamic Matching (HSDM) mechanism specifically designed for efficient and robust transit OD matching which balances storage, speed, and accuracy. Additionally, a multi-threaded design supports near real-time operation on edge devices, which also ensuring privacy protection. We also introduce a ReID dataset tailored for complex bus environments to address the lack of relevant training data. Experimental results demonstrate that TransitReID achieves state-of-the-art performance in ReID tasks, with an accuracy of approximately 90\% in bus route simulations. 

**Abstract (ZH)**: 基于传输的起源-目的地（OD）数据对于传输规划至关重要，特别是在路线优化和响应式交通辅助系统中。传统的方法，如手工调查，成本高且效率低下，而基于Bluetooth和WiFi的方法需要乘客携带特定设备，限制了数据覆盖范围。另一方面，大多数公共交通车辆都配备了用于监控的车内摄像头，为通过视觉再识别（ReID）在边缘设备上重新利用这些摄像头进行OD数据收集提供了机会。然而，此类方法面临严重遮挡和视角变化等重大挑战，这大大降低了匹配精度，阻碍了其广泛应用。此外，设计能在边缘设备上高效运行的有效算法仍然是一个开放性挑战。为了解决这些挑战，我们提出了TransitReID，一种新颖的个体级别传输OD数据收集框架。TransitReID 包含两个关键组件：（1）一种鲁棒性的ReID算法，采用变分自编码器引导区域注意力机制，通过重建损失优化权重分配，自适应地关注可见的身体区域；（2）一种层次存储和动态匹配（HSDM）机制，专门设计用于高效稳健的传输OD匹配，平衡存储、速度和准确性。此外，多线程设计支持近实时操作，同时确保隐私保护。我们还引入了一个专门针对复杂公交环境的ReID数据集，以解决相关训练数据不足的问题。实验结果表明，TransitReID 在ReID任务中达到了最先进的性能，在公交路线模拟中准确率达到约90%。 

---
# Flux Already Knows - Activating Subject-Driven Image Generation without Training 

**Title (ZH)**: Flux 已经知道 - 无需训练的主体驱动图像生成 

**Authors**: Hao Kang, Stathi Fotiadis, Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Min Jin Chong, Xin Lu  

**Link**: [PDF](https://arxiv.org/pdf/2504.11478)  

**Abstract**: We propose a simple yet effective zero-shot framework for subject-driven image generation using a vanilla Flux model. By framing the task as grid-based image completion and simply replicating the subject image(s) in a mosaic layout, we activate strong identity-preserving capabilities without any additional data, training, or inference-time fine-tuning. This "free lunch" approach is further strengthened by a novel cascade attention design and meta prompting technique, boosting fidelity and versatility. Experimental results show that our method outperforms baselines across multiple key metrics in benchmarks and human preference studies, with trade-offs in certain aspects. Additionally, it supports diverse edits, including logo insertion, virtual try-on, and subject replacement or insertion. These results demonstrate that a pre-trained foundational text-to-image model can enable high-quality, resource-efficient subject-driven generation, opening new possibilities for lightweight customization in downstream applications. 

**Abstract (ZH)**: 基于vanilla Flux模型的简单有效零样本主题驱动图像生成框架 

---
# SO-DETR: Leveraging Dual-Domain Features and Knowledge Distillation for Small Object Detection 

**Title (ZH)**: SO-DETR：利用双域特征和知识蒸馏进行小目标检测 

**Authors**: Huaxiang Zhang, Hao Zhang, Aoran Mei, Zhongxue Gan, Guo-Niu Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2504.11470)  

**Abstract**: Detection Transformer-based methods have achieved significant advancements in general object detection. However, challenges remain in effectively detecting small objects. One key difficulty is that existing encoders struggle to efficiently fuse low-level features. Additionally, the query selection strategies are not effectively tailored for small objects. To address these challenges, this paper proposes an efficient model, Small Object Detection Transformer (SO-DETR). The model comprises three key components: a dual-domain hybrid encoder, an enhanced query selection mechanism, and a knowledge distillation strategy. The dual-domain hybrid encoder integrates spatial and frequency domains to fuse multi-scale features effectively. This approach enhances the representation of high-resolution features while maintaining relatively low computational overhead. The enhanced query selection mechanism optimizes query initialization by dynamically selecting high-scoring anchor boxes using expanded IoU, thereby improving the allocation of query resources. Furthermore, by incorporating a lightweight backbone network and implementing a knowledge distillation strategy, we develop an efficient detector for small objects. Experimental results on the VisDrone-2019-DET and UAVVaste datasets demonstrate that SO-DETR outperforms existing methods with similar computational demands. The project page is available at this https URL. 

**Abstract (ZH)**: 基于检测变压器的方法在通用目标检测中取得了显著进展，但在有效检测小型目标方面仍面临挑战。现有编码器难以高效融合低级特征，同时查询选择策略也不适合小型目标。为解决这些挑战，本文提出了一种高效的模型——小型对象检测变压器（SO-DETR）。该模型包含三个关键组成部分：双域混合编码器、增强的查询选择机制以及知识蒸馏策略。双域混合编码器将空间域和频域结合，有效融合多尺度特征，增强高分辨率特征的表示能力，同时保持较低的计算开销。增强的查询选择机制通过动态选择扩展IoU的高分锚框来优化查询初始化，从而提高查询资源的分配。此外，结合轻量级骨干网络并实施知识蒸馏策略，我们开发了一种针对小型目标的高效检测器。实验结果表明，SO-DETR在VisDrone-2019-DET和UAVDataset数据集上的性能优于同类方法。更多信息请参见该项目页面：这个 <https://> 页面。 

---
# Do Segmentation Models Understand Vascular Structure? A Blob-Based XAI Framework 

**Title (ZH)**: 基于斑点的解释性人工智能框架：分割模型理解血管结构吗？ 

**Authors**: Guillaume Garret, Antoine Vacavant, Carole Frindel  

**Link**: [PDF](https://arxiv.org/pdf/2504.11469)  

**Abstract**: Deep learning models have achieved impressive performance in medical image segmentation, yet their black-box nature limits clinical adoption. In vascular applications, trustworthy segmentation should rely on both local image cues and global anatomical structures, such as vessel connectivity or branching. However, the extent to which models leverage such global context remains unclear. We present a novel explainability pipeline for 3D vessel segmentation, combining gradient-based attribution with graph-guided point selection and a blob-based analysis of Saliency maps. Using vascular graphs extracted from ground truth, we define anatomically meaningful points of interest (POIs) and assess the contribution of input voxels via Saliency maps. These are analyzed at both global and local scales using a custom blob detector. Applied to IRCAD and Bullitt datasets, our analysis shows that model decisions are dominated by highly localized attribution blobs centered near POIs. Attribution features show little correlation with vessel-level properties such as thickness, tubularity, or connectivity -- suggesting limited use of global anatomical reasoning. Our results underline the importance of structured explainability tools and highlight the current limitations of segmentation models in capturing global vascular context. 

**Abstract (ZH)**: 深度学习模型在医学图像分割中取得了显著性能，但其黑盒特性限制了临床应用。在血管应用中，可靠的分割应该依赖于局部图像线索和全局解剖结构，如血管连通性或分支。然而，模型利用这种全局上下文的程度尚不清楚。我们提出了一种新的3D血管分割可解释性管道，结合基于梯度的归因、图导向点选择和基于斑点的Saliency图分析。通过从ground truth提取的血管图，我们定义了解剖学上有意义的兴趣点（POIs），并通过Saliency图评估输入体素的贡献。这些Saliency图在全局和局部尺度上使用自定义的斑点检测器进行分析。在IRCAD和Bullitt数据集上应用我们的分析显示，模型决策主要由靠近POIs的高局部归因斑点主导。归因特征与血管级别属性（如厚度、管状性或连通性）的相关性较小——这表明模型在利用全局解剖推理方面有限。我们的结果强调了结构化可解释性工具的重要性，并突显了分割模型在捕捉全局血管上下文方面的现有限制。 

---
