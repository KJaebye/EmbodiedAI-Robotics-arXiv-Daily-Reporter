{'arxiv_id': 'arXiv:2504.12299', 'title': 'Adapting a World Model for Trajectory Following in a 3D Game', 'authors': 'Marko Tot, Shu Ishida, Abdelhak Lemkhenter, David Bignell, Pallavi Choudhury, Chris Lovett, Luis França, Matheus Ribeiro Furtado de Mendonça, Tarun Gupta, Darren Gehring, Sam Devlin, Sergio Valcarcel Macua, Raluca Georgescu', 'link': 'https://arxiv.org/abs/2504.12299', 'abstract': "Imitation learning is a powerful tool for training agents by leveraging expert knowledge, and being able to replicate a given trajectory is an integral part of it. In complex environments, like modern 3D video games, distribution shift and stochasticity necessitate robust approaches beyond simple action replay. In this study, we apply Inverse Dynamics Models (IDM) with different encoders and policy heads to trajectory following in a modern 3D video game -- Bleeding Edge. Additionally, we investigate several future alignment strategies that address the distribution shift caused by the aleatoric uncertainty and imperfections of the agent. We measure both the trajectory deviation distance and the first significant deviation point between the reference and the agent's trajectory and show that the optimal configuration depends on the chosen setting. Our results show that in a diverse data setting, a GPT-style policy head with an encoder trained from scratch performs the best, DINOv2 encoder with the GPT-style policy head gives the best results in the low data regime, and both GPT-style and MLP-style policy heads had comparable results when pre-trained on a diverse setting and fine-tuned for a specific behaviour setting.", 'abstract_zh': '模仿学习是一种利用专家知识训练代理的强大工具，复制给定轨迹是其核心组成部分。在现代3D视频游戏等复杂环境中，分布偏移和随机性需要超越简单的动作回放的稳健方法。在此研究中，我们应用不同编码器和策略头的逆动力学模型（IDM）在现代3D视频游戏——Bleeding Edge中进行轨迹跟随，并探讨了几种未来对齐策略，以解决由偶然不确定性引起的分布偏移和代理的不完美性。我们测量参考轨迹和代理轨迹之间的轨迹偏差距离及首次显著偏差点，并表明最优配置取决于所选设置。结果显示，在多样数据集设置中，从头训练的编码器与GPT风格的策略头相结合的配置效果最好；在数据有限的情况下，DINOv2编码器与GPT风格的策略头的效果最佳；当在多样数据集上预训练并在特定行为设置下微调时，GPT风格和MLP风格的策略头表现出类似的效果。', 'title_zh': '适配于3D游戏轨迹跟随的世界模型'}
{'arxiv_id': 'arXiv:2504.12254', 'title': 'Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning', 'authors': 'Mahmoud Salhab, Marwan Elghitany, Shameed Sait, Syed Sibghat Ullah, Mohammad Abusheikh, Hasan Abusheikh', 'link': 'https://arxiv.org/abs/2504.12254', 'abstract': 'Automatic speech recognition (ASR) is crucial for human-machine interaction in diverse applications like conversational agents, industrial robotics, call center automation, and automated subtitling. However, developing high-performance ASR models remains challenging, particularly for low-resource languages like Arabic, due to the scarcity of large, labeled speech datasets, which are costly and labor-intensive to produce. In this work, we employ weakly supervised learning to train an Arabic ASR model using the Conformer architecture. Our model is trained from scratch on 15,000 hours of weakly annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal Arabic (DA), eliminating the need for costly manual transcriptions. Despite the absence of human-verified labels, our approach attains state-of-the-art (SOTA) performance, exceeding all previous efforts in the field of Arabic ASR on the standard benchmarks. By demonstrating the effectiveness of weak supervision as a scalable, cost-efficient alternative to traditional supervised approaches, paving the way for improved ASR systems in low resource settings.', 'abstract_zh': '自动语音识别（ASR）在对话代理、工业机器人、呼叫中心自动化和自动字幕后等多样应用中的交互至关重要。然而，开发高性能的ASR模型对于低资源语言如阿拉伯语来说仍然具有挑战性，特别是由于缺乏大规模、标注的数据集，这些数据集的获取既昂贵又劳动密集。在本工作中，我们运用弱监督学习训练一种基于Conformer架构的阿拉伯语ASR模型，该模型完全从15,000小时的弱标注语音数据中进行训练，涵盖现代标准阿拉伯语（MSA）和方言阿拉伯语（DA），从而避免了昂贵的手工转录。尽管缺乏人工验证的标签，我们的方法在阿拉伯语ASR领域达到了最先进的（SOTA）性能，超过了所有以往的努力，特别是在标准基准上的表现。通过证明弱监督作为传统监督方法的一种可扩展且经济有效的替代方案的有效性，为低资源环境下的改进ASR系统铺平了道路。', 'title_zh': '通过大规模弱监督学习推动阿拉伯语音识别进步'}
{'arxiv_id': 'arXiv:2504.12110', 'title': 'Towards LLM Agents for Earth Observation', 'authors': 'Chia Hsiang Kao, Wenting Zhao, Shreelekha Revankar, Samuel Speas, Snehal Bhagat, Rajeev Datta, Cheng Perng Phoo, Utkarsh Mall, Carl Vondrick, Kavita Bala, Bharath Hariharan', 'link': 'https://arxiv.org/abs/2504.12110', 'abstract': 'Earth Observation (EO) provides critical planetary data for environmental monitoring, disaster management, climate science, and other scientific domains. Here we ask: Are AI systems ready for reliable Earth Observation? We introduce \\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth Observatory articles across 13 topics and 17 satellite sensors. Using Google Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33% because the code fails to run over 58% of the time. We improve the failure rate for open models by fine-tuning synthetic data, allowing much smaller models (Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g., DeepSeek-R1). Taken together, our findings identify significant challenges to be solved before AI agents can automate earth observation, and suggest paths forward. The project page is available at this https URL.', 'abstract_zh': 'AI系统准备好应对可靠的地球观测了吗？', 'title_zh': '面向地球观测的大型语言模型代理'}
{'arxiv_id': 'arXiv:2504.12090', 'title': 'Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A Memory-Augmented, Multi-Step Decision Framework', 'authors': 'Jack Preuveneers, Joseph Ternasky, Fuat Alican, Yigit Ihlamur', 'link': 'https://arxiv.org/abs/2504.12090', 'abstract': 'We present a novel framework that bridges the gap between the interpretability of decision trees and the advanced reasoning capabilities of large language models (LLMs) to predict startup success. Our approach leverages chain-of-thought prompting to generate detailed reasoning logs, which are subsequently distilled into structured, human-understandable logical rules. The pipeline integrates multiple enhancements - efficient data ingestion, a two-step refinement process, ensemble candidate sampling, simulated reinforcement learning scoring, and persistent memory - to ensure both stable decision-making and transparent output. Experimental evaluations on curated startup datasets demonstrate that our combined pipeline improves precision by 54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a standalone OpenAI o3 model. Notably, our model achieves over 2x the precision of a random classifier (16%). By combining state-of-the-art AI reasoning with explicit rule-based explanations, our method not only augments traditional decision-making processes but also facilitates expert intervention and continuous policy refinement. This work lays the foundation for the implementation of interpretable LLM-powered decision frameworks in high-stakes investment environments and other domains that require transparent and data-driven insights.', 'abstract_zh': '我们提出了一种新型框架，该框架弥合了决策树可解释性与大型语言模型（LLMs）高级推理能力之间的差距，用于预测初创公司成功。我们的方法利用链式思考提示生成详细的推理日志，随后将其提炼为结构化的人类可理解的逻辑规则。该流水线集成了多项增强——高效数据摄入、两步精炼过程、候选集采样、模拟强化学习评分和持久化内存，以确保决策的稳定性和输出的透明性。对精心挑选的初创公司数据集的实验评估表明，与独立的OpenAI o3模型相比，我们的综合流水线在精确度上提高了54%，从0.225提高到0.346，在准确度上提高了50%，从0.46提高到0.70。值得注意的是，我们的模型的精确度是随机分类器的2倍多（16%）。通过结合最先进的AI推理与明确的规则解释，我们的方法不仅增强了传统的决策过程，还促进了专家介入和连续政策改进。本研究为在高风险投资环境中实施具备解释性的LLM驱动决策框架及其他需要透明和数据驱动洞察的领域奠定了基础。', 'title_zh': '基于推理的初创企业评估人工智能（R.A.I.S.E.）：一种记忆增强的多步决策框架'}
{'arxiv_id': 'arXiv:2504.12012', 'title': 'Purposefully Induced Psychosis (PIP): Embracing Hallucination as Imagination in Large Language Models', 'authors': 'Kris Pilcher, Esen K. Tütüncü', 'link': 'https://arxiv.org/abs/2504.12012', 'abstract': 'Hallucinations in Large Language Models (LLMs) are widely regarded as errors - outputs that deviate from factual accuracy. However, in creative or exploratory contexts, these "mistakes" may represent unexpected avenues for innovation. We introduce Purposefully Induced Psychosis (PIP), a novel approach that amplifies LLM hallucinations for imaginative tasks such as speculative fiction, interactive storytelling, and mixed-reality simulations. Drawing on Herman Melville\'s Moby-Dick, where Pip\'s "madness" reveals profound insight, we reframe hallucinations as a source of computational imagination rather than a flaw. Our method fine-tunes LLMs to encourage speculative, metaphorical, and surreal outputs - hallucinations that are useful when factual accuracy is not the chief objective. Inspired by the consensual illusions of theater and stage magic, PIP situates these creative missteps in contexts where users willingly suspend disbelief, thereby transforming "errors" into catalysts for new ways of thinking. We discuss potential applications, design principles for ensuring user consent, preliminary observations, and implications for broader AI ethics and human-AI collaboration.', 'abstract_zh': 'Hallucinations in Large Language Models (LLMs) 作为广泛认为的事实偏差错误，在创造性和探索性背景下，这些“错误”可能代表了创新的未预见途径。我们引入了有意引发精神病态（PIP）的新方法，旨在放大LLM的幻觉用于诸如科幻创作、互动叙事和混合现实模拟等想象任务。借鉴赫尔曼·梅尔维尔的《白鲸记》，PIP将幻觉重新定义为计算性想象力的来源而非缺陷。该方法通过微调LLM以鼓励推测性、比喻性和超现实的输出，在事实准确性不是主要目标时，这些幻觉具有 usefulness。受到剧场和舞台魔术合意幻觉的启发，PIP将这些创造性的失误置于用户愿意暂时信以为真的情境中，从而将“错误”转化为思维新方式的催化剂。我们讨论了潜在应用、确保用户同意的设计原则、初步观察结果以及更广泛的人工智能伦理和人机协作的含义。', 'title_zh': '故意诱导的精神错乱（PIP）：在大型语言模型中拥抱幻觉为想象'}
{'arxiv_id': 'arXiv:2504.11977', 'title': 'Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews', 'authors': 'Sofia Krylova, Fabian Schmidt, Vladimir Vlassov', 'link': 'https://arxiv.org/abs/2504.11977', 'abstract': 'Many existing digital triage systems are questionnaire-based, guiding patients to appropriate care levels based on information (e.g., symptoms, medical history, and urgency) provided by the patients answering questionnaires. Such a system often uses a deterministic model with predefined rules to determine care levels. It faces challenges with incomplete triage interviews since it can only assist patients who finish the process. In this study, we explore the use of machine learning (ML) to predict outcomes of unfinished interviews, aiming to enhance patient care and service quality. Predicting triage outcomes from incomplete data is crucial for patient safety and healthcare efficiency. Our findings show that decision-tree models, particularly LGBMClassifier and CatBoostClassifier, achieve over 80\\% accuracy in predicting outcomes from complete interviews while having a linear correlation between the prediction accuracy and interview completeness degree. For example, LGBMClassifier achieves 88,2\\% prediction accuracy for interviews with 100\\% completeness, 79,6\\% accuracy for interviews with 80\\% completeness, 58,9\\% accuracy for 60\\% completeness, and 45,7\\% accuracy for 40\\% completeness. The TabTransformer model demonstrated exceptional accuracy of over 80\\% for all degrees of completeness but required extensive training time, indicating a need for more powerful computational resources. The study highlights the linear correlation between interview completeness and predictive power of the decision-tree models.', 'abstract_zh': '基于机器学习的不完整转诊访谈预测研究', 'title_zh': '利用机器学习模型预测数字医疗分流面试的结果'}
{'arxiv_id': 'arXiv:2504.11942', 'title': 'ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation', 'authors': 'Nada Shahin, Leila Ismail', 'link': 'https://arxiv.org/abs/2504.11942', 'abstract': 'Current sign language machine translation systems rely on recognizing hand movements, facial expressions and body postures, and natural language processing, to convert signs into text. Recent approaches use Transformer architectures to model long-range dependencies via positional encoding. However, they lack accuracy in recognizing fine-grained, short-range temporal dependencies between gestures captured at high frame rates. Moreover, their high computational complexity leads to inefficient training. To mitigate these issues, we propose an Adaptive Transformer (ADAT), which incorporates components for enhanced feature extraction and adaptive feature weighting through a gating mechanism to emphasize contextually relevant features while reducing training overhead and maintaining translation accuracy. To evaluate ADAT, we introduce MedASL, the first public medical American Sign Language dataset. In sign-to-gloss-to-text experiments, ADAT outperforms the encoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing training time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text experiments, it improves accuracy by 8.7% and reduces training time by 2.8% on PHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on MedASL. Compared to encoder-only and decoder-only baselines in sign-to-text, ADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its dual-stream structure.', 'abstract_zh': '当前的手语机器翻译系统依赖于识别手势、面部表情和身体姿态，并结合自然语言处理技术，将手语转换为文本。最近的方法使用Transformer架构通过位置编码建模长距离依赖关系，但它们在识别高帧率捕获的手势之间细微的、短距离的时序依赖关系方面缺乏准确性。此外，它们的高计算复杂性导致训练效率低下。为了缓解这些问题，我们提出了一种自适应Transformer（ADAT），它结合了增强特征提取和通过门控机制实现的自适应特征加权组件，强调上下文相关特征，同时减少训练开销并保持翻译准确性。为了评估ADAT，我们引入了MedASL，这是首个公开的医疗美式手语数据集。在手语到图解再到文本的实验中，ADAT在PHOENIX14T上的BLEU-4准确性提高了0.1%，训练时间减少了14.33%，在MedASL上的训练时间减少了3.24%。在手语到文本的实验中，ADAT在PHOENIX14T上的准确性提高了8.7%，训练时间减少了2.8%，在MedASL上的准确性提高了4.7%，训练速度加快了7.17%。与手语到文本中的编码器和解码器基线相比，尽管由于其双流结构导致速度慢了12.1%，但ADAT至少在准确性上提高了6.8%。', 'title_zh': 'ADAT：时间序列意识的自适应变换器架构的手语翻译'}
{'arxiv_id': 'arXiv:2504.11919', 'title': 'Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading', 'authors': 'Qianjin Yu, Keyu Wu, Zihan Chen, Chushu Zhang, Manlin Mei, Lingjun Huang, Fang Tan, Yongsheng Du, Kunlin Liu, Yurui Zhu', 'link': 'https://arxiv.org/abs/2504.11919', 'abstract': 'Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its excellent reasoning ability in complex tasks and has publiclyshared its methodology. This provides potentially high-quality chain-of-thought (CoT) data for stimulating the reasoning abilities of small-sized large language models (LLMs). To generate high-quality CoT data for different LLMs, we seek an efficient method for generating high-quality CoT data with LLM-Adaptive questiondifficulty levels. First, we grade the difficulty of the questions according to the reasoning ability of the LLMs themselves and construct a LLM-Adaptive question database. Second, we sample the problem database based on a distribution of difficulty levels of the questions and then use DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality CoT data with correct answers. Thanks to the construction of CoT data with LLM-Adaptive difficulty levels, we have significantly reduced the cost of data generation and enhanced the efficiency of model supervised fine-tuning (SFT). Finally, we have validated the effectiveness and generalizability of the proposed method in the fields of complex mathematical competitions and code generation tasks. Notably, with only 2k high-quality mathematical CoT data, our ZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly, with only 2k high-quality code CoT data, our ZCode-32B surpasses DeepSeek-Distill-32B in code reasoning tasks.', 'abstract_zh': '近期，DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) 在复杂任务中展示了其出色的理由推理能力，并公开分享了其方法论。这为激发小型大型语言模型（LLMs）的理由推理能力提供了潜在高质量的链式推理（CoT）数据。为了为不同LLMs生成高质量CoT数据，我们寻求一种生成具有LLM自适应问题难度级别的高质量CoT数据的有效方法。首先，我们根据LLMs自身的推理能力对其问题的难度进行分级，并构建一个LLM自适应问题数据库。其次，我们基于问题难度分布对问题数据库进行采样，然后使用DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) 生成相应的高质量CoT数据并给出正确答案。通过构建具有LLM自适应难度级别的CoT数据，我们显著降低了数据生成的成本，并提升了模型监督微调（SFT）的效率。最后，我们验证了所提出方法在复杂数学竞赛和代码生成任务中的有效性和通用性。值得注意的是，仅使用2k高质量数学CoT数据，我们的ZMath-32B在数学推理任务中超过了DeepSeek-Distill-32B；同样，仅使用2k高质量代码CoT数据，我们的ZCode-32B在代码推理任务中超过了DeepSeek-Distill-32B。', 'title_zh': '从LLM自适应问题难度分级的角度重新思考高质量CoT数据的生成'}
{'arxiv_id': 'arXiv:2504.11882', 'title': 'Seeking and leveraging alternative variable dependency concepts in gray-box-elusive bimodal land-use allocation problems', 'authors': 'J. Maciążek, M. W. Przewozniczek, J. Schwaab', 'link': 'https://arxiv.org/abs/2504.11882', 'abstract': 'Solving land-use allocation problems can help us to deal with some of the most urgent global environmental issues. Since these problems are NP-hard, effective optimizers are needed to handle them. The knowledge about variable dependencies allows for proposing such tools. However, in this work, we consider a real-world multi-objective problem for which standard variable dependency discovery techniques are inapplicable. Therefore, using linkage-based variation operators is unreachable. To address this issue, we propose a definition of problem-dedicated variable dependency. On this base, we propose obtaining masks of dependent variables. Using them, we construct three novel crossover operators. The results concerning real-world test cases show that introducing our propositions into two well-known optimizers (NSGA-II, MOEA/D) dedicated to multi-objective optimization significantly improves their effectiveness.', 'abstract_zh': '解决土地利用分配问题有助于应对一些最紧迫的全球环境问题。由于这些问题属于NP-hard问题，需要有效的优化器来处理它们。变量依赖性的知识有助于提出这样的工具。然而，在这项工作中，我们考虑了一个现实世界中的多目标问题，对于该问题，标准的变量依赖性发现技术是不适用的。因此，基于链接的变异操作是不可行的。为解决这一问题，我们提出了一种针对特定问题的变量依赖性的定义，并在此基础上提出了获取依赖变量掩码的方法。利用这些掩码，我们构建了三种新型交叉操作。关于实际案例的结果表明，将我们的提议引入两个著名的多目标优化器（NSGA-II，MOEA/D）中，显著提高了它们的有效性。', 'title_zh': '在灰箱逃逸的双模土地利用分配问题中寻求和利用替代变量依赖概念'}
{'arxiv_id': 'arXiv:2504.11864', 'title': 'Moving between high-quality optima using multi-satisfiability characteristics in hard-to-solve Max3Sat instances', 'authors': 'J. Piatek, M. W. Przewozniczek, F. Chicano, R. Tinós', 'link': 'https://arxiv.org/abs/2504.11864', 'abstract': 'Gray-box optimization proposes effective and efficient optimizers of general use. To this end, it leverages information about variable dependencies and the subfunction-based problem representation. These approaches were already shown effective by enabling \\textit{tunnelling} between local optima even if these moves require the modification of many dependent variables. Tunnelling is useful in solving the maximum satisfiability problem (MaxSat), which can be reformulated to Max3Sat. Since many real-world problems can be brought to solving the MaxSat/Max3Sat instances, it is important to solve them effectively and efficiently. Therefore, we focus on Max3Sat instances for which tunnelling fails to introduce improving moves between locally optimal high-quality solutions and the region of globally optimal solutions. We analyze the features of such instances on the ground of phase transitions. Based on these observations, we propose manipulating clause-satisfiability characteristics that allow connecting high-quality solutions distant in the solution space. We utilize multi-satisfiability characteristics in the optimizer built from typical gray-box mechanisms. The experimental study shows that the proposed optimizer can solve those Max3Sat instances that are out of the grasp of state-of-the-art gray-box optimizers. At the same time, it remains effective for instances that have already been successfully solved by gray-box.', 'abstract_zh': '灰箱优化提出了一类通用且有效的优化器。通过利用变量间依赖关系以及基于子函数的问题表示，这些方法已被证明即使涉及大量依赖变量的修改也能有效实现从局部最优解到高质量全局最优解区域的连接（tunnelling）。鉴于许多实际问题可以归约为求解Max3Sat实例，实现这些实例的有效且高效的求解显得尤为重要。因此，我们关注tunnelling无法在高质局部最优解与全局最优解区域之间引入改进策略的Max3Sat实例。基于相变现象分析这些实例的特性，并提出利用子句满足性特征以连接在解空间中相距较远的高质解。我们在基于典型灰箱机制构建的优化器中利用多满足性特征。实验研究显示，所提出的方法能够解决当前最先进灰箱优化器难以解决的Max3Sat实例，同时对于已由灰箱优化器成功解决的实例依然保持有效。', 'title_zh': '在难解的Max3Sat实例中利用多重满足性特性在高质量最优解之间移动'}
{'arxiv_id': 'arXiv:2504.11844', 'title': 'Evaluating the Goal-Directedness of Large Language Models', 'authors': 'Tom Everitt, Cristina Garbacea, Alexis Bellot, Jonathan Richens, Henry Papadatos, Siméon Campos, Rohin Shah', 'link': 'https://arxiv.org/abs/2504.11844', 'abstract': "To what extent do LLMs use their capabilities towards their given goal? We take this as a measure of their goal-directedness. We evaluate goal-directedness on tasks that require information gathering, cognitive effort, and plan execution, where we use subtasks to infer each model's relevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI, and Anthropic show that goal-directedness is relatively consistent across tasks, differs from task performance, and is only moderately sensitive to motivational prompts. Notably, most models are not fully goal-directed. We hope our goal-directedness evaluations will enable better monitoring of LLM progress, and enable more deliberate design choices of agentic properties in LLMs.", 'abstract_zh': 'LLMs在完成给定目标时使用其能力的程度：一种目标导向性的评估', 'title_zh': '评估大型语言模型的目的导向性'}
{'arxiv_id': 'arXiv:2504.11792', 'title': 'Large Language Models for Drug Overdose Prediction from Longitudinal Medical Records', 'authors': 'Md Sultan Al Nahian, Chris Delcher, Daniel Harris, Peter Akpunonu, Ramakanth Kavuluru', 'link': 'https://arxiv.org/abs/2504.11792', 'abstract': "The ability to predict drug overdose risk from a patient's medical records is crucial for timely intervention and prevention. Traditional machine learning models have shown promise in analyzing longitudinal medical records for this task. However, recent advancements in large language models (LLMs) offer an opportunity to enhance prediction performance by leveraging their ability to process long textual data and their inherent prior knowledge across diverse tasks. In this study, we assess the effectiveness of Open AI's GPT-4o LLM in predicting drug overdose events using patients' longitudinal insurance claims records. We evaluate its performance in both fine-tuned and zero-shot settings, comparing them to strong traditional machine learning methods as baselines. Our results show that LLMs not only outperform traditional models in certain settings but can also predict overdose risk in a zero-shot setting without task-specific training. These findings highlight the potential of LLMs in clinical decision support, particularly for drug overdose risk prediction.", 'abstract_zh': '基于患者医疗记录预测药物过量风险的能力对于及时干预和预防至关重要。传统的机器学习模型在通过分析纵向医疗记录进行此任务时展现出了潜力。然而，近期大型语言模型（LLMs）的发展提供了通过利用其处理长文本数据的能力和跨任务的潜在先验知识来提升预测性能的机会。在本研究中，我们评估了Open AI的GPT-4o LLM在使用患者纵向保险索赔记录预测药物过量事件方面的有效性。我们评估了其在微调和零样本设置中的性能，并将其与强的传统机器学习方法进行了基线比较。研究结果表明，LLMs不仅在某些设置中优于传统模型，还能够在无需特定任务训练的情况下预测过量风险。这些发现强调了LLMs在临床决策支持，特别是药物过量风险预测方面的潜在价值。', 'title_zh': '基于 longitudinal 医疗记录的大型语言模型在药物过量预测中的应用'}
{'arxiv_id': 'arXiv:2504.11765', 'title': 'Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs', 'authors': 'Hyungwoo Lee, Kihyun Kim, Jinwoo Kim, Jungmin So, Myung-Hoon Cha, Hong-Yeon Kim, James J. Kim, Youngjae Kim', 'link': 'https://arxiv.org/abs/2504.11765', 'abstract': 'Recent large language models (LLMs) face increasing inference latency as input context length and model size continue to grow. In particular, the retrieval-augmented generation (RAG) technique, which enhances LLM responses by incorporating external knowledge, exacerbates this issue by significantly increasing the number of input tokens. This expansion in token length leads to a substantial rise in computational overhead, particularly during the prefill stage, resulting in prolonged time-to-first-token (TTFT). To address this issue, this paper proposes a method to reduce TTFT by leveraging a disk-based key-value (KV) cache to lessen the computational burden during the prefill stage. We also introduce a disk-based shared KV cache management system, called Shared RAG-DCache, for multi-instance LLM RAG service environments. This system, together with an optimal system configuration, improves both throughput and latency under given resource constraints. Shared RAG-DCache exploits the locality of documents related to user queries in RAG, as well as the queueing delay in LLM inference services. It proactively generates and stores disk KV caches for query-related documents and shares them across multiple LLM instances to enhance inference performance. In experiments on a single host equipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in throughput and up to a 12~65% reduction in latency, depending on the resource configuration.', 'abstract_zh': 'Recent大型语言模型（LLMs）在输入上下文长度和模型规模持续增长的情况下面临推理延迟增加的问题。特别是检索增强生成（RAG）技术通过引入外部知识来增强LLM的响应，这极大地增加了输入令牌的数量，从而导致在预填充阶段的计算开销显著增加，特别是在预填充阶段导致首次令牌时间（TTFT）延长。为了解决这一问题，本文提出了一种方法，通过利用基于磁盘的键值（KV）缓存来减轻预填充阶段的计算负担。我们还为多实例LLM RAG服务环境引入了一种基于磁盘的共享键值缓存管理系统，称为Shared RAG-DCache。该系统结合最优系统配置，能够在给定的资源约束条件下提高吞吐量和降低延迟。Shared RAG-DCache利用与用户查询相关的文档的局部性以及LLM推理服务中的排队延迟，主动为查询相关的文档生成并存储磁盘KV缓存，并且跨多个LLM实例共享，以增强推理性能。在配备2块GPU和1块CPU的单机实验中，Shared RAG-DCache在不同的资源配置下，吞吐量提高了15~71%，并且延迟减少了12~65%。', 'title_zh': '基于共享磁盘的KV缓存管理以提高RAG增强的大语言模型多实例推理效率'}
{'arxiv_id': 'arXiv:2504.11741', 'title': "Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?", 'authors': 'Yiyou Sun, Georgia Zhou, Hao Wang, Dacheng Li, Nouha Dziri, Dawn Song', 'link': 'https://arxiv.org/abs/2504.11741', 'abstract': "Recent supervised fine-tuning (SFT) approaches have significantly improved language models' performance on mathematical reasoning tasks, even when models are trained at a small scale. However, the specific capabilities enhanced through such fine-tuning remain poorly understood. In this paper, we conduct a detailed analysis of model performance on the AIME24 dataset to understand how reasoning capabilities evolve. We discover a ladder-like structure in problem difficulty, categorize questions into four tiers (Easy, Medium, Hard, and Extremely Hard (Exh)), and identify the specific requirements for advancing between tiers. We find that progression from Easy to Medium tier requires adopting an R1 reasoning style with minimal SFT (500-1K instances), while Hard-level questions suffer from frequent model's errors at each step of the reasoning chain, with accuracy plateauing at around 65% despite logarithmic scaling. Exh-level questions present a fundamentally different challenge; they require unconventional problem-solving skills that current models uniformly struggle with. Additional findings reveal that carefully curated small-scale datasets offer limited advantage-scaling dataset size proves far more effective. Our analysis provides a clearer roadmap for advancing language model capabilities in mathematical reasoning.", 'abstract_zh': 'Recent supervised fine-tuning (SFT) 方法显著提升了小型训练规模的语言模型在数学推理任务上的性能，但这些方法具体增强的能力仍然了解不足。本文通过对AIME24数据集的详细分析，研究推理能力如何演变。我们发现了一种梯级结构的问题难度模式，将问题分为四个层级（简单、中等、困难和极困难），并确定了不同层级之间的具体要求。我们发现，从简单层级到中等层级的进步需要采用少量的SFT（500-1K实例）的R1推理风格，而困难层级的问题在推理链的每一步都频繁出错，即使在对数扩展的情况下准确率也仅在65%左右停滞不前。极困难层级的问题提出了完全不同类型的挑战；它们需要当前模型难以掌握的非传统问题解决技能。我们还发现，精心策划的小规模数据集提供的优势有限，扩展数据集规模的效果要显著得多。我们的分析为提升语言模型在数学推理方面的能力提供了更清晰的路径。', 'title_zh': '攀登推理阶梯：SFT之后LLM能解决和仍不能解决的问题'}
{'arxiv_id': 'arXiv:2504.11704', 'title': 'A Library of LLM Intrinsics for Retrieval-Augmented Generation', 'authors': 'Marina Danilevsky, Kristjan Greenewald, Chulaka Gunasekara, Maeda Hanafi, Lihong He, Yannis Katsis, Krishnateja Killamsetty, Yatin Nandwani, Lucian Popa, Dinesh Raghu, Frederick Reiss, Vraj Shah, Khoi-Nguyen Tran, Huaiyu Zhu, Luis Lastras', 'link': 'https://arxiv.org/abs/2504.11704', 'abstract': 'In the developer community for large language models (LLMs), there is not yet a clean pattern analogous to a software library, to support very large scale collaboration. Even for the commonplace use case of Retrieval-Augmented Generation (RAG), it is not currently possible to write a RAG application against a well-defined set of APIs that are agreed upon by different LLM providers. Inspired by the idea of compiler intrinsics, we propose some elements of such a concept through introducing a library of LLM Intrinsics for RAG. An LLM intrinsic is defined as a capability that can be invoked through a well-defined API that is reasonably stable and independent of how the LLM intrinsic itself is implemented. The intrinsics in our library are released as LoRA adapters on HuggingFace, and through a software interface with clear structured input/output characteristics on top of vLLM as an inference platform, accompanied in both places with documentation and code. This article describes the intended usage, training details, and evaluations for each intrinsic, as well as compositions of multiple intrinsics.', 'abstract_zh': '在大规模语言模型开发者社区中，尚未出现一个类似于软件库的清晰模式，以支持大规模协作。即使对于检索增强生成（RAG）这一常见的应用场景，目前也无法编写一个基于不同大规模语言模型提供者达成共识的API集的RAG应用程序。受编译器本征函数的启发，我们通过引入大规模语言模型本征库来提出这一概念的一部分。大规模语言模型本征被定义为一种可以通过稳定且独立于其实现方式的API调用的能力。我们的库中的本征以LoRA适配器的形式在HuggingFace上发布，并在vLLM推理平台上通过一个具有明确结构化输入/输出特性的软件接口提供，两个地方都附有文档和代码。本文描述了每个本征的预期用法、训练细节和评估，以及多个本征的组合。', 'title_zh': 'LLM内在机制库用于检索增强生成'}
{'arxiv_id': 'arXiv:2504.11671', 'title': "Steering Prosocial AI Agents: Computational Basis of LLM's Decision Making in Social Simulation", 'authors': 'Ji Ma', 'link': 'https://arxiv.org/abs/2504.11671', 'abstract': "Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game -- a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications.", 'abstract_zh': '大型语言模型（LLMs）在社会科学研究和实际应用中 increasingly 作为类人决策代理使用。这些LLM代理通常被赋予类人的角色，并置于现实情境中。然而，这些角色和情境如何影响LLM的行为仍缺乏探索。本研究提出并测试了在博弈论中的“分配者游戏”中探测、量化和修改LLM内部表示的方法，以探究公平性和利他行为。我们从LLM的内部状态中提取“变量变化向量”（例如，“男性”变“女性”）。在模型推理过程中操纵这些向量可以显著改变这些变量与模型决策之间的关系。这种方法提供了一种原则性的方法，用于研究和调控社会概念如何在变换器模型中编码和设计，对对齐、去偏见及设计用于社会模拟的AI代理在学术和商业应用中的意义具有影响。', 'title_zh': '引导亲社会AI代理：大型语言模型在社会模拟中决策的计算基础'}
{'arxiv_id': 'arXiv:2504.11571', 'title': 'GraphicBench: A Planning Benchmark for Graphic Design with Language Agents', 'authors': 'Dayeon Ki, Tianyi Zhou, Marine Carpuat, Gang Wu, Puneet Mathur, Viswanathan Swaminathan', 'link': 'https://arxiv.org/abs/2504.11571', 'abstract': 'Large Language Model (LLM)-powered agents have unlocked new possibilities for automating human tasks. While prior work has focused on well-defined tasks with specified goals, the capabilities of agents in creative design tasks with open-ended goals remain underexplored. We introduce GraphicBench, a new planning benchmark for graphic design that covers 1,079 user queries and input images across four design types. We further present GraphicTown, an LLM agent framework with three design experts and 46 actions (tools) to choose from for executing each step of the planned workflows in web environments. Experiments with six LLMs demonstrate their ability to generate workflows that integrate both explicit design constraints from user queries and implicit commonsense constraints. However, these workflows often do not lead to successful execution outcomes, primarily due to challenges in: (1) reasoning about spatial relationships, (2) coordinating global dependencies across experts, and (3) retrieving the most appropriate action per step. We envision GraphicBench as a challenging yet valuable testbed for advancing LLM-agent planning and execution in creative design tasks.', 'abstract_zh': 'Large Language Model (LLM)-赋能的代理解锁了自动化人类任务的新可能性。虽然先前的工作集中在具有明确目标的定义任务上，但具有开放目标的创意设计任务中代理的能力仍待探索。我们引入了GraphicBench，这是一个全新的图形设计规划基准，涵盖了四种设计类型中的1,079个用户查询和输入图像。我们进一步提出了GraphicTown，这是一个包含三位设计专家和46种执行每一步计划工作流的工具的LLM代理框架，在网络环境中使用这些工具。六个LLM的实验展示了它们生成的工作流能够整合来自用户查询的显式设计约束和来自常识的隐式约束的能力。然而，这些工作流往往不能实现成功的结果，主要由于在：（1）空间关系推理，（2）跨专家全局依赖协调，以及（3）每一步检索最合适的动作方面存在的挑战。我们设想GraphicBench是一个具有挑战性但有价值的测试平台，用于推进创意设计任务中的LLM-代理规划和执行。', 'title_zh': 'GraphicBench：一种基于语言代理的图形设计规划基准'}
{'arxiv_id': 'arXiv:2504.11547', 'title': 'Probabilistic causal graphs as categorical data synthesizers: Do they do better than Gaussian Copulas and Conditional Tabular GANs?', 'authors': 'Olha Shaposhnyk, Noor Abid, Mouri Zakir, Svetlana Yanushkevich', 'link': 'https://arxiv.org/abs/2504.11547', 'abstract': 'This study investigates the generation of high-quality synthetic categorical data, such as survey data, using causal graph models. Generating synthetic data aims not only to create a variety of data for training the models but also to preserve privacy while capturing relationships between the data. The research employs Structural Equation Modeling (SEM) followed by Bayesian Networks (BN). We used the categorical data that are based on the survey of accessibility to services for people with disabilities. We created both SEM and BN models to represent causal relationships and to capture joint distributions between variables. In our case studies, such variables include, in particular, demographics, types of disability, types of accessibility barriers and frequencies of encountering those barriers.\nThe study compared the SEM-based BN method with alternative approaches, including the probabilistic Gaussian copula technique and generative models like the Conditional Tabular Generative Adversarial Network (CTGAN). The proposed method outperformed others in statistical metrics, including the Chi-square test, Kullback-Leibler divergence, and Total Variation Distance (TVD). In particular, the BN model demonstrated superior performance, achieving the highest TVD, indicating alignment with the original data. The Gaussian Copula ranked second, while CTGAN exhibited moderate performance. These analyses confirmed the ability of the SEM-based BN to produce synthetic data that maintain statistical and relational validity while maintaining confidentiality. This approach is particularly beneficial for research on sensitive data, such as accessibility and disability studies.', 'abstract_zh': '本研究利用因果图模型探究高质合成分类数据（如调查数据）的生成方法。生成合成数据不仅旨在为模型训练提供多样化数据，而且还能在保护隐私的同时捕捉数据之间的关系。研究采用了结构方程建模（SEM）后接贝叶斯网络（BN）的方法。我们基于残疾人群体服务可及性的调查数据创建了SEM和BN模型，以表示因果关系并捕捉变量间的联合分布。在我们的案例研究中，变量包括但不限于人口统计学特征、残疾类型、可达性障碍类型及其遇到的频率。本研究将基于SEM的BN方法与其他方法，如概率高斯 copula 技术和生成模型（如条件表生成对抗网络CTGAN）进行了比较。基于统计指标（包括卡方检验、KL散度和Total Variation Distance），提出的模型表现优于其他方法。特别是贝叶斯网络模型表现出 superiority，取得最高的TVD值，表明其与原始数据的吻合度较高。高斯 copula 排名第二，CTGAN 表现一般。这些分析证实了基于SEM的BN方法能够生成既符合统计性和关系性要求又能保护隐私的合成数据。该方法特别适用于敏感数据的研究，如可达性和残疾研究。', 'title_zh': '基于概率因果图的类别数据合成器：它们比高斯copula和条件表型GAN更优吗？'}
{'arxiv_id': 'arXiv:2504.11544', 'title': 'NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes', 'authors': 'Tianyang Xu, Haojie Zheng, Chengze Li, Haoxiang Chen, Yixin Liu, Ruoxi Chen, Lichao Sun', 'link': 'https://arxiv.org/abs/2504.11544', 'abstract': 'Retrieval-augmented generation (RAG) empowers large language models to access external and private corpus, enabling factually consistent responses in specific domains. By exploiting the inherent structure of the corpus, graph-based RAG methods further enrich this process by building a knowledge graph index and leveraging the structural nature of graphs. However, current graph-based RAG approaches seldom prioritize the design of graph structures. Inadequately designed graph not only impede the seamless integration of diverse graph algorithms but also result in workflow inconsistencies and degraded performance. To further unleash the potential of graph for RAG, we propose NodeRAG, a graph-centric framework introducing heterogeneous graph structures that enable the seamless and holistic integration of graph-based methodologies into the RAG workflow. By aligning closely with the capabilities of LLMs, this framework ensures a fully cohesive and efficient end-to-end process. Through extensive experiments, we demonstrate that NodeRAG exhibits performance advantages over previous methods, including GraphRAG and LightRAG, not only in indexing time, query time, and storage efficiency but also in delivering superior question-answering performance on multi-hop benchmarks and open-ended head-to-head evaluations with minimal retrieval tokens. Our GitHub repository could be seen at this https URL.', 'abstract_zh': '基于图的检索增强生成（RAG）方法通过构建知识图谱索引并利用图的结构特性，进一步丰富了这一过程，但当前的基于图的RAG方法很少重视图结构的设计。设计不当的图不仅阻碍了多种图算法的无缝集成，还导致工作流程不一致且性能下降。为了进一步发挥图在RAG中的潜力，我们提出了一种节点中心框架NodeRAG，引入异质图结构，使基于图的方法能够无缝地整合到RAG工作流中。通过与LLMs的能力紧密对齐，该框架确保实现了一个完全一致且高效的端到端过程。通过大量实验，我们证明NodeRAG在索引时间、查询时间和存储效率方面优于GraphRAG和LightRAG，并在多跳基准测试和开放式头对头评估中提供了更好的问答性能，同时最小化检索令牌。我们的GitHub仓库地址为这个 https URL。', 'title_zh': 'NodeRAG: 基于异构节点的图结构RAG'}
{'arxiv_id': 'arXiv:2504.11543', 'title': 'REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites', 'authors': 'Divyansh Garg, Shaun VanWeelden, Diego Caples, Andis Draguns, Nikil Ravi, Pranav Putta, Naman Garg, Tomas Abraham, Michael Lara, Federico Lopez, James Liu, Atharva Gundawar, Prannay Hebbar, Youngchul Joo, Charles London, Christian Schroeder de Witt, Sumeet Motwani', 'link': 'https://arxiv.org/abs/2504.11543', 'abstract': 'We introduce REAL, a benchmark and framework for multi-turn agent evaluations on deterministic simulations of real-world websites. REAL comprises high-fidelity, deterministic replicas of 11 widely-used websites across domains such as e-commerce, travel, communication, and professional networking. We also release a benchmark consisting of 112 practical tasks that mirror everyday complex user interactions requiring both accurate information retrieval and state-changing actions. All interactions occur within this fully controlled setting, eliminating safety risks and enabling robust, reproducible evaluation of agent capability and reliability. Our novel evaluation framework combines programmatic checks of website state for action-based tasks with rubric-guided LLM-based judgments for information retrieval. The framework supports both open-source and proprietary agent systems through a flexible evaluation harness that accommodates black-box commands within browser environments, allowing research labs to test agentic systems without modification. Our empirical results show that frontier language models achieve at most a 41% success rate on REAL, highlighting critical gaps in autonomous web navigation and task completion capabilities. Our framework supports easy integration of new tasks, reproducible evaluation, and scalable data generation for training web agents. The websites, framework, and leaderboard are available at this https URL and this https URL.', 'abstract_zh': 'REAL：一个用于实时网站确定性模拟多轮对话代理评估的基准和框架', 'title_zh': 'REAL：在确定性模拟真实网站环境下的自主代理基准测试'}
{'arxiv_id': 'arXiv:2504.11524', 'title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'authors': 'Haokun Liu, Sicong Huang, Jingyu Hu, Yangqiaoyu Zhou, Chenhao Tan', 'link': 'https://arxiv.org/abs/2504.11524', 'abstract': 'There is growing interest in hypothesis generation with large language models (LLMs). However, fundamental questions remain: what makes a good hypothesis, and how can we systematically evaluate methods for hypothesis generation? To address this, we introduce HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate. HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets. We evaluate four state-of-the-art LLMs combined with six existing hypothesis-generation methods. Overall, our results suggest that existing methods are capable of discovering valid and novel patterns in the data. However, the results from synthetic datasets indicate that there is still significant room for improvement, as current hypothesis generation methods do not fully uncover all relevant or meaningful patterns. Specifically, in synthetic settings, as task difficulty increases, performance significantly drops, with best models and methods only recovering 38.8% of the ground-truth hypotheses. These findings highlight challenges in hypothesis generation and demonstrate that HypoBench serves as a valuable resource for improving AI systems designed to assist scientific discovery.', 'abstract_zh': '基于大规模语言模型的假设生成：HypoBench基准设计与初步探索', 'title_zh': 'HypoBench: 向系统性和原则性的假设生成基准测试迈进'}
{'arxiv_id': 'arXiv:2504.11514', 'title': 'Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models', 'authors': 'Nicolas Baumann, Cheng Hu, Paviththiren Sivasothilingam, Haotong Qin, Lei Xie, Michele Magno, Luca Benini', 'link': 'https://arxiv.org/abs/2504.11514', 'abstract': "Neural Networks (NNs) trained through supervised learning struggle with managing edge-case scenarios common in real-world driving due to the intractability of exhaustive datasets covering all edge-cases, making knowledge-driven approaches, akin to how humans intuitively detect unexpected driving behavior, a suitable complement to data-driven methods. This work proposes a hybrid architecture combining low-level Model Predictive Controller (MPC) with locally deployed Large Language Models (LLMs) to enhance decision-making and Human Machine Interaction (HMI). The DecisionxLLM module evaluates robotic state information against natural language instructions to ensure adherence to desired driving behavior. The MPCxLLM module then adjusts MPC parameters based on LLM-generated insights, achieving control adaptability while preserving the safety and constraint guarantees of traditional MPC systems. Further, to enable efficient on-board deployment and to eliminate dependency on cloud connectivity, we shift processing to the on-board computing platform: We propose an approach that exploits Retrieval Augmented Generation (RAG), Low Rank Adaptation (LoRA) fine-tuning, and quantization. Experimental results demonstrate that these enhancements yield significant improvements in reasoning accuracy by up to 10.45%, control adaptability by as much as 52.2%, and up to 10.5x increase in computational efficiency (tokens/s), validating the proposed framework's practicality for real-time deployment even on down-scaled robotic platforms. This work bridges high-level decision-making with low-level control adaptability, offering a synergistic framework for knowledge-driven and adaptive Autonomous Driving Systems (ADS).", 'abstract_zh': '基于知识驱动和适应性的自主驾驶系统中的混合架构：结合低层次模型预测控制与局部部署的大语言模型', 'title_zh': '在车载部署的大语言模型增强自主驾驶系统'}
{'arxiv_id': 'arXiv:2504.11459', 'title': 'From Conceptual Data Models to Multimodal Representation', 'authors': 'Peter Stockinger', 'link': 'https://arxiv.org/abs/2504.11459', 'abstract': '1) Introduction and Conceptual Framework: This document explores the concept of information design by dividing it into two major practices: defining the meaning of a corpus of textual data and its visual or multimodal representation. It draws on expertise in enriching textual corpora, particularly audiovisual ones, and transforming them into multiple narrative formats. The text highlights a crucial distinction between the semantic content of a domain and the modalities of its graphic expression, illustrating this approach with concepts rooted in structural semiotics and linguistics traditions.\n2) Modeling and Conceptual Design:  The article emphasizes the importance of semantic modeling, often achieved through conceptual networks or graphs. These tools enable the structuring of knowledge within a domain by accounting for relationships between concepts, contexts of use, and specific objectives. Stockinger also highlights the constraints and challenges involved in creating dynamic and adaptable models, integrating elements such as thesauri or interoperable ontologies to facilitate the analysis and publication of complex corpora.\n3) Applications and Multimodal Visualization:  The text concludes by examining the practical application of these models in work environments like OKAPI, developed to analyze, publish, and reuse audiovisual data. It also discusses innovative approaches such as visual storytelling and document reengineering, which involve transforming existing content into new resources tailored to various contexts. These methods emphasize interoperability, flexibility, and the intelligence of communication systems, paving the way for richer and more collaborative use of digital data. The content of this document was presented during the "Semiotics of Information Design" Day organized by Anne Beyaert-Geslin of the University of Bordeaux Montaigne (MICA laboratory) on June 21, 2018, in Bordeaux.', 'abstract_zh': '1) 介绍与概念框架：本文探讨了信息设计的概念，将其分为两大实践：定义文本数据集的意义及其视觉或多媒体表示。本文借鉴了丰富文本语料库，尤其是音视频语料库方面的专业知识，并将其转换为多种叙事格式。文章突出了领域语义内容与图形表达的模式之间的关键区别，通过结构语义学和语言学传统中的概念来说明这一方法。\n\n2) 模型与概念设计：文章强调了语义建模的重要性，通常通过概念网络或图来实现。这些工具能够通过考虑概念间的关联、使用场合以及特定目标来在领域内结构化知识。Stockinger还强调了在创建动态和可适应模型过程中所面临的约束与挑战，通过整合如词汇表或互操作本体等元素，来促进复杂语料库的分析与发布。\n\n3) 应用与多媒体可视化：本文最后考察了这些模型在OKAPI等工作环境中的实际应用，OKAPI开发用于分析、发布和重用音视频数据。此外，文章还讨论了视觉叙事和文档重构等创新方法，涉及将现有内容转化为适应不同上下文的新资源。这些方法强调了互操作性、灵活性和通信系统的智能性，为更丰富和更具协作性的数字数据使用铺平了道路。本文内容于2018年6月21日在波尔多举行的由波尔多蒙吐内大学Anne Beyaert-Geslin组织的“信息设计语义学”日活动中呈现，该活动由MICA实验室主办。', 'title_zh': '从概念数据模型到多模态表示'}
{'arxiv_id': 'arXiv:2504.12292', 'title': 'SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians', 'authors': 'Liam Schoneveld, Zhe Chen, Davide Davoli, Jiapeng Tang, Saimon Terazawa, Ko Nishino, Matthias Nießner', 'link': 'https://arxiv.org/abs/2504.12292', 'abstract': 'Accurate, real-time 3D reconstruction of human heads from monocular images and videos underlies numerous visual applications. As 3D ground truth data is hard to come by at scale, previous methods have sought to learn from abundant 2D videos in a self-supervised manner. Typically, this involves the use of differentiable mesh rendering, which is effective but faces limitations. To improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor Learned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a set of Gaussians that are rigged to this mesh. We then reanimate this rigged head avatar to match a target frame, and backpropagate photometric losses to both the 3DMM and Gaussian prediction networks. We find that using Gaussians for rendering substantially improves the effectiveness of this self-supervised approach. Training solely on 2D data, our method surpasses existing self-supervised approaches in geometric evaluations on the NoW benchmark for neutral faces and a new benchmark for non-neutral expressions. Our method also produces highly expressive meshes, outperforming state-of-the-art in emotion classification.', 'abstract_zh': '单目图像和视频中自监督的人头准确实时三维重建及其应用', 'title_zh': 'SHeaP: 自监督头部几何预测器，通过2D高斯学习获得'}
{'arxiv_id': 'arXiv:2504.12284', 'title': 'How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions', 'authors': 'Aditya Prakash, Benjamin Lundell, Dmitry Andreychuk, David Forsyth, Saurabh Gupta, Harpreet Sawhney', 'link': 'https://arxiv.org/abs/2504.12284', 'abstract': 'We tackle the novel problem of predicting 3D hand motion and contact maps (or Interaction Trajectories) given a single RGB view, action text, and a 3D contact point on the object as input. Our approach consists of (1) Interaction Codebook: a VQVAE model to learn a latent codebook of hand poses and contact points, effectively tokenizing interaction trajectories, (2) Interaction Predictor: a transformer-decoder module to predict the interaction trajectory from test time inputs by using an indexer module to retrieve a latent affordance from the learned codebook. To train our model, we develop a data engine that extracts 3D hand poses and contact trajectories from the diverse HoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger than existing works, in terms of diversity of objects and interactions observed, and test for generalization of the model across object categories, action categories, tasks, and scenes. Experimental results show the effectiveness of our approach over transformer & diffusion baselines across all settings.', 'abstract_zh': '基于单个RGB视图、动作文本和物体上的3D接触点预测3D手部运动和接触图（或交互轨迹）的问题研究：一种交互码本和交互预测器的方法及其应用', 'title_zh': '如何做到这一点？合成日常交互中的3D手部运动和接触'}
{'arxiv_id': 'arXiv:2504.12268', 'title': 'HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level Synthesis Design Tasks', 'authors': 'Stefan Abi-Karam, Cong Hao', 'link': 'https://arxiv.org/abs/2504.12268', 'abstract': 'The rapid scaling of large language model (LLM) training and inference has driven their adoption in semiconductor design across academia and industry. While most prior work evaluates LLMs on hardware description language (HDL) tasks, particularly Verilog, designers are increasingly using high-level synthesis (HLS) to build domain-specific accelerators and complex hardware systems. However, benchmarks and tooling to comprehensively evaluate LLMs for HLS design tasks remain scarce.\nTo address this, we introduce HLS-Eval, the first complete benchmark and evaluation framework for LLM-driven HLS design. HLS-Eval targets two core tasks: (1) generating HLS code from natural language descriptions, and (2) performing HLS-specific code edits to optimize performance and hardware efficiency. The benchmark includes 94 unique designs drawn from standard HLS benchmarks and novel sources. Each case is prepared via a semi-automated flow that produces a natural language description and a paired testbench for C-simulation and synthesis validation, ensuring each task is "LLM-ready."\nBeyond the benchmark, HLS-Eval offers a modular Python framework for automated, parallel evaluation of both local and hosted LLMs. It includes a parallel evaluation engine, direct HLS tool integration, and abstractions for to support different LLM interaction paradigms, enabling rapid prototyping of new benchmarks, tasks, and LLM methods.\nWe demonstrate HLS-Eval through baseline evaluations of open-source LLMs on Vitis HLS, measuring outputs across four key metrics - parseability, compilability, runnability, and synthesizability - reflecting the iterative HLS design cycle. We also report pass@k metrics, establishing clear baselines and reusable infrastructure for the broader LLM-for-hardware community.\nAll benchmarks, framework code, and results are open-sourced at this https URL.', 'abstract_zh': '大型语言模型（LLM）训练和推理的快速扩展推动了其在半导体设计领域的学术和工业应用。虽然早期大多数研究工作主要是在硬件描述语言（HDL）任务上评估LLM，特别是Verilog，设计师越来越多地使用高层次综合（HLS）构建专用加速器和复杂硬件系统。然而，用于HLS设计任务全面评估LLM的基准和工具仍然稀缺。\n为了解决这个问题，我们引入了HLS-Eval，这是首个用于LLM驱动HLS设计的完整基准和评估框架。HLS-Eval针对两个核心任务：（1）从自然语言描述生成HLS代码，以及（2）执行HLS特定的代码编辑以优化性能和硬件效率。基准包括94个独特的设计，来源包括标准HLS基准和新颖源。每个案例通过半自动化流程准备，生成自然语言描述和配套的测试平台，用于C仿真和综合验证，确保每个任务都是“LLM就绪”的。\n除了基准之外，HLS-Eval还提供了一个模块化的Python框架，用于自动并行评估本地和托管的LLM。该框架包括并行评估引擎、直接HLS工具集成以及支持不同LLM交互模式的抽象，使快速原型设计新基准、任务和LLM方法成为可能。\n我们通过在Vitis HLS上对开源LLM进行基准评估，展示了HLS-Eval，衡量了四个关键指标（可解析性、可编译性、可运行性和可综合性），反映了迭代的HLS设计周期。我们还将报告pass@k指标，为更广泛的硬件LLM社区建立清晰的基础和可重用的基础设施。\n所有基准、框架代码和结果均在此网址开放源代码：this https URL。', 'title_zh': 'HLS-Eval：评价高級綜合設計任務中大型语言模型的基准和框架'}
{'arxiv_id': 'arXiv:2504.12262', 'title': 'SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields', 'authors': 'David Keetae Park, Xihaier Luo, Guang Zhao, Seungjun Lee, Miruna Oprescu, Shinjae Yoo', 'link': 'https://arxiv.org/abs/2504.12262', 'abstract': 'Spatiotemporal learning is challenging due to the intricate interplay between spatial and temporal dependencies, the high dimensionality of the data, and scalability constraints. These challenges are further amplified in scientific domains, where data is often irregularly distributed (e.g., missing values from sensor failures) and high-volume (e.g., high-fidelity simulations), posing additional computational and modeling difficulties. In this paper, we present SCENT, a novel framework for scalable and continuity-informed spatiotemporal representation learning. SCENT unifies interpolation, reconstruction, and forecasting within a single architecture. Built on a transformer-based encoder-processor-decoder backbone, SCENT introduces learnable queries to enhance generalization and a query-wise cross-attention mechanism to effectively capture multi-scale dependencies. To ensure scalability in both data size and model complexity, we incorporate a sparse attention mechanism, enabling flexible output representations and efficient evaluation at arbitrary resolutions. We validate SCENT through extensive simulations and real-world experiments, demonstrating state-of-the-art performance across multiple challenging tasks while achieving superior scalability.', 'abstract_zh': '时空学习由于空间依赖性和时间依赖性的复杂交互、数据的高维度以及可扩展性约束而具有挑战性。在科学领域中，这些挑战因数据分布不规则（例如，传感器故障导致的缺失值）和高数据量（例如，高保真仿真）而被进一步放大，带来了额外的计算和建模困难。在这篇论文中，我们提出了一种新颖的框架SCENT，用于可扩展且注重连续性的时空表示学习。SCENT 在单个架构中统一了插值、重建和预测。基于基于变压器的编码器-处理器-解码器架构，SCENT 引入了可学习的查询以增强泛化能力，并引入了查询权wise交叉注意力机制以有效捕获多尺度依赖性。为了确保在数据量和模型复杂性方面的可扩展性，我们结合了稀疏注意力机制，这使得输出表示具有灵活性，并能以任意分辨率进行高效评估。我们通过广泛的仿真和实际实验验证了SCENT，展示了其在多个挑战性任务上的顶级性能，同时实现了卓越的可扩展性。', 'title_zh': 'SCENT: 基于可扩展条件神经场的稳健时空学习用于连续科学数据'}
{'arxiv_id': 'arXiv:2504.12256', 'title': 'FLIP Reasoning Challenge', 'authors': 'Andreas Plesner, Turlan Kuzhagaliyev, Roger Wattenhofer', 'link': 'https://arxiv.org/abs/2504.12256', 'abstract': 'Over the past years, advances in artificial intelligence (AI) have demonstrated how AI can solve many perception and generation tasks, such as image classification and text writing, yet reasoning remains a challenge. This paper introduces the FLIP dataset, a benchmark for evaluating AI reasoning capabilities based on human verification tasks on the Idena blockchain. FLIP challenges present users with two orderings of 4 images, requiring them to identify the logically coherent one. By emphasizing sequential reasoning, visual storytelling, and common sense, FLIP provides a unique testbed for multimodal AI systems. Our experiments evaluate state-of-the-art models, leveraging both vision-language models (VLMs) and large language models (LLMs). Results reveal that even the best open-sourced and closed-sourced models achieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot settings, compared to human performance of 95.3%. Captioning models aid reasoning models by providing text descriptions of images, yielding better results than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5 Pro. Combining the predictions from 15 models in an ensemble increases the accuracy to 85.2%. These findings highlight the limitations of existing reasoning models and the need for robust multimodal benchmarks like FLIP. The full codebase and dataset will be available at this https URL.', 'abstract_zh': '在过去几年中，人工智能（AI）的进步展示了AI在解决图像分类和文本生成等感知和生成任务方面的能力，但推理仍然是一个挑战。本文介绍了FLIP数据集，这是一个基于Idena区块链上人工验证任务的AI推理能力评估基准。FLIP挑战要求用户提供四个图像的两种排序，并要求他们识别出逻辑上更为连贯的一种。通过强调序列推理、视觉叙事和常识，FLIP为多模态AI系统提供了一个独特的测试床。我们的实验利用了最新的视觉-语言模型（VLMs）和大型语言模型（LLMs）评估了最先进的模型。结果表明，即使是最先进的开源和闭源模型，在零样本设置下的最大准确率分别为75.5%和77.9%，而人类的性能为95.3%。描述模型通过提供图像的文字描述来辅助推理模型，其表现优于直接使用原始图像的情况，比如Gemini 1.5 Pro的69.6% vs. 75.2%。通过集成15个模型的预测结果，准确率提高到85.2%。这些发现凸显了现有推理模型的局限性，并强调了像FLIP这样稳健的多模态基准测试的需求。完整代码库和数据集可在此处访问。', 'title_zh': '反转推理挑战'}
{'arxiv_id': 'arXiv:2504.12215', 'title': 'Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing', 'authors': 'Ilkin Sevgi Isler, David Mohaisen, Curtis Lisle, Damla Turgut, Ulas Bagci', 'link': 'https://arxiv.org/abs/2504.12215', 'abstract': 'Reliable tumor segmentation in thoracic computed tomography (CT) remains challenging due to boundary ambiguity, class imbalance, and anatomical variability. We propose an uncertainty-guided, coarse-to-fine segmentation framework that combines full-volume tumor localization with refined region-of-interest (ROI) segmentation, enhanced by anatomically aware post-processing. The first-stage model generates a coarse prediction, followed by anatomically informed filtering based on lung overlap, proximity to lung surfaces, and component size. The resulting ROIs are segmented by a second-stage model trained with uncertainty-aware loss functions to improve accuracy and boundary calibration in ambiguous regions. Experiments on private and public datasets demonstrate improvements in Dice and Hausdorff scores, with fewer false positives and enhanced spatial interpretability. These results highlight the value of combining uncertainty modeling and anatomical priors in cascaded segmentation pipelines for robust and clinically meaningful tumor delineation. On the Orlando dataset, our framework improved Swin UNETR Dice from 0.4690 to 0.6447. Reduction in spurious components was strongly correlated with segmentation gains, underscoring the value of anatomically informed post-processing.', 'abstract_zh': '可靠的胸部分割在胸部计算机断层扫描（CT）中仍然具有挑战性，由于边界模糊、类别不平衡和解剖变异。我们提出了一种基于不确定性、从粗到细的分割框架，结合全体积肿瘤定位与细化区域of兴趣（ROI）分割，并通过解剖学意识的后续处理加以增强。首先，模型生成粗略预测，然后基于肺重叠、靠近肺表面的距离以及组件大小进行解剖学指导下的过滤。结果提取的ROIs由一个第二阶段模型进行分割，该模型使用不确定性感知损失函数进行训练，以提高在模糊区域的准确性和边界校准。在私有和公共数据集上的实验结果显示，Dice和Hausdorff分数有所提高，减少了假阳性，并增强了空间可 interpretability。这些结果突显了在级联分割管道中结合不确定性建模和解剖先验的价值，以实现稳健且临床上有意义的肿瘤界定。在Orlando数据集中，我们的框架将Swin UNETR的Dice分数从0.4690提高到0.6447。假分数量的减少与分割改进显著相关，突显了解剖学指导后续处理的价值。', 'title_zh': '基于 Anatomy-Aware 后处理的不确定性引导从粗到细肿瘤分割'}
{'arxiv_id': 'arXiv:2504.12210', 'title': 'Communication Optimization for Decentralized Learning atop Bandwidth-limited Edge Networks', 'authors': 'Tingyang Sun, Tuan Nguyen, Ting He', 'link': 'https://arxiv.org/abs/2504.12210', 'abstract': 'Decentralized federated learning (DFL) is a promising machine learning paradigm for bringing artificial intelligence (AI) capabilities to the network edge. Running DFL on top of edge networks, however, faces severe performance challenges due to the extensive parameter exchanges between agents. Most existing solutions for these challenges were based on simplistic communication models, which cannot capture the case of learning over a multi-hop bandwidth-limited network. In this work, we address this problem by jointly designing the communication scheme for the overlay network formed by the agents and the mixing matrix that controls the communication demands between the agents. By carefully analyzing the properties of our problem, we cast each design problem into a tractable optimization and develop an efficient algorithm with guaranteed performance. Our evaluations based on real topology and data show that the proposed algorithm can reduce the total training time by over $80\\%$ compared to the baseline without sacrificing accuracy, while significantly improving the computational efficiency over the state of the art.', 'abstract_zh': '基于代理形成覆盖网络的联合设计和带宽受限多跳网络上的混合矩阵优化的去中心化联邦学习', 'title_zh': '带宽受限边缘网络中去中心化学习的通信优化'}
{'arxiv_id': 'arXiv:2504.12192', 'title': 'From Requirements to Architecture: Semi-Automatically Generating Software Architectures', 'authors': 'Tobias Eisenreich', 'link': 'https://arxiv.org/abs/2504.12192', 'abstract': "To support junior and senior architects, I propose developing a new architecture creation method that leverages LLMs' evolving capabilities to support the architect. This method involves the architect's close collaboration with LLM-fueled tooling over the whole process. The architect is guided through Domain Model creation, Use Case specification, architectural decisions, and architecture evaluation. While the architect can take complete control of the process and the results, and use the tooling as a building set, they can follow the intended process for maximum tooling support. The preliminary results suggest the feasibility of this process and indicate major time savings for the architect.", 'abstract_zh': '为了支持初级和资深架构师，我提议开发一种新的架构创建方法，利用LLM不断演进的能力来辅助架构师。该方法包括架构师与以LLM为动力的工具在整个过程中进行密切合作。该方法指导架构师进行领域模型创建、用例规范、架构决策和架构评估。虽然架构师可以完全控制整个过程及其结果，并将工具视为构建块，但他们可以遵循预期的过程以获得最大的工具支持。初步结果表明该过程的可行性，并指出这将为架构师节省大量时间。', 'title_zh': '从需求到架构：半自动生成软件架构'}
{'arxiv_id': 'arXiv:2504.12187', 'title': 'What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure', 'authors': 'Céline Budding', 'link': 'https://arxiv.org/abs/2504.12187', 'abstract': 'It is sometimes assumed that Large Language Models (LLMs) know language, or for example that they know that Paris is the capital of France. But what -- if anything -- do LLMs actually know? In this paper, I argue that LLMs can acquire tacit knowledge as defined by Martin Davies (1990). Whereas Davies himself denies that neural networks can acquire tacit knowledge, I demonstrate that certain architectural features of LLMs satisfy the constraints of semantic description, syntactic structure, and causal systematicity. Thus, tacit knowledge may serve as a conceptual framework for describing, explaining, and intervening on LLMs and their behavior.', 'abstract_zh': '大型语言模型是否具备默会知识：基于马丁·戴维斯（1990）的定义', 'title_zh': '大型语言模型知多少？默会知识作为一种潜在的因果解释结构'}
{'arxiv_id': 'arXiv:2504.12185', 'title': 'SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data', 'authors': 'Suyoung Bae, Hyojun Kim, YunSeok Choi, Jee-Hyong Lee', 'link': 'https://arxiv.org/abs/2504.12185', 'abstract': 'In various natural language processing (NLP) tasks, fine-tuning Pre-trained Language Models (PLMs) often leads to the issue of spurious correlations, which negatively impacts performance, particularly when dealing with out-of-distribution data. To address this problem, we propose SALAD}(Structure Aware and LLM-driven Augmented Data), a novel approach designed to enhance model robustness and generalization by generating structure-aware and counterfactually augmented data for contrastive learning. Our method leverages a tagging-based approach to generate structure-aware positive samples and utilizes large language models (LLMs) to generate counterfactual negative samples with diverse sentence patterns. By applying contrastive learning, SALAD enables the model to focus on learning the structural relationships between key sentence components while minimizing reliance on spurious correlations. We validate our approach through experiments on three tasks: Sentiment Classification, Sexism Detection, and Natural Language Inference. The results demonstrate that SALAD not only improves model robustness and performance across different environments but also enhances generalization to out-of-distribution datasets and cross-domain scenarios.', 'abstract_zh': '在各种自然语言处理（NLP）任务中，预训练语言模型（PLMs）的微调往往会导致虚假相关性问题，这对性能尤其是处理分布外数据时产生了负面影响。为了解决这个问题，我们提出了一种名为SALAD（Structure Aware and LLM-driven Augmented Data）的新方法，旨在通过生成结构感知和反事实增强的数据来增强模型的稳健性和泛化能力。该方法利用标记化方法生成结构感知的正样本，并利用大型语言模型生成具有多种句子模式的反事实负样本。通过应用对比学习，SALAD使模型能够专注于学习关键句子组件之间的结构关系，同时减少了对虚假相关性的依赖。我们通过情感分类、性别歧视检测和自然语言推理三项任务的实验验证了该方法。结果表明，SALAD不仅在不同环境提升了模型的稳健性和性能，还增强了对分布外数据集和跨域场景的泛化能力。', 'title_zh': 'SALAD: 通过结构感知和大语言模型驱动的增强数据对比学习提高鲁棒性和泛化能力'}
{'arxiv_id': 'arXiv:2504.12180', 'title': 'Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification', 'authors': 'Jaime E. Cuellar, Oscar Moreno-Martinez, Paula Sofia Torres-Rodriguez, Jaime Andres Pavlich-Mariscal, Andres Felipe Mican-Castiblanco, Juan Guillermo Torres-Hurtado', 'link': 'https://arxiv.org/abs/2504.12180', 'abstract': 'One fundamental question for the social sciences today is: how much can we trust highly complex predictive models like ChatGPT? This study tests the hypothesis that subtle changes in the structure of prompts do not produce significant variations in the classification results of sentiment polarity analysis generated by the Large Language Model GPT-4o mini. Using a dataset of 100.000 comments in Spanish on four Latin American presidents, the model classified the comments as positive, negative, or neutral on 10 occasions, varying the prompts slightly each time. The experimental methodology included exploratory and confirmatory analyses to identify significant discrepancies among classifications.\nThe results reveal that even minor modifications to prompts such as lexical, syntactic, or modal changes, or even their lack of structure impact the classifications. In certain cases, the model produced inconsistent responses, such as mixing categories, providing unsolicited explanations, or using languages other than Spanish. Statistical analysis using Chi-square tests confirmed significant differences in most comparisons between prompts, except in one case where linguistic structures were highly similar.\nThese findings challenge the robustness and trust of Large Language Models for classification tasks, highlighting their vulnerability to variations in instructions. Moreover, it was evident that the lack of structured grammar in prompts increases the frequency of hallucinations. The discussion underscores that trust in Large Language Models is based not only on technical performance but also on the social and institutional relationships underpinning their use.', 'abstract_zh': '今天的社会科学的一个基本问题是：我们能相信像ChatGPT这样的高度复杂预测模型多少？本研究检验了这样的假设：提示结构的微妙变化不会对大型语言模型GPT-4o mini生成的情感极性分类结果产生显著差异。使用西班牙语对四位拉丁美洲总统的100,000条评论数据集，模型在10次实验中每次微调提示，将其分类为正面、负面或中性。实验方法包括探索性和验证性分析，以识别分类结果中的显著差异。\n\n研究结果表明，即使是词法、句法或情态变化，甚至是缺乏结构，细微的提示修改也会影响分类结果。在某些情况下，模型产生了不一致的响应，如混合类别、提供不必要的解释或使用不同于西班牙语的语言。卡方检验的统计分析证实了大多数提示之间的显著差异，但在一种情况下，语言结构高度相似。\n\n这些发现质疑了大型语言模型在分类任务中的稳健性和可信度，突显了它们在指令变化方面的脆弱性。此外，很明显，提示中缺乏结构化的语法增加了幻觉的发生频率。讨论强调，对大型语言模型的信任不仅基于技术水平，还基于其使用背后的社会和制度关系。', 'title_zh': '信任CHATGPT：提示小微调如何导致情感分类重大差异'}
{'arxiv_id': 'arXiv:2504.12177', 'title': 'Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube', 'authors': 'Victor Manuel Hernandez Lopez, Jaime E. Cuellar', 'link': 'https://arxiv.org/abs/2504.12177', 'abstract': 'This article analyzes the Hamas-Israel controversy through 253,925 Spanish-language YouTube comments posted between October 2023 and January 2024, following the October 7 attack that escalated the conflict. Adopting an interdisciplinary approach, the study combines the analysis of controversies from Science and Technology Studies (STS) with advanced computational methodologies, specifically Natural Language Processing (NLP) using the BERT (Bidirectional Encoder Representations from Transformers) model. Using this approach, the comments were automatically classified into seven categories, reflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli positions, among others. The results show a predominance of pro- Palestinian comments, although pro-Israeli and anti-Palestinian comments received more "likes." This study also applies the agenda-setting theory to demonstrate how media coverage significantly influences public perception, observing a notable shift in public opinion, transitioning from a pro- Palestinian stance to a more critical position towards Israel. This work highlights the importance of combining social science perspectives with technological tools in the analysis of controversies, presenting a methodological innovation by integrating computational analysis with critical social theories to address complex public opinion phenomena and media narratives.', 'abstract_zh': '本文通过分析2023年10月至2024年1月期间发布的253,925条西班牙语YouTube评论，探讨哈马斯与以色列的争议。采用跨学科的方法，研究结合了科技研究（STS）中的争议分析与先进的计算方法，特别是使用BERT模型的自然语言处理（NLP）。通过这种方法，评论自动被分类为七个类别，反映了亲巴勒斯坦、亲以色列、反巴勒斯坦、反以色列等立场。研究结果表明，亲巴勒斯坦的评论占主导地位，尽管亲以色列和反巴勒斯坦的评论获得了更多的“点赞”。本文还应用议程设置理论，展示媒体 coverage 如何显著影响公众认知，并观察到公众立场出现了从亲巴勒斯坦向更批评以色列立场的转变。本文强调结合社会科学研究视角与技术工具在争议分析中的重要性，并通过将计算分析与批判性社会理论相结合，提出了一种方法论创新，以应对复杂的公众意见现象和媒体叙事。', 'title_zh': '使用人工智能映射争议：巴勒斯坦哈马斯与以色列冲突在YouTube上的分析'}
{'arxiv_id': 'arXiv:2504.12172', 'title': 'Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task', 'authors': 'Maged S. Al-Shaibani, Zaid Alyafeai, Irfan Ahmad', 'link': 'https://arxiv.org/abs/2504.12172', 'abstract': 'Arabic poetry is an essential and integral part of Arabic language and culture. It has been used by the Arabs to spot lights on their major events such as depicting brutal battles and conflicts. They also used it, as in many other languages, for various purposes such as romance, pride, lamentation, etc. Arabic poetry has received major attention from linguistics over the decades. One of the main characteristics of Arabic poetry is its special rhythmic structure as opposed to prose. This structure is referred to as a meter. Meters, along with other poetic characteristics, are intensively studied in an Arabic linguistic field called "\\textit{Aroud}". Identifying these meters for a verse is a lengthy and complicated process. It also requires technical knowledge in \\textit{Aruod}. For recited poetry, it adds an extra layer of processing. Developing systems for automatic identification of poem meters for recited poems need large amounts of labelled data. In this study, we propose a state-of-the-art framework to identify the poem meters of recited Arabic poetry, where we integrate two separate high-resource systems to perform the low-resource task. To ensure generalization of our proposed architecture, we publish a benchmark for this task for future research.', 'abstract_zh': '阿拉伯诗歌是阿拉伯语言和文化的重要组成部分，被阿拉伯人用于描绘重要事件，如激烈的战斗和冲突。它还被用于各种目的，如浪漫、自豪、哀悼等。阿拉伯诗歌在几十年里受到了语言学界的广泛关注。阿拉伯诗歌的主要特征之一是其特殊的韵律结构，不同于散文。这种结构被称为“音节格律”。音节格律以及其他诗歌特征在被称为“Aroud”的阿拉伯语言学领域得到了深入研究。识别诗歌的音节格律是一个漫长且复杂的过程，需要具备“Aroud”技术知识。对于朗诵诗歌而言，这一过程还增加了额外的处理层。为了自动识别朗诵阿拉伯诗歌的音节格律，需要大量的标注数据。在本研究中，我们提出了一种最先进的框架来识别朗诵阿拉伯诗歌的音节格律，通过整合两个高资源系统来完成这一低资源任务。为了确保我们提出架构的通用性，我们为该任务发布了基准数据集，供未来研究使用。', 'title_zh': '阿拉伯口传诗歌的音节格律分类：集成高资源系统以完成低资源任务'}
{'arxiv_id': 'arXiv:2504.12151', 'title': 'Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment Analysis', 'authors': 'Miaosen Luo, Yuncheng Jiang, Sijie Mai', 'link': 'https://arxiv.org/abs/2504.12151', 'abstract': "Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack of interpretability in the decision logic of multimodal fusion and modality imbalance caused by disparities in inter-modal information density. To address these issues, we propose KAN-MCP, a novel framework that integrates the interpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the Multimodal Clean Pareto (MCPareto) framework. First, KAN leverages its univariate function decomposition to achieve transparent analysis of cross-modal interactions. This structural design allows direct inspection of feature transformations without relying on external interpretation tools, thereby ensuring both high expressiveness and interpretability. Second, the proposed MCPareto enhances robustness by addressing modality imbalance and noise interference. Specifically, we introduce the Dimensionality Reduction and Denoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises and reduces feature dimensionality. This approach provides KAN with discriminative low-dimensional inputs to reduce the modeling complexity of KAN while preserving critical sentiment-related information. Furthermore, MCPareto dynamically balances gradient contributions across modalities using the purified features output by DRD-MIB, ensuring lossless transmission of auxiliary signals and effectively alleviating modality imbalance. This synergy of interpretability and robustness not only achieves superior performance on benchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers an intuitive visualization interface through KAN's interpretable architecture.", 'abstract_zh': '多模态情感分析中的KAN-MCP框架：可解释性和鲁棒性的协同提升', 'title_zh': '面向可解释融合与平衡学习的多模态情感分析'}
{'arxiv_id': 'arXiv:2504.12143', 'title': 'ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges', 'authors': 'Matteo Lupinacci, Francesco Blefari, Francesco Romeo, Francesco Aurelio Pironti, Angelo Furfaro', 'link': 'https://arxiv.org/abs/2504.12143', 'abstract': 'The growing and evolving landscape of cybersecurity threats necessitates the development of supporting tools and platforms that allow for the creation of realistic IT environments operating within virtual, controlled settings as Cyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and experimenting with the effectiveness of devised countermeasures, as well as serving as training environments for building cyber security skills and abilities for IT operators. This paper proposes ARCeR as an innovative solution for the automatic generation and deployment of CRs, starting from user-provided descriptions in a natural language. ARCeR relies on the Agentic RAG paradigm, which allows it to fully exploit state-of-art AI technologies. Experimental results show that ARCeR is able to successfully process prompts even in cases that LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is able to target any CR framework provided that specific knowledge is made available to it.', 'abstract_zh': '不断演变的网络安全威胁 landscape 需要开发支持工具和平台，使能够在虚拟、受控环境中创建现实的 IT 环境，作为 Cyber Ranges (CRs)。CRs 可用于分析漏洞并实验所设计的应对措施的有效性，同时作为培养 IT 运维人员网络安全技能和能力的培训环境。本文提出 ARCeR 作为一种创新解决方案，可以从用户提供的自然语言描述自动生成和部署 CRs。ARCeR 依赖于 Agentic RAG 原理，使其能够充分利用最新的 AI 技术。实验结果表明，ARCeR 能够成功处理即使对于大语言模型 (LLMs) 或基本 RAG 系统也无法应对的提示。此外，只要向 ARCeR 提供特定知识，它就能够针对任何 CR 框架进行操作。', 'title_zh': 'ARCeR：一种自主型RAG，用于自动化定义网络空间范围'}
{'arxiv_id': 'arXiv:2504.12137', 'title': 'Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -', 'authors': 'Laura Fieback, Nishilkumar Balar, Jakob Spiegelberg, Hanno Gottschalk', 'link': 'https://arxiv.org/abs/2504.12137', 'abstract': 'Despite recent advances in Large Vision Language Models (LVLMs), these models still suffer from generating hallucinatory responses that do not align with the visual input provided. To mitigate such hallucinations, we introduce Efficient Contrastive Decoding (ECD), a simple method that leverages probabilistic hallucination detection to shift the output distribution towards contextually accurate answers at inference time. By contrasting token probabilities and hallucination scores, ECD subtracts hallucinated concepts from the original distribution, effectively suppressing hallucinations. Notably, our proposed method can be applied to any open-source LVLM and does not require additional LVLM training. We evaluate our method on several benchmark datasets and across different LVLMs. Our experiments show that ECD effectively mitigates hallucinations, outperforming state-of-the-art methods with respect to performance on LVLM benchmarks and computation time.', 'abstract_zh': '尽管近年来大型视觉语言模型取得了进展，但这些模型仍会产生与提供的视觉输入不一致的幻觉响应。为了减轻这种幻觉，我们引入了高效对比解码（ECD）方法，该方法通过利用概率幻觉检测，在推理时将输出分布偏向于上下文准确的答案。通过对比令牌概率和幻觉分数，ECD从原始分布中减去幻觉概念，有效抑制了幻觉。值得注意的是，我们提出的方法可以应用于任何开源的大型视觉语言模型，不需要额外的大型视觉语言模型训练。我们在多个基准数据集和不同的大型视觉语言模型上评估了该方法。实验结果表明，ECD有效地减轻了幻觉，相比现有的顶级方法，在大型视觉语言模型基准测试性能和计算时间上均表现出优势。', 'title_zh': '高效的对比解码与概率幻觉检测——缓解大规模视觉语言模型中的幻觉现象'}
{'arxiv_id': 'arXiv:2504.12088', 'title': 'AttentionDrop: A Novel Regularization Method for Transformer Models', 'authors': 'Mirza Samad Ahmed Baig, Syeda Anshrah Gillani, Abdul Akbar Khan, Shahid Munir Shah', 'link': 'https://arxiv.org/abs/2504.12088', 'abstract': 'Transformer-based architectures achieve state-of-the-art performance across a wide range of tasks in natural language processing, computer vision, and speech. However, their immense capacity often leads to overfitting, especially when training data is limited or noisy. We propose AttentionDrop, a unified family of stochastic regularization techniques that operate directly on the self-attention distributions. We introduces three variants: 1. Hard Attention Masking: randomly zeroes out top-k attention logits per query to encourage diverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic Gaussian convolution over attention logits to diffuse overly peaked distributions. 3. Consistency-Regularized AttentionDrop: enforces output stability under multiple independent AttentionDrop perturbations via a KL-based consistency loss.', 'abstract_zh': '基于Transformer的架构在自然语言处理、计算机视觉和语音处理等多种任务中实现了最先进的性能。然而，其巨大的容量往往会导致过拟合，特别是在训练数据有限或噪声较大的情况下。我们提出了AttentionDrop，这是一种直接作用于自我注意力分布的统一的随机正则化技术家族。我们介绍了三种变体：1. 硬注意力掩码：每次查询随机清零前k个注意力logits以鼓励多样化的上下文利用。2. 模糊注意力平滑：在注意力logits上应用动态高斯卷积以弥散过于尖峰的分布。3. 一致性正则化AttentionDrop：通过基于KL散度的一致性损失强制在多组独立的AttentionDrop扰动下输出的稳定性。', 'title_zh': 'AttentionDrop：一种新的Transformer模型正则化方法'}
{'arxiv_id': 'arXiv:2504.12082', 'title': 'Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection', 'authors': 'Yumin Kim, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2504.12082', 'abstract': 'Hate speech detection is a crucial area of research in natural language processing, essential for ensuring online community safety. However, detecting implicit hate speech, where harmful intent is conveyed in subtle or indirect ways, remains a major challenge. Unlike explicit hate speech, implicit expressions often depend on context, cultural subtleties, and hidden biases, making them more challenging to identify consistently. Additionally, the interpretation of such speech is influenced by external knowledge and demographic biases, resulting in varied detection results across different language models. Furthermore, Large Language Models often show heightened sensitivity to toxic language and references to vulnerable groups, which can lead to misclassifications. This over-sensitivity results in false positives (incorrectly identifying harmless statements as hateful) and false negatives (failing to detect genuinely harmful content). Addressing these issues requires methods that not only improve detection precision but also reduce model biases and enhance robustness. To address these challenges, we propose a novel method, which utilizes in-context learning without requiring model fine-tuning. By adaptively retrieving demonstrations that focus on similar groups or those with the highest similarity scores, our approach enhances contextual comprehension. Experimental results show that our method outperforms current state-of-the-art techniques. Implementation details and code are available at TBD.', 'abstract_zh': '隐含仇恨言论检测是自然语言处理领域的一项关键研究，对于确保在线社区安全至关重要。然而，识别通过含蓄或间接的方式传达有害意图的隐含仇恨言论仍是一项重大挑战。与明确的仇恨言论不同，隐含表达往往依赖于上下文、文化细微差别和隐藏的偏见，使其更难一致地识别。此外，这种言论的解读受到外部知识和人口统计学偏见的影响，导致不同语言模型的检测结果存在差异。同时，大型语言模型往往对有毒语言和对脆弱群体的提及表现出过度敏感，这可能导致误分类。这种过度敏感性会导致假阳性（错误地将无害陈述识别为仇恨言论）和假阴性（未能检测到真正有害的内容）。为了解决这些问题，需要不仅能提高检测精度，还能减少模型偏见和增强鲁棒性的方法。为此，我们提出了一种新方法，该方法利用上下文学习而无需对模型进行微调。通过适应性检索重点相似群体或具有最高相似度评分的演示文稿，我们的方法增强了上下文理解。实验结果显示，我们的方法优于当前最先进的技术。实施细节和代码可在TBD获取。', 'title_zh': '选择性演示检索以提高隐含仇恨言论检测'}
{'arxiv_id': 'arXiv:2504.12063', 'title': 'Optimizing Compound Retrieval Systems', 'authors': 'Harrie Oosterhuis, Rolf Jagerman, Zhen Qin, Xuanhui Wang', 'link': 'https://arxiv.org/abs/2504.12063', 'abstract': 'Modern retrieval systems do not rely on a single ranking model to construct their rankings. Instead, they generally take a cascading approach where a sequence of ranking models are applied in multiple re-ranking stages. Thereby, they balance the quality of the top-K ranking with computational costs by limiting the number of documents each model re-ranks. However, the cascading approach is not the only way models can interact to form a retrieval system.\nWe propose the concept of compound retrieval systems as a broader class of retrieval systems that apply multiple prediction models. This encapsulates cascading models but also allows other types of interactions than top-K re-ranking. In particular, we enable interactions with large language models (LLMs) which can provide relative relevance comparisons. We focus on the optimization of compound retrieval system design which uniquely involves learning where to apply the component models and how to aggregate their predictions into a final ranking. This work shows how our compound approach can combine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM relevance predictions, while optimizing a given ranking metric and efficiency target. Our experimental results show optimized compound retrieval systems provide better trade-offs between effectiveness and efficiency than cascading approaches, even when applied in a self-supervised manner.\nWith the introduction of compound retrieval systems, we hope to inspire the information retrieval field to more out-of-the-box thinking on how prediction models can interact to form rankings.', 'abstract_zh': '现代检索系统不依赖单一排名模型构建其排名，而是通常采取级联方法，在多个重排名阶段应用一系列排名模型，从而平衡前K个文档的排名质量与计算成本。然而，级联方法并非模型间交互的唯一方式。我们提出了复合检索系统的概念，作为一种更广泛的检索系统类别，它应用多种预测模型。这一概念不仅包括级联模型，还允许其他类型的交互，尤其是与大型语言模型（LLMs）的交互，后者可以提供相关性比较。我们专注于复合检索系统设计的优化，这独特地涉及学习如何应用组件模型以及如何将它们的预测整合为最终排名。本研究展示了我们复合方法如何将经典的BM25检索模型与最先进的（成对）LLM相关性预测结合，并优化给定的排名度量和效率目标。实验结果表明，优化的复合检索系统在效率和效果之间提供了比级联方法更好的权衡，即使是自监督应用也是如此。通过引入复合检索系统，我们希望激发信息检索领域对预测模型如何形成排名的更具创新性的思考。', 'title_zh': '优化复合检索系统'}
{'arxiv_id': 'arXiv:2504.12039', 'title': 'RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model', 'authors': 'Yizhuo Wu, Francesco Fioranelli, Chang Gao', 'link': 'https://arxiv.org/abs/2504.12039', 'abstract': "Radar-based HAR has emerged as a promising alternative to conventional monitoring approaches, such as wearable devices and camera-based systems, due to its unique privacy preservation and robustness advantages. However, existing solutions based on convolutional and recurrent neural networks, although effective, are computationally demanding during deployment. This limits their applicability in scenarios with constrained resources or those requiring multiple sensors. Advanced architectures, such as ViT and SSM architectures, offer improved modeling capabilities and have made efforts toward lightweight designs. However, their computational complexity remains relatively high. To leverage the strengths of transformer architectures while simultaneously enhancing accuracy and reducing computational complexity, this paper introduces RadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM specifically tailored for radar-based HAR. Across three diverse datasets, RadMamba matches the top-performing previous model's 99.8% classification accuracy on Dataset DIAT with only 1/400 of its parameters and equals the leading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their parameters. In scenarios with continuous sequences of actions evaluated on Dataset UoG2020, RadMamba surpasses other models with significantly higher parameter counts by at least 3%, achieving this with only 6.7k parameters. Our code is available at: this https URL.", 'abstract_zh': '基于雷达的HAR已成为一种有前景的替代传统监控方法（如可穿戴设备和基于摄像头的系统）的方案，得益于其独特的隐私保护和鲁棒性优势。然而，现有基于卷积神经网络和循环神经网络的解决方案虽然有效，但在部署时计算需求高，限制了其在资源受限场景或需要多传感器的场景中的应用。先进的架构，如ViT和SSM架构，提供了改进的建模能力，并朝着轻量级设计做出了努力。然而，它们的计算复杂度仍然相对较高。为同时利用变换器架构的优势并提高准确性和降低计算复杂度，本文提出了一种参数效率高、雷达微多普勒导向的Mamba SSM——RadMamba，专门适用于基于雷达的HAR。在三个不同的数据集中，RadMamba仅使用前一个最佳模型四百分之一的参数，实现了DIAT数据集99.8%的分类准确率；仅使用前一个最佳模型十分之一的参数，实现了CI4R数据集92.0%的准确率。在对UoG2020数据集中的连续动作序列进行评估时，RadMamba仅用6.7千参数就超过了其他具有更高参数计数的模型，准确率高出至少3%。我们的代码可在以下链接获取：this https URL。', 'title_zh': 'RadMamba：基于雷达微多普勒导向的anoia状态空间模型的人体活动识别'}
{'arxiv_id': 'arXiv:2504.12031', 'title': 'Proof-Carrying Neuro-Symbolic Code', 'authors': 'Ekaterina Komendantskaya', 'link': 'https://arxiv.org/abs/2504.12031', 'abstract': 'This invited paper introduces the concept of "proof-carrying neuro-symbolic code" and explains its meaning and value, from both the "neural" and the "symbolic" perspectives. The talk outlines the first successes and challenges that this new area of research faces.', 'abstract_zh': '应邀论文介绍了“证明承载神经符号代码”的概念，从“神经”和“符号”两个视角解释其含义和价值，并概述了这一新研究领域取得的初步成果和面临的影响与挑战。', 'title_zh': '神经符号推理代码证明'}
{'arxiv_id': 'arXiv:2504.12011', 'title': 'Balancing Graph Embedding Smoothness in Self-Supervised Learning via Information-Theoretic Decomposition', 'authors': 'Heesoo Jung, Hogun Park', 'link': 'https://arxiv.org/abs/2504.12011', 'abstract': 'Self-supervised learning (SSL) in graphs has garnered significant attention, particularly in employing Graph Neural Networks (GNNs) with pretext tasks initially designed for other domains, such as contrastive learning and feature reconstruction. However, it remains uncertain whether these methods effectively reflect essential graph properties, precisely representation similarity with its neighbors. We observe that existing methods position opposite ends of a spectrum driven by the graph embedding smoothness, with each end corresponding to outperformance on specific downstream tasks. Decomposing the SSL objective into three terms via an information-theoretic framework with a neighbor representation variable reveals that this polarization stems from an imbalance among the terms, which existing methods may not effectively maintain. Further insights suggest that balancing between the extremes can lead to improved performance across a wider range of downstream tasks. A framework, BSG (Balancing Smoothness in Graph SSL), introduces novel loss functions designed to supplement the representation quality in graph-based SSL by balancing the derived three terms: neighbor loss, minimal loss, and divergence loss. We present a theoretical analysis of the effects of these loss functions, highlighting their significance from both the SSL and graph smoothness perspectives. Extensive experiments on multiple real-world datasets across node classification and link prediction consistently demonstrate that BSG achieves state-of-the-art performance, outperforming existing methods. Our implementation code is available at this https URL.', 'abstract_zh': '图上的自监督学习（SSL）引起了显著关注，特别是在使用初始设计用于其他领域的图神经网络（GNNs），如对比学习和特征重建的先验任务。然而，这些方法是否能有效地反映图的关键属性——与邻居之间的表示相似性——仍不确定。我们观察到，现有方法在网络嵌入平滑性驱动下处于一个光谱的两端，每端在特定下游任务上表现优异。通过信息论框架将SSL目标分解为三个通过邻居表示变量导出的项，表明这种极化源于这些项之间的不平衡，而现有方法可能无法有效维持这种平衡。进一步的洞察表明，在极端之间取得平衡可以提高更广泛范围的下游任务性能。我们提出了一种框架BSG（平衡图SSL中的平滑性），引入了新的损失函数，旨在通过平衡这三个项（邻居损失、最小损失和散度损失）来补充基于图的SSL的表示质量。我们从SSL和平滑性两个角度分析了这些损失函数的效果，展示了它们的重要性。在节点分类和链接预测等多个现实世界数据集上的广泛实验一致证明，BSG达到了最先进的性能，优于现有方法。我们的实现代码可在以下链接获取：this https URL。', 'title_zh': '基于信息论分解的自监督学习中图嵌入平滑性平衡'}
{'arxiv_id': 'arXiv:2504.12007', 'title': 'Generative Recommendation with Continuous-Token Diffusion', 'authors': 'Haohao Qu, Wenqi Fan, Shanru Lin', 'link': 'https://arxiv.org/abs/2504.12007', 'abstract': 'In recent years, there has been a significant trend toward using large language model (LLM)-based recommender systems (RecSys). Current research primarily focuses on representing complex user-item interactions within a discrete space to align with the inherent discrete nature of language models. However, this approach faces limitations due to its discrete nature: (i) information is often compressed during discretization; (ii) the tokenization and generation for the vast number of users and items in real-world scenarios are constrained by a limited vocabulary. Embracing continuous data presents a promising alternative to enhance expressive capabilities, though this approach is still in its early stages. To address this gap, we propose a novel framework, DeftRec, which incorporates \\textbf{de}noising di\\textbf{f}fusion models to enable LLM-based RecSys to seamlessly support continuous \\textbf{t}oken as input and target. First, we introduce a robust tokenizer with a masking operation and an additive K-way architecture to index users and items, capturing their complex collaborative relationships into continuous tokens. Crucially, we develop a denoising diffusion model to process user preferences within continuous domains by conditioning on reasoning content from pre-trained large language model. During the denoising process, we reformulate the objective to include negative interactions, building a comprehensive understanding of user preferences for effective and accurate recommendation generation. Finally, given a continuous token as output, recommendations can be easily generated through score-based retrieval. Extensive experiments demonstrate the effectiveness of the proposed methods, showing that DeftRec surpasses competitive benchmarks, including both traditional and emerging LLM-based RecSys.', 'abstract_zh': '近年来，大规模语言模型（LLM）为基础的推荐系统（RecSys）呈现出显著的趋势。当前研究主要集中在离散空间中表示复杂的用户项交互，以适应语言模型固有的离散性质。然而，这种方法因其实质上的离散性质而受到限制：（i）在离散化过程中会压缩信息；（ii）在真实场景中，对大量用户和项进行分词和生成受限于有限的词汇表。采用连续数据提供了一种增强表示能力的有前景的替代方案，尽管这种方法仍处于初步阶段。为解决这一差距，我们提出了一种名为DeftRec的新型框架，该框架结合了去噪扩散模型，使LLM-Based RecSys能够无缝支持连续分词作为输入和目标。首先，我们引入了一种稳健的分词器，并采用掩码操作和加性K路架构来索引用户和项目，并将它们的复杂协作关系转换为连续分词。至关重要的是，我们开发了一种去噪扩散模型，该模型可以在预训练的大规模语言模型推理内容条件下处理连续域中的用户偏好。在去噪过程中，我们将目标重述为包括负交互，从而为有效准确的推荐生成提供全面的理解。最后，给定连续分词作为输出，可以通过基于分数的检索轻松生成推荐。广泛的实验证明了所提出方法的有效性，表明DeftRec超越了包括传统和新兴LLM-Based RecSys在内的竞争基准。', 'title_zh': '连续令牌扩散 generative 推荐'}
{'arxiv_id': 'arXiv:2504.11997', 'title': 'A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear MDPs', 'authors': 'Kihyuk Hong, Ambuj Tewari', 'link': 'https://arxiv.org/abs/2504.11997', 'abstract': 'We study reinforcement learning in infinite-horizon average-reward settings with linear MDPs. Previous work addresses this problem by approximating the average-reward setting by discounted setting and employing a value iteration-based algorithm that uses clipping to constrain the span of the value function for improved statistical efficiency. However, the clipping procedure requires computing the minimum of the value function over the entire state space, which is prohibitive since the state space in linear MDP setting can be large or even infinite. In this paper, we introduce a value iteration method with efficient clipping operation that only requires computing the minimum of value functions over the set of states visited by the algorithm. Our algorithm enjoys the same regret bound as the previous work while being computationally efficient, with computational complexity that is independent of the size of the state space.', 'abstract_zh': '我们在无限_horizon 平均回报环境中研究线性MDP的强化学习。以往的工作通过将平均回报问题近似为折扣问题，并采用一种基于值迭代的算法，利用剪裁来约束值函数的范围以提高统计效率。然而，剪裁过程需要计算状态空间中值函数的最小值，这在线性MDP环境中由于状态空间可能很大甚至无限时是不可行的。在本文中，我们引入了一种高效的剪裁操作值迭代方法，只需计算算法访问的状态集上值函数的最小值。我们的算法享有与以往工作相同的遗憾界，同时具有计算效率，计算复杂度与状态空间的大小无关。', 'title_zh': '无限 horizon 平均奖励线性MDP的计算高效算法'}
{'arxiv_id': 'arXiv:2504.11986', 'title': 'Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems', 'authors': 'Jose Manuel Guevara-Vela', 'link': 'https://arxiv.org/abs/2504.11986', 'abstract': 'This essay proposes an analogy between large language models (LLMs) and quasicrystals: systems that exhibit global coherence without periodic repetition and that are generated through local constraints. While LLMs are often evaluated in terms of predictive accuracy, factuality, or alignment, this structural perspective suggests that their most characteristic behavior is the production of internally resonant linguistic patterns. Just as quasicrystals forced a redefinition of order in physical systems, viewing LLMs as generators of quasi-structured language opens new paths for evaluation and design: privileging propagation of constraint over token-level accuracy, and coherence of form over fixed meaning. LLM outputs should be read not only for what they say, but for the patterns of constraint and coherence that organize them. This shift reframes generative language as a space of emergent patterning: LLMs are neither fully random nor strictly rule-based, but defined by a logic of constraint, resonance, and structural depth.', 'abstract_zh': '大型语言模型(LLMs)与准晶体的类比：一种结构视角下的语言生成研究', 'title_zh': '语言模型作为准晶体思维：生成系统中的结构、约束与涌现'}
{'arxiv_id': 'arXiv:2504.11967', 'title': 'Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions', 'authors': 'Yifei Dong, Fengyi Wu, Sanjian Zhang, Guangyu Chen, Yuzhi Hu, Masumi Yano, Jingdong Sun, Siyu Huang, Feng Liu, Qi Dai, Zhi-Qi Cheng', 'link': 'https://arxiv.org/abs/2504.11967', 'abstract': 'Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure inspection, surveillance, and related tasks, yet they also introduce critical security challenges. This survey provides a wide-ranging examination of the anti-UAV domain, centering on three core objectives-classification, detection, and tracking-while detailing emerging methodologies such as diffusion-based data synthesis, multi-modal fusion, vision-language modeling, self-supervised learning, and reinforcement learning. We systematically evaluate state-of-the-art solutions across both single-modality and multi-sensor pipelines (spanning RGB, infrared, audio, radar, and RF) and discuss large-scale as well as adversarially oriented benchmarks. Our analysis reveals persistent gaps in real-time performance, stealth detection, and swarm-based scenarios, underscoring pressing needs for robust, adaptive anti-UAV systems. By highlighting open research directions, we aim to foster innovation and guide the development of next-generation defense strategies in an era marked by the extensive use of UAVs.', 'abstract_zh': '无人驾驶航空器（UAVs）在基础设施检查、监控及相关任务中不可或缺，但同时也引入了关键的安全挑战。本文综述了反UAV领域，围绕分类、检测和跟踪三大核心目标进行广泛考察，并详细介绍了诸如基于扩散的数据合成、多模态融合、视觉-语言建模、半监督学习和强化学习等新兴方法。我们系统地评估了现有解决方案在单模态和多传感器管道（涵盖RGB、红外、音频、雷达和RF）中的性能，并讨论了大型及对抗性导向的基准测试。我们的分析揭示了实时性能、隐身检测和集群场景中的持续差距，突显了对稳健、自适应反UAV系统的迫切需求。通过强调开放的研究方向，旨在促进创新并指导在广泛使用UAV的时代下新一代防御策略的发展。', 'title_zh': '保障天空安全：无人机反制方法综述、基准测试及未来方向'}
{'arxiv_id': 'arXiv:2504.11952', 'title': 'Robust and Fine-Grained Detection of AI Generated Texts', 'authors': 'Ram Mohan Rao Kadiyala, Siddartha Pullakhandam, Kanwal Mehreen, Drishti Sharma, Siddhant Gupta, Jebish Purbey, Ashay Srivastava, Subhasya TippaReddy, Arvind Reddy Bobbili, Suraj Telugara Chandrashekhar, Modabbir Adeeb, Srinadh Vura, Hamza Farooq', 'link': 'https://arxiv.org/abs/2504.11952', 'abstract': "An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.", 'abstract_zh': '一种理想的检测系统应能有效识别日益出现的更多先进语言模型生成的内容。现有的系统往往难以精确识别短文本中的AI生成内容。此外，并非所有文本完全由人类或语言模型创作，因此我们更关注部分情况，即人类与语言模型合著的文本。本文介绍了一组用于标记分类任务的模型，这些模型是基于大量的人机合著文本训练的，表现出了对未见过的领域、生成器、非母语作者以及对抗输入文本的良好识别能力。我们还引入了一个包含超过240万条此类文本的新数据集，这些文本大多由23种语言的多种流行专有语言模型合著。我们还展示了模型在每个领域和生成器的文本上的性能表现。此外，还包括对抗方法性能对比、输入文本长度以及生成文本与原始人类创作文本的特征比较的研究发现。', 'title_zh': 'AI生成文本的鲁棒且细粒度检测'}
{'arxiv_id': 'arXiv:2504.11944', 'title': 'VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning', 'authors': 'Xuyang Chen, Guojian Wang, Keyu Yan, Lin Zhao', 'link': 'https://arxiv.org/abs/2504.11944', 'abstract': 'Offline reinforcement learning (RL) learns effective policies from pre-collected datasets, offering a practical solution for applications where online interactions are risky or costly. Model-based approaches are particularly advantageous for offline RL, owing to their data efficiency and generalizability. However, due to inherent model errors, model-based methods often artificially introduce conservatism guided by heuristic uncertainty estimation, which can be unreliable. In this paper, we introduce VIPO, a novel model-based offline RL algorithm that incorporates self-supervised feedback from value estimation to enhance model training. Specifically, the model is learned by additionally minimizing the inconsistency between the value learned directly from the offline data and the one estimated from the model. We perform comprehensive evaluations from multiple perspectives to show that VIPO can learn a highly accurate model efficiently and consistently outperform existing methods. It offers a general framework that can be readily integrated into existing model-based offline RL algorithms to systematically enhance model accuracy. As a result, VIPO achieves state-of-the-art performance on almost all tasks in both D4RL and NeoRL benchmarks.', 'abstract_zh': '基于模型的离线强化学习（RL）通过预收集的数据集学习有效的策略，为在线交互具有风险或成本的应用提供了一种实际解决方案。基于模型的方法特别适用于离线RL，这得益于其数据效率和泛化能力。然而，由于固有的模型误差，基于模型的方法通常会根据启发式的不确定性估计人为引入保守性，这可能是不可靠的。在这篇文章中，我们引入了VIPO，这是一种新颖的基于模型的离线RL算法，通过价值估计的自我监督反馈来增强模型训练。具体而言，该模型通过额外最小化直接从离线数据学习的价值与从模型估计的价值之间的一致性差异来学习。我们从多个角度进行综合评估，以证明VIPO能够高效且一致地学习高度准确的模型，并且能够系统地超越现有方法。它提供了一个通用框架，可以轻松集成到现有的基于模型的离线RL算法中，以系统地提高模型准确度。结果，VIPO在D4RL和NeoRL基准测试中的几乎所有任务中都实现了最先进的性能。', 'title_zh': 'VIPO: 基于值函数不一致性惩罚的离线强化学习'}
{'arxiv_id': 'arXiv:2504.11901', 'title': 'Causality-enhanced Decision-Making for Autonomous Mobile Robots in Dynamic Environments', 'authors': 'Luca Castri, Gloria Beraldo, Nicola Bellotto', 'link': 'https://arxiv.org/abs/2504.11901', 'abstract': 'The growing integration of robots in shared environments -- such as warehouses, shopping centres, and hospitals -- demands a deep understanding of the underlying dynamics and human behaviours, including how, when, and where individuals engage in various activities and interactions. This knowledge goes beyond simple correlation studies and requires a more comprehensive causal analysis. By leveraging causal inference to model cause-and-effect relationships, we can better anticipate critical environmental factors and enable autonomous robots to plan and execute tasks more effectively. To this end, we propose a novel causality-based decision-making framework that reasons over a learned causal model to predict battery usage and human obstructions, understanding how these factors could influence robot task execution. Such reasoning framework assists the robot in deciding when and how to complete a given task. To achieve this, we developed also PeopleFlow, a new Gazebo-based simulator designed to model context-sensitive human-robot spatial interactions in shared workspaces. PeopleFlow features realistic human and robot trajectories influenced by contextual factors such as time, environment layout, and robot state, and can simulate a large number of agents. While the simulator is general-purpose, in this paper we focus on a warehouse-like environment as a case study, where we conduct an extensive evaluation benchmarking our causal approach against a non-causal baseline. Our findings demonstrate the efficacy of the proposed solutions, highlighting how causal reasoning enables autonomous robots to operate more efficiently and safely in dynamic environments shared with humans.', 'abstract_zh': '机器人在共享环境中的日益集成——如仓库、购物中心和医院等——需要深入理解其背后的动力学和人类行为，包括个体在何时、何地以及如何参与各种活动和互动。这一知识远远超出了简单的相关性研究，需要更全面的因果分析。通过利用因果推断来建模因果关系，我们能够更好地预见关键的环境因素，使自主机器人能够更有效地规划和执行任务。为此，我们提出了一种基于因果关系的决策框架，该框架基于学习到的因果模型来预测电池使用和人类干扰，理解这些因素如何影响机器人任务执行。这种推理框架帮助机器人决定何时以及如何完成给定任务。为此，我们还开发了PeopleFlow，这是一种新的基于Gazebo的模拟器，用于建模共享工作空间中上下文敏感的人机空间交互。PeopleFlow 功能包括受时间、环境布局和机器人状态等上下文因素影响的真实人类和机器人轨迹，并且可以模拟大量代理。虽然模拟器具有通用性，但在本文中，我们以类似仓库的环境为案例研究，进行了广泛的评估，将我们的因果方法与非因果基准进行了比较。我们的研究结果表明，所提出解决方案的有效性，突显了因果推理如何使自主机器人更高效、更安全地在与人类共享的动态环境中运行。', 'title_zh': '因果关系增强的自主移动机器人在动态环境中的决策制定'}
{'arxiv_id': 'arXiv:2504.11896', 'title': 'Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement', 'authors': 'Xingxing Yang, Jie Chen, Zaifeng Yang', 'link': 'https://arxiv.org/abs/2504.11896', 'abstract': 'Image decomposition offers deep insights into the imaging factors of visual data and significantly enhances various advanced computer vision tasks. In this work, we introduce a novel approach to low-light image enhancement based on decomposed physics-informed priors. Existing methods that directly map low-light to normal-light images in the sRGB color space suffer from inconsistent color predictions and high sensitivity to spectral power distribution (SPD) variations, resulting in unstable performance under diverse lighting conditions. To address these challenges, we introduce a Physics-informed Color-aware Transform (PiCat), a learning-based framework that converts low-light images from the sRGB color space into deep illumination-invariant descriptors via our proposed Color-aware Transform (CAT). This transformation enables robust handling of complex lighting and SPD variations. Complementing this, we propose the Content-Noise Decomposition Network (CNDN), which refines the descriptor distributions to better align with well-lit conditions by mitigating noise and other distortions, thereby effectively restoring content representations to low-light images. The CAT and the CNDN collectively act as a physical prior, guiding the transformation process from low-light to normal-light domains. Our proposed PiCat framework demonstrates superior performance compared to state-of-the-art methods across five benchmark datasets.', 'abstract_zh': '基于分解物理先验的低光图像增强方法', 'title_zh': '学习物理知情的颜色感知变换以增强低光照图像'}
{'arxiv_id': 'arXiv:2504.11855', 'title': 'EngramNCA: a Neural Cellular Automaton Model of Memory Transfer', 'authors': 'Etienne Guichard, Felix Reimers, Mia Kvalsund, Mikkel Lepperød, Stefano Nichele', 'link': 'https://arxiv.org/abs/2504.11855', 'abstract': 'This study introduces EngramNCA, a neural cellular automaton (NCA) that integrates both publicly visible states and private, cell-internal memory channels, drawing inspiration from emerging biological evidence suggesting that memory storage extends beyond synaptic modifications to include intracellular mechanisms. The proposed model comprises two components: GeneCA, an NCA trained to develop distinct morphologies from seed cells containing immutable "gene" encodings, and GenePropCA, an auxiliary NCA that modulates the private "genetic" memory of cells without altering their visible states. This architecture enables the encoding and propagation of complex morphologies through the interaction of visible and private channels, facilitating the growth of diverse structures from a shared "genetic" substrate. EngramNCA supports the emergence of hierarchical and coexisting morphologies, offering insights into decentralized memory storage and transfer in artificial systems. These findings have potential implications for the development of adaptive, self-organizing systems and may contribute to the broader understanding of memory mechanisms in both biological and synthetic contexts.', 'abstract_zh': 'EngramNCA：一种结合公开状态和细胞内私有记忆通道的神经细胞自动机模型', 'title_zh': 'EngramNCA：记忆迁移的神经细胞自动机模型'}
{'arxiv_id': 'arXiv:2504.11837', 'title': 'FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations', 'authors': 'Yue Zhao, Qingqing Gu, Xiaoyu Wang, Teng Chen, Zhonglin Jiang, Yong Chen, Luo Ji', 'link': 'https://arxiv.org/abs/2504.11837', 'abstract': "Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Finite State Machine (FSM) on LLMs, and propose a framework called FiSMiness. Our framework allows a single LLM to bootstrap the planning during ESC, and self-reason the seeker's emotion, support strategy and the final response upon each conversational turn. Substantial experiments on ESC datasets suggest that FiSMiness outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and external-assisted methods, even those with many more parameters.", 'abstract_zh': '情感支持对话（ESC）旨在通过有效的对话减轻个体的情感困扰。尽管大型语言模型（LLMs）在ESC方面取得了显著进步，但大多数研究可能从状态模型视角未明确定义图表，因此提供的解决方案可能无法长期满意。为解决这一问题，我们利用有限状态机（FSM）对LLMs进行赋能，并提出了一种称为FiSMiness的框架。该框架允许单个LLM在情感支持对话（ESC）期间进行规划，并在每次对话回合中自我推理寻求者的情绪、支持策略和最终回应。在ESC数据集上的大量实验表明，FiSMiness在多种基线方法中表现更优，包括直接推理、自我修正、思维链、微调以及外部辅助方法，即使后者具有更多的参数。', 'title_zh': '基于有限状态机范式的 Emotional Support 对话框架'}
{'arxiv_id': 'arXiv:2504.11829', 'title': 'Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation', 'authors': 'Julia Kreutzer, Eleftheria Briakou, Sweta Agrawal, Marzieh Fadaee, Kocmi Tom', 'link': 'https://arxiv.org/abs/2504.11829', 'abstract': 'Generation capabilities and language coverage of multilingual large language models (mLLMs) are advancing rapidly. However, evaluation practices for generative abilities of mLLMs are still lacking comprehensiveness, scientific rigor, and consistent adoption across research labs, which undermines their potential to meaningfully guide mLLM development. We draw parallels with machine translation (MT) evaluation, a field that faced similar challenges and has, over decades, developed transparent reporting standards and reliable evaluations for multilingual generative models. Through targeted experiments across key stages of the generative evaluation pipeline, we demonstrate how best practices from MT evaluation can deepen the understanding of quality differences between models. Additionally, we identify essential components for robust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are rigorously assessed. We distill these insights into a checklist of actionable recommendations for mLLM research and development.', 'abstract_zh': '多语言大型语言模型的生成能力和语言覆盖范围正在迅速发展。然而，多语言大型语言模型生成能力的评估实践仍然缺乏全面性、科学严谨性和一致的采用，这削弱了它们对指导多语言大型语言模型发展的潜在意义。我们借鉴机器翻译评估领域的经验，该领域也曾面临类似挑战，并逐渐发展出透明报告标准和可靠的多语言生成模型评估方法。通过针对生成评估流水线的关键阶段进行定向实验，我们展示了机器翻译评估最佳实践如何加深对不同模型质量差异的理解。此外，我们识别出确保多语言大型语言模型稳健元评估的基本要素，确保评估方法本身也得到严格审查。我们将这些洞见提炼成一份适用于多语言大型语言模型研究和开发的可操作性建议清单。', 'title_zh': 'déjà vu: 多语言LLM评价通过机器翻译评价的视角'}
{'arxiv_id': 'arXiv:2504.11820', 'title': 'Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting', 'authors': 'Delong Suzhang, Meng Yang', 'link': 'https://arxiv.org/abs/2504.11820', 'abstract': 'The low-quality structure in raw depth maps is prevalent in real-world RGB-D datasets, which makes real-world depth recovery a critical task in recent years. However, the lack of paired raw-ground truth (raw-GT) data in the real world poses challenges for generalized depth recovery. Existing methods insufficiently consider the diversity of structure misalignment in raw depth maps, which leads to poor generalization in real-world depth recovery. Notably, random structure misalignments are not limited to raw depth data but also affect GT depth in real-world datasets. In the proposed method, we tackle the generalization problem from both input and output perspectives. For input, we enrich the diversity of structure misalignment in raw depth maps by designing a new raw depth generation pipeline, which helps the network avoid overfitting to a specific condition. Furthermore, a structure uncertainty module is designed to explicitly identify the misaligned structure for input raw depth maps to better generalize in unseen scenarios. Notably the well-trained depth foundation model (DFM) can help the structure uncertainty module estimate the structure uncertainty better. For output, a robust feature alignment module is designed to precisely align with the accurate structure of RGB images avoiding the interference of inaccurate GT depth. Extensive experiments on multiple datasets demonstrate the proposed method achieves competitive accuracy and generalization capabilities across various challenging raw depth maps.', 'abstract_zh': '原始深度图中的低质量结构在现实世界的RGB-D数据集中普遍存在，使得现实世界的深度恢复成为近年来的关键任务。然而，现实世界中缺乏配对的原始真实数据对（raw-GT）数据为通用深度恢复带来了挑战。现有方法未能充分考虑原始深度图中结构错位的多样性，导致在现实世界的深度恢复中泛化能力不足。值得注意的是，随机的结构错位不仅影响原始深度数据，还影响现实世界数据集中的真实深度数据。在所提出的方法中，我们从输入和输出两个方面解决泛化问题。对于输入，我们通过设计新的原始深度生成管道来丰富原始深度图中的结构错位多样性，帮助网络避免过度拟合特定条件。此外，我们设计了一种结构不确定性模块，明确识别输入原始深度图中的错位结构，从而更好地区分在未见过的场景中泛化。值得注意的是，经过充分训练的深度基础模型（DFM）可以帮助结构不确定性模块更好地估计结构不确定性。对于输出，我们设计了一种鲁棒的特征对齐模块，精确对齐RGB图像的准确结构，避免了不准确的真实深度信息的干扰。在多个数据集上的广泛实验表明，所提出的方法在各种挑战性的原始深度图中实现了竞争性的准确性和泛化能力。', 'title_zh': '基于结构不确定性建模和不准确GT深度拟合的现实世界深度恢复'}
{'arxiv_id': 'arXiv:2504.11812', 'title': 'Learning Strategies in Particle Swarm Optimizer: A Critical Review and Performance Analysis', 'authors': 'Dikshit Chauhan, Shivani, P. N. Suganthan', 'link': 'https://arxiv.org/abs/2504.11812', 'abstract': "Nature has long inspired the development of swarm intelligence (SI), a key branch of artificial intelligence that models collective behaviors observed in biological systems for solving complex optimization problems. Particle swarm optimization (PSO) is widely adopted among SI algorithms due to its simplicity and efficiency. Despite numerous learning strategies proposed to enhance PSO's performance in terms of convergence speed, robustness, and adaptability, no comprehensive and systematic analysis of these strategies exists. We review and classify various learning strategies to address this gap, assessing their impact on optimization performance. Additionally, a comparative experimental evaluation is conducted to examine how these strategies influence PSO's search dynamics. Finally, we discuss open challenges and future directions, emphasizing the need for self-adaptive, intelligent PSO variants capable of addressing increasingly complex real-world problems.", 'abstract_zh': '自然界长期启发着 swarm intelligence (群体智能) 的发展，群体智能是人工intelligence的一个关键分支，它通过模仿生物系统中观察到的集体行为来解决复杂优化问题。粒子群优化（PSO）由于其简单性和效率，在群体智能算法中被广泛应用。尽管提出了许多学习策略来提高PSO在收敛速度、鲁棒性和适应性等方面的性能，但这些策略的综合和系统性分析尚不存在。我们回顾并分类了各种学习策略以填补这一空白，评估它们对优化性能的影响。此外，我们还进行了比较实验评估，以考察这些策略如何影响PSO的搜索动力学。最后，我们讨论了开放挑战和未来方向，强调需要能够处理日益复杂现实问题的自适应和智能PSO变体。', 'title_zh': '粒子群优化器中的学习策略：一项关键评述与性能分析'}
{'arxiv_id': 'arXiv:2504.11793', 'title': 'Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification', 'authors': 'Yue Li, Lihong Zhang', 'link': 'https://arxiv.org/abs/2504.11793', 'abstract': 'Federated Learning (FL) faces major challenges regarding communication overhead and model privacy when training large language models (LLMs), especially in healthcare applications. To address these, we introduce Selective Attention Federated Learning (SAFL), a novel approach that dynamically fine-tunes only those transformer layers identified as attention-critical. By employing attention patterns to determine layer importance, SAFL significantly reduces communication bandwidth and enhances differential privacy resilience. Evaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and MIMIC-III discharge summaries) demonstrate that SAFL achieves competitive performance with centralized models while substantially improving communication efficiency and privacy preservation.', 'abstract_zh': '选择性注意力联邦学习（SAFL）：一种用于大型语言模型训练的新方法', 'title_zh': '选择性注意联邦学习：提高临床文本分类的隐私性和效率'}
{'arxiv_id': 'arXiv:2504.11788', 'title': 'Enhancing Web Agents with Explicit Rollback Mechanisms', 'authors': 'Zhisong Zhang, Tianqing Fang, Kaixin Ma, Wenhao Yu, Hongming Zhang, Haitao Mi, Dong Yu', 'link': 'https://arxiv.org/abs/2504.11788', 'abstract': 'With recent advancements in large language models, web agents have been greatly improved. However, dealing with complex and dynamic web environments requires more advanced planning and search abilities. Previous studies usually adopt a greedy one-way search strategy, which may struggle to recover from erroneous states. In this work, we enhance web agents with an explicit rollback mechanism, enabling the agent to revert back to a previous state in its navigation trajectory. This mechanism gives the model the flexibility to directly control the search process, leading to an effective and efficient web navigation method. We conduct experiments on two live web navigation benchmarks with zero-shot and fine-tuning settings. The results demonstrate the effectiveness of our proposed approach.', 'abstract_zh': '随着大型语言模型的 Recent 进展，网络代理得到了极大改进。然而，应对复杂和动态的网络环境需要更高级的规划和搜索能力。前期研究通常采用贪婪的一维搜索策略，这可能难以从错误状态中恢复。在本工作中，我们通过引入显式的回滚机制增强网络代理，使代理能够回退到导航轨迹的先前状态。该机制赋予模型直接控制搜索过程的灵活性，从而实现一种有效且高效的网络导航方法。我们在两个实时网络导航基准上进行了零样本和微调设置的实验。结果证明了我们提出的方法的有效性。', 'title_zh': '增强Web代理的显式回滚机制'}
{'arxiv_id': 'arXiv:2504.11781', 'title': 'ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical Consensus State Space Model', 'authors': 'Guanchun Wang, Xiangrong Zhang, Yifei Zhang, Zelin Peng, Tianyang Zhang, Xu Tang, Licheng Jiao', 'link': 'https://arxiv.org/abs/2504.11781', 'abstract': 'Unsupervised anomaly detection in hyperspectral images (HSI), aiming to detect unknown targets from backgrounds, is challenging for earth surface monitoring. However, current studies are hindered by steep computational costs due to the high-dimensional property of HSI and dense sampling-based training paradigm, constraining their rapid deployment. Our key observation is that, during training, not all samples within the same homogeneous area are indispensable, whereas ingenious sampling can provide a powerful substitute for reducing costs. Motivated by this, we propose an Asymmetrical Consensus State Space Model (ACMamba) to significantly reduce computational costs without compromising accuracy. Specifically, we design an asymmetrical anomaly detection paradigm that utilizes region-level instances as an efficient alternative to dense pixel-level samples. In this paradigm, a low-cost Mamba-based module is introduced to discover global contextual attributes of regions that are essential for HSI reconstruction. Additionally, we develop a consensus learning strategy from the optimization perspective to simultaneously facilitate background reconstruction and anomaly compression, further alleviating the negative impact of anomaly reconstruction. Theoretical analysis and extensive experiments across eight benchmarks verify the superiority of ACMamba, demonstrating a faster speed and stronger performance over the state-of-the-art.', 'abstract_zh': '无监督高光谱图像异常检测（HSI）旨在从背景中检测未知目标，对于地球表面监测来说具有挑战性。然而，当前研究受限于高维性质导致的高额计算成本以及基于密集采样的训练范式，限制了其快速部署。我们的关键观察是，在训练过程中，并非所有同质区域内的样本都是不可或缺的，而巧妙的采样可以提供一个强大的替代方案来降低成本。基于此，我们提出了一种不对称一致状态空间模型（ACMamba），以显著降低计算成本而不牺牲准确性。具体来说，我们设计了一种利用区域级实例替代密集像素级样本的不对称异常检测范式。在此范式中，引入了一个低成本的Mamba基模块来发现对HSI重建至关重要的全局上下文属性。此外，我们从优化角度开发了一种一致学习策略，以同时促进背景重建和异常压缩，进一步缓解异常重建的负面影响。理论分析和在八个基准上的广泛实验验证了ACMamba的优势，证明其在速度和性能上均超过了现有最佳方法。', 'title_zh': 'ACMamba: 快速无监督异常检测基于不对称共识状态空间模型'}
{'arxiv_id': 'arXiv:2504.11780', 'title': "Agile Retrospectives: What went well? What didn't go well? What should we do?", 'authors': 'Maria Spichkova, Hina Lee, Kevin Iwan, Madeleine Zwart, Yuwon Yoon, Xiaohan Qin', 'link': 'https://arxiv.org/abs/2504.11780', 'abstract': "In Agile/Scrum software development, the idea of retrospective meetings (retros) is one of the core elements of the project process. In this paper, we present our work in progress focusing on two aspects: analysis of potential usage of generative AI for information interaction within retrospective meetings, and visualisation of retros' information to software development teams. We also present our prototype tool RetroAI++, focusing on retros-related functionalities.", 'abstract_zh': '在敏捷/Scrum软件开发中，回顾会议（Retros）的理念是项目过程中的核心元素之一。在本文中，我们呈现了我们正在进行的工作，重点关注两个方面：生成式AI在回顾会议中信息交互中的潜在应用分析，以及回顾会议信息的可视化展示给软件开发团队。我们还介绍了我们的原型工具RetroAI++，重点关注与回顾会议相关的功能。', 'title_zh': '敏捷回顾：什么是好的？什么是不好的？我们应该怎么做？'}
{'arxiv_id': 'arXiv:2504.11774', 'title': 'PCDiff: Proactive Control for Ownership Protection in Diffusion Models with Watermark Compatibility', 'authors': 'Keke Gai, Ziyue Shen, Jing Yu, Liehuang Zhu, Qi Wu', 'link': 'https://arxiv.org/abs/2504.11774', 'abstract': 'With the growing demand for protecting the intellectual property (IP) of text-to-image diffusion models, we propose PCDiff -- a proactive access control framework that redefines model authorization by regulating generation quality. At its core, PCDIFF integrates a trainable fuser module and hierarchical authentication layers into the decoder architecture, ensuring that only users with valid encrypted credentials can generate high-fidelity images. In the absence of valid keys, the system deliberately degrades output quality, effectively preventing unauthorized this http URL, while the primary mechanism enforces active access control through architectural intervention, its decoupled design retains compatibility with existing watermarking techniques. This satisfies the need of model owners to actively control model ownership while preserving the traceability capabilities provided by traditional watermarking this http URL experimental evaluations confirm a strong dependency between credential verification and image quality across various attack scenarios. Moreover, when combined with typical post-processing operations, PCDIFF demonstrates powerful performance alongside conventional watermarking methods. This work shifts the paradigm from passive detection to proactive enforcement of authorization, laying the groundwork for IP management of diffusion models.', 'abstract_zh': '文本到图像扩散模型知识产权保护的主动访问控制框架：PCDiff', 'title_zh': 'PCDiff: 兼顾水印兼容性的主动控制产权保护方法在扩散模型中'}
{'arxiv_id': 'arXiv:2504.11754', 'title': 'GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision', 'authors': 'Zihui Zhang, Yafei Yang, Hongtao Wen, Bo Yang', 'link': 'https://arxiv.org/abs/2504.11754', 'abstract': 'We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually limited to identifying simple objects like cars or their segmented objects are often inferior due to the lack of objectness in pretrained features. In this paper, we propose a new two-stage pipeline called GrabS. The core concept of our method is to learn generative and discriminative object-centric priors as a foundation from object datasets in the first stage, and then design an embodied agent to learn to discover multiple objects by querying against the pretrained generative priors in the second stage. We extensively evaluate our method on two real-world datasets and a newly created synthetic dataset, demonstrating remarkable segmentation performance, clearly surpassing all existing unsupervised methods.', 'abstract_zh': '我们研究在复杂点云中无需3D场景人工标注即进行3D物体分割的hard问题。通过依赖预训练2D特征的相似性或外部信号如运动来对3D点进行分组以形成物体，现有的无监督方法通常局限于识别简单的物体（如汽车），或者由于预训练特征缺乏物体特性而导致分割效果欠佳。在本文中，我们提出了一种新的两阶段管道方法称为GrabS。该方法的核心概念是在第一阶段从物体数据集中学习生成性和判别性的物体中心先验知识作为基础，并在第二阶段设计一种具身代理，通过查询预训练生成性先验知识来学习发现多个物体。我们在两个真实世界数据集和一个新创建的合成数据集上进行了广泛评估，展示了显著的分割性能，明显优于所有现有的无监督方法。', 'title_zh': 'GrabS: 生成式 embodied 代理用于无场景监督的 3D 物体分割'}
{'arxiv_id': 'arXiv:2504.11750', 'title': 'Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures', 'authors': 'Prabhu Vellaisamy, Thomas Labonte, Sourav Chakraborty, Matt Turner, Samantika Sury, John Paul Shen', 'link': 'https://arxiv.org/abs/2504.11750', 'abstract': "Large language model (LLM)-based inference workloads increasingly dominate data center costs and resource utilization. Therefore, understanding the inference workload characteristics on evolving CPU-GPU coupled architectures is crucial for optimization. This paper presents an in-depth analysis of LLM inference behavior on loosely-coupled (PCIe A100/H100) and closely-coupled (GH200) systems. We analyze performance dynamics using fine-grained operator-to-kernel trace analysis, facilitated by our novel profiler SKIP and metrics like Total Kernel Launch and Queuing Time (TKLQT). Results show that closely-coupled (CC) GH200 significantly outperforms loosely-coupled (LC) systems at large batch sizes, achieving 1.9x-2.7x faster prefill latency for Llama 3.2-1B. However, our analysis also reveals that GH200 remains CPU-bound up to 4x larger batch sizes than LC systems. In this extended CPU-bound region, we identify the performance characteristics of the Grace CPU as a key factor contributing to higher inference latency at low batch sizes on GH200. We demonstrate that TKLQT accurately identifies this CPU/GPU-bound transition point. Based on this analysis, we further show that kernel fusion offers significant potential to mitigate GH200's low-batch latency bottleneck by reducing kernel launch overhead. This detailed kernel-level characterization provides critical insights for optimizing diverse CPU-GPU coupling strategies. This work is an initial effort, and we plan to explore other major AI/DL workloads that demand different degrees of CPU-GPU heterogeneous architectures.", 'abstract_zh': '基于大型语言模型（LLM）的推理工作负载日益主导数据中心的成本和资源利用。因此，理解在不断演进的CPU-GPU耦合架构上的推理工作负载特性对于优化至关重要。本文对松散耦合（PCIe A100/H100）和紧密耦合（GH200）系统上的LLM推理行为进行了深入分析。我们使用精细粒度的操作符到内核跟踪分析方法，并借助我们新颖的探针SKIP和指标如总内核启动和队列时间（TKLQT）来分析性能动态。结果显示，在大规模批次中，紧密耦合（CC）GH200显著优于松散耦合（LC）系统，实现Llama 3.2-1B预填充延迟1.9倍至2.7倍的提升。然而，我们的分析还揭示了GH200在比LC系统大4倍的批次大小下仍然受CPU绑定。在这个延长的CPU绑定区域，我们确定Grace CPU的性能特性是GH200在低批次大小下推理延迟较高的关键因素。我们证明TKLQT能够准确识别这一CPU/GPU绑定转换点。基于此分析，我们进一步表明，内核融合可以通过减少内核启动开销，显著缓解GH200在小批次延迟瓶颈。这一详细的内核级表征为优化各种CPU-GPU耦合策略提供了宝贵见解。本工作是初步尝试，我们计划探索其他需要不同程度CPU-GPU异构架构的 major AI/DL工作负载。', 'title_zh': 'CPU-GPU 绑定架构上字符化和优化大模型推理工作负载'}
{'arxiv_id': 'arXiv:2504.11726', 'title': 'Saga: Capturing Multi-granularity Semantics from Massive Unlabelled IMU Data for User Perception', 'authors': 'Yunzhe Li, Facheng Hu, Hongzi Zhu, Shifan Zhang, Liang Zhang, Shan Chang, Minyi Guo', 'link': 'https://arxiv.org/abs/2504.11726', 'abstract': 'Inertial measurement units (IMUs), have been prevalently used in a wide range of mobile perception applications such as activity recognition and user authentication, where a large amount of labelled data are normally required to train a satisfactory model. However, it is difficult to label micro-activities in massive IMU data due to the hardness of understanding raw IMU data and the lack of ground truth. In this paper, we propose a novel fine-grained user perception approach, called Saga, which only needs a small amount of labelled IMU data to achieve stunning user perception accuracy. The core idea of Saga is to first pre-train a backbone feature extraction model, utilizing the rich semantic information of different levels embedded in the massive unlabelled IMU data. Meanwhile, for a specific downstream user perception application, Bayesian Optimization is employed to determine the optimal weights for pre-training tasks involving different semantic levels. We implement Saga on five typical mobile phones and evaluate Saga on three typical tasks on three IMU datasets. Results show that when only using about 100 training samples per class, Saga can achieve over 90% accuracy of the full-fledged model trained on over ten thousands training samples with no additional system overhead.', 'abstract_zh': '基于惯性测量单元的细粒度用户感知方法Saga：少量标注数据实现高效用户感知准确性', 'title_zh': 'Saga：从大量未标注的IMU数据中捕捉多粒度语义以分析用户感知'}
{'arxiv_id': 'arXiv:2504.11713', 'title': 'Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching', 'authors': 'Aaron Havens, Benjamin Kurt Miller, Bing Yan, Carles Domingo-Enrich, Anuroop Sriram, Brandon Wood, Daniel Levine, Bin Hu, Brandon Amos, Brian Karrer, Xiang Fu, Guan-Horng Liu, Ricky T. Q. Chen', 'link': 'https://arxiv.org/abs/2504.11713', 'abstract': 'We introduce Adjoint Sampling, a highly scalable and efficient algorithm for learning diffusion processes that sample from unnormalized densities, or energy functions. It is the first on-policy approach that allows significantly more gradient updates than the number of energy evaluations and model samples, allowing us to scale to much larger problem settings than previously explored by similar methods. Our framework is theoretically grounded in stochastic optimal control and shares the same theoretical guarantees as Adjoint Matching, being able to train without the need for corrective measures that push samples towards the target distribution. We show how to incorporate key symmetries, as well as periodic boundary conditions, for modeling molecules in both cartesian and torsional coordinates. We demonstrate the effectiveness of our approach through extensive experiments on classical energy functions, and further scale up to neural network-based energy models where we perform amortized conformer generation across many molecular systems. To encourage further research in developing highly scalable sampling methods, we plan to open source these challenging benchmarks, where successful methods can directly impact progress in computational chemistry.', 'abstract_zh': '我们介绍了一种称为伴随采样的高效算法，该算法适用于从非归一化密度或能量函数中采样的扩散过程的学习。这是首个能够在能量评估和模型样本数量显著多于梯度更新次数的情况下工作的在线策略方法，使我们能够处理比先前类似方法探索的更复杂的问题规模。我们的框架在随机最优控制理论上有坚实的理论基础，并与伴随匹配方法共享相同的理论保证，无需采取纠正措施将样本推送到目标分布即可进行训练。我们展示了如何在笛卡尔坐标和扭转坐标中结合关键对称性和周期边界条件，以用于分子建模。通过在经典能量函数和基于神经网络的能量模型上的广泛实验，我们展示了该方法的有效性，并进一步扩展到多个分子系统的构象生成任务。为促进对高效可扩展采样方法的进一步研究，我们计划开源这些具有挑战性的基准测试，成功的方法可以直接影响计算化学的发展。', 'title_zh': '伴随采样：通过伴随匹配实现的高度可扩展扩散采样器'}
{'arxiv_id': 'arXiv:2504.11711', 'title': "The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by LLMs", 'authors': 'Haonan Li, Hang Zhang, Kexin Pei, Zhiyun Qian', 'link': 'https://arxiv.org/abs/2504.11711', 'abstract': 'Static analysis is a cornerstone for software vulnerability detection, yet it often struggles with the classic precision-scalability trade-off. In practice, such tools often produce high false positive rates, particularly in large codebases like the Linux kernel. This imprecision can arise from simplified vulnerability modeling and over-approximation of path and data constraints. While large language models (LLMs) show promise in code understanding, their naive application to program analysis yields unreliable results due to inherent reasoning limitations. We introduce BugLens, a post-refinement framework that significantly improves static analysis precision. BugLens guides an LLM to follow traditional analysis steps by assessing buggy code patterns for security impact and validating the constraints associated with static warnings. Evaluated on real-world Linux kernel bugs, BugLens raises precision from 0.10 (raw) and 0.50 (semi-automated refinement) to 0.72, substantially reducing false positives and revealing four previously unreported vulnerabilities. Our results suggest that a structured LLM-based workflow can meaningfully enhance the effectiveness of static analysis tools.', 'abstract_zh': '静态分析是软件漏洞检测的基石，但往往难以克服精确性与扩展性的经典权衡。在实践中，此类工具在大规模代码库中，如Linux内核中，经常产生较高的假阳性率。这种不精确性可能源于简化了的漏洞建模和路径及数据约束的过度逼近。虽然大型语言模型在代码理解方面表现出潜力，但它们对程序分析的直接应用由于固有的推理限制而导致结果不可靠。我们引入了BugLens，这是一个后精炼框架，显著提高了静态分析的精确性。BugLens通过评估漏洞代码模式对安全性的影响并验证与静态警告相关的约束，引导大型语言模型遵循传统分析步骤。在实际的Linux内核漏洞上进行评估，BugLens将精确性从原始的0.10提高到0.72，大幅减少了假阳性，揭示了四个先前未报告的漏洞。我们的结果表明，结构化的基于大型语言模型的工作流程可以实质性地增强静态分析工具的有效性。', 'title_zh': '程序分析指南第二部分：来自大语言模型的深刻思考'}
{'arxiv_id': 'arXiv:2504.11707', 'title': 'Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset', 'authors': 'Muhammad Shahid Muneer, Simon S. Woo', 'link': 'https://arxiv.org/abs/2504.11707', 'abstract': 'In the past years, we have witnessed the remarkable success of Text-to-Image (T2I) models and their widespread use on the web. Extensive research in making T2I models produce hyper-realistic images has led to new concerns, such as generating Not-Safe-For-Work (NSFW) web content and polluting the web society. To help prevent misuse of T2I models and create a safer web environment for users features like NSFW filters and post-hoc security checks are used in these models. However, recent work unveiled how these methods can easily fail to prevent misuse. In particular, adversarial attacks on text and image modalities can easily outplay defensive measures. %Exploiting such leads to the growing concern of preventing adversarial attacks on text and image modalities. Moreover, there is currently no robust multimodal NSFW dataset that includes both prompt and image pairs and adversarial examples. This work proposes a million-scale prompt and image dataset generated using open-source diffusion models. Second, we develop a multimodal defense to distinguish safe and NSFW text and images, which is robust against adversarial attacks and directly alleviates current challenges. Our extensive experiments show that our model performs well against existing SOTA NSFW detection methods in terms of accuracy and recall, drastically reducing the Attack Success Rate (ASR) in multimodal adversarial attack scenarios. Code: this https URL.', 'abstract_zh': '近年来，我们见证了文本到图像（T2I）模型的显著成功及其在网上的广泛应用。为使T2I模型生成超现实图像的广泛研究已引发新的担忧，如生成不适合工作（NSFW）的网络内容和污染网络社会。为帮助防止T2I模型的滥用并为用户提供更安全的网络环境，这些模型中使用了像NSFW滤镜和事后安全检查这样的功能。然而，近期研究表明，这些方法容易失效。特别是，对文本和图像模态的对抗攻击可以轻易地超越防御措施。利用这些攻击的方法引发了防止对文本和图像模态进行对抗攻击的关注。此外，目前尚无包含提示和图像对以及对抗示例的稳健多模态NSFW数据集。本工作提出一个基于开源扩散模型生成的百万规模提示和图像数据集。其次，我们开发了一种多模态防御方法，用于区分安全和NSFW的文本和图像，并且对对抗攻击表现出鲁棒性，直接缓解了当前的挑战。我们的实验表明，与现有的顶级NSFW检测方法相比，在准确率和召回率方面，我们的模型表现出色，大幅降低了多模态对抗攻击场景中的攻击成功率（ASR）。代码：![](this https URL)', 'title_zh': '面向网络的合成图像安全生成：多模态鲁棒不适图防护及千万规模数据集'}
{'arxiv_id': 'arXiv:2504.11703', 'title': 'Progent: Programmable Privilege Control for LLM Agents', 'authors': 'Tianneng Shi, Jingxuan He, Zhun Wang, Linyu Wu, Hongwei Li, Wenbo Guo, Dawn Song', 'link': 'https://arxiv.org/abs/2504.11703', 'abstract': 'LLM agents are an emerging form of AI systems where large language models (LLMs) serve as the central component, utilizing a diverse set of tools to complete user-assigned tasks. Despite their great potential, LLM agents pose significant security risks. When interacting with the external world, they may encounter malicious commands from attackers, leading to the execution of dangerous actions. A promising way to address this is by enforcing the principle of least privilege: allowing only essential actions for task completion while blocking unnecessary ones. However, achieving this is challenging, as it requires covering diverse agent scenarios while preserving both security and utility.\nWe introduce Progent, the first privilege control mechanism for LLM agents. At its core is a domain-specific language for flexibly expressing privilege control policies applied during agent execution. These policies provide fine-grained constraints over tool calls, deciding when tool calls are permissible and specifying fallbacks if they are not. This enables agent developers and users to craft suitable policies for their specific use cases and enforce them deterministically to guarantee security. Thanks to its modular design, integrating Progent does not alter agent internals and requires only minimal changes to agent implementation, enhancing its practicality and potential for widespread adoption. To automate policy writing, we leverage LLMs to generate policies based on user queries, which are then updated dynamically for improved security and utility. Our extensive evaluation shows that it enables strong security while preserving high utility across three distinct scenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we perform an in-depth analysis, showcasing the effectiveness of its core components and the resilience of its automated policy generation against adaptive attacks.', 'abstract_zh': '基于LLM的代理的特权控制机制：Progent', 'title_zh': 'Progent: LLM代理的可编程特权控制'}
{'arxiv_id': 'arXiv:2504.11686', 'title': 'Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics', 'authors': 'Yiran He, Yun Cao, Bowen Yang, Zeyu Zhang', 'link': 'https://arxiv.org/abs/2504.11686', 'abstract': 'The rapid development of generative AI facilitates content creation and makes image manipulation easier and more difficult to detect. While multimodal Large Language Models (LLMs) have encoded rich world knowledge, they are not inherently tailored for combating AI-generated Content (AIGC) and struggle to comprehend local forgery details. In this work, we investigate the application of multimodal LLMs in forgery detection. We propose a framework capable of evaluating image authenticity, localizing tampered regions, providing evidence, and tracing generation methods based on semantic tampering clues. Our method demonstrates that the potential of LLMs in forgery analysis can be effectively unlocked through meticulous prompt engineering and the application of few-shot learning techniques. We conduct qualitative and quantitative experiments and show that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in LaMa, which is competitive with state-of-the-art AIGC detection methods. We further discuss the limitations of multimodal LLMs in such tasks and propose potential improvements.', 'abstract_zh': '生成式AI的快速发展促进了内容创作并使得图像操纵更加容易且更难以检测。尽管多模态大型语言模型（LLM）已嵌入丰富的世界知识，但它们并非天生适合对抗AI生成内容（AIGC），难以理解局部篡改细节。本文探讨了多模态LLM在伪造检测中的应用。我们提出了一种框架，能够评估图像的真实性、定位篡改区域、提供证据并基于语义篡改线索追踪生成方法。我们的方法表明，通过细致的提示工程和少样本学习技术的应用，可以有效解锁LLM在伪造分析中的潜在能力。我们进行了定性和定量实验，表明GPT4V在Autosplice中的准确率为92.1%，在LaMa中的准确率为86.3%，其性能与最先进的AIGC检测方法相当。我们进一步讨论了多模态LLM在这些任务中的局限性，并提出了潜在的改进方案。', 'title_zh': 'GPT能告诉我们这些图像为何是合成的吗？赋能多模态大型语言模型在 forensic 领域的应用'}
{'arxiv_id': 'arXiv:2504.11658', 'title': 'Improving LLM Interpretability and Performance via Guided Embedding Refinement for Sequential Recommendation', 'authors': 'Nanshan Jia, Chenfei Yuan, Yuhang Wu, Zeyu Zheng', 'link': 'https://arxiv.org/abs/2504.11658', 'abstract': 'The fast development of Large Language Models (LLMs) offers growing opportunities to further improve sequential recommendation systems. Yet for some practitioners, integrating LLMs to their existing base recommendation systems raises questions about model interpretability, transparency and related safety. To partly alleviate challenges from these questions, we propose guided embedding refinement, a method that carries out a guided and interpretable usage of LLM to enhance the embeddings associated with the base recommendation system. Instead of directly using LLMs as the backbone of sequential recommendation systems, we utilize them as auxiliary tools to emulate the sales logic of recommendation and generate guided embeddings that capture domain-relevant semantic information on interpretable attributes. Benefiting from the strong generalization capabilities of the guided embedding, we construct refined embedding by using the guided embedding and reduced-dimension version of the base embedding. We then integrate the refined embedding into the recommendation module for training and inference. A range of numerical experiments demonstrate that guided embedding is adaptable to various given existing base embedding models, and generalizes well across different recommendation tasks. The numerical results show that the refined embedding not only improves recommendation performance, achieving approximately $10\\%$ to $50\\%$ gains in Mean Reciprocal Rank (MRR), Recall rate, and Normalized Discounted Cumulative Gain (NDCG), but also enhances interpretability, as evidenced by case studies.', 'abstract_zh': '大型语言模型的快速发展为进一步改进序列推荐系统提供了越来越多的机会。然而，对于一些从业者来说，将大型语言模型集成到现有的基础推荐系统中引发了关于模型可解释性、透明性和相关安全性的担忧。为部分缓解这些问题，我们提出了一种引导嵌入精炼的方法，该方法通过一种指导性和可解释的方式来利用大型语言模型以增强与基础推荐系统关联的嵌入。我们不直接将大型语言模型作为序列推荐系统的主体，而是将其作为辅助工具来模拟推荐的销售逻辑，并生成能够捕捉可解释属性上领域相关语义信息的引导嵌入。得益于引导嵌入的强大泛化能力，我们通过使用引导嵌入和基础嵌入的降维版本构建精炼嵌入。然后，我们将精炼嵌入集成到推荐模块进行训练和推理。一系列数值实验表明，引导嵌入能够适应各种现有的基础嵌入模型，并在不同的推荐任务中表现出良好的泛化能力。数值结果表明，精炼嵌入不但改善了推荐性能，使平均倒数排名（MRR）、召回率和归一化累积收益（NDCG）分别提高了大约10%到50%，而且还提升了可解释性，这一点通过案例研究得到了证实。', 'title_zh': '通过引导嵌入精炼提高LLM解释性和序列推荐性能'}
{'arxiv_id': 'arXiv:2504.11650', 'title': 'Data driven approach towards more efficient Newton-Raphson power flow calculation for distribution grids', 'authors': 'Shengyuan Yan, Farzad Vazinram, Zeynab Kaseb, Lindsay Spoor, Jochen Stiasny, Betul Mamudi, Amirhossein Heydarian Ardakani, Ugochukwu Orji, Pedro P. Vergara, Yu Xiang, Jerry Guo', 'link': 'https://arxiv.org/abs/2504.11650', 'abstract': 'Power flow (PF) calculations are fundamental to power system analysis to ensure stable and reliable grid operation. The Newton-Raphson (NR) method is commonly used for PF analysis due to its rapid convergence when initialized properly. However, as power grids operate closer to their capacity limits, ill-conditioned cases and convergence issues pose significant challenges. This work, therefore, addresses these challenges by proposing strategies to improve NR initialization, hence minimizing iterations and avoiding divergence. We explore three approaches: (i) an analytical method that estimates the basin of attraction using mathematical bounds on voltages, (ii) Two data-driven models leveraging supervised learning or physics-informed neural networks (PINNs) to predict optimal initial guesses, and (iii) a reinforcement learning (RL) approach that incrementally adjusts voltages to accelerate convergence. These methods are tested on benchmark systems. This research is particularly relevant for modern power systems, where high penetration of renewables and decentralized generation require robust and scalable PF solutions. In experiments, all three proposed methods demonstrate a strong ability to provide an initial guess for Newton-Raphson method to converge with fewer steps. The findings provide a pathway for more efficient real-time grid operations, which, in turn, support the transition toward smarter and more resilient electricity networks.', 'abstract_zh': '基于幂流计算中牛顿-拉夫森方法初始化改进策略的研究', 'title_zh': '基于数据驱动的方法以提高配电网络牛顿-拉夫逊潮流计算效率'}
{'arxiv_id': 'arXiv:2504.11645', 'title': 'Achieving Tighter Finite-Time Rates for Heterogeneous Federated Stochastic Approximation under Markovian Sampling', 'authors': 'Feng Zhu, Aritra Mitra, Robert W. Heath', 'link': 'https://arxiv.org/abs/2504.11645', 'abstract': "Motivated by collaborative reinforcement learning (RL) and optimization with time-correlated data, we study a generic federated stochastic approximation problem involving $M$ agents, where each agent is characterized by an agent-specific (potentially nonlinear) local operator. The goal is for the agents to communicate intermittently via a server to find the root of the average of the agents' local operators. The generality of our setting stems from allowing for (i) Markovian data at each agent and (ii) heterogeneity in the roots of the agents' local operators. The limited recent work that has accounted for both these features in a federated setting fails to guarantee convergence to the desired point or to show any benefit of collaboration; furthermore, they rely on projection steps in their algorithms to guarantee bounded iterates. Our work overcomes each of these limitations. We develop a novel algorithm titled \\texttt{FedHSA}, and prove that it guarantees convergence to the correct point, while enjoying an $M$-fold linear speedup in sample-complexity due to collaboration. To our knowledge, \\emph{this is the first finite-time result of its kind}, and establishing it (without relying on a projection step) entails a fairly intricate argument that accounts for the interplay between complex temporal correlations due to Markovian sampling, multiple local steps to save communication, and the drift-effects induced by heterogeneous local operators. Our results have implications for a broad class of heterogeneous federated RL problems (e.g., policy evaluation and control) with function approximation, where the agents' Markov decision processes can differ in their probability transition kernels and reward functions.", 'abstract_zh': '受协作强化学习（RL）和具有时间相关数据的优化的启发，我们研究了一个通用的联邦随机逼近问题，涉及 $M$ 个代理，每个代理由一个特定于代理（可能是非线性的）局部操作符描述。目标是通过服务器使代理间间歇性通信，找到各局部操作符平均值的根。我们的设置之所以通用，是因为它允许每个代理具有马尔可夫数据，并且代理局部操作符的根具有异质性。近期有限的研究虽然在这两个特征上有所考虑，但在联邦设置中未能保证收敛到所需点，也未能展示合作的好处；此外，它们依赖投影步骤来保证迭代值的有界性。我们的工作克服了这些限制。我们开发了一种新颖的算法，命名为 \\texttt{FedHSA}，并证明该算法能够保证收敛到正确的点，同时由于合作而使样本复杂度提升 $M$ 倍。据我们所知，这是首个此类有限时间的结果，并且不依赖于投影步骤建立这一结果需要相当复杂的方法来解决马尔可夫采样引起的复杂时序相关性、节省通信的多步局部操作以及异质局部操作符诱导的漂移效应之间的互动问题。我们的结果对具有函数逼近的异质联邦RL问题（例如策略评估和控制）具有重要意义，其中代理的马尔可夫决策过程可能在转移概率核和奖励函数上有所不同。', 'title_zh': '实现更具紧致性的有限时间速率异构联邦随机逼近在马尔可夫采样下的研究'}
{'arxiv_id': 'arXiv:2504.11626', 'title': 'Improving Instruct Models for Free: A Study on Partial Adaptation', 'authors': 'Ozan İrsoy, Pengxiang Cheng, Jennifer L. Chen, Daniel Preoţiuc-Pietro, Shiyue Zhang, Duccio Pappadopulo', 'link': 'https://arxiv.org/abs/2504.11626', 'abstract': 'Instruct models, obtained from various instruction tuning or post-training steps, are commonly deemed superior and more usable than their base counterpart. While the model gains instruction following ability, instruction tuning may lead to forgetting the knowledge from pre-training or it may encourage the model being overly conversational or verbose. This, in turn, can lead to degradation of in-context few-shot learning performance. In this work, we study the performance trajectory between base and instruct models by scaling down the strength of instruction-tuning via the partial adaption method. We show that, across several model families and model sizes, reducing the strength of instruction-tuning results in material improvement on a few-shot in-context learning benchmark covering a variety of classic natural language tasks. This comes at the cost of losing some degree of instruction following ability as measured by AlpacaEval. Our study shines light on the potential trade-off between in-context learning and instruction following abilities that is worth considering in practice.', 'abstract_zh': '通过部分适配方法减弱指令调优强度，我们研究了基模型和指令模型之间的性能轨迹，并展示了在多种模型家族和模型规模下，减弱指令调优强度在涵盖多种经典自然语言任务的少量上下文few-shot学习基准上取得了实质性的改进，这在AlpacaEval衡量的指令遵循能力方面会有所损失。我们的研究揭示了在实践中值得考虑的上下文学习能力和指令遵循能力之间的潜在权衡。', 'title_zh': '免费提升指令模型性能：关于部分适应的研究'}
{'arxiv_id': 'arXiv:2504.11623', 'title': 'Possibility for Proactive Anomaly Detection', 'authors': 'Jinsung Jeon, Jaehyeon Park, Sewon Park, Jeongwhan Choi, Minjung Kim, Noseong Park', 'link': 'https://arxiv.org/abs/2504.11623', 'abstract': 'Time-series anomaly detection, which detects errors and failures in a workflow, is one of the most important topics in real-world applications. The purpose of time-series anomaly detection is to reduce potential damages or losses. However, existing anomaly detection models detect anomalies through the error between the model output and the ground truth (observed) value, which makes them impractical. In this work, we present a \\textit{proactive} approach for time-series anomaly detection based on a time-series forecasting model specialized for anomaly detection and a data-driven anomaly detection model. Our proactive approach establishes an anomaly threshold from training data with a data-driven anomaly detection model, and anomalies are subsequently detected by identifying predicted values that exceed the anomaly threshold. In addition, we extensively evaluated the model using four anomaly detection benchmarks and analyzed both predictable and unpredictable anomalies. We attached the source code as supplementary material.', 'abstract_zh': '基于时间序列预测模型和数据驱动异常检测模型的主动时间序列异常检测方法', 'title_zh': '主动性异常检测的可能性'}
{'arxiv_id': 'arXiv:2504.11609', 'title': 'Towards Interpretable Deep Generative Models via Causal Representation Learning', 'authors': 'Gemma E. Moran, Bryon Aragam', 'link': 'https://arxiv.org/abs/2504.11609', 'abstract': 'Recent developments in generative artificial intelligence (AI) rely on machine learning techniques such as deep learning and generative modeling to achieve state-of-the-art performance across wide-ranging domains. These methods\' surprising performance is due in part to their ability to learn implicit "representations\'\' of complex, multi-modal data. Unfortunately, deep neural networks are notoriously black boxes that obscure these representations, making them difficult to interpret or analyze. To resolve these difficulties, one approach is to build new interpretable neural network models from the ground up. This is the goal of the emerging field of causal representation learning (CRL) that uses causality as a vector for building flexible, interpretable, and transferable generative AI. CRL can be seen as a culmination of three intrinsically statistical problems: (i) latent variable models such as factor analysis; (ii) causal graphical models with latent variables; and (iii) nonparametric statistics and deep learning. This paper reviews recent progress in CRL from a statistical perspective, focusing on connections to classical models and statistical and causal identifiablity results. This review also highlights key application areas, implementation strategies, and open statistical questions in CRL.', 'abstract_zh': '近期生成人工智能（AI）的发展依赖于深度学习和生成建模等机器学习技术，在广泛的领域中实现了最先进的性能。这些方法令人惊讶的性能部分归因于它们能够学习复杂、多模态数据的隐式“表示”。不幸的是，深度神经网络通常是黑盒子，模糊了这些表示，使其难以解释或分析。为了解决这些问题，一种方法是自底向上构建新的可解释神经网络模型。这正是因果表示学习（CRL）新兴领域的目标，该领域利用因果关系作为构建灵活、可解释和可迁移的生成AI的向量。CRL 可以被视为三个内在统计问题的综合：（i）潜在变量模型，如因子分析；（ii）带有潜在变量的因果图模型；以及（iii）非参数统计和深度学习。本文从统计学的角度回顾了 CRL 的近期进展，重点关注与经典模型的联系以及统计和因果可识别性结果。该回顾还强调了 CRL 的关键应用领域、实施策略以及开放的统计问题。', 'title_zh': '通过因果表示学习实现可解释的深度生成模型'}
{'arxiv_id': 'arXiv:2504.11588', 'title': 'Deep Learning Approaches for Medical Imaging Under Varying Degrees of Label Availability: A Comprehensive Survey', 'authors': 'Siteng Ma, Honghui Du, Yu An, Jing Wang, Qinqin Wang, Haochang Wu, Aonghus Lawlor, Ruihai Dong', 'link': 'https://arxiv.org/abs/2504.11588', 'abstract': 'Deep learning has achieved significant breakthroughs in medical imaging, but these advancements are often dependent on large, well-annotated datasets. However, obtaining such datasets poses a significant challenge, as it requires time-consuming and labor-intensive annotations from medical experts. Consequently, there is growing interest in learning paradigms such as incomplete, inexact, and absent supervision, which are designed to operate under limited, inexact, or missing labels. This survey categorizes and reviews the evolving research in these areas, analyzing around 600 notable contributions since 2018. It covers tasks such as image classification, segmentation, and detection across various medical application areas, including but not limited to brain, chest, and cardiac imaging. We attempt to establish the relationships among existing research studies in related areas. We provide formal definitions of different learning paradigms and offer a comprehensive summary and interpretation of various learning mechanisms and strategies, aiding readers in better understanding the current research landscape and ideas. We also discuss potential future research challenges.', 'abstract_zh': '深度学习在医学影像领域的显著突破往往依赖于大规模且well-注释的数据集，然而获取这样的数据集面临着巨大的挑战，因为它需要耗时且劳动密集型的医学专家注释。因此，在有限、不精确或缺失标签的条件下，使用不完备、不精确和无监督学习范式的兴趣日益增长。本文对这些领域的研究进行分类和综述，自2018年以来分析了约600项重要的贡献。涵盖了包括但不限于脑部、胸部和心脏影像在内的各种医学应用领域的图像分类、分割和检测任务。我们试图建立现有研究之间的关系。我们为不同学习范式提供正式定义，并给出各种学习机制和策略的全面总结与解释，帮助读者更好地理解当前的研究景观和理念。我们还讨论了潜在的未来研究挑战。', 'title_zh': '深学习方法在不同标注程度下的医学影像应用：综述'}
{'arxiv_id': 'arXiv:2504.11575', 'title': 'MULTI-LF: A Unified Continuous Learning Framework for Real-Time DDoS Detection in Multi-Environment Networks', 'authors': 'Furqan Rustam, Islam Obaidat, Anca Delia Jurcut', 'link': 'https://arxiv.org/abs/2504.11575', 'abstract': 'Detecting Distributed Denial of Service (DDoS) attacks in Multi-Environment (M-En) networks presents significant challenges due to diverse malicious traffic patterns and the evolving nature of cyber threats. Existing AI-based detection systems struggle to adapt to new attack strategies and lack real-time attack detection capabilities with high accuracy and efficiency. This study proposes an online, continuous learning methodology for DDoS detection in M-En networks, enabling continuous model updates and real-time adaptation to emerging threats, including zero-day attacks. First, we develop a unique M-En network dataset by setting up a realistic, real-time simulation using the NS-3 tool, incorporating both victim and bot devices. DDoS attacks with varying packet sizes are simulated using the DDoSim application across IoT and traditional IP-based environments under M-En network criteria. Our approach employs a multi-level framework (MULTI-LF) featuring two machine learning models: a lightweight Model 1 (M1) trained on a selective, critical packet dataset for fast and efficient initial detection, and a more complex, highly accurate Model 2 (M2) trained on extensive data. When M1 exhibits low confidence in its predictions, the decision is escalated to M2 for verification and potential fine-tuning of M1 using insights from M2. If both models demonstrate low confidence, the system flags the incident for human intervention, facilitating model updates with human-verified categories to enhance adaptability to unseen attack patterns. We validate the MULTI-LF through real-world simulations, demonstrating superior classification accuracy of 0.999 and low prediction latency of 0.866 seconds compared to established baselines. Furthermore, we evaluate performance in terms of memory usage (3.632 MB) and CPU utilization (10.05%) in real-time scenarios.', 'abstract_zh': '检测多环境网络中的分布式拒绝服务（DDoS）攻击存在显著挑战，由于恶意流量模式多样和网络威胁的演变性。现有的基于AI的检测系统难以适应新的攻击策略，缺乏高准确性和高效性的实时攻击检测能力。本研究提出了一个在线持续学习方法，用于多环境网络中的DDoS检测，能够实现持续模型更新和对新兴威胁，包括零日攻击的实时适应。首先，我们利用NS-3工具搭建了一个现实且实时的模拟环境，包含了受害者和僵尸设备。使用DDoSim应用程序在符合多环境网络标准的物联网和传统IP环境中模拟了不同包大小的DDoS攻击。我们的方法采用多级框架（MULTI-LF），包含两个机器学习模型：轻量级的模型1（M1），用于快速高效地初始检测，以及更为复杂且准确的模型2（M2），用于全面数据训练。当M1预测不确定性较高时，决策将被提升到M2进行验证和可能的M1微调以利用M2的见解。如果两个模型的不确定性都较高，系统将该事件标记为需要人工干预，并通过人工验证的类别提升模型更新，以增强对未知攻击模式的适应能力。我们通过实际仿真验证了MULTI-LF，结果显示其分类精度高达0.999，预测延迟仅为0.866秒，优于现有基准。此外，我们还在实际场景中评估了其内存使用（3.632 MB）和CPU利用率（10.05%）。', 'title_zh': 'MULTI-LF：多环境网络中实时DDoS检测的统一连续学习框架'}
{'arxiv_id': 'arXiv:2504.11564', 'title': 'Perceptions of Agentic AI in Organizations: Implications for Responsible AI and ROI', 'authors': 'Lee Ackerman', 'link': 'https://arxiv.org/abs/2504.11564', 'abstract': 'As artificial intelligence (AI) systems rapidly gain autonomy, the need for robust responsible AI frameworks becomes paramount. This paper investigates how organizations perceive and adapt such frameworks amidst the emerging landscape of increasingly sophisticated agentic AI. Employing an interpretive qualitative approach, the study explores the lived experiences of AI professionals. Findings highlight that the inherent complexity of agentic AI systems and their responsible implementation, rooted in the intricate interconnectedness of responsible AI dimensions and the thematic framework (an analytical structure developed from the data), combined with the novelty of agentic AI, contribute to significant challenges in organizational adaptation, characterized by knowledge gaps, a limited emphasis on stakeholder engagement, and a strong focus on control. These factors, by hindering effective adaptation and implementation, ultimately compromise the potential for responsible AI and the realization of ROI.', 'abstract_zh': '随着人工智能（AI）系统迅速获得自主性，建立 robust 责任 AI 框架的需求变得至关重要。本文考察了组织在日益复杂的具身AI新兴景观中如何看待和适应这些框架的方式。通过一种解释性的定性方法，研究探讨了AI专业人员的 lived experiences。研究发现，具身AI系统的固有复杂性及其负责任实施的挑战，源于负责任AI维度和主题框架（从数据中开发的分析结构）的复杂交织关系，以及具身AI的新颖性，导致了组织适应中的显著挑战，包括知识空白、对利益相关者参与的重视不足以及对控制的强烈关注。这些因素通过阻碍有效的适应和实施，最终损害了责任AI及其回报的实现潜力。', 'title_zh': '组织中代理型人工智能的感知：负责任的人工智能与投资回报的影响'}
{'arxiv_id': 'arXiv:2504.11558', 'title': 'Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism', 'authors': 'Mete Erdogan, Cengiz Pehlevan, Alper T. Erdogan', 'link': 'https://arxiv.org/abs/2504.11558', 'abstract': 'We introduce the Error Broadcast and Decorrelation (EBD) algorithm, a novel learning framework that addresses the credit assignment problem in neural networks by directly broadcasting output error to individual layers. Leveraging the stochastic orthogonality property of the optimal minimum mean square error (MMSE) estimator, EBD defines layerwise loss functions to penalize correlations between layer activations and output errors, offering a principled approach to error broadcasting without the need for weight transport. The optimization framework naturally leads to the experimentally observed three-factor learning rule and integrates with biologically plausible frameworks to enhance performance and plausibility. Numerical experiments demonstrate that EBD achieves performance comparable to or better than known error-broadcast methods on benchmark datasets. While the scalability of EBD to very large or complex datasets remains to be further explored, our findings suggest it provides a biologically plausible, efficient, and adaptable alternative for neural network training. This approach could inform future advancements in artificial and natural learning paradigms.', 'abstract_zh': '我们介绍了Error Broadcast and Decorrelation (EBD)算法，这是一种通过直接广播输出误差到各个层来解决神经网络中信用分配问题的新颖学习框架。EBD利用最优最小均方误差（MMSE）估计器的随机正交性属性，通过定义层间损失函数来惩罚层激活与输出误差之间的相关性，从而提供了一种无需权重传输的有原则的误差广播方法。该优化框架自然地导致了实验中观察到的三因素学习规则，并与生物可实现框架集成，以提高性能和可实现性。数值实验表明，EBD在基准数据集上的性能与已知的误差广播方法相当或更好。虽然EBD在非常大规模或复杂数据集上的可扩展性仍需进一步探索，但我们的发现表明，EBD提供了一种生物可实现、高效且灵活的神经网络训练替代方案，该方法可能为未来的人工和自然学习范式提供指导。', 'title_zh': '误差广播与去相关作为潜在的人工和自然学习机制'}
{'arxiv_id': 'arXiv:2504.11536', 'title': 'ReTool: Reinforcement Learning for Strategic Tool Use in LLMs', 'authors': 'Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, Wanjun Zhong', 'link': 'https://arxiv.org/abs/2504.11536', 'abstract': "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems.", 'abstract_zh': 'ReTool：基于工具集成学习的长期推理增强', 'title_zh': 'ReTool: 强化学习在大语言模型中战略性工具使用中的应用'}
{'arxiv_id': 'arXiv:2504.11511', 'title': 'Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs', 'authors': 'Flint Xiaofeng Fan, Cheston Tan, Roger Wattenhofer, Yew-Soon Ong', 'link': 'https://arxiv.org/abs/2504.11511', 'abstract': 'The rise of reinforcement learning (RL) in critical real-world applications demands a fundamental rethinking of privacy in AI systems. Traditional privacy frameworks, designed to protect isolated data points, fall short for sequential decision-making systems where sensitive information emerges from temporal patterns, behavioral strategies, and collaborative dynamics. Modern RL paradigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in large language models (LLMs), exacerbate these challenges by introducing complex, interactive, and context-dependent learning environments that traditional methods do not address. In this position paper, we argue for a new privacy paradigm built on four core principles: multi-scale protection, behavioral pattern protection, collaborative privacy preservation, and context-aware adaptation. These principles expose inherent tensions between privacy, utility, and interpretability that must be navigated as RL systems become more pervasive in high-stakes domains like healthcare, autonomous vehicles, and decision support systems powered by LLMs. To tackle these challenges, we call for the development of new theoretical frameworks, practical mechanisms, and rigorous evaluation methodologies that collectively enable effective privacy protection in sequential decision-making systems.', 'abstract_zh': '强化学习在关键现实应用中的兴起要求对人工智能系统的隐私进行根本性的重新思考。传统的隐私框架设计用于保护孤立的数据点，在需要从时间模式、行为策略和协作动态中涌现的敏感信息的顺序决策系统中表现不足。现代强化学习范式，如联邦强化学习（FedRL）和大语言模型（LLMs）中的基于人类反馈的强化学习（RLHF），通过引入复杂、交互和上下文相关的学习环境加剧了这些挑战，而传统方法未能解决这些问题。在本文中，我们提出了基于四项核心原则的新隐私范式：多尺度保护、行为模式保护、协作隐私保留和上下文感知适应。这些原则揭示了在医疗保健、自主车辆以及由LLMs驱动的决策支持系统等高风险领域中，隐私、效用和可解释性之间固有的紧张关系，必须加以应对。为了应对这些挑战，我们呼吁开发新的理论框架、实际机制和严格的评估方法，以共同实现对顺序决策系统中有效隐私保护的支持。', 'title_zh': '位置论文：在大语言模型时代重新思考RL中的顺序决策隐私问题'}
{'arxiv_id': 'arXiv:2504.11510', 'title': 'RAID: An In-Training Defense against Attribute Inference Attacks in Recommender Systems', 'authors': 'Xiaohua Feng, Yuyuan Li, Fengyuan Yu, Ke Xiong, Junjie Fang, Li Zhang, Tianyu Du, Chaochao Chen', 'link': 'https://arxiv.org/abs/2504.11510', 'abstract': 'In various networks and mobile applications, users are highly susceptible to attribute inference attacks, with particularly prevalent occurrences in recommender systems. Attackers exploit partially exposed user profiles in recommendation models, such as user embeddings, to infer private attributes of target users, such as gender and political views. The goal of defenders is to mitigate the effectiveness of these attacks while maintaining recommendation performance. Most existing defense methods, such as differential privacy and attribute unlearning, focus on post-training settings, which limits their capability of utilizing training data to preserve recommendation performance. Although adversarial training extends defenses to in-training settings, it often struggles with convergence due to unstable training processes. In this paper, we propose RAID, an in-training defense method against attribute inference attacks in recommender systems. In addition to the recommendation objective, we define a defensive objective to ensure that the distribution of protected attributes becomes independent of class labels, making users indistinguishable from attribute inference attacks. Specifically, this defensive objective aims to solve a constrained Wasserstein barycenter problem to identify the centroid distribution that makes the attribute indistinguishable while complying with recommendation performance constraints. To optimize our proposed objective, we use optimal transport to align users with the centroid distribution. We conduct extensive experiments on four real-world datasets to evaluate RAID. The experimental results validate the effectiveness of RAID and demonstrate its significant superiority over existing methods in multiple aspects.', 'abstract_zh': '在各种网络和移动应用中，用户高度容易受到属性推理攻击的影响，特别是在推荐系统中尤为普遍。攻击者利用推荐模型中部分暴露的用户画像，如用户嵌入，来推断目标用户的私人属性，例如性别和政治观点。防御方的目标是在保持推荐性能的情况下，减轻这些攻击的有效性。现有的大多数防御方法，如差分隐私和属性遗忘，主要关注于训练后设置，这限制了它们利用训练数据来保持推荐性能的能力。尽管对抗训练将防御扩展到训练中设置，但往往由于不稳定的训练过程而在收敛性方面遇到困难。在本文中，我们提出了一种针对推荐系统中属性推理攻击的训练中防御方法RAID。除了推荐目标外，我们定义了一个防御目标，确保保护属性的分布与类别标签无关，从而使用户在属性推理攻击中不可区分。具体而言，该防御目标旨在解决一个受限的Wasserstein巴尔扎克问题，以确定使得属性不可区分的重心分布，同时遵守推荐性能约束条件。为了优化我们提出的目标，我们使用最优传输将用户与重心分布对齐。我们在四个现实世界的数据集上进行了广泛的实验以评估RAID。实验结果验证了RAID的有效性，并在多个方面展示了其相对于现有方法的显著优越性。', 'title_zh': 'RAID：推荐系统中对抗属性推断攻击的在训练防御方法'}
{'arxiv_id': 'arXiv:2504.11501', 'title': 'A Framework for the Private Governance of Frontier Artificial Intelligence', 'authors': 'Dean W. Ball', 'link': 'https://arxiv.org/abs/2504.11501', 'abstract': 'This paper presents a proposal for the governance of frontier AI systems through a hybrid public-private system. Private bodies, authorized and overseen by government, provide certifications to developers of frontier AI systems on an opt-in basis. In exchange for opting in, frontier AI firms receive protections from tort liability for customer misuse of their models. Before detailing the proposal, the paper explores more commonly discussed approaches to AI governance, analyzing their strengths and flaws. It also examines the nature of frontier AI governance itself. The paper includes consideration of the political economic, institutional, legal, safety, and other merits and tradeoffs inherent in the governance system it proposes.', 'abstract_zh': '本文提出了一种通过混合公私体系治理前沿人工智能系统的提案。在政府授权和监督下，私营机构为前沿人工智能系统的开发者提供可选认证。作为交换，前沿人工智能企业将获得对其模型客户误用的侵权责任保护。在详细阐述提案之前，本文探讨了更常讨论的人工智能治理方法，分析了它们的优点和缺点。同时，本文还考察了前沿人工智能治理本身的性质。本文还考虑了其所提议的治理体系在政治经济、制度、法律、安全以及其他方面的优势权衡。', 'title_zh': '前沿人工智能的私密治理框架'}
{'arxiv_id': 'arXiv:2504.11500', 'title': 'TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification', 'authors': 'Kaicong Huang, Talha Azfar, Jack Reilly, Ruimin Ke', 'link': 'https://arxiv.org/abs/2504.11500', 'abstract': 'Transit Origin-Destination (OD) data are essential for transit planning, particularly in route optimization and demand-responsive paratransit systems. Traditional methods, such as manual surveys, are costly and inefficient, while Bluetooth and WiFi-based approaches require passengers to carry specific devices, limiting data coverage. On the other hand, most transit vehicles are equipped with onboard cameras for surveillance, offering an opportunity to repurpose them for edge-based OD data collection through visual person re-identification (ReID). However, such approaches face significant challenges, including severe occlusion and viewpoint variations in transit environments, which greatly reduce matching accuracy and hinder their adoption. Moreover, designing effective algorithms that can operate efficiently on edge devices remains an open challenge. To address these challenges, we propose TransitReID, a novel framework for individual-level transit OD data collection. TransitReID consists of two key components: (1) An occlusion-robust ReID algorithm featuring a variational autoencoder guided region-attention mechanism that adaptively focuses on visible body regions through reconstruction loss-optimized weight allocation; and (2) a Hierarchical Storage and Dynamic Matching (HSDM) mechanism specifically designed for efficient and robust transit OD matching which balances storage, speed, and accuracy. Additionally, a multi-threaded design supports near real-time operation on edge devices, which also ensuring privacy protection. We also introduce a ReID dataset tailored for complex bus environments to address the lack of relevant training data. Experimental results demonstrate that TransitReID achieves state-of-the-art performance in ReID tasks, with an accuracy of approximately 90\\% in bus route simulations.', 'abstract_zh': '基于传输的起源-目的地（OD）数据对于传输规划至关重要，特别是在路线优化和响应式交通辅助系统中。传统的方法，如手工调查，成本高且效率低下，而基于Bluetooth和WiFi的方法需要乘客携带特定设备，限制了数据覆盖范围。另一方面，大多数公共交通车辆都配备了用于监控的车内摄像头，为通过视觉再识别（ReID）在边缘设备上重新利用这些摄像头进行OD数据收集提供了机会。然而，此类方法面临严重遮挡和视角变化等重大挑战，这大大降低了匹配精度，阻碍了其广泛应用。此外，设计能在边缘设备上高效运行的有效算法仍然是一个开放性挑战。为了解决这些挑战，我们提出了TransitReID，一种新颖的个体级别传输OD数据收集框架。TransitReID 包含两个关键组件：（1）一种鲁棒性的ReID算法，采用变分自编码器引导区域注意力机制，通过重建损失优化权重分配，自适应地关注可见的身体区域；（2）一种层次存储和动态匹配（HSDM）机制，专门设计用于高效稳健的传输OD匹配，平衡存储、速度和准确性。此外，多线程设计支持近实时操作，同时确保隐私保护。我们还引入了一个专门针对复杂公交环境的ReID数据集，以解决相关训练数据不足的问题。实验结果表明，TransitReID 在ReID任务中达到了最先进的性能，在公交路线模拟中准确率达到约90%。', 'title_zh': 'TransitReID：具有遮挡抵抗的动态乘客再识别的公交OD数据采集'}
{'arxiv_id': 'arXiv:2504.11493', 'title': 'Toward Aligning Human and Robot Actions via Multi-Modal Demonstration Learning', 'authors': 'Azizul Zahid, Jie Fan, Farong Wang, Ashton Dy, Sai Swaminathan, Fei Liu', 'link': 'https://arxiv.org/abs/2504.11493', 'abstract': 'Understanding action correspondence between humans and robots is essential for evaluating alignment in decision-making, particularly in human-robot collaboration and imitation learning within unstructured environments. We propose a multimodal demonstration learning framework that explicitly models human demonstrations from RGB video with robot demonstrations in voxelized RGB-D space. Focusing on the "pick and place" task from the RH20T dataset, we utilize data from 5 users across 10 diverse scenes. Our approach combines ResNet-based visual encoding for human intention modeling and a Perceiver Transformer for voxel-based robot action prediction. After 2000 training epochs, the human model reaches 71.67% accuracy, and the robot model achieves 71.8% accuracy, demonstrating the framework\'s potential for aligning complex, multimodal human and robot behaviors in manipulation tasks.', 'abstract_zh': '理解人类与机器人之间的动作对应对于评估决策一致性至关重要，特别是在不规则环境中的人类-机器人协作和模仿学习中。我们提出了一种多模态示范学习框架，该框架明确地将人类演示从RGB视频建模到体素化RGB-D空间中的机器人演示。以RH20T数据集中“拿起放置”任务为例，我们利用5名用户在10个不同场景下的数据。我们的方法结合了基于ResNet的视觉编码进行人类意图建模，以及基于体素的机器人动作预测的感知器变换器。经过2000个训练周期后，人类模型的准确率为71.67%，机器人模型的准确率为71.8%，展示了该框架在复杂多模态人类和机器人操作行为对齐中的潜力。', 'title_zh': '基于多模态示范学习的人机动作对齐'}
{'arxiv_id': 'arXiv:2504.11482', 'title': 'snnTrans-DHZ: A Lightweight Spiking Neural Network Architecture for Underwater Image Dehazing', 'authors': 'Vidya Sudevan, Fakhreddine Zayer, Rizwana Kausar, Sajid Javed, Hamad Karki, Giulia De Masi, Jorge Dias', 'link': 'https://arxiv.org/abs/2504.11482', 'abstract': 'Underwater image dehazing is critical for vision-based marine operations because light scattering and absorption can severely reduce visibility. This paper introduces snnTrans-DHZ, a lightweight Spiking Neural Network (SNN) specifically designed for underwater dehazing. By leveraging the temporal dynamics of SNNs, snnTrans-DHZ efficiently processes time-dependent raw image sequences while maintaining low power consumption. Static underwater images are first converted into time-dependent sequences by repeatedly inputting the same image over user-defined timesteps. These RGB sequences are then transformed into LAB color space representations and processed concurrently. The architecture features three key modules: (i) a K estimator that extracts features from multiple color space representations; (ii) a Background Light Estimator that jointly infers the background light component from the RGB-LAB images; and (iii) a soft image reconstruction module that produces haze-free, visibility-enhanced outputs. The snnTrans-DHZ model is directly trained using a surrogate gradient-based backpropagation through time (BPTT) strategy alongside a novel combined loss function. Evaluated on the UIEB benchmark, snnTrans-DHZ achieves a PSNR of 21.68 dB and an SSIM of 0.8795, and on the EUVP dataset, it yields a PSNR of 23.46 dB and an SSIM of 0.8439. With only 0.5670 million network parameters, and requiring just 7.42 GSOPs and 0.0151 J of energy, the algorithm significantly outperforms existing state-of-the-art methods in terms of efficiency. These features make snnTrans-DHZ highly suitable for deployment in underwater robotics, marine exploration, and environmental monitoring.', 'abstract_zh': '基于突触神经网络的snnTrans-DHZ水下去雾方法', 'title_zh': 'snnTrans-DHZ：一种用于水下图像去雾的轻量级脉冲神经网络架构'}
{'arxiv_id': 'arXiv:2504.11478', 'title': 'Flux Already Knows - Activating Subject-Driven Image Generation without Training', 'authors': 'Hao Kang, Stathi Fotiadis, Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Min Jin Chong, Xin Lu', 'link': 'https://arxiv.org/abs/2504.11478', 'abstract': 'We propose a simple yet effective zero-shot framework for subject-driven image generation using a vanilla Flux model. By framing the task as grid-based image completion and simply replicating the subject image(s) in a mosaic layout, we activate strong identity-preserving capabilities without any additional data, training, or inference-time fine-tuning. This "free lunch" approach is further strengthened by a novel cascade attention design and meta prompting technique, boosting fidelity and versatility. Experimental results show that our method outperforms baselines across multiple key metrics in benchmarks and human preference studies, with trade-offs in certain aspects. Additionally, it supports diverse edits, including logo insertion, virtual try-on, and subject replacement or insertion. These results demonstrate that a pre-trained foundational text-to-image model can enable high-quality, resource-efficient subject-driven generation, opening new possibilities for lightweight customization in downstream applications.', 'abstract_zh': '基于vanilla Flux模型的简单有效零样本主题驱动图像生成框架', 'title_zh': 'Flux 已经知道 - 无需训练的主体驱动图像生成'}
{'arxiv_id': 'arXiv:2504.11477', 'title': 'SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification', 'authors': 'Yunkai Zhang, Shiyin Wei, Yong Huang, Yawu Su, Shanshan Lu, Hui Li', 'link': 'https://arxiv.org/abs/2504.11477', 'abstract': 'Existing computer vision(CV)-based structural damage identification models demonstrate notable accuracy in categorizing and localizing damage. However, these models present several critical limitations that hinder their practical application in civil engineering(CE). Primarily, their ability to recognize damage types remains constrained, preventing comprehensive analysis of the highly varied and complex conditions encountered in real-world CE structures. Second, these models lack linguistic capabilities, rendering them unable to articulate structural damage characteristics through natural language descriptions. With the continuous advancement of artificial intelligence(AI), large multi-modal models(LMMs) have emerged as a transformative solution, enabling the unified encoding and alignment of textual and visual data. These models can autonomously generate detailed descriptive narratives of structural damage while demonstrating robust generalization across diverse scenarios and tasks. This study introduces SDIGLM, an innovative LMM for structural damage identification, developed based on the open-source VisualGLM-6B architecture. To address the challenge of adapting LMMs to the intricate and varied operating conditions in CE, this work integrates a U-Net-based semantic segmentation module to generate defect segmentation maps as visual Chain of Thought(CoT). Additionally, a multi-round dialogue fine-tuning dataset is constructed to enhance logical reasoning, complemented by a language CoT formed through prompt engineering. By leveraging this multi-modal CoT, SDIGLM surpasses general-purpose LMMs in structural damage identification, achieving an accuracy of 95.24% across various infrastructure types. Moreover, the model effectively describes damage characteristics such as hole size, crack direction, and corrosion severity.', 'abstract_zh': '基于现有计算机视觉(计算机视觉)-基于结构损伤识别模型的显著准确性在分类和定位损伤方面表现出色。然而，这些模型在土木工程(土木工程)中实际应用时呈现出若干关键限制。首先，它们识别损伤类型的能力受限，难以对真实世界土木工程结构中复杂多样的条件进行全面分析。其次，这些模型缺乏语言能力，无法通过自然语言描述来阐述结构损伤特征。随着人工智能的不断进步，多模态大型模型(LMMs)已成为一种变革性解决方案，使文本和视觉数据的统一编码和对齐成为可能。这些模型能够自主生成详细的结构损伤描述，并在多种场景和任务中表现出较强的泛化能力。本研究介绍了基于开源VisualGLM-6B架构的创新LMM——SDIGLM，以解决适应复杂多变的土木工程运行条件的挑战。研究中集成了一个基于U-Net的语义分割模块，生成缺陷分割图作为视觉思维链(Chain of Thought, CoT)。同时，构建了一个多轮对话细调数据集以增强逻辑推理能力，并通过提示工程形成语言CoT。利用这种多模态CoT，SDIGLM在结构损伤识别中的准确性超过通用LMMs，各类基础设施类型的准确率达到95.24%。此外，该模型还能有效描述损伤特征，如孔洞大小、裂缝方向和腐蚀程度。', 'title_zh': 'SDIGLM: 利用大规模语言模型和多模态推理进行结构损伤识别'}
{'arxiv_id': 'arXiv:2504.11474', 'title': 'Local Temporal Feature Enhanced Transformer with ROI-rank Based Masking for Diagnosis of ADHD', 'authors': 'Byunggun Kim, Younghun Kwon', 'link': 'https://arxiv.org/abs/2504.11474', 'abstract': 'In modern society, Attention-Deficit/Hyperactivity Disorder (ADHD) is one of the common mental diseases discovered not only in children but also in adults. In this context, we propose a ADHD diagnosis transformer model that can effectively simultaneously find important brain spatiotemporal biomarkers from resting-state functional magnetic resonance (rs-fMRI). This model not only learns spatiotemporal individual features but also learns the correlation with full attention structures specialized in ADHD diagnosis. In particular, it focuses on learning local blood oxygenation level dependent (BOLD) signals and distinguishing important regions of interest (ROI) in the brain. Specifically, the three proposed methods for ADHD diagnosis transformer are as follows. First, we design a CNN-based embedding block to obtain more expressive embedding features in brain region attention. It is reconstructed based on the previously CNN-based ADHD diagnosis models for the transformer. Next, for individual spatiotemporal feature attention, we change the attention method to local temporal attention and ROI-rank based masking. For the temporal features of fMRI, the local temporal attention enables to learn local BOLD signal features with only simple window masking. For the spatial feature of fMRI, ROI-rank based masking can distinguish ROIs with high correlation in ROI relationships based on attention scores, thereby providing a more specific biomarker for ADHD diagnosis. The experiment was conducted with various types of transformer models. To evaluate these models, we collected the data from 939 individuals from all sites provided by the ADHD-200 competition. Through this, the spatiotemporal enhanced transformer for ADHD diagnosis outperforms the performance of other different types of transformer variants. (77.78ACC 76.60SPE 79.22SEN 79.30AUC)', 'abstract_zh': '现代社会中，注意力缺陷多动障碍（ADHD）是一种不仅在儿童中发现，在成人中也常见的精神疾病。在此背景下，我们提出了一种ADHD诊断变换器模型，能够有效同时从静息状态功能性磁共振成像（rs-fMRI）中发现重要的脑时空生物标志物。该模型不仅学习时空个体内特征，还学习与ADHD诊断专门化全注意结构的相关性。特别是，该模型专注于学习局部血氧水平依赖（BOLD）信号并区分大脑中的重要感兴趣区域（ROI）。具体而言，ADHD诊断变换器提出的三种方法如下。首先，我们设计了一个基于CNN的嵌入块，以获取大脑区域注意力中的更具表达性的嵌入特征。该嵌入块基于之前基于CNN的ADHD诊断模型进行了重构。其次，对于个体时空特征注意力，我们将注意力方法更改为局部时间注意力和基于ROI的掩蔽。对于fMRI的时间特征，局部时间注意力仅通过简单的窗口掩蔽即可学习局部BOLD信号特征。对于fMRI的空间特征，基于ROI的排名掩蔽可以根据注意得分区分ROI关系中具有高相关性的ROI，从而为ADHD诊断提供更具针对性的生物标志物。在不同类型的变换器模型实验中，时空增强的ADHD诊断变换器模型在不同类型的变换器变体中表现最佳（准确率77.78%，特异度76.60%，灵敏度79.22%，AUC 79.30）。', 'title_zh': '基于ROI-rank基于掩码的局部时间特征增强变压器用于ADHD诊断'}
{'arxiv_id': 'arXiv:2504.11473', 'title': 'Visual moral inference and communication', 'authors': 'Warren Zhu, Aida Ramezani, Yang Xu', 'link': 'https://arxiv.org/abs/2504.11473', 'abstract': 'Humans can make moral inferences from multiple sources of input. In contrast, automated moral inference in artificial intelligence typically relies on language models with textual input. However, morality is conveyed through modalities beyond language. We present a computational framework that supports moral inference from natural images, demonstrated in two related tasks: 1) inferring human moral judgment toward visual images and 2) analyzing patterns in moral content communicated via images from public news. We find that models based on text alone cannot capture the fine-grained human moral judgment toward visual stimuli, but language-vision fusion models offer better precision in visual moral inference. Furthermore, applications of our framework to news data reveal implicit biases in news categories and geopolitical discussions. Our work creates avenues for automating visual moral inference and discovering patterns of visual moral communication in public media.', 'abstract_zh': '人类可以从多种输入来源进行道德推理，而人工智能的自动道德推理通常依赖于基于文本的语言模型。然而，道德还通过语言之外的其他模态传达。我们提出了一种计算框架，支持从自然图像进行道德推理，并通过两个相关任务进行了演示：1) 推断人类对视觉图像的道德判断；2) 分析通过公共新闻传播的道德内容模式。我们发现仅基于文本的模型无法捕捉人类对视觉刺激的细微道德判断，但语言-视觉融合模型在视觉道德推理方面提供了更高的精度。此外，将我们的框架应用于新闻数据揭示了新闻类别和地缘政治讨论中的隐含偏见。我们的研究为自动化视觉道德推理和发现公共媒体中视觉道德沟通模式提供了途径。', 'title_zh': '视觉道德推理与沟通'}
{'arxiv_id': 'arXiv:2504.11470', 'title': 'SO-DETR: Leveraging Dual-Domain Features and Knowledge Distillation for Small Object Detection', 'authors': 'Huaxiang Zhang, Hao Zhang, Aoran Mei, Zhongxue Gan, Guo-Niu Zhu', 'link': 'https://arxiv.org/abs/2504.11470', 'abstract': 'Detection Transformer-based methods have achieved significant advancements in general object detection. However, challenges remain in effectively detecting small objects. One key difficulty is that existing encoders struggle to efficiently fuse low-level features. Additionally, the query selection strategies are not effectively tailored for small objects. To address these challenges, this paper proposes an efficient model, Small Object Detection Transformer (SO-DETR). The model comprises three key components: a dual-domain hybrid encoder, an enhanced query selection mechanism, and a knowledge distillation strategy. The dual-domain hybrid encoder integrates spatial and frequency domains to fuse multi-scale features effectively. This approach enhances the representation of high-resolution features while maintaining relatively low computational overhead. The enhanced query selection mechanism optimizes query initialization by dynamically selecting high-scoring anchor boxes using expanded IoU, thereby improving the allocation of query resources. Furthermore, by incorporating a lightweight backbone network and implementing a knowledge distillation strategy, we develop an efficient detector for small objects. Experimental results on the VisDrone-2019-DET and UAVVaste datasets demonstrate that SO-DETR outperforms existing methods with similar computational demands. The project page is available at this https URL.', 'abstract_zh': '基于检测变压器的方法在通用目标检测中取得了显著进展，但在有效检测小型目标方面仍面临挑战。现有编码器难以高效融合低级特征，同时查询选择策略也不适合小型目标。为解决这些挑战，本文提出了一种高效的模型——小型对象检测变压器（SO-DETR）。该模型包含三个关键组成部分：双域混合编码器、增强的查询选择机制以及知识蒸馏策略。双域混合编码器将空间域和频域结合，有效融合多尺度特征，增强高分辨率特征的表示能力，同时保持较低的计算开销。增强的查询选择机制通过动态选择扩展IoU的高分锚框来优化查询初始化，从而提高查询资源的分配。此外，结合轻量级骨干网络并实施知识蒸馏策略，我们开发了一种针对小型目标的高效检测器。实验结果表明，SO-DETR在VisDrone-2019-DET和UAVDataset数据集上的性能优于同类方法。更多信息请参见该项目页面：这个 <https://> 页面。', 'title_zh': 'SO-DETR：利用双域特征和知识蒸馏进行小目标检测'}
{'arxiv_id': 'arXiv:2504.11469', 'title': 'Do Segmentation Models Understand Vascular Structure? A Blob-Based XAI Framework', 'authors': 'Guillaume Garret, Antoine Vacavant, Carole Frindel', 'link': 'https://arxiv.org/abs/2504.11469', 'abstract': 'Deep learning models have achieved impressive performance in medical image segmentation, yet their black-box nature limits clinical adoption. In vascular applications, trustworthy segmentation should rely on both local image cues and global anatomical structures, such as vessel connectivity or branching. However, the extent to which models leverage such global context remains unclear. We present a novel explainability pipeline for 3D vessel segmentation, combining gradient-based attribution with graph-guided point selection and a blob-based analysis of Saliency maps. Using vascular graphs extracted from ground truth, we define anatomically meaningful points of interest (POIs) and assess the contribution of input voxels via Saliency maps. These are analyzed at both global and local scales using a custom blob detector. Applied to IRCAD and Bullitt datasets, our analysis shows that model decisions are dominated by highly localized attribution blobs centered near POIs. Attribution features show little correlation with vessel-level properties such as thickness, tubularity, or connectivity -- suggesting limited use of global anatomical reasoning. Our results underline the importance of structured explainability tools and highlight the current limitations of segmentation models in capturing global vascular context.', 'abstract_zh': '深度学习模型在医学图像分割中取得了显著性能，但其黑盒特性限制了临床应用。在血管应用中，可靠的分割应该依赖于局部图像线索和全局解剖结构，如血管连通性或分支。然而，模型利用这种全局上下文的程度尚不清楚。我们提出了一种新的3D血管分割可解释性管道，结合基于梯度的归因、图导向点选择和基于斑点的Saliency图分析。通过从ground truth提取的血管图，我们定义了解剖学上有意义的兴趣点（POIs），并通过Saliency图评估输入体素的贡献。这些Saliency图在全局和局部尺度上使用自定义的斑点检测器进行分析。在IRCAD和Bullitt数据集上应用我们的分析显示，模型决策主要由靠近POIs的高局部归因斑点主导。归因特征与血管级别属性（如厚度、管状性或连通性）的相关性较小——这表明模型在利用全局解剖推理方面有限。我们的结果强调了结构化可解释性工具的重要性，并突显了分割模型在捕捉全局血管上下文方面的现有限制。', 'title_zh': '基于斑点的解释性人工智能框架：分割模型理解血管结构吗？'}
{'arxiv_id': 'arXiv:2504.11460', 'title': 'Semantic Matters: Multimodal Features for Affective Analysis', 'authors': 'Tobias Hallmen, Robin-Nico Kampa, Fabian Deuser, Norbert Oswald, Elisabeth André', 'link': 'https://arxiv.org/abs/2504.11460', 'abstract': 'In this study, we present our methodology for two tasks: the Behavioural Ambivalence/Hesitancy (BAH) Recognition Challenge and the Emotional Mimicry Intensity (EMI) Estimation Challenge, both conducted as part of the 8th Workshop and Competition on Affective & Behavior Analysis in-the-wild. Building on previous work, we utilize a Wav2Vec 2.0 model pre-trained on a large podcast dataset to extract various audio features, capturing both linguistic and paralinguistic information. Our approach incorporates a valence-arousal-dominance (VAD) module derived from Wav2Vec 2.0, a BERT-like encoder, and a vision transformer (ViT) with predictions subsequently processed through a long short-term memory (LSTM) architecture for temporal modeling. In this iteration, we integrate the textual and visual modality into our analysis, recognizing that semantic content provides valuable contextual cues and underscoring that the meaning of speech often conveys more critical insights than its acoustic counterpart alone. Fusing in the vision modality helps in some cases to interpret the textual modality more precisely. This combined approach yields significant performance improvements over baseline methods.', 'abstract_zh': '本文研究了在第8届野生情感与行为分析研讨会与竞赛中的两项任务：行为 ambivalence/犹豫（BAH）识别挑战和情感模仿强度（EMI）估计挑战的方案。我们构建的方法基于预训练于大量播客数据集的Wav2Vec 2.0模型提取多种音频特征，涵盖语言和副语言信息。该方法包含源自Wav2Vec 2.0的情感极性-唤醒-支配（VAD）模块、类似BERT的编码器和视觉变换器（ViT），预测结果随后通过长短期记忆（LSTM）架构进行时间建模。本研究将文本和视觉模态整合到分析中，强调语义内容提供的上下文线索价值，指出言语的意义往往比其声学特征本身提供更多的关键见解。在某些情况下，整合视觉模态有助于更精确地解释文本模态。这种结合的方法相较于基线方法取得了显著的性能提升。', 'title_zh': '语义为本：多模态特征的情感分析'}
