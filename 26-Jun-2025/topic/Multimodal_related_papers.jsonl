{'arxiv_id': 'arXiv:2506.20342', 'title': 'Feature Hallucination for Self-supervised Action Recognition', 'authors': 'Lei Wang, Piotr Koniusz', 'link': 'https://arxiv.org/abs/2506.20342', 'abstract': 'Understanding human actions in videos requires more than raw pixel analysis; it relies on high-level semantic reasoning and effective integration of multimodal features. We propose a deep translational action recognition framework that enhances recognition accuracy by jointly predicting action concepts and auxiliary features from RGB video frames. At test time, hallucination streams infer missing cues, enriching feature representations without increasing computational overhead. To focus on action-relevant regions beyond raw pixels, we introduce two novel domain-specific descriptors. Object Detection Features (ODF) aggregate outputs from multiple object detectors to capture contextual cues, while Saliency Detection Features (SDF) highlight spatial and intensity patterns crucial for action recognition. Our framework seamlessly integrates these descriptors with auxiliary modalities such as optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It remains compatible with state-of-the-art architectures, including I3D, AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE V2 and InternVideo2. To handle uncertainty in auxiliary features, we incorporate aleatoric uncertainty modeling in the hallucination step and introduce a robust loss function to mitigate feature noise. Our multimodal self-supervised action recognition framework achieves state-of-the-art performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and Something-Something V2, demonstrating its effectiveness in capturing fine-grained action dynamics.', 'abstract_zh': '基于多模态自监督的方法在视频中的精细粒度动作动态理解中取得最佳性能', 'title_zh': '自监督动作识别的特征 hallucination'}
{'arxiv_id': 'arXiv:2506.20100', 'title': 'MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations', 'authors': 'Vardhan Dongre, Chi Gui, Shubham Garg, Hooshang Nayyeri, Gokhan Tur, Dilek Hakkani-Tür, Vikram S. Adve', 'link': 'https://arxiv.org/abs/2506.20100', 'abstract': 'We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning and decision-making in consultative interaction settings. Designed for the agriculture domain, MIRAGE captures the full complexity of expert consultations by combining natural user queries, expert-authored responses, and image-based context, offering a high-fidelity benchmark for evaluating models on grounded reasoning, clarification strategies, and long-form generation in a real-world, knowledge-intensive domain. Grounded in over 35,000 real user-expert interactions and curated through a carefully designed multi-step pipeline, MIRAGE spans diverse crop health, pest diagnosis, and crop management scenarios. The benchmark includes more than 7,000 unique biological entities, covering plant species, pests, and diseases, making it one of the most taxonomically diverse benchmarks available for vision-language models, grounded in the real world. Unlike existing benchmarks that rely on well-specified user inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich scenarios with open-world settings, requiring models to infer latent knowledge gaps, handle rare entities, and either proactively guide the interaction or respond. Project Page: this https URL', 'abstract_zh': '我们介绍了MIRAGE，一个新的多模态专家级推理与决策基准，适用于咨询交互场景。MIRAGE针对农业领域，通过结合自然用户查询、专家撰写的回应以及基于图像的上下文，捕捉专家咨询的全部复杂性，为评估模型在现实世界、知识密集型领域中的基于事实推理、澄清策略和长文生成能力提供了一个高保真基准。MIRAGE基于超过35,000个真实的用户-专家交互，并通过精心设计的多步骤管道进行筛选，涵盖了作物健康、病虫害诊断和作物管理等多种场景。该基准包括超过7,000种独特的生物学实体，涵盖了植物种类、害虫和疾病，是目前可用于视觉-语言模型的、基于真实世界最多样化的基准之一。与依赖于明确用户输入和封闭分类体系的现有基准不同，MIRAGE包括未充分指定、上下文丰富的场景，要求模型推断潜在的知识空白、处理稀有实体，并主动引导交互或做出回应。项目页面：this [链接]。', 'title_zh': 'MIRAGE：农业专家指导对话中多模态信息查询与推理基准测试'}
