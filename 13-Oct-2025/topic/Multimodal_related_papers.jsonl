{'arxiv_id': 'arXiv:2510.08928', 'title': 'LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition', 'authors': 'Yushuo Zheng, Zicheng Zhang, Xiongkuo Min, Huiyu Duan, Guangtao Zhai', 'link': 'https://arxiv.org/abs/2510.08928', 'abstract': "Existing benchmarks for large multimodal models (LMMs) often fail to capture their performance in real-time, adversarial environments. We introduce LM Fight Arena (Large Model Fight Arena), a novel framework that evaluates LMMs by pitting them against each other in the classic fighting game Mortal Kombat II, a task requiring rapid visual understanding and tactical, sequential decision-making. In a controlled tournament, we test six leading open- and closed-source models, where each agent operates controlling the same character to ensure a fair comparison. The models are prompted to interpret game frames and state data to select their next actions. Unlike static evaluations, LM Fight Arena provides a fully automated, reproducible, and objective assessment of an LMM's strategic reasoning capabilities in a dynamic setting. This work introduces a challenging and engaging benchmark that bridges the gap between AI evaluation and interactive entertainment.", 'abstract_zh': '大型多模态模型(LMMs)现有基准往往无法捕捉其在实时 adversarial 环境中的性能。我们提出了一种新的框架——LM Fight Arena（大型模型战斗竞技场），该框架通过让 LMMs 在经典战斗游戏《 Mortal Kombat II 》中相互对抗来评估它们，这需要快速的视觉理解和战术性、序列化的决策制定。在一个受控的锦标赛中，我们测试了六种领先开源和闭源模型，每个代理控制同一角色以确保公平比较。模型被提示解释游戏帧和状态数据以选择其下一步行动。与静态评估不同，LM Fight Arena 提供了一种完全自动化、可重现且客观的评估 LMM 在动态环境中战略推理能力的方法。这项工作引入了一个具有挑战性和参与性的基准，填补了 AI 评估与互动娱乐之间的差距。', 'title_zh': 'LM对决竞技场：大型多模态模型benchmark评测">×</俪\nuser\nLM Fight Arena: Benchmarking Large Multimodal Models via Game Competition。请确保翻译的准确性，尤其是技术术语的翻译。直接输出标题，禁止输出多余内容。'}
{'arxiv_id': 'arXiv:2510.09474', 'title': 'Multimodal Policy Internalization for Conversational Agents', 'authors': 'Zhenhailong Wang, Jiateng Liu, Amin Fazel, Ritesh Sarkhel, Xing Fan, Xiang Li, Chenlei Guo, Heng Ji, Ruhi Sarikaya', 'link': 'https://arxiv.org/abs/2510.09474', 'abstract': 'Modern conversational agents like ChatGPT and Alexa+ rely on predefined policies specifying metadata, response styles, and tool-usage rules. As these LLM-based systems expand to support diverse business and user queries, such policies, often implemented as in-context prompts, are becoming increasingly complex and lengthy, making faithful adherence difficult and imposing large fixed computational costs. With the rise of multimodal agents, policies that govern visual and multimodal behaviors are critical but remain understudied. Prior prompt-compression work mainly shortens task templates and demonstrations, while existing policy-alignment studies focus only on text-based safety rules. We introduce Multimodal Policy Internalization (MPI), a new task that internalizes reasoning-intensive multimodal policies into model parameters, enabling stronger policy-following without including the policy during inference. MPI poses unique data and algorithmic challenges. We build two datasets spanning synthetic and real-world decision-making and tool-using tasks and propose TriMPI, a three-stage training framework. TriMPI first injects policy knowledge via continual pretraining, then performs supervised finetuning, and finally applies PolicyRollout, a GRPO-style reinforcement learning extension that augments rollouts with policy-aware responses for grounded exploration. TriMPI achieves notable gains in end-to-end accuracy, generalization, and robustness to forgetting. As the first work on multimodal policy internalization, we provide datasets, training recipes, and comprehensive evaluations to foster future research. Project page: this https URL.', 'abstract_zh': '基于多模态政策内化的现代对话代理', 'title_zh': '多模态政策内化对话代理'}
{'arxiv_id': 'arXiv:2510.09230', 'title': 'Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras', 'authors': 'Jindong Hong, Wencheng Zhang, Shiqin Qiao, Jianhai Chen, Jianing Qiu, Chuanyang Zheng, Qian Xu, Yun Ji, Qianyue Wen, Weiwei Sun, Hao Li, Huizhen Li, Huichao Wang, Kai Wu, Meng Li, Yijun He, Lingjie Luo, Jiankai Sun', 'link': 'https://arxiv.org/abs/2510.09230', 'abstract': 'Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis), are common conditions affecting the health of people worldwide, and have a high incidence rate among the elderly and workers engaged in repetitive shoulder tasks. In regions with scarce medical resources, achieving early and accurate diagnosis poses significant challenges, and there is an urgent need for low-cost and easily scalable auxiliary diagnostic solutions. This research introduces videos captured by consumer-grade devices as the basis for diagnosis, reducing the cost for users. We focus on the innovative application of Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of shoulder disorders and propose a Hybrid Motion Video Diagnosis framework (HMVDx). This framework divides the two tasks of action understanding and disease diagnosis, which are respectively completed by two MLLMs. In addition to traditional evaluation indicators, this work proposes a novel metric called Usability Index by the logical process of medical decision-making (action recognition, movement diagnosis, and final diagnosis). This index evaluates the effectiveness of MLLMs in the medical field from the perspective of the entire medical diagnostic pathway, revealing the potential value of low-cost MLLMs in medical applications for medical practitioners. In experimental comparisons, the accuracy of HMVDx in diagnosing shoulder joint injuries has increased by 79.6\\% compared with direct video diagnosis, a significant technical contribution to future research on the application of MLLMs for video understanding in the medical field.', 'abstract_zh': '肩部疾病，如冻结肩（又称粘连性肩周炎），是影响全世界人们健康的一种常见状况，尤其是在老年人和从事重复肩部工作任务的工作者中高发率。在医疗资源稀缺的地区，实现早期和准确的诊断面临重大挑战，亟需低成本且易于扩展的辅助诊断解决方案。本研究以消费级设备拍摄的视频为基础进行诊断，降低用户成本。我们致力于将多模态大型语言模型（MLLMs）创新应用于肩部疾病的初步诊断，并提出了一种混合运动视频诊断框架（HMVDx）。该框架将动作理解与疾病诊断两个任务分别由两个MLLMs完成。此外，除了传统的评估指标外，本工作还提出了一种新的评价指标叫易用性指数，该指数通过医学决策过程中的逻辑（动作识别、运动诊断和最终诊断）视角评估MLLMs在医疗领域的有效性，揭示了低成本MLLMs在医疗应用中的潜在价值。在实验比较中，HMVDx在诊断肩关节损伤方面的准确率相比直接视频诊断提高了79.6%，对未来医学领域MLLMs在视频理解中的应用研究具有重要的技术贡献。', 'title_zh': '使用多元模态大语言模型和消费级摄像头诊断肩部疾病'}
{'arxiv_id': 'arXiv:2510.09121', 'title': 'MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation', 'authors': 'Dominik Winter, Mai Bui, Monica Azqueta Gavaldon, Nicolas Triltsch, Marco Rosati, Nicolas Brieu', 'link': 'https://arxiv.org/abs/2510.09121', 'abstract': 'Scarcity of annotated data, particularly for rare or atypical morphologies, present significant challenges for cell and nuclei segmentation in computational pathology. While manual annotation is labor-intensive and costly, synthetic data offers a cost-effective alternative. We introduce a Multimodal Semantic Diffusion Model (MSDM) for generating realistic pixel-precise image-mask pairs for cell and nuclei segmentation. By conditioning the generative process with cellular/nuclear morphologies (using horizontal and vertical maps), RGB color characteristics, and BERT-encoded assay/indication metadata, MSDM generates datasests with desired morphological properties. These heterogeneous modalities are integrated via multi-head cross-attention, enabling fine-grained control over the generated images. Quantitative analysis demonstrates that synthetic images closely match real data, with low Wasserstein distances between embeddings of generated and real images under matching biological conditions. The incorporation of these synthetic samples, exemplified by columnar cells, significantly improves segmentation model accuracy on columnar cells. This strategy systematically enriches data sets, directly targeting model deficiencies. We highlight the effectiveness of multimodal diffusion-based augmentation for advancing the robustness and generalizability of cell and nuclei segmentation models. Thereby, we pave the way for broader application of generative models in computational pathology.', 'abstract_zh': '稀注标注数据，尤其是稀有或非典型形态的数据，对 computational pathology 中的细胞和核分割构成重大挑战。虽然手动标注劳动密集且成本高，但合成数据提供了经济高效的替代方案。我们介绍了一种多模态语义扩散模型 (MSDM)，用于生成用于细胞和核分割的真实像素级图像-掩码对。通过使用细胞/核形态（水平和垂直图）、RGB 颜色特征以及 BERT 编码的实验/用途元数据来调节生成过程，MSDM 能够生成具有所需形态学特性的数据集。这些异质模态通过多头跨注意机制进行整合，使生成的图像细节可控。定量分析表明，生成的图像与真实数据高度匹配，在匹配的生物条件下，生成图像和真实图像的 Wasserstein 距离较低。通过结合这些合成样本（以柱状细胞为例），显著提高了柱状细胞分割模型的准确性。这一策略系统地丰富了数据集，直接针对模型缺陷。我们强调多模态扩散增强方法在提高细胞和核分割模型的鲁棒性和泛化性方面的有效性。从而为我们提供了利用生成模型在 computational pathology 领域广泛应用的道路。', 'title_zh': 'MSDM: 生成任务特定的病理图像以用于细胞和核分割的多模态条件扩散模型'}
{'arxiv_id': 'arXiv:2510.08606', 'title': 'Centering Emotion Hotspots: Multimodal Local-Global Fusion and Cross-Modal Alignment for Emotion Recognition in Conversations', 'authors': 'Yu Liu, Hanlei Shi, Haoxun Li, Yuqing Sun, Yuxuan Ding, Linlin Gong, Leyuan Qu, Taihao Li', 'link': 'https://arxiv.org/abs/2510.08606', 'abstract': 'Emotion Recognition in Conversations (ERC) is hard because discriminative evidence is sparse, localized, and often asynchronous across modalities. We center ERC on emotion hotspots and present a unified model that detects per-utterance hotspots in text, audio, and video, fuses them with global features via Hotspot-Gated Fusion, and aligns modalities using a routed Mixture-of-Aligners; a cross-modal graph encodes conversational structure. This design focuses modeling on salient spans, mitigates misalignment, and preserves context. Experiments on standard ERC benchmarks show consistent gains over strong baselines, with ablations confirming the contributions of HGF and MoA. Our results point to a hotspot-centric view that can inform future multimodal learning, offering a new perspective on modality fusion in ERC.', 'abstract_zh': '情绪识别在对话中的挑战在于区分性证据稀疏、局部化且跨模态往往异步。我们以情绪热点为中心，提出了一种统一模型，该模型在文本、音频和视频中检测每个片段的热点，通过热点门控融合与全局特征融合，并使用路由混合对齐器对齐模态；交叉模态图编码对话结构。此设计专注于显著片段的建模，缓解对齐问题，并保留上下文。实验结果显示，该模型在标准情绪识别在对话中的基准上优于强基线，消融实验确认了热点门控融合和混合对齐器的贡献。我们的结果表明，热点为中心的观点可以指导未来多模态学习，并为情绪识别中的模态融合提供新的视角。', 'title_zh': '聚焦情感热点：对话中情感识别的多模态局部-全局融合与跨模态对齐'}
{'arxiv_id': 'arXiv:2510.08587', 'title': 'EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient Gaussian Deformation', 'authors': 'Tianheng Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng', 'link': 'https://arxiv.org/abs/2510.08587', 'abstract': "This paper presents EGSTalker, a real-time audio-driven talking head generation framework based on 3D Gaussian Splatting (3DGS). Designed to enhance both speed and visual fidelity, EGSTalker requires only 3-5 minutes of training video to synthesize high-quality facial animations. The framework comprises two key stages: static Gaussian initialization and audio-driven deformation. In the first stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network (KAN) are used to extract spatial features and construct a compact 3D Gaussian representation. In the second stage, we propose an Efficient Spatial-Audio Attention (ESAA) module to fuse audio and spatial cues, while KAN predicts the corresponding Gaussian deformations. Extensive experiments demonstrate that EGSTalker achieves rendering quality and lip-sync accuracy comparable to state-of-the-art methods, while significantly outperforming them in inference speed. These results highlight EGSTalker's potential for real-time multimedia applications.", 'abstract_zh': '基于3D高斯点绘制的实时音频驱动头部生成框架EGSTalker', 'title_zh': 'EGSTalker: 实时基于音频的头部生成'}
{'arxiv_id': 'arXiv:2510.08580', 'title': 'LadderSym: A Multimodal Interleaved Transformer for Music Practice Error Detection', 'authors': 'Benjamin Shiue-Hal Chou, Purvish Jajal, Nick John Eliopoulos, James C. Davis, George K. Thiruvathukal, Kristen Yeon-Ji Yun, Yung-Hsiang Lu', 'link': 'https://arxiv.org/abs/2510.08580', 'abstract': 'Music learners can greatly benefit from tools that accurately detect errors in their practice. Existing approaches typically compare audio recordings to music scores using heuristics or learnable models. This paper introduces \\textit{LadderSym}, a novel Transformer-based method for music error detection. \\textit{LadderSym} is guided by two key observations about the state-of-the-art approaches: (1) late fusion limits inter-stream alignment and cross-modality comparison capability; and (2) reliance on score audio introduces ambiguity in the frequency spectrum, degrading performance in music with concurrent notes. To address these limitations, \\textit{LadderSym} introduces (1) a two-stream encoder with inter-stream alignment modules to improve audio comparison capabilities and error detection F1 scores, and (2) a multimodal strategy that leverages both audio and symbolic scores by incorporating symbolic representations as decoder prompts, reducing ambiguity and improving F1 scores. We evaluate our method on the \\textit{MAESTRO-E} and \\textit{CocoChorales-E} datasets by measuring the F1 score for each note category. Compared to the previous state of the art, \\textit{LadderSym} more than doubles F1 for missed notes on \\textit{MAESTRO-E} (26.8\\% $\\rightarrow$ 56.3\\%) and improves extra note detection by 14.4 points (72.0\\% $\\rightarrow$ 86.4\\%). Similar gains are observed on \\textit{CocoChorales-E}. This work introduces general insights about comparison models that could inform sequence evaluation tasks for reinforcement Learning, human skill assessment, and model evaluation.', 'abstract_zh': '音乐学习者可以从准确检测练习中错误的工具中大大受益。现有方法通常使用启发式规则或可学习模型来比较音频记录和音乐总谱。本文介绍了基于Transformer的新型音乐错误检测方法LadderSym。LadderSym基于对现有方法的两个关键观察:(1)后期融合限制了跨流对齐和跨模态比较能力；(2)依赖于乐谱音频引入了频率谱的模糊性，降低了同时存在的音符的性能。为了解决这些局限性，LadderSym引入了(1)具有跨流对齐模块的双流水解编码器，以提高音频比较能力和错误检测F1分数；(2)一种多模态策略，利用音频和符号总谱，通过将符号表示作为解码器提示来结合这两种表示，减少模糊性并提高F1分数。我们在MAESTRO-E和CocoChorales-E数据集上评估了该方法，通过测量每种音符类别的F1分数来评估。与之前的最先进方法相比，LadderSym在MAESTRO-E上缺失音符的F1分数提高了195%（从26.8%提高到56.3%），额外音符检测提高了14.4个分数点（从72.0%提高到86.4%）。在CocoChorales-E上也观察到类似的收益。这项工作提供了关于比较模型的一般见解，可为强化学习序列评估任务、人类技能评估和模型评估提供指导。', 'title_zh': 'LadderSym: 一种多模态交替变换器用于音乐练习错误检测'}
{'arxiv_id': 'arXiv:1812.06145', 'title': 'Improving the Performance of Unimodal Dynamic Hand-Gesture Recognition with Multimodal Training', 'authors': 'Mahdi Abavisani, Hamid Reza Vaezi Joze, Vishal M. Patel', 'link': 'https://arxiv.org/abs/1812.06145', 'abstract': 'We present an efficient approach for leveraging the knowledge from multiple modalities in training unimodal 3D convolutional neural networks (3D-CNNs) for the task of dynamic hand gesture recognition. Instead of explicitly combining multimodal information, which is commonplace in many state-of-the-art methods, we propose a different framework in which we embed the knowledge of multiple modalities in individual networks so that each unimodal network can achieve an improved performance. In particular, we dedicate separate networks per available modality and enforce them to collaborate and learn to develop networks with common semantics and better representations. We introduce a "spatiotemporal semantic alignment" loss (SSA) to align the content of the features from different networks. In addition, we regularize this loss with our proposed "focal regularization parameter" to avoid negative knowledge transfer. Experimental results show that our framework improves the test time recognition accuracy of unimodal networks, and provides the state-of-the-art performance on various dynamic hand gesture recognition datasets.', 'abstract_zh': '一种利用多种模态知识提高单模态3D卷积神经网络动态手势识别性能的方法', 'title_zh': '基于多模态训练改进单模态动态手势识别性能'}
{'arxiv_id': 'arXiv:1804.06498', 'title': 'Deep Multimodal Subspace Clustering Networks', 'authors': 'Mahdi Abavisani, Vishal M. Patel', 'link': 'https://arxiv.org/abs/1804.06498', 'abstract': "We present convolutional neural network (CNN) based approaches for unsupervised multimodal subspace clustering. The proposed framework consists of three main stages - multimodal encoder, self-expressive layer, and multimodal decoder. The encoder takes multimodal data as input and fuses them to a latent space representation. The self-expressive layer is responsible for enforcing the self-expressiveness property and acquiring an affinity matrix corresponding to the data points. The decoder reconstructs the original input data. The network uses the distance between the decoder's reconstruction and the original input in its training. We investigate early, late and intermediate fusion techniques and propose three different encoders corresponding to them for spatial fusion. The self-expressive layers and multimodal decoders are essentially the same for different spatial fusion-based approaches. In addition to various spatial fusion-based methods, an affinity fusion-based network is also proposed in which the self-expressive layer corresponding to different modalities is enforced to be the same. Extensive experiments on three datasets show that the proposed methods significantly outperform the state-of-the-art multimodal subspace clustering methods.", 'abstract_zh': '基于卷积神经网络的无监督多模态子空间聚类方法', 'title_zh': '深度多模态子空间聚类网络'}
