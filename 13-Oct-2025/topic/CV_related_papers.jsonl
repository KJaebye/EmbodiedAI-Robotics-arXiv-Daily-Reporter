{'arxiv_id': 'arXiv:2510.09458', 'title': 'SilvaScenes: Tree Segmentation and Species Classification from Under-Canopy Images in Natural Forests', 'authors': 'David-Alexandre Duclos, William Guimont-Martin, Gabriel Jeanson, Arthur Larochelle-Tremblay, Théo Defosse, Frédéric Moore, Philippe Nolet, François Pomerleau, Philippe Giguère', 'link': 'https://arxiv.org/abs/2510.09458', 'abstract': 'Interest in robotics for forest management is growing, but perception in complex, natural environments remains a significant hurdle. Conditions such as heavy occlusion, variable lighting, and dense vegetation pose challenges to automated systems, which are essential for precision forestry, biodiversity monitoring, and the automation of forestry equipment. These tasks rely on advanced perceptual capabilities, such as detection and fine-grained species classification of individual trees. Yet, existing datasets are inadequate to develop such perception systems, as they often focus on urban settings or a limited number of species. To address this, we present SilvaScenes, a new dataset for instance segmentation of tree species from under-canopy images. Collected across five bioclimatic domains in Quebec, Canada, SilvaScenes features 1476 trees from 24 species with annotations from forestry experts. We demonstrate the relevance and challenging nature of our dataset by benchmarking modern deep learning approaches for instance segmentation. Our results show that, while tree segmentation is easy, with a top mean average precision (mAP) of 67.65%, species classification remains a significant challenge with an mAP of only 35.69%. Our dataset and source code will be available at this https URL.', 'abstract_zh': '森林管理中的机器人应用正在增加，但在复杂自然环境中的感知仍然是一个重大障碍。 Quebec, Canada 的五个生物气候域中收集的 SilvaScenes 数据集为林下图像中的树种实例分割提供了解决方案，该数据集包含 24 种树木的 1476 棵树，附有林业专家的标注。我们通过现代深度学习方法的基准测试展示了该数据集的相关性和挑战性。尽管树的分割表现良好，平均精度最高可达 67.65%，但物种分类仍是一个重大挑战，平均精度仅为 35.69%。数据集和源代码将在此处提供。', 'title_zh': 'SilvaScenes: 林下图像中自然森林树木分割与物种分类'}
{'arxiv_id': 'arXiv:2510.09035', 'title': 'Exploring Single Domain Generalization of LiDAR-based Semantic Segmentation under Imperfect Labels', 'authors': 'Weitong Kong, Zichao Zeng, Di Wen, Jiale Wei, Kunyu Peng, June Moh Goo, Jan Boehm, Rainer Stiefelhagen', 'link': 'https://arxiv.org/abs/2510.09035', 'abstract': 'Accurate perception is critical for vehicle safety, with LiDAR as a key enabler in autonomous driving. To ensure robust performance across environments, sensor types, and weather conditions without costly re-annotation, domain generalization in LiDAR-based 3D semantic segmentation is essential. However, LiDAR annotations are often noisy due to sensor imperfections, occlusions, and human errors. Such noise degrades segmentation accuracy and is further amplified under domain shifts, threatening system reliability. While noisy-label learning is well-studied in images, its extension to 3D LiDAR segmentation under domain generalization remains largely unexplored, as the sparse and irregular structure of point clouds limits direct use of 2D methods. To address this gap, we introduce the novel task Domain Generalization for LiDAR Semantic Segmentation under Noisy Labels (DGLSS-NL) and establish the first benchmark by adapting three representative noisy-label learning strategies from image classification to 3D segmentation. However, we find that existing noisy-label learning approaches adapt poorly to LiDAR data. We therefore propose DuNe, a dual-view framework with strong and weak branches that enforce feature-level consistency and apply cross-entropy loss based on confidence-aware filtering of predictions. Our approach shows state-of-the-art performance by achieving 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and 52.58% on SemanticPOSS under 10% symmetric label noise, with an overall Arithmetic Mean (AM) of 49.57% and Harmonic Mean (HM) of 48.50%, thereby demonstrating robust domain generalization in DGLSS-NL tasks. The code is available on our project page.', 'abstract_zh': '基于LiDAR语义分割的噪声标签领域泛化任务（DGLSS-NL）', 'title_zh': '基于LiDAR的语义分割在 imperfect 标签条件下的单域泛化探索'}
{'arxiv_id': 'arXiv:2510.08770', 'title': 'Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform', 'authors': 'Gregory Yeghiyan, Jurius Azar, Devson Butani, Chan-Jin Chung', 'link': 'https://arxiv.org/abs/2510.08770', 'abstract': 'This paper presents a real-time spill detection system that utilizes pretrained deep learning models with RGB and thermal imaging to classify spill vs. no-spill scenarios across varied environments. Using a balanced binary dataset (4,000 images), our experiments demonstrate the advantages of thermal imaging in inference speed, accuracy, and model size. We achieve up to 100% accuracy using lightweight models like VGG19 and NasNetMobile, with thermal models performing faster and more robustly across different lighting conditions. Our system runs on consumer-grade hardware (RTX 4080) and achieves inference times as low as 44 ms with model sizes under 350 MB, highlighting its deployability in safety-critical contexts. Results from experiments with a real robot and test datasets indicate that a VGG19 model trained on thermal imaging performs best.', 'abstract_zh': '利用预训练深度学习模型和RGB及热成像实时检测泄漏的系统', 'title_zh': '使用热成像、预训练深度学习模型和机器人平台检测泄漏'}
{'arxiv_id': 'arXiv:2510.08768', 'title': "Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem", 'authors': 'Francisco Pascoa, Ian Lalonde, Alexandre Girard', 'link': 'https://arxiv.org/abs/2510.08768', 'abstract': "Reinforcement learning (RL) policies often fail to generalize to new robots, tasks, or environments with different physical parameters, a challenge that limits their real-world applicability. This paper presents a simple, zero-shot transfer method based on Buckingham's Pi Theorem to address this limitation. The method adapts a pre-trained policy to new system contexts by scaling its inputs (observations) and outputs (actions) through a dimensionless space, requiring no retraining. The approach is evaluated against a naive transfer baseline across three environments of increasing complexity: a simulated pendulum, a physical pendulum for sim-to-real validation, and the high-dimensional HalfCheetah. Results demonstrate that the scaled transfer exhibits no loss of performance on dynamically similar contexts. Furthermore, on non-similar contexts, the scaled policy consistently outperforms the naive transfer, significantly expanding the volume of contexts where the original policy remains effective. These findings demonstrate that dimensional analysis provides a powerful and practical tool to enhance the robustness and generalization of RL policies.", 'abstract_zh': '基于布丰π定理的零shot迁移方法：强化学习策略的通用性提升', 'title_zh': '使用毕奥-萨伐尔定理在强化学习中实现零样本策略转移'}
{'arxiv_id': 'arXiv:2407.09646', 'title': 'Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba', 'authors': 'Haoye Dong, Aviral Chharia, Wenbo Gou, Francisco Vicente Carrasco, Fernando De la Torre', 'link': 'https://arxiv.org/abs/2407.09646', 'abstract': "3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, yet they do not fully achieve robust and accurate performance, primarily due to inefficiently modeling spatial relations between joints. To address this problem, we propose a novel graph-guided Mamba framework, named Hamba, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba's scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to efficiently learn the spatial relationships between joints for improving reconstruction performance. Specifically, we design a Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5% fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space features and jointly considers global and local features to improve performance. Experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND. At the time of this paper's acceptance, Hamba holds the top position, Rank 1 in two Competition Leaderboards on 3D hand reconstruction. Project Website: this https URL", 'abstract_zh': '单张RGB图像的3D手部重构因其 articulated 运动、自遮挡以及与物体的交互而具有挑战性。现有的SOTA方法采用基于注意力的变换器来学习3D手部姿态和形状，但未能完全实现稳健且准确的表现，主要原因是关节间空间关系建模效率不高。为解决这一问题，我们提出了一种新的图引导Mamba框架，名为Hamba，该框架将图学习与状态空间建模相结合。我们的核心思想是通过图引导的双向扫描将Mamba的扫描重构成3D重构，仅使用少量有效的tokens。这使我们能够高效地学习关节间的空间关系以提高重构性能。具体而言，我们设计了一种图引导状态空间（GSS）块，学习关节的图结构关系和空间序列，并比注意力方法使用少88.5%的tokens。此外，我们通过融合模块将状态空间特征和全局特征进行整合。通过利用GSS块和融合模块，Hamba有效地利用了图引导的状态空间特征，并共同考虑全局和局部特征以提高性能。在多个基准测试和户外测试中，Hamba显著优于现有SOTA方法，在FreiHAND数据集上实现了PA-MPVPE为5.3mm和F@15mm为0.992。该论文提交时，Hamba在两个3D手部重构竞赛leaderboard上保持第1名。Project Website: this https URL。', 'title_zh': 'Hamba：基于图引导双扫描的单视角3D手部重建'}
{'arxiv_id': 'arXiv:2510.09060', 'title': 'OSCAR: Orthogonal Stochastic Control for Alignment-Respecting Diversity in Flow Matching', 'authors': 'Jingxuan Wu, Zhenglin Wan, Xingrui Yu, Yuzhe Yang, Bo An, Ivor Tsang', 'link': 'https://arxiv.org/abs/2510.09060', 'abstract': 'Flow-based text-to-image models follow deterministic trajectories, forcing users to repeatedly sample to discover diverse modes, which is a costly and inefficient process. We present a training-free, inference-time control mechanism that makes the flow itself diversity-aware. Our method simultaneously encourages lateral spread among trajectories via a feature-space objective and reintroduces uncertainty through a time-scheduled stochastic perturbation. Crucially, this perturbation is projected to be orthogonal to the generation flow, a geometric constraint that allows it to boost variation without degrading image details or prompt fidelity. Our procedure requires no retraining or modification to the base sampler and is compatible with common flow-matching solvers. Theoretically, our method is shown to monotonically increase a volume surrogate while, due to its geometric constraints, approximately preserving the marginal distribution. This provides a principled explanation for why generation quality is robustly maintained. Empirically, across multiple text-to-image settings under fixed sampling budgets, our method consistently improves diversity metrics such as the Vendi Score and Brisque over strong baselines, while upholding image quality and alignment.', 'abstract_zh': '基于流的文本到图像模型遵循确定性的轨迹，迫使用户反复采样以发现多样模式，这是一个成本高且低效的过程。我们提出了一种无需训练、仅在推理时控制的机制，使流本身变得对多样模式敏感。该方法通过特征空间目标同时鼓励轨迹间的横向扩展，并通过时间调度的随机扰动重新引入不确定性。关键的是，这种扰动被投影为与生成流正交，这一几何约束使其能够在不降低图像细节或提示保真度的情况下增加变化量。该过程无需对基础采样器进行重新训练或修改，并且与常见的流匹配求解器兼容。理论上，该方法被证明可以单调地增加体积替代指标，由于其几何约束，大约保住了边缘分布。这为生成质量的稳健保持提供了理论解释。实验上，在固定采样预算的多个文本到图像设置下，该方法在与强基线相比时，一致地改进了多样性指标，如Vendi分数和Brisque，同时保持了图像质量和对齐。', 'title_zh': 'OSCAR: 正交随机控制以实现流匹配中的对齐保多样性'}
{'arxiv_id': 'arXiv:2510.09608', 'title': 'StreamingVLM: Real-Time Understanding for Infinite Video Streams', 'authors': 'Ruyi Xu, Guangxuan Xiao, Yukang Chen, Liuning He, Kelly Peng, Yao Lu, Song Han', 'link': 'https://arxiv.org/abs/2510.09608', 'abstract': 'Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at this https URL.', 'abstract_zh': 'Vision-language模型（VLMs）可以驱动实时助手和自主代理，但它们面临一个关键挑战：在不增加延迟和内存使用的情况下理解无尽的视频流。全注意力处理整个视频会导致二次计算成本，并且在长视频上性能不佳。同时，简单的滑动窗口方法也有缺陷，要么破坏连续性，要么由于冗余重新计算而导致高延迟。本文介绍了StreamingVLM，这是一种针对无限视觉输入的实时、稳定理解的模型。我们的方法是一个统一框架，将训练与流式推理对齐。在推理过程中，我们通过重新利用注意力汇集的状态、一个最近几帧的短窗口和一个最近几条文本的长窗口来维护紧凑的KV缓存。这种流式能力通过一种简单的监督微调（SFT）策略实现，该策略对短的重叠视频片段应用全注意力，从而在不使用长上下文信息的情况下有效地模拟推理时的注意力模式。为了评估，我们构建了Inf-Streams-Eval，这是一个新的基准，包含平均时长超过两小时的视频，需要在帧与文本之间进行密集的、毫秒级的对齐。在Inf-Streams-Eval上，StreamingVLM在与GPT-4O mini的对比中取得了66.18%的胜率，并在单个NVIDIA H100上保持了稳定的实时性能，最高达8 FPS。值得注意的是，我们的SFT策略也增强了通用的VQA能力，而无需任何形式的VQA特定微调，从而在LongVideoBench上提高了4.30%，在OVOBench Realtime上提高了5.96%。代码可在以下链接获取。', 'title_zh': 'StreamingVLM: 实时理解无限视频流'}
{'arxiv_id': 'arXiv:2510.09499', 'title': 'A methodology for clinically driven interactive segmentation evaluation', 'authors': 'Parhom Esmaeili, Virginia Fernandez, Pedro Borges, Eli Gibson, Sebastien Ourselin, M. Jorge Cardoso', 'link': 'https://arxiv.org/abs/2510.09499', 'abstract': 'Interactive segmentation is a promising strategy for building robust, generalisable algorithms for volumetric medical image segmentation. However, inconsistent and clinically unrealistic evaluation hinders fair comparison and misrepresents real-world performance. We propose a clinically grounded methodology for defining evaluation tasks and metrics, and built a software framework for constructing standardised evaluation pipelines. We evaluate state-of-the-art algorithms across heterogeneous and complex tasks and observe that (i) minimising information loss when processing user interactions is critical for model robustness, (ii) adaptive-zooming mechanisms boost robustness and speed convergence, (iii) performance drops if validation prompting behaviour/budgets differ from training, (iv) 2D methods perform well with slab-like images and coarse targets, but 3D context helps with large or irregularly shaped targets, (v) performance of non-medical-domain models (e.g. SAM2) degrades with poor contrast and complex shapes.', 'abstract_zh': '基于临床的医学图像体积分割评估方法与标准管线构建：挑战与发现', 'title_zh': '基于临床驱动的交互式分割评估方法ologie'}
{'arxiv_id': 'arXiv:2510.09302', 'title': 'CapGeo: A Caption-Assisted Approach to Geometric Reasoning', 'authors': 'Yuying Li, Siyi Qian, Hao Liang, Leqi Zheng, Ruichuan An, Yongzhen Guo, Wentao Zhang', 'link': 'https://arxiv.org/abs/2510.09302', 'abstract': 'Geometric reasoning remains a core challenge for Multimodal Large Language Models (MLLMs). Even the most advanced closed-source systems, such as GPT-O3 and Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite exhibiting strong textual reasoning abilities on tasks like the International Mathematical Olympiad (IMO). This gap suggests that the bottleneck lies in understanding geometric diagrams rather than reasoning itself. Since geometric figures can often be faithfully described in concise textual form, converting visual content into captions offers a promising direction. Motivated by this insight, we introduce CapGeo, a caption-assisted reasoning framework that bridges visual and textual modalities. Experiments show substantial improvements when models are equipped with captions: Qwen2.5-VL-72B improves from 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to 73.0%. To systematically evaluate and identify high-quality geometric captioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated figure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based evaluation metric that correlates strongly with downstream CapGeo performance, enabling reliable assessment of geometric captioning ability. Together, our framework and benchmark highlight a new pathway toward advancing geometric reasoning in MLLMs.', 'abstract_zh': '几何推理仍然是多模态大型语言模型（MLLMs）的核心挑战。即使是最先进的封闭源系统，如GPT-O3和Gemini-2.5-Pro，在国际数学奥林匹克（IMO）等文本推理任务中表现出强大的能力，但在可靠解决几何问题方面仍然存在困难。这一差距表明瓶颈在于理解几何图表，而非推理本身。由于几何图形往往可以用简洁的文本形式准确描述，将视觉内容转换为描述性文字提供了有希望的方向。受此启发，我们提出了CapGeo，一个基于描述的推理框架，将视觉和文本模态联系起来。实验结果显示，当模型配备了描述性文字时，性能有显著提升：Qwen2.5-VL-72B从仅视觉的8.6%提高到59.0%，而Claude-Opus-4从44.8%提高到73.0%。为进一步系统地评估和识别高质量的几何描述模型，我们提出了CapGeo-Bench，其中包括4,641个精心策划的图表-描述对数据集。 crucially, CapGeo-Bench 包括一个基于关键点的评估指标，该指标与后续的CapGeo性能高度相关，从而使几何描述能力的可靠评估成为可能。我们的框架和基准共同指出了多模态大型语言模型中推进几何推理的一个新路径。', 'title_zh': 'CapGeo: 一种辅助几何推理的描述方法'}
{'arxiv_id': 'arXiv:2510.09228', 'title': 'Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation', 'authors': 'Vijay M. Galshetwar, Praful Hambarde, Prashant W. Patil, Akshay Dudhane, Sachin Chaudhary, Santosh Kumar Vipparathi, Subrahmanyam Murala', 'link': 'https://arxiv.org/abs/2510.09228', 'abstract': 'Adverse weather conditions such as haze, rain, and snow significantly degrade the quality of images and videos, posing serious challenges to intelligent transportation systems (ITS) that rely on visual input. These degradations affect critical applications including autonomous driving, traffic monitoring, and surveillance. This survey presents a comprehensive review of image and video restoration techniques developed to mitigate weather-induced visual impairments. We categorize existing approaches into traditional prior-based methods and modern data-driven models, including CNNs, transformers, diffusion models, and emerging vision-language models (VLMs). Restoration strategies are further classified based on their scope: single-task models, multi-task/multi-weather systems, and all-in-one frameworks capable of handling diverse degradations. In addition, we discuss day and night time restoration challenges, benchmark datasets, and evaluation protocols. The survey concludes with an in-depth discussion on limitations in current research and outlines future directions such as mixed/compound-degradation restoration, real-time deployment, and agentic AI frameworks. This work aims to serve as a valuable reference for advancing weather-resilient vision systems in smart transportation environments. Lastly, to stay current with rapid advancements in this field, we will maintain regular updates of the latest relevant papers and their open-source implementations at this https URL', 'abstract_zh': '恶劣天气条件（如雾霾、降雨和降雪）显著降低了图像和视频的质量，给依赖视觉输入的智能交通运输系统（ITS）带来了严重挑战。这些降级影响到自动驾驶、交通监控和监视等关键应用。本文综述了为减轻天气引起的视觉障碍而开发的图像和视频恢复技术。我们将现有方法分为基于先验的传统方法和现代数据驱动模型，包括CNN、变压器、扩散模型和新兴的视觉-语言模型（VLM）。恢复策略根据其范围进一步分类：单任务模型、多任务/多天气系统以及能够处理各种降级现象的全合一框架。此外，本文还讨论了昼夜恢复挑战、基准数据集和评估协议。综述最后深入讨论了当前研究的局限性，并提出了未来方向，如混合/复合降级恢复、实时部署以及主动AI框架。本文旨在为改进智能交通运输环境下的抗恶劣天气视觉系统提供有价值的参考。此外，为了跟上该领域的迅速发展，我们将定期更新最新相关论文及其开源实现，网址为 this https URL。', 'title_zh': '清晰之路，清晰之见：智能交通多气象条件恢复技术进展'}
{'arxiv_id': 'arXiv:2510.09187', 'title': 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'authors': 'Sungwoo Kang', 'link': 'https://arxiv.org/abs/2510.09187', 'abstract': 'Cricket shot classification from video sequences remains a challenging problem in sports video analysis, requiring effective modeling of both spatial and temporal features. This paper presents the first comprehensive baseline study comparing seven different deep learning approaches across four distinct research paradigms for cricket shot classification. We implement and systematically evaluate traditional CNN-LSTM architectures, attention-based models, vision transformers, transfer learning approaches, and modern EfficientNet-GRU combinations on a unified benchmark. A critical finding of our study is the significant performance gap between claims in academic literature and practical implementation results. While previous papers reported accuracies of 96\\% (Balaji LRCN), 99.2\\% (IJERCSE), and 93\\% (Sensors), our standardized re-implementations achieve 46.0\\%, 55.6\\%, and 57.7\\% respectively. Our modern SOTA approach, combining EfficientNet-B0 with a GRU-based temporal model, achieves 92.25\\% accuracy, demonstrating that substantial improvements are possible with modern architectures and systematic optimization. All implementations follow modern MLOps practices with PyTorch Lightning, providing a reproducible research platform that exposes the critical importance of standardized evaluation protocols in sports video analysis research.', 'abstract_zh': '从视频序列中分类板球击球仍是一个具有挑战性的问题，需要有效地建模空间和时间特征。本文首次全面比较了四种不同研究范式下的七种不同深度学习方法在板球击球分类中的表现。我们在统一的基准上实现了并系统评估了传统CNN-LSTM架构、注意力模型、视觉Transformer、迁移学习方法以及现代EfficientNet-GRU组合。研究的关键发现之一是学术文献中的声称性能与实际实现结果之间的巨大差距。虽然以往的论文分别报告了96%（Balaji LRCN）、99.2%（IJERCSE）和93%（Sensors）的准确率，但我们的标准化重实现分别达到46.0%、55.6%和57.7%。我们的现代最佳方案结合EfficientNet-B0与基于GRU的时间模型，达到了92.25%的准确率，证明了使用现代架构和系统优化可以取得显著改进。所有实现均遵循现代MLOps实践并使用PyTorch Lightning，提供了一个可再现的研究平台，突显了标准化评估协议在体育视频分析研究中的关键重要性。', 'title_zh': '现代深度学习方法在板球击球分类中的综合基准研究'}
{'arxiv_id': 'arXiv:2510.09110', 'title': 'SOS: Synthetic Object Segments Improve Detection, Segmentation, and Grounding', 'authors': 'Weikai Huang, Jieyu Zhang, Taoyang Jia, Chenhao Zheng, Ziqi Gao, Jae Sung Park, Ranjay Krishna', 'link': 'https://arxiv.org/abs/2510.09110', 'abstract': 'Visual grouping -- operationalized via instance segmentation, visual grounding, and object detection -- underpins applications from robotic perception to photo editing. Large annotated datasets are costly, biased in coverage, and hard to scale. Synthetic data are promising but often lack flexibility, accuracy, and compositional diversity.\nWe present SOS, a simple and scalable data synthesis pipeline based on an object-centric composition strategy. It pastes high-quality synthetic object segments into new images using structured layout priors and generative relighting, producing accurate and diverse masks, boxes, and referring expressions. Models trained on 100000 synthetic images from SOS outperform those trained on larger real-image datasets such as GRIT (20M) and V3Det (200K) on detection and grounding tasks, achieving +10.9 AP on LVIS detection and +8.4 $N_{\\text{Acc}}$ on gRefCOCO grounding. SOS enables controllable dataset construction and improves generalization in both low-data and closed-vocabulary settings. Augmenting LVIS and COCO with synthetic object segments yields strong performance across real-data scales and even larger gains under extremely limited real data (for example, +3.83 $AP_{\\text{rare}}$ on LVIS instance segmentation and +6.59 AP with a 1 percent COCO setup). This controllability also supports targeted data generation for challenging intra-class referring in visual grounding.', 'abstract_zh': '基于对象中心合成策略的简单可扩展数据合成pipeline:SOS', 'title_zh': 'SOS: 合成对象片段 cải进检测、分割和接地'}
{'arxiv_id': 'arXiv:2510.08818', 'title': 'D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition', 'authors': 'Yiyang Huang, Yizhou Wang, Yun Fu', 'link': 'https://arxiv.org/abs/2510.08818', 'abstract': 'Video large language models (Vid-LLMs), which excel in diverse video-language tasks, can be effectively constructed by adapting image-pretrained vision-language models (VLMs). However, this adaptation remains challenging, as it requires processing dense and temporally extended visual inputs that exceed the capacity of image-based models. This paper identifies the perception bottleneck and token overload as key challenges in extending image-based VLMs to the video domain. To address these issues, we propose D-CoDe, a training-free adaptation framework that incorporates dynamic compression and question decomposition. Specifically, dynamic compression alleviates the perception bottleneck through adaptive selection of representative frames and content-aware aggregation of spatial tokens, thereby reducing redundancy while preserving informative content. In parallel, question decomposition mitigates token overload by reformulating the original query into sub-questions, guiding the model to focus on distinct aspects of the video and enabling more comprehensive understanding. Experiments demonstrate that D-CoDe effectively improves video understanding across various benchmarks. Furthermore, strong performance on the challenging long-video benchmark highlights the potential of D-CoDe in handling complex video-language tasks. Code is available at this https URL.', 'abstract_zh': '基于视觉语言模型的视频大型语言模型（Vid-LLMs）可以通过适应图像预训练的视觉语言模型（VLMs）来有效构建，然而这一过程仍具有挑战性，因为这需要处理超出图像模型容量的密集且时间上扩展的视觉输入。本文识别出感知瓶颈和令牌过载是将基于图像的VLMs扩展到视频领域的关键挑战。为了解决这些问题，我们提出了D-CoDe，这是一种无需训练的适应框架，结合了动态压缩和问题分解。具体来说，动态压缩通过自适应选择代表性帧和内容感知的空间令牌聚合来缓解感知瓶颈，从而减少冗余同时保留有信息的内容。与此同时，问题分解通过将原始查询重新拆分为子问题来缓解令牌过载，引导模型专注于视频的不同方面，从而实现更全面的理解。实验结果显示，D-CoDe在多种基准测试中有效提高了视频理解能力。此外，D-CoDe在具有挑战性的长视频基准测试中的出色表现展示了其在处理复杂视频语言任务的潜力。代码可在以下链接获取。', 'title_zh': 'D-CoDe: 通过动态压缩和问题分解将图像预训练的VLM扩展到视频'}
{'arxiv_id': 'arXiv:2510.08799', 'title': 'SkipSR: Faster Super Resolution with Token Skipping', 'authors': 'Rohan Choudhury, Shanchuan Lin, Jianyi Wang, Hao Chen, Qi Zhao, Feng Cheng, Lu Jiang, Kris Kitani, Laszlo A. Jeni', 'link': 'https://arxiv.org/abs/2510.08799', 'abstract': 'Diffusion-based super-resolution (SR) is a key component in video generation and video restoration, but is slow and expensive, limiting scalability to higher resolutions and longer videos. Our key insight is that many regions in video are inherently low-detail and gain little from refinement, yet current methods process all pixels uniformly. To take advantage of this, we propose SkipSR, a simple framework for accelerating video SR by identifying low-detail regions directly from low-resolution input, then skipping computation on them entirely, only super-resolving the areas that require refinement. This simple yet effective strategy preserves perceptual quality in both standard and one-step diffusion SR models while significantly reducing computation. In standard SR benchmarks, our method achieves up to 60% faster end-to-end latency than prior models on 720p videos with no perceptible loss in quality. Video demos are available at this https URL', 'abstract_zh': '基于扩散的超分辨率（SR）是视频生成和视频恢复中的关键组件，但速度慢且成本高，限制了其在更高分辨率和更长视频上的应用规模。我们的核心洞察是，视频中的许多区域本质上细节较低，从精细加工中获益不大，但当前方法会均匀处理所有像素。为了利用这一点，我们提出了一种SkipSR框架，通过直接从低分辨率输入中识别低细节区域，完全跳过这些区域的计算，仅对需要精细加工的区域进行超分辨率处理。这种简单而有效的策略在标准和单步扩散SR模型中保持了感知质量的同时，显著减少了计算量。在标准SR基准测试中，我们的方法在720p视频上的端到端延迟比先前模型快60%，而无明显的质量损失。视频演示可在以下链接查看：this https URL。', 'title_zh': 'SkipSR：更快的Token跳过超分辨率'}
{'arxiv_id': 'arXiv:2510.08775', 'title': 'Re-Identifying Kākā with AI-Automated Video Key Frame Extraction', 'authors': 'Paula Maddigan, Andrew Lensen, Rachael C. Shaw', 'link': 'https://arxiv.org/abs/2510.08775', 'abstract': 'Accurate recognition and re-identification of individual animals is essential for successful wildlife population monitoring. Traditional methods, such as leg banding of birds, are time consuming and invasive. Recent progress in artificial intelligence, particularly computer vision, offers encouraging solutions for smart conservation and efficient automation. This study presents a unique pipeline for extracting high-quality key frames from videos of kākā (Nestor meridionalis), a threatened forest-dwelling parrot in New Zealand. Key frame extraction is well-studied in person re-identification, however, its application to wildlife is limited. Using video recordings at a custom-built feeder, we extract key frames and evaluate the re-identification performance of our pipeline. Our unsupervised methodology combines object detection using YOLO and Grounding DINO, optical flow blur detection, image encoding with DINOv2, and clustering methods to identify representative key frames. The results indicate that our proposed key frame selection methods yield image collections which achieve high accuracy in kākā re-identification, providing a foundation for future research using media collected in more diverse and challenging environments. Through the use of artificial intelligence and computer vision, our non-invasive and efficient approach provides a valuable alternative to traditional physical tagging methods for recognising kākā individuals and therefore improving the monitoring of populations. This research contributes to developing fresh approaches in wildlife monitoring, with applications in ecology and conservation biology.', 'abstract_zh': '个体动物的准确识别和重新识别对于成功的野生动物种群监测至关重要。传统方法，如鸟类脚环标记，耗时且侵入性。近年来，人工智能的进步，特别是计算机视觉，为智能保护和高效自动化提供了令人鼓舞的解决方案。本研究提出了一种独特的流水线，用于从新西兰森林居留鹦鹉 kākā（Nestor meridionalis）的视频中提取高质量的关键帧。关键帧提取在人员重新识别领域已有研究，但在野生动物领域应用有限。通过在自建喂食器拍摄的视频记录中提取关键帧并评估流水线的重新识别性能，本研究结合了使用 YOLO 和 Grounding DINO 进行对象检测、光学流动模糊检测、使用 DINOv2 进行图像编码以及聚类方法来识别代表性关键帧。结果表明，提出的关键帧选择方法产生了在 kākā 重新识别中实现高精度的图像集，为在更多样且更具挑战性环境中收集的媒体资料进行未来研究奠定了基础。通过使用人工智能和计算机视觉，本研究的非侵入性和高效方法为识别 kākā 个体提供了传统物理标记方法的有价值替代方案，并因此改善了种群监测。本研究有助于开发野生动物监测的新方法，具有生态学和保护生物学的应用价值。', 'title_zh': '使用AI自动化视频关键帧提取进行考氏鹦鹉的身份再识别'}
{'arxiv_id': 'arXiv:2510.08656', 'title': 'A 3D Generation Framework from Cross Modality to Parameterized Primitive', 'authors': 'Yiming Liang, Huan Yu, Zili Wang, Shuyou Zhang, Guodong Yi, Jin Wang, Jianrong Tan', 'link': 'https://arxiv.org/abs/2510.08656', 'abstract': 'Recent advancements in AI-driven 3D model generation have leveraged cross modality, yet generating models with smooth surfaces and minimizing storage overhead remain challenges. This paper introduces a novel multi-stage framework for generating 3D models composed of parameterized primitives, guided by textual and image inputs. In the framework, A model generation algorithm based on parameterized primitives, is proposed, which can identifies the shape features of the model constituent elements, and replace the elements with parameterized primitives with high quality surface. In addition, a corresponding model storage method is proposed, it can ensure the original surface quality of the model, while retaining only the parameters of parameterized primitives. Experiments on virtual scene dataset and real scene dataset demonstrate the effectiveness of our method, achieving a Chamfer Distance of 0.003092, a VIoU of 0.545, a F1-Score of 0.9139 and a NC of 0.8369, with primitive parameter files approximately 6KB in size. Our approach is particularly suitable for rapid prototyping of simple models.', 'abstract_zh': 'Recent Advancements in AI驱动的3D模型生成中的跨模态应用：基于参数化基础体的多阶段框架及其优化存储方法的研究', 'title_zh': '从跨模态到参数化基本元素的3D生成框架'}
{'arxiv_id': 'arXiv:2510.08638', 'title': 'Into the Rabbit Hull: From Task-Relevant Concepts in DINO to Minkowski Geometry', 'authors': 'Thomas Fel, Binxu Wang, Michael A. Lepori, Matthew Kowal, Andrew Lee, Randall Balestriero, Sonia Joseph, Ekdeep S. Lubana, Talia Konkle, Demba Ba, Martin Wattenberg', 'link': 'https://arxiv.org/abs/2510.08638', 'abstract': 'DINOv2 is routinely deployed to recognize objects, scenes, and actions; yet the nature of what it perceives remains unknown. As a working baseline, we adopt the Linear Representation Hypothesis (LRH) and operationalize it using SAEs, producing a 32,000-unit dictionary that serves as the interpretability backbone of our study, which unfolds in three parts.\nIn the first part, we analyze how different downstream tasks recruit concepts from our learned dictionary, revealing functional specialization: classification exploits "Elsewhere" concepts that fire everywhere except on target objects, implementing learned negations; segmentation relies on boundary detectors forming coherent subspaces; depth estimation draws on three distinct monocular depth cues matching visual neuroscience principles.\nFollowing these functional results, we analyze the geometry and statistics of the concepts learned by the SAE. We found that representations are partly dense rather than strictly sparse. The dictionary evolves toward greater coherence and departs from maximally orthogonal ideals (Grassmannian frames). Within an image, tokens occupy a low dimensional, locally connected set persisting after removing position. These signs suggest representations are organized beyond linear sparsity alone.\nSynthesizing these observations, we propose a refined view: tokens are formed by combining convex mixtures of archetypes (e.g., a rabbit among animals, brown among colors, fluffy among textures). This structure is grounded in Gardenfors\' conceptual spaces and in the model\'s mechanism as multi-head attention produces sums of convex mixtures, defining regions bounded by archetypes. We introduce the Minkowski Representation Hypothesis (MRH) and examine its empirical signatures and implications for interpreting vision-transformer representations.', 'abstract_zh': 'DINOv2常规用于识别物体、场景和动作，但其感知的本质仍然未知。作为工作基准，我们采纳线性表示假说（LRH），并通过SAEs实现，生成一个包含32,000个单元的词典，作为我们研究的可解释性核心，研究分为三个部分。', 'title_zh': 'Into the Rabbit Hole: 从DINO的相关概念到闵可夫斯基几何'}
