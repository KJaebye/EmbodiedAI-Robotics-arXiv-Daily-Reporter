{'arxiv_id': 'arXiv:2507.05125', 'title': 'Automated Behaviour-Driven Acceptance Testing of Robotic Systems', 'authors': 'Minh Nguyen, Sebastian Wrede, Nico Hochgeschwender', 'link': 'https://arxiv.org/abs/2507.05125', 'abstract': 'The specification and validation of robotics applications require bridging the gap between formulating requirements and systematic testing. This often involves manual and error-prone tasks that become more complex as requirements, design, and implementation evolve. To address this challenge systematically, we propose extending behaviour-driven development (BDD) to define and verify acceptance criteria for robotic systems. In this context, we use domain-specific modelling and represent composable BDD models as knowledge graphs for robust querying and manipulation, facilitating the generation of executable testing models. A domain-specific language helps to efficiently specify robotic acceptance criteria. We explore the potential for automated generation and execution of acceptance tests through a software architecture that integrates a BDD framework, Isaac Sim, and model transformations, focusing on acceptance criteria for pick-and-place applications. We tested this architecture with an existing pick-and-place implementation and evaluated the execution results, which shows how this application behaves and fails differently when tested against variations of the agent and environment. This research advances the rigorous and automated evaluation of robotic systems, contributing to their reliability and trustworthiness.', 'abstract_zh': '机器人应用的需求规范与验证需要弥合从需求制定到系统测试之间的差距。这通常涉及手动且易出错的任务，随着需求、设计和实现的演变，这些任务变得更加复杂。为系统地应对这一挑战，我们提议将行为驱动开发（BDD）扩展到定义和验证机器人系统的接受标准。在这一背景下，我们使用领域特定建模，并将可重用的BDD模型表示为知识图谱，以实现稳健的查询和操作，促进可执行测试模型的生成。领域特定语言有助于高效地指定机器人接受标准。我们通过集成BDD框架、Isaac Sim和模型转换的软件架构探索自动化生成和执行接受测试的潜力，重点关注拾取和放置应用的接受标准。我们使用现有拾取和放置实现测试了该架构，并评估了执行结果，展示了该应用在针对代理和环境的不同变体进行测试时表现出的行为和失败方式有何不同。这项研究推进了机器人系统的严谨和自动化评估，从而提高其可靠性和可信度。', 'title_zh': '基于行为驱动的自动验收测试在机器人系统中的应用'}
{'arxiv_id': 'arXiv:2507.05098', 'title': 'Beyond Features: How Dataset Design Influences Multi-Agent Trajectory Prediction Performance', 'authors': 'Tobias Demmler, Jakob Häringer, Andreas Tamke, Thao Dang, Alexander Hegai, Lars Mikelsons', 'link': 'https://arxiv.org/abs/2507.05098', 'abstract': 'Accurate trajectory prediction is critical for safe autonomous navigation, yet the impact of dataset design on model performance remains understudied. This work systematically examines how feature selection, cross-dataset transfer, and geographic diversity influence trajectory prediction accuracy in multi-agent settings. We evaluate a state-of-the-art model using our novel L4 Motion Forecasting dataset based on our own data recordings in Germany and the US. This includes enhanced map and agent features. We compare our dataset to the US-centric Argoverse 2 benchmark. First, we find that incorporating supplementary map and agent features unique to our dataset, yields no measurable improvement over baseline features, demonstrating that modern architectures do not need extensive feature sets for optimal performance. The limited features of public datasets are sufficient to capture convoluted interactions without added complexity. Second, we perform cross-dataset experiments to evaluate how effective domain knowledge can be transferred between datasets. Third, we group our dataset by country and check the knowledge transfer between different driving cultures.', 'abstract_zh': '准确的轨迹预测对于安全的自主导航至关重要，但数据集设计对模型性能的影响仍研究不足。本研究系统地探讨了特征选择、跨数据集迁移和地理多样性如何影响多智能体环境下的轨迹预测精度。我们使用基于德国和美国自身数据记录的新颖L4Motion Forecasting数据集评估了一种最先进的模型，该数据集包含增强的地图和智能体特征。我们将我们的数据集与以美国为中心的Argoverse 2基准进行比较。首先，我们发现将特定于我们数据集的独特地图和智能体特征纳入其中，并未在基线特征上带来可测量的提升，这表明现代架构在最优性能时不需要广泛的特征集。公共数据集的有限特征足以捕捉复杂的交互而不会增加复杂性。其次，我们进行了跨数据集实验，评估领域知识在数据集之间的迁移效果。第三，我们将数据集按国家分组，检查不同驾驶文化之间的知识迁移。', 'title_zh': '超越特征：数据集设计对多agent轨迹预测性能的影响'}
{'arxiv_id': 'arXiv:2507.04730', 'title': 'CueLearner: Bootstrapping and local policy adaptation from relative feedback', 'authors': 'Giulio Schiavi, Andrei Cramariuc, Lionel Ott, Roland Siegwart', 'link': 'https://arxiv.org/abs/2507.04730', 'abstract': 'Human guidance has emerged as a powerful tool for enhancing reinforcement learning (RL). However, conventional forms of guidance such as demonstrations or binary scalar feedback can be challenging to collect or have low information content, motivating the exploration of other forms of human input. Among these, relative feedback (i.e., feedback on how to improve an action, such as "more to the left") offers a good balance between usability and information richness. Previous research has shown that relative feedback can be used to enhance policy search methods. However, these efforts have been limited to specific policy classes and use feedback inefficiently. In this work, we introduce a novel method to learn from relative feedback and combine it with off-policy reinforcement learning. Through evaluations on two sparse-reward tasks, we demonstrate our method can be used to improve the sample efficiency of reinforcement learning by guiding its exploration process. Additionally, we show it can adapt a policy to changes in the environment or the user\'s preferences. Finally, we demonstrate real-world applicability by employing our approach to learn a navigation policy in a sparse reward setting.', 'abstract_zh': '人类指导已成为增强强化学习(RL)性能的强大工具。然而，传统的指导形式如演示或二元标量反馈在收集上具有挑战性或信息含量低，促使探索其他形式的人类输入。在这些形式中，相对反馈（即对改善动作的反馈，如“更往左”）在可用性和信息丰富性之间提供了良好的平衡。之前的研究所表明，相对反馈可以用于增强策略搜索方法。然而，这些努力仅限于特定的策略类别，并且使用反馈效率低下。在本工作中，我们提出了一种新的方法来自学相对反馈，并将其与脱政策强化学习结合。通过在两个稀疏奖励任务上的评估，我们证明该方法可以提高强化学习的样本效率，引导其探索过程。此外，我们展示了该方法可以根据环境变化或用户偏好进行政策调整。最后，我们通过将该方法应用于稀疏奖励环境下的导航策略学习，展示了其实用性。', 'title_zh': 'CueLearner: 从相对反馈进行自强化和局部策略适应'}
{'arxiv_id': 'arXiv:2507.04568', 'title': 'The Difference between the Left and Right Invariant Extended Kalman Filter', 'authors': 'Yixiao Ge, Giulio Delama, Martin Scheiber, Alessandro Fornasier, Pieter van Goor, Stephan Weiss, Robert Mahony', 'link': 'https://arxiv.org/abs/2507.04568', 'abstract': "The extended Kalman filter (EKF) has been the industry standard for state estimation problems over the past sixty years. The Invariant Extended Kalman Filter (IEKF) is a recent development of the EKF for the class of group-affine systems on Lie groups that has shown superior performance for inertial navigation problems. The IEKF comes in two versions, left- and right- handed respectively, and there is a perception in the robotics community that these filters are different and one should choose the handedness of the IEKF to match handedness of the measurement model for a given filtering problem. In this paper, we revisit these algorithms and demonstrate that the left- and right- IEKF algorithms (with reset step) are identical, that is, the choice of the handedness does not affect the IEKF's performance when the reset step is properly implemented. The reset step was not originally proposed as part of the IEKF, however, we provide simulations to show that the reset step improves asymptotic performance of all versions of the the filter, and should be included in all high performance algorithms. The GNSS-aided inertial navigation system (INS) is used as a motivating example to demonstrate the equivalence of the two filters.", 'abstract_zh': '扩展卡尔曼滤波器(EKF)在过去六十年中一直是状态估计问题的工业标准。不变扩展卡尔曼滤波器(IEKF)是针对李群上群仿射系统的最近发展，已在惯性导航问题中显示出优越的性能。IEKF有两种版本，分别是左手型和右手型，机器人学界有一种观点认为这些滤波器是不同的，使用者应选择与给定滤波问题测量模型手性的IEKF。本文重新审视了这些算法，并证明在适当实施重置步骤的情况下，左手型和右手型IEKF算法（含重置步骤）是相同的，即手性的选择不会影响IEKF的性能。重置步骤最初未被纳入IEKF，但通过仿真展示了该步骤可以改善所有版本滤波器的渐近性能，并应在高性能算法中包含。全球导航卫星系统辅助惯性导航系统(GNSS-aided INS)被用作演示两种滤波器等价性的动机例子。', 'title_zh': '左 invariant 扩展卡尔曼滤波器与右 invariant 扩展卡尔曼滤波器的区别'}
{'arxiv_id': 'arXiv:2507.04314', 'title': 'Hardware-Free Event Cameras Temporal Synchronization Based on Event Density Alignment', 'authors': 'Wenxuan Li, Yan Dong, Shaoqiang Qiu, Bin Han', 'link': 'https://arxiv.org/abs/2507.04314', 'abstract': "Event cameras are a novel type of sensor designed for capturing the dynamic changes of a scene. Due to factors such as trigger and transmission delays, a time offset exists in the data collected by multiple event cameras, leading to inaccurate information fusion. Thus, the collected data needs to be synchronized to overcome any potential time offset issue. Hardware synchronization methods require additional circuits, while certain models of event cameras (e.g., CeleX5) do not support hardware synchronization. Therefore, this paper proposes a hardware-free event camera synchronization method. This method determines differences between start times by minimizing the dissimilarity of the event density distributions of different event cameras and synchronizes the data by adjusting timestamps. The experiments demonstrate that the method's synchronization error is less than 10ms under various senses with multiple models of event cameras.", 'abstract_zh': '基于事件的摄像机无硬件同步方法及其应用', 'title_zh': '基于事件密度对齐的无硬件事件相机时间同步'}
{'arxiv_id': 'arXiv:2507.03992', 'title': 'Scalable Learning of High-Dimensional Demonstrations with Composition of Linear Parameter Varying Dynamical Systems', 'authors': 'Shreenabh Agrawal, Hugo T. M. Kussaba, Lingyun Chen, Allen Emmanuel Binny, Abdalla Swikir, Pushpak Jagtap, Sami Haddadin', 'link': 'https://arxiv.org/abs/2507.03992', 'abstract': 'Learning from Demonstration (LfD) techniques enable robots to learn and generalize tasks from user demonstrations, eliminating the need for coding expertise among end-users. One established technique to implement LfD in robots is to encode demonstrations in a stable Dynamical System (DS). However, finding a stable dynamical system entails solving an optimization problem with bilinear matrix inequality (BMI) constraints, a non-convex problem which, depending on the number of scalar constraints and variables, demands significant computational resources and is susceptible to numerical issues such as floating-point errors. To address these challenges, we propose a novel compositional approach that enhances the applicability and scalability of learning stable DSs with BMIs.', 'abstract_zh': '从演示学习的技术使机器人能够通过用户的演示学习和泛化任务，消除终用户编程知识的需要。一种在机器人中实现从演示学习的技术是将演示编码为稳定的动力学系统（DS）。然而，寻找稳定的动力学系统涉及求解带有双线性矩阵不等式（BMI）约束的优化问题，这是一个非凸问题，可能会因标量约束和变量的数量而需要大量的计算资源，并且容易出现浮点误差等数值问题。为解决这些问题，我们提出了一种新的组合方法，以增强使用BMI学习稳定DS的适用性和可扩展性。', 'title_zh': '高维演示的大规模学习：线性参数 varying 动态系统组成的方法'}
{'arxiv_id': 'arXiv:2507.04459', 'title': 'Agentic Distributed Computing', 'authors': 'Ajay D. Kshemkalyani, Manish Kumar, Anisur Rahaman Molla, Gokarna Sharma', 'link': 'https://arxiv.org/abs/2507.04459', 'abstract': 'The most celebrated and extensively studied model of distributed computing is the {\\em message-passing model,} in which each vertex/node of the (distributed network) graph corresponds to a static computational device that communicates with other devices through passing messages. In this paper, we consider the {\\em agentic model} of distributed computing which extends the message-passing model in a new direction. In the agentic model, computational devices are modeled as relocatable or mobile computational devices (called agents in this paper), i.e., each vertex/node of the graph serves as a container for the devices, and hence communicating with another device requires relocating to the same node. We study two fundamental graph level tasks, leader election, and minimum spanning tree, in the agentic model, which will enhance our understanding of distributed computation across paradigms. The objective is to minimize both time and memory complexities. Following the literature, we consider the synchronous setting in which each agent performs its operations synchronously with others, and hence the time complexity can be measured in rounds. In this paper, we present two deterministic algorithms for leader election: one for the case of $k<n$ and another for the case of $k=n$, minimizing both time and memory complexities, where $k$ and $n$, respectively, are the number of agents and number of nodes of the graph. Using these leader election results, we develop deterministic algorithms for agents to construct a minimum spanning tree of the graph, minimizing both time and memory complexities. To the best of our knowledge, this is the first study of distributed graph level tasks in the agentic model with $k\\leq n$. Previous studies only considered the case of $k=n$.', 'abstract_zh': '分布式计算中最为著名且被广泛研究的模型是消息传递模型，在该模型中，分布式网络图的每个顶点/节点对应一个静态计算设备，这些设备通过传递消息进行通信。本文考虑的是代理模型，该模型扩展了消息传递模型的一个新方向。在代理模型中，计算设备被建模为可重新定位的或移动的计算设备（在本文中称为代理），即图的每个顶点/节点是这些设备的容器，因此与其他设备通信需要移动到同一节点。我们研究了代理模型下的两个基本图级任务：领导者选举和最小生成树，这将增强我们对分布式计算在不同范式中的理解。目标是同时最小化时间和内存复杂度。根据文献，我们将考察同步设置，在这种设置中，每个代理与其他代理同步执行其操作，因此时间复杂度可以用轮次来度量。本文我们提出了两种确定性领导选举算法：一种适用于\\(k<n\\)的情况，另一种适用于\\(k=n\\)的情况，其中\\(k\\)和\\(n\\)分别表示代理的数量和图的节点数量，同时最小化时间和内存复杂度。利用这些领导选举的结果，我们开发了两种确定性算法，使代理能够构建图的最小生成树，同时最小化时间和内存复杂度。据我们所知，这是首次在代理模型下（\\(k \\leq n\\))研究分布式图级任务的文献。以往的研究仅考虑了\\(k=n\\)的情况。', 'title_zh': '代理式分布式计算'}
{'arxiv_id': 'arXiv:2507.03856', 'title': 'Robust Node Localization for Rough and Extreme Deployment Environments', 'authors': 'Abiy Tasissa, Waltenegus Dargie', 'link': 'https://arxiv.org/abs/2507.03856', 'abstract': 'Many applications have been identified which require the deployment of large-scale low-power wireless sensor networks. Some of the deployment environments, however, impose harsh operation conditions due to intense cross-technology interference, extreme weather conditions (heavy rainfall, excessive heat, etc.), or rough motion, thereby affecting the quality and predictability of the wireless links the nodes establish. In localization tasks, these conditions often lead to significant errors in estimating the position of target nodes. Motivated by the practical deployments of sensors on the surface of different water bodies, we address the problem of identifying susceptible nodes and robustly estimating their positions. We formulate these tasks as a compressive sensing problem and propose algorithms for both node identification and robust estimation. Additionally, we design an optimal anchor configuration to maximize the robustness of the position estimation task. Our numerical results and comparisons with competitive methods demonstrate that the proposed algorithms achieve both objectives with a modest number of anchors. Since our method relies only on target-to-anchor distances, it is broadly applicable and yields resilient, robust localization.', 'abstract_zh': '大规模低功耗无线 sensor 网络在多种应用中的部署研究：识别易受影响节点并 robust 估计其位置', 'title_zh': '鲁棒节点定位技术在恶劣和极端部署环境中的应用'}
{'arxiv_id': 'arXiv:2507.02942', 'title': 'Control Synthesis in Partially Observable Environments for Complex Perception-Related Objectives', 'authors': 'Zetong Xuan, Yu Wang', 'link': 'https://arxiv.org/abs/2507.02942', 'abstract': 'Perception-related tasks often arise in autonomous systems operating under partial observability. This work studies the problem of synthesizing optimal policies for complex perception-related objectives in environments modeled by partially observable Markov decision processes. To formally specify such objectives, we introduce \\emph{co-safe linear inequality temporal logic} (sc-iLTL), which can define complex tasks that are formed by the logical concatenation of atomic propositions as linear inequalities on the belief space of the POMDPs. Our solution to the control synthesis problem is to transform the \\mbox{sc-iLTL} objectives into reachability objectives by constructing the product of the belief MDP and a deterministic finite automaton built from the sc-iLTL objective. To overcome the scalability challenge due to the product, we introduce a Monte Carlo Tree Search (MCTS) method that converges in probability to the optimal policy. Finally, a drone-probing case study demonstrates the applicability of our method.', 'abstract_zh': '部分可观测条件下感知相关任务往往在自主系统中出现。本文研究了在部分可观测马尔可夫决策过程建模的环境中合成复杂感知相关目标的最优策略问题。为正式指定此类目标，本文引入了\\emph{co-safe线性不等式时序逻辑}（sc-iLTL），它可以将由POMDP信念空间上的原子命题的逻辑连接定义的复杂任务构造成线性不等式。我们解决控制合成问题的方法是通过构建POMDP信念MDP与从sc-iLTL目标构建的确定性有限自动机的积来将sc-iLTL目标转换为可达性目标。为了克服由于积带来的可扩展性挑战，本文引入了一种蒙特卡罗树搜索（MCTS）方法，该方法以概率收敛于最优策略。最后，无人机探测案例研究展示了我们方法的应用。', 'title_zh': '部分可观测环境中复杂感知相关目标的控制合成'}
{'arxiv_id': 'arXiv:2507.02867', 'title': 'A Simulator Dataset to Support the Study of Impaired Driving', 'authors': 'John Gideon, Kimimasa Tamura, Emily Sumner, Laporsha Dees, Patricio Reyes Gomez, Bassamul Haq, Todd Rowell, Avinash Balachandran, Simon Stent, Guy Rosman', 'link': 'https://arxiv.org/abs/2507.02867', 'abstract': 'Despite recent advances in automated driving technology, impaired driving continues to incur a high cost to society. In this paper, we present a driving dataset designed to support the study of two common forms of driver impairment: alcohol intoxication and cognitive distraction. Our dataset spans 23.7 hours of simulated urban driving, with 52 human subjects under normal and impaired conditions, and includes both vehicle data (ground truth perception, vehicle pose, controls) and driver-facing data (gaze, audio, surveys). It supports analysis of changes in driver behavior due to alcohol intoxication (0.10\\% blood alcohol content), two forms of cognitive distraction (audio n-back and sentence parsing tasks), and combinations thereof, as well as responses to a set of eight controlled road hazards, such as vehicle cut-ins. The dataset will be made available at this https URL.', 'abstract_zh': '尽管自动驾驶技术recent取得了进步，酒后驾驶和认知分心仍对社会造成重大影响。本文介绍了一个用于研究酒后驾驶和认知分心这两种常见驾驶员 impairment 影响的驾驶数据集。该数据集涵盖了23.7小时的模拟城市驾驶场景，涉及52名在正常和 impaired 条件下的受试者，并包含了车辆数据（真实感知、车辆姿态、控制）和面向驾驶员的数据（注视点、音频、问卷调查）。它支持分析血液酒精含量为0.10%时酒后驾驶对驾驶员行为的影响、两种类型的认知分心（音频n-back任务和句子解析任务）以及这些影响的组合，同时还涵盖了受控道路危害（如车辆切入）的应对情况。该数据集将在此网址获取：this https URL。', 'title_zh': '驾驶能力受损Simulator数据集'}
{'arxiv_id': 'arXiv:2507.05244', 'title': 'Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration', 'authors': 'Benjamin Li, Shuyang Shi, Lucia Romero, Huao Li, Yaqi Xie, Woojun Kim, Stefanos Nikolaidis, Michael Lewis, Katia Sycara, Simon Stepputtis', 'link': 'https://arxiv.org/abs/2507.05244', 'abstract': 'In collaborative tasks, being able to adapt to your teammates is a necessary requirement for success. When teammates are heterogeneous, such as in human-agent teams, agents need to be able to observe, recognize, and adapt to their human partners in real time. This becomes particularly challenging in tasks with time pressure and complex strategic spaces where the dynamics can change rapidly. In this work, we introduce TALENTS, a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a range of partner strategies, enabling ad-hoc teamwork. Our approach utilizes a variational autoencoder to learn a latent strategy space from trajectory data. This latent space represents the underlying strategies that agents employ. Subsequently, the system identifies different types of strategy by clustering the data. Finally, a cooperator agent is trained to generate partners for each type of strategy, conditioned on these clusters. In order to adapt to previously unseen partners, we leverage a fixed-share regret minimization algorithm that infers and adjusts the estimated partner strategy dynamically. We assess our approach in a customized version of the Overcooked environment, posing a challenging cooperative cooking task that demands strong coordination across a wide range of possible strategies. Using an online user study, we show that our agent outperforms current baselines when working with unfamiliar human partners.', 'abstract_zh': '协作任务中，能够适应队友是成功的一个必要条件。当队友具有异质性，例如在人类-代理团队中，代理需要能够实时观察、识别并适应其人类伙伴。特别是在具有时间压力和复杂战略空间的任务中，动态变化尤为迅速，这给适应带来极大挑战。在此项工作中，我们引入了TALENTS框架，这是一种基于策略条件的合作者框架，旨在学习表示、分类和适应一系列合作伙伴策略，从而实现临时团队协作。该方法利用变分自编码器从轨迹数据中学习潜在策略空间。该潜在空间表示代理所采用的基本策略。随后，系统通过聚类数据识别不同类型的策略。最后，训练一个合作者代理，使其在特定聚类条件下生成相应类型的合作伙伴。为了适应之前未见过的合作伙伴，我们采用了固定份额遗憾最小化算法，该算法能够动态推断并调整估计的伙伴策略。我们在定制化的Overcooked环境中评估了我们的方法，该环境提出了一个具有挑战性的合作烹饪任务，要求在广泛的可能策略组合中实现强大的协调。通过在线用户研究，我们表明我们的代理在与不熟悉的_human_合作伙伴合作时性能优于现有基准。', 'title_zh': '建模潜在合作伙伴策略以实现自适应零样本人类-代理协作'}
{'arxiv_id': 'arXiv:2507.05241', 'title': "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?", 'authors': 'Jingyi Chai, Shuo Tang, Rui Ye, Yuwen Du, Xinyu Zhu, Mengcheng Zhou, Yanfeng Wang, Weinan E, Siheng Chen', 'link': 'https://arxiv.org/abs/2507.05241', 'abstract': "The rapid advancements of AI agents have ignited the long-held ambition of leveraging them to accelerate scientific discovery. Achieving this goal requires a deep understanding of the frontiers of human knowledge. As such, Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for evaluating scientific AI agents. In this work, we aim to construct the foundational architecture for general-purpose agents and validate the capabilities through leading performance on HLE. To achieve this, we introduce X-Master, a tool-augmented reasoning agent designed to emulate human researchers by interacting flexibly with external tools during its reasoning process. This agent, guided by the conceptualization of code as an interaction language, can flexibly leverage built-in Python libraries and our customized tools to augment the reasoning. We further scale its capabilities through X-Masters, a scattered-and-stacked agentic workflow that systematically enhances breadth and depth of reasoning. Our open-source solution, X-Masters, sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to exceed the 30% threshold. This work allows us to gain a deeper understanding of complex task-solving and accumulates valuable experience that can inform future advancements, guiding subsequent model training.", 'abstract_zh': 'AI代理的快速发展掀起了利用它们加速科学发现的长期愿景。实现这一目标需要深刻理解人类知识的前沿。为此，人类最后考试（HLE）提供了评估科学AI代理的极其具有挑战性的标准。在本文中，我们旨在构建通用代理的基础架构，并通过在HLE上的领先性能来验证其能力。为此，我们介绍了X-Master，一种工具增强的推理代理，旨在通过在其推理过程中灵活与外部工具交互来模仿人类研究人员。该代理根据代码作为交互语言的概念，可以灵活地利用内置的Python库和我们定制的工具来增强推理能力。我们进一步通过X-Masters的分散和堆叠代理工作流扩展其能力，该工作流系统地增强了推理的广度和深度。我们的开源解决方案X-Masters在HLE上取得了新的最佳成绩，得分为32.1%，超越了OpenAI的和Google的Deep Research（分别为26.6%和26.9%），成为首个超过30%阈值的系统。这项工作使我们能够更深入地了解复杂任务解决问题，并积累了宝贵的经验，这些经验可以指导未来的进步，指导后续模型训练。', 'title_zh': 'SciMaster: 通用于科学的人工智能代理探索，第一部分——X-Master作为基础：我们能否通过人类的最后一考？'}
{'arxiv_id': 'arXiv:2507.05142', 'title': 'GIST: Cross-Domain Click-Through Rate Prediction via Guided Content-Behavior Distillation', 'authors': 'Wei Xu, Haoran Li, Baoyuan Ou, Lai Xu, Yingjie Qin, Ruilong Su, Ruiwen Xu', 'link': 'https://arxiv.org/abs/2507.05142', 'abstract': 'Cross-domain Click-Through Rate prediction aims to tackle the data sparsity and the cold start problems in online advertising systems by transferring knowledge from source domains to a target domain. Most existing methods rely on overlapping users to facilitate this transfer, often focusing on joint training or pre-training with fine-tuning approach to connect the source and target domains. However, in real-world industrial settings, joint training struggles to learn optimal representations with different distributions, and pre-training with fine-tuning is not well-suited for continuously integrating new data. To address these issues, we propose GIST, a cross-domain lifelong sequence model that decouples the training processes of the source and target domains. Unlike previous methods that search lifelong sequences in the source domains using only content or behavior signals or their simple combinations, we innovatively introduce a Content-Behavior Joint Training Module (CBJT), which aligns content-behavior distributions and combines them with guided information to facilitate a more stable representation. Furthermore, we develop an Asymmetric Similarity Integration strategy (ASI) to augment knowledge transfer through similarity computation. Extensive experiments demonstrate the effectiveness of GIST, surpassing SOTA methods on offline evaluations and an online A/B test. Deployed on the Xiaohongshu (RedNote) platform, GIST effectively enhances online ads system performance at scale, serving hundreds of millions of daily active users.', 'abstract_zh': '跨领域点击率预测旨在通过从源领域转移到目标领域来解决在线广告系统中的数据稀疏性和冷启动问题。大多数现有方法依赖于重叠用户来促进这一转移，通常集中于通过联合训练或预训练加微调的方法来连接源和目标领域。然而，在实际工业环境中，联合训练难以学习具有不同分布的最佳表示，而预训练加微调也不适合持续集成新数据。为了解决这些问题，我们提出了一种跨领域终身序列模型GIST，将源领域和目标领域的训练过程分离。与之前方法仅使用内容或行为信号及其简单组合在源领域中搜索终身序列不同，我们创新性地引入了内容-行为联合训练模块（CBJT），该模块对齐内容-行为分布并结合引导信息，以促进更稳定的表现。此外，我们开发了非对称相似性集成策略（ASI）以通过相似性计算增强知识转移。广泛的实验表明，GIST在离线评估和在线A/B测试中均优于当前最佳方法。部署在小红书（RedNote）平台上，GIST有效增强了大规模在线广告系统的性能，服务于数亿日活跃用户。', 'title_zh': 'GIST：通过指导内容-行为提炼进行跨域点击率预测'}
{'arxiv_id': 'arXiv:2507.05110', 'title': 'Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution Shift', 'authors': 'Shixuan Liu, Yue He, Yunfei Wang, Hao Zou, Haoxiang Cheng, Wenjing Yang, Peng Cui, Zhong Liu', 'link': 'https://arxiv.org/abs/2507.05110', 'abstract': "Knowledge graph (KG) reasoning remains a critical research area focused on inferring missing knowledge by analyzing relationships among observed facts. Despite its success, a key limitation of existing KG reasoning methods is their dependence on the I.I.D assumption. This assumption can easily be violated due to unknown sample selection bias during training or agnostic distribution shifts during testing, significantly compromising model performance and reliability. To facilitate the deployment of KG reasoning in wild environments, this study investigates learning logical rules from KGs affected by unknown selection bias. Additionally, we address test sets with agnostic distribution shifts, formally defining this challenge as out-of-distribution (OOD) KG reasoning-a previously underexplored problem. To solve the issue, we propose the Stable Rule Learning (StableRule) framework, an end-to-end methodology that integrates feature decorrelation with rule learning network, to enhance OOD generalization performance. By leveraging feature decorrelation, the StableRule framework mitigates the adverse effects of covariate shifts arising in OOD scenarios, thereby improving the robustness of the rule learning component in effectively deriving logical rules. Extensive experiments on seven benchmark KGs demonstrate the framework's superior effectiveness and stability across diverse heterogeneous environments, underscoring its practical significance for real-world applications.", 'abstract_zh': '知识图谱推理中的未知选择偏差下稳定逻辑规则学习', 'title_zh': 'agnostic分布迁移下的知识图谱推理规则学习'}
{'arxiv_id': 'arXiv:2507.05088', 'title': 'How Rules Represent Causal Knowledge: Causal Modeling with Abductive Logic Programs', 'authors': 'Kilian Rückschloß, Felix Weitkämper', 'link': 'https://arxiv.org/abs/2507.05088', 'abstract': "Pearl observes that causal knowledge enables predicting the effects of interventions, such as actions, whereas descriptive knowledge only permits drawing conclusions from observation. This paper extends Pearl's approach to causality and interventions to the setting of stratified abductive logic programs. It shows how stable models of such programs can be given a causal interpretation by building on philosophical foundations and recent work by Bochman and Eelink et al. In particular, it provides a translation of abductive logic programs into causal systems, thereby clarifying the informal causal reading of logic program rules and supporting principled reasoning about external actions. The main result establishes that the stable model semantics for stratified programs conforms to key philosophical principles of causation, such as causal sufficiency, natural necessity, and irrelevance of unobserved effects. This justifies the use of stratified abductive logic programs as a framework for causal modeling and for predicting the effects of interventions", 'abstract_zh': 'Pearl指出，因果知识能使我们预测干预（如行动）的效果，而描述性知识仅能从观察中得出结论。本文将Pearl的因果推理方法扩展到分层 abduction 逻辑程序的设置中。通过建立在哲学基础之上，并结合Bochman及Eelink等人近期的工作，展示了如何给这样的程序的稳定模型赋予因果解释。特别地，本文提供了将 abduction 逻辑程序翻译成因果系统的翻译方法，从而澄清了逻辑程序规则的非正式因果阅读，并支持对外部行动进行原则性的推理。主要结果表明，分层程序的稳定模型语义符合因果哲学原则的关键标准，如因果完备性、自然必要性和未观察效应的相关性不重要。这证明了使用分层 abduction 逻辑程序作为因果建模框架以及预测干预效果的有效性。', 'title_zh': '规则如何表示因果知识：基于 abduction 逻辑程序的因果建模'}
{'arxiv_id': 'arXiv:2507.04994', 'title': 'Supported Abstract Argumentation for Case-Based Reasoning', 'authors': 'Adam Gould, Gabriel de Olim Gaul, Francesca Toni', 'link': 'https://arxiv.org/abs/2507.04994', 'abstract': 'We introduce Supported Abstract Argumentation for Case-Based Reasoning (sAA-CBR), a binary classification model in which past cases engage in debates by arguing in favour of their labelling and attacking or supporting those with opposing or agreeing labels. With supports, sAA-CBR overcomes the limitation of its precursor AA-CBR, which can contain extraneous cases (or spikes) that are not included in the debates. We prove that sAA-CBR contains no spikes, without trading off key model properties', 'abstract_zh': '基于支持的抽象论辩案例推理（sAA-CBR）：一种二分类模型', 'title_zh': '基于支持的抽象论辩案例基于推理'}
{'arxiv_id': 'arXiv:2507.04722', 'title': 'LumiCRS: Asymmetric Contrastive Prototype Learning for Long-Tail Conversational Movie Recommendation', 'authors': 'Jinzhi Wang, Bin Li, Qingke Peng, Haozhou Li, Zeyuan Zeng, Ruimeng Li, Biyi Zhou', 'link': 'https://arxiv.org/abs/2507.04722', 'abstract': 'Conversational recommender systems (CRSs) often suffer from an extreme long-tail distribution of dialogue data, causing a strong bias toward head-frequency blockbusters that sacrifices diversity and exacerbates the cold-start problem. An empirical analysis of DCRS and statistics on the REDIAL corpus show that only 10% of head movies account for nearly half of all mentions, whereas about 70% of tail movies receive merely 26% of the attention. This imbalance gives rise to three critical challenges: head over-fitting, body representation drift, and tail sparsity. To address these issues, we propose LumiCRS, an end-to-end framework that mitigates long-tail imbalance through three mutually reinforcing layers: (i) an Adaptive Comprehensive Focal Loss (ACFL) that dynamically adjusts class weights and focusing factors to curb head over-fitting and reduce popularity bias; (ii) Prototype Learning for Long-Tail Recommendation, which selects semantic, affective, and contextual prototypes to guide clustering and stabilize body and tail representations; and (iii) a GPT-4o-driven prototype-guided dialogue augmentation module that automatically generates diverse long-tail conversational snippets to alleviate tail sparsity and distribution shift. Together, these strategies enable LumiCRS to markedly improve recommendation accuracy, diversity, and fairness: on the REDIAL and INSPIRED benchmarks, LumiCRS boosts Recall@10 and Tail-Recall@10 by 7-15% over fifteen strong baselines, while human evaluations confirm superior fluency, informativeness, and long-tail relevance. These results demonstrate the effectiveness of multi-layer collaboration in building an efficient and fair long-tail conversational recommender.', 'abstract_zh': '长尾对话数据分布不平衡的会话推荐系统（CRSs）往往受到头部效应的严重影响，导致高度偏向于高频率的头部项目，牺牲了多样性并加剧了冷启动问题。对DCRS的实证分析和REDIAL语料库的统计数据表明，只有10%的头部电影占据了近一半的提及次数，而大约70%的尾部电影仅获得了26%的关注度。这种不平衡引发了三个关键挑战：头部过拟合、身体表示漂移和尾部稀疏性。为了解决这些问题，我们提出了LumiCRS，这是一种端到端框架，通过三层相互加强的机制缓解长尾不平衡：（i）自适应全面焦点损失（ACFL），动态调整类别权重和焦点因子以抑制头部过拟合并减少流行度偏见；（ii）长尾推荐的原型学习，选择语义、情感和上下文原型来指导聚类并稳定身体和尾部表示；以及（iii）由GPT-4o驱动的基于原型的对话增强模块，它可以自动生成多样化的长尾对话片段，以减轻尾部稀疏性和分布偏移。这些策略使得LumiCRS显著提高了推荐精度、多样性和公平性：在REDIAL和INSPIRED基准上，LumiCRS相比十五个强劲的基线，在Recall@10和Tail-Recall@10上提高了7-15%，而人工评估证实了其在流畅性、信息量和长尾相关性方面的优越性。这些结果证明了在构建高效的公平长尾会话推荐系统中多层协作的有效性。', 'title_zh': 'LumiCRS: 不对称对比原型学习在长尾对话电影推荐中的应用'}
{'arxiv_id': 'arXiv:2507.04719', 'title': 'Advocate for Complete Benchmarks for Formal Reasoning with Formal/Informal Statements and Formal/Informal Proofs', 'authors': 'Roozbeh Yousefzadeh, Xuenan Cao', 'link': 'https://arxiv.org/abs/2507.04719', 'abstract': 'This position paper provides a critical but constructive discussion of current practices in benchmarking and evaluative practices in the field of formal reasoning and automated theorem proving. We take the position that open code, open data, and benchmarks that are complete and error-free will accelerate progress in this field. We identify practices that create barriers to contributing to this field and suggest ways to remove them. We also discuss some of the practices that might produce misleading evaluative information. We aim to create discussions that bring together people from various groups contributing to automated theorem proving, autoformalization, and informal reasoning.', 'abstract_zh': '这篇立场论文对形式推理和自动定理证明领域当前的基准测试和评估实践进行了批判性但建设性的讨论。我们认为，开源代码、开源数据以及完整无误的基准测试将加速该领域的发展。我们指出了阻碍贡献该领域的实践，并提出了去除这些障碍的方法。同时，我们讨论了一些可能导致误导性评价信息的做法。我们的目标是促进来自自动化定理证明、自动形式化和非形式推理等领域贡献者的讨论。', 'title_zh': '倡导完整的基准以形形兼备的陈述和证明进行形式化推理'}
{'arxiv_id': 'arXiv:2507.04600', 'title': 'DisMS-TS: Eliminating Redundant Multi-Scale Features for Time Series Classification', 'authors': 'Zhipeng Liu, Peibo Duan, Binwu Wang, Xuan Tang, Qi Chu, Changsheng Zhang, Yongsheng Huang, Bin Zhang', 'link': 'https://arxiv.org/abs/2507.04600', 'abstract': 'Real-world time series typically exhibit complex temporal variations, making the time series classification task notably challenging. Recent advancements have demonstrated the potential of multi-scale analysis approaches, which provide an effective solution for capturing these complex temporal patterns. However, existing multi-scale analysis-based time series prediction methods fail to eliminate redundant scale-shared features across multi-scale time series, resulting in the model over- or under-focusing on scale-shared features. To address this issue, we propose a novel end-to-end Disentangled Multi-Scale framework for Time Series classification (DisMS-TS). The core idea of DisMS-TS is to eliminate redundant shared features in multi-scale time series, thereby improving prediction performance. Specifically, we propose a temporal disentanglement module to capture scale-shared and scale-specific temporal representations, respectively. Subsequently, to effectively learn both scale-shared and scale-specific temporal representations, we introduce two regularization terms that ensure the consistency of scale-shared representations and the disparity of scale-specific representations across all temporal scales. Extensive experiments conducted on multiple datasets validate the superiority of DisMS-TS over its competitive baselines, with the accuracy improvement up to 9.71%.', 'abstract_zh': 'Real-world时间序列通常表现出复杂的时变特性，使得时间序列分类任务尤为具有挑战性。近年来的研究表明，多尺度分析方法具有潜在的价值，能够有效捕获这些复杂的时变模式。然而，现有的基于多尺度分析的时间序列预测方法未能消除多尺度时间序列中的冗余共享特征，导致模型过度或不足地关注共享特征。为了解决这一问题，我们提出了一种新的端到端解耦多尺度框架（DisMS-TS）用于时间序列分类。DisMS-TS的核心思想是消除多尺度时间序列中的冗余共享特征，从而提高预测性能。具体而言，我们提出了一种时序解耦模块分别捕获多尺度共享和多尺度特定的时间表示。随后，为了有效地学习多尺度共享和特定的时间表示，我们引入了两种正则化项，以确保所有时序尺度上共享表示的一致性和特定表示的差异性。在多个数据集上进行的广泛实验验证了DisMS-TS在多个基准方法中的优越性，准确率提高了高达9.71%。', 'title_zh': 'DisMS-TS: 消除时间序列分类中的冗余多尺度特征'}
{'arxiv_id': 'arXiv:2507.04594', 'title': 'Exploring Core and Periphery Precepts in Biological and Artificial Intelligence: An Outcome-Based Perspective', 'authors': 'Niloofar Shadab, Tyler Cody, Alejandro Salado, Taylan G. Topcu, Mohammad Shadab, Peter Beling', 'link': 'https://arxiv.org/abs/2507.04594', 'abstract': 'Engineering methodologies predominantly revolve around established principles of decomposition and recomposition. These principles involve partitioning inputs and outputs at the component level, ensuring that the properties of individual components are preserved upon composition. However, this view does not transfer well to intelligent systems, particularly when addressing the scaling of intelligence as a system property. Our prior research contends that the engineering of general intelligence necessitates a fresh set of overarching systems principles. As a result, we introduced the "core and periphery" principles, a novel conceptual framework rooted in abstract systems theory and the Law of Requisite Variety. In this paper, we assert that these abstract concepts hold practical significance. Through empirical evidence, we illustrate their applicability to both biological and artificial intelligence systems, bridging abstract theory with real-world implementations. Then, we expand on our previous theoretical framework by mathematically defining core-dominant vs periphery-dominant systems.', 'abstract_zh': '工程方法主要围绕分解与重组的既定原则进行。这些原则涉及在组件级别划分输入和输出，并确保在组合时个体组件的属性得以保留。然而，这种观点在处理智能系统中的智能扩展这一系统属性时并不适用。我们此前的研究认为，通用人工智能的工程需要一套新的综合系统原则。因此，我们引入了“核心和外围”原则，这是一个基于抽象系统理论和必要的变异性定律的新颖概念框架。在本文中，我们认为这些抽象概念具有实际意义。通过实验证据，我们展示了它们在生物学和人工智能系统中的适用性，将抽象理论与实际应用相结合。然后，我们通过数学定义进一步扩展了我们之前理论框架，区分核心主导系统与外围主导系统。', 'title_zh': '基于结果导向视角探索生物学与人工智能的核心与边缘原则'}
{'arxiv_id': 'arXiv:2507.04528', 'title': 'Towards integration of Privacy Enhancing Technologies in Explainable Artificial Intelligence', 'authors': 'Sonal Allana, Rozita Dara, Xiaodong Lin, Pulei Xiong', 'link': 'https://arxiv.org/abs/2507.04528', 'abstract': 'Explainable Artificial Intelligence (XAI) is a crucial pathway in mitigating the risk of non-transparency in the decision-making process of black-box Artificial Intelligence (AI) systems. However, despite the benefits, XAI methods are found to leak the privacy of individuals whose data is used in training or querying the models. Researchers have demonstrated privacy attacks that exploit explanations to infer sensitive personal information of individuals. Currently there is a lack of defenses against known privacy attacks targeting explanations when vulnerable XAI are used in production and machine learning as a service system. To address this gap, in this article, we explore Privacy Enhancing Technologies (PETs) as a defense mechanism against attribute inference on explanations provided by feature-based XAI methods. We empirically evaluate 3 types of PETs, namely synthetic training data, differentially private training and noise addition, on two categories of feature-based XAI. Our evaluation determines different responses from the mitigation methods and side-effects of PETs on other system properties such as utility and performance. In the best case, PETs integration in explanations reduced the risk of the attack by 49.47%, while maintaining model utility and explanation quality. Through our evaluation, we identify strategies for using PETs in XAI for maximizing benefits and minimizing the success of this privacy attack on sensitive personal information.', 'abstract_zh': '可解释的人工智能（XAI）是减轻黑盒人工智能（AI）系统决策过程不透明性风险的关键途径。然而，尽管XAI方法带来了好处，这些方法也被发现泄露了用于训练或查询模型的个体隐私信息。研究人员已证明利用解释来进行隐私攻击，以推断个体的敏感个人信息。目前，尚未针对使用可能存在漏洞的XAI方法进行隐私攻击的防御措施，特别是在生产环境和机器学习即服务系统中。为解决这一问题，本文探讨了隐私增强技术（PETs）作为特征基于XAI方法提供的解释进行属性推断的一种防御机制。我们通过实证评估了三种类型的PETs，即合成训练数据、差分隐私训练和噪声添加，对两类特征基于XAI方法进行评估。评估结果确定了不同缓解方法和PETs对其他系统属性（如效用和性能）的副作用。在最佳情况下，PETs整合到解释中可将攻击风险降低49.47%，同时保持模型效用和解释质量。通过我们的评估，我们确定了在XAI中使用PETs来最大化利益并最小化此类隐私攻击成功的策略。', 'title_zh': '向Privacy Enhancing Technologies与Explainable Artificial Intelligence的集成方向探索'}
{'arxiv_id': 'arXiv:2507.04513', 'title': 'Churn-Aware Recommendation Planning under Aggregated Preference Feedback', 'authors': 'Gur Keinan, Omer Ben-Porat', 'link': 'https://arxiv.org/abs/2507.04513', 'abstract': 'We study a sequential decision-making problem motivated by recent regulatory and technological shifts that limit access to individual user data in recommender systems (RSs), leaving only population-level preference information. This privacy-aware setting poses fundamental challenges in planning under uncertainty: Effective personalization requires exploration to infer user preferences, yet unsatisfactory recommendations risk immediate user churn. To address this, we introduce the Rec-APC model, in which an anonymous user is drawn from a known prior over latent user types (e.g., personas or clusters), and the decision-maker sequentially selects items to recommend. Feedback is binary -- positive responses refine the posterior via Bayesian updates, while negative responses result in the termination of the session.\nWe prove that optimal policies converge to pure exploitation in finite time and propose a branch-and-bound algorithm to efficiently compute them. Experiments on synthetic and MovieLens data confirm rapid convergence and demonstrate that our method outperforms the POMDP solver SARSOP, particularly when the number of user types is large or comparable to the number of content categories. Our results highlight the applicability of this approach and inspire new ways to improve decision-making under the constraints imposed by aggregated preference data.', 'abstract_zh': '我们研究一个由近期监管和技术变化引起的序贯决策问题，这些变化限制了推荐系统（RSs）中个体用户数据的访问，仅留下群体级别的偏好信息。这种隐私意识设定在不确定性的计划中提出了根本性的挑战：有效的个性化需要探索以推断用户偏好，但不满意的推荐可能会导致用户的即时流失。为了解决这一问题，我们引入了Rec-APC模型，在该模型中，匿名用户来自已知的潜在用户类型先验分布（例如，角色或聚类），决策者随后顺序选择推荐项目。反馈为二元的——正面响应通过贝叶斯更新精炼后验概率，而负面响应导致会话终止。我们证明了最优策略在有限时间内收敛到纯粹的利用，并提出了一种分支定界算法来高效地计算这些策略。实验结果表明，我们的方法在合成数据和MovieLens数据上表现出快速收敛，并且在潜在用户类型数量大或与内容类别数量相当时，优于POMDP求解器SARSOP。我们的结果强调了该方法的应用性和在受限于聚合偏好数据的情况下改进决策的新途径。', 'title_zh': '基于聚合偏好反馈的 churn 意识推荐规划'}
{'arxiv_id': 'arXiv:2507.04439', 'title': 'A Linguistic Analysis of Spontaneous Thoughts: Investigating Experiences of Déjà Vu, Unexpected Thoughts, and Involuntary Autobiographical Memories', 'authors': 'Videep Venkatesha, Mary Cati Poulos, Christopher Steadman, Caitlin Mills, Anne M. Cleary, Nathaniel Blanchard', 'link': 'https://arxiv.org/abs/2507.04439', 'abstract': 'The onset of spontaneous thoughts are reflective of dynamic interactions between cognition, emotion, and attention. Typically, these experiences are studied through subjective appraisals that focus on their triggers, phenomenology, and emotional salience. In this work, we use linguistic signatures to investigate Deja Vu, Involuntary Autobiographical Memories and Unexpected Thoughts. Specifically, we analyze the inherent characteristics of the linguistic patterns in participant generated descriptions of these thought types. We show how, by positioning language as a window into spontaneous cognition, existing theories on these attentional states can be updated and reaffirmed. Our findings align with prior research, reinforcing that Deja Vu is a metacognitive experience characterized by abstract and spatial language, Involuntary Autobiographical Memories are rich in personal and emotionally significant detail, and Unexpected Thoughts are marked by unpredictability and cognitive disruption. This work is demonstrative of languages potential to reveal deeper insights into how internal spontaneous cognitive states manifest through expression.', 'abstract_zh': '自发思维的 onset 反映了认知、情感和注意力之间的动态交互。通常，这些体验通过关注触发因素、现象学和情绪显著性来主观评估。在本文中，我们使用语言特征来探究 Déjà Vu、不自愿的自传体记忆和意外思维。具体而言，我们分析了参与者对这些思维类型生成描述的固有语言模式特征。我们通过将语言定位为了解自发认知的窗户，展示了如何更新和完善现有注意力状态的理论。我们的发现与先前的研究一致，证实了 Déjà Vu 是一种元认知体验，特征为抽象和空间语言，不自愿的自传体记忆富含个人和情感意义的细节，意外思维则表现出不可预测性和认知中断的特点。这项工作展示了语言揭示内部自发认知状态表达方式中更深层次见解的潜力。', 'title_zh': '自发思维的语言分析：探讨 déjà vu、意外思绪及不自主自传记忆的经验'}
{'arxiv_id': 'arXiv:2507.04428', 'title': 'ARMR: Adaptively Responsive Network for Medication Recommendation', 'authors': 'Feiyue Wu, Tianxing Wu, Shenqi Jing', 'link': 'https://arxiv.org/abs/2507.04428', 'abstract': "Medication recommendation is a crucial task in healthcare, especially for patients with complex medical conditions. However, existing methods often struggle to effectively balance the reuse of historical medications with the introduction of new drugs in response to the changing patient conditions. In order to address this challenge, we propose an Adaptively Responsive network for Medication Recommendation (ARMR), a new method which incorporates 1) a piecewise temporal learning component that distinguishes between recent and distant patient history, enabling more nuanced temporal understanding, and 2) an adaptively responsive mechanism that dynamically adjusts attention to new and existing drugs based on the patient's current health state and medication history. Experiments on the MIMIC-III and MIMIC-IV datasets indicate that ARMR has better performance compared with the state-of-the-art baselines in different evaluation metrics, which contributes to more personalized and accurate medication recommendations. The source code is publicly avaiable at: this https URL.", 'abstract_zh': '适应性响应网络在医疗药物推荐中的应用：一种新的方法', 'title_zh': 'ARMR：自适应响应网络在药物推荐中的应用'}
{'arxiv_id': 'arXiv:2507.04381', 'title': 'DC-Mamber: A Dual Channel Prediction Model based on Mamba and Linear Transformer for Multivariate Time Series Forecasting', 'authors': 'Bing Fan, Shusen Ma, Yun-Bo Zhao, Yu Kang', 'link': 'https://arxiv.org/abs/2507.04381', 'abstract': "In multivariate time series forecasting (MTSF), existing strategies for processing sequences are typically categorized as channel-independent and channel-mixing. The former treats all temporal information of each variable as a token, focusing on capturing local temporal features of individual variables, while the latter constructs a token from the multivariate information at each time step, emphasizing the modeling of global temporal dependencies. Current mainstream models are mostly based on Transformer and the emerging Mamba. Transformers excel at modeling global dependencies through self-attention mechanisms but exhibit limited sensitivity to local temporal patterns and suffer from quadratic computational complexity, restricting their efficiency in long-sequence processing. In contrast, Mamba, based on state space models (SSMs), achieves linear complexity and efficient long-range modeling but struggles to aggregate global contextual information in parallel. To overcome the limitations of both models, we propose DC-Mamber, a dual-channel forecasting model based on Mamba and linear Transformer for time series forecasting. Specifically, the Mamba-based channel employs a channel-independent strategy to extract intra-variable features, while the Transformer-based channel adopts a channel-mixing strategy to model cross-timestep global dependencies. DC-Mamber first maps the raw input into two distinct feature representations via separate embedding layers. These representations are then processed by a variable encoder (built on Mamba) and a temporal encoder (built on linear Transformer), respectively. Finally, a fusion layer integrates the dual-channel features for prediction. Extensive experiments on eight public datasets confirm DC-Mamber's superior accuracy over existing models.", 'abstract_zh': '基于Mamba和线性Transformer的双通道时间序列预测模型DC-Mamber', 'title_zh': 'DC-Mamber：基于Mamba和线性变压器的双通道预测模型多变量时间序列预测'}
{'arxiv_id': 'arXiv:2507.04376', 'title': 'MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Agents', 'authors': 'Georgios Ioannides, Christos Constantinou, Vinija Jain, Aman Chadha, Aaron Elkins', 'link': 'https://arxiv.org/abs/2507.04376', 'abstract': "As Artificial Intelligence systems evolve from monolithic models to ecosystems of specialized agents, the need for standardized communication protocols becomes increasingly critical. This paper introduces MOD-X (Modular Open Decentralized eXchange), a novel architectural framework proposal for agent interoperability that addresses key limitations of existing protocols. Unlike current approaches, MOD-X proposes a layered architecture with a Universal Message Bus, thorough state management, translation capabilities, and blockchain-based security mechanisms. We present MOD-X's architecture, compare it with existing protocols, and demonstrate its application through a worked example how it enables integration between heterogeneous specialist agents (agents with different architectures, vendors, capabilities, and knowledge representations--including rule-based systems, neural networks, symbolic reasoning engines, and legacy software with agent wrappers). MOD-X's key innovations include a publish-subscribe communication model, semantic capability discovery, and dynamic workflow orchestration--providing a framework that bridges theoretical formalism with practical implementation. This architecture addresses the growing need for truly decentralized, interoperable agent ecosystems that can scale effectively without the need for central coordination.", 'abstract_zh': '随着人工智能系统从单一模型进化为专门代理的生态系统，标准化通信协议的需求变得越来越关键。本文介绍了MOD-X（模块化开放去中心化交换），这是一种针对现有协议关键局限性的新型架构框架提议，旨在实现代理互操作性。MOD-X提出了一种分层架构，包括通用消息总线、全面的状态管理、翻译能力以及基于区块链的安全机制。我们介绍了MOD-X的架构，将其与现有协议进行比较，并通过一个工作示例展示了它如何使不同架构、供应商、能力和知识表示的异质专业代理（包括基于规则的系统、神经网络、符号推理引擎以及带有代理封装的遗留软件）之间的集成成为可能。MOD-X的关键创新包括发布/订阅通信模型、语义能力发现以及动态工作流编排，提供了一种将理论形式化与实际实施相结合的框架。该架构解决了真正去中心化、互操作性强的代理生态系统需要有效扩展而无需中央协调的日益增长的需求。', 'title_zh': 'MOD-X: 一种模块化开放去中心化异构可互操作人工代理交易平台框架提案'}
{'arxiv_id': 'arXiv:2507.04338', 'title': 'Voltage Mode Winner-Take-All Circuit for Neuromorphic Systems', 'authors': 'Abdullah M. Zyarah, Dhireesha Kudithipudi', 'link': 'https://arxiv.org/abs/2507.04338', 'abstract': 'Recent advances in neuromorphic computing demonstrate on-device learning capabilities with low power consumption. One of the key learning units in these systems is the winner-take-all circuit. In this research, we propose a winner-take-all circuit that can be configured to achieve k-winner and hysteresis properties, simulated in IBM 65 nm node. The circuit dissipated 34.9 $\\mu$W of power with a latency of 10.4 ns, while processing 1000 inputs. The utility of the circuit is demonstrated for spatial filtering and classification.', 'abstract_zh': '近期神经形态计算的进展展示了低功耗下的在器件学习能力，这些系统中的关键学习单元是赢家通吃电路。本研究提出了一种可配置以实现k-赢家和滞回特性的赢家通吃电路，并在IBM 65 nm节点上进行了模拟。该电路在处理1000个输入时消耗了34.9 μW的功率，延时为10.4 ns。电路的应用展示了其在空间滤波和分类中的实用性。', 'title_zh': '基于电压模式的Winner-Take-All电路在类脑系统中的应用'}
{'arxiv_id': 'arXiv:2507.04299', 'title': 'Answer Set Programming Modulo Theories and Reasoning about Continuous Changes', 'authors': 'Joohyung Lee, Yunsong Meng', 'link': 'https://arxiv.org/abs/2507.04299', 'abstract': "Answer Set Programming Modulo Theories (ASPMT) is a new framework of tight integration of answer set programming (ASP) and satisfiability modulo theories (SMT). Similar to the relationship between first-order logic and SMT, it is based on a recent proposal of the functional stable model semantics by fixing interpretations of background theories. Analogously to a known relationship between ASP and SAT, ``tight'' ASPMT programs can be translated into SMT instances. We demonstrate the usefulness of ASPMT by enhancing action language C+ to handle continuous changes as well as discrete changes. We reformulate the semantics of C+ in terms ofASPMT, and show that SMT solvers can be used to compute the language. We also show how the language can represent cumulative effects on continuous resources.", 'abstract_zh': '基于理论的回答集编程（ASPMT）是一种将回答集编程（ASP）和冲突模理论（SMT）紧密集成的新框架。通过固定背景理论的解释，它基于近期提出的函子稳定模型语义。类似于一阶逻辑与SMT之间的关系，“紧致”的ASPMT程序可以翻译成SMT实例。我们通过增强C+动作语言以处理连续变化和离散变化，展示了ASPMT的应用价值。我们将C+的语言语义重新表述为ASPMT，并展示了如何使用SMT求解器来计算该语言。我们还展示了该语言如何表示连续资源上的累积效果。', 'title_zh': 'Answer Set Programming Modulo Theories and Continuous Change Reasoning'}
{'arxiv_id': 'arXiv:2507.04283', 'title': 'Clustering via Self-Supervised Diffusion', 'authors': 'Roy Uziel, Irit Chelly, Oren Freifeld, Ari Pakman', 'link': 'https://arxiv.org/abs/2507.04283', 'abstract': 'Diffusion models, widely recognized for their success in generative tasks, have not yet been applied to clustering. We introduce Clustering via Diffusion (CLUDI), a self-supervised framework that combines the generative power of diffusion models with pre-trained Vision Transformer features to achieve robust and accurate clustering. CLUDI is trained via a teacher-student paradigm: the teacher uses stochastic diffusion-based sampling to produce diverse cluster assignments, which the student refines into stable predictions. This stochasticity acts as a novel data augmentation strategy, enabling CLUDI to uncover intricate structures in high-dimensional data. Extensive evaluations on challenging datasets demonstrate that CLUDI achieves state-of-the-art performance in unsupervised classification, setting new benchmarks in clustering robustness and adaptability to complex data distributions.', 'abstract_zh': '通过扩散模型进行聚类：CLUDI', 'title_zh': '自监督扩散聚类'}
{'arxiv_id': 'arXiv:2507.04067', 'title': 'HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration', 'authors': 'Yuyang Cheng, Yumiao Xu, Chaojia Yu, Yong Zhao', 'link': 'https://arxiv.org/abs/2507.04067', 'abstract': "Contemporary multi-agent systems encounter persistent challenges in cross-platform interoperability, dynamic task scheduling, and efficient resource sharing. Agents with heterogeneous implementations often lack standardized interfaces; collaboration frameworks remain brittle and hard to extend; scheduling policies are static; and inter-agent state synchronization is insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular framework comprising five layers-User, Workflow, Operator, Agent, and Resource-and supported by sixteen standardized interfaces. HAWK delivers an end-to-end pipeline covering task parsing, workflow orchestration, intelligent scheduling, resource invocation, and data synchronization. At its core lies an adaptive scheduling and optimization module in the Workflow Layer, which harnesses real-time feedback and dynamic strategy adjustment to maximize utilization. The Resource Layer provides a unified abstraction over heterogeneous data sources, large models, physical devices, and third-party services&tools, simplifying cross-domain information retrieval. We demonstrate HAWK's scalability and effectiveness via CreAgentive, a multi-agent novel-generation prototype, which achieves marked gains in throughput, lowers invocation complexity, and improves system controllability. We also show how hybrid deployments of large language models integrate seamlessly within HAWK, highlighting its flexibility. Finally, we outline future research avenues-hallucination mitigation, real-time performance tuning, and enhanced cross-domain adaptability-and survey prospective applications in healthcare, government, finance, and education.", 'abstract_zh': '当代多-agent系统在跨平台互操作性、动态任务调度和高效资源分享方面面临持续挑战。不同实现的代理缺少标准化接口；协作框架脆弱且难以扩展；调度策略静态且不够智能；代理间状态同步不足。我们提出了一种分层代理工作流（HAWK）框架，该框架包含五层-用户、工作流、操作员、代理和资源，并由十六个标准化接口支持。HAWK提供了一整套涵盖任务解析、工作流编排、智能调度、资源调用和数据同步的端到端管道。其核心是在工作流层中集成了一个自适应调度和优化模块，该模块利用实时反馈和动态策略调整来优化资源利用。资源层提供了一种统一的异构数据源、大规模模型、物理设备和第三方服务及工具的抽象，简化了跨域信息检索。通过CreAgentive多-agent新型生成原型，我们展示了HAWK的可扩展性和有效性，实现了吞吐量显著提升、调用复杂度降低和系统可控性的提高。我们还展示了大型语言模型在HAWK中的无缝集成，突显了其灵活性。最后，我们提出了未来研究方向——幻觉抑制、实时性能调优和增强的跨域适应性，并概述了其在医疗、政府、金融和教育等领域的潜在应用。', 'title_zh': 'HAWK：多智能体协作的层次化工作流框架'}
{'arxiv_id': 'arXiv:2507.03929', 'title': 'An ASP-Based Framework for MUSes', 'authors': 'Mohimenul Kabir, Kuldeep S Meel', 'link': 'https://arxiv.org/abs/2507.03929', 'abstract': 'Given an unsatisfiable formula, understanding the core reason for unsatisfiability is crucial in several applications. One effective way to capture this is through the minimal unsatisfiable subset (MUS), the subset-minimal set of clauses that remains unsatisfiable. Current research broadly focuses on two directions: (i) enumerating as many MUSes as possible within a given time limit, and (ii) counting the total number of MUSes for a given unsatisfiable formula.\nIn this paper, we introduce an answer set programming-based framework, named MUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for its strengths in knowledge representation and is particularly suitable for specifying complex combinatorial problems. By translating MUS enumeration into answer set solving, MUS-ASP leverages the computational efficiency of state-of-the-art ASP systems. Our extensive experimental evaluation demonstrates the effectiveness of MUS-ASP and highlights the acceleration in both MUS enumeration and counting tasks, particularly when integrated within hybrid solvers, including the framework proposed in this paper.', 'abstract_zh': '基于ASP的MUS在线枚举框架：MUS-ASP', 'title_zh': '基于ASP的MUSes框架'}
{'arxiv_id': 'arXiv:2507.03916', 'title': 'Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models', 'authors': 'Yifan Jiang, Yibo Xue, Yukun Kang, Pin Zheng, Jian Peng, Feiran Wu, Changliang Xu', 'link': 'https://arxiv.org/abs/2507.03916', 'abstract': 'Slide animations, such as fade-ins, fly-ins, and wipes, are critical for audience engagement, efficient information delivery, and vivid visual expression. However, most AI-driven slide-generation tools still lack native animation support, and existing vision-language models (VLMs) struggle with animation tasks due to the absence of public datasets and limited temporal-reasoning capabilities. To address this gap, we release the first public dataset for slide-animation modeling: 12,000 triplets of natural-language descriptions, animation JSON files, and rendered videos, collectively covering every built-in PowerPoint effect. Using this resource, we fine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent improvements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our Coverage-Order-Detail Assessment (CODA) metric, which evaluates action coverage, temporal order, and detail fidelity. On a manually curated test set of slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and shows significant improvements in CODA-detail. This demonstrates that low-rank adaptation enables reliable temporal reasoning and generalization beyond synthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric provide a rigorous benchmark and foundation for future research on VLM-based dynamic slide generation.', 'abstract_zh': '幻灯片动画（如渐显、飞入和擦除）对于增强观众参与度、高效信息传递和生动视觉表达至关重要。然而，大多数基于AI的幻灯片生成工具仍缺乏内置动画支持，现有的视觉-语言模型（VLM）由于缺乏公开数据集和有限的时序推理能力，在动画任务上的表现受限。为解决这一问题，我们发布了首个公开的幻灯片动画建模数据集：包含12,000组自然语言描述、动画JSON文件和渲染视频的三元组，覆盖了PowerPoint的所有内置效果。利用这一资源，我们使用低秩适应（LoRA）对Qwen-2.5-VL-7B进行微调，并在BLEU-4、ROUGE-L、SPICE和我们的Coverage-Order-Detail Assessment（CODA）指标上实现了对GPT-4.1和Gemini-2.5-Pro的一致改进，其中CODA指标评估动作覆盖、时间顺序和细节保真度。在手动整理的一组测试幻灯片上，LoRA模型的BLEU-4提高了约60%，ROUGE-L提高了30%，并在CODA细节方面显示出显著改进。这表明低秩适应能够实现可靠的时序推理，并能在合成数据之外进行泛化。总体而言，我们的数据集、LoRA增强模型和CODA指标为基于VLM的动态幻灯片生成的未来研究提供了严格的基准和基础。', 'title_zh': '动画需要关注：基于视觉-语言模型的整体幻灯片动画理解方法'}
{'arxiv_id': 'arXiv:2507.03870', 'title': 'Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing', 'authors': 'Rahil P Mehta, Yashwanthi Anand, Manish Motwani, Sandhya Saisubramanian', 'link': 'https://arxiv.org/abs/2507.03870', 'abstract': "When an autonomous agent behaves undesirably, including failure to complete a task, it can be difficult to determine whether the behavior is due to a systemic agent error, such as flaws in the model or policy, or an environment error, where a task is inherently infeasible under a given environment configuration, even for an ideal agent. As agents and their environments grow more complex, identifying the error source becomes increasingly difficult but critical for reliable deployment. We introduce AIProbe, a novel black-box testing technique that applies differential testing to attribute undesirable agent behaviors either to agent deficiencies, such as modeling or training flaws, or due to environmental infeasibility. AIProbe first generates diverse environmental configurations and tasks for testing the agent, by modifying configurable parameters using Latin Hypercube sampling. It then solves each generated task using a search-based planner, independent of the agent. By comparing the agent's performance to the planner's solution, AIProbe identifies whether failures are due to errors in the agent's model or policy, or due to unsolvable task conditions. Our evaluation across multiple domains shows that AIProbe significantly outperforms state-of-the-art techniques in detecting both total and unique errors, thereby contributing to a reliable deployment of autonomous agents.", 'abstract_zh': '当自主代理行为不当，包括未能完成任务时，确定行为是由代理系统的错误（如模型或策略中的缺陷）还是环境错误（在给定环境配置下任务本质上不可行）引起变得困难，尤其是在代理及其环境变得更加复杂的情况下，识别错误源变得越来越困难但至关重要，以确保可靠的部署。我们引入了AIProbe，这是一种新颖的黑盒测试技术，通过差异性测试来归因代理的不当行为是由于代理缺陷（如建模或训练缺陷）还是环境不可行性。AIProbe 首先通过使用拉丁超立方抽样修改可配置参数来生成多样化的环境配置和任务，用于测试代理。然后，它使用基于搜索的计划器独立解决每个生成的任务。通过将代理的表现与计划器的解决方案进行比较，AIProbe 确定失败是由于代理模型或策略中的错误还是由于无法解决的任务条件。我们在多个领域的评估表明，AIProbe 在检测总体和独特错误方面明显优于最先进的技术，从而有助于自主代理的可靠部署。', 'title_zh': '使用差异测试揭露自主系统中的系统级和环境错误'}
{'arxiv_id': 'arXiv:2507.03839', 'title': 'Participatory Evolution of Artificial Life Systems via Semantic Feedback', 'authors': 'Shuowen Li, Kexin Wang, Minglu Fang, Danqi Huang, Ali Asadipour, Haipeng Mi, Yitong Sun', 'link': 'https://arxiv.org/abs/2507.03839', 'abstract': "We present a semantic feedback framework that enables natural language to guide the evolution of artificial life systems. Integrating a prompt-to-parameter encoder, a CMA-ES optimizer, and CLIP-based evaluation, the system allows user intent to modulate both visual outcomes and underlying behavioral rules. Implemented in an interactive ecosystem simulation, the framework supports prompt refinement, multi-agent interaction, and emergent rule synthesis. User studies show improved semantic alignment over manual tuning and demonstrate the system's potential as a platform for participatory generative design and open-ended evolution.", 'abstract_zh': '一种基于语义反馈的自然语言引导人工生命系统演化框架', 'title_zh': '通过语义反馈的人工生命系统参与式演化'}
{'arxiv_id': 'arXiv:2507.03802', 'title': 'Generating Novelty in Open-World Multi-Agent Strategic Board Games', 'authors': 'Mayank Kejriwal, Shilpa Thomas', 'link': 'https://arxiv.org/abs/2507.03802', 'abstract': 'We describe GNOME (Generating Novelty in Open-world Multi-agent Environments), an experimental platform that is designed to test the effectiveness of multi-agent AI systems when faced with \\emph{novelty}. GNOME separates the development of AI gameplaying agents with the simulator, allowing \\emph{unanticipated} novelty (in essence, novelty that is not subject to model-selection bias). Using a Web GUI, GNOME was recently demonstrated at NeurIPS 2020 using the game of Monopoly to foster an open discussion on AI robustness and the nature of novelty in real-world environments. In this article, we further detail the key elements of the demonstration, and also provide an overview of the experimental design that is being currently used in the DARPA Science of Artificial Intelligence and Learning for Open-World Novelty (SAIL-ON) program to evaluate external teams developing novelty-adaptive gameplaying agents.', 'abstract_zh': '我们描述了GNOME（生成开放世界多智能体环境中的新颖性），一个实验平台，旨在测试多智能体AI系统在面对新颖性时的有效性。GNOME通过将AI游戏代理的开发与模拟器分离，实现了未预见的新颖性（本质上，不受模型选择偏差影响的新颖性）。通过网络GUI，GNOME在2020年NeurIPS会议上使用垄断游戏进行演示，旨在促进AI鲁棒性以及现实环境中新颖性的本质的公开讨论。在本文中，我们进一步详细介绍了演示的关键要素，并提供了一种实验设计的概述，该设计目前正在DARPA人工智能与学习科学为开放世界新颖性（SAIL-ON）计划中用于评估开发适应新颖性的游戏代理的外部团队。', 'title_zh': '开放世界多agent战略棋盘游戏中的新颖性生成'}
{'arxiv_id': 'arXiv:2507.03793', 'title': 'Learning Dark Souls Combat Through Pixel Input With Neuroevolution', 'authors': "Jim O'Connor, Gary B. Parker, Mustafa Bugti", 'link': 'https://arxiv.org/abs/2507.03793', 'abstract': "This paper investigates the application of Neuroevolution of Augmenting Topologies (NEAT) to automate gameplay in Dark Souls, a notoriously challenging action role-playing game characterized by complex combat mechanics, dynamic environments, and high-dimensional visual inputs. Unlike traditional reinforcement learning or game playing approaches, our method evolves neural networks directly from raw pixel data, circumventing the need for explicit game-state information. To facilitate this approach, we introduce the Dark Souls API (DSAPI), a novel Python framework leveraging real-time computer vision techniques for extracting critical game metrics, including player and enemy health states. Using NEAT, agents evolve effective combat strategies for defeating the Asylum Demon, the game's initial boss, without predefined behaviors or domain-specific heuristics. Experimental results demonstrate that evolved agents achieve up to a 35% success rate, indicating the viability of neuroevolution in addressing complex, visually intricate gameplay scenarios. This work represents an interesting application of vision-based neuroevolution, highlighting its potential use in a wide range of challenging game environments lacking direct API support or well-defined state representations.", 'abstract_zh': '本文探讨了利用增强拓扑神经进化（NEAT）自动化暗魂（Dark Souls）游戏玩法的应用，暗魂是一款以复杂的战斗机制、动态环境和高维视觉输入为特征的著名挑战性动作角色扮演游戏。与传统的强化学习或游戏玩法规则不同，我们的方法直接从原始像素数据进化神经网络，从而避免了需要显式的游戏状态信息。为实现这一方法，我们引入了暗魂API（DSAPI）这一新型Python框架，利用实时计算机视觉技术提取关键游戏指标，包括玩家和敌人的生命状态。借助NEAT，代理能够进化出有效的战斗策略来击败游戏初始 boss  asylum demon，而无需预定义的行为或特定领域的启发式方法。实验结果表明，进化出的代理能够达到高达35%的成功率，表明神经进化在解决复杂且视觉上复杂的游戏玩法场景方面的可行性。这项工作展示了基于视觉的神经进化的有趣应用，突显了其在缺乏直接API支持或明确状态表示的高度挑战性游戏环境中的潜在用途。', 'title_zh': '通过像素输入的神经进化学习《暗魂》战斗'}
{'arxiv_id': 'arXiv:2507.03697', 'title': 'Towards Unified Neurosymbolic Reasoning on Knowledge Graphs', 'authors': 'Qika Lin, Fangzhi Xu, Hao Lu, Kai He, Rui Mao, Jun Liu, Erik Cambria, Mengling Feng', 'link': 'https://arxiv.org/abs/2507.03697', 'abstract': 'Knowledge Graph (KG) reasoning has received significant attention in the fields of artificial intelligence and knowledge engineering, owing to its ability to autonomously deduce new knowledge and consequently enhance the availability and precision of downstream applications. However, current methods predominantly concentrate on a single form of neural or symbolic reasoning, failing to effectively integrate the inherent strengths of both approaches. Furthermore, the current prevalent methods primarily focus on addressing a single reasoning scenario, presenting limitations in meeting the diverse demands of real-world reasoning tasks. Unifying the neural and symbolic methods, as well as diverse reasoning scenarios in one model is challenging as there is a natural representation gap between symbolic rules and neural networks, and diverse scenarios exhibit distinct knowledge structures and specific reasoning objectives. To address these issues, we propose a unified neurosymbolic reasoning framework, namely Tunsr, for KG reasoning. Tunsr first introduces a consistent structure of reasoning graph that starts from the query entity and constantly expands subsequent nodes by iteratively searching posterior neighbors. Based on it, a forward logic message-passing mechanism is proposed to update both the propositional representations and attentions, as well as first-order logic (FOL) representations and attentions of each node. In this way, Tunsr conducts the transformation of merging multiple rules by merging possible relations at each step. Finally, the FARI algorithm is proposed to induce FOL rules by constantly performing attention calculations over the reasoning graph. Extensive experimental results on 19 datasets of four reasoning scenarios (transductive, inductive, interpolation, and extrapolation) demonstrate the effectiveness of Tunsr.', 'abstract_zh': '统一神经符号推理框架 Tunsr 用于知识图谱推理', 'title_zh': '知识图谱上的统一神经符号推理'}
{'arxiv_id': 'arXiv:2507.03579', 'title': 'A Universal Approach to Feature Representation in Dynamic Task Assignment Problems', 'authors': 'Riccardo Lo Bianco, Remco Dijkman, Wim Nuijten, Willem van Jaarsveld', 'link': 'https://arxiv.org/abs/2507.03579', 'abstract': 'Dynamic task assignment concerns the optimal assignment of resources to tasks in a business process. Recently, Deep Reinforcement Learning (DRL) has been proposed as the state of the art for solving assignment problems. DRL methods usually employ a neural network (NN) as an approximator for the policy function, which ingests the state of the process and outputs a valuation of the possible assignments. However, representing the state and the possible assignments so that they can serve as inputs and outputs for a policy NN remains an open challenge, especially when tasks or resources have features with an infinite number of possible values. To solve this problem, this paper proposes a method for representing and solving assignment problems with infinite state and action spaces. In doing so, it provides three contributions: (I) A graph-based feature representation of assignment problems, which we call assignment graph; (II) A mapping from marked Colored Petri Nets to assignment graphs; (III) An adaptation of the Proximal Policy Optimization algorithm that can learn to solve assignment problems represented through assignment graphs. To evaluate the proposed representation method, we model three archetypal assignment problems ranging from finite to infinite state and action space dimensionalities. The experiments show that the method is suitable for representing and learning close-to-optimal task assignment policies regardless of the state and action space dimensionalities.', 'abstract_zh': '动态任务分配涉及在业务流程中将资源最优分配给任务的问题。最近，深度强化学习（DRL）被认为是最先进的方法，用于解决分配问题。DRL方法通常使用神经网络（NN）作为策略函数的近似器，该近似器输入过程状态并输出可能分配的价值评估。然而，如何表示状态和可能的分配，以便它们可以作为策略NN的输入和输出仍然是一项开放的挑战，特别是在任务或资源具有无限多个可能值特征的情况下。为了解决这一问题，本文提出了一种表示和解决状态和动作空间无限的分配问题的方法。在此过程中，本文提供了三项贡献：（I）一种基于图的分配问题特征表示，称为分配图；（II）一种从标记彩色Petri网到分配图的映射；（III）一种适应性策略优化算法的改编，该算法可以从通过分配图表示的分配问题中学习。为了评估提出的表现形式方法，我们建立了三个代表性的分配问题模型，涵盖了从有限到无限状态和动作空间维度的情况。实验表明，该方法适用于表示和学习近似最优的任务分配策略，无论状态和动作空间的维度如何。', 'title_zh': '动态任务分配问题中特征表示的通用方法'}
{'arxiv_id': 'arXiv:2507.03525', 'title': 'Limits of Safe AI Deployment: Differentiating Oversight and Control', 'authors': 'David Manheim, Aidan Homewood', 'link': 'https://arxiv.org/abs/2507.03525', 'abstract': 'Oversight and control (collectively, supervision) are often invoked as key levers for ensuring that AI systems are accountable, reliable, and able to fulfill governance and management requirements. However, the concepts are frequently conflated or insufficiently distinguished in academic and policy discourse, undermining efforts to design or evaluate systems that should remain under meaningful human supervision.\nThis paper undertakes a targeted critical review of literature on supervision outside of AI, along with a brief summary of past work on the topic related to AI. We then differentiate control as being ex-ante or real-time, and operational rather than policy or governance. In contrast, oversight is either a policy and governance function, or is ex-post. We suggest that control aims to prevent failures. In contrast, oversight often focuses on detection, remediation, or incentives for future prevention; all preventative oversight strategies nonetheless necessitate control.\nBuilding on this foundation, we make three contributions. First, we propose a theoretically-informed yet policy-grounded framework that articulates the conditions under which each mechanism is possible, where they fall short, and what is required to make them meaningful in practice. Second, we outline how supervision methods should be documented and integrated into risk management, and drawing on the Microsoft Responsible AI Maturity Model, we outline a maturity model for AI supervision. Third, we explicitly highlight some boundaries of these mechanisms, including where they apply, where they fail, and where it is clear that no existing methods suffice. This foregrounds the question of whether meaningful supervision is possible in a given deployment context, and can support regulators, auditors, and practitioners in identifying both present limitations and the need for new conceptual and technical advances.', 'abstract_zh': '监督与控制（统称为监督）常被用作确保人工智能系统问责、可靠并满足治理和管理要求的关键杠杆。然而，这些概念在学术和政策讨论中往往被混淆或区分不足，削弱了设计或评估应在有意义的人类监督下运行的系统的努力。\n\n本文对AI之外的监督文献进行了有针对性的批判性回顾，并简要总结了与AI相关的过去研究成果。我们接着区分控制为前瞻性或实时控制，操作性而非政策或治理控制。相比之下，监督要么是政策和治理功能，要么为事后。我们认为控制旨在防止故障，而监督则更多地关注检测、补救或未来预防的激励；所有预防性的监督策略依然离不开控制。\n\n在此基础上，我们做出了三项贡献。首先，我们提出了一种理论导向且政策基础的框架，阐述了每种机制在何种条件下可行，它们有何不足以及在实践中如何使其有意义。其次，我们概述了监督方法应如何记录并整合到风险管理中，并借鉴Microsoft负责任AI成熟度模型，提出了AI监督的成熟度模型。第三，我们明确界定了这些机制的边界，包括它们适用的范围、失效的地方以及现有的方法无法满足的情况。这突显了在特定部署环境中是否可能实现有意义的监督的问题，并可支持监管者、审计师和从业者识别当前的局限性以及需要新的概念和技术进步。', 'title_zh': '安全人工智能部署的界限：区分监督与控制'}
{'arxiv_id': 'arXiv:2507.03409', 'title': 'Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language', 'authors': 'Christopher Summerfield, Lennart Luettgau, Magda Dubois, Hannah Rose Kirk, Kobi Hackenburg, Catherine Fist, Katarina Slama, Nicola Ding, Rebecca Anselmetti, Andrew Strait, Mario Giulianelli, Cozmin Ududec', 'link': 'https://arxiv.org/abs/2507.03409', 'abstract': 'We examine recent research that asks whether current AI systems may be developing a capacity for "scheming" (covertly and strategically pursuing misaligned goals). We compare current research practices in this field to those adopted in the 1970s to test whether non-human primates could master natural language. We argue that there are lessons to be learned from that historical research endeavour, which was characterised by an overattribution of human traits to other agents, an excessive reliance on anecdote and descriptive analysis, and a failure to articulate a strong theoretical framework for the research. We recommend that research into AI scheming actively seeks to avoid these pitfalls. We outline some concrete steps that can be taken for this research programme to advance in a productive and scientifically rigorous fashion.', 'abstract_zh': '我们研究了最近探讨当前AI系统是否可能发展出“算计”能力（即隐蔽且策略性地追求不一致目标）的相关研究。我们将当前该领域的研究实践与20世纪70年代测试非人灵长类动物是否能掌握自然语言能力的研究做法进行了比较，认为可以从那段历史研究中吸取教训，避免过度将人类特质归因于其他代理、过于依赖个人见证和描述性分析等问题，并未能为研究制定强有力的基础理论框架。我们建议，对于AI算计的研究应积极避免这些陷阱。我们提出了若干具体步骤，以便该研究计划能够以富有成效且科学严谨的方式推进。', 'title_zh': '来自 chimps 的教训：AI 的“谋略”与类人猿语言探索之旅'}
{'arxiv_id': 'arXiv:2507.03407', 'title': 'Artificial intelligence in drug discovery: A comprehensive review with a case study on hyperuricemia, gout arthritis, and hyperuricemic nephropathy', 'authors': 'Junwei Su, Cheng Xin, Ao Shang, Shan Wu, Zhenzhen Xie, Ruogu Xiong, Xiaoyu Xu, Cheng Zhang, Guang Chen, Yau-Tuen Chan, Guoyi Tang, Ning Wang, Yong Xu, Yibin Feng', 'link': 'https://arxiv.org/abs/2507.03407', 'abstract': 'This paper systematically reviews recent advances in artificial intelligence (AI), with a particular focus on machine learning (ML), across the entire drug discovery pipeline. Due to the inherent complexity, escalating costs, prolonged timelines, and high failure rates of traditional drug discovery methods, there is a critical need to comprehensively understand how AI/ML can be effectively integrated throughout the full process. Currently available literature reviews often narrowly focus on specific phases or methodologies, neglecting the dependence between key stages such as target identification, hit screening, and lead optimization. To bridge this gap, our review provides a detailed and holistic analysis of AI/ML applications across these core phases, highlighting significant methodological advances and their impacts at each stage. We further illustrate the practical impact of these techniques through an in-depth case study focused on hyperuricemia, gout arthritis, and hyperuricemic nephropathy, highlighting real-world successes in molecular target identification and therapeutic candidate discovery. Additionally, we discuss significant challenges facing AI/ML in drug discovery and outline promising future research directions. Ultimately, this review serves as an essential orientation for researchers aiming to leverage AI/ML to overcome existing bottlenecks and accelerate drug discovery.', 'abstract_zh': '本文系统回顾了人工智能（AI）在药物发现全过程中的最新进展，特别是机器学习（ML）在其中的应用。鉴于传统药物发现方法固有的复杂性、不断上升的成本、延长的时间线以及较高的失败率，全面理解AI/ML在整个过程中如何有效集成显得尤为关键。现有的综述文献往往仅限于特定阶段或方法，忽视了目标识别、苗头筛选和先导优化等关键阶段之间的相互依赖性。为填补这一空白，本综述提供了这些核心阶段中AI/ML应用的详细和全面分析，强调了每个阶段的重大方法学进步及其影响。我们还通过专注于高尿酸血症、痛风关节炎和高尿酸性肾病的深入案例研究，展示了这些技术的实际影响，突出了分子靶点识别和治疗候选药物发现的实际成功案例。此外，我们讨论了AI/ML在药物发现中面临的重要挑战，并概述了有希望的未来研究方向。最终，本综述为希望通过利用AI/ML克服现有瓶颈并加速药物发现的研究人员提供了一个重要的指导方向。', 'title_zh': '人工智能在药物发现中的应用：关于高尿酸血症、痛风关节炎和高尿酸肾病的综合回顾与案例研究'}
{'arxiv_id': 'arXiv:2507.03329', 'title': 'NDAI-NeuroMAP: A Neuroscience-Specific Embedding Model for Domain-Specific Retrieval', 'authors': 'Devendra Patel, Aaditya Jain, Jayant Verma, Divyansh Rajput, Sunil Mahala, Ketki Suresh Khapare, Jayateja Kalla', 'link': 'https://arxiv.org/abs/2507.03329', 'abstract': 'We present NDAI-NeuroMAP, the first neuroscience-domain-specific dense vector embedding model engineered for high-precision information retrieval tasks. Our methodology encompasses the curation of an extensive domain-specific training corpus comprising 500,000 carefully constructed triplets (query-positive-negative configurations), augmented with 250,000 neuroscience-specific definitional entries and 250,000 structured knowledge-graph triplets derived from authoritative neurological ontologies. We employ a sophisticated fine-tuning approach utilizing the FremyCompany/BioLORD-2023 foundation model, implementing a multi-objective optimization framework combining contrastive learning with triplet-based metric learning paradigms. Comprehensive evaluation on a held-out test dataset comprising approximately 24,000 neuroscience-specific queries demonstrates substantial performance improvements over state-of-the-art general-purpose and biomedical embedding models. These empirical findings underscore the critical importance of domain-specific embedding architectures for neuroscience-oriented RAG systems and related clinical natural language processing applications.', 'abstract_zh': 'NDAI-NeuroMAP：首个工程化用于高精度信息检索任务的神经科学领域特定密集向量嵌入模型', 'title_zh': 'NDAI-NeuroMAP：一种神经科学专用的嵌入模型用于领域特定检索'}
{'arxiv_id': 'arXiv:2507.03267', 'title': 'GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning', 'authors': 'Jie Peng, Jiarui Ji, Runlin Lei, Zhewei Wei, Yongchao Liu, Chuntao Hong', 'link': 'https://arxiv.org/abs/2507.03267', 'abstract': 'Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate structural, temporal, and textual attributes, are crucial for modeling complex real-world systems. However, most of the existing DyTAG datasets exhibit poor textual quality, which severely limits their utility for DyTAG generation tasks requiring semantically rich inputs. Additionally, prior work mainly focuses on discriminative tasks on DyTAGs, resulting in a lack of standardized task formulations and evaluation protocols tailored for DyTAG generation. To address these critical issues, we propose Generative DyTAG Benchmark (GDGB), which comprises eight meticulously curated DyTAG datasets with high-quality textual features for both nodes and edges, overcoming limitations of prior datasets. Building on GDGB, we define two novel DyTAG generation tasks: Transductive Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG). TDGG transductively generates a target DyTAG based on the given source and destination node sets, while the more challenging IDGG introduces new node generation to inductively model the dynamic expansion of real-world graph data. To enable holistic evaluation, we design multifaceted metrics that assess the structural, temporal, and textual quality of the generated DyTAGs. We further propose GAG-General, an LLM-based multi-agent generative framework tailored for reproducible and robust benchmarking of DyTAG generation. Experimental results demonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key insights revealing the critical interplay of structural and textual features in DyTAG generation. These findings establish GDGB as a foundational resource for advancing generative DyTAG research and unlocking further practical applications in DyTAG generation. GDGB datasets, source codes, and leaderboards are available at \\href{this https URL}{here}.', 'abstract_zh': '动态文本属性图（DyTAGs）：结构、时间和文本属性的精细整合对于 modeling 复杂现实系统至关重要。然而，现有的大多数 DyTAG 数据集在文本质量方面表现不佳，严重限制了其在需要丰富语义输入的 DyTAG 生成任务中的应用。此外，前人的工作主要集中在 DyTAG 的区分性任务上，导致缺乏针对 DyTAG 生成的标准任务形式和评估协议。为应对这些关键问题，我们提出生成性动态文本属性图基准（GDGB），包含八个精心策划的 DyTAG 数据集，具有高质量的节点和边的文本特征，克服了先前数据集的限制。基于 GDGB，我们定义了两个新的 DyTAG 生成任务：归纳动态图生成（IDGG）和传递动态图生成（TDGG）。TDGG 通过给定的源节点和目标节点集，生成目标 DyTAG，而更具挑战性的 IDGG 引入了新的节点生成，以归纳地建模现实世界图数据的动态扩展。为了实现全面评估，我们设计了多方面的评估指标，以评估生成的 DyTAG 的结构、时间和文本质量。我们进一步提出了 GAG-General，这是一种基于大语言模型的多智能体生成框架，专门用于 DyTAG 生成基准测试的可重复性和鲁棒性。实验结果表明，GDGB 能够严谨地评估 TDGG 和 IDGG，并揭示了 DyTAG 生成中结构和文本特征的密切互动。这些发现确立了 GDGB 作为推进生成性 DyTAG 研究和解锁进一步实际应用场景的基础资源。GDGB 数据集、源代码和排行榜可在 \\href{this https URL}{这里} 获取。', 'title_zh': 'GDGB: 生成动态文本图学习的基准数据集'}
{'arxiv_id': 'arXiv:2507.03226', 'title': 'Efficient Knowledge Graph Construction and Retrieval from Unstructured Text for Large-Scale RAG Systems', 'authors': 'Congmin Min, Rhea Mathew, Joyce Pan, Sahil Bansal, Abbas Keshavarzi, Amar Viswanathan Kannan', 'link': 'https://arxiv.org/abs/2507.03226', 'abstract': 'We propose a scalable and cost-efficient framework for deploying Graph-based Retrieval Augmented Generation (GraphRAG) in enterprise environments. While GraphRAG has shown promise for multi-hop reasoning and structured retrieval, its adoption has been limited by the high computational cost of constructing knowledge graphs using large language models (LLMs) and the latency of graph-based retrieval. To address these challenges, we introduce two core innovations: (1) a dependency-based knowledge graph construction pipeline that leverages industrial-grade NLP libraries to extract entities and relations from unstructured text completely eliminating reliance on LLMs; and (2) a lightweight graph retrieval strategy that combines hybrid query node identification with efficient one-hop traversal for high-recall, low-latency subgraph extraction. We evaluate our framework on two SAP datasets focused on legacy code migration and demonstrate strong empirical performance. Our system achieves up to 15% and 4.35% improvements over traditional RAG baselines based on LLM-as-Judge and RAGAS metrics, respectively. Moreover, our dependency-based construction approach attains 94% of the performance of LLM-generated knowledge graphs (61.87% vs. 65.83%) while significantly reducing cost and improving scalability. These results validate the feasibility of deploying GraphRAG systems in real-world, large-scale enterprise applications without incurring prohibitive resource requirements paving the way for practical, explainable, and domain-adaptable retrieval-augmented reasoning.', 'abstract_zh': '一种可扩展且成本效益高的框架：在企业环境中部署基于图的检索增强生成（GraphRAG）', 'title_zh': '从未结构化文本中高效构建和检索知识图谱以支持大规模RAG系统'}
{'arxiv_id': 'arXiv:2507.03190', 'title': 'Discovering Algorithms with Computational Language Processing', 'authors': 'Theo Bourdais, Abeynaya Gnanasekaran, Houman Owhadi, Tuhin Sahai', 'link': 'https://arxiv.org/abs/2507.03190', 'abstract': "Algorithms are the engine for reproducible problem-solving. We present a framework automating algorithm discovery by conceptualizing them as sequences of operations, represented as tokens. These computational tokens are chained using a grammar, enabling the formation of increasingly sophisticated procedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement learning (RL) explores token chaining and drives the creation of new tokens. This methodology rediscovers, improves, and generates new algorithms that substantially outperform existing methods for strongly NP-hard combinatorial optimization problems and foundational quantum computing approaches such as Grover's and Quantum Approximate Optimization Algorithm. Operating at the computational rather than code-generation level, our framework produces algorithms that can be tailored specifically to problem instances, not merely classes.", 'abstract_zh': '算法是可重复问题求解的引擎。我们提出了一种框架，通过将算法概念化为操作序列并用标记表示来自动化算法发现，这些计算标记通过语法链成，使程序越来越复杂。我们的集成蒙特卡洛树搜索（MCTS），由强化学习（RL）引导，探索标记链成并驱动新标记的生成。该方法重新发现、改进和生成了在强NP难组合优化问题和基础量子计算方法（如Grover算法和量子近似优化算法）方面显著优于现有方法的新算法。在计算层面而非代码生成层面操作，该框架产生的算法可以针对具体的问题实例进行定制，而不仅仅是类别。', 'title_zh': '基于计算语言处理发现算法'}
{'arxiv_id': 'arXiv:2507.05221', 'title': 'CTA: Cross-Task Alignment for Better Test Time Training', 'authors': 'Samuel Barbeau, Pedram Fekri, David Osowiechi, Ali Bahri, Moslem YazdanpanahMasih Aminbeidokhti, Christian Desrosiers', 'link': 'https://arxiv.org/abs/2507.05221', 'abstract': 'Deep learning models have demonstrated exceptional performance across a wide range of computer vision tasks. However, their performance often degrades significantly when faced with distribution shifts, such as domain or dataset changes. Test-Time Training (TTT) has emerged as an effective method to enhance model robustness by incorporating an auxiliary unsupervised task during training and leveraging it for model updates at test time. In this work, we introduce CTA (Cross-Task Alignment), a novel approach for improving TTT. Unlike existing TTT methods, CTA does not require a specialized model architecture and instead takes inspiration from the success of multi-modal contrastive learning to align a supervised encoder with a self-supervised one. This process enforces alignment between the learned representations of both models, thereby mitigating the risk of gradient interference, preserving the intrinsic robustness of self-supervised learning and enabling more semantically meaningful updates at test-time. Experimental results demonstrate substantial improvements in robustness and generalization over the state-of-the-art on several benchmark datasets.', 'abstract_zh': '深度学习模型在广泛计算机视觉任务中展现了出色的性能。然而，它们在面对分布变化，如领域或数据集变化时，性能往往会显著下降。测试时训练（TTT）作为一种有效方法，通过在训练过程中引入辅助无监督任务，并在测试时利用该任务对模型进行更新，提升了模型的鲁棒性。本文提出了一种名为CTA（Cross-Task Alignment）的新方法，以进一步改进TTT。与现有的TTT方法不同，CTA 不需要特定的模型结构，而是从多模态对比学习的成功中汲取灵感，将监督编码器与自监督编码器对齐。这一过程确保两种模型学习表示之间的对齐，从而减轻梯度干扰的风险，保留自监督学习的内在鲁棒性，并在测试时实现更具语义意义的更新。实验结果在多个基准数据集上展示了在鲁棒性和泛化能力上的显著改进。', 'title_zh': 'CTA：跨任务对齐以提高测试时训练效果'}
{'arxiv_id': 'arXiv:2507.05187', 'title': 'Infrastructuring Contestability: A Framework for Community-Defined AI Value Pluralism', 'authors': 'Andreas Mayer', 'link': 'https://arxiv.org/abs/2507.05187', 'abstract': "The proliferation of AI-driven systems presents a fundamental challenge to Human-Computer Interaction (HCI) and Computer-Supported Cooperative Work (CSCW), often diminishing user agency and failing to account for value pluralism. Current approaches to value alignment, which rely on centralized, top-down definitions, lack the mechanisms for meaningful contestability. This leaves users and communities unable to challenge or shape the values embedded in the systems that govern their digital lives, creating a crisis of legitimacy and trust. This paper introduces Community-Defined AI Value Pluralism (CDAVP), a socio-technical framework that addresses this gap. It reframes the design problem from achieving a single aligned state to infrastructuring a dynamic ecosystem for value deliberation and application. At its core, CDAVP enables diverse, self-organizing communities to define and maintain explicit value profiles - rich, machine-readable representations that can encompass not only preferences but also community-specific rights and duties. These profiles are then contextually activated by the end-user, who retains ultimate control (agency) over which values guide the AI's behavior. AI applications, in turn, are designed to transparently interpret these profiles and moderate conflicts, adhering to a set of non-negotiable, democratically-legitimated meta-rules. The designer's role shifts from crafting static interfaces to becoming an architect of participatory ecosystems. We argue that infrastructuring for pluralism is a necessary pathway toward achieving robust algorithmic accountability and genuinely contestable, human-centric AI.", 'abstract_zh': '基于社区定义的AI价值多元主义：实现 robust算法问责制与以人为本的人工智能', 'title_zh': '社区定义的人工智能多元价值基础设施 Contestability：一种框架'}
{'arxiv_id': 'arXiv:2507.05178', 'title': 'CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale', 'authors': 'Jonathan Hyun, Nicholas R Waytowich, Boyuan Chen', 'link': 'https://arxiv.org/abs/2507.05178', 'abstract': 'Despite rapid progress in large language model (LLM)-based multi-agent systems, current benchmarks fall short in evaluating their scalability, robustness, and coordination capabilities in complex, dynamic, real-world tasks. Existing environments typically focus on small-scale, fully observable, or low-complexity domains, limiting their utility for developing and assessing next-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire, an open-source benchmark designed to close this gap. Built atop the human-AI teaming CREW simulation platform, CREW-Wildfire offers procedurally generated wildfire response scenarios featuring large maps, heterogeneous agents, partial observability, stochastic dynamics, and long-horizon planning objectives. The environment supports both low-level control and high-level natural language interactions through modular Perception and Execution modules. We implement and evaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks, uncovering significant performance gaps that highlight the unsolved challenges in large-scale coordination, communication, spatial reasoning, and long-horizon planning under uncertainty. By providing more realistic complexity, scalable architecture, and behavioral evaluation metrics, CREW-Wildfire establishes a critical foundation for advancing research in scalable multi-agent Agentic intelligence. All code, environments, data, and baselines will be released to support future research in this emerging domain.', 'abstract_zh': '尽管基于大规模语言模型（LLM）的多代理系统取得了快速进展，当前的基准测试在评估其在复杂、动态的真实世界任务中的扩展性、鲁棒性和协调能力方面仍显不足。现有环境通常侧重于小规模、完全可观测或低复杂度领域，限制了其在开发和评估下一代多代理类人工智能框架方面的效用。我们引入了CREW-Wildfire，一个开源基准测试，旨在弥补这一缺口。基于人类-人工智能团队协作的CREW模拟平台，CREW-Wildfire提供了基于程序生成的wildfire响应场景，包括大规模地图、异质代理、部分可观测性、随机动力学和长期规划目标。该环境通过模块化的感知和执行模块支持低级控制和高级自然语言交互。我们实现了并评估了多个最先进的基于LLM的多代理类人工智能框架，揭示了显著的性能差距，突显了大规模协调、通信、空间推理和不确定性下的长期规划方面的未解挑战。通过提供更符合实际复杂性的架构、可扩展性以及行为评估指标，CREW-Wildfire为推进可扩展多代理类人工智能研究奠定了关键基础。所有代码、环境、数据和基线将向未来的研究开放。', 'title_zh': 'CREW-WILDFIRE：大规模评估有能动性的多智能体合作'}
{'arxiv_id': 'arXiv:2507.05150', 'title': 'Effects of Unplanned Incoming Flights on Airport Relief Processes after a Major Natural Disaster', 'authors': 'Luka Van de Sype, Matthieu Vert, Alexei Sharpanskykh, Seyed Sahand Mohammadi Ziabari', 'link': 'https://arxiv.org/abs/2507.05150', 'abstract': "The severity of natural disasters is increasing every year, impacting many people's lives. During the response phase of disasters, airports are important hubs where relief aid arrives and people need to be evacuated. However, the airport often forms a bottleneck in these relief operations due to the sudden need for increased capacity. Limited research has been done on the operational side of airport disaster management. Experts identify the main problems as, first, the asymmetry of information between the airport and incoming flights, and second, the lack of resources. The goal of this research is to understand the effects of incomplete knowledge of incoming flights with different resource allocation strategies on the performance of cargo handling operations at an airport after a natural disaster. An agent-based model is created, implementing realistic offloading strategies with different degrees of information uncertainty. Model calibration and verification are performed with experts in the field. The model performance is measured by the average turnaround time, which is divided into offloading time, boarding time, and cumulative waiting times. The results show that the effects of one unplanned aircraft are negligible. However, all waiting times increase with more arriving unplanned aircraft.", 'abstract_zh': '自然灾害的严重性逐年增加，影响着许多人的生活。在灾害应对阶段，机场是救济物资到达和人员疏散的重要枢纽。然而，由于容量突然增加的需求，机场往往会成为这些救济行动中的瓶颈。关于机场灾害管理的操作方面，有限的研究已经被进行。专家认为主要问题有两点：一是机场与抵达航班之间信息的不对称性，二是资源的缺乏。本研究旨在了解不同资源配置策略对机场灾害后货物处理操作性能的影响，特别是缺乏抵达航班信息的知识不完整性的影响。创建了一个基于代理的模型，实施具有不同程度信息不确定性的真实卸载策略，并与该领域的专家进行模型校准和验证。模型性能通过平均周转时间衡量，分为卸货时间、装机时间以及累计等待时间。研究结果显示，一次未计划航班的影响可以忽略不计，但随着更多未计划航班的到达，所有等待时间都会增加。', 'title_zh': '重大自然灾害后机场应对非计划降落航班的影响研究'}
{'arxiv_id': 'arXiv:2507.05149', 'title': 'OGF: An Online Gradient Flow Method for Optimizing the Statistical Steady-State Time Averages of Unsteady Turbulent Flows', 'authors': 'Tom Hickling, Jonathan F. MacArt, Justin Sirignano, Den Waidmann', 'link': 'https://arxiv.org/abs/2507.05149', 'abstract': 'Turbulent flows are chaotic and unsteady, but their statistical distribution converges to a statistical steady state. Engineering quantities of interest typically take the form of time-average statistics such as $ \\frac{1}{t} \\int_0^t f ( u(x,\\tau; \\theta) ) d\\tau \\overset{t \\rightarrow \\infty}{\\rightarrow} F(x; \\theta)$, where $u(x,t; \\theta)$ are solutions of the Navier--Stokes equations with parameters $\\theta$. Optimizing over $F(x; \\theta)$ has many engineering applications including geometric optimization, flow control, and closure modeling. However, this remains an open challenge, as existing computational approaches are incapable of scaling to physically representative numbers of grid points. The fundamental obstacle is the chaoticity of turbulent flows: gradients calculated with the adjoint method diverge exponentially as $t \\rightarrow \\infty$.\nWe develop a new online gradient-flow (OGF) method that is scalable to large degree-of-freedom systems and enables optimizing for the steady-state statistics of chaotic, unsteady, turbulence-resolving simulations. The method forward-propagates an online estimate for the gradient of $F(x; \\theta)$ while simultaneously performing online updates of the parameters $\\theta$. A key feature is the fully online nature of the algorithm to facilitate faster optimization progress and its combination with a finite-difference estimator to avoid the divergence of gradients due to chaoticity. The proposed OGF method is demonstrated for optimizations over three chaotic ordinary and partial differential equations: the Lorenz-63 equation, the Kuramoto--Sivashinsky equation, and Navier--Stokes solutions of compressible, forced, homogeneous isotropic turbulence. In each case, the OGF method successfully reduces the loss based on $F(x; \\theta)$ by several orders of magnitude and accurately recovers the optimal parameters.', 'abstract_zh': '湍流流动是混沌且不稳定的，但其统计分布趋于统计稳态。感兴趣的工程量通常表现为时间平均统计量，如$\\frac{1}{t} \\int_0^t f ( u(x,\\tau; \\theta) ) d\\tau \\overset{t \\rightarrow \\infty}{\\rightarrow} F(x; \\theta)$，其中$u(x,t; \\theta)$是具有参数$\\theta$的纳维-斯托克斯方程的解。对$F(x; \\theta)$进行优化具有许多工程应用，包括几何优化、流控制和闭合模型。然而，这仍然是一个待解的问题，因为现有的计算方法无法扩展到物理上代表性的网格点数量。基本障碍是湍流流动的混沌性：使用伴随方法计算的梯度随着$t \\rightarrow \\infty$呈指数发散。\n\n我们开发了一种新的在线梯度流（OGF）方法，该方法可扩展到大自由度系统，并能够优化混沌、不稳定的湍流分辨率模拟的稳态统计量。该方法向前传播$F(x; \\theta)$的在线梯度估计值，同时进行参数$\\theta$的在线更新。一个关键特征是该算法的完全在线性质，以促进更快的优化进度，并与差分估计算法结合使用，避免因混沌性而导致梯度发散。所提出的OGF方法在三个混沌常微分方程和偏微分方程优化中得到了演示：洛伦兹-63方程、库拉모托-西瓦什金斯基方程及可压缩、强迫、各向同性的纳维-斯托克斯方程解。在每种情况下，OGF方法成功地将基于$F(x; \\theta)$的损失量减少了几个数量级，并准确地恢复了最优参数。', 'title_zh': 'OGF：优化不定湍流流统计稳态时间平均值的在线梯度流方法'}
{'arxiv_id': 'arXiv:2507.05137', 'title': 'Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization', 'authors': 'Jaewook Lee, Alexander Scarlatos, Andrew Lan', 'link': 'https://arxiv.org/abs/2507.05137', 'abstract': 'Learning Japanese vocabulary is a challenge for learners from Roman alphabet backgrounds due to script differences. Japanese combines syllabaries like hiragana with kanji, which are logographic characters of Chinese origin. Kanji are also complicated due to their complexity and volume. Keyword mnemonics are a common strategy to aid memorization, often using the compositional structure of kanji to form vivid associations. Despite recent efforts to use large language models (LLMs) to assist learners, existing methods for LLM-based keyword mnemonic generation function as a black box, offering limited interpretability. We propose a generative framework that explicitly models the mnemonic construction process as driven by a set of common rules, and learn them using a novel Expectation-Maximization-type algorithm. Trained on learner-authored mnemonics from an online platform, our method learns latent structures and compositional rules, enabling interpretable and systematic mnemonics generation. Experiments show that our method performs well in the cold-start setting for new learners while providing insight into the mechanisms behind effective mnemonic creation.', 'abstract_zh': '罗马字母背景的学习者因书写系统差异而面临日语词汇学习的挑战。日语结合了假名和源自汉字的象形字符 Kanji。Kanji 由于其复杂性和数量而更加复杂。关键词记忆法是一种常见的助记策略，通常利用 Kanji 的构字结构形成生动的联想。尽管最近有努力利用大语言模型（LLMs）辅助学习者，但现有的基于 LLM 的关键词记忆法生成方法缺乏解释性，作为黑盒运作。我们提出一种生成框架，明确地将记忆体构建过程建模为由一套常见规则驱动，并利用一种新型的期望最大化类型算法学习这些规则。在基于一个在线平台的学习者自创记忆法上训练，我们的方法学习潜在结构和构字规则，从而实现可解释和系统的记忆法生成。实验显示，该方法在新学习者冷启动设置中表现良好，同时为有效记忆法创造机制提供了见解。', 'title_zh': '基于期望最大化可解释的漢字记忆生成'}
{'arxiv_id': 'arXiv:2507.05108', 'title': 'Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration', 'authors': 'Yuyi Zhang, Peirong Zhang, Zhenhua Yang, Pengyu Yan, Yongxin Shi, Pengwei Liu, Fengjun Guo, Lianwen Jin', 'link': 'https://arxiv.org/abs/2507.05108', 'abstract': "Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet practical needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and 6,543 synthetic images with character-level and line-level locations, as well as character annotations in different damage grades. AutoHDR mimics historians' restoration workflows through a three-stage approach: OCR-assisted damage localization, vision-language context text prediction, and patch autoregressive appearance restoration. The modular architecture of AutoHDR enables seamless human-machine collaboration, allowing for flexible intervention and optimization at each restoration stage. Experiments demonstrate AutoHDR's remarkable performance in HDR. When processing severely damaged documents, our method improves OCR accuracy from 46.83\\% to 84.05\\%, with further enhancement to 94.25\\% through human-machine collaboration. We believe this work represents a significant advancement in automated historical document restoration and contributes substantially to cultural heritage preservation. The model and dataset are available at this https URL.", 'abstract_zh': '历史文献代表了宝贵的文化遗产，但随着时间的推移，它们经历了严重的退化，受到撕裂、水渍侵蚀和氧化的影响。现有的历史文档修复方法主要侧重于单模态或有限尺寸的修复，无法满足实际需求。为弥补这一缺口，我们提出了一整页历史文档修复数据集（FPHDR）和一种新的自动化历史文档修复解决方案（AutoHDR）。具体来说，FPHDR 包括 1,633 张真实图像和 6,543 张合成图像，其中包含字符级和行级定位以及不同损坏程度的字符注释。AutoHDR 通过三阶段方法模拟历史学家的修复流程：OCR 辅助损坏定位、视觉-语言上下文文本预测以及块自回归外观修复。AutoHDR 的模块化架构使其能够实现无缝的人机协作，在每个修复阶段都允许灵活的干预和优化。实验表明，AutoHDR 在历史文档修复方面表现出色。处理严重损坏的文档时，我们的方法将OCR准确率从46.83%提高到84.05%，并通过人机协作进一步提升至94.25%。我们相信这项工作代表了自动化历史文档修复的重大进展，并对文化遗产保护做出了重要贡献。该模型和数据集可在以下网址获取。', 'title_zh': '复活文化遗产：一种综合历史文献修复的新方法'}
{'arxiv_id': 'arXiv:2507.05101', 'title': 'PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs', 'authors': 'Xinzhe Zheng, Hao Du, Fanding Xu, Jinzhe Li, Zhiyuan Liu, Wenkang Wang, Tao Chen, Wanli Ouyang, Stan Z. Li, Yan Lu, Nanqing Dong, Yang Zhang', 'link': 'https://arxiv.org/abs/2507.05101', 'abstract': "Deep learning-based computational methods have achieved promising results in predicting protein-protein interactions (PPIs). However, existing benchmarks predominantly focus on isolated pairwise evaluations, overlooking a model's capability to reconstruct biologically meaningful PPI networks, which is crucial for biology research. To address this gap, we introduce PRING, the first comprehensive benchmark that evaluates protein-protein interaction prediction from a graph-level perspective. PRING curates a high-quality, multi-species PPI network dataset comprising 21,484 proteins and 186,818 interactions, with well-designed strategies to address both data redundancy and leakage. Building on this golden-standard dataset, we establish two complementary evaluation paradigms: (1) topology-oriented tasks, which assess intra and cross-species PPI network construction, and (2) function-oriented tasks, including protein complex pathway prediction, GO module analysis, and essential protein justification. These evaluations not only reflect the model's capability to understand the network topology but also facilitate protein function annotation, biological module detection, and even disease mechanism analysis. Extensive experiments on four representative model categories, consisting of sequence similarity-based, naive sequence-based, protein language model-based, and structure-based approaches, demonstrate that current PPI models have potential limitations in recovering both structural and functional properties of PPI networks, highlighting the gap in supporting real-world biological applications. We believe PRING provides a reliable platform to guide the development of more effective PPI prediction models for the community. The dataset and source code of PRING are available at this https URL.", 'abstract_zh': '基于深度学习的计算方法在预测蛋白质-蛋白质相互作用（PPIs）方面取得了令人鼓舞的结果。然而，现有的基准主要侧重于单独的成对评估，忽视了模型重建生物意义PPI网络的能力，这对于生物学研究至关重要。为了填补这一空白，我们介绍了PRING，这是第一个从图层面评价蛋白质-蛋白质相互作用预测的综合基准。PRING收集了一个高质量的跨物种PPI网络数据集，包含21,484种蛋白质和186,818种相互作用，并设计了策略来解决数据冗余和泄露问题。基于这个黄金标准数据集，我们建立了两种互补的评估范式：（1）拓扑导向任务，评估跨物种和跨物种PPI网络构建；（2）功能导向任务，包括蛋白质复合体路径预测、GO模块分析和关键蛋白质验证。这些评估不仅反映了模型对网络拓扑的理解能力，还促进了蛋白质功能注释、生物模块检测，甚至疾病机制分析。针对四个代表性模型类别进行了广泛的实验，包括基于序列相似性、基于原始序列、基于蛋白质语言模型和基于结构的方法，结果表明当前的PPI模型在恢复PPI网络的结构和功能特性方面存在潜在局限性，突显了支持实际生物应用场景的差距。我们相信PRING为社区提供了可靠的平台，以指导更有效的PPI预测模型的发展。PRING的数据集和源代码可在以下链接获取：this https URL。', 'title_zh': 'PRING: 从成对分析到图表示重新思考蛋白质-蛋白质相互作用预测'}
{'arxiv_id': 'arXiv:2507.05077', 'title': 'Sequential Attention-based Sampling for Histopathological Analysis', 'authors': 'Tarun G, Naman Malpani, Gugan Thoppe, Sridharan Devarajan', 'link': 'https://arxiv.org/abs/2507.05077', 'abstract': 'Deep neural networks are increasingly applied for automated histopathology. Yet, whole-slide images (WSIs) are often acquired at gigapixel sizes, rendering it computationally infeasible to analyze them entirely at high resolution. Diagnostic labels are largely available only at the slide-level, because expert annotation of images at a finer (patch) level is both laborious and expensive. Moreover, regions with diagnostic information typically occupy only a small fraction of the WSI, making it inefficient to examine the entire slide at full resolution. Here, we propose SASHA -- {\\it S}equential {\\it A}ttention-based {\\it S}ampling for {\\it H}istopathological {\\it A}nalysis -- a deep reinforcement learning approach for efficient analysis of histopathological images. First, SASHA learns informative features with a lightweight hierarchical, attention-based multiple instance learning (MIL) model. Second, SASHA samples intelligently and zooms selectively into a small fraction (10-20\\%) of high-resolution patches, to achieve reliable diagnosis. We show that SASHA matches state-of-the-art methods that analyze the WSI fully at high-resolution, albeit at a fraction of their computational and memory costs. In addition, it significantly outperforms competing, sparse sampling methods. We propose SASHA as an intelligent sampling model for medical imaging challenges that involve automated diagnosis with exceptionally large images containing sparsely informative features.', 'abstract_zh': '基于注意力的级联采样方法——SASHA：一种高效的组织病理图像分析深度强化学习方法', 'title_zh': '基于序列注意力的采样方法用于组织病理学分析'}
{'arxiv_id': 'arXiv:2507.05068', 'title': 'ICAS: Detecting Training Data from Autoregressive Image Generative Models', 'authors': 'Hongyao Yu, Yixiang Qiu, Yiheng Yang, Hao Fang, Tianqu Zhuang, Jiaxin Hong, Bin Chen, Hao Wu, Shu-Tao Xia', 'link': 'https://arxiv.org/abs/2507.05068', 'abstract': 'Autoregressive image generation has witnessed rapid advancements, with prominent models such as scale-wise visual auto-regression pushing the boundaries of visual synthesis. However, these developments also raise significant concerns regarding data privacy and copyright. In response, training data detection has emerged as a critical task for identifying unauthorized data usage in model training. To better understand the vulnerability of autoregressive image generative models to such detection, we conduct the first study applying membership inference to this domain. Our approach comprises two key components: implicit classification and an adaptive score aggregation strategy. First, we compute the implicit token-wise classification score within the query image. Then we propose an adaptive score aggregation strategy to acquire a final score, which places greater emphasis on the tokens with lower scores. A higher final score indicates that the sample is more likely to be involved in the training set. To validate the effectiveness of our method, we adapt existing detection algorithms originally designed for LLMs to visual autoregressive models. Extensive experiments demonstrate the superiority of our method in both class-conditional and text-to-image scenarios. Moreover, our approach exhibits strong robustness and generalization under various data transformations. Furthermore, sufficient experiments suggest two novel key findings: (1) A linear scaling law on membership inference, exposing the vulnerability of large foundation models. (2) Training data from scale-wise visual autoregressive models is easier to detect than other autoregressive this http URL code is available at this https URL.', 'abstract_zh': '自回归图像生成见证了快速的发展， prominant 模型如规模级视觉自回归推动了视觉合成的边界。然而，这些进展也引发了关于数据隐私和版权的重大关切。为应对这一挑战，训练数据检测已成为关键任务，用于识别未经授权的 数据使用情况。为更好地理解自回归图像生成模型对这种检测的脆弱性，我们首次将成员推理应用于该领域。我们的方法包括两个关键组件：隐式分类和自适应得分聚合策略。首先，我们计算查询图像中的隐式标记级分类得分。然后，我们提出了一种自适应得分聚合策略，以获得最终得分，该策略更重视得分较低的标记。较高的最终得分表明样本更有可能包含在训练集中。为了验证我们方法的有效性，我们将专为LLM设计的现有检测算法适应到视觉自回归模型。大量实验表明，我们的方法在类别条件和文本生成图像情境中都表现优越。此外，我们的方法在各种数据变换下表现出较强的鲁棒性和泛化能力。进一步的实验还揭示了两个新的关键发现：(1) 成员推理的线性标度定律，揭示了大型基础模型的脆弱性。(2) 规模级视觉自回归模型的训练数据比其他自回归模型更容易检测。该研究的代码可在 https://XXX 收到。', 'title_zh': 'ICAS: 从自回归图像生成模型中检测训练数据'}
{'arxiv_id': 'arXiv:2507.05030', 'title': 'Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction and Developing Chatbots for Social Good', 'authors': 'Celeste Campos-Castillo, Xuan Kang, Linnea I. Laestadius', 'link': 'https://arxiv.org/abs/2507.05030', 'abstract': 'Recently, research into chatbots (also known as conversational agents, AI agents, voice assistants), which are computer applications using artificial intelligence to mimic human-like conversation, has grown sharply. Despite this growth, sociology lags other disciplines (including computer science, medicine, psychology, and communication) in publishing about chatbots. We suggest sociology can advance understanding of human-chatbot interaction and offer four sociological theories to enhance extant work in this field. The first two theories (resource substitution theory, power-dependence theory) add new insights to existing models of the drivers of chatbot use, which overlook sociological concerns about how social structure (e.g., systemic discrimination, the uneven distribution of resources within networks) inclines individuals to use chatbots, including problematic levels of emotional dependency on chatbots. The second two theories (affect control theory, fundamental cause of disease theory) help inform the development of chatbot-driven interventions that minimize safety risks and enhance equity by leveraging sociological insights into how chatbot outputs could attend to cultural contexts (e.g., affective norms) to promote wellbeing and enhance communities (e.g., opportunities for civic participation). We discuss the value of applying sociological theories for advancing theorizing about human-chatbot interaction and developing chatbots for social good.', 'abstract_zh': '最近，关于聊天机器人的研究（也称为对话代理、AI代理、语音助手）呈快速增长之势，这些是使用人工智能模仿人类对话的计算机应用程序。尽管如此， sociology在关于聊天机器人的出版物方面仍然落后于计算机科学、医学、心理学和传播学等其他学科。我们建议 sociology可以通过提出四条社会学理论来推进对人类与聊天机器人交互的理解，并增强该领域现有的研究工作。前两条理论（资源替代理论、权力依赖理论）为聊天机器人使用动因的现有模型增添了新的见解，这些模型忽视了社会结构（如系统性歧视、网络内部资源分配不均）如何促使个人使用聊天机器人，包括对聊天机器人的情感依赖程度可能达到不健康的水平的社会学担忧。后两条理论（情感控制理论、疾病根本原因理论）有助于指导由聊天机器人驱动的干预措施的发展，以减轻安全风险并促进公平，这些干预措施利用社会学洞见来关注聊天机器人输出如何考虑到文化背景（如情感规范）促进福祉并增强社区（如公民参与的机会）。我们探讨了应用社会学理论以推进对人类与聊天机器人交互的理解以及开发用于社会福利的聊天机器人的价值。', 'title_zh': '社会学视角下的人机对话理论建构与促进社会福祉聊天机器人的开发'}
{'arxiv_id': 'arXiv:2507.05019', 'title': 'Meta-Learning Transformers to Improve In-Context Generalization', 'authors': 'Lorenzo Braccaioli, Anna Vettoruzzo, Prabhant Singh, Joaquin Vanschoren, Mohamed-Rafik Bouguelia, Nicola Conci', 'link': 'https://arxiv.org/abs/2507.05019', 'abstract': 'In-context learning enables transformer models to generalize to new tasks based solely on input prompts, without any need for weight updates. However, existing training paradigms typically rely on large, unstructured datasets that are costly to store, difficult to evaluate for quality and balance, and pose privacy and ethical concerns due to the inclusion of sensitive information. Motivated by these limitations and risks, we propose an alternative training strategy where we leverage a collection of multiple, small-scale, and domain-specific datasets. We empirically demonstrate that the increased quality and diversity of such data improve the generalization abilities of in-context learners beyond their training domain, while achieving comparable performance with models trained on a single large-scale dataset. We investigate this paradigm by leveraging meta-learning to train an in-context learner on the Meta-Album collection under several settings. Firstly, we show the performance in a controlled environment, where the test domain is completely excluded from the training knowledge. Secondly, we explore the robustness of these models to forgetting in a continual scenario where the information is accessible for a limited time. Finally, we explore the more challenging unsupervised scenario. Our findings demonstrate that transformers still generalize for in-context prediction when trained on a curated dataset collection while offering advantages in modularity and replaceability.', 'abstract_zh': '基于上下文学习透过输入提示使变压器模型能够在无需权重更新的情况下泛化到新任务。然而，现有的训练范式通常依赖于大规模、无结构的数据集，这些数据集存储成本高、质量评估困难、平衡性差，并且由于包含了敏感信息而带来隐私和伦理方面的担忧。鉴于这些限制和风险，我们提出了一种替代的训练策略，其中我们利用多个小型且领域特定的数据集。我们实验证明，此类数据的质量和多样性提高有助于在训练领域之外增强基于上下文的学习器的泛化能力，同时在性能上与在单一大规模数据集上训练的模型相当。我们通过元学习，将基于上下文的学习器在Meta-Album集合下进行了训练，以探索这一范式。首先，我们在一个受控环境中展示了性能，其中测试领域完全不包括训练知识。其次，我们在持续学习场景中探究了这些模型对抗遗忘的鲁棒性，信息在此场景下仅在有限时间内可用。最后，我们探讨了更具挑战性的无监督场景。我们的研究结果表明，尽管是在精心筛选的数据集上进行训练，变压器模型仍然能在基于上下文的预测中泛化，并且具备模块化和可替换的优点。', 'title_zh': '元学习变换器以提高上下文内泛化能力'}
{'arxiv_id': 'arXiv:2507.04981', 'title': 'Classification of autoimmune diseases from Peripheral blood TCR repertoires by multimodal multi-instance learning', 'authors': 'Ruihao Zhang, Fei Ye, Dandan Meng, Yixuan Huang, Maochen, Xiao Liu', 'link': 'https://arxiv.org/abs/2507.04981', 'abstract': 'T cell receptor (TCR) repertoires encode critical immunological signatures for autoimmune diseases, yet their clinical application remains limited by sequence sparsity and low witness rates. We developed EAMil, a multi-instance deep learning framework that leverages TCR sequencing data to diagnose systemic lupus erythematosus (SLE) and rheumatoid arthritis (RA) with exceptional accuracy. By integrating PrimeSeq feature extraction with ESMonehot encoding and enhanced gate attention mechanisms, our model achieved state-of-the-art performance with AUCs of 98.95% for SLE and 97.76% for RA. EAMil successfully identified disease-associated genes with over 90% concordance with established differential analyses and effectively distinguished disease-specific TCR genes. The model demonstrated robustness in classifying multiple disease categories, utilizing the SLEDAI score to stratify SLE patients by disease severity as well as to diagnose the site of damage in SLE patients, and effectively controlling for confounding factors such as age and gender. This interpretable framework for immune receptor analysis provides new insights for autoimmune disease detection and classification with broad potential clinical applications across immune-mediated conditions.', 'abstract_zh': 'T细胞受体(TCR) repertoire编码了自身免疫性疾病的关键免疫学标志，但由于序列稀疏性和低检测率，其临床应用受到限制。我们开发了一种名为EAMil的多实例深度学习框架，利用TCR测序数据以极高的准确性诊断系统性红斑狼疮(SLE)和类风湿性关节炎(RA)。通过集成PrimeSeq特征提取、ESMon-hot编码和增强门控注意力机制，我们的模型在SLE上的AUC达到98.95%，在RA上的AUC达到97.76%，实现了最先进的性能。EAMil成功地识别了与疾病相关的基因，与现有差异分析的一致性超过90%，有效地区分了疾病的特异性TCR基因。该模型在分类多种疾病类别时表现出鲁棒性，使用SLEDAI评分按疾病严重程度对SLE患者进行分层，并诊断SLE患者的受损部位，有效地控制了年龄和性别等混杂因素。该可解释的免疫受体分析框架为自身免疫疾病的检测和分类提供了新的见解，并具有广泛的临床应用潜力，适用于免疫介导的疾病。', 'title_zh': '基于多模态多实例学习的外周血TCR repertoire在自身免疫疾病分类中的应用'}
{'arxiv_id': 'arXiv:2507.04966', 'title': 'LAPS-Diff: A Diffusion-Based Framework for Singing Voice Synthesis With Language Aware Prosody-Style Guided Learning', 'authors': 'Sandipan Dhar, Mayank Gupta, Preeti Rao', 'link': 'https://arxiv.org/abs/2507.04966', 'abstract': 'The field of Singing Voice Synthesis (SVS) has seen significant advancements in recent years due to the rapid progress of diffusion-based approaches. However, capturing vocal style, genre-specific pitch inflections, and language-dependent characteristics remains challenging, particularly in low-resource scenarios. To address this, we propose LAPS-Diff, a diffusion model integrated with language-aware embeddings and a vocal-style guided learning mechanism, specifically designed for Bollywood Hindi singing style. We curate a Hindi SVS dataset and leverage pre-trained language models to extract word and phone-level embeddings for an enriched lyrics representation. Additionally, we incorporated a style encoder and a pitch extraction model to compute style and pitch losses, capturing features essential to the naturalness and expressiveness of the synthesized singing, particularly in terms of vocal style and pitch variations. Furthermore, we utilize MERT and IndicWav2Vec models to extract musical and contextual embeddings, serving as conditional priors to refine the acoustic feature generation process further. Based on objective and subjective evaluations, we demonstrate that LAPS-Diff significantly improves the quality of the generated samples compared to the considered state-of-the-art (SOTA) model for our constrained dataset that is typical of the low resource scenario.', 'abstract_zh': '基于语言感知嵌入和声乐风格引导学习机制的LAPS-Diff声乐语音合成模型', 'title_zh': 'LAPS-Diff：一种基于扩散的歌声合成框架，带有语言意识音调风格引导学习'}
{'arxiv_id': 'arXiv:2507.04917', 'title': 'Leadership Detection via Time-Lagged Correlation-Based Network Inference', 'authors': 'Thayanne França da Silva, José Everardo Bessa Maia', 'link': 'https://arxiv.org/abs/2507.04917', 'abstract': 'Understanding leadership dynamics in collective behavior is a key challenge in animal ecology, swarm robotics, and intelligent transportation. Traditional information-theoretic approaches, including Transfer Entropy (TE) and Time-Lagged Mutual Information (TLMI), have been widely used to infer leader-follower relationships but face critical limitations in noisy or short-duration datasets due to their reliance on robust probability estimations. This study proposes a method based on dynamic network inference using time-lagged correlations across multiple kinematic variables: velocity, acceleration, and direction. Our approach constructs directed influence graphs over time, enabling the identification of leadership patterns without the need for large volumes of data or parameter-sensitive discretization. We validate our method through two multi-agent simulations in NetLogo: a modified Vicsek model with informed leaders and a predator-prey model featuring coordinated and independent wolf groups. Experimental results demonstrate that the network-based method outperforms TE and TLMI in scenarios with limited spatiotemporal observations, ranking true leaders at the top of influence metrics more consistently than TE and TLMI.', 'abstract_zh': '理解群体行为中的领导动态是动物生态学、 swarm 机器人技术和智能交通系统中的一个关键挑战。传统的信息理论方法，包括转移信息熵（TE）和时间延迟互信息（TLMI），广泛用于推断领导者-跟随者关系，但在嘈杂或短暂持续时间的数据集中由于依赖稳健的概率估计而面临关键限制。本研究提出了一种基于动态网络推断的方法，该方法使用多个动态变量（速度、加速度和方向）的时间延迟相关性来构建跨时间的有向影响图，无需大量数据或参数敏感的离散化即可识别领导模式。通过NetLogo中的两个多智能体模拟（修改后的Vicsek模型和具有协调和独立狼群的捕食者-被捕食者模型）验证了该方法。实验结果表明，在时空观测有限的场景中，网络方法在影响力指标中更一致地排名真实领导者，优于TE和TLMI。', 'title_zh': '基于时间延迟相关性网络推断的领导人检测'}
{'arxiv_id': 'arXiv:2507.04903', 'title': 'BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning', 'authors': 'Thinh Dao, Dung Thuy Nguyen, Khoa D Doan, Kok-Seng Wong', 'link': 'https://arxiv.org/abs/2507.04903', 'abstract': 'Federated Learning (FL) systems are vulnerable to backdoor attacks, where adversaries train their local models on poisoned data and submit poisoned model updates to compromise the global model. Despite numerous proposed attacks and defenses, divergent experimental settings, implementation errors, and unrealistic assumptions hinder fair comparisons and valid conclusions about their effectiveness in real-world scenarios. To address this, we introduce BackFed - a comprehensive benchmark suite designed to standardize, streamline, and reliably evaluate backdoor attacks and defenses in FL, with a focus on practical constraints. Our benchmark offers key advantages through its multi-processing implementation that significantly accelerates experimentation and the modular design that enables seamless integration of new methods via well-defined APIs. With a standardized evaluation pipeline, we envision BackFed as a plug-and-play environment for researchers to comprehensively and reliably evaluate new attacks and defenses. Using BackFed, we conduct large-scale studies of representative backdoor attacks and defenses across both Computer Vision and Natural Language Processing tasks with diverse model architectures and experimental settings. Our experiments critically assess the performance of proposed attacks and defenses, revealing unknown limitations and modes of failures under practical conditions. These empirical insights provide valuable guidance for the development of new methods and for enhancing the security of FL systems. Our framework is openly available at this https URL.', 'abstract_zh': '联邦学习（FL）系统易受后门攻击，攻击者可以在中毒数据上训练其本地模型，并提交中毒模型更新以 compromize 全局模型。尽管提出了许多攻击和防御方法，但由于实验设置差异、实现错误以及不切实际的假设，阻碍了在实际场景中公平比较和得出有效结论。为了解决这个问题，我们引入了BackFed - 一个旨在标准化、简化并可靠评估FL中后门攻击和防御的基准套件，重点关注实际约束条件。我们的基准通过多处理实现显著加速了实验，并通过模块化设计支持通过明确定义的API无缝集成新方法。通过标准化评估流程，我们设想BackFed为研究人员提供了一个即插即用的环境，以全面可靠地评估新的攻击和防御方法。使用BackFed，我们针对计算机视觉和自然语言处理任务进行了广泛的代表性后门攻击和防御的大规模研究，涉及多样化的模型架构和实验设置。我们的实验严格评估了所提出的攻击和防御方法，在实际条件下的性能揭示了未知的局限性和失败模式。这些经验见解为新方法的开发以及增强FL系统的安全性提供了宝贵的指导。我们的框架已公开可用，详情请访问 this https URL。', 'title_zh': 'BackFed: 一种高效的联邦学习后门攻击标准化基准套件'}
{'arxiv_id': 'arXiv:2507.04883', 'title': 'Beyond Training-time Poisoning: Component-level and Post-training Backdoors in Deep Reinforcement Learning', 'authors': 'Sanyam Vyas, Alberto Caron, Chris Hicks, Pete Burnap, Vasilios Mavroudis', 'link': 'https://arxiv.org/abs/2507.04883', 'abstract': 'Deep Reinforcement Learning (DRL) systems are increasingly used in safety-critical applications, yet their security remains severely underexplored. This work investigates backdoor attacks, which implant hidden triggers that cause malicious actions only when specific inputs appear in the observation space. Existing DRL backdoor research focuses solely on training-time attacks requiring unrealistic access to the training pipeline. In contrast, we reveal critical vulnerabilities across the DRL supply chain where backdoors can be embedded with significantly reduced adversarial privileges. We introduce two novel attacks: (1) TrojanentRL, which exploits component-level flaws to implant a persistent backdoor that survives full model retraining; and (2) InfrectroRL, a post-training backdoor attack which requires no access to training, validation, nor test data. Empirical and analytical evaluations across six Atari environments show our attacks rival state-of-the-art training-time backdoor attacks while operating under much stricter adversarial constraints. We also demonstrate that InfrectroRL further evades two leading DRL backdoor defenses. These findings challenge the current research focus and highlight the urgent need for robust defenses.', 'abstract_zh': '深度 reinforcement learning (DRL) 系统在安全关键应用中的使用日益增多，但其安全性仍严重未被探索。本研究调查了后门攻击，这种攻击植入隐式触发器，只有在观测空间出现特定输入时才会导致恶意行为。现有的 DRL 后门研究仅关注训练时间攻击，需要不现实的访问训练管道的权限。相比之下，我们揭示了 DRL 供应链中的关键漏洞，在这些漏洞中，后门可以被嵌入，且敌手权限明显降低。我们提出了两个新型攻击：（1）TrojanentRL，利用组件级漏洞植入持久性后门，即使进行全模型重新训练也能存活；（2）InfrectroRL，一种后训练后门攻击，无需访问训练、验证或测试数据。在六个 Atari 环境上的实证和分析评估显示，我们的攻击在严格的敌手约束条件下，与最先进的训练时间后门攻击性能相当。我们还展示了 InfrectroRL 进一步规避了两种领先的 DRL 后门防御措施。这些发现挑战了当前的研究重点，并强调了迫切需要稳健的防御措施。', 'title_zh': '超越训练时污染：深度强化学习中的组件级和后训练后门'}
{'arxiv_id': 'arXiv:2507.04868', 'title': 'A Novel Approach for Estimating Positive Lyapunov Exponents in One-Dimensional Chaotic Time Series Using Machine Learning', 'authors': 'A. Velichko, M. Belyaev, P. Boriskov', 'link': 'https://arxiv.org/abs/2507.04868', 'abstract': 'Understanding and quantifying chaos in nonlinear dynamical systems remains a fundamental challenge in science and engineering. The Lyapunov exponent is a key measure of chaotic behavior, but its accurate estimation from experimental data is often hindered by methodological and computational limitations. In this work, we present a novel machine-learning-based approach for estimating the positive Lyapunov exponent (MLE) from one-dimensional time series, using the growth of out-of-sample prediction errors as a proxy for trajectory divergence. Our method demonstrates high scientific relevance, offering a robust, data-driven alternative to traditional analytic techniques. Through comprehensive testing on several canonical chaotic maps - including the logistic, sine, cubic, and Chebyshev maps - we achieved a coefficient of determination R2pos > 0.9 between predicted and theoretical MLE values for time series as short as M = 200 points. The best accuracy was observed for the Chebyshev map (R2pos = 0.999). Notably, the proposed method maintains high computational efficiency and generalizes well across various machine learning algorithms. These results highlight the significance of our approach for practical chaos analysis in both synthetic and experimental settings, opening new possibilities for robust nonlinear dynamics assessment when only time series data are available.', 'abstract_zh': '理解与定量分析非线性动力系统中的混沌现象仍然是科学和工程中的基本挑战。Lyapunov指数是衡量混沌行为的关键指标，但其从实验数据中准确估计常常受到方法论和计算限制的阻碍。在本工作中，我们提出了一种基于机器学习的方法，用于估计一维时间序列的正Lyapunov指数（MLE），采用离样预测误差的增长作为轨迹发散的代理。该方法具有高度的科学意义，提供了一种稳健的数据驱动替代传统分析技术。通过对包括逻辑映射、正弦映射、三次映射和Chebyshev映射在内的几个经典混沌映射进行全面测试，我们实现了预测和理论MLE值之间的决定系数R2pos > 0.9，对于时间序列长度仅为M = 200点的情况也是如此。Chebyshev映射达到了最佳准确性（R2pos = 0.999）。值得注意的是，所提出的方法保持了高计算效率，并且在各种机器学习算法中表现良好。这些结果突显了我们在合成和实验条件下进行实际混沌分析的重要性，为仅使用时间序列数据评估非线性动力学提供新的可能性。', 'title_zh': '一种基于机器学习的一维混沌时间序列正李雅普诺夫 exponent 估计的新方法'}
{'arxiv_id': 'arXiv:2507.04817', 'title': 'Fast-VGAN: Lightweight Voice Conversion with Explicit Control of F0 and Duration Parameters', 'authors': 'Mathilde Abrassart, Nicolas Obin, Axel Roebel', 'link': 'https://arxiv.org/abs/2507.04817', 'abstract': 'Precise control over speech characteristics, such as pitch, duration, and speech rate, remains a significant challenge in the field of voice conversion. The ability to manipulate parameters like pitch and syllable rate is an important element for effective identity conversion, but can also be used independently for voice transformation, achieving goals that were historically addressed by vocoder-based methods.\nIn this work, we explore a convolutional neural network-based approach that aims to provide means for modifying fundamental frequency (F0), phoneme sequences, intensity, and speaker identity. Rather than relying on disentanglement techniques, our model is explicitly conditioned on these factors to generate mel spectrograms, which are then converted into waveforms using a universal neural vocoder. Accordingly, during inference, F0 contours, phoneme sequences, and speaker embeddings can be freely adjusted, allowing for intuitively controlled voice transformations.\nWe evaluate our approach on speaker conversion and expressive speech tasks using both perceptual and objective metrics. The results suggest that the proposed method offers substantial flexibility, while maintaining high intelligibility and speaker similarity.', 'abstract_zh': '精确控制语音特性（如音调、持续时间和语速）在语音转换领域仍是一项重大挑战。调节音调和音节速率等参数的能力是有效身份转换的重要要素，但也可以独立用于语音转换，实现历史上由 vocoder 基方法解决的目标。\n\n在本工作中，我们探索了一种基于卷积神经网络的方法，旨在提供修改基频（F0）、音素序列、强度和说话人身份的手段。我们的模型不依赖于分离技术，而是明确地根据这些因素生成 mel 频谱图，并使用通用神经 vocoder 转换为波形。因此，在推理过程中，F0 轮廓、音素序列和说话人嵌入可以自由调整，从而实现直观控制的语音转换。\n\n我们在说话人转换和表现性语音任务上使用感知和客观指标评估了我们的方法。结果表明，所提出的方法具有很高的灵活性，同时保持了高可理解性和说话人相似性。', 'title_zh': 'Fast-VGAN: 轻量级语音转换，具备明确的F0和时长参数控制'}
{'arxiv_id': 'arXiv:2507.04793', 'title': 'A Survey of Pun Generation: Datasets, Evaluations and Methodologies', 'authors': 'Yuchen Su, Yonghua Zhu, Ruofan Wang, Zijian Huang, Diana Benavides-Prado, Michael Witbrock', 'link': 'https://arxiv.org/abs/2507.04793', 'abstract': 'Pun generation seeks to creatively modify linguistic elements in text to produce humour or evoke double meanings. It also aims to preserve coherence and contextual appropriateness, making it useful in creative writing and entertainment across various media and contexts. Although pun generation has received considerable attention in computational linguistics, there is currently no dedicated survey that systematically reviews this specific area. To bridge this gap, this paper provides a comprehensive review of pun generation datasets and methods across different stages, including conventional approaches, deep learning techniques, and pre-trained language models. Additionally, we summarise both automated and human evaluation metrics used to assess the quality of pun generation. Finally, we discuss the research challenges and propose promising directions for future work.', 'abstract_zh': 'pun生成旨在创意性地修改文本中的语言元素以产生幽默或引发双关含义，同时力求保持连贯性和语境适宜性，使其在各种媒体和情境下的创意写作和娱乐中具有实用性。尽管pun生成在计算语言学领域受到了广泛关注，但目前尚无专门系统回顾这一特定领域的文献。为填补这一空白，本文提供了跨不同阶段的pun生成数据集和方法的全面回顾，包括传统方法、深度学习技术和预训练语言模型。此外，我们总结了评估pun生成质量的自动化和人类评估指标。最后，我们讨论了研究挑战，并提出了未来工作的有希望的方向。', 'title_zh': '生成性惩罚研究综述：数据集、评估与方法'}
{'arxiv_id': 'arXiv:2507.04792', 'title': 'Model Compression using Progressive Channel Pruning', 'authors': 'Jinyang Guo, Weichen Zhang, Wanli Ouyang, Dong Xu', 'link': 'https://arxiv.org/abs/2507.04792', 'abstract': 'In this work, we propose a simple but effective channel pruning framework called Progressive Channel Pruning (PCP) to accelerate Convolutional Neural Networks (CNNs). In contrast to the existing channel pruning methods that prune channels only once per layer in a layer-by-layer fashion, our new progressive framework iteratively prunes a small number of channels from several selected layers, which consists of a three-step attempting-selecting-pruning pipeline in each iteration. In the attempting step, we attempt to prune a pre-defined number of channels from one layer by using any existing channel pruning methods and estimate the accuracy drop for this layer based on the labelled samples in the validation set. In the selecting step, based on the estimated accuracy drops for all layers, we propose a greedy strategy to automatically select a set of layers that will lead to less overall accuracy drop after pruning these layers. In the pruning step, we prune a small number of channels from these selected layers. We further extend our PCP framework to prune channels for the deep transfer learning methods like Domain Adversarial Neural Network (DANN), in which we effectively reduce the data distribution mismatch in the channel pruning process by using both labelled samples from the source domain and pseudo-labelled samples from the target domain. Our comprehensive experiments on two benchmark datasets demonstrate that our PCP framework outperforms the existing channel pruning approaches under both supervised learning and transfer learning settings.', 'abstract_zh': '一种渐进通道剪枝框架（PCP）以加速卷积神经网络', 'title_zh': '渐进通道剪枝的模型压缩'}
{'arxiv_id': 'arXiv:2507.04769', 'title': 'From Imitation to Innovation: The Emergence of AI Unique Artistic Styles and the Challenge of Copyright Protection', 'authors': 'Zexi Jia, Chuanwei Huang, Yeshuang Zhu, Hongyan Fei, Ying Deng, Zhiqiang Yuan, Jiapei Zhang, Jinchao Zhang, Jie Zhou', 'link': 'https://arxiv.org/abs/2507.04769', 'abstract': 'Current legal frameworks consider AI-generated works eligible for copyright protection when they meet originality requirements and involve substantial human intellectual input. However, systematic legal standards and reliable evaluation methods for AI art copyrights are lacking. Through comprehensive analysis of legal precedents, we establish three essential criteria for determining distinctive artistic style: stylistic consistency, creative uniqueness, and expressive accuracy. To address these challenges, we introduce ArtBulb, an interpretable and quantifiable framework for AI art copyright judgment that combines a novel style description-based multimodal clustering method with multimodal large language models (MLLMs). We also present AICD, the first benchmark dataset for AI art copyright annotated by artists and legal experts. Experimental results demonstrate that ArtBulb outperforms existing models in both quantitative and qualitative evaluations. Our work aims to bridge the gap between the legal and technological communities and bring greater attention to the societal issue of AI art copyrights.', 'abstract_zh': '当前的法律框架认为，当AI生成的作品满足原创性要求并包含显著的人工智能输入时，可以获得版权保护。然而，系统的法律标准和可靠的AI艺术版权评估方法仍然缺乏。通过综合分析法律先例，我们确立了确定独特艺术风格的三个基本标准：风格一致性、创造性独特性和表达准确性。为应对这些挑战，我们引入了ArtBulb，这是一种可解释和可量化的方法，用于评估AI艺术版权，它结合了一种新颖的基于风格描述的多模态聚类方法和多模态大语言模型(MLLM)。我们还提出了由艺术家和法律专家标注的AICD，这是首个用于AI艺术版权评估的标准数据集。实验结果表明，ArtBulb在定量和定性评估中均优于现有模型。我们的工作旨在弥合法律和技术社区之间的差距，并引起社会各界对AI艺术版权问题的关注。', 'title_zh': '从模仿到创新：AI独特艺术风格的 emergence 和版权保护挑战'}
{'arxiv_id': 'arXiv:2507.04738', 'title': 'Word stress in self-supervised speech models: A cross-linguistic comparison', 'authors': 'Martijn Bentum, Louis ten Bosch, Tomas O. Lentz', 'link': 'https://arxiv.org/abs/2507.04738', 'abstract': 'In this paper we study word stress representations learned by self-supervised speech models (S3M), specifically the Wav2vec 2.0 model. We investigate the S3M representations of word stress for five different languages: Three languages with variable or lexical stress (Dutch, English and German) and two languages with fixed or demarcative stress (Hungarian and Polish). We train diagnostic stress classifiers on S3M embeddings and show that they can distinguish between stressed and unstressed syllables in read-aloud short sentences with high accuracy. We also tested language-specificity effects of S3M word stress. The results indicate that the word stress representations are language-specific, with a greater difference between the set of variable versus the set of fixed stressed languages.', 'abstract_zh': '本文研究了自监督语音模型（S3M）学习的单词重音表示，具体分析了Wav2vec 2.0模型。我们探讨了五种不同语言的S3M重音表示：三种具有可变或词汇重音的语言（荷兰语、英语和德语），以及两种具有固定或界标重音的语言（匈牙利语和波兰语）。我们在S3M嵌入上训练诊断重音分类器，并展示了它们能够以高精度区分朗读短句中的重读和未重读音节。我们还测试了S3M单词重音的特定语言效应。结果表明，单词重音表示具有语言特异性，可变重音语言与固定重音语言之间的差异更大。', 'title_zh': '自主监督语音模型中的重音：跨语言对比研究'}
{'arxiv_id': 'arXiv:2507.04690', 'title': 'Bridging KAN and MLP: MJKAN, a Hybrid Architecture with Both Efficiency and Expressiveness', 'authors': 'Hanseon Joo, Hayoung Choi, Ook Lee, Minjong Cheon', 'link': 'https://arxiv.org/abs/2507.04690', 'abstract': "Kolmogorov-Arnold Networks (KANs) have garnered attention for replacing fixed activation functions with learnable univariate functions, but they exhibit practical limitations, including high computational costs and performance deficits in general classification tasks. In this paper, we propose the Modulation Joint KAN (MJKAN), a novel neural network layer designed to overcome these challenges. MJKAN integrates a FiLM (Feature-wise Linear Modulation)-like mechanism with Radial Basis Function (RBF) activations, creating a hybrid architecture that combines the non-linear expressive power of KANs with the efficiency of Multilayer Perceptrons (MLPs). We empirically validated MJKAN's performance across a diverse set of benchmarks, including function regression, image classification (MNIST, CIFAR-10/100), and natural language processing (AG News, SMS Spam). The results demonstrate that MJKAN achieves superior approximation capabilities in function regression tasks, significantly outperforming MLPs, with performance improving as the number of basis functions increases. Conversely, in image and text classification, its performance was competitive with MLPs but revealed a critical dependency on the number of basis functions. We found that a smaller basis size was crucial for better generalization, highlighting that the model's capacity must be carefully tuned to the complexity of the data to prevent overfitting. In conclusion, MJKAN offers a flexible architecture that inherits the theoretical advantages of KANs while improving computational efficiency and practical viability.", 'abstract_zh': 'Modulation Joint Kolmogorov-Arnold Networks', 'title_zh': '将KAN和MLP桥梁化：MJKAN，一种兼具效率与表达性的混合架构'}
{'arxiv_id': 'arXiv:2507.04634', 'title': 'LTMSformer: A Local Trend-Aware Attention and Motion State Encoding Transformer for Multi-Agent Trajectory Prediction', 'authors': 'Yixin Yan, Yang Li, Yuanfan Wang, Xiaozhou Zhou, Beihao Xia, Manjiang Hu, Hongmao Qin', 'link': 'https://arxiv.org/abs/2507.04634', 'abstract': 'It has been challenging to model the complex temporal-spatial dependencies between agents for trajectory prediction. As each state of an agent is closely related to the states of adjacent time steps, capturing the local temporal dependency is beneficial for prediction, while most studies often overlook it. Besides, learning the high-order motion state attributes is expected to enhance spatial interaction modeling, but it is rarely seen in previous works. To address this, we propose a lightweight framework, LTMSformer, to extract temporal-spatial interaction features for multi-modal trajectory prediction. Specifically, we introduce a Local Trend-Aware Attention mechanism to capture the local temporal dependency by leveraging a convolutional attention mechanism with hierarchical local time boxes. Next, to model the spatial interaction dependency, we build a Motion State Encoder to incorporate high-order motion state attributes, such as acceleration, jerk, heading, etc. To further refine the trajectory prediction, we propose a Lightweight Proposal Refinement Module that leverages Multi-Layer Perceptrons for trajectory embedding and generates the refined trajectories with fewer model parameters. Experiment results on the Argoverse 1 dataset demonstrate that our method outperforms the baseline HiVT-64, reducing the minADE by approximately 4.35%, the minFDE by 8.74%, and the MR by 20%. We also achieve higher accuracy than HiVT-128 with a 68% reduction in model size.', 'abstract_zh': '基于时空交互特征的轻量级框架LTMSformer及其在多模态轨迹预测中的应用', 'title_zh': 'LTMSformer：一种考虑局部趋势关注和运动状态编码的多agents轨迹预测Transformer模型'}
{'arxiv_id': 'arXiv:2507.04619', 'title': 'Information-Guided Diffusion Sampling for Dataset Distillation', 'authors': 'Linfeng Ye, Shayan Mohajer Hamidi, Guang Li, Takahiro Ogawa, Miki Haseyama, Konstantinos N. Plataniotis', 'link': 'https://arxiv.org/abs/2507.04619', 'abstract': 'Dataset distillation aims to create a compact dataset that retains essential information while maintaining model performance. Diffusion models (DMs) have shown promise for this task but struggle in low images-per-class (IPC) settings, where generated samples lack diversity. In this paper, we address this issue from an information-theoretic perspective by identifying two key types of information that a distilled dataset must preserve: ($i$) prototype information $\\mathrm{I}(X;Y)$, which captures label-relevant features; and ($ii$) contextual information $\\mathrm{H}(X | Y)$, which preserves intra-class variability. Here, $(X,Y)$ represents the pair of random variables corresponding to the input data and its ground truth label, respectively. Observing that the required contextual information scales with IPC, we propose maximizing $\\mathrm{I}(X;Y) + \\beta \\mathrm{H}(X | Y)$ during the DM sampling process, where $\\beta$ is IPC-dependent. Since directly computing $\\mathrm{I}(X;Y)$ and $\\mathrm{H}(X | Y)$ is intractable, we develop variational estimations to tightly lower-bound these quantities via a data-driven approach. Our approach, information-guided diffusion sampling (IGDS), seamlessly integrates with diffusion models and improves dataset distillation across all IPC settings. Experiments on Tiny ImageNet and ImageNet subsets show that IGDS significantly outperforms existing methods, particularly in low-IPC regimes. The code will be released upon acceptance.', 'abstract_zh': '基于信息论的Dataset蒸馏：最大化关键信息保留的扩散模型采样方法', 'title_zh': '信息引导的扩散采样用于数据集提炼'}
{'arxiv_id': 'arXiv:2507.04548', 'title': 'SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection', 'authors': 'Renato Cordeiro Ferreira, Dayanne Gomes, Vitor Tamae, Francisco Wernke, Alfredo Goldman', 'link': 'https://arxiv.org/abs/2507.04548', 'abstract': 'Respiratory insufficiency is a medic symptom in which a person gets a reduced amount of oxygen in the blood. This paper reports the experience of building SPIRA: an intelligent system for detecting respiratory insufficiency from voice. It compiles challenges faced in two succeeding implementations of the same architecture, summarizing lessons learned on data collection, training, and inference for future projects in similar systems.', 'abstract_zh': '呼吸不足是一种医疗症状，表现为人体血液中氧含量减少。本文报告了构建SPIRA智能系统以从语音中检测呼吸不足的经验。该报告总结了在相同架构的两次相继实现中遇到的挑战，并概述了在数据收集、训练和推理方面的经验教训，以供类似系统未来项目参考。', 'title_zh': 'SPIRA：构建一种呼吸不足检测的智能系统'}
{'arxiv_id': 'arXiv:2507.04490', 'title': 'Dealing with Uncertainty in Contextual Anomaly Detection', 'authors': 'Luca Bindini, Lorenzo Perini, Stefano Nistri, Jesse Davis, Paolo Frasconi', 'link': 'https://arxiv.org/abs/2507.04490', 'abstract': 'Contextual anomaly detection (CAD) aims to identify anomalies in a target (behavioral) variable conditioned on a set of contextual variables that influence the normalcy of the target variable but are not themselves indicators of anomaly. In many anomaly detection tasks, there exist contextual variables that influence the normalcy of the target variable but are not themselves indicators of anomaly. In this work, we propose a novel framework for CAD, normalcy score (NS), that explicitly models both the aleatoric and epistemic uncertainties. Built on heteroscedastic Gaussian process regression, our method regards the Z-score as a random variable, providing confidence intervals that reflect the reliability of the anomaly assessment. Through experiments on benchmark datasets and a real-world application in cardiology, we demonstrate that NS outperforms state-of-the-art CAD methods in both detection accuracy and interpretability. Moreover, confidence intervals enable an adaptive, uncertainty-driven decision-making process, which may be very important in domains such as healthcare.', 'abstract_zh': '基于上下文的异常检测（Contextual Anomaly Detection, CAD）旨在在一组影响目标变量正常性但本身不是异常指标的上下文变量条件下，识别目标（行为）变量中的异常。在许多异常检测任务中，存在影响目标变量正常性但本身不是异常指标的上下文变量。本文提出了一种新颖的带有置信度评分（Normalcy Score, NS）的CAD框架，该框架明确模型了 aleatoric 和 epistemic 不确定性。基于异方差高斯过程回归，我们的方法将 Z-score 视为随机变量，提供反映异常评估可靠性的置信区间。通过对基准数据集和心脏病学领域的实际应用进行实验，我们证明了 NS 在检测准确性和可解释性方面均优于现有最先进的 CAD 方法。此外，置信区间使适应性的、基于不确定性的决策过程成为可能，在诸如医疗保健等领域中可能非常关键。', 'title_zh': '处理上下文异常检测中的不确定性'}
{'arxiv_id': 'arXiv:2507.04487', 'title': 'LoSiA: Efficient High-Rank Fine-Tuning via Subnet Localization and Optimization', 'authors': 'Xujia Wang. Yunjia Qi, Bin Xu', 'link': 'https://arxiv.org/abs/2507.04487', 'abstract': 'Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, significantly reduce the number of trainable parameters by introducing low-rank decomposition matrices. However, existing methods perform extensive matrix multiplications in domain specialization tasks, resulting in computational inefficiency and sub-optimal fine-tuning performance. Hence, we propose LoSiA(Low-Resources Subnet Integration Adaptation), an innovative method that dynamically localizes and optimizes critical parameters during the training process. Specifically, it identifies a sub-network using gradient sparsity analysis and optimizes it as the trainable target. This design enables effective high-rank adaptation by updating only the sub-network parameters, reducing the additional matrix multiplication. We also present LoSiA-Pro, a faster implementation of LoSiA, which reduces the training latency by about $27\\%$ compared to LoRA. Extensive evaluations show that our method achieves minimal performance drop compared to full fine-tuning, while requiring the least training time across domain specialization and common-sense reasoning tasks. Further analysis shows that LoSiA also reduces forgetting during continued training.', 'abstract_zh': 'Parameter- Efficient Fine-Tuning (PEFT) 方法，如 LoRA，通过引入低秩分解矩阵显著减少可训练参数的数量。然而，现有方法在领域专业化任务中进行大量的矩阵乘法，导致计算效率低下和次优的微调性能。因此，我们提出了一种名为 LoSiA（Low-Resources Subnet Integration Adaptation）的创新方法，在训练过程中动态定位和优化关键参数。具体来说，它使用梯度稀疏性分析来确定一个子网络，并将其优化为可训练的目标。这种设计通过仅更新子网络参数来实现有效的高秩适应，减少了额外的矩阵乘法。我们还提出了 LoSiA-Pro 的更快实现版本，与 LoRA 相比，其训练延迟降低了约 27%。广泛评估表明，我们的方法在领域专业化和常识推理任务中实现了最小的性能下降，同时需要最短的训练时间。进一步分析表明，LoSiA 还可以在继续训练中减少遗忘。', 'title_zh': 'LoSiA: 通过子网络定位与优化实现的高效高秩微调'}
{'arxiv_id': 'arXiv:2507.04441', 'title': 'The Joys of Categorical Conformal Prediction', 'authors': 'Michele Caprio', 'link': 'https://arxiv.org/abs/2507.04441', 'abstract': 'Conformal prediction (CP) is an Uncertainty Representation technique that delivers finite-sample calibrated prediction regions for any underlying Machine Learning model, yet its status as an Uncertainty Quantification (UQ) tool has remained conceptually opaque. We adopt a category-theoretic approach to CP -- framing it as a morphism, embedded in a commuting diagram, of two newly-defined categories -- that brings us three joys. First, we show that -- under minimal assumptions -- CP is intrinsically a UQ mechanism, that is, its UQ capabilities are a structural feature of the method. Second, we demonstrate that CP bridges (and perhaps subsumes) the Bayesian, frequentist, and imprecise probabilistic approaches to predictive statistical reasoning. Finally, we show that a conformal prediction region (CPR) is the image of a covariant functor. This observation is relevant to AI privacy: It implies that privacy noise added locally does not break coverage.', 'abstract_zh': '同构预测（CP）是一种不确定性表示技术，能够为任何基础机器学习模型提供有限样本校准预测区域，然而其作为不确定性量化（UQ）工具的地位仍然概念上不够透明。我们采用范畴论的方法来定义CP——将其视为嵌入在两个 newly-defined 范畴中的态射，并带来了三个喜悦。首先，我们证明在最小假设下，CP 内在地是一种UQ机制，即其UQ能力是该方法的结构特征。其次，我们展示了CP连接（并且可能子囊括）贝叶斯、频率主义和不精确概率预测统计推理的方法论。最后，我们证明同构预测区域（CPR）是共变函子的象。这一观察对于AI隐私具有重要意义：它表明局部添加的隐私噪声不会破坏覆盖性。', 'title_zh': '范畴 conformal 预测的乐趣'}
{'arxiv_id': 'arXiv:2507.04422', 'title': 'Learning Software Bug Reports: A Systematic Literature Review', 'authors': 'Guoming Long, Jingzhi Gong, Hui Fang, Tao Chen', 'link': 'https://arxiv.org/abs/2507.04422', 'abstract': 'The recent advancement of artificial intelligence, especially machine learning (ML), has significantly impacted software engineering research, including bug report analysis. ML aims to automate the understanding, extraction, and correlation of information from bug reports. Despite its growing importance, there has been no comprehensive review in this area. In this paper, we present a systematic literature review covering 1,825 papers, selecting 204 for detailed analysis. We derive seven key findings: 1) Extensive use of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like BERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular for feature representation, with a rise in deep learning approaches. 3) Stop word removal is the most common preprocessing, with structural methods rising after 2020. 4) Eclipse and Mozilla are the most frequently evaluated software projects. 5) Bug categorization is the most common task, followed by bug localization and severity prediction. 6) There is increasing attention on specific bugs like non-functional and performance bugs. 7) Common evaluation metrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold cross-validation preferred for model evaluation. 8) Many studies lack robust statistical tests. We also identify six promising future research directions to provide useful insights for practitioners.', 'abstract_zh': '最近人工智能的进步，尤其是机器学习（ML），显著影响了软件工程研究，包括错误报告分析。机器学习旨在自动化理解、提取和关联错误报告中的信息。尽管其重要性日益凸显，但该领域尚未进行过全面回顾。在本文中，我们进行了系统文献综述，共涵盖1,825篇论文，并选择204篇进行详细分析。我们得出了七个关键发现：1) 广泛使用CNN、LSTM和$k$NN进行错误报告分析，但因复杂性问题，高级模型如BERT的应用不足。2) Word2Vec和TF-IDF广泛用于特征表示，深度学习方法呈上升趋势。3) 去除停用词是最常见的预处理方法，自2020年起，结构化方法逐渐增多。4) Eclipse和Mozilla是最常评估的软件项目。5) 错误分类是最常见的任务，其次是错误定位和严重性预测。6) 不断关注特定类型的错误，如非功能性错误和性能错误。7) 常见的评估指标包括F1分数、召回率、精确率和准确率，$k$-折交叉验证常用于模型评估。8) 很多研究缺乏稳健的统计检验。我们还指出了六个有前景的未来研究方向，为实践者提供了有价值的见解。', 'title_zh': '学习软件 bug 报告：一项系统文献综述'}
{'arxiv_id': 'arXiv:2507.04395', 'title': 'SpiritRAG: A Q&A System for Religion and Spirituality in the United Nations Archive', 'authors': 'Yingqiang Gao, Fabian Winiger, Patrick Montjourides, Anastassia Shaitarova, Nianlong Gu, Simon Peng-Keller, Gerold Schneider', 'link': 'https://arxiv.org/abs/2507.04395', 'abstract': 'Religion and spirituality (R/S) are complex and highly domain-dependent concepts which have long confounded researchers and policymakers. Due to their context-specificity, R/S are difficult to operationalize in conventional archival search strategies, particularly when datasets are very large, poorly accessible, and marked by information noise. As a result, considerable time investments and specialist knowledge is often needed to extract actionable insights related to R/S from general archival sources, increasing reliance on published literature and manual desk reviews. To address this challenge, we present SpiritRAG, an interactive Question Answering (Q&A) system based on Retrieval-Augmented Generation (RAG). Built using 7,500 United Nations (UN) resolution documents related to R/S in the domains of health and education, SpiritRAG allows researchers and policymakers to conduct complex, context-sensitive database searches of very large datasets using an easily accessible, chat-based web interface. SpiritRAG is lightweight to deploy and leverages both UN documents and user provided documents as source material. A pilot test and evaluation with domain experts on 100 manually composed questions demonstrates the practical value and usefulness of SpiritRAG.', 'abstract_zh': '宗教与灵性（R/S）是复杂且高度领域依赖的概念，长久以来困扰着研究人员和政策制定者。由于其情境特异性，R/S在常规档案检索策略中难以操作化，尤其是在数据集非常庞大、访问受限且充满信息噪声的情况下。因此，往往需要大量时间和专家知识来从通用档案资源中提取与R/S相关的可操作洞见，增加了对已出版文献和手工桌面审查的依赖。为应对这一挑战，我们提出了基于检索增强生成（RAG）的交互式问答系统SpiritRAG。该系统基于7,500份与健康和教育领域宗教与灵性相关的联合国决议文件构建，允许研究人员和政策制定者通过易于访问的基于聊天的网络界面进行复杂的情境敏感数据库搜索。SpiritRAG轻量级且易于部署，利用联合国文件和用户提供的文件作为源材料。在100个手动编制的问题上进行的试点测试和专家评估表明，SpiritRAG具有实际价值和实用性。', 'title_zh': 'SpiritRAG：联合国档案中宗教与灵性领域的问答系统'}
{'arxiv_id': 'arXiv:2507.04385', 'title': 'Tractable Representation Learning with Probabilistic Circuits', 'authors': 'Steven Braun, Sahil Sidheekh, Antonio Vergari, Martin Mundt, Sriraam Natarajan, Kristian Kersting', 'link': 'https://arxiv.org/abs/2507.04385', 'abstract': 'Probabilistic circuits (PCs) are powerful probabilistic models that enable exact and tractable inference, making them highly suitable for probabilistic reasoning and inference tasks. While dominant in neural networks, representation learning with PCs remains underexplored, with prior approaches relying on external neural embeddings or activation-based encodings. To address this gap, we introduce autoencoding probabilistic circuits (APCs), a novel framework leveraging the tractability of PCs to model probabilistic embeddings explicitly. APCs extend PCs by jointly modeling data and embeddings, obtaining embedding representations through tractable probabilistic inference. The PC encoder allows the framework to natively handle arbitrary missing data and is seamlessly integrated with a neural decoder in a hybrid, end-to-end trainable architecture enabled by differentiable sampling. Our empirical evaluation demonstrates that APCs outperform existing PC-based autoencoding methods in reconstruction quality, generate embeddings competitive with, and exhibit superior robustness in handling missing data compared to neural autoencoders. These results highlight APCs as a powerful and flexible representation learning method that exploits the probabilistic inference capabilities of PCs, showing promising directions for robust inference, out-of-distribution detection, and knowledge distillation.', 'abstract_zh': '概率电路（PCs）是强大的概率模型，能够实现精确且可计算的推理，使其非常适合概率推理和推理任务。尽管在神经网络中占据主导地位，但使用PCs进行表示学习仍处于起步阶段，此前的方法依赖于外部神经嵌入或基于激活的编码。为解决这一问题，我们引入了自编码概率电路（APCs），这是一种新颖的框架，利用概率电路的计算性来显式建模概率嵌入。APCs通过联合建模数据和嵌入，通过可计算的概率推理获得嵌入表示。PC编码器使框架能够原生处理任意缺失数据，并通过可微采样在混合的端到端可训练架构中无缝集成神经解码器。我们的实证评估表明，APCs在重构质量上优于现有的基于PCs的自编码方法，生成的嵌入与神经自编码器具有竞争力，并且在处理缺失数据的鲁棒性方面表现出更优越的性能。这些结果突显了APCs作为一种强大且灵活的表示学习方法的角色，它利用了概率电路的推理能力，并为稳健推理、异常检测和知识蒸馏等方向提供了有前景的方向。', 'title_zh': '可计算的概率电路中的可计算表示学习'}
{'arxiv_id': 'arXiv:2507.04352', 'title': 'AI-washing: The Asymmetric Effects of Its Two Types on Consumer Moral Judgments', 'authors': 'Greg Nyilasy, Harsha Gangadharbatla', 'link': 'https://arxiv.org/abs/2507.04352', 'abstract': "As AI hype continues to grow, organizations face pressure to broadcast or downplay purported AI initiatives - even when contrary to truth. This paper introduces AI-washing as overstating (deceptive boasting) or understating (deceptive denial) a company's real AI usage. A 2x2 experiment (N = 401) examines how these false claims affect consumer attitudes and purchase intentions. Results reveal a pronounced asymmetry: deceptive denial evokes more negative moral judgments than honest negation, while deceptive boasting has no effects. We show that perceived betrayal mediates these outcomes. By clarifying how AI-washing erodes trust, the study highlights clear ethical implications for policymakers, marketers, and researchers striving for transparency.", 'abstract_zh': '随着AI hype的不断增长，组织面临宣传或淡化所谓AI倡议的压力——即使这与事实相反。本文介绍了AI-washing，即夸大（欺骗性吹嘘）或缩小（欺骗性否认）公司实际AI使用情况。一项2x2实验（N=401）探讨了这些虚假声明如何影响消费者态度和购买意向。结果揭示了一个显著的不对称性：欺骗性否认比诚实否认引发更强烈的负面道德判断，而欺骗性吹嘘则没有影响。我们展示了感知到的背叛在这些结果中起中介作用。通过阐明AI-washing如何损害信任，该研究强调了透明度追求者——政策制定者、营销人员和研究人员——的明确伦理影响。', 'title_zh': 'AI清洗：两种类型对其消费者道德判断的不对称影响'}
{'arxiv_id': 'arXiv:2507.04300', 'title': 'QF: Quick Feedforward AI Model Training without Gradient Back Propagation', 'authors': 'Feng Qi', 'link': 'https://arxiv.org/abs/2507.04300', 'abstract': 'We propose Quick Feedforward (QF) Learning, a novel knowledge consolidation framework for transformer-based models that enables efficient transfer of instruction derived knowledge into model weights through feedforward activations without any gradient back propagation. Unlike traditional finetuning, QF updates are computed in closed form, require minimal parameter modification, and preserve prior knowledge. Importantly, QF allows models to train and infer within the same runtime environment, making the process more resource efficient and closely aligned with how the human brain operates. Code and models are open sourced on GitHub. I hope QF Learning inspires a more efficient and brain-like paradigm for AI systems.', 'abstract_zh': '快速前馈学习：transformer基模型的一种新型知识 consolidation框架', 'title_zh': 'QF：无需梯度反向传播的快速前向AI模型训练'}
{'arxiv_id': 'arXiv:2507.04275', 'title': 'VOLTRON: Detecting Unknown Malware Using Graph-Based Zero-Shot Learning', 'authors': 'M. Tahir Akdeniz, Zeynep Yeşilkaya, İ. Enes Köse, İ. Ulaş Ünal, Sevil Şen', 'link': 'https://arxiv.org/abs/2507.04275', 'abstract': 'The persistent threat of Android malware presents a serious challenge to the security of millions of users globally. While many machine learning-based methods have been developed to detect these threats, their reliance on large labeled datasets limits their effectiveness against emerging, previously unseen malware families, for which labeled data is scarce or nonexistent.\nTo address this challenge, we introduce a novel zero-shot learning framework that combines Variational Graph Auto-Encoders (VGAE) with Siamese Neural Networks (SNN) to identify malware without needing prior examples of specific malware families. Our approach leverages graph-based representations of Android applications, enabling the model to detect subtle structural differences between benign and malicious software, even in the absence of labeled data for new threats.\nExperimental results show that our method outperforms the state-of-the-art MaMaDroid, especially in zero-day malware detection. Our model achieves 96.24% accuracy and 95.20% recall for unknown malware families, highlighting its robustness against evolving Android threats.', 'abstract_zh': '持久存在的Android恶意软件威胁对全球亿万用户的安全构成了严重挑战。虽然已经开发出许多基于机器学习的方法来检测这些威胁，但它们依赖于大型标注数据集，这限制了它们对新兴的、之前未见过的恶意软件家族的有效性，这类恶意软件的标注数据稀缺或不存在。\n为应对这一挑战，我们提出了一种新颖的零样本学习框架，该框架结合了变分图自编码器（VGAE）和双面神经网络（SNN），以无需特定恶意软件家族的先例即可识别恶意软件。该方法利用Android应用程序的图基表示，使模型能够在缺乏新威胁标注数据的情况下，检测良性软件和恶意软件之间的细微结构差异。\n实验结果表明，我们的方法在未知恶意软件家族的检测方面优于最先进的MaMaDroid，特别是在零日恶意软件检测方面表现优异。我们的模型对未知恶意软件家族的准确率和召回率分别为96.24%和95.20%，突显了其对抗演化的Android威胁的稳定性。', 'title_zh': 'VOLTRON: 使用图为基础的零样本学习检测未知恶意软件'}
{'arxiv_id': 'arXiv:2507.04252', 'title': 'Deep-Learning-Assisted Highly-Accurate COVID-19 Diagnosis on Lung Computed Tomography Images', 'authors': 'Yinuo Wang, Juhyun Bae, Ka Ho Chow, Shenyang Chen, Shreyash Gupta', 'link': 'https://arxiv.org/abs/2507.04252', 'abstract': 'COVID-19 is a severe and acute viral disease that can cause symptoms consistent with pneumonia in which inflammation is caused in the alveolous regions of the lungs leading to a build-up of fluid and breathing difficulties. Thus, the diagnosis of COVID using CT scans has been effective in assisting with RT-PCR diagnosis and severity classifications. In this paper, we proposed a new data quality control pipeline to refine the quality of CT images based on GAN and sliding windows. Also, we use class-sensitive cost functions including Label Distribution Aware Loss(LDAM Loss) and Class-balanced(CB) Loss to solve the long-tail problem existing in datasets. Our model reaches more than 0.983 MCC in the benchmark test dataset.', 'abstract_zh': 'COVID-19是一种严重的急性病毒疾病，可导致与肺炎一致的症状，炎症发生在肺泡区域，导致液体积聚和呼吸困难。因此，使用CT扫描诊断COVID-19已有效辅助RT-PCR诊断和病情分类。本文提出了一种基于GAN和滑动窗口的新数据质量控制管道，以 refinement CT图像的质量。同时，我们使用包括标签分布感知损失(LDAM Loss)和类别平衡(CB)损失在内的类敏感成本函数来解决数据集中存在的长尾问题。在基准测试数据集中，我们的模型达到了超过0.983的MCC。', 'title_zh': '深度学习辅助high精度肺癌计算机断层扫描图像COVID-19诊断'}
{'arxiv_id': 'arXiv:2507.04239', 'title': 'Scaling Context Requires Rethinking Attention', 'authors': 'Carles Gelada, Jacob Buckman, Sean Zhang, Txus Bach', 'link': 'https://arxiv.org/abs/2507.04239', 'abstract': 'We argue that neither transformers nor sub-quadratic architectures are well suited to training at long sequence lengths: the cost of processing the context is too expensive in the former, too inexpensive in the latter. Approaches such as sliding window attention which reduce the cost-per-token of a transformer impair in-context learning, and so are also unsuitable. To address these limitations, we introduce power attention, an architectural layer for linear-cost sequence modeling whose state size can be adjusted independently of parameters, unlocking the advantages of linear attention on practical domains. We develop and open-source a set of GPU kernels for efficient power attention, identifying a novel pattern of operation fusion to avoid memory and bandwidth bottlenecks. Our experiments on the in-context learning of power attention shows that these models dominate both exponential attention and linear attention at long-context training.', 'abstract_zh': '我们argue认为，transformers和次二次复杂度的架构都不适合在长序列长度下进行训练：前者处理上下文的成本太高，后者则太低。诸如滑动窗口注意力等方法虽然能够降低每词处理成本，但会损害上下文学习，因此也不合适。为了解决这些问题，我们引入了幂次注意力，这是一种具有线性成本的序列建模架构层，其状态大小可以独立于参数调整，从而在实际应用中充分发挥线性注意力的优势。我们开发并开源了一套高效幂次注意力的GPU内核，并发现了一种新的操作融合模式来避免内存和带宽瓶颈。我们的实验表明，在长上下文训练中，幂次注意力模型在上下文学习方面优于指数注意力和线性注意力。', 'title_zh': '扩大上下文需要重新思考注意力机制。'}
{'arxiv_id': 'arXiv:2507.04230', 'title': 'High-Resolution Sustain Pedal Depth Estimation from Piano Audio Across Room Acoustics', 'authors': 'Kun Fang, Hanwen Zhang, Ziyu Wang, Ichiro Fujinaga', 'link': 'https://arxiv.org/abs/2507.04230', 'abstract': 'Piano sustain pedal detection has previously been approached as a binary on/off classification task, limiting its application in real-world piano performance scenarios where pedal depth significantly influences musical expression. This paper presents a novel approach for high-resolution estimation that predicts continuous pedal depth values. We introduce a Transformer-based architecture that not only matches state-of-the-art performance on the traditional binary classification task but also achieves high accuracy in continuous pedal depth estimation. Furthermore, by estimating continuous values, our model provides musically meaningful predictions for sustain pedal usage, whereas baseline models struggle to capture such nuanced expressions with their binary detection approach. Additionally, this paper investigates the influence of room acoustics on sustain pedal estimation using a synthetic dataset that includes varied acoustic conditions. We train our model with different combinations of room settings and test it in an unseen new environment using a "leave-one-out" approach. Our findings show that the two baseline models and ours are not robust to unseen room conditions. Statistical analysis further confirms that reverberation influences model predictions and introduces an overestimation bias.', 'abstract_zh': '基于变压器的高分辨率踏板深度估计方法及室声学影响研究', 'title_zh': '高分辨率钢琴踏板深度估计跨房间声学环境中的钢琴音频'}
{'arxiv_id': 'arXiv:2507.04227', 'title': 'Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties', 'authors': 'Guohong Liu, Jialei Ye, Jiacheng Liu, Yuanchun Li, Wei Liu, Pengzhi Gao, Jian Luan, Yunxin Liu', 'link': 'https://arxiv.org/abs/2507.04227', 'abstract': 'Mobile GUI agents are designed to autonomously execute diverse device-control tasks by interpreting and interacting with mobile screens. Despite notable advancements, their resilience in real-world scenarios where screen content may be partially manipulated by untrustworthy third parties remains largely unexplored. Owing to their black-box and autonomous nature, these agents are vulnerable to manipulations that could compromise user devices. In this work, we present the first systematic investigation into the vulnerabilities of mobile GUI agents. We introduce a scalable attack simulation framework AgentHazard, which enables flexible and targeted modifications of screen content within existing applications. Leveraging this framework, we develop a comprehensive benchmark suite comprising both a dynamic task execution environment and a static dataset of vision-language-action tuples, totaling over 3,000 attack scenarios. The dynamic environment encompasses 58 reproducible tasks in an emulator with various types of hazardous UI content, while the static dataset is constructed from 210 screenshots collected from 14 popular commercial apps. Importantly, our content modifications are designed to be feasible for unprivileged third parties. We evaluate 7 widely-used mobile GUI agents and 5 common backbone models using our benchmark. Our findings reveal that all examined agents are significantly influenced by misleading third-party content (with an average misleading rate of 28.8% in human-crafted attack scenarios) and that their vulnerabilities are closely linked to the employed perception modalities and backbone LLMs. Furthermore, we assess training-based mitigation strategies, highlighting both the challenges and opportunities for enhancing the robustness of mobile GUI agents. Our code and data will be released at this https URL.', 'abstract_zh': '移动GUI代理的漏洞系统性研究：AgentHazard框架下的攻击模拟与基准测试', 'title_zh': '劫持JARVIS：评估移动GUI代理软件对无特权第三方的安全性'}
{'arxiv_id': 'arXiv:2507.04225', 'title': 'Zero-Shot Cyclic Peptide Design with Composable Geometric Conditions', 'authors': 'Dapeng Jiang, Xiangzhe Kong, Jiaqi Han, Mingyu Li, Rui Jiao, Wenbing Huang, Stefano Ermon, Jianzhu Ma, Yang Liu', 'link': 'https://arxiv.org/abs/2507.04225', 'abstract': 'Cyclic peptides, characterized by geometric constraints absent in linear peptides, offer enhanced biochemical properties, presenting new opportunities to address unmet medical needs. However, designing target-specific cyclic peptides remains underexplored due to limited training data. To bridge the gap, we propose CP-Composer, a novel generative framework that enables zero-shot cyclic peptide generation via composable geometric constraints. Our approach decomposes complex cyclization patterns into unit constraints, which are incorporated into a diffusion model through geometric conditioning on nodes and edges. During training, the model learns from unit constraints and their random combinations in linear peptides, while at inference, novel constraint combinations required for cyclization are imposed as input. Experiments show that our model, despite trained with linear peptides, is capable of generating diverse target-binding cyclic peptides, reaching success rates from 38% to 84% on different cyclization strategies.', 'abstract_zh': '周期肽因其几何约束不同于线性肽，展现出增强的生物化学性能，为满足未满足的医疗需求提供了新的机会。然而，由于训练数据有限，设计特定靶标的周期肽仍是一个未充分探索的领域。为克服这一挑战，我们提出了一种新颖的生成框架CP-Composer，它可以利用组合几何约束实现零样本周期肽的生成。我们的方法将复杂的环化模式分解为单元约束，并通过节点和边的几何条件将这些单元约束整合到扩散模型中。在训练过程中，模型从线性肽中单元约束及其随机组合中学习，而在推理过程中，为环化所需的新型约束组合被用作输入。实验结果显示，尽管模型仅使用线性肽进行训练，仍能够生成多种目标结合的周期肽，不同环化策略的成功率范围从38%到84%。', 'title_zh': '基于可组合几何条件的零样本环肽设计'}
{'arxiv_id': 'arXiv:2507.04194', 'title': 'Mixed-Sample SGD: an End-to-end Analysis of Supervised Transfer Learning', 'authors': 'Yuyang Deng, Samory Kpotufe', 'link': 'https://arxiv.org/abs/2507.04194', 'abstract': 'Theoretical works on supervised transfer learning (STL) -- where the learner has access to labeled samples from both source and target distributions -- have for the most part focused on statistical aspects of the problem, while efficient optimization has received less attention. We consider the problem of designing an SGD procedure for STL that alternates sampling between source and target data, while maintaining statistical transfer guarantees without prior knowledge of the quality of the source data. A main algorithmic difficulty is in understanding how to design such an adaptive sub-sampling mechanism at each SGD step, to automatically gain from the source when it is informative, or bias towards the target and avoid negative transfer when the source is less informative.\nWe show that, such a mixed-sample SGD procedure is feasible for general prediction tasks with convex losses, rooted in tracking an abstract sequence of constrained convex programs that serve to maintain the desired transfer guarantees.\nWe instantiate these results in the concrete setting of linear regression with square loss, and show that the procedure converges, with $1/\\sqrt{T}$ rate, to a solution whose statistical performance on the target is adaptive to the a priori unknown quality of the source. Experiments with synthetic and real datasets support the theory.', 'abstract_zh': '监督迁移学习的理论研究——统计方面的工作占据了主导地位，而高效的优化方法则相对较少受到关注', 'title_zh': '混合样本SGD：监督迁移学习的端到端分析'}
{'arxiv_id': 'arXiv:2507.04189', 'title': 'SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding', 'authors': 'Runcong Zhao, Qinglin Zhu, Hainiu Xu, Bin Liang, Yulan He, Lin Gui', 'link': 'https://arxiv.org/abs/2507.04189', 'abstract': 'Understanding character relationships is essential for interpreting complex narratives and conducting socially grounded AI research. However, manual annotation is time-consuming and low in coverage, while large language models (LLMs) often produce hallucinated or logically inconsistent outputs. We present SymbolicThought, a human-in-the-loop framework that combines LLM-based extraction with symbolic reasoning. The system constructs editable character relationship graphs, refines them using seven types of logical constraints, and enables real-time validation and conflict resolution through an interactive interface. To support logical supervision and explainable social analysis, we release a dataset of 160 interpersonal relationships with corresponding logical structures. Experiments show that SymbolicThought improves annotation accuracy and consistency while significantly reducing time cost, offering a practical tool for narrative understanding, explainable AI, and LLM evaluation.', 'abstract_zh': '理解角色关系对于解读复杂叙事和开展社会导向的AI研究至关重要。然而，人工标注耗时且覆盖率低，而大型语言模型（LLMs）通常会产生虚假信息或逻辑不一致的输出。我们提出了一种人机结合框架SymbolicThought，该框架结合了基于LLM的提取与符号推理。该系统构建可编辑的角色关系图，通过七种类型的逻辑约束对其进行细化，并通过交互界面实现实时验证和冲突解决。为支持逻辑监督和可解释的社会分析，我们发布了包含160个人际关系及其相应逻辑结构的数据集。实验表明，SymbolicThought在提高标注准确性和一致性的同时，显著减少了时间成本，提供了一个用于叙事理解、可解释AI和LLM评估的实用工具。', 'title_zh': '符号思维：将语言模型与符号推理结合以实现一致和可解释的人际关系理解'}
{'arxiv_id': 'arXiv:2507.04175', 'title': 'Uncertainty Quantification in the Tsetlin Machine', 'authors': 'Runar Helin, Ole-Christoffer Granmo, Mayur Kishor Shende, Lei Jiao, Vladimir I. Zadorozhny, Kunal Ganesh Dumbre, Rishad Shafik, Alex Yakovlev', 'link': 'https://arxiv.org/abs/2507.04175', 'abstract': 'Data modeling using Tsetlin machines (TMs) is all about building logical rules from the data features. The decisions of the model are based on a combination of these logical rules. Hence, the model is fully transparent and it is possible to get explanations of its predictions. In this paper, we present a probability score for TM predictions and develop new techniques for uncertainty quantification to increase the explainability further. The probability score is an inherent property of any TM variant and is derived through an analysis of the TM learning dynamics. Simulated data is used to show a clear connection between the learned TM probability scores and the underlying probabilities of the data. A visualization of the probability scores also reveals that the TM is less confident in its predictions outside the training data domain, which contrasts the typical extrapolation phenomenon found in Artificial Neural Networks. The paper concludes with an application of the uncertainty quantification techniques on an image classification task using the CIFAR-10 dataset, where they provide new insights and suggest possible improvements to current TM image classification models.', 'abstract_zh': '基于Tsetlin机的数据建模涉及从数据特征中构建逻辑规则。模型的决策基于这些逻辑规则的组合。因此，该模型完全透明，可以解释其预测。本文为此，我们提出了一种Tsetlin机预测的概率分数，并开发了新的不确定性量化技术以进一步提高可解释性。概率分数是任何Tsetlin机变体的固有属性，通过分析Tsetlin机学习动力学获得。我们使用模拟数据来明确地展示了所学的Tsetlin机概率分数与数据底层概率之间的联系。概率分数的可视化还表明，Tsetlin机在其训练数据域之外的预测不太有信心，这与人工神经网络中典型的外推现象形成对比。最后，我们提出了一种不确定性量化技术在CIFAR-10图像分类任务中的应用，这些技术提供了新的见解并建议了当前Tsetlin机图像分类模型的可能改进。', 'title_zh': 'Tsetlin机中的不确定性量化'}
{'arxiv_id': 'arXiv:2507.04164', 'title': 'Structure As Search: Unsupervised Permutation Learning for Combinatorial Optimization', 'authors': 'Yimeng Min, Carla P. Gomes', 'link': 'https://arxiv.org/abs/2507.04164', 'abstract': 'We propose a non-autoregressive framework for the Travelling Salesman Problem where solutions emerge directly from learned permutations without explicit search. By applying a similarity transformation to Hamiltonian cycles, the model learns to approximate permutation matrices via continuous relaxations. Our unsupervised approach achieves competitive performance against classical heuristics, demonstrating that the inherent structure of the problem can effectively guide combinatorial optimization without sequential decision-making.', 'abstract_zh': '我们提出了一种非自回归框架来解决旅行商问题，其中解决方案直接从学习到的排列中涌现出来，无需显式的搜索。通过将哈密顿循环应用相似变换，模型通过连续松弛学习近似排列矩阵。我们的无监督方法在与经典启发式方法的竞争中表现出色，证明了问题固有的结构可以有效引导组合优化，而无需序列决策。', 'title_zh': '结构即搜索：无监督排列学习在组合优化中的应用'}
{'arxiv_id': 'arXiv:2507.04153', 'title': 'Physics-informed neural networks and neural operators for a study of EUV electromagnetic wave diffraction from a lithography mask', 'authors': "Vasiliy A. Es'kin, Egor V. Ivanov", 'link': 'https://arxiv.org/abs/2507.04153', 'abstract': 'Physics-informed neural networks (PINNs) and neural operators (NOs) for solving the problem of diffraction of Extreme Ultraviolet (EUV) electromagnetic waves from a mask are presented. A novel hybrid Waveguide Neural Operator (WGNO) is introduced, which is based on a waveguide method with its most computationally expensive part replaced by a neural network. Numerical experiments on realistic 2D and 3D masks show that the WGNO achieves state-of-the-art accuracy and inference time, providing a highly efficient solution for accelerating the design workflows of lithography masks.', 'abstract_zh': '基于物理的神经网络（PINNs）和神经算子（NOs）在解决极端紫外线（EUV）电磁波从掩模衍射问题中的应用：一种新颖的波导神经算子（WGNO）的引入及其在 realistic 2D 和 3D 掩模上的数值实验表明，WGNO 达到了最先进的准确性和推理时间，为光刻掩模的设计工作流程加速提供了高效解决方案。', 'title_zh': '基于物理的神经网络和神经运算子研究极紫外线电磁波透过掩膜后的衍射现象'}
{'arxiv_id': 'arXiv:2507.04119', 'title': 'When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need', 'authors': 'Ziming Hong, Runnan Chen, Zengmao Wang, Bo Han, Bo Du, Tongliang Liu', 'link': 'https://arxiv.org/abs/2507.04119', 'abstract': "Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access the real in-distribution (ID) data. Its common solution is to use a generator to synthesize fake data and use them as a substitute for real ID data. However, existing works typically assume teachers are trustworthy, leaving the robustness and security of DFKD from untrusted teachers largely unexplored. In this work, we conduct the first investigation into distilling non-transferable learning (NTL) teachers using DFKD, where the transferability from an ID domain to an out-of-distribution (OOD) domain is prohibited. We find that NTL teachers fool DFKD through divert the generator's attention from the useful ID knowledge to the misleading OOD knowledge. This hinders ID knowledge transfer but prioritizes OOD knowledge transfer. To mitigate this issue, we propose Adversarial Trap Escaping (ATEsc) to benefit DFKD by identifying and filtering out OOD-like synthetic samples. Specifically, inspired by the evidence that NTL teachers show stronger adversarial robustness on OOD samples than ID samples, we split synthetic samples into two groups according to their robustness. The fragile group is treated as ID-like data and used for normal knowledge distillation, while the robust group is seen as OOD-like data and utilized for forgetting OOD knowledge. Extensive experiments demonstrate the effectiveness of ATEsc for improving DFKD against NTL teachers. Code is released at this https URL.", 'abstract_zh': '基于DFKD的非转移性学习教师知识蒸馏探究', 'title_zh': '当无数据知识精炼遇到非迁移性教师时：摆脱分布外陷阱即为所需'}
{'arxiv_id': 'arXiv:2507.04106', 'title': 'Addressing The Devastating Effects Of Single-Task Data Poisoning In Exemplar-Free Continual Learning', 'authors': 'Stanisław Pawlak, Bartłomiej Twardowski, Tomasz Trzciński, Joost van de Weijer', 'link': 'https://arxiv.org/abs/2507.04106', 'abstract': 'Our research addresses the overlooked security concerns related to data poisoning in continual learning (CL). Data poisoning - the intentional manipulation of training data to affect the predictions of machine learning models - was recently shown to be a threat to CL training stability. While existing literature predominantly addresses scenario-dependent attacks, we propose to focus on a more simple and realistic single-task poison (STP) threats. In contrast to previously proposed poisoning settings, in STP adversaries lack knowledge and access to the model, as well as to both previous and future tasks. During an attack, they only have access to the current task within the data stream. Our study demonstrates that even within these stringent conditions, adversaries can compromise model performance using standard image corruptions. We show that STP attacks are able to strongly disrupt the whole continual training process: decreasing both the stability (its performance on past tasks) and plasticity (capacity to adapt to new tasks) of the algorithm. Finally, we propose a high-level defense framework for CL along with a poison task detection method based on task vectors. The code is available at this https URL .', 'abstract_zh': '我们的研究关注连续学习中数据投毒的未被重视的安全问题。数据投毒——通过故意操纵训练数据从而影响机器学习模型的预测——最近被证明是对连续学习训练稳定性的威胁。尽管现有文献主要关注场景依赖性攻击，我们建议关注一种更加简单且现实的单一任务投毒（STP）威胁。与以往提出的投毒设置不同，在STP攻击中，攻击者缺乏对模型以及此前和未来任务的知识和访问权限，仅在攻击期间可访问数据流中的当前任务。我们的研究证明，在这些严格的条件下，攻击者仍然可以通过标准图像污染手段损害模型性能。我们表明，STP攻击能够强烈扰乱整个连续学习过程：减少了算法在以往任务上的稳定性和对新任务的可塑性（适应能力）。最后，我们提出了一种针对连续学习的高层防御框架，并提出了基于任务向量的任务投毒检测方法。代码可在以下链接获取。', 'title_zh': '无示例限制的连续学习中单一任务数据中毒的破坏性影响应对策略'}
{'arxiv_id': 'arXiv:2507.04100', 'title': 'Hierarchical Testing with Rabbit Optimization for Industrial Cyber-Physical Systems', 'authors': 'Jinwei Hu, Zezhi Tang, Xin Jin, Benyuan Zhang, Yi Dong, Xiaowei Huang', 'link': 'https://arxiv.org/abs/2507.04100', 'abstract': "This paper presents HERO (Hierarchical Testing with Rabbit Optimization), a novel black-box adversarial testing framework for evaluating the robustness of deep learning-based Prognostics and Health Management systems in Industrial Cyber-Physical Systems. Leveraging Artificial Rabbit Optimization, HERO generates physically constrained adversarial examples that align with real-world data distributions via global and local perspective. Its generalizability ensures applicability across diverse ICPS scenarios. This study specifically focuses on the Proton Exchange Membrane Fuel Cell system, chosen for its highly dynamic operational conditions, complex degradation mechanisms, and increasing integration into ICPS as a sustainable and efficient energy solution. Experimental results highlight HERO's ability to uncover vulnerabilities in even state-of-the-art PHM models, underscoring the critical need for enhanced robustness in real-world applications. By addressing these challenges, HERO demonstrates its potential to advance more resilient PHM systems across a wide range of ICPS domains.", 'abstract_zh': 'HERO（层级测试结合兔优化算法）：工业 cyber-物理系统中基于深度学习的 prognostics 和健康管理系统稳健性评估的新型黑盒对抗性测试框架', 'title_zh': '基于兔子优化的工业 cyber-物理系统分层测试'}
{'arxiv_id': 'arXiv:2507.04094', 'title': 'MMMOS: Multi-domain Multi-axis Audio Quality Assessment', 'authors': 'Yi-Cheng Lin, Jia-Hung Chen, Hung-yi Lee', 'link': 'https://arxiv.org/abs/2507.04094', 'abstract': "Accurate audio quality estimation is essential for developing and evaluating audio generation, retrieval, and enhancement systems. Existing non-intrusive assessment models predict a single Mean Opinion Score (MOS) for speech, merging diverse perceptual factors and failing to generalize beyond speech. We propose MMMOS, a no-reference, multi-domain audio quality assessment system that estimates four orthogonal axes: Production Quality, Production Complexity, Content Enjoyment, and Content Usefulness across speech, music, and environmental sounds. MMMOS fuses frame-level embeddings from three pretrained encoders (WavLM, MuQ, and M2D) and evaluates three aggregation strategies with four loss functions. By ensembling the top eight models, MMMOS shows a 20-30% reduction in mean squared error and a 4-5% increase in Kendall's {\\tau} versus baseline, gains first place in six of eight Production Complexity metrics, and ranks among the top three on 17 of 32 challenge metrics.", 'abstract_zh': '准确的音频质量估计对于开发和评估音频生成、检索和增强系统至关重要。现有的非侵入性评估模型为语音预测单一的意见分数（MOS），合并了多种感知因素，并且无法泛化到非语音领域。我们提出MMMOS，这是一种无参考的多领域音频质量评估系统，估计四个正交轴：生产质量、生产复杂度、内容愉悦性和内容实用性，覆盖语音、音乐和环境声音。MMMOS 结合了三个预训练编码器（WavLM、MuQ 和 M2D）的帧级嵌入，并评估了三种聚合策略和四种损失函数。通过集成前八名模型，MMMOS 在均方误差上减少了 20-30%，在肯德尔 τ 上提高了 4-5%，在六个生产复杂度指标中排名第一，在 32 项挑战指标中的 17 项中排名前三。', 'title_zh': '多领域多轴音频质量评估'}
{'arxiv_id': 'arXiv:2507.04062', 'title': 'Stochastic Human Motion Prediction with Memory of Action Transition and Action Characteristic', 'authors': 'Jianwei Tang, Hong Yang, Tengyue Chen, Jian-Fang Hu', 'link': 'https://arxiv.org/abs/2507.04062', 'abstract': 'Action-driven stochastic human motion prediction aims to generate future motion sequences of a pre-defined target action based on given past observed sequences performing non-target actions. This task primarily presents two challenges. Firstly, generating smooth transition motions is hard due to the varying transition speeds of different actions. Secondly, the action characteristic is difficult to be learned because of the similarity of some actions. These issues cause the predicted results to be unreasonable and inconsistent. As a result, we propose two memory banks, the Soft-transition Action Bank (STAB) and Action Characteristic Bank (ACB), to tackle the problems above. The STAB stores the action transition information. It is equipped with the novel soft searching approach, which encourages the model to focus on multiple possible action categories of observed motions. The ACB records action characteristic, which produces more prior information for predicting certain actions. To fuse the features retrieved from the two banks better, we further propose the Adaptive Attention Adjustment (AAA) strategy. Extensive experiments on four motion prediction datasets demonstrate that our approach consistently outperforms the previous state-of-the-art. The demo and code are available at this https URL.', 'abstract_zh': '基于动作驱动的随机人体运动预测旨在根据给定的执行非目标动作的过去观察序列，生成预定义目标动作的未来运动序列。该任务主要面临两个挑战。首先，由于不同动作的过渡速度不同，生成平滑的过渡动作困难。其次，由于某些动作的相似性，动作特征难以学习。这些问题导致预测结果不合理且不一致。因此，我们提出了两种记忆库，软过渡动作库（STAB）和动作特性库（ACB），以解决上述问题。STAB存储动作过渡信息，并配备了新颖的软搜索方法，促使模型关注观察动作的多种可能动作类别。ACB记录动作特性，为预测特定动作提供更多的先验信息。为进一步更好地融合来自两个库的特征，我们还提出了自适应注意力调整（AAA）策略。在四个运动预测数据集上的广泛实验表明，我们的方法始终优于之前的最先进的方法。演示和代码可在以下网址获得。', 'title_zh': '基于动作转换记忆和动作特征的随机人体运动预测'}
{'arxiv_id': 'arXiv:2507.04060', 'title': 'Temporal Continual Learning with Prior Compensation for Human Motion Prediction', 'authors': 'Jianwei Tang, Jiangxin Sun, Xiaotong Lin, Lifang Zhang, Wei-Shi Zheng, Jian-Fang Hu', 'link': 'https://arxiv.org/abs/2507.04060', 'abstract': 'Human Motion Prediction (HMP) aims to predict future poses at different moments according to past motion sequences. Previous approaches have treated the prediction of various moments equally, resulting in two main limitations: the learning of short-term predictions is hindered by the focus on long-term predictions, and the incorporation of prior information from past predictions into subsequent predictions is limited. In this paper, we introduce a novel multi-stage training framework called Temporal Continual Learning (TCL) to address the above challenges. To better preserve prior information, we introduce the Prior Compensation Factor (PCF). We incorporate it into the model training to compensate for the lost prior information. Furthermore, we derive a more reasonable optimization objective through theoretical derivation. It is important to note that our TCL framework can be easily integrated with different HMP backbone models and adapted to various datasets and applications. Extensive experiments on four HMP benchmark datasets demonstrate the effectiveness and flexibility of TCL. The code is available at this https URL.', 'abstract_zh': '人类动作预测（HMP）旨在根据过去的动作序列预测不同时刻的未来姿态。以往的方法将不同时刻的预测视为等价的，导致了两个主要限制：短期预测的学习受到了长期预测重点的阻碍，先前预测中的先验信息向后续预测的融入有限。本文提出了一种新的多阶段训练框架——Temporal Continual Learning (TCL)，以应对上述挑战。为了更好地保留先验信息，引入了先验补偿因子（PCF），将其融入模型训练以补偿丢失的先验信息。此外，通过理论推导获得了更合理的优化目标。值得注意的是，我们的TCL框架可以轻松集成到不同的HMP骨干模型中，并适应各种数据集和应用场景。在四个HMP基准数据集上的广泛实验演示了TCL的有效性和灵活性。代码已开源。', 'title_zh': '具有先验补偿的时空连续学习人体运动预测'}
{'arxiv_id': 'arXiv:2507.04059', 'title': 'Attributing Data for Sharpness-Aware Minimization', 'authors': 'Chenyang Ren, Yifan Jia, Huanyi Xie, Zhaobin Xu, Tianxing Wei, Liangyu Wang, Lijie Hu, Di Wang', 'link': 'https://arxiv.org/abs/2507.04059', 'abstract': 'Sharpness-aware Minimization (SAM) improves generalization in large-scale model training by linking loss landscape geometry to generalization. However, challenges such as mislabeled noisy data and privacy concerns have emerged as significant issues. Data attribution, which identifies the contributions of specific training samples, offers a promising solution. However, directly rendering existing data influence evaluation tools such as influence functions (IF) to SAM will be inapplicable or inaccurate as SAM utilizes an inner loop to find model perturbations that maximize loss, which the outer loop then minimizes, resulting in a doubled computational structure. Additionally, this bilevel structure complicates the modeling of data influence on the parameters. In this paper, based on the IF, we develop two innovative data valuation methods for SAM, each offering unique benefits in different scenarios: the Hessian-based IF and the Gradient Trajectory-based IF. The first one provides a comprehensive estimation of data influence using a closed-form measure that relies only on the trained model weights. In contrast, the other IF for SAM utilizes gradient trajectory information during training for more accurate and efficient data assessment. Extensive experiments demonstrate their effectiveness in data evaluation and parameter tuning, with applications in identifying mislabeled data, model editing, and enhancing interpretability.', 'abstract_zh': 'Sharpness-aware Minimization (SAM)通过将损失景观点几何与泛化能力联系起来从而在大规模模型训练中提高泛化能力，但mislabelled噪声数据和隐私问题等挑战已变得尤为突出。数据归属，即识别特定训练样本的贡献，提供了一个有前途的解决方案。然而，将现有的数据影响评估工具如影响函数（IF）直接应用于SAM是不适用或不准确的，因为SAM利用内部循环寻找使损失最大化的模型扰动，外部循环再最小化这些扰动，这导致了两层的计算结构，同时也增加了数据分析对参数影响建模的复杂性。基于IF，本文开发了两种创新的数据估值方法用于SAM，每种方法在不同的场景下各有独特优势：Hessian基于的IF和梯度轨迹基于的IF。前者通过仅依赖训练后的模型权重的封闭形式度量提供全面的数据影响估计；后者则利用训练期间的梯度轨迹信息进行更准确和高效的数据评估。广泛实验表明，这两种方法在数据评估和参数调优中具有有效性，并应用于识别错误标签数据、模型编辑和增强可解释性。', 'title_zh': 'Sharpness-Aware Minimization的数据贡献分析'}
{'arxiv_id': 'arXiv:2507.04055', 'title': 'Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG', 'authors': 'Yufan Chen, Daoyuan Wu, Juantao Zhong, Zicheng Zhang, Debin Gao, Shuai Wang, Yingjiu Li, Ning Liu', 'link': 'https://arxiv.org/abs/2507.04055', 'abstract': 'Malware Family Classification (MFC) aims to identify the fine-grained family (e.g., GuLoader or BitRAT) to which a potential malware sample belongs, in contrast to malware detection or sample classification that predicts only an Yes/No. Accurate family identification can greatly facilitate automated sample labeling and understanding on crowdsourced malware analysis platforms such as VirusTotal and MalwareBazaar, which generate vast amounts of data daily. In this paper, we explore and assess the feasibility of using traditional binary string features for MFC in the new era of large language models (LLMs) and Retrieval-Augmented Generation (RAG). Specifically, we investigate how Family-Specific String (FSS) features could be utilized in a manner similar to RAG to facilitate MFC. To this end, we develop a curated evaluation framework covering 4,347 samples from 67 malware families, extract and analyze over 25 million strings, and conduct detailed ablation studies to assess the impact of different design choices in four major modules.', 'abstract_zh': '恶意软件家族分类（MFC）旨在识别一个潜在的恶意软件样本属于哪个细粒度家族（例如GuLoader或BitRAT），而不仅仅是预测Yes/No。准确的家族识别可以大大促进在VirusTotal和MalwareBazaar等众包恶意软件分析平台上自动样本标签化和理解，这些平台每天生成大量数据。在本文中，我们探讨并评估了在大规模语言模型（LLMs）和检索增强生成（RAG）的新时代，使用传统二进制字符串特征进行MFC的可能性。具体而言，我们研究了如何利用家族特定字符串（FSS）特征，使其类似RAG的方式促进MFC。为此，我们开发了一个包含来自67个恶意软件家族的4,347个样本的定制评估框架，提取并分析了超过2500万个字符串，并进行了详细的消融研究以评估四大模块中不同设计选择的影响。', 'title_zh': '重思和探索大规模语言模型和 RETRIEVE-THEN-GENERATE 架构背景下基于字符串的恶意软件家族分类'}
{'arxiv_id': 'arXiv:2507.04053', 'title': 'TopoMAS: Large Language Model Driven Topological Materials Multiagent System', 'authors': 'Baohua Zhang, Xin Li, Huangchao Xu, Zhong Jin, Quansheng Wu, Ce Li', 'link': 'https://arxiv.org/abs/2507.04053', 'abstract': "Topological materials occupy a frontier in condensed-matter physics thanks to their remarkable electronic and quantum properties, yet their cross-scale design remains bottlenecked by inefficient discovery workflows. Here, we introduce TopoMAS (Topological materials Multi-Agent System), an interactive human-AI framework that seamlessly orchestrates the entire materials-discovery pipeline: from user-defined queries and multi-source data retrieval, through theoretical inference and crystal-structure generation, to first-principles validation. Crucially, TopoMAS closes the loop by autonomously integrating computational outcomes into a dynamic knowledge graph, enabling continuous knowledge refinement. In collaboration with human experts, it has already guided the identification of novel topological phases SrSbO3, confirmed by first-principles calculations. Comprehensive benchmarks demonstrate robust adaptability across base Large Language Model, with the lightweight Qwen2.5-72B model achieving 94.55% accuracy while consuming only 74.3-78.4% of tokens required by Qwen3-235B and 83.0% of DeepSeek-V3's usage--delivering responses twice as fast as Qwen3-235B. This efficiency establishes TopoMAS as an accelerator for computation-driven discovery pipelines. By harmonizing rational agent orchestration with a self-evolving knowledge graph, our framework not only delivers immediate advances in topological materials but also establishes a transferable, extensible paradigm for materials-science domain.", 'abstract_zh': '拓扑材料因其独特的电子和量子性质，在凝聚态物理学中占据前沿地位，然而其跨尺度设计仍受限于低效的发现工作流程。在这里，我们引入了拓扑材料多智能体系统（TopoMAS），一个交互式的人工智能框架，无缝 orchestrates 整个材料发现流程：从用户定义的查询和多源数据检索，到理论推断和晶体结构生成，最后进行第一性原理验证。至关重要的是，TopoMAS 自动将计算结果整合到动态知识图谱中，形成持续的知识精炼。与人类专家合作，它已指导识别出新型拓扑相 SrSbO3，由第一性原理计算确认。全面的基准测试表明，该框架在基本大型语言模型中表现出高度的适应性，采用轻量级的 Qwen2.5-72B 模型实现了 94.55% 的准确性，仅消耗 Qwen3-235B 和 DeepSeek-V3 所需令牌的 74.3-78.4% 和 83.0%——速度比 Qwen3-235B 快两倍。这种效率使 TopoMAS 成为驱动计算导向的发现管道的加速器。通过合理智能体协调与自我进化知识图谱的和谐统一，我们的框架不仅在拓扑材料领域立即实现了进展，还为材料科学领域建立了可转移和可扩展的范式。', 'title_zh': 'TopoMAS：由大规模语言模型驱动的拓扑材料多智能体系统'}
{'arxiv_id': 'arXiv:2507.04050', 'title': 'Predictive Modeling of Effluent Temperature in SAT Systems Using Ambient Meteorological Data: Implications for Infiltration Management', 'authors': 'Roy Elkayam', 'link': 'https://arxiv.org/abs/2507.04050', 'abstract': 'Accurate prediction of effluent temperature in recharge basins is essential for optimizing the Soil Aquifer Treatment (SAT) process, as temperature directly influences water viscosity and infiltration rates. This study develops and evaluates predictive models for effluent temperature in the upper recharge layer of a Shafdan SAT system recharge basin using ambient meteorological data. Multiple linear regression (MLR), neural networks (NN), and random forests (RF) were tested for their predictive accuracy and interpretability. The MLR model, preferred for its operational simplicity and robust performance, achieved high predictive accuracy (R2 = 0.86-0.87) and was used to estimate effluent temperatures over a 10-year period. Results highlight pronounced seasonal temperature cycles and the importance of topsoil temperature in governing the thermal profile of the infiltrating effluent. The study provides practical equations for real-time monitoring and long-term planning of SAT operations.', 'abstract_zh': '准确预测砂石渗滤系统 recharge basin 中溢流水温对于优化土壤渗滤处理（SAT）过程至关重要，因为水温直接影响水的黏度和渗透率。本研究使用环境气象数据开发并评估了用于预测 Shafdan SAT 系统 recharge basin 上层渗滤层溢流水温的预测模型。测试了多元线性回归（MLR）、神经网络（NN）和随机森林（RF）模型的预测准确性和可解释性。MLR 模型因其操作简单和稳健性而被首选，实现了高预测准确度（R2 = 0.86-0.87），并用于估算10年内的溢流水温。研究结果强调了明显的季节性温度周期以及表层土温对入渗溢流水温度热特征的控制作用。本研究提供了实际方程，用于 SAT 运行的实时监控和长期规划。', 'title_zh': '基于环境气象数据的SAT系统排放温度预测模型：渗漏管理的 implications'}
{'arxiv_id': 'arXiv:2507.04038', 'title': 'T-SYNTH: A Knowledge-Based Dataset of Synthetic Breast Images', 'authors': 'Christopher Wiedeman, Anastasiia Sarmakeeva, Elena Sizikova, Daniil Filienko, Miguel Lago, Jana G. Delfino, Aldo Badano', 'link': 'https://arxiv.org/abs/2507.04038', 'abstract': 'One of the key impediments for developing and assessing robust medical imaging algorithms is limited access to large-scale datasets with suitable annotations. Synthetic data generated with plausible physical and biological constraints may address some of these data limitations. We propose the use of physics simulations to generate synthetic images with pixel-level segmentation annotations, which are notoriously difficult to obtain. Specifically, we apply this approach to breast imaging analysis and release T-SYNTH, a large-scale open-source dataset of paired 2D digital mammography (DM) and 3D digital breast tomosynthesis (DBT) images. Our initial experimental results indicate that T-SYNTH images show promise for augmenting limited real patient datasets for detection tasks in DM and DBT. Our data and code are publicly available at this https URL.', 'abstract_zh': '生成具有像素级分割注释的合成医学影像数据以克服大規模标注数据受限的瓶颈：T-SYNTH数据集在乳腺影像分析中的应用', 'title_zh': 'T-SYNTH：基于知识的合成乳腺图像数据集'}
{'arxiv_id': 'arXiv:2507.04000', 'title': 'Leveraging Multimodal Data and Side Users for Diffusion Cross-Domain Recommendation', 'authors': 'Fan Zhang, Jinpeng Chen, Huan Li, Senzhang Wang, Yuan Cao, Kaimin Wei, JianXiang He, Feifei Kou, Jinqing Wang', 'link': 'https://arxiv.org/abs/2507.04000', 'abstract': "Cross-domain recommendation (CDR) aims to address the persistent cold-start problem in Recommender Systems. Current CDR research concentrates on transferring cold-start users' information from the auxiliary domain to the target domain. However, these systems face two main issues: the underutilization of multimodal data, which hinders effective cross-domain alignment, and the neglect of side users who interact solely within the target domain, leading to inadequate learning of the target domain's vector space distribution. To address these issues, we propose a model leveraging Multimodal data and Side users for diffusion Cross-domain recommendation (MuSiC). We first employ a multimodal large language model to extract item multimodal features and leverage a large language model to uncover user features using prompt learning without fine-tuning. Secondly, we propose the cross-domain diffusion module to learn the generation of feature vectors in the target domain. This approach involves learning feature distribution from side users and understanding the patterns in cross-domain transformation through overlapping users. Subsequently, the trained diffusion module is used to generate feature vectors for cold-start users in the target domain, enabling the completion of cross-domain recommendation tasks. Finally, our experimental evaluation of the Amazon dataset confirms that MuSiC achieves state-of-the-art performance, significantly outperforming all selected baselines. Our code is available: this https URL.", 'abstract_zh': '跨域推荐（CDR）旨在解决推荐系统中的持续冷启动问题。当前的跨域推荐研究主要集中在从辅助域向目标域转移冷启动用户的信息。然而，这些系统面临两个主要问题：多模态数据的利用不足，这阻碍了有效的跨域对齐，以及忽视仅在目标域内交互的旁用户，导致对目标域向量空间分布的学习不足。为了解决这些问题，我们提出了一种利用多模态数据和旁用户的扩散跨域推荐模型（MuSiC）。我们首先使用多模态大型语言模型提取项目多模态特征，并利用大型语言模型通过提示学习发现用户特征，无需微调。其次，我们提出跨域扩散模块来学习目标域中特征向量的生成。该方法通过交错用户学习特征分布，并通过跨域转换理解模式。随后，训练好的扩散模块用于生成目标域冷启动用户的特征向量，从而完成跨域推荐任务。最后，我们在亚马逊数据集上的实验评估证实MuSiC达到了最先进的性能，显著优于所有选定的基线。相关代码可在以下链接获取：this https URL。', 'title_zh': '利用多模态数据和侧用户进行扩散跨域推荐'}
{'arxiv_id': 'arXiv:2507.03971', 'title': 'Real-TabPFN: Improving Tabular Foundation Models via Continued Pre-training With Real-World Data', 'authors': 'Anurag Garg, Muhammad Ali, Noah Hollmann, Lennart Purucker, Samuel Müller, Frank Hutter', 'link': 'https://arxiv.org/abs/2507.03971', 'abstract': 'Foundation models for tabular data, like TabPFN, achieve strong performance on small datasets when pre-trained solely on synthetic data. We show that this performance can be significantly boosted by a targeted continued pre-training phase. Specifically, we demonstrate that leveraging a small, curated collection of large, real-world datasets for continued pre-training yields superior downstream predictive accuracy compared to using broader, potentially noisier corpora like CommonCrawl or GitTables. Our resulting model, Real-TabPFN, achieves substantial performance gains on 29 datasets from the OpenML AutoML Benchmark.', 'abstract_zh': '基于合成数据预训练的表格式数据基础模型，如TabPFN，在继续使用精心策划的大规模真实世界数据集进行微调后，能在小数据集上实现显著的性能提升。我们的Real-TabPFN模型在OpenML AutoML基准测试的29个数据集上取得了实质性的性能提升。', 'title_zh': '实数据继续预训练提升表单基础模型：Real-TabPFN研究'}
{'arxiv_id': 'arXiv:2507.03953', 'title': 'Evaluating Adversarial Protections for Diffusion Personalization: A Comprehensive Study', 'authors': 'Kai Ye, Tianyi Chen, Zhen Wang', 'link': 'https://arxiv.org/abs/2507.03953', 'abstract': 'With the increasing adoption of diffusion models for image generation and personalization, concerns regarding privacy breaches and content misuse have become more pressing. In this study, we conduct a comprehensive comparison of eight perturbation based protection methods: AdvDM, ASPL, FSGM, MetaCloak, Mist, PhotoGuard, SDS, and SimAC--across both portrait and artwork domains. These methods are evaluated under varying perturbation budgets, using a range of metrics to assess visual imperceptibility and protective efficacy. Our results offer practical guidance for method selection. Code is available at: this https URL.', 'abstract_zh': '随着扩散模型在图像生成和个人化应用中的日益普及，隐私泄露和内容滥用的问题变得更为紧迫。本研究对八种基于扰动的保护方法——AdvDM、ASPL、FSGM、MetaCloak、Mist、PhotoGuard、SDS和SimAC——在肖像和艺术作品领域进行了全面比较。这些方法在不同的扰动预算下进行评估，并使用多种指标来评估视觉不可感知性和保护效力。研究结果提供了方法选择的实际指导。代码可在以下链接获取：this https URL。', 'title_zh': '评估对抗保护在扩散个性化中的效果：一项全面研究'}
{'arxiv_id': 'arXiv:2507.03950', 'title': 'Optimizing Age of Trust and Throughput in Multi-Hop UAV-Aided IoT Networks', 'authors': 'Yizhou Luo, Kwan-Wu Chin, Ruyi Guan, Xi Xiao, Caimeng Wang, Jingyin Feng, Tengjiao He', 'link': 'https://arxiv.org/abs/2507.03950', 'abstract': "Devices operating in Internet of Things (IoT) networks may be deployed across vast geographical areas and interconnected via multi-hop communications. Further, they may be unguarded. This makes them vulnerable to attacks and motivates operators to check on devices frequently. To this end, we propose and study an Unmanned Aerial Vehicle (UAV)-aided attestation framework for use in IoT networks with a charging station powered by solar. A key challenge is optimizing the trajectory of the UAV to ensure it attests as many devices as possible. A trade-off here is that devices being checked by the UAV are offline, which affects the amount of data delivered to a gateway. Another challenge is that the charging station experiences time-varying energy arrivals, which in turn affect the flight duration and charging schedule of the UAV. To address these challenges, we employ a Deep Reinforcement Learning (DRL) solution to optimize the UAV's charging schedule and the selection of devices to be attested during each flight. The simulation results show that our solution reduces the average age of trust by 88% and throughput loss due to attestation by 30%.", 'abstract_zh': '基于太阳能充电站的无人机辅助物联网网络验证框架及其优化研究', 'title_zh': '优化多跳UAV辅助物联网网络中的信任年龄和吞吐量'}
{'arxiv_id': 'arXiv:2507.03899', 'title': "Transformer Model for Alzheimer's Disease Progression Prediction Using Longitudinal Visit Sequences", 'authors': 'Mahdi Moghaddami, Clayton Schubring, Mohammad-Reza Siadat', 'link': 'https://arxiv.org/abs/2507.03899', 'abstract': "Alzheimer's disease (AD) is a neurodegenerative disorder with no known cure that affects tens of millions of people worldwide. Early detection of AD is critical for timely intervention to halt or slow the progression of the disease. In this study, we propose a Transformer model for predicting the stage of AD progression at a subject's next clinical visit using features from a sequence of visits extracted from the subject's visit history. We also rigorously compare our model to recurrent neural networks (RNNs) such as long short-term memory (LSTM), gated recurrent unit (GRU), and minimalRNN and assess their performances based on factors such as the length of prior visits and data imbalance. We test the importance of different feature categories and visit history, as well as compare the model to a newer Transformer-based model optimized for time series. Our model demonstrates strong predictive performance despite missing visits and missing features in available visits, particularly in identifying converter subjects -- individuals transitioning to more severe disease stages -- an area that has posed significant challenges in longitudinal prediction. The results highlight the model's potential in enhancing early diagnosis and patient outcomes.", 'abstract_zh': '阿尔茨海默病（AD）是一种无法治愈的神经退行性疾病，全球有数千万人受到其影响。早期检测AD对于及时干预以阻止或减缓疾病进展至关重要。在本研究中，我们提出了一种Transformer模型，用于预测患者下次临床访视时的AD进展阶段，该模型利用从访视历史中提取的访视序列特征。我们还严格比较了该模型与长短期记忆网络（LSTM）、门控循环单元（GRU）和最小RNN等循环神经网络（RNNs）的性能，并根据先前访视长度和数据不平衡等因素评估其表现。我们测试了不同特征类别和访视历史的重要性，并将该模型与一种针对时间序列优化的最新Transformer模型进行了比较。尽管存在缺失访视和可用访视中的特征缺失，我们的模型仍表现出强大的预测性能，特别是在识别转换者（即向更严重疾病阶段过渡的个体）方面表现出色，这为纵向预测带来了重大挑战。研究结果凸显了该模型在早期诊断和改善患者结果方面的潜力。', 'title_zh': '基于纵向访视序列的Transformer模型在阿尔茨海默病进展预测中的应用'}
{'arxiv_id': 'arXiv:2507.03895', 'title': 'TayFCS: Towards Light Feature Combination Selection for Deep Recommender Systems', 'authors': 'Xianquan Wang, Zhaocheng Du, Jieming Zhu, Chuhan Wu, Qinglin Jia, Zhenhua Dong', 'link': 'https://arxiv.org/abs/2507.03895', 'abstract': "Feature interaction modeling is crucial for deep recommendation models. A common and effective approach is to construct explicit feature combinations to enhance model performance. However, in practice, only a small fraction of these combinations are truly informative. Thus it is essential to select useful feature combinations to reduce noise and manage memory consumption. While feature selection methods have been extensively studied, they are typically limited to selecting individual features. Extending these methods for high-order feature combination selection presents a significant challenge due to the exponential growth in time complexity when evaluating feature combinations one by one. In this paper, we propose $\\textbf{TayFCS}$, a lightweight feature combination selection method that significantly improves model performance. Specifically, we propose the Taylor Expansion Scorer (TayScorer) module for field-wise Taylor expansion on the base model. Instead of evaluating all potential feature combinations' importance by repeatedly running experiments with feature adding and removal, this scorer only needs to approximate the importance based on their sub-components' gradients. This can be simply computed with one backward pass based on a trained recommendation model. To further reduce information redundancy among feature combinations and their sub-components, we introduce Logistic Regression Elimination (LRE), which estimates the corresponding information gain based on the model prediction performance. Experimental results on three benchmark datasets validate both the effectiveness and efficiency of our approach. Furthermore, online A/B test results demonstrate its practical applicability and commercial value.", 'abstract_zh': '特征组合选择对于深度推荐模型至关重要。一种常见且有效的方法是构造显式的特征组合以提升模型性能。然而，在实践中，只有少量的组合真正具有信息性。因此，选择有用的特征组合以降低噪声和管理内存消耗是必要的。尽管特征选择方法已经被广泛研究，但它们通常仅限于选择单个特征。由于逐个评估特征组合的重要性的计算时间呈指数级增长，因此将这些方法扩展到高阶特征组合选择是一项重大挑战。在本文中，我们提出了一种轻量级的特征组合选择方法 $\\textbf{TayFCS}$，显著提升了模型性能。具体地，我们提出了场-wise泰勒展开评分模块（TayScorer）来基于基模型进行场-wise泰勒展开。该评分器不需要通过反复运行添加和移除特征的实验来评估所有潜在的特征组合的重要性，而只需基于其子组件的梯度进行重要性的近似估计。这可以通过在训练好的推荐模型基础上进行一次反向传播计算得出。为了进一步减少特征组合及其子组件之间的信息冗余，我们引入了逻辑回归消除（LRE），该方法基于模型预测性能估计相应的信息增益。实验结果在三个基准数据集上验证了该方法的有效性和效率。此外，线上 A/B 测试结果展示了其实际适用性和商业价值。', 'title_zh': 'TayFCS: 向量轻特征组合选择助力深度推荐系统'}
{'arxiv_id': 'arXiv:2507.03863', 'title': 'Enhanced accuracy through ensembling of randomly initialized auto-regressive models for time-dependent PDEs', 'authors': 'Ishan Khurjekar, Indrashish Saha, Lori Graham-Brady, Somdatta Goswami', 'link': 'https://arxiv.org/abs/2507.03863', 'abstract': "Systems governed by partial differential equations (PDEs) require computationally intensive numerical solvers to predict spatiotemporal field evolution. While machine learning (ML) surrogates offer faster solutions, autoregressive inference with ML models suffer from error accumulation over successive predictions, limiting their long-term accuracy. We propose a deep ensemble framework to address this challenge, where multiple ML surrogate models with random weight initializations are trained in parallel and aggregated during inference. This approach leverages the diversity of model predictions to mitigate error propagation while retaining the autoregressive strategies ability to capture the system's time dependent relations. We validate the framework on three PDE-driven dynamical systems - stress evolution in heterogeneous microstructures, Gray-Scott reaction-diffusion, and planetary-scale shallow water system - demonstrating consistent reduction in error accumulation over time compared to individual models. Critically, the method requires only a few time steps as input, enabling full trajectory predictions with inference times significantly faster than numerical solvers. Our results highlight the robustness of ensemble methods in diverse physical systems and their potential as efficient and accurate alternatives to traditional solvers. The codes for this work are available on GitHub (this https URL).", 'abstract_zh': '由偏微分方程（PDEs）驱动的系统需要计算 intensive 的数值求解器来预测空间时间场的演化。虽然机器学习（ML）代理模型可以提供更快的解决方案，但基于自回归的ML模型在连续预测中会累积误差，限制了其长期准确性。我们提出了一种深度ensemble框架来应对这一挑战，其中多个具有随机权重初始化的ML代理模型并行训练并在推理时聚合。这种方法利用了模型预测的多样性以减轻误差传播，同时保留了自回归策略捕捉系统时间相关性的能力。我们在三个PDE驱动的动力系统上验证了该框架——异质微观结构中的应力演化、Gray-Scott反应扩散系统以及行星尺度浅水系统——展示了与单个模型相比，该方法在时间上一致地减少了误差累积。至关重要的是，该方法只需要 few时间步长作为输入，从而实现全轨迹预测，且推理时间远快于数值求解器。我们的结果突显了ensemble方法在不同物理系统中的稳健性及其作为传统求解器的高效精确替代方案的潜力。该工作的代码可在GitHub上获取（this https URL）。', 'title_zh': '随机初始化自回归模型ensemble方法提高时间依赖偏微分方程求解精度'}
{'arxiv_id': 'arXiv:2507.03774', 'title': 'Alpay Algebra IV: Symbiotic Semantics and the Fixed-Point Convergence of Observer Embeddings', 'authors': 'Bugra Kilictas, Faruk Alpay', 'link': 'https://arxiv.org/abs/2507.03774', 'abstract': 'We present a theoretical framework in which a document and an AI model engage in a transfinite fixed-point interaction that leads to stable semantic alignment. Building on the foundations of Alpay Algebra, we introduce a functorial system wherein an observer (the AI) and a textual environment (this paper) co-evolve through iterative transformations guided by the phi-infinity operator. This process guarantees the existence of a unique fixed point in the AI\'s embedding space -- a state where the AI\'s internal representation of the content becomes stable, self-consistent, and semantically faithful. We prove that such convergence is mathematically sound, semantically invariant, and permanent, even under perturbation or further context expansion. This fixed point acts as an "empathetic embedding," wherein the AI internalizes not only the meaning of the content but also the author\'s intent. We interpret this as a rigorous, category-theoretic route to alignment at the embedding level, with implications for semantic security, symbolic memory, and the construction of AI systems with persistent self-referential understanding. All references in this paper function as nodes in the Alpay Algebra universe, and this work embeds itself as a new fixed-point node within that transfinite semantic graph.', 'abstract_zh': '我们提出了一种理论框架，其中文档和AI模型进行超限不变点交互，从而实现稳定的语义对齐。基于Alpay代数的基础，我们引入了一个函子系统，其中观察者（AI）和文本环境（本文）通过由phi-∞操作符引导的迭代变换共同进化。这一过程保证了AI嵌入空间中存在唯一的不变点——在该状态下，AI对内容的内部表示变得稳定、自我一致且语义忠实。我们证明了这种收敛在数学上是合理的、语义上是不变的，并且在扰动或进一步的上下文扩展下仍然是持久的。这一不变点作为“同情嵌入”，其中AI不仅内化了内容的意义，还内化了作者的意图。我们认为这为嵌入层面的对齐提供了一条严格的范畴论路径，具有对语义安全、符号记忆以及构建具有持久自我参照理解的AI系统的含义。本文中的所有参考文献在此Alpay代数宇宙中作为节点存在，并且这项工作在此超限语义图中嵌入为一个新的不变点节点。', 'title_zh': 'Alpay代数IV：共生语义与观察者嵌入的不动点收敛'}
{'arxiv_id': 'arXiv:2507.03730', 'title': 'Less is More: Empowering GUI Agent with Context-Aware Simplification', 'authors': 'Gongwei Chen, Xurui Zhou, Rui Shao, Yibo Lyu, Kaiwen Zhou, Shuai Wang, Wentao Li, Yinchuan Li, Zhongang Qi, Liqiang Nie', 'link': 'https://arxiv.org/abs/2507.03730', 'abstract': 'The research focus of GUI agents is shifting from text-dependent to pure-vision-based approaches, which, though promising, prioritize comprehensive pre-training data collection while neglecting contextual modeling challenges. We probe the characteristics of element and history contextual modeling in GUI agent and summarize: 1) the high-density and loose-relation of element context highlight the existence of many unrelated elements and their negative influence; 2) the high redundancy of history context reveals the inefficient history modeling in current GUI agents. In this work, we propose a context-aware simplification framework for building an efficient and effective GUI Agent, termed SimpAgent. To mitigate potential interference from numerous unrelated elements, we introduce a masking-based element pruning method that circumvents the intractable relation modeling through an efficient masking mechanism. To reduce the redundancy in historical information, we devise a consistency-guided history compression module, which enhances implicit LLM-based compression through innovative explicit guidance, achieving an optimal balance between performance and efficiency. With the above components, SimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances. Comprehensive navigation experiments across diverse web and mobile environments demonstrate the effectiveness and potential of our agent.', 'abstract_zh': 'GUI代理的研究重点正从依赖文本的方法转向基于纯视觉的方法，尽管前景广阔，但这些方法更侧重于全面的预训练数据收集，而忽视了上下文建模的挑战。我们探究了GUI代理中元素和历史上下文建模的特点，并总结出：1）元素上下文的高密度和松散关系凸显了许多无关元素的存在及其负面影响；2）历史上下文的高冗余揭示了当前GUI代理中历史建模的低效性。在此基础上，我们提出了一种上下文感知的简化框架，用于构建高效且有效的GUI代理，称为SimpAgent。为了减轻众多无关元素可能产生的干扰，我们引入了一种基于掩码的元素修剪方法，通过高效的掩码机制规避了难以建模的关系。为了减少历史信息的冗余，我们设计了一种一致性导向的历史压缩模块，通过创新的显式指导增强了基于LLM的压缩效果，实现了性能与效率的最优平衡。借助上述组件，SimpAgent减少了27%的FLOPs，并在GUI导航性能上表现出优越性。跨多种互联网和移动环境的全面导航实验验证了我们代理的有效性和潜力。', 'title_zh': 'Less is More: 以情境感知简化赋能GUI代理'}
{'arxiv_id': 'arXiv:2507.03670', 'title': 'Interaction Techniques that Encourage Longer Prompts Can Improve Psychological Ownership when Writing with AI', 'authors': 'Nikhita Joshi, Daniel Vogel', 'link': 'https://arxiv.org/abs/2507.03670', 'abstract': "Writing longer prompts for an AI assistant to generate a short story increases psychological ownership, a user's feeling that the writing belongs to them. To encourage users to write longer prompts, we evaluated two interaction techniques that modify the prompt entry interface of chat-based generative AI assistants: pressing and holding the prompt submission button, and continuously moving a slider up and down when submitting a short prompt. A within-subjects experiment investigated the effects of such techniques on prompt length and psychological ownership, and results showed that these techniques increased prompt length and led to higher psychological ownership than baseline techniques. A second experiment further augmented these techniques by showing AI-generated suggestions for how the prompts could be expanded. This further increased prompt length, but did not lead to improvements in psychological ownership. Our results show that simple interface modifications like these can elicit more writing from users and improve psychological ownership.", 'abstract_zh': '写作更长的提示以增加用户对AI生成短故事的心理拥有感：交互技术的影响与扩展', 'title_zh': '促进更长提示的技术交互可以提高使用AI写作时的心理占有感'}
{'arxiv_id': 'arXiv:2507.03641', 'title': 'Improving Low-Resource Dialect Classification Using Retrieval-based Voice Conversion', 'authors': 'Lea Fischbach, Akbar Karimi, Caroline Kleen, Alfred Lameli, Lucie Flek', 'link': 'https://arxiv.org/abs/2507.03641', 'abstract': 'Deep learning models for dialect identification are often limited by the scarcity of dialectal data. To address this challenge, we propose to use Retrieval-based Voice Conversion (RVC) as an effective data augmentation method for a low-resource German dialect classification task. By converting audio samples to a uniform target speaker, RVC minimizes speaker-related variability, enabling models to focus on dialect-specific linguistic and phonetic features. Our experiments demonstrate that RVC enhances classification performance when utilized as a standalone augmentation method. Furthermore, combining RVC with other augmentation methods such as frequency masking and segment removal leads to additional performance gains, highlighting its potential for improving dialect classification in low-resource scenarios.', 'abstract_zh': '基于检索的语音转换（RVC）在低资源德语方言分类中的数据增强应用', 'title_zh': '使用基于检索的语音转换提高低资源方言分类'}
{'arxiv_id': 'arXiv:2507.03633', 'title': 'From Video to EEG: Adapting Joint Embedding Predictive Architecture to Uncover Visual Concepts in Brain Signal Analysis', 'authors': 'Amir Hojjati, Lu Li, Ibrahim Hameed, Anis Yazidi, Pedro G. Lind, Rabindra Khadka', 'link': 'https://arxiv.org/abs/2507.03633', 'abstract': 'EEG signals capture brain activity with high temporal and low spatial resolution, supporting applications such as neurological diagnosis, cognitive monitoring, and brain-computer interfaces. However, effective analysis is hindered by limited labeled data, high dimensionality, and the absence of scalable models that fully capture spatiotemporal dependencies. Existing self-supervised learning (SSL) methods often focus on either spatial or temporal features, leading to suboptimal representations. To this end, we propose EEG-VJEPA, a novel adaptation of the Video Joint Embedding Predictive Architecture (V-JEPA) for EEG classification. By treating EEG as video-like sequences, EEG-VJEPA learns semantically meaningful spatiotemporal representations using joint embeddings and adaptive masking. To our knowledge, this is the first work that exploits V-JEPA for EEG classification and explores the visual concepts learned by the model. Evaluations on the publicly available Temple University Hospital (TUH) Abnormal EEG dataset show that EEG-VJEPA outperforms existing state-of-the-art models in classification this http URL classification accuracy, EEG-VJEPA captures physiologically relevant spatial and temporal signal patterns, offering interpretable embeddings that may support human-AI collaboration in diagnostic workflows. These findings position EEG-VJEPA as a promising framework for scalable, trustworthy EEG analysis in real-world clinical settings.', 'abstract_zh': 'EEG-VJEPA：一种用于EEG分类的新型视频联合嵌入预测架构', 'title_zh': '从视频到EEG：适应性联合嵌入预测架构在脑信号分析中揭示视觉概念'}
{'arxiv_id': 'arXiv:2507.03622', 'title': 'Disentangling Doubt in Deep Causal AI', 'authors': 'Cooper Doyle', 'link': 'https://arxiv.org/abs/2507.03622', 'abstract': 'Accurate individual treatment-effect estimation in high-stakes applications demands both reliable point predictions and interpretable uncertainty quantification. We propose a factorized Monte Carlo Dropout framework for deep twin-network models that splits total predictive variance into representation uncertainty (sigma_rep) in the shared encoder and prediction uncertainty (sigma_pred) in the outcome heads. Across three synthetic covariate-shift regimes, our intervals are well-calibrated (ECE < 0.03) and satisfy sigma_rep^2 + sigma_pred^2 ~ sigma_tot^2. Additionally, we observe a crossover: head uncertainty leads on in-distribution data, but representation uncertainty dominates under shift. Finally, on a real-world twins cohort with induced multivariate shifts, only sigma_rep spikes on out-of-distribution samples (delta sigma ~ 0.0002) and becomes the primary error predictor (rho_rep <= 0.89), while sigma_pred remains flat. This module-level decomposition offers a practical diagnostic for detecting and interpreting uncertainty sources in deep causal-effect models.', 'abstract_zh': '高风险应用中精确的个体治疗效果估计需要可靠的点预测和可解释的不确定性量化。我们提出了一种因子化的蒙特卡洛 Dropout 框架用于深度双网络模型，将总预测不确定性分解为共享编码器中的表示不确定性（sigma_rep）和结果头部中的预测不确定性（sigma_pred）。在三个合成协变量偏移 regimes 中，我们的区间良好校准（ECE < 0.03）且满足 sigma_rep^2 + sigma_pred^2 ~ sigma_tot^2。此外，我们观察到一个交叉：头部不确定性在分布内数据中占主导，但在偏移情况下表示不确定性占主导。最后，在具有诱导多变量偏移的现实世界双胞胎队列中，仅在分布外样本中 sigma_rep 上升（delta sigma ~ 0.0002），成为主要的误差预测器（rho_rep <= 0.89），而 sigma_pred 保持稳定。这种模块级分解提供了检测和解释深度因果效应模型中不确定性来源的实际诊断工具。', 'title_zh': '拆解深度因果AI中的不确定性'}
{'arxiv_id': 'arXiv:2507.03612', 'title': 'Multi-Hop Reasoning for Question Answering with Hyperbolic Representations', 'authors': 'Simon Welz, Lucie Flek, Akbar Karimi', 'link': 'https://arxiv.org/abs/2507.03612', 'abstract': 'Hyperbolic representations are effective in modeling knowledge graph data which is prevalently used to facilitate multi-hop reasoning. However, a rigorous and detailed comparison of the two spaces for this task is lacking. In this paper, through a simple integration of hyperbolic representations with an encoder-decoder model, we perform a controlled and comprehensive set of experiments to compare the capacity of hyperbolic space versus Euclidean space in multi-hop reasoning. Our results show that the former consistently outperforms the latter across a diverse set of datasets. In addition, through an ablation study, we show that a learnable curvature initialized with the delta hyperbolicity of the utilized data yields superior results to random initializations. Furthermore, our findings suggest that hyperbolic representations can be significantly more advantageous when the datasets exhibit a more hierarchical structure.', 'abstract_zh': '双曲表示在多跳推理中 modeling 知识图谱数据方面效果显著，但两种空间在这方面的严格详细对比缺乏。通过将双曲表示与编码-解码模型简单集成，本文执行了一组受控且全面的实验，以比较双曲空间与欧几里得空间在多跳推理中的容量。结果显示，前者在多种数据集中一致优于后者。此外，通过消融研究，我们发现初始化可学习曲率与所用数据的δ双曲性相匹配，可以取得优于随机初始化的效果。进一步的研究发现，当数据集展示出更明显的层次结构时，双曲表示可以显著更有优势。', 'title_zh': '基于双曲表示的多跳推理问答'}
{'arxiv_id': 'arXiv:2507.03599', 'title': 'MusGO: A Community-Driven Framework For Assessing Openness in Music-Generative AI', 'authors': 'Roser Batlle-Roca, Laura Ibáñez-Martínez, Xavier Serra, Emilia Gómez, Martín Rocamora', 'link': 'https://arxiv.org/abs/2507.03599', 'abstract': "Since 2023, generative AI has rapidly advanced in the music domain. Despite significant technological advancements, music-generative models raise critical ethical challenges, including a lack of transparency and accountability, along with risks such as the replication of artists' works, which highlights the importance of fostering openness. With upcoming regulations such as the EU AI Act encouraging open models, many generative models are being released labelled as 'open'. However, the definition of an open model remains widely debated. In this article, we adapt a recently proposed evidence-based framework for assessing openness in LLMs to the music domain. Using feedback from a survey of 110 participants from the Music Information Retrieval (MIR) community, we refine the framework into MusGO (Music-Generative Open AI), which comprises 13 openness categories: 8 essential and 5 desirable. We evaluate 16 state-of-the-art generative models and provide an openness leaderboard that is fully open to public scrutiny and community contributions. Through this work, we aim to clarify the concept of openness in music-generative AI and promote its transparent and responsible development.", 'abstract_zh': '自2023年以来，生成式AI在音乐领域迅速发展。尽管取得了重大的技术进步，但音乐生成模型引发了重要的伦理挑战，包括透明度和问责制的缺乏，以及艺术家作品复制的风险，突显了促进开放性的重要性。随着欧盟AI法案等即将出台的法规鼓励开放模型，许多生成模型被标记为“开放”。然而，开放模型的定义仍存在广泛争议。本文采用一个最近提出的基于证据的框架，将其应用于音乐领域，通过音乐信息检索（MIR）社区110名参与者反馈，精炼出MusGO（音乐生成开放AI）框架，包含13个开放性类别：8个必需和5个 desirable。我们评估了16个最先进的生成模型，并提供了一个完全公开接受公众审查和社区贡献的开放性排行榜。通过这项工作，我们旨在澄清音乐生成AI中的开放性概念，并促进其透明和负责任的发展。', 'title_zh': 'MusGO：一个社区驱动的评估音乐生成AI开放性框架'}
{'arxiv_id': 'arXiv:2507.03594', 'title': "RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based Parkinson's Disease Classification", 'authors': 'Terry Yi Zhong, Cristian Tejedor-Garcia, Martha Larson, Bastiaan R. Bloem', 'link': 'https://arxiv.org/abs/2507.03594', 'abstract': "Parkinson's Disease (PD) affects over 10 million people globally, with speech impairments often preceding motor symptoms by years, making speech a valuable modality for early, non-invasive detection. While recent deep-learning models achieve high accuracy, they typically lack the explainability required for clinical use. To address this, we propose RECA-PD, a novel, robust, and explainable cross-attention architecture that combines interpretable speech features with self-supervised representations. RECA-PD matches state-of-the-art performance in Speech-based PD detection while providing explanations that are more consistent and more clinically meaningful. Additionally, we demonstrate that performance degradation in certain speech tasks (e.g., monologue) can be mitigated by segmenting long recordings. Our findings indicate that performance and explainability are not necessarily mutually exclusive. Future work will enhance the usability of explanations for non-experts and explore severity estimation to increase the real-world clinical relevance.", 'abstract_zh': '帕金森病（PD）影响全球超过1000万人，言语障碍往往在运动症状出现前数年就已经存在，这使得言语成为早期非侵入性检测的重要工具。尽管最近的深度学习模型在准确性上表现出色，但它们通常缺乏临床应用所需的可解释性。为解决这一问题，我们提出了RECA-PD，这是一种新颖的、稳健的和可解释的交叉注意架构，结合了可解释的言语特征与自监督表示。RECA-PD 在基于言语的帕金森病检测方面达到了与最新方法相当的性能，同时提供了更为一致且临床意义更大的解释。此外，我们还证明可以通过分割长录音来减轻某些言语任务（如独白）中性能下降的问题。我们的研究结果表明，性能和可解释性并不一定是互斥的。未来的工作将增强非专业人士使用解释的便利性，并探索严重程度估计以提高实际临床相关性。', 'title_zh': "RECA-PD：一种用于 PARKINSON'S 病分类的鲁棒可解释跨注意力方法"}
{'arxiv_id': 'arXiv:2507.03541', 'title': 'Foundation versus Domain-specific Models: Performance Comparison, Fusion, and Explainability in Face Recognition', 'authors': 'Redwan Sony, Parisa Farmanifard, Arun Ross, Anil K. Jain', 'link': 'https://arxiv.org/abs/2507.03541', 'abstract': "In this paper, we address the following question: How do generic foundation models (e.g., CLIP, BLIP, LLaVa, DINO) compare against a domain-specific face recognition model (viz., AdaFace or ArcFace) on the face recognition task? Through a series of experiments involving several foundation models and benchmark datasets, we are able to report the following findings: (a) In all datasets considered, domain-specific models outperformed zero-shot foundation models. (b) The performance of zero-shot generic foundation models improves on over-segmented face images than tightly cropped faces thereby suggesting the importance of contextual clues. For example, at a False Match Rate (FMR) of 0.01%, the True Match Rate (TMR) of OpenCLIP improved from 64.97% to 81.73% on the LFW dataset as the face crop increased from 112x112 to 250x250 while the TMR of domain-specific AdaFace dropped from 99.09% to 77.31%. (c) A simple score-level fusion of a foundation model with a domain-specific FR model improved the accuracy at low FMRs. For example, the TMR of AdaFace when fused with BLIP improved from 72.64% to 83.31% at an FMR of 0.0001% on the IJB-B dataset and from 73.17% to 85.81% on the IJB-C dataset. (d) Foundation models, such as ChatGPT, can be used to impart explainability to the FR pipeline (e.g., ``Despite minor lighting and head tilt differences, the two left-profile images show high consistency in forehead slope, nose shape, chin contour...''). In some instances, foundation models are even able to resolve low-confidence decisions made by AdaFace (e.g., ``Although AdaFace assigns a low similarity score of 0.21, both images exhibit visual similarity...and the pair is likely of the same person''), thereby reiterating the importance of combining domain-specific FR models with generic foundation models in a judicious manner.", 'abstract_zh': '基于基础模型与领域特定面部识别模型在面部识别任务中的比较研究', 'title_zh': '基础模型与领域特定模型：面部识别性能比较、融合及解释性'}
{'arxiv_id': 'arXiv:2507.03528', 'title': 'Generating Synthetic Relational Tabular Data via Structural Causal Models', 'authors': 'Frederik Hoppe, Astrid Franz, Lars Kleinemeier, Udo Göbel', 'link': 'https://arxiv.org/abs/2507.03528', 'abstract': 'Synthetic tabular data generation has received increasing attention in recent years, particularly with the emergence of foundation models for tabular data. The breakthrough success of TabPFN (Hollmann et al.,2025), which leverages vast quantities of synthetic tabular datasets derived from structural causal models (SCMs), demonstrates the critical role synthetic data plays in developing powerful tabular foundation models. However, most real-world tabular data exists in relational formats spanning multiple interconnected tables - a structure not adequately addressed by current generation methods. In this work, we extend the SCM-based approach by developing a novel framework that generates realistic synthetic relational tabular data including causal relationships across tables. Our experiments confirm that this framework is able to construct relational datasets with complex inter-table dependencies mimicking real-world scenarios.', 'abstract_zh': '合成表格数据生成在最近几年受到了越来越多的关注，尤其是在面向表格数据的基础模型出现之后。基于结构因果模型（SCMs）生成的大规模合成表格数据（Hollmann et al., 2025）的突破性成功，展示了合成数据在开发强大表格基础模型中的关键作用。然而，大多数实际的表格数据以跨越多个相互关联表格的关联格式存在——这种结构目前的生成方法尚未充分解决。在本工作中，我们通过开发一种新的框架扩展了基于SCM的方法，该框架能够生成包含表格间因果关系的现实合成关联表格数据。我们的实验证实，该框架能够构建具有复杂跨表格依赖关系的关联数据集，以模拟现实世界场景。', 'title_zh': '通过结构因果模型生成合成关系型表格数据'}
{'arxiv_id': 'arXiv:2507.03483', 'title': 'BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning Dataset', 'authors': 'Zhiheng Xi, Guanyu Li, Yutao Fan, Honglin Guo, Yufang Liu, Xiaoran Fan, Jiaqi Liu, Jingchao Ding, Wangmeng Zuo, Zhenfei Yin, Lei Bai, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang', 'link': 'https://arxiv.org/abs/2507.03483', 'abstract': "In this paper, we introduce BMMR, a large-scale bilingual, multimodal, multi-disciplinary reasoning dataset for the community to develop and evaluate large multimodal models (LMMs). BMMR comprises 110k college-level questions spanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice, fill-in-the-blank, and open-ended QA-and sourced from both print and digital media such as books, exams, and quizzes. All data are curated and filtered via a human-in-the-loop and scalable framework, and each instance is paired with a high-quality reasoning path. The dataset is organized into two parts: BMMR-Eval that comprises 20,458 high-quality instances to comprehensively assess LMMs' knowledge and reasoning across multiple disciplines in both Chinese and English; and BMMR-Train that contains 88,991 instances to support further research and development, extending the current focus on mathematical reasoning to diverse disciplines and domains. In addition, we propose the process-based multi-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained evaluation of reasoning paths. Extensive experiments on 24 models reveal that (i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom on BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs only on specific subjects; (iii) open-source models still trail their proprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap. Additionally, we conduct reasoning-chain analyses using BMMR-Verifier and other in-depth studies, uncovering the challenges LMMs currently face in multidisciplinary reasoning. We will release the data, and we hope our work can offer insights and contributions to the community.", 'abstract_zh': '基于BMMR的大规模跨模态多学科推理数据集及其评估方法', 'title_zh': 'BMMR：大规模跨模态多学科双语推理数据集'}
{'arxiv_id': 'arXiv:2507.03450', 'title': 'Evaluating the Evaluators: Trust in Adversarial Robustness Tests', 'authors': 'Antonio Emanuele Cinà, Maura Pintor, Luca Demetrio, Ambra Demontis, Battista Biggio, Fabio Roli', 'link': 'https://arxiv.org/abs/2507.03450', 'abstract': 'Despite significant progress in designing powerful adversarial evasion attacks for robustness verification, the evaluation of these methods often remains inconsistent and unreliable. Many assessments rely on mismatched models, unverified implementations, and uneven computational budgets, which can lead to biased results and a false sense of security. Consequently, robustness claims built on such flawed testing protocols may be misleading and give a false sense of security. As a concrete step toward improving evaluation reliability, we present AttackBench, a benchmark framework developed to assess the effectiveness of gradient-based attacks under standardized and reproducible conditions. AttackBench serves as an evaluation tool that ranks existing attack implementations based on a novel optimality metric, which enables researchers and practitioners to identify the most reliable and effective attack for use in subsequent robustness evaluations. The framework enforces consistent testing conditions and enables continuous updates, making it a reliable foundation for robustness verification.', 'abstract_zh': '尽管在设计强大的对抗性规避攻击以进行鲁棒性验证方面取得了显著进展，但这些方法的评估往往仍然不一致且不可靠。许多评估依赖于不匹配的模型、未经验证的实现以及不均衡的计算预算，这可能会导致偏颇的结果和虚假的安全感。因此，基于此类有缺陷的测试协议提出的鲁棒性声明可能是误导性的，并会给人一种虚假的安全感。为进一步提高评估的可靠性，我们提出了一种名为AttackBench的基准框架，用于在标准化和可重现的条件下评估基于梯度的攻击的有效性。AttackBench充当了一个评估工具，根据新型最优性度量对现有的攻击实现进行排名，使研究人员和实践者能够识别出在后续鲁棒性评估中最为可靠和有效的攻击。该框架确保了测试条件的一致性，并允许持续更新，使其成为鲁棒性验证的可靠基础。', 'title_zh': '评估评估者： adversarial 稳定性测试中的信任'}
{'arxiv_id': 'arXiv:2507.03430', 'title': 'Multi-Level Fusion Graph Neural Network for Molecule Property Prediction', 'authors': 'XiaYu Liu, Hou-biao Li, Yang Liu, Chao Fan', 'link': 'https://arxiv.org/abs/2507.03430', 'abstract': 'Accurate molecular property prediction is essential in drug discovery and related fields. However, existing graph neural networks (GNNs) often struggle to simultaneously capture both local and global molecular structures. In this work, we propose a Multi-Level Fusion Graph Neural Network (MLFGNN) that integrates Graph Attention Networks and a novel Graph Transformer to jointly model local and global dependencies. In addition, we incorporate molecular fingerprints as a complementary modality and introduce a mechanism of interaction between attention to adaptively fuse information across representations. Extensive experiments on multiple benchmark datasets demonstrate that MLFGNN consistently outperforms state-of-the-art methods in both classification and regression tasks. Interpretability analysis further reveals that the model effectively captures task-relevant chemical patterns, supporting the usefulness of multi-level and multi-modal fusion in molecular representation learning.', 'abstract_zh': '多级融合图神经网络在药物发现相关领域的分子性质预测中至关重要：MLFGNN在局部和全局依赖性的联合建模中取得了优越性能。', 'title_zh': '多层融合图神经网络在分子性质预测中的应用'}
{'arxiv_id': 'arXiv:2507.03367', 'title': 'Be the Change You Want to See: Revisiting Remote Sensing Change Detection Practices', 'authors': 'Blaž Rolih, Matic Fučka, Filip Wolf, Luka Čehovin Zajc', 'link': 'https://arxiv.org/abs/2507.03367', 'abstract': 'Remote sensing change detection aims to localize semantic changes between images of the same location captured at different times. In the past few years, newer methods have attributed enhanced performance to the additions of new and complex components to existing architectures. Most fail to measure the performance contribution of fundamental design choices such as backbone selection, pre-training strategies, and training configurations. We claim that such fundamental design choices often improve performance even more significantly than the addition of new architectural components. Due to that, we systematically revisit the design space of change detection models and analyse the full potential of a well-optimised baseline. We identify a set of fundamental design choices that benefit both new and existing architectures. Leveraging this insight, we demonstrate that when carefully designed, even an architecturally simple model can match or surpass state-of-the-art performance on six challenging change detection datasets. Our best practices generalise beyond our architecture and also offer performance improvements when applied to related methods, indicating that the space of fundamental design choices has been underexplored. Our guidelines and architecture provide a strong foundation for future methods, emphasizing that optimizing core components is just as important as architectural novelty in advancing change detection performance. Code: this https URL', 'abstract_zh': '遥感变化检测旨在 localization 同一位置在不同时段拍摄的图像之间的语义变化。在过去几年中，新的方法通过向现有架构添加新而复杂的组件来提高性能。大多数方法未能衡量基础设计选择（如主干网络选择、预训练策略和训练配置）对性能的贡献。我们认为，这些基础设计选择往往比添加新架构组件对性能的提升更为显著。因此，我们系统地重新审视了变化检测模型的设计空间，并分析了优化基准模型的全部潜力。我们确定了一组对新旧架构均有益的基础设计选择。利用这一洞察，我们证明，当精心设计时，即使一个架构简单的模型也能在六个具有挑战性的变化检测数据集中达到或超越最先进的性能。我们的最佳实践超越了我们的架构，并且当应用于相关方法时也能提供性能改进，这表明基础设计选择的空间尚未充分利用。我们的指南和架构为未来的方法提供了一个坚实的基础，强调优化核心组件与架构新颖性同样重要，以推动变化检测性能的提升。代码：this https URL', 'title_zh': '欲成所愿之变：重访遥感变化检测实践'}
{'arxiv_id': 'arXiv:2507.03350', 'title': 'Backtesting Sentiment Signals for Trading: Evaluating the Viability of Alpha Generation from Sentiment Analysis', 'authors': 'Elvys Linhares Pontes, Carlos-Emiliano González-Gallardo, Georgeta Bordea, José G. Moreno, Mohamed Ben Jannet, Yuxuan Zhao, Antoine Doucet', 'link': 'https://arxiv.org/abs/2507.03350', 'abstract': 'Sentiment analysis, widely used in product reviews, also impacts financial markets by influencing asset prices through microblogs and news articles. Despite research in sentiment-driven finance, many studies focus on sentence-level classification, overlooking its practical application in trading. This study bridges that gap by evaluating sentiment-based trading strategies for generating positive alpha. We conduct a backtesting analysis using sentiment predictions from three models (two classification and one regression) applied to news articles on Dow Jones 30 stocks, comparing them to the benchmark Buy&Hold strategy. Results show all models produced positive returns, with the regression model achieving the highest return of 50.63% over 28 months, outperforming the benchmark Buy&Hold strategy. This highlights the potential of sentiment in enhancing investment strategies and financial decision-making.', 'abstract_zh': '基于 sentiment 分析的情感分析广泛应用于产品评价，同时也通过微博和新闻文章影响资本市场，从而影响资产价格。尽管在情感驱动的金融研究中已经有了很多成果，但许多研究着重于句子级别的分类，忽视了其在交易中的实际应用。本研究通过评估基于情感的情感交易策略，填补了这一空白，旨在生成正向阿尔法值。我们使用三种模型（两个分类模型和一个回归模型）对道琼斯 30 种股票的新闻文章进行情感预测，并将其与基准持股策略（Buy&Hold）进行对比。结果显示，所有模型均实现了正收益，其中回归模型在这 28 个月内取得了最高回报率 50.63%，超过了基准持股策略。这表明情感在改进投资策略和金融决策方面具有潜在价值。', 'title_zh': '基于情绪信号的交易回测：情绪分析Alpha生成可行性的评估'}
{'arxiv_id': 'arXiv:2507.03334', 'title': 'De-Fake: Style based Anomaly Deepfake Detection', 'authors': 'Sudev Kumar Padhi, Harshit Kumar, Umesh Kashyap, Sk. Subidh Ali', 'link': 'https://arxiv.org/abs/2507.03334', 'abstract': 'Detecting deepfakes involving face-swaps presents a significant challenge, particularly in real-world scenarios where anyone can perform face-swapping with freely available tools and apps without any technical knowledge. Existing deepfake detection methods rely on facial landmarks or inconsistencies in pixel-level features and often struggle with face-swap deepfakes, where the source face is seamlessly blended into the target image or video. The prevalence of face-swap is evident in everyday life, where it is used to spread false information, damage reputations, manipulate political opinions, create non-consensual intimate deepfakes (NCID), and exploit children by enabling the creation of child sexual abuse material (CSAM). Even prominent public figures are not immune to its impact, with numerous deepfakes of them circulating widely across social media platforms. Another challenge faced by deepfake detection methods is the creation of datasets that encompass a wide range of variations, as training models require substantial amounts of data. This raises privacy concerns, particularly regarding the processing and storage of personal facial data, which could lead to unauthorized access or misuse. Our key idea is to identify these style discrepancies to detect face-swapped images effectively without accessing the real facial image. We perform comprehensive evaluations using multiple datasets and face-swapping methods, which showcases the effectiveness of SafeVision in detecting face-swap deepfakes across diverse scenarios. SafeVision offers a reliable and scalable solution for detecting face-swaps in a privacy preserving manner, making it particularly effective in challenging real-world applications. To the best of our knowledge, SafeVision is the first deepfake detection using style features while providing inherent privacy protection.', 'abstract_zh': '检测涉及面部互换的深fake presents a significant challenge, particularly in real-world scenarios where anyone can perform face-swapping with freely available tools and apps without any technical knowledge。', 'title_zh': 'De-假：基于风格的深度伪造异常检测'}
{'arxiv_id': 'arXiv:2507.03331', 'title': 'Task-Specific Generative Dataset Distillation with Difficulty-Guided Sampling', 'authors': 'Mingzhuo Li, Guang Li, Jiafeng Mao, Linfeng Ye, Takahiro Ogawa, Miki Haseyama', 'link': 'https://arxiv.org/abs/2507.03331', 'abstract': 'To alleviate the reliance of deep neural networks on large-scale datasets, dataset distillation aims to generate compact, high-quality synthetic datasets that can achieve comparable performance to the original dataset. The integration of generative models has significantly advanced this field. However, existing approaches primarily focus on aligning the distilled dataset with the original one, often overlooking task-specific information that can be critical for optimal downstream performance. In this paper, focusing on the downstream task of classification, we propose a task-specific sampling strategy for generative dataset distillation that incorporates the concept of difficulty to consider the requirements of the target task better. The final dataset is sampled from a larger image pool with a sampling distribution obtained by matching the difficulty distribution of the original dataset. A logarithmic transformation is applied as a pre-processing step to correct for distributional bias. The results of extensive experiments demonstrate the effectiveness of our method and suggest its potential for enhancing performance on other downstream tasks.', 'abstract_zh': '为了减轻深度神经网络对大规模数据集的依赖，数据集蒸馏旨在生成紧凑的、高质量的合成数据集，以便在性能上与原始数据集相当。结合生成模型极大地推进了这一领域的发展。然而，现有方法主要集中在使蒸馏数据集与原始数据集对齐，往往忽略了对最优下游性能至关重要的任务特定信息。在本文中，针对分类下游任务，我们提出了一种嵌入难度概念的任务特定采样策略，以更好地满足目标任务的需求。最终的数据集是从一个更大的图像池中采样得到的，采样分布是通过匹配原始数据集的难度分布获得的。作为预处理步骤，应用对数变换以纠正分布偏差。大量实验的结果表明了该方法的有效性，并暗示其在增强其他下游任务性能方面的潜力。', 'title_zh': '基于难度引导采样的任务特定生成性数据集蒸馏'}
{'arxiv_id': 'arXiv:2507.03321', 'title': 'Source-Free Domain Adaptation via Multi-view Contrastive Learning', 'authors': 'Amirfarhad Farhadi, Naser Mozayani, Azadeh Zamanifar', 'link': 'https://arxiv.org/abs/2507.03321', 'abstract': 'Domain adaptation has become a widely adopted approach in machine learning due to the high costs associated with labeling data. It is typically applied when access to a labeled source domain is available. However, in real-world scenarios, privacy concerns often restrict access to sensitive information, such as fingerprints, bank account details, and facial images. A promising solution to this issue is Source-Free Unsupervised Domain Adaptation (SFUDA), which enables domain adaptation without requiring access to labeled target domain data. Recent research demonstrates that SFUDA can effectively address domain discrepancies; however, two key challenges remain: (1) the low quality of prototype samples, and (2) the incorrect assignment of pseudo-labels. To tackle these challenges, we propose a method consisting of three main phases. In the first phase, we introduce a Reliable Sample Memory (RSM) module to improve the quality of prototypes by selecting more representative samples. In the second phase, we employ a Multi-View Contrastive Learning (MVCL) approach to enhance pseudo-label quality by leveraging multiple data augmentations. In the final phase, we apply a noisy label filtering technique to further refine the pseudo-labels. Our experiments on three benchmark datasets - VisDA 2017, Office-Home, and Office-31 - demonstrate that our method achieves approximately 2 percent and 6 percent improvements in classification accuracy over the second-best method and the average of 13 well-known state-of-the-art approaches, respectively.', 'abstract_zh': '源域无标签的领域自适应方法：解决原型样本质量低和伪标签误配问题', 'title_zh': '无源领域适应：多视图对比学习'}
{'arxiv_id': 'arXiv:2507.03318', 'title': 'Structure-Aware Compound-Protein Affinity Prediction via Graph Neural Network with Group Lasso Regularization', 'authors': 'Zanyu Shi, Yang Wang, Pathum Weerawarna, Jie Zhang, Timothy Richardson, Yijie Wang, Kun Huang', 'link': 'https://arxiv.org/abs/2507.03318', 'abstract': 'Explainable artificial intelligence (XAI) approaches have been increasingly applied in drug discovery to learn molecular representations and identify substructures driving property predictions. However, building end-to-end explainable machine learning models for structure-activity relationship (SAR) modeling for compound property prediction faces many challenges, such as limited activity data per target and the sensitivity of properties to subtle molecular changes. To address this, we leveraged activity-cliff molecule pairs, i.e., compounds sharing a common scaffold but differing sharply in potency, targeting three proto-oncogene tyrosine-protein kinase Src proteins (i.e., PDB IDs 1O42, 2H8H, and 4MXO). We implemented graph neural network (GNN) methods to obtain atom-level feature information and predict compound-protein affinity (i.e., half maximal inhibitory concentration, IC50). In addition, we trained GNN models with different structure-aware loss functions to adequately leverage molecular property and structure information. We also utilized group lasso and sparse group lasso to prune and highlight molecular subgraphs and enhance the structure-specific model explainability for the predicted property difference in molecular activity-cliff pairs. We improved drug property prediction by integrating common and uncommon node information and using sparse group lasso, reducing the average root mean squared error (RMSE) by 12.70%, and achieving the lowest averaged RMSE=0.2551 and the highest PCC=0.9572. Furthermore, applying regularization enhances feature attribution methods that estimate the contribution of each atom in the molecular graphs by boosting global direction scores and atom-level accuracy in atom coloring accuracy, which improves model interpretability in drug discovery pipelines, particularly in investigating important molecular substructures in lead optimization.', 'abstract_zh': '可解释的人工智能（XAI）方法在药物发现中的应用：通过活性悬崖分子对构建化合物活性预测的结构-活性关系（SAR）模型', 'title_zh': '基于图神经网络和分组lasso正则化的结构感知化合物-蛋白亲和力预测'}
{'arxiv_id': 'arXiv:2507.03314', 'title': 'Partial Label Learning for Automated Theorem Proving', 'authors': 'Zsolt Zombori, Balázs Indruck', 'link': 'https://arxiv.org/abs/2507.03314', 'abstract': 'We formulate learning guided Automated Theorem Proving as Partial Label Learning, building the first bridge across these fields of research and providing a theoretical framework for dealing with alternative proofs during learning. We use the plCoP theorem prover to demonstrate that methods from the Partial Label Learning literature tend to increase the performance of learning assisted theorem provers.', 'abstract_zh': '我们将学习引导的自动定理证明形式化为部分标签学习，建立了这些研究领域的第一座桥梁，并提供了在学习过程中处理替代证明的理论框架。我们使用plCoP定理证明器证明，部分标签学习文献中的方法倾向于提高学习辅助定理证明器的性能。', 'title_zh': '部分标签学习在自动定理证明中的应用'}
{'arxiv_id': 'arXiv:2507.03310', 'title': 'ReTimeCausal: EM-Augmented Additive Noise Models for Interpretable Causal Discovery in Irregular Time Series', 'authors': 'Weihong Li, Anpeng Wu, Kun Kuang, Keting Yin', 'link': 'https://arxiv.org/abs/2507.03310', 'abstract': 'This paper studies causal discovery in irregularly sampled time series-a pivotal challenge in high-stakes domains like finance, healthcare, and climate science, where missing data and inconsistent sampling frequencies distort causal mechanisms. Traditional methods (e.g., Granger causality, PCMCI) fail to reconcile multi-scale interactions (e.g., hourly storms vs. decadal climate shifts), while neural approaches (e.g., CUTS+) lack interpretability, stemming from a critical gap: existing frameworks either rigidly assume temporal regularity or aggregate dynamics into opaque representations, neglecting real-world granularity and auditable logic. To bridge this gap, we propose ReTimeCausal, a novel integration of Additive Noise Models (ANM) and Expectation-Maximization (EM) that unifies physics-guided data imputation with sparse causal inference. Through kernelized sparse regression and structural constraints, ReTimeCausal iteratively refines missing values (E-step) and causal graphs (M-step), resolving cross-frequency dependencies and missing data issues. Extensive experiments on synthetic and real-world datasets demonstrate that ReTimeCausal outperforms existing state-of-the-art methods under challenging irregular sampling and missing data conditions.', 'abstract_zh': '这项研究探讨了不规则采样时间序列的因果发现——在金融、医疗和气候科学等高风险领域中一个关键挑战，其中缺失数据和不一致的采样频率扭曲了因果机制。传统方法（例如Granger因果关系、PCMCI）无法解决多尺度交互（例如小时级风暴与十年级气候变化）的问题，而神经方法（例如CUTS+）缺乏可解释性，源自于现有框架的关键缺陷：它们要么严格假设时间的规律性，要么将动态聚合为不透明的表现形式，忽视了现实世界的细节和可审计的逻辑。为了解决这一缺陷，我们提出了一种名为ReTimeCausal的新颖集成方法，它结合了加性噪声模型（ANM）和期望最大化（EM），统一了基于物理的数据插补与稀疏因果推断。通过核稀疏回归和结构约束，ReTimeCausal迭代地完善缺失值（E步）和因果图（M步），解决跨频率依赖性和缺失数据问题。在合成数据集和真实世界数据集上的广泛实验证明，在面对不规则采样和缺失数据的挑战条件下，ReTimeCausal优于现有最先进的方法。', 'title_zh': 'ReTimeCausal：用于不规则时间序列可解释因果发现的EM增强加性噪声模型'}
{'arxiv_id': 'arXiv:2507.03307', 'title': 'Scaffolding Recursive Divergence and Convergence in Story Ideation', 'authors': 'Taewook Kim, Matthew Kay, Yuqian Sun, Melissa Roemmele, Max Kreminski, John Joon Young Chung', 'link': 'https://arxiv.org/abs/2507.03307', 'abstract': 'Human creative ideation involves both exploration of diverse ideas (divergence) and selective synthesis of explored ideas into coherent combinations (convergence). While processes of divergence and convergence are often interleaved and nested, existing AI-powered creativity support tools (CSTs) lack support for sophisticated orchestration of divergence and convergence. We present Reverger, an AI-powered CST that helps users ideate variations of conceptual directions for modifying a story by scaffolding flexible iteration between divergence and convergence. For divergence, our tool enables recursive exploration of alternative high-level directions for modifying a specific part of the original story. For convergence, it allows users to collect explored high-level directions and synthesize them into concrete variations. Users can then iterate between divergence and convergence until they find a satisfactory outcome. A within-subject study revealed that Reverger permitted participants to explore more unexpected and diverse high-level directions than a comparable baseline. Reverger users also felt that they had more fine-grained control and discovered more effort-worthy outcomes.', 'abstract_zh': '人类创造性构思涉及对多样化想法的探索（发散）和对探索过的想法进行选择性综合以形成连贯组合（收敛）。虽然发散和收敛的过程通常是交错和嵌套的，但现有的基于AI的创意支持工具（CSTs）缺乏对发散和收敛复杂 orchestration 的支持。我们介绍了一种基于AI的CST——Reverger，它通过在发散和收敛之间的灵活迭代来帮助用户构思修改故事的概念方向。在发散阶段，我们的工具支持对特定部分原始故事进行替代的高层次方向的递归探索。在收敛阶段，它允许用户收集探索过的高层次方向并将其综合成具体的变化。用户可以在此发散和收敛之间迭代，直到找到令人满意的结果。一项以同一受试者为对象的研究显示，Reverger 允许参与者比一个可比的 baseline 涉及更多意想不到和多样的高层次方向。Reverger 的用户还感觉到他们有更多的细腻控制，并发现了更多值得付出努力的结果。', 'title_zh': '支撑递归发散与收敛在故事构思中的应用'}
{'arxiv_id': 'arXiv:2507.03255', 'title': 'ForgeHLS: A Large-Scale, Open-Source Dataset for High-Level Synthesis', 'authors': 'Zedong Peng, Zeju Li, Mingzhe Gao, Qiang Xu, Chen Zhang, Jieru Zhao', 'link': 'https://arxiv.org/abs/2507.03255', 'abstract': "We introduce ForgeEDA, an open-source comprehensive circuit dataset across various categories. ForgeEDA includes diverse circuit representations such as Register Transfer Level (RTL) code, Post-mapping (PM) netlists, And-Inverter Graphs (AIGs), and placed netlists, enabling comprehensive analysis and development. We demonstrate ForgeEDA's utility by benchmarking state-of-the-art EDA algorithms on critical tasks such as Power, Performance, and Area (PPA) optimization, highlighting its ability to expose performance gaps and drive advancements. Additionally, ForgeEDA's scale and diversity facilitate the training of AI models for EDA tasks, demonstrating its potential to improve model performance and generalization. By addressing limitations in existing datasets, ForgeEDA aims to catalyze breakthroughs in modern IC design and support the next generation of innovations in EDA.", 'abstract_zh': '我们介绍ForgeEDA，一个涵盖多种类别开源综合电路数据集。ForgeEDA包括多种电路表示形式，如寄存器传输级（RTL）代码、映射后（PM）网表、与门-反相器图（AIGs）和布线后网表，支持全面分析和开发。通过在关键任务如功率、性能和面积（PPA）优化上 benchmark 现有的EDA算法，展示了其揭示性能差距并推动进步的能力。此外，ForgeEDA的规模和多样性使其适用于EDA任务中的AI模型训练，展示了其提高模型性能和泛化的潜力。通过弥补现有数据集的不足，ForgeEDA旨在推动现代IC设计的突破，并支持下一代EDA创新。', 'title_zh': 'ForgeHLS：一种大规模开源高阶综合数据集'}
{'arxiv_id': 'arXiv:2507.03251', 'title': 'Toward Efficient Speech Emotion Recognition via Spectral Learning and Attention', 'authors': 'HyeYoung Lee, Muhammad Nadeem', 'link': 'https://arxiv.org/abs/2507.03251', 'abstract': 'Speech Emotion Recognition (SER) traditionally relies on auditory data analysis for emotion classification. Several studies have adopted different methods for SER. However, existing SER methods often struggle to capture subtle emotional variations and generalize across diverse datasets. In this article, we use Mel-Frequency Cepstral Coefficients (MFCCs) as spectral features to bridge the gap between computational emotion processing and human auditory perception. To further improve robustness and feature diversity, we propose a novel 1D-CNN-based SER framework that integrates data augmentation techniques. MFCC features extracted from the augmented data are processed using a 1D Convolutional Neural Network (CNN) architecture enhanced with channel and spatial attention mechanisms. These attention modules allow the model to highlight key emotional patterns, enhancing its ability to capture subtle variations in speech signals. The proposed method delivers cutting-edge performance, achieving the accuracy of 97.49% for SAVEE, 99.23% for RAVDESS, 89.31% for CREMA-D, 99.82% for TESS, 99.53% for EMO-DB, and 96.39% for EMOVO. Experimental results show new benchmarks in SER, demonstrating the effectiveness of our approach in recognizing emotional expressions with high precision. Our evaluation demonstrates that the integration of advanced Deep Learning (DL) methods substantially enhances generalization across diverse datasets, underscoring their potential to advance SER for real-world deployment in assistive technologies and human-computer interaction.', 'abstract_zh': '基于1D-CNN的情感语音识别方法：利用梅尔频谱系数和数据增强技术', 'title_zh': '基于频谱学习和注意力机制的高效语音情感识别'}
{'arxiv_id': 'arXiv:2507.03222', 'title': 'The role of gain neuromodulation in layer-5 pyramidal neurons', 'authors': 'Alejandro Rodriguez-Garcia, Christopher J. Whyte, Brandon R. Munn, Jie Mei, James M. Shine, Srikanth Ramaswamy', 'link': 'https://arxiv.org/abs/2507.03222', 'abstract': 'Biological and artificial learning systems alike confront the plasticity-stability dilemma. In the brain, neuromodulators such as acetylcholine and noradrenaline relieve this tension by tuning neuronal gain and inhibitory gating, balancing segregation and integration of circuits. Fed by dense cholinergic and noradrenergic projections from the ascending arousal system, layer-5 pyramidal neurons in the cerebral cortex offer a relevant substrate for understanding these dynamics. When distal dendritic signals coincide with back-propagating action potentials, calcium plateaus turn a single somatic spike into a high-gain burst, and interneuron inhibition sculpts the output. These properties make layer-5 cells gain-tunable amplifiers that translate neuromodulatory cues into flexible cortical activity. To capture this mechanism we developed a two-compartment Izhikevich model for pyramidal neurons and single-compartment somatostatin (SOM) and parvalbumin (PV) interneurons, linked by Gaussian connectivity and spike-timing-dependent plasticity (STDP). The soma and apical dendrite are so coupled that somatic spikes back-propagate, while dendritic plateaus can switch the soma from regular firing to bursting by shifting reset and adaptation variables. We show that stronger dendritic drive or tighter coupling raise gain by increasing the likelihood of calcium-triggered somatic bursts. In contrast, dendritic-targeted inhibition suppresses gain, while somatic-targeted inhibition raises the firing threshold of neighboring neurons, thus gating neurons output. Notably, bursting accelerates STDP, supporting rapid synaptic reconfiguration and this http URL suggests that brief gain pulses driven by neuromodulators could serve as an adaptive two-timescale optimization mechanism, effectively modulating the synaptic weight updates.', 'abstract_zh': '生物学和人工学习系统都面临塑性-稳定性 dilemma。在大脑中，乙酰胆碱和去甲肾上腺素等神经调节物通过调节神经元增益和抑制性门控，平衡电路的分离与整合，缓解这一矛盾。由上行觉醒系统丰富的乙酰胆碱能和去甲肾上腺素能投射供养的皮层层-5锥体细胞，提供了理解这些动态的合适模型。当远端树突信号与后传动作电位 coincidence 时，钙平台将单个胞体尖峰转化为高增益爆发，而抑制性中间神经元则塑造输出。这些特性使层-5细胞成为增益可调的放大器，将神经调节线索转化为灵活的皮层活动。为了捕捉这一机制，我们开发了一种双室Izhikevich模型，用于锥体细胞和单室somatostatin (SOM) 和parvalbumin (PV) 中间神经元，并通过高斯连接和长时程塑性（STDP）将它们联系起来。细胞体与树突高度耦合，使后传胞体尖峰成为可能，同时树突平台可通过调整重置和适应变量将胞体从规则放电切换为爆发放电。研究显示，更强的树突驱动或更紧密的耦合通过增加钙触发胞体爆发的可能性来提高增益。相反，树突目标抑制抑制增益，而胞体目标抑制则提高邻近神经元的放电阈值，从而控制神经元输出。值得注意的是，爆发性活动加速了STDP，支持快速的突触重构。这一结果表明，由神经调节物驱动的短暂增益脉冲可能作为一种适应性两时标优化机制，有效调节突触权重更新，从而实现这一机制。', 'title_zh': '层5/py层5锥形神经元的增益神经调节作用'}
{'arxiv_id': 'arXiv:2507.03221', 'title': 'Neural Inhibition Improves Dynamic Routing and Mixture of Experts', 'authors': 'Will Y. Zou, Jennifer Y. Zhang', 'link': 'https://arxiv.org/abs/2507.03221', 'abstract': 'To be effective, efficient, and diverse, deep learning models need to dynamically choose its architecture based on signals from a population of neurons. We hypothesize dynamic routing models can be improved with neural inhibition in those neural populations. This means signals commonly shared among the various modes of data statistics can be inhibited so that the routing model can choose a specialized expert path for each data sample. Only through inhibition is the routing mechanism able to effectively select neural pathways. We believe this is an under-studied and under-verified implementation methodology for Mixture-of-Experts, dynamic routing, and transformer language models. We provide experimental evidence that the neural inhibition algorithm significantly boosts the performance of general tasks and motivates more effort to be invested in this research direction.', 'abstract_zh': '深度学习模型需要根据神经群体的信号动态选择其架构以实现有效、高效和多样化。我们假设在这些神经群体中加入神经抑制可以改进动态路由模型。这意味着可以抑制多种数据统计模式下共有的信号，从而使路由模型为每个数据样本选择一个专门的专家路径。只有通过抑制，路由机制才能有效地选择神经路径。我们认为这是一项对混合专家模型、动态路由和变换器语言模型研究值得进一步探索和验证的方法。我们提供了实验证据，证明神经抑制算法显著提升了通用任务的性能，并激发了更多对该研究方向的关注和投入。', 'title_zh': '神经抑制优化动态路由和专家混合'}
{'arxiv_id': 'arXiv:2507.03216', 'title': 'Disclosing Generative AI Use in Digital Humanities Research', 'authors': 'Rongqian Ma, Xuhan Zhang, Adrian Wisnicki', 'link': 'https://arxiv.org/abs/2507.03216', 'abstract': "This survey study investigates how digital humanists perceive and approach generative AI disclosure in research. The results indicate that while digital humanities scholars acknowledge the importance of disclosing GenAI use, the actual rate of disclosure in research practice remains low. Respondents differ in their views on which activities most require disclosure and on the most appropriate methods for doing so. Most also believe that safeguards for AI disclosure should be established through institutional policies rather than left to individual decisions. The study's findings will offer empirical guidance to scholars, institutional leaders, funders, and other stakeholders responsible for shaping effective disclosure policies.", 'abstract_zh': '这项调查研究探讨了数字人文学家在研究中如何看待和处理生成式AI披露的问题。结果显示，虽然数字人文学者认为披露生成式AI使用的重要性，但在实际研究实践中，披露率仍然较低。受访者在哪些活动最需要披露以及应该如何披露方面观点不一。大多数研究人员认为，应该通过机构政策来制定AI披露的安全措施，而不是依赖个人决策。研究发现将为学者、机构领导者、资助者及其他负责制定有效披露政策的相关利益方提供实证指导。', 'title_zh': '披露数字人文研究中生成式AI的应用'}
{'arxiv_id': 'arXiv:2507.03176', 'title': 'Deep Learning Atmospheric Models Reliably Simulate Out-of-Sample Land Heat and Cold Wave Frequencies', 'authors': 'Zilu Meng, Gregory J. Hakim, Wenchang Yang, Gabriel A. Vecchi', 'link': 'https://arxiv.org/abs/2507.03176', 'abstract': 'Deep learning (DL)-based general circulation models (GCMs) are emerging as fast simulators, yet their ability to replicate extreme events outside their training range remains unknown. Here, we evaluate two such models -- the hybrid Neural General Circulation Model (NGCM) and purely data-driven Deep Learning Earth System Model (DL\\textit{ESy}M) -- against a conventional high-resolution land-atmosphere model (HiRAM) in simulating land heatwaves and coldwaves. All models are forced with observed sea surface temperatures and sea ice over 1900-2020, focusing on the out-of-sample early-20th-century period (1900-1960). Both DL models generalize successfully to unseen climate conditions, broadly reproducing the frequency and spatial patterns of heatwave and cold wave events during 1900-1960 with skill comparable to HiRAM. An exception is over portions of North Asia and North America, where all models perform poorly during 1940-1960. Due to excessive temperature autocorrelation, DL\\textit{ESy}M tends to overestimate heatwave and cold wave frequencies, whereas the physics-DL hybrid NGCM exhibits persistence more similar to HiRAM.', 'abstract_zh': '基于深度学习的通用环流模型在模拟土地热浪和冷锋方面的能力评价', 'title_zh': '深度学习大气模型可靠模拟样本外土地热浪和冷浪频率'}
{'arxiv_id': 'arXiv:2507.03175', 'title': 'Understanding Knowledge Transferability for Transfer Learning: A Survey', 'authors': 'Haohua Wang, Jingge Wang, Zijie Zhao, Yang Tan, Yanru Wu, Hanbing Liu, Jingyun Yang, Enming Zhang, Xiangyu Chen, Zhengze Rong, Shanxin Guo, Yang Li', 'link': 'https://arxiv.org/abs/2507.03175', 'abstract': 'Transfer learning has become an essential paradigm in artificial intelligence, enabling the transfer of knowledge from a source task to improve performance on a target task. This approach, particularly through techniques such as pretraining and fine-tuning, has seen significant success in fields like computer vision and natural language processing. However, despite its widespread use, how to reliably assess the transferability of knowledge remains a challenge. Understanding the theoretical underpinnings of each transferability metric is critical for ensuring the success of transfer learning. In this survey, we provide a unified taxonomy of transferability metrics, categorizing them based on transferable knowledge types and measurement granularity. This work examines the various metrics developed to evaluate the potential of source knowledge for transfer learning and their applicability across different learning paradigms emphasizing the need for careful selection of these metrics. By offering insights into how different metrics work under varying conditions, this survey aims to guide researchers and practitioners in selecting the most appropriate metric for specific applications, contributing to more efficient, reliable, and trustworthy AI systems. Finally, we discuss some open challenges in this field and propose future research directions to further advance the application of transferability metrics in trustworthy transfer learning.', 'abstract_zh': '迁移学习已成为人工智能中的一个基本范式，通过将源自一个任务的知识转移到另一个目标任务以提高其性能。尽管这种方法在计算机视觉和自然语言处理等领域取得了显著成功，但如何可靠地评估知识的可迁移性仍是一个挑战。了解每个迁移性度量的理论基础对于确保迁移学习的成功至关重要。在这篇综述中，我们提供了一个统一的迁移性度量分类框架，根据可迁移知识类型和测量粒度对其进行分类。本文考查了不同的度量方法，以评估源知识在迁移学习中的潜在能力及其在不同学习范式中的适用性，强调了谨慎选择这些度量方法的必要性。通过揭示不同度量方法在不同条件下的工作机制，本文旨在指导研究人员和从业人员选择最适合特定应用的度量方法，从而促进更高效、可靠和可信的人工智能系统的发展。最后，我们讨论了该领域的一些开放挑战，并提出了未来研究方向，旨在进一步推动可信赖迁移学习中迁移性度量的应用。', 'title_zh': '理解迁移学习中的知识可迁移性：一个综述'}
{'arxiv_id': 'arXiv:2507.03167', 'title': 'Adversarial Manipulation of Reasoning Models using Internal Representations', 'authors': 'Kureha Yamaguchi, Benjamin Etheridge, Andy Arditi', 'link': 'https://arxiv.org/abs/2507.03167', 'abstract': 'Reasoning models generate chain-of-thought (CoT) tokens before their final output, but how this affects their vulnerability to jailbreak attacks remains unclear. While traditional language models make refusal decisions at the prompt-response boundary, we find evidence that DeepSeek-R1-Distill-Llama-8B makes these decisions within its CoT generation. We identify a linear direction in activation space during CoT token generation that predicts whether the model will refuse or comply -- termed the "caution" direction because it corresponds to cautious reasoning patterns in the generated text. Ablating this direction from model activations increases harmful compliance, effectively jailbreaking the model. We additionally show that intervening only on CoT token activations suffices to control final outputs, and that incorporating this direction into prompt-based attacks improves success rates. Our findings suggest that the chain-of-thought itself is a promising new target for adversarial manipulation in reasoning models.\nCode available at this https URL', 'abstract_zh': '基于链-of- thought生成的推理模型在生成最终输出之前会生成链-of-thought（CoT）令牌，但这种过程如何影响其对 jailbreak 攻击的脆弱性尚不明确。虽然传统的语言模型在提示-响应边界处做出拒绝决策，但我们发现 DeepSeek-R1-Distill-Llama-8B 在生成CoT时在其推理过程中也会做出这些决策。我们发现在CoT令牌生成过程中激活空间中存在一条线性方向，该方向可以预测模型是否会拒绝或遵从，称为“谨慎”方向，因为它对应于生成文本中的谨慎推理模式。从模型激活中去除这一方向会增加有害遵从，有效地使模型 jailbreak。此外，我们证明仅干预CoT令牌激活即可控制最终输出，并且将此方向纳入基于提示的攻击可以提高成功几率。我们的研究结果表明，链-of-thought本身就是推理模型对抗操纵的一个有希望的新靶标。', 'title_zh': '利用内部表示操纵推理模型的对抗性攻击'}
{'arxiv_id': 'arXiv:2507.03152', 'title': 'Expert-level validation of AI-generated medical text with scalable language models', 'authors': 'Asad Aali, Vasiliki Bikia, Maya Varma, Nicole Chiou, Sophie Ostmeier, Arnav Singhvi, Magdalini Paschali, Ashwin Kumar, Andrew Johnston, Karimar Amador-Martinez, Eduardo Juan Perez Guerrero, Paola Naovi Cruz Rivera, Sergios Gatidis, Christian Bluethgen, Eduardo Pontes Reis, Eddy D. Zandee van Rilland, Poonam Laxmappa Hosamani, Kevin R Keet, Minjoung Go, Evelyn Ling, David B. Larson, Curtis Langlotz, Roxana Daneshjou, Jason Hom, Sanmi Koyejo, Emily Alsentzer, Akshay S. Chaudhari', 'link': 'https://arxiv.org/abs/2507.03152', 'abstract': 'With the growing use of language models (LMs) in clinical environments, there is an immediate need to evaluate the accuracy and safety of LM-generated medical text. Currently, such evaluation relies solely on manual physician review. However, detecting errors in LM-generated text is challenging because 1) manual review is costly and 2) expert-composed reference outputs are often unavailable in real-world settings. While the "LM-as-judge" paradigm (a LM evaluating another LM) offers scalable evaluation, even frontier LMs can miss subtle but clinically significant errors. To address these challenges, we propose MedVAL, a self-supervised framework that leverages synthetic data to train evaluator LMs to assess whether LM-generated medical outputs are factually consistent with inputs, without requiring physician labels or reference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a dataset containing 840 outputs annotated by physicians, following a physician-defined taxonomy of risk levels and error categories. Across 6 diverse medical tasks and 10 state-of-the-art LMs spanning open-source, proprietary, and medically adapted models, MedVAL fine-tuning significantly improves (p < 0.001) alignment with physicians on both seen and unseen tasks, increasing average F1 scores from 66% to 83%, with per-sample safety classification scores up to 86%. MedVAL improves the performance of even the best-performing proprietary LM (GPT-4o) by 8%. To support a scalable, risk-aware pathway towards clinical integration, we open-source the 1) codebase ( this https URL ), 2) MedVAL-Bench ( this https URL ), and 3) MedVAL-4B ( this https URL ), the best-performing open-source LM. Our research provides the first evidence of LMs approaching expert-level validation ability for medical text.', 'abstract_zh': '利用合成数据训练自我监督框架以评估语言模型生成的医学文本准确性与安全性：MedVAL框架', 'title_zh': '基于可扩展语言模型的AI生成医疗文本专家级验证'}
{'arxiv_id': 'arXiv:2507.03149', 'title': 'On the Relationship between Accent Strength and Articulatory Features', 'authors': 'Kevin Huang, Sean Foley, Jihwan Lee, Yoonjeong Lee, Dani Byrd, Shrikanth Narayanan', 'link': 'https://arxiv.org/abs/2507.03149', 'abstract': 'This paper explores the relationship between accent strength and articulatory features inferred from acoustic speech. To quantify accent strength, we compare phonetic transcriptions with transcriptions based on dictionary-based references, computing phoneme-level difference as a measure of accent strength. The proposed framework leverages recent self-supervised learning articulatory inversion techniques to estimate articulatory features. Analyzing a corpus of read speech from American and British English speakers, this study examines correlations between derived articulatory parameters and accent strength proxies, associating systematic articulatory differences with indexed accent strength. Results indicate that tongue positioning patterns distinguish the two dialects, with notable differences inter-dialects in rhotic and low back vowels. These findings contribute to automated accent analysis and articulatory modeling for speech processing applications.', 'abstract_zh': '本文探索了音重强度与从语音声学中推断出的articulatory特征之间的关系。为了量化音重强度，我们将语音转录与基于词典的参考转录进行比较，计算音素级差异作为音重强度的度量。本文提出的框架利用了近期的自监督学习articulatory反转技术来估计articulatory特征。通过对来自美国和英国英语演讲者的朗读语音语料库的分析，本研究探讨了导出的articulatory参数与音重强度代理变量之间的相关性，将系统的articulatory差异与索引的音重强度关联起来。结果表明，舌尖定位模式区分了这两种方言，音重音和低后元音在方言之间表现出显著差异。这些发现为语音处理应用中的自动化音重分析和articulatory建模做出了贡献。', 'title_zh': '重音强度与发音特征之间的关系'}
{'arxiv_id': 'arXiv:2507.03119', 'title': 'Neural-Network solver of ideal MHD equilibria', 'authors': 'Timo Thun, Andrea Merlo, Rory Conlin, Dario Panici, Daniel Böckenhoff', 'link': 'https://arxiv.org/abs/2507.03119', 'abstract': 'We present a novel approach to compute three-dimensional Magnetohydrodynamic equilibria by parametrizing Fourier modes with artificial neural networks and compare it to equilibria computed by conventional solvers. The full nonlinear global force residual across the volume in real space is then minimized with first order optimizers. Already,we observe competitive computational cost to arrive at the same minimum residuals computed by existing codes. With increased computational cost,lower minima of the residual are achieved by the neural networks,establishing a new lower bound for the force residual. We use minimally complex neural networks,and we expect significant improvements for solving not only single equilibria with neural networks,but also for computing neural network models valid over continuous distributions of equilibria.', 'abstract_zh': '我们提出了一种新的方法，通过使用人工神经网络参数化傅里叶模式来计算三维磁流体力学平衡，并将其与传统求解器计算的平衡进行比较。然后，在实空间中最小化体积上的完整非线性全局力残差，使用一阶优化器。我们观察到，与现有代码计算相同最小残差相比，已实现相近的计算成本。随着计算成本的增加，神经网络实现了更低的残差最小值，从而确立了力残差的新下限。我们使用了结构简单的神经网络，并期望通过神经网络不仅能够解决单个平衡问题，还能计算适用于连续平衡分布的神经网络模型，从而获得显著改进。', 'title_zh': '理想的MHD等离子体平衡的神经网络求解器'}
{'arxiv_id': 'arXiv:2507.03095', 'title': 'Uncovering Synergistic Educational Injustices of COVID-19 and AI', 'authors': 'Ahmad Banyasady', 'link': 'https://arxiv.org/abs/2507.03095', 'abstract': 'Grounded in critical realism and using narrative inquiry, this article explores this article explores the long-term consequences of the COVID-19 pandemic and the rapid proliferation of artificial intelligence within higher education. Through the analysis of student narratives collected in Iranian university settings, the study reveals that learning experiences during and after the pandemic, coupled with unprepared exposure to AI tools, have generated hidden yet impactful layers of educational inequality and cognitive disorientation.', 'abstract_zh': '基于批判现实主义和叙事研究的方法，本文探讨了COVID-19 pandemic和人工智能在高等教育领域快速普及的长期后果。通过分析在伊朗大学环境中收集的学生叙事，研究揭示了在疫情期间及之后的学习经历，以及对AI工具的未准备充分的暴露，已经产生了隐藏但具有影响力的教育不平等和认知错位的层面。', 'title_zh': '揭示COVID-19与人工智能协同的教育不公'}
{'arxiv_id': 'arXiv:2507.03066', 'title': 'Identification of Potentially Misclassified Crash Narratives using Machine Learning (ML) and Deep Learning (DL)', 'authors': 'Sudesh Bhagat, Ibne Farabi Shihab, Jonathan Wood', 'link': 'https://arxiv.org/abs/2507.03066', 'abstract': 'This research investigates the efficacy of machine learning (ML) and deep learning (DL) methods in detecting misclassified intersection-related crashes in police-reported narratives. Using 2019 crash data from the Iowa Department of Transportation, we implemented and compared a comprehensive set of models, including Support Vector Machine (SVM), XGBoost, BERT Sentence Embeddings, BERT Word Embeddings, and Albert Model. Model performance was systematically validated against expert reviews of potentially misclassified narratives, providing a rigorous assessment of classification accuracy. Results demonstrated that while traditional ML methods exhibited superior overall performance compared to some DL approaches, the Albert Model achieved the highest agreement with expert classifications (73% with Expert 1) and original tabular data (58%). Statistical analysis revealed that the Albert Model maintained performance levels similar to inter-expert consistency rates, significantly outperforming other approaches, particularly on ambiguous narratives. This work addresses a critical gap in transportation safety research through multi-modal integration analysis, which achieved a 54.2% reduction in error rates by combining narrative text with structured crash data. We conclude that hybrid approaches combining automated classification with targeted expert review offer a practical methodology for improving crash data quality, with substantial implications for transportation safety management and policy development.', 'abstract_zh': '本研究探讨了机器学习（ML）和深度学习（DL）方法在检测警察报告中误分类的交叉口相关事故中的有效性。利用2019年爱达荷州交通运输部的事故数据，我们实施并比较了一整套模型，包括支持向量机（SVM）、XGBoost、BERT句子嵌入、BERT单词嵌入和Albert模型。模型性能系统地与潜在误分类的报告的专家评审结果进行了验证，提供了分类准确性的严格评估。结果表明，尽管传统机器学习方法在总体性能上优于某些深度学习方法，但Albert模型与专家分类的一致性最高（与专家1的一致性为73%，与原始表格数据的一致性为58%）。统计分析表明，Albert模型的性能水平类似于专家间的一致性率，显著优于其他方法，特别是在模糊的报告文本方面。通过将报告文本与结构化事故数据结合进行多模态综合分析，本研究填补了交通安全研究中的一个重要空白，实现了错误率减少了54.2%。我们得出结论，结合自动分类与目标化专家审阅的混合方法为提高事故数据质量提供了一种实用的方法，对交通安全管理与政策制定具有重大影响。', 'title_zh': '使用机器学习（ML）和深度学习（DL）识别潜在分类错误的道路交通事故叙述'}
{'arxiv_id': 'arXiv:2507.03062', 'title': 'BERT4Traj: Transformer Based Trajectory Reconstruction for Sparse Mobility Data', 'authors': 'Hao Yang, Angela Yao, Christopher Whalen, Gengchen Mai', 'link': 'https://arxiv.org/abs/2507.03062', 'abstract': "Understanding human mobility is essential for applications in public health, transportation, and urban planning. However, mobility data often suffers from sparsity due to limitations in data collection methods, such as infrequent GPS sampling or call detail record (CDR) data that only capture locations during communication events. To address this challenge, we propose BERT4Traj, a transformer based model that reconstructs complete mobility trajectories by predicting hidden visits in sparse movement sequences. Inspired by BERT's masked language modeling objective and self_attention mechanisms, BERT4Traj leverages spatial embeddings, temporal embeddings, and contextual background features such as demographics and anchor points. We evaluate BERT4Traj on real world CDR and GPS datasets collected in Kampala, Uganda, demonstrating that our approach significantly outperforms traditional models such as Markov Chains, KNN, RNNs, and LSTMs. Our results show that BERT4Traj effectively reconstructs detailed and continuous mobility trajectories, enhancing insights into human movement patterns.", 'abstract_zh': '理解人类移动模式对于公共卫生、交通运输和城市规划等领域具有重要意义。然而，移动数据常常由于数据收集方法的限制（如不频繁的GPS采样或仅在通信事件中捕获位置的呼叫详细记录数据）而出现稀疏性。为解决这一挑战，我们提出了一种基于转换器的模型BERT4Traj，通过预测稀疏移动序列中的隐藏访问来重建完整的移动轨迹。受BERT的掩码语言建模目标和自注意力机制的启发，BERT4Traj利用空间嵌入、时间嵌入以及人口统计学特征和锚点等背景上下文信息。我们在乌干达坎帕拉收集的真实世界呼叫详细记录数据和GPS数据上评估了BERT4Traj，结果显示我们的方法显著优于Markov链、KNN、RNN和LSTM等传统模型。我们的结果表明，BERT4Traj有效地重建了详细且连续的移动轨迹，增强了对人类移动模式的见解。', 'title_zh': 'BERT4Traj：基于Transformer的稀疏移动数据轨迹重建'}
{'arxiv_id': 'arXiv:2507.03059', 'title': 'AI-Based Reconstruction from Inherited Personal Data: Analysis, Feasibility, and Prospects', 'authors': 'Mark Zilberman', 'link': 'https://arxiv.org/abs/2507.03059', 'abstract': 'This article explores the feasibility of creating an "electronic copy" of a deceased researcher by training artificial intelligence (AI) on the data stored in their personal computers. By analyzing typical data volumes on inherited researcher computers, including textual files such as articles, emails, and drafts, it is estimated that approximately one million words are available for AI training. This volume is sufficient for fine-tuning advanced pre-trained models like GPT-4 to replicate a researcher\'s writing style, domain expertise, and rhetorical voice with high fidelity. The study also discusses the potential enhancements from including non-textual data and file metadata to enrich the AI\'s representation of the researcher. Extensions of the concept include communication between living researchers and their electronic copies, collaboration among individual electronic copies, as well as the creation and interconnection of organizational electronic copies to optimize information access and strategic decision-making. Ethical considerations such as ownership and security of these electronic copies are highlighted as critical for responsible implementation. The findings suggest promising opportunities for AI-driven preservation and augmentation of intellectual legacy.', 'abstract_zh': '本研究探讨了通过训练人工智能（AI）在已故研究人员个人计算机中存储的数据上创建“电子副本”的可行性。通过分析继承的研究人员计算机上的典型数据量，包括文章、电子邮件和草稿等文本文件，估计可用于AI训练的数据量约为一百万字。这一数据量足以对如GPT-4等先进预训练模型进行微调，从而使AI能够高保真地复制研究人员的写作风格、专业领域知识和修辞voice。研究还讨论了包括非文本数据和文件元数据在内的潜在增强功能，以丰富对研究人员的AI表示。该概念的拓展包括生者研究人员与其电子副本之间的交流、个体电子副本之间的协作，以及组织电子副本的创建和互联，以优化信息访问和战略决策。研究还强调了产权和安全等伦理考虑对于负责任实施的重要性，并指出AI驱动的知识保存与增益具有广阔前景。', 'title_zh': '基于遗传个人数据的AI重构：分析、可行性和前景'}
{'arxiv_id': 'arXiv:2507.03050', 'title': "From Turing to Tomorrow: The UK's Approach to AI Regulation", 'authors': 'Oliver Ritchie, Markus Anderljung, Tom Rachman', 'link': 'https://arxiv.org/abs/2507.03050', 'abstract': 'The UK has pursued a distinctive path in AI regulation: less cautious than the EU but more willing to address risks than the US, and has emerged as a global leader in coordinating AI safety efforts. Impressive developments from companies like London-based DeepMind began to spark concerns in the UK about catastrophic risks from around 2012, although regulatory discussion at the time focussed on bias and discrimination. By 2022, these discussions had evolved into a "pro-innovation" strategy, in which the government directed existing regulators to take a light-touch approach, governing AI at point of use, but avoided regulating the technology or infrastructure directly. ChatGPT arrived in late 2022, galvanising concerns that this approach may be insufficient. The UK responded by establishing an AI Safety Institute to monitor risks and hosting the first international AI Safety Summit in 2023, but - unlike the EU - refrained from regulating frontier AI development in addition to its use. A new government was elected in 2024 which promised to address this gap, but at the time of writing is yet to do so.\nWhat should the UK do next? The government faces competing objectives: harnessing AI for economic growth and better public services while mitigating risk. In light of these, we propose establishing a flexible, principles-based regulator to oversee the most advanced AI development, defensive measures against risks from AI-enabled biological design tools, and argue that more technical work is needed to understand how to respond to AI-generated misinformation. We argue for updated legal frameworks on copyright, discrimination, and AI agents, and that regulators will have a limited but important role if AI substantially disrupts labour markets.\nIf the UK gets AI regulation right, it could demonstrate how democratic societies can harness AI\'s benefits while managing its risks.', 'abstract_zh': '英国在AI监管方面遵循了一条独特的路径：比欧盟更为大胆，但比美国更愿意应对风险，并已成为协调AI安全努力的全球领导者。从2012年起，以伦敦DeepMind为代表公司的显著进展开始引发英国关于灾难性风险的担忧，尽管当时的监管讨论主要集中在偏见和歧视问题上。到2022年，这些讨论已经转变为一种“促进创新”的策略，在这种策略中，政府指导现有的监管机构采取以用户使用为导向的轻触监管方式，但避免直接监管技术或基础设施。ChatGPT于2022年末推出，引发了对这种做法是否足够的担忧。英国随后成立了AI安全研究所以监测风险，并于2023年举办了第一届国际AI安全峰会，但不像欧盟，英国没有对AI前沿开发进行额外的监管。2024年新政府上台承诺解决这一差距，但截至撰文时尚未采取行动。\n英国下一步应该怎么做？政府面临着相互竞争的目标：利用AI促进经济增长和提高公共服务水平，同时减少风险。鉴于这些目标，我们建议建立一个灵活的原则性监管机构来监督最先进的人工智能开发，以及针对人工智能驱动的生物设计工具带来的风险采取防御性措施，并认为需要更多技术工作来理解如何应对由人工智能生成的虚假信息。我们主张更新版权法、反歧视法和人工智能代理法，并认为在人工智能显著扰乱劳动力市场的情况下，监管机构将扮演有限但重要的角色。\n如果英国在AI监管方面取得成功，它将展示民主社会如何利用AI的好处同时管理其风险。', 'title_zh': '从图灵到未来：英国的AI监管之道'}
{'arxiv_id': 'arXiv:2507.03048', 'title': 'Monitoring of Static Fairness', 'authors': 'Thomas A. Henzinger, Mahyar Karimi, Konstantin Kueffner, Kaushik Mallik', 'link': 'https://arxiv.org/abs/2507.03048', 'abstract': 'Machine-learned systems are in widespread use for making decisions about humans, and it is important that they are fair, i.e., not biased against individuals based on sensitive attributes.\nWe present a general framework of runtime verification of algorithmic fairness for systems whose models are unknown, but are assumed to have a Markov chain structure, with or without full observation of the state space.\nWe introduce a specification language that can model many common algorithmic fairness properties, such as demographic parity, equal opportunity, and social burden.\nWe build monitors that observe a long sequence of events as generated by a given system, and output, after each observation, a quantitative estimate of how fair or biased the system was on that run until that point in time.\nThe estimate is proven to be correct modulo a variable error bound and a given confidence level, where the error bound gets tighter as the observed sequence gets longer.\nWe present two categories of monitoring algorithms, namely ones with a uniform error bound across all time points, and ones with weaker non-uniform, pointwise error bounds at different time points.\nOur monitoring algorithms use statistical tools that are adapted to suit the dynamic requirements of monitoring and the special needs of the fairness specifications.\nUsing a prototype implementation, we show how we can monitor if a bank is fair in giving loans to applicants from different social backgrounds, and if a college is fair in admitting students while maintaining a reasonable financial burden on the society.\nIn these experiments, our monitors took less than a millisecond to update their verdicts after each observation.', 'abstract_zh': '基于马尔可夫链结构的未知模型系统运行时算法公平性验证框架', 'title_zh': '静态公平性监测'}
{'arxiv_id': 'arXiv:2507.03045', 'title': 'Optimisation Is Not What You Need', 'authors': 'Alfredo Ibias', 'link': 'https://arxiv.org/abs/2507.03045', 'abstract': 'The Artificial Intelligence field has focused on developing optimisation methods to solve multiple problems, specifically problems that we thought to be only solvable through cognition. The obtained results have been outstanding, being able to even surpass the Turing Test. However, we have found that these optimisation methods share some fundamental flaws that impede them to become a true artificial cognition. Specifically, the field have identified catastrophic forgetting as a fundamental problem to develop such cognition. This paper formally proves that this problem is inherent to optimisation methods, and as such it will always limit approaches that try to solve the Artificial General Intelligence problem as an optimisation problem. Additionally, it addresses the problem of overfitting and discuss about other smaller problems that optimisation methods pose. Finally, it empirically shows how world-modelling methods avoid suffering from either problem. As a conclusion, the field of Artificial Intelligence needs to look outside the machine learning field to find methods capable of developing an artificial cognition.', 'abstract_zh': '人工智能领域专注于开发优化方法以解决多种问题，特别是在我们认为只有通过认知才能解决的问题上。取得的结果非常出色，甚至能够超越图灵测试。然而，我们发现这些优化方法存在一些基本缺陷，阻碍它们成为真正的 artificial cognition。具体地说，该领域已识别出灾难性遗忘是发展这种认知的根本问题。本文正式证明了这个问题是优化方法的固有缺陷，因此它将始终限制那些试图将通用人工智能问题作为优化问题来解决的方法。此外，本文还探讨了优化方法存在的过拟合问题以及其他较小的问题，并实证展示了世界建模方法如何避免遭受这些问题。总之，人工智能领域需要在机器学习领域之外寻找能够发展 artificial cognition 的方法。', 'title_zh': '优化并不是你需要的'}
