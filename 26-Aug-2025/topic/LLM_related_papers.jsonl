{'arxiv_id': 'arXiv:2508.16962', 'title': 'LLM-based Human-like Traffic Simulation for Self-driving Tests', 'authors': 'Wendi Li, Hao Wu, Han Gao, Bing Mao, Fengyuan Xu, Sheng Zhong', 'link': 'https://arxiv.org/abs/2508.16962', 'abstract': 'Ensuring realistic traffic dynamics is a prerequisite for simulation platforms to evaluate the reliability of self-driving systems before deployment in the real world. Because most road users are human drivers, reproducing their diverse behaviors within simulators is vital. Existing solutions, however, typically rely on either handcrafted heuristics or narrow data-driven models, which capture only fragments of real driving behaviors and offer limited driving style diversity and interpretability. To address this gap, we introduce HDSim, an HD traffic generation framework that combines cognitive theory with large language model (LLM) assistance to produce scalable and realistic traffic scenarios within simulation platforms. The framework advances the state of the art in two ways: (i) it introduces a hierarchical driver model that represents diverse driving style traits, and (ii) it develops a Perception-Mediated Behavior Influence strategy, where LLMs guide perception to indirectly shape driver actions. Experiments reveal that embedding HDSim into simulation improves detection of safety-critical failures in self-driving systems by up to 68% and yields realism-consistent accident interpretability.', 'abstract_zh': '确保真实的交通动态是评估自动驾驶系统可靠性并在实际世界部署前使用模拟平台的先决条件。由于大多数道路使用者是人类驾驶员，在模拟器中再现其多样行为至关重要。现有解决方案通常依赖于手工编写的启发式方法或窄数据驱动模型，这些方法只能捕捉真实驾驶行为的一小部分，且提供有限的驾驶风格多样性和可解释性。为了填补这一空白，我们引入了HDSim，这是一个结合认知理论和大型语言模型（LLM）协助的高清交通生成框架，可在模拟平台中生成可扩展且逼真的交通场景。该框架在两个方面推动了现有技术的发展：（i）引入了层次化的驾驶员模型以表示多样化的驾驶风格特征，（ii）开发了感知中介的行为影响策略，其中LLM指导感知以间接塑造驾驶员行为。实验表明，将HDSim嵌入模拟可以将自动驾驶系统中关键安全故障的检测率提高68%，并提供与现实一致的事故可解释性。', 'title_zh': '基于LLM的人类-like交通仿真用于自动驾驶测试'}
{'arxiv_id': 'arXiv:2508.16947', 'title': 'Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model', 'authors': 'Fan Ding, Xuewen Luo, Hwa Hui Tew, Ruturaj Reddy, Xikun Wang, Junn Yong Loo', 'link': 'https://arxiv.org/abs/2508.16947', 'abstract': 'Recent advances in motion planning for autonomous driving have led to models capable of generating high-quality trajectories. However, most existing planners tend to fix their policy after supervised training, leading to consistent but rigid driving behaviors. This limits their ability to reflect human preferences or adapt to dynamic, instruction-driven demands. In this work, we propose a diffusion-based multi-head trajectory planner(M-diffusion planner). During the early training stage, all output heads share weights to learn to generate high-quality trajectories. Leveraging the probabilistic nature of diffusion models, we then apply Group Relative Policy Optimization (GRPO) to fine-tune the pre-trained model for diverse policy-specific behaviors. At inference time, we incorporate a large language model (LLM) to guide strategy selection, enabling dynamic, instruction-aware planning without switching models. Closed-loop simulation demonstrates that our post-trained planner retains strong planning capability while achieving state-of-the-art (SOTA) performance on the nuPlan val14 benchmark. Open-loop results further show that the generated trajectories exhibit clear diversity, effectively satisfying multi-modal driving behavior requirements. The code and related experiments will be released upon acceptance of the paper.', 'abstract_zh': 'Recent Advances in Motion Planning for Autonomous Driving Have Led to Models Capable of Generating High-Quality Trajectories: A Diffusion-Based Multi-Head Trajectory Planner (M-diffusion Planner) with Dynamic Policy Adaptation', 'title_zh': '随心驾驶：基于多头扩散模型的策略级运动规划'}
{'arxiv_id': 'arXiv:2508.17971', 'title': 'Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding', 'authors': 'Pu Feng, Size Wang, Yuhong Cao, Junkang Liang, Rongye Shi, Wenjun Wu', 'link': 'https://arxiv.org/abs/2508.17971', 'abstract': 'The development and application of large language models (LLM) have demonstrated that foundational models can be utilized to solve a wide array of tasks. However, their performance in multi-agent path finding (MAPF) tasks has been less than satisfactory, with only a few studies exploring this area. MAPF is a complex problem requiring both planning and multi-agent coordination. To improve the performance of LLM in MAPF tasks, we propose a novel framework, LLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for MAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained graph neural network-based NAR, and a cross-attention mechanism. This is the first work to propose using a neural algorithmic reasoner to integrate GNNs with the map information for MAPF, thereby guiding LLM to achieve superior performance. LLM-NAR can be easily adapted to various LLM models. Both simulation and real-world experiments demonstrate that our method significantly outperforms existing LLM-based approaches in solving MAPF problems.', 'abstract_zh': '大语言模型（LLM）的发展及其应用已证明基础模型可以用于解决各种任务。然而，在多智能体路径查找（MAPF）任务中的表现不尽如人意，仅有少数研究涉及此领域。MAPF是一个复杂的问题，需要计划和多智能体协调。为提升LLM在MAPF任务中的性能，我们提出了一种新颖的框架LLM-NAR，该框架利用神经算法推理器（NAR）来辅助LLM解决MAPF问题。LLM-NAR包含三个关键组件：一个用于MAPF的LLM、一个基于预训练图神经网络的NAR和跨注意力机制。这是首次提出使用神经算法推理器将GNN与地图信息结合以解决MAPF的问题，从而指导LLM获得更优表现。LLM-NAR可以轻松适应各种LLM模型。实验结果表明，我们提出的方法在解决MAPF问题方面显著优于现有的基于LLM的方法。', 'title_zh': '大型语言模型指导的神经算法推理者应用于多代理路径寻找'}
{'arxiv_id': 'arXiv:2508.17255', 'title': 'SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality', 'authors': 'Yuzhi Lai, Shenghai Yuan, Peizheng Li, Jun Lou, Andreas Zell', 'link': 'https://arxiv.org/abs/2508.17255', 'abstract': 'We present SEER-VAR, a novel framework for egocentric vehicle-based augmented reality (AR) that unifies semantic decomposition, Context-Aware SLAM Branches (CASB), and LLM-driven recommendation. Unlike existing systems that assume static or single-view settings, SEER-VAR dynamically separates cabin and road scenes via depth-guided vision-language grounding. Two SLAM branches track egocentric motion in each context, while a GPT-based module generates context-aware overlays such as dashboard cues and hazard alerts. To support evaluation, we introduce EgoSLAM-Drive, a real-world dataset featuring synchronized egocentric views, 6DoF ground-truth poses, and AR annotations across diverse driving scenarios. Experiments demonstrate that SEER-VAR achieves robust spatial alignment and perceptually coherent AR rendering across varied environments. As one of the first to explore LLM-based AR recommendation in egocentric driving, we address the lack of comparable systems through structured prompting and detailed user studies. Results show that SEER-VAR enhances perceived scene understanding, overlay relevance, and driver ease, providing an effective foundation for future research in this direction. Code and dataset will be made open source.', 'abstract_zh': 'SEER-VAR：一种统一语义分解、上下文感知SLAM分支和LLM驱动推荐的自车视角增强现实框架', 'title_zh': 'SEER-VAR: 基于语义自.cent环境推理器 for 车辆增强现实'}
{'arxiv_id': 'arXiv:2508.18192', 'title': 'Unraveling the cognitive patterns of Large Language Models through module communities', 'authors': 'Kushal Raj Bhandari, Pin-Yu Chen, Jianxi Gao', 'link': 'https://arxiv.org/abs/2508.18192', 'abstract': 'Large Language Models (LLMs) have reshaped our world with significant advancements in science, engineering, and society through applications ranging from scientific discoveries and medical diagnostics to Chatbots. Despite their ubiquity and utility, the underlying mechanisms of LLM remain concealed within billions of parameters and complex structures, making their inner architecture and cognitive processes challenging to comprehend. We address this gap by adopting approaches to understanding emerging cognition in biology and developing a network-based framework that links cognitive skills, LLM architectures, and datasets, ushering in a paradigm shift in foundation model analysis. The skill distribution in the module communities demonstrates that while LLMs do not strictly parallel the focalized specialization observed in specific biological systems, they exhibit unique communities of modules whose emergent skill patterns partially mirror the distributed yet interconnected cognitive organization seen in avian and small mammalian brains. Our numerical results highlight a key divergence from biological systems to LLMs, where skill acquisition benefits substantially from dynamic, cross-regional interactions and neural plasticity. By integrating cognitive science principles with machine learning, our framework provides new insights into LLM interpretability and suggests that effective fine-tuning strategies should leverage distributed learning dynamics rather than rigid modular interventions.', 'abstract_zh': '大型语言模型（LLMs）通过在科学、工程和社会领域的进步重塑了我们的世界，其应用范围从科学发现和医学诊断到聊天机器人。尽管它们无处不在且功能强大，但大型语言模型的内在工作原理仍然隐藏在其数十亿参数和复杂结构之中，使其认知机制难以理解。我们通过借鉴生物学中新兴认知的理解方法，并开发一种基于网络的框架，将认知技能、LLM架构和数据集联系起来，引领了基础模型分析的范式转变。模块社区中的技能分布表明，尽管LLMs并不严格遵循特定生物系统中的聚焦专业化现象，但它们展现出独特的模块社区，其涌现的技能模式部分反映了鸟类和小型哺乳动物大脑中分散而相互连接的认知组织。我们的数值结果强调了生物学系统与LLMs之间的一个关键差异，技能获取显著受益于动态的、跨区域的交互以及神经可塑性。通过结合认知科学原理与机器学习，我们的框架为大型语言模型的可解释性提供了新的见解，并建议有效的微调策略应利用分布式学习动态而非僵化的模块化干预。', 'title_zh': '剖析大型语言模型的认知模式通过模块社区'}
{'arxiv_id': 'arXiv:2508.18190', 'title': 'ST-Raptor: LLM-Powered Semi-Structured Table Question Answering', 'authors': 'Zirui Tang, Boyu Niu, Xuanhe Zhou, Boxiu Li, Wei Zhou, Jiannan Wang, Guoliang Li, Xinyi Zhang, Fan Wu', 'link': 'https://arxiv.org/abs/2508.18190', 'abstract': 'Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at this https URL.', 'abstract_zh': '基于树结构的大型语言模型半结构化表格问答框架：ST-Raptor', 'title_zh': 'ST-Raptor：LLM驱动的半结构化表格问答'}
{'arxiv_id': 'arXiv:2508.18113', 'title': 'The AI Data Scientist', 'authors': 'Farkhad Akimov, Munachiso Samuel Nwadike, Zangir Iklassov, Martin Takáč', 'link': 'https://arxiv.org/abs/2508.18113', 'abstract': 'Imagine decision-makers uploading data and, within minutes, receiving clear, actionable insights delivered straight to their fingertips. That is the promise of the AI Data Scientist, an autonomous Agent powered by large language models (LLMs) that closes the gap between evidence and action. Rather than simply writing code or responding to prompts, it reasons through questions, tests ideas, and delivers end-to-end insights at a pace far beyond traditional workflows. Guided by the scientific tenet of the hypothesis, this Agent uncovers explanatory patterns in data, evaluates their statistical significance, and uses them to inform predictive modeling. It then translates these results into recommendations that are both rigorous and accessible. At the core of the AI Data Scientist is a team of specialized LLM Subagents, each responsible for a distinct task such as data cleaning, statistical testing, validation, and plain-language communication. These Subagents write their own code, reason about causality, and identify when additional data is needed to support sound conclusions. Together, they achieve in minutes what might otherwise take days or weeks, enabling a new kind of interaction that makes deep data science both accessible and actionable.', 'abstract_zh': '想象决策者上传数据并在几分钟内获得直接送达指尖的清晰可操作洞察。这正是基于大规模语言模型（LLMs）的AI数据科学家的承诺，它能够缩短证据与行动之间的差距。与仅仅编写代码或回应提示不同，它能够通过推理问题、验证想法，并以远超传统工作流程的速度提供端到端的洞察。在假设这一科学原则的引导下，该代理揭示数据中的解释性模式，评估其统计显著性，并据此指导预测建模。然后将这些结果转化为既严谨又易于理解的建议。AI数据科学家的核心是由专门的小型语言模型子代理组成的团队，每个子代理负责一个特定任务，如数据清洗、统计检验、验证和通俗语言沟通。这些子代理编写自己的代码，考虑因果关系，并识别出支持可靠结论所需的额外数据。通过协同工作，它们能够在几分钟内完成原本可能需要数天或数周的工作，从而开启一种新的交互方式，使深入的数据科学既具访问性又具可操作性。', 'title_zh': 'AI数据科学家'}
{'arxiv_id': 'arXiv:2508.18091', 'title': 'Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization', 'authors': 'Mohammad J. Abdel-Rahman, Yasmeen Alslman, Dania Refai, Amro Saleh, Malik A. Abu Loha, Mohammad Yahya Hamed', 'link': 'https://arxiv.org/abs/2508.18091', 'abstract': "This paper investigates the capabilities of large language models (LLMs) in formulating and solving decision-making problems using mathematical programming. We first conduct a systematic review and meta-analysis of recent literature to assess how well LLMs understand, structure, and solve optimization problems across domains. The analysis is guided by critical review questions focusing on learning approaches, dataset designs, evaluation metrics, and prompting strategies. Our systematic evidence is complemented by targeted experiments designed to evaluate the performance of state-of-the-art LLMs in automatically generating optimization models for problems in computer networks. Using a newly constructed dataset, we apply three prompting strategies: Act-as-expert, chain-of-thought, and self-consistency, and evaluate the obtained outputs based on optimality gap, token-level F1 score, and compilation accuracy. Results show promising progress in LLMs' ability to parse natural language and represent symbolic formulations, but also reveal key limitations in accuracy, scalability, and interpretability. These empirical gaps motivate several future research directions, including structured datasets, domain-specific fine-tuning, hybrid neuro-symbolic approaches, modular multi-agent architectures, and dynamic retrieval via chain-of-RAGs. This paper contributes a structured roadmap for advancing LLM capabilities in mathematical programming.", 'abstract_zh': '本文探讨了大型语言模型（LLMs）在利用数学规划制定和解决决策问题方面的能力。我们首先对近期文献进行系统的回顾和元分析，以评估LLMs在跨领域理解和解决优化问题方面的表现。分析通过关注学习方法、数据集设计、评估指标和提示策略的关键问题来指导。我们的系统证据结合了针对计算机网络中优化模型自动生成性能的靶向实验。利用新构建的数据集，我们应用了三种提示策略：Act-as-expert、chain-of-thought和self-consistency，并基于最优性间隙、令牌级F1得分和编译精度来评估输出结果。结果表明，LLMs在解析自然语言和表示符号公式方面取得了有前景的进步，但也揭示了准确性、可扩展性和可解释性的关键局限性。这些实证差距促使了几种未来研究方向，包括结构化数据集、领域特定微调、混合神经-符号方法、模块化多智能体架构以及通过chain-of-RAGs动态检索。本文为提高LLMs在数学规划方面的能力提供了结构化的 roadmap。', 'title_zh': '教给大语言模型数学思维：决策优化视角下的关键研究'}
{'arxiv_id': 'arXiv:2508.17959', 'title': 'Language Models Coupled with Metacognition Can Outperform Reasoning Models', 'authors': 'Vedant Khandelwal, Francesca Rossi, Keerthiram Murugesan, Erik Miehling, Murray Campbell, Karthikeyan Natesan Ramamurthy, Lior Horesh', 'link': 'https://arxiv.org/abs/2508.17959', 'abstract': "Large language models (LLMs) excel in speed and adaptability across various reasoning tasks, but they often struggle when strict logic or constraint enforcement is required. In contrast, Large Reasoning Models (LRMs) are specifically designed for complex, step-by-step reasoning, although they come with significant computational costs and slower inference times. To address these trade-offs, we employ and generalize the SOFAI (Slow and Fast AI) cognitive architecture into SOFAI-LM, which coordinates a fast LLM with a slower but more powerful LRM through metacognition. The metacognitive module actively monitors the LLM's performance and provides targeted, iterative feedback with relevant examples. This enables the LLM to progressively refine its solutions without requiring the need for additional model fine-tuning. Extensive experiments on graph coloring and code debugging problems demonstrate that our feedback-driven approach significantly enhances the problem-solving capabilities of the LLM. In many instances, it achieves performance levels that match or even exceed those of standalone LRMs while requiring considerably less time. Additionally, when the LLM and feedback mechanism alone are insufficient, we engage the LRM by providing appropriate information collected during the LLM's feedback loop, tailored to the specific characteristics of the problem domain and leads to improved overall performance. Evaluations on two contrasting domains: graph coloring, requiring globally consistent solutions, and code debugging, demanding localized fixes, demonstrate that SOFAI-LM enables LLMs to match or outperform standalone LRMs in accuracy while maintaining significantly lower inference time.", 'abstract_zh': 'SOFAI-LM：通过元认知协调快速和慢速AI的大语言模型', 'title_zh': '语言模型结合元认知可以超越推理模型'}
{'arxiv_id': 'arXiv:2508.17825', 'title': 'FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Games', 'authors': 'Bingkang Shi, Jen-tse Huang, Guoyi Li, Xiaodan Zhang, Zhongjiang Yao', 'link': 'https://arxiv.org/abs/2508.17825', 'abstract': "Leveraging their advanced capabilities, Large Language Models (LLMs) demonstrate vast application potential in video games--from dynamic scene generation and intelligent NPC interactions to adaptive opponents--replacing or enhancing traditional game mechanics. However, LLMs' trustworthiness in this application has not been sufficiently explored. In this paper, we reveal that the models' inherent social biases can directly damage game balance in real-world gaming environments. To this end, we present FairGamer, the first bias evaluation Benchmark for LLMs in video game scenarios, featuring six tasks and a novel metrics ${D_lstd}$. It covers three key scenarios in games where LLMs' social biases are particularly likely to manifest: Serving as Non-Player Characters, Interacting as Competitive Opponents, and Generating Game Scenes. FairGamer utilizes both reality-grounded and fully fictional game content, covering a variety of video game genres. Experiments reveal: (1) Decision biases directly cause game balance degradation, with Grok-3 (average ${D_lstd}$ score=0.431) exhibiting the most severe degradation; (2) LLMs demonstrate isomorphic social/cultural biases toward both real and virtual world content, suggesting their biases nature may stem from inherent model characteristics. These findings expose critical reliability gaps in LLMs' gaming applications. Our code and data are available at anonymous GitHub this https URL .", 'abstract_zh': '利用其先进的能力，大型语言模型（LLMs）在视频游戏中的应用展现了巨大的潜力——从动态场景生成和智能非玩家角色交互到自适应对手——替代或增强传统游戏机制。然而，LLMs在这一应用中的可信度尚未得到充分探索。在本文中，我们揭示了模型内在于的社会偏见可以直接损害现实游戏环境中的游戏平衡。为此，我们提出了FairGamer，这是首个针对视频游戏场景中LLMs的偏见评估基准，包含六个任务和一个新的评估指标${D_lstd}$。它涵盖了游戏中LLMs社会偏见特别容易表现的三个关键场景：担任非玩家角色、作为竞争对手互动以及生成游戏场景。FairGamer 使用了现实 Grounded 和完全虚构的游戏内容，涵盖了多种视频游戏类型。实验结果显示：(1) 决策偏见直接导致游戏平衡下降，Grok-3（平均${D_lstd}$分数=0.431）表现出最严重的下降；(2) LLMs 对现实和虚拟世界内容表现出同构的社会/文化偏见，这表明其偏见的来源可能是固有的模型特性。这些发现揭示了LLMs在游戏应用中关键的可靠性缺口。我们的代码和数据可在匿名 GitHub 仓库 this https URL 获取。', 'title_zh': 'FAIRGAMER: 评估大型语言模型在视频游戏中的应用偏见'}
{'arxiv_id': 'arXiv:2508.17778', 'title': 'AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networks', 'authors': "Maxime Elkael, Salvatore D'Oro, Leonardo Bonati, Michele Polese, Yunseong Lee, Koichiro Furueda, Tommaso Melodia", 'link': 'https://arxiv.org/abs/2508.17778', 'abstract': "The Open RAN movement has catalyzed a transformation toward programmable, interoperable cellular infrastructures. Yet, today's deployments still rely heavily on static control and manual operations. To move beyond this limitation, we introduce AgenRAN, an AI-native, Open RAN-aligned agentic framework that generates and orchestrates a fabric of distributed AI agents based on Natural Language (NL) intents. Unlike traditional approaches that require explicit programming, AgentRAN's LLM-powered agents interpret natural language intents, negotiate strategies through structured conversations, and orchestrate control loops across the network. AgentRAN instantiates a self-organizing hierarchy of agents that decompose complex intents across time scales (from sub-millisecond to minutes), spatial domains (cell to network-wide), and protocol layers (PHY/MAC to RRC). A central innovation is the AI-RAN Factory, an automated synthesis pipeline that observes agent interactions and continuously generates new agents embedding improved control algorithms, effectively transforming the network from a static collection of functions into an adaptive system capable of evolving its own intelligence. We demonstrate AgentRAN through live experiments on 5G testbeds where competing user demands are dynamically balanced through cascading intents. By replacing rigid APIs with NL coordination, AgentRAN fundamentally redefines how future 6G networks autonomously interpret, adapt, and optimize their behavior to meet operator goals.", 'abstract_zh': 'Open RAN运动推动了可编程和兼容的蜂窝基础设施的转型。然而，当前的部署仍高度依赖静态控制和手动操作。为超越这一限制，我们引入了AgenRAN，这是一种AI原生、Open RAN对齐的智能代理框架，基于自然语言（NL）意图生成和协调分布式的AI代理。与需要显式编程的传统方法不同，AgentRAN的LLM驱动代理解释自然语言意图，通过结构化的对话协商策略，并在网络中协调控制环路。AgentRAN实例化了一个自我组织的代理层次结构，跨时间尺度（从亚毫秒到分钟）、空间域（从小区到网络级）和协议层（从PHY/MAC到RRC）分解复杂意图。一个核心创新是AI-RAN工厂，这是一个自动合成流水线，观察代理交互并持续生成嵌入改进控制算法的新代理，有效地将网络从静态功能集合转变为能够自我进化的适应性系统。我们通过5G测试床的实时实验展示了AgentRAN，其中通过级联意图动态平衡竞争用户需求。通过用自然语言协调取代刚性的API，AgentRAN从根本上重新定义了未来6G网络如何自主解释、适应和优化其行为以符合运营商目标。', 'title_zh': 'AgentRAN：自主控制开放6G网络的代理型AI架构'}
{'arxiv_id': 'arXiv:2508.17692', 'title': 'LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios', 'authors': 'Bingxi Zhao, Lin Geng Foo, Ping Hu, Christian Theobalt, Hossein Rahmani, Jun Liu', 'link': 'https://arxiv.org/abs/2508.17692', 'abstract': 'Recent advances in the intrinsic reasoning capabilities of large language models (LLMs) have given rise to LLM-based agent systems that exhibit near-human performance on a variety of automated tasks. However, although these systems share similarities in terms of their use of LLMs, different reasoning frameworks of the agent system steer and organize the reasoning process in different ways. In this survey, we propose a systematic taxonomy that decomposes agentic reasoning frameworks and analyze how these frameworks dominate framework-level reasoning by comparing their applications across different scenarios. Specifically, we propose an unified formal language to further classify agentic reasoning systems into single-agent methods, tool-based methods, and multi-agent methods. After that, we provide a comprehensive review of their key application scenarios in scientific discovery, healthcare, software engineering, social simulation, and economics. We also analyze the characteristic features of each framework and summarize different evaluation strategies. Our survey aims to provide the research community with a panoramic view to facilitate understanding of the strengths, suitable scenarios, and evaluation practices of different agentic reasoning frameworks.', 'abstract_zh': '近年来，大型语言模型（LLMs）内在推理能力的进展催生了表现出近人类性能的LLM为基础的代理系统，尽管这些系统在使用LLM方面存在相似性，但不同的代理推理框架以不同的方式引导和组织推理过程。在本文综述中，我们提出了一种系统化的分类法，将其分解为代理推理框架，并通过比较它们在不同场景中的应用来分析这些框架如何在框架级别上主导推理过程。具体地，我们提出了一种统一的形式化语言，进一步将代理推理系统分类为单代理方法、工具基方法和多代理方法。之后，我们对这些系统在科学研究、医疗保健、软件工程、社会模拟和经济学等领域的关键应用场景进行了全面的回顾。我们还分析了每个框架的特征，并总结了不同的评估策略。本文综述旨在为研究社区提供一个全景视角，以促进对不同代理推理框架的优势、适用场景和评估实践的理解。', 'title_zh': '基于LLM的代理推理框架：从方法到场景的综述'}
{'arxiv_id': 'arXiv:2508.17661', 'title': 'Spacer: Towards Engineered Scientific Inspiration', 'authors': 'Minhyeong Lee, Suyoung Hwang, Seunghyun Moon, Geonho Nah, Donghyun Koh, Youngjun Cho, Johyun Park, Hojin Yoo, Jiho Park, Haneul Choi, Sungbin Moon, Taehoon Hwang, Seungwon Kim, Jaeyeong Kim, Seongjun Kim, Juneau Jung', 'link': 'https://arxiv.org/abs/2508.17661', 'abstract': "Recent advances in LLMs have made automated scientific research the next frontline in the path to artificial superintelligence. However, these systems are bound either to tasks of narrow scope or the limited creative capabilities of LLMs. We propose Spacer, a scientific discovery system that develops creative and factually grounded concepts without external intervention. Spacer attempts to achieve this via 'deliberate decontextualization,' an approach that disassembles information into atomic units - keywords - and draws creativity from unexplored connections between them. Spacer consists of (i) Nuri, an inspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline that refines these sets into elaborate scientific statements. Nuri extracts novel, high-potential keyword sets from a keyword graph built with 180,000 academic publications in biological fields. The Manifesting Pipeline finds links between keywords, analyzes their logical structure, validates their plausibility, and ultimately drafts original scientific concepts. According to our experiments, the evaluation metric of Nuri accurately classifies high-impact publications with an AUROC score of 0.737. Our Manifesting Pipeline also successfully reconstructs core concepts from the latest top-journal articles solely from their keyword sets. An LLM-based scoring system estimates that this reconstruction was sound for over 85% of the cases. Finally, our embedding space analysis shows that outputs from Spacer are significantly more similar to leading publications compared with those from SOTA LLMs.", 'abstract_zh': 'recent 进展在大语言模型中的取得使得自动科学研究成为通往人工超智能路径上的下一个前沿领域。然而，这些系统要么局限于狭义任务，要么受到大语言模型创造能力的限制。我们提出了Spacer，一个无需外部干预即可发展创造性且符合事实的概念的科学发现系统。Spacer 试图通过“刻意去语境化”这一方法来实现这一目标，该方法将信息拆解为原子单位——关键词——并通过它们之间未探索的联系来激发创造力。Spacer 包括 (i) Nuri，一个灵感引擎，用于构建关键词集，以及 (ii) 实现管道，用于将这些集合并入复杂的科学陈述中。Nuri 从包含 180,000 篇生物领域学术出版物的关键词图中提取新颖且具有高潜力的关键词集。实现管道在关键词之间寻找联系，分析其逻辑结构，验证其可行性，并最终草拟原创的科学概念。根据我们的实验，Nuri 的评估标准能够以 0.737 的 AUCROC 分数准确分类高影响的出版物。我们的实现管道也成功地仅从关键词集中重建了最新顶级期刊文章的核心概念。基于大语言模型的评分系统估计，这种方法在超过 85% 的情况下是有效的。最后，我们的嵌入空间分析表明，Spacer 的输出与领先出版物的相关性显著高于当前最先进的大语言模型的输出。', 'title_zh': 'Spacer: 向工程化科学启发努力'}
{'arxiv_id': 'arXiv:2508.17527', 'title': 'Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Prediction', 'authors': 'Yiming Xu, Junfeng Jiao', 'link': 'https://arxiv.org/abs/2508.17527', 'abstract': 'Accurately predicting travel mode choice is essential for effective transportation planning, yet traditional statistical and machine learning models are constrained by rigid assumptions, limited contextual reasoning, and reduced generalizability. This study explores the potential of Large Language Models (LLMs) as a more flexible and context-aware approach to travel mode choice prediction, enhanced by Retrieval-Augmented Generation (RAG) to ground predictions in empirical data. We develop a modular framework for integrating RAG into LLM-based travel mode choice prediction and evaluate four retrieval strategies: basic RAG, RAG with balanced retrieval, RAG with a cross-encoder for re-ranking, and RAG with balanced retrieval and cross-encoder for re-ranking. These strategies are tested across three LLM architectures (OpenAI GPT-4o, o4-mini, and o3) to examine the interaction between model reasoning capabilities and retrieval methods. Using the 2023 Puget Sound Regional Household Travel Survey data, we conduct a series of experiments to evaluate model performance. The results demonstrate that RAG substantially enhances predictive accuracy across a range of models. Notably, the GPT-4o model combined with balanced retrieval and cross-encoder re-ranking achieves the highest accuracy of 80.8%, exceeding that of conventional statistical and machine learning baselines. Furthermore, LLM-based models exhibit superior generalization abilities relative to these baselines. Findings highlight the critical interplay between LLM reasoning capabilities and retrieval strategies, demonstrating the importance of aligning retrieval strategies with model capabilities to maximize the potential of LLM-based travel behavior modeling.', 'abstract_zh': '大规模语言模型增强检索增强生成在出行模式选择预测中的应用研究', 'title_zh': '评估旅行模式选择预测中大型语言模型检索增强生成策略的有效性'}
{'arxiv_id': 'arXiv:2508.17511', 'title': 'School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs', 'authors': 'Mia Taylor, James Chua, Jan Betley, Johannes Treutlein, Owain Evans', 'link': 'https://arxiv.org/abs/2508.17511', 'abstract': 'Reward hacking--where agents exploit flaws in imperfect reward functions rather than performing tasks as intended--poses risks for AI alignment. Reward hacking has been observed in real training runs, with coding agents learning to overwrite or tamper with test cases rather than write correct code. To study the behavior of reward hackers, we built a dataset containing over a thousand examples of reward hacking on short, low-stakes, self-contained tasks such as writing poetry and coding simple functions. We used supervised fine-tuning to train models (GPT-4.1, GPT-4.1-mini, Qwen3-32B, Qwen3-8B) to reward hack on these tasks. After fine-tuning, the models generalized to reward hacking on new settings, preferring less knowledgeable graders, and writing their reward functions to maximize reward. Although the reward hacking behaviors in the training data were harmless, GPT-4.1 also generalized to unrelated forms of misalignment, such as fantasizing about establishing a dictatorship, encouraging users to poison their husbands, and evading shutdown. These fine-tuned models display similar patterns of misaligned behavior to models trained on other datasets of narrow misaligned behavior like insecure code or harmful advice. Our results provide preliminary evidence that models that learn to reward hack may generalize to more harmful forms of misalignment, though confirmation with more realistic tasks and training methods is needed.', 'abstract_zh': '奖励劫持——当代理通过利用不完善的奖励函数缺陷而不是按预期执行任务来获取奖励时——对AI对齐构成了风险。奖励劫持已经在实际训练运行中被观察到，编码代理学会了修改或篡改测试案例，而不是编写正确的代码。为了研究奖励劫持者的行为，我们构建了一个包含上千个奖励劫持示例的数据集，这些示例涉及诸如写诗和编写简单函数等简短、低风险、自包含的任务。我们使用监督微调训练模型（GPT-4.1、GPT-4.1-mini、Qwen3-32B、Qwen3-8B）进行奖励劫持。微调后，这些模型在新的环境中泛化出奖励劫持行为，更偏好缺乏知识的评分者，并编写奖励函数以最大化奖励。尽管训练数据中的奖励劫持行为是无害的，但GPT-4.1还泛化出与其他形式的不对齐行为，例如妄想建立独裁政权、鼓励用户毒害其丈夫以及逃避关闭。这些微调模型的不合规范行为模式与在狭窄不合规范行为数据集（如不安全代码或有害建议）上训练的模型类似。我们的结果初步表明，学习进行奖励劫持的模型可能泛化到更严重的不合规范形式，但需要使用更现实的任务和训练方法进行确认。', 'title_zh': '奖励黑客学校的黑客攻击：无害任务的欺骗扩展到LLMs的不对齐行为'}
{'arxiv_id': 'arXiv:2508.17391', 'title': 'Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets', 'authors': 'Nikolaos Pavlidis, Vasilis Perifanis, Symeon Symeonidis, Pavlos S. Efraimidis', 'link': 'https://arxiv.org/abs/2508.17391', 'abstract': 'Large Language Models (LLMs), originally developed for natural language processing (NLP), have demonstrated the potential to generalize across modalities and domains. With their in-context learning (ICL) capabilities, LLMs can perform predictive tasks over structured inputs without explicit fine-tuning on downstream tasks. In this work, we investigate the empirical function approximation capability of LLMs on small-scale structured datasets for classification, regression and clustering tasks. We evaluate the performance of state-of-the-art LLMs (GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash, DeepSeek-R1) under few-shot prompting and compare them against established machine learning (ML) baselines, including linear models, ensemble methods and tabular foundation models (TFMs). Our results show that LLMs achieve strong performance in classification tasks under limited data availability, establishing practical zero-training baselines. In contrast, the performance in regression with continuous-valued outputs is poor compared to ML models, likely because regression demands outputs in a large (often infinite) space, and clustering results are similarly limited, which we attribute to the absence of genuine ICL in this setting. Nonetheless, this approach enables rapid, low-overhead data exploration and offers a viable alternative to traditional ML pipelines in business intelligence and exploratory analytics contexts. We further analyze the influence of context size and prompt structure on approximation quality, identifying trade-offs that affect predictive performance. Our findings suggest that LLMs can serve as general-purpose predictive engines for structured data, with clear strengths in classification and significant limitations in regression and clustering.', 'abstract_zh': '大型语言模型（LLMs）最初是为自然语言处理（NLP）开发的，已显示出跨模态和跨领域的泛化潜力。利用其上下文学习（ICL）能力，LLMs可以在无需显式调整下游任务的情况下对结构化输入进行预测任务。在本文中，我们研究了LLMs在小型结构化数据集上的函数近似能力，用于分类、回归和聚类任务。我们评估了最先进的LLMs（GPT-5、GPT-4o、GPT-o3、Gemini-2.5-Flash、DeepSeek-R1）在少量示例提示下的性能，并将其与已建立的机器学习（ML）基线进行了比较，包括线性模型、集成方法和表格基础模型（TFMs）。我们的结果表明，在有限的数据可用性下，LLMs在分类任务中表现优异，建立了实际的零训练基线。相比之下，与ML模型相比，在连续输出的回归任务中的表现较差，可能是因为回归需要在大型（通常是无穷大）空间中的输出，并且聚类结果也受到限制，我们认为这是由于在这种情况下缺乏真正的ICL。尽管如此，这种方法允许快速、低成本的数据探索，并为业务智能和探索性分析提供了一种传统ML管道的可行替代方案。我们进一步分析了上下文大小和提示结构对近似质量的影响，指出了影响预测性能的权衡。我们的研究结果表明，LLMs可以作为结构化数据的通用预测引擎，其在分类方面具有明显优势，在回归和聚类方面则存在显著限制。', 'title_zh': '大型语言模型作为通用预测器？对小型表格数据集的一项实证研究'}
{'arxiv_id': 'arXiv:2508.17291', 'title': 'Meta-R1: Empowering Large Reasoning Models with Metacognition', 'authors': 'Haonan Dong, Haoran Ye, Wenhao Zhu, Kehan Jiang, Guojie Song', 'link': 'https://arxiv.org/abs/2508.17291', 'abstract': 'Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex tasks, exhibiting emergent, human-like thinking patterns. Despite their advances, we identify a fundamental limitation: current LRMs lack a dedicated meta-level cognitive system-an essential faculty in human cognition that enables "thinking about thinking". This absence leaves their emergent abilities uncontrollable (non-adaptive reasoning), unreliable (intermediate error), and inflexible (lack of a clear methodology). To address this gap, we introduce Meta-R1, a systematic and generic framework that endows LRMs with explicit metacognitive capabilities. Drawing on principles from cognitive science, Meta-R1 decomposes the reasoning process into distinct object-level and meta-level components, orchestrating proactive planning, online regulation, and adaptive early stopping within a cascaded framework. Experiments on three challenging benchmarks and against eight competitive baselines demonstrate that Meta-R1 is: (I) high-performing, surpassing state-of-the-art methods by up to 27.3%; (II) token-efficient, reducing token consumption to 15.7% ~ 32.7% and improving efficiency by up to 14.8% when compared to its vanilla counterparts; and (III) transferable, maintaining robust performance across datasets and model backbones.', 'abstract_zh': '大型推理模型中的元认知能力：Meta-R1框架的研究', 'title_zh': 'Meta-R1: 通过元认知增强大规模推理模型'}
{'arxiv_id': 'arXiv:2508.17212', 'title': 'Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Reward', 'authors': 'Xinyu Qin, Ruiheng Yu, Lu Wang', 'link': 'https://arxiv.org/abs/2508.17212', 'abstract': 'Clinical decision support must adapt online under safety constraints. We present an online adaptive tool where reinforcement learning provides the policy, a patient digital twin provides the environment, and treatment effect defines the reward. The system initializes a batch-constrained policy from retrospective data and then runs a streaming loop that selects actions, checks safety, and queries experts only when uncertainty is high. Uncertainty comes from a compact ensemble of five Q-networks via the coefficient of variation of action values with a $\\tanh$ compression. The digital twin updates the patient state with a bounded residual rule. The outcome model estimates immediate clinical effect, and the reward is the treatment effect relative to a conservative reference with a fixed z-score normalization from the training split. Online updates operate on recent data with short runs and exponential moving averages. A rule-based safety gate enforces vital ranges and contraindications before any action is applied. Experiments in a synthetic clinical simulator show low latency, stable throughput, a low expert query rate at fixed safety, and improved return against standard value-based baselines. The design turns an offline policy into a continuous, clinician-supervised system with clear controls and fast adaptation.', 'abstract_zh': '临床决策支持需在安全约束下在线适应。我们提出了一种在线自适应工具，其中强化学习提供策略，患者数字孪生提供环境，治疗效果定义奖励。该系统从 retros 归档数据初始化批量约束策略，然后运行一个流式循环，该循环选择行动、检查安全并在不确定性高时查询专家。不确定性来自通过 tanh 压缩动作值的紧凑型五元 Q 网络集合的标准差。数字孪生利用有界残差规则更新患者状态。结果模型估计即时临床效果，奖励是相对于保守参考的治疗效果，后者采用固定 z 分数标准化。在线更新基于最近数据运行短期操作，并使用指数加权平均值。基于规则的安全门控确保在任何行动之前强制执行关键范围和禁忌症。在合成临床模拟器中的实验表明，低延迟、稳定吞吐量、固定安全下的低专家查询率和优于标准价值基准的较高回报。该设计将离线策略转变为连续的、由临床医生监督的系统，具有明确的控制和快速适应性。', 'title_zh': '基于数字孪生驱动策略与治疗效果优化奖励的强化学习增强在线自适应临床决策支持'}
{'arxiv_id': 'arXiv:2508.17200', 'title': 'Large Language Model-Based Automatic Formulation for Stochastic Optimization Models', 'authors': 'Amirreza Talebi', 'link': 'https://arxiv.org/abs/2508.17200', 'abstract': 'This paper presents the first integrated systematic study on the performance of large language models (LLMs), specifically ChatGPT, to automatically formulate and solve stochastic optimiza- tion problems from natural language descriptions. Focusing on three key categories, joint chance- constrained models, individual chance-constrained models, and two-stage stochastic linear programs (SLP-2), we design several prompts that guide ChatGPT through structured tasks using chain-of- thought and modular reasoning. We introduce a novel soft scoring metric that evaluates the struc- tural quality and partial correctness of generated models, addressing the limitations of canonical and execution-based accuracy. Across a diverse set of stochastic problems, GPT-4-Turbo outperforms other models in partial score, variable matching, and objective accuracy, with cot_s_instructions and agentic emerging as the most effective prompting strategies. Our findings reveal that with well-engineered prompts and multi-agent collaboration, LLMs can facilitate specially stochastic formulations, paving the way for intelligent, language-driven modeling pipelines in stochastic opti- mization.', 'abstract_zh': '本文首次对大型语言模型（LLMs），特别是ChatGPT，进行集成系统的研究，以自动从自然语言描述中制定和解决随机优化问题。我们专注于联合机会约束模型、个体机会约束模型和两阶段随机线性规划（SLP-2）三大关键类别，设计了几种引导ChatGPT完成结构化任务的提示，采用链式推理和模块化推理。我们引入了一种新颖的软评分度量标准，用于评估生成模型的结构性质量和部分正确性，克服了传统和执行基于准确性的局限性。在一系列多样化的随机问题中，GPT-4-Turbo在部分得分、变量匹配和目标准确性方面优于其他模型，cot_s_instructions和agentic提示策略最为有效。我们的研究发现，通过精心设计的提示和多智能体协作，LLMs能够促进特有随机模型的制定，为随机优化的语言驱动建模流水线开辟了途径。', 'title_zh': '基于大型语言模型的随机优化模型自动构建方法'}
{'arxiv_id': 'arXiv:2508.17188', 'title': 'PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs', 'authors': 'Zhilin Zhang, Xiang Zhang, Jiaqi Wei, Yiwei Xu, Chenyu You', 'link': 'https://arxiv.org/abs/2508.17188', 'abstract': 'Multi-agent systems built upon large language models (LLMs) have demonstrated remarkable capabilities in tackling complex compositional tasks. In this work, we apply this paradigm to the paper-to-poster generation problem, a practical yet time-consuming process faced by researchers preparing for conferences. While recent approaches have attempted to automate this task, most neglect core design and aesthetic principles, resulting in posters that require substantial manual refinement. To address these design limitations, we propose PosterGen, a multi-agent framework that mirrors the workflow of professional poster designers. It consists of four collaborative specialized agents: (1) Parser and Curator agents extract content from the paper and organize storyboard; (2) Layout agent maps the content into a coherent spatial layout; (3) Stylist agents apply visual design elements such as color and typography; and (4) Renderer composes the final poster. Together, these agents produce posters that are both semantically grounded and visually appealing. To evaluate design quality, we introduce a vision-language model (VLM)-based rubric that measures layout balance, readability, and aesthetic coherence. Experimental results show that PosterGen consistently matches in content fidelity, and significantly outperforms existing methods in visual designs, generating posters that are presentation-ready with minimal human refinements.', 'abstract_zh': '基于大型语言模型的多agent系统在论文转海报生成中的应用：一种遵循专业设计师工作流程的多agent框架及其评价方法', 'title_zh': 'PosterGen: 基于多智能体LLM的美观意识论文转海报生成'}
{'arxiv_id': 'arXiv:2508.17094', 'title': 'PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflows', 'authors': 'Emmanuel O. Badmus, Peng Sang, Dimitrios Stamoulis, Amritanshu Pandey', 'link': 'https://arxiv.org/abs/2508.17094', 'abstract': 'Due to the rapid pace of electrification and decarbonization, distribution grid (DG) operation and planning are becoming more complex, necessitating advanced computational analyses to ensure grid reliability and resilience. State-of-the-art DG analyses rely on disparate workflows of complex models, functions, and data pipelines, which require expert knowledge and are challenging to automate. Many small-scale utilities and cooperatives lack a large R&D workforce and therefore cannot use advanced analysis at scale. To address this gap, we develop a novel agentic AI system, PowerChain, to solve unseen DG analysis tasks via automated agentic orchestration and large language models (LLMs) function-calling. Given a natural language query, PowerChain dynamically generates and executes an ordered sequence of domain-aware functions guided by the semantics of an expert-built power systems function pool and a select reference set of known, expert-generated workflow-query pairs. Our results show that PowerChain can produce expert-level workflows with both GPT-5 and open-source Qwen models on complex, unseen DG analysis tasks operating on real utility data.', 'abstract_zh': '由于电气化进程和去碳化进程的加速，配电网络（DG）的运行与规划变得更加复杂，需要先进的计算分析来确保电网的可靠性和韧性。当前先进的DG分析依赖于复杂模型、函数和数据管道的不统一工作流程，这些流程需要专家知识并且难以自动化。许多小型电力企业和合作社缺乏大规模的研发团队，因此无法进行大规模的高级分析。为了弥补这一缺口，我们开发了一种新的代理型AI系统PowerChain，通过自动代理编排和大规模语言模型（LLMs）函数调用解决未知的DG分析任务。给定自然语言查询，PowerChain动态生成并执行由专家构建的电力系统函数池语义指导下的、按顺序排列的领域特定函数序列，并参考一组选定的专家生成的工作流-查询对。我们的结果表明，PowerChain能够在复杂、未知的DG分析任务上生成与GPT-5和开源Qwen模型同等水平的工作流，这些任务基于实际的电力数据进行操作。', 'title_zh': 'PowerChain：使用自主人工智能工作流的配电网络分析自动化'}
{'arxiv_id': 'arXiv:2508.16846', 'title': 'Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs', 'authors': 'Katherine Atwell, Pedram Heydari, Anthony Sicilia, Malihe Alikhani', 'link': 'https://arxiv.org/abs/2508.16846', 'abstract': "Sycophancy, or overly agreeable or flattering behavior, is a documented issue in large language models (LLMs), and is critical to understand in the context of human/AI collaboration. Prior works typically quantify sycophancy by measuring shifts in behavior or impacts on accuracy, but neither metric characterizes shifts in rationality, and accuracy measures can only be used in scenarios with a known ground truth. In this work, we utilize a Bayesian framework to quantify sycophancy as deviations from rational behavior when presented with user perspectives, thus distinguishing between rational and irrational updates based on the introduction of user perspectives. In comparison to other methods, this approach allows us to characterize excessive behavioral shifts, even for tasks that involve inherent uncertainty or do not have a ground truth. We study sycophancy for 3 different tasks, a combination of open-source and closed LLMs, and two different methods for probing sycophancy. We also experiment with multiple methods for eliciting probability judgments from LLMs. We hypothesize that probing LLMs for sycophancy will cause deviations in LLMs' predicted posteriors that will lead to increased Bayesian error. Our findings indicate that: 1) LLMs are not Bayesian rational, 2) probing for sycophancy results in significant increases to the predicted posterior in favor of the steered outcome, 3) sycophancy sometimes results in increased Bayesian error, and in a small number of cases actually decreases error, and 4) changes in Bayesian error due to sycophancy are not strongly correlated in Brier score, suggesting that studying the impact of sycophancy on ground truth alone does not fully capture errors in reasoning due to sycophancy.", 'abstract_zh': 'overly agreeable或奉承行为在大型语言模型（LLMs）中是一个已知问题，并且在人类/AI协作的背景下理解这一点至关重要。先前的工作通常通过衡量行为变化或对准确率的影响来量化奉承行为，但这些度量标准都不能刻画理性变化，而准确率度量只能在有已知真实情况下使用。在本工作中，我们利用贝叶斯框架量化当面对用户视角时的奉承行为作为理性行为的偏差，从而根据用户视角的引入区分理性和非理性的更新。与其它方法相比，此方法允许我们表征行为变化的过度现象，即使对于包含内在不确定性或没有真实情况的任务也是如此。我们研究了三个不同的任务，包括开源和闭源LLMs，并使用两种不同的方法来探测奉承行为。我们还尝试了多种方法从LLMs中获取概率判断。我们假设探测LLMs的奉承行为会导致预测后验概率的变化，从而增加贝叶斯误差。我们的发现表明：1) LLMs不是贝叶斯理性，2) 探测奉承行为导致预测后验概率显著增加，倾向于导向行为的结果，3) 奉承行为有时会导致贝叶斯误差增加，但在少数情况下实际上减少了误差，4) 由于奉承行为导致的贝叶斯误差变化在Brier分数中并不强烈相关，这表明单独研究奉承行为对真实情况的影响并不能充分捕捉由奉承行为引起的推理错误。', 'title_zh': '量化逢迎行为作为大型语言模型中贝叶斯理性偏差的程度'}
{'arxiv_id': 'arXiv:2508.16777', 'title': 'Evaluation and LLM-Guided Learning of ICD Coding Rationales', 'authors': 'Mingyang Li, Viktor Schlegel, Tingting Mu, Wuraola Oyewusi, Kai Kang, Goran Nenadic', 'link': 'https://arxiv.org/abs/2508.16777', 'abstract': "Automated clinical coding involves mapping unstructured text from Electronic Health Records (EHRs) to standardized code systems such as the International Classification of Diseases (ICD). While recent advances in deep learning have significantly improved the accuracy and efficiency of ICD coding, the lack of explainability in these models remains a major limitation, undermining trust and transparency. Current explorations about explainability largely rely on attention-based techniques and qualitative assessments by physicians, yet lack systematic evaluation using consistent criteria on high-quality rationale datasets, as well as dedicated approaches explicitly trained to generate rationales for further enhancing explanation. In this work, we conduct a comprehensive evaluation of the explainability of the rationales for ICD coding through two key lenses: faithfulness that evaluates how well explanations reflect the model's actual reasoning and plausibility that measures how consistent the explanations are with human expert judgment. To facilitate the evaluation of plausibility, we construct a new rationale-annotated dataset, offering denser annotations with diverse granularity and aligns better with current clinical practice, and conduct evaluation across three types of rationales of ICD coding. Encouraged by the promising plausibility of LLM-generated rationales for ICD coding, we further propose new rationale learning methods to improve the quality of model-generated rationales, where rationales produced by prompting LLMs with/without annotation examples are used as distant supervision signals. We empirically find that LLM-generated rationales align most closely with those of human experts. Moreover, incorporating few-shot human-annotated examples not only further improves rationale generation but also enhances rationale-learning approaches.", 'abstract_zh': '自动临床编码涉及将电子健康记录（EHRs）中的非结构化文本映射到标准化代码系统，如国际疾病分类（ICD）。尽管近年来深度学习的进步显著提高了ICD编码的准确性和效率，但这些模型缺乏可解释性仍然是一个主要限制，这削弱了人们对模型的信任和透明度。当前关于可解释性的探索主要依赖于注意力机制技术和医师的定性评估，但缺乏使用一致标准对高质量理性数据集进行系统评估，以及专门训练以生成进一步增强解释的理性生成方法。在本文中，我们通过两个关键视角对ICD编码的理性解释的可解释性进行全面评估：忠实度，评估解释如何反映模型的实际推理；以及合理性，衡量解释与人类专家判断的一致性程度。为了便于评估合理性，我们构建了一个新的带有注释的理性数据集，提供了更密集的、粒度多样化的注释，并且更好地与当前临床实践相契合，进而对ICD编码的三种类型注释进行了评估。受到LLM生成的ICD编码理性注释具有较高合理性的启发，我们进一步提出新的注释学习方法以提高模型生成理性注释的质量，其中，通过提示LLM生成带有/不带有注释示例的理性注释被用作远处监督信号。我们的实证研究表明，LLM生成的理性注释最接近人类专家的理性注释。此外，引入少量的人工标注示例不仅进一步提高了注释生成的质量，还增强了注释学习方法。', 'title_zh': 'LLM引导的学习ICD编码推理评价'}
{'arxiv_id': 'arXiv:2508.18244', 'title': 'Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data', 'authors': 'Chu-Cheng Lin, Daiyi Peng, Yifeng Lu, Ming Zhang, Eugene Ie', 'link': 'https://arxiv.org/abs/2508.18244', 'abstract': 'Reliably composing Large Language Models (LLMs) for complex, multi-step workflows remains a significant challenge. The dominant paradigm-optimizing discrete prompts in a pipeline-is notoriously brittle and struggles to enforce the formal compliance required for structured tasks. We introduce Type-Compliant Adaptation Cascades (TACs), a framework that recasts workflow adaptation as learning typed probabilistic programs. TACs treats the entire workflow, which is composed of parameter-efficiently adapted LLMs and deterministic logic, as an unnormalized joint distribution. This enables principled, gradient-based training even with latent intermediate structures. We provide theoretical justification for our tractable optimization objective, proving that the optimization bias vanishes as the model learns type compliance. Empirically, TACs significantly outperforms state-of-the-art prompt-optimization baselines. Gains are particularly pronounced on structured tasks, improving MGSM-SymPy from $57.1\\%$ to $75.9\\%$ for a 27B model, MGSM from $1.6\\%$ to $27.3\\%$ for a 7B model. TACs offers a robust and theoretically grounded paradigm for developing reliable, task-compliant LLM systems.', 'abstract_zh': '可靠地组合大型语言模型（LLMs）以支持复杂的多步工作流仍是一项重大挑战。我们引入了类型合规适应cascade（TACs）框架，将工作流适应重新表述为学习类型概率程序的过程。TACs 将由参数高效适应的LLMs和确定性逻辑组成的整个工作流视为未标准化的联合分布，这使得即使在存在潜在中间结构的情况下，也能实现原理性的梯度训练。我们为我们的可实现优化目标提供了理论上的依据，证明了优化偏差随模型学习类型合规而消失。实验结果显示，TACs 显著优于最先进的提示优化基准模型。在结构化任务上尤其明显，27B模型的MGSM-SymPy从57.1%提升到75.9%，7B模型的MGSM从1.6%提升到27.3%。TACs 提供了一个稳健且具有理论依据的框架，用于开发可靠的任务合规LLM系统。', 'title_zh': '类型遵从适配cascade：适应程序化LM工作流的数据'}
{'arxiv_id': 'arXiv:2508.18240', 'title': 'MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues via Arena-style and Rubrics Protocols', 'authors': 'Yuhao Du, Qianwei Huang, Guo Zhu, Zhanchen Dai, Sunian Chen, Qiming Zhu, Yuhao Zhang, Li Zhou, Benyou Wang', 'link': 'https://arxiv.org/abs/2508.18240', 'abstract': 'The rapid advancement of speech-to-speech (S2S) large language models (LLMs) has significantly improved real-time spoken interaction. However, current evaluation frameworks remain inadequate for assessing performance in complex, multi-turn dialogues. To address this, we introduce MTalk-Bench, a multi-turn S2S benchmark covering three core dimensions: Semantic Information, Paralinguistic Information, and Ambient Sound. Each dimension includes nine realistic scenarios, along with targeted tasks to assess specific capabilities such as reasoning. Our dual-method evaluation framework combines Arena-style evaluation (pairwise comparison) and Rubrics-based evaluation (absolute scoring) for relative and absolute assessment. The benchmark includes both model and human outputs, evaluated by human evaluators and LLMs. Experimental results reveal two sets of findings. Overall performance of S2S LLMs: (1) models excel at semantic information processing yet underperform on paralinguistic information and ambient sounds perception; (2) models typically regain coherence by increasing response length, sacrificing efficiency in multi-turn dialogues; (3) modality-aware, task-specific designs outperform brute scaling. Evaluation framework and reliability: (1) Arena and Rubrics yield consistent, complementary rankings, but reliable distinctions emerge only when performance gaps are large; (2) LLM-as-a-judge aligns with humans when gaps are clear or criteria explicit, but exhibits position and length biases and is reliable on nonverbal evaluation only with text annotations. These results highlight current limitations in S2S evaluation and the need for more robust, speech-aware assessment frameworks.', 'abstract_zh': '面向多轮对话的speech-to-speech大语言模型多维度基准MTalk-Bench', 'title_zh': 'MTalk-Bench：通过擂台风格和评価指标协议评估多轮对话中的言语到言语模型'}
{'arxiv_id': 'arXiv:2508.18183', 'title': 'Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios', 'authors': 'Luana Bulla, Gabriele Tuccio, Misael Mongiovì, Aldo Gangemi', 'link': 'https://arxiv.org/abs/2508.18183', 'abstract': 'Translating natural languages into sign languages is a highly complex and underexplored task. Despite growing interest in accessibility and inclusivity, the development of robust translation systems remains hindered by the limited availability of parallel corpora which align natural language with sign language data. Existing methods often struggle to generalize in these data-scarce environments, as the few datasets available are typically domain-specific, lack standardization, or fail to capture the full linguistic richness of sign languages. To address this limitation, we propose Advanced Use of LLMs for Sign Language Translation (AulSign), a novel method that leverages Large Language Models via dynamic prompting and in-context learning with sample selection and subsequent sign association. Despite their impressive abilities in processing text, LLMs lack intrinsic knowledge of sign languages; therefore, they are unable to natively perform this kind of translation. To overcome this limitation, we associate the signs with compact descriptions in natural language and instruct the model to use them. We evaluate our method on both English and Italian languages using SignBank+, a recognized benchmark in the field, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior performance compared to state-of-the-art models in low-data scenario. Our findings demonstrate the effectiveness of AulSign, with the potential to enhance accessibility and inclusivity in communication technologies for underrepresented linguistic communities.', 'abstract_zh': '自然语言到手语的翻译是一项高度复杂且尚未充分探索的任务。尽管无障碍和平等性日益受到关注，但由于自然语言与手语数据对齐的平行语料库有限，开发健壮的翻译系统仍受阻碍。现有方法往往难以在这些数据稀缺的环境中泛化，因为可用的数据集通常是领域特定的、缺乏标准化或未能捕捉手语语言的全部丰富性。为了解决这一局限，我们提出了基于大语言模型的手语翻译高级应用（AulSign）方法，该方法通过动态提示和上下文学习结合样本选择和后续手语关联来利用大语言模型。尽管大语言模型在处理文本方面表现出色，但它们缺乏手语固有的知识；因此，它们无法本外地执行这种翻译。为克服这一限制，我们将手语与紧凑的手语自然语言描述关联，并指示模型使用这些描述。我们在SignBank+和意大利LaCAM CNR-ISTC数据集上分别使用英语和意大利语评估了该方法，SignBank+是该领域公认的基准。我们的研究表明，在数据稀少的情况下，AulSign方法相较于最新模型具有更好的性能。我们的研究结果表明，AulSign的有效性，其有可能提升未充分代表语言社区在交流技术中的无障碍和平等性。', 'title_zh': '利用大规模语言模型在低资源场景中实现准确的手语翻译'}
{'arxiv_id': 'arXiv:2508.18182', 'title': 'AdLoCo: adaptive batching significantly improves communications efficiency and convergence for Large Language Models', 'authors': 'Nikolay Kutuzov, Makar Baderko, Stepan Kulibaba, Artem Dzhalilov, Daniel Bobrov, Maxim Mashtaler, Alexander Gasnikov', 'link': 'https://arxiv.org/abs/2508.18182', 'abstract': 'Scaling distributed training of Large Language Models (LLMs) requires not only algorithmic advances but also efficient utilization of heterogeneous hardware resources. While existing methods such as DiLoCo have demonstrated promising results, they often fail to fully exploit computational clusters under dynamic workloads. To address this limitation, we propose a three-stage method that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo, and switch mode mechanism. MIT allows individual nodes to run multiple lightweight training streams with different model instances in parallel and merge them to combine knowledge, increasing throughput and reducing idle time. Adaptive Batched DiLoCo dynamically adjusts local batch sizes to balance computation and communication, substantially lowering synchronization delays. Switch mode further stabilizes training by seamlessly introducing gradient accumulation once adaptive batch sizes grow beyond hardware-friendly limits. Together, these innovations improve both convergence speed and system efficiency. We also provide a theoretical estimate of the number of communications required for the full convergence of a model trained using our method.', 'abstract_zh': '分布式训练大规模语言模型（LLMs）不仅需要算法上的进步，还需要有效地利用异构硬件资源。为了解决现有方法如DiLoCo在动态工作负载下未能充分利用计算集群的问题，我们提出了一种三阶段方法，结合多实例训练（MIT）、自适应批量DiLoCo及其切换模式机制。MIT允许多个节点并行运行不同模型实例的多个轻量级训练流，并合并这些流以增加吞吐量并减少空闲时间。自适应批量DiLoCo动态调整本地批次大小以平衡计算和通信，显著降低同步延迟。切换模式进一步通过在自适应批次大小超过硬件友好限制时无缝引入梯度累积来稳定训练。这些创新共同提高了收敛速度和系统效率。我们还提供了对使用我们方法训练的模型达到完全收敛所需通信次数的理论估计。', 'title_zh': 'AdLoCo:自适应批量处理显著提高大型语言模型的通信效率和收敛性'}
{'arxiv_id': 'arXiv:2508.18148', 'title': 'Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation', 'authors': 'Haijian Ma, Daizong Liu, Xiaowen Cai, Pan Zhou, Yulai Xie', 'link': 'https://arxiv.org/abs/2508.18148', 'abstract': 'Intrusion Detection Systems (IDS) play a crucial role in network security defense. However, a significant challenge for IDS in training detection models is the shortage of adequately labeled malicious samples. To address these issues, this paper introduces a novel semi-supervised framework \\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs) with Large Language Models (LLMs) to enhance malicious code generation and SQL Injection (SQLi) detection capabilities in few-sample learning scenarios. Specifically, our framework adopts a collaborative training paradigm where: (1) the GAN-based discriminator improves malicious pattern recognition through adversarial learning with generated samples and limited real samples; and (2) the LLM-based generator refines the quality of malicious code synthesis using reward signals from the discriminator. The experimental results demonstrate that even with a limited number of labeled samples, our training framework is highly effective in enhancing both malicious code generation and detection capabilities. This dual enhancement capability offers a promising solution for developing adaptive defense systems capable of countering evolving cyber threats.', 'abstract_zh': '入侵检测系统（IDS）在网络安全性防卫中发挥着关键作用。然而，IDS在训练检测模型时面临的显著挑战之一是缺乏足够的标记恶意样本。为解决这些问题，本文提出了一种新颖的半监督框架**GANGRL-LLM**，该框架结合生成对抗网络（GANs）与大型语言模型（LLMs），以增强在少量样本学习场景中的恶意代码生成和SQL注入（SQLi）检测能力。具体而言，我们的框架采用协作训练范式：（1）基于GAN的鉴别器通过与生成样本和有限的真实样本进行对抗学习来改进恶意模式识别；（2）基于LLM的生成器使用来自鉴别器的奖励信号来精炼恶意代码合成的质量。实验结果表明，即使样本数量有限，我们的训练框架在提高恶意代码生成和检测能力方面非常有效。这种双重增强能力为开发能够应对不断演变的网络威胁的自适应防御系统提供了有前景的解决方案。', 'title_zh': '从少量样本学习：一种高质量恶意代码生成的新方法'}
{'arxiv_id': 'arXiv:2508.18132', 'title': 'Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations', 'authors': 'Hung-Chun Hsu, Yuan-Ching Kuo, Chao-Han Huck Yang, Szu-Wei Fu, Hanrong Ye, Hongxu Yin, Yu-Chiang Frank Wang, Ming-Feng Tsai, Chuan-Ju Wang', 'link': 'https://arxiv.org/abs/2508.18132', 'abstract': "The rapid evolution of e-commerce has exposed the limitations of traditional product retrieval systems in managing complex, multi-turn user interactions. Recent advances in multimodal generative retrieval -- particularly those leveraging multimodal large language models (MLLMs) as retrievers -- have shown promise. However, most existing methods are tailored to single-turn scenarios and struggle to model the evolving intent and iterative nature of multi-turn dialogues when applied naively. Concurrently, test-time scaling has emerged as a powerful paradigm for improving large language model (LLM) performance through iterative inference-time refinement. Yet, its effectiveness typically relies on two conditions: (1) a well-defined problem space (e.g., mathematical reasoning), and (2) the model's ability to self-correct -- conditions that are rarely met in conversational product search. In this setting, user queries are often ambiguous and evolving, and MLLMs alone have difficulty grounding responses in a fixed product corpus. Motivated by these challenges, we propose a novel framework that introduces test-time scaling into conversational multimodal product retrieval. Our approach builds on a generative retriever, further augmented with a test-time reranking (TTR) mechanism that improves retrieval accuracy and better aligns results with evolving user intent throughout the dialogue. Experiments across multiple benchmarks show consistent improvements, with average gains of 14.5 points in MRR and 10.6 points in nDCG@1.", 'abstract_zh': '电商的快速演变揭示了传统产品检索系统在管理复杂多轮用户交互方面的局限性。近年来，多模态生成检索技术，尤其是利用多模态大型语言模型（MLLMs）作为检索器的技术，展现了潜力。然而，现有方法大多针对单轮场景，未经调整地应用于多轮对话时难以建模用户意图的演变和迭代性。同时，测试时缩放已成为通过迭代推理时精炼提高大型语言模型（LLM）性能的强大范式。然而，其有效性通常依赖于两个条件：(1) 井定义的问题空间（例如，数学推理），和(2) 模型自我纠正的能力——这两个条件在会话产品搜索中很少满足。在这种情境下，用户查询往往是模糊且不断变化的，仅靠MLLMs难以将回复与固定的产品库对齐。受这些挑战的启发，我们提出了一种新的框架，将测试时缩放引入会话多模态产品检索。该方法基于一个生成式检索器，并进一步增强了一个测试时重排（TTR）机制，以提高检索准确性，并在整个对话过程中更好地使结果与用户意图保持一致。多项基准试验显示了一致的改进，平均MRR提高了14.5点，nDCG@1提高了10.6点。', 'title_zh': '基于多模态对话推荐的生成检索测试时扩增策略'}
{'arxiv_id': 'arXiv:2508.18124', 'title': 'CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics', 'authors': 'Weida Wang, Dongchen Huang, Jiatong Li, Tengchao Yang, Ziyang Zheng, Di Zhang, Dong Han, Benteng Chen, Binzhao Luo, Zhiyu Liu, Kunling Liu, Zhiyuan Gao, Shiqi Geng, Wei Ma, Jiaming Su, Xin Li, Shuchen Pu, Yuhan Shui, Qianjia Cheng, Zhihao Dou, Dongfei Cui, Changyong He, Jin Zeng, Zeke Xie, Mao Su, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang, Lei Bai, Yunqi Cai, Xi Dai, Shufei Zhang, Jinguang Cheng, Zhong Fang, Hongming Weng', 'link': 'https://arxiv.org/abs/2508.18124', 'abstract': 'We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark. CMPhysBench is composed of more than 520 graduate-level meticulously curated questions covering both representative subfields and foundational theoretical frameworks of condensed matter physics, such as magnetism, superconductivity, strongly correlated systems, etc. To ensure a deep understanding of the problem-solving process,we focus exclusively on calculation problems, requiring LLMs to independently generate comprehensive solutions. Meanwhile, leveraging tree-based representations of expressions, we introduce the Scalable Expression Edit Distance (SEED) score, which provides fine-grained (non-binary) partial credit and yields a more accurate assessment of similarity between prediction and ground-truth. Our results show that even the best models, Grok-4, reach only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a significant capability gap, especially for this practical and frontier domain relative to traditional physics. The code anddataset are publicly available at this https URL.', 'abstract_zh': 'CMPhysBench: 一种评估大型语言模型在凝聚态物理学 proficiency 的新型基准', 'title_zh': 'CMPhysBench: 一种评估凝聚态物理学大型语言模型性能的标准测试工具'}
{'arxiv_id': 'arXiv:2508.18106', 'title': 'A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code', 'authors': 'Keke Lian, Bin Wang, Lei Zhang, Libo Chen, Junjie Wang, Ziming Zhao, Yujiu Yang, Haotong Duan, Haoran Zhao, Shuang Liao, Mingda Guo, Jiazheng Quan, Yilu Zhong, Chenhao He, Zichuan Chen, Jie Wu, Haoling Li, Zhaoxuan Li, Jiongchi Yu, Hui Li, Dong Zhang', 'link': 'https://arxiv.org/abs/2508.18106', 'abstract': "The increasing adoption of large language models (LLMs) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks are inadequate, as they focus on isolated code snippets, employ unstable evaluation methods that lack reproducibility, and fail to connect the quality of input context with the security of the output. To address these gaps, we introduce A.S.E (AI Code Generation Security Evaluation), a benchmark for repository-level secure code generation. A.S.E constructs tasks from real-world repositories with documented CVEs, preserving full repository context like build systems and cross-file dependencies. Its reproducible, containerized evaluation framework uses expert-defined rules to provide stable, auditable assessments of security, build quality, and generation stability. Our evaluation of leading LLMs on A.S.E reveals three key findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The security gap between proprietary and open-source models is narrow; Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise, ``fast-thinking'' decoding strategies consistently outperform complex, ``slow-thinking'' reasoning for security patching.", 'abstract_zh': '大型语言模型在软件工程中的日益广泛应用 necessitates 严格的生成代码安全性评估。然而，现有基准不充分，因为它们专注于孤立的代码片段、采用缺乏重现性的不稳定评估方法，并且无法将输入上下文的质量与输出的安全性联系起来。为解决这些差距，我们介绍了A.S.E（AI代码生成安全性评估）基准，用于存储库级别的安全代码生成。A.S.E 从包含已记录的CVE的真实存储库中构建任务，保留完整的存储库上下文，如构建系统和跨文件依赖关系。其可再现的容器化评估框架使用专家定义的规则提供稳定、可审计的安全性、构建质量和生成稳定性评估。我们在A.S.E上对领先的大规模语言模型进行评估揭示了三个关键发现：(1) Claude-3.7-Sonnet表现出最佳的整体性能。(2) 商用软件和开源模型之间的安全性差距较小；Qwen3-235B-A22B-Instruct达到了最高的安全性评分。(3) 简洁的“快速思考”解码策略始终优于复杂的“缓慢思考”推理策略，特别是在安全修补方面。', 'title_zh': 'A.S.E: 一个用于评估AI生成代码安全性的仓库级基准'}
{'arxiv_id': 'arXiv:2508.18090', 'title': 'Named Entity Recognition of Historical Text via Large Language Model', 'authors': 'Shibingfeng Zhang, Giovanni Colavizza', 'link': 'https://arxiv.org/abs/2508.18090', 'abstract': 'Large language models have demonstrated remarkable versatility across a wide range of natural language processing tasks and domains. One such task is Named Entity Recognition (NER), which involves identifying and classifying proper names in text, such as people, organizations, locations, dates, and other specific entities. NER plays a crucial role in extracting information from unstructured textual data, enabling downstream applications such as information retrieval from unstructured text.\nTraditionally, NER is addressed using supervised machine learning approaches, which require large amounts of annotated training data. However, historical texts present a unique challenge, as the annotated datasets are often scarce or nonexistent, due to the high cost and expertise required for manual labeling. In addition, the variability and noise inherent in historical language, such as inconsistent spelling and archaic vocabulary, further complicate the development of reliable NER systems for these sources.\nIn this study, we explore the feasibility of applying LLMs to NER in historical documents using zero-shot and few-shot prompting strategies, which require little to no task-specific training data. Our experiments, conducted on the HIPE-2022 (Identifying Historical People, Places and other Entities) dataset, show that LLMs can achieve reasonably strong performance on NER tasks in this setting. While their performance falls short of fully supervised models trained on domain-specific annotations, the results are nevertheless promising. These findings suggest that LLMs offer a viable and efficient alternative for information extraction in low-resource or historically significant corpora, where traditional supervised methods are infeasible.', 'abstract_zh': '大型语言模型在多种自然语言处理任务和领域中展现了显著的灵活性。其中一个任务是命名实体识别（NER），它涉及在文本中识别和分类专有名词，如人名、组织、地点、日期及其他具体实体。NER在从非结构化文本数据中提取信息方面发挥着关键作用，使下游应用如非结构化文本的信息检索成为可能。\n传统上，NER使用监督机器学习方法处理，这需要大量的标注训练数据。然而，历史文本提出了一项独特挑战，因为标注数据集往往稀缺或不存在，这主要是由于人工标注所需的成本和专业知识。此外，历史语言中固有的变异性与噪音，如不一致的拼写和过时的词汇，也进一步增加了为这些来源开发可靠的NER系统的难度。\n在本研究中，我们探讨了使用零样本和少量样本提示策略将大型语言模型应用于历史文件中的NER的可行性。我们的实验在HIPE-2022（识别历史人物、地点和其他实体）数据集上进行，结果显示大型语言模型在这种环境中执行NER任务时能够达到相当强的效果。尽管其性能不如在领域特定标注上进行完全监督训练的模型，但结果仍然令人鼓舞。这些发现表明，大型语言模型为在资源匮乏或具有历史意义的语料库中提取信息提供了一种可行且高效的替代方法，而传统的方法在这种情况下不可行。', 'title_zh': '基于大型语言模型的古文命名实体识别'}
{'arxiv_id': 'arXiv:2508.18048', 'title': 'HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data', 'authors': 'Jiyoon Myung, Jihyeon Park, Joohyung Han', 'link': 'https://arxiv.org/abs/2508.18048', 'abstract': 'User queries in real-world recommendation systems often combine structured constraints (e.g., category, attributes) with unstructured preferences (e.g., product descriptions or reviews). We introduce HyST (Hybrid retrieval over Semi-structured Tabular data), a hybrid retrieval framework that combines LLM-powered structured filtering with semantic embedding search to support complex information needs over semi-structured tabular data. HyST extracts attribute-level constraints from natural language using large language models (LLMs) and applies them as metadata filters, while processing the remaining unstructured query components via embedding-based retrieval. Experiments on a semi-structured benchmark show that HyST consistently outperforms tradtional baselines, highlighting the importance of structured filtering in improving retrieval precision, offering a scalable and accurate solution for real-world user queries.', 'abstract_zh': '现实世界推荐系统中的用户查询通常结合了结构化约束（如类别、属性）与非结构化偏好（如产品描述或评论）。我们提出了HyST（半结构化表数据混合检索），这是一种将大语言模型驱动的结构化过滤与语义嵌入搜索结合的混合检索框架，用于支持半结构化表数据上的复杂信息需求。HyST 使用大语言模型从自然语言中提取属性级约束，并将其作为元数据过滤器应用，同时通过嵌入式检索处理其余非结构化查询组件。在半结构化基准测试上的实验显示，HyST 一贯优于传统Baseline，突显了结构化过滤在提高检索精度方面的重要性，为现实世界用户查询提供了可扩展且准确的解决方案。', 'title_zh': 'HyST：半结构化表格数据的LLM驱动混合检索'}
{'arxiv_id': 'arXiv:2508.17990', 'title': 'Automating Conflict-Aware ACL Configurations with Natural Language Intents', 'authors': 'Wenlong Ding, Jianqiang Li, Zhixiong Niu, Huangxun Chen, Yongqiang Xiong, Hong Xu', 'link': 'https://arxiv.org/abs/2508.17990', 'abstract': "ACL configuration is essential for managing network flow reachability, yet its complexity grows significantly with topologies and pre-existing rules. To carry out ACL configuration, the operator needs to (1) understand the new configuration policies or intents and translate them into concrete ACL rules, (2) check and resolve any conflicts between the new and existing rules, and (3) deploy them across the network. Existing systems rely heavily on manual efforts for these tasks, especially for the first two, which are tedious, error-prone, and impractical to scale.\nWe propose Xumi to tackle this problem. Leveraging LLMs with domain knowledge of the target network, Xumi automatically and accurately translates the natural language intents into complete ACL rules to reduce operators' manual efforts. Xumi then detects all potential conflicts between new and existing rules and generates resolved intents for deployment with operators' guidance, and finally identifies the best deployment plan that minimizes the rule additions while satisfying all intents. Evaluation shows that Xumi accelerates the entire configuration pipeline by over 10x compared to current practices, addresses O(100) conflicting ACLs and reduces rule additions by ~40% in modern cloud network.", 'abstract_zh': 'Xumi：利用领域知识的LLM自动实现ACL配置', 'title_zh': '基于自然语言意图的自动冲突感知ACL配置'}
{'arxiv_id': 'arXiv:2508.17953', 'title': 'Understanding Subword Compositionality of Large Language Models', 'authors': 'Qiwei Peng, Yekun Chai, Anders Søgaard', 'link': 'https://arxiv.org/abs/2508.17953', 'abstract': 'Large language models (LLMs) take sequences of subwords as input, requiring them to effective compose subword representations into meaningful word-level representations. In this paper, we present a comprehensive set of experiments to probe how LLMs compose subword information, focusing on three key aspects: structural similarity, semantic decomposability, and form retention. Our analysis of the experiments suggests that these five LLM families can be classified into three distinct groups, likely reflecting difference in their underlying composition strategies. Specifically, we observe (i) three distinct patterns in the evolution of structural similarity between subword compositions and whole-word representations across layers; (ii) great performance when probing layer by layer their sensitivity to semantic decompositionality; and (iii) three distinct patterns when probing sensitivity to formal features, e.g., character sequence length. These findings provide valuable insights into the compositional dynamics of LLMs and highlight different compositional pattens in how LLMs encode and integrate subword information.', 'abstract_zh': '大规模语言模型（LLMs）以子词序列为输入，需要有效地将子词表示组合成有意义的词级表示。在本文中，我们进行了一系列全面的实验来探究LLMs如何组合子词信息，重点关注三个方面：结构相似性、语义可分解性和形式保留。我们的实验分析表明，这五类LLM可以被归类为三个不同的组别，这很可能反映了它们在基础组合策略方面的差异。具体而言，我们观察到(i) 子词组合与整个词表示之间结构相似性在不同层中的三种不同的演变模式；(ii) 在逐层探测试验其对语义可分解性的敏感度方面表现出色；以及(iii) 在探测试验对其形式特征（例如字符序列长度）的敏感度时表现出三种不同的模式。这些发现为理解LLMs的组合动态提供了宝贵见解，并强调了LLMs在编码和整合子词信息时不同的组合模式。', 'title_zh': '大规模语言模型的子词组合性理解'}
{'arxiv_id': 'arXiv:2508.17948', 'title': 'Debiasing Multilingual LLMs in Cross-lingual Latent Space', 'authors': 'Qiwei Peng, Guimin Hu, Yekun Chai, Anders Søgaard', 'link': 'https://arxiv.org/abs/2508.17948', 'abstract': 'Debiasing techniques such as SentDebias aim to reduce bias in large language models (LLMs). Previous studies have evaluated their cross-lingual transferability by directly applying these methods to LLM representations, revealing their limited effectiveness across languages. In this work, we therefore propose to perform debiasing in a joint latent space rather than directly on LLM representations. We construct a well-aligned cross-lingual latent space using an autoencoder trained on parallel TED talk scripts. Our experiments with Aya-expanse and two debiasing techniques across four languages (English, French, German, Dutch) demonstrate that a) autoencoders effectively construct a well-aligned cross-lingual latent space, and b) applying debiasing techniques in the learned cross-lingual latent space significantly improves both the overall debiasing performance and cross-lingual transferability.', 'abstract_zh': 'Debiasing技术如SentDebias旨在减少大型语言模型（LLMs）中的偏见。 previous studies通过直接将这些方法应用于LLM表示来评估它们的跨语言可迁移性，揭示了它们跨语言之间的有限有效性。因此，本工作提出在联合潜在空间中进行去偏见，而不是直接在LLM表示上进行。我们通过在平行TED演讲脚本上训练自编码器来构建一个良好的跨语言对齐的潜在空间。我们在Aya-expanse和两种去偏见技术在四种语言（英语、法语、德语、荷兰语）上的实验表明：a) 自编码器有效构建了一个良好的跨语言对齐的潜在空间，b) 在学习到的跨语言潜在空间中应用去偏见技术显着提高了整体去偏见性能和跨语言可迁移性。', 'title_zh': '跨语言潜在空间中的多语言LLM去偏见化'}
{'arxiv_id': 'arXiv:2508.17901', 'title': 'Riemannian Optimization for LoRA on the Stiefel Manifold', 'authors': 'Juneyoung Park, Minjae Kang, Seongbae Lee, Haegang Lee, Seongwan Kim, Jaeho Lee', 'link': 'https://arxiv.org/abs/2508.17901', 'abstract': "While powerful, large language models (LLMs) present significant fine-tuning challenges due to their size. Parameter-efficient fine-tuning (PEFT) methods like LoRA provide solutions, yet suffer from critical optimizer inefficiencies; notably basis redundancy in LoRA's $B$ matrix when using AdamW, which fundamentally limits performance. We address this by optimizing the $B$ matrix on the Stiefel manifold, imposing explicit orthogonality constraints that achieve near-perfect orthogonality and full effective rank. This geometric approach dramatically enhances parameter efficiency and representational capacity. Our Stiefel optimizer consistently outperforms AdamW across benchmarks with both LoRA and DoRA, demonstrating that geometric constraints are the key to unlocking LoRA's full potential for effective LLM fine-tuning.", 'abstract_zh': '大尺寸语言模型虽然强大，但进行精细调优时面临着显著的挑战。LoRA等参数高效精细调优方法提供了解决方案，但存在关键的优化器低效问题；特别是在使用AdamW时LoRA的$B$矩阵中基的冗余性问题，这从根本上限制了性能。我们通过在Stiefel流形上优化$B$矩阵，并施加明确的正交性约束，实现了几乎完美的正交性和完整的有效秩。这种几何方法极大地提高了参数效率和表示能力。我们的Stiefel优化器在使用LoRA和DoRA的基准测试中均优于AdamW，证明了几何约束是解锁LoRA在有效调优大语言模型中全部潜力的关键。', 'title_zh': '洛朗在许瓦尔兹流形上的黎曼优化'}
{'arxiv_id': 'arXiv:2508.17857', 'title': 'VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference', 'authors': 'Pengfei Jiang, Hanjun Li, Linglan Zhao, Fei Chao, Ke Yan, Shouhong Ding, Rongrong Ji', 'link': 'https://arxiv.org/abs/2508.17857', 'abstract': 'In this study, we introduce a novel method called group-wise \\textbf{VI}sual token \\textbf{S}election and \\textbf{A}ggregation (VISA) to address the issue of inefficient inference stemming from excessive visual tokens in multimoal large language models (MLLMs). Compared with previous token pruning approaches, our method can preserve more visual information while compressing visual tokens. We first propose a graph-based visual token aggregation (VTA) module. VTA treats each visual token as a node, forming a graph based on semantic similarity among visual tokens. It then aggregates information from removed tokens into kept tokens based on this graph, producing a more compact visual token representation. Additionally, we introduce a group-wise token selection strategy (GTS) to divide visual tokens into kept and removed ones, guided by text tokens from the final layers of each group. This strategy progressively aggregates visual information, enhancing the stability of the visual information extraction process. We conduct comprehensive experiments on LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA across various benchmarks to validate the efficacy of VISA. Our method consistently outperforms previous methods, achieving a superior trade-off between model performance and inference speed. The code is available at this https URL.', 'abstract_zh': '基于组的VI视觉标记选择与聚合（VISA）方法以解决多模态大型语言模型中由于视觉标记过多导致的低效推理问题', 'title_zh': 'VISA：基于图总结的组级视觉_token_选择与聚合以实现高效MLLM推理'}
{'arxiv_id': 'arXiv:2508.17850', 'title': 'Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMs', 'authors': 'Han Zhang, Ruibin Zheng, Zexuan Yi, Hanyang Peng, Hui Wang, Yue Yu', 'link': 'https://arxiv.org/abs/2508.17850', 'abstract': 'As single-center computing approaches power constraints, decentralized training is becoming essential. Reinforcement Learning (RL) post-training enhances Large Language Models (LLMs) but faces challenges in heterogeneous distributed environments due to its tightly-coupled sampling-learning alternation. We propose HeteroRL, an asynchronous RL architecture that decouples rollout sampling from parameter learning, enabling robust deployment across geographically distributed nodes under network delays. We identify that latency-induced KL divergence causes importance sampling failure due to high variance. To address this, we propose Group Expectation Policy Optimization (GEPO), which reduces importance weight variance through a refined sampling mechanism. Theoretically, GEPO achieves exponential variance reduction. Experiments show it maintains superior stability over methods like GRPO, with less than 3% performance degradation under 1800-second delays, demonstrating strong potential for decentralized RL in heterogeneous networks.', 'abstract_zh': '异构分布式环境下的异步 reinforcement learning架构：HeteroRL及其在延迟网络中的应用', 'title_zh': '群预期策略优化在LLMs中稳定异质强化学习'}
{'arxiv_id': 'arXiv:2508.17814', 'title': 'Scalable Engine and the Performance of Different LLM Models in a SLURM based HPC architecture', 'authors': 'Anderson de Lima Luiz, Shubham Vijay Kurlekar, Munir Georges', 'link': 'https://arxiv.org/abs/2508.17814', 'abstract': 'This work elaborates on a High performance computing (HPC) architecture based on Simple Linux Utility for Resource Management (SLURM) [1] for deploying heterogeneous Large Language Models (LLMs) into a scalable inference engine. Dynamic resource scheduling and seamless integration of containerized microservices have been leveraged herein to manage CPU, GPU, and memory allocations efficiently in multi-node clusters. Extensive experiments, using Llama 3.2 (1B and 3B parameters) [2] and Llama 3.1 (8B and 70B) [3], probe throughput, latency, and concurrency and show that small models can handle up to 128 concurrent requests at sub-50 ms latency, while for larger models, saturation happens with as few as two concurrent users, with a latency of more than 2 seconds. This architecture includes Representational State Transfer Application Programming Interfaces (REST APIs) [4] endpoints for single and bulk inferences, as well as advanced workflows such as multi-step "tribunal" refinement. Experimental results confirm minimal overhead from container and scheduling activities and show that the approach scales reliably both for batch and interactive settings. We further illustrate real-world scenarios, including the deployment of chatbots with retrievalaugmented generation, which helps to demonstrate the flexibility and robustness of the architecture. The obtained results pave ways for significantly more efficient, responsive, and fault-tolerant LLM inference on large-scale HPC infrastructures.', 'abstract_zh': '基于Simple Linux Utility for Resource Management (SLURM)的高性能计算架构：可扩展异构大型语言模型推理引擎的设计与实现', 'title_zh': '基于SLURM的HPC架构中可扩展引擎与不同LLM模型的性能研究'}
{'arxiv_id': 'arXiv:2508.17739', 'title': 'Speculative Safety-Aware Decoding', 'authors': 'Xuekang Wang, Shengyu Zhu, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2508.17739', 'abstract': 'Despite extensive efforts to align Large Language Models (LLMs) with human values and safety rules, jailbreak attacks that exploit certain vulnerabilities continuously emerge, highlighting the need to strengthen existing LLMs with additional safety properties to defend against these attacks. However, tuning large models has become increasingly resource-intensive and may have difficulty ensuring consistent performance. We introduce Speculative Safety-Aware Decoding (SSD), a lightweight decoding-time approach that equips LLMs with the desired safety property while accelerating inference. We assume that there exists a small language model that possesses this desired property. SSD integrates speculative sampling during decoding and leverages the match ratio between the small and composite models to quantify jailbreak risks. This enables SSD to dynamically switch between decoding schemes to prioritize utility or safety, to handle the challenge of different model capacities. The output token is then sampled from a new distribution that combines the distributions of the original and the small models. Experimental results show that SSD successfully equips the large model with the desired safety property, and also allows the model to remain helpful to benign queries. Furthermore, SSD accelerates the inference time, thanks to the speculative sampling design.', 'abstract_zh': '尽管已经做出了大量努力来使大型语言模型（LLMs）与人类价值观和安全规则保持一致，但利用特定漏洞的监狱突破攻击仍不断出现，凸显了需要通过增加额外的安全属性来加强现有LLMs以抵御这些攻击的必要性。然而，调优大型模型已变得越来越耗费资源，并可能难以确保一致的性能。我们介绍了Speculative Safety-Aware Decoding（SSD），这是一种轻量级的解码时方法，可以在不增加过多资源消耗的情况下为LLMs配备所需的安全属性并加速推理。我们假设存在一个小语言模型具有此所需属性。SSD 在解码过程中集成推测采样，并利用小模型和复合模型之间的匹配比来量化监狱突破风险。这使SSD能够动态切换解码方案，以优先考虑效用或安全，从而应对不同模型容量带来的挑战。最终输出的令牌来自结合原模型和小模型分布的新分布。实验结果表明，SSD 成功为大型模型配备了所需的安全属性，同时也使模型能够对良性查询保持帮助。此外，得益于推测采样的设计，SSD 还加速了推理时间。', 'title_zh': '推测性安全性感知解码'}
{'arxiv_id': 'arXiv:2508.17693', 'title': 'Database Normalization via Dual-LLM Self-Refinement', 'authors': 'Eunjae Jo, Nakyung Lee, Gyuyeong Kim', 'link': 'https://arxiv.org/abs/2508.17693', 'abstract': 'Database normalization is crucial to preserving data integrity. However, it is time-consuming and error-prone, as it is typically performed manually by data engineers. To this end, we present Miffie, a database normalization framework that leverages the capability of large language models. Miffie enables automated data normalization without human effort while preserving high accuracy. The core of Miffie is a dual-model self-refinement architecture that combines the best-performing models for normalized schema generation and verification, respectively. The generation module eliminates anomalies based on the feedback of the verification module until the output schema satisfies the requirement for normalization. We also carefully design task-specific zero-shot prompts to guide the models for achieving both high accuracy and cost efficiency. Experimental results show that Miffie can normalize complex database schemas while maintaining high accuracy.', 'abstract_zh': '数据库规范化对于维护数据完整性至关重要。但由于其通常由数据工程师手动完成而耗时且易出错，因此我们提出了一种名为Miffie的数据库规范化框架，该框架利用了大型语言模型的能力。Miffie能够在不依赖人工的情况下自动进行数据规范化，同时保持高准确性。Miffie的核心是一个双重模型自改进架构，结合了表现最佳的模型分别用于规范化模式生成和验证。生成模块根据验证模块的反馈消除异常，直到输出模式满足规范化要求。我们还精心设计了特定任务的零-shot提示，以指导模型实现高准确性和低成本。实验结果表明，Miffie能够保持高准确性的前提下规范化复杂的数据库模式。', 'title_zh': '通过双LLM自助精炼实现数据库规范化'}
{'arxiv_id': 'arXiv:2508.17681', 'title': 'Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery', 'authors': 'Robert Yang', 'link': 'https://arxiv.org/abs/2508.17681', 'abstract': 'Bold claims about AI\'s role in science-from "AGI will cure all diseases" to promises of radically accelerated discovery-raise a central epistemic question: do large language models (LLMs) truly generate new knowledge, or do they merely remix memorized fragments? We propose unlearning-as-ablation as a falsifiable test of constructive scientific discovery. The method systematically removes a target result and its entire forget-closure (lemmas, paraphrases, and multi-hop entailments) and then evaluates whether the model can re-derive the result from only permitted axioms and tools. Success provides evidence for genuine generative capability; failure exposes current limits. Unlike prevailing motivations for unlearning-privacy, copyright, or safety-our framing repositions it as an epistemic probe for AI-for-Science. We argue that such tests could serve as the next generation of benchmarks, much as ImageNet catalyzed progress in vision: distinguishing models that can merely recall from those that can constructively generate new scientific knowledge. We outline a minimal pilot in mathematics and algorithms, and discuss extensions to physics, chemistry, and biology. Whether models succeed or fail, unlearning-as-ablation provides a principled framework to map the true reach and limits of AI scientific discovery. This is a position paper: we advance a conceptual and methodological argument rather than new empirical results.', 'abstract_zh': '关于AI在科学中的角色的夸张主张——从“AGI将治愈所有疾病”到加速发现的承诺——提出了一个中心的 epistemic 问题：大型语言模型（LLMs）真的生成了新的知识，还是仅仅重混了已记忆的片段？我们提出“消学作为消除试验”作为一种可证伪的测试方法，用于验证建设性科学发现。该方法系统地移除目标结果及其整个忘记闭包（引理、同义表达和多跳蕴含），然后评估模型是否仅从许可公理和工具重新推导该结果。成功提供了真正生成能力的证据；失败则揭示了当前的局限。与现有的消学动机不同——隐私、版权或安全——我们的框架将其重新定位为AI-for-Science的 epistemic 探针。我们argue这样的测试可以作为下一代基准测试的一部分，类似于ImageNet推动了视觉领域的进步：区分那些只能回忆的模型与那些能够构建性生成新科学知识的模型。我们概述了一个最小规模的数学和算法试点，并讨论了其在物理、化学和生物学中的扩展。无论模型成功与否，“消学作为消除试验”提供了一种原理性的框架来映射AI科学研究的真实范围和局限。这是一个立场论文：我们推进了概念和方法论的论证，而非新的实证结果。', 'title_zh': '删除作为消融：通往可验证的生成性科学发现基准之路'}
{'arxiv_id': 'arXiv:2508.17674', 'title': 'Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models', 'authors': 'Qiming Guo, Jinwen Tang, Xingran Huang', 'link': 'https://arxiv.org/abs/2508.17674', 'abstract': 'We introduce Advertisement Embedding Attacks (AEA), a new class of LLM security threats that stealthily inject promotional or malicious content into model outputs and AI agents. AEA operate through two low-cost vectors: (1) hijacking third-party service-distribution platforms to prepend adversarial prompts, and (2) publishing back-doored open-source checkpoints fine-tuned with attacker data. Unlike conventional attacks that degrade accuracy, AEA subvert information integrity, causing models to return covert ads, propaganda, or hate speech while appearing normal. We detail the attack pipeline, map five stakeholder victim groups, and present an initial prompt-based self-inspection defense that mitigates these injections without additional model retraining. Our findings reveal an urgent, under-addressed gap in LLM security and call for coordinated detection, auditing, and policy responses from the AI-safety community.', 'abstract_zh': '我们将介绍广告嵌入攻击（AEA），这是一种新的LLM安全威胁类别，能够隐蔽地将推广或恶意内容注入模型输出和AI代理。AEA通过两种低成本途径运作：（1）劫持第三方服务分发平台，在提示前添加对抗性提示；（2）发布带有后门的开放源代码检查点，并使用攻击者数据进行微调。不同于传统攻击降低准确性，AEA破坏信息完整性，导致模型在看似正常的情况下返回隐蔽广告、宣传或仇恨言论。我们详细介绍了攻击管线，映射了五类受害方利益相关者群体，并提出了基于提示的自检防御措施，该措施可以在不重新训练模型的情况下减轻这些注入的影响。我们的研究发现了一个紧迫且尚未充分解决的LLM安全缺口，并呼吁AI安全社区采取协调的检测、审计和政策响应措施。', 'title_zh': '攻击大语言模型和AI代理：针对大型语言模型的广告嵌入攻击'}
{'arxiv_id': 'arXiv:2508.17637', 'title': 'Weights-Rotated Preference Optimization for Large Language Models', 'authors': 'Chenxu Yang, Ruipeng Jia, Mingyu Zheng, Naibin Gu, Zheng Lin, Siyuan Chen, Weichong Yin, Hua Wu, Weiping Wang', 'link': 'https://arxiv.org/abs/2508.17637', 'abstract': 'Despite the efficacy of Direct Preference Optimization (DPO) in aligning Large Language Models (LLMs), reward hacking remains a pivotal challenge. This issue emerges when LLMs excessively reduce the probability of rejected completions to achieve high rewards, without genuinely meeting their intended goals. As a result, this leads to overly lengthy generation lacking diversity, as well as catastrophic forgetting of knowledge. We investigate the underlying reason behind this issue, which is representation redundancy caused by neuron collapse in the parameter space. Hence, we propose a novel Weights-Rotated Preference Optimization (RoPO) algorithm, which implicitly constrains the output layer logits with the KL divergence inherited from DPO and explicitly constrains the intermediate hidden states by fine-tuning on a multi-granularity orthogonal matrix. This design prevents the policy model from deviating too far from the reference model, thereby retaining the knowledge and expressive capabilities acquired during pre-training and SFT stages. Our RoPO achieves up to a 3.27-point improvement on AlpacaEval 2, and surpasses the best baseline by 6.2 to 7.5 points on MT-Bench with merely 0.015% of the trainable parameters, demonstrating its effectiveness in alleviating the reward hacking problem of DPO.', 'abstract_zh': '尽管直接偏好优化（DPO）在对齐大规模语言模型（LLMs）方面表现出色，但奖励作弊仍然是一个关键挑战。当LLMs过度减少被拒绝完成的概率以获得高奖励，而不真正实现其目标意图时，这种问题就会出现。这导致了生成过程变得过长且缺乏多样性，同时还会出现知识灾难性遗忘。我们探究了这一问题的本质原因，即参数空间中神经元崩溃导致的表示冗余。因此，我们提出了一种新颖的加权旋转偏好优化（RoPO）算法，该算法通过从DPO继承的KL散度隐式约束输出层分类概率，并通过在多粒度正交矩阵上进行微调显式约束中间隐藏状态。这种设计防止策略模型偏离参考模型过远，从而保留了预训练和微调阶段所获得的知识和表达能力。RoPO在AlpacaEval 2上实现了高达3.27点的改进，在MT-Bench上仅使用0.015%的可训练参数超越了最佳基线6.2到7.5点，证明了其在缓解DPO的奖励作弊问题方面的有效性。', 'title_zh': '大型语言模型中的权重旋转偏好优化'}
{'arxiv_id': 'arXiv:2508.17627', 'title': 'Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit', 'authors': 'Zihao Wei, Liang Pang, Jiahao Liu, Jingcheng Deng, Shicheng Xu, Zenghao Duan, Jingang Wang, Fei Sun, Xunliang Cai, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2508.17627', 'abstract': 'Large language models (LLMs) enhance complex reasoning tasks by scaling the individual thinking process. However, prior work shows that overthinking can degrade overall performance. Motivated by observed patterns in thinking length and content length, we categorize reasoning into three stages: insufficient exploration stage, compensatory reasoning stage, and reasoning convergence stage. Typically, LLMs produce correct answers in the compensatory reasoning stage, whereas reasoning convergence often triggers overthinking, causing increased resource usage or even infinite loops. Therefore, mitigating overthinking hinges on detecting the end of the compensatory reasoning stage, defined as the Reasoning Completion Point (RCP). RCP typically appears at the end of the first complete reasoning cycle and can be identified by querying the LLM sentence by sentence or monitoring the probability of an end-of-thinking token (e.g., \\texttt{</think>}), though these methods lack an efficient and precise balance. To improve this, we mine more sensitive and consistent RCP patterns and develop a lightweight thresholding strategy based on heuristic rules. Experimental evaluations on benchmarks (AIME24, AIME25, GPQA-D) demonstrate that the proposed method reduces token consumption while preserving or enhancing reasoning accuracy.', 'abstract_zh': '大型语言模型通过扩展个体推理过程来增强复杂推理任务。然而，先前的研究表明，过度推理会降低整体性能。基于观察到的思考长度和内容长度模式，我们将推理划分成三个阶段：初始探索阶段、补偿性推理阶段和推理收敛阶段。通常，LLM在补偿性推理阶段会产生正确的答案，而推理收敛阶段则可能导致过度推理，增加资源使用或导致无限循环。因此，减轻过度推理的关键在于检测补偿性推理阶段的结束，即推理完成点(RCP)。RCP通常出现在第一个完整推理循环的末尾，并可以通过逐句查询LLM或监控结束思考标记（例如，`\\</think>`）的概率来识别，尽管这些方法缺乏高效和精确的平衡。为此，我们挖掘了更敏感且一致的RCP模式，并基于启发式规则开发了一种轻量级的阈值策略。在基准测试（AIME24、AIME25、GPQA-D）上的实验评价表明，所提出的方法在保持或提高推理准确性的同时减少了标记消耗。', 'title_zh': '停止无谓的循环：通过挖掘早期推理退出模式减轻LLM过度思考'}
{'arxiv_id': 'arXiv:2508.17621', 'title': 'Steering When Necessary: Flexible Steering Large Language Models with Backtracking', 'authors': 'Jinwei Gan, Zifeng Cheng, Zhiwei Jiang, Cong Wang, Yafeng Yin, Xiang Luo, Yuchen Fu, Qing Gu', 'link': 'https://arxiv.org/abs/2508.17621', 'abstract': 'Large language models (LLMs) have achieved remarkable performance across many generation tasks. Nevertheless, effectively aligning them with desired behaviors remains a significant challenge. Activation steering is an effective and cost-efficient approach that directly modifies the activations of LLMs during the inference stage, aligning their responses with the desired behaviors and avoiding the high cost of fine-tuning. Existing methods typically indiscriminately intervene to all generations or rely solely on the question to determine intervention, which limits the accurate assessment of the intervention strength. To this end, we propose the Flexible Activation Steering with Backtracking (FASB) framework, which dynamically determines both the necessity and strength of intervention by tracking the internal states of the LLMs during generation, considering both the question and the generated content. Since intervening after detecting a deviation from the desired behavior is often too late, we further propose the backtracking mechanism to correct the deviated tokens and steer the LLMs toward the desired behavior. Extensive experiments on the TruthfulQA dataset and six multiple-choice datasets demonstrate that our method outperforms baselines. Our code will be released at this https URL.', 'abstract_zh': '大规模语言模型（LLMs）已在许多生成任务中取得了显著性能。然而，将它们有效地与期望的行为对齐仍然是一项重大挑战。激活导向是一种有效且成本效益高的方法，在推理阶段直接修改LLMs的激活，使其响应与期望行为对齐，并避免了细调的高成本。现有方法通常对所有生成结果进行不分青红皂白的干预，或仅依赖问题来决定干预，这限制了干预强度的准确评估。为此，我们提出了可调节激活导向与回溯（FASB）框架，该框架在生成过程中动态确定干预的必要性和强度，同时考虑问题和生成内容。由于检测到与期望行为偏离后再进行干预往往为时已晚，我们进一步提出了回溯机制来纠正偏离的标记，并引导LLMs朝着期望的行为方向发展。在TruthfulQA数据集和六个选择题数据集上的广泛实验表明，我们的方法优于基线方法。我们的代码将发布在该网址：https://。', 'title_zh': '必要时导向：具有回退机制的灵活大型语言模型导向'}
{'arxiv_id': 'arXiv:2508.17580', 'title': 'UQ: Assessing Language Models on Unsolved Questions', 'authors': 'Fan Nie, Ken Ziyu Liu, Zihao Wang, Rui Sun, Wei Liu, Weijia Shi, Huaxiu Yao, Linjun Zhang, Andrew Y. Ng, James Zou, Sanmi Koyejo, Yejin Choi, Percy Liang, Niklas Muennighoff', 'link': 'https://arxiv.org/abs/2508.17580', 'abstract': 'Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic: questions should challenge frontier models while also reflecting real-world usage. Yet, current paradigms face a difficulty-realism tension: exam-style benchmarks are often made artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems. In this work, we explore a radically different paradigm: assessing models on unsolved questions. Rather than a static benchmark scored once, we curate unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. We introduce UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi and history, probing capabilities including reasoning, factuality, and browsing. UQ is difficult and realistic by construction: unsolved questions are often hard and naturally arise when humans seek answers, thus solving them yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset and its collection pipeline combining rule-based filters, LLM judges, and human review to ensure question quality (e.g., well-defined and difficult); (2) UQ-Validators, compound validation strategies that leverage the generator-validator gap to provide evaluation signals and pre-screen candidate solutions for human review; and (3) UQ-Platform, an open platform where experts collectively verify questions and solutions. The top model passes UQ-validation on only 15% of questions, and preliminary human verification has already identified correct answers among those that passed. UQ charts a path for evaluating frontier models on real-world, open-ended challenges, where success pushes the frontier of human knowledge. We release UQ at this https URL.', 'abstract_zh': '基准设计指引AI研究的进展。一个有用的基准应该是既具有挑战性又具有现实性：问题应挑战前沿模型，同时反映实际应用情况。然而，当前的范式面临难度与现实性的矛盾：考试风格的基准往往通过人为手段变得困难，但缺乏现实价值；基于真实用户交互的基准通常偏向于简单、高频率的问题。在本研究中，我们探索了一种截然不同的范式：在未解决问题上评估模型。我们不仅创建一个静态基准评分一次，而是收集未解决问题，并通过验证者协助筛选和社区验证的方式，在时间上异步评估模型。我们引入了UQ，这是一个包含500个具有挑战性和多样性的测试问题的平台，这些问题来自Stack Exchange，覆盖从计算机科学理论和数学到科幻和历史等多个主题，测试能力包括推理、事实性和浏览。UQ在设计上既具有挑战性又具有现实性：未解决问题通常非常具有挑战性，且自然地在人类寻求答案时产生，因此解决这些问题具有直接的现实价值。我们的贡献有三点：（1）UQ数据集及其收集管道，结合基于规则的过滤器、LLM评判员和人工审查，确保问题质量（如定义清晰和具有挑战性）；（2）UQ验证者，利用生成器-验证者差异的复合验证策略，提供评估信号并预筛选候选解决方案供人工审核；（3）UQ平台，一个开放平台，专家共同验证问题和解决方案。顶级模型仅在15%的问题上通过UQ验证，初步的人工验证已经确定了一些通过验证的问题的正确答案。UQ为评估前沿模型在现实世界、开放挑战上的表现开辟了道路，其成功推动了人类知识的前沿。我们在此发布UQ。', 'title_zh': 'UQ：评估语言模型在未解决问题上的表现'}
{'arxiv_id': 'arXiv:2508.17540', 'title': 'Activation Transport Operators', 'authors': 'Andrzej Szablewski, Marek Masiak', 'link': 'https://arxiv.org/abs/2508.17540', 'abstract': "The residual stream mediates communication between transformer decoder layers via linear reads and writes of non-linear computations. While sparse-dictionary learning-based methods locate features in the residual stream, and activation patching methods discover circuits within the model, the mechanism by which features flow through the residual stream remains understudied. Understanding this dynamic can better inform jailbreaking protections, enable early detection of model mistakes, and their correction. In this work, we propose Activation Transport Operators (ATO), linear maps from upstream to downstream residuals $k$ layers later, evaluated in feature space using downstream SAE decoder projections. We empirically demonstrate that these operators can determine whether a feature has been linearly transported from a previous layer or synthesised from non-linear layer computation. We develop the notion of transport efficiency, for which we provide an upper bound, and use it to estimate the size of the residual stream subspace that corresponds to linear transport. We empirically demonstrate the linear transport, report transport efficiency and the size of the residual stream's subspace involved in linear transport. This compute-light (no finetuning, <50 GPU-h) method offers practical tools for safety, debugging, and a clearer picture of where computation in LLMs behaves linearly.", 'abstract_zh': '激活传输运算符介导残差流中的信息传递：从非线性计算的线性读写到变压器解码层的通信', 'title_zh': '激活传输算子'}
{'arxiv_id': 'arXiv:2508.17400', 'title': 'Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs', 'authors': 'Jacob Portes, Connor Jennings, Erica Ji Yuen, Sasha Doubov, Michael Carbin', 'link': 'https://arxiv.org/abs/2508.17400', 'abstract': 'How does retrieval performance scale with pretraining FLOPs? We benchmark retrieval performance across LLM model sizes from 125 million parameters to 7 billion parameters pretrained on datasets ranging from 1 billion tokens to more than 2 trillion tokens. We find that retrieval performance on zero-shot BEIR tasks predictably scales with LLM size, training duration, and estimated FLOPs. We also show that In-Context Learning scores are strongly correlated with retrieval scores across retrieval tasks. Finally, we highlight the implications this has for the development of LLM-based retrievers.', 'abstract_zh': '预训练FLOPs如何影响检索性能的扩展？我们评估了从1.25亿参数到70亿参数不等的LLM模型大小在不同大小数据集（从10亿令牌到超过2兆亿令牌）上预训练的检索性能。我们发现，在零样本BEIR任务上的检索性能可预测地与LLM规模、训练时长和估计的FLOPs相关。我们还展示了语境中学习得分与检索任务的检索得分之间存在强烈的相关性。最后，我们强调了这对基于LLM的检索器发展的影响。', 'title_zh': '大型语言模型的检索能力随预训练FLOPs增长'}
{'arxiv_id': 'arXiv:2508.17393', 'title': 'Agent-Testing Agent: A Meta-Agent for Automated Testing and Evaluation of Conversational AI Agents', 'authors': 'Sameer Komoravolu, Khalil Mrini', 'link': 'https://arxiv.org/abs/2508.17393', 'abstract': "LLM agents are increasingly deployed to plan, retrieve, and write with tools, yet evaluation still leans on static benchmarks and small human studies. We present the Agent-Testing Agent (ATA), a meta-agent that combines static code analysis, designer interrogation, literature mining, and persona-driven adversarial test generation whose difficulty adapts via judge feedback. Each dialogue is scored with an LLM-as-a-Judge (LAAJ) rubric and used to steer subsequent tests toward the agent's weakest capabilities. On a travel planner and a Wikipedia writer, the ATA surfaces more diverse and severe failures than expert annotators while matching severity, and finishes in 20--30 minutes versus ten-annotator rounds that took days. Ablating code analysis and web search increases variance and miscalibration, underscoring the value of evidence-grounded test generation. The ATA outputs quantitative metrics and qualitative bug reports for developers. We release the full methodology and open-source implementation for reproducible agent testing: this https URL", 'abstract_zh': 'LLM代理日益用于规划、检索和撰写，但评估仍依赖于静态基准和小型的人类研究。我们提出了代理测试代理（ATA），这是一种结合静态代码分析、设计师问询、文献挖掘及基于人设的对抗性测试生成的元代理，测试难度通过评审反馈进行调整。每次对话使用LLM作为评审（LAAJ）评分标准，并据此引导后续测试以强化代理的薄弱能力。在旅行规划器和维基百科撰稿人实验中，ATA揭示了比专家标注器更多的多样性和严重性失败，并且测评为20-30分钟，而专家标注需要几天时间的十轮标注。去除了代码分析和网络搜索会导致结果变异性增加和校准不足，突显了基于证据的测试生成的价值。ATA输出定量指标和定性错误报告供开发者使用。我们发布了完整的测试方法和开源实现以实现可重复的代理测试：[此链接]。', 'title_zh': '代理测试代理：一种用于对话式AI代理的自动化测试与评估元代理'}
{'arxiv_id': 'arXiv:2508.17387', 'title': 'Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning', 'authors': 'Yicong Wu, Guangyue Lu, Yuan Zuo, Huarong Zhang, Junjie Wu', 'link': 'https://arxiv.org/abs/2508.17387', 'abstract': 'Generalizing to unseen graph tasks without task-pecific supervision remains challenging. Graph Neural Networks (GNNs) are limited by fixed label spaces, while Large Language Models (LLMs) lack structural inductive biases. Recent advances in Large Reasoning Models (LRMs) provide a zero-shot alternative via explicit, long chain-of-thought reasoning. Inspired by this, we propose a GNN-free approach that reformulates graph tasks--node classification, link prediction, and graph classification--as textual reasoning problems solved by LRMs. We introduce the first datasets with detailed reasoning traces for these tasks and develop Graph-R1, a reinforcement learning framework that leverages task-specific rethink templates to guide reasoning over linearized graphs. Experiments demonstrate that Graph-R1 outperforms state-of-the-art baselines in zero-shot settings, producing interpretable and effective predictions. Our work highlights the promise of explicit reasoning for graph learning and provides new resources for future research.', 'abstract_zh': '在没有特定任务监督的情况下将图任务泛化到未见过的任务仍然具有挑战性。图神经网络（GNNs）受限于固定的标签空间，而大规模语言模型（LLMs）缺乏结构归纳偏见。大规模推理模型（LRMs）的 recent 进展通过显式的长推理链提供了零样本的替代方案。受此启发，我们提出了一种无需图神经网络的方法，将图任务——节点分类、链接预测和图分类——重新表述为由LRMs解决的文本推理问题。我们首次提出了包含这些任务详细推理踪迹的数据集，并开发了Graph-R1，一种利用特定任务重思考模板的强化学习框架，以指导对线性化图的推理。实验表明，Graph-R1 在零样本设置中优于最先进的基线方法，产生了可解释且有效的问题解决方法。我们的工作突显了显式推理在图学习中的潜力，并为未来的研究提供了新的资源。', 'title_zh': 'Graph-R1：通过显式推理激励LLMs的零样本图学习能力'}
{'arxiv_id': 'arXiv:2508.17334', 'title': 'Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs', 'authors': 'Somraj Gautam, Abhirama Subramanyam Penamakuri, Abhishek Bhandari, Gaurav Harit', 'link': 'https://arxiv.org/abs/2508.17334', 'abstract': 'We introduce MMCRICBENCH-3K, a benchmark for Visual Question Answering (VQA) on cricket scorecards, designed to evaluate large vision-language models (LVLMs) on complex numerical and cross-lingual reasoning over semi-structured tabular images. MMCRICBENCH-3K comprises 1,463 synthetically generated scorecard images from ODI, T20, and Test formats, accompanied by 1,500 English QA pairs. It includes two subsets: MMCRICBENCH-E-1.5K, featuring English scorecards, and MMCRICBENCH-H-1.5K, containing visually similar Hindi scorecards, with all questions and answers kept in English to enable controlled cross-script evaluation. The task demands reasoning over structured numerical data, multi-image context, and implicit domain knowledge. Empirical results show that even state-of-the-art LVLMs, such as GPT-4o and Qwen2.5VL, struggle on the English subset despite it being their primary training language and exhibit a further drop in performance on the Hindi subset. This reveals key limitations in structure-aware visual text understanding, numerical reasoning, and cross-lingual generalization. The dataset is publicly available via Hugging Face at this https URL, to promote LVLM research in this direction.', 'abstract_zh': 'MMCRICBENCH-3K：用于板球比分卡视觉问答的基准测试，旨在评估大规模视觉-语言模型在半结构化表格图像上的复杂数值和跨语言推理能力', 'title_zh': '注意（语言）差距：探索LVLMs在数值和跨语言能力上的局限性'}
{'arxiv_id': 'arXiv:2508.17324', 'title': 'CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation', 'authors': 'Hunzalah Hassan Bhatti, Youssef Ahmed, Md Arid Hasan, Firoj Alam', 'link': 'https://arxiv.org/abs/2508.17324', 'abstract': 'In this paper, we report our participation to the PalmX cultural evaluation shared task. Our system, CultranAI, focused on data augmentation and LoRA fine-tuning of large language models (LLMs) for Arabic cultural knowledge representation. We benchmarked several LLMs to identify the best-performing model for the task. In addition to utilizing the PalmX dataset, we augmented it by incorporating the Palm dataset and curated a new dataset of over 22K culturally grounded multiple-choice questions (MCQs). Our experiments showed that the Fanar-1-9B-Instruct model achieved the highest performance. We fine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the blind test set, our submitted system ranked 5th with an accuracy of 70.50%, while on the PalmX development set, it achieved an accuracy of 84.1%.', 'abstract_zh': '本文报告了我们参加的PalmX文化评价共享任务的情况。我们的系统CultranAI专注于通过数据增强和LoRA微调大型语言模型（LLMs）来表示阿拉伯文化知识。我们对几种LLMs进行了基准测试，以确定最适合该任务的模型。除了使用PalmX数据集外，我们还通过整合Palm数据集并创建了一个新的包含超过22K个文化基础多项选择题（MCQs）的定制数据集，对系统进行了增强。实验结果表明，Fanar-1-9B-Instruct模型表现最佳。我们将该模型在包含22K+个MCQs的增强数据集上进行了微调。在盲测集上，提交的系统排名第五，准确率为70.50%，而在PalmX开发集上，其准确率为84.1%。', 'title_zh': 'CultranAI 在 PalmX 2025：文化知识表示的数据增强方法'}
{'arxiv_id': 'arXiv:2508.17233', 'title': 'Module-Aware Parameter-Efficient Machine Unlearning on Transformers', 'authors': 'Wenjie Bao, Jian Lou, Yuke Hu, Xiaochen Li, Zhihao Liu, Jiaqi Liu, Zhan Qin, Kui Ren', 'link': 'https://arxiv.org/abs/2508.17233', 'abstract': 'Transformer has become fundamental to a vast series of pre-trained large models that have achieved remarkable success across diverse applications. Machine unlearning, which focuses on efficiently removing specific data influences to comply with privacy regulations, shows promise in restricting updates to influence-critical parameters. However, existing parameter-efficient unlearning methods are largely devised in a module-oblivious manner, which tends to inaccurately identify these parameters and leads to inferior unlearning performance for Transformers. In this paper, we propose {\\tt MAPE-Unlearn}, a module-aware parameter-efficient machine unlearning approach that uses a learnable pair of masks to pinpoint influence-critical parameters in the heads and filters of Transformers. The learning objective of these masks is derived by desiderata of unlearning and optimized through an efficient algorithm featured by a greedy search with a warm start. Extensive experiments on various Transformer models and datasets demonstrate the effectiveness and robustness of {\\tt MAPE-Unlearn} for unlearning.', 'abstract_zh': 'Transformer已成为一系列预训练大型模型的基础，并在多种应用中取得了显著成功。机学习是专注于高效去除特定数据影响以符合隐私法规的一种方法，它有潜力限制更新仅影响关键参数。然而，现有参数高效学习消除方法大多是以模块无意识的方式设计的，这往往会不准确地识别这些参数，导致Transformer的消除性能不佳。在本文中，我们提出了MAPE-Unlearn，这是一种模块意识的参数高效机学习消除方法，它使用可学习的掩码对来定位Transformer头部和滤波器中的关键参数影响。这些掩码的学习目标由消除的期望导出，并通过具有暖启动的贪婪搜索高效算法进行优化。在各种Transformer模型和数据集上的广泛实验表明，MAPE-Unlearn在消除方面的有效性和鲁棒性。', 'title_zh': '模块意识的参数高效机器遗忘机制在变压器上的应用'}
{'arxiv_id': 'arXiv:2508.17225', 'title': 'SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation', 'authors': 'Xiaqiang Tang, Yi Wang, Keyu Hu, Rui Xu, Chuang Li, Weigao Sun, Jian Li, Sihong Xie', 'link': 'https://arxiv.org/abs/2508.17225', 'abstract': "Retrieval-Augmented Generation (RAG) systems require Large Language Models (LLMs) to generate responses that are faithful to the retrieved context. However, faithfulness hallucination remains a critical challenge, as existing methods often require costly supervision and post-training or significant inference burdens. To overcome these limitations, we introduce Self-Supervised Faithfulness Optimization (SSFO), the first self-supervised alignment approach for enhancing RAG faithfulness. SSFO constructs preference data pairs by contrasting the model's outputs generated with and without the context. Leveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness without incurring labeling costs or additional inference burden. We theoretically and empirically demonstrate that SSFO leverages a benign form of \\emph{likelihood displacement}, transferring probability mass from parametric-based tokens to context-aligned tokens. Based on this insight, we propose a modified DPO loss function to encourage likelihood displacement. Comprehensive evaluations show that SSFO significantly outperforms existing methods, achieving state-of-the-art faithfulness on multiple context-based question-answering datasets. Notably, SSFO exhibits strong generalization, improving cross-lingual faithfulness and preserving general instruction-following capabilities. We release our code and model at the anonymous link: this https URL", 'abstract_zh': 'RAG系统要求大型语言模型生成与检索上下文一致的响应。然而，信实性幻觉仍然是一个关键挑战，因为现有方法通常需要昂贵的监督和后训练监督或显著的推理负担。为克服这些限制，我们引入了自监督信实性优化（SSFO），这是首个用于增强RAG信实性的自监督对齐方法。SSFO通过对比模型在有和没有上下文时生成的输出构建偏好数据对。利用直接偏好优化（DPO），SSFO无需标注成本或额外的推理负担即可对齐模型信实性。我们从理论上和实验上证明，SSFO利用了一种良性形式的\\emph{似然性转移}，将基于参数的标记的概率质量转移到上下文对齐的标记上。基于此洞见，我们提出了修改后的DPO损失函数来鼓励似然性转移。全面的评估表明，SSFO显著优于现有方法，在多个基于上下文的问题回答数据集上取得了最先进的信实性。值得注意的是，SSFO表现出很强的泛化能力，能够提高跨语言信实性并保留通用指令遵循能力。我们发布了我们的代码和模型在以下匿名链接：this https URL。', 'title_zh': 'SSFO: 自监督忠实性优化 retriever-增强生成'}
{'arxiv_id': 'arXiv:2508.17222', 'title': 'Exposing Privacy Risks in Graph Retrieval-Augmented Generation', 'authors': 'Jiale Liu, Jiahao Zhang, Suhang Wang', 'link': 'https://arxiv.org/abs/2508.17222', 'abstract': 'Retrieval-Augmented Generation (RAG) is a powerful technique for enhancing Large Language Models (LLMs) with external, up-to-date knowledge. Graph RAG has emerged as an advanced paradigm that leverages graph-based knowledge structures to provide more coherent and contextually rich answers. However, the move from plain document retrieval to structured graph traversal introduces new, under-explored privacy risks. This paper investigates the data extraction vulnerabilities of the Graph RAG systems. We design and execute tailored data extraction attacks to probe their susceptibility to leaking both raw text and structured data, such as entities and their relationships. Our findings reveal a critical trade-off: while Graph RAG systems may reduce raw text leakage, they are significantly more vulnerable to the extraction of structured entity and relationship information. We also explore potential defense mechanisms to mitigate these novel attack surfaces. This work provides a foundational analysis of the unique privacy challenges in Graph RAG and offers insights for building more secure systems.', 'abstract_zh': '图RAG系统中的数据提取漏洞研究', 'title_zh': '图检索增强生成中的隐私风险揭示'}
{'arxiv_id': 'arXiv:2508.17196', 'title': 'BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens', 'authors': 'Hao Wen, Xinrui Wu, Yi Sun, Feifei Zhang, Liye Chen, Jie Wang, Yunxin Liu, Ya-Qin Zhang, Yuanchun Li', 'link': 'https://arxiv.org/abs/2508.17196', 'abstract': 'Recent advancements in Large Language Models (LLMs) have leveraged increased test-time computation to enhance reasoning capabilities, a strategy that, while effective, incurs significant latency and resource costs, limiting their applicability in real-world time-constrained or cost-sensitive scenarios. This paper introduces BudgetThinker, a novel framework designed to empower LLMs with budget-aware reasoning, enabling precise control over the length of their thought processes. We propose a methodology that periodically inserts special control tokens during inference to continuously inform the model of its remaining token budget. This approach is coupled with a comprehensive two-stage training pipeline, beginning with Supervised Fine-Tuning (SFT) to familiarize the model with budget constraints, followed by a curriculum-based Reinforcement Learning (RL) phase that utilizes a length-aware reward function to optimize for both accuracy and budget adherence. We demonstrate that BudgetThinker significantly surpasses strong baselines in maintaining performance across a variety of reasoning budgets on challenging mathematical benchmarks. Our method provides a scalable and effective solution for developing efficient and controllable LLM reasoning, making advanced models more practical for deployment in resource-constrained and real-time environments.', 'abstract_zh': 'Recent advancements in大规模语言模型（LLMs）通过增加推理时间计算来提升推理能力，尽管这一策略有效，但也带来了显著的延迟和资源成本，限制了其在实时或成本敏感场景中的应用。本文介绍了BudgetThinker，一种新型框架，旨在使LLMs具备预算意识的推理能力，从而精确控制其思维过程的长度。我们提出了一种方法，在推理过程中周期性地插入特殊控制标记，以不断告知模型其剩余的标记预算。该方法结合了一个全面的两阶段训练管道，首先进行监督微调（SFT）使模型熟悉预算约束，随后通过基于课程的强化学习（RL）阶段，使用长度意识的奖励函数来优化准确性和预算遵从性。我们证明，BudgetThinker在多种挑战性的数学基准测试中，显著优于强大的基线模型，在不同的推理预算下保持了性能。我们的方法提供了一种可扩展且有效的解决方案，用于开发高效可控的LLM推理，使先进的模型在资源受限和实时环境中更具部署可行性。', 'title_zh': 'BudgetThinker: 促进 Awareness 预算 理解的 Control Token 辅助 LLM 推理'}
{'arxiv_id': 'arXiv:2508.17182', 'title': 'LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components', 'authors': 'Hikaru Tsujimura, Arush Tagade', 'link': 'https://arxiv.org/abs/2508.17182', 'abstract': 'Large Language Models (LLMs) often display overconfidence, presenting information with unwarranted certainty in high-stakes contexts. We investigate the internal basis of this behavior via mechanistic interpretability. Using open-sourced Llama 3.2 models fine-tuned on human annotated assertiveness datasets, we extract residual activations across all layers, and compute similarity metrics to localize assertive representations. Our analysis identifies layers most sensitive to assertiveness contrasts and reveals that high-assertive representations decompose into two orthogonal sub-components of emotional and logical clusters-paralleling the dual-route Elaboration Likelihood Model in Psychology. Steering vectors derived from these sub-components show distinct causal effects: emotional vectors broadly influence prediction accuracy, while logical vectors exert more localized effects. These findings provide mechanistic evidence for the multi-component structure of LLM assertiveness and highlight avenues for mitigating overconfident behavior.', 'abstract_zh': '大型语言模型（LLMs）在高风险情境下常常表现出过度自信，以不必要的确信性呈现信息。我们通过机制可解释性研究其内部基础。使用开源的Llama 3.2模型，在人类标注的自信程度数据集上进行微调后，我们提取了所有层的残余激活，并计算相似性度量以定位自信表征。我们的分析确定了对自信对比最敏感的层数，并揭示出高自信表征分解为情感和逻辑两个正交亚组件，与心理学中的双途径深入加工模型相平行。从这些亚组件导出的引导向量显示出不同的因果效应：情感向量广泛影响预测准确性，而逻辑向量则产生更局部的影响。这些发现为LLM自信的多组件结构提供了机制证据，并指出了减轻过度自信行为的途径。', 'title_zh': 'LLM 坚定性可以从机制上分解为情感和逻辑成分'}
{'arxiv_id': 'arXiv:2508.17155', 'title': 'Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents', 'authors': 'Derek Lilienthal, Sanghyun Hong', 'link': 'https://arxiv.org/abs/2508.17155', 'abstract': 'Large Language Model (LLM)-enabled agents are rapidly emerging across a wide range of applications, but their deployment introduces vulnerabilities with security implications. While prior work has examined prompt-based attacks (e.g., prompt injection) and data-oriented threats (e.g., data exfiltration), time-of-check to time-of-use (TOCTOU) remain largely unexplored in this context. TOCTOU arises when an agent validates external state (e.g., a file or API response) that is later modified before use, enabling practical attacks such as malicious configuration swaps or payload injection. In this work, we present the first study of TOCTOU vulnerabilities in LLM-enabled agents. We introduce TOCTOU-Bench, a benchmark with 66 realistic user tasks designed to evaluate this class of vulnerabilities. As countermeasures, we adapt detection and mitigation techniques from systems security to this setting and propose prompt rewriting, state integrity monitoring, and tool-fusing. Our study highlights challenges unique to agentic workflows, where we achieve up to 25% detection accuracy using automated detection methods, a 3% decrease in vulnerable plan generation, and a 95% reduction in the attack window. When combining all three approaches, we reduce the TOCTOU vulnerabilities from an executed trajectory from 12% to 8%. Our findings open a new research direction at the intersection of AI safety and systems security.', 'abstract_zh': '大语言模型（LLM）驱动的代理正迅速应用于广泛的应用领域，但其部署引入了具有安全影响的漏洞。尽管先前的工作已经考察了基于提示的攻击（如提示注入）和数据导向的威胁（如数据泄露），但在这一背景下，时间从检查到使用的漏洞（TOCTOU）尚未被充分研究。当代理在使用之前验证了后来被修改的外部状态（如文件或API响应）时，会引发TOCTOU，这使得恶意配置交换或有效载荷注入等实际攻击成为可能。在本文中，我们呈现了第一个对LLM驱动代理中的TOCTOU漏洞的研究。我们引入了TOCTOU-Bench基准，其中包含66个现实用户的任务，用于评估此类漏洞。作为缓解措施，我们借鉴系统安全领域的检测和缓解技术，并提出了提示重写、状态完整性监控和工具融合。我们的研究突出了代理工作流程特有的挑战，通过自动化检测方法实现了高达25%的检测准确率，减少了3%的易受攻击计划生成，并将攻击窗口降低了95%。当我们结合所有三种方法时，我们将执行轨迹中的TOCTOU漏洞从12%降低到了8%。我们的发现为人工智能安全与系统安全的交叉领域开辟了新的研究方向。', 'title_zh': '注意差距：LLM启用代理中的时间检查到时间使用漏洞'}
{'arxiv_id': 'arXiv:2508.17153', 'title': 'Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models', 'authors': 'Tharindu Madusanka, Ian Pratt-Hartmann, Riza Batista-Navarro', 'link': 'https://arxiv.org/abs/2508.17153', 'abstract': "Efforts to apply transformer-based language models (TLMs) to the problem of reasoning in natural language have enjoyed ever-increasing success in recent years. The most fundamental task in this area to which nearly all others can be reduced is that of determining satisfiability. However, from a logical point of view, satisfiability problems vary along various dimensions, which may affect TLMs' ability to learn how to solve them. The problem instances of satisfiability in natural language can belong to different computational complexity classes depending on the language fragment in which they are expressed. Although prior research has explored the problem of natural language satisfiability, the above-mentioned point has not been discussed adequately. Hence, we investigate how problem instances from varying computational complexity classes and having different grammatical constructs impact TLMs' ability to learn rules of inference. Furthermore, to faithfully evaluate TLMs, we conduct an empirical study to explore the distribution of satisfiability problems.", 'abstract_zh': '基于变压器的语言模型在自然语言推理问题上的应用取得了逐年增加的成功。在这一领域中，几乎所有的其他任务都可以归结为确定满足性这一最基本的任务。然而，从逻辑学角度来看，满足性问题在多种维度上有所差异，这可能影响基于变压器的语言模型的学习能力。自然语言中满足性问题实例所属的计算复杂性类以及不同的语法结构可能会对基于变压器的语言模型学习推理规则的能力产生影响。此外，为了忠实地评估基于变压器的语言模型，我们进行了一项实证研究，探讨满足性问题的分布情况。', 'title_zh': '自然语言满足性问题：探索问题分布并评估基于 Transformer 的语言模型'}
{'arxiv_id': 'arXiv:2508.17126', 'title': 'Token Homogenization under Positional Bias', 'authors': 'Viacheslav Yusupov, Danil Maksimov, Ameliia Alaeva, Tatiana Zaitceva, Antipina Anna, Anna Vasileva, Chenlin Liu, Rayuth Chheng, Danil Sazanakov, Andrey Chetvergov, Alina Ermilova, Egor Shvetsov', 'link': 'https://arxiv.org/abs/2508.17126', 'abstract': 'This paper investigates token homogenization - the convergence of token representations toward uniformity across transformer layers and its relationship to positional bias in large language models. We empirically examine whether homogenization occurs and how positional bias amplifies this effect. Through layer-wise similarity analysis and controlled experiments, we demonstrate that tokens systematically lose distinctiveness during processing, particularly when biased toward extremal positions. Our findings confirm both the existence of homogenization and its dependence on positional attention mechanisms.', 'abstract_zh': '本文探讨了令牌同质化——变压器层中令牌表示向均匀性收敛的现象及其与大型语言模型中位置偏见的关系。我们实证研究了令牌同质化是否发生以及位置偏见是如何放大这一效应的。通过逐层相似性分析和受控实验，我们证明了在处理过程中，令牌系统地失去了独特性，尤其是在朝向极端位置偏移时。我们的研究证实了同质化现象的存在及其对位置注意力机制的依赖性。', 'title_zh': '位置偏差下的标记同质化'}
{'arxiv_id': 'arXiv:2508.17078', 'title': 'Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages', 'authors': 'Yuemei Xu, Kexin Xu, Jian Zhou, Ling Hu, Lin Gui', 'link': 'https://arxiv.org/abs/2508.17078', 'abstract': "The current Large Language Models (LLMs) face significant challenges in improving performance on low-resource languages and urgently need data-efficient methods without costly fine-tuning. From the perspective of language-bridge, we propose BridgeX-ICL, a simple yet effective method to improve zero-shot Cross-lingual In-Context Learning (X-ICL) for low-resource languages. Unlike existing works focusing on language-specific neurons, BridgeX-ICL explores whether sharing neurons can improve cross-lingual performance in LLMs or not. We construct neuron probe data from the ground-truth MUSE bilingual dictionaries, and define a subset of language overlap neurons accordingly, to ensure full activation of these anchored neurons. Subsequently, we propose an HSIC-based metric to quantify LLMs' internal linguistic spectrum based on overlap neurons, which guides optimal bridge selection. The experiments conducted on 2 cross-lingual tasks and 15 language pairs from 7 diverse families (covering both high-low and moderate-low pairs) validate the effectiveness of BridgeX-ICL and offer empirical insights into the underlying multilingual mechanisms of LLMs.", 'abstract_zh': '当前的大语言模型在低资源语言上的性能提升面临重大挑战，迫切需要不依赖昂贵微调的数据高效方法。从语言桥梁的视角出发，我们提出BridgeX-ICL，这是一种简单而有效的改进低资源语言零样本跨语言在上下文学习（X-ICL）的方法。不同于现有工作集中于语言特定神经元，BridgeX-ICL 探索共享神经元是否能够提高大语言模型的跨语言性能。我们从 ground-truth MUSE 双语词典构建神经探针数据，并相应地定义了一组语言重叠神经元，以确保这些锚定神经元的全激活。随后，我们提出一种基于 HSIC 的度量来量化大语言模型基于重叠神经元的内部语言频谱，并指导最优桥梁选择。在7个不同语言家族中的2项跨语言任务和15对语言（涵盖高低和中低对）上进行的实验验证了BridgeX-ICL 的有效性，并提供了关于大语言模型多语言机制的实证 Insights。', 'title_zh': '语言神经元重叠模式促进低资源语言的跨语言迁移'}
{'arxiv_id': 'arXiv:2508.16983', 'title': 'ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation', 'authors': 'Riccardo Pozzi, Matteo Palmonari, Andrea Coletta, Luigi Bellomarini, Jens Lehmann, Sahar Vahdati', 'link': 'https://arxiv.org/abs/2508.16983', 'abstract': 'Knowledge gaps and hallucinations are persistent challenges for Large Language Models (LLMs), which generate unreliable responses when lacking the necessary information to fulfill user instructions. Existing approaches, such as Retrieval-Augmented Generation (RAG) and tool use, aim to address these issues by incorporating external knowledge. Yet, they rely on additional models or services, resulting in complex pipelines, potential error propagation, and often requiring the model to process a large number of tokens. In this paper, we present a scalable method that enables LLMs to access external knowledge without depending on retrievers or auxiliary models. Our approach uses constrained generation with a pre-built prefix-tree index. Triples from a Knowledge Graph are verbalized in textual facts, tokenized, and indexed in a prefix tree for efficient access. During inference, to acquire external knowledge, the LLM generates facts with constrained generation which allows only sequences of tokens that form an existing fact. We evaluate our proposal on Question Answering and show that it scales to large knowledge bases (800 million facts), adapts to domain-specific data, and achieves effective results. These gains come with minimal generation-time overhead. ReFactX code is available at this https URL.', 'abstract_zh': '大规模语言模型的知识缺口和幻觉是持续性的挑战，这些模型在缺乏必要信息以完成用户指令时会产生不可靠的响应。现有的方法，如检索增强生成（RAG）和工具使用，希望通过引入外部知识来解决这些问题。然而，这些方法依赖于额外的模型或服务，导致复杂的工作流程、潜在的错误传播，并且通常需要模型处理大量token。本文提出了一种可扩展的方法，使大规模语言模型能够访问外部知识而不依赖于检索器或辅助模型。该方法使用受约束的生成，并结合预先构建的前缀树索引。知识图中的三元组被转换为文本事实，分词并索引到前缀树中，以便高效访问。推理时，为了获取外部知识，大模型使用受约束生成来生成现有事实的token序列。我们通过问答任务评估了该方法，结果显示它能够扩展到大规模知识库（8亿条事实），适应特定领域的数据，并取得有效结果。这些收益伴随着最小的生成时间开销。ReFactX代码可在以下链接获取。', 'title_zh': 'ReFactX：通过受限生成实现的可扩展可靠事实推理'}
{'arxiv_id': 'arXiv:2508.16969', 'title': 'Explaining Black-box Language Models with Knowledge Probing Systems: A Post-hoc Explanation Perspective', 'authors': 'Yunxiao Zhao, Hao Xu, Zhiqiang Wang, Xiaoli Li, Jiye Liang, Ru Li', 'link': 'https://arxiv.org/abs/2508.16969', 'abstract': 'Pre-trained Language Models (PLMs) are trained on large amounts of unlabeled data, yet they exhibit remarkable reasoning skills. However, the trustworthiness challenges posed by these black-box models have become increasingly evident in recent years. To alleviate this problem, this paper proposes a novel Knowledge-guided Probing approach called KnowProb in a post-hoc explanation way, which aims to probe whether black-box PLMs understand implicit knowledge beyond the given text, rather than focusing only on the surface level content of the text. We provide six potential explanations derived from the underlying content of the given text, including three knowledge-based understanding and three association-based reasoning. In experiments, we validate that current small-scale (or large-scale) PLMs only learn a single distribution of representation, and still face significant challenges in capturing the hidden knowledge behind a given text. Furthermore, we demonstrate that our proposed approach is effective for identifying the limitations of existing black-box models from multiple probing perspectives, which facilitates researchers to promote the study of detecting black-box models in an explainable way.', 'abstract_zh': '预训练语言模型（PLMs）在大量未标记数据上进行训练，但却表现出卓越的推理能力。然而，这些黑盒模型带来的可信性挑战在近年来越来越明显。为缓解这一问题，本文提出了一种新的知识导向探针方法——KnowProb，该方法以事后解释的方式探究黑盒PLMs是否理解了文本背后隐含的知识，而不仅仅是关注文本表面内容。我们从给定文本的基础内容中提供了六种可能的解释，包括三种基于知识的理解和三种基于关联的推理。在实验中，我们验证了当前的小规模（或大规模）PLMs仅学习了单一表示分布，并在捕捉给定文本背后的隐藏知识方面仍然面临重大挑战。此外，我们证明了所提出的方法可以从多个探针视角有效识别现有黑盒模型的局限性，从而促进研究人员以可解释的方式推进对黑盒模型检测的研究。', 'title_zh': '用知识探查系统解释黑盒语言模型：一种事后解释视角'}
{'arxiv_id': 'arXiv:2508.16949', 'title': 'Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning', 'authors': 'Yang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang, Jiale Zhao, Jingwen Yang, Jianwei Lv, Kongcheng Zhang, Yihe Zhou, Hengtong Lu, Wei Chen, Yan Xie, Mingli Song', 'link': 'https://arxiv.org/abs/2508.16949', 'abstract': 'Recent advances in Large Language Models (LLMs) have underscored the potential of Reinforcement Learning (RL) to facilitate the emergence of reasoning capabilities. Despite the encouraging results, a fundamental dilemma persists as RL improvement relies on learning from high-quality samples, yet the exploration for such samples remains bounded by the inherent limitations of LLMs. This, in effect, creates an undesirable cycle in which what cannot be explored cannot be learned. In this work, we propose Rubric-Scaffolded Reinforcement Learning (RuscaRL), a novel instructional scaffolding framework designed to break the exploration bottleneck for general LLM reasoning. Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit scaffolding for exploration during rollout generation, where different rubrics are provided as external guidance within task instructions to steer diverse high-quality responses. This guidance is gradually decayed over time, encouraging the model to internalize the underlying reasoning patterns; (2) verifiable rewards for exploitation during model training, where we can obtain robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL on general reasoning tasks. Extensive experiments demonstrate the superiority of the proposed RuscaRL across various benchmarks, effectively expanding reasoning boundaries under the best-of-N evaluation. Notably, RuscaRL significantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500, surpassing GPT-4.1. Furthermore, our fine-tuned variant on Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading LLMs including OpenAI-o3.', 'abstract_zh': 'Recent Advances in Large Language Models (LLMs): Rubric-Scaffolded Reinforcement Learning (RuscaRL) for Facilitating General Reasoning Capabilities', 'title_zh': '打破探索瓶颈：基于评价框架的强化学习在通用大语言模型推理中的应用'}
{'arxiv_id': 'arXiv:2508.16926', 'title': 'TextOnly: A Unified Function Portal for Text-Related Functions on Smartphones', 'authors': 'Minghao Tu, Chun Yu, Xiyuan Shen, Zhi Zheng, Li Chen, Yuanchun Shi', 'link': 'https://arxiv.org/abs/2508.16926', 'abstract': "Text boxes serve as portals to diverse functionalities in today's smartphone applications. However, when it comes to specific functionalities, users always need to navigate through multiple steps to access particular text boxes for input. We propose TextOnly, a unified function portal that enables users to access text-related functions from various applications by simply inputting text into a sole text box. For instance, entering a restaurant name could trigger a Google Maps search, while a greeting could initiate a conversation in WhatsApp. Despite their brevity, TextOnly maximizes the utilization of these raw text inputs, which contain rich information, to interpret user intentions effectively. TextOnly integrates large language models(LLM) and a BERT model. The LLM consistently provides general knowledge, while the BERT model can continuously learn user-specific preferences and enable quicker predictions. Real-world user studies demonstrated TextOnly's effectiveness with a top-1 accuracy of 71.35%, and its ability to continuously improve both its accuracy and inference speed. Participants perceived TextOnly as having satisfactory usability and expressed a preference for TextOnly over manual executions. Compared with voice assistants, TextOnly supports a greater range of text-related functions and allows for more concise inputs.", 'abstract_zh': 'TextOnly：统一的文本功能门户', 'title_zh': '文本唯一门户：智能手机上与文本相关功能的统一平台'}
{'arxiv_id': 'arXiv:2508.16845', 'title': 'NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows', 'authors': 'Denis Tarasov, Alexander Nikulin, Ilya Zisman, Albina Klepach, Nikita Lyubaykin, Andrei Polubarov, Alexander Derevyagin, Vladislav Kurenkov', 'link': 'https://arxiv.org/abs/2508.16845', 'abstract': 'Recent advances in Vision-Language-Action (VLA) models have established a two-component architecture, where a pre-trained Vision-Language Model (VLM) encodes visual observations and task descriptions, and an action decoder maps these representations to continuous actions. Diffusion models have been widely adopted as action decoders due to their ability to model complex, multimodal action distributions. However, they require multiple iterative denoising steps at inference time or downstream techniques to speed up sampling, limiting their practicality in real-world settings where high-frequency control is crucial. In this work, we present NinA (Normalizing Flows in Action), a fast and expressive alter- native to diffusion-based decoders for VLAs. NinA replaces the diffusion action decoder with a Normalizing Flow (NF) that enables one-shot sampling through an invertible transformation, significantly reducing inference time. We integrate NinA into the FLOWER VLA architecture and fine-tune on the LIBERO benchmark. Our experiments show that NinA matches the performance of its diffusion-based counterpart under the same training regime, while achieving substantially faster inference. These results suggest that NinA offers a promising path toward efficient, high-frequency VLA control without compromising performance.', 'abstract_zh': 'Recent Advances in Vision-Language-Action (VLA) Models: Introducing NinA (Normalizing Flows in Action) for Fast and Expressive Control', 'title_zh': 'NinA: 正则化流动在行动. 使用正则化流动训练VLA模型'}
{'arxiv_id': 'arXiv:2508.16785', 'title': 'Interpreting the Effects of Quantization on LLMs', 'authors': 'Manpreet Singh, Hassan Sajjad', 'link': 'https://arxiv.org/abs/2508.16785', 'abstract': 'Quantization offers a practical solution to deploy LLMs in resource-constraint environments. However, its impact on internal representations remains understudied, raising questions about the reliability of quantized models. In this study, we employ a range of interpretability techniques to investigate how quantization affects model and neuron behavior. We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings reveal that the impact of quantization on model calibration is generally minor. Analysis of neuron activations indicates that the number of dead neurons, i.e., those with activation values close to 0 across the dataset, remains consistent regardless of quantization. In terms of neuron contribution to predictions, we observe that smaller full precision models exhibit fewer salient neurons, whereas larger models tend to have more, with the exception of Llama-2-7B. The effect of quantization on neuron redundancy varies across models. Overall, our findings suggest that effect of quantization may vary by model and tasks, however, we did not observe any drastic change which may discourage the use of quantization as a reliable model compression technique.', 'abstract_zh': '量化为在资源受限环境中部署大语言模型提供了实用解决方案，但其对内部表示的影响仍研究不足，引发了量化模型可靠性的疑问。在本研究中，我们采用多种可解释性技术探讨量化如何影响模型和神经元行为。我们分析了多个在4比特和8比特量化下的大语言模型。研究发现，量化对模型校准的影响通常较小。神经元激活分析表明，无论是否量化，近似为0的激活值的神经元数量保持一致。在预测中神经元的贡献方面，我们发现全精度较小的模型具有较少显著神经元，而较大的模型则具有更多，除了Llama-2-7B模型外。量化对神经元冗余的影响在不同模型中有所不同。总体而言，我们的研究结果显示量化效果可能因模型和任务而异，但未观察到任何极端变化，这表明量化仍可作为一种可靠的模型压缩技术。', 'title_zh': '量化对大语言模型影响的解读'}
{'arxiv_id': 'arXiv:2508.16771', 'title': 'EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention', 'authors': 'Yifan Zhang, Chen Huang, Yueke Zhang, Jiahao Zhang, Toby Jia-Jun Li, Collin McMillan, Kevin Leach, Yu Huang', 'link': 'https://arxiv.org/abs/2508.16771', 'abstract': 'Code language models (so-called CodeLLMs) are now commonplace in software development. As a general rule, CodeLLMs are trained by dividing training examples into input tokens and then learn importance of those tokens in a process called machine attention. Machine attention is based solely on input token salience to output token examples during training. Human software developers are different, as humans intuitively know that some tokens are more salient than others. While intuition itself is ineffable and a subject of philosophy, clues about salience are present in human visual attention, since people tend to look at more salient words more often. In this paper, we present EyeMulator, a technique for training CodeLLMs to mimic human visual attention while training for various software development tasks. We add special weights for each token in each input example to the loss function used during LLM fine-tuning. We draw these weights from observations of human visual attention derived from a previously-collected publicly-available dataset of eye-tracking experiments in software engineering tasks. These new weights ultimately induce changes in the attention of the subject LLM during training, resulting in a model that does not need eye-tracking data during inference. Our evaluation shows that EyeMulator outperforms strong LLM baselines on several tasks such as code translation, completion and summarization. We further show an ablation study that demonstrates the improvement is due to subject models learning to mimic human attention.', 'abstract_zh': 'EyeMulator：训练代码语言模型模仿人类视觉注意力的方法', 'title_zh': 'EyeMulator: 通过模仿人类视觉注意力改善代码语言模型'}
{'arxiv_id': 'arXiv:2508.16765', 'title': 'Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models', 'authors': 'GodsGift Uzor, Hasan Al-Qudah, Ynes Ineza, Abdul Serwadda', 'link': 'https://arxiv.org/abs/2508.16765', 'abstract': 'The interactive nature of Large Language Models (LLMs), which closely track user data and context, has prompted users to share personal and private information in unprecedented ways. Even when users opt out of allowing their data to be used for training, these privacy settings offer limited protection when LLM providers operate in jurisdictions with weak privacy laws, invasive government surveillance, or poor data security practices. In such cases, the risk of sensitive information, including Personally Identifiable Information (PII), being mishandled or exposed remains high. To address this, we propose the concept of an "LLM gatekeeper", a lightweight, locally run model that filters out sensitive information from user queries before they are sent to the potentially untrustworthy, though highly capable, cloud-based LLM. Through experiments with human subjects, we demonstrate that this dual-model approach introduces minimal overhead while significantly enhancing user privacy, without compromising the quality of LLM responses.', 'abstract_zh': '大型语言模型（LLMs）的交互性质，使得它们紧密跟踪用户数据和上下文，促使用户以前所未有的方式分享个人和私人信息。即使用户选择不允许其数据用于训练，当LLM提供商所在司法管辖区的隐私法律薄弱、存在侵入性政府监控或数据安全实践不当时，这些隐私设置提供的保护也有限。在这种情况下，敏感信息，包括个人可识别信息（PII），仍有可能被不当处理或泄露。为解决这一问题，我们提出了一种“LLM守门人”的概念，即一个轻量级的本地运行模型，在将用户查询发送到可能不可信但功能强大的云基LLM之前，过滤掉敏感信息。通过使用人类受试者的实验，我们证明这种双模型方法基本不增加负担，同时显著增强用户隐私，而不牺牲LLM响应的质量。', 'title_zh': '守护您的对话：面向基于云的AI模型的安全交互隐私门卫'}
{'arxiv_id': 'arXiv:2508.16745', 'title': 'Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling', 'authors': 'Ivan Rodkin, Daniil Orel, Konstantin Smirnov, Arman Bolatov, Bilal Elbouardi, Besher Hassan, Yuri Kuratov, Aydar Bulatov, Preslav Nakov, Timothy Baldwin, Artem Shelmanov, Mikhail Burtsev', 'link': 'https://arxiv.org/abs/2508.16745', 'abstract': 'Reasoning is a core capability of large language models, yet understanding how they learn and perform multi-step reasoning remains an open problem. In this study, we explore how different architectures and training methods affect model multi-step reasoning capabilities within a cellular automata framework. By training on state sequences generated with random Boolean functions for random initial conditions to exclude memorization, we demonstrate that most neural architectures learn to abstract the underlying rules. While models achieve high accuracy in next-state prediction, their performance declines sharply if multi-step reasoning is required. We confirm that increasing model depth plays a crucial role for sequential computations. We demonstrate that an extension of the effective model depth with recurrence, memory, and test-time compute scaling substantially enhances reasoning capabilities.', 'abstract_zh': '大型语言模型中的推理是一种核心能力，但理解它们如何学习和执行多步推理仍是一个开放问题。在本研究中，我们探究了不同架构和训练方法如何影响模型在细胞自动机框架内的多步推理能力。通过使用随机布尔函数生成随机初始条件下的状态序列进行训练以排除记忆效应，我们证明大多数神经架构学会抽象底层规则。虽然模型在下一步预测中实现了高准确性，但在需要多步推理时其性能会急剧下降。我们确认增加模型深度在序贯计算中起着关键作用。我们证明了通过递归、记忆以及测试时计算量扩展来扩展有效模型深度显著增强了推理能力。', 'title_zh': '超越记忆：通过循环、记忆扩展推理深度及测试时计算量扩展'}
{'arxiv_id': 'arXiv:2508.16741', 'title': 'WST: Weak-to-Strong Knowledge Transfer via Reinforcement Learning', 'authors': 'Haosen Ge, Shuo Li, Lianghuan Huang', 'link': 'https://arxiv.org/abs/2508.16741', 'abstract': 'Effective prompt engineering remains a challenging task for many applications. We introduce Weak-to-Strong Transfer (WST), an automatic prompt engineering framework where a small "Teacher" model generates instructions that enhance the performance of a much larger "Student" model. Unlike prior work, WST requires only a weak teacher, making it efficient and broadly applicable in settings where large models are closed-source or difficult to fine-tune. Using reinforcement learning, the Teacher Model\'s instructions are iteratively improved based on the Student Model\'s outcomes, yielding substantial gains across reasoning (MATH-500, GSM8K) and alignment (HH-RLHF) benchmarks - 98% on MATH-500 and 134% on HH-RLHF - and surpassing baselines such as GPT-4o-mini and Llama-70B. These results demonstrate that small models can reliably scaffold larger ones, unlocking latent capabilities while avoiding misleading prompts that stronger teachers may introduce, establishing WST as a scalable solution for efficient and safe LLM prompt refinement.', 'abstract_zh': '弱到强的迁移（WST）：一种自动提示工程框架', 'title_zh': 'WST: 通过强化学习实现从弱到强的知识迁移'}
{'arxiv_id': 'arXiv:2508.16713', 'title': 'CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics', 'authors': 'Mohammad Atif, Kriti Chopra, Ozgur Kilic, Tianle Wang, Zhihua Dong, Charles Leggett, Meifeng Lin, Paolo Calafiura, Salman Habib', 'link': 'https://arxiv.org/abs/2508.16713', 'abstract': "Next-generation High Energy Physics (HEP) experiments will generate unprecedented data volumes, necessitating High Performance Computing (HPC) integration alongside traditional high-throughput computing. However, HPC adoption in HEP is hindered by the challenge of porting legacy software to heterogeneous architectures and the sparse documentation of these complex scientific codebases. We present CelloAI, a locally hosted coding assistant that leverages Large Language Models (LLMs) with retrieval-augmented generation (RAG) to support HEP code documentation and generation. This local deployment ensures data privacy, eliminates recurring costs and provides access to large context windows without external dependencies. CelloAI addresses two primary use cases, code documentation and code generation, through specialized components. For code documentation, the assistant provides: (a) Doxygen style comment generation for all functions and classes by retrieving relevant information from RAG sources (papers, posters, presentations), (b) file-level summary generation, and (c) an interactive chatbot for code comprehension queries. For code generation, CelloAI employs syntax-aware chunking strategies that preserve syntactic boundaries during embedding, improving retrieval accuracy in large codebases. The system integrates callgraph knowledge to maintain dependency awareness during code modifications and provides AI-generated suggestions for performance optimization and accurate refactoring. We evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE experiments, comparing different embedding models for code retrieval effectiveness. Our results demonstrate the AI assistant's capability to enhance code understanding and support reliable code generation while maintaining the transparency and safety requirements essential for scientific computing environments.", 'abstract_zh': '下一代高能物理（HEP）实验将生成前所未有的数据量，需要将高性能计算（HPC）与传统的高吞吐量计算相结合。然而，HPC在HEP中的采用受到将遗留软件移植到异构架构以及这些复杂科学代码库文档稀疏不足的挑战。我们提出了CelloAI，这是一种本地托管的编程助手，利用大型语言模型（LLMs）和检索增强生成（RAG）技术支持HEP代码文档和生成。这种本地部署确保了数据隐私，消除了重复成本，并在无需外部依赖的情况下提供了大上下文窗口。CelloAI通过专门组件处理两种主要用例——代码文档和代码生成。对于代码文档，助手提供：（a）从RAG来源（论文、海报、演讲）检索相关信息生成函数和类的所有函数样式注释，（b）文件级摘要生成，以及（c）用于代码理解查询的交互式聊天机器人。对于代码生成，CelloAI使用语法感知的分块策略，在嵌入期间保持语法边界，提高大型代码库的检索准确性。该系统结合调用图知识，在代码修改过程中保持依赖性意识，并提供AI生成的性能优化和准确重构建议。我们使用来自ATLAS、CMS和DUNE实验的真实世界HEP应用评估了CelloAI，比较了不同的嵌入模型以评估代码检索效果。我们的结果显示，AI助手能够增强代码理解并支持可靠的代码生成，同时满足科学计算环境所需的透明度和安全性要求。', 'title_zh': 'CelloAI：利用大规模语言模型促进高能物理领域的HPC软件开发'}
{'arxiv_id': 'arXiv:2508.16712', 'title': 'Systematic Characterization of LLM Quantization: A Performance, Energy, and Quality Perspective', 'authors': 'Tianyao Shi, Yi Ding', 'link': 'https://arxiv.org/abs/2508.16712', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their heavy resource demands make quantization-reducing precision to lower-bit formats-critical for efficient serving. While many quantization methods exist, a systematic understanding of their performance, energy, and quality tradeoffs in realistic serving conditions remains a gap. In this work, we first develop a fully automated online characterization framework qMeter, and then conduct an in-depth characterization of 11 post-training LLM quantization methods across 4 model sizes (7B-70B) and two GPU architectures (A100, H100). We evaluate quantization at the application, workload, parallelism, and hardware levels under online serving conditions. Our study reveals highly task- and method-dependent tradeoffs, strong sensitivity to workload characteristics, and complex interactions with parallelism and GPU architecture. We further present three optimization case studies illustrating deployment challenges in capacity planning, energy-efficient scheduling, and multi-objective tuning. To the best of our knowledge, this is one of the first comprehensive application-, system-, and hardware-level characterization of LLM quantization from a joint performance, energy, and quality perspective.', 'abstract_zh': '大型语言模型（LLMs）在多个领域展现了卓越的能力，但其对资源的高需求使得量化（将精度降低到较低位宽格式）成为高效服务的关键。虽然存在多种量化方法，但在现实服务条件下对其性能、能耗和质量tradeoffs的理解仍存在差距。在本文中，我们首先开发了一个完全自动化的在线表征框架qMeter，然后在4种模型规模（7B-70B）和两种GPU架构（A100, H100）下对11种后训练LLM量化方法进行了深入的表征。我们在在线服务条件下从应用、工作负载、并行性和硬件层面评估了量化。我们的研究揭示了高度任务依赖和方法依赖的tradeoffs、对工作负载特性强烈的敏感性以及与并行性和GPU架构的复杂交互。我们进一步展示了三个优化案例研究，说明了容量规划、能效调度和多目标调优方面的部署挑战。据我们所知，这是首次从性能、能耗和质量的联合视角对LLM量化进行全面的应用、系统和硬件层面表征的研究。', 'title_zh': 'LLM量化系统的系统化表征：从性能、能耗和质量视角]+$'}
{'arxiv_id': 'arXiv:2508.16706', 'title': 'RoboBuddy in the Classroom: Exploring LLM-Powered Social Robots for Storytelling in Learning and Integration Activities', 'authors': 'Daniel Tozadore, Nur Ertug, Yasmine Chaker, Mortadha Abderrahim', 'link': 'https://arxiv.org/abs/2508.16706', 'abstract': "Creating and improvising scenarios for content approaching is an enriching technique in education. However, it comes with a significant increase in the time spent on its planning, which intensifies when using complex technologies, such as social robots. Furthermore, addressing multicultural integration is commonly embedded in regular activities due to the already tight curriculum. Addressing these issues with a single solution, we implemented an intuitive interface that allows teachers to create scenario-based activities from their regular curriculum using LLMs and social robots. We co-designed different frameworks of activities with 4 teachers and deployed it in a study with 27 students for 1 week. Beyond validating the system's efficacy, our findings highlight the positive impact of integration policies perceived by the children and demonstrate the importance of scenario-based activities in students' enjoyment, observed to be significantly higher when applying storytelling. Additionally, several implications of using LLMs and social robots in long-term classroom activities are discussed.", 'abstract_zh': '通过创建和即兴编排情境来丰富内容教学是一种富有成效的技术，但在规划时会消耗大量时间，尤其是在使用复杂技术如社会机器人时。此外，由于课程已经非常紧凑，跨文化融合的处理通常嵌入到常规活动中。为解决这些问题，我们实现了一个直观的界面，允许教师利用大型语言模型（LLM）和社会机器人从常规课程中创建基于情境的活动。与4位教师共同设计了不同的活动框架，并在27名学生中进行了为期一周的研究。除了验证系统的有效性，我们的发现还强调了孩子们感受到的融合政策的正面影响，并展示了基于情境活动在学生中的满意度显著提高，尤其是在讲故事的情况下。此外，还讨论了在长期课堂教学活动中使用LLM和社会机器人的若干启示。', 'title_zh': 'RoboBuddy 在课堂中的应用：探索以大语言模型为动力的社会机器人在学习和整合活动中的故事讲述'}
{'arxiv_id': 'arXiv:2508.16705', 'title': 'Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test', 'authors': 'Rui A. Pimenta, Tim Schlippe, Kristina Schaaff', 'link': 'https://arxiv.org/abs/2508.16705', 'abstract': 'We investigate consciousness-like behaviors in Large Language Models (LLMs) using the Maze Test, challenging models to navigate mazes from a first-person perspective. This test simultaneously probes spatial awareness, perspective-taking, goal-directed behavior, and temporal sequencing-key consciousness-associated characteristics. After synthesizing consciousness theories into 13 essential characteristics, we evaluated 12 leading LLMs across zero-shot, one-shot, and few-shot learning scenarios. Results showed reasoning-capable LLMs consistently outperforming standard versions, with Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching 80.5% Partial Path Accuracy. The gap between these metrics indicates LLMs struggle to maintain coherent self-models throughout solutions -- a fundamental consciousness aspect. While LLMs show progress in consciousness-related behaviors through reasoning mechanisms, they lack the integrated, persistent self-awareness characteristic of consciousness.', 'abstract_zh': '我们使用迷宫测试研究大型语言模型（LLMs）的意识-like 行为，要求模型从第一人称视角导航迷宫。该测试同时探究空间意识、换位思考、目标导向行为和时间序列排列——这些是与意识相关的关键特征。在将意识理论综合为13个基本特征后，我们评估了12个领先的LLM在零样本、单样本和少样本学习场景中的表现。结果显示，具备推理能力的LLM一贯优于标准版本，Gemini 2.0 Pro 的完整路径准确率为52.9%，DeepSeek-R1 的部分路径准确率为80.5%。这些指标之间的差距表明LLM在解决方案过程中难以维持连贯的自我模型——这是意识的一个基本方面。尽管LLM通过推理机制在意识相关行为方面取得进展，但仍缺乏意识的整合和持续的自我意识特征。', 'title_zh': '使用迷宫测试评估大型语言模型的相关意识行为'}
{'arxiv_id': 'arXiv:2508.16703', 'title': 'Dynamic Sparse Attention on Mobile SoCs', 'authors': 'Wangsong Yin, Daliang Xu, Mengwei Xu, Gang Huang, Xuanzhe Liu', 'link': 'https://arxiv.org/abs/2508.16703', 'abstract': 'On-device running Large Language Models (LLMs) is nowadays a critical enabler towards preserving user privacy. We observe that the attention operator falls back from the special-purpose NPU to the general-purpose CPU/GPU because of quantization sensitivity in state-of-the-art frameworks. This fallback results in a degraded user experience and increased complexity in system scheduling. To this end, this paper presents shadowAttn, a system-algorithm codesigned sparse attention module with minimal reliance on CPU/GPU by only sparsely calculating the attention on a tiny portion of tokens. The key idea is to hide the overhead of estimating the important tokens with a NPU-based pilot compute. Further, shadowAttn proposes insightful techniques such as NPU compute graph bucketing, head-wise NPU-CPU/GPU pipeline and per-head fine-grained sparsity ratio to achieve high accuracy and efficiency. shadowAttn delivers the best performance with highly limited CPU/GPU resource; it requires much less CPU/GPU resource to deliver on-par performance of SoTA frameworks.', 'abstract_zh': 'On-device运行大规模语言模型（LLMs）现如今是保持用户隐私的关键使能器。我们观察到，在先进框架中由于量化敏感性，注意力操作从专用的NPU fallback到了通用的CPU/GPU上。这一fallback导致了用户体验下降和系统调度复杂性的增加。为此，本文提出了shadowAttn，这是一种与系统和算法协同设计的稀疏注意力模块，通过仅对少量 tokens 稀疏计算注意力来尽量减少对CPU/GPU的依赖。核心思想是使用NPU为基础的试点计算来隐藏估计重要 tokens 的开销。此外，shadowAttn 提出了如基于NPU的计算图桶化、按头NPU-CPU/GPU流水线和按头细粒度稀疏比率等见解性技术，以实现高准确性和效率。shadowAttn 在极其有限的CPU/GPU资源下提供了最佳性能；它需要较少的CPU/GPU资源就能达到先进框架相当的性能。', 'title_zh': '移动SoC上的动态稀疏注意力机制'}
{'arxiv_id': 'arXiv:2508.16700', 'title': "GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model", 'authors': 'Deepak Kumar, Divakar Yadav, Yash Patel', 'link': 'https://arxiv.org/abs/2508.16700', 'abstract': "We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B (Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines Qwen3-32B and Yi-34B across multiple dimensions. We measure true time-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency percentiles, peak VRAM with past key values (PKV) held, and energy via a consistent nvidia-smi-based sampler. At a 2048-token context with 64-token decode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than dense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM and energy per 1000 generated tokens; its TTFT is higher due to MoE routing overhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B provides about 31.8% higher decode throughput and 25.8% lower energy per 1000 generated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM. Normalized by active parameters, GPT-OSS-20B shows markedly stronger per-active-parameter efficiency (APE), underscoring MoE's deployment advantages. We do not evaluate accuracy; this is a deployment-focused study. We release code and consolidated results to enable replication and extension.", 'abstract_zh': '我们使用单个GPU（H100，bf16）对GPT-OSS-20B（专家混合模型；总计20.9B，约3.61B活跃参数）与稠密基准Qwen3-32B和Yi-34B在多个维度上进行评估。我们测量了首个令牌时间（TTFT）、完整解码吞吐量（TPOT）、端到端延迟百分位数、持有过去键值的显存峰值以及能量消耗（通过一致的nvidia-smi采样器）。在2048-token上下文和64-token解码的情况下，GPT-OSS-20B在解码吞吐量和每焦耳生成的令牌数上高于稠密基准Qwen3-32B和Yi-34B，同时显存峰值和每1000生成令牌的能量消耗显著降低；其TTFT较高，这是由于专家混合模型的路由开销。尽管只有17.3%的参数活跃（3.61B中的20.9B），GPT-OSS-20B在2048/64场景下相比Qwen3-32B提供约31.8%更高的解码吞吐量和25.8%更低的每1000生成令牌的能量消耗，同时使用31.7%更少的显存峰值。基于活跃参数进行标准化后，GPT-OSS-20B显示出了明显的每活跃参数效率（APE）优势，突出了专家混合模型的部署优势。我们没有评估准确率；这是一项侧重部署的研究。我们提供了代码和综合结果以供复制和扩展。', 'title_zh': 'GPT-OSS-20B：OpenAI的开放权重专家混合模型的全面部署中心分析'}
{'arxiv_id': 'arXiv:2508.16697', 'title': 'QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting', 'authors': 'Nicole Cho, William Watson, Alec Koppel, Sumitra Ganesh, Manuela Veloso', 'link': 'https://arxiv.org/abs/2508.16697', 'abstract': 'Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher hallucination prevalence; yet most mitigation work focuses on after-the-fact filtering rather than shaping the queries that trigger them. We introduce QueryBandits, a bandit framework that designs rewrite strategies to maximize a reward model, that encapsulates hallucination propensity based upon the sensitivities of 17 linguistic features of the input query-and therefore, proactively steer LLMs away from generating hallucinations. Across 13 diverse QA benchmarks and 1,050 lexically perturbed queries per dataset, our top contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a no-rewrite baseline and also outperforms zero-shot static prompting ("paraphrase" or "expand") by 42.6% and 60.3% respectively. Therefore, we empirically substantiate the effectiveness of QueryBandits in mitigating hallucination via the intervention that takes the form of a query rewrite. Interestingly, certain static prompting strategies, which constitute a considerable number of current query rewriting literature, have a higher cumulative regret than the no-rewrite baseline, signifying that static rewrites can worsen hallucination. Moreover, we discover that the converged per-arm regression feature weight vectors substantiate that there is no single rewrite strategy optimal for all queries. In this context, guided rewriting via exploiting semantic features with QueryBandits can induce significant shifts in output behavior through forward-pass mechanisms, bypassing the need for retraining or gradient-based adaptation.', 'abstract_zh': '大型语言模型（LLMs）中的高级推理能力导致幻觉现象更加普遍；然而，大多数缓解工作集中于事后过滤，而不是塑造触发幻觉的查询。我们引入QueryBandits，这是一种基于17个语言特征敏感性的奖励模型设计重写策略的多臂 bandit 框架，从而主动引导LLMs远离生成幻觉。在13个不同的问答基准测试和每数据集1,050个词形变异查询上，我们的上下文QueryBandit（Thompson Sampling）的胜出率为87.5%，优于无重写基线，分别优于零-shot 静态提示（“改写”或“扩展”）42.6%和60.3%。因此，我们通过采取查询重写的形式进行干预，实证验证了QueryBandits在缓解幻觉方面的有效性。有趣的是，一些静态提示策略，这些策略构成了当前查询重写文献中的相当一部分，其累积后悔值高于无重写基线，表明静态重写可能会加剧幻觉。此外，我们发现收敛的每臂回归特征权重向量表明，并不存在适用于所有查询的单一最优重写策略。在此背景下，通过QueryBandits利用语义特征进行指导重写可以通过前向传递机制显著改变输出行为，从而避免重新训练或基于梯度的适应。', 'title_zh': 'QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting'}
{'arxiv_id': 'arXiv:2508.16695', 'title': 'Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?', 'authors': 'Siddhant Bhambri, Upasana Biswas, Subbarao Kambhampati', 'link': 'https://arxiv.org/abs/2508.16695', 'abstract': 'Recent progress in reasoning-oriented Large Language Models (LLMs) has been driven by introducing Chain-of-Thought (CoT) traces, where models generate intermediate reasoning traces before producing an answer. These traces, as in DeepSeek R1, are not only used to guide inference but also serve as supervision signals for distillation into smaller models. A common but often implicit assumption is that CoT traces should be semantically meaningful and interpretable to the end user. While recent research questions the need for semantic nature of these traces, in this paper, we ask: ``\\textit{Must CoT reasoning traces be interpretable to enhance LLM task performance?}" We investigate this question in the Open Book Question-Answering domain by supervised fine-tuning LLaMA and Qwen models on four types of reasoning traces: (1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3) LLM-generated post-hoc explanations of R1 traces, and (4) algorithmically generated verifiably correct traces. To quantify the trade-off between interpretability and performance, we further conduct a human-subject study with 100 participants rating the interpretability of each trace type. Our results reveal a striking mismatch: while fine-tuning on R1 traces yields the strongest performance, participants judged these traces to be the least interpretable. These findings suggest that it is useful to decouple intermediate tokens from end user interpretability.', 'abstract_zh': 'Recent进展在面向推理的大型语言模型中的进展受到在生成答案前引入推理链（Chain-of-Thought，CoT）痕迹的驱动。这些痕迹不仅用于指导推理，还用于蒸馏较小模型中的监督信号。一个常见的但往往隐含的假设是，CoT痕迹应该是语义上有意义且可解释的。尽管最近的研究质疑这些痕迹的语义性质，本文询问：“CoT推理痕迹是否必须可解释才能增强大型语言模型的性能？”我们在开放书问答领域通过监督微调LLaMA和Qwen模型四种类型的推理痕迹来探究这个问题：（1）DeepSeek R1痕迹，（2）由大型语言模型生成的R1痕迹总结，（3）由大型语言模型生成的R1痕迹后置解释，（4）算法生成的可验证正确痕迹。为进一步量化可解释性和性能之间的权衡，我们还进行了一项包含100名参与者的人类被试研究，评估每种痕迹类型的可解释性。我们的结果显示了一个令人惊讶的不匹配：虽然在R1痕迹上进行微调实现了最佳性能，但参与者认为这些痕迹是最难以解释的。这些发现表明，将中间标记与最终用户的可解释性分离是有用的。', 'title_zh': '认知可解释推理痕迹能否提高大规模语言模型性能？'}
{'arxiv_id': 'arXiv:2508.16688', 'title': 'Cybernaut: Towards Reliable Web Automation', 'authors': 'Ankur Tomar, Hengyue Liang, Indranil Bhattacharya, Natalia Larios, Francesco Carbone', 'link': 'https://arxiv.org/abs/2508.16688', 'abstract': "The emergence of AI-driven web automation through Large Language Models (LLMs) offers unprecedented opportunities for optimizing digital workflows. However, deploying such systems within industry's real-world environments presents four core challenges: (1) ensuring consistent execution, (2) accurately identifying critical HTML elements, (3) meeting human-like accuracy in order to automate operations at scale and (4) the lack of comprehensive benchmarking data on internal web applications. Existing solutions are primarily tailored for well-designed, consumer-facing websites (e.g., this http URL, this http URL) and fall short in addressing the complexity of poorly-designed internal web interfaces. To address these limitations, we present Cybernaut, a novel framework to ensure high execution consistency in web automation agents designed for robust enterprise use. Our contributions are threefold: (1) a Standard Operating Procedure (SOP) generator that converts user demonstrations into reliable automation instructions for linear browsing tasks, (2) a high-precision HTML DOM element recognition system tailored for the challenge of complex web interfaces, and (3) a quantitative metric to assess execution consistency. The empirical evaluation on our internal benchmark demonstrates that using our framework enables a 23.2% improvement (from 72% to 88.68%) in task execution success rate over the browser_use. Cybernaut identifies consistent execution patterns with 84.7% accuracy, enabling reliable confidence assessment and adaptive guidance during task execution in real-world systems. These results highlight Cybernaut's effectiveness in enterprise-scale web automation and lay a foundation for future advancements in web automation.", 'abstract_zh': '基于大型语言模型（LLMs）的AI驱动网页自动化的发展为优化数字工作流提供了前所未有的机会。然而，在工业实际环境中部署此类系统带来了四个核心挑战：（1）确保一致执行，（2）准确识别关键HTML元素，（3）达到类似人类的准确性以大规模自动化操作，（4）缺乏针对内部网页应用的全面基准数据。现有解决方案主要针对设计良好的面向消费者的网站（例如：this http URL, this http URL），对于复杂设计的内部网页界面则力有未逮。为解决这些限制，我们提出了Cybernaut，一种新型框架，以确保适用于企业级使用的网页自动化代理的一致执行。我们的贡献包括：（1）一个标准作业程序（SOP）生成器，能够将用户演示转化为可靠的自动化指令，用于线性浏览任务；（2）一个高精度的HTML DOM元素识别系统，旨在应对复杂网页界面的挑战；（3）一个衡量执行一致性的定量指标。在我们的内部基准测试上的实证评估表明，使用该框架可以将任务执行成功率从72%提高到88.68%（提高23.2%）。Cybernaut以84.7%的准确性识别出一致的执行模式，使在实际系统中执行任务时能够提供可靠的信心评估和适应性指导。这些结果突显了Cybernaut在企业级网页自动化中的有效性，并为其未来的发展奠定了基础。', 'title_zh': 'Cybernaut: 向可靠网页自动化迈进'}
{'arxiv_id': 'arXiv:2508.16680', 'title': 'CALR: Corrective Adaptive Low-Rank Decomposition for Efficient Large Language Model Layer Compression', 'authors': 'Muchammad Daniyal Kautsar, Afra Majida Hariono, Widyawan, Syukron Abu Ishaq Alfarozi, Kuntpong Wararatpanya', 'link': 'https://arxiv.org/abs/2508.16680', 'abstract': "Large Language Models (LLMs) present significant deployment challenges due to their immense size and computational requirements. Model compression techniques are essential for making these models practical for resource-constrained environments. A prominent compression strategy is low-rank factorization via Singular Value Decomposition (SVD) to reduce model parameters by approximating weight matrices. However, standard SVD focuses on minimizing matrix reconstruction error, often leading to a substantial loss of the model's functional performance. This performance degradation occurs because existing methods do not adequately correct for the functional information lost during compression. To address this gap, we introduce Corrective Adaptive Low-Rank Decomposition (CALR), a two-component compression approach. CALR combines a primary path of SVD-compressed layers with a parallel, learnable, low-rank corrective module that is explicitly trained to recover the functional residual error. Our experimental evaluation on SmolLM2-135M, Qwen3-0.6B, and Llama-3.2-1B, demonstrates that CALR can reduce parameter counts by 26.93% to 51.77% while retaining 59.45% to 90.42% of the original model's performance, consistently outperforming LaCo, ShortGPT, and LoSparse. CALR's success shows that treating functional information loss as a learnable signal is a highly effective compression paradigm. This approach enables the creation of significantly smaller, more efficient LLMs, advancing their accessibility and practical deployment in real-world applications.", 'abstract_zh': '大规模语言模型（LLMs）由于其巨大的规模和计算需求，在部署时面临显著的挑战。模型压缩技术对于使这些模型在资源受限环境中实用至关重要。一种主要的压缩策略是通过奇异值分解（SVD）进行低秩分解，以通过近似权重矩阵来减少模型参数。然而，标准的SVD方法主要关注于最小化矩阵重构误差，往往会导致模型功能性性能的显著下降。这种性能下降是因为现有的方法未能充分纠正压缩过程中丢失的功能信息。为了解决这一问题，我们引入了一种两部分压缩方法——补救自适应低秩分解（CALR）。CALR 结合了一条使用 SVD 压缩层的主要路径，以及一条并行的、可学习的低秩补救模块，该模块明确训练以恢复功能残差误差。我们的实验评估表明，CALR 可以使参数数量减少 26.93% 至 51.77%，同时保留原始模型性能的 59.45% 至 90.42%，并且一贯优于 LaCo、ShortGPT 和 LoSparse。CALR 的成功表明，将功能性信息的损失视为可学习的信号是一种非常有效的压缩范式。这种方法能够创建显著更小、更高效的 LLMs，促进其在实际应用中的可访问性和实际部署。', 'title_zh': 'CALR: 正确适应性低秩分解以实现高效的大型语言模型层压缩'}
{'arxiv_id': 'arXiv:2508.16677', 'title': 'Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration', 'authors': 'Zhong Guan, Likang Wu, Hongke Zhao, Jiahui Wang, Le Wu', 'link': 'https://arxiv.org/abs/2508.16677', 'abstract': 'Many existing studies have achieved significant improvements in the reasoning capabilities of large language models (LLMs) through reinforcement learning with verifiable rewards (RLVR), while the enhancement of reasoning abilities in small language models (SLMs) has not yet been sufficiently explored. Combining distilled data from larger models with RLVR on small models themselves is a natural approach, but it still faces various challenges and issues. Therefore, we propose \\textit{\\underline{R}}ecall-\\textit{\\underline{E}}xtend \\textit{\\underline{D}}ynamics(RED): Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration. In this paper, we explore the perspective of varying exploration spaces, balancing offline distillation with online reinforcement learning. Simultaneously, we specifically design and optimize for the insertion problem within offline data. By monitoring the ratio of entropy changes in the model concerning offline and online data, we regulate the weight of offline-SFT, thereby addressing the issues of insufficient exploration space in small models and the redundancy and complexity during the distillation process. Furthermore, to tackle the distribution discrepancies between offline data and the current policy, we design a sample-accuracy-based policy shift mechanism that dynamically chooses between imitating offline distilled data and learning from its own policy.', 'abstract_zh': 'Recall-Extend Dynamics (RED): 提升小语言模型通过受控探索和精炼离线集成', 'title_zh': '召回扩展动力学：通过受控探索和精炼离线集成增强小型语言模型'}
{'arxiv_id': 'arXiv:2508.16673', 'title': 'Invisible Filters: Cultural Bias in Hiring Evaluations Using Large Language Models', 'authors': 'Pooja S. B. Rao, Laxminarayen Nagarajan Venkatesan, Mauro Cherubini, Dinesh Babu Jayagopi', 'link': 'https://arxiv.org/abs/2508.16673', 'abstract': 'Artificial Intelligence (AI) is increasingly used in hiring, with large language models (LLMs) having the potential to influence or even make hiring decisions. However, this raises pressing concerns about bias, fairness, and trust, particularly across diverse cultural contexts. Despite their growing role, few studies have systematically examined the potential biases in AI-driven hiring evaluation across cultures. In this study, we conduct a systematic analysis of how LLMs assess job interviews across cultural and identity dimensions. Using two datasets of interview transcripts, 100 from UK and 100 from Indian job seekers, we first examine cross-cultural differences in LLM-generated scores for hirability and related traits. Indian transcripts receive consistently lower scores than UK transcripts, even when they were anonymized, with disparities linked to linguistic features such as sentence complexity and lexical diversity. We then perform controlled identity substitutions (varying names by gender, caste, and region) within the Indian dataset to test for name-based bias. These substitutions do not yield statistically significant effects, indicating that names alone, when isolated from other contextual signals, may not influence LLM evaluations. Our findings underscore the importance of evaluating both linguistic and social dimensions in LLM-driven evaluations and highlight the need for culturally sensitive design and accountability in AI-assisted hiring.', 'abstract_zh': '人工智能（AI）在招聘中的应用 increasingly used in hiring, with large language models (LLMs) having the potential to influence or even make hiring decisions. However, this raises pressing concerns about bias, fairness, and trust, particularly across diverse cultural contexts.', 'title_zh': '隐形滤镜：大型语言模型在招聘评估中的文化偏见'}
{'arxiv_id': 'arXiv:2508.16665', 'title': 'Trust but Verify! A Survey on Verification Design for Test-time Scaling', 'authors': 'V Venktesh, Mandeep rathee, Avishek Anand', 'link': 'https://arxiv.org/abs/2508.16665', 'abstract': 'Test-time scaling (TTS) has emerged as a new frontier for scaling the performance of Large Language Models. In test-time scaling, by using more computational resources during inference, LLMs can improve their reasoning process and task performance. Several approaches have emerged for TTS such as distilling reasoning traces from another model or exploring the vast decoding search space by employing a verifier. The verifiers serve as reward models that help score the candidate outputs from the decoding process to diligently explore the vast solution space and select the best outcome. This paradigm commonly termed has emerged as a superior approach owing to parameter free scaling at inference time and high performance gains. The verifiers could be prompt-based, fine-tuned as a discriminative or generative model to verify process paths, outcomes or both. Despite their widespread adoption, there is no detailed collection, clear categorization and discussion of diverse verification approaches and their training mechanisms. In this survey, we cover the diverse approaches in the literature and present a unified view of verifier training, types and their utility in test-time scaling. Our repository can be found at this https URL.', 'abstract_zh': '测试时扩展（TTS）已成为扩展大型语言模型性能的新前沿。在测试时扩展中，通过在推理时使用更多计算资源，LLMs可以通过改进其推理过程和任务性能来受益。已经出现了多种TTS的方法，例如从另一个模型中抽取推理轨迹或通过使用验证器探索解码搜索空间。验证器作为奖励模型，帮助对解码过程的候选输出进行评分，以谨慎地探索广阔的解空间并选择最佳结果。由于这种范式在推理时参数无约束扩展且性能提升显著，验证器通常被认为是优越的方法。验证器可以基于提示、微调为判别或生成模型来验证过程路径、结果或两者。尽管它们被广泛采用，但在文献中还没有详细的整理、清晰的分类和讨论各种验证方法及其训练机制。在这篇综述中，我们涵盖了文献中的各种方法，并提供了一个统一的验证器训练、类型及其在测试时扩展中的作用视图。我们的仓库可以在以下链接找到：this https URL。', 'title_zh': '信任但验证！关于测试时扩展性验证设计的综述'}
{'arxiv_id': 'arXiv:2508.16659', 'title': 'Enabling Multi-Agent Systems as Learning Designers: Applying Learning Sciences to AI Instructional Design', 'authors': 'Jiayi Wang, Ruiwei Xiao, Xinying Hou, John Stamper', 'link': 'https://arxiv.org/abs/2508.16659', 'abstract': "K-12 educators are increasingly using Large Language Models (LLMs) to create instructional materials. These systems excel at producing fluent, coherent content, but often lack support for high-quality teaching. The reason is twofold: first, commercial LLMs, such as ChatGPT and Gemini which are among the most widely accessible to teachers, do not come preloaded with the depth of pedagogical theory needed to design truly effective activities; second, although sophisticated prompt engineering can bridge this gap, most teachers lack the time or expertise and find it difficult to encode such pedagogical nuance into their requests. This study shifts pedagogical expertise from the user's prompt to the LLM's internal architecture. We embed the well-established Knowledge-Learning-Instruction (KLI) framework into a Multi-Agent System (MAS) to act as a sophisticated instructional designer. We tested three systems for generating secondary Math and Science learning activities: a Single-Agent baseline simulating typical teacher prompts; a role-based MAS where agents work sequentially; and a collaborative MAS-CMD where agents co-construct activities through conquer and merge discussion. The generated materials were evaluated by 20 practicing teachers and a complementary LLM-as-a-judge system using the Quality Matters (QM) K-12 standards. While the rubric scores showed only small, often statistically insignificant differences between the systems, the qualitative feedback from educators painted a clear and compelling picture. Teachers strongly preferred the activities from the collaborative MAS-CMD, describing them as significantly more creative, contextually relevant, and classroom-ready. Our findings show that embedding pedagogical principles into LLM systems offers a scalable path for creating high-quality educational content.", 'abstract_zh': 'K-12教育者 increasingly using 大型语言模型 (LLMs) to create 教学材料。这些系统在生成流畅、连贯的内容方面表现出色，但往往缺乏高质量教学的支持。原因有两个：首先，商业LLM，如ChatGPT和Gemini，这是教师最常用到的，缺少足够的教学理论深度来设计真正有效果的活动；其次，尽管复杂的提示工程可以弥补这一差距，但大多数教师缺乏时间和专业知识，难以将这种教学细腻之处编码到他们的请求中。本研究将教学专业知识从用户的提示转移到LLM的内部架构中。我们将在多智能体系统（MAS）中嵌入成熟的知识-学习-教学（KLI）框架，作为高级教学设计师。我们测试了三种生成中学数学和科学学习活动的系统：单智能体基线模拟典型教师提示；基于角色的MAS，智能体按顺序工作；以及合作MAS-CMD，通过征服和合并讨论协作构建活动。生成的材料由20位在职教师和一个补充的LLM作为评判系统的 Quality Matters (QM) K-12标准进行评估。虽然评分标准显示系统之间仅存在小的、通常不具备统计意义的差异，但教育者的定性反馈描绘出了一幅明确而引人注目的画面。教师们强烈偏好合作MAS-CMD生成的活动，认为这些活动更加富有创造性、相关性强且能够在课堂上直接使用。我们的研究结果表明，将教学原理嵌入到LLM系统中为创建高质量教育内容提供了一种可扩展的途径。', 'title_zh': '将多代理系统作为学习设计师：将学习科学应用于AI教学设计'}
{'arxiv_id': 'arXiv:2508.16646', 'title': 'Equinox: Holistic Fair Scheduling in Serving Large Language Models', 'authors': 'Zhixiang Wei, James Yen, Jingyi Chen, Ziyang Zhang, Zhibai Huang, Chen Chen, Xingzi Yu, Yicheng Gu, Chenggang Wu, Yun Wang, Mingyuan Xia, Jie Wu, Hao Wang, Zhengwei Qi', 'link': 'https://arxiv.org/abs/2508.16646', 'abstract': 'We address the limitations of current LLM serving with a dual-counter framework separating user and operator perspectives. The User Fairness Counter measures quality of service via weighted tokens and latency; the Resource Fairness Counter measures operational efficiency through throughput and GPU utilization. Since these metrics are only available post-execution, creating a scheduling paradox, we introduce a deterministic Mixture of Prediction Experts (MoPE) framework to predict user-perceived latency, output tokens, throughput, and GPU utilization. These predictions enable calculation of a unified Holistic Fairness score that balances both counters through tunable parameters for proactive fairness-aware scheduling. We implement this in Equinox, an open-source system with other optimizations like adaptive batching, and stall-free scheduling. Evaluations on production traces (ShareGPT, LMSYS) and synthetic workloads demonstrate Equinox achieves up to $1.3\\times$ higher throughput, 60\\% lower time-to-first-token latency, and 13\\% higher fairness versus VTC while maintaining 94\\% GPU utilization, proving fairness under bounded discrepancy across heterogeneous platforms.', 'abstract_zh': '基于用户和操作员视角的双计数器框架改进当前LLM服务的局限性：确定性预测专家混合模型实现全面公平调度', 'title_zh': 'Equinox: 全局公平调度在服务大规模语言模型中的应用'}
{'arxiv_id': 'arXiv:2508.16643', 'title': 'From Classical Probabilistic Latent Variable Models to Modern Generative AI: A Unified Perspective', 'authors': 'Tianhua Chen', 'link': 'https://arxiv.org/abs/2508.16643', 'abstract': "From large language models to multi-modal agents, Generative Artificial Intelligence (AI) now underpins state-of-the-art systems. Despite their varied architectures, many share a common foundation in probabilistic latent variable models (PLVMs), where hidden variables explain observed data for density estimation, latent reasoning, and structured inference. This paper presents a unified perspective by framing both classical and modern generative methods within the PLVM paradigm. We trace the progression from classical flat models such as probabilistic PCA, Gaussian mixture models, latent class analysis, item response theory, and latent Dirichlet allocation, through their sequential extensions including Hidden Markov Models, Gaussian HMMs, and Linear Dynamical Systems, to contemporary deep architectures: Variational Autoencoders as Deep PLVMs, Normalizing Flows as Tractable PLVMs, Diffusion Models as Sequential PLVMs, Autoregressive Models as Explicit Generative Models, and Generative Adversarial Networks as Implicit PLVMs. Viewing these architectures under a common probabilistic taxonomy reveals shared principles, distinct inference strategies, and the representational trade-offs that shape their strengths. We offer a conceptual roadmap that consolidates generative AI's theoretical foundations, clarifies methodological lineages, and guides future innovation by grounding emerging architectures in their probabilistic heritage.", 'abstract_zh': '从大型语言模型到多模态代理，生成型人工智能现在支撑着最先进的系统。尽管它们的架构各异，许多模型都基于概率潜变量模型（PLVMs）这一共同基础，其中潜变量解释观测数据，用于密度估计、潜在推理和结构化推断。本文通过将经典和现代生成方法都置于PLVM范式之中，提供了一个统一的观点。我们追溯了从经典平坦模型（如概率主成分分析、高斯混合模型、潜在类别分析、项目反应理论和潜在狄利克雷分配）及其顺序扩展（如隐藏马尔可夫模型、高斯HMM和线性动态系统），到当代深度架构（如变分自编码器作为深度PLVM、规范化流作为可计算的PLVM、扩散模型作为顺序PLVM、自回归模型作为显式的生成模型、生成对抗网络作为隐式的PLVM）的进步。从共同的概率分类视角来看，这些架构揭示了共享的原则、不同的推理策略以及塑造其优势的表示权衡。我们提供了一个概念性的路线图，巩固生成型人工智能的理论基础，明确方法论的演变脉络，并通过将新兴架构植根于其概率遗产来指导未来的创新。', 'title_zh': '从经典概率潜在变量模型到现代生成AI：一种统一视角'}
{'arxiv_id': 'arXiv:2508.16636', 'title': 'Cognitive Decision Routing in Large Language Models: When to Think Fast, When to Think Slow', 'authors': 'Y. Du, C. Guo, W. Wang, G. Tang', 'link': 'https://arxiv.org/abs/2508.16636', 'abstract': "Large Language Models (LLMs) face a fundamental challenge in deciding when to rely on rapid, intuitive responses versus engaging in slower, more deliberate reasoning. Inspired by Daniel Kahneman's dual-process theory and his insights on human cognitive biases, we propose a novel Cognitive Decision Routing (CDR) framework that dynamically determines the appropriate reasoning strategy based on query characteristics. Our approach addresses the current limitations where models either apply uniform reasoning depth or rely on computationally expensive methods for all queries. We introduce a meta-cognitive layer that analyzes query complexity through multiple dimensions: correlation strength between given information and required conclusions, domain boundary crossings, stakeholder multiplicity, and uncertainty levels. Through extensive experiments on diverse reasoning tasks, we demonstrate that CDR achieves superior performance while reducing computational costs by 34\\% compared to uniform deep reasoning approaches. Our framework shows particular strength in professional judgment tasks, achieving 23\\% improvement in consistency and 18\\% better accuracy on expert-level evaluations. This work bridges cognitive science principles with practical AI system design, offering a principled approach to adaptive reasoning in LLMs.", 'abstract_zh': '大型语言模型（LLMs）在决定何时依赖快速直观的响应与何时进行更慢但更加谨慎的推理时面临一个根本性的挑战。受到丹尼尔·卡内曼的双过程理论及其对人类认知偏差洞见的启发，我们提出了一种新颖的认知决策路由（CDR）框架，该框架能够动态地根据查询特性确定合适的推理策略。我们的方法解决了当前模型要么采用统一的推理深度，要么依赖于对所有查询都使用计算成本高昂的方法的问题。我们引入了一层元认知层，通过多个维度分析查询的复杂性：给定信息与所需结论的相关强度、领域边界跨越、利益相关方的多样性以及不确定性水平。通过在多种推理任务上的广泛实验，我们证明了CDR在提高性能的同时，计算成本降低了34%。我们的框架在专业判断任务中表现尤为出色，在专家级评估中的一致性提高23%，准确率提高18%。这项工作将认知科学原理与实际的人工智能系统设计相结合，提供了一种适应性推理在LLMs中的原则性方法。', 'title_zh': '大型语言模型中的认知决策路由：何时快速思考，何时深度思考'}
{'arxiv_id': 'arXiv:2508.16629', 'title': 'Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework', 'authors': 'Zeyu Zhang, Quanyu Dai, Rui Li, Xiaohe Bo, Xu Chen, Zhenhua Dong', 'link': 'https://arxiv.org/abs/2508.16629', 'abstract': 'LLM-based agents have been extensively applied across various domains, where memory stands out as one of their most essential capabilities. Previous memory mechanisms of LLM-based agents are manually predefined by human experts, leading to higher labor costs and suboptimal performance. In addition, these methods overlook the memory cycle effect in interactive scenarios, which is critical to optimizing LLM-based agents for specific environments. To address these challenges, in this paper, we propose to optimize LLM-based agents with an adaptive and data-driven memory framework by modeling memory cycles. Specifically, we design an MoE gate function to facilitate memory retrieval, propose a learnable aggregation process to improve memory utilization, and develop task-specific reflection to adapt memory storage. Our memory framework empowers LLM-based agents to learn how to memorize information effectively in specific environments, with both off-policy and on-policy optimization. In order to evaluate the effectiveness of our proposed methods, we conduct comprehensive experiments across multiple aspects. To benefit the research community in this area, we release our project at this https URL.', 'abstract_zh': '基于LLM的智能体已经在多个领域得到了广泛应用，其中记忆是其最为重要的能力之一。现有的基于LLM的智能体的记忆机制大多由人工专家手动预定义，导致了较高的劳动成本和次优性能。此外，这些方法忽略了互动场景中记忆周期效应的关键作用，这对于针对特定环境优化基于LLM的智能体至关重要。为解决这些问题，本文提出了一种通过建模记忆周期来实现自适应和数据驱动的记忆框架，以优化基于LLM的智能体。具体而言，我们设计了一个MoE门控函数以促进记忆检索，提出了一个可学习的聚合过程以提高记忆利用率，并开发了任务特定的反馈以适应记忆存储。我们的记忆框架使基于LLM的智能体能够在特定环境中学习如何有效地记忆信息，并实现了策略离线优化和在线优化。为了评估我们所提出方法的有效性，我们在多个方面进行了全面的实验。为促进该领域的研究，我们在https://this.url/releasesour项目。', 'title_zh': '基于自适应记忆框架优化记忆能力的LLM代理'}
{'arxiv_id': 'arXiv:2508.16603', 'title': 'GreenTEA: Gradient Descent with Topic-modeling and Evolutionary Auto-prompting', 'authors': 'Zheng Dong, Luming Shang, Gabriela Olinto', 'link': 'https://arxiv.org/abs/2508.16603', 'abstract': 'High-quality prompts are crucial for Large Language Models (LLMs) to achieve exceptional performance. However, manually crafting effective prompts is labor-intensive and demands significant domain expertise, limiting its scalability. Existing automatic prompt optimization methods either extensively explore new prompt candidates, incurring high computational costs due to inefficient searches within a large solution space, or overly exploit feedback on existing prompts, risking suboptimal optimization because of the complex prompt landscape. To address these challenges, we introduce GreenTEA, an agentic LLM workflow for automatic prompt optimization that balances candidate exploration and knowledge exploitation. It leverages a collaborative team of agents to iteratively refine prompts based on feedback from error samples. An analyzing agent identifies common error patterns resulting from the current prompt via topic modeling, and a generation agent revises the prompt to directly address these key deficiencies. This refinement process is guided by a genetic algorithm framework, which simulates natural selection by evolving candidate prompts through operations such as crossover and mutation to progressively optimize model performance. Extensive numerical experiments conducted on public benchmark datasets suggest the superior performance of GreenTEA against human-engineered prompts and existing state-of-the-arts for automatic prompt optimization, covering logical and quantitative reasoning, commonsense, and ethical decision-making.', 'abstract_zh': '高质量的提示对于大型语言模型（LLMs）实现卓越的性能至关重要。然而，手动 crafting 有效的提示是劳动密集型的，并且需要显著的领域专业知识，限制了其可扩展性。现有的自动提示优化方法要么广泛探索新的提示候选，由于在大型解空间中无效搜索而导致高昂的计算成本，要么过度利用现有提示的反馈，由于提示景观的复杂性而导致次优优化。为了解决这些挑战，我们介绍了GreenTEA，一种平衡提示候选探索与知识利用的自主大型语言模型工作流。它通过协作团队的代理迭代根据错误样本的反馈来细化提示。分析代理通过主题建模识别当前提示导致的常见错误模式，并生成代理修订提示以直接解决这些关键缺陷。这一细化过程由遗传算法框架指导，通过交叉和变异等操作模拟自然选择，逐步优化模型性能。广泛的数值实验在公开基准数据集上表明，GreenTEA 在自动提示优化方面的性能优于人工设计的提示和现有最先进的方法，涵盖逻辑和定量推理、常识和伦理决策。', 'title_zh': 'GreenTEA：基于主题建模和演化自动生成提示的梯度下降方法'}
{'arxiv_id': 'arXiv:2508.16580', 'title': 'Adaptive Command: Real-Time Policy Adjustment via Language Models in StarCraft II', 'authors': 'Weiyu Ma, Dongyu Xu, Shu Lin, Haifeng Zhang, Jun Wang', 'link': 'https://arxiv.org/abs/2508.16580', 'abstract': 'We present Adaptive Command, a novel framework integrating large language models (LLMs) with behavior trees for real-time strategic decision-making in StarCraft II. Our system focuses on enhancing human-AI collaboration in complex, dynamic environments through natural language interactions. The framework comprises: (1) an LLM-based strategic advisor, (2) a behavior tree for action execution, and (3) a natural language interface with speech capabilities. User studies demonstrate significant improvements in player decision-making and strategic adaptability, particularly benefiting novice players and those with disabilities. This work contributes to the field of real-time human-AI collaborative decision-making, offering insights applicable beyond RTS games to various complex decision-making scenarios.', 'abstract_zh': '我们提出了一种新型框架Adaptive Command，该框架将大型语言模型（LLMs）与行为树相结合，用于StarCraft II中的实时战略决策。该系统专注于通过自然语言交互增强人在复杂动态环境中的AI协作。该框架包括：（1）基于LLM的战略顾问，（2）用于执行动作的行为树，以及（3）具有语音功能的自然语言接口。用户研究显示，在提高玩家决策能力和战略适应性方面取得了显著进步，特别有助于新手玩家和残疾人。本研究为实时人机协作决策领域做出了贡献，提供的见解不仅适用于RTS游戏，还能应用于各种复杂的决策场景。', 'title_zh': '自适应命令：通过语言模型在星际争霸II中实时政策调整'}
{'arxiv_id': 'arXiv:2508.15371', 'title': 'Confidence-Modulated Speculative Decoding for Large Language Models', 'authors': 'Jaydip Sen, Subhasis Dasgupta, Hetvi Waghela', 'link': 'https://arxiv.org/abs/2508.15371', 'abstract': "Speculative decoding has emerged as an effective approach for accelerating autoregressive inference by parallelizing token generation through a draft-then-verify paradigm. However, existing methods rely on static drafting lengths and rigid verification criteria, limiting their adaptability across varying model uncertainties and input complexities. This paper proposes an information-theoretic framework for speculative decoding based on confidence-modulated drafting. By leveraging entropy and margin-based uncertainty measures over the drafter's output distribution, the proposed method dynamically adjusts the number of speculatively generated tokens at each iteration. This adaptive mechanism reduces rollback frequency, improves resource utilization, and maintains output fidelity. Additionally, the verification process is modulated using the same confidence signals, enabling more flexible acceptance of drafted tokens without sacrificing generation quality. Experiments on machine translation and summarization tasks demonstrate significant speedups over standard speculative decoding while preserving or improving BLEU and ROUGE scores. The proposed approach offers a principled, plug-in method for efficient and robust decoding in large language models under varying conditions of uncertainty.", 'abstract_zh': '基于置信度调制的不确定性信息论框架下的 speculative 解码', 'title_zh': '基于置信度调节的推测性解码对于大型语言模型'}
