{'arxiv_id': 'arXiv:2509.04996', 'title': 'FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies', 'authors': 'Moritz Reuss, Hongyi Zhou, Marcel Rühle, Ömer Erdinç Yağmurlu, Fabian Otto, Rudolf Lioutikov', 'link': 'https://arxiv.org/abs/2509.04996', 'abstract': 'Developing efficient Vision-Language-Action (VLA) policies is crucial for practical robotics deployment, yet current approaches face prohibitive computational costs and resource requirements. Existing diffusion-based VLA policies require multi-billion-parameter models and massive datasets to achieve strong performance. We tackle this efficiency challenge with two contributions: intermediate-modality fusion, which reallocates capacity to the diffusion head by pruning up to $50\\%$ of LLM layers, and action-specific Global-AdaLN conditioning, which cuts parameters by $20\\%$ through modular adaptation. We integrate these advances into a novel 950 M-parameter VLA called FLOWER. Pretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance with bigger VLAs across $190$ tasks spanning ten simulation and real-world benchmarks and demonstrates robustness across diverse robotic embodiments. In addition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark. Demos, code and pretrained weights are available at this https URL.', 'abstract_zh': '开发高效的视觉-语言-行动（VLA）策略对于实际机器人部署至关重要，但当前方法面临巨大的计算成本和资源要求。基于扩散的VLA策略需要多百亿参数的模型和大量数据才能实现出色的性能。我们通过两项贡献来应对效率挑战：中间模态融合，通过剪枝最多50%的LLM层从而重新分配给扩散头部的能力；以及针对特定行动的全局-AdaLN调节，通过模块化适应减少20%的参数。我们将这些进展整合到一个新的950M参数VLA模型FLOWER中。FLOWER仅在200个H100 GPU小时内进行预训练，即可在涵盖十个模拟和现实世界基准的190个任务中与更大的VLA模型实现竞争力的性能表现，并在多种机器人实体中展现出鲁棒性。此外，FLOWER在CALVIN ABC基准上的得分达到新的SOTA水平4.53。相关演示、代码和预训练权重可在以下链接获取。', 'title_zh': 'FLOWER: 通过高效视觉-语言-动作流策略实现通用机器人政策的民主化'}
{'arxiv_id': 'arXiv:2509.04970', 'title': 'DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation', 'authors': 'Tien Pham, Xinyun Chi, Khang Nguyen, Manfred Huber, Angelo Cangelosi', 'link': 'https://arxiv.org/abs/2509.04970', 'abstract': 'Reinforcement learning (RL) agents can learn to solve complex tasks from visual inputs, but generalizing these learned skills to new environments remains a major challenge in RL application, especially robotics. While data augmentation can improve generalization, it often compromises sample efficiency and training stability. This paper introduces DeGuV, an RL framework that enhances both generalization and sample efficiency. In specific, we leverage a learnable masker network that produces a mask from the depth input, preserving only critical visual information while discarding irrelevant pixels. Through this, we ensure that our RL agents focus on essential features, improving robustness under data augmentation. In addition, we incorporate contrastive learning and stabilize Q-value estimation under augmentation to further enhance sample efficiency and training stability. We evaluate our proposed method on the RL-ViGen benchmark using the Franka Emika robot and demonstrate its effectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV outperforms state-of-the-art methods in both generalization and sample efficiency while also improving interpretability by highlighting the most relevant regions in the visual input', 'abstract_zh': '基于深度感知输入的强化学习框架DeGuV：提升泛化能力和样本效率', 'title_zh': 'DeGuV: 基于深度引导的视觉强化学习在操作中的泛化和可解释性'}
{'arxiv_id': 'arXiv:2509.04853', 'title': 'A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing', 'authors': 'Chengkai Xu, Jiaqi Liu, Yicheng Guo, Peng Hang, Jian Sun', 'link': 'https://arxiv.org/abs/2509.04853', 'abstract': 'End-to-end autonomous driving remains constrained by the need to generate multi-modal actions, maintain temporal stability, and generalize across diverse scenarios. Existing methods often collapse multi-modality, struggle with long-horizon consistency, or lack modular adaptability. This paper presents KDP, a knowledge-driven diffusion policy that integrates generative diffusion modeling with a sparse mixture-of-experts routing mechanism. The diffusion component generates temporally coherent and multi-modal action sequences, while the expert routing mechanism activates specialized and reusable experts according to context, enabling modular knowledge composition. Extensive experiments across representative driving scenarios demonstrate that KDP achieves consistently higher success rates, reduced collision risk, and smoother control compared to prevailing paradigms. Ablation studies highlight the effectiveness of sparse expert activation and the Transformer backbone, and activation analyses reveal structured specialization and cross-scenario reuse of experts. These results establish diffusion with expert routing as a scalable and interpretable paradigm for knowledge-driven end-to-end autonomous driving.', 'abstract_zh': '基于知识驱动的扩散策略：一种集成生成性扩散建模与稀疏专家路由机制的端到端自主驾驶方法', 'title_zh': '基于专家路径规划的知识驱动扩散策略在端到端自主驾驶中的应用'}
{'arxiv_id': 'arXiv:2509.04836', 'title': 'COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of Everyday Tasks', 'authors': 'Dongping Li, Shaoting Peng, John Pohovey, Katherine Rose Driggs-Campbell', 'link': 'https://arxiv.org/abs/2509.04836', 'abstract': "Continuous advancements in robotics and AI are driving the integration of robots from industry into everyday environments. However, dynamic and unpredictable human activities in daily lives would directly or indirectly conflict with robot actions. Besides, due to the social attributes of such human-induced conflicts, solutions are not always unique and depend highly on the user's personal preferences. To address these challenges and facilitate the development of household robots, we propose COMMET, a system for human-induced COnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid detection approach, which begins with multi-modal retrieval and escalates to fine-tuned model inference for low-confidence cases. Based on collected user preferred options and settings, GPT-4o will be used to summarize user preferences from relevant cases. In preliminary studies, our detection module shows better accuracy and latency compared with GPT models. To facilitate future research, we also design a user-friendly interface for user data collection and demonstrate an effective workflow for real-world deployments.", 'abstract_zh': '持续的机器人与AI进步推动了工业机器人向日常生活环境的集成。然而，日常生活中动态且不可预测的人类活动会直接或间接地与机器人行动产生冲突。由于此类人类引发冲突的社会属性，解决方案并非总是唯一的，高度依赖用户的个人偏好。为应对这些挑战并促进家庭机器人的发展，我们提出了COMMET系统，这是一个针对移动操作日常任务中人类引发冲突的系统。COMMET采用混合检测方法，始于多模态检索，并在低置信度情况下升级为细调模型推理。基于收集的用户偏好选项和设置，将使用GPT-4o总结相关案例中的用户偏好。初步研究表明，我们的检测模块在准确性和延迟方面优于GPT模型。为了促进未来研究，我们还设计了一个用户友好的界面以收集用户数据，并展示了适用于实际部署的有效工作流。', 'title_zh': 'COMMET：一种用于移动执行日常任务中人工引发冲突的系统'}
{'arxiv_id': 'arXiv:2509.04737', 'title': 'Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics', 'authors': 'Ryoga Oishi, Sho Sakaino, Toshiaki Tsuji', 'link': 'https://arxiv.org/abs/2509.04737', 'abstract': 'In the field of robot learning, coordinating robot actions through language instructions is becoming increasingly feasible. However, adapting actions to human instructions remains challenging, as such instructions are often qualitative and require exploring behaviors that satisfy varying conditions. This paper proposes a motion generation model that adapts robot actions in response to modifier directives human instructions imposing behavioral conditions during task execution. The proposed method learns a mapping from modifier directives to actions by segmenting demonstrations into short sequences, assigning weakly supervised labels corresponding to specific modifier types. We evaluated our method in wiping and pick and place tasks. Results show that it can adjust motions online in response to modifier directives, unlike conventional batch-based methods that cannot adapt during execution.', 'abstract_zh': '基于语言指令协调机器人动作生成模型', 'title_zh': '基于行为特征解耦表示的学习模仿'}
{'arxiv_id': 'arXiv:2509.04722', 'title': 'Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots', 'authors': 'Adrian B. Ghansah, Sergio A. Esteban, Aaron D. Ames', 'link': 'https://arxiv.org/abs/2509.04722', 'abstract': 'As humanoid robots enter real-world environments, ensuring robust locomotion across diverse environments is crucial. This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models -- enabling versatile step planning and incorporating arm and torso dynamics to better stabilize the walking. At the high level, we use the step-to-step dynamics of the ALIP model to simultaneously optimize over step periods, step lengths, and ankle torques via nonlinear MPC. The ALIP trajectories are used as references to a linear MPC framework that extends the standard SRB-MPC to also include simplified arm and torso dynamics. We validate the performance of our approach through simulation and hardware experiments on the Unitree G1 humanoid robot. In the proposed framework the high-level step planner runs at 40 Hz and the mid-level MPC at 500 Hz using the onboard mini-PC. Adaptive step timing increased the push recovery success rate by 36%, and the upper body control improved the yaw disturbance rejection. We also demonstrate robust locomotion across diverse indoor and outdoor terrains, including grass, stone pavement, and uneven gym mats.', 'abstract_zh': 'humanoid机器人进入真实环境后，确保其在多样化环境中具有稳健的行走能力至关重要。本文提出了一种基于降阶模型的高效层次控制框架，用于人形机器人行走控制——该框架使步态规划更具灵活性，并整合了上肢和躯干动力学以更好地稳定行走。在高层次上，我们利用ALIP模型的步距间动力学，通过非线性MPC同时优化步长周期、步长长度和踝关节扭矩。ALIP轨迹被用作扩展了标准SRB-MPC框架的线性MPC框架的参考，该框架还包含了简化后的上肢和躯干动力学。我们通过在Unitree G1人形机器人上的仿真和硬件实验验证了本方法的性能。在所提出框架中，高层次的步态规划器运行频率为40Hz，中间层的MPC运行频率为500Hz，使用机载微型PC。自适应步态定时将推动恢复成功率提高了36%，上身控制改善了对偏航扰动的抑制。我们还展示了机器人在多种室内和室外地形上的稳健行走能力，包括草地、石板路和凹凸不平的体育馆垫子。', 'title_zh': 'humanoid机器人稳健分级降阶模型预测控制'}
{'arxiv_id': 'arXiv:2509.04712', 'title': 'Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving', 'authors': 'Zhihao Zhang, Chengyang Peng, Ekim Yurtsever, Keith A. Redmill', 'link': 'https://arxiv.org/abs/2509.04712', 'abstract': 'Automated vehicle control using reinforcement learning (RL) has attracted significant attention due to its potential to learn driving policies through environment interaction. However, RL agents often face training challenges in sample efficiency and effective exploration, making it difficult to discover an optimal driving strategy. To address these issues, we propose guiding the RL driving agent with a demonstration policy that need not be a highly optimized or expert-level controller. Specifically, we integrate a rule-based lane change controller with the Soft Actor Critic (SAC) algorithm to enhance exploration and learning efficiency. Our approach demonstrates improved driving performance and can be extended to other driving scenarios that can similarly benefit from demonstration-based guidance.', 'abstract_zh': '使用强化学习的自动驾驶车辆控制吸引了广泛关注，因为它可以通过环境交互学习驾驶策略。然而，强化学习代理在样本效率和有效探索方面常常面临训练挑战，使得难以发现最优驾驶策略。为应对这些问题，我们提出使用一个不需要是高度优化或专家级控制的演示策略来引导强化学习驾驶代理。具体而言，我们将基于规则的变道控制器与Soft Actor-Critic (SAC) 算法结合以增强探索能力和学习效率。我们的方法展示了改进的驾驶性能，并可以扩展到其他可以从演示指导中受益的驾驶场景。', 'title_zh': '基于次优策略的增强学习自举方法在自主驾驶中的应用'}
{'arxiv_id': 'arXiv:2509.04535', 'title': 'In-Context Policy Adaptation via Cross-Domain Skill Diffusion', 'authors': 'Minjong Yoo, Woo Kyung Kim, Honguk Woo', 'link': 'https://arxiv.org/abs/2509.04535', 'abstract': 'In this work, we present an in-context policy adaptation (ICPAD) framework designed for long-horizon multi-task environments, exploring diffusion-based skill learning techniques in cross-domain settings. The framework enables rapid adaptation of skill-based reinforcement learning policies to diverse target domains, especially under stringent constraints on no model updates and only limited target domain data. Specifically, the framework employs a cross-domain skill diffusion scheme, where domain-agnostic prototype skills and a domain-grounded skill adapter are learned jointly and effectively from an offline dataset through cross-domain consistent diffusion processes. The prototype skills act as primitives for common behavior representations of long-horizon policies, serving as a lingua franca to bridge different domains. Furthermore, to enhance the in-context adaptation performance, we develop a dynamic domain prompting scheme that guides the diffusion-based skill adapter toward better alignment with the target domain. Through experiments with robotic manipulation in Metaworld and autonomous driving in CARLA, we show that our $\\oursol$ framework achieves superior policy adaptation performance under limited target domain data conditions for various cross-domain configurations including differences in environment dynamics, agent embodiment, and task horizon.', 'abstract_zh': '基于上下文的多任务环境长时域策略适应框架：跨域扩散技能学习方法', 'title_zh': '基于跨域技能扩散的上下文适配策略调整'}
{'arxiv_id': 'arXiv:2509.05116', 'title': 'Analyzing Gait Adaptation with Hemiplegia Simulation Suits and Digital Twins', 'authors': 'Jialin Chen, Jeremie Clos, Dominic Price, Praminda Caleb-Solly', 'link': 'https://arxiv.org/abs/2509.05116', 'abstract': 'To advance the development of assistive and rehabilitation robots, it is essential to conduct experiments early in the design cycle. However, testing early prototypes directly with users can pose safety risks. To address this, we explore the use of condition-specific simulation suits worn by healthy participants in controlled environments as a means to study gait changes associated with various impairments and support rapid prototyping. This paper presents a study analyzing the impact of a hemiplegia simulation suit on gait. We collected biomechanical data using a Vicon motion capture system and Delsys Trigno EMG and IMU sensors under four walking conditions: with and without a rollator, and with and without the simulation suit. The gait data was integrated into a digital twin model, enabling machine learning analyses to detect the use of the simulation suit and rollator, identify turning behavior, and evaluate how the suit affects gait over time. Our findings show that the simulation suit significantly alters movement and muscle activation patterns, prompting users to compensate with more abrupt motions. We also identify key features and sensor modalities that are most informative for accurately capturing gait dynamics and modeling human-rollator interaction within the digital twin framework.', 'abstract_zh': '基于条件特定模拟服的健康参与者在受控环境中的步态变化研究以支持助行与康复机器人快速原型开发', 'title_zh': '基于半身瘫痪模拟装置与数字孪生的步态适应性分析'}
{'arxiv_id': 'arXiv:2509.04731', 'title': 'Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning', 'authors': 'Brennen Hill', 'link': 'https://arxiv.org/abs/2509.04731', 'abstract': 'The convergence of Language models, Agent models, and World models represents a critical frontier for artificial intelligence. While recent progress has focused on scaling Language and Agent models, the development of sophisticated, explicit World Models remains a key bottleneck, particularly for complex, long-horizon multi-agent tasks. In domains such as robotic soccer, agents trained via standard reinforcement learning in high-fidelity but structurally-flat simulators often fail due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing capable agents lies in creating environments that possess an explicit, hierarchical World Model. We contend that this is best achieved through hierarchical scaffolding, where complex goals are decomposed into structured, manageable subgoals. Drawing evidence from a systematic review of 2024 research in multi-agent soccer, we identify a clear and decisive trend towards integrating symbolic and hierarchical methods with multi-agent reinforcement learning (MARL). These approaches implicitly or explicitly construct a task-based world model to guide agent learning. We then propose a paradigm shift: leveraging Large Language Models to dynamically generate this hierarchical scaffold, effectively using language to structure the World Model on the fly. This language-driven world model provides an intrinsic curriculum, dense and meaningful learning signals, and a framework for compositional learning, enabling Agent Models to acquire sophisticated, strategic behaviors with far greater sample efficiency. By building environments with explicit, language-configurable task layers, we can bridge the gap between low-level reactive behaviors and high-level strategic team play, creating a powerful and generalizable framework for training the next generation of intelligent agents.', 'abstract_zh': '语言模型、代理模型和世界模型的收敛代表了人工智能的一个关键前沿。尽管近期的研究重点在于扩展语言和代理模型，但开发复杂、显式的世界模型仍然是一个关键瓶颈，特别是在复杂的、长期多代理任务中。在诸如机器人足球的领域中，通过在高保真但结构平坦的模拟器中使用标准强化学习训练的代理，常常由于难以探索的空间和稀疏的奖励而失败。本文认为，在开发强大代理的下一个前沿是创建具备明确层次结构的世界模型的环境。我们主张这可以通过层次结构的脚手架来实现，即将复杂的目标分解为结构化、可管理的子目标。通过系统回顾2024年多代理足球研究，我们发现了一种清晰而明显的趋势，即将符号和层次方法与多代理强化学习（MARL）集成。这些方法隐式或显式地构建基于任务的世界模型，以指导代理学习。随后，我们提出了一个范式转变：利用大型语言模型动态生成这种层次结构的脚手架，有效地使用语言在实时构建世界模型。这种语言驱动的世界模型提供了内在的学习课程、密集且有意义的学习信号，并为组合学习提供了框架，使代理模型能够以极高的样本效率获得复杂的、战略性的行为。通过构建具有明确语言可配置任务层的环境，我们可以弥合低层级反应行为与高层级战略团队玩法之间的差距，从而为训练下一代智能代理提供一个强大且通用的框架。', 'title_zh': '语言驱动的层次任务结构作为多智能体学习的显式世界模型'}
{'arxiv_id': 'arXiv:2509.05263', 'title': 'LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation', 'authors': 'Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Hao Jiang, Kang Chen, Shuang Qiu', 'link': 'https://arxiv.org/abs/2509.05263', 'abstract': 'Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at this https URL', 'abstract_zh': 'recent 研究越来越多地关注开发能够模拟复杂真实世界场景的3D世界模型。世界模型在各类领域中找到了广泛的应用，包括具身AI、自主驾驶、娱乐等。更真实的模拟和精确的物理模型将有效缩小仿真与现实之间的差距，并且便于我们方便地收集有关现实世界的丰富信息。虽然传统的手动建模能够创建虚拟3D场景，但现代方法利用了先进的机器学习算法进行3D世界生成，其中最近的进展主要集中在基于用户指令生成虚拟世界的生成方法上。本研究通过提出LatticeWorld，一个简单而有效的3D世界生成框架来探索这一研究方向，该框架简化了3D环境的工业生产流程。LatticeWorld 利用轻量级的LLM（如LaMA-2-7B）以及工业级渲染引擎（例如Unreal Engine 5）生成动态环境。我们提出的框架接受文本描述和视觉指令作为多模态输入，并能够生成具有高度交互性的大规模3D世界，这些世界特征包括高性能的多智能体交互、高保真的物理模拟以及实时渲染。我们进行了全面的实验来评估LatticeWorld，结果显示其在场景布局生成和视觉保真度方面取得了卓越的准确性。此外，LatticeWorld 在工业生产效率上实现了超过90倍的提升，同时保持了与传统手动生产方法相当的高质量。我们的演示视频可在以下链接访问。', 'title_zh': 'LatticeWorld: 一个基于大规模多模态语言模型的交互式复杂世界生成框架'}
{'arxiv_id': 'arXiv:2509.04908', 'title': 'SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing', 'authors': 'Hongyi Jing, Jiafu Chen, Chen Rao, Ziqiang Dang, Jiajie Teng, Tianyi Chu, Juncheng Mo, Shuo Fang, Huaizhong Lin, Rui Lv, Chenguang Ma, Lei Zhao', 'link': 'https://arxiv.org/abs/2509.04908', 'abstract': 'The existing Multimodal Large Language Models (MLLMs) for GUI perception have made great progress. However, the following challenges still exist in prior methods: 1) They model discrete coordinates based on text autoregressive mechanism, which results in lower grounding accuracy and slower inference speed. 2) They can only locate predefined sets of elements and are not capable of parsing the entire interface, which hampers the broad application and support for downstream tasks. To address the above issues, we propose SparkUI-Parser, a novel end-to-end framework where higher localization precision and fine-grained parsing capability of the entire interface are simultaneously achieved. Specifically, instead of using probability-based discrete modeling, we perform continuous modeling of coordinates based on a pre-trained Multimodal Large Language Model (MLLM) with an additional token router and coordinate decoder. This effectively mitigates the limitations inherent in the discrete output characteristics and the token-by-token generation process of MLLMs, consequently boosting both the accuracy and the inference speed. To further enhance robustness, a rejection mechanism based on a modified Hungarian matching algorithm is introduced, which empowers the model to identify and reject non-existent elements, thereby reducing false positives. Moreover, we present ScreenParse, a rigorously constructed benchmark to systematically assess structural perception capabilities of GUI models across diverse scenarios. Extensive experiments demonstrate that our approach consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2, CAGUI-Grounding and ScreenParse benchmarks. The resources are available at this https URL.', 'abstract_zh': '现有的多模态大型语言模型（MLLMs）在GUI感知方面取得了显著进展，但仍存在以下挑战：1) 基于文本自回归机制建模离散坐标，导致较低的语义关联准确性和较慢的推理速度。2) 只能定位预定义的元素集，无法解析整个界面，限制了下游任务的广泛应用和支持。为解决这些问题，我们提出了一种名为SparkUI-Parser的新颖端到端框架，同时实现了更高的局部化精度和对整个界面的细粒度解析能力。具体来说，我们采用预训练的多模态大型语言模型（MLLM）和附加的令牌路由器及坐标解码器进行连续坐标建模，而非基于概率的离散建模。这有效地缓解了MLLM固有的离散输出特性和逐令牌生成过程的局限性，从而提高准确性和推理速度。为进一步增强鲁棒性，我们引入了一种基于修改后的匈牙利匹配算法的拒绝机制，使模型能够识别并拒绝不存在的元素，从而降低误报率。此外，我们提出了ScreenParse，这是一个严格构建的基准，系统评估GUI模型在多种场景下的结构感知能力。广泛实验表明，我们的方法在ScreenSpot、ScreenSpot-v2、CAGUI-Grounding和ScreenParse基准上始终优于当前最佳方法。资源可在以下网址获取。', 'title_zh': 'SparkUI-Parser：增强UI感知的稳健_grounding_和解析'}
{'arxiv_id': 'arXiv:2509.05066', 'title': 'ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions', 'authors': 'Matteo Bortoletto, Constantin Ruhdorfer, Andreas Bulling', 'link': 'https://arxiv.org/abs/2509.05066', 'abstract': "Most existing Theory of Mind (ToM) benchmarks for foundation models rely on variations of the Sally-Anne test, offering only a very limited perspective on ToM and neglecting the complexity of human social interactions. To address this gap, we propose ToM-SSI: a new benchmark specifically designed to test ToM capabilities in environments rich with social interactions and spatial dynamics. While current ToM benchmarks are limited to text-only or dyadic interactions, ToM-SSI is multimodal and includes group interactions of up to four agents that communicate and move in situated environments. This unique design allows us to study, for the first time, mixed cooperative-obstructive settings and reasoning about multiple agents' mental state in parallel, thus capturing a wider range of social cognition than existing benchmarks. Our evaluations reveal that the current models' performance is still severely limited, especially in these new tasks, highlighting critical gaps for future research.", 'abstract_zh': 'ToM-SSI: 一种新的社会互动和空间动态丰富的Theory of Mind基准', 'title_zh': 'ToM-SSI: 评估情境社会互动中的心智理论'}
{'arxiv_id': 'arXiv:2509.04772', 'title': 'FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph', 'authors': 'Zhangding Liu, Neda Mohammadi, John E. Taylor', 'link': 'https://arxiv.org/abs/2509.04772', 'abstract': "Timely and accurate floodwater depth estimation is critical for road accessibility and emergency response. While recent computer vision methods have enabled flood detection, they suffer from both accuracy limitations and poor generalization due to dependence on fixed object detectors and task-specific training. To enable accurate depth estimation that can generalize across diverse flood scenarios, this paper presents FloodVision, a zero-shot framework that combines the semantic reasoning abilities of the foundation vision-language model GPT-4o with a structured domain knowledge graph. The knowledge graph encodes canonical real-world dimensions for common urban objects including vehicles, people, and infrastructure elements to ground the model's reasoning in physical reality. FloodVision dynamically identifies visible reference objects in RGB images, retrieves verified heights from the knowledge graph to mitigate hallucination, estimates submergence ratios, and applies statistical outlier filtering to compute final depth values. Evaluated on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and surpassing prior CNN-based methods. The system generalizes well across varying scenes and operates in near real-time, making it suitable for future integration into digital twin platforms and citizen-reporting apps for smart city flood resilience.", 'abstract_zh': '及时准确的水位深度估计对于道路通行能力和应急响应至关重要。虽然最近的计算机视觉方法能够实现洪水检测，但它们在准确性和泛化能力上存在局限性，这是因为依赖于固定的对象检测器和特定任务的训练。为了实现能够跨多样的洪水场景泛化的准确深度估计，本文提出了FloodVision，该框架结合了基础视觉-语言模型GPT-4o的语义推理能力和结构化的领域知识图谱。知识图谱编码了常见的城市对象（包括车辆、人员和基础设施元素）的典型现实世界尺寸，以使模型的推理基于物理现实。FloodVision动态识别RGB图像中的可见参考对象，从知识图谱中检索验证后的高度以减轻幻觉，估计淹没比率，并应用统计异常值过滤来计算最终的深度值。在MyCoast New York提供的110张 crowdsourced 图像上评估，FloodVision的平均绝对误差为8.17 cm，相对于GPT-4o基线减少了10.28 cm的20.5%，并超越了先前的基于CNN的方法。该系统在各种场景下表现出良好的泛化能力，并能够在近乎实时地运行，使其适合未来集成到数字孪生平台和市民报告应用中以增强智慧城市抗洪能力。', 'title_zh': 'FloodVision：基于基础视觉-语言模型和领域知识图的城市内涝深度估计'}
{'arxiv_id': 'arXiv:2509.04633', 'title': 'Scaling Environments for Organoid Intelligence with LLM-Automated Design and Plasticity-Based Evaluation', 'authors': 'Brennen Hill', 'link': 'https://arxiv.org/abs/2509.04633', 'abstract': 'As the complexity of artificial agents increases, the design of environments that can effectively shape their behavior and capabilities has become a critical research frontier. We propose a framework that extends this principle to a novel class of agents: biological neural networks in the form of neural organoids. This paper introduces three scalable, closed-loop virtual environments designed to train organoid-based biological agents and probe the underlying mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments with increasing complexity: (1) a conditional avoidance task, (2) a one-dimensional predator-prey scenario, and (3) a replication of the classic Pong game. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation. Furthermore, we propose a novel meta-learning approach where a Large Language Model (LLM) is used to automate the generation and optimization of experimental protocols, scaling the process of environment and curriculum design. Finally, we outline a multi-modal approach for evaluating learning by measuring synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between computational neuroscience and agent-based AI, offering a unique platform for studying embodiment, learning, and intelligence in a controlled biological substrate.', 'abstract_zh': '随着人工代理复杂性的增加，设计能够有效塑造其行为和能力的环境已成为一个关键的研究前沿。本文提出了一种框架，将其原则扩展到一类新型代理：神经器官oid形式的生物神经网络。本文介绍了三种可扩展的闭环虚拟环境，旨在训练基于器官oid的生物代理，并探究诸如长时 potentiation（LTP）和长时去 potentiation（LTD）等学习机制的内在机制。我们详细介绍了三种不同复杂度的任务环境设计：（1）条件回避任务；（2）一维捕食者与猎物场景；（3）经典的Pong游戏的复现。对于每个环境，我们形式化了状态空间和动作空间，感知编码和运动解码机制，以及基于可预测（奖励）和不可预测（惩罚）刺激的反馈协议。此外，我们提出了一种新型的元学习方法，其中大型语言模型（LLM）用于自动化实验协议的生成和优化，扩大环境和课程设计的过程。最后，我们概述了一种多模态方法，通过在电生理、细胞和分子水平上测量突触可塑性来评估学习。这项工作在计算神经科学和基于代理的人工智能之间架起了桥梁，提供了一个独特平台，用于在受控的生物基质中研究体认、学习和智能。', 'title_zh': '基于LLM自动设计和塑性评价的类器官智能扩展环境'}
{'arxiv_id': 'arXiv:2509.04465', 'title': 'Emotionally-Aware Agents for Dispute Resolution', 'authors': 'Sushrita Rakshit, James Hale, Kushal Chawla, Jeanne M. Brett, Jonathan Gratch', 'link': 'https://arxiv.org/abs/2509.04465', 'abstract': "In conflict, people use emotional expressions to shape their counterparts' thoughts, feelings, and actions. This paper explores whether automatic text emotion recognition offers insight into this influence in the context of dispute resolution. Prior work has shown the promise of such methods in negotiations; however, disputes evoke stronger emotions and different social processes. We use a large corpus of buyer-seller dispute dialogues to investigate how emotional expressions shape subjective and objective outcomes. We further demonstrate that large-language models yield considerably greater explanatory power than previous methods for emotion intensity annotation and better match the decisions of human annotators. Findings support existing theoretical models for how emotional expressions contribute to conflict escalation and resolution and suggest that agent-based systems could be useful in managing disputes by recognizing and potentially mitigating emotional escalation.", 'abstract_zh': '冲突中，人们通过情感表达来影响对方的思维、情感和行为。本文探讨自动文本情感识别在纠纷解决情境下对这种影响的洞察。以往研究显示此类方法在谈判中有潜力；然而，纠纷引发的情感更强且涉及不同的社会过程。我们使用大量的买家卖家纠纷对话数据，研究情感表达如何影响主观和客观结果。此外，我们证明大规模语言模型在情绪强度标注上的解释力显著优于之前的方法，并更好地匹配了人类标注者的决策。研究结果支持现有的关于情感表达在冲突升级和解决中作用的理论模型，并表明基于代理的系统在通过识别和潜在缓解情绪升级来管理纠纷方面可能有用。', 'title_zh': '情绪感知代理在纠纷解决中的应用'}
