{'arxiv_id': 'arXiv:2510.00783', 'title': 'Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions', 'authors': 'Thanh Nguyen Canh, Haolan Zhang, Xiem HoangVan, Nak Young Chong', 'link': 'https://arxiv.org/abs/2510.00783', 'abstract': 'Semantic Simultaneous Localization and Mapping (SLAM) is a critical area of research within robotics and computer vision, focusing on the simultaneous localization of robotic systems and associating semantic information to construct the most accurate and complete comprehensive model of the surrounding environment. Since the first foundational work in Semantic SLAM appeared more than two decades ago, this field has received increasing attention across various scientific communities. Despite its significance, the field lacks comprehensive surveys encompassing recent advances and persistent challenges. In response, this study provides a thorough examination of the state-of-the-art of Semantic SLAM techniques, with the aim of illuminating current trends and key obstacles. Beginning with an in-depth exploration of the evolution of visual SLAM, this study outlines its strengths and unique characteristics, while also critically assessing previous survey literature. Subsequently, a unified problem formulation and evaluation of the modular solution framework is proposed, which divides the problem into discrete stages, including visual localization, semantic feature extraction, mapping, data association, and loop closure optimization. Moreover, this study investigates alternative methodologies such as deep learning and the utilization of large language models, alongside a review of relevant research about contemporary SLAM datasets. Concluding with a discussion on potential future research directions, this study serves as a comprehensive resource for researchers seeking to navigate the complex landscape of Semantic SLAM.', 'abstract_zh': '语义同步定位与建图（SLAM）是机器人学和计算机视觉领域的关键研究领域，专注于同时定位机器人系统并关联语义信息以构建周围环境的最准确和完整的综合模型。自语义SLAM的第一个基础性工作出现以来二十年多的时间里，该领域受到了跨多个科学社群的越来越多的关注。尽管其重要性日益凸显，但该领域缺少全面综述，涵盖近期进展和持久性挑战。为此，本研究提供了对先进语义SLAM技术的全面考察，旨在揭示当前趋势和关键障碍。首先深入探讨视觉SLAM的发展演变，概述其优势和独特特性，并对之前的综述文献进行批判性评估。随后，提出统一的问题表述和模块化解决方案框架，将问题分解为视觉定位、语义特征提取、建图、数据关联和回路闭合优化等离散阶段。此外，本研究还探讨了深度学习等替代方法论及其采用大规模语言模型的应用，并回顾了关于当前SLAM数据集的相关研究。最后，讨论潜在的未来研究方向，本研究为希望导航复杂语义SLAM领域的研究者提供了一份全面资源。', 'title_zh': '基于语义的视觉同步定位与建图：对当前研究、挑战及未来方向的综述'}
{'arxiv_id': 'arXiv:2510.00401', 'title': 'Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting', 'authors': 'Shounak Sural, Charles Kekeh, Wenliang Liu, Federico Pecora, Mouhacine Benosman', 'link': 'https://arxiv.org/abs/2510.00401', 'abstract': 'Long-horizon motion forecasting for multiple autonomous robots is challenging due to non-linear agent interactions, compounding prediction errors, and continuous-time evolution of dynamics. Learned dynamics of such a system can be useful in various applications such as travel time prediction, prediction-guided planning and generative simulation. In this work, we aim to develop an efficient trajectory forecasting model conditioned on multi-agent goals. Motivated by the recent success of physics-guided deep learning for partially known dynamical systems, we develop a model based on neural Controlled Differential Equations (CDEs) for long-horizon motion forecasting. Unlike discrete-time methods such as RNNs and transformers, neural CDEs operate in continuous time, allowing us to combine physics-informed constraints and biases to jointly model multi-robot dynamics. Our approach, named PINCoDE (Physics-Informed Neural Controlled Differential Equations), learns differential equation parameters that can be used to predict the trajectories of a multi-agent system starting from an initial condition. PINCoDE is conditioned on future goals and enforces physics constraints for robot motion over extended periods of time. We adopt a strategy that scales our model from 10 robots to 100 robots without the need for additional model parameters, while producing predictions with an average ADE below 0.5 m for a 1-minute horizon. Furthermore, progressive training with curriculum learning for our PINCoDE model results in a 2.7X reduction of forecasted pose error over 4 minute horizons compared to analytical models.', 'abstract_zh': '长时 horizon 多自主机器人运动预测具有挑战性，由于非线性代理交互、累积预测误差以及动力学的连续时间演变。此类系统中学到的动力学在旅行时间预测、预测指导规划和生成性模拟等多种应用中可能是有用的。在这项工作中，我们旨在开发一种基于多代理目标条件的高效轨迹预测模型。受最近物理引导的深度学习在部分已知动力学系统中的成功启发，我们基于神经控制微分方程（CDEs）开发了一种长时 horizon 运动预测模型。与像 RNN 和变压器这样的离散时间方法不同，神经 CDEs 在连续时间中操作，使我们能够结合物理约束和偏置来联合建模多机器人动力学。我们的方法名为 PINCoDE（物理引导的神经控制微分方程），能够从初始条件预测多代理系统的轨迹。PINCoDE 依赖于未来的任务目标，并在长时间段内强制执行机器人运动的物理约束。我们采用一种策略，无需增加模型参数即可将模型扩展从 10 个机器人到 100 个机器人，同时在 1 分钟的展望期内平均 ADE 低于 0.5 米。此外，对于我们的 PINCoDE 模型采用逐级训练与课程学习策略，在 4 分钟的展望期内，预测姿态误差降低了 2.7 倍，相比于分析模型而言。', 'title_zh': '基于物理的神经控制微分方程在可扩展的多agents长时间轨迹预测中的应用'}
{'arxiv_id': 'arXiv:2510.01049', 'title': 'KeySG: Hierarchical Keyframe-Based 3D Scene Graphs', 'authors': 'Abdelrhman Werby, Dennis Rotondi, Fabio Scaparro, Kai O. Arras', 'link': 'https://arxiv.org/abs/2510.01049', 'abstract': "In recent years, 3D scene graphs have emerged as a powerful world representation, offering both geometric accuracy and semantic richness. Combining 3D scene graphs with large language models enables robots to reason, plan, and navigate in complex human-centered environments. However, current approaches for constructing 3D scene graphs are semantically limited to a predefined set of relationships, and their serialization in large environments can easily exceed an LLM's context window. We introduce KeySG, a framework that represents 3D scenes as a hierarchical graph consisting of floors, rooms, objects, and functional elements, where nodes are augmented with multi-modal information extracted from keyframes selected to optimize geometric and visual coverage. The keyframes allow us to efficiently leverage VLM to extract scene information, alleviating the need to explicitly model relationship edges between objects, enabling more general, task-agnostic reasoning and planning. Our approach can process complex and ambiguous queries while mitigating the scalability issues associated with large scene graphs by utilizing a hierarchical retrieval-augmented generation (RAG) pipeline to extract relevant context from the graph. Evaluated across four distinct benchmarks --including 3D object segmentation and complex query retrieval-- KeySG outperforms prior approaches on most metrics, demonstrating its superior semantic richness and efficiency.", 'abstract_zh': '近年来，3D 场景图已成为一种强大的世界表示形式，提供了几何精确性和语义丰富性。将3D 场景图与大规模语言模型结合，使机器人能够对在复杂的人本环境中进行推理、规划和导航。然而，当前构建3D 场景图的方法在语义上局限于预定义的关系集，其在大规模环境中的序列化可能会很容易超出语言模型的上下文窗口。我们引入了KeySG框架，该框架将3D 场景表示为一个分层图，包含楼层、房间、物体和功能性元素，节点通过从优化几何和视觉覆盖的关键帧中提取的多模态信息进行扩充。关键帧使我们能够高效地利用视觉语言模型提取场景信息，减少了显式建模物体间关系边的需求，从而实现更具通用性、任务无关的推理和规划。该方法能够在处理复杂和模糊查询时，通过利用分层检索增强生成（RAG）管道从图中提取相关上下文，缓解大规模场景图相关的问题。在四个不同的基准测试中，包括3D物体分割和复杂查询检索，KeySG在大多数指标上优于先前的方法，证明了其卓越的语义丰富性和效率。', 'title_zh': 'KeySG：基于分层关键帧的3D场景图'}
{'arxiv_id': 'arXiv:2510.00060', 'title': 'Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving', 'authors': 'Sheng Yang, Tong Zhan, Guancheng Chen, Yanfeng Lu, Jian Wang', 'link': 'https://arxiv.org/abs/2510.00060', 'abstract': 'In this work, we reconceptualize autonomous driving as a generalized language and formulate the trajectory planning task as next waypoint prediction. We introduce Max-V1, a novel framework for one-stage end-to-end autonomous driving. Our framework presents a single-pass generation paradigm that aligns with the inherent sequentiality of driving. This approach leverages the generative capacity of the VLM (Vision-Language Model) to enable end-to-end trajectory prediction directly from front-view camera input. The efficacy of this method is underpinned by a principled supervision strategy derived from statistical modeling. This provides a well-defined learning objective, which makes the framework highly amenable to master complex driving policies through imitation learning from large-scale expert demonstrations. Empirically, our method achieves the state-of-the-art performance on the nuScenes dataset, delivers an overall improvement of over 30% compared to prior baselines. Furthermore, it exhibits superior generalization performance on cross-domain datasets acquired from diverse vehicles, demonstrating notable potential for cross-vehicle robustness and adaptability. Due to these empirical strengths, this work introduces a model enabling fundamental driving behaviors, laying the foundation for the development of more capable self-driving agents. Code will be available upon publication.', 'abstract_zh': '本研究重新conceptualize自动驾驶为一种通用语言，并将轨迹规划任务形式化为下一目标点预测。我们引入Max-V1，一种新的端到端自动驾驶一阶段框架。该框架采用单次通过生成 paradigm，符合驾驶固有的序列性。该方法利用VLM（视觉-语言模型）的生成能力，直接从前方摄像头输入实现端到端的轨迹预测。该方法的有效性源于从统计建模中得出的原则性监督策略，这为通过大规模专家演示模仿学习掌握复杂驾驶策略提供了明确的学习目标。实验上，该方法在nuScenes数据集上达到state-of-the-art性能，并比先前基线提高了超过30%的整体性能。此外，该方法在不同车辆采集的跨域数据集中表现出优越的泛化性能，展示了跨车辆鲁棒性和适应性的显著潜力。由于这些实验上的优势，本研究引入了一个模型，奠定开发更强大自动驾驶代理的基础。代码将在发表后对外开放。', 'title_zh': '少即是多：轻量而强大的 autonomous driving 视觉-语言模型'}
{'arxiv_id': 'arXiv:2510.00627', 'title': 'Collaborative-Distilled Diffusion Models (CDDM) for Accelerated and Lightweight Trajectory Prediction', 'authors': 'Bingzhang Wang, Kehua Chen, Yinhai Wang', 'link': 'https://arxiv.org/abs/2510.00627', 'abstract': "Trajectory prediction is a fundamental task in Autonomous Vehicles (AVs) and Intelligent Transportation Systems (ITS), supporting efficient motion planning and real-time traffic safety management. Diffusion models have recently demonstrated strong performance in probabilistic trajectory prediction, but their large model size and slow sampling process hinder real-world deployment. This paper proposes Collaborative-Distilled Diffusion Models (CDDM), a novel method for real-time and lightweight trajectory prediction. Built upon Collaborative Progressive Distillation (CPD), CDDM progressively transfers knowledge from a high-capacity teacher diffusion model to a lightweight student model, jointly reducing both the number of sampling steps and the model size across distillation iterations. A dual-signal regularized distillation loss is further introduced to incorporate guidance from both the teacher and ground-truth data, mitigating potential overfitting and ensuring robust performance. Extensive experiments on the ETH-UCY pedestrian benchmark and the nuScenes vehicle benchmark demonstrate that CDDM achieves state-of-the-art prediction accuracy. The well-distilled CDDM retains 96.2% and 95.5% of the baseline model's ADE and FDE performance on pedestrian trajectories, while requiring only 231K parameters and 4 or 2 sampling steps, corresponding to 161x compression, 31x acceleration, and 9 ms latency. Qualitative results further show that CDDM generates diverse and accurate trajectories under dynamic agent behaviors and complex social interactions. By bridging high-performing generative models with practical deployment constraints, CDDM enables resource-efficient probabilistic prediction for AVs and ITS. Code is available at this https URL.", 'abstract_zh': '协作精简扩散模型（CDDM）：面向实时轻量级轨迹预测的新型方法', 'title_zh': '协作精简扩散模型（CDDM）用于加速和轻量级轨迹预测'}
{'arxiv_id': 'arXiv:2510.01174', 'title': 'Code2Video: A Code-centric Paradigm for Educational Video Generation', 'authors': 'Yanzhe Chen, Kevin Qinghong Lin, Mike Zheng Shou', 'link': 'https://arxiv.org/abs/2510.01174', 'abstract': 'While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at this https URL.', 'abstract_zh': '尽管近期生成模型在像素空间视频合成方面取得了进展，它们在生成专业教育视频方面的应用仍然受限，因为教育视频需要学科知识、精确的视觉结构和连贯的过渡，限制了其在教育场景中的应用。直观上， 이러한要求可以通过可渲染环境的操控来更好地解决，该环境可以通过逻辑命令（例如代码）明确控制。在本文中，我们提出了Code2Video，这是一种通过可执行Python代码生成教育视频的代码为中心的代理框架。该框架包括三个协作代理：（i）规划者，将讲座内容结构化为时间上连贯的流程，并准备相应的视觉资产；（ii）编程员，将结构化指令转换为可执行的Python代码，同时结合范围引导的自动修复以提高效率；以及（iii）评论员，利用带有视觉锚点提示的多模态视觉语言模型（VLM）来优化空间布局并确保清晰度。为了支持系统的评估，我们构建了MMMC基准，这是一个由专业制作、学科特定的教育视频组成的基准。我们从多个维度评估MMMC，包括使用VLM作为评判者的美学分数、代码效率，特别是TeachQuiz，这是一个新颖的端到端度量标准，量化了一个VLM在忘记之后通过观看生成的视频恢复知识的程度。我们的结果显示，Code2Video作为一种可扩展、可解释且可控的方法，其性能比直接代码生成提高了40%，并与人工制作的教程生成的视频相当。代码和数据集可在以下链接获取。', 'title_zh': '代码为中心的教育视频生成范式'}
{'arxiv_id': 'arXiv:2510.01173', 'title': 'EditTrack: Detecting and Attributing AI-assisted Image Editing', 'authors': 'Zhengyuan Jiang, Yuyang Zhang, Moyang Guo, Neil Zhenqiang Gong', 'link': 'https://arxiv.org/abs/2510.01173', 'abstract': 'In this work, we formulate and study the problem of image-editing detection and attribution: given a base image and a suspicious image, detection seeks to determine whether the suspicious image was derived from the base image using an AI editing model, while attribution further identifies the specific editing model responsible. Existing methods for detecting and attributing AI-generated images are insufficient for this problem, as they focus on determining whether an image was AI-generated/edited rather than whether it was edited from a particular base image. To bridge this gap, we propose EditTrack, the first framework for this image-editing detection and attribution problem. Building on four key observations about the editing process, EditTrack introduces a novel re-editing strategy and leverages carefully designed similarity metrics to determine whether a suspicious image originates from a base image and, if so, by which model. We evaluate EditTrack on five state-of-the-art editing models across six datasets, demonstrating that it consistently achieves accurate detection and attribution, significantly outperforming five baselines.', 'abstract_zh': '本研究提出了图像编辑检测与归因的问题建模与研究：给定一个基图像和一个可疑图像，检测旨在确定可疑图像是否是基于基图像使用AI编辑模型生成的，而归因进一步识别具体的编辑模型。现有的检测和归因AI生成图像的方法不足以解决这个问题，因为它们侧重于确定图像是否是AI生成或编辑，而非是否从特定基图像生成。为弥补这一差距，我们提出了EditTrack，这是第一个针对此图像编辑检测与归因问题的框架。基于编辑过程的四个关键观察，EditTrack引入了一种新颖的再编辑策略，并利用精心设计的相似性度量来确定可疑图像是否源自基图像，以及如果是，则是由哪个模型生成的。我们在六大数据集上对EditTrack进行了五种最先进的编辑模型的评估，展示了它在检测和归因方面的一致准确性，并且显著优于五个基线方法。', 'title_zh': 'EditTrack: 检测与归因AI辅助图像编辑'}
{'arxiv_id': 'arXiv:2510.01004', 'title': 'TextCAM: Explaining Class Activation Map with Text', 'authors': 'Qiming Zhao, Xingjian Li, Xiaoyu Cao, Xiaolong Wu, Min Xu', 'link': 'https://arxiv.org/abs/2510.01004', 'abstract': 'Deep neural networks (DNNs) have achieved remarkable success across domains but remain difficult to interpret, limiting their trustworthiness in high-stakes applications. This paper focuses on deep vision models, for which a dominant line of explainability methods are Class Activation Mapping (CAM) and its variants working by highlighting spatial regions that drive predictions. We figure out that CAM provides little semantic insight into what attributes underlie these activations. To address this limitation, we propose TextCAM, a novel explanation framework that enriches CAM with natural languages. TextCAM combines the precise spatial localization of CAM with the semantic alignment of vision-language models (VLMs). Specifically, we derive channel-level semantic representations using CLIP embeddings and linear discriminant analysis, and aggregate them with CAM weights to produce textual descriptions of salient visual evidence. This yields explanations that jointly specify where the model attends and what visual attributes likely support its decision. We further extend TextCAM to generate feature channels into semantically coherent groups, enabling more fine-grained visual-textual explanations. Experiments on ImageNet, CLEVR, and CUB demonstrate that TextCAM produces faithful and interpretable rationales that improve human understanding, detect spurious correlations, and preserve model fidelity.', 'abstract_zh': '深度神经网络（DNNs）在各个领域取得了显著成功，但仍然难以解释，限制了其在高风险应用中的可信度。本文专注于深度视觉模型，对于这类模型，主要的可解释性方法包括基于激活映射（Class Activation Mapping, CAM）及其变种方法，这些方法通过突出显示驱动预测的空间区域来工作。我们发现在这些激活中，CAM提供的语义洞察能力有限。为了解决这一局限性，我们提出了一种名为TextCAM的新颖解释框架，该框架将自然语言与CAM结合在一起。TextCAM结合了CAM精确的空间定位与视觉语言模型（VLMs）的语义对齐。具体来说，我们使用CLIP嵌入和线性判别分析从通道级别提取语义表示，并将其与CAM权重聚合以生成显著视觉证据的文字描述。这提供了既能说明模型关注的位置又能说明支持其决策的视觉属性的解释。我们进一步将TextCAM扩展为生成语义连贯的特征通道组，从而实现更精细的视觉-文本解释。在ImageNet、CLEVR和CUB上的实验表明，TextCAM生成了忠于事实且可解释的合理性描述，有助于提高人类理解、检测伪相关，并保持模型的准确性。', 'title_zh': 'TextCAM：基于文本的类激活图解释'}
{'arxiv_id': 'arXiv:2510.00862', 'title': 'Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model', 'authors': 'Hyun-kyu Ko, Youbin Kim, Jihyeon Park, Dongheok Park, Gyeongjin Kang, Wonjun Cho, Hyung Yi, Eunbyung Park', 'link': 'https://arxiv.org/abs/2510.00862', 'abstract': 'State Space Models (SSMs)-most notably RNNs-have historically played a central role in sequential modeling. Although attention mechanisms such as Transformers have since dominated due to their ability to model global context, their quadratic complexity and limited scalability make them less suited for long sequences. Video super-resolution (VSR) methods have traditionally relied on recurrent architectures to propagate features across frames. However, such approaches suffer from well-known issues including vanishing gradients, lack of parallelism, and slow inference speed. Recent advances in selective SSMs like Mamba offer a compelling alternative: by enabling input-dependent state transitions with linear-time complexity, Mamba mitigates these issues while maintaining strong long-range modeling capabilities. Despite this potential, Mamba alone struggles to capture fine-grained spatial dependencies due to its causal nature and lack of explicit context aggregation. To address this, we propose a hybrid architecture that combines shifted window self-attention for spatial context aggregation with Mamba-based selective scanning for efficient temporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an alignment-aware mechanism that warps features toward a center anchor frame within the temporal window before Mamba propagation and scatters them back afterward, effectively reducing occlusion artifacts and ensuring effective redistribution of aggregated information across all frames. The official implementation is provided at: this https URL.', 'abstract_zh': '基于状态空间模型（SSMs）如RNNs的方法在序列建模中历来扮演着中心角色。尽管Transformer等注意力机制因其能够建模全局上下文而占据主导地位，但由于其二次复杂性和有限的可扩展性，它们并不适合长序列。视频超分辨率（VSR）方法传统上依赖递归架构在帧间传播特征。然而，此类方法面临着vanishing gradients、缺乏并行性和慢速推理速度等已知问题。近来，基于选择性SSMs的方法如Mamba提出了具有吸引力的替代方案：通过使用线性时间复杂度实现输入依赖的状态转换，Mamba缓解了这些问题同时保持了强大的长距离建模能力。尽管如此，Mamba单独使用时难以捕捉细粒度的空间依赖性，这主要是由于其因果性质和显式上下文聚合的缺失。为此，我们提出了一种混合架构，结合了移窗自注意力机制进行空间上下文聚合，并利用基于Mamba的选择性扫描进行高效的时序传播。此外，我们引入了一种对齐感知机制Gather-Scatter Mamba（GSM），该机制在Mamba传播前将特征向时间窗内的中心锚帧进行扭曲，并在传播后将它们散射回原位，从而有效减少了遮挡伪影并确保所有帧间聚集信息的有效再分配。官方实现代码可在以下链接获取：this https URL。', 'title_zh': 'Gather-Scatter Mamba: 加速状态空间模型下的传播计算'}
{'arxiv_id': 'arXiv:2510.00837', 'title': 'Feature Identification for Hierarchical Contrastive Learning', 'authors': 'Julius Ott, Nastassia Vysotskaya, Huawei Sun, Lorenzo Servadei, Robert Wille', 'link': 'https://arxiv.org/abs/2510.00837', 'abstract': 'Hierarchical classification is a crucial task in many applications, where objects are organized into multiple levels of categories. However, conventional classification approaches often neglect inherent inter-class relationships at different hierarchy levels, thus missing important supervisory signals. Thus, we propose two novel hierarchical contrastive learning (HMLC) methods. The first, leverages a Gaussian Mixture Model (G-HMLC) and the second uses an attention mechanism to capture hierarchy-specific features (A-HMLC), imitating human processing. Our approach explicitly models inter-class relationships and imbalanced class distribution at higher hierarchy levels, enabling fine-grained clustering across all hierarchy levels. On the competitive CIFAR100 and ModelNet40 datasets, our method achieves state-of-the-art performance in linear evaluation, outperforming existing hierarchical contrastive learning methods by 2 percentage points in terms of accuracy. The effectiveness of our approach is backed by both quantitative and qualitative results, highlighting its potential for applications in computer vision and beyond.', 'abstract_zh': '层次分类是许多应用中的关键任务，其中对象被组织成多个层次的类别。然而，传统的分类方法往往忽略了不同层次间的固有类间关系，从而错过了重要的监督信号。因此，我们提出了两种新颖的层次对比学习（HMLC）方法。第一种方法利用高斯混合模型（G-HMLC），第二种方法使用注意力机制捕获层次特定特征（A-HMLC），模拟人类处理方式。我们的方法明确建模了高层次类别间的相互关系和类别不平衡分布，从而在所有层次上实现了精细聚类。在竞争性的CIFAR100和ModelNet40数据集上，我们的方法在线性评估中取得了最先进的性能，在准确率方面比现有层次对比学习方法高出2个百分点。我们的方法的有效性由定量和定性结果支持，突显了其在计算机视觉等领域应用的潜力。', 'title_zh': '层次对比学习中的特征识别'}
{'arxiv_id': 'arXiv:2510.00797', 'title': 'Solar PV Installation Potential Assessment on Building Facades Based on Vision and Language Foundation Models', 'authors': 'Ruyu Liu, Dongxu Zhuang, Jianhua Zhang, Arega Getaneh Abate, Per Sieverts Nielsen, Ben Wang, Xiufeng Liu', 'link': 'https://arxiv.org/abs/2510.00797', 'abstract': "Building facades represent a significant untapped resource for solar energy generation in dense urban environments, yet assessing their photovoltaic (PV) potential remains challenging due to complex geometries and semantic com ponents. This study introduces SF-SPA (Semantic Facade Solar-PV Assessment), an automated framework that transforms street-view photographs into quantitative PV deployment assessments. The approach combines com puter vision and artificial intelligence techniques to address three key challenges: perspective distortion correction, semantic understanding of facade elements, and spatial reasoning for PV layout optimization. Our four-stage pipeline processes images through geometric rectification, zero-shot semantic segmentation, Large Language Model (LLM) guided spatial reasoning, and energy simulation. Validation across 80 buildings in four countries demonstrates ro bust performance with mean area estimation errors of 6.2% &#177; 2.8% compared to expert annotations. The auto mated assessment requires approximately 100 seconds per building, a substantial gain in efficiency over manual methods. Simulated energy yield predictions confirm the method's reliability and applicability for regional poten tial studies, urban energy planning, and building-integrated photovoltaic (BIPV) deployment. Code is available at: https:github.com/CodeAXu/Solar-PV-Installation", 'abstract_zh': '基于语义的外墙光伏评估：SF-SPA框架研究', 'title_zh': '基于视觉与语言基础模型的建筑外墙太阳能光伏安装潜力评估'}
{'arxiv_id': 'arXiv:2510.00773', 'title': 'Uncertainty-Aware Concept Bottleneck Models with Enhanced Interpretability', 'authors': 'Haifei Zhang, Patrick Barry, Eduardo Brandao', 'link': 'https://arxiv.org/abs/2510.00773', 'abstract': 'In the context of image classification, Concept Bottleneck Models (CBMs) first embed images into a set of human-understandable concepts, followed by an intrinsically interpretable classifier that predicts labels based on these intermediate representations. While CBMs offer a semantically meaningful and interpretable classification pipeline, they often sacrifice predictive performance compared to end-to-end convolutional neural networks. Moreover, the propagation of uncertainty from concept predictions to final label decisions remains underexplored. In this paper, we propose a novel uncertainty-aware and interpretable classifier for the second stage of CBMs. Our method learns a set of binary class-level concept prototypes and uses the distances between predicted concept vectors and each class prototype as both a classification score and a measure of uncertainty. These prototypes also serve as interpretable classification rules, indicating which concepts should be present in an image to justify a specific class prediction. The proposed framework enhances both interpretability and robustness by enabling conformal prediction for uncertain or outlier inputs based on their deviation from the learned binary class-level concept prototypes.', 'abstract_zh': '基于图像分类的不确定性感知和可解释分类器研究', 'title_zh': '具有增强可解释性的不确定性意识概念瓶颈模型'}
{'arxiv_id': 'arXiv:2510.00728', 'title': 'Extreme Blind Image Restoration via Prompt-Conditioned Information Bottleneck', 'authors': 'Hongeun Kim, Bryan Sangwoo Kim, Jong Chul Ye', 'link': 'https://arxiv.org/abs/2510.00728', 'abstract': 'Blind Image Restoration (BIR) methods have achieved remarkable success but falter when faced with Extreme Blind Image Restoration (EBIR), where inputs suffer from severe, compounded degradations beyond their training scope. Directly learning a mapping from extremely low-quality (ELQ) to high-quality (HQ) images is challenging due to the massive domain gap, often leading to unnatural artifacts and loss of detail. To address this, we propose a novel framework that decomposes the intractable ELQ-to-HQ restoration process. We first learn a projector that maps an ELQ image onto an intermediate, less-degraded LQ manifold. This intermediate image is then restored to HQ using a frozen, off-the-shelf BIR model. Our approach is grounded in information theory; we provide a novel perspective of image restoration as an Information Bottleneck problem and derive a theoretically-driven objective to train our projector. This loss function effectively stabilizes training by balancing a low-quality reconstruction term with a high-quality prior-matching term. Our framework enables Look Forward Once (LFO) for inference-time prompt refinement, and supports plug-and-play strengthening of existing image restoration models without need for finetuning. Extensive experiments under severe degradation regimes provide a thorough analysis of the effectiveness of our work.', 'abstract_zh': '盲图像恢复方法在应对极端盲图像恢复任务时表现出色但在输入遭受超出现有训练范围的严重复合退化时会遇到困难。直接学习从极度低质量到高质量图像的映射因领域差距巨大而极具挑战性，常导致不自然的伪影和细节丢失。为解决这一问题，我们提出了一种新颖的框架，将难以处理的极度低质量到高质量的恢复过程分解为两个步骤。首先，学习一个投影器将极度低质量图像映射到一个中间的、退化较轻的低质量流形。然后，使用一个冻结的现成盲图像恢复模型恢复该中间图像为高质量图像。我们的方法基于信息论；我们提供了一种新的视角将图像恢复问题视为信息瓶颈问题，并推导出一个理论上驱动的目标来训练我们的投影器。该损失函数通过平衡低质量重建项和高质量先验匹配项来有效稳定训练。我们的框架允许在推理时进行一次前瞻性的提示细化，并支持无需微调即可增强现有图像恢复模型。在严重的退化条件下进行的广泛实验提供了我们工作的有效性分析。', 'title_zh': '极值盲图像恢复 via 提示条件信息瓶颈'}
{'arxiv_id': 'arXiv:2510.00585', 'title': 'U-DFA: A Unified DINOv2-Unet with Dual Fusion Attention for Multi-Dataset Medical Segmentation', 'authors': 'Zulkaif Sajjad, Furqan Shaukat, Junaid Mir', 'link': 'https://arxiv.org/abs/2510.00585', 'abstract': 'Accurate medical image segmentation plays a crucial role in overall diagnosis and is one of the most essential tasks in the diagnostic pipeline. CNN-based models, despite their extensive use, suffer from a local receptive field and fail to capture the global context. A common approach that combines CNNs with transformers attempts to bridge this gap but fails to effectively fuse the local and global features. With the recent emergence of VLMs and foundation models, they have been adapted for downstream medical imaging tasks; however, they suffer from an inherent domain gap and high computational cost. To this end, we propose U-DFA, a unified DINOv2-Unet encoder-decoder architecture that integrates a novel Local-Global Fusion Adapter (LGFA) to enhance segmentation performance. LGFA modules inject spatial features from a CNN-based Spatial Pattern Adapter (SPA) module into frozen DINOv2 blocks at multiple stages, enabling effective fusion of high-level semantic and spatial features. Our method achieves state-of-the-art performance on the Synapse and ACDC datasets with only 33\\% of the trainable model parameters. These results demonstrate that U-DFA is a robust and scalable framework for medical image segmentation across multiple modalities.', 'abstract_zh': '准确的医学图像分割在整体诊断中发挥着关键作用，是诊断流程中最重要的任务之一。尽管基于CNN的模型被广泛应用，但由于局部感受野的限制，它们难以捕捉全局上下文。结合CNN与 transformer 的常用方法试图弥合这一差距，但未能有效地融合局部和全局特征。随着VLMs和基础模型的出现，它们已被适应于下游医学成像任务，然而仍存在固有的领域差距和高计算成本的问题。为了解决这些问题，我们提出了一种统一的DINOv2-Unet编码-解码架构U-DFA，该架构结合了一个新型的局部-全局融合适配器（LGFA）模块以提高分割性能。LGFA模块将基于CNN的空间模式适配器（SPA）模块的空间特征注入冻结的DINOv2块中，多阶段实现高级语义和空间特征的有效融合。我们的方法在Synapse和ACDC数据集上实现了最先进的性能，仅需33%的可训练模型参数。这些结果表明，U-DFA是一种稳健且可扩展的跨多种模态医学图像分割框架。', 'title_zh': 'U-DFA: 结合双融合注意力机制的统一DINOv2-Unet多数据集医学分割'}
{'arxiv_id': 'arXiv:2510.00547', 'title': 'Forestpest-YOLO: A High-Performance Detection Framework for Small Forestry Pests', 'authors': 'Aoduo Li, Peikai Lin, Jiancheng Li, Zhen Zhang, Shiting Wu, Zexiao Liang, Zhifa Jiang', 'link': 'https://arxiv.org/abs/2510.00547', 'abstract': 'Detecting agricultural pests in complex forestry environments using remote sensing imagery is fundamental for ecological preservation, yet it is severely hampered by practical challenges. Targets are often minuscule, heavily occluded, and visually similar to the cluttered background, causing conventional object detection models to falter due to the loss of fine-grained features and an inability to handle extreme data imbalance. To overcome these obstacles, this paper introduces Forestpest-YOLO, a detection framework meticulously optimized for the nuances of forestry remote sensing. Building upon the YOLOv8 architecture, our framework introduces a synergistic trio of innovations. We first integrate a lossless downsampling module, SPD-Conv, to ensure that critical high-resolution details of small targets are preserved throughout the network. This is complemented by a novel cross-stage feature fusion block, CSPOK, which dynamically enhances multi-scale feature representation while suppressing background noise. Finally, we employ VarifocalLoss to refine the training objective, compelling the model to focus on high-quality and hard-to-classify samples. Extensive experiments on our challenging, self-constructed ForestPest dataset demonstrate that Forestpest-YOLO achieves state-of-the-art performance, showing marked improvements in detecting small, occluded pests and significantly outperforming established baseline models.', 'abstract_zh': '利用遥感影像检测复杂森林环境中农业害虫是生态保护的基础，但受到实际挑战的严重影响。目标往往很小、严重遮挡且与杂乱背景视觉相似，导致传统目标检测模型因细粒度特征丢失和极端数据不平衡难以应对。为克服这些困难，本文提出Forestpest-YOLO，一个专为森林遥感检测精细特性优化的检测框架。基于YOLOv8架构，该框架引入了三项协同创新。首先，整合无损下采样模块SPD-Conv，确保网络中关键的小目标的高分辨率细节得以保留。其次，采用一种新颖的跨阶段特征融合模块CSPOK，动态增强多尺度特征表示并抑制背景噪声。最后，采用VarifocalLoss精炼训练目标，促使模型关注高质量和难以分类的样本。在我们挑战性的自构建ForestPest数据集上的广泛实验表明，Forestpest-YOLO达到了最先进的性能，在检测小且被遮挡的害虫方面有显著改进，并远超现有基准模型。', 'title_zh': 'Forestpest-YOLO: 一种高性能的小型林业害虫检测框架'}
{'arxiv_id': 'arXiv:2510.00500', 'title': 'Relative-Absolute Fusion: Rethinking Feature Extraction in Image-Based Iterative Method Selection for Solving Sparse Linear Systems', 'authors': 'Kaiqi Zhang, Mingguan Yang, Dali Chang, Chun Chen, Yuxiang Zhang, Kexun He, Jing Zhao', 'link': 'https://arxiv.org/abs/2510.00500', 'abstract': 'Iterative method selection is crucial for solving sparse linear systems because these methods inherently lack robustness. Though image-based selection approaches have shown promise, their feature extraction techniques might encode distinct matrices into identical image representations, leading to the same selection and suboptimal method. In this paper, we introduce RAF (Relative-Absolute Fusion), an efficient feature extraction technique to enhance image-based selection approaches. By simultaneously extracting and fusing image representations as relative features with corresponding numerical values as absolute features, RAF achieves comprehensive matrix representations that prevent feature ambiguity across distinct matrices, thus improving selection accuracy and unlocking the potential of image-based selection approaches. We conducted comprehensive evaluations of RAF on SuiteSparse and our developed BMCMat (Balanced Multi-Classification Matrix dataset), demonstrating solution time reductions of 0.08s-0.29s for sparse linear systems, which is 5.86%-11.50% faster than conventional image-based selection approaches and achieves state-of-the-art (SOTA) performance. BMCMat is available at this https URL.', 'abstract_zh': '迭代方法选择对于求解稀疏线性系统至关重要，因为这些方法本身缺乏稳定性。尽管基于图像的选择方法显示出潜力，但其特征提取技术可能会将不同的矩阵编码为相同图像表示，导致相同的选型和次优方法。本文介绍了一种高效的特征提取技术RAF（相对-绝对融合），以增强基于图像的选择方法。通过同时提取并融合图像表示作为相对特征，以及与相应的数值值作为绝对特征，RAF实现了全面的矩阵表示，防止了不同矩阵间的特征模糊性，从而提高选择准确性并解锁基于图像选择方法的潜力。我们对RAF在SuiteSparse和我们开发的BMCMat（平衡多分类矩阵数据集）上的全面评估表明，对于稀疏线性系统，解的时间减少了0.08秒-0.29秒，比传统基于图像的选择方法快5.86%-11.50%，并达到了最先进的（SOTA）性能。BMCMat可在以下链接获取：this https URL。', 'title_zh': '基于图像的迭代方法选择求解稀疏线性系统中相对-绝对融合：重新思考特征提取'}
{'arxiv_id': 'arXiv:2510.00454', 'title': 'Measuring and Controlling the Spectral Bias for Self-Supervised Image Denoising', 'authors': 'Wang Zhang, Huaqiu Li, Xiaowan Hu, Tao Jiang, Zikang Chen, Haoqian Wang', 'link': 'https://arxiv.org/abs/2510.00454', 'abstract': 'Current self-supervised denoising methods for paired noisy images typically involve mapping one noisy image through the network to the other noisy image. However, after measuring the spectral bias of such methods using our proposed Image Pair Frequency-Band Similarity, it suffers from two practical limitations. Firstly, the high-frequency structural details in images are not preserved well enough. Secondly, during the process of fitting high frequencies, the network learns high-frequency noise from the mapped noisy images. To address these challenges, we introduce a Spectral Controlling network (SCNet) to optimize self-supervised denoising of paired noisy images. First, we propose a selection strategy to choose frequency band components for noisy images, to accelerate the convergence speed of training. Next, we present a parameter optimization method that restricts the learning ability of convolutional kernels to high-frequency noise using the Lipschitz constant, without changing the network structure. Finally, we introduce the Spectral Separation and low-rank Reconstruction module (SSR module), which separates noise and high-frequency details through frequency domain separation and low-rank space reconstruction, to retain the high-frequency structural details of images. Experiments performed on synthetic and real-world datasets verify the effectiveness of SCNet.', 'abstract_zh': '基于频带控制的配对 noisy 图像自监督去噪方法', 'title_zh': '测量和控制自监督图像去噪中的光谱偏差'}
{'arxiv_id': 'arXiv:2510.00416', 'title': 'Domain-Specialized Interactive Segmentation Framework for Meningioma Radiotherapy Planning', 'authors': 'Junhyeok Lee, Han Jang, Kyu Sung Choi', 'link': 'https://arxiv.org/abs/2510.00416', 'abstract': 'Precise delineation of meningiomas is crucial for effective radiotherapy (RT) planning, directly influencing treatment efficacy and preservation of adjacent healthy tissues. While automated deep learning approaches have demonstrated considerable potential, achieving consistently accurate clinical segmentation remains challenging due to tumor heterogeneity. Interactive Medical Image Segmentation (IMIS) addresses this challenge by integrating advanced AI techniques with clinical input. However, generic segmentation tools, despite widespread applicability, often lack the specificity required for clinically critical and disease-specific tasks like meningioma RT planning. To overcome these limitations, we introduce Interactive-MEN-RT, a dedicated IMIS tool specifically developed for clinician-assisted 3D meningioma segmentation in RT workflows. The system incorporates multiple clinically relevant interaction methods, including point annotations, bounding boxes, lasso tools, and scribbles, enhancing usability and clinical precision. In our evaluation involving 500 contrast-enhanced T1-weighted MRI scans from the BraTS 2025 Meningioma RT Segmentation Challenge, Interactive-MEN-RT demonstrated substantial improvement compared to other segmentation methods, achieving Dice similarity coefficients of up to 77.6\\% and Intersection over Union scores of 64.8\\%. These results emphasize the need for clinically tailored segmentation solutions in critical applications such as meningioma RT planning. The code is publicly available at: this https URL', 'abstract_zh': '精确界定脑膜瘤对于有效的放射治疗计划至关重要，直接关系到治疗效果和相邻健康组织的保护。尽管自动深度学习方法表现出显著潜力，但由于肿瘤异质性，实现一致准确的临床分割仍然具有挑战性。交互式医学图像分割（IMIS）通过整合先进的AI技术与临床输入来应对这一挑战。然而，通用分割工具虽具有广泛的适用性，但在如脑膜瘤放射治疗计划等临床关键和疾病特异性任务中往往缺乏所需的特异性。为克服这些限制，我们提出了一种专门用于脑膜瘤放射治疗工作流程中临床辅助3D分割的Interactive-MEN-RT交互式分割工具。该系统采用多种临床相关交互方法，包括点标注、边界框、套索工具和涂抹工具，增强了易用性和临床精度。在对2025 BraTS脑膜瘤放射治疗分割挑战中500张对比增强T1加权MRI扫描进行评价后，Interactive-MEN-RT相比其他分割方法显著提高，Dice相似系数达到77.6%，交并比达到64.8%。这些结果强调了在如脑膜瘤放射治疗计划等关键应用中需要临床定制的分割解决方案。代码公开链接：this https URL', 'title_zh': '用于脑膜瘤放射治疗计划的领域特化交互分割框架'}
{'arxiv_id': 'arXiv:2510.00411', 'title': 'David and Goliath in Medical Vision: Convolutional Networks vs Biomedical Vision Language Models', 'authors': 'Ran Tong, Jiaqi Liu, Su Liu, Jiexi Xu, Lanruo Wang, Tong Wang', 'link': 'https://arxiv.org/abs/2510.00411', 'abstract': "The accurate interpretation of chest radiographs using automated methods is a critical task in medical imaging. This paper presents a comparative analysis between a supervised lightweight Convolutional Neural Network (CNN) and a state-of-the-art, zero-shot medical Vision-Language Model (VLM), BiomedCLIP, across two distinct diagnostic tasks: pneumonia detection on the PneumoniaMNIST benchmark and tuberculosis detection on the Shenzhen TB dataset. Our experiments show that supervised CNNs serve as highly competitive baselines in both cases. While the default zero-shot performance of the VLM is lower, we demonstrate that its potential can be unlocked via a simple yet crucial remedy: decision threshold calibration. By optimizing the classification threshold on a validation set, the performance of BiomedCLIP is significantly boosted across both datasets. For pneumonia detection, calibration enables the zero-shot VLM to achieve a superior F1-score of 0.8841, surpassing the supervised CNN's 0.8803. For tuberculosis detection, calibration dramatically improves the F1-score from 0.4812 to 0.7684, bringing it close to the supervised baseline's 0.7834. This work highlights a key insight: proper calibration is essential for leveraging the full diagnostic power of zero-shot VLMs, enabling them to match or even outperform efficient, task-specific supervised models.", 'abstract_zh': '使用自动方法准确解读胸部X光片是医学成像中的关键任务。本文在肺炎检测（基于PneumoniaMNIST基准）和肺结核检测（基于深圳肺结核数据集）两个不同的诊断任务中，比较分析了监督的轻量级卷积神经网络（CNN）和最先进的零样本医学视觉-语言模型（VLM）BiomedCLIP的表现。实验结果显示，监督的CNN在两个任务中均作为强有力的基线模型。虽然VLM的零样本默认性能较低，但我们证明通过一个简单而关键的改进——决策阈值校准，可以充分释放其潜力。通过在验证集上优化分类阈值，BiomedCLIP在两个数据集上的性能显著提升。对于肺炎检测，校准使其零样本VLM的F1分数达到0.8841，超过监督CNN的0.8803。对于肺结核检测，校准将F1分数提升至0.7684，接近监督基线的0.7834。本文强调了一个重要见解：适当的校准对于充分发挥零样本VLM的诊断潜力至关重要，使其能够匹配甚至超越高效的任务特定监督模型。', 'title_zh': '医疗视觉中的大卫与歌利亚：卷积网络 vs 生物医学视觉语言模型'}
{'arxiv_id': 'arXiv:2510.00072', 'title': 'Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning', 'authors': 'Chenhui Xu, Fuxun Yu, Michael J. Bianco, Jacob Kovarskiy, Raphael Tang, Qi Zhang, Zirui Xu, Will LeVine, Brandon Dubbs, Heming Liao, Cassandra Burgess, Suvam Bag, Jay Patravali, Rupanjali Kukal, Mikael Figueroa, Rishi Madhok, Nikolaos Karianakis, Jinjun Xiong', 'link': 'https://arxiv.org/abs/2510.00072', 'abstract': 'We introduce Geo-R1, a reasoning-centric post-training framework that unlocks geospatial reasoning in vision-language models by combining thinking scaffolding and elevating. In the scaffolding stage, Geo-R1 instills a ``geospatial thinking paradigm" via supervised fine-tuning on synthetic chain-of-thought exemplars, enabling models to connect visual cues with geographic priors without costly human reasoning annotations. In the elevating stage, it uses GRPO-based reinforcement learning on a weakly-supervised cross-view pairing proxy. This design supplies a verifiable and scalable reward signal: teaching models to capture and reconcile features across modalities, and harnessing reasoning for accurate prediction. Geo-R1 extends geospatial modeling from domain pretraining / supervised finetuning to reasoning-first post-training, and achieves state-of-the-art performance across various geospatial reasoning benchmarks. Our model is available at this https URL.', 'abstract_zh': 'Geo-R1：一种以推理为中心的后训练框架，通过结合支撑和提升解锁视觉语言模型的空间推理能力', 'title_zh': 'Geo-R1: 通过跨视图强化学习解锁VLM的地理空间推理能力'}
{'arxiv_id': 'arXiv:2510.00059', 'title': 'FSDENet: A Frequency and Spatial Domains based Detail Enhancement Network for Remote Sensing Semantic Segmentation', 'authors': 'Jiahao Fu, Yinfeng Yu, Liejun Wang', 'link': 'https://arxiv.org/abs/2510.00059', 'abstract': "To fully leverage spatial information for remote sensing image segmentation and address semantic edge ambiguities caused by grayscale variations (e.g., shadows and low-contrast regions), we propose the Frequency and Spatial Domains based Detail Enhancement Network (FSDENet). Our framework employs spatial processing methods to extract rich multi-scale spatial features and fine-grained semantic details. By effectively integrating global and frequency-domain information through the Fast Fourier Transform (FFT) in global mappings, the model's capability to discern global representations under grayscale variations is significantly strengthened. Additionally, we utilize Haar wavelet transform to decompose features into high- and low-frequency components, leveraging their distinct sensitivity to edge information to refine boundary segmentation. The model achieves dual-domain synergy by integrating spatial granularity with frequency-domain edge sensitivity, substantially improving segmentation accuracy in boundary regions and grayscale transition zones. Comprehensive experimental results demonstrate that FSDENet achieves state-of-the-art (SOTA) performance on four widely adopted datasets: LoveDA, Vaihingen, Potsdam, and iSAID.", 'abstract_zh': '基于频域和空域细节增强网络（FSDENet）：充分利用空间信息并通过灰度变化（例如阴影和低对比度区域）引起的语义边缘模糊性进行遥感图像分割', 'title_zh': '基于频域与空域的细节增强网络：遥感语义分割中的应用'}
{'arxiv_id': 'arXiv:2510.00049', 'title': 'AI-Based Stroke Rehabilitation Domiciliary Assessment System with ST_GCN Attention', 'authors': 'Suhyeon Lim, Ye-eun Kim, Andrew J. Choi', 'link': 'https://arxiv.org/abs/2510.00049', 'abstract': 'Effective stroke recovery requires continuous rehabilitation integrated with daily living. To support this need, we propose a home-based rehabilitation exercise and feedback system. The system consists of (1) hardware setup with RGB-D camera and wearable sensors to capture Stroke movements, (2) a mobile application for exercise guidance, and (3) an AI server for assessment and feedback. When Stroke user exercises following the application guidance, the system records skeleton sequences, which are then Assessed by the deep learning model, RAST-G@. The model employs a spatio-temporal graph convolutional network (ST-GCN) to extract skeletal features and integrates transformer-based temporal attention to figure out action quality. For system implementation, we constructed the NRC dataset, include 10 upper-limb activities of daily living (ADL) and 5 range-of-motion (ROM) collected from stroke and non-disabled participants, with Score annotations provided by licensed physiotherapists. Results on the KIMORE and NRC datasets show that RAST-G@ improves over baseline in terms of MAD, RMSE, and MAPE. Furthermore, the system provides user feedback that combines patient-centered assessment and monitoring. The results demonstrate that the proposed system offers a scalable approach for quantitative and consistent domiciliary rehabilitation assessment.', 'abstract_zh': '有效的中风恢复需要结合日常生活的持续康复。为此，我们提出了一种基于家庭的康复锻炼和反馈系统。该系统包括（1）使用RGB-D相机和可穿戴传感器的硬件设置以捕捉中风患者的动作，（2）用于指导锻炼的移动应用，以及（3）用于评估和反馈的AI服务器。当中风患者按照应用程序指导进行锻炼时，系统记录骨架序列，这些序列随后由深度学习模型RAST-G@评估。该模型使用空间-时间图卷积网络（ST-GCN）提取骨骼特征，并结合基于 Transformer 的时间注意力机制来确定动作质量。在系统实现方面，我们构建了NRC数据集，包括来自中风和非残疾参与者、由认证理疗师提供评分注释的10种日常生活中上肢活动和5种关节活动范围（ROM）。在KIMORE和NRC数据集上的结果显示，RAST-G@在MAD、RMSE和MAPE方面优于基线。此外，该系统提供了结合患者中心评估和监测的用户反馈。结果表明，所提出的系统为定量和一致的家庭康复评估提供了一种可扩展的方法。', 'title_zh': '基于AI的ST_GCN注意力机制家用卒中康复评估系统'}
{'arxiv_id': 'arXiv:2510.00047', 'title': 'Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations', 'authors': 'Sihao Ding, Santosh Vasa, Aditi Ramadwar', 'link': 'https://arxiv.org/abs/2510.00047', 'abstract': "Vision-Language Models (VLMs) often produce fluent Natural Language Explanations (NLEs) that sound convincing but may not reflect the causal factors driving predictions. This mismatch of plausibility and faithfulness poses technical and governance risks. We introduce Explanation-Driven Counterfactual Testing (EDCT), a fully automated verification procedure for a target VLM that treats the model's own explanation as a falsifiable hypothesis. Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2) parses the NLE into testable visual concepts, (3) generates targeted counterfactual edits via generative inpainting, and (4) computes a Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes in both answers and explanations. Across 120 curated OK-VQA examples and multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides regulator-aligned audit artifacts indicating when cited concepts fail causal tests.", 'abstract_zh': 'Vision-Language模型（VLMs）生成的自然语言解释（NLEs）通常流畅且听起来令人信服，但可能并未反映预测背后的因果因素。这种可信度和忠实度之间的不匹配带来了技术风险和治理风险。我们引入了基于解释的反事实测试（EDCT），这是一种全自动验证程序，将模型自身解释视为可证伪的假说。给定图像-问题对，EDCT：（1）获取模型的答案和NLE，（2）将NLE解析为可测试的视觉概念，（3）通过生成填充生成针对性的反事实编辑，（4）使用LLM辅助分析答案和解释的变化来计算反事实一致性分数（CCS）。在120个精心策划的OK-VQA示例和多种VLMs上，EDCT发现了显著的忠实度差距，并提供了符合监管要求的审计证据，表明引用的概念在因果测试中失败。', 'title_zh': '基于解释驱动的反事实测试以确保视觉-语言模型解释的忠实性'}
{'arxiv_id': 'arXiv:2510.00035', 'title': 'Deep Learning-Based Pneumonia Detection from Chest X-ray Images: A CNN Approach with Performance Analysis and Clinical Implications', 'authors': 'P K Dutta, Anushri Chowdhury, Anouska Bhattacharyya, Shakya Chakraborty, Sujatra Dey', 'link': 'https://arxiv.org/abs/2510.00035', 'abstract': 'Deep learning integration into medical imaging systems has transformed disease detection and diagnosis processes with a focus on pneumonia identification. The study introduces an intricate deep learning system using Convolutional Neural Networks for automated pneumonia detection from chest Xray images which boosts diagnostic precision and speed. The proposed CNN architecture integrates sophisticated methods including separable convolutions along with batch normalization and dropout regularization to enhance feature extraction while reducing overfitting. Through the application of data augmentation techniques and adaptive learning rate strategies the model underwent training on an extensive collection of chest Xray images to enhance its generalization capabilities. A convoluted array of evaluation metrics such as accuracy, precision, recall, and F1 score collectively verify the model exceptional performance by recording an accuracy rate of 91. This study tackles critical clinical implementation obstacles such as data privacy protection, model interpretability, and integration with current healthcare systems beyond just model performance. This approach introduces a critical advancement by integrating medical ontologies with semantic technology to improve diagnostic accuracy. The study enhances AI diagnostic reliability by integrating machine learning outputs with structured medical knowledge frameworks to boost interpretability. The findings demonstrate AI powered healthcare tools as a scalable efficient pneumonia detection solution. This study advances AI integration into clinical settings by developing more precise automated diagnostic methods that deliver consistent medical imaging results.', 'abstract_zh': '深度学习在医学影像系统中的集成改变了肺炎检测和诊断流程：一种基于卷积神经网络的自动化肺炎检测系统及其应用', 'title_zh': '基于深度学习的胸部X光图像肺炎检测：一种CNN方法及其性能分析和临床意义'}
{'arxiv_id': 'arXiv:2509.26007', 'title': 'MARS: Audio Generation via Multi-Channel Autoregression on Spectrograms', 'authors': 'Eleonora Ristori, Luca Bindini, Paolo Frasconi', 'link': 'https://arxiv.org/abs/2509.26007', 'abstract': 'Research on audio generation has progressively shifted from waveform-based approaches to spectrogram-based methods, which more naturally capture harmonic and temporal structures. At the same time, advances in image synthesis have shown that autoregression across scales, rather than tokens, improves coherence and detail. Building on these ideas, we introduce MARS (Multi-channel AutoRegression on Spectrograms), a framework that treats spectrograms as multi-channel images and employs channel multiplexing (CMX), a reshaping technique that lowers height and width without discarding information. A shared tokenizer provides consistent discrete representations across scales, enabling a transformer-based autoregressor to refine spectrograms from coarse to fine resolutions efficiently. Experiments on a large-scale dataset demonstrate that MARS performs comparably or better than state-of-the-art baselines across multiple evaluation metrics, establishing an efficient and scalable paradigm for high-fidelity audio generation.', 'abstract_zh': '基于光谱的多通道自回归音频生成研究', 'title_zh': 'MARS：基于多通道自回归谱图的音频生成'}
{'arxiv_id': 'arXiv:2509.25774', 'title': 'PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models', 'authors': 'Jeongjae Lee, Jong Chul Ye', 'link': 'https://arxiv.org/abs/2509.25774', 'abstract': 'While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO.', 'abstract_zh': '尽管强化学习提高了文本到图像（T2I）模型的一致性，但最先进的策略梯度方法仍然受到训练不稳定性和高方差的困扰，影响了收敛速度并损害了图像质量。我们的分析指出这一不稳定性的一个关键原因：生成采样器的数学结构导致了时序间不稳定的、不成比例的反馈。为解决这一问题，我们提出了比例信用策略优化（PCPO）框架，该框架通过稳定的目标重构和合理的时序重新加权来确保比例信用分配，从而稳定训练过程，显著加速收敛并提高图像质量。质量的提升直接来源于缓解了递归训练中常见的模型崩溃问题。在所有方面，包括最先进的DanceGRPO，PCPO都显著优于现有策略梯度基线。', 'title_zh': 'PCPO: 比例信用政策优化以对齐图像生成模型'}
{'arxiv_id': 'arXiv:2509.23769', 'title': 'ReLumix: Extending Image Relighting to Video via Video Diffusion Models', 'authors': 'Lezhong Wang, Shutong Jin, Ruiqi Cui, Anders Bjorholm Dahl, Jeppe Revall Frisvad, Siavash Bigdeli', 'link': 'https://arxiv.org/abs/2509.23769', 'abstract': "Controlling illumination during video post-production is a crucial yet elusive goal in computational photography. Existing methods often lack flexibility, restricting users to certain relighting models. This paper introduces ReLumix, a novel framework that decouples the relighting algorithm from temporal synthesis, thereby enabling any image relighting technique to be seamlessly applied to video. Our approach reformulates video relighting into a simple yet effective two-stage process: (1) an artist relights a single reference frame using any preferred image-based technique (e.g., Diffusion Models, physics-based renderers); and (2) a fine-tuned stable video diffusion (SVD) model seamlessly propagates this target illumination throughout the sequence. To ensure temporal coherence and prevent artifacts, we introduce a gated cross-attention mechanism for smooth feature blending and a temporal bootstrapping strategy that harnesses SVD's powerful motion priors. Although trained on synthetic data, ReLumix shows competitive generalization to real-world videos. The method demonstrates significant improvements in visual fidelity, offering a scalable and versatile solution for dynamic lighting control.", 'abstract_zh': '在视频后处理中控制照明是计算摄影中的一个关键但难以实现的目标。现有的方法往往缺乏灵活性，限制用户只能使用特定的光照重建模型。本文介绍了ReLumix，这是一个新颖的框架，它将光照重建算法与时空合成解耦，从而允许任何图像光照技术无缝应用于视频。我们的方法将视频光照重建重新定义为一个简单而有效的两阶段过程：（1）艺术家使用任何首选的基于图像的技术（例如，扩散模型、基于物理的渲染器）对单个参考帧进行光照重建；（2）一个微调过的稳定视频扩散（SVD）模型无缝地将这种目标照明传播到整个序列中。为了确保时空一致性并防止伪像，我们引入了一种门控交叉注意力机制以实现平滑特征混合，并利用SVD强大的运动先验引入了一种时空-bootstrap策略。尽管是在合成数据上训练的，ReLumix显示了在真实世界视频上的竞争性泛化能力。该方法在视觉保真度方面表现出显著的改进，提供了一种可扩展且多功能的动态照明控制解决方案。', 'title_zh': 'ReLumix: 通过视频扩散模型将图像光照更改扩展到视频'}
