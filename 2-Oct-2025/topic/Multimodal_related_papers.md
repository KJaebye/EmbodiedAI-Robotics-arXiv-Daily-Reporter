# MultiPhysio-HRC: Multimodal Physiological Signals Dataset for industrial Human-Robot Collaboration 

**Title (ZH)**: 多模态生理信号数据集：工业人机协作中的多物理量生理信号数据集 

**Authors**: Andrea Bussolan, Stefano Baraldo, Oliver Avram, Pablo Urcola, Luis Montesano, Luca Maria Gambardella, Anna Valente  

**Link**: [PDF](https://arxiv.org/pdf/2510.00703)  

**Abstract**: Human-robot collaboration (HRC) is a key focus of Industry 5.0, aiming to enhance worker productivity while ensuring well-being. The ability to perceive human psycho-physical states, such as stress and cognitive load, is crucial for adaptive and human-aware robotics. This paper introduces MultiPhysio-HRC, a multimodal dataset containing physiological, audio, and facial data collected during real-world HRC scenarios. The dataset includes electroencephalography (EEG), electrocardiography (ECG), electrodermal activity (EDA), respiration (RESP), electromyography (EMG), voice recordings, and facial action units. The dataset integrates controlled cognitive tasks, immersive virtual reality experiences, and industrial disassembly activities performed manually and with robotic assistance, to capture a holistic view of the participants' mental states. Rich ground truth annotations were obtained using validated psychological self-assessment questionnaires. Baseline models were evaluated for stress and cognitive load classification, demonstrating the dataset's potential for affective computing and human-aware robotics research. MultiPhysio-HRC is publicly available to support research in human-centered automation, workplace well-being, and intelligent robotic systems. 

**Abstract (ZH)**: Human-robot合作（HRC）是工业4.0的关键焦点，旨在提升员工 productivity同时确保 well-being。准确感知人类的心理-生理状态，如压力和认知负荷，是适应性和人性化机器人技术的关键。本文介绍了MultiPhysio-HRC，这是一个包含生理、音频和面部数据的多模态数据集，数据来源于实际的HRC场景。数据集包括脑电图（EEG）、心电图（ECG）、皮肤电活动（EDA）、呼吸（RESP）、肌电图（EMG）、语音录制和面部动作单元。该数据集结合了控制认知任务、沉浸式虚拟现实体验以及手动和机器人辅助的工业拆卸活动，以捕获参与者心理状态的全面视图。使用验证的心理自我评估问卷获得了丰富的地面真实标注。基准模型用于评估压力和认知负荷分类，展示了数据集在情感计算和人性化机器人技术研究中的潜力。MultiPhysio-HRC面向公众，支持以人为中心的自动化、工作场所well-being和智能机器人系统的研究。 

---
# Apriel-1.5-15b-Thinker 

**Title (ZH)**: April-1.5-15B-思辨者 

**Authors**: Shruthan Radhakrishna, Aman Tiwari, Aanjaneya Shukla, Masoud Hashemi, Rishabh Maheshwary, Shiva Krishna Reddy Malay, Jash Mehta, Pulkit Pattnaik, Saloni Mittal, Khalil Slimi, Kelechi Ogueji, Akintunde Oladipo, Soham Parikh, Oluwanifemi Bamgbose, Toby Liang, Ahmed Masry, Khyati Mahajan, Sai Rajeswar Mudumba, Vikas Yadav, Sathwik Tejaswi Madhusudhan, Torsten Scholak, Sagar Davasam, Srinivas Sunkara, Nicholas Chapados  

**Link**: [PDF](https://arxiv.org/pdf/2510.01141)  

**Abstract**: We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights multimodal reasoning model that achieves frontier-level performance through training design rather than sheer scale. Starting from Pixtral-12B, we apply a progressive three-stage methodology: (1) depth upscaling to expand reasoning capacity without pretraining from scratch, (2) staged continual pre-training that first develops foundational text and vision understanding, then enhances visual reasoning through targeted synthetic data generation addressing spatial structure, compositional understanding, and fine-grained perception, and (3) high-quality text-only supervised fine-tuning on curated instruction-response pairs with explicit reasoning traces spanning mathematics, coding, science, and tool use. Notably, our model achieves competitive results without reinforcement learning or preference optimization, isolating the contribution of our data-centric continual pre-training approach. On the Artificial Analysis Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching DeepSeek-R1-0528 despite requiring significantly fewer computational resources. Across ten image benchmarks, its performance is on average within five points of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model operating within single-GPU deployment constraints. Our results demonstrate that thoughtful mid-training 2 design can close substantial capability gaps without massive scale, making frontier-level multimodal reasoning accessible to organizations with limited infrastructure. We release the model checkpoint, all training recipes, and evaluation protocols under the MIT license to to advance open-source research. 

**Abstract (ZH)**: 我们呈现了Apriel-1.5-15B-Thinker，这是一个通过训练设计而非单纯规模来实现前沿性能的150亿参数开放权重多模态推理模型。从Pixtral-12B出发，我们应用了渐进的三阶段方法：(1) 深度扩展以扩大推理能力而无需从头预训练，(2) 阶段性持续预训练，首先发展基础的文字和视觉理解，然后通过针对空间结构、组合理解及细粒度感知的目标合成数据生成来增强视觉推理，以及(3) 在精心挑选的指令-响应对上进行高质量的仅文本监督微调，这些对跨越了数学、编程、科学和工具使用领域，并含有显式的推理痕迹。值得注意的是，我们的模型在没有应用强化学习或偏好优化的情况下达到了竞争力的结果，从而隔离了我们以数据为中心的持续预训练方法的贡献。Apriel-1.5-15B-Thinker在人工分析人工智能指数上的得分为52，尽管计算资源需求显著较少，仍与DeepSeek-R1-0528相当。在整个十项图像基准测试中，其平均性能与Gemini-2.5-Flash和Claude Sonnet-3.7相差不超过五个点，这是一个在单GPU部署限制下模型的重要成就。我们的结果表明，经过深思熟虑的中训练设计可以在无需大规模扩展的情况下填补重要的能力差距，使前沿水平的多模态推理对基础设施有限的组织来说变得可行。我们以MIT许可证发布模型检查点、所有训练食谱和评估协议，以促进开源研究。 

---
# FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge Graphs 

**Title (ZH)**: Few-Shot Relation Learning in Multimodal Knowledge Graphs 的融合适配器 

**Authors**: Ran Liu, Yuan Fang, Xiaoli Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.00894)  

**Abstract**: Multimodal Knowledge Graphs (MMKGs) incorporate various modalities, including text and images, to enhance entity and relation representations. Notably, different modalities for the same entity often present complementary and diverse information. However, existing MMKG methods primarily align modalities into a shared space, which tends to overlook the distinct contributions of specific modalities, limiting their performance particularly in low-resource settings. To address this challenge, we propose FusionAdapter for the learning of few-shot relationships (FSRL) in MMKG. FusionAdapter introduces (1) an adapter module that enables efficient adaptation of each modality to unseen relations and (2) a fusion strategy that integrates multimodal entity representations while preserving diverse modality-specific characteristics. By effectively adapting and fusing information from diverse modalities, FusionAdapter improves generalization to novel relations with minimal supervision. Extensive experiments on two benchmark MMKG datasets demonstrate that FusionAdapter achieves superior performance over state-of-the-art methods. 

**Abstract (ZH)**: 多模态知识图谱中的少量样本关系学习：FusionAdapter方法 

---
# ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning 

**Title (ZH)**: 自适应课程策略优化：面向复杂推理中视觉语言模型对齐 

**Authors**: Yunhao Wang, Ziting Li, Shuai Chen, Tao Liu, Chao Song, Junjie Jiang, Jian Zhu, Peng Gao, Bin Qin  

**Link**: [PDF](https://arxiv.org/pdf/2510.00690)  

**Abstract**: Aligning large-scale vision-language models (VLMs) for complex reasoning via reinforcement learning is often hampered by the limitations of existing policy optimization algorithms, such as static training schedules and the rigid, uniform clipping mechanism in Proximal Policy Optimization (PPO). In this work, we introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework that addresses these challenges through a dual-component adaptive learning strategy. First, ACPO employs a dynamic curriculum that orchestrates a principled transition from a stable, near on-policy exploration phase to an efficient, off-policy exploitation phase by progressively increasing sample reuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism that replaces the fixed clipping hyperparameter with dynamic, sample-wise bounds modulated by the normalized advantage of each token. This allows for more granular and robust policy updates, enabling larger gradients for high-potential samples while safeguarding against destructive ones. We conduct extensive experiments on a suite of challenging multimodal reasoning benchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate that ACPO consistently outperforms strong baselines such as DAPO and PAPO, achieving state-of-the-art performance, accelerated convergence, and superior training stability. 

**Abstract (ZH)**: 通过强化学习调整大规模视觉-语言模型进行复杂推理：Adaptive Curriculum Policy Optimization (ACPO) 通过双组件自适应学习策略解决现有挑战 

---
# VIRTUE: Visual-Interactive Text-Image Universal Embedder 

**Title (ZH)**: 视觉互动文本图像统一嵌入模型：VIRTUE 

**Authors**: Wei-Yao Wang, Kazuya Tateishi, Qiyu Wu, Shusuke Takahashi, Yuki Mitsufuji  

**Link**: [PDF](https://arxiv.org/pdf/2510.00523)  

**Abstract**: Multimodal representation learning models have demonstrated successful operation across complex tasks, and the integration of vision-language models (VLMs) has further enabled embedding models with instruction-following capabilities. However, existing embedding models lack visual-interactive capabilities to specify regions of interest from users (e.g., point, bounding box, mask), which have been explored in generative models to broaden their human-interactive applicability. Equipping embedding models with visual interactions not only would unlock new applications with localized grounding of user intent, which remains unexplored, but also enable the models to learn entity-level information within images to complement their global representations for conventional embedding tasks. In this paper, we propose a novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends the capabilities of the segmentation model and the vision-language model to the realm of representation learning. In VIRTUE, the segmentation model can process visual prompts that pinpoint specific regions within an image, thereby enabling the embedder to handle complex and ambiguous scenarios more precisely. To evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples that aims to retrieve the text caption by jointly considering the entity with a specific object and image scene. VIRTUE consistently achieves a state-of-the-art performance with significant improvements across 36 universal MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks. 

**Abstract (ZH)**: 具有视觉互动能力的文本-图像通用嵌入器（VIRTUE） 

---
# Multi-Objective Task-Aware Predictor for Image-Text Alignment 

**Title (ZH)**: 面向多目标任务的图像-文本对齐预测器 

**Authors**: Eunki Kim, Na Min An, James Thorne, Hyunjung Shim  

**Link**: [PDF](https://arxiv.org/pdf/2510.00766)  

**Abstract**: Evaluating image-text alignment while reflecting human preferences across multiple aspects is a significant issue for the development of reliable vision-language applications. It becomes especially crucial in real-world scenarios where multiple valid descriptions exist depending on contexts or user needs. However, research progress is hindered by the lack of comprehensive benchmarks and existing evaluation predictors lacking at least one of these key properties: (1) Alignment with human judgments, (2) Long-sequence processing, (3) Inference efficiency, and (4) Applicability to multi-objective scoring. To address these challenges, we propose a plug-and-play architecture to build a robust predictor, MULTI-TAP (Multi-Objective Task-Aware Predictor), capable of both multi and single-objective scoring. MULTI-TAP can produce a single overall score, utilizing a reward head built on top of a large vision-language model (LVLMs). We show that MULTI-TAP is robust in terms of application to different LVLM architectures, achieving significantly higher performance than existing metrics and even on par with the GPT-4o-based predictor, G-VEval, with a smaller size (7-8B). By training a lightweight ridge regression layer on the frozen hidden states of a pre-trained LVLM, MULTI-TAP can produce fine-grained scores for multiple human-interpretable objectives. MULTI-TAP performs better than VisionREWARD, a high-performing multi-objective reward model, in both performance and efficiency on multi-objective benchmarks and our newly released text-image-to-text dataset, EYE4ALL. Our new dataset, consisting of chosen/rejected human preferences (EYE4ALLPref) and human-annotated fine-grained scores across seven dimensions (EYE4ALLMulti), can serve as a foundation for developing more accessible AI systems by capturing the underlying preferences of users, including blind and low-vision (BLV) individuals. 

**Abstract (ZH)**: 评估多方面反映人类偏好的图像-文本对齐对于可靠视觉语言应用的发展是一项重要问题。在多种有效描述依赖于上下文或用户需求的现实场景中，这一问题变得尤为重要。然而，研究进展受限于缺乏全面基准和现有评价预测器至少缺乏一种关键属性：（1）与人类判断的一致性，（2）长序列处理能力，（3）推理效率，以及（4）多目标评分的适用性。为应对这些挑战，我们提出了一种插件式架构来构建一个稳健的预测器MULTI-TAP（多目标任务感知预测器），该预测器能够进行多目标和单目标评分。MULTI-TAP可以生成一个综合评分，利用大型视觉语言模型（LVLMs）顶部构建的奖励头实现。我们展示了MULTI-TAP在不同LVLM架构中的稳健性，其性能显著高于现有指标，甚至在规模较小（7-8B）的情况下与基于GPT-4o的预测器G-VEval相当。通过在预训练LVLM的冻结隐藏状态上训练一个轻量级岭回归层，MULTI-TAP能够为多个可解释的人类目标生成细粒度评分。在多目标基准和我们新发布的文本-图像到文本数据集EYE4ALL上，MULTI-TAP在性能和效率方面均优于高性能的多目标奖励模型VisionREWARD。我们的新数据集EYE4ALLPref和EYE4ALLMulti分别由选择/拒绝的人类偏好和七个维度上的细粒度人工注释评分组成，可以作为开发更易用的AI系统的基础，以捕捉用户的潜在偏好，包括盲人和低视力个体。 

---
# Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs 

**Title (ZH)**: 图评：通过知识图谱自动生成代理的多模态任务 

**Authors**: Yurun Chen, Xavier Hu, Yuhan Liu, Ziqi Wang, Zeyi Liao, Lin Chen, Feng Wei, Yuxi Qian, Bo Zheng, Keting Yin, Shengyu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2510.00507)  

**Abstract**: As multimodal LLM-driven agents continue to advance in autonomy and generalization, evaluation based on static datasets can no longer adequately assess their true capabilities in dynamic environments and diverse tasks. Existing LLM-based synthetic data methods are largely designed for LLM training and evaluation, and thus cannot be directly applied to agent tasks that require tool use and interactive capabilities. While recent studies have explored automatic agent task generation with LLMs, most efforts remain limited to text or image analysis, without systematically modeling multi-step interactions in web environments. To address these challenges, we propose Graph2Eval, a knowledge graph-based framework that automatically generates both multimodal document comprehension tasks and web interaction tasks, enabling comprehensive evaluation of agents' reasoning, collaboration, and interactive capabilities. In our approach, knowledge graphs constructed from multi-source external data serve as the task space, where we translate semantic relations into structured multimodal tasks using subgraph sampling, task templates, and meta-paths. A multi-stage filtering pipeline based on node reachability, LLM scoring, and similarity analysis is applied to guarantee the quality and executability of the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures reasoning, collaboration, and interaction capabilities. We instantiate the framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning document comprehension and web interaction scenarios. Experiments show that Graph2Eval efficiently generates tasks that differentiate agent and model performance, revealing gaps in reasoning, collaboration, and web interaction across different settings and offering a new perspective for agent evaluation. 

**Abstract (ZH)**: 基于图的知识图谱驱动多模态代理评估框架：Graph2Eval 

---
# PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation 

**Title (ZH)**: PodEval：播客音频生成的多模态评估框架 

**Authors**: Yujia Xiao, Liumeng Xue, Lei He, Xinyi Chen, Aemon Yat Fei Chiu, Wenjie Tian, Shaofei Zhang, Qiuqiang Kong, Xinfa Zhu, Wei Xue, Tan Lee  

**Link**: [PDF](https://arxiv.org/pdf/2510.00485)  

**Abstract**: Recently, an increasing number of multimodal (text and audio) benchmarks have emerged, primarily focusing on evaluating models' understanding capability. However, exploration into assessing generative capabilities remains limited, especially for open-ended long-form content generation. Significant challenges lie in no reference standard answer, no unified evaluation metrics and uncontrollable human judgments. In this work, we take podcast-like audio generation as a starting point and propose PodEval, a comprehensive and well-designed open-source evaluation framework. In this framework: 1) We construct a real-world podcast dataset spanning diverse topics, serving as a reference for human-level creative quality. 2) We introduce a multimodal evaluation strategy and decompose the complex task into three dimensions: text, speech and audio, with different evaluation emphasis on "Content" and "Format". 3) For each modality, we design corresponding evaluation methods, involving both objective metrics and subjective listening test. We leverage representative podcast generation systems (including open-source, close-source, and human-made) in our experiments. The results offer in-depth analysis and insights into podcast generation, demonstrating the effectiveness of PodEval in evaluating open-ended long-form audio. This project is open-source to facilitate public use: this https URL. 

**Abstract (ZH)**: 近期，涌现出越来越多的多模态（文本和音频）基准，主要侧重于评估模型的理解能力。然而，对生成能力的评估探索仍相对有限，尤其是对于开放式长格式内容生成。由于缺乏参考标准答案、统一的评估指标和可控的人工判断，存在重大挑战。本文以类似播客的音频生成作为起点，提出PodEval，一种全面且设计合理的开源评估框架。在该框架中：1) 构建了一个涵盖多样主题的现实播客数据集，作为人类水平创造力的参考。2) 引入了多模态评估策略，并将复杂任务分解为三个维度：文本、语音和音频，对“内容”和“格式”有不同的评估重点。3) 对每个模态，设计了相应的评估方法，包括客观指标和主观听力测试。我们在实验中使用了代表性的播客生成系统（包括开源、闭源和人为制作的系统）。实验结果提供了对播客生成的深入分析和见解，证明了PodEval在评估开放式长格式音频方面的有效性。该项目已开源以方便公共使用：this https URL。 

---
# Object-AVEdit: An Object-level Audio-Visual Editing Model 

**Title (ZH)**: 对象级音视频编辑模型 

**Authors**: Youquan Fu, Ruiyang Si, Hongfa Wang, Dongzhan Zhou, Jiacheng Sun, Ping Luo, Di Hu, Hongyuan Zhang, Xuelong Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.00050)  

**Abstract**: There is a high demand for audio-visual editing in video post-production and the film making field. While numerous models have explored audio and video editing, they struggle with object-level audio-visual operations. Specifically, object-level audio-visual editing requires the ability to perform object addition, replacement, and removal across both audio and visual modalities, while preserving the structural information of the source instances during the editing process. In this paper, we present \textbf{Object-AVEdit}, achieving the object-level audio-visual editing based on the inversion-regeneration paradigm. To achieve the object-level controllability during editing, we develop a word-to-sounding-object well-aligned audio generation model, bridging the gap in object-controllability between audio and current video generation models. Meanwhile, to achieve the better structural information preservation and object-level editing effect, we propose an inversion-regeneration holistically-optimized editing algorithm, ensuring both information retention during the inversion and better regeneration effect. Extensive experiments demonstrate that our editing model achieved advanced results in both audio-video object-level editing tasks with fine audio-visual semantic alignment. In addition, our developed audio generation model also achieved advanced performance. More results on our project page: this https URL. 

**Abstract (ZH)**: 高需求驱动的音频-视觉编辑在视频后期制作和电影制作领域中日益增长。尽管众多模型已经探索了音频和视频编辑，但在对象级音频-视觉操作方面仍存在问题。具体而言，对象级音频-视觉编辑需要在保持源实例结构信息的前提下，在音频和视觉模态上执行对象添加、替换和移除操作。本文中，我们提出\textbf{Object-AVEdit}，基于反转再生范式实现了对象级音频-视觉编辑。为了在编辑过程中实现对象级别的可控性，我们开发了一种词至声音对象高对齐的音频生成模型，缩小了音频和当前视频生成模型之间对象可控性的差距。同时，为了在保持更好结构信息和对象级编辑效果的前提下，我们提出了一种整体优化的编辑算法，确保反转过程中的信息保留和更好的再生效果。大量实验表明，我们的编辑模型在音频-视频对象级编辑任务中实现了高度精细的音频-视觉语义对齐。此外，我们开发的音频生成模型也取得了优异的性能。更多实验结果请参见项目页面：this https URL。 

---
# On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations 

**Title (ZH)**: 基于多模态扰动的视觉-语言-动作模型鲁棒性研究 

**Authors**: Jianing Guo, Zhenhong Wu, Chang Tu, Yiyao Ma, Xiangqi Kong, Zhiqian Liu, Jiaming Ji, Shuning Zhang, Yuanpei Chen, Kai Chen, Xianglong Liu, Qi Dou, Yaodong Yang, Huijie Zhao, Weifeng Lv, Simin Li  

**Link**: [PDF](https://arxiv.org/pdf/2510.00037)  

**Abstract**: In Vision-Language-Action (VLA) models, robustness to real-world perturbations is critical for deployment. Existing methods target simple visual disturbances, overlooking the broader multi-modal perturbations that arise in actions, instructions, environments, and observations. Here, we first evaluate the robustness of mainstream VLAs under 17 perturbations across four modalities. We find (1) actions as the most fragile modality, (2) Existing visual-robust VLA do not gain robustness in other modality, and (3) pi0 demonstrates superior robustness with a diffusion-based action head. To build multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA inputs and outputs. For output robustness, we perform offline robust optimization against worst-case action noise that maximizes mismatch in flow matching objective. This can be seen as adversarial training, label smoothing, and outlier penalization. For input robustness, we enforce consistent actions across input variations that preserve task semantics. To account for multiple perturbations, we formulate robustness as a multi-armed bandit problem and apply an upper confidence bound algorithm to automatically identify the most harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations. Our RobustVLA is particularly effective on real-world FR5 robot with limited demonstrations, showing absolute gains by 65.6% under perturbations of four modalities. 

**Abstract (ZH)**: 多模态视觉语言行动（VLA）模型在现实世界扰动下的鲁棒性对于部署至关重要：RobustVLA——对抗VLA输入和输出扰动的多模态鲁棒模型 

---
