{'arxiv_id': 'arXiv:2510.00933', 'title': 'Product-oriented Product-Process-Resource Asset Network and its Representation in AutomationML for Asset Administration Shell', 'authors': 'Sara Strakosova, Petr Novak, Petr Kadera', 'link': 'https://arxiv.org/abs/2510.00933', 'abstract': 'Current products, especially in the automotive sector, pose complex technical systems having a multi-disciplinary mechatronic nature. Industrial standards supporting system engineering and production typically (i) address the production phase only, but do not cover the complete product life cycle, and (ii) focus on production processes and resources rather than the products themselves. The presented approach is motivated by incorporating impacts of end-of-life phase of the product life cycle into the engineering phase. This paper proposes a modelling approach coming up from the Product-Process-Resource (PPR) modeling paradigm. It combines requirements on (i) respecting the product structure as a basis for the model, and (ii) it incorporates repairing, remanufacturing, or upcycling within cyber-physical production systems. The proposed model called PoPAN should accompany the product during the entire life cycle as a digital shadow encapsulated within the Asset Administration Shell of a product. To facilitate the adoption of the proposed paradigm, the paper also proposes serialization of the model in the AutomationML data format. The model is demonstrated on a use-case for disassembling electric vehicle batteries to support their remanufacturing for stationary battery applications.', 'abstract_zh': '当前产品，尤其是在汽车领域，具有多学科机电集成的复杂技术系统。支持系统工程和生产的工业标准通常仅（i）关注生产阶段，而不涵盖完整的产品生命周期，（ii）侧重生产过程和资源而非产品本身。本文提出的方法旨在将产品生命周期结束阶段的影响纳入工程阶段。本文提出了一种源自产品-过程-资源（PPR）建模范式的建模方法，该方法结合了对（i）尊重产品结构作为模型基础的要求，以及（ii）在数字化物理生产系统中纳入修复、再制造或升级改造。所提出的模型称为PoPAN，应在产品整个生命周期中作为嵌入产品资产管理壳中的数字影子伴随产品。为了促进该范式的采用，本文还建议将以AutomationML数据格式对模型进行序列化。该模型通过一个用于拆解电动汽车电池的用例来支持其用于固定电池应用的再制造进行演示。', 'title_zh': '面向产品的产品-工艺-资源资产网络及其在自动化建模语言中的表示用于资产行政壳体'}
{'arxiv_id': 'arXiv:2510.00619', 'title': 'What Did I Learn? Operational Competence Assessment for AI-Based Trajectory Planners', 'authors': 'Michiel Braat, Maren Buermann, Marijke van Weperen, Jan-Pieter Paardekooper', 'link': 'https://arxiv.org/abs/2510.00619', 'abstract': "Automated driving functions increasingly rely on machine learning for tasks like perception and trajectory planning, requiring large, relevant datasets. The performance of these algorithms depends on how closely the training data matches the task. To ensure reliable functioning, it is crucial to know what is included in the dataset to assess the trained model's operational risk. We aim to enhance the safe use of machine learning in automated driving by developing a method to recognize situations that an automated vehicle has not been sufficiently trained on. This method also improves explainability by describing the dataset at a human-understandable level. We propose modeling driving data as knowledge graphs, representing driving scenes with entities and their relationships. These graphs are queried for specific sub-scene configurations to check their occurrence in the dataset. We estimate a vehicle's competence in a driving scene by considering the coverage and complexity of sub-scene configurations in the training set. Higher complexity scenes require greater coverage for high competence. We apply this method to the NuPlan dataset, modeling it with knowledge graphs and analyzing the coverage of specific driving scenes. This approach helps monitor the competence of machine learning models trained on the dataset, which is essential for trustworthy AI to be deployed in automated driving.", 'abstract_zh': '自动化驾驶功能 increasingly 越来越多地依赖机器学习来进行感知和轨迹规划，这需要大量相关数据集。这些算法的性能取决于训练数据与任务的接近程度。为了确保可靠运行，了解数据集的内容以评估训练模型的操作风险至关重要。我们旨在通过开发一种方法来识别自动化车辆未充分训练的情况，以增强机器学习在自动化驾驶中的安全使用。该方法通过以人类可理解的方式描述数据集来提高模型的可解释性。我们建议将驾驶数据建模为知识图谱，用实体及其关系来表示驾驶场景。这些图谱查询特定子场景配置以检查它们在数据集中的出现情况。我们通过考虑训练集中子场景配置的覆盖范围和复杂性来估计车辆在驾驶场景中的能力。较高复杂度的场景需要更高的覆盖范围才能实现高水平的能力。我们将此方法应用于NuPlan数据集，用知识图谱建模并分析特定驾驶场景的覆盖范围。这种方法有助于监控基于数据集训练的机器学习模型的能力，这对可信的人工智能在自动化驾驶中的部署至关重要。', 'title_zh': '我学到了什么？基于AI的航迹规划器的操作能力评估'}
{'arxiv_id': 'arXiv:2510.00524', 'title': 'Two stage GNSS outlier detection for factor graph optimization based GNSS-RTK/INS/odometer fusion', 'authors': 'Baoshan Song, Penggao Yan, Xiao Xia, Yihan Zhong, Weisong Wen, Li-Ta Hsu', 'link': 'https://arxiv.org/abs/2510.00524', 'abstract': 'Reliable GNSS positioning in complex environments remains a critical challenge due to non-line-of-sight (NLOS) propagation, multipath effects, and frequent signal blockages. These effects can easily introduce large outliers into the raw pseudo-range measurements, which significantly degrade the performance of global navigation satellite system (GNSS) real-time kinematic (RTK) positioning and limit the effectiveness of tightly coupled GNSS-based integrated navigation system. To address this issue, we propose a two-stage outlier detection method and apply the method in a tightly coupled GNSS-RTK, inertial navigation system (INS), and odometer integration based on factor graph optimization (FGO). In the first stage, Doppler measurements are employed to detect pseudo-range outliers in a GNSS-only manner, since Doppler is less sensitive to multipath and NLOS effects compared with pseudo-range, making it a more stable reference for detecting sudden inconsistencies. In the second stage, pre-integrated inertial measurement units (IMU) and odometer constraints are used to generate predicted double-difference pseudo-range measurements, which enable a more refined identification and rejection of remaining outliers. By combining these two complementary stages, the system achieves improved robustness against both gross pseudo-range errors and degraded satellite measuring quality. The experimental results demonstrate that the two-stage detection framework significantly reduces the impact of pseudo-range outliers, and leads to improved positioning accuracy and consistency compared with representative baseline approaches. In the deep urban canyon test, the outlier mitigation method has limits the RMSE of GNSS-RTK/INS/odometer fusion from 0.52 m to 0.30 m, with 42.3% improvement.', 'abstract_zh': '可靠的GNSS定位在复杂环境中的保持问题依旧是一个关键挑战，由于非视距(NLOS)传播、多路径效应和频繁的信号阻塞。这些效应容易在原始伪距测量中引入大量离群值，显著降低全球导航卫星系统(GNSS)实时动态(RTK)定位性能，并限制基于GNSS的紧密耦合综合导航系统的有效性。为了解决这一问题，我们提出了一种两阶段离群值检测方法，并将其应用于基于因子图优化(FGO)的GNSS-RTK、惯性导航系统(INS)和里程计集成系统中。在第一阶段，采用多普勒测量来仅通过GNSS方式检测伪距离群值，因为与伪距相比，多普勒对多路径和NLOS效应的敏感度较低，因此它可以作为检测突然不一致性的更稳定的参考。在第二阶段，利用预积分惯性测量单元(IMU)和里程计约束生成预测的双差伪距测量，这使得更精细地识别并剔除剩余离群值成为可能。通过结合这两个互补的阶段，系统能够更好地抵抗粗伪距误差和卫星测量质量下降的影响。实验结果表明，两阶段检测框架显著减少了伪距离群值的影响，并与代表性基准方法相比，提高了定位精度和一致性。在深入城市峡谷测试中，离群值缓解方法将GNSS-RTK/INS/里程计融合的RMSE从0.52 m降低到0.30 m，提高了42.3%。', 'title_zh': '基于GNSS-RTK/INS/里程计融合的两阶段GNSS离群值检测因子图优化方法'}
{'arxiv_id': 'arXiv:2510.00225', 'title': 'TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks', 'authors': 'Yue Meng, Fei Chen, Chuchu Fan', 'link': 'https://arxiv.org/abs/2510.00225', 'abstract': 'Learning control policies for complex, long-horizon tasks is a central challenge in robotics and autonomous systems. Signal Temporal Logic (STL) offers a powerful and expressive language for specifying such tasks, but its non-Markovian nature and inherent sparse reward make it difficult to be solved via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus only on limited STL fragments or use STL robustness scores as sparse terminal rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization, to solve general STL tasks. TGPO decomposes STL into timed subgoals and invariant constraints and provides a hierarchical framework to tackle the problem. The high-level component of TGPO proposes concrete time allocations for these subgoals, and the low-level time-conditioned policy learns to achieve the sequenced subgoals using a dense, stage-wise reward signal. During inference, we sample various time allocations and select the most promising assignment for the policy network to rollout the solution trajectory. To foster efficient policy learning for complex STL with multiple subgoals, we leverage the learned critic to guide the high-level temporal search via Metropolis-Hastings sampling, focusing exploration on temporally feasible solutions. We conduct experiments on five environments, ranging from low-dimensional navigation to manipulation, drone, and quadrupedal locomotion. Under a wide range of STL tasks, TGPO significantly outperforms state-of-the-art baselines (especially for high-dimensional and long-horizon cases), with an average of 31.6% improvement in task success rate compared to the best baseline. The code will be available at this https URL', 'abstract_zh': '学习复杂长期任务的控制策略是机器人技术和自主系统领域的一个核心挑战。Signal Temporal Logic (STL) 提供了一种强大且表达力强的语言来指定此类任务，但由于其非马尔可夫性质和固有的稀疏奖励，通过标准强化学习 (RL) 算法解决它颇具难度。先前的 RL 方法仅关注有限的 STL 片段，或使用 STL 坚固性分数作为稀疏终态奖励。在本文中，我们提出了 TGPO（Temporal Grounded Policy Optimization），以解决通用的 STL 任务。TGPO 将 STL 分解为时间子目标和不变约束，并提供了一个分层框架来解决这个问题。TGPO 的高层组件为这些子目标提供具体的时间分配，而低层时间条件策略则学习使用密集的阶段奖励信号来实现按序排列的子目标。在推断过程中，我们采样不同的时间分配，并选择最有希望的分配供策略网络通过执行解算轨迹。为了促进复杂多子目标 STL 的高效策略学习，我们利用所学习的评论者通过 Metropolis-Hastings 抽样引导高层时间搜索，将探索重点放在时间可行的解决方案上。我们在五个环境中进行了实验，从低维度导航到操作、无人机和四足运动。在广泛的 STL 任务下，TGPO 显著优于最先进的基线（特别是在高维度和长期情况下），平均任务成功率提高了 31.6%。代码将在以下网址获取：this https URL', 'title_zh': 'TGPO: 时间依托策略优化方法用于信号时序逻辑任务'}
{'arxiv_id': 'arXiv:2510.01059', 'title': 'Predictive Control Barrier Functions for Discrete-Time Linear Systems with Unmodeled Delays', 'authors': 'Juan Augusto Paredes Salazar, James Usevitch, Ankit Goel', 'link': 'https://arxiv.org/abs/2510.01059', 'abstract': 'This paper introduces a predictive control barrier function (PCBF) framework for enforcing state constraints in discrete-time systems with unknown relative degree, which can be caused by input delays or unmodeled input dynamics. Existing discrete-time CBF formulations typically require the construction of auxiliary barrier functions when the relative degree is greater than one, which complicates implementation and may yield conservative safe sets. The proposed PCBF framework addresses this challenge by extending the prediction horizon to construct a CBF for an associated system with relative degree one. As a result, the superlevel set of the PCBF coincides with the safe set, simplifying constraint enforcement and eliminating the need for auxiliary functions. The effectiveness of the proposed method is demonstrated on a discrete-time double integrator with input delay and a bicopter system with position constraints.', 'abstract_zh': '基于未知相对度的离散时间系统预测控制障碍函数框架', 'title_zh': '离散时间线性系统中未建模延迟的预测控制屏障函数'}
{'arxiv_id': 'arXiv:2510.00425', 'title': 'Conflict-Based Search as a Protocol: A Multi-Agent Motion Planning Protocol for Heterogeneous Agents, Solvers, and Independent Tasks', 'authors': 'Rishi Veerapaneni, Alvin Tang, Haodong He, Sophia Zhao, Viraj Shah, Yidai Cen, Ziteng Ji, Gabriel Olin, Jon Arrizabalaga, Yorai Shaoul, Jiaoyang Li, Maxim Likhachev', 'link': 'https://arxiv.org/abs/2510.00425', 'abstract': 'Imagine the future construction site, hospital, office, or even sophisticated household with dozens of robots bought from different manufacturers. How can we enable these different systems to effectively move in a shared environment, given that each robot may have its own independent motion planning system? This work shows how we can get efficient collision-free movements between algorithmically heterogeneous agents by using Conflict-Based Search (Sharon et al. 2015) as a protocol. At its core, the CBS Protocol requires one specific single-agent motion planning API; finding a collision-free path that satisfies certain space-time constraints. Given such an API, CBS uses a central planner to find collision-free paths - independent of how the API is implemented. We show how this protocol enables multi-agent motion planning for a heterogeneous team of agents completing independent tasks with a variety of single-agent planners including: Heuristic Search (e.g., A*), Sampling Based Search (e.g., RRT), Optimization (e.g., Direct Collocation), Diffusion, and Reinforcement Learning.', 'abstract_zh': '想象未来的建筑工地、医院、办公室，甚至是拥有几十台来自不同制造商机器人的复杂家庭。如何在这些不同的系统能够有效共享同一环境进行移动，尤其是每台机器人可能具有独立的运动规划系统时？本研究展示了如何通过使用冲突基于搜索（CBS，Sharon等，2015）协议来实现算法异构代理之间的高效无碰撞移动。CBS协议的核心在于需要一个特定的单一代理运动规划API；即在满足特定空间时间约束的前提下，寻找一条无碰撞路径。利用这样一个API，CBS使用一个中央规划器来找到无碰撞路径——这与API的实现方式无关。我们展示了如何通过这种协议实现异构多代理团队的运动规划，该团队由各种单一代理规划器独立完成任务，包括启发式搜索（例如A*）、采样基于搜索（例如RRT）、优化（例如直接共轭）、扩散以及强化学习。', 'title_zh': '基于冲突的搜索作为一种协议：异构代理、求解器和独立任务的多Agent运动规划协议'}
{'arxiv_id': 'arXiv:2510.00120', 'title': 'The Formation of Trust in Autonomous Vehicles after Interacting with Robotaxis on Public Roads', 'authors': 'Xiang Chang, Zhijie Yi, Yichang Liu, Hongling Sheng, Dengbo He', 'link': 'https://arxiv.org/abs/2510.00120', 'abstract': 'This study investigates how pedestrian trust, receptivity, and behavior evolve during interactions with Level-4 autonomous vehicles (AVs) at uncontrolled urban intersections in a naturalistic setting. While public acceptance is critical for AV adoption, most prior studies relied on simplified simulations or field tests. We conducted a real-world experiment in a commercial Robotaxi operation zone, where 33 participants repeatedly crossed an uncontrolled intersection with frequent Level-4 Robotaxi traffic. Participants completed the Pedestrian Behavior Questionnaire (PBQ), Pedestrian Receptivity Questionnaire for Fully AVs (PRQF), pre- and post-experiment Trust in AVs Scale, and Personal Innovativeness Scale (PIS). Results showed that trust in AVs significantly increased post-experiment, with the increase positively associated with the Interaction component of PRQF. Additionally, both the Positive and Error subscales of the PBQ significantly influenced trust change. This study reveals how trust forms in real-world pedestrian-AV encounters, offering insights beyond lab-based research by accounting for population heterogeneity.', 'abstract_zh': '本研究探讨了行人与未设交通信号控制的城市交叉口处的四级自动驾驶车辆（AVs）互动过程中，行人信任、接受度和行为如何演变。在真实环境中，我们在中国一家商用Robotaxi运营区进行了实验，33名参与者频繁与四级Robotaxi交通互动后穿过一个未设交通信号控制的交叉口。参与者完成了行人行为问卷（PBQ）、全自动驾驶车辆接受度问卷（PRQF）、实验前后的AVs信任量表以及个人创新度量表（PIS）。结果显示，实验后行人对AVs的信任显著增加，且这种增加与PRQF的互动成分正相关。此外，PBQ的积极成分和错误成分显著影响了信任的变化。本研究揭示了在真实世界行人-AV相遇中信任形成的机制，通过考虑人群异质性提供了超越基于实验室研究的见解。', 'title_zh': '公共道路上与Robotaxi交互后自动驾驶车辆信任形成的机制'}
{'arxiv_id': 'arXiv:2510.01115', 'title': 'Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis', 'authors': 'Evan Heus, Rick Bookstaber, Dhruv Sharma', 'link': 'https://arxiv.org/abs/2510.01115', 'abstract': "Large Language Models (LLMs) struggle with the complex, multi-modal, and network-native data underlying financial risk. Standard Retrieval-Augmented Generation (RAG) oversimplifies relationships, while specialist models are costly and static. We address this gap with an LLM-centric agent framework for supply chain risk analysis. Our core contribution is to exploit the inherent duality between networks and knowledge graphs (KG). We treat the supply chain network as a KG, allowing us to use structural network science principles for retrieval. A graph traverser, guided by network centrality scores, efficiently extracts the most economically salient risk paths. An agentic architecture orchestrates this graph retrieval alongside data from numerical factor tables and news streams. Crucially, it employs novel ``context shells'' -- descriptive templates that embed raw figures in natural language -- to make quantitative data fully intelligible to the LLM. This lightweight approach enables the model to generate concise, explainable, and context-rich risk narratives in real-time without costly fine-tuning or a dedicated graph database.", 'abstract_zh': '大型语言模型（LLMs）在处理金融风险背后的复杂、多模态和网络原生数据时面临挑战。标准的检索增强生成（RAG）过分简化了关系，而专业模型则成本高昂且静态。我们通过一个以LLM为中心的代理框架来解决这一问题，用于供应链风险分析。我们核心的贡献在于利用网络和知识图谱（KG）之间的内在二元性。我们将供应链网络视为一个KG，从而可以利用结构网络科学的原则来进行检索。图遍历器在网络中心性分数的引导下，高效地提取出最具经济意义的风险路径。一种代理架构协调这一图检索过程，同时整合数值因子表数据和新闻流数据。关键的是，该架构采用了新型的“上下文壳”——描述性模板，将原始数字信息嵌入自然语言中，从而使定量数据对LLM完全可理解。这一轻量级的方法使模型能够实时生成简洁、可解释且富含上下文的风险叙事，而无需昂贵的微调或专用图数据库。', 'title_zh': '探索网络-知识图谱二元性：代理供应链风险分析案例研究'}
{'arxiv_id': 'arXiv:2510.01114', 'title': 'PRISM-Consult: A Panel-of-Experts Architecture for Clinician-Aligned Diagnosis', 'authors': 'Lionel Levine, John Santerre, Alexander S. Young, T. Barry Levine, Francis Campion, Majid Sarrafzadeh', 'link': 'https://arxiv.org/abs/2510.01114', 'abstract': "We present PRISM-Consult, a clinician-aligned panel-of-experts architecture that extends the compact PRISM sequence model into a routed family of domain specialists. Episodes are tokenized as structured clinical events; a light-weight router reads the first few tokens and dispatches to specialist models (Cardiac-Vascular, Pulmonary, Gastro-Oesophageal, Musculoskeletal, Psychogenic). Each specialist inherits PRISM's small transformer backbone and token template, enabling parameter efficiency and interpretability. On real-world Emergency Department cohorts, specialists exhibit smooth convergence with low development perplexities across domains, while the router achieves high routing quality and large compute savings versus consult-all under a safety-first policy. We detail the data methodology (initial vs. conclusive ICD-9 families), routing thresholds and calibration, and report per-domain results to avoid dominance by common events. The framework provides a practical path to safe, auditable, and low-latency consult at scale, and we outline validation steps-external/temporal replication, asymmetric life-threat thresholds, and multi-label arbitration-to meet prospective clinical deployment standards.", 'abstract_zh': 'PRISM-Consult：一种符合临床医生需求的专科专家架构', 'title_zh': 'PRISM-Consult: 专家面板架构以实现临床导向的诊断'}
{'arxiv_id': 'arXiv:2510.01094', 'title': 'Optimizing Fairness in Production Planning: A Human-Centric Approach to Machine and Workforce Allocation', 'authors': 'Alexander Nasuta, Alessandro Cisi, Sylwia Olbrych, Gustavo Vieira, Rui Fernandes, Lucas Paletta, Marlene Mayr, Rishyank Chevuri, Robert Woitsch, Hans Aoyang Zhou, Anas Abdelrazeq, Robert H. Schmitt', 'link': 'https://arxiv.org/abs/2510.01094', 'abstract': 'This work presents a two-layer, human-centric production planning framework designed to optimize both operational efficiency and workforce fairness in industrial manufacturing. The first layer formulates the Order-Line allocation as a Constraint Programming (CP) problem, generating high-utilization production schedules that respect machine capacities, processing times, and due dates. The second layer models Worker-Line allocation as a Markov Decision Process (MDP), integrating human factors such as worker preference, experience, resilience, and medical constraints into the assignment process. Three solution strategies, greedy allocation, MCTS, and RL, are implemented and compared across multiple evaluation scenarios. The proposed system is validated through 16 test sessions with domain experts from the automotive industry, combining quantitative key performance indicators (KPIs) with expert ratings. Results indicate that the CP-based scheduling approach produces compact, feasible production plans with low tardiness, while the MDP-based worker allocation significantly improves fairness and preference alignment compared to baseline approaches. Domain experts rated both the Order-Line and Worker-Line components as effective and highlighted opportunities to further refine the objective function to penalize excessive earliness and improve continuity in worker assignments. Overall, the findings demonstrate that combining CP with learning-based decision-making provides a robust approach for human-centric production planning. The approach enables simultaneous optimization of throughput and workforce well-being, offering a practical foundation for fair and efficient manufacturing scheduling in industrial settings.', 'abstract_zh': '一种兼顾运营效率与 workforce 公平性的两层人类中心生产规划框架', 'title_zh': '基于以人为本的方法优化生产规划中的公平性：机器与劳动力分配的公平性优化方法'}
{'arxiv_id': 'arXiv:2510.01038', 'title': 'Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI', 'authors': 'Akchunya Chanchal, David A. Kelly, Hana Chockler', 'link': 'https://arxiv.org/abs/2510.01038', 'abstract': "Black-box explainability methods are popular tools for explaining the decisions of image classifiers. A major drawback of these tools is their reliance on mutants obtained by occluding parts of the input, leading to out-of-distribution images. This raises doubts about the quality of the explanations. Moreover, choosing an appropriate occlusion value often requires domain knowledge. In this paper we introduce a novel forward-pass paradigm Activation-Deactivation (AD), which removes the effects of occluded input features from the model's decision-making by switching off the parts of the model that correspond to the occlusions. We introduce ConvAD, a drop-in mechanism that can be easily added to any trained Convolutional Neural Network (CNN), and which implements the AD paradigm. This leads to more robust explanations without any additional training or fine-tuning. We prove that the ConvAD mechanism does not change the decision-making process of the network. We provide experimental evaluation across several datasets and model architectures. We compare the quality of AD-explanations with explanations achieved using a set of masking values, using the proxies of robustness, size, and confidence drop-off. We observe a consistent improvement in robustness of AD explanations (up to 62.5%) compared to explanations obtained with occlusions, demonstrating that ConvAD extracts more robust explanations without the need for domain knowledge.", 'abstract_zh': '黑盒可解释性方法是解释图像分类器决策的流行工具。这些工具的主要缺点是依赖于通过遮蔽输入部分获得的变异体，导致生成离分布图像。这引发了对解释质量的怀疑。此外，选择合适的遮蔽值通常需要领域知识。在本文中，我们引入了一种新的前向传递范式：激活-去激活（AD），通过关闭与遮蔽部分对应的模型部分来消除遮蔽输入特征对模型决策的影响。我们引入了ConvAD，这是一种可轻松添加到任何训练好的卷积神经网络（CNN）中的机制，并实现了AD范式。这带来了更稳健的解释，而无需额外的训练或微调。我们证明ConvAD机制不会改变网络的决策过程。我们在多个数据集和模型架构上提供了实验评估。我们用鲁棒性、大小和置信度下降作为代理，将AD解释的质量与使用一系列遮蔽值获得的解释进行比较。我们观察到AD解释的鲁棒性在某些情况下提高了62.5%（与遮蔽获得的解释相比），证明ConvAD在不需要领域知识的情况下提取了更稳健的解释。', 'title_zh': '激活-去激活：稳健的后验可解释人工智能 geral 框架'}
{'arxiv_id': 'arXiv:2510.01006', 'title': 'Integrating AI and Ensemble Forecasting: Explainable Materials Planning with Scorecards and Trend Insights for a Large-Scale Manufacturer', 'authors': 'Saravanan Venkatachalam', 'link': 'https://arxiv.org/abs/2510.01006', 'abstract': 'This paper presents a practical architecture for after-sales demand forecasting and monitoring that unifies a revenue- and cluster-aware ensemble of statistical, machine-learning, and deep-learning models with a role-driven analytics layer for scorecards and trend diagnostics. The framework ingests exogenous signals (installed base, pricing, macro indicators, life cycle, seasonality) and treats COVID-19 as a distinct regime, producing country-part forecasts with calibrated intervals. A Pareto-aware segmentation forecasts high-revenue items individually and pools the long tail via clusters, while horizon-aware ensembling aligns weights with business-relevant losses (e.g., WMAPE). Beyond forecasts, a performance scorecard delivers decision-focused insights: accuracy within tolerance thresholds by revenue share and count, bias decomposition (over- vs under-forecast), geographic and product-family hotspots, and ranked root causes tied to high-impact part-country pairs. A trend module tracks trajectories of MAPE/WMAPE and bias across recent months, flags entities that are improving or deteriorating, detects change points aligned with known regimes, and attributes movements to lifecycle and seasonal factors. LLMs are embedded in the analytics layer to generate role-aware narratives and enforce reporting contracts. They standardize business definitions, automate quality checks and reconciliations, and translate quantitative results into concise, explainable summaries for planners and executives. The system exposes a reproducible workflow - request specification, model execution, database-backed artifacts, and AI-generated narratives - so planners can move from "How accurate are we now?" to "Where is accuracy heading and which levers should we pull?", closing the loop between forecasting, monitoring, and inventory decisions across more than 90 countries and about 6,000 parts.', 'abstract_zh': '一种基于角色的统计、机器学习与深度学习模型集成的售后需求预测和监控实用架构及其性能分析与趋势诊断框架', 'title_zh': '将AI与 ensemble 预测集成：具有评分卡和趋势洞察的可解释材料规划'}
{'arxiv_id': 'arXiv:2510.00976', 'title': 'Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware Secure Aggregation', 'authors': 'Aueaphum Aueawatthanaphisut', 'link': 'https://arxiv.org/abs/2510.00976', 'abstract': 'Rare-disease diagnosis remains one of the most pressing challenges in digital health, hindered by extreme data scarcity, privacy concerns, and the limited resources of edge devices. This paper proposes the Adaptive Federated Few-Shot Rare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i) few-shot federated optimization with meta-learning to generalize from limited patient samples, (ii) energy-aware client scheduling to mitigate device dropouts and ensure balanced participation, and (iii) secure aggregation with calibrated differential privacy to safeguard sensitive model updates. Unlike prior work that addresses these aspects in isolation, AFFR unifies them into a modular pipeline deployable on real-world clinical networks. Experimental evaluation on simulated rare-disease detection datasets demonstrates up to 10% improvement in accuracy compared with baseline FL, while reducing client dropouts by over 50% without degrading convergence. Furthermore, privacy-utility trade-offs remain within clinically acceptable bounds. These findings highlight AFFR as a practical pathway for equitable and trustworthy federated diagnosis of rare conditions.', 'abstract_zh': 'Rare疾病的诊断仍然是数字健康领域最具挑战性的问题之一，受到数据极度稀缺、隐私担忧以及边缘设备资源限制的阻碍。本文提出了一种适配 federated 少样本罕见疾病诊断框架（Adaptive Federated Few-Shot Rare-Disease Diagnosis, AFFR），该框架整合了三个支柱：(i) 结合元学习的少样本 federated 优化，以从有限的患者样本中泛化；(ii) 能量感知客户端调度，以减少设备掉线并确保参与的均衡；(iii) 校准差分隐私的安全聚合，以保护敏感模型更新。与以往各自解决这些方面的工作不同，AFFR 将它们统一到一个模块化的管道中，可在实际临床网络中部署。在模拟的罕见疾病检测数据集上的实验评估显示，与基线 federated 学习相比，准确性提高了多达 10%，同时降低了超过 50% 的客户端掉线率，而不会影响收敛性。此外，隐私-效用权衡仍然在临床可接受的范围内。这些发现突显了 AFFR 作为公平和可信赖的 federated 罕见疾病诊断实践路径的重要性。', 'title_zh': '自适应联邦少量示例罕见疾病诊断的能源感知安全聚合'}
{'arxiv_id': 'arXiv:2510.00960', 'title': 'A Neuro-Fuzzy System for Interpretable Long-Term Stock Market Forecasting', 'authors': 'Miha Ožbot, Igor Škrjanc, Vitomir Štruc', 'link': 'https://arxiv.org/abs/2510.00960', 'abstract': 'In the complex landscape of multivariate time series forecasting, achieving both accuracy and interpretability remains a significant challenge. This paper introduces the Fuzzy Transformer (Fuzzformer), a novel recurrent neural network architecture combined with multi-head self-attention and fuzzy inference systems to analyze multivariate stock market data and conduct long-term time series forecasting. The method leverages LSTM networks and temporal attention to condense multivariate data into interpretable features suitable for fuzzy inference systems. The resulting architecture offers comparable forecasting performance to conventional models such as ARIMA and LSTM while providing meaningful information flow within the network. The method was examined on the real world stock market index S\\&P500. Initial results show potential for interpretable forecasting and identify current performance tradeoffs, suggesting practical application in understanding and forecasting stock market behavior.', 'abstract_zh': '在多变量时间序列预测的复杂景观中，同时实现准确性和可解释性仍是一项重大挑战。本文介绍了Fuzzy Transformer（Fuzzformer）这一新颖的递归神经网络架构，该架构结合了多头自注意力机制和模糊推理系统，用于分析多变量股市数据并进行长期时间序列预测。该方法利用LSTM网络和时间注意力机制将多变量数据压缩成适用于模糊推理系统的可解释特征。所提出的方法在与ARIMA和LSTM等传统模型相当的预测性能的同时，提供了网络内部有意义的信息流。该方法在实际的标普500指数股市上进行了测试。初步结果表明，其在可解释预测方面具有潜力，并识别出当前的表现权衡，暗示了其在理解和预测股市行为中的实际应用价值。', 'title_zh': '一种用于可解释的长期股市 Forecasting 的神经模糊系统'}
{'arxiv_id': 'arXiv:2510.00958', 'title': 'Test-Time Search in Neural Graph Coarsening Procedures for the Capacitated Vehicle Routing Problem', 'authors': 'Yoonju Sim, Hyeonah Kim, Changhyun Kwon', 'link': 'https://arxiv.org/abs/2510.00958', 'abstract': 'The identification of valid inequalities, such as the rounded capacity inequalities (RCIs), is a key component of cutting plane methods for the Capacitated Vehicle Routing Problem (CVRP). While a deep learning-based separation method can learn to find high-quality cuts, our analysis reveals that the model produces fewer cuts than expected because it is insufficiently sensitive to generate a diverse set of generated subsets. This paper proposes an alternative: enhancing the performance of a trained model at inference time through a new test-time search with stochasticity. First, we introduce stochastic edge selection into the graph coarsening procedure, replacing the previously proposed greedy approach. Second, we propose the Graph Coarsening History-based Partitioning (GraphCHiP) algorithm, which leverages coarsening history to identify not only RCIs but also, for the first time, the Framed capacity inequalities (FCIs). Experiments on randomly generated CVRP instances demonstrate the effectiveness of our approach in reducing the dual gap compared to the existing neural separation method. Additionally, our method discovers effective FCIs on a specific instance, despite the challenging nature of identifying such cuts.', 'abstract_zh': '基于深度学习的比例容量不等式分离方法在容量约束车辆路径问题中的改进研究', 'title_zh': '容量约束车辆 routing 问题中神经图粗化过程的测试时搜索'}
{'arxiv_id': 'arXiv:2510.00876', 'title': 'Unveiling Interesting Insights: Monte Carlo Tree Search for Knowledge Discovery', 'authors': 'Pietro Totis, Alberto Pozanco, Daniel Borrajo', 'link': 'https://arxiv.org/abs/2510.00876', 'abstract': "Organizations are increasingly focused on leveraging data from their processes to gain insights and drive decision-making. However, converting this data into actionable knowledge remains a difficult and time-consuming task. There is often a gap between the volume of data collected and the ability to process and understand it, which automated knowledge discovery aims to fill. Automated knowledge discovery involves complex open problems, including effectively navigating data, building models to extract implicit relationships, and considering subjective goals and knowledge. In this paper, we introduce a novel method for Automated Insights and Data Exploration (AIDE), that serves as a robust foundation for tackling these challenges through the use of Monte Carlo Tree Search (MCTS). We evaluate AIDE using both real-world and synthetic data, demonstrating its effectiveness in identifying data transformations and models that uncover interesting data patterns. Among its strengths, AIDE's MCTS-based framework offers significant extensibility, allowing for future integration of additional pattern extraction strategies and domain knowledge. This makes AIDE a valuable step towards developing a comprehensive solution for automated knowledge discovery.", 'abstract_zh': '组织越来越注重利用其过程中的数据以获得洞察并推动决策。然而，将这些数据转化为可操作的知识仍然是一个困难且耗时的过程。往往存在收集的数据量与处理和理解数据的能力之间的差距，自动化知识发现旨在填补这一差距。自动化知识发现涉及复杂的开放性问题，包括有效地导航数据、构建模型以提取隐含关系、以及考虑主观目标和知识。在本文中，我们介绍了一种新型的自动化洞察与数据探索（AIDE）方法，通过蒙特卡洛树搜索（MCTS）的应用为其提供了坚实的基础，以应对这些挑战。我们使用真实世界和合成数据评估了AIDE，展示了其在识别数据转换和模型方面的有效性，这些模型能够揭示有趣的数据模式。AIDE的基于MCTS的框架具有显著的扩展性，未来可以集成其他模式提取策略和领域知识，使其成为开发全面的自动化知识发现解决方案的重要一步。', 'title_zh': '揭开有趣洞察的面纱：蒙特卡洛树搜索在知识发现中的应用'}
{'arxiv_id': 'arXiv:2510.00836', 'title': 'Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based Models and Synthetic Oversampling Techniques', 'authors': 'Jieun Yu, Minjung Park, Sangmi Chai', 'link': 'https://arxiv.org/abs/2510.00836', 'abstract': 'This study aims to detect pump and dump (P&D) manipulation in cryptocurrency markets, where the scarcity of such events causes severe class imbalance and hinders accurate detection. To address this issue, the Synthetic Minority Oversampling Technique (SMOTE) was applied, and advanced ensemble learning models were evaluated to distinguish manipulative trading behavior from normal market activity. The experimental results show that applying SMOTE greatly enhanced the ability of all models to detect P&D events by increasing recall and improving the overall balance between precision and recall. In particular, XGBoost and LightGBM achieved high recall rates (94.87% and 93.59%, respectively) with strong F1-scores and demonstrated fast computational performance, making them suitable for near real time surveillance. These findings indicate that integrating data balancing techniques with ensemble methods significantly improves the early detection of manipulative activities, contributing to a fairer, more transparent, and more stable cryptocurrency market.', 'abstract_zh': '本研究旨在检测加密货币市场中的泵抬饯卖（P&D）操纵，由于此类事件的稀有性导致严重的类别不平衡，阻碍了准确检测。为解决这一问题，本研究应用了合成少数类过采样技术（SMOTE），并评估了先进的集成学习模型，以区分操纵性交易行为和正常市场活动。实验结果表明，应用SMOTE显著增强了所有模型检测P&D事件的能力，通过提高召回率并改善精确率和召回率之间的整体平衡。特别是XGBoost和LightGBM实现了高召回率（分别为94.87%和93.59%），具有强大的F1分数和快速的计算性能，使其适合近实时监控。这些发现表明，将数据平衡技术与集成方法结合显著提高了操纵性活动的早期检测能力，有助于构建更加公平、透明和稳定的加密货币市场。', 'title_zh': '基于集成模型和合成过采样技术的加密货币 Pump-and-Dump 检测改进方法'}
{'arxiv_id': 'arXiv:2510.00831', 'title': 'Benchmarking Machine Learning Models for Fault Classification and Localization in Power System Protection', 'authors': 'Julian Oelhaf, Georg Kordowich, Changhun Kim, Paula Andrea Pérez-Toro, Christian Bergler, Andreas Maier, Johann Jäger, Siming Bayer', 'link': 'https://arxiv.org/abs/2510.00831', 'abstract': 'The increasing integration of distributed energy resources (DERs), particularly renewables, poses significant challenges for power system protection, with fault classification (FC) and fault localization (FL) being among the most critical tasks. Conventional protection schemes, based on fixed thresholds, cannot reliably identify and localize short circuits with the increasing complexity of the grid under dynamic conditions. Machine learning (ML) offers a promising alternative; however, systematic benchmarks across models and settings remain limited. This work presents, for the first time, a comparative benchmarking study of classical ML models for FC and FL in power system protection based on EMT data. Using voltage and current waveforms segmented into sliding windows of 10 ms to 50 ms, we evaluate models under realistic real-time constraints. Performance is assessed in terms of accuracy, robustness to window size, and runtime efficiency. The best-performing FC model achieved an F1 score of 0.992$\\pm$0.001, while the top FL model reached an R2 of 0.806$\\pm$0.008 with a mean processing time of 0.563 ms.', 'abstract_zh': '分布式能源资源（DERs）特别是可再生能源 increasing 连接对电力系统保护带来的挑战：短路分类（FC）和短路定位（FL）的关键任务分析及基于EMT数据的经典机器学习模型比较研究', 'title_zh': '基于电力系统保护中故障分类与定位的机器学习模型 benchmark研究'}
{'arxiv_id': 'arXiv:2510.00821', 'title': 'Logical Consistency Between Disagreeing Experts and Its Role in AI Safety', 'authors': 'Andrés Corrada-Emmanuel', 'link': 'https://arxiv.org/abs/2510.00821', 'abstract': 'If two experts disagree on a test, we may conclude both cannot be 100 per cent correct. But if they completely agree, no possible evaluation can be excluded. This asymmetry in the utility of agreements versus disagreements is explored here by formalizing a logic of unsupervised evaluation for classifiers. Its core problem is computing the set of group evaluations that are logically consistent with how we observe them agreeing and disagreeing in their decisions. Statistical summaries of their aligned decisions are inputs into a Linear Programming problem in the integer space of possible correct or incorrect responses given true labels. Obvious logical constraints, such as, the number of correct responses cannot exceed the number of observed responses, are inequalities. But in addition, there are axioms, universally applicable linear equalities that apply to all finite tests. The practical and immediate utility of this approach to unsupervised evaluation using only logical consistency is demonstrated by building no-knowledge alarms that can detect when one or more LLMs-as-Judges are violating a minimum grading threshold specified by the user.', 'abstract_zh': '探讨一致性和分歧在评估中的不对称性：基于无监督评价的逻辑 formalization及其应用', 'title_zh': '分歧专家之间的逻辑一致性及其在AI安全中的作用'}
{'arxiv_id': 'arXiv:2510.00817', 'title': 'Semantic Bridges Between First Order c-Representations and Cost-Based Semantics: An Initial Perspective', 'authors': 'Nicholas Leisegang, Giovanni Casini, Thomas Meyer', 'link': 'https://arxiv.org/abs/2510.00817', 'abstract': 'Weighted-knowledge bases and cost-based semantics represent a recent formalism introduced by Bienvenu et al. for Ontology Mediated Data Querying in the case where a given knowledge base is inconsistent. This is done by adding a weight to each statement in the knowledge base (KB), and then giving each DL interpretation a cost based on how often it breaks rules in the KB. In this paper we compare this approach with c-representations, a form of non-monotonic reasoning originally introduced by Kern-Isberner. c-Representations describe a means to interpret defeasible concept inclusions in the first-order case. This is done by assigning a numerical ranking to each interpretations via penalties for each violated conditional. We compare these two approaches on a semantic level. In particular, we show that under certain conditions a weighted knowledge base and a set of defeasible conditionals can generate the same ordering on interpretations, and therefore an equivalence of semantic structures up to relative cost. Moreover, we compare entailment described in both cases, where certain notions are equivalently expressible in both formalisms. Our results have the potential to benefit further work on both cost-based semantics and c-representations', 'abstract_zh': '基于权重的知识本体和成本导向语义在本体介导数据查询中的应用：与Kern-Isberner提出c-表示的比较', 'title_zh': '一阶 c-表示与成本基础语义之间的语义桥梁：初步视角'}
{'arxiv_id': 'arXiv:2510.00795', 'title': 'Benchmarking Agentic Systems in Automated Scientific Information Extraction with ChemX', 'authors': 'Anastasia Vepreva, Julia Razlivina, Maria Eremeeva, Nina Gubina, Anastasia Orlova, Aleksei Dmitrenko, Ksenya Kapranova, Susan Jyakhwo, Nikita Vasilev, Arsen Sarkisyan, Ivan Yu. Chernyshov, Vladimir Vinogradov, Andrei Dmitrenko', 'link': 'https://arxiv.org/abs/2510.00795', 'abstract': 'The emergence of agent-based systems represents a significant advancement in artificial intelligence, with growing applications in automated data extraction. However, chemical information extraction remains a formidable challenge due to the inherent heterogeneity of chemical data. Current agent-based approaches, both general-purpose and domain-specific, exhibit limited performance in this domain. To address this gap, we present ChemX, a comprehensive collection of 10 manually curated and domain-expert-validated datasets focusing on nanomaterials and small molecules. These datasets are designed to rigorously evaluate and enhance automated extraction methodologies in chemistry. To demonstrate their utility, we conduct an extensive benchmarking study comparing existing state-of-the-art agentic systems such as ChatGPT Agent and chemical-specific data extraction agents. Additionally, we introduce our own single-agent approach that enables precise control over document preprocessing prior to extraction. We further evaluate the performance of modern baselines, such as GPT-5 and GPT-5 Thinking, to compare their capabilities with agentic approaches. Our empirical findings reveal persistent challenges in chemical information extraction, particularly in processing domain-specific terminology, complex tabular and schematic representations, and context-dependent ambiguities. The ChemX benchmark serves as a critical resource for advancing automated information extraction in chemistry, challenging the generalization capabilities of existing methods, and providing valuable insights into effective evaluation strategies.', 'abstract_zh': '基于代理的系统 emergence 代表了人工智能的重要进展，并在自动化数据提取方面有着日益增长的应用。然而，由于化学数据的固有异质性，化学信息提取仍然是一个严峻的挑战。现有的通用和领域特定的基于代理的方法在这个领域表现有限。为了解决这一差距，我们提出了 ChemX，这是一个包含 10 个手工精选和领域专家验证的数据集的综合集合，专注于纳米材料和小分子。这些数据集旨在严格评估和提升化学领域的自动化提取方法。为了展示其用途，我们进行了广泛的标准测试，比较了现有的基于代理的系统（如 ChatGPT Agent 和化学特定数据提取代理）。此外，我们还引入了一种自己的单代理方法，使其能够在提取前对文档预处理进行精确控制。我们进一步评估了现代基线方法（如 GPT-5 和 GPT-5 Thinking），以比较它们与基于代理的方法的能力差异。我们的实证发现揭示了化学信息提取中持续存在的挑战，特别是在处理领域特定术语、复杂表格和示意图表示以及上下文依赖性歧义方面。ChemX 基准测试作为推动化学领域自动化信息提取进展的宝贵资源，挑战现有方法的泛化能力，并提供了关于有效评估策略的有价值的见解。', 'title_zh': '基于ChemX的自动化科学研究信息抽取中代理系统基准测试'}
{'arxiv_id': 'arXiv:2510.00793', 'title': 'AI in data science education: experiences from the classroom', 'authors': 'J.A. Hageman, C.F.W. Peeters', 'link': 'https://arxiv.org/abs/2510.00793', 'abstract': "This study explores the integration of AI, particularly large language models (LLMs) like ChatGPT, into educational settings, focusing on the implications for teaching and learning. Through interviews with course coordinators from data science courses at Wageningen University, this research identifies both the benefits and challenges associated with AI in the classroom. While AI tools can streamline tasks and enhance learning, concerns arise regarding students' overreliance on these technologies, potentially hindering the development of essential cognitive and problem solving skills. The study highlights the importance of responsible AI usage, ethical considerations, and the need for adapting assessment methods to ensure educational outcomes are met. With careful integration, AI can be a valuable asset in education, provided it is used to complement rather than replace fundamental learning processes.", 'abstract_zh': '本研究探讨了人工智能，特别是大型语言模型（LLMs）如ChatGPT，融入教育环境中的方式，重点关注其对教学和学习的影响。通过访谈瓦格宁根大学数据科学课程的课程协调者，本研究识别了人工智能在学校环境中使用的优势与挑战。尽管人工智能工具可以简化任务并增强学习，但学生过度依赖这些技术可能导致其认知和解决问题能力的发展受阻。研究强调负责任地使用人工智能、伦理考虑以及适应性评估方法调整的重要性，以确保教育成果的实现。通过谨慎融合，人工智能可以成为教育领域的有价值的资产，前提是将其用于补充而不是替代基本的学习过程。', 'title_zh': 'AI在数据科学教育中的应用：课堂教学经验'}
{'arxiv_id': 'arXiv:2510.00778', 'title': 'DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models', 'authors': 'Seunghoo Hong, Geonho Son, Juhun Lee, Simon S. Woo', 'link': 'https://arxiv.org/abs/2510.00778', 'abstract': 'Diffusion models have shown to be strong representation learners, showcasing state-of-the-art performance across multiple domains. Aside from accelerated sampling, DDIM also enables the inversion of real images back to their latent codes. A direct inheriting application of this inversion operation is real image editing, where the inversion yields latent trajectories to be utilized during the synthesis of the edited image. Unfortunately, this practical tool has enabled malicious users to freely synthesize misinformative or deepfake contents with greater ease, which promotes the spread of unethical and abusive, as well as privacy-, and copyright-infringing contents. While defensive algorithms such as AdvDM and Photoguard have been shown to disrupt the diffusion process on these images, the misalignment between their objectives and the iterative denoising trajectory at test time results in weak disruptive this http URL this work, we present the DDIM Inversion Attack (DIA) that attacks the integrated DDIM trajectory path. Our results support the effective disruption, surpassing previous defensive methods across various editing methods. We believe that our frameworks and results can provide practical defense methods against the malicious use of AI for both the industry and the research community. Our code is available here: this https URL.', 'abstract_zh': '基于DDIM的逆过程攻击：有效抵御AI滥用的防御方法', 'title_zh': 'DIA：确定性反演在扩散模型中的对抗曝光'}
{'arxiv_id': 'arXiv:2510.00706', 'title': 'AttentionDep: Domain-Aware Attention for Explainable Depression Severity Assessment', 'authors': 'Yusif Ibrahimov, Tarique Anwar, Tommy Yuan, Turan Mutallimov, Elgun Hasanov', 'link': 'https://arxiv.org/abs/2510.00706', 'abstract': "In today's interconnected society, social media platforms provide a window into individuals' thoughts, emotions, and mental states. This paper explores the use of platforms like Facebook, X (formerly Twitter), and Reddit for depression severity detection. We propose AttentionDep, a domain-aware attention model that drives explainable depression severity estimation by fusing contextual and domain knowledge. Posts are encoded hierarchically using unigrams and bigrams, with attention mechanisms highlighting clinically relevant tokens. Domain knowledge from a curated mental health knowledge graph is incorporated through a cross-attention mechanism, enriching the contextual features. Finally, depression severity is predicted using an ordinal regression framework that respects the clinical-relevance and natural ordering of severity levels. Our experiments demonstrate that AttentionDep outperforms state-of-the-art baselines by over 5% in graded F1 score across datasets, while providing interpretable insights into its predictions. This work advances the development of trustworthy and transparent AI systems for mental health assessment from social media.", 'abstract_zh': '今天相互连接的社会中，社交媒体平台为了解个体的思想、情感和心理状态提供了一个窗口。本文探讨了在Facebook、X（原Twitter）和Reddit等平台中检测抑郁严重程度的方法。我们提出了一种AttentionDep模型，该模型通过融合上下文和领域知识来驱动可解释的抑郁严重程度估计。帖子通过单克隆和双克隆逐层编码，并使用注意力机制突出显示临床相关的单词。通过交叉注意力机制整合精心构建的心理健康知识图谱领域的知识，丰富了上下文特征。最后，使用一个尊重临床相关性和自然严重程度级别顺序的序数回归框架来预测抑郁严重程度。我们的实验表明，AttentionDep在多个数据集上的分级F1分数上比最先进的基线方法高5%以上，并提供了对其预测的可解释洞见。这项工作促进了从社交媒体进行心理健康评估的可信和透明AI系统的开发。', 'title_zh': 'AttentionDep: 域aware注意力机制用于抑郁严重程度可解释评估'}
{'arxiv_id': 'arXiv:2510.00689', 'title': 'Relevance-Zone Reduction in Game Solving', 'authors': 'Chi-Huang Lin, Ting Han Wei, Chun-Jui Wang, Hung Guei, Chung-Chin Shih, Yun-Jui Tsai, I-Chen Wu, Ti-Rong Wu', 'link': 'https://arxiv.org/abs/2510.00689', 'abstract': 'Game solving aims to find the optimal strategies for all players and determine the theoretical outcome of a game. However, due to the exponential growth of game trees, many games remain unsolved, even though methods like AlphaZero have demonstrated super-human level in game playing. The Relevance-Zone (RZ) is a local strategy reuse technique that restricts the search to only the regions relevant to the outcome, significantly reducing the search space. However, RZs are not unique. Different solutions may result in RZs of varying sizes. Smaller RZs are generally more favorable, as they increase the chance of reuse and improve pruning efficiency. To this end, we propose an iterative RZ reduction method that repeatedly solves the same position while gradually restricting the region involved, guiding the solver toward smaller RZs. We design three constraint generation strategies and integrate an RZ Pattern Table to fully leverage past solutions. In experiments on 7x7 Killall-Go, our method reduces the average RZ size to 85.95% of the original. Furthermore, the reduced RZs can be permanently stored as reusable knowledge for future solving tasks, especially for larger board sizes or different openings.', 'abstract_zh': '游戏求解旨在找到所有玩家的最优策略并确定游戏的理论结果。然而，由于游戏树的指数增长，很多游戏仍处于未解状态，尽管像AlphaZero这样的方法已经在游戏对弈中展现了超人类水平。相关性区域（RZ）是一种局部策略重用技术，它将搜索限制在对结果相关的区域，显著减少了搜索空间。然而，RZ并不唯一。不同的解决方案可能导致不同大小的RZ。较小的RZ通常更为有利，因为它们增加了重用的机会并提高了剪枝效率。为此，我们提出了一种迭代的RZ缩减方法，该方法在每次解决相同位置时逐步限制涉及的区域，引导求解器趋向于更小的RZ。我们设计了三种约束生成策略并集成了一个RZ模式表，以充分利用以往的解决方案。在7x7 Killall-Go的实验中，我们的方法将平均RZ大小减少了85.95%。此外，缩减后的RZ可以被永久存储为可重复使用的知识，特别适用于更大棋盘大小或不同开局的解题任务。', 'title_zh': '游戏求解中的相关性区域缩减'}
{'arxiv_id': 'arXiv:2510.00664', 'title': 'Batch-CAM: Introduction to better reasoning in convolutional deep learning models', 'authors': 'Giacomo Ignesti, Davide Moroni, Massimo Martinelli', 'link': 'https://arxiv.org/abs/2510.00664', 'abstract': 'Understanding the inner workings of deep learning models is crucial for advancing artificial intelligence, particularly in high-stakes fields such as healthcare, where accurate explanations are as vital as precision. This paper introduces Batch-CAM, a novel training paradigm that fuses a batch implementation of the Grad-CAM algorithm with a prototypical reconstruction loss. This combination guides the model to focus on salient image features, thereby enhancing its performance across classification tasks. Our results demonstrate that Batch-CAM achieves a simultaneous improvement in accuracy and image reconstruction quality while reducing training and inference times. By ensuring models learn from evidence-relevant information,this approach makes a relevant contribution to building more transparent, explainable, and trustworthy AI systems.', 'abstract_zh': '理解深度学习模型的内部工作机制对于促进人工智能的发展至关重要，尤其是在如医疗健康这样高 stakes 的领域，准确的解释与精确性同样重要。本文提出了一种新的训练范式 Batch-CAM，它将批处理实现的 Grad-CAM 算法与原型重建损失相结合。这种结合引导模型关注关键图像特征，从而在分类任务中提升其性能。我们的结果表明，Batch-CAM 同时提高了分类准确率和图像重建质量，并缩短了训练和推理时间。通过确保模型从相关证据中学习，这种方法为构建更透明、可解释和值得信赖的 AI 系统做出了相关贡献。', 'title_zh': '批量CAM：介绍在卷积深度学习模型中更好地进行推理的方法'}
{'arxiv_id': 'arXiv:2510.00620', 'title': 'HARPA: A Testability-Driven, Literature-Grounded Framework for Research Ideation', 'authors': 'Rosni Vasu, Peter Jansen, Pao Siangliulue, Cristina Sarasua, Abraham Bernstein, Peter Clark, Bhavana Dalvi Mishra', 'link': 'https://arxiv.org/abs/2510.00620', 'abstract': "While there has been a surge of interest in automated scientific discovery (ASD), especially with the emergence of LLMs, it remains challenging for tools to generate hypotheses that are both testable and grounded in the scientific literature. Additionally, existing ideation tools are not adaptive to prior experimental outcomes. We developed HARPA to address these challenges by incorporating the ideation workflow inspired by human researchers. HARPA first identifies emerging research trends through literature mining, then explores hypothesis design spaces, and finally converges on precise, testable hypotheses by pinpointing research gaps and justifying design choices. Our evaluations show that HARPA-generated hypothesis-driven research proposals perform comparably to a strong baseline AI-researcher across most qualitative dimensions (e.g., specificity, novelty, overall quality), but achieve significant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness (+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the ASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11 out of 40) and fewer failures (16 vs. 21 out of 40), showing that expert feasibility judgments track with actual execution success. Furthermore, to simulate how researchers continuously refine their understanding of what hypotheses are both testable and potentially interesting from experience, HARPA learns a reward model that scores new hypotheses based on prior experimental outcomes, achieving approx. a 28\\% absolute gain over HARPA's untrained baseline scorer. Together, these methods represent a step forward in the field of AI-driven scientific discovery.", 'abstract_zh': '自动化科学发现中的HARPA：应对假设生成挑战的新型工具', 'title_zh': 'HARPA：一种以测试性驱动、文献为基础的研究构想框架'}
{'arxiv_id': 'arXiv:2510.00552', 'title': 'Data Quality Challenges in Retrieval-Augmented Generation', 'authors': 'Leopold Müller, Joshua Holstein, Sarah Bause, Gerhard Satzger, Niklas Kühl', 'link': 'https://arxiv.org/abs/2510.00552', 'abstract': 'Organizations increasingly adopt Retrieval-Augmented Generation (RAG) to enhance Large Language Models with enterprise-specific knowledge. However, current data quality (DQ) frameworks have been primarily developed for static datasets, and only inadequately address the dynamic, multi-stage nature of RAG systems. This study aims to develop DQ dimensions for this new type of AI-based systems. We conduct 16 semi-structured interviews with practitioners of leading IT service companies. Through a qualitative content analysis, we inductively derive 15 distinct DQ dimensions across the four processing stages of RAG systems: data extraction, data transformation, prompt & search, and generation. Our findings reveal that (1) new dimensions have to be added to traditional DQ frameworks to also cover RAG contexts; (2) these new dimensions are concentrated in early RAG steps, suggesting the need for front-loaded quality management strategies, and (3) DQ issues transform and propagate through the RAG pipeline, necessitating a dynamic, step-aware approach to quality management.', 'abstract_zh': '组织越来越多地采用检索增强生成（RAG）来增强大型语言模型的企业特定知识。然而，当前的数据质量（DQ）框架主要为静态数据集开发，仅不充分地解决了RAG系统的动态、多阶段性质。本研究旨在为这种新型的基于AI的系统开发DQ维度。我们对主要IT服务公司的从业人员进行了16次半结构化访谈，并通过定性内容分析，归纳出15个跨RAG系统四个处理阶段的数据质量维度：数据提取、数据转换、提示与搜索以及生成。我们的发现表明：（1）需要向传统的DQ框架添加新的维度以涵盖RAG背景；（2）这些新维度集中在RAG的早期步骤，表明需要前置的数据质量管理策略；（3）DQ问题在整个RAG管道中变换和传播，需要采用动态的、阶段意识的数据质量管理方法。', 'title_zh': '检索增强生成中的数据质量挑战'}
{'arxiv_id': 'arXiv:2510.00436', 'title': 'Automated Evaluation can Distinguish the Good and Bad AI Responses to Patient Questions about Hospitalization', 'authors': 'Sarvesh Soni, Dina Demner-Fushman', 'link': 'https://arxiv.org/abs/2510.00436', 'abstract': 'Automated approaches to answer patient-posed health questions are rising, but selecting among systems requires reliable evaluation. The current gold standard for evaluating the free-text artificial intelligence (AI) responses--human expert review--is labor-intensive and slow, limiting scalability. Automated metrics are promising yet variably aligned with human judgments and often context-dependent. To address the feasibility of automating the evaluation of AI responses to hospitalization-related questions posed by patients, we conducted a large systematic study of evaluation approaches. Across 100 patient cases, we collected responses from 28 AI systems (2800 total) and assessed them along three dimensions: whether a system response (1) answers the question, (2) appropriately uses clinical note evidence, and (3) uses general medical knowledge. Using clinician-authored reference answers to anchor metrics, automated rankings closely matched expert ratings. Our findings suggest that carefully designed automated evaluation can scale comparative assessment of AI systems and support patient-clinician communication.', 'abstract_zh': '自动回答患者提出健康问题的方法正在兴起，但在多种系统中进行选择需要可靠的评估。目前评估自由文本人工智能（AI）响应的标准——人类专家审查——耗费人力且速度慢，限制了其可扩展性。自动化指标前景广阔但与人类判断的契合度不一，且常依赖于具体情境。为解决自动化评估医院化相关患者提问的AI响应可行性问题，我们进行了大规模的评估方法系统研究。在100个患者案例中，我们收集了28个AI系统（总计2800个响应）的回复，并从三个维度对其进行评估：（1）系统响应是否回答了问题，（2）是否恰当地使用了临床笔记证据，以及（3）是否运用了通用医学知识。利用临床专家撰写的参考答案作为指标的基础，自动排名与专家评分高度一致。我们的发现表明，精心设计的自动化评估可以实现AI系统的比较评估，并支持患者与临床医生之间的沟通。', 'title_zh': '自动评估可以区分AI对患者关于住院问题的回答好坏'}
{'arxiv_id': 'arXiv:2510.00381', 'title': 'Semantic-Driven AI Agent Communications: Challenges and Solutions', 'authors': 'Kaiwen Yu, Mengying Sun, Zhijin Qin, Xiaodong Xu, Ping Yang, Yue Xiao, Gang Wu', 'link': 'https://arxiv.org/abs/2510.00381', 'abstract': 'With the rapid growth of intelligent services, communication targets are shifting from humans to artificial intelligent (AI) agents, which require new paradigms to enable real-time perception, decision-making, and collaboration. Semantic communication, which conveys task-relevant meaning rather than raw data, offers a promising solution. However, its practical deployment remains constrained by dynamic environments and limited resources. To address these issues, this article proposes a semantic-driven AI agent communication framework and develops three enabling techniques. First, semantic adaptation transmission applies fine-tuning with real or generative samples to efficiently adapt models to varying environments. Second, semantic lightweight transmission incorporates pruning, quantization, and perception-aware sampling to reduce model complexity and alleviate computational burden on edge agents. Third, semantic self-evolution control employs distributed hierarchical decision-making to optimize multi-dimensional resources, enabling robust multi-agent collaboration in dynamic environments. Simulation results show that the proposed solutions achieve faster convergence and stronger robustness, while the proposed distributed hierarchical optimization method significantly outperforms conventional decision-making schemes, highlighting its potential for AI agent communication networks.', 'abstract_zh': '智能服务快速发展背景下，通信目标从人类转向人工智能（AI）代理，这需要新的 paradigms 以支持实时感知、决策和协作。基于语义的通信通过传达任务相关的意义而非原始数据，提供了有希望的解决方案。然而，其实用部署仍受到动态环境和有限资源的限制。为解决这些问题，本文提出了一种基于语义的 AI 代理通信框架，并开发了三种使能技术。首先，语义自适应传输通过使用真实或生成样本进行微调，以高效适应不同环境。其次，语义轻量级传输结合剪枝、量化和感知自适应采样来降低模型复杂度并缓解边缘代理的计算负担。第三，语义自我进化控制采用分布式分层决策制定以优化多维资源，在动态环境中实现稳健的多代理协作。仿真结果表明，所提出的方法实现了更快的收敛和更强的鲁棒性，而提出的分布式分层优化方法显著优于传统决策方案，突显了其在 AI 代理通信网络中的潜在优势。', 'title_zh': '基于语义的AI代理通信：挑战与解决方案'}
{'arxiv_id': 'arXiv:2510.00355', 'title': 'Hierarchical Reasoning Model: A Critical Supplementary Material', 'authors': 'Renee Ge, Qianli Liao, Tomaso Poggio', 'link': 'https://arxiv.org/abs/2510.00355', 'abstract': 'Transformers have demonstrated remarkable performance in natural language processing and related domains, as they largely focus on sequential, autoregressive next-token prediction tasks. Yet, they struggle in logical reasoning, not necessarily because of a fundamental limitation of these models, but possibly due to the lack of exploration of more creative uses, such as latent space and recurrent reasoning. An emerging exploration in this direction is the Hierarchical Reasoning Model (Wang et al., 2025), which introduces a novel type of recurrent reasoning in the latent space of transformers, achieving remarkable performance on a wide range of 2D reasoning tasks. Despite the promising results, this line of models is still at an early stage and calls for in-depth investigation. In this work, we perform a critical review on this class of models, examine key design choices and present intriguing variants that achieve significantly better performance on the Sudoku-Extreme and Maze-Hard tasks than previously reported. Our results also raise surprising observations and intriguing directions for further research.', 'abstract_zh': '变换器在自然语言处理及相关领域展示了卓越性能，主要是因为它们主要关注序贯的自回归下一个词预测任务。然而，在逻辑推理方面，它们面临挑战，这并不是因为这些模型本身存在根本限制，而是可能由于缺乏更具创意的应用探索，如潜在空间和递归推理。这一方向上的一项新兴探索是层次推理模型（Wang et al., 2025），它在变换器的潜在空间中引入了一种新颖的递归推理方式，实现了广泛2D推理任务的卓越性能。尽管取得了一定的成果，但这类模型仍处于早期阶段，需要深入研究。在这项工作中，我们对这类模型进行了关键性回顾，检查了关键设计选择，并展示了在数独极难和迷宫极难任务上显著优于先前报告的有趣变体。我们的结果还提出了进一步研究中令人惊讶的发现和有趣的方向。', 'title_zh': '层次推理模型：关键补充材料'}
{'arxiv_id': 'arXiv:2510.00332', 'title': 'When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets', 'authors': 'Zeshi Dai, Zimo Peng, Zerui Cheng, Ryan Yihe Li', 'link': 'https://arxiv.org/abs/2510.00332', 'abstract': 'We present CAIA, a benchmark exposing a critical blind spot in AI evaluation: the inability of state-of-the-art models to operate in adversarial, high-stakes environments where misinformation is weaponized and errors are irreversible. While existing benchmarks measure task completion in controlled settings, real-world deployment demands resilience against active deception. Using crypto markets as a testbed where $30 billion was lost to exploits in 2024, we evaluate 17 models on 178 time-anchored tasks requiring agents to distinguish truth from manipulation, navigate fragmented information landscapes, and make irreversible financial decisions under adversarial pressure.\nOur results reveal a fundamental capability gap: without tools, even frontier models achieve only 28% accuracy on tasks junior analysts routinely handle. Tool augmentation improves performance but plateaus at 67.4% versus 80% human baseline, despite unlimited access to professional resources. Most critically, we uncover a systematic tool selection catastrophe: models preferentially choose unreliable web search over authoritative data, falling for SEO-optimized misinformation and social media manipulation. This behavior persists even when correct answers are directly accessible through specialized tools, suggesting foundational limitations rather than knowledge gaps. We also find that Pass@k metrics mask dangerous trial-and-error behavior for autonomous deployment.\nThe implications extend beyond crypto to any domain with active adversaries, e.g. cybersecurity, content moderation, etc. We release CAIA with contamination controls and continuous updates, establishing adversarial robustness as a necessary condition for trustworthy AI autonomy. The benchmark reveals that current models, despite impressive reasoning scores, remain fundamentally unprepared for environments where intelligence must survive active opposition.', 'abstract_zh': 'CAIA：揭示AI评估关键盲点的基准测试', 'title_zh': '当幻觉成本以百万计： adversarial 财经市场中 AI 代理的基准测试'}
{'arxiv_id': 'arXiv:2510.00274', 'title': 'MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning', 'authors': 'Maisha Maliha, Dean Hougen', 'link': 'https://arxiv.org/abs/2510.00274', 'abstract': 'Understanding the decision-making process of Deep Reinforcement Learning agents remains a key challenge for deploying these systems in safety-critical and multi-agent environments. While prior explainability methods like StateMask, have advanced the identification of critical states, they remain limited by computational cost, exploration coverage, and lack of adaptation to multi-agent settings. To overcome these limitations, we propose a mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent Collaboration with Mask-Based Explainability for Reinforcement Learning), that extends perturbation-based explanation to Multi-Agent Reinforcement Learning. Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy exploration, and lightweight inter-agent collaboration to share masked state information and peer experience. This collaboration enables each agent to perform saliency-guided masking and share reward-based insights with peers, reducing the time required for critical state discovery, improving explanation fidelity, and leading to faster and more robust learning. The core novelty of our approach lies in generalizing explainability from single-agent to multi-agent systems through a unified mathematical formalism built on trajectory perturbation, reward fidelity analysis, and Kullback-Leibler divergence regularization. This framework yields localized, interpretable explanations grounded in probabilistic modeling and multi-agent Markov decision processes. We validate our framework on both single-agent and multi-agent benchmarks, including a multi-agent highway driving environment and Google Research Football, demonstrating that MAGIC-MASK consistently outperforms state-of-the-art baselines in fidelity, learning efficiency, and policy robustness while offering interpretable and transferable explanations.', 'abstract_zh': '理解深度强化学习代理的决策过程仍然是将这些系统应用于关键安全和多代理环境中的一个关键挑战。为了克服先前方法如StateMask所面临的计算成本、探索覆盖范围以及不适应多代理设置的局限，我们提出了一种基于数学原理的框架MAGIC-MASK（多代理引导的跨代理协作及基于掩码的强化学习解释方法），该框架将扰动解释扩展到了多代理强化学习中。该方法结合了接近策略优化、自适应ε-贪婪探索以及轻量级的代理间协作，以共享掩码状态信息和同伴体验。这种协作使每个代理能够进行显著性指导的掩码，并与同伴分享基于奖励的见解，从而减少关键状态的发现时间，提高解释的准确度，并加快和增强学习过程。我们的方法的核心创新之处在于通过基于轨迹扰动、奖励保真分析和Kullback-Leibler散度正则化的统一数学形式，将解释性从单代理系统推广到了多代理系统。该框架提供了基于概率建模和多代理马尔可夫决策过程的局部可解释性解释。我们在单代理和多代理基准测试上验证了该框架，包括多代理高速公路驾驶环境和Google Research Football，结果显示MAGIC-MASK在准确度、学习效率和策略稳健性方面均优于现有最先进的基线方法，同时提供了可解释性和可传递性解释。', 'title_zh': 'MAGIC-MASK：基于掩码解释的多Agent引导跨Agent协作强化学习'}
{'arxiv_id': 'arXiv:2510.00186', 'title': 'Thinkquel: A Model Dedicated to Text-to-dbt Using Synthetic Data and a Span-Aware Objective', 'authors': 'Anni Li, Aria Attar, Paul Dong', 'link': 'https://arxiv.org/abs/2510.00186', 'abstract': 'Transforming natural-language requests into reliable, production-ready data transformations remains challenging: correctness depends on precise schema linking and warehouse-specific SQL dialects, while the strongest supervision available during training--execution success and result matching--are provided only at the sequence level. At the same time, assembling large, execution-validated corpora is costly, and token-level objectives misalign with these global signals, yielding unstable optimization and limited portability. We introduce Thinkquel, a fine-tuned model for producing robust, portable, and execution-validated database queries. Methodologies in Thinkquel integrates a novel synthetic data pipeline, TS-SQL, that leverages dbt as a portable intermediate representation with a span-aware reinforcement learning objective, and Token-Sequence GRPO (TS-GRPO), specifically designed to bridge the gap between token-level training signals and sequence-level execution rewards when finetuning LLMs. On the 500-example TS-SQL test set, Thinkquel (32B) reaches 93.2\\% execution success and 61.8\\% exact-result match with a two-stage SFT curriculum, improving over the base model by 67.2\\% (exec.) and 44.4\\% (match). In Spider (14B) experiments, TS-GRPO increases training stability and speeds convergence of the execution-match reward relative to GRPO and GSPO.', 'abstract_zh': '将自然语言请求转化为可靠且生产-ready的数据转换仍然具有挑战性：正确性依赖于精确的模式链接和特定于数据仓库的SQL方言，而可用的最强监督——执行成功和结果匹配——仅提供在序列级别。同时，构建大型、执行验证的语料库成本高昂，而基于token级别的目标与这些全局信号不一致，导致优化不稳定且适用性有限。我们引入Thinkquel，一种细调模型，用于生成稳健、可移植且执行验证的数据库查询。Thinkquel中的方法结合了新型合成数据管道TS-SQL，利用dbt作为可移植的中间表示，并采用基于跨度感知的强化学习目标，以及专门设计的Token-Sequence GRPO（TS-GRPO），用于在微调LLM时弥合基于token级别的训练信号和基于序列级别的执行奖励之间的差距。在500个示例TS-SQL测试集上，Thinkquel（32B）通过两阶段SFT课程达到93.2%的执行成功率和61.8%的确切结果匹配率，分别比基础模型提高了67.2%（执行）和44.4%（匹配）。在Spider（14B）实验中，TS-GRPO相对于GRPO和GSPO提高了训练的稳定性并加快了执行匹配奖励的收敛速度。', 'title_zh': 'Thinkquel：一个专用的文本到对话模型，采用合成数据和区间感知目标'}
{'arxiv_id': 'arXiv:2510.00185', 'title': 'Object-Centric Case-Based Reasoning via Argumentation', 'authors': 'Gabriel de Olim Gaul, Adam Gould, Avinash Kori, Francesca Toni', 'link': 'https://arxiv.org/abs/2510.00185', 'abstract': 'We introduce Slot Attention Argumentation for Case-Based Reasoning (SAA-CBR), a novel neuro-symbolic pipeline for image classification that integrates object-centric learning via a neural Slot Attention (SA) component with symbolic reasoning conducted by Abstract Argumentation for Case-Based Reasoning (AA-CBR). We explore novel integrations of AA-CBR with the neural component, including feature combination strategies, casebase reduction via representative samples, novel count-based partial orders, a One-Vs-Rest strategy for extending AA-CBR to multi-class classification, and an application of Supported AA-CBR, a bipolar variant of AA-CBR. We demonstrate that SAA-CBR is an effective classifier on the CLEVR-Hans datasets, showing competitive performance against baseline models.', 'abstract_zh': 'Slot 注意论辩案例基推理 (SAA-CBR) 用于图像分类的新型神经符号管道', 'title_zh': '基于论据的对象中心案例推理'}
{'arxiv_id': 'arXiv:2510.00156', 'title': 'AuditAgent: Expert-Guided Multi-Agent Reasoning for Cross-Document Fraudulent Evidence Discovery', 'authors': 'Songran Bai, Bingzhe Wu, Yiwei Zhang, Chengke Wu, Xiaolong Zheng, Yaze Yuan, Ke Wu, Jianqiang Li', 'link': 'https://arxiv.org/abs/2510.00156', 'abstract': 'Financial fraud detection in real-world scenarios presents significant challenges due to the subtlety and dispersion of evidence across complex, multi-year financial disclosures. In this work, we introduce a novel multi-agent reasoning framework AuditAgent, enhanced with auditing domain expertise, for fine-grained evidence chain localization in financial fraud cases. Leveraging an expert-annotated dataset constructed from enforcement documents and financial reports released by the China Securities Regulatory Commission, our approach integrates subject-level risk priors, a hybrid retrieval strategy, and specialized agent modules to efficiently identify and aggregate cross-report evidence. Extensive experiments demonstrate that our method substantially outperforms General-Purpose Agent paradigm in both recall and interpretability, establishing a new benchmark for automated, transparent financial forensics. Our results highlight the value of domain-specific reasoning and dataset construction for advancing robust financial fraud detection in practical, real-world regulatory applications.', 'abstract_zh': '现实世界场景中的财务欺诈检测由于证据在复杂多年的财务披露中具有细微性和分散性而面临重大挑战。本文介绍了一个增强审计专业知识的新型多智能体推理框架AuditAgent，用于财务欺诈案件中的细粒度证据链定位。通过利用由中国证券监督管理委员会发布的执法文件和财务报告构建的专家标注数据集，我们的方法结合了主题级风险先验、混合检索策略和专门的智能体模块，以高效地识别和聚合跨报告证据。大量实验表明，我们的方法在召回率和可解释性上显著优于通用智能体 paradigm，建立了自动化、透明财务 forensic 的新基准。我们的结果突显了在实际现实世界监管应用中推进稳健财务欺诈检测的价值，特别是在专业领域推理和数据集构建方面。', 'title_zh': 'AuditAgent: 专家引导的多代理跨文档欺诈证据推理'}
{'arxiv_id': 'arXiv:2510.00084', 'title': 'Towards a Framework for Supporting the Ethical and Regulatory Certification of AI Systems', 'authors': 'Fabian Kovac, Sebastian Neumaier, Timea Pahi, Torsten Priebe, Rafael Rodrigues, Dimitrios Christodoulou, Maxime Cordy, Sylvain Kubler, Ali Kordia, Georgios Pitsiladis, John Soldatos, Petros Zervoudakis', 'link': 'https://arxiv.org/abs/2510.00084', 'abstract': "Artificial Intelligence has rapidly become a cornerstone technology, significantly influencing Europe's societal and economic landscapes. However, the proliferation of AI also raises critical ethical, legal, and regulatory challenges. The CERTAIN (Certification for Ethical and Regulatory Transparency in Artificial Intelligence) project addresses these issues by developing a comprehensive framework that integrates regulatory compliance, ethical standards, and transparency into AI systems. In this position paper, we outline the methodological steps for building the core components of this framework. Specifically, we present: (i) semantic Machine Learning Operations (MLOps) for structured AI lifecycle management, (ii) ontology-driven data lineage tracking to ensure traceability and accountability, and (iii) regulatory operations (RegOps) workflows to operationalize compliance requirements. By implementing and validating its solutions across diverse pilots, CERTAIN aims to advance regulatory compliance and to promote responsible AI innovation aligned with European standards.", 'abstract_zh': '人工智能已成为核心技术，显著影响欧洲的社会和经济格局。然而，人工智能的广泛应用也引发了关键的伦理、法律和监管挑战。CERTAIN（伦理与监管透明度认证的人工智能）项目通过开发综合框架来应对这些问题，该框架将监管合规性、伦理标准和透明性整合到人工智能系统中。在本文中，我们概述了构建该框架核心组件的方法步骤。具体而言，我们介绍了：(i) 语义机器学习运营 (MLOps) 以实现结构化的人工智能生命周期管理，(ii) 基于本体的数据血缘追踪以确保可追溯性和问责制，以及(iii) 监管运营 (RegOps) 工作流以实现合规要求的操作化。通过在多样化的试点项目中实施和验证其解决方案，CERTAIN 目标在于推动符合欧洲标准的责任型人工智能创新。', 'title_zh': '面向支持人工智能系统伦理与监管认证的框架'}
{'arxiv_id': 'arXiv:2510.00075', 'title': 'NeurIPS should lead scientific consensus on AI policy', 'authors': 'Rishi Bommasani', 'link': 'https://arxiv.org/abs/2510.00075', 'abstract': "Designing wise AI policy is a grand challenge for society. To design such policy, policymakers should place a premium on rigorous evidence and scientific consensus. While several mechanisms exist for evidence generation, and nascent mechanisms tackle evidence synthesis, we identify a complete void on consensus formation. In this position paper, we argue NeurIPS should actively catalyze scientific consensus on AI policy. Beyond identifying the current deficit in consensus formation mechanisms, we argue that NeurIPS is the best option due its strengths and the paucity of compelling alternatives. To make progress, we recommend initial pilots for NeurIPS by distilling lessons from the IPCC's leadership to build scientific consensus on climate policy. We dispel predictable counters that AI researchers disagree too much to achieve consensus and that policy engagement is not the business of NeurIPS. NeurIPS leads AI on many fronts, and it should champion scientific consensus to create higher quality AI policy.", 'abstract_zh': '设计明智的AI政策是社会面临的重大挑战。为了设计这样的政策，政策制定者应高度重视严谨的证据和科学共识。尽管存在多种证据生成机制，且初步机制致力于证据综合，但我们发现共识形成机制存在重大缺失。在这一立场论文中，我们主张NeurIPS应积极催化AI政策的科学共识。除了指出当前共识形成机制的不足，我们认为NeurIPS是最合适的选择，因其优势显著且缺乏更具说服力的替代方案。为了取得进展，我们建议NeurIPS通过借鉴IPCC领导力建设气候变化政策科学共识的经验，开展初步试点。我们驳斥了AI研究人员分歧过大难以达成共识以及政策参与不属于NeurIPS职责范围的预期反对意见。NeurIPS在AI领域引领多项进展，它应当倡导科学共识，以创造更高质量的AI政策。', 'title_zh': 'NeURIPS 应引领AI政策的科学共识'}
{'arxiv_id': 'arXiv:2510.01178', 'title': 'COM-BOM: Bayesian Exemplar Search for Efficiently Exploring the Accuracy-Calibration Pareto Frontier', 'authors': 'Gaoxiang Luo, Aryan Deshwal', 'link': 'https://arxiv.org/abs/2510.01178', 'abstract': 'Selecting an optimal set of exemplars is critical for good performance of in-context learning. However, prior exemplar search methods narrowly optimize for predictive accuracy, critically neglecting model calibration--a key determinant of trustworthiness and safe deployment. In this paper, we formulate exemplar selection as a multi-objective optimization problem, explicitly targeting both the maximization of predictive accuracy and the minimization of expected calibration error. We solve this problem with a sample-efficient Combinatorial Bayesian Optimization algorithm (COM-BOM) to find the Pareto front that optimally trades off the two objectives of accuracy and calibration. We evaluate COM-BOM on multiple tasks from unsaturated MMLU-Pro benchmark and find that COM-BOM beats or matches the baselines at jointly optimizing the two objectives, while requiring a minimal number of LLM API calls.', 'abstract_zh': '选择最优示例集对于上下文学习的良好表现至关重要。然而，先前的示例搜索方法仅狭隘地优化预测准确性，严重忽视了模型校准——这是信任度和安全部署的关键决定因素。本文将示例选择形式化为一个多目标优化问题，明确针对预测准确性的最大化和期望校准误差的最小化。我们使用一种高效的组合贝叶斯优化算法（COM-BOM）来解决该问题，以找到在准确性和校准之间最优权衡的帕累托前沿。我们在未饱和MMLU-Pro基准上的多个任务上评估了COM-BOM，发现它能够在同时优化两个目标方面优于或匹配基线模型，同时仅需最少的LLM API调用。', 'title_zh': 'COM-BOM: 基于贝叶斯原型搜索的高效探索准确率校准帕多瓦前沿方法'}
{'arxiv_id': 'arXiv:2510.01169', 'title': 'Fiaingen: A financial time series generative method matching real-world data quality', 'authors': 'Jože M. Rožanec, Tina Žezlin, Laurentiu Vasiliu, Dunja Mladenić, Radu Prodan, Dumitru Roman', 'link': 'https://arxiv.org/abs/2510.01169', 'abstract': 'Data is vital in enabling machine learning models to advance research and practical applications in finance, where accurate and robust models are essential for investment and trading decision-making. However, real-world data is limited despite its quantity, quality, and variety. The data shortage of various financial assets directly hinders the performance of machine learning models designed to trade and invest in these assets. Generative methods can mitigate this shortage. In this paper, we introduce a set of novel techniques for time series data generation (we name them Fiaingen) and assess their performance across three criteria: (a) overlap of real-world and synthetic data on a reduced dimensionality space, (b) performance on downstream machine learning tasks, and (c) runtime performance. Our experiments demonstrate that the methods achieve state-of-the-art performance across the three criteria listed above. Synthetic data generated with Fiaingen methods more closely mirrors the original time series data while keeping data generation time close to seconds - ensuring the scalability of the proposed approach. Furthermore, models trained on it achieve performance close to those trained with real-world data.', 'abstract_zh': '金融领域中基于时间序列数据的生成方法在增强机器学习模型性能方面的研究与评估', 'title_zh': 'Fiaingen: 一种匹配实际数据质量的金融时间序列生成方法'}
{'arxiv_id': 'arXiv:2510.01136', 'title': 'TabINR: An Implicit Neural Representation Framework for Tabular Data Imputation', 'authors': 'Vincent Ochs, Florentin Bieder, Sidaty el Hadramy, Paul Friedrich, Stephanie Taha-Mehlitz, Anas Taha, Philippe C. Cattin', 'link': 'https://arxiv.org/abs/2510.01136', 'abstract': 'Tabular data builds the basis for a wide range of applications, yet real-world datasets are frequently incomplete due to collection errors, privacy restrictions, or sensor failures. As missing values degrade the performance or hinder the applicability of downstream models, and while simple imputing strategies tend to introduce bias or distort the underlying data distribution, we require imputers that provide high-quality imputations, are robust across dataset sizes and yield fast inference. We therefore introduce TabINR, an auto-decoder based Implicit Neural Representation (INR) framework that models tables as neural functions. Building on recent advances in generalizable INRs, we introduce learnable row and feature embeddings that effectively deal with the discrete structure of tabular data and can be inferred from partial observations, enabling instance adaptive imputations without modifying the trained model. We evaluate our framework across a diverse range of twelve real-world datasets and multiple missingness mechanisms, demonstrating consistently strong imputation accuracy, mostly matching or outperforming classical (KNN, MICE, MissForest) and deep learning based models (GAIN, ReMasker), with the clearest gains on high-dimensional datasets.', 'abstract_zh': '基于自动解码器的隐式神经表示（INR）框架TabINR：建模表格数据及其实例自适应插补方法', 'title_zh': 'TabINR: 一种用于表格式数据插补的隐式神经表示框架'}
{'arxiv_id': 'arXiv:2510.01052', 'title': 'Hybrid Dialogue State Tracking for Persian Chatbots: A Language Model-Based Approach', 'authors': 'Samin Mahdipour Aghabagher, Saeedeh Momtazi', 'link': 'https://arxiv.org/abs/2510.01052', 'abstract': 'Dialogue State Tracking (DST) is an essential element of conversational AI with the objective of deeply understanding the conversation context and leading it toward answering user requests. Due to high demands for open-domain and multi-turn chatbots, the traditional rule-based DST is not efficient enough, since it cannot provide the required adaptability and coherence for human-like experiences in complex conversations. This study proposes a hybrid DST model that utilizes rule-based methods along with language models, including BERT for slot filling and intent detection, XGBoost for intent validation, GPT for DST, and online agents for real-time answer generation. This model is uniquely designed to be evaluated on a comprehensive Persian multi-turn dialogue dataset and demonstrated significantly improved accuracy and coherence over existing methods in Persian-based chatbots. The results demonstrate how effectively a hybrid approach may improve DST capabilities, paving the way for conversational AI systems that are more customized, adaptable, and human-like.', 'abstract_zh': '对话状态跟踪(DST)是会话AI的核心组成部分，旨在深入理解对话背景并引导对话以回答用户请求。由于对开放式和多轮聊天机器人的高需求，传统的基于规则的DST不足以提供复杂对话中所需的适应性和连贯性，以实现类似人类的体验。本研究提出了一种混合DST模型，该模型结合了基于规则的方法和语言模型，包括BERT用于槽填充和意图检测、XGBoost用于意图验证、GPT用于DST，以及在线代理用于实时答案生成。该模型专门设计用于在全面的波斯语多轮对话数据集上进行评估，并证明在波斯语基于聊天机器人中具有显著更高的准确性和连贯性。结果展示了混合方法如何有效提升DST能力，为更个性化、适应性强且类似人类的会话AI系统铺平了道路。', 'title_zh': '基于语言模型的混合对话状态跟踪方法：以波斯聊天机器人为例'}
{'arxiv_id': 'arXiv:2510.01047', 'title': 'Authentic Discrete Diffusion Model', 'authors': 'Xiao Li, Jiaqi Zhang, Shuxiang Zhang, Tianshui Chen, Liang Lin, Guangrun Wang', 'link': 'https://arxiv.org/abs/2510.01047', 'abstract': 'We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally redefines prior pseudo-discrete approaches by preserving core diffusion characteristics directly in the one-hot space through a suite of coordinated mechanisms. Unlike conventional "pseudo" discrete diffusion (PDD) methods, ADD reformulates the diffusion input by directly using float-encoded one-hot class data, without relying on diffusing in the continuous latent spaces or masking policies. At its core, a timestep-conditioned cross-entropy loss is introduced between the diffusion model\'s outputs and the original one-hot labels. This synergistic design establishes a bridge between discriminative and generative learning. Our experiments demonstrate that ADD not only achieves superior performance on classification tasks compared to the baseline, but also exhibits excellent text generation capabilities on Image captioning. Extensive ablations validate the measurable gains of each component.', 'abstract_zh': '我们提出了一种真实性离散扩散（ADD）框架，该框架通过一系列协调机制，直接在独热空间中保留核心扩散特性，从根本上重新定义了先前三伪离散方法。与传统的“伪”离散扩散（PDD）方法不同，ADD 通过直接使用浮点编码的独热类别数据重新形式化扩散输入，而不依赖于在连续潜在空间中扩散或使用遮罩策略。核心上，引入了条件时间步长交叉熵损失，该损失在扩散模型的输出和原始独热标签之间建立桥梁。这种协同设计建立了辨别学习与生成学习之间的联系。实验结果表明，ADD 不仅在分类任务上的性能优于基线，还具备出色的图像 Caption 生成能力。广泛的消融实验验证了每个组件的可衡量改进。', 'title_zh': '真实离散扩散模型'}
{'arxiv_id': 'arXiv:2510.01020', 'title': 'The Good, the Bad, and the Sampled: a No-Regret Approach to Safe Online Classification', 'authors': 'Tavor Z. Baharav, Spyros Dragazis, Aldo Pacchiano', 'link': 'https://arxiv.org/abs/2510.01020', 'abstract': "We study the problem of sequentially testing individuals for a binary disease outcome whose true risk is governed by an unknown logistic model. At each round, a patient arrives with feature vector $x_t$, and the decision maker may either pay to administer a (noiseless) diagnostic test--revealing the true label--or skip testing and predict the patient's disease status based on their feature vector and prior history. Our goal is to minimize the total number of costly tests required while guaranteeing that the fraction of misclassifications does not exceed a prespecified error tolerance $\\alpha$, with probability at least $1-\\delta$. To address this, we develop a novel algorithm that interleaves label-collection and distribution estimation to estimate both $\\theta^{*}$ and the context distribution $P$, and computes a conservative, data-driven threshold $\\tau_t$ on the logistic score $|x_t^\\top\\theta|$ to decide when testing is necessary. We prove that, with probability at least $1-\\delta$, our procedure does not exceed the target misclassification rate, and requires only $O(\\sqrt{T})$ excess tests compared to the oracle baseline that knows both $\\theta^{*}$ and the patient feature distribution $P$. This establishes the first no-regret guarantees for error-constrained logistic testing, with direct applications to cost-sensitive medical screening. Simulations corroborate our theoretical guarantees, showing that in practice our procedure efficiently estimates $\\theta^{*}$ while retaining safety guarantees, and does not require too many excess tests.", 'abstract_zh': '我们研究了一个二元疾病结果个体的序贯测试问题，其真实风险由未知的逻辑模型支配。在每一轮中，一名患者带着特征向量 $x_t$ 到达，决策者可以选择支付进行无噪声诊断测试（揭示真实标签）或跳过测试并基于特征向量和过往历史预测患者的疾病状态。我们的目标是在最小化昂贵测试总数的同时，确保误分类的比例不超过事先指定的误差容忍度 $\\alpha$，且概率不低于 $1-\\delta$。为此，我们提出了一种新颖的算法，交替进行标签收集和分布估计，以估计 $\\theta^{*}$ 和上下文分布 $P$，并基于逻辑评分 $|x_t^\\top\\theta|$ 计算保守的数据驱动阈值 $\\tau_t$ 以决定何时进行测试。我们证明，以概率不低于 $1-\\delta$ 的概率，我们的流程不会超过目标误分类率，并且相比于知道 $\\theta^{*}$ 和患者特征分布 $P$ 的理想基准，仅需要 $O(\\sqrt{T})$ 的额外测试。这首次为误差约束逻辑测试建立了无遗憾保证，直接应用于成本敏感医疗筛查。模拟结果验证了我们的理论保证，显示我们的流程在实践中能够有效地估计 $\\theta^{*}$ 同时保留安全保证，并不需要太多额外的测试。', 'title_zh': '好的、坏的和抽样的：一种无悔的在线分类安全方法'}
{'arxiv_id': 'arXiv:2510.00966', 'title': 'Deep Learning-Based Approach for Improving Relational Aggregated Search', 'authors': 'Sara Saad Soliman, Ahmed Younes, Islam Elkabani, Ashraf Elsayed', 'link': 'https://arxiv.org/abs/2510.00966', 'abstract': 'Due to an information explosion on the internet, there is a need for the development of aggregated search systems that can boost the retrieval and management of content in various formats. To further improve the clustering of Arabic text data in aggregated search environments, this research investigates the application of advanced natural language processing techniques, namely stacked autoencoders and AraBERT embeddings. By transcending the limitations of traditional search engines, which are imprecise, not contextually relevant, and not personalized, we offer more enriched, context-aware characterizations of search results, so we used a K-means clustering algorithm to discover distinctive features and relationships in these results, we then used our approach on different Arabic queries to evaluate its effectiveness. Our model illustrates that using stacked autoencoders in representation learning suits clustering tasks and can significantly improve clustering search results. It also demonstrates improved accuracy and relevance of search results.', 'abstract_zh': '由于互联网上的信息爆炸，需要开发聚合搜索系统以提升各类格式内容的检索和管理。为进一步提高聚合搜索环境中阿拉伯文本数据的聚类效果，本研究探讨了高级自然语言处理技术，即堆叠自动编码器和AraBERT嵌入的应用。通过超越传统搜索引擎的精度不足、缺乏上下文相关性和个性化的问题，我们提供了更加丰富和上下文相关的检索结果表征，利用K-means聚类算法发现这些结果的独特特征和关系，并在不同阿拉伯查询上评估了该方法的有效性。我们的模型说明了在表示学习中使用堆叠自动编码器适合聚类任务，并能显著提高聚类搜索结果的质量，同时也展示了检索结果的准确性和相关性得到了提升。', 'title_zh': '基于深度学习的方法以提升关系聚合搜索'}
{'arxiv_id': 'arXiv:2510.00956', 'title': 'Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning', 'authors': 'Carlos Güemes-Palau, Miquel Ferriol-Galmés, Jordi Paillisse-Vilanova, Albert López-Brescó, Pere Barlet-Ros, Albert Cabellos-Aparicio', 'link': 'https://arxiv.org/abs/2510.00956', 'abstract': 'Machine Learning (ML)-based network models provide fast and accurate predictions for complex network behaviors but require substantial training data. Collecting such data from real networks is often costly and limited, especially for critical scenarios like failures. As a result, researchers commonly rely on simulated data, which reduces accuracy when models are deployed in real environments. We propose a hybrid approach leveraging transfer learning to combine simulated and real-world data. Using RouteNet-Fermi, we show that fine-tuning a pre-trained model with a small real dataset significantly improves performance. Our experiments with OMNeT++ and a custom testbed reduce the Mean Absolute Percentage Error (MAPE) in packet delay prediction by up to 88%. With just 10 real scenarios, MAPE drops by 37%, and with 50 scenarios, by 48%.', 'abstract_zh': '基于迁移学习的混合数据方法在混合仿真与实网数据中的应用：提高网络行为预测性能', 'title_zh': '使用迁移学习弥合模拟网络数据与真实网络数据之间的差距'}
{'arxiv_id': 'arXiv:2510.00909', 'title': '"We are not Future-ready": Understanding AI Privacy Risks and Existing Mitigation Strategies from the Perspective of AI Developers in Europe', 'authors': 'Alexandra Klymenko, Stephen Meisenbacher, Patrick Gage Kelley, Sai Teja Peddinti, Kurt Thomas, Florian Matthes', 'link': 'https://arxiv.org/abs/2510.00909', 'abstract': 'The proliferation of AI has sparked privacy concerns related to training data, model interfaces, downstream applications, and more. We interviewed 25 AI developers based in Europe to understand which privacy threats they believe pose the greatest risk to users, developers, and businesses and what protective strategies, if any, would help to mitigate them. We find that there is little consensus among AI developers on the relative ranking of privacy risks. These differences stem from salient reasoning patterns that often relate to human rather than purely technical factors. Furthermore, while AI developers are aware of proposed mitigation strategies for addressing these risks, they reported minimal real-world adoption. Our findings highlight both gaps and opportunities for empowering AI developers to better address privacy risks in AI.', 'abstract_zh': 'AI的发展引发了对训练数据、模型接口、下游应用等方面的隐私担忧：我们采访了25名基于欧洲的AI开发者，以了解他们认为对用户、开发者和企业构成最大风险的隐私威胁，并探讨是否有防护策略能够缓解这些风险。我们发现，AI开发者在隐私风险的相对排名上缺乏共识。这些差异源于显著的原因模式，往往与人为因素而非纯粹的技术因素相关。此外，尽管AI开发者意识到了应对这些风险的防护策略，但他们报告的实际应用程度很低。我们的研究结果突显了赋能AI开发者更好地应对AI隐私风险的空白与机遇。', 'title_zh': '“我们还未准备好迎接未来”:从欧洲AI开发者视角理解AI隐私风险及现有缓解策略'}
{'arxiv_id': 'arXiv:2510.00906', 'title': 'TubeDAgger: Reducing the Number of Expert Interventions with Stochastic Reach-Tubes', 'authors': 'Julian Lemmel, Manuel Kranzl, Adam Lamine, Philipp Neubauer, Radu Grosu, Sophie A. Neubauer', 'link': 'https://arxiv.org/abs/2510.00906', 'abstract': 'Interactive Imitation Learning deals with training a novice policy from expert demonstrations in an online fashion. The established DAgger algorithm trains a robust novice policy by alternating between interacting with the environment and retraining of the network. Many variants thereof exist, that differ in the method of discerning whether to allow the novice to act or return control to the expert. We propose the use of stochastic reachtubes - common in verification of dynamical systems - as a novel method for estimating the necessity of expert intervention. Our approach does not require fine-tuning of decision thresholds per environment and effectively reduces the number of expert interventions, especially when compared with related approaches that make use of a doubt classification model.', 'abstract_zh': '基于交互模仿学习的动态系统验证中随机可达管的专家干预估计方法', 'title_zh': 'TubeDAgger: 减少专家干预次数的随机可达管方法'}
{'arxiv_id': 'arXiv:2510.00883', 'title': 'GLAI: GreenLightningAI for Accelerated Training through Knowledge Decoupling', 'authors': 'Jose I. Mestre, Alberto Fernández-Hernández, Cristian Pérez-Corral, Manuel F. Dolz, Jose Duato, Enrique S. Quintana-Ortí', 'link': 'https://arxiv.org/abs/2510.00883', 'abstract': 'In this work we introduce GreenLightningAI (GLAI), a new architectural block designed as an alternative to conventional MLPs. The central idea is to separate two types of knowledge that are usually entangled during training: (i) *structural knowledge*, encoded by the stable activation patterns induced by ReLU activations; and (ii) *quantitative knowledge*, carried by the numerical weights and biases. By fixing the structure once stabilized, GLAI reformulates the MLP as a combination of paths, where only the quantitative component is optimized. This reformulation retains the universal approximation capabilities of MLPs, yet achieves a more efficient training process, reducing training time by ~40% on average across the cases examined in this study. Crucially, GLAI is not just another classifier, but a generic block that can replace MLPs wherever they are used, from supervised heads with frozen backbones to projection layers in self-supervised learning or few-shot classifiers. Across diverse experimental setups, GLAI consistently matches or exceeds the accuracy of MLPs with an equivalent number of parameters, while converging faster. Overall, GLAI establishes a new design principle that opens a direction for future integration into large-scale architectures such as Transformers, where MLP blocks dominate the computational footprint.', 'abstract_zh': 'GreenLightningAI：一种替代常规MLP的新架构模块', 'title_zh': 'GLAI：通过知识解耦加速训练的GreenLightningAI'}
{'arxiv_id': 'arXiv:2510.00877', 'title': 'A Technique Based on Trade-off Maps to Visualise and Analyse Relationships Between Objectives in Optimisation Problems', 'authors': 'Rodrigo Lankaites Pinheiro, Dario Landa-Silva, Jason Atkin', 'link': 'https://arxiv.org/abs/2510.00877', 'abstract': 'Understanding the relationships between objectives in a multiobjective optimisation problem is important for developing tailored and efficient solving techniques. In particular, when tackling combinatorial optimisation problems with many objectives, that arise in real-world logistic scenarios, better support for the decision maker can be achieved through better understanding of the often complex fitness landscape. This paper makes a contribution in this direction by presenting a technique that allows a visualisation and analysis of the local and global relationships between objectives in optimisation problems with many objectives. The proposed technique uses four steps: First, the global pairwise relationships are analysed using the Kendall correlation method; then, the ranges of the values found on the given Pareto front are estimated and assessed; next, these ranges are used to plot a map using Gray code, similar to Karnaugh maps, that has the ability to highlight the trade-offs between multiple objectives; and finally, local relationships are identified using scatter plots. Experiments are presented for three combinatorial optimisation problems: multiobjective multidimensional knapsack problem, multiobjective nurse scheduling problem, and multiobjective vehicle routing problem with time windows . Results show that the proposed technique helps in the gaining of insights into the problem difficulty arising from the relationships between objectives.', 'abstract_zh': '多目标优化问题中目标间关系的理解对于开发定制化和高效的求解技术至关重要。特别是在处理多个目标的组合优化问题时，通过对通常复杂的fitness景观的更好理解，可以为决策者提供更好的支持。本文在此方向上作出贡献，通过提出一种技术来可视化和分析多目标优化问题中局部和全局的目标关系。提出的该技术通过四个步骤进行：首先，使用Kendall相关性方法分析全局成对关系；然后，估计并评估给定Pareto前沿上找到的值的范围；接着，使用Gray代码绘制类似Karnaugh图的地图，能够突出多目标之间的权衡；最后，使用散点图识别局部关系。实验结果表明，所提出的技术有助于理解由目标间关系引起的问题难度。', 'title_zh': '基于权衡图的技术：用于优化问题中目标间关系的可视化与分析方法'}
{'arxiv_id': 'arXiv:2510.00845', 'title': 'Mechanistic Interpretability as Statistical Estimation: A Variance Analysis of EAP-IG', 'authors': 'Maxime Méloux, Maxime Peyrard, François Portet', 'link': 'https://arxiv.org/abs/2510.00845', 'abstract': "The development of trustworthy artificial intelligence requires moving beyond black-box performance metrics toward an understanding of models' internal computations. Mechanistic Interpretability (MI) aims to meet this need by identifying the algorithmic mechanisms underlying model behaviors. Yet, the scientific rigor of MI critically depends on the reliability of its findings. In this work, we argue that interpretability methods, such as circuit discovery, should be viewed as statistical estimators, subject to questions of variance and robustness. To illustrate this statistical framing, we present a systematic stability analysis of a state-of-the-art circuit discovery method: EAP-IG. We evaluate its variance and robustness through a comprehensive suite of controlled perturbations, including input resampling, prompt paraphrasing, hyperparameter variation, and injected noise within the causal analysis itself. Across a diverse set of models and tasks, our results demonstrate that EAP-IG exhibits high structural variance and sensitivity to hyperparameters, questioning the stability of its findings. Based on these results, we offer a set of best-practice recommendations for the field, advocating for the routine reporting of stability metrics to promote a more rigorous and statistically grounded science of interpretability.", 'abstract_zh': '可信人工智能的发展需要超越黑箱性能指标，转向理解模型内部计算。机制可解释性（MI）旨在通过识别模型行为背后的算法机制来满足这一需求。然而，MI的科学严谨性关键取决于其发现的可靠性。在本文中，我们argue that解释方法，如电路发现，应被视为统计估计量，受方差和稳健性问题的影响。为了说明这种统计框架，我们对一种最先进的电路发现方法EAP-IG进行了系统的稳定性分析。我们通过一系列受控扰动进行全面评估，包括输入重新采样、指令改写、超参数变化以及因果分析中的注入噪声。在多种模型和任务上，我们的结果表明，EAP-IG表现出高度的结构方差和对超参数的敏感性，质疑其发现的稳定性。基于这些结果，我们为该领域提供了一套最佳实践建议，提倡常规报告稳定性指标，促进更严谨和统计基础的解释性科学。', 'title_zh': '机制可解释性作为一种统计估计：EAP-IG的方差分析'}
{'arxiv_id': 'arXiv:2510.00833', 'title': 'Towards Verifiable Federated Unlearning: Framework, Challenges, and The Road Ahead', 'authors': 'Thanh Linh Nguyen, Marcela Tuler de Oliveira, An Braeken, Aaron Yi Ding, Quoc-Viet Pham', 'link': 'https://arxiv.org/abs/2510.00833', 'abstract': 'Federated unlearning (FUL) enables removing the data influence from the model trained across distributed clients, upholding the right to be forgotten as mandated by privacy regulations. FUL facilitates a value exchange where clients gain privacy-preserving control over their data contributions, while service providers leverage decentralized computing and data freshness. However, this entire proposition is undermined because clients have no reliable way to verify that their data influence has been provably removed, as current metrics and simple notifications offer insufficient assurance. We envision unlearning verification becoming a pivotal and trust-by-design part of the FUL life-cycle development, essential for highly regulated and data-sensitive services and applications like healthcare. This article introduces veriFUL, a reference framework for verifiable FUL that formalizes verification entities, goals, approaches, and metrics. Specifically, we consolidate existing efforts and contribute new insights, concepts, and metrics to this domain. Finally, we highlight research challenges and identify potential applications and developments for verifiable FUL and veriFUL.', 'abstract_zh': '联邦遗忘（FUL）使客户端能够从分布式训练模型中去除其数据的影响，遵守隐私法规规定的数据被遗忘的权利。FUL 促进了数据贡献方在保护隐私的情况下对数据贡献的控制权，同时也使服务提供商能够利用分布式计算和数据新鲜度。然而，这一主张因客户端缺乏可靠的方法来验证其数据影响是否已被确凿地去除而受到削弱，当前的度量标准和简单通知提供的保障不足。我们设想可验证性遗忘验证将成为FUL生命周期发展中的关键且设计即信任的一部分，对于如医疗保健等高度监管和数据敏感的服务与应用至关重要。本文介绍了一种名为veriFUL的可验证FUL参考框架，正式化了验证实体、目标、方法和度量。我们具体整合了现有努力，并在此领域贡献了新的见解、概念和度量标准。最后，我们指出了研究挑战，并确定了可验证FUL和veriFUL的潜在应用和发展方向。', 'title_zh': '可验证联邦卸载: 框架、挑战及未来之路'}
{'arxiv_id': 'arXiv:2510.00808', 'title': 'What You See is What You Ask: Evaluating Audio Descriptions', 'authors': 'Divy Kala, Eshika Khandelwal, Makarand Tapaswi', 'link': 'https://arxiv.org/abs/2510.00808', 'abstract': 'Audio descriptions (ADs) narrate important visual details in movies, enabling Blind and Low Vision (BLV) users to understand narratives and appreciate visual details. Existing works in automatic AD generation mostly focus on few-second trimmed clips, and evaluate them by comparing against a single ground-truth reference AD. However, writing ADs is inherently subjective. Through alignment and analysis of two independent AD tracks for the same movies, we quantify the subjectivity in when and whether to describe, and what and how to highlight. Thus, we show that working with trimmed clips is inadequate. We propose ADQA, a QA benchmark that evaluates ADs at the level of few-minute long, coherent video segments, testing whether they would help BLV users understand the story and appreciate visual details. ADQA features visual appreciation (VA) questions about visual facts and narrative understanding (NU) questions based on the plot. Through ADQA, we show that current AD generation methods lag far behind human-authored ADs. We conclude with several recommendations for future work and introduce a public leaderboard for benchmarking.', 'abstract_zh': '基于音频描述的回答质量评估（ADQA）：评估电影中多分钟连续视频段的视觉细节理解和叙事理解', 'title_zh': '你所听即你所问：评估音频描述'}
{'arxiv_id': 'arXiv:2510.00805', 'title': 'MG2FlowNet: Accelerating High-Reward Sample Generation via Enhanced MCTS and Greediness Control', 'authors': 'Rui Zhu, Xuan Yu, Yudong Zhang, Chen Zhang, Xu Wang, Yang Wang', 'link': 'https://arxiv.org/abs/2510.00805', 'abstract': 'Generative Flow Networks (GFlowNets) have emerged as a powerful tool for generating diverse and high-reward structured objects by learning to sample from a distribution proportional to a given reward function. Unlike conventional reinforcement learning (RL) approaches that prioritize optimization of a single trajectory, GFlowNets seek to balance diversity and reward by modeling the entire trajectory distribution. This capability makes them especially suitable for domains such as molecular design and combinatorial optimization. However, existing GFlowNets sampling strategies tend to overexplore and struggle to consistently generate high-reward samples, particularly in large search spaces with sparse high-reward regions. Therefore, improving the probability of generating high-reward samples without sacrificing diversity remains a key challenge under this premise. In this work, we integrate an enhanced Monte Carlo Tree Search (MCTS) into the GFlowNets sampling process, using MCTS-based policy evaluation to guide the generation toward high-reward trajectories and Polynomial Upper Confidence Trees (PUCT) to balance exploration and exploitation adaptively, and we introduce a controllable mechanism to regulate the degree of greediness. Our method enhances exploitation without sacrificing diversity by dynamically balancing exploration and reward-driven guidance. The experimental results show that our method can not only accelerate the speed of discovering high-reward regions but also continuously generate high-reward samples, while preserving the diversity of the generative distribution. All implementations are available at this https URL.', 'abstract_zh': '生成流网络（GFlowNets）作为一种通过学习从给定奖励函数成比例的分布中采样来生成多样化和高奖励结构化对象的强大工具而 emerge，尤其适用于分子设计和组合优化等领域。然而，现有的 GFlowNets 抽样策略倾向于过度探索，并且在稀疏高奖励区域广泛存在的大型搜索空间中难以一致地生成高奖励样本。因此，在不牺牲多样性的前提下提高生成高奖励样本的概率仍然是一个关键挑战。本文将增强的蒙特卡罗树搜索（MCTS）集成到 GFlowNets 的抽样过程中，使用基于 MCTS 的策略评估引导生成向高奖励轨迹，并通过多项式上置信树（PUCT）自适应地平衡探索与利用，同时引入可控机制调节贪婪程度。通过动态平衡探索和奖励驱动的引导，我们的方法能够在不牺牲多样性的情况下增强利用。实验证明，我们的方法不仅能加速高奖励区域的发现速度，还能连续生成高奖励样本，同时保持生成分布的多样性。所有实现均可在以下链接获取：this https URL。', 'title_zh': 'MG2FlowNet: 通过增强的MCTS和贪婪性控制加速高奖励样本生成'}
{'arxiv_id': 'arXiv:2510.00799', 'title': 'Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors', 'authors': 'Gautier Evennou, Vivien Chappelier, Ewa Kijak', 'link': 'https://arxiv.org/abs/2510.00799', 'abstract': 'Most image watermarking systems focus on robustness, capacity, and imperceptibility while treating the embedded payload as meaningless bits. This bit-centric view imposes a hard ceiling on capacity and prevents watermarks from carrying useful information. We propose LatentSeal, which reframes watermarking as semantic communication: a lightweight text autoencoder maps full-sentence messages into a compact 256-dimensional unit-norm latent vector, which is robustly embedded by a finetuned watermark model and secured through a secret, invertible rotation. The resulting system hides full-sentence messages, decodes in real time, and survives valuemetric and geometric attacks. It surpasses prior state of the art in BLEU-4 and Exact Match on several benchmarks, while breaking through the long-standing 256-bit payload ceiling. It also introduces a statistically calibrated score that yields a ROC AUC score of 0.97-0.99, and practical operating points for deployment. By shifting from bit payloads to semantic latent vectors, LatentSeal enables watermarking that is not only robust and high-capacity, but also secure and interpretable, providing a concrete path toward provenance, tamper explanation, and trustworthy AI governance. Models, training and inference code, and data splits will be available upon publication.', 'abstract_zh': 'LatentSeal: Semantic Watermarking for Robust and Secure Full-Sentence Embedding', 'title_zh': '基于自编码文本向量的快速、安全且高容量图像水印算法'}
{'arxiv_id': 'arXiv:2510.00796', 'title': 'MetaLogic: Robustness Evaluation of Text-to-Image Models via Logically Equivalent Prompts', 'authors': 'Yifan Shen, Yangyang Shu, Hye-young Paik, Yulei Sui', 'link': 'https://arxiv.org/abs/2510.00796', 'abstract': "Recent advances in text-to-image (T2I) models, especially diffusion-based architectures, have significantly improved the visual quality of generated images. However, these models continue to struggle with a critical limitation: maintaining semantic consistency when input prompts undergo minor linguistic variations. Despite being logically equivalent, such prompt pairs often yield misaligned or semantically inconsistent images, exposing a lack of robustness in reasoning and generalisation. To address this, we propose MetaLogic, a novel evaluation framework that detects T2I misalignment without relying on ground truth images. MetaLogic leverages metamorphic testing, generating image pairs from prompts that differ grammatically but are semantically identical. By directly comparing these image pairs, the framework identifies inconsistencies that signal failures in preserving the intended meaning, effectively diagnosing robustness issues in the model's logic understanding. Unlike existing evaluation methods that compare a generated image to a single prompt, MetaLogic evaluates semantic equivalence between paired images, offering a scalable, ground-truth-free approach to identifying alignment failures. It categorises these alignment errors (e.g., entity omission, duplication, positional misalignment) and surfaces counterexamples that can be used for model debugging and refinement. We evaluate MetaLogic across multiple state-of-the-art T2I models and reveal consistent robustness failures across a range of logical constructs. We find that even the SOTA text-to-image models like this http URL and DALLE-3 demonstrate a 59 percent and 71 percent misalignment rate, respectively. Our results show that MetaLogic is not only efficient and scalable, but also effective in uncovering fine-grained logical inconsistencies that are overlooked by existing evaluation metrics.", 'abstract_zh': 'Recent Advances in Text-to-Image Models: Addressing Semantic Consistency Challenges with MetaLogic', 'title_zh': 'MetaLogic：通过逻辑等价提示评估文本到图像模型的稳健性'}
{'arxiv_id': 'arXiv:2510.00771', 'title': 'UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free Flow Matching', 'authors': 'Woongjib Choi, Sangmin Lee, Hyungseob Lim, Hong-Goo Kang', 'link': 'https://arxiv.org/abs/2510.00771', 'abstract': 'In this paper, we present a vocoder-free framework for audio super-resolution that employs a flow matching generative model to capture the conditional distribution of complex-valued spectral coefficients. Unlike conventional two-stage diffusion-based approaches that predict a mel-spectrogram and then rely on a pre-trained neural vocoder to synthesize waveforms, our method directly reconstructs waveforms via the inverse Short-Time Fourier Transform (iSTFT), thereby eliminating the dependence on a separate vocoder. This design not only simplifies end-to-end optimization but also overcomes a critical bottleneck of two-stage pipelines, where the final audio quality is fundamentally constrained by vocoder performance. Experiments show that our model consistently produces high-fidelity 48 kHz audio across diverse upsampling factors, achieving state-of-the-art performance on both speech and general audio datasets.', 'abstract_zh': '基于流匹配生成模型的无 vocoder 音频超分辨率框架', 'title_zh': 'UniverSR: 无需 vocoder 的统一高效音频超分辨率方法'}
{'arxiv_id': 'arXiv:2510.00743', 'title': 'From Scores to Preferences: Redefining MOS Benchmarking for Speech Quality Reward Modeling', 'authors': 'Yifei Cao, Changhao Jiang, Jiabao Zhuang, Jiajun Sun, Ming Zhang, Zhiheng Xi, Hui Li, Shihan Dou, Yuran Wang, Yunke Zhang, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang', 'link': 'https://arxiv.org/abs/2510.00743', 'abstract': 'Assessing the perceptual quality of synthetic speech is crucial for guiding the development and refinement of speech generation models. However, it has traditionally relied on human subjective ratings such as the Mean Opinion Score (MOS), which depend on manual annotations and often suffer from inconsistent rating standards and poor reproducibility. To address these limitations, we introduce MOS-RMBench, a unified benchmark that reformulates diverse MOS datasets into a preference-comparison setting, enabling rigorous evaluation across different datasets. Building on MOS-RMBench, we systematically construct and evaluate three paradigms for reward modeling: scalar reward models, semi-scalar reward models, and generative reward models (GRMs). Our experiments reveal three key findings: (1) scalar models achieve the strongest overall performance, consistently exceeding 74% accuracy; (2) most models perform considerably worse on synthetic speech than on human speech; and (3) all models struggle on pairs with very small MOS differences. To improve performance on these challenging pairs, we propose a MOS-aware GRM that incorporates an MOS-difference-based reward function, enabling the model to adaptively scale rewards according to the difficulty of each sample pair. Experimental results show that the MOS-aware GRM significantly improves fine-grained quality discrimination and narrows the gap with scalar models on the most challenging cases. We hope this work will establish both a benchmark and a methodological framework to foster more rigorous and scalable research in automatic speech quality assessment.', 'abstract_zh': '评估合成语音的感知质量对于指导语音生成模型的发展和 refinement 至关重要。然而，这一直依赖于人力主观评分，如平均意见分 (MOS)，这需要手工注释且通常存在评分标准不一致和重现性差的问题。为解决这些局限性，我们引入了 MOS-RMBench，一种统一的基准，将多样化的 MOS 数据集重新阐述为偏好比较设置，从而在不同数据集上实现严格的评估。基于 MOS-RMBench，我们系统地构建和评估了三种奖励建模范式：标量奖励模型、半标量奖励模型和生成奖励模型（GRMs）。实验揭示了三个关键发现：(1) 标量模型的整体性能最佳，持续超过 74% 的准确率；(2) 大多数模型在合成语音上的表现远不如在人类语音上的表现；(3) 所有模型在 MOS 差异非常小的配对上表现都很差。为了提高在这些具有挑战性配对上的性能，我们提出了一种 Awareness MOS 的 GRM，其中包含基于 MOS 差异的奖励函数，使模型能够根据每个样本配对的难度自适应地调整奖励。实验结果表明，Awareness MOS 的 GRM 显著提高了细致质量区分，并在最具有挑战性的案例上缩小了与标量模型之间的差距。我们希望这项工作能够建立一个基准和方法论框架，以促进自动语音质量评估研究的更严格和可扩展。', 'title_zh': '从评分到偏好：重新定义MOS基准以适应语音质量奖励建模'}
{'arxiv_id': 'arXiv:2510.00733', 'title': 'Neural Diffusion Processes for Physically Interpretable Survival Prediction', 'authors': 'Alessio Cristofoletto, Cesare Rollo, Giovanni Birolo, Piero Fariselli', 'link': 'https://arxiv.org/abs/2510.00733', 'abstract': 'We introduce DeepFHT, a survival-analysis framework that couples deep neural networks with first hitting time (FHT) distributions from stochastic process theory. Time to event is represented as the first passage of a latent diffusion process to an absorbing boundary. A neural network maps input variables to physically meaningful parameters including initial condition, drift, and diffusion, within a chosen FHT process such as Brownian motion, both with drift and driftless. This yields closed-form survival and hazard functions and captures time-varying risk without assuming proportional-hazards.\nWe compare DeepFHT with Cox regression and other existing parametric survival models, using synthetic and real-world datasets. The method achieves predictive accuracy on par with state-of-the-art approaches, while maintaining a physics-based interpretable parameterization that elucidates the relation between input features and risk. This combination of stochastic process theory and deep learning provides a principled avenue for modeling survival phenomena in complex systems.', 'abstract_zh': 'DeepFHT：一种将深度神经网络与随机过程理论中的首次触碰时间分布相结合的生存分析框架', 'title_zh': '物理可解释的生存预测的神经扩散过程'}
{'arxiv_id': 'arXiv:2510.00658', 'title': 'Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents', 'authors': 'Beomsu Kim, Byunghee Cha, Jong Chul Ye', 'link': 'https://arxiv.org/abs/2510.00658', 'abstract': 'With diffusion and flow matching models achieving state-of-the-art generating performance, the interest of the community now turned to reducing the inference time without sacrificing sample quality. Consistency Models (CMs), which are trained to be consistent on diffusion or probability flow ordinary differential equation (PF-ODE) trajectories, enable one or two-step flow or diffusion sampling. However, CMs typically require prolonged training with large batch sizes to obtain competitive sample quality. In this paper, we examine the training dynamics of CMs near convergence and discover that CM tangents -- CM output update directions -- are quite oscillatory, in the sense that they move parallel to the data manifold, not towards the manifold. To mitigate oscillatory tangents, we propose a new loss function, called the manifold feature distance (MFD), which provides manifold-aligned tangents that point toward the data manifold. Consequently, our method -- dubbed Align Your Tangent (AYT) -- can accelerate CM training by orders of magnitude and even out-perform the learned perceptual image patch similarity metric (LPIPS). Furthermore, we find that our loss enables training with extremely small batch sizes without compromising sample quality. Code: this https URL', 'abstract_zh': '一致模型训练动态分析及调和切线方向以加速训练和提升性能：Align Your Tangent (AYT)', 'title_zh': '对齐你的切线：通过流形对齐切线训练更好的一致性模型'}
{'arxiv_id': 'arXiv:2510.00629', 'title': 'Tenyidie Syllabification corpus creation and deep learning applications', 'authors': 'Teisovi Angami, Kevisino Khate', 'link': 'https://arxiv.org/abs/2510.00629', 'abstract': 'The Tenyidie language is a low-resource language of the Tibeto-Burman family spoken by the Tenyimia Community of Nagaland in the north-eastern part of India and is considered a major language in Nagaland. It is tonal, Subject-Object-Verb, and highly agglutinative in nature. Being a low-resource language, very limited research on Natural Language Processing (NLP) has been conducted. To the best of our knowledge, no work on syllabification has been reported for this language. Among the many NLP tasks, syllabification or syllabication is an important task in which the given word syllables are identified. The contribution of this work is the creation of 10,120 syllabified Tenyidie words and the application of the Deep Learning techniques on the created corpus. In this paper, we have applied LSTM, BLSTM, BLSTM+CRF, and Encoder-decoder deep learning architectures on our created dataset. In our dataset split of 80:10:10 (train:validation:test) set, we achieved the highest accuracy of 99.21% with BLSTM model on the test set. This work will find its application in numerous other NLP applications, such as morphological analysis, part-of-speech tagging, machine translation, etc, for the Tenyidie Language.\nKeywords: Tenyidie; NLP; syllabification; deep learning; LSTM; BLSTM; CRF; Encoder-decoder', 'abstract_zh': 'Tenyidie语言的声节划分及其深度学习应用研究：基于LSTM、BLSTM、BLSTM+CRF和Encoder-decoder的探索', 'title_zh': 'Tenhyidie 音节分割语料库构建与深度学习应用'}
{'arxiv_id': 'arXiv:2510.00621', 'title': 'FAME: Adaptive Functional Attention with Expert Routing for Function-on-Function Regression', 'authors': 'Yifei Gao, Yong Chen, Chen Zhang', 'link': 'https://arxiv.org/abs/2510.00621', 'abstract': 'Functional data play a pivotal role across science and engineering, yet their infinite-dimensional nature makes representation learning challenging. Conventional statistical models depend on pre-chosen basis expansions or kernels, limiting the flexibility of data-driven discovery, while many deep-learning pipelines treat functions as fixed-grid vectors, ignoring inherent continuity. In this paper, we introduce Functional Attention with a Mixture-of-Experts (FAME), an end-to-end, fully data-driven framework for function-on-function regression. FAME forms continuous attention by coupling a bidirectional neural controlled differential equation with MoE-driven vector fields to capture intra-functional continuity, and further fuses change to inter-functional dependencies via multi-head cross attention. Extensive experiments on synthetic and real-world functional-regression benchmarks show that FAME achieves state-of-the-art accuracy, strong robustness to arbitrarily sampled discrete observations of functions.', 'abstract_zh': '功能数据在科学和技术中扮演着关键角色，但由于其无限维性质，使得表示学习具有挑战性。传统统计模型依赖于预先选择的基展开或核，限制了数据驱动发现的灵活性，而许多深度学习管道将函数视为固定网格向量，忽略了固有的连续性。本文引入了基于Mixture-of-Experts的功能注意力（FAME）框架，这是一种端到端的完全数据驱动的方法，用于函数对函数回归。FAME通过将双向神经控制微分方程与MoE驱动的向量场耦合来形成连续注意力，以捕捉函数内部的连续性，并通过多头交叉注意力融合变化以捕捉函数间的依赖关系。在合成和真实世界的功能回归基准上的 extensive 实验表明，FAME 达到了最先进的准确率，并且对任意采样离散观察函数具有很强的鲁棒性。', 'title_zh': 'FAME：基于专家路由的自适应函数注意力用于函数到函数的回归'}
{'arxiv_id': 'arXiv:2510.00591', 'title': 'AI-Driven Self-Evolving Software: A Promising Path Toward Software Automation', 'authors': 'Liyi Cai, Yijie Ren, Yitong Zhang, Jia Li', 'link': 'https://arxiv.org/abs/2510.00591', 'abstract': 'Software automation has long been a central goal of software engineering, striving for software development that proceeds without human intervention. Recent efforts have leveraged Artificial Intelligence (AI) to advance software automation with notable progress. However, current AI functions primarily as assistants to human developers, leaving software development still dependent on explicit human intervention. This raises a fundamental question: Can AI move beyond its role as an assistant to become a core component of software, thereby enabling genuine software automation? To investigate this vision, we introduce AI-Driven Self-Evolving Software, a new form of software that evolves continuously through direct interaction with users. We demonstrate the feasibility of this idea with a lightweight prototype built on a multi-agent architecture that autonomously interprets user requirements, generates and validates code, and integrates new functionalities. Case studies across multiple representative scenarios show that the prototype can reliably construct and reuse functionality, providing early evidence that such software systems can scale to more sophisticated applications and pave the way toward truly automated software development. We make code and cases in this work publicly available at this https URL.', 'abstract_zh': '基于AI的自演化软件：迈向真正的软件自动化', 'title_zh': '基于AI的自进化软件：通往软件自动化的一条有前途的道路'}
{'arxiv_id': 'arXiv:2510.00582', 'title': 'SAGE-LD: Towards Scalable and Generalizable End-to-End Language Diarization via Simulated Data Augmentation', 'authors': 'Sangmin Lee, Woongjib Choi, Jihyun Kim, Hong-Goo Kang', 'link': 'https://arxiv.org/abs/2510.00582', 'abstract': 'In this paper, we present a neural spoken language diarization model that supports an unconstrained span of languages within a single framework. Our approach integrates a learnable query-based architecture grounded in multilingual awareness, with large-scale pretraining on simulated code-switching data. By jointly leveraging these two components, our method overcomes the limitations of conventional approaches in data scarcity and architecture optimization, and generalizes effectively to real-world multilingual settings across diverse environments. Experimental results demonstrate that our approach achieves state-of-the-art performance on several language diarization benchmarks, with a relative performance improvement of 23% to 52% over previous methods. We believe that this work not only advances research in language diarization but also establishes a foundational framework for code-switching speech technologies.', 'abstract_zh': '本研究提出了一种支持单框架内多种语言未受约束的神经语音语言日记模型，该模型结合了多语言意识下的可学习查询架构和大规模模拟代码切换数据的预训练。通过联合利用这两部分，我们的方法克服了传统方法在数据稀缺性和架构优化方面的限制，并在多种环境中有效泛化到真实世界的多语言场景。实验结果表明，我们的方法在多种语言日记基准测试中实现了最先进的性能，相对于之前的方法，性能提高了23%到52%。我们相信，这项工作不仅推进了语言日记的研究，还为代码切换语音技术建立了基础框架。', 'title_zh': 'SAGE-LD：通过模拟数据增强实现可扩展和通用的端到端语言分离'}
{'arxiv_id': 'arXiv:2510.00570', 'title': 'Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning', 'authors': 'Minghao Yang, Ren Togo, Guang Li, Takahiro Ogawa, Miki Haseyama', 'link': 'https://arxiv.org/abs/2510.00570', 'abstract': 'Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task learning (MTL). However, existing MoE-MTL methods often rely on single-task pretrained backbones and suffer from redundant adaptation and inefficient knowledge sharing during the transition from single-task to multi-task learning (STL to MTL). To address these limitations, we propose adaptive shared experts (ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are assigned router-computed gating weights jointly normalized with sparse experts. This design facilitates STL to MTL transition, enhances expert specialization, and cooperation. Furthermore, we incorporate fine-grained experts by increasing the number of LoRA experts while proportionally reducing their rank, enabling more effective knowledge sharing under a comparable parameter budget. Extensive experiments on the PASCAL-Context benchmark, under unified training settings, demonstrate that ASE consistently improves performance across diverse configurations and validates the effectiveness of fine-grained designs for MTL.', 'abstract_zh': '混合专家模型（MoE）已成为多任务学习（MTL）的一种强大框架。然而，现有的MoE-MTL方法往往依赖于单任务预训练的骨干网络，并在从单任务学习（STL）过渡到多任务学习的过程中面临冗余适应和知识分享效率低下的问题。为解决这些问题，我们提出了一种基于低秩适应（LoRA）的MoE中的自适应共享专家（ASE），其中共享专家与稀疏专家一起被路由计算的门控权重联合规范化。这种设计促进了从STL到MTL的过渡，增强了专家的专业化和合作。此外，通过增加LoRA专家的数量同时按比例减少其秩，我们引入了细粒度专家，这在相近的参数预算下实现了更有效的知识共享。在统一训练设置下的PASCAL-Context基准测试上进行的广泛实验表明，ASE在多种配置下均能提升性能，并验证了细粒度设计在MTL中的有效性。', 'title_zh': '基于LoRA基混合专家的自适应共享专家多任务学习'}
{'arxiv_id': 'arXiv:2510.00566', 'title': 'Panorama: Fast-Track Nearest Neighbors', 'authors': 'Vansh Ramani, Alexis Schlomer, Akash Nayar, Panagiotis Karras, Sayan Ranu, Jignesh M. Patel', 'link': 'https://arxiv.org/abs/2510.00566', 'abstract': "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose embeddings are close to that of a given query in a high-dimensional space, aiming to balance accuracy with speed. Used in recommendation systems, image and video retrieval, natural language processing, and retrieval-augmented generation (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT utilize graph, tree, clustering, and quantization techniques to navigate large vector spaces. Despite this progress, ANNS systems spend up to 99\\% of query time to compute distances in their final refinement phase. In this paper, we present PANORAMA, a machine learning-driven approach that tackles the ANNS verification bottleneck through data-adaptive learned orthogonal transforms that facilitate the accretive refinement of distance bounds. Such transforms compact over 90\\% of signal energy into the first half of dimensions, enabling early candidate pruning with partial distance computations. We integrate PANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and Annoy, without index modification, using level-major memory layouts, SIMD-vectorized partial distance computations, and cache-aware access patterns. Experiments across diverse datasets -- from image-based CIFAR-10 and GIST to modern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate that PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.", 'abstract_zh': '基于机器学习的PANORAMA：通过数据自适应学习正交变换解决近邻搜索验证瓶颈', 'title_zh': '全景：快速 nearest neighbors 探索'}
{'arxiv_id': 'arXiv:2510.00563', 'title': 'Memory Determines Learning Direction: A Theory of Gradient-Based Optimization in State Space Models', 'authors': 'JingChuan Guan, Tomoyuki Kubota, Yasuo Kuniyoshi, Kohei Nakajima', 'link': 'https://arxiv.org/abs/2510.00563', 'abstract': "State space models (SSMs) have gained attention by showing potential to outperform Transformers. However, previous studies have not sufficiently addressed the mechanisms underlying their high performance owing to a lack of theoretical explanation of SSMs' learning dynamics. In this study, we provide such an explanation and propose an improved training strategy. The memory capacity of SSMs can be evaluated by examining how input time series are stored in their current state. Such an examination reveals a tradeoff between memory accuracy and length, as well as the theoretical equivalence between the structured state space sequence model (S4) and a simplified S4 with diagonal recurrent weights. This theoretical foundation allows us to elucidate the learning dynamics, proving the importance of initial parameters. Our analytical results suggest that successful learning requires the initial memory structure to be the longest possible even if memory accuracy may deteriorate or the gradient lose the teacher information. Experiments on tasks requiring long memory confirmed that extending memory is difficult, emphasizing the importance of initialization. Furthermore, we found that fixing recurrent weights can be more advantageous than adapting them because it achieves comparable or even higher performance with faster convergence. Our results provide a new theoretical foundation for SSMs and potentially offer a novel optimization strategy.", 'abstract_zh': '状态空间模型（SSMs）通过表现出超越Transformer的潜力而引起了关注。然而，先前的研究尚未充分探讨其高性能背后的机制，这主要是由于缺乏对SSMs学习动态的理论解释。在本研究中，我们提供了这样的解释并提出了一种改进的训练策略。通过检查输入时间序列在当前状态中的存储情况，可以评估SSMs的记忆容量。这种检查揭示了记忆准确性和长度之间的权衡，并且证明了结构化状态空间序列模型（S4）与简化后的具有对角循环权重的S4在理论上等价。基于这一理论基础，我们可以阐明学习动态，证明初始参数的重要性。我们的分析结果表明，即使可能牺牲记忆准确性或梯度失去教师信息，成功的学习也需要初始记忆结构尽可能长。实验结果证实，在需要长记忆的任务上，延长记忆是困难的，强调了初始化的重要性。此外，我们发现固定循环权重可能比适应它们更具优势，因为这不仅能够实现相当甚至更高的性能，还能实现更快的收敛。我们的结果为SSMs提供了一个新的理论基础，并可能提供一种新的优化策略。', 'title_zh': '记忆决定学习方向：状态空间模型中基于梯度优化的理论'}
{'arxiv_id': 'arXiv:2510.00549', 'title': 'EMR-AGENT: Automating Cohort and Feature Extraction from EMR Databases', 'authors': 'Kwanhyung Lee, Sungsoo Hong, Joonhyung Park, Jeonghyeop Lim, Juhwan Choi, Donghwee Yoon, Eunho Yang', 'link': 'https://arxiv.org/abs/2510.00549', 'abstract': 'Machine learning models for clinical prediction rely on structured data extracted from Electronic Medical Records (EMRs), yet this process remains dominated by hardcoded, database-specific pipelines for cohort definition, feature selection, and code mapping. These manual efforts limit scalability, reproducibility, and cross-institutional generalization. To address this, we introduce EMR-AGENT (Automated Generalized Extraction and Navigation Tool), an agent-based framework that replaces manual rule writing with dynamic, language model-driven interaction to extract and standardize structured clinical data. Our framework automates cohort selection, feature extraction, and code mapping through interactive querying of databases. Our modular agents iteratively observe query results and reason over schema and documentation, using SQL not just for data retrieval but also as a tool for database observation and decision making. This eliminates the need for hand-crafted, schema-specific logic. To enable rigorous evaluation, we develop a benchmarking codebase for three EMR databases (MIMIC-III, eICU, SICdb), including both seen and unseen schema settings. Our results demonstrate strong performance and generalization across these databases, highlighting the feasibility of automating a process previously thought to require expert-driven design. The code will be released publicly at this https URL. For a demonstration, please visit our anonymous demo page: this https URL', 'abstract_zh': '基于电子医疗记录的机器学习模型依赖于从电子病历（EMRs）中提取的结构化数据，但这一过程仍然主要由硬编码的、数据库特定的流水线来确定队列、特征选择和代码映射。这些手动努力限制了可扩展性、复现性和跨机构的一般化能力。为解决这一问题，我们引入了EMR-AGENT（自动化通用提取和导航工具）框架，该框架使用基于代理的方法，用动态的语言模型驱动交互来取代手动规则编写，以提取和标准化结构化临床数据。我们的框架通过交互查询数据库来自动化队列选择、特征提取和代码映射。我们模块化的代理迭代观察查询结果，并在包括模式和文档在内的所有方面进行推理，使用SQL不仅用于数据检索，还作为数据库观察和决策的工具。这消除了手写特定模式逻辑的需要。为实现严格评估，我们为三个电子医疗记录数据库（MIMIC-III、eICU、SICdb）开发了基准代码库，包括已见和未知模式设置。我们的结果显示了这些数据库的强大性能和泛化能力，突显了自动化以往被认为需要专家驱动设计的过程的可能性。代码将在此公开发布：this https URL。如需演示，请访问我们的匿名演示页面：this https URL。', 'title_zh': 'EMR-AGENT: 自动化从EMR数据库中提取队列和特征'}
{'arxiv_id': 'arXiv:2510.00519', 'title': 'Architectural Transformations and Emerging Verification Demands in AI-Enabled Cyber-Physical Systems', 'authors': 'Hadiza Umar Yusuf, Khouloud Gaaloul', 'link': 'https://arxiv.org/abs/2510.00519', 'abstract': 'In the world of Cyber-Physical Systems (CPS), a captivating real-time fusion occurs where digital technology meets the physical world. This synergy has been significantly transformed by the integration of artificial intelligence (AI), a move that dramatically enhances system adaptability and introduces a layer of complexity that impacts CPS control optimization and reliability. Despite advancements in AI integration, a significant gap remains in understanding how this shift affects CPS architecture, operational complexity, and verification practices. The extended abstract addresses this gap by investigating architectural distinctions between AI-driven and traditional control models designed in Simulink and their respective implications for system verification.', 'abstract_zh': '在 cyber-物理系统（CPS）领域，数字技术与物理世界的精彩实时融合吸引了广泛关注。人工智能（AI）的融合极大地改变了这种 synergy，显著增强了系统的适应性，同时也引入了一层复杂性，影响了 CPS 控制优化和可靠性。尽管在 AI 融合方面取得了进展，但在了解这种转变如何影响 CPS 架构、操作复杂性和验证实践方面仍存在较大缺口。扩展摘要通过探讨 Simulink 中设计的 AI 驱动控制模型与传统控制模型之间的架构区别及其对系统验证的相应影响，来填补这一缺口。', 'title_zh': 'AI使能的网络物理系统中架构变换与新兴验证要求'}
{'arxiv_id': 'arXiv:2510.00512', 'title': 'Adaptive Data-Knowledge Alignment in Genetic Perturbation Prediction', 'authors': 'Yuanfang Xiang, Lun Ai', 'link': 'https://arxiv.org/abs/2510.00512', 'abstract': "The transcriptional response to genetic perturbation reveals fundamental insights into complex cellular systems. While current approaches have made progress in predicting genetic perturbation responses, they provide limited biological understanding and cannot systematically refine existing knowledge. Overcoming these limitations requires an end-to-end integration of data-driven learning and existing knowledge. However, this integration is challenging due to inconsistencies between data and knowledge bases, such as noise, misannotation, and incompleteness. To address this challenge, we propose ALIGNED (Adaptive aLignment for Inconsistent Genetic kNowledgE and Data), a neuro-symbolic framework based on the Abductive Learning (ABL) paradigm. This end-to-end framework aligns neural and symbolic components and performs systematic knowledge refinement. We introduce a balanced consistency metric to evaluate the predictions' consistency against both data and knowledge. Our results show that ALIGNED outperforms state-of-the-art methods by achieving the highest balanced consistency, while also re-discovering biologically meaningful knowledge. Our work advances beyond existing methods to enable both the transparency and the evolution of mechanistic biological understanding.", 'abstract_zh': '遗传扰动的转录反应揭示了复杂细胞系统的基本见解。虽然目前的方法在预测遗传扰动反应方面取得了进展，但它们提供的生物学理解有限，无法系统性地完善现有知识。克服这些局限性需要端到端地整合数据驱动学习和现有知识。然而，这种整合由于数据和知识库之间的一致性问题，如噪声、误注释和不完整性而面临挑战。为了解决这一挑战，我们提出了一种基于归纳推理学习（ABL）范式的神经符号框架ALIGNED（自适应不一致遗传知识和数据的对齐）。这一端到端框架对齐了神经和符号组件，并进行系统的知识完善。我们引入了一致性度量来评估预测的一致性，既针对数据也针对知识。我们的结果显示，ALIGNED在平衡一致性度量上优于现有方法，并且重新发现了一些生物学上有意义的知识。我们的工作超越了现有方法，使机制生物学理解的透明性和进化成为可能。', 'title_zh': '基因扰动预测中的自适应数据-知识对齐'}
{'arxiv_id': 'arXiv:2510.00495', 'title': 'Normal-Abnormal Guided Generalist Anomaly Detection', 'authors': 'Yuexin Wang, Xiaolei Wang, Yizheng Gong, Jimin Xiao', 'link': 'https://arxiv.org/abs/2510.00495', 'abstract': 'Generalist Anomaly Detection (GAD) aims to train a unified model on an original domain that can detect anomalies in new target domains. Previous GAD methods primarily use only normal samples as references, overlooking the valuable information contained in anomalous samples that are often available in real-world scenarios. To address this limitation, we propose a more practical approach: normal-abnormal-guided generalist anomaly detection, which leverages both normal and anomalous samples as references to guide anomaly detection across diverse domains. We introduce the Normal-Abnormal Generalist Learning (NAGL) framework, consisting of two key components: Residual Mining (RM) and Anomaly Feature Learning (AFL). RM extracts abnormal patterns from normal-abnormal reference residuals to establish transferable anomaly representations, while AFL adaptively learns anomaly features in query images through residual mapping to identify instance-aware anomalies. Our approach effectively utilizes both normal and anomalous references for more accurate and efficient cross-domain anomaly detection. Extensive experiments across multiple benchmarks demonstrate that our method significantly outperforms existing GAD approaches. This work represents the first to adopt a mixture of normal and abnormal samples as references in generalist anomaly detection. The code and datasets are available at this https URL.', 'abstract_zh': '通用异常检测（GAD）旨在训练一个统一模型，在原始领域中学习，以检测新目标领域的异常。以往的GAD方法主要仅使用正常样本作为参考，忽视了异常样本中包含的重要信息，而这些异常样本在现实场景中通常可获得。为解决这一局限，我们提出了一种更为实用的方法：正常-异常引导的通用异常检测，该方法利用正常和异常样本作为参考，指导跨域异常检测。我们引入了正常-异常通用学习（NAGL）框架，包含两个关键组件：残差挖掘（RM）和异常特征学习（AFL）。RM从正常-异常参考残差中提取异常模式，以建立可转移的异常表示；AFL通过残差映射在查询图像中自适应地学习异常特征，以识别实例感知的异常。我们的方法有效地利用了正常和异常参考，以实现更准确和高效的跨域异常检测。在多个基准上的广泛实验表明，我们的方法显著优于现有GAD方法。这是首次在通用异常检测中采用正常和异常样本混合作为参考的工作。代码和数据集可在以下链接获取。', 'title_zh': '正常-异常引导的通用异常检测'}
{'arxiv_id': 'arXiv:2510.00487', 'title': 'Black-Box Time-Series Domain Adaptation via Cross-Prompt Foundation Models', 'authors': 'M. T. Furqon, Mahardhika Pratama, Igor Skrjanc, Lin Liu, Habibullah Habibullah, Kutluyil Dogancay', 'link': 'https://arxiv.org/abs/2510.00487', 'abstract': 'The black-box domain adaptation (BBDA) topic is developed to address the privacy and security issues where only an application programming interface (API) of the source model is available for domain adaptations. Although the BBDA topic has attracted growing research attentions, existing works mostly target the vision applications and are not directly applicable to the time-series applications possessing unique spatio-temporal characteristics. In addition, none of existing approaches have explored the strength of foundation model for black box time-series domain adaptation (BBTSDA). This paper proposes a concept of Cross-Prompt Foundation Model (CPFM) for the BBTSDA problems. CPFM is constructed under a dual branch network structure where each branch is equipped with a unique prompt to capture different characteristics of data distributions. In the domain adaptation phase, the reconstruction learning phase in the prompt and input levels is developed. All of which are built upon a time-series foundation model to overcome the spatio-temporal dynamic. Our rigorous experiments substantiate the advantage of CPFM achieving improved results with noticeable margins from its competitors in three time-series datasets of different application domains.', 'abstract_zh': '黑箱时间序列域适应中的跨提示基础模型（CPFM）', 'title_zh': '黑盒时间序列域适应通过跨提示基础模型'}
{'arxiv_id': 'arXiv:2510.00468', 'title': 'Feature Identification via the Empirical NTK', 'authors': 'Jennifer Lin', 'link': 'https://arxiv.org/abs/2510.00468', 'abstract': 'We provide evidence that eigenanalysis of the empirical neural tangent kernel (eNTK) can surface the features used by trained neural networks. Across two standard toy models for mechanistic interpretability, Toy Models of Superposition (TMS) and a 1-layer MLP trained on modular addition, we find that the eNTK exhibits sharp spectral cliffs whose top eigenspaces align with ground-truth features. In TMS, the eNTK recovers the ground-truth features in both the sparse (high superposition) and dense regimes. In modular arithmetic, the eNTK can be used to recover Fourier feature families. Moreover, we provide evidence that a layerwise eNTK localizes features to specific layers and that the evolution of the eNTK eigenspectrum can be used to diagnose the grokking phase transition. These results suggest that eNTK analysis may provide a practical handle for feature discovery and for detecting phase changes in small models.', 'abstract_zh': '我们提供了实证证据表明，神经 tangent 核的特征分析能够揭示训练神经网络使用的特点。在两个标准的机制可解释性玩具模型——叠加模型（TMS）和用于模Arithmetic加运算训练的单层MLP中，我们发现eNTK表现出尖锐的谱阶，其主导特征空间与真实特征对齐。在TMS中，eNTK能够在稀疏和稠密模式下恢复真实特征。在模Arithmetic中，eNTK可用于恢复Fourier特征家族。此外，我们提供了证据表明，分层的eNTK能够将特征局部化到特定层，并且eNTK特征谱的变化可以用于诊断掌握相变。这些结果表明，eNTK分析可能为特征发现和小型模型中检测相变提供实用的方法。', 'title_zh': '经验NTK特征识别'}
{'arxiv_id': 'arXiv:2510.00461', 'title': 'TimeEmb: A Lightweight Static-Dynamic Disentanglement Framework for Time Series Forecasting', 'authors': 'Mingyuan Xia, Chunxu Zhang, Zijian Zhang, Hao Miao, Qidong Liu, Yuanshao Zhu, Bo Yang', 'link': 'https://arxiv.org/abs/2510.00461', 'abstract': 'Temporal non-stationarity, the phenomenon that time series distributions change over time, poses fundamental challenges to reliable time series forecasting. Intuitively, the complex time series can be decomposed into two factors, \\ie time-invariant and time-varying components, which indicate static and dynamic patterns, respectively. Nonetheless, existing methods often conflate the time-varying and time-invariant components, and jointly learn the combined long-term patterns and short-term fluctuations, leading to suboptimal performance facing distribution shifts. To address this issue, we initiatively propose a lightweight static-dynamic decomposition framework, TimeEmb, for time series forecasting. TimeEmb innovatively separates time series into two complementary components: (1) time-invariant component, captured by a novel global embedding module that learns persistent representations across time series, and (2) time-varying component, processed by an efficient frequency-domain filtering mechanism inspired by full-spectrum analysis in signal processing. Experiments on real-world datasets demonstrate that TimeEmb outperforms state-of-the-art baselines and requires fewer computational resources. We conduct comprehensive quantitative and qualitative analyses to verify the efficacy of static-dynamic disentanglement. This lightweight framework can also improve existing time-series forecasting methods with simple integration. To ease reproducibility, the code is available at this https URL.', 'abstract_zh': '时间非平稳现象给可靠的时间序列预测带来了根本性挑战。直观地讲，复杂的时间序列可以分解为两个因素，即时间不变和时间变化的组件，分别代表静态和动态模式。然而，现有方法往往将时间变化和时间不变的组件混淆在一起，联合学习长期趋势和短期波动，导致在分布转移面前表现不佳。为解决这一问题，我们提出了一种轻量级的静态-动态分解框架TimeEmb用于时间序列预测。TimeEmb创新性地将时间序列分解为两个互补的组件：（1）时间不变组件，通过一个新颖的全局嵌入模块学习跨时间序列的持久表示；（2）时间变化组件，通过灵感源自信号处理中全谱分析的高度有效的频域滤波机制处理。实验证明，TimeEmb在实际数据集上的表现优于最先进的基线，并且需要更少的计算资源。通过对静态-动态分解的有效性进行全面的定量和定性分析，验证了该轻量级框架的优越性。该框架还可通过简单集成来改进现有的时间序列预测方法，以促进可再现性，代码已开源。', 'title_zh': 'TimeEmb：一种轻量级静态-动态解耦框架用于时间序列预测'}
{'arxiv_id': 'arXiv:2510.00457', 'title': 'UrbanGraph: Physics-Informed Spatio-Temporal Dynamic Heterogeneous Graphs for Urban Microclimate Prediction', 'authors': 'Weilin Xin, Chenyu Huang, Peilin Li, Jing Zhong, Jiawei Yao', 'link': 'https://arxiv.org/abs/2510.00457', 'abstract': 'With rapid urbanization, predicting urban microclimates has become critical, as it affects building energy demand and public health risks. However, existing generative and homogeneous graph approaches fall short in capturing physical consistency, spatial dependencies, and temporal variability. To address this, we introduce UrbanGraph, a physics-informed framework integrating heterogeneous and dynamic spatio-temporal graphs. It encodes key physical processes -- vegetation evapotranspiration, shading, and convective diffusion -- while modeling complex spatial dependencies among diverse urban entities and their temporal evolution. We evaluate UrbanGraph on UMC4/12, a physics-based simulation dataset covering diverse urban configurations and climates. Results show that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0% over all baselines, with heterogeneous and dynamic graphs contributing 3.5% and 7.1% gains. Our dataset provides the first high-resolution benchmark for spatio-temporal microclimate modeling, and our method extends to broader urban heterogeneous dynamic computing tasks.', 'abstract_zh': '随着快速城市化，预测城市微气候变得至关重要，因为它影响建筑能源需求和公共健康风险。然而，现有的生成性和同质性图方法在捕捉物理一致性、空间依赖性和时间变异性方面存在不足。为解决这一问题，我们引入了UrbanGraph，这是一种结合了异质性和动态时空图的物理导向框架。它编码了关键的物理过程——植被蒸腾、遮荫和对流扩散，同时建模了各种城市实体之间复杂的空间关系及其时间演变。我们在基于物理的模拟数据集UMC4/12上评估了UrbanGraph，该数据集涵盖了多种城市配置和气候。结果表明，与所有基线方法相比，UrbanGraph的$R^2$提高了10.8%，计算量减少了17.0%，异质性和动态图分别贡献了3.5%和7.1%的改善。我们的数据集提供了首个高分辨率的时空微气候建模基准，并且我们的方法扩展到更广泛的异质动态城市计算任务。', 'title_zh': 'UrbanGraph: 物理约束的空间-时间动态异质图城市微气候预测'}
{'arxiv_id': 'arXiv:2510.00428', 'title': 'Automated Structured Radiology Report Generation with Rich Clinical Context', 'authors': 'Seongjae Kang, Dong Bok Lee, Juho Jung, Dongseop Kim, Won Hwa Kim, Sunghoon Joo', 'link': 'https://arxiv.org/abs/2510.00428', 'abstract': 'Automated structured radiology report generation (SRRG) from chest X-ray images offers significant potential to reduce workload of radiologists by generating reports in structured formats that ensure clarity, consistency, and adherence to clinical reporting standards. While radiologists effectively utilize available clinical contexts in their diagnostic reasoning, existing SRRG systems overlook these essential elements. This fundamental gap leads to critical problems including temporal hallucinations when referencing non-existent clinical contexts. To address these limitations, we propose contextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical context for SRRG. We curate C-SRRG dataset by integrating comprehensive clinical context encompassing 1) multi-view X-ray images, 2) clinical indication, 3) imaging techniques, and 4) prior studies with corresponding comparisons based on patient histories. Through extensive benchmarking with state-of-the-art multimodal large language models, we demonstrate that incorporating clinical context with the proposed C-SRRG significantly improves report generation quality. We publicly release dataset, code, and checkpoints to facilitate future research for clinically-aligned automated RRG at this https URL.', 'abstract_zh': '基于胸片的自动结构化放射学报告生成（C-SRRG）通过生成符合临床报告标准的结构化报告，显著减少了放射科医师的工作负荷。当前的SRRG系统忽视了这些重要元素，导致在引用不存在的临床背景时出现时间幻觉等关键问题。为了弥补这些不足，我们提出了一种综合临床上下文的C-SRRG方法。通过整合包含多视角X光图像、临床指征、成像技术以及基于患者历史的先前研究和相应比较的综合临床背景，我们利用最新的多模态大型语言模型进行了广泛基准测试，证明了综合临床背景的C-SRRG显著提高了报告生成质量。现公开发布数据集、代码和检查点，以促进未来与临床对齐的自动RGG研究，详情请访问此链接：https://xxxxxx。', 'title_zh': '丰富的临床上下文条件下的自动结构化放射学报告生成'}
{'arxiv_id': 'arXiv:2510.00401', 'title': 'Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting', 'authors': 'Shounak Sural, Charles Kekeh, Wenliang Liu, Federico Pecora, Mouhacine Benosman', 'link': 'https://arxiv.org/abs/2510.00401', 'abstract': 'Long-horizon motion forecasting for multiple autonomous robots is challenging due to non-linear agent interactions, compounding prediction errors, and continuous-time evolution of dynamics. Learned dynamics of such a system can be useful in various applications such as travel time prediction, prediction-guided planning and generative simulation. In this work, we aim to develop an efficient trajectory forecasting model conditioned on multi-agent goals. Motivated by the recent success of physics-guided deep learning for partially known dynamical systems, we develop a model based on neural Controlled Differential Equations (CDEs) for long-horizon motion forecasting. Unlike discrete-time methods such as RNNs and transformers, neural CDEs operate in continuous time, allowing us to combine physics-informed constraints and biases to jointly model multi-robot dynamics. Our approach, named PINCoDE (Physics-Informed Neural Controlled Differential Equations), learns differential equation parameters that can be used to predict the trajectories of a multi-agent system starting from an initial condition. PINCoDE is conditioned on future goals and enforces physics constraints for robot motion over extended periods of time. We adopt a strategy that scales our model from 10 robots to 100 robots without the need for additional model parameters, while producing predictions with an average ADE below 0.5 m for a 1-minute horizon. Furthermore, progressive training with curriculum learning for our PINCoDE model results in a 2.7X reduction of forecasted pose error over 4 minute horizons compared to analytical models.', 'abstract_zh': '长时程多自主机器人运动预测因非线性代理交互、累积预测误差及动态的连续时间演化而具有挑战性。此类系统中学到的动力学在诸如旅行时间预测、预测引导规划和生成仿真等众多应用中具有重要意义。本文旨在开发一种基于多agent目标的高效轨迹预测模型。受近期物理引导深度学习在部分已知动力学系统中的成功启发，我们基于神经控制差分方程（CDEs）开发了一种长时程运动预测模型。与RNN和变压器等离散时间方法不同，神经CDEs在连续时间下操作，使我们能够结合物理启发的约束和偏见，共同建模多机器人动力学。我们的方法名为PINCoDE（物理启发的神经控制差分方程），它能够从初始条件开始学习微分方程参数，以预测多agent系统的轨迹。PINCoDE受到未来目标的条件限制，并在长时间内约束机器人运动的物理约束。我们采用一种策略，无需增加额外模型参数即可将模型从10个机器人扩展到100个机器人，同时在1分钟时间范围内生成平均ADE低于0.5米的预测结果。此外，我们的PINCoDE模型通过进阶训练和课程学习，4分钟时间范围内的预测姿态误差比分析模型降低2.7倍。', 'title_zh': '基于物理信息神经控制差分方程的可扩展多agent长时 horizon 运动预测'}
{'arxiv_id': 'arXiv:2510.00395', 'title': 'SAGE-Music: Low-Latency Symbolic Music Generation via Attribute-Specialized Key-Value Head Sharing', 'authors': 'Jiaye Tan, Haonan Luo, Linfeng Song, Shuaiqi Chen, Yishan Lyu, Zian Zhong, Roujia Wang, Daniel Jiang, Haoran Zhang, Jiaming Bai, Haoran Cheng, Q. Vera Liao, Hao-Wen Dong', 'link': 'https://arxiv.org/abs/2510.00395', 'abstract': "Low-latency symbolic music generation is essential for real-time improvisation and human-AI co-creation. Existing transformer-based models, however, face a trade-off between inference speed and musical quality. Traditional acceleration techniques such as embedding pooling significantly degrade quality, while recently proposed Byte Pair Encoding (BPE) methods - though effective on single-track piano data - suffer large performance drops in multi-track settings, as revealed by our analysis. We propose Attribute-Specialized Key-Value Head Sharing (AS-KVHS), adapted to music's structured symbolic representation, achieving about 30% inference speedup with only a negligible (about 0.4%) quality drop in objective evaluations and slight improvements in subjective listening tests. Our main contributions are (1) the first systematic study of BPE's generalizability in multi-track symbolic music, and (2) the introduction of AS-KVHS for low-latency symbolic music generation. Beyond these, we also release SAGE-Music, an open-source benchmark that matches or surpasses state-of-the-art models in generation quality.", 'abstract_zh': '低延迟符号音乐生成对于实时即兴和人机共创至关重要。现有的基于变压器的模型在推断速度和音乐质量之间面临权衡。传统的加速技术如嵌入池化显著降质，而最近提出的字节对编码（BPE）方法虽然在单轨钢琴数据上有效，但在多轨设置中性能大幅下降，如我们分析所示。我们提出了适应音乐结构化符号表示的属性特殊化键值头共享（AS-KVHS），在客观评估中仅导致微小的质量下降（约0.4%），并在主观听感测试中略有改进，实现了约30%的推断速度提升。我们的主要贡献包括（1）多轨符号音乐中BPE通用性的首次系统研究，以及（2）用于低延迟符号音乐生成的AS-KVHS方法。此外，我们还发布了SAGE-Music，一个开源基准，其生成质量与最先进的模型相当或超越。', 'title_zh': 'SAGE-Music: 基于属性专业化键值头共享的低延迟符号音乐生成'}
{'arxiv_id': 'arXiv:2510.00386', 'title': 'Train on Validation (ToV): Fast data selection with applications to fine-tuning', 'authors': 'Ayush Jain, Andrea Montanari, Eren Sasoglu', 'link': 'https://arxiv.org/abs/2510.00386', 'abstract': 'State-of-the-art machine learning often follows a two-stage process: $(i)$~pre-training on large, general-purpose datasets; $(ii)$~fine-tuning on task-specific data. In fine-tuning, selecting training examples that closely reflect the target distribution is crucial. However, it is often the case that only a few samples are available from the target distribution. Existing data selection methods treat these target samples as a validation set and estimate the effect of adding or removing a single sample from the training pool by performing inference on the validation set.\nWe propose a simpler and faster alternative that inverts the usual role of train and validation: we perform inference on the training pool before and after fine-tuning on the validation set. We then select samples whose predictions change the most. Our key insight is that the training samples most affected by fine-tuning on a small validation set tend to be the most beneficial for reducing test loss on the target distribution. Experiments on instruction tuning and named entity recognition tasks show that, in most cases, our method achieves lower test log-loss than state-of-the-art approaches. We support our findings with theoretical analysis.', 'abstract_zh': '最先进的机器学习方法通常遵循一个两阶段的过程：(i) 在大规模通用数据集上进行预训练；(ii) 在任务特定数据上进行微调。在微调过程中，选择与目标分布密切对应的训练样本至关重要。然而，目标分布的数据样本往往很少。现有的数据选择方法将这些目标样本视为验证集，并通过在验证集上进行推理来估计添加或删除训练池中一个样本的影响。我们提出了一种更简单和更快的替代方案，逆转了训练集和验证集的常规作用：在微调验证集之前和之后对训练池进行推理，然后选择预测变化最大的样本。我们的关键洞察是，对小型验证集进行微调后受影响最大的训练样本通常对降低目标分布上的测试损失最具益处。我们在指令调优和命名实体识别任务上的实验表明，在大多数情况下，我们的方法在测试日志损失上优于最先进的方法。我们通过理论分析支持了这些发现。', 'title_zh': '在验证集上训练 (ToV): 快速数据选择及其在微调中的应用'}
{'arxiv_id': 'arXiv:2510.00376', 'title': 'Discrete Wavelet Transform as a Facilitator for Expressive Latent Space Representation in Variational Autoencoders in Satellite Imagery', 'authors': 'Arpan Mahara, Md Rezaul Karim Khan, Naphtali Rishe, Wenjia Wang, Seyed Masoud Sadjadi', 'link': 'https://arxiv.org/abs/2510.00376', 'abstract': "Latent Diffusion Models (LDM), a subclass of diffusion models, mitigate the computational complexity of pixel-space diffusion by operating within a compressed latent space constructed by Variational Autoencoders (VAEs), demonstrating significant advantages in Remote Sensing (RS) applications. Though numerous studies enhancing LDMs have been conducted, investigations explicitly targeting improvements within the intrinsic latent space remain scarce. This paper proposes an innovative perspective, utilizing the Discrete Wavelet Transform (DWT) to enhance the VAE's latent space representation, designed for satellite imagery. The proposed method, ExpDWT-VAE, introduces dual branches: one processes spatial domain input through convolutional operations, while the other extracts and processes frequency-domain features via 2D Haar wavelet decomposition, convolutional operation, and inverse DWT reconstruction. These branches merge to create an integrated spatial-frequency representation, further refined through convolutional and diagonal Gaussian mapping into a robust latent representation. We utilize a new satellite imagery dataset housed by the TerraFly mapping system to validate our method. Experimental results across several performance metrics highlight the efficacy of the proposed method at enhancing latent space representation.", 'abstract_zh': '基于离散小波变换的ExpDWT-VAE在遥感应用中的改进', 'title_zh': '离散小波变换在卫星图像变分自编码器中促进具表现力的潜在空间表示'}
{'arxiv_id': 'arXiv:2510.00361', 'title': 'Attribution Gradients: Incrementally Unfolding Citations for Critical Examination of Attributed AI Answers', 'authors': 'Hita Kambhamettu, Alyssa Hwang, Philippe Laban, Andrew Head', 'link': 'https://arxiv.org/abs/2510.00361', 'abstract': 'AI question answering systems increasingly generate responses with attributions to sources. However, the task of verifying the actual content of these attributions is in most cases impractical. In this paper, we present attribution gradients as a solution. Attribution gradients provide integrated, incremental affordances for diving into an attributed passage. A user can decompose a sentence of an answer into its claims. For each claim, the user can view supporting and contradictory excerpts mined from sources. Those excerpts serve as clickable conduits into the source (in our application, scientific papers). When evidence itself contains more citations, the UI unpacks the evidence into excerpts from the cited sources. These features of attribution gradients facilitate concurrent interconnections among answer, claim, excerpt, and context. In a usability study, we observed greater engagement with sources and richer revision in a task where participants revised an attributed AI answer with attribution gradients and a baseline.', 'abstract_zh': 'AI问答系统越来越多地生成带有来源归因的回应，然而核实这些归因内容的真实性在大多数情况下是不切实际的。本文提出归因梯度作为解决方案。归因梯度为深入分析归因段落提供集成且逐步的功能。用户可以将答案中的句子分解为其主张，并为每个主张查看支持性和反驳性片断，这些片断作为可点击链接通往来源（在我们的应用中为科学论文）。当证据本身包含更多引用时，用户界面会将证据分解为所引用来源中的片断。这些归因梯度的功能促进答案、主张、片断和语境之间的并发连接。在可用性研究中，我们观察到在使用归因梯度和基线重写带归因的AI回答的任务中，参与者与来源的互动更加积极，修订也更加丰富。', 'title_zh': 'Attribution Gradients: 逐步展开引用来批判性检查attributed AI答案'}
{'arxiv_id': 'arXiv:2510.00339', 'title': 'Navigating the Synchrony-Stability Frontier in Adaptive Chatbots', 'authors': 'T. James Brandt', 'link': 'https://arxiv.org/abs/2510.00339', 'abstract': 'Adaptive chatbots that mimic a user\'s linguistic style can build rapport and engagement, yet unconstrained mimicry risks an agent that feels unstable or sycophantic. We present a computational evaluation framework that makes the core design tension explicit: balancing moment-to-moment linguistic synchrony against long-term persona stability. Using an 8-dimensional style vector and a closed-loop "base+delta" prompting architecture, we simulate and compare explicit adaptation policies - Uncapped, Cap, Exponential Moving Average (EMA), Dead-Band, and Hybrids - on a human-log dataset. Our analysis maps a clear Pareto frontier: bounded policies achieve substantial gains in stability at a modest cost to synchrony. For example, a Hybrid (EMA+Cap) raises stability from 0.542 to 0.878 (+62%) while reducing synchrony by only 17%. We confirm this trade-off through large-scale replications on three public corpora (DailyDialog, Persona-Chat, EmpatheticDialogues) and LLM-in-the-loop validation across two model families. Furthermore, we quantify "prompt legibility," showing that frontier policies reduce instruction churn and cut jarring register flips (major tone changes) from 0.254 to 0.092, yielding systems that are easier to reason about and maintain. Taken together, our framework provides a general evaluation harness for style adaptation; a systematic ablation that identifies Pareto-efficient policies; robust validation across diverse datasets and models; and novel legibility metrics linking policy choices to system maintainability.', 'abstract_zh': '适应性聊天机器人可以通过模仿用户的语言风格建立关系和参与，但不受约束的模仿可能会导致感觉不稳定或逢迎的效果。我们提出了一种计算评估框架，明确表达了核心设计权衡：在即时语言同步与长期人设稳定之间平衡。使用8维风格向量和闭环“基础+增量”提示架构，我们在一个人类日志数据集上模拟并比较了显式适应策略——无上限、上限、指数移动平均（EMA）、死区以及混合策略的效果。我们的分析绘制了一个明确的帕累托前沿：受限策略在适度牺牲同步性的基础上显著提高了稳定性。例如，混合策略（EMA+上限）将稳定性从0.542提高到0.878（+62%），同时仅减少17%的同步性。我们通过对三个公开语料库（DailyDialog、Persona-Chat、EmpatheticDialogues）的大规模复制以及两种模型家族的LLM闭环验证确认了这一权衡。此外，我们量化了“提示可读性”，显示前沿策略减少了指令变化，并将突兀的体裁翻转（重大语气变化）从0.254减少到0.092，从而使系统更易于推理和维护。综合来看，我们的框架提供了一种风格适应的一般评估工具、系统性的消融分析以识别帕累托高效策略、跨多种数据集和模型的稳健验证以及与策略选择相关的新型可读性指标，从而连接策略选择与系统维护性。', 'title_zh': '在自适应聊天机器人中导航同步-稳定前沿'}
{'arxiv_id': 'arXiv:2510.00334', 'title': 'Structural Refinement of Bayesian Networks for Efficient Model Parameterisation', 'authors': 'Kieran Drury, Martine J. Barons, Jim Q. Smith', 'link': 'https://arxiv.org/abs/2510.00334', 'abstract': 'Many Bayesian network modelling applications suffer from the issue of data scarcity. Hence the use of expert judgement often becomes necessary to determine the parameters of the conditional probability tables (CPTs) throughout the network. There are usually a prohibitively large number of these parameters to determine, even when complementing any available data with expert judgements. To address this challenge, a number of CPT approximation methods have been developed that reduce the quantity and complexity of parameters needing to be determined to fully parameterise a Bayesian network. This paper provides a review of a variety of structural refinement methods that can be used in practice to efficiently approximate a CPT within a Bayesian network. We not only introduce and discuss the intrinsic properties and requirements of each method, but we evaluate each method through a worked example on a Bayesian network model of cardiovascular risk assessment. We conclude with practical guidance to help Bayesian network practitioners choose an alternative approach when direct parameterisation of a CPT is infeasible.', 'abstract_zh': '许多Bayesian网络建模应用受到数据稀缺性的困扰。因此，在整个网络中确定条件概率表（CPTs）的参数时常需要专家判断。即使利用可用数据和专家判断来补充，也需要确定的参数数量通常是巨大的。为应对这一挑战，开发了多种CPT近似方法，以减少需要确定的数量和复杂性，从而完全参数化一个Bayesian网络。本文提供了多种结构细化方法的综述，这些方法可以用于实践中的Bayesian网络中高效地近似CPT。我们不仅介绍了并讨论了每种方法的内在特性和要求，还通过心血管风险评估的Bayesian网络模型实例评估了每种方法。最后，我们提供了实用指导，帮助Bayesian网络实践者在直接参数化CPT不可行时选择替代方法。', 'title_zh': '贝叶斯网络的结构精炼以提高模型参数化效率'}
{'arxiv_id': 'arXiv:2510.00321', 'title': 'A Framework for Selection of Machine Learning Algorithms Based on Performance Metrices and Akaike Information Criteria in Healthcare, Telecommunication, and Marketing Sector', 'authors': 'A. K. Hamisu, K. Jasleen', 'link': 'https://arxiv.org/abs/2510.00321', 'abstract': 'The exponential growth of internet generated data has fueled advancements in artificial intelligence (AI), machine learning (ML), and deep learning (DL) for extracting actionable insights in marketing,telecom, and health sectors. This chapter explores ML applications across three domains namely healthcare, marketing, and telecommunications, with a primary focus on developing a framework for optimal ML algorithm selection. In healthcare, the framework addresses critical challenges such as cardiovascular disease prediction accounting for 28.1% of global deaths and fetal health classification into healthy or unhealthy states, utilizing three datasets. ML algorithms are categorized into eager, lazy, and hybrid learners, selected based on dataset attributes, performance metrics (accuracy, precision, recall), and Akaike Information Criterion (AIC) scores. For validation, eight datasets from the three sectors are employed in the experiments. The key contribution is a recommendation framework that identifies the best ML model according to input attributes, balancing performance evaluation and model complexity to enhance efficiency and accuracy in diverse real-world applications. This approach bridges gaps in automated model selection, offering practical implications for interdisciplinary ML deployment.', 'abstract_zh': '互联网生成数据的指数增长激发了人工智能、机器学习和深度学习在营销、电信和健康领域的应用进展。本章探讨了机器学习在医疗保健、营销和电信三大领域的应用，并重点开发了一种优化机器学习算法选择的框架。在医疗保健领域，框架解决了包括心血管疾病预测（占全球死亡的28.1%）和胎儿健康状态分类（健康或不健康）在内的关键挑战，利用了三个数据集。机器学习算法被分为急切学习者、懒学习者和混合学习者，根据数据集属性、性能指标（准确性、精确度、召回率）和赤氏信息标准（AIC）分数进行选择。通过对三个领域中的八个数据集进行实验验证，该研究的关键贡献是一个推荐框架，能够根据输入属性识别最佳机器学习模型，并平衡性能评估和模型复杂性，以提高各种实际应用中的效率和准确性。该方法填补了自动化模型选择的空白，为跨学科机器学习部署提供了实际意义。', 'title_zh': '基于性能指标和赤池信息准则的选择机器学习算法框架：医疗、电信和市场营销领域'}
{'arxiv_id': 'arXiv:2510.00317', 'title': 'MAVUL: Multi-Agent Vulnerability Detection via Contextual Reasoning and Interactive Refinement', 'authors': 'Youpeng Li, Kartik Joshi, Xinda Wang, Eric Wong', 'link': 'https://arxiv.org/abs/2510.00317', 'abstract': "The widespread adoption of open-source software (OSS) necessitates the mitigation of vulnerability risks. Most vulnerability detection (VD) methods are limited by inadequate contextual understanding, restrictive single-round interactions, and coarse-grained evaluations, resulting in undesired model performance and biased evaluation results. To address these challenges, we propose MAVUL, a novel multi-agent VD system that integrates contextual reasoning and interactive refinement. Specifically, a vulnerability analyst agent is designed to flexibly leverage tool-using capabilities and contextual reasoning to achieve cross-procedural code understanding and effectively mine vulnerability patterns. Through iterative feedback and refined decision-making within cross-role agent interactions, the system achieves reliable reasoning and vulnerability prediction. Furthermore, MAVUL introduces multi-dimensional ground truth information for fine-grained evaluation, thereby enhancing evaluation accuracy and reliability.\nExtensive experiments conducted on a pairwise vulnerability dataset demonstrate MAVUL's superior performance. Our findings indicate that MAVUL significantly outperforms existing multi-agent systems with over 62% higher pairwise accuracy and single-agent systems with over 600% higher average performance. The system's effectiveness is markedly improved with increased communication rounds between the vulnerability analyst agent and the security architect agent, underscoring the importance of contextual reasoning in tracing vulnerability flows and the crucial feedback role. Additionally, the integrated evaluation agent serves as a critical, unbiased judge, ensuring a more accurate and reliable estimation of the system's real-world applicability by preventing misleading binary comparisons.", 'abstract_zh': '开源软件（OSS）的广泛采用 necessitates 漏洞风险的缓解。大多数漏洞检测（VD）方法受限于对上下文理解不足、单轮交互限制以及粗粒度评估，导致模型性能不佳和评估结果偏差。为应对这些挑战，我们提出了一种新的多代理漏洞检测系统MAVUL，该系统结合了上下文推理和交互式细化。具体而言，设计了一个漏洞分析师代理，灵活利用工具使用能力和上下文推理，实现跨过程代码理解并有效挖掘漏洞模式。通过跨角色代理之间的迭代反馈和精细化决策，系统实现了可靠推理和漏洞预测。此外，MAVUL 引入多维度的 ground truth 信息进行细粒度评估，从而提高评估准确性和可靠性。', 'title_zh': 'MAVUL：基于上下文推理和交互精炼的多agent漏洞检测'}
{'arxiv_id': 'arXiv:2510.00312', 'title': 'Digital Domination: A Case for Republican Liberty in Artificial Intelligence', 'authors': 'Matthew David Hamilton', 'link': 'https://arxiv.org/abs/2510.00312', 'abstract': 'Artificial intelligence is set to revolutionize social and political life in unpredictable ways, raising questions about the principles that ought to guide its development and regulation. By examining digital advertising and social media algorithms, this article highlights how artificial intelligence already poses a significant threat to the republican conception of liberty -- or freedom from unaccountable power -- and thereby highlights the necessity of protecting republican liberty when integrating artificial intelligence into society. At an individual level, these algorithms can subconsciously influence behavior and thought, and those subject to this influence have limited power over the algorithms they engage. At the political level, these algorithms give technology company executives and other foreign parties the power to influence domestic political processes, such as elections; the multinational nature of algorithm-based platforms and the speed with which technology companies innovate make incumbent state institutions ineffective at holding these actors accountable. At both levels, artificial intelligence has thus created a new form of unfreedom: digital domination. By drawing on the works of Quentin Skinner, Philip Pettit, and other republican theorists, this article asserts that individuals must have mechanisms to hold algorithms (and those who develop them) accountable in order to be truly free.', 'abstract_zh': '人工智能将以不可预测的方式革新社会和政治生活，引发了对其发展和监管应遵循的基本原则的思考。通过研究数字广告和社会媒体算法，本文揭示了人工智能已对共和主义自由观（即免于不受制约的权力的自由）构成了重大威胁，从而强调了在社会中整合人工智能时保护共和主义自由的必要性。在个体层面，这些算法可以无意识地影响行为和思维，并且受到这种影响的人对这些算法的控制能力有限。在政治层面，这些算法赋予了科技公司高管和其他外国势力影响国内政治过程（如选举）的权力；基于算法的平台的跨国性质和科技公司创新的速度使现有国家机构在追究这些行为者的责任方面变得无效。因此，人工智能已经创造了一种新的不自由形式：数字支配。本文参考昆廷·斯金纳、菲利普·佩蒂特等共和主义理论家的作品，主张必须有机制对算法（以及其开发者）进行问责，个体才能真正自由。', 'title_zh': '数字主导权：论人工智能中的共和自由'}
{'arxiv_id': 'arXiv:2510.00304', 'title': 'Barriers for Learning in an Evolving World: Mathematical Understanding of Loss of Plasticity', 'authors': 'Amir Joudaki, Giulia Lanzillotta, Mohammad Samragh Razlighi, Iman Mirzadeh, Keivan Alizadeh, Thomas Hofmann, Mehrdad Farajtabar, Fartash Faghri', 'link': 'https://arxiv.org/abs/2510.00304', 'abstract': 'Deep learning models excel in stationary data but struggle in non-stationary environments due to a phenomenon known as loss of plasticity (LoP), the degradation of their ability to learn in the future. This work presents a first-principles investigation of LoP in gradient-based learning. Grounded in dynamical systems theory, we formally define LoP by identifying stable manifolds in the parameter space that trap gradient trajectories. Our analysis reveals two primary mechanisms that create these traps: frozen units from activation saturation and cloned-unit manifolds from representational redundancy. Our framework uncovers a fundamental tension: properties that promote generalization in static settings, such as low-rank representations and simplicity biases, directly contribute to LoP in continual learning scenarios. We validate our theoretical analysis with numerical simulations and explore architectural choices or targeted perturbations as potential mitigation strategies.', 'abstract_zh': '深度学习模型在静态数据中表现出色但在非静态环境中挣扎，这是因为被称为退化可塑性（LoP）的现象，即它们未来学习能力的下降。本文基于动力系统理论，对基于梯度的学习中的LoP进行了第一性原理探讨。我们通过识别参数空间中的稳定流形来正式定义LoP，这些稳定流形将梯度轨迹困住。分析揭示了两种主要机制：激活饱和导致的冻结单元以及表示冗余导致的克隆单元流形。我们的框架揭示了一个根本的紧张关系：在静态设置中促进泛化的特性，如低秩表示和简单偏置，直接导致了持续学习场景中的LoP。我们通过数值模拟验证了我们的理论分析，并探索了架构选择或针对性干扰作为潜在缓解策略。', 'title_zh': '变化世界中学习的障碍：关于可塑性丧失的数学理解'}
{'arxiv_id': 'arXiv:2510.00283', 'title': 'Data driven approaches in nanophotonics: A review of AI-enabled metadevices', 'authors': 'Huanshu Zhang, Lei Kang, Sawyer D. Campbell, Jacob T. Young, Douglas H. Werner', 'link': 'https://arxiv.org/abs/2510.00283', 'abstract': 'Data-driven approaches have revolutionized the design and optimization of photonic metadevices by harnessing advanced artificial intelligence methodologies. This review takes a model-centric perspective that synthesizes emerging design strategies and delineates how traditional trial-and-error and computationally intensive electromagnetic simulations are being supplanted by deep learning frameworks that efficiently navigate expansive design spaces. We discuss artificial intelligence implementation in several metamaterial design aspects from high-degree-of-freedom design to large language model-assisted design. By addressing challenges such as transformer model implementation, fabrication limitations, and intricate mutual coupling effects, these AI-enabled strategies not only streamline the forward modeling process but also offer robust pathways for the realization of multifunctional and fabrication-friendly nanophotonic devices. This review further highlights emerging opportunities and persistent challenges, setting the stage for next-generation strategies in nanophotonic engineering.', 'abstract_zh': '数据驱动的方法通过利用先进的人工智能方法，已经彻底改变了光子元器件的设计与优化。本综述从模型中心的角度出发，综合阐述了新兴的设计策略，并阐明了传统试错法和计算密集型电磁仿真如何被高效的深度学习框架替代。我们讨论了人工智能在多种元材料设计方面的应用，从高自由度设计到大型语言模型辅助设计。通过解决如变压器模型实现、制备限制和复杂的相互耦合效应等挑战，这些基于人工智能的策略不仅简化了前向建模过程，还为多功能和制备友好的纳米光子器件的实现提供了稳健的途径。本综述进一步突出了新兴机遇和持续性的挑战，为下一代纳米光子工程策略的构建奠定了基础。', 'title_zh': '数据驱动方法在纳米光子学中的应用：AI增强元器件的综述'}
{'arxiv_id': 'arXiv:2510.00279', 'title': 'SLogic: Subgraph-Informed Logical Rule Learning for Knowledge Graph Completion', 'authors': 'Trung Hoang Le, Tran Cao Son, Huiping Cao', 'link': 'https://arxiv.org/abs/2510.00279', 'abstract': "Logical rule-based methods offer an interpretable approach to knowledge graph completion by capturing compositional relationships in the form of human-readable inference rules. However, current approaches typically treat logical rules as universal, assigning each rule a fixed confidence score that ignores query-specific context. This is a significant limitation, as a rule's importance can vary depending on the query. To address this, we introduce SLogic (Subgraph-Informed Logical Rule learning), a novel framework that assigns query-dependent scores to logical rules. The core of SLogic is a scoring function that utilizes the subgraph centered on a query's head entity, allowing the significance of each rule to be assessed dynamically. Extensive experiments on benchmark datasets show that by leveraging local subgraph context, SLogic consistently outperforms state-of-the-art baselines, including both embedding-based and rule-based methods.", 'abstract_zh': '基于逻辑规则的方法通过以人类可读的推理规则形式捕获组合关系，提供了知识图谱完成的可解释性方法。然而，当前的方法通常将逻辑规则视为通用的，为每条规则分配一个固定的置信分数，忽略了查询特定的上下文。这是一大局限性，因为规则的重要性可能依赖于查询而变化。为解决这一问题，我们提出了SLogic（基于子图的逻辑规则学习）这一新型框架，为逻辑规则分配查询依赖的分数。SLogic的核心是利用以查询头实体为中心的子图的评分函数，使得每条规则的重要性可以动态评估。在基准数据集上的广泛实验表明，通过利用局部子图上下文，SLogic在多种基于嵌入和规则的方法中表现出色，始终优于最先进的基线方法。', 'title_zh': 'SLogic：基于子图的逻辑规则学习的知识图谱补全'}
{'arxiv_id': 'arXiv:2510.00260', 'title': 'Learning Energy-based Variational Latent Prior for VAEs', 'authors': 'Debottam Dutta, Chaitanya Amballa, Zhongweiyang Xu, Yu-Lin Wei, Romit Roy Choudhury', 'link': 'https://arxiv.org/abs/2510.00260', 'abstract': 'Variational Auto-Encoders (VAEs) are known to generate blurry and inconsistent samples. One reason for this is the "prior hole" problem. A prior hole refers to regions that have high probability under the VAE\'s prior but low probability under the VAE\'s posterior. This means that during data generation, high probability samples from the prior could have low probability under the posterior, resulting in poor quality data. Ideally, a prior needs to be flexible enough to match the posterior while retaining the ability to generate samples fast. Generative models continue to address this tradeoff. This paper proposes to model the prior as an energy-based model (EBM). While EBMs are known to offer the flexibility to match posteriors (and also improving the ELBO), they are traditionally slow in sample generation due to their dependency on MCMC methods. Our key idea is to bring a variational approach to tackle the normalization constant in EBMs, thus bypassing the expensive MCMC approaches. The variational form can be approximated with a sampler network, and we show that such an approach to training priors can be formulated as an alternating optimization problem. Moreover, the same sampler reduces to an implicit variational prior during generation, providing efficient and fast sampling. We compare our Energy-based Variational Latent Prior (EVaLP) method to multiple SOTA baselines and show improvements in image generation quality, reduced prior holes, and better sampling efficiency.', 'abstract_zh': '基于能景模型的变分先验方法（EVaLP）', 'title_zh': '基于能量的学习变分潜空间先验的VAEs'}
{'arxiv_id': 'arXiv:2510.00245', 'title': 'Can AI agents understand spoken conversations about data visualizations in online meetings?', 'authors': 'Rizul Sharma, Tianyu Jiang, Seokki Lee, Jillian Aurisano', 'link': 'https://arxiv.org/abs/2510.00245', 'abstract': "In this short paper, we present work evaluating an AI agent's understanding of spoken conversations about data visualizations in an online meeting scenario. There is growing interest in the development of AI-assistants that support meetings, such as by providing assistance with tasks or summarizing a discussion. The quality of this support depends on a model that understands the conversational dialogue. To evaluate this understanding, we introduce a dual-axis testing framework for diagnosing the AI agent's comprehension of spoken conversations about data. Using this framework, we designed a series of tests to evaluate understanding of a novel corpus of 72 spoken conversational dialogues about data visualizations. We examine diverse pipelines and model architectures, LLM vs VLM, and diverse input formats for visualizations (the chart image, its underlying source code, or a hybrid of both) to see how this affects model performance on our tests. Using our evaluation methods, we found that text-only input modalities achieved the best performance (96%) in understanding discussions of visualizations in online meetings.", 'abstract_zh': '在此短文中，我们介绍了对AI代理在在线会议场景中对数据可视化对话的理解进行评估的工作。在线会议辅助型AI助手的发展越来越受到关注，例如通过提供任务协助或总结讨论来支持会议。这种支持的质量取决于一个能够理解对话对话模型。为了评估这种理解，我们引入了一种双轴测试框架，用于诊断AI代理对数据对话的理解。利用这一框架，我们设计了一系列测试，以评估对72个新颖的数据可视化对话口语对话语料库的理解。我们研究了不同的流水线和模型架构，LLM与VLM，以及不同的可视化输入格式（图表图像、其底层源代码或两者的混合）来观察这对我们在测试中的模型性能有何影响。通过我们的评估方法，我们发现仅文本输入模态在理解在线会议中关于可视化的讨论方面的表现最佳，达到了96%的成绩。', 'title_zh': 'AI代理能否理解关于数据可视化在线会议中的 spoken conversations？'}
{'arxiv_id': 'arXiv:2510.00237', 'title': 'Debunk the Myth of SFT Generalization', 'authors': 'Xiaofeng Lin, Hejian Sang, Zhipeng Wang, Xuezhou Zhang', 'link': 'https://arxiv.org/abs/2510.00237', 'abstract': "A prevailing view holds that supervised fine-tuning (SFT) memorizes training data and fails to generalize, whereas reinforcement learning (RL) attains broader robustness. We revisit this claim through a systematic evaluation on two decision-making benchmarks, Sokoban and General Points, and arrive at a different conclusion. We show that much of SFT's perceived failure stems from frozen-prompt artifacts: when trained on fixed instruction templates, SFT models cling to training semantics rather than adapting to new ones. Introducing prompt diversity during training breaks this shortcut and yields strong generalization to unseen instruction variants without harming in-distribution performance. Beyond instruction shifts, we ask whether SFT can generalize to strictly harder tasks. Here, chain-of-thought (CoT) supervision provides an algorithmic scaffold that markedly improves transfer to more difficult regimes, such as larger Sokoban grids with additional boxes and arithmetic with out-of-distribution values or five-card compositions that increase combinatorial complexity. Finally, combining prompt diversity with CoT achieves the best of both worlds: robust generalization across both instruction-variant and difficulty-variant settings, matching or surpassing RL baselines on our benchmarks while retaining SFT's simplicity and stability. These findings challenge the narrative that SFT is inherently inferior to RL and support a data-centric perspective: with appropriately curated demonstrations, vanilla SFT can generalize as strongly as RL. Code reproducing the results in the paper can be found at: this https URL.", 'abstract_zh': '监督微调的普遍观点认为它 memorizes 训练数据并无法泛化，而强化学习则展现出更广泛和稳健的性能。我们通过系统评价 Sokoban 和 General Points 两个决策基准，得出了不同的结论。我们证明了监督微调 perceived 失败的主要原因在于固定提示模板的 artifacts：在固定指令模板上训练时，监督微调模型固守训练中的语义，而无法适应该新语义。在训练中引入提示多样性可以打破这种捷径，并在不损害分布内性能的情况下，强大地泛化到未见过的指令变体。除了指令的变化，我们还探讨监督微调能否泛化到更难的任务上。这里，思维链 (CoT) 监督提供了一种算法框架，显著提高了向更难的领域转移的能力，例如更大的 Sokoban 网格和附加盒子的算术，或超出分布值或五张牌组合，它们增加了组合复杂性。最后，结合提示多样性和思维链 (CoT) 实现了两者的最佳成果：在指令变体和难度变体情景中都表现出强大的稳健泛化能力，在基准测试中匹配或超过 RL 基线，同时保持监督微调的简单性和稳定性。这些发现挑战了监督微调本身在本质上劣于强化学习的叙事，并支持一种以数据为中心的观点：适当策划的演示可以使得裸监督微调的泛化能力达到与强化学习相媲美的程度。该论文结果的代码可以在此找到：this https URL。', 'title_zh': '戳穿SFT泛化的神话'}
{'arxiv_id': 'arXiv:2510.00231', 'title': 'The Pitfalls of KV Cache Compression', 'authors': 'Alex Chen, Renato Geh, Aditya Grover, Guy Van den Broeck, Daniel Israel', 'link': 'https://arxiv.org/abs/2510.00231', 'abstract': 'KV cache compression promises increased throughput and efficiency with negligible loss in performance. While the gains in throughput are indisputable and recent literature has indeed shown minimal degradation on particular benchmarks, in general the consequences of compression in realistic scenarios such as multi-instruction prompting have been insufficiently studied. In this paper, we identify several pitfalls practitioners should be aware of when deploying KV cache compressed LLMs. Importantly, we show that certain instructions degrade much more rapidly with compression, effectively causing them to be completely ignored by the LLM. As a practical example of that, we highlight system prompt leakage as a case study, empirically showing the impact of compression on leakage and general instruction following. We show several factors that play a role in prompt leakage: compression method, instruction order, and KV eviction bias. We then propose simple changes to KV cache eviction policies that can reduce the impact of these factors and improve the overall performance in multi-instruction tasks.', 'abstract_zh': 'KV缓存压缩在合理损失性能的前提下承诺提高吞吐量和效率。尽管吞吐量的提升是无可争议的，近期文献也的确在某些基准上展示了最小的性能下降，但在多指令提示等现实场景中的压缩后果尚不足充分研究。在本文中，我们指出了当部署压缩的KV缓存LLM时，从业者需要意识到的几个潜在问题。重要的是，我们展示了某些指令在压缩后性能急剧下降，实质上被LLM完全忽略。作为实际例子，我们以系统提示泄露为案例研究，实证展示了压缩对泄露和指令执行的一般影响。我们指出了提示泄露涉及的几个因素：压缩方法、指令顺序和KV淘汰偏向。随后，我们提出了简单的KV缓存淘汰策略调整方案，这些调整可以降低这些因素的影响并提高多指令任务的整体性能。', 'title_zh': 'KV缓存压缩的 pitfalls'}
{'arxiv_id': 'arXiv:2510.00219', 'title': 'Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space', 'authors': 'Houjun Liu, Shikhar Murty, Christopher D. Manning, Róbert Csordás', 'link': 'https://arxiv.org/abs/2510.00219', 'abstract': 'Current approaches for scaling inference-time compute in transformers rely on training them to emit explicit chain-of-thought tokens before producing an answer. While these methods are powerful, they are limited because they cannot be applied during pretraining and are limited to only serially-generated, natural-language verbalization to scale inference-time compute. In this work, we propose Thoughtbubbles, a transformer variant that natively performs parallel adaptive computation in latent space by learning to fork or delete residual streams. Thus, tokens that require a large amount of computation can form a "bubble" of cloned residuals in the middle of the network for additional thinking. Crucially, this behavior is learned during pretraining with only language modeling loss. Thoughtbubbles outperforms both standard decoder LMs as well as non-adaptive parallel computation approaches on OpenWebText and peS2o perplexity and in zero-shot evaluations such as HellaSwag and LAMBADA after pretraining across 150M to 772M parameter scales. The implicit nature of our method enables adaptive computation to be learned starting at pretraining time, paving the way to unify train and test-time behavior for reasoning models.', 'abstract_zh': 'Thoughtbubbles: 一种在潜空间中进行并行自适应计算的变压器变体', 'title_zh': '思绪气泡：潜空间中的无监督并行思考方法'}
{'arxiv_id': 'arXiv:2510.00212', 'title': 'Directed-MAML: Meta Reinforcement Learning Algorithm with Task-directed Approximation', 'authors': 'Yang Zhang, Huiwen Yan, Mushuang Liu', 'link': 'https://arxiv.org/abs/2510.00212', 'abstract': "Model-Agnostic Meta-Learning (MAML) is a versatile meta-learning framework applicable to both supervised learning and reinforcement learning (RL). However, applying MAML to meta-reinforcement learning (meta-RL) presents notable challenges. First, MAML relies on second-order gradient computations, leading to significant computational and memory overhead. Second, the nested structure of optimization increases the problem's complexity, making convergence to a global optimum more challenging. To overcome these limitations, we propose Directed-MAML, a novel task-directed meta-RL algorithm. Before the second-order gradient step, Directed-MAML applies an additional first-order task-directed approximation to estimate the effect of second-order gradients, thereby accelerating convergence to the optimum and reducing computational cost. Experimental results demonstrate that Directed-MAML surpasses MAML-based baselines in computational efficiency and convergence speed in the scenarios of CartPole-v1, LunarLander-v2 and two-vehicle intersection crossing. Furthermore, we show that task-directed approximation can be effectively integrated into other meta-learning algorithms, such as First-Order Model-Agnostic Meta-Learning (FOMAML) and Meta Stochastic Gradient Descent(Meta-SGD), yielding improved computational efficiency and convergence speed.", 'abstract_zh': '定向元学习（Directed-MAML）：一种适用于元强化学习的新型任务导向元学习算法', 'title_zh': '面向任务的MAML：具有任务导向近似的方法强化学习算法'}
{'arxiv_id': 'arXiv:2510.00165', 'title': 'Privacy-Preserving Learning-Augmented Data Structures', 'authors': 'Prabhav Goyal, Vinesh Sridhar, Wilson Zheng', 'link': 'https://arxiv.org/abs/2510.00165', 'abstract': 'Learning-augmented data structures use predicted frequency estimates to retrieve frequently occurring database elements faster than standard data structures. Recent work has developed data structures that optimally exploit these frequency estimates while maintaining robustness to adversarial prediction errors. However, the privacy and security implications of this setting remain largely unexplored.\nIn the event of a security breach, data structures should reveal minimal information beyond their current contents. This is even more crucial for learning-augmented data structures, whose layout adapts to the data. A data structure is history independent if its memory representation reveals no information about past operations except what is inferred from its current contents. In this work, we take the first step towards privacy and security guarantees in this setting by proposing the first learning-augmented data structure that is strongly history independent, robust, and supports dynamic updates.\nTo achieve this, we introduce two techniques: thresholding, which automatically makes any learning-augmented data structure robust, and pairing, a simple technique that provides strong history independence in the dynamic setting. Our experimental results demonstrate a tradeoff between security and efficiency but are still competitive with the state of the art.', 'abstract_zh': '学习增强数据结构通过预测频率估计快速检索频繁出现的数据库元素，比标准数据结构更快。最近的工作已经开发了在保持对敌对预测错误的鲁棒性的同时最优利用这些频率估计的数据结构。然而，这一环境下的隐私和安全影响仍然很大程度上未被探索。\n在这种情况下，数据结构在遭遇安全漏洞时应仅泄露其当前内容之外的最小信息。对于其布局适应数据的学习增强数据结构，这一点更为重要。历史无关的数据结构的内存表示除了从当前内容中推断的内容之外不透露任何关于过去操作的信息。在本文中，我们通过提出第一个既强大又历史无关、具有鲁棒性且支持动态更新的学习增强数据结构，迈出了隐私和安全性保证的第一步。\n为了实现这一点，我们引入了两种技术：阈值技术，可以自动使任何学习增强数据结构具有鲁棒性，以及配对技术，这是一种简单的技术，在动态环境中提供强历史无关性。我们的实验结果表明，在安全性和效率之间存在权衡，但仍与现有技术具有竞争力。', 'title_zh': '隐私保留的学习增强数据结构'}
{'arxiv_id': 'arXiv:2510.00163', 'title': 'Partial Identification Approach to Counterfactual Fairness Assessment', 'authors': 'Saeyoung Rho, Junzhe Zhang, Elias Bareinboim', 'link': 'https://arxiv.org/abs/2510.00163', 'abstract': 'The wide adoption of AI decision-making systems in critical domains such as criminal justice, loan approval, and hiring processes has heightened concerns about algorithmic fairness. As we often only have access to the output of algorithms without insights into their internal mechanisms, it was natural to examine how decisions would alter when auxiliary sensitive attributes (such as race) change. This led the research community to come up with counterfactual fairness measures, but how to evaluate the measure from available data remains a challenging task. In many practical applications, the target counterfactual measure is not identifiable, i.e., it cannot be uniquely determined from the combination of quantitative data and qualitative knowledge. This paper addresses this challenge using partial identification, which derives informative bounds over counterfactual fairness measures from observational data. We introduce a Bayesian approach to bound unknown counterfactual fairness measures with high confidence. We demonstrate our algorithm on the COMPAS dataset, examining fairness in recidivism risk scores with respect to race, age, and sex. Our results reveal a positive (spurious) effect on the COMPAS score when changing race to African-American (from all others) and a negative (direct causal) effect when transitioning from young to old age.', 'abstract_zh': 'AI决策系统在刑事司法、贷款审批和招聘过程等关键领域的广泛应用加剧了对算法公平性的关注。当我们只能访问算法的输出而无法了解其内部机制时，自然会考虑辅助敏感属性（如种族）变化时决策将如何改变。这促使研究界提出了反事实公平性度量，但如何从现有数据中评估这些度量仍然是一个挑战。在许多实际应用中，目标的反事实度量是不可识别的，即无法从定量数据和定性知识的组合中唯一确定。本文使用部分识别方法应对这一挑战，通过观察数据推导出反事实公平性度量的信息性边界。我们提出了一种贝叶斯方法，以高置信度界定了未知的反事实公平性度量。我们在COMPAS数据集上展示了我们的算法，探讨了种族、年龄和性别对再犯风险评分的公平性。结果显示，将种族更改为非洲裔美国人（从其他人中）对COMPAS评分有正向（虚假）影响，而从年轻变为年老则有负向（直接因果）影响。', 'title_zh': '部分识别方法在反事实公平性评估中的应用'}
{'arxiv_id': 'arXiv:2510.00151', 'title': 'Stealing AI Model Weights Through Covert Communication Channels', 'authors': 'Valentin Barbaza, Alan Rodrigo Diaz-Rizo, Hassan Aboushady, Spyridon Raptis, Haralampos-G. Stratigopoulos', 'link': 'https://arxiv.org/abs/2510.00151', 'abstract': "AI models are often regarded as valuable intellectual property due to the high cost of their development, the competitive advantage they provide, and the proprietary techniques involved in their creation. As a result, AI model stealing attacks pose a serious concern for AI model providers. In this work, we present a novel attack targeting wireless devices equipped with AI hardware accelerators. The attack unfolds in two phases. In the first phase, the victim's device is compromised with a hardware Trojan (HT) designed to covertly leak model weights through a hidden communication channel, without the victim realizing it. In the second phase, the adversary uses a nearby wireless device to intercept the victim's transmission frames during normal operation and incrementally reconstruct the complete weight matrix. The proposed attack is agnostic to both the AI model architecture and the hardware accelerator used. We validate our approach through a hardware-based demonstration involving four diverse AI models of varying types and sizes. We detail the design of the HT and the covert channel, highlighting their stealthy nature. Additionally, we analyze the impact of bit error rates on the reception and propose an error mitigation technique. The effectiveness of the attack is evaluated based on the accuracy of the reconstructed models with stolen weights and the time required to extract them. Finally, we explore potential defense mechanisms.", 'abstract_zh': '基于无线设备的硬件 Trojan攻击：针对AI硬件加速器的新型攻击技术', 'title_zh': '通过隐蔽通信渠道窃取AI模型权重'}
{'arxiv_id': 'arXiv:2510.00144', 'title': 'Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback', 'authors': 'Shreyas Chaudhari, Renhao Zhang, Philip S. Thomas, Bruno Castro da Silva', 'link': 'https://arxiv.org/abs/2510.00144', 'abstract': 'The ability of reinforcement learning algorithms to learn effective policies is determined by the rewards available during training. However, for practical problems, obtaining large quantities of reward labels is often infeasible due to computational or financial constraints, particularly when relying on human feedback. When reinforcement learning must proceed with limited feedback -- only a fraction of samples get rewards labeled -- a fundamental question arises: which samples should be labeled to maximize policy performance? We formalize this problem of reward selection for reinforcement learning from limited feedback (RLLF), introducing a new problem formulation that facilitates the study of strategies for selecting impactful rewards. Two types of selection strategies are investigated: (i) heuristics that rely on reward-free information such as state visitation and partial value functions, and (ii) strategies pre-trained using auxiliary evaluative feedback. We find that critical subsets of rewards are those that (1) guide the agent along optimal trajectories, and (2) support recovery toward near-optimal behavior after deviations. Effective selection methods yield near-optimal policies with significantly fewer reward labels than full supervision, establishing reward selection as a powerful paradigm for scaling reinforcement learning in feedback-limited settings.', 'abstract_zh': '限制反馈条件下强化学习的奖励选择（Reward Selection for Reinforcement Learning from Limited Feedback, RLLF）', 'title_zh': '哪些奖励值得关注？在有限反馈下的强化学习奖励选择'}
{'arxiv_id': 'arXiv:2510.00137', 'title': 'Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval', 'authors': 'Nima Sheikholeslami, Erfan Hosseini, Patrice Bechard, Srivatsava Daruru, Sai Rajeswar', 'link': 'https://arxiv.org/abs/2510.00137', 'abstract': 'Dual-encoder retrievers depend on the principle that relevant documents should score higher than irrelevant ones for a given query. Yet the dominant Noise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss, optimizes a softened ranking surrogate that we rigorously prove is fundamentally oblivious to score separation quality and unrelated to AUC. This mismatch leads to poor calibration and suboptimal performance in downstream tasks like retrieval-augmented generation (RAG). To address this fundamental limitation, we introduce the MW loss, a new training objective that maximizes the Mann-Whitney U statistic, which is mathematically equivalent to the Area under the ROC Curve (AUC). MW loss encourages each positive-negative pair to be correctly ranked by minimizing binary cross entropy over score differences. We provide theoretical guarantees that MW loss directly upper-bounds the AoC, better aligning optimization with retrieval goals. We further promote ROC curves and AUC as natural threshold free diagnostics for evaluating retriever calibration and ranking quality. Empirically, retrievers trained with MW loss consistently outperform contrastive counterparts in AUC and standard retrieval metrics. Our experiments show that MW loss is an empirically superior alternative to Contrastive Loss, yielding better-calibrated and more discriminative retrievers for high-stakes applications like RAG.', 'abstract_zh': 'Dual-encoder检索器依赖于相关文档相对于给定查询应比不相关文档得分更高的原则。然而，奠定对比损失基础的噪声对比估计（NCE）目标优化了一个软化的排名近似，我们严格证明其从根本上忽略了得分区分质量，与AUC无关。这种不匹配导致下游任务（如检索增强生成）中校准不良和性能不佳。为解决这一根本局限，我们引入了MW损失，这是一个新的训练目标，最大化Mann-Whitney U统计量，这在数学上相当于受控操作曲线下的面积（AUC）。MW损失通过最小化得分差异的二元交叉熵来鼓励正负对正确排序，从而直接上界AoC，更好地使优化与检索目标保持一致。我们进一步推广ROC曲线和AUC作为评估检索器校准和排名质量的自然无阈值诊断工具。实验结果表明，在AUC和标准检索指标上，使用MW损失训练的检索器均优于对比损失的对应版本。我们的实验表明，MW损失在高风险应用（如RAG）中是一种经验上更优的替代品，能够提供更为校准良好且更具区分性的检索器。', 'title_zh': '优化关键指标：基于AUC的鲁棒神经检索学习'}
{'arxiv_id': 'arXiv:2510.00136', 'title': 'Nonparametric Identification of Latent Concepts', 'authors': 'Yujia Zheng, Shaoan Xie, Kun Zhang', 'link': 'https://arxiv.org/abs/2510.00136', 'abstract': 'We are born with the ability to learn concepts by comparing diverse observations. This helps us to understand the new world in a compositional manner and facilitates extrapolation, as objects naturally consist of multiple concepts. In this work, we argue that the cognitive mechanism of comparison, fundamental to human learning, is also vital for machines to recover true concepts underlying the data. This offers correctness guarantees for the field of concept learning, which, despite its impressive empirical successes, still lacks general theoretical support. Specifically, we aim to develop a theoretical framework for the identifiability of concepts with multiple classes of observations. We show that with sufficient diversity across classes, hidden concepts can be identified without assuming specific concept types, functional relations, or parametric generative models. Interestingly, even when conditions are not globally satisfied, we can still provide alternative guarantees for as many concepts as possible based on local comparisons, thereby extending the applicability of our theory to more flexible scenarios. Moreover, the hidden structure between classes and concepts can also be identified nonparametrically. We validate our theoretical results in both synthetic and real-world settings.', 'abstract_zh': '我们天生具备通过比较多样化观察来学习概念的能力。这有助于我们以组合方式理解新世界，并促进泛化，因为物体自然由多个概念构成。在本文中，我们argue比较认知机制对于人类学习至关重要的过程同样对于机器恢复数据背后的真正概念至关重要。这为尽管在其实验性成功方面令人印象深刻的领域——概念学习——仍缺乏一般的理论支持提供了正确性保证。具体而言，我们旨在开发一种理论框架，以识别具有多种观察类别的概念。我们证明，在类别之间具备足够的多样性的情况下，可以在不假设特定概念类型、函数关系或参数生成模型的前提下识别隐藏概念。有趣的是，即使在条件不完全满足时，我们也可以基于局部比较为尽可能多的概念提供替代保证，从而将我们理论的应用扩展到更灵活的情景中。此外，类别和概念之间的隐藏结构也可以非参数化地识别。我们在合成和真实世界环境中验证了我们的理论结果。', 'title_zh': '非参数识别潜在概念'}
{'arxiv_id': 'arXiv:2510.00091', 'title': 'Simulating Student Success in the Age of GenAI: A Kantian-Axiomatic Perspective', 'authors': 'Seyma Yaman Kayadibi', 'link': 'https://arxiv.org/abs/2510.00091', 'abstract': "This study reinterprets a Monte Carlo simulation of students' perceived success with generative AI (GenAI) through a Kantian-axiomatic lens. Building on prior work, theme-level survey statistics Ease of Use and Learnability, System Efficiency and Learning Burden, and Perceived Complexity and Integration from a representative dataset are used to generate 10,000 synthetic scores per theme on the [1,5] Likert scale. The simulated outputs are evaluated against the axioms of dense linear order without endpoints (DLO): irreflexivity, transitivity, total comparability (connectedness), no endpoints (no greatest and no least; A4-A5), and density (A6). At the data level, the basic ordering axioms (A1-A3) are satisfied, whereas no-endpoints (A4-A5) and density (A6) fail as expected. Likert clipping introduces minimum and maximum observed values, and a finite, discretized sample need not contain a value strictly between any two distinct scores. These patterns are read not as methodological defects but as markers of an epistemological boundary. Following Kant and Friedman, the findings suggest that what simulations capture finite, quantized observations cannot instantiate the ideal properties of an unbounded, dense continuum. Such properties belong to constructive intuition rather than to finite sampling alone. A complementary visualization contrasts the empirical histogram with a sine-curve proxy to clarify this divide. The contribution is interpretive rather than data-expansive: it reframes an existing simulation as a probe of the synthetic a priori structure underlying students' perceptions, showing how formal order-theoretic coherence coexists with principled failures of endpoint-freeness and density in finite empirical models.", 'abstract_zh': '基于康德公理视角的学生感知生成式AI成功蒙特卡洛模拟再诠释', 'title_zh': 'GenAI时代的学生成功模拟：康德公理解释视角'}
{'arxiv_id': 'arXiv:2510.00080', 'title': 'SoREX: Towards Self-Explainable Social Recommendation with Relevant Ego-Path Extraction', 'authors': 'Hanze Guo, Yijun Ma, Xiao Zhou', 'link': 'https://arxiv.org/abs/2510.00080', 'abstract': 'Social recommendation has been proven effective in addressing data sparsity in user-item interaction modeling by leveraging social networks. The recent integration of Graph Neural Networks (GNNs) has further enhanced prediction accuracy in contemporary social recommendation algorithms. However, many GNN-based approaches in social recommendation lack the ability to furnish meaningful explanations for their predictions. In this study, we confront this challenge by introducing SoREX, a self-explanatory GNN-based social recommendation framework. SoREX adopts a two-tower framework enhanced by friend recommendation, independently modeling social relations and user-item interactions, while jointly optimizing an auxiliary task to reinforce social signals. To offer explanations, we propose a novel ego-path extraction approach. This method involves transforming the ego-net of a target user into a collection of multi-hop ego-paths, from which we extract factor-specific and candidate-aware ego-path subsets as explanations. This process facilitates the summarization of detailed comparative explanations among different candidate items through intricate substructure analysis. Furthermore, we conduct explanation re-aggregation to explicitly correlate explanations with downstream predictions, imbuing our framework with inherent self-explainability. Comprehensive experiments conducted on four widely adopted benchmark datasets validate the effectiveness of SoREX in predictive accuracy. Additionally, qualitative and quantitative analyses confirm the efficacy of the extracted explanations in SoREX. Our code and data are available at this https URL.', 'abstract_zh': '基于社交网络的自我解释图神经网络推荐框架SoREX', 'title_zh': 'SoREX: 向往关联自心路径提取的自我解释性社会推荐'}
{'arxiv_id': 'arXiv:2510.00073', 'title': 'Identifying All ε-Best Arms in (Misspecified) Linear Bandits', 'authors': 'Zhekai Li, Tianyi Ma, Cheng Hua, Ruihao Zhu', 'link': 'https://arxiv.org/abs/2510.00073', 'abstract': 'Motivated by the need to efficiently identify multiple candidates in high trial-and-error cost tasks such as drug discovery, we propose a near-optimal algorithm to identify all {\\epsilon}-best arms (i.e., those at most {\\epsilon} worse than the optimum). Specifically, we introduce LinFACT, an algorithm designed to optimize the identification of all {\\epsilon}-best arms in linear bandits. We establish a novel information-theoretic lower bound on the sample complexity of this problem and demonstrate that LinFACT achieves instance optimality by matching this lower bound up to a logarithmic factor. A key ingredient of our proof is to integrate the lower bound directly into the scaling process for upper bound derivation, determining the termination round and thus the sample complexity. We also extend our analysis to settings with model misspecification and generalized linear models. Numerical experiments, including synthetic and real drug discovery data, demonstrate that LinFACT identifies more promising candidates with reduced sample complexity, offering significant computational efficiency and accelerating early-stage exploratory experiments.', 'abstract_zh': '受高试错成本任务（如药物发现）中高效识别多个候选方案需求的驱动，我们提出了一种近最优算法，用于识别所有ε-best arms（即那些最多比最优解差ε的方案）。具体地，我们引入了LinFACT算法，该算法旨在线性贝叶斯环境中优化所有ε-best arms的识别。我们建立了该问题的新型信息论下界，并证明LinFACT通过在对数因子内的下界达到实例最优性。我们证明的关键要素是将下界直接整合到上限界的缩放过程中，确定终止轮次，从而确定样本复杂度。我们也扩展了我们的分析，涵盖了模型设定错误和广义线性模型的情况。数值实验，包括合成和真实药物发现数据，表明LinFACT能够以减少样本复杂性的成本识别出更有前景的候选方案，提供显著的计算效率并加速早期探索性实验。', 'title_zh': '识别线性 bandit 中的所有 ε-最优臂（未正确指定的情况下）'}
{'arxiv_id': 'arXiv:2510.00067', 'title': 'Intelligent 5S Audit: Application of Artificial Intelligence for Continuous Improvement in the Automotive Industry', 'authors': 'Rafael da Silva Maciel, Lucio Veraldo Jr', 'link': 'https://arxiv.org/abs/2510.00067', 'abstract': "The evolution of the 5S methodology with the support of artificial intelligence techniques represents a significant opportunity to improve industrial organization audits in the automotive chain, making them more objective, efficient and aligned with Industry 4.0 standards. This work developed an automated 5S audit system based on large-scale language models (LLM), capable of assessing the five senses (Seiri, Seiton, Seiso, Seiketsu, Shitsuke) in a standardized way through intelligent image analysis. The system's reliability was validated using Cohen's concordance coefficient (kappa = 0.75), showing strong alignment between the automated assessments and the corresponding human audits. The results indicate that the proposed solution contributes significantly to continuous improvement in automotive manufacturing environments, speeding up the audit process by 50% of the traditional time and maintaining the consistency of the assessments, with a 99.8% reduction in operating costs compared to traditional manual audits. The methodology presented establishes a new paradigm for integrating lean systems with emerging AI technologies, offering scalability for implementation in automotive plants of different sizes.", 'abstract_zh': '基于人工智能技术的5S方法学演化为汽车产业链工业组织审计提供了提升机会，使其更具客观性、效率并符合第四次工业革命标准。本研究开发了一种基于大规模语言模型（LLM）的自动化5S审计系统，能够通过智能图像分析以标准化方式评估五感（整理、整顿、清扫、标准化、自律）。系统可靠性通过科恩一致性系数（κ=0.75）验证，显示自动评估与相应的人工审计之间有强烈的一致性。结果表明，所提出解决方案对汽车制造环境的持续改进具有重要意义，审计过程比传统方法快50%，且保持评估一致性，与传统人工审计相比，运营成本降低99.8%。所提出的方法建立了将精益系统与新兴人工智能技术整合的新 paradign，为不同规模的汽车工厂实施提供了可扩展性。', 'title_zh': '智能5S审核：人工智能在汽车行业持续改进中的应用'}
{'arxiv_id': 'arXiv:2510.00062', 'title': 'Efficient CNN Compression via Multi-method Low Rank Factorization and Feature Map Similarity', 'authors': 'M. Kokhazadeh, G. Keramidas, V. Kelefouras', 'link': 'https://arxiv.org/abs/2510.00062', 'abstract': 'Low-Rank Factorization (LRF) is a widely adopted technique for compressing deep neural networks (DNNs). However, it faces several challenges, including optimal rank selection, a vast design space, long fine-tuning times, and limited compatibility with different layer types and decomposition methods. This paper presents an end-to-end Design Space Exploration (DSE) methodology and framework for compressing convolutional neural networks (CNNs) that addresses all these issues. We introduce a novel rank selection strategy based on feature map similarity, which captures non-linear interactions between layer outputs more effectively than traditional weight-based approaches. Unlike prior works, our method uses a one-shot fine-tuning process, significantly reducing the overall fine-tuning time. The proposed framework is fully compatible with all types of convolutional (Conv) and fully connected (FC) layers. To further improve compression, the framework integrates three different LRF techniques for Conv layers and three for FC layers, applying them selectively on a per-layer basis. We demonstrate that combining multiple LRF methods within a single model yields better compression results than using a single method uniformly across all layers. Finally, we provide a comprehensive evaluation and comparison of the six LRF techniques, offering practical insights into their effectiveness across different scenarios. The proposed work is integrated into TensorFlow 2.x, ensuring compatibility with widely used deep learning workflows. Experimental results on 14 CNN models across eight datasets demonstrate that the proposed methodology achieves substantial compression with minimal accuracy loss, outperforming several state-of-the-art techniques.', 'abstract_zh': '基于端到端设计空间探索的卷积神经网络低秩分解压缩方法', 'title_zh': '基于多方法低秩分解和特征图相似性的高效CNN压缩'}
{'arxiv_id': 'arXiv:2510.00061', 'title': 'Survey of AI-Powered Approaches for Osteoporosis Diagnosis in Medical Imaging', 'authors': 'Abdul Rahman, Bumshik Lee', 'link': 'https://arxiv.org/abs/2510.00061', 'abstract': 'Osteoporosis silently erodes skeletal integrity worldwide; however, early detection through imaging can prevent most fragility fractures. Artificial intelligence (AI) methods now mine routine Dual-energy X-ray Absorptiometry (DXA), X-ray, Computed Tomography (CT), and Magnetic Resonance Imaging (MRI) scans for subtle, clinically actionable markers, but the literature is fragmented. This survey unifies the field through a tri-axial framework that couples imaging modalities with clinical tasks and AI methodologies (classical machine learning, convolutional neural networks (CNNs), transformers, self-supervised learning, and explainable AI). Following a concise clinical and technical primer, we detail our Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)-guided search strategy, introduce the taxonomy via a roadmap figure, and synthesize cross-study insights on data scarcity, external validation, and interpretability. By identifying emerging trends, open challenges, and actionable research directions, this review provides AI scientists, medical imaging researchers, and musculoskeletal clinicians with a clear compass to accelerate rigorous, patient-centered innovation in osteoporosis care. The project page of this survey can also be found on Github.', 'abstract_zh': '骨质疏松悄无声息地削弱骨骼完整性；然而，通过成像早期检测可以预防大多数脆性骨折。现有的人工智能方法正在挖掘常规的双能X射线 absorptiometry (DXA)、X射线、计算机断层扫描(CT)和磁共振成像(MRI)扫描以寻找细微的、具有临床意义的标志物，但文献资料碎片化。本文通过一个三维框架统一了该领域，该框架将成像模态与临床任务和人工智能方法（经典机器学习、卷积神经网络(CNNs)、变压器、半监督学习和可解释的人工智能）耦合在一起。在简要介绍临床和技术基础知识之后，我们详细阐述了一种由系统综述和meta分析指南(PRISMA)指导的搜索策略，通过路线图图表引入分类，并综合跨研究视角的数据稀缺性、外部验证和可解释性。通过识别新兴趋势、开放挑战和可操作的研究方向，本文为人工智能科学家、医学成像研究人员和骨骼肌肉临床医生提供了一个清晰的方向，以加速注重患者需求的骨质疏松症护理创新。该项目页面也可以在Github上找到。', 'title_zh': '人工智能驱动的骨质疏松诊断在医学 imaging 中的方法调研'}
{'arxiv_id': 'arXiv:2510.00048', 'title': 'Deep Learning Approaches with Explainable AI for Differentiating Alzheimer Disease and Mild Cognitive Impairment', 'authors': 'Fahad Mostafa, Kannon Hossain, Hafiz Khan', 'link': 'https://arxiv.org/abs/2510.00048', 'abstract': 'Early and accurate diagnosis of Alzheimer Disease is critical for effective clinical intervention, particularly in distinguishing it from Mild Cognitive Impairment, a prodromal stage marked by subtle structural changes. In this study, we propose a hybrid deep learning ensemble framework for Alzheimer Disease classification using structural magnetic resonance imaging. Gray and white matter slices are used as inputs to three pretrained convolutional neural networks such as ResNet50, NASNet, and MobileNet, each fine tuned through an end to end process. To further enhance performance, we incorporate a stacked ensemble learning strategy with a meta learner and weighted averaging to optimally combine the base models. Evaluated on the Alzheimer Disease Neuroimaging Initiative dataset, the proposed method achieves state of the art accuracy of 99.21% for Alzheimer Disease vs. Mild Cognitive Impairment and 91.0% for Mild Cognitive Impairment vs. Normal Controls, outperforming conventional transfer learning and baseline ensemble methods. To improve interpretability in image based diagnostics, we integrate Explainable AI techniques by Gradient weighted Class Activation, which generates heatmaps and attribution maps that highlight critical regions in gray and white matter slices, revealing structural biomarkers that influence model decisions. These results highlight the frameworks potential for robust and scalable clinical decision support in neurodegenerative disease diagnostics.', 'abstract_zh': '早期且准确的阿尔茨海默病诊断对于有效临床干预至关重要，特别是对于轻度认知障碍的区分，后者是一个前驱阶段，伴有细微的结构变化。本研究提出了一种混合深度学习集成框架，用于利用结构磁共振成像对阿尔茨海默病进行分类。灰质和白质切片作为输入，传递给预训练的ResNet50、NASNet和MobileNet三种卷积神经网络，并通过端到端的过程进行微调。为进一步提高性能，我们引入了一种带有元学习器和加权平均的堆叠集成学习策略，以最优地组合基模型。在ADNI数据集上评估，本方法实现了阿尔茨海默病与轻度认知障碍之间99.21%的准确率，与正常对照之间91.0%的准确率，优于传统迁移学习和基准集成方法。为了改善基于图像的诊断解释性，我们通过梯度加权分类激活的可解释人工智能技术，生成热点图和归因图，突出灰质和白质切片中的关键区域，揭示影响模型决策的结构性生物标志物。这些结果突显了该框架在神经退行性疾病诊断中的稳健性和可扩展性临床决策支持潜力。', 'title_zh': '基于可解释人工智能的深度学习方法在区分阿尔茨海默病和轻度认知 impairment 方面的应用'}
{'arxiv_id': 'arXiv:2510.00045', 'title': 'Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study on Hospital Professions', 'authors': 'Franck Vandewiele, Remi Synave, Samuel Delepoulle, Remi Cozot', 'link': 'https://arxiv.org/abs/2510.00045', 'abstract': 'Text-to-image (TTI) models are increasingly used in professional, educational, and creative contexts, yet their outputs often embed and amplify social biases. This paper investigates gender representation in six state-of-the-art open-weight models: HunyuanImage 2.1, HiDream-I1-dev, Qwen-Image, FLUX.1-dev, Stable-Diffusion 3.5 Large, and Stable-Diffusion-XL. Using carefully designed prompts, we generated 100 images for each combination of five hospital-related professions (cardiologist, hospital director, nurse, paramedic, surgeon) and five portrait qualifiers ("", corporate, neutral, aesthetic, beautiful).\nOur analysis reveals systematic occupational stereotypes: all models produced nurses exclusively as women and surgeons predominantly as men. However, differences emerge across models: Qwen-Image and SDXL enforce rigid male dominance, HiDream-I1-dev shows mixed outcomes, and FLUX.1-dev skews female in most roles. HunyuanImage 2.1 and Stable-Diffusion 3.5 Large also reproduce gender stereotypes but with varying degrees of sensitivity to prompt formulation. Portrait qualifiers further modulate gender balance, with terms like corporate reinforcing male depictions and beautiful favoring female ones. Sensitivity varies widely: Qwen-Image remains nearly unaffected, while FLUX.1-dev, SDXL, and SD3.5 show strong prompt dependence.\nThese findings demonstrate that gender bias in TTI models is both systematic and model-specific. Beyond documenting disparities, we argue that prompt wording plays a critical role in shaping demographic outcomes. The results underscore the need for bias-aware design, balanced defaults, and user guidance to prevent the reinforcement of occupational stereotypes in generative AI.', 'abstract_zh': '文本到图像（TTI）模型在专业、教育和创意领域中的应用日益增多，但其输出往往嵌入并放大了社会偏见。本文调查了六款当今最先进的预训练模型中的人物性别表现：HunyuanImage 2.1、HiDream-I1-dev、Qwen-Image、FLUX.1-dev、Stable-Diffusion 3.5 Large和Stable-Diffusion-XL。通过精心设计的提示，我们为五种与医院相关的职业（心脏病专家、医院院长、护士、救护员、外科医生）和五种肖像修饰词（ "", 专业性、中性、审美性、美丽）每种组合生成了100张图像。', 'title_zh': '超越提示：文本生成图像模型中的性别偏见，以医院职业为例'}
{'arxiv_id': 'arXiv:2510.00039', 'title': 'AutoPK: Leveraging LLMs and a Hybrid Similarity Metric for Advanced Retrieval of Pharmacokinetic Data from Complex Tables and Documents', 'authors': 'Hossein Sholehrasa, Amirhossein Ghanaatian, Doina Caragea, Lisa A. Tell, Jim E. Riviere, Majid Jaberi-Douraki', 'link': 'https://arxiv.org/abs/2510.00039', 'abstract': "Pharmacokinetics (PK) plays a critical role in drug development and regulatory decision-making for human and veterinary medicine, directly affecting public health through drug safety and efficacy assessments. However, PK data are often embedded in complex, heterogeneous tables with variable structures and inconsistent terminologies, posing significant challenges for automated PK data retrieval and standardization. AutoPK, a novel two-stage framework for accurate and scalable extraction of PK data from complex scientific tables. In the first stage, AutoPK identifies and extracts PK parameter variants using large language models (LLMs), a hybrid similarity metric, and LLM-based validation. The second stage filters relevant rows, converts the table into a key-value text format, and uses an LLM to reconstruct a standardized table. Evaluated on a real-world dataset of 605 PK tables, including captions and footnotes, AutoPK shows significant improvements in precision and recall over direct LLM baselines. For instance, AutoPK with LLaMA 3.1-70B achieved an F1-score of 0.92 on half-life and 0.91 on clearance parameters, outperforming direct use of LLaMA 3.1-70B by margins of 0.10 and 0.21, respectively. Smaller models such as Gemma 3-27B and Phi 3-12B with AutoPK achieved 2-7 fold F1 gains over their direct use, with Gemma's hallucination rates reduced from 60-95% down to 8-14%. Notably, AutoPK enabled open-source models like Gemma 3-27B to outperform commercial systems such as GPT-4o Mini on several PK parameters. AutoPK enables scalable and high-confidence PK data extraction, making it well-suited for critical applications in veterinary pharmacology, drug safety monitoring, and public health decision-making, while addressing heterogeneous table structures and terminology and demonstrating generalizability across key PK parameters. Code and data: this https URL", 'abstract_zh': '药代动力学（PK）在药物开发和人用及兽用药物监管决策中起着关键作用，直接影响公共健康的药物安全性与有效性评估。然而，PK数据通常嵌入在结构复杂、异质性高且术语不一致的表格中，这给自动化PK数据检索与标准化带来了巨大挑战。AutoPK，一种新颖的两阶段框架，用于从复杂科学表格中准确且可扩展地提取PK数据。第一阶段使用大型语言模型（LLMs）、混合相似度度量和基于LLM的验证来识别和提取PK参数变体。第二阶段过滤相关行，将表格转换为键值文本格式，并使用LLM重建标准化表格。在包含605张PK表格（包括表格标题和脚注）的真实数据集上评估，AutoPK在精度和召回率方面显著优于直接的LLM基线。例如，使用LLaMA 3.1-70B的AutoPK在半衰期参数上的F1分数为0.92，清除率参数上的F1分数为0.91，分别比直接使用LLaMA 3.1-70B高0.10和0.21。较小的模型如Gemma 3-27B和Phi 3-12B使用AutoPK实现了2-7倍的F1分数提升，Gemma的幻觉率从60-95%降至8-14%。值得注意的是，AutoPK使开源模型Gemma 3-27B在某些PK参数上超越了像GPT-4o Mini这样的商业系统。AutoPK能够实现可扩展且高置信度的PK数据提取，使其适用于兽医药理学、药物安全性监测和公共卫生决策等关键应用，并解决异质表格结构和术语问题，展示了在关键PK参数上的普适性。代码和数据：this https URL。', 'title_zh': 'AutoPK：利用大规模语言模型和混合相似度度量从复杂表格和文档中进行高级药代动力学数据检索'}
{'arxiv_id': 'arXiv:2510.00033', 'title': 'Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution', 'authors': 'Usman Muhammad, Jorma Laaksonen', 'link': 'https://arxiv.org/abs/2510.00033', 'abstract': 'Hyperspectral single image super-resolution (SISR) is a challenging task due to the difficulty of restoring fine spatial details while preserving spectral fidelity across a wide range of wavelengths, which limits the performance of conventional deep learning models. To address this challenge, we introduce Spectral-Spatial Unmixing Fusion (SSUF), a novel module that can be seamlessly integrated into standard 2D convolutional architectures to enhance both spatial resolution and spectral integrity. The SSUF combines spectral unmixing with spectral--spatial feature extraction and guides a ResNet-based convolutional neural network for improved reconstruction. In addition, we propose a custom Spatial-Spectral Gradient Loss function that integrates mean squared error with spatial and spectral gradient components, encouraging accurate reconstruction of both spatial and spectral features. Experiments on three public remote sensing hyperspectral datasets demonstrate that the proposed hybrid deep learning model achieves competitive performance while reducing model complexity.', 'abstract_zh': '基于光谱-空间非线性解混融洽的高光谱单图像超分辨率（SISR）', 'title_zh': '混合深度学习在高光谱单张图像超分辨中的应用'}
{'arxiv_id': 'arXiv:2510.00030', 'title': 'Temporal-Aware Iterative Speech Model for Dementia Detection', 'authors': 'Chukwuemeka Ugwu, Oluwafemi Oyeleke', 'link': 'https://arxiv.org/abs/2510.00030', 'abstract': 'Deep learning systems often struggle with processing long sequences, where computational complexity can become a bottleneck. Current methods for automated dementia detection using speech frequently rely on static, time-agnostic features or aggregated linguistic content, lacking the flexibility to model the subtle, progressive deterioration inherent in speech production. These approaches often miss the dynamic temporal patterns that are critical early indicators of cognitive decline. In this paper, we introduce TAI-Speech, a Temporal Aware Iterative framework that dynamically models spontaneous speech for dementia detection. The flexibility of our method is demonstrated through two key innovations: 1) Optical Flow-inspired Iterative Refinement: By treating spectrograms as sequential frames, this component uses a convolutional GRU to capture the fine-grained, frame-to-frame evolution of acoustic features. 2) Cross-Attention Based Prosodic Alignment: This component dynamically aligns spectral features with prosodic patterns, such as pitch and pauses, to create a richer representation of speech production deficits linked to functional decline (IADL). TAI-Speech adaptively models the temporal evolution of each utterance, enhancing the detection of cognitive markers. Experimental results on the DementiaBank dataset show that TAI-Speech achieves a strong AUC of 0.839 and 80.6\\% accuracy, outperforming text-based baselines without relying on ASR. Our work provides a more flexible and robust solution for automated cognitive assessment, operating directly on the dynamics of raw audio.', 'abstract_zh': '深度学习系统在处理长序列时常常遇到困难，其中计算复杂性可能成为瓶颈。目前使用语音进行痴呆症自动检测的方法通常依赖于静态、时间无关的特征或汇总的语义内容，缺乏灵活性来建模言语产生的微妙且渐进的退化。这些方法往往忽略了早期认知衰退的关键动态时间模式。本文 Introduction: Temporal Aware Iterative Framework for Spontaneous Speech in Dementia Detection', 'title_zh': '带时aware迭代语音模型的痴呆检测'}
{'arxiv_id': 'arXiv:2510.00029', 'title': 'Enhancing Safety in Diabetic Retinopathy Detection: Uncertainty-Aware Deep Learning Models with Rejection Capabilities', 'authors': 'Madhushan Ramalingam, Yaish Riaz, Priyanthi Rajamanoharan, Piyumi Dasanayaka', 'link': 'https://arxiv.org/abs/2510.00029', 'abstract': "Diabetic retinopathy (DR) is a major cause of visual impairment, and effective treatment options depend heavily on timely and accurate diagnosis. Deep learning models have demonstrated great success identifying DR from retinal images. However, relying only on predictions made by models, without any indication of model confidence, creates uncertainty and poses significant risk in clinical settings. This paper investigates an alternative in uncertainty-aware deep learning models, including a rejection mechanism to reject low-confidence predictions, contextualized by deferred decision-making in clinical practice. The results show there is a trade-off between prediction coverage and coverage reliability. The Variational Bayesian model adopted a more conservative strategy when predicting DR, subsequently rejecting the uncertain predictions. The model is evaluated by means of important performance metrics such as Accuracy on accepted predictions, the proportion of accepted cases (coverage), the rejection-ratio, and Expected Calibration Error (ECE). The findings also demonstrate a clear trade-off between accuracy and caution, establishing that the use of uncertainty estimation and selective rejection improves the model's reliability in safety-critical diagnostic use cases.", 'abstract_zh': '糖尿病视网膜病变（DR）是导致视觉障碍的主要原因之一，有效的治疗选项依赖于及时准确的诊断。深度学习模型在从视网膜图像中识别DR方面取得了巨大成功。然而，仅依靠模型的预测而不提供模型置信度的指示，在临床环境中会带来不确定性并造成重大风险。本文研究了一种新的不确定性感知深度学习模型，包括拒绝机制以拒绝低置信度的预测，并针对临床实践中延迟决策的背景进行考量。结果表明，在预测覆盖率与可靠性之间存在权衡。采用变分贝叶斯模型在预测DR时采取了更为保守的策略，随后拒绝了不确定的预测。该模型通过准确率、接受案例比例（覆盖率）、拒绝率以及预期校准误差（ECE）等关键性能指标进行了评估。研究结果还表明，准确性和谨慎性之间存在明显的权衡，证实了在关键安全诊断用例中使用不确定性估计和选择性拒绝可以提高模型的可靠性。', 'title_zh': '提高糖尿病视网膜病变检测的安全性：具有拒识能力的不确定性aware深度学习模型'}
{'arxiv_id': 'arXiv:2510.00027', 'title': 'Learning Inter-Atomic Potentials without Explicit Equivariance', 'authors': 'Ahmed A. Elhag, Arun Raja, Alex Morehead, Samuel M. Blau, Garrett M. Morris, Michael M. Bronstein', 'link': 'https://arxiv.org/abs/2510.00027', 'abstract': 'Accurate and scalable machine-learned inter-atomic potentials (MLIPs) are essential for molecular simulations ranging from drug discovery to new material design. Current state-of-the-art models enforce roto-translational symmetries through equivariant neural network architectures, a hard-wired inductive bias that can often lead to reduced flexibility, computational efficiency, and scalability. In this work, we introduce TransIP: Transformer-based Inter-Atomic Potentials, a novel training paradigm for interatomic potentials achieving symmetry compliance without explicit architectural constraints. Our approach guides a generic non-equivariant Transformer-based model to learn SO(3)-equivariance by optimizing its representations in the embedding space. Trained on the recent Open Molecules (OMol25) collection, a large and diverse molecular dataset built specifically for MLIPs and covering different types of molecules (including small organics, biomolecular fragments, and electrolyte-like species), TransIP attains comparable performance in machine-learning force fields versus state-of-the-art equivariant baselines. Further, compared to a data augmentation baseline, TransIP achieves 40% to 60% improvement in performance across varying OMol25 dataset sizes. More broadly, our work shows that learned equivariance can be a powerful and efficient alternative to equivariant or augmentation-based MLIP models.', 'abstract_zh': '基于变换器的原子间势（TransIP）：无需显式架构约束的原子间势训练新范式', 'title_zh': '学习原子间势能而不显式保证等变性'}
{'arxiv_id': 'arXiv:2510.00021', 'title': 'IA aplicada al análisis del conflicto Irán-Israel: Mapeo de discursos en YouTube', 'authors': 'Alvaro Vallejo Ramírez', 'link': 'https://arxiv.org/abs/2510.00021', 'abstract': 'Purpose. This study analyzes the digital representation of the Iran-Israel conflict that occurred in June 2025, based on 120,000 comments posted on YouTube. It sought to identify discursive positions regarding the actors involved and to examine how media and algorithmic biases shape digital conversations. Methodology. A mixed-methods design with triangulation was adopted. In the quantitative phase, natural language processing techniques and machine learning models (BERT and XLM-RoBERTa) were used to classify comments into ten categories. In the qualitative phase, a critical analysis of media context and ideological narratives was conducted, complemented by manual annotation and supervised training. This strategy enabled the integration of statistical robustness with contextual understanding. Results and conclusions. The findings reveal a clear overrepresentation of pro-Palestinian and anti-United States/Israel discourses, while pro-United States and anti-Palestinian positions were marginal. Iran, usually rendered invisible in global media, emerged as a central actor in the digital conversation during the conflict, suggesting a narrative shift away from previous hegemonic frameworks. Likewise, the results confirm the influence of algorithmic biases in amplifying certain discourses while limiting others. Original contributions. This work combines computational analysis and philosophical critique for the study of digital controversies, providing a methodological framework replicable in geopolitical contexts. It is one of the first Spanish-language studies to map, through artificial intelligence and critical analysis, discourses on an international conflict on YouTube, highlighting asymmetries and narrative disputes that are often overlooked.', 'abstract_zh': '目的. 本文基于2025年6月发生在伊朗与以色列之间的冲突在YouTube上发布的12万条评论，分析了该冲突的数字化表现，并旨在识别涉事各方的话语立场，同时考察媒体和算法偏见如何塑造数字对话。方法. 采用混合方法并进行 triangulation。在定量阶段，使用自然语言处理技术及机器学习模型（BERT和XLM-RoBERTa）对评论进行分类，分为十个类别。在定性阶段，进行了媒体背景和意识形态叙事的批判分析，辅以手工标注和监督训练。该策略实现了统计稳健性与情境理解的融合。结果与结论. 研究发现显示，亲巴勒斯坦和反美国/以色列话语明显占上风，而亲美国和反巴勒斯坦立场则处于边缘地位。通常在全球媒体中被忽视的伊朗，在冲突期间的数字对话中成为了中心角色，表明叙事转向了偏离以往主导框架的领域。同样，研究结果证实了算法偏见在放大某些话语、抑制其他话语方面的影响力。原创贡献. 该研究结合了计算分析和哲学批判，为数字争议的研究提供了可复制的方法论框架，尤其是在地缘政治背景下。它是首部利用人工智能和批判性分析在YouTube上绘制国际冲突话语图谱的西班牙语研究之一，突显了经常被忽视的不对称性与叙事争端。', 'title_zh': 'IA应用于伊朗-以色列冲突分析：YouTube上话语 mapping'}
{'arxiv_id': 'arXiv:2510.00001', 'title': 'Methodological Framework for Quantifying Semantic Test Coverage in RAG Systems', 'authors': 'Noah Broestl, Adel Nasser Abdalla, Rajprakash Bale, Hersh Gupta, Max Struever', 'link': 'https://arxiv.org/abs/2510.00001', 'abstract': 'Reliably determining the performance of Retrieval-Augmented Generation (RAG) systems depends on comprehensive test questions. While a proliferation of evaluation frameworks for LLM-powered applications exists, current practices lack a systematic method to ensure these test sets adequately cover the underlying knowledge base, leaving developers with significant blind spots. To address this, we present a novel, applied methodology to quantify the semantic coverage of RAG test questions against their underlying documents. Our approach leverages existing technologies, including vector embeddings and clustering algorithms, to create a practical framework for validating test comprehensiveness. Our methodology embeds document chunks and test questions into a unified vector space, enabling the calculation of multiple coverage metrics: basic proximity, content-weighted coverage, and multi-topic question coverage. Furthermore, we incorporate outlier detection to filter irrelevant questions, allowing for the refinement of test sets. Experimental evidence from two distinct use cases demonstrates that our framework effectively quantifies test coverage, identifies specific content areas with inadequate representation, and provides concrete recommendations for generating new, high-value test questions. This work provides RAG developers with essential tools to build more robust test suites, thereby improving system reliability and extending to applications such as identifying misaligned documents.', 'abstract_zh': '可靠地确定 Retrieval-Augmented Generation (RAG) 系统的性能取决于全面的测试问题。尽管存在多种针对 LLM 动力应用的评估框架，但当前的实践方法缺乏确保测试集充分覆盖底层知识库的系统方法，给开发者留下了显著的知识盲区。为解决这一问题，我们提出了一种新颖的应用方法，用于量化 RAG 测试问题对其底层文档的语义覆盖程度。我们的方法利用现有的技术，包括向量嵌入和聚类算法，以创建一个实用的验证测试全面性的框架。我们的方法将文档片段和测试问题嵌入到统一的向量空间中，以便计算多种覆盖度量：基本接近度、内容加权覆盖度以及多主题问题覆盖度。此外，我们还引入了离群值检测来过滤无关问题，从而 refinethe 测试集。来自两个不同应用场景的实验证据显示，我们的框架有效地量化了测试覆盖度，识别了内容表示不足的具体领域，并提供了生成新的高质量测试问题的具体建议。这项工作为 RAG 开发者提供了重要工具，以构建更 robust 的测试集，从而提高系统可靠性，并扩展到诸如识别不一致文档等应用。', 'title_zh': '用于量化RAG系统语义测试覆盖率的方法论框架'}
