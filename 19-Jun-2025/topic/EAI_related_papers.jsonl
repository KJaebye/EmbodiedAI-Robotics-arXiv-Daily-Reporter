{'arxiv_id': 'arXiv:2506.15666', 'title': 'Vision in Action: Learning Active Perception from Human Demonstrations', 'authors': 'Haoyu Xiong, Xiaomeng Xu, Jimmy Wu, Yifan Hou, Jeannette Bohg, Shuran Song', 'link': 'https://arxiv.org/abs/2506.15666', 'abstract': "We present Vision in Action (ViA), an active perception system for bimanual robot manipulation. ViA learns task-relevant active perceptual strategies (e.g., searching, tracking, and focusing) directly from human demonstrations. On the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to enable flexible, human-like head movements. To capture human active perception strategies, we design a VR-based teleoperation interface that creates a shared observation space between the robot and the human operator. To mitigate VR motion sickness caused by latency in the robot's physical movements, the interface uses an intermediate 3D scene representation, enabling real-time view rendering on the operator side while asynchronously updating the scene with the robot's latest observations. Together, these design elements enable the learning of robust visuomotor policies for three complex, multi-stage bimanual manipulation tasks involving visual occlusions, significantly outperforming baseline systems.", 'abstract_zh': 'Vision在行动（ViA）：一种双臂机器人操作的主动感知系统', 'title_zh': '行动中的视觉：从人类示范中学习主动感知'}
{'arxiv_id': 'arXiv:2506.15607', 'title': 'GRIM: Task-Oriented Grasping with Conditioning on Generative Examples', 'authors': 'Shailesh, Alok Raj, Nayan Kumar, Priya Shukla, Andrew Melnik, Micheal Beetz, Gora Chand Nandi', 'link': 'https://arxiv.org/abs/2506.15607', 'abstract': 'Task-Oriented Grasping (TOG) presents a significant challenge, requiring a nuanced understanding of task semantics, object affordances, and the functional constraints dictating how an object should be grasped for a specific task. To address these challenges, we introduce GRIM (Grasp Re-alignment via Iterative Matching), a novel training-free framework for task-oriented grasping. Initially, a coarse alignment strategy is developed using a combination of geometric cues and principal component analysis (PCA)-reduced DINO features for similarity scoring. Subsequently, the full grasp pose associated with the retrieved memory instance is transferred to the aligned scene object and further refined against a set of task-agnostic, geometrically stable grasps generated for the scene object, prioritizing task compatibility. In contrast to existing learning-based methods, GRIM demonstrates strong generalization capabilities, achieving robust performance with only a small number of conditioning examples.', 'abstract_zh': '面向任务的抓取（TOG） presents a significant challenge, requiring a nuanced understanding of task semantics, object affordances, and the functional constraints dictating how an object should be grasped for a specific task. To address these challenges, we introduce GRIM (Grasp Re-alignment via Iterative Matching), a novel training-free framework for task-oriented grasping. Initially, a coarse alignment strategy is developed using a combination of geometric cues and principal component analysis (PCA)-reduced DINO features for similarity scoring. Subsequently, the full grasp pose associated with the retrieved memory instance is transferred to the aligned scene object and further refined against a set of task-agnostic, geometrically stable grasps generated for the scene object, prioritizing task compatibility. In contrast to existing learning-based methods, GRIM demonstrates strong generalization capabilities, achieving robust performance with only a small number of conditioning examples.', 'title_zh': 'GRIM: 以生成示例为条件的面向任务的抓取'}
{'arxiv_id': 'arXiv:2506.15380', 'title': 'Efficient Navigation Among Movable Obstacles using a Mobile Manipulator via Hierarchical Policy Learning', 'authors': 'Taegeun Yang, Jiwoo Hwang, Jeil Jeong, Minsung Yoon, Sung-Eui Yoon', 'link': 'https://arxiv.org/abs/2506.15380', 'abstract': 'We propose a hierarchical reinforcement learning (HRL) framework for efficient Navigation Among Movable Obstacles (NAMO) using a mobile manipulator. Our approach combines interaction-based obstacle property estimation with structured pushing strategies, facilitating the dynamic manipulation of unforeseen obstacles while adhering to a pre-planned global path. The high-level policy generates pushing commands that consider environmental constraints and path-tracking objectives, while the low-level policy precisely and stably executes these commands through coordinated whole-body movements. Comprehensive simulation-based experiments demonstrate improvements in performing NAMO tasks, including higher success rates, shortened traversed path length, and reduced goal-reaching times, compared to baselines. Additionally, ablation studies assess the efficacy of each component, while a qualitative analysis further validates the accuracy and reliability of the real-time obstacle property estimation.', 'abstract_zh': '基于层次强化学习的移动 manipulator 在移动障碍物环境下高效导航框架', 'title_zh': '基于分层策略学习的移动 manipulator 在移动障碍物间高效导航'}
{'arxiv_id': 'arXiv:2506.15343', 'title': 'Offensive Robot Cybersecurity', 'authors': 'Víctor Mayoral-Vilches', 'link': 'https://arxiv.org/abs/2506.15343', 'abstract': "Offensive Robot Cybersecurity introduces a groundbreaking approach by advocating for offensive security methods empowered by means of automation. It emphasizes the necessity of understanding attackers' tactics and identifying vulnerabilities in advance to develop effective defenses, thereby improving robots' security posture. This thesis leverages a decade of robotics experience, employing Machine Learning and Game Theory to streamline the vulnerability identification and exploitation process. Intrinsically, the thesis uncovers a profound connection between robotic architecture and cybersecurity, highlighting that the design and creation aspect of robotics deeply intertwines with its protection against attacks. This duality -- whereby the architecture that shapes robot behavior and capabilities also necessitates a defense mechanism through offensive and defensive cybersecurity strategies -- creates a unique equilibrium. Approaching cybersecurity with a dual perspective of defense and attack, rooted in an understanding of systems architecture, has been pivotal. Through comprehensive analysis, including ethical considerations, the development of security tools, and executing cyber attacks on robot software, hardware, and industry deployments, this thesis proposes a novel architecture for cybersecurity cognitive engines. These engines, powered by advanced game theory and machine learning, pave the way for autonomous offensive cybersecurity strategies for robots, marking a significant shift towards self-defending robotic systems. This research not only underscores the importance of offensive measures in enhancing robot cybersecurity but also sets the stage for future advancements where robots are not just resilient to cyber threats but are equipped to autonomously safeguard themselves.", 'abstract_zh': '进攻性机器人网络安全', 'title_zh': '进攻性机器人网络安全'}
{'arxiv_id': 'arXiv:2506.15249', 'title': 'Context-Aware Deep Lagrangian Networks for Model Predictive Control', 'authors': 'Lucas Schulze, Jan Peters, Oleg Arenz', 'link': 'https://arxiv.org/abs/2506.15249', 'abstract': 'Controlling a robot based on physics-informed dynamic models, such as deep Lagrangian networks (DeLaN), can improve the generalizability and interpretability of the resulting behavior. However, in complex environments, the number of objects to potentially interact with is vast, and their physical properties are often uncertain. This complexity makes it infeasible to employ a single global model. Therefore, we need to resort to online system identification of context-aware models that capture only the currently relevant aspects of the environment. While physical principles such as the conservation of energy may not hold across varying contexts, ensuring physical plausibility for any individual context-aware model can still be highly desirable, particularly when using it for receding horizon control methods such as Model Predictive Control (MPC). Hence, in this work, we extend DeLaN to make it context-aware, combine it with a recurrent network for online system identification, and integrate it with a MPC for adaptive, physics-informed control. We also combine DeLaN with a residual dynamics model to leverage the fact that a nominal model of the robot is typically available. We evaluate our method on a 7-DOF robot arm for trajectory tracking under varying loads. Our method reduces the end-effector tracking error by 39%, compared to a 21% improvement achieved by a baseline that uses an extended Kalman filter.', 'abstract_zh': '基于物理知情动力学模型的机器人控制可以提高结果行为的一般化能力和可解释性。然而，在复杂环境中，潜在可互动的对象众多，且其物理属性往往具有不确定性。这种复杂性使得使用单一全局模型变得不切实际。因此，我们需要采用在线系统识别方法，构建仅捕捉当前相关环境方面的情境感知模型。尽管在不同情境下可能会违反物理原理，如能量守恒定律，但对于用于预测控制方法如模型预测控制（MPC）的情境感知模型，确保其物理合理性仍然是非常重要的。因此，在本文中，我们扩展了DeLaN，使其具有情境感知能力，将其与循环网络结合用于在线系统识别，并与MPC集成，以实现自适应的物理知情控制。我们还将DeLaN与残差动力学模型结合，利用通常可用的机器人名义模型的优势。我们在不同负载条件下的7-DOF机器人臂轨迹跟踪中评估了该方法，该方法将末端执行器跟踪误差减少了39%，而基线方法（使用扩展卡尔曼滤波器）仅实现了21%的改进。', 'title_zh': '基于上下文的深拉格朗日网络模型预测控制'}
{'arxiv_id': 'arXiv:2506.15157', 'title': "Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation", 'authors': 'Hanbit Oh, Andrea M. Salcedo-Vázquez, Ixchel G. Ramirez-Alpizar, Yukiyasu Domae', 'link': 'https://arxiv.org/abs/2506.15157', 'abstract': "Imitation learning (IL) aims to enable robots to perform tasks autonomously by observing a few human demonstrations. Recently, a variant of IL, called In-Context IL, utilized off-the-shelf large language models (LLMs) as instant policies that understand the context from a few given demonstrations to perform a new task, rather than explicitly updating network models with large-scale demonstrations. However, its reliability in the robotics domain is undermined by hallucination issues such as LLM-based instant policy, which occasionally generates poor trajectories that deviate from the given demonstrations. To alleviate this problem, we propose a new robust in-context imitation learning algorithm called the robust instant policy (RIP), which utilizes a Student's t-regression model to be robust against the hallucinated trajectories of instant policies to allow reliable trajectory generation. Specifically, RIP generates several candidate robot trajectories to complete a given task from an LLM and aggregates them using the Student's t-distribution, which is beneficial for ignoring outliers (i.e., hallucinations); thereby, a robust trajectory against hallucinations is generated. Our experiments, conducted in both simulated and real-world environments, show that RIP significantly outperforms state-of-the-art IL methods, with at least $26\\%$ improvement in task success rates, particularly in low-data scenarios for everyday tasks. Video results available at this https URL.", 'abstract_zh': '基于上下文的鲁棒 imitation 学习算法：利用 t 回归模型对抗幻觉轨迹', 'title_zh': '鲁棒即时策略：利用t-回归模型实现机器人 manipulation 的鲁棒上下文模仿学习'}
{'arxiv_id': 'arXiv:2506.15150', 'title': 'Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation', 'authors': 'Yuanlong Ji, Xingbang Yang, Ruoqi Zhao, Qihan Ye, Quan Zheng, Yubo Fan', 'link': 'https://arxiv.org/abs/2506.15150', 'abstract': 'Gait phase estimation based on inertial measurement unit (IMU) signals facilitates precise adaptation of exoskeletons to individual gait variations. However, challenges remain in achieving high accuracy and robustness, particularly during periods of terrain changes. To address this, we develop a gait phase estimation neural network based on implicit modeling of human locomotion, which combines temporal convolution for feature extraction with transformer layers for multi-channel information fusion. A channel-wise masked reconstruction pre-training strategy is proposed, which first treats gait phase state vectors and IMU signals as joint observations of human locomotion, thus enhancing model generalization. Experimental results demonstrate that the proposed method outperforms existing baseline approaches, achieving a gait phase RMSE of $2.729 \\pm 1.071%$ and phase rate MAE of $0.037 \\pm 0.016%$ under stable terrain conditions with a look-back window of 2 seconds, and a phase RMSE of $3.215 \\pm 1.303%$ and rate MAE of $0.050 \\pm 0.023%$ under terrain transitions. Hardware validation on a hip exoskeleton further confirms that the algorithm can reliably identify gait cycles and key events, adapting to various continuous motion scenarios. This research paves the way for more intelligent and adaptive exoskeleton systems, enabling safer and more efficient human-robot interaction across diverse real-world environments.', 'abstract_zh': '基于惯性测量单元信号的人体步行相位估计神经网络能够促进外骨骼对个体步行变化的精确适应。然而，在地形变化期间实现高精度和鲁棒性仍然面临挑战。为了解决这个问题，我们开发了一种基于人类运动隐式建模的步行相位估计神经网络，该网络结合了时间卷积用于特征提取，并使用 Transformer 层进行多通道信息融合。提出了一种通道级遮罩重建预训练策略，首先将步行相位状态向量和惯性测量单元信号视为人类运动的联合观测，从而增强模型的泛化能力。实验结果表明，在稳定的地形条件下，使用2秒的视窗，所提出的方法在步行相位RMSE方面优于现有基线方法，达到$2.729 \\pm 1.071\\%$，相位率MAE为$0.037 \\pm 0.016\\%$；在地形过渡期间，相位RMSE达到$3.215 \\pm 1.303\\%$，相位率MAE为$0.050 \\pm 0.023\\%$。基于髋关节外骨骼的硬件验证进一步证实了该算法可以可靠地识别步行周期和关键事件，适应各种连续运动场景。本研究为进一步智能化和适应性强的外骨骼系统铺平了道路，使人类与机器人在多种实际环境中实现更安全、更高效的交互成为可能。', 'title_zh': '基于人体运动隐式建模的实时步态相位估计'}
{'arxiv_id': 'arXiv:2506.15146', 'title': 'TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality', 'authors': 'Masaki Murooka, Takahiro Hoshi, Kensuke Fukumitsu, Shimpei Masuda, Marwan Hamze, Tomoya Sasaki, Mitsuharu Morisawa, Eiichi Yoshida', 'link': 'https://arxiv.org/abs/2506.15146', 'abstract': 'Manipulation with whole-body contact by humanoid robots offers distinct advantages, including enhanced stability and reduced load. On the other hand, we need to address challenges such as the increased computational cost of motion generation and the difficulty of measuring broad-area contact. We therefore have developed a humanoid control system that allows a humanoid robot equipped with tactile sensors on its upper body to learn a policy for whole-body manipulation through imitation learning based on human teleoperation data. This policy, named tactile-modality extended ACT (TACT), has a feature to take multiple sensor modalities as input, including joint position, vision, and tactile measurements. Furthermore, by integrating this policy with retargeting and locomotion control based on a biped model, we demonstrate that the life-size humanoid robot RHP7 Kaleido is capable of achieving whole-body contact manipulation while maintaining balance and walking. Through detailed experimental verification, we show that inputting both vision and tactile modalities into the policy contributes to improving the robustness of manipulation involving broad and delicate contact.', 'abstract_zh': '人形机器人通过全身接触进行操作具有独特的优势，包括增强的稳定性及减少的负载。然而，我们也面临着运动生成计算成本增加以及难以测量大面积接触的挑战。因此，我们开发了一种人形机器人控制系统，该系统允许装备了上身触觉传感器的人形机器人通过模仿基于人类远程操作数据的学习来学习全身操作策略。该策略名为触觉模态扩展的ACT（TACT），能够接受关节位置、视觉和触觉等多种传感模态的输入。此外，通过将该策略与基于双足模型的重定位和运动控制相结合，我们展示出全尺寸人形机器人RHP7 Kaleido能够在保持平衡和行走的同时实现全身接触操作。通过详细的实验验证，我们证明将视觉和触觉模态同时输入策略能够提高涉及广泛和精细接触的操作鲁棒性。', 'title_zh': 'TACT：通过触觉模态深度模仿学习实现的人形全身接触操作'}
{'arxiv_id': 'arXiv:2506.15132', 'title': 'Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion', 'authors': 'Yushi Wang, Penghui Chen, Xinyu Han, Feng Wu, Mingguo Zhao', 'link': 'https://arxiv.org/abs/2506.15132', 'abstract': 'Recent advancements in reinforcement learning (RL) have led to significant progress in humanoid robot locomotion, simplifying the design and training of motion policies in simulation. However, the numerous implementation details make transferring these policies to real-world robots a challenging task. To address this, we have developed a comprehensive code framework that covers the entire process from training to deployment, incorporating common RL training methods, domain randomization, reward function design, and solutions for handling parallel structures. This library is made available as a community resource, with detailed descriptions of its design and experimental results. We validate the framework on the Booster T1 robot, demonstrating that the trained policies seamlessly transfer to the physical platform, enabling capabilities such as omnidirectional walking, disturbance resistance, and terrain adaptability. We hope this work provides a convenient tool for the robotics community, accelerating the development of humanoid robots. The code can be found in this https URL.', 'abstract_zh': 'Recent Advancements in Reinforcement Learning for Humanoid Robot Locomotion: A Comprehensive Code Framework for Deployment', 'title_zh': 'Booster Gym: 人体形机器人运动控制的端到端强化学习框架'}
{'arxiv_id': 'arXiv:2506.15126', 'title': 'VIMS: A Visual-Inertial-Magnetic-Sonar SLAM System in Underwater Environments', 'authors': 'Bingbing Zhang, Huan Yin, Shuo Liu, Fumin Zhang, Wen Xu', 'link': 'https://arxiv.org/abs/2506.15126', 'abstract': 'In this study, we present a novel simultaneous localization and mapping (SLAM) system, VIMS, designed for underwater navigation. Conventional visual-inertial state estimators encounter significant practical challenges in perceptually degraded underwater environments, particularly in scale estimation and loop closing. To address these issues, we first propose leveraging a low-cost single-beam sonar to improve scale estimation. Then, VIMS integrates a high-sampling-rate magnetometer for place recognition by utilizing magnetic signatures generated by an economical magnetic field coil. Building on this, a hierarchical scheme is developed for visual-magnetic place recognition, enabling robust loop closure. Furthermore, VIMS achieves a balance between local feature tracking and descriptor-based loop closing, avoiding additional computational burden on the front end. Experimental results highlight the efficacy of the proposed VIMS, demonstrating significant improvements in both the robustness and accuracy of state estimation within underwater environments.', 'abstract_zh': '海底导航用的新型同时定位与建图系统VIMS', 'title_zh': 'VIMS： underwater环境下的一种视觉-惯性-磁敏-声呐 SLAM 系统'}
{'arxiv_id': 'arXiv:2506.15107', 'title': "I Know You're Listening: Adaptive Voice for HRI", 'authors': 'Paige Tuttösí', 'link': 'https://arxiv.org/abs/2506.15107', 'abstract': 'While the use of social robots for language teaching has been explored, there remains limited work on a task-specific synthesized voices for language teaching robots. Given that language is a verbal task, this gap may have severe consequences for the effectiveness of robots for language teaching tasks. We address this lack of L2 teaching robot voices through three contributions: 1. We address the need for a lightweight and expressive robot voice. Using a fine-tuned version of Matcha-TTS, we use emoji prompting to create an expressive voice that shows a range of expressivity over time. The voice can run in real time with limited compute resources. Through case studies, we found this voice more expressive, socially appropriate, and suitable for long periods of expressive speech, such as storytelling. 2. We explore how to adapt a robot\'s voice to physical and social ambient environments to deploy our voices in various locations. We found that increasing pitch and pitch rate in noisy and high-energy environments makes the robot\'s voice appear more appropriate and makes it seem more aware of its current environment. 3. We create an English TTS system with improved clarity for L2 listeners using known linguistic properties of vowels that are difficult for these listeners. We used a data-driven, perception-based approach to understand how L2 speakers use duration cues to interpret challenging words with minimal tense (long) and lax (short) vowels in English. We found that the duration of vowels strongly influences the perception for L2 listeners and created an "L2 clarity mode" for Matcha-TTS that applies a lengthening to tense vowels while leaving lax vowels unchanged. Our clarity mode was found to be more respectful, intelligible, and encouraging than base Matcha-TTS while reducing transcription errors in these challenging tense/lax minimal pairs.', 'abstract_zh': '社会机器人用于语言教学的特定任务合成语音研究', 'title_zh': '我知道你在听：适应性语音在人机交互中的应用'}
{'arxiv_id': 'arXiv:2506.15096', 'title': 'DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory', 'authors': 'Zihe Ji, Huangxuan Lin, Yue Gao', 'link': 'https://arxiv.org/abs/2506.15096', 'abstract': "We present DyNaVLM, an end-to-end vision-language navigation framework using Vision-Language Models (VLM). In contrast to prior methods constrained by fixed angular or distance intervals, our system empowers agents to freely select navigation targets via visual-language reasoning. At its core lies a self-refining graph memory that 1) stores object locations as executable topological relations, 2) enables cross-robot memory sharing through distributed graph updates, and 3) enhances VLM's decision-making via retrieval augmentation. Operating without task-specific training or fine-tuning, DyNaVLM demonstrates high performance on GOAT and ObjectNav benchmarks. Real-world tests further validate its robustness and generalization. The system's three innovations: dynamic action space formulation, collaborative graph memory, and training-free deployment, establish a new paradigm for scalable embodied robot, bridging the gap between discrete VLN tasks and continuous real-world navigation.", 'abstract_zh': 'DyNaVLM：一种基于视觉语言模型的端到端视觉语言导航框架', 'title_zh': 'DyNaVLM：具有动态视角和自我优化图记忆的零样本视觉-语言导航系统'}
{'arxiv_id': 'arXiv:2506.15085', 'title': 'EmojiVoice: Towards long-term controllable expressivity in robot speech', 'authors': 'Paige Tuttösí, Shivam Mehta, Zachary Syvenky, Bermet Burkanova, Gustav Eje Henter, Angelica Lim', 'link': 'https://arxiv.org/abs/2506.15085', 'abstract': "Humans vary their expressivity when speaking for extended periods to maintain engagement with their listener. Although social robots tend to be deployed with ``expressive'' joyful voices, they lack this long-term variation found in human speech. Foundation model text-to-speech systems are beginning to mimic the expressivity in human speech, but they are difficult to deploy offline on robots. We present EmojiVoice, a free, customizable text-to-speech (TTS) toolkit that allows social roboticists to build temporally variable, expressive speech on social robots. We introduce emoji-prompting to allow fine-grained control of expressivity on a phase level and use the lightweight Matcha-TTS backbone to generate speech in real-time. We explore three case studies: (1) a scripted conversation with a robot assistant, (2) a storytelling robot, and (3) an autonomous speech-to-speech interactive agent. We found that using varied emoji prompting improved the perception and expressivity of speech over a long period in a storytelling task, but expressive voice was not preferred in the assistant use case.", 'abstract_zh': '人类在长时间交谈中会通过变化表达性来维持与听众的互动。尽管社会机器人通常配备“表达性”的喜悦声音，但缺乏人类语言中长期存在的表达性变化。基础模型文本到语音系统开始模仿人类语言的表达性，但在机器人上离线部署起来较为困难。我们提出了一种名为EmojiVoice的免费可定制文本到语音(TTS)工具包，使社会机器人专家能够在社会机器人上构建时间变化的、富有表现力的语音。我们介绍了emoji提示，以实现语音表达性在相位层面的精细控制，并使用轻量级的Matcha-TTS骨干网络实现实时语音生成。我们探讨了三个案例研究：(1) 与机器人助手进行剧本对话、(2) 故事讲述机器人，以及(3) 自主的语音到语音交互代理。我们发现，在故事讲述任务中使用多样的emoji提示可以提高长时间内语音的感知和表现性，但在助手使用场景中，富有表现力的声音并不受欢迎。', 'title_zh': 'EmojiVoice：朝向长期可控表达性的机器人语音研究'}
{'arxiv_id': 'arXiv:2506.14968', 'title': 'FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization', 'authors': 'Rajat Kumar Jenamani, Tom Silver, Ben Dodson, Shiqin Tong, Anthony Song, Yuting Yang, Ziang Liu, Benjamin Howe, Aimee Whitneck, Tapomayukh Bhattacharjee', 'link': 'https://arxiv.org/abs/2506.14968', 'abstract': "Physical caregiving robots hold promise for improving the quality of life of millions worldwide who require assistance with feeding. However, in-home meal assistance remains challenging due to the diversity of activities (e.g., eating, drinking, mouth wiping), contexts (e.g., socializing, watching TV), food items, and user preferences that arise during deployment. In this work, we propose FEAST, a flexible mealtime-assistance system that can be personalized in-the-wild to meet the unique needs of individual care recipients. Developed in collaboration with two community researchers and informed by a formative study with a diverse group of care recipients, our system is guided by three key tenets for in-the-wild personalization: adaptability, transparency, and safety. FEAST embodies these principles through: (i) modular hardware that enables switching between assisted feeding, drinking, and mouth-wiping, (ii) diverse interaction methods, including a web interface, head gestures, and physical buttons, to accommodate diverse functional abilities and preferences, and (iii) parameterized behavior trees that can be safely and transparently adapted using a large language model. We evaluate our system based on the personalization requirements identified in our formative study, demonstrating that FEAST offers a wide range of transparent and safe adaptations and outperforms a state-of-the-art baseline limited to fixed customizations. To demonstrate real-world applicability, we conduct an in-home user study with two care recipients (who are community researchers), feeding them three meals each across three diverse scenarios. We further assess FEAST's ecological validity by evaluating with an Occupational Therapist previously unfamiliar with the system. In all cases, users successfully personalize FEAST to meet their individual needs and preferences. Website: this https URL", 'abstract_zh': '物理照护机器人有望提高数百万需要进食协助的全球人群的生活质量。然而，在家庭环境中提供餐食协助仍然具有挑战性，因为这涉及到多样化的活动（如进食、饮水、面部清洁）、情境（如社交、看电视）、食物种类和用户偏好。本文提出FEAST，一种灵活的餐食协助系统，可以在实际环境中个性化配置以满足不同照护对象的特殊需求。该系统是在与两位社区研究人员合作，并借鉴一次多元化照护对象参与的形塑研究的基础上开发的，遵循三项关键原则：适应性、透明度和安全性。FEAST通过以下方式体现这些原则：（i）模块化硬件，允许切换到进食、饮水和面部清洁辅助；（ii）多样化的交互方式，包括网页界面、头部手势和物理按钮，以适应不同的功能能力和偏好；（iii）参数化的行为树，可以通过大型语言模型安全透明地进行调整。我们根据形塑研究中确定的个性化需求评估了该系统，结果显示FEAST提供了广泛的安全和透明的适应选项，并优于有限的固定定制基准。为了展示其实际应用性，我们在两名照护对象（他们也是社区研究人员）家中进行了用户研究，让他们各自在三种不同情境下食用三餐。我们进一步通过一位之前未接触过该系统的职业治疗师评估了FEAST的生态效度。在所有情况下，参与者成功地个性化了FEAST以满足他们的个人需求和偏好。网址：这个游戏链接', 'title_zh': 'FEAST: 一种灵活的用餐辅助系统，实现真实的个性化体验'}
{'arxiv_id': 'arXiv:2506.14855', 'title': 'Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation -- Adios low-level controllers', 'authors': 'Tommaso Belvedere, Michael Ziegltrum, Giulio Turrisi, Valerio Modugno', 'link': 'https://arxiv.org/abs/2506.14855', 'abstract': 'Model Predictive Path Integral control is a powerful sampling-based approach suitable for complex robotic tasks due to its flexibility in handling nonlinear dynamics and non-convex costs. However, its applicability in real-time, highfrequency robotic control scenarios is limited by computational demands. This paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments standard MPPI by computing local linear feedback gains derived from sensitivity analysis inspired by Riccati-based feedback used in gradient-based MPC. These gains allow for rapid closed-loop corrections around the current state without requiring full re-optimization at each timestep. We demonstrate the effectiveness of F-MPPI through simulations and real-world experiments on two robotic platforms: a quadrupedal robot performing dynamic locomotion on uneven terrain and a quadrotor executing aggressive maneuvers with onboard computation. Results illustrate that incorporating local feedback significantly improves control performance and stability, enabling robust, high-frequency operation suitable for complex robotic systems.', 'abstract_zh': '反馈-MPPP 控制：一种通过局部反馈增强的 MPPI 新框架', 'title_zh': '反馈-MPPI：通过回溯梯度采样优化的快速MPC——再见低级控制器'}
{'arxiv_id': 'arXiv:2506.15677', 'title': 'Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence', 'authors': 'Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan James Wang, Kai-Wei Chang', 'link': 'https://arxiv.org/abs/2506.15677', 'abstract': 'AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page this https URL.', 'abstract_zh': '当前的AI代理大多是独立的——它们要么在线检索和推理大量数字信息和知识；要么通过具身感知、规划和行动与物理世界互动——但鲜有同时具备两者能力的情况。这种分离限制了它们解决需要整合物理和数字智能的任务的能力，例如依据网络食谱烹饪、使用动态地图数据导航或利用网络知识解释现实世界地标。我们引入了具身体联网（Embodied Web Agents），这是一种新型的AI代理范式，可以流畅地结合具身智能和大规模网络推理。为践行这一理念，我们首先开发了具身体联网任务环境，这是一个统一的模拟平台，紧密整合了逼真的3D室内和室外环境与功能性网络接口。在此基础上，我们构建并发布了具身体联网基准测试，涵盖了烹饪、导航、购物、旅游和地理位置等一系列任务，所有任务均要求在物理和数字域之间协调推理以实现多域智能的系统性评估。实验结果揭示了最先进的AI系统与人类能力之间的显著性能差距，确立了体现在身认知与大规模网络知识访问交汇处的挑战与机遇。所有数据集、代码和网站均可在我们的项目页面公开访问。', 'title_zh': '具身网络代理：整合物理与数字领域的代理智能'}
{'arxiv_id': 'arXiv:2506.15635', 'title': 'FindingDory: A Benchmark to Evaluate Memory in Embodied Agents', 'authors': 'Karmesh Yadav, Yusuf Ali, Gunshi Gupta, Yarin Gal, Zsolt Kira', 'link': 'https://arxiv.org/abs/2506.15635', 'abstract': 'Large vision-language models have recently demonstrated impressive performance in planning and control tasks, driving interest in their application to real-world robotics. However, deploying these models for reasoning in embodied contexts is limited by their ability to incorporate long-term experience collected across multiple days and represented by vast collections of images. Current VLMs typically struggle to process more than a few hundred images concurrently, highlighting the need for more efficient mechanisms to handle long-term memory in embodied settings. To effectively evaluate these models for long-horizon control, a benchmark must specifically target scenarios where memory is crucial for success. Existing long-video QA benchmarks overlook embodied challenges like object manipulation and navigation, which demand low-level skills and fine-grained reasoning over past interactions. Moreover, effective memory integration in embodied agents involves both recalling relevant historical information and executing actions based on that information, making it essential to study these aspects together rather than in isolation. In this work, we introduce a new benchmark for long-range embodied tasks in the Habitat simulator. This benchmark evaluates memory-based capabilities across 60 tasks requiring sustained engagement and contextual awareness in an environment. The tasks can also be procedurally extended to longer and more challenging versions, enabling scalable evaluation of memory and reasoning. We also present baselines that integrate state-of-the-art VLMs with low level navigation policies, assessing their performance on these memory-intensive tasks and highlight areas for improvement.', 'abstract_zh': '大型多模态视觉语言模型在规划和控制任务中 recently demonstrated impressive performance, driving interest in their application to real-world robotics. However, deploying these models for reasoning in embodied contexts is limited by their ability to incorporate long-term experience collected across multiple days and represented by vast collections of images. Current VLMs typically struggle to process more than a few hundred images concurrently, highlighting the need for more efficient mechanisms to handle long-term memory in embodied settings. To effectively evaluate these models for long-horizon control, a benchmark must specifically target scenarios where memory is crucial for success. Existing long-video QA benchmarks overlook embodied challenges like object manipulation and navigation, which demand low-level skills and fine-grained reasoning over past interactions. Moreover, effective memory integration in embodied agents involves both recalling relevant historical information and executing actions based on that information, making it essential to study these aspects together rather than in isolation. In this work, we introduce a new benchmark for long-range embodied tasks in the Habitat simulator. This benchmark evaluates memory-based capabilities across 60 tasks requiring sustained engagement and contextual awareness in an environment. The tasks can also be procedurally extended to longer and more challenging versions, enabling scalable evaluation of memory and reasoning. We also present baselines that integrate state-of-the-art VLMs with low-level navigation policies, assessing their performance on these memory-intensive tasks and highlight areas for improvement。', 'title_zh': 'FindingDory：评估具身智能体记忆能力的标准 benchmark'}
{'arxiv_id': 'arXiv:2506.15207', 'title': 'Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study', 'authors': 'Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk', 'link': 'https://arxiv.org/abs/2506.15207', 'abstract': 'The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions.', 'abstract_zh': '低地球轨道卫星的指数增长已彻底变革了地球观测任务，解决了气候监测、灾害管理等领域中的诸多挑战。然而，多卫星系统的自主协调仍然是一个基本难题。传统的优化方法难以应对动态地球观测任务中的实时决策需求，这要求使用强化学习（RL）和多智能体强化学习（MARL）。在本文中，我们通过建模单卫星操作并使用MARL框架扩展到多卫星星座，研究基于RL的自主地球观测任务规划。我们解决了包括能源和数据存储限制、卫星观测不确定性以及部分可观测下的去中心化协调等关键挑战。通过利用接近现实的卫星仿真环境，我们评估了最先进的MARL算法（包括PPO、IPPO、MAPPO和HAPPO）的训练稳定性和性能。研究结果表明，MARL能够有效平衡成像和资源管理，并在多卫星协调中处理非平稳性和奖励相互依赖性。本研究获取的洞见为自主卫星操作奠定了基础，并为提高去中心化地球观测任务中的策略学习提供了实用指南。', 'title_zh': '多智能体强化学习在自主多卫星地球观测中的应用：一个实际案例研究'}
{'arxiv_id': 'arXiv:2506.15065', 'title': 'HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by Large Language Models', 'authors': 'Trishna Chakraborty, Udita Ghosh, Xiaopan Zhang, Fahim Faisal Niloy, Yue Dong, Jiachen Li, Amit K. Roy-Chowdhury, Chengyu Song', 'link': 'https://arxiv.org/abs/2506.15065', 'abstract': 'Large language models (LLMs) are increasingly being adopted as the cognitive core of embodied agents. However, inherited hallucinations, which stem from failures to ground user instructions in the observed physical environment, can lead to navigation errors, such as searching for a refrigerator that does not exist. In this paper, we present the first systematic study of hallucinations in LLM-based embodied agents performing long-horizon tasks under scene-task inconsistencies. Our goal is to understand to what extent hallucinations occur, what types of inconsistencies trigger them, and how current models respond. To achieve these goals, we construct a hallucination probing set by building on an existing benchmark, capable of inducing hallucination rates up to 40x higher than base prompts. Evaluating 12 models across two simulation environments, we find that while models exhibit reasoning, they fail to resolve scene-task inconsistencies-highlighting fundamental limitations in handling infeasible tasks. We also provide actionable insights on ideal model behavior for each scenario, offering guidance for developing more robust and reliable planning strategies.', 'abstract_zh': '基于大语言模型的体素代理在场景任务不一致情况下长时 horizon 任务中幻觉现象的系统研究', 'title_zh': 'HEAL：大型语言模型驱动的具身代理幻觉现象的实证研究'}
{'arxiv_id': 'arXiv:2506.15672', 'title': 'SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence', 'authors': 'Yao Zhang, Chenyang Lin, Shijie Tang, Haokun Chen, Shijie Zhou, Yunpu Ma, Volker Tresp', 'link': 'https://arxiv.org/abs/2506.15672', 'abstract': 'The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at this https URL.', 'abstract_zh': '大型语言模型的迅速发展推动了自主系统在决策、协调和任务执行方面的进步。然而，现有的自主系统生成框架缺乏真正的自主性，缺少从零开始生成代理、自我优化的代理功能以及协作能力，这限制了系统的适应性和可扩展性。我们提出了SwarmAgentic框架，这是一种完全自动化的自主系统生成框架，能够从零开始构建自主系统，并通过语言驱动的探索联合优化代理功能和协作。为了实现系统级别结构的有效搜索，SwarmAgentic维护了一群候选系统，并通过反馈指导的更新进行演化，受到粒子群优化（PSO）的启发。我们在六个实际任务中评估了该方法，这些任务涉及高层规划、系统级协调和创造性推理。仅给定任务描述和目标函数，SwarmAgentic在TravelPlanner基准测试中超过了所有基线方法，相对于ADAS实现了261.8%的相对改进，突显了在结构约束不严的任务中全面自动化的效果。该框架标志着自主系统设计领域的一个重要进展，将群体智能与完全自动化的多方自主系统生成相结合。我们的代码已公开发布在该网址。', 'title_zh': 'SwarmAgentic：通过 swarm 智能实现完全自动化代理系统生成'}
{'arxiv_id': 'arXiv:2506.15377', 'title': 'Efficient and Generalizable Environmental Understanding for Visual Navigation', 'authors': 'Ruoyu Wang, Xinshu Li, Chen Wang, Lina Yao', 'link': 'https://arxiv.org/abs/2506.15377', 'abstract': "Visual Navigation is a core task in Embodied AI, enabling agents to navigate complex environments toward given objectives. Across diverse settings within Navigation tasks, many necessitate the modelling of sequential data accumulated from preceding time steps. While existing methods perform well, they typically process all historical observations simultaneously, overlooking the internal association structure within the data, which may limit the potential for further improvements in task performance. We address this by examining the unique characteristics of Navigation tasks through the lens of causality, introducing a causal framework to highlight the limitations of conventional sequential methods. Leveraging this insight, we propose Causality-Aware Navigation (CAN), which incorporates a Causal Understanding Module to enhance the agent's environmental understanding capability. Empirical evaluations show that our approach consistently outperforms baselines across various tasks and simulation environments. Extensive ablations studies attribute these gains to the Causal Understanding Module, which generalizes effectively in both Reinforcement and Supervised Learning settings without computational overhead.", 'abstract_zh': '视觉导航是具身人工智能中的核心任务，使代理能够导航至复杂环境中的给定目标。通过对导航任务中序列数据的独特特征进行因果分析，我们引入了一个因果框架来揭示传统序列方法的局限性，并提出了因果感知导航（CAN），该方法通过因果理解模块增强代理的环境理解能力。实验证明，我们的方法在各种任务和仿真环境中均优于基线方法。广泛的消融研究将这些改进归因于因果理解模块，该模块在强化学习和监督学习环境中均能有效泛化，而无需增加计算开销。', 'title_zh': '高效的通用环境理解在视觉导航中'}
{'arxiv_id': 'arXiv:2506.14936', 'title': 'CALM: Contextual Analog Logic with Multimodality', 'authors': 'Maxwell J. Jacobson, Corey J. Maley, Yexiang Xue', 'link': 'https://arxiv.org/abs/2506.14936', 'abstract': 'In this work, we introduce Contextual Analog Logic with Multimodality (CALM). CALM unites symbolic reasoning with neural generation, enabling systems to make context-sensitive decisions grounded in real-world multi-modal data.\nBackground: Classic bivalent logic systems cannot capture the nuance of human decision-making. They also require human grounding in multi-modal environments, which can be ad-hoc, rigid, and brittle. Neural networks are good at extracting rich contextual information from multi-modal data, but lack interpretable structures for reasoning.\nObjectives: CALM aims to bridge the gap between logic and neural perception, creating an analog logic that can reason over multi-modal inputs. Without this integration, AI systems remain either brittle or unstructured, unable to generalize robustly to real-world tasks. In CALM, symbolic predicates evaluate to analog truth values computed by neural networks and constrained search.\nMethods: CALM represents each predicate using a domain tree, which iteratively refines its analog truth value when the contextual groundings of its entities are determined. The iterative refinement is predicted by neural networks capable of capturing multi-modal information and is filtered through a symbolic reasoning module to ensure constraint satisfaction.\nResults: In fill-in-the-blank object placement tasks, CALM achieved 92.2% accuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It also demonstrated spatial heatmap generation aligned with logical constraints and delicate human preferences, as shown by a human study.\nConclusions: CALM demonstrates the potential to reason with logic structure while aligning with preferences in multi-modal environments. It lays the foundation for next-gen AI systems that require the precision and interpretation of logic and the multimodal information processing of neural networks.', 'abstract_zh': 'Contextual Analog Logic with Multimodality (CALM)', 'title_zh': 'CALM: 基于多模态的上下文类比逻辑'}
{'arxiv_id': 'arXiv:2506.15675', 'title': 'Sekai: A Video Dataset towards World Exploration', 'authors': 'Zhen Li, Chuanhao Li, Xiaofeng Mao, Shaoheng Lin, Ming Li, Shitian Zhao, Zhaopan Xu, Xinyue Li, Yukang Feng, Jianwen Sun, Zizhen Li, Fanrui Zhang, Jiaxin Ai, Zhixiang Wang, Yuwei Wu, Tong He, Jiangmiao Pang, Yu Qiao, Yunde Jia, Kaipeng Zhang', 'link': 'https://arxiv.org/abs/2506.15675', 'abstract': "Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications.", 'abstract_zh': '视频生成技术已取得显著进步，有望成为交互式世界探索的基础。然而，现有的视频生成数据集不适合用于世界探索训练，因为它们存在一些局限性：地点有限、持续时间短、场景静态以及缺乏关于探索和世界的注释。在本文中，我们介绍了Sekai（日语中“世界”的意思），这是一个高质量的第一人称视角全球视频数据集，包含丰富的世界探索注释。该数据集包含来自全球100多个国家和地区超过750座城市的超过5000小时的步行或无人机视点（FPV和UVA）视频。我们开发了一套高效有效的工具箱来收集、预处理和标注视频，包含位置、场景、天气、人群密度、字幕和摄像头轨迹等注释。实验表明了数据集的质量，并使用其中一部分训练了一个交互式视频世界探索模型，名为YUME（日语中“梦想”的意思）。我们相信Sekai将惠及视频生成和世界探索领域，并激发有价值的应用。', 'title_zh': 'sekai: 一个面向世界探索的视频数据集'}
{'arxiv_id': 'arXiv:2506.15047', 'title': "Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers", 'authors': 'Jiayue Melissa Shi, Dong Whi Yoo, Keran Wang, Violeta J. Rodriguez, Ravi Karkar, Koustuv Saha', 'link': 'https://arxiv.org/abs/2506.15047', 'abstract': "Family caregivers of individuals with Alzheimer's Disease and Related Dementia (AD/ADRD) face significant emotional and logistical challenges that place them at heightened risk for stress, anxiety, and depression. Although recent advances in generative AI -- particularly large language models (LLMs) -- offer new opportunities to support mental health, little is known about how caregivers perceive and engage with such technologies. To address this gap, we developed Carey, a GPT-4o-based chatbot designed to provide informational and emotional support to AD/ADRD caregivers. Using Carey as a technology probe, we conducted semi-structured interviews with 16 family caregivers following scenario-driven interactions grounded in common caregiving stressors. Through inductive coding and reflexive thematic analysis, we surface a systemic understanding of caregiver needs and expectations across six themes -- on-demand information access, emotional support, safe space for disclosure, crisis management, personalization, and data privacy. For each of these themes, we also identified the nuanced tensions in the caregivers' desires and concerns. We present a mapping of caregiver needs, AI chatbot's strengths, gaps, and design recommendations. Our findings offer theoretical and practical insights to inform the design of proactive, trustworthy, and caregiver-centered AI systems that better support the evolving mental health needs of AD/ADRD caregivers.", 'abstract_zh': '家庭照护者在阿尔茨海默病及相关痴呆（AD/ADRD）患者中的情感与 logistical 挑战及其对压力、焦虑和抑郁的风险，虽然生成式 AI（特别是大型语言模型 LLMs）的最新进展提供了新的支持心理健康的机会，但尚不清楚照护者如何感知和使用这些技术。为此，我们开发了基于 GPT-4o 的聊天机器人 Carey，旨在为 AD/ADRD 照护者提供信息和支持。通过情景驱动的交互和常见的照护压力情境进行半结构化访谈，我们与 16 名家庭照护者进行了交流，通过归纳编码和反思性主题分析，我们揭示了照护者需求和期望的系统理解，包括按需信息访问、情感支持、安全披露空间、危机管理、个性化和数据隐私六个主题。我们还识别了照护者在愿望和担忧中的细微矛盾。我们提出了照护者需求、AI 聊天机器人的优势、差距和设计建议的映射。我们的研究提供了理论和实践上的见解，以指导设计更积极主动、可信赖且以照护者为中心的 AI 系统，更好地支持 AD/ADRD 照护者的不断变化的心理健康需求。', 'title_zh': '将照护需求映射至AI聊天机器人设计：阿尔茨海默病和痴呆症照护者心理健康支持的优势与缺口'}
{'arxiv_id': 'arXiv:2506.15019', 'title': 'Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment', 'authors': 'Yue Gao', 'link': 'https://arxiv.org/abs/2506.15019', 'abstract': 'Effective reinforcement learning (RL) for sepsis treatment depends on learning stable, clinically meaningful state representations from irregular ICU time series. While previous works have explored representation learning for this task, the critical challenge of training instability in sequential representations and its detrimental impact on policy performance has been overlooked. This work demonstrates that Controlled Differential Equations (CDE) state representation can achieve strong RL policies when two key factors are met: (1) ensuring training stability through early stopping or stabilization methods, and (2) enforcing acuity-aware representations by correlation regularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the MIMIC-III sepsis cohort reveal that stable CDE autoencoder produces representations strongly correlated with acuity scores and enables RL policies with superior performance (WIS return $> 0.9$). In contrast, unstable CDE representation leads to degraded representations and policy failure (WIS return $\\sim$ 0). Visualizations of the latent space show that stable CDEs not only separate survivor and non-survivor trajectories but also reveal clear acuity score gradients, whereas unstable training fails to capture either pattern. These findings highlight practical guidelines for using CDEs to encode irregular medical time series in clinical RL, emphasizing the need for training stability in sequential representation learning.', 'abstract_zh': '有效治疗严重败血症的强化学习（RL）依赖于从不规则ICU时间序列中学习稳定且临床意义显著的状态表示。本研究证明，在满足两个关键条件时，受控微分方程（CDE）状态表示可以实现强RL策略：（1）通过早期停止或稳定方法确保训练稳定性；（2）通过与临床评分（SOFA、SAPS-II、OASIS）的相关正则化强制实施急性意识状态感知表示。MIMIC-III败血症队列的实验表明，稳定的CDE自编码器生成与急性意识状态评分高度相关的表示，并且能够实现高性能策略（WIS回报>0.9）。相比之下，不稳定的CDE表示导致表示质量下降并导致策略失败（WIS回报≈0）。潜在空间的可视化表明，稳定的CDE不仅可以区分幸存者和非幸存者轨迹，还能揭示清晰的急性意识状态评分梯度，而不稳定的训练则无法捕捉这两种模式。这些发现突出了在临床RL中使用CDE编码不规则医疗时间序列的实际指导原则，强调了顺序表示学习中训练稳定性的必要性。', 'title_zh': '急性菌血症治疗中基于 Offline Reinforcement Learning 的稳定 CDE 自编码器及其锐利度正则化方法'}
{'arxiv_id': 'arXiv:2506.14907', 'title': 'PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning', 'authors': 'Yizhen Zhang, Yang Ding, Shuoshuo Zhang, Xinchen Zhang, Haoling Li, Zhong-zhi Li, Peijie Wang, Jie Wu, Lei Ji, Yelong Shen, Yujiu Yang, Yeyun Gong', 'link': 'https://arxiv.org/abs/2506.14907', 'abstract': 'Inspired by the impressive reasoning capabilities demonstrated by reinforcement learning approaches like DeepSeek-R1, recent emerging research has begun exploring the use of reinforcement learning (RL) to enhance vision-language models (VLMs) for multimodal reasoning tasks. However, most existing multimodal reinforcement learning approaches remain limited to spatial reasoning within single-image contexts, yet still struggle to generalize to more complex and real-world scenarios involving multi-image positional reasoning, where understanding the relationships across images is crucial. To address this challenge, we propose a general reinforcement learning approach PeRL tailored for interleaved multimodal tasks, and a multi-stage strategy designed to enhance the exploration-exploitation trade-off, thereby improving learning efficiency and task performance. Specifically, we introduce permutation of image sequences to simulate varied positional relationships to explore more spatial and positional diversity. Furthermore, we design a rollout filtering mechanism for resampling to focus on trajectories that contribute most to learning optimal behaviors to exploit learned policies effectively. We evaluate our model on 5 widely-used multi-image benchmarks and 3 single-image benchmarks. Our experiments confirm that PeRL trained model consistently surpasses R1-related and interleaved VLM baselines by a large margin, achieving state-of-the-art performance on multi-image benchmarks, while preserving comparable performance on single-image tasks.', 'abstract_zh': '受DeepSeek-R1等强化学习方法令人印象深刻的推理能力启发，近期研究开始探索使用强化学习（RL）来增强多模态语言模型（VLMs）以提高多模态推理任务的能力。然而，大多数现有的多模态强化学习方法仍然局限于单图像范围内的空间推理，难以泛化到包含多图像位置推理的更复杂和现实生活场景，其中理解图像间的相互关系至关重要。为应对这一挑战，我们提出了一种针对交错多模态任务设计的一般强化学习方法PeRL，并设计一个多阶段策略以优化探索与利用之间的权衡，从而提高学习效率和任务性能。具体而言，我们引入了图像序列的排列以模拟不同的位置关系，探索更多的空间和位置多样性。此外，我们设计了一种滚动策略过滤机制以重采样，专注于最有助于学习最优行为的轨迹，从而有效利用所学策略。我们在5个广泛使用的多图像基准和3个单图像基准上评估了我们的模型。实验结果证实，与R1相关的和交错的VLM基线相比，PeRL训练的模型在多图像基准上取得了显著的性能优势，同时在单图像任务上保持了可比拟的性能。', 'title_zh': 'PeRL：排序增强强化学习在交错视觉-语言推理中的应用'}
{'arxiv_id': 'arXiv:2506.14787', 'title': 'Topology-Aware and Highly Generalizable Deep Reinforcement Learning for Efficient Retrieval in Multi-Deep Storage Systems', 'authors': 'Funing Li, Yuan Tian, Ruben Noortwyck, Jifeng Zhou, Liming Kuang, Robert Schulz', 'link': 'https://arxiv.org/abs/2506.14787', 'abstract': "In modern industrial and logistics environments, the rapid expansion of fast delivery services has heightened the demand for storage systems that combine high efficiency with increased density. Multi-deep autonomous vehicle storage and retrieval systems (AVS/RS) present a viable solution for achieving greater storage density. However, these systems encounter significant challenges during retrieval operations due to lane blockages. A conventional approach to mitigate this issue involves storing items with homogeneous characteristics in a single lane, but this strategy restricts the flexibility and adaptability of multi-deep storage systems.\nIn this study, we propose a deep reinforcement learning-based framework to address the retrieval problem in multi-deep storage systems with heterogeneous item configurations. Each item is associated with a specific due date, and the objective is to minimize total tardiness. To effectively capture the system's topology, we introduce a graph-based state representation that integrates both item attributes and the local topological structure of the multi-deep warehouse. To process this representation, we design a novel neural network architecture that combines a Graph Neural Network (GNN) with a Transformer model. The GNN encodes topological and item-specific information into embeddings for all directly accessible items, while the Transformer maps these embeddings into global priority assignments. The Transformer's strong generalization capability further allows our approach to be applied to storage systems with diverse layouts. Extensive numerical experiments, including comparisons with heuristic methods, demonstrate the superiority of the proposed neural network architecture and the effectiveness of the trained agent in optimizing retrieval tardiness.", 'abstract_zh': '基于深度强化学习的异构物品配置多深存储系统检索问题解决方案', 'title_zh': '面向拓扑结构且高度泛化的深度强化学习在多级存储系统中高效检索'}
