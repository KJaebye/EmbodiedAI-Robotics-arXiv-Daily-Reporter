# SHeRLoc: Synchronized Heterogeneous Radar Place Recognition for Cross-Modal Localization 

**Title (ZH)**: SHeRLoc: 同步异构雷达场所识别在跨模态定位中的应用 

**Authors**: Hanjun Kim, Minwoo Jung, Wooseong Yang, Ayoung Kim  

**Link**: [PDF](https://arxiv.org/pdf/2506.15175)  

**Abstract**: Despite the growing adoption of radar in robotics, the majority of research has been confined to homogeneous sensor types, overlooking the integration and cross-modality challenges inherent in heterogeneous radar technologies. This leads to significant difficulties in generalizing across diverse radar data types, with modality-aware approaches that could leverage the complementary strengths of heterogeneous radar remaining unexplored. To bridge these gaps, we propose SHeRLoc, the first deep network tailored for heterogeneous radar, which utilizes RCS polar matching to align multimodal radar data. Our hierarchical optimal transport-based feature aggregation method generates rotationally robust multi-scale descriptors. By employing FFT-similarity-based data mining and adaptive margin-based triplet loss, SHeRLoc enables FOV-aware metric learning. SHeRLoc achieves an order of magnitude improvement in heterogeneous radar place recognition, increasing recall@1 from below 0.1 to 0.9 on a public dataset and outperforming state of-the-art methods. Also applicable to LiDAR, SHeRLoc paves the way for cross-modal place recognition and heterogeneous sensor SLAM. The source code will be available upon acceptance. 

**Abstract (ZH)**: 尽管雷达在机器人领域的应用日益增长，大多数研究仍局限于同种传感器类型，忽视了异构雷达技术固有的跨模态整合挑战。这导致在处理多种多样的雷达数据时存在显著困难，利用异构雷达互补优势的方法尚未得到探索。为解决这些差距，我们提出SHeRLoc——首个针对异构雷达的深度网络，利用RCS极化匹配对多模态雷达数据进行对齐。我们的基于分层 optimal transport 的特征聚合方法生成旋转鲁棒的多尺度描述子。通过采用FFT相似性基于的数据挖掘和自适应边界三元组损失，SHeRLoc 实现了视场意识的度量学习。SHeRLoc 在异构雷达位置识别上取得了数量级的改进，在公共数据集上将 recall@1 从低于0.1 提升至0.9，并优于现有最佳方法。SHeRLoc 还适用于 LiDAR，并为跨模态位置识别和异构传感器 SLAM 开辟了道路。接受后将提供源代码。 

---
# Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces 

**Title (ZH)**: 设计意图：工业工作空间中人机合作的多模态框架 

**Authors**: Francesco Chiossi, Julian Rasch, Robin Welsch, Albrecht Schmidt, Florian Michahelles  

**Link**: [PDF](https://arxiv.org/pdf/2506.15293)  

**Abstract**: As robots enter collaborative workspaces, ensuring mutual understanding between human workers and robotic systems becomes a prerequisite for trust, safety, and efficiency. In this position paper, we draw on the cooperation scenario of the AIMotive project in which a human and a cobot jointly perform assembly tasks to argue for a structured approach to intent communication. Building on the Situation Awareness-based Agent Transparency (SAT) framework and the notion of task abstraction levels, we propose a multidimensional design space that maps intent content (SAT1, SAT3), planning horizon (operational to strategic), and modality (visual, auditory, haptic). We illustrate how this space can guide the design of multimodal communication strategies tailored to dynamic collaborative work contexts. With this paper, we lay the conceptual foundation for a future design toolkit aimed at supporting transparent human-robot interaction in the workplace. We highlight key open questions and design challenges, and propose a shared agenda for multimodal, adaptive, and trustworthy robotic collaboration in hybrid work environments. 

**Abstract (ZH)**: 随着机器人进入协作 workspace，确保人类工人与机器人系统之间的相互理解成为信任、安全和效率的前提。在本文中，我们借助 AIMotive 项目中的合作场景，即人类和协作机器人共同执行装配任务，论述了一种结构化的意图沟通方法。基于情境意识为基础的代理透明度（SAT）框架和任务抽象层次的概念，我们提出一个多维设计空间，该空间映射意图内容（SAT1, SAT3）、规划 horizon（从操作性到战略性的差异）以及模态（视觉、听觉、触觉）。我们展示了如何利用这一空间来指导适应动态协作工作环境的多模态沟通策略的设计。通过本文，我们为支持工作场所透明人机交互的未来设计工具包奠定了概念基础。我们强调了关键的开放问题和设计挑战，并提出了一项关于多模态、自适应和可信机器人协作在混合工作环境中的共同议程。 

---
# WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts 

**Title (ZH)**: WikiMixQA：基于表格和图表的多模态问答基准数据集 

**Authors**: Negar Foroutan, Angelika Romanou, Matin Ansaripour, Julian Martin Eisenschlos, Karl Aberer, Rémi Lebret  

**Link**: [PDF](https://arxiv.org/pdf/2506.15594)  

**Abstract**: Documents are fundamental to preserving and disseminating information, often incorporating complex layouts, tables, and charts that pose significant challenges for automatic document understanding (DU). While vision-language large models (VLLMs) have demonstrated improvements across various tasks, their effectiveness in processing long-context vision inputs remains unclear. This paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice questions (MCQs) designed to evaluate cross-modal reasoning over tables and charts extracted from 4,000 Wikipedia pages spanning seven distinct topics. Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring models to synthesize information from multiple modalities. We evaluate 12 state-of-the-art vision-language models, revealing that while proprietary models achieve ~70% accuracy when provided with direct context, their performance deteriorates significantly when retrieval from long documents is required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this setting, whereas open-source models perform considerably worse, with a maximum accuracy of 27%. These findings underscore the challenges of long-context, multi-modal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research. 

**Abstract (ZH)**: WikiMixQA: 一个用于评估多模态推理的基准数据集，包含来自4000个Wikipedia页面的1000个选择题 

---
# Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust 

**Title (ZH)**: 使用多模态基础模型分析媒体内容中的人物表示：有效性与可信度 

**Authors**: Evdoxia Taka, Debadyuti Bhattacharya, Joanne Garde-Hansen, Sanjay Sharma, Tanaya Guha  

**Link**: [PDF](https://arxiv.org/pdf/2506.14799)  

**Abstract**: Recent advances in AI has enabled automated analysis of complex media content at scale and generate actionable insights regarding character representation along such dimensions as gender and age. Past work focused on quantifying representation from audio/video/text using various ML models, but without having the audience in the loop. We ask, even if character distribution along demographic dimensions are available, how useful are they to the general public? Do they actually trust the numbers generated by AI models? Our work addresses these questions through a user study, while proposing a new AI-based character representation and visualization tool. Our tool based on the Contrastive Language Image Pretraining (CLIP) foundation model to analyze visual screen data to quantify character representation across dimensions of age and gender. We also designed effective visualizations suitable for presenting such analytics to lay audience. Next, we conducted a user study to seek empirical evidence on the usefulness and trustworthiness of the AI-generated results for carefully chosen movies presented in the form of our visualizations. We note that participants were able to understand the analytics from our visualization, and deemed the tool `overall useful'. Participants also indicated a need for more detailed visualizations to include more demographic categories and contextual information of the characters. Participants' trust in AI-based gender and age models is seen to be moderate to low, although they were not against the use of AI in this context. Our tool including code, benchmarking, and data from the user study can be found here: this https URL 

**Abstract (ZH)**: 近期人工智能的进展使大规模自动化分析复杂媒体内容成为可能，并生成有关角色表示（如性别和年龄维度）的具体行动建议。以往研究侧重于使用各种机器学习模型量化角色表示，但未能将受众纳入其中。我们提出的问题是，即使有按人口统计维度的分布情况，这些信息对大众有多大用处？他们是否真正信任AI模型生成的数字？我们通过用户研究来回答这些问题，并提出了一个基于AI的角色表示和可视化新工具。该工具基于对比语言图像预训练（CLIP）基础模型，分析屏幕视觉数据，量化年龄和性别维度的角色表示。我们还设计了适合展示此类分析结果的有效可视化工具。随后，我们开展了一项用户研究，以通过我们的可视化形式展示的精心选择的电影为例，寻求AI生成结果的实际效用和可信度的实验证据。研究结果显示，参与者能够理解我们可视化中的分析，并认为该工具“总体上是有用的”。参与者还表示需要更详细的可视化，以包含更多的人口统计类别和角色的背景信息。尽管参与者对基于AI的性别和年龄模型的信任度适度至较低，但他们并不反对在这一背景下使用AI。有关该工具的代码、基准测试以及用户研究的数据，可在此链接找到：this https URL。 

---
