# Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence 

**Title (ZH)**: 具身网络代理：整合物理与数字领域的代理智能 

**Authors**: Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan James Wang, Kai-Wei Chang  

**Link**: [PDF](https://arxiv.org/pdf/2506.15677)  

**Abstract**: AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page this https URL. 

**Abstract (ZH)**: 当前的AI代理大多是独立的——它们要么在线检索和推理大量数字信息和知识；要么通过具身感知、规划和行动与物理世界互动——但鲜有同时具备两者能力的情况。这种分离限制了它们解决需要整合物理和数字智能的任务的能力，例如依据网络食谱烹饪、使用动态地图数据导航或利用网络知识解释现实世界地标。我们引入了具身体联网（Embodied Web Agents），这是一种新型的AI代理范式，可以流畅地结合具身智能和大规模网络推理。为践行这一理念，我们首先开发了具身体联网任务环境，这是一个统一的模拟平台，紧密整合了逼真的3D室内和室外环境与功能性网络接口。在此基础上，我们构建并发布了具身体联网基准测试，涵盖了烹饪、导航、购物、旅游和地理位置等一系列任务，所有任务均要求在物理和数字域之间协调推理以实现多域智能的系统性评估。实验结果揭示了最先进的AI系统与人类能力之间的显著性能差距，确立了体现在身认知与大规模网络知识访问交汇处的挑战与机遇。所有数据集、代码和网站均可在我们的项目页面公开访问。 

---
# SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence 

**Title (ZH)**: SwarmAgentic：通过 swarm 智能实现完全自动化代理系统生成 

**Authors**: Yao Zhang, Chenyang Lin, Shijie Tang, Haokun Chen, Shijie Zhou, Yunpu Ma, Volker Tresp  

**Link**: [PDF](https://arxiv.org/pdf/2506.15672)  

**Abstract**: The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at this https URL. 

**Abstract (ZH)**: 大型语言模型的迅速发展推动了自主系统在决策、协调和任务执行方面的进步。然而，现有的自主系统生成框架缺乏真正的自主性，缺少从零开始生成代理、自我优化的代理功能以及协作能力，这限制了系统的适应性和可扩展性。我们提出了SwarmAgentic框架，这是一种完全自动化的自主系统生成框架，能够从零开始构建自主系统，并通过语言驱动的探索联合优化代理功能和协作。为了实现系统级别结构的有效搜索，SwarmAgentic维护了一群候选系统，并通过反馈指导的更新进行演化，受到粒子群优化（PSO）的启发。我们在六个实际任务中评估了该方法，这些任务涉及高层规划、系统级协调和创造性推理。仅给定任务描述和目标函数，SwarmAgentic在TravelPlanner基准测试中超过了所有基线方法，相对于ADAS实现了261.8%的相对改进，突显了在结构约束不严的任务中全面自动化的效果。该框架标志着自主系统设计领域的一个重要进展，将群体智能与完全自动化的多方自主系统生成相结合。我们的代码已公开发布在该网址。 

---
# Exploring and Exploiting the Inherent Efficiency within Large Reasoning Models for Self-Guided Efficiency Enhancement 

**Title (ZH)**: 探索并利用大型推理模型内部固有的高效性以实现自我引导的效率增强 

**Authors**: Weixiang Zhao, Jiahe Guo, Yang Deng, Xingyu Sui, Yulin Hu, Yanyan Zhao, Wanxiang Che, Bing Qin, Tat-Seng Chua, Ting Liu  

**Link**: [PDF](https://arxiv.org/pdf/2506.15647)  

**Abstract**: Recent advancements in large reasoning models (LRMs) have significantly enhanced language models' capabilities in complex problem-solving by emulating human-like deliberative thinking. However, these models often exhibit overthinking (i.e., the generation of unnecessarily verbose and redundant content), which hinders efficiency and inflates inference cost. In this work, we explore the representational and behavioral origins of this inefficiency, revealing that LRMs inherently possess the capacity for more concise reasoning. Empirical analyses show that correct reasoning paths vary significantly in length, and the shortest correct responses often suffice, indicating untapped efficiency potential. Exploiting these findings, we propose two lightweight methods to enhance LRM efficiency. First, we introduce Efficiency Steering, a training-free activation steering technique that modulates reasoning behavior via a single direction in the model's representation space. Second, we develop Self-Rewarded Efficiency RL, a reinforcement learning framework that dynamically balances task accuracy and brevity by rewarding concise correct solutions. Extensive experiments on seven LRM backbones across multiple mathematical reasoning benchmarks demonstrate that our methods significantly reduce reasoning length while preserving or improving task performance. Our results highlight that reasoning efficiency can be improved by leveraging and guiding the intrinsic capabilities of existing models in a self-guided manner. 

**Abstract (ZH)**: Recent advancements in大型推理模型（LRMs）显著增强了语言模型在复杂问题解决方面的能力，通过模拟类似人类的深思熟虑的思维过程。然而，这些模型常常表现出过度思考（即生成不必要的冗长和重复的内容），这阻碍了效率并增加了推断成本。在这种工作中，我们探索了这种低效性的代表性和行为根源，揭示了LRMs本质上具有更简洁推理的能力。实证分析表明，正确的推理路径在长度上存在显著差异，最短的正确回答往往就足够了，表明存在未开发的效率潜力。利用这些发现，我们提出了两种轻量级方法来提高LRM的效率。首先，我们引入了无需训练的激活调节技术Efficiency Steering，通过模型表示空间中的单一方向调节推理行为。其次，我们开发了自我奖励效率强化学习框架Self-Rewarded Efficiency RL，该框架通过奖励简洁的正确解决方案动态平衡任务的准确性和简洁性。在七个不同数学推理基准上的LRM框架上的广泛实验表明，我们的方法在保持或提高任务性能的同时显著减少了推理长度。我们的结果突显了可以通过在自我引导的方式下利用和引导现有模型的内在能力来提高推理效率。 

---
# The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy 

**Title (ZH)**: AI政策模块：培养计算机科学学生在AI伦理与政策方面的能力 

**Authors**: James Weichert, Daniel Dunlap, Mohammed Farghally, Hoda Eldardiry  

**Link**: [PDF](https://arxiv.org/pdf/2506.15639)  

**Abstract**: As artificial intelligence (AI) further embeds itself into many settings across personal and professional contexts, increasing attention must be paid not only to AI ethics, but also to the governance and regulation of AI technologies through AI policy. However, the prevailing post-secondary computing curriculum is currently ill-equipped to prepare future AI practitioners to confront increasing demands to implement abstract ethical principles and normative policy preferences into the design and development of AI systems. We believe that familiarity with the 'AI policy landscape' and the ability to translate ethical principles to practices will in the future constitute an important responsibility for even the most technically-focused AI engineers.
Toward preparing current computer science (CS) students for these new expectations, we developed an AI Policy Module to introduce discussions of AI policy into the CS curriculum. Building on a successful pilot in fall 2024, in this innovative practice full paper we present an updated and expanded version of the module, including a technical assignment on "AI regulation". We present the findings from our pilot of the AI Policy Module 2.0, evaluating student attitudes towards AI ethics and policy through pre- and post-module surveys. Following the module, students reported increased concern about the ethical impacts of AI technologies while also expressing greater confidence in their abilities to engage in discussions about AI regulation. Finally, we highlight the AI Regulation Assignment as an effective and engaging tool for exploring the limits of AI alignment and emphasizing the role of 'policy' in addressing ethical challenges. 

**Abstract (ZH)**: 随着人工智能（AI）进一步渗透到个人和专业环境中的众多领域，不仅需要关注AI伦理，还需要通过AI政策来治理和监管AI技术。当前的大学计算机课程尚不足以使未来的AI从业者准备好应对将抽象的伦理原则和规范性政策偏好融入AI系统设计和开发中的日益增长的要求。我们认为，熟悉“AI政策 landscape”并能够将伦理原则转化为实践将成为未来技术重点的AI工程师的重要责任。

为准备当前计算机科学（CS）学生应对这些新期望，我们开发了一个AI政策模块，将AI政策讨论引入CS课程中。基于2024年秋季成功的试点项目，本文介绍了更新和扩展后的模块版本，并包括了一个“AI监管”技术作业。本文报告了AI政策模块2.0试点项目的成果，通过模块前后的调查评估了学生对AI伦理和政策的态度变化。学生在完成模块后报告称，他们对AI技术的伦理影响更加关注，并表达了在讨论AI监管方面更强的信心。最后，本文强调AI监管作业是一个有效且有趣的工具，用于探讨AI对齐的局限性，并强调“政策”在解决伦理挑战中的作用。 

---
# The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games 

**Title (ZH)**: 动态路由游戏中文本表示对LLM代理行为的影响 

**Authors**: Lyle Goodyear, Rachel Guo, Ramesh Johari  

**Link**: [PDF](https://arxiv.org/pdf/2506.15624)  

**Abstract**: Large Language Models (LLMs) have shown promise as decision-makers in dynamic settings, but their stateless nature necessitates creating a natural language representation of history. We present a unifying framework for systematically constructing natural language "state" representations for prompting LLM agents in repeated multi-agent games. Previous work on games with LLM agents has taken an ad hoc approach to encoding game history, which not only obscures the impact of state representation on agents' behavior, but also limits comparability between studies. Our framework addresses these gaps by characterizing methods of state representation along three axes: action informativeness (i.e., the extent to which the state representation captures actions played); reward informativeness (i.e., the extent to which the state representation describes rewards obtained); and prompting style (or natural language compression, i.e., the extent to which the full text history is summarized).
We apply this framework to a dynamic selfish routing game, chosen because it admits a simple equilibrium both in theory and in human subject experiments \cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find that there are key dependencies of LLM agent behavior on the natural language state representation. In particular, we observe that representations which provide agents with (1) summarized, rather than complete, natural language representations of past history; (2) information about regrets, rather than raw payoffs; and (3) limited information about others' actions lead to behavior that more closely matches game theoretic equilibrium predictions, and with more stable game play by the agents. By contrast, other representations can exhibit either large deviations from equilibrium, higher variation in dynamic game play over time, or both. 

**Abstract (ZH)**: 一种统一框架：构建大型语言模型代理在重复多代理博弈中的自然语言“状态”表示 

---
# Managing Complex Failure Analysis Workflows with LLM-based Reasoning and Acting Agents 

**Title (ZH)**: 基于LLM推理与执行代理的复杂故障分析工作流管理 

**Authors**: Aline Dobrovsky, Konstantin Schekotihin, Christian Burmer  

**Link**: [PDF](https://arxiv.org/pdf/2506.15567)  

**Abstract**: Failure Analysis (FA) is a highly intricate and knowledge-intensive process. The integration of AI components within the computational infrastructure of FA labs has the potential to automate a variety of tasks, including the detection of non-conformities in images, the retrieval of analogous cases from diverse data sources, and the generation of reports from annotated images. However, as the number of deployed AI models increases, the challenge lies in orchestrating these components into cohesive and efficient workflows that seamlessly integrate with the FA process.
This paper investigates the design and implementation of a Large Language Model (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their analysis cases. The LPA integrates LLMs with advanced planning capabilities and external tool utilization, enabling autonomous processing of complex queries, retrieval of relevant data from external systems, and generation of human-readable responses. Evaluation results demonstrate the agent's operational effectiveness and reliability in supporting FA tasks. 

**Abstract (ZH)**: 基于大型语言模型的规划代理设计与实现：助力故障分析工程师解决分析案例 

---
# Efficient and Generalizable Environmental Understanding for Visual Navigation 

**Title (ZH)**: 高效的通用环境理解在视觉导航中 

**Authors**: Ruoyu Wang, Xinshu Li, Chen Wang, Lina Yao  

**Link**: [PDF](https://arxiv.org/pdf/2506.15377)  

**Abstract**: Visual Navigation is a core task in Embodied AI, enabling agents to navigate complex environments toward given objectives. Across diverse settings within Navigation tasks, many necessitate the modelling of sequential data accumulated from preceding time steps. While existing methods perform well, they typically process all historical observations simultaneously, overlooking the internal association structure within the data, which may limit the potential for further improvements in task performance. We address this by examining the unique characteristics of Navigation tasks through the lens of causality, introducing a causal framework to highlight the limitations of conventional sequential methods. Leveraging this insight, we propose Causality-Aware Navigation (CAN), which incorporates a Causal Understanding Module to enhance the agent's environmental understanding capability. Empirical evaluations show that our approach consistently outperforms baselines across various tasks and simulation environments. Extensive ablations studies attribute these gains to the Causal Understanding Module, which generalizes effectively in both Reinforcement and Supervised Learning settings without computational overhead. 

**Abstract (ZH)**: 视觉导航是具身人工智能中的核心任务，使代理能够导航至复杂环境中的给定目标。通过对导航任务中序列数据的独特特征进行因果分析，我们引入了一个因果框架来揭示传统序列方法的局限性，并提出了因果感知导航（CAN），该方法通过因果理解模块增强代理的环境理解能力。实验证明，我们的方法在各种任务和仿真环境中均优于基线方法。广泛的消融研究将这些改进归因于因果理解模块，该模块在强化学习和监督学习环境中均能有效泛化，而无需增加计算开销。 

---
# Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels 

**Title (ZH)**: 基于UAV和船舶协同的不确定性海洋MEC的联合计算卸载与资源分配 

**Authors**: Jiahao You, Ziye Jia, Chao Dong, Qihui Wu, Zhu Han  

**Link**: [PDF](https://arxiv.org/pdf/2506.15225)  

**Abstract**: The computation demands from the maritime Internet of Things (MIoT) increase rapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels based multi-access edge computing (MEC) can fulfill these MIoT requirements. However, the uncertain maritime tasks present significant challenges of inefficient computation offloading and resource allocation. In this paper, we focus on the maritime computation offloading and resource allocation through the cooperation of UAVs and vessels, with consideration of uncertain tasks. Specifically, we propose a cooperative MEC framework for computation offloading and resource allocation, including MIoT devices, UAVs and vessels. Then, we formulate the optimization problem to minimize the total execution time. As for the uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the unpredictable task arrivals and varying computational resource availability. 
By converting the long-term constraints into short-term constraints, we obtain a set of small-scale optimization problems. Further, considering the heterogeneity of actions and resources of UAVs and vessels, we reformulate the small-scale optimization problem into a Markov game (MG). Moreover, a heterogeneous-agent soft actor-critic is proposed to sequentially update various neural networks and effectively solve the MG problem. 
Finally, simulations are conducted to verify the effectiveness in addressing computational offloading and resource allocation. 

**Abstract (ZH)**: 海洋物联网计算卸载与资源分配中的无人飞行器和船只合作框架研究 

---
# Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study 

**Title (ZH)**: 多智能体强化学习在自主多卫星地球观测中的应用：一个实际案例研究 

**Authors**: Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk  

**Link**: [PDF](https://arxiv.org/pdf/2506.15207)  

**Abstract**: The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions. 

**Abstract (ZH)**: 低地球轨道卫星的指数增长已彻底变革了地球观测任务，解决了气候监测、灾害管理等领域中的诸多挑战。然而，多卫星系统的自主协调仍然是一个基本难题。传统的优化方法难以应对动态地球观测任务中的实时决策需求，这要求使用强化学习（RL）和多智能体强化学习（MARL）。在本文中，我们通过建模单卫星操作并使用MARL框架扩展到多卫星星座，研究基于RL的自主地球观测任务规划。我们解决了包括能源和数据存储限制、卫星观测不确定性以及部分可观测下的去中心化协调等关键挑战。通过利用接近现实的卫星仿真环境，我们评估了最先进的MARL算法（包括PPO、IPPO、MAPPO和HAPPO）的训练稳定性和性能。研究结果表明，MARL能够有效平衡成像和资源管理，并在多卫星协调中处理非平稳性和奖励相互依赖性。本研究获取的洞见为自主卫星操作奠定了基础，并为提高去中心化地球观测任务中的策略学习提供了实用指南。 

---
# HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges 

**Title (ZH)**: HeurAgenix：利用大语言模型解决复杂组合优化挑战 

**Authors**: Xianliang Yang, Ling Zhang, Haolong Qian, Lei Song, Jiang Bian  

**Link**: [PDF](https://arxiv.org/pdf/2506.15196)  

**Abstract**: Heuristic algorithms play a vital role in solving combinatorial optimization (CO) problems, yet traditional designs depend heavily on manual expertise and struggle to generalize across diverse instances. We introduce \textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large language models (LLMs) that first evolves heuristics and then selects among them automatically. In the heuristic evolution phase, HeurAgenix leverages an LLM to compare seed heuristic solutions with higher-quality solutions and extract reusable evolution strategies. During problem solving, it dynamically picks the most promising heuristic for each problem state, guided by the LLM's perception ability. For flexibility, this selector can be either a state-of-the-art LLM or a fine-tuned lightweight model with lower inference cost. To mitigate the scarcity of reliable supervision caused by CO complexity, we fine-tune the lightweight heuristic selector with a dual-reward mechanism that jointly exploits singals from selection preferences and state perception, enabling robust selection under noisy annotations. Extensive experiments on canonical benchmarks show that HeurAgenix not only outperforms existing LLM-based hyper-heuristics but also matches or exceeds specialized solvers. Code is available at this https URL. 

**Abstract (ZH)**: HeurAgenix：由大型语言模型驱动的两阶段超超问题求解框架 

---
# Truncated Proximal Policy Optimization 

**Title (ZH)**: 截断 proximity 策略优化 

**Authors**: Tiantian Fan, Lingjun Liu, Yu Yue, Jiaze Chen, Chengyi Wang, Qiying Yu, Chi Zhang, Zhiqi Lin, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Bole Ma, Mofan Zhang, Gaohong Liu, Ru Zhang, Haotian Zhou, Cong Xie, Ruidong Zhu, Zhi Zhang, Xin Liu, Mingxuan Wang, Lin Yan, Yonghui Wu  

**Link**: [PDF](https://arxiv.org/pdf/2506.15050)  

**Abstract**: Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5x and outperforms its existing competitors. 

**Abstract (ZH)**: Recently, 测试时缩放大型语言模型 (LLMs) 通过生成长链式思考 (CoT) 在科学和专业任务中展现了卓越的推理能力。通过代理强化学习 (RL)，以近端策略优化 (PPO) 及其变种为代表，模型能够通过尝试和错误来学习。然而，PPO 由于其固有的在线策略性质，响应长度增加时，训练过程可能会变得耗时。在本工作中，我们提出了截断近端策略优化 (T-PPO)，这是一种对 PPO 的新颖扩展，通过简化策略更新和长度受限的响应生成来提高训练效率。T-PPO 缓解了全同步长生成过程中固有的硬件利用率低的问题，即在完整采样期间，资源往往处于闲置状态。我们的贡献包括两方面。首先，我们提出了扩展的一般优势估计 (EGAE)，用于从不完整响应中进行优势估计，同时保持策略学习的完整性。其次，我们设计了一种计算优化机制，允许策略模型和价值模型独立优化。通过选择性过滤提示和截断的令牌，该机制减少冗余计算，加速训练过程而不牺牲收敛性能。我们使用 AIME 2024 和 32B 基模型验证了 T-PPO 的有效性和效率。实验结果表明，T-PPO 将推理 LLMs 的训练效率提高了 2.5 倍，并且优于其现有竞争对手。 

---
# MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning 

**Title (ZH)**: MEAL：持续多智能体强化学习的标准基准 

**Authors**: Tristan Tomilin, Luka van den Boogaard, Samuel Garcin, Bram Grooten, Meng Fang, Mykola Pechenizkiy  

**Link**: [PDF](https://arxiv.org/pdf/2506.14990)  

**Abstract**: Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms, with environment availability strongly impacting research. One particularly underexplored intersection is continual learning (CL) in cooperative multi-agent settings. To remedy this, we introduce MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark tailored for continual multi-agent reinforcement learning (CMARL). Existing CL benchmarks run environments on the CPU, leading to computational bottlenecks and limiting the length of task sequences. MEAL leverages JAX for GPU acceleration, enabling continual learning across sequences of 100 tasks on a standard desktop PC in a few hours. We show that naively combining popular CL and MARL methods yields strong performance on simple environments, but fails to scale to more complex settings requiring sustained coordination and adaptation. Our ablation study identifies architectural and algorithmic features critical for CMARL on MEAL. 

**Abstract (ZH)**: 基于MEAL的持续多代理强化学习基准 

---
# CALM: Contextual Analog Logic with Multimodality 

**Title (ZH)**: CALM: 基于多模态的上下文类比逻辑 

**Authors**: Maxwell J. Jacobson, Corey J. Maley, Yexiang Xue  

**Link**: [PDF](https://arxiv.org/pdf/2506.14936)  

**Abstract**: In this work, we introduce Contextual Analog Logic with Multimodality (CALM). CALM unites symbolic reasoning with neural generation, enabling systems to make context-sensitive decisions grounded in real-world multi-modal data.
Background: Classic bivalent logic systems cannot capture the nuance of human decision-making. They also require human grounding in multi-modal environments, which can be ad-hoc, rigid, and brittle. Neural networks are good at extracting rich contextual information from multi-modal data, but lack interpretable structures for reasoning.
Objectives: CALM aims to bridge the gap between logic and neural perception, creating an analog logic that can reason over multi-modal inputs. Without this integration, AI systems remain either brittle or unstructured, unable to generalize robustly to real-world tasks. In CALM, symbolic predicates evaluate to analog truth values computed by neural networks and constrained search.
Methods: CALM represents each predicate using a domain tree, which iteratively refines its analog truth value when the contextual groundings of its entities are determined. The iterative refinement is predicted by neural networks capable of capturing multi-modal information and is filtered through a symbolic reasoning module to ensure constraint satisfaction.
Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2% accuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It also demonstrated spatial heatmap generation aligned with logical constraints and delicate human preferences, as shown by a human study.
Conclusions: CALM demonstrates the potential to reason with logic structure while aligning with preferences in multi-modal environments. It lays the foundation for next-gen AI systems that require the precision and interpretation of logic and the multimodal information processing of neural networks. 

**Abstract (ZH)**: Contextual Analog Logic with Multimodality (CALM) 

---
# Dense SAE Latents Are Features, Not Bugs 

**Title (ZH)**: 密集SAE潜在变量是特征，不是漏洞 

**Authors**: Xiaoqing Sun, Alessandro Stolfo, Joshua Engels, Ben Wu, Senthooran Rajamanoharan, Mrinmaya Sachan, Max Tegmark  

**Link**: [PDF](https://arxiv.org/pdf/2506.15679)  

**Abstract**: Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are \emph{dense}), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs -- suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and finally to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise. 

**Abstract (ZH)**: 稀疏自编码器（SAEs）设计用于通过施加稀疏性约束从语言模型中提取可解释的特征。理想情况下，训练SAE会生成既稀疏又语义上有意义的潜在特征。然而，许多SAE的潜在特征频繁激活（即，是密集的），这引起了对其可能是训练过程的有害副产品的担忧。在本文中，我们系统地研究了密集潜在特征的几何结构、功能及其起源，并表明它们不仅持久存在，而且通常反映了有意义的模型表示。我们首先展示了密集潜在特征倾向于形成反极点对，这些反极点对重建残差流中的特定方向，并且消融其子空间会抑制重新训练的SAE中新密集特征的出现——这表明高密度特征是残差空间的内在特性。然后，我们引入了一种密集潜在特征的分类，揭示了与位置跟踪、上下文绑定、熵调节、特定字母输出信号、词性标记和主成分重建相关的类别。最后，我们分析了这些特征在层间的变化，揭示了从早期层的结构特征、中期层的语义特征到最终层的输出导向信号的转变。我们的研究结果表明，密集潜在特征在语言模型计算中发挥功能作用，不应被视为训练噪声。 

---
# Sekai: A Video Dataset towards World Exploration 

**Title (ZH)**: sekai: 一个面向世界探索的视频数据集 

**Authors**: Zhen Li, Chuanhao Li, Xiaofeng Mao, Shaoheng Lin, Ming Li, Shitian Zhao, Zhaopan Xu, Xinyue Li, Yukang Feng, Jianwen Sun, Zizhen Li, Fanrui Zhang, Jiaxin Ai, Zhixiang Wang, Yuwei Wu, Tong He, Jiangmiao Pang, Yu Qiao, Yunde Jia, Kaipeng Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2506.15675)  

**Abstract**: Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications. 

**Abstract (ZH)**: 视频生成技术已取得显著进步，有望成为交互式世界探索的基础。然而，现有的视频生成数据集不适合用于世界探索训练，因为它们存在一些局限性：地点有限、持续时间短、场景静态以及缺乏关于探索和世界的注释。在本文中，我们介绍了Sekai（日语中“世界”的意思），这是一个高质量的第一人称视角全球视频数据集，包含丰富的世界探索注释。该数据集包含来自全球100多个国家和地区超过750座城市的超过5000小时的步行或无人机视点（FPV和UVA）视频。我们开发了一套高效有效的工具箱来收集、预处理和标注视频，包含位置、场景、天气、人群密度、字幕和摄像头轨迹等注释。实验表明了数据集的质量，并使用其中一部分训练了一个交互式视频世界探索模型，名为YUME（日语中“梦想”的意思）。我们相信Sekai将惠及视频生成和世界探索领域，并激发有价值的应用。 

---
# Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers 

**Title (ZH)**: 漏泄的思考：大规模推理模型并非私密的思想者 

**Authors**: Tommaso Green, Martin Gubri, Haritz Puerto, Sangdoo Yun, Seong Joon Oh  

**Link**: [PDF](https://arxiv.org/pdf/2506.15674)  

**Abstract**: We study privacy leakage in the reasoning traces of large reasoning models used as personal agents. Unlike final outputs, reasoning traces are often assumed to be internal and safe. We challenge this assumption by showing that reasoning traces frequently contain sensitive user data, which can be extracted via prompt injections or accidentally leak into outputs. Through probing and agentic evaluations, we demonstrate that test-time compute approaches, particularly increased reasoning steps, amplify such leakage. While increasing the budget of those test-time compute approaches makes models more cautious in their final answers, it also leads them to reason more verbosely and leak more in their own thinking. This reveals a core tension: reasoning improves utility but enlarges the privacy attack surface. We argue that safety efforts must extend to the model's internal thinking, not just its outputs. 

**Abstract (ZH)**: 我们在大型推理模型作为个人代理的推理轨迹中研究隐私泄露问题。与最终输出不同，推理轨迹通常被认为是内部且安全的。我们通过展示推理轨迹频繁包含敏感用户数据，这些数据可以通过提示注入或意外地泄露到输出中来挑战这一假设。通过探查和代理评估，我们证明了测试时计算方法，尤其是增加推理步骤，会放大这种泄露。尽管增加这些测试时计算方法的成本会使模型在最终答案上更加谨慎，但也导致它们在思考过程中更加冗长和泄露更多。这揭示了一个核心矛盾：推理提高了实用性但扩大了隐私攻击面。我们argue认为，安全努力必须扩展到模型的内部思考，而不仅仅限于其输出。 

---
# AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning 

**Title (ZH)**: AutoRule: 基于推理链的规则提取奖励改进偏好学习 

**Authors**: Tevin Wang, Chenyan Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2506.15651)  

**Abstract**: Rule-based rewards offer a promising strategy for improving reinforcement learning from human feedback (RLHF), but current approaches often rely on manual rule engineering. We present AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chain of these interpretations, and synthesizes them into a unified rule set. Leveraging the finalized rule set, we employ language-model verifiers to compute the fraction of rules satisfied by each output, using this metric as an auxiliary reward alongside the learned reward model during policy optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\% relative improvement in length-controlled win rate on AlpacaEval2.0, and a 6.1\% relative gain in second-turn performance on a held-out MT-Bench subset, compared to a GRPO baseline trained with the same learned reward model but without the rule-based auxiliary reward. Our analysis confirms that the extracted rules exhibit good agreement with dataset preference. We find that AutoRule demonstrates reduced reward hacking compared to a learned reward model when run over two episodes. Finally, our case study suggests that the extracted rules capture unique qualities valued in different datasets. The extracted rules are provided in the appendix, and the code is open-sourced at this https URL. 

**Abstract (ZH)**: 基于规则的奖励规则自动化提取为人类反馈强化学习（RLHF）提供了一种有希望的策略，但现有方法往往依赖于手动规则工程。我们提出AutoRule，这是一种完全自动化的从偏好反馈中提取规则并将其格式化为基于规则的奖励的方法。AutoRule 提取过程分为三个阶段：利用推理模型解释用户偏好、从这些解释的推理链中识别候选规则，并将它们合成统一的规则集。利用最终确定的规则集，我们使用语言模型验证器计算每个输出满足规则的比例，并将此指标作为策略优化期间辅助奖励与学习奖励模型的一致奖励。使用AutoRule训练Llama-3-8B模型在AlpacaEval2.0上的长度控制获胜率相对提高28.6%，在保留的MT-Bench子集上第二轮性能相对提升6.1%，相较于使用相同学习奖励模型但没有基于规则的辅助奖励的GRPO基线训练模型。我们的分析确认提取的规则与数据集偏好具有良好的一致性。我们发现，当在两个回合中运行时，AutoRule与学习奖励模型相比表现出较低的奖励作弊倾向。最后，我们的案例研究表明，提取的规则捕捉到了不同数据集中独特的价值属性。提取的规则附在附录中，代码开源于此网址。 

---
# Demystifying the Visual Quality Paradox in Multimodal Large Language Models 

**Title (ZH)**: 揭开多模态大型语言模型中视觉质量悖论的迷思 

**Authors**: Shuo Xing, Lanqing Guo, Hongyuan Hua, Seoyoung Lee, Peiran Li, Yufei Wang, Zhangyang Wang, Zhengzhong Tu  

**Link**: [PDF](https://arxiv.org/pdf/2506.15645)  

**Abstract**: Recent Multimodal Large Language Models (MLLMs) excel on benchmark vision-language tasks, yet little is known about how input visual quality shapes their responses. Does higher perceptual quality of images already translate to better MLLM understanding? We conduct the first systematic study spanning leading MLLMs and a suite of vision-language benchmarks, applying controlled degradations and stylistic shifts to each image. Surprisingly, we uncover a visual-quality paradox: model, task, and even individual-instance performance can improve when images deviate from human-perceived fidelity. Off-the-shelf restoration pipelines fail to reconcile these idiosyncratic preferences. To close the gap, we introduce Visual-Quality Test-Time Tuning (VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable, low-rank kernel before the frozen vision encoder to modulate frequency content; and (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT dynamically adjusts each input image in a single forward pass, aligning it with task-specific model preferences. Across the evaluated MLLMs and all datasets, VQ-TTT lifts significant average accuracy, with no external models, cached features, or extra training data. These findings redefine ``better'' visual inputs for MLLMs and highlight the need for adaptive, rather than universally ``clean'', imagery, in the new era of AI being the main data customer. 

**Abstract (ZH)**: Recent Multimodal Large Language Models (MLLMs)在基准视觉-语言任务中的表现优异，但输入视觉质量如何影响其回应尚不明确。图像的感知质量已经转化为更好的MLLM理解吗？我们首次系统研究了领先MLLM和一系列视觉-语言基准任务，通过控制降级和风格转变对每个图像进行处理。令人惊讶的是，我们发现了一个视觉质量悖论：当图像偏离人类感知保真度时，模型、任务甚至单个实例的性能可以得到提升。即用的修复管道无法调和这些独特偏好。为弥补差距，我们引入了Visual-Quality Test-Time Tuning (VQ-TTT)——一种轻量级的适应模块，该模块：（1）在冻结的视觉编码器之前插入一个可学习的低秩内核来调节频率内容；（2）仅通过LoRA微调浅层视觉编码器层。VQ-TTT在单一前向传递中动态调整每个输入图像，使其与特定任务的需求相匹配。在评估的MLLM和所有数据集上，VQ-TTT显著提升了平均准确率，无需外部模型、缓存特征或额外训练数据。这些发现重新定义了适用于MLLMs的“更好”视觉输入，并强调了在AI成为主要数据客户的新时代，需要适应性而非普遍“干净”的图像的需求。 

---
# Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability 

**Title (ZH)**: 重新审视大型语言模型的组合泛化能力及其指令遵循能力 

**Authors**: Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe  

**Link**: [PDF](https://arxiv.org/pdf/2506.15629)  

**Abstract**: In generative commonsense reasoning tasks such as CommonGen, generative large language models (LLMs) compose sentences that include all given concepts. However, when focusing on instruction-following capabilities, if a prompt specifies a concept order, LLMs must generate sentences that adhere to the specified order. To address this, we propose Ordered CommonGen, a benchmark designed to evaluate the compositional generalization and instruction-following abilities of LLMs. This benchmark measures ordered coverage to assess whether concepts are generated in the specified order, enabling a simultaneous evaluation of both abilities. We conducted a comprehensive analysis using 36 LLMs and found that, while LLMs generally understand the intent of instructions, biases toward specific concept order patterns often lead to low-diversity outputs or identical results even when the concept order is altered. Moreover, even the most instruction-compliant LLM achieved only about 75% ordered coverage, highlighting the need for improvements in both instruction-following and compositional generalization capabilities. 

**Abstract (ZH)**: 有序常识推理基准：评估生成大型语言模型的指令遵循和组合泛化能力 

---
# Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction 

**Title (ZH)**: 基于MRI的脑年龄联邦学习：多中心Stroke后功能结局预测研究 

**Authors**: Vincent Roca, Marc Tommasi, Paul Andrey, Aurélien Bellet, Markus D. Schirmer, Hilde Henon, Laurent Puy, Julien Ramon, Grégory Kuchcinski, Martin Bretzner, Renaud Lopes  

**Link**: [PDF](https://arxiv.org/pdf/2506.15626)  

**Abstract**: $\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a neuroimaging biomarker reflecting brain health. However, training robust BrainAGE models requires large datasets, often restricted by privacy concerns. This study evaluates the performance of federated learning (FL) for BrainAGE estimation in ischemic stroke patients treated with mechanical thrombectomy, and investigates its association with clinical phenotypes and functional outcomes.
$\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients across 16 hospital centers. We implemented standard machine learning and deep learning models for BrainAGE estimates under three data management strategies: centralized learning (pooled data), FL (local training at each site), and single-site learning. We reported prediction errors and examined associations between BrainAGE and vascular risk factors (e.g., diabetes mellitus, hypertension, smoking), as well as functional outcomes at three months post-stroke. Logistic regression evaluated BrainAGE's predictive value for these outcomes, adjusting for age, sex, vascular risk factors, stroke severity, time between MRI and arterial puncture, prior intravenous thrombolysis, and recanalisation outcome.
$\textbf{Results:}$ While centralized learning yielded the most accurate predictions, FL consistently outperformed single-site models. BrainAGE was significantly higher in patients with diabetes mellitus across all models. Comparisons between patients with good and poor functional outcomes, and multivariate predictions of these outcomes showed the significance of the association between BrainAGE and post-stroke recovery.
$\textbf{Conclusion:}$ FL enables accurate age predictions without data centralization. The strong association between BrainAGE, vascular risk factors, and post-stroke recovery highlights its potential for prognostic modeling in stroke care. 

**Abstract (ZH)**: objectives: Brain-predicted年龄difference (BrainAGE) 是一个反映大脑健康的神经影像学生物标志物。然而，训练稳健的BrainAGE模型需要大量数据，通常受到隐私问题的限制。本研究评估了联邦学习（FL）在机械取栓治疗的缺血性中风患者中预测BrainAGE的表现，并探讨了其与临床表型和功能预后的关系。 

---
# GFLC: Graph-based Fairness-aware Label Correction for Fair Classification 

**Title (ZH)**: 基于图的公平感知标签修正以实现公平分类 

**Authors**: Modar Sulaiman, Kallol Roy  

**Link**: [PDF](https://arxiv.org/pdf/2506.15620)  

**Abstract**: Fairness in machine learning (ML) has a critical importance for building trustworthy machine learning system as artificial intelligence (AI) systems increasingly impact various aspects of society, including healthcare decisions and legal judgments. Moreover, numerous studies demonstrate evidence of unfair outcomes in ML and the need for more robust fairness-aware methods. However, the data we use to train and develop debiasing techniques often contains biased and noisy labels. As a result, the label bias in the training data affects model performance and misrepresents the fairness of classifiers during testing. To tackle this problem, our paper presents Graph-based Fairness-aware Label Correction (GFLC), an efficient method for correcting label noise while preserving demographic parity in datasets. In particular, our approach combines three key components: prediction confidence measure, graph-based regularization through Ricci-flow-optimized graph Laplacians, and explicit demographic parity incentives. Our experimental findings show the effectiveness of our proposed approach and show significant improvements in the trade-off between performance and fairness metrics compared to the baseline. 

**Abstract (ZH)**: 基于图的公平性aware标签矫正（GFLC）：在保持人口统计学平等的情况下纠正标签噪声 

---
# The Compositional Architecture of Regret in Large Language Models 

**Title (ZH)**: 大型语言模型中后悔的组成功能架构 

**Authors**: Xiangxiang Cui, Shu Yang, Tianjin Huang, Wanyu Lin, Lijie Hu, Di Wang  

**Link**: [PDF](https://arxiv.org/pdf/2506.15617)  

**Abstract**: Regret in Large Language Models refers to their explicit regret expression when presented with evidence contradicting their previously generated misinformation. Studying the regret mechanism is crucial for enhancing model reliability and helps in revealing how cognition is coded in neural networks. To understand this mechanism, we need to first identify regret expressions in model outputs, then analyze their internal representation. This analysis requires examining the model's hidden states, where information processing occurs at the neuron level. However, this faces three key challenges: (1) the absence of specialized datasets capturing regret expressions, (2) the lack of metrics to find the optimal regret representation layer, and (3) the lack of metrics for identifying and analyzing regret neurons. Addressing these limitations, we propose: (1) a workflow for constructing a comprehensive regret dataset through strategically designed prompting scenarios, (2) the Supervised Compression-Decoupling Index (S-CDI) metric to identify optimal regret representation layers, and (3) the Regret Dominance Score (RDS) metric to identify regret neurons and the Group Impact Coefficient (GIC) to analyze activation patterns. Our experimental results successfully identified the optimal regret representation layer using the S-CDI metric, which significantly enhanced performance in probe classification experiments. Additionally, we discovered an M-shaped decoupling pattern across model layers, revealing how information processing alternates between coupling and decoupling phases. Through the RDS metric, we categorized neurons into three distinct functional groups: regret neurons, non-regret neurons, and dual neurons. 

**Abstract (ZH)**: 大型语言模型中的后悔机制指的是在面对与其先前生成的错误信息相矛盾的证据时，模型explicit地表达后悔的情况。研究后悔机制对于提高模型可靠性至关重要，并有助于揭示认知是如何在神经网络中编码的。为了理解这一机制，我们需要首先识别模型输出中的后悔表达，然后分析其内部表示。这一分析需要检查模型的隐藏状态，在这里信息处理发生在神经元层面。然而，这面临着三个关键挑战：(1) 缺乏专门捕捉后悔表达的数据集，(2) 缺乏评估最佳后悔表示层的度量标准，(3) 缺乏识别和分析后悔神经元的度量标准。为了解决这些限制，我们提出：(1) 通过战略性设计提示场景构建全面后悔数据集的工作流程，(2) 使用监督压缩-解耦指数(S-CDI)度量来识别最佳后悔表示层，(3) 使用后悔主导分数(RDS)度量来识别后悔神经元，并使用组影响系数(GIC)来分析激活模式。实验结果成功使用S-CDI度量确定了最佳后悔表示层，显著提升了探针分类实验中的性能。此外，我们发现模型层中存在M形解耦模式，揭示了信息处理在耦合和解耦阶段之间的交替。通过RDS度量，我们将神经元分类为三类功能性组别：后悔神经元、非后悔神经元和双功能神经元。 

---
# LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning 

**Title (ZH)**: LoX: 低秩外推增强LLM安全xing对抗微调 

**Authors**: Gabrel J. Perin, Runjin Chen, Xuxi Chen, Nina S. T. Hirata, Zhangyang Wang, Junyuan Hong  

**Link**: [PDF](https://arxiv.org/pdf/2506.15606)  

**Abstract**: Large Language Models (LLMs) have become indispensable in real-world applications. However, their widespread adoption raises significant safety concerns, particularly in responding to socially harmful questions. Despite substantial efforts to improve model safety through alignment, aligned models can still have their safety protections undermined by subsequent fine-tuning - even when the additional training data appears benign. In this paper, we empirically demonstrate that this vulnerability stems from the sensitivity of safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building on this insight, we propose a novel training-free method, termed Low-Rank Extrapolation (LoX), to enhance safety robustness by extrapolating the safety subspace of an aligned LLM. Our experimental results confirm the effectiveness of LoX, demonstrating significant improvements in robustness against both benign and malicious fine-tuning attacks while preserving the model's adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute reductions in attack success rates (ASR) facing benign or malicious fine-tuning attacks. By investigating the ASR landscape of parameters, we attribute the success of LoX to that the extrapolation moves LLM parameters to a flatter zone, thereby less sensitive to perturbations. The code is available at this http URL. 

**Abstract (ZH)**: 大型语言模型参数中关键低秩子空间对微调的敏感性导致其安全性防护漏洞：一种基于低秩外推的训练-free 方法以增强安全性鲁棒性 

---
# From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns 

**Title (ZH)**: 从模型到课堂：基于叙事和难度考量评估生成的葡萄牙语选择题 

**Authors**: Bernardo Leite, Henrique Lopes Cardoso, Pedro Pinto, Abel Ferreira, Luís Abreu, Isabel Rangel, Sandra Monteiro  

**Link**: [PDF](https://arxiv.org/pdf/2506.15598)  

**Abstract**: While MCQs are valuable for learning and evaluation, manually creating them with varying difficulty levels and targeted reading skills remains a time-consuming and costly task. Recent advances in generative AI provide an opportunity to automate MCQ generation efficiently. However, assessing the actual quality and reliability of generated MCQs has received limited attention -- particularly regarding cases where generation fails. This aspect becomes particularly important when the generated MCQs are meant to be applied in real-world settings. Additionally, most MCQ generation studies focus on English, leaving other languages underexplored. This paper investigates the capabilities of current generative models in producing MCQs for reading comprehension in Portuguese, a morphologically rich language. Our study focuses on generating MCQs that align with curriculum-relevant narrative elements and span different difficulty levels. We evaluate these MCQs through expert review and by analyzing the psychometric properties extracted from student responses to assess their suitability for elementary school students. Our results show that current models can generate MCQs of comparable quality to human-authored ones. However, we identify issues related to semantic clarity and answerability. Also, challenges remain in generating distractors that engage students and meet established criteria for high-quality MCQ option design. 

**Abstract (ZH)**: 生成式AI在自动生成葡萄牙语阅读理解选择题中的能力研究 

---
# WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts 

**Title (ZH)**: WikiMixQA：基于表格和图表的多模态问答基准数据集 

**Authors**: Negar Foroutan, Angelika Romanou, Matin Ansaripour, Julian Martin Eisenschlos, Karl Aberer, Rémi Lebret  

**Link**: [PDF](https://arxiv.org/pdf/2506.15594)  

**Abstract**: Documents are fundamental to preserving and disseminating information, often incorporating complex layouts, tables, and charts that pose significant challenges for automatic document understanding (DU). While vision-language large models (VLLMs) have demonstrated improvements across various tasks, their effectiveness in processing long-context vision inputs remains unclear. This paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice questions (MCQs) designed to evaluate cross-modal reasoning over tables and charts extracted from 4,000 Wikipedia pages spanning seven distinct topics. Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring models to synthesize information from multiple modalities. We evaluate 12 state-of-the-art vision-language models, revealing that while proprietary models achieve ~70% accuracy when provided with direct context, their performance deteriorates significantly when retrieval from long documents is required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this setting, whereas open-source models perform considerably worse, with a maximum accuracy of 27%. These findings underscore the challenges of long-context, multi-modal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research. 

**Abstract (ZH)**: WikiMixQA: 一个用于评估多模态推理的基准数据集，包含来自4000个Wikipedia页面的1000个选择题 

---
# One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution 

**Title (ZH)**: 一步扩散生成细节丰富且时序连贯的视频超分辨率 

**Authors**: Yujing Sun, Lingchen Sun, Shuaizheng Liu, Rongyuan Wu, Zhengqiang Zhang, Lei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2506.15591)  

**Abstract**: It is a challenging problem to reproduce rich spatial details while maintaining temporal consistency in real-world video super-resolution (Real-VSR), especially when we leverage pre-trained generative models such as stable diffusion (SD) for realistic details synthesis. Existing SD-based Real-VSR methods often compromise spatial details for temporal coherence, resulting in suboptimal visual quality. We argue that the key lies in how to effectively extract the degradation-robust temporal consistency priors from the low-quality (LQ) input video and enhance the video details while maintaining the extracted consistency priors. To achieve this, we propose a Dual LoRA Learning (DLoRAL) paradigm to train an effective SD-based one-step diffusion model, achieving realistic frame details and temporal consistency simultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module to aggregate complementary information across frames, and train a Consistency-LoRA (C-LoRA) to learn robust temporal representations from degraded inputs. After consistency learning, we fix the CFR and C-LoRA modules and train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with the temporal space defined by C-LoRA to keep temporal coherence. The two phases alternate iteratively for optimization, collaboratively delivering consistent and detail-rich outputs. During inference, the two LoRA branches are merged into the SD model, allowing efficient and high-quality video restoration in a single diffusion step. Experiments show that DLoRAL achieves strong performance in both accuracy and speed. Code and models are available at this https URL. 

**Abstract (ZH)**: 实时视频超分辨率中保留丰富空间细节和时间连贯性的再现是一个具有挑战性的问题，特别是在利用稳定扩散（SD）等预训练生成模型合成逼真细节时。现有的基于SD的实时视频超分辨率方法往往在空间细节和时间一致性之间权衡，导致视觉质量不佳。我们argue认为关键在于如何有效提取低质量输入视频中的去噪鲁棒时间一致性先验，并在保留提取的一致性先验的同时增强视频细节。为此，我们提出了一种双LoRA学习（DLoRAL）范式，训练一个有效的基于SD的一步扩散模型，同时实现逼真帧细节和时间一致性。具体来说，我们引入了一个跨帧检索（CFR）模块来聚合跨帧的互补信息，并训练一个一致性LoRA（C-LoRA）来从降质输入中学习鲁棒的时间表示。在一致性学习之后，我们固定CFR和C-LoRA模块，并训练一个细节LoRA（D-LoRA）来增强空间细节，同时与C-LoRA定义的时间空间对齐以保持时间连贯性。两个阶段交替迭代优化，协作生成一致性和细节丰富的输出。在推理过程中，两个LoRA分支合并到SD模型中，允许在单一扩散步骤中高效地实现高质量的视频恢复。实验表明，DLoRAL在准确性和速度上均表现出色。代码和模型可在以下链接获取。 

---
# Towards Explainable Indoor Localization: Interpreting Neural Network Learning on Wi-Fi Fingerprints Using Logic Gates 

**Title (ZH)**: 基于逻辑门解释的室内定位可解释性研究：解析Wi-Fi指纹学习過程 

**Authors**: Danish Gufran, Sudeep Pasricha  

**Link**: [PDF](https://arxiv.org/pdf/2506.15559)  

**Abstract**: Indoor localization using deep learning (DL) has demonstrated strong accuracy in mapping Wi-Fi RSS fingerprints to physical locations; however, most existing DL frameworks function as black-box models, offering limited insight into how predictions are made or how models respond to real-world noise over time. This lack of interpretability hampers our ability to understand the impact of temporal variations - caused by environmental dynamics - and to adapt models for long-term reliability. To address this, we introduce LogNet, a novel logic gate-based framework designed to interpret and enhance DL-based indoor localization. LogNet enables transparent reasoning by identifying which access points (APs) are most influential for each reference point (RP) and reveals how environmental noise disrupts DL-driven localization decisions. This interpretability allows us to trace and diagnose model failures and adapt DL systems for more stable long-term deployments. Evaluations across multiple real-world building floorplans and over two years of temporal variation show that LogNet not only interprets the internal behavior of DL models but also improves performance-achieving up to 1.1x to 2.8x lower localization error, 3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to prior DL-based models. 

**Abstract (ZH)**: 基于逻辑门的室内定位解释框架LogNet：提高DL模型的可解释性和性能 

---
# DAILOC: Domain-Incremental Learning for Indoor Localization using Smartphones 

**Title (ZH)**: DAILOC：基于智能手机的领域增量学习室内定位方法 

**Authors**: Akhil Singampalli, Danish Gufran, Sudeep Pasricha  

**Link**: [PDF](https://arxiv.org/pdf/2506.15554)  

**Abstract**: Wi-Fi fingerprinting-based indoor localization faces significant challenges in real-world deployments due to domain shifts arising from device heterogeneity and temporal variations within indoor environments. Existing approaches often address these issues independently, resulting in poor generalization and susceptibility to catastrophic forgetting over time. In this work, we propose DAILOC, a novel domain-incremental learning framework that jointly addresses both temporal and device-induced domain shifts. DAILOC introduces a novel disentanglement strategy that separates domain shifts from location-relevant features using a multi-level variational autoencoder. Additionally, we introduce a novel memory-guided class latent alignment mechanism to address the effects of catastrophic forgetting over time. Experiments across multiple smartphones, buildings, and time instances demonstrate that DAILOC significantly outperforms state-of-the-art methods, achieving up to 2.74x lower average error and 4.6x lower worst-case error. 

**Abstract (ZH)**: 基于Wi-Fi指纹的室内定位在实际部署中面临着因设备异质性和室内环境内的时域变化引起的大域转移问题，现有方法往往独立解决这些问题，导致泛化能力差且容易在长时间内发生灾难性遗忘。本文提出DAILOC，一种新的大域增量学习框架，同时解决时域和设备引起的领域转移问题。DAILOC引入了一种新颖的分离策略，利用多级变异自编码器将领域转移与位置相关的特征分离，并引入了一种新颖的基于记忆的类别潜在对齐机制来应对时间内的灾难性遗忘。实验表明，DAILOC在多个智能手机、建筑物和时间实例上显著优于现有方法，平均误差降低了2.74倍，最坏情况误差降低了4.6倍。 

---
# CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse Myocardial Scar Synthesis and Segmentation 

**Title (ZH)**: 临床指导的LGE增强以实现真实和多样的心肌疤痕合成与分割 

**Authors**: Farheen Ramzan, Yusuf Kiberu, Nikesh Jathanna, Shahnaz Jamil-Copley, Richard H. Clayton, Chen, Chen  

**Link**: [PDF](https://arxiv.org/pdf/2506.15549)  

**Abstract**: Deep learning-based myocardial scar segmentation from late gadolinium enhancement (LGE) cardiac MRI has shown great potential for accurate and timely diagnosis and treatment planning for structural cardiac diseases. However, the limited availability and variability of LGE images with high-quality scar labels restrict the development of robust segmentation models. To address this, we introduce CLAIM: \textbf{C}linically-Guided \textbf{L}GE \textbf{A}ugmentation for Real\textbf{i}stic and Diverse \textbf{M}yocardial Scar Synthesis and Segmentation framework, a framework for anatomically grounded scar generation and segmentation. At its core is the SMILE module (Scar Mask generation guided by cLinical knowledgE), which conditions a diffusion-based generator on the clinically adopted AHA 17-segment model to synthesize images with anatomically consistent and spatially diverse scar patterns. In addition, CLAIM employs a joint training strategy in which the scar segmentation network is optimized alongside the generator, aiming to enhance both the realism of synthesized scars and the accuracy of the scar segmentation performance. Experimental results show that CLAIM produces anatomically coherent scar patterns and achieves higher Dice similarity with real scar distributions compared to baseline models. Our approach enables controllable and realistic myocardial scar synthesis and has demonstrated utility for downstream medical imaging task. 

**Abstract (ZH)**: 基于深度学习的晚期钆增强（LGE）心脏MRI心肌疤痕分割在结构性心脏疾病的准确和及时诊断及治疗规划中展现了巨大的潜力。然而，高质量疤痕标签的LGE图像的有限可用性和变异性限制了稳健分割模型的发展。为了解决这一问题，我们引入了CLAIM：基于临床指导的LGE增强现实和多样心肌疤痕合成与分割框架，一种基于解剖学的心肌疤痕生成和分割框架。其核心是SMILE模块（由临床知识指导的心肌疤痕掩模生成），该模块以临床采用的AHA 17段模型为条件，以生成具有解剖一致性且空间多样性的心肌疤痕图样。此外，CLAIM采用了一种联合训练策略，使疤痕分割网络与生成器同步优化，旨在提高合成疤痕的现实性和疤痕分割性能的准确性。实验结果表明，CLAIM生成了解剖学上连贯的疤痕图样，并在与真实疤痕分布的Dice相似度方面超过了基线模型。我们的方法实现了可控且现实的心肌疤痕合成，并在下游医疗影像任务中展现了应用价值。 

---
# Learning Algorithms in the Limit 

**Title (ZH)**: 限届学习算法 

**Authors**: Hristo Papazov, Nicolas Flammarion  

**Link**: [PDF](https://arxiv.org/pdf/2506.15543)  

**Abstract**: This paper studies the problem of learning computable functions in the limit by extending Gold's inductive inference framework to incorporate \textit{computational observations} and \textit{restricted input sources}. Complimentary to the traditional Input-Output Observations, we introduce Time-Bound Observations, and Policy-Trajectory Observations to study the learnability of general recursive functions under more realistic constraints. While input-output observations do not suffice for learning the class of general recursive functions in the limit, we overcome this learning barrier by imposing computational complexity constraints or supplementing with approximate time-bound observations. Further, we build a formal framework around observations of \textit{computational agents} and show that learning computable functions from policy trajectories reduces to learning rational functions from input and output, thereby revealing interesting connections to finite-state transducer inference. On the negative side, we show that computable or polynomial-mass characteristic sets cannot exist for the class of linear-time computable functions even for policy-trajectory observations. 

**Abstract (ZH)**: 本文通过将Gold的归纳推理框架扩展以融入计算观察和受限输入源，研究在极限学习可计算函数的问题。作为传统输入-输出观察的补充，我们引入了时间限定观察和策略-轨迹观察，以在更现实的约束条件下研究通用递归函数的学习能力。尽管输入-输出观察不足以在极限学习中覆盖所有通用递归函数类，但通过施加计算复杂性约束或补充近似时间限定观察，我们克服了这一学习障碍。我们围绕计算代理的观察建立了一套正式框架，表明从策略轨迹学习可计算函数等价于从输入和输出学习有理函数，从而揭示了有限状态转换推测的有趣联系。在负面方面，我们证明即使在策略-轨迹观察下，线性时间可计算函数或多项式质量特征集也不存在。 

---
# Intrinsic and Extrinsic Organized Attention: Softmax Invariance and Network Sparsity 

**Title (ZH)**: 内在与外在组织的注意力：Softmax 不变性与网络稀疏性 

**Authors**: Oluwadamilola Fasina, Ruben V.C. Pohle, Pei-Chun Su, Ronald R. Coifman  

**Link**: [PDF](https://arxiv.org/pdf/2506.15541)  

**Abstract**: We examine the intrinsic (within the attention head) and extrinsic (amongst the attention heads) structure of the self-attention mechanism in transformers. Theoretical evidence for invariance of the self-attention mechanism to softmax activation is obtained by appealing to paradifferential calculus, (and is supported by computational examples), which relies on the intrinsic organization of the attention heads. Furthermore, we use an existing methodology for hierarchical organization of tensors to examine network structure by constructing hierarchal partition trees with respect to the query, key, and head axes of network 3-tensors. Such an organization is consequential since it allows one to profitably execute common signal processing tasks on a geometry where the organized network 3-tensors exhibit regularity. We exemplify this qualitatively, by visualizing the hierarchical organization of the tree comprised of attention heads and the diffusion map embeddings, and quantitatively by investigating network sparsity with the expansion coefficients of individual attention heads and the entire network with respect to the bi and tri-haar bases (respectively) on the space of queries, keys, and heads of the network. To showcase the utility of our theoretical and methodological findings, we provide computational examples using vision and language transformers. The ramifications of these findings are two-fold: (1) a subsequent step in interpretability analysis is theoretically admitted, and can be exploited empirically for downstream interpretability tasks (2) one can use the network 3-tensor organization for empirical network applications such as model pruning (by virtue of network sparsity) and network architecture comparison. 

**Abstract (ZH)**: 我们探讨了变压器中自注意力机制的内在（_within each attention head_）与外在（_among the attention heads_）结构。通过调和差分微积分，获得了自注意力机制对softmax激活不变性的理论证据（并由计算例子支持），这一证据依赖于注意力头的内在组织结构。此外，我们利用现有的张量分层组织方法，通过构建与网络3张量的query、key和head轴相关的分层分区树，来研究网络结构。这种组织方式是有重要意义的，因为它允许我们在3张量有序性表现出规律性的几何上执行诸如信号处理等任务。我们从定性的角度通过可视化由注意力头和扩散映射嵌入组成的树的分层组织来举例说明这一点，从定量的角度通过检查各个注意力头以及整个网络在查询、键和头空间中相对于双haar和三haar基的展开系数来研究网络稀疏性。为了展示我们理论和方法论发现的应用价值，我们提供了视觉和语言变压器的计算例子。这些发现的影响是两方面的：（1）随后的可解释性分析步骤具有理论上的可行性，可以为下游可解释性任务提供经验支持；（2）可以利用3张量的网络组织来进行如模型剪枝（通过网络稀疏性）和网络架构比较等经验性网络应用。 

---
# Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework 

**Title (ZH)**: 用PRISM捕捉多义性：一种多概念特征描述框架 

**Authors**: Laura Kopf, Nils Feldhus, Kirill Bykov, Philine Lou Bommer, Anna Hedström, Marina M.-C. Höhne, Oliver Eberle  

**Link**: [PDF](https://arxiv.org/pdf/2506.15538)  

**Abstract**: Automated interpretability research aims to identify concepts encoded in neural network features to enhance human understanding of model behavior. Current feature description methods face two critical challenges: limited robustness and the flawed assumption that each neuron encodes only a single concept (monosemanticity), despite growing evidence that neurons are often polysemantic. This assumption restricts the expressiveness of feature descriptions and limits their ability to capture the full range of behaviors encoded in model internals. To address this, we introduce Polysemantic FeatuRe Identification and Scoring Method (PRISM), a novel framework that captures the inherent complexity of neural network features. Unlike prior approaches that assign a single description per feature, PRISM provides more nuanced descriptions for both polysemantic and monosemantic features. We apply PRISM to language models and, through extensive benchmarking against existing methods, demonstrate that our approach produces more accurate and faithful feature descriptions, improving both overall description quality (via a description score) and the ability to capture distinct concepts when polysemanticity is present (via a polysemanticity score). 

**Abstract (ZH)**: 基于多义性特征识别和评分方法的自动可解释性研究 

---
# RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented Generation 

**Title (ZH)**: RePCS: 在LLM驱动的检索增强生成中诊断数据记忆化 

**Authors**: Le Vu Anh, Nguyen Viet Anh, Mehmet Dik, Luong Van Nghia  

**Link**: [PDF](https://arxiv.org/pdf/2506.15513)  

**Abstract**: Retrieval-augmented generation (RAG) has become a common strategy for updating large language model (LLM) responses with current, external information. However, models may still rely on memorized training data, bypass the retrieved evidence, and produce contaminated outputs. We introduce Retrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects such behavior without requiring model access or retraining. RePCS compares two inference paths: (i) a parametric path using only the query, and (ii) a retrieval-augmented path using both the query and retrieved context by computing the Kullback-Leibler (KL) divergence between their output distributions. A low divergence suggests that the retrieved context had minimal impact, indicating potential memorization. This procedure is model-agnostic, requires no gradient or internal state access, and adds only a single additional forward pass. We further derive PAC-style guarantees that link the KL threshold to user-defined false positive and false negative rates. On the Prompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result outperforms the strongest prior method by 6.5 percentage points while keeping latency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight, black-box safeguard to verify whether a RAG system meaningfully leverages retrieval, making it especially valuable in safety-critical applications. 

**Abstract (ZH)**: 检索增强生成(RAG)的检索路径污染评分（RePCS）：一种无需模型访问或重新训练的诊断方法 

---
# Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach 

**Title (ZH)**: 基于LangChain和GPT集成的CoT增强提示工程方法优化Web-Based AI查询检索 

**Authors**: Wenqi Guan, Yang Fang  

**Link**: [PDF](https://arxiv.org/pdf/2506.15512)  

**Abstract**: Large Language Models have brought a radical change in the process of remote learning students, among other aspects of educative activities. Current retrieval of remote learning resources lacks depth in contextual meaning that provides comprehensive information on complex student queries. This work proposes a novel approach to enhancing remote learning retrieval by integrating GPT-based models within the LangChain framework. We achieve this system in a more intuitive and productive manner using CoT reasoning and prompt engineering. The framework we propose puts much emphasis on increasing the precision and relevance of the retrieval results to return comprehensive and contextually enriched explanations and resources that best suit each student's needs. We also assess the effectiveness of our approach against paradigmatic LLMs and report improvements in user satisfaction and learning outcomes. 

**Abstract (ZH)**: 大型语言模型带来了远程学习过程的深刻变革，以及其他教育活动方面。现有的远程学习资源检索缺乏对复杂学生查询的深刻语境意义。本工作提出了一种将基于GPT的模型融入LangChain框架内的新颖方法，通过CoT推理和提示工程以更直观和高效的方式提高远程学习检索。我们所提出的框架强调提高检索结果的精度和相关性，返回全面且语境丰富的解释和资源，以满足每位学生的需求。我们还评估了我们方法的有效性，并针对典型的LLM进行了比较，报告了用户满意度和学习成果的改进。 

---
# Over-squashing in Spatiotemporal Graph Neural Networks 

**Title (ZH)**: 时空图神经网络中的过度压缩 

**Authors**: Ivan Marisca, Jacob Bamberger, Cesare Alippi, Michael M. Bronstein  

**Link**: [PDF](https://arxiv.org/pdf/2506.15507)  

**Abstract**: Graph Neural Networks (GNNs) have achieved remarkable success across various domains. However, recent theoretical advances have identified fundamental limitations in their information propagation capabilities, such as over-squashing, where distant nodes fail to effectively exchange information. While extensively studied in static contexts, this issue remains unexplored in Spatiotemporal GNNs (STGNNs), which process sequences associated with graph nodes. Nonetheless, the temporal dimension amplifies this challenge by increasing the information that must be propagated. In this work, we formalize the spatiotemporal over-squashing problem and demonstrate its distinct characteristics compared to the static case. Our analysis reveals that counterintuitively, convolutional STGNNs favor information propagation from points temporally distant rather than close in time. Moreover, we prove that architectures that follow either time-and-space or time-then-space processing paradigms are equally affected by this phenomenon, providing theoretical justification for computationally efficient implementations. We validate our findings on synthetic and real-world datasets, providing deeper insights into their operational dynamics and principled guidance for more effective designs. 

**Abstract (ZH)**: Graph神经网络（GNNs）在各个领域取得了显著的成功。然而，近年来的理论进展揭示了它们在信息传播能力上的根本限制，例如过榨干现象，即远距离节点无法有效交换信息。虽然在静态环境中这项问题得到了广泛研究，但在处理与图节点关联的时间空间序列的时空GNNs（STGNNs）中，这一问题仍未被探索。而时间维度的增加通过提高需要传播的信息量来加剧这一挑战。在本文中，我们正式化了时空过榨干问题，并展示了它与静态情况的区别特征。我们的分析表明，出乎意料的是，卷积时空GNNs更倾向于从时间上远离的节点传播信息，而不是时间上接近的节点。此外，我们证明了遵循时间和空间或时间后处理空间处理范式的架构都会受到这种现象的影响，从而为高效实现提供了理论依据。我们在合成数据集和真实世界数据集上验证了我们的发现，提供了对其操作动态的更深入见解，并为更有效的设计提供了原则性指导。 

---
# Pixel-level Certified Explanations via Randomized Smoothing 

**Title (ZH)**: 像素级认证解释方法：随机平滑 

**Authors**: Alaa Anani, Tobias Lorenz, Mario Fritz, Bernt Schiele  

**Link**: [PDF](https://arxiv.org/pdf/2506.15499)  

**Abstract**: Post-hoc attribution methods aim to explain deep learning predictions by highlighting influential input pixels. However, these explanations are highly non-robust: small, imperceptible input perturbations can drastically alter the attribution map while maintaining the same prediction. This vulnerability undermines their trustworthiness and calls for rigorous robustness guarantees of pixel-level attribution scores. We introduce the first certification framework that guarantees pixel-level robustness for any black-box attribution method using randomized smoothing. By sparsifying and smoothing attribution maps, we reformulate the task as a segmentation problem and certify each pixel's importance against $\ell_2$-bounded perturbations. We further propose three evaluation metrics to assess certified robustness, localization, and faithfulness. An extensive evaluation of 12 attribution methods across 5 ImageNet models shows that our certified attributions are robust, interpretable, and faithful, enabling reliable use in downstream tasks. Our code is at this https URL. 

**Abstract (ZH)**: 后验归因方法通过突出显示有影响力的输入像素来解释深度学习预测，但这些解释高度不 robust：即使是不可感知的输入扰动也可能大幅改变归因图，同时保持相同的预测。这种脆弱性损害了其可靠性，并要求像素级归因得分的严格 robustness 保证。我们引入了第一个使用随机化平滑技术为任何黑盒归因方法提供像素级 robustness 保证的认证框架。通过稀疏化和平滑化归因图，我们将任务重新形式化为分割问题，并对每个像素的重要性在 $\ell_2$ 界定的扰动下进行认证。我们进一步提出了三种评估认证 robustness、定位和忠实地度量标准。在对 5 种 ImageNet 模型的 12 种归因方法进行广泛评估后，结果显示我们认证的归因是 robust、可解释且忠实的，可在下游任务中可靠使用。我们的代码位于 <https://>。 

---
# SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling 

**Title (ZH)**: SPARE: 单次标注与参考导向评估自动过程监督和奖励建模 

**Authors**: Md Imbesat Hassan Rizvi, Xiaodan Zhu, Iryna Gurevych  

**Link**: [PDF](https://arxiv.org/pdf/2506.15498)  

**Abstract**: Process or step-wise supervision has played a crucial role in advancing complex multi-step reasoning capabilities of Large Language Models (LLMs). However, efficient, high-quality automated process annotation remains a significant challenge. To address this, we introduce Single-Pass Annotation with Reference-Guided Evaluation (SPARE), a novel structured framework that enables single-pass, per-step annotation by aligning each solution step to one or multiple steps in a reference solution, accompanied by explicit reasoning for evaluation. We show that reference-guided step-level evaluation effectively facilitates process supervision on four datasets spanning three domains: mathematical reasoning, multi-hop compositional question answering, and spatial reasoning. We demonstrate that SPARE, when compared to baselines, improves reasoning performance when used for: (1) fine-tuning models in an offline RL setup for inference-time greedy-decoding, and (2) training reward models for ranking/aggregating multiple LLM-generated outputs. Additionally, SPARE achieves competitive performance on challenging mathematical datasets while offering 2.6 times greater efficiency, requiring only 38% of the runtime, compared to tree search-based automatic annotation. The codebase, along with a trained SPARE-PRM model, is publicly released to facilitate further research and reproducibility. 

**Abstract (ZH)**: 单次通过参考引导评估的标注框架（SPARE）：提高大型语言模型多层次推理能力的新范式 

---
# GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects 

**Title (ZH)**: GenHOI: 依赖文本泛化生成未知对象的4D人体-对象交互合成 

**Authors**: Shujia Li, Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Yutong Ban  

**Link**: [PDF](https://arxiv.org/pdf/2506.15483)  

**Abstract**: While diffusion models and large-scale motion datasets have advanced text-driven human motion synthesis, extending these advances to 4D human-object interaction (HOI) remains challenging, mainly due to the limited availability of large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel two-stage framework aimed at achieving two key objectives: 1) generalization to unseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the initial stage of our framework, we employ an Object-AnchorNet to reconstruct sparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI datasets, thereby mitigating the dependence on large-scale 4D HOI datasets. Subsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the second stage to seamlessly interpolate sparse 3D HOI keyframes into densely temporally coherent 4D HOI sequences. To enhance the quality of generated 4D HOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to extract human-object contact patterns and a novel Contact-Aware HOI Attention to effectively integrate the contact signals into diffusion models. Experimental results show that we achieve state-of-the-art results on the publicly available OMOMO and 3D-FUTURE datasets, demonstrating strong generalization abilities to unseen objects, while enabling high-fidelity 4D HOI generation. 

**Abstract (ZH)**: GenHOI：针对未见物体的高保真4D人体-物体交互合成 

---
# Context-Informed Grounding Supervision 

**Title (ZH)**: 基于上下文的语义关联监督 

**Authors**: Hyunji Lee, Seunghyun Yoon, Yunjae Won, Hanseok Oh, Geewook Kim, Trung Bui, Franck Dernoncourt, Elias Stengel-Eskin, Mohit Bansal, Minjoon Seo  

**Link**: [PDF](https://arxiv.org/pdf/2506.15480)  

**Abstract**: Large language models (LLMs) are often supplemented with external knowledge to provide information not encoded in their parameters or to reduce hallucination. In such cases, we expect the model to generate responses by grounding its response in the provided external context. However, prior work has shown that simply appending context at inference time does not ensure grounded generation. To address this, we propose Context-INformed Grounding Supervision (CINGS), a post-training supervision in which the model is trained with relevant context prepended to the response, while computing the loss only over the response tokens and masking out the context. Our experiments demonstrate that models trained with CINGS exhibit stronger grounding in both textual and visual domains compared to standard instruction-tuned models. In the text domain, CINGS outperforms other training methods across 11 information-seeking datasets and is complementary to inference-time grounding techniques. In the vision-language domain, replacing a vision-language model's LLM backbone with a CINGS-trained model reduces hallucinations across four benchmarks and maintains factual consistency throughout the generated response. This improved grounding comes without degradation in general downstream performance. Finally, we analyze the mechanism underlying the enhanced grounding in CINGS and find that it induces a shift in the model's prior knowledge and behavior, implicitly encouraging greater reliance on the external context. 

**Abstract (ZH)**: 大型语言模型（LLMs）通常通过附加外部知识来提供模型参数中未编码的信息，或减少幻觉。在这种情况下，我们期望模型将其响应扎根于提供的外部上下文中。然而，先前的工作表明，仅仅在推理时附加上下文并不能确保扎根生成。为了解决这个问题，我们提出了上下文指引的扎根监督（CINGS），这是一种后训练监督方法，在此方法中，模型在响应前附加相关上下文进行训练，但在计算损失时仅考虑响应标记，同时屏蔽上下文。我们的实验表明，使用CINGS训练的模型在文本和视觉领域中展现出更强的扎根能力，优于标准指令调优模型。在文本领域，CINGS在11个信息寻求数据集中均优于其他训练方法，并且与推理时的扎根技术互补。在视觉-语言领域，用CINGS训练的模型替换视觉-语言模型的LLM骨干可以减少四个基准测试中的幻觉，并在整个生成的响应中保持事实一致性。这种改进的扎根并未牺牲通用下游性能。最后，我们分析了CINGS中增强扎根的机制，并发现它促使模型的先验知识和行为发生了转变，隐式地鼓励模型更多依赖外部上下文。 

---
# Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI 

**Title (ZH)**: 基于人类与AI之间梅尔索-哈斯廷斯交互的协同创作学习 

**Authors**: Ryota Okumura, Tadahiro Taniguchi, Akira Taniguchi, Yoshinobu Hagiwara  

**Link**: [PDF](https://arxiv.org/pdf/2506.15468)  

**Abstract**: We propose co-creative learning as a novel paradigm where humans and AI, i.e., biological and artificial agents, mutually integrate their partial perceptual information and knowledge to construct shared external representations, a process we interpret as symbol emergence. Unlike traditional AI teaching based on unilateral knowledge transfer, this addresses the challenge of integrating information from inherently different modalities. We empirically test this framework using a human-AI interaction model based on the Metropolis-Hastings naming game (MHNG), a decentralized Bayesian inference mechanism. In an online experiment, 69 participants played a joint attention naming game (JA-NG) with one of three computer agent types (MH-based, always-accept, or always-reject) under partial observability. Results show that human-AI pairs with an MH-based agent significantly improved categorization accuracy through interaction and achieved stronger convergence toward a shared sign system. Furthermore, human acceptance behavior aligned closely with the MH-derived acceptance probability. These findings provide the first empirical evidence for co-creative learning emerging in human-AI dyads via MHNG-based interaction. This suggests a promising path toward symbiotic AI systems that learn with humans, rather than from them, by dynamically aligning perceptual experiences, opening a new venue for symbiotic AI alignment. 

**Abstract (ZH)**: 我们提出共创学习作为一种新型范式，人类和AI，即生物和人工代理，相互整合其部分感知信息和知识以构建共享的外部表示，我们认为这一过程是符号 emergence。不同于传统的单向知识转移的AI教学，这种方法解决了来自固有不同模态的信息整合挑战。我们通过基于Metropolis-Hastings命名游戏（MHNG）的人机交互模型，利用分散的贝叶斯推断机制，进行了实证测试。在线实验中，69名参与者在部分可观测性条件下，与三种计算机代理类型之一（基于MH、总是接受或总是拒绝）共同参与注意力共享命名游戏（JA-NG）。结果显示，与基于MH的代理交互的人机配对在交互中显著提高了分类准确性，并且更趋于达成共享符号系统的更强一致性。此外，人类的接受行为与MH推断出的接受概率高度一致。这些发现提供了共创学习在基于MHNG的人机交互中首次实证证据。这表明，通过动态对齐感知体验，朝向共生AI系统的潜在路径，这种系统与人类共同学习而非仅从人类学习，从而为共生AI对齐开辟新的前景。 

---
# RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation 

**Title (ZH)**: RE-想象：推理评价中的符号基准合成 

**Authors**: Xinnuo Xu, Rachel Lawrence, Kshitij Dubey, Atharva Pandey, Risa Ueno, Fabian Falck, Aditya V. Nori, Rahul Sharma, Amit Sharma, Javier Gonzalez  

**Link**: [PDF](https://arxiv.org/pdf/2506.15455)  

**Abstract**: Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true reasoning or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE, a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy. 

**Abstract (ZH)**: 近期的大规模语言模型在推理基准测试中展示了高准确性，但观察到的结果尚不清楚是真正的推理能力还是训练集的统计召回。受因果梯子（Pearl, 2009）及其三个层次（关联、干预和反事实）的启发，本文提出了一种框架，即RE-IMAGINE，用于刻画大规模语言模型的推理能力层次，并开发了一种自动管道生成不同层次的问题变体。通过在中间符号表示中改变问题，RE-IMAGINE能够生成仅依赖记忆无法解决的任意多个问题。此外，该框架具有通用性，可以应用于包括数学、代码和逻辑在内的多个推理领域。我们利用四个广泛使用的基准测试评估了几类大规模语言模型，并观察到当模型被问及问题变体时性能下降。这些评估表明，模型在过去表现中对统计召回存在一定依赖，并开启了针对推理层次内技能的研究。 

---
# Uncovering Intention through LLM-Driven Code Snippet Description Generation 

**Title (ZH)**: 通过LLM驱动的代码片段描述生成揭示意图 

**Authors**: Yusuf Sulistyo Nugroho, Farah Danisha Salam, Brittany Reid, Raula Gaikovina Kula, Kazumasa Shimari, Kenichi Matsumoto  

**Link**: [PDF](https://arxiv.org/pdf/2506.15453)  

**Abstract**: Documenting code snippets is essential to pinpoint key areas where both developers and users should pay attention. Examples include usage examples and other Application Programming Interfaces (APIs), which are especially important for third-party libraries. With the rise of Large Language Models (LLMs), the key goal is to investigate the kinds of description developers commonly use and evaluate how well an LLM, in this case Llama, can support description generation. We use NPM Code Snippets, consisting of 185,412 packages with 1,024,579 code snippets. From there, we use 400 code snippets (and their descriptions) as samples. First, our manual classification found that the majority of original descriptions (55.5%) highlight example-based usage. This finding emphasizes the importance of clear documentation, as some descriptions lacked sufficient detail to convey intent. Second, the LLM correctly identified the majority of original descriptions as "Example" (79.75%), which is identical to our manual finding, showing a propensity for generalization. Third, compared to the originals, the produced description had an average similarity score of 0.7173, suggesting relevance but room for improvement. Scores below 0.9 indicate some irrelevance. Our results show that depending on the task of the code snippet, the intention of the document may differ from being instructions for usage, installations, or descriptive learning examples for any user of a library. 

**Abstract (ZH)**: 记录代码片段对于确定开发者和用户应关注的关键区域至关重要。示例包括使用示例和其他应用程序编程接口（APIs），特别是在第三方库方面。随着大型语言模型（LLMs）的兴起，关键目标是调查开发人员常用的描述种类，并评估在这种情况下 llama 生成描述的能力。我们使用 NPM 代码片段，包含 185,412 个软件包和 1,024,579 个代码片段。从中，我们使用 400 个代码片段及其描述作为样本。首先，人工分类发现原始描述中大多数（55.5%）强调基于示例的使用。这一发现强调了清晰文档的重要性，因为一些描述缺乏足够的细节来传达意图。其次，LLM 正确识别了大多数原始描述为“示例”（79.75%），这与我们的人工发现一致，表明其具有一般化倾向。第三，与原始描述相比，生成的描述的平均相似度得分为 0.7173，表明相关性但尚有改进空间。低于 0.9 的得分表明一定程度的不相关。我们的结果显示，根据代码片段的任务，文档的目的可能从使用说明、安装指南到为任何库使用者提供的描述性学习示例都有所不同。 

---
# Warping and Matching Subsequences Between Time Series 

**Title (ZH)**: 时间序列之间的时间 warp 和子序列匹配 

**Authors**: Simiao Lin, Wannes Meert, Pieter Robberechts, Hendrik Blockeel  

**Link**: [PDF](https://arxiv.org/pdf/2506.15452)  

**Abstract**: Comparing time series is essential in various tasks such as clustering and classification. While elastic distance measures that allow warping provide a robust quantitative comparison, a qualitative comparison on top of them is missing. Traditional visualizations focus on point-to-point alignment and do not convey the broader structural relationships at the level of subsequences. This limitation makes it difficult to understand how and where one time series shifts, speeds up or slows down with respect to another. To address this, we propose a novel technique that simplifies the warping path to highlight, quantify and visualize key transformations (shift, compression, difference in amplitude). By offering a clearer representation of how subsequences match between time series, our method enhances interpretability in time series comparison. 

**Abstract (ZH)**: 在聚类和分类等多种任务中，比较时间序列是必不可少的。虽然允许拉伸的弹性距离度量可以提供稳健的定量比较，但其上的定性比较却缺失。传统的可视化方法侧重于点对点对齐，无法传达子序列层面的 broader 结构关系。这一限制使得理解一个时间序列相对于另一个如何、何时以及在何处发生位移、加速或减速变得困难。为了解决这个问题，我们提出了一种新颖的技术，简化拉伸路径以突出、量化和可视化关键转换（位移、压缩、振幅差异）。通过提供时间序列之间子序列匹配的更清晰表示，我们的方法增强了时间序列比较的可解释性。 

---
# Zero-Shot Reinforcement Learning Under Partial Observability 

**Title (ZH)**: 零样本部分可观测性的强化学习 

**Authors**: Scott Jeen, Tom Bewley, Jonathan M. Cullen  

**Link**: [PDF](https://arxiv.org/pdf/2506.15446)  

**Abstract**: Recent work has shown that, under certain assumptions, zero-shot reinforcement learning (RL) methods can generalise to any unseen task in an environment after reward-free pre-training. Access to Markov states is one such assumption, yet, in many real-world applications, the Markov state is only partially observable. Here, we explore how the performance of standard zero-shot RL methods degrades when subjected to partially observability, and show that, as in single-task RL, memory-based architectures are an effective remedy. We evaluate our memory-based zero-shot RL methods in domains where the states, rewards and a change in dynamics are partially observed, and show improved performance over memory-free baselines. Our code is open-sourced via: this https URL. 

**Abstract (ZH)**: 近期研究表明，在某些假设下，零样本强化学习（RL）方法在奖励免费预训练之后，可以在环境中任何未见过的任务上泛化。当Markov状态部分可观测时，标准的零样本RL方法的性能会下降，类似单任务RL，基于记忆的架构是一种有效的解决方案。我们在部分可观测状态、奖励和动力学变化的领域评估了我们的基于记忆的零样本RL方法，并展示了相对于无记忆基线的性能提升。我们的代码在以下地址开源：this https URL。 

---
# Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material 

**Title (ZH)**: 混沌3D 2.1：从图像到具有生产就绪PBR材质的高保真3D资产 

**Authors**: Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, Qingxiang Lin, Zeqiang Lai, Xianghui Yang, Huiwen Shi, Zibo Zhao, Bowen Zhang, Hongyu Yan, Lifu Wang, Sicong Liu, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Dongyuan Guo, Junlin Yu, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Shida Wei, Chao Zhang, Yonghao Tan, Yifu Sun, Lin Niu, Shirui Huang, Bojian Zheng, Shu Liu, Shilin Chen, Xiang Yuan, Xiaofeng Yang, Kai Liu, Jianchen Zhu, Peng Chen, Tian Liu, Di Wang, Yuhong Liu, Linus, Jie Jiang, Jingwei Huang, Chunchao Guo  

**Link**: [PDF](https://arxiv.org/pdf/2506.15442)  

**Abstract**: 3D AI-generated content (AIGC) is a passionate field that has significantly accelerated the creation of 3D models in gaming, film, and design. Despite the development of several groundbreaking models that have revolutionized 3D generation, the field remains largely accessible only to researchers, developers, and designers due to the complexities involved in collecting, processing, and training 3D models. To address these challenges, we introduce Hunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a comprehensive, step-by-step guide on processing 3D data, training a 3D generative model, and evaluating its performance using Hunyuan3D 2.1, an advanced system for producing high-resolution, textured 3D assets. The system comprises two core components: the Hunyuan3D-DiT for shape generation and the Hunyuan3D-Paint for texture synthesis. We will explore the entire workflow, including data preparation, model architecture, training strategies, evaluation metrics, and deployment. By the conclusion of this tutorial, you will have the knowledge to finetune or develop a robust 3D generative model suitable for applications in gaming, virtual reality, and industrial design. 

**Abstract (ZH)**: 3D AI生成内容（AIGC）是一个充满激情的领域，极大地加速了游戏、电影和设计中的3D模型创建。尽管已经开发出多种革命性的模型来重塑3D生成，但由于收集、处理和训练3D模型的复杂性，该领域仍主要 доступ仅限于研究人员、开发人员和设计师。为应对这些挑战，本文通过Hunyuan3D 2.1作为案例研究介绍了这一教程。本教程提供了一个全面的、分步骤的指南，用于处理3D数据、训练3D生成模型以及使用Hunyuan3D 2.1（一个生产高分辨率、带纹理的3D资产的先进系统）评估其性能。该系统包括两个核心组件：Hunyuan3D-DiT用于形状生成和Hunyuan3D-Paint用于纹理合成。我们将探索整个工作流程，包括数据准备、模型架构、训练策略、评估指标和部署。通过完成本教程，您将具备调整或开发适用于游戏、虚拟现实和工业设计应用的稳健3D生成模型的知识。 

---
# Reward Models in Deep Reinforcement Learning: A Survey 

**Title (ZH)**: 深度强化学习中的奖励模型：一个综述 

**Authors**: Rui Yu, Shenghua Wan, Yucen Wang, Chen-Xiao Gao, Le Gan, Zongzhang Zhang, De-Chuan Zhan  

**Link**: [PDF](https://arxiv.org/pdf/2506.15421)  

**Abstract**: In reinforcement learning (RL), agents continually interact with the environment and use the feedback to refine their behavior. To guide policy optimization, reward models are introduced as proxies of the desired objectives, such that when the agent maximizes the accumulated reward, it also fulfills the task designer's intentions. Recently, significant attention from both academic and industrial researchers has focused on developing reward models that not only align closely with the true objectives but also facilitate policy optimization. In this survey, we provide a comprehensive review of reward modeling techniques within the deep RL literature. We begin by outlining the background and preliminaries in reward modeling. Next, we present an overview of recent reward modeling approaches, categorizing them based on the source, the mechanism, and the learning paradigm. Building on this understanding, we discuss various applications of these reward modeling techniques and review methods for evaluating reward models. Finally, we conclude by highlighting promising research directions in reward modeling. Altogether, this survey includes both established and emerging methods, filling the vacancy of a systematic review of reward models in current literature. 

**Abstract (ZH)**: 在强化学习中的reward模型技术综述：从背景到评估与展望 

---
# Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI 

**Title (ZH)**: 统一VXAI：可解释人工智能的系统回顾与评估框架 

**Authors**: David Dembinsky, Adriano Lucieri, Stanislav Frolov, Hiba Najjar, Ko Watanabe, Andreas Dengel  

**Link**: [PDF](https://arxiv.org/pdf/2506.15408)  

**Abstract**: Modern AI systems frequently rely on opaque black-box models, most notably Deep Neural Networks, whose performance stems from complex architectures with millions of learned parameters. While powerful, their complexity poses a major challenge to trustworthiness, particularly due to a lack of transparency. Explainable AI (XAI) addresses this issue by providing human-understandable explanations of model behavior. However, to ensure their usefulness and trustworthiness, such explanations must be rigorously evaluated. Despite the growing number of XAI methods, the field lacks standardized evaluation protocols and consensus on appropriate metrics. To address this gap, we conduct a systematic literature review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a unified framework for the eValuation of XAI (VXAI). We identify 362 relevant publications and aggregate their contributions into 41 functionally similar metric groups. In addition, we propose a three-dimensional categorization scheme spanning explanation type, evaluation contextuality, and explanation quality desiderata. Our framework provides the most comprehensive and structured overview of VXAI to date. It supports systematic metric selection, promotes comparability across methods, and offers a flexible foundation for future extensions. 

**Abstract (ZH)**: 现代AI系统通常依赖于不透明的黑盒模型，尤其是深度神经网络，其性能源自具有数百万个学习参数的复杂架构。尽管这些模型非常强大，但其复杂性对可信度构成了重大挑战，特别是在透明度缺乏的情况下。可解释AI（XAI）通过提供易于人类理解的模型行为解释来解决这一问题。然而，为了确保其有用性和可信度，这些解释必须经过严格的评估。尽管可解释AI方法的数量在不断增加，但该领域缺乏标准化的评估协议和适当度量标准的共识。为了填补这一空白，我们遵循系统评价和荟萃分析的首选报告项目（PRISMA）指南，进行了系统文献综述，并引入了统一的XAI评估框架（VXAI）。我们识别出362篇相关文献，并将其贡献整合为41组功能上相似的度量标准组。此外，我们提出了一种涵盖解释类型、评估背景和解释质量要求的三维分类方案。该框架提供了迄今为止最全面和结构化的VXAI综述。它支持系统的度量标准选择、促进方法之间的可比性，并为未来扩展提供了灵活的基础。 

---
# MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System 

**Title (ZH)**: 多目全景对象SLAM系统 

**Authors**: Miaoxin Pan, Jinnan Li, Yaowen Zhang, Yi Yang, Yufeng Yue  

**Link**: [PDF](https://arxiv.org/pdf/2506.15402)  

**Abstract**: Object-level SLAM offers structured and semantically meaningful environment representations, making it more interpretable and suitable for high-level robotic tasks. However, most existing approaches rely on RGB-D sensors or monocular views, which suffer from narrow fields of view, occlusion sensitivity, and limited depth perception-especially in large-scale or outdoor environments. These limitations often restrict the system to observing only partial views of objects from limited perspectives, leading to inaccurate object modeling and unreliable data association. In this work, we propose MCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully leverages surround-view camera configurations to achieve robust, consistent, and semantically enriched mapping in complex outdoor scenarios. Our approach integrates point features and object-level landmarks enhanced with open-vocabulary semantics. A semantic-geometric-temporal fusion strategy is introduced for robust object association across multiple views, leading to improved consistency and accurate object modeling, and an omnidirectional loop closure module is designed to enable viewpoint-invariant place recognition using scene-level descriptors. Furthermore, the constructed map is abstracted into a hierarchical 3D scene graph to support downstream reasoning tasks. Extensive experiments in real-world demonstrate that MCOO-SLAM achieves accurate localization and scalable object-level mapping with improved robustness to occlusion, pose variation, and environmental complexity. 

**Abstract (ZH)**: 基于多摄像头全景对象的SLAM系统：在复杂户外场景中实现稳健、一致和语义丰富的映射 

---
# A Real-time Endoscopic Image Denoising System 

**Title (ZH)**: 实时内窥镜图像去噪系统 

**Authors**: Yu Xing, Shishi Huang, Meng Lv, Guo Chen, Huailiang Wang, Lingzhi Sui  

**Link**: [PDF](https://arxiv.org/pdf/2506.15395)  

**Abstract**: Endoscopes featuring a miniaturized design have significantly enhanced operational flexibility, portability, and diagnostic capability while substantially reducing the invasiveness of medical procedures. Recently, single-use endoscopes equipped with an ultra-compact analogue image sensor measuring less than 1mm x 1mm bring revolutionary advancements to medical diagnosis. They reduce the structural redundancy and large capital expenditures associated with reusable devices, eliminate the risk of patient infections caused by inadequate disinfection, and alleviate patient suffering. However, the limited photosensitive area results in reduced photon capture per pixel, requiring higher photon sensitivity settings to maintain adequate brightness. In high-contrast medical imaging scenarios, the small-sized sensor exhibits a constrained dynamic range, making it difficult to simultaneously capture details in both highlights and shadows, and additional localized digital gain is required to compensate. Moreover, the simplified circuit design and analog signal transmission introduce additional noise sources. These factors collectively contribute to significant noise issues in processed endoscopic images. In this work, we developed a comprehensive noise model for analog image sensors in medical endoscopes, addressing three primary noise types: fixed-pattern noise, periodic banding noise, and mixed Poisson-Gaussian noise. Building on this analysis, we propose a hybrid denoising system that synergistically combines traditional image processing algorithms with advanced learning-based techniques for captured raw frames from sensors. Experiments demonstrate that our approach effectively reduces image noise without fine detail loss or color distortion, while achieving real-time performance on FPGA platforms and an average PSNR improvement from 21.16 to 33.05 on our test dataset. 

**Abstract (ZH)**: 带有微型化设计的内窥镜显著提升了操作灵活性、便携性和诊断能力，并大幅减少了医疗程序的侵入性。近期，配备超紧凑模拟图像传感器（尺寸小于1mm x 1mm）的一次性内窥镜为医疗诊断带来了革命性的进步。它们减少了与可重复使用设备相关的结构冗余和大规模资本支出，消除了因消毒不彻底导致的患者感染风险，并减轻了患者的痛苦。然而，有限的感光区域导致每个像素的光子捕获减少，需要设置更高的光子敏感度以维持足够的亮度。在高对比度的医疗成像场景中，小型传感器表现出受限的动态范围，难以同时捕捉高光和阴影中的细节，需要额外的局部数字增益来补偿。此外，简化的电路设计和模拟信号传输引入了额外的噪声源。这些因素共同导致了处理后的内窥镜图像中的显著噪声问题。在本文中，我们为医疗内窥镜中的模拟图像传感器开发了一个全面的噪声模型，解决了三种主要的噪声类型：固定模式噪声、周期性条纹噪声和混合泊松-高斯噪声。在此基础上，我们提出了一种结合传统图像处理算法和先进学习技术的混合去噪系统，用于传感器捕获的原始帧。实验结果表明，我们的方法在不牺牲细节和颜色保真度的情况下有效减少了图像噪声，同时在FPGA平台上实现了实时性能，并在我们的测试数据集上平均PSNR提高了从21.16到33.05。 

---
# Evaluation Pipeline for systematically searching for Anomaly Detection Systems 

**Title (ZH)**: 系统性搜索异常检测系统的评估管道 

**Authors**: Florian Rokohl, Alexander Lehnert, Marc Reichenbach  

**Link**: [PDF](https://arxiv.org/pdf/2506.15388)  

**Abstract**: Digitalization in the medical world provides major benefits while making it a target for attackers and thus hard to secure. To deal with network intruders we propose an anomaly detection system on hardware to detect malicious clients in real-time. We meet real-time and power restrictions using FPGAs. Overall system performance is achieved via the presented holistic system evaluation. 

**Abstract (ZH)**: 数字化在医疗领域的应用提供了巨大好处，但也使其成为攻击目标，难以确保安全。为应对网络入侵者，我们提出一种基于硬件的异常检测系统，以实时检测恶意客户端。我们通过使用FPGAs满足实时性和能耗限制。整体系统性能通过呈现的综合评估实现。 

---
# Open-World Object Counting in Videos 

**Title (ZH)**: 开放世界视频中对象计数 

**Authors**: Niki Amini-Naieni, Andrew Zisserman  

**Link**: [PDF](https://arxiv.org/pdf/2506.15368)  

**Abstract**: We introduce a new task of open-world object counting in videos: given a text description, or an image example, that specifies the target object, the objective is to enumerate all the unique instances of the target objects in the video. This task is especially challenging in crowded scenes with occlusions and similar objects, where avoiding double counting and identifying reappearances is crucial. To this end, we make the following contributions: we introduce a model, CountVid, for this task. It leverages an image-based counting model, and a promptable video segmentation and tracking model to enable automated, open-world object counting across video frames. To evaluate its performance, we introduce VideoCount, a new dataset for our novel task built from the TAO and MOT20 tracking datasets, as well as from videos of penguins and metal alloy crystallization captured by x-rays. Using this dataset, we demonstrate that CountVid provides accurate object counts, and significantly outperforms strong baselines. The VideoCount dataset, the CountVid model, and all the code are available at this https URL. 

**Abstract (ZH)**: 开放世界视频中对象计数任务：给定文本描述或图像示例指定目标对象，目标是计算视频中目标对象的所有唯一实例。特别是在遮挡和相似对象众多的场景中，避免重复计数和识别再出现是尤其具有挑战性的任务。为此，我们做出了以下贡献：我们引入了一个名为CountVid的模型来解决这一任务。该模型结合了基于图像的计数模型和可调提示的视频分割与跟踪模型，能够在视频帧间实现自动的开放世界对象计数。为了评估其性能，我们构建了一个新的数据集VideoCount，该项目基于TAO和MOT20跟踪数据集，并包含了通过X射线拍摄的企鹅和金属合金结晶学视频。利用这个数据集，我们展示了CountVid提供了准确的对象计数，并显著优于强基线。VideoCount数据集、CountVid模型以及所有相关代码均可从此链接获得。 

---
# When and How Unlabeled Data Provably Improve In-Context Learning 

**Title (ZH)**: 何时以及如何无标签数据可以证明改进上下文学习 

**Authors**: Yingcong Li, Xiangyu Chang, Muti Kara, Xiaofeng Liu, Amit Roy-Chowdhury, Samet Oymak  

**Link**: [PDF](https://arxiv.org/pdf/2506.15329)  

**Abstract**: Recent research shows that in-context learning (ICL) can be effective even when demonstrations have missing or incorrect labels. To shed light on this capability, we examine a canonical setting where the demonstrations are drawn according to a binary Gaussian mixture model (GMM) and a certain fraction of the demonstrations have missing labels. We provide a comprehensive theoretical study to show that: (1) The loss landscape of one-layer linear attention models recover the optimal fully-supervised estimator but completely fail to exploit unlabeled data; (2) In contrast, multilayer or looped transformers can effectively leverage unlabeled data by implicitly constructing estimators of the form $\sum_{i\ge 0} a_i (X^\top X)^iX^\top y$ with $X$ and $y$ denoting features and partially-observed labels (with missing entries set to zero). We characterize the class of polynomials that can be expressed as a function of depth and draw connections to Expectation Maximization, an iterative pseudo-labeling algorithm commonly used in semi-supervised learning. Importantly, the leading polynomial power is exponential in depth, so mild amount of depth/looping suffices. As an application of theory, we propose looping off-the-shelf tabular foundation models to enhance their semi-supervision capabilities. Extensive evaluations on real-world datasets show that our method significantly improves the semisupervised tabular learning performance over the standard single pass inference. 

**Abstract (ZH)**: 近期研究表明，在上下文学习（ICL）中，即使示例具有缺失或错误的标签，也能取得有效成果。为深入探讨这一能力，我们考察了这样一种经典设置：示例根据二元高斯混合模型（GMM）抽取，其中一部分示例具有缺失标签。我们进行了一项全面的理论研究，表明：（1）单层线性注意力模型的损失景观能够恢复最优的全监督估计器，但完全无法利用未标注数据；（2）相比之下，多层或循环的变压器能够通过隐式构造形如$\sum_{i\ge 0} a_i (X^\top X)^iX^\top y$的估计器有效利用未标注数据，其中$X$和$y$分别表示特征和部分可观测的标签（缺失项设为零）。我们刻画了能够表示为深度函数的多项式类，并将其与期望最大化的迭代伪标签算法联系起来，该算法是半监督学习中常见的算法。重要的是，主要的多项式幂是深度的指数函数，因此轻微的深度/循环即可。作为理论应用，我们提议循环现成的表格基础模型以增强其半监督能力。实证研究显示，我们的方法在实际数据集上的半监督表格学习性能显著优于标准的一次性推理。 

---
# J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image Sensor 

**Title (ZH)**: J3DAI：基于3D堆叠CMOS图像传感器的超小型DNN加速器 

**Authors**: Benoit Tain, Raphael Millet, Romain Lemaire, Michal Szczepanski, Laurent Alacoque, Emmanuel Pluchart, Sylvain Choisnet, Rohit Prasad, Jerome Chossat, Pascal Pierunek, Pascal Vivet, Sebastien Thuries  

**Link**: [PDF](https://arxiv.org/pdf/2506.15316)  

**Abstract**: This paper presents J3DAI, a tiny deep neural network-based hardware accelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial intelligence (AI) chip integrating a Deep Neural Network (DNN)-based accelerator. The DNN accelerator is designed to efficiently perform neural network tasks such as image classification and segmentation. This paper focuses on the digital system of J3DAI, highlighting its Performance-Power-Area (PPA) characteristics and showcasing advanced edge AI capabilities on a CMOS image sensor. To support hardware, we utilized the Aidge comprehensive software framework, which enables the programming of both the host processor and the DNN accelerator. Aidge supports post-training quantization, significantly reducing memory footprint and computational complexity, making it crucial for deploying models on resource-constrained hardware like J3DAI. Our experimental results demonstrate the versatility and efficiency of this innovative design in the field of edge AI, showcasing its potential to handle both simple and computationally intensive tasks. Future work will focus on further optimizing the architecture and exploring new applications to fully leverage the capabilities of J3DAI. As edge AI continues to grow in importance, innovations like J3DAI will play a crucial role in enabling real-time, low-latency, and energy-efficient AI processing at the edge. 

**Abstract (ZH)**: 基于深度神经网络的J3DAI小型硬件加速器：一种适用于3层3D堆叠CMOS图像传感器的人工智能芯片设计及其先进边缘AI能力展示 

---
# MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual Learning 

**Title (ZH)**: MapFM：基于多任务上下文学习的基础模型驱动高精度地图构建 

**Authors**: Leonid Ivanov, Vasily Yuryev, Dmitry Yudin  

**Link**: [PDF](https://arxiv.org/pdf/2506.15313)  

**Abstract**: In autonomous driving, high-definition (HD) maps and semantic maps in bird's-eye view (BEV) are essential for accurate localization, planning, and decision-making. This paper introduces an enhanced End-to-End model named MapFM for online vectorized HD map generation. We show significantly boost feature representation quality by incorporating powerful foundation model for encoding camera images. To further enrich the model's understanding of the environment and improve prediction quality, we integrate auxiliary prediction heads for semantic segmentation in the BEV representation. This multi-task learning approach provides richer contextual supervision, leading to a more comprehensive scene representation and ultimately resulting in higher accuracy and improved quality of the predicted vectorized HD maps. The source code is available at this https URL. 

**Abstract (ZH)**: 在自动驾驶中，鸟瞰视角（BEV）的高定义（HD）地图和语义地图对于准确的位置定位、规划和决策至关重要。本文介绍了一种增强的端到端模型MapFM，用于在线生成矢量化的HD地图。通过融入强大的基础模型来编码摄像头图像，显著提升了特征表示的质量。为进一步丰富模型对环境的理解并提高预测质量，我们将语义分割辅助预测头整合到了BEV表示中。这种多任务学习方法提供了更加丰富的上下文监督，导致更全面的场景表示，从而提高了矢量化HD地图预测的准确性和质量。源代码可在此处访问：this https URL。 

---
# Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation 

**Title (ZH)**: 基于主动学习的Seq2Seq变分自编码器多目标抑制剂生成 

**Authors**: Júlia Vilalta-Mor, Alexis Molina, Laura Ortega Varga, Isaac Filella-Merce, Victor Guallar  

**Link**: [PDF](https://arxiv.org/pdf/2506.15309)  

**Abstract**: Simultaneously optimizing molecules against multiple therapeutic targets remains a profound challenge in drug discovery, particularly due to sparse rewards and conflicting design constraints. We propose a structured active learning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational autoencoder (VAE) into iterative loops designed to balance chemical diversity, molecular quality, and multi-target affinity. Our method alternates between expanding chemically feasible regions of latent space and progressively constraining molecules based on increasingly stringent multi-target docking thresholds. In a proof-of-concept study targeting three related coronavirus main proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently generated a structurally diverse set of pan-inhibitor candidates. We demonstrate that careful timing and strategic placement of chemical filters within this active learning pipeline markedly enhance exploration of beneficial chemical space, transforming the sparse-reward, multi-objective drug design problem into an accessible computational task. Our framework thus provides a generalizable roadmap for efficiently navigating complex polypharmacological landscapes. 

**Abstract (ZH)**: 同时优化多种治疗靶点的分子仍然被认为是药物发现中的一个深刻挑战，尤其是在稀疏奖励和冲突的设计约束方面。我们提出了一种结构化的主动学习（AL）范式，将序列到序列（Seq2Seq）变分自编码器（VAE）集成到迭代循环中，以平衡化学多样性、分子质量和多靶点亲和力。我们的方法交替进行潜在空间中化学可行区域的扩展和基于越来越严格的多靶点对接阈值逐步限制分子。在针对三种相关冠状病毒主蛋白酶（SARS-CoV-2、SARS-CoV、MERS-CoV）的目标概念验证研究中，我们的方式高效地生成了一组结构多样性的广谱抑制剂候选物。我们证明，在此主动学习管道中仔细安排化学滤镜的时间和位置显著增强了有利化学空间的探索，将稀疏奖励、多目标药物设计问题转化为可访问的计算任务。因此，我们的框架提供了一种通用的路线图，以有效地导航复杂的多药理学景观。 

---
# ConLID: Supervised Contrastive Learning for Low-Resource Language Identification 

**Title (ZH)**: ConLID: 监督对比学习在低资源语言识别中的应用 

**Authors**: Negar Foroutan, Jakhongir Saydaliev, Ye Eun Kim, Antoine Bosselut  

**Link**: [PDF](https://arxiv.org/pdf/2506.15304)  

**Abstract**: Language identification (LID) is a critical step in curating multilingual LLM pretraining corpora from web crawls. While many studies on LID model training focus on collecting diverse training data to improve performance, low-resource languages -- often limited to single-domain data, such as the Bible -- continue to perform poorly. To resolve these class imbalance and bias issues, we propose a novel supervised contrastive learning (SCL) approach to learn domain-invariant representations for low-resource languages. Through an extensive analysis, we show that our approach improves LID performance on out-of-domain data for low-resource languages by 3.2%, demonstrating its effectiveness in enhancing LID models. 

**Abstract (ZH)**: 低资源语言网络身份识别的监督对比学习方法 

---
# Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment 

**Title (ZH)**: 群体发现：基于LLM的临床试验招募综述 

**Authors**: Shrestha Ghosh, Moritz Schneider, Carina Reinicke, Carsten Eickhoff  

**Link**: [PDF](https://arxiv.org/pdf/2506.15301)  

**Abstract**: Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet, their adoption in critical domains, such as clinical trial recruitment, remains limited. As trials are designed in natural language and patient data is represented as both structured and unstructured text, the task of matching trials and patients benefits from knowledge aggregation and reasoning abilities of LLMs. Classical approaches are trial-specific and LLMs with their ability to consolidate distributed knowledge hold the potential to build a more general solution. Yet recent applications of LLM-assisted methods rely on proprietary models and weak evaluation benchmarks. In this survey, we are the first to analyze the task of trial-patient matching and contextualize emerging LLM-based approaches in clinical trial recruitment. We critically examine existing benchmarks, approaches and evaluation frameworks, the challenges to adopting LLM technologies in clinical research and exciting future directions. 

**Abstract (ZH)**: 近年来，大规模语言模型在通用领域的自然语言处理任务中取得了显著进步。然而，在如临床试验招募这样关键领域的应用仍然有限。由于试验设计使用自然语言，并且患者数据以结构化和非结构化文本形式存在，通过知识聚合和推理能力匹配试验与患者的任务具有重要意义。传统方法针对特定试验，而拥有汇总分布式知识能力的大规模语言模型具有构建更通用解决方案的潜力。然而，近期的大规模语言模型辅助方法依赖于专有模型和弱评价基准。在这项综述中，我们首次分析了试验-患者匹配任务，并将新兴的大规模语言模型基础方法置于临床试验招募的背景下。我们批判性地评估了现有的基准、方法和评估框架，采用大规模语言模型技术的挑战以及令人兴奋的未来方向。 

---
# Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models 

**Title (ZH)**: 基于衣物意识扩散模型的宽松稀疏惯性传感器人体运动捕捉 

**Authors**: Andela Ilic, Jiaxi Jiang, Paul Streli, Xintong Liu, Christian Holz  

**Link**: [PDF](https://arxiv.org/pdf/2506.15290)  

**Abstract**: Motion capture using sparse inertial sensors has shown great promise due to its portability and lack of occlusion issues compared to camera-based tracking. Existing approaches typically assume that IMU sensors are tightly attached to the human body. However, this assumption often does not hold in real-world scenarios. In this paper, we present a new task of full-body human pose estimation using sparse, loosely attached IMU sensors. To solve this task, we simulate IMU recordings from an existing garment-aware human motion dataset. We developed transformer-based diffusion models to synthesize loose IMU data and estimate human poses based on this challenging loose IMU data. In addition, we show that incorporating garment-related parameters while training the model on simulated loose data effectively maintains expressiveness and enhances the ability to capture variations introduced by looser or tighter garments. Experiments show that our proposed diffusion methods trained on simulated and synthetic data outperformed the state-of-the-art methods quantitatively and qualitatively, opening up a promising direction for future research. 

**Abstract (ZH)**: 基于稀疏松挂惯性传感器的全身人体姿态估计 

---
# Unlocking Post-hoc Dataset Inference with Synthetic Data 

**Title (ZH)**: 基于合成数据的后验数据集推断解锁 

**Authors**: Bihe Zhao, Pratyush Maini, Franziska Boenisch, Adam Dziedzic  

**Link**: [PDF](https://arxiv.org/pdf/2506.15271)  

**Abstract**: The remarkable capabilities of Large Language Models (LLMs) can be mainly attributed to their massive training datasets, which are often scraped from the internet without respecting data owners' intellectual property rights. Dataset Inference (DI) offers a potential remedy by identifying whether a suspect dataset was used in training, thereby enabling data owners to verify unauthorized use. However, existing DI methods require a private set-known to be absent from training-that closely matches the compromised dataset's distribution. Such in-distribution, held-out data is rarely available in practice, severely limiting the applicability of DI. In this work, we address this challenge by synthetically generating the required held-out set. Our approach tackles two key obstacles: (1) creating high-quality, diverse synthetic data that accurately reflects the original distribution, which we achieve via a data generator trained on a carefully designed suffix-based completion task, and (2) bridging likelihood gaps between real and synthetic data, which is realized through post-hoc calibration. Extensive experiments on diverse text datasets show that using our generated data as a held-out set enables DI to detect the original training sets with high confidence, while maintaining a low false positive rate. This result empowers copyright owners to make legitimate claims on data usage and demonstrates our method's reliability for real-world litigations. Our code is available at this https URL. 

**Abstract (ZH)**: 大型语言模型的显著能力主要归因于其庞大的训练数据集，这些数据集通常未经数据所有者的知识产权许可从互联网上抓取。数据集推断（DI）通过识别疑似用于训练的数据集来提供一种潜在的解决方案，从而允许数据所有者验证未经授权的使用。然而，现有的DI方法需要一个私有设置，该设置在训练中不存在且与受攻击数据集的分布高度匹配。这种同分布的保留数据在实践中很少可用，极大地限制了DI的应用。在本文中，我们通过合成生成所需的保留集来应对这一挑战。我们的方法克服了两个关键障碍：(1) 创建高质量、多样化的合成数据，准确反映原始分布，我们通过一个在精心设计的后缀基完成任务上训练的数据生成器来实现；(2) 缩小真实数据和合成数据之间的似然性差距，这通过后处理校准来实现。对多种文本数据集的广泛实验表明，使用我们生成的数据作为保留集可以使DI以高信心检测原始训练集，同时保持较低的假阳性率。这一结果使版权持有者能够合法地对数据使用提出主张，并证明了该方法在实际诉讼中的可靠性。我们的代码可在以下链接获取。 

---
# Domain Adaptation for Image Classification of Defects in Semiconductor Manufacturing 

**Title (ZH)**: 半导体制造中缺陷图像分类的领域适应方法 

**Authors**: Adrian Poniatowski, Natalie Gentner, Manuel Barusco, Davide Dalle Pezze, Samuele Salti, Gian Antonio Susto  

**Link**: [PDF](https://arxiv.org/pdf/2506.15260)  

**Abstract**: In the semiconductor sector, due to high demand but also strong and increasing competition, time to market and quality are key factors in securing significant market share in various application areas. Thanks to the success of deep learning methods in recent years in the computer vision domain, Industry 4.0 and 5.0 applications, such as defect classification, have achieved remarkable success. In particular, Domain Adaptation (DA) has proven highly effective since it focuses on using the knowledge learned on a (source) domain to adapt and perform effectively on a different but related (target) domain. By improving robustness and scalability, DA minimizes the need for extensive manual re-labeling or re-training of models. This not only reduces computational and resource costs but also allows human experts to focus on high-value tasks. Therefore, we tested the efficacy of DA techniques in semi-supervised and unsupervised settings within the context of the semiconductor field. Moreover, we propose the DBACS approach, a CycleGAN-inspired model enhanced with additional loss terms to improve performance. All the approaches are studied and validated on real-world Electron Microscope images considering the unsupervised and semi-supervised settings, proving the usefulness of our method in advancing DA techniques for the semiconductor field. 

**Abstract (ZH)**: 在半导体领域，由于市场需求高且竞争激烈，上市时间和产品质量是确保在各类应用领域获得显著市场份额的关键因素。得益于近年来深度学习方法在计算机视觉领域、 Industry 4.0 和 Industry 5.0 应用（如缺陷分类）中的成功，域适应（DA）技术已 proven 高效有效。特别是，域适应专注于利用（源）域中学到的知识来适应和在不同的但相关（目标）域中表现良好，通过提高鲁棒性和可扩展性，DA 最小化了对大量手动重新标签或重新训练模型的需求。这不仅减少了计算和资源成本，还允许人类专家专注于高价值任务。因此，我们测试了域适应技术在半导体领域半监督和无监督设置中的有效性，并提出了一种受 CycleGAN 启发的 DBACS 方法，该方法通过增加额外的损失项以提高性能。所有方法均基于实际的电子显微镜图像在无监督和半监督设置下进行了研究和验证，证明了我们的方法在推进半导体领域的域适应技术方面的重要性。 

---
# RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments 

**Title (ZH)**: RAS-Eval: 万物实境中LLM代理安全评估的综合基准 

**Authors**: Yuchuan Fu, Xiaohan Yuan, Dongxia Wang  

**Link**: [PDF](https://arxiv.org/pdf/2506.15253)  

**Abstract**: The rapid deployment of Large language model (LLM) agents in critical domains like healthcare and finance necessitates robust security frameworks. To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution. RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Notably, scaling laws held for security capabilities, with larger models outperforming smaller counterparts. Our findings expose critical risks in real-world agent deployments and provide a foundational framework for future security research. Code and data are available at this https URL. 

**Abstract (ZH)**: 大型语言模型（LLM）代理在健康 care 和金融领域等关键领域的快速部署需要 robust 安全框架。为了应对动态环境中缺乏标准化评估基准的问题，我们引入了 RAS-Eval，这是一个支持模拟和现实世界工具执行的全面安全基准，包含 80 项测试案例和 3,802 个攻击任务，覆盖 11 个共性弱点枚举（CWE）类别，并以 JSON、LangGraph 和 Model Context Protocol（MCP）格式实现。我们评估了 6 个最先进的 LLM，在多种场景下揭示了重大漏洞：攻击平均降低了 36.78% 的任务完成率（TCR），并在学术环境中实现了 85.65% 的成功率。值得注意的是，安全能力遵循规模法则，较大模型优于较小模型。我们的发现揭示了实际部署代理中的关键风险，并提供了未来安全研究的基础框架。代码和数据可在以下网址获取。 

---
# Singular Value Decomposition on Kronecker Adaptation for Large Language Model 

**Title (ZH)**: Kronecker适应化的奇异值分解在大型语言模型中的应用 

**Authors**: Yee Hin Chong, Peng Qu  

**Link**: [PDF](https://arxiv.org/pdf/2506.15251)  

**Abstract**: Large pre-trained Transformer models achieve state-of-the-art results across diverse language and reasoning tasks, but full fine-tuning incurs substantial storage, memory, and computational overhead. Parameter-efficient fine-tuning (PEFT) methods mitigate these costs by learning only a small subset of task-specific parameters, yet existing approaches either introduce inference-time latency (adapter modules), suffer from suboptimal convergence (randomly initialized low-rank updates), or rely on fixed rank choices that may not match task complexity (Kronecker-based decompositions).
We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that combines Kronecker-product tensor factorization with SVD-driven initialization and spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD) procedure extracts principal components of the full weight update into compact Kronecker factors, while an adaptive rank selection algorithm uses energy-threshold and elbow-point criteria to prune negligible components.
Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal mathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires only 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or exceeding baseline performance. Moreover, SoKA exhibits faster convergence and more stable gradients, highlighting its robustness and efficiency for large-scale model adaptation. 

**Abstract (ZH)**: SoKA (SVD on Kronecker Adaptation): A Parameter-Efficient Fine-Tuning Strategy for Large Pre-trained Transformers 

---
# A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals 

**Title (ZH)**: 大型语言模型用于识别可持续发展目标的任务适应技术比较研究 

**Authors**: Andrea Cadeddu, Alessandro Chessa, Vincenzo De Leo, Gianni Fenu, Enrico Motta, Francesco Osborne, Diego Reforgiato Recupero, Angelo Salatino, Luca Secchi  

**Link**: [PDF](https://arxiv.org/pdf/2506.15208)  

**Abstract**: In 2012, the United Nations introduced 17 Sustainable Development Goals (SDGs) aimed at creating a more sustainable and improved future by 2030. However, tracking progress toward these goals is difficult because of the extensive scale and complexity of the data involved. Text classification models have become vital tools in this area, automating the analysis of vast amounts of text from a variety of sources. Additionally, large language models (LLMs) have recently proven indispensable for many natural language processing tasks, including text classification, thanks to their ability to recognize complex linguistic patterns and semantics. This study analyzes various proprietary and open-source LLMs for a single-label, multi-class text classification task focused on the SDGs. Then, it also evaluates the effectiveness of task adaptation techniques (i.e., in-context learning approaches), namely Zero-Shot and Few-Shot Learning, as well as Fine-Tuning within this domain. The results reveal that smaller models, when optimized through prompt engineering, can perform on par with larger models like OpenAI's GPT (Generative Pre-trained Transformer). 

**Abstract (ZH)**: 2012年，联合国引入了17个可持续发展目标（SDGs），旨在到2030年创造一个更加可持续和改进的未来。然而，由于涉及数据的广阔规模和复杂性，追踪这些目标的进展情况颇具挑战性。文本分类模型已成为这一领域的关键工具，能够自动化分析来自各种来源的大量文本信息。此外，由于其识别复杂语言模式和语义的能力，大型语言模型（LLMs）近期在包括文本分类在内的许多自然语言处理任务中变得不可或缺。本文研究了多种专有和开源LLMs在专注于SDGs的单标签多分类文本分类任务中的性能。同时，评估了任务适配技术（即上下文学习方法，包括零样本学习和少样本学习，以及微调）在这一领域的有效性。结果显示，通过提示工程优化的小型模型可以与像OpenAI的GPT（生成式预训练变换器）这样的大型模型表现相当。 

---
# Accessible Gesture-Driven Augmented Reality Interaction System 

**Title (ZH)**: 可访问的手势驱动增强现实交互系统 

**Authors**: Yikan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2506.15189)  

**Abstract**: Augmented reality (AR) offers immersive interaction but remains inaccessible for users with motor impairments or limited dexterity due to reliance on precise input methods. This study proposes a gesture-based interaction system for AR environments, leveraging deep learning to recognize hand and body gestures from wearable sensors and cameras, adapting interfaces to user capabilities. The system employs vision transformers (ViTs), temporal convolutional networks (TCNs), and graph attention networks (GATs) for gesture processing, with federated learning ensuring privacy-preserving model training across diverse users. Reinforcement learning optimizes interface elements like menu layouts and interaction modes. Experiments demonstrate a 20% improvement in task completion efficiency and a 25% increase in user satisfaction for motor-impaired users compared to baseline AR systems. This approach enhances AR accessibility and scalability. Keywords: Deep learning, Federated learning, Gesture recognition, Augmented reality, Accessibility, Human-computer interaction 

**Abstract (ZH)**: 基于手势的增强现实交互系统：通过深度学习提升肢体障碍用户 accessibility 和可扩展性 

---
# Classification of Multi-Parametric Body MRI Series Using Deep Learning 

**Title (ZH)**: 多参数身体MRI系列的深度学习分类 

**Authors**: Boah Kim, Tejas Sudharshan Mathai, Kimberly Helm, Peter A. Pinto, Ronald M. Summers  

**Link**: [PDF](https://arxiv.org/pdf/2506.15182)  

**Abstract**: Multi-parametric magnetic resonance imaging (mpMRI) exams have various series types acquired with different imaging protocols. The DICOM headers of these series often have incorrect information due to the sheer diversity of protocols and occasional technologist errors. To address this, we present a deep learning-based classification model to classify 8 different body mpMRI series types so that radiologists read the exams efficiently. Using mpMRI data from various institutions, multiple deep learning-based classifiers of ResNet, EfficientNet, and DenseNet are trained to classify 8 different MRI series, and their performance is compared. Then, the best-performing classifier is identified, and its classification capability under the setting of different training data quantities is studied. Also, the model is evaluated on the out-of-training-distribution datasets. Moreover, the model is trained using mpMRI exams obtained from different scanners in two training strategies, and its performance is tested. Experimental results show that the DenseNet-121 model achieves the highest F1-score and accuracy of 0.966 and 0.972 over the other classification models with p-value$<$0.05. The model shows greater than 0.95 accuracy when trained with over 729 studies of the training data, whose performance improves as the training data quantities grew larger. On the external data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and 0.810 accuracy for each. These results indicate that in both the internal and external datasets, the DenseNet-121 model attains high accuracy for the task of classifying 8 body MRI series types. 

**Abstract (ZH)**: 基于深度学习的多参数磁共振成像序列分类模型 

---
# LLM Agent for Hyper-Parameter Optimization 

**Title (ZH)**: 基于LLM代理的超参数优化 

**Authors**: Wanzhe Wang, Jianqiu Peng, Menghao Hu, Weihuang Zhong, Tong Zhang, Shuai Wang, Yixin Zhang, Mingjie Shao, Wanli Ni  

**Link**: [PDF](https://arxiv.org/pdf/2506.15167)  

**Abstract**: Hyper-parameters are essential and critical for the performance of communication algorithms. However, current hyper-parameters tuning methods for warm-start particles swarm optimization with cross and mutation (WS-PSO-CM) algortihm for radio map-enabled unmanned aerial vehicle (UAV) trajectory and communication are primarily heuristic-based, exhibiting low levels of automation and unsatisfactory performance. In this paper, we design an large language model (LLM) agent for automatic hyper-parameters-tuning, where an iterative framework and model context protocol (MCP) are applied. In particular, the LLM agent is first setup via a profile, which specifies the mission, background, and output format. Then, the LLM agent is driven by the prompt requirement, and iteratively invokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent autonomously terminates the loop and returns a set of hyper-parameters. Our experiment results show that the minimal sum-rate achieved by hyper-parameters generated via our LLM agent is significantly higher than those by both human heuristics and random generation methods. This indicates that an LLM agent with PSO knowledge and WS-PSO-CM algorithm background is useful in finding high-performance hyper-parameters. 

**Abstract (ZH)**: 基于大型语言模型的自动超参数调优方法：应用于无线电图辅助无人驾驶飞行器轨迹和通信的WS-PSO-CM算法 

---
# SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning 

**Title (ZH)**: SonicVerse：基于音乐特征的多任务学习captioning 

**Authors**: Anuradha Chopra, Abhinaba Roy, Dorien Herremans  

**Link**: [PDF](https://arxiv.org/pdf/2506.15154)  

**Abstract**: Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions. 

**Abstract (ZH)**: 一种将音高检测、人声检测等辅助音乐特征检测任务与_caption生成_结合的多任务音乐 captioning 模型：SonicVerse及其应用 

---
# Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models 

**Title (ZH)**: Thunder-Tok: 最小化韩文分词中每个词的令牌数量以适应生成型语言模型 

**Authors**: Gyeongje Cho, Yeonkyoun So, Chanwoo Park, Sangmin Lee, Sungmok Jung, Jaejin Lee  

**Link**: [PDF](https://arxiv.org/pdf/2506.15138)  

**Abstract**: This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce token fertility without compromising model performance. Our approach uses a rule-based pre-tokenization method that aligns with the linguistic structure of the Korean language. We also create a seed vocabulary containing tokens that resemble linguistic units and employ a branching entropy-based selection algorithm. These techniques increase the average token length, thus lowering fertility while preserving linguistic information. Experimental results indicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces the number of tokens by 10%, improving the inference speed by 10%) compared to BPE without compromising performance across various downstream tasks. These findings demonstrate that our linguistically informed approach is effective and practical for designing efficient tokenizers for language models. 

**Abstract (ZH)**: Thunder-Tok：一种通过减少标记丰度同时不牺牲模型性能的新韩语分词器 

---
# Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs 

**Title (ZH)**: 基于大语言模型在开放领域对话中建模一对多属性 

**Authors**: Jing Yang Lee, Kong-Aik Lee, Woon-Seng Gan  

**Link**: [PDF](https://arxiv.org/pdf/2506.15131)  

**Abstract**: Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby multiple appropriate responses exist for a single dialogue context. Despite prior research showing that modeling this property boosts response diversity, most modern LLM-based dialogue agents do not explicitly do so. In this work, we model the o2m property of OD in LLMs by decomposing OD generation into two key tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS), which entail generating a set of n semantically and lexically diverse high-quality responses for a given dialogue context, followed by selecting a single response based on human preference, respectively. To facilitate MRG and PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the o2m property by featuring multiple plausible responses for each context. Leveraging o2mDial, we propose new in-context learning and instruction-tuning strategies, as well as novel evaluation metrics for MRG, alongside a model-based approach for PS. Empirical results demonstrate that applying the proposed two-stage framework to smaller LLMs for OD generation enhances overall response diversity while maintaining contextual coherence, improving response quality by up to 90%, bringing them closer to the performance of larger models. 

**Abstract (ZH)**: 开放域对话（OD）展现出一对多（o2m）的特性，即对于一个单一的对话背景，存在多个合适的回应。尽管先前的研究表明，建模这一特性可以提升回应的多样性，但大多数基于大语言模型的对话代理并未明确实现这一点。在本文中，我们通过分解OD生成为两个关键任务——多回应生成（MRG）和基于偏好的选择（PS）——来在大语言模型（LLM）中建模OD的一对多特性。具体而言，MRG涉及生成一组具有语义和词汇多样性且高质量的回应，而PS则基于人类偏好选择其中一个回应。为了促进MRG和PS，我们引入了o2mDial对话语料库，该语料库明确设计以捕获一对多特性，每个上下文包含多个合理的回应。利用o2mDial，我们提出了新的上下文学习和指令调优策略，以及MRG的新评价指标，并提出了一种基于模型的方法来实现PS。实验结果表明，将提出的两阶段框架应用于较小的LLM进行OD生成，能够提升整体的回应多样性同时保持上下文连贯性，回应质量最多可提高90%，使其更接近大模型的表现。 

---
# Advancing Loss Functions in Recommender Systems: A Comparative Study with a Rényi Divergence-Based Solution 

**Title (ZH)**: 基于Rényi距离的解决方案：推荐系统中损失函数进展的比较研究 

**Authors**: Shengjia Zhang, Jiawei Chen, Changdong Li, Sheng Zhou, Qihao Shi, Yan Feng, Chun Chen, Can Wang  

**Link**: [PDF](https://arxiv.org/pdf/2506.15120)  

**Abstract**: Loss functions play a pivotal role in optimizing recommendation models. Among various loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are particularly effective. Their theoretical connections and differences warrant in-depth exploration. This work conducts comprehensive analyses of these losses, yielding significant insights: 1) Common strengths -- both can be viewed as augmentations of traditional losses with Distributional Robust Optimization (DRO), enhancing robustness to distributional shifts; 2) Respective limitations -- stemming from their use of different distribution distance metrics in DRO optimization, SL exhibits high sensitivity to false negative instances, whereas CCL suffers from low data utilization. To address these limitations, this work proposes a new loss function, DrRL, which generalizes SL and CCL by leveraging Rényi-divergence in DRO optimization. DrRL incorporates the advantageous structures of both SL and CCL, and can be demonstrated to effectively mitigate their limitations. Extensive experiments have been conducted to validate the superiority of DrRL on both recommendation accuracy and robustness. 

**Abstract (ZH)**: Softmax Loss 和 Cosine Contrastive Loss 的理论联系与差异及其改进研究：基于 Rényi 散度的 DrRL 方法 

---
# Transit for All: Mapping Equitable Bike2Subway Connection using Region Representation Learning 

**Title (ZH)**: 全民通勤：基于区域表示学习的公平自行车至地铁接驳 Mapping 

**Authors**: Min Namgung, JangHyeon Lee, Fangyi Ding, Yao-Yi Chiang  

**Link**: [PDF](https://arxiv.org/pdf/2506.15113)  

**Abstract**: Ensuring equitable public transit access remains challenging, particularly in densely populated cities like New York City (NYC), where low-income and minority communities often face limited transit accessibility. Bike-sharing systems (BSS) can bridge these equity gaps by providing affordable first- and last-mile connections. However, strategically expanding BSS into underserved neighborhoods is difficult due to uncertain bike-sharing demand at newly planned ("cold-start") station locations and limitations in traditional accessibility metrics that may overlook realistic bike usage potential. We introduce Transit for All (TFA), a spatial computing framework designed to guide the equitable expansion of BSS through three components: (1) spatially-informed bike-sharing demand prediction at cold-start stations using region representation learning that integrates multimodal geospatial data, (2) comprehensive transit accessibility assessment leveraging our novel weighted Public Transport Accessibility Level (wPTAL) by combining predicted bike-sharing demand with conventional transit accessibility metrics, and (3) strategic recommendations for new bike station placements that consider potential ridership and equity enhancement. Using NYC as a case study, we identify transit accessibility gaps that disproportionately impact low-income and minority communities in historically underserved neighborhoods. Our results show that strategically placing new stations guided by wPTAL notably reduces disparities in transit access related to economic and demographic factors. From our study, we demonstrate that TFA provides practical guidance for urban planners to promote equitable transit and enhance the quality of life in underserved urban communities. 

**Abstract (ZH)**: 确保公共 Transit 访问的公平性在 densely populated 城市如纽约市仍然具有挑战性，特别是低收入和少数族裔社区常常面临 Transit 访问受限的情况。共享单车系统（BSS）可以通过提供经济实惠的首末公里接驳来弥补这些公平性差距。然而，由于新规划站点（“冷启动”站点）位置的骑行需求不确定性和传统可达性评估指标的局限性（可能忽视实际的骑行使用潜力），在欠服务社区战略性扩张 BSS 比较困难。为此，我们引入了 Transit for All（TFA）空间计算框架，旨在通过三个组成部分指导 BSS 的公平扩展：（1）使用区域表示学习融合多模式地理空间数据的空间导向型共享单车需求预测；（2）综合评估 Transit 访问性，通过将预测的共享单车需求与传统 Transit 访可达性指标结合来利用我们新颖的加权公共交通可达性水平（wPTAL）；（3）在考虑潜在骑行量和公平性提升的前提下，提出新的自行车站布局战略建议。以纽约市为例，我们识别出历史上欠服务社区中低收入和少数族裔社区在 Transit 访问性方面的不成比例差距。我们的结果显示，根据 wPTAL 指导战略性设置新站点显著减少了与经济和社会因素相关的 Transit 访问性不平等。从我们的研究来看，TFA 为城市规划者提供了实用指南，以促进公平的 Transit 并提升欠服务城市社区的生活质量。 

---
# Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification 

**Title (ZH)**: 通过话语意识的语句澄清改进对话话语解析 

**Authors**: Yaxin Fan, Peifeng Li, Qiaoming Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2506.15081)  

**Abstract**: Dialogue discourse parsing aims to identify and analyze discourse relations between the utterances within dialogues. However, linguistic features in dialogues, such as omission and idiom, frequently introduce ambiguities that obscure the intended discourse relations, posing significant challenges for parsers. To address this issue, we propose a Discourse-aware Clarification Module (DCM) to enhance the performance of the dialogue discourse parser. DCM employs two distinct reasoning processes: clarification type reasoning and discourse goal reasoning. The former analyzes linguistic features, while the latter distinguishes the intended relation from the ambiguous one. Furthermore, we introduce Contribution-aware Preference Optimization (CPO) to mitigate the risk of erroneous clarifications, thereby reducing cascading errors. CPO enables the parser to assess the contributions of the clarifications from DCM and provide feedback to optimize the DCM, enhancing its adaptability and alignment with the parser's requirements. Extensive experiments on the STAC and Molweni datasets demonstrate that our approach effectively resolves ambiguities and significantly outperforms the state-of-the-art (SOTA) baselines. 

**Abstract (ZH)**: 对话话语解析旨在识别和分析对话中话语单元之间的关系。但由于对话中的语言特征，如省略和成语，经常引入歧义，模糊了本意的话语关系，给解析器带来了重大挑战。为解决这一问题，我们提出了一种话语感知澄清模块（DCM）以提高话语解析器的性能。DCM运用了两种不同的推理过程：澄清类型推理和话语目标推理。前者分析语言特征，后者区分意图关系与歧义关系。此外，我们引入了贡献感知偏好优化（CPO）来减少错误澄清的风险，从而减少级联错误。CPO使解析器能够评估DCM澄清的贡献，并提供反馈以优化DCM，增强其适应性和与解析器需求的一致性。在STAC和Molweni数据集上的广泛实验表明，我们的方法有效解决了歧义问题，并显著优于当前最先进的（SOTA）基线。 

---
# Sequential Policy Gradient for Adaptive Hyperparameter Optimization 

**Title (ZH)**: 随序列策略梯度适应性超参数优化 

**Authors**: Zheng Li, Jerry Cheng, Huanying Helen Gu  

**Link**: [PDF](https://arxiv.org/pdf/2506.15051)  

**Abstract**: Reinforcement learning is essential for neural architecture search and hyperparameter optimization, but the conventional approaches impede widespread use due to prohibitive time and computational costs. Inspired by DeepSeek-V3 multi-token prediction architecture, we propose Sequential Policy Gradient modeling (SPG), a novel trajectory generation paradigm for lightweight online hyperparameter optimization. In contrast to conventional policy gradient methods, SPG extends the base model with temporary modules, enabling it to generate state-action (padded) trajectories in a single forward pass. Our experiments demonstrate that models gain performance when retrained with SPG on their original datasets and also outperform standard transfer fine-tuning. We evaluate on five datasets spanning computer vision (ImageNet, COCO), natural language processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial applicability of SPG. The proposed method demonstrates consistent improvements across widely adopted models, achieving performance gains of $+0.2\sim7\%$, with significantly low computational costs. Fully reproducible code and pre-trained models: this https URL. 

**Abstract (ZH)**: 强化学习对于神经架构搜索和超参数优化至关重要，但传统方法因其高昂的时间和计算成本而阻碍了其广泛应用。受DeepSeek-V3多令牌预测架构启发，我们提出了一种轻量级在线超参数优化的序贯策略梯度建模（SPG），这是一种新颖的轨迹生成范式。与传统的策略梯度方法不同，SPG通过在基础模型中引入临时模块，使其能够在单次前向传播中生成状态动作（填充）轨迹。我们的实验表明，使用SPG重新训练模型后可在原始数据集上获得性能提升，并且优于标准的迁移微调方法。我们在计算机视觉（ImageNet、COCO）、自然语言处理（GLUE、SQuAD）和音频（SUPERB）的五个数据集上进行了评估，以评估SPG的工业应用潜力。该方法在广泛采用的模型中显示出一致的性能提升，性能提升范围为2%至7%，同时计算成本显著降低。可完全复现的代码和预训练模型：this https URL。 

---
# Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers 

**Title (ZH)**: 将照护需求映射至AI聊天机器人设计：阿尔茨海默病和痴呆症照护者心理健康支持的优势与缺口 

**Authors**: Jiayue Melissa Shi, Dong Whi Yoo, Keran Wang, Violeta J. Rodriguez, Ravi Karkar, Koustuv Saha  

**Link**: [PDF](https://arxiv.org/pdf/2506.15047)  

**Abstract**: Family caregivers of individuals with Alzheimer's Disease and Related Dementia (AD/ADRD) face significant emotional and logistical challenges that place them at heightened risk for stress, anxiety, and depression. Although recent advances in generative AI -- particularly large language models (LLMs) -- offer new opportunities to support mental health, little is known about how caregivers perceive and engage with such technologies. To address this gap, we developed Carey, a GPT-4o-based chatbot designed to provide informational and emotional support to AD/ADRD caregivers. Using Carey as a technology probe, we conducted semi-structured interviews with 16 family caregivers following scenario-driven interactions grounded in common caregiving stressors. Through inductive coding and reflexive thematic analysis, we surface a systemic understanding of caregiver needs and expectations across six themes -- on-demand information access, emotional support, safe space for disclosure, crisis management, personalization, and data privacy. For each of these themes, we also identified the nuanced tensions in the caregivers' desires and concerns. We present a mapping of caregiver needs, AI chatbot's strengths, gaps, and design recommendations. Our findings offer theoretical and practical insights to inform the design of proactive, trustworthy, and caregiver-centered AI systems that better support the evolving mental health needs of AD/ADRD caregivers. 

**Abstract (ZH)**: 家庭照护者在阿尔茨海默病及相关痴呆（AD/ADRD）患者中的情感与 logistical 挑战及其对压力、焦虑和抑郁的风险，虽然生成式 AI（特别是大型语言模型 LLMs）的最新进展提供了新的支持心理健康的机会，但尚不清楚照护者如何感知和使用这些技术。为此，我们开发了基于 GPT-4o 的聊天机器人 Carey，旨在为 AD/ADRD 照护者提供信息和支持。通过情景驱动的交互和常见的照护压力情境进行半结构化访谈，我们与 16 名家庭照护者进行了交流，通过归纳编码和反思性主题分析，我们揭示了照护者需求和期望的系统理解，包括按需信息访问、情感支持、安全披露空间、危机管理、个性化和数据隐私六个主题。我们还识别了照护者在愿望和担忧中的细微矛盾。我们提出了照护者需求、AI 聊天机器人的优势、差距和设计建议的映射。我们的研究提供了理论和实践上的见解，以指导设计更积极主动、可信赖且以照护者为中心的 AI 系统，更好地支持 AD/ADRD 照护者的不断变化的心理健康需求。 

---
# Advanced Prediction of Hypersonic Missile Trajectories with CNN-LSTM-GRU Architectures 

**Title (ZH)**: 基于CNN-LSTM-GRU架构的高超声速导弹 trajectories 高级预测 

**Authors**: Amir Hossein Baradaran  

**Link**: [PDF](https://arxiv.org/pdf/2506.15043)  

**Abstract**: Advancements in the defense industry are paramount for ensuring the safety and security of nations, providing robust protection against emerging threats. Among these threats, hypersonic missiles pose a significant challenge due to their extreme speeds and maneuverability, making accurate trajectory prediction a critical necessity for effective countermeasures. This paper addresses this challenge by employing a novel hybrid deep learning approach, integrating Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs). By leveraging the strengths of these architectures, the proposed method successfully predicts the complex trajectories of hypersonic missiles with high accuracy, offering a significant contribution to defense strategies and missile interception technologies. This research demonstrates the potential of advanced machine learning techniques in enhancing the predictive capabilities of defense systems. 

**Abstract (ZH)**: 国防工业的进步对于确保国家的安全与安全至关重要，提供对新兴威胁的 robust 保护。其中，由于其极高速度和机动性，高超声速导弹构成了重大挑战，因此准确的轨迹预测是有效反制措施的必要条件。本文通过采用一种新颖的混合深度学习方法来应对这一挑战，该方法结合了卷积神经网络（CNNs）、长短期记忆（LSTM）网络和门控循环单元（GRUs）。通过利用这些架构的优势，所提出的方法成功地以高精度预测了高超声速导弹的复杂轨迹，为防御策略和导弹拦截技术做出了重要贡献。这项研究展示了先进机器学习技术在增强防御系统预测能力方面的潜力。 

---
# Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size 

**Title (ZH)**: LLMs中最优嵌入学习率：词汇量大小的影响 

**Authors**: Soufiane Hayou, Liyuan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2506.15025)  

**Abstract**: Pretraining large language models is a costly process. To make this process more efficient, several methods have been proposed to optimize model architecture/parametrization and hardware use. On the parametrization side, $\mu P$ (Maximal Update Parametrization) parametrizes model weights and learning rate (LR) in a way that makes hyperparameters (HPs) transferable with width (embedding dimension): HPs can be tuned for a small model and used for larger models without additional tuning. While $\mu$P showed impressive results in practice, recent empirical studies have reported conflicting observations when applied to LLMs. One limitation of the theory behind $\mu$P is the fact that input dimension (vocabulary size in LLMs) is considered fixed when taking the width to infinity. This is unrealistic since vocabulary size is generally much larger than width in practice. In this work, we provide a theoretical analysis of the effect of vocabulary size on training dynamics, and subsequently show that as vocabulary size increases, the training dynamics \emph{interpolate between the $\mu$P regime and another regime that we call Large Vocab (LV) Regime}, where optimal scaling rules are different from those predicted by $\mu$P. Our analysis reveals that in the LV regime, the optimal embedding LR to hidden LR ratio should roughly scale as $\Theta(\sqrt{width})$, surprisingly close to the empirical findings previously reported in the literature, and different from the $\Theta(width)$ ratio predicted by $\mu$P. We conduct several experiments to validate our theory, and pretrain a 1B model from scratch to show the benefit of our suggested scaling rule for the embedding LR. 

**Abstract (ZH)**: 预训练大规模语言模型是一个成本高昂的过程。为了使这一过程更加高效，提出了几种方法来优化模型架构/参数化和硬件使用。从参数化角度来看，$\mu P$（最大更新参数化）通过一种方式参数化模型权重和学习率（LR），使得超参数（HPs）在宽度（嵌入维度）上具有可转移性：HPs可以在小型模型上调整，并在大型模型上使用而无需额外调整。虽然$\mu$P在实践中显示出令人印象深刻的成果，但最近的应用到语言模型（LLMs）的经验研究表明存在矛盾的观察结果。$\mu$P理论的一个局限性在于，在将宽度趋向无穷大时，输入维度（语言模型中的词汇量）被认为是固定的。然而在实践中，词汇量通常远大于宽度。在本文中，我们对词汇量对训练动力学的影响进行了理论分析，并表明随着词汇量的增加，训练动力学在$\mu$P区间和我们称为大型词汇（LV）区间之间进行插值，而在LV区间内最佳缩放规则与$\mu$P预测的截然不同。我们的分析表明，在LV区间内，理想的嵌入LR与隐藏LR的比例应大致与$\sqrt{宽度}$成正比，这一结果与文献中先前报告的经验发现非常接近，且不同于$\mu$P预测的$\Theta(宽度)$比例。我们进行了多项实验验证我们的理论，并从头预训练了一个1B模型以展示我们建议的嵌入LR缩放规则的优势。 

---
# SFT-GO: Supervised Fine-Tuning with Group Optimization for Large Language Models 

**Title (ZH)**: SFT-GO：监督微调与分组优化用于大型语言模型 

**Authors**: Gyuhak Kim, Sumiran Singh Thakur, Su Min Park, Wei Wei, Yujia Bao  

**Link**: [PDF](https://arxiv.org/pdf/2506.15021)  

**Abstract**: Supervised fine-tuning (SFT) has become an essential step in tailoring large language models (LLMs) to align with human expectations and specific downstream tasks. However, existing SFT methods typically treat each training instance as a uniform sequence, giving equal importance to all tokens regardless of their relevance. This overlooks the fact that only a subset of tokens often contains critical, task-specific information. To address this limitation, we introduce Supervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that treats groups of tokens differently based on their this http URL-GO groups tokens in each sample based on their importance values and optimizes the LLM using a weighted combination of the worst-group loss and the standard cross-entropy loss. This mechanism adaptively emphasizes the most challenging token groups and guides the model to better handle different group distributions, thereby improving overall learning dynamics. We provide a theoretical analysis of SFT-GO's convergence rate, demonstrating its efficiency. Empirically, we apply SFT-GO with three different token grouping strategies and show that models trained with SFT-GO consistently outperform baseline approaches across popular LLM benchmarks. These improvements hold across various datasets and base models, demonstrating the robustness and the effectiveness of our method. 

**Abstract (ZH)**: 监督微调结合组优化（SFT-GO）：基于组优先级的细调方法 

---
# Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment 

**Title (ZH)**: 急性菌血症治疗中基于 Offline Reinforcement Learning 的稳定 CDE 自编码器及其锐利度正则化方法 

**Authors**: Yue Gao  

**Link**: [PDF](https://arxiv.org/pdf/2506.15019)  

**Abstract**: Effective reinforcement learning (RL) for sepsis treatment depends on learning stable, clinically meaningful state representations from irregular ICU time series. While previous works have explored representation learning for this task, the critical challenge of training instability in sequential representations and its detrimental impact on policy performance has been overlooked. This work demonstrates that Controlled Differential Equations (CDE) state representation can achieve strong RL policies when two key factors are met: (1) ensuring training stability through early stopping or stabilization methods, and (2) enforcing acuity-aware representations by correlation regularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the MIMIC-III sepsis cohort reveal that stable CDE autoencoder produces representations strongly correlated with acuity scores and enables RL policies with superior performance (WIS return $> 0.9$). In contrast, unstable CDE representation leads to degraded representations and policy failure (WIS return $\sim$ 0). Visualizations of the latent space show that stable CDEs not only separate survivor and non-survivor trajectories but also reveal clear acuity score gradients, whereas unstable training fails to capture either pattern. These findings highlight practical guidelines for using CDEs to encode irregular medical time series in clinical RL, emphasizing the need for training stability in sequential representation learning. 

**Abstract (ZH)**: 有效治疗严重败血症的强化学习（RL）依赖于从不规则ICU时间序列中学习稳定且临床意义显著的状态表示。本研究证明，在满足两个关键条件时，受控微分方程（CDE）状态表示可以实现强RL策略：（1）通过早期停止或稳定方法确保训练稳定性；（2）通过与临床评分（SOFA、SAPS-II、OASIS）的相关正则化强制实施急性意识状态感知表示。MIMIC-III败血症队列的实验表明，稳定的CDE自编码器生成与急性意识状态评分高度相关的表示，并且能够实现高性能策略（WIS回报>0.9）。相比之下，不稳定的CDE表示导致表示质量下降并导致策略失败（WIS回报≈0）。潜在空间的可视化表明，稳定的CDE不仅可以区分幸存者和非幸存者轨迹，还能揭示清晰的急性意识状态评分梯度，而不稳定的训练则无法捕捉这两种模式。这些发现突出了在临床RL中使用CDE编码不规则医疗时间序列的实际指导原则，强调了顺序表示学习中训练稳定性的必要性。 

---
# Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output 

**Title (ZH)**: 基于现实数据的生成式AI设计洞察：文本转图像输出的实现 

**Authors**: Richa Gupta, Alexander Htet Kyaw  

**Link**: [PDF](https://arxiv.org/pdf/2506.15008)  

**Abstract**: Generative AI, specifically text-to-image models, have revolutionized interior architectural design by enabling the rapid translation of conceptual ideas into visual representations from simple text prompts. While generative AI can produce visually appealing images they often lack actionable data for designers In this work, we propose a novel pipeline that integrates DALL-E 3 with a materials dataset to enrich AI-generated designs with sustainability metrics and material usage insights. After the model generates an interior design image, a post-processing module identifies the top ten materials present and pairs them with carbon dioxide equivalent (CO2e) values from a general materials dictionary. This approach allows designers to immediately evaluate environmental impacts and refine prompts accordingly. We evaluate the system through three user tests: (1) no mention of sustainability to the user prior to the prompting process with generative AI, (2) sustainability goals communicated to the user before prompting, and (3) sustainability goals communicated along with quantitative CO2e data included in the generative AI outputs. Our qualitative and quantitative analyses reveal that the introduction of sustainability metrics in the third test leads to more informed design decisions, however, it can also trigger decision fatigue and lower overall satisfaction. Nevertheless, the majority of participants reported incorporating sustainability principles into their workflows in the third test, underscoring the potential of integrated metrics to guide more ecologically responsible practices. Our findings showcase the importance of balancing design freedom with practical constraints, offering a clear path toward holistic, data-driven solutions in AI-assisted architectural design. 

**Abstract (ZH)**: 生成式AI，特别是文本到图像模型，通过将简单的文本提示快速转换为视觉表示，彻底改变了室内建筑设计。尽管生成式AI能够生成视觉吸引力强的图像，但它们往往缺乏可操作的数据供设计师使用。在这项工作中，我们提出了一种新的流水线方法，将DALL-E 3与材料数据集集成，以在AI生成的设计中增加可持续性指标和材料使用洞察。在模型生成室内设计图像后，后处理模块识别出前十个材料，并与通用材料字典中的二氧化碳当量（CO2e）值配对。这种方法允许设计师立即评估环境影响并据此调整提示。我们通过三个用户测试评估了该系统：（1）在使用生成式AI之前不提及可持续性；（2）在使用生成式AI之前向用户传达可持续性目标；（3）在使用生成式AI生成输出时同时传达可持续性目标和定量的CO2e数据。我们的定性和定量分析表明，在第三次测试中引入可持续性指标会导致更具信息性的设计决策，但也可能引发决策疲劳并降低总体满意度。然而，大多数参与者在第三次测试中报告将可持续性原则纳入其工作流程，这突显了综合指标在引导更生态负责的做法方面的潜在价值。我们的研究结果展示了在保持设计自由的同时平衡实际限制的重要性，并为AI辅助建筑设计的全面、数据驱动解决方案提供了明确路径。 

---
# Scaling Intelligence: Designing Data Centers for Next-Gen Language Models 

**Title (ZH)**: 扩展现识：为下一代语言模型设计数据中心 

**Authors**: Jesmin Jahan Tithi, Hanjiang Wu, Avishaii Abuhatzera, Fabrizio Petrini  

**Link**: [PDF](https://arxiv.org/pdf/2506.15006)  

**Abstract**: The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8 trillion parameters - demands a radical rethinking of data center architecture to ensure scalability, efficiency, and cost-effectiveness. Our work provides a comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat optical), the size of the scale-out domain, and popular parallelism/optimization strategies used in LLMs. We introduce and evaluate FullFlat network architectures, which provide uniform high-bandwidth, low-latency connectivity between all nodes, and demonstrate their transformative impact on performance and scalability. Through detailed sensitivity analyses, we quantify the benefits of overlapping compute and communication, leveraging hardware-accelerated collectives, wider scale-out domains, and larger memory capacity. Our study spans both sparse (mixture of experts) and dense transformer-based LLMs, revealing how system design choices affect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens per sec / Peak flops of the hardware) and overall throughput. For the co-design study, we extended and validated a performance modeling tool capable of predicting LLM runtime within 10% of real-world measurements. Our findings offer actionable insights and a practical roadmap for designing AI data centers that can efficiently support trillion-parameter models, reduce optimization complexity, and sustain the rapid evolution of AI capabilities. 

**Abstract (ZH)**: 大规模语言模型（LLMs）的爆炸性增长——例如GPT-4拥有1.8万亿个参数——要求对数据中心架构进行根本性的重新思考，以确保可扩展性、效率和成本效益。我们的工作提供了一个综合的协同设计框架，联合探索FLOPS、HBM带宽和容量、多种网络拓扑（两层架构 vs. 全平坦光网络）、规模扩展域的大小以及在LLMs中广泛使用的并行/优化策略。我们介绍了并评估了全平坦网络架构，这些架构在所有节点之间提供了均匀的高带宽、低延迟连接，并证明了其对性能和可扩展性的推动作用。通过详细的敏感性分析，我们量化了重叠计算和通信、利用硬件加速的集合操作、更大的规模扩展域和更大的内存容量带来的好处。我们的研究涵盖了稀疏（专家混合）和密集的变压器基底LLMs，揭示了系统设计选择如何影响模型FLOPS利用率（MFU = 模型每令牌FLOPS × 观测每秒令牌数 / 硬件峰值FLOPS）和总体吞吐量。对于协同设计研究，我们扩展并验证了一个性能建模工具，可在10%以内预测LLM的运行时。我们的研究结果提供了可操作的见解和实用的道路图，以设计能够高效支持万亿参数模型、降低优化复杂性和维持人工智能能力快速演进的数据中心。 

---
# Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings 

**Title (ZH)**: 记忆令牌：大型语言模型可以生成可逆的句子嵌入 

**Authors**: Ignacio Sastre, Aiala Rosá  

**Link**: [PDF](https://arxiv.org/pdf/2506.15001)  

**Abstract**: In this work, we observe an interesting phenomenon: it is possible to generate reversible sentence embeddings that allow an LLM to reconstruct the original text exactly, without modifying the model's weights. This is achieved by introducing a special memory token, whose embedding is optimized through training on a fixed sequence. When prompted with this embedding, the model reconstructs the fixed sequence exactly. We evaluate this phenomenon across English and Spanish datasets, sequences of up to approximately 240 tokens, and model scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B successfully reconstructs all tested sequences. Our findings highlight an interesting capability of LLMs and suggest potential applications in memory-based retrieval, compression, and controlled text generation. 

**Abstract (ZH)**: 在此工作中，我们观察到一种有趣的现象：可以通过引入一种特殊的记忆令牌生成可逆的句子嵌入，从而使LLM在不修改模型权重的情况下精确重建原始文本。这一效果是通过在固定序列上训练来优化该特殊记忆令牌的嵌入实现的。当模型接收到此嵌入时，它可以精确地重建固定序列。我们在英语和西班牙语数据集中评估了这一现象，涉及多达约240个令牌的序列，并且涉及从100M到8B参数的不同模型规模。值得注意的是，Llama 3.1 8B成功重建了所有测试序列。我们的发现凸显了LLM的一种有趣能力，并暗示了其在基于记忆的检索、压缩和可控文本生成等方面的潜在应用。 

---
# Improved Image Reconstruction and Diffusion Parameter Estimation Using a Temporal Convolutional Network Model of Gradient Trajectory Errors 

**Title (ZH)**: 基于梯度轨迹误差的时序卷积网络模型改进图像重建和扩散参数估计 

**Authors**: Jonathan B. Martin, Hannah E. Alderson, John C. Gore, Mark D. Does, Kevin D. Harkins  

**Link**: [PDF](https://arxiv.org/pdf/2506.14995)  

**Abstract**: Summary: Errors in gradient trajectories introduce significant artifacts and distortions in magnetic resonance images, particularly in non-Cartesian imaging sequences, where imperfect gradient waveforms can greatly reduce image quality. Purpose: Our objective is to develop a general, nonlinear gradient system model that can accurately predict gradient distortions using convolutional networks. Methods: A set of training gradient waveforms were measured on a small animal imaging system, and used to train a temporal convolutional network to predict the gradient waveforms produced by the imaging system. Results: The trained network was able to accurately predict nonlinear distortions produced by the gradient system. Network prediction of gradient waveforms was incorporated into the image reconstruction pipeline and provided improvements in image quality and diffusion parameter mapping compared to both the nominal gradient waveform and the gradient impulse response function. Conclusion: Temporal convolutional networks can more accurately model gradient system behavior than existing linear methods and may be used to retrospectively correct gradient errors. 

**Abstract (ZH)**: 摘要：梯度轨迹中的误差会引入磁共振图像中的显著伪影和失真，特别是在非笛卡尔成像序列中，不完美的梯度波形可以显著降低图像质量。目的：我们的目标是开发一个通用的非线性梯度系统模型，使用卷积网络准确预测梯度失真。方法：在小型动物成像系统中测量了一组训练梯度波形，并使用这些波形训练时间卷积网络以预测成像系统产生的梯度波形。结果：训练后的网络能够准确预测梯度系统产生的非线性失真。将网络预测的梯度波形整合到图像重建管道中，与名义梯度波形和梯度冲击响应函数相比，提供了图像质量和扩散参数映射的改进。结论：时间卷积网络可以比现有线性方法更准确地建模梯度系统行为，并可能用于回顾性纠正梯度误差。 

---
# Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits 

**Title (ZH)**: 公平多代理多臂-bandits算法探查方法 

**Authors**: Tianyi Xu, Jiaxin Liu, Zizhan Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2506.14988)  

**Abstract**: We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at ensuring fair outcomes across agents while maximizing overall system performance. A key challenge in this setting is decision-making under limited information about arm rewards. To address this, we introduce a novel probing framework that strategically gathers information about selected arms before allocation. In the offline setting, where reward distributions are known, we leverage submodular properties to design a greedy probing algorithm with a provable performance bound. For the more complex online setting, we develop an algorithm that achieves sublinear regret while maintaining fairness. Extensive experiments on synthetic and real-world datasets show that our approach outperforms baseline methods, achieving better fairness and efficiency. 

**Abstract (ZH)**: 我们提出一种多代理多臂 bandit (MA-MAB) 框架，旨在确保代理间的公平性同时最大化整体系统性能。在仅有限了解臂奖励信息的情况下作出决策是一个关键挑战。为此，我们引入了一个新颖的信息探查框架，该框架有选择地收集信息后再进行分配。在奖励分布已知的离线场景中，我们利用亚模性质设计了一个具有可证明性能边界的贪婪探查算法。在更加复杂的在线场景中，我们开发了一种算法，该算法在保持公平性的同时实现了次线性遗憾。我们在合成数据集和现实世界数据集上的 extensive 实验表明，我们的方法优于基准方法，实现了更好的公平性和效率。 

---
# Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition 

**Title (ZH)**: 直接性思考：面向多说话人方向性语音识别的言语大规模语言模型 

**Authors**: Jiamin Xie, Ju Lin, Yiteng Huang, Tyler Vuong, Zhaojiang Lin, Zhaojun Yang, Peng Su, Prashant Rawat, Sangeeta Srivastava, Ming Sun, Florian Metze  

**Link**: [PDF](https://arxiv.org/pdf/2506.14973)  

**Abstract**: Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech recognition capabilities. However, the ability of Speech LLMs to comprehend and process multi-channel audio with spatial cues remains a relatively uninvestigated area of research. In this work, we present directional-SpeechLlama, a novel approach that leverages the microphone array of smart glasses to achieve directional speech recognition, source localization, and bystander cross-talk suppression. To enhance the model's ability to understand directivity, we propose two key techniques: serialized directional output training (S-DOT) and contrastive direction data augmentation (CDDA). Experimental results show that our proposed directional-SpeechLlama effectively captures the relationship between textual cues and spatial audio, yielding strong performance in both speech recognition and source localization tasks. 

**Abstract (ZH)**: 近期的研究表明，用音频编码提示大型语言模型（LLM）能够有效实现语音识别能力。然而，语音LLM在处理具有空间线索的多通道音频时理解和处理能力仍是一个相对未被充分研究的领域。在本文中，我们提出了方向性-SpeechLlama，这是一种利用智能眼镜麦克风阵列实现方向性语音识别、声源定位和旁白交叉谈话抑制的新方法。为了增强模型对方向性的理解能力，我们提出了两种关键技术：序列化方向输出训练（S-DOT）和对比方向数据增强（CDDA）。实验结果表明，我们提出的方向性-SpeechLlama能够有效捕捉文本提示和空间音频之间的关系，在语音识别和声源定位任务中表现出出色的性能。 

---
# FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization 

**Title (ZH)**: FEAST: 一种灵活的用餐辅助系统，实现真实的个性化体验 

**Authors**: Rajat Kumar Jenamani, Tom Silver, Ben Dodson, Shiqin Tong, Anthony Song, Yuting Yang, Ziang Liu, Benjamin Howe, Aimee Whitneck, Tapomayukh Bhattacharjee  

**Link**: [PDF](https://arxiv.org/pdf/2506.14968)  

**Abstract**: Physical caregiving robots hold promise for improving the quality of life of millions worldwide who require assistance with feeding. However, in-home meal assistance remains challenging due to the diversity of activities (e.g., eating, drinking, mouth wiping), contexts (e.g., socializing, watching TV), food items, and user preferences that arise during deployment. In this work, we propose FEAST, a flexible mealtime-assistance system that can be personalized in-the-wild to meet the unique needs of individual care recipients. Developed in collaboration with two community researchers and informed by a formative study with a diverse group of care recipients, our system is guided by three key tenets for in-the-wild personalization: adaptability, transparency, and safety. FEAST embodies these principles through: (i) modular hardware that enables switching between assisted feeding, drinking, and mouth-wiping, (ii) diverse interaction methods, including a web interface, head gestures, and physical buttons, to accommodate diverse functional abilities and preferences, and (iii) parameterized behavior trees that can be safely and transparently adapted using a large language model. We evaluate our system based on the personalization requirements identified in our formative study, demonstrating that FEAST offers a wide range of transparent and safe adaptations and outperforms a state-of-the-art baseline limited to fixed customizations. To demonstrate real-world applicability, we conduct an in-home user study with two care recipients (who are community researchers), feeding them three meals each across three diverse scenarios. We further assess FEAST's ecological validity by evaluating with an Occupational Therapist previously unfamiliar with the system. In all cases, users successfully personalize FEAST to meet their individual needs and preferences. Website: this https URL 

**Abstract (ZH)**: 物理照护机器人有望提高数百万需要进食协助的全球人群的生活质量。然而，在家庭环境中提供餐食协助仍然具有挑战性，因为这涉及到多样化的活动（如进食、饮水、面部清洁）、情境（如社交、看电视）、食物种类和用户偏好。本文提出FEAST，一种灵活的餐食协助系统，可以在实际环境中个性化配置以满足不同照护对象的特殊需求。该系统是在与两位社区研究人员合作，并借鉴一次多元化照护对象参与的形塑研究的基础上开发的，遵循三项关键原则：适应性、透明度和安全性。FEAST通过以下方式体现这些原则：（i）模块化硬件，允许切换到进食、饮水和面部清洁辅助；（ii）多样化的交互方式，包括网页界面、头部手势和物理按钮，以适应不同的功能能力和偏好；（iii）参数化的行为树，可以通过大型语言模型安全透明地进行调整。我们根据形塑研究中确定的个性化需求评估了该系统，结果显示FEAST提供了广泛的安全和透明的适应选项，并优于有限的固定定制基准。为了展示其实际应用性，我们在两名照护对象（他们也是社区研究人员）家中进行了用户研究，让他们各自在三种不同情境下食用三餐。我们进一步通过一位之前未接触过该系统的职业治疗师评估了FEAST的生态效度。在所有情况下，参与者成功地个性化了FEAST以满足他们的个人需求和偏好。网址：这个游戏链接 

---
# Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective 

**Title (ZH)**: 从跨域视角 revisiting reinforcement learning for LLM reasoning 

**Authors**: Zhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao Zhuang, Nilabjo Dey, Yuheng Zha, Yi Gu, Kun Zhou, Yuqi Wang, Yuan Li, Richard Fan, Jianshu She, Chengqian Gao, Abulhair Saparov, Haonan Li, Taylor W. Killian, Mikhail Yurochkin, Zhengzhong Liu, Eric P. Xing, Zhiting Hu  

**Link**: [PDF](https://arxiv.org/pdf/2506.14965)  

**Abstract**: Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domains--Math, Code, Science, Logic, Simulation, and Tabular--each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: this https URL 

**Abstract (ZH)**: 强化学习（RL）已成为提高大型语言模型（LLM）推理能力的有前途的方法，然而大多数公开的努力主要集中在数学和代码上，限制了我们对其在一般推理中的更广泛适用性的理解。一个关键挑战在于缺乏可靠的、可扩展的RL奖励信号，以覆盖多种推理领域。我们介绍了Guru，这是一个精心挑选的RL推理语料库，包含9.2万个可验证的示例，这些示例覆盖六个推理领域——数学、代码、科学、逻辑、模拟和表格——每个领域均通过特定领域的奖励设计、去重和过滤，以确保可靠的训练效果。基于Guru，我们系统地重新审视了在LLM推理中已建立的RL发现，并观察到各领域之间存在显著差异。例如，虽然之前的研究表明RL主要激发预训练模型中的现有知识，但我们的结果显示更复杂的模式：在预训练过程中经常出现的领域（数学、代码、科学）容易从跨领域训练中获益，而预训练曝光有限的领域（逻辑、模拟和表格）需要领域内训练才能实现有意义的性能提升，这表明RL很可能促进真正技能的获取。最后，我们介绍了Guru-7B和Guru-32B两种模型，它们在使用公开数据进行RL训练的开放模型中实现了最先进的性能，与我们在涵盖六个推理领域的17项任务评估套件中的最优基线相比，性能分别提高了7.9%和6.7%。我们还展示了我们的模型能够有效地提高基础模型的Pass@k性能，特别是在预训练数据中不太可能出现的复杂任务上。我们发布了数据、模型、训练和评估代码以促进通用推理的广泛应用：this https URL。 

---
# Flat Channels to Infinity in Neural Loss Landscapes 

**Title (ZH)**: 无限延伸的平坦通道在神经网络损失景观中 

**Authors**: Flavio Martinelli, Alexander Van Meegen, Berfin Şimşek, Wulfram Gerstner, Johanni Brea  

**Link**: [PDF](https://arxiv.org/pdf/2506.14951)  

**Abstract**: The loss landscapes of neural networks contain minima and saddle points that may be connected in flat regions or appear in isolation. We identify and characterize a special structure in the loss landscape: channels along which the loss decreases extremely slowly, while the output weights of at least two neurons, $a_i$ and $a_j$, diverge to $\pm$infinity, and their input weight vectors, $\mathbf{w_i}$ and $\mathbf{w_j}$, become equal to each other. At convergence, the two neurons implement a gated linear unit: $a_i\sigma(\mathbf{w_i} \cdot \mathbf{x}) + a_j\sigma(\mathbf{w_j} \cdot \mathbf{x}) \rightarrow \sigma(\mathbf{w} \cdot \mathbf{x}) + (\mathbf{v} \cdot \mathbf{x}) \sigma'(\mathbf{w} \cdot \mathbf{x})$. Geometrically, these channels to infinity are asymptotically parallel to symmetry-induced lines of critical points. Gradient flow solvers, and related optimization methods like SGD or ADAM, reach the channels with high probability in diverse regression settings, but without careful inspection they look like flat local minima with finite parameter values. Our characterization provides a comprehensive picture of these quasi-flat regions in terms of gradient dynamics, geometry, and functional interpretation. The emergence of gated linear units at the end of the channels highlights a surprising aspect of the computational capabilities of fully connected layers. 

**Abstract (ZH)**: 神经网络的损失景观包含连接在平坦区域的极小值和鞍点，或孤立出现的极小值和鞍点。我们识别并characterized一种特殊的损失景观结构：在这些结构中，沿某些通道损失变化极慢，同时至少有两个神经元$a_i$和$a_j$的输出权重发散至无穷大正负方向，且它们输入权重向量$\mathbf{w_i}$和$\mathbf{w_j}$变得相等。在收敛时，这两个神经元实现了一个门控线性单元：$a_i\sigma(\mathbf{w_i} \cdot \mathbf{x}) + a_j\sigma(\mathbf{w_j} \cdot \mathbf{x}) \rightarrow \sigma(\mathbf{w} \cdot \mathbf{x}) + (\mathbf{v} \cdot \mathbf{x}) \sigma'(\mathbf{w} \cdot \mathbf{x})$。几何上，这些通往无穷的通道在经由对称性诱导的临界点线方面渐近平行。梯度流求解器以及相关优化方法如SGD或ADAM在多种回归设置中以高概率到达这些通道，但如果没有仔细检查，这些通道看起来像是具有有限参数值的平坦局部极小值。我们的characterization为这些准平坦区域提供了从梯度动力学、几何和功能解释的全面图景。通道末端出现门控线性单元的现象揭示了全连接层计算能力的一种令人惊讶的方面。 

---
# Determinação Automática de Limiar de Detecção de Ataques em Redes de Computadores Utilizando Autoencoders 

**Title (ZH)**: 利用自编码器自动确定计算机网络攻击检测阈值 

**Authors**: Luan Gonçalves Miranda, Pedro Ivo da Cruz, Murilo Bellezoni Loiola  

**Link**: [PDF](https://arxiv.org/pdf/2506.14937)  

**Abstract**: Currently, digital security mechanisms like Anomaly Detection Systems using Autoencoders (AE) show great potential for bypassing problems intrinsic to the data, such as data imbalance. Because AE use a non-trivial and nonstandardized separation threshold to classify the extracted reconstruction error, the definition of this threshold directly impacts the performance of the detection process. Thus, this work proposes the automatic definition of this threshold using some machine learning algorithms. For this, three algorithms were evaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine. 

**Abstract (ZH)**: 当前，使用自动编码器（AE）的异常检测系统在克服数据固有难题（如数据不平衡）方面展现出巨大潜力。由于AE使用一个非平凡且非标准化的分离阈值来分类重构误差，因此该阈值的定义直接影响检测过程的性能。为此，本研究提出了一种使用某些机器学习算法自动定义该阈值的方法。为此，评估了三种算法：K-最近邻、K-均值和支撑向量机。 

---
# Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based Crypto Anomaly Detection 

**Title (ZH)**: 先解释，后信任：基于图的加密异常检测的LLM增强解释 

**Authors**: Adriana Watson  

**Link**: [PDF](https://arxiv.org/pdf/2506.14933)  

**Abstract**: The decentralized finance (DeFi) community has grown rapidly in recent years, pushed forward by cryptocurrency enthusiasts interested in the vast untapped potential of new markets. The surge in popularity of cryptocurrency has ushered in a new era of financial crime. Unfortunately, the novelty of the technology makes the task of catching and prosecuting offenders particularly challenging. Thus, it is necessary to implement automated detection tools related to policies to address the growing criminality in the cryptocurrency realm. 

**Abstract (ZH)**: 去中心化金融（DeFi）社区在近年来迅速增长，得益于对新市场巨大潜力感兴趣的加密货币爱好者们。加密货币 popularity 的激增带来了金融犯罪的新时代。不幸的是，该技术的全新特性使得捕获和起诉罪犯的任务尤为具有挑战性。因此，有必要实施与政策相关的自动化检测工具，以应对加密货币领域日益严重的犯罪问题。 

---
# MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance 

**Title (ZH)**: MDBench：一种基于知识指导生成的合成多文档推理基准 

**Authors**: Joseph J. Peper, Wenzhao Qiu, Ali Payani, Lu Wang  

**Link**: [PDF](https://arxiv.org/pdf/2506.14927)  

**Abstract**: Natural language processing evaluation has made significant progress, largely driven by the proliferation of powerful large language mod-els (LLMs). New evaluation benchmarks are of increasing priority as the reasoning capabilities of LLMs are expanding at a rapid pace. In particular, while multi-document (MD) reasoning is an area of extreme relevance given LLM capabilities in handling longer-context inputs, few benchmarks exist to rigorously examine model behavior in this setting. Moreover, the multi-document setting is historically challenging for benchmark creation due to the expensive cost of annotating long inputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs on the task of multi-document reasoning. Notably, MDBench is created through a novel synthetic generation process, allowing us to controllably and efficiently generate challenging document sets and the corresponding question-answer (QA) examples. Our novel technique operates on condensed structured seed knowledge, modifying it through LLM-assisted edits to induce MD-specific reasoning challenges. We then convert this structured knowledge into a natural text surface form, generating a document set and corresponding QA example. We analyze the behavior of popular LLMs and prompting techniques, finding that MDBENCH poses significant challenges for all methods, even with relatively short document sets. We also see our knowledge-guided generation technique (1) allows us to readily perform targeted analysis of MD-specific reasoning capabilities and (2) can be adapted quickly to account for new challenges and future modeling improvements. 

**Abstract (ZH)**: 自然语言处理评价取得了显著进步，主要得益于强大大型语言模型（LLMs）的普及。随着LLMs处理长上下文输入的推理能力迅速增强，新的评价基准日益重要。特别是，鉴于LLMs在处理长上下文输入方面的能力，多文档（MD）推理是一个极其相关的研究领域，然而鲜有基准能够严格检测模型在这种环境下的行为。此外，由于注释长输入的高昂成本，历史数据集创建在多文档设置下尤为艰难。在这项工作中，我们引入了MDBench，这是一个新的数据集，用于评估LLMs在多文档推理任务上的表现。通过一种新颖的合成生成过程，MDBench能够可控且高效地生成具有挑战性的文档集及其对应的问答示例。我们的新颖技术基于压缩结构化的种子知识，通过LLM辅助编辑来诱导特定于MD的推理挑战，然后将这种结构化的知识转化为自然文本形式，生成文档集及其对应的问答示例。我们分析了流行的LLMs和提示技术的行为，发现即使在相对较短的文档集下，MDBENCH仍能对所有方法提出显著挑战。我们还看到，我们的知识引导生成技术不仅使我们能够针对特定的MD推理能力进行有针对性的分析，而且还能够快速适应新的挑战和未来的建模改进。 

---
# Forecasting the spatiotemporal evolution of fluid-induced microearthquakes with deep learning 

**Title (ZH)**: 基于深度学习的流体诱发微地震的时空演化预测 

**Authors**: Jaehong Chung, Michael Manga, Timothy Kneafsey, Tapan Mukerji, Mengsu Hu  

**Link**: [PDF](https://arxiv.org/pdf/2506.14923)  

**Abstract**: Microearthquakes (MEQs) generated by subsurface fluid injection record the evolving stress state and permeability of reservoirs. Forecasting their full spatiotemporal evolution is therefore critical for applications such as enhanced geothermal systems (EGS), CO$_2$ sequestration and other geo-engineering applications. We present a transformer-based deep learning model that ingests hydraulic stimulation history and prior MEQ observations to forecast four key quantities: cumulative MEQ count, cumulative logarithmic seismic moment, and the 50th- and 95th-percentile extents ($P_{50}, P_{95}$) of the MEQ cloud. Applied to the EGS Collab Experiment 1 dataset, the model achieves $R^2 >0.98$ for the 1-second forecast horizon and $R^2 >0.88$ for the 15-second forecast horizon across all targets, and supplies uncertainty estimates through a learned standard deviation term. These accurate, uncertainty-quantified forecasts enable real-time inference of fracture propagation and permeability evolution, demonstrating the strong potential of deep-learning approaches to improve seismic-risk assessment and guide mitigation strategies in future fluid-injection operations. 

**Abstract (ZH)**: 微震事件（MEQs）由地下流体注入产生，记录了储层应力状态和渗透率的变化。预测其全空间-时间演化对于增强地热系统（EGS）、CO₂密封和其它地质工程应用至关重要。我们提出了一种基于变压器的深度学习模型，该模型整合了液压刺激历史和前期微震观测数据，预测四个关键量：累计微震事件计数、累计对数地震矩以及微震云的第50百分位（P₅₀）和第95百分位（P₉₅）范围。该模型应用于EGS Collab Experiment 1数据集，对于1秒和15秒的预测窗口，在所有目标上的决定系数（$R^2$）分别大于0.98和0.88，并通过学习得到的标准差项提供不确定性估计。这些准确且具有不确定量化测的预测能够实时推断裂缝扩展和渗透率演化，展示了深度学习方法在提高地震风险评估并指导未来流体注入操作中的缓解策略方面的强大潜力。 

---
# Foundation Artificial Intelligence Models for Health Recognition Using Face Photographs (FAHR-Face) 

**Title (ZH)**: 基于面部照片的健康识别基础人工智能模型（FAHR-Face） 

**Authors**: Fridolin Haugg, Grace Lee, John He, Leonard Nürnberg, Dennis Bontempi, Danielle S. Bitterman, Paul Catalano, Vasco Prudente, Dmitrii Glubokov, Andrew Warrington, Suraj Pai, Dirk De Ruysscher, Christian Guthier, Benjamin H. Kann, Vadim N. Gladyshev, Hugo JWL Aerts, Raymond H. Mak  

**Link**: [PDF](https://arxiv.org/pdf/2506.14909)  

**Abstract**: Background: Facial appearance offers a noninvasive window into health. We built FAHR-Face, a foundation model trained on >40 million facial images and fine-tuned it for two distinct tasks: biological age estimation (FAHR-FaceAge) and survival risk prediction (FAHR-FaceSurvival).
Methods: FAHR-FaceAge underwent a two-stage, age-balanced fine-tuning on 749,935 public images; FAHR-FaceSurvival was fine-tuned on 34,389 photos of cancer patients. Model robustness (cosmetic surgery, makeup, pose, lighting) and independence (saliency mapping) was tested extensively. Both models were clinically tested in two independent cancer patient datasets with survival analyzed by multivariable Cox models and adjusted for clinical prognostic factors.
Findings: For age estimation, FAHR-FaceAge had the lowest mean absolute error of 5.1 years on public datasets, outperforming benchmark models and maintaining accuracy across the full human lifespan. In cancer patients, FAHR-FaceAge outperformed a prior facial age estimation model in survival prognostication. FAHR-FaceSurvival demonstrated robust prediction of mortality, and the highest-risk quartile had more than triple the mortality of the lowest (adjusted hazard ratio 3.22; P<0.001). These findings were validated in the independent cohort and both models showed generalizability across age, sex, race and cancer subgroups. The two algorithms provided distinct, complementary prognostic information; saliency mapping revealed each model relied on distinct facial regions. The combination of FAHR-FaceAge and FAHR-FaceSurvival improved prognostic accuracy.
Interpretation: A single foundation model can generate inexpensive, scalable facial biomarkers that capture both biological ageing and disease-related mortality risk. The foundation model enabled effective training using relatively small clinical datasets. 

**Abstract (ZH)**: 背景：面部外观提供了健康状况的非侵入性窗口。我们构建了FAHR-Face基础模型，该模型训练于超过4000万张面部图像，并针对两种不同的任务进行了微调：生物学年龄估计（FAHR-FaceAge）和生存风险预测（FAHR-FaceSurvival）。 

---
# PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning 

**Title (ZH)**: PeRL：排序增强强化学习在交错视觉-语言推理中的应用 

**Authors**: Yizhen Zhang, Yang Ding, Shuoshuo Zhang, Xinchen Zhang, Haoling Li, Zhong-zhi Li, Peijie Wang, Jie Wu, Lei Ji, Yelong Shen, Yujiu Yang, Yeyun Gong  

**Link**: [PDF](https://arxiv.org/pdf/2506.14907)  

**Abstract**: Inspired by the impressive reasoning capabilities demonstrated by reinforcement learning approaches like DeepSeek-R1, recent emerging research has begun exploring the use of reinforcement learning (RL) to enhance vision-language models (VLMs) for multimodal reasoning tasks. However, most existing multimodal reinforcement learning approaches remain limited to spatial reasoning within single-image contexts, yet still struggle to generalize to more complex and real-world scenarios involving multi-image positional reasoning, where understanding the relationships across images is crucial. To address this challenge, we propose a general reinforcement learning approach PeRL tailored for interleaved multimodal tasks, and a multi-stage strategy designed to enhance the exploration-exploitation trade-off, thereby improving learning efficiency and task performance. Specifically, we introduce permutation of image sequences to simulate varied positional relationships to explore more spatial and positional diversity. Furthermore, we design a rollout filtering mechanism for resampling to focus on trajectories that contribute most to learning optimal behaviors to exploit learned policies effectively. We evaluate our model on 5 widely-used multi-image benchmarks and 3 single-image benchmarks. Our experiments confirm that PeRL trained model consistently surpasses R1-related and interleaved VLM baselines by a large margin, achieving state-of-the-art performance on multi-image benchmarks, while preserving comparable performance on single-image tasks. 

**Abstract (ZH)**: 受DeepSeek-R1等强化学习方法令人印象深刻的推理能力启发，近期研究开始探索使用强化学习（RL）来增强多模态语言模型（VLMs）以提高多模态推理任务的能力。然而，大多数现有的多模态强化学习方法仍然局限于单图像范围内的空间推理，难以泛化到包含多图像位置推理的更复杂和现实生活场景，其中理解图像间的相互关系至关重要。为应对这一挑战，我们提出了一种针对交错多模态任务设计的一般强化学习方法PeRL，并设计一个多阶段策略以优化探索与利用之间的权衡，从而提高学习效率和任务性能。具体而言，我们引入了图像序列的排列以模拟不同的位置关系，探索更多的空间和位置多样性。此外，我们设计了一种滚动策略过滤机制以重采样，专注于最有助于学习最优行为的轨迹，从而有效利用所学策略。我们在5个广泛使用的多图像基准和3个单图像基准上评估了我们的模型。实验结果证实，与R1相关的和交错的VLM基线相比，PeRL训练的模型在多图像基准上取得了显著的性能优势，同时在单图像任务上保持了可比拟的性能。 

---
# Preparing for the Intelligence Explosion 

**Title (ZH)**: 准备迎接智能爆炸 

**Authors**: William MacAskill, Fin Moorhouse  

**Link**: [PDF](https://arxiv.org/pdf/2506.14863)  

**Abstract**: AI that can accelerate research could drive a century of technological progress over just a few years. During such a period, new technological or political developments will raise consequential and hard-to-reverse decisions, in rapid succession. We call these developments grand challenges. These challenges include new weapons of mass destruction, AI-enabled autocracies, races to grab offworld resources, and digital beings worthy of moral consideration, as well as opportunities to dramatically improve quality of life and collective decision-making. We argue that these challenges cannot always be delegated to future AI systems, and suggest things we can do today to meaningfully improve our prospects. AGI preparedness is therefore not just about ensuring that advanced AI systems are aligned: we should be preparing, now, for the disorienting range of developments an intelligence explosion would bring. 

**Abstract (ZH)**: AI能够加速研发可能在几年内推动百年科技进步。在这段时间内，新的技术或政治发展将迅速提出重要且难以逆转的决策。我们称之为宏伟挑战。这些挑战包括新型大规模毁灭性武器、AI驱动的獨裁政权、争夺外太空资源的竞争，以及值得道德关怀的数字生命体，同时也有大幅提高生活质量及集体决策的机会。我们主张这些挑战不能总是留给未来的AI系统处理，建议我们现在可以采取措施以实质性地改善我们的前景。因此，超人工智能准备不仅关乎确保先进AI系统的对齐：我们应当为智力爆炸带来的各种错综复杂的发展做好准备。 

---
# Identifiability by common backdoor in summary causal graphs of time series 

**Title (ZH)**: 时间序列汇总因果图中的公共后门可识别性 

**Authors**: Clément Yvernes, Charles K. Assaad, Emilie Devijver, Eric Gaussier  

**Link**: [PDF](https://arxiv.org/pdf/2506.14862)  

**Abstract**: The identifiability problem for interventions aims at assessing whether the total effect of some given interventions can be written with a do-free formula, and thus be computed from observational data only. We study this problem, considering multiple interventions and multiple effects, in the context of time series when only abstractions of the true causal graph in the form of summary causal graphs are available. We focus in this study on identifiability by a common backdoor set, and establish, for time series with and without consistency throughout time, conditions under which such a set exists. We also provide algorithms of limited complexity to decide whether the problem is identifiable or not. 

**Abstract (ZH)**: 干预的可识别性问题旨在评估某些给定干预的总效果是否可以用 do-自由公式表示，并从而仅通过观测数据进行计算。我们在仅可获取真实因果图的抽象形式即汇总因果图的时间序列情境下，研究此问题，考虑多个干预和多个效果。本研究聚焦于由共同后门集的可识别性，并为具有和不具时间一致性的时间序列建立了这样的集存在条件。我们还提供了复杂度有限的算法来决定该问题是否可识别。 

---
# BMFM-RNA: An Open Framework for Building and Evaluating Transcriptomic Foundation Models 

**Title (ZH)**: BMFM-RNA：一个构建和评估转录组基础模型的开源框架 

**Authors**: Bharath Dandala, Michael M. Danziger, Ella Barkan, Tanwi Biswas, Viatcheslav Gurev, Jianying Hu, Matthew Madgwick, Akira Koseki, Tal Kozlovski, Michal Rosen-Zvi, Yishai Shimoni, Ching-Huei Tsou  

**Link**: [PDF](https://arxiv.org/pdf/2506.14861)  

**Abstract**: Transcriptomic foundation models (TFMs) have recently emerged as powerful tools for analyzing gene expression in cells and tissues, supporting key tasks such as cell-type annotation, batch correction, and perturbation prediction. However, the diversity of model implementations and training strategies across recent TFMs, though promising, makes it challenging to isolate the contribution of individual design choices or evaluate their potential synergies. This hinders the field's ability to converge on best practices and limits the reproducibility of insights across studies. We present BMFM-RNA, an open-source, modular software package that unifies diverse TFM pretraining and fine-tuning objectives within a single framework. Leveraging this capability, we introduce a novel training objective, whole cell expression decoder (WCED), which captures global expression patterns using an autoencoder-like CLS bottleneck representation. In this paper, we describe the framework, supported input representations, and training objectives. We evaluated four model checkpoints pretrained on CELLxGENE using combinations of masked language modeling (MLM), WCED and multitask learning. Using the benchmarking capabilities of BMFM-RNA, we show that WCED-based models achieve performance that matches or exceeds state-of-the-art approaches like scGPT across more than a dozen datasets in both zero-shot and fine-tuning tasks. BMFM-RNA, available as part of the biomed-multi-omics project ( this https URL ), offers a reproducible foundation for systematic benchmarking and community-driven exploration of optimal TFM training strategies, enabling the development of more effective tools to leverage the latest advances in AI for understanding cell biology. 

**Abstract (ZH)**: 转录组基石模型（TFMs） recently emerged as powerful tools for analyzing gene expression in cells and tissues, supporting key tasks such as cell-type annotation, batch correction, and perturbation prediction. However, the diversity of model implementations and training strategies across recent TFMs, though promising, makes it challenging to isolate the contribution of individual design choices or evaluate their potential synergies. This hinders the field's ability to converge on best practices and limits the reproducibility of insights across studies. We present BMFM-RNA, an open-source, modular software package that unifies diverse TFM pretraining and fine-tuning objectives within a single framework. Leveraging this capability, we introduce a novel training objective, whole cell expression decoder (WCED), which captures global expression patterns using an autoencoder-like CLS bottleneck representation. In this paper, we describe the framework, supported input representations, and training objectives. We evaluated four model checkpoints pretrained on CELLxGENE using combinations of masked language modeling (MLM), WCED and multitask learning. Using the benchmarking capabilities of BMFM-RNA, we show that WCED-based models achieve performance that matches or exceeds state-of-the-art approaches like scGPT across more than a dozen datasets in both zero-shot and fine-tuning tasks. BMFM-RNA, available as part of the biomed-multi-omics project (<https://github.com/biomed-multi-omics/bmfm-rna>), offers a reproducible foundation for systematic benchmarking and community-driven exploration of optimal TFM training strategies, enabling the development of more effective tools to leverage the latest advances in AI for understanding cell biology. 

---
# Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction 

**Title (ZH)**: 探索未知：基于神经不确定性图的主动视图选择在三维重建中的应用 

**Authors**: Zhengquan Zhang, Feng Xu, Mengmi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2506.14856)  

**Abstract**: Some perspectives naturally provide more information than others. How can an AI system determine which viewpoint offers the most valuable insight for accurate and efficient 3D object reconstruction? Active view selection (AVS) for 3D reconstruction remains a fundamental challenge in computer vision. The aim is to identify the minimal set of views that yields the most accurate 3D reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian Splatting, from a current observation and computing uncertainty for each candidate viewpoint, we introduce a novel AVS approach guided by neural uncertainty maps predicted by a lightweight feedforward deep neural network, named UPNet. UPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, representing uncertainty values across all possible candidate viewpoints. By leveraging heuristics derived from observing many natural objects and their associated uncertainty patterns, we train UPNet to learn a direct mapping from viewpoint appearance to uncertainty in the underlying volumetric representations. Next, our approach aggregates all previously predicted neural uncertainty maps to suppress redundant candidate viewpoints and effectively select the most informative one. Using these selected viewpoints, we train 3D neural rendering models and evaluate the quality of novel view synthesis against other competitive AVS methods. Remarkably, despite using half of the viewpoints than the upper bound, our method achieves comparable reconstruction accuracy. In addition, it significantly reduces computational overhead during AVS, achieving up to a 400 times speedup along with over 50\% reductions in CPU, RAM, and GPU usage compared to baseline methods. Notably, our approach generalizes effectively to AVS tasks involving novel object categories, without requiring any additional training. 

**Abstract (ZH)**: 一些视角自然提供了更多的信息。如何使AI系统确定哪些视角能提供最 valuable insight 以实现准确且高效的3D物体重构？基于神经不确定性图的主动视角选择（AVS）在计算机视觉中仍然是一个基本挑战。目标是识别出能够实现最准确3D重构的最小视角集。不同于从当前观察中学习辐射场或3D高斯点云方法，我们提出了一种新的由轻量级前馈深度神经网络（UPNet）引导的AVS方法，并预测神经不确定性图。UPNet接收3D物体的一张输入图像，并输出预测的不确定性图，表示所有候选视角的不确定性值。通过利用从观察许多自然物体及其相关不确定性模式中得到的经验法则，我们训练UPNet学习从视角外观到底层体和平面表示中的不确定性之间的直接映射。然后，我们的方法聚合所有先前预测的神经不确定性图以抑制冗余的候选视角，并有效选择最信息量的视角。使用这些选定的视角，我们训练3D神经渲染模型，并将新型视角合成的质量与其他竞争性AVS方法进行评估。令人惊讶的是，尽管使用的视角数量仅为上限的一半，我们的方法仍能达到相似的重构准确度。此外，它显著降低了AVS过程中的计算开销，与基线方法相比，最多可实现400倍的速度提升，并且在CPU、RAM和GPU使用上降低超过50%。值得注意的是，我们的方法在不需要任何额外训练的情况下有效泛化到涉及新物体类别的AVS任务中。 

---
# Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation -- Adios low-level controllers 

**Title (ZH)**: 反馈-MPPI：通过回溯梯度采样优化的快速MPC——再见低级控制器 

**Authors**: Tommaso Belvedere, Michael Ziegltrum, Giulio Turrisi, Valerio Modugno  

**Link**: [PDF](https://arxiv.org/pdf/2506.14855)  

**Abstract**: Model Predictive Path Integral control is a powerful sampling-based approach suitable for complex robotic tasks due to its flexibility in handling nonlinear dynamics and non-convex costs. However, its applicability in real-time, highfrequency robotic control scenarios is limited by computational demands. This paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments standard MPPI by computing local linear feedback gains derived from sensitivity analysis inspired by Riccati-based feedback used in gradient-based MPC. These gains allow for rapid closed-loop corrections around the current state without requiring full re-optimization at each timestep. We demonstrate the effectiveness of F-MPPI through simulations and real-world experiments on two robotic platforms: a quadrupedal robot performing dynamic locomotion on uneven terrain and a quadrotor executing aggressive maneuvers with onboard computation. Results illustrate that incorporating local feedback significantly improves control performance and stability, enabling robust, high-frequency operation suitable for complex robotic systems. 

**Abstract (ZH)**: 反馈-MPPP 控制：一种通过局部反馈增强的 MPPI 新框架 

---
# Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis 

**Title (ZH)**: 高效的零售视频标注：一种用于产品和顾客交互分析的稳健关键帧生成方法 

**Authors**: Varun Mannam, Zhenyu Shi  

**Link**: [PDF](https://arxiv.org/pdf/2506.14854)  

**Abstract**: Accurate video annotation plays a vital role in modern retail applications, including customer behavior analysis, product interaction detection, and in-store activity recognition. However, conventional annotation methods heavily rely on time-consuming manual labeling by human annotators, introducing non-robust frame selection and increasing operational costs. To address these challenges in the retail domain, we propose a deep learning-based approach that automates key-frame identification in retail videos and provides automatic annotations of products and customers. Our method leverages deep neural networks to learn discriminative features by embedding video frames and incorporating object detection-based techniques tailored for retail environments. Experimental results showcase the superiority of our approach over traditional methods, achieving accuracy comparable to human annotator labeling while enhancing the overall efficiency of retail video annotation. Remarkably, our approach leads to an average of 2 times cost savings in video annotation. By allowing human annotators to verify/adjust less than 5% of detected frames in the video dataset, while automating the annotation process for the remaining frames without reducing annotation quality, retailers can significantly reduce operational costs. The automation of key-frame detection enables substantial time and effort savings in retail video labeling tasks, proving highly valuable for diverse retail applications such as shopper journey analysis, product interaction detection, and in-store security monitoring. 

**Abstract (ZH)**: 准确的视频标注在现代零售应用中发挥着重要作用，包括顾客行为分析、产品交互检测和店内活动识别。然而，传统的标注方法高度依赖耗时的手动标注，导致帧选择不够稳健并增加了运营成本。为了解决零售领域的这些挑战，我们提出了一种基于深度学习的方法，自动识别零售视频的关键帧，并提供产品和顾客的自动标注。该方法利用深度神经网络学习具有鉴别性的特征，通过嵌入视频帧并结合适用于零售环境的对象检测技术。实验结果展示了我们方法在准确性和传统方法相比的优势，同时提高了零售视频标注的整体效率。令人印象深刻的是，我们的方法在视频标注方面平均节省了20%的成本。通过让人工标注员验证/调整视频数据集中少于5%的检测帧，而自动化剩余帧的标注过程，零售商可以显著降低运营成本。关键帧检测的自动化在零售视频标签任务中节省了大量的时间和精力，对多种零售应用，如购物者旅程分析、产品交互检测和店内安全监控，都非常有价值。 

---
# Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching 

**Title (ZH)**: 基于测试时计划缓存的LLM代理高效服务 

**Authors**: Qizheng Zhang, Michael Wornow, Kunle Olukotun  

**Link**: [PDF](https://arxiv.org/pdf/2506.14852)  

**Abstract**: LLM-based agentic applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs due to extensive planning and reasoning requirements. Existing LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agentic applications where outputs depend on external data or environmental contexts. We propose agentic plan caching, a novel approach that extracts, stores, adapts, and reuses structured plan templates from planning stages of agentic applications across semantically similar tasks to reduce the cost of serving. Unlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. Evaluation across multiple real-world agentic applications shows that our system can reduce costs by 46.62% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures. 

**Abstract (ZH)**: 基于LLM的代理型应用在复杂工作流中显示出了 increasingly remarkable capabilities，但由于广泛的规划和推理需求而产生显著成本。现有的LLM缓存技术（如上下文缓存和语义缓存）主要针对聊天机器人设计，在代理型应用中不足，因为其输出依赖于外部数据或环境上下文。我们提出了代理型计划缓存这一新的方法，该方法从具有语义相似性的任务的代理应用规划阶段中抽取出、存储、适应并重用结构化的计划模板，以降低服务成本。与传统的语义缓存不同，我们的系统在测试时从已完成的代理执行中提取计划模板，使用关键词提取将新请求与缓存的计划匹配，并利用轻量级模型将这些模板适应为具体任务的计划，包含特定上下文。在多个实际代理型应用中的评估表明，我们的系统平均可降低46.62%的成本同时保持性能，提供了一种补充现有LLM服务基础设施的更高效解决方案。 

---
# Efficient Serving of LLM Applications with Probabilistic Demand Modeling 

**Title (ZH)**: 基于概率需求建模的大规模语言模型应用高效服务 

**Authors**: Yifei Liu, Zuo Gan, Zhenghao Gan, Weiye Wang, Chen Chen, Yizhou Shan, Xusheng Chen, Zhenhua Han, Yifei Zhu, Shixuan Sun, Minyi Guo  

**Link**: [PDF](https://arxiv.org/pdf/2506.14851)  

**Abstract**: Applications based on Large Language Models (LLMs) contains a series of tasks to address real-world problems with boosted capability, which have dynamic demand volumes on diverse backends. Existing serving systems treat the resource demands of LLM applications as a blackbox, compromising end-to-end efficiency due to improper queuing order and backend warm up latency. We find that the resource demands of LLM applications can be modeled in a general and accurate manner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which leverages PDGraph for efficient serving of LLM applications. Confronting probabilistic demand description, Hermes applies the Gittins policy to determine the scheduling order that can minimize the average application completion time. It also uses the PDGraph model to help prewarm cold backends at proper moments. Experiments with diverse LLM applications confirm that Hermes can effectively improve the application serving efficiency, reducing the average completion time by over 70% and the P95 completion time by over 80%. 

**Abstract (ZH)**: 基于大型语言模型的应用包含一系列任务以提升解决实际问题的能力，这些任务对多样化的后端资源有动态的需求量。现有的服务系统将大型语言模型应用的资源需求视为黑盒，由于不当的队列顺序和后端预热延迟，导致端到端效率受损。我们发现，可以使用概率需求图（PDGraph）以通用且精确的方式建模大型语言模型应用的资源需求。我们提出了Hermes，它利用PDGraph实现大型语言模型应用的高效服务。面对概率性需求描述，Hermes应用吉廷斯策略以最小化平均应用完成时间确定调度顺序，并使用PDGraph模型在适当时刻预热冷后端。多样化的大型语言模型应用实验表明，Hermes能够有效提升应用服务效率，平均完成时间缩短超过70%，P95完成时间缩短超过80%。 

---
# Finding Optimal Kernel Size and Dimension in Convolutional Neural Networks An Architecture Optimization Approach 

**Title (ZH)**: 在卷积神经网络中寻找最优核大小和维度的架构优化方法 

**Authors**: Shreyas Rajeev, B Sathish Babu  

**Link**: [PDF](https://arxiv.org/pdf/2506.14846)  

**Abstract**: Kernel size selection in Convolutional Neural Networks (CNNs) is a critical but often overlooked design decision that affects receptive field, feature extraction, computational cost, and model accuracy. This paper proposes the Best Kernel Size Estimation Function (BKSEF), a mathematically grounded and empirically validated framework for optimal, layer-wise kernel size determination. BKSEF balances information gain, computational efficiency, and accuracy improvements by integrating principles from information theory, signal processing, and learning theory. Extensive experiments on CIFAR-10, CIFAR-100, ImageNet-lite, ChestX-ray14, and GTSRB datasets demonstrate that BKSEF-guided architectures achieve up to 3.1 percent accuracy improvement and 42.8 percent reduction in FLOPs compared to traditional models using uniform 3x3 kernels. Two real-world case studies further validate the approach: one for medical image classification in a cloud-based setup, and another for traffic sign recognition on edge devices. The former achieved enhanced interpretability and accuracy, while the latter reduced latency and model size significantly, with minimal accuracy trade-off. These results show that kernel size can be an active, optimizable parameter rather than a fixed heuristic. BKSEF provides practical heuristics and theoretical support for researchers and developers seeking efficient and application-aware CNN designs. It is suitable for integration into neural architecture search pipelines and real-time systems, offering a new perspective on CNN optimization. 

**Abstract (ZH)**: 卷积神经网络（CNN）中的内核大小选择是一种关键但常常被忽视的设计决策，它影响着感受野、特征提取、计算成本和模型准确性。本文提出了最佳内核大小估计函数（BKSEF），这是一种基于数学原理并经过实验证明的框架，用于最优的逐层内核大小确定。BKSEF通过结合信息理论、信号处理和学习理论的原则，平衡信息增益、计算效率和精度提升。在CIFAR-10、CIFAR-100、ImageNet-lite、ChestX-ray14和GTSRB数据集上的广泛实验表明，BKSEF引导的架构相比使用统一3x3内核的传统模型，可以实现多达3.1个百分点的准确性提升和42.8个百分点的FLOPs减少。两个实际案例进一步验证了该方法：一个是在云环境中进行医学图像分类的应用，另一个是在边缘设备上进行交通标志识别的应用。前者提高了解释性和准确性，后者显著减少了延迟和模型大小，同时仅有轻微的准确性妥协。这些结果表明，内核大小可以是一个活跃的、可优化的参数，而不仅仅是一个固定的启发式参数。BKSEF为寻求高效和应用感知的CNN设计的研究人员和开发人员提供了实用的启发式方法和理论支持。它可以集成到神经架构搜索管道和实时系统中，为CNN优化提供了一个新的视角。 

---
# PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers 

**Title (ZH)**: PictSure: 预训练嵌入对于基于上下文学习图像分类器很重要 

**Authors**: Lukas Schiesser, Cornelius Wolff, Sophie Haas, Simon Pukrop  

**Link**: [PDF](https://arxiv.org/pdf/2506.14842)  

**Abstract**: Building image classification models remains cumbersome in data-scarce domains, where collecting large labeled datasets is impractical. In-context learning (ICL) has emerged as a promising paradigm for few-shot image classification (FSIC), enabling models to generalize across domains without gradient-based adaptation. However, prior work has largely overlooked a critical component of ICL-based FSIC pipelines: the role of image embeddings. In this work, we present PictSure, an ICL framework that places the embedding model -- its architecture, pretraining, and training dynamics -- at the center of analysis. We systematically examine the effects of different visual encoder types, pretraining objectives, and fine-tuning strategies on downstream FSIC performance. Our experiments show that the training success and the out-of-domain performance are highly dependent on how the embedding models are pretrained. Consequently, PictSure manages to outperform existing ICL-based FSIC models on out-of-domain benchmarks that differ significantly from the training distribution, while maintaining comparable results on in-domain tasks. Code can be found at this https URL. 

**Abstract (ZH)**: 在数据稀缺领域建立图像分类模型仍然较为繁琐，其中收集大型带标签数据集是不实际的。上下文学习（ICL）已成为少量样本图像分类（FSIC）的一种有前景的范式，使模型能够在无需基于梯度的适应情况下跨域泛化。然而，先前的工作大多忽视了ICL基FSIC管道中的一个关键组成部分：图像嵌入的作用。本文中，我们提出PictSure，这是一种将嵌入模型——其架构、预训练和训练动力学——放在核心分析位置的ICL框架。我们系统地研究了不同类型视觉编码器、预训练目标和微调策略对下游FSIC性能的影响。我们的实验表明，训练成功率和域外性能高度依赖于嵌入模型的预训练方式。因此，PictSure在训练分布与域外基准有显著差异的情况下取得了优于现有ICL基FSIC模型的表现，同时在域内任务上保持了相当的结果。代码可在此链接找到。 

---
# Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction 

**Title (ZH)**: 基于结构化指令的图表到代码生成改进迭代精炼方法 

**Authors**: Chengzhi Xu, Yuyang Wang, Lai Wei, Lichao Sun, Weiran Huang  

**Link**: [PDF](https://arxiv.org/pdf/2506.14837)  

**Abstract**: Recently, multimodal large language models (MLLMs) have attracted increasing research attention due to their powerful visual understanding capabilities. While they have achieved impressive results on various vision tasks, their performance on chart-to-code generation remains suboptimal. This task requires MLLMs to generate executable code that can reproduce a given chart, demanding not only precise visual understanding but also accurate translation of visual elements into structured code. Directly prompting MLLMs to perform this complex task often yields unsatisfactory results. To address this challenge, we propose {ChartIR}, an iterative refinement method based on structured instruction. First, we distinguish two tasks: visual understanding and code translation. To accomplish the visual understanding component, we design two types of structured instructions: description and difference. The description instruction captures the visual elements of the reference chart, while the difference instruction characterizes the discrepancies between the reference chart and the generated chart. These instructions effectively transform visual features into language representations, thereby facilitating the subsequent code translation process. Second, we decompose the overall chart generation pipeline into two stages: initial code generation and iterative refinement, enabling progressive enhancement of the final output. Experimental results show that, compared to other method, our method achieves superior performance on both the open-source model Qwen2-VL and the closed-source model GPT-4o. 

**Abstract (ZH)**: 最近，基于多模态的大语言模型（MLLMs）由于其强大的视觉理解能力而吸引了越来越多的研究关注。尽管它们在各种视觉任务上取得了 impressive 的成果，但在图表到代码生成方面的表现仍然不尽如人意。这项任务要求MLLMs生成可以重现给定图表的可执行代码，不仅需要精确的视觉理解，还需要将视觉元素准确地转化为结构化的代码。直接促使MLLMs执行这一复杂任务往往会产生不令人满意的结果。为此，我们提出了一种基于结构化指令的迭代精炼方法{ChartIR}。首先，我们将任务区分为视觉理解与代码翻译两个部分。为了完成视觉理解部分，我们设计了两种类型的结构化指令：描述指令和差异指令。描述指令捕获参考图表的视觉元素，而差异指令则描述参考图表与生成图表之间的差异。这些指令有效地将视觉特征转换为语言表示，从而促进后续的代码翻译过程。其次，我们将整体图表生成流程分解为两个阶段：初始代码生成和迭代精炼，这使得最终输出可以逐步增强。实验结果表明，与其它方法相比，我们的方法在开源模型Qwen2-VL和封闭源模型GPT-4o上均取得了更优的性能。 

---
# Deploying and Evaluating Multiple Deep Learning Models on Edge Devices for Diabetic Retinopathy Detection 

**Title (ZH)**: 在边缘设备上部署和评估多种深度学习模型以检测糖尿病视网膜病变 

**Authors**: Akwasi Asare, Dennis Agyemanh Nana Gookyi, Derrick Boateng, Fortunatus Aabangbio Wulnye  

**Link**: [PDF](https://arxiv.org/pdf/2506.14834)  

**Abstract**: Diabetic Retinopathy (DR), a leading cause of vision impairment in individuals with diabetes, affects approximately 34.6% of diabetes patients globally, with the number of cases projected to reach 242 million by 2045. Traditional DR diagnosis relies on the manual examination of retinal fundus images, which is both time-consuming and resource intensive. This study presents a novel solution using Edge Impulse to deploy multiple deep learning models for real-time DR detection on edge devices. A robust dataset of over 3,662 retinal fundus images, sourced from the Kaggle EyePACS dataset, was curated, and enhanced through preprocessing techniques, including augmentation and normalization. Using TensorFlow, various Convolutional Neural Networks (CNNs), such as MobileNet, ShuffleNet, SqueezeNet, and a custom Deep Neural Network (DNN), were designed, trained, and optimized for edge deployment. The models were converted to TensorFlowLite and quantized to 8-bit integers to reduce their size and enhance inference speed, with minimal trade-offs in accuracy. Performance evaluations across different edge hardware platforms, including smartphones and microcontrollers, highlighted key metrics such as inference speed, accuracy, precision, and resource utilization. MobileNet achieved an accuracy of 96.45%, while SqueezeNet demonstrated strong real-time performance with a small model size of 176 KB and latency of just 17 ms on GPU. ShuffleNet and the custom DNN achieved moderate accuracy but excelled in resource efficiency, making them suitable for lower-end devices. This integration of edge AI technology into healthcare presents a scalable, cost-effective solution for early DR detection, providing timely and accurate diagnosis, especially in resource-constrained and remote healthcare settings. 

**Abstract (ZH)**: 糖尿病视网膜病变（DR）：导致糖尿病患者视力损伤的主要原因，全球约影响34.6%的糖尿病患者，预计到2045年病例数将达到242 million。传统的DR诊断依赖于对眼底图像的手动检查，这既耗时又资源密集。本研究提出了一个使用Edge Impulse的新型解决方案，以在边缘设备上实现实时DR检测。一个包含超过3,662张眼底图像的稳健数据集，来源于Kaggle EyePACS数据集，并通过预处理技术，包括增强和归一化，进行了增强。使用TensorFlow设计并训练了多种卷积神经网络（CNNs），如MobileNet、ShuffleNet、SqueezeNet，以及一个自定义的深度神经网络（DNN），并进行了边端部署优化。将模型转换为TensorFlowLite并量化为8位整数，以减少模型大小和提高推理速度，同时保持较高的准确性。在不同的边缘硬件平台上，包括智能手机和微控制器，进行了性能评估，重点考察了推理速度、准确性、精度和资源利用率等关键指标。MobileNet的准确率为96.45%，而SqueezeNet展示了强大的实时性能，模型大小仅为176 KB，GPU上的延迟仅为17 ms。ShuffleNet和自定义DNN的准确率中等，但在资源效率方面表现出色，适用于低端设备。将边缘AI技术融入医疗保健为早期DR检测提供了一个可扩展且成本效益高的解决方案，尤其是在资源有限和偏远的医疗保健环境中提供了及时且准确的诊断。 

---
# Real-Time, Low-Latency Surveillance Using Entropy-Based Adaptive Buffering and MobileNetV2 on Edge Devices 

**Title (ZH)**: 基于熵自适应缓冲和MobileNetV2的边缘设备实时低延时 surveillance 技术 

**Authors**: Poojashree Chandrashekar Pankaj M Sajjanar  

**Link**: [PDF](https://arxiv.org/pdf/2506.14833)  

**Abstract**: This paper describes a high-performance, low-latency video surveillance system designed for resource-constrained environments. We have proposed a formal entropy-based adaptive frame buffering algorithm and integrated that with MobileNetV2 to achieve high throughput with low latency. The system is capable of processing live streams of video with sub-50ms end-to-end inference latency on resource-constrained devices (embedding platforms) such as Raspberry Pi, Amazon, and NVIDIA Jetson Nano. Our method maintains over 92% detection accuracy on standard datasets focused on video surveillance and exhibits robustness to varying lighting, backgrounds, and speeds. A number of comparative and ablation experiments validate the effectiveness of our design. Finally, our architecture is scalable, inexpensive, and compliant with stricter data privacy regulations than common surveillance systems, so that the system could coexist in a smart city or embedded security architecture. 

**Abstract (ZH)**: 一种资源受限环境下高性能低延迟的视频 surveillance 系统设计与实现 

---
# ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural Shapes 

**Title (ZH)**: ArchShapeNet：一个可解释的3D-CNN框架，用于评估建筑形态 

**Authors**: Jun Yin, Jing Zhong, Pengyu Zeng, Peilin Li, Zixuan Dai, Miao Zhang, Shuai Lu  

**Link**: [PDF](https://arxiv.org/pdf/2506.14832)  

**Abstract**: In contemporary architectural design, the growing complexity and diversity of design demands have made generative plugin tools essential for quickly producing initial concepts and exploring novel 3D forms. However, objectively analyzing the differences between human-designed and machine-generated 3D forms remains a challenge, limiting our understanding of their respective strengths and hindering the advancement of generative tools.
To address this, we built ArchForms-4000, a dataset containing 2,000 architect-designed and 2,000 Evomass-generated 3D forms; Proposed ArchShapeNet, a 3D convolutional neural network tailored for classifying and analyzing architectural forms, incorporating a saliency module to highlight key spatial features aligned with architectural reasoning; And conducted comparative experiments showing our model outperforms human experts in distinguishing form origins, achieving 94.29% accuracy, 96.2% precision, and 98.51% recall.
This study not only highlights the distinctive advantages of human-designed forms in spatial organization, proportional harmony, and detail refinement but also provides valuable insights for enhancing generative design tools in the future. 

**Abstract (ZH)**: 当前建筑设计中日益增长的复杂性和多样性需求使得生成式插件工具成为快速产生初步概念和探索新型3D形式的必要工具。然而，客观分析人类设计与机器生成的3D形式之间的差异仍然颇具挑战性，这限制了我们对其各自优势的理解，并阻碍了生成工具的发展。

为此，我们构建了ArchForms-4000数据集，包含2000个人类设计的和2000个由Evomass生成的3D形式；提出了ArchShapeNet，一种专为分类和分析建筑形式设计的3D卷积神经网络，该网络包含一个显著性模块，用于突出与建筑推理相一致的关键空间特征；并通过比较实验展示了我们的模型在区分形式起源方面优于人类专家，准确率达94.29%，精确率达96.2%，召回率达98.51%。

该研究不仅突显了人类设计形式在空间组织、比例和谐和细节精炼方面的独特优势，还为未来增强生成设计工具提供了宝贵的见解。 

---
# Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model 

**Title (ZH)**: 基于多头注意力机制的双向门控环形单元优化在SSD健康状态分类模型中 

**Authors**: Zhizhao Wen, Ruoxin Zhang, Chao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2506.14830)  

**Abstract**: Aiming at the critical role of SSD health state prediction in data reliability assurance, this study proposes a hybrid BiGRU-MHA model that incorporates a multi-head attention mechanism to enhance the accuracy and stability of storage device health classification. The model innovatively integrates temporal feature extraction and key information focusing capabilities. Specifically, it leverages the bidirectional timing modeling advantages of the BiGRU network to capture both forward and backward dependencies of SSD degradation features. Simultaneously, the multi-head attention mechanism dynamically assigns feature weights, improving the model's sensitivity to critical health indicators. Experimental results show that the proposed model achieves classification accuracies of 92.70% on the training set and 92.44% on the test set, with a minimal performance gap of only 0.26%, demonstrating excellent generalization ability. Further analysis using the receiver operating characteristic (ROC) curve shows an area under the curve (AUC) of 0.94 on the test set, confirming the model's robust binary classification performance. This work not only presents a new technical approach for SSD health prediction but also addresses the generalization bottleneck of traditional models, offering a verifiable method with practical value for preventive maintenance of industrial-grade storage systems. The results show the model can significantly reduce data loss risks by providing early failure warnings and help optimize maintenance costs, supporting intelligent decision-making in building reliable storage systems for cloud computing data centers and edge storage environments. 

**Abstract (ZH)**: 针对SSD健康状态预测在数据可靠性保障中的关键作用，本研究提出了一种结合多头注意力机制的混合BiGRU-MHA模型，以提高存储设备健康分类的准确性和稳定性。该模型创新性地整合了时间特征提取和关键信息聚焦能力。具体而言，它利用BiGRU网络的双向时间建模优势，捕捉SSD退化特征的前后依赖关系。同时，多头注意力机制动态分配特征权重，提高模型对关键健康指标的敏感性。实验结果表明，所提出模型在训练集上的分类准确率为92.70%，在测试集上的分类准确率为92.44%，性能差距仅为0.26%，展现出优秀的泛化能力。进一步使用接收者操作特征（ROC）曲线分析，测试集下的曲线下面积（AUC）为0.94，证实了模型的稳健二分类性能。本工作不仅提出了一种新的SSD健康预测技术方法，还解决了传统模型的泛化瓶颈问题，提供了一种具有实际价值的验证方法，用于工业级存储系统的预防性维护。研究结果表明，该模型可以显著降低数据丢失风险并提供早期故障预警，有助于优化维护成本，支持在云计算数据中心和边缘存储环境中构建可靠存储系统的智能决策。 

---
# The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges & Opportunities 

**Title (ZH)**: 在人工智能社会影响研究中实现影响的挑战与机遇：地面视角下的难度encent
note
纠正了最后的错误，输出为：
在人工智能社会影响研究中实现影响的挑战与机遇：地面视角下的 hardness 

**Authors**: Aditya Majumdar, Wenbo Zhang, Kashvi Prawal, Amulya Yadav  

**Link**: [PDF](https://arxiv.org/pdf/2506.14829)  

**Abstract**: In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects focus on harnessing AI to address societal issues in areas such as healthcare, social justice, etc. Unfortunately, despite growing interest in AI4SI, achieving tangible, on-the-ground impact remains a significant challenge. For example, identifying and engaging motivated collaborators who are willing to co-design and deploy AI based solutions in real-world settings is often difficult. Even when such partnerships are established, many AI4SI projects "fail" to progress beyond the proof-of-concept stage, and hence, are unable to transition to at-scale production-level solutions. Furthermore, the unique challenges faced by AI4SI researchers are not always fully recognized within the broader AI community, where such work is sometimes viewed as primarily applied and not aligning with the traditional criteria for novelty emphasized in core AI venues. This paper attempts to shine a light on the diverse challenges faced in AI4SI research by diagnosing a multitude of factors that prevent AI4SI partnerships from achieving real-world impact on the ground. Drawing on semi-structured interviews with six leading AI4SI researchers - complemented by the authors' own lived experiences in conducting AI4SI research - this paper attempts to understand the day-to-day difficulties faced in developing and deploying socially impactful AI solutions. Through thematic analysis, we identify structural and organizational, communication, collaboration, and operational challenges as key barriers to deployment. While there are no easy fixes, we synthesize best practices and actionable strategies drawn from these interviews and our own work in this space. In doing so, we hope this paper serves as a practical reference guide for AI4SI researchers and partner organizations seeking to engage more effectively in socially impactful AI collaborations. 

**Abstract (ZH)**: 利用人工智能促进社会影响以应对联合国可持续发展目标的挑战与策略：识别并解决实际应用中的关键障碍 

---
# DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning 

**Title (ZH)**: DAVID-XR1: 可解释推理检测AI生成视频 

**Authors**: Yifeng Gao, Yifan Ding, Hongyu Su, Juncheng Li, Yunhan Zhao, Lin Luo, Zixing Chen, Li Wang, Xin Wang, Yixu Wang, Xingjun Ma, Yu-Gang Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2506.14827)  

**Abstract**: As AI-generated video becomes increasingly pervasive across media platforms, the ability to reliably distinguish synthetic content from authentic footage has become both urgent and essential. Existing approaches have primarily treated this challenge as a binary classification task, offering limited insight into where or why a model identifies a video as AI-generated. However, the core challenge extends beyond simply detecting subtle artifacts; it requires providing fine-grained, persuasive evidence that can convince auditors and end-users alike. To address this critical gap, we introduce DAVID-X, the first dataset to pair AI-generated videos with detailed defect-level, temporal-spatial annotations and written rationales. Leveraging these rich annotations, we present DAVID-XR1, a video-language model designed to deliver an interpretable chain of visual reasoning-including defect categorization, temporal-spatial localization, and natural language explanations. This approach fundamentally transforms AI-generated video detection from an opaque black-box decision into a transparent and verifiable diagnostic process. We demonstrate that a general-purpose backbone, fine-tuned on our compact dataset and enhanced with chain-of-thought distillation, achieves strong generalization across a variety of generators and generation modes. Our results highlight the promise of explainable detection methods for trustworthy identification of AI-generated video content. 

**Abstract (ZH)**: 随着AI生成视频在各类媒体平台上的广泛应用，可靠地区分合成内容与真实 footage的能力变得至关重要且不可或缺。现有的方法主要将这一挑战视为二元分类任务，提供的洞察有限，难以解释模型为何将视频识别为AI生成。然而，核心挑战远不止于检测细微的瑕疵，还要求提供精细、有说服力的证据，以使审计人员和最终用户信服。为填补这一重要空白，我们引入了DAVID-X，这是首个将AI生成视频与详细的缺陷级别、时空注释及书面解释相匹配的数据集。利用这些丰富的注释，我们提出了DAVID-XR1，一种视频-语言模型，旨在提供可解释的视觉推理链，包括缺陷分类、时空定位和自然语言解释。这种方法从根本上将AI生成视频的检测从不透明的黑盒决策转变为透明且可验证的诊断过程。我们证明，一个通用的骨干网络，在我们的紧凑数据集上微调，并结合链式思维提炼，能够跨越多种生成器和生成模式实现强大的泛化能力。我们的结果展示了可解释检测方法在可靠识别AI生成视频内容方面的前景。 

---
# Collaborative Interest-aware Graph Learning for Group Identification 

**Title (ZH)**: 协作式兴趣感知图学习在群体识别中的应用 

**Authors**: Rui Zhao, Beihong Jin, Beibei Li, Yiyuan Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2506.14826)  

**Abstract**: With the popularity of social media, an increasing number of users are joining group activities on online social platforms. This elicits the requirement of group identification (GI), which is to recommend groups to users. We reveal that users are influenced by both group-level and item-level interests, and these dual-level interests have a collaborative evolution relationship: joining a group expands the user's item interests, further prompting the user to join new groups. Ultimately, the two interests tend to align dynamically. However, existing GI methods fail to fully model this collaborative evolution relationship, ignoring the enhancement of group-level interests on item-level interests, and suffering from false-negative samples when aligning cross-level interests. In order to fully model the collaborative evolution relationship between dual-level user interests, we propose CI4GI, a Collaborative Interest-aware model for Group Identification. Specifically, we design an interest enhancement strategy that identifies additional interests of users from the items interacted with by the groups they have joined as a supplement to item-level interests. In addition, we adopt the distance between interest distributions of two users to optimize the identification of negative samples for a user, mitigating the interference of false-negative samples during cross-level interests alignment. The results of experiments on three real-world datasets demonstrate that CI4GI significantly outperforms state-of-the-art models. 

**Abstract (ZH)**: 基于协作兴趣的认知群体识别模型 CI4GI 

---
# GraphGSOcc: Semantic and Geometric Graph Transformer for 3D Gaussian Splating-based Occupancy Prediction 

**Title (ZH)**: GraphGSOcc: 具有语义和几何图变换器的3D高斯分裂基于占用预测 

**Authors**: Ke Song, Yunhe Wu, Chunchit Siu, Huiyuan Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2506.14825)  

**Abstract**: Addressing the task of 3D semantic occupancy prediction for autonomous driving, we tackle two key issues in existing 3D Gaussian Splating (3DGS) methods: (1) unified feature aggregation neglecting semantic correlations among similar categories and across regions, and (2) boundary ambiguities caused by the lack of geometric constraints in MLP iterative optimization. We propose the GraphGSOcc model, a novel framework that combines semantic and geometric graph Transformer for 3D Gaussian Splating-based Occupancy Prediction. We propose the Dual Gaussians Graph Attenntion, which dynamically constructs dual graph structures: a geometric graph adaptively calculating KNN search radii based on Gaussian poses, enabling large-scale Gaussians to aggregate features from broader neighborhoods while compact Gaussians focus on local geometric consistency; a semantic graph retaining top-M highly correlated nodes via cosine similarity to explicitly encode semantic relationships within and across instances. Coupled with the Multi-scale Graph Attention framework, fine-grained attention at lower layers optimizes boundary details, while coarse-grained attention at higher layers models object-level topology. Experiments on the SurroundOcc dataset achieve an mIoU of 24.10%, reducing GPU memory to 6.1 GB, demonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to GaussianWorld 

**Abstract (ZH)**: 基于3D高斯分裂的方法解决自主驾驶中的3D语义占有预测任务，我们提出了GraphGSOcc模型，该模型结合语义和几何图变换器以解决现有3D高斯分裂方法中的两个关键问题：统一特征聚合忽视了相似类别内的语义关联和跨区域的语义关联，以及由于MLP迭代优化中缺乏几何约束导致的边界模糊。 

---
# FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models 

**Title (ZH)**: FedNano: 朝向轻量级预训练多模态大型语言模型的联邦调优 

**Authors**: Yao Zhang, Hewei Gao, Haokun Chen, Weiguo Li, Yunpu Ma, Volker Tresp  

**Link**: [PDF](https://arxiv.org/pdf/2506.14824)  

**Abstract**: Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements. Federated Learning (FL) offers a solution by enabling collaborative model training without centralizing data. However, realizing FL for MLLMs presents significant challenges, including high computational demands, limited client capacity, substantial communication costs, and heterogeneous client data. Existing FL methods assume client-side deployment of full models, an assumption that breaks down for large-scale MLLMs due to their massive size and communication demands. To address these limitations, we propose FedNano, the first FL framework that centralizes the LLM on the server while introducing NanoEdge, a lightweight module for client-specific adaptation. NanoEdge employs modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation. This design eliminates the need to deploy LLM on clients, reducing client-side storage by 95%, and limiting communication overhead to only 0.01% of the model parameters. By transmitting only compact NanoAdapter updates, FedNano handles heterogeneous client data and resource constraints while preserving privacy. Experiments demonstrate that FedNano outperforms prior FL baselines, bridging the gap between MLLM scale and FL feasibility, and enabling scalable, decentralized multimodal AI systems. 

**Abstract (ZH)**: 面向大规模多模态语言模型的联邦学习框架FedNano：中央化LLM与轻量级NanoEdge模块相结合 

---
# ViLLa: A Neuro-Symbolic approach for Animal Monitoring 

**Title (ZH)**: ViLLa: 一种神经符号方法用于动物监测 

**Authors**: Harsha Koduri  

**Link**: [PDF](https://arxiv.org/pdf/2506.14823)  

**Abstract**: Monitoring animal populations in natural environments requires systems that can interpret both visual data and human language queries. This work introduces ViLLa (Vision-Language-Logic Approach), a neuro-symbolic framework designed for interpretable animal monitoring. ViLLa integrates three core components: a visual detection module for identifying animals and their spatial locations in images, a language parser for understanding natural language queries, and a symbolic reasoning layer that applies logic-based inference to answer those queries. Given an image and a question such as "How many dogs are in the scene?" or "Where is the buffalo?", the system grounds visual detections into symbolic facts and uses predefined rules to compute accurate answers related to count, presence, and location. Unlike end-to-end black-box models, ViLLa separates perception, understanding, and reasoning, offering modularity and transparency. The system was evaluated on a range of animal imagery tasks and demonstrates the ability to bridge visual content with structured, human-interpretable queries. 

**Abstract (ZH)**: 在自然环境中的动物种群监测需要能够解释视觉数据和人类语言查询的系统。本文介绍了ViLLa（视觉-语言-逻辑方法），这是一种用于可解释动物监测的神经符号框架。ViLLa 结合了三个核心组件：一个视觉检测模块，用于识别图像中的动物及其空间位置；一个语言解析器，用于理解自然语言查询；以及一个符号推理层，利用基于逻辑的推断来回答这些查询。给定一张图片和一个问题，比如“场景中有多少条狗？”或“水牛在哪里？”，该系统将视觉检测结果转化为符号事实，并使用预定义的规则来计算与数量、存在和位置相关的准确答案。与端到端的黑盒模型不同，ViLLa 将感知、理解和推理分离，提供了模块化和透明性。该系统在一系列动物图像任务上进行了评估，并展示了将视觉内容与结构化的人类可解释查询相结合的能力。 

---
# Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints 

**Title (ZH)**: 强化VLMs在资源约束下的工具使用能力以进行详细的视觉推理 

**Authors**: Sunil Kumar, Bowen Zhao, Leo Dirac, Paulina Varshavskaya  

**Link**: [PDF](https://arxiv.org/pdf/2506.14821)  

**Abstract**: Despite tremendous recent advances in large model reasoning ability, vision-language models (VLMs) still struggle with detailed visual reasoning, especially when compute resources are limited. To address this challenge, we draw inspiration from methods like Deepseek-r1 for VLMs and train smaller-scale models with Group Relative Policy Optimization (GRPO) to use external tools such as zoom. The greatest benefit is obtained with a combination of GRPO learning, a simple reward structure, a simplified tool-calling interface, allocating additional tokens to the result of the tool call, and a training data mix that over-represents visually difficult examples. Compared to similarly-sized baseline models, our method achieves better performance on some visual question-answering (VQA) tasks, thanks to the detailed visual information gathered from the external tool. 

**Abstract (ZH)**: 尽管大型模型的推理能力取得了巨大的近期进展，视觉-语言模型（VLMs）仍然在详细视觉推理方面存在问题，尤其是在计算资源有限的情况下。为了解决这一挑战，我们从Deepseek-r1等方法中汲取灵感，使用Group Relative Policy Optimization (GRPO) 训练较小规模的模型，并利用外部工具如放大镜。最大的收益来自于结合GRPO学习、简单的奖励结构、简化的工作调用接口、为工具调用的结果分配额外的标记以及过度代表视觉困难示例的训练数据混合。与具有相似规模的基础模型相比，我们的方法在某些视觉问答（VQA）任务上取得了更好的性能，这得益于从外部工具中获取的详细视觉信息。 

---
# Next-Generation Conflict Forecasting: Unleashing Predictive Patterns through Spatiotemporal Learning 

**Title (ZH)**: 下一代冲突预测：通过空间时间学习释放预测模式 

**Authors**: Simon P. von der Maase  

**Link**: [PDF](https://arxiv.org/pdf/2506.14817)  

**Abstract**: Forecasting violent conflict at high spatial and temporal resolution remains a central challenge for both researchers and policymakers. This study presents a novel neural network architecture for forecasting three distinct types of violence -- state-based, non-state, and one-sided -- at the subnational (priogrid-month) level, up to 36 months in advance. The model jointly performs classification and regression tasks, producing both probabilistic estimates and expected magnitudes of future events. It achieves state-of-the-art performance across all tasks and generates approximate predictive posterior distributions to quantify forecast uncertainty.
The architecture is built on a Monte Carlo Dropout Long Short-Term Memory (LSTM) U-Net, integrating convolutional layers to capture spatial dependencies with recurrent structures to model temporal dynamics. Unlike many existing approaches, it requires no manual feature engineering and relies solely on historical conflict data. This design enables the model to autonomously learn complex spatiotemporal patterns underlying violent conflict.
Beyond achieving state-of-the-art predictive performance, the model is also highly extensible: it can readily integrate additional data sources and jointly forecast auxiliary variables. These capabilities make it a promising tool for early warning systems, humanitarian response planning, and evidence-based peacebuilding initiatives. 

**Abstract (ZH)**: 高空间和时间分辨率下预测暴力冲突仍是一项重大挑战：一种新型神经网络架构及其在非国家、单方面和国家间暴力冲突预测中的应用及其不确定性量化 

---
# Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks 

**Title (ZH)**: 自信训练：通过自动化主动检查捕获深度学习训练中的隐形错误 

**Authors**: Yuxuan Jiang, Ziming Zhou, Boyu Xu, Beijie Liu, Runhui Xu, Peng Huang  

**Link**: [PDF](https://arxiv.org/pdf/2506.14813)  

**Abstract**: Training deep learning (DL) models is a complex process, making it prone to silent errors that are challenging to detect and diagnose. This paper presents TRAINCHECK, a framework that takes a proactive checking approach to address silent training errors. TRAINCHECK automatically infers invariants tailored for DL training. It uses these invariants to proactively detect silent errors during the training process while providing debugging help. To evaluate TRAINCHECK, we reproduce 20 real-world silent training errors with diverse root causes. TRAINCHECK successfully detects 18 errors within a single training iteration. It also uncovers 6 unknown bugs in popular training libraries that lead to silent errors. 

**Abstract (ZH)**: 训练深度学习模型是一个复杂的过程，容易产生难以检测和诊断的隐形错误。本文提出了一种名为TRAINCHECK的框架，采取主动检查的方法来应对隐形训练错误。TRAINCHECK自动推断适用于深度学习训练的不变式，并在训练过程中主动检测隐形错误，同时提供调试帮助。为了评估TRAINCHECK，我们重现了20个具有不同根本原因的实世界隐形训练错误。TRAINCHECK成功在单个训练迭代中检测到18个错误，并发现了6个在流行训练库中导致隐形错误的未知 bug。 

---
# Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes? 

**Title (ZH)**: Argus检测：多模态大型语言模型拥有Panoptes的眼睛吗？ 

**Authors**: Yang Yao, Lingyu Li, Jiaxin Song, Chiyu Chen, Zhenqi He, Yixu Wang, Xin Wang, Tianle Gu, Jie Li, Yan Teng, Yingchun Wang  

**Link**: [PDF](https://arxiv.org/pdf/2506.14805)  

**Abstract**: As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs. 

**Abstract (ZH)**: 随着多模态大型语言模型（MLLMs）的不断进化，它们的认知和推理能力取得了显著进步。然而，视觉细粒度感知和常识因果推理仍存在挑战。本文介绍了Argus Inspection这一包含两个难度级别的多模态基准，强调详细的视觉识别，同时融入现实世界的常识理解以评估因果推理能力。在此基础上，我们提出了Panoptes框架，该框架结合了二元参数化Sigmoid度量和指示函数，能够更全面地评估MLLMs在意见ベース推理任务中的响应。在对26种主流MLLMs进行的实验中，视觉细粒度推理的最高性能仅为0.46，表明存在显著的改进潜力。我们的研究为MLLMs的持续优化提供了有价值的观点。 

---
# ss-Mamba: Semantic-Spline Selective State-Space Model 

**Title (ZH)**: 基于语义样条的选择性状态空间模型:ss-Mamba 

**Authors**: Zuochen Ye  

**Link**: [PDF](https://arxiv.org/pdf/2506.14802)  

**Abstract**: We propose ss-Mamba, a novel foundation model that enhances time series forecasting by integrating semantic-aware embeddings and adaptive spline-based temporal encoding within a selective state-space modeling framework. Building upon the recent success of Transformer architectures, ss-Mamba adopts the Mamba selective state space model as an efficient alternative that achieves comparable performance while significantly reducing computational complexity from quadratic to linear time. Semantic index embeddings, initialized from pretrained language models, allow effective generalization to previously unseen series through meaningful semantic priors. Additionally, spline-based Kolmogorov-Arnold Networks (KAN) dynamically and interpretably capture complex seasonalities and non-stationary temporal effects, providing a powerful enhancement over conventional temporal feature encodings. Extensive experimental evaluations confirm that ss-Mamba delivers superior accuracy, robustness, and interpretability, demonstrating its capability as a versatile and computationally efficient alternative to traditional Transformer-based models in time-series forecasting. 

**Abstract (ZH)**: 我们提出ss-Mamba，一种新型基础模型，通过在选择性状态空间建模框架内整合语义意识嵌入和自适应样条时间编码来增强时间序列预测。基于Transformer架构的 recent 成功，ss-Mamba 采用 Mamba 选择性状态空间模型作为高效的替代方案，既能实现类似性能，又大大减少了从二次到线性的时间计算复杂度。预训练语言模型初始化的语义索引嵌入，通过有意义的语义先验有效泛化到以前未见过的时间序列。此外，基于样条的柯尔莫哥洛夫-阿诺尔德网络 (KAN) 动态且可解释地捕获复杂季节性和非 stationary 时间效果，提供了对传统时间特征编码的强大增强。大量实验证明，ss-Mamba 在准确性、鲁棒性和可解释性方面表现出优越性，证明了其作为传统基于Transformer的模型在时间序列预测中的一种灵活且计算高效的替代方案的能力。 

---
# Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust 

**Title (ZH)**: 使用多模态基础模型分析媒体内容中的人物表示：有效性与可信度 

**Authors**: Evdoxia Taka, Debadyuti Bhattacharya, Joanne Garde-Hansen, Sanjay Sharma, Tanaya Guha  

**Link**: [PDF](https://arxiv.org/pdf/2506.14799)  

**Abstract**: Recent advances in AI has enabled automated analysis of complex media content at scale and generate actionable insights regarding character representation along such dimensions as gender and age. Past work focused on quantifying representation from audio/video/text using various ML models, but without having the audience in the loop. We ask, even if character distribution along demographic dimensions are available, how useful are they to the general public? Do they actually trust the numbers generated by AI models? Our work addresses these questions through a user study, while proposing a new AI-based character representation and visualization tool. Our tool based on the Contrastive Language Image Pretraining (CLIP) foundation model to analyze visual screen data to quantify character representation across dimensions of age and gender. We also designed effective visualizations suitable for presenting such analytics to lay audience. Next, we conducted a user study to seek empirical evidence on the usefulness and trustworthiness of the AI-generated results for carefully chosen movies presented in the form of our visualizations. We note that participants were able to understand the analytics from our visualization, and deemed the tool `overall useful'. Participants also indicated a need for more detailed visualizations to include more demographic categories and contextual information of the characters. Participants' trust in AI-based gender and age models is seen to be moderate to low, although they were not against the use of AI in this context. Our tool including code, benchmarking, and data from the user study can be found here: this https URL 

**Abstract (ZH)**: 近期人工智能的进展使大规模自动化分析复杂媒体内容成为可能，并生成有关角色表示（如性别和年龄维度）的具体行动建议。以往研究侧重于使用各种机器学习模型量化角色表示，但未能将受众纳入其中。我们提出的问题是，即使有按人口统计维度的分布情况，这些信息对大众有多大用处？他们是否真正信任AI模型生成的数字？我们通过用户研究来回答这些问题，并提出了一个基于AI的角色表示和可视化新工具。该工具基于对比语言图像预训练（CLIP）基础模型，分析屏幕视觉数据，量化年龄和性别维度的角色表示。我们还设计了适合展示此类分析结果的有效可视化工具。随后，我们开展了一项用户研究，以通过我们的可视化形式展示的精心选择的电影为例，寻求AI生成结果的实际效用和可信度的实验证据。研究结果显示，参与者能够理解我们可视化中的分析，并认为该工具“总体上是有用的”。参与者还表示需要更详细的可视化，以包含更多的人口统计类别和角色的背景信息。尽管参与者对基于AI的性别和年龄模型的信任度适度至较低，但他们并不反对在这一背景下使用AI。有关该工具的代码、基准测试以及用户研究的数据，可在此链接找到：this https URL。 

---
# MODS: Multi-source Observations Conditional Diffusion Model for Meteorological State Downscaling 

**Title (ZH)**: MODS：多源观测条件扩散模型气象状态精细化模拟能力 

**Authors**: Siwei Tu, Jingyi Xu, Weidong Yang, Lei Bai, Ben Fei  

**Link**: [PDF](https://arxiv.org/pdf/2506.14798)  

**Abstract**: Accurate acquisition of high-resolution surface meteorological conditions is critical for forecasting and simulating meteorological variables. Directly applying spatial interpolation methods to derive meteorological values at specific locations from low-resolution grid fields often yields results that deviate significantly from the actual conditions. Existing downscaling methods primarily rely on the coupling relationship between geostationary satellites and ERA5 variables as a condition. However, using brightness temperature data from geostationary satellites alone fails to comprehensively capture all the changes in meteorological variables in ERA5 maps. To address this limitation, we can use a wider range of satellite data to make more full use of its inversion effects on various meteorological variables, thus producing more realistic results across different meteorological variables. To further improve the accuracy of downscaling meteorological variables at any location, we propose the Multi-source Observation Down-Scaling Model (MODS). It is a conditional diffusion model that fuses data from multiple geostationary satellites GridSat, polar-orbiting satellites (AMSU-A, HIRS, and MHS), and topographic data (GEBCO), as conditions, and is pre-trained on the ERA5 reanalysis dataset. During training, latent features from diverse conditional inputs are extracted separately and fused into ERA5 maps via a multi-source cross-attention module. By exploiting the inversion relationships between reanalysis data and multi-source atmospheric variables, MODS generates atmospheric states that align more closely with real-world conditions. During sampling, MODS enhances downscaling consistency by incorporating low-resolution ERA5 maps and station-level meteorological data as guidance. Experimental results demonstrate that MODS achieves higher fidelity when downscaling ERA5 maps to a 6.25 km resolution. 

**Abstract (ZH)**: 准确获取高分辨率地表气象条件对于气象变量的预报和模拟至关重要。直接将低分辨率网格场中的气象值通过空间插值方法推导至特定位置往往会导致与实际条件显著偏差的结果。现有的降尺度方法主要依赖于静止卫星和ERA5变量之间的耦合关系。然而，仅使用静止卫星的亮度温度数据无法全面捕捉ERA5地图中气象变量的所有变化。为解决这一局限性，可以使用更广泛的卫星数据来充分发挥其对各种气象变量的反演效果，从而在不同气象变量中生成更加符合实际情况的结果。为进一步提高在任何地点降尺度气象变量的准确性，我们提出了多源观测降尺度模型（MODS）。该模型是一个条件扩散模型，结合来自多个静止卫星（GridSat）、极轨卫星（AMSU-A、HIRS和MHS）以及地形数据（GEBCO）的条件输入数据，并在ERA5再分析数据集上预先训练。在训练过程中，多种条件输入的潜在特征被分别提取并通过多源交叉注意力模块融合到ERA5地图中。通过利用再分析数据和多源大气变量之间的反演关系，MODS生成的气象状态更贴近实际情况。在采样过程中，MODS通过纳入低分辨率ERA5地图和站级气象数据来增强降尺度一致性。实验结果表明，MODS在降尺度ERA5地图至6.25公里分辨率时获得了更高的保真度。 

---
# Bound by semanticity: universal laws governing the generalization-identification tradeoff 

**Title (ZH)**: 受语义性约束：泛化-识别权衡的基本规律 

**Authors**: Marco Nurisso, Jesseba Fernando, Raj Deshpande, Alan Perotti, Raja Marjieh, Steven M. Frankland, Richard L. Lewis, Taylor W. Webb, Declan Campbell, Francesco Vaccarino, Jonathan D. Cohen, Giovanni Petri  

**Link**: [PDF](https://arxiv.org/pdf/2506.14797)  

**Abstract**: Intelligent systems must deploy internal representations that are simultaneously structured -- to support broad generalization -- and selective -- to preserve input identity. We expose a fundamental limit on this tradeoff. For any model whose representational similarity between inputs decays with finite semantic resolution $\varepsilon$, we derive closed-form expressions that pin its probability of correct generalization $p_S$ and identification $p_I$ to a universal Pareto front independent of input space geometry. Extending the analysis to noisy, heterogeneous spaces and to $n>2$ inputs predicts a sharp $1/n$ collapse of multi-input processing capacity and a non-monotonic optimum for $p_S$. A minimal ReLU network trained end-to-end reproduces these laws: during learning a resolution boundary self-organizes and empirical $(p_S,p_I)$ trajectories closely follow theoretical curves for linearly decaying similarity. Finally, we demonstrate that the same limits persist in two markedly more complex settings -- a convolutional neural network and state-of-the-art vision-language models -- confirming that finite-resolution similarity is a fundamental emergent informational constraint, not merely a toy-model artifact. Together, these results provide an exact theory of the generalization-identification trade-off and clarify how semantic resolution shapes the representational capacity of deep networks and brains alike. 

**Abstract (ZH)**: 智能系统必须部署同时具备结构化以支持广泛泛化和选择性以保留输入身份的内部表示。我们揭示了这一权衡的基本限制。对于任何代表相似性随有限语义分辨率ε衰减的模型，我们推导出闭合形式的表达式，将其正确泛化概率$p_S$和识别概率$p_I$限制在一个与输入空间几何形状无关的万能帕累托前沿。将分析扩展到噪声和异质空间以及$n>2$个输入预测了多输入处理能力的锐利$1/n$崩溃以及$p_S$的非单调最优值。端到端训练的极小ReLU网络再现了这些定律：在学习过程中，一个分辨率边界自行组织，并且经验$(p_S,p_I)$轨迹紧密遵循线性衰减相似性理论曲线。最后，我们在两个更为复杂的环境中展示了相同限制的存在——卷积神经网络和最先进的视觉-语言模型，证实有限分辨率相似性是一种基本 emergent 信息系统约束，而不仅仅是玩具模型的特征。这些结果共同提供了泛化与识别权衡的确切理论，并阐明了语义分辨率如何塑造深度网络乃至大脑的表示能力。 

---
# PFMBench: Protein Foundation Model Benchmark 

**Title (ZH)**: PFMBench: 蛋白质基础模型基准 

**Authors**: Zhangyang Gao, Hao Wang, Cheng Tan, Chenrui Xu, Mengdi Liu, Bozhen Hu, Linlin Chao, Xiaoming Zhang, Stan Z. Li  

**Link**: [PDF](https://arxiv.org/pdf/2506.14796)  

**Abstract**: This study investigates the current landscape and future directions of protein foundation model research. While recent advancements have transformed protein science and engineering, the field lacks a comprehensive benchmark for fair evaluation and in-depth understanding. Since ESM-1B, numerous protein foundation models have emerged, each with unique datasets and methodologies. However, evaluations often focus on limited tasks tailored to specific models, hindering insights into broader generalization and limitations. Specifically, researchers struggle to understand the relationships between tasks, assess how well current models perform across them, and determine the criteria in developing new foundation models. To fill this gap, we present PFMBench, a comprehensive benchmark evaluating protein foundation models across 38 tasks spanning 8 key areas of protein science. Through hundreds of experiments on 17 state-of-the-art models across 38 tasks, PFMBench reveals the inherent correlations between tasks, identifies top-performing models, and provides a streamlined evaluation protocol. Code is available at \href{this https URL}{\textcolor{blue}{GitHub}}. 

**Abstract (ZH)**: 本研究探讨了蛋白质基础模型研究的当前状况和未来方向。尽管近期进展已改变蛋白质科学与工程的面貌，该领域仍缺乏一个全面的基准来公平评价和深入理解。自ESM-1B以来，涌现出了众多蛋白质基础模型，每个模型都有独特的数据集和方法。然而，评估往往集中在针对特定模型的特定任务上，阻碍了对更广泛泛化能力和限制的理解。具体而言，研究人员难以理解任务之间的关系，评估当前模型在这些任务上的表现情况，并确定开发新基础模型的标准。为了填补这一空白，我们提出了PFMBench基准，该基准涵盖38项任务，涉及蛋白质科学八大关键领域，对17个前沿模型进行了数百次实验，揭示了任务之间的固有关联，确定了表现最佳的模型，并提供了一套简化评估协议。代码可在GitHub（\href{this https URL}{GitHub}）获取。 

---
# Comparative Analysis of QNN Architectures for Wind Power Prediction: Feature Maps and Ansatz Configurations 

**Title (ZH)**: 基于量子神经网络架构的风功率预测比较分析：特征图和Ansatz配置 

**Authors**: Batuhan Hangun, Emine Akpinar, Oguz Altun, Onder Eyecioglu  

**Link**: [PDF](https://arxiv.org/pdf/2506.14795)  

**Abstract**: Quantum Machine Learning (QML) is an emerging field at the intersection of quantum computing and machine learning, aiming to enhance classical machine learning methods by leveraging quantum mechanics principles such as entanglement and superposition. However, skepticism persists regarding the practical advantages of QML, mainly due to the current limitations of noisy intermediate-scale quantum (NISQ) devices. This study addresses these concerns by extensively assessing Quantum Neural Networks (QNNs)-quantum-inspired counterparts of Artificial Neural Networks (ANNs), demonstrating their effectiveness compared to classical methods. We systematically construct and evaluate twelve distinct QNN configurations, utilizing two unique quantum feature maps combined with six different entanglement strategies for ansatz design. Experiments conducted on a wind energy dataset reveal that QNNs employing the Z feature map achieve up to 93% prediction accuracy when forecasting wind power output using only four input parameters. Our findings show that QNNs outperform classical methods in predictive tasks, underscoring the potential of QML in real-world applications. 

**Abstract (ZH)**: 量子机器学习（QML）是量子计算与机器学习交叉领域的新兴领域，旨在通过利用量子力学原理如纠缠和叠加来增强经典机器学习方法。然而，关于QML实用优势的疑虑仍然存在，主要原因是目前_noisy intermediate-scale quantum (NISQ)_设备的局限性。本研究通过广泛评估量子神经网络（QNNs）-受人工神经网络（ANNs）启发的量子对应物，展示了它们在预测任务中的有效性，相比经典方法有所提升。我们系统地构建和评估了十二种不同的QNN配置，结合了两种独特的量子特征映射和六种不同的纠缠策略用于Ansatz设计。在风能数据集上的实验结果显示，使用Z特征映射的QNN在仅使用四个输入参数预测风功率输出时，可以达到93%的预测精度。我们的研究发现表明，QNN在预测任务中优于经典方法，强调了量子机器学习在实际应用中的潜力。 

---
# Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors 

**Title (ZH)**: 专家组装：Chimera LLM变体的线性时间构建及其涌现和 adaptable 行为 

**Authors**: Henrik Klagges, Robert Dahlke, Fabian Klemm, Benjamin Merkel, Daniel Klingmann, David A. Reiss, Dan Zecha  

**Link**: [PDF](https://arxiv.org/pdf/2506.14794)  

**Abstract**: Requiring $10^{13}$-$10^{15}$ FLOPs to calculate one 8 bit weight in an LLM during pretraining is extremely expensive and seems inefficient. To better leverage the huge investments made into pretrained models, we develop the new "Assembly-of-Experts" (AoE) construction method to create capable child variants of existing Mixture-of-Experts parent models in linear time. Model weight tensors get interpolated individually, allowing to enhance or suppress semantic features of the parents.
Varying the proportion of weights taken from the parent models, we observe some properties of the AoE child model changing gradually, while other behavioral traits emerge with a sharp transition. Surprisingly, nearly every generated model is functional and capable, which makes searching the model space straightforward.
We construct the DeepSeek R1T "Chimera", a 671B open-weights hybrid model combining DeepSeek's V3-0324 and R1 model variants. The child inherits only the routed expert tensors of R1, but still achieves about R1-level intelligence. At the same time, it uses about 40\% fewer output tokens, close to V3 speed. Constructed without any fine-tuning or distillation, the Chimera exhibits surprisingly compact, orderly reasoning compared to its parent models. 

**Abstract (ZH)**: 要求在预训练过程中计算一个8位权重需要 \(10^{13}\)-\(10^{15}\) FLOPs，这极其昂贵且似乎不够高效。为了更好地利用对预训练模型的巨大投资，我们开发了一种新的“专家组装”（AoE）构造方法，能够在线性时间内创建现有Mixture-of-Experts父模型的有能力的子模型变体。模型权重张量独立插值，允许增强或抑制父模型的语义特征。

我们通过调整从父模型中获取的权重比例，观察到AoE子模型的一些属性会逐渐变化，而其他行为特征则会出现突然转变。令人惊讶的是，几乎每个生成的模型都是功能性和有能力的，这使得搜索模型空间变得简单直接。

我们构建了DeepSeek R1T“ chimera”，一个671B开源权重混合模型，结合了DeepSeek的V3-0324和R1模型变体。子模型仅继承R1的路由专家张量，但仍能达到R1级别的智能水平。同时，它使用大约40%更少的输出令牌，接近V3的速度。通过无需任何微调或蒸馏构建，chimera表现出与父模型相比令人惊讶的紧凑有序的推理能力。 

---
# Topology-Aware and Highly Generalizable Deep Reinforcement Learning for Efficient Retrieval in Multi-Deep Storage Systems 

**Title (ZH)**: 面向拓扑结构且高度泛化的深度强化学习在多级存储系统中高效检索 

**Authors**: Funing Li, Yuan Tian, Ruben Noortwyck, Jifeng Zhou, Liming Kuang, Robert Schulz  

**Link**: [PDF](https://arxiv.org/pdf/2506.14787)  

**Abstract**: In modern industrial and logistics environments, the rapid expansion of fast delivery services has heightened the demand for storage systems that combine high efficiency with increased density. Multi-deep autonomous vehicle storage and retrieval systems (AVS/RS) present a viable solution for achieving greater storage density. However, these systems encounter significant challenges during retrieval operations due to lane blockages. A conventional approach to mitigate this issue involves storing items with homogeneous characteristics in a single lane, but this strategy restricts the flexibility and adaptability of multi-deep storage systems.
In this study, we propose a deep reinforcement learning-based framework to address the retrieval problem in multi-deep storage systems with heterogeneous item configurations. Each item is associated with a specific due date, and the objective is to minimize total tardiness. To effectively capture the system's topology, we introduce a graph-based state representation that integrates both item attributes and the local topological structure of the multi-deep warehouse. To process this representation, we design a novel neural network architecture that combines a Graph Neural Network (GNN) with a Transformer model. The GNN encodes topological and item-specific information into embeddings for all directly accessible items, while the Transformer maps these embeddings into global priority assignments. The Transformer's strong generalization capability further allows our approach to be applied to storage systems with diverse layouts. Extensive numerical experiments, including comparisons with heuristic methods, demonstrate the superiority of the proposed neural network architecture and the effectiveness of the trained agent in optimizing retrieval tardiness. 

**Abstract (ZH)**: 基于深度强化学习的异构物品配置多深存储系统检索问题解决方案 

---
# PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series 

**Title (ZH)**: PIPE：结合物理信息的位置编码在卫星图像与时序数据对齐中的应用 

**Authors**: Haobo Li, Eunseo Jung, Zixin Chen, Zhaowei Wang, Yueya Wang, Huamin Qu, Alexis Kai Hon Lau  

**Link**: [PDF](https://arxiv.org/pdf/2506.14786)  

**Abstract**: Multimodal time series forecasting is foundational in various fields, such as utilizing satellite imagery and numerical data for predicting typhoons in climate science. However, existing multimodal approaches primarily focus on utilizing text data to help time series forecasting, leaving the visual data in existing time series datasets untouched. Furthermore, it is challenging for models to effectively capture the physical information embedded in visual data, such as satellite imagery's temporal and geospatial context, which extends beyond images themselves. To address this gap, we propose physics-informed positional encoding (PIPE), a lightweight method that embeds physical information into vision language models (VLMs). PIPE introduces two key innovations: (1) a physics-informed positional indexing scheme for mapping physics to positional IDs, and (2) a variant-frequency positional encoding mechanism for encoding frequency information of physical variables and sequential order of tokens within the embedding space. By preserving both the physical information and sequential order information, PIPE significantly improves multimodal alignment and forecasting accuracy. Through the experiments on the most representative and the largest open-sourced satellite image dataset, PIPE achieves state-of-the-art performance in both deep learning forecasting and climate domain methods, demonstrating superiority across benchmarks, including a 12% improvement in typhoon intensity forecasting over prior works. Our code is provided in the supplementary material. 

**Abstract (ZH)**: 多模态时间序列预测在利用卫星图像和数值数据预测台风等领域基础扎实，然而现有的多模态方法主要侧重于利用文本数据辅助时间序列预测，忽视了现有时间序列数据集中现有的视觉数据。同时，模型难以有效捕捉嵌入在视觉数据中的物理信息，如卫星图像的时间和地理空间上下文，这些信息超越了图像本身。为解决这一问题，我们提出了物理信息嵌入位置编码（PIPE），这是一种轻量级方法，将物理信息嵌入到视觉语言模型中。PIPE引入了两项关键创新：（1）一种物理信息导向的位置索引方案，用于将物理信息映射到位置ID，（2）一种变频位置编码机制，用于编码物理变量的频率信息以及嵌入空间中标记的顺序信息。通过保留物理信息和顺序信息，PIPE显著提高了多模态对齐和预测准确性。通过在最具代表性和最大的开源卫星图像数据集上进行实验，PIPE在深度学习预测和气候领域方法中均取得了最先进的性能，优于基准方法，台风强度预测性能提升了12%。我们在补充材料中提供了代码。 

---
# WebXAII: an open-source web framework to study human-XAI interaction 

**Title (ZH)**: WebXAII: 一个用于研究人类-XAI交互的开源网络框架 

**Authors**: Jules Leguy, Pierre-Antoine Jean, Felipe Torres Figueroa, Sébastien Harispe  

**Link**: [PDF](https://arxiv.org/pdf/2506.14777)  

**Abstract**: This article introduces WebXAII, an open-source web framework designed to facilitate research on human interaction with eXplainable Artificial Intelligence (XAI) systems. The field of XAI is rapidly expanding, driven by the growing societal implications of the widespread adoption of AI (and in particular machine learning) across diverse applications. Researchers who study the interaction between humans and XAI techniques typically develop ad hoc interfaces in order to conduct their studies. These interfaces are usually not shared alongside the results of the studies, which limits their reusability and the reproducibility of experiments. In response, we design and implement WebXAII, a web-based platform that can embody full experimental protocols, meaning that it can present all aspects of the experiment to human participants and record their responses. The experimental protocols are translated into a composite architecture of generic views and modules, which offers a lot of flexibility. The architecture is defined in a structured configuration file, so that protocols can be implemented with minimal programming skills. We demonstrate that WebXAII can effectively embody relevant protocols, by reproducing the protocol of a state-of-the-art study of the literature. The framework is available at this https URL. 

**Abstract (ZH)**: 本文介绍了WebXAII，一个开源框架，旨在促进对可解释人工智能(XAI)系统的人机交互研究。随着AI（特别是机器学习）在各类应用中广泛应用所带来的社会影响日益显著，XAI领域正在迅速扩展。研究人机交互的研究人员通常会开发临时界面来进行研究，但这些界面通常不会与研究结果一并分享，这限制了它们的可重用性和实验的可再现性。为此，我们设计并实现了WebXAII，这是一个基于Web的平台，能够涵盖完整的实验流程，即它可以向人类参与者呈现实验的所有方面并记录其反应。实验流程被翻译成一个由通用视图和模块组成的综合架构，提供了很大的灵活性。架构定义在一个结构化的配置文件中，使得协议可以用最少的编程技能来实现。我们通过重现文献中一项前沿研究的实验流程，证明了WebXAII的有效性。该框架可在以下链接获取：这是一个HTTPS链接。 

---
# See What I Mean? CUE: A Cognitive Model of Understanding Explanations 

**Title (ZH)**: 见我所言？CUE：一种理解解释的心智模型 

**Authors**: Tobias Labarta, Nhi Hoang, Katharina Weitz, Wojciech Samek, Sebastian Lapuschkin, Leander Weber  

**Link**: [PDF](https://arxiv.org/pdf/2506.14775)  

**Abstract**: As machine learning systems increasingly inform critical decisions, the need for human-understandable explanations grows. Current evaluations of Explainable AI (XAI) often prioritize technical fidelity over cognitive accessibility which critically affects users, in particular those with visual impairments. We propose CUE, a model for Cognitive Understanding of Explanations, linking explanation properties to cognitive sub-processes: legibility (perception), readability (comprehension), and interpretability (interpretation). In a study (N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we found comparable task performance but lower confidence/effort for visually impaired users. Unlike expected, these gaps were not mitigated and sometimes worsened by accessibility-focused color maps like Cividis. These results challenge assumptions about perceptual optimization and support the need for adaptive XAI interfaces. They also validate CUE by demonstrating that altering explanation legibility affects understandability. We contribute: (1) a formalized cognitive model for explanation understanding, (2) an integrated definition of human-centered explanation properties, and (3) empirical evidence motivating accessible, user-tailored XAI. 

**Abstract (ZH)**: 随着机器学习系统在越来越多的关键决策中发挥作用，对人类可理解解释的需要也在增长。当前对可解释人工智能（XAI）的评估往往优先考虑技术准确度而忽视了认知易用性，这严重影响了用户，特别是视觉障碍用户。我们提出了CUE模型，链接了解释属性与认知子过程：可读性（感知）、可读性（理解）和解释性（解释）。在一项包含455名受试者的研究中，我们发现不同色板（BWR、Cividis、Coolwarm）的热图在任务性能上表现出色，但视觉障碍用户的信心和努力程度较低。出乎意料的是，这些差距并未被聚焦于无障碍性的色板如Cividis减轻，有时甚至加剧。这些结果挑战了关于感知优化的假设，并支持需要适应性XAI界面。它们还通过实证证明改变了解释的可读性会影响理解性，从而验证了CUE。我们贡献了：（1）一种形式化的认知模型以解释理解，（2）一个联系人类中心解释属性的综合定义，以及（3）支持可访问性和用户定制的XAI的实验证据。 

---
# MedSyn: Enhancing Diagnostics with Human-AI Collaboration 

**Title (ZH)**: MedSyn: 人工智能与人类合作提升诊断性能 

**Authors**: Burcu Sayin, Ipek Baris Schlicht, Ngoc Vo Hong, Sara Allievi, Jacopo Staiano, Pasquale Minervini, Andrea Passerini  

**Link**: [PDF](https://arxiv.org/pdf/2506.14774)  

**Abstract**: Clinical decision-making is inherently complex, often influenced by cognitive biases, incomplete information, and case ambiguity. Large Language Models (LLMs) have shown promise as tools for supporting clinical decision-making, yet their typical one-shot or limited-interaction usage may overlook the complexities of real-world medical practice. In this work, we propose a hybrid human-AI framework, MedSyn, where physicians and LLMs engage in multi-step, interactive dialogues to refine diagnoses and treatment decisions. Unlike static decision-support tools, MedSyn enables dynamic exchanges, allowing physicians to challenge LLM suggestions while the LLM highlights alternative perspectives. Through simulated physician-LLM interactions, we assess the potential of open-source LLMs as physician assistants. Results show open-source LLMs are promising as physician assistants in the real world. Future work will involve real physician interactions to further validate MedSyn's usefulness in diagnostic accuracy and patient outcomes. 

**Abstract (ZH)**: 临床决策本质上是复杂的，常受认知偏差、信息不完整和病例模糊性的影响。大规模语言模型（LLMs）显示出了作为临床决策支持工具的潜力，但它们通常是一次性的或有限交互的使用方式可能忽略了现实医疗实践的复杂性。本研究提出了一种混合人机框架MedSyn，其中医生和LLMs通过多步骤的互动对话来 refinement 诊断和治疗决策。与其他静态的决策支持工具不同，MedSyn 允许动态的交流，使医生能够挑战LLM的建议，同时LLM突出其他视角。通过模拟医生-LLM交互，我们评估了开源LLM作为医生助手的潜力。结果显示，开源LLM在现实世界中作为医生助手是颇具潜力的。未来的工作将涉及真实医生的交互，进一步验证MedSyn在诊断准确性及患者结局方面的有效性。 

---
# MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition 

**Title (ZH)**: MixRep: 隐藏表示混叠方法在低资源语音识别中的应用 

**Authors**: Jiamin Xie, John H.L. Hansen  

**Link**: [PDF](https://arxiv.org/pdf/2310.18450)  

**Abstract**: In this paper, we present MixRep, a simple and effective data augmentation strategy based on mixup for low-resource ASR. MixRep interpolates the feature dimensions of hidden representations in the neural network that can be applied to both the acoustic feature input and the output of each layer, which generalizes the previous MixSpeech method. Further, we propose to combine the mixup with a regularization along the time axis of the input, which is shown as complementary. We apply MixRep to a Conformer encoder of an E2E LAS architecture trained with a joint CTC loss. We experiment on the WSJ dataset and subsets of the SWB dataset, covering reading and telephony conversational speech. Experimental results show that MixRep consistently outperforms other regularization methods for low-resource ASR. Compared to a strong SpecAugment baseline, MixRep achieves a +6.5\% and a +6.7\% relative WER reduction on the eval92 set and the Callhome part of the eval'2000 set. 

**Abstract (ZH)**: 基于mixup的MixRep：一种适用于低资源ASR的简单有效数据增强策略 

---
# Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model 

**Title (ZH)**: 动态ASR路径：面向多语言ASR模型高效剪枝的自适应掩蔽方法 

**Authors**: Jiamin Xie, Ke Li, Jinxi Guo, Andros Tjandra, Yuan Shangguan, Leda Sari, Chunyang Wu, Junteng Jia, Jay Mahadeokar, Ozlem Kalinli  

**Link**: [PDF](https://arxiv.org/pdf/2309.13018)  

**Abstract**: Neural network pruning offers an effective method for compressing a multilingual automatic speech recognition (ASR) model with minimal performance loss. However, it entails several rounds of pruning and re-training needed to be run for each language. In this work, we propose the use of an adaptive masking approach in two scenarios for pruning a multilingual ASR model efficiently, each resulting in sparse monolingual models or a sparse multilingual model (named as Dynamic ASR Pathways). Our approach dynamically adapts the sub-network, avoiding premature decisions about a fixed sub-network structure. We show that our approach outperforms existing pruning methods when targeting sparse monolingual models. Further, we illustrate that Dynamic ASR Pathways jointly discovers and trains better sub-networks (pathways) of a single multilingual model by adapting from different sub-network initializations, thereby reducing the need for language-specific pruning. 

**Abstract (ZH)**: 神经网络剪枝提供了一种有效的方法来压缩多语言自动语音识别（ASR）模型，同时将性能损失降至最低。然而，这需要为每种语言运行多轮剪枝和重新训练。在本文中，我们提出了一种自适应掩码方法，在两种场景下高效地修剪多语言ASR模型，分别生成稀疏的单语言模型或稀疏的多语言模型（名为动态ASR路径）。我们的方法动态地适应子网络，避免了过早决定固定子网络结构。我们显示，当目标是稀疏的单语言模型时，我们的方法优于现有的剪枝方法。进一步地，我们展示了动态ASR路径通过从不同的子网络初始化进行适应，共同发现和训练单多语言模型的更好子网络（路径），从而减少了语言特定剪枝的需要。 

---
# DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition 

**Title (ZH)**: DEFORMER: 结合变形局部模式与全局上下文以实现稳健的端到端语音识别 

**Authors**: Jiamin Xie, John H.L. Hansen  

**Link**: [PDF](https://arxiv.org/pdf/2207.01732)  

**Abstract**: Convolutional neural networks (CNN) have improved speech recognition performance greatly by exploiting localized time-frequency patterns. But these patterns are assumed to appear in symmetric and rigid kernels by the conventional CNN operation. It motivates the question: What about asymmetric kernels? In this study, we illustrate adaptive views can discover local features which couple better with attention than fixed views of the input. We replace depthwise CNNs in the Conformer architecture with a deformable counterpart, dubbed this "Deformer". By analyzing our best-performing model, we visualize both local receptive fields and global attention maps learned by the Deformer and show increased feature associations on the utterance level. The statistical analysis of learned kernel offsets provides an insight into the change of information in features with the network depth. Finally, replacing only half of the layers in the encoder, the Deformer improves +5.6% relative WER without a LM and +6.4% relative WER with a LM over the Conformer baseline on the WSJ eval92 set. 

**Abstract (ZH)**: 卷积神经网络通过利用局部时频模式大幅提高了语音识别性能，但这些模式假定以传统的卷积操作对称和刚性的核出现。这引发了这样的问题：非对称核会怎样？在此研究中，我们展示了自适应视角可以发现与注意力耦合更好的局部特征，优于固定视角。我们将Conformer架构中的深度卷积神经网络替换为一个可变形的对应物，称为“Deformer”。通过分析表现最佳的模型，我们可视化了Deformer学习到的局部感受野和全局注意力图，并展示了在短语水平上特征关联的增强。学习到的核偏移的统计分析揭示了网络深度对特征中信息变化的洞察。最后，在编码器中仅替换一半层，Deformer在WSJ eval92集上相对于Conformer基线，无语言模型获得了相对WER提高5.6%，有语言模型提高了6.4%。 

---
