{'arxiv_id': 'arXiv:2510.12733', 'title': 'HYPE: Hybrid Planning with Ego Proposal-Conditioned Predictions', 'authors': 'Hang Yu, Julian Jordan, Julian Schmidt, Silvan Lindner, Alessandro Canevaro, Wilhelm Stork', 'link': 'https://arxiv.org/abs/2510.12733', 'abstract': 'Safe and interpretable motion planning in complex urban environments needs to reason about bidirectional multi-agent interactions. This reasoning requires to estimate the costs of potential ego driving maneuvers. Many existing planners generate initial trajectories with sampling-based methods and refine them by optimizing on learned predictions of future environment states, which requires a cost function that encodes the desired vehicle behavior. Designing such a cost function can be very challenging, especially if a wide range of complex urban scenarios has to be considered. We propose HYPE: HYbrid Planning with Ego proposal-conditioned predictions, a planner that integrates multimodal trajectory proposals from a learned proposal model as heuristic priors into a Monte Carlo Tree Search (MCTS) refinement. To model bidirectional interactions, we introduce an ego-conditioned occupancy prediction model, enabling consistent, scene-aware reasoning. Our design significantly simplifies cost function design in refinement by considering proposal-driven guidance, requiring only minimalistic grid-based cost terms. Evaluations on large-scale real-world benchmarks nuPlan and DeepUrban show that HYPE effectively achieves state-of-the-art performance, especially in safety and adaptability.', 'abstract_zh': '混合规划与基于自我提案条件的预测在复杂城市环境中的安全可解释运动规划需要考虑双向多Agent交互。通过引入基于自我提案的占用率预测模型，HYPE: 混合规划与基于自我提案条件的预测在蒙特卡洛树搜索（MCTS） refinement中整合多模式轨迹提案作为启发式先验，以建模仿真双向交互并实现一致和场景感知推理。HYPE的设计在refinement中通过考虑提案驱动的指导来显著简化成本函数设计，仅需最小化的基于网格的成本项。在大规模的现实世界基准nuPlan和DeepUrban上的评估表明，HYPE在安全性及适应性方面达到了最先进的性能。', 'title_zh': 'HYPE: 基于ego提案条件预测的混合规划'}
{'arxiv_id': 'arXiv:2510.12174', 'title': 'UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering', 'authors': 'Yusen Xie, Zhenmin Huang, Jianhao Jiao, Dimitrios Kanoulas, Jun Ma', 'link': 'https://arxiv.org/abs/2510.12174', 'abstract': 'In this paper, we propose UniGS, a unified map representation and differentiable framework for high-fidelity multimodal 3D reconstruction based on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated rasterization pipeline capable of rendering photo-realistic RGB images, geometrically accurate depth maps, consistent surface normals, and semantic logits simultaneously. We redesign the rasterization to render depth via differentiable ray-ellipsoid intersection rather than using Gaussian centers, enabling effective optimization of rotation and scale attribute through analytic depth gradients. Furthermore, we derive the analytic gradient formulation for surface normal rendering, ensuring geometric consistency among reconstructed 3D scenes. To improve computational and storage efficiency, we introduce a learnable attribute that enables differentiable pruning of Gaussians with minimal contribution during training. Quantitative and qualitative experiments demonstrate state-of-the-art reconstruction accuracy across all modalities, validating the efficacy of our geometry-aware paradigm. Source code and multimodal viewer will be available on GitHub.', 'abstract_zh': '在本文中，我们提出了一种基于3D高斯斑点的统一地图表示和可微框架UniGS，用于高保真多模态3D重建。我们的框架集成了一种CUDA加速的渲染管道，能够同时渲染照片级真实的RGB图像、几何精确的深度图、一致的法线以及语义logits。我们重新设计了渲染管道，通过可微的射线-椭球相交来渲染深度，而不是使用高斯中心，从而能够通过解析深度梯度有效地优化旋转和尺度属性。此外，我们推导了表面法线渲染的解析梯度公式，以确保重建3D场景之间的几何一致性。为了提高计算和存储效率，我们引入了一个可学习的属性，能够在训练过程中最小化高斯分布的可微剪枝。定量和定性实验表明，我们的方法在所有模态下的重建精度达到了最先进的水平，验证了我们几何感知范式的有效性。源代码和多模态查看器将在GitHub上提供。', 'title_zh': 'UniGS：统一的几何感知高斯点云多模态渲染'}
{'arxiv_id': 'arXiv:2510.12785', 'title': 'MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars', 'authors': 'Felix Taubner, Ruihang Zhang, Mathieu Tuli, Sherwin Bahmani, David B. Lindell', 'link': 'https://arxiv.org/abs/2510.12785', 'abstract': 'Digital human avatars aim to simulate the dynamic appearance of humans in virtual environments, enabling immersive experiences across gaming, film, virtual reality, and more. However, the conventional process for creating and animating photorealistic human avatars is expensive and time-consuming, requiring large camera capture rigs and significant manual effort from professional 3D artists. With the advent of capable image and video generation models, recent methods enable automatic rendering of realistic animated avatars from a single casually captured reference image of a target subject. While these techniques significantly lower barriers to avatar creation and offer compelling realism, they lack constraints provided by multi-view information or an explicit 3D representation. So, image quality and realism degrade when rendered from viewpoints that deviate strongly from the reference image. Here, we build a video model that generates animatable multi-view videos of digital humans based on a single reference image and target expressions. Our model, MVP4D, is based on a state-of-the-art pre-trained video diffusion model and generates hundreds of frames simultaneously from viewpoints varying by up to 360 degrees around a target subject. We show how to distill the outputs of this model into a 4D avatar that can be rendered in real-time. Our approach significantly improves the realism, temporal consistency, and 3D consistency of generated avatars compared to previous methods.', 'abstract_zh': '基于单张参考图像和目标表情生成可动画化多视角视频的数字人类视频模型', 'title_zh': 'MVP4D：多视角肖像视频扩散生成可动画化的4Davatar'}
{'arxiv_id': 'arXiv:2510.12768', 'title': 'Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction', 'authors': 'Fengzhi Guo, Chih-Chuan Hsu, Sihao Ding, Cheng Zhang', 'link': 'https://arxiv.org/abs/2510.12768', 'abstract': 'Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our key insight is to estimate time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints.', 'abstract_zh': '基于单目输入重构动态3D场景从根本上说是欠约束的， occlusion和极端新颖视角会导致歧义。尽管动态Gaussian Splatting 提供了高效的表示，但vanilla模型会均匀优化所有Gaussian原语，忽视它们的观测质量。这种限制会导致遮挡下的运动漂移以及在预测未见视角时的合成退化。我们认为不确定性很重要：在不同视角和时间上反复出现观测的Gaussian充当可靠的锚点以引导运动，而那些观测有限的Gaussian则被认为是不太可靠的。为此，我们提出了USplat4D，一种新的不确定性感知动态Gaussian Splatting框架，通过传播可靠的运动线索来增强4D重构。我们的核心见解是估计每Gaussian的时间变化不确定性，并利用它构建时空图以进行不确定性感知优化。在多种真实和合成数据集上的实验表明，明确建模不确定性可以一致地改善动态Gaussian Splatting模型，在遮挡下的几何结构更稳定，并且在极端视角下合成质量更高。', 'title_zh': '不确定性在单目4D重建的动态高斯散列中的重要性'}
{'arxiv_id': 'arXiv:2510.12704', 'title': 'Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray Diagnosis', 'authors': 'Shelley Zixin Shu, Haozhe Luo, Alexander Poellinger, Mauricio Reyes', 'link': 'https://arxiv.org/abs/2510.12704', 'abstract': 'Transformer-based deep learning models have demonstrated exceptional performance in medical imaging by leveraging attention mechanisms for feature representation and interpretability. However, these models are prone to learning spurious correlations, leading to biases and limited generalization. While human-AI attention alignment can mitigate these issues, it often depends on costly manual supervision. In this work, we propose a Hybrid Explanation-Guided Learning (H-EGL) framework that combines self-supervised and human-guided constraints to enhance attention alignment and improve generalization. The self-supervised component of H-EGL leverages class-distinctive attention without relying on restrictive priors, promoting robustness and flexibility. We validate our approach on chest X-ray classification using the Vision Transformer (ViT), where H-EGL outperforms two state-of-the-art Explanation-Guided Learning (EGL) methods, demonstrating superior classification accuracy and generalization capability. Additionally, it produces attention maps that are better aligned with human expertise.', 'abstract_zh': '基于Transformer的深度学习模型通过利用注意力机制在医学影像领域展示出了卓越的性能，但这些模型容易学习到伪相关性，导致偏差和泛化能力有限。尽管人类-AI注意力对齐可以缓解这些问题，但通常依赖于昂贵的人工监督。在此项工作中，我们提出了一种混合解释引导学习（H-EGL）框架，该框架结合了自我监督和人工引导的约束，以增强注意力对齐并提高泛化能力。H-EGL的自我监督组件利用类特异性注意力，不依赖于严格的先验知识，从而增强鲁棒性和灵活性。我们使用Vision Transformer（ViT）在胸部X光分类任务上验证了该方法，H-EGL在分类准确性和泛化能力上超越了两种最先进的解释引导学习（EGL）方法，并生成了更好地与人类专业知识对齐的注意力图。', 'title_zh': '基于Transformer的胸部X光诊断的混合解释引导学习'}
{'arxiv_id': 'arXiv:2510.12537', 'title': 'Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion', 'authors': 'David Björkstrand, Tiesheng Wang, Lars Bretzner, Josephine Sullivan', 'link': 'https://arxiv.org/abs/2510.12537', 'abstract': 'Recent work has explored a range of model families for human motion generation, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and diffusion-based models. Despite their differences, many methods rely on over-parameterized input features and auxiliary losses to improve empirical results. These strategies should not be strictly necessary for diffusion models to match the human motion distribution. We show that on par with state-of-the-art results in unconditional human motion generation are achievable with a score-based diffusion model using only careful feature-space normalization and analytically derived weightings for the standard L2 score-matching loss, while generating both motion and shape directly, thereby avoiding slow post hoc shape recovery from joints. We build the method step by step, with a clear theoretical motivation for each component, and provide targeted ablations demonstrating the effectiveness of each proposed addition in isolation.', 'abstract_zh': '最近的研究已经探索了多种人类运动生成的模型家族，包括变分自编码器（VAEs）、生成对抗网络（GANs）和基于扩散的模型。尽管这些方法在输入特征和辅助损失上有所不同，许多方法仍然依赖于过度参数化的输入特征和辅助损失来提高实验结果。这些策略并非必须适用于达到与最先进的无条件人类运动生成结果相当的结果。我们证明，仅通过仔细的空间归一化和标准L2评分匹配损失的分析权重，即可实现与最先进的结果相当的无条件人类运动生成结果，同时直接生成运动和形状，从而避免了从关节后处理形状恢复的缓慢过程。我们逐步构建该方法，并为每个组件提供清晰的理论动机，同时通过针对性的消融实验展示了每个提出的添加项的有效性。', 'title_zh': '无条件的人运动态和形状生成通过平衡的分数基于扩散模型'}
{'arxiv_id': 'arXiv:2510.12408', 'title': 'Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model', 'authors': 'Huu Tien Nguyen, Ahmed Karam Eldaly', 'link': 'https://arxiv.org/abs/2510.12408', 'abstract': 'This paper introduces a novel framework for image quality transfer based on conditional flow matching (CFM). Unlike conventional generative models that rely on iterative sampling or adversarial objectives, CFM learns a continuous flow between a noise distribution and target data distributions through the direct regression of an optimal velocity field. We evaluate this approach in the context of low-field magnetic resonance imaging (LF-MRI), a rapidly emerging modality that offers affordable and portable scanning but suffers from inherently low signal-to-noise ratio and reduced diagnostic quality. Our framework is designed to reconstruct high-field-like MR images from their corresponding low-field inputs, thereby bridging the quality gap without requiring expensive infrastructure. Experiments demonstrate that CFM not only achieves state-of-the-art performance, but also generalizes robustly to both in-distribution and out-of-distribution data. Importantly, it does so while utilizing significantly fewer parameters than competing deep learning methods. These results underline the potential of CFM as a powerful and scalable tool for MRI reconstruction, particularly in resource-limited clinical environments.', 'abstract_zh': '基于条件流匹配的图像质量转移新型框架', 'title_zh': '低场磁共振图像质量增强的条件流匹配模型'}
{'arxiv_id': 'arXiv:2510.12265', 'title': 'Human-in-the-Loop Bandwidth Estimation for Quality of Experience Optimization in Real-Time Video Communication', 'authors': 'Sami Khairy, Gabriel Mittag, Vishak Gopal, Ross Cutler', 'link': 'https://arxiv.org/abs/2510.12265', 'abstract': 'The quality of experience (QoE) delivered by video conferencing systems is significantly influenced by accurately estimating the time-varying available bandwidth between the sender and receiver. Bandwidth estimation for real-time communications remains an open challenge due to rapidly evolving network architectures, increasingly complex protocol stacks, and the difficulty of defining QoE metrics that reliably improve user experience. In this work, we propose a deployed, human-in-the-loop, data-driven framework for bandwidth estimation to address these challenges. Our approach begins with training objective QoE reward models derived from subjective user evaluations to measure audio and video quality in real-time video conferencing systems. Subsequently, we collect roughly $1$M network traces with objective QoE rewards from real-world Microsoft Teams calls to curate a bandwidth estimation training dataset. We then introduce a novel distributional offline reinforcement learning (RL) algorithm to train a neural-network-based bandwidth estimator aimed at improving QoE for users. Our real-world A/B test demonstrates that the proposed approach reduces the subjective poor call ratio by $11.41\\%$ compared to the baseline bandwidth estimator. Furthermore, the proposed offline RL algorithm is benchmarked on D4RL tasks to demonstrate its generalization beyond bandwidth estimation.', 'abstract_zh': '视频会议系统中提供的体验质量（QoE）受到准确估计发送者和接收者之间时变可用带宽的影响。由于网络架构的快速演变、日益复杂的协议栈以及定义能够可靠提高用户体验的QoE指标的难度，实时通信中的带宽估计仍然是一个开放的挑战。在此工作中，我们提出了一种部署的、包含人类反馈的数据驱动框架，以应对这些挑战。我们的方法首先通过对主观用户评估进行客观QoE奖励模型的训练，来测量实时视频会议系统中的音频和视频质量。随后，我们从实际的Microsoft Teams通话中收集了约100万条网络轨迹，其中包含客观QoE奖励，以构建带宽估计训练数据集。我们进而引入了一种新颖的分布式离线强化学习（RL）算法，以训练基于神经网络的带宽估计器，旨在提高用户体验。我们的真实世界A/B测试表明，所提出的方法将主观差通话比率降低了11.41%。此外，我们还将所提出的离线RL算法在D4RL任务上进行基准测试，以展示其超越带宽估计的泛化能力。', 'title_zh': '基于人类在环带宽估算的实时视频通信体验质量优化'}
{'arxiv_id': 'arXiv:2510.12075', 'title': 'A Review on Domain Adaption and Generative Adversarial Networks(GANs)', 'authors': 'Aashish Dhawan, Divyanshu Mudgal', 'link': 'https://arxiv.org/abs/2510.12075', 'abstract': "The major challenge in today's computer vision scenario is the availability of good quality labeled data. In a field of study like image classification, where data is of utmost importance, we need to find more reliable methods which can overcome the scarcity of data to produce results comparable to previous benchmark results. In most cases, obtaining labeled data is very difficult because of the high cost of human labor and in some cases impossible. The purpose of this paper is to discuss Domain Adaptation and various methods to implement it. The main idea is to use a model trained on a particular dataset to predict on data from a different domain of the same kind, for example - a model trained on paintings of airplanes predicting on real images of airplanes", 'abstract_zh': '当今计算机视觉领域的主要挑战是高质量标注数据的获取。在诸如图像分类这样的领域，数据至关重要，因此我们需要找到更可靠的方法，克服数据稀缺性，以产生可与以往基准结果相媲美的结果。在大多数情况下，获得标注数据非常困难，原因在于人力成本高，而在某些情况下则完全不可能。本文旨在讨论领域适应及其各种实现方法。主要思想是利用在特定数据集上训练的模型，对同一类别的不同领域数据进行预测，例如，一个在飞机绘画上训练的模型预测真实飞机图像。', 'title_zh': '_domain适应与生成对抗网络（GANs）综述_'}
{'arxiv_id': 'arXiv:2510.11992', 'title': 'PanoTPS-Net: Panoramic Room Layout Estimation via Thin Plate Spline Transformation', 'authors': 'Hatem Ibrahem, Ahmed Salem, Qinmin Vivian Hu, Guanghui Wang', 'link': 'https://arxiv.org/abs/2510.11992', 'abstract': "Accurately estimating the 3D layout of rooms is a crucial task in computer vision, with potential applications in robotics, augmented reality, and interior design. This paper proposes a novel model, PanoTPS-Net, to estimate room layout from a single panorama image. Leveraging a Convolutional Neural Network (CNN) and incorporating a Thin Plate Spline (TPS) spatial transformation, the architecture of PanoTPS-Net is divided into two stages: First, a convolutional neural network extracts the high-level features from the input images, allowing the network to learn the spatial parameters of the TPS transformation. Second, the TPS spatial transformation layer is generated to warp a reference layout to the required layout based on the predicted parameters. This unique combination empowers the model to properly predict room layouts while also generalizing effectively to both cuboid and non-cuboid layouts. Extensive experiments on publicly available datasets and comparisons with state-of-the-art methods demonstrate the effectiveness of the proposed method. The results underscore the model's accuracy in room layout estimation and emphasize the compatibility between the TPS transformation and panorama images. The robustness of the model in handling both cuboid and non-cuboid room layout estimation is evident with a 3DIoU value of 85.49, 86.16, 81.76, and 91.98 on PanoContext, Stanford-2D3D, Matterport3DLayout, and ZInD datasets, respectively. The source code is available at: this https URL.", 'abstract_zh': '准确估计房间的3D布局是计算机视觉中的一个关键任务，潜在的应用包括机器人技术、增强现实和室内设计。本文提出了一种新型模型PanoTPS-Net，用于从单张全景图像估计房间布局。该模型结合卷积神经网络（CNN）和薄板样条（TPS）空间变换，其架构分为两个阶段：首先，卷积神经网络从输入图像中提取高级特征，使网络学习TPS变换的空间参数。其次，根据预测的参数生成TPS空间变换层，将参考布局变换为所需布局。这种独特的组合使模型能够正确预测房间布局，并且能够有效地泛化到规则和不规则布局。在公开数据集上的广泛实验和与最先进的方法的比较表明所提出方法的有效性。实验结果强调了该模型在房间布局估计中的准确性，并突出了TPS变换与全景图像之间的兼容性。模型在PanoContext、Stanford-2D3D、Matterport3DLayout和ZInD数据集上的鲁棒性估计分别达到3DIoU值85.49、86.16、81.76和91.98。源代码可在以下链接获取：this https URL。', 'title_zh': 'PanoTPS-Net：基于薄板样条变换的全景房间布局估计'}
{'arxiv_id': 'arXiv:2510.11883', 'title': 'MammoDINO: Anatomically Aware Self-Supervision for Mammographic Images', 'authors': 'Sicheng Zhou, Lei Wu, Cao Xiao, Parminder Bhatia, Taha Kass-Hout', 'link': 'https://arxiv.org/abs/2510.11883', 'abstract': "Self-supervised learning (SSL) has transformed vision encoder training in general domains but remains underutilized in medical imaging due to limited data and domain specific biases. We present MammoDINO, a novel SSL framework for mammography, pretrained on 1.4 million mammographic images. To capture clinically meaningful features, we introduce a breast tissue aware data augmentation sampler for both image-level and patch-level supervision and a cross-slice contrastive learning objective that leverages 3D digital breast tomosynthesis (DBT) structure into 2D pretraining. MammoDINO achieves state-of-the-art performance on multiple breast cancer screening tasks and generalizes well across five benchmark datasets. It offers a scalable, annotation-free foundation for multipurpose computer-aided diagnosis (CAD) tools for mammogram, helping reduce radiologists' workload and improve diagnostic efficiency in breast cancer screening.", 'abstract_zh': '自助监督学习（SSL）已在一般领域中transformed视觉编码器训练，但在因数据有限和领域特定偏见而未能充分利用的医疗成像领域中，仍有很大的应用潜力。我们提出了MammoDINO，一种新型的自助监督学习框架，基于140万张乳腺X线图像进行预训练。为了捕捉临床有意义的特征，我们引入了一种乳腺组织感知的数据增强采样器，适用于图像级和patches级监督，并提出了一种跨切片对比学习目标，利用3D数字乳腺断层合成（DBT）结构进行二维预训练。MammoDINO在多个乳腺癌筛查任务中达到了最先进的性能，并在五个基准数据集中表现出良好的泛化能力。它为乳腺X线图像的多功能计算机辅助诊断（CAD）工具提供了一个可扩展且无需标注的基础，有助于减轻放射科医生的工作负担并提高乳腺癌筛查的诊断效率。', 'title_zh': 'MammoDINO：解剖结构意识的自监督学习方法在 mammographic 图像中的应用'}
