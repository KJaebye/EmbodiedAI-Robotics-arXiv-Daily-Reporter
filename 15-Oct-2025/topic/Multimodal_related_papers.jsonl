{'arxiv_id': 'arXiv:2510.12789', 'title': 'UniFusion: Vision-Language Model as Unified Encoder in Image Generation', 'authors': 'Kevin Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, Ajinkya Kale', 'link': 'https://arxiv.org/abs/2510.12789', 'abstract': "Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large unified models jointly for text and image generation, which demands substantial computational resources and large-scale data, limiting its this http URL present UniFusion, a diffusion-based generative model conditioned on a frozen large vision-language model (VLM) that serves as a unified multimodal encoder. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high level semantics and low level details from text and visual tokens of a frozen VLM to condition a diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithful transfer of visual information from VLM to the diffusion model which is key for editing. We propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI), which conditions a diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. VERIFI combines the alignment of the conditioning distribution with the VLM's reasoning capabilities for increased capabilities and flexibility at inference. In addition, finetuning on editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities. Our model when trained on single image editing, zero-shot generalizes to multiple image references further motivating the unified encoder design of UniFusion.", 'abstract_zh': 'UniFusion：基于扩散的条件生成模型，-conditioned on 冻结的大型Vision-Language模型', 'title_zh': 'UniFusion: 视觉-语言模型作为图像生成的统一编码器'}
{'arxiv_id': 'arXiv:2510.12603', 'title': 'Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space', 'authors': 'Chao Chen, Zhixin Ma, Yongqi Li, Yupeng Hu, Yinwei Wei, Wenjie Li, Liqiang Nie', 'link': 'https://arxiv.org/abs/2510.12603', 'abstract': 'Multimodal reasoning aims to enhance the capabilities of MLLMs by incorporating intermediate reasoning steps before reaching the final answer. It has evolved from text-only reasoning to the integration of visual information, enabling the thought process to be conveyed through both images and text. Despite its effectiveness, current multimodal reasoning methods depend on explicit reasoning steps that require labor-intensive vision-text annotations and inherently introduce significant inference latency. To address these issues, we introduce multimodal latent reasoning with the advantages of multimodal representation, reduced annotation, and inference efficiency. To facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR), which injects both visual and textual information in the reasoning process within the latent space. Specifically, IVT-LR represents each reasoning step by combining two implicit parts: latent text (the hidden states from the previous step) and latent vision (a set of selected image embeddings). We further introduce a progressive multi-stage training strategy to enable MLLMs to perform the above multimodal latent reasoning steps. Experiments on M3CoT and ScienceQA demonstrate that our IVT-LR method achieves an average performance increase of 5.45% in accuracy, while simultaneously achieving a speed increase of over 5 times compared to existing approaches. Code available at this https URL.', 'abstract_zh': '多模态潜推理：多模态表示、标注减少及推理效率的优势', 'title_zh': '在黑暗中推理：潜在空间中的交互式视觉-文本推理'}
{'arxiv_id': 'arXiv:2510.12482', 'title': 'A Text-Image Fusion Method with Data Augmentation Capabilities for Referring Medical Image Segmentation', 'authors': 'Shurong Chai, Rahul Kumar JAIN, Rui Xu, Shaocong Mo, Ruibo Hou, Shiyu Teng, Jiaqing Liu, Lanfen Lin, Yen-Wei Chen', 'link': 'https://arxiv.org/abs/2510.12482', 'abstract': 'Deep learning relies heavily on data augmentation to mitigate limited data, especially in medical imaging. Recent multimodal learning integrates text and images for segmentation, known as referring or text-guided image segmentation. However, common augmentations like rotation and flipping disrupt spatial alignment between image and text, weakening performance. To address this, we propose an early fusion framework that combines text and visual features before augmentation, preserving spatial consistency. We also design a lightweight generator that projects text embeddings into visual space, bridging semantic gaps. Visualization of generated pseudo-images shows accurate region localization. Our method is evaluated on three medical imaging tasks and four segmentation frameworks, achieving state-of-the-art results. Code is publicly available on GitHub: this https URL.', 'abstract_zh': '深度学习高度依赖数据增强以缓解数据不足的问题，特别是在医学影像领域。最近的多模态学习将文本和图像结合用于分割，这已知为参考或文本引导的图像分割。然而，常见的数据增强方式如旋转和翻转会破坏图像和文本之间的空间对齐性，削弱性能。为解决这一问题，我们提出了一种早期融合框架，在增强之前将文本和视觉特征结合，以保持空间一致性。我们还设计了一个轻量级生成器，将文本嵌入投影到视觉空间，以弥合语义差距。生成的伪图像可视化显示了准确的区域定位。我们的方法在三项医学影像任务和四种分割框架上进行了评估，取得了当前最佳结果。代码已在GitHub上公开：this https URL。', 'title_zh': '一种具备数据增强能力的文本-图像融合方法及其在参考医学图像分割中的应用'}
{'arxiv_id': 'arXiv:2510.11760', 'title': 'Audio-Guided Visual Perception for Audio-Visual Navigation', 'authors': 'Yi Wang, Yinfeng Yu, Fuchun Sun, Liejun Wang, Wendong Zheng', 'link': 'https://arxiv.org/abs/2510.11760', 'abstract': 'Audio-Visual Embodied Navigation aims to enable agents to autonomously navigate to sound sources in unknown 3D environments using auditory cues. While current AVN methods excel on in-distribution sound sources, they exhibit poor cross-source generalization: navigation success rates plummet and search paths become excessively long when agents encounter unheard sounds or unseen environments. This limitation stems from the lack of explicit alignment mechanisms between auditory signals and corresponding visual regions. Policies tend to memorize spurious \\enquote{acoustic fingerprint-scenario} correlations during training, leading to blind exploration when exposed to novel sound sources. To address this, we propose the AGVP framework, which transforms sound from policy-memorable acoustic fingerprint cues into spatial guidance. The framework first extracts global auditory context via audio self-attention, then uses this context as queries to guide visual feature attention, highlighting sound-source-related regions at the feature level. Subsequent temporal modeling and policy optimization are then performed. This design, centered on interpretable cross-modal alignment and region reweighting, reduces dependency on specific acoustic fingerprints. Experimental results demonstrate that AGVP improves both navigation efficiency and robustness while achieving superior cross-scenario generalization on previously unheard sounds.', 'abstract_zh': '基于视听融合的小型化自主声源定位导航框架：AGVP', 'title_zh': '基于音频的视听感知方法在视听导航中的应用'}
{'arxiv_id': 'arXiv:2510.11738', 'title': 'SeeingSounds: Learning Audio-to-Visual Alignment via Text', 'authors': 'Simone Carnemolla, Matteo Pennisi, Chiara Russo, Simone Palazzo, Daniela Giordano, Concetto Spampinato', 'link': 'https://arxiv.org/abs/2510.11738', 'abstract': 'We introduce SeeingSounds, a lightweight and modular framework for audio-to-image generation that leverages the interplay between audio, language, and vision-without requiring any paired audio-visual data or training on visual generative models. Rather than treating audio as a substitute for text or relying solely on audio-to-text mappings, our method performs dual alignment: audio is projected into a semantic language space via a frozen language encoder, and, contextually grounded into the visual domain using a vision-language model. This approach, inspired by cognitive neuroscience, reflects the natural cross-modal associations observed in human perception. The model operates on frozen diffusion backbones and trains only lightweight adapters, enabling efficient and scalable learning. Moreover, it supports fine-grained and interpretable control through procedural text prompt generation, where audio transformations (e.g., volume or pitch shifts) translate into descriptive prompts (e.g., "a distant thunder") that guide visual outputs. Extensive experiments across standard benchmarks confirm that SeeingSounds outperforms existing methods in both zero-shot and supervised settings, establishing a new state of the art in controllable audio-to-visual generation.', 'abstract_zh': 'SeeingSounds：一种基于音频、语言和视觉交互的轻量级模块化图像生成框架', 'title_zh': 'SeeingSounds：通过文本学习音频到视觉对齐'}
