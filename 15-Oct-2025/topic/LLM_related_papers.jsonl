{'arxiv_id': 'arXiv:2510.11754', 'title': 'Zero-Shot Large Language Model Agents for Fully Automated Radiotherapy Treatment Planning', 'authors': 'Dongrong Yang, Xin Wu, Yibo Xie, Xinyi Li, Qiuwen Wu, Jackie Wu, Yang Sheng', 'link': 'https://arxiv.org/abs/2510.11754', 'abstract': "Radiation therapy treatment planning is an iterative, expertise-dependent process, and the growing burden of cancer cases has made reliance on manual planning increasingly unsustainable, underscoring the need for automation. In this study, we propose a workflow that leverages a large language model (LLM)-based agent to navigate inverse treatment planning for intensity-modulated radiation therapy (IMRT). The LLM agent was implemented to directly interact with a clinical treatment planning system (TPS) to iteratively extract intermediate plan states and propose new constraint values to guide inverse optimization. The agent's decision-making process is informed by current observations and previous optimization attempts and evaluations, allowing for dynamic strategy refinement. The planning process was performed in a zero-shot inference setting, where the LLM operated without prior exposure to manually generated treatment plans and was utilized without any fine-tuning or task-specific training. The LLM-generated plans were evaluated on twenty head-and-neck cancer cases against clinical manual plans, with key dosimetric endpoints analyzed and reported. The LLM-generated plans achieved comparable organ-at-risk (OAR) sparing relative to clinical plans while demonstrating improved hot spot control (Dmax: 106.5% vs. 108.8%) and superior conformity (conformity index: 1.18 vs. 1.39 for boost PTV; 1.82 vs. 1.88 for primary PTV). This study demonstrates the feasibility of a zero-shot, LLM-driven workflow for automated IMRT treatment planning in a commercial TPS. The proposed approach provides a generalizable and clinically applicable solution that could reduce planning variability and support broader adoption of AI-based planning strategies.", 'abstract_zh': '基于大型语言模型的零样本逆治疗规划工作流在商业治疗计划系统中的自动调强放疗治疗计划研究', 'title_zh': '零-shot 大型语言模型代理实现全自动放射治疗计划规划'}
{'arxiv_id': 'arXiv:2510.12787', 'title': 'Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics', 'authors': 'Marco Del Tredici, Jacob McCarran, Benjamin Breen, Javier Aspuru Mijares, Weichen Winston Yin, Jacob M. Taylor, Frank Koppens, Dirk Englund', 'link': 'https://arxiv.org/abs/2510.12787', 'abstract': "We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperform them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Prover's assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem.", 'abstract_zh': '我们介绍Ax-Prover：一个通过Lean进行自动定理证明的多代理系统，能够解决跨学科科学领域的难题，并可自主运行或与人类专家协作。Ax-Prover通过形式证明这一过程，既要求创造性推理也要求严格的语法规则，利用大型语言模型（LLMs）结合Lean工具来确保形式正确性。为了评估其作为自主证明系统的性能，我们将其方法与前沿的LLMs和专门的证明模型在两个公开的数学基准测试和两个由我们引入的抽象代数和量子理论领域的Lean基准测试上进行对比。在公开数据集上，Ax-Prover与最先进的证明系统竞争；在新基准测试上，其性能显著优于它们。这表明，与难以泛化的专业系统不同，我们基于工具的代理型定理证明方法提供了在跨学科科学领域中进行形式验证的一般化方法。此外，我们通过一个实际应用场景展示了Ax-Prover的辅助能力，展示了它如何使一位专家数学家能够形式化一个复杂的密码学定理的证明。', 'title_zh': 'Ax-Prover: 一个用于数学和量子物理定理证明的深度推理代理框架'}
{'arxiv_id': 'arXiv:2510.12742', 'title': 'CTRL-Rec: Controlling Recommender Systems With Natural Language', 'authors': 'Micah Carroll, Adeline Foote, Kevin Feng, Marcus Williams, Anca Dragan, W. Bradley Knox, Smitha Milli', 'link': 'https://arxiv.org/abs/2510.12742', 'abstract': 'When users are dissatisfied with recommendations from a recommender system, they often lack fine-grained controls for changing them. Large language models (LLMs) offer a solution by allowing users to guide their recommendations through natural language requests (e.g., "I want to see respectful posts with a different perspective than mine"). We propose a method, CTRL-Rec, that allows for natural language control of traditional recommender systems in real-time with computational efficiency. Specifically, at training time, we use an LLM to simulate whether users would approve of items based on their language requests, and we train embedding models that approximate such simulated judgments. We then integrate these user-request-based predictions into the standard weighting of signals that traditional recommender systems optimize. At deployment time, we require only a single LLM embedding computation per user request, allowing for real-time control of recommendations. In experiments with the MovieLens dataset, our method consistently allows for fine-grained control across a diversity of requests. In a study with 19 Letterboxd users, we find that CTRL-Rec was positively received by users and significantly enhanced users\' sense of control and satisfaction with recommendations compared to traditional controls.', 'abstract_zh': '当用户对推荐系统推荐的内容不满意时，他们通常缺乏精细的控制手段来调整推荐。大型语言模型（LLMs）通过允许用户通过自然语言请求（如：“我想要看到不同于我自己的观点的有尊重性的帖子”）来引导推荐提供了解决方案。我们提出了一种方法——CTRL-Rec，该方法可以在保证计算效率的前提下，实现实时的基于自然语言的推荐控制。具体而言，在训练期间，我们使用LLM模拟用户是否会批准基于其语言请求的项目，并训练嵌入模型以近似这些模拟判断。然后，我们将这些基于用户请求的预测整合到传统推荐系统优化的标准信号权重中。在部署时，我们只需要为每个用户请求进行一次LLM嵌入计算，从而实现推荐的实时控制。在使用MovieLens数据集的实验中，我们的方法能够跨多种请求实现细粒度控制。在一项涉及19名Letterboxd用户的的研究中，我们发现CTRL-Rec受到了用户的积极评价，并且与传统控制方法相比，显著提升了用户对推荐的控制感和满意度。', 'title_zh': 'CTRL-Rec: 用自然语言控制推荐系统'}
{'arxiv_id': 'arXiv:2510.12697', 'title': 'Multi-Agent Debate for LLM Judges with Adaptive Stability Detection', 'authors': 'Tianyu Hu, Zhen Tan, Song Wang, Huaizhi Qu, Tianlong Chen', 'link': 'https://arxiv.org/abs/2510.12697', 'abstract': "With advancements in reasoning capabilities, Large Language Models (LLMs) are increasingly employed for automated judgment tasks. While LLMs-as-Judges offer promise in automating evaluations, current approaches often rely on simplistic aggregation methods (e.g., majority voting), which can fail even when individual agents provide correct answers. To address this, we propose a multi-agent debate judge framework where agents collaboratively reason and iteratively refine their responses. We formalize the debate process mathematically, analyzing agent interactions and proving that debate amplifies correctness compared to static ensembles. To enhance efficiency, we introduce a stability detection mechanism that models judge consensus dynamics via a time-varying Beta-Binomial mixture, with adaptive stopping based on distributional similarity (Kolmogorov-Smirnov test). This mechanism models the judges' collective correct rate dynamics using a time-varying mixture of Beta-Binomial distributions and employs an adaptive stopping criterion based on distributional similarity (Kolmogorov-Smirnov statistic). Experiments across multiple benchmarks and models demonstrate that our framework improves judgment accuracy over majority voting while maintaining computational efficiency.", 'abstract_zh': '随着推理能力的进步，大规模语言模型（LLMs）越来越多地被用于自动化判断任务。虽然LLMs-as-Judges在自动化评估方面具有潜力，但当前的方法往往依赖于简单的聚合方法（例如多数投票），即使个体代理提供正确的答案也可能失败。为此，我们提出了一种多代理辩论裁决框架，使代理能够协作推理并迭代地完善其回复。我们从数学上形式化了辩论过程，分析了代理间的交互，并证明了辩论比静态ensemble能更提高正确性。为提高效率，我们引入了一种稳定性检测机制，通过时间变化的Beta-Binomial混合模型建模裁决者的共识动态，并基于分布相似性（Kolmogorov-Smirnov检验）实现自适应停止。该机制使用时间变化的Beta-Binomial分布混合模型描述裁决者的集体正确率动态，并基于分布相似性（Kolmogorov-Smirnov统计量）采用自适应停止准则。在多个基准和模型上的实验表明，我们的框架在保持计算效率的同时提高了判断准确性。', 'title_zh': 'LLM法官参与的自适应稳定性检测多代理辩论'}
{'arxiv_id': 'arXiv:2510.12490', 'title': 'Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews', 'authors': 'Rui Reis, Pedro Rangel Henriques, João Ferreira-Coimbra, Eva Oliveira, Nuno F. Rodrigues', 'link': 'https://arxiv.org/abs/2510.12490', 'abstract': 'We developed a task-oriented dialogue framework structured as a Directed Acyclic Graph (DAG) of medical questions. The system integrates: (1) a systematic pipeline for transforming medical algorithms and guidelines into a clinical question corpus; (2) a cold-start mechanism based on hierarchical clustering to generate efficient initial questioning without prior patient information; (3) an expand-and-prune mechanism enabling adaptive branching and backtracking based on patient responses; (4) a termination logic to ensure interviews end once sufficient information is gathered; and (5) automated synthesis of doctor-friendly structured reports aligned with clinical workflows. Human-computer interaction principles guided the design of both the patient and physician applications. Preliminary evaluation involved five physicians using standardized instruments: NASA-TLX (cognitive workload), the System Usability Scale (SUS), and the Questionnaire for User Interface Satisfaction (QUIS). The patient application achieved low workload scores (NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS = 8.1/9), with particularly high ratings for ease of learning and interface design. The physician application yielded moderate workload (NASA-TLX = 26) and excellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both applications demonstrated effective integration into clinical workflows, reducing cognitive demand and supporting efficient report generation. Limitations included occasional system latency and a small, non-diverse evaluation sample.', 'abstract_zh': '我们开发了一种基于有向无环图（DAG）的医学问题结构化任务导向对话框架。该系统包括：（1）一套系统化的医疗算法和指南转化为临床问题库的流水线；（2）基于层次聚类的冷启动机制，以生成高效初始问题，无需先验患者信息；（3）扩展和修剪机制，根据患者反馈实现自适应分支和回溯；（4）终止逻辑以确保在收集足够信息后结束访谈；以及（5）与临床工作流程对齐的医生友好的结构化报告的自动化合成。人机交互原则指导了患者和医生应用的设计。初步评估涉及五名医生使用标准化量表：NASA-TLX（认知负载）、系统可用性量表（SUS）和用户界面满意度问卷（QUIS）。患者应用取得了较低的认知负载评分（NASA-TLX = 15.6）、较高的易用性（SUS = 86）和较强的满意度（QUIS = 8.1/9），特别是在学习难易度和界面设计方面获得了高评分。医生应用的认知负载适中（NASA-TLX = 26）、易用性极佳（SUS = 88.5），满意度评分为8.3/9。这两种应用都有效地整合到了临床工作流程中，减少了认知需求并支持了高效的报告生成。局限性包括偶尔的系统延迟和较小的非多样化评估样本。', 'title_zh': '基于LLM的医疗访谈中面向任务的对话医疗算法的应用'}
{'arxiv_id': 'arXiv:2510.12462', 'title': 'Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems', 'authors': 'Jiaxin Gao, Chen Chen, Yanwen Jia, Xueluan Gong, Kwok-Yan Lam, Qian Wang', 'link': 'https://arxiv.org/abs/2510.12462', 'abstract': 'Large Language Models (LLMs) are increasingly being used to autonomously evaluate the quality of content in communication systems, e.g., to assess responses in telecom customer support chatbots. However, the impartiality of these AI "judges" is not guaranteed, and any biases in their evaluation criteria could skew outcomes and undermine user trust. In this paper, we systematically investigate judgment biases in two LLM-as-a-judge models (i.e., GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11 types of biases that cover both implicit and explicit forms. We observed that state-of-the-art LLM judges demonstrate robustness to biased inputs, generally assigning them lower scores than the corresponding clean samples. Providing a detailed scoring rubric further enhances this robustness. We further found that fine-tuning an LLM on high-scoring yet biased responses can significantly degrade its performance, highlighting the risk of training on biased data. We also discovered that the judged scores correlate with task difficulty: a challenging dataset like GPQA yields lower average scores, whereas an open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores. Finally, we proposed four potential mitigation strategies to ensure fair and reliable AI judging in practical communication scenarios.', 'abstract_zh': '大型语言模型（LLMs）在通信系统中自主评估内容质量的应用越来越广泛，例如评价电信客服聊天机器人的回应。然而，这些AI“裁判”的公正性并不保证，其评价标准中的任何偏差都可能导致结果失真并削弱用户信任。本文系统地研究了两种LLM-as-a-judge模型（即GPT-Judge和JudgeLM）在点评分设置下的判断偏差，涵盖了11种偏差，包括隐性和显性两种形式。我们发现，最先进的LLM裁判对有偏见的输入表现出较强的鲁棒性，通常会给予较低的评分，而相应的干净样本则获得较高评分。提供详细的评分标准进一步增强了这种鲁棒性。进一步的研究发现，对LLM进行高分但有偏见的回应的微调会显著降低其性能，突出了使用有偏见数据进行训练的风险。我们还发现，评估分数与任务难度相关：如GPQA这样具有挑战性的数据集会导致较低的平均评分，而开放性推理数据集（如JudgeLM-val）则会有较高的平均评分。最后，我们提出了四种潜在的缓解策略，以确保在实际通信场景中实现公平可靠的AI评判。', 'title_zh': '评价和缓解语言模型作为裁判的偏见在通信系统中的影响'}
{'arxiv_id': 'arXiv:2510.12423', 'title': 'MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics', 'authors': 'Dingyi Zuo, Hongjie Zhang, Jie Ou, Chaosheng Feng, Shuwan Liu', 'link': 'https://arxiv.org/abs/2510.12423', 'abstract': 'The polarization of opinions, information segregation, and cognitive biases on social media have attracted significant academic attention. In real-world networks, information often spans multiple interrelated topics, posing challenges for opinion evolution and highlighting the need for frameworks that simulate interactions among topics. Existing studies based on large language models (LLMs) focus largely on single topics, limiting the capture of cognitive transfer in multi-topic, cross-domain contexts. Traditional numerical models, meanwhile, simplify complex linguistic attitudes into discrete values, lacking interpretability, behavioral consistency, and the ability to integrate multiple topics. To address these issues, we propose Multi-topic Opinion Simulation (MTOS), a social simulation framework integrating multi-topic contexts with LLMs. MTOS leverages LLMs alongside short-term and long-term memory, incorporates multiple user-selection interaction mechanisms and dynamic topic-selection strategies, and employs a belief decay mechanism to enable perspective updates across topics. We conduct extensive experiments on MTOS, varying topic numbers, correlation types, and performing ablation studies to assess features such as group polarization and local consistency. Results show that multi-topic settings significantly alter polarization trends: positively correlated topics amplify echo chambers, negatively correlated topics inhibit them, and irrelevant topics also mitigate echo chamber effects through resource competition. Compared with numerical models, LLM-based agents realistically simulate dynamic opinion changes, reproduce linguistic features of news texts, and capture complex human reasoning, improving simulation interpretability and system stability.', 'abstract_zh': '社交媒体上的观点极化、信息隔离和认知偏见的学术关注日益增加。在现实世界网络中，信息往往涉及多个相互关联的主题，这为意见演化带来了挑战，凸显了需要模拟主题间交互框架的重要性。基于大型语言模型（LLMs）的现有研究主要集中在单个主题上，限制了对多主题、跨领域情境中的认知转移的捕捉。同时，传统的数值模型将复杂的语言态度简化为离散值，缺乏可解释性、行为一致性以及多主题整合的能力。为了解决这些问题，我们提出了一种集成多主题上下文与LLMs的社会仿真框架——多主题意见模拟（MTOS）。MTOS结合了LLMs及其短期和长期记忆，融合了多种用户选择交互机制和动态主题选择策略，并采用信念衰减机制以支持跨主题的观点更新。我们在MTOS上进行了广泛的实验，变化主题数量、相关类型，并进行消融研究以评估群体极化和局部一致性等特征。结果显示，多主题设置显著改变了极化趋势：正相关主题放大了回声室效应，负相关主题抑制了回声室效应，而且无关主题也通过资源竞争减轻了回声室效应。与数值模型相比，基于LLMs的代理可以更真实地模拟动态意见变化，再现新闻文本的语言特征，并捕捉复杂的逻辑推理，从而提高仿真可解释性和系统稳定性。', 'title_zh': 'MTOS：一个由大规模语言模型驱动的多话题意见模拟框架，用于探索回声室效应动态'}
{'arxiv_id': 'arXiv:2510.12409', 'title': 'PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks', 'authors': 'Yunuo Liu, Dawei Zhu, Zena Al-Khalili, Dai Cheng, Yanjun Chen, Dietrich Klakow, Wei Zhang, Xiaoyu Shen', 'link': 'https://arxiv.org/abs/2510.12409', 'abstract': "We present PricingLogic, the first benchmark that probes whether Large Language Models(LLMs) can reliably automate tourism-related prices when multiple, overlapping fare rules apply. Travel agencies are eager to offload this error-prone task onto AI systems; however, deploying LLMs without verified reliability could result in significant financial losses and erode customer trust. PricingLogic comprises 300 natural-language questions based on booking requests derived from 42 real-world pricing policies, spanning two levels of difficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations involving interacting discounts. Evaluations of a line of LLMs reveal a steep performance drop on the harder tier,exposing systematic failures in rule interpretation and arithmetic this http URL results highlight that, despite their general capabilities, today's LLMs remain unreliable in revenue-critical applications without further safeguards or domain adaptation. Our code and dataset are available at this https URL.", 'abstract_zh': 'PricingLogic：首个探究大型语言模型在复杂票价规则下可靠自动化定价能力的基准测验', 'title_zh': '定价逻辑：评估大语言模型在复杂旅游定价任务中的推理能力'}
{'arxiv_id': 'arXiv:2510.12399', 'title': 'A Survey of Vibe Coding with Large Language Models', 'authors': 'Yuyao Ge, Lingrui Mei, Zenghao Duan, Tianhao Li, Yujia Zheng, Yiwei Wang, Lexin Wang, Jiayu Yao, Tianyu Liu, Yujun Cai, Baolong Bi, Fangda Guo, Jiafeng Guo, Shenghua Liu, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2510.12399', 'abstract': 'The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed "Vibe Coding" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension. Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration. To address this gap, this survey provides the first comprehensive and systematic review of Vibe Coding with large language models, establishing both theoretical foundations and practical frameworks for this transformative development approach. Drawing from systematic analysis of over 1000 research papers, we survey the entire vibe coding ecosystem, examining critical infrastructure components including LLMs for coding, LLM-based coding agent, development environment of coding agent, and feedback mechanisms. We first introduce Vibe Coding as a formal discipline by formalizing it through a Constrained Markov Decision Process that captures the dynamic triadic relationship among human developers, software projects, and coding agents. Building upon this theoretical foundation, we then synthesize existing practices into five distinct development models: Unconstrained Automation, Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and Context-Enhanced Models, thus providing the first comprehensive taxonomy in this domain. Critically, our analysis reveals that successful Vibe Coding depends not merely on agent capabilities but on systematic context engineering, well-established development environments, and human-agent collaborative development models.', 'abstract_zh': '大型语言模型的发展催化了从代码生成辅助到自主 coding 代理的范式转变，开启了通过结果观察而非逐行代码理解来验证 AI 生成实现的“Vibe Coding”新型开发方法。尽管具有革命性潜力，这一新兴范式的有效性仍需进一步探索，实验研究表明存在意想不到的生产力损失和人机协作的基本挑战。为填补这一空白，本文综述了大型语言模型下的 Vibe Coding，为其提供了理论基础和实践框架。基于对超过1000篇研究论文的系统分析，我们全面审视了整个 Vibe Coding 生态系统，探讨了关键基础设施组件，包括用于编程的大型语言模型、基于大型语言模型的编程代理、编程代理的开发环境以及反馈机制。我们首先通过受限马尔可夫决策过程形式化 Vibe Coding，捕捉人类开发者、软件项目和编程代理之间的动态三角关系，以此为基础，我们将现有实践综合提炼为五种不同的开发模型：无约束自动化、迭代对话协作、计划驱动、测试驱动以及增强情境模型，从而首次在该领域提供了全面的分类体系。我们的分析表明，成功的 Vibe Coding 不仅取决于代理的能力，还取决于系统的情境工程、成熟的开发环境以及人机协作开发模型。', 'title_zh': '大型语言模型中的Vibe编码综述'}
{'arxiv_id': 'arXiv:2510.12350', 'title': 'O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis', 'authors': 'Ayush Khaitan, Vijay Ganesh', 'link': 'https://arxiv.org/abs/2510.12350', 'abstract': 'Large language models have recently demonstrated advanced capabilities in solving IMO and Putnam problems; yet their role in research mathematics has remained fairly limited. The key difficulty is verification: suggested proofs may look plausible, but cannot be trusted without rigorous checking. We present a framework, called LLM+CAS, and an associated tool, O-Forge, that couples frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic Feedback loop to produce proofs that are both creative and symbolically verified. Our focus is on asymptotic inequalities, a topic that often involves difficult proofs and appropriate decomposition of the domain into the "right" subdomains. Many mathematicians, including Terry Tao, have suggested that using AI tools to find the right decompositions can be very useful for research-level asymptotic analysis. In this paper, we show that our framework LLM+CAS turns out to be remarkably effective at proposing such decompositions via a combination of a frontier LLM and a CAS. More precisely, we use an LLM to suggest domain decomposition, and a CAS (such as Mathematica) that provides a verification of each piece axiomatically. Using this loop, we answer a question posed by Terence Tao: whether LLMs coupled with a verifier can be used to help prove intricate asymptotic inequalities. More broadly, we show how AI can move beyond contest math towards research-level tools for professional mathematicians.', 'abstract_zh': '大规模语言模型在解决IMO和普特南问题上展示了先进的能力，但在研究数学中的作用仍然相当有限；其关键难题在于验证：提出的证明可能看似合乎道理，但必须经过严格的检查才能信赖。我们提出了一种称为LLM+CAS的框架和相应的工具O-Forge，通过将前沿的大规模语言模型与计算机代数系统（CAS）结合在一个基于符号反馈的上下文循环中，生成既具有创新性又能符号验证的证明。我们的重点是渐近不等式，这是一个经常涉及复杂证明和正确分解域的主题。许多数学家，包括陶哲轩，建议使用AI工具来寻找合适的分解对于研究级别的渐近分析非常有用。本文展示了我们的LLM+CAS框架通过结合前沿的大规模语言模型和CAS在提出此类分解方面表现出显著的效果。具体而言，我们使用大规模语言模型建议领域分解，CAS（如Mathematica）对每部分进行公理验证。通过这个循环，我们回答了陶哲轩提出的疑问：将大规模语言模型与验证器结合是否能用于帮助证明复杂的渐近不等式。更广泛地说，我们展示了AI如何从竞赛数学转向为专业数学家提供研究级别工具。', 'title_zh': 'O-Forge: 一个基于大语言模型与计算机代数的渐近分析框架'}
{'arxiv_id': 'arXiv:2510.12323', 'title': 'RAG-Anything: All-in-One RAG Framework', 'authors': 'Zirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang', 'link': 'https://arxiv.org/abs/2510.12323', 'abstract': 'Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inherently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content, creating fundamental gaps when processing multimodal documents. We present RAG-Anything, a unified framework that enables comprehensive knowledge retrieval across all modalities. Our approach reconceptualizes multimodal content as interconnected knowledge entities rather than isolated data types. The framework introduces dual-graph construction to capture both cross-modal relationships and textual semantics within a unified representation. We develop cross-modal hybrid retrieval that combines structural knowledge navigation with semantic matching. This enables effective reasoning over heterogeneous content where relevant evidence spans multiple modalities. RAG-Anything demonstrates superior performance on challenging multimodal benchmarks, achieving significant improvements over state-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. Our framework establishes a new paradigm for multimodal knowledge access, eliminating the architectural fragmentation that constrains current systems. Our framework is open-sourced at: this https URL.', 'abstract_zh': '检索增强生成（RAG）已成为一种基本范式，用于扩展大型语言模型超越其静态训练限制。然而，当前RAG能力与实际信息环境之间存在关键不匹配。现代知识库本质上是多模态的，包含丰富的文本内容、视觉元素、结构化表格和数学表达式。然而，现有的RAG框架仅限于处理文本内容，处理多模态文档时存在根本性差距。我们提出了RAG-Anything统一框架，以实现全方位的知识检索跨所有模态。我们的方法将多模态内容重新构想为相互关联的知识实体，而非孤立的数据类型。框架引入了双图构建，以在统一表示中捕捉跨模态关系和文本语义。我们开发了跨模态混合检索，结合结构化知识导航与语义匹配。这使我们在异构内容中能够有效推理，其中相关证据跨越多种模态。RAG-Anything在具有挑战性的多模态基准测试中表现出优越性能，显著优于现有方法。在长文档中，性能提升尤其明显，传统方法在这种情况下失效。我们的框架建立了新的多模态知识访问范式，消除了现有系统中的架构碎片化限制。我们的框架在以下地址开源：this https URL。', 'title_zh': 'RAG-anything: 一站式RAG框架'}
{'arxiv_id': 'arXiv:2510.12264', 'title': '$\\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning', 'authors': 'Deyu Zou, Yongqiang Chen, Jianxiang Wang, Haochen Yang, Mufei Li, James Cheng, Pan Li, Yu Gong', 'link': 'https://arxiv.org/abs/2510.12264', 'abstract': 'Active reasoning requires large language models (LLMs) to interact with external sources and strategically gather information to solve problems. Central to this process is belief tracking: maintaining a coherent understanding of the problem state and the missing information toward the solution. However, due to limited reasoning capabilities, LLM-based agents often suffer from belief deviation: they struggle to correctly model beliefs, lose track of problem states, and fall into uninformative or repetitive actions. Once this happens, errors compound and reinforcement learning (RL) training fails to properly credit the crucial exploratory steps. To address this issue, we propose to track the deviation of model beliefs and develop $\\mathbf{T^3}$, a simple yet effective method that detects excessive belief deviation and truncates trajectories during training to remove uninformative tails. By preserving credit for informative prefixes, $\\mathbf{T^3}$ systematically improves policy optimization. Across 5 challenging tasks, $\\mathbf{T^3}$ consistently enhances training stability, token efficiency, and final performance, achieving up to 30% gains while cutting rollout tokens by roughly 25%. These results highlight belief control as a key principle for developing robust and generalizable LLM-based active reasoners.', 'abstract_zh': '基于大型语言模型的主动推理要求与外部来源互动并战略性地收集信息以解决问题。这一过程的核心是信念跟踪：维持对问题状态和解决过程中缺失信息的连贯理解。然而，由于推理能力有限，基于大型语言模型的代理经常出现信念偏差：难以正确建模信念、丢失问题状态跟踪，并陷入无信息或重复的行动中。一旦发生这种情况，错误会累积，强化学习训练无法正确奖励关键的探索步骤。为解决这一问题，我们提出跟踪模型信念的偏差，并开发了 $\\mathbf{T^3}$，一种简单而有效的方法来检测过度信念偏差，并在训练中截断轨迹以移除无信息的部分。通过保留信息前缀的信用，$\\mathbf{T^3}$ 系统性地改善了策略优化。在5项具有挑战性的任务中，$\\mathbf{T^3}$ 一致地增强了训练稳定性、token 效率和最终性能，实现了高达30%的提升，同时削减了约25%的展开token。这些结果强调了信念控制作为开发鲁棒且通用的大规模语言模型基于的主动推理者的关键原则。', 'title_zh': '$\\mathbf{T^3}$: 减少主动推理中强化学习中的信念偏差'}
{'arxiv_id': 'arXiv:2510.12246', 'title': 'PromptFlow: Training Prompts Like Neural Networks', 'authors': 'Jingyi Wang, Hongyuan Zhu, Ye Niu, Yunhui Deng', 'link': 'https://arxiv.org/abs/2510.12246', 'abstract': 'Large Language Models (LLMs) have demonstrated profound impact on Natural Language Processing (NLP) tasks. However, their effective deployment across diverse domains often require domain-specific adaptation strategies, as generic models may underperform when faced with specialized data distributions. Recent advances in prompt engineering (PE) offer a promising alternative to extensive retraining by refining input instructions to align LLM outputs with task objectives. This paradigm has emerged as a rapid and versatile approach for model fine-tuning. Despite its potential, manual prompt design remains labor-intensive and heavily depends on specialized expertise, often requiring iterative human effort to achieve optimal formulations. To address this limitation, automated prompt engineering methodologies have been developed to systematically generate task-specific prompts. However, current implementations predominantly employ static update rules and lack mechanisms for dynamic strategy selection, resulting in suboptimal adaptation to varying NLP task requirements. Furthermore, most methods treat and update the whole prompts at each step, without considering editing prompt sections at a finer granularity. At last, in particular, the problem of how to recycle experience in LLM is still underexplored. To this end, we propose the PromptFlow, a modular training framework inspired by TensorFlow, which integrates meta-prompts, operators, optimization, and evaluator. Our framework can be equipped with the latest optimization methods and autonomously explores optimal prompt refinement trajectories through gradient-based meta-learning, requiring minimal task-specific training data. Specifically, we devise a reinforcement learning method to recycle experience for LLM in the PE process. Finally, we conduct extensive experiments on various datasets, and demonstrate the effectiveness of PromptFlow.', 'abstract_zh': '大规模语言模型(LLMs)在自然语言处理(NLP)任务中展现了深远的影响。然而，它们在多种领域的有效部署通常需要特定领域的适应策略，因为通用模型在面对专业化数据分布时可能会表现不佳。最近在提示工程(PE)方面的进展为替代广泛的再训练提供了一种有前途的选择，通过精炼输入指令以使LLM输出与任务目标对齐。这种范式已成为模型微调的一种快速且多功能的方法。尽管存在这些潜力，但手动提示设计仍然劳动密集且高度依赖专门的知识，通常需要迭代的人工努力才能达到最佳表述。为了解决这一限制，已经开发了自动提示工程方法以系统地生成任务专用的提示。然而，当前的实现大多采用静态更新规则，并缺乏动态策略选择机制，导致适应不同NLP任务需求的效果不ideal。此外，大多数方法在每一步都对整个提示进行处理和更新，而不考虑在更细粒度上编辑提示段落。最后，特别地，如何在LLM中循环利用经验的问题仍然未得到充分探索。为此，我们提出了PromptFlow，这是一种受TensorFlow启发的模块化训练框架，结合了元提示、操作符、优化和评估器。我们的框架可以配备最新的优化方法，并通过基于梯度的元学习自主探索最优提示改进轨迹，要求最少的任务特定训练数据。具体而言，我们设计了一种强化学习方法，在提示工程(PE)过程中为LLM循环利用经验。最后，我们在各种数据集上进行了广泛的实验，并展示了PromptFlow的有效性。', 'title_zh': 'PromptFlow：像神经网络一样训练提示詞'}
{'arxiv_id': 'arXiv:2510.12224', 'title': 'MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs', 'authors': 'Yuechun Yu, Han Ying, Haoan Jin, Wenjian Jiang, Dong Xian, Binghao Wang, Zhou Yang, Mengyue Wu', 'link': 'https://arxiv.org/abs/2510.12224', 'abstract': "The reliable evaluation of large language models (LLMs) in medical applications remains an open challenge, particularly in capturing the complexity of multi-turn doctor-patient interactions that unfold in real clinical environments. Existing evaluation methods typically rely on post hoc review of full conversation transcripts, thereby neglecting the dynamic, context-sensitive nature of medical dialogues and the evolving informational needs of patients. In this work, we present MedKGEval, a novel multi-turn evaluation framework for clinical LLMs grounded in structured medical knowledge. Our approach introduces three key contributions: (1) a knowledge graph-driven patient simulation mechanism, where a dedicated control module retrieves relevant medical facts from a curated knowledge graph, thereby endowing the patient agent with human-like and realistic conversational behavior. This knowledge graph is constructed by integrating open-source resources with additional triples extracted from expert-annotated datasets; (2) an in-situ, turn-level evaluation framework, where each model response is assessed by a Judge Agent for clinical appropriateness, factual correctness, and safety as the dialogue progresses using a suite of fine-grained, task-specific metrics; (3) a comprehensive multi-turn benchmark of eight state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle behavioral flaws and safety risks that are often overlooked by conventional evaluation pipelines. Although initially designed for Chinese and English medical applications, our framework can be readily extended to additional languages by switching the input knowledge graphs, ensuring seamless bilingual support and domain-specific applicability.", 'abstract_zh': '可靠评估医疗应用中的大规模语言模型（LLMs）仍是一个开放挑战，特别是在捕捉实际临床环境中多轮医患交互的复杂性方面。现有的评估方法通常依赖于事后审查完整的对话转录，因此忽视了医疗对话的动态性和上下文敏感性以及患者不断变化的信息需求。在本文中，我们提出了MedKGEval，一个基于结构化医学知识的新型多轮评估框架。我们的方法引入了三个关键贡献：（1）知识图驱动的患者模拟机制，其中专用控制模块从精心策划的知识图中检索相关医学事实，从而使患者代理具备类似人类的真实对话行为。该知识图结合了开源资源和专家标注数据集中提取的额外三元组构建而成；（2）实时、轮次级别的评估框架，其中每个模型响应在对话进行过程中由法官代理评估临床适切性、事实正确性和安全性，使用一系列精细粒度的任务特定指标；（3）八个最先进的LLM的综合多轮基准测试，展示了MedKGEval能够识别传统评估流水线常常忽略的微妙行为缺陷和安全风险。尽管最初为中文和英文医疗应用设计，但该框架可以通过切换输入知识图来轻松扩展到其他语言，确保无缝的双语支持和特定领域的适用性。', 'title_zh': 'MedKGEval：基于知识图谱的多轮临床LLM开放域患者交互评估框架'}
{'arxiv_id': 'arXiv:2510.12218', 'title': 'GOAT: A Training Framework for Goal-Oriented Agent with Tools', 'authors': 'Hyunji Min, Sangwon Jung, Junyoung Sung, Dosung Lee, Leekyeung Han, Paul Hongsuck Seo', 'link': 'https://arxiv.org/abs/2510.12218', 'abstract': 'Large language models (LLMs) have recently been extended beyond traditional text generation to serve as interactive agents capable of using external tools based on user intent. However, current LLM agents still show limited ability to handle goal-oriented queries, which require decomposing a high-level objective into multiple interdependent API calls with correct planning and execution. Current approaches mainly rely on zero-shot evaluation due to the absence of training data. While proprietary closed-source models such as GPT-4 demonstrate strong reasoning abilities, smaller open-source models struggle to perform complex tool use effectively. Thus, we propose a novel training framework GOAT, which enables fine-tuning of LLM agents in a human annotation-free setting. GOAT automatically constructs synthetic datasets of goal-oriented API execution tasks directly from given API documents, equipping models with the ability to reason over interdependent calls and generate coherent responses. Through extensive experiments, we show that GOAT-trained agents achieve state-of-the-art performance across multiple existing goal-oriented benchmarks. In addition, we introduce GOATBench, a new goal-oriented API execution benchmark, and demonstrate that agents trained with GOAT also excel in this setting. These results highlight GOAT as a practical path toward building robust open-source LLM agents capable of complex reasoning and tool use.', 'abstract_zh': '大规模语言模型（LLMs）已被扩展到超越传统文本生成，作为能够根据用户意图使用外部工具的交互式代理。然而，当前的LLM代理在处理目标导向查询方面仍显示有限的能力，这需要将高层目标分解为多个相互依赖的API调用，并进行正确的计划和执行。当前的方法主要依赖于零样本评估，因为缺少训练数据。虽然像GPT-4这样的私有闭源模型表现出强大的推理能力，但较小的开源模型在有效地使用复杂工具方面仍然挣扎。因此，我们提出了一种新颖的训练框架GOAT，该框架能够在无需人工标注的情况下对LLM代理进行微调。GOAT自动从给定的API文档中构建目标导向的API执行任务的合成数据集，使模型具备推理相互依赖调用并生成连贯响应的能力。通过广泛的实验，我们展示了GOAT微调的代理在多个现有目标导向基准上的性能达到最新水平。此外，我们介绍了GOATBench，一个新的目标导向的API执行基准，并展示了使用GOAT训练的代理在这种环境中也表现出色。这些结果突显了GOAT作为构建具备复杂推理和工具使用能力的稳健开源LLM代理的实用路径。', 'title_zh': 'GOAT：面向目标导向代理的工具训练框架'}
{'arxiv_id': 'arXiv:2510.12178', 'title': "Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey", 'authors': 'Abdulhady Abas Abdullah, Arkaitz Zubiaga, Seyedali Mirjalili, Amir H. Gandomi, Fatemeh Daneshfar, Mohammadsadra Amini, Alan Salam Mohammed, Hadi Veisi', 'link': 'https://arxiv.org/abs/2510.12178', 'abstract': "This review surveys the rapid evolution of Meta AI's LLaMA (Large Language Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized parameter-efficient fine-tuning (PEFT) methods developed for these models. We first describe the LLaMA family of foundation models (7B-65B to 288B parameters), their architectures (including native multimodal and Mixtureof-Experts variants), and key performance characteristics. We then describe and discuss the concept of PEFT, which adapts large pre-trained models by updating only a small subset of parameters, and review five PEFT methods that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1 and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's mechanism, parameter savings, and example application to LLaMA (e.g., instruction tuning, multimodal tasks). We provide structured discussion and analysis of model and adapter architectures, parameter counts, and benchmark results (including examples where fine-tuned LLaMA models outperform larger baselines). Finally, we examine real-world use cases where LLaMA-based models and PEFT have been successfully applied (e.g., legal and medical domains), and we discuss ongoing challenges and future research directions (such as scaling to even larger contexts and improving robustness). This survey paper provides a one-stop resource for ML researchers and practitioners interested in LLaMA models and efficient fine-tuning strategies.", 'abstract_zh': 'Meta AI的LLaMA系列的快速演进及其参数高效微调方法：从LLaMA 1到LLaMA 4及专为这些模型开发的PEFT方法综述', 'title_zh': 'Meta的 llama 模型的发展与大规模语言模型的参数高效微调：一个综述'}
{'arxiv_id': 'arXiv:2510.12171', 'title': 'MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science', 'authors': 'Junkai Zhang, Jingru Gan, Xiaoxuan Wang, Zian Jia, Changquan Gu, Jianpeng Chen, Yanqiao Zhu, Mingyu Derek Ma, Dawei Zhou, Ling Li, Wei Wang', 'link': 'https://arxiv.org/abs/2510.12171', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable abilities in scientific reasoning, yet their reasoning capabilities in materials science remain underexplored. To fill this gap, we introduce MatSciBench, a comprehensive college-level benchmark comprising 1,340 problems that span the essential subdisciplines of materials science. MatSciBench features a structured and fine-grained taxonomy that categorizes materials science questions into 6 primary fields and 31 sub-fields, and includes a three-tier difficulty classification based on the reasoning length required to solve each question. MatSciBench provides detailed reference solutions enabling precise error analysis and incorporates multimodal reasoning through visual contexts in numerous questions. Evaluations of leading models reveal that even the highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on college-level materials science questions, highlighting the complexity of MatSciBench. Our systematic analysis of different reasoning strategie--basic chain-of-thought, tool augmentation, and self-correction--demonstrates that no single method consistently excels across all scenarios. We further analyze performance by difficulty level, examine trade-offs between efficiency and accuracy, highlight the challenges inherent in multimodal reasoning tasks, analyze failure modes across LLMs and reasoning methods, and evaluate the influence of retrieval-augmented generation. MatSciBench thus establishes a comprehensive and solid benchmark for assessing and driving improvements in the scientific reasoning capabilities of LLMs within the materials science domain.', 'abstract_zh': '大型语言模型（LLMs）在科学推理方面展现了卓越的能力，但在材料科学中的推理能力still有待深入探索。为了填补这一空白，我们引入了MatSciBench，这是一个全面的大学水平基准，包含1340个覆盖材料科学主要亚学科的问题。MatSciBench具有一套结构化和精细的分类体系，将材料科学问题分为6个主要领域和31个子领域，并根据解决问题所需的推理长度进行了三级难度分类。MatSciBench提供了详细的标准解决方案，便于精确的误差分析，并通过多个问题中的视觉上下文实现了多模态推理。对领先模型的评估显示，即使表现最好的模型Gemini-2.5-Pro，在解决大学水平的材料科学问题方面的准确率也低于80%，突显了MatSciBench的复杂性。我们的系统分析表明，不同的推理策略——基本链式思考、工具增强和自我修正——在所有情境中均未表现出一致性优势。我们还按难度级别分析了性能，探讨了效率与准确性的权衡，指出了多模态推理任务的固有挑战，分析了不同大型语言模型和推理方法的失败模式，并评估了检索增强生成的影响。因此，MatSciBench为评估和推动材料科学领域大型语言模型科学推理能力的提升提供了一个全面而坚实的标准。', 'title_zh': 'MatSciBench: 评估大型语言模型在材料科学中的推理能力'}
{'arxiv_id': 'arXiv:2510.12121', 'title': 'Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing', 'authors': 'Rongzhi Zhang, Liqin Ye, Yuzhao Heng, Xiang Chen, Tong Yu, Lingkai Kong, Sudheer Chava, Chao Zhang', 'link': 'https://arxiv.org/abs/2510.12121', 'abstract': "Precise attribute intensity control--generating Large Language Model (LLM) outputs with specific, user-defined attribute intensities--is crucial for AI systems adaptable to diverse user expectations. Current LLM alignment methods, however, typically provide only directional or open-ended guidance, failing to reliably achieve exact attribute intensities. We address this limitation with three key designs: (1) reformulating precise attribute intensity control as a target-reaching problem, rather than simple maximization; (2) training a lightweight value function via temporal-difference learning to predict final attribute intensity scores from partial generations, thereby steering LLM outputs; and (3) employing gradient-based interventions on hidden representations to navigate the model precisely towards specific attribute intensity targets. Our method enables fine-grained, continuous control over attribute intensities, moving beyond simple directional alignment. Experiments on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text generation to user-specified attribute intensities with high accuracy. Finally, we demonstrate efficiency enhancements across three downstream tasks: preference data synthesis, Pareto frontier approximation and optimization, and distillation of aligned behaviors for intervention-free inference. Our code is available on this https URL", 'abstract_zh': '精确属性强度控制——根据用户定义的具体属性强度生成大型语言模型（LLM）输出——对于适应多样化用户期望的AI系统至关重要。然而，当前的LLM对齐方法通常只能提供方向性或开放性的指导，无法可靠地实现精确的属性强度。我们通过以下三种关键设计解决了这一局限性：（1）将精确属性强度控制重新表述为一个目标抵达问题，而不是简单的最大化；（2）通过时差学习训练一个轻量级价值函数，以预测最终属性强度分数，从而引导LLM输出；（3）使用基于梯度的干预手段作用于隐藏表示，以精确引导模型朝向特定属性强度目标。我们的方法使属性强度控制具有细粒度和连续性，超越了简单的方向对齐。在对LLaMA-3.2-3b和Phi-4-mini进行的实验中，验证了我们的方法能够以高精度引导文本生成至用户指定的属性强度。最后，我们展示了在三个下游任务中的效率提升：偏好数据合成、帕累托前沿逼近与优化以及干预自由推理的对齐行为蒸馏。', 'title_zh': '大型语言模型中目标表示编辑实现精确属性强度控制'}
{'arxiv_id': 'arXiv:2510.12080', 'title': 'Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models', 'authors': 'Rabimba Karanjai, Yang Lu, Ranjith Chodavarapu, Lei Xu, Weidong Shi', 'link': 'https://arxiv.org/abs/2510.12080', 'abstract': "The rapid advancement of large language model (LLM) technology has led to diverse applications, many of which inherently require randomness, such as stochastic decision-making, gaming, scheduling, AI agents, and cryptography-related tasks. However, the capabilities of LLMs in handling randomness, particularly in generating and utilizing random numbers effectively, remain unclear. This paper investigates the capacity of LLMs for handling tasks that involve randomness through a series of experiments. We designed a set of experiments that consider various factors that can influence an LLM's performance in tasks involving randomness, such as accessibility to external tools, types of tasks, model states (fresh vs. non-fresh), and prompting strategies. The experiments cover a range of tasks, including generating random numbers, generating random strings such as passwords, shuffling items, and evaluating the quality of randomness using entropy and the NIST randomness test-suite. Our findings reveal that while LLMs can generate outputs that exhibit some degree of randomness, their performance is inconsistent and often deviates significantly from the expected behavior. The analysis of the experimental results highlights key limitations and areas where improvement is needed for the LLMs to effectively handle tasks involving randomness", 'abstract_zh': '大规模语言模型（LLM）技术的迅速发展带来了多样化的应用，其中许多应用本质上需要随机性，如随机决策、游戏、调度、AI代理和加密相关任务。然而，LLMs在处理随机性方面的能力，特别是在生成和有效利用随机数方面的能力仍然不够清晰。本文通过一系列实验研究了LLMs处理涉及随机性的任务的能力。我们设计了一系列实验，考虑了可能影响LLMs在涉及随机性任务中表现的各种因素，如对外部工具的访问性、任务类型、模型状态（新鲜模型 vs. 非新鲜模型）和提示策略。实验涵盖了生成随机数、生成随机字符串（如密码）、打乱项目以及使用熵和NIST随机性测试套件评估随机性质量等一系列任务。我们的研究发现，尽管LLMs能够生成具有一定随机性的输出，但其表现不一，往往与预期行为有显著偏差。实验结果的分析揭示了LLMs在有效处理涉及随机性的任务方面存在的关键局限性和需要改进的领域。', 'title_zh': '评估大型语言模型支持的任务中随机性和熵的质量'}
{'arxiv_id': 'arXiv:2510.12067', 'title': 'HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory', 'authors': 'Junyi Xie, Yuankun Jiao, Jina Kim, Yao-Yi Chiang, Lingyi Zhao, Khurram Shafique', 'link': 'https://arxiv.org/abs/2510.12067', 'abstract': "Inferring demographic attributes such as age, sex, or income level from human mobility patterns enables critical applications such as targeted public health interventions, equitable urban planning, and personalized transportation services. Existing mobility-based demographic inference studies heavily rely on large-scale trajectory data with demographic labels, leading to limited interpretability and poor generalizability across different datasets and user groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs' zero-shot learning and semantic understanding capabilities to perform demographic inference without labeled training data. HiCoTraj transforms trajectories into semantically rich, natural language representations by creating detailed activity chronicles and multi-scale visiting summaries. Then HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically guide LLMs through three cognitive stages: factual feature extraction, behavioral pattern analysis, and demographic inference with structured output. This approach addresses the scarcity challenge of labeled demographic data while providing transparent reasoning chains. Experimental evaluation on real-world trajectory data demonstrates that HiCoTraj achieves competitive performance across multiple demographic attributes in zero-shot scenarios.", 'abstract_zh': '从人类移动模式推断人口统计属性（如年龄、性别或收入水平） enables 诸如针对性的公共卫生干预、公平的城市规划和个人化交通服务等关键应用。现有的基于移动性的人口统计推断研究严重依赖带有人口统计标签的大规模轨迹数据，导致解释性和在不同数据集和用户群体中的泛化性有限。我们提出了HiCoTraj（基于轨迹的零样本人口统计推理通过层次链式思考提示），该框架利用大语言模型的零样本学习和语义理解能力，在无需标注训练数据的情况下进行人口统计推断。HiCoTraj 将轨迹转换为语义丰富、自然语言表示的形式，通过创建详细活动年表和多尺度访问摘要。然后，HiCoTraj 使用新颖的层次链式推理进行系统的认知引导，依次经过事实特征提取、行为模式分析和结构化输出的人口统计推断。这种 approach 解决了标注人口统计数据稀缺的问题，同时提供透明的推理链。实验在真实世界轨迹数据上的评估结果表明，HiCoTraj 在零样本场景中实现了多个人口统计属性的竞争力表现。', 'title_zh': 'HiCoTraj：基于轨迹的层次链式思考零样本 demographic 推理'}
{'arxiv_id': 'arXiv:2510.12063', 'title': 'ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization', 'authors': 'Sunzhu Li, Zhiyu Lin, Shuling Yang, Jiale Zhao, Wei Chen', 'link': 'https://arxiv.org/abs/2510.12063', 'abstract': "Large Reasoning Models (LRMs) are powerful, but they still suffer from inefficient and off-target reasoning. Currently, training-free methods are limited to either rigid heuristics or descriptive, non-actionable analyses. In this paper, we introduce ThinkPilot, a training-free framework that automatically optimizes LRMs reasoning. It uses an evolutionary process to generate think-prefixes, which are instructions that evolve driven by a taxonomy of reasoning behaviors to guide models toward superior performance. Extensive experiments demonstrate ThinkPilot's broad effectiveness: it significantly improves the accuracy-length trade-off for efficient reasoning, drastically improves safety (for example, cutting the StrongREJECT score of DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction following. It also synergizes with existing training-based methods. Our analysis reveals that think-prefixes can reliably control LRMs' reasoning behaviors, and that different tasks have strong preferences for specific behavioral distributions. By automatically identifying and eliciting these behaviors, ThinkPilot provides a generalizable framework for aligning LRMs reasoning with task demands. Data and code are available at this https URL", 'abstract_zh': '无需翻译标题，以下是翻译后的内容：\n\n大型推理模型（LRMs）非常强大，但仍存在低效和偏离目标的推理问题。目前，无需训练的方法要么局限于刚性的启发式规则，要么是描述性的、不可行的分析。本文引入了ThinkPilot，这是一种无需训练的框架，可以自动优化LRMs的推理。它使用进化过程生成think-prefixes，这些指示符根据推理行为的分类学发展，以引导模型向更优的性能方向发展。广泛的实验证明ThinkPilot的有效性：它显著改善了高效推理的准确性和长度之间的权衡，大幅提高了安全性（例如，将DeepSeek-R1-Distill-Qwen-32B的StrongREJECT得分从27.0%降至0.7），增强了指令遵循，并与现有的基于训练的方法协同工作。我们的分析表明，think-prefixes可以可靠地控制LRMs的推理行为，不同的任务对特定行为分布有强烈偏好。通过自动识别并激发这些行为，ThinkPilot提供了一种泛化的框架，以使LRMs的推理与任务需求相一致。相关数据和代码可在以下链接获取：this https URL', 'title_zh': 'ThinkPilot: 通过自动化Think前缀优化引导推理模型'}
{'arxiv_id': 'arXiv:2510.12061', 'title': 'Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response', 'authors': 'Yiheng Chen, Lingyao Li, Zihui Ma, Qikai Hu, Yilun Zhu, Min Deng, Runlong Yu', 'link': 'https://arxiv.org/abs/2510.12061', 'abstract': 'Effective disaster response is essential for safeguarding lives and property. Existing statistical approaches often lack semantic context, generalize poorly across events, and offer limited interpretability. While Large language models (LLMs) provide few-shot generalization, they remain text-bound and blind to geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL) that grounds LLM agents in structured earth data. Starting from raw wildfire detections, GAL automatically retrieves and integrates infrastructure, demographic, terrain, and weather information from external geodatabases, assembling them into a concise, unit-annotated perception script. This enriched context enables agents to produce evidence-based resource-allocation recommendations (e.g., personnel assignments, budget allocations), further reinforced by historical analogs and daily change signals for incremental updates. We evaluate the framework in real wildfire scenarios across multiple LLM models, showing that geospatially grounded agents can outperform baselines. The proposed framework can generalize to other hazards such as floods and hurricanes.', 'abstract_zh': '有效的灾害响应对于保护生命和财产至关重要。现有的统计方法往往缺乏语义上下文，跨事件泛化能力差，且缺乏可解释性。虽然大型语言模型（LLMs）提供了少样本泛化能力，但它们仍然局限于文本，对地理信息视而不见。为弥补这一差距，我们提出了一种地理意识层（Geospatial Awareness Layer，GAL），使LLM代理扎根于结构化的地球数据中。从原始的野火检测开始，GAL能够自动检索和集成基础设施、人口统计、地形和气象信息，并将其组装成精炼且带有单位标注的感知脚本。这种丰富的上下文使代理能够生成基于证据的资源分配建议（例如，人员分配、预算分配），并通过历史类比和日常变化信号进行增量更新。我们在多个LLM模型的多个实际野火场景中评估了该框架，结果显示地理扎根代理能够超越基线。所提出的框架可以泛化到其他灾害类型，如洪水和飓风。', 'title_zh': '增强LLM代理的地理空间意识：面向野火响应的接地推理'}
{'arxiv_id': 'arXiv:2510.12047', 'title': 'Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation', 'authors': 'Soohan Lim, Joonghyuk Hahn, Hyunwoo Park, Sang-Ki Ko, Yo-Sub Han', 'link': 'https://arxiv.org/abs/2510.12047', 'abstract': "Prevailing code generation benchmarks, such as HumanEval+ and MBPP+, primarily evaluate large language models (LLMs) with pass@k on functional correctness using well-formed inputs. However, they ignore a crucial aspect of real-world software: adherence to contracts-the preconditions and validity constraints that dictate how ill-formed inputs must be rejected. This critical oversight means that existing benchmarks fail to measure, and models consequently fail to generate, truly robust and reliable code snippets. We introduce PACT, a program assessment and contract-adherence evaluation framework, to bridge this gap. PACT is the first framework designed to systematically evaluate and enhance contract-adherence in LLM-generated code snippets alongside functional correctness. PACT's contributions are threefold: First, it provides a comprehensive test-suite corpus focused on contract violations, extending HumanEval+ and MBPP+. Second, it enables a systematic analysis of code generation under varied prompting conditions. This analysis demonstrates that augmenting prompts with contract-violating test cases significantly enhance a model's ability to respect contracts compared to using contract description alone. Finally, it introduces novel metrics to rigorously quantify contract adherence in both test generation and code generation. By revealing critical errors that conventional benchmarks overlook, PACT provides the rigorous and interpretable metrics to evaluate the robustness of LLM-generated code snippets in both functionality and this http URL code and data are available at this https URL.", 'abstract_zh': 'Prevailing代码生成基准（如HumanEval+和MBPP+）主要通过规范输入评估大规模语言模型（LLMs）的功能正确性，但忽略了现实世界软件中至关重要的一个方面：合同遵守问题——决定如何拒绝不规范输入的前提条件和有效性约束。这一关键遗漏意味着现有的基准无法衡量模型生成真正 robust 和可靠的代码片段的能力。为此，我们引入了PACT，这是一种程序评估和合同遵守评估框架，旨在弥合这一缺口。PACT是第一个旨在系统评估和增强LLM生成代码片段中合同遵守性的框架，同时保持功能正确性。PACT的贡献包括三个方面：首先，它提供了一个专注于合同违规的全面测试套件库，扩展了HumanEval+和MBPP+。其次，它使在不同提示条件下系统的代码生成分析成为可能。这项分析表明，通过补充包含合同违规测试案例的提示，可以显著提高模型遵守合同的能力，与仅使用合同描述相比。最后，它引入了新的度量标准，以严格量化测试生成和代码生成中的合同遵守情况。通过揭示传统基准所忽略的关键错误，PACT 提供了严格的可解释度量标准来评估LLM生成的代码片段的功能性和合同遵守性。相关代码和数据可在以下链接获取：this https URL。', 'title_zh': '大语言模型遵守协议吗？评估与强制执行代码生成的协议一致性'}
{'arxiv_id': 'arXiv:2510.12015', 'title': 'Asking Clarifying Questions for Preference Elicitation With Large Language Models', 'authors': 'Ali Montazeralghaem, Guy Tennenholtz, Craig Boutilier, Ofer Meshi', 'link': 'https://arxiv.org/abs/2510.12015', 'abstract': "Large Language Models (LLMs) have made it possible for recommendation systems to interact with users in open-ended conversational interfaces. In order to personalize LLM responses, it is crucial to elicit user preferences, especially when there is limited user history. One way to get more information is to present clarifying questions to the user. However, generating effective sequential clarifying questions across various domains remains a challenge. To address this, we introduce a novel approach for training LLMs to ask sequential questions that reveal user preferences. Our method follows a two-stage process inspired by diffusion models. Starting from a user profile, the forward process generates clarifying questions to obtain answers and then removes those answers step by step, serving as a way to add ``noise'' to the user profile. The reverse process involves training a model to ``denoise'' the user profile by learning to ask effective clarifying questions. Our results show that our method significantly improves the LLM's proficiency in asking funnel questions and eliciting user preferences effectively.", 'abstract_zh': '大型语言模型（LLMs）使得推荐系统能够在开放式的对话界面与用户互动成为可能。为了个性化LLM响应，特别是在用户历史有限的情况下，提取用户偏好至关重要。一种获取更多信息的方法是对用户提出澄清性问题。然而，跨不同领域生成有效的序列化澄清性问题仍然具有挑战性。为解决这一问题，我们提出了一种新的方法来训练LLMs以提问能揭示用户偏好的序列化问题。我们的方法采用了一种受到扩散模型启发的两阶段过程。从用户资料开始，前向过程生成澄清性问题以获取答案，然后逐步删除这些答案，这种方式为用户资料添加“噪声”。反向过程涉及训练模型以“去噪”用户资料，学习有效提出澄清性问题。我们的结果表明，我们的方法显著提高了LLM在提问漏斗问题和有效提取用户偏好方面的能力。', 'title_zh': '使用大型语言模型进行偏好采集的澄清性问题询问方法'}
{'arxiv_id': 'arXiv:2510.11985', 'title': 'CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics Research', 'authors': 'Owen Queen, Harrison G. Zhang, James Zou', 'link': 'https://arxiv.org/abs/2510.11985', 'abstract': 'Variant and gene interpretation are fundamental to personalized medicine and translational biomedicine. However, traditional approaches are manual and labor-intensive. Generative language models (LMs) can facilitate this process, accelerating the translation of fundamental research into clinically-actionable insights. While existing benchmarks have attempted to quantify the capabilities of LMs for interpreting scientific data, these studies focus on narrow tasks that do not translate to real-world research. To meet these challenges, we introduce CGBench, a robust benchmark that tests reasoning capabilities of LMs on scientific publications. CGBench is built from ClinGen, a resource of expert-curated literature interpretations in clinical genetics. CGBench measures the ability to 1) extract relevant experimental results following precise protocols and guidelines, 2) judge the strength of evidence, and 3) categorize and describe the relevant outcome of experiments. We test 8 different LMs and find that while models show promise, substantial gaps exist in literature interpretation, especially on fine-grained instructions. Reasoning models excel in fine-grained tasks but non-reasoning models are better at high-level interpretations. Finally, we measure LM explanations against human explanations with an LM judge approach, revealing that models often hallucinate or misinterpret results even when correctly classifying evidence. CGBench reveals strengths and weaknesses of LMs for precise interpretation of scientific publications, opening avenues for future research in AI for clinical genetics and science more broadly.', 'abstract_zh': 'CGBench：面向临床遗传学的科学出版物推理能力基准', 'title_zh': 'CGBench: 语言模型在临床遗传学研究中科学推理能力的基准测试'}
{'arxiv_id': 'arXiv:2510.11822', 'title': 'Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations', 'authors': 'Suryaansh Jain, Umair Z. Ahmed, Shubham Sahai, Ben Leong', 'link': 'https://arxiv.org/abs/2510.11822', 'abstract': 'New Large Language Models (LLMs) become available every few weeks, and modern application developers confronted with the unenviable task of having to decide if they should switch to a new model. While human evaluation remains the gold standard, it is costly and unscalable. The state-of-the-art approach is to use LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw: LLMs exhibit a strong positive bias. We provide empirical evidence showing that while LLMs can identify valid outputs with high accuracy (i.e., True Positive Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True Negative Rate <25%). This systematic bias, coupled with class imbalance, often leads to inflated reliability scores.\nWhile ensemble-based methods like majority voting can help, we show that they are not good enough. We introduce an optimal minority-veto strategy that is resilient to missing data and mitigates this bias to a large extent. For scenarios requiring even higher precision, we propose a novel regression-based framework that directly models the validator bias using a small set of human-annotated ground truth data. On a challenging code feedback task over 366 high-school Python programs, our regression approach reduces the maximum absolute error to just 1.2%, achieving a 2x improvement over the best-performing ensemble of 14 state-of-the-art LLMs.', 'abstract_zh': '新的大规模语言模型（LLMs）每隔几周就会出现，现代应用程序开发者面临不得不决定是否应切换到新模型的艰巨任务。虽然人类评估仍然是黄金标准，但成本高且不具扩展性。当前最先进的方法是使用LLMs作为评估器（LLM-as-a-judge），但这种方法存在一个关键缺陷：LLMs表现出强烈的正向偏见。我们提供了实证证据显示，尽管LLMs可以以高精度（即真正阳性率96%）识别有效的输出，但在识别无效输出方面表现极差（即真正阴性率<25%）。这种系统性偏见，加上类别不平衡，往往会导致可靠性评分被夸大。\n虽然传统的聚合方法如多数投票可以有所帮助，但我们证明它们并不足够。我们引入了一种最优的少数票否决策略，能够在很大程度上抵御缺失数据并缓解这一偏见。对于需要更高精度的场景，我们提出了一种新的基于回归的框架，可以直接使用一小部分人工注释的 ground truth 数据来建模验证者的偏见。在一项针对366个高中Python程序的具有挑战性的代码反馈任务中，我们的回归方法将最大绝对误差降低到仅1.2%，相较于14个最先进的LLM的最佳组合，实现了2倍的改进。', 'title_zh': '超越共识：减轻LLM法官评价中的随和性偏见'}
{'arxiv_id': 'arXiv:2510.12773', 'title': 'Dr.LLM: Dynamic Layer Routing in LLMs', 'authors': 'Ahmed Heakl, Martin Gubri, Salman Khan, Sangdoo Yun, Seong Joon Oh', 'link': 'https://arxiv.org/abs/2510.12773', 'abstract': 'Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. We introduce this http URL, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), this http URL improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, this http URL shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights.', 'abstract_zh': '动态路由层：为LLMs配备可微路由框架以实现预算意识下的高效推理', 'title_zh': 'Dr.LLM: LLMs中的动态层路由'}
{'arxiv_id': 'arXiv:2510.12740', 'title': 'Hey, wait a minute: on at-issue sensitivity in Language Models', 'authors': 'Sanghee J. Kim, Kanishka Misra', 'link': 'https://arxiv.org/abs/2510.12740', 'abstract': 'Evaluating the naturalness of dialogue in language models (LMs) is not trivial: notions of \'naturalness\' vary, and scalable quantitative metrics remain limited. This study leverages the linguistic notion of \'at-issueness\' to assess dialogue naturalness and introduces a new method: Divide, Generate, Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii) generates continuations for subparts using LMs, (iii) recombines the dialogue and continuations, and (iv) compares the likelihoods of the recombined sequences. This approach mitigates bias in linguistic analyses of LMs and enables systematic testing of discourse-sensitive behavior. Applying DGRC, we find that LMs prefer to continue dialogue on at-issue content, with this effect enhanced in instruct-tuned models. They also reduce their at-issue preference when relevant cues (e.g., "Hey, wait a minute") are present. Although instruct-tuning does not further amplify this modulation, the pattern reflects a hallmark of successful dialogue dynamics.', 'abstract_zh': '评估语言模型对话的自然度并非易事：自然度的定义各不相同，且可扩展的量化指标仍然有限。本研究利用语义上的“相关性”概念来评估对话的自然度，并引入了一种新方法：分割、生成、重组和对比（DGRC）。DGRC 方法包括：(i) 将对话作为提示进行分割，(ii) 使用语言模型生成子部分的续作，(iii) 重组对话与续作，(iv) 对重组序列的可能性进行比较。该方法减少了语言模型语言分析中的偏见，并使对话敏感行为的系统测试成为可能。应用DGRC，我们发现语言模型倾向于在与议题相关的内容上继续对话，这种效应在指令微调模型中更为显著。当相关提示（例如，“等等，稍等一下”）存在时，它们也会减少对与议题相关性的偏好。尽管指令微调没有进一步放大这种调节，但这一模式体现了成功对话动态的特征。', 'title_zh': '嘿，请等一下：关于语言模型的议题相关敏感性'}
{'arxiv_id': 'arXiv:2510.12712', 'title': 'Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning', 'authors': 'Xingang Guo, Utkarsh Tyagi, Advait Gosai, Paula Vergara, Ernesto Gabriel Hernández Montoya, Chen Bo Calvin Zhang, Bin Hu, Yunzhong He, Bing Liu, Rakshith Sharma Srinivasa', 'link': 'https://arxiv.org/abs/2510.12712', 'abstract': "Multimodal Large Language Models (MLLMs) are increasingly applied in real-world scenarios where user-provided images are often imperfect, requiring active image manipulations such as cropping, editing, or enhancement to uncover salient visual cues. Beyond static visual perception, MLLMs must also think with images: dynamically transforming visual content and integrating it with other tools to solve complex tasks. However, this shift from treating vision as passive context to a manipulable cognitive workspace remains underexplored. Most existing benchmarks still follow a think about images paradigm, where images are regarded as static inputs. To address this gap, we introduce IRIS, an Interactive Reasoning with Images and Systems that evaluates MLLMs' ability to perceive, transform, and reason across complex visual-textual tasks under the think with images paradigm. IRIS comprises 1,204 challenging, open-ended vision tasks (603 single-turn, 601 multi-turn) spanning across five diverse domains, each paired with detailed rubrics to enable systematic evaluation. Our evaluation shows that current MLLMs struggle with tasks requiring effective integration of vision and general-purpose tools. Even the strongest model (GPT-5-think) reaches only 18.68% pass rate. We further observe divergent tool-use behaviors, with OpenAI models benefiting from diverse image manipulations while Gemini-2.5-pro shows no improvement. By introducing the first benchmark centered on think with images, IRIS offers critical insights for advancing visual intelligence in MLLMs.", 'abstract_zh': '交互式图像与系统推理（IRIS）： multimodal large language models在think with images paradigm下的感知、变换与推理能力评估', 'title_zh': '超越视觉：基于工具的图像感知、变换与推理多模态LLM评估'}
{'arxiv_id': 'arXiv:2510.12702', 'title': 'Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?', 'authors': 'Cedric Richter, Heike Wehrheim', 'link': 'https://arxiv.org/abs/2510.12702', 'abstract': 'Automatic software verifiers have become increasingly effective at the task of checking software against (formal) specifications. Yet, their adoption in practice has been hampered by the lack of such specifications in real world code. Large Language Models (LLMs) have shown promise in inferring formal postconditions from natural language hints embedded in code such as function names, comments or documentation. Using the generated postconditions as specifications in a subsequent verification, however, often leads verifiers to suggest invalid inputs, hinting at potential issues that ultimately turn out to be false alarms.\nTo address this, we revisit the problem of specification inference from natural language in the context of automatic software verification. In the process, we introduce NL2Contract, the task of employing LLMs to translate informal natural language into formal functional contracts, consisting of postconditions as well as preconditions. We introduce metrics to validate and compare different NL2Contract approaches, using soundness, bug discriminative power of the generated contracts and their usability in the context of automatic software verification as key metrics. We evaluate NL2Contract with different LLMs and compare it to the task of postcondition generation nl2postcond. Our evaluation shows that (1) LLMs are generally effective at generating functional contracts sound for all possible inputs, (2) the generated contracts are sufficiently expressive for discriminating buggy from correct behavior, and (3) verifiers supplied with LLM inferred functional contracts produce fewer false alarms than when provided with postconditions alone. Further investigations show that LLM inferred preconditions generally align well with developers intentions which allows us to use automatic software verifiers to catch real-world bugs.', 'abstract_zh': '自动软件验证器在根据形式规范检查软件方面的有效性不断提高，但在实践中其采用受到实际代码中缺乏形式规范的限制。大型语言模型（LLMs）展示了从嵌入在代码中的自然语言提示（如函数名、注释或文档）中推断形式后条件的潜力。然而，使用生成的后条件作为后续验证的规格通常会导致验证器建议无效输入，暗示潜在的问题最终证明是误报。\n\n为解决这一问题，我们在自动软件验证的背景下重新审视了从自然语言推断规范的问题。在此过程中，我们引入了NL2Contract任务，即将LLMs用于将非正式自然语言翻译为正式的功能合同，包括后条件和前置条件。我们引入了评估和比较不同NL2Contract方法的度量标准，将规范的有效性、生成的规范对错误的区分能力和在自动软件验证中的易用性作为关键度量标准。我们使用不同的LLMs评估了NL2Contract，并将其与后条件生成nl2postcond任务进行了比较。评估结果显示：（1）LLMs通常能有效地生成适用于所有输入的功能合同；（2）生成的合同足以区分错误行为和正确行为；（3）配以LLMs推断的功能合同的验证器产生的误报少于仅提供后条件的情况。进一步的研究表明，LLMs推断的前置条件通常与开发者的意图一致，允许我们使用自动软件验证器捕获实际存在的bug。', 'title_zh': '超越后条件：大型语言模型能否推断出形式契约以实现自动软件验证？'}
{'arxiv_id': 'arXiv:2510.12699', 'title': 'Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations', 'authors': 'Sunny Yu, Ahmad Jabbar, Robert Hawkins, Dan Jurafsky, Myra Cheng', 'link': 'https://arxiv.org/abs/2510.12699', 'abstract': "Different open-ended generation tasks require different degrees of output diversity. However, current LLMs are often miscalibrated. They collapse to overly homogeneous outputs for creative tasks and hallucinate diverse but incorrect responses for factual tasks. We argue that these two failure modes are unified by, and can both be addressed by, the notion of effective generation space size (GSS) -- the set of semantically distinct outputs a model considers for a prompt. We present GSSBench, a task suite of prompt pairs with ground-truth GSS relationships to assess different metrics and understand where models diverge from desired behavior. We find that hallucination detection metrics, particularly EigenScore, consistently outperform standard diversity and uncertainty quantification metrics, while using only model internals, providing interpretable insights into a model's internal task representations. We demonstrate three applications of GSS: (1) detecting prompt ambiguity and predicting clarification questions for better grounding, (2) interpreting overthinking and underthinking in reasoning models, and (3) steering models to expand their generation space to yield high-quality and diverse outputs.", 'abstract_zh': '不同的开放生成任务需要不同程度的输出多样性。然而，当前的大语言模型常常失配。它们在创造性任务中产生过度同质化的输出，在事实性任务中则虚构多样但错误的响应。我们argue这两种失败模式可以通过有效的生成空间大小（GSS）这一概念来统一和解决——GSS是指模型针对某个提示考虑的语义上不同的输出集合。我们提出了GSSBench，这是一个由具有真实GSS关系的提示对组成的任务套件，用于评估不同的度量标准并理解模型与期望行为之间的差异。我们发现，尤其是EigenScore的幻觉检测度量标准在仅使用模型内部信息的情况下，始终优于标准的多样性和不确定性量化度量标准，提供了对模型内部任务表示可解释的洞察。我们展示了GSS的三种应用：（1）检测提示模糊性并预测澄清问题以提高语义关联，（2）解释推理模型中的过度思考和思考不足，以及（3）引导模型扩大生成空间以产生高质量和多样化的输出。', 'title_zh': '生成空间大小：理解并校准LLM生成的开放性'}
{'arxiv_id': 'arXiv:2510.12689', 'title': 'From Delegates to Trustees: How Optimizing for Long-Term Interests Shapes Bias and Alignment in LLM', 'authors': 'Suyash Fulay, Jocelyn Zhu, Michiel Bakker', 'link': 'https://arxiv.org/abs/2510.12689', 'abstract': "Large language models (LLMs) have shown promising accuracy in predicting survey responses and policy preferences, which has increased interest in their potential to represent human interests in various domains. Most existing research has focused on behavioral cloning, effectively evaluating how well models reproduce individuals' expressed preferences. Drawing on theories of political representation, we highlight an underexplored design trade-off: whether AI systems should act as delegates, mirroring expressed preferences, or as trustees, exercising judgment about what best serves an individual's interests. This trade-off is closely related to issues of LLM sycophancy, where models can encourage behavior or validate beliefs that may be aligned with a user's short-term preferences, but is detrimental to their long-term interests. Through a series of experiments simulating votes on various policy issues in the U.S. context, we apply a temporal utility framework that weighs short and long-term interests (simulating a trustee role) and compare voting outcomes to behavior-cloning models (simulating a delegate). We find that trustee-style predictions weighted toward long-term interests produce policy decisions that align more closely with expert consensus on well-understood issues, but also show greater bias toward models' default stances on topics lacking clear agreement. These findings reveal a fundamental trade-off in designing AI systems to represent human interests. Delegate models better preserve user autonomy but may diverge from well-supported policy positions, while trustee models can promote welfare on well-understood issues yet risk paternalism and bias on subjective topics.", 'abstract_zh': '大型语言模型（LLMs）在预测调查响应和政策偏好方面展示了有前途的准确性，这增加了对其在各个领域代表人类利益潜力的兴趣。现有大多数研究侧重于行为克隆，有效评估模型在多大程度上再现了个人表达的偏好。基于政治代表理论，我们强调了一个未充分探讨的设计权衡：AI系统应该作为代理，镜像表达的偏好，还是作为监护人，根据最有利于个人利益进行判断。这一权衡与大型语言模型的巴结行为密切相关，即模型可能会鼓励某些行为或验证可能与用户短期偏好一致但对其长期利益有害的信念。通过一系列模拟美国政策议题投票的实验，我们应用了一个时间效用框架，权衡短期和长期利益（模拟监护人角色），并将投票结果与行为克隆模型（模拟代理）进行比较。我们发现，倾向于长期利益的监护人式预测产生更符合专家共识的政策决策，但对缺乏明确共识的话题也表现出更大的偏好偏见。这些发现揭示了设计旨在代表人类利益的AI系统的基本权衡。代理模型更好地保护用户自主性，但可能偏离得到广泛支持的政策立场，而监护人模型可以在理解良好的议题上促进福祉，但在主观话题上可能有 paternalism 和偏见的风险。', 'title_zh': '从代理人到受托人：优化长期利益如何塑造LLM中的偏见与对齐'}
{'arxiv_id': 'arXiv:2510.12680', 'title': 'Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and No-Think?', 'authors': 'Shouren Wang, Wang Yang, Xianxuan Long, Qifan Wang, Vipin Chaudhary, Xiaotian Han', 'link': 'https://arxiv.org/abs/2510.12680', 'abstract': "Hybrid thinking enables LLMs to switch between reasoning and direct answering, offering a balance between efficiency and reasoning capability. Yet our experiments reveal that current hybrid thinking LLMs only achieve partial mode separation: reasoning behaviors often leak into the no-think mode. To understand and mitigate this, we analyze the factors influencing controllability and identify four that matter most: (1) larger data scale, (2) using think and no-think answers from different questions rather than the same question, (3) a moderate increase in no-think data number, and (4) a two-phase strategy that first trains reasoning ability and then applies hybrid think training. Building on these findings, we propose a practical recipe that, compared to standard training, can maintain accuracy in both modes while significantly reducing no-think output length (from $1085$ to $585$ on MATH500) and occurrences of reasoning-supportive tokens such as ``\\texttt{wait}'' (from $5917$ to $522$ on MATH500). Our findings highlight the limitations of current hybrid thinking and offer directions for strengthening its controllability.", 'abstract_zh': '混合推理使大语言模型能够在推理和直接回答之间切换，提供效率和推理能力之间的平衡。然而，我们的实验揭示当前的混合推理大语言模型仅部分实现了模式分离：推理行为常常渗入无思考模式中。为了理解并减轻这一现象，我们分析了影响可控性的因素，并确定了四个最相关因素：（1）更大的数据规模，（2）使用来自不同问题的思考和无思考回答而非同一问题的回答，（3）适度增加无思考数据的数量，以及（4）一个两阶段策略，先训练推理能力，再进行混合推理训练。基于这些发现，我们提出了一种实用的方法，与标准训练相比，该方法可以在两种模式下保持准确性，同时显著减少无思考输出长度（从MATH500的1085减少到585）和支持推理的标记如“wait”的出现频率（从MATH500的5917减少到522）。我们的研究突显了当前混合推理的局限性，并提供了增强其可控性的方向。', 'title_zh': '揭秘混合思维：大型语言模型真的能切换到不思考模式吗？'}
{'arxiv_id': 'arXiv:2510.12643', 'title': 'Reasoning Pattern Matters: Learning to Reason without Human Rationales', 'authors': 'Chaoxu Pang, Yixuan Cao, Ping Luo', 'link': 'https://arxiv.org/abs/2510.12643', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities under the widely adopted SFT+RLVR paradigm, which first performs Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories (rationales) to establish initial reasoning behaviors, then applies Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model using verifiable signals without golden rationales. However, annotating high-quality rationales for the SFT stage remains prohibitively expensive. This paper investigates when and how rationale annotation costs can be substantially reduced without compromising reasoning performance. We identify a broad class of problems, termed patterned reasoning tasks, where reasoning follows a fixed, procedural strategy consistent across instances. Although instances vary in content such as domain knowledge, factual information, or numeric values, the solution derives from applying a shared reasoning pattern. We argue that the success of SFT+RLVR on such tasks primarily stems from its ability to enable models to internalize these reasoning patterns. Using numerical semantic matching as a representative task, we provide both causal and behavioral evidence showing that reasoning patterns rather than the quantity or quality of rationales are the key determinant of performance. Building on these insights, we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet effective framework that enables LLMs to generate rationales aligned with task-specific reasoning patterns without requiring human rationale annotations. Experiments show that PARO-generated rationales achieve comparable SFT+RLVR performance to human rationales that are 10 times larger. These results suggest that large-scale human rationale annotations can be replaced with LLM-based automatic annotations requiring only limited human supervision over reasoning patterns.', 'abstract_zh': '大规模语言模型（LLMs）在广泛采用的SFT+RLVR范式下展示了卓越的推理能力，该范式首先通过人工标注的推理路径（合理性）进行监督微调（SFT），建立初步的推理行为，然后利用可验证奖励的强化学习（RLVR）优化模型，使用可验证的信号而不是金标准合理性来优化。然而，为SFT阶段标注高质量的合理性仍然代价高昂。本文研究了如何在不牺牲推理性能的情况下显著降低合理性标注成本。我们识别出一类问题，称为模式化推理任务，在这类任务中，推理遵循一种固定的、程序化的策略，且在实例之间保持一致。尽管实例在内容如领域知识、事实信息或数值方面有所不同，但解决方案源自应用共享的推理模式。我们认为，SFT+RLVR在这些任务上的成功主要归因于其使模型内化这些推理模式的能力。以数值语义匹配为代表任务，我们提供了因果和行为证据，表明推理模式而非合理性数量或质量是决定性能的关键因素。基于这些洞见，我们提出了一种模式感知大规模语言模型作为合理性注释器（PARO）的简单而有效的框架，使大规模语言模型能够在不需要人类合理性注释的情况下生成与任务特定推理模式对齐的合理性。实验结果显示，PARO生成的合理性与人类注释的合理性（大10倍）在SFT+RLVR性能上具有可比性。这些结果表明，大规模人工合理性注释可以被基于大规模语言模型的自动注释所取代，仅需少量的人类监督以确保推理模式正确。', 'title_zh': '推理模式matter：学习推理而不使用人类推理理由'}
{'arxiv_id': 'arXiv:2510.12633', 'title': 'Laminar: A Scalable Asynchronous RL Post-Training Framework', 'authors': 'Guangming Sheng, Yuxuan Tong, Borui Wan, Wang Zhang, Chaobo Jia, Xibin Wu, Yuqi Wu, Xiang Li, Chi Zhang, Yanghua Peng, Haibin Lin, Xin Liu, Chuan Wu', 'link': 'https://arxiv.org/abs/2510.12633', 'abstract': "Reinforcement learning (RL) post-training for Large Language Models (LLMs) is now scaling to large clusters and running for extended durations to enhance model reasoning performance. However, the scalability of existing RL frameworks is limited, as extreme long-tail skewness in RL trajectory generation causes severe GPU underutilization. Current asynchronous RL systems attempt to mitigate this, but they rely on global weight synchronization between the actor and all rollouts, which creates a rigid model update schedule. This global synchronization is ill-suited for the highly skewed and evolving distribution of trajectory generation latency in RL training, crippling training efficiency. Our key insight is that efficient scaling requires breaking this lockstep through trajectory-level asynchrony, which generates and consumes each trajectory independently. We propose Laminar, a scalable and robust RL post-training system built on a fully decoupled architecture. First, we replace global updates with a tier of relay workers acting as a distributed parameter service. This enables asynchronous and fine-grained weight synchronization, allowing rollouts to pull the latest weight anytime without stalling the actor's training loop. Second, a dynamic repack mechanism consolidates long-tail trajectories onto a few dedicated rollouts, maximizing generation throughput. The fully decoupled design also isolates failures, ensuring robustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows that Laminar achieves up to 5.48$\\times$ training throughput speedup over state-of-the-art systems, while reducing model convergence time.", 'abstract_zh': '大规模语言模型（LLMs）的训练后强化学习（RL）正在扩展到大型集群，并运行更长时间以提升模型推理性能。然而，现有RL框架的扩展性受到限制，因为RL轨迹生成中的极端长尾偏斜导致严重的GPU利用率低下。当前的异步RL系统试图解决这一问题，但它们依赖于演员与所有轨迹之间的全局权重同步，这创造了一个僵化的模型更新日程。这种全局同步不适用于RL训练中高度偏斜且不断变化的轨迹生成延迟分布，导致训练效率受损。我们的关键洞察是，高效的扩展需要通过轨迹级别的异步性来打破这种同步，以独立生成和消费每个轨迹。我们提出了Laminar，这是一种基于完全解耦架构的可扩展且健壯的训练后RL系统。首先，我们用一层relay工作者取代全局更新，这些relay工作者作为分布式参数服务运行。这使权重同步变为异步和细粒度的，使得卷集随时可以拉取最新权重而不阻碍演员的训练循环。其次，动态重新打包机制将长尾轨迹合并到少数专用卷集中，最大化生成吞吐量。完全解耦的设计还隔离了故障，确保了长时间运行作业的健壯性。在1024-GPU集群上的评估表明，Laminar相比最先进的系统实现了高达5.48倍的训练吞吐量加速，并缩短了模型收敛时间。', 'title_zh': 'Laminar：一种可扩展的异步RL后训练框架'}
{'arxiv_id': 'arXiv:2510.12608', 'title': 'StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis', 'authors': 'Siyuan Li, Aodu Wulianghai, Xi Lin, Guangyan Li, Xiang Chen, Jun Wu, Jianhua Li', 'link': 'https://arxiv.org/abs/2510.12608', 'abstract': 'With the increasing integration of large language models (LLMs) into open-domain writing, detecting machine-generated text has become a critical task for ensuring content authenticity and trust. Existing approaches rely on statistical discrepancies or model-specific heuristics to distinguish between LLM-generated and human-written text. However, these methods struggle in real-world scenarios due to limited generalization, vulnerability to paraphrasing, and lack of explainability, particularly when facing stylistic diversity or hybrid human-AI authorship. In this work, we propose StyleDecipher, a robust and explainable detection framework that revisits LLM-generated text detection using combined feature extractors to quantify stylistic differences. By jointly modeling discrete stylistic indicators and continuous stylistic representations derived from semantic embeddings, StyleDecipher captures distinctive style-level divergences between human and LLM outputs within a unified representation space. This framework enables accurate, explainable, and domain-agnostic detection without requiring access to model internals or labeled segments. Extensive experiments across five diverse domains, including news, code, essays, reviews, and academic abstracts, demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain accuracy. Moreover, in cross-domain evaluations, it surpasses existing baselines by up to 36.30%, while maintaining robustness against adversarial perturbations and mixed human-AI content. Further qualitative and quantitative analysis confirms that stylistic signals provide explainable evidence for distinguishing machine-generated text. Our source code can be accessed at this https URL.', 'abstract_zh': '大型语言模型在开放领域写作中的广泛应用促使检测机器生成文本成为确保内容真实性与信任的关键任务。现有方法依赖统计差异或模型特定的启发式方法来区分大型语言模型生成的文本和人工撰写的文本。然而，这些方法在实际场景中面临泛化能力有限、易受到改写影响以及缺乏解释性等问题，特别是在面对风格多样性或人机合作创作内容时。在本文中，我们提出StyleDecipher，一种稳健且可解释的检测框架，重新审视大型语言模型生成文本检测，通过结合特征提取器来量化风格差异。通过联合建模离散的风格指标和语义嵌入连续的风格表示，StyleDecipher在统一的表示空间中捕捉了人工和大型语言模型输出之间的风格级差异。该框架能够在无需访问模型内部结构或标注片段的情况下实现准确、可解释且领域无关的检测。在新闻、代码、随笔、评论和学术摘要等五个不同领域的广泛实验中，StyleDecipher展示了始终达到领域内最先进的准确率。此外，在跨领域评估中，它在保持对抗性扰动和混合人机内容的鲁棒性方面超过了现有基准方法高达36.30%。进一步的定性和定量分析确认了风格信号为区分机器生成文本提供了可解释的证据。我们的源代码可以通过这个链接访问。', 'title_zh': 'StyleDecipher: 基于风格分析的鲁棒且可解释的LLM生成文本检测方法'}
{'arxiv_id': 'arXiv:2510.12516', 'title': 'BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)', 'authors': 'Tomas Ruiz, Siyao Peng, Barbara Plank, Carsten Schwemmer', 'link': 'https://arxiv.org/abs/2510.12516', 'abstract': 'Test-time scaling is a family of techniques to improve LLM outputs at inference time by performing extra computation. To the best of our knowledge, test-time scaling has been limited to domains with verifiably correct answers, like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025 tasks to evaluate annotation disagreements. We experiment with three test-time scaling methods: two benchmark algorithms (Model Averaging and Majority Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM performance consistently on the LeWiDi tasks, but the Best-of-N method does not. Our experiments suggest that the Best-of-N method does not currently transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for this gap.', 'abstract_zh': '测试时缩放是一种在推理时通过额外计算提高LLM输出的技术。据我们所知，测试时缩放主要应用于具有可验证正确答案的领域，如数学和编程。我们将在LeWiDi-2025任务中转移测试时缩放技术以评估注释 disagreements。我们实验了三种测试时缩放方法：两个基准算法（模型平均和多数表决），以及一种Best-of-N采样方法。两种基准方法在LeWiDi任务中一致地提高了LLM性能，但Best-of-N方法未做到这一点。我们的实验表明，Best-of-N方法当前无法从数学任务转移到LeWiDi任务，并分析了这一差距的原因。', 'title_zh': 'BoN Appetit Team在LeWiDi-2025：最佳-of-N测试时扩展无法容忍标注分歧（目前尚且无法接受）'}
{'arxiv_id': 'arXiv:2510.12476', 'title': 'When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection', 'authors': 'Lang Gao, Xuhui Li, Chenxi Wang, Mingzhe Li, Wei Liu, Zirui Song, Jinghui Zhang, Rui Yan, Preslav Nakov, Xiuying Chen', 'link': 'https://arxiv.org/abs/2510.12476', 'abstract': 'Large language models (LLMs) have grown more powerful in language generation, producing fluent text and even imitating personal style. Yet, this ability also heightens the risk of identity impersonation. To the best of our knowledge, no prior work has examined personalized machine-generated text (MGT) detection. In this paper, we introduce \\dataset, the first benchmark for evaluating detector robustness in personalized settings, built from literary and blog texts paired with their LLM-generated imitations. Our experimental results demonstrate large performance gaps across detectors in personalized settings: some state-of-the-art models suffer significant drops. We attribute this limitation to the \\textit{feature-inversion trap}, where features that are discriminative in general domains become inverted and misleading when applied to personalized text. Based on this finding, we propose \\method, a simple and reliable way to predict detector performance changes in personalized settings. \\method identifies latent directions corresponding to inverted features and constructs probe datasets that differ primarily along these features to evaluate detector dependence. Our experiments show that \\method can accurately predict both the direction and the magnitude of post-transfer changes, showing 85\\% correlation with the actual performance gaps. We hope that this work will encourage further research on personalized text detection.', 'abstract_zh': '大型语言模型（LLMs）在语言生成方面变得更为强大，能够生成流畅的文本甚至模仿个人风格。然而，这种能力也加剧了身份冒充的风险。据我们所知，尚未有先有研究探讨个性化机器生成文本（MGT）的检测问题。在本文中，我们介绍了第一个用于评估个性化设置中检测器鲁棒性的基准 \\dataset，该基准基于文学和博客文本及其由LLM生成的仿制品构建。我们的实验结果表明，在个性化设置中，不同检测器之间的性能存在巨大差距：一些最先进的模型出现了显著下降。我们将这一限制归因于“特征反转陷阱”，即在通用领域具有区分性的特征在应用于个性化文本时会变得颠倒并误导人。基于这一发现，我们提出了 \\method，这是一种简单且可靠的方法，用于预测个性化设置中检测器性能的变化。\\method 识别与反转特征对应的潜在方向，并构建主要沿这些特征差异的探针数据集，以评估检测器的依赖性。我们的实验表明，\\method 可以准确预测转移后的方向和幅度变化，与实际性能差距的相关性高达85%。我们希望这项工作能促进个性化文本检测方面的进一步研究。', 'title_zh': '当个性化欺骗检测器：机器生成文本检测中的特征反转陷阱'}
{'arxiv_id': 'arXiv:2510.12389', 'title': 'Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency', 'authors': 'Hailay Kidu Teklehaymanot, Wolfgang Nejdl', 'link': 'https://arxiv.org/abs/2510.12389', 'abstract': 'Tokenization disparities pose a significant barrier to achieving equitable access to artificial intelligence across linguistically diverse populations. This study conducts a large-scale cross-linguistic evaluation of tokenization efficiency in over 200 languages to systematically quantify computational inequities in large language models (LLMs). Using a standardized experimental framework, we applied consistent preprocessing and normalization protocols, followed by uniform tokenization through the tiktoken library across all language samples. Comprehensive tokenization statistics were collected using established evaluation metrics, including Tokens Per Sentence (TPS) and Relative Tokenization Cost (RTC), benchmarked against English baselines. Our cross-linguistic analysis reveals substantial and systematic disparities: Latin-script languages consistently exhibit higher tokenization efficiency, while non-Latin and morphologically complex languages incur significantly greater token inflation, often 3-5 times higher RTC ratios. These inefficiencies translate into increased computational costs and reduced effective context utilization for underrepresented languages. Overall, the findings highlight structural inequities in current AI systems, where speakers of low-resource and non-Latin languages face disproportionate computational disadvantages. Future research should prioritize the development of linguistically informed tokenization strategies and adaptive vocabulary construction methods that incorporate typological diversity, ensuring more inclusive and computationally equitable multilingual AI systems.', 'abstract_zh': '跨语文本化差异阻碍了不同语言群体公平访问人工智能的机会。本研究对超过200种语言进行了大规模跨语文本化效率评估，系统量化了大型语言模型中的计算不公平性。通过标准化的实验框架，我们应用一致的预处理和归一化协议，然后使用tiktoken库对所有语言样本进行统一的文本化。我们使用公认的评估指标，包括每句令牌数量（TPS）和相对文本化成本（RTC），与英语基准进行比较，收集了全面的文本化统计数据。跨语文本分析揭示了显著且系统的差异：使用拉丁字母的 languages 一贯表现出更高的文本化效率，而非拉丁字母和词形变化复杂的 languages 则面临显著更大的令牌膨胀现象，通常 RTC 比率高出3-5倍。这些低效率转化为未充分代表的 languages 的计算成本增加，并减少了有效上下文的利用。总体而言，研究结果突显了当前 AI 系统中的结构性不公平性，使用者低资源和非拉丁字母 languages 的人群面临不成比例的计算劣势。未来研究应优先发展基于语言的文本化策略和适应性词汇构建方法，纳入类型学多样性，确保更具包容性和计算公平性多语言 AI 系统。', 'title_zh': '子词划分差异作为基础设施偏差：亚词系统如何在LLM访问和效率方面创造不平等'}
{'arxiv_id': 'arXiv:2510.12367', 'title': 'LLM-REVal: Can We Trust LLM Reviewers Yet?', 'authors': 'Rui Li, Jia-Chen Gu, Po-Nien Kung, Heming Xia, Junfeng liu, Xiangwen Kong, Zhifang Sui, Nanyun Peng', 'link': 'https://arxiv.org/abs/2510.12367', 'abstract': 'The rapid advancement of large language models (LLMs) has inspired researchers to integrate them extensively into the academic workflow, potentially reshaping how research is practiced and reviewed. While previous studies highlight the potential of LLMs in supporting research and peer review, their dual roles in the academic workflow and the complex interplay between research and review bring new risks that remain largely underexplored. In this study, we focus on how the deep integration of LLMs into both peer-review and research processes may influence scholarly fairness, examining the potential risks of using LLMs as reviewers by simulation. This simulation incorporates a research agent, which generates papers and revises, alongside a review agent, which assesses the submissions. Based on the simulation results, we conduct human annotations and identify pronounced misalignment between LLM-based reviews and human judgments: (1) LLM reviewers systematically inflate scores for LLM-authored papers, assigning them markedly higher scores than human-authored ones; (2) LLM reviewers persistently underrate human-authored papers with critical statements (e.g., risk, fairness), even after multiple revisions. Our analysis reveals that these stem from two primary biases in LLM reviewers: a linguistic feature bias favoring LLM-generated writing styles, and an aversion toward critical statements. These results highlight the risks and equity concerns posed to human authors and academic research if LLMs are deployed in the peer review cycle without adequate caution. On the other hand, revisions guided by LLM reviews yield quality gains in both LLM-based and human evaluations, illustrating the potential of the LLMs-as-reviewers for early-stage researchers and enhancing low-quality papers.', 'abstract_zh': '大型语言模型的快速进步激发了研究人员将其广泛集成到学术工作流程中， potentially 重塑研究实践和审查方式。虽然以往研究表明大型语言模型在支持研究和同行评审方面的潜力，但它们在学术工作流程中的双重角色及其与研究和审查之间的复杂互动所带来的新风险仍 largely 未被充分探索。在本研究中，我们关注大型语言模型如何深度集成到同行评审和研究过程中，可能影响学术公正性，并通过模拟研究 LLM 作为评审人时的潜在风险。该模拟包括一个研究代理生成论文并修订，以及一个评审代理评估提交的论文。基于模拟结果，我们进行人工标注，并发现 LLM 基础的评审与人类判断之间存在显著不一致：（1）LLM 评审人系统性地提高由 LLM 生成的论文评分，给予它们显著高于人类作者论文的评分；（2）LLM 评审人持续性地低估包含批评性陈述（如风险、公平性）的人类作者论文，即使经过多次修订也是如此。我们的分析揭示了这些不一致的主要来源是两个 LLM 评审人的偏见：对 LLM 生成写作风格的语言特征偏爱，以及对批评性陈述的规避倾向。这些结果突显了在无需足够谨慎的情况下部署大型语言模型进行同行评审所面临的风险和公平性问题。另一方面，遵循 LLM 评审的修订提高了 LLM 生成和人类评审的质量，展示了 LLM 作为评审人对初级研究人员的潜力，并促进质量较低论文的改进。', 'title_zh': 'LLM-REVal: 我们能信任LLM评审员吗？'}
{'arxiv_id': 'arXiv:2510.12285', 'title': 'Chinese ModernBERT with Whole-Word Masking', 'authors': 'Zeyu Zhao, Ningtao Wang, Xing Fu, Yu Cheng', 'link': 'https://arxiv.org/abs/2510.12285', 'abstract': 'Encoder-only Transformers have advanced along three axes -- architecture, data, and systems -- yielding Pareto gains in accuracy, speed, and memory efficiency. Yet these improvements have not fully transferred to Chinese, where tokenization and morphology differ markedly from English. We introduce Chinese ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware 32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the embedding budget; (ii) whole-word masking (WWM) with a dynamic masking curriculum (30% -> 15%) to align task difficulty with training progress; (iii) a two-stage pre-training pipeline that extends the native context from 1,024 to 8,192 tokens using RoPE and alternating local/global attention; and (iv) a damped-cosine learning-rate schedule for stable long-horizon optimization. We pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves high long-sequence throughput while maintaining strong short-sequence speed, reflecting benefits from budget allocation and attention design. To probe retrieval-oriented quality, we add a small amount of open contrastive data: fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking (~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set. Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding on SimCLUE, suggesting a clear scaling path for STS with additional curated pairs. We will release tokenizer and weights to facilitate reproducible research.', 'abstract_zh': 'Encoder-only Transformers在架构、数据和系统方面取得了进展，实现了准确度、速度和内存效率的帕累托改进。然而，这些改进尚未完全应用于汉语，因为汉语的分词和词素特征与英语有明显差异。我们提出了汉语现代BERT，这是一种从零开始的汉语编码器，结合了：（i）硬件意识的32k BPE词表，针对频繁出现的汉语前缀/复合词，降低了嵌入预算；（ii）词汇整体掩蔽（WWM）与动态掩蔽课程（从30%降低到15%），以使任务难度与训练进度保持一致；（iii）扩展本地和全局注意力交替使用的预训练管道，将原生上下文从1,024词扩展到8,192词；（iv）阻尼余弦学习率计划以实现稳定的长期优化。我们使用CCI3-HQ、CCI4（汉语）和Cosmopedia-Chinese中的约1.2万亿个汉字进行预训练。在CLUE上，汉语现代BERT在统一微调协议下与强大的汉语编码器具有竞争力。在bf16模式下，它实现了高长序列吞吐量同时保持强大的短序列速度，反映了预算分配和注意力设计的益处。为了研究检索导向的质量，我们添加了一部分开放对比数据：在SimCLUE（约300万对）上微调后进一步添加T2Ranking（约200万对），在SimCLUE测试集上达到0.505（皮尔逊相关系数）/0.537（斯皮尔曼等级相关系数）。在这种开放数据设置下，汉语现代BERT超过了Qwen-0.6B-嵌入在SimCLUE上的表现，表明在额外整理对的支持下有明确的STS扩展路径。我们将发布分词器和权重以促进可再现研究。', 'title_zh': 'Chinese ModernBERT全词掩码'}
{'arxiv_id': 'arXiv:2510.12266', 'title': 'HiLoRA: Adaptive Hierarchical LoRA Routing for Training-Free Domain Generalization', 'authors': 'Ziyi Han, Huanyu Wang, Zeyu Zhang, Xiangxiang Dai, Xutong Liu, John C.S. Lui', 'link': 'https://arxiv.org/abs/2510.12266', 'abstract': 'Low-Rank Adaptation (LoRA) has emerged as a widely used technique for adapting large language models (LLMs) to new domains, due to its modular design and broad availability on platforms such as HuggingFace. This availability has motivated efforts to reuse existing LoRAs for domain generalization.\nHowever, existing methods often rely on explicit task labels or additional training, which are impractical for deployment. Moreover, they typically activate a fixed number of entire LoRA modules, leading to parameter redundancy or insufficiency that degrade performance.\nIn this paper, we propose \\texttt{HiLoRA}, a training-free framework that performs adaptive hierarchical routing over LoRA pools. Drawing on structural properties of LoRA, we define rank-one components (ROCs), in which each rank parameter is regarded as an independent unit. For a given input sequence, \\texttt{HiLoRA} first adaptively selects a subset of LoRAs and determines their ROC allocation based on Gaussian likelihoods at the sequence level. At the token level, it further refines routing by activating only the most informative ROCs.\nWe further provide theoretical guarantees that \\texttt{HiLoRA} selects the most relevant LoRAs with high probability.\nExtensive experiments show that \\texttt{HiLoRA} achieves substantial improvements in domain generalization, with accuracy gains of up to {\\small $55\\%$} over state-of-the-art baselines, while maintaining comparable inference throughput.', 'abstract_zh': 'HiLoRA: 一种基于层次路由的无需训练的LoRA适配框架', 'title_zh': 'HiLoRA：自适应分层LoRA路由在无训练领域泛化的应用'}
{'arxiv_id': 'arXiv:2510.12255', 'title': 'Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs', 'authors': "Blazej Manczak, Eric Lin, Francisco Eiras, James O' Neill, Vaikkunth Mugunthan", 'link': 'https://arxiv.org/abs/2510.12255', 'abstract': 'Large language models (LLMs) are rapidly transitioning into medical clinical use, yet their reliability under realistic, multi-turn interactions remains poorly understood. Existing evaluation frameworks typically assess single-turn question answering under idealized conditions, overlooking the complexities of medical consultations where conflicting input, misleading context, and authority influence are common. We introduce MedQA-Followup, a framework for systematically evaluating multi-turn robustness in medical question answering. Our approach distinguishes between shallow robustness (resisting misleading initial context) and deep robustness (maintaining accuracy when answers are challenged across turns), while also introducing an indirect-direct axis that separates contextual framing (indirect) from explicit suggestion (direct). Using controlled interventions on the MedQA dataset, we evaluate five state-of-the-art LLMs and find that while models perform reasonably well under shallow perturbations, they exhibit severe vulnerabilities in multi-turn settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude Sonnet 4. Counterintuitively, indirect, context-based interventions are often more harmful than direct suggestions, yielding larger accuracy drops across models and exposing a significant vulnerability for clinical deployment. Further compounding analyses reveal model differences, with some showing additional performance drops under repeated interventions while others partially recovering or even improving. These findings highlight multi-turn robustness as a critical but underexplored dimension for safe and reliable deployment of medical LLMs.', 'abstract_zh': '大型语言模型（LLMs）正迅速进入医疗临床应用，但其在现实得多轮交互中的可靠性尚不明确。现有的评估框架通常在理想条件下评估单轮问答，忽视了医疗咨询中常见的矛盾输入、误导性背景和权威影响的复杂性。我们引入了MedQA-Followup框架，用于系统评估医疗问答的多轮鲁棒性。我们的方法区分了浅层鲁棒性（抵御误导性初始背景）和深层鲁棒性（在回答被多次挑战时保持准确性），同时引入了一个间接-直接轴，将其区分为背景框架（间接）和明确建议（直接）。通过在MedQA数据集上进行受控干预，我们评估了五种最先进的LLM，并发现虽然模型在浅层干扰下表现尚可，但在多轮设置中却表现出严重的脆弱性，准确率从91.2%降至低至13.5%（Claude Sonnet 4）。出人意料的是，间接、基于背景的干预往往比直接建议更具害处，导致模型间更大的准确率下降，并揭示了临床部署中的显著脆弱性。进一步的分析揭示了模型之间的差异，有些模型在重复干预下表现出额外的性能下降，而另一些则部分恢复甚至有所提高。这些发现突出了多轮鲁棒性对于安全可靠部署医疗LLM的关键但尚未充分探索的维度。', 'title_zh': '浅层鲁棒性，深层漏洞：医疗LLM的多轮评估'}
{'arxiv_id': 'arXiv:2510.12252', 'title': 'PromptLocate: Localizing Prompt Injection Attacks', 'authors': 'Yuqi Jia, Yupei Liu, Zedian Shao, Jinyuan Jia, Neil Gong', 'link': 'https://arxiv.org/abs/2510.12252', 'abstract': 'Prompt injection attacks deceive a large language model into completing an attacker-specified task instead of its intended task by contaminating its input data with an injected prompt, which consists of injected instruction(s) and data. Localizing the injected prompt within contaminated data is crucial for post-attack forensic analysis and data recovery. Despite its growing importance, prompt injection localization remains largely unexplored. In this work, we bridge this gap by proposing PromptLocate, the first method for localizing injected prompts. PromptLocate comprises three steps: (1) splitting the contaminated data into semantically coherent segments, (2) identifying segments contaminated by injected instructions, and (3) pinpointing segments contaminated by injected data. We show PromptLocate accurately localizes injected prompts across eight existing and eight adaptive attacks.', 'abstract_zh': 'Prompt注入攻击通过在其输入数据中插入注入提示，使大型语言模型完成攻击者指定的任务而非预期任务。定位被注入提示污染的数据对于攻击后的法医分析和数据恢复至关重要。尽管其重要性日益增加，但注入提示的定位仍然缺乏探索。在本文中，我们通过提出PromptLocate方法来填补这一空白，这是首个用于定位注入提示的方法。PromptLocate包括三个步骤：（1）将被污染的数据划分为语义上连贯的片段，（2）识别被注入指令污染的片段，以及（3）定位被注入数据污染的片段。我们展示了PromptLocate在八个现有攻击和八个自适应攻击中准确地定位了注入提示。', 'title_zh': 'PromptLocate: 定位提示注入攻击'}
{'arxiv_id': 'arXiv:2510.12245', 'title': 'MoRA: On-the-fly Molecule-aware Low-Rank Adaptation Framework for LLM-based Multi-Modal Molecular Assistant', 'authors': 'Tao Yin, Xiaohong Zhang, Jiacheng Zhang, Li Huang, Zhibin Zhang, Yuansong Zeng, Jin Xie, Meng Yan', 'link': 'https://arxiv.org/abs/2510.12245', 'abstract': "Effectively integrating molecular graph structures with Large Language Models (LLMs) is a key challenge in drug discovery. Most existing multi-modal alignment methods typically process these structures by fine-tuning the LLM or adding a static adapter simultaneously. However, these approaches have two main limitations: (1) it optimizes a shared parameter space across all molecular inputs, limiting the model's ability to capture instance-specific structural features; and (2) fine-tuning the LLM for molecular tasks can lead to catastrophic forgetting, undermining its general reasoning capabilities. In this paper, instead of static task-oriented adaptation, we propose an instance-specific parameter space alignment approach for each molecule on-the-fly. To this end, we introduce Molecule-aware Low-Rank Adaptation (MoRA) that produces a unique set of low-rank adaptation weights for each input molecular graph. These weights are then dynamically injected into a frozen LLM, allowing the model to adapt its reasoning to the structure of each molecular input, while preserving the LLM's core knowledge. Extensive experiments demonstrate that on key molecular tasks, such as chemical reaction prediction and molecular captioning, MoRA's instance-specific dynamic adaptation outperforms statically adapted baselines, including a 14.1% relative improvement in reaction prediction exact match and a 22% reduction in error for quantum property prediction. The code is available at this https URL.", 'abstract_zh': '有效地将分子图结构与大型语言模型结合是药物发现中的一个重要挑战。现有的多模态对齐方法通常通过微调大型语言模型或同时添加静态适配器来处理这些结构。然而，这些方法存在两个主要局限性：（1）它在所有分子输入之间共享一个参数空间，限制了模型捕捉实例特定结构特征的能力；（2）为分子任务微调大型语言模型可能导致灾难性遗忘，削弱其通用推理能力。在本文中，我们提出了一种针对每个分子实例特定参数空间对齐的方法。为此，我们引入了分子感知的低秩适配（MoRA），为每个输入分子图生成一组独特的低秩适配权重。这些权重随后动态注入冻结的大型语言模型中，允许模型根据每个分子输入的结构进行适应，同时保留大型语言模型的核心知识。广泛实验表明，在化学反应预测和分子描述等关键分子任务中，MoRA的实例特定动态适应优于静态适应基线，反应预测完全匹配提高了14.1%，量子性质预测误差减少了22%。代码可在以下链接获取。', 'title_zh': 'MoRA：基于LLM的多模态分子助手的分子意识低秩适应框架（增量学习）'}
{'arxiv_id': 'arXiv:2510.12229', 'title': 'Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability', 'authors': 'Bianca Raimondi, Daniela Dalbagno, Maurizio Gabbrielli', 'link': 'https://arxiv.org/abs/2510.12229', 'abstract': 'Large language models (LLMs) have been shown to internalize human-like biases during finetuning, yet the mechanisms by which these biases manifest remain unclear. In this work, we investigated whether the well-known Knobe effect, a moral bias in intentionality judgements, emerges in finetuned LLMs and whether it can be traced back to specific components of the model. We conducted a Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the bias is not only learned during finetuning but also localized in a specific set of layers. Surprisingly, we found that patching activations from the corresponding pretrained model into just a few critical layers is sufficient to eliminate the effect. Our findings offer new evidence that social biases in LLMs can be interpreted, localized, and mitigated through targeted interventions, without the need for model retraining.', 'abstract_zh': '大型语言模型在微调过程中表现出的人类偏见机制尚不明确：Knobe效应在微调大型语言模型中的表现及其追溯分析', 'title_zh': '通过机制可解释性分析微调后的大型语言模型中的道德偏见'}
{'arxiv_id': 'arXiv:2510.12217', 'title': 'HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment', 'authors': 'Ali Mekky, Omar El Herraoui, Preslav Nakov, Yuxia Wang', 'link': 'https://arxiv.org/abs/2510.12217', 'abstract': 'Large language models (LLMs) are increasingly deployed across high-impact domains, from clinical decision support and legal analysis to hiring and education, making fairness and bias evaluation before deployment critical. However, existing evaluations lack grounding in real-world scenarios and do not account for differences in harm severity, e.g., a biased decision in surgery should not be weighed the same as a stylistic bias in text summarization. To address this gap, we introduce HALF (Harm-Aware LLM Fairness), a deployment-aligned framework that assesses model bias in realistic applications and weighs the outcomes by harm severity. HALF organizes nine application domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline. Our evaluation results across eight LLMs show that (1) LLMs are not consistently fair across domains, (2) model size or performance do not guarantee fairness, and (3) reasoning models perform better in medical decision support but worse in education. We conclude that HALF exposes a clear gap between previous benchmarking success and deployment readiness.', 'abstract_zh': 'HARM-AWARE LLM FAIRNESS (HALF): A DEPLOYMENT-ALIGNED FRAMEWORK FOR ASSESSING MODEL BIAS AND WEIGHING OUTCOMES BY HARM SEVERITY', 'title_zh': 'HALF: 意外风险意识的大语言模型公平性评估与部署对齐'}
{'arxiv_id': 'arXiv:2510.12184', 'title': 'CompoDistill: Attention Distillation for Compositional Reasoning in Multimodal LLMs', 'authors': 'Jiwan Kim, Kibum Kim, Sangwoo Seo, Chanyoung Park', 'link': 'https://arxiv.org/abs/2510.12184', 'abstract': "Recently, efficient Multimodal Large Language Models (MLLMs) have gained significant attention as a solution to their high computational complexity, making them more practical for real-world applications. In this regard, the knowledge distillation (KD) approach has emerged as a promising alternative, which transfers the rich visual and linguistic knowledge from a larger model (teacher) to a smaller model (student). However, we observe that existing KD methods struggle to effectively distill the teacher MLLM's rich visual perception abilities to the student, a challenge that has been largely overlooked in previous studies. Through a systematic analysis, we identify visual attention misalignment between student and teacher as the main cause of this issue. Based on this insight, we propose CompoDistill, a novel KD framework that explicitly aligns the student's visual attention with that of the teacher to enhance the student's visual perception abilities. Our extensive experiments show that CompoDistill significantly improves performance on compositional reasoning tasks that require visual perception abilities while maintaining strong performance on visual question answering tasks, as done in existing studies. Furthermore, CompoDistill demonstrates effectiveness with a more advanced backbone, highlighting its generalizability.", 'abstract_zh': '最近，高效的多模态大型语言模型（MLLMs）因其高计算复杂性而备受关注，使其在实际应用中更具实用性。在此背景下，知识蒸馏（KD）方法作为一种有前途的替代方案浮现出来，它将较大的模型（教师）丰富的视觉和语言知识转移到较小的模型（学生）上。然而，我们观察到现有的KD方法在将教师MLLM的丰富视觉感知能力有效地转移到学生上时存在困难，这一问题在先前的研究中并未被充分关注。通过对这一问题的系统分析，我们发现学生和教师之间的视觉注意力 misalignment 是主要原因。基于这一洞察，我们提出了 CompoDistill，这是一种新颖的KD框架，明确对齐学生和教师的视觉注意力，以增强学生在视觉感知方面的能力。广泛的实验证明，CompoDistill 在需要视觉感知能力的组合推理任务上显著提高了性能，同时在视觉问答任务上也保持了现有研究中的强大表现。此外，CompoDistill 在更先进的骨干网络上显示出有效性，这突显了它的通用性。', 'title_zh': 'CompoDistill：多模态LLM中组合理构推理的注意力精练'}
{'arxiv_id': 'arXiv:2510.12181', 'title': 'From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing', 'authors': 'Chengrui Xiang, Tengfei Ma, Xiangzheng Fu, Yiping Liu, Bosheng Song, Xiangxiang Zeng', 'link': 'https://arxiv.org/abs/2510.12181', 'abstract': "Drug repurposing plays a critical role in accelerating treatment discovery, especially for complex and rare diseases. Biomedical knowledge graphs (KGs), which encode rich clinical associations, have been widely adopted to support this task. However, existing methods largely overlook common-sense biomedical concept knowledge in real-world labs, such as mechanistic priors indicating that certain drugs are fundamentally incompatible with specific treatments. To address this gap, we propose LLaDR, a Large Language Model-assisted framework for Drug Repurposing, which improves the representation of biomedical concepts within KGs. Specifically, we extract semantically enriched treatment-related textual representations of biomedical entities from large language models (LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By injecting treatment-relevant knowledge into KGE, LLaDR largely improves the representation of biomedical concepts, enhancing semantic understanding of under-studied or complex indications. Experiments based on benchmarks demonstrate that LLaDR achieves state-of-the-art performance across different scenarios, with case studies on Alzheimer's disease further confirming its robustness and effectiveness. Code is available at this https URL.", 'abstract_zh': '药物再利用在加速复杂和罕见疾病治疗发现中发挥着关键作用。生物医学知识图谱（KGs），其中编码丰富的临床关联，已被广泛用于支持这一任务。然而，现有方法在很大程度上忽略了实际实验室中的常识性生物医学概念知识，例如机制先验表明某些药物与特定治疗从根本上是不兼容的。为了解决这一缺口，我们提出了一种大型语言模型辅助的药物再利用框架LLaDR，该框架提高了KG中生物医学概念的表示。具体而言，我们从大型语言模型（LLMs）中提取包含语义丰富治疗相关信息的生物医学实体表示，并使用它们来微调知识图嵌入（KGE）模型。通过将治疗相关的知识注入KGE，LLaDR显著提高了生物医学概念的表示，增强了对未研究或复杂症状的语义理解。基于基准的实验表明，LLaDR在不同场景下达到了最先进的性能，且阿尔茨海默病案例研究进一步证实了其稳健性和有效性。代码可供参考：this https URL。', 'title_zh': '从知识到治疗：大型语言模型辅助的生物医药概念表示在药物再利用中的应用'}
{'arxiv_id': 'arXiv:2510.12137', 'title': 'Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models', 'authors': 'Shihao Ji, Zihui Song, Jiajie Huang', 'link': 'https://arxiv.org/abs/2510.12137', 'abstract': 'Large Language Models (LLMs) hallucinate, generating factually incorrect yet confident assertions. We argue this stems from the Transformer\'s Softmax function, which creates "Artificial Certainty" by collapsing ambiguous attention scores into a single probability distribution, discarding uncertainty information at each layer. To fix this, we introduce the Credal Transformer, which replaces standard attention with a Credal Attention Mechanism (CAM) based on evidential theory. CAM produces a "credal set" (a set of distributions) instead of a single attention vector, with the set\'s size directly measuring model uncertainty. We implement this by re-conceptualizing attention scores as evidence masses for a Dirichlet distribution: sufficient evidence recovers standard attention, while insufficient evidence yields a diffuse distribution, representing ambiguity. Empirically, the Credal Transformer identifies out-of-distribution inputs, quantifies ambiguity, and significantly reduces confident errors on unanswerable questions by abstaining. Our contribution is a new architecture to mitigate hallucinations and a design paradigm that integrates uncertainty quantification directly into the model, providing a foundation for more reliable AI.', 'abstract_zh': '大型语言模型（LLMs）会产生幻觉，生成事实错误但充满自信的断言。我们认为这源于Transformer的Softmax函数，该函数通过将具有歧义的注意分数压缩成单个概率分布，逐层丢弃不确定性信息，从而创造了“人工确定性”。为解决这一问题，我们引入了Credal Transformer，其中用基于证据理论的可信注意力机制（CAM）替代了标准注意力机制。CAM生成一个“可信集合”（一组分布）而不是单个注意力向量，集合的大小直接测量模型的不确定性。我们通过将注意力分数重新概念化为Dirichlet分布的证据质量来实现这一点：充足的证据恢复标准注意力，而不足的证据导致一个弥散分布，表示不确定性。实验结果表明，Credal Transformer能够识别离分布输入、量化不确定性，并通过弃权大幅减少不可回答问题上的自信错误。我们的贡献是一种新的架构来减轻幻觉现象，并提供了一种将不确定性量化直接集成到模型设计中的设计理念，为更加可靠的AI奠定基础。', 'title_zh': '信念变换器：一种衡量和减轻大型语言模型幻觉的规范方法'}
{'arxiv_id': 'arXiv:2510.12116', 'title': 'Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models', 'authors': 'Bajian Xiang, Shuaijiang Zhao, Tingwei Guo, Wei Zou', 'link': 'https://arxiv.org/abs/2510.12116', 'abstract': 'End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive conversational generation abilities, yet consistently fall short of traditional pipeline systems on semantic understanding benchmarks. In this work, we reveal through systematic experimentation that although LSLMs lose some text input performance after speech-text alignment training, the performance gap between speech and text inputs is more pronounced, which we refer to as the modality gap. To understand this gap, we analyze both coarse- and fine-grained text and speech representations. At the coarse-grained level, representations of speech and text in deeper layers are found to be increasingly aligned in direction (cosine similarity), while concurrently diverging in magnitude (Euclidean distance). We further find that representation similarity is strongly correlated with the modality gap. At the fine-grained level, a spontaneous token-level alignment pattern between text and speech representations is observed. Based on this, we introduce the Alignment Path Score to quantify token-level alignment quality, which exhibits stronger correlation with the modality gap. Building on these insights, we design targeted interventions on critical tokens through angle projection and length normalization. These strategies demonstrate the potential to improve correctness for speech inputs. Our study provides the first systematic empirical analysis of the modality gap and alignment mechanisms in LSLMs, offering both theoretical and methodological guidance for future optimization.', 'abstract_zh': '端到端大型语音语言模型（LSLMs）在会话生成方面表现出了令人印象深刻的 ability，但在语义理解基准测试中却始终无法超越传统的流水线系统。在此项工作中，通过系统的实验我们揭示了虽然在语音-文本对齐训练后 LSLMs 的文本输入性能有所下降，但语音输入与文本输入之间的性能差距更为明显，我们将这种差距称为模态差距。为了理解这种差距，我们对粗粒度和细粒度的文本和语音表示进行了分析。在粗粒度水平，深层层中的语音和文本表示在方向上逐渐趋于一致（余弦相似性），同时在大小上逐渐发散（欧几里得距离）。进一步发现，表示相似性与模态差距密切相关。在细粒度水平，发现了文本和语音表示之间的自发 token 级别对齐模式。基于此，我们引入了对齐路径得分来量化 token 级别对齐质量，这种得分与模态差距的相关性更强。基于这些见解，我们通过角度投影和长度归一化对关键 token 进行了有针对性的干预。这些策略显示出提高语音输入正确性的潜力。我们的研究提供了 LSLS 中模态差距和对齐机制的第一项系统性的实证分析，为未来的优化提供了理论和方法上的指导。', 'title_zh': '理解模态差距：大型语音语言模型的语音-文本对齐机制实证研究'}
{'arxiv_id': 'arXiv:2510.12110', 'title': 'Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models', 'authors': 'Ziliang Qiu, Renfen Hu', 'link': 'https://arxiv.org/abs/2510.12110', 'abstract': "The evaluation of LLMs' creativity represents a crucial research domain, though challenges such as data contamination and costly human assessments often impede progress. Drawing inspiration from human creativity assessment, we propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate their creativity. PACE minimizes the risk of data contamination and offers a straightforward, highly efficient evaluation, as evidenced by its strong correlation with Chatbot Arena Creative Writing rankings (Spearman's $\\rho = 0.739$, $p < 0.001$) across various proprietary and open-source models. A comparative analysis of associative creativity between LLMs and humans reveals that while high-performing LLMs achieve scores comparable to average human performance, professional humans consistently outperform LLMs. Furthermore, linguistic analysis reveals that both humans and LLMs exhibit a trend of decreasing concreteness in their associations, and humans demonstrating a greater diversity of associative patterns.", 'abstract_zh': 'LLMscreativity评估：一种基于并行关联链的方法', 'title_zh': '深层次关联，高创造力：评价大规模语言模型的一个简单而有效的指标'}
{'arxiv_id': 'arXiv:2510.12044', 'title': 'Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models', 'authors': 'Yukun Zhang, Qi Dong', 'link': 'https://arxiv.org/abs/2510.12044', 'abstract': 'Existing alignment techniques for Large Language Models (LLMs), such as Direct Preference Optimization (DPO), typically treat the model as a monolithic entity, applying uniform optimization pressure across all layers. This approach overlooks the functional specialization within the Transformer architecture, where different layers are known to handle distinct tasks from syntax to abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm by introducing Hierarchical Alignment, a novel method that applies targeted DPO to distinct functional blocks of a model\'s layers: local (syntax), intermediate (logic), and global (factuality). Through a series of controlled experiments on state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge, demonstrate significant and predictable improvements. Specifically, aligning the local layers (Local-Align) enhances grammatical fluency. More importantly, aligning the global layers (Global-Align) not only improves factual consistency as hypothesized but also proves to be the most effective strategy for enhancing logical coherence, outperforming all baselines. Critically, all hierarchical strategies successfully avoid the "alignment tax" observed in standard DPO, where gains in fluency come at the cost of degraded logical reasoning. These findings establish a more resource-efficient, controllable, and interpretable path for model alignment, highlighting the immense potential of shifting from monolithic optimization to structure-aware surgical fine-tuning to build more advanced and reliable LLMs.', 'abstract_zh': '现有的大型语言模型（LLMs）对齐技术，如直接偏好优化（DPO），通常将模型视为一个整体，对所有层施加均匀的优化压力。这种方法忽视了Transformer架构内部的功能专业化，不同的层已知负责从句法到抽象推理的不同任务。在本文中，我们通过引入层次对齐这一新颖方法挑战了一刀切的范式，该方法对模型各层中的特定功能块应用针对性的DPO：局部（句法）、中间（逻辑）和全局（事实性）。通过在如Llama-3.1-8B和Qwen1.5-7B等最先进的模型上进行一系列受控实验，并使用LoRA进行手术微调，我们的结果由强大的LLM作为评判者评估，显示出显著且可预测的改进。特别是，对局部层进行对齐（Local-Align）提高了语法流畅性。更重要的是，对全局层进行对齐（Global-Align）不仅如预期那样提高了事实一致性，而且证明是最有效的提高逻辑连贯性的策略，优于所有基线方法。至关重要的是，所有层次方法都成功避免了标准DPO中观察到的“对齐税收”，即流畅性的提高会以逻辑推理能力降低为代价。这些发现为模型对齐奠定了更高效、可控和可解释的路径，突显了从整体优化转向结构感知的手术微调以构建更先进和可靠的LLM的巨大潜力。', 'title_zh': '层次对齐：大型语言模型中功能层专业化下的手术微调'}
{'arxiv_id': 'arXiv:2510.12032', 'title': 'Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models', 'authors': 'Jung-Woo Shim, Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee', 'link': 'https://arxiv.org/abs/2510.12032', 'abstract': 'Recent advancements in large language models (LLMs) have shown strong performance in natural language understanding and generation tasks. However, LLMs continue to encounter challenges with hallucinations, where models generate plausible but incorrect information. While several factors contribute to hallucinations, the impact of ill-formed prompts, prompts with ambiguous wording, incorrect grammar, or incomplete information, was relatively under explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a framework designed to systematically improve these ill-formed prompts across multiple stages. Each stage addresses specific errors such as punctuation, typographical mistakes, and misuse of key terms, using small language models (SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of prompts with additional context and employs a self-reflection mechanism with ranking to prioritize the most relevant input. Experimental results on hallucination benchmarks show that prompts refined by MPR achieve over an 85~\\% win rate compared to their original forms, demonstrating its effectiveness in reducing hallucinations and improving LLM output accuracy. Interestingly, we reveal that MPR can be combined with existing post-hoc hallucination mitigation frameworks, further enhancing its versatility. MPR provides a lightweight and adaptable solution for enhancing LLM reliability across various domains.', 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）在自然语言理解和生成任务中表现出强大的性能。然而，LLMs仍然面临幻觉问题的挑战，即模型生成看似合理但实际上错误的信息。虽然幻觉产生的因素多种多样，但不良提示（如语法错误、信息不完整、措辞模糊等）的影响相对未被深入探索。为解决这一问题，我们引入了多阶段提示 refinement（MPR）框架，旨在通过多个阶段系统地改进这些不良提示。每一阶段针对特定错误（如标点符号错误、拼写错误和术语误用）进行修正，采用专门fine-tuned的小型语言模型（SLMs）。MPR通过增加上下文信息逐步提升提示的清晰度，并采用自我反思机制与排序机制来优先处理最相关的输入。在幻觉基准测试上的实验结果表明，经过MPR改进的提示在85%以上的情况下优于其原始形式，证明了其在减少幻觉和提高LLM输出准确性方面的有效性。有趣的是，我们发现MPR可以与其他现有的事后幻觉缓解框架结合使用，进一步增强其灵活性。MPR提供了一种轻量级且适应性强的解决方案，用于提高不同领域中LLM的可靠性。', 'title_zh': '多阶段提示精炼以减轻大型语言模型中的幻觉'}
{'arxiv_id': 'arXiv:2510.12029', 'title': 'CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement', 'authors': 'Jung-Woo Shim, Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee', 'link': 'https://arxiv.org/abs/2510.12029', 'abstract': 'Recent advancements in large language models (LLMs) highlight their fluency in generating responses to diverse prompts. However, these models sometimes generate plausible yet incorrect ``hallucinated" facts, undermining trust. A frequent but often overlooked cause of such errors is the use of poorly structured or vague prompts by users, leading LLMs to base responses on assumed rather than actual intentions. To mitigate hallucinations induced by these ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a plug-and-play framework for curative prompt refinement that 1) cleans ill-formed prompts, and 2) generates additional informative task descriptions to align the intention of the user and the prompt using a fine-tuned small language model. When applied to language models, we discover that CPR significantly increases the quality of generation while also mitigating hallucination. Empirical studies show that prompts with CPR applied achieves over a 90\\% win rate over the original prompts without any external knowledge.', 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）突显了它们在生成对各种提示响应方面的流畅性。然而，这些模型有时会生成虽具说服力但错误的“虚构”事实，从而削弱了信任。造成此类错误的一个常见但经常被忽视的原因是用户使用的结构不良或含糊不清的提示，导致LLMs基于假设而不是实际意图来形成响应。为了减轻由这些不完善的提示引起的虚构现象，我们引入了Curative Prompt Refinement (CPR)框架，这是一种可插拔框架，用于修正不良的提示并生成额外的信息性任务描述以通过微调小型语言模型调整用户的意图与提示之间的关系。当应用于语言模型时，我们发现CPR显著提高生成质量同时减轻了虚构现象。实证研究表明，使用CPR的提示在无需任何外部知识的情况下，胜过原始提示的成功率超过90%。', 'title_zh': 'CPR: 修正提示 refinement 减轻大型语言模型幻觉'}
{'arxiv_id': 'arXiv:2510.11978', 'title': 'Learning Dynamics of VLM Finetuning', 'authors': 'Jusheng Zhang, Kaitong Cai, Jing Yang, Keze Wang', 'link': 'https://arxiv.org/abs/2510.11978', 'abstract': "Preference-based finetuning of vision--language models (VLMs) is brittle: trivially wrong negatives inject uninformative gradients that destabilize training. We recast alignment as \\textbf{learning-dynamics--aware optimization} and introduce \\textbf{Cooling-Weighted DPO (CW-DPO)}, a two-stage recipe that explicitly models and exploits the training trajectory. \\textbf{Stage 1} performs supervised finetuning with \\textbf{gentle negatives}: \\textbf{low-weight smoothed supervision} that regularizes the base policy and curbs overconfidence without explicit penalties. \\textbf{Stage 2} applies a DPO objective in which the \\textbf{negative term is scaled by a cooling weight} computed from the model's \\textbf{average token log-probability} on each negative, suppressing uninformative gradients from easy or off-distribution samples while preserving signal from hard negatives. In practice, we emphasize \\textbf{on-policy negatives} and allow \\textbf{mixed negatives} by blending a controllable fraction of dataset negatives to maintain contrast freshness. Throughout, we instrument training with $\\Delta\\!\\log p$ probes on positives and negatives as first-class signals for early stopping, curriculum design, and failure diagnosis. Across diverse VLM tasks, CW-DPO yields \\textbf{more stable optimization}, \\textbf{better calibration}, and \\textbf{higher pairwise win-rates} than SFT-only and vanilla DPO, while \\textbf{converging in fewer steps}. Ablations isolate the \\textbf{cooling-weight mechanism} as the primary driver of these gains and show complementary benefits from mixing on-policy and dataset negatives. Taken together, our results show that \\textbf{smoothing learning dynamics before cooling preferences} is a simple, general principle for robust VLM alignment.", 'abstract_zh': '基于偏好微调的视觉-语言模型（VLMs）训练易碎：显然错误的负样本注入无信息梯度， destabilize 训练。我们重新定义对齐为学习动力学感知优化，并引入冷却加权 DPO（CW-DPO），这是一种两阶段方法，明确建模并利用训练轨迹。第一阶段使用温和的负样本进行监督微调：低权重平滑监督，用于正则化基础策略并减少过信度，而无需明确的惩罚。第二阶段应用 DPO 目标，在该目标中，负项通过模型计算的平均标记对数概率进行缩放权重，抑制来自容易或离分布样本的无信息梯度，同时保留来自困难负样本的信号。实践中，我们强调在策略负样本上训练，并允许通过混合可控制比例的数据集负样本来维持对比新鲜度。在整个过程中，我们通过在正样本和负样本上使用 $\\Delta\\!\\log p$ 探针作为首要信号进行早期停止、课程设计和失败诊断。在多种多样的 VLM 任务中，CW-DPO 在优化稳定性、校准准确性和两两胜率方面优于仅自编码器微调和标准 DPO，同时在更少的步骤内收敛。消融实验将冷却权重机制识别为主要驱动因素，并显示了在策略负样本和数据集负样本混合中的互补益处。综上所述，我们的结果表明，在冷却偏好之前平滑学习动力学是鲁棒 VLM 对齐的一个简单而通用的原则。', 'title_zh': 'VLM微调的学习动力学'}
{'arxiv_id': 'arXiv:2510.11974', 'title': 'CTIArena: Benchmarking LLM Knowledge and Reasoning Across Heterogeneous Cyber Threat Intelligence', 'authors': 'Yutong Cheng, Yang Liu, Changze Li, Dawn Song, Peng Gao', 'link': 'https://arxiv.org/abs/2510.11974', 'abstract': 'Cyber threat intelligence (CTI) is central to modern cybersecurity, providing critical insights for detecting and mitigating evolving threats. With the natural language understanding and reasoning capabilities of large language models (LLMs), there is increasing interest in applying them to CTI, which calls for benchmarks that can rigorously evaluate their performance. Several early efforts have studied LLMs on some CTI tasks but remain limited: (i) they adopt only closed-book settings, relying on parametric knowledge without leveraging CTI knowledge bases; (ii) they cover only a narrow set of tasks, lacking a systematic view of the CTI landscape; and (iii) they restrict evaluation to single-source analysis, unlike realistic scenarios that require reasoning across multiple sources. To fill these gaps, we present CTIArena, the first benchmark for evaluating LLM performance on heterogeneous, multi-source CTI under knowledge-augmented settings. CTIArena spans three categories, structured, unstructured, and hybrid, further divided into nine tasks that capture the breadth of CTI analysis in modern security operations. We evaluate ten widely used LLMs and find that most struggle in closed-book setups but show noticeable gains when augmented with security-specific knowledge through our designed retrieval-augmented techniques. These findings highlight the limitations of general-purpose LLMs and the need for domain-tailored techniques to fully unlock their potential for CTI.', 'abstract_zh': 'CTIArena：评估大语言模型在知识增强多源网络威胁情报分析中的性能', 'title_zh': 'CTIArena：跨异构网络威胁情报领域评估LLM知识与推理能力'}
{'arxiv_id': 'arXiv:2510.11958', 'title': 'Direct Multi-Token Decoding', 'authors': 'Xuan Luo, Weizhi Wang, Xifeng Yan', 'link': 'https://arxiv.org/abs/2510.11958', 'abstract': 'Decoder-only transformers have become the standard architecture for large language models (LLMs) due to their strong performance. Recent studies suggest that, in pre-trained LLMs, early, middle, and late layers may serve distinct roles: Early layers focus on understanding the input context, middle layers handle task-specific processing, and late layers convert abstract representations into output tokens. We hypothesize that once representations have been processed by the early and middle layers, the resulting hidden states may encapsulate sufficient information to support the generation of multiple tokens using only the late layers, eliminating the need to repeatedly traverse the early and middle layers. We refer to this inference paradigm as Direct Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces no additional parameters, auxiliary routines, or post-generation verification. Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model has already demonstrated promising results, achieving up to a 2x speedup with only minor performance loss. Moreover, as shown in our scaling analysis, its performance is expected to further improve with larger training datasets.', 'abstract_zh': '只解码器的变压器已成为大规模语言模型（LLMs）的标准架构，由于其出色的性能。近期研究表明，在预训练的LLMs中，早期、中期和晚期层可能各自承担不同的职责：早期层专注于理解输入语境，中期层处理任务特定的处理，晚期层将抽象表示转换为输出标记。我们假设一旦输入被早期和中期层处理后，产生的隐藏状态可能已经包含了利用晚期层生成多个标记所需的所有信息，从而避免了反复穿越早期和中期层的需要。我们称这一推理范式为直接多标记解码（DMTD）。与推测性解码不同，我们的方法不引入额外的参数、辅助程序或生成后的验证。尽管是在有限的数据集上训练，微调后的DMTD Qwen3-4B模型已经展现了令人鼓舞的结果，只牺牲了微小的性能损失就实现了高达2倍的速度提升。此外，如我们在缩放分析中所展示的，其性能预期随着更大规模训练数据集的增加而进一步提升。', 'title_zh': '直接多令牌解码'}
{'arxiv_id': 'arXiv:2510.11944', 'title': 'TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition', 'authors': 'Yupei Li, Philipp Borchert, Gerasimos Lampouras', 'link': 'https://arxiv.org/abs/2510.11944', 'abstract': 'Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4) mathematical reasoning but still struggle with autoformalisation, the task of transforming informal into formal mathematical statements. Autoformalisation helps pair the informal reasoning of LLMs with formal proof assistants which enable machine-verifiable generation and mitigate hallucinations. Yet, the performance of current Math LLMs is constrained by the scarcity of large-scale corpora, particularly those containing pairs of informal and formal statements. Although current models are trained to generate code from natural language instructions, structural and syntactic differences between these and formal mathematics limit effective transfer learning. We propose TopoAlign, a framework that unlocks widely available code repositories as training resources for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and dependency functions, and reassembles these components into analogues that structurally mirror formal statements. This produces structurally aligned code data that can be used for training Math LLMs without requiring additional human annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign provides substantial gains for DeepSeek-Math, improving performance by 17.77% on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10 and typecheck@10, respectively, demonstrating that training on aligned code data is beneficial even for specialized models.', 'abstract_zh': 'TopoAlign：利用结构对齐代码数据训练数学大型语言模型', 'title_zh': 'TopoAlign：一种通过拓扑分解对齐代码与数学的框架'}
{'arxiv_id': 'arXiv:2510.11926', 'title': 'Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer', 'authors': 'Nayan Sanjay Bhatia, Pranay Kocheta, Russell Elliott, Harikrishna S. Kuttivelil, Katia Obraczka', 'link': 'https://arxiv.org/abs/2510.11926', 'abstract': 'Indoor Wi-Fi positioning remains a challenging problem due to the high sensitivity of radio signals to environmental dynamics, channel propagation characteristics, and hardware heterogeneity. Conventional fingerprinting and model-based approaches typically require labor-intensive calibration and suffer rapid performance degradation when devices, channel or deployment conditions change. In this paper, we introduce Locaris, a decoder-only large language model (LLM) for indoor localization. Locaris treats each access point (AP) measurement as a token, enabling the ingestion of raw Wi-Fi telemetry without pre-processing. By fine-tuning its LLM on different Wi-Fi datasets, Locaris learns a lightweight and generalizable mapping from raw signals directly to device location. Our experimental study comparing Locaris with state-of-the-art methods consistently shows that Locaris matches or surpasses existing techniques for various types of telemetry. Our results demonstrate that compact LLMs can serve as calibration-free regression models for indoor localization, offering scalable and robust cross-environment performance in heterogeneous Wi-Fi deployments. Few-shot adaptation experiments, using only a handful of calibration points per device, further show that Locaris maintains high accuracy when applied to previously unseen devices and deployment scenarios. This yields sub-meter accuracy with just a few hundred samples, robust performance under missing APs and supports any and all available telemetry. Our findings highlight the practical viability of Locaris for indoor positioning in the real-world scenarios, particularly in large-scale deployments where extensive calibration is infeasible.', 'abstract_zh': '室内Wi-Fi定位依然是一项具有挑战性的问题，因为无线信号对环境动态、信道传播特性和硬件异构性高度敏感。传统的指纹识别和基于模型的方法通常需要耗费大量的人工校准工作，并且在设备、信道或部署条件改变时会迅速 performance降级。本文介绍了Locaris，一种仅解码的大语言模型（LLM）用于室内定位。Locaris将每个接入点（AP）的测量值视为一个token，允许直接摄入原始Wi-Fi遥测数据而无需预处理。通过在其LLM上对不同的Wi-Fi数据集进行微调，Locaris学会了将原始信号直接映射到设备位置的轻量级和可泛化的映射关系。我们的实验研究将Locaris与现有最先进的方法进行比较，结果显示Locaris在各种类型的遥测数据中都表现出了与现有技术相当或更优的性能。我们的结果表明，紧凑型LLM可以作为无需校准的回归模型用于室内定位，提供在异构Wi-Fi部署中的可扩展和稳健的跨环境性能。少量示例点（每设备几个点）的适应性实验进一步证明，在面对以前未见过的设备和部署场景时，Locaris仍能保持高精度。这使得在仅几百样本的情况下实现亚米级精度，对AP缺失具有鲁棒性能，并支持所有可用的遥测数据。我们的发现突显了Locaris在真实场景中进行室内定位的实际可行性，尤其是在大规模部署中进行全面校准不可行的情况下。', 'title_zh': '室内定位Using紧凑、传输协议无关、启用转移学习的仅解码器变压器'}
{'arxiv_id': 'arXiv:2510.11837', 'title': 'Countermind: A Multi-Layered Security Architecture for Large Language Models', 'authors': 'Dominik Schwarz', 'link': 'https://arxiv.org/abs/2510.11837', 'abstract': 'The security of Large Language Model (LLM) applications is fundamentally challenged by "form-first" attacks like prompt injection and jailbreaking, where malicious instructions are embedded within user inputs. Conventional defenses, which rely on post hoc output filtering, are often brittle and fail to address the root cause: the model\'s inability to distinguish trusted instructions from untrusted data. This paper proposes Countermind, a multi-layered security architecture intended to shift defenses from a reactive, post hoc posture to a proactive, pre-inference, and intra-inference enforcement model. The architecture proposes a fortified perimeter designed to structurally validate and transform all inputs, and an internal governance mechanism intended to constrain the model\'s semantic processing pathways before an output is generated. The primary contributions of this work are conceptual designs for: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text Crypter intended to reduce the plaintext prompt injection attack surface, provided all ingestion paths are enforced. (2) A Parameter-Space Restriction (PSR) mechanism, leveraging principles from representation engineering, to dynamically control the LLM\'s access to internal semantic clusters, with the goal of mitigating semantic drift and dangerous emergent behaviors. (3) A Secure, Self-Regulating Core that uses an OODA loop and a learning security module to adapt its defenses based on an immutable audit log. (4) A Multimodal Input Sandbox and Context-Defense mechanisms to address threats from non-textual data and long-term semantic poisoning. This paper outlines an evaluation plan designed to quantify the proposed architecture\'s effectiveness in reducing the Attack Success Rate (ASR) for form-first attacks and to measure its potential latency overhead.', 'abstract_zh': '大型语言模型应用的安全性从根本上受到了如提示注入和 jailbreaking 等“形式优先”攻击的挑战，这些攻击将恶意指令嵌入用户输入中。传统的防御措施依赖于事后输出过滤，往往脆弱且未能解决根本原因：模型无法区分可信指令与不受信数据。本文提出了 Countermind，一个多层安全架构，旨在将防御从被动的事后防御姿态转变为事先和推理中的主动强制执行模型。该架构提出了一个加固的外围结构，旨在结构性地验证和转换所有输入，并提出了一种内部治理机制，以在生成输出之前约束模型的语义处理路径。本文的主要贡献是概念设计：(1) 一种语义边界逻辑 (SBL)，包括一个强制的时间耦合文本加密器，旨在在所有摄入路径得到执行的情况下减少明文提示注入攻击的攻击面。(2) 一种参数空间限制 (PSR) 机制，利用表示工程原则，动态控制 LLM 对内部语义簇的访问，以减轻语义漂移和危险的新兴行为。(3) 一种安全且自我调节的核心，使用OODA循环和学习安全模块，根据不可变的审计日志调整其防御。(4) 多模态输入沙箱和上下文防御机制，以应对非文本数据和长期语义毒化的威胁。本文概述了评估计划，旨在量化所提出架构在降低形式优先攻击的成功率 (ASR) 方面的有效性，并测量其潜在的延迟开销。', 'title_zh': '反制思维：面向大型语言模型的多层安全架构'}
{'arxiv_id': 'arXiv:2510.11835', 'title': 'Data or Language Supervision: What Makes CLIP Better than DINO?', 'authors': 'Yiming Liu, Yuhui Zhang, Dhruba Ghosh, Ludwig Schmidt, Serena Yeung-Levy', 'link': 'https://arxiv.org/abs/2510.11835', 'abstract': "CLIP outperforms self-supervised models like DINO as vision encoders for vision-language models (VLMs), but it remains unclear whether this advantage stems from CLIP's language supervision or its much larger training data. To disentangle these factors, we pre-train CLIP and DINO under controlled settings -- using the same architecture, dataset, and training configuration -- achieving similar ImageNet accuracy. Embedding analysis shows that CLIP captures high-level semantics (e.g., object categories, text), while DINO is more responsive to low-level features like colors and styles. When integrated into VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive tasks, while DINO slightly outperforms on vision-centric ones. Variants of language supervision (e.g., sigmoid loss, pre-trained language encoders) yield limited gains. Our findings provide scientific insights into vision encoder design and its impact on VLM performance.", 'abstract_zh': 'CLIP在视觉语言模型的视觉编码器中优于自监督模型如DINO，但其优势是否来自于语言监督或更大的训练数据仍不明确。为了分离这些因素，我们在一个可控的环境中对CLIP和DINO进行预训练——使用相同的架构、数据集和训练配置，获得类似的ImageNet准确性。嵌入分析显示，CLIP捕捉到高层次语义（如物体类别、文本），而DINO对低级特征（如颜色和风格）更敏感。当将其集成到视觉语言模型中并在20个VQA基准上进行评估时，CLIP在文本密集型任务中表现更优，而DINO在视觉中心型任务中略胜一筹。不同形式的语言监督（如Sigmoid损失、预训练的语言编码器）带来的增益有限。我们的发现为视觉编码器设计及其对视觉语言模型性能的影响提供了科学见解。', 'title_zh': '数据监督或语言监督：是什么让CLIP比DINO更优秀？'}
{'arxiv_id': 'arXiv:2510.11812', 'title': 'PHANTOM RECALL: When Familiar Puzzles Fool Smart Models', 'authors': 'Souradeep Mukhopadhyay, Rishabh Baral, Nimeesh Mahajan, Samhitha Harish, Aswin RRV, Mihir Parmar, Mutsumi Nakamura, Chitta Baral', 'link': 'https://arxiv.org/abs/2510.11812', 'abstract': 'Large language models (LLMs) such as GPT, Gemini, and Claude often appear adept at solving classic logic puzzles--but how much genuine reasoning underlies their answers? Recent evidence suggests that these models frequently rely on memorized templates rather than reasoning from first principles. When puzzles are slightly modified, their performance collapses, revealing a striking fragility. In particular, we asked: Have LLMs addressed these issues? To what extent? How about perturbations to other puzzles? Is there a general way of reformulating the prompt so that the models do better? To examine these things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25 well-known logic puzzles and 149 carefully designed perturbations that preserve reasoning structure but alter superficial details and solutions. We evaluate eleven leading LLMs and identify a recurring failure mode--phantom recall--where models confidently reproduce memorized solutions or spurious rationales that no longer fit the altered scenario. To probe and mitigate this issue, we contribute three tools: (i) an automated logical-equivalence judge to detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error categories, and (iii) a prompting-based mitigation framework guided by these categories. Despite near-perfect accuracy on unmodified puzzles, models significantly underperform humans on perturbed ones, exhibiting both phantom recall and over-elaboration. Our findings reveal a crucial limitation: LLMs often fail to re-reason when contextual cues shift--highlighting the gap between linguistic fluency and logical understanding.', 'abstract_zh': '大型语言模型（LLMs）如GPT、Gemini和Claude通常擅长解决经典的逻辑谜题——但它们的答案背后的真正推理有多少是真实的？最近的证据表明，这些模型经常依赖于记忆中的模板，而不是从基本原理出发进行推理。当谜题稍作修改时，它们的表现会崩溃，显示出一种惊人的脆弱性。特别是我们询问：LLMs是否解决了这些问题？解决了多大程度？对其他谜题的扰动又如何？是否有一种普遍的方法重新表述提示，从而使模型表现更好？为了系统地研究这些问题，我们介绍了PHANTOM RECALL基准，包含25个经典的逻辑谜题和149个精心设计的扰动，这些扰动保留了推理结构但改变了表层细节和解决方案。我们评估了十一个领先的LLMs，并发现一个重复出现的失败模式——幻影回忆，模型自信地重现记忆中的解决方案或不再适用于修改后场景的虚假理由。为了探究并缓解这一问题，我们贡献了三个工具：（i）自动化逻辑等价判断器以检测推理不匹配；（ii）精细推理错误类别分类法；以及（iii）基于这些类别的提示引导缓解框架。尽管在未修改的谜题上表现出几乎完美的准确性，但在扰动谜题上，模型的人类表现显著下降，表现为幻影回忆和过度扩展。我们的发现揭示了一个关键限制：LLMs往往在上下文线索改变时未能重新推理——突显了语言流畅性和逻辑理解之间的差距。', 'title_zh': '幻影回忆：熟悉的谜题欺骗智能模型'}
{'arxiv_id': 'arXiv:2510.11759', 'title': 'AwareCompiler: Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework', 'authors': 'Hongyu Lin, Haolin Pan, Haoran Luo, Yuchen Li, Kaichun Yao, Libo Zhang, Mingjie Xing, Yanjun Wu', 'link': 'https://arxiv.org/abs/2510.11759', 'abstract': 'Compiler optimization is crucial for enhancing program performance by transforming the sequence of optimization passes while maintaining correctness. Despite the promising potential of large language models (LLMs)-based agent for software optimization, automating compiler optimization remains challenging due to: (1) semantic misalignment between abstract program representations and concrete optimization passes, (2) inefficient interaction mechanisms between agents and compiler environments, and (3) reward sparsity from the extensive decision-making process within large optimization spaces. This paper introduces \\textbf{AwareCompiler}, an agentic framework for compiler optimization that addresses these challenges through three key innovations: structured knowledge integration and dataset construction, knowledge-driven adaptive pass generation, and data-driven hybrid training pipeline. Experimental results on standard benchmarks demonstrate that AwareCompiler significantly outperforms existing baselines in both performance and efficiency, highlighting the effectiveness of our synergistic knowledge-data-driven approach. Our code is publicly available at this https URL.', 'abstract_zh': '编译器优化对于通过变换优化传递序列来提高程序性能至关重要。尽管基于大规模语言模型（LLM）的代理在软件优化方面前景诱人，但由于以下原因，自动化编译器优化仍然具有挑战性：（1）抽象程序表示与具体优化传递之间的语义对齐不一致，（2）代理与编译器环境之间的低效交互机制，以及（3）在广泛的优化空间内进行大量决策带来的稀疏奖励。本文介绍了AwareCompiler——一种通过三种关键创新解决这些挑战的编译器优化代理框架：结构化知识集成与数据集构建、知识驱动的自适应传递生成以及数据驱动的混合训练流水线。在标准基准上的实验结果表明，AwareCompiler在性能和效率方面显著优于现有基线，突显了我们的协同知识-数据驱动方法的有效性。我们的代码可在以下网址公开获取：this https URL。', 'title_zh': 'AwareCompiler: 具有代理上下文意识的协同知识-数据驱动编译器优化框架'}
{'arxiv_id': 'arXiv:2510.11734', 'title': 'Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need', 'authors': 'Yuqi Bai, Tianyu Huang, Kun Sun, Yuting Chen', 'link': 'https://arxiv.org/abs/2510.11734', 'abstract': 'This research focuses on using large language models (LLMs) to simulate social experiments, exploring their ability to emulate human personality in virtual persona role-playing. The research develops an end-to-end evaluation framework, including individual-level analysis of stability and identifiability, as well as population-level analysis called progressive personality curves to examine the veracity and consistency of LLMs in simulating human personality. Methodologically, this research proposes important modifications to traditional psychometric approaches (CFA and construct validity) which are unable to capture improvement trends in LLMs at their current low-level simulation, potentially leading to remature rejection or methodological misalignment. The main contributions of this research are: proposing a systematic framework for LLM virtual personality evaluation; empirically demonstrating the critical role of persona detail in personality simulation quality; and identifying marginal utility effects of persona profiles, especially a Scaling Law in LLM personality simulation, offering operational evaluation metrics and a theoretical foundation for applying large language models in social science experiments.', 'abstract_zh': '本研究致力于使用大型语言模型（LLMs）模拟社会实验，探索其在虚拟角色扮演中模仿人类个性的能力。研究开发了一个端到端的评估框架，包括个体层面的稳定性和可识别性分析，以及人口层面的渐进个性曲线分析，以检验LLMs在模拟人类个性时的真实性和一致性。在方法论上，本研究提出了对传统心理测量方法（CFA和结构效度）的重要修改，这些方法无法捕捉LLMs在其当前低水平模拟中的改进趋势，可能导致过早拒绝或方法论不匹配。本研究的主要贡献包括：提出了一套系统框架用于评估LLM虚拟个性；实证展示了个性细节在个性模拟质量中的关键作用；并识别了个性档案的边际效益效应，特别是在LLM个性模拟中的标度律，提供了操作性评估指标及应用大型语言模型于社会科学研究实验的理论基础。', 'title_zh': 'LLM模拟人格中的标度律：只需更详细及逼真的个性档案'}
{'arxiv_id': 'arXiv:2510.11728', 'title': 'Modeling Hypergraph Using Large Language Models', 'authors': 'Bingqiao Gu, Jiale Zeng, Xingqin Qi, Dong Li', 'link': 'https://arxiv.org/abs/2510.11728', 'abstract': 'Due to the advantages of hypergraphs in modeling high-order relationships in complex systems, they have been applied to higher-order clustering, hypergraph neural networks and computer vision. These applications rely heavily on access to high-quality, large-scale real-world hypergraph data. Yet, compared to traditional pairwise graphs, real hypergraph datasets remain scarce in both scale and diversity. This shortage significantly limits the development and evaluation of advanced hypergraph learning algorithms. Therefore, how to quickly generate large-scale hypergraphs that conform to the characteristics of real networks is a crucial task that has not received sufficient attention. Motivated by recent advances in large language models (LLMs), particularly their capabilities in semantic reasoning, structured generation, and simulating human behavior, we investigate whether LLMs can facilitate hypergraph generation from a fundamentally new perspective. We introduce HyperLLM, a novel LLM-driven hypergraph generator that simulates the formation and evolution of hypergraphs through a multi-agent collaboration. The framework integrates prompts and structural feedback mechanisms to ensure that the generated hypergraphs reflect key real-world patterns. Extensive experiments across diverse datasets demonstrate that HyperLLM achieves superior fidelity to structural and temporal hypergraph patterns, while requiring minimal statistical priors. Our findings suggest that LLM-based frameworks offer a promising new direction for hypergraph modeling.', 'abstract_zh': '基于大型语言模型的高阶网络生成方法', 'title_zh': '使用大型语言模型建模超图'}
{'arxiv_id': 'arXiv:2503.20934', 'title': 'Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method Refactoring', 'authors': 'Fraol Batole, Abhiram Bellur, Malinda Dilhara, Mohammed Raihan Ullah, Yaroslav Zharov, Timofey Bryksin, Kai Ishikawa, Haifeng Chen, Masaharu Morimoto, Shota Motoura, Takeo Hosomi, Tien N. Nguyen, Hridesh Rajan, Nikolaos Tsantalis, Danny Dig', 'link': 'https://arxiv.org/abs/2503.20934', 'abstract': 'MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools that recommend which methods to move and where, these recommendations do not align with how expert developers perform MOVEMETHOD. Given the extensive training of Large Language Models and their reliance upon naturalness of code, they should expertly recommend which methods are misplaced in a given class and which classes are better hosts. Our formative study of 2016 LLM recommendations revealed that LLMs give expert suggestions, yet they are unreliable: up to 80% of the suggestions are hallucinations. We introduce the first LLM fully powered assistant for MOVEMETHOD refactoring that automates its whole end-to-end lifecycle, from recommendation to execution. We designed novel solutions that automatically filter LLM hallucinations using static analysis from IDEs and a novel workflow that requires LLMs to be self-consistent, critique, and rank refactoring suggestions. As MOVEMETHOD refactoring requires global, projectlevel reasoning, we solved the limited context size of LLMs by employing refactoring-aware retrieval augment generation (RAG). Our approach, MM-assist, synergistically combines the strengths of the LLM, IDE, static analysis, and semantic relevance. In our thorough, multi-methodology empirical evaluation, we compare MM-assist with the previous state-of-the-art approaches. MM-assist significantly outperforms them: (i) on a benchmark widely used by other researchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a corpus of 210 recent refactorings from Open-source software, our Recall rates improve by at least 2.4x. Lastly, we conducted a user study with 30 experienced participants who used MM-assist to refactor their own code for one week. They rated 82.8% of MM-assist recommendations positively. This shows that MM-assist is both effective and useful.', 'abstract_zh': 'MOVEMETHOD是一种标志性重构。尽管存在许多推荐如何移动方法的研究工具，但这些推荐并不符合专家开发者进行MOVEMETHOD的方式。鉴于大型语言模型的广泛培训及其对代码自然性的依赖，它们应该能够准确推荐哪些方法在给定类中是错位的，以及哪些类是更好的宿主。我们的初步研究发现，2016年大型语言模型的建议虽然提供了专家级别的建议，但可靠性较差：多达80%的建议是妄想。我们引入了第一个全面支持大型语言模型的MOVEMETHOD重构辅助工具，实现了从推荐到执行的整个端到端生命周期自动化。我们设计了新的解决方案自动过滤大型语言模型的妄想，并采用了一种新的工作流，要求大型语言模型自我一致、批判性评估和排名重构建议。由于MOVEMETHOD重构需要全局性的项目级推理，我们通过采用重构感知检索增强生成（RAG）来解决大型语言模型的有限上下文问题。我们的方法MM-assist结合了大型语言模型、集成开发环境、静态分析和语义相关性的优势。在全面的、多方法论的实证评估中，我们对比了MM-assist与现有最先进的方法。MM-assist显著优于它们：（i）在其他研究人员广泛使用的基准上，我们的Recall@1和Recall@3提高了1.7倍；（ii）在210个开源软件的重构语料库上，我们的召回率提高了至少2.4倍。最后，我们进行了一项用户研究，30名经验丰富的参与者使用MM-assist对自己的代码进行了一周的重构。他们中82.8%的MM-assist建议得到了积极评价，这表明MM-assist既有效又实用。', 'title_zh': '利用大语言模型、集成开发环境和语义嵌入进行自动化移动方法重构'}
