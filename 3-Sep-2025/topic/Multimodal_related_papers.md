# Human-Inspired Soft Anthropomorphic Hand System for Neuromorphic Object and Pose Recognition Using Multimodal Signals 

**Title (ZH)**: 基于多模态信号的人类启发式软类人手系统及其在神经形态物体与姿态识别中的应用 

**Authors**: Fengyi Wang, Xiangyu Fu, Nitish Thakor, Gordon Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2509.02275)  

**Abstract**: The human somatosensory system integrates multimodal sensory feedback, including tactile, proprioceptive, and thermal signals, to enable comprehensive perception and effective interaction with the environment. Inspired by the biological mechanism, we present a sensorized soft anthropomorphic hand equipped with diverse sensors designed to emulate the sensory modalities of the human hand. This system incorporates biologically inspired encoding schemes that convert multimodal sensory data into spike trains, enabling highly-efficient processing through Spiking Neural Networks (SNNs). By utilizing these neuromorphic signals, the proposed framework achieves 97.14% accuracy in object recognition across varying poses, significantly outperforming previous studies on soft hands. Additionally, we introduce a novel differentiator neuron model to enhance material classification by capturing dynamic thermal responses. Our results demonstrate the benefits of multimodal sensory fusion and highlight the potential of neuromorphic approaches for achieving efficient, robust, and human-like perception in robotic systems. 

**Abstract (ZH)**: 人类本体感觉系统整合多模态感觉反馈，包括触觉、本体感觉和温度信号，以实现对环境的全面感知和有效交互。受生物机制启发，我们提出了一种装备多种传感器的人工仿生软手，旨在模拟人手的感觉模态。该系统结合了生物启发的编码方案，将多模态感觉数据转换为尖锐脉冲序列，通过脉冲神经网络（SNNs）实现高效处理。利用这些类神经形态信号，所提出的框架在不同姿态下实现了97.14%的物体识别准确率，显著优于先前关于软手的研究。此外，我们引入了一种新的微分神经元模型，通过捕捉动态的温度响应来增强材料分类能力。实验结果表明，多模态感觉融合的益处，并突显了神经形态方法在实现机器人系统中高效、鲁棒且类人感知方面的潜力。 

---
# MIRAGE: Multimodal Intention Recognition and Admittance-Guided Enhancement in VR-based Multi-object Teleoperation 

**Title (ZH)**: MIRAGE: 多模态意图识别与接纳指导增强在基于VR的多对象远程操控中的应用 

**Authors**: Chi Sun, Xian Wang, Abhishek Kumar, Chengbin Cui, Lik-Hang Lee  

**Link**: [PDF](https://arxiv.org/pdf/2509.01996)  

**Abstract**: Effective human-robot interaction (HRI) in multi-object teleoperation tasks faces significant challenges due to perceptual ambiguities in virtual reality (VR) environments and the limitations of single-modality intention recognition. This paper proposes a shared control framework that combines a virtual admittance (VA) model with a Multimodal-CNN-based Human Intention Perception Network (MMIPN) to enhance teleoperation performance and user experience. The VA model employs artificial potential fields to guide operators toward target objects by adjusting admittance force and optimizing motion trajectories. MMIPN processes multimodal inputs, including gaze movement, robot motions, and environmental context, to estimate human grasping intentions, helping to overcome depth perception challenges in VR. Our user study evaluated four conditions across two factors, and the results showed that MMIPN significantly improved grasp success rates, while the VA model enhanced movement efficiency by reducing path lengths. Gaze data emerged as the most crucial input modality. These findings demonstrate the effectiveness of combining multimodal cues with implicit guidance in VR-based teleoperation, providing a robust solution for multi-object grasping tasks and enabling more natural interactions across various applications in the future. 

**Abstract (ZH)**: 多模态 CNN 基于的人意图感知网络与虚拟阻抗模型的结合在多对象远程操作中的有效人机交互 

---
# Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought Imagination 

**Title (ZH)**: 执画在心:基于链式思维想象的精确图像编辑 

**Authors**: Ziyun Zeng, Junhao Zhang, Wei Li, Mike Zheng Shou  

**Link**: [PDF](https://arxiv.org/pdf/2509.01986)  

**Abstract**: In recent years, integrating multimodal understanding and generation into a single unified model has emerged as a promising paradigm. While this approach achieves strong results in text-to-image (T2I) generation, it still struggles with precise image editing. We attribute this limitation to an imbalanced division of responsibilities. The understanding module primarily functions as a translator that encodes user instructions into semantic conditions, while the generation module must simultaneously act as designer and painter, inferring the original layout, identifying the target editing region, and rendering the new content. This imbalance is counterintuitive because the understanding module is typically trained with several times more data on complex reasoning tasks than the generation module. To address this issue, we introduce Draw-In-Mind (DIM), a dataset comprising two complementary subsets: (i) DIM-T2I, containing 14M long-context image-text pairs to enhance complex instruction comprehension; and (ii) DIM-Edit, consisting of 233K chain-of-thought imaginations generated by GPT-4o, serving as explicit design blueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable SANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM dataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale, DIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and GEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1 and Step1X-Edit. These findings demonstrate that explicitly assigning the design responsibility to the understanding module provides significant benefits for image editing. Our dataset and models will be available at this https URL. 

**Abstract (ZH)**: 近年来，将多模态理解和生成整合到单一统一模型中已成为一个有前途的范式。虽然这种方法在文本到图像（T2I）生成任务中取得了强大的成果，但在精确图像编辑方面仍然面临挑战。我们将其局限性归因于职责分配的不平衡。理解模块主要作为翻译器，将用户指令编码为语义条件，而生成模块则必须同时承担设计师和画家的角色，推断原始布局、识别目标编辑区域并呈现新的内容。这种不平衡违反直觉，因为理解模块通常在复杂推理任务上接受的数据量是生成模块的几倍。为了解决这一问题，我们引入了Draw-In-Mind (DIM)，该数据集包含两个互补子集：(i) DIM-T2I，包含1400万长上下文的图像文本对，以增强复杂指令的解释；(ii) DIM-Edit，包含由GPT-4o生成的23.3万条详尽想象的图像，作为明确的设计蓝图，用于图像编辑。我们将一个冻结的Qwen2.5-VL-3B与一个可训练的SANA1.5-1.6B通过轻量级的两层MLP连接，并在所提出的DIM数据集上进行训练，得到了DIM-4.6B-T2I/Edit。尽管其参数量相对较小，但DIM-4.6B-Edit在ImgEdit和GEdit-Bench基准测试中实现了SOTA或竞争性表现，超越了更大的模型如UniWorld-V1和Step1X-Edit。这些发现表明，明确将设计责任分配给理解模块在图像编辑中提供了显著优势。我们的数据集和模型将可通过以下链接获取：this https URL。 

---
# Structure-aware Contrastive Learning for Diagram Understanding of Multimodal Models 

**Title (ZH)**: 结构感知对比学习多模态模型中的图表理解 

**Authors**: Hiroshi Sasaki  

**Link**: [PDF](https://arxiv.org/pdf/2509.01959)  

**Abstract**: Multimodal models, such as the Contrastive Language-Image Pre-training (CLIP) model, have demonstrated remarkable success in aligning visual and linguistic representations. However, these models exhibit limitations when applied to specialised visual domains, such as diagrams, which encode structured, symbolic information distinct from that of natural imagery.
In this paper, we introduce a novel training paradigm explicitly designed to enhance the comprehension of diagrammatic images within vision-language models. Our approach uses ``hard'' samples for our proposed contrastive learning that incorporates two specialised loss functions that leverage the inherent structural properties of diagrams. By integrating these objectives into model training, our method enables models to develop a more structured and semantically coherent understanding of diagrammatic content.
We empirically validate our approach on a benchmark dataset of flowcharts, as a representative class of diagrammatic imagery, demonstrating substantial improvements over standard CLIP and conventional hard negative CLIP learning paradigms for both image-text matching and visual question answering tasks. Our findings underscore the significance of tailored training strategies for specialised tasks and contribute to advancing diagrammatic understanding within the broader landscape of vision-language integration. 

**Abstract (ZH)**: 多模态模型，如对比语言-图像预训练（CLIP）模型，已经在视觉和语言表示的对齐方面取得了显著的成功。然而，这些模型在应用到专门的视觉领域，如图表中时表现出局限性，因为图表编码了与自然图像不同的结构化和符号性信息。

在本文中，我们提出了一种新的训练 paradign，专门设计用于增强视觉语言模型对图表图像的理解。我们的方法使用“硬”样本进行对比学习，结合了两种专门的损失函数，利用了图表固有的结构特性。通过将这些目标纳入模型训练中，我们的方法使模型能够发展出更结构化和语义一致的图表内容理解。

我们通过基准数据集中的流程图，代表性的图表图像类，实验验证了我们的方法，证明了在图像文本匹配和视觉问答任务上，我们的方法相比标准CLIP和传统的硬负例CLIP训练范式有显著的改进。我们的研究成果强调了针对专门任务定制训练策略的重要性，并促进了图表理解在视觉语言整合领域的进展。 

---
# DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion 

**Title (ZH)**: DynaMind: 通过时间动态和多模态语义对齐重建基于EEG的动态视觉场景 

**Authors**: Junxiang Liu, Junming Lin, Jiangtong Li, Jie Li  

**Link**: [PDF](https://arxiv.org/pdf/2509.01177)  

**Abstract**: Reconstruction dynamic visual scenes from electroencephalography (EEG) signals remains a primary challenge in brain decoding, limited by the low spatial resolution of EEG, a temporal mismatch between neural recordings and video dynamics, and the insufficient use of semantic information within brain activity. Therefore, existing methods often inadequately resolve both the dynamic coherence and the complex semantic context of the perceived visual stimuli. To overcome these limitations, we introduce DynaMind, a novel framework that reconstructs video by jointly modeling neural dynamics and semantic features via three core modules: a Regional-aware Semantic Mapper (RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video Reconstructor (DGVR). The RSM first utilizes a regional-aware encoder to extract multimodal semantic features from EEG signals across distinct brain regions, aggregating them into a unified diffusion prior. In the mean time, the TDA generates a dynamic latent sequence, or blueprint, to enforce temporal consistency between the feature representations and the original neural recordings. Together, guided by the semantic diffusion prior, the DGVR translates the temporal-aware blueprint into a high-fidelity video reconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art (SOTA), boosting reconstructed video accuracies (video- and frame-based) by 12.5 and 10.3 percentage points, respectively. It also achieves a leap in pixel-level quality, showing exceptional visual fidelity and temporal coherence with a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical advancement, bridging the gap between neural dynamics and high-fidelity visual semantics. 

**Abstract (ZH)**: 从脑电图（EEG）信号重建动态视觉场景：Dynamind框架 

---
# Multimodal Iterative RAG for Knowledge Visual Question Answering 

**Title (ZH)**: 多模态迭代检索生成在知识视觉问答中的应用 

**Authors**: Changin Choi, Wonseok Lee, Jungmin Ko, Wonjong Rhee  

**Link**: [PDF](https://arxiv.org/pdf/2509.00798)  

**Abstract**: While Multimodal Large Language Models (MLLMs) have significantly advanced multimodal understanding, their performance remains limited on knowledge-intensive visual questions that require external knowledge beyond the image. Retrieval-Augmented Generation (RAG) has become a promising solution for providing models with external knowledge, its conventional single-pass framework often fails to gather sufficient knowledge. To overcome this limitation, we propose MI-RAG, a Multimodal Iterative RAG framework that leverages reasoning to enhance retrieval and update reasoning over newly retrieved knowledge across modalities. At each iteration, MI-RAG leverages an accumulated reasoning record to dynamically formulate a multi-query. These queries then drive a joint search across heterogeneous knowledge bases containing both visually-grounded and textual knowledge. The newly acquired knowledge is synthesized into the reasoning record, progressively refining understanding across iterations. Experiments on challenging benchmarks, including Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG significantly improves both retrieval recall and answer accuracy, establishing a scalable approach for compositional reasoning in knowledge-intensive VQA. 

**Abstract (ZH)**: 多模态迭代RAG框架：增强检索并跨模态更新推理以提升知识密集型视觉问答任务 

---
# Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model 

**Title (ZH)**: 融合提升：融合视觉编码器提升多模态语言模型 

**Authors**: Yifei She, Huangxuan Wu  

**Link**: [PDF](https://arxiv.org/pdf/2509.00664)  

**Abstract**: Multimodal Large Language Models (MLLMs) have made significant progress in bridging visual perception with high-level textual reasoning. However, they face a fundamental contradiction: while excelling at complex semantic understanding, these models often fail at basic visual tasks that require precise detail perception. This deficiency primarily stems from the prevalent architectural reliance on a single vision encoder optimized for high-level semantic alignment, which inherently sacrifices the ability to capture fine-grained visual information. To address this issue, we introduce Fusion to Enhance (FtZ), a novel vision tower framework. FtZ moves beyond the single-encoder design by innovatively composing a semantically powerful anchor encoder with a perception-rich augmenting encoder via a lightweight Multi-Head Cross-Attention mechanism. Experimental results demonstrate that on several challenging benchmarks demanding fine-grained visual understanding, such as TextVQA, POPE, MMMU, MME and MM-Vet, our FtZ model significantly outperforms baselines that use only a single encoder or existing feature fusion methods. This work proves that composing heterogeneous expert encoders is an efficient and effective path to overcoming the visual perception bottleneck in current MLLMs, offering a new design paradigm for building next-generation AI systems with stronger perceptual capabilities. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）在串联视觉感知与高级文本推理方面取得了显著进展。然而，它们面临一个根本性的矛盾：虽然在复杂的语义理解上表现出色，但在需要精确细节感知的基本视觉任务上经常失败。这一缺陷主要源自对单一优化于高层语义对齐的视觉编码器的普遍依赖，这本身牺牲了捕捉细粒度视觉信息的能力。为了解决这一问题，我们引入了融合以提升(FtZ)的新型视觉塔框架。FtZ超越了单编码器设计，通过轻量级的多头交叉注意力机制创新性地结合了一个强大的语义锚编码器和一个丰富的感知增强编码器。实验结果表明，在如TextVQA、POPE、MMMU、MME和MM-Vet等多个需要细粒度视觉理解的挑战性基准上，我们的FtZ模型显著优于仅使用单个编码器或现有特征融合方法的基线模型。这项工作证明了结合异构专家编码器是克服当前MLLMs视觉感知瓶颈的有效途径，为构建具备更强感知能力的下一代AI系统提供了新的设计范式。 

---
# VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding 

**Title (ZH)**: VideoRewardBench: 多模态奖励模型在视频理解中的全面评估 

**Authors**: Zhihong Zhang, Xiaojian Huang, Jin Xu, Zhuodong Luo, Xinzhi Wang, Jiansheng Wei, Xuejin Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.00484)  

**Abstract**: Multimodal reward models (MRMs) play a crucial role in the training, inference, and evaluation of Large Vision Language Models (LVLMs) by assessing response quality. However, existing benchmarks for evaluating MRMs in the video domain suffer from a limited number and diversity of questions, a lack of comprehensive evaluation dimensions, and inadequate evaluation of diverse types of MRMs. To address these gaps, we introduce VideoRewardBench, the first comprehensive benchmark covering four core aspects of video understanding: perception, knowledge, reasoning, and safety. Through our AI-assisted data pipeline, we curate a high-quality preference dataset of 1,563 annotated samples, including 1,482 unique videos and 1,559 distinct questions--15 times the number found in the most question-rich prior benchmark. Each sample is a triplet consisting of a video-text prompt, a chosen response, and a rejected response. We also conduct a comprehensive evaluation across 28 multimodal reward models spanning three categories: generative, discriminative, and semi-scalar. Results show that even the top-performing model GPT-4o achieves only 57.0% overall accuracy, and the state-of-the-art open-source model Qwen2.5-VL-72B reaches merely 53.3%. Our analysis further reveals three key insights: (i) MRMs trained with reinforcement learning (RL) do not necessarily exhibit stronger cross-modal generalization than those trained without RL; (ii) except for discriminative MRMs, other types of MRMs across varying model capacities can benefit from inference-time scaling; and (iii) variations in input video frame count have different effects on different types of MRMs. We believe VideoRewardBench offers a challenging and valuable benchmark for advancing the evaluation and development of MRMs in the video domain. 

**Abstract (ZH)**: 多模态奖励模型综合基准：VideoRewardBench 

---
# SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding 

**Title (ZH)**: SurgLLM：一种具有空间聚焦和时间意识的多功能多模态手术视频理解模型 

**Authors**: Zhen Chen, Xingjian Luo, Kun Yuan, Jinlin Wu, Danny T.M. Chan, Nassir Navab, Hongbin Liu, Zhen Lei, Jiebo Luo  

**Link**: [PDF](https://arxiv.org/pdf/2509.00357)  

**Abstract**: Surgical video understanding is crucial for facilitating Computer-Assisted Surgery (CAS) systems. Despite significant progress in existing studies, two major limitations persist, including inadequate visual content perception and insufficient temporal awareness in surgical videos, and hinder the development of versatile CAS solutions. In this work, we propose the SurgLLM framework, an effective large multimodal model tailored for versatile surgical video understanding tasks with enhanced spatial focus and temporal awareness. Specifically, to empower the spatial focus of surgical videos, we first devise Surgical Context-aware Multimodal Pretraining (Surg-Pretrain) for the video encoder of SurgLLM, by performing instrument-centric Masked Video Reconstruction (MV-Recon) and subsequent multimodal alignment. To incorporate surgical temporal knowledge into SurgLLM, we further propose Temporal-aware Multimodal Tuning (TM-Tuning) to enhance temporal reasoning with interleaved multimodal embeddings. Moreover, to accommodate various understanding tasks of surgical videos without conflicts, we devise a Surgical Task Dynamic Ensemble to efficiently triage a query with optimal learnable parameters in our SurgLLM. Extensive experiments performed on diverse surgical video understanding tasks, including captioning, general VQA, and temporal VQA, demonstrate significant improvements over the state-of-the-art approaches, validating the effectiveness of our SurgLLM in versatile surgical video understanding. The source code is available at this https URL. 

**Abstract (ZH)**: 手术视频理解对于促进计算机辅助手术系统（CAS）至关重要。尽管现有研究取得了显著进展，但仍存在两个主要限制，包括手术视频中的不足视觉内容感知和缺乏时间意识，阻碍了多功能CAS解决方案的发展。在本文中，我们提出了SurgLLM框架，这是一种针对增强空间聚焦和时间意识的多功能手术视频理解任务的有效多模态模型。具体而言，为了增强SurgLLM的空间聚焦，我们首先设计了手术上下文感知的多模态预训练（Surg-Pretrain），通过仪器为中心的掩码视频重建（MV-Recon）和后续的多模态对齐来实现。为了将手术时间知识融入SurgLLM，我们进一步提出了时间感知的多模态调整（TM-Tuning），以交错的多模态嵌入增强时间推理。此外，为了在不冲突的情况下适应各种手术视频理解任务，我们设计了一种手术任务动态集成，以有效地对查询进行优先处理，并在我们的SurgLLM中使用可学习参数。在包括描述、通用VQA和时间VQA等多种手术视频理解任务上的广泛实验表明，SurgLLM在这些任务上的表现显著优于现有最新方法，验证了SurgLLM在多功能手术视频理解中的有效性。源代码可在以下链接获取。 

---
# AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with Imagery and Sensor Data 

**Title (ZH)**: AQFusionNet：基于图像和传感器数据的多模态深度学习空气质量指数预测 

**Authors**: Koushik Ahmed Kushal, Abdullah Al Mamun  

**Link**: [PDF](https://arxiv.org/pdf/2509.00353)  

**Abstract**: Air pollution monitoring in resource-constrained regions remains challenging due to sparse sensor deployment and limited infrastructure. This work introduces AQFusionNet, a multimodal deep learning framework for robust Air Quality Index (AQI) prediction. The framework integrates ground-level atmospheric imagery with pollutant concentration data using lightweight CNN backbones (MobileNetV2, ResNet18, EfficientNet-B0). Visual and sensor features are combined through semantically aligned embedding spaces, enabling accurate and efficient prediction. Experiments on more than 8,000 samples from India and Nepal demonstrate that AQFusionNet consistently outperforms unimodal baselines, achieving up to 92.02% classification accuracy and an RMSE of 7.70 with the EfficientNet-B0 backbone. The model delivers an 18.5% improvement over single-modality approaches while maintaining low computational overhead, making it suitable for deployment on edge devices. AQFusionNet provides a scalable and practical solution for AQI monitoring in infrastructure-limited environments, offering robust predictive capability even under partial sensor availability. 

**Abstract (ZH)**: 资源受限地区空气污染监测由于传感器部署稀疏和基础设施有限仍具有挑战性。本文介绍了一种多模态深度学习框架AQFusionNet，用于稳健的空气质量指数（AQI）预测。该框架通过使用轻量级CNN骨干网络（MobileNetV2、ResNet18、EfficientNet-B0）将地面大气图像与污染物浓度数据结合在一起。视觉和传感器特征通过语义对齐的嵌入空间进行组合，实现准确且高效的预测。印度和尼泊尔超过8000个样本的实验表明，AQFusionNet在单模态基线方法上表现出色，使用EfficientNet-B0骨干网络时分类准确率达到92.02%，RMSE为7.70。该模型在保持低计算开销的同时，比单模态方法高出18.5%的性能，适合部署在边缘设备上。AQFusionNet为基础设施受限环境中的空气质量指数监测提供了可扩展且实用的解决方案，即使在部分传感器可用的情况下也能提供稳健的预测能力。 

---
# Multimodal Deep Learning for Phyllodes Tumor Classification from Ultrasound and Clinical Data 

**Title (ZH)**: 多模态深度学习在超声和临床数据联合识别fyloides肿瘤中的应用 

**Authors**: Farhan Fuad Abir, Abigail Elliott Daly, Kyle Anderman, Tolga Ozmen, Laura J. Brattain  

**Link**: [PDF](https://arxiv.org/pdf/2509.00213)  

**Abstract**: Phyllodes tumors (PTs) are rare fibroepithelial breast lesions that are difficult to classify preoperatively due to their radiological similarity to benign fibroadenomas. This often leads to unnecessary surgical excisions. To address this, we propose a multimodal deep learning framework that integrates breast ultrasound (BUS) images with structured clinical data to improve diagnostic accuracy. We developed a dual-branch neural network that extracts and fuses features from ultrasound images and patient metadata from 81 subjects with confirmed PTs. Class-aware sampling and subject-stratified 5-fold cross-validation were applied to prevent class imbalance and data leakage. The results show that our proposed multimodal method outperforms unimodal baselines in classifying benign versus borderline/malignant PTs. Among six image encoders, ConvNeXt and ResNet18 achieved the best performance in the multimodal setting, with AUC-ROC scores of 0.9427 and 0.9349, and F1-scores of 0.6720 and 0.7294, respectively. This study demonstrates the potential of multimodal AI to serve as a non-invasive diagnostic tool, reducing unnecessary biopsies and improving clinical decision-making in breast tumor management. 

**Abstract (ZH)**: 叶状肿瘤（PTs）是一类罕见的纤维上皮性乳腺病变，由于其与良性纤维腺瘤在影像学上的相似性，术前难以分类，常常导致不必要的手术切除。为解决这一问题，我们提出了一种多模态深度学习框架，结合乳腺超声（BUS）图像和结构化临床数据以提高诊断准确性。我们开发了一个双分支神经网络，从81例确诊为PTs的患者中提取并融合超声图像特征和患者元数据特征。采用类感知采样和受试者分层5折交叉验证以防止类别不平衡和数据泄漏。结果表明，我们提出的方法在分类良性与边界/恶性PTs方面优于单一模态基线。在六种图像编码器中，ConvNeXt和ResNet18在多模态设置下表现最佳，AUC-ROC得分为0.9427和0.9349，F1得分为0.6720和0.7294。本研究展示了多模态AI作为无创诊断工具的潜力，减少了不必要的活检并改善了乳腺肿瘤管理中的临床决策。 

---
# Traj-MLLM: Can Multimodal Large Language Models Reform Trajectory Data Mining? 

**Title (ZH)**: Traj-MLLM：多模态大型语言模型能否改造轨迹数据挖掘？ 

**Authors**: Shuo Liu, Di Yao, Yan Lin, Gao Cong, Jingping Bi  

**Link**: [PDF](https://arxiv.org/pdf/2509.00053)  

**Abstract**: Building a general model capable of analyzing human trajectories across different geographic regions and different tasks becomes an emergent yet important problem for various applications. However, existing works suffer from the generalization problem, \ie, they are either restricted to train for specific regions or only suitable for a few tasks. Given the recent advances of multimodal large language models (MLLMs), we raise the question: can MLLMs reform current trajectory data mining and solve the problem? Nevertheless, due to the modality gap of trajectory, how to generate task-independent multimodal trajectory representations and how to adapt flexibly to different tasks remain the foundational challenges. In this paper, we propose \texttt{Traj-MLLM}}, which is the first general framework using MLLMs for trajectory data mining. By integrating multiview contexts, \texttt{Traj-MLLM}} transforms raw trajectories into interleaved image-text sequences while preserving key spatial-temporal characteristics, and directly utilizes the reasoning ability of MLLMs for trajectory analysis. Additionally, a prompt optimization method is proposed to finalize data-invariant prompts for task adaptation. Extensive experiments on four publicly available datasets show that \texttt{Traj-MLLM}} outperforms state-of-the-art baselines by $48.05\%$, $15.52\%$, $51.52\%$, $1.83\%$ on travel time estimation, mobility prediction, anomaly detection and transportation mode identification, respectively. \texttt{Traj-MLLM}} achieves these superior performances without requiring any training data or fine-tuning the MLLM backbones. 

**Abstract (ZH)**: 利用 multimodal 大语言模型构建跨地理区域和任务的通用轨迹分析模型 

---
# Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary 

**Title (ZH)**: 基于深度学习的厨艺场景中多模态物体检测与运动分析 

**Authors**: Tahoshin Alam Ishat  

**Link**: [PDF](https://arxiv.org/pdf/2509.00033)  

**Abstract**: This is a research exploring existing models and fine tuning them to combine a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence and a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to predict the recipe and generate text creating a step by step guide for the cooking procedure. All the data were gathered by the author for a robust task specific system to perform best in complex and challenging environments proving the extension and endless application of computer vision in daily activities such as kitchen work. This work extends the field for many more crucial task of our day to day life. 

**Abstract (ZH)**: 本研究探索现有模型并微调它们，结合使用YOLOv8分割模型、基于手部动作序列训练的LSTM模型以及whisper-base进行ASR处理，提取足够的数据以供TinyLLaMa等LLM预测食谱和生成步骤指南，从而创建烹饪过程的逐步指导。所有数据均由作者收集，以构建适用于复杂和挑战性环境的健壯任务特定系统，证明了计算机视觉在日常活动如厨房工作中的扩展及其无限应用前景。本工作扩大了计算机视觉在我们日常生活中许多关键任务领域的应用范围。 

---
