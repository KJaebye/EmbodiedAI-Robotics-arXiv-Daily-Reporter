{'arxiv_id': 'arXiv:2511.16624', 'title': 'SAM 3D: 3Dfy Anything in Images', 'authors': 'SAM 3D Team, Xingyu Chen, Fu-Jen Chu, Pierre Gleize, Kevin J Liang, Alexander Sax, Hao Tang, Weiyao Wang, Michelle Guo, Thibaut Hardin, Xiang Li, Aohan Lin, Jiawei Liu, Ziqi Ma, Anushka Sagar, Bowen Song, Xiaodong Wang, Jianing Yang, Bowen Zhang, Piotr Dollár, Georgia Gkioxari, Matt Feiszli, Jitendra Malik', 'link': 'https://arxiv.org/abs/2511.16624', 'abstract': 'We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D "data barrier". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.', 'abstract_zh': 'SAM 3D：一种基于视觉的三维物体重建生成模型', 'title_zh': 'SAM 3D: 图像中anything的3D化'}
{'arxiv_id': 'arXiv:2511.16619', 'title': 'Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning', 'authors': 'Satyam Gaba', 'link': 'https://arxiv.org/abs/2511.16619', 'abstract': 'Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.\nAdditionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.', 'abstract_zh': '长尾分布2D物体检测中的挑战及解决方案：基于LVISv1数据集的研究', 'title_zh': '改进长尾目标检测：基于平衡组Softmax和度量学习的方法'}
{'arxiv_id': 'arXiv:2511.16617', 'title': 'Generative AI for Enhanced Wildfire Detection: Bridging the Synthetic-Real Domain Gap', 'authors': 'Satyam Gaba', 'link': 'https://arxiv.org/abs/2511.16617', 'abstract': 'The early detection of wildfires is a critical environmental challenge, with timely identification of smoke plumes being key to mitigating large-scale damage. While deep neural networks have proven highly effective for localization tasks, the scarcity of large, annotated datasets for smoke detection limits their potential. In response, we leverage generative AI techniques to address this data limitation by synthesizing a comprehensive, annotated smoke dataset. We then explore unsupervised domain adaptation methods for smoke plume segmentation, analyzing their effectiveness in closing the gap between synthetic and real-world data. To further refine performance, we integrate advanced generative approaches such as style transfer, Generative Adversarial Networks (GANs), and image matting. These methods aim to enhance the realism of synthetic data and bridge the domain disparity, paving the way for more accurate and scalable wildfire detection models.', 'abstract_zh': '早期野火检测是关键的环境挑战，及时识别烟雾柱对于减轻大规模损害至关重要。虽然深度神经网络在定位任务中表现出高度有效性，但由于烟雾检测标注数据集的稀缺性限制了其潜在应用。为此，我们利用生成对抗技术解决这一数据局限性，合成一个全面且标注的烟雾数据集。然后，我们探讨无监督领域适应方法在烟雾柱分割中的应用，分析其在缩小合成数据与实际数据差距方面的有效性。为进一步提高性能，我们整合了先进的生成方法，如风格迁移、生成对抗网络（GANs）和图像抠图技术，旨在增强合成数据的逼真度并弥合领域差异，从而为更准确和可扩展的野火检测模型铺平道路。', 'title_zh': '增强 wildfires 检测的生成 AI：弥合合成-真实领域差距'}
{'arxiv_id': 'arXiv:2511.16541', 'title': 'Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution', 'authors': 'Jaime Álvarez Urueña, David Camacho, Javier Huertas Tato', 'link': 'https://arxiv.org/abs/2511.16541', 'abstract': 'The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical.\nThis work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators.\nWith merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3\\%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70\\% and 4.27\\% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.', 'abstract_zh': '生成人工智能的快速进步使得合成图像越来越难以与真实内容区分开来，对数字媒体完整性构成了重大挑战。这个问题因新型生成模型的加速发布周期而加剧，使得依赖于定期重新训练的传统检测方法在计算上不可行且操作上不实用。\n\n本文提出了一种新颖的两阶段检测框架，旨在解决合成图像检测中的泛化挑战。第一阶段采用通过监督对比学习训练的视觉深度学习模型，从输入图像中提取具有区分性的嵌入表示。关键在于，该模型是在战略分割的可用生成器子集上进行训练的，特定的架构被排除在训练之外，以严格消除跨生成器的泛化能力。第二阶段利用在少量未见过的测试生成器样本中进行少量样本学习范式下训练的k-最邻近(k-NN)分类器，在学习嵌入空间中进行操作。\n\n在少量样本学习范式中，每个类别仅有150张图像，这些图像很容易从当前的生成模型中获得，所提出的框架实现了平均检测准确率91.3%，比现有方法提高了5.2个百分点。在开放集分类的上下文中，对于源归属任务，所提出的方法分别在AUC和OSCR上取得了14.70%和4.27%的改进，标志着朝着适应演化中的生成AI环境的稳健、可扩展的法医归属系统迈出了重要一步，而不需要耗尽性的重新训练协议。', 'title_zh': '监督对比学习在少量样本AI生成图像检测与归属中的应用'}
{'arxiv_id': 'arXiv:2511.16501', 'title': 'ODE-ViT: Plug & Play Attention Layer from the Generalization of the ViT as an Ordinary Differential Equation', 'authors': 'Carlos Boned Riera, David Romero Sanchez, Oriol Ramos Terrades', 'link': 'https://arxiv.org/abs/2511.16501', 'abstract': 'In recent years, increasingly large models have achieved outstanding performance across CV tasks. However, these models demand substantial computational resources and storage, and their growing complexity limits our understanding of how they make decisions. Most of these architectures rely on the attention mechanism within Transformer-based designs. Building upon the connection between residual neural networks and ordinary differential equations (ODEs), we introduce ODE-ViT, a Vision Transformer reformulated as an ODE system that satisfies the conditions for well-posed and stable dynamics. Experiments on CIFAR-10 and CIFAR-100 demonstrate that ODE-ViT achieves stable, interpretable, and competitive performance with up to one order of magnitude fewer parameters, surpassing prior ODE-based Transformer approaches in classification tasks. We further propose a plug-and-play teacher-student framework in which a discrete ViT guides the continuous trajectory of ODE-ViT by treating the intermediate representations of the teacher as solutions of the ODE. This strategy improves performance by more than 10% compared to training a free ODE-ViT from scratch.', 'abstract_zh': '近年来，越来越大的模型在CV任务中取得了卓越的性能。然而，这些模型需要大量的计算资源和存储空间，其复杂的结构限制了我们对其决策过程的理解。大多数这些架构依赖于Transformer设计中的注意力机制。基于残差神经网络与常微分方程（ODEs）之间的联系，我们提出了ODE-ViT，这是一种以ODE系统形式重构的Vision Transformer，满足良好定义和稳定动力学的条件。实验表明，ODE-ViT在CIFAR-10和CIFAR-100上的表现稳定、可解释且具有竞争力，参数量减少了一个数量级，并且在分类任务上超越了之前的基于ODE的Transformer方法。我们进一步提出了一种即插即用的教师-学生框架，其中离散的ViT通过将教师的中间表示视为ODE的解来引导连续的ODE-ViT轨迹。这种方法相较于从头训练一个自由的ODE-ViT，性能提升超过10%。', 'title_zh': 'ODE-ViT: 从ViT作为常微分方程的推广实现即插即用注意力层'}
{'arxiv_id': 'arXiv:2511.16494', 'title': 'Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation', 'authors': 'Zongcai Tan, Lan Wei, Dandan Zhang', 'link': 'https://arxiv.org/abs/2511.16494', 'abstract': 'Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent this http URL work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.', 'abstract_zh': '基于波光学的物理引导深度生成学习框架在微纳机器人姿态估计中的高效合成高保真显微镜图像', 'title_zh': '基于物理的机器学习在微小物体姿态估计中的高效模拟到现实数据增强'}
{'arxiv_id': 'arXiv:2511.16026', 'title': 'Towards a Safer and Sustainable Manufacturing Process: Material classification in Laser Cutting Using Deep Learning', 'authors': 'Mohamed Abdallah Salem, Hamdy Ahmed Ashur, Ahmed Elshinnawy', 'link': 'https://arxiv.org/abs/2511.16026', 'abstract': "Laser cutting is a widely adopted technology in material processing across various industries, but it generates a significant amount of dust, smoke, and aerosols during operation, posing a risk to both the environment and workers' health. Speckle sensing has emerged as a promising method to monitor the cutting process and identify material types in real-time. This paper proposes a material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process. The proposed method involves training a convolutional neural network (CNN) on a dataset of laser speckle patterns to recognize distinct material types for safe and efficient cutting. Previous methods for material classification using speckle sensing may face issues when the color of the laser used to produce the speckle pattern is changed. Experiments conducted in this study demonstrate that the proposed method achieves high accuracy in material classification, even when the laser color is changed. The model achieved an accuracy of 98.30 % on the training set and 96.88% on the validation set. Furthermore, the model was evaluated on a set of 3000 new images for 30 different materials, achieving an F1-score of 0.9643. The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing.", 'abstract_zh': '基于-speckle-模式的深度学习材料分类方法在激光切割中的应用', 'title_zh': '面向更安全和可持续的制造过程：激光切割中基于深度学习的材料分类'}
{'arxiv_id': 'arXiv:2511.15884', 'title': 'Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes', 'authors': 'Yintao Ma, Sajjad Pakdamansavoji, Amir Rasouli, Tongtong Cao', 'link': 'https://arxiv.org/abs/2511.15884', 'abstract': 'Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.\nTo this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.', 'abstract_zh': '基于类别级别的仓储存储盒6D姿态估计方法Box6d及其高效精确的姿态估计方法', 'title_zh': 'Box6D：仓库箱子的零样本类别级6D姿态估计'}
{'arxiv_id': 'arXiv:2511.15874', 'title': 'WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion', 'authors': 'Sajjad Pakdamansavoji, Yintao Ma, Amir Rasouli, Tongtong Cao', 'link': 'https://arxiv.org/abs/2511.15874', 'abstract': 'Accurate 6D object pose estimation is vital for robotics, augmented reality, and scene understanding. For seen objects, high accuracy is often attainable via per-object fine-tuning but generalizing to unseen objects remains a challenge. To address this problem, past arts assume access to CAD models at test time and typically follow a multi-stage pipeline to estimate poses: detect and segment the object, propose an initial pose, and then refine it. Under occlusion, however, the early-stage of such pipelines are prone to errors, which can propagate through the sequential processing, and consequently degrade the performance. To remedy this shortcoming, we propose four novel extensions to model-based 6D pose estimation methods: (i) a dynamic non-uniform dense sampling strategy that focuses computation on visible regions, reducing occlusion-induced errors; (ii) a multi-hypothesis inference mechanism that retains several confidence-ranked pose candidates, mitigating brittle single-path failures; (iii) iterative refinement to progressively improve pose accuracy; and (iv) series of occlusion-focused training augmentations that strengthen robustness and generalization. Furthermore, we propose a new weighted by visibility metric for evaluation under occlusion to minimize the bias in the existing protocols. Via extensive empirical evaluations, we show that our proposed approach achieves more than 5% improvement in accuracy on ICBIN and more than 2% on BOP dataset benchmarks, while achieving approximately 3 times faster inference.', 'abstract_zh': '准确的6D物体姿态估计对于机器人技术、增强现实和场景理解至关重要。对于已见物体，通过对象层面的微调可以获得较高的准确度，但将其推广到未见物体仍然是一个挑战。为了解决这一问题，过去的研究假设在测试时可以访问CAD模型，并通常遵循多阶段管道来估计姿态：先检测和分割物体，提出初始姿态，然后进行细化。然而，在遮挡情况下，此类管道的早期阶段容易出错，这些错误可能在序列处理中传播，从而降低性能。为弥补这一不足，我们提出了面向模型的6D姿态估计方法的四种新扩展：（i）动态非均匀密集采样策略，专注于可见区域的计算，减少遮挡引起的错误；（ii）多假设推理机制，保留多个置信度排名的姿态候选者，减轻单路径失效的脆弱性；（iii）迭代细化以逐步提高姿态精度；以及（iv）一系列针对遮挡的训练增强措施，增强鲁棒性和泛化能力。此外，我们提出了一种基于可见度加权的新评价指标，以在遮挡条件下最小化现有协议中的偏差。通过广泛的实证评估，我们展示了我们的方法在ICBIN和BOP数据集基准测试中分别实现了超过5%和超过2%的准确性提升，同时实现大约三倍的推理速度。', 'title_zh': 'WALDO: 未见模型导向的6D姿态估计与遮挡的交汇处'}
{'arxiv_id': 'arXiv:2511.15833', 'title': 'EfficientSAM3: Progressive Hierarchical Distillation for Video Concept Segmentation from SAM1, 2, and 3', 'authors': 'Chengxi Zeng, Yuxuan Jiang, Aaron Zhang', 'link': 'https://arxiv.org/abs/2511.15833', 'abstract': 'The Segment Anything Model 3 (SAM3) advances visual understanding with Promptable Concept Segmentation (PCS) across images and videos, but its unified architecture (shared vision backbone, DETR-style detector, dense-memory tracker) remains prohibitive for on-device use. We present EfficientSAM3, a family of efficient models built on Progressive Hierarchical Distillation (PHD) that transfers capability from SAM3 to lightweight students in three stages: (1) Encoder Distillation aligns image features via prompt-in-the-loop training on SA-1B; (2) Temporal Memory Distillation replaces dense memory with a compact Perceiver-based module trained on SA-V to compress and retrieve spatiotemporal features efficiently; and (3) End-to-End Fine-Tuning refines the full pipeline on the official SAM3 PCS data to preserve concept-level performance. PHD yields a spectrum of student variants using RepViT, TinyViT, and EfficientViT backbones, enabling on-device concept segmentation and tracking while maintaining high fidelity to teacher behavior. We benchmark on popular VOS datasets, and compare with varies of releated work, achieing strong performance-efficiency trade-offs.', 'abstract_zh': 'EfficientSAM3：基于渐进分层蒸馏的SAM3高效模型族', 'title_zh': 'EfficientSAM3：从SAM1、SAM2和SAM3进行分层 progressive 渐进式视频概念分割的知识蒸馏'}
