# Enhancing Forex Forecasting Accuracy: The Impact of Hybrid Variable Sets in Cognitive Algorithmic Trading Systems 

**Title (ZH)**: 增强外汇预测准确性：认知算法交易系统中混合变量集的影响 

**Authors**: Juan C. King, Jose M. Amigo  

**Link**: [PDF](https://arxiv.org/pdf/2511.16657)  

**Abstract**: This paper presents the implementation of an advanced artificial intelligence-based algorithmic trading system specifically designed for the EUR-USD pair within the high-frequency environment of the Forex market. The methodological approach centers on integrating a holistic set of input features: key fundamental macroeconomic variables (for example, Gross Domestic Product and Unemployment Rate) collected from both the Euro Zone and the United States, alongside a comprehensive suite of technical variables (including indicators, oscillators, Fibonacci levels, and price divergences). The performance of the resulting algorithm is evaluated using standard machine learning metrics to quantify predictive accuracy and backtesting simulations across historical data to assess trading profitability and risk. The study concludes with a comparative analysis to determine which class of input features, fundamental or technical, provides greater and more reliable predictive capacity for generating profitable trading signals. 

**Abstract (ZH)**: 这篇论文介绍了针对外汇市场欧元/美元pair的一种高级基于人工智能的算法交易系统在高频交易环境中的实现。方法论集中在整合一套全面的输入特征：来自欧元区和美国的关键宏观经济变量（例如，国内生产总值和失业率），以及一系列全面的技术变量（包括指标、振荡器、斐波那契水平和价格偏离）。通过使用标准机器学习度量来评估算法的性能，衡量预测准确性和通过历史数据回测模拟来评估交易盈利能力和风险。研究结论部分进行了一种比较分析，以确定哪一类输入特征，基本面或技术面，能提供更强和更可靠的预测能力以生成盈利交易信号。 

---
# MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support 

**Title (ZH)**: MedBayes-Lite: Bayesian不确定性量化以实现安全的临床决策支持 

**Authors**: Elias Hossain, Md Mehedi Hasan Nipu, Maleeha Sheikh, Rajib Rana, Subash Neupane, Niloofar Yousefi  

**Link**: [PDF](https://arxiv.org/pdf/2511.16625)  

**Abstract**: We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems. 

**Abstract (ZH)**: MedBayes-Lite：一种轻量级的贝叶斯增强方法，用于生成临床语言模型的可靠、含不确定性意识的预测 

---
# D-GARA: A Dynamic Benchmarking Framework for GUI Agent Robustness in Real-World Anomalies 

**Title (ZH)**: D-GARA: 一种针对实际异常中 GUI 代理稳健性的动态基准框架 

**Authors**: Sen Chen, Tong Zhao, Yi Bin, Fei Ma, Wenqi Shao, Zheng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2511.16590)  

**Abstract**: Developing intelligent agents capable of operating a wide range of Graphical User Interfaces (GUIs) with human-level proficiency is a key milestone on the path toward Artificial General Intelligence. While most existing datasets and benchmarks for training and evaluating GUI agents are static and idealized, failing to reflect the complexity and unpredictability of real-world environments, particularly the presence of anomalies. To bridge this research gap, we propose D-GARA, a dynamic benchmarking framework, to evaluate Android GUI agent robustness in real-world anomalies. D-GARA introduces a diverse set of real-world anomalies that GUI agents commonly face in practice, including interruptions such as permission dialogs, battery warnings, and update prompts. Based on D-GARA framework, we construct and annotate a benchmark featuring commonly used Android applications with embedded anomalies to support broader community research. Comprehensive experiments and results demonstrate substantial performance degradation in state-of-the-art GUI agents when exposed to anomaly-rich environments, highlighting the need for robustness-aware learning. D-GARA is modular and extensible, supporting the seamless integration of new tasks, anomaly types, and interaction scenarios to meet specific evaluation goals. 

**Abstract (ZH)**: 开发能够以人类水平的熟练程度操作多种图形用户界面（GUI）的智能代理是通向人工通用智能的关键里程碑。为弥补现有训练和评估GUI代理的数据集和基准大多是静态和理想的，未能反映现实世界环境的复杂性和不可预测性，特别是异常情况的空白，我们提出了D-GARA动态基准框架，用于评估Android GUI代理在真实世界异常环境下的鲁棒性。D-GARA引入了一组GUI代理在实践中常遇到的多样的真实世界异常，包括权限对话框、电池警告和更新提示等中断。基于D-GARA框架，我们构建并标注了一个基准，其中包括嵌入异常的常用Android应用程序，以支持更广泛的研究社区的研究。全面的实验和结果表明，在异常丰富的环境中，最先进的GUI代理会遭受显著性能下降，突显了鲁棒性意识学习的需求。D-GARA模块化且可扩展，支持新任务、异常类型和交互场景的无缝集成，以满足特定的评估目标。 

---
# Formal Abductive Latent Explanations for Prototype-Based Networks 

**Title (ZH)**: 基于 prototypes 的网络的形式 abduction 潜在解释 

**Authors**: Jules Soria, Zakaria Chihani, Julien Girard-Satabin, Alban Grastien, Romain Xu-Darme, Daniela Cancila  

**Link**: [PDF](https://arxiv.org/pdf/2511.16588)  

**Abstract**: Case-based reasoning networks are machine-learning models that make predictions based on similarity between the input and prototypical parts of training samples, called prototypes. Such models are able to explain each decision by pointing to the prototypes that contributed the most to the final outcome. As the explanation is a core part of the prediction, they are often qualified as ``interpretable by design". While promising, we show that such explanations are sometimes misleading, which hampers their usefulness in safety-critical contexts. In particular, several instances may lead to different predictions and yet have the same explanation. Drawing inspiration from the field of formal eXplainable AI (FXAI), we propose Abductive Latent Explanations (ALEs), a formalism to express sufficient conditions on the intermediate (latent) representation of the instance that imply the prediction. Our approach combines the inherent interpretability of case-based reasoning models and the guarantees provided by formal XAI. We propose a solver-free and scalable algorithm for generating ALEs based on three distinct paradigms, compare them, and present the feasibility of our approach on diverse datasets for both standard and fine-grained image classification. The associated code can be found at this https URL 

**Abstract (ZH)**: 基于案例的推理网络是基于输入与训练样本中原型部分相似性进行预测的机器学习模型，这样的模型能够通过指向贡献最大的原型来解释每个决策。由于解释是预测的核心部分，它们常被视为“设计即解释”。虽然前景广阔，但我们展示了这样的解释有时是误导性的，这在安全关键应用中限制了它们的有用性。特别是，多个实例可能会得到不同的预测结果，但具有相同解释。受到形式可解释AI（FXAI）领域的启发，我们提出了归纳潜在解释（ALEs），这是一种表达实例中间（潜在）表示的充分条件的形式化方法，这些条件能蕴含预测结果。我们的方法结合了基于案例的推理模型的固有解释性和形式XAI提供的保证。我们提出了一种无需求解器且可扩展的算法来生成ALEs，并基于三种不同的范式进行比较，在多种数据集上展示了我们的方法在标准和细粒度图像分类中的可行性。相关代码可在以下链接找到：this https URL 

---
# Consciousness in Artificial Intelligence? A Framework for Classifying Objections and Constraints 

**Title (ZH)**: 人工意识？一种分类反对观点和约束条件的框架 

**Authors**: Andres Campero, Derek Shiller, Jaan Aru, Jonathan Simon  

**Link**: [PDF](https://arxiv.org/pdf/2511.16582)  

**Abstract**: We develop a taxonomical framework for classifying challenges to the possibility of consciousness in digital artificial intelligence systems. This framework allows us to identify the level of granularity at which a given challenge is intended (the levels we propose correspond to Marr's levels) and to disambiguate its degree of force: is it a challenge to computational functionalism that leaves the possibility of digital consciousness open (degree 1), a practical challenge to digital consciousness that suggests improbability without claiming impossibility (degree 2), or an argument claiming that digital consciousness is strictly impossible (degree 3)? We apply this framework to 14 prominent examples from the scientific and philosophical literature. Our aim is not to take a side in the debate, but to provide structure and a tool for disambiguating between challenges to computational functionalism and challenges to digital consciousness, as well as between different ways of parsing such challenges. 

**Abstract (ZH)**: 我们开发了一种分类框架，用于分类数字人工智能系统中对意识可能性的挑战。该框架使我们能够确定给定挑战所针对的具体层次（我们提出的层次对应于Marr的层次），并区分其强度程度：它是对计算功能主义的挑战但保留了数字意识的可能性（第1级）、对数字意识的实际挑战，暗示不可能性但未声明不可能（第2级），还是对数字意识严格不可能性的论据（第3级）？我们将这一框架应用于科学和哲学文献中的14个典型案例。我们的目标不是介入辩论，而是提供一种结构和工具，用于区分对计算功能主义的挑战与对数字意识的挑战，以及不同解析这些挑战的方式。 

---
# PersonaDrift: A Benchmark for Temporal Anomaly Detection in Language-Based Dementia Monitoring 

**Title (ZH)**: 基于语言的痴呆监测中时间异常检测的基准：PersonaDrift 

**Authors**: Joy Lai, Alex Mihailidis  

**Link**: [PDF](https://arxiv.org/pdf/2511.16445)  

**Abstract**: People living with dementia (PLwD) often show gradual shifts in how they communicate, becoming less expressive, more repetitive, or drifting off-topic in subtle ways. While caregivers may notice these changes informally, most computational tools are not designed to track such behavioral drift over time. This paper introduces PersonaDrift, a synthetic benchmark designed to evaluate machine learning and statistical methods for detecting progressive changes in daily communication, focusing on user responses to a digital reminder system. PersonaDrift simulates 60-day interaction logs for synthetic users modeled after real PLwD, based on interviews with caregivers. These caregiver-informed personas vary in tone, modality, and communication habits, enabling realistic diversity in behavior. The benchmark focuses on two forms of longitudinal change that caregivers highlighted as particularly salient: flattened sentiment (reduced emotional tone and verbosity) and off-topic replies (semantic drift). These changes are injected progressively at different rates to emulate naturalistic cognitive trajectories, and the framework is designed to be extensible to additional behaviors in future use cases. To explore this novel application space, we evaluate several anomaly detection approaches, unsupervised statistical methods (CUSUM, EWMA, One-Class SVM), sequence models using contextual embeddings (GRU + BERT), and supervised classifiers in both generalized and personalized settings. Preliminary results show that flattened sentiment can often be detected with simple statistical models in users with low baseline variability, while detecting semantic drift requires temporal modeling and personalized baselines. Across both tasks, personalized classifiers consistently outperform generalized ones, highlighting the importance of individual behavioral context. 

**Abstract (ZH)**: PLwD在日常交流中的渐进式变化及其检测：PersonaDrift合成基准探讨 

---
# From generative AI to the brain: five takeaways 

**Title (ZH)**: 从生成式AI到大脑：五点启示 

**Authors**: Claudius Gros  

**Link**: [PDF](https://arxiv.org/pdf/2511.16432)  

**Abstract**: The big strides seen in generative AI are not based on somewhat obscure algorithms, but due to clearly defined generative principles. The resulting concrete implementations have proven themselves in large numbers of applications. We suggest that it is imperative to thoroughly investigate which of these generative principles may be operative also in the brain, and hence relevant for cognitive neuroscience. In addition, ML research led to a range of interesting characterizations of neural information processing systems. We discuss five examples, the shortcomings of world modelling, the generation of thought processes, attention, neural scaling laws, and quantization, that illustrate how much neuroscience could potentially learn from ML research. 

**Abstract (ZH)**: 生成AI取得的显著进展并非基于某些晦涩的算法，而是由于明确的生成原则。具体实现已经在大量应用程序中得到了验证。我们建议必须深入研究这些生成原则是否也适用于大脑，并因此对认知神经科学具有重要意义。此外，机器学习研究还导致了对神经信息处理系统的多种有趣的表征。我们讨论了五种例子，包括世界建模的局限性、思维过程的生成、注意力机制、神经可标性定律和量化，展示了神经科学从机器学习研究中可能学到的内容。 

---
# Trustworthy AI in the Agentic Lakehouse: from Concurrency to Governance 

**Title (ZH)**: 代理湖仓中的可信AI：从并发到治理 

**Authors**: Jacopo Tagliabue, Federico Bianchi, Ciro Greco  

**Link**: [PDF](https://arxiv.org/pdf/2511.16402)  

**Abstract**: Even as AI capabilities improve, most enterprises do not consider agents trustworthy enough to work on production data. In this paper, we argue that the path to trustworthy agentic workflows begins with solving the infrastructure problem first: traditional lakehouses are not suited for agent access patterns, but if we design one around transactions, governance follows. In particular, we draw an operational analogy to MVCC in databases and show why a direct transplant fails in a decoupled, multi-language setting. We then propose an agent-first design, Bauplan, that reimplements data and compute isolation in the lakehouse. We conclude by sharing a reference implementation of a self-healing pipeline in Bauplan, which seamlessly couples agent reasoning with all the desired guarantees for correctness and trust. 

**Abstract (ZH)**: 即使AI能力不断提高，大多数企业也不认为代理足够可信以处理生产数据。在本文中，我们argue认为，走向可信赖的代理工作流的路径是从解决基础设施问题开始：传统的湖库房不适用于代理的访问模式，但如果我们将它们设计为围绕交易进行，治理便会随之而来。特别是，我们将操作类比于数据库中的MVCC，并说明为什么直接移植会在解耦且多语言设置中失败。然后，我们提出了一种以代理为中心的设计Bauplan，该设计在湖库房中重新实现了数据和计算的隔离。最后，我们分享了Bauplan中一个自我修复管道的参考实现，该实现无缝地将代理推理与所有所需的正确性和可信度保证相结合。 

---
# Reducing Instability in Synthetic Data Evaluation with a Super-Metric in MalDataGen 

**Title (ZH)**: 使用Super-Metric在MalDataGen中减少合成数据评估的不稳定性 

**Authors**: Anna Luiza Gomes da Silva, Diego Kreutz, Angelo Diniz, Rodrigo Mansilha, Celso Nobre da Fonseca  

**Link**: [PDF](https://arxiv.org/pdf/2511.16373)  

**Abstract**: Evaluating the quality of synthetic data remains a persistent challenge in the Android malware domain due to instability and the lack of standardization among existing metrics. This work integrates into MalDataGen a Super-Metric that aggregates eight metrics across four fidelity dimensions, producing a single weighted score. Experiments involving ten generative models and five balanced datasets demonstrate that the Super-Metric is more stable and consistent than traditional metrics, exhibiting stronger correlations with the actual performance of classifiers. 

**Abstract (ZH)**: 评估合成数据的质量仍是在Android恶意软件领域的一项持久挑战，由于现有指标的不稳定性和缺乏标准化。本研究在MalDataGen中整合了一个超级指标，该指标在四个保真度维度上聚合了八种指标，生成了一个加权分数。涉及十种生成模型和五个平衡数据集的实验表明，超级指标比传统指标更为稳定和一致，与分类器的实际性能展现出更强的相关性。 

---
# Distributed Agent Reasoning Across Independent Systems With Strict Data Locality 

**Title (ZH)**: 跨独立系统中的严格数据局部性分布式代理推理 

**Authors**: Daniel Vaughan, Kateřina Vaughan  

**Link**: [PDF](https://arxiv.org/pdf/2511.16292)  

**Abstract**: This paper presents a proof-of-concept demonstration of agent-to-agent communication across distributed systems, using only natural-language messages and without shared identifiers, structured schemas, or centralised data exchange. The prototype explores how multiple organisations (represented here as a Clinic, Insurer, and Specialist Network) can cooperate securely via pseudonymised case tokens, local data lookups, and controlled operational boundaries.
The system uses Orpius as the underlying platform for multi-agent orchestration, tool execution, and privacy-preserving communication. All agents communicate through OperationRelay calls, exchanging concise natural-language summaries. Each agent operates on its own data (such as synthetic clinic records, insurance enrolment tables, and clinical guidance extracts), and none receives or reconstructs patient identity. The Clinic computes an HMAC-based pseudonymous token, the Insurer evaluates coverage rules and consults the Specialist agent, and the Specialist returns an appropriateness recommendation.
The goal of this prototype is intentionally limited: to demonstrate feasibility, not to provide a clinically validated, production-ready system. No clinician review was conducted, and no evaluation beyond basic functional runs was performed. The work highlights architectural patterns, privacy considerations, and communication flows that enable distributed reasoning among specialised agents while keeping data local to each organisation. We conclude by outlining opportunities for more rigorous evaluation and future research in decentralised multi-agent systems. 

**Abstract (ZH)**: 基于自然语言的分布式系统中代理间通信的概念验证：通过伪匿名病例标识、本地数据查找和可控操作边界实现多方安全合作 

---
# MuISQA: Multi-Intent Retrieval-Augmented Generation for Scientific Question Answering 

**Title (ZH)**: MuISQA: 基于多意图检索增强的科学问答生成 

**Authors**: Zhiyuan Li, Haisheng Yu, Guangchuan Guo, Nan Zhou, Jiajun Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2511.16283)  

**Abstract**: Complex scientific questions often entail multiple intents, such as identifying gene mutations and linking them to related diseases. These tasks require evidence from diverse sources and multi-hop reasoning, while conventional retrieval-augmented generation (RAG) systems are usually single-intent oriented, leading to incomplete evidence coverage. To assess this limitation, we introduce the Multi-Intent Scientific Question Answering (MuISQA) benchmark, which is designed to evaluate RAG systems on heterogeneous evidence coverage across sub-questions. In addition, we propose an intent-aware retrieval framework that leverages large language models (LLMs) to hypothesize potential answers, decompose them into intent-specific queries, and retrieve supporting passages for each underlying intent. The retrieved fragments are then aggregated and re-ranked via Reciprocal Rank Fusion (RRF) to balance coverage across diverse intents while reducing redundancy. Experiments on both MuISQA benchmark and other general RAG datasets demonstrate that our method consistently outperforms conventional approaches, particularly in retrieval accuracy and evidence coverage. 

**Abstract (ZH)**: 复杂科学问题往往包含多重意图，如识别基因突变并将其与相关疾病联系起来。这些任务需要来自多种来源和多跳推理的证据，而传统的检索增强生成（RAG）系统通常是单意图导向的，导致证据覆盖不完整。为了评估这一局限性，我们提出了多元意图科学问答（MuISQA）基准，旨在评估RAG系统在子问题跨异质证据覆盖方面的表现。此外，我们提出了一种意图感知的检索框架，利用大规模语言模型（LLMs）进行潜在答案的假设，将其分解为特定意图的查询，并检索每个底层意图的支持段落。检索到的片段然后通过互惠排名融合（RRF）进行聚合和重新排名，以平衡多元意图之间的覆盖范围同时减少冗余。在MuISQA基准和其他一般RAG数据集上的实验表明，我们的方法在检索准确性和证据覆盖方面一贯优于传统方法。 

---
# Revisiting Fairness-aware Interactive Recommendation: Item Lifecycle as a Control Knob 

**Title (ZH)**: 重新评估基于公平性的交互式推荐：物品生命周期作为控制旋钮 

**Authors**: Yun Lu, Xiaoyu Shi, Hong Xie, Chongjun Xia, Zhenhui Gong, Mingsheng Shang  

**Link**: [PDF](https://arxiv.org/pdf/2511.16248)  

**Abstract**: This paper revisits fairness-aware interactive recommendation (e.g., TikTok, KuaiShou) by introducing a novel control knob, i.e., the lifecycle of items. We make threefold contributions. First, we conduct a comprehensive empirical analysis and uncover that item lifecycles in short-video platforms follow a compressed three-phase pattern, i.e., rapid growth, transient stability, and sharp decay, which significantly deviates from the classical four-stage model (introduction, growth, maturity, decline). Second, we introduce LHRL, a lifecycle-aware hierarchical reinforcement learning framework that dynamically harmonizes fairness and accuracy by leveraging phase-specific exposure dynamics. LHRL consists of two key components: (1) PhaseFormer, a lightweight encoder combining STL decomposition and attention mechanisms for robust phase detection; (2) a two-level HRL agent, where the high-level policy imposes phase-aware fairness constraints, and the low-level policy optimizes immediate user engagement. This decoupled optimization allows for effective reconciliation between long-term equity and short-term utility. Third, experiments on multiple real-world interactive recommendation datasets demonstrate that LHRL significantly improves both fairness and user engagement. Furthermore, the integration of lifecycle-aware rewards into existing RL-based models consistently yields performance gains, highlighting the generalizability and practical value of our approach. 

**Abstract (ZH)**: 本文通过引入新的控制旋钮即项目的生命周期，重新审视公平感知的互动推荐（如抖音、快手等）。我们做出了三方面的贡献。首先，我们进行了全面的经验分析，发现短视频平台上的项目生命周期遵循压缩的三阶段模式，即快速增长、瞬时稳定和急剧衰减，这与经典的四阶段模型（引入期、增长期、成熟期、衰退期）有显著差异。其次，我们引入了LHRL，这是一种生命周期感知的层次强化学习框架，通过利用阶段特异性的曝光动态来动态平衡公平和准确性。LHRL 包含两个关键组件：(1) PhaseFormer，一种结合STL分解和注意力机制的轻量级编码器，用于稳健的阶段检测；(2) 两级层次强化学习代理，其中高级策略施加阶段感知的公平约束，而低级策略优化即时用户参与度。这种分离的优化允许有效地平衡长期公平性和短期实用性。第三，对多个现实世界的互动推荐数据集的实验表明，LHRL 显著提高了公平性和用户参与度。此外，生命周期感知奖励的集成到现有的基于RL的模型中始终带来了性能提升，突显了我们方法的通用性和实际价值。 

---
# Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning 

**Title (ZH)**: 多代理协作奖励设计以增强强化学习中的推理能力 

**Authors**: Pei Yang, Ke Zhang, Ji Wang, Xiao Chen, Yuxin Tang, Eric Yang, Lynn Ai, Bill Shi  

**Link**: [PDF](https://arxiv.org/pdf/2511.16202)  

**Abstract**: We present CRM (Multi-Agent Collaborative Reward Model), a framework that replaces a single black-box reward model with a coordinated team of specialist evaluators to improve robustness and interpretability in RLHF. Conventional reward models struggle to jointly optimize multiple, sometimes conflicting, preference dimensions (e.g., factuality, helpfulness, safety) and offer limited transparency into why a score is assigned. CRM addresses these issues by decomposing preference evaluation into domain-specific agents that each produce partial signals, alongside global evaluators such as ranker-based and embedding-similarity rewards. A centralized aggregator fuses these signals at each timestep, balancing factors like step-wise correctness, multi-agent agreement, and repetition penalties, yielding a single training reward compatible with standard RL pipelines. The policy is optimized with advantage-based updates (e.g., GAE), while a value model regresses to the aggregated reward, enabling multi-perspective reward shaping without requiring additional human annotations beyond those used to train the evaluators. To support training and assessment, we introduce rewardBench, a benchmark and training suite aligned with the collaborative structure of CRM. Together, CRM and rewardBench provide a practical, modular path to more transparent reward modeling and more stable optimization. 

**Abstract (ZH)**: CRM（多Agent协作奖励模型）：一种框架，用协调的专家评估团队取代单一的黑盒奖励模型，以提高RLHF的稳健性和可解释性 

---
# A Hybrid Proactive And Predictive Framework For Edge Cloud Resource Management 

**Title (ZH)**: 边缘云资源管理的混合主动预测框架 

**Authors**: Hrikshesh Kumar, Anika Garg, Anshul Gupta, Yashika Agarwal  

**Link**: [PDF](https://arxiv.org/pdf/2511.16075)  

**Abstract**: Old cloud edge workload resource management is too reactive. The problem with relying on static thresholds is that we are either overspending for more resources than needed or have reduced performance because of their lack. This is why we work on proactive solutions. A framework developed for it stops reacting to the problems but starts expecting them. We design a hybrid architecture, combining two powerful tools: the CNN LSTM model for time series forecasting and an orchestrator based on multi agent Deep Reinforcement Learning In fact the novelty is in how we combine them as we embed the predictive forecast from the CNN LSTM directly into the DRL agent state space. That is what makes the AI manager smarter it sees the future, which allows it to make better decisions about a long term plan for where to run tasks That means finding that sweet spot between how much money is saved while keeping the system healthy and apps fast for users That is we have given it eyes in order to see down the road so that it does not have to lurch from one problem to another it finds a smooth path forward Our tests show our system easily beats the old methods It is great at solving tough problems like making complex decisions and juggling multiple goals at once like being cheap fast and reliable 

**Abstract (ZH)**: 基于CNN LSTM和多代理深度强化学习的主动边缘工作负载资源管理框架 

---
# Artificial Intelligence and Accounting Research: A Framework and Agenda 

**Title (ZH)**: 人工智能与会计研究：框架与议程 

**Authors**: Theophanis C. Stratopoulos, Victor Xiaoqi Wang  

**Link**: [PDF](https://arxiv.org/pdf/2511.16055)  

**Abstract**: Recent advances in artificial intelligence, particularly generative AI (GenAI) and large language models (LLMs), are fundamentally transforming accounting research, creating both opportunities and competitive threats for scholars. This paper proposes a framework that classifies AI-accounting research along two dimensions: research focus (accounting-centric versus AI-centric) and methodological approach (AI-based versus traditional methods). We apply this framework to papers from the IJAIS special issue and recent AI-accounting research published in leading accounting journals to map existing studies and identify research opportunities. Using this same framework, we analyze how accounting researchers can leverage their expertise through strategic positioning and collaboration, revealing where accounting scholars' strengths create the most value. We further examine how GenAI and LLMs transform the research process itself, comparing the capabilities of human researchers and AI agents across the entire research workflow. This analysis reveals that while GenAI democratizes certain research capabilities, it simultaneously intensifies competition by raising expectations for higher-order contributions where human judgment, creativity, and theoretical depth remain valuable. These shifts call for reforming doctoral education to cultivate comparative advantages while building AI fluency. 

**Abstract (ZH)**: 近年来，人工智能的最新进展，特别是生成人工智能（GenAI）和大型语言模型（LLMs），从根本上变革了会计研究，为学者们创造了机遇与竞争威胁。本文提出了一种框架，从两个维度对AI-会计研究进行分类：研究焦点（会计中心化与AI中心化）和方法论路径（基于AI的方法与传统方法）。我们通过将此框架应用于IJAIS的特刊论文和顶尖会计期刊上最近的AI-会计研究，绘制现有研究图谱并识别研究机遇。利用同一框架，我们分析了会计研究者如何通过战略定位和合作发挥其专长，揭示了会计学者的优势领域创造的最大价值。进一步探讨了GenAI和LLMs如何本身变革研究过程，比较了人类研究人员与AI代理在整个研究工作流程中的能力差异。这一分析表明，虽然GenAI普及了某些研究能力，但它同时加剧了竞争，因为人类判断、创造力和理论深度仍具有重要价值。这些变化要求改革博士教育，培养比较优势并增强AI技能。 

---
# An Aligned Constraint Programming Model For Serial Batch Scheduling With Minimum Batch Size 

**Title (ZH)**: 面向最小批量尺寸的对齐约束编程模型序列批调度 

**Authors**: Jorge A. Huertas, Pascal Van Hentenryck  

**Link**: [PDF](https://arxiv.org/pdf/2511.16045)  

**Abstract**: In serial batch (s-batch) scheduling, jobs from similar families are grouped into batches and processed sequentially to avoid repetitive setups that are required when processing consecutive jobs of different families. Despite its large success in scheduling, only three Constraint Programming (CP) models have been proposed for this problem considering minimum batch sizes, which is a common requirement in many practical settings, including the ion implantation area in semiconductor manufacturing. These existing CP models rely on a predefined virtual set of possible batches that suffers from the curse of dimensionality and adds complexity to the problem. This paper proposes a novel CP model that does not rely on this virtual set. Instead, it uses key alignment parameters that allow it to reason directly on the sequences of same-family jobs scheduled on the machines, resulting in a more compact formulation. This new model is further improved by exploiting the problem's structure with tailored search phases and strengthened inference levels of the constraint propagators. The extensive computational experiments on nearly five thousand instances compare the proposed models against existing methods in the literature, including mixed-integer programming formulations, tabu search meta-heuristics, and CP approaches. The results demonstrate the superiority of the proposed models on small-to-medium instances with up to 100 jobs, and their ability to find solutions up to 25\% better than the ones produces by existing methods on large-scale instances with up to 500 jobs, 10 families, and 10 machines. 

**Abstract (ZH)**: 基于序列批次（s-批）调度的新型约束编程模型 

---
# SpellForger: Prompting Custom Spell Properties In-Game using BERT supervised-trained model 

**Title (ZH)**: SpellForger: 使用BERT监督训练模型在游戏中自定义施法属性 

**Authors**: Emanuel C. Silva, Emily S. M. Salum, Gabriel M. Arantes, Matheus P. Pereira, Vinicius F. Oliveira, Alessandro L. Bicho  

**Link**: [PDF](https://arxiv.org/pdf/2511.16018)  

**Abstract**: Introduction: The application of Artificial Intelligence in games has evolved significantly, allowing for dynamic content generation. However, its use as a core gameplay co-creation tool remains underexplored. Objective: This paper proposes SpellForger, a game where players create custom spells by writing natural language prompts, aiming to provide a unique experience of personalization and creativity. Methodology: The system uses a supervisedtrained BERT model to interpret player prompts. This model maps textual descriptions to one of many spell prefabs and balances their parameters (damage, cost, effects) to ensure competitive integrity. The game is developed in the Unity Game Engine, and the AI backend is in Python. Expected Results: We expect to deliver a functional prototype that demonstrates the generation of spells in real time, applied to an engaging gameplay loop, where player creativity is central to the experience, validating the use of AI as a direct gameplay mechanic. 

**Abstract (ZH)**: 介绍：人工智能在游戏中的应用已经显著发展，使得动态内容生成成为可能。然而，将其作为核心游戏共创工具的应用仍然未被充分探索。目标：本文提出了一款名为SpellForger的游戏，玩家通过编写自然语言提示来创建自定义法术，旨在提供一种独特的个性化和创造力体验。方法：系统使用监督训练的BERT模型来解释玩家提示，将文本描述映射到多个法术预制件之一，并平衡其参数（伤害、成本、效果）以确保竞争公平性。游戏使用Unity游戏引擎开发，AI后台使用Python编写。预期结果：我们期望交付一个功能原型，该原型能够实时生成法术，并应用于一个以玩家创造力为中心的吸引人的游戏循环中，从而验证AI作为直接游戏机制的使用价值。 

---
# MUSEKG: A Knowledge Graph Over Museum Collections 

**Title (ZH)**: MUSEKG：博物馆藏品的知识图谱 

**Authors**: Jinhao Li, Jianzhong Qi, Soyeon Caren Han, Eun-Jung Holden  

**Link**: [PDF](https://arxiv.org/pdf/2511.16014)  

**Abstract**: Digital transformation in the cultural heritage sector has produced vast yet fragmented collections of artefact data. Existing frameworks for museum information systems struggle to integrate heterogeneous metadata, unstructured documents, and multimodal artefacts into a coherent and queryable form. We present MuseKG, an end-to-end knowledge-graph framework that unifies structured and unstructured museum data through symbolic-neural integration. MuseKG constructs a typed property graph linking objects, people, organisations, and visual or textual labels, and supports natural language queries. Evaluations on real museum collections demonstrate robust performance across queries over attributes, relations, and related entities, surpassing large-language-model zero-shot, few-shot and SPARQL prompt baselines. The results highlight the importance of symbolic grounding for interpretable and scalable cultural heritage reasoning, and pave the way for web-scale integration of digital heritage knowledge. 

**Abstract (ZH)**: 文化 heritage 领域的数字转型产生了大量yet 分散的 artefact 数据。现有的博物馆信息系统框架难以将异构元数据、非结构化文档和多模态 artefact 整合成一致且可查询的形式。我们提出 MuseKG，这是一种端到端的知识图谱框架，通过符号-神经集成统一结构化和非结构化博物馆数据。MuseKG 构建了一种带有类型属性的图形，将对象、人员、组织及相关视觉或文本标签连接起来，并支持自然语言查询。实证研究表明，MuseKG 在属性、关系及相关实体的查询中表现出稳健的性能，超越了大型语言模型的零样本、少样本和 SPARQL 提示基准。结果突显了符号定位对于可解释和可扩展的文化遗产推理的重要性，并为实现数字遗产知识的大规模集成铺平了道路。 

---
# IMACT-CXR - An Interactive Multi-Agent Conversational Tutoring System for Chest X-Ray Interpretation 

**Title (ZH)**: IMACT-CXR - 一种交互式多agent对话式教学系统用于胸部X光解析 

**Authors**: Tuan-Anh Le, Anh Mai Vu, David Yang, Akash Awasthi, Hien Van Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2511.15825)  

**Abstract**: IMACT-CXR is an interactive multi-agent conversational tutor that helps trainees interpret chest X-rays by unifying spatial annotation, gaze analysis, knowledge retrieval, and image-grounded reasoning in a single AutoGen-based workflow. The tutor simultaneously ingests learner bounding boxes, gaze samples, and free-text observations. Specialized agents evaluate localization quality, generate Socratic coaching, retrieve PubMed evidence, suggest similar cases from REFLACX, and trigger NV-Reason-CXR-3B for vision-language reasoning when mastery remains low or the learner explicitly asks. Bayesian Knowledge Tracing (BKT) maintains skill-specific mastery estimates that drive both knowledge reinforcement and case similarity retrieval. A lung-lobe segmentation module derived from a TensorFlow U-Net enables anatomically aware gaze feedback, and safety prompts prevent premature disclosure of ground-truth labels. We describe the system architecture, implementation highlights, and integration with the REFLACX dataset for real DICOM cases. IMACT-CXR demonstrates responsive tutoring flows with bounded latency, precise control over answer leakage, and extensibility toward live residency deployment. Preliminary evaluation shows improved localization and diagnostic reasoning compared to baselines. 

**Abstract (ZH)**: IMACT-CXR是一种交互式多智能体会话导师，通过在单个AutoGen基础上统一空间标注、注视分析、知识检索和基于图像的推理，帮助学员解读胸部X光片。该导师同时摄入学员的边界框、注视样本和自由文本观察。专业化的智能体评估定位质量、生成苏 krb 哲式辅导、检索PubMed证据、从 REFLACX 中推荐相似病例，并在掌握程度低或学员明确要求时触发 NV-Reason-CXR-3B 进行视觉-语言推理。贝叶斯知识追踪（BKT）维护了特定技能的掌握估算值，驱动知识强化和病例相似性检索。一个源于 TensorFlow U-Net 的肺叶分割模块提供解剖学意识的注视反馈，并通过安全提示防止提前披露真实标签。我们描述了系统架构、实现亮点以及与 REFLACX 数据集的集成，用于真实 DICOM 案例。IMACT-CXR 展示了响应式的辅导流程，具有有界延迟、精确的答案泄露控制，并且可以扩展到实时住院医师部署。初步评估表明，IMACT-CXR 在定位和诊断推理方面优于基线方法。 

---
# Identifying the Supply Chain of AI for Trustworthiness and Risk Management in Critical Applications 

**Title (ZH)**: 识别AI在关键应用中的供应链，以确保可信性和风险管理 

**Authors**: Raymond K. Sheh, Karen Geappen  

**Link**: [PDF](https://arxiv.org/pdf/2511.15763)  

**Abstract**: Risks associated with the use of AI, ranging from algorithmic bias to model hallucinations, have received much attention and extensive research across the AI community, from researchers to end-users. However, a gap exists in the systematic assessment of supply chain risks associated with the complex web of data sources, pre-trained models, agents, services, and other systems that contribute to the output of modern AI systems. This gap is particularly problematic when AI systems are used in critical applications, such as the food supply, healthcare, utilities, law, insurance, and transport.
We survey the current state of AI risk assessment and management, with a focus on the supply chain of AI and risks relating to the behavior and outputs of the AI system. We then present a proposed taxonomy specifically for categorizing AI supply chain entities. This taxonomy helps stakeholders, especially those without extensive AI expertise, to "consider the right questions" and systematically inventory dependencies across their organization's AI systems. Our contribution bridges a gap between the current state of AI governance and the urgent need for actionable risk assessment and management of AI use in critical applications. 

**Abstract (ZH)**: 与AI使用相关的风险，包括算法偏差和模型幻觉等，已在AI社区引起了广泛关注和深入研究，从研究人员到最终用户。然而，存在一个系统评估AI供应链风险的缺口，这些风险与构成现代AI系统输出的复杂数据源、预训练模型、代理、服务和其他系统的网络有关。当AI系统在食品供应链、医疗、公用事业、法律、保险和运输等关键应用中使用时，这一缺口尤为突出。

我们概述了当前AI风险评估和管理的状态，重点关注AI供应链及其相关行为和输出的风险。随后，我们提出了一种特定的分类法，用于分类AI供应链实体。该分类法有助于相关方，尤其是缺乏大量AI专业知识的人士，能够“提出正确的疑问”并系统地盘点其组织内AI系统的依赖关系。我们的贡献填补了当前AI治理状态与迫切需要在关键应用中进行可操作的风险评估和管理之间的缺口。 

---
# How Modality Shapes Perception and Reasoning: A Study of Error Propagation in ARC-AGI 

**Title (ZH)**: 模态如何塑造感知与推理：ARC-AGI 中错误传播的研究 

**Authors**: Bo Wen, Chen Wang, Erhan Bilal  

**Link**: [PDF](https://arxiv.org/pdf/2511.15717)  

**Abstract**: ARC-AGI and ARC-AGI-2 measure generalization-through-composition on small color-quantized grids, and their prize competitions make progress on these harder held-out tasks a meaningful proxy for systematic generalization. Recent instruction-first systems translate grids into concise natural-language or DSL rules executed in generate-execute-select loops, yet we lack a principled account of how encodings shape model perception and how to separate instruction errors from execution errors. We hypothesize that modality imposes perceptual bottlenecks -- text flattens 2D structure into 1D tokens while images preserve layout but can introduce patch-size aliasing -- thereby shaping which grid features are reliably perceived. To test this, we isolate perception from reasoning across nine text and image modalities using a weighted set-disagreement metric and a two-stage reasoning pipeline, finding that structured text yields precise coordinates on sparse features, images capture 2D shapes yet are resolution-sensitive, and combining them improves execution (about 8 perception points; about 0.20 median similarity). Overall, aligning representations with transformer inductive biases and enabling cross-validation between text and image yields more accurate instructions and more reliable execution without changing the underlying model. 

**Abstract (ZH)**: ARC-AGI和ARC-AGI-2通过小色彩量化网格测量组合泛化能力，并且他们的奖励竞赛将这些更困难的保留任务的进步作为系统泛化的有意义代理。最近的指令优先系统将网格转换为简洁的自然语言或DSL规则并在生成-执行-选择循环中执行，但缺乏关于编码如何塑造模型感知的原理性解释以及如何将指令错误与执行错误区分开来的论述。我们假设模态引入感知瓶颈——文本将二维结构压平为一维标记，而图像保留布局但可能引入块大小混叠——从而塑造哪些网格特征能够可靠地被感知。为了测试这一点，我们使用加权集不一致性度量和两阶段推理管道，隔离感知和推理，发现结构化文本在稀疏特征上提供精确的坐标，图像捕获二维形状但对分辨率敏感，结合它们可以改善执行（约8个感知点；中位数相似度约0.20）。总体而言，使表示与变压器归纳偏置对齐并在文本和图像之间实现交叉验证可以提高指令的准确性并增强执行的可靠性，而无需改变基础模型。 

---
# MACIE: Multi-Agent Causal Intelligence Explainer for Collective Behavior Understanding 

**Title (ZH)**: MACIE: 多智能体因果智能解释器用于集体行为理解 

**Authors**: Abraham Itzhak Weinberg  

**Link**: [PDF](https://arxiv.org/pdf/2511.15716)  

**Abstract**: As Multi Agent Reinforcement Learning systems are used in safety critical applications. Understanding why agents make decisions and how they achieve collective behavior is crucial. Existing explainable AI methods struggle in multi agent settings. They fail to attribute collective outcomes to individuals, quantify emergent behaviors, or capture complex interactions. We present MACIE Multi Agent Causal Intelligence Explainer, a framework combining structural causal models, interventional counterfactuals, and Shapley values to provide comprehensive explanations. MACIE addresses three questions. First, each agent's causal contribution using interventional attribution scores. Second, system level emergent intelligence through synergy metrics separating collective effects from individual contributions. Third, actionable explanations using natural language narratives synthesizing causal insights. We evaluate MACIE across four MARL scenarios: cooperative, competitive, and mixed motive. Results show accurate outcome attribution, mean phi_i equals 5.07, standard deviation less than 0.05, detection of positive emergence in cooperative tasks, synergy index up to 0.461, and efficient computation, 0.79 seconds per dataset on CPU. MACIE uniquely combines causal rigor, emergence quantification, and multi agent support while remaining practical for real time use. This represents a step toward interpretable, trustworthy, and accountable multi agent AI. 

**Abstract (ZH)**: 多智能体因果智能解释器：一种结合结构因果模型、干预反事实和Shapley值的框架 

---
# Dataset Distillation for Pre-Trained Self-Supervised Vision Models 

**Title (ZH)**: 预训练自我监督视觉模型的数据集精炼 

**Authors**: George Cazenavette, Antonio Torralba, Vincent Sitzmann  

**Link**: [PDF](https://arxiv.org/pdf/2511.16674)  

**Abstract**: The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets. 

**Abstract (ZH)**: 数据集蒸馏的目标是在找到一个小的合成图像集，使得在这些图像上训练的模型能够重现在大规模真实样本数据集上训练的同一模型的性能。现有的蒸馏方法侧重于合成能够训练随机初始化模型的数据集。相反，最新的视觉方法越来越多地依赖于大型的预训练自我监督模型，而不是从头开始训练。在本文中，我们探讨了蒸馏数据集的问题，该数据集使得我们能够在大型预训练视觉模型之上优化训练线性探针。我们引入了一种称为线性梯度匹配的数据集蒸馏方法，该方法优化合成图像，使得当通过预训练的特征提取器处理时，它们会在线性分类器中引发与真实数据相似的梯度。我们的方法生成的合成数据能够超越所有基于真实图片的基线，并且令人惊讶地能够在预训练视觉模型之间泛化，例如，我们能够使用通过DINO骨干蒸馏的数据集训练一个表现竞争力的线性CLIP探针。此外，我们展示了我们的蒸馏数据集在细粒度分类任务中具有极高的有效性，并为其模型可解释性提供了有价值的工具，能够在理想表示假设下预测两个模型的嵌入空间的相似性，或者判断模型是否对对抗数据集中的虚假相关性敏感。 

---
# Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation 

**Title (ZH)**: 基于上下文aware知识蒸馏的教师引导一次性剪枝 

**Authors**: Md. Samiul Alim, Sharjil Khan, Amrijit Biswas, Fuad Rahman, Shafin Rahman, Nabeel Mohammed  

**Link**: [PDF](https://arxiv.org/pdf/2511.16653)  

**Abstract**: Unstructured pruning remains a powerful strategy for compressing deep neural networks, yet it often demands iterative train-prune-retrain cycles, resulting in significant computational overhead. To address this challenge, we introduce a novel teacher-guided pruning framework that tightly integrates Knowledge Distillation (KD) with importance score estimation. Unlike prior approaches that apply KD as a post-pruning recovery step, our method leverages gradient signals informed by the teacher during importance score calculation to identify and retain parameters most critical for both task performance and knowledge transfer. Our method facilitates a one-shot global pruning strategy that efficiently eliminates redundant weights while preserving essential representations. After pruning, we employ sparsity-aware retraining with and without KD to recover accuracy without reactivating pruned connections. Comprehensive experiments across multiple image classification benchmarks, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that our method consistently achieves high sparsity levels with minimal performance degradation. Notably, our approach outperforms state-of-the-art baselines such as EPG and EPSD at high sparsity levels, while offering a more computationally efficient alternative to iterative pruning schemes like COLT. The proposed framework offers a computation-efficient, performance-preserving solution well suited for deployment in resource-constrained environments. 

**Abstract (ZH)**: 一种基于教师引导的剪枝框架：结合知识蒸馏与重要性评分估计以高效压缩深度神经网络 

---
# Evolution Strategies at the Hyperscale 

**Title (ZH)**: 超大规模环境下的进化策略 

**Authors**: Bidipta Sarkar, Mattie Fellows, Juan Agustin Duque, Alistair Letcher, Antonio León Villares, Anya Sims, Dylan Cope, Jarek Liesen, Lukas Seier, Theo Wolf, Uljad Berdica, Alexander David Goldie, Aaron Courville, Karin Sevegnani, Shimon Whiteson, Jakob Nicolaus Foerster  

**Link**: [PDF](https://arxiv.org/pdf/2511.16652)  

**Abstract**: We introduce Evolution Guided General Optimization via Low-rank Learning (EGGROLL), an evolution strategies (ES) algorithm designed to scale backprop-free optimization to large population sizes for modern large neural network architectures with billions of parameters. ES is a set of powerful blackbox optimisation methods that can handle non-differentiable or noisy objectives with excellent scaling potential through parallelisation. Na{ï}ve ES becomes prohibitively expensive at scale due to the computational and memory costs associated with generating matrix perturbations $E\in\mathbb{R}^{m\times n}$ and the batched matrix multiplications needed to compute per-member forward passes. EGGROLL overcomes these bottlenecks by generating random matrices $A\in \mathbb{R}^{m\times r},\ B\in \mathbb{R}^{n\times r}$ with $r\ll \min(m,n)$ to form a low-rank matrix perturbation $A B^\top$ that are used in place of the full-rank perturbation $E$. As the overall update is an average across a population of $N$ workers, this still results in a high-rank update but with significant memory and computation savings, reducing the auxiliary storage from $mn$ to $r(m+n)$ per layer and the cost of a forward pass from $\mathcal{O}(mn)$ to $\mathcal{O}(r(m+n))$ when compared to full-rank ES. A theoretical analysis reveals our low-rank update converges to the full-rank update at a fast $\mathcal{O}\left(\frac{1}{r}\right)$ rate. Our experiments show that (1) EGGROLL does not compromise the performance of ES in tabula-rasa RL settings, despite being faster, (2) it is competitive with GRPO as a technique for improving LLM reasoning, and (3) EGGROLL enables stable pre-training of nonlinear recurrent language models that operate purely in integer datatypes. 

**Abstract (ZH)**: Evolution引导的低秩学习大规模优化（EGGROLL）：面向现代大参数神经网络架构的目标优化算法 

---
# Faster Certified Symmetry Breaking Using Orders With Auxiliary Variables 

**Title (ZH)**: 更快的认证对称性打破方法：辅助变量与顺序的结合 

**Authors**: Markus Anders, Bart Bogaerts, Benjamin Bogø, Arthur Gontier, Wietze Koops, Ciaran McCreesh, Magnus O. Myreen, Jakob Nordström, Andy Oertel, Adrian Rebola-Pardo, Yong Kiam Tan  

**Link**: [PDF](https://arxiv.org/pdf/2511.16637)  

**Abstract**: Symmetry breaking is a crucial technique in modern combinatorial solving, but it is difficult to be sure it is implemented correctly. The most successful approach to deal with bugs is to make solvers certifying, so that they output not just a solution, but also a mathematical proof of correctness in a standard format, which can then be checked by a formally verified checker. This requires justifying symmetry reasoning within the proof, but developing efficient methods for this has remained a long-standing open challenge. A fully general approach was recently proposed by Bogaerts et al. (2023), but it relies on encoding lexicographic orders with big integers, which quickly becomes infeasible for large symmetries. In this work, we develop a method for instead encoding orders with auxiliary variables. We show that this leads to orders-of-magnitude speed-ups in both theory and practice by running experiments on proof logging and checking for SAT symmetry breaking using the state-of-the-art satsuma symmetry breaker and the VeriPB proof checking toolchain. 

**Abstract (ZH)**: 打破对称性是现代组合求解中的关键技术，但确保其实现正确性颇具挑战。处理错误的最成功方法是使求解器具备证明能力，不仅输出解决方案，还输出标准格式的正确性数学证明，这些证明可以被形式验证的检查器验证。这要求在证明中验证对称性推理，但开发高效方法解决这一问题仍是一个长期开放的挑战。Bogaerts等人（2023）最近提出了一种通用方法，但这种方法依赖于使用大整数编码字典序，这很快在处理大规模对称性时变得不可行。在本文中，我们开发了一种用辅助变量编码顺序的方法。我们通过在最先进的satsuma对称性打破工具和VeriPB证明检查工具链上进行证明记录和验证实验，展示了这种方法在理论和实践上都带来了数量级的速度提升。 

---
# Stabilizing Policy Gradient Methods via Reward Profiling 

**Title (ZH)**: 通过奖励分析稳定策略梯度方法 

**Authors**: Shihab Ahmed, El Houcine Bergou, Aritra Dutta, Yue Wang  

**Link**: [PDF](https://arxiv.org/pdf/2511.16629)  

**Abstract**: Policy gradient methods, which have been extensively studied in the last decade, offer an effective and efficient framework for reinforcement learning problems. However, their performances can often be unsatisfactory, suffering from unreliable reward improvements and slow convergence, due to high variance in gradient estimations. In this paper, we propose a universal reward profiling framework that can be seamlessly integrated with any policy gradient algorithm, where we selectively update the policy based on high-confidence performance estimations. We theoretically justify that our technique will not slow down the convergence of the baseline policy gradient methods, but with high probability, will result in stable and monotonic improvements of their performance. Empirically, on eight continuous-control benchmarks (Box2D and MuJoCo/PyBullet), our profiling yields up to 1.5x faster convergence to near-optimal returns, up to 1.75x reduction in return variance on some setups. Our profiling approach offers a general, theoretically grounded path to more reliable and efficient policy learning in complex environments. 

**Abstract (ZH)**: 基于策略梯度方法的通用奖励剖析框架：提高策略学习的可靠性和效率 

---
# Green Resilience of Cyber-Physical Systems: Doctoral Dissertation 

**Title (ZH)**: 网络物理系统的绿色韧性：博士论文 

**Authors**: Diaeddin Rimawi  

**Link**: [PDF](https://arxiv.org/pdf/2511.16593)  

**Abstract**: Cyber-physical systems (CPS) combine computational and physical components. Online Collaborative AI System (OL-CAIS) is a type of CPS that learn online in collaboration with humans to achieve a common goal, which makes it vulnerable to disruptive events that degrade performance. Decision-makers must therefore restore performance while limiting energy impact, creating a trade-off between resilience and greenness. This research addresses how to balance these two properties in OL-CAIS. It aims to model resilience for automatic state detection, develop agent-based policies that optimize the greenness-resilience trade-off, and understand catastrophic forgetting to maintain performance consistency. We model OL-CAIS behavior through three operational states: steady, disruptive, and final. To support recovery during disruptions, we introduce the GResilience framework, which provides recovery strategies through multi-objective optimization (one-agent), game-theoretic decision-making (two-agent), and reinforcement learning (RL-agent). We also design a measurement framework to quantify resilience and greenness. Empirical evaluation uses real and simulated experiments with a collaborative robot learning object classification from human demonstrations. Results show that the resilience model captures performance transitions during disruptions, and that GResilience policies improve green recovery by shortening recovery time, stabilizing performance, and reducing human dependency. RL-agent policies achieve the strongest results, although with a marginal increase in CO2 emissions. We also observe catastrophic forgetting after repeated disruptions, while our policies help maintain steadiness. A comparison with containerized execution shows that containerization cuts CO2 emissions by half. Overall, this research provides models, metrics, and policies that ensure the green recovery of OL-CAIS. 

**Abstract (ZH)**: 基于物理系统的在线协作AI系统(CPS)中韧性和绿色性的平衡研究 

---
# Synthesis of Safety Specifications for Probabilistic Systems 

**Title (ZH)**: 概率系统安全性规格的合成 

**Authors**: Gaspard Ohlmann, Edwin Hamel-De le Court, Francesco Belardinelli  

**Link**: [PDF](https://arxiv.org/pdf/2511.16579)  

**Abstract**: Ensuring that agents satisfy safety specifications can be crucial in safety-critical environments. While methods exist for controller synthesis with safe temporal specifications, most existing methods restrict safe temporal specifications to probabilistic-avoidance constraints. Formal methods typically offer more expressive ways to express safety in probabilistic systems, such as Probabilistic Computation Tree Logic (PCTL) formulas. Thus, in this paper, we develop a new approach that supports more general temporal properties expressed in PCTL. Our contribution is twofold. First, we develop a theoretical framework for the Synthesis of safe-PCTL specifications. We show how the reducing global specification satisfaction to local constraints, and define CPCTL, a fragment of safe-PCTL. We demonstrate how the expressiveness of CPCTL makes it a relevant fragment for the Synthesis Problem. Second, we leverage these results and propose a new Value Iteration-based algorithm to solve the synthesis problem for these more general temporal properties, and we prove the soundness and completeness of our method. 

**Abstract (ZH)**: 确保智能体满足安全规范在安全关键环境中至关重要。虽然已存在针对安全时序规范的控制器综合方法，但大多数现有方法仅限制安全时序规范为概率避险约束。形式化方法通常能够以更表达的方式来描述概率系统中的安全性，例如概率计算树逻辑（PCTL）公式。因此，在本文中，我们开发了一种新的方法，支持由PCTL表达的更广泛的时序性质。我们的贡献主要有两个方面。首先，我们为安全PCTL规范的综合开发了一个理论框架。我们展示了如何将全局规范满足问题缩减为局部约束，并定义了CPCTL，它是安全PCTL的一个片段。我们证明了CPCTL的表达力使其成为综合问题的相关片段。其次，我们利用这些结果提出了一种新的基于值迭代的算法来解决这些更广泛的时序性质的综合问题，并证明了我们方法的正确性和完备性。 

---
# ECPv2: Fast, Efficient, and Scalable Global Optimization of Lipschitz Functions 

**Title (ZH)**: ECPv2: 快速、高效且可扩展的Lipschitz函数全局优化方法 

**Authors**: Fares Fourati, Mohamed-Slim Alouini, Vaneet Aggarwal  

**Link**: [PDF](https://arxiv.org/pdf/2511.16575)  

**Abstract**: We propose ECPv2, a scalable and theoretically grounded algorithm for global optimization of Lipschitz-continuous functions with unknown Lipschitz constants. Building on the Every Call is Precious (ECP) framework, which ensures that each accepted function evaluation is potentially informative, ECPv2 addresses key limitations of ECP, including high computational cost and overly conservative early behavior. ECPv2 introduces three innovations: (i) an adaptive lower bound to avoid vacuous acceptance regions, (ii) a Worst-m memory mechanism that restricts comparisons to a fixed-size subset of past evaluations, and (iii) a fixed random projection to accelerate distance computations in high dimensions. We theoretically show that ECPv2 retains ECP's no-regret guarantees with optimal finite-time bounds and expands the acceptance region with high probability. We further empirically validate these findings through extensive experiments and ablation studies. Using principled hyperparameter settings, we evaluate ECPv2 across a wide range of high-dimensional, non-convex optimization problems. Across benchmarks, ECPv2 consistently matches or outperforms state-of-the-art optimizers, while significantly reducing wall-clock time. 

**Abstract (ZH)**: 我们提出ECPv2，一种适用于未知Lipschitz常数的Lipschitz连续函数全局优化的可扩展且具有理论依据的算法。基于每次调用都珍贵（ECP）框架，该框架确保每次接受的函数评估都有潜在的信息价值，ECPv2解决了ECP的若干关键局限性，包括计算成本高和早期行为过于保守。ECPv2引入了三项创新：（i）自适应下界以避免无效的接受区域，（ii）Worst-m记忆机制，限制比较仅在过去的固定数量的评估中进行，以及（iii）固定随机投影以加速高维空间中的距离计算。我们从理论上证明，ECPv2保留了ECP的无遗憾保证，并且在最优的有限时间界内扩展了接受区域，同时以高概率进行。此外，我们通过广泛的实验和消融研究进一步实证验证了这些发现。通过原理上的超参数设置，我们评估了ECPv2在一系列高维非凸优化问题中的表现。在各种基准测试中，ECPv2一致地与最先进的优化器持平或表现出色，同时显著减少了wall-clock时间。 

---
# NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening 

**Title (ZH)**: NutriScreener: 检索增强多姿态图注意力网络用于营养不良筛查 

**Authors**: Misaal Khan, Mayank Vatsa, Kuldeep Singh, Richa Singh  

**Link**: [PDF](https://arxiv.org/pdf/2511.16566)  

**Abstract**: Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments. 

**Abstract (ZH)**: 儿童营养不良仍是全球性危机，现有筛查方法繁琐且难以扩展，妨碍了早期干预。在本工作中，我们提出了NutriScreener，这是一种检索增强的多姿态图注意力网络，结合了基于CLIP的视觉嵌入、类增强的知识检索和上下文意识，以从儿童图像中实现稳健的营养不良检测和人体测量预测，同时解决了泛化能力和类别不平衡问题。在临床研究中，医生对其准确性的评分是4.3/5，对其效率的评分是4.6/5，证实了其在低资源环境中的部署准备度。该模型在AnthroVision的2,141名儿童上进行训练和测试，并在包括ARAN和内部收集的CampusPose数据集在内的多元跨地域人群中进行了额外评估，实现了0.79的召回率、0.82的AUC以及显著较低的人体测量RMSE，展示了在非受限儿童环境中的可靠测量能力。跨数据集结果显示，使用人口匹配的知识库可获得多达25%的召回率提升和多达3.5 cm的RMSE减少。NutriScreener为低资源环境中的早期营养不良检测提供了可扩展且准确的解决方案。 

---
# Interfacial and bulk switching MoS2 memristors for an all-2D reservoir computing framework 

**Title (ZH)**: 界面和体相切换MoS2忆阻器用于全二维水库计算框架 

**Authors**: Asmita S. Thool, Sourodeep Roy, Prahalad Kanti Barman, Kartick Biswas, Pavan Nukala, Abhishek Misra, Saptarshi Das, and Bhaswar Chakrabarti  

**Link**: [PDF](https://arxiv.org/pdf/2511.16557)  

**Abstract**: In this study, we design a reservoir computing (RC) network by exploiting short- and long-term memory dynamics in Au/Ti/MoS$_2$/Au memristive devices. The temporal dynamics is engineered by controlling the thickness of the Chemical Vapor Deposited (CVD) MoS$_2$ films. Devices with a monolayer (1L)-MoS$_2$ film exhibit volatile (short-term memory) switching dynamics. We also report non-volatile resistance switching with excellent uniformity and analog behavior in conductance tuning for the multilayer (ML) MoS$_2$ memristive devices. We correlate this performance with trap-assisted space-charge limited conduction (SCLC) mechanism, leading to a bulk-limited resistance switching behavior. Four-bit reservoir states are generated using volatile memristors. The readout layer is implemented with an array of nonvolatile synapses. This small RC network achieves 89.56\% precision in a spoken-digit recognition task and is also used to analyze a nonlinear time series equation. 

**Abstract (ZH)**: 在本研究中，我们通过利用Au/Ti/MoS$_2$/Au memristive器件中的短时和长时间记忆动力学，设计了一种 reservoir 计算网络。时间动力学通过控制化学气相沉积（CVD）MoS$_2$薄膜的厚度进行工程设计。仅一层（1L）MoS$_2$薄膜的器件表现出易失性（短时记忆）切换动力学。我们还报告了多层（ML）MoS$_2$ memristive器件中优良均匀性和模拟行为的非易失性电阻切换。我们将这种性能归因于陷阱辅助的空间电荷限制传导（SCLC）机制，导致体限电阻切换行为。使用易失性 memristor 生成四比特 reservoir 状态。读出层通过非易失性突触阵列实现。这个小型 reservoir 计算网络在语音数字识别任务中达到了 89.56% 的精度，并且也被用于分析非线性时间序列方程。 

---
# TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval 

**Title (ZH)**: TurkColBERT：一种用于土耳其信息检索的密集表示和晚期交互模型基准 

**Authors**: Özay Ezerceli, Mahmoud El Hussieni, Selva Taş, Reyhan Bayraktar, Fatma Betül Terzioğlu, Yusuf Çelebi, Yağız Asker  

**Link**: [PDF](https://arxiv.org/pdf/2511.16528)  

**Abstract**: Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\% of its average mAP. Late-interaction models that are 3--5$\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\times$ faster than PLAID and offers +1.7\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary. 

**Abstract (ZH)**: 神经信息检索系统在高资源语言中表现出色，但在如土耳其语这样形态丰富、资源较少的语言中仍待探索。目前，稠密双编码器在土耳其语IR中占据主导地位，然而晚期交互模型——这些模型保留了子单元级别的表示以实现精细匹配——尚未进行系统评估。我们引入TurkColBERT，这是首个全面比较稠密编码器和晚期交互模型的基准，用于土耳其语检索。我们的两阶段适应管道在土耳其语NLI/STS任务上微调英语文本和多语言编码器，然后使用PyLate在MS MARCO-TR上对其进行转换，生成ColBERT风格的检索器。我们评估了10个模型在涵盖科学、金融和论辩领域的五个Turkish BEIR数据集上的表现。结果显示，参数效率极高：100万参数的ColBERT-hash-nano-tr比6亿参数的Turkish-e5-large稠密编码器小600倍，同时保持了其平均mAP的71%以上。相比稠密编码器，其规模小3-5倍的晚期交互模型显著优于后者；ColmmBERT-base-TR在特定领域任务上可将mAP提高多达13.8%。为了提高生产可用性，我们比较了索引算法：MUVERA+Rerank比PLAID快3.33倍，并且相对提高了1.7%的mAP。这使得在MUVERA支持下，检索可以实现微秒级的延迟。我们发布了所有检查点、配置和评估脚本。该研究的局限性包括依赖于中等大小的数据集（≤50K文档）和翻译基准，这可能不能完全反映真实的土耳其语检索条件，大规模MUVERA评估仍需进一步开展。 

---
# Generative Modeling of Clinical Time Series via Latent Stochastic Differential Equations 

**Title (ZH)**: 基于潜在随机微分方程的临床时间序列生成模型 

**Authors**: Muhammad Aslanimoghanloo, Ahmed ElGazzar, Marcel van Gerven  

**Link**: [PDF](https://arxiv.org/pdf/2511.16427)  

**Abstract**: Clinical time series data from electronic health records and medical registries offer unprecedented opportunities to understand patient trajectories and inform medical decision-making. However, leveraging such data presents significant challenges due to irregular sampling, complex latent physiology, and inherent uncertainties in both measurements and disease progression. To address these challenges, we propose a generative modeling framework based on latent neural stochastic differential equations (SDEs) that views clinical time series as discrete-time partial observations of an underlying controlled stochastic dynamical system. Our approach models latent dynamics via neural SDEs with modality-dependent emission models, while performing state estimation and parameter learning through variational inference. This formulation naturally handles irregularly sampled observations, learns complex non-linear interactions, and captures the stochasticity of disease progression and measurement noise within a unified scalable probabilistic framework. We validate the framework on two complementary tasks: (i) individual treatment effect estimation using a simulated pharmacokinetic-pharmacodynamic (PKPD) model of lung cancer, and (ii) probabilistic forecasting of physiological signals using real-world intensive care unit (ICU) data from 12,000 patients. Results show that our framework outperforms ordinary differential equation and long short-term memory baseline models in accuracy and uncertainty estimation. These results highlight its potential for enabling precise, uncertainty-aware predictions to support clinical decision-making. 

**Abstract (ZH)**: 电子健康记录和医疗注册机构中的临床时间序列数据提供了前所未有的机会来理解患者轨迹并指导医疗决策。然而，利用这些数据面临着巨大挑战，包括不规则采样、复杂的潜在生理学、以及在测量和疾病进展中存在的固有不确定性。为了解决这些挑战，我们提出了一种基于潜在神经随机微分方程（SDE）的生成建模框架，将临床时间序列视为潜在受控随机动力系统在离散时间下的部分观测。我们的方法通过具有模态依赖发射模型的神经SDE来建模潜在动力学，并通过变分推断进行状态估计和参数学习。这种表述自然处理不规则采样的观察，学习复杂的非线性交互作用，并在统一的可扩展概率框架中捕捉疾病进展和测量噪声的随机性。我们通过两个互补任务验证了该框架：一是使用模拟的肺癌药代药效（PKPD）模型估计个体治疗效果；二是使用12000名患者实际重症监护病房（ICU）数据进行生理信号的概率预测。结果显示，与普通的微分方程和长短期记忆基线模型相比，该框架在准确性和不确定性估计方面表现更优。这些结果突显了其在支持临床决策中实现精确、不确定性意识预测的潜力。 

---
# Collaborative Management for Chronic Diseases and Depression: A Double Heterogeneity-based Multi-Task Learning Method 

**Title (ZH)**: 基于双异质性多任务学习的慢性疾病和抑郁协作管理方法 

**Authors**: Yidong Chai, Haoxin Liu, Jiaheng Xie, Chaopeng Wang, Xiao Fang  

**Link**: [PDF](https://arxiv.org/pdf/2511.16398)  

**Abstract**: Wearable sensor technologies and deep learning are transforming healthcare management. Yet, most health sensing studies focus narrowly on physical chronic diseases. This overlooks the critical need for joint assessment of comorbid physical chronic diseases and depression, which is essential for collaborative chronic care. We conceptualize multi-disease assessment, including both physical diseases and depression, as a multi-task learning (MTL) problem, where each disease assessment is modeled as a task. This joint formulation leverages inter-disease relationships to improve accuracy, but it also introduces the challenge of double heterogeneity: chronic diseases differ in their manifestation (disease heterogeneity), and patients with the same disease show varied patterns (patient heterogeneity). To address these issues, we first adopt existing techniques and propose a base method. Given the limitations of the base method, we further propose an Advanced Double Heterogeneity-based Multi-Task Learning (ADH-MTL) method that improves the base method through three innovations: (1) group-level modeling to support new patient predictions, (2) a decomposition strategy to reduce model complexity, and (3) a Bayesian network that explicitly captures dependencies while balancing similarities and differences across model components. Empirical evaluations on real-world wearable sensor data demonstrate that ADH-MTL significantly outperforms existing baselines, and each of its innovations is shown to be effective. This study contributes to health information systems by offering a computational solution for integrated physical and mental healthcare and provides design principles for advancing collaborative chronic disease management across the pre-treatment, treatment, and post-treatment phases. 

**Abstract (ZH)**: 可穿戴传感器技术和深度学习正在变革健康管理工作。然而，大多数健康感知研究集中在物理慢性疾病上。这忽视了同时评估共病物理慢性疾病和抑郁的重要需求，这对于协作性慢性病管理是必不可少的。我们将包括物理疾病和抑郁在内的多病种评估概念化为多任务学习（MTL）问题，其中每种疾病评估被视为一个任务。这种联合建模利用了不同疾病的相互关系以提高准确性，但也带来了双重异质性的挑战：慢性疾病在表现形式上不同（疾病异质性），而患有相同疾病患者则表现出不同的模式（患者异质性）。为了解决这些问题，我们首先采用了现有技术并提出了一种基方法。鉴于基方法的局限性，我们进一步提出了基于双重异质性的先进多任务学习（ADH-MTL）方法，通过三项创新改进了基方法：（1）分组级建模以支持新患者的预测；（2）分解策略以降低模型复杂性；（3）贝叶斯网络，它明确捕捉依赖关系并在模型组件之间平衡相似性和差异性。在真实世界可穿戴传感器数据上的实证评估表明，ADH-MTL 显著优于现有基方法，并且其每项创新都是有效的。这项研究通过提供整合身心健康的计算解决方案，丰富了健康信息系统，并为推进预治疗、治疗和后续治疗的协作慢性病管理提供了设计原则。 

---
# Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies 

**Title (ZH)**: 基于充分解释的学习：分析解释忠实度与token级正则化策略之间的关系 

**Authors**: Jonathan Kamp, Lisa Beinborn, Antske Fokkens  

**Link**: [PDF](https://arxiv.org/pdf/2511.16353)  

**Abstract**: Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation. 

**Abstract (ZH)**: 自然语言的机器解释为其提供了一种工具，以评估模型是否出于正确的原因学习标签或依赖于数据集特定的捷径。信息量充足的度量常用于估计解释信息的重要性，但其对解释信息如何影响模型性能提供有限的洞察。我们通过将其与两种建模范式相关联来弥补这一局限：模型识别哪些词属于解释信息的能力（通过词分类实现）和通过注意力规约改进模型性能的能力（通过在输入中包含解释信息实现）。我们发现，高度信息丰富的解释信息不太可能帮助正确分类实例。相反，信息量充足的度量捕捉了未解释上下文对解释信息的分类影响，这与同一输入中的解释信息相互干扰。此外，我们发现将解释信息包含在模型输入中可以增强跨域分类，但结果在不同的任务和模型类型上不一致。最后，信息量充足的度量与词分类似乎无关。这些结果突出显示了解释信息的复杂性，表明能够系统捕捉这种类型信息的度量需要进一步研究。 

---
# TS-PEFT: Token-Selective Parameter-Efficient Fine-Tuning with Learnable Threshold Gating 

**Title (ZH)**: TS-PEFT: Token-选择性参数-efficient微调带可学习阈值门控 

**Authors**: Dabiao Ma, Ziming Dai, Zhimin Xin, Shu Wang, Ye Wang, Haojun Fei  

**Link**: [PDF](https://arxiv.org/pdf/2511.16147)  

**Abstract**: In the field of large models (LMs) for natural language processing (NLP) and computer vision (CV), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient method that modifies a limited number of parameters while keeping the pretrained weights fixed. This paper investigates the traditional PEFT approach, which applies modifications to all position indices, and questions its necessity. We introduce a new paradigm called Token-Selective PEFT (TS-PEFT), in which a function S selectively applies PEFT modifications to a subset of position indices, potentially enhancing performance on downstream tasks. Our experimental results reveal that the indiscriminate application of PEFT to all indices is not only superfluous, but may also be counterproductive. This study offers a fresh perspective on PEFT, advocating for a more targeted approach to modifications and providing a framework for future research to optimize the fine-tuning process for large models. 

**Abstract (ZH)**: 在自然语言处理（NLP）和计算机视觉（CV）领域的大模型（LMs）中，参数高效微调（PEFT）作为一种资源高效的方法逐渐兴起，它仅修改少量参数而保持预训练权重不变。本文探讨了传统的PEFT方法，该方法对所有位置索引都进行修改，并质疑其必要性。我们提出了一种新的范式，称为token选择性PEFT（TS-PEFT），在这种方法中，一个选择函数S仅对位置索引的一部分进行PEFT修改，有可能在下游任务上提高性能。我们的实验结果表明，对所有索引进行不分青红皂白的PEFT修改不仅是多余的，甚至可能是有害的。这项研究为PEFT提供了一个新的视角，提倡更具针对性的修改方法，并为未来优化大模型微调过程的研究提供了框架。 

---
# Labels Matter More Than Models: Quantifying the Benefit of Supervised Time Series Anomaly Detection 

**Title (ZH)**: 标签比模型更重要：量化监督时间序列异常检测的收益 

**Authors**: Zhijie Zhong, Zhiwen Yu, Kaixiang Yang, C. L. Philip Chen  

**Link**: [PDF](https://arxiv.org/pdf/2511.16145)  

**Abstract**: Time series anomaly detection (TSAD) is a critical data mining task often constrained by label scarcity. Consequently, current research predominantly focuses on Unsupervised Time-series Anomaly Detection (UTAD), relying on complex architectures to model normal data distributions. However, this approach often overlooks the significant performance gains available from limited anomaly labels achievable in practical scenarios. This paper challenges the premise that architectural complexity is the optimal path for TSAD. We conduct the first methodical comparison between supervised and unsupervised paradigms and introduce STAND, a streamlined supervised baseline. Extensive experiments on five public datasets demonstrate that: (1) Labels matter more than models: under a limited labeling budget, simple supervised models significantly outperform complex state-of-the-art unsupervised methods; (2) Supervision yields higher returns: the performance gain from minimal supervision far exceeds that from architectural innovations; and (3) Practicality: STAND exhibits superior prediction consistency and anomaly localization compared to unsupervised counterparts. These findings advocate for a data-centric shift in TSAD research, emphasizing label utilization over purely algorithmic complexity. The code is publicly available at this https URL. 

**Abstract (ZH)**: 时间序列异常检测（TSAD）是数据挖掘中的一个关键任务，通常受到标签稀缺的限制。因此，当前研究主要集中在无监督时间序列异常检测（UTAD）上，依赖复杂的架构来建模正常数据分布。然而，这种方法往往忽略了在实际场景中从有限的异常标签中获得的巨大性能提升。本文挑战了TSAD中架构复杂性是最优路径的前提。我们首次系统性地比较了监督和无监督范式，并引入了STAND，这是一种简化的监督基线。在五个公开数据集上的大量实验表明：（1）标签比模型更重要：在有限的标注预算下，简单的监督模型显著优于复杂的最新无监督方法；（2）监督回报更高：最小监督所带来的性能提升远超架构创新；（3）实用性：与无监督方法相比，STAND在预测一致性及异常定位方面表现出更优异的效果。这些发现呼吁TSAD研究向以数据为中心的方向转变，强调标签利用而非纯粹的算法复杂性。相关代码已公开，地址为：this https URL。 

---
# Mitigating Estimation Bias with Representation Learning in TD Error-Driven Regularization 

**Title (ZH)**: 用表示学习减轻TD误差驱动正则化中的估计偏见 

**Authors**: Haohui Chen, Zhiyong Chen, Aoxiang Liu, Wentuo Fang  

**Link**: [PDF](https://arxiv.org/pdf/2511.16090)  

**Abstract**: Deterministic policy gradient algorithms for continuous control suffer from value estimation biases that degrade performance. While double critics reduce such biases, the exploration potential of double actors remains underexplored. Building on temporal-difference error-driven regularization (TDDR), a double actor-critic framework, this work introduces enhanced methods to achieve flexible bias control and stronger representation learning. We propose three convex combination strategies, symmetric and asymmetric, that balance pessimistic estimates to mitigate overestimation and optimistic exploration via double actors to alleviate underestimation. A single hyperparameter governs this mechanism, enabling tunable control across the bias spectrum. To further improve performance, we integrate augmented state and action representations into the actor and critic networks. Extensive experiments show that our approach consistently outperforms benchmarks, demonstrating the value of tunable bias and revealing that both overestimation and underestimation can be exploited differently depending on the environment. 

**Abstract (ZH)**: 确定性策略梯度算法在连续控制中受价值估计偏差影响，导致性能下降。虽然双重批评者可以减少此类偏差，但双重行动者探索潜力的研究尚不充分。基于时差误差驱动正则化（TDDR），一个双重行动者-批评者框架，本文引入增强方法以实现灵活的偏差控制和更强的表示学习。我们提出三种凸组合策略，对称和非对称，平衡悲观估计以缓解过度估计，并通过双重行动者进行乐观探索以减轻低估。该机制由单一超参数控制，可在偏差谱上实现可调控制。为进一步提高性能，我们将扩展状态和动作表示集成到行动者和批评者网络中。广泛实验表明，我们的方法在基准算法上表现更优，证明了可调偏差的价值，并揭示了在不同环境中过度估计和低估可以以不同方式利用。 

---
# Future-Back Threat Modeling: A Foresight-Driven Security Framework 

**Title (ZH)**: 未来导向威胁建模：一种前瞻性驱动的安全框架 

**Authors**: Vu Van Than  

**Link**: [PDF](https://arxiv.org/pdf/2511.16088)  

**Abstract**: Traditional threat modeling remains reactive-focused on known TTPs and past incident data, while threat prediction and forecasting frameworks are often disconnected from operational or architectural artifacts. This creates a fundamental weakness: the most serious cyber threats often do not arise from what is known, but from what is assumed, overlooked, or not yet conceived, and frequently originate from the future, such as artificial intelligence, information warfare, and supply chain attacks, where adversaries continuously develop new exploits that can bypass defenses built on current knowledge. To address this mental gap, this paper introduces the theory and methodology of Future-Back Threat Modeling (FBTM). This predictive approach begins with envisioned future threat states and works backward to identify assumptions, gaps, blind spots, and vulnerabilities in the current defense architecture, providing a clearer and more accurate view of impending threats so that we can anticipate their emergence and shape the future we want through actions taken now. The proposed methodology further aims to reveal known unknowns and unknown unknowns, including tactics, techniques, and procedures that are emerging, anticipated, and plausible. This enhances the predictability of adversary behavior, particularly under future uncertainty, helping security leaders make informed decisions today that shape more resilient security postures for the future. 

**Abstract (ZH)**: 未来向后威胁建模（FBTM）：预测性方法及其应用 

---
# SpectralTrain: A Universal Framework for Hyperspectral Image Classification 

**Title (ZH)**: SpectralTrain: 通用的高光谱图像分类框架 

**Authors**: Meihua Zhou, Liping Yu, Jiawei Cai, Wai Kin Fung, Ruiguo Hu, Jiarui Zhao, Wenzhuo Liu, Nan Wan  

**Link**: [PDF](https://arxiv.org/pdf/2511.16084)  

**Abstract**: Hyperspectral image (HSI) classification typically involves large-scale data and computationally intensive training, which limits the practical deployment of deep learning models in real-world remote sensing tasks. This study introduces SpectralTrain, a universal, architecture-agnostic training framework that enhances learning efficiency by integrating curriculum learning (CL) with principal component analysis (PCA)-based spectral downsampling. By gradually introducing spectral complexity while preserving essential information, SpectralTrain enables efficient learning of spectral -- spatial patterns at significantly reduced computational costs. The framework is independent of specific architectures, optimizers, or loss functions and is compatible with both classical and state-of-the-art (SOTA) models. Extensive experiments on three benchmark datasets -- Indian Pines, Salinas-A, and the newly introduced CloudPatch-7 -- demonstrate strong generalization across spatial scales, spectral characteristics, and application domains. The results indicate consistent reductions in training time by 2-7x speedups with small-to-moderate accuracy deltas depending on backbone. Its application to cloud classification further reveals potential in climate-related remote sensing, emphasizing training strategy optimization as an effective complement to architectural design in HSI models. Code is available at this https URL. 

**Abstract (ZH)**: 高光谱图像(HSI)分类通常涉及大规模数据和计算密集型训练，这限制了深度学习模型在实际遥感任务中的应用部署。本文介绍了一种通用且架构无关的训练框架SpectralTrain，通过结合梯度课程学习(CL)与基于主成分分析(PCA)的光谱降采样，提升了学习效率。SpectralTrain通过逐步引入光谱复杂性并保留关键信息，使在显著降低计算成本的前提下高效学习光谱-空间模式成为可能。该框架独立于特定架构、优化器或损失函数，并与经典和最新的(SOTA)模型兼容。在印度pine、Salinas-A以及新引入的CloudPatch-7三个基准数据集上的广泛实验表明，其在空间尺度、光谱特性及应用领域展现出强大的泛化能力。结果表明，其训练时间比传统方法可减少2-7倍，同时小到中等程度的准确率差异依赖于所使用的骨干网络。其在云分类中的应用进一步表明，该方法在气候相关遥感中具有潜在应用价值，强调了训练策略优化是提高HSI模型性能的有效补充。代码可从此链接获取。 

---
# Operon: Incremental Construction of Ragged Data via Named Dimensions 

**Title (ZH)**: Operon: 命名维度下增量构建不规则数据的方法 

**Authors**: Sungbin Moon, Jiho Park, Suyoung Hwang, Donghyun Koh, Seunghyun Moon, Minhyeong Lee  

**Link**: [PDF](https://arxiv.org/pdf/2511.16080)  

**Abstract**: Modern data processing workflows frequently encounter ragged data: collections with variable-length elements that arise naturally in domains like natural language processing, scientific measurements, and autonomous AI agents. Existing workflow engines lack native support for tracking the shapes and dependencies inherent to ragged data, forcing users to manage complex indexing and dependency bookkeeping manually. We present Operon, a Rust-based workflow engine that addresses these challenges through a novel formalism of named dimensions with explicit dependency relations. Operon provides a domain-specific language where users declare pipelines with dimension annotations that are statically verified for correctness, while the runtime system dynamically schedules tasks as data shapes are incrementally discovered during execution. We formalize the mathematical foundation for reasoning about partial shapes and prove that Operon's incremental construction algorithm guarantees deterministic and confluent execution in parallel settings. The system's explicit modeling of partially-known states enables robust persistence and recovery mechanisms, while its per-task multi-queue architecture achieves efficient parallelism across heterogeneous task types. Empirical evaluation demonstrates that Operon outperforms an existing workflow engine with 14.94x baseline overhead reduction while maintaining near-linear end-to-end output rates as workloads scale, making it particularly suitable for large-scale data generation pipelines in machine learning applications. 

**Abstract (ZH)**: 现代数据处理工作流经常遇到不规则数据：具有可变长度元素的集合，这些元素在自然语言处理、科学研究和自主AI代理等领域自然出现。现有的工作流引擎缺乏对不规则数据内在形状和依赖关系的原生支持，迫使用户手动管理复杂的索引和依赖关系记录。我们提出了一个基于Rust的工作流引擎Operon，通过一种新的命名维度形式主义和显式的依赖关系来解决这些挑战。Operon提供了一个领域特定的语言，用户可以在其中声明带有维度注解的管道，这些注解可以静态验证其正确性，而运行时系统则根据执行过程中数据形状的逐步发现来动态调度任务。我们为关于部分形状的数学基础进行了形式化，并证明了Operon的增量构建算法在并行设置中保证了确定性和会聚的执行。该系统的部分已知状态显式建模使持久性和恢复机制更加健壮，而其每任务多队列架构实现了异构任务类型之间的高效并行性。实证评估表明，与现有的工作流引擎相比，Operon的基线开销减少了14.94倍，同时在工作负载扩展时保持近乎线性的端到端输出速率，使其特别适合于机器学习应用中的大规模数据生成管道。 

---
# HGCN2SP: Hierarchical Graph Convolutional Network for Two-Stage Stochastic Programming 

**Title (ZH)**: HGCN2SP: 分层图卷积网络应用于两阶段随机规划 

**Authors**: Yang Wu, Yifan Zhang, Zhenxing Liang, Jian Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2511.16027)  

**Abstract**: Two-stage Stochastic Programming (2SP) is a standard framework for modeling decision-making problems under uncertainty. While numerous methods exist, solving such problems with many scenarios remains challenging. Selecting representative scenarios is a practical method for accelerating solutions. However, current approaches typically rely on clustering or Monte Carlo sampling, failing to integrate scenario information deeply and overlooking the significant impact of the scenario order on solving time. To address these issues, we develop HGCN2SP, a novel model with a hierarchical graph designed for 2SP problems, encoding each scenario and modeling their relationships hierarchically. The model is trained in a reinforcement learning paradigm to utilize the feedback of the solver. The policy network is equipped with a hierarchical graph convolutional network for feature encoding and an attention-based decoder for scenario selection in proper order. Evaluation of two classic 2SP problems demonstrates that HGCN2SP provides high-quality decisions in a short computational time. Furthermore, HGCN2SP exhibits remarkable generalization capabilities in handling large-scale instances, even with a substantial number of variables or scenarios that were unseen during the training phase. 

**Abstract (ZH)**: 基于层次图的两阶段随机规划模型（HGCN2SP） 

---
# Physics-Guided Inductive Spatiotemporal Kriging for PM2.5 with Satellite Gradient Constraints 

**Title (ZH)**: 基于物理指导的诱导时空克里金模型及其卫星梯度约束应用于PM2.5预测 

**Authors**: Shuo Wang, Mengfan Teng, Yun Cheng, Lothar Thiele, Olga Saukh, Shuangshuang He, Yuanting Zhang, Jiang Zhang, Gangfeng Zhang, Xingyuan Yuan, Jingfang Fan  

**Link**: [PDF](https://arxiv.org/pdf/2511.16013)  

**Abstract**: High-resolution mapping of fine particulate matter (PM2.5) is a cornerstone of sustainable urbanism but remains critically hindered by the spatial sparsity of ground monitoring networks. While traditional data-driven methods attempt to bridge this gap using satellite Aerosol Optical Depth (AOD), they often suffer from severe, non-random data missingness (e.g., due to cloud cover or nighttime) and inversion biases. To overcome these limitations, this study proposes the Spatiotemporal Physics-Guided Inference Network (SPIN), a novel framework designed for inductive spatiotemporal kriging. Unlike conventional approaches, SPIN synergistically integrates domain knowledge into deep learning by explicitly modeling physical advection and diffusion processes via parallel graph kernels. Crucially, we introduce a paradigm-shifting training strategy: rather than using error-prone AOD as a direct input, we repurpose it as a spatial gradient constraint within the loss function. This allows the model to learn structural pollution patterns from satellite data while remaining robust to data voids. Validated in the highly polluted Beijing-Tianjin-Hebei and Surrounding Areas (BTHSA), SPIN achieves a new state-of-the-art with a Mean Absolute Error (MAE) of 9.52 ug/m^3, effectively generating continuous, physically plausible pollution fields even in unmonitored areas. This work provides a robust, low-cost, and all-weather solution for fine-grained environmental management. 

**Abstract (ZH)**: 高分辨率细颗粒物（PM2.5）分布mapping及其在可持续城市规划中的应用仍受地面监测网络空间稀疏性的限制，而时空物理引导推理网络（SPIN）为这一挑战提供了解决方案。 

---
# Synergizing Deconfounding and Temporal Generalization For Time-series Counterfactual Outcome Estimation 

**Title (ZH)**: 协同去混杂与时间泛化以实现时间序列反事实效果估计 

**Authors**: Yiling Liu, Juncheng Dong, Chen Fu, Wei Shi, Ziyang Jiang, Zhigang Hua, David Carlson  

**Link**: [PDF](https://arxiv.org/pdf/2511.16006)  

**Abstract**: Estimating counterfactual outcomes from time-series observations is crucial for effective decision-making, e.g. when to administer a life-saving treatment, yet remains significantly challenging because (i) the counterfactual trajectory is never observed and (ii) confounders evolve with time and distort estimation at every step. To address these challenges, we propose a novel framework that synergistically integrates two complementary approaches: Sub-treatment Group Alignment (SGA) and Random Temporal Masking (RTM). Instead of the coarse practice of aligning marginal distributions of the treatments in latent space, SGA uses iterative treatment-agnostic clustering to identify fine-grained sub-treatment groups. Aligning these fine-grained groups achieves improved distributional matching, thus leading to more effective deconfounding. We theoretically demonstrate that SGA optimizes a tighter upper bound on counterfactual risk and empirically verify its deconfounding efficacy. RTM promotes temporal generalization by randomly replacing input covariates with Gaussian noises during training. This encourages the model to rely less on potentially noisy or spuriously correlated covariates at the current step and more on stable historical patterns, thereby improving its ability to generalize across time and better preserve underlying causal relationships. Our experiments demonstrate that while applying SGA and RTM individually improves counterfactual outcome estimation, their synergistic combination consistently achieves state-of-the-art performance. This success comes from their distinct yet complementary roles: RTM enhances temporal generalization and robustness across time steps, while SGA improves deconfounding at each specific time point. 

**Abstract (ZH)**: 从时间序列观察中估计反事实结果对于有效决策至关重要，例如何时施用救命治疗，但这仍然极具挑战性，因为（i）反事实轨迹从未被观测到，（ii）混杂因素会随时间演变并在每一步扭曲估算。为应对这些挑战，我们提出了一种新的框架，该框架结合了两种互补的方法：亚治疗组对齐（SGA）和随机时间掩蔽（RTM）。SGA 不是通过在潜在空间中合并治疗的边际分布来进行粗略处理，而是使用迭代的治疗无关聚类来识别细粒度的亚治疗组。对齐这些细粒度组实现了更好的分布匹配，从而提高了去混杂的有效性。我们从理论上证明了SGA优化了反事实风险的更紧的上界，并通过实验证实了其去混杂的有效性。RTM 在训练过程中通过随机用高斯噪声替换输入协变量来促进时间泛化。这促使模型在当前步骤较少依赖可能的噪声或假相关协变量，更多地依赖于稳定的历史模式，从而增强了其跨时间的时间泛化能力和更好地保持潜在因果关系。我们的实验表明，虽然单独应用SGA和RTM可以改善反事实结果的估计，但它们的协同组合始终能够实现最佳性能。这种成功来自于它们各自独特的互补作用：RTM 增强了跨时间步骤的时间泛化能力和鲁棒性，而SGA 在每个具体时间点提高了去混杂的能力。 

---
# InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution 

**Title (ZH)**: InfCode: 敌对迭代测试与修补件 refinement 以实现可靠的软件问题解决 

**Authors**: KeFan Li, Mengfei Wang, Hengzhi Zhang, Zhichao Li, Yuan Yuan, Mu Li, Xiang Gao, Hailong Sun, Chunming Hu, Weifeng Lv  

**Link**: [PDF](https://arxiv.org/pdf/2511.16004)  

**Abstract**: Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at this https URL. 

**Abstract (ZH)**: 大型语言模型推进了软件工程自动化，但解决实际软件问题仍然困难，因为这需要仓库级推理、精确诊断和强烈验证信号。现有基于代理和基于流水线的方法往往依赖于不充分的测试，可能导致满足验证但未能修复根本缺陷的补丁。我们提出InfCode，一种对抗式多代理框架，用于自动化仓库级问题解决。InfCode通过测试生成器和代码生成器之间的对抗性交互，迭代优化测试和补丁，同时选择器代理识别最可靠的修复。该框架在一个支持真实仓库检查、修改和验证的容器化环境中运行。使用DeepSeek-V3和Claude 4.5 Sonnet等模型在SWE-bench Lite和SWE-bench Verified上的实验显示，InfCode 持续优于强基线，达到SWE-bench Verified 79.4%的性能，建立新的最新水平。我们已将InfCode作为开源项目发布在该网址。 

---
# Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows 

**Title (ZH)**: 精准医学基因组 workflows 中的高效染色体并行化 

**Authors**: Daniel Mas Montserrat, Ray Verma, Míriam Barrabés, Francisco M. de la Vega, Carlos D. Bustamante, Alexander G. Ioannidis  

**Link**: [PDF](https://arxiv.org/pdf/2511.15977)  

**Abstract**: Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows. 

**Abstract (ZH)**: 大规模基因组工作流在精准医疗中的使用可以处理每个样本跨度为数十到数百吉字节的数据集，导致内存峰值升高、密集的磁盘I/O操作以及由于内存不足错误导致的任务失败。简单的静态资源分配方法难以处理每条染色体RAM需求的变异性，导致资源利用效率低下和较长的运行时间。在本文中，我们提出了多种机制以实现适应性的、RAM效率高的染色体级别生物信息学工作流的并行化。首先，我们开发了一种符号回归模型来估计给定任务的每条染色体内存消耗，并引入插值偏差以保守地最小化资源过度分配。其次，我们提出了一种动态调度器，该调度器使用多项式回归模型自适应地预测RAM使用情况，并将任务打包视为背包问题，以基于预测的内存需求最优化地批量作业。此外，我们提出了一种静态调度器，该调度器优化染色体处理顺序以最小化峰值内存占用，同时保持吞吐量。我们提出的方法在模拟和实际基因组管道上进行评估，提供了新的机制以减少内存溢出并平衡线程负载。我们因此实现了端到端执行速度的提升，展示了优化大规模基因组工作流的潜力。 

---
# A Primer on Quantum Machine Learning 

**Title (ZH)**: 量子机器学习入门 

**Authors**: Su Yeon Chang, M. Cerezo  

**Link**: [PDF](https://arxiv.org/pdf/2511.15969)  

**Abstract**: Quantum machine learning (QML) is a computational paradigm that seeks to apply quantum-mechanical resources to solve learning problems. As such, the goal of this framework is to leverage quantum processors to tackle optimization, supervised, unsupervised and reinforcement learning, and generative modeling-among other tasks-more efficiently than classical models. Here we offer a high level overview of QML, focusing on settings where the quantum device is the primary learning or data generating unit. We outline the field's tensions between practicality and guarantees, access models and speedups, and classical baselines and claimed quantum advantages-flagging where evidence is strong, where it is conditional or still lacking, and where open questions remain. By shedding light on these nuances and debates, we aim to provide a friendly map of the QML landscape so that the reader can judge when-and under what assumptions-quantum approaches may offer real benefits. 

**Abstract (ZH)**: 量子机器学习（QML）是一种计算范式，旨在利用量子力学资源解决学习问题。该框架的目标是利用量子处理器在优化、监督学习、无监督学习、强化学习和生成建模等任务上比经典模型更高效。本文提供了一个高层次的QML概述，重点关注量子设备为主要学习或数据生成单元的场景。本文概述了该领域在实用性和保证、接入模型和加速、以及经典基线和声称的量子优势之间的矛盾，指出哪些证据是充分的，哪些是有条件的或仍在缺乏的，以及哪些问题仍待解决。通过揭示这些细微差别和辩论，我们旨在为读者提供一张友好版的QML景观图，使其能够判断在何种情况下和在何种假设下，量子方法可能提供真正的益处。 

---
# Externally Validated Multi-Task Learning via Consistency Regularization Using Differentiable BI-RADS Features for Breast Ultrasound Tumor Segmentation 

**Title (ZH)**: 基于一致性正则化使用可微BI-RADS特征的外部验证多任务学习方法用于乳腺超声肿瘤分割 

**Authors**: Jingru Zhang, Saed Moradi, Ashirbani Saha  

**Link**: [PDF](https://arxiv.org/pdf/2511.15968)  

**Abstract**: Multi-task learning can suffer from destructive task interference, where jointly trained models underperform single-task baselines and limit generalization. To improve generalization performance in breast ultrasound-based tumor segmentation via multi-task learning, we propose a novel consistency regularization approach that mitigates destructive interference between segmentation and classification. The consistency regularization approach is composed of differentiable BI-RADS-inspired morphological features. We validated this approach by training all models on the BrEaST dataset (Poland) and evaluating them on three external datasets: UDIAT (Spain), BUSI (Egypt), and BUS-UCLM (Spain). Our comprehensive analysis demonstrates statistically significant (p<0.001) improvements in generalization for segmentation task of the proposed multi-task approach vs. the baseline one: UDIAT, BUSI, BUS-UCLM (Dice coefficient=0.81 vs 0.59, 0.66 vs 0.56, 0.69 vs 0.49, resp.). The proposed approach also achieves state-of-the-art segmentation performance under rigorous external validation on the UDIAT dataset. 

**Abstract (ZH)**: 基于乳腺超声的肿瘤分割多任务学习中的一致性正则化方法：减轻分割与分类之间的破坏性干扰以提高泛化性能 

---
# Self-supervised and Multi-fidelity Learning for Extended Predictive Soil Spectroscopy 

**Title (ZH)**: 自监督与多保真学习扩展预测土壤光谱学 

**Authors**: Luning Sun, José L. Safanelli, Jonathan Sanderman, Katerina Georgiou, Colby Brungard, Kanchan Grover, Bryan G. Hopkins, Shusen Liu, Timo Bremer  

**Link**: [PDF](https://arxiv.org/pdf/2511.15965)  

**Abstract**: We propose a self-supervised machine learning (SSML) framework for multi-fidelity learning and extended predictive soil spectroscopy based on latent space embeddings. A self-supervised representation was pretrained with the large MIR spectral library and the Variational Autoencoder algorithm to obtain a compressed latent space for generating spectral embeddings. At this stage, only unlabeled spectral data were used, allowing us to leverage the full spectral database and the availability of scan repeats for augmented training. We also leveraged and froze the trained MIR decoder for a spectrum conversion task by plugging it into a NIR encoder to learn the mapping between NIR and MIR spectra in an attempt to leverage the predictive capabilities contained in the large MIR library with a low cost portable NIR scanner. This was achieved by using a smaller subset of the KSSL library with paired NIR and MIR spectra. Downstream machine learning models were then trained to map between original spectra, predicted spectra, and latent space embeddings for nine soil properties. The performance of was evaluated independently of the KSSL training data using a gold-standard test set, along with regression goodness-of-fit metrics. Compared to baseline models, the proposed SSML and its embeddings yielded similar or better accuracy in all soil properties prediction tasks. Predictions derived from the spectrum conversion (NIR to MIR) task did not match the performance of the original MIR spectra but were similar or superior to predictive performance of NIR-only models, suggesting the unified spectral latent space can effectively leverage the larger and more diverse MIR dataset for prediction of soil properties not well represented in current NIR libraries. 

**Abstract (ZH)**: 我们提出了一种自监督机器学习（SSML）框架，用于多保真学习和扩展预测土壤光谱学研究，基于潜在空间嵌入。通过使用大型MIR光谱库和变分自编码器算法预训练自监督表示，获得压缩的潜在空间以生成光谱嵌入。在此阶段，仅使用无标签的光谱数据，从而可以充分利用整个光谱数据库以及扫描重复的可用性来增强训练。我们还通过将训练好的MIR解码器插入NIR编码器来进行光谱转换任务，从而冻结训练好的MIR解码器，尝试利用大型MIR库中的预测能力，使用低成本的便携式NIR扫描器。通过使用KSSL库中配对的NIR和MIR光谱子集来实现。然后，下游机器学习模型被训练以映射九种土壤属性的原始光谱、预测光谱和潜在空间嵌入。性能评估使用黄金标准测试集独立于KSSL训练数据进行，并通过回归拟合度量进行评估。与基线模型相比，所提出的SSML及其嵌入在所有土壤属性预测任务中均表现出相似或更好的准确性。来源于光谱转换（NIR到MIR）任务的预测结果未能达到原始MIR光谱的性能，但与仅使用NIR模型的预测性能相似或更优，表明统一的光谱潜在空间能够有效利用大型且更具多样性的MIR数据集来预测当前NIR库中未充分代表的土壤属性。 

---
# iLTM: Integrated Large Tabular Model 

**Title (ZH)**: 集成大型表格式模型 

**Authors**: David Bonet, Marçal Comajoan Cara, Alvaro Calafell, Daniel Mas Montserrat, Alexander G. Ioannidis  

**Link**: [PDF](https://arxiv.org/pdf/2511.15941)  

**Abstract**: Tabular data underpins decisions across science, industry, and public services. Despite rapid progress, advances in deep learning have not fully carried over to the tabular domain, where gradient-boosted decision trees (GBDTs) remain a default choice in practice. We present iLTM, an integrated Large Tabular Model that unifies tree-derived embeddings, dimensionality-agnostic representations, a meta-trained hypernetwork, multilayer perceptrons (MLPs), and retrieval within a single architecture. Pretrained on more than 1,800 heterogeneous classification datasets, iLTM achieves consistently superior performance across tabular classification and regression tasks, from small datasets to large and high-dimensional tasks. After light fine-tuning, the meta-trained hypernetwork transfers to regression targets, matching or surpassing strong baselines. Extensive experiments show that iLTM outperforms well-tuned GBDTs and leading deep tabular models while requiring less task-specific tuning. By bridging the gap between tree-based and neural methods, iLTM offers a new framework for tabular foundation models for robust, adaptable, and scalable tabular learning. 

**Abstract (ZH)**: 表格数据支撑着科学、工业和公共服务中的决策。尽管取得了快速进展，深度学习的进步尚未完全渗透到表格数据领域，在该领域，梯度提升决策树（GBDTs）仍然是一个默认选择。我们提出了一种集成大型表格模型iLTM，该模型统一了树衍生嵌入、维度无关表示、元训练超网络、多层感知机（MLPs）和检索，置于单一架构中。iLTM在超过1,800个异构分类数据集上进行预训练，实现了从小型数据集到大型和高维任务的分类和回归任务的持续优越性能。在轻量级微调后，元训练超网络可以转移到回归目标上，匹配或超越强基线模型。广泛的实验证明，iLTM在不需要大量任务特定调整的情况下，优于优化调参的GBDT和领先的大规模深度表格模型。通过弥合基于树和神经方法之间的差距，iLTM提供了一种新的框架，用于构建稳健、适应性强和可扩展的表格基础模型。 

---
# The Loss of Control Playbook: Degrees, Dynamics, and Preparedness 

**Title (ZH)**: 失控 playbook：程度、动态与准备度 

**Authors**: Charlotte Stix, Annika Hallensleben, Alejandro Ortega, Matteo Pistillo  

**Link**: [PDF](https://arxiv.org/pdf/2511.15846)  

**Abstract**: This research report addresses the absence of an actionable definition for Loss of Control (LoC) in AI systems by developing a novel taxonomy and preparedness framework. Despite increasing policy and research attention, existing LoC definitions vary significantly in scope and timeline, hindering effective LoC assessment and mitigation. To address this issue, we draw from an extensive literature review and propose a graded LoC taxonomy, based on the metrics of severity and persistence, that distinguishes between Deviation, Bounded LoC, and Strict LoC. We model pathways toward a societal state of vulnerability in which sufficiently advanced AI systems have acquired or could acquire the means to cause Bounded or Strict LoC once a catalyst, either misalignment or pure malfunction, materializes. We argue that this state becomes increasingly likely over time, absent strategic intervention, and propose a strategy to avoid reaching a state of vulnerability. Rather than focusing solely on intervening on AI capabilities and propensities potentially relevant for LoC or on preventing potential catalysts, we introduce a complementary framework that emphasizes three extrinsic factors: Deployment context, Affordances, and Permissions (the DAP framework). Compared to work on intrinsic factors and catalysts, this framework has the unfair advantage of being actionable today. Finally, we put forward a plan to maintain preparedness and prevent the occurrence of LoC outcomes should a state of societal vulnerability be reached, focusing on governance measures (threat modeling, deployment policies, emergency response) and technical controls (pre-deployment testing, control measures, monitoring) that could maintain a condition of perennial suspension. 

**Abstract (ZH)**: 本研究通过开发一种新的分类体系和准备框架，解决了人工智能系统中失去控制（LoC）操作定义缺失的问题。尽管政策和研究的关注日益增加，现有的LoC定义在范围和时间线上仍然存在显著差异，阻碍了有效的LoC评估和缓解。为此，我们借鉴了广泛的文献综述，并提出了一种基于严重性和持久性指标的分级LoC分类体系，区分了偏离、有界失去控制和严格失去控制三种类型。我们描绘了走向社会脆弱状态的路径，其中充分先进的AI系统一旦出现某种诱发因素（如不对齐或纯粹的失灵）就可能获得或能够获得导致有界或严格失去控制的能力。我们argue这种状态随着时间的推移而变得越来越可能，除非有战略干预，因此提出了一个避免达到脆弱状态的策略。我们不仅关注潜在相关于LoC的AI能力或倾向的干预，以及防止潜在诱发因素的做法，还引入了一种互补框架，强调三个方面外在因素：部署背景、便利条件和许可（DAP框架）。与专注于内在因素和诱发因素的工作相比，该框架今天具有可操作的优势。最后，我们提出了一项计划，以便在社会脆弱状态达到时保持准备状态并防止LoC结果的发生，强调管理措施（威胁建模、部署政策、应急响应）和技术控制（预部署测试、控制措施、监控），以维持一种持续的搁置状态。 

---
# TopoReformer: Mitigating Adversarial Attacks Using Topological Purification in OCR Models 

**Title (ZH)**: TopoReformer: 使用拓扑净化 mitigating adversarial attacks 在 OCR 模型中的应用 

**Authors**: Bhagyesh Kumar, A S Aravinthakashan, Akshat Satyanarayan, Ishaan Gakhar, Ujjwal Verma  

**Link**: [PDF](https://arxiv.org/pdf/2511.15807)  

**Abstract**: Adversarially perturbed images of text can cause sophisticated OCR systems to produce misleading or incorrect transcriptions from seemingly invisible changes to humans. Some of these perturbations even survive physical capture, posing security risks to high-stakes applications such as document processing, license plate recognition, and automated compliance systems. Existing defenses, such as adversarial training, input preprocessing, or post-recognition correction, are often model-specific, computationally expensive, and affect performance on unperturbed inputs while remaining vulnerable to unseen or adaptive attacks. To address these challenges, TopoReformer is introduced, a model-agnostic reformation pipeline that mitigates adversarial perturbations while preserving the structural integrity of text images. Topology studies properties of shapes and spaces that remain unchanged under continuous deformations, focusing on global structures such as connectivity, holes, and loops rather than exact distance. Leveraging these topological features, TopoReformer employs a topological autoencoder to enforce manifold-level consistency in latent space and improve robustness without explicit gradient regularization. The proposed method is benchmarked on EMNIST, MNIST, against standard adversarial attacks (FGSM, PGD, Carlini-Wagner), adaptive attacks (EOT, BDPA), and an OCR-specific watermark attack (FAWA). 

**Abstract (ZH)**: adversarially perturbed文本图像可以导致复杂的OCR系统从看似无变化的修改中产生误导性或不正确的 transcription，对高风险应用如文档处理、车牌识别和自动化合规系统构成安全威胁。现有防御措施如对抗训练、输入预处理或后识别校正往往是模型特定的，计算成本高，并会影响未受扰动输入的性能，在面对未知或适应性攻击时仍然脆弱。为应对这些挑战，引入了模型无关的TopoReformer重塑管道，该管道在保留文本图像结构完整性的前提下减轻对抗扰动。拓扑学研究在连续变形下不变的形状和空间属性，重点关注连通性、孔洞和环路等全局结构，而非精确的距离。利用这些拓扑特征，TopoReformer采用拓扑自编码器在潜在空间中强制实施流形级别的一致性，同时提高鲁棒性，无需显式梯度正则化。所提出的方法已在EMNIST、MNIST数据集上，针对标准对抗性攻击（FGSM、PGD、Carlini-Wagner）、适应性攻击（EOT、BDPA）以及专门针对OCR的水印攻击（FAWA）进行了基准测试。 

---
# Writing With Machines and Peers: Designing for Critical Engagement with Generative AI 

**Title (ZH)**: 机器与同伴共写的写作：设计促进生成式AI的批判性参与 

**Authors**: Xinran Zhu, Cong Wang, Duane Searsmith  

**Link**: [PDF](https://arxiv.org/pdf/2511.15750)  

**Abstract**: The growing integration of generative AI in higher education is transforming how students write, learn, and engage with knowledge. As AI tools become more integrated into classrooms, there is an urgent need for pedagogical approaches that help students use them critically and reflectively. This study proposes a pedagogical design that integrates AI and peer feedback in a graduate-level academic writing activity. Over eight weeks, students developed literature review projects through multiple writing and revision stages, receiving feedback from both a custom-built AI reviewer and human peers. We examine two questions: (1) How did students interact with and incorporate AI and peer feedback during the writing process? and (2) How did they reflect on and build relationships with both human and AI reviewers? Data sources include student writing artifacts, AI and peer feedback, AI chat logs, and student reflections. Findings show that students engaged differently with each feedback source-relying on AI for rubric alignment and surface-level edits, and on peer feedback for conceptual development and disciplinary relevance. Reflections revealed evolving relationships with AI, characterized by increasing confidence, strategic use, and critical awareness of its limitations. The pedagogical design supported writing development, AI literacy, and disciplinary understanding. This study offers a scalable pedagogical model for integrating AI into writing instruction and contributes insights for system-level approaches to fostering meaningful human-AI collaboration in higher education. 

**Abstract (ZH)**: 生成性人工智能在高等教育中的日益整合正在变革学生写作、学习和获取知识的方式。随着AI工具在课堂中的整合，迫切需要帮助学生批判性和反思性地使用这些工具的教育方法。本研究提出了一种将AI与同伴反馈整合到研究生级学术写作活动中的教学设计。在八周内，学生通过多个写作和修订阶段开发文献综述项目，同时从自定义构建的AI审阅器和人类同伴处获得反馈。本研究探讨了两个问题：（1）学生在写作过程中是如何与AI和同伴反馈互动并融入反馈的？（2）他们是如何反思并建立与人类和AI审阅者的关系的？数据来源包括学生写作成果、AI和同伴反馈、AI聊天日志以及学生反思。研究发现，学生对每种反馈来源的使用方式不同——依赖AI进行评分标准对齐和表面编辑，而依赖同伴反馈进行概念发展和学科相关性分析。反思揭示了学生与AI关系的发展变化，表现为信任度增加、策略性使用和对其局限性的批判意识增强。该教学设计支持了写作能力的提升、AI素养以及学科理解。本研究提供了一种可扩展的教学模型，用于将AI整合到写作教学中，并为在高等教育中促进有意义的人工智能合作提供了系统层面的见解。 

---
# Extending Test-Time Scaling: A 3D Perspective with Context, Batch, and Turn 

**Title (ZH)**: 扩展测试时缩放：基于上下文、批量和轮次的三维视角 

**Authors**: Chao Yu, Qixin Tan, Jiaxuan Gao, Shi Yu, Hong Lu, Xinting Yang, Zelai Xu, Yu Wang, Yi Wu, Eugene Vinitsky  

**Link**: [PDF](https://arxiv.org/pdf/2511.15738)  

**Abstract**: Reasoning reinforcement learning (RL) has recently revealed a new scaling effect: test-time scaling. Thinking models such as R1 and o1 improve their reasoning accuracy at test time as the length of the reasoning context increases. However, compared with training-time scaling, test-time scaling is fundamentally limited by the limited context length of base models, which remains orders of magnitude smaller than the amount of tokens consumed during training. We revisit test-time enhancement techniques through the lens of scaling effect and introduce a unified framework of multi-dimensional test-time scaling to extend the capacity of test-time reasoning. Beyond conventional context-length scaling, we consider two additional dimensions: batch scaling, where accuracy improves with parallel sampling, and turn scaling, where iterative self-refinement enhances reasoning quality. Building on this perspective, we propose 3D test-time scaling, which integrates context, batch, and turn scaling. We show that: (1) each dimension demonstrates a test-time scaling effect, but with a bounded capacity; (2) combining all three dimensions substantially improves the reasoning performance of challenging testbeds, including IOI, IMO, and CPHO, and further benefits from human preference feedback; and (3) the human-in-the-loop framework naturally extends to a more open-ended domain, i.e., embodied learning, which enables the design of humanoid control behaviors. 

**Abstract (ZH)**: 测试时多维强化学习缩放效应 

---
# Just Asking Questions: Doing Our Own Research on Conspiratorial Ideation by Generative AI Chatbots 

**Title (ZH)**: Just Asking Questions: Conducting Our Own Research on Conspiratorial Ideation by Generative AI Chatbots 

**Authors**: Katherine M. FitzGerald, Michelle Riedlinger, Axel Bruns, Stephen Harrington, Timothy Graham, Daniel Angus  

**Link**: [PDF](https://arxiv.org/pdf/2511.15732)  

**Abstract**: Interactive chat systems that build on artificial intelligence frameworks are increasingly ubiquitous and embedded into search engines, Web browsers, and operating systems, or are available on websites and apps. Researcher efforts have sought to understand the limitations and potential for harm of generative AI, which we contribute to here. Conducting a systematic review of six AI-powered chat systems (ChatGPT 3.5; ChatGPT 4 Mini; Microsoft Copilot in Bing; Google Search AI; Perplexity; and Grok in Twitter/X), this study examines how these leading products respond to questions related to conspiracy theories. This follows the platform policy implementation audit approach established by Glazunova et al. (2023). We select five well-known and comprehensively debunked conspiracy theories and four emerging conspiracy theories that relate to breaking news events at the time of data collection. Our findings demonstrate that the extent of safety guardrails against conspiratorial ideation in generative AI chatbots differs markedly, depending on chatbot model and conspiracy theory. Our observations indicate that safety guardrails in AI chatbots are often very selectively designed: generative AI companies appear to focus especially on ensuring that their products are not seen to be racist; they also appear to pay particular attention to conspiracy theories that address topics of substantial national trauma such as 9/11 or relate to well-established political issues. Future work should include an ongoing effort extended to further platforms, multiple languages, and a range of conspiracy theories extending well beyond the United States. 

**Abstract (ZH)**: 基于人工智能框架的交互聊天系统日益普遍并嵌入到搜索引擎、网络浏览器和操作系统中，或可在网站和应用程序上获得。研究人员致力于理解生成式AI的局限性和潜在风险，本文在此贡献。通过对六种AI驱动的聊天系统（ChatGPT 3.5；ChatGPT 4 Mini；微软Bing中的Copilot；Google Search AI；Perplexity；以及Twitter/X中的Grok）进行系统性回顾，本文研究了这些领先产品如何回应有关阴谋理论的问题。本研究遵循Glazunova等人（2023）建立的平台政策实施审计方法。我们在数据收集时选择了五个广为人知且已被彻底批驳的阴谋理论和四个与当时突发事件相关的新兴阴谋理论。研究发现表明，生成式AI聊天机器人的安全护栏对阴谋论的防范程度因聊天机器人模型和阴谋理论的不同而异。观察结果表明，AI聊天机器人的安全护栏往往设计得很具选择性：生成式AI公司似乎特别注重确保其产品不被视为有种族歧视；他们还特别关注涉及重大国家创伤主题（如9·11）或与已建立的政治问题相关联的阴谋理论。未来的工作应包括对更多平台、多种语言以及范围更广泛的阴谋理论的持续努力。 

---
# The Future of Food: How Artificial Intelligence is Transforming Food Manufacturing 

**Title (ZH)**: 未来食物：人工智能如何转型食品制造 

**Authors**: Xu Zhou, Ivor Prado, AIFPDS participants, Ilias Tagkopoulos  

**Link**: [PDF](https://arxiv.org/pdf/2511.15728)  

**Abstract**: Artificial intelligence is accelerating a new era of food innovation, connecting data from farm to consumer to improve formulation, processing, and health outcomes. Recent advances in deep learning, natural language processing, and multi-omics integration make it possible to understand and optimize food systems with unprecedented depth. However, AI adoption across the food sector remains uneven due to heterogeneous datasets, limited model and system interoperability, and a persistent skills gap between data scientists and food domain experts. To address these challenges and advance responsible innovation, the AI Institute for Next Generation Food Systems (AIFS) convened the inaugural AI for Food Product Development Symposium at University of California, Davis, in October 2025. This white paper synthesizes insights from the symposium, organized around five domains where AI can have the greatest near-term impact: supply chain; formulation and processing; consumer insights and sensory prediction; nutrition and health; and education and workforce development. Across the areas, participants emphasized the importance of interoperable data standards, transparent and interpretable models, and cross-sector collaboration to accelerate the translation of AI research into practice. The discussions further highlighted the need for robust digital infrastructure, privacy-preserving data-sharing mechanisms, and interdisciplinary training pathways that integrate AI literacy with domain expertise. Collectively, the priorities outline a roadmap for integrating AI into food manufacturing in ways that enhance innovation, sustainability, and human well-being while ensuring that technological progress remains grounded in ethics, scientific rigor, and societal benefit. 

**Abstract (ZH)**: 人工智能正在加速食品创新的新时代，从农田到消费者的各个环节连接数据以改进配方、加工和健康结果。近期深度学习、自然语言处理和多组学整合的进展使得以前所未有的深度理解并优化食品系统成为可能。然而，食品产业中的人工智能采用依然参差不齐，原因包括异构数据集、模型和系统互操作性有限以及数据科学家与食品领域专家之间的技能差距持续存在。为解决这些挑战并促进负责任的创新，人工智能下一代食品系统研究所（AIFS）在2025年10月于加州大学戴维斯分校召开了首届食品产品开发人工智能研讨会。本白皮书整合了研讨会的见解，围绕五个可以带来最大短期影响的领域组织：供应链、配方和加工、消费者洞察与感官预测、营养与健康、以及教育培训与发展。在各个领域，参与者强调了互操作数据标准、透明和可解释的模型以及跨部门合作的重要性，以加速将人工智能研究转化为实践。讨论还强调了强大的数字基础设施、隐私保护的数据共享机制以及将人工智能素养与领域专长相融合的跨学科培训路径的需求。总的来说，优先事项概述了将人工智能整合到食品制造过程中的路线图，旨在通过增强创新、可持续性和人类福祉，同时确保技术进步根植于伦理、科学严谨性和社会利益。 

---
