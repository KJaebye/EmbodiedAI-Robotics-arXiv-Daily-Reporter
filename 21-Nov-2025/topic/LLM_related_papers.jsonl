{'arxiv_id': 'arXiv:2511.16660', 'title': 'Cognitive Foundations for Reasoning and Their Manifestation in LLMs', 'authors': 'Priyanka Kargupta, Shuyue Stella Li, Haocheng Wang, Jinu Lee, Shan Chen, Orevaoghene Ahia, Dean Light, Thomas L. Griffiths, Max Kleiman-Weiner, Jiawei Han, Asli Celikyilmaz, Yulia Tsvetkov', 'link': 'https://arxiv.org/abs/2511.16660', 'abstract': 'Large language models solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. We synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning computational constraints, meta-cognitive controls, knowledge representations, and transformation operations, then analyze their behavioral manifestations in reasoning traces. We propose a fine-grained cognitive evaluation framework and conduct the first large-scale analysis of 170K traces from 17 models across text, vision, and audio modalities, alongside 54 human think-aloud traces, which we make publicly available. Our analysis reveals systematic structural differences: humans employ hierarchical nesting and meta-cognitive monitoring while models rely on shallow forward chaining, with divergence most pronounced on ill-structured problems. Meta-analysis of 1,598 LLM reasoning papers reveals the research community concentrates on easily quantifiable behaviors (sequential organization: 55%, decomposition: 60%) while neglecting meta-cognitive controls (self-awareness: 16%, evaluation: 8%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 60% on complex problems. By bridging cognitive science and LLM research, we establish a foundation for developing models that reason through principled cognitive mechanisms rather than brittle spurious reasoning shortcuts or memorization, opening new directions for both improving model capabilities and testing theories of human cognition at scale.', 'abstract_zh': '大型语言模型能够解决复杂问题但在简单变体上失败，这表明它们通过与人类推理本质上不同机制实现正确输出。我们综合认知科学研究，构建涵盖计算限制、元认知控制、知识表示和变换操作的28个认知元素分类体系，然后在推理轨迹中分析其行为表现。我们提出了一种精细的认知评估框架，并对来自17种模型、涵盖文本、视觉和音频模态的17万个推理轨迹进行了首次大规模分析，同时还包括了54个手动思考 aloud 的轨迹，我们已将这些数据公开。我们的分析揭示了系统性的结构差异：人类运用层次嵌套和元认知监控，而模型依赖浅层向前链式推理，差异在构架不良的问题上最为明显。对1598篇大规模语言模型推理论文的元分析显示，研究界集中在易于量化的行为（顺序组织：55%，分解：60%）上，而忽视与成功相关的元认知控制（自我意识：16%，评价：8%）。模型拥有与成功相关的行为 repertoire，但无法自发使用它们。利用这些模式，我们开发了一种推理指导，在测试时自动搭建成功的结构，复杂问题上的性能提升高达60%。通过连接认知科学与大型语言模型研究，我们为通过原理性的认知机制开发能进行推理的模型奠定了基础，而不是脆弱的虚假推理捷径或记忆，为提高模型能力并大规模测试人类认知理论提供了新方向。', 'title_zh': '认知基础推理及其在大规模语言模型中的表现'}
{'arxiv_id': 'arXiv:2511.16600', 'title': 'You Only Forward Once: An Efficient Compositional Judging Paradigm', 'authors': 'Tianlong Zhang, Hongwei Xue, Shilin Yan, Di Wu, Chen Xu, Yunyun Yang', 'link': 'https://arxiv.org/abs/2511.16600', 'abstract': 'Multimodal large language models (MLLMs) show strong potential as judges. However, existing approaches face a fundamental trade-off: adapting MLLMs to output a single score misaligns with the generative nature of MLLMs and limits fine-grained requirement understanding, whereas autoregressively generating judging analyses is prohibitively slow in high-throughput settings. Observing that judgment reduces to verifying whether inputs satisfy a set of structured requirements, we propose YOFO, a template-conditioned method that judges all requirements in a single forward pass. Built on an autoregressive model, YOFO accepts a structured requirement template and, in one inference step, produces a binary yes/no decision for each requirement by reading the logits of the final token associated with that requirement. This design yields orders-of-magnitude speedups while preserving interpretability. Extensive experiments show that YOFO not only achieves state-of-the-art results on standard recommendation datasets, but also supports dependency-aware analysis-where subsequent judgments are conditioned on previous ones-and further benefits from post-hoc CoT.', 'abstract_zh': '多模态大型语言模型（MLLMs）作为裁判显示出强烈的潜力。然而，现有的方法面临一个基本的权衡：使MLLMs适应输出单一分数与MLLMs的生成性质相冲突，并限制了细粒度的要求理解，而自回归生成裁判分析在高通量设置中是不可行的。观察到裁判归结为验证输入是否满足一组结构化要求，我们提出YOFO，这是一种模板条件化方法，可以在单次前向传递中评判所有要求。基于自回归模型，YOFO接受一个结构化要求模板，并在一次推理步骤中通过阅读与该要求相关的最终词元的logits生成每个要求的二元是/否决策。这种设计在保持可解释性的同时实现了数量级的速度提升。广泛实验表明，YOFO不仅在标准推荐数据集上达到了最先进的性能，还能支持依赖意识的分析——后续判断依赖于先前的判断——并且进一步受益于事后解释性推理（CoT）。', 'title_zh': '你只需前馈一次：一个高效的组合判断范式'}
{'arxiv_id': 'arXiv:2511.16548', 'title': 'Utilizing Large Language Models for Zero-Shot Medical Ontology Extension from Clinical Notes', 'authors': 'Guanchen Wu, Yuzhang Xie, Huanwei Wu, Zhe He, Hui Shao, Xiao Hu, Carl Yang', 'link': 'https://arxiv.org/abs/2511.16548', 'abstract': 'Integrating novel medical concepts and relationships into existing ontologies can significantly enhance their coverage and utility for both biomedical research and clinical applications. Clinical notes, as unstructured documents rich with detailed patient observations, offer valuable context-specific insights and represent a promising yet underutilized source for ontology extension. Despite this potential, directly leveraging clinical notes for ontology extension remains largely unexplored. To address this gap, we propose CLOZE, a novel framework that uses large language models (LLMs) to automatically extract medical entities from clinical notes and integrate them into hierarchical medical ontologies. By capitalizing on the strong language understanding and extensive biomedical knowledge of pre-trained LLMs, CLOZE effectively identifies disease-related concepts and captures complex hierarchical relationships. The zero-shot framework requires no additional training or labeled data, making it a cost-efficient solution. Furthermore, CLOZE ensures patient privacy through automated removal of protected health information (PHI). Experimental results demonstrate that CLOZE provides an accurate, scalable, and privacy-preserving ontology extension framework, with strong potential to support a wide range of downstream applications in biomedical research and clinical informatics.', 'abstract_zh': '将新颖的医学概念和关系集成到现有本体中，可以显著增强其在生物医药研究和临床应用中的覆盖范围和实用性。临床笔记作为一种富含详细患者观察信息的非结构化文档，提供了有价值的具体背景见解，并代表了一种有潜力但尚未充分利用的本体扩展来源。尽管存在这种潜力，但直接利用临床笔记进行本体扩展的研究仍处于起步阶段。为解决这一缺口，我们提出了CLOZE，一个新颖的框架，利用大规模语言模型（LLMs）自动从临床笔记中提取医学实体并将其集成到医学本体中。通过充分利用预训练LLMs强大的语言理解和广泛生物医药知识，CLOZE有效地识别疾病相关的概念并捕获复杂的层级关系。零样本框架不需要额外的训练或标注数据，使其成为一种成本效益高的解决方案。此外，CLOZE通过自动化去除受保护的健康信息（PHI）来确保患者隐私。实验结果表明，CLOZE提供了一种准确、可扩展且保护隐私的本体扩展框架，具有强大的潜力支持生物医药研究和临床信息技术等一系列下游应用。', 'title_zh': '利用大规模语言模型实现从临床笔记零样本扩展医疗本体的研究'}
{'arxiv_id': 'arXiv:2511.16395', 'title': 'CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference', 'authors': 'Kangwei Xu, Grace Li Zhang, Ulf Schlichtmann, Bing Li', 'link': 'https://arxiv.org/abs/2511.16395', 'abstract': "Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs). However, their inherent tendency toward hallucination often introduces functional errors into the generated HDL designs. To address this issue, we propose the framework CorrectHDL that leverages high-level synthesis (HLS) results as functional references to correct potential errors in LLM-generated HDL this http URL input to the proposed framework is a C/C++ program that specifies the target circuit's functionality. The program is provided to an LLM to directly generate an HDL design, whose syntax errors are repaired using a Retrieval-Augmented Generation (RAG) mechanism. The functional correctness of the LLM-generated circuit is iteratively improved by comparing its simulated behavior with an HLS reference design produced by conventional HLS tools, which ensures the functional correctness of the result but can lead to suboptimal area and power efficiency. Experimental results demonstrate that circuits generated by the proposed framework achieve significantly better area and power efficiency than conventional HLS designs and approach the quality of human-engineered circuits. Meanwhile, the correctness of the resulting HDL implementation is maintained, highlighting the effectiveness and potential of agentic HDL design leveraging the generative capabilities of LLMs and the rigor of traditional correctness-driven IC design flows.", 'abstract_zh': 'Large Language Models (LLMs)在硬件前端设计中的潜力已经得到彰显，特别是在硬件描述语言（HDLs）领域。然而，LLMs固有的虚幻倾向往往会引入功能错误到生成的HDL设计中。为了解决这一问题，我们提出了一种名为CorrectHDL的框架，利用高层次综合（HLS）结果作为功能参考来纠正LLM生成的HDL中的潜在错误。该框架的输入是一个C/C++程序，该程序指定了目标电路的功能。该程序提供给LLM直接生成HDL设计，通过检索增强生成（RAG）机制修复其语法错误。通过将生成的电路的模拟行为与由传统HLS工具产生的HLS参考设计进行比较，逐步改进生成电路的功能正确性，尽管这可能导致面积和能效不足。实验结果表明，由该框架生成的电路在面积和能效方面显著优于传统的HLS设计，并接近于手工设计电路的质量。同时，保持了生成HDL实现的正确性，突显了利用LLMs生成能力与传统正确性驱动的IC设计流程严谨性相结合的代理HDL设计的有效性和潜力。', 'title_zh': 'CorrectHDL: 以高层次综合为参考的由自主agents编写的HDL设计'}
{'arxiv_id': 'arXiv:2511.16383', 'title': 'An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models', 'authors': 'Alexander Zadorojniy, Segev Wasserkrug, Eitan Farchi', 'link': 'https://arxiv.org/abs/2511.16383', 'abstract': 'Recently, using Large Language Models (LLMs) to generate optimization models from natural language descriptions has became increasingly popular. However, a major open question is how to validate that the generated models are correct and satisfy the requirements defined in the natural language description. In this work, we propose a novel agent-based method for automatic validation of optimization models that builds upon and extends methods from software testing to address optimization modeling . This method consists of several agents that initially generate a problem-level testing API, then generate tests utilizing this API, and, lastly, generate mutations specific to the optimization model (a well-known software testing technique assessing the fault detection power of the test suite). In this work, we detail this validation framework and show, through experiments, the high quality of validation provided by this agent ensemble in terms of the well-known software testing measure called mutation coverage.', 'abstract_zh': '最近，使用大型语言模型（LLMs）从自然语言描述生成优化模型的做法越来越流行。然而，一个主要的开放问题是如何验证生成的模型是否正确且满足自然语言描述中定义的要求。在本文中，我们提出了一种基于软件测试方法的新型代理方法，用于自动验证优化模型，该方法扩展了软件测试方法以应对优化建模需求。该方法包括多个代理，首先生成一个问题级别测试API，然后利用此API生成测试，最后生成针对优化模型的具体变异（一种公认的方法，用于评估测试套件的故障检测能力）。在本文中，我们详细介绍了这一验证框架，并通过实验展示了该代理集合在公认的软件测试度量标准——变异覆盖率方面的高质量验证结果。', 'title_zh': '基于代理的数学优化模型自动验证框架'}
{'arxiv_id': 'arXiv:2511.16216', 'title': 'FlipVQA-Miner: Cross-Page Visual Question-Answer Mining from Textbooks', 'authors': 'Zhen Hao Wong, Jingwen Deng, Hao Liang, Runming He, Chengyu Shen, Wentao Zhang', 'link': 'https://arxiv.org/abs/2511.16216', 'abstract': 'The development of Large Language Models (LLMs) increasingly depends on high-quality supervised data, yet existing instruction-tuning and RL datasets remain costly to curate and often rely on synthetic samples that introduce hallucination and limited diversity. At the same time, textbooks and exercise materials contain abundant, high-quality human-authored Question-Answer(QA) content that remains underexploited due to the difficulty of transforming raw PDFs into AI-ready supervision. Although modern OCR and vision-language models can accurately parse document structure, their outputs lack the semantic alignment required for training. We propose an automated pipeline that extracts well-formed QA and visual-QA (VQA) pairs from educational documents by combining layout-aware OCR with LLM-based semantic parsing. Experiments across diverse document types show that the method produces accurate, aligned, and low-noise QA/VQA pairs. This approach enables scalable use of real-world educational content and provides a practical alternative to synthetic data generation for improving reasoning-oriented LLM training. All code and data-processing pipelines are open-sourced at this https URL.', 'abstract_zh': '大型语言模型（LLMs）的发展越来越依赖高质量的监督数据，然而现有的指令调优和强化学习数据集在维护上依然成本高昂，并且常常依赖合成样本，这会导致幻觉和多样性有限的问题。同时，教科书和练习材料中包含大量高质量的人工撰写的问题-答案（QA）内容，但由于将原始PDF转换为AI可用的监督数据的难度较大，这些内容仍然没有得到充分开发利用。尽管现代OCR和跨模态模型能够准确解析文档结构，但其输出缺乏用于训练所需的语义对齐。我们提出了一种自动化流水线，通过结合布局感知的OCR与基于LLM的语义解析，从教育文档中提取良好的问题-答案（QA）和视觉-问题-答案（VQA）对。实验表明，该方法生成的QA/VQA对准确、对齐且噪声较低。此方法使得实际教育内容的大规模利用成为可能，并提供了一种改进以推理为导向的LLM训练的实用替代方案，合成数据生成。所有代码和数据处理流水线均已开源。', 'title_zh': '翻转VQA矿工：从教科书中跨页视觉问答挖掘'}
{'arxiv_id': 'arXiv:2511.16139', 'title': 'Multidimensional Rubric-oriented Reward Model Learning via Geometric Projection Reference Constraints', 'authors': 'Yongnan Jin, Xurui Li, Feng Cao, Liucun Gao, Juanjuan Yao', 'link': 'https://arxiv.org/abs/2511.16139', 'abstract': 'The integration of large language models (LLMs) into medical practice holds transformative potential, yet their real-world clinical utility remains limited by critical alignment challenges: (1) a disconnect between static evaluation benchmarks and dynamic clinical cognitive needs, (2) difficulties in adapting to evolving, multi-source medical standards, and (3) the inability of conventional reward models to capture nuanced, multi-dimensional medical quality criteria. To address these gaps, we propose MR-RML (Multidimensional Rubric-oriented Reward Model Learning) via GPRC (Geometric Projection Reference Constraints), a novel alignment framework that integrates medical standards into a structured "Dimensions-Scenarios-Disciplines" matrix to guide data generation and model optimization. MR-RML introduces three core innovations: (1) a "Dimensions-Scenarios-Disciplines" medical standard system that embeds domain standards into the full training pipeline; (2) an independent multi-dimensional reward model that decomposes evaluation criteria, shifting from real-time rubric-based scoring to internalized reward modeling for improved consistency and cost-efficiency; (3) geometric projection reference constraints that transform medical cognitive logic into mathematical regularization, aligning scoring gradients with clinical reasoning and enabling synthetic data-driven training. Through extensive evaluations on the authoritative medical benchmark Healthbench, our method yields substantial performance gains over the base LLM Qwen-32B (45% on the full subset and 85% on Hard subset, respectively). It achieves a SOTA among open-source LLMs with scores of 62.7 (full subset) and 44.7 (hard subset), while also outperforming the majority of closed-source models.', 'abstract_zh': '大型语言模型（LLMs）在医疗实践中的集成具有变革潜力，但其实用性受限于关键的对齐挑战：（1）静态评估标准与动态临床认知需求之间的脱节，（2）难以适应不断演变的多源医疗标准，（3）传统奖励模型无法捕捉复杂的多维度医疗质量标准。为了解决这些问题，我们提出了一种名为MR-RML（基于多维度标准导向奖励模型学习）的新颖对齐框架，该框架通过GPRC（几何投影参考约束）将医疗标准整合到一个结构化的“维度-场景-学科”矩阵中，以指导数据生成和模型优化。MR-RML引入了三项核心创新：（1）一个“维度-场景-学科”医疗标准系统，将领域标准嵌入到完整的训练管道中；（2）一个独立的多维度奖励模型，将评估标准分解，从实时标准评分转向内部化奖励建模，提高一致性和成本效益；（3）几何投影参考约束，将医疗认知逻辑转换为数学正则化，使评分梯度与临床推理对齐，并实现基于合成数据的训练。通过在权威医疗基准Healthbench上的广泛评估，该方法在基模型Qwen-32B的基础上取得了显著的表现提升（全子集45%，硬子集85%）。在开源模型中，其得分为62.7（全子集）和44.7（硬子集），并优于大多数封闭源模型。', 'title_zh': '基于几何投影参考约束的多维度评标导向奖励模型学习'}
{'arxiv_id': 'arXiv:2511.16108', 'title': 'SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent', 'authors': 'Shiyi Cao, Dacheng Li, Fangzhou Zhao, Shuo Yuan, Sumanth R. Hegde, Connor Chen, Charlie Ruan, Tyler Griggs, Shu Liu, Eric Tang, Richard Liaw, Philipp Moritz, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica', 'link': 'https://arxiv.org/abs/2511.16108', 'abstract': "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. It provides efficient asynchronous dispatching, lightweight tool integration, and flexible backend interoperability, enabling seamless use with existing RL frameworks such as SkyRL-train, VeRL, and Tinker.\nUsing SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Together, these optimizations enable SA-SWE-32B to reach 39.4% Pass@1 on SWE-Bench Verified with more than 2x cost reduction compared to prior models reaching similar performance. Despite being trained solely on SWE tasks, SA-SWE-32B generalizes effectively to other agentic tasks, including Terminal-Bench, BrowseComp-Plus, and WebArena. We further demonstrate SkyRL-Agent's extensibility through case studies on deep research, computer use, and memory agents, each trained using a different training backend.", 'abstract_zh': 'SkyRL-Agent：一种高效的多轮长时间代理训练与评估框架', 'title_zh': 'SkyRL-Agent: 高效的多轮对话大语言模型代理的RL训练方法'}
{'arxiv_id': 'arXiv:2511.15994', 'title': 'CARE-RAG - Clinical Assessment and Reasoning in RAG', 'authors': 'Deepthi Potluri, Aby Mammen Mathew, Jeffrey B DeWitt, Alexander L. Rasgon, Yide Hao, Junyuan Hong, Ying Ding', 'link': 'https://arxiv.org/abs/2511.15994', 'abstract': 'Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions, we find that errors persist even when authoritative passages are provided. To address this, we propose an evaluation framework that measures accuracy, consistency, and fidelity of reasoning. Our results highlight both the potential and the risks: retrieval-augmented generation (RAG) can constrain outputs, but safe deployment requires assessing reasoning as rigorously as retrieval.', 'abstract_zh': '获取正确的证据并不保证大语言模型（LLMs）能够正确地进行推理。这种从检索到推理之间的差距，在临床环境中尤为令人担忧，因为在这些环境中，输出必须与结构化协议保持一致。我们使用书写暴露疗法（WET）指南作为试验平台，研究这一差距。在评估模型对经过临床专家筛选的问题作出的回答时，即使提供了权威的段落，我们也发现错误仍然存在。为此，我们提出了一种评估框架，用于衡量推理的准确性、一致性和忠实性。我们的结果既突显了检索增强生成（RAG）的潜力，也指出了其风险：RAG 可以限制输出，但安全部署需要像评估检索一样严格地评估推理。', 'title_zh': 'CARE-RAG - 临床评估与推理在RAG中的应用'}
{'arxiv_id': 'arXiv:2511.15992', 'title': 'Detecting Sleeper Agents in Large Language Models via Semantic Drift Analysis', 'authors': 'Shahin Zanbaghi, Ryan Rostampour, Farhan Abid, Salim Al Jarmakani', 'link': 'https://arxiv.org/abs/2511.15992', 'abstract': 'Large Language Models (LLMs) can be backdoored to exhibit malicious behavior under specific deployment conditions while appearing safe during training a phenomenon known as "sleeper agents." Recent work by Hubinger et al. demonstrated that these backdoors persist through safety training, yet no practical detection methods exist. We present a novel dual-method detection system combining semantic drift analysis with canary baseline comparison to identify backdoored LLMs in real-time. Our approach uses Sentence-BERT embeddings to measure semantic deviation from safe baselines, complemented by injected canary questions that monitor response consistency. Evaluated on the official Cadenza-Labs dolphin-llama3-8B sleeper agent model, our system achieves 92.5% accuracy with 100% precision (zero false positives) and 85% recall. The combined detection method operates in real-time (<1s per query), requires no model modification, and provides the first practical solution to LLM backdoor detection. Our work addresses a critical security gap in AI deployment and demonstrates that embedding-based detection can effectively identify deceptive model behavior without sacrificing deployment efficiency.', 'abstract_zh': '大型语言模型可以通过回门手段在特定部署条件下表现出恶意行为而在训练期间看似安全，这种现象被称为“ sleeper agents”。我们提出了一种结合语义漂移分析和金丝雀基线比较的新型双方法检测系统，用于实时检测被回门的大型语言模型。我们的方法使用Sentence-BERT嵌入度量与安全基线的语义偏差，并通过注入的金丝雀问题来监控响应一致性。在官方Cadenza-Labs dolphin-llama3-8B sleeper agent模型上评估，我们的系统实现了92.5%的准确率、100%的精确率（零假阳性）和85%的召回率。结合的检测方法实时运行（每个查询<1秒），不需要对模型进行修改，并提供第一个实用的大规模语言模型回门检测解决方案。我们的工作填补了人工智能部署中的一个重要安全缺口，并证明了基于嵌入的检测可以有效地识别欺骗性模型行为而不牺牲部署效率。', 'title_zh': '通过语义漂移分析检测大型语言模型中的潜入Agent'}
{'arxiv_id': 'arXiv:2511.15974', 'title': 'KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy', 'authors': 'Zhe Li, Yehan Qiu, Yujie Chen, Xiang Zhou', 'link': 'https://arxiv.org/abs/2511.15974', 'abstract': "Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles, host factors, pharmacological properties of antimicrobials, and the severity of this http URL complexity imposes fundamental limitations on the applicability of Large Language Models (LLMs) in high-stakes clinical decision-making including knowledge gaps, data privacy concerns, high deployment costs, and limited reasoning capabilities. To address these challenges, we propose KRAL (Knowledge and Reasoning Augmented Learning), a low-cost, scalable, privacy-preserving paradigm that leverages teacher-model reasoning to automatically distill knowledge and reasoning trajectories via answer-to-question reverse generation, employs heuristic learning for semi-supervised data augmentation (reducing manual annotation requirements by approximately 80%), and utilizes agentic reinforcement learning to jointly enhance medical knowledge and reasoning while optimizing computational and memory efficiency. A hierarchical evaluation employing diverse teacher-model proxies reduces assessment costs, while modular interface design facilitates seamless system updates. Experimental results demonstrate that KRAL significantly outperforms traditional Retrieval-Augmented Generation (RAG) and Supervised Fine-Tuning (SFT) methods. It improves knowledge question-answering capability (Accuracy@1 on the external open-source benchmark MEDQA increased by 1.8% vs. SFT and 3.6% vs. RAG) and reasoning capability (Pass@1 on the external benchmark PUMCH Antimicrobial increased by 27% vs. SFT and 27.2% vs. RAG), achieved at ~20% of SFT's long-term training costs. This establishes KRAL as an effective solution for enhancing local LLMs' clinical diagnostic capabilities, enabling low-cost, high-safety deployment in complex medical decision support.", 'abstract_zh': '临床抗微生物治疗需要动态整合病原体特征、宿主因素、抗微生物药物的药理特性以及病情严重程度。这种复杂性对大型语言模型（LLMs）在高风险临床决策中的应用提出了根本性的局限性，包括知识空白、数据隐私问题、高部署成本和有限的推理能力。为解决这些挑战，我们提出了KRAL（Knowledge and Reasoning Augmented Learning）框架，这是一种低成本、可扩展、保护隐私的方法，它利用教师模型的推理自动提取知识和推理轨迹，通过答案到问题的逆向生成，利用启发式学习进行半监督数据增强（减少约80%的手动标注需求），并利用代理强化学习同时增强医学知识和推理，优化计算和内存效率。多层次的评估采用多样化的教师模型代理降低了评估成本，模块化界面设计便于系统更新。实验结果表明，KRAL 显著优于传统的检索增强生成（RAG）和监督微调（SFT）方法。KRAL 提高了知识问答能力（在外部开源基准 MEDQA 上的 Accuracy@1 提高了 1.8% 对比 SFT 和 3.6% 对比 RAG），以及推理能力（在外部基准 PUMCH Antimicrobial 上的 Pass@1 提高了 27% 对比 SFT 和 27.2% 对比 RAG），实现成本约为 SFT 长期训练成本的 20%。这表明 KRAL 是增强本地 LLM 临床诊断能力的有效方案，能够以低成本、高安全性部署在复杂的医学决策支持中。', 'title_zh': 'KRAL: 知识与推理增强的学习在LLM辅助临床抗菌治疗中的应用'}
{'arxiv_id': 'arXiv:2511.15958', 'title': 'JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning Evaluation', 'authors': 'Zhenyu Bi, Gaurav Srivastava, Yang Li, Meng Lu, Swastik Roy, Morteza Ziyadi, Xuan Wang', 'link': 'https://arxiv.org/abs/2511.15958', 'abstract': 'While small language models (SLMs) have shown promise on various reasoning tasks, their ability to judge the correctness of answers remains unclear compared to large language models (LLMs). Prior work on LLM-as-a-judge frameworks typically relies on comparing candidate answers against ground-truth labels or other candidate answers using predefined metrics like entailment. However, this approach is inherently indirect and difficult to fully automate, offering limited support for fine-grained and scalable evaluation of reasoning outputs. In this work, we propose JudgeBoard, a novel evaluation pipeline that directly queries models to assess the correctness of candidate answers without requiring extra answer comparisons. We focus on two core reasoning domains: mathematical reasoning and science/commonsense reasoning, and construct task-specific evaluation leaderboards using both accuracy-based ranking and an Elo-based rating system across five benchmark datasets, enabling consistent model comparison as judges rather than comparators. To improve judgment performance in lightweight models, we propose MAJ (Multi-Agent Judging), a novel multi-agent evaluation framework that leverages multiple interacting SLMs with distinct reasoning profiles to approximate LLM-level judgment accuracy through collaborative deliberation. Experimental results reveal a significant performance gap between SLMs and LLMs in isolated judging tasks. However, our MAJ framework substantially improves the reliability and consistency of SLMs. On the MATH dataset, MAJ using smaller-sized models as backbones performs comparatively well or even better than their larger-sized counterparts. Our findings highlight that multi-agent SLM systems can potentially match or exceed LLM performance in judgment tasks, with implications for scalable and efficient assessment.', 'abstract_zh': 'JudgeBoard：面向推理输出直接评估的新型评价pipeline', 'title_zh': 'JudgeBoard: 评估和增强小型语言模型在推理评估中的基准测试与提升'}
{'arxiv_id': 'arXiv:2511.15921', 'title': 'Thinking, Faithful and Stable: Mitigating Hallucinations in LLMs', 'authors': 'Chelsea Zou, Yiheng Yao, Basant Khalil', 'link': 'https://arxiv.org/abs/2511.15921', 'abstract': "This project develops a self correcting framework for large language models (LLMs) that detects and mitigates hallucinations during multi-step reasoning. Rather than relying solely on final answer correctness, our approach leverages fine grained uncertainty signals: 1) self-assessed confidence alignment, and 2) token-level entropy spikes to detect unreliable and unfaithful reasoning in real time. We design a composite reward function that penalizes unjustified high confidence and entropy spikes, while encouraging stable and accurate reasoning trajectories. These signals guide a reinforcement learning (RL) policy that makes the model more introspective and shapes the model's generation behavior through confidence-aware reward feedback, improving not just outcome correctness but the coherence and faithfulness of their intermediate reasoning steps. Experiments show that our method improves both final answer accuracy and reasoning calibration, with ablations validating the individual contribution of each signal.", 'abstract_zh': '本项目开发了一个自我矫正框架，用于大型语言模型（LLMs），该框架在多步推理过程中检测并减轻幻觉现象。我们 approach 不仅依赖最终答案的正确性，还利用细粒度的不确定性信号：1) 自我评估的信心对齐，以及 2) 令牌级的熵突增，以实时检测不可靠和不忠实的推理。我们设计了一个复合奖励函数，该函数惩罚不合理的高信心和熵突增，同时鼓励稳定和准确的推理轨迹。这些信号引导强化学习（RL）策略，通过信心意识的奖励反馈，使模型更加反思，并通过生成行为塑造提高最终答案的准确性以及中间推理步骤的一致性和忠实性。实验表明，我们的方法不仅提高了最终答案的准确性，还改进了推理的校准，消融实验验证了每个信号的独立贡献。', 'title_zh': '思辨、忠实且稳定：减轻LLM中的幻觉'}
{'arxiv_id': 'arXiv:2511.15895', 'title': 'Decomposing Theory of Mind: How Emotional Processing Mediates ToM Abilities in LLMs', 'authors': 'Ivan Chulo, Ananya Joshi', 'link': 'https://arxiv.org/abs/2511.15895', 'abstract': "Recent work shows activation steering substantially improves language models' Theory of Mind (ToM) (Bortoletto et al. 2024), yet the mechanisms of what changes occur internally that leads to different outputs remains unclear. We propose decomposing ToM in LLMs by comparing steered versus baseline LLMs' activations using linear probes trained on 45 cognitive actions. We applied Contrastive Activation Addition (CAA) steering to Gemma-3-4B and evaluated it on 1,000 BigToM forward belief scenarios (Gandhi et al. 2023), we find improved performance on belief attribution tasks (32.5\\% to 46.7\\% accuracy) is mediated by activations processing emotional content : emotion perception (+2.23), emotion valuing (+2.20), while suppressing analytical processes: questioning (-0.78), convergent thinking (-1.59). This suggests that successful ToM abilities in LLMs are mediated by emotional understanding, not analytical reasoning.", 'abstract_zh': "Recent Work Shows Activation Steering Substantially Improves Language Models' Theory of Mind (ToM), but the Internal Mechanisms Remain unclear. We Propose Decomposing ToM in LLMs by Comparing Steered Versus Baseline LLMs' Activations Using Linear Probes Trained on 45 Cognitive Actions. We Applied Contrastive Activation Addition (CAA) Steering to Gemma-3-4B and Evaluated It on 1,000 BigToM Forward Belief Scenarios, Finding Improved Performance on Belief Attribution Tasks (32.5% to 46.7% Accuracy) Is Mediated by Activations Processing Emotional Content: Emotion Perception (+2.23), Emotion Valuing (+2.20), While Suppressing Analytical Processes: Questioning (-0.78), Convergent Thinking (-1.59). This Suggests That Successful ToM Abilities in LLMs Are Mediated by Emotional Understanding, Not Analytical Reasoning.", 'title_zh': '分解理论心智：情绪处理如何中介LLMs的理论心智能力'}
{'arxiv_id': 'arXiv:2511.15848', 'title': 'Step-Audio-R1 Technical Report', 'authors': 'Fei Tian, Xiangyu Tony Zhang, Yuxin Zhang, Haoyang Zhang, Yuxin Li, Daijiao Liu, Yayue Deng, Donghang Wu, Jun Chen, Liang Zhao, Chengyuan Yao, Hexin Liu, Eng Siong Chng, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, Gang Yu', 'link': 'https://arxiv.org/abs/2511.15848', 'abstract': 'Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.', 'abstract_zh': 'Recent Advances in Audio Reasoning Models: From Rejection of Deliberative Thinking to Successful Integration in Step-Audio-R1', 'title_zh': '分步音频-R1 技术报告'}
{'arxiv_id': 'arXiv:2511.15830', 'title': 'Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions', 'authors': 'Stéphane Aroca-Ouellette, Ian Berlot-Attwell, Panagiotis Lymperopoulos, Abhiramon Rajasekharan, Tongqi Zhu, Herin Kang, Kaheer Suleman, Sam Pasupalak', 'link': 'https://arxiv.org/abs/2511.15830', 'abstract': "Despite rapid progress in artificial intelligence, current systems struggle with the interconnected challenges that define real-world decision making. Practical domains, such as business management, require optimizing an open-ended and multi-faceted objective, actively learning environment dynamics from sparse experience, planning over long horizons in stochastic settings, and reasoning over spatial information. Yet existing human--AI benchmarks isolate subsets of these capabilities, limiting our ability to assess holistic decision-making competence. We introduce Mini Amusement Parks (MAPs), an amusement-park simulator designed to evaluate an agent's ability to model its environment, anticipate long-term consequences under uncertainty, and strategically operate a complex business. We provide human baselines and a comprehensive evaluation of state-of-the-art LLM agents, finding that humans outperform these systems by 6.5x on easy mode and 9.8x on medium mode. Our analysis reveals persistent weaknesses in long-horizon optimization, sample-efficient learning, spatial reasoning, and world modelling. By unifying these challenges within a single environment, MAPs offers a new foundation for benchmarking agents capable of adaptable decision making. Code: this https URL", 'abstract_zh': '尽管人工智能取得了快速进展，当前的系统在处理定义现实世界决策的相互联系的挑战方面仍然存在问题。商业管理等实际领域需要优化一个开放且多维度的目标，从稀疏的经验中积极学习环境动态，在不确定性环境下进行长远规划，并在空间信息层面进行推理。然而，现有的人类-人工智能基准测试将这些能力的子集隔离开来，限制了我们全面评估决策能力的能力。我们引入了迷你游乐园（MAPs），这是一个游乐园模拟器，旨在评估代理建模环境、在不确定性下预测长期后果以及战略性运营复杂企业的能力。我们提供了人类基准线，并全面评估了最新一代的LLM代理，发现人类在容易模式上比这些系统高出6.5倍，在中等模式上高出9.8倍。我们的分析揭示了长期优化、样本高效学习、空间推理和世界建模的持久性弱点。通过在单一环境中统一这些挑战，MAPs为评估具备适应性决策能力的代理提供了新的基础。代码：https://thisurl.com', 'title_zh': '迷你游乐园 (MAPs): 业务决策建模的实验平台'}
{'arxiv_id': 'arXiv:2511.15778', 'title': 'Balancing Natural Language Processing Accuracy and Normalisation in Extracting Medical Insights', 'authors': 'Paulina Tworek, Miłosz Bargieł, Yousef Khan, Tomasz Pełech-Pilichowski, Marek Mikołajczyk, Roman Lewandowski, Jose Sousa', 'link': 'https://arxiv.org/abs/2511.15778', 'abstract': 'Extracting structured medical insights from unstructured clinical text using Natural Language Processing (NLP) remains an open challenge in healthcare, particularly in non-English contexts where resources are scarce. This study presents a comparative analysis of NLP low-compute rule-based methods and Large Language Models (LLMs) for information extraction from electronic health records (EHR) obtained from the Voivodeship Rehabilitation Hospital for Children in Ameryka, Poland. We evaluate both approaches by extracting patient demographics, clinical findings, and prescribed medications while examining the effects of lack of text normalisation and translation-induced information loss. Results demonstrate that rule-based methods provide higher accuracy in information retrieval tasks, particularly for age and sex extraction. However, LLMs offer greater adaptability and scalability, excelling in drug name recognition. The effectiveness of the LLMs was compared with texts originally in Polish and those translated into English, assessing the impact of translation. These findings highlight the trade-offs between accuracy, normalisation, and computational cost when deploying NLP in healthcare settings. We argue for hybrid approaches that combine the precision of rule-based systems with the adaptability of LLMs, offering a practical path toward more reliable and resource-efficient clinical NLP in real-world hospitals.', 'abstract_zh': '使用自然语言处理（NLP）从非英语背景下的未结构化临床文本中提取结构化医疗见解仍是一项开放性的挑战：低计算量规则基于方法与大型语言模型在电子健康记录信息提取中的比较研究', 'title_zh': '平衡自然语言处理准确性与规范化在提取医学洞察中的关系'}
{'arxiv_id': 'arXiv:2511.15755', 'title': 'Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response', 'authors': 'Philip Drammeh', 'link': 'https://arxiv.org/abs/2511.15755', 'abstract': 'Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present this http URL, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.', 'abstract_zh': '大型语言模型（LLMs）有潜力加速生产系统的故障响应，但单agent方法生成的是模糊且不可用的建议。我们展示了这个网址，一个可重复的容器化框架，证明了多agent编排从根本上转变了基于LLM的故障响应质量。通过348次受控试验，比较单agent copilot与多agent系统在相同故障场景下的表现，我们发现多agent编排实现了100%可操作建议率，而单agent方法仅为1.7%，在行动具体性和解决方案正确性方面分别提高了80倍和140倍。关键的是，多agent系统在整个试验中的质量没有差异性，使生产SLA承诺不再成为可能的不一致单agent输出的问题。两种架构的理解延迟相似（约40秒），这表明架构价值在于确定性质量，而不是速度。我们提出了决策质量（DQ）作为一个新的度量标准，捕获了现有LLM度量标准所缺乏的操作部署中的有效性和具体性等关键属性。这些发现将多agent编排从性能优化重新定义为基于LLM的故障响应的生产就绪要求。所有代码、Docker配置和试验数据均可公开获取以供复制。', 'title_zh': '多代理大语言模型编排实现确定性、高质量的事件响应决策支持'}
{'arxiv_id': 'arXiv:2511.15752', 'title': 'Build AI Assistants using Large Language Models and Agents to Enhance the Engineering Education of Biomechanics', 'authors': 'Hanzhi Yan, Qin Lu, Xianqiao Wang, Xiaoming Zhai, Tianming Liu, He Li', 'link': 'https://arxiv.org/abs/2511.15752', 'abstract': "While large language models (LLMs) have demonstrated remarkable versatility across a wide range of general tasks, their effectiveness often diminishes in domain-specific applications due to inherent knowledge gaps. Moreover, their performance typically declines when addressing complex problems that require multi-step reasoning and analysis. In response to these challenges, we propose leveraging both LLMs and AI agents to develop education assistants aimed at enhancing undergraduate learning in biomechanics courses that focus on analyzing the force and moment in the musculoskeletal system of the human body. To achieve our goal, we construct a dual-module framework to enhance LLM performance in biomechanics educational tasks: 1) we apply Retrieval-Augmented Generation (RAG) to improve the specificity and logical consistency of LLM's responses to the conceptual true/false questions; 2) we build a Multi-Agent System (MAS) to solve calculation-oriented problems involving multi-step reasoning and code execution. Specifically, we evaluate the performance of several LLMs, i.e., Qwen-1.0-32B, Qwen-2.5-32B, and Llama-70B, on a biomechanics dataset comprising 100 true/false conceptual questions and problems requiring equation derivation and calculation. Our results demonstrate that RAG significantly enhances the performance and stability of LLMs in answering conceptual questions, surpassing those of vanilla models. On the other hand, the MAS constructed using multiple LLMs demonstrates its ability to perform multi-step reasoning, derive equations, execute code, and generate explainable solutions for tasks that require calculation. These findings demonstrate the potential of applying RAG and MAS to enhance LLM performance for specialized courses in engineering curricula, providing a promising direction for developing intelligent tutoring in engineering education.", 'abstract_zh': '大型语言模型（LLMs）在广泛通用任务中展现了显著的 versatility，但在特定领域应用中因固有的知识差距而效果减弱。此外，它们在解决需要多步推理和分析的复杂问题时表现也通常较差。为应对这些挑战，我们提出结合使用大型语言模型和AI代理，开发教育助手，旨在提升生物力学课程中分析人体运动系统中力和力矩的本专科生学习效果。为实现这一目标，我们构建了一个双模块框架以增强语言模型在生物力学教育任务中的性能：1）应用检索增强生成（RAG）以提高其对概念性真伪问题响应的特异性和逻辑一致性；2）构建一个多代理系统（MAS）以解决涉及多步推理和代码执行的计算导向问题。我们对几种大型语言模型，即Qwen-1.0-32B、Qwen-2.5-32B和Llama-70B，进行了评估，这些模型用于包含100个概念性真伪问题和需要方程推导与计算的问题的生物力学数据集。结果表明，RAG显著提高了语言模型在回答概念性问题时的性能和稳定性，超越了基础模型。另一方面，使用多个大型语言模型构建的MAS展示了其进行多步推理、方程推导、代码执行和生成可解释解决方案的能力，适用于需要计算的任务。这些发现表明，结合RAG和MAS有可能提升大型语言模型在工程课程中特定领域的表现，为开发工程教育中的智能辅导提供了有前景的方向。', 'title_zh': '使用大规模语言模型和智能体构建AI助手以增强生物力学工程教育'}
{'arxiv_id': 'arXiv:2511.15719', 'title': 'Chain of Summaries: Summarization Through Iterative Questioning', 'authors': 'William Brach, Lukas Galke Poech', 'link': 'https://arxiv.org/abs/2511.15719', 'abstract': "Large Language Models (LLMs) are increasingly using external web content. However, much of this content is not easily digestible by LLMs due to LLM-unfriendly formats and limitations of context length. To address this issue, we propose a method for generating general-purpose, information-dense summaries that act as plain-text repositories of web content. Inspired by Hegel's dialectical method, our approach, denoted as Chain of Summaries (CoS), iteratively refines an initial summary (thesis) by identifying its limitations through questioning (antithesis), leading to a general-purpose summary (synthesis) that can satisfy current and anticipate future information needs. Experiments on the TriviaQA, TruthfulQA, and SQUAD datasets demonstrate that CoS outperforms zero-shot LLM baselines by up to 66% and specialized summarization methods such as BRIO and PEGASUS by up to 27%. CoS-generated summaries yield higher Q&A performance compared to the source content, while requiring substantially fewer tokens and being agnostic to the specific downstream LLM. CoS thus resembles an appealing option for website maintainers to make their content more accessible for LLMs, while retaining possibilities for human oversight.", 'abstract_zh': '大型语言模型（LLMs）越来越多地使用外部网页内容。然而，其中很大一部分内容由于LLM不友好的格式和背景长度的限制，难以被LLMs轻松消化。为了解决这一问题，我们提出了一种生成通用、信息密集型摘要的方法，这些摘要可作为网页内容的纯文本存储库。我们的方法借鉴了黑格尔辩证法，称为摘要链（CoS），通过迭代细化初始摘要（命题），并通过质疑其局限性（反题），最终生成一种可满足当前信息需求并能预见未来信息需求的一般性摘要（合题）。在TriviaQA、TruthfulQA和SQUAD数据集上的实验表明，CoS在零-shot LLM基线和专用摘要方法（如BRIO和PEGASUS）方面表现更优，分别提高了66%和27%。CoS生成的摘要相比源内容能提供更高的问答性能，同时需要较少的标记，并且对特定的下游LLM无偏见。因此，CoS为网站维护人员提供了一个吸引人的选项，使其内容对LLMs更易访问，同时保留了人类监督的可能性。', 'title_zh': '链式摘要：通过迭代提问进行摘要总结'}
{'arxiv_id': 'arXiv:2511.15718', 'title': 'ToolMind Technical Report: A Large-Scale, Reasoning-Enhanced Tool-Use Dataset', 'authors': 'Chen Yang, Ran Le, Yun Xing, Zhenwei An, Zongchao Chen, Wayne Xin Zhao, Yang Song, Tao Zhang', 'link': 'https://arxiv.org/abs/2511.15718', 'abstract': 'Large Language Model (LLM) agents have developed rapidly in recent years to solve complex real-world problems using external tools. However, the scarcity of high-quality trajectories still hinders the development of stronger LLM agents. Most existing works on multi-turn dialogue synthesis validate correctness only at the trajectory level, which may overlook turn-level errors that can propagate during training and degrade model performance. To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Our data synthesis pipeline first constructs a function graph based on parameter correlations and then uses a multi-agent framework to simulate realistic user-assistant-tool interactions. Beyond trajectory-level validation, we employ fine-grained turn-level filtering to remove erroneous or suboptimal steps, ensuring that only high-quality reasoning traces are retained. This approach mitigates error amplification during training while preserving self-corrective reasoning signals essential for robust tool-use learning. Models fine-tuned on ToolMind show significant improvements over baselines on several benchmarks.', 'abstract_zh': 'Large Language Model (LLM) 剂型在利用外部工具解决复杂现实问题方面发展迅速，但高质量的轨迹数据仍然稀缺，限制了更强的LLM剂型的发展。现有的多轮对话合成工作主要在轨迹级别验证正确性，这可能会忽略在训练过程中传播的轮次级别错误，从而降低模型性能。为解决这些问题，我们介绍了ToolMind，这是一个大规模、高质量的工具-剂型数据集，包含160k个使用超过20k个工具生成的合成数据实例，以及200k个增强的开源数据实例。我们的数据合成管道首先根据参数相关性构建一个功能图，然后使用多剂型框架模拟现实的用户-助手-工具交互。除轨迹级别验证外，我们还采用细粒度的轮次级别筛选以移除错误或次优步骤，确保仅保留高质量的推理轨迹。这种方法可以减轻训练过程中的错误放大，同时保留对于稳健工具使用学习至关重要的自我纠正推理信号。在ToolMind上进行微调的模型在多个基准上表现出显著的性能提升。', 'title_zh': 'ToolMind 技术报告：大规模推理增强工具使用数据集'}
{'arxiv_id': 'arXiv:2511.15715', 'title': 'Graph-Memoized Reasoning: Foundations Structured Workflow Reuse in Intelligent Systems', 'authors': 'Yash Raj Singh', 'link': 'https://arxiv.org/abs/2511.15715', 'abstract': 'Modern large language model-based reasoning systems frequently recompute similar reasoning steps across tasks, wasting computational resources, inflating inference latency, and limiting reproducibility. These inefficiencies underscore the need for persistent reasoning mechanisms that can recall and reuse prior computational traces.\nWe introduce Graph-Memoized Reasoning, a formal framework for representing, storing, and reusing reasoning workflows as graph-structured memory. By encoding past decision graphs and retrieving them through structural and semantic similarity, our approach enables compositional reuse of subgraphs across new reasoning tasks.\nWe formulate an optimization objective that minimizes total reasoning cost regularized by inconsistency between stored and generated workflows, providing a theoretical foundation for efficiency-consistency trade-offs in intelligent systems. We outline a conceptual evaluation protocol aligned with the proposed optimization objective.\nThis framework establishes the groundwork for interpretable, cost-efficient, and self-improving reasoning architectures, offering a step toward persistent memory in large-scale agentic systems.', 'abstract_zh': '基于现代大型语言模型的推理系统经常在不同任务中重新计算相似的推理步骤，浪费计算资源，增加推理延迟，并限制可重现性。这些低效性凸显了需要持久化的推理机制，以召回和复用先前的计算轨迹。\n\n我们引入了图缓存推理（Graph-Memoized Reasoning），这是一种形式化框架，用于表示、存储和复用以图结构记忆形式的推理工作流。通过编码过去的决策图并通过结构和语义相似性进行检索，我们的方法能够在新的推理任务中组合复用子图。\n\n我们提出了一个优化目标，该目标最小化总推理成本，并通过存储和生成工作流之间的一致性进行正则化，为此类智能系统的效率-一致性权衡提供了理论基础。我们概述了一个与提出的优化目标相一致的概念性评估协议。\n\n该框架为可解释、低成本和自我改进的推理架构奠定了基础，为大型代理系统中的持久化记忆提供了一步之遥。', 'title_zh': '图记忆推理：智能系统中结构化工作流重用的理论基础'}
{'arxiv_id': 'arXiv:2511.15714', 'title': 'Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization', 'authors': 'Ariel Kamen, Yakov Kamen', 'link': 'https://arxiv.org/abs/2511.15714', 'abstract': 'This study introduces an ensemble framework for unstructured text categorization using large language models (LLMs). By integrating multiple models, the ensemble large language model (eLLM) framework addresses common weaknesses of individual systems, including inconsistency, hallucination, category inflation, and misclassification. The eLLM approach yields a substantial performance improvement of up to 65\\% in F1-score over the strongest single model. We formalize the ensemble process through a mathematical model of collective decision-making and establish principled aggregation criteria. Using the Interactive Advertising Bureau (IAB) hierarchical taxonomy, we evaluate ten state-of-the-art LLMs under identical zero-shot conditions on a human-annotated corpus of 8{,}660 samples. Results show that individual models plateau in performance due to the compression of semantically rich text into sparse categorical representations, while eLLM improves both robustness and accuracy. With a diverse consortium of models, eLLM achieves near human-expert-level performance, offering a scalable and reliable solution for taxonomy-based classification that may significantly reduce dependence on human expert labeling.', 'abstract_zh': '本研究介绍了一种使用大规模语言模型（LLMs）进行非结构化文本分类的集成框架。通过集成多个模型，集成大规模语言模型（eLLM）框架解决了单个系统中常见的不一致性、幻觉、类别膨胀和误分类等问题。eLLM方法在F1分数上较最强单个模型提高了高达65%的性能。我们通过集体决策的数学模型正式化了集成过程，并建立了原则性的聚合标准。基于互动广告局（IAB）层次分类法，在相同零样本条件下对十个最先进的LLMs进行评估，使用了8,660个手工标注的数据样本。结果显示，个体模型由于语义丰富文本向稀疏类别表示的压缩而性能达到饱和，而eLLM则提高了稳定性和准确性。通过一个多元化的模型集合，eLLM实现了接近人类专家级别的性能，提供了一种可扩展且可靠的基于层次分类法的分类解决方案，可能显著减少对人类专家标注的依赖。', 'title_zh': '多数裁决：大型语言模型集成是内容类别划分的胜出方法'}
{'arxiv_id': 'arXiv:2511.16665', 'title': 'Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter', 'authors': 'Qinghao Hu, Shang Yang, Junxian Guo, Xiaozhe Yao, Yujun Lin, Yuxian Gu, Han Cai, Chuang Gan, Ana Klimovic, Song Han', 'link': 'https://arxiv.org/abs/2511.16665', 'abstract': 'The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very long responses dominate execution time, wasting resources and inflating costs. To address this, we propose TLT, a system that accelerates reasoning RL training losslessly by integrating adaptive speculative decoding. Applying speculative decoding in RL is challenging due to the dynamic workloads, evolving target model, and draft model training overhead. TLT overcomes these obstacles with two synergistic components: (1) Adaptive Drafter, a lightweight draft model trained continuously on idle GPUs during long-tail generation to maintain alignment with the target model at no extra cost; and (2) Adaptive Rollout Engine, which maintains a memory-efficient pool of pre-captured CUDAGraphs and adaptively select suitable SD strategies for each input batch. Evaluations demonstrate that TLT achieves over 1.7x end-to-end RL training speedup over state-of-the-art systems, preserves the model accuracy, and yields a high-quality draft model as a free byproduct suitable for efficient deployment. Code is released at this https URL.', 'abstract_zh': '大型语言模型（LLMs）的出现标志着强大的推理能力的一个重要里程碑，开启了复杂问题解决的新前沿。然而，这些推理模型的训练通常使用强化学习（RL），遇到了关键的效率瓶颈：在RL训练过程中，响应生成表现出持久的长尾分布，其中少数非常长的响应主导了执行时间，浪费了资源并增加了成本。为了解决这个问题，我们提出了TLT系统，通过集成适应性推测解码无损加速推理RL训练。在RL中应用推测解码具有挑战性，因为它涉及到动态工作负载、不断变化的目标模型和草稿模型的训练开销。TLT通过两个协同工作的组件克服了这些障碍：（1）适应性草稿生成器，一个轻量级的草稿模型，在长尾生成期间连续在空闲GPU上训练，以无额外成本的方式保持与目标模型的一致性；（2）适应性展开引擎，维护一个高效内存池的预捕获CUDAGraphs，并为每个输入批次适配选择合适的推测解码策略。评估表明，TLT比现有系统的端到端RL训练速度提高了超过1.7倍，保持了模型的准确性，并且免费产生了高质量的草稿模型，适合高效部署。代码发布于此https URL。', 'title_zh': '长尾现象驯化：自适应投手下的高效推理RL训练'}
{'arxiv_id': 'arXiv:2511.16577', 'title': 'Integrating Symbolic Natural Language Understanding and Language Models for Word Sense Disambiguation', 'authors': 'Kexin Zhao, Ken Forbus', 'link': 'https://arxiv.org/abs/2511.16577', 'abstract': 'Word sense disambiguation is a fundamental challenge in natural language understanding. Current methods are primarily aimed at coarse-grained representations (e.g. WordNet synsets or FrameNet frames) and require hand-annotated training data to construct. This makes it difficult to automatically disambiguate richer representations (e.g. built on OpenCyc) that are needed for sophisticated inference. We propose a method that uses statistical language models as oracles for disambiguation that does not require any hand-annotation of training data. Instead, the multiple candidate meanings generated by a symbolic NLU system are converted into distinguishable natural language alternatives, which are used to query an LLM to select appropriate interpretations given the linguistic context. The selected meanings are propagated back to the symbolic NLU system. We evaluate our method against human-annotated gold answers to demonstrate its effectiveness.', 'abstract_zh': '词义消歧是自然语言理解中的一个基本挑战。现有方法主要针对粗粒度表示（如WordNet同义集或FrameNet框架）并需要人工标注的训练数据来构建。这使得自动消歧更丰富的表示（如基于OpenCyc构建的表示）变得困难，而这些表示对于复杂的推理是必要的。我们提出了一种方法，该方法使用统计语言模型作为消歧的先知，无需任何人工标注的训练数据。相反，符号化的自然语言理解系统生成的多个候选意义被转换为可区分的自然语言替代选项，然后用于查询LLM以根据语言上下文选择适当的解释。选定的意义随后被传递回符号化的自然语言理解系统。我们通过与人工标注的正确答案进行比较来评估该方法的有效性。', 'title_zh': '整合符号自然语言理解和语言模型进行词义消歧'}
{'arxiv_id': 'arXiv:2511.16544', 'title': 'WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue', 'authors': 'Zachary Ellis, Jared Joselowitz, Yash Deo, Yajie He, Anna Kalygina, Aisling Higham, Mana Rahimzadeh, Yan Jia, Ibrahim Habli, Ernest Lim', 'link': 'https://arxiv.org/abs/2511.16544', 'abstract': "As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $\\kappa$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.", 'abstract_zh': "自动语音识别（ASR）在临床对话中越来越广泛地应用，但标准评估仍然主要依赖词错误率（WER）。本文挑战这一标准，探讨转录错误的临床影响是否与WER或其他常见指标相关。我们通过让专家临床医生对比真实语音片段与其ASR生成的版本，并在两个不同的医生-患者对话数据集中标记发现的任何差异的临床影响，建立了一个黄金标准基准。我们的分析显示，WER和现有的各种综合指标与临床医生分配的风险标签（无影响、轻微影响或显著影响）的相关性较差。为了弥合这一评估缺口，我们引入了一个基于LLM的法官，使用GEPA程序化优化以复制专家临床评估。优化后的法官（Gemini-2.5-Pro）达到了与人类相当的性能，准确率为90%，Cohen's $\\kappa$值为0.816。本文提供了一个经验证的自动化框架，将ASR评估从简单的文本忠实度提高到临床对话中必要的、可扩展的安全性评估。", 'title_zh': 'WER不知情：评估ASR错误如何在面向患者的对话中扭曲临床理解'}
{'arxiv_id': 'arXiv:2511.16543', 'title': 'The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation', 'authors': 'Jiaheng Zhang, Daqiang Zhang', 'link': 'https://arxiv.org/abs/2511.16543', 'abstract': 'The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.\nInspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.\nExtensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.', 'abstract_zh': '大型语言模型（LLMs）在可解释推荐系统中的集成通常会导致端到端架构中的性能-效率权衡，在此架构中，排名和解释的联合优化可能导致次优化的妥协。为了解决这一问题，我们提出了Prism，一种新颖的解耦框架，该框架严格将推荐过程分解为专门的排名阶段和解释生成阶段。\n\nPrism借鉴知识蒸馏的理念，利用一个强大的教师LLM（例如FLAN-T5-XXL）作为Oracle，生成高保真解释性知识。一个紧凑且微调的小型学生模型（例如BART-Base）Prism，则专门用于将这些知识合成个性化解释。这种分解确保每个组件都针对其特定目标进行优化，从而消除耦合模型中的固有冲突。\n\n广泛的基准数据集实验表明，我们的140M参数Prism模型在人类评估的忠实性和个性化方面显著优于其11B参数的教师模型，同时推理过程中实现了24倍的加速和10倍的内存消耗减少。这些结果验证了解耦与针对性蒸馏相结合为高质量可解释推荐提供了一条高效且有效的方法。', 'title_zh': 'acle和棱镜：一种解耦且高效的生成推荐解释框架'}
{'arxiv_id': 'arXiv:2511.16485', 'title': 'LLM4EO: Large Language Model for Evolutionary Optimization in Flexible Job Shop Scheduling', 'authors': 'Rongjie Liao, Junhao Qiu, Xin Chen, Xiaoping Li', 'link': 'https://arxiv.org/abs/2511.16485', 'abstract': 'Customized static operator design has enabled widespread application of Evolutionary Algorithms (EAs), but their search performance is transient during iterations and prone to degradation. Dynamic operators aim to address this but typically rely on predefined designs and localized parameter control during the search process, lacking adaptive optimization throughout evolution. To overcome these limitations, this work leverages Large Language Models (LLMs) to perceive evolutionary dynamics and enable operator-level meta-evolution. The proposed framework, LLMs for Evolutionary Optimization (LLM4EO), comprises three components: knowledge-transfer-based operator design, evolution perception and analysis, and adaptive operator evolution. Firstly, initialization of operators is performed by transferring the strengths of classical operators via LLMs. Then, search preferences and potential limitations of operators are analyzed by integrating fitness performance and evolutionary features, accompanied by corresponding suggestions for improvement. Upon stagnation of population evolution, gene selection priorities of operators are dynamically optimized via improvement prompting strategies. This approach achieves co-evolution of populations and operators in the search, introducing a novel paradigm for enhancing the efficiency and adaptability of EAs. Finally, a series of validations on multiple benchmark datasets of the flexible job shop scheduling problem demonstrate that LLM4EO accelerates population evolution and outperforms both mainstream evolutionary programming and traditional EAs.', 'abstract_zh': '基于大型语言模型的进化优化（LLM4EO）算法框架', 'title_zh': 'LLM4EO：大型语言模型在柔性作业车间调度中的进化优化'}
{'arxiv_id': 'arXiv:2511.16483', 'title': 'Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense', 'authors': 'Sayak Mukherjee, Samrat Chatterjee, Emilie Purvine, Ted Fujimoto, Tegan Emerson', 'link': 'https://arxiv.org/abs/2511.16483', 'abstract': 'Designing rewards for autonomous cyber attack and defense learning agents in a complex, dynamic environment is a challenging task for subject matter experts. We propose a large language model (LLM)-based reward design approach to generate autonomous cyber defense policies in a deep reinforcement learning (DRL)-driven experimental simulation environment. Multiple attack and defense agent personas were crafted, reflecting heterogeneity in agent actions, to generate LLM-guided reward designs where the LLM was first provided with contextual cyber simulation environment information. These reward structures were then utilized within a DRL-driven attack-defense simulation environment to learn an ensemble of cyber defense policies. Our results suggest that LLM-guided reward designs can lead to effective defense strategies against diverse adversarial behaviors.', 'abstract_zh': '基于大规模语言模型的奖励设计方法：在复杂动态环境中利用深度强化学习驱动的实验模拟环境自动生成自主网络防御策略', 'title_zh': '基于大型语言模型的奖励设计——面向深度强化学习驱动的自主网络安全防御'}
{'arxiv_id': 'arXiv:2511.16467', 'title': 'Anatomy of an Idiom: Tracing Non-Compositionality in Language Models', 'authors': 'Andrew Gomes', 'link': 'https://arxiv.org/abs/2511.16467', 'abstract': "We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.", 'abstract_zh': '基于电路发现与分析技术：探讨Transformer模型中惯用语的处理机制', 'title_zh': '习语剖析：追踪语言模型中的非组合性'}
{'arxiv_id': 'arXiv:2511.16375', 'title': 'Are Foundation Models Useful for Bankruptcy Prediction?', 'authors': 'Marcin Kostrzewa, Oleksii Furman, Roman Furman, Sebastian Tomczak, Maciej Zięba', 'link': 'https://arxiv.org/abs/2511.16375', 'abstract': 'Foundation models have shown promise across various financial applications, yet their effectiveness for corporate bankruptcy prediction remains systematically unevaluated against established methods. We study bankruptcy forecasting using Llama-3.3-70B-Instruct and TabPFN, evaluated on large, highly imbalanced datasets of over one million company records from the Visegrád Group. We provide the first systematic comparison of foundation models against classical machine learning baselines for this task. Our results show that models such as XGBoost and CatBoost consistently outperform foundation models across all prediction horizons. LLM-based approaches suffer from unreliable probability estimates, undermining their use in risk-sensitive financial settings. TabPFN, while competitive with simpler baselines, requires substantial computational resources with costs not justified by performance gains. These findings suggest that, despite their generality, current foundation models remain less effective than specialized methods for bankruptcy forecasting.', 'abstract_zh': '基础模型在各类金融应用中展现了潜力，但其在公司破产预测方面的有效性仍系统性地未与传统方法进行评估。我们使用Llama-3.3-70B-Instruct和TabPFN对Visegrád集团超过一百万公司记录的大规模、高度不平衡数据集进行破产预测研究。我们提供了基础模型与传统机器学习基线方法在这项任务上的首次系统性比较。结果显示，在所有预测时段，XGBoost和CatBoost等模型的一贯表现优于基础模型。基于大语言模型的方法因不可靠的概率估计，在风险敏感的金融环境中不具优势。虽然TabPFN在与简单基线竞争中有竞争力，但其所需的大量计算资源并未因性能提升而得到合理化。这些发现表明，尽管基础模型具有通用性，但当前的方法在破产预测中仍不如专门方法有效。', 'title_zh': '金融科技模型在破产预测中有效吗？'}
{'arxiv_id': 'arXiv:2511.16324', 'title': 'SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning', 'authors': 'Wei Xia, Zhi-Hong Deng', 'link': 'https://arxiv.org/abs/2511.16324', 'abstract': 'With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.', 'abstract_zh': '随着大型语言模型（LLMs）的快速进步，其在实际应用中的部署日益广泛。LLMs 希望能够在多种任务、用户偏好和实际场景中提供稳健的性能。然而，随着需求的增长，确保 LLMs 产生的响应与人类意图保持一致仍然是一个基础挑战。特别是在推理过程中有效地和高效地对齐模型行为，而无需高昂的重新训练或大量的监督，这既是关键要求也是非平凡的技术任务。为应对这一挑战，我们提出了一种名为 SDA（Steering-Driven Distribution Alignment）的训练免费且模型无关的对齐框架，旨在为开源 LLMs 提供支持。SDA 动态重分布模型输出概率，基于用户定义的对齐指令增强模型行为与人类意图之间的对齐，而不进行微调。该方法轻量级、资源高效，并与多种开源 LLMs 兼容。它可以在推理过程中独立运行，也可以与基于训练的对齐策略结合使用。此外，SDA 支持个性化偏好对齐，允许灵活地控制模型响应行为。实证结果表明，SDA 在三个关键对齐维度（帮助性、无害性和诚实性）上持续改进了 8 种不同规模和起源的开源 LLMs 的对齐性能。具体而言，SDA 在测试模型中的帮助性提高了 64.4%，诚实性提高了 30%，无害性提高了 11.5%，表明其在不同模型和应用场景中的有效性和泛化能力。', 'title_zh': 'SDA: 驾驶式分布对齐以实现无需微调的开放大型语言模型'}
{'arxiv_id': 'arXiv:2511.16278', 'title': '"To Survive, I Must Defect": Jailbreaking LLMs via the Game-Theory Scenarios', 'authors': 'Zhen Sun, Zongmin Zhang, Deqi Liang, Han Sun, Yule Liu, Yun Shen, Xiangshan Gao, Yilong Yang, Shuai Liu, Yutao Yue, Xinlei He', 'link': 'https://arxiv.org/abs/2511.16278', 'abstract': 'As LLMs become more common, non-expert users can pose risks, prompting extensive research into jailbreak attacks. However, most existing black-box jailbreak attacks rely on hand-crafted heuristics or narrow search spaces, which limit scalability. Compared with prior attacks, we propose Game-Theory Attack (GTA), an scalable black-box jailbreak framework. Concretely, we formalize the attacker\'s interaction against safety-aligned LLMs as a finite-horizon, early-stoppable sequential stochastic game, and reparameterize the LLM\'s randomized outputs via quantal response. Building on this, we introduce a behavioral conjecture "template-over-safety flip": by reshaping the LLM\'s effective objective through game-theoretic scenarios, the originally safety preference may become maximizing scenario payoffs within the template, which weakens safety constraints in specific contexts. We validate this mechanism with classical game such as the disclosure variant of the Prisoner\'s Dilemma, and we further introduce an Attacker Agent that adaptively escalates pressure to increase the ASR. Experiments across multiple protocols and datasets show that GTA achieves over 95% ASR on LLMs such as Deepseek-R1, while maintaining efficiency. Ablations over components, decoding, multilingual settings, and the Agent\'s core model confirm effectiveness and generalization. Moreover, scenario scaling studies further establish scalability. GTA also attains high ASR on other game-theoretic scenarios, and one-shot LLM-generated variants that keep the model mechanism fixed while varying background achieve comparable ASR. Paired with a Harmful-Words Detection Agent that performs word-level insertions, GTA maintains high ASR while lowering detection under prompt-guard models. Beyond benchmarks, GTA jailbreaks real-world LLM applications and reports a longitudinal safety monitoring of popular HuggingFace LLMs.', 'abstract_zh': '基于博弈论的可扩展黑盒 Jailbreak 攻击框架', 'title_zh': '“为了生存，我必须背叛”：通过博弈论场景破解LLMs'}
{'arxiv_id': 'arXiv:2511.16275', 'title': 'SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs', 'authors': 'Xingtao Zhao, Hao Peng, Dingli Su, Xianghua Zeng, Chunyang Liu, Jinzhi Liao, Philip S. Yu', 'link': 'https://arxiv.org/abs/2511.16275', 'abstract': 'Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinating falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation -- where existing methods often rely on heuristic sample-and-count techniques -- we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.', 'abstract_zh': '可靠的不确定性量化 (UQ) 是在关键安全场景中部署大型语言模型 (LLMs) 的基础，因为它使它们能够在不确定时避免响应，从而避免产生 falsehood。然而，最先进的 UQ 方法主要依赖于语义概率分布或成对距离，忽视了潜在的语义结构信息，这些信息能够提供更精确的不确定性估计。本文提出了语义结构熵 (SeSE)，这是一种基于结构信息视角量化 LLMs 内在语义不确定性并用于幻觉检测的原则性 UQ 框架。具体来说，为了有效建模语义空间，我们首先开发了一种自适应稀疏化有向语义图构建算法，该算法捕获了方向性语义依赖性，并自动修剪可能引入负干扰的不必要的连接。然后，通过层次抽象利用潜在的语义结构信息：SeSE 定义为最优语义编码树的结构熵，形式化语义空间在最优压缩后的内在不确定性。SeSE 值越高表示不确定性越大，表明 LLMs 高可能性生成幻觉。此外，为了增强长段落生成中的细粒度不确定性量化——现有方法通常依赖于启发式的样本-计数技术——我们将 SeSE 扩展到通过建模个体断言的随机语义交互来量化其不确定性，从而提供理论可解释的幻觉检测。跨 29 种模型-数据集组合的广泛实验表明，SeSE 显著优于先进的 UQ 基线，包括强大的监督方法和最近提出的 KLE。', 'title_zh': 'SeSE：一种结构信息引导的不确定性量化框架用于LLMs的幻觉检测'}
{'arxiv_id': 'arXiv:2511.16229', 'title': 'Q-MLLM: Vector Quantization for Robust Multimodal Large Language Model Security', 'authors': 'Wei Zhao, Zhe Li, Yige Li, Jun Sun', 'link': 'https://arxiv.org/abs/2511.16229', 'abstract': 'Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in cross-modal understanding, but remain vulnerable to adversarial attacks through visual inputs despite robust textual safety mechanisms. These vulnerabilities arise from two core weaknesses: the continuous nature of visual representations, which allows for gradient-based attacks, and the inadequate transfer of text-based safety mechanisms to visual content. We introduce Q-MLLM, a novel architecture that integrates two-level vector quantization to create a discrete bottleneck against adversarial attacks while preserving multimodal reasoning capabilities. By discretizing visual representations at both pixel-patch and semantic levels, Q-MLLM blocks attack pathways and bridges the cross-modal safety alignment gap. Our two-stage training methodology ensures robust learning while maintaining model utility. Experiments demonstrate that Q-MLLM achieves significantly better defense success rate against both jailbreak attacks and toxic image attacks than existing approaches. Notably, Q-MLLM achieves perfect defense success rate (100\\%) against jailbreak attacks except in one arguable case, while maintaining competitive performance on multiple utility benchmarks with minimal inference overhead. This work establishes vector quantization as an effective defense mechanism for secure multimodal AI systems without requiring expensive safety-specific fine-tuning or detection overhead. Code is available at this https URL.', 'abstract_zh': '多模态大型语言模型（Q-MLLM）通过集成两级向量量化增强对抗攻击防御能力的同时保持多模态推理能力', 'title_zh': 'Q-MLLM：多模态大型语言模型的鲁棒向量量化安全性'}
{'arxiv_id': 'arXiv:2511.16193', 'title': 'Fast LLM Post-training via Decoupled and Best-of-N Speculation', 'authors': 'Rongxin Cheng, Kai Zhou, Xingda Wei, Siyuan Liu, Mingcong Han, Mingjing Ai, Yeju Zhou, Baoquan Zhong, Wencong Xiao, Xin Liu, Rong Chen, Haibo Chen', 'link': 'https://arxiv.org/abs/2511.16193', 'abstract': 'Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \\emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \\emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\\sys} is {1.3--1.7}\\,$\\times$ faster than common post-training baselines, and is {1.3--1.5}\\,$\\times$ faster compared to naively adopting speculative decoding for rollout.', 'abstract_zh': 'Rollout在大型语言模型后训练中主导了训练时间，SpecActor通过猜测解码实现了快速rollout，同时通过原模型快速并行验证确保输出正确性，通过动态解耦猜测执行方法和动态最佳猜测方法解决了猜测rollout的两大基础挑战。.SpecActor比常见后训练基线快1.3-1.7倍，比简单采用猜测解码的rollout快1.3-1.5倍。', 'title_zh': '基于解耦和最优批次推测的快速LLM后训练'}
{'arxiv_id': 'arXiv:2511.16135', 'title': 'CoSP: Reconfigurable Multi-State Metamaterial Inverse Design via Contrastive Pretrained Large Language Model', 'authors': 'Shujie Yang, Xuzhe Zhao, Yuqi Zhang, Yansong Tang, Kaichen Dong', 'link': 'https://arxiv.org/abs/2511.16135', 'abstract': "Metamaterials, known for their ability to manipulate light at subwavelength scales, face significant design challenges due to their complex and sophisticated structures. Consequently, deep learning has emerged as a powerful tool to streamline their design process. Reconfigurable multi-state metamaterials (RMMs) with adjustable parameters can switch their optical characteristics between different states upon external stimulation, leading to numerous applications. However, existing deep learning-based inverse design methods fall short in considering reconfigurability with multi-state switching. To address this challenge, we propose CoSP, an intelligent inverse design method based on contrastive pretrained large language model (LLM). By performing contrastive pretraining on multi-state spectrum, a well-trained spectrum encoder capable of understanding the spectrum is obtained, and it subsequently interacts with a pretrained LLM. This approach allows the model to preserve its linguistic capabilities while also comprehending Maxwell's Equations, enabling it to describe material structures with target optical properties in natural language. Our experiments demonstrate that CoSP can design corresponding thin-film metamaterial structures for arbitrary multi-state, multi-band optical responses, showing great potentials in the intelligent design of RMMs for versatile applications.", 'abstract_zh': '基于对比预训练大语言模型的智能逆设计方法CoSP：面向可重构多状态 metamaterials 的智能设计', 'title_zh': 'CoSP: 基于对比预训练大语言模型的可配置多态超材料逆设计'}
{'arxiv_id': 'arXiv:2511.16131', 'title': 'AskDB: An LLM Agent for Natural Language Interaction with Relational Databases', 'authors': 'Xuan-Quang Phan, Tan-Ha Mai, Thai-Duy Dinh, Minh-Thuan Nguyen, Lam-Son Lê', 'link': 'https://arxiv.org/abs/2511.16131', 'abstract': 'Interacting with relational databases remains challenging for users across different expertise levels, particularly when composing complex analytical queries or performing administrative tasks. Existing systems typically address either natural language querying or narrow aspects of database administration, lacking a unified and intelligent interface for general-purpose database interaction. We introduce AskDB, a large language model powered agent designed to bridge this gap by supporting both data analysis and administrative operations over SQL databases through natural language. Built on Gemini 2, AskDB integrates two key innovations: a dynamic schema-aware prompting mechanism that effectively incorporates database metadata, and a task decomposition framework that enables the agent to plan and execute multi-step actions. These capabilities allow AskDB to autonomously debug derived SQL, retrieve contextual information via real-time web search, and adaptively refine its responses. We evaluate AskDB on a widely used Text-to-SQL benchmark and a curated set of DBA tasks, demonstrating strong performance in both analytical and administrative scenarios. Our results highlight the potential of AskDB as a unified and intelligent agent for relational database systems, offering an intuitive and accessible experience for end users.', 'abstract_zh': '使用大规模语言模型的AskDBagents通过自然语言支持SQL数据库的数据分析和管理操作', 'title_zh': 'AskDB：与关系型数据库自然语言交互的LLM代理'}
{'arxiv_id': 'arXiv:2511.16122', 'title': 'ELPO: Ensemble Learning Based Prompt Optimization for Large Language Models', 'authors': 'Qing Zhang, Bing Xu, Xudong Zhang, Yifan Shi, Yang Li, Chen Zhang, Yik Chung Wu, Ngai Wong, Yijie Chen, Hong Dai, Xiansen Chen, Mian Zhang', 'link': 'https://arxiv.org/abs/2511.16122', 'abstract': 'The remarkable performance of Large Language Models (LLMs) highly relies on crafted prompts. However, manual prompt engineering is a laborious process, creating a core bottleneck for practical application of LLMs. This phenomenon has led to the emergence of a new research area known as Automatic Prompt Optimization (APO), which develops rapidly in recent years. Existing APO methods such as those based on evolutionary algorithms or trial-and-error approaches realize an efficient and accurate prompt optimization to some extent. However, those researches focus on a single model or algorithm for the generation strategy and optimization process, which limits their performance when handling complex tasks. To address this, we propose a novel framework called Ensemble Learning based Prompt Optimization (ELPO) to achieve more accurate and robust results. Motivated by the idea of ensemble learning, ELPO conducts voting mechanism and introduces shared generation strategies along with different search methods for searching superior prompts. Moreover, ELPO creatively presents more efficient algorithms for the prompt generation and search process. Experimental results demonstrate that ELPO outperforms state-of-the-art prompt optimization methods across different tasks, e.g., improving F1 score by 7.6 on ArSarcasm dataset.', 'abstract_zh': '大型语言模型的出色表现高度依赖于精心设计的提示。然而，手动提示工程是一个繁琐的过程，成为实际应用大型语言模型的核心瓶颈。这一现象导致了自动提示优化（APO）这一新研究领域的出现，该领域近年来快速发展。现有的APO方法，如基于进化算法或试错方法的研究，在一定程度上实现了高效和准确的提示优化。然而，这些研究集中在单一模型或算法的生成策略和优化过程上，当处理复杂任务时限制了它们的性能。为了解决这个问题，我们提出了一种新的框架，称为基于集成学习的提示优化（ELPO），以实现更准确和鲁棒的结果。受集成学习思想的启发，ELPO进行了投票机制，引入了共享的生成策略以及不同的搜索方法来寻找最优提示。此外，ELPO创造性地提出了更高效的提示生成和搜索算法。实验结果表明，ELPO在不同任务中均优于最先进的提示优化方法，例如，在ArSarcasm数据集上将F1分数提高了7.6%。', 'title_zh': 'ELPO：基于集成学习的(prompt优化)大规模语言模型提示优化'}
{'arxiv_id': 'arXiv:2511.16107', 'title': 'T2T-VICL: Unlocking the Boundaries of Cross-Task Visual In-Context Learning via Implicit Text-Driven VLMs', 'authors': 'Shao-Jun Xia, Huixin Zhang, Zhengzhong Tu', 'link': 'https://arxiv.org/abs/2511.16107', 'abstract': 'In large language models (LLM), in-context learning (ICL) refers to performing new tasks by conditioning on small demonstrations provided in the input context. Recent advances in visual in-context learning (VICL) demonstrate promising capabilities for solving downstream tasks by unified vision-language models (VLMs). When the visual prompt and the target images originate from different visual tasks, can VLMs still enable VICL? In the paper, we propose a fully collaborative pipeline, i.e. T2T-VICL, for VLMs to investigate the potential of cross-task VICL. Fundamentally, we design a mechanism to generate and select text prompts that best implicitly describe the differences between two distinct low-level vision tasks, and construct the first cross-task VICL dataset. Building upon this, we propose a novel inference framework that combines perceptual score-based reasoning with traditional evaluation metrics to perform cross-task VICL. Our approach achieves top-tier results across nine cross-task scenarios and second-tier performance in ten additional scenarios, unlocking the boundaries of cross-task VICL within VLMs.', 'abstract_zh': '大规模语言模型中跨任务视觉上下文学习的研究', 'title_zh': 'T2T-VICL: 通过隐式文本驱动的VLM解锁跨任务视觉在_CONTEXT学习的边界'}
{'arxiv_id': 'arXiv:2511.16073', 'title': 'A Mathematical Framework for Custom Reward Functions in Job Application Evaluation using Reinforcement Learning', 'authors': 'Shreyansh Jain, Madhav Singhvi, Shreya Rahul Jain, Pranav S, Dishaa Lokesh, Naren Chittibabu, Akash Anandhan', 'link': 'https://arxiv.org/abs/2511.16073', 'abstract': 'Conventional Applicant Tracking Systems (ATS) tend to be inflexible keyword-matchers, and deny gifted candidates a role due to a few minor semantic mismatches. This article describes a new two-step process to design a more refined resume evaluation model based on a small language model (<600M parameters) that is finetuned using GRPO on a custom reward function. To begin with, Supervised Fine-Tuning (SFT) was used to build a solid baseline model. Second, this SFT model was also optimized with the help of Reinforcement Learning (RL) through GRPO under the guidance of a new, multi-component reward function that can holistically assess candidates beyond simple keyword matching. We indicate that the RL application presents a critical problem of reward hacking due to the initial experiments of aggressive penalties, which produces faulty, excessively negative model behaviors. We have overcome this challenge by refining the reward function repeatedly and training hyperparameters into a stable "gentle polishing process" of the reward function. Our resulting GRPO-polished model demonstrates significant real-world efficacy, achieving a final accuracy of 91% on unseen test data. The model shows a strong ability to correctly identify qualified candidates (recall of 0.85 for the \'SELECTED\' class) while also showing exceptional precision (1.0), confirming its reliability. These results indicate that a properly executed, two-step fine-tuning procedure can indeed effectively refine a small language model to be able to conduct fine-tuned and human-like candidate scoring, overcoming the drawbacks of both traditional ATS and naive RL usage.', 'abstract_zh': '基于小型语言模型和强化学习的应聘者精细评估模型设计方法', 'title_zh': '基于强化学习的工作申请评估中自定义奖励函数的数学框架'}
{'arxiv_id': 'arXiv:2511.16072', 'title': 'Early science acceleration experiments with GPT-5', 'authors': 'Sébastien Bubeck, Christian Coester, Ronen Eldan, Timothy Gowers, Yin Tat Lee, Alexandru Lupsasca, Mehtaab Sawhney, Robert Scherrer, Mark Sellke, Brian K. Spears, Derya Unutmaz, Kevin Weil, Steven Yin, Nikita Zhivotovskiy', 'link': 'https://arxiv.org/abs/2511.16072', 'abstract': 'AI models like GPT-5 are an increasingly valuable tool for scientists, but many remain unaware of the capabilities of frontier AI. We present a collection of short case studies in which GPT-5 produced new, concrete steps in ongoing research across mathematics, physics, astronomy, computer science, biology, and materials science. In these examples, the authors highlight how AI accelerated their work, and where it fell short; where expert time was saved, and where human input was still key. We document the interactions of the human authors with GPT-5, as guiding examples of fruitful collaboration with AI. Of note, this paper includes four new results in mathematics (carefully verified by the human authors), underscoring how GPT-5 can help human mathematicians settle previously unsolved problems. These contributions are modest in scope but profound in implication, given the rate at which frontier AI is progressing.', 'abstract_zh': 'AI模型如GPT-5已成为科学家们越来越有价值的工具，但许多科学家仍 unaware 于前沿AI的能力。我们介绍了GPT-5在数学、物理学、天文学、计算机科学、生物学和材料科学等领域推动正在进行的研究的新颖具体案例。在这些案例中，作者强调了AI如何加速他们的工作，以及它的局限性；哪些专家时间得到了节省，哪些环节仍需人类输入。我们记录了人类作者与GPT-5的互动，作为与AI进行富有成效合作的范例。值得注意的是，本文包括了四项新的数学成果（由人类作者仔细验证），突显了GPT-5如何帮助人类数学家解决以前未能解决的问题。这些贡献在范围上可能相对有限，但在给定前沿AI快速进展的背景下，其影响是深远的。', 'title_zh': 'GPT-5早期科学加速实验'}
{'arxiv_id': 'arXiv:2511.16054', 'title': 'Learning Tractable Distributions Of Language Model Continuations', 'authors': 'Gwen Yidou-Weng, Ian Li, Anji Liu, Oliver Broadrick, Guy Van den Broeck, Benjie Wang', 'link': 'https://arxiv.org/abs/2511.16054', 'abstract': "Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intractable. Prior work uses tractable surrogates such as hidden Markov models (HMMs) to approximate the distribution over continuations and adjust the model's next-token logits at decoding time. However, we find that these surrogates are often weakly context aware, which reduces query quality. We propose Learning to Look Ahead (LTLA), a hybrid approach that pairs the same base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. Two efficiency pitfalls arise when adding neural context: (i) naively rescoring the prefix with every candidate next token requires a sweep over the entire vocabulary at each step, and (ii) predicting fresh surrogate parameters for each prefix, although tractable at a single step, forces recomputation of future probabilities for every new prefix and eliminates reuse. LTLA avoids both by using a single batched HMM update to account for all next-token candidates at once, and by conditioning only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed, so computations can be reused across prefixes. Empirically, LTLA attains higher conditional likelihood than an unconditional HMM, approximates continuation distributions for vision-language models where a standalone HMM cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks, with minimal inference overhead.", 'abstract_zh': 'Controlled语言生成通过序列级约束（例如句法、风格或安全性）对文本进行控制。这些约束可能依赖于未来的词，这使得直接条件化自回归语言模型（LM）通常变得不可行。先前的工作使用隐马尔可夫模型（HMMs）等可处理的替代模型来近似后续内容的分布并在解码时调整模型的下一个词的概率。然而，我们发现这些替代模型往往对上下文的感知较弱，这降低了查询质量。我们提出了一种结合方法——Learn to Look Ahead (LTLA)。该方法通过使用丰富的前缀编码基础语言模型与固定可处理替代模型相结合，后者可以计算精确的后续概率。当添加神经上下文时，会出现两种效率陷阱：（i）朴素地用每个候选的下一个词重新评分前缀要求在每一步中对整个词汇表进行扫描；（ii）对于每个前缀预测新鲜的替代参数虽然单步是可处理的，但要求重新计算每个新前缀的未来概率且不能再利用前缀之间的计算结果。LTLA 通过一次性使用批量HMM更新来同时考虑所有下一个词的候选，避免了上述两种陷阱，并且仅通过基础语言模型的隐藏表示条件化替代模型的潜状态，而保持替代模型解码器固定，从而使计算结果可以在不同前缀之间重用。实验结果表明，LTLA 相比于无条件的HMM具有更高的条件似然，可以为视觉-语言模型近似后续内容的分布，即使独立的HMM无法编码视觉上下文，并且在控制生成任务中以相似的流畅度提高约束满足度，同时具有最小的推理开销。', 'title_zh': '学习可计算的语言模型续篇分布'}
{'arxiv_id': 'arXiv:2511.16035', 'title': "Liars' Bench: Evaluating Lie Detectors for Language Models", 'authors': 'Kieron Kretschmar, Walter Laurito, Sharan Maiya, Samuel Marks', 'link': 'https://arxiv.org/abs/2511.16035', 'abstract': "Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generating statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies LLMs can generate. We introduce LIARS' BENCH, a testbed consisting of 72,863 examples of lies and honest responses generated by four open-weight models across seven datasets. Our settings capture qualitatively different types of lies and vary along two dimensions: the model's reason for lying and the object of belief targeted by the lie. Evaluating three black- and white-box lie detection techniques on LIARS' BENCH, we find that existing techniques systematically fail to identify certain types of lies, especially in settings where it's not possible to determine whether the model lied from the transcript alone. Overall, LIARS' BENCH reveals limitations in prior techniques and provides a practical testbed for guiding progress in lie detection.", 'abstract_zh': "Prior Work Has Introduced Techniques for Detecting When Large Language Models Lie: LIARS' BENCH, a Testbed Consisting of 72,863 Examples of Lies and Honest Responses Generated by Four Open-Weight Models Across Seven Datasets", 'title_zh': '说谎者的长凳：评估语言模型中的说谎检测器'}
{'arxiv_id': 'arXiv:2511.16016', 'title': 'CARE: Turning LLMs Into Causal Reasoning Expert', 'authors': 'Juncheng Dong, Yiling Liu, Ahmed Aloui, Vahid Tarokh, David Carlson', 'link': 'https://arxiv.org/abs/2511.16016', 'abstract': "Large language models (LLMs) have recently demonstrated impressive capabilities across a range of reasoning and generation tasks. However, research studies have shown that LLMs lack the ability to identify causal relationships, a fundamental cornerstone of human intelligence. We first conduct an exploratory investigation of LLMs' behavior when asked to perform a causal-discovery task and find that they mostly rely on the semantic meaning of variable names, ignoring the observation data. This is unsurprising, given that LLMs were never trained to process structural datasets. To first tackle this challenge, we prompt the LLMs with the outputs of established causal discovery algorithms designed for observational datasets. These algorithm outputs effectively serve as the sufficient statistics of the observation data. However, quite surprisingly, we find that prompting the LLMs with these sufficient statistics decreases the LLMs' performance in causal discovery. To address this current limitation, we propose CARE, a framework that enhances LLMs' causal-reasoning ability by teaching them to effectively utilize the outputs of established causal-discovery algorithms through supervised fine-tuning. Experimental results show that a finetuned Qwen2.5-1.5B model produced by CARE significantly outperforms both traditional causal-discovery algorithms and state-of-the-art LLMs with over a thousand times more parameters, demonstrating effective utilization of its own knowledge and the external algorithmic clues.", 'abstract_zh': '大型语言模型（LLMs）在一系列推理和生成任务中展示了令人印象深刻的能力。然而，研究显示LLMs缺乏识别因果关系的能力，这是人类智能的一个基本要素。我们首先对LLMs在执行因果发现任务时的行为进行了探索性研究，发现它们主要依赖变量名称的语义含义，忽视了观测数据。鉴于LLMs从未被训练处理结构性数据集，我们首先尝试通过提供用于观察数据集的现有因果发现算法的输出来应对这一挑战。这些算法的输出有效充当了观测数据的充分统计量。然而，令人惊讶的是，我们发现提供这些充分统计量给LLMs反而降低了它们在因果发现任务上的表现。为了应对这一当前限制，我们提出了CARE框架，通过监督微调方式教会LLMs有效利用现有因果发现算法的输出来增强其因果推理能力。实验结果显示，CARE生成的微调后的Qwen2.5-1.5B模型在因果发现任务上的表现显著优于传统因果发现算法和具有数千倍更多参数的最新LLMs，展示了其有效利用自身知识和外部算法线索的能力。', 'title_zh': 'CARE: 将大语言模型转化为因果推理专家'}
{'arxiv_id': 'arXiv:2511.16005', 'title': 'InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution', 'authors': 'Qingao Dong, Mengfei Wang, Hengzhi Zhang, Zhichao Li, Yuan Yuan, Mu Li, Xiang Gao, Hailong Sun, Chunming Hu, Weifeng Lv', 'link': 'https://arxiv.org/abs/2511.16005', 'abstract': 'Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for this http URL components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \\texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.', 'abstract_zh': 'Large语言模型（LLM）代理在仓库级别问题解决方面表现出强大的性能，但现有系统几乎仅设计用于Python，并且严重依赖于词汇检索和浅层代码导航。这些方法在C++项目中表现不佳，因为C++项目中的重载标识符、嵌套命名空间、模板实例化以及复杂的控制流结构使上下文检索和故障定位变得更加困难。因此，最先进的面向Python的代理在MultiSWE-bench的C++子集中显示出显著的性能下降。我们提出了INFCODE-C++，这是第一个具备C++意识的端到端问题解决自主系统。该系统结合了语义代码意图检索和确定性AST结构查询两种互补的检索机制，以构建准确的语言意识上下文，精确定位和生成大型静态类型C++仓库中的稳定补丁。INFCODE-C++在\\texttt{MultiSWE-bench-CPP}基准测试中实现了25.58%的问题解决率，比 strongest 的先前代理高10.85个百分点，并且将MSWE-agent的性能提高了一倍多。进一步的消融研究和行为研究表明，在C++问题解决中语义检索、结构分析和精确重现场景的角色至关重要。INFCODE-C++强调多语言软件代理中语言意识推理的必要性，并为未来研究大规模、基于LLM的复杂静态类型生态系统修复奠定了基础。', 'title_zh': 'InfCode-C++: 意图导向的语义检索与AST结构化搜索以解决C++问题'}
{'arxiv_id': 'arXiv:2511.15998', 'title': 'Hiding in the AI Traffic: Abusing MCP for LLM-Powered Agentic Red Teaming', 'authors': 'Strahinja Janjuesvic, Anna Baron Garcia, Sohrob Kazerounian', 'link': 'https://arxiv.org/abs/2511.15998', 'abstract': 'Generative AI is reshaping offensive cybersecurity by enabling autonomous red team agents that can plan, execute, and adapt during penetration tests. However, existing approaches face trade-offs between generality and specialization, and practical deployments reveal challenges such as hallucinations, context limitations, and ethical concerns. In this work, we introduce a novel command & control (C2) architecture leveraging the Model Context Protocol (MCP) to coordinate distributed, adaptive reconnaissance agents covertly across networks. Notably, we find that our architecture not only improves goal-directed behavior of the system as whole, but also eliminates key host and network artifacts that can be used to detect and prevent command & control behavior altogether. We begin with a comprehensive review of state-of-the-art generative red teaming methods, from fine-tuned specialist models to modular or agentic frameworks, analyzing their automation capabilities against task-specific accuracy. We then detail how our MCP-based C2 can overcome current limitations by enabling asynchronous, parallel operations and real-time intelligence sharing without periodic beaconing. We furthermore explore advanced adversarial capabilities of this architecture, its detection-evasion techniques, and address dual-use ethical implications, proposing defensive measures and controlled evaluation in lab settings. Experimental comparisons with traditional C2 show drastic reductions in manual effort and detection footprint. We conclude with future directions for integrating autonomous exploitation, defensive LLM agents, predictive evasive maneuvers, and multi-agent swarms. The proposed MCP-enabled C2 framework demonstrates a significant step toward realistic, AI-driven red team operations that can simulate advanced persistent threats while informing the development of next-generation defensive systems.', 'abstract_zh': '基于Model Context Protocol的命令与控制架构：重塑生成式红队攻击', 'title_zh': '在AI流量中隐身：滥用MCP进行由大语言模型驱动的红队演练'}
{'arxiv_id': 'arXiv:2511.15950', 'title': 'A Scalable NorthPole System with End-to-End Vertical Integration for Low-Latency and Energy-Efficient LLM Inference', 'authors': 'Michael V. DeBole, Rathinakumar Appuswamy, Neil McGlohon, Brian Taba, Steven K. Esser, Filipp Akopyan, John V. Arthur, Arnon Amir, Alexander Andreopoulos, Peter J. Carlson, Andrew S. Cassidy, Pallab Datta, Myron D. Flickner, Rajamohan Gandhasri, Guillaume J. Garreau, Megumi Ito, Jennifer L. Klamo, Jeffrey A. Kusnitz, Nathaniel J. McClatchey, Jeffrey L. McKinstry, Tapan K. Nayak, Carlos Ortega Otero, Hartmut Penner, William P. Risk, Jun Sawada, Jay Sivagnaname, Daniel F. Smith, Rafael Sousa, Ignacio Terrizzano, Takanori Ueda, Trent Gray-Donald, David Cox, Dharmendra S. Modha', 'link': 'https://arxiv.org/abs/2511.15950', 'abstract': 'A vertically integrated, end-to-end, research prototype system combines 288 NorthPole neural inference accelerator cards, offline training algorithms, a high-performance runtime stack, and a containerized inference pipeline to deliver a scalable and efficient cloud inference service. The system delivers 115 peta-ops at 4-bit integer precision and 3.7 PB/s of memory bandwidth across 18 2U servers, while consuming only 30 kW of power and weighing 730 kg in a 0.67 m^2 42U rack footprint. The system can run 3 simultaneous instances of the 8-billion-parameter open-source IBM Granite-3.3-8b-instruct model at 2,048 context length with 28 simultaneous users and a per-user inter-token latency of 2.8 ms. The system is scalable, modular, and reconfigurable, supporting various model sizes and context lengths, and is ideal for deploying agentic workflows for enterprise AI applications in existing data center (cloud, on-prem) environments. For example, the system can support 18 instances of a 3-billion-parameter model or a single instance of a 70-billion-parameter model.', 'abstract_zh': '一种垂直整合的端到端研究原型系统结合了288块北极神经推理加速卡、离线训练算法、高性能运行时堆栈以及容器化推理管道，实现了可扩展且高效的云推理服务。该系统在18个2U服务器上实现了115 peta-OPS的性能和每秒3.7 PB的内存带宽，仅消耗30 kW的电力且重量为730 kg，占用0.67平方米42U机架的空间。该系统可以同时运行80亿参数的开源IBM Granite-3.3-8b-instruct模型的3个实例，上下文长度为2048，支持28个同时用户，并且每个用户的跨词元延迟为2.8毫秒。该系统具有可扩展性、模块化和可重构性，支持各种模型大小和上下文长度，适用于现有数据中心（云、内部部署）环境的企业AI应用程序的代理工作流部署。例如，该系统可以支持18个30亿参数模型的实例或一个700亿参数模型的实例。', 'title_zh': '面向低延迟和能效的LLM推理的可扩展北极系统及其端到端垂直整合'}
{'arxiv_id': 'arXiv:2511.15927', 'title': 'Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone', 'authors': 'Vaibhav Singh, Oleksiy Ostapenko, Pierre-André Noël, Torsten Scholak', 'link': 'https://arxiv.org/abs/2511.15927', 'abstract': 'Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language model built on a bidirectional Mamba backbone that combines the diffusion objective with linear-time sequence modeling. DiffuApriel matches the performance of Transformer-based diffusion models while achieving up to 4.4x higher inference throughput for long sequences with a 1.3B model. We further propose DiffuApriel-H, a hybrid variant that interleaves attention and mamba layers, offering up to 2.6x throughput improvement with balanced global and local context modeling. Our results demonstrate that bidirectional state-space architectures serve as strong denoisers in masked diffusion LMs, providing a practical and scalable foundation for faster, memory-efficient text generation.', 'abstract_zh': '基于扩散的语言模型最近 emerged as a 有前景的自回归生成替代方案，但它们依赖于 Transformer 局部结构导致注意力和 KV 缓存开销的二次时间复杂性，限制了推理效率。在本文中，我们介绍了一种基于双向 Mamba 局部结构的屏蔽扩散语言模型 DiffuApriel，该模型结合了扩散目标与线性时间序列建模。DiffuApriel 在使用 1.3B 参数时，长序列的推理吞吐量比基于 Transformer 的扩散模型高 4.4 倍。我们还提出了 DiffuApriel-H，一种交错使用注意力和 Mamba 层的混合变体，提供高达 2.6 倍的吞吐量改进，并实现了全局和局部上下文建模的平衡。我们的结果显示，双向状态空间架构在屏蔽扩散语言模型中表现出强大的去噪能力，为更快、更高效的文本生成提供了实用且可扩展的基础。', 'title_zh': '打破瓶颈：基于Mamba骨干网的高通量扩散语言模型DiffuApriel'}
{'arxiv_id': 'arXiv:2511.15857', 'title': 'A Crowdsourced Study of ChatBot Influence in Value-Driven Decision Making Scenarios', 'authors': 'Anthony Wise, Xinyi Zhou, Martin Reimann, Anind Dey, Leilani Battle', 'link': 'https://arxiv.org/abs/2511.15857', 'abstract': 'Similar to social media bots that shape public opinion, healthcare and financial decisions, LLM-based ChatBots like ChatGPT can persuade users to alter their behavior. Unlike prior work that persuades via overt-partisan bias or misinformation, we test whether framing alone suffices. We conducted a crowdsourced study, where 336 participants interacted with a neutral or one of two value-framed ChatBots while deciding to alter US defense spending. In this single policy domain with controlled content, participants exposed to value-framed ChatBots significantly changed their budget choices relative to the neutral control. When the frame misaligned with their values, some participants reinforced their original preference, revealing a potentially replicable backfire effect, originally considered rare in the literature. These findings suggest that value-framing alone lowers the barrier for manipulative uses of LLMs, revealing risks distinct from overt bias or misinformation, and clarifying risks to countering misinformation.', 'abstract_zh': '类似于通过塑造公众意见、影响健康和财务决策的社交媒体机器人，基于大语言模型的聊天机器人如ChatGPT也可以说服用户改变行为。不同于以往通过显性的党派偏见或错误信息进行说服的研究，我们测试了仅通过框架化是否足够。我们进行了一项众包研究，其中336名参与者在决定是否改变美国防务支出时与中立的聊天机器人或两种价值观框架的聊天机器人互动。在这一具有控制内容的单一政策领域，接触到价值观框架化聊天机器人的参与者相对于中立控制组在预算选择上显著发生变化。当框架与参与者的价值观不一致时，一些参与者强化了他们最初的观点，揭示了一种可能可复制的回火效应，这种效应在文献中原本被认为是罕见的。这些发现表明，仅通过价值观框架化降低了利用大语言模型进行操纵性使用的门槛，揭示了与显性偏见或错误信息不同的风险，并阐明了对抗错误信息的潜在风险。', 'title_zh': '众包视角下聊天机器人在价值驱动决策场景中的影响研究'}
{'arxiv_id': 'arXiv:2511.15767', 'title': 'TB or Not TB: Coverage-Driven Direct Preference Optimization for Verilog Stimulus Generation', 'authors': 'Bardia Nadimi, Khashayar Filom, Deming Chen, Hao Zheng', 'link': 'https://arxiv.org/abs/2511.15767', 'abstract': 'With the rapid advancement of Large Language Models (LLMs), there is growing interest in applying them to hardware design and verification. Among these stages, design verification remains the most time-consuming and resource-intensive phase, where generating effective stimuli for the design under test (DUT) is both critical and labor-intensive. We present {\\it TB or not TB}, a framework for automated stimulus generation using LLMs fine-tuned through Coverage-Driven Direct Preference Optimization (CD-DPO). To enable preference-based training, we introduce PairaNet, a dataset derived from PyraNet that pairs high- and low-quality testbenches labeled using simulation-derived coverage metrics. The proposed CD-DPO method integrates quantitative coverage feedback directly into the optimization objective, guiding the model toward generating stimuli that maximize verification coverage. Experiments on the CVDP CID12 benchmark show that {\\it TB or not TB} outperforms both open-source and commercial baselines, achieving up to 77.27\\% improvement in code coverage, demonstrating the effectiveness of Coverage-driven preference optimization for LLM-based hardware verification.', 'abstract_zh': '基于覆盖驱动直接偏好优化的大型语言模型自动化刺激生成框架：TB or not TB', 'title_zh': 'TB或不TB：基于覆盖驱动的直接偏好优化以生成Verilog测试刺激信号'}
{'arxiv_id': 'arXiv:2511.15762', 'title': 'A time for monsters: Organizational knowing after LLMs', 'authors': 'Samer Faraj, Joel Perez Torrents, Saku Mantere, Anand Bhardwaj', 'link': 'https://arxiv.org/abs/2511.15762', 'abstract': 'Large Language Models (LLMs) are reshaping organizational knowing by unsettling the epistemological foundations of representational and practice-based perspectives. We conceptualize LLMs as Haraway-ian monsters, that is, hybrid, boundary-crossing entities that destabilize established categories while opening new possibilities for inquiry. Focusing on analogizing as a fundamental driver of knowledge, we examine how LLMs generate connections through large-scale statistical inference. Analyzing their operation across the dimensions of surface/deep analogies and near/far domains, we highlight both their capacity to expand organizational knowing and the epistemic risks they introduce. Building on this, we identify three challenges of living with such epistemic monsters: the transformation of inquiry, the growing need for dialogical vetting, and the redistribution of agency. By foregrounding the entangled dynamics of knowing-with-LLMs, the paper extends organizational theory beyond human-centered epistemologies and invites renewed attention to how knowledge is created, validated, and acted upon in the age of intelligent technologies.', 'abstract_zh': '大型语言模型（LLMs）正在重塑组织认知，动摇表现主义和实践主义视角的 epistemological 基础。我们将 LLMs 视为哈拉维式的怪物，即杂交、跨越边界的实体，它们动摇已建立的类别，同时为 inquiry 开启新的可能性。聚焦于类比作为一种基本的知识驱动因素，我们探讨了 LLMs 通过大规模统计推理生成连接的方式。通过分析其在表面/深层类比和近/远领域维度的操作，我们突出展示了它们扩展组织认知的能力及其引入的 epistemic 风险。在此基础上，我们指出了与这种 epistemic 怪物共存的三大挑战：探究的转变、日益增长的对话验证需求以及代理权的重新分配。通过强调与 LLMs 互动的认知交织动态，本文将组织理论扩展到了以人类为中心的 epistemology 之外，并邀请重新关注智能技术时代知识的创造、验证和付诸行动的过程。', 'title_zh': '怪兽的时代：大语言模型之后的组织认知'}
{'arxiv_id': 'arXiv:2511.15759', 'title': 'Securing AI Agents Against Prompt Injection Attacks', 'authors': 'Badrinath Ramakrishnan, Akshaya Balaji', 'link': 'https://arxiv.org/abs/2511.15759', 'abstract': 'Retrieval-augmented generation (RAG) systems have become widely used for enhancing large language model capabilities, but they introduce significant security vulnerabilities through prompt injection attacks. We present a comprehensive benchmark for evaluating prompt injection risks in RAG-enabled AI agents and propose a multi-layered defense framework. Our benchmark includes 847 adversarial test cases across five attack categories: direct injection, context manipulation, instruction override, data exfiltration, and cross-context contamination. We evaluate three defense mechanisms: content filtering with embedding-based anomaly detection, hierarchical system prompt guardrails, and multi-stage response verification, across seven state-of-the-art language models. Our combined framework reduces successful attack rates from 73.2% to 8.7% while maintaining 94.3% of baseline task performance. We release our benchmark dataset and defense implementation to support future research in AI agent security.', 'abstract_zh': '检索增强生成（RAG）系统已广泛用于增强大型语言模型的能力，但它们通过提示注入攻击引入了重要的安全漏洞。我们提出了一套全面的基准来评估RAG使能的AI代理中的提示注入风险，并建议了一个多层防御框架。我们的基准包括5个攻击类别（直接注入、背景操控、指令覆盖、数据泄露和跨背景污染）中的847个对抗性测试案例。我们评估了三种防御机制：基于嵌入的异常检测内容过滤、分层系统提示限制以及多阶段响应验证，这些都在七个最先进的语言模型上进行了评估。结合我们的框架将成功攻击率从73.2%降低到8.7%，同时保持了94.3%的基础任务性能。我们发布了基准数据集和防御实现，以支持未来在AI代理安全方面的研究。', 'title_zh': '对抗提示注入攻击的AI代理安全保护'}
{'arxiv_id': 'arXiv:2511.15733', 'title': 'Technique to Baseline QE Artefact Generation Aligned to Quality Metrics', 'authors': 'Eitan Farchi, Kiran Nayak, Papia Ghosh Majumdar, Saritha Route', 'link': 'https://arxiv.org/abs/2511.15733', 'abstract': 'Large Language Models (LLMs) are transforming Quality Engineering (QE) by automating the generation of artefacts such as requirements, test cases, and Behavior Driven Development (BDD) scenarios. However, ensuring the quality of these outputs remains a challenge. This paper presents a systematic technique to baseline and evaluate QE artefacts using quantifiable metrics. The approach combines LLM-driven generation, reverse generation , and iterative refinement guided by rubrics technique for clarity, completeness, consistency, and testability. Experimental results across 12 projects show that reverse-generated artefacts can outperform low-quality inputs and maintain high standards when inputs are strong. The framework enables scalable, reliable QE artefact validation, bridging automation with accountability.', 'abstract_zh': '大型语言模型（LLMs）正在通过自动化生成需求、测试用例和行为驱动开发（BDD）场景等 artefacts 来变革质量工程（QE）。然而，确保这些输出的质量仍是一项挑战。本文提出了一种系统化的基线建立和评估 QE artefacts 的方法，该方法利用可量化的指标。该方法结合了基于 LLM 的生成、反向生成以及由评分标准指导的迭代细化，以保证清晰性、完整性、一致性及测试性。跨12个项目的经验研究表明，反向生成的 artefacts 可以超越低质量的输入，并在输入较强时维持高标准。该框架促进了可扩展且可靠的 QE artefacts 验证，实现了自动化与问责制的结合。', 'title_zh': '基线QE Artefact生成技术与质量指标对齐'}
