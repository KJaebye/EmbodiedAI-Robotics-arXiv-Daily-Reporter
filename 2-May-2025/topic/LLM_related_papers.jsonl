{'arxiv_id': 'arXiv:2505.00368', 'title': 'Urban Air Mobility as a System of Systems: An LLM-Enhanced Holonic Approach', 'authors': 'Ahmed R. Sadik, Muhammad Ashfaq, Niko Mäkitalo, Tommi Mikkonen', 'link': 'https://arxiv.org/abs/2505.00368', 'abstract': 'Urban Air Mobility (UAM) is an emerging System of System (SoS) that faces challenges in system architecture, planning, task management, and execution. Traditional architectural approaches struggle with scalability, adaptability, and seamless resource integration within dynamic and complex environments. This paper presents an intelligent holonic architecture that incorporates Large Language Model (LLM) to manage the complexities of UAM. Holons function semi autonomously, allowing for real time coordination among air taxis, ground transport, and vertiports. LLMs process natural language inputs, generate adaptive plans, and manage disruptions such as weather changes or airspace this http URL a case study of multimodal transportation with electric scooters and air taxis, we demonstrate how this architecture enables dynamic resource allocation, real time replanning, and autonomous adaptation without centralized control, creating more resilient and efficient urban transportation networks. By advancing decentralized control and AI driven adaptability, this work lays the groundwork for resilient, human centric UAM ecosystems, with future efforts targeting hybrid AI integration and real world validation.', 'abstract_zh': '城市空中交通(UAM)是一种新兴的系统体系结构(SoS)，面临系统架构、规划、任务管理和执行等方面的挑战。传统的架构方法难以应对动态复杂环境中规模性、适应性和无缝资源集成的需求。本文提出了一种智能holonic架构，结合大型语言模型(LLM)来管理UAM的复杂性。holons半自主运行，允许空中出租车、地面运输和 vertiports 之间实时协调。LLM处理自然语言输入，生成适应性计划，并管理天气变化或空域更改等中断。通过一个涉及电动滑板车和空中出租车的多模式交通案例研究，我们展示了该架构如何实现动态资源分配、实时再规划和无需中心控制的自主适应，从而构建更具有韧性和效率的城市交通网络。通过推进去中心化控制和AI驱动的适应性，这项工作为韧性的人本中心UAM生态系统奠定了基础，未来努力将集中在混合AI集成和实地验证上。', 'title_zh': '城市空中移动作为系统_of_系统：基于LLM的holonic方法'}
{'arxiv_id': 'arXiv:2505.00651', 'title': 'Open-Source LLM-Driven Federated Transformer for Predictive IoV Management', 'authors': 'Yazan Otoum, Arghavan Asad, Ishtiaq Ahmad', 'link': 'https://arxiv.org/abs/2505.00651', 'abstract': 'The proliferation of connected vehicles within the Internet of Vehicles (IoV) ecosystem presents critical challenges in ensuring scalable, real-time, and privacy-preserving traffic management. Existing centralized IoV solutions often suffer from high latency, limited scalability, and reliance on proprietary Artificial Intelligence (AI) models, creating significant barriers to widespread deployment, particularly in dynamic and privacy-sensitive environments. Meanwhile, integrating Large Language Models (LLMs) in vehicular systems remains underexplored, especially concerning prompt optimization and effective utilization in federated contexts. To address these challenges, we propose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel framework that leverages open-source LLMs for predictive IoV management. FPoTT introduces a dynamic prompt optimization mechanism that iteratively refines textual prompts to enhance trajectory prediction. The architecture employs a dual-layer federated learning paradigm, combining lightweight edge models for real-time inference with cloud-based LLMs to retain global intelligence. A Transformer-driven synthetic data generator is incorporated to augment training with diverse, high-fidelity traffic scenarios in the Next Generation Simulation (NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing EleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data while maintaining high performance on synthetic datasets. These results underscore the potential of open-source LLMs in enabling secure, adaptive, and scalable IoV management, offering a promising alternative to proprietary solutions in smart mobility ecosystems.', 'abstract_zh': '联邦提示优化交通变换器：面向物联网汽车的开放式提示优化预测框架', 'title_zh': '开源LLM驱动的联邦变换器预测IoV管理'}
{'arxiv_id': 'arXiv:2505.00610', 'title': 'Combining LLMs with Logic-Based Framework to Explain MCTS', 'authors': 'Ziyan An, Xia Wang, Hendrik Baier, Zirong Chen, Abhishek Dubey, Taylor T. Johnson, Jonathan Sprinkle, Ayan Mukhopadhyay, Meiyi Ma', 'link': 'https://arxiv.org/abs/2505.00610', 'abstract': 'In response to the lack of trust in Artificial Intelligence (AI) for sequential planning, we design a Computational Tree Logic-guided large language model (LLM)-based natural language explanation framework designed for the Monte Carlo Tree Search (MCTS) algorithm. MCTS is often considered challenging to interpret due to the complexity of its search trees, but our framework is flexible enough to handle a wide range of free-form post-hoc queries and knowledge-based inquiries centered around MCTS and the Markov Decision Process (MDP) of the application domain. By transforming user queries into logic and variable statements, our framework ensures that the evidence obtained from the search tree remains factually consistent with the underlying environmental dynamics and any constraints in the actual stochastic control process. We evaluate the framework rigorously through quantitative assessments, where it demonstrates strong performance in terms of accuracy and factual consistency.', 'abstract_zh': '针对人工智能（AI）在序列规划中的信任缺失，我们设计了一个由计算树逻辑引导的大语言模型（LLM）为基础的自然语言解释框架，该框架适用于蒙特卡洛树搜索（MCTS）算法。虽然MCTS由于其搜索树的复杂性而常常难以解释，但我们的框架足够灵活，能够处理与MCTS和应用领域马尔可夫决策过程（MDP）相关的广泛自由格式的后验查询和基于知识的询问。通过将用户查询转换为逻辑和变量声明，我们的框架确保从搜索树中获得的证据与基础环境动态及其实际随机控制过程中的任何约束保持事实上的连贯性。我们通过严格的定量评估对该框架进行了评估，结果显示其在准确性和事实一致性方面表现优异。', 'title_zh': '将逻辑推理框架与大语言模型结合以解释MCTS'}
{'arxiv_id': 'arXiv:2505.00603', 'title': 'Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4', 'authors': 'Phanish Puranam, Prothit Sen, Maciej Workiewicz', 'link': 'https://arxiv.org/abs/2505.00603', 'abstract': 'This study investigates whether large language models, specifically GPT4, can match human capabilities in analogical reasoning within strategic decision making contexts. Using a novel experimental design involving source to target matching, we find that GPT4 achieves high recall by retrieving all plausible analogies but suffers from low precision, frequently applying incorrect analogies based on superficial similarities. In contrast, human participants exhibit high precision but low recall, selecting fewer analogies yet with stronger causal alignment. These findings advance theory by identifying matching, the evaluative phase of analogical reasoning, as a distinct step that requires accurate causal mapping beyond simple retrieval. While current LLMs are proficient in generating candidate analogies, humans maintain a comparative advantage in recognizing deep structural similarities across domains. Error analysis reveals that AI errors arise from surface level matching, whereas human errors stem from misinterpretations of causal structure. Taken together, the results suggest a productive division of labor in AI assisted organizational decision making where LLMs may serve as broad analogy generators, while humans act as critical evaluators, applying the most contextually appropriate analogies to strategic problems.', 'abstract_zh': '本研究探讨了大型语言模型，特别是GPT4，在战略决策情境中的类比推理能力是否能与人类相媲美。通过一种新颖的实验设计——源到目标匹配，我们发现GPT4能够通过检索所有合理类比而实现高召回率，但精度较低，经常基于表层相似性应用不正确的类比。相比之下，人类参与者表现出高精度但低召回率，选择较少的类比但因果对齐更强。这些发现通过识别类比推理中的匹配阶段作为需要超越简单检索的准确因果映射的独立步骤，推进了理论发展。尽管当前的大型语言模型在生成候选类比方面表现 proficient，人类在跨领域识别深层结构相似性方面依然具有比较优势。错误分析表明，AI错误源于表层匹配，而人类错误源于对因果结构的误解。综合来看，这些结果表明，在人工智能辅助组织决策中，大型语言模型可能充当广泛的类比生成者，而人类则作为关键的评估者，应用最合适的类比来解决战略性问题。', 'title_zh': '大规模语言模型能否帮助提高战略决策中的类比推理能力？来自人类和GPT-4的实验证据'}
{'arxiv_id': 'arXiv:2505.00204', 'title': "RAIL in the Wild: Operationalizing Responsible AI Evaluation Using Anthropic's Value Dataset", 'authors': 'Sumit Verma, Pritam Prasun, Arpit Jaiswal, Pritish Kumar', 'link': 'https://arxiv.org/abs/2505.00204', 'abstract': 'As AI systems become embedded in real-world applications, ensuring they meet ethical standards is crucial. While existing AI ethics frameworks emphasize fairness, transparency, and accountability, they often lack actionable evaluation methods. This paper introduces a systematic approach using the Responsible AI Labs (RAIL) framework, which includes eight measurable dimensions to assess the normative behavior of large language models (LLMs). We apply this framework to Anthropic\'s "Values in the Wild" dataset, containing over 308,000 anonymized conversations with Claude and more than 3,000 annotated value expressions. Our study maps these values to RAIL dimensions, computes synthetic scores, and provides insights into the ethical behavior of LLMs in real-world use.', 'abstract_zh': '随着AI系统嵌入到实际应用中，确保其符合伦理标准变得至关重要。虽然现有的AI伦理框架强调公平、透明和问责制，但它们往往缺乏可操作的评估方法。本文介绍了使用责任AI实验室（RAIL）框架的一种系统方法，该框架包括八个可测量维度以评估大型语言模型（LLMs）的规范行为。我们将这一框架应用于Anthropic的“野外价值观”数据集，该数据集包含超过308,000次匿名与Claude的对话和超过3,000个标注的价值表达。我们的研究将这些价值观映射到RAIL维度，计算合成评分，并提供关于LLMs在实际应用中伦理行为的见解。', 'title_zh': 'RAIL in the Wild: 使用Anthropic的价值数据集实现负责任AI评估的操作化'}
{'arxiv_id': 'arXiv:2505.00662', 'title': 'DeepCritic: Deliberate Critique with Large Language Models', 'authors': 'Wenkai Yang, Jingwen Chen, Yankai Lin, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2505.00662', 'abstract': 'As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.', 'abstract_zh': '随着大型语言模型（LLMs）的迅速演进，提供准确反馈和可扩展监督其输出成为一个迫切且关键的问题。利用LLMs作为批判模型以实现自动化监督是一种有前途的解决方案。在本文中，我们专注于研究和增强LLMs的数学批判能力。当前的LLM批判性评价过于浅显，导致判断准确性低，并且难以为LLM生成器提供足够的反馈以纠正错误。为解决这一问题，我们提出了一种新颖且有效的两阶段框架，以开发能够在数学解题每个推理步骤上进行刻意批判的LLM批判性评价模型。在第一阶段，我们利用Qwen2.5-72B-Instruct生成4.5K长格式批判性评价作为监督微调的种子数据。每个种子批判性评价包括多角度验证以及对每个推理步骤初始批判性评价的深入批判。然后，我们通过强化学习对微调后的模型进行进一步训练，使用现有的从PRM800K中获得的人工标注数据或通过基于蒙特卡洛采样方法获得的自动标注数据（用于正确性估计），以进一步激励其批判性评价能力。基于Qwen2.5-7B-Instruct开发的批判性评价模型不仅在各种错误识别基准上显著优于现有的LLM批判性评价模型（包括同规模的DeepSeek-R1-distill模型和GPT-4o），还能更有效地通过更详细的反馈帮助LLM生成器改进错误步骤。', 'title_zh': 'DeepCritic: 详尽批判与大规模语言模型'}
{'arxiv_id': 'arXiv:2505.00661', 'title': 'On the generalization of language models from in-context learning and finetuning: a controlled study', 'authors': 'Andrew K. Lampinen, Arslan Chaudhry, Stephanie C.Y. Chan, Cody Wild, Diane Wan, Alex Ku, Jörg Bornschein, Razvan Pascanu, Murray Shanahan, James L. McClelland', 'link': 'https://arxiv.org/abs/2505.00661', 'abstract': "Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning -- from failing to generalize to simple reversals of relations they are trained on, to missing logical deductions that can be made from trained information. These failures to generalize from fine-tuning can hinder practical application of these models. However, language models' in-context learning shows different inductive biases, and can generalize better in some of these cases. Here, we explore these differences in generalization between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' ability to generalize from finetuning data. The datasets are constructed to isolate the knowledge in the dataset from that in pretraining, to create clean tests of generalization. We expose pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance.", 'abstract_zh': '大型语言模型展示了令人兴奋的能力，但在微调后却表现出惊人的狭窄泛化能力——从无法泛化到简单的关系反转，到忽略从训练信息中可以得出的逻辑推理。这些微调后的泛化失败可能阻碍这些模型的实际应用。然而，语言模型的上下文学习显示出不同的归纳偏见，并能在某些情况下更好地泛化。为此，我们构建了多个新的数据集，以评估和改进模型从微调数据中泛化的能力。这些数据集的构建旨在将数据集中的知识与预训练中的知识隔离，以创建干净的泛化测试。我们对预训练的大模型进行控制性的信息暴露，要么在上下文中，要么通过微调，并在需要各种类型泛化的测试集上评估其性能。我们发现，在数据匹配设置中，上下文学习可以比微调更灵活地泛化（尽管我们还发现了一些先前发现的限制性条件，例如微调可以在更广泛的结构中泛化到关系反转的情况）。我们基于这些发现提出了一种方法，以改善从微调中泛化的性能：将上下文推断添加到微调数据中。我们展示了这种方法在我们的数据集和其他基准测试的各种拆分上都提高了泛化能力。我们的研究结果对理解语言模型不同学习模式的归纳偏见具有重要意义，并实际提高了它们的性能。', 'title_zh': '基于上下文学习和微调的语言模型泛化能力的研究：一个受控实验'}
{'arxiv_id': 'arXiv:2505.00654', 'title': 'Large Language Models Understanding: an Inherent Ambiguity Barrier', 'authors': 'Daniel N. Nissani', 'link': 'https://arxiv.org/abs/2505.00654', 'abstract': 'A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean.', 'abstract_zh': '一场生动的持续争论正在进行，关于大型语言模型（LLMs）的能力，即它们理解世界并捕捉参与对话含义的能力。基于思辨实验、LLMs与人类的 anecdotal 对话、统计语言学分析、哲学考量等提出的论点和反论点已经提出。在本文中，我们基于一个思辨实验和半形式化的考量提出一个反论点，指出这种固有的歧义障碍阻止LLMs理解其惊人流畅的对话意味着什么。', 'title_zh': '大型语言模型理解：固有的歧义障碍'}
{'arxiv_id': 'arXiv:2505.00626', 'title': 'The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)', 'authors': 'Zihao Wang, Yibo Jiang, Jiahao Yu, Heqing Huang', 'link': 'https://arxiv.org/abs/2505.00626', 'abstract': "Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \\emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \\emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \\emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \\emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.", 'abstract_zh': '大型语言模型（LLMs）整合了多种输入角色（如系统指令、用户查询、外部工具输出）的比例日益增加。确保模型准确区分每个角色的信息——我们称之为“角色分离”——对于一致的多角色行为至关重要。尽管近期研究往往针对最先进的提示注入防御方法，但尚不清楚这些方法是否真正教会LLMs区分角色，还是仅仅记忆已知触发器。在本文中，我们研究了“角色分离学习”：教会LLMs稳健地区分系统和用户标记的过程。通过一个简单的可控实验框架，我们发现微调模型通常依赖于两种角色识别的捷径：（1）任务类型利用，（2）接近文本起始位置。尽管数据增强可在一定程度上减轻这些捷径的影响，但它通常导致迭代修补而不是根本解决。为解决这一问题，我们提出加强具有标志性的信号来标记角色边界，通过调整模型输入编码中的标记级提示来进行。特别是，操纵位置ID有助于模型学习更清晰的区分，减少对表面捷径的依赖。通过聚焦于机制中心的观点，我们的工作揭示了如何使LLMs更可靠地保持一致的多角色行为，而不仅仅是记忆已知提示或触发器。', 'title_zh': '角色分离的错觉：LLM角色学习中的隐藏捷径及其修复方法'}
{'arxiv_id': 'arXiv:2505.00624', 'title': 'FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation', 'authors': 'Chaitali Bhattacharyya, Yeseong Kim', 'link': 'https://arxiv.org/abs/2505.00624', 'abstract': 'Training large language models (LLMs) from scratch requires significant computational resources, driving interest in developing smaller, domain-specific LLMs that maintain both efficiency and strong task performance. Medium-sized models such as LLaMA, llama} have served as starting points for domain-specific adaptation, but they often suffer from accuracy degradation when tested on specialized datasets. We introduce FineScope, a framework for deriving compact, domain-optimized LLMs from larger pretrained models. FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its ability to produce interpretable feature representations, to extract domain-specific subsets from large datasets. We apply structured pruning with domain-specific constraints, ensuring that the resulting pruned models retain essential knowledge for the target domain. To further enhance performance, these pruned models undergo self-data distillation, leveraging SAE-curated datasets to restore key domain-specific information lost during pruning. Extensive experiments and ablation studies demonstrate that FineScope achieves highly competitive performance, outperforming several large-scale state-of-the-art LLMs in domain-specific tasks. Additionally, our results show that FineScope enables pruned models to regain a substantial portion of their original performance when fine-tuned with SAE-curated datasets. Furthermore, applying these datasets to fine-tune pretrained LLMs without pruning also improves their domain-specific accuracy, highlighting the robustness of our approach. The code will be released.', 'abstract_zh': '一种用于 derived 域优化大语言模型的 FineScope 框架', 'title_zh': 'FineScope : 基于SAE引导的自数据培养的领域专用大型语言模型精确定量剪枝'}
{'arxiv_id': 'arXiv:2505.00570', 'title': 'FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension', 'authors': 'Jushi Kai, Boyi Zeng, Yixuan Wang, Haoli Bai, Bo Jiang, Zhouhan Lin', 'link': 'https://arxiv.org/abs/2505.00570', 'abstract': 'Extending the context window in large language models (LLMs) is essential for applications involving long-form content generation. However, the linear increase in key-value (KV) cache memory requirements and the quadratic complexity of self-attention with respect to sequence length present significant challenges during fine-tuning and inference. Existing methods suffer from performance degradation when extending to longer contexts. In this work, we introduce a novel context extension method that optimizes both fine-tuning and inference efficiency. Our method exploits a key observation: in the frequency domain, the energy distribution of the KV cache is primarily concentrated in low-frequency components. By filtering out the high-frequency components, the KV cache can be effectively compressed with minimal information loss. Building on this insight, we propose an efficient compression technique, FreqKV, that iteratively compresses the increasing KV cache to a fixed size in the frequency domain, applicable to both fine-tuning and inference. FreqKV introduces no additional parameters or architectural modifications. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window efficiently. Experiments on various long context language modeling and understanding tasks demonstrate the efficiency and efficacy of the proposed method.', 'abstract_zh': '扩展大型语言模型上下文窗口的方法：一种优化fine-tuning和推理效率的新技术', 'title_zh': 'FreqKV: 频域键值压缩以实现高效上下文窗口扩展'}
{'arxiv_id': 'arXiv:2505.00557', 'title': 'Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models', 'authors': 'Makoto Sato', 'link': 'https://arxiv.org/abs/2505.00557', 'abstract': 'Hallucinations in large language models (LLMs) present a growing challenge across real-world applications, from healthcare to law, where factual reliability is essential. Despite advances in alignment and instruction tuning, LLMs can still generate outputs that are fluent yet fundamentally untrue. Understanding the cognitive dynamics that underlie these hallucinations remains an open problem. In this study, we propose a prompt-based framework to systematically trigger and quantify hallucination: a Hallucination-Inducing Prompt (HIP), which synthetically fuses semantically distant concepts (e.g., periodic table of elements and tarot divination) in a misleading way, and a Hallucination Quantifying Prompt (HQP), which scores the plausibility, confidence, and coherence of the output. Controlled experiments across multiple LLMs revealed that HIPs consistently produced less coherent and more hallucinated responses than their null-fusion controls. These effects varied across models, with reasoning-oriented LLMs showing distinct profiles from general-purpose ones. Our framework provides a reproducible testbed for studying hallucination vulnerability, and opens the door to developing safer, more introspective LLMs that can detect and self-regulate the onset of conceptual instability.', 'abstract_zh': '大型语言模型中的幻觉现象在从医疗到法律等实际应用领域构成了日益严峻的挑战，其中事实可靠性至关重要。尽管在对齐和指令调优方面取得了进步，大型语言模型仍然可以生成流畅而根本上不真实的内容。理解这些幻觉所 underlying 的认知机制仍然是一个开放的问题。在本研究中，我们提出了一种基于提示的框架，以系统地触发和量化幻觉：一种幻觉诱导提示（HIP），它以误导性的方式综合了语义上相距甚远的概念（例如，元素周期表和塔罗占卜），以及一种幻觉量化提示（HQP），用于评估输出的合理性、信心和一致性。通过对多个大型语言模型进行受控实验，我们发现HIPs产生了比其无融合对照组更不连贯且更多的幻觉反应。这些效应在不同模型之间有所不同，推理导向的大型语言模型与通用型模型显示出不同的特征。我们的框架提供了一个可重复的研究平台，用于研究幻觉脆弱性，并为开发更安全、更具内省能力的大型语言模型打开了大门，这些模型能够检测并自我调节概念不稳定的开始。', 'title_zh': '在LLMs中诱发幻觉：大规模语言模型基于提示诱导幻觉的定量研究'}
{'arxiv_id': 'arXiv:2505.00506', 'title': 'HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection', 'authors': 'Deanna Emery, Michael Goitia, Freddie Vargus, Iulia Neagu', 'link': 'https://arxiv.org/abs/2505.00506', 'abstract': 'As large language models (LLMs) are increasingly deployed in high-stakes domains, detecting hallucinated content$\\unicode{x2013}$text that is not grounded in supporting evidence$\\unicode{x2013}$has become a critical challenge. Existing benchmarks for hallucination detection are often synthetically generated, narrowly focused on extractive question answering, and fail to capture the complexity of real-world scenarios involving multi-document contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a diverse, task-agnostic dataset that includes examples from a range of domains and formats. Using this benchmark, we evaluate seven hallucination detection systems$\\unicode{x2013}$both open and closed source$\\unicode{x2013}$highlighting differences in performance across tasks, document lengths, and input representations. Our analysis highlights substantial performance disparities between short and long contexts, with critical implications for real-world Retrieval Augmented Generation (RAG) implementations. Quotient Detections achieves the best overall performance, with an accuracy of 0.82 and an F1 score of 0.84.', 'abstract_zh': '随着大型语言模型（LLMs）在高风险领域中的广泛应用，检测幻觉内容（即缺乏支持证据的文字）已成为一项关键挑战。现有的幻觉检测基准通常是由合成数据生成的，专注于提取式问答，并未能捕捉到涉及多文档上下文和完整句子输出的现实世界场景的复杂性。我们引入了HalluMix基准，这是一个多样化的、任务无关的数据集，包含多种领域和格式的示例。使用此基准，我们评估了七种幻觉检测系统（包括开源和闭源系统），展示了不同任务、文档长度和输入表示下的性能差异。我们的分析指出了短和长上下文之间显著的性能差异，对于现实世界的检索增强生成（RAG）实现具有关键性影响。Quotient Detections在整体性能上表现最佳，准确率为0.82，F1分为0.84。', 'title_zh': 'HalluMix: 一种任务无关的多领域现实世界hallucination检测基准'}
{'arxiv_id': 'arXiv:2505.00467', 'title': 'Red Teaming Large Language Models for Healthcare', 'authors': 'Vahid Balazadeh, Michael Cooper, David Pellow, Atousa Assadi, Jennifer Bell, Jim Fackler, Gabriel Funingana, Spencer Gable-Cook, Anirudh Gangadhar, Abhishek Jaiswal, Sumanth Kaja, Christopher Khoury, Randy Lin, Kaden McKeen, Sara Naimimohasses, Khashayar Namdar, Aviraj Newatia, Allan Pang, Anshul Pattoo, Sameer Peesapati, Diana Prepelita, Bogdana Rakova, Saba Sadatamin, Rafael Schulman, Ajay Shah, Syed Azhar Shah, Syed Ahmar Shah, Babak Taati, Balagopal Unnikrishnan, Stephanie Williams, Rahul G Krishnan', 'link': 'https://arxiv.org/abs/2505.00467', 'abstract': 'We present the design process and findings of the pre-conference workshop at the Machine Learning for Healthcare Conference (2024) entitled Red Teaming Large Language Models for Healthcare, which took place on August 15, 2024. Conference participants, comprising a mix of computational and clinical expertise, attempted to discover vulnerabilities -- realistic clinical prompts for which a large language model (LLM) outputs a response that could cause clinical harm. Red-teaming with clinicians enables the identification of LLM vulnerabilities that may not be recognised by LLM developers lacking clinical expertise. We report the vulnerabilities found, categorise them, and present the results of a replication study assessing the vulnerabilities across all LLMs provided.', 'abstract_zh': '我们 presents 机器学习医疗大会（2024）前会议工作会议《红队测试医疗领域的大型语言模型》的设计过程与发现，该会议于2024年8月15日举行。会议参与者包括计算和临床专业背景的混合群体，他们尝试发现漏洞——即大型语言模型（LLM）对临床有害响应的临床提示。临床红队测试有助于识别缺乏临床背景的大型语言模型开发者可能未意识到的漏洞。我们报告发现的漏洞，对其进行分类，并展示一项复制研究的结果，评估这些漏洞在所有提供的大型语言模型中的一致性。', 'title_zh': '面向医疗健康的红队测试大型语言模型'}
{'arxiv_id': 'arXiv:2505.00367', 'title': 'KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis', 'authors': 'JunSeo Kim, HyeHyeon Kim', 'link': 'https://arxiv.org/abs/2505.00367', 'abstract': 'Cognitive distortion refers to negative thinking patterns that can lead to mental health issues like depression and anxiety in adolescents. Previous studies using natural language processing (NLP) have focused mainly on small-scale adult datasets, with limited research on adolescents. This study introduces KoACD, the first large-scale dataset of cognitive distortions in Korean adolescents, containing 108,717 instances. We applied a multi-Large Language Model (LLM) negotiation method to refine distortion classification and generate synthetic data using two approaches: cognitive clarification for textual clarity and cognitive balancing for diverse distortion representation. Validation through LLMs and expert evaluations showed that while LLMs classified distortions with explicit markers, they struggled with context-dependent reasoning, where human evaluators demonstrated higher accuracy. KoACD aims to enhance future research on cognitive distortion detection.', 'abstract_zh': '认知 distortion 在青少年中的消极思维模式与其心理健康问题如抑郁和焦虑有关。 previous 研究主要使用自然语言处理 (NLP) 技术集中在小型成人数据集上，对青少年的研究较少。本研究介绍了 KoACD，这是第一个包含 108,717 个实例的韩语青少年认知 distortion 大规模数据集。我们应用多大型语言模型 (LLM) 协商方法细化 distortion 分类，并使用两种方法生成合成数据：认知澄清以提高文本清晰度，认知平衡以实现多样化的 distortion 表现。通过大型语言模型和专家评估的验证显示，虽然 LLM 能够分类具有明确标记的 distortion，但在依赖上下文的推理方面却表现不佳，人类评估者表现出更高的准确性。KoACD旨在增强未来关于认知 distortion 检测的研究。', 'title_zh': 'KoACD: 首个青少年认知扭曲分析数据集'}
{'arxiv_id': 'arXiv:2505.00358', 'title': 'R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training', 'authors': 'Albert Ge, Tzu-Heng Huang, John Cooper, Avi Trost, Ziyi Chu, Satya Sai Srinath Namburi GNVV, Ziyang Cai, Kendall Park, Nicholas Roberts, Frederic Sala', 'link': 'https://arxiv.org/abs/2505.00358', 'abstract': "Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&B matches or exceeds the performance of state-of-the-art data mixing strategies.", 'abstract_zh': '数据混合作策略已成功降低了训练语言模型的成本。然而，这些方法存在两个缺陷。首先，它们依赖于预先定义的数据领域（例如，数据来源、任务类型），这可能会错过关键的语义细微差别，从而导致性能的损失。其次，这些方法随着领域的数量增加在计算上变得不可行。我们通过R&B框架解决这些挑战，该框架基于语义相似性重新分区训练数据（Regroup），并通过利用训练过程中获得的领域梯度诱导的Gram矩阵来高效优化数据组成（Balance）。与先前的工作不同，它消除了获取评估信息（如损失或梯度）所需的额外计算需求。我们在此标准正则条件下分析此技术，并提供理论见解以证明R&B相对于非自适应混合作方法的有效性。实验结果显示，R&B在五个不同的数据集上有效，这些数据集涵盖了从自然语言处理到推理和多模态任务的范围。即使是额外计算开销仅为0.01%，R&B也能匹配或超越最先进的数据混合作策略的性能。', 'title_zh': 'R&B：领域重新分组与数据混合平衡的高效基础模型训练'}
{'arxiv_id': 'arXiv:2505.00350', 'title': 'Optimizing Deep Neural Networks using Safety-Guided Self Compression', 'authors': 'Mohammad Zbeeb, Mariam Salman, Mohammad Bazzi, Ammar Mohanna', 'link': 'https://arxiv.org/abs/2505.00350', 'abstract': 'The deployment of deep neural networks on resource-constrained devices necessitates effective model com- pression strategies that judiciously balance the reduction of model size with the preservation of performance. This study introduces a novel safety-driven quantization framework that leverages preservation sets to systematically prune and quantize neural network weights, thereby optimizing model complexity without compromising accuracy. The proposed methodology is rigorously evaluated on both a convolutional neural network (CNN) and an attention-based language model, demonstrating its applicability across diverse architectural paradigms. Experimental results reveal that our framework achieves up to a 2.5% enhancement in test accuracy relative to the original unquantized models while maintaining 60% of the initial model size. In comparison to conventional quantization techniques, our approach not only augments generalization by eliminating parameter noise and retaining essential weights but also reduces variance, thereby ensuring the retention of critical model features. These findings underscore the efficacy of safety-driven quantization as a robust and reliable strategy for the efficient optimization of deep learn- ing models. The implementation and comprehensive experimental evaluations of our framework are publicly accessible at GitHub.', 'abstract_zh': '在资源受限设备上部署深度神经网络需要有效的模型压缩策略，以适度地平衡模型大小的减小与性能的保留。本研究介绍了一种新的安全性驱动的量化框架，该框架利用保留集系统地剪枝和量化神经网络权重，从而在不牺牲准确性的前提下优化模型复杂度。所提出的方法在卷积神经网络（CNN）和基于注意力的语言模型上进行了严格的评估，证明了其在不同架构范式中的适用性。实验结果表明，与原始未量化模型相比，该框架在测试准确率上提高了2.5%，同时保持了初始模型大小的60%。与传统量化技术相比，我们的方法不仅通过消除参数噪声并保留关键权重来增强泛化能力，还减少了变异，从而确保关键模型特征的保留。这些发现强调了安全性驱动量化作为深度学习模型高效优化稳健可靠策略的有效性。我们的框架的实现和全面的实验评估可以在GitHub上公开访问。', 'title_zh': '使用安全性引导自我压缩优化深度神经网络'}
{'arxiv_id': 'arXiv:2505.00268', 'title': 'Consistency in Language Models: Current Landscape, Challenges, and Future Directions', 'authors': 'Jekaterina Novikova, Carol Anderson, Borhane Blili-Hamelin, Subhabrata Majumdar', 'link': 'https://arxiv.org/abs/2505.00268', 'abstract': 'The hallmark of effective language use lies in consistency -- expressing similar meanings in similar contexts and avoiding contradictions. While human communication naturally demonstrates this principle, state-of-the-art language models struggle to maintain reliable consistency across different scenarios. This paper examines the landscape of consistency research in AI language systems, exploring both formal consistency (including logical rule adherence) and informal consistency (such as moral and factual coherence). We analyze current approaches to measure aspects of consistency, identify critical research gaps in standardization of definitions, multilingual assessment, and methods to improve consistency. Our findings point to an urgent need for robust benchmarks to measure and interdisciplinary approaches to ensure consistency in the application of language models on domain-specific tasks while preserving the utility and adaptability.', 'abstract_zh': '有效语言运用的标志性特征在于一致性——在类似的情境下表达相似的意义并避免矛盾。尽管人类通信自然地体现了这一原则，但最先进的语言模型在不同场景下保持可靠一致性的能力仍有待提高。本文探讨了人工智能语言系统中一致性的研究现状，探讨了正式一致性（包括逻辑规则合规性）和非正式一致性（如道德和事实的一致性）。我们分析了当前衡量一致性各个方面的方法，指出了标准化定义、多语言评估和提高一致性的方法方面的关键研究缺口。我们的研究结果指出了在特定领域任务中衡量和确保语言模型一致性的重要需求，同时保持其实用性和适应性。', 'title_zh': '语言模型的一致性：当前概览、挑战及未来方向'}
{'arxiv_id': 'arXiv:2505.00240', 'title': 'LLM-Based Threat Detection and Prevention Framework for IoT Ecosystems', 'authors': 'Yazan Otoum, Arghavan Asad, Amiya Nayak', 'link': 'https://arxiv.org/abs/2505.00240', 'abstract': 'The increasing complexity and scale of the Internet of Things (IoT) have made security a critical concern. This paper presents a novel Large Language Model (LLM)-based framework for comprehensive threat detection and prevention in IoT environments. The system integrates lightweight LLMs fine-tuned on IoT-specific datasets (IoT-23, TON_IoT) for real-time anomaly detection and automated, context-aware mitigation strategies optimized for resource-constrained devices. A modular Docker-based deployment enables scalable and reproducible evaluation across diverse network conditions. Experimental results in simulated IoT environments demonstrate significant improvements in detection accuracy, response latency, and resource efficiency over traditional security methods. The proposed framework highlights the potential of LLM-driven, autonomous security solutions for future IoT ecosystems.', 'abstract_zh': '物联网环境中的大型语言模型驱动的全面威胁检测与预防框架', 'title_zh': '基于LLM的物联网生态系统威胁检测与预防框架'}
{'arxiv_id': 'arXiv:2505.00232', 'title': 'Scaling On-Device GPU Inference for Large Generative Models', 'authors': 'Jiuqiang Tang, Raman Sarokin, Ekaterina Ignasheva, Grant Jensen, Lin Chen, Juhyun Lee, Andrei Kulik, Matthias Grundmann', 'link': 'https://arxiv.org/abs/2505.00232', 'abstract': 'Driven by the advancements in generative AI, large machine learning models have revolutionized domains such as image processing, audio synthesis, and speech recognition. While server-based deployments remain the locus of peak performance, the imperative for on-device inference, necessitated by privacy and efficiency considerations, persists. Recognizing GPUs as the on-device ML accelerator with the widest reach, we present ML Drift--an optimized framework that extends the capabilities of state-of-the-art GPU-accelerated inference engines. ML Drift enables on-device execution of generative AI workloads which contain 10 to 100x more parameters than existing on-device generative AI models. ML Drift addresses intricate engineering challenges associated with cross-GPU API development, and ensures broad compatibility across mobile and desktop/laptop platforms, thereby facilitating the deployment of significantly more complex models on resource-constrained devices. Our GPU-accelerated ML/AI inference engine achieves an order-of-magnitude performance improvement relative to existing open-source GPU inference engines.', 'abstract_zh': '基于生成型AI技术的进步，大型机器学习模型在图像处理、音频合成和语音识别等领域引发了革命。虽然基于服务器的部署仍然是高性能的焦点，但由隐私和效率考量驱动的设备端推理需求仍然重要。鉴于GPU在设备端机器学习加速器中的广泛应用，我们提出了一种优化框架ML Drift，该框架扩展了最先进的GPU加速推理引擎的功能。ML Drift允许执行包含比现有设备端生成型AI模型多10到100倍参数的工作负载。ML Drift解决了跨GPU API开发的复杂工程挑战，并确保在移动和桌面/笔记本平台之间具有广泛的兼容性，从而在资源受限的设备上部署更为复杂的模型。我们的GPU加速机器学习/人工智能推理引擎在性能上比现有开源GPU推理引擎提升了数量级。', 'title_zh': '在设备上扩展大型生成模型的GPU推理'}
{'arxiv_id': 'arXiv:2505.00127', 'title': 'Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs', 'authors': 'Jinyan Su, Jennifer Healey, Preslav Nakov, Claire Cardie', 'link': 'https://arxiv.org/abs/2505.00127', 'abstract': "Large language models (LLMs) are increasingly optimized for long reasoning, under the assumption that more reasoning leads to better performance. However, emerging evidence suggests that longer responses can sometimes degrade accuracy rather than improve it. In this paper, we conduct a systematic empirical study of the relationship between reasoning length and answer correctness. We find that LLMs tend to overthink simple problems, generating unnecessarily long outputs, and underthink harder ones, failing to extend their reasoning when it is most needed. This indicates that models might misjudge problem difficulty and fail to calibrate their response length appropriately. Furthermore, we investigate the effects of length reduction with a preference optimization algorithm when simply preferring the shorter responses regardless of answer correctness. Experiments show that the generation length can be significantly reduced while maintaining acceptable accuracy. Our findings highlight generation length as a meaningful signal for reasoning behavior and motivate further exploration into LLMs' self-awareness in reasoning length adaptation.", 'abstract_zh': '大型语言模型（LLMs）越来越多地被优化以进行长时间推理，假设更多的推理能带来更好的性能。然而，新兴的证据表明，较长的回答有时会降低准确性而不是提高它。在本文中，我们进行了一项系统的实证研究，探讨推理长度与答案正确性之间的关系。我们发现，LLMs往往会过度思考简单问题，产生不必要的长输出，并且在更难的问题上思考不足，未能在其最需要时扩展推理。这表明模型可能错误地判断问题的难度，并未能适当校准其回答长度。此外，我们通过偏好优化算法研究了在不考虑答案正确性的情况下偏好较短回答时长度减少的影响。实验结果显示，生成长度可以显著减少同时保持可接受的准确性。我们的研究结果突出了生成长度作为推理行为的有意义信号，并激发进一步探索LLMs在推理长度调整方面的自我意识。', 'title_zh': '在欠思考与过思考之间：对LLMs推理长度与正确性的实证研究'}
{'arxiv_id': 'arXiv:2505.00114', 'title': 'Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese', 'authors': 'Silvana Yakhni, Ali Chehab', 'link': 'https://arxiv.org/abs/2505.00114', 'abstract': 'This paper examines the effectiveness of Large Language Models (LLMs) in translating the low-resource Lebanese dialect, focusing on the impact of culturally authentic data versus larger translated datasets. We compare three fine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using open-source Aya23 models. Experiments reveal that models fine-tuned on a smaller but culturally aware Lebanese dataset (LW) consistently outperform those trained on larger, non-native data. The best results were achieved through contrastive fine-tuning paired with contrastive prompting, which indicates the benefits of exposing translation models to bad examples. In addition, to ensure authentic evaluation, we introduce LebEval, a new benchmark derived from native Lebanese content, and compare it to the existing FLoRes benchmark. Our findings challenge the "More Data is Better" paradigm and emphasize the crucial role of cultural authenticity in dialectal translation. We made our datasets and code available on Github.', 'abstract_zh': '本文考察了大型语言模型（LLMs）在翻译低资源黎巴嫩方言方面的有效性，重点关注文化内涵数据与大规模翻译数据集的影响。我们比较了三种微调方法：基础方法、对比方法和语法提示微调，使用开源Aya23模型。实验显示，使用较小但文化意识较强的黎巴嫩数据集（LW）进行微调的模型，优于使用更大规模非母语数据集进行训练的模型。最佳效果通过结合使用对比微调和对比提示实现，这表明暴露翻译模型于不良示例的益处。此外，为确保评估的真实性和准确性，我们引入了LebEval这一新的基准，该基准源自母语黎巴嫩内容，并将其与现有的FLoRes基准进行比较。我们的研究结果挑战了“数据越多越好”的观念，强调了方言翻译中文化真实性的重要性。我们已在Github上发布了数据集和代码。', 'title_zh': 'Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese方言的低资源语料库迁移学习：以黎巴嫩方言为例'}
{'arxiv_id': 'arXiv:2505.00060', 'title': 'Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5', 'authors': 'Jeho Choi', 'link': 'https://arxiv.org/abs/2505.00060', 'abstract': "Large Language Models (LLMs) have shown promise in enabling natural language interfaces for structured data querying through text-to-SQL generation. However, their application in real-world Business Intelligence (BI) contexts remains limited due to semantic hallucinations, structural errors, and a lack of domain-specific evaluation frameworks. In this study, we propose a Fact-Consistency Evaluation Framework for assessing the semantic accuracy of LLM-generated SQL outputs using Exaone 3.5--an instruction-tuned, bilingual LLM optimized for enterprise tasks. We construct a domain-specific benchmark comprising 219 natural language business questions across five SQL complexity levels, derived from actual sales data in LG Electronics' internal BigQuery environment. Each question is paired with a gold-standard SQL query and a validated ground-truth answer. We evaluate model performance using answer accuracy, execution success rate, semantic error rate, and non-response rate. Experimental results show that while Exaone 3.5 performs well on simple aggregation tasks (93% accuracy in L1), it exhibits substantial degradation in arithmetic reasoning (4% accuracy in H1) and grouped ranking tasks (31% in H4), with semantic errors and non-responses concentrated in complex cases. Qualitative error analysis further identifies common failure types such as misapplied arithmetic logic, incomplete filtering, and incorrect grouping operations. Our findings highlight the current limitations of LLMs in business-critical environments and underscore the need for fact-consistency validation layers and hybrid reasoning approaches. This work contributes a reproducible benchmark and evaluation methodology for advancing reliable natural language interfaces to structured enterprise data systems.", 'abstract_zh': '大型语言模型（LLMs）在通过文本生成SQL查询实现结构化数据查询自然语言接口方面显示出潜力。然而，它们在实际商业智能（BI）环境中的应用受限于语义幻觉、结构错误以及缺乏特定领域的评估框架。在本研究中，我们提出了一种基于Exaone 3.5的语义一致性评估框架，用于评估LLM生成的SQL输出的语义准确性，Exaone 3.5是一种指令调优的双语LLM，专为企业任务优化。我们构建了一个包含219个覆盖五种SQL复杂度级别的自然语言企业问题的特定领域基准，这些问题是根据LG电子内部BigQuery环境的实际销售数据得出的。每个问题都配有一个金标准SQL查询和一个验证过的正确答案。我们使用答案准确性、执行成功率、语义错误率和无回应率来评估模型性能。实验结果表明，Exaone 3.5在简单聚合任务上表现良好（L1准确率为93%），但在算术推理（H1准确率为4%）和分组排序任务（H4准确率为31%）上表现出显著下降，语义错误和无回应率集中在复杂情况中。定性的错误分析进一步揭示了常见的错误类型，如错误应用算术逻辑、不完整的过滤和不正确的分组操作。我们的研究结果突显了LLMs在关键商业环境中的当前局限性，并强调了需要事实一致性验证层和混合推理方法的重要性。本研究为推进可靠自然语言接口到结构化企业数据系统提供了可重复基准和评估方法。', 'title_zh': '使用Exaone 3.5对商务智能中文本到SQL生成的事实一致性评估'}
{'arxiv_id': 'arXiv:2505.00034', 'title': 'Improving Phishing Email Detection Performance of Small Large Language Models', 'authors': 'Zijie Lin, Zikang Liu, Hanbo Fan', 'link': 'https://arxiv.org/abs/2505.00034', 'abstract': 'Large language models(LLMs) have demonstrated remarkable performance on many natural language processing(NLP) tasks and have been employed in phishing email detection research. However, in current studies, well-performing LLMs typically contain billions or even tens of billions of parameters, requiring enormous computational resources. To reduce computational costs, we investigated the effectiveness of small-parameter LLMs for phishing email detection. These LLMs have around 3 billion parameters and can run on consumer-grade GPUs. However, small LLMs often perform poorly in phishing email detection task. To address these issues, we designed a set of methods including Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email detection capabilities of small LLMs. We validated the effectiveness of our approach through experiments, significantly improving accuracy on the SpamAssassin dataset from around 0.5 for baseline models like Qwen2.5-1.5B-Instruct to 0.976.', 'abstract_zh': '小型参数大型语言模型在钓鱼邮件检测中的有效性研究及其改进方法', 'title_zh': '提升小型大型语言模型 phishing 邮件检测性能'}
{'arxiv_id': 'arXiv:2505.00032', 'title': 'MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis', 'authors': 'Yuyang Sha, Hongxin Pan, Wei Xu, Weiyu Meng, Gang Luo, Xinyu Du, Xiaobing Zhai, Henry H. Y. Tong, Caijuan Shi, Kefeng Li', 'link': 'https://arxiv.org/abs/2505.00032', 'abstract': 'Major depressive disorder (MDD) impacts more than 300 million people worldwide, highlighting a significant public health issue. However, the uneven distribution of medical resources and the complexity of diagnostic methods have resulted in inadequate attention to this disorder in numerous countries and regions. This paper introduces a high-performance MDD diagnosis tool named MDD-LLM, an AI-driven framework that utilizes fine-tuned large language models (LLMs) and extensive real-world samples to tackle challenges in MDD diagnosis. Therefore, we select 274,348 individual information from the UK Biobank cohort to train and evaluate the proposed method. Specifically, we select 274,348 individual records from the UK Biobank cohort and design a tabular data transformation method to create a large corpus for training and evaluating the proposed approach. To illustrate the advantages of MDD-LLM, we perform comprehensive experiments and provide several comparative analyses against existing model-based solutions across multiple evaluation metrics. Experimental results show that MDD-LLM (70B) achieves an accuracy of 0.8378 and an AUC of 0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming existing machine learning and deep learning frameworks for MDD diagnosis. Given the limited exploration of LLMs in MDD diagnosis, we examine numerous factors that may influence the performance of our proposed method, such as tabular data transformation techniques and different fine-tuning strategies.', 'abstract_zh': '重大抑郁障碍（MDD）影响全球超过30亿人，凸显出一个重要的公共健康问题。然而，医疗资源的不均衡分布和诊断方法的复杂性导致了这一障碍在众多国家和地区缺乏足够的重视。本文介绍了一种高性能的MDD诊断工具——MDD-LLM，这是一种基于AI的框架，利用微调的大语言模型（LLMs）和大量的实际样本来应对MDD诊断的挑战。因此，我们从英国生物银行队列中选择了274,348个个体信息进行训练和评估。具体而言，我们从英国生物银行队列中选择了274,348个个体记录，并设计了一种表格数据转换方法，以创建训练和评估所提出方法的大规模语料库。为了说明MDD-LLM的优势，我们进行了全面的实验，并在多个评估指标上与现有的模型解决方案进行了多个对比分析。实验结果显示，MDD-LLM（70B）的准确率为0.8378，AUC为0.8919（95% CI：0.8799 - 0.9040），显著优于现有的机器学习和深度学习框架在MDD诊断中的表现。鉴于在MDD诊断中对LLMs的有限探索，我们研究了可能影响所提出方法性能的多种因素，包括表格数据转换技术和不同的微调策略。', 'title_zh': 'MDD-LLM：面向重大抑郁障碍诊断的大规模语言模型准确性提升'}
{'arxiv_id': 'arXiv:2505.00031', 'title': 'Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving', 'authors': 'Jin Zhang, Flood Sung, Zhilin Yang, Yang Gao, Chongjie Zhang', 'link': 'https://arxiv.org/abs/2505.00031', 'abstract': 'In the field of large language model (LLM) post-training, the effectiveness of utilizing synthetic data generated by the LLM itself has been well-presented. However, a key question remains unaddressed: what essential information should such self-generated data encapsulate? Existing approaches only produce step-by-step problem solutions, and fail to capture the abstract meta-knowledge necessary for generalization across similar problems. Drawing insights from cognitive science, where humans employ high-level abstraction to simplify complex problems before delving into specifics, we introduce a novel self-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains the LLM to formulate anticipatory plans, which serve as abstract meta-knowledge for problem-solving, before engaging with the intricacies of problems. This approach not only outlines the solution generation path but also shields the LLM from the distraction of irrelevant details. During data generation, LEPA first crafts an anticipatory plan based on the problem, and then generates a solution that aligns with both the plan and the problem. LEPA refines the plan through self-reflection, aiming to acquire plans that are instrumental in yielding correct solutions. During model optimization, the LLM is trained to predict both the refined plans and the corresponding solutions. By efficiently extracting and utilizing the anticipatory plans, LEPA demonstrates remarkable superiority over conventional algorithms on various challenging natural language reasoning benchmarks.', 'abstract_zh': '在大型语言模型（LLM）后训练领域，利用LLM本身生成的合成数据的有效性已被充分展示。然而，一个关键问题仍未得到解决：这种自动生成的数据应包含哪些本质信息？现有方法仅生成逐步问题解决方案，未能捕捉跨类似问题泛化的抽象元知识。借鉴认知科学的见解，人类在深入具体问题之前先运用高层次抽象简化复杂问题，我们提出了一种新颖的自训练算法：LEarning to Plan before Answering (LEPA)。LEPA训练LLM在处理具体问题之前制定预见性计划，这些计划作为问题解决的抽象元知识。该方法不仅明确了解决方案生成路径，还使LLM免受无关细节的干扰。在数据生成过程中，LEPA首先基于问题制定预见性计划，然后生成符合计划和问题的解决方案。LEPA通过自我反思精炼计划，以获得有助于产生正确解决方案的计划。在模型优化过程中，LLM被训练预测精炼的计划及其相应的解决方案。通过高效提取和利用预见性计划，LEPA在各种具有挑战性的自然语言推理基准测试中展现出了显著优势。', 'title_zh': '学习在回答之前规划：自我教学的LLM学习抽象计划以解决问题'}
{'arxiv_id': 'arXiv:2505.00026', 'title': 'Theory of Mind in Large Language Models: Assessment and Enhancement', 'authors': 'Ruirui Chen, Weifeng Jiang, Chengwei Qin, Cheston Tan', 'link': 'https://arxiv.org/abs/2505.00026', 'abstract': "Theory of Mind (ToM)-the ability to infer and reason about others' mental states-is fundamental to human social intelligence. As Large Language Models (LLMs) become increasingly integrated into daily life, it is crucial to assess and enhance their capacity to interpret and respond to human mental states. In this paper, we review LLMs' ToM capabilities by examining both evaluation benchmarks and the strategies designed to improve them. We focus on widely adopted story-based benchmarks and provide an in-depth analysis of methods aimed at enhancing ToM in LLMs. Furthermore, we outline promising future research directions informed by recent benchmarks and state-of-the-art approaches. Our survey serves as a valuable resource for researchers interested in advancing LLMs' ToM capabilities.", 'abstract_zh': 'Theory of Mind (ToM)能力——推断和理解他人心理状态的能力——是人类社会智能的基础。随着大型语言模型（LLMs）越来越多地融入日常生活，评估并增强其解读和回应人类心理状态的能力变得至关重要。在本文中，我们通过分析评估基准和提高这些基准的方法来审查LLMs的ToM能力。我们着重探讨广泛采用的故事基准，并对旨在提高LLMs ToM能力的方法进行了深入分析。此外，我们概述了受近期基准和最新方法启发的有 promise 的未来研究方向。我们的综述为致力于推进LLMs ToM能力的研究人员提供了一项宝贵资源。', 'title_zh': '大型语言模型中的理论思维：评估与增强'}
{'arxiv_id': 'arXiv:2505.00025', 'title': 'A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1', 'authors': 'Mingda Zhang, Jianglong Qin', 'link': 'https://arxiv.org/abs/2505.00025', 'abstract': 'In recent years, despite foundation models like DeepSeek-R1 and ChatGPT demonstrating significant capabilities in general tasks, professional knowledge barriers, computational resource requirements, and deployment environment limitations have severely hindered their application in actual medical scenarios. Addressing these challenges, this paper proposes an efficient lightweight medical vertical large language model architecture method, systematically solving the lightweight problem of medical large models from three dimensions: knowledge acquisition, model compression, and computational optimization. At the knowledge acquisition level, a knowledge transfer pipeline is designed from the fine-tuned DeepSeek-R1-Distill-70B teacher model to the DeepSeek-R1-Distill-7B student model, and Low-Rank Adaptation (LoRA) technology is adopted to precisely adjust key attention layers. At the model compression level, compression techniques including 4-bit weight quantization are implemented while preserving the core representation ability for medical reasoning. At the computational optimization level, inference optimization techniques such as Flash Attention acceleration and continuous batching are integrated, and a professional prompt template system is constructed to adapt to different types of medical problems. Experimental results on medical question-answering datasets show that the method proposed in this paper maintains professional accuracy while reducing memory consumption by 64.7\\% and inference latency by 12.4\\%, providing an effective solution for the application of medical large models in resource-constrained environments such as edge computing devices.', 'abstract_zh': '近年来，尽管基础模型如DeepSeek-R1和ChatGPT在通用任务中展现了显著的能力，但专业性知识障碍、计算资源需求和部署环境限制严重阻碍了它们在实际医疗场景中的应用。为应对这些挑战，本文提出了一种高效轻量级医疗垂直大型语言模型架构方法，从三个维度系统地解决了医疗大型模型的轻量化问题：知识获取、模型压缩和计算优化。在知识获取层面，设计了一条从fine-tuned DeepSeek-R1-Distill-70B教师模型到DeepSeek-R1-Distill-7B学生模型的知识转换管道，并采用低秩适应（LoRA）技术精确调整关键注意力层。在模型压缩层面，实现包括4位权重量化在内的压缩技术，同时保留核心表示能力以支持医疗推理。在计算优化层面，集成推理优化技术如Flash Attention加速和连续批量处理，并构建了一个专业的提示模板系统以适应不同类型医疗问题。实验结果表明，本方法在维持专业准确性的同时，减少了64.7%的内存消耗和12.4%的推理延迟，为边缘计算设备等资源受限环境中的医疗大型模型应用提供了有效解决方案。', 'title_zh': '基于DeepSeek R1的医疗垂直大语言模型架构方法'}
{'arxiv_id': 'arXiv:2505.00022', 'title': 'Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation', 'authors': 'Thomas F Burns, Letitia Parcalabescu, Stephan Wäldchen, Michael Barlow, Gregor Ziegltrum, Volker Stampa, Bastian Harren, Björn Deiseroth', 'link': 'https://arxiv.org/abs/2505.00022', 'abstract': 'Scaling data quantity is essential for large language models (LLMs), yet recent findings show that data quality can significantly boost performance and training efficiency. We introduce a German-language dataset curation pipeline that combines heuristic and model-based filtering techniques with synthetic data generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a large-scale German pre-training dataset which draws from: (1) Common Crawl web data, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual, organic web data. We evaluate our dataset by pre-training both a 1B Llama-style model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A comparison on German-language benchmarks, including MMMLU, shows significant performance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage holds at the 8B scale even when FineWeb2 is enriched by human-curated high-quality data sources such as Wikipedia. Our findings support the growing body of evidence that model-based data curation and synthetic data generation can significantly enhance LLM pre-training datasets.', 'abstract_zh': '大规模语言模型（LLMs）的数据量扩展是必要的，然而近期的研究显示，数据质量可以显著提升性能和训练效率。我们提出了一种德语数据集编纂流水线，该流水线结合了启发式和模型基础的过滤技术以及合成数据生成方法。我们利用该流水线创建了Aleph-Alpha-GermanWeb，这是一个大规模的德语预训练数据集，来源于：（1）Common Crawl网页数据，（2）FineWeb2，以及（3）基于实际有机网页数据生成的合成数据。我们通过预训练一个1B的Llama风格模型和一个8B的Tokenizer-Free层次自回归变压器（HAT）来评估我们的数据集。在包括MMMLU等德语基准测试中的比较表明，Aleph-Alpha-GermanWeb在性能上明显优于仅使用FineWeb2的数据。即使FineWeb2通过包含Wikipedia等高质量的人工筛选数据源而得到丰富，这一优势在8B规模下仍然存在。我们的研究结果支持了现有越来越多的证据，即基于模型的数据编纂和合成数据生成可以显著增强LLM预训练数据集的质量。', 'title_zh': 'Aleph-Alpha-GermanWeb：基于模型的数据筛选和合成数据生成以提高德语语言大模型预训练'}
{'arxiv_id': 'arXiv:2505.00020', 'title': 'Beyond Public Access in LLM Pre-Training Data', 'authors': "Sruly Rosenblat, Tim O'Reilly, Ilan Strauss", 'link': 'https://arxiv.org/abs/2505.00020', 'abstract': "Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we apply the DE-COP membership inference attack method to investigate whether OpenAI's large language models were trained on copyrighted content without consent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable model, demonstrates strong recognition of paywalled O'Reilly book content (AUROC = 82\\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast, GPT-3.5 Turbo shows greater relative recognition of publicly accessible O'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge of public or non-public O'Reilly Media content when tested (AUROC $\\approx$ 50\\%). Testing multiple models, with the same cutoff date, helps us account for potential language shifts over time that might bias our findings. These results highlight the urgent need for increased corporate transparency regarding pre-training data sources as a means to develop formal licensing frameworks for AI content training", 'abstract_zh': "使用合法获取的34本O'Reilly Media版权所有书籍数据集，我们应用DE-COP成员推理攻击方法，调查OpenAI的大语言模型是否在未经许可的情况下训练于受版权保护的内容。我们的AUROC评分表明，OpenAI较新的且更具能力的模型GPT-4o对受付费墙保护的O'Reilly书籍内容表现出强烈的识别能力（AUROC=82%），而OpenAI较早的模型GPT-3.5 Turbo则表现出对公开访问的O'Reilly书籍样本的相对更强的识别能力。相比之下，作为更小的模型，GPT-4o Mini在测试中对公开或非公开的O'Reilly Media内容毫无认识（AUROC≈50%）。在同一截止日期测试多个模型有助于我们考虑可能的时间语言变化偏差。这些结果突显了提高企业在预训练数据源方面透明度的迫切需要，作为为AI内容训练制定正式许可框架的手段。", 'title_zh': '超出公共访问范围的LLM预训练数据'}
{'arxiv_id': 'arXiv:2505.00019', 'title': 'An Empirical Study on Prompt Compression for Large Language Models', 'authors': 'Zheng Zhang, Jinyi Li, Yihuai Lan, Xiang Wang, Hao Wang', 'link': 'https://arxiv.org/abs/2505.00019', 'abstract': 'Prompt engineering enables Large Language Models (LLMs) to perform a variety of tasks. However, lengthy prompts significantly increase computational complexity and economic costs. To address this issue, we study six prompt compression methods for LLMs, aiming to reduce prompt length while maintaining LLM response quality. In this paper, we present a comprehensive analysis covering aspects such as generation performance, model hallucinations, efficacy in multimodal tasks, word omission analysis, and more. We evaluate these methods across 13 datasets, including news, scientific articles, commonsense QA, math QA, long-context QA, and VQA datasets. Our experiments reveal that prompt compression has a greater impact on LLM performance in long contexts compared to short ones. In the Longbench evaluation, moderate compression even enhances LLM performance. Our code and data is available at this https URL.', 'abstract_zh': '提示工程使大规模语言模型（LLMs）能够执行多种任务。然而，冗长的提示显著增加了计算复杂性和经济成本。为了解决这一问题，我们研究了六种提示压缩方法，旨在减少提示长度的同时保持LLM响应质量。在本文中，我们对生成性能、模型幻觉、多模态任务效果、单词省略分析等方面进行了全面分析。我们在包括新闻、科学文章、常识问答、数学问答、长上下文问答和VQA数据集在内的13个数据集上评估了这些方法。我们的实验表明，提示压缩对长上下文中的LLM性能影响更大，甚至适度压缩在Longbench评估中还提升了LLM性能。我们的代码和数据可在以下链接获取。', 'title_zh': '大型语言模型的提示压缩实证研究'}
{'arxiv_id': 'arXiv:2505.00017', 'title': 'ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation', 'authors': 'Dezheng Han, Yibin Jia, Ruxiao Chen, Wenjie Han, Shuaishuai Guo, Jianbo Wang', 'link': 'https://arxiv.org/abs/2505.00017', 'abstract': 'To enable precise and fully automated cell type annotation with large language models (LLMs), we developed a graph structured feature marker database to retrieve entities linked to differential genes for cell reconstruction. We further designed a multi task workflow to optimize the annotation process. Compared to general purpose LLMs, our method improves human evaluation scores by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while more closely aligning with the cognitive logic of manual annotation.', 'abstract_zh': '利用大型语言模型实现精确且全自动化细胞类型注释的图结构特征标记数据库及多任务工作流方法', 'title_zh': 'ReCellTy: 域特定知识图谱检索增强的LLM单细胞注释工作流'}
{'arxiv_id': 'arXiv:2505.00016', 'title': 'Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning', 'authors': 'Josefa Lia Stoisser, Marc Boubnovski Martell, Julien Fauqueur', 'link': 'https://arxiv.org/abs/2505.00016', 'abstract': 'This work reframes the Text-to-SQL task as a pathway for teaching large language models (LLMs) to reason over and manipulate tabular data--moving beyond the traditional focus on query generation. We propose a two-stage framework that leverages SQL supervision to develop transferable table reasoning capabilities. First, we synthesize detailed chain-of-thought (CoT) traces from real-world SQL queries, providing step-by-step, clause-level supervision that teaches the model how to traverse, filter, and aggregate table fields. Second, we introduce a Group Relative Policy Optimization (GRPO) reinforcement learning objective that connects SQL execution accuracy to generalizable reasoning by encouraging steps that extend beyond task-specific syntax and transfer across datasets. Empirically, our approach improves performance on standard Text-to-SQL benchmarks and achieves substantial gains on reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced generalization and interpretability. Specifically, the distilled-quantized LLaMA model achieved a 20\\% increase in accuracy when trained on Text-to-SQL tasks, while Qwen achieved a 5\\% increase. These results suggest that SQL can serve not only as a target formalism but also as an effective scaffold for learning robust, transferable reasoning over structured data.', 'abstract_zh': '将文本转SQL任务重新定义为教学大型语言模型（LLMs）进行表数据推理和操作的路径——超越传统的查询生成关注。我们提出了一种两阶段框架，利用SQL监督来发展转移性表推理能力。首先，我们从实际的SQL查询中综合详细的推理链（CoT）跟踪，提供逐步骤、逐子句级别的监督，教导模型如何遍历、过滤和聚合表字段。其次，我们引入了一种基于组相对策略优化（GRPO）的强化学习目标，将SQL执行准确性与可泛化的推理连接起来，鼓励超出现有任务特定语法步骤并跨数据集进行转移。实验结果显示，我们的方法在标准的文本转SQL基准测试中提高了性能，在推理密集的数据集BIRD和CRT-QA中实现了显著的提升，展示了更强的泛化能力和可解释性。具体而言，蒸馏量化后的LLaMA模型在训练于文本转SQL任务时准确率提高了20%，而Qwen提高了5%。这些结果表明，SQL不仅可以作为目标形式语言，还可以作为学习在结构化数据上进行稳健的、可转移推理的有效支架。', 'title_zh': '通过Text2SQL强化学习激发的表格推理火花'}
{'arxiv_id': 'arXiv:2505.00013', 'title': 'Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa', 'authors': 'Yoichi Takenaka', 'link': 'https://arxiv.org/abs/2505.00013', 'abstract': 'Background Practical applications such as social media monitoring and customer-feedback analysis require accurate emotion detection for Japanese text, yet resource scarcity and class imbalance hinder model performance.\nObjective This study aims to build a high-accuracy model for predicting the presence or absence of eight Plutchik emotions in Japanese sentences.\nMethods Using the WRIME corpus, we transform reader-averaged intensity scores into binary labels and fine-tune four pre-trained language models (BERT, RoBERTa, DeBERTa-v3-base, DeBERTa-v3-large). For context, we also assess two large language models (TinySwallow-1.5B-Instruct and ChatGPT-4o). Accuracy and F1-score serve as evaluation metrics.\nResults DeBERTa-v3-large attains the best mean accuracy (0.860) and F1-score (0.662), outperforming all other models. It maintains robust F1 across both high-frequency emotions (e.g., Joy, Anticipation) and low-frequency emotions (e.g., Anger, Trust). The LLMs lag, with ChatGPT-4o and TinySwallow-1.5B-Instruct scoring 0.527 and 0.292 in mean F1, respectively.\nConclusion The fine-tuned DeBERTa-v3-large model currently offers the most reliable solution for binary emotion classification in Japanese. We release this model as a pip-installable package (pip install deberta-emotion-predictor). Future work should augment data for rare emotions, reduce model size, and explore prompt engineering to improve LLM performance.\nThis manuscript is under review for possible publication in New Generation Computing.', 'abstract_zh': '背景 实际应用如社交媒体监控和客户反馈分析需要准确的日本文字情感检测，但资源稀缺和类别不平衡阻碍了模型性能的提升。\n\n目标 本研究旨在构建一个高精度模型，用于预测日本句子中是否存在八种普拉奇克情感。\n\n方法 使用WRIME语料库，我们将读者平均强度评分转换为二元标签，并对四种预训练语言模型（BERT、RoBERTa、DeBERTa-v3-base、DeBERTa-v3-large）进行微调。此外，我们还评估了两个大型语言模型（TinySwallow-1.5B-Instruct和ChatGPT-4o）。准确率和F1分数用作评估指标。\n\n结果 微调后的DeBERTa-v3-large模型在平均准确率（0.860）和F1分数（0.662）上表现最佳，优于所有其他模型。它在高频情感（如快乐、期待）和低频情感（如愤怒、信任）上都保持了稳健的F1分数。大型语言模型表现落后，ChatGPT-4o和TinySwallow-1.5B-Instruct的平均F1分数分别为0.527和0.292。\n\n结论 当前，微调后的DeBERTa-v3-large模型提供了最可靠的二元情感分类解决方案。我们以pip可安装包的形式发布了该模型（pip install deberta-emotion-predictor）。未来工作应增加稀有情感的数据、减少模型大小，并探索提示工程以提高大型语言模型的性能。\n\n本文正在《新一代 computing》期刊审核中，以期发表。', 'title_zh': '使用RoBERTa和DeBERTa对日语情感分类 performance 评价'}
{'arxiv_id': 'arXiv:2505.00010', 'title': 'Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models', 'authors': 'Tri Nguyen, Lohith Srikanth Pentapalli, Magnus Sieverding, Laurah Turner, Seth Overla, Weibing Zheng, Chris Zhou, David Furniss, Danielle Weber, Michael Gharib, Matt Kelleher, Michael Shukis, Cameron Pawlik, Kelly Cohen', 'link': 'https://arxiv.org/abs/2505.00010', 'abstract': 'Jailbreaking in Large Language Models (LLMs) threatens their safe use in sensitive domains like education by allowing users to bypass ethical safeguards. This study focuses on detecting jailbreaks in 2-Sigma, a clinical education platform that simulates patient interactions using LLMs. We annotated over 2,300 prompts across 158 conversations using four linguistic variables shown to correlate strongly with jailbreak behavior. The extracted features were used to train several predictive models, including Decision Trees, Fuzzy Logic-based classifiers, Boosting methods, and Logistic Regression. Results show that feature-based predictive models consistently outperformed Prompt Engineering, with the Fuzzy Decision Tree achieving the best overall performance. Our findings demonstrate that linguistic-feature-based models are effective and explainable alternatives for jailbreak detection. We suggest future work explore hybrid frameworks that integrate prompt-based flexibility with rule-based robustness for real-time, spectrum-based jailbreak monitoring in educational LLMs.', 'abstract_zh': '大型语言模型（LLMs）中的越界攻击威胁其在敏感领域如教育中的安全使用。本研究专注于检测2-Sigma临床教育平台中由LLM模拟的患者互动中的越界行为。我们使用四种与越界行为高度相关的语言变量对158次对话中的2,300多个提示进行了标注。提取的特征被用于训练多种预测模型，包括决策树、基于模糊逻辑的分类器、提升方法和逻辑回归。结果表明，基于特征的预测模型整体上优于提示工程，模糊决策树表现出最佳性能。本研究发现表明，基于语言特征的模型是有效且可解释的越界检测替代方案。我们建议未来工作探索结合基于提示的灵活性与基于规则的稳健性的混合框架，以实现实时、频谱基于的教育LLM中越界监测。', 'title_zh': '基于特征预测模型的临床训练大语言模型逃逸检测'}
{'arxiv_id': 'arXiv:2505.00008', 'title': 'A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination', 'authors': 'Zhaoyi Sun, Wen-Wai Yim, Ozlem Uzuner, Fei Xia, Meliha Yetisgen', 'link': 'https://arxiv.org/abs/2505.00008', 'abstract': 'Objective: This review aims to explore the potential and challenges of using Natural Language Processing (NLP) to detect, correct, and mitigate medically inaccurate information, including errors, misinformation, and hallucination. By unifying these concepts, the review emphasizes their shared methodological foundations and their distinct implications for healthcare. Our goal is to advance patient safety, improve public health communication, and support the development of more reliable and transparent NLP applications in healthcare.\nMethods: A scoping review was conducted following PRISMA guidelines, analyzing studies from 2020 to 2024 across five databases. Studies were selected based on their use of NLP to address medically inaccurate information and were categorized by topic, tasks, document types, datasets, models, and evaluation metrics.\nResults: NLP has shown potential in addressing medically inaccurate information on the following tasks: (1) error detection (2) error correction (3) misinformation detection (4) misinformation correction (5) hallucination detection (6) hallucination mitigation. However, challenges remain with data privacy, context dependency, and evaluation standards.\nConclusion: This review highlights the advancements in applying NLP to tackle medically inaccurate information while underscoring the need to address persistent challenges. Future efforts should focus on developing real-world datasets, refining contextual methods, and improving hallucination management to ensure reliable and transparent healthcare applications.', 'abstract_zh': '客观目标：本综述旨在探索使用自然语言处理（NLP）检测、纠正和缓解医学不准确信息（包括错误、 misinformation 和幻觉）的潜力和挑战。通过将这些概念统一，综述强调了它们共同的方法学基础及其对医疗保健的不同影响。我们的目标是提高患者安全、改善公共卫生沟通，并支持在医疗保健中开发更可靠和透明的NLP应用。\n\n方法：根据PRISMA指南，本综述进行了范围回顾，分析了2020年至2024年间五个数据库中的研究。根据研究使用NLP解决医学不准确信息的情况，研究被按主题、任务、文档类型、数据集、模型和评估指标分类。\n\n结果：NLP在以下任务中显示出解决医学不准确信息的潜力：（1）错误检测（2）错误纠正（3）错误信息检测（4）错误信息纠正（5）幻觉检测（6）幻觉缓解。然而，仍存在数据隐私、上下文依赖性和评估标准等方面的挑战。\n\n结论：本综述强调了将NLP应用于解决医学不准确信息的进展，同时也指出了需要解决的持续挑战。未来的研究应关注开发现实世界的数据集、细化上下文方法和改善幻觉管理，以确保医疗保健应用的可靠性和透明度。', 'title_zh': '自然语言处理在应对医学不准确信息中的综述：错误、 misinformation 和幻觉'}
{'arxiv_id': 'arXiv:2505.00004', 'title': 'LangVAE and LangSpace: Building and Probing for Language Model VAEs', 'authors': 'Danilo S. Carvalho, Yingji Zhang, Harriet Unsworth, André Freitas', 'link': 'https://arxiv.org/abs/2505.00004', 'abstract': 'We present LangVAE, a novel framework for modular construction of variational autoencoders (VAEs) on top of pre-trained large language models (LLMs). Such language model VAEs can encode the knowledge of their pre-trained components into more compact and semantically disentangled representations. The representations obtained in this way can be analysed with the LangVAE companion framework: LangSpace, which implements a collection of probing methods, such as vector traversal and interpolation, disentanglement measures, and cluster visualisations. LangVAE and LangSpace offer a flexible, efficient and scalable way of building and analysing textual representations, with simple integration for models available on the HuggingFace Hub. Additionally, we conducted a set of experiments with different encoder and decoder combinations, as well as annotated inputs, revealing a wide range of interactions across architectural families and sizes w.r.t. generalisation and disentanglement. Our findings demonstrate a promising framework for systematising the experimentation and understanding of textual representations.', 'abstract_zh': '我们提出LangVAE，一种基于预训练大型语言模型构建模块化变分自编码器（VAEs）的新框架。此类语言模型VAE能够将其预训练组件的知识编码为更为紧凑且语义上分离的表示。通过LangVAE配套框架LangSpace可以获得的表示，可以使用该框架实现的多种探查方法进行分析，如向量遍历和插值、分离度量以及聚类可视化。LangVAE和LangSpace提供了一种灵活、高效且可扩展的文本表示构建和分析方法，支持从HuggingFace Hub集成不同模型。此外，我们还通过不同的编码器和解码器组合以及标注输入进行了实验，揭示了不同架构家族和规模在泛化和分离性方面的广泛交互。我们的研究结果表明，LangVAE为系统化文本表示的实验和理解提供了一个有前景的框架。', 'title_zh': 'LangVAE和LangSpace：构建与探测语言模型VAE'}
