{'arxiv_id': 'arXiv:2510.07028', 'title': 'Temporal-Prior-Guided View Planning for Periodic 3D Plant Reconstruction', 'authors': 'Sicong Pan, Xuying Huang, Maren Bennewitz', 'link': 'https://arxiv.org/abs/2510.07028', 'abstract': 'Periodic 3D reconstruction is essential for crop monitoring, but costly when each cycle restarts from scratch, wasting resources and ignoring information from previous captures. We propose temporal-prior-guided view planning for periodic plant reconstruction, in which a previously reconstructed model of the same plant is non-rigidly aligned to a new partial observation to form an approximation of the current geometry. To accommodate plant growth, we inflate this approximation and solve a set covering optimization problem to compute a minimal set of views. We integrated this method into a complete pipeline that acquires one additional next-best view before registration for robustness and then plans a globally shortest path to connect the planned set of views and outputs the best view sequence. Experiments on maize and tomato under hemisphere and sphere view spaces show that our system maintains or improves surface coverage while requiring fewer views and comparable movement cost compared to state-of-the-art baselines.', 'abstract_zh': '基于时间先验的周期性植物重建视图规划', 'title_zh': '基于时间先验的视图规划以实现周期性三维植物重建'}
{'arxiv_id': 'arXiv:2510.06754', 'title': 'UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene', 'authors': 'Christian Maurer, Snehal Jauhri, Sophie Lueth, Georgia Chalvatzaki', 'link': 'https://arxiv.org/abs/2510.06754', 'abstract': 'Comprehensive visual, geometric, and semantic understanding of a 3D scene is crucial for successful execution of robotic tasks, especially in unstructured and complex environments. Additionally, to make robust decisions, it is necessary for the robot to evaluate the reliability of perceived information. While recent advances in 3D neural feature fields have enabled robots to leverage features from pretrained foundation models for tasks such as language-guided manipulation and navigation, existing methods suffer from two critical limitations: (i) they are typically scene-specific, and (ii) they lack the ability to model uncertainty in their predictions. We present UniFField, a unified uncertainty-aware neural feature field that combines visual, semantic, and geometric features in a single generalizable representation while also predicting uncertainty in each modality. Our approach, which can be applied zero shot to any new environment, incrementally integrates RGB-D images into our voxel-based feature representation as the robot explores the scene, simultaneously updating uncertainty estimation. We evaluate our uncertainty estimations to accurately describe the model prediction errors in scene reconstruction and semantic feature prediction. Furthermore, we successfully leverage our feature predictions and their respective uncertainty for an active object search task using a mobile manipulator robot, demonstrating the capability for robust decision-making.', 'abstract_zh': '全面的视觉、几何和语义理解对于机器人在不规则和复杂环境中成功执行任务至关重要。此外，为了做出稳健的决策，机器人需要评估其感知信息的可靠性。虽然近期在三维神经特征场方面的进展使机器人能够利用预训练基础模型的特征进行语言引导的操作和导航等任务，但现有方法存在两个关键局限性：（i）它们通常是场景特定的；（ii）缺乏在其预测中建模不确定性的能力。我们提出了一种统一的不确定性感知神经特征场UniFField，该方法将视觉、语义和几何特征结合在一个通用表示中，同时预测每个模态的不确定性。我们的方法可以零样本迁移到任何新的环境，在机器人探索场景时，逐步将RGB-D图像集成到基于体素的特征表示中，并同时更新不确定性估计。我们评估了不确定性估计，以准确描述场景重建和语义特征预测中的模型预测误差。此外，我们成功利用特征预测及其各自的不确定性进行了一个基于移动操作机器人的主动对象搜索任务，展示了其进行稳健决策的能力。', 'title_zh': 'UniFField：一种通用的统一神经特征场，用于任意场景中的视觉、语义和空间不确定性'}
{'arxiv_id': 'arXiv:2510.06876', 'title': 'HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation', 'authors': 'Samir Abou Haidar, Alexandre Chariot, Mehdi Darouich, Cyril Joly, Jean-Emmanuel Deschaud', 'link': 'https://arxiv.org/abs/2510.06876', 'abstract': 'LiDAR semantic segmentation is crucial for autonomous vehicles and mobile robots, requiring high accuracy and real-time processing, especially on resource-constrained embedded systems. Previous state-of-the-art methods often face a trade-off between accuracy and speed. Point-based and sparse convolution-based methods are accurate but slow due to the complexity of neighbor searching and 3D convolutions. Projection-based methods are faster but lose critical geometric information during the 2D projection. Additionally, many recent methods rely on test-time augmentation (TTA) to improve performance, which further slows the inference. Moreover, the pre-processing phase across all methods increases execution time and is demanding on embedded platforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR semantic segmentation network. We first propose a novel pre-processing methodology that significantly reduces computational overhead. Then, we design the Conv-SE-NeXt feature extraction block to efficiently capture representations without deep layer stacking per network stage. We also employ a multi-scale range-point fusion backbone that leverages information at multiple abstraction levels to preserve essential geometric details, thereby enhancing accuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that HARP-NeXt achieves a superior speed-accuracy trade-off compared to all state-of-the-art methods, and, without relying on ensemble models or TTA, is comparable to the top-ranked PTv3, while running 24$\\times$ faster. The code is available at this https URL', 'abstract_zh': 'LiDAR语义分割对自主车辆和移动机器人至关重要，需要高精度和实时处理，特别是在资源受限的嵌入式系统中。先前的先进方法往往在精度和速度之间存在权衡。基于点的方法和稀疏卷积方法虽然准确但速度较慢，因为邻居搜索和三维卷积的复杂性。投影方法速度快但会在二维投影过程中丢失重要的几何信息。此外，许多最近的方法依赖测试时增强（TTA）来提高性能，这进一步减慢了推理速度。而且，所有方法的预处理阶段都会增加执行时间，并对嵌入式平台提出更高要求。因此，我们引入了HARP-NeXt，一种高速高精度的LiDAR语义分割网络。我们首先提出了一种新的预处理方法，大幅减少了计算开销。然后，我们设计了Conv-SE-NeXt特征提取块，以有效地捕获表示而不必在网络的每一阶段进行深层次层堆叠。我们还采用了一种多尺度范围点融合骨干网络，利用多个抽象层次上的信息来保留关键的几何细节，从而提高准确性。在nuScenes和SemanticKITTI基准上的实验表明，HARP-NeXt在速度和精度之间实现了优于所有先进方法的妥协，并且在无需依赖集成模型或TTA的情况下，与排名靠前的PTv3相当，但运行速度快24倍。代码可在以下链接获取。', 'title_zh': 'HARP-NeXt：高-Speed和高精度的3D LiDAR语义分割范围点融合网络'}
{'arxiv_id': 'arXiv:2510.06582', 'title': 'Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation', 'authors': 'Fei Zhang, Rob Chancia, Josie Clapp, Amirhossein Hassanzadeh, Dimah Dera, Richard MacKenzie, Jan van Aardt', 'link': 'https://arxiv.org/abs/2510.06582', 'abstract': 'Accurate semantic segmentation of terrestrial laser scanning (TLS) point clouds is limited by costly manual annotation. We propose a semi-automated, uncertainty-aware pipeline that integrates spherical projection, feature enrichment, ensemble learning, and targeted annotation to reduce labeling effort, while sustaining high accuracy. Our approach projects 3D points to a 2D spherical grid, enriches pixels with multi-source features, and trains an ensemble of segmentation networks to produce pseudo-labels and uncertainty maps, the latter guiding annotation of ambiguous regions. The 2D outputs are back-projected to 3D, yielding densely annotated point clouds supported by a three-tier visualization suite (2D feature maps, 3D colorized point clouds, and compact virtual spheres) for rapid triage and reviewer guidance. Using this pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove forests. We further evaluate data efficiency and feature importance to address two key questions: (1) how much annotated data are needed and (2) which features matter most. Results show that performance saturates after ~12 annotated scans, geometric features contribute the most, and compact nine-channel stacks capture nearly all discriminative power, with the mean Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm the generalization of our feature-enrichment strategy through cross-dataset tests on ForestSemantic and Semantic3D.\nOur contributions include: (i) a robust, uncertainty-aware TLS annotation pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii) empirical guidance on data efficiency and feature importance, thus enabling scalable, high-quality segmentation of TLS point clouds for ecological monitoring and beyond. The dataset and processing scripts are publicly available at this https URL.', 'abstract_zh': '一种基于球面投影和特征增强的半自动化激光扫描点云语义分割方法：减少标注努力并维持高精度', 'title_zh': '从LiDAR视角出发：一种用于地表点云分割的特征丰富且考虑不确定性注释管道'}
{'arxiv_id': 'arXiv:2510.07217', 'title': 'GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation', 'authors': 'Wen Ye, Zhaocheng Liu, Yuwei Gui, Tingyu Yuan, Yunyue Su, Bowen Fang, Chaoyang Zhao, Qiang Liu, Liang Wang', 'link': 'https://arxiv.org/abs/2510.07217', 'abstract': 'Text-to-image synthesis has made remarkable progress, yet accurately interpreting complex and lengthy prompts remains challenging, often resulting in semantic inconsistencies and missing details. Existing solutions, such as fine-tuning, are model-specific and require training, while prior automatic prompt optimization (APO) approaches typically lack systematic error analysis and refinement strategies, resulting in limited reliability and effectiveness. Meanwhile, test-time scaling methods operate on fixed prompts and on noise or sample numbers, limiting their interpretability and adaptability. To solve these, we introduce a flexible and efficient test-time prompt optimization strategy that operates directly on the input text. We propose a plug-and-play multi-agent system called GenPilot, integrating error analysis, clustering-based adaptive exploration, fine-grained verification, and a memory module for iterative optimization. Our approach is model-agnostic, interpretable, and well-suited for handling long and complex prompts. Simultaneously, we summarize the common patterns of errors and the refinement strategy, offering more experience and encouraging further exploration. Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7% demonstrate the strong capability of our methods in enhancing the text and image consistency and structural coherence of generated images, revealing the effectiveness of our test-time prompt optimization strategy. The code is available at this https URL.', 'abstract_zh': '文本到图像合成取得了显著进展，但准确解释复杂的长提示依然具有挑战性，常常导致语义不一致和缺失细节。现有解决方案，如微调，是模型特定的并且需要训练，而先前的自动提示优化（APO）方法通常缺乏系统性的错误分析和优化策略，导致其可靠性有限且效果有限。同时，测试时缩放方法仅适用于固定提示或噪声或样本数量，限制了其可解释性和适应性。为了解决这些问题，我们提出了一种灵活且高效的测试时提示优化策略，可以直接作用于输入文本。我们提出了一个即插即用的多代理系统GenPilot，该系统结合了错误分析、基于聚类的自适应探索、细粒度验证和记忆模块，实现迭代优化。我们的方法是模型无关的、可解释的，并且非常适合处理长且复杂的提示。同时，我们总结了常见的错误模式和优化策略，提供了更多的经验并鼓励进一步探索。在DPG-bench和Geneval上的实验表明，我们的方法在提高生成图的文本和图像一致性及结构连贯性方面具有显著的能力，证实了我们测试时提示优化策略的有效性。代码可在此处访问。', 'title_zh': 'GenPilot: 一种用于图像生成测试时提示优化的多agent系统'}
{'arxiv_id': 'arXiv:2510.07191', 'title': 'Resolution scaling governs DINOv3 transfer performance in chest radiograph classification', 'authors': 'Soroosh Tayebi Arasteh, Mina Shaigan, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn', 'link': 'https://arxiv.org/abs/2510.07191', 'abstract': "Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n>814,000). Two representative backbones were evaluated: ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.", 'abstract_zh': '自监督学习（SSL）在胸部X线成像中的价值：DINOv3在高通量细粒度影像数据中的表现评估', 'title_zh': '分辨率缩放决定了DINOv3在胸部X光分类中的迁移学习性能。'}
{'arxiv_id': 'arXiv:2510.07129', 'title': 'Graph Conditioned Diffusion for Controllable Histopathology Image Generation', 'authors': 'Sarah Cechnicka, Matthew Baugh, Weitong Zhang, Mischa Dombrowski, Zhe Li, Johannes C. Paetzold, Candice Roufosse, Bernhard Kainz', 'link': 'https://arxiv.org/abs/2510.07129', 'abstract': 'Recent advances in Diffusion Probabilistic Models (DPMs) have set new standards in high-quality image synthesis. Yet, controlled generation remains challenging, particularly in sensitive areas such as medical imaging. Medical images feature inherent structure such as consistent spatial arrangement, shape or texture, all of which are critical for diagnosis. However, existing DPMs operate in noisy latent spaces that lack semantic structure and strong priors, making it difficult to ensure meaningful control over generated content. To address this, we propose graph-based object-level representations for Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding to each major structure in the image, encapsulating their individual features and relationships. These graph representations are processed by a transformer module and integrated into a diffusion model via the text-conditioning mechanism, enabling fine-grained control over generation. We evaluate this approach using a real-world histopathology use case, demonstrating that our generated data can reliably substitute for annotated patient data in downstream segmentation tasks. The code is available here.', 'abstract_zh': '最近在扩散概率模型（DPMs）方面的进展为高质量图像合成设定了新标准。然而，在医疗成像等敏感领域，受控生成仍然具有挑战性。医疗图像具有内在结构，如一致的空间排列、形状或纹理，这些都是诊断的关键。然而，现有的DPMs在缺乏语义结构和强先验知识的噪声潜在空间中运行，这使得确保生成内容的有意义控制变得困难。为了解决这个问题，我们提出了基于图的对象级表示以实现条件图扩散。本方法生成对应于图像中每个主要结构的图节点，封装其各自的特征和关系。这些图表示通过一个变压器模块进行处理，并通过文本引导机制集成到扩散模型中，从而实现对生成的精细控制。我们使用实际病理学案例进行了评估，结果显示我们生成的数据可以可靠地替代注释的患者数据用于后续分割任务。代码可在此获得。', 'title_zh': '基于图条件的扩散模型在可控制的病理图像生成中的应用'}
{'arxiv_id': 'arXiv:2510.06969', 'title': 'Learning Global Representation from Queries for Vectorized HD Map Construction', 'authors': 'Shoumeng Qiu, Xinrun Li, Yang Long, Xiangyang Xue, Varun Ojha, Jian Pu', 'link': 'https://arxiv.org/abs/2510.06969', 'abstract': 'The online construction of vectorized high-definition (HD) maps is a cornerstone of modern autonomous driving systems. State-of-the-art approaches, particularly those based on the DETR framework, formulate this as an instance detection problem. However, their reliance on independent, learnable object queries results in a predominantly local query perspective, neglecting the inherent global representation within HD maps. In this work, we propose \\textbf{MapGR} (\\textbf{G}lobal \\textbf{R}epresentation learning for HD \\textbf{Map} construction), an architecture designed to learn and utilize a global representations from queries. Our method introduces two synergistic modules: a Global Representation Learning (GRL) module, which encourages the distribution of all queries to better align with the global map through a carefully designed holistic segmentation task, and a Global Representation Guidance (GRG) module, which endows each individual query with explicit, global-level contextual information to facilitate its optimization. Evaluations on the nuScenes and Argoverse2 datasets validate the efficacy of our approach, demonstrating substantial improvements in mean Average Precision (mAP) compared to leading baselines.', 'abstract_zh': '基于全局表示学习的高精度emap构建方法：MapGR', 'title_zh': '基于查询学习全局表示的向量化高清地图构建'}
{'arxiv_id': 'arXiv:2510.06967', 'title': 'Generating Surface for Text-to-3D using 2D Gaussian Splatting', 'authors': 'Huanning Dong, Fan Li, Ping Kuang, Jianwen Min', 'link': 'https://arxiv.org/abs/2510.06967', 'abstract': 'Recent advancements in Text-to-3D modeling have shown significant potential for the creation of 3D content. However, due to the complex geometric shapes of objects in the natural world, generating 3D content remains a challenging task. Current methods either leverage 2D diffusion priors to recover 3D geometry, or train the model directly based on specific 3D representations. In this paper, we propose a novel method named DirectGaussian, which focuses on generating the surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize conditional text generation models and the surface of a 3D object is rendered by 2D Gaussian splatting with multi-view normal and texture priors. For multi-view geometric consistency problems, DirectGaussian incorporates curvature constraints on the generated surface during optimization process. Through extensive experiments, we demonstrate that our framework is capable of achieving diverse and high-fidelity 3D content creation.', 'abstract_zh': 'Recent advancements in Text-to-3D modeling have shown significant potential for the creation of 3D content. However, due to the complex geometric shapes of objects in the natural world, generating 3D content remains a challenging task. Current methods either leverage 2D diffusion priors to recover 3D geometry, or train the model directly based on specific 3D representations. In this paper, we propose a novel method named DirectGaussian, which focuses on generating the surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize conditional text generation models and the surface of a 3D object is rendered by 2D Gaussian splatting with multi-view normal and texture priors. For multi-view geometric consistency problems, DirectGaussian incorporates curvature constraints on the generated surface during optimization process. Through extensive experiments, we demonstrate that our framework is capable of achieving diverse and high-fidelity 3D content creation.', 'title_zh': '使用2D高斯点绘制的从文本生成三维表面方法'}
{'arxiv_id': 'arXiv:2510.06858', 'title': 'Explaining raw data complexity to improve satellite onboard processing', 'authors': 'Adrien Dorise, Marjorie Bellizzi, Adrien Girard, Benjamin Francesconi, Stéphane May', 'link': 'https://arxiv.org/abs/2510.06858', 'abstract': 'With increasing processing power, deploying AI models for remote sensing directly onboard satellites is becoming feasible. However, new constraints arise, mainly when using raw, unprocessed sensor data instead of preprocessed ground-based products. While current solutions primarily rely on preprocessed sensor images, few approaches directly leverage raw data. This study investigates the effects of utilising raw data on deep learning models for object detection and classification tasks. We introduce a simulation workflow to generate raw-like products from high-resolution L1 imagery, enabling systemic evaluation. Two object detection models (YOLOv11s and YOLOX-S) are trained on both raw and L1 datasets, and their performance is compared using standard detection metrics and explainability tools. Results indicate that while both models perform similarly at low to medium confidence thresholds, the model trained on raw data struggles with object boundary identification at high confidence levels. It suggests that adapting AI architectures with improved contouring methods can enhance object detection on raw images, improving onboard AI for remote sensing.', 'abstract_zh': '随着计算能力的增强，在卫星上直接部署AI模型变得可行。然而，当使用原始未处理的传感器数据而不是预处理的地面产品时，新的约束条件随之而来。尽管当前的方法主要依赖预处理的传感器图像，但很少有方法直接利用原始数据。本文研究了利用原始数据对目标检测和分类任务的深度学习模型的影响。我们引入了一套仿真工作流，从高分辨率L1影像中生成类似原始产品的数据，从而实现系统的评估。两个目标检测模型（YOLOv11s和YOLOX-S）分别在校准和L1数据集上训练，并使用标准检测指标和解释性工具比较其性能。结果显示，在低到中等置信度阈值下，两种模型表现相似，但在高置信度水平下，基于原始数据训练的模型在目标边界识别方面遇到困难。这表明，通过改进边缘提取方法适应AI架构可以增强对原始图像的目标检测能力，从而提高遥感领域的机载AI性能。', 'title_zh': '解释原始数据复杂性以提高卫星机载处理能力'}
{'arxiv_id': 'arXiv:2510.06840', 'title': 'CNN-TFT explained by SHAP with multi-head attention weights for time series forecasting', 'authors': 'Stefano F. Stefenon, João P. Matos-Carvalho, Valderi R. Q. Leithardt, Kin-Choong Yow', 'link': 'https://arxiv.org/abs/2510.06840', 'abstract': 'Convolutional neural networks (CNNs) and transformer architectures offer strengths for modeling temporal data: CNNs excel at capturing local patterns and translational invariances, while transformers effectively model long-range dependencies via self-attention. This paper proposes a hybrid architecture integrating convolutional feature extraction with a temporal fusion transformer (TFT) backbone to enhance multivariate time series forecasting. The CNN module first applies a hierarchy of one-dimensional convolutional layers to distill salient local patterns from raw input sequences, reducing noise and dimensionality. The resulting feature maps are then fed into the TFT, which applies multi-head attention to capture both short- and long-term dependencies and to weigh relevant covariates adaptively. We evaluate the CNN-TFT on a hydroelectric natural flow time series dataset. Experimental results demonstrate that CNN-TFT outperforms well-established deep learning models, with a mean absolute percentage error of up to 2.2%. The explainability of the model is obtained by a proposed Shapley additive explanations with multi-head attention weights (SHAP-MHAW). Our novel architecture, named CNN-TFT-SHAP-MHAW, is promising for applications requiring high-fidelity, multivariate time series forecasts, being available for future analysis at this https URL .', 'abstract_zh': '卷积神经网络（CNNs）和变压器架构在建模时序数据方面表现出色：卷积神经网络擅长捕捉局部模式和平移不变性，而变压器通过自注意力机制有效建模长距离依赖关系。本文提出了一种将卷积特征提取与时间融合变压器（TFT）骨干网络相结合的混合架构，以增强多变量时间序列预测。卷积模块首先应用一系列一维卷积层从原始输入序列中提取显著的局部模式，减少噪声和维度。提取的特征图随后输入TFT，后者通过多头注意力机制捕获短长期依赖关系，并适应性地加权相关协变量。我们使用水力发电天然流量时序数据集对CNN-TFT进行了评估。实验结果表明，CNN-TFT在均绝对百分比误差方面优于现有的深度学习模型，最高可达2.2%。通过提出的具有多头注意力权重的Shapley加性解释（SHAP-MHAW）方法获得了模型的可解释性。我们提出的新型架构CNN-TFT-SHAP-MHAW适用于需要高保真度多变量时间序列预测的应用，并可在<https://这一网址提供>供未来分析使用。', 'title_zh': '基于多头注意力权重的SHAP解释的CNN-TFT时间序列 forecasting'}
{'arxiv_id': 'arXiv:2510.06791', 'title': 'Extreme Amodal Face Detection', 'authors': 'Changlin Song, Yunzhong Hou, Michael Randall Barnes, Rahul Shome, Dylan Campbell', 'link': 'https://arxiv.org/abs/2510.06791', 'abstract': 'Extreme amodal detection is the task of inferring the 2D location of objects that are not fully visible in the input image but are visible within an expanded field-of-view. This differs from amodal detection, where the object is partially visible within the input image, but is occluded. In this paper, we consider the sub-problem of face detection, since this class provides motivating applications involving safety and privacy, but do not tailor our method specifically to this class. Existing approaches rely on image sequences so that missing detections may be interpolated from surrounding frames or make use of generative models to sample possible completions. In contrast, we consider the single-image task and propose a more efficient, sample-free approach that makes use of the contextual cues from the image to infer the presence of unseen faces. We design a heatmap-based extreme amodal object detector that addresses the problem of efficiently predicting a lot (the out-of-frame region) from a little (the image) with a selective coarse-to-fine decoder. Our method establishes strong results for this new task, even outperforming less efficient generative approaches.', 'abstract_zh': '极端无框检测是推断输入图像中未完全可见但存在于扩展视野中的物体2D位置的任务。这与部分可见的无框检测不同，在后者中，物体在输入图像中部分可见但被遮挡。在本文中，我们考虑人脸检测的子问题，因为该类别提供了涉及安全和隐私的激励应用，但我们并未专门针对该类别设计我们的方法。现有方法依赖于图像序列，以便从相邻帧中插值缺失的检测，或者利用生成模型采样可能的完成。相比之下，我们考虑单张图像任务，并提出了一种更高效、无需样本的方法，利用图像中的上下文线索推断未见人脸的存在。我们设计了一种基于热图的极端无框物体检测器，该检测器通过选择性粗细解码器高效地解决了从少量（图像）中预测大量（超出图像范围区域）的问题。我们的方法在该新任务中取得了强有力的成果，甚至优于更不高效的生成方法。', 'title_zh': '极端非视界面部检测'}
{'arxiv_id': 'arXiv:2510.06687', 'title': 'Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion', 'authors': 'Jie Luo, Yuxuan Jiang, Xin Jin, Mingyu Liu, Yihui Fan', 'link': 'https://arxiv.org/abs/2510.06687', 'abstract': 'Semantic segmentation serves as a cornerstone of scene understanding in autonomous driving but continues to face significant challenges under complex conditions such as occlusion. Light field and LiDAR modalities provide complementary visual and spatial cues that are beneficial for robust perception; how- ever, their effective integration is hindered by limited viewpoint diversity and inherent modality discrepancies. To address these challenges, the first multimodal semantic segmentation dataset integrating light field data and point cloud data is proposed. Based on this dataset, we proposed a multi-modal light field point-cloud fusion segmentation network(Mlpfseg), incorporating feature completion and depth perception to segment both camera images and LiDAR point clouds simultaneously. The feature completion module addresses the density mismatch between point clouds and image pixels by performing differential re- construction of point-cloud feature maps, enhancing the fusion of these modalities. The depth perception module improves the segmentation of occluded objects by reinforcing attention scores for better occlusion awareness. Our method outperforms image- only segmentation by 1.71 Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38 mIoU, demonstrating its effectiveness.', 'abstract_zh': '多模态光场点云融合分割网络（Mlpfseg）：基于光场数据和点云数据的语义分割', 'title_zh': '基于轻量级字段和LiDAR融合的语义分割算法'}
{'arxiv_id': 'arXiv:2510.06669', 'title': 'Automated Neural Architecture Design for Industrial Defect Detection', 'authors': 'Yuxi Liu, Yunfeng Ma, Yi Tang, Min Liu, Shuai Jiang, Yaonan Wang', 'link': 'https://arxiv.org/abs/2510.06669', 'abstract': 'Industrial surface defect detection (SDD) is critical for ensuring product quality and manufacturing reliability. Due to the diverse shapes and sizes of surface defects, SDD faces two main challenges: intraclass difference and interclass similarity. Existing methods primarily utilize manually designed models, which require extensive trial and error and often struggle to address both challenges effectively. To overcome this, we propose AutoNAD, an automated neural architecture design framework for SDD that jointly searches over convolutions, transformers, and multi-layer perceptrons. This hybrid design enables the model to capture both fine-grained local variations and long-range semantic context, addressing the two key challenges while reducing the cost of manual network design. To support efficient training of such a diverse search space, AutoNAD introduces a cross weight sharing strategy, which accelerates supernet convergence and improves subnet performance. Additionally, a searchable multi-level feature aggregation module (MFAM) is integrated to enhance multi-scale feature learning. Beyond detection accuracy, runtime efficiency is essential for industrial deployment. To this end, AutoNAD incorporates a latency-aware prior to guide the selection of efficient architectures. The effectiveness of AutoNAD is validated on three industrial defect datasets and further applied within a defect imaging and detection platform. Code will be available at this https URL.', 'abstract_zh': '工业表面缺陷检测（SDD）对于确保产品质量和制造可靠性至关重要。由于表面缺陷的多样形状和尺寸，SDD 面临两大主要挑战：类内差异和类间相似性。现有方法主要依赖手工设计的模型，这需要大量的试错，并且往往难以同时有效应对这两个挑战。为克服这一问题，我们提出了一种名为 AutoNAD 的自动化神经架构设计框架，用于 SDD，该框架联合搜索卷积、变压器和多层感知机。这种混合设计使模型能够同时捕捉细粒度的局部变化和长范围的语义上下文，从而解决两个关键挑战，同时减少手动网络设计的成本。为了支持高效训练这种多样化的搜索空间，AutoNAD 引入了跨权重共享策略，加速超网络的收敛并提高子网的性能。此外，还集成了可搜索的多尺度特征聚合模块（MFAM），以增强多尺度特征学习。除了检测精度，运行时效率对于工业部署也至关重要。为此，AutoNAD 融合了感知延迟的先验知识，指导高效架构的选择。AutoNAD 的有效性已在三个工业缺陷数据集上得到验证，并进一步应用于缺陷影像和检测平台。代码将在此网址 https:// 提供。', 'title_zh': '工业缺陷检测的自动化神经网络架构设计'}
{'arxiv_id': 'arXiv:2510.06512', 'title': 'LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval', 'authors': 'Avishree Khare, Hideki Okamoto, Bardh Hoxha, Georgios Fainekos, Rajeev Alur', 'link': 'https://arxiv.org/abs/2510.06512', 'abstract': 'Neural models such as YOLO and HuBERT can be used to detect local properties such as objects ("car") and emotions ("angry") in individual frames of videos and audio clips respectively. The likelihood of these detections is indicated by scores in [0, 1]. Lifting these scores to temporal properties over sequences can be useful for several downstream applications such as query matching (e.g., "does the speaker eventually sound happy in this audio clip?"), and ranked retrieval (e.g., "retrieve top 5 videos with a 10 second scene where a car is detected until a pedestrian is detected"). In this work, we formalize this problem of assigning Scores for TempOral Properties (STOPs) over sequences, given potentially noisy score predictors for local properties. We then propose a scoring function called LogSTOP that can efficiently compute these scores for temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP, with YOLO and HuBERT, outperforms Large Vision / Audio Language Models and other Temporal Logic-based baselines by at least 16% on query matching with temporal properties over objects-in-videos and emotions-in-speech respectively. Similarly, on ranked retrieval with temporal properties over objects and actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a 19% and 16% increase in mean average precision and recall over zero-shot text-to-video retrieval baselines respectively.', 'abstract_zh': '基于序列的时间属性评分问题：给定局部属性分数预测器，利用神经模型进行时间属性评分', 'title_zh': 'LogSTOP: 预测序列的时序得分用于匹配和检索'}
{'arxiv_id': 'arXiv:2510.06353', 'title': 'TransFIRA: Transfer Learning for Face Image Recognizability Assessment', 'authors': 'Allen Tu, Kartik Narayan, Joshua Gleason, Jennifer Xu, Matthew Meyn, Tom Goldstein, Vishal M. Patel', 'link': 'https://arxiv.org/abs/2510.06353', 'abstract': "Face recognition in unconstrained environments such as surveillance, video, and web imagery must contend with extreme variation in pose, blur, illumination, and occlusion, where conventional visual quality metrics fail to predict whether inputs are truly recognizable to the deployed encoder. Existing FIQA methods typically rely on visual heuristics, curated annotations, or computationally intensive generative pipelines, leaving their predictions detached from the encoder's decision geometry. We introduce TransFIRA (Transfer Learning for Face Image Recognizability Assessment), a lightweight and annotation-free framework that grounds recognizability directly in embedding space. TransFIRA delivers three advances: (i) a definition of recognizability via class-center similarity (CCS) and class-center angular separation (CCAS), yielding the first natural, decision-boundary--aligned criterion for filtering and weighting; (ii) a recognizability-informed aggregation strategy that achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly doubling correlation with true recognizability, all without external labels, heuristics, or backbone-specific training; and (iii) new extensions beyond faces, including encoder-grounded explainability that reveals how degradations and subject-specific factors affect recognizability, and the first recognizability-aware body recognition assessment. Experiments confirm state-of-the-art results on faces, strong performance on body recognition, and robustness under cross-dataset shifts. Together, these contributions establish TransFIRA as a unified, geometry-driven framework for recognizability assessment -- encoder-specific, accurate, interpretable, and extensible across modalities -- significantly advancing FIQA in accuracy, explainability, and scope.", 'abstract_zh': '基于传输学习的脸像可识别性评估', 'title_zh': 'TransFIRA: 转移学习在面部图像可识别性评估中的应用'}
{'arxiv_id': 'arXiv:2510.06298', 'title': 'RGBD Gaze Tracking Using Transformer for Feature Fusion', 'authors': 'Tobias J. Bauer', 'link': 'https://arxiv.org/abs/2510.06298', 'abstract': 'Subject of this thesis is the implementation of an AI-based Gaze Tracking system using RGBD images that contain both color (RGB) and depth (D) information. To fuse the features extracted from the images, a module based on the Transformer architecture is used. The combination of RGBD input images and Transformers was chosen because it has not yet been investigated. Furthermore, a new dataset is created for training the AI models as existing datasets either do not contain depth information or only contain labels for Gaze Point Estimation that are not suitable for the task of Gaze Angle Estimation. Various model configurations are trained, validated and evaluated on a total of three different datasets. The trained models are then to be used in a real-time pipeline to estimate the gaze direction and thus the gaze point of a person in front of a computer screen. The AI model architecture used in this thesis is based on an earlier work by Lian et al. It uses a Generative Adversarial Network (GAN) to simultaneously remove depth map artifacts and extract head pose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their own dataset ShanghaiTechGaze+. In this thesis, a model architecture with a Transformer module for feature fusion achieves a mean Euclidean error of 55.3mm on the same dataset, but we show that using no pre-trained GAN module leads to a mean Euclidean error of 30.1mm. Replacing the Transformer module with a Multilayer Perceptron (MLP) improves the error to 26.9mm. These results are coherent with the ones on the other two datasets. On the ETH-XGaze dataset, the model with Transformer module achieves a mean angular error of 3.59° and without Transformer module 3.26°, whereas the fundamentally different model architecture used by the dataset authors Zhang et al. achieves a mean angular error of 2.04°. On the OTH-Gaze-Estimation dataset created for...', 'abstract_zh': '基于RGBD图像的人工智能眼动追踪系统实现：transformer架构在深度与颜色信息融合中的应用及新型数据集创建研究', 'title_zh': '使用变换器进行特征融合的RGBD凝视跟踪'}
{'arxiv_id': 'arXiv:2510.06295', 'title': 'Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling', 'authors': 'Young D. Kwon, Abhinav Mehrotra, Malcolm Chadwick, Alberto Gil Ramos, Sourav Bhattacharya', 'link': 'https://arxiv.org/abs/2510.06295', 'abstract': 'High-resolution (4K) image-to-image synthesis has become increasingly important for mobile applications. Existing diffusion models for image editing face significant challenges, in terms of memory and image quality, when deployed on resource-constrained devices. In this paper, we present MobilePicasso, a novel system that enables efficient image editing at high resolutions, while minimising computational cost and memory usage. MobilePicasso comprises three stages: (i) performing image editing at a standard resolution with hallucination-aware loss, (ii) applying latent projection to overcome going to the pixel space, and (iii) upscaling the edited image latent to a higher resolution with adaptive context-preserving tiling. Our user study with 46 participants reveals that MobilePicasso not only improves image quality by 18-48% but reduces hallucinations by 14-51% over existing methods. MobilePicasso demonstrates significantly lower latency, e.g., up to 55.8$\\times$ speed-up, yet with a small increase in runtime memory, e.g., a mere 9% increase over prior work. Surprisingly, the on-device runtime of MobilePicasso is observed to be faster than a server-based high-resolution image editing model running on an A100 GPU.', 'abstract_zh': '高分辨率（4K）图像到图像合成在移动应用中变得越来越重要。现有的基于扩散模型的图像编辑方法在资源受限设备上部署时，在内存和图像质量方面面临重大挑战。本文提出了一种名为MobilePicasso的新型系统，能够在保持高效图像编辑的同时，最大限度地降低计算成本和内存使用。MobilePicasso包括三个阶段：（i）在标准分辨率下进行带有幻觉意识损失的图像编辑；（ii）应用潜在投影以克服像素空间转换；（iii）使用自适应上下文保留切片方法将编辑后的图像潜在特征放大到更高分辨率。我们的用户研究结果表明，MobilePicasso不仅将图像质量提高了18-48%，而且将幻觉减少了14-51%，优于现有方法。MobilePicasso表现出显著更低的延迟，例如最高55.8倍的速度提升，同时运行时内存仅增加了9%，相较于以往工作。令人惊讶的是，MobilePicasso的设备运行时间比在A100 GPU上运行的服务器端高分辨率图像编辑模型还要快。', 'title_zh': '带有幻觉意识损失和自适应切分的高效高分辨率图像编辑'}
{'arxiv_id': 'arXiv:2510.06293', 'title': 'BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression', 'authors': 'Cristian Meo, Varun Sarathchandran, Avijit Majhi, Shao Hung, Carlo Saccardi, Ruben Imhoff, Roberto Deidda, Remko Uijlenhoet, Justin Dauwels', 'link': 'https://arxiv.org/abs/2510.06293', 'abstract': 'Predicting precipitation maps is a highly complex spatiotemporal modeling task, critical for mitigating the impacts of extreme weather events. Short-term precipitation forecasting, or nowcasting, requires models that are not only accurate but also computationally efficient for real-time applications. Current methods, such as token-based autoregressive models, often suffer from flawed inductive biases and slow inference, while diffusion models can be computationally intensive. To address these limitations, we introduce BlockGPT, a generative autoregressive transformer using batched tokenization (Block) method that predicts full two-dimensional fields (frames) at each time step. Conceived as a model-agnostic paradigm for video prediction, BlockGPT factorizes space-time by using self-attention within each frame and causal attention across frames; in this work, we instantiate it for precipitation nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI (Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet) models. The results show that BlockGPT achieves superior accuracy, event localization as measured by categorical metrics, and inference speeds up to 31x faster than comparable baselines.', 'abstract_zh': '生成式自回归transformer在批量化标记方法下的短时降水nowcasting及其应用：BlockGPT方法的研究', 'title_zh': 'BlockGPT：基于帧级自回归的时空降雨 modeling'}
{'arxiv_id': 'arXiv:2510.06281', 'title': 'Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning', 'authors': 'Chenyang Li, Qin Li, Haimin Wang, Bo Shen', 'link': 'https://arxiv.org/abs/2510.06281', 'abstract': 'High-resolution (HR) solar imaging is crucial for capturing fine-scale dynamic features such as filaments and fibrils. However, the spatial resolution of the full-disk H$\\alpha$ images is limited and insufficient to resolve these small-scale structures. To address this, we propose a GAN-based superresolution approach to enhance low-resolution (LR) full-disk H$\\alpha$ images from the Global Oscillation Network Group (GONG) to a quality comparable with HR observations from the Big Bear Solar Observatory/Goode Solar Telescope (BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a relativistic discriminator. We carefully aligned GONG-GST pairs. The model effectively recovers fine details within sunspot penumbrae and resolves fine details in filaments and fibrils, achieving an average mean squared error (MSE) of 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC) of 0.7794. Slight misalignments between image pairs limit quantitative performance, which we plan to address in future work alongside dataset expansion to further improve reconstruction quality.', 'abstract_zh': '基于GAN的高分辨率太阳高分辨率成像：将全球振荡网络群（GONG）低分辨率全盘H$\\alpha$图像增强至比格熊太阳 observatory/古多太阳望远镜（BBSO/GST）高分辨率观测更好的质量', 'title_zh': '使用深度学习提高GONG太阳图像的空间分辨率至GST质量'}
{'arxiv_id': 'arXiv:2510.06276', 'title': 'A Total Variation Regularized Framework for Epilepsy-Related MRI Image Segmentation', 'authors': 'Mehdi Rabiee, Sergio Greco, Reza Shahbazian, Irina Trubitsyna', 'link': 'https://arxiv.org/abs/2510.06276', 'abstract': 'Focal Cortical Dysplasia (FCD) is a primary cause of drug-resistant epilepsy and is difficult to detect in brain {magnetic resonance imaging} (MRI) due to the subtle and small-scale nature of its lesions. Accurate segmentation of FCD regions in 3D multimodal brain MRI images is essential for effective surgical planning and treatment. However, this task remains highly challenging due to the limited availability of annotated FCD datasets, the extremely small size and weak contrast of FCD lesions, the complexity of handling 3D multimodal inputs, and the need for output smoothness and anatomical consistency, which is often not addressed by standard voxel-wise loss functions. This paper presents a new framework for segmenting FCD regions in 3D brain MRI images. We adopt state-of-the-art transformer-enhanced encoder-decoder architecture and introduce a novel loss function combining Dice loss with an anisotropic {Total Variation} (TV) term. This integration encourages spatial smoothness and reduces false positive clusters without relying on post-processing. The framework is evaluated on a public FCD dataset with 85 epilepsy patients and demonstrates superior segmentation accuracy and consistency compared to standard loss formulations. The model with the proposed TV loss shows an 11.9\\% improvement on the Dice coefficient and 13.3\\% higher precision over the baseline model. Moreover, the number of false positive clusters is reduced by 61.6%', 'abstract_zh': 'FCD在3D多模态脑MRI图像中区域分割的新框架：结合Dice损失和各向异性Total Variation术语', 'title_zh': '基于总量变正则化的与癫痫相关的MRI图像分割框架'}
