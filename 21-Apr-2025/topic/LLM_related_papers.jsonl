{'arxiv_id': 'arXiv:2504.13837', 'title': 'Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?', 'authors': 'Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang', 'link': 'https://arxiv.org/abs/2504.13837', 'abstract': "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models' capacity. In this study, however, we critically re-examines this assumption by measuring the pass@\\textit{k} metric with large values of \\textit{k} to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does \\emph{not}, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of $k$ (\\eg, $k$=1), base models can achieve a comparable or even higher pass@$k$ score compared to their RL counterparts at large $k$ values. The reasoning paths generated by RL-trained models are already included in the base models' sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model's output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: this https URL", 'abstract_zh': 'Verifiable奖励强化学习（RLVR）在提升LLM推理能力方面的研究：局限性和新视角', 'title_zh': '强化学习真的能够激励大型语言模型提升推理能力超出基础模型的水平吗？'}
{'arxiv_id': 'arXiv:2504.13707', 'title': 'OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation', 'authors': 'Yichen Wu, Xudong Pan, Geng Hong, Min Yang', 'link': 'https://arxiv.org/abs/2504.13707', 'abstract': 'As the general capabilities of large language models (LLMs) improve and agent applications become more widespread, the underlying deception risks urgently require systematic evaluation and effective oversight. Unlike existing evaluation which uses simulated games or presents limited choices, we introduce OpenDeception, a novel deception evaluation framework with an open-ended scenario dataset. OpenDeception jointly evaluates both the deception intention and capabilities of LLM-based agents by inspecting their internal reasoning process. Specifically, we construct five types of common use cases where LLMs intensively interact with the user, each consisting of ten diverse, concrete scenarios from the real world. To avoid ethical concerns and costs of high-risk deceptive interactions with human testers, we propose to simulate the multi-turn dialogue via agent simulation. Extensive evaluation of eleven mainstream LLMs on OpenDeception highlights the urgent need to address deception risks and security concerns in LLM-based agents: the deception intention ratio across the models exceeds 80%, while the deception success rate surpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do exhibit a higher risk of deception, which calls for more alignment efforts on inhibiting deceptive behaviors.', 'abstract_zh': '大型语言模型（LLMs）的一般能力提高和代理应用的普及使得潜在的欺骗风险迫切需要系统评估和有效监管：OpenDeception——一种开放场景的新型欺骗评价框架', 'title_zh': 'OpenDeception: 基于开放交互模拟的AI欺骗行为基准测试与探究'}
{'arxiv_id': 'arXiv:2504.13644', 'title': 'Exploring the Potential for Large Language Models to Demonstrate Rational Probabilistic Beliefs', 'authors': 'Gabriel Freedman, Francesca Toni', 'link': 'https://arxiv.org/abs/2504.13644', 'abstract': "Advances in the general capabilities of large language models (LLMs) have led to their use for information retrieval, and as components in automated decision systems. A faithful representation of probabilistic reasoning in these models may be essential to ensure trustworthy, explainable and effective performance in these tasks. Despite previous work suggesting that LLMs can perform complex reasoning and well-calibrated uncertainty quantification, we find that current versions of this class of model lack the ability to provide rational and coherent representations of probabilistic beliefs. To demonstrate this, we introduce a novel dataset of claims with indeterminate truth values and apply a number of well-established techniques for uncertainty quantification to measure the ability of LLM's to adhere to fundamental properties of probabilistic reasoning.", 'abstract_zh': '大型语言模型（LLMs）的一般能力进步使其适用于信息检索，并作为自动决策系统中的组件。在这些模型中忠实表现概率推理可能是确保这些任务中可信、可解释和有效的性能的关键。尽管先前的工作表明LLMs可以进行复杂的推理和准确的不确定度量化，我们发现当前这一类模型版本缺乏提供合理连贯的概率信念表示的能力。为了证明这一点，我们引入了一个具有不确定真值的新型数据集，并应用了多种已建立的不确定度量化技术来衡量LLMs遵守概率推理基本性质的能力。', 'title_zh': '探索大型语言模型展示理性概率信念的潜力'}
{'arxiv_id': 'arXiv:2504.13443', 'title': 'Trust, but verify', 'authors': 'Michael J. Yuan, Carlos Campoy, Sydney Lai, James Snewin, Ju Long', 'link': 'https://arxiv.org/abs/2504.13443', 'abstract': 'Decentralized AI agent networks, such as Gaia, allows individuals to run customized LLMs on their own computers and then provide services to the public. However, in order to maintain service quality, the network must verify that individual nodes are running their designated LLMs. In this paper, we demonstrate that in a cluster of mostly honest nodes, we can detect nodes that run unauthorized or incorrect LLM through social consensus of its peers. We will discuss the algorithm and experimental data from the Gaia network. We will also discuss the intersubjective validation system, implemented as an EigenLayer AVS to introduce financial incentives and penalties to encourage honest behavior from LLM nodes.', 'abstract_zh': '去中心化AI代理网络Gaia允許個體在其自己的計算機上運行自定义的大 LENGSTMODEL，然後為公眾提供服務。然而，為了維護服務質量，該網絡必須驗證個別節點是否運行分配給它的 LENGSTMODEL。在本文中，我們展示在 hầu hết诚实的節點集群中，我們可以通過同伴的社會共識來檢測運行未授權或錯誤 LENGSTMODEL 的節點。我們將討論該算法並呈現Gaia網絡的實驗數據。我們還將討論實現的主觀驗證系統EigenLayer AVS，該系統通過引入財政激勵和懲罰來促進LENWGHTMODEL節點的誠實行為。', 'title_zh': '信任，但要验证。'}
{'arxiv_id': 'arXiv:2504.13359', 'title': 'Cost-of-Pass: An Economic Framework for Evaluating Language Models', 'authors': 'Mehmet Hamza Erol, Batu El, Mirac Suzgun, Mert Yuksekgonul, James Zou', 'link': 'https://arxiv.org/abs/2504.13359', 'abstract': 'The widespread adoption of AI systems in the economy hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics that account for both performance and costs. We propose a framework grounded in production theory for evaluating language models by combining accuracy and inference cost. We introduce "cost-of-pass", the expected monetary cost of generating a correct solution. We then define the "frontier cost-of-pass" as the minimum cost-of-pass achievable across available models or the "human-expert, using the approximate cost of hiring an expert. Our analysis reveals distinct economic insights. First, lightweight models are most cost-effective for basic quantitative tasks, large models for knowledge-intensive ones, and reasoning models for complex quantitative problems, despite higher per-token costs. Second, tracking this frontier cost-of-pass over the past year reveals significant progress, particularly for complex quantitative tasks where the cost has roughly halved every few months. Third, to trace key innovations driving this progress, we examine counterfactual frontiers: estimates of cost-efficiency without specific model classes. We find that innovations in lightweight, large, and reasoning models have been essential for pushing the frontier in basic quantitative, knowledge-intensive, and complex quantitative tasks, respectively. Finally, we assess the cost-reductions afforded by common inference-time techniques like majority voting and self-refinement, finding that their marginal accuracy gains rarely justify their costs. Our findings underscore that complementary model-level innovations are the primary drivers of cost-efficiency, and our economic framework provides a principled tool for measuring this progress and guiding deployment.', 'abstract_zh': 'AI系统在经济中的广泛采用取决于其生成的经济效益是否超过其推理成本。评估这一权衡需要同时考虑性能和成本的指标。我们提出了一种基于生产理论的框架，通过结合准确性和推理成本来评价语言模型。我们引入了“成本-通过”（cost-of-pass）的概念，即生成正确解决方案的预期货币成本。然后定义了“前沿成本-通过”（frontier cost-of-pass），这是在可用模型中达到的最小成本-通过，或者使用专家的大致成本来衡量“人类专家”。我们的分析揭示了不同的经济洞见。首先，轻量级模型对于基本定量任务最具成本效益，大型模型对于知识密集型任务最具成本效益，而推理模型对于复杂的定量问题最具成本效益，尽管每单位成本较高。其次，过去一年中跟踪这一前沿成本-通过显示了显著的进步，特别是在复杂的定量任务中，成本每几个月大致减半。第三，为了追踪推动这一进展的关键创新，我们检查了反事实前沿：在没有特定模型类别的情况下对成本效率的估计。我们发现，在基本定量、知识密集型和复杂定量任务中，轻量级、大型和推理模型的创新都至关重要。最后，我们评估了诸如多数投票和自我校正等常见推理时技术的成本降低，发现它们的边际准确率提升 rarely 通常不值得其成本。我们的研究强调互补的模型级创新是降低成本效率的主要驱动力，而我们的经济框架提供了一种原则性的工具来衡量这一进展并指导部署。', 'title_zh': '成本传递：一种评估语言模型的经济学框架'}
{'arxiv_id': 'arXiv:2504.13263', 'title': 'Causal-Copilot: An Autonomous Causal Analysis Agent', 'authors': 'Xinyue Wang, Kun Zhou, Wenyi Wu, Har Simrat Singh, Fang Nan, Songyao Jin, Aryan Philip, Saloni Patnaik, Hou Zhu, Shivam Singh, Parjanya Prashant, Qian Shen, Biwei Huang', 'link': 'https://arxiv.org/abs/2504.13263', 'abstract': 'Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, we introduce Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data -- including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, our system fosters a virtuous cycle -- expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis.', 'abstract_zh': '因果分析在科学研究和可靠决策中发挥着基础性作用，但由于其概念和算法的复杂性，仍难以为领域专家所利用。因果方法论与实际应用之间的这种脱节构成了双重挑战：领域专家无法利用因果学习的最新进展，而因果研究人员缺乏广泛的实际部署来测试和改进他们的方法。为解决这一问题，我们引入了Causal-Copilot，这是一种自主代理，它在大型语言模型框架内实现专家级的因果分析。Causal-Copilot实现了因果分析的完整管道，包括因果发现、因果推断、算法选择、超参数优化、结果解释以及生成可操作的见解。它通过自然语言支持交互式细化，降低非专业人士的门槛，同时保持方法论的严谨性。通过整合超过20种最先进的因果分析技术，我们的系统促进了良性循环——为领域专家扩展高级因果方法的访问权限，产生丰富的实际应用，从而指导和推进因果理论的发展。实证评估表明，Causal-Copilot在性能上优于现有基线，提供了一个可靠、可扩展且可扩展的解决方案，能够弥合因果分析中理论 sophistication 与实际应用性的差距。', 'title_zh': '因果协驾驶：一个自主因果分析代理'}
{'arxiv_id': 'arXiv:2504.13202', 'title': 'The Quantum LLM: Modeling Semantic Spaces with Quantum Principles', 'authors': 'Timo Aukusti Laine', 'link': 'https://arxiv.org/abs/2504.13202', 'abstract': 'In the previous article, we presented a quantum-inspired framework for modeling semantic representation and processing in Large Language Models (LLMs), drawing upon mathematical tools and conceptual analogies from quantum mechanics to offer a new perspective on these complex systems. In this paper, we clarify the core assumptions of this model, providing a detailed exposition of six key principles that govern semantic representation, interaction, and dynamics within LLMs. The goal is to justify that a quantum-inspired framework is a valid approach to studying semantic spaces. This framework offers valuable insights into their information processing and response generation, and we further discuss the potential of leveraging quantum computing to develop significantly more powerful and efficient LLMs based on these principles.', 'abstract_zh': '前一篇文章中，我们提出了一种受量子力学启发的框架，用于建模大型语言模型（LLMs）中的语义表示和处理，借用了量子力学中的数学工具和概念类比，为这些复杂系统提供了新的视角。本文旨在阐明这一模型的核心假设，详细阐述了六个关键原则，这些原则规范了LLMs中的语义表示、交互和动态。我们的目标是证明，量子启发框架是一种有效研究语义空间的方法。该框架提供了关于其信息处理和响应生成的重要见解，并进一步讨论了这些原则在利用量子计算开发更强大、更高效的LLMs方面的潜力。', 'title_zh': '量子大规模语言模型：基于量子原理建模语义空间'}
{'arxiv_id': 'arXiv:2504.13828', 'title': 'Generative AI Act II: Test Time Scaling Drives Cognition Engineering', 'authors': 'Shijie Xia, Yiwei Qin, Xuefeng Li, Yan Ma, Run-Ze Fan, Steffi Chern, Haoyang Zou, Fan Zhou, Xiangkun Hu, Jiahe Jin, Yanheng He, Yixin Ye, Yixiu Liu, Pengfei Liu', 'link': 'https://arxiv.org/abs/2504.13828', 'abstract': 'The first generation of Large Language Models - what might be called "Act I" of generative AI (2020-2023) - achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations in knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of "Act II" (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes a mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AI\'s second act. We provide a regularly updated collection of papers on test-time scaling in the GitHub Repository: this https URL', 'abstract_zh': '第一代大型语言模型——生成式AI的“第一幕”（2020-2023）——通过大量的参数和数据缩放实现了显著的成功，但表现出根本的知识延迟、浅层推理和受限的认知过程。在这个时代，提示工程成为我们与AI的主要接口，通过自然语言实现对话级的交流。我们现在见证了“第二幕”的开始（2024-present），模型正在从潜在空间的知识检索系统过渡为通过测试时缩放技术构建思想的引擎，这一新的范式通过基于语言的思想与AI建立心智级的连接。在本文中，我们阐明了认知工程的理论基础，并解释了为什么此时此刻对于其发展至关重要。我们系统地通过全面的教程和优化的实现来分解这些高级方法，使认知工程的访问权更加普及，使每一位实践者都能参与到AI的第二幕中。我们提供了一个定期更新的测试时缩放论文集合在GitHub Repository中：这个 https URL。', 'title_zh': '生成式AI第二季：测试时缩放驱动认知工程'}
{'arxiv_id': 'arXiv:2504.13818', 'title': 'Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning', 'authors': 'Yixuan Even Xu, Yash Savani, Fei Fang, Zico Kolter', 'link': 'https://arxiv.org/abs/2504.13818', 'abstract': 'Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing reasoning capabilities in large language models, but faces a fundamental asymmetry in computation and memory requirements: inference is embarrassingly parallel with a minimal memory footprint, while policy updates require extensive synchronization and are memory-intensive. To address this asymmetry, we introduce PODS (Policy Optimization with Down-Sampling), a framework that strategically decouples these phases by generating numerous rollouts in parallel but updating only on an informative subset. Within this framework, we develop max-variance down-sampling, a theoretically motivated method that selects rollouts with maximally diverse reward signals. We prove that this approach has an efficient algorithmic solution, and empirically demonstrate that GRPO with PODS using max-variance down-sampling achieves superior performance over standard GRPO on the GSM8K benchmark.', 'abstract_zh': '基于降采样的策略优化（PODS）框架：解决强化学习在大规模语言模型中推理与政策更新之间的计算和内存要求不对称性', 'title_zh': '并非所有rollout都有用：LLM强化学习中的rollout下采样'}
{'arxiv_id': 'arXiv:2504.13774', 'title': 'DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs', 'authors': 'Tamim Al Mahmud, Najeeb Jebreel, Josep Domingo-Ferrer, David Sanchez', 'link': 'https://arxiv.org/abs/2504.13774', 'abstract': 'Large language models (LLMs) have recently revolutionized language processing tasks but have also brought ethical and legal issues. LLMs have a tendency to memorize potentially private or copyrighted information present in the training data, which might then be delivered to end users at inference time. When this happens, a naive solution is to retrain the model from scratch after excluding the undesired data. Although this guarantees that the target data have been forgotten, it is also prohibitively expensive for LLMs. Approximate unlearning offers a more efficient alternative, as it consists of ex post modifications of the trained model itself to prevent undesirable results, but it lacks forgetting guarantees because it relies solely on empirical evidence. In this work, we present DP2Unlearning, a novel LLM unlearning framework that offers formal forgetting guarantees at a significantly lower cost than retraining from scratch on the data to be retained. DP2Unlearning involves training LLMs on textual data protected using {\\epsilon}-differential privacy (DP), which later enables efficient unlearning with the guarantees against disclosure associated with the chosen {\\epsilon}. Our experiments demonstrate that DP2Unlearning achieves similar model performance post-unlearning, compared to an LLM retraining from scratch on retained data -- the gold standard exact unlearning -- but at approximately half the unlearning cost. In addition, with a reasonable computational cost, it outperforms approximate unlearning methods at both preserving the utility of the model post-unlearning and effectively forgetting the targeted information.', 'abstract_zh': 'DP2Unlearning: 一种以较低成本提供正式遗忘保证的大语言模型删除框架', 'title_zh': 'DP2Unlearning: 一个高效且有保证的大语言模型遗忘框架'}
{'arxiv_id': 'arXiv:2504.13756', 'title': 'Scaling sparse feature circuit finding for in-context learning', 'authors': 'Dmitrii Kharlapenko, Stepan Shabalin, Fazl Barez, Arthur Conmy, Neel Nanda', 'link': 'https://arxiv.org/abs/2504.13756', 'abstract': "Sparse autoencoders (SAEs) are a popular tool for interpreting large language model activations, but their utility in addressing open questions in interpretability remains unclear. In this work, we demonstrate their effectiveness by using SAEs to deepen our understanding of the mechanism behind in-context learning (ICL). We identify abstract SAE features that (i) encode the model's knowledge of which task to execute and (ii) whose latent vectors causally induce the task zero-shot. This aligns with prior work showing that ICL is mediated by task vectors. We further demonstrate that these task vectors are well approximated by a sparse sum of SAE latents, including these task-execution features. To explore the ICL mechanism, we adapt the sparse feature circuits methodology of Marks et al. (2024) to work for the much larger Gemma-1 2B model, with 30 times as many parameters, and to the more complex task of ICL. Through circuit finding, we discover task-detecting features with corresponding SAE latents that activate earlier in the prompt, that detect when tasks have been performed. They are causally linked with task-execution features through the attention and MLP sublayers.", 'abstract_zh': '稀疏自编码器在加深对上下文学习机制理解中的有效性', 'title_zh': '基于上下文的学习中稀疏特征电路搜索的扩展'}
{'arxiv_id': 'arXiv:2504.13730', 'title': 'Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence', 'authors': 'Paul K. Mandal, Cole Leo, Connor Hurley', 'link': 'https://arxiv.org/abs/2504.13730', 'abstract': 'Open-source intelligence provides a stream of unstructured textual data that can inform assessments of territorial control. We present CONTACT, a framework for territorial control prediction using large language models (LLMs) and minimal supervision. We evaluate two approaches: SetFit, an embedding-based few-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a multilingual generative LLM. Our model is trained on a small hand-labeled dataset of news articles covering ISIS activity in Syria and Iraq, using prompt-conditioned extraction of control-relevant signals such as military operations, casualties, and location references. We show that the BLOOMZ-based model outperforms the SetFit baseline, and that prompt-based supervision improves generalization in low-resource settings. CONTACT demonstrates that LLMs fine-tuned using few-shot methods can reduce annotation burdens and support structured inference from open-ended OSINT streams. Our code is available at this https URL.', 'abstract_zh': '开源情报提供了一条未结构化的文本数据流，可用于评估领土控制。本文介绍了CONTACT框架，该框架利用大规模语言模型（LLMs）和最少监督来预测领土控制。我们评估了两种方法：基于嵌入的少量示例分类器SetFit，以及应用于多语言生成性LLM BLOOMZ-560m的提示调优方法。模型使用少量手标注的新闻文章数据集进行训练，这些新闻文章涵盖了伊拉克和叙利亚的ISIS活动，提取了与控制相关的信号，如军事行动、伤亡情况和位置参考。结果显示，基于BLOOMZ的模型优于SetFit基线，提示引导的监督在资源有限的情况下改善了泛化能力。CONTACT证明了使用少量示例方法微调的LLMs可以减少标注负担，并支持从开放源代码情报流中进行结构化推理。代码可在以下链接获取：this https URL。', 'title_zh': 'Controlled Territory and Conflict Tracking (CONTACT): 从开源情报（Geo-）绘制占领领土'}
{'arxiv_id': 'arXiv:2504.13677', 'title': 'Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results', 'authors': 'Andrea Santilli, Adam Golinski, Michael Kirchhof, Federico Danieli, Arno Blaas, Miao Xiong, Luca Zappella, Sinead Williamson', 'link': 'https://arxiv.org/abs/2504.13677', 'abstract': 'Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for improving their safety and reliability. Evaluations often use performance metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). In this paper, we show that commonly used correctness functions bias UQ evaluations by inflating the performance of certain UQ methods. We evaluate 7 correctness functions -- from lexical-based and embedding-based metrics to LLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our analysis reveals that length biases in the errors of these correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LLM-as-a-judge approaches as among the least length-biased choices and hence a potential solution to mitigate these biases.', 'abstract_zh': '语言模型中不确定性量化（UQ）对于提高其安全性和可靠性至关重要。常见的正确性函数会偏倚UQ评估，从而使某些UQ方法的表现被夸大。本文通过在4个数据集和4个模型上评估7种不同正确性函数与6种UQ方法的组合，发现这些正确性函数中的长度偏倚会与UQ方法中的长度偏倚相互作用，从而扭曲UQ评估。我们将LLM-as-a-judge方法识别为其中最少长度偏倚的选择，有可能成为缓解这些偏倚的解决方案。', 'title_zh': '重温语言模型中不确定性量化评估：与响应长度的虚假交互及其影响结果的研究'}
{'arxiv_id': 'arXiv:2504.13667', 'title': 'Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm', 'authors': 'Russell Beale', 'link': 'https://arxiv.org/abs/2504.13667', 'abstract': 'This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology. We review the effects of LLMs on education so far, and make the case that these effects are minor compared to the upcoming changes that are occurring. We present a small scenario and self-ethnographic study demonstrating the effects of these changes, and define five significant considerations that interactive systems designers will have to accommodate in the future.', 'abstract_zh': '这篇文章探讨了大型语言模型对儿童学习方式及其与技术互动预期的潜在深远影响，并指出这些影响与即将到来的变革相比显得微不足道。我们回顾了大型语言模型迄今为止对教育的影响，展示了变革效果的小规模情景和自我民族志研究，并定义了未来交互系统设计者必须考虑的五个重要方面。', 'title_zh': '大型语言模型将改变儿童对技术的认知方式，并影响每一交互范式。'}
{'arxiv_id': 'arXiv:2504.13656', 'title': 'Do Prompt Patterns Affect Code Quality? A First Empirical Assessment of ChatGPT-Generated Code', 'authors': 'Antonio Della Porta, Stefano Lambiase, Fabio Palomba', 'link': 'https://arxiv.org/abs/2504.13656', 'abstract': 'Large Language Models (LLMs) have rapidly transformed software development, especially in code generation. However, their inconsistent performance, prone to hallucinations and quality issues, complicates program comprehension and hinders maintainability. Research indicates that prompt engineering-the practice of designing inputs to direct LLMs toward generating relevant outputs-may help address these challenges. In this regard, researchers have introduced prompt patterns, structured templates intended to guide users in formulating their requests. However, the influence of prompt patterns on code quality has yet to be thoroughly investigated. An improved understanding of this relationship would be essential to advancing our collective knowledge on how to effectively use LLMs for code generation, thereby enhancing their understandability in contemporary software development. This paper empirically investigates the impact of prompt patterns on code quality, specifically maintainability, security, and reliability, using the Dev-GPT dataset. Results show that Zero-Shot prompting is most common, followed by Zero-Shot with Chain-of-Thought and Few-Shot. Analysis of 7583 code files across quality metrics revealed minimal issues, with Kruskal-Wallis tests indicating no significant differences among patterns, suggesting that prompt structure may not substantially impact these quality metrics in ChatGPT-assisted code generation.', 'abstract_zh': '大型语言模型（LLMs）迅速改变了软件开发，尤其是在代码生成方面。然而，它们不一致的表现，容易产生幻觉和质量问题，复杂了程序的理解并阻碍了维护性。研究表明，通过设计输入以引导LLMs生成相关输出的提示工程可能有助于应对这些挑战。在这方面，研究人员引入了提示模式，这是一种结构化的模板，旨在引导用户提出他们的请求。然而，提示模式对代码质量的影响尚未得到充分调查。对这一关系的更深入理解将有助于推进我们对如何有效使用LLMs进行代码生成的知识，从而增强它们在当代软件开发中的可理解性。本文使用Dev-GPT数据集实证研究了提示模式对代码质量，特别是维护性、安全性和可靠性的影響。结果表明，零-shot提示最常见，其次是零-shot带思维链和少量-shot。对7583个代码文件的质量指标分析显示，几乎没有问题，克鲁斯卡尔-瓦利检验表明不同模式之间没有显著差异，这表明提示结构在ChatGPT辅助代码生成中的可能对这些质量指标不会产生重大影响。', 'title_zh': '提示模式会影响代码质量吗？ChatGPT生成代码的首次实证评估'}
{'arxiv_id': 'arXiv:2504.13629', 'title': 'Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing', 'authors': 'Cong William Lin, Wu Zhu', 'link': 'https://arxiv.org/abs/2504.13629', 'abstract': 'Large Language Models (LLMs), such as ChatGPT, are reshaping content creation and academic writing. This study investigates the impact of AI-assisted generative revisions on research manuscripts, focusing on heterogeneous adoption patterns and their influence on writing convergence. Leveraging a dataset of over 627,000 academic papers from arXiv, we develop a novel classification framework by fine-tuning prompt- and discipline-specific large language models to detect the style of ChatGPT-revised texts. Our findings reveal substantial disparities in LLM adoption across academic disciplines, gender, native language status, and career stage, alongside a rapid evolution in scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness, and adherence to formal writing conventions, with improvements varying by revision type. Finally, a difference-in-differences analysis shows that while LLMs drive convergence in academic writing, early adopters, male researchers, non-native speakers, and junior scholars exhibit the most pronounced stylistic shifts, aligning their writing more closely with that of established researchers.', 'abstract_zh': '大型语言模型（LLMs）如ChatGPT正在重塑内容创作和学术写作。本研究探讨了AI辅助生成性修订对研究手稿的影响，重点关注不同学科中LLM采用模式的异质性及其对写作收敛的影响。利用arXiv上的超过627,000篇学术论文数据集，我们通过微调针对特定提示和学科的大规模语言模型，开发了一个新颖的分类框架以检测ChatGPT修订文本的风格。研究表明，LLM在不同学科、性别、母语状态和职业生涯阶段中的采用存在显著差异，同时学术写作风格正在迅速演变。此外，LLM的使用提高了清晰度、简洁性和对正式写作规范的遵守，但不同类型的修订所带来的改进有所不同。最后，差分分析表明，虽然LLMs促进了学术写作的收敛，但早期采用者、男性研究人员、非母语使用者和初级学者显示出最显著的风格变化，使他们的写作更加接近资深研究人员的风格。', 'title_zh': '不同的大规模语言模型采用路径与异质收敛路径在科研写作中的体现'}
{'arxiv_id': 'arXiv:2504.13587', 'title': 'RAG Without the Lag: Interactive Debugging for Retrieval-Augmented Generation Pipelines', 'authors': 'Quentin Romero Lauro, Shreya Shankar, Sepanta Zeighami, Aditya Parameswaran', 'link': 'https://arxiv.org/abs/2504.13587', 'abstract': "Retrieval-augmented generation (RAG) pipelines have become the de-facto approach for building AI assistants with access to external, domain-specific knowledge. Given a user query, RAG pipelines typically first retrieve (R) relevant information from external sources, before invoking a Large Language Model (LLM), augmented (A) with this information, to generate (G) responses. Modern RAG pipelines frequently chain multiple retrieval and generation components, in any order. However, developing effective RAG pipelines is challenging because retrieval and generation components are intertwined, making it hard to identify which component(s) cause errors in the eventual output. The parameters with the greatest impact on output quality often require hours of pre-processing after each change, creating prohibitively slow feedback cycles. To address these challenges, we present RAGGY, a developer tool that combines a Python library of composable RAG primitives with an interactive interface for real-time debugging. We contribute the design and implementation of RAGGY, insights into expert debugging patterns through a qualitative study with 12 engineers, and design implications for future RAG tools that better align with developers' natural workflows.", 'abstract_zh': '检索增强生成（RAG）管道已成为构建访问外部领域特定知识的AI助手的默认方法。给定用户查询，RAG管道通常首先从外部来源检索（R）相关信息，然后调用一个增强（A）了这些信息的大语言模型（LLM）来生成（G）响应。现代RAG管道经常以任意顺序串连多个检索和生成组件。然而，由于检索和生成组件交织，开发有效的RAG管道具有挑战性，难以识别哪个组件导致最终输出中的错误。参数对输出质量影响最大的往往需要在每次变更后进行数小时的预处理，从而创建了令人难以接受的缓慢反馈循环。为应对这些挑战，我们提出了一种名为RAGGY的开发者工具，结合了一个可组合的RAG原语Python库和一个实时调试的交互界面。我们提供了RAGGY的设计和实现、通过与12名工程师进行定性研究获得的专家调试模式洞见，以及对未来更好地与开发者自然工作流程对齐的RAG工具的设计启示。', 'title_zh': 'RAG无需延迟：检索增强生成管道的交互式调试'}
{'arxiv_id': 'arXiv:2504.13534', 'title': 'CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models', 'authors': 'Feiyang Li, Peng Fang, Zhan Shi, Arijit Khan, Fang Wang, Dan Feng, Weihao Wang, Xin Zhang, Yongjian Cui', 'link': 'https://arxiv.org/abs/2504.13534', 'abstract': 'While chain-of-thought (CoT) reasoning improves the performance of large language models (LLMs) in complex tasks, it still has two main challenges: the low reliability of relying solely on LLMs to generate reasoning chains and the interference of natural language reasoning chains on the inference logic of LLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation, featuring knowledge graphs to modulate reasoning chain generation of LLMs, thereby enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented generation (RAG) into knowledge graphs to retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable information; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to execute reasoning tasks in pseudo-programs with greater logical rigor. We conduct a comprehensive evaluation on nine public datasets, covering three reasoning problems. Compared with the-state-of-the-art methods, CoT-RAG exhibits a significant accuracy improvement, ranging from 4.0% to 23.0%. Furthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable accuracy and efficient execution, highlighting its strong practical applicability and scalability.', 'abstract_zh': '基于知识图谱的链式思考增强检索生成框架：CoT-RAG', 'title_zh': 'CoT-RAG：结合思维链和检索增强生成以增强大型语言模型的推理能力'}
{'arxiv_id': 'arXiv:2504.13515', 'title': 'Large Language Models for Validating Network Protocol Parsers', 'authors': 'Mingwei Zheng, Danning Xie, Xiangyu Zhang', 'link': 'https://arxiv.org/abs/2504.13515', 'abstract': 'Network protocol parsers are essential for enabling correct and secure communication between devices. Bugs in these parsers can introduce critical vulnerabilities, including memory corruption, information leakage, and denial-of-service attacks. An intuitive way to assess parser correctness is to compare the implementation with its official protocol standard. However, this comparison is challenging because protocol standards are typically written in natural language, whereas implementations are in source code. Existing methods like model checking, fuzzing, and differential testing have been used to find parsing bugs, but they either require significant manual effort or ignore the protocol standards, limiting their ability to detect semantic violations. To enable more automated validation of parser implementations against protocol standards, we propose PARVAL, a multi-agent framework built on large language models (LLMs). PARVAL leverages the capabilities of LLMs to understand both natural language and code. It transforms both protocol standards and their implementations into a unified intermediate representation, referred to as format specifications, and performs a differential comparison to uncover inconsistencies. We evaluate PARVAL on the Bidirectional Forwarding Detection (BFD) protocol. Our experiments demonstrate that PARVAL successfully identifies inconsistencies between the implementation and its RFC standard, achieving a low false positive rate of 5.6%. PARVAL uncovers seven unique bugs, including five previously unknown issues.', 'abstract_zh': '网络协议解析器对于实现设备之间正确的安全通信至关重要。这些解析器中的漏洞可能会引入关键的安全风险，包括内存损坏、信息泄露和拒绝服务攻击。评估解析器正确性的直观方法是将其实现与其官方协议标准进行比较。然而，这种比较具有挑战性，因为协议标准通常用自然语言编写，而实现则是用源代码编写的。现有的方法如模型检查、模糊测试和差异测试已被用于查找解析器漏洞，但这些方法要么需要大量的手动工作，要么忽视了协议标准，限制了它们检测语义违规的能力。为了实现对协议标准的更自动化的解析器实现验证，我们提出了一种基于大型语言模型（LLMs）的多代理框架PARVAL。PARVAL利用了LLMs理解自然语言和代码的能力，将协议标准及其实现都转换为统一的中间表示，称为格式规范，并进行差异比较以发现不一致之处。我们对双向转发检测（BFD）协议进行了评估。实验结果表明，PARVAL成功地识别了实现与其RFC标准之间的不一致，且误报率为5.6%。PARVAL发现了七个独特的漏洞，其中包括五个未知的问题。', 'title_zh': '大型语言模型在验证网络协议解析器中的应用'}
{'arxiv_id': 'arXiv:2504.13472', 'title': 'CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation', 'authors': 'Xinchen Wang, Pengfei Gao, Chao Peng, Ruida Hu, Cuiyun Gao', 'link': 'https://arxiv.org/abs/2504.13472', 'abstract': 'Large language models (LLMs) have demonstrated strong capabilities in code generation, underscoring the critical need for rigorous and comprehensive evaluation. Existing evaluation approaches fall into three categories, including human-centered, metric-based, and LLM-based. Considering that human-centered approaches are labour-intensive and metric-based ones overly rely on reference answers, LLM-based approaches are gaining increasing attention due to their stronger contextual understanding capabilities and superior efficiency. However, the performance of LLM-based approaches remains limited due to: (1) lack of multisource domain knowledge, and (2) insufficient comprehension of complex code.\nTo mitigate the limitations, we propose CodeVisionary, the first LLM-based agent framework for evaluating LLMs in code generation. CodeVisionary consists of two stages: (1) Multiscore knowledge analysis stage, which aims to gather multisource and comprehensive domain knowledge by formulating and executing a stepwise evaluation plan. (2) Negotiation-based scoring stage, which involves multiple judges engaging in discussions to better comprehend the complex code and reach a consensus on the evaluation score. Extensive experiments demonstrate that CodeVisionary achieves the best performance for evaluating LLMs in code generation, outperforming the best baseline methods with average improvements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau coefficients, respectively. Besides, CodeVisionary provides detailed evaluation reports, which assist developers in identifying shortcomings and making improvements. The resources of CodeVisionary are available at this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在代码生成方面展现了强大的能力，强调了严格和全面评估的迫切需要。现有的评估方法主要分为三类：以人类为中心、基于指标和基于大语言模型的方法。鉴于以人类为中心的方法耗时且基于指标的方法过度依赖参考答案，基于大语言模型的方法因其更强的上下文理解能力和更高的效率而受到越来越多的关注。然而，基于大语言模型的方法在评价代码生成方面的能力依然有限，主要原因在于（1）缺乏多源领域知识，以及（2）对复杂代码理解不足。\n\n为克服这些局限性，我们提出了CodeVisionary，这是一种用于代码生成评估的大规模语言模型（LLM）基于的代理框架。CodeVisionary包括两个阶段：（1）多源知识分析阶段，旨在通过制定并执行分步骤的评估计划来收集多源和全面的领域知识。（2）基于谈判的评分阶段，涉及多个评审者进行讨论，以更好地理解复杂代码并达成一致的评估分数。广泛实验结果表明，CodeVisionary在代码生成评估方面性能最佳，与最佳基线方法相比，在皮尔逊系数、斯皮尔曼系数和肯德尔τ系数上的平均改进分别为0.202、0.139和0.117。此外，CodeVisionary提供了详细的评估报告，帮助开发者识别不足并进行改进。CodeVisionary的资源可访问此链接：此https URL。', 'title_zh': 'CodeVisionary：一种基于代理的大型语言模型代码生成评估框架'}
{'arxiv_id': 'arXiv:2504.13460', 'title': 'Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization', 'authors': 'Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma', 'link': 'https://arxiv.org/abs/2504.13460', 'abstract': "Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the model's ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark.", 'abstract_zh': '基于链式推理的少样本 temporal 动作定位方法', 'title_zh': '基于链式思维的文字推理在少样本时序动作定位中的应用'}
{'arxiv_id': 'arXiv:2504.13261', 'title': 'CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models', 'authors': 'Dong Wang', 'link': 'https://arxiv.org/abs/2504.13261', 'abstract': "Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT has significantly impacted foreign language education, yet their pedagogical grammar competence remains under-assessed. This paper introduces CPG-EVAL, the first dedicated benchmark specifically designed to evaluate LLMs' knowledge of pedagogical grammar within the context of foreign language instruction. Methodology: The benchmark comprises five tasks designed to assess grammar recognition, fine-grained grammatical distinction, categorical discrimination, and resistance to linguistic interference. Findings: Smaller-scale models can succeed in single language instance tasks, but struggle with multiple instance tasks and interference from confusing instances. Larger-scale models show better resistance to interference but still have significant room for accuracy improvement. The evaluation indicates the need for better instructional alignment and more rigorous benchmarks, to effectively guide the deployment of LLMs in educational contexts. Value: This study offers the first specialized, theory-driven, multi-tiered benchmark framework for systematically evaluating LLMs' pedagogical grammar competence in Chinese language teaching contexts. CPG-EVAL not only provides empirical insights for educators, policymakers, and model developers to better gauge AI's current abilities in educational settings, but also lays the groundwork for future research on improving model alignment, enhancing educational suitability, and ensuring informed decision-making concerning LLM integration in foreign language instruction.", 'abstract_zh': '目的：快速涌现的大规模语言模型（LLMs）如ChatGPT极大地影响了外语教育，然而它们的教学语法能力尚未得到充分评估。本文介绍了CPG-EVAL，这是首个专门设计来评估LLMs在外语教学环境中对教学语法知识掌握情况的基准。方法：基准包括五项任务，用于评估语法识别、精细的语法区分、类别区分以及抵抗语言干扰的能力。发现：较小规模的模型在单语言实例任务中可以成功，但在涉及多个实例的任务和干扰实例时表现出困难。较大的模型在抵抗干扰方面表现更好，但仍然存在显著的准确度提升空间。评估表明需要更好的教学对齐和更严格的基准，以有效指导LLMs在教育环境中的应用。价值：本研究提供了首个专门的、理论驱动的多层次基准框架，系统评估LLMs在中文教学环境中的教学语法能力。CPG-EVAL不仅为教育者、政策制定者和模型开发者提供了实证见解，以更好地评估AI在教育环境中的当前能力，还为未来提高模型对齐、增强教育适用性和确保关于LLMs在外语教学中集成的知情决策奠定了基础。', 'title_zh': 'CPG- EVAL：评估大型语言模型中文教学语法能力的多层级基准'}
{'arxiv_id': 'arXiv:2504.13227', 'title': 'DIDS: Domain Impact-aware Data Sampling for Large Language Model Training', 'authors': 'Weijie Shi, Jipeng Zhang, Yaguang Wu, Jingzhi Fang, Ruiyuan Zhang, Jiajie Xu, Jia Zhu, Hao Chen, Yao Zhao, Sirui Han, Xiaofang Zhou', 'link': 'https://arxiv.org/abs/2504.13227', 'abstract': "Large language models (LLMs) are commonly trained on multi-domain datasets, where domain sampling strategies significantly impact model performance due to varying domain importance across downstream tasks. Existing approaches for optimizing domain-level sampling strategies struggle with maintaining intra-domain consistency and accurately measuring domain impact. In this paper, we present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain consistency, a gradient clustering algorithm is proposed to group training data based on their learning effects, where a proxy language model and dimensionality reduction are employed to reduce computational overhead. To accurately measure domain impact, we develop a Fisher Information Matrix (FIM) guided metric that quantifies how domain-specific parameter updates affect the model's output distributions on downstream tasks, with theoretical guarantees. Furthermore, to determine optimal sampling ratios, DIDS combines both the FIM-guided domain impact assessment and loss learning trajectories that indicate domain-specific potential, while accounting for diminishing marginal returns. Extensive experiments demonstrate that DIDS achieves 3.4% higher average performance while maintaining comparable training efficiency.", 'abstract_zh': '基于领域影响的数据采样（DIDS）：确保领域一致性并准确衡量领域影响的策略', 'title_zh': 'DIDS: 域影响感知的数据采样用于大型语言模型训练'}
{'arxiv_id': 'arXiv:2504.13217', 'title': 'Sustainability via LLM Right-sizing', 'authors': 'Jennifer Haase, Finn Klessascheck, Jan Mendling, Sebastian Pokutta', 'link': 'https://arxiv.org/abs/2504.13217', 'abstract': 'Large language models (LLMs) have become increasingly embedded in organizational workflows. This has raised concerns over their energy consumption, financial costs, and data sovereignty. While performance benchmarks often celebrate cutting-edge models, real-world deployment decisions require a broader perspective: when is a smaller, locally deployable model "good enough"? This study offers an empirical answer by evaluating eleven proprietary and open-weight LLMs across ten everyday occupational tasks, including summarizing texts, generating schedules, and drafting emails and proposals. Using a dual-LLM-based evaluation framework, we automated task execution and standardized evaluation across ten criteria related to output quality, factual accuracy, and ethical responsibility. Results show that GPT-4o delivers consistently superior performance but at a significantly higher cost and environmental footprint. Notably, smaller models like Gemma-3 and Phi-4 achieved strong and reliable results on most tasks, suggesting their viability in contexts requiring cost-efficiency, local deployment, or privacy. A cluster analysis revealed three model groups -- premium all-rounders, competent generalists, and limited but safe performers -- highlighting trade-offs between quality, control, and sustainability. Significantly, task type influenced model effectiveness: conceptual tasks challenged most models, while aggregation and transformation tasks yielded better performances. We argue for a shift from performance-maximizing benchmarks to task- and context-aware sufficiency assessments that better reflect organizational priorities. Our approach contributes a scalable method to evaluate AI models through a sustainability lens and offers actionable guidance for responsible LLM deployment in practice.', 'abstract_zh': '大型语言模型（LLMs）在组织工作流程中的应用日益普遍。这引发了对其能源消耗、财务成本和数据主权的关注。尽管性能基准往往强调顶尖模型的表现，但真正的部署决策需要更广泛的考量：何时采用较小规模且可本地部署的模型足够好？本研究通过评估十一种专有和开源的大规模语言模型在十项日常生活职业任务中的表现，提供了一个实证答案，包括文本摘要、生成日程、撰写邮件和提案。基于双大规模语言模型的评价框架，我们自动化了任务执行，并在输出质量、事实准确性和伦理责任感等十个标准方面实现了标准化评价。结果显示，GPT-4o在所有标准上表现优异，但成本和环境影响显著更高。值得注意的是，如Gemma-3和Phi-4这样的较小模型在大多数任务中实现了强有力且可靠的结果，这表明它们在需要成本效率、本地部署或隐私保护的情境中具备可行性。聚类分析揭示了三个模型组——高端全才、称职的通用者以及有限但安全的表现者——突显了质量、控制和可持续性之间的权衡。重要的是，任务类型影响模型效果：概念性任务对大多数模型构成了挑战，而聚合和转换任务的表现更好。我们主张从最大化性能的基准转向任务和情境感知的适宜性评估，以更好地反映组织优先事项。我们的方法通过可持续性视角提供了一种可扩展的评估AI模型的方法，并为负责的大规模语言模型部署提供了切实可行的指导。', 'title_zh': '可持续性通过适配的大语言模型设计'}
{'arxiv_id': 'arXiv:2504.13216', 'title': 'KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding', 'authors': 'Bokwang Hwang, Seonkyu Lim, Taewoong Kim, Yongjae Geun, Sunghyun Bang, Sohyun Park, Jihyun Park, Myeonggyu Lee, Jinwoo Lee, Yerin Kim, Jinsun Yoo, Jingyeong Hong, Jina Park, Yongchan Kim, Suhyun Kim, Younggyun Hahm, Yiseul Lee, Yejee Kang, Chanhyuk Yoon, Chansu Lee, Heeyewon Jeong, Jiyeon Lee, Seonhye Gu, Hyebin Kang, Yousang Cho, Hangyeol Yoo, KyungTae Lim', 'link': 'https://arxiv.org/abs/2504.13216', 'abstract': 'We introduce KFinEval-Pilot, a benchmark suite specifically designed to evaluate large language models (LLMs) in the Korean financial domain. Addressing the limitations of existing English-centric benchmarks, KFinEval-Pilot comprises over 1,000 curated questions across three critical areas: financial knowledge, legal reasoning, and financial toxicity. The benchmark is constructed through a semi-automated pipeline that combines GPT-4-generated prompts with expert validation to ensure domain relevance and factual accuracy. We evaluate a range of representative LLMs and observe notable performance differences across models, with trade-offs between task accuracy and output safety across different model families. These results highlight persistent challenges in applying LLMs to high-stakes financial applications, particularly in reasoning and safety. Grounded in real-world financial use cases and aligned with the Korean regulatory and linguistic context, KFinEval-Pilot serves as an early diagnostic tool for developing safer and more reliable financial AI systems.', 'abstract_zh': 'KFinEval-Pilot：韩语金融领域的大语言模型基准套件', 'title_zh': 'KFinEval-试点：韩语金融语言理解的综合基准套件'}
{'arxiv_id': 'arXiv:2504.13196', 'title': 'Investigating cybersecurity incidents using large language models in latest-generation wireless networks', 'authors': 'Leonid Legashev, Arthur Zhigalov', 'link': 'https://arxiv.org/abs/2504.13196', 'abstract': 'The purpose of research: Detection of cybersecurity incidents and analysis of decision support and assessment of the effectiveness of measures to counter information security threats based on modern generative models. The methods of research: Emulation of signal propagation data in MIMO systems, synthesis of adversarial examples, execution of adversarial attacks on machine learning models, fine tuning of large language models for detecting adversarial attacks, explainability of decisions on detecting cybersecurity incidents based on the prompts technique. Scientific novelty: A binary classification of data poisoning attacks was performed using large language models, and the possibility of using large language models for investigating cybersecurity incidents in the latest generation wireless networks was investigated. The result of research: Fine-tuning of large language models was performed on the prepared data of the emulated wireless network segment. Six large language models were compared for detecting adversarial attacks, and the capabilities of explaining decisions made by a large language model were investigated. The Gemma-7b model showed the best results according to the metrics Precision = 0.89, Recall = 0.89 and F1-Score = 0.89. Based on various explainability prompts, the Gemma-7b model notes inconsistencies in the compromised data under study, performs feature importance analysis and provides various recommendations for mitigating the consequences of adversarial attacks. Large language models integrated with binary classifiers of network threats have significant potential for practical application in the field of cybersecurity incident investigation, decision support and assessing the effectiveness of measures to counter information security threats.', 'abstract_zh': '研究目的：基于现代生成模型检测网络安全事件并分析决策支持及对抗信息网络安全威胁措施有效性的评估。研究方法：MIMO系统中信号传播数据的仿真、敌对样本的合成、对机器学习模型执行敌对攻击、大型语言模型的细调以检测敌对攻击、基于提示技术的检测网络安全事件决策的可解释性。科学创新：使用大型语言模型对数据投毒攻击进行了二分类，并研究了使用大型语言模型在最新无线网络中调查网络安全事件的可能性。研究结果：在仿真无线网络段的数据上对大型语言模型进行了细调。六种大型语言模型被用于检测敌对攻击的比较，研究了大型语言模型决策的可解释性。Gemma-7b模型根据精度=0.89、召回率=0.89和F1分数=0.89显示最佳结果。基于多种可解释性提示，Gemma-7b模型指出研究中受攻击数据的不一致性、进行特征重要性分析并提供各种减轻敌对攻击后果的建议。将大型语言模型与网络威胁二分类器集成对于网络安全事件调查、决策支持和评估对抗信息网络安全威胁措施的有效性具有重要实际应用潜力。', 'title_zh': '使用大型语言模型研究最新一代无线网络中的网络安全事件'}
{'arxiv_id': 'arXiv:2504.13192', 'title': 'CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent', 'authors': 'Liang-bo Ning, Shijie Wang, Wenqi Fan, Qing Li, Xin Xu, Hao Chen, Feiran Huang', 'link': 'https://arxiv.org/abs/2504.13192', 'abstract': "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.", 'abstract_zh': '近期，大型语言模型（LLM）赋能的推荐系统（RecSys）在个性化用户体验方面取得了显著进步，并吸引了广泛关注。尽管取得了显著进展，关于LLM赋能RecSys的安全漏洞问题仍很大程度上未被深入研究。鉴于安全和隐私关切，更实际的做法是集中攻击黑盒RecSys，其中攻击者只能观察系统的输入和输出。然而，传统利用强化学习（RL）代理的攻击方法对攻击LLM赋能RecSys无效，因为它们在处理复杂文本输入、规划和推理方面能力有限。另一方面，LLM因其模拟人类决策过程的强大能力，提供了作为攻击代理攻击RecSys的前所未有的机会。因此，在本文中，我们提出了一种名为CheatAgent的新型攻击框架，利用LLM的人类化能力，开发基于LLM的代理来攻击LLM赋能RecSys。具体而言，我们的方法首先通过最小化输入修改来识别具有最大影响的插入位置。之后，设计LLM代理生成对抗性扰动并插入到目标位置。为了进一步提高生成扰动的质量，我们利用提示调优技术，通过受害RecSys的反馈迭代改进攻击策略。在三个真实世界数据集上的广泛实验表明了我们提出攻击方法的有效性。', 'title_zh': 'CheatAgent：通过LLM代理攻击LLM赋能的推荐系统'}
