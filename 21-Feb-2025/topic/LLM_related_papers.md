# Optimizing Model Selection for Compound AI Systems 

**Title (ZH)**: 优化复合人工智能系统中的模型选择 

**Authors**: Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Matei Zaharia, James Zou, Ion Stoica  

**Link**: [PDF](https://arxiv.org/pdf/2502.14815)  

**Abstract**: Compound AI systems that combine multiple LLM calls, such as self-refine and multi-agent-debate, achieve strong performance on many AI tasks. We address a core question in optimizing compound systems: for each LLM call or module in the system, how should one decide which LLM to use? We show that these LLM choices have a large effect on quality, but the search space is exponential. We propose LLMSelector, an efficient framework for model selection in compound systems, which leverages two key empirical insights: (i) end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed, and (ii) per-module performance can be estimated accurately by an LLM. Building upon these insights, LLMSelector iteratively selects one module and allocates to it the model with the highest module-wise performance, as estimated by an LLM, until no further gain is possible. LLMSelector is applicable to any compound system with a bounded number of modules, and its number of API calls scales linearly with the number of modules, achieving high-quality model allocation both empirically and theoretically. Experiments with popular compound systems such as multi-agent debate and self-refine using LLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector confers 5%-70% accuracy gains compared to using the same LLM for all modules. 

**Abstract (ZH)**: 结合多个LLM调用的复合AI系统，如自我 refinement 和多智能体辩论，在许多AI任务上表现出色。我们解决了一个核心问题：在系统中的每个LLM调用或模块中，应该如何决定使用哪个LLM？我们展示出这些LLM选择对质量有重大影响，但搜索空间是指数级的。我们提出LLMSelector，这是一种在复合系统中进行模型选择的高效框架，该框架利用了两个关键的实证洞见：(i) 在其他模块固定的情况下，端到端性能通常随着每个模块性能的提高而单调增加；(ii) 每个模块的性能可以通过LLM准确估计。基于这些洞见，LLMSelector 逐步选择一个模块，并分配给它由LLM估计性能最高的模型，直到无法进一步提高为止。LLMSelector适用于具有有限模块数量的任何复合系统，其API调用次数随模块数量线性变化，从实证和理论角度来看，都能实现高质量模型分配。使用GPT-4o、Claude 3.5 Sonnet 和 Gemini 1.5等LLM的多智能体辩论和自我 refinement 等 popular 复合系统实验表明，与为所有模块使用同一LLM相比，LLMSelector 可提高5%-70%的准确性。 

---
# EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations 

**Title (ZH)**: EquivaMap: 利用大规模语言模型进行优化公式等价性自动检查 

**Authors**: Haotian Zhai, Connor Lawless, Ellen Vitercik, Liu Leqi  

**Link**: [PDF](https://arxiv.org/pdf/2502.14760)  

**Abstract**: A fundamental problem in combinatorial optimization is identifying equivalent formulations, which can lead to more efficient solution strategies and deeper insights into a problem's computational complexity. The need to automatically identify equivalence between problem formulations has grown as optimization copilots--systems that generate problem formulations from natural language descriptions--have proliferated. However, existing approaches to checking formulation equivalence lack grounding, relying on simple heuristics which are insufficient for rigorous validation. Inspired by Karp reductions, in this work we introduce quasi-Karp equivalence, a formal criterion for determining when two optimization formulations are equivalent based on the existence of a mapping between their decision variables. We propose EquivaMap, a framework that leverages large language models to automatically discover such mappings, enabling scalable and reliable equivalence verification. To evaluate our approach, we construct the first open-source dataset of equivalent optimization formulations, generated by applying transformations such as adding slack variables or valid inequalities to existing formulations. Empirically, EquivaMap significantly outperforms existing methods, achieving substantial improvements in correctly identifying formulation equivalence. 

**Abstract (ZH)**: 组合优化中的一个基本问题是识别等价形式，这可以导致更高效的求解策略，并深入理解问题的计算复杂性。随着优化合作者系统的 proliferate——这些系统能够从自然语言描述中生成问题形式——自动识别问题形式之间的等价性需求日益增长。然而，现有的形式等价性验证方法缺乏坚实的依据，依赖于简单的启发式方法，这些方法不足以进行严格的验证。受 Karp 减少的启发，在这项工作中我们引入了一种准 Karp 等价性形式标准，该标准基于决策变量之间的映射来确定两个优化形式是否等价。我们提出了一种名为 EquivaMap 的框架，利用大型语言模型自动发现这样的映射，从而实现可扩展和可靠的等价性验证。为了评估我们的方法，我们构建了首个开源的等价优化形式数据集，通过应用如添加松弛变量或有效不等式的变换来生成现有的形式。实验结果显示，EquivaMap 显著优于现有方法，在正确识别形式等价性方面取得了显著的性能改进。 

---
# From Knowledge Generation to Knowledge Verification: Examining the BioMedical Generative Capabilities of ChatGPT 

**Title (ZH)**: 从知识生成到知识验证：考察ChatGPT的生物医药生成能力 

**Authors**: Ahmed Abdeen Hamed, Byung Suk Lee  

**Link**: [PDF](https://arxiv.org/pdf/2502.14714)  

**Abstract**: The generative capabilities of LLM models present opportunities in accelerating tasks and concerns with the authenticity of the knowledge it produces. To address the concerns, we present a computational approach that systematically evaluates the factual accuracy of biomedical knowledge that an LLM model has been prompted to generate. Our approach encompasses two processes: the generation of disease-centric associations and the verification of them using the semantic knowledge of the biomedical ontologies. Using ChatGPT as the select LLM model, we designed a set of prompt-engineering processes to generate linkages between diseases, drugs, symptoms, and genes to establish grounds for assessments. Experimental results demonstrate high accuracy in identifying disease terms (88%-97%), drug names (90%-91%), and genetic information (88%-98%). The symptom term identification accuracy was notably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO ontologies accordingly. The verification of associations reveals literature coverage rates of (89%-91%) among disease-drug and disease-gene associations. The low identification accuracy for symptom terms also contributed to the verification of symptom-related associations (49%-62%). 

**Abstract (ZH)**: LLM模型在加速任务中的生成能力及其生物医学知识真实性评估的系统性方法 

---
# Plan-over-Graph: Towards Parallelable LLM Agent Schedule 

**Title (ZH)**: 图上计划：走向可并行的LLM代理调度 

**Authors**: Shiqi Zhang, Xinbei Ma, Zouying Cao, Zhuosheng Zhang, Hai Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2502.14563)  

**Abstract**: Large Language Models (LLMs) have demonstrated exceptional abilities in reasoning for task planning. However, challenges remain under-explored for parallel schedules. This paper introduces a novel paradigm, plan-over-graph, in which the model first decomposes a real-life textual task into executable subtasks and constructs an abstract task graph. The model then understands this task graph as input and generates a plan for parallel execution. To enhance the planning capability of complex, scalable graphs, we design an automated and controllable pipeline to generate synthetic graphs and propose a two-stage training scheme. Experimental results show that our plan-over-graph method significantly improves task performance on both API-based LLMs and trainable open-sourced LLMs. By normalizing complex tasks as graphs, our method naturally supports parallel execution, demonstrating global efficiency. The code and data are available at this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）在任务规划推理方面展现了出色的能力，但在并行调度方面仍存在未探索的挑战。本文提出了一种新的范式——plan-over-graph，其中模型首先将实际文本任务分解为可执行的子任务并构建抽象的任务图，然后将该任务图作为输入生成并行执行的计划。为了增强对复杂、可扩展图的规划能力，我们设计了一个自动且可控的管道生成合成图，并提出了一种两阶段训练方案。实验结果表明，我们的plan-over-graph方法显著提高了基于API的LLMs和可训练的开源LLMs的任务性能。通过将复杂任务规范为图，该方法自然支持并行执行，展现了全局效率。代码和数据可访问此处：this https URL。 

---
# HPS: Hard Preference Sampling for Human Preference Alignment 

**Title (ZH)**: HPS: Hard Preference Sampling for Human Preference Alignment 

**Authors**: Xiandong Zou, Wanyu Lin, Yuchen Li, Pan Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2502.14400)  

**Abstract**: Aligning Large Language Model (LLM) responses with human preferences is vital for building safe and controllable AI systems. While preference optimization methods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown promise, they face challenges such as poor handling of harmful content, inefficient use of dispreferred responses, and, specifically for PL, high computational costs. To address these issues, we propose Hard Preference Sampling (HPS), a novel framework for robust and efficient human preference alignment. HPS introduces a training loss that prioritizes the most preferred response while rejecting all dispreferred and harmful ones. It emphasizes "hard" dispreferred responses--those closely resembling preferred ones--to enhance the model's rejection capabilities. By leveraging a single-sample Monte Carlo sampling strategy, HPS reduces computational overhead while maintaining alignment quality. Theoretically, HPS improves sample efficiency over existing PL methods and maximizes the reward margin between preferred and dispreferred responses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety datasets validate HPS's effectiveness, achieving comparable BLEU and reward scores while greatly improving reward margins and thus reducing harmful content generation. 

**Abstract (ZH)**: 大型语言模型（LLM）响应与人类偏好对齐对于构建安全可控的AI系统至关重要。尽管基于Plackett-Luce（PL）和Bradley-Terry（BT）模型的偏好优化方法显示出前景，但它们面临着有害内容处理不佳、不偏好响应利用效率低等问题，尤其是对于PL模型，计算成本较高。为解决这些问题，我们提出了一种名为Hard Preference Sampling（HPS）的新颖框架，以实现稳健高效的人类偏好对齐。HPS引入了一种训练损失，优先选择最偏好响应并拒绝所有不偏好和有害响应。它强调“硬”不偏好响应——那些与偏好响应高度相似的——以增强模型的拒绝能力。通过利用单样本蒙特卡洛采样策略，HPS降低了计算开销同时保持对齐质量。理论上，HPS在样本效率上优于现有PL方法，并最大化了偏好和不偏好响应之间的奖励差距，确保了更清晰的区分。对HH-RLHF和PKU-Safety数据集的实验验证了HPS的有效性，在实现类似BLEU和奖励分数的同时，大幅提高了奖励差距，从而减少了有害内容的生成。 

---
# Retrieval-Augmented Process Reward Model for Generalizable Mathematical Reasoning 

**Title (ZH)**: 检索增强过程奖励模型的通用数学推理 

**Authors**: Jiachen Zhu, Congmin Zheng, Jianghao Lin, Kounianhua Du, Ying Wen, Yong Yu, Jun Wang, Weinan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.14361)  

**Abstract**: While large language models (LLMs) have significantly advanced mathematical reasoning, Process Reward Models (PRMs) have been developed to evaluate the logical validity of reasoning steps. However, PRMs still struggle with out-of-distribution (OOD) challenges. This paper identifies key OOD issues, including step OOD, caused by differences in reasoning patterns across model types and sizes, and question OOD, which arises from dataset shifts between training data and real-world problems. To address these issues, we introduce Retrieval-Augmented Process Reward Model (RetrievalPRM), a novel framework designed to tackle these OOD issues. By utilizing a two-stage retrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar questions and steps as a warmup, enhancing PRM's ability to evaluate target steps and improving generalization and reasoning consistency across different models and problem types. Our extensive experiments demonstrate that RetrievalPRM outperforms existing baselines across multiple real-world datasets. Our open-source contributions include a retrieval-enhanced dataset, a tuning framework for PRM training, and the RetrievalPRM model, establishing a new standard for PRM performance. 

**Abstract (ZH)**: 尽管大规模语言模型（LLMs）在数学推理方面取得了显著进步，过程奖励模型（PRMs）已被开发出来评估推理步骤的逻辑有效性。然而，PRMs仍然难以应对离群分布（OOD）挑战。本文识别了关键的OOD问题，包括由于不同模型类型和规模的推理模式差异导致的步骤OOD问题，以及由于训练数据与真实世界问题之间的数据集转移导致的问题OOD问题。为了应对这些问题，我们提出了检索增强过程奖励模型（RetrievalPRM）这一新颖框架。通过利用两阶段的检索增强机制，RetrievalPRM 在预热阶段检索语义上相似的问题和步骤，从而增强PRM评估目标步骤的能力，并提高不同模型和问题类型下的泛化能力和推理一致性。我们的广泛实验表明，RetrievalPRM 在多个真实世界数据集上优于现有基线。我们的开源贡献包括检索增强数据集、PRM 训练的调整框架以及 RetrievalPRM 模型，确立了PRM性能的新标准。 

---
# Investigating the Impact of LLM Personality on Cognitive Bias Manifestation in Automated Decision-Making Tasks 

**Title (ZH)**: 探究语言模型人格对其在自动化决策任务中认知偏差表现的影响 

**Authors**: Jiangen He, Jiqun Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.14219)  

**Abstract**: Large Language Models (LLMs) are increasingly used in decision-making, yet their susceptibility to cognitive biases remains a pressing challenge. This study explores how personality traits influence these biases and evaluates the effectiveness of mitigation strategies across various model architectures. Our findings identify six prevalent cognitive biases, while the sunk cost and group attribution biases exhibit minimal impact. Personality traits play a crucial role in either amplifying or reducing biases, significantly affecting how LLMs respond to debiasing techniques. Notably, Conscientiousness and Agreeableness may generally enhance the efficacy of bias mitigation strategies, suggesting that LLMs exhibiting these traits are more receptive to corrective measures. These findings address the importance of personality-driven bias dynamics and highlight the need for targeted mitigation approaches to improve fairness and reliability in AI-assisted decision-making. 

**Abstract (ZH)**: 大型语言模型（LLMs）在决策中的应用日益增多，但它们的认知偏见易感性仍然是一个紧迫的挑战。本研究探讨个性特质如何影响这些偏见，并评估不同模型架构下缓解策略的有效性。研究发现六种常见的认知偏见，而沉没成本偏见和归因偏差的影响最小。个性特质在放大或减少偏见方面发挥着关键作用，显著影响LLMs对去偏见技术的响应。值得注意的是，责任心和宜人性可能会普遍增强偏见缓解策略的效果，表明具备这些特质的LLMs更可能对纠正措施产生积极反应。这些发现强调了基于个性的偏见动态的重要性，并突出了需要针对特定缓解方法以提高AI辅助决策的公平性和可靠性的必要性。 

---
# Giving AI Personalities Leads to More Human-Like Reasoning 

**Title (ZH)**: 赋予AI人格特质会导致更具人性化的推理 

**Authors**: Animesh Nighojkar, Bekhzodbek Moydinboyev, My Duong, John Licato  

**Link**: [PDF](https://arxiv.org/pdf/2502.14155)  

**Abstract**: In computational cognitive modeling, capturing the full spectrum of human judgment and decision-making processes, beyond just optimal behaviors, is a significant challenge. This study explores whether Large Language Models (LLMs) can emulate the breadth of human reasoning by predicting both intuitive, fast System 1 and deliberate, slow System 2 processes. We investigate the potential of AI to mimic diverse reasoning behaviors across a human population, addressing what we call the {\em full reasoning spectrum problem}. We designed reasoning tasks using a novel generalization of the Natural Language Inference (NLI) format to evaluate LLMs' ability to replicate human reasoning. The questions were crafted to elicit both System 1 and System 2 responses. Human responses were collected through crowd-sourcing and the entire distribution was modeled, rather than just the majority of the answers. We used personality-based prompting inspired by the Big Five personality model to elicit AI responses reflecting specific personality traits, capturing the diversity of human reasoning, and exploring how personality traits influence LLM outputs. Combined with genetic algorithms to optimize the weighting of these prompts, this method was tested alongside traditional machine learning models. The results show that LLMs can mimic human response distributions, with open-source models like Llama and Mistral outperforming proprietary GPT models. Personality-based prompting, especially when optimized with genetic algorithms, significantly enhanced LLMs' ability to predict human response distributions, suggesting that capturing suboptimal, naturalistic reasoning may require modeling techniques incorporating diverse reasoning styles and psychological profiles. The study concludes that personality-based prompting combined with genetic algorithms is promising for enhancing AI's \textit{human-ness} in reasoning. 

**Abstract (ZH)**: 在计算认知建模中，超越最优行为捕获人类判断和决策的全谱是一种重大挑战。本研究探讨大型语言模型是否能够通过预测直观快速的系统1和审慎缓慢的系统2过程来模拟人类推理的广泛行为。我们通过设计使用自然语言推理（NLI）新扩展格式的推理任务，评估大型语言模型复制人类推理的能力，旨在解决我们所称的“全推理谱问题”。我们使用基于人格的提示，受五大人格特质模型启发，以捕捉人类推理的多样性，并探索人格特质如何影响大型语言模型的输出。结合使用遗传算法优化这些提示的权重，这种方法与传统机器学习模型一同进行了测试。结果显示，开源模型如Llama和Mistral优于专有GPT模型，人格导向的提示，尤其是当与遗传算法优化结合时，显著增强了大型语言模型预测人类反应分布的能力，表明捕捉非最优的自然推理可能需要包含多样化推理风格和心理特质的建模技术。研究得出结论，结合使用人格导向的提示和遗传算法有助于提升人工智能在推理中的“人类特质”。 

---
# Investigating Non-Transitivity in LLM-as-a-Judge 

**Title (ZH)**: 基于LLM的法官角色中非传递性研究 

**Authors**: Yi Xu, Laura Ruis, Tim Rocktäschel, Robert Kirk  

**Link**: [PDF](https://arxiv.org/pdf/2502.14074)  

**Abstract**: Automatic evaluation methods based on large language models (LLMs) are emerging as the standard tool for assessing the instruction-following abilities of LLM-based agents. The most common method in this paradigm, pairwise comparisons with a baseline model, critically depends on the assumption of transitive preferences. However, the validity of this assumption remains largely unexplored. In this study, we investigate the presence of non-transitivity within the AlpacaEval framework and analyze its effects on model rankings. We find that LLM judges exhibit non-transitive preferences, leading to rankings that are sensitive to the choice of the baseline model. To mitigate this issue, we show that round-robin tournaments combined with Bradley-Terry models of preference can produce more reliable rankings. Notably, our method increases both the Spearman correlation and the Kendall correlation with Chatbot Arena (95.0% -> 96.4% and 82.1% -> 86.3% respectively). To address the computational cost of round-robin tournaments, we propose Swiss-Wise Iterative Matchmaking (Swim) tournaments, using a dynamic matching strategy to capture the benefits of round-robin tournaments while maintaining computational efficiency. 

**Abstract (ZH)**: 基于大规模语言模型的自动评估方法逐渐成为评估基于语言模型代理-following能力的标准工具。在这种范式中最常见的方法，即与基准模型进行成对比较，关键依赖于传递偏好假设。然而，这一假设的有效性仍很大程度上未经探索。在本研究中，我们考察了AlpacaEval框架内的非传递性现象，并分析其对模型排名的影响。我们发现，语言模型法官表现出非传递性偏好，导致排名对基准模型选择高度敏感。为了缓解这一问题，我们展示了轮比赛结合布雷德利-特里模型能够产生更可靠的排名。值得注意的是，我们的方法提高了与Chatbot Arena的相关性（单变量Spearman相关性从95.0%提高到96.4%，肯德尔相关性从82.1%提高到86.3%）。为解决轮比赛的计算成本问题，我们提出了Smartwise迭代匹配赛制（Swim），采用动态匹配策略以保持计算效率并利用轮比赛的优势。 

---
# LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention 

**Title (ZH)**: LServe: 效率提升的统一稀疏注意力长序列LLM服务 

**Authors**: Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, Song Han  

**Link**: [PDF](https://arxiv.org/pdf/2502.14866)  

**Abstract**: Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at this https URL. 

**Abstract (ZH)**: 大规模语言模型(LLMs)在处理长序列方面展现了显著潜力，但由于预填充阶段注意力机制的二次计算复杂度和解码阶段KV缓存的大量内存占用，高效服务于这些长上下文模型仍然具有挑战性。为了解决这些问题，我们引入了LServe系统，通过混合稀疏注意力加速长序列LLM服务。这种方法将预填充和解码注意力的不同硬件友好结构稀疏模式统一到一个框架中，其中对不太重要的标记进行块级跳过计算。LServe展示了静态和动态稀疏性在长上下文LLM注意力中的兼容性。此设计通过结合这些优化实现了加速。具体来说，我们将在预填充和解码阶段将一半的注意力头转换为几乎免费的流式头。此外，我们发现只需要一个常数数量的KV页面即可保留长上下文能力，与上下文长度无关。然后，我们设计了一种分层KV页面选择策略，根据查询中心相似性动态剪枝KV页面。平均而言，LServe在LLM预填充方面的加速为2.9倍，在解码方面的加速为1.3至2.1倍，同时保持长上下文准确性。代码发布于此https网址。 

---
# FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling 

**Title (ZH)**: FR-Spec: 通过频率排序推测性采样加速大规模语言模型 

**Authors**: Weilin Zhao, Tengyu Pan, Xu Han, Yudi Zhang, Ao Sun, Yuxiang Huang, Kaihuo Zhang, Weilun Zhao, Yuxuan Li, Jianyong Wang, Zhiyuan Liu, Maosong Sun  

**Link**: [PDF](https://arxiv.org/pdf/2502.14856)  

**Abstract**: Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12$\times$ speedup over the state-of-the-art speculative sampling method EAGLE-2. 

**Abstract (ZH)**: 基于频率排名的推测性采样框架FR-Spec通过词汇空间压缩优化候选词选择，以加速大规模语言模型的自回归生成过程。 

---
# Revealing and Mitigating Over-Attention in Knowledge Editing 

**Title (ZH)**: 揭示并缓解知识编辑中的过度关注问题 

**Authors**: Pinzheng Wang, Zecheng Tang, Keyan Zhou, Juntao Li, Qiaoming Zhu, Min Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.14838)  

**Abstract**: Large Language Models have demonstrated superior performance across a wide range of tasks, but they still exhibit undesirable errors due to incorrect knowledge learned from the training data. To avoid this, knowledge editing methods emerged to precisely edit the specific model knowledge via efficiently modifying a very small percentage of parameters. % However, those methods can lead to the problem of Specificity Failure: when the content related to the edited knowledge occurs in the context, it can inadvertently corrupt other pre-existing knowledge. However, those methods can lead to the problem of Specificity Failure, where the existing knowledge and capabilities are severely degraded due to editing. Our preliminary indicates that Specificity Failure primarily stems from the model's attention heads assigning excessive attention scores to entities related to the edited knowledge, thereby unduly focusing on specific snippets within the context, which we denote as the Attention Drift phenomenon. To mitigate such Attention Drift issue, we introduce a simple yet effective method Selective Attention Drift Restriction}(SADR), which introduces an additional regularization term during the knowledge editing process to restrict changes in the attention weight distribution, thereby preventing undue focus on the edited entity. Experiments on five frequently used strong LLMs demonstrate the effectiveness of our method, where SADR can significantly mitigate Specificity Failure in the predominant knowledge editing tasks. 

**Abstract (ZH)**: 大型语言模型在广泛的任务中展现了出色的表现，但由于训练数据中的错误知识，它们仍然会出现不良错误。为了避免这种情况，出现了知识编辑方法，通过高效地修改极少数参数来精确编辑特定的模型知识。然而，这些方法可能导致特定性失效的问题：当与编辑知识相关的内容出现在上下文中时，可能会无意中损害其他现有的知识。我们的初步研究表明，特定性失效主要源自模型的注意力头对与编辑知识相关的实体赋予过高的注意力分数，从而不当地专注于上下文中的特定片段，我们将其称为注意力漂移现象。为了缓解这种注意力漂移问题，我们提出了一种简单而有效的方法——选择性注意力漂移限制（SADR），该方法在知识编辑过程中引入了一个额外的正则化项，以限制注意力权重分布的变化，从而防止对编辑实体的不适当关注。在五个常用的强大语言模型上的实验表明，我们的方法能够显著缓解主要的知识编辑任务中的特定性失效。 

---
# Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs 

**Title (ZH)**: 面向经济高效的推理：使 DeepSeek 的多头潜在意图注意适用于任意基于Transformer的大语言模型 

**Authors**: Tao Ji, Bin Guo, Yuanbin Wu, Qipeng Guo, Lixing Shen, Zhan Chen, Xipeng Qiu, Qi Zhang, Tao Gui  

**Link**: [PDF](https://arxiv.org/pdf/2502.14837)  

**Abstract**: Multi-head Latent Attention (MLA) is an innovative architecture proposed by DeepSeek, designed to ensure efficient and economical inference by significantly compressing the Key-Value (KV) cache into a latent vector. Compared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its variants such as Grouped-Query Attention (GQA) exhibit significant cost disadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA without pre-training from scratch is both meaningful and challenging. This paper proposes the first data-efficient fine-tuning method for transitioning from MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE, we remove RoPE from dimensions of queries and keys that contribute less to the attention scores, for low-rank approximation, we introduce joint SVD approximations based on the pre-trained parameters of keys and values. These carefully designed strategies enable MHA2MLA to recover performance using only a small fraction (0.3% to 0.6%) of the data, significantly reducing inference costs while seamlessly integrating with compression techniques such as KV cache quantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%, with only a 0.5% drop in LongBench performance. 

**Abstract (ZH)**: 多头潜在注意力（MLA）是一种由DeepSeek提出的创新架构，旨在通过显著压缩键值（KV）缓存为潜在向量来确保高效的经济推理。与MLA相比，使用多头注意力（MHA）及其变体如分组查询注意力（GQA）的标准大型语言模型（LLM）显示出明显的成本劣势。使已训练良好的LLM（如Llama）能够快速适应MLA而无需从头开始预训练是一项既有意义又具有挑战性的工作。本文提出了首个数据高效微调方法MHA2MLA，该方法包括两个关键组成部分：对于部分RoPE，删除对注意力得分贡献较少的查询和键的RoPE；对于低秩近似，基于键和值的预训练参数引入联合SVD近似。这些精心设计的策略使MHA2MLA仅使用极小比例的数据（0.3%到0.6%）就能恢复性能，显著降低推理成本，同时无缝集成压缩技术，如键值缓存量化。例如，Llama2-7B的键值缓存大小减少了92.19%，长本廷基准性能下降了0.5%。 

---
# LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models 

**Title (ZH)**: LongWriter-V： enables ultra-long and high-fidelity generation in vision-language models 

**Authors**: Shangqing Tu, Yucheng Wang, Daniel Zhang-Li, Yushi Bai, Jifan Yu, Yuhao Wu, Lei Hou, Huiqin Liu, Zhiyuan Liu, Bin Xu, Juanzi Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.14834)  

**Abstract**: Existing Large Vision-Language Models (LVLMs) can process inputs with context lengths up to 128k visual and text tokens, yet they struggle to generate coherent outputs beyond 1,000 words. We find that the primary limitation is the absence of long output examples during supervised fine-tuning (SFT). To tackle this issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158 examples, each with multiple input images, an instruction, and corresponding outputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that maintain high-fidelity to the input images, we employ Direct Preference Optimization (DPO) to the SFT model. Given the high cost of collecting human feedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which breaks long outputs into segments and uses iterative corrections to form preference pairs with the original outputs. Additionally, we develop MMLongBench-Write, a benchmark featuring six tasks to evaluate the long-generation capabilities of VLMs. Our 7B parameter model, trained with LongWriter-V-22k and IterDPO, achieves impressive performance on this benchmark, outperforming larger proprietary models like GPT-4o. Code and data: this https URL 

**Abstract (ZH)**: 现有的大型视觉-语言模型（LVLMs）能处理长达128k视觉和文本token的输入，但在生成超过1,000字的连贯输出时存在困难。我们发现主要的限制在于监督微调（SFT）过程中缺乏长输出示例。为了解决这一问题，我们引入了LongWriter-V-22k，这是一个包含22,158个示例的SFT数据集，每个示例包含多张输入图像、一条指令以及从0到10,000字不等的对应输出。此外，为了生成与输入图像保真度高的长输出，我们在SFT模型中采用了直接偏好优化（DPO）。鉴于收集长输出（例如3,000字）的人类反馈成本很高，我们提出了IterDPO，该方法将长输出分解成段落，并通过迭代修正形成与原始输出的偏好对。此外，我们开发了MMLongBench-Write基准，包含六个任务以评估VLMs的长生成能力。使用LongWriter-V-22k和IterDPO训练的7B参数模型在该基准上表现优异，超过了更大的专用模型如GPT-4o。代码和数据：[链接]。 

---
# Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs 

**Title (ZH)**: 中层表示对齐在细调的LLM中的跨语言迁移学习中应用 

**Authors**: Danni Liu, Jan Niehues  

**Link**: [PDF](https://arxiv.org/pdf/2502.14830)  

**Abstract**: While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. Our code is publicly available (this https URL). 

**Abstract (ZH)**: 尽管通过微调，大型语言模型在特定任务应用中展现了显著的能力，但在多种语言之间的扩展这些益处对于广泛 accessibility 至关重要。然而，有效的跨语言迁移受到语言间 LLM 性能差距以及许多语言的微调数据稀缺性的阻碍。通过对超过1,000多种语言对的 LLM 内部表示进行分析，我们发现中间层具有最强的跨语言对齐潜力。基于这一发现，我们提出了一种集成在特定任务训练中的中间层对齐目标。我们在槽填充、机器翻译和结构化文本生成任务上的实验显示，在跨语言迁移中具有一致的改进，尤其是在低资源语言上更为显著。该方法对对齐语言的选择具有鲁棒性，并能泛化到训练期间未见过的语言。此外，我们展示了单独训练的对齐模块可以与现有的特定任务模块合并，从而在不需要完整重新训练的情况下提升跨语言能力。我们的代码已公开（this https URL）。 

---
# eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables 

**Title (ZH)**: eC-Tab2Text：基于电商产品表格的方面导向文本生成 

**Authors**: Luis Antonio Gutiérrez Guanilo, Mir Tafseer Nayeem, Cristian López, Davood Rafiei  

**Link**: [PDF](https://arxiv.org/pdf/2502.14820)  

**Abstract**: Large Language Models (LLMs) have demonstrated exceptional versatility across diverse domains, yet their application in e-commerce remains underexplored due to a lack of domain-specific datasets. To address this gap, we introduce eC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce, including detailed product attributes and user-specific queries. Leveraging eC-Tab2Text, we focus on text generation from product tables, enabling LLMs to produce high-quality, attribute-specific product reviews from structured tabular data. Fine-tuned models were rigorously evaluated using standard Table2Text metrics, alongside correctness, faithfulness, and fluency assessments. Our results demonstrate substantial improvements in generating contextually accurate reviews, highlighting the transformative potential of tailored datasets and fine-tuning methodologies in optimizing e-commerce workflows. This work highlights the potential of LLMs in e-commerce workflows and the essential role of domain-specific datasets in tailoring them to industry-specific challenges. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在电子商务领域的应用由于缺乏专用数据集而未得到充分探索，为此我们引入了eC-Tab2Text数据集，旨在捕捉电子商务领域的复杂性，包括详细的产品属性和用户特定的查询。利用eC-Tab2Text数据集，我们专注于从产品表格生成文本，使得语言模型能够从结构化的表格数据中生成高质量、属性特定的产品评价。经过精细调优的模型使用标准的Table2Text指标进行评估，并结合准确性和忠实性评估。我们的结果表明，在生成上下文相关性评价方面取得了显著改进，突显了定制数据集和调优方法在优化电子商务工作流程中的潜在变革性潜力。该工作突显了语言模型在电子商务工作流程中的潜力以及专用数据集在适应行业特定挑战中的关键作用。 

---
# From RAG to Memory: Non-Parametric Continual Learning for Large Language Models 

**Title (ZH)**: 从RAG到记忆：大型语言模型的非参数连续学习 

**Authors**: Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, Yu Su  

**Link**: [PDF](https://arxiv.org/pdf/2502.14802)  

**Abstract**: Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Our code and data will be released at this https URL. 

**Abstract (ZH)**: 我们持续获取、组织和利用知识的能力是人类智能的关键特征，AI系统必须近似这种能力以充分发挥其潜力。鉴于大规模语言模型（LLMs）连续学习的挑战，检索增强生成（RAG）已成为引入新信息的主要方式。然而，其对向量检索的依赖性限制了其模仿人类长期记忆的动态和相互关联性质的能力。最近的RAG方法通过使用知识图等结构来增强向量嵌入，以解决部分差距，如意义构建和关联性。然而，它们在更基本的事实记忆任务上的表现显著低于标准RAG。我们解决了这一意外的退化，并提出了HippoRAG 2框架，该框架在事实记忆、意义构建和关联记忆任务上全面优于标准RAG。HippoRAG 2以HippoRAG中使用的个性化PageRank算法为基础，并通过更深层次的段落整合和更有效的LLM在线使用来增强它。这种组合使RAG系统更接近人类长期记忆的效果，在关联记忆任务上比最先进的嵌入模型提高了7%的表现，同时在事实知识和意义构建记忆能力方面也表现出更优越的表现。本工作为LLMs提供了一种非参数化的连续学习途径。我们的代码和数据将发布在该网址。 

---
# Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning 

**Title (ZH)**: 逻辑-RL：基于规则的强化学习释放LLM推理能力 

**Authors**: Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, Chong Luo  

**Link**: [PDF](https://arxiv.org/pdf/2502.14768)  

**Abstract**: Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skills-such as reflection, verification, and summarization-that are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC. 

**Abstract (ZH)**: 受DeepSeek-R1成功的影响，我们探索了基于规则的强化学习（RL）在大规模推理模型中的潜力。为了分析推理动态，我们使用合成逻辑谜题作为训练数据，因为这些数据具有可控的复杂度和直接的答案验证方式。我们做出了一些关键技术贡献，这些贡献使得有效的和稳定的RL训练成为可能：强调思考和回答过程的系统提示、严格格式的奖励函数，以及实现稳定收敛的简单训练配方。我们的7B模型发展了逻辑语料库中缺失的高级推理技能——如反思、验证和总结。令人惊讶的是，经过仅仅5000个逻辑问题的训练，它展示了对具有挑战性的数学基准AIME和AMC的泛化能力。 

---
# On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems 

**Title (ZH)**: 基于检索增强生成系统的上下文大小与模型选择的影响探究 

**Authors**: Juraj Vladika, Florian Matthes  

**Link**: [PDF](https://arxiv.org/pdf/2502.14759)  

**Abstract**: Retrieval-augmented generation (RAG) has emerged as an approach to augment large language models (LLMs) by reducing their reliance on static knowledge and improving answer factuality. RAG retrieves relevant context snippets and generates an answer based on them. Despite its increasing industrial adoption, systematic exploration of RAG components is lacking, particularly regarding the ideal size of provided context, and the choice of base LLM and retrieval method. To help guide development of robust RAG systems, we evaluate various context sizes, BM25 and semantic search as retrievers, and eight base LLMs. Moving away from the usual RAG evaluation with short answers, we explore the more challenging long-form question answering in two domains, where a good answer has to utilize the entire context. Our findings indicate that final QA performance improves steadily with up to 15 snippets but stagnates or declines beyond that. Finally, we show that different general-purpose LLMs excel in the biomedical domain than the encyclopedic one, and that open-domain evidence retrieval in large corpora is challenging. 

**Abstract (ZH)**: 检索增强生成（RAG）已成为一种通过减少对静态知识的依赖并提高答案事实性的方法来扩充大型语言模型（LLMs）的方法。RAG检索相关上下文片段并基于它们生成答案。尽管其在工业界的应用越来越广泛，但对RAG组件的系统性探索仍然不足，特别是在提供的上下文的理想大小和基底LLM及检索方法的选择方面。为了指导稳健的RAG系统的开发，我们评估了多种上下文大小、BM25和语义搜索作为检索器，以及八种基底LLM。我们从通常的短答案RAG评估转向探索更具挑战性的长格式问答问题，在两个领域进行研究，其中好的回答需要利用整个上下文。我们的研究发现表明，最终的问答性能随着片段数量的增加而稳步提升，但在一定数量后趋于稳定或下降。最后，我们展示了不同的通用型LLM在生物医学领域和百科领域中表现不同，而且在大规模语料库中进行开放式领域证据检索颇具挑战性。 

---
# EAGER-LLM: Enhancing Large Language Models as Recommenders through Exogenous Behavior-Semantic Integration 

**Title (ZH)**: EAGER-LLM: 通过外生行为语义集成增强大型语言模型的推荐能力 

**Authors**: Minjie Hong, Yan Xia, Zehan Wang, Jieming Zhu, Ye Wang, Sihang Cai, Xiaoda Yang, Quanyu Dai, Zhenhua Dong, Zhimeng Zhang, Zhou Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2502.14735)  

**Abstract**: Large language models (LLMs) are increasingly leveraged as foundational backbones in the development of advanced recommender systems, offering enhanced capabilities through their extensive knowledge and reasoning. Existing llm-based recommender systems (RSs) often face challenges due to the significant differences between the linguistic semantics of pre-trained LLMs and the collaborative semantics essential for RSs. These systems use pre-trained linguistic semantics but learn collaborative semantics from scratch via the llm-Backbone. However, LLMs are not designed for recommendations, leading to inefficient collaborative learning, weak result correlations, and poor integration of traditional RS features. To address these challenges, we propose EAGER-LLM, a decoder-only llm-based generative recommendation framework that integrates endogenous and exogenous behavioral and semantic information in a non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich item indices that integrates indexing sequences for exogenous signals, enabling efficient link-wide processing; 2)non-invasive multiscale alignment reconstruction tasks guide the model toward a deeper understanding of both collaborative and semantic signals; 3)an annealing adapter designed to finely balance the model's recommendation performance with its comprehension capabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing on three public benchmarks. 

**Abstract (ZH)**: 大型语言模型（LLMs）日益成为高级推荐系统开发中的基础架构核心，通过其广泛的知识和推理能力提供增强功能。现有的基于LLM的推荐系统（RSs）往往因预训练LLM的语义与RS所需的协作语义之间存在的显著差异而面临挑战。这些系统使用预训练的语言语义，但通过LLM骨干从零学习协作语义，然而，LLMs并非为推荐设计，导致协作学习效率低下、结果相关性薄弱以及传统RS特征整合不佳。为解决这些问题，我们提出EAGER-LLM，这是一种仅解码器的基于LLM的生成推荐框架，以非侵入方式整合内在和外在的行为和语义信息。具体而言，我们提出1)双源知识丰富项指标，结合外生信号的索引序列，实现高效的全局链接处理；2)非侵入式的多尺度对齐重构任务引导模型更深地理解协作和语义信号；3)一种退火适配器，旨在精细平衡模型的推荐性能与理解能力。我们通过在三个公开基准上的严格测试展示了EAGER-LLM的有效性。 

---
# WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models 

**Title (ZH)**: WavRAG：集成音频的检索增强生成用于口语对话模型 

**Authors**: Yifu Chen, Shengpeng Ji, Haoxiao Wang, Ziqing Wang, Siyu Chen, Jinzheng He, Jin Xu, Zhou Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2502.14727)  

**Abstract**: Retrieval Augmented Generation (RAG) has gained widespread adoption owing to its capacity to empower large language models (LLMs) to integrate external knowledge. However, existing RAG frameworks are primarily designed for text-based LLMs and rely on Automatic Speech Recognition to process speech input, which discards crucial audio information, risks transcription errors, and increases computational overhead. Therefore, we introduce WavRAG, the first retrieval augmented generation framework with native, end-to-end audio support. WavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw audio for both embedding and retrieval. 2) WavRAG integrates audio and text into a unified knowledge representation. Specifically, we propose the WavRetriever to facilitate the retrieval from a text-audio hybrid knowledge base, and further enhance the in-context capabilities of spoken dialogue models through the integration of chain-of-thought reasoning. In comparison to state-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval performance while delivering a 10x acceleration. Furthermore, WavRAG's unique text-audio hybrid retrieval capability extends the boundaries of RAG to the audio modality. 

**Abstract (ZH)**: 检索增强生成（RAG）由于能够增强大型语言模型（LLMs）整合外部知识的能力而得到了广泛应用。然而，现有的RAG框架主要针对文本型LLMs，并依赖自动语音识别（ASR）处理语音输入，这会丢弃关键的音频信息、增加转录错误风险和提升计算开销。因此，我们介绍了WavRAG，这是首个具备原生端到端音频支持的检索增强生成框架。WavRAG具有两大关键特性：1）绕过ASR，WavRAG直接处理原始音频以进行嵌入和检索；2）WavRAG将音频和文本整合到统一的知识表示中。具体而言，我们提出了WavRetriever以促进从文本-音频混合知识库中的检索，并通过引入链式推理进一步增强口语对话模型的上下文能力。与最先进的ASR-文本RAG管线相比，WavRAG在检索性能上表现出色，同时实现了10倍的加速效果。此外，WavRAG独特的文本-音频混合检索能力将RAG的应用边界扩展至音频模态。 

---
# Data-Constrained Synthesis of Training Data for De-Identification 

**Title (ZH)**: 基于数据约束的去标识化训练数据合成 

**Authors**: Thomas Vakili, Aron Henriksson, Hercules Dalianis  

**Link**: [PDF](https://arxiv.org/pdf/2502.14677)  

**Abstract**: Many sensitive domains -- such as the clinical domain -- lack widely available datasets due to privacy risks. The increasing generative capabilities of large language models (LLMs) have made synthetic datasets a viable path forward. In this study, we domain-adapt LLMs to the clinical domain and generate synthetic clinical texts that are machine-annotated with tags for personally identifiable information using capable encoder-based NER models. The synthetic corpora are then used to train synthetic NER models. The results show that training NER models using synthetic corpora incurs only a small drop in predictive performance. The limits of this process are investigated in a systematic ablation study -- using both Swedish and Spanish data. Our analysis shows that smaller datasets can be sufficient for domain-adapting LLMs for data synthesis. Instead, the effectiveness of this process is almost entirely contingent on the performance of the machine-annotating NER models trained using the original data. 

**Abstract (ZH)**: 许多敏感领域，如临床领域，由于隐私风险缺乏广泛可用的数据集。大型语言模型不断增强的生成能力使其能够生成合成数据集作为可行的选择。在本研究中，我们将大型语言模型适应临床领域，并使用高性能的编码器基NER模型为个人可识别信息生成机器标注的合成临床文本。然后使用这些合成语料库训练合成NER模型。研究结果显示，使用合成语料库训练NER模型仅会导致轻微的预测性能下降。这一过程的限制通过系统性的消融研究进行了探索，使用了瑞典语和西班牙语数据。我们的分析表明，较小的数据集对于适应数据合成的大型语言模型来说可能是足够的，而这一过程的有效性几乎完全取决于使用原始数据训练的机器标注NER模型的性能。 

---
# Explanations of Deep Language Models Explain Language Representations in the Brain 

**Title (ZH)**: 深度语言模型的解释揭示脑中语言表示 

**Authors**: Maryam Rahimi, Yadollah Yaghoobzadeh, Mohammad Reza Daliri  

**Link**: [PDF](https://arxiv.org/pdf/2502.14671)  

**Abstract**: Recent advances in artificial intelligence have given rise to large language models (LLMs) that not only achieve human-like performance but also share computational principles with the brain's language processing mechanisms. While previous research has primarily focused on aligning LLMs' internal representations with neural activity, we introduce a novel approach that leverages explainable AI (XAI) methods to forge deeper connections between the two domains. Using attribution methods, we quantified how preceding words contribute to an LLM's next-word predictions and employed these explanations to predict fMRI recordings from participants listening to the same narratives. Our findings demonstrate that attribution methods robustly predict brain activity across the language network, surpassing traditional internal representations in early language areas. This alignment is hierarchical: early-layer explanations correspond to the initial stages of language processing in the brain, while later layers align with more advanced stages. Moreover, the layers more influential on LLM next-word prediction$\unicode{x2014}$those with higher attribution scores$\unicode{x2014}$exhibited stronger alignment with neural activity. This work establishes a bidirectional bridge between AI and neuroscience. First, we demonstrate that attribution methods offer a powerful lens for investigating the neural mechanisms of language comprehension, revealing how meaning emerges from preceding context. Second, we propose using brain alignment as a metric to evaluate the validity of attribution methods, providing a framework for assessing their biological plausibility. 

**Abstract (ZH)**: 最近人工智能的进步产生了大型语言模型（LLMs），不仅实现了类人的性能，而且还与大脑的语言处理机制共享计算原理。尽管以前的研究主要集中在使LLMs的内部表示与神经活动对齐上，我们介绍了一种新的方法，利用可解释人工智能（XAI）方法来加深这两个领域之间的联系。通过归因方法，我们量化了前一个词对LLM下一词预测的贡献，并使用这些解释预测了参与者听取相同叙述时的fMRI记录。我们的发现表明，归因方法能够稳健地预测语言网络中的脑活动，超过了早期语言区域的传统内部表示。这种对齐是分层的：早期层的解释对应于大脑中语言处理的初期阶段，而后续层则与更高级阶段对齐。此外，对LLM下一词预测影响更大的层——即归因分数更高的层——与神经活动的对齐更为强烈。这项工作建立了人工智能与神经科学之间的双向桥梁。首先，我们证明了归因方法为研究语言理解的神经机制提供了一个强大的视角，揭示了意义是如何从先前的背景中产生的。其次，我们提出了使用大脑对齐作为评估归因方法有效性的指标，提供了一个评估其生物学合理性的框架。 

---
# Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs 

**Title (ZH)**: 一次编辑，全面更新：LLM中跨语言知识同步的简单框架 

**Authors**: Yuchen Wu, Liang Ding, Li Shen, Dacheng Tao  

**Link**: [PDF](https://arxiv.org/pdf/2502.14645)  

**Abstract**: Knowledge editing allows for efficient adaptation of large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on either single-language editing or basic multilingual editing, failing to achieve true cross-linguistic knowledge synchronization. To address this, we present a simple and practical state-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE), designed to propagate knowledge from a dominant language to other languages effectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition Instruction Tuning (XE-IT), which fine-tunes the model on a curated parallel dataset to modify in-scope knowledge while preserving unrelated information, and (ii) Target-language Preference Optimization (TL-PO), which applies advanced optimization techniques to ensure consistency across languages, fostering the transfer of updates. Additionally, we contribute a high-quality, cross-lingual dataset, specifically designed to enhance knowledge transfer across languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks show that X-KDE significantly enhances cross-lingual performance, achieving an average improvement of +8.19%, while maintaining high accuracy in monolingual settings. 

**Abstract (ZH)**: 跨语言知识民主编辑：高效传播多语言知识的新方法 

---
# Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline Comparison for Semantic Similarity 

**Title (ZH)**: 探索RWKV在句子嵌入中的应用：逐层分析与语义相似性基线比较 

**Authors**: Xinghan Pan  

**Link**: [PDF](https://arxiv.org/pdf/2502.14620)  

**Abstract**: This paper investigates the efficacy of RWKV, a novel language model architecture known for its linear attention mechanism, for generating sentence embeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate the semantic similarity captured by embeddings from different hidden layers of a pre-trained RWKV model. The performance is assessed on the Microsoft Research Paraphrase Corpus (MRPC) dataset using Spearman correlation and compared against a GloVe-based baseline. My results indicate that while RWKV embeddings capture some semantic relatedness, they underperform compared to the GloVe baseline in terms of Spearman correlation. I also analyze the inference time and GPU memory usage, highlighting the computational trade-offs associated with RWKV embeddings. The findings suggest that while RWKV offers potential advantages in terms of linear scaling, its zero-shot sentence embedding quality for semantic similarity tasks requires further investigation and potential task-specific fine-tuning to match or exceed simpler baselines. 

**Abstract (ZH)**: 本文调查了RWKV新型语言模型架构在零样本设置下生成句子嵌入的有效性，该架构因其线性注意力机制而闻名。本文逐层分析了预训练RWKV模型的不同隐藏层嵌入捕获的语义相似性，并使用Spearman相关系数在Microsoft Research 偏句对语料库（MRPC）数据集上评估其性能，并将其与基于GloVe的基线进行比较。结果显示，虽然RWKV嵌入捕获了一些语义相关性，但在Spearman相关系数方面仍不如GloVe基线。此外，还分析了推理时间和GPU内存使用情况，突出了与RWKV嵌入相关的计算权衡。研究结果表明，虽然RWKV在线性扩展方面具有潜在优势，但其在语义相似性任务中的零样本句嵌表示质量仍需进一步调查，并可能需要特定任务的微调以匹配或超越更简单的基线模型。 

---
# Reward Models Identify Consistency, Not Causality 

**Title (ZH)**: 奖励模型识别一致性和因果性 

**Authors**: Yuhui Xu, Hanze Dong, Lei Wang, Caiming Xiong, Junnan Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.14619)  

**Abstract**: Reward models (RMs) play a crucial role in aligning large language models (LLMs) with human preferences and enhancing reasoning quality. Traditionally, RMs are trained to rank candidate outputs based on their correctness and coherence. However, in this work, we present several surprising findings that challenge common assumptions about RM behavior. Our analysis reveals that state-of-the-art reward models prioritize structural consistency over causal correctness. Specifically, removing the problem statement has minimal impact on reward scores, whereas altering numerical values or disrupting the reasoning flow significantly affects RM outputs. Furthermore, RMs exhibit a strong dependence on complete reasoning trajectories truncated or incomplete steps lead to significant variations in reward assignments, indicating that RMs primarily rely on learned reasoning patterns rather than explicit problem comprehension. These findings hold across multiple architectures, datasets, and tasks, leading to three key insights: (1) RMs primarily assess coherence rather than true reasoning quality; (2) The role of explicit problem comprehension in reward assignment is overstated; (3) Current RMs may be more effective at ranking responses than verifying logical validity. Our results suggest a fundamental limitation in existing reward modeling approaches, emphasizing the need for a shift toward causality-aware reward models that go beyond consistency-driven evaluation. 

**Abstract (ZH)**: 奖励模型在引导大型语言模型与人类偏好一致并提高推理质量方面发挥着关键作用。传统上，奖励模型被训练以根据正确性和连贯性对候选输出进行排序。然而，在本工作中，我们提出了几个令人惊讶的发现，挑战了关于奖励模型行为的常见假设。我们的分析揭示了，最先进的奖励模型优先考虑结构一致性而非因果正确性。具体而言，去除问题陈述对奖励分数几乎没有影响，而修改数值或破坏推理流程则显著影响奖励模型的输出。此外，奖励模型强烈依赖于完整的推理轨迹；截断或中断的部分步骤会导致奖励分配的重大变化，表明奖励模型主要依赖于学习到的推理模式，而不是明确的问题理解。这些发现适用于多种架构、数据集和任务，得出三个关键见解：（1）奖励模型主要评估连贯性而非真正的推理质量；（2）明确的问题理解在奖励分配中的作用被高估了；（3）当前的奖励模型可能在对响应进行排名上比验证逻辑有效性更有效。我们的结果指出了现有奖励建模方法的基本局限性，强调了转向因果性感知奖励模型的重要性，这种奖励模型超出了基于一致性的评估。 

---
# Less is More: Improving LLM Alignment via Preference Data Selection 

**Title (ZH)**: 少即是多：通过偏好数据选择提高LLM对齐程度 

**Authors**: Xun Deng, Han Zhong, Rui Ai, Fuli Feng, Zheng Wang, Xiangnan He  

**Link**: [PDF](https://arxiv.org/pdf/2502.14560)  

**Abstract**: Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To accurately estimate margins for data selection, we propose a dual-margin guided approach that considers both external reward margins and implicit DPO reward margins. Extensive experiments demonstrate that our method reduces computational cost dramatically while improving performance. Remarkably, by using just 10\% of the Ultrafeedback dataset, our approach achieves 3\% to 8\% improvements across various Llama and Mistral series models on the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\% improvement with 25\% online data, while further reducing training time. These results highlight the potential of data selection strategies for advancing preference optimization. 

**Abstract (ZH)**: 直接偏好优化(DPO)作为一种使大型语言模型与人类偏好对齐的有前景方法已逐渐兴起。虽然先前的工作主要从目标函数的角度扩展了DPO，我们则从被广泛忽视但至关重要的数据选择角度改进了DPO。具体而言，我们提出了一种新颖的边际最大化原则来解决由嘈杂数据引起的参数收缩问题，并应用于DPO训练的数据集筛选。为了准确估计用于数据选择的边际，我们提出了一种双边际引导方法，该方法同时考虑外部奖励边际和隐式DPO奖励边际。大量实验表明，我们的方法大幅降低了计算成本，同时提升了性能。此外，通过使用Ultrafeedback数据集的10%，我们的方法在AlpacaEval 2.0基准上各系列Llama和Mistral模型上实现了3%到8%的性能提升。进一步地，我们的方法无缝扩展到迭代DPO，仅使用25%的在线数据即可获得约3%的性能提升，同时进一步缩短了训练时间。这些结果突显了数据选择策略在推动偏好优化方面的潜力。 

---
# Multiscale Byte Language Models -- A Hierarchical Architecture for Causal Million-Length Sequence Modeling 

**Title (ZH)**: 多尺度字级语言模型——一种用于因果千万长度序列建模的分层架构 

**Authors**: Eric Egli, Matteo Manica, Jannis Born  

**Link**: [PDF](https://arxiv.org/pdf/2502.14553)  

**Abstract**: Bytes form the basis of the digital world and thus are a promising building block for multimodal foundation models. Recently, Byte Language Models (BLMs) have emerged to overcome tokenization, yet the excessive length of bytestreams requires new architectural paradigms. Therefore, we present the Multiscale Byte Language Model (MBLM), a model-agnostic hierarchical decoder stack that allows training with context windows of $5$M bytes on single GPU in full model precision. We thoroughly examine MBLM's performance with Transformer and Mamba blocks on both unimodal and multimodal tasks. Our experiments demonstrate that hybrid architectures are efficient in handling extremely long byte sequences during training while achieving near-linear generational efficiency. To the best of our knowledge, we present the first evaluation of BLMs on visual Q\&A tasks and find that, despite serializing images and the absence of an encoder, a MBLM with pure next token prediction can match custom CNN-LSTM architectures with designated classification heads. We show that MBLMs exhibit strong adaptability in integrating diverse data representations, including pixel and image filestream bytes, underlining their potential toward omnimodal foundation models. Source code is publicly available at: this https URL 

**Abstract (ZH)**: 字节构成了数字世界的基石，因此是多模态基础模型的有前途的构建块。最近，字节语言模型（BLMs）涌现出来以克服分词问题，然而字节流的过长长度需要新的架构范式。因此，我们提出了多尺度字节语言模型（MBLM），一种模型无关的分层解码堆栈，允许在单个GPU上以全模型精度训练 CONTEXT WINDOWS 大小为5M字节。我们在单模态和多模态任务中使用Transformer和Mamba块全面检查了MBLM的性能。我们的实验表明，混合架构在处理训练中的极长长字节序列时效率高，同时实现接近线性的生成效率。据我们所知，我们首次在视觉问答任务上评估了BLMs，并发现尽管序列化了图像且没有编码器，一个纯下一个词预测的MBLM可以匹配带有专用分类头的自定义CNN-LSTM架构。我们展示了MBLMs在整合各种数据表示方面的强大适应性，包括像素和图像文件流字节，这突显了它们向全方位模态基础模型的潜力。源代码可在以下网址获取：this https URL 

---
# CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models 

**Title (ZH)**: 基于大型语言模型的CORBA：多代理系统中的传染性递归阻塞攻击 

**Authors**: Zhenhong Zhou, Zherui Li, Jie Zhang, Yuanhe Zhang, Kun Wang, Yang Liu, Qing Guo  

**Link**: [PDF](https://arxiv.org/pdf/2502.14529)  

**Abstract**: Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting harmful instructions through alignment, their security remains largely unexplored. This gap leaves LLM-MASs vulnerable to targeted disruptions. In this paper, we introduce Contagious Recursive Blocking Attacks (Corba), a novel and simple yet highly effective attack that disrupts interactions between agents within an LLM-MAS. Corba leverages two key properties: its contagious nature allows it to propagate across arbitrary network topologies, while its recursive property enables sustained depletion of computational resources. Notably, these blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods. We evaluate Corba on two widely-used LLM-MASs, namely, AutoGen and Camel across various topologies and commercial models. Additionally, we conduct more extensive experiments in open-ended interactive LLM-MASs, demonstrating the effectiveness of Corba in complex topology structures and open-source models. Our code is available at: this https URL. 

**Abstract (ZH)**: 基于大规模语言模型的多智能体系统（LLM-MASs）展示了显著的实际能力，有效协作以完成复杂任务。尽管这些系统设计了诸如通过对齐拒绝有害指令的安全机制，其安全性仍 largely未被探索。这一差距使LLM-MASs暴露于有针对性的干扰之下。在本文中，我们介绍了传染性递归阻塞攻击（Corba），这是一种新颖且简单的高效攻击方法，能够破坏LLM-MAS中智能体之间的交互。Corba 利用了两个关键属性：其传染性质使其能够在任意网络拓扑中传播，而其递归性质则使其能够持续耗尽计算资源。值得注意的是，这些阻塞攻击往往涉及看似无害的指令，这使得使用传统的对齐方法进行缓解尤为具有挑战性。我们在 AutoGen 和 Camel 这两种广泛使用的 LLM-MAS 上评估了 Corba，涉及多种拓扑结构和商用模型。此外，我们在开放式的交互式 LLM-MAS 中进行了更为广泛的实验，证明了 Corba 在复杂拓扑结构和开源模型中的有效性。代码可在此处访问：this https URL。 

---
# MLGym: A New Framework and Benchmark for Advancing AI Research Agents 

**Title (ZH)**: MLGym：一个新的框架和基准，用于推动AI研究代理发展 

**Authors**: Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach, William Yang Wang, Roberta Raileanu  

**Link**: [PDF](https://arxiv.org/pdf/2502.14499)  

**Abstract**: We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents. 

**Abstract (ZH)**: Meta MLGym和MLGym-Bench：一个新的框架和基准，用于评估和开发在AI研究任务上工作的LLM代理 

---
# Enhancing Smart Environments with Context-Aware Chatbots using Large Language Models 

**Title (ZH)**: 利用大规模语言模型增强基于情境意识的聊天机器人在智能环境中的应用 

**Authors**: Aurora Polo-Rodríguez, Laura Fiorini, Erika Rovini, Filippo Cavallo, Javier Medina-Quero  

**Link**: [PDF](https://arxiv.org/pdf/2502.14469)  

**Abstract**: This work presents a novel architecture for context-aware interactions within smart environments, leveraging Large Language Models (LLMs) to enhance user experiences. Our system integrates user location data obtained through UWB tags and sensor-equipped smart homes with real-time human activity recognition (HAR) to provide a comprehensive understanding of user context. This contextual information is then fed to an LLM-powered chatbot, enabling it to generate personalised interactions and recommendations based on the user's current activity and environment. This approach moves beyond traditional static chatbot interactions by dynamically adapting to the user's real-time situation. A case study conducted from a real-world dataset demonstrates the feasibility and effectiveness of our proposed architecture, showcasing its potential to create more intuitive and helpful interactions within smart homes. The results highlight the significant benefits of integrating LLM with real-time activity and location data to deliver personalised and contextually relevant user experiences. 

**Abstract (ZH)**: 本研究提出了一种新的架构，用于智能环境中的上下文感知交互，借助大型语言模型（LLMs）增强用户体验。该系统将通过UWB标签和传感器装备的智能家庭获取的用户位置数据与实时人体活动识别（HAR）相结合，提供对用户上下文的全面理解。随后，这些上下文信息被输入到LLM驱动的聊天机器人中，使其能够根据用户的当前活动和环境生成个性化的交互和建议。该方法超越了传统的静态聊天机器人交互方式，能够动态适应用户的实时情况。从实际数据集进行的案例研究证明了所提出架构的可行性和有效性，展示了其在智能家庭中创建更加直观和有帮助的交互的潜力。结果强调了将LLM与实时活动和位置数据结合使用以实现个性化和上下文相关用户体验的重大优势。 

---
# Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing 

**Title (ZH)**: Llamba：扩展蒸馏循环模型以实现高效语言处理 

**Authors**: Aviv Bick, Tobias Katsch, Nimit Sohoni, Arjun Desai, Albert Gu  

**Link**: [PDF](https://arxiv.org/pdf/2502.14458)  

**Abstract**: We introduce Llamba, a family of efficient recurrent language models distilled from Llama-3.x into the Mamba architecture. The series includes Llamba-1B, Llamba-3B, and Llamba-8B, which achieve higher inference throughput and handle significantly larger batch sizes than Transformer-based models while maintaining comparable benchmark performance. Furthermore, Llamba demonstrates the effectiveness of cross-architecture distillation using MOHAWK (Bick et al., 2024), achieving these results with less than 0.1% of the training data typically used for models of similar size. To take full advantage of their efficiency, we provide an optimized implementation of Llamba for resource-constrained devices such as smartphones and edge platforms, offering a practical and memory-efficient alternative to Transformers. Overall, Llamba improves the tradeoff between speed, memory efficiency, and performance, making high-quality language models more accessible. 

**Abstract (ZH)**: Llamba：一种从Llama-3.x通过Mamba架构提炼的高效递归语言模型系列 

---
# PredictaBoard: Benchmarking LLM Score Predictability 

**Title (ZH)**: PredictaBoard: 评估大型语言模型得分可预测性基准 

**Authors**: Lorenzo Pacchiardi, Konstantinos Voudouris, Ben Slater, Fernando Martínez-Plumed, José Hernández-Orallo, Lexin Zhou, Wout Schellaert  

**Link**: [PDF](https://arxiv.org/pdf/2502.14445)  

**Abstract**: Despite possessing impressive skills, Large Language Models (LLMs) often fail unpredictably, demonstrating inconsistent success in even basic common sense reasoning tasks. This unpredictability poses a significant challenge to ensuring their safe deployment, as identifying and operating within a reliable "safe zone" is essential for mitigating risks. To address this, we present PredictaBoard, a novel collaborative benchmarking framework designed to evaluate the ability of score predictors (referred to as assessors) to anticipate LLM errors on specific task instances (i.e., prompts) from existing datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering the rejection rate at different tolerance errors. As such, PredictaBoard stimulates research into developing better assessors and making LLMs more predictable, not only with a higher average performance. We conduct illustrative experiments using baseline assessors and state-of-the-art LLMs. PredictaBoard highlights the critical need to evaluate predictability alongside performance, paving the way for safer AI systems where errors are not only minimised but also anticipated and effectively mitigated. Code for our benchmark can be found at this https URL 

**Abstract (ZH)**: 尽管拥有令人印象深刻的技能，大型语言模型（LLMs）常常不可预测地失败，在甚至是最基本的常识推理任务上表现出不一致的成功率。这种不可预测性对确保其安全部署构成了重大挑战，因为在安全区内可靠运行是降低风险的关键。为此，我们提出PredictaBoard，这是一种新型的协作基准框架，旨在评估分数预测器（称为评估器）的能力，使其能够预测大型语言模型在特定任务实例（即提示）上的错误。PredictaBoard通过考虑不同容错错误条件下的拒绝率来评估大型语言模型和评估器的配对。因此，PredictaBoard促进了开发更好的评估器和使大型语言模型更具可预测性的研究，不仅在平均性能上更高。我们使用基础评估器和最先进的大型语言模型进行了演示性实验。PredictaBoard突显了评估可预测性与性能同样重要，为安全的人工智能系统铺平了道路，在这种系统中，错误不仅被最小化，还会被预见并有效缓解。我们的基准代码可以在以下链接找到：this https URL。 

---
# S*: Test Time Scaling for Code Generation 

**Title (ZH)**: S*: 测试时代码生成的缩放方法 

**Authors**: Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph E. Gonzalez, Ion Stoica  

**Link**: [PDF](https://arxiv.org/pdf/2502.14382)  

**Abstract**: Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. S* extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions. We evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) S* consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables non-reasoning models to surpass reasoning models - GPT-4o-mini with S* outperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts state-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S* achieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be available under this https URL. 

**Abstract (ZH)**: 增加测试时计算量对大型语言模型和大型推理模型显示出跨领域潜力，但在代码生成领域尚未得到充分探索，尽管在数学领域已有广泛研究。本文提出S*，这是一种新颖的混合测试时扩展框架，显著提高了生成代码的覆盖面和选择准确性。S*扩展了现有的并行扩展范式，加入序列扩展以推动性能边界，并结合了一种新颖的选择机制，该机制自适应地生成区分性输入用于成对比较，结合执行基于的信息以稳健地识别正确解。我们在12个大型语言模型和大型推理模型上进行评估，结果显示：（1）S*在不同模型家族和规模上一致提高性能，使一个3B模型能够超越GPT-4o-mini；（2）S*使非推理模型超越推理模型——配备S*的GPT-4o-mini在LiveCodeBench上比o1-preview高出3.7%；（3）S*进一步提升了最先进的推理模型——配备S*的DeepSeek-R1-Distill-Qwen-32B在LiveCodeBench上取得85.7%的分数，接近o1（高水平）的88.5%。代码将在以下链接下提供：https URL。 

---
# A Survey on Feedback-based Multi-step Reasoning for Large Language Models on Mathematics 

**Title (ZH)**: 基于反馈的多步推理研究：大型语言模型在数学中的应用 

**Authors**: Ting-Ruen Wei, Haowei Liu, Xuyang Wu, Yi Fang  

**Link**: [PDF](https://arxiv.org/pdf/2502.14333)  

**Abstract**: Recent progress in large language models (LLM) found chain-of-thought prompting strategies to improve the reasoning ability of LLMs by encouraging problem solving through multiple steps. Therefore, subsequent research aimed to integrate the multi-step reasoning process into the LLM itself through process rewards as feedback and achieved improvements over prompting strategies. Due to the cost of step-level annotation, some turn to outcome rewards as feedback. Aside from these training-based approaches, training-free techniques leverage frozen LLMs or external tools for feedback at each step to enhance the reasoning process. With the abundance of work in mathematics due to its logical nature, we present a survey of strategies utilizing feedback at the step and outcome levels to enhance multi-step math reasoning for LLMs. As multi-step reasoning emerges a crucial component in scaling LLMs, we hope to establish its foundation for easier understanding and empower further research. 

**Abstract (ZH)**: 最近大语言模型中基于链式推理的提示策略进展：通过多步过程提升推理能力并促进训练与无训练方法的发展 

---
# Line Goes Up? Inherent Limitations of Benchmarks for Evaluating Large Language Models 

**Title (ZH)**: 线性增长？大型语言模型评估基准的固有限制。 

**Authors**: James Fodor  

**Link**: [PDF](https://arxiv.org/pdf/2502.14318)  

**Abstract**: Large language models (LLMs) regularly demonstrate new and impressive performance on a wide range of language, knowledge, and reasoning benchmarks. Such rapid progress has led many commentators to argue that LLM general cognitive capabilities have likewise rapidly improved, with the implication that such models are becoming progressively more capable on various real-world tasks. Here I summarise theoretical and empirical considerations to challenge this narrative. I argue that inherent limitations with the benchmarking paradigm, along with specific limitations of existing benchmarks, render benchmark performance highly unsuitable as a metric for generalisable competence over cognitive tasks. I also contend that alternative methods for assessing LLM capabilities, including adversarial stimuli and interpretability techniques, have shown that LLMs do not have robust competence in many language and reasoning tasks, and often fail to learn representations which facilitate generalisable inferences. I conclude that benchmark performance should not be used as a reliable indicator of general LLM cognitive capabilities. 

**Abstract (ZH)**: 大语言模型（LLMs）在语言、知识和推理等多个基准上持续展现新的卓越性能。这种快速的进步使得许多评论家认为，LLMs 的通用认知能力也得到了快速提升，进而认为这些模型在各种实际任务上变得越来越有能力。在这里，我总结了一些理论和实证考虑，以挑战这种叙述。我认为，基准测试范式的固有限制，以及现有基准的具体限制，使得基准性能非常不适合用作认知任务上可泛化的 competence 的度量标准。我还主张，评估 LLM 能力的替代方法，包括对抗性刺激和可解释性技术，表明LLMs 在许多语言和推理任务上缺乏稳健的能力，并且往往无法学习出促进可泛化推理的表示。我得出结论，基准性能不宜用作可靠的大语言模型认知能力的指标。 

---
# MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models 

**Title (ZH)**: MedHallu: 大规模语言模型中医学幻觉检测的综合基准 

**Authors**: Shrey Pandit, Jiawei Xu, Junyuan Hong, Zhangyang Wang, Tianlong Chen, Kaidi Xu, Ying Ding  

**Link**: [PDF](https://arxiv.org/pdf/2502.14302)  

**Abstract**: Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting "hard" category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a "not sure" category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines. 

**Abstract (ZH)**: 大语言模型（LLMs）的进步及其在医疗问答领域的日益广泛应用 necessitate 严格评估其可靠性。幻觉是关键挑战之一，模型会生成听起来合理但实际上错误的输出。在医疗领域，这会对患者安全和临床决策造成严重风险。为应对这一挑战，我们引入了MedHallu，这是首个专门设计用于医疗幻觉检测的标准。MedHallu 包含来自PubMedQA的 10,000 个高质量的问答对，并通过受控管道系统性地生成了幻觉答案。我们的实验表明，最先进的 LLMs，包括 GPT-4o、Llama-3.1 和医学微调的 UltraMedical，在这种二元幻觉检测任务上表现不佳，最好模型在检测“困难”类幻觉时的 F1 分数低至 0.625。通过双向推论聚类，我们发现难以检测的幻觉与其真实答案在语义上更接近。通过实验，我们还展示了结合领域特定知识并将“不确定”类别作为回答类别之一可以将精度和 F1 分数相对基线提高最多 38%。 

---
# SEA-HELM: Southeast Asian Holistic Evaluation of Language Models 

**Title (ZH)**: SEA-HELM: 东南亚综合语言模型评估 

**Authors**: Yosephine Susanto, Adithya Venkatadri Hulagadri, Jann Railey Montalan, Jian Gang Ngui, Xian Bin Yong, Weiqi Leong, Hamsawardhini Rengarajan, Peerat Limkonchotiwat, Yifan Mai, William Chandra Tjhi  

**Link**: [PDF](https://arxiv.org/pdf/2502.14301)  

**Abstract**: With the rapid emergence of novel capabilities in Large Language Models (LLMs), the need for rigorous multilingual and multicultural benchmarks that are integrated has become more pronounced. Though existing LLM benchmarks are capable of evaluating specific capabilities of LLMs in English as well as in various mid- to low-resource languages, including those in the Southeast Asian (SEA) region, a comprehensive and authentic evaluation suite for the SEA languages has not been developed thus far. Here, we present SEA-HELM, a holistic linguistic and cultural LLM evaluation suite that emphasizes SEA languages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the SEA-HELM leaderboard, which allows users to understand models' multilingual and multicultural performance in a systematic and user-friendly manner. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）新型能力的迅速涌现，对于多语言和多文化的整合基准需求变得更加迫切。尽管现有的LLM基准能够评估LLMs在英语以及各种中低资源语言（包括东南亚地区语言）中的特定能力，但迄今为止尚未开发出全面且真实的东南亚语言评价套件。因此，我们提出了SEA-HELM，一个以东南亚语言为核心的综合语言和文化LLM评价套件，包含五大核心支柱：（1）NLP经典任务，（2）LLM专属性，（3）东南亚语言学，（4）东南亚文化，（5）安全。SEA-HELM目前支持菲律宾语、印尼语、泰米尔语、泰语和越南语。我们还引入了SEA-HELM排行榜，以帮助用户以系统且用户友好的方式理解模型在多语言和多文化方面的表现。 

---
# EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts 

**Title (ZH)**: EpMAN: 基于情景记忆注意力的长期上下文泛化 

**Authors**: Subhajit Chaudhury, Payel Das, Sarathkrishna Swaminathan, Georgios Kollias, Elliot Nelson, Khushbu Pahwa, Tejaswini Pedapati, Igor Melnyk, Matthew Riemer  

**Link**: [PDF](https://arxiv.org/pdf/2502.14280)  

**Abstract**: Recent advances in Large Language Models (LLMs) have yielded impressive successes on many language tasks. However, efficient processing of long contexts using LLMs remains a significant challenge. We introduce \textbf{EpMAN} -- a method for processing long contexts in an \textit{episodic memory} module while \textit{holistically attending to} semantically relevant context chunks. The output of \textit{episodic attention} is then used to reweigh the decoder's self-attention to the stored KV cache of the context during training and generation. When an LLM decoder is trained using \textbf{EpMAN}, its performance on multiple challenging single-hop long-context recall and question-answering benchmarks is found to be stronger and more robust across the range from 16k to 256k tokens than baseline decoders trained with self-attention, and popular retrieval-augmented generation frameworks. 

**Abstract (ZH)**: Recent Advances in Large Language Models (LLMs): Efficient Processing of Long Contexts with EpMAN 

---
# STeCa: Step-level Trajectory Calibration for LLM Agent Learning 

**Title (ZH)**: STeCa: 步骤级轨迹校准用于LLM代理学习 

**Authors**: Hanlin Wang, Jian Wang, Chak Tou Leong, Wenjie Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.14276)  

**Abstract**: Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations and preference learning through exploratory trajectory sampling. However, these methods often struggle in long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. These calibrated trajectories, together with successful trajectory data, are utilized for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that step-level calibration enables agents to complete tasks with greater robustness. Our code and data are available at this https URL. 

**Abstract (ZH)**: 基于大型语言模型的代理在解决复杂任务方面通过与环境动态交互展现出了潜力。现有工作主要集中在从专家示范行为克隆和通过探索性轨迹采样进行偏好学习上。然而，在长期任务中，这些方法往往难以应对，因为次优动作会逐步积累，导致代理偏离正确的任务轨迹。为解决这一问题，我们强调了及时校准的重要性，并提出了自动构建校准轨迹的需求。我们提出了一种新型的基于大型语言模型的代理学习框架——步骤级轨迹校准（STeCa）。具体而言，STeCa 通过在探索过程中进行步骤级奖励比较来识别次优动作，并利用大型语言模型驱动的反思构建校准轨迹，使代理能够从改进的决策过程中学习。这些校准轨迹与成功的轨迹数据一起用于强化训练。大量实验显示，STeCa 显著优于现有方法。进一步分析表明，步骤级校准使代理能够以更大的鲁棒性完成任务。我们的代码和数据可供查看：this https URL。 

---
# LLM-EvRep: Learning an LLM-Compatible Event Representation Using a Self-Supervised Framework 

**Title (ZH)**: LLM-EvRep: 学习一种适合LLM的事件表示-using一种自监督框架 

**Authors**: Zongyou Yu, Qiang Qu, Qian Zhang, Nan Zhang, Xiaoming Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.14273)  

**Abstract**: Recent advancements in event-based recognition have demonstrated significant promise, yet most existing approaches rely on extensive training, limiting their adaptability for efficient processing of event-driven visual content. Meanwhile, large language models (LLMs) have exhibited remarkable zero-shot capabilities across diverse domains, but their application to event-based visual recognition remains largely unexplored. To bridge this gap, we propose \textbf{LLM-EvGen}, an event representation generator that produces LLM-compatible event representations \textbf{LLM-EvRep}, thereby enhancing the performance of LLMs on event recognition tasks. The generator is trained using a self-supervised framework, aligning the generated representations with semantic consistency and structural fidelity. Comprehensive experiments were conducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results demonstrate that our method, \textbf{LLM-EvRep}, outperforms the event-to-video method, E2VID, by 15.93\%, 0.82\%, and 50.21\%, respectively, in recognition tasks when evaluated using GPT-4o. 

**Abstract (ZH)**: Recent advancements in event-based recognition have demonstrated significant promise, yet most existing approaches rely on extensive training, limiting their adaptability for efficient processing of event-driven visual content. Meanwhile, large language models (LLMs) have exhibited remarkable zero-shot capabilities across diverse domains, but their application to event-based visual recognition remains largely unexplored. To bridge this gap, we propose LLM-EvGen, an event representation generator that produces LLM-compatible event representations LLM-EvRep, thereby enhancing the performance of LLMs on event recognition tasks. 

---
# Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models 

**Title (ZH)**: 捕捉细微偏好：面向偏好的蒸馏方法用于小型语言模型 

**Authors**: Yanggan Gu, Junzhuo Li, Sirui Huang, Xin Zou, Zhenghua Li, Xuming Hu  

**Link**: [PDF](https://arxiv.org/pdf/2502.14272)  

**Abstract**: Aligning small language models (SLMs) with human values typically involves distilling preference knowledge from large language models (LLMs). However, existing distillation methods model preference knowledge in teacher LLMs by comparing pairwise responses, overlooking the extent of difference between responses. This limitation hinders student SLMs from capturing the nuanced preferences for multiple responses. In this paper, we propose a Preference-Aligned Distillation (PAD) framework, which models teacher's preference knowledge as a probability distribution over all potential preferences, thereby providing more nuanced supervisory signals. Our insight in developing PAD is rooted in the demonstration that language models can serve as reward functions, reflecting their intrinsic preferences. Based on this, PAD comprises three key steps: (1) sampling diverse responses using high-temperature; (2) computing rewards for both teacher and student to construct their intrinsic preference; and (3) training the student's intrinsic preference distribution to align with the teacher's. Experiments on four mainstream alignment benchmarks demonstrate that PAD consistently and significantly outperforms existing approaches, achieving over 20\% improvement on AlpacaEval 2 and Arena-Hard, indicating superior alignment with human preferences. Notably, on MT-Bench, using the \textsc{Gemma} model family, the student trained by PAD surpasses its teacher, further validating the effectiveness of our PAD. 

**Abstract (ZH)**: 将小型语言模型与人类价值观对齐通常涉及从大型语言模型中提取偏好知识。然而，现有的蒸馏方法通过比较成对的回答来建模教师大型语言模型中的偏好知识，忽略了回答之间的差异程度。这一限制阻碍了学生小型语言模型捕捉多个回答的微妙偏好。在本文中，我们提出了一种偏好对齐蒸馏（PAD）框架，将教师的偏好知识建模为所有潜在偏好的一种概率分布，从而提供更细致的监督信号。我们开发PAD的见解根植于语言模型可以作为奖励函数运行的事实，反映了它们固有的偏好。基于此，PAD包含三个关键步骤：（1）使用高温度采样多样化的回答；（2）为教师和学生计算奖励以构建其固有偏好；（3）训练学生的固有偏好分布以与教师对齐。在四个主流对齐基准上的实验表明，PAD在AlpacaEval 2和Arena-Hard上的一致且显著地优于现有方法，表明其与人类偏好对齐的优越性。值得注意的是，在MT-Bench上，使用Gemma模型家族训练的学生超过了其教师，进一步验证了PAD的有效性。 

---
# MCQA-Eval: Efficient Confidence Evaluation in NLG with Gold-Standard Correctness Labels 

**Title (ZH)**: MCQA-Eval: 高效评估自然语言生成中的置信度与标准正确性标签 

**Authors**: Xiaoou Liu, Zhen Lin, Longchao Da, Chacha Chen, Shubhendu Trivedi, Hua Wei  

**Link**: [PDF](https://arxiv.org/pdf/2502.14268)  

**Abstract**: Large Language Models (LLMs) require robust confidence estimation, particularly in critical domains like healthcare and law where unreliable outputs can lead to significant consequences. Despite much recent work in confidence estimation, current evaluation frameworks rely on correctness functions -- various heuristics that are often noisy, expensive, and possibly introduce systematic biases. These methodological weaknesses tend to distort evaluation metrics and thus the comparative ranking of confidence measures. We introduce MCQA-Eval, an evaluation framework for assessing confidence measures in Natural Language Generation (NLG) that eliminates dependence on an explicit correctness function by leveraging gold-standard correctness labels from multiple-choice datasets. MCQA-Eval enables systematic comparison of both internal state-based white-box (e.g. logit-based) and consistency-based black-box confidence measures, providing a unified evaluation methodology across different approaches. Through extensive experiments on multiple LLMs and widely used QA datasets, we report that MCQA-Eval provides efficient and more reliable assessments of confidence estimation methods than existing approaches. 

**Abstract (ZH)**: 大型语言模型（LLMs）需要稳健的置信度估计，尤其是在医疗和法律等关键领域，不稳定的输出可能导致严重后果。尽管在置信度估计方面已经开展了大量研究工作，但现有的评估框架仍然依赖于各种噪声较大、成本昂贵且可能引入系统性偏差的正确性函数。这些方法论上的缺陷往往会扭曲评价指标，从而影响置信度度量的比较排名。我们提出了一种MCQA-Eval评估框架，用于评估自然语言生成（NLG）中的置信度度量，该框架通过利用多选项数据集的金标准正确性标签而无需依赖显式的正确性函数。MCQA-Eval能够系统地比较基于内部状态的白盒度量（例如，logit基度量）和基于一致性的黑盒度量，提供了不同方法之间的统一评估方法。通过在多个大型语言模型和广泛使用的问答数据集上进行大量实验，我们发现MCQA-Eval相比现有方法提供更高效且更可靠的置信度估计方法评估。 

---
# Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information 

**Title (ZH)**: 时间有其位置吗？时间头：语言模型回忆时间特定信息 

**Authors**: Yein Park, Chanwoong Yoon, Jungwoo Park, Minbyul Jeong, Jaewoo Kang  

**Link**: [PDF](https://arxiv.org/pdf/2502.14258)  

**Abstract**: While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads primarily responsible for processing temporal knowledge through circuit analysis. We confirm that these heads are present across multiple models, though their specific locations may vary, and their responses differ depending on the type of knowledge and its corresponding years. Disabling these heads degrades the model's ability to recall time-specific knowledge while maintaining its general capabilities without compromising time-invariant and question-answering performances. Moreover, the heads are activated not only numeric conditions ("In 2004") but also textual aliases ("In the year ..."), indicating that they encode a temporal dimension beyond simple numerical representation. Furthermore, we expand the potential of our findings by demonstrating how temporal knowledge can be edited by adjusting the values of these heads. 

**Abstract (ZH)**: 语言模型处理时间变化事实的能力虽已广泛研究，但时间头的发现及其在处理时间知识中的作用尚待探索。我们通过电路分析发现了时间头，这些特定的注意力头主要负责处理时间知识。我们确认这些头在多个模型中普遍存在，但它们的具体位置可能有所不同，且对不同类型的知识及其对应年份的响应也不同。禁用这些头会降低模型回忆时间特定知识的能力，同时保持其一般能力而不影响时间不变的表现和问答性能。此外，这些头不仅对数值条件（“在2004年”）作出反应，还对文本别名（“在……年”）作出反应，表明它们编码了超越简单数值表示的时间维度。进一步地，我们展示了如何通过调整这些头的值来编辑时间知识的潜在应用。 

---
# Effects of Prompt Length on Domain-specific Tasks for Large Language Models 

**Title (ZH)**: 大型语言模型在专业领域任务中提示长度的影响 

**Authors**: Qibang Liu, Wenzhe Wang, Jeffrey Willard  

**Link**: [PDF](https://arxiv.org/pdf/2502.14255)  

**Abstract**: In recent years, Large Language Models have garnered significant attention for their strong performance in various natural language tasks, such as machine translation and question answering. These models demonstrate an impressive ability to generalize across diverse tasks. However, their effectiveness in tackling domain-specific tasks, such as financial sentiment analysis and monetary policy understanding, remains a topic of debate, as these tasks often require specialized knowledge and precise reasoning. To address such challenges, researchers design various prompts to unlock the models' abilities. By carefully crafting input prompts, researchers can guide these models to produce more accurate responses. Consequently, prompt engineering has become a key focus of study. Despite the advancements in both models and prompt engineering, the relationship between the two-specifically, how prompt design impacts models' ability to perform domain-specific tasks-remains underexplored. This paper aims to bridge this research gap. 

**Abstract (ZH)**: 近年来，大规模语言模型因其在机器翻译、问答等各类自然语言任务中的出色表现而受到广泛关注。这些模型展现了跨任务泛化的 impressive 能力。然而，它们在处理如金融情感分析和货币政策理解等特定领域任务时的有效性仍存争议，因为这些任务往往需要专门的知识和精确的推理。为了应对这些挑战，研究人员设计了各种提示来激发模型的能力。通过精心设计输入提示，研究人员可以引导这些模型产生更准确的响应。因此，提示工程已经成为一个关键的研究焦点。尽管在模型和提示工程方面取得了进展，但两者之间的关系——尤其是提示设计如何影响模型在特定领域任务上的表现——仍然未得到充分探索。本文旨在填补这一研究空白。 

---
# Towards Secure Program Partitioning for Smart Contracts with LLM's In-Context Learning 

**Title (ZH)**: 面向智能合约的安全程序划分方法：基于LLM的上下文学习 

**Authors**: Ye Liu, Yuqing Niu, Chengyan Ma, Ruidong Han, Wei Ma, Yi Li, Debin Gao, David Lo  

**Link**: [PDF](https://arxiv.org/pdf/2502.14215)  

**Abstract**: Smart contracts are highly susceptible to manipulation attacks due to the leakage of sensitive information. Addressing manipulation vulnerabilities is particularly challenging because they stem from inherent data confidentiality issues rather than straightforward implementation bugs. To tackle this by preventing sensitive information leakage, we present PartitionGPT, the first LLM-driven approach that combines static analysis with the in-context learning capabilities of large language models (LLMs) to partition smart contracts into privileged and normal codebases, guided by a few annotated sensitive data variables. We evaluated PartitionGPT on 18 annotated smart contracts containing 99 sensitive functions. The results demonstrate that PartitionGPT successfully generates compilable, and verified partitions for 78% of the sensitive functions while reducing approximately 30% code compared to function-level partitioning approach. Furthermore, we evaluated PartitionGPT on nine real-world manipulation attacks that lead to a total loss of 25 million dollars, PartitionGPT effectively prevents eight cases, highlighting its potential for broad applicability and the necessity for secure program partitioning during smart contract development to diminish manipulation vulnerabilities. 

**Abstract (ZH)**: 基于大型语言模型的分区方法PartitionGPT：防范智能合约敏感信息泄漏以应对操纵攻击 

---
# On-the-fly Preference Alignment via Principle-Guided Decoding 

**Title (ZH)**: 根据原则导向解码实现的实时偏好对齐 

**Authors**: Mingye Zhu, Yi Liu, Lei Zhang, Junbo Guo, Zhendong Mao  

**Link**: [PDF](https://arxiv.org/pdf/2502.14204)  

**Abstract**: With the rapidly expanding landscape of large language models, aligning model generations with human values and preferences is becoming increasingly important. Popular alignment methods, such as Reinforcement Learning from Human Feedback, have shown significant success in guiding models with greater control. However, these methods require considerable computational resources, which is inefficient, and substantial collection of training data to accommodate the diverse and pluralistic nature of human preferences, which is impractical. These limitations significantly constrain the scope and efficacy of both task-specific and general preference alignment methods. In this work, we introduce On-the-fly Preference Alignment via Principle-Guided Decoding (OPAD) to directly align model outputs with human preferences during inference, eliminating the need for fine-tuning. Our approach involves first curating a surrogate solution to an otherwise infeasible optimization problem and then designing a principle-guided reward function based on this surrogate. The final aligned policy is derived by maximizing this customized reward, which exploits the discrepancy between the constrained policy and its unconstrained counterpart. OPAD directly modifies the model's predictions during inference, ensuring principle adherence without incurring the computational overhead of retraining or fine-tuning. Experiments show that OPAD achieves competitive or superior performance in both general and personalized alignment tasks, demonstrating its efficiency and effectiveness compared to state-of-the-art baselines. 

**Abstract (ZH)**: 随用随调的原理指导解码偏倚对齐方法（OPAD）：直接在推理时调整模型输出以符合人类偏好 

---
# Do LLMs Consider Security? An Empirical Study on Responses to Programming Questions 

**Title (ZH)**: 大规模语言模型考虑安全性吗？编程问题回应的实证研究 

**Authors**: Amirali Sajadi, Binh Le, Anh Nguyen, Kostadin Damevski, Preetha Chatterjee  

**Link**: [PDF](https://arxiv.org/pdf/2502.14202)  

**Abstract**: The widespread adoption of conversational LLMs for software development has raised new security concerns regarding the safety of LLM-generated content. Our motivational study outlines ChatGPT's potential in volunteering context-specific information to the developers, promoting safe coding practices. Motivated by this finding, we conduct a study to evaluate the degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and Llama 3. We prompt these LLMs with Stack Overflow questions that contain vulnerable code to evaluate whether they merely provide answers to the questions or if they also warn users about the insecure code, thereby demonstrating a degree of security awareness. Further, we assess whether LLM responses provide information about the causes, exploits, and the potential fixes of the vulnerability, to help raise users' awareness. Our findings show that all three models struggle to accurately detect and warn users about vulnerabilities, achieving a detection rate of only 12.6% to 40% across our datasets. We also observe that the LLMs tend to identify certain types of vulnerabilities related to sensitive information exposure and improper input neutralization much more frequently than other types, such as those involving external control of file names or paths. Furthermore, when LLMs do issue security warnings, they often provide more information on the causes, exploits, and fixes of vulnerabilities compared to Stack Overflow responses. Finally, we provide an in-depth discussion on the implications of our findings and present a CLI-based prompting tool that can be used to generate significantly more secure LLM responses. 

**Abstract (ZH)**: 广泛采用的对话型大语言模型在软件开发中的应用引发了对其生成内容安全性的新关切。我们通过动机研究概述了ChatGPT在志愿提供上下文相关信息方面的潜力，及其在促进安全编码实践方面的角色。受此发现的启发，我们研究了Claude 3、GPT-4和Llama 3这三种主要大语言模型的安全意识程度。我们通过向这些模型提供包含漏洞代码的Stack Overflow问题来测试它们是否仅仅回答问题还是也警告用户关于不安全代码，从而展示它们的安全意识程度。进一步评估模型的响应是否提供了漏洞的原因、利用方法和潜在修复措施的信息，以帮助提高用户的意识。我们的研究发现，所有三个模型在检测和警告用户漏洞方面都存在困难，在我们的数据集中检测率仅为12.6%到40%。我们还观察到，模型更频繁地识别与敏感信息暴露和输入不适当中和相关的漏洞类型。此外，当大语言模型发出安全警告时，它们提供的漏洞原因、利用方法和修复措施的信息通常比Stack Overflow的回答更详细。最后，我们深入讨论了研究发现的意义，并提出了一种基于命令行接口的提示工具，可以生成更加安全的大语言模型响应。 

---
# Navigating Semantic Relations: Challenges for Language Models in Abstract Common-Sense Reasoning 

**Title (ZH)**: 导航语义关系：语言模型在抽象常识推理中的挑战 

**Authors**: Cole Gawin, Yidan Sun, Mayank Kejriwal  

**Link**: [PDF](https://arxiv.org/pdf/2502.14086)  

**Abstract**: Large language models (LLMs) have achieved remarkable performance in generating human-like text and solving reasoning tasks of moderate complexity, such as question-answering and mathematical problem-solving. However, their capabilities in tasks requiring deeper cognitive skills, such as common-sense understanding and abstract reasoning, remain under-explored. In this paper, we systematically evaluate abstract common-sense reasoning in LLMs using the ConceptNet knowledge graph. We propose two prompting approaches: instruct prompting, where models predict plausible semantic relationships based on provided definitions, and few-shot prompting, where models identify relations using examples as guidance. Our experiments with the gpt-4o-mini model show that in instruct prompting, consistent performance is obtained when ranking multiple relations but with substantial decline when the model is restricted to predicting only one relation. In few-shot prompting, the model's accuracy improves significantly when selecting from five relations rather than the full set, although with notable bias toward certain relations. These results suggest significant gaps still, even in commercially used LLMs' abstract common-sense reasoning abilities, compared to human-level understanding. However, the findings also highlight the promise of careful prompt engineering, based on selective retrieval, for obtaining better performance. 

**Abstract (ZH)**: 大型语言模型（LLMs）在生成类人类文本和解决中等复杂性推理任务（如问答和数学问题解决）方面取得了显著成绩，但是在需要更深层次认知技能的任务（如常识理解和抽象推理）方面的能力仍需进一步探索。在本文中，我们利用ConceptNet知识图谱系统评估LLMs的抽象常识推理能力。我们提出了两种提示方法：指令提示，模型基于提供的定义预测可能的语义关系；少量示例提示，模型使用示例作为指导识别关系。我们的实验表明，在指令提示中，当对多种关系进行排名时，可获得一致性能，但当模型仅限于预测单一关系时，性能有显著下降；在少量示例提示中，当从五个关系中选择时，模型的准确性显著提高，尽管存在对某些关系的明显偏好。这些结果表明，即使在商业使用的LLMs中，其抽象常识推理能力与人类水平的理解之间仍存在显著差距。然而，这些发现也暗示了基于选择性检索的细致提示工程能够在性能上取得更好的效果。 

---
# Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder 

**Title (ZH)**: 基于稀疏自编码器的多样性驱动数据选择以调优语言模型 

**Authors**: Xianjun Yang, Shaoliang Nie, Lijuan Liu, Suchin Gururangan, Ujjwal Karn, Rui Hou, Madian Khabsa, Yuning Mao  

**Link**: [PDF](https://arxiv.org/pdf/2502.14050)  

**Abstract**: Current pre-trained large language models typically need instruction tuning to align with human preferences. However, instruction tuning data is often quantity-saturated due to the large volume of data collection and fast model iteration, leaving coreset data selection important but underexplored. On the other hand, existing quality-driven data selection methods such as LIMA (NeurIPS 2023 (Zhou et al., 2024)) and AlpaGasus (ICLR 2024 (Chen et al.)) generally ignore the equal importance of data diversity and complexity. In this work, we aim to design a diversity-aware data selection strategy and creatively propose using sparse autoencoders to tackle the challenge of data diversity measure. In addition, sparse autoencoders can also provide more interpretability of model behavior and explain, e.g., the surprising effectiveness of selecting the longest response (ICML 2024 (Zhao et al.)). Using effective data selection, we experimentally prove that models trained on our selected data can outperform other methods in terms of model capabilities, reduce training cost, and potentially gain more control over model behaviors. 

**Abstract (ZH)**: 当前预训练的大语言模型通常需要指令调优以与人类偏好对齐。然而，由于数据采集量大和模型迭代速度快，指令调优数据往往已达到数量饱和，使得核心集数据选择变得重要但尚未得到充分探索。另一方面，现有的以质量为导向的数据选择方法，如LIMA（NeurIPS 2023，Zhou et al., 2024）和AlpaGasus（ICLR 2024，Chen et al.），通常忽视了数据多样性和复杂性同等重要的地位。在这项工作中，我们旨在设计一种多样性意识的数据选择策略，并创造性地提出使用稀疏自编码器来应对数据多样性的挑战度量问题。此外，稀疏自编码器还可以提供模型行为的更多可解释性，例如解释选择最长响应的惊人效果（ICML 2024，Zhao et al.）。通过有效的数据选择，我们实验证明，使用我们选择的数据训练的模型可以在模型能力、降低训练成本以及可能更好地控制模型行为方面超越其他方法。 

---
# Semantic Decomposition and Selective Context Filtering -- Text Processing Techniques for Context-Aware NLP-Based Systems 

**Title (ZH)**: 语义分解与选择性语境过滤——面向语境感知NLP系统的文本处理技术 

**Authors**: Karl John Villardar  

**Link**: [PDF](https://arxiv.org/pdf/2502.14048)  

**Abstract**: In this paper, we present two techniques for use in context-aware systems: Semantic Decomposition, which sequentially decomposes input prompts into a structured and hierarchal information schema in which systems can parse and process easily, and Selective Context Filtering, which enables systems to systematically filter out specific irrelevant sections of contextual information that is fed through a system's NLP-based pipeline. We will explore how context-aware systems and applications can utilize these two techniques in order to implement dynamic LLM-to-system interfaces, improve an LLM's ability to generate more contextually cohesive user-facing responses, and optimize complex automated workflows and pipelines. 

**Abstract (ZH)**: 本文介绍了两种应用于情境感知系统的技术：语义分解，该技术按顺序将输入提示分解为一种结构化和分层的信息模式，使系统能够轻松解析和处理；选择性情境过滤，该技术使系统能够系统地过滤掉特定的无关情境信息，这些信息通过系统的基于自然语言处理的工作流程传递。我们将探讨这两种技术如何应用于情境感知系统和应用，以实现动态的LLM-系统接口，提高LLM生成上下文连贯的用户面向响应的能力，并优化复杂的自动化工作流和管道。 

---
# Asking for Help Enables Safety Guarantees Without Sacrificing Effectiveness 

**Title (ZH)**: 请求帮助能够在不牺牲有效性的情况下提供安全性保证。 

**Authors**: Benjamin Plaut, Juan Liévano-Karim, Stuart Russell  

**Link**: [PDF](https://arxiv.org/pdf/2502.14043)  

**Abstract**: Most reinforcement learning algorithms with regret guarantees rely on a critical assumption: that all errors are recoverable. Recent work by Plaut et al. discarded this assumption and presented algorithms that avoid "catastrophe" (i.e., irreparable errors) by asking for help. However, they provided only safety guarantees and did not consider reward maximization. We prove that any algorithm that avoids catastrophe in their setting also guarantees high reward (i.e., sublinear regret) in any Markov Decision Process (MDP), including MDPs with irreversible costs. This constitutes the first no-regret guarantee for general MDPs. More broadly, our result may be the first formal proof that it is possible for an agent to obtain high reward while becoming self-sufficient in an unknown, unbounded, and high-stakes environment without causing catastrophe or requiring resets. 

**Abstract (ZH)**: 大多数具有遗憾保证的强化学习算法依赖于一个关键假设：所有错误都是可恢复的。Plaut等人的近期工作舍弃了这一假设，并提出了通过请求帮助来避免“灾难”（即不可恢复的错误）的算法。然而，他们仅提供了安全保证，并未考虑奖励最大化。我们证明，在他们设定的场景中，任何避免灾难的算法也将在任何马尔可夫决策过程（MDP）中保证高奖励（即亚线性遗憾）。这构成了对一般MDP的第一个无遗憾保证。更广泛地说，我们的结果可能是第一个形式证明，在未知、未 bound 和高风险的环境中，代理能够获得高奖励并变得自给自足而不会造成灾难或需要重置。 

---
# DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation 

**Title (ZH)**: DiffSampling: 提升神经文本生成中的多样性和准确性 

**Authors**: Giorgio Franceschelli, Mirco Musolesi  

**Link**: [PDF](https://arxiv.org/pdf/2502.14037)  

**Abstract**: Despite their increasing performance, large language models still tend to reproduce training data, generate several repetitions, and focus on the most common grammatical structures and words. A possible cause is the decoding strategy adopted: the most common ones either consider only the most probable tokens, reducing output diversity, or increase the likelihood of unlikely tokens at the cost of output accuracy and correctness. In this paper, we propose a family of three new decoding methods by leveraging a mathematical analysis of the token probability distribution. In particular, the difference between consecutive, sorted probabilities can be used to avoid incorrect tokens and increase the chance of low-probable but accurate words. Experiments concerning math problem solving, extreme summarization, and the divergent association task show that our approach consistently performs at least as well as current alternatives in terms of quality and diversity. 

**Abstract (ZH)**: 尽管大型语言模型的性能不断提升，但仍倾向于复制训练数据、生成多次重复，并且集中在最常见的语法结构和词汇上。一种可能的原因是所采用的解码策略：最常见的策略要么只考虑最可能的词元，减少了输出的多样性，要么增加了不太可能的词元的概率，但牺牲了输出的准确性和正确性。本文通过利用词元概率分布的数学分析，提出了一类三种新的解码方法。具体而言，连续排列后的概率差可以用来避免错误的词元并增加低概率但准确的词汇出现的机会。关于数学问题求解、极端摘要和发散关联任务的实验表明，我们的方法在质量和多样性方面至少与当前替代方法相当。 

---
# Which Attention Heads Matter for In-Context Learning? 

**Title (ZH)**: 上下文学习中哪些注意力头更重要？ 

**Authors**: Kayo Yin, Jacob Steinhardt  

**Link**: [PDF](https://arxiv.org/pdf/2502.14010)  

**Abstract**: Large language models (LLMs) exhibit impressive in-context learning (ICL) capability, enabling them to perform new tasks using only a few demonstrations in the prompt. Two different mechanisms have been proposed to explain ICL: induction heads that find and copy relevant tokens, and function vector (FV) heads whose activations compute a latent encoding of the ICL task. To better understand which of the two distinct mechanisms drives ICL, we study and compare induction heads and FV heads in 12 language models.
Through detailed ablations, we discover that few-shot ICL performance depends primarily on FV heads, especially in larger models. In addition, we uncover that FV and induction heads are connected: many FV heads start as induction heads during training before transitioning to the FV mechanism. This leads us to speculate that induction facilitates learning the more complex FV mechanism that ultimately drives ICL. 

**Abstract (ZH)**: 大型语言模型（LLMs）展现出令人印象深刻的上下文内学习（ICL）能力，能够在提示中仅通过几个示范执行新任务。有两种不同的机制被提出解释ICL：归纳头部通过查找和复制相关tokens，以及功能向量（FV）头部，其激活计算ICL任务的潜在编码。为了更好地理解是哪种机制驱动ICL，我们研究并比较了12种语言模型中的归纳头部和FV头部。通过详细的消融实验，我们发现少量样本的ICL性能主要依赖于FV头部，尤其是在较大模型中。此外，我们发现FV头部和归纳头部之间存在联系：许多FV头部在训练过程中最初是归纳头部，之后转变为FV机制。这使我们推测，归纳过程有助于学习更复杂的FV机制，最终驱动ICL。 

---
# MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures 

**Title (ZH)**: MaskPrune: 基于掩码的层级均匀结构大语言模型剪枝 

**Authors**: Jiayu Qin, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Wei Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.14008)  

**Abstract**: The remarkable performance of large language models (LLMs) in various language tasks has attracted considerable attention. However, the ever-increasing size of these models presents growing challenges for deployment and inference. Structured pruning, an effective model compression technique, is gaining increasing attention due to its ability to enhance inference efficiency. Nevertheless, most previous optimization-based structured pruning methods sacrifice the uniform structure across layers for greater flexibility to maintain performance. The heterogeneous structure hinders the effective utilization of off-the-shelf inference acceleration techniques and impedes efficient configuration for continued training. To address this issue, we propose a novel masking learning paradigm based on minimax optimization to obtain the uniform pruned structure by optimizing the masks under sparsity regularization. Extensive experimental results demonstrate that our method can maintain high performance while ensuring the uniformity of the pruned model structure, thereby outperforming existing SOTA methods. 

**Abstract (ZH)**: 大型语言模型在各类语言任务中的出色表现吸引了广泛关注。然而，这些模型日益增大的规模给部署和推理带来了不断增加的挑战。基于最小最大化优化的新型掩码学习范式通过在稀疏正则化下优化掩码获得了均匀的裁剪结构，有效模型压缩技术因此能够提高推理效率。尽管大多数基于优化的结构化裁剪方法牺牲了层间的一致性以换取更大的灵活性从而维持性能，但异构结构阻碍了现成推理加速技术的有效利用，并阻碍了持续训练的高效配置。为解决这一问题，我们提出了一种基于最小最大化优化的新型掩码学习范式，通过在稀疏正则化下优化掩码获得了均匀的裁剪结构，从而在保持高性能的同时优于现有最佳方法。 

---
