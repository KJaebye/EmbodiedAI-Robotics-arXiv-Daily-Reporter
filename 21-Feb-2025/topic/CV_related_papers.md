# Exploring Advanced Techniques for Visual Question Answering: A Comprehensive Comparison 

**Title (ZH)**: 探索视觉问答中的高级技术：全面比较 

**Authors**: Aiswarya Baby, Tintu Thankom Koshy  

**Link**: [PDF](https://arxiv.org/pdf/2502.14827)  

**Abstract**: Visual Question Answering (VQA) has emerged as a pivotal task in the intersection of computer vision and natural language processing, requiring models to understand and reason about visual content in response to natural language questions. Analyzing VQA datasets is essential for developing robust models that can handle the complexities of multimodal reasoning. Several approaches have been developed to examine these datasets, each offering distinct perspectives on question diversity, answer distribution, and visual-textual correlations. Despite significant progress, existing VQA models face challenges related to dataset bias, limited model complexity, commonsense reasoning gaps, rigid evaluation methods, and generalization to real world scenarios. This paper presents a comprehensive comparative study of five advanced VQA models: ABC-CNN, KICNLE, Masked Vision and Language Modeling, BLIP-2, and OFA, each employing distinct methodologies to address these challenges. 

**Abstract (ZH)**: 视觉问答（VQA）已成为计算机视觉和自然语言处理交叉领域的一个关键任务，要求模型理解并推理视觉内容以回答自然语言问题。分析VQA数据集对于开发能够处理多模态推理复杂性的稳健模型至关重要。已经开发出了多种方法来检查这些数据集，每种方法都从不同的视角提供了关于问题多样性、答案分布和视觉-文本相关性的见解。尽管取得了显著进展，现有的VQA模型仍面临数据集偏差、模型复杂度有限、常识推理缺口、僵化的评估方法以及向现实场景泛化的挑战。本文对五种先进的VQA模型——ABC-CNN、KICNLE、遮蔽视觉和语言模型、BLIP-2和OFA进行了全面比较研究，每种模型采用了不同的方法来应对这些挑战。 

---
# FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image Analysis 

**Title (ZH)**: 胎儿CLIP：一种胎儿超声图像分析的视觉-语言基础模型 

**Authors**: Fadillah Maani, Numan Saeed, Tausifa Saleem, Zaid Farooq, Hussain Alasmawi, Werner Diehl, Ameera Mohammad, Gareth Waring, Saudabi Valappi, Leanne Bricker, Mohammad Yaqub  

**Link**: [PDF](https://arxiv.org/pdf/2502.14807)  

**Abstract**: Foundation models are becoming increasingly effective in the medical domain, offering pre-trained models on large datasets that can be readily adapted for downstream tasks. Despite progress, fetal ultrasound images remain a challenging domain for foundation models due to their inherent complexity, often requiring substantial additional training and facing limitations due to the scarcity of paired multimodal data. To overcome these challenges, here we introduce FetalCLIP, a vision-language foundation model capable of generating universal representation of fetal ultrasound images. FetalCLIP was pre-trained using a multimodal learning approach on a diverse dataset of 210,035 fetal ultrasound images paired with text. This represents the largest paired dataset of its kind used for foundation model development to date. This unique training approach allows FetalCLIP to effectively learn the intricate anatomical features present in fetal ultrasound images, resulting in robust representations that can be used for a variety of downstream applications. In extensive benchmarking across a range of key fetal ultrasound applications, including classification, gestational age estimation, congenital heart defect (CHD) detection, and fetal structure segmentation, FetalCLIP outperformed all baselines while demonstrating remarkable generalizability and strong performance even with limited labeled data. We plan to release the FetalCLIP model publicly for the benefit of the broader scientific community. 

**Abstract (ZH)**: 基础模型在医学领域日益有效，提供了预训练的大数据分析模型，可以快速适应下游任务。尽管取得了进展，但由于其固有的复杂性，胎儿超声图像仍然是基础模型面临的一大挑战，需要大量额外训练，并且受限于成对多模态数据的稀缺性。为克服这些挑战，我们介绍了FetalCLIP，一种能够生成胎儿超声图像通用表示的基础视觉-语言模型。FetalCLIP使用多模态学习方法，在包含210,035张胎儿超声图像及其文本配对数据的多样化数据集上进行预训练。这代表了迄今为止用于基础模型开发的最大规模的成对数据集。这种独特的训练方法使FetalCLIP能够有效地学习胎儿超声图像中复杂的解剖特征，从而生成可用于多种下游应用的稳健表示。在涵盖胎儿超声图像分类、孕周估计、先天性心脏病检测（CHD）和胎儿结构分割等关键应用的广泛基准测试中，FetalCLIP在所有基线中表现优异，展示了卓越的泛化能力和即使在有限标注数据的情况下仍能表现出色的能力。我们计划将FetalCLIP模型公开发布，以造福更广泛的科学界。 

---
# A Survey on Text-Driven 360-Degree Panorama Generation 

**Title (ZH)**: 文本驱动的360度全景生成综述 

**Authors**: Hai Wang, Xiaoyu Xiang, Weihao Xia, Jing-Hao Xue  

**Link**: [PDF](https://arxiv.org/pdf/2502.14799)  

**Abstract**: The advent of text-driven 360-degree panorama generation, enabling the synthesis of 360-degree panoramic images directly from textual descriptions, marks a transformative advancement in immersive visual content creation. This innovation significantly simplifies the traditionally complex process of producing such content. Recent progress in text-to-image diffusion models has accelerated the rapid development in this emerging field. This survey presents a comprehensive review of text-driven 360-degree panorama generation, offering an in-depth analysis of state-of-the-art algorithms and their expanding applications in 360-degree 3D scene generation. Furthermore, we critically examine current limitations and propose promising directions for future research. A curated project page with relevant resources and research papers is available at this https URL. 

**Abstract (ZH)**: 文本驱动的360度全景生成的兴起，使得可以直接从文本描述合成360度全景图像，标志着沉浸式视觉内容创作的一个转型性进步。这一创新显著简化了传统上复杂的内容生成过程。最近在文本到图像扩散模型方面取得的进展加速了这一新兴领域的快速发展。本文综述了文本驱动的360度全景生成，提供了对最先进的算法及其在360度3D场景生成中的扩展应用的深入分析。此外，我们对当前的局限性进行了批判性检视，并提出了未来研究的有希望的方向。相关资源和研究论文可在以下链接中找到：这个https URL。 

---
# MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders 

**Title (ZH)**: MedVAE: 高效的大型通用自编码器驱动的医学图像自动解释方法 

**Authors**: Maya Varma, Ashwin Kumar, Rogier van der Sluijs, Sophie Ostmeier, Louis Blankemeier, Pierre Chambon, Christian Bluethgen, Jip Prince, Curtis Langlotz, Akshay Chaudhari  

**Link**: [PDF](https://arxiv.org/pdf/2502.14753)  

**Abstract**: Medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. Consequently, training deep learning models on medical images can incur large computational costs. In this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. We introduce MedVAE, a family of six large-scale 2D and 3D autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. We train MedVAE autoencoders using a novel two-stage training approach with 1,052,730 medical images. Across diverse tasks obtained from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits (up to 70x improvement in throughput) while simultaneously preserving clinically-relevant features and (2) MedVAE can decode latent representations back to high-resolution images with high fidelity. Our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. Our code is available at this https URL. 

**Abstract (ZH)**: 医学图像以高分辨率和大视野获取，以捕捉临床决策所需的细微特征。因此，训练深度学习模型时可能会产生巨大的计算成本。本文旨在通过压缩医学图像以提高计算效率同时保留临床相关特征，解决这一挑战。我们引入了MedVAE，这是一种由六种大规模2D和3D自编码器组成的家族，能够将医学图像编码为压缩的潜在表示，并将潜在表示解码回高分辨率图像。我们使用新颖的两阶段训练方法，利用1,052,730张医学图像来训练MedVAE自编码器。在来自20个医学图像数据集的多种任务中，我们证明了：（1）在训练下游模型时使用MedVAE潜在表示代替高分辨率图像可以带来效率优势（吞吐量最高可提升70倍）同时保留临床相关特征；（2）MedVAE能够以高保真度将潜在表示解码回高分辨率图像。我们的工作表明，大规模、可泛化的自编码器可以帮助解决医学领域中的关键效率挑战。代码可在以下链接获得：this https URL。 

---
# YOLOv12: A Breakdown of the Key Architectural Features 

**Title (ZH)**: YOLOv12：关键架构特征解析 

**Authors**: Mujadded Al Rabbani Alif, Muhammad Hussain  

**Link**: [PDF](https://arxiv.org/pdf/2502.14740)  

**Abstract**: This paper presents an architectural analysis of YOLOv12, a significant advancement in single-stage, real-time object detection building upon the strengths of its predecessors while introducing key improvements. The model incorporates an optimised backbone (R-ELAN), 7x7 separable convolutions, and FlashAttention-driven area-based attention, improving feature extraction, enhanced efficiency, and robust detections. With multiple model variants, similar to its predecessors, YOLOv12 offers scalable solutions for both latency-sensitive and high-accuracy applications. Experimental results manifest consistent gains in mean average precision (mAP) and inference speed, making YOLOv12 a compelling choice for applications in autonomous systems, security, and real-time analytics. By achieving an optimal balance between computational efficiency and performance, YOLOv12 sets a new benchmark for real-time computer vision, facilitating deployment across diverse hardware platforms, from edge devices to high-performance clusters. 

**Abstract (ZH)**: YOLOv12：基于前代模型改进的实时单阶段目标检测架构分析 

---
# Single-image Reflectance and Transmittance Estimation from Any Flatbed Scanner 

**Title (ZH)**: 单张图像从任意平板扫描仪估计反射率和透射率 

**Authors**: Carlos Rodriguez-Pardo, David Pascual-Hernandez, Javier Rodriguez-Vazquez, Jorge Lopez-Moreno, Elena Garces  

**Link**: [PDF](https://arxiv.org/pdf/2502.14462)  

**Abstract**: Flatbed scanners have emerged as promising devices for high-resolution, single-image material capture. However, existing approaches assume very specific conditions, such as uniform diffuse illumination, which are only available in certain high-end devices, hindering their scalability and cost. In contrast, in this work, we introduce a method inspired by intrinsic image decomposition, which accurately removes both shading and specularity, effectively allowing captures with any flatbed scanner. Further, we extend previous work on single-image material reflectance capture with the estimation of opacity and transmittance, critical components of full material appearance (SVBSDF), improving the results for any material captured with a flatbed scanner, at a very high resolution and accuracy 

**Abstract (ZH)**: 基于平面扫描仪的单图像材料捕捉方法：去 shading 和 specularity 以适用于任意扫描仪并估计不透明度和透射率 

---
# Reliable Explainability of Deep Learning Spatial-Spectral Classifiers for Improved Semantic Segmentation in Autonomous Driving 

**Title (ZH)**: 深度学习空谱分类器的可靠可解释性在自主驾驶改进语义分割中的应用 

**Authors**: Jon Gutiérrez-Zaballa, Koldo Basterretxea, Javier Echanobe  

**Link**: [PDF](https://arxiv.org/pdf/2502.14416)  

**Abstract**: Integrating hyperspectral imagery (HSI) with deep neural networks (DNNs) can strengthen the accuracy of intelligent vision systems by combining spectral and spatial information, which is useful for tasks like semantic segmentation in autonomous driving. To advance research in such safety-critical systems, determining the precise contribution of spectral information to complex DNNs' output is needed. To address this, several saliency methods, such as class activation maps (CAM), have been proposed primarily for image classification. However, recent studies have raised concerns regarding their reliability. In this paper, we address their limitations and propose an alternative approach by leveraging the data provided by activations and weights from relevant DNN layers to better capture the relationship between input features and predictions. The study aims to assess the superior performance of HSI compared to 3-channel and single-channel DNNs. We also address the influence of spectral signature normalization for enhancing DNN robustness in real-world driving conditions. 

**Abstract (ZH)**: 将高光谱成像（HSI）与深度神经网络（DNNs）集成可以增强智能视觉系统的准确性，通过结合光谱和空间信息，这有助于自动驾驶中的语义分割等任务。为了推进此类安全关键系统的研究，确定光谱信息对复杂DNNs输出的精确贡献是必要的。为此，已经提出了多种显著性方法，如类激活图（CAM），主要用于图像分类。然而，近期研究对它们的可靠性和准确性提出了质疑。本文旨在解决这些问题，并通过利用相关DNN层中的激活数据和权重，提出一种新的方法来更好地捕捉输入特征与预测之间的关系。研究旨在评估HSI相较于3通道和单通道DNN的优越性能。我们还探讨了光谱特征规范化对提高DNN在真实驾驶条件下的鲁棒性的影响。 

---
# Textured 3D Regenerative Morphing with 3D Diffusion Prior 

**Title (ZH)**: 具有3D扩散先验的纹理化3D再生形变 

**Authors**: Songlin Yang, Yushi Lan, Honghua Chen, Xingang Pan  

**Link**: [PDF](https://arxiv.org/pdf/2502.14316)  

**Abstract**: Textured 3D morphing creates smooth and plausible interpolation sequences between two 3D objects, focusing on transitions in both shape and texture. This is important for creative applications like visual effects in filmmaking. Previous methods rely on establishing point-to-point correspondences and determining smooth deformation trajectories, which inherently restrict them to shape-only morphing on untextured, topologically aligned datasets. This restriction leads to labor-intensive preprocessing and poor generalization. To overcome these challenges, we propose a method for 3D regenerative morphing using a 3D diffusion prior. Unlike previous methods that depend on explicit correspondences and deformations, our method eliminates the additional need for obtaining correspondence and uses the 3D diffusion prior to generate morphing. Specifically, we introduce a 3D diffusion model and interpolate the source and target information at three levels: initial noise, model parameters, and condition features. We then explore an Attention Fusion strategy to generate more smooth morphing sequences. To further improve the plausibility of semantic interpolation and the generated 3D surfaces, we propose two strategies: (a) Token Reordering, where we match approximate tokens based on semantic analysis to guide implicit correspondences in the denoising process of the diffusion model, and (b) Low-Frequency Enhancement, where we enhance low-frequency signals in the tokens to improve the quality of generated surfaces. Experimental results show that our method achieves superior smoothness and plausibility in 3D morphing across diverse cross-category object pairs, offering a novel regenerative method for 3D morphing with textured representations. 

**Abstract (ZH)**: 纹理化的3D形变创建了两个3D对象之间平滑且可信的插值序列，着重于形态和纹理的过渡。该方法对于电影制作中的视觉 effects 等创意应用非常重要。先前的方法依赖于建立点对点对应关系并确定平滑变形轨迹，这使它们本质上仅限于未纹理化且拓扑对齐的数据集上的形态形变。这种限制导致预处理工作量大且泛化能力差。为了克服这些挑战，我们提出了一种基于3D扩散先验的3D再生形变方法。与依赖明确对应关系和变形的方法不同，我们的方法消除了额外获取对应关系的需要，并利用3D扩散先验生成形变。具体而言，我们引入了一种3D扩散模型，并在初始噪声、模型参数和条件特征三个层级上插值源和目标信息。然后，我们探索了一种注意力融合策略以生成更平滑的形变序列。为了进一步提高语义插值和生成的3D表面的合理性，我们提出了两种策略：(a) Token 重新排序，在去噪过程中基于语义分析匹配近似标记以引导隐式对应关系；(b) 低频增强，在标记中增强低频信号以提高生成表面的质量。实验结果表明，我们的方法在多种类别间对象对的3D形变中实现了更优的平滑性和合理性，提供了一种用于具有纹理表示的3D形变的新型再生方法。 

---
# Pandora3D: A Comprehensive Framework for High-Quality 3D Shape and Texture Generation 

**Title (ZH)**: Pandora3D：高质量3D形状和纹理生成的综合框架 

**Authors**: Jiayu Yang, Taizhang Shang, Weixuan Sun, Xibin Song, Ziang Chen, Senbo Wang, Shenzhou Chen, Weizhe Liu, Hongdong Li, Pan Ji  

**Link**: [PDF](https://arxiv.org/pdf/2502.14247)  

**Abstract**: This report presents a comprehensive framework for generating high-quality 3D shapes and textures from diverse input prompts, including single images, multi-view images, and text descriptions. The framework consists of 3D shape generation and texture generation. (1). The 3D shape generation pipeline employs a Variational Autoencoder (VAE) to encode implicit 3D geometries into a latent space and a diffusion network to generate latents conditioned on input prompts, with modifications to enhance model capacity. An alternative Artist-Created Mesh (AM) generation approach is also explored, yielding promising results for simpler geometries. (2). Texture generation involves a multi-stage process starting with frontal images generation followed by multi-view images generation, RGB-to-PBR texture conversion, and high-resolution multi-view texture refinement. A consistency scheduler is plugged into every stage, to enforce pixel-wise consistency among multi-view textures during inference, ensuring seamless integration.
The pipeline demonstrates effective handling of diverse input formats, leveraging advanced neural architectures and novel methodologies to produce high-quality 3D content. This report details the system architecture, experimental results, and potential future directions to improve and expand the framework. The source code and pretrained weights are released at: \url{this https URL}. 

**Abstract (ZH)**: 本报告提出了一种综合框架，用于从单张图像、多视角图像和文本描述等多种输入提示中生成高质量的3D形状和纹理。该框架包括3D形状生成和纹理生成。(1) 3D形状生成管道采用变分自编码器(VAE)将隐式3D几何编码到潜在空间，并使用扩散网络根据输入提示生成条件化潜在变量，同时进行了改进以增强模型容量。还探索了艺术家创建的网格(AM)生成方法，对简单几何形状产生了令人鼓舞的结果。(2) 纹理生成涉及多阶段过程，包括从正面图像生成开始， followed by 多视角图像生成、RGB到PBR纹理转换，以及高分辨率多视角纹理细化。每个阶段插入一致性调度器，以在推断过程中确保多视角纹理之间的像素级一致性，从而实现平滑的整合。该管道有效地处理了多种输入格式，利用先进的神经架构和新颖的方法论生成高质量的3D内容。本报告详细介绍了系统架构、实验结果及框架改进和扩展的潜在方向。源代码和预训练权重可在以下链接获取：\url{this https URL}。 

---
# OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving 

**Title (ZH)**: OG-Gaussian: 基于 occupancy 的街道高斯模型在自动驾驶中的应用 

**Authors**: Yedong Shen, Xinran Zhang, Yifan Duan, Shiqi Zhang, Heng Li, Yilong Wu, Jianmin Ji, Yanyong Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.14235)  

**Abstract**: Accurate and realistic 3D scene reconstruction enables the lifelike creation of autonomous driving simulation environments. With advancements in 3D Gaussian Splatting (3DGS), previous studies have applied it to reconstruct complex dynamic driving scenes. These methods typically require expensive LiDAR sensors and pre-annotated datasets of dynamic objects. To address these challenges, we propose OG-Gaussian, a novel approach that replaces LiDAR point clouds with Occupancy Grids (OGs) generated from surround-view camera images using Occupancy Prediction Network (ONet). Our method leverages the semantic information in OGs to separate dynamic vehicles from static street background, converting these grids into two distinct sets of initial point clouds for reconstructing both static and dynamic objects. Additionally, we estimate the trajectories and poses of dynamic objects through a learning-based approach, eliminating the need for complex manual annotations. Experiments on Waymo Open dataset demonstrate that OG-Gaussian is on par with the current state-of-the-art in terms of reconstruction quality and rendering speed, achieving an average PSNR of 35.13 and a rendering speed of 143 FPS, while significantly reducing computational costs and economic overhead. 

**Abstract (ZH)**: 准确且逼真的3D场景重建使自主驾驶仿真环境的栩栩如生创建成为可能。借助3D高斯点绘制技术的发展，先前的研究将其应用于重建复杂的动态驾驶场景。这些方法通常需要昂贵的激光雷达传感器和动态对象的预先标注数据集。为应对这些挑战，我们提出OG-Gaussian这一新型方法，该方法用Occupancy Grids（占用格网）替换激光雷达点云，占用格网通过Occupancy Prediction Network（占用预测网络）从环视相机图像中生成。我们的方法利用占用格网中的语义信息将动态车辆与静态街道背景区分开来，将这些格网转换为两个独立的初始点云集，用于重建静态和动态对象。此外，我们通过基于学习的方法估计动态对象的轨迹和姿态，从而消除复杂的手动标注需求。在Waymo Open数据集上的实验表明，OG-Gaussian在重建质量和渲染速度方面与当前最先进的技术相当，平均PSNR为35.13，渲染速率为143 FPS，同时显著降低计算成本和经济开销。 

---
# PitVQA++: Vector Matrix-Low-Rank Adaptation for Open-Ended Visual Question Answering in Pituitary Surgery 

**Title (ZH)**: PitVQA++: 垂体手术中开放性视觉问答的向量矩阵低秩适应 

**Authors**: Runlong He, Danyal Z. Khan, Evangelos B. Mazomenos, Hani J. Marcus, Danail Stoyanov, Matthew J. Clarkson, Mobarakol Islam  

**Link**: [PDF](https://arxiv.org/pdf/2502.14149)  

**Abstract**: Vision-Language Models (VLMs) in visual question answering (VQA) offer a unique opportunity to enhance intra-operative decision-making, promote intuitive interactions, and significantly advancing surgical education. However, the development of VLMs for surgical VQA is challenging due to limited datasets and the risk of overfitting and catastrophic forgetting during full fine-tuning of pretrained weights. While parameter-efficient techniques like Low-Rank Adaptation (LoRA) and Matrix of Rank Adaptation (MoRA) address adaptation challenges, their uniform parameter distribution overlooks the feature hierarchy in deep networks, where earlier layers, that learn general features, require more parameters than later ones. This work introduces PitVQA++ with an open-ended PitVQA dataset and vector matrix-low-rank adaptation (Vector-MoLoRA), an innovative VLM fine-tuning approach for adapting GPT-2 to pituitary surgery. Open-Ended PitVQA comprises around 101,803 frames from 25 procedural videos with 745,972 question-answer sentence pairs, covering key surgical elements such as phase and step recognition, context understanding, tool detection, localization, and interactions recognition. Vector-MoLoRA incorporates the principles of LoRA and MoRA to develop a matrix-low-rank adaptation strategy that employs vector ranking to allocate more parameters to earlier layers, gradually reducing them in the later layers. Our approach, validated on the Open-Ended PitVQA and EndoVis18-VQA datasets, effectively mitigates catastrophic forgetting while significantly enhancing performance over recent baselines. Furthermore, our risk-coverage analysis highlights its enhanced reliability and trustworthiness in handling uncertain predictions. Our source code and dataset is available at~\url{this https URL}. 

**Abstract (ZH)**: 视觉语言模型（VLMs）在手术视觉问答（Surgical VQA）中的应用为优化术中决策、促进直观交互以及显著推进 surgical 教育提供了独特的机会。然而，由于数据集有限以及全量微调预训练权重时存在过拟合和灾难性遗忘的风险，开发适用于手术 VQA 的 VLMs 具有挑战性。尽管参数高效的技术，如 Low-Rank Adaptation (LoRA) 和 Matrix of Rank Adaptation (MoRA) 可以解决适应性挑战，但它们均匀的参数分布忽略了深度网络中的特征层次结构，导致早期层（学习一般特征）所需的参数多于后期层。本文介绍了带有开放性 PitVQA 数据集的 PitVQA++ 及 Vector-MoLoRA，Vector-MoLoRA 是一种创新的 VLM 微调方法，用于将 GPT-2 调整为垂体手术。开放性 PitVQA 包含约 101,803 帧，来自 25 个手术视频，共有 745,972 个问题-答案句子对，涵盖了关键的手术要素如阶段和步骤识别、上下文理解、工具检测、定位和交互识别。Vector-MoLoRA 结合了 LoRA 和 MoRA 的原理，开发了一种基于向量排名的矩阵低秩适应策略，以更多地分配参数到早期层，并逐渐减少后期层的参数。该方法在开放性 PitVQA 和 EndoVis18-VQA 数据集上的验证表明，它有效缓解了灾难性遗忘，同时显著增强了相较于最近基线的性能。此外，我们的风险覆盖分析突显了其处理不确定预测的增强可靠性和可信度。我们的源代码和数据集可在 \url{此链接} 获取。 

---
# A Racing Dataset and Baseline Model for Track Detection in Autonomous Racing 

**Title (ZH)**: 一种用于自主赛车道检测的数据集和基准模型 

**Authors**: Shreya Ghosh, Yi-Huan Chen, Ching-Hsiang Huang, Abu Shafin Mohammad Mahdee Jameel, Chien Chou Ho, Aly El Gamal, Samuel Labi  

**Link**: [PDF](https://arxiv.org/pdf/2502.14068)  

**Abstract**: A significant challenge in racing-related research is the lack of publicly available datasets containing raw images with corresponding annotations for the downstream task. In this paper, we introduce RoRaTrack, a novel dataset that contains annotated multi-camera image data from racing scenarios for track detection. The data is collected on a Dallara AV-21 at a racing circuit in Indiana, in collaboration with the Indy Autonomous Challenge (IAC). RoRaTrack addresses common problems such as blurriness due to high speed, color inversion from the camera, and absence of lane markings on the track. Consequently, we propose RaceGAN, a baseline model based on a Generative Adversarial Network (GAN) that effectively addresses these challenges. The proposed model demonstrates superior performance compared to current state-of-the-art machine learning models in track detection. The dataset and code for this work are available at this http URL. 

**Abstract (ZH)**: 赛车相关研究中的一项重要挑战是缺乏包含对应标注的原始图像的公开数据集，用于下游任务。本文介绍了RoRaTrack，这是一种新颖的数据集，包含来自赛车场景的多摄像头图像标注数据，用于赛道检测。数据在印第安纳州的一个赛车场的Dallara AV-21上收集，与印第安纳自主挑战赛(IAC)合作收集。RoRaTrack解决了由于高速引起的模糊性、摄像头导致的颜色反转以及赛道上缺少车道标记等问题。因此，我们提出了基于生成对抗网络(GAN)的基准模型RaceGAN，有效解决了这些问题。所提出的模型在赛道检测方面表现出色，超过了当前最先进的机器学习模型。该项目的数据集和代码可在以下网址获得。 

---
# Triad: Vision Foundation Model for 3D Magnetic Resonance Imaging 

**Title (ZH)**: Triad: 视觉基础模型用于3D磁共振成像 

**Authors**: Shansong Wang, Mojtaba Safari, Qiang Li, Chih-Wei Chang, Richard LJ Qiu, Justin Roper, David S. Yu, Xiaofeng Yang  

**Link**: [PDF](https://arxiv.org/pdf/2502.14064)  

**Abstract**: Vision foundation models (VFMs) are pre-trained on extensive image datasets to learn general representations for diverse types of data. These models can subsequently be fine-tuned for specific downstream tasks, significantly boosting performance across a broad range of applications. However, existing vision foundation models that claim to be applicable to various radiology tasks are mostly pre-trained on 3D computed tomography (CT), which benefits from the availability of extensive 3D CT databases. Significant differences between CT and magnetic resonance imaging (MRI) in imaging principles, signal characteristics, and data distribution may hinder their practical performance and versatility in MRI-specific applications. Here, we propose Triad, a vision foundation model for 3D MRI. Triad adopts a widely used autoencoder architecture to learn robust representations from 131,170 3D MRI volumes and uses organ-independent imaging descriptions to constrain the semantic distribution of the visual modality. The above pre-training dataset is called Triad-131K, which is currently the largest 3D MRI pre-training dataset. We evaluate Triad across three tasks, namely, organ/tumor segmentation, organ/cancer classification, and medical image registration, in two data modalities (within-domain and out-of-domain) settings using 25 downstream datasets. By initializing models with Triad's pre-trained weights, nnUNet-Triad improves segmentation performance by 6.88% compared to nnUNet-Scratch across 17 datasets. Swin-B-Triad achieves a 3.97% improvement over Swin-B-Scratch in classification tasks across five datasets. SwinUNETR-Triad improves by 4.00% compared to SwinUNETR-Scratch in registration tasks across two datasets. Our study demonstrates that pre-training can maximize performance when the data modalities and organs of upstream and downstream tasks are consistent. 

**Abstract (ZH)**: 一种用于3D MRI的视觉基础模型：Triad 

---
# EfficientPose 6D: Scalable and Efficient 6D Object Pose Estimation 

**Title (ZH)**: EfficientPose 6D: 可扩展且高效的6D物体姿态估计 

**Authors**: Zixuan Fang, Thomas Pöllabauer, Tristan Wirth, Sarah Berkei, Volker Knauthe, Arjan Kuijper  

**Link**: [PDF](https://arxiv.org/pdf/2502.14061)  

**Abstract**: In industrial applications requiring real-time feedback, such as quality control and robotic manipulation, the demand for high-speed and accurate pose estimation remains critical. Despite advances improving speed and accuracy in pose estimation, finding a balance between computational efficiency and accuracy poses significant challenges in dynamic environments. Most current algorithms lack scalability in estimation time, especially for diverse datasets, and the state-of-the-art (SOTA) methods are often too slow. This study focuses on developing a fast and scalable set of pose estimators based on GDRNPP to meet or exceed current benchmarks in accuracy and robustness, particularly addressing the efficiency-accuracy trade-off essential in real-time scenarios. We propose the AMIS algorithm to tailor the utilized model according to an application-specific trade-off between inference time and accuracy. We further show the effectiveness of the AMIS-based model choice on four prominent benchmark datasets (LM-O, YCB-V, T-LESS, and ITODD). 

**Abstract (ZH)**: 在需要实时反馈的工业应用中，如质量控制和机器人操作，对高速高精度姿态估计的需求仍然至关重要。尽管在姿态估计的速度和准确性方面取得了一定进展，但在动态环境中实现计算效率和准确性之间的平衡仍然面临重大挑战。现有算法在估计时间上的可扩展性普遍不足，尤其是在处理多样化的数据集时，最先进的方法往往速度过慢。本研究旨在基于GDRNPP开发快速且可扩展的姿态估计器，以达到或超越当前的准确性和鲁棒性基准，特别是在实时场景中解决效率-准确性权衡问题。我们提出了AMIS算法，以根据特定应用之间的推理时间和准确性的权衡来调整所使用的模型。我们进一步在四个典型的基准数据集（LM-O、YCB-V、T-LESS和ITODD）上展示了基于AMIS的模型选择的有效性。 

---
# Appeal prediction for AI up-scaled Images 

**Title (ZH)**: AI增强图像的吸引力预测 

**Authors**: Steve Göring, Rasmus Merten, Alexander Raake  

**Link**: [PDF](https://arxiv.org/pdf/2502.14013)  

**Abstract**: DNN- or AI-based up-scaling algorithms are gaining in popularity due to the improvements in machine learning. Various up-scaling models using CNNs, GANs or mixed approaches have been published. The majority of models are evaluated using PSRN and SSIM or only a few example images. However, a performance evaluation with a wide range of real-world images and subjective evaluation is missing, which we tackle in the following paper. For this reason, we describe our developed dataset, which uses 136 base images and five different up-scaling methods, namely Real-ESRGAN, BSRGAN, waifu2x, KXNet, and Lanczos. Overall the dataset consists of 1496 annotated images. The labeling of our dataset focused on image appeal and has been performed using crowd-sourcing employing our open-source tool AVRate Voyager. We evaluate the appeal of the different methods, and the results indicate that Real-ESRGAN and BSRGAN are the best. Furthermore, we train a DNN to detect which up-scaling method has been used, the trained models have a good overall performance in our evaluation. In addition to this, we evaluate state-of-the-art image appeal and quality models, here none of the models showed a high prediction performance, therefore we also trained two own approaches. The first uses transfer learning and has the best performance, and the second model uses signal-based features and a random forest model with good overall performance. We share the data and implementation to allow further research in the context of open science. 

**Abstract (ZH)**: 基于DNN或AI的比例缩放算法得益于机器学习的进步而日益流行。使用CNN、GAN或混合方法的各种比例缩放模型已被发表。大多数模型仅使用PSNR和SSIM进行评估，或仅使用少数几个示例图像。然而，缺乏针对广泛真实世界图像和主观评价的性能评估，这是我们在本文中解决的问题。为此，我们描述了所开发的数据集，该数据集使用136张基础图像和五种不同的缩放方法，即Real-ESRGAN、BSRGAN、waifu2x、KXNet和Lanczos。整体而言，数据集包含1496张标注图像。我们数据集的标签聚焦于图像吸引力，并使用众包方法通过我们的开源工具AVRate Voyager进行标注。我们评估了不同方法的吸引力，结果表明Real-ESRGAN和BSRGAN表现最佳。此外，我们训练了一个DNN以识别已使用的缩放方法，训练模型在评估中表现出良好的整体性能。此外，我们还评估了最新图像吸引力和质量模型，其中没有任何模型表现出高的预测性能，因此我们还训练了两种自有方法。第一种方法使用迁移学习，性能最佳，第二种模型使用基于信号的特征和随机森林模型，整体性能良好。我们分享了数据和实现，以便在开放科学的背景下进行进一步研究。 

---
# A Baseline Method for Removing Invisible Image Watermarks using Deep Image Prior 

**Title (ZH)**: 基于深度图像先验的隐形图像水印去除基线方法 

**Authors**: Hengyue Liang, Taihui Li, Ju Sun  

**Link**: [PDF](https://arxiv.org/pdf/2502.13998)  

**Abstract**: Image watermarks have been considered a promising technique to help detect AI-generated content, which can be used to protect copyright or prevent fake image abuse. In this work, we present a black-box method for removing invisible image watermarks, without the need of any dataset of watermarked images or any knowledge about the watermark system. Our approach is simple to implement: given a single watermarked image, we regress it by deep image prior (DIP). We show that from the intermediate steps of DIP one can reliably find an evasion image that can remove invisible watermarks while preserving high image quality. Due to its unique working mechanism and practical effectiveness, we advocate including DIP as a baseline invasion method for benchmarking the robustness of watermarking systems. Finally, by showing the limited ability of DIP and other existing black-box methods in evading training-based visible watermarks, we discuss the positive implications on the practical use of training-based visible watermarks to prevent misinformation abuse. 

**Abstract (ZH)**: 一种黑盒方法用于去除不可见图像水印 

---
# Generative Detail Enhancement for Physically Based Materials 

**Title (ZH)**: 基于物理的材料的生成细节增强 

**Authors**: Saeed Hadadan, Benedikt Bitterli, Tizian Zeltner, Jan Novák, Fabrice Rousselle, Jacob Munkberg, Jon Hasselgren, Bartlomiej Wronski, Matthias Zwicker  

**Link**: [PDF](https://arxiv.org/pdf/2502.13994)  

**Abstract**: We present a tool for enhancing the detail of physically based materials using an off-the-shelf diffusion model and inverse rendering. Our goal is to enhance the visual fidelity of materials with detail that is often tedious to author, by adding signs of wear, aging, weathering, etc. As these appearance details are often rooted in real-world processes, we leverage a generative image model trained on a large dataset of natural images with corresponding visuals in context. Starting with a given geometry, UV mapping, and basic appearance, we render multiple views of the object. We use these views, together with an appearance-defining text prompt, to condition a diffusion model. The details it generates are then backpropagated from the enhanced images to the material parameters via inverse differentiable rendering. For inverse rendering to be successful, the generated appearance has to be consistent across all the images. We propose two priors to address the multi-view consistency of the diffusion model. First, we ensure that the initial noise that seeds the diffusion process is itself consistent across views by integrating it from a view-independent UV space. Second, we enforce geometric consistency by biasing the attention mechanism via a projective constraint so that pixels attend strongly to their corresponding pixel locations in other views. Our approach does not require any training or finetuning of the diffusion model, is agnostic of the material model used, and the enhanced material properties, i.e., 2D PBR textures, can be further edited by artists. 

**Abstract (ZH)**: 一种使用现成扩散模型和逆渲染增强物理基于材料细节的工具 

---
