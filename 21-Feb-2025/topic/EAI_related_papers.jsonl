{'arxiv_id': 'arXiv:2502.14814', 'title': 'VB-Com: Learning Vision-Blind Composite Humanoid Locomotion Against Deficient Perception', 'authors': 'Junli Ren, Tao Huang, Huayi Wang, Zirui Wang, Qingwei Ben, Jiangmiao Pang, Ping Luo', 'link': 'https://arxiv.org/abs/2502.14814', 'abstract': 'The performance of legged locomotion is closely tied to the accuracy and comprehensiveness of state observations. Blind policies, which rely solely on proprioception, are considered highly robust due to the reliability of proprioceptive observations. However, these policies significantly limit locomotion speed and often require collisions with the terrain to adapt. In contrast, Vision policies allows the robot to plan motions in advance and respond proactively to unstructured terrains with an online perception module. However, perception is often compromised by noisy real-world environments, potential sensor failures, and the limitations of current simulations in presenting dynamic or deformable terrains. Humanoid robots, with high degrees of freedom and inherently unstable morphology, are particularly susceptible to misguidance from deficient perception, which can result in falls or termination on challenging dynamic terrains. To leverage the advantages of both vision and blind policies, we propose VB-Com, a composite framework that enables humanoid robots to determine when to rely on the vision policy and when to switch to the blind policy under perceptual deficiency. We demonstrate that VB-Com effectively enables humanoid robots to traverse challenging terrains and obstacles despite perception deficiencies caused by dynamic terrains or perceptual noise.', 'abstract_zh': '基于视觉和盲政策的复合框架（VB-Com）：在感知不足时使类人机器人有效穿越复杂地形和障碍', 'title_zh': '视觉盲复合 humanoid 行走学习: 对抗感知不足的学习方法'}
{'arxiv_id': 'arXiv:2502.14795', 'title': 'Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration', 'authors': 'Pengxiang Ding, Jianfei Ma, Xinyang Tong, Binghong Zou, Xinxin Luo, Yiguo Fan, Ting Wang, Hongchao Lu, Panzhong Mo, Jinxin Liu, Yuefan Wang, Huaicheng Zhou, Wenshuo Feng, Jiacheng Liu, Siteng Huang, Donglin Wang', 'link': 'https://arxiv.org/abs/2502.14795', 'abstract': 'This paper addresses the limitations of current humanoid robot control frameworks, which primarily rely on reactive mechanisms and lack autonomous interaction capabilities due to data scarcity. We propose Humanoid-VLA, a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control. Humanoid-VLA begins with language-motion pre-alignment using non-egocentric human motion datasets paired with textual descriptions, allowing the model to learn universal motion patterns and action semantics. We then incorporate egocentric visual context through a parameter efficient video-conditioned fine-tuning, enabling context-aware motion generation. Furthermore, we introduce a self-supervised data augmentation strategy that automatically generates pseudoannotations directly derived from motion data. This process converts raw motion sequences into informative question-answer pairs, facilitating the effective use of large-scale unlabeled video data. Built upon whole-body control architectures, extensive experiments show that Humanoid-VLA achieves object interaction and environment exploration tasks with enhanced contextual awareness, demonstrating a more human-like capacity for adaptive and intelligent engagement.', 'abstract_zh': '本文针对当前类人机器人控制框架主要依赖于反应机制且因数据稀缺而缺乏自主交互能力的局限性，提出了一种新型框架Humanoid-VLA，该框架整合了语言理解、第一人称场景感知和运动控制，实现通用类人控制。Humanoid-VLA 通过使用非第一人称的人类运动数据集与文本描述配对，进行语言-运动前期对齐，从而使模型学习到通用的运动模式和动作语义。我们随后通过参数高效的视频条件微调引入第一人称视觉上下文，使运动生成具备上下文感知能力。此外，我们引入了一种自监督数据增强策略，自动从运动数据中生成伪标注，将原始运动序列转换为信息丰富的问答对，便于有效利用大规模的未标注视频数据。基于全身控制架构，广泛实验表明，Humanoid-VLA 在对象交互和环境探索任务中表现出增强的上下文感知能力，展示出更强的适应性和智能交互能力。', 'title_zh': '类人视觉集成控制：通往通用类人控制的途径'}
{'arxiv_id': 'arXiv:2502.14457', 'title': 'Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control', 'authors': 'Tan-Dzung Do, Nandiraju Gireesh, Jilong Wang, He Wang', 'link': 'https://arxiv.org/abs/2502.14457', 'abstract': 'Articulated object manipulation poses a unique challenge compared to rigid object manipulation as the object itself represents a dynamic environment. In this work, we present a novel RL-based pipeline equipped with variable impedance control and motion adaptation leveraging observation history for generalizable articulated object manipulation, focusing on smooth and dexterous motion during zero-shot sim-to-real transfer. To mitigate the sim-to-real gap, our pipeline diminishes reliance on vision by not leveraging the vision data feature (RGBD/pointcloud) directly as policy input but rather extracting useful low-dimensional data first via off-the-shelf modules. Additionally, we experience less sim-to-real gap by inferring object motion and its intrinsic properties via observation history as well as utilizing impedance control both in the simulation and in the real world. Furthermore, we develop a well-designed training setting with great randomization and a specialized reward system (task-aware and motion-aware) that enables multi-staged, end-to-end manipulation without heuristic motion planning. To the best of our knowledge, our policy is the first to report 84\\% success rate in the real world via extensive experiments with various unseen objects.', 'abstract_zh': '具身物体操作相较于刚体物体操作提出了独特的挑战，因为物体本身代表了一个动态环境。本文提出一种新颖的基于强化学习的流水线，该流水线结合了可变阻抗控制和基于观测历史的运动适应，以实现通用的具身物体操作，重点关注零样本模拟到现实转移中的平滑灵巧运动。为了缓解模拟与现实之间的差距，该流水线通过不直接将视觉数据特征（RGBD/点云）作为策略输入，而是首先通过现成模块提取有用的低维数据来减少对视觉数据的依赖。此外，通过利用观测历史推断物体运动及其固有特性，并在模拟和现实世界中均使用阻抗控制，我们体验到了较小的模拟到现实的差距。进一步地，我们设计了一个包含大量随机化的训练设置，以及一个专门的奖励系统（任务感知和运动感知），这使得多阶段、端到端的操作成为可能，且无需启发式运动规划。据我们所知，我们的策略是首个通过各种未见过的物体的广泛实验报告在真实世界中取得84%成功率的策略。', 'title_zh': '减少观看，增强感受：基于运动适应和阻抗控制的通用articulated对象操作的Sim-to-Real RL研究'}
{'arxiv_id': 'arXiv:2502.14420', 'title': 'ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model', 'authors': 'Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, Feifei Feng', 'link': 'https://arxiv.org/abs/2502.14420', 'abstract': "Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly. To overcome these limitations, we propose ChatVLA, a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference. ChatVLA demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action (VLA) methods on multimodal understanding benchmarks. Notably, it achieves a six times higher performance on MMMU and scores 47.2% on MMStar with a more parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates superior performance on 25 real-world robot manipulation tasks compared to existing VLA methods like OpenVLA. Our findings highlight the potential of our unified framework for achieving both robust multimodal understanding and effective robot control.", 'abstract_zh': '人类拥有感知、理解和与物理世界交互的统一认知能力。为何大型语言模型无法复制这种整体理解能力？通过对视觉-语言-动作模型（VLA）现有训练范式的系统分析，我们识别出两个关键挑战：伪遗忘，即机器人训练会覆盖关键的视觉-文本对齐；任务干扰，竞争控制和理解任务在联合训练时会降低性能。为克服这些限制，我们提出了ChatVLA，这是一种新颖的框架，具备分阶段对齐训练，该框架在初始控制掌握后逐步整合多模态数据，以及混合专家架构以最小化任务干扰。ChatVLA在视觉问答数据集上表现出竞争性性能，并在多模态理解基准上显著超越最先进的视觉-语言-动作（VLA）方法。值得注意的是，它在MMMU上的性能提高了六倍，并在MMStar上的得分为47.2%，其参数效率设计优于ECoT。此外，ChatVLA在25个实际机器人操作任务上的表现优于现有的VLA方法如OpenVLA。我们的发现强调了我们统一框架在实现稳健的多模态理解与有效的机器人控制方面的潜力。', 'title_zh': 'ChatVLA：统一的多模态理解和机器人控制的视觉-语言-动作模型'}
{'arxiv_id': 'arXiv:2502.14254', 'title': 'Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation', 'authors': 'Lingfeng Zhang, Yuecheng Liu, Zhanguang Zhang, Matin Aghaei, Yaochen Hu, Hongjian Gu, Mohammad Ali Alomrani, David Gamaliel Arcos Bravo, Raika Karimi, Atia Hamidizadeh, Haoping Xu, Guowei Huang, Zhanpeng Zhang, Tongtong Cao, Weichao Qiu, Xingyue Quan, Jianye Hao, Yuzheng Zhuang, Yingxue Zhang', 'link': 'https://arxiv.org/abs/2502.14254', 'abstract': "Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have made them powerful tools in embodied navigation, enabling agents to leverage commonsense and spatial reasoning for efficient exploration in unfamiliar environments. Existing LLM-based approaches convert global memory, such as semantic or topological maps, into language descriptions to guide navigation. While this improves efficiency and reduces redundant exploration, the loss of geometric information in language-based representations hinders spatial reasoning, especially in intricate environments. To address this, VLM-based approaches directly process ego-centric visual inputs to select optimal directions for exploration. However, relying solely on a first-person perspective makes navigation a partially observed decision-making problem, leading to suboptimal decisions in complex environments. In this paper, we present a novel vision-language model (VLM)-based navigation framework that addresses these challenges by adaptively retrieving task-relevant cues from a global memory module and integrating them with the agent's egocentric observations. By dynamically aligning global contextual information with local perception, our approach enhances spatial reasoning and decision-making in long-horizon tasks. Experimental results demonstrate that the proposed method surpasses previous state-of-the-art approaches in object navigation tasks, providing a more effective and scalable solution for embodied navigation.", 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）和视觉-语言模型（VLMs）使其成为有代理能力导航的强大工具，使代理能够利用常识和空间推理在陌生环境中进行高效的探索。现有的基于LLM的方法将全局记忆，如语义或拓扑地图，转换为语言描述以指导导航。虽然这提高了效率并减少了冗余探索，但基于语言的表示形式中几何信息的损失阻碍了空间推理，特别是在复杂的环境中。为了解决这个问题，基于VLM的方法直接处理自中心视觉输入，以选择最佳的探索方向。然而，仅依赖第一人称视角使得导航成为一个部分可观测的决策问题，在复杂环境中导致次优决策。在本文中，我们提出了一种新的基于VLM的导航框架，通过适应性地从全局记忆模块中检索与任务相关的线索，并将其与代理的自中心观察相结合，以解决这些挑战。通过动态对齐全局上下文信息与局部感知，我们的方法增强了长时间任务中的空间推理和决策。实验结果表明，所提出的方法在物体导航任务中超越了先前的最佳方法，为有代理能力的导航提供了更有效和可扩展的解决方案。', 'title_zh': 'Mem2Ego: 为长时_horizon 体态导航增强视觉-语言模型的全局到 ego 内存能力'}
{'arxiv_id': 'arXiv:2502.14140', 'title': 'ModSkill: Physical Character Skill Modularization', 'authors': 'Yiming Huang, Zhiyang Dou, Lingjie Liu', 'link': 'https://arxiv.org/abs/2502.14140', 'abstract': 'Human motion is highly diverse and dynamic, posing challenges for imitation learning algorithms that aim to generalize motor skills for controlling simulated characters. Previous methods typically rely on a universal full-body controller for tracking reference motion (tracking-based model) or a unified full-body skill embedding space (skill embedding). However, these approaches often struggle to generalize and scale to larger motion datasets. In this work, we introduce a novel skill learning framework, ModSkill, that decouples complex full-body skills into compositional, modular skills for independent body parts. Our framework features a skill modularization attention layer that processes policy observations into modular skill embeddings that guide low-level controllers for each body part. We also propose an Active Skill Learning approach with Generative Adaptive Sampling, using large motion generation models to adaptively enhance policy learning in challenging tracking scenarios. Our results show that this modularized skill learning framework, enhanced by generative sampling, outperforms existing methods in precise full-body motion tracking and enables reusable skill embeddings for diverse goal-driven tasks.', 'abstract_zh': '基于模块化技能的学习框架在精确全身运动跟踪中的表现及其应用', 'title_zh': 'ModSkill: 物理特性技能模块化'}
{'arxiv_id': 'arXiv:2502.14777', 'title': 'Making Universal Policies Universal', 'authors': 'Niklas Höpner, David Kuric, Herke van Hoof', 'link': 'https://arxiv.org/abs/2502.14777', 'abstract': "The development of a generalist agent capable of solving a wide range of sequential decision-making tasks remains a significant challenge. We address this problem in a cross-agent setup where agents share the same observation space but differ in their action spaces. Our approach builds on the universal policy framework, which decouples policy learning into two stages: a diffusion-based planner that generates observation sequences and an inverse dynamics model that assigns actions to these plans. We propose a method for training the planner on a joint dataset composed of trajectories from all agents. This method offers the benefit of positive transfer by pooling data from different agents, while the primary challenge lies in adapting shared plans to each agent's unique constraints. We evaluate our approach on the BabyAI environment, covering tasks of varying complexity, and demonstrate positive transfer across agents. Additionally, we examine the planner's generalisation ability to unseen agents and compare our method to traditional imitation learning approaches. By training on a pooled dataset from multiple agents, our universal policy achieves an improvement of up to $42.20\\%$ in task completion accuracy compared to a policy trained on a dataset from a single agent.", 'abstract_zh': '一种通用代理的发展，能够在广泛范围内的序列决策任务上求解仍然是一个重大挑战。我们通过跨代理设置解决这一问题，在该设置中，代理共享相同的状态观察空间，但其行动空间不同。我们的方法基于通用策略框架，将策略学习分为两个阶段：一种基于扩散的规划器生成状态观察序列，以及逆动力学模型将行动分配给这些计划。我们提出了一种方法，在一个由所有代理轨迹组成的联合数据集上训练规划器。该方法通过从不同代理中汇集数据，提供了积极迁移的优势，但主要挑战在于适应每个代理的独特约束。我们在BabyAI环境中评估了我们的方法，该环境涵盖了不同复杂度的任务，并展示了代理间的积极迁移。此外，我们还考察了规划器对未见代理的泛化能力，并将我们的方法与传统的模仿学习方法进行了比较。通过在多代理联合数据集上训练，我们的通用策略在任务完成准确性上相对于单一代理数据集训练的策略实现了最高42.20%的改进。', 'title_zh': '制作普适性政策以求普适性'}
{'arxiv_id': 'arXiv:2502.14706', 'title': 'Building reliable sim driving agents by scaling self-play', 'authors': 'Daphne Cornelisse, Aarav Pandya, Kevin Joseph, Joseph Suárez, Eugene Vinitsky', 'link': 'https://arxiv.org/abs/2502.14706', 'abstract': "Simulation agents are essential for designing and testing systems that interact with humans, such as autonomous vehicles (AVs). These agents serve various purposes, from benchmarking AV performance to stress-testing the system's limits, but all use cases share a key requirement: reliability. A simulation agent should behave as intended by the designer, minimizing unintended actions like collisions that can compromise the signal-to-noise ratio of analyses. As a foundation for reliable sim agents, we propose scaling self-play to thousands of scenarios on the Waymo Open Motion Dataset under semi-realistic limits on human perception and control. Training from scratch on a single GPU, our agents nearly solve the full training set within a day. They generalize effectively to unseen test scenes, achieving a 99.8% goal completion rate with less than 0.8% combined collision and off-road incidents across 10,000 held-out scenarios. Beyond in-distribution generalization, our agents show partial robustness to out-of-distribution scenes and can be fine-tuned in minutes to reach near-perfect performance in those cases. Demonstrations of agent behaviors can be found at this link. We open-source both the pre-trained agents and the complete code base. Demonstrations of agent behaviors can be found at \\url{this https URL}.", 'abstract_zh': '仿真代理对于设计和测试与人类互动的系统（如自动驾驶车辆）至关重要。这些代理具有多种用途，从评估自动驾驶车辆性能到测试系统的极限，但所有应用场景都共享一个关键要求：可靠性。为了构建可靠的仿真代理，我们建议在Waymo开放运动数据集中，于半现实的人类感知和控制限制条件下，将自我对弈扩展到数千种场景。我们从单个GPU开始训练，代理几乎在一天内解决了整个训练集。它们有效泛化到未见过的测试场景，在10,000个保留场景中，目标完成率为99.8%，且碰撞和离路事件的总发生率不到0.8%。除了分布内泛化，我们的代理部分抵抗分布外场景，并可在几分钟内微调至这些情况下近乎完美表现。代理行为演示可在此链接中找到：\\url{this https URL}。我们开源了预训练代理和完整的代码库。代理行为演示可在此链接中找到：\\url{this https URL}。', 'title_zh': '通过扩展自我对弈构建可靠的模拟驾驶代理'}
{'arxiv_id': 'arXiv:2502.14345', 'title': 'FlowAgent: Achieving Compliance and Flexibility for Workflow Agents', 'authors': 'Yuchen Shi, Siqi Cai, Zihan Xu, Yuei Qin, Gang Li, Hang Shao, Jiawei Chen, Deqing Yang, Ke Li, Xing Sun', 'link': 'https://arxiv.org/abs/2502.14345', 'abstract': "The integration of workflows with large language models (LLMs) enables LLM-based agents to execute predefined procedures, enhancing automation in real-world applications. Traditional rule-based methods tend to limit the inherent flexibility of LLMs, as their predefined execution paths restrict the models' action space, particularly when the unexpected, out-of-workflow (OOW) queries are encountered. Conversely, prompt-based methods allow LLMs to fully control the flow, which can lead to diminished enforcement of procedural compliance. To address these challenges, we introduce FlowAgent, a novel agent framework designed to maintain both compliance and flexibility. We propose the Procedure Description Language (PDL), which combines the adaptability of natural language with the precision of code to formulate workflows. Building on PDL, we develop a comprehensive framework that empowers LLMs to manage OOW queries effectively, while keeping the execution path under the supervision of a set of controllers. Additionally, we present a new evaluation methodology to rigorously assess an LLM agent's ability to handle OOW scenarios, going beyond routine flow compliance tested in existing benchmarks. Experiments on three datasets demonstrate that FlowAgent not only adheres to workflows but also effectively manages OOW queries, highlighting its dual strengths in compliance and flexibility. The code is available at this https URL.", 'abstract_zh': '大语言模型与工作流的集成使基于大语言模型的代理能够执行预定义的程序，从而增强实际应用中的自动化。传统的基于规则的方法往往会限制大语言模型的固有灵活性，因为它们预定义的执行路径限制了模型的行动空间，尤其是在遇到工作流外（OOW）查询时。相反，基于提示的方法可以让大语言模型全面控制流程，可能导致程序合规性限制减弱。为了解决这些挑战，我们提出了FlowAgent，这是一种新型的代理框架，旨在同时保持合规性和灵活性。我们提出了过程描述语言（PDL），它结合了自然语言的灵活性和代码的精确性来定义工作流。基于PDL，我们开发了一个全面的框架，使大语言模型能够有效管理工作流外查询，同时在一组控制器的监督下保持执行路径。此外，我们提出了一种新的评估方法，以严格评估大语言模型代理处理工作流外场景的能力，超越现有基准测试中常规流程合规性的评估。在三个数据集上的实验表明，FlowAgent不仅遵循工作流，还有效地管理了工作流外查询，突显了其在合规性和灵活性方面的双重优势。代码可在以下链接获取：this https URL。', 'title_zh': 'FlowAgent: 实现工作流代理的合规性和灵活性'}
{'arxiv_id': 'arXiv:2502.14455', 'title': 'An Efficient Ground-aerial Transportation System for Pest Control Enabled by AI-based Autonomous Nano-UAVs', 'authors': 'Luca Crupi, Luca Butera, Alberto Ferrante, Alessandro Giusti, Daniele Palossi', 'link': 'https://arxiv.org/abs/2502.14455', 'abstract': 'Efficient crop production requires early detection of pest outbreaks and timely treatments; we consider a solution based on a fleet of multiple autonomous miniaturized unmanned aerial vehicles (nano-UAVs) to visually detect pests and a single slower heavy vehicle that visits the detected outbreaks to deliver treatments. To cope with the extreme limitations aboard nano-UAVs, e.g., low-resolution sensors and sub-100 mW computational power budget, we design, fine-tune, and optimize a tiny image-based convolutional neural network (CNN) for pest detection. Despite the small size of our CNN (i.e., 0.58 GOps/inference), on our dataset, it scores a mean average precision (mAP) of 0.79 in detecting harmful bugs, i.e., 14% lower mAP but 32x fewer operations than the best-performing CNN in the literature. Our CNN runs in real-time at 6.8 frame/s, requiring 33 mW on a GWT GAP9 System-on-Chip aboard a Crazyflie nano-UAV. Then, to cope with in-field unexpected obstacles, we leverage a global+local path planner based on the A* algorithm. The global path planner determines the best route for the nano-UAV to sweep the entire area, while the local one runs up to 50 Hz aboard our nano-UAV and prevents collision by adjusting the short-distance path. Finally, we demonstrate with in-simulator experiments that once a 25 nano-UAVs fleet has combed a 200x200 m vineyard, collected information can be used to plan the best path for the tractor, visiting all and only required hotspots. In this scenario, our efficient transportation system, compared to a traditional single-ground vehicle performing both inspection and treatment, can save up to 20 h working time.', 'abstract_zh': '基于纳米无人机的高效作物生产：早期害虫爆发检测与及时治疗的解决方案', 'title_zh': '基于AI自主纳米无人机的高效地面-空中害虫防控交通系统'}
{'arxiv_id': 'arXiv:2502.14019', 'title': 'Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems', 'authors': 'Myra Cheng, Su Lin Blodgett, Alicia DeVrio, Lisa Egede, Alexandra Olteanu', 'link': 'https://arxiv.org/abs/2502.14019', 'abstract': "As text generation systems' outputs are increasingly anthropomorphic -- perceived as human-like -- scholars have also raised increasing concerns about how such outputs can lead to harmful outcomes, such as users over-relying or developing emotional dependence on these systems. How to intervene on such system outputs to mitigate anthropomorphic behaviors and their attendant harmful outcomes, however, remains understudied. With this work, we aim to provide empirical and theoretical grounding for developing such interventions. To do so, we compile an inventory of interventions grounded both in prior literature and a crowdsourced study where participants edited system outputs to make them less human-like. Drawing on this inventory, we also develop a conceptual framework to help characterize the landscape of possible interventions, articulate distinctions between different types of interventions, and provide a theoretical basis for evaluating the effectiveness of different interventions.", 'abstract_zh': '随着文本生成系统生成的输出越来越具拟人性，被人感知为类似人类，学者们也对这些输出可能导致的不良后果表达了越来越多的担忧，例如用户过度依赖或产生情感依赖于这些系统。如何干预这些系统输出以减少拟人性行为及其伴随的不良后果仍研究不足。通过这项工作，我们旨在为开发此类干预措施提供实证和理论基础。为此，我们编制了一份基于先前文献和一项众包研究的干预措施清单，在这项众包研究中，参与者编辑系统输出以使其显得不那么拟人性。利用这份清单，我们还开发了一个概念框架，以帮助描述可能的干预措施 landscape、区分不同类型干预措施，并为评估不同干预措施的有效性提供理论基础。', 'title_zh': '人性化机器：减轻文本生成系统中的拟人类化行为'}
{'arxiv_id': 'arXiv:2502.14000', 'title': 'Human-Artificial Interaction in the Age of Agentic AI: A System-Theoretical Approach', 'authors': 'Uwe M. Borghoff, Paolo Bottoni, Remo Pareschi', 'link': 'https://arxiv.org/abs/2502.14000', 'abstract': 'This paper presents a novel perspective on human-computer interaction (HCI), framing it as a dynamic interplay between human and computational agents within a networked system. Going beyond traditional interface-based approaches, we emphasize the importance of coordination and communication among heterogeneous agents with different capabilities, roles, and goals. A key distinction is made between multi-agent systems (MAS) and Centaurian systems, which represent two different paradigms of human-AI collaboration. MAS maintain agent autonomy, with structured protocols enabling cooperation, while Centaurian systems deeply integrate human and AI capabilities, creating unified decision-making entities.\nTo formalize these interactions, we introduce a framework for communication spaces, structured into surface, observation, and computation layers, ensuring seamless integration between MAS and Centaurian architectures, where colored Petri nets effectively represent structured Centaurian systems and high-level reconfigurable networks address the dynamic nature of MAS.\nOur research has practical applications in autonomous robotics, human-in-the-loop decision making, and AI-driven cognitive architectures, and provides a foundation for next-generation hybrid intelligence systems that balance structured coordination with emergent behavior.', 'abstract_zh': '本文从一个新的角度探讨了人机交互（HCI），将其视为网络系统中人类代理与计算代理之间动态交互的过程。超越传统的基于界面的方法，我们强调了不同能力、角色和目标的异构代理之间的协调和通信的重要性。我们区分了多代理系统（MAS）和半人马系统（Centaurian systems），两者代表了人类与人工智能合作的两种不同范式。MAS维护代理的自主性，通过结构化的协议实现合作，而半人马系统则深度融合了人类和人工智能的能力，创建统一的决策实体。\n\n为了正式化这些交互，我们引入了一种通信空间框架，分为表面、观测和计算三层，确保了MAS和半人马系统架构之间的无缝集成，其中彩色Petri网有效地表示了结构化的半人马系统，而高级可重构网络则应对了MAS的动态性质。\n\n我们的研究成果在自主机器人、人机环决策以及基于AI的认知架构中有实际应用，并为下一代平衡结构化协调与涌现行为的混合智能系统奠定了基础。', 'title_zh': '代理人工智能时代的主-机互动：一种系统理论的方法'}
{'arxiv_id': 'arXiv:2502.13983', 'title': 'Gesture-Aware Zero-Shot Speech Recognition for Patients with Language Disorders', 'authors': 'Seungbae Kim, Daeun Lee, Brielle Stark, Jinyoung Han', 'link': 'https://arxiv.org/abs/2502.13983', 'abstract': 'Individuals with language disorders often face significant communication challenges due to their limited language processing and comprehension abilities, which also affect their interactions with voice-assisted systems that mostly rely on Automatic Speech Recognition (ASR). Despite advancements in ASR that address disfluencies, there has been little attention on integrating non-verbal communication methods, such as gestures, which individuals with language disorders substantially rely on to supplement their communication. Recognizing the need to interpret the latent meanings of visual information not captured by speech alone, we propose a gesture-aware ASR system utilizing a multimodal large language model with zero-shot learning for individuals with speech impairments. Our experiment results and analyses show that including gesture information significantly enhances semantic understanding. This study can help develop effective communication technologies, specifically designed to meet the unique needs of individuals with language impairments.', 'abstract_zh': '基于手势的自动语音识别系统：适用于语言障碍个体的多模态大语言模型研究', 'title_zh': '语言障碍患者面向手势的零样本语音识别'}
