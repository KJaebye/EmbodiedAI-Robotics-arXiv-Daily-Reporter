# Online Mapping for Autonomous Driving: Addressing Sensor Generalization and Dynamic Map Updates in Campus Environments 

**Title (ZH)**: 校园环境中的在线地图构建：应对传感器泛化和动态地图更新问题 

**Authors**: Zihan Zhang, Abhijit Ravichandran, Pragnya Korti, Luobin Wang, Henrik I. Christensen  

**Link**: [PDF](https://arxiv.org/pdf/2509.25542)  

**Abstract**: High-definition (HD) maps are essential for autonomous driving, providing precise information such as road boundaries, lane dividers, and crosswalks to enable safe and accurate navigation. However, traditional HD map generation is labor-intensive, expensive, and difficult to maintain in dynamic environments. To overcome these challenges, we present a real-world deployment of an online mapping system on a campus golf cart platform equipped with dual front cameras and a LiDAR sensor. Our work tackles three core challenges: (1) labeling a 3D HD map for campus environment; (2) integrating and generalizing the SemVecMap model onboard; and (3) incrementally generating and updating the predicted HD map to capture environmental changes. By fine-tuning with campus-specific data, our pipeline produces accurate map predictions and supports continual updates, demonstrating its practical value in real-world autonomous driving scenarios. 

**Abstract (ZH)**: 高分辨率（HD）地图对于自动驾驶至关重要，可以提供精准的道路边界、车道分隔线和人行横道等信息，以实现安全准确的导航。然而，传统的HD地图生成过程耗时、昂贵且难以在动态环境中维护。为克服这些挑战，我们在一个配备了双前摄像头和激光雷达传感器的校园高尔夫车平台上部署了一个在线制图系统。我们的工作解决了三个核心挑战：（1）为校园环境标注3D HD地图；（2）在车载上整合并泛化SemVecMap模型；（3）增量生成和更新预测的HD地图以捕捉环境变化。通过使用特定于校园的数据进行微调，我们的流程生成了准确的地图预测，并支持持续更新，证明了其在实际自动驾驶场景中的实用价值。 

---
# BEV-VLM: Trajectory Planning via Unified BEV Abstraction 

**Title (ZH)**: BEV-VLM：统一BEV抽象轨迹规划 

**Authors**: Guancheng Chen, Sheng Yang, Tong Zhan, Jian Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.25249)  

**Abstract**: This paper introduces BEV-VLM, a novel framework for trajectory planning in autonomous driving that leverages Vision-Language Models (VLMs) with Bird's-Eye View (BEV) feature maps as visual inputs. Unlike conventional approaches that rely solely on raw visual data such as camera images, our method utilizes highly compressed and informative BEV representations, which are generated by fusing multi-modal sensor data (e.g., camera and LiDAR) and aligning them with HD Maps. This unified BEV-HD Map format provides a geometrically consistent and rich scene description, enabling VLMs to perform accurate trajectory planning. Experimental results on the nuScenes dataset demonstrate 44.8% improvements in planning accuracy and complete collision avoidance. Our work highlights that VLMs can effectively interpret processed visual representations like BEV features, expanding their applicability beyond raw images in trajectory planning. 

**Abstract (ZH)**: BEV-VLM：一种基于鸟瞰图特征映射的视觉-语言模型在自主驾驶中的轨迹规划新框架 

---
# Benchmarking Egocentric Visual-Inertial SLAM at City Scale 

**Title (ZH)**: 基于城市规模的主观视角视觉-惯性SLAM基准测试 

**Authors**: Anusha Krishnan, Shaohui Liu, Paul-Edouard Sarlin, Oscar Gentilhomme, David Caruso, Maurizio Monge, Richard Newcombe, Jakob Engel, Marc Pollefeys  

**Link**: [PDF](https://arxiv.org/pdf/2509.26639)  

**Abstract**: Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard sensors is critical for wearable devices capturing egocentric data, which exhibits specific challenges, such as a wider diversity of motions and viewpoints, prevalent dynamic visual content, or long sessions affected by time-varying sensor calibration. While recent progress on SLAM has been swift, academic research is still driven by benchmarks that do not reflect these challenges or do not offer sufficiently accurate ground truth poses. In this paper, we introduce a new dataset and benchmark for visual-inertial SLAM with egocentric, multi-modal data. We record hours and kilometers of trajectories through a city center with glasses-like devices equipped with various sensors. We leverage surveying tools to obtain control points as indirect pose annotations that are metric, centimeter-accurate, and available at city scale. This makes it possible to evaluate extreme trajectories that involve walking at night or traveling in a vehicle. We show that state-of-the-art systems developed by academia are not robust to these challenges and we identify components that are responsible for this. In addition, we design tracks with different levels of difficulty to ease in-depth analysis and evaluation of less mature approaches. The dataset and benchmark are available at this https URL. 

**Abstract (ZH)**: 基于穿戴设备的六自由度同时定位与建图：从车载传感器精确捕捉第一人称多模态数据的独特挑战及新数据集与基准 

---
# Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity 

**Title (ZH)**: 受限计算环境中的显著边缘渲染与加权汉明相似性稳健视觉定位 

**Authors**: Tu-Hoa Pham, Philip Bailey, Daniel Posada, Georgios Georgakis, Jorge Enriquez, Surya Suresh, Marco Dolci, Philip Twu  

**Link**: [PDF](https://arxiv.org/pdf/2509.25520)  

**Abstract**: We consider the problem of vision-based 6-DoF object pose estimation in the context of the notional Mars Sample Return campaign, in which a robotic arm would need to localize multiple objects of interest for low-clearance pickup and insertion, under severely constrained hardware. We propose a novel localization algorithm leveraging a custom renderer together with a new template matching metric tailored to the edge domain to achieve robust pose estimation using only low-fidelity, textureless 3D models as inputs. Extensive evaluations on synthetic datasets as well as from physical testbeds on Earth and in situ Mars imagery shows that our method consistently beats the state of the art in compute and memory-constrained localization, both in terms of robustness and accuracy, in turn enabling new possibilities for cheap and reliable localization on general-purpose hardware. 

**Abstract (ZH)**: 基于视觉的6自由度物体姿态估计在设想中的火星样本返回任务中，利用定制渲染器和边缘域定制模板匹配度量实现受硬件限制条件下的鲁棒姿态估计 

---
# HilbertA: Hilbert Attention for Image Generation with Diffusion Models 

**Title (ZH)**: HilbertA: 图像生成中的一种希尔伯特注意力机制 

**Authors**: Shaoyi Zheng, Wenbo Lu, Yuxuan Xia, Haomin Liu, Shengjie Wang  

**Link**: [PDF](https://arxiv.org/pdf/2509.26538)  

**Abstract**: Designing sparse attention for diffusion transformers requires reconciling two-dimensional spatial locality with GPU efficiency, a trade-off that current methods struggle to achieve. Existing approaches enforce two-dimensional spatial locality but often incur uncoalesced memory access. We present HilbertA, a 2D-aware and GPU-efficient sparse attention mechanism. HilbertA reorders image tokens along Hilbert curves to achieve a contiguous memory layout while preserving spatial neighborhoods, and employs a sliding schedule across layers to enable long-range information propagation without repeated or uncoalesced memory access. To further enhance cross-tile communication and positional awareness, HilbertA introduces a small central shared region. Implemented in Triton, HilbertA delivers comparable image quality with significant acceleration over prior methods on Flux.1-dev, demonstrating the feasibility of hardware-aligned two-dimensional sparse attention for high-resolution image generation. HilbertA delivers attention speedups of $2.3\times$ when generating $1024\times 1024$ images, and up to $4.17\times$ at $2048\times 2048$, while achieving image quality comparable to or surpassing baselines. 

**Abstract (ZH)**: 设计稀疏注意机制需平衡二维空间局部性与GPU效率，这是当前方法难以兼顾的权衡问题。HilbertA：一种二维意识的GPU高效稀疏注意机制 

---
# Transformer Classification of Breast Lesions: The BreastDCEDL_AMBL Benchmark Dataset and 0.92 AUC Baseline 

**Title (ZH)**: 基于Transformer的乳腺肿块分类：BreastDCEDL_AMBL标准数据集及0.92 AUC基准 

**Authors**: Naomi Fridman, Anat Goldstein  

**Link**: [PDF](https://arxiv.org/pdf/2509.26440)  

**Abstract**: The error is caused by special characters that arXiv's system doesn't recognize. Here's the cleaned version with all problematic characters replaced: Breast magnetic resonance imaging is a critical tool for cancer detection and treatment planning, but its clinical utility is hindered by poor specificity, leading to high false-positive rates and unnecessary biopsies. This study introduces a transformer-based framework for automated classification of breast lesions in dynamic contrast-enhanced MRI, addressing the challenge of distinguishing benign from malignant findings. We implemented a SegFormer architecture that achieved an AUC of 0.92 for lesion-level classification, with 100% sensitivity and 67% specificity at the patient level - potentially eliminating one-third of unnecessary biopsies without missing malignancies. The model quantifies malignant pixel distribution via semantic segmentation, producing interpretable spatial predictions that support clinical decision-making. To establish reproducible benchmarks, we curated BreastDCEDL_AMBL by transforming The Cancer Imaging Archive's AMBL collection into a standardized deep learning dataset with 88 patients and 133 annotated lesions (89 benign, 44 malignant). This resource addresses a key infrastructure gap, as existing public datasets lack benign lesion annotations, limiting benign-malignant classification research. Training incorporated an expanded cohort of over 1,200 patients through integration with BreastDCEDL datasets, validating transfer learning approaches despite primary tumor-only annotations. Public release of the dataset, models, and evaluation protocols provides the first standardized benchmark for DCE-MRI lesion classification, enabling methodological advancement toward clinical deployment. 

**Abstract (ZH)**: 由arXiv系统不识别的特殊字符导致的错误。以下是清理版本，已将所有有问题的字符替换： breast磁共振成像是癌症检测和治疗计划中的关键工具，但由于其特异性差，导致假阳性率高和不必要的活检。本研究介绍了一种基于变压器的框架，用于动态对比增强MRI中乳腺病灶的自动分类，以解决良性与恶性发现区分的挑战。我们实现了一种SegFormer架构，实现了病变水平0.92的AUC值，在患者水平上达到了100%的敏感性和67%的特异性——可能消除三分之一不必要的活检，同时不遗漏恶性病灶。该模型通过语义分割量化恶性像素分布，生成可解释的空间预测，支持临床决策。为了建立可重复的基准，我们通过将The Cancer Imaging Archive的AMBL集合标准化，创建了包含88名患者和133个注释病灶（89个良性，44个恶性）的BreastDCEDL_AMBL资源，解决了现有公共数据集中缺乏良性病灶标注的瓶颈，限制了良性与恶性分类研究。通过与BreastDCEDL数据集整合增加超过1200名患者的队列，验证了迁移学习方法的有效性，尽管主要肿瘤仅标注。公开发布该数据集、模型和评估协议提供了首个标准化的DCE-MRI病灶分类基准，推动了方法学的发展，以便临床应用。 

---
# Saliency Guided Longitudinal Medical Visual Question Answering 

**Title (ZH)**: 注意力引导的 longitudal 医学视觉问答 

**Authors**: Jialin Wu, Xiaofeng Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.25374)  

**Abstract**: Longitudinal medical visual question answering (Diff-VQA) requires comparing paired studies from different time points and answering questions about clinically meaningful changes. In this setting, the difference signal and the consistency of visual focus across time are more informative than absolute single-image findings. We propose a saliency-guided encoder-decoder for chest X-ray Diff-VQA that turns post-hoc saliency into actionable supervision. The model first performs a lightweight near-identity affine pre-alignment to reduce nuisance motion between visits. It then executes a within-epoch two-step loop: step 1 extracts a medically relevant keyword from the answer and generates keyword-conditioned Grad-CAM on both images to obtain disease-focused saliency; step 2 applies the shared saliency mask to both time points and generates the final answer. This closes the language-vision loop so that the terms that matter also guide where the model looks, enforcing spatially consistent attention on corresponding anatomy. On Medical-Diff-VQA, the approach attains competitive performance on BLEU, ROUGE-L, CIDEr, and METEOR while providing intrinsic interpretability. Notably, the backbone and decoder are general-domain pretrained without radiology-specific pretraining, highlighting practicality and transferability. These results support saliency-conditioned generation with mild pre-alignment as a principled framework for longitudinal reasoning in medical VQA. 

**Abstract (ZH)**: 纵向医学视觉问答（Diff-VQA）要求比较不同时间点的配对研究并回答关于临床有意义变化的问题。在这种情况下，时间上的差异信号和视觉关注的一致性比单张图像的绝对发现更具信息量。我们提出了一种基于显著性引导的编码器-解码器模型，将事后显著性转化为可操作的监督。该模型首先执行一种轻量级的近恒同仿射预对齐，以减少访问间的多余运动。然后执行一个epoch内的两步循环：第一步从答案中提取医学相关的关键词，并在两张图像上生成条件下的Grad-CAM以获得疾病焦点的显著性；第二步将共享的显著性掩码应用于两个时间点并生成最终答案。这关闭了语言-视觉循环，使重要的术语也指导模型看的位置，加强空间上一致的注意力在相应的解剖结构上。在Medical-Diff-VQA上，该方法在BLEU、ROUGE-L、CIDEr和METEOR上取得了竞争力的性能，同时提供了内在的可解释性。值得注意的是，该模型的骨干和解码器在一般领域预训练而无需放射学特定的预训练，突显其实用性和可迁移性。这些结果支持在医学VQA中的纵向推理中需要轻微预对齐的条件显著性生成作为一种原则性框架。 

---
# Learning Generalizable Shape Completion with SIM(3) Equivariance 

**Title (ZH)**: 基于SIM(3)不变性的学习可泛化的形状补全 

**Authors**: Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2509.26631)  

**Abstract**: 3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: this https URL. 

**Abstract (ZH)**: 3D形状完成方法通常假定扫描已对齐到一个基准框架。这泄露了姿态和尺度线索，网络可能会利用这些线索来记忆绝对位置而非推断固有几何。当实际数据中缺少这种对齐时，性能会崩溃。我们主张稳健的泛化需要架构对相似性群SIM(3)的不变性，从而使模型对姿态和尺度保持无偏见。遵循这一原则，我们介绍了首个SIM(3)不变的形状完成网络，其模块化层依次规范化特征、推断相似性不变的几何关系，并恢复原始框架。在移除了隐藏线索的去偏评价协议下，我们的模型在PCN基准上优于等变性和增强基线。它还在真实驾驶和室内扫描跨域任务上设立了新记录，分别将KITTI上的最小匹配距离降低了17%和OmniObject3D上的切比雪夫距离$\ell_1$降低了14%。或许令人惊讶的是，在更严格的协议下，我们的方法即使在对手的带有偏见设置下也表现更优。这些结果确立了全SIM(3)不变性作为实现真正泛化形状完成的有效途径。 

---
# Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification 

**Title (ZH)**: 场景图上的注意力：面向CSAI分类的室内场景表示 

**Authors**: Artur Barros, Carlos Caetano, João Macedo, Jefersson A. dos Santos, Sandra Avila  

**Link**: [PDF](https://arxiv.org/pdf/2509.26457)  

**Abstract**: Indoor scene classification is a critical task in computer vision, with wide-ranging applications that go from robotics to sensitive content analysis, such as child sexual abuse imagery (CSAI) classification. The problem is particularly challenging due to the intricate relationships between objects and complex spatial layouts. In this work, we propose the Attention over Scene Graphs for Sensitive Content Analysis (ASGRA), a novel framework that operates on structured graph representations instead of raw pixels. By first converting images into Scene Graphs and then employing a Graph Attention Network for inference, ASGRA directly models the interactions between a scene's components. This approach offers two key benefits: (i) inherent explainability via object and relationship identification, and (ii) privacy preservation, enabling model training without direct access to sensitive images. On Places8, we achieve 81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI evaluation with law enforcement yields 74.27% balanced accuracy. Our results establish structured scene representations as a robust paradigm for indoor scene classification and CSAI classification. Code is publicly available at this https URL. 

**Abstract (ZH)**: 室内场景分类是计算机视觉中的一个关键任务，广泛应用于从机器人技术到敏感内容分析（如儿童性虐待图像CSAI分类）等多个领域。由于场景中对象之间的复杂关系以及复杂的空间布局，该问题极具挑战性。本文提出了一种新型框架Attention over Scene Graphs for Sensitive Content Analysis (ASGRA)，该框架基于结构化的图表示而非原始像素。通过首先将图像转换为场景图，然后使用图注意网络进行推理，ASGRA可以直接建模场景组件之间的交互关系。该方法具有两大优势：(i) 通过对象和关系识别实现固有的可解释性，(ii) 保护隐私，允许在不直接访问敏感图像的情况下进行模型训练。在Places8数据集上，我们实现了81.27%的均衡准确性，超过基于图像的方法。在执法部门进行的现实世界CSAI评估中，我们实现了74.27%的均衡准确性。我们的结果证实了结构化场景表示在室内场景分类和CSAI分类中的稳健范式。代码已公开。 

---
# TimeScope: Towards Task-Oriented Temporal Grounding In Long Videos 

**Title (ZH)**: 时间范围：面向任务导向的长视频时间定位 

**Authors**: Xiangrui Liu, Minghao Qin, Yan Shu, Zhengyang Liang, Yang Tian, Chen Jason Zhang, Bo Zhao, Zheng Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.26360)  

**Abstract**: Identifying key moments in long videos is essential for downstream understanding and reasoning tasks. In this paper, we introduce a new problem, Taskoriented Temporal Grounding ToTG, which aims to localize time intervals containing the necessary information based on a task's natural description. Along with the definition, we also present ToTG Bench, a comprehensive benchmark for evaluating the performance on ToTG. ToTG is particularly challenging for traditional approaches due to their limited generalizability and difficulty in handling long videos. To address these challenges, we propose TimeScope, a novel framework built upon progressive reasoning. TimeScope first identifies a coarse-grained temporal scope in the long video that likely contains the key moments, and then refines this scope through finegrained moment partitioning. Additionally, we curate a highquality dataset, namely ToTG Pile, to enhance TimeScope's ability to perform progressive temporal grounding effectively. Extensive experiments demonstrate that TimeScope consistently outperforms both existing temporalgrounding methods and popular MLLMs across various settings, highlighting its effectiveness in addressing this new challenging problem. 

**Abstract (ZH)**: 识别长视频中的关键时刻对于下游理解和推理任务至关重要。在本文中，我们引入了一个新的问题，面向任务的时序定位（Task-oriented Temporal Grounding, ToTG），其目标是基于任务的自然描述，定位包含必要信息的时间区间。此外，我们还提出了ToTG Bench，一个全面的基准测试，用于评估ToTG的表现。由于传统方法的有限泛化能力和长视频处理难度，ToTG特别具有挑战性。为应对这些挑战，我们提出了一种名为TimeScope的新框架，该框架建立在逐步推理的基础上。TimeScope首先在长视频中识别出一个粗粒度的时序范围，很可能包含关键时刻，然后通过细粒度的时刻分割细化这一范围。此外，我们精心策划了一个高质量的数据集，即ToTG Pile，以增强TimeScope在有效地执行逐步时序定位方面的能力。广泛的实验表明，TimeScope在各种设置下一致地优于现有的时序定位方法和流行的MLLMs，突显了其解决这一全新挑战性问题的有效性。 

---
# 3DiFACE: Synthesizing and Editing Holistic 3D Facial Animation 

**Title (ZH)**: 3DiFACE：合成与编辑全方位三维面部动画 

**Authors**: Balamurugan Thambiraja, Malte Prinzler, Sadegh Aliakbarian, Darren Cosker, Justus Thies  

**Link**: [PDF](https://arxiv.org/pdf/2509.26233)  

**Abstract**: Creating personalized 3D animations with precise control and realistic head motions remains challenging for current speech-driven 3D facial animation methods. Editing these animations is especially complex and time consuming, requires precise control and typically handled by highly skilled animators. Most existing works focus on controlling style or emotion of the synthesized animation and cannot edit/regenerate parts of an input animation. They also overlook the fact that multiple plausible lip and head movements can match the same audio input. To address these challenges, we present 3DiFACE, a novel method for holistic speech-driven 3D facial animation. Our approach produces diverse plausible lip and head motions for a single audio input and allows for editing via keyframing and interpolation. Specifically, we propose a fully-convolutional diffusion model that can leverage the viseme-level diversity in our training corpus. Additionally, we employ a speaking-style personalization and a novel sparsely-guided motion diffusion to enable precise control and editing. Through quantitative and qualitative evaluations, we demonstrate that our method is capable of generating and editing diverse holistic 3D facial animations given a single audio input, with control between high fidelity and diversity. Code and models are available here: this https URL 

**Abstract (ZH)**: 基于精确控制和真实头动的个性化3D动画创建仍是对当前语音驱动3D面部动画方法的一大挑战。编辑这些动画特别复杂且耗时，需要精细控制，通常由技术高超的动画师处理。现有大多数工作集中在控制合成动画的风格或情感，而无法编辑或重新生成输入动画的特定部分。他们也忽略了这样一个事实：同一个音频输入可以匹配多种可行的唇部和头部运动。为应对这些挑战，我们提出了3DiFACE，一种面向整体的语音驱动3D面部动画的新方法。我们的方法能够为单一音频输入生成多样且合理的唇部和头部运动，并可通过关键帧编辑和插值进行编辑。具体而言，我们提出了一种全卷积扩散模型，可以利用我们在训练语料库中的唇形级别多样性。此外，我们采用了一种语音风格个性化和新颖的稀疏引导运动扩散技术，以实现精确控制和编辑。通过定量和定性评估，我们证明了我们的方法能够在单一音频输入下生成和编辑多样化的整体3D面部动画，并在保真度和多样性之间提供控制。代码和模型可在以下链接获取：this https URL。 

---
# Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation 

**Title (ZH)**: 超越像素：通过稀疏高斯表示高效的 dataset 降解方法 

**Authors**: Chenyang Jiang, Zhengcen Li, Hang Zhao, Qiben Shan, Shaocong Wu, Jingyong Su  

**Link**: [PDF](https://arxiv.org/pdf/2509.26219)  

**Abstract**: Dataset distillation has emerged as a promising paradigm that synthesizes compact, informative datasets capable of retaining the knowledge of large-scale counterparts, thereby addressing the substantial computational and storage burdens of modern model training. Conventional approaches typically rely on dense pixel-level representations, which introduce redundancy and are difficult to scale up. In this work, we propose GSDD, a novel and efficient sparse representation for dataset distillation based on 2D Gaussians. Instead of representing all pixels equally, GSDD encodes critical discriminative information in a distilled image using only a small number of Gaussian primitives. This sparse representation could improve dataset diversity under the same storage budget, enhancing coverage of difficult samples and boosting distillation performance. To ensure both efficiency and scalability, we adapt CUDA-based splatting operators for parallel inference and training, enabling high-quality rendering with minimal computational and memory overhead. Our method is simple yet effective, broadly applicable to different distillation pipelines, and highly scalable. Experiments show that GSDD achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet subsets, while remaining highly efficient encoding and decoding cost. Our code is available at this https URL. 

**Abstract (ZH)**: 基于2D高斯的稀疏表示在数据集蒸馏中的应用 

---
# AttriGen: Automated Multi-Attribute Annotation for Blood Cell Datasets 

**Title (ZH)**: AttriGen: 自动化多属性标注的血细胞数据集标注方法 

**Authors**: Walid Houmaidi, Youssef Sabiri, Fatima Zahra Iguenfer, Amine Abouaomar  

**Link**: [PDF](https://arxiv.org/pdf/2509.26185)  

**Abstract**: We introduce AttriGen, a novel framework for automated, fine-grained multi-attribute annotation in computer vision, with a particular focus on cell microscopy where multi-attribute classification remains underrepresented compared to traditional cell type categorization. Using two complementary datasets: the Peripheral Blood Cell (PBC) dataset containing eight distinct cell types and the WBC Attribute Dataset (WBCAtt) that contains their corresponding 11 morphological attributes, we propose a dual-model architecture that combines a CNN for cell type classification, as well as a Vision Transformer (ViT) for multi-attribute classification achieving a new benchmark of 94.62\% accuracy. Our experiments demonstrate that AttriGen significantly enhances model interpretability and offers substantial time and cost efficiency relative to conventional full-scale human annotation. Thus, our framework establishes a new paradigm that can be extended to other computer vision classification tasks by effectively automating the expansion of multi-attribute labels. 

**Abstract (ZH)**: 我们介绍了AttriGen，一种用于计算机视觉领域的新型自动精细化多属性标注框架，特别关注细胞显微镜领域，多属性分类相较于传统细胞类型分类仍相对不足。利用两个互补的数据集：包含八种不同细胞类型的外周血细胞(PBC)数据集和包含相应11个形态属性的WBC属性数据集(WBCAtt)，我们提出了一个双模型架构，结合了CNN用于细胞类型分类，以及Vision Transformer (ViT)用于多属性分类，实现了94.62%的新基准准确率。我们的实验表明，AttriGen显著提高了模型的可解释性，并在相对传统全面人工标注方面提供了显著的时间和成本效率。因此，我们的框架建立了可以有效自动化多属性标签扩展的新范式，适用于其他计算机视觉分类任务。 

---
# PFDepth: Heterogeneous Pinhole-Fisheye Joint Depth Estimation via Distortion-aware Gaussian-Splatted Volumetric Fusion 

**Title (ZH)**: PFDepth: 带有失真感知高斯聚合体素融合的高斯鱼眼与针孔联合深度估计 

**Authors**: Zhiwei Zhang, Ruikai Xu, Weijian Zhang, Zhizhong Zhang, Xin Tan, Jingyu Gong, Yuan Xie, Lizhuang Ma  

**Link**: [PDF](https://arxiv.org/pdf/2509.26008)  

**Abstract**: In this paper, we present the first pinhole-fisheye framework for heterogeneous multi-view depth estimation, PFDepth. Our key insight is to exploit the complementary characteristics of pinhole and fisheye imagery (undistorted vs. distorted, small vs. large FOV, far vs. near field) for joint optimization. PFDepth employs a unified architecture capable of processing arbitrary combinations of pinhole and fisheye cameras with varied intrinsics and extrinsics. Within PFDepth, we first explicitly lift 2D features from each heterogeneous view into a canonical 3D volumetric space. Then, a core module termed Heterogeneous Spatial Fusion is designed to process and fuse distortion-aware volumetric features across overlapping and non-overlapping regions. Additionally, we subtly reformulate the conventional voxel fusion into a novel 3D Gaussian representation, in which learnable latent Gaussian spheres dynamically adapt to local image textures for finer 3D aggregation. Finally, fused volume features are rendered into multi-view depth maps. Through extensive experiments, we demonstrate that PFDepth sets a state-of-the-art performance on KITTI-360 and RealHet datasets over current mainstream depth networks. To the best of our knowledge, this is the first systematic study of heterogeneous pinhole-fisheye depth estimation, offering both technical novelty and valuable empirical insights. 

**Abstract (ZH)**: 基于针孔鱼眼框架的异构多视图深度估计：PFDepth 

---
# Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations 

**Title (ZH)**: 基于人类叙述的弱监督自手内物体分割学习 

**Authors**: Nicola Messina, Rosario Leonardi, Luca Ciampi, Fabio Carrara, Giovanni Maria Farinella, Fabrizio Falchi, Antonino Furnari  

**Link**: [PDF](https://arxiv.org/pdf/2509.26004)  

**Abstract**: Pixel-level recognition of objects manipulated by the user from egocentric images enables key applications spanning assistive technologies, industrial safety, and activity monitoring. However, progress in this area is currently hindered by the scarcity of annotated datasets, as existing approaches rely on costly manual labels. In this paper, we propose to learn human-object interaction detection leveraging narrations -- natural language descriptions of the actions performed by the camera wearer which contain clues about manipulated objects (e.g., "I am pouring vegetables from the chopping board to the pan"). Narrations provide a form of weak supervision that is cheap to acquire and readily available in state-of-the-art egocentric datasets. We introduce Narration-Supervised in-Hand Object Segmentation (NS-iHOS), a novel task where models have to learn to segment in-hand objects by learning from natural-language narrations. Narrations are then not employed at inference time. We showcase the potential of the task by proposing Weakly-Supervised In-hand Object Segmentation from Human Narrations (WISH), an end-to-end model distilling knowledge from narrations to learn plausible hand-object associations and enable in-hand object segmentation without using narrations at test time. We benchmark WISH against different baselines based on open-vocabulary object detectors and vision-language models, showing the superiority of its design. Experiments on EPIC-Kitchens and Ego4D show that WISH surpasses all baselines, recovering more than 50% of the performance of fully supervised methods, without employing fine-grained pixel-wise annotations. 

**Abstract (ZH)**: 基于述说监督的手持物体分割：从摄像佩戴者执行的动作自然语言描述中学习人体-物体交互检测 

---
# VRWKV-Editor: Reducing quadratic complexity in transformer-based video editing 

**Title (ZH)**: VRWKV-Editor: 降低基于变压器的视频编辑中的二次复杂度 

**Authors**: Abdelilah Aitrouga, Youssef Hmamouche, Amal El Fallah Seghrouchni  

**Link**: [PDF](https://arxiv.org/pdf/2509.25998)  

**Abstract**: In light of recent progress in video editing, deep learning models focusing on both spatial and temporal dependencies have emerged as the primary method. However, these models suffer from the quadratic computational complexity of traditional attention mechanisms, making them difficult to adapt to long-duration and high-resolution videos. This limitation restricts their applicability in practical contexts such as real-time video processing. To tackle this challenge, we introduce a method to reduce both time and space complexity of these systems by proposing VRWKV-Editor, a novel video editing model that integrates a linear spatio-temporal aggregation module into video-based diffusion models. VRWKV-Editor leverages bidirectional weighted key-value recurrence mechanism of the RWKV transformer to capture global dependencies while preserving temporal coherence, achieving linear complexity without sacrificing quality. Extensive experiments demonstrate that the proposed method achieves up to 3.7x speedup and 60% lower memory usage compared to state-of-the-art diffusion-based video editing methods, while maintaining competitive performance in frame consistency and text alignment. Furthermore, a comparative analysis we conducted on videos with different sequence lengths confirms that the gap in editing speed between our approach and architectures with self-attention becomes more significant with long videos. 

**Abstract (ZH)**: 基于近期视频编辑领域的进展，专注于空�.utils空和时序依赖性的深度学习模型已成为主流方法。然而，这些模型受到传统注意力机制二次计算复杂度的限制，难以适应长时间和高分辨率视频的应用需求。这一局限性限制了它们在实时视频处理等实际应用场景中的适用性。为了应对这一挑战，我们提出了一种通过结合线性空冋试用时序聚合模块到基于视频的扩散模型中来降低时空复杂度的方法，并提出了VRWKV-编辑器这一新型视频编辑模型。VRWKV-编辑器利用RWKV变换器的双向加权键值循环机制来捕捉全局依赖关系同时保持时序一致性，实现了线性复杂度而不牺牲质量。广泛实验证明，与最先进的基于扩散的视频编辑方法相比，该方法可实现最高3.7倍的速度提升和60%更低的内存使用率，同时在帧一致性和文本对齐方面保持了竞争力。此外，我们在不同序列长度的视频上进行的比较分析证实，与采用自我注意力的架构相比，我们方法的编辑速度差距在长时间视频上更为显著。 

---
# From MNIST to ImageNet: Understanding the Scalability Boundaries of Differentiable Logic Gate Networks 

**Title (ZH)**: 从MNIST到ImageNet：理解可微逻辑门网络的可扩展性边界 

**Authors**: Sven Brändle, Till Aczel, Andreas Plesner, Roger Wattenhofer  

**Link**: [PDF](https://arxiv.org/pdf/2509.25933)  

**Abstract**: Differentiable Logic Gate Networks (DLGNs) are a very fast and energy-efficient alternative to conventional feed-forward networks. With learnable combinations of logical gates, DLGNs enable fast inference by hardware-friendly execution. Since the concept of DLGNs has only recently gained attention, these networks are still in their developmental infancy, including the design and scalability of their output layer. To date, this architecture has primarily been tested on datasets with up to ten classes.
This work examines the behavior of DLGNs on large multi-class datasets. We investigate its general expressiveness, its scalability, and evaluate alternative output strategies. Using both synthetic and real-world datasets, we provide key insights into the importance of temperature tuning and its impact on output layer performance. We evaluate conditions under which the Group-Sum layer performs well and how it can be applied to large-scale classification of up to 2000 classes. 

**Abstract (ZH)**: Differentiable Logic Gate Networks on Large Multi-Class Datasets：Behavior, Scalability, and Output Strategies 

---
# Vector sketch animation generation with differentialable motion trajectories 

**Title (ZH)**: 基于可微动捕轨迹的向量草图动画生成 

**Authors**: Xinding Zhu, Xinye Yang, Shuyang Zheng, Zhexin Zhang, Fei Gao, Jing Huang, Jiazhou Chen  

**Link**: [PDF](https://arxiv.org/pdf/2509.25857)  

**Abstract**: Sketching is a direct and inexpensive means of visual expression. Though image-based sketching has been well studied, video-based sketch animation generation is still very challenging due to the temporal coherence requirement. In this paper, we propose a novel end-to-end automatic generation approach for vector sketch animation. To solve the flickering issue, we introduce a Differentiable Motion Trajectory (DMT) representation that describes the frame-wise movement of stroke control points using differentiable polynomial-based trajectories. DMT enables global semantic gradient propagation across multiple frames, significantly improving the semantic consistency and temporal coherence, and producing high-framerate output. DMT employs a Bernstein basis to balance the sensitivity of polynomial parameters, thus achieving more stable optimization. Instead of implicit fields, we introduce sparse track points for explicit spatial modeling, which improves efficiency and supports long-duration video processing. Evaluations on DAVIS and LVOS datasets demonstrate the superiority of our approach over SOTA methods. Cross-domain validation on 3D models and text-to-video data confirms the robustness and compatibility of our approach. 

**Abstract (ZH)**: 基于视频的矢量素描动画生成：一种端到端自动生成方法 

---
# Training-Free Reward-Guided Image Editing via Trajectory Optimal Control 

**Title (ZH)**: 基于轨迹最优控制的无训练数据奖励引导图像编辑 

**Authors**: Jinho Chang, Jaemin Kim, Jong Chul Ye  

**Link**: [PDF](https://arxiv.org/pdf/2509.25845)  

**Abstract**: Recent advancements in diffusion and flow-matching models have demonstrated remarkable capabilities in high-fidelity image synthesis. A prominent line of research involves reward-guided guidance, which steers the generation process during inference to align with specific objectives. However, leveraging this reward-guided approach to the task of image editing, which requires preserving the semantic content of the source image while enhancing a target reward, is largely unexplored. In this work, we introduce a novel framework for training-free, reward-guided image editing. We formulate the editing process as a trajectory optimal control problem where the reverse process of a diffusion model is treated as a controllable trajectory originating from the source image, and the adjoint states are iteratively updated to steer the editing process. Through extensive experiments across distinct editing tasks, we demonstrate that our approach significantly outperforms existing inversion-based training-free guidance baselines, achieving a superior balance between reward maximization and fidelity to the source image without reward hacking. 

**Abstract (ZH)**: 近期在扩散和流动匹配模型方面的进展展示了在高保真图像合成方面的非凡能力。奖励引导方法是其中一条重要研究路线，该方法在推断过程中引导生成过程以满足特定目标。然而，将这种奖励引导方法应用到需要保留源图像语义内容并增强目标奖励的图像编辑任务中，这一领域尚未充分探索。在本文中，我们提出了一种无需训练的奖励引导图像编辑新型框架。我们将编辑过程形式化为一个轨迹最优控制问题，其中扩散模型的反向过程被视为从源图像出发的可控轨迹，通过迭代更新伴随状态来引导编辑过程。通过在不同编辑任务上的广泛实验，展示了我们的方法在无需训练的前提下显著优于现有的基于反转的无需训练引导基线方法，实现了奖励最大化与对源图像保真度之间的更优越平衡，且没有出现奖励作弊现象。 

---
# Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation 

**Title (ZH)**: 基于目标图像编码的可编辑噪声图反转：高保真图像操纵 

**Authors**: Mingyu Kang, Yong Suk Choi  

**Link**: [PDF](https://arxiv.org/pdf/2509.25776)  

**Abstract**: Text-to-image diffusion models have achieved remarkable success in generating high-quality and diverse images. Building on these advancements, diffusion models have also demonstrated exceptional performance in text-guided image editing. A key strategy for effective image editing involves inverting the source image into editable noise maps associated with the target image. However, previous inversion methods face challenges in adhering closely to the target text prompt. The limitation arises because inverted noise maps, while enabling faithful reconstruction of the source image, restrict the flexibility needed for desired edits. To overcome this issue, we propose Editable Noise Map Inversion (ENM Inversion), a novel inversion technique that searches for optimal noise maps to ensure both content preservation and editability. We analyze the properties of noise maps for enhanced editability. Based on this analysis, our method introduces an editable noise refinement that aligns with the desired edits by minimizing the difference between the reconstructed and edited noise maps. Extensive experiments demonstrate that ENM Inversion outperforms existing approaches across a wide range of image editing tasks in both preservation and edit fidelity with target prompts. Our approach can also be easily applied to video editing, enabling temporal consistency and content manipulation across frames. 

**Abstract (ZH)**: 基于文本的图像扩散模型已在生成高质量和多样化图像方面取得了显著成功。在此基础上，扩散模型也在文本引导的图像编辑方面展现了卓越性能。有效的图像编辑的关键策略是将源图像反转为与目标图像相关的可编辑噪声图。然而，之前的反转方法在遵循目标文本提示方面面临挑战。这是因为反转的噪声图虽然能够忠实重建源图像，但限制了实现所需编辑所需的灵活性。为了解决这一问题，我们提出了一种新的反转技术——可编辑噪声图反转（ENM Inversion），该技术寻找最优噪声图以同时确保内容保留和编辑性。我们分析了噪声图的属性以提高编辑性。基于这一分析，我们的方法引入了一种可编辑噪声精炼，通过最小化重建噪声图和编辑噪声图之间的差异来与所需的编辑对齐。广泛实验表明，ENM Inversion在保留和编辑忠实度方面均优于现有方法，特别是在具有目标提示的各种图像编辑任务中。此外，我们的方法也可以轻松应用于视频编辑，实现帧间的时间一致性及内容操控。 

---
# YOLO-Based Defect Detection for Metal Sheets 

**Title (ZH)**: 基于YOLO的金属板材缺陷检测 

**Authors**: Po-Heng Chou, Chun-Chi Wang, Wei-Lung Mao  

**Link**: [PDF](https://arxiv.org/pdf/2509.25659)  

**Abstract**: In this paper, we propose a YOLO-based deep learning (DL) model for automatic defect detection to solve the time-consuming and labor-intensive tasks in industrial manufacturing. In our experiments, the images of metal sheets are used as the dataset for training the YOLO model to detect the defects on the surfaces and in the holes of metal sheets. However, the lack of metal sheet images significantly degrades the performance of detection accuracy. To address this issue, the ConSinGAN is used to generate a considerable amount of data. Four versions of the YOLO model (i.e., YOLOv3, v4, v7, and v9) are combined with the ConSinGAN for data augmentation. The proposed YOLOv9 model with ConSinGAN outperforms the other YOLO models with an accuracy of 91.3%, and a detection time of 146 ms. The proposed YOLOv9 model is integrated into manufacturing hardware and a supervisory control and data acquisition (SCADA) system to establish a practical automated optical inspection (AOI) system. Additionally, the proposed automated defect detection is easily applied to other components in industrial manufacturing. 

**Abstract (ZH)**: 基于YOLO的深度学习自动缺陷检测模型在工业制造中的应用：ConSinGAN数据增强提高检测性能 

---
# K-Prism: A Knowledge-Guided and Prompt Integrated Universal Medical Image Segmentation Model 

**Title (ZH)**: K-棱镜：一种知识导向和提示集成的通用医疗图像分割模型 

**Authors**: Bangwei Guo, Yunhe Gao, Meng Ye, Difei Gu, Yang Zhou, Leon Axel, Dimitris Metaxas  

**Link**: [PDF](https://arxiv.org/pdf/2509.25594)  

**Abstract**: Medical image segmentation is fundamental to clinical decision-making, yet existing models remain fragmented. They are usually trained on single knowledge sources and specific to individual tasks, modalities, or organs. This fragmentation contrasts sharply with clinical practice, where experts seamlessly integrate diverse knowledge: anatomical priors from training, exemplar-based reasoning from reference cases, and iterative refinement through real-time interaction. We present $\textbf{K-Prism}$, a unified segmentation framework that mirrors this clinical flexibility by systematically integrating three knowledge paradigms: (i) $\textit{semantic priors}$ learned from annotated datasets, (ii) $\textit{in-context knowledge}$ from few-shot reference examples, and (iii) $\textit{interactive feedback}$ from user inputs like clicks or scribbles. Our key insight is that these heterogeneous knowledge sources can be encoded into a dual-prompt representation: 1-D sparse prompts defining $\textit{what}$ to segment and 2-D dense prompts indicating $\textit{where}$ to attend, which are then dynamically routed through a Mixture-of-Experts (MoE) decoder. This design enables flexible switching between paradigms and joint training across diverse tasks without architectural modifications. Comprehensive experiments on 18 public datasets spanning diverse modalities (CT, MRI, X-ray, pathology, ultrasound, etc.) demonstrate that K-Prism achieves state-of-the-art performance across semantic, in-context, and interactive segmentation settings. Code will be released upon publication. 

**Abstract (ZH)**: medical图像分割是临床决策的基础，但现有的模型仍然支离破碎。它们通常仅基于单个知识来源并在特定任务、模态或器官上进行训练。这种分割与临床实践形成鲜明对比，在临床实践中，专家能够无缝地整合多种知识：基于标注数据的学习先验知识、基于参考案例的示例推理以及通过实时交互进行的迭代细化。我们提出了K-Prism，这是一种统一的分割框架，通过系统地整合三种知识范式来模拟这种临床灵活性：(i) 从标注数据中学习的语义先验；(ii) 来自少量参考示例的上下文知识；(iii) 用户输入（如点击或涂抹）的交互反馈。我们的核心洞察是，这些异质性知识来源可以编码为双提示表示：1-D稀疏提示定义要分割的内容，2-D密集提示指示需要关注的位置，这些信息随后通过专家混合模型(MoE)解码器动态路由。这种设计允许在不修改架构的情况下灵活切换范式并在多种任务中进行联合训练。18个来自不同模态（CT、MRI、X射线、病理学、超声波等）的公共数据集上进行了全面实验，结果表明K-Prism在语义、上下文和交互分割设置中均实现了最先进的性能。代码将在发表后公开。 

---
# AttentionViG: Cross-Attention-Based Dynamic Neighbor Aggregation in Vision GNNs 

**Title (ZH)**: AttentionViG: 基于交叉注意力的动态邻域聚合在视觉图神经网络中的应用 

**Authors**: Hakan Emre Gedik, Andrew Martin, Mustafa Munir, Oguzhan Baser, Radu Marculescu, Sandeep P. Chinchali, Alan C. Bovik  

**Link**: [PDF](https://arxiv.org/pdf/2509.25570)  

**Abstract**: Vision Graph Neural Networks (ViGs) have demonstrated promising performance in image recognition tasks against Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). An essential part of the ViG framework is the node-neighbor feature aggregation method. Although various graph convolution methods, such as Max-Relative, EdgeConv, GIN, and GraphSAGE, have been explored, a versatile aggregation method that effectively captures complex node-neighbor relationships without requiring architecture-specific refinements is needed. To address this gap, we propose a cross-attention-based aggregation method in which the query projections come from the node, while the key projections come from its neighbors. Additionally, we introduce a novel architecture called AttentionViG that uses the proposed cross-attention aggregation scheme to conduct non-local message passing. We evaluated the image recognition performance of AttentionViG on the ImageNet-1K benchmark, where it achieved SOTA performance. Additionally, we assessed its transferability to downstream tasks, including object detection and instance segmentation on MS COCO 2017, as well as semantic segmentation on ADE20K. Our results demonstrate that the proposed method not only achieves strong performance, but also maintains efficiency, delivering competitive accuracy with comparable FLOPs to prior vision GNN architectures. 

**Abstract (ZH)**: 基于视觉的图神经网络（ViGs）在图像识别任务中展现了优于卷积神经网络（CNNs）和视觉变换器（ViTs）的 promising 性能。ViG 框架中的关键组成部分是节点-邻居特征聚合方法。尽管已经探索了多种图卷积方法，如 Max-Relative、EdgeConv、GIN 和 GraphSAGE，但仍需要一种能有效捕捉复杂节点-邻居关系且无需特定架构改进的通用聚合方法。为解决这一问题，我们提出了一种基于交叉注意力的聚合方法，其中查询投影来自节点，而键投影来自其邻居。此外，我们引入了一种名为 AttentionViG 的新架构，利用提出的交叉注意力聚合方案进行非局部消息传递。我们在 ImageNet-1K 基准上评估了 AttentionViG 的图像识别性能，实现了 SOTA 性能。此外，我们还评估了其在诸如 MSR COCO 2017 的物体检测和实例分割以及 ADE20K 的语义分割等下游任务上的可迁移性。实验结果表明，所提出的方法不仅具有强大的性能，而且保持了高效性，与先前的视觉 GNN 架构相比，提供了具有竞争力的准确性和相似的 FLOPs。 

---
# Hybrid Approach for Enhancing Lesion Segmentation in Fundus Images 

**Title (ZH)**: 基金图像中病灶分割增强的混合方法 

**Authors**: Mohammadmahdi Eshragh, Emad A. Mohammed, Behrouz Far, Ezekiel Weis, Carol L Shields, Sandor R Ferenczy, Trafford Crump  

**Link**: [PDF](https://arxiv.org/pdf/2509.25549)  

**Abstract**: Choroidal nevi are common benign pigmented lesions in the eye, with a small risk of transforming into melanoma. Early detection is critical to improving survival rates, but misdiagnosis or delayed diagnosis can lead to poor outcomes. Despite advancements in AI-based image analysis, diagnosing choroidal nevi in colour fundus images remains challenging, particularly for clinicians without specialized expertise. Existing datasets often suffer from low resolution and inconsistent labelling, limiting the effectiveness of segmentation models. This paper addresses the challenge of achieving precise segmentation of fundus lesions, a critical step toward developing robust diagnostic tools. While deep learning models like U-Net have demonstrated effectiveness, their accuracy heavily depends on the quality and quantity of annotated data. Previous mathematical/clustering segmentation methods, though accurate, required extensive human input, making them impractical for medical applications. This paper proposes a novel approach that combines mathematical/clustering segmentation models with insights from U-Net, leveraging the strengths of both methods. This hybrid model improves accuracy, reduces the need for large-scale training data, and achieves significant performance gains on high-resolution fundus images. The proposed model achieves a Dice coefficient of 89.7% and an IoU of 80.01% on 1024*1024 fundus images, outperforming the Attention U-Net model, which achieved 51.3% and 34.2%, respectively. It also demonstrated better generalizability on external datasets. This work forms a part of a broader effort to develop a decision support system for choroidal nevus diagnosis, with potential applications in automated lesion annotation to enhance the speed and accuracy of diagnosis and monitoring. 

**Abstract (ZH)**: 视网膜色素痣是眼睛中常见的良性色素病变，虽有转变为黑色素瘤的小概率，但早期发现对于提高生存率至关重要，但误诊或延迟诊断可能导致不良后果。尽管基于AI的图像分析取得了进展，但在彩色底片图像中诊断视网膜色素痣对缺乏专业背景的临床医生来说仍然具有挑战性。现有数据集往往分辨率低且标注不一致，限制了分割模型的有效性。本文解决了精确分割底片病变的挑战，这是开发稳健诊断工具的关键步骤。虽然如U-Net等深度学习模型展示了有效性，但其准确性高度依赖于标注数据的质量和数量。之前的数学/聚类分割方法虽然准确，但需要大量的人工输入，使其在医疗应用中不可行。本文提出了一种新的方法，结合了数学/聚类分割模型与U-Net的见解，利用两种方法的优势。该混合模型提高了准确性，减少了大规模训练数据的需求，并在高分辨率底片图像上实现了显著的性能提升。所提出模型在1024×1024底片图像上的Dice系数为89.7%，IoU为80.01%，远超Attention U-Net模型的51.3%和34.2%。此外，它在外部数据集上展示了更好的泛化能力。这项工作是开发用于视网膜色素痣诊断决策支持系统的一部分，有可能应用于自动病灶标注以提高诊断和监测的速度和准确性。 

---
# DeepFake Detection in Dyadic Video Calls using Point of Gaze Tracking 

**Title (ZH)**: 基于眼动追踪的双人视频通话DeepFake检测 

**Authors**: Odin Kohler, Rahul Vijaykumar, Masudul H. Imtiaz  

**Link**: [PDF](https://arxiv.org/pdf/2509.25503)  

**Abstract**: With recent advancements in deepfake technology, it is now possible to generate convincing deepfakes in real-time. Unfortunately, malicious actors have started to use this new technology to perform real-time phishing attacks during video meetings. The nature of a video call allows access to what the deepfake is ``seeing,'' that is, the screen displayed to the malicious actor. Using this with the estimated gaze from the malicious actors streamed video enables us to estimate where the deepfake is looking on screen, the point of gaze. Because the point of gaze during conversations is not random and is instead used as a subtle nonverbal communicator, it can be used to detect deepfakes, which are not capable of mimicking this subtle nonverbal communication. This paper proposes a real-time deepfake detection method adapted to this genre of attack, utilizing previously unavailable biometric information. We built our model based on explainable features selected after careful review of research on gaze patterns during dyadic conversations. We then test our model on a novel dataset of our creation, achieving an accuracy of 82\%. This is the first reported method to utilize point-of-gaze tracking for deepfake detection. 

**Abstract (ZH)**: 随着深度合成技术的 recent advancements，现在可以在实时生成逼真的深度合成内容。不幸的是，恶意行为者已经开始利用这项新技术在视频会议中进行实时欺诈攻击。视频通话的性质使得可以访问深度合成所“看到”的内容，即显示给恶意行为者的屏幕内容。通过结合从恶意行为者流媒体视频中估计的注视方向，可以估计深度合成在屏幕上注视的点，即注视点。在对话中，注视点不是随机的，而是一种微妙的非语言沟通方式，因此可以用于检测无法模仿这种微妙非语言沟通的深度合成。本文提出了一种适应此类攻击的实时深度合成检测方法，利用过去不可用的生物特征信息。我们在仔细审阅关于双人对话期间注视模式的研究后选择了可解释的特征构建了我们的模型，并在我们创建的一个新数据集上测试了我们的模型，达到了82%的准确率。这是首次使用注视点跟踪进行深度合成检测的方法报道。 

---
# SpinBench: Perspective and Rotation as a Lens on Spatial Reasoning in VLMs 

**Title (ZH)**: SpinBench: 观点与旋转作为空间推理在VLMs中的透镜 

**Authors**: Yuyou Zhang, Radu Corcodel, Chiori Hori, Anoop Cherian, Ding Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2509.25390)  

**Abstract**: We present SpinBench, a cognitively grounded diagnostic benchmark for evaluating spatial reasoning in vision language models (VLMs). SpinBench is designed around the core challenge of spatial reasoning: perspective taking, the ability to reason about how scenes and object relations change under viewpoint transformation. Since perspective taking requires multiple cognitive capabilities, such as recognizing objects across views, relative positions grounding, and mentally simulating transformations, SpinBench introduces a set of fine-grained diagnostic categories. Our categories target translation, rotation, object relative pose, and viewpoint change, and are progressively structured so that single-object simpler tasks scaffold toward the most demanding multi-object perspective-taking setting. We evaluate 37 state-of-the-art VLMs, both proprietary and open source. Results reveal systematic weaknesses: strong egocentric bias, poor rotational understanding, and inconsistencies under symmetrical and syntactic reformulations. Scaling analysis shows both smooth improvements and emergent capabilities. While human subjects achieve high accuracy (91.2\%), task difficulty as measured by human response time shows strong correlation with VLM accuracy, indicating that SpinBench captures spatial reasoning challenges shared across humans and VLMs. We believe SpinBench provides critical insights into spatial reasoning in VLMs and highlights key gaps in their ability to reason about physical space. Our website can be found at this https URL. 

**Abstract (ZH)**: 我们介绍SpinBench：一种基于认知的心理诊断基准，用于评估视觉语言模型的空间推理能力 

---
# VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes 

**Title (ZH)**: 视觉过载：探究VLMs在Really Dense场景中的视觉理解能力 

**Authors**: Paul Gavrikov, Wei Lin, M. Jehanzeb Mirza, Soumya Jahagirdar, Muhammad Huzaifa, Sivan Doveh, Serena Yeung-Levy, James Glass, Hilde Kuehne  

**Link**: [PDF](https://arxiv.org/pdf/2509.25339)  

**Abstract**: Is basic visual understanding really solved in state-of-the-art VLMs? We present VisualOverload, a slightly different visual question answering (VQA) benchmark comprising 2,720 question-answer pairs, with privately held ground-truth responses. Unlike prior VQA datasets that typically focus on near global image understanding, VisualOverload challenges models to perform simple, knowledge-free vision tasks in densely populated (or, overloaded) scenes. Our dataset consists of high-resolution scans of public-domain paintings that are populated with multiple figures, actions, and unfolding subplots set against elaborately detailed backdrops. We manually annotated these images with questions across six task categories to probe for a thorough understanding of the scene. We hypothesize that current benchmarks overestimate the performance of VLMs, and encoding and reasoning over details is still a challenging task for them, especially if they are confronted with densely populated scenes. Indeed, we observe that even the best model (o3) out of 37 tested models only achieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on all questions. Beyond a thorough evaluation, we complement our benchmark with an error analysis that reveals multiple failure modes, including a lack of counting skills, failure in OCR, and striking logical inconsistencies under complex tasks. Altogether, VisualOverload exposes a critical gap in current vision models and offers a crucial resource for the community to develop better models.
Benchmark: this http URL 

**Abstract (ZH)**: 基准数据集：这个链接 

---
# Perceptual Influence: Improving the Perceptual Loss Design for Low-Dose CT Enhancement 

**Title (ZH)**: 感知影响：改进低剂量CT增强的感知损失设计 

**Authors**: Gabriel A. Viana, Luis F. Alves Pereira, Tsang Ing Ren, George D. C. Cavalcanti, Jan Sijbers  

**Link**: [PDF](https://arxiv.org/pdf/2509.23025)  

**Abstract**: Perceptual losses have emerged as powerful tools for training networks to enhance Low-Dose Computed Tomography (LDCT) images, offering an alternative to traditional pixel-wise losses such as Mean Squared Error, which often lead to over-smoothed reconstructions and loss of clinically relevant details in LDCT images. The perceptual losses operate in a latent feature space defined by a pretrained encoder and aim to preserve semantic content by comparing high-level features rather than raw pixel values. However, the design of perceptual losses involves critical yet underexplored decisions, including the feature representation level, the dataset used to pretrain the encoder, and the relative importance assigned to the perceptual component during optimization. In this work, we introduce the concept of perceptual influence (a metric that quantifies the relative contribution of the perceptual loss term to the total loss) and propose a principled framework to assess the impact of the loss design choices on the model training performance. Through systematic experimentation, we show that the widely used configurations in the literature to set up a perceptual loss underperform compared to better-designed alternatives. Our findings show that better perceptual loss designs lead to significant improvements in noise reduction and structural fidelity of reconstructed CT images, without requiring any changes to the network architecture. We also provide objective guidelines, supported by statistical analysis, to inform the effective use of perceptual losses in LDCT denoising. Our source code is available at this https URL. 

**Abstract (ZH)**: 感知损失已成为训练网络提升低剂量计算机断层扫描（LDCT）图像的强大工具，提供了一种不同于传统的像素级损失（如均方误差），后者常导致过度平滑的重构并丢失临床相关的细节。感知损失在由预训练编码器定义的潜在特征空间中操作，通过比较高层特征而非原始像素值来保留语义内容。然而，感知损失的设计包含许多关键但尚未充分探索的决策，包括特征表示层、用于预训练编码器的数据集，以及优化过程中感知组件的重要性权重。在本文中，我们引入了感知影响（量化感知损失项对总损失相对贡献的度量）的概念，并提出了一种原则性的框架来评估损失设计选择对模型训练性能的影响。通过系统实验，我们表明文献中广泛使用的感知损失配置表现不如更好设计的替代方案。我们的发现表明，更好的感知损失设计在不改变网络架构的情况下，显著提高了重构CT图像的降噪性能和结构保真度。我们还提供了基于统计分析的支持性客观指南，以指导感知损失在LDCT降噪中的有效使用。我们的源代码可在以下链接获取。 

---
