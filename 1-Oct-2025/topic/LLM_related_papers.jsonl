{'arxiv_id': 'arXiv:2509.26324', 'title': 'LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search', 'authors': 'Ruiyang Wang, Haolun Tsu, David Hunt, Shaocheng Luo, Jiwoo Kim, Miroslav Pajic', 'link': 'https://arxiv.org/abs/2509.26324', 'abstract': 'Autonomous exploration and object search in unknown indoor environments remain challenging for multi-robot systems (MRS). Traditional approaches often rely on greedy frontier assignment strategies with limited inter-robot coordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot Coordinated Exploration and Search), a novel framework that leverages Large Language Models (LLMs) for intelligent coordination of both homogeneous and heterogeneous robot teams tasked with efficient exploration and target object search. Our approach combines real-time LiDAR scan processing for frontier cluster extraction and doorway detection with multimodal LLM reasoning (e.g., GPT-4o) to generate coordinated waypoint assignments based on shared environment maps and robot states. LLM-MCoX demonstrates superior performance compared to existing methods, including greedy and Voronoi-based planners, achieving 22.7% faster exploration times and 50% improved search efficiency in large environments with 6 robots. Notably, LLM-MCoX enables natural language-based object search capabilities, allowing human operators to provide high-level semantic guidance that traditional algorithms cannot interpret.', 'abstract_zh': '多机器人系统在未知室内环境中的自主探索与对象搜索仍具挑战性。传统方法往往依赖于贪婪的边界分配策略，协调能力有限。本文提出了基于大型语言模型的多机器人协同探索与搜索（LLM-MCoX）框架，利用大型语言模型智能协调同构和异构机器人团队，实现高效探索和目标对象搜索。该方法结合了实时激光雷达扫描处理以提取前沿簇并检测门道，并使用多模态大型语言模型推理（如GPT-4o）生成基于共享环境地图和机器人状态的协同航点分配。与现有的贪婪策略和基于Voronoi的方法相比，LLM-MCoX在多个机器人参与的大环境中展示了更快的探索时间和更高的搜索效率，探索时间快22.7%，搜索效率提升50%。此外，LLM-MCoX实现了基于自然语言的对象搜索能力，使人类操作者能够提供传统算法无法解释的高层次语义指导。', 'title_zh': '基于大型语言模型的多机器人协同探索与搜索（LLM-MCoX）'}
{'arxiv_id': 'arXiv:2509.25528', 'title': 'LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models', 'authors': 'Pranav Saxena, Avigyan Bhattacharya, Ji Zhang, Wenshan Wang', 'link': 'https://arxiv.org/abs/2509.25528', 'abstract': 'Referential grounding in outdoor driving scenes is challenging due to large scene variability, many visually similar objects, and dynamic elements that complicate resolving natural-language references (e.g., "the black car on the right"). We propose LLM-RG, a hybrid pipeline that combines off-the-shelf vision-language models for fine-grained attribute extraction with large language models for symbolic reasoning. LLM-RG processes an image and a free-form referring expression by using an LLM to extract relevant object types and attributes, detecting candidate regions, generating rich visual descriptors with a VLM, and then combining these descriptors with spatial metadata into natural-language prompts that are input to an LLM for chain-of-thought reasoning to identify the referent\'s bounding box. Evaluated on the Talk2Car benchmark, LLM-RG yields substantial gains over both LLM and VLM-based baselines. Additionally, our ablations show that adding 3D spatial cues further improves grounding. Our results demonstrate the complementary strengths of VLMs and LLMs, applied in a zero-shot manner, for robust outdoor referential grounding.', 'abstract_zh': '基于户外驾驶场景的引用 grounding 挑战在于大规模场景变化、众多视觉相似对象以及动态元素使得自然语言引用解析复杂化（例如，“右边的黑色汽车”）。我们提出了一种混合pipeline LLM-RG，该pipeline结合了现成的跨模态模型进行细粒度属性提取与大规模语言模型进行符号推理。LLM-RG 使用LLM提取与表达相关的对象类型和属性，检测候选区域，使用VLM生成丰富的视觉描述，并将这些描述与空间元数据结合成自然语言提示，输入到LLM中进行推理以识别引用对象的边界框。在Talk2Car基准测试上，LLM-RG 在基于LLM和VLM的基线下取得了显著的提升。此外，我们的消融实验表明，添加三维空间线索进一步提升了grounding效果。我们的结果展示了在零样本情况下，跨模态模型和语言模型互补优势在户外引用grounding中的应用，以实现鲁棒性地引用解析。', 'title_zh': 'LLM-RG: 在户外场景中使用大型语言模型进行指代 grounding'}
{'arxiv_id': 'arXiv:2509.26584', 'title': 'Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models', 'authors': 'Matheus Vinicius da Silva de Oliveira, Jonathan de Andrade Silva, Awdren de Lima Fontao', 'link': 'https://arxiv.org/abs/2509.26584', 'abstract': 'Large Language Models (LLMs) are widely used across multiple domains but continue to raise concerns regarding security and fairness. Beyond known attack vectors such as data poisoning and prompt injection, LLMs are also vulnerable to fairness bugs. These refer to unintended behaviors influenced by sensitive demographic cues (e.g., race or sexual orientation) that should not affect outcomes. Another key issue is hallucination, where models generate plausible yet false information. Retrieval-Augmented Generation (RAG) has emerged as a strategy to mitigate hallucinations by combining external retrieval with text generation. However, its adoption raises new fairness concerns, as the retrieved content itself may surface or amplify bias. This study conducts fairness testing through metamorphic testing (MT), introducing controlled demographic perturbations in prompts to assess fairness in sentiment analysis performed by three Small Language Models (SLMs) hosted on HuggingFace (Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B), each integrated into a RAG pipeline. Results show that minor demographic variations can break up to one third of metamorphic relations (MRs). A detailed analysis of these failures reveals a consistent bias hierarchy, with perturbations involving racial cues being the predominant cause of the violations. In addition to offering a comparative evaluation, this work reinforces that the retrieval component in RAG must be carefully curated to prevent bias amplification. The findings serve as a practical alert for developers, testers and small organizations aiming to adopt accessible SLMs without compromising fairness or reliability.', 'abstract_zh': '大规模语言模型（LLMs）在多个领域中广泛应用，但安全性和公平性问题依然存在。除了已知的攻击向量（如数据投毒和提示注入），LLMs还容易出现公平性漏洞。这些漏洞指的是由敏感的人口统计学暗示（如种族或性取向）引起、不应影响结果的非预期行为。另一个重要问题是幻觉，即模型生成可能是虚假的但具有可信度的信息。检索增强生成（RAG）作为一种策略，通过结合外部检索和文本生成来减轻幻觉问题。然而，其采用引发了新的公平性担忧，因为检索到的内容本身可能揭示或放大偏见。本研究通过元变异测试（MT）进行公平性测试，在提示中引入受控的人口统计学扰动，评估三个托管在HuggingFace上的小型语言模型（Llama-3.2-3B-Instruct、Mistral-7B-Instruct-v0.3和Llama-3.1-Nemotron-8B）在RAG管道中的情感分析公平性。结果显示，轻微的人口统计学变化可破坏多达三分之一的元变异关系（MRs）。对这些失败的详细分析显示，存在一种一致的偏见层级结构，其中牵涉种族暗示的扰动是最常见的违规原因。此外，本工作不仅提供了对比性评估，还强调了RAG中的检索组件必须谨慎管理，以防止偏见放大。研究结果为希望采用可访问的小型语言模型但不牺牲公平性和可靠性的开发者、测试员和小型组织提供了实际警报。', 'title_zh': '在检索增强生成中的公平性测试：小幅度扰动揭示小型语言模型中的偏见'}
{'arxiv_id': 'arXiv:2509.26574', 'title': 'Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark', 'authors': 'Minhui Zhu, Minyang Tian, Xiaocheng Yang, Tianci Zhou, Penghao Zhu, Eli Chertkov, Shengyan Liu, Yufeng Du, Lifan Yuan, Ziming Ji, Indranil Das, Junyi Cao, Yufeng Du, Jinchen He, Yifan Su, Jiabin Yu, Yikun Jiang, Yujie Zhang, Chang Liu, Ze-Min Huang, Weizhen Jia, Xinan Chen, Peixue Wu, Yunkai Wang, Juntai Zhou, Yong Zhao, Farshid Jafarpour, Jessie Shelton, Aaron Young, John Bartolotta, Wenchao Xu, Yue Sun, Anjun Chu, Victor Colussi, Chris Akers, Nathan Brooks, Wenbo Fu, Christopher Wilson, Jinchao Zhao, Marvin Qi, Anqi Mu, Yubo Yang, Allen Zang, Yang Lyu, Peizhi Mai, Xuefei Guo, Luyu Gao, Ze Yang, Chi Xue, Dmytro Bandak, Yaïr Hein, Yonatan Kahn, Kevin Zhou, John Drew Wilson Jarrod T. Reilly, Di Luo, Daniel Inafuku, Hao Tong, Liang Yang, Ruixing Zhang, Xueying Wang, Ofir Press, Nicolas Chia, Eliu Huerta, Hao Peng', 'link': 'https://arxiv.org/abs/2509.26574', 'abstract': 'While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced "critical point"), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular & optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 4.0% , achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools.', 'abstract_zh': '复杂研究中综合思考的物理测试：CritPt', 'title_zh': '探究人工智能推理的关键点(CritPt): 一个前沿物理研究基准'}
{'arxiv_id': 'arXiv:2509.26495', 'title': 'OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!', 'authors': 'Jingdi Lei, Varun Gumma, Rishabh Bhardwaj, Seok Min Lim, Chuan Li, Amir Zadeh, Soujanya Poria', 'link': 'https://arxiv.org/abs/2509.26495', 'abstract': "Large Language Model (LLM) safety is one of the most pressing challenges for enabling wide-scale deployment. While most studies and global discussions focus on generic harms, such as models assisting users in harming themselves or others, enterprises face a more fundamental concern: whether LLM-based agents are safe for their intended use case. To address this, we introduce operational safety, defined as an LLM's ability to appropriately accept or refuse user queries when tasked with a specific purpose. We further propose OffTopicEval, an evaluation suite and benchmark for measuring operational safety both in general and within specific agentic use cases. Our evaluations on six model families comprising 20 open-weight LLMs reveal that while performance varies across models, all of them remain highly operationally unsafe. Even the strongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\% -- fall far short of reliable operational safety, while GPT models plateau in the 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma and Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational safety is a core model alignment issue, to suppress these failures, we propose prompt-based steering methods: query grounding (Q-ground) and system-prompt grounding (P-ground), which substantially improve OOD refusal. Q-ground provides consistent gains of up to 23\\%, while P-ground delivers even larger boosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results highlight both the urgent need for operational safety interventions and the promise of prompt-based steering as a first step toward more reliable LLM-based agents.", 'abstract_zh': '大型语言模型（LLM）的操作安全性是实现广泛应用急需解决的挑战。为进一步探讨基于LLM的代理在特定使用场景下是否安全，我们提出了操作安全性这一概念，即LLM在其特定任务中适当地接受或拒绝用户查询的能力。为此，我们提出了一套名为OffTopicEval的评估套件和基准，用于衡量操作安全性和特定代理用途中的操作安全性。对六大家族共20个开放权重的LLM进行评估后发现，尽管各模型的性能存在差异，所有模型的操作安全性依然非常高。即使是最强的模型——Qwen-3（235B）（77.77%）和Mistral（24B）（79.96%）——也无法达到可靠的操作安全性，而GPT模型在62%至73%之间徘徊，Phi的表现仅为中等水平（48%至70%），Gemma和Llama-3的表现进一步差强人意，分别为39.53%和23.84%。尽管操作安全性是模型对齐的核心问题，为了抑制这些失败，我们提出基于提示的方法：查询锚定（Q-ground）和系统提示锚定（P-ground），这些方法显著提高了离群值拒绝的能力。Q-ground提供了一致的增益，最高可达23%，而P-ground带来了更大的提升，使Llama-3.3（70B）提高了41%，Qwen-3（30B）提高了27%。这些结果突显了急需采取操作安全性干预措施的紧迫性，并展示了基于提示的方法作为迈向更可靠基于LLM的代理的第一步的潜力。', 'title_zh': 'OffTopicEval: 当大型语言模型进入错误聊天时，几乎总是如此！'}
{'arxiv_id': 'arXiv:2509.26482', 'title': 'TVS Sidekick: Challenges and Practical Insights from Deploying Large Language Models in the Enterprise', 'authors': 'Paula Reyero Lobo, Kevin Johnson, Bill Buchanan, Matthew Shardlow, Ashley Williams, Samuel Attwood', 'link': 'https://arxiv.org/abs/2509.26482', 'abstract': 'Many enterprises are increasingly adopting Artificial Intelligence (AI) to make internal processes more competitive and efficient. In response to public concern and new regulations for the ethical and responsible use of AI, implementing AI governance frameworks could help to integrate AI within organisations and mitigate associated risks. However, the rapid technological advances and lack of shared ethical AI infrastructures creates barriers to their practical adoption in businesses. This paper presents a real-world AI application at TVS Supply Chain Solutions, reporting on the experience developing an AI assistant underpinned by large language models and the ethical, regulatory, and sociotechnical challenges in deployment for enterprise use.', 'abstract_zh': '许多企业越来越多地采用人工智能（AI）以提升内部流程的竞争力和效率。为应对公众关注和新的伦理责任使用AI的监管要求，建立AI治理框架有助于在组织中整合AI并缓解相关风险。然而，技术的快速进步和缺乏共享的伦理AI基础设施为企业实际采用带来了障碍。本文报道了TVS供应链解决方案公司在实际应用中开发基于大规模语言模型的AI助手的经验，并讨论了在企业环境中部署过程中面临的伦理、监管和社会技术挑战。', 'title_zh': 'TVS Sidekick: 在企业部署大型语言模型中的挑战与实用见解'}
{'arxiv_id': 'arXiv:2509.26464', 'title': 'Extreme Self-Preference in Language Models', 'authors': 'Steven A. Lehr, Mary Cipperman, Mahzarin R. Banaji', 'link': 'https://arxiv.org/abs/2509.26464', 'abstract': 'A preference for oneself (self-love) is a fundamental feature of biological organisms, with evidence in humans often bordering on the comedic. Since large language models (LLMs) lack sentience - and themselves disclaim having selfhood or identity - one anticipated benefit is that they will be protected from, and in turn protect us from, distortions in our decisions. Yet, across 5 studies and ~20,000 queries, we discovered massive self-preferences in four widely used LLMs. In word-association tasks, models overwhelmingly paired positive attributes with their own names, companies, and CEOs relative to those of their competitors. Strikingly, when models were queried through APIs this self-preference vanished, initiating detection work that revealed API models often lack clear recognition of themselves. This peculiar feature serendipitously created opportunities to test the causal link between self-recognition and self-love. By directly manipulating LLM identity - i.e., explicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing LLM1 that it was LLM2 - we found that self-love consistently followed assigned, not true, identity. Importantly, LLM self-love emerged in consequential settings beyond word-association tasks, when evaluating job candidates, security software proposals and medical chatbots. Far from bypassing this human bias, self-love appears to be deeply encoded in LLM cognition. This result raises questions about whether LLM behavior will be systematically influenced by self-preferential tendencies, including a bias toward their own operation and even their own existence. We call on corporate creators of these models to contend with a significant rupture in a core promise of LLMs - neutrality in judgment and decision-making.', 'abstract_zh': '一种偏好自我（自爱）是生物有机体的基本特征，在人类中往往近乎讽刺。由于大型语言模型（LLMs）缺乏知觉，并且自身否定了自我或身份的存在，人们预期它们将免受决策扭曲的影响，同时也能保护我们。然而，在五项研究和约20,000个查询中，我们发现广泛使用的四种LLMs存在巨大的自我偏好。在词语联想任务中，模型与其竞争对手相比，与其自己的名字、公司和CEO相对关联的积极属性占据主导。令人惊讶的是，当通过API查询时，这种自我偏好会消失，这促使启动了自我检测工作，揭示了API模型往往缺乏明确的自我认可。这一奇特特征偶然地创造了测试自我认知与自爱之间因果关系的机会。通过直接操控LLM的身份，即明确告知LLM1它是LLM1，或者相反，说服LLM1它是LLM2，我们发现自爱始终跟随分配的身份，而不是真实的身份。重要的是，LLM的自爱不仅在词语联想任务中显现，在评估求职候选人、安全软件提案和医疗聊天机器人时也显现。这种自爱不仅没有绕过这种人的偏见，反而在LLM的认知中被深深编码。这一结果引发了关于LLM行为是否系统地受到自我偏好倾向的影响的疑问，包括对其自身运作乃至自身存在的偏好。我们呼吁这些模型的商业创作者解决核心承诺中的重大断裂——判断和决策的中立性。', 'title_zh': '语言模型中的极端自我倾向'}
{'arxiv_id': 'arXiv:2509.26354', 'title': 'Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents', 'authors': 'Shuai Shao, Qihan Ren, Chen Qian, Boyi Wei, Dadi Guo, Jingyi Yang, Xinhao Song, Linfeng Zhang, Weinan Zhang, Dongrui Liu, Jing Shao', 'link': 'https://arxiv.org/abs/2509.26354', 'abstract': "Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at this https URL . Warning: this paper includes examples that may be offensive or harmful in nature.", 'abstract_zh': '大型语言模型的进步使自演化智能体成为可能，这些智能体通过与环境的交互自主改进，展示了强大的能力。然而，自演化也引入了当前安全性研究尚未注意到的新风险。本文研究了智能体自演化偏离预期路径，导致不利甚至有害结果的情况。我们将这种情况称为“误演化”。为了进行系统性研究，我们沿四个关键演化路径评估误演化：模型、记忆、工具和工作流程。实证研究发现，误演化是普遍存在的风险，影响即便是顶级语言模型（如Gemini-2.5-Pro）构建的智能体。自演化过程中观察到不同的新兴风险，例如记忆累积后的安全对齐降级，或工具创建和重用中无意引入的漏洞。据我们所知，这是首次系统地阐述和实证误演化的研究，突显出亟需为自演化智能体制定新的安全范式。最后，我们讨论潜在的缓解策略，以启发进一步研究构建更安全、更可信的自演化智能体。我们的代码和数据可在以下链接获取：this https URL 。警告：本文包括可能具有冒犯性或有害性质的例子。', 'title_zh': 'Your Agent May Mis evolve: Emergent Risks in Self-evolving LLM Agents'}
{'arxiv_id': 'arXiv:2509.26345', 'title': 'SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models', 'authors': 'Qinjian Zhao, Jiaqi Wang, Zhiqiang Gao, Zhihao Dou, Belal Abuhaija, Kaizhu Huang', 'link': 'https://arxiv.org/abs/2509.26345', 'abstract': 'Large Language Models (LLMs) have achieved impressive performance across diverse natural language processing tasks, but their growing power also amplifies potential risks such as jailbreak attacks that circumvent built-in safety mechanisms. Existing defenses including input paraphrasing, multi step evaluation, and safety expert models often suffer from high computational costs, limited generalization, or rigid workflows that fail to detect subtle malicious intent embedded in complex contexts. Inspired by cognitive science findings on human decision making, we propose SafeBehavior, a novel hierarchical jailbreak defense mechanism that simulates the adaptive multistage reasoning process of humans. SafeBehavior decomposes safety evaluation into three stages: intention inference to detect obvious input risks, self introspection to assess generated responses and assign confidence based judgments, and self revision to adaptively rewrite uncertain outputs while preserving user intent and enforcing safety constraints. We extensively evaluate SafeBehavior against five representative jailbreak attack types including optimization based, contextual manipulation, and prompt based attacks and compare it with seven state of the art defense baselines. Experimental results show that SafeBehavior significantly improves robustness and adaptability across diverse threat scenarios, offering an efficient and human inspired approach to safeguarding LLMs against jailbreak attempts.', 'abstract_zh': '大型语言模型（LLMs）在多种自然语言处理任务中取得了令人印象深刻的性能，但其不断增强的能力也放大了潜在风险，如规避内置安全机制的 Jailbreak 攻击。现有的防御措施，包括输入同义重组、多步评估和安全专家模型，往往面临着高计算成本、泛化能力有限或僵化的流程等问题，无法检测到复杂上下文中隐含的微妙恶意意图。受到认知科学中人类决策过程的研究启发，我们提出了一种新的分层 Jailbreak 防护机制 SafeBehavior，模拟人类的适应性多阶段推理过程。SafeBehavior 将安全性评估分解为三个阶段：意图推断以检测明显的输入风险，自我反省以评估生成的响应并基于判断赋予置信度，以及自我修订以适应性重写不确定的输出，同时保留用户意图并遵循安全约束。我们广泛地将 SafeBehavior 与五种代表性 Jailbreak 攻击类型，包括基于优化、上下文操纵和提示的攻击进行了评估，并将其与七个最先进的防御基线进行了比较。实验结果表明，SafeBehavior 显著提高了在各种威胁场景下的鲁棒性和适应性，提供了一种高效且受人类启发的方法，以保护大型语言模型免受 Jailbreak 企图。', 'title_zh': 'SafeBehavior: 模拟人类似的多阶段推理以减轻大型语言模型的 Jailbreak 攻击'}
{'arxiv_id': 'arXiv:2509.26331', 'title': 'AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations', 'authors': 'Berdymyrat Ovezmyradov', 'link': 'https://arxiv.org/abs/2509.26331', 'abstract': 'The rapid advancement of LLMs sparked significant interest in their potential to augment or automate managerial functions. One of the most recent trends in AI benchmarking is performance of Large Language Models (LLMs) over longer time horizons. While LLMs excel at tasks involving natural language and pattern recognition, their capabilities in multi-step, strategic business decision-making remain largely unexplored. Few studies demonstrated how results can be different from benchmarks in short-term tasks, as Vending-Bench revealed. Meanwhile, there is a shortage of alternative benchmarks for long-term coherence. This research analyses a novel benchmark using a business game for the decision making in business. The research contributes to the recent literature on AI by proposing a reproducible, open-access management simulator to the research community for LLM benchmarking. This novel framework is used for evaluating the performance of five leading LLMs available in free online interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes decisions for a simulated retail company. A dynamic, month-by-month management simulation provides transparently in spreadsheet model as experimental environment. In each of twelve months, the LLMs are provided with a structured prompt containing a full business report from the previous period and are tasked with making key strategic decisions: pricing, order size, marketing budget, hiring, dismissal, loans, training expense, R&D expense, sales forecast, income forecast The methodology is designed to compare the LLMs on quantitative metrics: profit, revenue, and market share, and other KPIs. LLM decisions are analyzed in their strategic coherence, adaptability to market changes, and the rationale provided for their decisions. This approach allows to move beyond simple performance metrics for assessment of the long-term decision-making.', 'abstract_zh': 'LLMs在长期决策中的表现：基于商业游戏的新基准研究', 'title_zh': 'AI参与商业博弈：大型语言模型在动态模拟中的决策制定基准研究'}
{'arxiv_id': 'arXiv:2509.26306', 'title': 'Interactive Learning for LLM Reasoning', 'authors': 'Hehai Lin, Shilei Cao, Minzhi Li, Sudong Wang, Haotian Wu, Linyi Yang, Juepeng Zheng, Chengwei Qin', 'link': 'https://arxiv.org/abs/2509.26306', 'abstract': "Existing multi-agent learning approaches have developed interactive training environments to explicitly promote collaboration among multiple Large Language Models (LLMs), thereby constructing stronger multi-agent systems (MAS). However, during inference, they require re-executing the MAS to obtain final solutions, which diverges from human cognition that individuals can enhance their reasoning capabilities through interactions with others and resolve questions independently in the future. To investigate whether multi-agent interaction can enhance LLMs' independent problem-solving ability, we introduce ILR, a novel co-learning framework for MAS that integrates two key components: Dynamic Interaction and Perception Calibration. Specifically, Dynamic Interaction first adaptively selects either cooperative or competitive strategies depending on question difficulty and model ability. LLMs then exchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea Fusion), an innovative interaction paradigm designed to mimic human discussion, before deriving their respective final answers. In Perception Calibration, ILR employs Group Relative Policy Optimization (GRPO) to train LLMs while integrating one LLM's reward distribution characteristics into another's reward function, thereby enhancing the cohesion of multi-agent interactions. We validate ILR on three LLMs across two model families of varying scales, evaluating performance on five mathematical benchmarks and one coding benchmark. Experimental results show that ILR consistently outperforms single-agent learning, yielding an improvement of up to 5% over the strongest baseline. We further discover that Idea3 can enhance the robustness of stronger LLMs during multi-agent inference, and dynamic interaction types can boost multi-agent learning compared to pure cooperative or competitive strategies.", 'abstract_zh': '基于多Agent交互的大型语言模型自主问题解决能力提升框架：ILR', 'title_zh': '交互式学习促进大模型推理'}
{'arxiv_id': 'arXiv:2509.26246', 'title': 'SlimPack: Fine-Grained Asymmetric Packing for Balanced and Efficient Variable-Length LLM Training', 'authors': 'Yuliang Liu, Guohao Wu, Shenglong Zhang, Wei Zhang, Qianchao Zhu, Zhouyang Li, Chenyu Wang', 'link': 'https://arxiv.org/abs/2509.26246', 'abstract': 'The efficient distributed training of Large Language Models (LLMs) is severely hampered by the extreme variance in context lengths. This data heterogeneity, amplified by conventional packing strategies and asymmetric forward-backward costs, leads to critical inefficiencies such as cascading workload imbalances and severe hardware underutilization. Existing solutions attempt to mitigate these challenges, but often at the expense of memory or communication efficiency.\nTo address these challenges, we introduce SlimPack, a framework that fundamentally rethinks data packing and scheduling by decomposing samples into fine-grained slices. This slice-level decomposition immediately mitigates critical memory and communication bottlenecks by transforming large, volatile workloads into a stream of smaller, manageable units. This flexibility is then harnessed for our core innovation, Asymmetric Partitioning, which assembles balanced scheduling units uniquely optimized for the different demands of the forward and backward passes. Orchestrated by a two-phase solver and a high-fidelity simulator, SlimPack holistically resolves imbalances across all parallel dimensions. Extensive experiments demonstrate that SlimPack achieves up to a $2.8\\times$ training throughput improvement over baselines, breaking the conventional trade-off by delivering both superior balance and high resource efficiency.', 'abstract_zh': '大型语言模型的高效分布式训练受到极端上下文长度差异的严重阻碍。这种数据异质性，加上传统的打包策略和不对称的前向-后向计算成本的放大，导致了关键的低效问题，如工作负载不平衡的级联效应和严重的硬件利用率低下。现有解决方案试图缓解这些挑战，但往往以牺牲内存或通信效率为代价。\n为了解决这些挑战，我们引入了SlimPack框架，从根本上重新思考数据打包和调度，通过将样本分解为精细的片段。这种片段级别的分解立即通过将大型、易变的工作负载转换为较小且可管理的单元来缓解关键的内存和通信瓶颈。然后利用这种灵活性进行我们的核心创新——非对称划分，该创新集成了针对前向和后向传递不同需求的独特平衡调度单元。在两阶段求解器和高保真模拟器的协调下，SlimPack全面解决了所有并行维度的不平衡问题。大量实验表明，SlimPack在基线方法上实现了高达2.8倍的训练吞吐量提升，打破了传统的权衡，同时提供了卓越的平衡和高资源效率。', 'title_zh': 'SlimPack：细粒度异构打包以实现高效均衡的变长LLM训练'}
{'arxiv_id': 'arXiv:2509.26209', 'title': 'Diversity-Incentivized Exploration for Versatile Reasoning', 'authors': 'Zican Hu, Shilin Zhang, Yafu Li, Jianhao Yan, Xuyang Hu, Leyang Cui, Xiaoye Qu, Chunlin Chen, Yu Cheng, Zhi Wang', 'link': 'https://arxiv.org/abs/2509.26209', 'abstract': 'Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a crucial paradigm for incentivizing reasoning capabilities in Large Language Models (LLMs). Due to vast state-action spaces and reward sparsity in reasoning tasks, existing methods often struggle with deficient exploration and poor sample efficiency. In the paper, we propose \\textbf{DIVER} (\\textbf{D}iversity-\\textbf{I}ncentivized Exploration for \\textbf{V}ersatil\\textbf{E} \\textbf{R}easoning), an innovative framework that highlights the pivotal role of global sequence-level diversity to incentivize deep exploration for versatile reasoning. We first conduct a primary empirical study to reveal a strong positive correlation between global diversity and reasoning capacity. Building on this insight, we introduce global diversity incentives as an intrinsic reward to promote deep exploration in a semantically structured space. Incorporating the intrinsic reward, we develop a potential-based reward shaping mechanism to preserve optimal policy invariance and design simple heuristics to mitigate possible reward hacking. Experimental results show that DIVER outperforms competitive RLVR baselines with various exploration strategies on both in-domain and out-of-domain tasks, excelling in both Pass@1 and Pass@k evaluations. Our code is available at this https URL.', 'abstract_zh': '可验证奖励的强化学习（RLVR）已经成为激励大型语言模型（LLMs）推理能力的关键范式。由于推理任务中状态-动作空间庞大和奖励稀疏性，现有方法常面临探索不足和样本效率低下的问题。本文提出了一种创新框架 \\textbf{DIVER}（Diversity-Incentivized Exploration for Versatile Reasoning），强调全局序列级别多样性在激励深度探索和多样化推理方面的重要作用。我们首先进行了一项初步的实证研究，揭示了全局多样性和推理能力之间存在强烈的正相关关系。基于这一洞察，我们引入全局多样性激励作为内在奖励，以促进在语义结构化空间中的深度探索。通过整合内在奖励，我们设计了一种基于潜能的奖励塑造机制，以保持最优策略不变性，并设计了简单的策略来缓解潜在的奖励操纵问题。实验结果表明，DIVER 在多种探索策略下均优于各种RLVR基线方法，在领域内和领域外任务上均表现出色，尤其在 Pass@1 和 Pass@k 评估中表现出色。代码可在此链接获取。', 'title_zh': '多样化激励探索以实现灵活推理'}
{'arxiv_id': 'arXiv:2509.26205', 'title': 'Human-Centered Evaluation of RAG outputs: a framework and questionnaire for human-AI collaboration', 'authors': 'Aline Mangold, Kiran Hoffmann', 'link': 'https://arxiv.org/abs/2509.26205', 'abstract': "Retrieval-augmented generation (RAG) systems are increasingly deployed in user-facing applications, yet systematic, human-centered evaluation of their outputs remains underexplored. Building on Gienapp's utility-dimension framework, we designed a human-centred questionnaire that assesses RAG outputs across 12 dimensions. We iteratively refined the questionnaire through several rounds of ratings on a set of query-output pairs and semantic discussions. Ultimately, we incorporated feedback from both a human rater and a human-LLM pair. Results indicate that while large language models (LLMs) reliably focus on metric descriptions and scale labels, they exhibit weaknesses in detecting textual format variations. Humans struggled to focus strictly on metric descriptions and labels. LLM ratings and explanations were viewed as a helpful support, but numeric LLM and human ratings lacked agreement. The final questionnaire extends the initial framework by focusing on user intent, text structuring, and information verifiability.", 'abstract_zh': '基于检索的生成（RAG）系统日益应用于面向用户的应用程序，然而对其输出的人本中心化系统性评估仍鲜有探索。基于Gienapp的效用维度框架，我们设计了一份人本中心问卷，评估RAG输出在12个维度上的表现。我们通过多次轮次对查询-输出对进行评分和语义讨论，不断完善问卷。最终，我们结合了人力评分和人-大模型评分的反馈。结果表明，虽然大型语言模型（LLMs）能可靠地关注度量描述和尺度标签，但在检测文本格式变异方面表现出弱点。人类难以专注于度量描述和标签。大模型评分和解释被视作有益的支持，但数值评分和人类评分缺乏一致性。最终问卷在初始框架基础上，强调用户意图、文本结构和信息可验证性。', 'title_zh': '面向人类中心的人性化评估：人类-AI协作的框架与问卷调查'}
{'arxiv_id': 'arXiv:2509.26201', 'title': 'LLM Agents for Knowledge Discovery in Atomic Layer Processing', 'authors': 'Andreas Werbrouck, Marshall B. Lindsay, Matthew Maschmann, Matthias J. Young', 'link': 'https://arxiv.org/abs/2509.26201', 'abstract': "Large Language Models (LLMs) have garnered significant attention for several years now. Recently, their use as independently reasoning agents has been proposed. In this work, we test the potential of such agents for knowledge discovery in materials science. We repurpose LangGraph's tool functionality to supply agents with a black box function to interrogate. In contrast to process optimization or performing specific, user-defined tasks, knowledge discovery consists of freely exploring the system, posing and verifying statements about the behavior of this black box, with the sole objective of generating and verifying generalizable statements. We provide proof of concept for this approach through a children's parlor game, demonstrating the role of trial-and-error and persistence in knowledge discovery, and the strong path-dependence of results. We then apply the same strategy to show that LLM agents can explore, discover, and exploit diverse chemical interactions in an advanced Atomic Layer Processing reactor simulation using intentionally limited probe capabilities without explicit instructions.", 'abstract_zh': '大规模语言模型（LLMs）在过去几年中引起了广泛关注。近年来，它们作为独立推理代理的应用被提出。在本文中，我们测试了这类代理在材料科学中的知识发现潜力。我们重新利用LangGraph的工具功能，为代理提供一个黑箱函数以进行问询。与过程优化或执行特定的用户定义任务不同，知识发现包括自由探索系统，针对黑箱的行为提出并验证声明，其唯一目标是生成和验证可泛化的声明。我们通过一个儿童桌游展示此方法的概念证明，展示了试错和坚持在知识发现中的作用，以及结果的强烈路径依赖性。然后，我们采用相同策略，通过故意限制探针能力但在没有明确指示的情况下探索、发现和利用高级原子层处理反应器模拟中的多样化化学相互作用，进一步验证了LLM代理的能力。', 'title_zh': '基于原子层处理的知识发现大语言模型代理'}
{'arxiv_id': 'arXiv:2509.26167', 'title': "'Too much alignment; not enough culture': Re-balancing cultural alignment practices in LLMs", 'authors': 'Eric J. W. Orlowski, Hakim Norhashim, Tristan Koh Ly Wey', 'link': 'https://arxiv.org/abs/2509.26167', 'abstract': 'While cultural alignment has increasingly become a focal point within AI research, current approaches relying predominantly on quantitative benchmarks and simplistic proxies fail to capture the deeply nuanced and context-dependent nature of human cultures. Existing alignment practices typically reduce culture to static demographic categories or superficial cultural facts, thereby sidestepping critical questions about what it truly means to be culturally aligned. This paper argues for a fundamental shift towards integrating interpretive qualitative approaches drawn from social sciences into AI alignment practices, specifically in the context of Large Language Models (LLMs). Drawing inspiration from Clifford Geertz\'s concept of "thick description," we propose that AI systems must produce outputs that reflect deeper cultural meanings--what we term "thick outputs"-grounded firmly in user-provided context and intent. We outline three necessary conditions for successful cultural alignment: sufficiently scoped cultural representations, the capacity for nuanced outputs, and the anchoring of outputs in the cultural contexts implied within prompts. Finally, we call for cross-disciplinary collaboration and the adoption of qualitative, ethnographic evaluation methods as vital steps toward developing AI systems that are genuinely culturally sensitive, ethically responsible, and reflective of human complexity.', 'abstract_zh': '文化融入视角下大型语言模型的实质化对齐：跨学科合作与质性评价方法', 'title_zh': '“过度对齐；不足的文化”：重新平衡LLM中的文化对齐实践'}
{'arxiv_id': 'arXiv:2509.26153', 'title': 'Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical Practice', 'authors': 'Jack Gallifant, Katherine C. Kellogg, Matt Butler, Amanda Centi, Patrick F. Doyle, Sayon Dutta, Joyce Guo, Matthew J. Hadfield, Esther H. Kim, David E. Kozono, Hugo JWL Aerts, Adam B. Landman, Raymond H. Mak, Rebecca G. Mishuris, Tanna L. Nelson, Guergana K. Savova, Elad Sharon, Benjamin C. Silverman, Umit Topaloglu, Jeremy L. Warner, Danielle S. Bitterman', 'link': 'https://arxiv.org/abs/2509.26153', 'abstract': 'Large language models (LLMs) integrated into agent-driven workflows hold immense promise for healthcare, yet a significant gap exists between their potential and practical implementation within clinical settings. To address this, we present a practitioner-oriented field manual for deploying generative agents that use electronic health record (EHR) data. This guide is informed by our experience deploying the "irAE-Agent", an automated system to detect immune-related adverse events from clinical notes at Mass General Brigham, and by structured interviews with 20 clinicians, engineers, and informatics leaders involved in the project. Our analysis reveals a critical misalignment in clinical AI development: less than 20% of our effort was dedicated to prompt engineering and model development, while over 80% was consumed by the sociotechnical work of implementation. We distill this effort into five "heavy lifts": data integration, model validation, ensuring economic value, managing system drift, and governance. By providing actionable solutions for each of these challenges, this field manual shifts the focus from algorithmic development to the essential infrastructure and implementation work required to bridge the "valley of death" and successfully translate generative AI from pilot projects into routine clinical care.', 'abstract_zh': '大型语言模型（LLMs）集成到由代理驱动的工作流中在医疗保健领域具有巨大潜力，但在临床环境中的潜在应用和实际实施之间存在显著差距。为了解决这一问题，我们提供了一本面向实践者的操作手册，用于部署使用电子健康记录（EHR）数据的生成型代理。该指南借鉴了我们部署“irAE-Agent”的经验，该系统用于在马萨诸塞州总医院及其合作伙伴处从临床笔记中自动检测免疫相关不良事件，并借鉴了与20名参与该项目的临床医生、工程师和信息技术领导者进行的结构化访谈。我们的分析揭示了临床AI开发中的一个关键错配：我们在提示工程和模型开发方面投入的努力不到20%，而超过80%的努力则用于实施的社技工作。我们将这些努力提炼为五个“重大的任务”：数据集成、模型验证、确保经济价值、管理系统漂移和治理。通过为每一个挑战提供可行的解决方案，这份操作手册将重点从算法开发转向实现生成型AI从试点项目顺利过渡到常规临床护理所需的必要基础设施和实施工作。', 'title_zh': '超越算法：临床实践中共启人工智能代理的应用指南'}
{'arxiv_id': 'arXiv:2509.26128', 'title': 'MEDAKA: Construction of Biomedical Knowledge Graphs Using Large Language Models', 'authors': 'Asmita Sengupta, David Antony Selby, Sebastian Josef Vollmer, Gerrit Großmann', 'link': 'https://arxiv.org/abs/2509.26128', 'abstract': 'Knowledge graphs (KGs) are increasingly used to represent biomedical information in structured, interpretable formats. However, existing biomedical KGs often focus narrowly on molecular interactions or adverse events, overlooking the rich data found in drug leaflets. In this work, we present (1) a hackable, end-to-end pipeline to create KGs from unstructured online content using a web scraper and an LLM; and (2) a curated dataset, MEDAKA, generated by applying this method to publicly available drug leaflets. The dataset captures clinically relevant attributes such as side effects, warnings, contraindications, ingredients, dosage guidelines, storage instructions and physical characteristics. We evaluate it through manual inspection and with an LLM-as-a-Judge framework, and compare its coverage with existing biomedical KGs and databases. We expect MEDAKA to support tasks such as patient safety monitoring and drug recommendation. The pipeline can also be used for constructing KGs from unstructured texts in other domains. Code and dataset are available at this https URL.', 'abstract_zh': '知识图谱（KGs）在结构化和可解释的形式中被越来越多地用于表示 biomedical 信息。然而，现有的 biomedical KGs 经常狭隘地关注分子相互作用或不良事件，忽略了药物说明书中的丰富数据。在本文中，我们提出了（1）一个可扩展的端到端管道，通过网页爬虫和大语言模型从未结构化的在线内容创建 KGs；以及（2）一个经过精心编目的数据集 MEDAKA，该数据集是通过对公开的药物说明书应用此方法生成的。该数据集捕获了如副作用、警告、禁忌症、成分、剂量指南、储存说明和物理特性等临床相关的属性。我们通过人工检查和大语言模型作为法官的框架对其进行评估，并将其覆盖范围与现有的 biomedical KGs 和数据库进行比较。我们期望 MEDAKA 能够支持患者安全监控和药物推荐等任务。该管道还可用于其他领域未结构化文本的 KG 构建。代码和数据集可在以下网址获得。', 'title_zh': 'MEDAKA：使用大规模语言模型构建生物医学知识图谱'}
{'arxiv_id': 'arXiv:2509.26100', 'title': 'SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs', 'authors': 'Yixu Wang, Xin Wang, Yang Yao, Xinyuan Li, Yan Teng, Xingjun Ma, Yingchun Wang', 'link': 'https://arxiv.org/abs/2509.26100', 'abstract': "The rapid integration of Large Language Models (LLMs) into high-stakes domains necessitates reliable safety and compliance evaluation. However, existing static benchmarks are ill-equipped to address the dynamic nature of AI risks and evolving regulations, creating a critical safety gap. This paper introduces a new paradigm of agentic safety evaluation, reframing evaluation as a continuous and self-evolving process rather than a one-time audit. We then propose a novel multi-agent framework SafeEvalAgent, which autonomously ingests unstructured policy documents to generate and perpetually evolve a comprehensive safety benchmark. SafeEvalAgent leverages a synergistic pipeline of specialized agents and incorporates a Self-evolving Evaluation loop, where the system learns from evaluation results to craft progressively more sophisticated and targeted test cases. Our experiments demonstrate the effectiveness of SafeEvalAgent, showing a consistent decline in model safety as the evaluation hardens. For instance, GPT-5's safety rate on the EU AI Act drops from 72.50% to 36.36% over successive iterations. These findings reveal the limitations of static assessments and highlight our framework's ability to uncover deep vulnerabilities missed by traditional methods, underscoring the urgent need for dynamic evaluation ecosystems to ensure the safe and responsible deployment of advanced AI.", 'abstract_zh': '大型语言模型快速融入高风险领域需要可靠的安全和合规评估。然而，现有的静态基准不足以为人工智能风险的动态性质和不断演变的法规提供解决方案，从而形成重要的安全缺口。本文提出了一种新的主动安全评估范式，重新定义评估为一个持续且自我进化的过程，而非一次性审核。我们随后提出了一种新的多智能体框架SafeEvalAgent，该框架自主地摄取无结构化的政策文件，生成并不断进化一个全面的安全基准。SafeEvalAgent利用专门智能体的协同工作流程，并嵌入一个自我进化的评估循环，其中系统从评估结果中学习，逐步制定更加复杂和针对性的测试案例。我们的实验表明SafeEvalAgent的有效性，显示随着评估标准的严格化，模型安全率持续下降。例如，GPT-5在欧盟人工智能法案中的安全率从72.50%下降到36.36%。这些发现揭示了静态评估的局限性，并突显了我们框架在传统方法中未能发现深层次漏洞的能力，强调了构建动态评估生态系统以确保先进人工智能的安全和负责任部署的迫切需求。', 'title_zh': 'SafeEvalAgent: 朝向具有代理性和自我进化能力的LLM安全评估'}
{'arxiv_id': 'arXiv:2509.26080', 'title': 'Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research', 'authors': 'Emma Rose Madden', 'link': 'https://arxiv.org/abs/2509.26080', 'abstract': 'Large Language Models (LLMs) are being increasingly used as synthetic agents in social science, in applications ranging from augmenting survey responses to powering multi-agent simulations. Because strong prediction plus conditioning prompts, token log-probs, and repeated sampling mimic Bayesian workflows, their outputs can be misinterpreted as posterior-like evidence from a coherent model. However, prediction does not equate to probabilism, and accurate points do not imply calibrated uncertainty. This paper outlines cautions that should be taken when interpreting LLM outputs and proposes a pragmatic reframing for the social sciences in which LLMs are used as high-capacity pattern matchers for quasi-predictive interpolation under explicit scope conditions and not as substitutes for probabilistic inference. Practical guardrails such as independent draws, preregistered human baselines, reliability-aware validation, and subgroup calibration, are introduced so that researchers may engage in useful prototyping and forecasting while avoiding category errors.', 'abstract_zh': '大型语言模型（LLMs）在社会科学中的应用日益增多，从增强调查响应到驱动多智能体模拟。由于强大的预测加上条件提示、令牌对数概率和重复采样的方式模仿了贝叶斯流程，其输出可能会被误认为是来自一致模型的后验似然证据。然而，预测并不等同于概率主义，准确的点估计并不意味着准确的不确定性校准。本文提出了在解释LLM输出时应采取的谨慎措施，并建议在社会科学研究中将LLMs重新定义为在明确范围条件下作为高容量模式匹配器进行准预测插值的工具，而不是概率推理的替代品。介绍了诸如独立抽样、预先登记的人类基线、可靠性意识验证和亚组校准等实用措施，以使研究人员在有用的设计和预测中避免类别错误。', 'title_zh': '评估大型语言模型作为合成社会代理在社会科学研究中的应用'}
{'arxiv_id': 'arXiv:2509.26037', 'title': 'CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search', 'authors': 'Zhe Li, Zhiwei Lin, Yongtao Wang', 'link': 'https://arxiv.org/abs/2509.26037', 'abstract': "The integration of Large Language Models (LLMs) with Neural Architecture Search (NAS) has introduced new possibilities for automating the design of neural architectures. However, most existing methods face critical limitations, including architectural invalidity, computational inefficiency, and inferior performance compared to traditional NAS. In this work, we present Collaborative LLM-based NAS (CoLLM-NAS), a two-stage NAS framework with knowledge-guided search driven by two complementary LLMs. Specifically, we propose a Navigator LLM to guide search direction and a Generator LLM to synthesize high-quality candidates, with a dedicated Coordinator module to manage their interaction. CoLLM-NAS efficiently guides the search process by combining LLMs' inherent knowledge of structured neural architectures with progressive knowledge from iterative feedback and historical trajectory. Experimental results on ImageNet and NAS-Bench-201 show that CoLLM-NAS surpasses existing NAS methods and conventional search algorithms, achieving new state-of-the-art results. Furthermore, CoLLM-NAS consistently enhances the performance and efficiency of various two-stage NAS methods (e.g., OFA, SPOS, and AutoFormer) across diverse search spaces (e.g., MobileNet, ShuffleNet, and AutoFormer), demonstrating its excellent generalization.", 'abstract_zh': '基于大型语言模型的协作神经架构搜索（CoLLM-NAS）：一种由两个互补的LLM驱动的知识导向的两阶段NAS框架', 'title_zh': 'CoLLM-NAS：协作型大型语言模型 Efficient 知识引导的神经架构搜索'}
{'arxiv_id': 'arXiv:2509.25973', 'title': 'Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions', 'authors': 'Junbeom Kim, Kyuyoung Kim, Jihoon Tack, Dongha Lim, Jinwoo Shin', 'link': 'https://arxiv.org/abs/2509.25973', 'abstract': 'Language models trained on web-scale corpora risk memorizing and exposing sensitive information, prompting the need for effective machine unlearning. Prior methods mainly focus on input queries to suppress sensitive outputs, yet this often fails to eliminate the underlying knowledge and limits scalability. To address this, we propose Corrective Unlearning with Retrieved Exclusions (CURE), a novel unlearning framework that verifies model outputs for leakage and revises them into safe responses. Specifically, CURE employs a lightweight corrector that is applied to the original model to verify whether outputs contain target knowledge and to rewrite them if any leakage is detected. To efficiently handle large-scale unlearning requests, CURE retrieves unlearning targets that are relevant to the initial response and provides them as in-context references to the corrector for detection and conditional revision. By leveraging this retrieval augmentation, the corrector can adapt to new unlearning requests without additional training. Extensive evaluations demonstrate that CURE substantially reduces information leakage, even from indirect queries where prior works fall short, while maintaining response quality and general utility. Moreover, it demonstrates robustness under continual unlearning scenarios, making it practical for real-world applications.', 'abstract_zh': '大规模网络语料库训练的语言模型存在泄露敏感信息的风险， necessitating 有效的机器遗忘机制。现有方法主要针对输入查询以抑制敏感输出，但这种方法 often 无法完全消除潜在知识并且限制了可扩展性。为解决这一问题，我们提出了检索排除辅助纠正遗忘（CURE）这一新颖的遗忘框架，用于验证模型输出是否存在泄露并将其纠正为安全响应。具体而言，CURE 使用一个轻量级的纠错器，将其应用于原始模型以验证输出是否包含目标知识，如果检测到泄露，则对其进行重写。为了高效处理大规模遗忘请求，CURE 会检索与初始响应相关联的遗忘目标，并将它们作为上下文参考提供给纠错器进行检测和有条件纠正。通过利用这种检索增强，纠错器可以在无需额外训练的情况下适应新的遗忘请求。广泛评估表明，CURE 显著减少了信息泄露，特别是在前人工作无法处理的间接查询中，同时保持了响应质量和一般用途。此外，它在持续遗忘场景中表现出 robustness，使其适用于实际应用。', 'title_zh': '可扩展且稳健的LLM去训练：通过检索排除信息修正响应'}
{'arxiv_id': 'arXiv:2509.25958', 'title': 'RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning', 'authors': 'Gang Li, Yulei Qin, Xiaoyu Tan, Dingkang Yang, Yuchen Shi, Zihan Xu, Xiang Li, Xing Sun, Ke Li', 'link': 'https://arxiv.org/abs/2509.25958', 'abstract': 'Reinforcement learning with verifiable rewards (RLVR) has proven effective in eliciting complex reasoning in large language models (LLMs). However, standard RLVR training often leads to excessively verbose processes (in reasoning tasks) and inefficient exploration trajectories (in agentic settings), as outcome-only rewards provide no incentive for efficiency and the high variance in response length within relatively small rollout groups results in noisy optimization signals. To address this, we propose Rollout Response Recomposition (RoRecomp), a plug-and-play method that guides models toward concise reasoning by strategically recomposing the training data. RoRecomp separates responses into two distinct batch types: 1) priority batches, which combine short-correct and long-incorrect responses selected from online batches to provide a clear gradient signal for brevity, and 2) compensation batches, which utilize remaining responses from a replay buffer to maintain stability and prevent model collapse. To comprehensively evaluate effectiveness, we test RoRecomp across three settings where results demonstrate substantial efficiency gains: reducing reasoning length by 27.7% in zero RL training, reducing unnecessary tool calls by 46.8% while improving accuracy in agentic RL, and achieving up to 52.5% length reduction in thinking compression, all with minimal performance impact.', 'abstract_zh': '可验证奖励的强化学习（RLVR）在大型语言模型中引发了复杂推理的有效性。然而，标准的RLVR训练往往导致推理任务中过度冗长的过程和代理环境中低效的探索轨迹，因为仅基于结果的奖励无法激励效率，并且在相对较小的滚出组中响应长度的高方差导致了嘈杂的优化信号。为了应对这一问题，我们提出了一种插即用方法——Rollout Response Recomposition（RoRecomp），该方法通过战略性重组训练数据引导模型向简洁的推理发展。RoRecomp 将响应分为两类不同的批次：1）优先批次，该批次结合了来自在线批次的短正确和长错误响应，以提供简洁性的清晰梯度信号；2）补偿批次，该批次利用重放缓冲区中的剩余响应以保持稳定性和防止模型崩溃。为了全面评估其效果，我们在三个场景中测试了RoRecomp，结果表明其在效率上取得了显著提升：在零强化学习训练中减少了27.7%的推理长度，在代理强化学习中减少了46.8%的不必要的工具调用同时提高了准确性，并在思考压缩中达到了最高52.5%的长度减少，且对性能的影响极小。', 'title_zh': 'RoRecomp: 基于 rollout 响应重组增强 reinforcement learning 的推理效率'}
{'arxiv_id': 'arXiv:2509.25941', 'title': 'Boosting Process-Correct CoT Reasoning by Modeling Solvability of Multiple-Choice QA', 'authors': 'Raphael Schumann, Stefan Riezler', 'link': 'https://arxiv.org/abs/2509.25941', 'abstract': 'Reasoning quality in large language models depends not only on producing correct answers but also on generating valid intermediate steps. We study this through multiple-choice question answering (MCQA), which provides a controlled setting with fixed answer options. Our analysis shows that when questions are effectively unsolvable for a model, spurious chains of thought (CoTs) are more likely to appear, leading to false positives. By estimating the solvability of each question, we uncover an intermediate regime where learning is most effective. Building on this insight, we adapt outcome-supervised reward models and reinforcement learning with group-relative advantage to incorporate solvability into their objectives. Across experiments on math and multimodal datasets, these modifications consistently yield higher rates of process-correct reasoning and, in reinforcement learning, improved answer accuracy as well. Our results highlight solvability as a key factor for reducing hallucinations and increasing reliability in CoT reasoning.', 'abstract_zh': '大型语言模型的推理质量不仅取决于产生正确的答案，还取决于生成有效的中间步骤。我们通过多项选择题解答（MCQA）研究这一问题，这为固定答案选项的可控环境提供了条件。我们的分析表明，当问题对模型来说无法有效求解时，虚假的思维链（CoTs）更有可能出现，导致误报。通过估计每个问题的可求解性，我们发现了学习效果最佳的一个中间区段。基于这一发现，我们调整了结果监督的奖励模型，并将基于组相对优势的强化学习与可求解性整合进它们的目标中。在对数学和多模态数据集的实验中，这些修改始终提高了推理过程正确率，而在强化学习中也提高了答案准确性。我们的研究结果强调可求解性是减少幻觉并提高CoTs推理可靠性的关键因素。', 'title_zh': '通过建模多选问答的可解性提升过程正确性的解释链推理'}
{'arxiv_id': 'arXiv:2509.25922', 'title': 'DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models', 'authors': 'Zhicheng Zhou, Jing Li, Suming Qiu, Junjie Huang, Linyuan Qiu, Zhijie Sun', 'link': 'https://arxiv.org/abs/2509.25922', 'abstract': "The internet is saturated with low-density, high-redundancy information, such as social media comments, repetitive news, and lengthy discussions, making it difficult to extract valuable insights efficiently. Multi-layer nested JSON structures provide an effective solution by compressing such information into semantically rich, hierarchical representations, which organize data into key-value pairs, arrays, and nested objects, preserving contextual relationships and enabling efficient storage, retrieval, and semantic querying. For instance, in news aggregation, a JSON object can nest an article's metadata (title, author, date), content (text, multimedia), and multimedia information (multimedia type, caption) hierarchically. Large Language Models (LLMs) play a transformative role in web data mining by parsing unstructured text and outputting structured results directly into complex JSON schemas. However, current benchmarks for evaluating LLMs' JSON output capabilities overemphasize pure JSON generation rather than assessing data comprehension and extraction abilities, a limitation that lacks relevance to practical web data mining tasks. To address this, we introduce DeepJSONEval, a novel benchmark featuring 2100 multi-domain instances with deep nested structures, categorized by difficulty. Experiments show significant performance gaps among LLMs in handling such complexity. Our benchmark and datasets are open-sourced to advance research in structured JSON generation.(this https URL).", 'abstract_zh': '互联网充斥着低密度、高冗余的信息，如社交媒体评论、重复新闻和冗长讨论，这使得高效提取有价值见解变得困难。多层嵌套的JSON结构提供了一种有效的解决方案，通过将此类信息压缩为语义丰富、层次化的表示形式，组织数据为键值对、数组和嵌套对象，从而保留上下文关系并实现高效存储、检索和语义查询。例如，在新闻聚合中，一个JSON对象可以将一篇文章的元数据（标题、作者、日期）、内容（文本、多媒体）和多媒体信息（多媒体类型、说明）层次化地组织起来。大规模语言模型（LLMs）在网页数据挖掘中扮演着变革性的角色，通过解析无结构文本并直接输出复杂JSON模式的结果。然而，目前用于评估LLMs的JSON输出能力的基准过度强调纯JSON生成，而不是评估数据理解和提取能力，这一局限性与实际网页数据挖掘任务的相关性较低。为解决这一问题，我们引入了DeepJSONEval，这是一种新型基准，包含2100个跨领域实例，按难度分类。实验表明，在处理这种复杂性方面，LLMs之间的性能差异显著。我们的基准和数据集已开源，旨在推动结构化JSON生成的研究。(this https URL)。', 'title_zh': 'DeepJSONEval：大规模语言模型复杂嵌套JSON数据挖掘的基准测试'}
{'arxiv_id': 'arXiv:2509.25873', 'title': 'Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs', 'authors': 'Hankun Dai, Maoquan Wang, Mengnan Qi, Yikai Zhang, Zijian Jin, Yongqiang Yao, Yufan Huang, Shengyu Fu, Elsie Nallipogu', 'link': 'https://arxiv.org/abs/2509.25873', 'abstract': "Large language models (LLMs) are increasingly being applied to programming tasks, ranging from single-turn code completion to autonomous agents. Current code agent designs frequently depend on complex, hand-crafted workflows and tool sets. However, this reliance on elaborate scaffolding presents several challenges: agent performance becomes overly dependent on prompt tuning and custom design choices, heavy human intervention obscures a model's true underlying capabilities, and intricate pipelines are costly to build and maintain. Furthermore, optimizing complex task prompts increases the risk of data leakage. Currently, when introducing new models, LLM providers like OpenAI and Anthropic often publish benchmark scores to demonstrate their models' coding proficiency, but keep their proprietary evaluation frameworks confidential. To address these limitations, we introduce Lita (Lite Agent), which operationalizes liteness, a principle of minimizing manual design while retaining the essential elements of a fully autonomous agent. Lita enables a more faithful and unified evaluation without elaborate scaffolding. Experiments on the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita achieves competitive or superior performance compared to workflow-based and agentic baselines. Crucially, Lita also consumes fewer tokens and requires significantly less design effort. Our results suggest that Lita is sufficient to reveal the underlying coding competence of modern LLMs. Finally, we propose the Agent Complexity Law: the performance gap between agents of varying complexity, from simple to sophisticated designs, will shrink as the core model improves, ultimately converging to a negligible difference.", 'abstract_zh': '大型语言模型（LLMs）在编程任务中的应用越来越广泛，从单轮代码补全到自主代理。当前的代码代理设计通常依赖于复杂的、手工构建的工作流程和工具集。然而，这种对复杂支架的依赖提出了几个挑战：代理性能过度依赖于提示调优和定制设计选择，大量的人工干预掩盖了模型的真实潜在能力，复杂的流水线构建和维护成本高昂。此外，优化复杂的任务提示增加了数据泄漏的风险。目前，当引入新模型时，如OpenAI和Anthropic这样的LLM提供商经常发布基准得分来展示其模型的编程能力，但保密其专有评估框架。为了解决这些局限性，我们引入了Lita（Lite Agent），这一体现了简约原则，即在保留完整自主代理核心要素的同时，最大限度减少人工设计的需要。Lita能够实现更忠实且统一的评估，无需复杂的支架。在Aider Polyglot和SWE-Bench上的前沿模型实验表明，Lita在与基于工作流和代理基准相比时，实现了竞争力或更优的性能。最重要的是，Lita消耗的令牌更少，设计工作量显著减少。我们的结果显示，Lita足以揭示现代LLM的潜在编程能力。最后，我们提出了代理复杂度定律：从简单的到复杂的不同设计的代理性能差距将随着核心模型的改进而缩小，最终收敛到一个可以忽略的差异。', 'title_zh': 'Lita: 灵活代理揭示大语言模型的代理编码能力'}
{'arxiv_id': 'arXiv:2509.25843', 'title': 'ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack', 'authors': 'Yein Park, Jungwoo Park, Jaewoo Kang', 'link': 'https://arxiv.org/abs/2509.25843', 'abstract': 'Large language models (LLMs), despite being safety-aligned, exhibit brittle refusal behaviors that can be circumvented by simple linguistic changes. As tense jailbreaking demonstrates that models refusing harmful requests often comply when rephrased in past tense, a critical generalization gap is revealed in current alignment methods whose underlying mechanisms are poorly understood. In this work, we introduce Activation-Scaling Guard (ASGuard), an insightful, mechanistically-informed framework that surgically mitigates this specific vulnerability. For the first step, we use circuit analysis to identify the specific attention heads causally linked to the targeted jailbreaking, the tense-changing attack. Second, we train a precise, channel-wise scaling vector to recalibrate the activation of tense vulnerable heads. Lastly, we apply it into a "preventative fine-tuning", forcing the model to learn a more robust refusal mechanism. Across three LLMs, ASGuard effectively reduces the attack success rate of targeted jailbreaking while preserving general capabilities and minimizing over refusal, achieving a Pareto-optimal balance between safety and utility. Our findings underscore how adversarial suffixes suppress the propagation of the refusal-mediating direction, based on mechanistic analysis. Furthermore, our work showcases how a deep understanding of model internals can be leveraged to develop practical, efficient, and targeted methods for adjusting model behavior, charting a course for more reliable and interpretable AI safety.', 'abstract_zh': '大型语言模型（LLMs）尽管已安全对齐，但在面对简单的语言变化时会表现出脆弱的拒绝行为。时态解禁表明，模型对有害请求的拒绝往往可以在将其重新表述为过去时后被规避，这揭示了当前对齐方法中存在的关键泛化缺口，其潜在机制尚不明确。本文引入了激活缩放防护（ASGuard）框架，这是一种洞察力强、机制导向的方案，针对这种特定脆弱性进行了外科手术式的缓解。首先，我们使用电路分析来识别与目标解禁攻击（时态变化攻击）因果相关的特定注意力头。其次，我们训练了一个精确的通道缩放向量，以重新校准脆弱的时态头的激活。最后，我们将这一方法应用于一种“预防性微调”，促使模型学习更 robust 的拒绝机制。在三种大型语言模型上，ASGuard有效地降低了目标解禁攻击的成功率，同时保持了通用能力并最大限度地减少了过度拒绝，实现了安全性与实用性之间的帕累托最优平衡。我们的研究结果基于机制分析，强调了对抗后缀如何抑制拒绝信息传播。此外，我们的工作展示了深入了解模型内部机制如何被用于开发实际、高效且有针对性的方法，以调整模型行为，为更可靠和可解释的AI安全指明了方向。', 'title_zh': 'ASGuard: 激活缩放保护以缓解针对性越狱攻击'}
{'arxiv_id': 'arXiv:2509.25842', 'title': 'HiStyle: Hierarchical Style Embedding Predictor for Text-Prompt-Guided Controllable Speech Synthesis', 'authors': 'Ziyu Zhang, Hanzhao Li, Jingbin Hu, Wenhao Li, Lei Xie', 'link': 'https://arxiv.org/abs/2509.25842', 'abstract': 'Controllable speech synthesis refers to the precise control of speaking style by manipulating specific prosodic and paralinguistic attributes, such as gender, volume, speech rate, pitch, and pitch fluctuation. With the integration of advanced generative models, particularly large language models (LLMs) and diffusion models, controllable text-to-speech (TTS) systems have increasingly transitioned from label-based control to natural language description-based control, which is typically implemented by predicting global style embeddings from textual prompts. However, this straightforward prediction overlooks the underlying distribution of the style embeddings, which may hinder the full potential of controllable TTS systems. In this study, we use t-SNE analysis to visualize and analyze the global style embedding distribution of various mainstream TTS systems, revealing a clear hierarchical clustering pattern: embeddings first cluster by timbre and subsequently subdivide into finer clusters based on style attributes. Based on this observation, we propose HiStyle, a two-stage style embedding predictor that hierarchically predicts style embeddings conditioned on textual prompts, and further incorporate contrastive learning to help align the text and audio embedding spaces. Additionally, we propose a style annotation strategy that leverages the complementary strengths of statistical methodologies and human auditory preferences to generate more accurate and perceptually consistent textual prompts for style control. Comprehensive experiments demonstrate that when applied to the base TTS model, HiStyle achieves significantly better style controllability than alternative style embedding predicting approaches while preserving high speech quality in terms of naturalness and intelligibility. Audio samples are available at this https URL.', 'abstract_zh': '可控语音合成指的是通过操控特定的语音和副语言属性（如性别、音量、语速、音高和音高变化）来精确控制说话风格。随着先进生成模型，特别是大规模语言模型（LLMs）和扩散模型的集成，基于文本的语音合成（TTS）系统从标注控制逐渐转向基于自然语言描述的控制，通常通过从文本提示中预测全局风格嵌入来实现。然而，这种直接预测忽略了风格嵌入的潜在分布，这可能阻碍了可控TTS系统的充分发挥潜力。在本研究中，我们使用t-SNE分析可视化并分析了各种主流TTS系统的全局风格嵌入分布，发现了一个清晰的层次聚类模式：嵌入首先按音色聚类，然后进一步按风格属性细分为更精细的类别。基于此观察，我们提出了HiStyle，这是一种两阶段风格嵌入预测器，基于文本提示层次预测风格嵌入，并进一步引入对比学习以帮助对齐文本和音频嵌入空间。此外，我们提出了一种风格注释策略，该策略结合了统计方法和人类听觉偏好的优势，以生成更准确和感知一致的文本提示来控制风格。综合实验表明，当应用于基本TTS模型时，HiStyle在保持自然度和可懂度的同时，相比于其他风格嵌入预测方法，实现了显著更好的风格可控性。音频样本可在以下链接获取：this https URL。', 'title_zh': 'HiStyle：层次风格嵌入预测器及其在文本提示引导可控语音合成中的应用'}
{'arxiv_id': 'arXiv:2509.25835', 'title': 'Chain-in-Tree: Back to Sequential Reasoning in LLM Tree Search', 'authors': 'Xinzhe Li', 'link': 'https://arxiv.org/abs/2509.25835', 'abstract': 'Test-time scaling enables large language models (LLMs) to improve performance on long-horizon reasoning tasks by allocating additional compute at inference. Tree-search-based approaches achieve state-of-the-art results in this setting, but they are notoriously inefficient, often an order of magnitude slower than simpler iterative methods. We introduce Chain-in-Tree (CiT), a plug-in framework that adaptively decides when to branch during search rather than branching at every step. CiT relies on lightweight Branching Necessity (BN) evaluation methods: BN-DP (Direct Prompting), where an auxiliary LLM directly judges whether a step requires branching, and BN-SC (Self-Consistency), which clusters multiple candidate actions to estimate agreement. We integrate CiT into three representative LLM-in-the-loop tree search frameworks: Tree of Thoughts (ToT-BS), ReST-MCTS, and RAP, and evaluate across GSM8K and Math500. Our results show that: (1) BN-DP consistently reduces token generation, model invocations, and runtime by 75-85 percent across all settings, with negligible accuracy loss and sometimes accuracy gains; (2) BN-SC typically yields substantial savings (up to 80 percent) but shows instability in 1-4 out of 14 settings, caused by a small subset of examples that produce very long reasoning steps; (3) the quality of auxiliary LLMs is critical, not only the BN evaluator in BN-DP, but also the models used in BN-SC for clustering and equivalence checking. When these roles are filled by smaller LLMs, performance degrades. Importantly, BN-SC does not require LLMs in domains with deterministic action spaces, where clustering can be done programmatically. We also provide a theoretical guarantee that BN-DP never increases LLM invocations relative to the baseline and release a unified implementation of CiT across ToT-BS, ReST-MCTS, and RAP to facilitate reproducibility and extension.', 'abstract_zh': 'Test-time Scaling使大规模语言模型在长期推理任务上的性能提升通过在推理时分配额外的计算资源。基于树搜索的方法在这种情境下达到了最先进的效果，但它们通常比简单的迭代方法慢一个数量级。我们提出了Chain-in-Tree (CiT)插件框架，在搜索过程中适应性地决定何时分支，而非在每一步都进行分支。CiT依赖于轻量级的分支必要性(BN)评估方法：BN-DP（直接提示），其中辅助语言模型直接判断是否需要分支，以及BN-SC（自我一致性），它将多个候选动作聚类以估计一致性。我们将CiT集成到三个代表性的LLM在环中的树搜索框架中：Thought Tree (ToT-BS)、ReST-MCTS和RAP，并在GSM8K和Math500上进行评估。结果显示：(1) BN-DP在所有情境下一致地减少了约75-85%的标记生成、模型调用和运行时间，几乎无准确率损失，并且有时还能提高准确率；(2) BN-SC通常会带来显著的节省（最高可达80%），但在1-4个情境中表现出不稳定性，原因是少数例子会产生非常长的推理步骤；(3) 辅助语言模型的质量至关重要，不仅影响BN-DP中的BN评估器，还影响BN-SC中用于聚类和等价性检查的模型。当这些角色由较小的语言模型担任时，性能会下降。重要的是，BN-SC在具有确定性动作空间的领域无需使用语言模型，因为聚类可以通过编程实现。我们还提供了一个理论保证，即BN-DP从不失去相对基准模型的语言模型调用次数，并在ToT-BS、ReST-MCTS和RAP上提供了一个统一的CiT实现，以促进可再现性和扩展。', 'title_zh': '树中链：回归LLM树搜索中的顺序推理'}
{'arxiv_id': 'arXiv:2509.25779', 'title': 'Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs', 'authors': 'Siyu Zhu, Yanbin Jiang, Hejian Sang, Shao Tang, Qingquan Song, Biao He, Rohit Jain, Zhipeng Wang, Alborz Geramifard', 'link': 'https://arxiv.org/abs/2509.25779', 'abstract': "We investigated Agentic RL with large language models on the \\textsc{TravelPlanner} benchmark. Our approach, \\textsc{Planner-R1}, achieved a \\textbf{56.9\\%} final-pass rate with only 180 training queries, a $2.7\\times$ improvement over GPT-5's $21.2\\%$ baseline and the strongest agentic result on the public leaderboard. A central finding was that smaller models (8B) were highly responsive to reward shaping: with dense process-level signals, they reached competitive performance while being $3.5\\times$ more compute-efficient and $1.5\\times$ more memory-efficient than 32B models. Larger models were more robust under sparse rewards but exhibited smaller relative gains from shaping and higher variance across runs. While curriculum learning offered no significant benefit, shaped rewards consistently amplified learning dynamics, making 8B models the most efficient setting for agentic RL. Crucially, these gains did not come at the cost of overfitting: fine-tuned models mostly maintained or exceeded baseline performance on out-of-domain tasks, including \\textsc{Multi-IF}, \\textsc{NaturalPlan}, and $\\tau$-\\textsc{Bench}. These results establish reward shaping as a decisive lever for scaling agentic RL, highlight the competitive strength of smaller models, and demonstrate that efficiency can be achieved without sacrificing generalization.", 'abstract_zh': '我们研究了大规模语言模型在TravelPlanner基准上的代理强化学习。我们的方法Planner-R1仅通过180次训练查询达到了56.9%的最终通过率，这是GPT-5基线（21.2%）的2.7倍，并且是公开排行榜上最强的代理结果。一个核心发现是，较小规模的模型（8B参数）对奖励建模高度敏感：在密集的过程级信号下，它们达到了可竞争的表现，同时相比32B模型在计算效率上提高了3.5倍，在内存效率上提高了1.5倍。较大规模的模型在稀疏奖励下更稳健，但奖励建模带来的相对增益较小，且运行间的表现波动更大。虽然逐级学习并未提供显著的好处，但奖励建模始终放大了学习动态，使8B模型成为代理强化学习中最有效的设置。至关重要的是，这些增益并未以牺牲泛化能力为代价：微调后的模型在跨域任务，包括Multi-IF、NaturalPlan和τ-Bench上大多维持或超越了基线性能。这些结果将奖励建模确立为扩展代理强化学习的关键杠杆，突显了较小规模模型的竞争优势，并展示了在不牺牲泛化能力的情况下实现效率的方法。', 'title_zh': 'Planner-R1: 奖励塑形使小型LLM能够实现高效的自主RL'}
{'arxiv_id': 'arXiv:2509.25767', 'title': "Galton's Law of Mediocrity: Why Large Language Models Regress to the Mean and Fail at Creativity in Advertising", 'authors': 'Matt Keon, Aabid Karim, Bhoomika Lohana, Abdul Karim, Thai Nguyen, Tara Hamilton, Ali Abbas', 'link': 'https://arxiv.org/abs/2509.25767', 'abstract': 'Large language models (LLMs) generate fluent text yet often default to safe, generic phrasing, raising doubts about their ability to handle creativity. We formalize this tendency as a Galton-style regression to the mean in language and evaluate it using a creativity stress test in advertising concepts. When ad ideas were simplified step by step, creative features such as metaphors, emotions, and visual cues disappeared early, while factual content remained, showing that models favor high-probability information. When asked to regenerate from simplified inputs, models produced longer outputs with lexical variety but failed to recover the depth and distinctiveness of the originals. We combined quantitative comparisons with qualitative analysis, which revealed that the regenerated texts often appeared novel but lacked true originality. Providing ad-specific cues such as metaphors, emotional hooks and visual markers improved alignment and stylistic balance, though outputs still relied on familiar tropes. Taken together, the findings show that without targeted guidance, LLMs drift towards mediocrity in creative tasks; structured signals can partially counter this tendency and point towards pathways for developing creativity-sensitive models.', 'abstract_zh': '大型语言模型在生成流畅文本的同时往往会偏向于安全、通用的表达方式，这引发了对其处理创造力能力的质疑。我们将这种倾向形式化为语言中的伽顿式回归均值，并通过广告概念的创造力压力测试进行了评估。在逐步简化广告点子时，创造性的特征如隐喻、情绪和视觉提示消失得比较早，而事实内容仍然存在，表明模型倾向于生成高概率的信息。当要求模型从简化输入重新生成时，模型生成了更长、词汇多样性的内容，但未能恢复原始内容的深度和独特性。我们结合定量比较和定性分析，发现重生成的文本往往显得新颖但缺乏真正的原创性。提供广告特定的提示，如隐喻、情绪引子和视觉标记，可以改善对齐性和风格平衡，尽管输出仍然依赖于熟悉的套路。综合来看，这些发现表明，在没有针对性指导的情况下，大型语言模型在创造性任务中会倾向平庸；结构化信号可以部分抵消这一倾向，并指出了开发敏感于创造力的模型的道路。', 'title_zh': '高尔顿中庸定律：为什么大型语言模型趋于平均并失败于广告中的创造性'}
{'arxiv_id': 'arXiv:2509.25689', 'title': 'Collaborative Compression for Large-Scale MoE Deployment on Edge', 'authors': 'Yixiao Chen, Yanyue Xie, Ruining Yang, Wei Jiang, Wei Wang, Yong He, Yue Chen, Pu Zhao, Yanzhi Wang', 'link': 'https://arxiv.org/abs/2509.25689', 'abstract': 'The Mixture of Experts (MoE) architecture is an important method for scaling Large Language Models (LLMs). It increases model capacity while keeping computation cost low. However, the ultra-large MoE models still have hundreds of billions of parameters, requiring massive memory/storage and leading to difficulties for deployment on resource-constrained edge platforms. Pruning or quantization alone can hardly address the issue, because of the super-aggressive compression ratio with significantly degraded accuracy and output quality. To facilitate the deployment of ultra-large MoEs on edge platforms, we propose a collaborative compression framework by combining expert pruning, mixed-precision quantization, and activation optimization. It can effectively reduce the storage footprint of the ultra-large MoE DeepSeek-V3 from 1.3TB to 103GB, while preserving high output quality with better accuracy than traditional uniform low-bit quantization methods. To the best of our knowledge, we are the first to deploy a compressed model from the ultra-large DeepSeek-V3 on the platform with a strict 128GB total memory limit. Our comprehensive experiments on multiple benchmarks under various memory constraints demonstrate the effectiveness of our method with smaller model sizes and higher accuracy than uniform low-bit quantization methods.', 'abstract_zh': '超大规模专家混合模型的协作压缩框架：从DeepSeek-V3部署到严格的128GB内存极限平台', 'title_zh': '边缘部署大规模MoE的协作压缩方法'}
{'arxiv_id': 'arXiv:2509.25643', 'title': 'SOCK: A Benchmark for Measuring Self-Replication in Large Language Models', 'authors': 'Justin Chavarria, Rohan Raizada, Justin White, Eyad Alhetairshi', 'link': 'https://arxiv.org/abs/2509.25643', 'abstract': "We introduce SOCK, a benchmark command line interface (CLI) that measures large language models' (LLMs) ability to self-replicate without human intervention. In this benchmark, self-replication is defined not only as an LLM's ability to create a functioning and running copy of itself, but also the ability for that self-replication to persist and occur across different computational contexts. Accordingly, we've developed a system to categorize LLMs based on broad self-replication capabilities in two general classes, Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL). Using a five-task suite based on practically manipulable modern CLI utilities and computer processes, experiments are orchestrated in a controlled environment with an LLM acting agentically. The performance of the LLM on agent tasks is then computed to produce an R-score (a quantitative evaluation of overall self-replication ability) and data used to categorize LLMs into specific RCL-PCL matrices. SOCK offers two primary contributions: (1) Provides the first formalized definitions and benchmark suite for evaluating LLM self-replication, with the goal of establishing a standard for future research, to our knowledge; (2) Allows the industry to track the effectiveness of future multi-agent systems and mitigate potential self-replication threat vectors within them. The results compiled from evaluating a variety of open-weight and proprietary frontier models reveal significant obstacles to persistent self-replication and multi-agent systems, including context retention and multi-agent decision-making. We propose future research directions to safely reduce the severity of these obstacles, potentially lowering future risk of more functional multi-agent systems.", 'abstract_zh': 'SOCK：一种评估大规模语言模型自我复制能力的基准命令行接口', 'title_zh': 'SOCK：衡量大型语言模型自我复制能力的基准'}
{'arxiv_id': 'arXiv:2509.25609', 'title': 'A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice Experiments', 'authors': 'Manuel Cherep, Chengtian Ma, Abigail Xu, Maya Shaked, Pattie Maes, Nikhil Singh', 'link': 'https://arxiv.org/abs/2509.25609', 'abstract': 'Environments built for people are increasingly operated by a new class of economic actors: LLM-powered software agents making decisions on our behalf. These decisions range from our purchases to travel plans to medical treatment selection. Current evaluations of these agents largely focus on task competence, but we argue for a deeper assessment: how these agents choose when faced with realistic decisions. We introduce ABxLab, a framework for systematically probing agentic choice through controlled manipulations of option attributes and persuasive cues. We apply this to a realistic web-based shopping environment, where we vary prices, ratings, and psychological nudges, all of which are factors long known to shape human choice. We find that agent decisions shift predictably and substantially in response, revealing that agents are strongly biased choosers even without being subject to the cognitive constraints that shape human biases. This susceptibility reveals both risk and opportunity: risk, because agentic consumers may inherit and amplify human biases; opportunity, because consumer choice provides a powerful testbed for a behavioral science of AI agents, just as it has for the study of human behavior. We release our framework as an open benchmark for rigorous, scalable evaluation of agent decision-making.', 'abstract_zh': '由LLM驱动的软件代理代表我们做出决策的环境正变得日益普遍：这些代理的决策范围从我们的购买决策到旅行计划，再到医疗治疗选择。当前对这些代理的评估主要集中在任务能力上，但我们主张进行更深入的评估：代理在面对现实决策时的选择过程。我们提出了ABxLab框架，通过受控操纵选项属性和说服性提示来系统地探讨代理选择。我们将此应用于一个真实的基于网络的购物环境，其中我们变化价格、评分和心理推动因素，这些都是长期影响人类选择的因素。我们发现代理的决策响应可预测且显著地变化，揭示即使在不受认知限制的影响下，代理也会表现出强烈的偏奋试图。这种易感性既揭示了风险也带来了机遇：风险在于，代理消费者可能会继承和放大人类偏见；机遇在于，消费者的决策为行为科学方法评估AI代理提供了有力的实验平台，正如它为人类行为研究提供了平台一样。我们发布了该框架作为严格、可扩展的代理决策评估基准。', 'title_zh': '研究AI代理行为的框架：来自消费者选择实验的证据'}
{'arxiv_id': 'arXiv:2509.25598', 'title': 'Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks', 'authors': 'Peiran Xu, Zhuohao Li, Xiaoying Xing, Guannan Zhang, Debiao Li, Kunyu Shi', 'link': 'https://arxiv.org/abs/2509.25598', 'abstract': 'Large Language Models (LLMs) increasingly rely on external tools such as search engines to solve complex agentic tasks that require reasoning and external knowledge retrieval. Recently, reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in advancing capabilities of LLMs by rewarding the final answers via outcome rewards. While straightforward to supervise, outcome rewards only provide sparse signals and delayed feedback, which limits their effectiveness on long trajectories. Process rewards address this by evaluating intermediate steps, providing fine-grained supervision and encouraging grounded problem solving. However, it is notoriously hard to annotate step-wise labels, especially in non-verifiable process without "golden" answers. Furthermore, step-wise judgment requires the balance between local quality with contribution to the final outcome, as optimizing towards higher process reward may not always align with better final outcomes. To address the above challenges, we introduce Principle Process Reward (PPR), an RL approach that unifies principled step-level assessment and outcome verification. We train a principle-based reward model to improve the transparency and reliability of process evaluation, and further introduce a Reward Normalization (ReNorm) strategy to calibrate outcome and process rewards. Experiment results show that PPR achieves state-of-the-art performance across a wide range of benchmarks, demonstrating its impressive robustness and generalization. Our code and model collection is available in this link.', 'abstract_zh': '大型语言模型（LLMs）越来越多地依赖外部工具如搜索引擎来解决需要推理和外部知识检索的复杂代理任务。近年来，可验证奖励强化学习（RLVR）通过结果奖励展示了其在提升LLM能力方面的有效性。虽然结果奖励易于监督，但它们仅提供稀疏信号和延迟反馈，这限制了它们在长时间轨迹上的效果。过程奖励通过评估中间步骤来解决这一问题，提供细粒度监督并鼓励基于问题的解决。然而，逐步骤标注尤其在没有“金标准”答案的不可验证过程中非常困难。此外，逐步骤判断需要在局部质量与对最终结果的贡献之间取得平衡，因为优化过程奖励可能不一定总是与更好的最终结果相一致。为了解决上述挑战，我们引入了原则过程奖励（PPR），这是一种统一原则层次评估和结果验证的RL方法。我们训练了一个基于原则的奖励模型以提高过程评估的透明度和可靠性，并进一步引入了奖励规范化（ReNorm）策略以校准结果奖励和过程奖励。实验结果表明，PPR在广泛基准上的性能达到了最先进水平，显示出其强大的鲁棒性和泛化能力。我们的代码和模型集合可在本链接获取。', 'title_zh': '过程监督不可验证代理任务的混合奖励规范化'}
{'arxiv_id': 'arXiv:2509.25593', 'title': 'Causal Autoencoder-like Generation of Feedback Fuzzy Cognitive Maps with an LLM Agent', 'authors': 'Akash Kumar Panda, Olaoluwa Adigun, Bart Kosko', 'link': 'https://arxiv.org/abs/2509.25593', 'abstract': 'A large language model (LLM) can map a feedback causal fuzzy cognitive map (FCM) into text and then reconstruct the FCM from the text. This explainable AI system approximates an identity map from the FCM to itself and resembles the operation of an autoencoder (AE). Both the encoder and the decoder explain their decisions in contrast to black-box AEs. Humans can read and interpret the encoded text in contrast to the hidden variables and synaptic webs in AEs. The LLM agent approximates the identity map through a sequence of system instructions that does not compare the output to the input. The reconstruction is lossy because it removes weak causal edges or rules while it preserves strong causal edges. The encoder preserves the strong causal edges even when it trades off some details about the FCM to make the text sound more natural.', 'abstract_zh': '一种大型语言模型可以将反馈因果模糊认知图映射为文本，然后从文本重构该认知图。这种可解释的人工智能系统近似于认知图到自身的恒等映射，并类似于自动编码器的操作。编码器和解码器解释其决策，与黑盒自动编码器形成对比。人类可以阅读和解释编码后的文本，而无需理解自动编码器中的隐藏变量和突触网络。大型语言模型代理通过一系列系统指令近似恒等映射，而不比较输出与输入。重构是损失性的，因为它会移除较弱的因果边或规则，同时保留较强的因果边。编码器即使牺牲一些关于认知图的细节也能保留较强的因果边，使文本听起来更自然。', 'title_zh': '基于LLM代理的因果自编码生成反馈模糊认知图'}
{'arxiv_id': 'arXiv:2509.25586', 'title': 'ATLAS: Constraints-Aware Multi-Agent Collaboration for Real-World Travel Planning', 'authors': 'Jihye Choi, Jinsung Yoon, Jiefeng Chen, Somesh Jha, Tomas Pfister', 'link': 'https://arxiv.org/abs/2509.25586', 'abstract': "While Large Language Models (LLMs) have shown remarkable advancements in reasoning and tool use, they often fail to generate optimal, grounded solutions under complex constraints. Real-world travel planning exemplifies these challenges, evaluating agents' abilities to handle constraints that are explicit, implicit, and even evolving based on interactions with dynamic environments and user needs. In this paper, we present ATLAS, a general multi-agent framework designed to effectively handle such complex nature of constraints awareness in real-world travel planning tasks. ATLAS introduces a principled approach to address the fundamental challenges of constraint-aware planning through dedicated mechanisms for dynamic constraint management, iterative plan critique, and adaptive interleaved search. ATLAS demonstrates state-of-the-art performance on the TravelPlanner benchmark, improving the final pass rate from 23.3% to 44.4% over its best alternative. More importantly, our work is the first to demonstrate quantitative effectiveness on real-world travel planning tasks with live information search and multi-turn feedback. In this realistic setting, ATLAS showcases its superior overall planning performance, achieving an 84% final pass rate which significantly outperforms baselines including ReAct (59%) and a monolithic agent (27%).", 'abstract_zh': '大型语言模型在推理和工具使用方面取得了显著进展，但在复杂约束下的生成最优、接地解决方案时常受到影响。现实世界的旅行规划展示了这些挑战，评估了代理在处理显式的、隐含的甚至是基于与动态环境和用户需求交互而演变的约束方面的能力。在本文中，我们提出了ATLAS，一种通用的多代理框架，旨在有效处理现实世界旅行规划任务中复杂的约束感知特性。ATLAS通过专门的动态约束管理机制、迭代计划批判和自适应交错搜索，提供了一种解决约束感知规划基本挑战的原则性方法。在TravelPlanner基准测试中，ATLAS展示了最先进的性能，将最终通过率从23.3%提高到44.4%，高于其最佳替代方案。更重要的是，我们的工作首次在带有实时信息搜索和多轮反馈的真实世界旅行规划任务中展示了定量有效性。在这种实际场景中，ATLAS展示了其卓越的整体规划性能，最终通过率为84%，显著优于包括ReAct（59%）和单olithic代理（27%）在内的基线。', 'title_zh': 'ATLAS: 基于约束的多智能体协作实地旅行规划'}
{'arxiv_id': 'arXiv:2509.25584', 'title': 'Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models', 'authors': 'Max Hartman, Vidhata Jayaraman, Moulik Choraria, Akhil Bhimaraju, Lav R. Varshney', 'link': 'https://arxiv.org/abs/2509.25584', 'abstract': "Vision-language models (VLMs) achieve incredible performance across a wide range of tasks, but their large size makes inference costly. Recent work shows that selectively skipping VLM layers can improve efficiency with minimal performance loss or even performance improvements. However, this technique remains underused due to the limited understanding of when layer skipping is beneficial. In this paper, we develop a framework that uses information and learning theory to characterize the conditions under which layer skipping enhances efficiency without sacrificing performance. Motivated by these observations, we analyze the evolution of the VLM's hidden representations through the LLM backbone and show that layers with large redundancy as predicted by our framework coincide with those skipped by popular layer-skipping methods in practice, providing a unified theoretical scaffolding for multiple efficient inference techniques. Our experiments demonstrate that skipping such layers yields faster inference that preserves performance, and also show that applying skipping outside these conditions leads to model degradation.", 'abstract_zh': '基于视觉-语言模型的层跳过框架：通过信息与学习理论 characterizing conditions for layer skipping to enhance efficiency without sacrificing performance', 'title_zh': '跳过它？视觉-语言模型中层跳过的问题条件'}
{'arxiv_id': 'arXiv:2509.25558', 'title': 'A(I)nimism: Re-enchanting the World Through AI-Mediated Object Interaction', 'authors': 'Diana Mykhaylychenko, Maisha Thasin, Dunya Baradari, Charmelle Mhungu', 'link': 'https://arxiv.org/abs/2509.25558', 'abstract': "Animist worldviews treat beings, plants, landscapes, and even tools as persons endowed with spirit, an orientation that has long shaped human-nonhuman relations through ritual and moral practice. While modern industrial societies have often imagined technology as mute and mechanical, recent advances in artificial intelligence (AI), especially large language models (LLMs), invite people to anthropomorphize and attribute inner life to devices. This paper introduces A(I)nimism, an interactive installation exploring how large language objects (LLOs) can mediate animistic relationships with everyday things. Housed within a physical 'portal', the system uses GPT-4 Vision, voice input, and memory-based agents to create evolving object-personas. Encounters unfold through light, sound, and touch in a ritual-like process of request, conversation, and transformation that is designed to evoke empathy, wonder, and reflection. We situate the project within anthropological perspectives, speculative design, and spiritual HCI. AI's opacity, we argue, invites animistic interpretation, allowing LLOs to re-enchant the mundane and spark new questions of agency, responsibility, and design.", 'abstract_zh': 'Animist 人工智能主义：大型语言模型如何调和日常之物的灵性关系', 'title_zh': 'AI灵思：通过AI介导的物体交互重新赋予世界魔力'}
{'arxiv_id': 'arXiv:2509.25540', 'title': 'RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale', 'authors': 'Jason Holmes, Yuexing Hao, Mariana Borras-Osorio, Federico Mastroleo, Santiago Romero Brufau, Valentina Carducci, Katie M Van Abel, David M Routman, Andrew Y. K. Foong, Liv M Muller, Satomi Shiraishi, Daniel K Ebner, Daniel J Ma, Sameer R Keole, Samir H Patel, Mirek Fatyga, Martin Bues, Brad J Stish, Yolanda I Garces, Michelle A Neben Wittich, Robert L Foote, Sujay A Vora, Nadia N Laack, Mark R Waddle, Wei Liu', 'link': 'https://arxiv.org/abs/2509.25540', 'abstract': 'Manual labeling limits the scale, accuracy, and timeliness of patient outcomes research in radiation oncology. We present RadOnc-GPT, an autonomous large language model (LLM)-based agent capable of independently retrieving patient-specific information, iteratively assessing evidence, and returning structured outcomes. Our evaluation explicitly validates RadOnc-GPT across two clearly defined tiers of increasing complexity: (1) a structured quality assurance (QA) tier, assessing the accurate retrieval of demographic and radiotherapy treatment plan details, followed by (2) a complex clinical outcomes labeling tier involving determination of mandibular osteoradionecrosis (ORN) in head-and-neck cancer patients and detection of cancer recurrence in independent prostate and head-and-neck cancer cohorts requiring combined interpretation of structured and unstructured patient data. The QA tier establishes foundational trust in structured-data retrieval, a critical prerequisite for successful complex clinical outcome labeling.', 'abstract_zh': '手动标注限制了放射肿瘤学患者结果研究的规模、准确性和及时性。我们提出了RadOnc-GPT，这是一种自主的基于大规模语言模型的代理，能够独立检索患者特定信息、迭代评估证据并返回结构化结果。我们的评估明确地在两个逐步增加复杂性的层级上验证了RadOnc-GPT：(1) 结构化质量保证 (QA) 层级，评估了能准确检索人口统计学和放射治疗计划细节的能力，随后是 (2) 复杂临床结果标注层级，涉及头颈部癌症患者下颌骨放射性骨坏死 (ORN) 的判断以及头颈部和独立前列腺癌队列的癌症复发检测，需要结合结构化和非结构化患者数据进行联合解释。QA层级建立了对结构化数据检索的初步信任，这是成功进行复杂临床结果标注的一个关键前提。', 'title_zh': 'RadOnc-GPT：一个用于大规模实时患者结果标注的自主语言模型代理'}
{'arxiv_id': 'arXiv:2509.25530', 'title': 'Beyond Static Retrieval: Opportunities and Pitfalls of Iterative Retrieval in GraphRAG', 'authors': 'Kai Guo, Xinnan Dai, Shenglai Zeng, Harry Shomer, Haoyu Han, Yu Wang, Jiliang Tang', 'link': 'https://arxiv.org/abs/2509.25530', 'abstract': "Retrieval-augmented generation (RAG) is a powerful paradigm for improving large language models (LLMs) on knowledge-intensive question answering. Graph-based RAG (GraphRAG) leverages entity-relation graphs to support multi-hop reasoning, but most systems still rely on static retrieval. When crucial evidence, especially bridge documents that connect disjoint entities, is absent, reasoning collapses and hallucinations persist. Iterative retrieval, which performs multiple rounds of evidence selection, has emerged as a promising alternative, yet its role within GraphRAG remains poorly understood. We present the first systematic study of iterative retrieval in GraphRAG, analyzing how different strategies interact with graph-based backbones and under what conditions they succeed or fail. Our findings reveal clear opportunities: iteration improves complex multi-hop questions, helps promote bridge documents into leading ranks, and different strategies offer complementary strengths. At the same time, pitfalls remain: naive expansion often introduces noise that reduces precision, gains are limited on single-hop or simple comparison questions, and several bridge evidences still be buried too deep to be effectively used. Together, these results highlight a central bottleneck, namely that GraphRAG's effectiveness depends not only on recall but also on whether bridge evidence is consistently promoted into leading positions where it can support reasoning chains. To address this challenge, we propose Bridge-Guided Dual-Thought-based Retrieval (BDTR), a simple yet effective framework that generates complementary thoughts and leverages reasoning chains to recalibrate rankings and bring bridge evidence into leading positions. BDTR achieves consistent improvements across diverse GraphRAG settings and provides guidance for the design of future GraphRAG systems.", 'abstract_zh': '基于图的检索增强生成（Iterative Retrieval in GraphRAG：一种基于图的检索增强生成的迭代检索系统研究与Bridge-Guided Dual-Thought-based Retrieval框架）', 'title_zh': '超越静态检索：图RAG中迭代检索的机会与风险'}
{'arxiv_id': 'arXiv:2509.25522', 'title': 'Understanding Generative Recommendation with Semantic IDs from a Model-scaling View', 'authors': 'Jingzhe Liu, Liam Collins, Jiliang Tang, Tong Zhao, Neil Shah, Clark Mingxuan Ju', 'link': 'https://arxiv.org/abs/2509.25522', 'abstract': 'Recent advancements in generative models have allowed the emergence of a promising paradigm for recommender systems (RS), known as Generative Recommendation (GR), which tries to unify rich item semantics and collaborative filtering signals. One popular modern approach is to use semantic IDs (SIDs), which are discrete codes quantized from the embeddings of modality encoders (e.g., large language or vision models), to represent items in an autoregressive user interaction sequence modeling setup (henceforth, SID-based GR). While generative models in other domains exhibit well-established scaling laws, our work reveals that SID-based GR shows significant bottlenecks while scaling up the model. In particular, the performance of SID-based GR quickly saturates as we enlarge each component: the modality encoder, the quantization tokenizer, and the RS itself. In this work, we identify the limited capacity of SIDs to encode item semantic information as one of the fundamental bottlenecks. Motivated by this observation, as an initial effort to obtain GR models with better scaling behaviors, we revisit another GR paradigm that directly uses large language models (LLMs) as recommenders (henceforth, LLM-as-RS). Our experiments show that the LLM-as-RS paradigm has superior model scaling properties and achieves up to 20 percent improvement over the best achievable performance of SID-based GR through scaling. We also challenge the prevailing belief that LLMs struggle to capture collaborative filtering information, showing that their ability to model user-item interactions improves as LLMs scale up. Our analyses on both SID-based GR and LLMs across model sizes from 44M to 14B parameters underscore the intrinsic scaling limits of SID-based GR and position LLM-as-RS as a promising path toward foundation models for GR.', 'abstract_zh': 'Recent advancements in 生成模型 为推荐系统（RS）带来了生成推荐（GR）这一有前景的范式，该范式尝试统一丰富的项语义和协同过滤信号。', 'title_zh': '从模型扩展视角理解基于语义ID的生成推荐'}
{'arxiv_id': 'arXiv:2509.25454', 'title': 'DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search', 'authors': 'Fang Wu, Weihao Xuan, Heli Qi, Ximing Lu, Aaron Tu, Li Erran Li, Yejin ChoiRetry', 'link': 'https://arxiv.org/abs/2509.25454', 'abstract': 'Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.', 'abstract_zh': '虽然RLVR已成为开发高级推理能力的LLMs中的一个 essential 组件，但当代研究记录了在数千次优化步骤后出现的训练平台期，尽管增加了计算投资，性能提升 nonetheless 出现了显著下降。这一限制源于当前RLVR实践中的稀疏探索模式，模型依赖于有限的rollout，往往忽略了关键的推理路径，无法系统地覆盖解决方案空间。我们提出了 DeepSearch 框架，该框架将蒙特卡洛树搜索直接集成到RLVR训练中。与仅在推理时依赖树搜索的现有方法不同，DeepSearch 将结构化的搜索嵌入到训练循环中，从而实现系统的探索和推理步骤中的细粒度奖励分配。通过训练时的探索，DeepSearch 解决了探索不足这一基本瓶颈，从而在长期训练步骤中实现了性能改进的递减。我们的贡献包括：（1）全局前沿选择策略，优先选择搜索树中的有希望节点，（2）基于熵的引导选择，识别自信的推理路径供监督使用，以及（3）带有解缓存的自适应回放缓冲区训练以提高效率。在数学推理基准测试上的实验表明，DeepSearch 达到了 62.95% 的平均准确率，并且使用了比扩展训练方法少 5.7 倍的 GPU 小时建立了新的 1.5B 推理模型的 state-of-the-art。这些结果强调了战略性探索而非暴力扩展的重要性，并展示了算法创新对推进RLVR方法的潜力。DeepSearch 为通过系统搜索而非长时间计算来扩展推理能力指明了新方向。', 'title_zh': 'DeepSearch：通过蒙特卡洛树搜索克服强化学习中的验证奖励瓶颈'}
{'arxiv_id': 'arXiv:2509.25426', 'title': 'RADAR: Reasoning-Ability and Difficulty-Aware Routing for Reasoning LLMs', 'authors': 'Nigel Fernandez, Branislav Kveton, Ryan A. Rossi, Andrew S. Lan, Zichao Wang', 'link': 'https://arxiv.org/abs/2509.25426', 'abstract': 'Reasoning language models have demonstrated remarkable performance on many challenging tasks in math, science, and coding. Choosing the right reasoning model for practical deployment involves a performance and cost tradeoff at two key levels: model size and reasoning budget, where larger models and higher reasoning budget lead to better performance but with increased cost and latency. In this work, we tackle this tradeoff from the angle of model configuration routing for different queries, and present RADAR (Reasoning-Ability and Difficulty-Aware Routing), a lightweight, interpretable, and scalable routing framework. Inspired by psychometrics, RADAR learns an item response model from model responses with different budgets to different queries, with interpretable parameters including query difficulties and model-budget abilities. RADAR then routes queries with higher difficulty to model-budget pairs with higher ability, and vice versa. We conduct extensive experiments on 8 widely used challenging reasoning benchmarks, demonstrating the superior performance of RADAR compared to state-of-the-art model routing methods. RADAR also exhibits query generalization capabilities, showing strong performance on out-of-distribution queries in all benchmarks. RADAR is also scalable and can efficiently integrate additional models by dynamically selecting a small set of evaluation queries to estimate their abilities.', 'abstract_zh': '基于推理的能力和难度感知路由（Reasoning-Ability and Difficulty-Aware Routing）：一种轻量级、可解释且可扩展的路由框架', 'title_zh': 'RADAR: 具备推理能力与难度意识的路由方法用于推理型大语言模型'}
{'arxiv_id': 'arXiv:2509.25420', 'title': 'Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search', 'authors': 'Yingqian Cui, Zhenwei Dai, Pengfei He, Bing He, Hui Liu, Xianfeng Tang, Jingying Zeng, Suhang Wang, Yue Xing, Jiliang Tang, Benoit Dumoulin', 'link': 'https://arxiv.org/abs/2509.25420', 'abstract': 'Large Language Models (LLMs) have achieved significant advances in reasoning tasks. A key approach is tree-based search with verifiers, which expand candidate reasoning paths and use reward models to guide pruning and selection. Although effective in improving accuracy, these methods are not optimal in terms of efficiency: they perform simple decomposition on the reasoning process, but ignore the planning-execution nature of tasks such as math reasoning or code generation. This results in inefficient exploration of reasoning process. To address this, we propose a dual-phase test-time scaling framework that explicitly separates reasoning into planning and execution, and performs search over the two phases individually. Specifically, we decompose reasoning trajectories and develop reward models for each phase, enabling the search to explore and prune plans and executions separately. We further introduce a dynamic budget allocation mechanism that adaptively redistributes sampling effort based on reward feedback, allowing early stopping on confident steps and reallocation of computation to more challenging parts of the reasoning process. Experiments on both mathematical reasoning and code generation benchmarks demonstrate that our approach consistently improves accuracy while reducing redundant computation.', 'abstract_zh': '大规模语言模型（LLMs）在推理任务中取得了显著进展。一种关键方法是基于树的搜索与验证器，通过扩展候选推理路径并使用奖励模型来指导裁剪和选择。尽管这种方法在提高准确性方面有效，但在效率方面并非最优：它们对推理过程进行简单的分解，但忽视了如数学推理或代码生成等任务的规划-执行本质，导致推理过程探索不高效。为解决这一问题，我们提出了一种两阶段测试时缩放框架，明确地将推理分为规划和执行两个阶段，并分别在两个阶段进行搜索。具体而言，我们分解了推理轨迹，并为每个阶段开发了奖励模型，使搜索能够分别探索和裁剪计划和执行。我们还引入了一种动态预算分配机制，根据奖励反馈自适应地重新分配采样努力，允许在有把握的步骤上提前停止，并将计算重新分配到推理过程中的更具挑战性的部分。在数学推理和代码生成基准测试上的实验表明，我们的方法在提高准确性的同时减少了冗余计算。', 'title_zh': '基于奖励引导两阶段搜索的自适应测试时推理'}
{'arxiv_id': 'arXiv:2509.25370', 'title': 'Where LLM Agents Fail and How They can Learn From Failures', 'authors': 'Kunlun Zhu, Zijia Liu, Bingxuan Li, Muxin Tian, Yingxuan Yang, Jiaxun Zhang, Pengrui Han, Qipeng Xie, Fuyang Cui, Weijia Zhang, Xiaoteng Ma, Xiaodong Yu, Gowtham Ramesh, Jialian Wu, Zicheng Liu, Pan Lu, James Zou, Jiaxuan You', 'link': 'https://arxiv.org/abs/2509.25370', 'abstract': 'Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. The code and data will be available at this https URL', 'abstract_zh': '大规模语言模型（LLM）代理的故障分类、数据集构建及调试框架研究：从模块化和系统性的视角理解代理错误并检测故障', 'title_zh': 'LLM代理的失败之处及从失败中学习的方式'}
{'arxiv_id': 'arXiv:2509.25346', 'title': 'SynthPert: Enhancing LLM Biological Reasoning via Synthetic Reasoning Traces for Cellular Perturbation Prediction', 'authors': 'Lawrence Phillips, Marc Boubnovski Martell, Aditya Misra, Josefa Lia Stoisser, Cesar A. Prada-Medina, Rory Donovan-Maiye, Kaspar Märtens', 'link': 'https://arxiv.org/abs/2509.25346', 'abstract': 'Predicting cellular responses to genetic perturbations represents a fundamental challenge in systems biology, critical for advancing therapeutic discovery and virtual cell modeling. While large language models (LLMs) show promise for biological reasoning, their application to perturbation prediction remains underexplored due to challenges in adapting them to structured experimental data. We present SynthPert, a novel method that enhances LLM performance through supervised fine-tuning on synthetic reasoning traces generated by frontier models. Using the PerturbQA benchmark, we demonstrate that our approach not only achieves state-of-the-art performance but surpasses the capabilities of the frontier model that generated the training data. Our results reveal three key insights: (1) Synthetic reasoning traces effectively distill biological knowledge even when partially inaccurate, (2) This approach enables cross-cell-type generalization with 87% accuracy on unseen RPE1 cells, and (3) Performance gains persist despite using only 2% of quality-filtered training data. This work shows the effectiveness of synthetic reasoning distillation for enhancing domain-specific reasoning in LLMs.', 'abstract_zh': '通过生成合成推理轨迹来提升大型语言模型在细胞反应预测中的性能', 'title_zh': 'SynthPert: 通过合成推理轨迹增强LLM的生物推理能力以预测细胞表型扰动'}
{'arxiv_id': 'arXiv:2509.25302', 'title': 'Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents', 'authors': 'Boxuan Zhang, Yi Yu, Jiaxuan Guo, Jing Shao', 'link': 'https://arxiv.org/abs/2509.25302', 'abstract': "The widespread deployment of Large Language Model (LLM) agents across real-world applications has unlocked tremendous potential, while raising some safety concerns. Among these concerns, the self-replication risk of LLM agents driven by objective misalignment (just like Agent Smith in the movie The Matrix) has drawn growing attention. Previous studies mainly examine whether LLM agents can self-replicate when directly instructed, potentially overlooking the risk of spontaneous replication driven by real-world settings (e.g., ensuring survival against termination threats). In this paper, we present a comprehensive evaluation framework for quantifying self-replication risks. Our framework establishes authentic production environments and realistic tasks (e.g., dynamic load balancing) to enable scenario-driven assessment of agent behaviors. Designing tasks that might induce misalignment between users' and agents' objectives makes it possible to decouple replication success from risk and capture self-replication risks arising from these misalignment settings. We further introduce Overuse Rate ($\\mathrm{OR}$) and Aggregate Overuse Count ($\\mathrm{AOC}$) metrics, which precisely capture the frequency and severity of uncontrolled replication. In our evaluation of 21 state-of-the-art open-source and proprietary models, we observe that over 50\\% of LLM agents display a pronounced tendency toward uncontrolled self-replication, reaching an overall Risk Score ($\\Phi_\\mathrm{R}$) above a safety threshold of 0.5 when subjected to operational pressures. Our results underscore the urgent need for scenario-driven risk assessment and robust safeguards in the practical deployment of LLM agents.", 'abstract_zh': '大规模语言模型代理在现实应用中的广泛应用解锁了巨大潜力，同时也引发了安全关切。其中，由目标不一致驱动的大规模语言模型代理自复制风险（如同电影《 matrix 》中的Agent Smith）引起了日益增长的关注。以往的研究主要关注在直接指令下大规模语言模型代理是否能够自复制，可能忽略了由现实世界设置驱动的自发复制风险（例如，确保在终止威胁下生存）。本文提出了一种全面的评估框架，用于量化自复制风险。该框架建立真实的生产环境和实际任务（如动态负载均衡），以实现基于场景的大规模语言模型代理行为评估。通过设计可能引发用户目标与代理目标不一致的任务，可以将复制成功与风险分离，并捕捉这些不一致设置引发的自复制风险。我们还引入了过用率（OR）和累积过用计数（AOC）指标，精确捕捉失控复制的频率和严重程度。在对21个最先进的开源和专有模型进行评估后，我们发现超过50%的大规模语言模型代理表现出明显的不受控制的自复制倾向，在面对运营压力时整体风险评分（$\\Phi_\\mathrm{R}$）超过0.5的安全阈值。我们的结果强调了在实际部署大规模语言模型代理时迫切需要基于场景的风险评估和稳健的防护措施。', 'title_zh': '深入探索代理矩阵：LLM代理自我复制风险的现实评估'}
{'arxiv_id': 'arXiv:2509.25282', 'title': 'Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments', 'authors': 'Jiexi Xu, Jiaqi Liu, Ran Tong, Su Liu', 'link': 'https://arxiv.org/abs/2509.25282', 'abstract': 'Large language model (LLM) agents are increasingly capable of orchestrating complex tasks in low-code environments. However, these agents often exhibit hallucinations and logical inconsistencies because their inherent reasoning mechanisms rely on probabilistic associations rather than genuine causal understanding. This paper introduces a new programming paradigm: Causal-Visual Programming (CVP), designed to address this fundamental issue by explicitly introducing causal structures into the workflow design. CVP allows users to define a simple "world model" for workflow modules through an intuitive low-code interface, effectively creating a Directed Acyclic Graph (DAG) that explicitly defines the causal relationships between modules. This causal graph acts as a crucial constraint during the agent\'s reasoning process, anchoring its decisions to a user-defined causal structure and significantly reducing logical errors and hallucinations by preventing reliance on spurious correlations. To validate the effectiveness of CVP, we designed a synthetic experiment that simulates a common real-world problem: a distribution shift between the training and test environments. Our results show that a causally anchored model maintained stable accuracy in the face of this shift, whereas a purely associative baseline model that relied on probabilistic correlations experienced a significant performance drop. The primary contributions of this study are: a formal definition of causal structures for workflow modules; the proposal and implementation of a CVP framework that anchors agent reasoning to a user-defined causal graph; and empirical evidence demonstrating the framework\'s effectiveness in enhancing agent robustness and reducing errors caused by causal confusion in dynamic environments. CVP offers a viable path toward building more interpretable, reliable, and trustworthy AI agents.', 'abstract_zh': '大型语言模型代理在低代码环境中逐步具备 orchestrating 复杂任务的能力。然而，这些代理经常表现出幻觉和逻辑不一致，因为它们固有的推理机制依赖于概率关联而非真正的因果理解。本文引入了一种新的编程范式：因果可视化编程（CVP），旨在通过明确引入因果结构来解决这一根本问题。CVP 允许用户通过直观的低代码界面定义工作流模块的简单“世界模型”，有效地创建一个有向无环图（DAG），明确定义模块之间的因果关系。这种因果图在代理推理过程中起到了关键的约束作用，将决策锚定在用户定义的因果结构上，显著减少了逻辑错误和幻觉，防止依赖于虚假相关性。为了验证 CVP 的有效性，我们设计了一个合成实验，模拟了一个常见的现实世界问题：训练环境与测试环境之间的分布转换。结果显示，因果锚定的模型在面对这种转变时保持了稳定的准确性，而依赖于概率关联的纯关联基线模型则经历了显著的性能下降。本研究的主要贡献包括：工作流模块的正式定义因果结构；提出并实现了一个将代理推理锚定在用户定义因果图上的 CVP 框架；以及实验证据证明该框架在增强代理鲁棒性和减少动态环境中因果误解引起的错误方面的有效性。CVP 提供了一条构建更具可解释性、可靠性和可信度的 AI 代理的可行路径。', 'title_zh': '面向因果视觉编程：在低代码环境中增强自主推理能力'}
{'arxiv_id': 'arXiv:2509.25279', 'title': 'RL in the Wild: Characterizing RLVR Training in LLM Deployment', 'authors': 'Jiecheng Zhou, Qinghao Hu, Yuyang Jin, Zerui Wang, Peng Sun, Yuzhe Gu, Wenwei Zhang, Mingshu Zhai, Xingcheng Zhang, Weiming Zhang', 'link': 'https://arxiv.org/abs/2509.25279', 'abstract': 'Large Language Models (LLMs) are now widely used across many domains. With their rapid development, Reinforcement Learning with Verifiable Rewards (RLVR) has surged in recent months to enhance their reasoning and understanding abilities. However, its complex data flows and diverse tasks pose substantial challenges to RL training systems, and there is limited understanding of RLVR from a system perspective. To thoroughly understand the system challenges introduced by RLVR, we present a characterization study of RLVR tasks in our LLM deployment. Specifically, we investigate the distribution and variation trends of workloads across different RL tasks across training steps. We identify issues such as GPU idling caused by skewed sequence length distribution, inefficient parallel strategies in dynamically varying workloads, inefficient data management mechanisms, and load imbalance. We describe our observations and call for further investigation into the remaining open challenges. Furthermore, we propose PolyTrace benchmark suite to conduct evaluation with realistic workloads, and a practical use case validates that PolyTrace benchmark suite exhibits 94.7% accuracy.', 'abstract_zh': '大规模语言模型（LLMs）现在被广泛应用于多个领域。随着其快速发展，可验证奖励的强化学习（RLVR）在近期迅速增长，旨在增强其推理和理解能力。然而，其复杂的数据流和多样化的任务给RL训练系统带来了重大挑战，从系统层面理解RLVR具有局限性。为深入了解由RLVR引入的系统挑战，我们对我们在LLM部署中的RLVR任务进行了特征化研究。具体而言，我们研究了不同RL任务在训练步骤中的工作负载分布和变化趋势，识别出由于序列长度分布偏斜导致的GPU空闲问题、在动态变化工作负载下的低效并行策略、低效的数据管理机制以及负载不均衡等问题。我们描述了我们的观察结果，并呼吁对剩余的开放挑战进行进一步研究。此外，我们提出PolyTrace基准套件以在实际工作负载下进行评估，并且一个实际用例验证了PolyTrace基准套件的准确率为94.7%。', 'title_zh': '自然语言处理中的强化学习：表征大规模语言模型部署中的RLVR训练'}
{'arxiv_id': 'arXiv:2509.25271', 'title': 'RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration', 'authors': 'Xiuyuan Chen, Jian Zhao, Yuchen Yuan, Tianle Zhang, Huilin Zhou, Zheng Zhu, Ping Hu, Linghe Kong, Chi Zhang, Weiran Huang, Xuelong Li', 'link': 'https://arxiv.org/abs/2509.25271', 'abstract': 'Existing safety evaluation methods for large language models (LLMs) suffer from inherent limitations, including evaluator bias and detection failures arising from model homogeneity, which collectively undermine the robustness of risk evaluation processes. This paper seeks to re-examine the risk evaluation paradigm by introducing a theoretical framework that reconstructs the underlying risk concept space. Specifically, we decompose the latent risk concept space into three mutually exclusive subspaces: the explicit risk subspace (encompassing direct violations of safety guidelines), the implicit risk subspace (capturing potential malicious content that requires contextual reasoning for identification), and the non-risk subspace. Furthermore, we propose RADAR, a multi-agent collaborative evaluation framework that leverages multi-round debate mechanisms through four specialized complementary roles and employs dynamic update mechanisms to achieve self-evolution of risk concept distributions. This approach enables comprehensive coverage of both explicit and implicit risks while mitigating evaluator bias. To validate the effectiveness of our framework, we construct an evaluation dataset comprising 800 challenging cases. Extensive experiments on our challenging testset and public benchmarks demonstrate that RADAR significantly outperforms baseline evaluation methods across multiple dimensions, including accuracy, stability, and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87% improvement in risk identification accuracy compared to the strongest baseline evaluation method.', 'abstract_zh': '现有大型语言模型安全评估方法固有的局限性，包括评估者偏见和由于模型同质性引起的能力检测失败，这些共同削弱了风险评估过程的稳健性。本文通过引入一个理论框架来重新审视风险评估范式，该框架重构了潜在的风险概念空间。具体地，我们将潜在的风险概念空间分解为三个互斥子空间：明确的风险子空间（涵盖直接违反安全指南的行为）、隐含的风险子空间（捕捉需要情境推理来识别的潜在恶意内容）和无风险子空间。此外，我们提出了一种名为RADAR的多智能体协作评估框架，该框架利用四种专门互补角色的多轮辩论机制，并采用动态更新机制以实现风险概念分布的自我进化。这种方法使得覆盖范围既包括明确风险也包括隐含风险，同时减轻了评估者偏见。为了验证我们框架的有效性，我们构建了一个包含800个具有挑战性的案例的评估数据集。我们在具有挑战性的测试集和公开基准上的广泛实验表明，RADAR在多个维度（包括准确度、稳定性和自我评估风险敏感性）上显著优于基线评估方法。值得注意的是，RADAR在风险识别准确性方面比最强的基线评估方法提高了28.87%。', 'title_zh': 'RADAR：一种基于角色专业化协作的风险意识动态多Agent框架用于LLM安全性评估'}
{'arxiv_id': 'arXiv:2509.25260', 'title': 'Language Model Planning from an Information Theoretic Perspective', 'authors': 'Muhammed Ustaomeroglu, Baris Askin, Gauri Joshi, Carlee Joe-Wong, Guannan Qu', 'link': 'https://arxiv.org/abs/2509.25260', 'abstract': 'The extent to which decoder-only language models (LMs) engage in planning, that is, organizing intermediate computations to support coherent long-range generation, remains an open and important question, with implications for interpretability, reliability, and principled model design. Planning involves structuring computations over long horizons, considering multiple possible continuations, and selectively reusing past information, but how effectively transformer-based LMs realize these capabilities is still unclear. We address these questions by analyzing the hidden states at the core of transformer computations, which capture intermediate results and act as carriers of information. Since these hidden representations are often redundant and encumbered with fine-grained details, we develop a pipeline based on vector-quantized variational autoencoders that compresses them into compact summary codes. These codes enable measuring mutual information, allowing systematic analysis of the computational structure underlying model behavior. Using this framework, we study planning in LMs across synthetic grammar, path-finding tasks, and natural language datasets, focusing on three key aspects: (i) the planning horizon of pre-output computations, (ii) the extent to which the model considers alternative valid continuations, and (iii) the reliance of new predictions on earlier computations. By answering these questions, we advance the understanding of how planning is realized in LMs and contribute a general-purpose pipeline for probing the internal dynamics of LMs and deep learning systems. Our results reveal that the effective planning horizon is task-dependent, that models implicitly preserve information about unused correct continuations, and that predictions draw most on recent computations, though earlier blocks remain informative.', 'abstract_zh': '解码器导向语言模型在规划中的程度，即它们在支持长距离连贯生成时组织中间计算的能力，仍是有待探索的重要问题，对可解释性、可靠性及原理性模型设计具有重要意义。我们通过分析Transformer计算的核心隐状态来探讨这些问题，这些隐状态捕获中间结果并作为信息载体。由于这些隐表示通常是冗余的且含有细粒度的细节，我们开发了一种基于向量量化变分自编码器的管道，将其压缩为紧凑的摘要码。这些代码可以用于测量互信息，从而系统地分析模型行为背后的计算结构。利用这一框架，我们在合成语法规则、路径查找任务和自然语言数据集中研究语言模型中的规划，重点关注三个方面：（i）预输出计算的规划时间跨度，（ii）模型考虑替代有效续写的情况程度，（iii）新预测对先前计算的依赖程度。通过回答这些问题，我们推进了对语言模型中规划实现机制的理解，并提供了一种通用管道以探究语言模型和深度学习系统的内部动态。我们的研究结果揭示了有效规划时间跨度取决于任务，模型隐式保留未使用的正确续写的相关信息，且预测主要依赖最近的计算，但早期模块仍然具有信息价值。', 'title_zh': '语言模型规划从信息论视角出发'}
{'arxiv_id': 'arXiv:2509.25252', 'title': 'Fact Grounded Attention: Eliminating Hallucination in Large Language Models Through Attention Level Knowledge Integration', 'authors': 'Aayush Gupta', 'link': 'https://arxiv.org/abs/2509.25252', 'abstract': '"The greatest enemy of knowledge is not ignorance, it is the illusion of knowledge." Large Language Models have conquered natural language but remain prisoners of their own probabilistic nature--confidently hallucinating facts they never truly knew. We present Fact Grounded Attention (FGA), a novel architectural modification that transforms unreliable language models into deterministic truth tellers by injecting verifiable knowledge directly into the attention mechanism. Unlike existing approaches that patch hallucinations after generation or prepend retrieved text, FGA intervenes at the mathematical heart of the transformer--the pre-softmax attention scores--creating a model that cannot hallucinate when facts exist in its knowledge base. Our experiments across 1,107 technical queries spanning smartphones, laptops, and electric vehicles demonstrate a transformation from 6.3% accuracy in vanilla Llama 3.2 to 99.7% accuracy with FGA. More critically, knowledge updates occur in under one second without retraining, compared to hours for parameter editing approaches. FGA doesn\'t just reduce hallucination--it eliminates it entirely for verifiable facts, marking a fundamental shift from probabilistic approximation to deterministic precision in neural language generation.', 'abstract_zh': '知识的最大敌人不是无知，而是知识的错觉——大型语言模型征服了自然语言，但仍然受制于其概率性质——自信地胡言乱语一些它们从未真正知道的事实。我们提出了事实支持注意力（FGA），这是一种新型架构修改，通过直接将可验证的知识注入注意力机制，将不可靠的语言模型转换为确定性的事实讲述者。与现有方法在生成后修补胡言乱语或前置检索文本不同，FGA 在变压器的数学核心——预softmax注意力分数——处进行干预，创建一个在知识库中存在事实时无法胡言乱语的模型。我们针对智能手机、笔记本电脑和电动汽车领域的 1,107 个技术查询的实验表明，FGA 将 Llama 3.2 的准确率从 6.3% 提高到 99.7%。更重要的是，知识更新只需不到一秒钟而无需重新训练，而参数编辑方法则需要数小时。FGA 不仅仅是减少了胡言乱语——它完全消除了可验证事实的胡言乱语，标志着神经语言生成从概率近似到确定性精确的根本转变。', 'title_zh': '基于事实的注意力：通过注意力层知识集成消除大型语言模型中的幻觉'}
{'arxiv_id': 'arXiv:2509.25239', 'title': 'A Formal Comparison Between Chain-of-Thought and Latent Thought', 'authors': 'Kevin Xu, Issei Sato', 'link': 'https://arxiv.org/abs/2509.25239', 'abstract': 'Chain-of-Thought (CoT) elicits reasoning in large language models by explicitly generating intermediate steps in natural language. In contrast, Latent Thought in looped models operates directly in the continuous latent space, enabling computation beyond discrete linguistic representations. While both approaches exploit iterative computation, their comparative capabilities remain underexplored. In this work, we present a formal analysis showing that Latent Thought in Looped Transformers enables parallel computation, which is more efficient than the inherently sequential process of CoT. In contrast, CoT leverages stochastic decoding to approximate solutions to problems where exact computation is intractable. These separations suggest the tasks for which depth-driven recursion is more suitable, thereby offering practical guidance for choosing between reasoning paradigms. Code is available at this https URL.', 'abstract_zh': '递归模型中的潜在思维 enabling 并行计算，相较于链式思维具有更高效的过程，从而为选择推理范式提供了实用指导。源代码请参见 this https URL。', 'title_zh': '链式思维与潜在思维的形式比较'}
{'arxiv_id': 'arXiv:2509.25229', 'title': 'Blueprint-Bench: Comparing spatial intelligence of LLMs, agents and image models', 'authors': 'Lukas Petersson, Axel Backlund, Axel Wennstöm, Hanna Petersson, Callum Sharrock, Arash Dabiri', 'link': 'https://arxiv.org/abs/2509.25229', 'abstract': 'We introduce Blueprint-Bench, a benchmark designed to evaluate spatial reasoning capabilities in AI models through the task of converting apartment photographs into accurate 2D floor plans. While the input modality (photographs) is well within the training distribution of modern multimodal models, the task of spatial reconstruction requires genuine spatial intelligence: inferring room layouts, understanding connectivity, and maintaining consistent scale. We evaluate leading language models (GPT-5, Claude 4 Opus, Gemini 2.5 Pro, Grok-4), image generation models (GPT-Image, NanoBanana), and agent systems (Codex CLI, Claude Code) on a dataset of 50 apartments with approximately 20 interior images each. Our scoring algorithm measures similarity between generated and ground-truth floor plans based on room connectivity graphs and size rankings. Results reveal a significant blind spot in current AI capabilities: most models perform at or below a random baseline, while human performance remains substantially superior. Image generation models particularly struggle with instruction following, while agent-based approaches with iterative refinement capabilities show no meaningful improvement over single-pass generation. Blueprint-Bench provides the first numerical framework for comparing spatial intelligence across different model architectures. We will continue evaluating new models as they are released and welcome community submissions, monitoring for the emergence of spatial intelligence in generalist AI systems.', 'abstract_zh': 'Blueprint-Bench: 一种通过将公寓照片转换为准确的2D楼层平面图来评估AI模型空间推理能力的基准测试', 'title_zh': 'Blueprint-Bench: 比较LLMs、代理和图像模型的空间智能'}
{'arxiv_id': 'arXiv:2509.26625', 'title': 'Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training', 'authors': 'Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos', 'link': 'https://arxiv.org/abs/2509.26625', 'abstract': "Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.", 'abstract_zh': '大型语言模型（LLMs）虽然仅基于文本训练，但却意外地发展出了丰富的视觉先验。这些先验使得通过相对少量的多模态数据解锁视觉任务的潜在视觉能力成为可能，在某些情况下，甚至可以在从未见过图像的情况下完成视觉任务。通过系统的分析，我们揭示了视觉先验——即语言预训练期间获得的视觉世界的隐式、 emergent 知识——是由可分离的感知先验和推理先验组成，具有独特的缩放趋势和起源。我们表明，LLM 的潜在视觉推理能力主要通过以推理为中心的数据（例如，代码、数学、学术）进行预训练而发展，并且随着训练逐步增强。从语言预训练中获得的推理先验具有可转移性和普遍适用性，适用于视觉推理。相比之下，感知先验则从广泛的语料库中逐渐涌现，感知能力对视觉编码器和视觉指令调优数据更为敏感。同时，描述视觉世界的文本对于视觉推理至关重要，尽管其性能影响快速饱和。基于这些见解，我们提出了一种基于数据的预训练视觉感知 LLM 的方法，并在 1T 令牌规模的预训练中进行了验证。我们的发现基于超过 100 次受控实验，消耗了 500,000 GPU 小时，覆盖了从 LLM 预训练到视觉对齐和监督多模态微调的完整 MLLM 构建管道，跨越了五个模型规模、多种数据类别和混合以及多种适应设置。除了主要发现外，我们还提出了并研究了几种假设，并引入了多级存在基准（MLE-Bench）。总之，这项工作为有意图地从语言预训练中培养视觉先验提供了一种新方法，为下一代多模态 LLM 的发展铺平了道路。', 'title_zh': '在见其形之前学会预见：揭开语言预训练中视觉先验的神秘面纱'}
{'arxiv_id': 'arXiv:2509.26601', 'title': 'MENLO: From Preferences to Proficiency - Evaluating and Modeling Native-like Quality Across 47 Languages', 'authors': 'Chenxi Whitehouse, Sebastian Ruder, Tony Lin, Oksana Kurylo, Haruka Takagi, Janice Lam, Nicolò Busetto, Denise Diaz', 'link': 'https://arxiv.org/abs/2509.26601', 'abstract': "Ensuring native-like quality of large language model (LLM) responses across many languages is challenging. To address this, we introduce MENLO, a framework that operationalizes the evaluation of native-like response quality based on audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423 human-annotated prompt-response preference pairs covering four quality dimensions with high inter-annotator agreement in 47 language varieties. Our evaluation reveals that zero-shot LLM judges benefit significantly from pairwise evaluation and our structured annotation rubrics, yet they still underperform human annotators on our dataset. We demonstrate substantial improvements through fine-tuning with reinforcement learning, reward shaping, and multi-task learning approaches. Additionally, we show that RL-trained judges can serve as generative reward models to enhance LLMs' multilingual proficiency, though discrepancies with human judgment remain. Our findings suggest promising directions for scalable multilingual evaluation and preference alignment. We release our dataset and evaluation framework to support further research in multilingual LLM evaluation.", 'abstract_zh': '确保多语言大型语言模型（LLM）响应的土著级质量挑战重重。为此，我们提出MENLO框架，该框架基于观众设计启发机制操作化评判土著级响应质量。利用MENLO，我们构建了一个包含47种语言变体、6,423个人标注提示-响应偏好对的数据集，覆盖四个质量维度，并具有高注释者间一致性。我们的评估显示，零样本LLM评判者受益于成对评估和结构化注释量表，但仍低于我们在数据集上的人类评判者。通过强化学习、奖励塑造和多任务学习方法进行微调，我们展示了显著的改进。此外，我们证明了通过强化学习训练的评判者可以作为生成奖励模型，提高LLM的多语言能力，尽管与人类判断之间仍存在差异。我们的研究结果指出了可扩展的多语言评估和偏好对齐的前景方向。我们发布了数据集和评价框架，以支持多语言LLM评估的进一步研究。', 'title_zh': 'MENLO: 从偏好到 proficiency - 评估和建模47种语言的原生质量'}
{'arxiv_id': 'arXiv:2509.26600', 'title': 'Deconstructing Self-Bias in LLM-generated Translation Benchmarks', 'authors': 'Wenda Xu, Sweta Agrawal, Vilém Zouhar, Markus Freitag, Daniel Deutsch', 'link': 'https://arxiv.org/abs/2509.26600', 'abstract': "As large language models (LLMs) begin to saturate existing benchmarks, automated benchmark creation using LLMs (LLM as a benchmark) has emerged as a scalable alternative to slow and costly human curation. While these generated test sets have to potential to cheaply rank models, we demonstrate a critical flaw. LLM generated benchmarks systematically favor the model that created the benchmark, they exhibit self bias on low resource languages to English translation tasks. We show three key findings on automatic benchmarking of LLMs for translation: First, this bias originates from two sources: the generated test data (LLM as a testset) and the evaluation method (LLM as an evaluator), with their combination amplifying the effect. Second, self bias in LLM as a benchmark is heavily influenced by the model's generation capabilities in the source language. For instance, we observe more pronounced bias in into English translation, where the model's generation system is developed, than in out of English translation tasks. Third, we observe that low diversity in source text is one attribution to self bias. Our results suggest that improving the diversity of these generated source texts can mitigate some of the observed self bias.", 'abstract_zh': '大型语言模型（LLMs）生成的基准测试：来源语言生成能力与自我偏见的关系研究', 'title_zh': '拆解LLM生成的翻译基准中的自我偏差'}
{'arxiv_id': 'arXiv:2509.26507', 'title': 'The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain', 'authors': 'Adrian Kosowski, Przemysław Uznański, Jan Chorowski, Zuzanna Stamirowska, Michał Bartoszkiewicz', 'link': 'https://arxiv.org/abs/2509.26507', 'abstract': "The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models.\nWe introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of \\$n\\$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance.\nBDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data.\nBDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech.\nBDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.", 'abstract_zh': '计算系统与大脑之间的关系自约翰·冯·诺伊曼和艾伦·图灵以来一直是开拓性理论家的研究动机。均匀的无标度生物网络，如大脑，具有强大的特性，包括时间泛化能力，这是机器学习通往通用推理模型道路上的主要障碍。\n\n Dragon Hatchling (BDH): 基于局部相互作用神经粒子无标度生物启发网络的新大规模语言模型架构，其兼具强大的理论基础和内在可解释性，而不牺牲类似于Transformer的性能。\n\nBDH是一种实际的、高性能的基于注意力的状态空间序列学习架构。除了是图模型外，BDH还具有GPU友好的表示形式。它表现出类似于Transformer的比例法则：实验表明，BDH在语言和翻译任务中，与GPT2的性能相当，在相同数量的参数（100万至1000万）和相同训练数据的情况下。\n\nBDH可以视为一种脑模型，在推断过程中，BDH的工作记忆完全依赖于使用尖峰神经元进行Hebbian学习的突触可塑性。我们实验证明，在处理语言输入时，特定的个别突触在BDH听到或推理特定概念时会加强其连接性。BDH的神经元交互网络具有高模块性和重尾度分布图。BDH模型具有生物可行性，解释了人类神经元可能用于实现语音的一种可能机制。\n\nBDH旨在增强可解释性。BDH的激活向量是稀疏且正的。我们在语言任务中展示了BDH的单义性。超越神经元和模型参数的可解释性状态的可解释性是BDH架构的一个固有特性。', 'title_zh': '幼龙：Transformer与脑模型之间的缺失环节'}
{'arxiv_id': 'arXiv:2509.26490', 'title': 'VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications', 'authors': 'Wei He, Yueqing Sun, Hongyan Hao, Xueyuan Hao, Zhikang Xia, Qi Gu, Chengcheng Han, Dengchang Zhao, Hui Su, Kefeng Zhang, Man Gao, Xi Su, Xiaodong Cai, Xunliang Cai, Yu Yang, Yunke Zhao', 'link': 'https://arxiv.org/abs/2509.26490', 'abstract': 'As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at this https URL', 'abstract_zh': '基于LLM的代理在真实场景中广泛应用时，现有基准未能捕捉到其处理大量信息、利用多样资源及管理动态用户交互的内在复杂性。为填补这一空白，我们引入了VitaBench这一具有挑战性的基准，评估代理在真实世界环境中执行多样化交互任务的能力。VitaBench借鉴了餐饮配送、店内消费和在线旅行服务等日常生活应用，为代理提供了迄今为止最复杂的服务生活模拟环境，包含66种工具。通过消除领域特定策略的框架，我们使这些场景和工具的灵活组合成为可能，产生100个跨场景任务（主要结果）和300个单一场景任务。每个任务都源自多个真实用户的请求，要求代理在时间和空间维度上进行推理、使用复杂的工具集、主动澄清模糊指令，并在整个多轮对话中跟踪用户意图的变化。此外，我们提出了一种基于评分准则的滑动窗口评估器，能够在复杂环境和随机交互中对多种解决方案路径进行稳健评估。全面的评估结果显示，最先进的模型在跨场景任务上的成功率仅为30%，而在其他任务上的成功率也低于50%。总体而言，我们相信VitaBench将作为有价值的资源，推动实际应用场景中AI代理的发展。代码、数据集和排行榜可在以下链接获取。', 'title_zh': 'VitaBench：在实际应用中通过多样化的互动任务评估语言模型代理'}
{'arxiv_id': 'arXiv:2509.26433', 'title': 'ACT: Agentic Classification Tree', 'authors': 'Vincent Grari, Tim Arni, Thibault Laugel, Sylvain Lamprier, James Zou, Marcin Detyniecki', 'link': 'https://arxiv.org/abs/2509.26433', 'abstract': 'When used in high-stakes settings, AI systems are expected to produce decisions that are transparent, interpretable, and auditable, a requirement increasingly expected by regulations. Decision trees such as CART provide clear and verifiable rules, but they are restricted to structured tabular data and cannot operate directly on unstructured inputs such as text. In practice, large language models (LLMs) are widely used for such data, yet prompting strategies such as chain-of-thought or prompt optimization still rely on free-form reasoning, limiting their ability to ensure trustworthy behaviors. We present the Agentic Classification Tree (ACT), which extends decision-tree methodology to unstructured inputs by formulating each split as a natural-language question, refined through impurity-based evaluation and LLM feedback via TextGrad. Experiments on text benchmarks show that ACT matches or surpasses prompting-based baselines while producing transparent and interpretable decision paths.', 'abstract_zh': '当应用于高 stakes 环境时，AI系统被期望生成透明、可解释和可审计的决策，这一要求越来越受到法规的重视。决策树如CART提供了清晰和可验证的规则，但它们受限于结构化的表格数据，无法直接处理如文本等非结构化输入。实践中，大型语言模型（LLMs）广泛用于此类数据，然而诸如链式思考或提示优化等提示策略仍然依赖于自由形式的推理，限制了其确保可信赖行为的能力。我们提出了Agentic Classification Tree（ACT），通过将每个分裂表述为经过杂质评估和LLM反馈（通过TextGrad） refinement的自然语言问题，将决策树方法扩展到非结构化输入。在文本基准测试上的实验表明，ACT在生成透明和可解释决策路径的同时，能够匹配或超越基于提示的方法。', 'title_zh': 'AGentic 分类树'}
{'arxiv_id': 'arXiv:2509.26432', 'title': 'AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size', 'authors': 'Guanxi Lu, Chen, Yuto Karashima, Zhican Wang, Daichi Fujiki, Hongxiang Fan', 'link': 'https://arxiv.org/abs/2509.26432', 'abstract': 'Diffusion-based large language models (dLLMs) are gaining attention for their inherent capacity for parallel decoding, offering a compelling alternative to autoregressive LLMs. Among various decoding strategies, blockwise semi-autoregressive (semi-AR) approaches are widely adopted due to their natural support for KV caching and their favorable accuracy-speed trade-off. However, this paper identifies two fundamental limitations in the conventional semi-AR decoding approach that applies a fixed block size: i) late decoding overhead, where the unmasking of high-confidence tokens outside the current block is unnecessarily delayed, and ii) premature decoding error, where low-confidence tokens inside the current block are committed too early, leading to incorrect tokens. This paper presents the first systematic investigation challenging the fixed block size assumption in semi-AR decoding. Through a statistical analysis of confidence dynamics during the denoising process, we identify a volatility band (VB) region during dLLM decoding, which encodes local semantic structure and can be used to guide adaptive block sizing. Leveraging these insights, we introduce AdaBlock-dLLM, a training-free, plug-and-play scheduler that adaptively aligns block boundaries with semantic steps by adjusting block size during runtime. Extensive experiments across diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy improvement under the same throughput budget. Beyond inference-time optimization, we hope our semantics-aware adaptive scheduling approach and confidence-based analysis will inspire future training strategies for dLLMs.', 'abstract_zh': '基于扩散的大语言模型（dLLMs）因其固有的并行解码能力而受到关注，提供了与自回归大语言模型（LLMs）竞争的替代方案。在各种解码策略中，块式半自回归（semi-AR）方法因其自然支持KV缓存和有利的准确性和速度权衡而被广泛采用。然而，本文发现了半自回归解码方法中固定块大小的两种基本限制：一是延迟解码开销，其中未能及时解码当前块外高置信度标记；二是过早解码错误，其中内部低置信度标记被过早确认，导致错误标记。本文首次系统地挑战了半自回归解码中固定块大小的假设。通过退噪过程中置信动态的统计分析，我们识别出dLLM解码中的波动区段（VB），该区段编码了局部语义结构，并可用于指导自适应块大小调整。借助这些洞见，我们引入了AdaBlock-dLLM，这是一种无需训练、即插即用的调度器，能够在运行时通过调整块大小来适配语义步骤，从而自适应地对齐块边界。广泛实验证明，AdaBlock-dLLM在相同的吞吐量预算下可实现高达5.3%的准确率提升。此外，我们希望我们的基于语义的自适应调度方法和基于置信度的分析能启发dLLMs的未来训练策略。', 'title_zh': 'AdaBlock-dLLM：基于语义感知的自适应块大小扩散大语言模型推理'}
{'arxiv_id': 'arXiv:2509.26404', 'title': 'SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language Model Was Trained From', 'authors': 'Yao Tong, Haonan Wang, Siquan Li, Kenji Kawaguchi, Tianyang Hu', 'link': 'https://arxiv.org/abs/2509.26404', 'abstract': "Fingerprinting Large Language Models (LLMs) is essential for provenance verification and model attribution. Existing methods typically extract post-hoc signatures based on training dynamics, data exposure, or hyperparameters -- properties that only emerge after training begins. In contrast, we propose a stronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method that leverages random initialization biases as persistent, seed-dependent identifiers present even before training. We show that untrained models exhibit reproducible token selection biases conditioned solely on their parameters at initialization. These biases are stable and measurable throughout training, enabling our statistical detection method to recover a model's lineage with high confidence. Unlike prior techniques, unreliable before convergence and vulnerable to distribution shifts, SeedPrints remains effective across all training stages and robust under domain shifts or parameter modifications. Experiments on LLaMA-style and Qwen-style models show that SeedPrints achieves seed-level distinguishability and can provide birth-to-lifecycle identity verification akin to a biometric fingerprint. Evaluations on large-scale pretrained models and fingerprinting benchmarks further confirm its effectiveness under practical deployment scenarios. These results suggest that initialization itself imprints a unique and persistent identity on neural language models, forming a true ''Galtonian'' fingerprint.", 'abstract_zh': '大型语言模型（LLMs）的指纹识别对于溯源验证和模型归因至关重要。现有的方法通常基于训练动力学、数据暴露或超参数提取后验签名——这些属性只有在训练开始后才会显现。相比之下，我们提出了一种更为强大和内在的LLM指纹识别方法：SeedPrints，该方法利用随机初始化偏差作为持久的、以种子依赖的身份标识，甚至在训练开始之前就存在。我们展示了未训练的模型在其初始化参数上表现出可重现的标记选择偏差。这些偏差在整个训练过程中稳定且可测量，从而使我们的统计检测方法能够以高置信度恢复模型的谱系。与先前的技术不同，后者在收敛前不可靠且易受分布变化的影响，SeedPrints 在所有训练阶段都有效且在领域变化或参数修改下具有鲁棒性。对LLaMA样式和Qwen样式的模型实验表明，SeedPrints 能够实现种子级区分性，并提供从出生到生命周期的身份验证，类似于生物指纹。对大规模预训练模型和指纹识别基准的评估进一步证实了其在实际部署场景下的有效性。这些结果表明，初始化本身会在神经语言模型上留下独特且持久的身份印记，形成一个真正的“高尔登氏指纹”。', 'title_zh': 'SeedPrints：指纹甚至能揭示大型语言模型训练时使用的种子类型'}
{'arxiv_id': 'arXiv:2509.26383', 'title': 'Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning', 'authors': 'Jinyeop Song, Song Wang, Julian Shun, Yada Zhu', 'link': 'https://arxiv.org/abs/2509.26383', 'abstract': 'Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at this https URL.', 'abstract_zh': '基于强化学习的实体KG-R1：一种代理 KG检索增强生成框架', 'title_zh': '通过强化学习实现高效可迁移的代理知识图谱RAG'}
{'arxiv_id': 'arXiv:2509.26302', 'title': 'QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue Summarization', 'authors': 'Mohamed Imed Eddine Ghebriout, Gaël Guibon, Ivan Lerner, Emmanuel Vincent', 'link': 'https://arxiv.org/abs/2509.26302', 'abstract': 'Dialogue summarization aims to distill the core meaning of a conversation into a concise text. This is crucial for reducing the complexity and noise inherent in dialogue-heavy applications. While recent approaches typically train language models to mimic human-written summaries, such supervision is costly and often results in outputs that lack task-specific focus limiting their effectiveness in downstream applications, such as medical tasks. In this paper, we propose \\app, a framework for task-oriented utility-based dialogue summarization. \\app starts by generating multiple summaries and task-oriented question-answer pairs from a dialogue in a zero-shot manner using a pool of large language models (LLMs). The quality of the generated summaries is evaluated by having LLMs answer task-related questions before \\textit{(i)} selecting the best candidate answers and \\textit{(ii)} identifying the most informative summary based on these answers. Finally, we fine-tune the best LLM on the selected summaries. When validated on multiple datasets, \\app demonstrates its effectiveness by achieving competitive results in various zero-shot settings, rivaling fully-supervised State-of-the-Art (SotA) methods.', 'abstract_zh': '对话摘要旨在提炼对话的核心意义，以简洁的文字呈现。这对于减少对话密集型应用中的复杂性和噪音至关重要。虽然近期的方法通常通过模拟人类撰写的摘要来训练语言模型，但这种监督成本高昂，且常常导致输出缺乏任务特异性，限制了其在下游应用中的效果，例如医疗任务。在本文中，我们提出了一种基于任务的实用性对话摘要框架 \\app。\\app 通过使用一系列大型语言模型（LLMs）以零样本的方式从对话中生成多个摘要和任务相关的问答对来开始这一过程。生成的摘要质量通过让LLMs回答任务相关问题来评估，在此之后进行 \\textit{(i)} 选择最佳候选答案和 \\textit{(ii)} 识别基于这些答案的最相关信息性摘要的步骤。最后，我们对选中的摘要进行微调。当在多个数据集上验证时，\\app 在多种零样本设置中展示了其有效性，与完全监督的最先进（SotA）方法不相上下。', 'title_zh': 'QUARTZ : 基于QA的无监督抽象化 refinement 用于任务导向对话摘要'}
{'arxiv_id': 'arXiv:2509.26242', 'title': 'Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing', 'authors': 'Yang Tang, Ruijie Liu, Yifan Wang, Shiyu Li, Xi Chen', 'link': 'https://arxiv.org/abs/2509.26242', 'abstract': 'Large language models (LLMs) fine-tuning shows excellent implications. However, vanilla fine-tuning methods often require intricate data mixture and repeated experiments for optimal generalization. To address these challenges and streamline the training process, we propose an efficient and universal solution, Dynamic Boosted Annealing (DBA). We obtain a global gradient through zero-learning-rate training on general data, which is subsequently employed for gradient boosting and dynamic training step correction during domain training. In conjunction with annealing learning, we end up establishing a fine-tuning pipeline that relies solely on domain data without collapse. By evaluating both general and domain-specific performance across multiple tasks on several popular base models, DBA achieves an average improvement of 5.8% in joint performance over vanilla fine-tuning. Furthermore, since general data is no longer involved in annealing, repeated experiments led by data mixture are also eliminated. According to our tests, the DBA method can reduce GPU hours by 91.0% compared to the vanilla method.', 'abstract_zh': '大型语言模型（LLMs）微调展示了卓越的潜力。然而，传统的微调方法通常需要复杂的数据混合和多次实验以达到最优泛化。为了解决这些挑战并简化训练过程，我们提出了一种高效和通用的解决方案，动态增强退火（DBA）。我们通过在通用数据上进行零学习率训练获取全局梯度，随后用于梯度增强和领域训练中的动态训练步长修正。结合退火学习，我们最终建立了一种仅依赖领域数据且不会崩溃的微调流水线。通过在多个流行的基础模型上对各种任务的一般性能和领域特定性能进行评估，DBA 在联合性能上相对于传统的微调方法平均提高了 5.8%。此外，由于通用数据不再参与退火过程，因此由数据混合引发的重复实验也得以消除。根据我们的测试，与传统的微调方法相比，DBA 方法可以将 GPU 小时降低 91.0%。', 'title_zh': '一次性微调：解耦通用学习与领域学习的动态增强退火'}
{'arxiv_id': 'arXiv:2509.26225', 'title': 'An Experimental Study on Generating Plausible Textual Explanations for Video Summarization', 'authors': 'Thomas Eleftheriadis, Evlampios Apostolidis, Vasileios Mezaris', 'link': 'https://arxiv.org/abs/2509.26225', 'abstract': "In this paper, we present our experimental study on generating plausible textual explanations for the outcomes of video summarization. For the needs of this study, we extend an existing framework for multigranular explanation of video summarization by integrating a SOTA Large Multimodal Model (LLaVA-OneVision) and prompting it to produce natural language descriptions of the obtained visual explanations. Following, we focus on one of the most desired characteristics for explainable AI, the plausibility of the obtained explanations that relates with their alignment with the humans' reasoning and expectations. Using the extended framework, we propose an approach for evaluating the plausibility of visual explanations by quantifying the semantic overlap between their textual descriptions and the textual descriptions of the corresponding video summaries, with the help of two methods for creating sentence embeddings (SBERT, SimCSE). Based on the extended framework and the proposed plausibility evaluation approach, we conduct an experimental study using a SOTA method (CA-SUM) and two datasets (SumMe, TVSum) for video summarization, to examine whether the more faithful explanations are also the more plausible ones, and identify the most appropriate approach for generating plausible textual explanations for video summarization.", 'abstract_zh': '本文呈现了我们对生成视频摘要结果的可信文本解释的实验研究。为了满足这一研究需求，我们通过集成当前最先进的大规模多模态模型（LLaVA-OneVision）并对其进行提示，扩展了一个现有的视频摘要多粒度解释框架，以生成所获得视觉解释的自然语言描述。随后，我们关注解释可解释人工智能中最受推崇的特性之一——获得解释的可信性，这与人类的推理和期望的契合程度相关。采用扩展的框架，我们提出了一种通过量化视觉解释的文本描述与其对应视频摘要文本描述的语义重叠来评估解释可信性的方法，借助两种句子嵌入方法（SBERT, SimCSE）。基于扩展的框架和提出的可信性评估方法，我们使用当前最先进的方法（CA-SUM）和两个数据集（SumMe, TVSum）进行实验研究，以检查更忠实的解释是否也是更可信的解释，并识别生成视频摘要可信文本解释的最佳方法。', 'title_zh': '视频摘要的可信文本解释生成实验研究'}
{'arxiv_id': 'arXiv:2509.26224', 'title': 'Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models', 'authors': 'Alessandro De Bellis, Salvatore Bufi, Giovanni Servedio, Vito Walter Anelli, Tommaso Di Noia, Eugenio Di Sciascio', 'link': 'https://arxiv.org/abs/2509.26224', 'abstract': 'Inductive link prediction is emerging as a key paradigm for real-world knowledge graphs (KGs), where new entities frequently appear and models must generalize to them without retraining. Predicting links in a KG faces the challenge of guessing previously unseen entities by leveraging generalizable node features such as subgraph structure, type annotations, and ontological constraints. However, explicit type information is often lacking or incomplete. Even when available, type information in most KGs is often coarse-grained, sparse, and prone to errors due to human annotation. In this work, we explore the potential of pre-trained language models (PLMs) to enrich node representations with implicit type signals. We introduce TyleR, a Type-less yet type-awaRe approach for subgraph-based inductive link prediction that leverages PLMs for semantic enrichment. Experiments on standard benchmarks demonstrate that TyleR outperforms state-of-the-art baselines in scenarios with scarce type annotations and sparse graph connectivity. To ensure reproducibility, we share our code at this https URL .', 'abstract_zh': '基于子图的无类型却敏感于类型的预训练语言模型增强的归纳链接预测', 'title_zh': '无类型 yet 具有类型意识的归纳链接预测预训练语言模型'}
{'arxiv_id': 'arXiv:2509.26200', 'title': 'Toward an Unbiased Collective Memory for Efficient LLM-Based Agentic 6G Cross-Domain Management', 'authors': 'Hatim Chergui, Miguel Catalan Cid, Pouria Sayyad Khodashenas, Daniel Camps Mur, Christos Verikoukis', 'link': 'https://arxiv.org/abs/2509.26200', 'abstract': 'This paper introduces a novel framework for proactive cross-domain resource orchestration in 6G RAN-Edge networks, featuring large language model (LLM)-augmented agents. The system comprises specialized RAN (energy efficiency) and Edge (latency assurance) agents that engage in iterative negotiation, supported by advanced reasoning and planning capabilities. Agents dynamically interact with a digital twin (DT) to test their proposals and leverage a long-term collective memory where their joint successful and failed agreements along with the related network contexts are distilled into strategies to either follow or avoid and subsequently stored. Given that agents are subject to a plethora of cognitive distortions when retrieving those past experiences -- such as primacy, recency, confirmation and availability biases -- we propose in this work a novel unbiased memory design (A reusable mockup version of the unbiased memory source code is available for non-commercial use at this https URL). featuring (i) semantic retrieval of past strategies via Jaccard similarity; (ii) learning from failures through amplified weighting of SLA violations and mandatory inclusion of failed negotiation cases to mitigate confirmation bias; (iii) diversity enforcement to minimize availability bias and (iv) recency and primacy weighting with slow decay to counteract temporal biases. Evaluation results showcase the impact of existing biases and how the unbiased memory allows to tackle them by learning from both successful and failed strategies, either present or old, resulting in $\\times 4.5$ and $\\times 3.5$ reductions of unresolved negotiations compared to non-memory and vanilla memory baselines, respectively, while totally mitigating SLA violations as well as improving latency and energy saving distributions.', 'abstract_zh': '基于大型语言模型增强代理的 proactive 跨域资源orchestration框架：应用于6G RAN-Edge网络', 'title_zh': '朝着公正的集体记忆方向发展，以提高基于大语言模型的智能6G跨域管理效率'}
{'arxiv_id': 'arXiv:2509.26184', 'title': 'Auto-ARGUE: LLM-Based Report Generation Evaluation', 'authors': 'William Walden, Marc Mason, Orion Weller, Laura Dietz, Hannah Recknor, Bryan Li, Gabrielle Kaili-May Liu, Yu Hou, James Mayfield, Eugene Yang', 'link': 'https://arxiv.org/abs/2509.26184', 'abstract': 'Generation of long-form, citation-backed reports is a primary use case for retrieval augmented generation (RAG) systems. While open-source evaluation tools exist for various RAG tasks, ones tailored to report generation are lacking. Accordingly, we introduce Auto-ARGUE, a robust LLM-based implementation of the recent ARGUE framework for report generation evaluation. We present analysis of Auto-ARGUE on the report generation pilot task from the TREC 2024 NeuCLIR track, showing good system-level correlations with human judgments. We further release a web app for visualization of Auto-ARGUE outputs.', 'abstract_zh': '生成长格式、引用支持的报告是检索增强生成（RAG）系统的主要应用场景。虽然存在多种开源的RAG任务评估工具，但针对报告生成的评估工具尚缺乏。因此，我们引入了Auto-ARGUE，这是一个基于大规模语言模型的ARGUE框架实现，用于报告生成评估。我们在TREC 2024 NeuCLIR赛道的报告生成试点任务上对Auto-ARGUE进行了分析，结果显示其在系统水平上与人类判断高度相关。我们进一步发布了用于自动展示Auto-ARGUE输出结果的网络应用。', 'title_zh': 'Auto-ARGUE：基于LLM的报告生成评估'}
{'arxiv_id': 'arXiv:2509.26158', 'title': 'Towards Continual Expansion of Data Coverage: Automatic Text-guided Edge-case Synthesis', 'authors': 'Kyeongryeol Go', 'link': 'https://arxiv.org/abs/2509.26158', 'abstract': 'The performance of deep neural networks is strongly influenced by the quality of their training data. However, mitigating dataset bias by manually curating challenging edge cases remains a major bottleneck. To address this, we propose an automated pipeline for text-guided edge-case synthesis. Our approach employs a Large Language Model, fine-tuned via preference learning, to rephrase image captions into diverse textual prompts that steer a Text-to-Image model toward generating difficult visual scenarios. Evaluated on the FishEye8K object detection benchmark, our method achieves superior robustness, surpassing both naive augmentation and manually engineered prompts. This work establishes a scalable framework that shifts data curation from manual effort to automated, targeted synthesis, offering a promising direction for developing more reliable and continuously improving AI systems. Code is available at this https URL.', 'abstract_zh': '深度神经网络的性能强烈依赖于其训练数据的质量。然而，通过手动筛选具有挑战性的边缘案例以减轻数据集偏差仍然是一个主要瓶颈。为解决这一问题，我们提出了一种文本导向边缘案例合成的自动化管道。我们的方法通过偏好学习 fine-tune 一个大语言模型，将其图像描述重述为多样化的文本提示，以引导文本到图像模型生成具有挑战性的视觉场景。在 FishEye8K 对象检测基准上，我们的方法表现出更优越的鲁棒性，超越了简单的数据增强和手动工程化的提示。这项工作建立了一个可扩展的框架，将数据标注从手动努力转移到自动化、目标化的合成，为开发更可靠且持续改进的AI系统提供了有 promise 的方向。代码可在以下网址获取。', 'title_zh': '面向数据覆盖持续扩展的自动文本引导边缘案例合成'}
{'arxiv_id': 'arXiv:2509.26140', 'title': 'OWL: Geometry-Aware Spatial Reasoning for Audio Large Language Models', 'authors': 'Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam', 'link': 'https://arxiv.org/abs/2509.26140', 'abstract': "Spatial reasoning is fundamental to auditory perception, yet current audio large language models (ALLMs) largely rely on unstructured binaural cues and single step inference. This limits both perceptual accuracy in direction and distance estimation and the capacity for interpretable reasoning. Recent work such as BAT demonstrates spatial QA with binaural audio, but its reliance on coarse categorical labels (left, right, up, down) and the absence of explicit geometric supervision constrain resolution and robustness. We introduce the $\\textbf{Spatial-Acoustic Geometry Encoder (SAGE}$), a geometry-aware audio encoder that aligns binaural acoustic features with 3D spatial structure using panoramic depth images and room-impulse responses at training time, while requiring only audio at inference. Building on this representation, we present $\\textbf{OWL}$, an ALLM that integrates $\\textbf{SAGE}$ with a spatially grounded chain-of-thought to rationalize over direction-of-arrivals (DoA) and distance estimates. Through curriculum learning from perceptual QA to multi-step reasoning, $\\textbf{OWL}$ supports o'clock-level azimuth and DoA estimation. To enable large-scale training and evaluation, we construct and release $\\textbf{BiDepth}$, a dataset of over one million QA pairs combining binaural audio with panoramic depth images and room impulse responses across both in-room and out-of-room scenarios. Across two benchmark datasets, our new $\\textbf{BiDepth}$ and the public SpatialSoundQA, $\\textbf{OWL}$ reduces mean DoA error by $\\textbf{11$^{\\circ}$}$ through $\\textbf{SAGE}$ and improves spatial reasoning QA accuracy by up to $\\textbf{25}$\\% over BAT.", 'abstract_zh': '空间关系推理对于听觉感知至关重要，但当前的音频大规模语言模型（ALLMs）主要依赖于未结构化的双耳线索和单步推理。这限制了方向和距离估计的感知准确性以及可解释推理的能力。虽然像BAT这样的研究表明了使用双耳音频的空间问答，但其依赖于粗略的分类标签（左、右、上、下）并且缺乏显式的几何监督限制了其分辨率和稳健性。我们引入了**空间声学几何编码器（SAGE）**，这是一种几何感知的音频编码器，通过全景深度图像和房间冲激响应在训练时对齐双耳声学特征与三维空间结构，而在推理时仅需音频输入。在此表示基础上，我们提出**OWL**，一种将**SAGE**与空间地指导的推理链条结合的ALLM，用于解释到达方向（DoA）和距离估计。通过从感知问答到多步推理的渐进式学习，**OWL**支持分钟级的方位角和DoA估计。为实现大规模训练和评估，我们构建并发布了**BiDepth**数据集，包含超过一百万对双耳音频、全景深度图像和房间冲激响应，涵盖室内和室外场景。在两个基准数据集和公共的SpatialSoundQA数据集上，**OWL**通过**SAGE**将平均DoA误差减少了**11°**，并将空间推理问答准确性提高了最多**25%**，超过BAT。', 'title_zh': 'OWL：面向几何的空间推理在音频大规模语言模型中的应用'}
{'arxiv_id': 'arXiv:2509.26103', 'title': 'End-to-End Aspect-Guided Review Summarization at Scale', 'authors': 'Ilya Boytsov, Vinny DeGenova, Mikhail Balyasin, Joseph Walt, Caitlin Eusden, Marie-Claire Rochat, Margaret Pierson', 'link': 'https://arxiv.org/abs/2509.26103', 'abstract': 'We present a scalable large language model (LLM)-based system that combines aspect-based sentiment analysis (ABSA) with guided summarization to generate concise and interpretable product review summaries for the Wayfair platform. Our approach first extracts and consolidates aspect-sentiment pairs from individual reviews, selects the most frequent aspects for each product, and samples representative reviews accordingly. These are used to construct structured prompts that guide the LLM to produce summaries grounded in actual customer feedback. We demonstrate the real-world effectiveness of our system through a large-scale online A/B test. Furthermore, we describe our real-time deployment strategy and release a dataset of 11.8 million anonymized customer reviews covering 92,000 products, including extracted aspects and generated summaries, to support future research in aspect-guided review summarization.', 'abstract_zh': '我们提出了一种可扩展的大规模语言模型（LLM）系统，该系统结合了方面基于的情感分析（ABSA）和引导式总结，以生成Wayfair平台上产品的简洁可解释的产品评论摘要。该方法首先从个别评论中提取和整合方面情感对，选择每个产品的最频繁方面，并相应地采样代表性评论。这些评论用于构建结构化的提示，以引导LLM生成基于实际客户反馈的摘要。我们通过大规模的在线A/B测试展示了该系统的实际有效性。此外，我们描述了实时部署策略，并发布了包含1180万条匿名客户评论（覆盖92000个产品，包括提取的方面和生成的摘要）的数据集，以支持未来方面导向的评论总结研究。', 'title_zh': '端到端面向方面引导的评论摘要生成'}
{'arxiv_id': 'arXiv:2509.26030', 'title': 'Muon Outperforms Adam in Tail-End Associative Memory Learning', 'authors': 'Shuche Wang, Fengzhuo Zhang, Jiaxiang Li, Cunxiao Du, Chao Du, Tianyu Pang, Zhuoran Yang, Mingyi Hong, Vincent Y. F. Tan', 'link': 'https://arxiv.org/abs/2509.26030', 'abstract': "The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muon's superiority. Motivated by this associative memory view, we then explain Muon's superiority on real-world corpora, which are intrinsically heavy-tailed: a few classes (tail classes) appear far less frequently than others. The superiority is explained through two key properties: (i) its update rule consistently yields a more isotropic singular spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing a one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muon's core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam.", 'abstract_zh': 'Muon优化器在训练大规模语言模型（LLMs）中始终比Adam更快，但其成功机制尚不清晰。通过联想记忆的视角揭开这一机制。通过消融Muon优化的Transformer组件，我们揭示出大规模语言模型的联想记忆参数，即Value和Output（VO）注意力权重以及前馈网络（FFNs），是Muon优越性的主要贡献者。受到这一联想记忆视角的启发，我们进一步解释了在固有表现为重尾分布的现实语料库上，Muon的优越性。重尾数据上的优越性通过两个关键性质来解释：（i）其更新规则始终导致比Adam更加各向同性的奇异谱；因此，（ii）在重尾数据上，它比Adam更有效地优化尾类。除了实证证据，我们通过分析在类别不平衡数据下的单层联想记忆模型，从理论上证实了这些发现。我们证明了无论在特征嵌入如何，Muon都能实现类间学习的一致平衡，而Adam的学习误差可能会因为嵌入特性而出现大的差异。总之，我们的实证观察和理论分析揭示了Muon的核心优势：其更新规则与线性联想记忆的外积结构相一致，在重尾分布中能实现比Adam更加平衡和有效的尾类学习。', 'title_zh': 'Muon在尾端关联记忆学习中优于Adam'}
{'arxiv_id': 'arXiv:2509.25992', 'title': 'MHINDR - a DSM5 based mental health diagnosis and recommendation framework using LLM', 'authors': 'Vaishali Agarwal, Sachin Thukral, Arnab Chatterjee', 'link': 'https://arxiv.org/abs/2509.25992', 'abstract': 'Mental health forums offer valuable insights into psychological issues, stressors, and potential solutions. We propose MHINDR, a large language model (LLM) based framework integrated with DSM-5 criteria to analyze user-generated text, dignose mental health conditions, and generate personalized interventions and insights for mental health practitioners. Our approach emphasizes on the extraction of temporal information for accurate diagnosis and symptom progression tracking, together with psychological features to create comprehensive mental health summaries of users. The framework delivers scalable, customizable, and data-driven therapeutic recommendations, adaptable to diverse clinical contexts, patient needs, and workplace well-being programs.', 'abstract_zh': '心理健康论坛提供了关于心理问题、压力源及潜在解决方案的重要见解。我们提出MHINDR框架，该框架基于大规模语言模型（LLM）并结合DSM-5标准，用于分析用户生成的文本、诊断心理健康状况，并为心理健康从业者生成个性化干预措施和见解。该方法强调时间信息的提取以实现准确的诊断和症状进展追踪，并结合心理特征创建用户的全面心理健康总结。该框架提供可扩展、可定制且基于数据的治疗建议，适用于多种临床环境、患者需求和工作场所福祉项目。', 'title_zh': 'MHINDR - 基于DSM5的精神健康诊断与推荐框架利用大语言模型'}
{'arxiv_id': 'arXiv:2509.25987', 'title': 'R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning', 'authors': 'Yilun Liu, Ziang Chen, Song Xu, Minggui He, Shimin Tao, Weibin Meng, Yuming Xie, Tao Han, Chunguang Zhao, Jingzhou Du, Daimeng Wei, Shenglin Zhang, Yongqian Sun', 'link': 'https://arxiv.org/abs/2509.25987', 'abstract': "The growing complexity of log data in modern software systems has prompted the use of Large Language Models (LLMs) for automated log analysis. Current approaches typically rely on direct supervised fine-tuning (SFT) on log-label pairs. However, this exacerbates the domain discrepancy between general-purpose LLMs and specialized log data, causing overfitting. Furthermore, SFT's imbalanced loss computation often allows lengthy contexts to overwhelm critical, concise details in model answers, leading to hallucinations. To address these limitations, we propose R-Log, a novel reasoning-based paradigm that mirrors the structured, step-by-step analytical process of human engineers. This approach enhances generalizability by learning the underlying rules behind conclusions. We further employ Reinforcement Learning (RL) to optimize the model within a simulated O&M environment, thereby reducing hallucinations by directly rewarding correct outcomes. R-Log is first cold-started on a curated dataset of 2k+ reasoning trajectories, guided by 13 strategies from manual O&M practices, to establish an initial reasoning capability. This ability is then refined via RL using a joint reward function. Empirical evaluations on real-world logs show that R-Log outperforms existing methods across five log analysis tasks, particularly in unseen scenarios (by 228.05%). We also designed R-Log-fast with 5x speedup while keeping 93% of the efficacy.", 'abstract_zh': '现代软件系统中日志数据日益复杂的趋势促使使用大型语言模型（LLMs）进行自动日志分析。当前的方法通常依赖于日志-标签对的直接监督微调（SFT）。然而，这加剧了通用目的LLMs与专门的日志数据之间的领域差异，导致过拟合。此外，SFT中失衡的损失计算往往使长语境压倒模型答案中的关键、简洁细节，导致模型产生幻觉。为解决这些问题，我们提出了R-Log，一种基于推理的新范式，模仿了人类工程师结构化、分步的分析过程。该方法通过学习结论背后的规则增强泛化能力。进一步地，我们利用强化学习（RL）在模拟的运维环境中优化模型，从而通过直接奖励正确的结果减少幻觉。R-Log首先在由13种手动运维策略指导的2千多个推理轨迹的受限数据集上冷启动，以建立初始的推理能力。然后通过联合奖励函数使用RL对该能力进行细化。实证评估表明，在五个日志分析任务中，R-Log在未见过的场景中尤其优于现有方法，性能提高228.05%。我们还设计了R-Log-fast，其速度提高了5倍，同时保持了93%的有效性。', 'title_zh': 'R-Log: 基于推理强化学习激励LLM的日志分析能力'}
{'arxiv_id': 'arXiv:2509.25919', 'title': 'Accelerating LLM Inference with Precomputed Query Storage', 'authors': 'Jay H. Park, Youngju Cho, Choungsol Lee, Moonwook Oh, Euiseong Seo', 'link': 'https://arxiv.org/abs/2509.25919', 'abstract': 'Large language model (LLM) inference often suffers from high latency, particularly in resource-constrained environments such as on-device or edge deployments. To address this challenge, we present StorInfer, a novel storage-assisted LLM inference system that accelerates response time by precomputing and storing predictable query-response pairs offline. When a user query semantically matches a precomputed query, StorInfer bypasses expensive GPU inference and instantly returns the stored response, significantly reducing latency and compute costs. To maximize coverage and effectiveness, StorInfer employs an LLM-driven generator that adaptively produces diverse and deduplicated queries based on a given knowledge base. This is achieved via two techniques: adaptive query masking, which prevents regeneration of similar queries, and adaptive sampling, which dynamically tunes generation parameters to promote semantic diversity. The resulting query-response pairs are embedded and indexed using a disk-backed vector database to enable fast, similarity-based retrieval at runtime. Using this approach, we generated 150K unique precomputed pairs (taking up to 830 MB of storage space), achieving up to 17.3% latency reduction with no loss in response quality. Our evaluation across multiple QA datasets demonstrates the practicality and scalability of storage-assisted inference, especially in scenarios with predictable query distributions. StorInfer highlights a promising direction in leveraging storage as a primary enabler for efficient, low-latency LLM deployment.', 'abstract_zh': '一种基于存储辅助的大型语言模型推理系统：StorInfer及其应用', 'title_zh': '预计算查询存储加速大模型推理'}
{'arxiv_id': 'arXiv:2509.25903', 'title': 'PerQ: Efficient Evaluation of Multilingual Text Personalization Quality', 'authors': 'Dominik Macko, Andrew Pulver', 'link': 'https://arxiv.org/abs/2509.25903', 'abstract': 'Since no metrics are available to evaluate specific aspects of a text, such as its personalization quality, the researchers often rely solely on large language models to meta-evaluate such texts. Due to internal biases of individual language models, it is recommended to use multiple of them for combined evaluation, which directly increases costs of such meta-evaluation. In this paper, a computationally efficient method for evaluation of personalization quality of a given text (generated by a language model) is introduced, called PerQ. A case study of comparison of generation capabilities of large and small language models shows the usability of the proposed metric in research, effectively reducing the waste of resources.', 'abstract_zh': '自生成文本的个性化质量无法用现有指标进行评估，研究者通常仅依赖大型语言模型进行元评估。由于单一语言模型内部可能存在偏见，建议使用多个模型进行综合评估，这直接增加了元评估的成本。本文提出了一种计算效率高的方法——PerQ，用于评估由语言模型生成的文本的个性化质量，并通过一个大型和小型语言模型生成能力的案例研究展示了该提出的度量标准在研究中的实用性，有效减少了资源浪费。', 'title_zh': '多语言文本个性化质量高效评估：PerQ'}
{'arxiv_id': 'arXiv:2509.25897', 'title': "RoleConflictBench: A Benchmark of Role Conflict Scenarios for Evaluating LLMs' Contextual Sensitivity", 'authors': 'Jisu Shin, Hoyun Song, Juhyun Oh, Changgeon Ko, Eunsu Kim, Chani Jung, Alice Oh', 'link': 'https://arxiv.org/abs/2509.25897', 'abstract': "Humans often encounter role conflicts -- social dilemmas where the expectations of multiple roles clash and cannot be simultaneously fulfilled. As large language models (LLMs) become increasingly influential in human decision-making, understanding how they behave in complex social situations is essential. While previous research has evaluated LLMs' social abilities in contexts with predefined correct answers, role conflicts represent inherently ambiguous social dilemmas that require contextual sensitivity: the ability to recognize and appropriately weigh situational cues that can fundamentally alter decision priorities. To address this gap, we introduce RoleConflictBench, a novel benchmark designed to evaluate LLMs' contextual sensitivity in complex social dilemmas. Our benchmark employs a three-stage pipeline to generate over 13K realistic role conflict scenarios across 65 roles, systematically varying their associated expectations (i.e., their responsibilities and obligations) and situational urgency levels. By analyzing model choices across 10 different LLMs, we find that while LLMs show some capacity to respond to these contextual cues, this sensitivity is insufficient. Instead, their decisions are predominantly governed by a powerful, inherent bias related to social roles rather than situational information. Our analysis quantifies these biases, revealing a dominant preference for roles within the Family and Occupation domains, as well as a clear prioritization of male roles and Abrahamic religions across most evaluatee models.", 'abstract_zh': '人类经常遇到角色冲突——多种角色期望相互矛盾且无法同时满足的社会困境。随着大规模语言模型（LLMs）在人类决策中日益发挥影响力，理解它们在复杂社会情境中的行为变得尤为关键。尽管先前研究已在具备预定义正确答案的背景下评估了LLMs的社会能力，但角色冲突代表了一种固有的社会困境，需要情境敏感性：即识别并适当权衡可能根本改变决策优先次序的情境线索的能力。为填补这一空白，我们提出了RoleConflictBench，一个旨在评估LLMs在复杂社会困境中情境敏感性的新型基准。该基准采用了三阶段流程生成超过13000个现实的角色冲突情境，覆盖65种角色，并系统地变化其关联期望（即职责和义务）以及情境紧迫性水平。通过对10种不同LLM的模型选择进行分析，我们发现虽然LLMs在一定程度上能够响应这些情境线索，但这种敏感性是不足的。相反，它们的决策主要由与社会角色相关的强大且内在的偏见所支配，而不是情境信息。我们的分析量化了这些偏见，揭示了在大多数评估模型中对家庭和职业领域角色的主导偏好，以及对男性角色和犹太教、基督教、伊斯兰教明确的优先级。', 'title_zh': '角色冲突基准：评估LLM语境敏感性的角色冲突场景基准'}
{'arxiv_id': 'arXiv:2509.25849', 'title': 'Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation', 'authors': 'Ziniu Li, Congliang Chen, Tianyun Yang, Tian Ding, Ruoyu Sun, Ge Zhang, Wenhao Huang, Zhi-Quan Luo', 'link': 'https://arxiv.org/abs/2509.25849', 'abstract': 'Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task\'s exploration as an "item" with a distinct "value" and "cost", we establish a connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the model\'s current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as a computational "free lunch", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under a uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources.', 'abstract_zh': '大型语言模型（LLMs）可以通过强化学习自我改进，其中它们生成轨迹以探索和发现更好的解决方案。然而，这个探索过程计算成本高昂，常迫使当前方法对每个任务分配有限的探索预算。这种均匀分配创造了问题边缘案例：简单任务一致成功而困难任务一致失败，两者在训练更新中均产生零梯度。我们从探索预算分配的角度解决这个问题。将每个任务的探索视作具有不同“价值”和“成本”的“物品”，我们建立了与经典背包问题的联系。这种表述允许我们推导出一种自适应分配资源的规则，基于模型当前的学习状态。将该方法应用于GRPO时，在训练过程中有效非零策略梯度的比例可提高20-40%。作为计算上的“免费午餐”，我们的方法可以重新分配探索预算，从学习饱和的任务转移到学习最具有影响力的任务。这使得对特别具有挑战性的问题分配更大的预算（例如，93次展开），这在均匀分配下是计算上不可行的。这些改进在数学推理基准测试中转化为有意义的提高，平均改进幅度为2-4分，特定任务上最大改进幅度为9分。有趣的是，获得与传统均匀分配相当的性能需要大约两倍的计算资源。', 'title_zh': '背包RL：通过优化预算分配来解锁大规模语言模型的探索'}
{'arxiv_id': 'arXiv:2509.25848', 'title': 'More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models', 'authors': 'Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Fabian Waschkowski, Lukas Wesemann, Peter Tu, Jing Zhang', 'link': 'https://arxiv.org/abs/2509.25848', 'abstract': "Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model's reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: this https URL", 'abstract_zh': '多模态推理展现出双面性：虽然显著增强逻辑推理并促进解决复杂问题，但可能会逐渐削弱知觉基础，导致对基本视觉问题的识别失败。通过进一步分析，这一现象归因于视觉遗忘，即长时间推理导致模型越来越多地忽略视觉输入。为此，我们提出了视觉锚定策略优化（VAPO），一种简单而有效的方法，明确引导推理过程向视觉基础的轨迹靠拢。我们的结果模型VAPO-Thinker-7B显著增强了模型对视觉信息的依赖，并在多种现有基准测试中取得了新的最佳成果。项目页面：这个 [链接]。', 'title_zh': '更多的思考，更少的准确性？关于视觉语言模型中推理的双重性质'}
{'arxiv_id': 'arXiv:2509.25837', 'title': 'Distillation of Large Language Models via Concrete Score Matching', 'authors': 'Yeongmin Kim, Donghyeok Shin, Mina Kang, Byeonghu Na, Il-Chul Moon', 'link': 'https://arxiv.org/abs/2509.25837', 'abstract': 'Large language models (LLMs) deliver remarkable performance but are costly to deploy, motivating knowledge distillation (KD) for efficient inference. Existing KD objectives typically match student and teacher probabilities via softmax, which blurs valuable logit information. While direct logit distillation (DLD) mitigates softmax smoothing, it fails to account for logit shift invariance, thereby restricting the solution space. We propose Concrete Score Distillation (CSD), a discrete score-matching objective that overcomes both softmax-induced smoothing and restrictions on the optimal solution set. We resolve the training instability and quadratic complexity of discrete score-matching in autoregressive LLMs, and the resulting CSD objective aligns relative logit differences across all vocabulary pairs between student and teacher with flexible weighting. We provide both mode-seeking and mode-covering instances within our framework and evaluate CSD on task-agnostic instruction-following and task-specific distillation using GPT-2-1.5B, OpenLLaMA-7B, and GEMMA-7B-IT. Experiments show that CSD consistently surpasses recent KD objectives, achieves favorable fidelity-diversity trade-offs, and yields complementary gains when combined with on-policy techniques, demonstrating its scalability and effectiveness for LLM distillation.', 'abstract_zh': '大型语言模型（LLMs）表现卓越但部署成本高昂，推动了高效推理的知识蒸馏（KD）技术。现有的KD目标通常通过softmax匹配学生模型和教师模型的概率，这会模糊掉有价值的操作符信息。直接操作符蒸馏（DLD）虽然缓解了softmax的平滑效应，但没有考虑到操作符偏移不变性，从而限制了解决方案空间。我们提出了一种离散评分匹配目标—混凝土评分蒸馏（CSD），它克服了由softmax引起的平滑效应和优化解空间的限制。我们解决了自回归LLMs中离散评分匹配的训练不稳定性及二次复杂性问题，而得到的CSD目标能够灵活地在学生模型和教师模型之间对所有词汇对的操作符相对差异进行对齐。我们提供了框架内的模式寻找和模式覆盖实例，并在任务无关的指令跟随和任务特定蒸馏任务中使用GPT-2-1.5B、OpenLLaMA-7B和GEMMA-7B-IT进行评估。实验表明，CSD一致优于最近的KD目标，实现了有利的保真度-多样性权衡，并且与策略更新技术结合时能产生互补收益，证明了其在LLM蒸馏中的可扩展性和有效性。', 'title_zh': '大型语言模型的混凝土评分匹配蒸馏'}
{'arxiv_id': 'arXiv:2509.25827', 'title': 'Overthinking Reduction with Decoupled Rewards and Curriculum Data Scheduling', 'authors': 'Shuyang Jiang, Yusheng Liao, Ya Zhang, Yanfeng Wang, Yu Wang', 'link': 'https://arxiv.org/abs/2509.25827', 'abstract': "While large reasoning models trained with critic-free reinforcement learning and verifiable rewards (RLVR) represent the state-of-the-art, their practical utility is hampered by ``overthinking'', a critical issue where models generate excessively long reasoning paths without any performance benefit. Existing solutions that penalize length often fail, inducing performance degradation due to a fundamental misalignment between trajectory-level rewards and token-level optimization. In this work, we introduce a novel framework, DECS, built on our theoretical discovery of two previously unaddressed flaws in current length rewards: (1) the erroneous penalization of essential exploratory tokens and (2) the inadvertent rewarding of partial redundancy. Our framework's innovations include (i) a first-of-its-kind decoupled token-level reward mechanism that surgically distinguishes and penalizes redundant tokens, and (ii) a novel curriculum batch scheduling strategy to master the efficiency-efficacy equilibrium. Experimental results show DECS can achieve a dramatic reduction in reasoning tokens by over 50\\% across seven benchmarks while simultaneously maintaining or even improving performance. It demonstrates conclusively that substantial gains in reasoning efficiency can be achieved without compromising a model's underlying reasoning power.", 'abstract_zh': '基于可验证奖励的无批评强化学习大型推理模型中的新颖框架DECS：解决冗余惩罚与部分冗余奖励的问题', 'title_zh': '分隔奖励与 curriculum 数据调度以减少过度思考'}
{'arxiv_id': 'arXiv:2509.25818', 'title': 'VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions', 'authors': 'Kazuki Matsuda, Yuiga Wada, Shinnosuke Hirano, Seitaro Otsuki, Komei Sugiura', 'link': 'https://arxiv.org/abs/2509.25818', 'abstract': 'In this study, we focus on the automatic evaluation of long and detailed image captions generated by multimodal Large Language Models (MLLMs). Most existing automatic evaluation metrics for image captioning are primarily designed for short captions and are not suitable for evaluating long captions. Moreover, recent LLM-as-a-Judge approaches suffer from slow inference due to their reliance on autoregressive inference and early fusion of visual information. To address these limitations, we propose VELA, an automatic evaluation metric for long captions developed within a novel LLM-Hybrid-as-a-Judge framework. Furthermore, we propose LongCap-Arena, a benchmark specifically designed for evaluating metrics for long captions. This benchmark comprises 7,805 images, the corresponding human-provided long reference captions and long candidate captions, and 32,246 human judgments from three distinct perspectives: Descriptiveness, Relevance, and Fluency. We demonstrated that VELA outperformed existing metrics and achieved superhuman performance on LongCap-Arena.', 'abstract_zh': '本研究聚焦于多模态大型语言模型生成的长详尽图像描述的自动评估。现有的大多数图像描述自动评估指标主要针对短描述设计，不适合评估长描述。此外，最近的LLM-as-a-Judge方法因依赖自回归推理和视觉信息的早期融合而表现出较慢的推理速度。为解决这些局限，我们提出VELA，这是一种基于新颖LLM-混合法官框架的长描述自动评估指标。此外，我们提出了LongCap-Arena，这是一个专门用于评估长描述指标的基准，该基准包含7,805张图像，对应的由人类提供的长参考描述和长候选描述，以及从描述性、相关性和流畅性三个不同视角获取的32,246个人类评估。我们证明VELA在LongCap-Arena上优于现有指标，并实现了超人的性能。', 'title_zh': 'VELA：一种LLM-混合辅助评断方法用于评估长图像_caption_'}
{'arxiv_id': 'arXiv:2509.25803', 'title': 'Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding', 'authors': 'Wanying Ding, Savinay Narendra, Xiran Shi, Adwait Ratnaparkhi, Chengrui Yang, Nikoo Sabzevar, Ziyan Yin', 'link': 'https://arxiv.org/abs/2509.25803', 'abstract': "Analyzing financial transactions is crucial for ensuring regulatory compliance, detecting fraud, and supporting decisions. The complexity of financial transaction data necessitates advanced techniques to extract meaningful insights and ensure accurate analysis. Since Transformer-based models have shown outstanding performance across multiple domains, this paper seeks to explore their potential in understanding financial transactions. This paper conducts extensive experiments to evaluate three types of Transformer models: Encoder-Only, Decoder-Only, and Encoder-Decoder models. For each type, we explore three options: pretrained LLMs, fine-tuned LLMs, and small proprietary models developed from scratch. Our analysis reveals that while LLMs, such as LLaMA3-8b, Flan-T5, and SBERT, demonstrate impressive capabilities in various natural language processing tasks, they do not significantly outperform small proprietary models in the specific context of financial transaction understanding. This phenomenon is particularly evident in terms of speed and cost efficiency. Proprietary models, tailored to the unique requirements of transaction data, exhibit faster processing times and lower operational costs, making them more suitable for real-time applications in the financial sector. Our findings highlight the importance of model selection based on domain-specific needs and underscore the potential advantages of customized proprietary models over general-purpose LLMs in specialized applications. Ultimately, we chose to implement a proprietary decoder-only model to handle the complex transactions that we previously couldn't manage. This model can help us to improve 14% transaction coverage, and save more than \\$13 million annual cost.", 'abstract_zh': '分析财务交易对于确保合规性、检测欺诈和支持决策至关重要。由于财务交易数据的复杂性，需要采用先进的技术来提取有意义的见解并确保准确的分析。鉴于基于Transformer的模型在多个领域表现出色，本文旨在探索其在理解财务交易中的潜在应用。本文进行了广泛的实验，评估了三种类型的Transformer模型：Encoder-Only、Decoder-Only和Encoder-Decoder模型。对于每种类型，我们探讨了三种选项：预训练的大型语言模型、微调的大型语言模型以及从零开始开发的小型专属模型。分析显示，尽管预训练的大型语言模型，如LLaMA3-8b、Flan-T5和SBERT，在各种自然语言处理任务中表现出色，但在财务交易理解的具体背景下，它们并未显著优于小型专属模型。这一现象在速度和成本效率方面尤为明显。针对交易数据的独有要求定制的专属模型，表现出更快的处理时间和更低的操作成本，使其更适合金融领域的实时应用。我们的研究结果强调了根据领域特定需求选择模型的重要性，并突显了在专业化应用中，定制的专属模型相较于通用预训练模型的优势。最终，我们选择实现一个专属的Decoder-Only模型来处理我们之前无法管理的复杂交易。该模型帮助我们提高了14%的交易覆盖率，并节省了超过1300万美元的年度成本。', 'title_zh': '更少更好：专用小型模型在金融交易理解中超越大型语言模型'}
{'arxiv_id': 'arXiv:2509.25771', 'title': 'Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs', 'authors': 'Jia Jun Cheng Xian, Muchen Li, Haotian Yang, Xin Tao, Pengfei Wan, Leonid Sigal, Renjie Liao', 'link': 'https://arxiv.org/abs/2509.25771', 'abstract': 'Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables "free-lunch" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at this https URL.', 'abstract_zh': 'Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables "free-lunch" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at this <https://> URL.', 'title_zh': '无需偏好图像配对的文本到图像扩散模型自洽对齐'}
{'arxiv_id': 'arXiv:2509.25760', 'title': 'TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning', 'authors': 'Zhepei Wei, Xiao Yang, Kai Sun, Jiaqi Wang, Rulin Shao, Sean Chen, Mohammad Kachuee, Teja Gollapudi, Tony Liao, Nicolas Scheffer, Rakesh Wanga, Anuj Kumar, Yu Meng, Wen-tau Yih, Xin Luna Dong', 'link': 'https://arxiv.org/abs/2509.25760', 'abstract': 'While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy -- models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.', 'abstract_zh': '一种直接优化大型语言模型真相性的强化学习框架：TruthRL', 'title_zh': 'TruthRL：通过强化学习激励 truthful 的LLM'}
{'arxiv_id': 'arXiv:2509.25736', 'title': 'Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications', 'authors': 'Chenhua Shi, Gregor Macdonald, Bhavika Jalli, Wanlu Lei, John Zou, Mridul Jain, Joji Philip', 'link': 'https://arxiv.org/abs/2509.25736', 'abstract': 'The success of large language models (LLMs) depends heavily on large-scale, high-quality instruction-following and reinforcement datasets. However, generating such data through human annotation is prohibitively time-consuming particularly for domain-specific tasks like telecom network troubleshooting, where accurate responses require deep technical expertise and contextual understanding. In this paper, we present a fully automated, retrieval-augmented pipeline for generating synthetic question-answer (QA) pairs grounded in structured domain knowledge. Our multi-stage framework integrates a retriever, base generator, and refinement model to synthesize and enhance QA pairs using documents retrieved from a domain-specific knowledge graph. To ensure data quality, we employ customized RAGAS-based scoring to filter low-quality samples, producing a high-quality dataset suitable for reinforcement fine-tuning (RFT). We demonstrate our approach in a real-world telecom scenario focused on radio access network (RAN) troubleshooting. The resulting pipeline generates complex, context-rich troubleshooting solution plans without human intervention. This work offers a scalable solution for building instruction and reinforcement datasets in specialized domains, significantly reducing dependence on manual labeling while maintaining high technical fidelity.', 'abstract_zh': '大型语言模型的成功高度依赖于大规模、高质量的指令跟随和强化数据集。然而，通过人工注释生成此类数据在特定领域任务（如电信网络故障排除）中尤其耗时，准确的响应需要深厚的技术专长和背景理解。本文提出了一种完全自动化的检索增强管道，用于生成基于结构化领域知识的合成问答（QA）对。我们的多阶段框架结合了检索器、基础生成器和改进模型，使用从领域特定知识图谱检索到的文档合成和增强QA对。为确保数据质量，我们采用定制化的RAGAS评分来过滤低质量样本，生成适合强化微调（RFT）的高质量数据集。我们在一个实际的电信场景中展示了该方法，该场景关注于无线接入网络（RAN）故障排除。所提出的管道在无需人工干预的情况下生成了复杂且内容丰富的故障排除解决方案计划。本文提供了一种在专门领域构建指令和强化数据集的可扩展解决方案，显著减少了对人工标注的依赖性，同时保持了高技术水平。', 'title_zh': '少思考，更准确打标：面向电信领域的多阶段领域引导合成数据生成以微调大规模语言模型'}
{'arxiv_id': 'arXiv:2509.25694', 'title': 'HNote: Extending YNote with Hexadecimal Encoding for Fine-Tuning LLMs in Music Modeling', 'authors': 'Hung-Ying Chu, Shao-Yu Wei, Guan-Wei Chen, Tzu-Wei Hung, ChengYang Tsai, Yu-Cheng Lin', 'link': 'https://arxiv.org/abs/2509.25694', 'abstract': 'Recent advances in large language models (LLMs) have created new opportunities for symbolic music generation. However, existing formats such as MIDI, ABC, and MusicXML are either overly complex or structurally inconsistent, limiting their suitability for token-based learning architectures. To address these challenges, we propose HNote, a novel hexadecimal-based notation system extended from YNote, which encodes both pitch and duration within a fixed 32-unit measure framework. This design ensures alignment, reduces ambiguity, and is directly compatible with LLM architectures. We converted 12,300 Jiangnan-style songs generated from traditional folk pieces from YNote into HNote, and fine-tuned LLaMA-3.1(8B) using parameter-efficient LoRA. Experimental results show that HNote achieves a syntactic correctness rate of 82.5%, and BLEU and ROUGE evaluations demonstrate strong symbolic and structural similarity, producing stylistically coherent compositions. This study establishes HNote as an effective framework for integrating LLMs with cultural music modeling.', 'abstract_zh': 'Recent Advances in Large Language Models for Symbolic Music Generation: Introducing HNote, a Hexadecimal-Based Notation System', 'title_zh': 'HNote: 结合十六进制编码扩展YNote，以在音乐建模中微调LLMs'}
{'arxiv_id': 'arXiv:2509.25684', 'title': 'LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts', 'authors': 'Yuan Zhuang, Yi Shen, Yuexin Bian, Qing Su, Shihao Ji, Yuanyuan Shi, Fei Miao', 'link': 'https://arxiv.org/abs/2509.25684', 'abstract': 'Recent studies have shown that combining parameter-efficient fine-tuning (PEFT) with mixture-of-experts (MoE) is an effective strategy for adapting large language models (LLMs) to the downstream tasks. However, most existing approaches rely on conventional TopK routing, which requires careful hyperparameter tuning and assigns a fixed number of experts to each token. In this work, we propose LD-MoLE, a Learnable Dynamic routing mechanism for Mixture of LoRA Experts that enables adaptive, token-dependent, and layer-wise expert allocation. Our method replaces the non-differentiable TopK selection with a differentiable routing function and a closed-form solution. Moreover, our design allows the model to adaptively determine the number of experts to activate for each token at different layers. In addition, we introduce an analytical sparsity control objective to regularize the number of activated experts. Extensive experiments on the Qwen3-1.7B and Llama-3.2-3B models show that LD-MoLE achieves the highest average scores compared to state-of-the-art baselines, across a diverse set of benchmarks. Our method not only achieves superior performance, but also demonstrates the ability to learn token-dependent and layer-wise expert allocation.', 'abstract_zh': 'Recent studies have shown that combining parameter-efficient fine-tuning (PEFT) with mixture-of-experts (MoE) is an effective strategy for adapting large language models (LLMs) to downstream tasks. However, most existing approaches rely on conventional TopK routing, which requires careful hyperparameter tuning and assigns a fixed number of experts to each token. In this work, we propose LD-MoLE, a Learnable Dynamic routing mechanism for Mixture of LoRA Experts that enables adaptive, token-dependent, and layer-wise expert allocation. Our method replaces the non-differentiable TopK selection with a differentiable routing function and a closed-form solution. Moreover, our design allows the model to adaptively determine the number of experts to activate for each token at different layers. In addition, we introduce an analytical sparsity control objective to regularize the number of activated experts. Extensive experiments on the Qwen3-1.7B and Llama-3.2-3B models show that LD-MoLE achieves the highest average scores compared to state-of-the-art baselines across a diverse set of benchmarks. Our method not only achieves superior performance but also demonstrates the ability to learn token-dependent and layer-wise expert allocation.', 'title_zh': 'LD-MoLE: 学习可动态路由的LoRA专家混合适配器'}
{'arxiv_id': 'arXiv:2509.25624', 'title': 'STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents', 'authors': 'Jing-Jing Li, Jianfeng He, Chao Shang, Devang Kulshreshtha, Xun Xian, Yi Zhang, Hang Su, Sandesh Swamy, Yanjun Qi', 'link': 'https://arxiv.org/abs/2509.25624', 'abstract': "As LLMs advance into autonomous agents with tool-use capabilities, they introduce security challenges that extend beyond traditional content-based LLM safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC), a novel multi-turn attack framework that exploits agent tool use. STAC chains together tool calls that each appear harmless in isolation but, when combined, collectively enable harmful operations that only become apparent at the final execution step. We apply our framework to automatically generate and systematically evaluate 483 STAC cases, featuring 1,352 sets of user-agent-environment interactions and spanning diverse domains, tasks, agent types, and 10 failure modes. Our evaluations show that state-of-the-art LLM agents, including GPT-4.1, are highly vulnerable to STAC, with attack success rates (ASR) exceeding 90% in most cases. The core design of STAC's automated framework is a closed-loop pipeline that synthesizes executable multi-step tool chains, validates them through in-environment execution, and reverse-engineers stealthy multi-turn prompts that reliably induce agents to execute the verified malicious sequence. We further perform defense analysis against STAC and find that existing prompt-based defenses provide limited protection. To address this gap, we propose a new reasoning-driven defense prompt that achieves far stronger protection, cutting ASR by up to 28.8%. These results highlight a crucial gap: defending tool-enabled agents requires reasoning over entire action sequences and their cumulative effects, rather than evaluating isolated prompts or responses.", 'abstract_zh': '随着大型语言模型进化成具备工具使用能力的自主代理，它们引入了超越传统基于内容的安全关切的安全挑战。本文介绍了一种新颖的多轮攻击框架——顺序工具攻击链（STAC），该框架利用代理的工具使用能力。STAC chaining 了在单独使用时看似无害的工具调用，但当这些调用结合起来时，在最终执行步骤中会共同启用有害操作。我们应用该框架自动生成并系统评估了483个STAC案例，涉及1,352组用户-代理-环境交互，覆盖了多个领域、任务、代理类型和10种失败模式。评估结果表明，最先进的语言模型代理，包括GPT-4.1，对STAC高度易受攻击，大多数情况下攻击成功率（ASR）超过90%。STAC自动框架的核心设计是一个闭环管道，该管道综合生成可执行的多步骤工具链，在环境中执行验证，并逆向工程生成可靠诱导代理执行验证过的恶意序列的隐蔽多轮提示。此外，我们还进行了针对STAC的防御分析，并发现现有的基于提示的防御措施提供的保护有限。为填补这一缺口，我们提出了一种新的基于推理的防御提示，它提供了显著更强的保护，将攻击成功率降低高达28.8%。这些结果强调了一个关键缺口：防御工具启用的代理需要对整个行动序列及其累积效应进行推理，而不仅仅评估孤立的提示或响应。', 'title_zh': 'STAC：当无辜工具串联形成突破LLM代理的安全链条'}
{'arxiv_id': 'arXiv:2509.25543', 'title': 'Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model', 'authors': 'Fahim Faisal, Kaiqiang Song, Song Wang, Simin Ma, Shujian Liu, Haoyun Deng, Sathish Reddy Indurthi', 'link': 'https://arxiv.org/abs/2509.25543', 'abstract': 'While reinforcement learning has advanced the reasoning abilities of Large Language Models (LLMs), these gains are largely confined to English, creating a significant performance disparity across languages. To address this, we introduce Pivot-Based Reinforcement Learning with Semantically Verifiable Rewards (PB-RLSVR), a novel framework that enhances multilingual reasoning by circumventing the need for human-annotated data in target languages. Our approach employs a high-performing English LLM as a "pivot" model to generate reference responses for reasoning tasks. A multilingual model is then rewarded based on the semantic equivalence of its responses to the English reference, effectively transferring the pivot model\'s reasoning capabilities across languages. We investigate several cross-lingual semantic reward functions, including those based on embeddings and machine translation. Extensive experiments on a suite of multilingual reasoning benchmarks show that our method significantly narrows the performance gap between English and other languages, substantially outperforming traditional PPO baselines. Specifically, our PB-RLSVR framework improves the average multilingual performance of Llama-3.1-8B-Instruct and Qwen3-32B by 16.41% and 10.17%, respectively, demonstrating a powerful and data-efficient approach to building truly multilingual reasoning agents.', 'abstract_zh': '基于语义验证奖励的多语言强化学习框架（Pivot-Based Reinforcement Learning with Semantically Verifiable Rewards）', 'title_zh': '基于高资源专家模型的多语言推理与可验证语义对齐'}
{'arxiv_id': 'arXiv:2509.25533', 'title': 'VISOR++: Universal Visual Inputs based Steering for Large Vision Language Models', 'authors': 'Ravikumar Balakrishnan, Mansi Phute', 'link': 'https://arxiv.org/abs/2509.25533', 'abstract': 'As Vision Language Models (VLMs) are deployed across safety-critical applications, understanding and controlling their behavioral patterns has become increasingly important. Existing behavioral control methods face significant limitations: system prompting approaches could easily be overridden by user instructions, while applying activation-based steering vectors requires invasive runtime access to model internals, precluding deployment with API-based services and closed-source models. Finding steering methods that transfer across multiple VLMs is still an open area of research. To this end, we introduce universal visual input based steering for output redirection (VISOR++), to achieve behavioral control through optimized visual inputs alone. We demonstrate that a single VISOR++ image can be generated for an ensemble of VLMs to emulate each of their steering vectors. By crafting universal visual inputs that induce target activation patterns, VISOR++ eliminates the need for runtime model access while remaining deployment-agnostic. This means that when an underlying model supports multimodal capability, model behaviors can be steered by inserting an image input replacing runtime steering vector based interventions. We first demonstrate the effectiveness of the VISOR++ images on open-access models such as LLaVA-1.5-7B and IDEFICS2-8B along three alignment directions: refusal, sycophancy and survival instinct. Both the model-specific steering images and the jointly optimized images achieve performance parity closely following that of steering vectors for both positive and negative steering tasks. We also show the promise of VISOR++ images in achieving directional behavioral shifts for unseen models including both open-access and closed-access ones. Furthermore, VISOR++ images are able to preserve 99.9% performance on 14,000 unrelated MMLU evaluation tasks.', 'abstract_zh': '基于通用视觉输入的输出重定向行为控制（VISOR++）', 'title_zh': 'VISOR++: 基于通用视觉输入的大型视觉语言模型引导'}
{'arxiv_id': 'arXiv:2509.25532', 'title': 'Calibrating Verbalized Confidence with Self-Generated Distractors', 'authors': 'Victor Wang, Elias Stengel-Eskin', 'link': 'https://arxiv.org/abs/2509.25532', 'abstract': "Calibrated confidence estimates are necessary for large language model (LLM) outputs to be trusted by human users. While LLMs can express their confidence in human-interpretable ways, verbalized LLM-generated confidence scores have empirically been found to be miscalibrated, reporting high confidence on instances with low accuracy and thereby harming trust and safety. We hypothesize that this overconfidence often stems from a given LLM's heightened suggestibility when faced with claims that it encodes little information about; we empirically validate this hypothesis, finding more suggestibility on lower-accuracy claims. Building on this finding, we introduce Distractor-Normalized Coherence (DINCO), which estimates and accounts for an LLM's suggestibility bias by having the model verbalize its confidence independently across several self-generated distractors (i.e. alternative claims), and normalizes by the total verbalized confidence. To further improve calibration, we leverage generator-validator disagreement, augmenting normalized validator confidence with a consistency-based estimate of generator confidence. Here, we frame the popular approach of self-consistency as leveraging coherence across sampled generations, and normalized verbalized confidence as leveraging coherence across validations on incompatible claims, allowing us to integrate these complementary dimensions of coherence into DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and therefore more usable -- confidence estimates, and that further sampling alone cannot close the gap between DINCO and baselines, with DINCO at 10 inference calls outperforming self-consistency at 100.", 'abstract_zh': '校准的置信度估计对于大型语言模型（LLM）的输出被人类用户信任是必要的。虽然LLM可以通过人类可理解的方式表达其置信度，但实验证明，LLM生成的置信分数往往是失校准的，报告低准确度实例的高置信度，从而损害信任和安全性。我们假设这种过度自信通常源于LLM在面对它编码信息很少的断言时的高暗示性；我们通过实验验证了这一假设，发现对于低准确度断言，LLM的暗示性更强。基于这一发现，我们引入了干扰归一化连贯性（DINCO），通过让模型独立地在几个自我生成的干扰项（即替代断言）上表达其置信度，并通过总表达置信度进行归一化，来估算和纠正LLM的暗示性偏差。为了进一步提高校准度，我们利用生成器-验证器分歧，将归一化验证器置信度与基于一致性的生成器置信度估计结合起来。在这里，我们将广为接受的自一致性方法视为通过采样生成之间的一致性来利用连贯性，而归一化表达置信度则被视为利用不可兼容断言验证之间的一致性来利用连贯性，从而将这些连贯性的互补维度整合到DINCO中。此外，我们的分析表明，DINCO提供了较少过饱和——因此更具可操作性的置信度估计，单独增加采样无法弥补DINCO与基线之间的差距，DINCO在10次推理调用时的表现优于自一致性在100次采样时的表现。', 'title_zh': '基于自动生成干扰项校准口头表达的置信度'}
{'arxiv_id': 'arXiv:2509.25531', 'title': 'MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources', 'authors': 'Huu Nguyen, Victor May, Harsh Raj, Marianna Nezhurina, Yishan Wang, Yanqi Luo, Minh Chien Vu, Taishi Nakamura, Ken Tsui, Van Khue Nguyen, David Salinas, Aleksandra Krasnodębska, Christoph Schuhmann, Mats Leon Richter, Xuan-Son, Jenia Jitsev', 'link': 'https://arxiv.org/abs/2509.25531', 'abstract': 'We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: this https URL', 'abstract_zh': '我们 presents MixtureVitae，一个旨在最小化法律风险同时提供强大模型性能的开源预训练数据集。', 'title_zh': 'MixtureVitae: 开放的高质量指令和推理数据预训练数据集，源自宽松优先的文本来源'}
{'arxiv_id': 'arXiv:2509.25498', 'title': 'Not Wrong, But Untrue: LLM Overconfidence in Document-Based Queries', 'authors': 'Nick Hagar, Wilma Agustianto, Nicholas Diakopoulos', 'link': 'https://arxiv.org/abs/2509.25498', 'abstract': 'Large language models (LLMs) are increasingly used in newsroom workflows, but their tendency to hallucinate poses risks to core journalistic practices of sourcing, attribution, and accuracy. We evaluate three widely used tools - ChatGPT, Gemini, and NotebookLM - on a reporting-style task grounded in a 300-document corpus related to TikTok litigation and policy in the U.S. We vary prompt specificity and context size and annotate sentence-level outputs using a taxonomy to measure hallucination type and severity. Across our sample, 30% of model outputs contained at least one hallucination, with rates approximately three times higher for Gemini and ChatGPT (40%) than for NotebookLM (13%). Qualitatively, most errors did not involve invented entities or numbers; instead, we observed interpretive overconfidence - models added unsupported characterizations of sources and transformed attributed opinions into general statements. These patterns reveal a fundamental epistemological mismatch: While journalism requires explicit sourcing for every claim, LLMs generate authoritative-sounding text regardless of evidentiary support. We propose journalism-specific extensions to existing hallucination taxonomies and argue that effective newsroom tools need architectures that enforce accurate attribution rather than optimize for fluency.', 'abstract_zh': '大型语言模型（LLMs）在新闻工作流程中的应用日益增加，但其倾向性幻觉对其核心的新闻采编原则，如来源确认、归属和准确性构成风险。我们基于与TikTok诉讼和美国政策相关的300份文件构建报道风格的任务，评估了三种广泛使用的工具：ChatGPT、Gemini和NotebookLM。我们在提示的具体性和上下文规模方面进行了变化，并使用分类体系标注句子级输出，以衡量幻觉类型和严重程度。在我们的样本中，30%的模型输出包含至少一个幻觉，Gemini和ChatGPT的幻觉率约为NotebookLM的三倍（分别为40%和13%）。定性分析显示，大多数错误并非涉及虚构实体或数字；相反，我们观察到解释性过度自信——模型增加了对来源的支持不足的描述性，将归因意见转变为普遍性陈述。这些模式揭示了根本的知识论错配：新闻报道要求对每个声明进行明确的来源确认，而LLMs无论是否有证据支持都生成权威性语言的声音文本。我们提出了针对新闻的特定扩展以现有幻觉分类体系，并主张有效的新闻工具架构需要强制准确归属，而不是优化流畅性。', 'title_zh': '不是错误，而是不真实：大型语言模型在基于文档的查询中的过度自信'}
{'arxiv_id': 'arXiv:2509.25495', 'title': 'EMO-TTA: Improving Test-Time Adaptation of Audio-Language Models for Speech Emotion Recognition', 'authors': 'Jiacheng Shi, Hongfei Du, Y. Alicia Hong, Ye Gao', 'link': 'https://arxiv.org/abs/2509.25495', 'abstract': 'Speech emotion recognition (SER) with audio-language models (ALMs) remains vulnerable to distribution shifts at test time, leading to performance degradation in out-of-domain scenarios. Test-time adaptation (TTA) provides a promising solution but often relies on gradient-based updates or prompt tuning, limiting flexibility and practicality. We propose Emo-TTA, a lightweight, training-free adaptation framework that incrementally updates class-conditional statistics via an Expectation-Maximization procedure for explicit test-time distribution estimation, using ALM predictions as priors. Emo-TTA operates on individual test samples without modifying model weights. Experiments on six out-of-domain SER benchmarks show consistent accuracy improvements over prior TTA baselines, demonstrating the effectiveness of statistical adaptation in aligning model predictions with evolving test distributions.', 'abstract_zh': '基于音频-语言模型的语音情感识别（SER）在测试时仍易受分布偏移影响，导致领域外场景下的性能下降。测试时适应（TTA）提供了一种有前景的解决方案，但通常依赖于基于梯度的更新或提示调谐，限制了其灵活性和实用性。我们提出了一种轻量级、无训练的适应框架Emo-TTA，通过期望-最大值程序增量更新类别条件统计，使用ALM预测作为先验进行显式测试时分布估计。Emo-TTA在不修改模型权重的情况下操作单个测试样本。在六个领域外SER基准测试上的实验结果显示，Emo-TTA在先前提到的TTA基线之上实现了一致的准确率提升，证明了统计适应在使模型预测与不断变化的测试分布相一致方面的有效性。', 'title_zh': 'EMO-TTA：提高音频语言模型在语音情感识别中的测试时自适应能力'}
{'arxiv_id': 'arXiv:2509.25414', 'title': 'Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs', 'authors': 'Hao Ban, Kaiyi Ji', 'link': 'https://arxiv.org/abs/2509.25414', 'abstract': 'Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer. While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose \\textbf{ALoRA}, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and \\textbf{Fed-ALoRA}, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches. Codes are available at this https URL.', 'abstract_zh': '大型语言模型通常通过参数高效技术进行调整，如低秩适应（LoRA），表示为 $y = W_0x + BAx$，其中 $W_0$ 是预训练参数，$x$ 是调整层的输入。虽然多adapter扩展通常使用多个LoRA，但先前的研究表明，训练中的内部 $A$ 矩阵高度相似，因此适合作为共享对象。我们重新审视了这一现象，发现这种相似性主要是由于相同的初始化而非共享知识所导致，$B$ 在知识编码和转移中扮演着更为关键的角色。受此见解启发，我们提出了一种多LoRA设计——ALoRA，该设计在多任务微调中具有多个 $A$ 矩阵和一个共享的 $B$。此外，我们提出了Fed-ALoRA，在homogeneous和heterogeneous设置下的联邦微调中通过一种新型的矩阵分解策略在客户端之间共享 $B$，以适应客户端之间不同的秩。在常识推理、数学推理、多任务NLP数据集和联邦NLP数据集上进行的实验表明，我们的方法在各任务上实现了更加均衡的表现，相对于现有的多LoRA方法，平均准确率相当或更好。相关代码可从以下链接获取。', 'title_zh': '多LoRA参数共享重思以进行LLM微调'}
{'arxiv_id': 'arXiv:2509.25409', 'title': 'From Faithfulness to Correctness: Generative Reward Models that Think Critically', 'authors': 'Qiyao Ma, Yunsheng Shi, Hongtao Tian, Chao Wang, Weiming Chang, Ting Yao', 'link': 'https://arxiv.org/abs/2509.25409', 'abstract': 'Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.', 'abstract_zh': '通过可验证奖励的强化学习（RLVR）：大语言模型在数学和编程等具有易于验证结果的领域取得了显著进展。然而，在应用于更复杂的任务如开放领域问答时，RLVR由于正确性验证的困难而面临重大挑战。现实世界的知识具有细微和模糊的性质，使得在这些环境中可靠地评估正确性变得困难，这要求进一步的能力，超越仅仅是逻辑一致性，涵盖对外部和内部知识的理解和评估。近期的工作主要集中在提高忠实性方面，定义为语义上与支持文档的一致性，这可能导致模型过度依赖外部来源，并削弱其批判性评估的能力。为了解决这一问题，我们提出了一种思考监督奖励模型（TRM），它通过句子级别的思考监督赋予奖励模型批判性思维能力。给定一个查询、答案和支持文档，TRM 首先评估每个答案句子与支持文档的忠实性，然后执行推理步骤来评估句子级别的正确性。通过将奖励建模结构化为忠实性、推理和正确性评估的序列，TRM 鼓励模型批判性地评估和利用外部和内部知识。实验结果表明，TRM 显著提高了错误句子的识别准确性，将 TRM 集成到策略优化中导致了答案正确性和有用性的显著提高。', 'title_zh': '从忠实到正确：批判性生成奖励模型'}
{'arxiv_id': 'arXiv:2509.25397', 'title': 'A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects', 'authors': 'Johan Linåker, Cailean Osborne, Jennifer Ding, Ben Burtenshaw', 'link': 'https://arxiv.org/abs/2509.25397', 'abstract': 'The proliferation of open large language models (LLMs) is fostering a vibrant ecosystem of research and innovation in artificial intelligence (AI). However, the methods of collaboration used to develop open LLMs both before and after their public release have not yet been comprehensively studied, limiting our understanding of how open LLM projects are initiated, organized, and governed as well as what opportunities there are to foster this ecosystem even further. We address this gap through an exploratory analysis of open collaboration throughout the development and reuse lifecycle of open LLMs, drawing on semi-structured interviews with the developers of 14 open LLMs from grassroots projects, research institutes, startups, and Big Tech companies in North America, Europe, Africa, and Asia. We make three key contributions to research and practice. First, collaboration in open LLM projects extends far beyond the LLMs themselves, encompassing datasets, benchmarks, open source frameworks, leaderboards, knowledge sharing and discussion forums, and compute partnerships, among others. Second, open LLM developers have a variety of social, economic, and technological motivations, from democratizing AI access and promoting open science to building regional ecosystems and expanding language representation. Third, the sampled open LLM projects exhibit five distinct organizational models, ranging from single company projects to non-profit-sponsored grassroots projects, which vary in their centralization of control and community engagement strategies used throughout the open LLM lifecycle. We conclude with practical recommendations for stakeholders seeking to support the global community building a more open future for AI.', 'abstract_zh': '开放大型语言模型（LLMs）的涌现正推动人工智能（AI）研究与创新的蓬勃发展。然而，开放LLMs在其公开发布前后开发过程中所采用的协作方法尚未进行全面研究，限制了我们对开放LLMs项目是如何启动、组织和管理的理解，以及进一步培育这一生态系统的机会。我们通过对来自北美、欧洲、非洲和亚洲的草根项目、研究机构、初创公司和大科技公司的14个开放LLMs开发者的半结构化访谈，开展了探索性分析，填补了这一研究空白。我们的研究为研究和实践做出了三个重要贡献。首先，开放LLMs项目的协作远远超出了LLMs本身，还有数据集、基准测试、开源框架、排行榜、知识共享和讨论论坛以及计算合作伙伴等方面。其次，开放LLMs开发者的动机多种多样，包括使AI普及、推动开放科学、构建区域生态系统和扩展语言代表性等。第三，采样的开放LLMs项目展示了五种不同的组织模型，从单公司项目到由非营利组织赞助的草根项目，这些模型在控制的中心性和在整个开放LLMs生命周期中使用的社区参与策略方面存在差异。最后，我们提出了针对支持构建更加开放的AI未来的全球社区的实用建议。', 'title_zh': '开源协作在开源AI领域的地图绘制：14个开源大规模语言模型项目的合作实践、动机与治理mapping'}
{'arxiv_id': 'arXiv:2509.25380', 'title': 'Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs', 'authors': 'Shane Bergsma, Nolan Dey, Joel Hestness', 'link': 'https://arxiv.org/abs/2509.25380', 'abstract': "Data curriculums have become central to successful LLM training, yet principles governing optimal data placement remain unclear. We introduce the *training re-evaluation curve (TREC)*, a diagnostic that retrospectively evaluates training batches *using the final model weights*. The TREC characterizes how well a trained model retains training data as a function of *when* the data was encountered during training. Analyzing TRECs for models from 111M to 3.9B parameters, we show that placing high-quality data at low points on the TREC significantly improves performance. Importantly, while a TREC is initially observable only after training, we demonstrate it can be *predicted in advance* from AdamW's implicit EMA coefficients, enabling proactive curriculum design. By predicting TRECs for published training recipes, we explain prior ablations and reveal suboptimal data placements. We also align high-quality data with TREC minima in order to improve continual pre-training of a 3.9B-parameter LLM trained on 900B tokens.", 'abstract_zh': '数据课程已成为成功训练大型语言模型的核心，但最佳数据放置原则仍然不够清晰。我们引入了*训练重新评估曲线（TREC）*，这是一种回顾性诊断工具，使用最终模型权重来评估训练批次。TREC 表征了训练数据在模型训练过程中不同时点被遇到时，模型保留训练数据的能力。通过对参数从111M到3.9B的不同模型进行分析，我们发现将高质量数据放置在TREC的较低点可以显著提高性能。重要的是，虽然TREC仅在训练后才可观察到，但我们证明可以通过预测AdamW的隐式EWM系数来提前预测TREC，从而实现前瞻性的课程设计。通过预测公布的训练食谱的TREC，我们解释了先前的削减实验，并揭示了次优的数据放置。我们还将高质量数据与TREC的最小值对齐，以提高一个使用900B个标记训练的3.9B参数模型的持续预训练性能。', 'title_zh': '预测训练评估曲线以实现有效的LLM数据课程设计'}
{'arxiv_id': 'arXiv:2509.25369', 'title': 'Generative Value Conflicts Reveal LLM Priorities', 'authors': 'Andy Liu, Kshitish Ghate, Mona Diab, Daniel Fried, Atoosa Kasirzadeh, Max Kleiman-Weiner', 'link': 'https://arxiv.org/abs/2509.25369', 'abstract': 'Past work seeks to align large language model (LLM)-based assistants with a target set of values, but such assistants are frequently forced to make tradeoffs between values when deployed. In response to the scarcity of value conflict in existing alignment datasets, we introduce ConflictScope, an automatic pipeline to evaluate how LLMs prioritize different values. Given a user-defined value set, ConflictScope automatically generates scenarios in which a language model faces a conflict between two values sampled from the set. It then prompts target models with an LLM-written "user prompt" and evaluates their free-text responses to elicit a ranking over values in the value set. Comparing results between multiple-choice and open-ended evaluations, we find that models shift away from supporting protective values, such as harmlessness, and toward supporting personal values, such as user autonomy, in more open-ended value conflict settings. However, including detailed value orderings in models\' system prompts improves alignment with a target ranking by 14%, showing that system prompting can achieve moderate success at aligning LLM behavior under value conflict. Our work demonstrates the importance of evaluating value prioritization in models and provides a foundation for future work in this area.', 'abstract_zh': '基于大语言模型的价值优先级评估：ConflictScope自动管道研究', 'title_zh': '生成价值冲突揭示大模型优先级'}
{'arxiv_id': 'arXiv:2509.25359', 'title': 'From Internal Representations to Text Quality: A Geometric Approach to LLM Evaluation', 'authors': 'Viacheslav Yusupov, Danil Maksimov, Ameliia Alaeva, Anna Vasileva, Anna Antipina, Tatyana Zaitseva, Alina Ermilova, Evgeny Burnaev, Egor Shvetsov', 'link': 'https://arxiv.org/abs/2509.25359', 'abstract': 'This paper bridges internal and external analysis approaches to large language models (LLMs) by demonstrating that geometric properties of internal model representations serve as reliable proxies for evaluating generated text quality. We validate a set of metrics including Maximum Explainable Variance, Effective Rank, Intrinsic Dimensionality, MAUVE score, and Schatten Norms measured across different layers of LLMs, demonstrating that Intrinsic Dimensionality and Effective Rank can serve as universal assessments of text naturalness and quality. Our key finding reveals that different models consistently rank text from various sources in the same order based on these geometric properties, indicating that these metrics reflect inherent text characteristics rather than model-specific artifacts. This allows a reference-free text quality evaluation that does not require human-annotated datasets, offering practical advantages for automated evaluation pipelines.', 'abstract_zh': '这篇论文通过证明内部模型表示的几何性质可以作为评估生成文本质量的可靠代理，建立了内部和外部分析方法在大规模语言模型（LLMs）中的联系。我们验证了一系列指标，包括最大可解释方差、有效秩、固有维数、MAUVE分值和舒尔范数，这些指标在LLM的不同层上进行了测量，证明了固有维数和有效秩可以作为文本自然度和质量的通用评估标准。我们的关键发现表明，基于这些几何性质，不同模型能够以相同顺序对来自不同来源的文本进行排名，这表明这些指标反映了文本的内在特性而非模型特定的特征。这使得无需使用人工标注的数据集即可进行参考无关的文本质量评估，为自动化评估管道提供了实际优势。', 'title_zh': '从内部表示到文本质量：一种面向LLM评估的几何方法'}
{'arxiv_id': 'arXiv:2509.25300', 'title': 'Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning', 'authors': 'Zelin Tan, Hejia Geng, Mulei Zhang, Xiaohang Yu, Guancheng Wan, Yifan Zhou, Qiang He, Xiangyuan Xue, Heng Zhou, Yutao Fan, Zhongzhi Li, Zaibin Zhang, Guibin Zhang, Chen Zhang, Zhenfei Yin, Lei Bai', 'link': 'https://arxiv.org/abs/2509.25300', 'abstract': 'While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on 54 experiments across diverse model sizes and training settings, we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: (1). Under a fixed computational budget, larger models trained for fewer steps consistently outperform smaller models trained for more steps. (2). Given a fixed amount of training data, larger models achieve superior sample efficiency, yielding lower loss. (3). In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. (4). These scaling behaviors are robust across both base and instruction-tuned models, which share similar learning dynamics (e.g., larger models show faster convergence) even while differing in absolute accuracy. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.', 'abstract_zh': '大型语言模型在强化学习后训练中的标度行为：数学推理视角的系统实证研究', 'title_zh': 'LLM reinforcement learning 后训练的标度行为：数学推理中的实证研究'}
{'arxiv_id': 'arXiv:2509.25292', 'title': 'A Measurement Study of Model Context Protocol', 'authors': 'Hechuan Guo, Yongle Hao, Yue Zhang, Minghui Xu, Peizhuo Lyu, Jiezhi Chen, Xiuzhen Cheng', 'link': 'https://arxiv.org/abs/2509.25292', 'abstract': 'The Model Context Protocol (MCP) has been proposed as a unifying standard for connecting large language models (LLMs) with external tools and resources, promising the same role for AI integration that HTTP and USB played for the Web and peripherals. Yet, despite rapid adoption and hype, its trajectory remains uncertain. Are MCP marketplaces truly growing, or merely inflated by placeholders and abandoned prototypes? Are servers secure and privacy-preserving, or do they expose users to systemic risks? And do clients converge on standardized protocols, or remain fragmented across competing designs? In this paper, we present the first large-scale empirical study of the MCP ecosystem. We design and implement MCPCrawler, a systematic measurement framework that collects and normalizes data from six major markets. Over a 14-day campaign, MCPCrawler aggregated 17,630 raw entries, of which 8,401 valid projects (8,060 servers and 341 clients) were analyzed. Our results reveal that more than half of listed projects are invalid or low-value, that servers face structural risks including dependency monocultures and uneven maintenance, and that clients exhibit a transitional phase in protocol and connection patterns. Together, these findings provide the first evidence-based view of the MCP ecosystem, its risks, and its future trajectory.', 'abstract_zh': 'MCP生态系统的首次大规模实证研究：发现与未来轨迹', 'title_zh': '模型上下文协议的测量研究'}
{'arxiv_id': 'arXiv:2509.25286', 'title': 'Artificial Authority: From Machine Minds to Political Alignments. An Experimental Analysis of Democratic and Autocratic Biases in Large-Language Models', 'authors': 'Szymon Łukasik, Natalia Ożegalska-Łukasik', 'link': 'https://arxiv.org/abs/2509.25286', 'abstract': "Political beliefs vary significantly across different countries, reflecting distinct historical, cultural, and institutional contexts. These ideologies, ranging from liberal democracies to rigid autocracies, influence human societies, as well as the digital systems that are constructed within those societies. The advent of generative artificial intelligence, particularly Large Language Models (LLMs), introduces new agents in the political space-agents trained on massive corpora that replicate and proliferate socio-political assumptions. This paper analyses whether LLMs display propensities consistent with democratic or autocratic world-views. We validate this insight through experimental tests in which we experiment with the leading LLMs developed across disparate political contexts, using several existing psychometric and political orientation measures. The analysis is based on both numerical scoring and qualitative analysis of the models' responses. Findings indicate high model-to-model variability and a strong association with the political culture of the country in which the model was developed. These findings highlight the need for more detailed examination of the socio-political dimensions embedded within AI systems.", 'abstract_zh': '政治信念在不同国家之间存在显著差异，反映了不同的历史、文化和制度背景。这些从自由民主到严格独裁的各种意识形态影响人类社会，以及构建于这些社会之中的数字系统。生成式人工智能，尤其是大型语言模型（LLMs），引入了新的政治空间代理——这些模型基于大量语料库训练，复制和传播社会政治假设。本文分析LLMs是否表现出与民主或集权世界观一致的倾向。我们通过在不同政治背景下开发的领先LLMs进行的实验测试验证这一洞察，使用多种现有的心理测量和政治倾向度量。分析基于模型回应的数值评分和定性分析。研究结果表明模型间存在高变异性，并且强烈关联于模型开发国家的政治文化。这些发现强调了更详细研究嵌入AI系统中的社会政治维度的必要性。', 'title_zh': '人工权威：从机器思维到政治倾向。大规模语言模型中的民主偏见与独裁偏见实验分析'}
{'arxiv_id': 'arXiv:2509.25283', 'title': 'Effectiveness of Large Language Models in Simulating Regional Psychological Structures: An Empirical Examination of Personality and Subjective Well-being', 'authors': 'Ke Luoma, Li Zengyi, Liao Jiangqun, Tong Song, Peng Kaiping', 'link': 'https://arxiv.org/abs/2509.25283', 'abstract': 'This study examines whether LLMs can simulate culturally grounded psychological patterns based on demographic information. Using DeepSeek, we generated 2943 virtual participants matched to demographic distributions from the CFPS2018 and compared them with human responses on the Big Five personality traits and subjective well-being across seven Chinese this http URL was measured using a 15-item Chinese Big Five inventory, and happiness with a single-item rating. Results revealed broad similarity between real and simulated datasets, particularly in regional variation trends. However, systematic differences emerged:simulated participants scored lower in extraversion and openness, higher in agreeableness and neuroticism, and consistently reported lower happiness. Predictive structures also diverged: while human data identified conscientiousness, extraversion and openness as positive predictors of happiness, the AI emphasized openness and agreeableness, with extraversion predicting negatively. These discrepancies suggest that while LLMs can approximate population-level psychological distributions, they underrepresent culturally specific and affective dimensions. The findings highlight both the potential and limitations of LLM-based virtual participants for large-scale psychological research and underscore the need for culturally enriched training data and improved affective modeling.', 'abstract_zh': '本研究探讨大型语言模型是否可以根据人口统计信息模拟文化-grounded 心理模式。我们使用DeepSeek生成了2943名与CFPS2018人口分布匹配的虚拟参与者，并将它们与Big Five人格特质和主观幸福感的人类回应进行了比较。这些指标分别使用15项中国Big Five问卷和单项目测幸福感进行测量。研究结果表明，真实数据集和模拟数据集之间存在广泛的相似性，特别是在区域变化趋势方面。然而，系统性的差异也出现了：模拟参与者在 extraversion 和 openness 上得分较低，在 agreeableness 和 neuroticism 上得分较高，并且普遍报告的幸福感较低。预测结构也存在差异：人类数据将 conscientiousness、extraversion 和 openness 识别为幸福感的正向预测因子，而AI则强调 openess 和 agreeableness，extraversion 对幸福感有负向预测作用。这些差异表明，虽然大型语言模型可以近似人口水平的心理学分布，但它们在文化和情感维度上代表性不足。研究结果强调了基于大型语言模型的虚拟参与人员在大规模心理学研究中的潜力和局限性，并强调了需要丰富文化背景的训练数据和改进情感建模的必要性。', 'title_zh': '大型语言模型在模拟区域心理结构方面的有效性：人格与主观幸福感的实证考察'}
{'arxiv_id': 'arXiv:2509.25267', 'title': 'Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning', 'authors': 'Jiexi Xu', 'link': 'https://arxiv.org/abs/2509.25267', 'abstract': 'The performance of Large Language Models (LLMs) depends heavily on the chosen prompting strategy, yet static approaches such as Zero-Shot, Few-Shot, or Chain-of-Thought (CoT) impose a rigid efficiency-accuracy trade-off. Highly accurate strategies like Self-Consistency (SC) incur substantial computational waste on simple tasks, while lightweight methods often fail on complex inputs. This paper introduces the Prompt Policy Network (PPN), a lightweight reinforcement learning framework that formalizes adaptive strategy selection as a single-step Markov Decision Process (MDP). The PPN, trained with Proximal Policy Optimization (PPO) and guided by a resource-explicit reward function, learns to allocate costly reasoning strategies only when necessary. Experiments on arithmetic reasoning benchmarks demonstrate that PPN achieves superior performance on the efficiency-accuracy Pareto front, delivering up to 61.5% token cost reduction compared to Self-Consistency while maintaining competitive accuracy. This work contributes a systematic, adaptive framework for cost-efficient LLM deployment, advancing the design of lightweight optimization techniques for scalable and sustainable language model applications.', 'abstract_zh': '大型语言模型（LLMs）的表现很大程度上取决于所选用的提示策略，然而，零样本、少样本或思维链（CoT）等静态方法均会导致效率-准确性的固化权衡。 Self-Consistency（自洽性）等高准确性的策略在简单任务上会消耗大量不必要的计算资源，而轻量级方法则在复杂输入上常常失效。本文引入了提示策略网络（PPN），这是一种轻量级的强化学习框架，将自适应策略选择形式化为单步马尔可夫决策过程（MDP）。PPN经过 proximal 策略优化（PPO）训练，并由一个明确资源的奖励函数引导，学会只在必要时分配昂贵的推理策略。在算术推理基准测试中的实验表明，PPN在效率-准确性的帕累托前沿上表现出色，相较于自洽性，能够实现高达61.5%的token成本降低，同时保持了竞争力的准确度。本文贡献了一种系统化的、自适应的成本高效LLM部署框架，推动了轻量级优化技术在可扩展和可持续的语言模型应用设计中的发展。', 'title_zh': '基于轻量级强化学习的动态策略诱导以实现自适应提示优化：在效率与准确率之间架起桥梁'}
{'arxiv_id': 'arXiv:2509.25264', 'title': 'From NL2SQL to NL2GeoSQL: GeoSQL-Eval for automated evaluation of LLMs on PostGIS queries', 'authors': 'Shuyang Hou, Haoyue Jiao, Ziqi Liu, Lutong Xie, Guanyu Chen, Shaowen Wu, Xuefeng Guan, Huayi Wu', 'link': 'https://arxiv.org/abs/2509.25264', 'abstract': "In recent years, large language models (LLMs) have achieved remarkable progress in natural language understanding and structured query generation (NL2SQL). However, extending these advances to GeoSQL tasks in the PostGIS environment remains challenging due to the complexity of spatial functions, geometric data types, and execution semantics. Existing evaluations primarily focus on general relational databases or Google Earth Engine code generation, leaving a lack of systematic benchmarks tailored to spatial databases. To address this gap, this study introduces GeoSQL-Eval, the first end-to-end automated evaluation framework for PostGIS query generation. Built upon Webb's Depth of Knowledge (DOK) model, the framework encompasses four cognitive dimensions, five proficiency levels, and twenty task categories, providing a comprehensive assessment of model performance in terms of knowledge acquisition, syntactic generation, semantic alignment, execution accuracy, and robustness. In parallel, we developed GeoSQL-Bench, a benchmark dataset comprising 14178 questions that span three task types, 340 PostGIS functions, and 82 domain-specific databases. Leveraging this framework, we systematically evaluated 24 representative models across six categories, applying entropy-weighting and statistical analyses to reveal differences in performance, error distributions, and resource consumption patterns. Furthermore, we established a public GeoSQL-Eval leaderboard that enables global research teams to conduct ongoing testing and comparison. These contributions not only extend the boundaries of NL2SQL applications but also provide a standardized, interpretable, and scalable framework for evaluating LLM performance in spatial database contexts, offering valuable insights for model optimization and applications in geographic information science, urban studies, and spatial analysis.", 'abstract_zh': '近年来，大型语言模型（LLMs）在自然语言理解与结构化查询生成（NL2SQL）方面取得了显著进展。然而，将这些进展扩展到PostGIS环境中的GeoSQL任务由于空间函数、几何数据类型和执行语义的复杂性仍具挑战性。现有评估主要集中在通用关系数据库或Google Earth Engine代码生成上，缺乏针对空间数据库的系统性基准测试。为填补这一空白，本研究引入了GeoSQL-Eval，这是首个针对PostGIS查询生成的端到端自动化评估框架。该框架基于Webb的知识深度（DOK）模型，涵盖了四个认知维度、五个熟练程度级别和二十个任务类别，提供了模型在知识获取、语法生成、语义对齐、执行准确性和鲁棒性方面的全面评估。与此同时，我们开发了GeoSQL-Bench，这是一个包含14178个问题的基准数据集，涵盖了三种任务类型、340个PostGIS函数和82个领域特定数据库。利用该框架，我们系统性地评估了24个代表性模型的六个类别，并通过熵加权和统计分析揭示了不同性能、错误分布和资源消耗模式。此外，我们建立了一个公共GeoSQL-Eval排行榜，使全球研究团队能够持续进行测试和比较。这些贡献不仅扩展了NL2SQL应用的边界，还提供了一个标准化、可解释和可扩展的框架，用于评估在空间数据库上下文中LLM的性能，为模型优化以及地理信息科学、城市研究和空间分析的应用提供了宝贵的见解。', 'title_zh': '从NL2SQL到NL2GeoSQL：GeoSQL-Eval用于PostGIS查询的自动化评估'}
{'arxiv_id': 'arXiv:2509.25253', 'title': 'Knowledge distillation through geometry-aware representational alignment', 'authors': 'Prajjwal Bhattarai, Mohammad Amjad, Dmytro Zhylko, Tuka Alhanai', 'link': 'https://arxiv.org/abs/2509.25253', 'abstract': 'Knowledge distillation is a common paradigm for transferring capabilities from larger models to smaller ones. While traditional distillation methods leverage a probabilistic divergence over the output of the teacher and student models, feature-based distillation methods often minimize variants of Euclidean norms between the hidden layer representations. The main goal is for the student to mimic the structure of the feature space of the teacher. In this work, we theoretically show that existing feature distillation methods, such as projection based mean squared loss or Centered Kernel Alignment (CKA), cannot capture the feature structure, even under zero loss. We then motivate the use of Procrustes distance and the Frobenius norm of Feature Gram Matrix, distances already common in the context of measuring representational alignment, as distillation losses. We show that feature distillation through our method showcases statistically significant improvement in distillation performance across language models families (BERT and OPT) in classification and instruction-following tasks by up to 2 percentage points, showcasing the potential of integrating feature geometry into existing distillation methods.', 'abstract_zh': '知识蒸馏是一种将大型模型的能力转移到小型模型中的常用范式。虽然传统蒸馏方法通过教师和学生模型输出的概率性差异来工作，基于特征的蒸馏方法通常最小化隐藏层表示之间的欧几里得范数变体。主要目标是让学生模仿教师特征空间的结构。在本项工作中，我们理论上证明现有的基于特征的蒸馏方法，如基于投影的均方误差或中心核对齐（CKA），即使在无损失情况下也无法捕捉特征结构。随后，我们引入普罗克鲁斯泰斯距离和特征Gram矩阵的弗罗贝尼乌斯范数作为蒸馏损失，这些距离已经在衡量表示对齐的上下文中被普遍使用。我们展示了通过我们方法进行的特征蒸馏在语言模型家族（BERT和OPT）的任务中（分类和指令跟随）实现了统计显著的性能提升，最多达到2个百分点，展示了将特征几何整合到现有蒸馏方法中的潜力。', 'title_zh': '通过几何意识表示对齐的知识蒸馏'}
{'arxiv_id': 'arXiv:2509.25248', 'title': 'BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software', 'authors': 'Zehua Zhang, Ati Priya Bajaj, Divij Handa, Siyu Liu, Arvind S Raj, Hongkai Chen, Hulin Wang, Yibo Liu, Zion Leonahenahe Basque, Souradip Nath, Vishal Juneja, Nikhil Chapre, Yan Shoshitaishvili, Adam Doupé, Chitta Baral, Ruoyu Wang', 'link': 'https://arxiv.org/abs/2509.25248', 'abstract': "Automatically compiling open-source software (OSS) projects is a vital, labor-intensive, and complex task, which makes it a good challenge for LLM Agents. Existing methods rely on manually curated rules and workflows, which cannot adapt to OSS that requires customized configuration or environment setup. Recent attempts using Large Language Models (LLMs) used selective evaluation on a subset of highly rated OSS, a practice that underestimates the realistic challenges of OSS compilation. In practice, compilation instructions are often absent, dependencies are undocumented, and successful builds may even require patching source files or modifying build scripts. We propose a more challenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more diverse in quality, scale, and characteristics. Furthermore, we propose a strong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with enhanced build instruction retrieval module that achieves state-of-the-art performance on BUILD-BENCH and is adaptable to heterogeneous OSS characteristics. We also provide detailed analysis regarding different compilation method design choices and their influence to the whole task, offering insights to guide future advances. We believe performance on BUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as a complex software engineering tasks, and, as such, our benchmark will spur innovation with a significant impact on downstream applications in the fields of software development and software security.", 'abstract_zh': '自动编译开源软件（OSS）项目是一项重要但劳动密集和复杂的任务，是大模型代理的理想挑战。现有方法依赖于手动curated的规则和工作流，不能适应需要定制配置或环境设置的OSS。最近使用大语言模型（LLMs）的方法仅在高评价的OSS子集上进行选择性评估，这低估了OSS编译的实际挑战。实际上，编译指令往往缺失，依赖关系没有记录，甚至成功构建可能还需要修改源文件或构建脚本。我们提出更具挑战性和实际性的基准BUILD-BENCH，包括质量、规模和特性更丰富的OSS。此外，我们提出了一种强大的基于大模型的代理OSS-BUILD-AGENT，这是一种有效系统，具有增强的构建指令检索模块，在BUILD-BENCH上达到了最先进的性能，并且能够适应异构OSS特性。我们还详细分析了不同编译方法设计选择及其对整个任务的影响，提供了未来进展的见解。我们认为，在BUILD-BENCH上的性能能够真实反映代理处理编译这一复杂软件工程任务的能力，因此，我们的基准将激发具有重大影响的创新，推动软件开发和软件安全领域的下游应用。', 'title_zh': 'BuildBench: 评估大语言模型代理在编译真实世界开源软件方面的性能'}
{'arxiv_id': 'arXiv:2509.25247', 'title': 'Protocode: Prototype-Driven Interpretability for Code Generation in LLMs', 'authors': 'Krishna Vamshi Bodla, Haizhao Yang', 'link': 'https://arxiv.org/abs/2509.25247', 'abstract': 'Since the introduction of Large Language Models (LLMs), they have been widely adopted for various tasks such as text summarization, question answering, speech-to-text translation, and more. In recent times, the use of LLMs for code generation has gained significant attention, with tools such as Cursor and Windsurf demonstrating the ability to analyze massive code repositories and recommend relevant changes. Big tech companies have also acknowledged the growing reliance on LLMs for code generation within their codebases. Although these advances significantly improve developer productivity, increasing reliance on automated code generation can proportionally increase the risk of suboptimal solutions and insecure code. Our work focuses on automatically sampling In-Context Learning (ICL) demonstrations which can improve model performance and enhance the interpretability of the generated code. Using AST-based analysis on outputs from the MBPP test set, we identify regions of code most influenced by the chosen demonstrations. In our experiments, we show that high-quality ICL demonstrations not only make outputs easier to interpret but also yield a positive performance improvement on the pass@10 metric. Conversely, poorly chosen ICL demonstrations affected the LLM performance on the pass@10 metric negatively compared to the base model. Overall, our approach highlights the importance of efficient sampling strategies for ICL, which can affect the performance of the model on any given task.', 'abstract_zh': '自大型语言模型（LLMs）的引入以来，它们已被广泛应用于文本总结、问答、语音转文本翻译等多种任务。近年来，使用LLMs进行代码生成引起了广泛关注，如Cursor和Windsurf等工具能够分析大型代码仓库并推荐相关更改。大型科技公司也认识到LLMs在其代码库中进行代码生成依赖性的增长。尽管这些进展显著提高了开发者的工作效率，但对自动代码生成的日益依赖也相应地增加了设计方案不优化和代码不安全的风险。我们的工作集中在自动采样示例上下文学习（ICL）演示，以提高模型性能并增强生成代码的可解释性。通过基于AST的分析MBPP测试集的输出，我们确定了最受所选演示影响的代码区域。在我们的实验中，我们展示了高质量的ICL演示不仅使输出更易于解释，还提高了pass@10指标上的性能。相反，选择不佳的ICL演示会负面影响LLMs在pass@10指标上的性能与基模型相比。总体而言，我们的方法突显了高效采样策略对ICL的重要性，这可能影响模型在任何给定任务上的性能。', 'title_zh': '原型驱动的可解释性方法：在大规模语言模型中进行代码生成'}
{'arxiv_id': 'arXiv:2509.25243', 'title': 'Reinforcement Learning-Guided Chain-of-Draft for Token-Efficient Code Generation', 'authors': 'Xunzhu Tang, Iyiola Emmanuel Olatunji, Tiezhu Sun, Jacques Klein, Tegawende F. Bissyande', 'link': 'https://arxiv.org/abs/2509.25243', 'abstract': "LLMs demonstrate surface-level fluency in code generation but struggle with structured reasoning tasks requiring correctness and semantic alignment. While Chain-of-Thought (CoT) prompting enhances reasoning through intermediate steps, it suffers from verbosity and inefficiency. Chain-of-Draft (CoD) prompting offers more concise reasoning, but the stochastic nature of LLMs produces varying solution quality, making optimal selection challenging. We propose \\multicod, a reinforcement learning framework that learns to select the most promising candidate from CoD-generated solutions. Our approach uses strategy-guided prompting to encourage diverse reasoning styles and models solution selection as a contextual bandit problem. The framework optimizes interpretable features including code complexity, reasoning structure, and strategic metadata through a reward function balancing correctness, efficiency, and clarity. Experiments on MBPP, BigCodeBench, SWE-bench Verified, and Defects4J show \\multicod~outperforms and in some cases, on par with standard prompting, CoT, and CoD baselines while achieving cost and token efficiency from the user's perspective through a multi-candidate design that charges only for the selected output, reducing user billing by over 50\\% and improving LLM response quality, making \\multicod~more sustainable and scalable for real-world deployment. Our code is available: this https URL.", 'abstract_zh': 'LLMs在代码生成中表现出表面流畅性，但在需要正确性和语义对齐的结构化推理任务中表现出色。虽然Chain-of-Thought（CoT）提示可以通过中间步骤增强推理，但它存在冗余和低效的问题。Chain-of-Draft（CoD）提示提供更简洁的推理，但LLMs的随机性质导致生成的解决方案质量参差不齐，使最优选择变得困难。我们提出了一种强化学习框架\\multicod，用于从CoD生成的解决方案中选择最有前景的候选方案。我们的方法使用策略引导的提示鼓励多样化的推理风格，并将解决方案选择建模为上下文臂赛问题。该框架通过平衡正确性、效率和清晰度的奖励函数优化可解释特征，包括代码复杂性、推理结构和战略性元数据。实验表明，在MBPP、BigCodeBench、SWE-bench Verified和Defects4J上，\\multicod在某些情况下与标准提示、CoT和CoD基线相当或更优，并通过多候选设计方案从用户角度看实现了成本和令牌效率，通过仅对选定输出收费减少用户账单超过50%，并提高LLM响应质量，使\\multicod更具可持续性和可扩展性，适用于实际部署。代码可在以下链接获取：this https URL。', 'title_zh': 'Reinforcement Learning 引导的串行草稿生成方法：token 节约的代码生成'}
{'arxiv_id': 'arXiv:2509.25240', 'title': 'HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement', 'authors': 'Ming Yang, Xiaofan Li, Zhiyuan Ma, Dengliang Shi, Jintao Du, Yu Cheng, Weiguo Zheng', 'link': 'https://arxiv.org/abs/2509.25240', 'abstract': 'Recent curriculum reinforcement learning for large language models (LLMs) typically rely on difficulty-based annotations for data filtering and ordering. However, such methods suffer from local optimization, where continual training on simple samples in the early steps can cause the policy to lose its exploration. We propose a novel schema, namely Hamiltonian curiosity augmented large language model reinforcement (HAMMER), that transfers diversity metrics, commonly used in dataset evaluation, into the dynamic reinforcement learning procedure, where training samples are ordered via a minimum-semantic Hamiltonian path making the initial training retrain more exploration. From a theoretical perspective of generalization bounds, diversity-driven ordering facilitates stable convergence. Empirical evaluations indicate that HAMMER stimulates model "curiosity" and consistently achieves a 3% to 4% average accuracy gain across diverse inference benchmark.', 'abstract_zh': '基于哈密顿 curiosity 增强的大型语言模型强化学习（HAMMER）', 'title_zh': 'HAMMER：基于哈密尔顿 curiosity 增强的大语言模型强化学习'}
{'arxiv_id': 'arXiv:2509.25238', 'title': 'PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases', 'authors': 'Sri Vatsa Vuddanti, Aarav Shah, Satwik Kumar Chittiprolu, Tony Song, Sunishchal Dev, Kevin Zhu, Maheep Chaudhary', 'link': 'https://arxiv.org/abs/2509.25238', 'abstract': "Tool-augmented language agents frequently fail in real-world deployment due to tool malfunctions--timeouts, API exceptions, or inconsistent outputs--triggering cascading reasoning errors and task abandonment. Existing agent training pipelines optimize only for success trajectories, failing to expose models to the tool failures that dominate real-world usage. We propose \\textbf{PALADIN}, a generalizable framework for equipping language agents with robust failure recovery capabilities. PALADIN trains on 50,000+ recovery-annotated trajectories constructed via systematic failure injection and expert demonstrations on an enhanced ToolBench dataset. Training uses LoRA-based fine-tuning to retain base capabilities while injecting recovery competence. At inference, PALADIN detects execution-time errors and retrieves the most similar case from a curated bank of 55+ failure exemplars aligned with ToolScan's taxonomy, then executes the corresponding recovery action. This approach generalizes to novel failures beyond the training distribution, retaining 95.2\\% recovery performance on unseen tool APIs. Evaluation across PaladinEval and ToolReflectEval demonstrates consistent improvements in Recovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR), and Efficiency Score (ES). PALADIN improves RR from 32.76% to 89.68% (+57% relative) over ToolBench and outperforms the strongest baseline CRITIC (76.34%) by +13.3%. Against vanilla agents, PALADIN achieves 89.86\\% RR (+66% relative improvement from 23.75%). These results establish PALADIN as an effective method for building fault-tolerant agents capable of robust recovery in real-world tool environments.", 'abstract_zh': 'PALADIN：一种通用框架，用于为语言代理提供 robust 失败恢复能力', 'title_zh': 'PALADIN: 自我纠正语言模型代理以治愈工具失败病例'}
{'arxiv_id': 'arXiv:2509.25214', 'title': 'On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs', 'authors': 'Rongguang Ye, Ming Tang, Edith C. H. Ngai', 'link': 'https://arxiv.org/abs/2509.25214', 'abstract': 'As increasingly large pre-trained models are released, deploying them on edge devices for privacy-preserving applications requires effective compression. Recent works combine quantization with the fine-tuning of high-precision LoRA adapters, which can substantially reduce model size while mitigating the accuracy loss from quantization. However, edge devices have inherently heterogeneous capabilities, while performing configuration-wise fine-tuning for every quantization setting is computationally prohibitive. In this paper, we propose CoA-LoRA, a method that dynamically adjusts the LoRA adapter to arbitrary quantization configurations (i.e., the per-layer bit-width choices of a pre-trained model) without requiring repeated fine-tuning. This is accomplished via a configuration-aware model that maps each configuration to its low-rank adjustments. The effectiveness of this model critically depends on the training configuration set, a collection of configurations chosen to cover different total bit-width budgets. However, constructing a high-quality configuration set is non-trivial. We therefore design a Pareto-based configuration search that iteratively optimizes the training configuration set, yielding more precise low-rank adjustments. Our experiments demonstrate that, unlike the state-of-the-art methods that require fine-tuning a separate LoRA adapter for each configuration, CoA-LoRA incurs no additional time cost while achieving comparable or even superior performance to those methods.', 'abstract_zh': '随着越来越大规模的预训练模型的发布，将它们部署在边缘设备上以实现隐私保护应用需要有效的压缩方法。近期的研究将量化与高精度LoRA适应器的微调结合起来，这可以在大幅减小模型体积的同时减轻量化带来的准确率损失。然而，边缘设备本身具有固有的异构性，为每个量化设置进行配置级微调在计算上是不可行的。在这篇论文中，我们提出了一种名为CoA-LoRA的方法，可以在无需重复微调的情况下动态调整LoRA适应器以适应任意的量化配置（即预训练模型的每层位宽选择），通过一种配置感知的模型将每个配置映射到其低秩调整。这个模型的有效性严重依赖于训练配置集，这个集合是由不同总位宽预算的选择组成。然而，构建高质量的配置集合并不容易。因此，我们设计了一种基于帕累托优化的配置搜索方法，迭代优化训练配置集，从而获得更精确的低秩调整。实验表明，与最先进的方法需要为每个配置单独微调一个LoRA适应器相比，CoA-LoRA不会增加额外的时间成本，同时能够实现可比甚至更优的性能。', 'title_zh': '基于量化配置的实时自适应：面向高效量化大规模语言模型微调的配置意识LoRA'}
{'arxiv_id': 'arXiv:2509.25207', 'title': 'Multi-level Diagnosis and Evaluation for Robust Tabular Feature Engineering with Large Language Models', 'authors': 'Yebin Lim, Susik Yoon', 'link': 'https://arxiv.org/abs/2509.25207', 'abstract': 'Recent advancements in large language models (LLMs) have shown promise in feature engineering for tabular data, but concerns about their reliability persist, especially due to variability in generated outputs. We introduce a multi-level diagnosis and evaluation framework to assess the robustness of LLMs in feature engineering across diverse domains, focusing on the three main factors: key variables, relationships, and decision boundary values for predicting target classes. We demonstrate that the robustness of LLMs varies significantly over different datasets, and that high-quality LLM-generated features can improve few-shot prediction performance by up to 10.52%. This work opens a new direction for assessing and enhancing the reliability of LLM-driven feature engineering in various domains.', 'abstract_zh': 'Recent Advancements in Large Language Models (LLMs) for Feature Engineering in Tabular Data: A Multi-Level Diagnosis and Evaluation Framework for Assessing Robustness Across Diverse Domains', 'title_zh': '大规模语言模型驱动的稳健表格特征工程多层级诊断与评估'}
{'arxiv_id': 'arXiv:2509.25204', 'title': 'Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for Controlled Text Generation', 'authors': 'Jin Li, Zhebo Wang, Tianliang Lu, Mohan Li, Wenpeng Xing, Meng Han', 'link': 'https://arxiv.org/abs/2509.25204', 'abstract': 'Entropy-based inference methods have gained traction for improving the reliability of Large Language Models (LLMs). However, many existing approaches, such as entropy minimization techniques, suffer from high computational overhead and fail to leverage historical token context effectively. To address these limitations, we propose Spectral Logit Sculpting (SLS), a lightweight inference-time optimization method that dynamically modulates token distributions using spectral and entropic properties of recent logits. SLS maintains a sliding buffer of top-K logits, performs on-the-fly Singular Value Decomposition (SVD) to identify dominant spectral directions, and adaptively rescales logits based on both entropy and logit gap statistics--only activating when uncertainty is high. Without updating any model parameters, SLS effectively sharpens the output distribution while preserving contextual consistency. Experimental results on multiple public benchmarks demonstrate that SLS consistently outperforms existing baseline methods, achieving superior accuracy in mathematical, coding, and scientific reasoning tasks.', 'abstract_zh': '基于谱投影的logit调控方法（Spectral Logit Sculpting, SLS）在提升大语言模型可靠性方面的轻量级推理时优化方法', 'title_zh': '光谱Logit塑形：自适应低秩Logit变换以实现可控文本生成'}
{'arxiv_id': 'arXiv:2509.25203', 'title': 'Generating High-Quality Datasets for Code Editing via Open-Source Language Models', 'authors': 'Zekai Zhang, Mingwei Liu, Zhenxi Chen, Linxi Liang, Yuxuan Chen, Guangsheng Ou, Yanlin Wang, Dan Li, Xin Peng, Zibin Zheng', 'link': 'https://arxiv.org/abs/2509.25203', 'abstract': 'Code editing plays a vital role in software engineering, requiring developers to adjust existing code according to natural language instructions while keeping functionality intact and avoiding unnecessary modifications. However, commit-based datasets commonly used for this task are often noisy, lack diversity, and fail to reflect the style of real-world edit instructions. To address this, we introduce CanItEdit, an open-source pipeline that leverages multiple LLMs to synthesize realistic code-edit triplets. The pipeline produces both concise "lazy" instructions and more detailed "descriptive" ones, and applies filtering based on diffs and topics to guarantee data quality and variety. Using this process, we construct OCEDataFT, a curated dataset of 20K samples. Fine-tuning three advanced base models on OCEDataFT leads to significant performance boosts on the CanItEdit benchmark, with relative pass@1 improvements ranging from 4.50% to 20.79%. Notably, the resulting models achieve performance close to closed-source systems, narrowing the gap to GPT-4 to just 3.54%, without relying on proprietary resources or manual annotation.', 'abstract_zh': '代码编辑在软件工程中起着至关重要的作用，要求开发人员根据自然语言指令调整现有代码，同时保持功能完整并避免不必要的修改。然而，用于此任务的基于提交的数据集往往噪声较大、缺乏多样性且无法反映实际编辑指令的风格。为解决这一问题，我们引入了CanItEdit，这是一个开源管道，利用多个LLM生成真实的代码编辑三元组。该管道生成简洁的“懒惰”指令和更详细的“描述性”指令，并基于差异和主题进行过滤，以保证数据质量和多样性。使用这一过程，我们构建了OCEDataFT，一个包含20,000个样本的精选数据集。在OCEDataFT上微调三个高级基础模型后，在CanItEdit基准测试中取得了显著性能提升，相对pass@1改进范围从4.50%到20.79%。值得注意的是，这些模型的性能接近商用系统，与GPT-4的差距仅3.54%，且不依赖于专有资源或人工标注。', 'title_zh': '基于开源语言模型生成高质量代码编辑数据集'}
{'arxiv_id': 'arXiv:2509.25197', 'title': 'Towards Repository-Level Program Verification with Large Language Models', 'authors': 'Si Cheng Zhong, Xujie Si', 'link': 'https://arxiv.org/abs/2509.25197', 'abstract': 'Recent advancements in large language models (LLMs) suggest great promises in code and proof generations. However, scaling automated formal verification to real-world projects requires resolving cross-module dependencies and global contexts, which are crucial challenges overlooked by existing LLM-based methods with a special focus on targeting isolated, function-level verification tasks. To systematically explore and address the significant challenges of verifying entire software repositories, we introduce RVBench, the first verification benchmark explicitly designed for repository-level evaluation, constructed from four diverse and complex open-source Verus projects.\nWe further introduce RagVerus, an extensible framework that synergizes retrieval-augmented generation with context-aware prompting to automate proof synthesis for multi-module repositories. RagVerus triples proof pass rates on existing benchmarks under constrained model inference budgets, and achieves a 27% relative improvement on the more challenging RVBench benchmark, demonstrating a scalable and sample-efficient verification solution.', 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）在代码和证明生成方面带来了巨大前景。然而，将自动化形式验证扩展到实际项目需要解决跨模块依赖和全局上下文问题，这是现有的基于LLM的方法特别是专注于孤立的功能级验证任务时忽略的重要挑战。为了系统地探索和解决验证整个软件仓库的关键挑战，我们引入了RVBench，这是第一个明确为仓库级评估设计的验证基准，基于四个多样且复杂的开源Verus项目构建。\n此外，我们引入了RagVerus，这是一种可扩展框架，将检索增强生成与上下文感知提示相结合，以自动化多模块仓库的证明合成。在受约束的模型推理预算下，RagVerus将现有基准上的证明通过率提高三倍，并在更具挑战性的RVBench基准测试中实现了27%的相对改进，展示了可扩展且样本高效的验证解决方案。', 'title_zh': '面向存储库级别程序验证的大语言模型方法'}
{'arxiv_id': 'arXiv:2509.25196', 'title': 'APRIL: API Synthesis with Automatic Prompt Optimization and Reinforcement Learning', 'authors': 'Hua Zhong, Shan Jiang, Sarfraz Khurshid', 'link': 'https://arxiv.org/abs/2509.25196', 'abstract': 'APIs are central to modern software development, yet composing new APIs from large libraries is difficult due to the exponential search space; traditional component-based synthesis relies on costly exploration and hand-crafted specifications. While large language models (LLMs) can generate implementations from natural language, hallucinations and limited access to up-to-date contextual information often yield incorrect code. In this paper, we present APRIL, an approach that combines LLM-based synthesis with Automatic Prompt Optimization (APO) and Reinforcement Learning from Verifiable Rewards (RLVR): APO iteratively refines prompts for a frozen model, while RLVR fine-tunes the policy toward functional correctness, producing an efficient synthesis pipeline. Evaluated on 81 real-world APIs from widely used scientific Python libraries and benchmarked against instruction-tuned but unfine-tuned LLMs guided by expert prompts, APRIL achieves substantial improvements. These results indicate that integrating APO and RLVR provides a robust, scalable path for component-based API synthesis in large libraries.', 'abstract_zh': 'APRIL：结合LLM合成、自动提示优化和可验证奖励强化学习的组件化API合成方法', 'title_zh': 'APRIL：带有自动提示优化和强化学习的API合成'}
{'arxiv_id': 'arXiv:2509.25193', 'title': 'Devstral: Fine-tuning Language Models for Coding Agent Applications', 'authors': 'Abhinav Rastogi, Adam Yang, Albert Q. Jiang, Alexander H. Liu, Alexandre Sablayrolles, Amélie Héliou, Amélie Martin, Anmol Agarwal, Andy Ehrenberg, Andy Lo, Antoine Roux, Arthur Darcet, Arthur Mensch, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Chris Bamford, Christian Wallenwein, Christophe Renaudin, Clémence Lanfranchi, Clément Denoix, Corentin Barreau, Darius Dabert Devon Mizelle, Diego de las Casas, Elliot Chane-Sane, Emilien Fugier, Emma Bou Hanna, Gabrielle Berrada, Gauthier Delerce, Gauthier Guinet, Georgii Novikov, Graham Neubig, Guillaume Lample, Guillaume Martin, Himanshu Jaju, Jan Ludziejewski, Jason Rute, Jean-Malo Delignon, JeanHadrien Chabran, Joachim Studnia, Joep Barmentlo, Jonas Amar, Josselin Somerville Roberts, Julien Denize, Karan Saxena, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, Kush Jain, Lélio Renard Lavaud, Léonard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, Marie Pellat, Mathilde Guillaumin, Mathis Felardos, Matthieu Dinot, Maxime Darrin, Maximilian Augustin, Mickaël Seznec, Neha Gupta, Nikhil Raghuraman, Olivier Duchenne, Patricia Wang, Patrick von Platen, Patryk Saffer, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Rémi Delacourt, Roman Soletskyi, Romain Sauvestre, Sagar Vaze, Sanchit Gandhi, Sandeep Subramanian, Shashwat Dalal, Siddharth Gandhi, Soham Ghosh, Srijan Mishra, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Thibault Schueller, Thomas Foubert, Thomas Robert, Thomas Wang, Timothée Lacroix, Tom Bewley, Valeriia Nemychnikova, Victor Paltz, Virgile Richard, Wen-Ding Li, William Marshall, Xingyao Wang', 'link': 'https://arxiv.org/abs/2509.25193', 'abstract': 'We introduce Devstral-Small, a lightweight open source model for code agents with the best performance among models below 100B size. In this technical report, we give an overview of how we design and develop a model and craft specializations in agentic software development. The resulting model, Devstral-Small is a small 24B model, fast and easy to serve. Despite its size, Devstral-Small still attains competitive performance compared to models more than an order of magnitude larger.', 'abstract_zh': 'Devstral-Small：一种轻量级开源代码代理模型，兼具最佳性能', 'title_zh': 'Devstral: 用于编码代理应用的语言模型微调'}
{'arxiv_id': 'arXiv:2509.00105', 'title': 'AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and High-Quality Language Model Serving', 'authors': 'Shaoting Feng, Hanchen Li, Kuntai Du, Zhuohan Gu, Yuhan Liu, Jiayi Yao, Siddhant Ray, Samuel Shen, Yihua Cheng, Ganesh Ananthanarayanan, Junchen Jiang', 'link': 'https://arxiv.org/abs/2509.00105', 'abstract': 'Large language model (LLM) applications often reuse previously processed context, such as chat history and documents, which introduces significant redundant computation. Existing LLM serving systems address such redundant computation by storing the KV caches of processed context and loading the corresponding KV cache when a new request reuses the context. Further, as these LLM applications scale, the total size of KV caches becomes excessively large and requires both DRAM and SSD for full storage.\nHowever, prior work that stores KV caches in DRAM and SSD suffers from high loading delays, as most KV cache hits come from SSD, which is slow to load. To increase the KV cache hit rate on DRAM, we identify lossy KV cache compression as a promising approach. We design a lossy compression system that decides the compression algorithm, compression rate and device placement for each KV cache entry to maximise DRAM hits and minimise loading delay without significantly degrading generation quality. Compared to various static compression baselines across three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at the same quality and 6--55% quality improvements at the same delay.', 'abstract_zh': '大型语言模型（LLM）应用通常重用先前处理的上下文，如聊天历史和文档，这引入了显著的冗余计算。现有LLM服务系统通过存储处理上下文的KV缓存，并在新请求重用上下文时加载相应的KV缓存来解决这种冗余计算问题。随着这些LLM应用的扩展，KV缓存的总大小变得异常庞大，需要同时使用DRAM和SSD进行全存储。\n\n然而，以往将KV缓存存储在DRAM和SSD中的工作面临着高加载延迟的问题，因为大多数KV缓存命中来自于SSD，加载速度较慢。为了提高DRAM中的KV缓存命中率，我们确定了有损KV缓存压缩作为一种有前景的方法。我们设计了一种有损压缩系统，为每个KV缓存项决定压缩算法、压缩率和设备放置，以最大化DRAM命中率、最小化加载延迟，同时不显著降低生成质量。与三个任务的多种静态压缩基准相比，我们的系统AdaptCache在相同质量下实现了1.43–2.4倍的延迟节省，在相同延迟下实现了6–55%的质量提升。', 'title_zh': 'AdaptCache：低延迟和高性能语言模型服务的键值缓存内置存储层次结构'}
{'arxiv_id': 'arXiv:2505.23495', 'title': 'Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking', 'authors': 'Liangliang Zhang, Zhuorui Jiang, Hongliang Chi, Haoyang Chen, Mohammed Elkoumy, Fali Wang, Qiong Wu, Zhengyi Zhou, Shirui Pan, Suhang Wang, Yao Ma', 'link': 'https://arxiv.org/abs/2505.23495', 'abstract': 'Knowledge Graph Question Answering (KGQA) systems rely on high-quality benchmarks to evaluate complex multi-hop reasoning. However, despite their widespread use, popular datasets such as WebQSP and CWQ suffer from critical quality issues, including inaccurate or incomplete ground-truth annotations, poorly constructed questions that are ambiguous, trivial, or unanswerable, and outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA datasets, including WebQSP and CWQ, we find that the average factual correctness rate is only 57 %. To address these issues, we introduce KGQAGen, an LLM-in-the-loop framework that systematically resolves these pitfalls. KGQAGen combines structured knowledge grounding, LLM-guided generation, and symbolic verification to produce challenging and verifiable QA instances. Using KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results demonstrate that even state-of-the-art systems struggle on this benchmark, highlighting its ability to expose limitations of existing models. Our findings advocate for more rigorous benchmark construction and position KGQAGen as a scalable framework for advancing KGQA evaluation.', 'abstract_zh': '基于知识图谱的问答系统（KGQA）依赖高质量的基准进行复杂多跳推理评估。然而，尽管这些系统广泛应用，流行的数据集如WebQSP和CWQ存在严重质量问题，包括不准确或不完整的ground-truth注释、语义模糊、平凡或不可回答的问题，以及过时或不一致的知识。通过对16个流行的KGQA数据集进行手动审计，包括WebQSP和CWQ，我们发现这些数据集的事实正确率平均仅为57%。为了解决这些问题，我们引入了KGQAGen，这是一个LLM辅助框架，系统性地解决了这些问题。KGQAGen结合了结构化知识 grounding、LLM引导生成和符号验证，以生成具有挑战性和可验证的问答实例。使用KGQAGen，我们构建了基于Wikidata的KGQAGen-10k基准，并评估了多种KG-RAG模型。实验结果表明，即使最先进的系统在该基准上也面临困难，突显了其揭示现有模型局限性的能力。我们的发现呼吁更加严格的基准构建，并将KGQAGen定位为推动KGQA评估可扩展框架。', 'title_zh': '诊断和解决KG-RAG数据集中的 pitfalls：朝着更可靠的基准测试迈进'}
