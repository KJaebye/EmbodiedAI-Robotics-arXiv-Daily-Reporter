{'arxiv_id': 'arXiv:2509.26642', 'title': 'MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation', 'authors': 'Zhuoyang Liu, Jiaming Liu, Jiadong Xu, Nuowei Han, Chenyang Gu, Hao Chen, Kaichen Zhou, Renrui Zhang, Kai Chin Hsieh, Kun Wu, Zhengping Che, Jian Tang, Shanghang Zhang', 'link': 'https://arxiv.org/abs/2509.26642', 'abstract': "Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation. Most VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world. This gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control. To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling. Specifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence. To further enhance MLA's understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation. For evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations. Project website: this https URL", 'abstract_zh': '视觉-语言-动作模型（VLAs）通过继承视觉-语言模型（VLMs）的能力并在学习动作生成的同时展示出在机器人操作任务中的泛化能力。大多数VLA模型专注于通过解释视觉和语言来生成动作，而机器人必须感知并与其所处的三维物理世界进行互动。这一差距突显了对特定于机器人多模态信息的全面理解的需求，这对于实现复杂且接触密集的控制至关重要。为此，我们提出了一种多感官语言-动作（MLA）模型，该模型能够协同感知异构的感官模态，并预测未来的多感官目标，以促进物理世界的建模。具体来说，为了增强感知表示，我们提出了一种无需编码器的多模态对齐方案，通过位置对应关系直接利用大型语言模型来解释2D图像、3D点云和触觉标记等多模态提示。为进一步增强MLA对物理动力学的理解，我们设计了一种未来多感官生成后训练策略，使MLA能够在语义、几何和交互信息方面进行推理，从而为动作生成提供更稳健的条件。在评估中，MLA模型在复杂且接触密集的现实世界任务中分别比之前最先进的2D和3D VLA方法展示了12%和24%的性能提升，同时在未见过的配置方面也展示了更好的泛化能力。项目网站：this https URL', 'title_zh': 'MLA：一种多感官语言-动作模型，用于机器人操作中的多模态理解和预测。'}
{'arxiv_id': 'arXiv:2509.26633', 'title': 'OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction', 'authors': 'Lujie Yang, Xiaoyu Huang, Zhen Wu, Angjoo Kanazawa, Pieter Abbeel, Carmelo Sferrazza, C. Karen Liu, Rocky Duan, Guanya Shi', 'link': 'https://arxiv.org/abs/2509.26633', 'abstract': 'A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum.', 'abstract_zh': '一种针对教学型人形机器人复杂技能训练的主导范式是将人类动作重新定位为运动参考，以训练强化学习策略。然而，现有的重新定位管道常常难以克服人类和机器人之间显著的实体差距，产生诸如脚位滑动和穿插等物理上不可能的_artifacts。更重要的是，常见的重新定位方法忽视了表达性运动和运动操作中至关重要的人体-物体和人体-环境交互。为了解决这一问题，我们引入了基于交互网格的OmniRetarget，这是一种交互保留的数据生成引擎，明确建模并保留了智能体、地形和操纵物体之间的重要空间和接触关系。通过最小化人类和机器人网格之间的Laplacian变形并施加运动约束，OmniRetarget生成了运动学可行的轨迹。此外，保留与任务相关的交互使得从单个演示到不同机器人实体、地形和物体配置的数据增强变得更加高效。我们通过将OmniRetarget应用于OMOMO、LAFAN1和我们内部的MoCap数据集中的动作，生成超过8小时的轨迹，这些轨迹在满足运动学约束和保持接触方面优于广泛使用的基线。高质量的数据使得本体感受性RL策略能够在仅使用5个奖励项和所有任务共享的简单环境随机化的情况下，成功执行长达30秒的公园our和运动操作技能，而无需任何学习课程。', 'title_zh': '泛在适配：保持交互的数据生成为人形全身动操作和场景交互'}
{'arxiv_id': 'arXiv:2509.26513', 'title': 'Learning from Hallucinating Critical Points for Navigation in Dynamic Environments', 'authors': 'Saad Abdul Ghani, Kameron Lee, Xuesu Xiao', 'link': 'https://arxiv.org/abs/2509.26513', 'abstract': 'Generating large and diverse obstacle datasets to learn motion planning in environments with dynamic obstacles is challenging due to the vast space of possible obstacle trajecto- ries. Inspired by hallucination-based data synthesis approaches, we propose Learning from Hallucinating Critical Points (LfH- CP), a self-supervised framework for creating rich dynamic ob- stacle datasets based on existing optimal motion plans without requiring expensive expert demonstrations or trial-and-error exploration. LfH-CP factorizes hallucination into two stages: first identifying when and where obstacles must appear in order to result in an optimal motion plan, i.e., the critical points, and then procedurally generating diverse trajectories that pass through these points while avoiding collisions. This factorization avoids generative failures such as mode collapse and ensures coverage of diverse dynamic behaviors. We further introduce a diversity metric to quantify dataset richness and show that LfH-CP produces substantially more varied training data than existing baselines. Experiments in simulation demonstrate that planners trained on LfH-CP datasets achieves higher success rates compared to a prior hallucination method.', 'abstract_zh': '利用幻视关键点学习（LfH-CR）自监督框架生成丰富多样的动态障碍数据集以学习具有动态障碍环境中的运动规划', 'title_zh': '基于生成关键点进行动态环境中导航的学习'}
{'arxiv_id': 'arXiv:2509.26375', 'title': 'SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning', 'authors': 'Zichao Shen, Chen Gao, Jiaqi Yuan, Tianchen Zhu, Xingcheng Fu, Qingyun Sun', 'link': 'https://arxiv.org/abs/2509.26375', 'abstract': 'Embodied task planning requires agents to produce executable actions in a close-loop manner within the environment. With progressively improving capabilities of LLMs in task decomposition, planning, and generalization, current embodied task planning methods adopt LLM-based this http URL, existing LLM-based planners remain limited in three aspects, i.e., fixed planning paradigms, lack of action sequence constraints, and error-agnostic. In this work, we propose SDA-PLANNER, enabling an adaptive planning paradigm, state-dependency aware and error-aware mechanisms for comprehensive embodied task planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to explicitly model action preconditions and effects, guiding the dynamic revision. To handle execution error, it employs an error-adaptive replanning strategy consisting of Error Backtrack and Diagnosis and Adaptive Action SubTree Generation, which locally reconstructs the affected portion of the plan based on the current environment state. Experiments demonstrate that SDA-PLANNER consistently outperforms baselines in success rate and goal completion, particularly under diverse error conditions.', 'abstract_zh': '具身任务规划要求代理在环境中以闭环方式生成可执行动作。随着LLMs在任务分解、规划和泛化能力上的不断提高，当前的具身任务规划方法采用了基于LLMs的方法，但现有的基于LLMs的规划者在三个方面仍受到限制，即固定的规划范式、缺乏动作序列约束以及对错误无感知。在本文中，我们提出了SDA-PLANNER，使其能够实现自适应规划范式，并具备状态依赖性和错误感知机制，从而实现全面的具身任务规划。具体而言，SDA-PLANNER引入了状态依赖图来明确建模动作的前提条件和效果，指导动态修订。为了处理执行错误，它采用了一种基于当前环境状态局部重构受影响部分计划的错误自适应重规划策略，该策略由错误回退、诊断和自适应动作子树生成组成。实验表明，SDA-PLANNER在成功率和目标完成方面始终优于基准方法，特别是在多种错误条件下。', 'title_zh': 'SDA-规划器：感知状态依赖性的自适应体态任务规划器'}
{'arxiv_id': 'arXiv:2509.26308', 'title': 'Anomaly detection for generic failure monitoring in robotic assembly, screwing and manipulation', 'authors': 'Niklas Grambow, Lisa-Marie Fenner, Felipe Kempkes, Philip Hotz, Dingyuan Wan, Jörg Krüger, Kevin Haninger', 'link': 'https://arxiv.org/abs/2509.26308', 'abstract': 'Out-of-distribution states in robot manipulation often lead to unpredictable robot behavior or task failure, limiting success rates and increasing risk of damage. Anomaly detection (AD) can identify deviations from expected patterns in data, which can be used to trigger failsafe behaviors and recovery strategies. Prior work has applied data-driven AD to time series data in specific robotic tasks, but its transferability across control strategies and task types has not been shown. Leveraging time series data, such as force/torque signals, allows to directly capture robot-environment interactions, crucial for manipulation and online failure detection. Their broad availability, high sampling rates, and low dimensionality enable high temporal resolution and efficient processing. As robotic tasks can have widely signal characteristics and requirements, AD methods which can be applied in the same way to a wide range of tasks is needed, ideally with good data efficiency. We examine three industrial robotic tasks, each presenting several anomalies. Test scenarios in robotic cabling, screwing, and sanding are built, and multimodal time series data is gathered. Several autoencoder-based methods are compared, evaluating generalization across tasks and control methods (diffusion policy, position, and impedance control). This allows us to validate the integration of AD in complex tasks involving tighter tolerances and variation from both the robot and its environment. Additionally, we evaluate data efficiency, detection latency, and task characteristics which support robust detection. The results indicate reliable detection with AUROC exceeding 0.93 in failures in the cabling and screwing task, such as incorrect or misaligned parts and obstructed targets. In the polishing task, only severe failures were reliably detected, while more subtle failure types remained undetected.', 'abstract_zh': '机器人操作中的离分布状态往往会导致不可预测的机器人行为或任务失败，限制了成功率并增加了损坏风险。异常检测（AD）可以通过识别数据中的偏差来触发故障安全行为和恢复策略。以往的工作已经在特定的机器人任务中的时间序列数据上应用了数据驱动的AD，但其在控制策略和任务类型之间的可转移性尚未得到验证。通过利用如力/扭矩信号的时间序列数据，可以直接捕获机器人与环境的相互作用，这对于操作和在线故障检测至关重要。这些数据的广泛可获得性、高采样率和低维度使得时间分辨率高且处理效率高。由于机器人任务可以具有广泛的不同信号特征和要求，需要一种可以在多种任务中以相同方式应用且具有良好的数据效率的AD方法，理想情况下效果良好。我们研究了三个工业机器人任务，每个任务都表现出多种异常。我们构建了在电缆装配、螺丝紧固和打磨中的测试场景，并收集了多模态时间序列数据。比较了几种基于自编码器的方法，评估了其在不同任务和控制方法（扩散策略、位置控制和阻抗控制）下的泛化能力。这使我们能够验证AD在涉及更严格公差和来自机器人及其环境的更大变化的复杂任务中的集成。此外，我们还评估了数据效率、检测延迟以及支持稳健检测的任务特性。结果显示，在电缆装配和螺丝紧固任务中的故障中，AUROC超过0.93，能够可靠地检测出诸如错误部件或对齐不良、目标受阻等情况。在抛光任务中，只有严重的故障得到了可靠检测，而更微妙的故障类型仍未被检测到。', 'title_zh': '机器人装配、拧紧和操作中通用故障监测的异常检测'}
{'arxiv_id': 'arXiv:2509.26236', 'title': 'ISyHand: A Dexterous Multi-finger Robot Hand with an Articulated Palm', 'authors': 'Benjamin A. Richardson, Felix Grüninger, Lukas Mack, Joerg Stueckler, Katherine J. Kuchenbecker', 'link': 'https://arxiv.org/abs/2509.26236', 'abstract': "The rapid increase in the development of humanoid robots and customized manufacturing solutions has brought dexterous manipulation to the forefront of modern robotics. Over the past decade, several expensive dexterous hands have come to market, but advances in hardware design, particularly in servo motors and 3D printing, have recently facilitated an explosion of cheaper open-source hands. Most hands are anthropomorphic to allow use of standard human tools, and attempts to increase dexterity often sacrifice anthropomorphism. We introduce the open-source ISyHand (pronounced easy-hand), a highly dexterous, low-cost, easy-to-manufacture, on-joint servo-driven robot hand. Our hand uses off-the-shelf Dynamixel motors, fasteners, and 3D-printed parts, can be assembled within four hours, and has a total material cost of about 1,300 USD. The ISyHands's unique articulated-palm design increases overall dexterity with only a modest sacrifice in anthropomorphism. To demonstrate the utility of the articulated palm, we use reinforcement learning in simulation to train the hand to perform a classical in-hand manipulation task: cube reorientation. Our novel, systematic experiments show that the simulated ISyHand outperforms the two most comparable hands in early training phases, that all three perform similarly well after policy convergence, and that the ISyHand significantly outperforms a fixed-palm version of its own design. Additionally, we deploy a policy trained on cube reorientation on the real hand, demonstrating its ability to perform real-world dexterous manipulation.", 'abstract_zh': '开源易手（ISyHand）：低成本高灵巧性关节驱动机器人手', 'title_zh': 'ISyHand: 一个具有articulated palm的灵巧多指机器人手'}
{'arxiv_id': 'arXiv:2509.26222', 'title': 'Terrain-Awared LiDAR-Inertial Odometry for Legged-Wheel Robots Based on Radial Basis Function Approximation', 'authors': 'Yizhe Liu, Han Zhang', 'link': 'https://arxiv.org/abs/2509.26222', 'abstract': 'An accurate odometry is essential for legged-wheel robots operating in unstructured terrains such as bumpy roads and staircases. Existing methods often suffer from pose drift due to their ignorance of terrain geometry. We propose a terrain-awared LiDAR-Inertial odometry (LIO) framework that approximates the terrain using Radial Basis Functions (RBF) whose centers are adaptively selected and weights are recursively updated. The resulting smooth terrain manifold enables ``soft constraints" that regularize the odometry optimization and mitigates the $z$-axis pose drift under abrupt elevation changes during robot\'s maneuver. To ensure the LIO\'s real-time performance, we further evaluate the RBF-related terms and calculate the inverse of the sparse kernel matrix with GPU parallelization. Experiments on unstructured terrains demonstrate that our method achieves higher localization accuracy than the state-of-the-art baselines, especially in the scenarios that have continuous height changes or sparse features when abrupt height changes occur.', 'abstract_zh': '一种适应地形的LiDAR-惯性里程计框架在不规则地形上的应用：该框架通过径向基函数近似地形，并通过自适应选择中心和递归更新权重实现地形aware的里程计优化，从而在高度突变时减少姿态漂移，实现实时性能并提高定位精度。', 'title_zh': '基于径向基函数近似的地形aware激光雷达-惯性里程计算法及其在足轮机器人上的应用'}
{'arxiv_id': 'arXiv:2509.26106', 'title': 'Autonomous Multi-Robot Infrastructure for AI-Enabled Healthcare Delivery and Diagnostics', 'authors': 'Nakhul Kalaivanan, Senthil Arumugam Muthukumaraswamy, Girish Balasubramanian', 'link': 'https://arxiv.org/abs/2509.26106', 'abstract': 'This research presents a multi-robot system for inpatient care, designed using swarm intelligence principles and incorporating wearable health sensors, RF-based communication, and AI-driven decision support. Within a simulated hospital environment, the system adopts a leader-follower swarm configuration to perform patient monitoring, medicine delivery, and emergency assistance. Due to ethical constraints, live patient trials were not conducted; instead, validation was carried out through controlled self-testing with wearable sensors. The Leader Robot acquires key physiological parameters, including temperature, SpO2, heart rate, and fall detection, and coordinates other robots when required. The Assistant Robot patrols corridors for medicine delivery, while a robotic arm provides direct drug administration. The swarm-inspired leader-follower strategy enhanced communication reliability and ensured continuous monitoring, including automated email alerts to healthcare staff. The system hardware was implemented using Arduino, Raspberry Pi, NRF24L01 RF modules, and a HuskyLens AI camera. Experimental evaluation showed an overall sensor accuracy above 94%, a 92% task-level success rate, and a 96% communication reliability rate, demonstrating system robustness. Furthermore, the AI-enabled decision support was able to provide early warnings of abnormal health conditions, highlighting the potential of the system as a cost-effective solution for hospital automation and patient safety.', 'abstract_zh': '基于 swarm intelligence 的穿戴健康传感器多机器人病房护理系统及其实验评估', 'title_zh': '自主多机器人基础设施及其在AI赋能的健康护理交付与诊断中的应用'}
{'arxiv_id': 'arXiv:2509.26082', 'title': 'Evolutionary Continuous Adaptive RL-Powered Co-Design for Humanoid Chin-Up Performance', 'authors': 'Tianyi Jin, Melya Boukheddimi, Rohit Kumar, Gabriele Fadini, Frank Kirchner', 'link': 'https://arxiv.org/abs/2509.26082', 'abstract': 'Humanoid robots have seen significant advancements in both design and control, with a growing emphasis on integrating these aspects to enhance overall performance. Traditionally, robot design has followed a sequential process, where control algorithms are developed after the hardware is finalized. However, this can be myopic and prevent robots to fully exploit their hardware capabilities. Recent approaches advocate for co-design, optimizing both design and control in parallel to maximize robotic capabilities. This paper presents the Evolutionary Continuous Adaptive RL-based Co-Design (EA-CoRL) framework, which combines reinforcement learning (RL) with evolutionary strategies to enable continuous adaptation of the control policy to the hardware. EA-CoRL comprises two key components: Design Evolution, which explores the hardware choices using an evolutionary algorithm to identify efficient configurations, and Policy Continuous Adaptation, which fine-tunes a task-specific control policy across evolving designs to maximize performance rewards. We evaluate EA-CoRL by co-designing the actuators (gear ratios) and control policy of the RH5 humanoid for a highly dynamic chin-up task, previously unfeasible due to actuator limitations. Comparative results against state-of-the-art RL-based co-design methods show that EA-CoRL achieves higher fitness score and broader design space exploration, highlighting the critical role of continuous policy adaptation in robot co-design.', 'abstract_zh': 'humanoid机器人在设计和控制方面的进展显著，对这两者的整合越来越受到重视，以提升整体性能。传统上，机器人设计遵循一个顺序过程，即在硬件最终确定后才开发控制算法。然而，这种方法可能过于狭隘，限制了机器人充分利用其硬件能力。近年来，协同设计方法倡导并行优化设计和控制，以最大化机器人能力。本文提出了一种结合强化学习（RL）与进化策略的进化连续自适应RL基于协同设计（EA-CoRL）框架，该框架能够使控制策略连续适应硬件。EA-CoRL包括两个关键组件：设计进化，利用进化算法探索硬件选择以识别高效配置，以及策略连续适应，针对进化中的设计微调特定任务的控制策略以最大化性能奖励。我们通过协同设计RH5人形机器人的执行器（传动比）和控制策略，对一个此前因执行器限制而不可行的高度动态引体向上任务进行了评估。与最先进的基于RL的协同设计方法相比，EA-CoRL显示出更高的适应度评分和更广泛的硬件配置探索范围，突出了连续策略适应在机器人协同设计中的重要作用。', 'title_zh': '基于进化连续自适应RL的类人引体向上性能协同设计'}
{'arxiv_id': 'arXiv:2509.25986', 'title': "Emotionally Expressive Robots: Implications for Children's Behavior toward Robot", 'authors': 'Elisabetta Zibetti, Sureya Waheed Palmer, Rebecca Stower, Salvatore M Anzalone', 'link': 'https://arxiv.org/abs/2509.25986', 'abstract': "The growing development of robots with artificial emotional expressiveness raises important questions about their persuasive potential in children's behavior. While research highlights the pragmatic value of emotional expressiveness in human social communication, the extent to which robotic expressiveness can or should influence empathic responses in children is grounds for debate. In a pilot study with 22 children (aged 7-11) we begin to explore the ways in which different levels of embodied expressiveness (body only, face only, body and face) of two basic emotions (happiness and sadness) displayed by an anthropomorphic robot (QTRobot) might modify children's behavior in a child-robot cooperative turn-taking game. We observed that children aligned their behavior to the robot's inferred emotional state. However, higher levels of expressiveness did not result in increased alignment. The preliminary results reported here provide a starting point for reflecting on robotic expressiveness and its role in shaping children's social-emotional behavior toward robots as social peers in the near future.", 'abstract_zh': '随着具有人工情感表达能力的机器人不断发展，它们在儿童行为中潜在的说服力引起了重要问题。虽然研究强调了情感表达在人类社会交流中的实用价值，但机器人的情感表达在多大程度上或是否应该影响儿童的共情反应尚存争议。在一项针对22名儿童（7-11岁）的试点研究中，我们开始探讨不同水平的身体与面部表达（仅身体、仅面部、身体和面部）以及两种基本情绪（快乐和悲伤）由类人机器人（QTRobot）展示时，如何可能改变儿童在儿童-机器人合作轮流游戏中的行为。我们观察到，儿童会调整自己的行为以匹配机器人推断的情感状态。然而，更高的表达水平并未导致更强的匹配度。这里报告的初步结果为反思机器人的表达性和其对未来儿童与机器人作为社会同伴的社会情感行为塑造作用提供了起点。', 'title_zh': '情感表达型机器人：对儿童对机器人行为的影响'}
{'arxiv_id': 'arXiv:2509.25951', 'title': 'Towards Intuitive Human-Robot Interaction through Embodied Gesture-Driven Control with Woven Tactile Skins', 'authors': 'ChunPing Lam, Xiangjia Chen, Chenming Wu, Hao Chen, Binzhi Sun, Guoxin Fang, Charlie C.L. Wang, Chengkai Dai, Yeung Yam', 'link': 'https://arxiv.org/abs/2509.25951', 'abstract': 'This paper presents a novel human-robot interaction (HRI) framework that enables intuitive gesture-driven control through a capacitance-based woven tactile skin. Unlike conventional interfaces that rely on panels or handheld devices, the woven tactile skin integrates seamlessly with curved robot surfaces, enabling embodied interaction and narrowing the gap between human intent and robot response. Its woven design combines fabric-like flexibility with structural stability and dense multi-channel sensing through the interlaced conductive threads. Building on this capability, we define a gesture-action mapping of 14 single- and multi-touch gestures that cover representative robot commands, including task-space motion and auxiliary functions. A lightweight convolution-transformer model designed for gesture recognition in real time achieves an accuracy of near-100%, outperforming prior baseline approaches. Experiments on robot arm tasks, including pick-and-place and pouring, demonstrate that our system reduces task completion time by up to 57% compared with keyboard panels and teach pendants. Overall, our proposed framework demonstrates a practical pathway toward more natural and efficient embodied HRI.', 'abstract_zh': '基于电容式编织触觉皮肤的新型人机交互框架：直观手势驱动控制及应用', 'title_zh': '基于编织触感皮肤的体现手势驱动控制以实现直观的人机交互'}
{'arxiv_id': 'arXiv:2509.25852', 'title': 'Reinforced Embodied Planning with Verifiable Reward for Real-World Robotic Manipulation', 'authors': 'Zitong Bo, Yue Hu, Jinming Ma, Mingliang Zhou, Junhui Yin, Yachen Kang, Yuqi Liu, Tong Wu, Diyun Xiang, Hao Chen', 'link': 'https://arxiv.org/abs/2509.25852', 'abstract': 'Enabling robots to execute long-horizon manipulation tasks from free-form language instructions remains a fundamental challenge in embodied AI. While vision-language models (VLMs) have shown promise as high-level planners, their deployment in the real world is hindered by two gaps: (i) the scarcity of large-scale, sequential manipulation data that couples natural language with multi-step action plans, and (ii) the absence of dense, interpretable rewards for fine-tuning VLMs on planning objectives. To address these issues, we propose REVER, a framework that empowers VLMs to generate and validate long-horizon manipulation plans from natural language instructions in real-world scenarios. Under REVER we train and release RoboFarseer, a VLM incentivized to emit chain-of-thought that perform temporal and spatial reasoning, ensuring physically plausible and logically coherent plans. To obtain training data, we leverage the Universal Manipulation Interface framework to capture hardware-agnostic demonstrations of atomic skills. An automated annotation engine converts each demonstration into vision-instruction-plan triplet. We introduce a verifiable reward that scores the generated plan by its ordered bipartite matching overlap with the ground-truth skill sequence. At run time, the fine-tuned VLM functions both as a planner and as a monitor, verifying step-wise completion. RoboFarseer matches or exceeds the performance of proprietary models that are orders of magnitude larger, while on open-ended planning it surpasses the best baseline by more than 40%. In real-world, long-horizon tasks, the complete system boosts overall success by roughly 60% compared with the same low-level controller without the planner. We will open-source both the dataset and the trained model upon publication.', 'abstract_zh': '使机器人能够从自然语言指令执行长期 horizon 的 manipulation 任务仍然是嵌入式 AI 中的一项基本挑战。尽管视觉-语言模型 (VLMs) 在高阶规划中显示出了潜力，但在实际部署中受到了两个差距的阻碍：（i）缺乏将自然语言与多步行动计划耦合的大规模序列 manipulation 数据，（ii）缺乏密集的、可解释的奖励来对 VLMs 进行规划目标的微调。为了解决这些问题，我们提出了 REVER 框架，使 VLMs 能够从自然语言指令生成和验证实际场景中的长期 horizon manipulation 计划。在 REVER 框架下，我们训练并发布了 RoboFarseer，一种激励 VLMs 生成执行时间和空间推理的链式思考的 VLM，确保其生成的计划具有物理合理性和逻辑一致性。为获得训练数据，我们利用 Universal Manipulation Interface 框架来捕捉原子技能的硬件无关的演示。自动化注释引擎将每个演示转换为视觉-指令-计划三元组。我们引入了一个可验证的奖励，通过其有序的二分匹配重叠来评分生成的计划与真实技能序列的匹配度。在运行时，微调后的 VLM 同时作为规划者和监控器，验证每一步的完成情况。RoboFarseer 在性能上匹配或超越了规模大几个数量级的专有模型，而在开放性规划中，它超出了最佳基线 40% 以上。在实际场景中执行长期 horizon 任务时，完整系统将总体成功率提升约 60%，相较于相同低级控制器而无规划器的情况。我们在发布时会开源该数据集和训练模型。', 'title_zh': '强化体态规划与可验证奖励的世界机器人操作规划'}
{'arxiv_id': 'arXiv:2509.25822', 'title': 'Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies', 'authors': 'Jing Wang, Weiting Peng, Jing Tang, Zeyu Gong, Xihua Wang, Bo Tao, Li Cheng', 'link': 'https://arxiv.org/abs/2509.25822', 'abstract': "Existing imitation learning methods decouple perception and action, which overlooks the causal reciprocity between sensory representations and action execution that humans naturally leverage for adaptive behaviors. To bridge this gap, we introduce Action--Guided Diffusion Policy (DP--AG), a unified representation learning that explicitly models a dynamic interplay between perception and action through probabilistic latent dynamics. DP--AG encodes latent observations into a Gaussian posterior via variational inference and evolves them using an action-guided SDE, where the Vector-Jacobian Product (VJP) of the diffusion policy's noise predictions serves as a structured stochastic force driving latent updates. To promote bidirectional learning between perception and action, we introduce a cycle--consistent contrastive loss that organizes the gradient flow of the noise predictor into a coherent perception--action loop, enforcing mutually consistent transitions in both latent updates and action refinements. Theoretically, we derive a variational lower bound for the action-guided SDE, and prove that the contrastive objective enhances continuity in both latent and action trajectories. Empirically, DP--AG significantly outperforms state--of--the--art methods across simulation benchmarks and real-world UR5 manipulation tasks. As a result, our DP--AG offers a promising step toward bridging biological adaptability and artificial policy learning.", 'abstract_zh': '行动导向扩散策略：通过概率潜动态建模感知与行动的动态互动', 'title_zh': '行动以感知，感知以行动：扩散驱动的感知-行动交互对自适应策略的影响'}
{'arxiv_id': 'arXiv:2509.25756', 'title': 'SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling', 'authors': "Yixian Zhang, Shu'ang Yu, Tonghe Zhang, Mo Guang, Haojia Hui, Kaiwen Long, Yu Wang, Chao Yu, Wenbo Ding", 'link': 'https://arxiv.org/abs/2509.25756', 'abstract': 'Training expressive flow-based policies with off-policy reinforcement learning is notoriously unstable due to gradient pathologies in the multi-step action sampling process. We trace this instability to a fundamental connection: the flow rollout is algebraically equivalent to a residual recurrent computation, making it susceptible to the same vanishing and exploding gradients as RNNs. To address this, we reparameterize the velocity network using principles from modern sequential models, introducing two stable architectures: Flow-G, which incorporates a gated velocity, and Flow-T, which utilizes a decoded velocity. We then develop a practical SAC-based algorithm, enabled by a noise-augmented rollout, that facilitates direct end-to-end training of these policies. Our approach supports both from-scratch and offline-to-online learning and achieves state-of-the-art performance on continuous control and robotic manipulation benchmarks, eliminating the need for common workarounds like policy distillation or surrogate objectives.', 'abstract_zh': '使用离策学习训练表达性强的流动基策略因多步动作采样的梯度病理问题而 notoriously 不稳定。我们将这种不稳定性追溯到一个基本的联系：流动展开在代数上等价于残差递归计算，使其容易受到与递归神经网络相同的梯度消失和梯度爆炸问题。为了解决这一问题，我们借用现代序列模型的原则重新参数化速度网络，引入了两种稳定的架构：Flow-G，其包含门控速度，Flow-T，其利用解码速度。然后，我们开发了一种实用的基于SAC的算法，该算法通过噪声增强的展开过程启用直接端到端训练这些策略。我们的方法支持从头学习和离线到在线学习，并在连续控制和机器人操作基准测试中实现了最先进的性能，消除了如策略蒸馏或替代目标等常见工作-around 解决方法的需要。', 'title_zh': 'SAC流：基于速度重参数化序贯建模的样本高效流基于策略的强化学习'}
{'arxiv_id': 'arXiv:2509.25747', 'title': 'Best of Sim and Real: Decoupled Visuomotor Manipulation via Learning Control in Simulation and Perception in Real', 'authors': 'Jialei Huang, Zhaoheng Yin, Yingdong Hu, Shuo Wang, Xingyu Lin, Yang Gao', 'link': 'https://arxiv.org/abs/2509.25747', 'abstract': 'Sim-to-real transfer remains a fundamental challenge in robot manipulation due to the entanglement of perception and control in end-to-end learning. We present a decoupled framework that learns each component where it is most reliable: control policies are trained in simulation with privileged state to master spatial layouts and manipulation dynamics, while perception is adapted only at deployment to bridge real observations to the frozen control policy. Our key insight is that control strategies and action patterns are universal across environments and can be learned in simulation through systematic randomization, while perception is inherently domain-specific and must be learned where visual observations are authentic. Unlike existing end-to-end approaches that require extensive real-world data, our method achieves strong performance with only 10-20 real demonstrations by reducing the complex sim-to-real problem to a structured perception alignment task. We validate our approach on tabletop manipulation tasks, demonstrating superior data efficiency and out-of-distribution generalization compared to end-to-end baselines. The learned policies successfully handle object positions and scales beyond the training distribution, confirming that decoupling perception from control fundamentally improves sim-to-real transfer.', 'abstract_zh': '基于分解框架的从仿真到现实的机器人 manipulation 转移', 'title_zh': '最佳的模拟与现实：通过模拟学习控制与现实感知的解耦视觉与运动 manipulation'}
{'arxiv_id': 'arXiv:2509.25718', 'title': 'VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning', 'authors': 'Si-Cheng Wang, Tian-Yu Xiang, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Ao-Qun Jin, Zeng-Guang Hou', 'link': 'https://arxiv.org/abs/2509.25718', 'abstract': 'Reinforcement learning (RL) is a promising avenue for post-training vision-language-action (VLA) models, but practical deployment is hindered by sparse rewards and unstable training. This work mitigates these challenges by introducing an action chunk based on proximal policy optimization (PPO) with behavior cloning using self-collected demonstrations. Aggregating consecutive actions into chunks improves the temporal consistency of the policy and the density of informative feedback. In addition, an auxiliary behavior cloning loss is applied with a dynamically updated demonstration buffer that continually collects high-quality task trials during training. The relative weight between the action-chunked PPO objective and the self behavior clone auxiliary loss is adapted online to stabilize the post-training process. Experiments on the MetaWorld benchmark indicate improved performance over supervised fine-tuning, achieving a high success rate (0.93) and few steps to success (42.17). These results demonstrate the viability of RL for VLA post-training and help lay the groundwork for downstream VLA applications.', 'abstract_zh': '基于 proximal policy optimization 的动作块和行为克隆在后训练视觉-语言-行动模型中的应用', 'title_zh': '基于动作分块PPO和自我行为克隆的VLA模型后训练'}
{'arxiv_id': 'arXiv:2509.25687', 'title': 'OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation', 'authors': 'Xinda Xue, Junjun Hu, Minghua Luo, Xie Shichao, Jintao Chen, Zixun Xie, Quan Kuichen, Guo Wei, Mu Xu, Zedong Chu', 'link': 'https://arxiv.org/abs/2509.25687', 'abstract': "Embodied navigation presents a core challenge for intelligent robots, requiring the comprehension of visual environments, natural language instructions, and autonomous exploration. Existing models often fall short in offering a unified solution across diverse navigation paradigms, resulting in low success rates and limited generalization. We introduce OmniNav, a unified framework addressing instruct-goal, object-goal, point-goal navigation, and frontier-based exploration within a single architecture. Our approach features a lightweight, low-latency policy that accurately predicts continuous-space waypoints (coordinates and orientations). This policy surpasses action-chunk methods in precision and supports real-world deployment at control frequencies up to 5 Hz. Architecturally, OmniNav employs a fast-slow system design: a fast module generates waypoints using short-horizon visual context and subtasks, while a slow module performs deliberative planning with long-horizon observations and candidate frontiers to select subsequent subgoals and subtasks. This collaboration enhances path efficiency and maintains trajectory coherence, particularly in exploration and memory-intensive scenarios. Crucially, we identify that the primary bottleneck isn't merely navigation policy learning, but a robust understanding of general instructions and objects. To boost generalization, OmniNav integrates large-scale, general-purpose training datasets, including those for image captioning and visual recognition, into a joint multi-task regimen. This significantly improves success rates and robustness. Extensive experiments confirm OmniNav's state-of-the-art performance across various navigation benchmarks, with real-world deployment further validating its efficacy. OmniNav provides practical insights for embodied navigation, charting a scalable path towards versatile, highly generalizable robotic intelligence.", 'abstract_zh': '全方位导航为智能机器人提出了核心挑战，要求其理解视觉环境、自然语言指令并实现自主探索。现有的模型往往在提供跨异构导航范式的统一解决方案方面存在不足，导致成功率低且泛化能力有限。我们提出了一体化框架OmniNav，该框架在单一架构中解决了指令目标、物体目标、点目标导航以及基于前沿的探索问题。我们的方法采用轻量级、低延迟策略，能够准确预测连续空间航点（坐标和方向）。该策略的精度超过了基于动作片段的方法，并支持在高达5 Hz的控制频率下进行实际部署。架构上，OmniNav 采用快慢系统设计：快速模块使用短时视觉上下文和子任务生成航点，而缓慢模块则利用长时间观察和候选前沿进行详尽规划，以选择后续的子目标和子任务。这种协作增强了路径效率并保持了轨迹一致性，尤其是在探索和记忆密集型场景中。最关键的是，我们发现主要瓶颈不仅在于导航策略学习，还在于对通用指令和物体的稳健理解。为了提高泛化能力，OmniNav 将大规模、通用的数据集，包括图像字幕和视觉识别数据集，整合到联合多任务训练中，这显著提高了成功率和鲁棒性。广泛的实验结果表明，OmniNav 在各种导航基准测试中表现出目前最先进的性能，在实际部署中进一步验证了其有效性。OmniNav 为实体导航提供了实用洞察，描绘了一条实现多功能、高度泛化的机器人智能的可扩展路径。', 'title_zh': 'OmniNav：综合框架用于前瞻性探索和视觉语言导航'}
{'arxiv_id': 'arXiv:2509.25443', 'title': 'CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation', 'authors': 'Zewen He, Chenyuan Chen, Dilshod Azizov, Yoshihiko Nakamura', 'link': 'https://arxiv.org/abs/2509.25443', 'abstract': 'Humanoid whole-body locomotion control is a critical approach for humanoid robots to leverage their inherent advantages. Learning-based control methods derived from retargeted human motion data provide an effective means of addressing this issue. However, because most current human datasets lack measured force data, and learning-based robot control is largely position-based, achieving appropriate compliance during interaction with real environments remains challenging. This paper presents Compliant Task Pipeline (CoTaP): a pipeline that leverages compliance information in the learning-based structure of humanoid robots. A two-stage dual-agent reinforcement learning framework combined with model-based compliance control for humanoid robots is proposed. In the training process, first a base policy with a position-based controller is trained; then in the distillation, the upper-body policy is combined with model-based compliance control, and the lower-body agent is guided by the base policy. In the upper-body control, adjustable task-space compliance can be specified and integrated with other controllers through compliance modulation on the symmetric positive definite (SPD) manifold, ensuring system stability. We validated the feasibility of the proposed strategy in simulation, primarily comparing the responses to external disturbances under different compliance settings.', 'abstract_zh': '基于顺应性的类人全身运动控制管道（Compliant Task Pipeline）：一种结合模型驱动顺应控制的双Agent强化学习框架', 'title_zh': 'CoTaP: 合成型任务管道及其控制器的顺应性调节强化学习'}
{'arxiv_id': 'arXiv:2509.25402', 'title': 'Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning Models', 'authors': 'Hanlan Yang, Itamar Mishani, Luca Pivetti, Zachary Kingston, Maxim Likhachev', 'link': 'https://arxiv.org/abs/2509.25402', 'abstract': 'Actor-Critic models are a class of model-free deep reinforcement learning (RL) algorithms that have demonstrated effectiveness across various robot learning tasks. While considerable research has focused on improving training stability and data sampling efficiency, most deployment strategies have remained relatively simplistic, typically relying on direct actor policy rollouts. In contrast, we propose \\pachs{} (\\textit{P}arallel \\textit{A}ctor-\\textit{C}ritic \\textit{H}euristic \\textit{S}earch), an efficient parallel best-first search algorithm for inference that leverages both components of the actor-critic architecture: the actor network generates actions, while the critic network provides cost-to-go estimates to guide the search. Two levels of parallelism are employed within the search -- actions and cost-to-go estimates are generated in batches by the actor and critic networks respectively, and graph expansion is distributed across multiple threads. We demonstrate the effectiveness of our approach in robotic manipulation tasks, including collision-free motion planning and contact-rich interactions such as non-prehensile pushing. Visit this http URL for demonstrations and examples.', 'abstract_zh': 'Actor-Critic模型是一类模型自由的深度强化学习（RL）算法，已在各种机器人学习任务中展示了有效性。尽管在提高训练稳定性和数据采样效率方面进行了大量研究，但大多数部署策略仍相对简单，通常依赖于直接的actor策略采样。相反，我们提出了一种高效的并行最佳优先搜索算法\\pachs{}（Parallel Actor-Critic Heuristic Search），该算法利用了actor-critic架构的两个组成部分：actor网络生成动作，而critic网络提供成本到终点估计以引导搜索。搜索中采用了两个层次的并行性——动作和成本到终点估计分别由actor和critic网络以批次形式生成，图扩展分布到多个线程中。我们通过机器人操作任务展示了该方法的有效性，包括无碰撞运动规划和接触丰富的交互，如非抓握推拉。请访问此网址以获取演示和示例。', 'title_zh': '并行启发式搜索作为演员-评论家强化学习模型的推理'}
{'arxiv_id': 'arXiv:2509.25358', 'title': 'SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation', 'authors': 'Qianzhong Chen, Justin Yu, Mac Schwager, Pieter Abbeel, Fred Shentu, Philipp Wu', 'link': 'https://arxiv.org/abs/2509.25358', 'abstract': 'Large-scale robot learning has recently shown promise for enabling robots to perform complex tasks by integrating perception, control, and language understanding. Yet, it struggles with long-horizon, contact-rich manipulation such as deformable object handling, where demonstration quality is inconsistent. Reward modeling offers a natural solution: by providing grounded progress signals, it transforms noisy demonstrations into stable supervision that generalizes across diverse trajectories. We introduce a stage-aware, video-based reward modeling framework that jointly predicts high-level task stages and fine-grained progress. Reward labels are automatically derived from natural language subtask annotations, ensuring consistent progress estimation across variable-length demonstrations. This design overcomes frame-index labeling, which fails in variable-duration tasks like folding a T-shirt. Our reward model demonstrates robustness to variability, generalization to out-of-distribution settings, and strong utility for policy training. Building on it, we propose Reward-Aligned Behavior Cloning (RA-BC), which filters high-quality data and reweights samples by reward. Experiments show the reward model alone outperforms baselines on validation and real robot rollouts. Integrated into RA-BC, our approach achieves 83\\% success on folding T-shirts from the flattened state and 67\\% from the crumpled state -- far surpassing vanilla behavior cloning, which attains only 8\\% and 0\\% success. Overall, our results highlight reward modeling as a key enabler for scalable, annotation-efficient, and robust imitation learning in long-horizon manipulation.', 'abstract_zh': '大规模机器人学习在实现具有感知、控制和语言理解能力的复杂任务方面 Recent Progress：一种阶段意识的视频基奖励建模框架及其在长时 horizon 操作中的应用', 'title_zh': '面向长期 horizon 机器人操作的阶段aware奖励建模'}
{'arxiv_id': 'arXiv:2509.25200', 'title': 'When and How to Express Empathy in Human-Robot Interaction Scenarios', 'authors': 'Christian Arzate Cruz, Edwin C. Montiel-Vazquez, Chikara Maeda, Randy Gomez', 'link': 'https://arxiv.org/abs/2509.25200', 'abstract': 'Incorporating empathetic behavior into robots can improve their social effectiveness and interaction quality. In this paper, we present whEE (when and how to express empathy), a framework that enables social robots to detect when empathy is needed and generate appropriate responses. Using large language models, whEE identifies key behavioral empathy cues in human interactions. We evaluate it in human-robot interaction scenarios with our social robot, Haru. Results show that whEE effectively identifies and responds to empathy cues, providing valuable insights for designing social robots capable of adaptively modulating their empathy levels across various interaction contexts.', 'abstract_zh': '将共情行为融入机器人可以提高其社会效果和交互质量。本文提出了一种名为whEE（何时以及如何表达共情）的框架，使社会机器人能够检测出何时需要表达共情，并生成合适的响应。通过使用大规模语言模型，whEE识别出人类互动中的关键共情行为线索。我们在与我们的社会机器人Haru的人机交互场景中对其进行评估。结果显示，whEE有效识别并响应共情行为线索，为设计能够在各种交互情境中适配性调节共情水平的社会机器人提供了宝贵见解。', 'title_zh': '在人机交互场景中何时及如何表达同理心'}
{'arxiv_id': 'arXiv:2509.26627', 'title': 'TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance', 'authors': 'Yuyang Liu, Chuan Wen, Yihang Hu, Dinesh Jayaraman, Yang Gao', 'link': 'https://arxiv.org/abs/2509.26627', 'abstract': 'Designing dense rewards is crucial for reinforcement learning (RL), yet in robotics it often demands extensive manual effort and lacks scalability. One promising solution is to view task progress as a dense reward signal, as it quantifies the degree to which actions advance the system toward task completion over time. We present TimeRewarder, a simple yet effective reward learning method that derives progress estimation signals from passive videos, including robot demonstrations and human videos, by modeling temporal distances between frame pairs. We then demonstrate how TimeRewarder can supply step-wise proxy rewards to guide reinforcement learning. In our comprehensive experiments on ten challenging Meta-World tasks, we show that TimeRewarder dramatically improves RL for sparse-reward tasks, achieving nearly perfect success in 9/10 tasks with only 200,000 interactions per task with the environment. This approach outperformed previous methods and even the manually designed environment dense reward on both the final success rate and sample efficiency. Moreover, we show that TimeRewarder pretraining can exploit real-world human videos, highlighting its potential as a scalable approach path to rich reward signals from diverse video sources.', 'abstract_zh': '设计稠密奖励对于强化学习至关重要，但在机器人领域往往需要大量的手动努力且缺乏可扩展性。一种有前景的解决方案是将任务进度视为稠密奖励信号，因为它量化了动作随时间推进系统向任务完成程度。我们提出了TimeRewarder，一种简单有效的奖励学习方法，通过建模帧对之间的时间距离从被动视频中提取进度估计信号，包括机器人演示和人类视频。然后我们展示了TimeRewarder如何提供逐步的代理奖励来引导强化学习。在对十个具有挑战性的Meta-World任务进行全面实验中，我们表明TimeRewarder显著提高了稀疏奖励任务的强化学习性能，仅需每任务20万次与环境交互，便在9/10任务中实现了近乎完美的成功率。该方法在最终成功率和样本效率上均优于以往方法，甚至优于人工设计的环境密集奖励。此外，我们展示了TimeRewarder预训练可以利用现实世界的视频，突显了其从多种视频源生成丰富奖励信号的潜在可扩展途径。', 'title_zh': 'TimeRewarder：通过帧级时间距离从被动视频学习密集奖励'}
{'arxiv_id': 'arXiv:2509.26536', 'title': 'OceanGym: A Benchmark Environment for Underwater Embodied Agents', 'authors': 'Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen', 'link': 'https://arxiv.org/abs/2509.26536', 'abstract': "We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at this https URL.", 'abstract_zh': '我们介绍OceanGym，这是首个全面的海洋水下具身智能体基准，旨在推动在最具挑战性的现实环境之一中的人工智能发展。不同于陆地或航空领域，水下环境提出了极端的感知和决策挑战，包括低能见度和动态洋流，使得有效智能体部署异常困难。OceanGym涵盖了八个现实的任务域，并由多模态大语言模型（MLLM）统一驱动的智能体框架，该框架整合了感知、记忆和序列决策。智能体需要在这些苛刻的条件下理解光学和声纳数据，自主探索复杂环境，并实现长期目标。广泛的实验揭示了基于最先进的MLLM驱动智能体与人类专家之间的巨大差距，强调了在海洋水下环境中感知、规划和适应性的持续困难。通过提供一个高度逼真、严格设计的平台，OceanGym为开发稳健的具身人工智能及其向实际自主水下海洋车辆的转移建立了测试床，标志着朝着能够操作地球最后一片未开发前沿之一的智能代理的重要一步。代码和数据可在以下网址获取。', 'title_zh': 'OceanGym: 一种水下实体代理基准环境'}
{'arxiv_id': 'arXiv:2509.26002', 'title': 'Towards Human Engagement with Realistic AI Combat Pilots', 'authors': 'Ardian Selmonaj, Giacomo Del Rio, Adrian Schneider, Alessandro Antonucci', 'link': 'https://arxiv.org/abs/2509.26002', 'abstract': 'We present a system that enables real-time interaction between human users and agents trained to control fighter jets in simulated 3D air combat scenarios. The agents are trained in a dedicated environment using Multi-Agent Reinforcement Learning. A communication link is developed to allow seamless deployment of trained agents into VR-Forces, a widely used defense simulation tool for realistic tactical scenarios. This integration allows mixed simulations where human-controlled entities engage with intelligent agents exhibiting distinct combat behaviors. Our interaction model creates new opportunities for human-agent teaming, immersive training, and the exploration of innovative tactics in defense contexts.', 'abstract_zh': '我们提出了一种系统，使得人类用户能够实时与在模拟3D空战场景中训练用于控制战斗机的智能体进行交互。这些智能体是在专用环境中使用多智能体强化学习训练的。我们开发了一种通信链接，以便无缝地将训练好的智能体部署到VR-Forces中，这是一个广泛使用的国防仿真工具，用于实现逼真的战术场景。这种集成允许人类控制的实体与表现出不同作战行为的智能体进行混合模拟。我们的交互模型为人类-智能体协同作战、沉浸式训练以及在国防背景下探索创新战术提供了新的机会。', 'title_zh': '面向真实主义AI战斗飞行员的人机互动研究'}
{'arxiv_id': 'arXiv:2509.25727', 'title': 'Boundary-to-Region Supervision for Offline Safe Reinforcement Learning', 'authors': 'Huikang Su, Dengyun Peng, Zifeng Zhuang, YuHan Liu, Qiguang Chen, Donglin Wang, Qinghe Liu', 'link': 'https://arxiv.org/abs/2509.25727', 'abstract': 'Offline safe reinforcement learning aims to learn policies that satisfy predefined safety constraints from static datasets. Existing sequence-model-based methods condition action generation on symmetric input tokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry: return-to-go (RTG) serves as a flexible performance target, while cost-to-go (CTG) should represent a rigid safety boundary. This symmetric conditioning leads to unreliable constraint satisfaction, especially when encountering out-of-distribution cost trajectories. To address this, we propose Boundary-to-Region (B2R), a framework that enables asymmetric conditioning through cost signal realignment . B2R redefines CTG as a boundary constraint under a fixed safety budget, unifying the cost distribution of all feasible trajectories while preserving reward structures. Combined with rotary positional embeddings , it enhances exploration within the safe region. Experimental results show that B2R satisfies safety constraints in 35 out of 38 safety-critical tasks while achieving superior reward performance over baseline methods. This work highlights the limitations of symmetric token conditioning and establishes a new theoretical and practical approach for applying sequence models to safe RL. Our code is available at this https URL.', 'abstract_zh': 'Offline Safe Reinforcement Learning via Asymmetric Conditioning with Boundary-to-Region Framework', 'title_zh': '离线安全强化学习中的边界到区域监督'}
{'arxiv_id': 'arXiv:2509.25518', 'title': 'World Model for AI Autonomous Navigation in Mechanical Thrombectomy', 'authors': 'Harry Robertshaw, Han-Ru Wu, Alejandro Granados, Thomas C Booth', 'link': 'https://arxiv.org/abs/2509.25518', 'abstract': "Autonomous navigation for mechanical thrombectomy (MT) remains a critical challenge due to the complexity of vascular anatomy and the need for precise, real-time decision-making. Reinforcement learning (RL)-based approaches have demonstrated potential in automating endovascular navigation, but current methods often struggle with generalization across multiple patient vasculatures and long-horizon tasks. We propose a world model for autonomous endovascular navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL agent across multiple endovascular navigation tasks in ten real patient vasculatures, comparing performance against the state-of-the-art Soft Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly outperforms SAC in multi-task learning, achieving a 65% mean success rate compared to SAC's 37%, with notable improvements in path ratio. TD-MPC2 exhibited increased procedure times, suggesting a trade-off between success rate and execution speed. These findings highlight the potential of world models for improving autonomous endovascular navigation and lay the foundation for future research in generalizable AI-driven robotic interventions.", 'abstract_zh': '自主机械取栓的自主导航仍然是一个关键挑战，由于血管解剖结构的复杂性和需要进行精确的实时决策。基于强化学习的方法在自动化血管内导航中显示出潜力，但目前的方法在跨多个患者血管结构的泛化能力和长期任务处理上常常表现不足。我们提出了一种使用TD-MPC2（基于模型的RL算法）的世界模型来进行自主血管内导航。我们在十个真实患者的血管结构中训练了一个单一的RL代理，并与最先进的Soft Actor-Critic（SAC）方法进行了性能比较。结果表明，TD-MPC2在多任务学习中显著优于SAC，成功率达到65%，而SAC仅为37%，并且在路径比率上有明显的提升。TD-MPC2的程序时间有所增加，表明了成功率和执行速度之间的权衡。这些发现突显了世界模型在提升自主血管内导航中的潜在价值，并为未来通用AI驱动的机器人干预研究奠定了基础。', 'title_zh': 'AI自主导航的机械取栓世界模型'}
{'arxiv_id': 'arXiv:2509.25482', 'title': 'Message passing-based inference in an autoregressive active inference agent', 'authors': 'Wouter M. Kouw, Tim N. Nisslbeck, Wouter L.N. Nuijten', 'link': 'https://arxiv.org/abs/2509.25482', 'abstract': "We present the design of an autoregressive active inference agent in the form of message passing on a factor graph. Expected free energy is derived and distributed across a planning graph. The proposed agent is validated on a robot navigation task, demonstrating exploration and exploitation in a continuous-valued observation space with bounded continuous-valued actions. Compared to a classical optimal controller, the agent modulates action based on predictive uncertainty, arriving later but with a better model of the robot's dynamics.", 'abstract_zh': '基于因子图上的消息传递设计自回归主动推断代理：在连续观测空间中的探索与利用验证', 'title_zh': '基于消息传递的自回归主动推理代理的推理研究'}
{'arxiv_id': 'arXiv:2509.26605', 'title': 'Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning', 'authors': 'Maël Macuglia, Paul Friedrich, Giorgia Ramponi', 'link': 'https://arxiv.org/abs/2509.26605', 'abstract': 'Deploying reinforcement learning (RL) in robotics, industry, and health care is blocked by two obstacles: the difficulty of specifying accurate rewards and the risk of unsafe, data-hungry exploration. We address this by proposing a two-stage framework that first learns a safe initial policy from a reward-free dataset of expert demonstrations, then fine-tunes it online using preference-based human feedback. We provide the first principled analysis of this offline-to-online approach and introduce BRIDGE, a unified algorithm that integrates both signals via an uncertainty-weighted objective. We derive regret bounds that shrink with the number of offline demonstrations, explicitly connecting the quantity of offline data to online sample efficiency. We validate BRIDGE in discrete and continuous control MuJoCo environments, showing it achieves lower regret than both standalone behavioral cloning and online preference-based RL. Our work establishes a theoretical foundation for designing more sample-efficient interactive agents.', 'abstract_zh': '部署强化学习（RL）在机器人、工业和医疗领域的应用受制于两大障碍：准确奖励的难 SPECIFICATION 和不安全的数据驱动探索的风险。我们通过提出一个两阶段框架来解决这一问题，该框架首先从专家演示的无奖励数据集中学习一个安全的初始策略，然后通过基于偏好的人类反馈在线fine-tune该策略。我们提供了这种离线到在线方法的第一个原则性分析，并提出了一种统一算法BRIDGE，该算法通过不确定性加权目标将这两种信号结合起来。我们推导出随离线演示数量增加而缩小的遗憾界，明确地将离线数据的数量与在线采样效率联系起来。我们在离散和连续控制的MuJoCo环境中验证了BRIDGE，结果显示它在遗憾界方面优于独立的行为克隆和在线基于偏好的RL。我们的工作为设计更高效的交互式智能体奠定了理论基础。', 'title_zh': '基于偏好强化学习的行為克隆策略微调'}
{'arxiv_id': 'arXiv:2509.26255', 'title': 'ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning', 'authors': 'Yichao Liang, Dat Nguyen, Cambridge Yang, Tianyang Li, Joshua B. Tenenbaum, Carl Edward Rasmussen, Adrian Weller, Zenna Tavares, Tom Silver, Kevin Ellis', 'link': 'https://arxiv.org/abs/2509.26255', 'abstract': "Long-horizon embodied planning is challenging because the world does not only change through an agent's actions: exogenous processes (e.g., water heating, dominoes cascading) unfold concurrently with the agent's actions. We propose a framework for abstract world models that jointly learns (i) symbolic state representations and (ii) causal processes for both endogenous actions and exogenous mechanisms. Each causal process models the time course of a stochastic causal-effect relation. We learn these world models from limited data via variational Bayesian inference combined with LLM proposals. Across five simulated tabletop robotics environments, the learned models enable fast planning that generalizes to held-out tasks with more objects and more complex goals, outperforming a range of baselines.", 'abstract_zh': '长时程体态规划具有挑战性，因为世界的变化不仅通过代理的动作发生：外生过程（例如，热水加热、多米诺骨牌连锁反应）会与代理的动作同时进行。我们提出了一种抽象世界模型的框架，该框架联合学习（i）符号状态表示和（ii）因果过程，包括内生动作和外生机制。每个因果过程都建模了一个随机因果影响关系的时间进程。我们通过结合变分贝叶斯推断和大型语言模型提案从有限数据中学习这些世界模型。在五个模拟的桌面机器人环境中，学习到的模型能够实现快速规划，并对新的包含更多物体和更复杂目标的任务进行了泛化，优于多种基准方法。', 'title_zh': 'ExoPredicator：学习动态世界抽象模型的机器人规划方法'}
{'arxiv_id': 'arXiv:2509.25885', 'title': 'SafeMind: Benchmarking and Mitigating Safety Risks in Embodied LLM Agents', 'authors': 'Ruolin Chen, Yinqian Sun, Jihang Wang, Mingyang Lv, Qian Zhang, Yi Zeng', 'link': 'https://arxiv.org/abs/2509.25885', 'abstract': 'Embodied agents powered by large language models (LLMs) inherit advanced planning capabilities; however, their direct interaction with the physical world exposes them to safety vulnerabilities. In this work, we identify four key reasoning stages where hazards may arise: Task Understanding, Environment Perception, High-Level Plan Generation, and Low-Level Action Generation. We further formalize three orthogonal safety constraint types (Factual, Causal, and Temporal) to systematically characterize potential safety violations. Building on this risk model, we present SafeMindBench, a multimodal benchmark with 5,558 samples spanning four task categories (Instr-Risk, Env-Risk, Order-Fix, Req-Align) across high-risk scenarios such as sabotage, harm, privacy, and illegal behavior. Extensive experiments on SafeMindBench reveal that leading LLMs (e.g., GPT-4o) and widely used embodied agents remain susceptible to safety-critical failures. To address this challenge, we introduce SafeMindAgent, a modular Planner-Executor architecture integrated with three cascaded safety modules, which incorporate safety constraints into the reasoning process. Results show that SafeMindAgent significantly improves safety rate over strong baselines while maintaining comparable task completion. Together, SafeMindBench and SafeMindAgent provide both a rigorous evaluation suite and a practical solution that advance the systematic study and mitigation of safety risks in embodied LLM agents.', 'abstract_zh': '由大型语言模型驱动的具身代理的动力学特性：安全性推理与保障机制', 'title_zh': 'SafeMind: 评估与缓解体感LLM代理的安全风险'}
{'arxiv_id': 'arXiv:2509.25757', 'title': 'NePTune: A Neuro-Pythonic Framework for Tunable Compositional Reasoning on Vision-Language', 'authors': 'Danial Kamali, Parisa Kordjamshidi', 'link': 'https://arxiv.org/abs/2509.25757', 'abstract': 'Modern Vision-Language Models (VLMs) have achieved impressive performance in various tasks, yet they often struggle with compositional reasoning, the ability to decompose and recombine concepts to solve novel problems. While neuro-symbolic approaches offer a promising direction, they are typically constrained by crisp logical execution or predefined predicates, which limit flexibility. In this work, we introduce NePTune, a neuro-symbolic framework that overcomes these limitations through a hybrid execution model that integrates the perception capabilities of foundation vision models with the compositional expressiveness of symbolic reasoning. NePTune dynamically translates natural language queries into executable Python programs that blend imperative control flow with soft logic operators capable of reasoning over VLM-generated uncertainty. Operating in a training-free manner, NePTune, with a modular design, decouples perception from reasoning, yet its differentiable operations support fine-tuning. We evaluate NePTune on multiple visual reasoning benchmarks and various domains, utilizing adversarial tests, and demonstrate a significant improvement over strong base models, as well as its effective compositional generalization and adaptation capabilities in novel environments.', 'abstract_zh': '现代视觉-语言模型（VLMs）在各种任务上取得了显著性能，但常常在组合推理方面表现不佳，即分解和重新组合概念以解决新颖问题的能力有限。虽然神经符号方法提供了有前景的方向，但它们通常受限于明确的逻辑执行或预定义的谓词，从而限制了灵活性。在本工作中，我们引入了NePTune，这是一种通过结合基础视觉模型的感知能力和符号推理的组合表达性来克服这些限制的神经符号框架。NePTune通过一种混合执行模型动态地将自然语言查询转换为可执行的Python程序，这些程序融合了命令式控制流与软逻辑运算符，可以在VLM生成的不确定性上进行推理。无需训练，NePTune具有模块化设计，将感知与推理解耦，同时其可微操作支持微调。我们在多个视觉推理基准和各种领域上评估了NePTune，并利用对抗性测试，展示了其对强基线模型的显著改进，以及在新颖环境中有效组合泛化和适应能力。', 'title_zh': 'NePTune: 一种神经Pythonic框架，用于可调 composing 推理视觉-语言。'}
{'arxiv_id': 'arXiv:2509.25655', 'title': 'Landmark-Guided Knowledge for Vision-and-Language Navigation', 'authors': 'Dongsheng Yang, Meiling Zhu, Yinfeng Yu', 'link': 'https://arxiv.org/abs/2509.25655', 'abstract': 'Vision-and-language navigation is one of the core tasks in embodied intelligence, requiring an agent to autonomously navigate in an unfamiliar environment based on natural language instructions. However, existing methods often fail to match instructions with environmental information in complex scenarios, one reason being the lack of common-sense reasoning ability. This paper proposes a vision-and-language navigation method called Landmark-Guided Knowledge (LGK), which introduces an external knowledge base to assist navigation, addressing the misjudgment issues caused by insufficient common sense in traditional methods. Specifically, we first construct a knowledge base containing 630,000 language descriptions and use knowledge Matching to align environmental subviews with the knowledge base, extracting relevant descriptive knowledge. Next, we design a Knowledge-Guided by Landmark (KGL) mechanism, which guides the agent to focus on the most relevant parts of the knowledge by leveraging landmark information in the instructions, thereby reducing the data bias that may arise from incorporating external knowledge. Finally, we propose Knowledge-Guided Dynamic Augmentation (KGDA), which effectively integrates language, knowledge, vision, and historical information. Experimental results demonstrate that the LGK method outperforms existing state-of-the-art methods on the R2R and REVERIE vision-and-language navigation datasets, particularly in terms of navigation error, success rate, and path efficiency.', 'abstract_zh': '基于视觉-语言导向的知识引导导航方法（Landmark-Guided Knowledge for Vision-and-Language Navigation）', 'title_zh': '基于地标引导的知识指导视觉-语言导航'}
{'arxiv_id': 'arXiv:2509.25550', 'title': 'Learning to Interact in World Latent for Team Coordination', 'authors': 'Dongsu Lee, Daehee Lee, Yaru Niu, Honguk Woo, Amy Zhang, Ding Zhao', 'link': 'https://arxiv.org/abs/2509.25550', 'abstract': 'This work presents a novel representation learning framework, interactive world latent (IWoL), to facilitate team coordination in multi-agent reinforcement learning (MARL). Building effective representation for team coordination is a challenging problem, due to the intricate dynamics emerging from multi-agent interaction and incomplete information induced by local observations. Our key insight is to construct a learnable representation space that jointly captures inter-agent relations and task-specific world information by directly modeling communication protocols. This representation, we maintain fully decentralized execution with implicit coordination, all while avoiding the inherent drawbacks of explicit message passing, e.g., slower decision-making, vulnerability to malicious attackers, and sensitivity to bandwidth constraints. In practice, our representation can be used not only as an implicit latent for each agent, but also as an explicit message for communication. Across four challenging MARL benchmarks, we evaluate both variants and show that IWoL provides a simple yet powerful key for team coordination. Moreover, we demonstrate that our representation can be combined with existing MARL algorithms to further enhance their performance.', 'abstract_zh': '本研究提出了一种新颖的表示学习框架——交互世界隐空间(IWoL)，以促进多代理强化学习(MARL)中的团队协作。构建有效的团队协作表示是一个具有挑战性的问题，这是因为来自多代理交互的复杂动力学以及由于局部观测引起的不完整信息。我们的核心洞察是构建一个可学习的表示空间，该空间能同时捕捉代理间的关系和任务特定的世界信息，并通过直接建模通信协议来实现这一目标。此表示方法能够在保持完全去中心化执行的同时实现隐式协作，同时避免了显式消息传递的固有缺点，例如决策速度较慢、容易受到恶意攻击者的影响以及对带宽约束敏感。在实践中，该表示不仅可以作为每个代理的隐式潜在表示，还可以作为通信的显式消息。在四个具有挑战性的MARL基准测试中，我们评估了两种变体，并展示了IWoL为团队协作提供了一个简单而强大的关键。此外，我们证明了该表示可以与现有的MARL算法结合使用，进一步提升其性能。', 'title_zh': 'Learning to Interact in World Latent for Team Coordination'}
{'arxiv_id': 'arXiv:2509.25373', 'title': 'From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models', 'authors': 'Chenyue Zhou, Mingxuan Wang, Yanbiao Ma, Chenxu Wu, Wanyi Chen, Zhe Qian, Xinyu Liu, Yiwei Zhang, Junhao Wang, Hengbo Xu, Fei Luo, Xiaohua Chen, Xiaoshuai Hao, Hehan Li, Andi Zhang, Wenxuan Wang, Lingling Li, Zhiwu Lu, Yang Lu, Yike Guo', 'link': 'https://arxiv.org/abs/2509.25373', 'abstract': 'Multimodal Large Language Models (MLLMs) strive to achieve a profound, human-like understanding of and interaction with the physical world, but often exhibit a shallow and incoherent integration when acquiring information (Perception) and conducting reasoning (Cognition). This disconnect leads to a spectrum of reasoning failures, with hallucination being the most prominent. Collectively, these issues expose a fundamental challenge: the ability to process pixels does not yet confer the ability to construct a coherent, credible internal world model. To systematically dissect and address this challenge, this survey introduces a novel and unified analytical framework: ``From Perception to Cognition." We deconstruct the complex process of vision-language interactive understanding into two interdependent layers: Perception, the foundational ability to accurately extract visual information and achieve fine-grained alignment with textual instructions; and Cognition, the higher-order capability for proactive, multi-step, goal-oriented reasoning built upon this perceptual foundation, the core of which is the formation of a dynamic observe-think-verify reasoning loop. Guided by this framework, this paper systematically analyzes the key bottlenecks of current MLLMs at both layers. It surveys the landscape of cutting-edge methods designed to address these challenges, spanning from techniques that enhance low-level visual representations to those that improve high-level reasoning paradigms. Furthermore, we review critical benchmarks and delineate future research directions. This survey aims to provide the research community with a clear, structured perspective for understanding the intrinsic limitations of current MLLMs and to illuminate the path toward building next-generation models capable of deep reasoning and a genuine understanding of the world.', 'abstract_zh': '多模态大语言模型（MLLMs）致力于实现对物理世界的深刻、类人的理解和交互，但在获取信息（感知）和进行推理（认知）时常常表现出浅薄且不一致的整合。这种脱节导致了一系列推理失败，其中幻觉尤为突出。这些问题共同揭示了一个根本性挑战：处理像素的能力尚未赋予构建连贯、可信的内部世界模型的能力。为了系统地剖析并解决这一挑战，本文引入了一个新的统一分析框架：“从感知到认知”。我们拆解了视觉语言互动理解这一复杂过程为两个相互依存的层次：感知，即准确提取视觉信息并精密对齐文本指令的基础能力；以及认知，即在这一感知基础之上构建的高阶能力，核心是形成动态观察-思考-验证推理循环。遵循这一框架，本文系统分析了当前MLLMs在两个层次上的关键瓶颈。我们概述了旨在应对这些挑战的最前沿方法，涵盖从提升低层次视觉表示技术到改进高层次推理范式的各个方面。此外，我们回顾了关键基准，并阐明了未来研究方向。本文旨在为研究社区提供一个清晰的结构化视角，以理解当前MLLMs的内在局限性，并照亮构建新一代能够进行深入推理和真正理解世界的模型的道路。', 'title_zh': '从感知到认知：多模态大语言模型中视觉-语言交互推理综述'}
{'arxiv_id': 'arXiv:2509.26598', 'title': 'Are Robust LLM Fingerprints Adversarially Robust?', 'authors': 'Anshul Nasery, Edoardo Contente, Alkin Kaz, Pramod Viswanath, Sewoong Oh', 'link': 'https://arxiv.org/abs/2509.26598', 'abstract': 'Model fingerprinting has emerged as a promising paradigm for claiming model ownership. However, robustness evaluations of these schemes have mostly focused on benign perturbations such as incremental fine-tuning, model merging, and prompting. Lack of systematic investigations into {\\em adversarial robustness} against a malicious model host leaves current systems vulnerable. To bridge this gap, we first define a concrete, practical threat model against model fingerprinting. We then take a critical look at existing model fingerprinting schemes to identify their fundamental vulnerabilities. Based on these, we develop adaptive adversarial attacks tailored for each vulnerability, and demonstrate that these can bypass model authentication completely for ten recently proposed fingerprinting schemes while maintaining high utility of the model for the end users. Our work encourages fingerprint designers to adopt adversarial robustness by design. We end with recommendations for future fingerprinting methods.', 'abstract_zh': '模型指纹识别已成为一种有前途的模型所有权声明 paradigma。然而，这些方案的鲁棒性评估主要集中在良性扰动上，如逐步微调、模型合并和提示。缺乏对抗恶意模型宿主的系统研究使当前系统面临风险。为进一步弥补这一差距，我们首先定义了一个具体的、可操作的对抗性威胁模型，针对模型指纹识别。然后，我们批判性地审视现有的模型指纹识别方案，以识别其根本性漏洞。基于此，我们开发了针对每种漏洞的适应性对抗性攻击，并证明这些攻击可以完全绕过十种最近提出的指纹识别方案的模型认证，同时确保模型对最终用户的高度实用性。我们的工作鼓励指纹设计者在设计时采用对抗性鲁棒性。最后，我们提出了未来指纹识别方法的建议。', 'title_zh': '稳健的大语言模型指纹是否具有对抗性稳健性？'}
{'arxiv_id': 'arXiv:2509.25876', 'title': 'Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space', 'authors': 'Xinyu Zhang, Aishik Deb, Klaus Mueller', 'link': 'https://arxiv.org/abs/2509.25876', 'abstract': 'Policy-gradient methods such as Proximal Policy Optimization (PPO) are typically updated along a single stochastic gradient direction, leaving the rich local structure of the parameter space unexplored. Previous work has shown that the surrogate gradient is often poorly correlated with the true reward landscape. Building on this insight, we visualize the parameter space spanned by policy checkpoints within an iteration and reveal that higher performing solutions often lie in nearby unexplored regions. To exploit this opportunity, we introduce ExploRLer, a pluggable pipeline that seamlessly integrates with on-policy algorithms such as PPO and TRPO, systematically probing the unexplored neighborhoods of surrogate on-policy gradient updates. Without increasing the number of gradient updates, ExploRLer achieves significant improvements over baselines in complex continuous control environments. Our results demonstrate that iteration-level exploration provides a practical and effective way to strengthen on-policy reinforcement learning and offer a fresh perspective on the limitations of the surrogate objective.', 'abstract_zh': '基于策略梯度的方法如proximal策略优化(PPO)通常沿单一的随机梯度方向更新，未能充分利用参数空间的丰富局部结构。已有工作表明，近端梯度往往是与真正奖赏景观相关性较差的。基于这一洞察，我们可视化迭代过程中策略检查点所覆盖的参数空间，并揭示出高性能的解决方案往往位于未探索的邻近区域。为了利用这一机遇，我们提出了一种可插拔的ExploRLer管道，该管道能够无缝集成到如PPO和TRPO这类基于策略的方法中，系统地探究替代策略梯度更新的未探索邻域。在不增加梯度更新次数的情况下，ExploRLer在复杂连续控制环境中显著优于基线方法。我们的结果表明，迭代级探索为强化学习，特别是基于策略的强化学习提供了一种实用且有效的方法，同时也为我们重新审视替代目标的局限性提供了新视角。', 'title_zh': '通过稀疏参数空间探索实现高效的策略优化强化学习'}
{'arxiv_id': 'arXiv:2509.25794', 'title': 'Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding', 'authors': 'Haotian Xue, Yunhao Ge, Yu Zeng, Zhaoshuo Li, Ming-Yu Liu, Yongxin Chen, Jiaojiao Fan', 'link': 'https://arxiv.org/abs/2509.25794', 'abstract': 'Vision-Language Models (VLMs) have demonstrated impressive world knowledge across a wide range of tasks, making them promising candidates for embodied reasoning applications. However, existing benchmarks primarily evaluate the embodied reasoning ability of VLMs through multiple-choice questions based on image annotations -- for example, selecting which trajectory better describes an event in the image. In this work, we introduce the Point-It-Out (PIO) benchmark, a novel benchmark designed to systematically assess the embodied reasoning abilities of VLMs through precise visual grounding. We propose a hierarchical evaluation protocol spanning three stages (S1: referred-object localization, S2: task-driven pointing, and S3: visual trace prediction), with data collected from critical domains for embodied intelligence, including indoor, kitchen, driving, and robotic manipulation scenarios. Extensive experiments with over ten state-of-the-art VLMs reveal several interesting findings. For example, strong general-purpose models such as GPT-4o, while excelling on many benchmarks (e.g., language, perception, and reasoning), underperform compared to some open-source models in precise visual grounding; models such as MoLMO perform well in S1 and S2 but struggle in S3, where requires grounding combined with visual trace planning.', 'abstract_zh': 'Vision-Language模型（VLMs）在广泛的任务中展现了令人印象深刻的 worldly 知识，使它们成为体现推理应用的有希望的候选者。然而，现有的基准主要通过基于图像标注的多选题来评估VLMs的体现推理能力——例如，选择哪个轨迹更能描述图像中的事件。在本文中，我们引入了指向它（Point-It-Out，PIO）基准，这是一个新颖的基准，旨在通过精确的视觉定位系统性评估VLMs的体现推理能力。我们提出了一个分层评估协议，包含三个阶段（S1：referenced对象定位，S2：任务驱动的指针指向，S3：视觉轨迹预测），数据来自体现智能的关键领域，包括室内、厨房、驾驶和机器人操作场景。通过多个最先进的VLMs进行的大量实验揭示了几项有趣的发现。例如，强大的通用模型如GPT-4o，在许多基准（如语言、感知和推理）上表现出色，但在精确的视觉定位上却不如一些开源模型；模型如MoLMO在S1和S2阶段表现良好，但在S3阶段却遇到困难，这需要结合视觉定位和轨迹规划。', 'title_zh': 'Point-It-Out: 多阶段视觉定位中视觉语言模型感知推理基准测试'}
{'arxiv_id': 'arXiv:2509.25661', 'title': 'Deep Reinforcement Learning-Based Precoding for Multi-RIS-Aided Multiuser Downlink Systems with Practical Phase Shift', 'authors': 'Po-Heng Chou, Bo-Ren Zheng, Wan-Jen Huang, Walid Saad, Yu Tsao, Ronald Y. Chang', 'link': 'https://arxiv.org/abs/2509.25661', 'abstract': 'This study considers multiple reconfigurable intelligent surfaces (RISs)-aided multiuser downlink systems with the goal of jointly optimizing the transmitter precoding and RIS phase shift matrix to maximize spectrum efficiency. Unlike prior work that assumed ideal RIS reflectivity, a practical coupling effect is considered between reflecting amplitude and phase shift for the RIS elements. This makes the optimization problem non-convex. To address this challenge, we propose a deep deterministic policy gradient (DDPG)-based deep reinforcement learning (DRL) framework. The proposed model is evaluated under both fixed and random numbers of users in practical mmWave channel settings. Simulation results demonstrate that, despite its complexity, the proposed DDPG approach significantly outperforms optimization-based algorithms and double deep Q-learning, particularly in scenarios with random user distributions.', 'abstract_zh': '本研究考虑了多可重构智能表面（RIS）辅助的多用户下行系统，旨在联合优化发射端预编码和RIS相位移矩阵，以最大化频谱效率。不同于以往工作假设理想的RIS反射率，本研究考虑了RIS元器件反射幅度与相位移之间的实际耦合效应，这使得优化问题变为非凸问题。为应对这一挑战，我们提出了一种基于深度确定性策略梯度（DDPG）的深度强化学习（DRL）框架。该模型在实际毫米波信道条件下，分别在固定和随机用户数量的情况下进行了评估。仿真结果表明，尽管复杂度较高，所提出的DDPG方法在随机用户分布场景下显著优于基于优化的方法和双深度Q学习。', 'title_zh': '基于多用户下行系统的实用相移器辅助多RIS系统深度强化学习预编码方法'}
{'arxiv_id': 'arXiv:2509.25534', 'title': 'Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning', 'authors': 'Zhiling Ye, Yun Yue, Haowen Wang, Xudong Han, Jiadi Jiang, Cheng Wei, Lei Fan, Jiaxin Liang, Shuowen Zhang, Ji Li, Chunxiao Guo, Jian Wang, Peng Wei, Jinjie Gu', 'link': 'https://arxiv.org/abs/2509.25534', 'abstract': 'Open-ended evaluation is essential for deploying large language models in real-world settings. In studying HealthBench, we observe that using the model itself as a grader and generating rubric-based reward signals substantially improves reasoning performance. Remarkably, the trained model also becomes a stronger grader. Motivated by this, we introduce Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning, a lightweight framework that enables faster and more resource-efficient training while surpassing baselines. Remarkably, on Qwen3-32B, training with just the 4000-sample HealthBench Easy subset is sufficient to obtain a model that exceeds GPT-5 on HealthBench Hard. Incorporating a small amount of teacher-graded data further enhances performance for less capable models.', 'abstract_zh': '自回归评分基于奖励信号的开放性推理强化学习：一种轻量级框架', 'title_zh': '基于自我奖励评述的开放性推理强化学习'}
{'arxiv_id': 'arXiv:2509.25466', 'title': 'Data-Efficient Multitask DAgger', 'authors': 'Haotian Fu, Ran Gong, Xiaohan Zhang, Maria Vittoria Minniti, Jigarkumar Patel, Karl Schmeckpeper', 'link': 'https://arxiv.org/abs/2509.25466', 'abstract': "Generalist robot policies that can perform many tasks typically require extensive expert data or simulations for training. In this work, we propose a novel Data-Efficient multitask DAgger framework that distills a single multitask policy from multiple task-specific expert policies. Our approach significantly increases the overall task success rate by actively focusing on tasks where the multitask policy underperforms. The core of our method is a performance-aware scheduling strategy that tracks how much each task's learning process benefits from the amount of data, using a Kalman filter-based estimator to robustly decide how to allocate additional demonstrations across tasks. We validate our approach on MetaWorld, as well as a suite of diverse drawer-opening tasks in IsaacLab. The resulting policy attains high performance across all tasks while using substantially fewer expert demonstrations, and the visual policy learned with our method in simulation shows better performance than naive DAgger and Behavior Cloning when transferring zero-shot to a real robot without using real data.", 'abstract_zh': '一种数据高效多任务DAgger框架：从多个任务特定专家策略中提炼单一多任务策略', 'title_zh': '数据效率多任务DAgger'}
{'arxiv_id': 'arXiv:2509.25455', 'title': 'PIPer: On-Device Environment Setup via Online Reinforcement Learning', 'authors': 'Alexander Kovrigin, Aleksandra Eliseeva, Konstantin Grotov, Egor Bogomolov, Yaroslav Zharov', 'link': 'https://arxiv.org/abs/2509.25455', 'abstract': 'Environment setup-the process of configuring the system to work with a specific software project-represents a persistent challenge in Software Engineering (SE). Automated environment setup methods could assist developers by providing fully configured environments for arbitrary repositories without manual effort. This also helps SE researchers to scale execution-based benchmarks. However, recent studies reveal that even state-of-the-art Large Language Models (LLMs) achieve limited success in automating this task. To address this limitation, we tune a specialized model for environment setup. We combine supervised fine-tuning for generating correct Bash scripts and Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model runnable on consumer hardware) to perform on par with larger models-Qwen3-32B and GPT-4o. The training code and model checkpoints are available online: this https URL.', 'abstract_zh': '环境配置——软件工程中配置系统以与特定软件项目协同工作的过程——代表了持续的挑战。自动环境配置方法可以通过提供无需人工努力即可完全配置的环境来协助开发人员，并且还可以帮助软件工程研究人员扩大基于执行的基准测试。然而，近期的研究表明，即使是最先进的大语言模型（LLMs）在自动化这一任务方面也取得有限的成功。为了解决这一局限性，我们针对环境配置微调了一个专门的模型。我们结合监督微调生成正确的Bash脚本，并结合可验证奖励强化学习（RLVR）使其适应环境配置任务。在EnvBench-Python上，我们的方法使Qwen3-8B（可在消费者硬件上运行的模型）能够与更大规模的模型Qwen3-32B和GPT-4o表现相当。训练代码和模型检查点已在线提供：this https URL。', 'title_zh': 'PIPer：基于在线强化学习的设备端环境设置'}
{'arxiv_id': 'arXiv:2509.25424', 'title': 'Polychromic Objectives for Reinforcement Learning', 'authors': 'Jubayer Ibn Hamid, Ifdita Hasan Orney, Ellen Xu, Chelsea Finn, Dorsa Sadigh', 'link': 'https://arxiv.org/abs/2509.25424', 'abstract': 'Reinforcement learning fine-tuning (RLFT) is a dominant paradigm for improving pretrained policies for downstream tasks. These pretrained policies, trained on large datasets, produce generations with a broad range of promising but unrefined behaviors. Often, a critical failure mode of RLFT arises when policies lose this diversity and collapse into a handful of easily exploitable outputs. This convergence hinders exploration, which is essential for expanding the capabilities of the pretrained policy and for amplifying the benefits of test-time compute scaling. To address this, we introduce an objective for policy gradient methods that explicitly enforces the exploration and refinement of diverse generations, which we call a polychromic objective. We then show how proximal policy optimization (PPO) can be adapted to optimize this objective. Our method (1) employs vine sampling to collect on-policy rollouts and (2) modifies the advantage function to reflect the advantage under our new objective. Experiments on BabyAI, Minigrid, and Algorithmic Creativity show that our method improves success rates by reliably solving a larger set of environment configurations and generalizes better under large perturbations. Moreover, when given multiple attempts in pass@$k$ experiments, the policy achieves substantially higher coverage, demonstrating its ability to maintain and exploit a diverse repertoire of strategies.', 'abstract_zh': 'reinforcement learning fine-tuning (RLFT) 是改进预训练策略以适应下游任务的主要范式。这些预训练策略在大规模数据集上训练，会产生具有广泛潜力但未精炼的行为。当策略失去这种多样性并收敛于少数易于利用的输出时，RLFT 的一个关键失败模式就会出现。这种收敛阻碍了探索，这对于扩展预训练策略的能力以及放大测试时计算扩展的好处至关重要。为了解决这一问题，我们提出了一种显式促进多样生成的策略梯度方法的目标，称为多色目标。然后，我们展示了如何通过近端策略优化（PPO）来优化这一目标。我们的方法（1）采用藤蔓采样收集在线策略轨迹，并（2）修改优势函数以反映在我们新目标下的优势。在 BabyAI、Minigrid 和 算法创造力 上的实验表明，我们的方法通过更可靠地解决更多的环境配置并能更好地适应大型扰动来提高成功率。此外，在 pass@$k$ 实验中给予策略多次尝试时，其覆盖率明显提高，展示了其维持和利用多样化策略组合的能力。', 'title_zh': '多色目标强化学习'}
{'arxiv_id': 'arXiv:2509.22628', 'title': 'UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning', 'authors': 'Hongyu Chen, Guangrun Wang', 'link': 'https://arxiv.org/abs/2509.22628', 'abstract': 'Chain-of-Thought (CoT) prompting improves reasoning in large language models (LLMs), but its reliance on unstructured text limits interpretability and executability in embodied tasks. Prior work has explored structured CoTs using scene or logic graphs, yet these remain fundamentally limited: they model only low-order relations, lack constructs like inheritance or behavioral abstraction, and provide no standardized semantics for sequential or conditional planning. We propose UML-CoT, a structured reasoning and planning framework that leverages Unified Modeling Language (UML) to generate symbolic CoTs and executable action plans. UML class diagrams capture compositional object semantics, while activity diagrams model procedural control flow. Our three-stage training pipeline combines supervised fine-tuning with Group Relative Policy Optimization (GRPO), including reward learning from answer-only data. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered room-cleaning scenarios. UML-CoT outperforms unstructured CoTs in interpretability, planning coherence, and execution success, highlighting UML as a more expressive and actionable structured reasoning formalism.', 'abstract_zh': 'UML-CoT：一种基于统一建模语言的结构化推理和规划框架', 'title_zh': 'UML-CoT: 基于统一建模语言的结构化推理与规划在机器人房间清洁中的应用'}
