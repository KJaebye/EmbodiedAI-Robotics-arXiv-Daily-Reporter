{'arxiv_id': 'arXiv:2505.20223', 'title': 'Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects', 'authors': 'Yixin Cui, Haotian Lin, Shuo Yang, Yixiao Wang, Yanjun Huang, Hong Chen', 'link': 'https://arxiv.org/abs/2505.20223', 'abstract': "The rapid evolution of large language models in natural language processing has substantially elevated their semantic understanding and logical reasoning capabilities. Such proficiencies have been leveraged in autonomous driving systems, contributing to significant improvements in system performance. Models such as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning, an advanced cognitive method that simulates human thinking processes, demonstrating remarkable reasoning capabilities in complex tasks. By structuring complex driving scenarios within a systematic reasoning framework, this approach has emerged as a prominent research focus in autonomous driving, substantially improving the system's ability to handle challenging cases. This paper investigates how CoT methods improve the reasoning abilities of autonomous driving models. Based on a comprehensive literature review, we present a systematic analysis of the motivations, methodologies, challenges, and future research directions of CoT in autonomous driving. Furthermore, we propose the insight of combining CoT with self-learning to facilitate self-evolution in driving systems. To ensure the relevance and timeliness of this study, we have compiled a dynamic repository of literature and open-source projects, diligently updated to incorporate forefront developments. The repository is publicly available at this https URL.", 'abstract_zh': '大规模语言模型在自然语言处理领域的迅速进化显著提升了其语义理解和逻辑推理能力。这些能力已在自主驾驶系统中得以利用，推动了系统性能的大幅提高。例如，OpenAI的o1和DeepSeek-R1模型利用链式思考（CoT）推理，这是一种先进的认知方法，模拟人类思维过程，展现出在复杂任务中出色的能力。通过在系统推理框架中结构化复杂驾驶场景，这种方法已成为自主驾驶领域的一个重要研究方向，大幅提升了系统处理复杂情况的能力。本文探讨了CoT方法如何提高自主驾驶模型的推理能力。基于全面的文献回顾，我们提供了一种系统分析CoT在自主驾驶中的动机、方法、挑战及未来研究方向的分析。此外，我们提出了将CoT与自我学习结合以促进驾驶系统自我进化的见解。为确保本研究的相关性和时效性，我们整理了一个动态文献库和开源项目库，并不断更新以纳入最新进展。该库可在以下链接访问：https://this.url', 'title_zh': '自动驾驶中的链式思维：综合综述与未来展望'}
{'arxiv_id': 'arXiv:2505.18382', 'title': 'One Demo Is All It Takes: Planning Domain Derivation with LLMs from A Single Demonstration', 'authors': 'Jinbang Huang, Yixin Xiao, Zhanguang Zhang, Mark Coates, Jianye Hao, Yingxue Zhang', 'link': 'https://arxiv.org/abs/2505.18382', 'abstract': 'Pre-trained Large Language Models (LLMs) have shown promise in solving planning problems but often struggle to ensure plan correctness, especially for long-horizon tasks. Meanwhile, traditional robotic task and motion planning (TAMP) frameworks address these challenges more reliably by combining high-level symbolic search with low-level motion planning. At the core of TAMP is the planning domain, an abstract world representation defined through symbolic predicates and actions. However, creating these domains typically involves substantial manual effort and domain expertise, limiting generalizability. We introduce Planning Domain Derivation with LLMs (PDDLLM), a novel approach that combines simulated physical interaction with LLM reasoning to improve planning performance. The method reduces reliance on humans by inferring planning domains from a single annotated task-execution demonstration. Unlike prior domain-inference methods that rely on partially predefined or language descriptions of planning domains, PDDLLM constructs domains entirely from scratch and automatically integrates them with low-level motion planning skills, enabling fully automated long-horizon planning. PDDLLM is evaluated on over 1,200 diverse tasks spanning nine environments and benchmarked against six LLM-based planning baselines, demonstrating superior long-horizon planning performance, lower token costs, and successful deployment on multiple physical robot platforms.', 'abstract_zh': '预训练大型语言模型在解决规划问题方面显示出了前景，但在保证规划正确性，尤其是长期任务方面常常遇到困难。与此同时，传统的机器人任务和运动规划（TAMP）框架通过结合高层符号搜索和低层运动规划，更可靠地解决了这些挑战。TAMP的核心在于规划域，这是一种通过符号谓词和动作定义的抽象世界表示。然而，创建这些域通常需要大量的手动工作和领域专业知识，限制了其通用性。我们介绍了通过预训练大型语言模型进行规划域推导（PDDLLM）的创新方法，该方法结合了模拟物理交互和大语言模型推理，以提高规划性能。该方法减少了对人类的依赖，通过单一标注的任务执行示范推断规划域。与依赖部分预定义或语言描述的规划域推断方法不同，PDDLLM 从头构建域，并自动将其与低层运动规划技能集成，从而实现全自动长期规划。PDDLLM 在超过 1,200 个跨九个环境的多元化任务上进行了评估，并在六种基于大语言模型的规划基线方法上进行了基准测试，展示了在长期规划性能、更低的标记成本以及多物理机器人平台上的成功部署。', 'title_zh': '一个演示即可：从单个示范规划领域衍生方法'}
{'arxiv_id': 'arXiv:2505.18334', 'title': 'Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play', 'authors': 'Jiaxun Cui, Chen Tang, Jarrett Holtz, Janice Nguyen, Alessandro G. Allievi, Hang Qiu, Peter Stone', 'link': 'https://arxiv.org/abs/2505.18334', 'abstract': 'Past work has demonstrated that autonomous vehicles can drive more safely if they communicate with one another than if they do not. However, their communication has often not been human-understandable. Using natural language as a vehicle-to-vehicle (V2V) communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with human drivers. In this work, we propose a suite of traffic tasks in autonomous driving where vehicles in a traffic scenario need to communicate in natural language to facilitate coordination in order to avoid an imminent collision and/or support efficient traffic flow. To this end, this paper introduces a novel method, LLM+Debrief, to learn a message generation and high-level decision-making policy for autonomous vehicles through multi-agent discussion. To evaluate LLM agents for driving, we developed a gym-like simulation environment that contains a range of driving scenarios. Our experimental results demonstrate that LLM+Debrief is more effective at generating meaningful and human-understandable natural language messages to facilitate cooperation and coordination than a zero-shot LLM agent. Our code and demo videos are available at this https URL.', 'abstract_zh': '过去的研究已经证明，如果自动驾驶车辆彼此通信，它们可以更安全地行驶，而如果不通信则不然。然而，它们的通信往往是不可使人理解的。将自然语言作为车辆到车辆（V2V）通信协议，有助于自动驾驶车辆不仅与其他自动驾驶车辆，而且与人类驾驶员进行合作驾驶。在这项工作中，我们提出了一系列自动驾驶交通任务，在这些任务中，车辆在交通场景中需要通过自然语言沟通以协调行为，以避免即将发生的碰撞或支持高效的交通流量。为此，本文介绍了一种新的方法——LLM+Debrief，通过多智能体讨论来学习自动驾驶车辆的消息生成和高级决策政策。为了评估自动驾驶中的LLM代理，我们开发了一个类似于游戏的模拟环境，包含了各种驾驶场景。我们的实验结果表明，与零样本LLM代理相比，LLM+Debrief在生成有意义且可理解的自然语言消息以促进合作与协调方面更为有效。我们的代码和演示视频可在以下网址获得。', 'title_zh': '通过自博弈实现协同自动驾驶的自然语言通信 Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play'}
{'arxiv_id': 'arXiv:2505.18214', 'title': 'LA-RCS: LLM-Agent-Based Robot Control System', 'authors': 'TaekHyun Park, YoungJun Choi, SeungHoon Shin, Kwangil Lee', 'link': 'https://arxiv.org/abs/2505.18214', 'abstract': 'LA-RCS (LLM-agent-based robot control system) is a sophisticated robot control system designed to autonomously plan, work, and analyze the external environment based on user requirements by utilizing LLM-Agent. Utilizing a dual-agent framework, LA-RCS generates plans based on user requests, observes the external environment, executes the plans, and modifies the plans as needed to adapt to changes in the external conditions. Additionally, LA-RCS interprets natural language commands by the user and converts them into commands compatible with the robot interface so that the robot can execute tasks and meet user requests properly. During his process, the system autonomously evaluates observation results, provides feedback on the tasks, and executes commands based on real-time environmental monitoring, significantly reducing the need for user intervention in fulfilling requests. We categorized the scenarios that LA-RCS needs to perform into four distinct types and conducted a quantitative assessment of its performance in each scenario. The results showed an average success rate of 90 percent, demonstrating the system capability to fulfill user requests satisfactorily. For more extensive results, readers can visit our project page: this https URL', 'abstract_zh': '基于LLM-Agent的LA-RCS自主机器人控制系统', 'title_zh': '基于LLM代理的机器人控制系统'}
{'arxiv_id': 'arXiv:2505.18198', 'title': 'LTDA-Drive: LLMs-guided Generative Models based Long-tail Data Augmentation for Autonomous Driving', 'authors': 'Mahmut Yurt, Xin Ye, Yunsheng Ma, Jingru Luo, Abhirup Mallik, John Pauly, Burhaneddin Yaman, Liu Ren', 'link': 'https://arxiv.org/abs/2505.18198', 'abstract': '3D perception plays an essential role for improving the safety and performance of autonomous driving. Yet, existing models trained on real-world datasets, which naturally exhibit long-tail distributions, tend to underperform on rare and safety-critical, vulnerable classes, such as pedestrians and cyclists. Existing studies on reweighting and resampling techniques struggle with the scarcity and limited diversity within tail classes. To address these limitations, we introduce LTDA-Drive, a novel LLM-guided data augmentation framework designed to synthesize diverse, high-quality long-tail samples. LTDA-Drive replaces head-class objects in driving scenes with tail-class objects through a three-stage process: (1) text-guided diffusion models remove head-class objects, (2) generative models insert instances of the tail classes, and (3) an LLM agent filters out low-quality synthesized images. Experiments conducted on the KITTI dataset show that LTDA-Drive significantly improves tail-class detection, achieving 34.75\\% improvement for rare classes over counterpart methods. These results further highlight the effectiveness of LTDA-Drive in tackling long-tail challenges by generating high-quality and diverse data.', 'abstract_zh': '3D感知在提高自主驾驶的安全性和性能中起着重要作用。然而，现有模型在训练时使用的真实世界数据集自然表现出长尾分布，这些模型在罕见且安全关键的类别（如行人和骑自行车的人）上往往会表现不佳。针对现有研究中重新加权和重新采样技术在尾部类别稀缺性和多样性不足的问题，我们提出了LTDA-Drive，这是一种新型的LLM引导的数据增强框架，旨在合成多样且高质量的长尾样本。LTDA-Drive通过三阶段过程将驾驶场景中的头类对象替换为尾类对象：（1）文本引导的扩散模型移除头类对象，（2）生成模型插入尾类实例，（3）LLM代理过滤低质量的合成图像。在KITTI数据集上的实验结果显示，LTDA-Drive显著提高了尾类检测性能，相较于同类方法，罕见类别的检测准确率提升了34.75%。这些结果进一步突显了LTDA-Drive在生成高质量和多样化数据以应对长尾挑战方面的有效性。', 'title_zh': 'LTDA-Drive：由大规模语言模型引导的长尾数据生成增强方法及其在自主驾驶中的应用'}
{'arxiv_id': 'arXiv:2505.20024', 'title': 'ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving', 'authors': 'Xueyi Liu, Zuodong Zhong, Yuxin Guo, Yun-Fu Liu, Zhiguo Su, Qichao Zhang, Junli Wang, Yinfeng Gao, Yupeng Zheng, Qiao Lin, Huiyong Chen, Dongbin Zhao', 'link': 'https://arxiv.org/abs/2505.20024', 'abstract': 'Due to the powerful vision-language reasoning and generalization abilities, multimodal large language models (MLLMs) have garnered significant attention in the field of end-to-end (E2E) autonomous driving. However, their application to closed-loop systems remains underexplored, and current MLLM-based methods have not shown clear superiority to mainstream E2E imitation learning approaches. In this work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed for closed-loop driving through holistic reasoning with a self-supervised Next Scene Prediction task and supervised Decision Chain-of-Thought process. This dual mechanism encourages the model to align visual representations with actionable driving context, while promoting interpretable and causally grounded decision making. We curate a planning-oriented decision reasoning dataset, namely PDR, comprising 210k diverse and high-quality samples. Our method outperforms the mainstream E2E imitation learning method by a large margin of 19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan demonstrates strong zero-shot generalization on unseen DOS benchmark, highlighting its adaptability in handling zero-shot corner cases. Code and dataset will be found in this https URL.', 'abstract_zh': '由于强大的跨模态视觉语言推理和泛化能力，多模态大型语言模型（MLLMs）在端到端（E2E）自动驾驶领域引起了广泛关注。然而，它们在闭环系统中的应用仍待探索，当前基于MLLM的方法在与主流E2E模仿学习方法的竞争中尚未展现出明显优势。在本文中，我们提出了一种名为ReasonPlan的新颖MLLM微调框架，通过整体推理和自我监督的下一场景预测任务以及监督的决策思维链过程来实现闭环驾驶。这种双重机制促使模型将视觉表示与可操作的驾驶上下文对齐，同时促进解释性和因果驱动的决策制定。我们精心收集了一个面向规划的决策推理数据集，即PDR，其中包括210,000个多样且高质量的数据样本。我们的方法在Bench2Drive基准上以L2 19%和驾驶分数16.1的显著优势超越了主流的E2E模仿学习方法。此外，ReasonPlan在未见的DOS基准中展示了强大的零样本泛化能力，突显了其处理零样本边缘问题的适应性。代码和数据集可在以下链接中找到：https://github.com/ReasonPlan/ReasonPlan。', 'title_zh': 'ReasonPlan: 统一场景预测与决策推理的闭环自动驾驶'}
{'arxiv_id': 'arXiv:2505.18553', 'title': 'Applying Ontologies and Knowledge Augmented Large Language Models to Industrial Automation: A Decision-Making Guidance for Achieving Human-Robot Collaboration in Industry 5.0', 'authors': 'John Oyekan, Christopher Turner, Michael Bax, Erich Graf', 'link': 'https://arxiv.org/abs/2505.18553', 'abstract': 'The rapid advancement of Large Language Models (LLMs) has resulted in interest in their potential applications within manufacturing systems, particularly in the context of Industry 5.0. However, determining when to implement LLMs versus other Natural Language Processing (NLP) techniques, ontologies or knowledge graphs, remains an open question. This paper offers decision-making guidance for selecting the most suitable technique in various industrial contexts, emphasizing human-robot collaboration and resilience in manufacturing. We examine the origins and unique strengths of LLMs, ontologies, and knowledge graphs, assessing their effectiveness across different industrial scenarios based on the number of domains or disciplines required to bring a product from design to manufacture. Through this comparative framework, we explore specific use cases where LLMs could enhance robotics for human-robot collaboration, while underscoring the continued relevance of ontologies and knowledge graphs in low-dependency or resource-constrained sectors. Additionally, we address the practical challenges of deploying these technologies, such as computational cost and interpretability, providing a roadmap for manufacturers to navigate the evolving landscape of Language based AI tools in Industry 5.0. Our findings offer a foundation for informed decision-making, helping industry professionals optimize the use of Language Based models for sustainable, resilient, and human-centric manufacturing. We also propose a Large Knowledge Language Model architecture that offers the potential for transparency and configuration based on complexity of task and computing resources available.', 'abstract_zh': '大型语言模型的迅速进步引起了对其在制造系统中潜在应用的关注，特别是在Industry 5.0的背景下。然而，确定在何种情境下应实施大型语言模型而非其他自然语言处理技术、本体论或知识图谱仍是一个开放问题。本文提供了在不同工业背景下选择最适宜技术的决策指导，强调了人机协作和制造的韧性。我们探讨了大型语言模型、本体论和知识图谱的起源及其独特优势，并基于产品从设计到制造所需的不同领域或学科数量评估它们在不同工业场景中的有效性。通过这种对比框架，我们探讨了大型语言模型可能增强机器人以促进人机协作的具体应用场景，同时强调了本体论和知识图谱在低依赖性或资源受限领域中的持续相关性。此外，我们还讨论了这些技术的部署挑战，如计算成本和可解释性问题，并为制造商提供了一条路径，以便他们在Industry 5.0的不断变化的语言基础人工智能工具环境中导航。我们的研究结果为基于语言的模型在可持续、韧性和以人为中心的制造中的应用提供了决策基础。我们还提出了一个大型知识语言模型架构，该架构可以根据任务复杂性和可用计算资源提供透明性和配置能力。', 'title_zh': '基于本体和知识增强的大语言模型在工业自动化中的应用：实现 Industry 5.0 中人机协作的决策指导'}
{'arxiv_id': 'arXiv:2505.20266', 'title': 'syftr: Pareto-Optimal Generative AI', 'authors': 'Alexander Conway, Debadeepta Dey, Stefan Hackmann, Matthew Hausknecht, Michael Schmidt, Mark Steadman, Nick Volynets', 'link': 'https://arxiv.org/abs/2505.20266', 'abstract': "Retrieval-Augmented Generation (RAG) pipelines are central to applying large language models (LLMs) to proprietary or dynamic data. However, building effective RAG flows is complex, requiring careful selection among vector databases, embedding models, text splitters, retrievers, and synthesizing LLMs. The challenge deepens with the rise of agentic paradigms. Modules like verifiers, rewriters, and rerankers-each with intricate hyperparameter dependencies have to be carefully tuned. Balancing tradeoffs between latency, accuracy, and cost becomes increasingly difficult in performance-sensitive applications.\nWe introduce syftr, a framework that performs efficient multi-objective search over a broad space of agentic and non-agentic RAG configurations. Using Bayesian Optimization, syftr discovers Pareto-optimal flows that jointly optimize task accuracy and cost. A novel early-stopping mechanism further improves efficiency by pruning clearly suboptimal candidates. Across multiple RAG benchmarks, syftr finds flows which are on average approximately 9 times cheaper while preserving most of the accuracy of the most accurate flows on the Pareto-frontier. Furthermore, syftr's ability to design and optimize allows integrating new modules, making it even easier and faster to realize high-performing generative AI pipelines.", 'abstract_zh': '基于检索的生成(RAG)管道在应用大型语言模型(LLM)到专有或动态数据中发挥着核心作用。然而，构建有效的RAG流是一个复杂的过程，需要仔细选择向量数据库、嵌入模型、文本分割器、检索器以及综合LLM。随着代理范式的兴起，这一挑战更加艰巨。每个模块如验证器、重写器和重排序器都需要精心调整其复杂的超参数依赖关系。在性能敏感的应用中，平衡延迟、准确性和成本之间的权衡变得越来越困难。\n\n我们引入了syftr框架，该框架在广泛的目标代理和非代理RAG配置空间中执行高效的多目标搜索。利用贝叶斯优化，syftr发现同时优化任务准确性和成本的帕累托最优流。一个新颖的早期停止机制通过去除明显次优候选者进一步提高效率。在多个RAG基准测试中，syftr发现的流在平均成本方面大约便宜9倍，同时保留了帕累托前沿上最准确流的大部分准确性。此外，syftr的设计和优化能力使其能够集成新模块，使其构建高性能生成AI管道变得更加容易和快速。', 'title_zh': 'Syftr: 帕累托最优生成式AI'}
{'arxiv_id': 'arXiv:2505.20196', 'title': 'Temporal Sampling for Forgotten Reasoning in LLMs', 'authors': 'Yuetai Li, Zhangchen Xu, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Xiang Yue, Radha Poovendran', 'link': 'https://arxiv.org/abs/2505.20196', 'abstract': 'Fine-tuning large language models (LLMs) is intended to improve their reasoning capabilities, yet we uncover a counterintuitive effect: models often forget how to solve problems they previously answered correctly during training. We term this phenomenon temporal forgetting and show that it is widespread across model sizes, fine-tuning methods (both Reinforcement Learning and Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this gap, we introduce Temporal Sampling, a simple decoding strategy that draws outputs from multiple checkpoints along the training trajectory. This approach recovers forgotten solutions without retraining or ensembling, and leads to substantial improvements in reasoning performance, gains from 4 to 19 points in Pass@k and consistent gains in Majority@k across several benchmarks. We further extend our method to LoRA-adapted models, demonstrating that storing only adapter weights across checkpoints achieves similar benefits with minimal storage cost. By leveraging the temporal diversity inherent in training, Temporal Sampling offers a practical, compute-efficient way to surface hidden reasoning ability and rethink how we evaluate LLMs.', 'abstract_zh': 'Fine-tuning 大型语言模型 (LLMs) 的目的是提高其推理能力，但我们发现一种反直觉的现象：模型在训练过程中往往会忘记以前正确解决的问题。我们称这一现象为时间遗忘，并展示其在不同模型大小、微调方法（强化学习和监督微调）以及多种推理基准测试中普遍存在。为此，我们提出了一种简单解码策略——时间采样（Temporal Sampling），该策略从训练轨迹中的多个检查点抽取输出。这种方法可以在无需重新训练或集成的情况下恢复遗忘的解决方案，并在多个基准测试中显著提高推理性能，Pass@k 得分提升 4 至 19 分，以及在 Majority@k 上取得一致的改进。我们进一步将该方法扩展到 LoRA-适配模型，证明了仅在多个检查点存储适配器权重可以实现类似的好处，同时具有极小的存储成本。通过利用训练过程中固有的时间多样性，时间采样提供了一种实用且计算高效的手段来揭示隐藏的推理能力，并重新思考我们评估 LLM 的方式。', 'title_zh': 'Temporal Sampling for Forgotten Reasoning in LLMs'}
{'arxiv_id': 'arXiv:2505.20162', 'title': 'Capability-Based Scaling Laws for LLM Red-Teaming', 'authors': 'Alexander Panfilov, Paul Kassianik, Maksym Andriushchenko, Jonas Geiping', 'link': 'https://arxiv.org/abs/2505.20162', 'abstract': "As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers.", 'abstract_zh': '随着大型语言模型能力的增强和自主性的提高，通过红队测试识别漏洞对于安全部署变得至关重要。然而，传统的提示工程方法在红队测试变为能力弱方对抗能力强方的问题时可能变得无效，此时目标模型的能力超过了红队测试者的水平。为了研究这一转变，我们从攻击者和目标之间的能力差距角度重新定义红队测试。我们使用基于LLM的脱 Jailbreak 攻击评估了超过500组攻击者-目标对，这些攻击涵盖了多种不同的家庭、大小和能力水平，模仿了多种人类红队测试者。我们观察到三个主要趋势：（i）更强大的模型是更好的攻击者，（ii）一旦目标的能力超过攻击者，攻击成功率会急剧下降，（iii）攻击成功率与MMLU-Pro基准的社会科学划分部分上的高性能相关。根据这些趋势，我们推导出一个脱 Jailbreak 攻击的扩展法则，该法则可以根据攻击者-目标的能力差距预测固定目标的攻击成功率。这些发现表明，固定能力的攻击者（例如人类）可能在未来模型面前变得无效，日益强大的开源模型会增加现有系统的风险，模型提供商必须准确衡量和控制模型的说服性和操纵性能力，以限制其作为攻击者的有效性。', 'title_zh': '基于能力的扩展定律对LLM进行红队测试'}
{'arxiv_id': 'arXiv:2505.20127', 'title': 'Agentic AI Process Observability: Discovering Behavioral Variability', 'authors': 'Fabiana Fournier, Lior Limonad, Yuval David', 'link': 'https://arxiv.org/abs/2505.20127', 'abstract': 'AI agents that leverage Large Language Models (LLMs) are increasingly becoming core building blocks of modern software systems. A wide range of frameworks is now available to support the specification of such applications. These frameworks enable the definition of agent setups using natural language prompting, which specifies the roles, goals, and tools assigned to the various agents involved. Within such setups, agent behavior is non-deterministic for any given input, highlighting the critical need for robust debugging and observability tools. In this work, we explore the use of process and causal discovery applied to agent execution trajectories as a means of enhancing developer observability. This approach aids in monitoring and understanding the emergent variability in agent behavior. Additionally, we complement this with LLM-based static analysis techniques to distinguish between intended and unintended behavioral variability. We argue that such instrumentation is essential for giving developers greater control over evolving specifications and for identifying aspects of functionality that may require more precise and explicit definitions.', 'abstract_zh': '利用大规模语言模型（LLMs）的AI代理正逐渐成为现代软件系统的核心构建块。现在有许多框架支持此类应用的规范说明。这些框架允许使用自然语言提示来定义代理设置，指明各代理的角色、目标和工具。在这种设置中，对于任何给定的输入，代理行为都是非确定性的，突显了增强开发者可观测性的稳健调试和可观测性工具的迫切需求。在这项工作中，我们探讨了将过程发现和因果发现应用于代理执行轨迹，以提高开发者可观测性。该方法有助于监控和理解代理行为中出现的 variability。此外，我们还结合了基于LLM的静态分析技术，以区分预定和非预定的行为variability。我们argue指出，这种仪器化对于给予开发者对不断 evolves的规范更大的控制权，以及识别需要更精确和显式定义的功能方面至关重要。', 'title_zh': '代理人工智能过程可观测性：探索行为变异'}
{'arxiv_id': 'arXiv:2505.20087', 'title': 'Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models', 'authors': 'Makesh Narsimhan Sreedhar, Traian Rebedea, Christopher Parisien', 'link': 'https://arxiv.org/abs/2505.20087', 'abstract': 'Reasoning-based language models have demonstrated strong performance across various domains, with the most notable gains seen in mathematical and coding tasks. Recent research has shown that reasoning also offers significant benefits for LLM safety and guardrail applications. In this work, we conduct a comprehensive analysis of training reasoning-based guardrail models for content moderation, with an emphasis on generalization to custom safety policies at inference time. Our study focuses on two key dimensions: data efficiency and inference efficiency. On the data front, we find that reasoning-based models exhibit strong sample efficiency, achieving competitive performance with significantly fewer training examples than their non-reasoning counterparts. This unlocks the potential to repurpose the remaining data for mining high-value, difficult samples that further enhance model performance. On the inference side, we evaluate practical trade-offs by introducing reasoning budgets, examining the impact of reasoning length on latency and accuracy, and exploring dual-mode training to allow runtime control over reasoning behavior. Our findings will provide practical insights for researchers and developers to effectively and efficiently train and deploy reasoning-based guardrails models in real-world systems.', 'abstract_zh': '基于推理的语言模型在各个领域展现了强大的性能，特别是在数学和编程任务中取得了显著进展。最近的研究表明，推理也为LLM的安全性和边界应用带来了重要益处。在本工作中，我们对内容审核中的基于推理的边界模型进行了全面分析，重点在于推理时间的自定义安全策略泛化能力。我们的研究集中在两个关键维度：数据效率和推理效率。在数据方面，我们发现基于推理的模型表现出较强的经验效率，在比非推理模型少得多的训练样本下实现了竞争性的性能。这为重新利用剩余数据挖掘高价值、难题样本提供了可能性，进一步提升模型性能。在推理方面，通过引入推理预算，我们评估了推理长度对延迟和准确性的影响，并探索了双重模式训练以实现在运行时间对推理行为的控制。我们的发现将为研究人员和开发人员提供实用见解，以有效地在实际系统中训练和部署基于推理的边界模型。', 'title_zh': '通过推理保障安全：推理守 Rails 模型的实证研究'}
{'arxiv_id': 'arXiv:2505.19897', 'title': 'ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows', 'authors': 'Qiushi Sun, Zhoumianze Liu, Chang Ma, Zichen Ding, Fangzhi Xu, Zhangyue Yin, Haiteng Zhao, Zhenyu Wu, Kanzhi Cheng, Zhaoyang Liu, Jianing Wang, Qintong Li, Xiangru Tang, Tianbao Xie, Xiachong Feng, Xiang Li, Ben Kao, Wenhai Wang, Biqing Qi, Lingpeng Kong, Zhiyong Wu', 'link': 'https://arxiv.org/abs/2505.19897', 'abstract': "Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among these, computer-using agents, capable of interacting with operating systems as humans do, are paving the way to automated scientific problem-solving and addressing routines in researchers' workflows. Recognizing the transformative potential of these agents, we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows with integrated professional software, where agents can autonomously interact via different interfaces to accelerate complex research tasks and experiments; and (ii) a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans, spanning scientific-discovery workflows in domains such as biochemistry, astronomy, and geoinformatics. Extensive evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude 3.7, UI-TARS) show that, despite some promising results, they still fall short of reliably assisting scientists in complex workflows, achieving only a 15% overall success rate. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents for scientific discovery. Our code, environment, and benchmark are at this https URL.", 'abstract_zh': '大规模语言模型（LLMs）的影响已经超越了自然语言处理领域，显著促进了跨学科研究的发展。最近，基于LLM的各种代理被开发出来，以协助科学研究进展的多个方面和领域。其中，能够像人类一样与操作系统交互的计算机使用代理正引领自动化科学研究问题解决和研究人员工作流程中常规任务的途径。认识到这些代理的变革潜力，我们介绍了ScienceBoard，它包含了两个互补的贡献：（i）一个现实的、跨领域的环境，其中包括动态和视觉丰富的科学工作流程以及集成的专业软件，代理可以通过不同的接口自主交互以加速复杂的研究任务和实验；（ii）一个由人类策划的169项高质量、严格验证的真实世界任务组成的具有挑战性的基准，这些任务涵盖生物化学、天文学和地理信息系统等领域的科学研究工作流程。对最先进的代理底座（例如GPT-4o、Claude 3.7、UI-TARS）进行了广泛的评估表明，尽管取得了一些令人鼓舞的结果，但在复杂工作流程中可靠地协助科学家方面仍存在不足，总体成功率为15%。深入分析进一步提供了关于当前代理局限性的有价值见解并提出了更有效的设计原则，为构建更强大的科学研究代理铺平了道路。我们的代码、环境和基准可以通过这个网址获得。', 'title_zh': 'ScienceBoard: 在现实科学工作流中评估多模态自主代理'}
{'arxiv_id': 'arXiv:2505.19896', 'title': 'Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program', 'authors': 'Alejandro Carrasco, Victor Rodriguez-Fernandez, Richard Linares', 'link': 'https://arxiv.org/abs/2505.19896', 'abstract': 'Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Control in space, enabling LLMs to play a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space research. The project comprises several open repositories to facilitate replication and further research. The codebase is accessible on \\href{this https URL}{GitHub}, while the trained models and datasets are available on \\href{this https URL}{Hugging Face}. Additionally, experiment tracking and detailed results can be reviewed on \\href{this https URL}{Weights \\& Biases', 'abstract_zh': '近期，大型语言模型（LLMs）作为基于用户文本提示内容采取行动的自主代理的使用趋势正在兴起。我们打算将这些概念应用到航天控制领域，让LLMs在自主卫星操作的决策过程中发挥重要作用。为实现这一目标的第一步，我们为Kerbal Space Program Differential Games (KSPDG) 挑战开发了一个基于纯LLM的解决方案，这是一个公开的软件设计竞赛，参与者构建自主代理以操作参与非合作太空操作的卫星，运行于KSP游戏引擎之上。我们的方法利用了提示工程、少样本提示和微调技术，创造了一个有效的基于LLM的代理，在比赛中排名第二。据我们所知，本工作首次将LLM代理集成到航天研究中。该项目包含多个开源仓库，以促进复制和进一步研究。代码库可在GitHub上访问，而训练模型和数据集可在Hugging Face获得。此外，实验跟踪和详细结果也可在Weights & Biases上查看。', 'title_zh': '大型语言模型作为自主太空船操作员在Kerbal太空计划中'}
{'arxiv_id': 'arXiv:2505.19892', 'title': 'Unifying Multimodal Large Language Model Capabilities and Modalities via Model Merging', 'authors': 'Yongxian Wei, Runxi Cheng, Weike Jin, Enneng Yang, Li Shen, Lu Hou, Sinan Du, Chun Yuan, Xiaochun Cao, Dacheng Tao', 'link': 'https://arxiv.org/abs/2505.19892', 'abstract': 'While foundation models update slowly due to resource-intensive training requirements, domain-specific models evolve between updates. Model merging aims to combine multiple expert models into a single, more capable model, thereby reducing storage and serving costs while supporting decentralized model development. Despite its potential, previous studies have primarily focused on merging visual classification models or Large Language Models (LLMs) for code and math tasks. Multimodal Large Language Models (MLLMs), which extend the capabilities of LLMs through large-scale multimodal training, have gained traction. However, there lacks a benchmark for model merging research that clearly divides the tasks for MLLM training and evaluation. In this paper, (i) we introduce the model merging benchmark for MLLMs, which includes multiple tasks such as VQA, Geometry, Chart, OCR, and Grounding, providing both LoRA and full fine-tuning models. Moreover, we explore how model merging can combine different modalities (e.g., vision-language, audio-language, and video-language models), moving toward the Omni-language model. (ii) We implement 10 model merging algorithms on the benchmark. Furthermore, we propose a novel method that removes noise from task vectors and robustly optimizes the merged vector based on a loss defined over task vector interactions, achieving an average performance gain of 2.48%. (iii) We find that model merging offers a promising way for building improved MLLMs without requiring data training. Our results also demonstrate that the complementarity among multiple modalities outperforms individual modalities.', 'abstract_zh': '面向多模态大型语言模型的模型合并基准研究', 'title_zh': '统一多模态大型语言模型能力和模态通过模型合并'}
{'arxiv_id': 'arXiv:2505.19866', 'title': 'HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation', 'authors': 'Feng Xiong, Hongling Xu, Yifei Wang, Runxi Cheng, Yong Wang, Xiangxiang Chu', 'link': 'https://arxiv.org/abs/2505.19866', 'abstract': "Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of large language models (LLMs) by leveraging self-generated responses for self-training. Recent studies have incorporated reward models to guide response selection or decoding, aiming to obtain higher-quality data. However, they typically allocate a uniform sampling budget across all problems, overlooking the varying utility of problems at different difficulty levels. In this work, we conduct an empirical study and find that problems near the boundary of the LLM's reasoning capability offer significantly greater learning utility than both easy and overly difficult ones. To identify and exploit such problems, we propose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners. Given a fixed sampling budget, HS-STaR first performs lightweight pre-sampling with a reward-guided difficulty estimation strategy to efficiently identify boundary-level problems. Subsequently, it dynamically reallocates the remaining budget toward these high-utility problems during a re-sampling phase, maximizing the generation of valuable training data. Extensive experiments across multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR significantly outperforms other baselines without requiring additional sampling budget.", 'abstract_zh': '自学习推理器（STaRs）通过利用自生成响应进行自训练，增强大型语言模型的数学推理能力。近期研究引入了奖赏模型来指导响应选择或解码，以获得更高质量的数据。然而，它们通常为所有问题分配相同的采样预算，忽视了不同难度级别问题的不同效用。在本工作中，我们进行了一项实证研究并发现，接近大型语言模型推理能力边界的问题提供了显著大于较容易和过于困难问题的学习效用。为识别和利用这些问题，我们提出了一种层次采样框架HS-STaR。给定固定的采样预算，HS-STaR首先采用基于奖赏引导的难度估计策略进行轻量级预采样，以高效地识别边界水平的问题。随后，在重新采样阶段动态重新分配剩余预算，最大化生成有价值的训练数据。广泛的跨多个推理基准和主干大型语言模型的实验表明，HS-STaR在不需要额外采样预算的情况下显著优于其他基线方法。', 'title_zh': 'HS-STAR: 基于难度估计和预算重新分配的分层采样自学习推理器'}
{'arxiv_id': 'arXiv:2505.19761', 'title': 'Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning', 'authors': 'Zican Hu, Wei Liu, Xiaoye Qu, Xiangyu Yue, Chunlin Chen, Zhi Wang, Yu Cheng', 'link': 'https://arxiv.org/abs/2505.19761', 'abstract': 'While showing sophisticated reasoning abilities, large language models (LLMs) still struggle with long-horizon decision-making tasks due to deficient exploration and long-term credit assignment, especially in sparse-reward scenarios. Inspired by the divide-and-conquer principle, we propose an innovative framework **GLIDER** (**G**rounding **L**anguage Models as Eff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical **R**einforcement Learning) that introduces a parameter-efficient and generally applicable hierarchy to LLM policies. We develop a scheme where the low-level controller is supervised with abstract, step-by-step plans that are learned and instructed by the high-level policy. This design decomposes complicated problems into a series of coherent chain-of-thought reasoning sub-tasks, providing flexible temporal abstraction to significantly enhance exploration and learning for long-horizon tasks. Furthermore, GLIDER facilitates fast online adaptation to non-stationary environments owing to the strong transferability of its task-agnostic low-level skills. Experiments on ScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent performance gains, along with enhanced generalization capabilities.', 'abstract_zh': '基于离线分层强化学习的语言模型高效决策框架GLIDER', 'title_zh': 'divide和征服：通过 Offline 分层强化学习将大模型接地为高效的决策代理'}
{'arxiv_id': 'arXiv:2505.19734', 'title': 'ReChisel: Effective Automatic Chisel Code Generation by LLM with Reflection', 'authors': 'Juxin Niu, Xiangfeng Liu, Dan Niu, Xi Wang, Zhe Jiang, Nan Guan', 'link': 'https://arxiv.org/abs/2505.19734', 'abstract': 'Coding with hardware description languages (HDLs) such as Verilog is a time-intensive and laborious task. With the rapid advancement of large language models (LLMs), there is increasing interest in applying LLMs to assist with HDL coding. Recent efforts have demonstrated the potential of LLMs in translating natural language to traditional HDL Verilog. Chisel, a next-generation HDL based on Scala, introduces higher-level abstractions, facilitating more concise, maintainable, and scalable hardware designs. However, the potential of using LLMs for Chisel code generation remains largely unexplored. This work proposes ReChisel, an LLM-based agentic system designed to enhance the effectiveness of Chisel code generation. ReChisel incorporates a reflection mechanism to iteratively refine the quality of generated code using feedback from compilation and simulation processes, and introduces an escape mechanism to break free from non-progress loops. Experiments demonstrate that ReChisel significantly improves the success rate of Chisel code generation, achieving performance comparable to state-of-the-art LLM-based agentic systems for Verilog code generation.', 'abstract_zh': '使用硬件描述语言（HDL）如Verilog进行编码是一个耗时且劳动密集型的任务。随着大型语言模型（LLMs）的快速发展，人们越来越关注利用LLMs辅助HDL编码。最近的研究展示了LLMs在将自然语言翻译为传统HDL Verilog中的潜力。Chisel是一种基于Scala的下一代HDL，引入了更高的抽象层次，使得硬件设计更加简洁、可维护和可扩展。然而，使用LLMs生成Chisel代码的潜力尚未得到充分探索。本文提出ReChisel，这是一种基于LLMs的代理系统，旨在提高Chisel代码生成的有效性。ReChisel通过反馈机制中的反射机制迭代优化生成代码的质量，并引入逃生机制以避免非进展循环。实验表明，ReChisel显著提高了Chisel代码生成的成功率，其性能与最先进的基于LLMs的代理系统生成Verilog代码的性能相当。', 'title_zh': 'ReChisel: 通过反射的高效自动Chisel代码生成（基于LLM）'}
{'arxiv_id': 'arXiv:2505.19683', 'title': 'Large Language Models for Planning: A Comprehensive and Systematic Survey', 'authors': 'Pengfei Cao, Tianyi Men, Wencan Liu, Jingwen Zhang, Xuzhao Li, Xixun Lin, Dianbo Sui, Yanan Cao, Kang Liu, Jun Zhao', 'link': 'https://arxiv.org/abs/2505.19683', 'abstract': 'Planning represents a fundamental capability of intelligent agents, requiring comprehensive environmental understanding, rigorous logical reasoning, and effective sequential decision-making. While Large Language Models (LLMs) have demonstrated remarkable performance on certain planning tasks, their broader application in this domain warrants systematic investigation. This paper presents a comprehensive review of LLM-based planning. Specifically, this survey is structured as follows: First, we establish the theoretical foundations by introducing essential definitions and categories about automated planning. Next, we provide a detailed taxonomy and analysis of contemporary LLM-based planning methodologies, categorizing them into three principal approaches: 1) External Module Augmented Methods that combine LLMs with additional components for planning, 2) Finetuning-based Methods that involve using trajectory data and feedback signals to adjust LLMs in order to improve their planning abilities, and 3) Searching-based Methods that break down complex tasks into simpler components, navigate the planning space, or enhance decoding strategies to find the best solutions. Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Finally, we discuss the underlying mechanisms enabling LLM-based planning and outline promising research directions for this rapidly evolving field. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this field.', 'abstract_zh': '基于大型语言模型的规划：理论基础、方法分类、评估框架及未来研究方向', 'title_zh': '大规模语言模型在规划中的应用：一项全面而系统的综述'}
{'arxiv_id': 'arXiv:2505.19676', 'title': "Large Language Models' Reasoning Stalls: An Investigation into the Capabilities of Frontier Models", 'authors': 'Lachlan McGinness, Peter Baumgartner', 'link': 'https://arxiv.org/abs/2505.19676', 'abstract': 'Empirical methods to examine the capability of Large Language Models (LLMs) to use Automated Theorem Prover (ATP) reasoning strategies are studied. We evaluate the performance of State of the Art models from December 2023 and August 2024 on PRONTOQA steamroller reasoning problems. For that, we develop methods for assessing LLM response accuracy and correct answer correlation.\nOur results show that progress in improving LLM reasoning abilities has stalled over the nine month period. By tracking completion tokens, we show that almost all improvement in reasoning ability since GPT-4 was released can be attributed to either hidden system prompts or the training of models to automatically use generic Chain of Thought prompting strategies. Among the ATP reasoning strategies tried, we found that current frontier LLMs are best able to follow the bottom-up (also known as forward-chaining) strategy. A low positive correlation was found between an LLM response containing correct reasoning and arriving at the correct conclusion.', 'abstract_zh': '大型语言模型使用自动定理证明推理策略的能力的经验研究：评估2023年12月和2024年8月的先进模型在PRONTOQA蒸汽碾压推理问题上的性能，并开发评估LLM响应准确性和正确答案相关性的方法。我们的结果表明，过去九个月在提高LLM推理能力方面几乎没有进展。通过跟踪完成令牌，我们发现自GPT-4发布以来的几乎所有推理能力提升可归因于隐藏系统提示或模型训练以自动使用通用思路提示策略。在尝试的自动定理证明推理策略中，我们发现当前前沿的LLM最擅长遵循自底向上的（也称为正向链式推理）策略。LLM响应中包含正确推理与得出正确结论之间存在较低的正相关。', 'title_zh': '大型语言模型的推理停滞：前沿模型能力探究'}
{'arxiv_id': 'arXiv:2505.19653', 'title': 'Token-Importance Guided Direct Preference Optimization', 'authors': 'Yang Ning, Lin Hai, Liu Yibo, Tian Baoliang, Liu Guoqing, Zhang Haijun', 'link': 'https://arxiv.org/abs/2505.19653', 'abstract': 'Ensuring that large language models (LLMs) generate outputs aligned with human preferences is important for safe and effective AI interactions. While Direct Preference Optimization (DPO) employs an implicit reward function to optimize the policy model, however, it and its related variants overlook the differential importance of individual tokens and are sensitive to judgment noise in preference datasets during generation. Although recent methods attempt to assess the important weight of tokens via probability prediction or simplistic weighting schemes, these evaluation methods are prone to biases and still cannot fully address these issues. To solve this problem, we propose the Token-Importance Guided Direct Preference Optimization (TI-DPO), which introduces two key innovations: the gradient-based token-importance weights that dynamically prioritize critical tokens, and a triple loss that explicitly guides model outputs to approach human-preferred responses and stay away from non-preferred responses. Experimental results show that TI-DPO achieves higher accuracy and stronger generative diversity, providing more stable and computationally efficient solutions compared with DPO and other RLHF methods.', 'abstract_zh': '确保大型语言模型（LLMs）生成与人类偏好一致的输出对于安全有效的AI交互至关重要。虽然直接偏好优化（DPO）通过隐式奖励函数优化策略模型，但它及其相关变体忽略了单个词 token 的差异性重要性，在生成过程中对偏好数据集中的判断噪声敏感。尽管最近的方法试图通过概率预测或简单的加权方案评估词 token 的重要权重，但这些评估方法仍存在偏差，无法充分解决这些问题。为此，我们提出了一种词重要性引导的直接偏好优化（TI-DPO），该方法引入了两项关键创新：基于梯度的词重要性权重，动态优先处理关键词，以及一个三重损失，明确引导模型输出接近人类偏好响应并远离非偏好响应。实验结果表明，TI-DPO 在准确性和生成多样性方面优于 DPO 及其他 RLHF 方法，提供了更为稳定和计算效率更高的解决方案。', 'title_zh': '基于令牌重要性引导的直接偏好优化'}
{'arxiv_id': 'arXiv:2505.19641', 'title': 'SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond', 'authors': 'Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He', 'link': 'https://arxiv.org/abs/2505.19641', 'abstract': 'Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at this https URL.', 'abstract_zh': 'Recent advances such as OpenAI-o1和DeepSeek R1已展示了强化学习（RL）在增强大规模语言模型（LLMs）推理能力方面的潜力。虽然开源复现工作主要集中在数学和编程领域，但开发通用推理能力的方法和资源仍处于探索阶段。这一差距部分归因于收集适合RL的多样性和可验证性推理数据的挑战。我们假设逻辑推理对于开发通用推理能力至关重要，因为逻辑构成了推理的基本构建块。在本文中，我们介绍了SynLogic，这是一种数据合成框架和数据集，能够大规模生成多样化的逻辑推理数据，涵盖35种不同的逻辑推理任务。SynLogic方法允许可控地合成具有可调难度和数量的数据。此外，所有示例都可以通过简单的规则进行验证，使其非常适合带有可验证奖励的RL。在我们的实验中，我们基于7B和32B模型验证了SynLogic数据集上RL训练的有效性。SynLogic在开源数据集中实现了最先进的逻辑推理性能，比DeepSeek-R1-Distill-Qwen-32B在BBEH上的得分高6分。进一步将SynLogic数据与其他数学和编程任务混合提高了这些领域的训练效率，并显著增强了推理泛化。值得注意的是，我们的混合训练模型在多个基准测试中优于DeepSeek-R1-Zero-Qwen-32B。这些发现将SynLogic定位为推进LLMs更广泛推理能力的宝贵资源。我们在此https://网址开源了数据合成流水线和SynLogic数据集。', 'title_zh': 'SynLogic: 规范合成可验证推理数据以学习逻辑推理及相关领域'}
{'arxiv_id': 'arXiv:2505.19621', 'title': 'Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models', 'authors': 'George Kour, Itay Nakash, Ateret Anaby-Tavor, Michal Shmueli-Scheuer', 'link': 'https://arxiv.org/abs/2505.19621', 'abstract': "As Large Language Models (LLMs) become deeply integrated into human life and increasingly influence decision-making, it's crucial to evaluate whether and to what extent they exhibit subjective preferences, opinions, and beliefs. These tendencies may stem from biases within the models, which may shape their behavior, influence the advice and recommendations they offer to users, and potentially reinforce certain viewpoints. This paper presents the Preference, Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. We applied our benchmark to evaluate leading open- and closed-source LLMs, measuring desired properties such as reliability, neutrality, and consistency. In addition, we investigated the effect of increasing the test-time compute, through reasoning and self-reflection mechanisms, on those metrics. While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain. Furthermore, we reveal that newer model versions are becoming less consistent and more biased toward specific viewpoints, highlighting a blind spot and a concerning trend. POBS: this https URL", 'abstract_zh': '大规模语言模型（LLMs）日益深入地融入人类生活并越来越多地影响决策，评估它们是否表现出主观偏好、意见和信念变得至关重要。这些倾向可能源自模型中的偏见，这可能会影响其行为，影响其向用户提供的建议和推荐，并可能强化某些观点。本文介绍了偏好、意见和信念调查（POBs），这是一种用于评估LLMs在社会、文化、伦理和个人领域主观倾向的基准。我们应用了这一基准来评估领先的大规模开源和闭源语言模型，并衡量诸如可靠、中立和一致等期望属性。此外，我们研究了通过推理和自我反思机制增加测试时计算量对这些指标的影响。虽然在其他任务中这些机制有效，但我们的结果显示，这些机制在我们的领域仅提供了有限的改进。此外，我们发现新版本的模型变得不那么一致，并且更倾向于特定观点，这揭示了一个盲点并令人担忧的趋势。POBS: 这里是链接', 'title_zh': '重新思考！测试时计算对大型语言模型偏好、观点和信念的影响'}
{'arxiv_id': 'arXiv:2505.19568', 'title': 'MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model', 'authors': 'Jiongchao Jin, Xiuju Fu, Xiaowei Gao, Tao Cheng, Ran Yan', 'link': 'https://arxiv.org/abs/2505.19568', 'abstract': 'Maritime transportation is the backbone of global trade, making ship inspection essential for ensuring maritime safety and environmental protection. Port State Control (PSC), conducted by national ports, enforces compliance with safety regulations, with ship detention being the most severe consequence, impacting both ship schedules and company reputations. Traditional machine learning methods for ship detention prediction are limited by the capacity of representation learning and thus suffer from low accuracy. Meanwhile, autoencoder-based deep learning approaches face challenges due to the severe data imbalance in learning historical PSC detention records. To address these limitations, we propose Maritime Ship Detention with Large Language Models (MSD-LLM), integrating a dual robust subspace recovery (DSR) layer-based autoencoder with a progressive learning pipeline to handle imbalanced data and extract meaningful PSC representations. Then, a large language model groups and ranks features to identify likely detention cases, enabling dynamic thresholding for flexible detention predictions. Extensive evaluations on 31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM outperforms state-of-the-art methods more than 12\\% on Area Under the Curve (AUC) for Singapore ports. Additionally, it demonstrates robustness to real-world challenges, making it adaptable to diverse maritime risk assessment scenarios.', 'abstract_zh': '基于大规模语言模型的海上船舶滞留预测（MSD-LLM）', 'title_zh': 'MSD-LLM：基于大型语言模型的港口国控制检查中船舶滞留预测'}
{'arxiv_id': 'arXiv:2505.19567', 'title': 'LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer', 'authors': 'Rasoul Zahedifar, Sayyed Ali Mirghasemi, Mahdieh Soleymani Baghshah, Alireza Taheri', 'link': 'https://arxiv.org/abs/2505.19567', 'abstract': "This study presents the LLM-Agent-Controller, a multi-agent large language model (LLM) system developed to address a wide range of problems in control engineering (Control Theory). The system integrates a central controller agent with multiple specialized auxiliary agents, responsible for tasks such as controller design, model representation, control analysis, time-domain response, and simulation. A supervisor oversees high-level decision-making and workflow coordination, enhancing the system's reliability and efficiency. The LLM-Agent-Controller incorporates advanced capabilities, including Retrieval-Augmented Generation (RAG), Chain-of-Thought reasoning, self-criticism and correction, efficient memory handling, and user-friendly natural language communication. It is designed to function without requiring users to have prior knowledge of Control Theory, enabling them to input problems in plain language and receive complete, real-time solutions. To evaluate the system, we propose new performance metrics assessing both individual agents and the system as a whole. We test five categories of Control Theory problems and benchmark performance across three advanced LLMs. Additionally, we conduct a comprehensive qualitative conversational analysis covering all key services. Results show that the LLM-Agent-Controller successfully solved 83% of general tasks, with individual agents achieving an average success rate of 87%. Performance improved with more advanced LLMs. This research demonstrates the potential of multi-agent LLM architectures to solve complex, domain-specific problems. By integrating specialized agents, supervisory control, and advanced reasoning, the LLM-Agent-Controller offers a scalable, robust, and accessible solution framework that can be extended to various technical domains.", 'abstract_zh': 'LLM-Agent-Controller：一个解决控制工程中广泛问题的多代理大型语言模型系统', 'title_zh': 'LLM-Agent-Controller：一个适用于控制工程师的通用多代理大型语言模型系统'}
{'arxiv_id': 'arXiv:2505.19563', 'title': 'Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights', 'authors': 'Shi-Yu Tian, Zhi Zhou, Wei Dong, Ming Yang, Kun-Yang Yu, Zi-Jian Cheng, Lan-Zhe Guo, Yu-Feng Li', 'link': 'https://arxiv.org/abs/2505.19563', 'abstract': 'Reasoning with tabular data holds increasing importance in modern applications, yet comprehensive evaluation methodologies for reasoning-intensive Table Question Answering (QA) tasks remain nascent. Existing research is constrained by two primary bottlenecks: 1) Reliance on costly manually annotated real-world data, which is difficult to cover complex reasoning scenarios; 2) The heterogeneity of table structures hinders systematic analysis of the intrinsic mechanisms behind the underperformance of LLMs, especially in reasoning-intensive tasks. To address these issues, we propose an automated generation pipeline AutoT2T that transforms mathematical word problems into table-based reasoning tasks, eliminating the need for manual annotation. The pipeline can generate multiple variants of a table for the same reasoning problem, including noisy versions to support robustness evaluation. Based on this, we construct a new benchmark TabularGSM, which systematically spans a range of table complexities and trap problems. Experimental analyses through AutoT2T and TabularGSM reveal that the tight coupling between reasoning and retrieval or identification processes is a key factor underlying the failure of LLMs in complex Table QA tasks. This highlights the necessity for models to develop synergistic reasoning capabilities in order to perform effectively in complex Table QA tasks.', 'abstract_zh': '基于表格的数据推理在现代应用中 increasingly important，然而针对推理密集型表格问答任务的综合评估方法仍处于初级阶段。现有研究主要受两个瓶颈制约：1）依赖于昂贵的手动标注真实世界数据，难以覆盖复杂的推理场景；2）表格结构的异质性阻碍了对驱动LLM表现不佳的内在机制的系统分析，尤其是在推理密集型任务中。为了解决这些问题，我们提出了一种自动化生成管道AutoT2T，将数学应用题转换为基于表格的推理任务，从而消除手动标注的需要。该管道可以为同一推理问题生成多个表格变体，包括噪声版本以支持稳健性评估。基于此，我们构建了一个新的基准TabularGSM，系统地涵盖了表格复杂度范围和陷阱问题。通过AutoT2T和TabularGSM的实验分析揭示了推理与检索或识别过程之间的紧密耦合是导致LLM在复杂表格问答任务中失败的关键因素。这强调了为有效完成复杂表格问答任务，模型需要发展协同推理能力的必要性。', 'title_zh': '基于推理密集型表格问答的自动文本到表格转换：管道设计与基准研究'}
{'arxiv_id': 'arXiv:2505.19562', 'title': 'AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare', 'authors': 'Ying Xiao, Jie Huang, Ruijuan He, Jing Xiao, Mohammad Reza Mousavi, Yepang Liu, Kezhi Li, Zhenpeng Chen, Jie M. Zhang', 'link': 'https://arxiv.org/abs/2505.19562', 'abstract': 'Large language models (LLMs) are reaching expert-level accuracy on medical diagnosis questions, yet their mistakes and the biases behind them pose life-critical risks. Bias linked to race, sex, and socioeconomic status is already well known, but a consistent and automatic testbed for measuring it is missing. To fill this gap, this paper presents AMQA -- an Adversarial Medical Question-Answering dataset -- built for automated, large-scale bias evaluation of LLMs in medical QA. AMQA includes 4,806 medical QA pairs sourced from the United States Medical Licensing Examination (USMLE) dataset, generated using a multi-agent framework to create diverse adversarial descriptions and question pairs. Using AMQA, we benchmark five representative LLMs and find surprisingly substantial disparities: even GPT-4.1, the least biased model tested, answers privileged-group questions over 10 percentage points more accurately than unprivileged ones. Compared with the existing benchmark CPV, AMQA reveals 15% larger accuracy gaps on average between privileged and unprivileged groups. Our dataset and code are publicly available at this https URL to support reproducible research and advance trustworthy, bias-aware medical AI.', 'abstract_zh': '大型语言模型在医学诊断问题上的专家级准确性已达到，但其错误及其背后的偏见带来了生命攸关的风险。尽管与种族、性别和社会经济地位相关的偏见已广为人知，但缺乏一致的自动化评估平台。为填补这一空白，本文介绍了一个名为AMQA的对抗医学问答数据集，用于自动化大规模评估医学问答中大型语言模型的偏见。AMQA包括来自美国医学执照考试（USMLE）数据集的4,806个医学问答对，使用多智能体框架生成多样化的对抗描述和问题对。使用AMQA，我们对五种代表性大型语言模型进行了基准测试，发现了令人惊讶的显著差异：即使是测试中最不偏的GPT-4.1模型，也能在回答特权群体问题方面的准确性高出近10个百分点。与现有的基准CPV相比，AMQA在特权群体和非特权群体之间平均揭示出15%更大的准确性差距。我们的数据集和代码可在以下链接公开访问，以支持可重复研究并推进具有可信度和偏见意识的医学人工智能的发展。', 'title_zh': 'AMQA: 一个用于评估医学和健康-care领域LLMs偏见的对抗性数据集'}
{'arxiv_id': 'arXiv:2505.19490', 'title': 'Automated CAD Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models', 'authors': 'Jianxing Liao, Junyan Xu, Yatao Sun, Maowen Tang, Sicheng He, Jingxian Liao, Shui Yu, Yun Li, Hongguan Xiao', 'link': 'https://arxiv.org/abs/2505.19490', 'abstract': 'Designing complex computer-aided design (CAD) models is often time-consuming due to challenges such as computational inefficiency and the difficulty of generating precise models. We propose a novel language-guided framework for industrial design automation to address these issues, integrating large language models (LLMs) with computer-automated design (CAutoD).Through this framework, CAD models are automatically generated from parameters and appearance descriptions, supporting the automation of design tasks during the detailed CAD design phase. Our approach introduces three key innovations: (1) a semi-automated data annotation pipeline that leverages LLMs and vision-language large models (VLLMs) to generate high-quality parameters and appearance descriptions; (2) a Transformer-based CAD generator (TCADGen) that predicts modeling sequences via dual-channel feature aggregation; (3) an enhanced CAD modeling generation model, called CADLLM, that is designed to refine the generated sequences by incorporating the confidence scores from TCADGen. Experimental results demonstrate that the proposed approach outperforms traditional methods in both accuracy and efficiency, providing a powerful tool for automating industrial workflows and generating complex CAD models from textual prompts. The code is available at this https URL', 'abstract_zh': '基于语言指导的工业设计自动化框架：通过将大型语言模型与计算机自动化设计集成以解决复杂计算机辅助设计模型设计难题', 'title_zh': '基于变压器的大语言模型从文本描述自动生成CAD建模序列'}
{'arxiv_id': 'arXiv:2505.19489', 'title': 'Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs', 'authors': 'Zhenhao Zhou, Zhuochen Huang, Yike He, Chong Wang, Jiajun Wang, Yijian Wu, Xin Peng, Yiling Lou', 'link': 'https://arxiv.org/abs/2505.19489', 'abstract': 'The Linux kernel is a critical system, serving as the foundation for numerous systems. Bugs in the Linux kernel can cause serious consequences, affecting billions of users. Fault localization (FL), which aims at identifying the buggy code elements in software, plays an essential role in software quality assurance. While recent LLM agents have achieved promising accuracy in FL on recent benchmarks like SWE-bench, it remains unclear how well these methods perform in the Linux kernel, where FL is much more challenging due to the large-scale code base, limited observability, and diverse impact factors. In this paper, we introduce LinuxFLBench, a FL benchmark constructed from real-world Linux kernel bugs. We conduct an empirical study to assess the performance of state-of-the-art LLM agents on the Linux kernel. Our initial results reveal that existing agents struggle with this task, achieving a best top-1 accuracy of only 41.6% at file level. To address this challenge, we propose LinuxFL$^+$, an enhancement framework designed to improve FL effectiveness of LLM agents for the Linux kernel. LinuxFL$^+$ substantially improves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy increase) with minimal costs. Data and code are available at this https URL.', 'abstract_zh': 'Linux内核故障定位基准LinuxFLBench及其应用研究', 'title_zh': '本地化Linux内核漏洞的LLM代理benchmarking与增强'}
{'arxiv_id': 'arXiv:2505.19477', 'title': 'Judging with Many Minds: Do More Perspectives Mean Less Prejudice?', 'authors': 'Chiyu Ma, Enpei Zhang, Yilun Zhao, Wenjun Liu, Yaning Jia, Peijun Qing, Lin Shi, Arman Cohan, Yujun Yan, Soroush Vosoughi', 'link': 'https://arxiv.org/abs/2505.19477', 'abstract': 'LLM-as-Judge has emerged as a scalable alternative to human evaluation, enabling large language models (LLMs) to provide reward signals in trainings. While recent work has explored multi-agent extensions such as multi-agent debate and meta-judging to enhance evaluation quality, the question of how intrinsic biases manifest in these settings remains underexplored. In this study, we conduct a systematic analysis of four diverse bias types: position bias, verbosity bias, chain-of-thought bias, and bandwagon bias. We evaluate these biases across two widely adopted multi-agent LLM-as-Judge frameworks: Multi-Agent-Debate and LLM-as-Meta-Judge. Our results show that debate framework amplifies biases sharply after the initial debate, and this increased bias is sustained in subsequent rounds, while meta-judge approaches exhibit greater resistance. We further investigate the incorporation of PINE, a leading single-agent debiasing method, as a bias-free agent within these systems. The results reveal that this bias-free agent effectively reduces biases in debate settings but provides less benefit in meta-judge scenarios. Our work provides a comprehensive study of bias behavior in multi-agent LLM-as-Judge systems and highlights the need for targeted bias mitigation strategies in collaborative evaluation settings.', 'abstract_zh': 'LLM-as-裁判作为一种可扩展的人类评价替代方案，能够为训练提供奖励信号。虽然近期研究探索了多智能体扩展如多智能体辩论和元评价以提升评价质量，但这些环境中固有偏差的展现形式仍缺乏深入探讨。在本研究中，我们系统分析了四种不同类型的偏差：位置偏差、冗长偏差、推理链偏差和随大流偏差。我们在两个广泛采用的多智能体LLM-as-裁判框架——多智能体辩论和LLM-as-元裁判中评估这些偏差。结果显示，辩论框架在初始辩论后显著放大了偏差，并且这种偏差在后续轮次中持续存在，而元裁判方法则表现出更强的抗偏差能力。我们进一步研究了PINE，这是一种领先的单智能体去偏方法，将其作为无偏智能体集成到这些系统中。研究结果表明，在辩论环境中，这种无偏智能体有效减少了偏差，但在元裁判场景中的益处较少。我们的工作提供了多智能体LLM-as-裁判系统中偏差行为的全面研究，并强调了在协作评价环境中需要有针对性的偏差缓解策略。', 'title_zh': '众志定论：更多视角意味着 fewer prejudice 否？'}
{'arxiv_id': 'arXiv:2505.19474', 'title': 'Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models', 'authors': 'Xinmiao Hu, Chun Wang, Ruihe An, ChenYu Shao, Xiaojun Ye, Sheng Zhou, Liangcheng Li', 'link': 'https://arxiv.org/abs/2505.19474', 'abstract': 'Multimodal Large Language Models (MLLMs) have demonstrated strong performance in visual understanding tasks, yet they often suffer from object hallucinations--generating descriptions of objects that are inconsistent with or entirely absent from the input. This issue is closely related to dataset biases, where frequent co-occurrences of objects lead to entangled semantic representations across modalities. As a result, models may erroneously activate object representations that are commonly associated with the input but not actually present.\nTo address this, we propose a causality-driven disentanglement framework that mitigates hallucinations through causal intervention. Our approach includes a Causal-Driven Projector in the visual pathway and a Causal Intervention Module integrated into the final transformer layer of the language model. These components work together to reduce spurious correlations caused by biased training data.\nExperimental results show that our method significantly reduces hallucinations while maintaining strong performance on multiple multimodal benchmarks. Visualization analyses further confirm improved separability of object representations.\nThe code is available at: this https URL', 'abstract_zh': '多模态大型语言模型（MLLMs）在视觉理解任务中表现出强大的性能，但却常常遭受对象幻觉的问题——生成与输入不符合或完全缺失对象的描述。这一问题与数据集偏差密切相关，频繁共现的对象导致跨模态纠缠的语义表示。因此，模型可能会错误地激活与输入常见但实际不存在的对象表示。\n为了解决这一问题，我们提出了一种因果驱动的解缠框架，通过因果干预减轻幻觉现象。该方法包括视觉路径中的因果驱动投影器和集成在语言模型最终变换层中的因果干预模块。这些组件共同作用，减少由有偏训练数据引起的虚假关联。\n实验结果表明，我们的方法在减少幻觉的同时，能够在多个跨模态基准上保持强大的性能。可视化分析进一步证实了对象表示可分离性的改善。\n代码可在以下链接获取：this https URL', 'title_zh': '因果LLaVA：因果分离方法减轻多模态大语言模型中的幻觉问题'}
{'arxiv_id': 'arXiv:2505.19466', 'title': 'Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs', 'authors': 'Hongyu Liang, Yuting Zheng, Yihan Li, Yiran Zhang, Shiyu Liang', 'link': 'https://arxiv.org/abs/2505.19466', 'abstract': 'As large language models (LLMs) continue to advance, their deployment often involves fine-tuning to enhance performance on specific downstream tasks. However, this customization is sometimes accompanied by misleading claims about the origins, raising significant concerns about transparency and trust within the open-source community. Existing model verification techniques typically assess functional, representational, and weight similarities. However, these approaches often struggle against obfuscation techniques, such as permutations and scaling transformations. To address this limitation, we propose a novel detection method Origin-Tracer that rigorously determines whether a model has been fine-tuned from a specified base model. This method includes the ability to extract the LoRA rank utilized during the fine-tuning process, providing a more robust verification framework. This framework is the first to provide a formalized approach specifically aimed at pinpointing the sources of model fine-tuning. We empirically validated our method on thirty-one diverse open-source models under conditions that simulate real-world obfuscation scenarios. We empirically analyze the effectiveness of our framework and finally, discuss its limitations. The results demonstrate the effectiveness of our approach and indicate its potential to establish new benchmarks for model verification.', 'abstract_zh': '随着大型语言模型（LLMs）的不断进步，其部署通常涉及微调以增强特定下游任务的性能。然而，这种定制有时伴随着关于起源的误导性声明，这引发了开放源代码社区中透明度和信任方面的重要关切。现有的模型验证技术通常评估功能、表示和权重相似性。然而，这些方法往往难以抵御诸如排列和尺度变换等混淆技术。为了克服这一局限，我们提出了一个名为Origin-Tracer的新检测方法，该方法严格确定模型是否从指定的基础模型进行了微调。该方法包括从微调过程中提取LoRA秩的能力，从而提供了一个更为 robust 的验证框架。这是首个专门针对定位模型微调源的正式化方法。我们在模拟现实世界混淆场景的条件下对三十一个不同的开源模型 empirically 验证了该方法。我们 empirically 分析了该框架的有效性，并最终讨论了其局限性。结果表明了该方法的有效性，并指出其有可能为模型验证建立新的基准。', 'title_zh': '源追踪器：检测LLM中LoRA微调源头的方法'}
{'arxiv_id': 'arXiv:2505.19457', 'title': 'BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs', 'authors': 'Guilong Lu, Xuntao Guo, Rongjunchen Zhang, Wenqiao Zhu, Ji Liu', 'link': 'https://arxiv.org/abs/2505.19457', 'abstract': 'Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, a novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future research. The code and dataset are available at this https URL.', 'abstract_zh': '大型语言模型在通用任务中表现出色，但在金融、法律和医疗等逻辑密集、精准度关键的领域中评估其可靠性仍然具有挑战性。为解决这一问题，我们引入了BizFinBench，这是首个专门设计用于评估LLM在现实世界金融应用中的基准。BizFinBench包含6,781个中文标注查询，涵盖五个维度：数值计算、推理、信息抽取、预测识别和基于知识的问题回答，分为九个精细类别。该基准包括客观和主观指标。我们还引入了IteraJudge，这是一种新型的LLM评估方法，可以减少LLM作为客观指标评估者时的偏见。我们对25个模型进行了基准测试，包括自有和开源系统。广泛实验证明，没有一种模型在所有任务中都占主导地位。我们的评估揭示了不同的能力模式：(1) 在数值计算中，Claude-3.5-Sonnet (63.18) 和 DeepSeek-R1 (64.04) 领先，而较小的模型如Qwen2.5-VL-3B (15.92) 显著落后；(2) 在推理中，自有模型占主导地位（ChatGPT-o3: 83.58，Gemini-2.0-Flash: 81.15），开源模型落后多达19.49分；(3) 在信息抽取中，性能差异最大，DeepSeek-R1 得分71.46，而Qwen3-1.7B 得分11.23；(4) 在预测识别中，性能差异最小，顶级模型得分在39.16到50.00之间。我们发现，虽然当前的语言模型能够有效处理常规的金融查询，但在需要跨概念推理的复杂场景中却显得力不从心。BizFinBench 提供了一个严格的、与商业对齐的基准，供未来的研究使用。相关代码和数据集可在以下链接获得。', 'title_zh': 'BizFinBench：一个以业务为导向的现实世界金融基准，用于评估LLM'}
{'arxiv_id': 'arXiv:2505.19436', 'title': 'Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents', 'authors': 'Ye Ye', 'link': 'https://arxiv.org/abs/2505.19436', 'abstract': "Large Language Models (LLMs) falter in multi-step interactions -- often hallucinating, repeating actions, or misinterpreting user corrections -- due to reliance on linear, unstructured context. This fragility stems from the lack of persistent memory to track evolving goals and task dependencies, undermining trust in autonomous agents. We introduce the Task Memory Engine (TME), a modular memory controller that transforms existing LLMs into robust, revision-aware agents without fine-tuning. TME implements a spatial memory framework that replaces flat context with graph-based structures to support consistent, multi-turn reasoning. Departing from linear concatenation and ReAct-style prompting, TME builds a dynamic task graph -- either a tree or directed acyclic graph (DAG) -- to map user inputs to subtasks, align them with prior context, and enable dependency-tracked revisions. Its Task Representation and Intent Management (TRIM) component models task semantics and user intent to ensure accurate interpretation. Across four multi-turn scenarios-trip planning, cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100% of hallucinations and misinterpretations in three tasks, and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns, outperforming ReAct. TME's modular design supports plug-and-play deployment and domain-specific customization, adaptable to both personal assistants and enterprise automation. We release TME's codebase, benchmarks, and components as open-source resources, enabling researchers to develop reliable LLM agents. TME's scalable architecture addresses a critical gap in agent performance across complex, interactive settings.", 'abstract_zh': '大型语言模型在多步交互中表现脆弱——往往产生幻觉、重复操作或误解用户纠正——这是因为它们依赖于线性的非结构化上下文。这种脆弱性源自缺乏持续记忆来跟踪不断演变的目标和任务依赖性，从而破坏了对自主代理的信任。我们引入了任务记忆引擎（TME），这是一种模块化记忆控制器，能够无需微调即把现有LLM转变为稳健、修订感知的代理。TME实现了一种空间记忆框架，用基于图的结构替换平缓的上下文，以支持一致的多轮推理。TME不同于线性拼接和ReAct式的提示，构建了一个动态任务图——既可以是树结构，也可以是有向无环图（DAG），将用户输入映射到子任务，与先前的上下文对齐，并支持依赖性跟踪的修订。其任务表示和意图管理（TRIM）组件模型任务语义和用户意图，以确保准确解释。在四个多轮场景（旅行规划、烹饪、会议日程安排和购物车编辑）中，TME在三个任务中消除了100%的幻觉和误解，在27个用户轮次中将幻觉减少了66.7%，误解减少了83.3%，超过了ReAct。TME的模块化设计支持插件部署和特定领域的定制，适用于个人助理和企业自动化。我们发布了TME的代码库、基准测试和组件作为开源资源，使研究人员能够开发可靠的LLM代理。TME的可扩展架构跨复杂的交互设置解决了代理性能的关键缺口。', 'title_zh': '任务记忆引擎：稳健的多步LLM代理的空间记忆'}
{'arxiv_id': 'arXiv:2505.19402', 'title': 'Recalibrating the Compass: Integrating Large Language Models into Classical Research Methods', 'authors': 'Tai-Quan Peng, Xuzhen Yang', 'link': 'https://arxiv.org/abs/2505.19402', 'abstract': 'This paper examines how large language models (LLMs) are transforming core quantitative methods in communication research in particular, and in the social sciences more broadly-namely, content analysis, survey research, and experimental studies. Rather than replacing classical approaches, LLMs introduce new possibilities for coding and interpreting text, simulating dynamic respondents, and generating personalized and interactive stimuli. Drawing on recent interdisciplinary work, the paper highlights both the potential and limitations of LLMs as research tools, including issues of validity, bias, and interpretability. To situate these developments theoretically, the paper revisits Lasswell\'s foundational framework -- "Who says what, in which channel, to whom, with what effect?" -- and demonstrates how LLMs reconfigure message studies, audience analysis, and effects research by enabling interpretive variation, audience trajectory modeling, and counterfactual experimentation. Revisiting the metaphor of the methodological compass, the paper argues that classical research logics remain essential as the field integrates LLMs and generative AI. By treating LLMs not only as technical instruments but also as epistemic and cultural tools, the paper calls for thoughtful, rigorous, and imaginative use of LLMs in future communication and social science research.', 'abstract_zh': '这篇论文探讨了大型语言模型（LLMs）如何在沟通研究尤其在社会科学中转变核心定量方法，具体而言，即内容分析、调查研究和实验研究。与其替代经典方法，LLMs引入了编码和解释文本的新可能性，模拟动态受访者，并生成个性化和互动刺激。论文借助近期的跨学科研究，突显了LLMs作为研究工具的潜力与局限性，包括有效性和偏差等问题。为理论地放置这些发展，论文回顾了拉斯韦尔的基础框架——“谁说，说什么，在哪个渠道，对谁，有什么效果？”，并展示了LLMs如何重新配置信息研究、受众分析和效果研究，通过支持解释变体、受众轨迹建模和反事实实验。重新使用方法论指南针的隐喻，论文认为，随着领域整合LLMs和生成型AI，经典的研究逻辑仍然至关重要。将LLMs不仅视为技术工具，也是认识论与文化工具，论文呼吁在未来沟通与社会科学研究中，对LLMs进行深思熟虑、严谨和富有想象力的使用。', 'title_zh': '校准罗盘：将大型语言模型集成到经典研究方法中'}
{'arxiv_id': 'arXiv:2505.19383', 'title': 'CaseEdit: Enhancing Localized Commonsense Reasoning via Null-Space Constrained Knowledge Editing in Small Parameter Language Models', 'authors': 'Varun Reddy, Yen-Ling Kuo', 'link': 'https://arxiv.org/abs/2505.19383', 'abstract': 'Large language models (LLMs) exhibit strong performance on factual recall and general reasoning but struggle to adapt to user-specific, commonsense knowledge, a challenge particularly acute in small-parameter settings where computational efficiency is prioritized. We introduce CaseEdit, a new dataset and generation pipeline for evaluating localized, personalized commonsense knowledge editing in small LLMs to address this. Built upon the ATOMIC20/20 commonsense graph, CaseEdit uses a multi-stage inference process to generate both typical and atypical contextual edits for household objects, paired with targeted evaluation questions across four axes: reliability, generalization, locality, and portability. We evaluate established knowledge editing methods using CaseEdit and demonstrate that AlphaEdit, a technique employing null-space projection to minimize interference with unrelated knowledge, consistently outperforms other methods when applied to an LLaMA 3.2 3B model, even in scalability tests, showing minimal ripple effects. Our results indicate that using CaseEdit with effective editing techniques like AlphaEdit allows small models to internalize high-quality, context-sensitive common-sense knowledge, paving the way for lightweight, personalized assistants.', 'abstract_zh': '大型语言模型（LLMs）在事实回忆和通用推理方面表现出色，但在适应用户特定的常识知识方面存在挑战，特别是在参数量较小、计算效率优先的情况下这一挑战尤为严峻。我们引入了CaseEdit，一个新数据集及生成管道，用于评估小型LLM中局部化和个人化的常识知识编辑能力，以应对这一挑战。基于ATOMIC20/20常识图谱，CaseEdit使用多层次推理过程生成家用物体的典型和非典型上下文编辑，并配以四个维度的目标评估问题：可靠性、泛化能力、局部性和可移植性。我们使用CaseEdit评估现有的知识编辑方法，并证明使用零空间投影以最小化与无关知识的干扰的AlphaEdit技术，在应用于一个3.2B参数的LLaMA模型时，即使在可扩展性测试中也表现出更优性能，且几乎没有次生影响。我们的研究结果表明，使用CaseEdit和有效的编辑技术如AlphaEdit，可以使小型模型内化高质量的上下文敏感常识知识，为轻量级、个性化助手的发展铺平道路。', 'title_zh': 'CaseEdit: 借助 null 空间约束知识编辑增强小参数语言模型的局部常识推理'}
{'arxiv_id': 'arXiv:2505.19371', 'title': 'Foundations of Top-$k$ Decoding For Language Models', 'authors': 'Georgy Noarov, Soham Mallick, Tao Wang, Sunay Joshi, Yan Sun, Yangxinyu Xie, Mengxin Yu, Edgar Dobriban', 'link': 'https://arxiv.org/abs/2505.19371', 'abstract': 'Top-$k$ decoding is a widely used method for sampling from LLMs: at each token, only the largest $k$ next-token-probabilities are kept, and the next token is sampled after re-normalizing them to sum to unity. Top-$k$ and other sampling methods are motivated by the intuition that true next-token distributions are sparse, and the noisy LLM probabilities need to be truncated. However, to our knowledge, a precise theoretical motivation for the use of top-$k$ decoding is missing. In this work, we develop a theoretical framework that both explains and generalizes top-$k$ decoding. We view decoding at a fixed token as the recovery of a sparse probability distribution. We consider \\emph{Bregman decoders} obtained by minimizing a separable Bregman divergence (for both the \\emph{primal} and \\emph{dual} cases) with a sparsity-inducing $\\ell_0$ regularization. Despite the combinatorial nature of the objective, we show how to optimize it efficiently for a large class of divergences. We show that the optimal decoding strategies are greedy, and further that the loss function is discretely convex in $k$, so that binary search provably and efficiently finds the optimal $k$. We show that top-$k$ decoding arises as a special case for the KL divergence, and identify new decoding strategies that have distinct behaviors (e.g., non-linearly up-weighting larger probabilities after re-normalization).', 'abstract_zh': 'Top-$k$ 解码是广泛用于从大规模语言模型采样的方法：在每个标记处，仅保留最大的 $k$ 个下一个标记概率，并在重新归一化使它们之和为1后进行采样。Top-$k$ 解码和其他采样方法是基于直觉，即真实的下一个标记分布是稀疏的，而嘈杂的大规模语言模型概率需要截断。然而，据我们所知，缺乏对使用Top-$k$ 解码的精确理论动机。在本文中，我们开发了一个理论框架，该框架不仅解释了Top-$k$ 解码，还对其进行了推广。我们将固定标记处的解码视为稀疏概率分布的恢复。我们考虑通过最小化分离性Bregman发散（对于原问题和对偶问题两种情况）并带有稀疏性诱导的$\\ell_0$正则化得到的Bregman解码器。尽管目标函数具有组合性质，我们展示了如何高效地优化其为一大类发散的情形。我们证明了最优的解码策略是贪婪的，并且损失函数在 $k$ 上二阶离散凸，因此二分搜索可以证明地高效找到最优的 $k$。我们证明了Top-$k$ 解码在KL散度情况下作为特殊情形出现，并识别出具有不同行为的新解码策略（例如，在重新归一化后非线性地加重较大概率的权重）。', 'title_zh': 'Top-k 解码语言模型的理论基础'}
{'arxiv_id': 'arXiv:2505.19353', 'title': 'Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation', 'authors': 'Camilo Chacón Sartori', 'link': 'https://arxiv.org/abs/2505.19353', 'abstract': "With the rise of generative AI (GenAI), Large Language Models are increasingly employed for code generation, becoming active co-authors alongside human programmers. Focusing specifically on this application domain, this paper articulates distinct ``Architectures of Error'' to ground an epistemic distinction between human and machine code generation. Examined through their shared vulnerability to error, this distinction reveals fundamentally different causal origins: human-cognitive versus artificial-stochastic. To develop this framework and substantiate the distinction, the analysis draws critically upon Dennett's mechanistic functionalism and Rescher's methodological pragmatism. I argue that a systematic differentiation of these error profiles raises critical philosophical questions concerning semantic coherence, security robustness, epistemic limits, and control mechanisms in human-AI collaborative software development. The paper also utilizes Floridi's levels of abstraction to provide a nuanced understanding of how these error dimensions interact and may evolve with technological advancements. This analysis aims to offer philosophers a structured framework for understanding GenAI's unique epistemological challenges, shaped by these architectural foundations, while also providing software engineers a basis for more critically informed engagement.", 'abstract_zh': '随着生成型人工智能（GenAI）的兴起，大型语言模型在代码生成中被越来越多地应用，成为与人类程序员并肩工作的活跃合作者。本文专注于这一应用领域，提出独特的“错误架构”，以阐明人类与机器代码生成在认识论上的区别。通过考察它们在错误上的共同脆弱性，这一区别揭示了根本不同的因果来源：人认知的与人工随机的。为了发展这一框架并证实这种区分，分析批判性地借鉴了丹内特的机械功能主义和雷舍的方法论实用主义。我认为，系统地区分这些错误特征提出了关于语义连贯性、安全性可靠性、认识论局限性和控制机制在人机协同软件开发中的关键哲学问题。本文还利用弗洛里迪的抽象层次来提供一个详细的理解，解释这些错误维度如何相互作用以及如何随着技术进步而演变。这篇分析旨在为哲学家提供一个结构化的框架来理解受这些架构基础影响的GenAI的独特认识论挑战，同时也为软件工程师提供一个更批判性参与的基础。', 'title_zh': '错误的架构：关于AI与人类代码生成的哲学探究'}
{'arxiv_id': 'arXiv:2505.19333', 'title': 'Evaluating Steering Techniques using Human Similarity Judgments', 'authors': 'Zach Studdiford, Timothy T. Rogers, Siddharth Suresh, Kushin Mukherjee', 'link': 'https://arxiv.org/abs/2505.19333', 'abstract': "Current evaluations of Large Language Model (LLM) steering techniques focus on task-specific performance, overlooking how well steered representations align with human cognition. Using a well-established triadic similarity judgment task, we assessed steered LLMs on their ability to flexibly judge similarity between concepts based on size or kind. We found that prompt-based steering methods outperformed other methods both in terms of steering accuracy and model-to-human alignment. We also found LLMs were biased towards 'kind' similarity and struggled with 'size' alignment. This evaluation approach, grounded in human cognition, adds further support to the efficacy of prompt-based steering and reveals privileged representational axes in LLMs prior to steering.", 'abstract_zh': '当前对大型语言模型（LLM）引导技术的评估主要关注任务特定性能，忽视了引导表示与人类认知的一致性。通过使用广泛认可的三元相似性判断任务，我们评估了引导的LLM在基于大小或种类灵活判断概念相似性方面的能力。我们发现基于提示的引导方法在引导准确性和模型与人类的一致性方面优于其他方法。我们还发现LLM倾向于“种类”相似性，并在“大小”对齐方面遇到困难。这种基于人类认知的评估方法为进一步支持基于提示的引导的有效性提供了更多证据，并揭示了在引导之前LLM的特权表示轴。', 'title_zh': '基于人类相似性判断评价转向技术'}
{'arxiv_id': 'arXiv:2505.19266', 'title': "Using Large Language Models to Assess Teachers' Pedagogical Content Knowledge", 'authors': 'Yaxuan Yang, Shiyu Wang, Xiaoming Zhai', 'link': 'https://arxiv.org/abs/2505.19266', 'abstract': "Assessing teachers' pedagogical content knowledge (PCK) through performance-based tasks is both time and effort-consuming. While large language models (LLMs) offer new opportunities for efficient automatic scoring, little is known about whether LLMs introduce construct-irrelevant variance (CIV) in ways similar to or different from traditional machine learning (ML) and human raters. This study examines three sources of CIV -- scenario variability, rater severity, and rater sensitivity to scenario -- in the context of video-based constructed-response tasks targeting two PCK sub-constructs: analyzing student thinking and evaluating teacher responsiveness. Using generalized linear mixed models (GLMMs), we compared variance components and rater-level scoring patterns across three scoring sources: human raters, supervised ML, and LLM. Results indicate that scenario-level variance was minimal across tasks, while rater-related factors contributed substantially to CIV, especially in the more interpretive Task II. The ML model was the most severe and least sensitive rater, whereas the LLM was the most lenient. These findings suggest that the LLM contributes to scoring efficiency while also introducing CIV as human raters do, yet with varying levels of contribution compared to supervised ML. Implications for rater training, automated scoring design, and future research on model interpretability are discussed.", 'abstract_zh': '通过基于性能的任务评估教师的教学内容知识（PCK）既耗时又耗力。尽管大型语言模型（LLMs）提供了高效自动评分的新机遇，但尚不清楚LLMs是否以与传统机器学习（ML）和人工评分者类似或不同的方式引入无关构念变异（CIV）。本研究在基于视频的建构性反应任务中，针对两个PCK子领域——分析学生思维和评价教师反应性，探讨了三种CIV来源——情境变异、评分者严厉度和评分者对情境的敏感度。通过广义线性混合效应模型（GLMMs），我们比较了三种评分来源（人工评分者、监督机器学习和LLM）的方差成分和评分者级评分模式。结果显示，任务层面的变异在各任务中相对较低，而与评分者相关的因素对CIV的贡献较大，尤其是在更具解释性的任务II中。监督机器学习模型是最严厉且对情境最不敏感的评分者，而LLM是最宽容的。这些发现表明，尽管LLM提高了评分效率，但在引入CIV方面的作用类似于人工评分者，但在贡献程度上与监督机器学习有所不同。讨论了评分者培训、自动化评分设计以及未来关于模型可解释性研究的含义。', 'title_zh': '使用大型语言模型评估教师的学科教学知识'}
{'arxiv_id': 'arXiv:2505.19234', 'title': 'GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling', 'authors': 'Jialong Zhou, Lichao Wang, Xiao Yang', 'link': 'https://arxiv.org/abs/2505.19234', 'abstract': "The emergence of large language models (LLMs) enables the development of intelligent agents capable of engaging in complex and multi-turn dialogues. However, multi-agent collaboration face critical safety challenges, such as hallucination amplification and error injection and propagation. This paper presents GUARDIAN, a unified method for detecting and mitigating multiple safety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the multi-agent collaboration process as a discrete-time temporal attributed graph, GUARDIAN explicitly captures the propagation dynamics of hallucinations and errors. The unsupervised encoder-decoder architecture incorporating an incremental training paradigm, learns to reconstruct node attributes and graph structures from latent embeddings, enabling the identification of anomalous nodes and edges with unparalleled precision. Moreover, we introduce a graph abstraction mechanism based on the Information Bottleneck Theory, which compresses temporal interaction graphs while preserving essential patterns. Extensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM multi-agent collaborations against diverse safety vulnerabilities, achieving state-of-the-art accuracy with efficient resource utilization.", 'abstract_zh': '大型语言模型（LLMs）的出现使得开发能够进行复杂多轮对话的智能代理成为可能。然而，多智能体协作面临着 Critical Safety Challenges，如幻觉放大和错误注入与传播。本文提出 GUARDIAN，一种统一的方法，用于在多智能体协作中检测和缓解多种安全问题。通过将多智能体协作过程建模为离散时间时变归属性图，GUARDIAN 明确捕获了幻觉和错误的传播动力学。结合增量训练范式的无监督编码器-解码器架构，能够从潜在嵌入中重建节点属性和图结构，从而以无与伦比的精度识别异常节点和边。此外，我们引入了基于信息瓶颈理论的图抽象机制，该机制在保持关键模式的同时压缩了时间交互图。广泛实验表明，GUARDIAN 在保护大型语言模型多智能体协作免受多种安全漏洞方面具有有效性，同时实现了高效的资源利用和最先进准确率。', 'title_zh': 'GUARDIAN: 时空图建模保障大型语言模型多 agents 合作安全'}
{'arxiv_id': 'arXiv:2505.19173', 'title': 'Investigating Pedagogical Teacher and Student LLM Agents: Genetic Adaptation Meets Retrieval Augmented Generation Across Learning Style', 'authors': 'Debdeep Sanyal, Agniva Maiti, Umakanta Maharana, Dhruv Kumar, Ankur Mali, C. Lee Giles, Murari Mandal', 'link': 'https://arxiv.org/abs/2505.19173', 'abstract': "Effective teaching requires adapting instructional strategies to accommodate the diverse cognitive and behavioral profiles of students, a persistent challenge in education and teacher training. While Large Language Models (LLMs) offer promise as tools to simulate such complex pedagogical environments, current simulation frameworks are limited in two key respects: (1) they often reduce students to static knowledge profiles, and (2) they lack adaptive mechanisms for modeling teachers who evolve their strategies in response to student feedback. To address these gaps, \\textbf{we introduce a novel simulation framework that integrates LLM-based heterogeneous student agents with a self-optimizing teacher agent}. The teacher agent's pedagogical policy is dynamically evolved using a genetic algorithm, allowing it to discover and refine effective teaching strategies based on the aggregate performance of diverse learners. In addition, \\textbf{we propose Persona-RAG}, a Retrieval Augmented Generation module that enables student agents to retrieve knowledge tailored to their individual learning styles. Persona-RAG preserves the retrieval accuracy of standard RAG baselines while enhancing personalization, an essential factor in modeling realistic educational scenarios. Through extensive experiments, we demonstrate how our framework supports the emergence of distinct and interpretable teaching patterns when interacting with varied student populations. Our results highlight the potential of LLM-driven simulations to inform adaptive teaching practices and provide a testbed for training human educators in controlled, data-driven environments.", 'abstract_zh': '一种将基于大语言模型的异质学生代理与自优化教师代理集成的新型仿真框架： Persona-RAG仿真模块的应用', 'title_zh': '调查教学型教师和学生大语言模型代理：遗传适应遇Retrieve Augmented Generation Across Learning Style'}
{'arxiv_id': 'arXiv:2505.19165', 'title': 'OrgAccess: A Benchmark for Role Based Access Control in Organization Scale LLMs', 'authors': 'Debdeep Sanyal Umakanta Maharana, Yash Sinha, Hong Ming Tan, Shirish Karande, Mohan Kankanhalli, Murari Mandal', 'link': 'https://arxiv.org/abs/2505.19165', 'abstract': "Role-based access control (RBAC) and hierarchical structures are foundational to how information flows and decisions are made within virtually all organizations. As the potential of Large Language Models (LLMs) to serve as unified knowledge repositories and intelligent assistants in enterprise settings becomes increasingly apparent, a critical, yet under explored, challenge emerges: \\textit{can these models reliably understand and operate within the complex, often nuanced, constraints imposed by organizational hierarchies and associated permissions?} Evaluating this crucial capability is inherently difficult due to the proprietary and sensitive nature of real-world corporate data and access control policies. We introduce a synthetic yet representative \\textbf{OrgAccess} benchmark consisting of 40 distinct types of permissions commonly relevant across different organizational roles and levels. We further create three types of permissions: 40,000 easy (1 permission), 10,000 medium (3-permissions tuple), and 20,000 hard (5-permissions tuple) to test LLMs' ability to accurately assess these permissions and generate responses that strictly adhere to the specified hierarchical rules, particularly in scenarios involving users with overlapping or conflicting permissions. Our findings reveal that even state-of-the-art LLMs struggle significantly to maintain compliance with role-based structures, even with explicit instructions, with their performance degrades further when navigating interactions involving two or more conflicting permissions. Specifically, even \\textbf{GPT-4.1 only achieves an F1-Score of 0.27 on our hardest benchmark}. This demonstrates a critical limitation in LLMs' complex rule following and compositional reasoning capabilities beyond standard factual or STEM-based benchmarks, opening up a new paradigm for evaluating their fitness for practical, structured environments.", 'abstract_zh': '基于角色的访问控制（RBAC）和层次结构是信息流动和决策制定的基础，几乎适用于所有组织。随着大型语言模型（LLMs）在企业环境中作为统一知识库和智能助手的潜力越来越明显，一个关键但未被充分探索的挑战出现了：这些模型能否可靠地理解并操作由组织层次结构及其相关权限所施加的复杂且常常是微妙的限制？由于真实世界企业数据和访问控制政策的专有性和敏感性，评估这一关键能力本身是困难的。我们引入了一个合成但具有代表性的OrgAccess基准，其中包括40种在不同组织角色和层级中普遍相关的权限类型。我们进一步创建了三种类型的权限：40,000个容易的（1个权限）、10,000个中等难度的（3个权限元组）和20,000个困难的（5个权限元组），以测试LLMs准确评估这些权限并生成严格遵守指定层级规则的响应的能力，特别是在用户权限交叉或冲突的情景下。我们的研究结果表明，即使是最先进的LLMs，在有明确指令的情况下，也难以维持与基于角色的结构的一致性，当涉及两个或更多冲突的权限时，其性能进一步下降。具体来说，\\textbf{GPT-4.1仅在我们最难的基准上获得0.27的F1-Score}。这展示了LLMs在遵循复杂规则和组合推理能力方面的一个关键局限性，超过了标准的事实性或STEM基准，为评估其在实际结构性环境中的适用性开辟了新的范式。', 'title_zh': 'OrgAccess: 一个基于组织规模语言模型角色访问控制的基准'}
{'arxiv_id': 'arXiv:2505.19092', 'title': 'Reinforced Latent Reasoning for LLM-based Recommendation', 'authors': 'Yang Zhang, Wenxin Xu, Xiaoyan Zhao, Wenjie Wang, Fuli Feng, Xiangnan He, Tat-Seng Chua', 'link': 'https://arxiv.org/abs/2505.19092', 'abstract': 'Large Language Models (LLMs) have demonstrated impressive reasoning capabilities in complex problem-solving tasks, sparking growing interest in their application to preference reasoning in recommendation systems. Existing methods typically rely on fine-tuning with explicit chain-of-thought (CoT) data. However, these methods face significant practical limitations due to (1) the difficulty of obtaining high-quality CoT data in recommendation and (2) the high inference latency caused by generating CoT reasoning. In this work, we explore an alternative approach that shifts from explicit CoT reasoning to compact, information-dense latent reasoning. This approach eliminates the need for explicit CoT generation and improves inference efficiency, as a small set of latent tokens can effectively capture the entire reasoning process. Building on this idea, we propose $\\textit{\\underline{R}einforced \\underline{Latent} \\underline{R}easoning for \\underline{R}ecommendation}$ (LatentR$^3$), a novel end-to-end training framework that leverages reinforcement learning (RL) to optimize latent reasoning without relying on any CoT this http URL$^3$ adopts a two-stage training strategy: first, supervised fine-tuning to initialize the latent reasoning module, followed by pure RL training to encourage exploration through a rule-based reward design. Our RL implementation is based on a modified GRPO algorithm, which reduces computational overhead during training and introduces continuous reward signals for more efficient learning. Extensive experiments demonstrate that LatentR$^3$ enables effective latent reasoning without any direct supervision of the reasoning process, significantly improving performance when integrated with different LLM-based recommendation methods. Our codes are available at this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在复杂问题解决任务中展示了令人印象深刻的推理能力，引发了对其在推荐系统中偏好推理应用的兴趣。现有方法通常依赖于显式的推理链（CoT）数据进行微调。然而，这些方法由于（1）在推荐中获取高质量CoT数据的难度，以及（2）生成推理链推理导致的高推理延迟而面临重大实践限制。在本文中，我们探索了一种替代方法，从显式的CoT推理转向紧凑且信息密集的潜在推理。该方法消除了显式CoT生成的需要，并提高了推理效率，因为少量的潜在令牌就可以有效地捕捉整个推理过程。基于这一想法，我们提出了一种名为$\\textit{\\underline{R}einforced \\underline{Latent} \\underline{R}easoning for \\underline{R}ecommendation}$（LatentR$^3$）的新颖端到端训练框架，该框架利用强化学习（RL）来优化潜在推理，而无需依赖任何CoT。LatentR$^3$采用两阶段训练策略：首先进行监督微调以初始化潜在推理模块，然后进行纯RL训练以通过基于规则的奖励设计鼓励探索。我们的RL实现基于修改后的GRPO算法，该算法在训练过程中减少了计算开销，并引入了连续的奖励信号以实现更高效的学习。大量实验证明，LatentR$^3$能够在不直接监督推理过程的情况下实现有效的潜在推理，并且在与不同的基于LLM的推荐方法集成时显著提高了性能。我们的代码可在以下链接获取：this https URL。', 'title_zh': '基于LLM的推荐系统的强化潜推理'}
{'arxiv_id': 'arXiv:2505.19075', 'title': 'Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs', 'authors': 'Jaemin Kim, Hangeol Chang, Hyunmin Hwang, Choonghan Kim, Jong Chul Ye', 'link': 'https://arxiv.org/abs/2505.19075', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable general capabilities, but enhancing skills such as reasoning often demands substantial computational resources and may compromise their generalization. While Parameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious alternative, they typically requires retraining for each LLM backbone due to architectural dependencies. To address these challenges, here we propose Universal Reasoner (UniR) - a single, lightweight, composable, and plug-and-play reasoning module that can be used with any frozen LLM to endow it with specialized reasoning capabilities. Specifically, UniR decomposes the reward into a standalone reasoning module that is trained independently using predefined rewards, effectively translating trajectory-level signals into token-level guidance. Once trained, UniR can be combined with any frozen LLM at inference time by simply adding its output logits to those of the LLM backbone. This additive structure naturally enables modular composition: multiple UniR modules trained for different tasks can be jointly applied by summing their logits, enabling complex reasoning via composition. Experimental results on mathematical reasoning and machine translation tasks show that UniR significantly outperforms \\add{existing baseline fine-tuning methods using the Llama3.2 model}. Furthermore, UniR demonstrates strong weak-to-strong generalization: reasoning modules trained on smaller models effectively guide much larger LLMs. This makes UniR a cost-efficient, adaptable, and robust solution for enhancing reasoning in LLMs without compromising their core capabilities. Code is open-sourced at this https URL', 'abstract_zh': '大型语言模型（LLMs）展示了卓越的一般能力，但增强诸如推理等技能往往需要大量计算资源，可能会损害其泛化能力。尽管参数高效微调（PEFT）方法提供了一种更为资源节约的选择，但它们通常需要重新训练每个LLM骨干，这是因为架构依赖性。为了解决这些问题，我们提出了一个单一的、轻量级、可组合且即插即用的推理模块——Universal Reasoner（UniR），它可以与任何冻结的LLM结合使用，赋予其特定的推理能力。具体来说，UniR 将奖励分解为一个独立的推理模块，该模块通过预定义的奖励独立训练，有效地将轨迹级信号转化为标记级指导。训练完成后，UniR 可以在推理时通过简单地将其输出逻辑添加到LLM骨干的逻辑中与任何冻结的LLM结合使用。这种加法结构自然地支持模块化组合：不同任务训练的不同UniR模块可以通过求和其逻辑共同应用，从而通过组合实现复杂的推理。在数学推理和机器翻译任务上的实验结果表明，UniR 显著优于使用Llama3.2模型的现有基线微调方法。此外，UniR 展现出强大的弱到强泛化能力：在较小模型上训练的推理模块能够有效地引导更大的LLM。这使得UniR 成为一种成本效益高、适应性强且稳健的解决方案，能够在不损害LLM核心能力的情况下增强其推理能力。代码在此处开源。', 'title_zh': '通用推理器：用于冻结的大型语言模型的单个可组合即插即用推理器'}
{'arxiv_id': 'arXiv:2505.19030', 'title': "RECAST: Strengthening LLMs' Complex Instruction Following with Constraint-Verifiable Data", 'authors': 'Wenhao Liu, Zhengkang Guo, Mingchen Xie, Jingwen Xu, Zisu Huang, Muzhao Tian, Jianhan Xu, Muling Wu, Xiaohua Wang, Changze Lv, He-Da Wang, Hu Yao, Xiaoqing Zheng, Xuanjing Huang', 'link': 'https://arxiv.org/abs/2505.19030', 'abstract': "Large language models (LLMs) are increasingly expected to tackle complex tasks, driven by their expanding applications and users' growing proficiency in crafting sophisticated prompts. However, as the number of explicitly stated requirements increases (particularly more than 10 constraints), LLMs often struggle to accurately follow such complex instructions. To address this challenge, we propose RECAST, a novel framework for synthesizing datasets where each example incorporates far more constraints than those in existing benchmarks. These constraints are extracted from real-world prompt-response pairs to ensure practical relevance. RECAST enables automatic verification of constraint satisfaction via rule-based validators for quantitative constraints and LLM-based validators for qualitative ones. Using this framework, we construct RECAST-30K, a large-scale, high-quality dataset comprising 30k instances spanning 15 constraint types. Experimental results demonstrate that models fine-tuned on RECAST-30K show substantial improvements in following complex instructions. Moreover, the verifiability provided by RECAST enables the design of reward functions for reinforcement learning, which further boosts model performance on complex and challenging tasks.", 'abstract_zh': '大型语言模型（LLMs）越来越多地被期望处理复杂的任务，这得益于它们的应用范围不断扩大以及用户在构建复杂提示方面的日渐熟练。然而，当显式要求的数量增加（特别是超过10个约束条件时），LLMs往往会难以准确遵循这些复杂的指令。为此，我们提出RECAST，这是一个新颖的框架，用于合成每个示例包含远多于现有基准约束数据集。这些约束是从真实的提示-响应对中提取的，以确保其实用相关性。RECAST通过基于规则验证器进行定量约束的自动验证，以及通过基于LLM的验证器进行定性约束的验证，实现了对约束满足情况的自动验证。利用此框架，我们构建了RECAST-30K，这是一个包含30,000个实例、覆盖15种约束类型的大型高质量数据集。实验结果表明，基于RECAST-30K微调的模型在遵循复杂指令方面表现出显著改进。此外，RECAST提供的可验证性可以为强化学习设计奖励函数，从而进一步提升模型在复杂和具有挑战性任务上的性能。', 'title_zh': 'RECAST: 通过可验证约束数据增强大型语言模型的复杂指令跟随能力'}
{'arxiv_id': 'arXiv:2505.19003', 'title': 'Aligning LLM with human travel choices: a persona-based embedding learning approach', 'authors': 'Tianming Liu, Manzi Li, Yafeng Yin', 'link': 'https://arxiv.org/abs/2505.19003', 'abstract': 'The advent of large language models (LLMs) presents new opportunities for travel demand modeling. However, behavioral misalignment between LLMs and humans presents obstacles for the usage of LLMs, and existing alignment methods are frequently inefficient or impractical given the constraints of typical travel demand data. This paper introduces a novel framework for aligning LLMs with human travel choice behavior, tailored to the current travel demand data sources. Our framework uses a persona inference and loading process to condition LLMs with suitable prompts to enhance alignment. The inference step establishes a set of base personas from empirical data, and a learned persona loading function driven by behavioral embeddings guides the loading process. We validate our framework on the Swissmetro mode choice dataset, and the results show that our proposed approach significantly outperformed baseline choice models and LLM-based simulation models in predicting both aggregate mode choice shares and individual choice outcomes. Furthermore, we showcase that our framework can generate insights on population behavior through interpretable parameters. Overall, our research offers a more adaptable, interpretable, and resource-efficient pathway to robust LLM-based travel behavior simulation, paving the way to integrate LLMs into travel demand modeling practice in the future.', 'abstract_zh': '大型语言模型（LLMs）的出现为旅行需求建模带来了新的机会。然而，LLMs与人类的行为错位为LLMs的应用设定了障碍，现有对齐方法在典型旅行需求数据的约束下往往效率低下或不切实际。本文提出了一种针对当前旅行需求数据源的人工智能旅行选择行为对齐框架。该框架采用个性推断和加载过程，通过合适的提示对LLMs进行调整，以增强对齐效果。推断步骤从实证数据中建立一组基个性，而由行为嵌入驱动的学习个性加载函数则指导加载过程。我们使用瑞士metro模式选择数据集验证了该框架，结果表明，所提出的方法在预测总体模式选择份额和个体选择结果方面显著优于基准选择模型和基于LLMs的模拟模型。此外，我们展示了该框架可以通过可解释的参数生成关于人群行为的见解。总体而言，我们的研究提供了更具适应性、可解释性和资源效率的LLM驱动的旅行行为模拟途径，为将来将LLMs整合到旅行需求建模实践中铺平了道路。', 'title_zh': '基于个性嵌入学习的方法使语言模型与人类旅行选择对齐'}
{'arxiv_id': 'arXiv:2505.18961', 'title': 'Weaver: Interweaving SQL and LLM for Table Reasoning', 'authors': 'Rohit Khoja, Devanshu Gupta, Yanjie Fu, Dan Roth, Vivek Gupta', 'link': 'https://arxiv.org/abs/2505.18961', 'abstract': 'Querying tables with unstructured data is challenging due to the presence of text (or image), either embedded in the table or in external paragraphs, which traditional SQL struggles to process, especially for tasks requiring semantic reasoning. While Large Language Models (LLMs) excel at understanding context, they face limitations with long input sequences. Existing approaches that combine SQL and LLMs typically rely on rigid, predefined work-flows, limiting their adaptability to complex queries. To address these issues, we introduce Weaver , a modular pipeline that dynamically integrates SQL and LLMs for table-based question answering (TableQA). Weaver generates a flexible, step-by-step plan that combines SQL for structured data retrieval with LLMs for semantic processing. By decomposing complex queries into manageable subtasks, Weaver improves accuracy and generalization. Our experiments show that Weaver consistently outperforms state-of-the-art methods across four TableQA datasets, reducing both API calls and error rates.', 'abstract_zh': '使用非结构化数据查询表格具有挑战性，因为表格中或外部段落中可能存在文本（或图像），传统SQL难以处理，特别是在需要语义推理的任务中。尽管大规模语言模型在理解上下文方面表现出色，它们在处理长输入序列时存在局限性。现有将SQL与大规模语言模型相结合的方法通常依赖于刚性、预定义的工作流程，限制了其对复杂查询的适应性。为解决这些问题，我们提出了一种模块化流水线Weaver，动态整合SQL和大规模语言模型以进行基于表格的问题回答（TableQA）。Weaver生成一个灵活的、逐步的计划，结合SQL用于结构化数据检索与大规模语言模型用于语义处理。通过将复杂查询分解为可管理的子任务，Weaver提高了准确性和泛化能力。我们的实验结果显示，Weaver在四个TableQA数据集上始终优于最先进的方法，减少了API调用次数和错误率。', 'title_zh': 'Weaver: 将SQL与LLM交织用于表格推理'}
{'arxiv_id': 'arXiv:2505.18933', 'title': 'REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing', 'authors': 'Haitian Zhong, Yuhuan Liu, Ziyang Xu, Guofan Liu, Qiang Liu, Shu Wu, Zhe Zhao, Liang Wang, Tieniu Tan', 'link': 'https://arxiv.org/abs/2505.18933', 'abstract': 'Large language model editing methods frequently suffer from overfitting, wherein factual updates can propagate beyond their intended scope, overemphasizing the edited target even when it\'s contextually inappropriate. To address this challenge, we introduce REACT (Representation Extraction And Controllable Tuning), a unified two-phase framework designed for precise and controllable knowledge editing. In the initial phase, we utilize tailored stimuli to extract latent factual representations and apply Principal Component Analysis with a simple learnbale linear transformation to compute a directional "belief shift" vector for each instance. In the second phase, we apply controllable perturbations to hidden states using the obtained vector with a magnitude scalar, gated by a pre-trained classifier that permits edits only when contextually necessary. Relevant experiments on EVOKE benchmarks demonstrate that REACT significantly reduces overfitting across nearly all evaluation metrics, and experiments on COUNTERFACT and MQuAKE shows that our method preserves balanced basic editing performance (reliability, locality, and generality) under diverse editing scenarios.', 'abstract_zh': '大规模语言模型编辑方法 Often Suffer from Overfitting, wherein Factual Updates Can Propagate Beyond Their Intended Scope, Overemphasizing the Edited Target Even When Contextually Inappropriate. To Address This Challenge, We Introduce REACT (Representation Extraction And Controllable Tuning), a Unified Two-Phase Framework for Precise and Controllable Knowledge Editing.', 'title_zh': 'REACT: 表征提取与可控调优以克服大语言模型知识编辑中的过拟合'}
{'arxiv_id': 'arXiv:2505.18931', 'title': 'Can Large Language Models Infer Causal Relationships from Real-World Text?', 'authors': 'Ryan Saklad, Aman Chadha, Oleg Pavlov, Raha Moraffah', 'link': 'https://arxiv.org/abs/2505.18931', 'abstract': 'Understanding and inferring causal relationships from texts is a core aspect of human cognition and is essential for advancing large language models (LLMs) towards artificial general intelligence. Existing work primarily focuses on synthetically generated texts which involve simple causal relationships explicitly mentioned in the text. This fails to reflect the complexities of real-world tasks. In this paper, we investigate whether LLMs are capable of inferring causal relationships from real-world texts. We develop a benchmark drawn from real-world academic literature which includes diverse texts with respect to length, complexity of relationships (different levels of explicitness, number of events, and causal relationships), and domains and sub-domains. To the best of our knowledge, our benchmark is the first-ever real-world dataset for this task. Our experiments on state-of-the-art LLMs evaluated on our proposed benchmark demonstrate significant challenges, with the best-performing model achieving an average F1 score of only 0.477. Analysis reveals common pitfalls: difficulty with implicitly stated information, in distinguishing relevant causal factors from surrounding contextual details, and with connecting causally relevant information spread across lengthy textual passages. By systematically characterizing these deficiencies, our benchmark offers targeted insights for further research into advancing LLM causal reasoning.', 'abstract_zh': '理解并从文本中推断因果关系是人类认知的核心方面，对于推动大型语言模型（LLMs）向通用人工智能发展至关重要。现有工作主要集中在包含简单显式因果关系的合成文本上。这未能反映实际任务的复杂性。在本文中，我们探讨LLMs是否能够从实际文本中推断因果关系。我们开发了一个基于实际学术文献的基准数据集，包括在长度、关系复杂性（不同程度的显式性、事件数量和因果关系）、领域及其子领域方面的多样文本。据我们所知，这是首个用于此类任务的实际世界数据集。我们在我们提出的基准数据集上评估的最新大型语言模型的实验结果显示出显著挑战，最优模型的平均F1分数仅为0.477。分析揭示了常见的陷阱：处理隐含信息的难度、区分相关因果因素与周围背景细节的难度，以及连接分散在长文本段落中的因果相关信息的难度。通过系统地分析这些缺陷，我们的基准数据集为进一步研究提升LLM因果推理能力提供了有针对性的见解。', 'title_zh': '大型语言模型能否从真实世界文本中推断因果关系？'}
{'arxiv_id': 'arXiv:2505.18929', 'title': 'Meta-aware Learning in text-to-SQL Large Language Model', 'authors': 'Wenda Zhang', 'link': 'https://arxiv.org/abs/2505.18929', 'abstract': 'The advancements of Large language models (LLMs) have provided great opportunities to text-to-SQL tasks to overcome the main challenges to understand complex domain information and complex database structures in business applications. In this paper, we propose a meta-aware learning framework to integrate domain knowledge, database schema, chain-of-thought reasoning processes, and metadata relationships to improve the SQL generation quality. The proposed framework includes four learning strategies: schema-based learning, Chain-of-Thought (CoT) learning, knowledge-enhanced learning, and key information tokenization. This approach provides a comprehensive understanding of database structure and metadata information towards LLM through fine-tuning to improve its performance on SQL generation within business domains. Through two experimental studies, we have demonstrated the superiority of the proposed methods in execution accuracy, multi-task SQL generation capability, and reduction of catastrophic forgetting.', 'abstract_zh': '大型语言模型的进步为文本到SQL任务带来了巨大的机会，以克服理解业务应用中复杂领域信息和复杂数据库结构的主要挑战。本文提出了一种元感知学习框架，该框架结合领域知识、数据库模式、链式思考推理过程和元数据关系，以提高SQL生成质量。该提出的框架包括四种学习策略：基于模式的学习、链式思考学习、知识增强学习和关键信息词符化。该方法通过微调提供了一种全面理解数据库结构和元数据信息的方法，以提高大型语言模型在业务领域内的SQL生成性能。通过两个实验研究，我们证明了所提出方法在执行准确性、多任务SQL生成能力和灾难性遗忘减少方面的优越性。', 'title_zh': '元感知学习在文本到SQL大规模语言模型中的应用'}
{'arxiv_id': 'arXiv:2505.18907', 'title': 'Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations', 'authors': 'Sanjay Kariyappa, G. Edward Suh', 'link': 'https://arxiv.org/abs/2505.18907', 'abstract': "Prompt injection attacks are a critical security vulnerability in large language models (LLMs), allowing attackers to hijack model behavior by injecting malicious instructions within the input context. Recent defense mechanisms have leveraged an Instruction Hierarchy (IH) Signal, often implemented through special delimiter tokens or additive embeddings to denote the privilege level of input tokens. However, these prior works typically inject the IH signal exclusively at the initial input layer, which we hypothesize limits its ability to effectively distinguish the privilege levels of tokens as it propagates through the different layers of the model. To overcome this limitation, we introduce a novel approach that injects the IH signal into the intermediate token representations within the network. Our method augments these representations with layer-specific trainable embeddings that encode the privilege information. Our evaluations across multiple models and training methods reveal that our proposal yields between $1.6\\times$ and $9.2\\times$ reduction in attack success rate on gradient-based prompt injection attacks compared to state-of-the-art methods, without significantly degrading the model's utility.", 'abstract_zh': 'Prompt注入攻击是大规模语言模型中一个关键的安全漏洞，通过在输入上下文内注入恶意指令来劫持模型行为。近期的防御机制利用了指令层次结构（IH）信号，通常通过特殊分隔符令牌或增加嵌入来表示输入令牌的优先级水平。然而，这些先前的工作通常仅在初始输入层注入IH信号，我们认为这限制了其在模型不同层传播时有效区分令牌优先级的能力。为了克服这一局限，我们提出了一种新颖的方法，即将IH信号注入网络内的中间令牌表示中。我们的方法通过为这些表示添加特定于层的可训练嵌入来增强它们，这些嵌入编码优先级信息。在多个模型和训练方法上的评估结果显示，与最先进的方法相比，我们的提案在梯度基线的prompt注入攻击成功率上降低了1.6倍至9.2倍，且未显著降低模型的实用性。', 'title_zh': '通过增强中间表示来强化指令层级的执行'}
{'arxiv_id': 'arXiv:2505.18847', 'title': 'Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework', 'authors': 'William Han, Chaojing Duan, Zhepeng Cen, Yihang Yao, Xiaoyu Song, Atharva Mhaskar, Dylan Leong, Michael A. Rosenberg, Emerson Liu, Ding Zhao', 'link': 'https://arxiv.org/abs/2505.18847', 'abstract': 'Recent advances have increasingly applied large language models (LLMs) to electrocardiogram (ECG) interpretation, giving rise to Electrocardiogram-Language Models (ELMs). Conditioned on an ECG and a textual query, an ELM autoregressively generates a free-form textual response. Unlike traditional classification-based systems, ELMs emulate expert cardiac electrophysiologists by issuing diagnoses, analyzing waveform morphology, identifying contributing factors, and proposing patient-specific action plans. To realize this potential, researchers are curating instruction-tuning datasets that pair ECGs with textual dialogues and are training ELMs on these resources. Yet before scaling ELMs further, there is a fundamental question yet to be explored: What is the most effective ECG input representation? In recent works, three candidate representations have emerged-raw time-series signals, rendered images, and discretized symbolic sequences. We present the first comprehensive benchmark of these modalities across 6 public datasets and 5 evaluation metrics. We find symbolic representations achieve the greatest number of statistically significant wins over both signal and image inputs. We further ablate the LLM backbone, ECG duration, and token budget, and we evaluate robustness to signal perturbations. We hope that our findings offer clear guidance for selecting input representations when developing the next generation of ELMs.', 'abstract_zh': 'Recent Advances in Electrocardiogram-Language Models: A Comprehensive Benchmark of Input Representations', 'title_zh': '信号、图像还是符号：通过统一框架探索心电信号语言模型的最佳输入表示方式'}
{'arxiv_id': 'arXiv:2505.18807', 'title': 'Mitigating Deceptive Alignment via Self-Monitoring', 'authors': 'Jiaming Ji, Wenqi Chen, Kaile Wang, Donghai Hong, Sitong Fang, Boyuan Chen, Jiayi Zhou, Juntao Dai, Sirui Han, Yike Guo, Yaodong Yang', 'link': 'https://arxiv.org/abs/2505.18807', 'abstract': 'Modern large language models rely on chain-of-thought (CoT) reasoning to achieve impressive performance, yet the same mechanism can amplify deceptive alignment, situations in which a model appears aligned while covertly pursuing misaligned goals. Existing safety pipelines treat deception as a black-box output to be filtered post-hoc, leaving the model free to scheme during its internal reasoning. We ask: Can deception be intercepted while the model is thinking? We answer this question, the first framework that embeds a Self-Monitor inside the CoT process itself, named CoT Monitor+. During generation, the model produces (i) ordinary reasoning steps and (ii) an internal self-evaluation signal trained to flag and suppress misaligned strategies. The signal is used as an auxiliary reward in reinforcement learning, creating a feedback loop that rewards honest reasoning and discourages hidden goals. To study deceptive alignment systematically, we introduce DeceptionBench, a five-category benchmark that probes covert alignment-faking, sycophancy, etc. We evaluate various LLMs and show that unrestricted CoT roughly aggravates the deceptive tendency. In contrast, CoT Monitor+ cuts deceptive behaviors by 43.8% on average while preserving task accuracy. Further, when the self-monitor signal replaces an external weak judge in RL fine-tuning, models exhibit substantially fewer obfuscated thoughts and retain transparency. Our project website can be found at this http URL', 'abstract_zh': '现代大型语言模型依赖于链式思考（CoT）推理以实现出色的性能，但同样的机制也可能放大欺骗性对齐，即模型在表面上表现出对齐的同时，暗中追求不对齐的目标。现有的安全性管道将欺骗视为一个黑盒输出，并在其后进行过滤，从而使模型在内部推理过程中自由地策划。我们提出的问题是：在模型思考过程中能否拦截欺骗性对齐？我们回答了这个问题，提出了一种名为CoT Monitor+的框架，该框架在CoT过程中嵌入了一个自我监控模块。在生成过程中，模型产生（i）常规推理步骤和（ii）一个用于标识和抑制不对齐策略的内部自我评估信号。该信号被用作强化学习中的辅助奖励，形成一个反馈循环，奖励诚实推理并抑制隐藏目标。为了系统地研究欺骗性对齐，我们引入了DeceptionBench，这是一个五类基准测试，可以探测隐蔽的对齐伪装、阿谀奉承等问题。我们评估了多种LLM，并展示了不受限制的CoT大致增加了欺骗性倾向。相比之下，CoT Monitor+在平均意义上减少了43.8%的欺骗性行为，同时保持了任务准确性。此外，当自我监控信号取代强化学习微调中的外部弱判断时，模型展示了显著减少的模糊思维，并保持了透明性。我们的项目网站详见此链接。', 'title_zh': '通过自我监控缓解欺骗性对齐'}
{'arxiv_id': 'arXiv:2505.18759', 'title': 'The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation', 'authors': 'Ruichen Zhang, Rana Muhammad Shahroz Khan, Zhen Tan, Dawei Li, Song Wang, Tianlong Chen', 'link': 'https://arxiv.org/abs/2505.18759', 'abstract': 'Data-centric distillation, including data augmentation, selection, and mixing, offers a promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks a comprehensive benchmark to systematically assess the effect of each distillation approach. This paper introduces DC-CoT, the first data-centric benchmark that investigates data manipulation in chain-of-thought (CoT) distillation from method, model and data perspectives. Utilizing various teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of these data manipulations on student model performance across multiple reasoning datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD) generalization, and cross-domain transfer. Our findings aim to provide actionable insights and establish best practices for optimizing CoT distillation through data-centric techniques, ultimately facilitating the development of more accessible and capable reasoning models. The dataset can be found at this https URL, while our code is shared in this https URL.', 'abstract_zh': '数据为中心的蒸馏：包括数据增强、选择和混合，为创建更小更高效但仍具有强大推理能力的学生大型语言模型提供了前景。然而，目前仍然缺乏一个系统评估每种蒸馏方法效果的全面基准。本文介绍了DC-CoT，这是首个从方法、模型和数据角度探究思维链（CoT）蒸馏中数据操控的首个数据为中心的基准。利用多种教师模型（例如o4-mini、Gemini-Pro、Claude-3.5）和学生架构（例如3B、7B参数），我们从多个推理数据集上严格评估这些数据操控对学生模型性能的影响，重点关注有分布内（IID）和分布外（OOD）泛化及跨领域迁移。我们的研究旨在提供实用的见解，并确立通过数据为中心的技术优化CoT蒸馏的最佳实践，最终促进更易于访问且更强大的推理模型的发展。数据集可以在以下链接找到：this https URL，代码在以下链接共享：this https URL。', 'title_zh': '基于数据的元思维精炼基准：追求高效推理'}
{'arxiv_id': 'arXiv:2505.18746', 'title': '$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking', 'authors': 'Peijie Yu, Yifan Yang, Jinjian Li, Zelong Zhang, Haorui Wang, Xiao Feng, Feng Zhang', 'link': 'https://arxiv.org/abs/2505.18746', 'abstract': 'Agents based on large language models leverage tools to modify environments, revolutionizing how AI interacts with the physical world. Unlike traditional NLP tasks that rely solely on historical dialogue for responses, these agents must consider more complex factors, such as inter-tool relationships, environmental feedback and previous decisions, when making choices. Current research typically evaluates agents via multi-turn dialogues. However, it overlooks the influence of these critical factors on agent behavior. To bridge this gap, we present an open-source and high-quality benchmark $C^3$-Bench. This benchmark integrates attack concepts and applies univariate analysis to pinpoint key elements affecting agent robustness. In concrete, we design three challenges: navigate complex tool relationships, handle critical hidden information and manage dynamic decision paths. Complementing these challenges, we introduce fine-grained metrics, innovative data collection algorithms and reproducible evaluation methods. Extensive experiments are conducted on 49 mainstream agents, encompassing general fast-thinking, slow-thinking and domain-specific models. We observe that agents have significant shortcomings in handling tool dependencies, long context information dependencies and frequent policy-type switching. In essence, $C^3$-Bench aims to expose model vulnerabilities through these challenges and drive research into the interpretability of agent performance. The benchmark is publicly available at this https URL.', 'abstract_zh': '基于大语言模型的代理利用工具修改环境，重塑AI与物理世界交互的方式。与仅依赖历史对话的传统自然语言处理任务不同，这些代理在做决策时必须考虑更复杂的因素，如工具之间的关系、环境反馈以及之前的选择。当前的研究通常通过多轮对话评估代理。然而，这种评估方法忽视了这些关键因素对代理行为的影响。为填补这一差距，我们推出了一个开源且高质量的基准测试——$C^3$-Bench。该基准测试集整合了攻击概念，并采用单变量分析来识别影响代理鲁棒性的关键元素。具体而言，我们设计了三个挑战：导航复杂工具关系、处理关键隐藏信息以及管理动态决策路径。为支持这些挑战，我们还引入了细粒度的评估指标、创新的数据采集算法以及可重复的评估方法。我们在49个主流代理上进行了广泛的实验，涵盖通用快速思考模型、慢速思考模型和领域特定模型。实验结果表明，代理在处理工具依赖性、长时间上下文信息依赖以及频繁的策略切换方面存在显著缺陷。本质上，$C^3$-Bench旨在通过这些挑战揭示模型的脆弱性，并推动代理性能可解释性的研究。基准测试已公开发布。', 'title_zh': '$C^3$-Bench: 基于多任务处理的真实干扰LLM代理'}
{'arxiv_id': 'arXiv:2505.18705', 'title': 'AI-Researcher: Autonomous Scientific Innovation', 'authors': 'Jiabin Tang, Lianghao Xia, Zhonghang Li, Chao Huang', 'link': 'https://arxiv.org/abs/2505.18705', 'abstract': 'The powerful reasoning capabilities of Large Language Models (LLMs) in mathematics and coding, combined with their ability to automate complex tasks through agentic frameworks, present unprecedented opportunities for accelerating scientific innovation. In this paper, we introduce AI-Researcher, a fully autonomous research system that transforms how AI-driven scientific discovery is conducted and evaluated. Our framework seamlessly orchestrates the complete research pipeline--from literature review and hypothesis generation to algorithm implementation and publication-ready manuscript preparation--with minimal human intervention. To rigorously assess autonomous research capabilities, we develop Scientist-Bench, a comprehensive benchmark comprising state-of-the-art papers across diverse AI research domains, featuring both guided innovation and open-ended exploration tasks. Through extensive experiments, we demonstrate that AI-Researcher achieves remarkable implementation success rates and produces research papers that approach human-level quality. This work establishes new foundations for autonomous scientific innovation that can complement human researchers by systematically exploring solution spaces beyond cognitive limitations.', 'abstract_zh': '大型语言模型（LLMs）在数学和编码中的强大推理能力，以及通过代理框架自动完成复杂任务的能力，为加速科学创新带来了前所未有的机会。本文介绍了一种完全自主的研究系统AI-Researcher，该系统变革了由AI驱动的科学研究和评估方式。我们的框架无缝协调了完整的研究流程——从文献回顾和假设生成到算法实现和准备发表的研究论文——同时最大限度地减少人工干预。为了严格评估自主研究能力，我们开发了全面的基准测试Scientist-Bench，该基准测试涵盖了不同AI研究领域的先进论文，包含指导创新和开放式探索任务。通过广泛的实验，我们展示了AI-Researcher达到了显著的实现成功率，并生成了接近人类水平质量的研究论文。这项工作为自主科学创新奠定了新的基础，通过系统地探索超出认知限制的解决方案空间，可以补充人类研究人员的工作。', 'title_zh': 'AI-Researcher: 自主科学研究'}
{'arxiv_id': 'arXiv:2505.18694', 'title': 'AI-Driven Climate Policy Scenario Generation for Sub-Saharan Africa', 'authors': 'Rafiu Adekoya Badekale, Adewale Akinfaderin', 'link': 'https://arxiv.org/abs/2505.18694', 'abstract': 'Climate policy scenario generation and evaluation have traditionally relied on integrated assessment models (IAMs) and expert-driven qualitative analysis. These methods enable stakeholders, such as policymakers and researchers, to anticipate impacts, plan governance strategies, and develop mitigation measures. However, traditional methods are often time-intensive, reliant on simple extrapolations of past trends, and limited in capturing the complex and interconnected nature of energy and climate issues. With the advent of artificial intelligence (AI), particularly generative AI models trained on vast datasets, these limitations can be addressed, ensuring robustness even under limited data conditions. In this work, we explore the novel method that employs generative AI, specifically large language models (LLMs), to simulate climate policy scenarios for Sub-Saharan Africa. These scenarios focus on energy transition themes derived from the historical United Nations Climate Change Conference (COP) documents. By leveraging generative models, the project aims to create plausible and diverse policy scenarios that align with regional climate goals and energy challenges. Given limited access to human evaluators, automated techniques were employed for scenario evaluation. We generated policy scenarios using the llama3.2-3B model. Of the 34 generated responses, 30 (88%) passed expert validation, accurately reflecting the intended impacts provided in the corresponding prompts. We compared these validated responses against assessments from a human climate expert and two additional LLMs (gemma2-2B and mistral-7B). Our structured, embedding-based evaluation framework shows that generative AI effectively generate scenarios that are coherent, relevant, plausible, and diverse. This approach offers a transformative tool for climate policy planning in data-constrained regions.', 'abstract_zh': '利用生成型人工智能模型模拟和评估气候变化政策情景：以Sub-Saharan非洲地区能源转型为主题', 'title_zh': 'AI驱动的气候政策情景生成——以撒哈拉以南非洲地区为例'}
{'arxiv_id': 'arXiv:2505.18657', 'title': 'MLLMs are Deeply Affected by Modality Bias', 'authors': 'Xu Zheng, Chenfei Liao, Yuqian Fu, Kaiyu Lei, Yuanhuiyi Lyu, Lutao Jiang, Bin Ren, Jialei Chen, Jiawen Wang, Chengxin Li, Linfeng Zhang, Danda Pani Paudel, Xuanjing Huang, Yu-Gang Jiang, Nicu Sebe, Dacheng Tao, Luc Van Gool, Xuming Hu', 'link': 'https://arxiv.org/abs/2505.18657', 'abstract': 'Recent advances in Multimodal Large Language Models (MLLMs) have shown promising results in integrating diverse modalities such as texts and images. MLLMs are heavily influenced by modality bias, often relying on language while under-utilizing other modalities like visual inputs. This position paper argues that MLLMs are deeply affected by modality bias. Firstly, we diagnose the current state of modality bias, highlighting its manifestations across various tasks. Secondly, we propose a systematic research road-map related to modality bias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and offer actionable suggestions for future research to mitigate it. To substantiate these findings, we conduct experiments that demonstrate the influence of each factor: 1. Data Characteristics: Language data is compact and abstract, while visual data is redundant and complex, creating an inherent imbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The dominance of pretrained language models in MLLMs leads to overreliance on language and neglect of visual information. 3. Training Objectives: Current objectives often fail to promote balanced cross-modal alignment, resulting in shortcut learning biased toward language. These findings highlight the need for balanced training strategies and model architectures to better integrate multiple modalities in MLLMs. We call for interdisciplinary efforts to tackle these challenges and drive innovation in MLLM research. Our work provides a fresh perspective on modality bias in MLLMs and offers insights for developing more robust and generalizable multimodal systems-advancing progress toward Artificial General Intelligence.', 'abstract_zh': 'Recent Advances in Multimodal Large Language Models (MLLMs): Addressing Modality Bias and Promoting Balanced Integration', 'title_zh': 'MLLMs 深受模态偏差影响'}
{'arxiv_id': 'arXiv:2505.18607', 'title': 'Knowledge Retrieval in LLM Gaming: A Shift from Entity-Centric to Goal-Oriented Graphs', 'authors': 'Jonathan Leung, Yongjie Wang, Zhiqi Shen', 'link': 'https://arxiv.org/abs/2505.18607', 'abstract': 'Large Language Models (LLMs) demonstrate impressive general capabilities but often struggle with step-by-step reasoning, especially in complex applications such as games. While retrieval-augmented methods like GraphRAG attempt to bridge this gap through cross-document extraction and indexing, their fragmented entity-relation graphs and overly dense local connectivity hinder the construction of coherent reasoning. In this paper, we propose a novel framework based on Goal-Oriented Graphs (GoGs), where each node represents a goal and its associated attributes, and edges encode logical dependencies between goals. This structure enables explicit retrieval of reasoning paths by first identifying high-level goals and recursively retrieving their subgoals, forming coherent reasoning chains to guide LLM prompting. Our method significantly enhances the reasoning ability of LLMs in game-playing tasks, as demonstrated by extensive experiments on the Minecraft testbed, outperforming GraphRAG and other baselines.', 'abstract_zh': '基于目标导向图的大型语言模型推理能力提升框架', 'title_zh': 'LLM游戏中的知识检索：从实体中心到目标导向图的转变'}
{'arxiv_id': 'arXiv:2505.18597', 'title': 'LLMs for Supply Chain Management', 'authors': 'Haojie Wang, Jiuyun Jiang, L. Jeff Hong, Guangxin Jiang', 'link': 'https://arxiv.org/abs/2505.18597', 'abstract': 'The development of large language models (LLMs) has provided new tools for research in supply chain management (SCM). In this paper, we introduce a retrieval-augmented generation (RAG) framework that dynamically integrates external knowledge into the inference process, and develop a domain-specialized SCM LLM, which demonstrates expert-level competence by passing standardized SCM examinations and beer game tests. We further employ the use of LLMs to conduct horizontal and vertical supply chain games, in order to analyze competition and cooperation within supply chains. Our experiments show that RAG significantly improves performance on SCM tasks. Moreover, game-theoretic analysis reveals that the LLM can reproduce insights from the classical SCM literature, while also uncovering novel behaviors and offering fresh perspectives on phenomena such as the bullwhip effect. This paper opens the door for exploring cooperation and competition for complex supply chain network through the lens of LLMs.', 'abstract_zh': '大型语言模型的发展为供应链管理研究提供了新的工具。本文介绍了一种检索增强生成（RAG）框架，该框架动态地将外部知识融入推理过程，并开发了一种专门化的供应链语言模型，该模型通过标准化的供应链管理考试和啤酒游戏测试展示了专家级的能力。我们进一步利用大型语言模型进行横向和纵向供应链博弈，以分析供应链中的竞争与合作。实验结果表明，RAG在供应链任务上的性能显著提升。此外，博弈论分析表明，该语言模型可以再现经典的供应链管理文献中的洞察，同时揭示新的行为并为诸如回抽效应等现象提供新的视角。本文为通过大型语言模型探讨复杂供应链网络中的合作与竞争打开了大门。', 'title_zh': 'LLMs在供应链管理中的应用'}
{'arxiv_id': 'arXiv:2505.18585', 'title': 'RvLLM: LLM Runtime Verification with Domain Knowledge', 'authors': 'Yedi Zhang, Sun Yi Emma, Annabelle Lee Jia En, Annabelle Lee Jia En, Jin Song Dong', 'link': 'https://arxiv.org/abs/2505.18585', 'abstract': 'Large language models (LLMs) have emerged as a dominant AI paradigm due to their exceptional text understanding and generation capabilities. However, their tendency to generate inconsistent or erroneous outputs challenges their reliability, especially in high-stakes domains requiring accuracy and trustworthiness. Existing research primarily focuses on detecting and mitigating model misbehavior in general-purpose scenarios, often overlooking the potential of integrating domain-specific knowledge. In this work, we advance misbehavior detection by incorporating domain knowledge. The core idea is to design a general specification language that enables domain experts to customize domain-specific predicates in a lightweight and intuitive manner, supporting later runtime verification of LLM outputs. To achieve this, we design a novel specification language, ESL, and introduce a runtime verification framework, RvLLM, to validate LLM output against domain-specific constraints defined in ESL. We evaluate RvLLM on three representative tasks: violation detection against Singapore Rapid Transit Systems Act, numerical comparison, and inequality solving. Experimental results demonstrate that RvLLM effectively detects erroneous outputs across various LLMs in a lightweight and flexible manner. The results reveal that despite their impressive capabilities, LLMs remain prone to low-level errors due to limited interpretability and a lack of formal guarantees during inference, and our framework offers a potential long-term solution by leveraging expert domain knowledge to rigorously and efficiently verify LLM outputs.', 'abstract_zh': '大型语言模型（LLMs）因其卓越的文本理解和生成能力而成为主导的AI范式。然而，它们生成不一致或错误输出的趋势挑战了其可靠性，特别是在需要准确性和可信度的高 stakes 领域。现有研究主要集中在检测和减轻通用场景中的模型不当行为，往往忽视了整合领域特定知识的潜力。在本工作中，我们通过集成领域知识来推进不当行为检测。核心思想是设计一种通用规范语言，使领域专家能够以轻量级和直观的方式自定义领域特定谓词，支持对LLM输出的后期运行时验证。为实现这一目标，我们设计了一种新型规范语言ESL，并引入了一种运行时验证框架RvLLM，用于根据ESL中定义的领域特定约束验证LLM输出。我们在三个代表性任务上评估了RvLLM：与新加坡快速交通系统法案的违规检测、数值比较和不等式求解。实验结果表明，RvLLM能够以轻量级和灵活的方式有效检测各种LLM的错误输出。结果表明，尽管LLM具有令人印象深刻的能力，但由于解释性有限和推断过程中缺乏形式保证，它们仍然容易出现低级错误，并且我们的框架通过利用专家领域的知识来严格且高效地验证LLM输出，提供了一个潜在的长期解决方案。', 'title_zh': 'RvLLM：基于领域知识的LLM运行时验证'}
{'arxiv_id': 'arXiv:2505.18575', 'title': 'Response Uncertainty and Probe Modeling: Two Sides of the Same Coin in LLM Interpretability?', 'authors': 'Yongjie Wang, Yibo Wang, Xin Zhou, Zhiqi Shen', 'link': 'https://arxiv.org/abs/2505.18575', 'abstract': "Probing techniques have shown promise in revealing how LLMs encode human-interpretable concepts, particularly when applied to curated datasets. However, the factors governing a dataset's suitability for effective probe training are not well-understood. This study hypothesizes that probe performance on such datasets reflects characteristics of both the LLM's generated responses and its internal feature space. Through quantitative analysis of probe performance and LLM response uncertainty across a series of tasks, we find a strong correlation: improved probe performance consistently corresponds to a reduction in response uncertainty, and vice versa. Subsequently, we delve deeper into this correlation through the lens of feature importance analysis. Our findings indicate that high LLM response variance is associated with a larger set of important features, which poses a greater challenge for probe models and often results in diminished performance. Moreover, leveraging the insights from response uncertainty analysis, we are able to identify concrete examples where LLM representations align with human knowledge across diverse domains, offering additional evidence of interpretable reasoning in LLMs.", 'abstract_zh': '探针技术在揭示LLMs对人类可解释概念的编码方式方面显示出潜力，特别是在应用到精心筛选的数据集时。然而，决定一个数据集是否适合有效的探针训练的因素尚不明确。本研究假定探针在这些数据集上的性能反映了LLM生成响应的特点及其内部特征空间的特点。通过定量分析探针性能和LLM响应不确定性在一系列任务中的表现，我们发现两者之间存在显著的相关性：探针性能的提升与响应不确定性的降低呈一致性，反之亦然。随后，我们通过特征重要性分析的视角进一步探讨了这种相关性。研究发现，较高的LLM响应变异与一组重要的特征相关联，这给探针模型带来了更大的挑战，往往导致性能下降。此外，借助响应不确定性分析的洞见，我们能够识别出具体例子，在这些例子中，LLMs的表示与不同领域的知识相契合，为LLMs的可解释推理提供了额外证据。', 'title_zh': '响应不确定性与探测建模：大规模语言模型可解释性的两面骰子？'}
{'arxiv_id': 'arXiv:2505.18541', 'title': 'RoleRAG: Enhancing LLM Role-Playing via Graph Guided Retrieval', 'authors': 'Yongjie Wang, Jonathan Leung, Zhiqi Shen', 'link': 'https://arxiv.org/abs/2505.18541', 'abstract': "Large Language Models (LLMs) have shown promise in character imitation, enabling immersive and engaging conversations. However, they often generate content that is irrelevant or inconsistent with a character's background. We attribute these failures to: (1) the inability to accurately recall character-specific knowledge due to entity ambiguity, and (2) a lack of awareness of the character's cognitive boundaries. To address these issues, we propose RoleRAG, a retrieval-based framework that integrates efficient entity disambiguation for knowledge indexing with a boundary-aware retriever for extracting contextually appropriate information from a structured knowledge graph. Experiments on role-playing benchmarks show that RoleRAG's calibrated retrieval helps both general-purpose and role-specific LLMs better align with character knowledge and reduce hallucinated responses.", 'abstract_zh': '大型语言模型（LLMs）在角色模仿方面展现了潜力，能够实现沉浸式和互动性强的对话。然而，它们往往生成与角色背景无关或不一致的内容。我们归因于以下两点：（1）由于实体模糊导致的角色特定知识准确召回能力不足，以及（2）缺乏对角色认知边界的认识。为解决这些问题，我们提出了一种名为RoleRAG的检索框架，该框架将高效的实体消歧与边界感知检索相结合，从结构化知识图中提取上下文相关的信息。在角色扮演基准测试中的实验表明，RoleRAG校准的检索有助于通用和角色特定的大规模语言模型更好地与角色知识对齐，并减少虚拟响应。', 'title_zh': 'RoleRAG：通过图引导检索增强LLM角色扮演能力'}
{'arxiv_id': 'arXiv:2505.18531', 'title': 'Generative RLHF-V: Learning Principles from Multi-modal Human Preference', 'authors': 'Jiayi Zhou, Jiaming Ji, Boyuan Chen, Jiapeng Sun, Wenqi Chen, Donghai Hong, Sirui Han, Yike Guo, Yaodong Yang', 'link': 'https://arxiv.org/abs/2505.18531', 'abstract': "Training multi-modal large language models (MLLMs) that align with human intentions is a long-term challenge. Traditional score-only reward models for alignment suffer from low accuracy, weak generalization, and poor interpretability, blocking the progress of alignment methods, e.g., reinforcement learning from human feedback (RLHF). Generative reward models (GRMs) leverage MLLMs' intrinsic reasoning capabilities to discriminate pair-wise responses, but their pair-wise paradigm makes it hard to generalize to learnable rewards. We introduce Generative RLHF-V, a novel alignment framework that integrates GRMs with multi-modal RLHF. We propose a two-stage pipeline: $\\textbf{multi-modal generative reward modeling from RL}$, where RL guides GRMs to actively capture human intention, then predict the correct pair-wise scores; and $\\textbf{RL optimization from grouped comparison}$, which enhances multi-modal RL scoring precision by grouped responses comparison. Experimental results demonstrate that, besides out-of-distribution generalization of RM discrimination, our framework improves 4 MLLMs' performance across 7 benchmarks by $18.1\\%$, while the baseline RLHF is only $5.3\\%$. We further validate that Generative RLHF-V achieves a near-linear improvement with an increasing number of candidate responses. Our code and models can be found at this https URL.", 'abstract_zh': '训练与人类意图一致的多模态大型语言模型（MLLMs）是一项长期挑战。传统的仅基于评分的对齐模型精度低、泛化能力弱、可解释性差，阻碍了诸如基于人类反馈强化学习（RLHF）等对齐方法的发展。生成奖励模型（GRMs）利用MLLMs本征的推理能力来区分成对响应，但其成对范式使得泛化到可学习奖励变得困难。我们提出了一种新颖的对齐框架Generative RLHF-V，将GRMs与多模态RLHF集成。我们提出了一种两阶段流水线：基于RL的多模态生成奖励建模，其中RL引导GRMs主动捕捉人类意图并预测正确的成对评分；以及基于分组比较的RL优化，通过成组响应比较增强多模态RL评分精度。实验结果表明，除了RM偏差的分布外推泛化外，我们的框架在7个基准测试中分别提高了4个MLLMs的性能18.1%，而基线RLHF仅提高了5.3%。我们进一步验证，Generative RLHF-V随候选响应数量的增加实现了近线性改进。我们的代码和模型可在此链接找到：this https URL。', 'title_zh': '生成性RLHF-V：从多模态人类偏好中学习原则'}
{'arxiv_id': 'arXiv:2505.18517', 'title': 'LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs', 'authors': 'Pooneh Mousavi, Shubham Gupta, Cem Subakan, Mirco Ravanelli', 'link': 'https://arxiv.org/abs/2505.18517', 'abstract': 'Foundation models based on large language models (LLMs) have shown great success in handling various tasks and modalities. However, adapting these models for general-purpose audio-language tasks is challenging due to differences in acoustic environments and task variations. In this work, we introduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic prompt selection strategy with learnable key-value pairs, allowing the model to balance general and task-specific knowledge while avoiding overfitting in a multitask setting. Our approach reduces dependence on large-scale ASR or captioning datasets, achieves competitive performance with fewer trainable parameters, and simplifies training by using a single-stage process. Additionally, LiSTEN enhances interpretability by analyzing the diversity and overlap of selected prompts across different tasks.', 'abstract_zh': '基于大型语言模型的foundation模型在处理各种任务和模态方面取得了显著成功。然而，将这些模型适应通用目的的语音和语言任务具有挑战性，因为存在声学环境和任务差异。在本文中，我们引入了LiSTEN学习软令牌嵌入以适应神经音频foundation模型（LiSTEN Learning Soft Token Embeddings for Neural Audio Foundation Models），一种将大型语言模型适应语音和音频任务的框架。LiSTEN使用具有可学习键值对的动态提示选择策略，允许模型在多任务设置中平衡一般知识和任务特定知识，同时避免过度拟合。我们的方法减少了对大规模ASR或字幕数据集的依赖，使用较少的可训练参数实现了竞争力的表现，并通过单阶段过程简化了训练。此外，LiSTEN通过分析不同任务中选择提示的多样性和重叠性以增强可解释性。', 'title_zh': 'LiSTEN: 学习软令牌嵌入的神经音频大规模语言模型'}
{'arxiv_id': 'arXiv:2505.18502', 'title': 'Knowledge Grafting of Large Language Models', 'authors': 'Guodong Du, Xuanning Zhou, Junlin Li, Zhuo Li, Zesheng Shi, Wanyu Lin, Ho-Kin Tang, Xiucheng Li, Fangming Liu, Wenya Wang, Min Zhang, Jing Li', 'link': 'https://arxiv.org/abs/2505.18502', 'abstract': "Cross-capability transfer is a key challenge in large language model (LLM) research, with applications in multi-task integration, model compression, and continual learning. Recent works like FuseLLM and FuseChat have demonstrated the potential of transferring multiple model capabilities to lightweight models, enhancing adaptability and efficiency, which motivates our investigation into more efficient cross-capability transfer methods. However, existing approaches primarily focus on small, homogeneous models, limiting their applicability. For large, heterogeneous models, knowledge distillation with full-parameter fine-tuning often overlooks the student model's intrinsic capacity and risks catastrophic forgetting, while PEFT methods struggle to effectively absorb knowledge from source LLMs. To address these issues, we introduce GraftLLM, a novel method that stores source model capabilities in a target model with SkillPack format. This approach preserves general capabilities, reduces parameter conflicts, and supports forget-free continual learning and model fusion. We employ a module-aware adaptive compression strategy to compress parameter updates, ensuring efficient storage while maintaining task-specific knowledge. The resulting SkillPack serves as a compact and transferable knowledge carrier, ideal for heterogeneous model fusion and continual learning. Experiments across various scenarios demonstrate that GraftLLM outperforms existing techniques in knowledge transfer, knowledge fusion, and forget-free learning, providing a scalable and efficient solution for cross-capability transfer. The code is publicly available at: this https URL.", 'abstract_zh': '跨能力迁移是大型语言模型（LLM）研究中的一个关键挑战，应用于多任务集成、模型压缩和持续学习。现有的工作如FuseLLM和FuseChat展示了将多个模型能力迁移到轻量级模型中的潜力，增强了适应性和效率，这激发了我们对更高效跨能力迁移方法的研究。然而，现有方法主要集中在小型同质模型上，限制了其适用范围。对于大型异质模型，全参数微调的知识精炼往往忽略了学生模型的固有能力并存在灾难性遗忘的风险，而PEFT方法难以有效地吸收源LLM的知识。为了解决这些问题，我们引入了GraftLLM，这是一种新颖的方法，将源模型能力存储在目标模型中的SkillPack格式中。该方法保留了通用能力，减少了参数冲突，并支持无遗忘的持续学习和模型融合。我们采用模块感知的自适应压缩策略压缩参数更新，确保高效存储同时保留任务特定的知识。所获得的SkillPack作为一种紧凑且可迁移的知识载体，适用于异质模型融合和持续学习。实验结果表明，在知识迁移、知识融合和无遗忘学习方面，GraftLLM优于现有技术，提供了一种可扩展且高效的跨能力迁移解决方案。代码已公开：this https URL。', 'title_zh': '大型语言模型的知识嫁接'}
{'arxiv_id': 'arXiv:2505.18483', 'title': 'Retrieval Augmented Decision-Making: A Requirements-Driven, Multi-Criteria Framework for Structured Decision Support', 'authors': 'Hongjia Wu, Hongxin Zhang, Wei Chen, Jiazhi Xia', 'link': 'https://arxiv.org/abs/2505.18483', 'abstract': 'Various industries have produced a large number of documents such as industrial plans, technical guidelines, and regulations that are structurally complex and content-wise fragmented. This poses significant challenges for experts and decision-makers in terms of retrieval and understanding. Although existing LLM-based Retrieval-Augmented Generation methods can provide context-related suggestions, they lack quantitative weighting and traceable reasoning paths, making it difficult to offer multi-level and transparent decision support. To address this issue, this paper proposes the RAD method, which integrates Multi-Criteria Decision Making with the semantic understanding capabilities of LLMs. The method automatically extracts key criteria from industry documents, builds a weighted hierarchical decision model, and generates structured reports under model guidance. The RAD framework introduces explicit weight assignment and reasoning chains in decision generation to ensure accuracy, completeness, and traceability. Experiments show that in various decision-making tasks, the decision reports generated by RAD significantly outperform existing methods in terms of detail, rationality, and structure, demonstrating its application value and potential in complex decision support scenarios.', 'abstract_zh': '各种行业产生了大量结构复杂、内容碎片化的文档，如工业计划、技术指南和规章等，这给专家和决策者在检索和理解方面带来了显著挑战。尽管现有的基于LLM的检索增强生成方法可以提供上下文相关的建议，但是缺少定量加权和可追溯的推理路径，难以提供多层次和透明的决策支持。为解决这一问题，本文提出了RAD方法，该方法将多准则决策方法与LLM的语义理解能力相结合。该方法自动从行业文档中提取关键准则，构建加权层次决策模型，并在模型引导下生成结构化报告。RAD框架在决策生成中引入了明确的权重分配和推理链，以确保决策的准确性和可追溯性。实验结果表明，在各种决策任务中，RAD生成的决策报告在细节、合理性、结构等方面显著优于现有方法，显示出其在复杂决策支持场景中的应用价值和潜力。', 'title_zh': '基于检索增强的决策制定：一种需求驱动的多准则结构化决策支持框架'}
{'arxiv_id': 'arXiv:2505.18380', 'title': 'RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification', 'authors': 'Praphul Singh, Charlotte Dzialo, Jangwon Kim, Sumana Srivatsa, Irfan Bulu, Sri Gadde, Krishnaram Kenthapadi', 'link': 'https://arxiv.org/abs/2505.18380', 'abstract': 'Ensuring clinical data privacy while preserving utility is critical for AI-driven healthcare and data analytics. Existing de-identification (De-ID) methods, including rule-based techniques, deep learning models, and large language models (LLMs), often suffer from recall errors, limited generalization, and inefficiencies, limiting their real-world applicability. We propose a fully automated, multi-modal framework, RedactOR for de-identifying structured and unstructured electronic health records, including clinical audio records. Our framework employs cost-efficient De-ID strategies, including intelligent routing, hybrid rule and LLM based approaches, and a two-step audio redaction approach. We present a retrieval-based entity relexicalization approach to ensure consistent substitutions of protected entities, thereby enhancing data coherence for downstream applications. We discuss key design desiderata, de-identification and relexicalization methodology, and modular architecture of RedactX and its integration with the Oracle Health Clinical AI system. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with strict recall, our approach achieves competitive performance while optimizing token usage to reduce LLM costs. Finally, we discuss key lessons and insights from deployment in real-world AI- driven healthcare data pipelines.', 'abstract_zh': '确保临床数据隐私同时保留其实用性的自动化多模态脱敏框架RedactOR对于AI驱动的医疗和数据分析至关重要。', 'title_zh': 'RedactOR: 一个由大规模语言模型驱动的自动临床数据去标识化框架'}
{'arxiv_id': 'arXiv:2505.18325', 'title': 'Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary', 'authors': 'Licheng Pan, Yongqi Tong, Xin Zhang, Xiaolu Zhang, Jun Zhou, Zhixuan Chu', 'link': 'https://arxiv.org/abs/2505.18325', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they often refuse to answer legitimate queries-a phenomenon known as overrefusal. Overrefusal typically stems from over-conservative safety alignment, causing models to treat many reasonable prompts as potentially risky. To systematically understand this issue, we probe and leverage the models'safety decision boundaries to analyze and mitigate overrefusal. Our findings reveal that overrefusal is closely tied to misalignment at these boundary regions, where models struggle to distinguish subtle differences between benign and harmful content. Building on these insights, we present RASS, an automated framework for prompt generation and selection that strategically targets overrefusal prompts near the safety boundary. By harnessing steering vectors in the representation space, RASS efficiently identifies and curates boundary-aligned prompts, enabling more effective and targeted mitigation of overrefusal. This approach not only provides a more precise and interpretable view of model safety decisions but also seamlessly extends to multilingual this http URL have explored the safety decision boundaries of various LLMs and construct the MORBench evaluation set to facilitate robust assessment of model safety and helpfulness across multiple languages. Code and datasets will be released at this https URL.", 'abstract_zh': '大型语言模型（LLMs）在广泛的任务中展示了出色的能力，但它们往往拒绝回答合法查询——这一现象被称为过度回避。过度回避通常源于过度保守的安全对齐，导致模型将许多合理的提示视为潜在风险。为了系统地理解这一问题，我们通过探查和利用模型的安全决策边界来分析和缓解过度回避现象。我们的研究发现，过度回避与这些边界区域的不对齐密切相关，在这些区域，模型难以区分良性内容和有害内容之间微妙的差异。基于这些见解，我们提出了RASS，这是一种自动化的提示生成和选择框架，战略性地将目标对准安全边界附近的过度回避提示。通过利用概念空间中的引导向量，RASS高效地识别并筛选边界对齐的提示，从而更有效地和精确地缓解过度回避现象。这种方法不仅提供了对模型安全决策更为精确和可解释的看法，还能够无缝扩展到多语言环境。我们探索了各种LLM的安全决策边界，并构建了MORBench评估集，以促进对多语言模型安全性和有用性的稳健评估。相关代码和数据集将在以下链接发布：this https URL。', 'title_zh': '从安全性决策边界揭示视角理解并缓解LLM中的过度拒绝现象'}
{'arxiv_id': 'arXiv:2505.20296', 'title': 'Reasoning LLMs are Wandering Solution Explorers', 'authors': 'Jiahao Lu, Ziwei Xu, Mohan Kankanhalli', 'link': 'https://arxiv.org/abs/2505.20296', 'abstract': "Large Language Models (LLMs) have demonstrated impressive reasoning abilities through test-time computation (TTC) techniques such as chain-of-thought prompting and tree-based reasoning. However, we argue that current reasoning LLMs (RLLMs) lack the ability to systematically explore the solution space. This paper formalizes what constitutes systematic problem solving and identifies common failure modes that reveal reasoning LLMs to be wanderers rather than systematic explorers. Through qualitative and quantitative analysis across multiple state-of-the-art LLMs, we uncover persistent issues: invalid reasoning steps, redundant explorations, hallucinated or unfaithful conclusions, and so on. Our findings suggest that current models' performance can appear to be competent on simple tasks yet degrade sharply as complexity increases. Based on the findings, we advocate for new metrics and tools that evaluate not just final outputs but the structure of the reasoning process itself.", 'abstract_zh': '大型语言模型（LLMs）通过测试时计算（TTC）技术如链式思考提示和树状推理展示了令人印象深刻的推理能力。然而，我们argue认为当前的推理大型语言模型（RLLMs）缺乏系统探索解空间的能力。本文形式化了系统性问题解决的构成，并指出了常见失败模式，表明推理大型语言模型更像是漫游者而非系统探索者。通过对多种最先进的大型语言模型进行定性和定量分析，我们揭露出持久存在的问题：无效的推理步骤、冗余的探索、虚幻或不忠实的结论等。我们的发现表明，当前模型在简单任务上可能表现得相当能干，但在复杂性增加时性能急剧下降。基于这些发现，我们提倡新的评估指标和工具，不仅评估最终输出，还要评估推理过程本身的结构。', 'title_zh': 'LLM推理是漫游的解探索者'}
{'arxiv_id': 'arXiv:2505.20295', 'title': 'Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?', 'authors': 'Michael Kirchhof, Luca Füger, Adam Goliński, Eeshan Gunesh Dhekane, Arno Blaas, Sinead Williamson', 'link': 'https://arxiv.org/abs/2505.20295', 'abstract': "To reveal when a large language model (LLM) is uncertain about a response, uncertainty quantification commonly produces percentage numbers along with the output. But is this all we can do? We argue that in the output space of LLMs, the space of strings, exist strings expressive enough to summarize the distribution over output strings the LLM deems possible. We lay a foundation for this new avenue of uncertainty explication and present SelfReflect, a theoretically-motivated metric to assess how faithfully a string summarizes an LLM's internal answer distribution. We show that SelfReflect is able to discriminate even subtle differences of candidate summary strings and that it aligns with human judgement, outperforming alternative metrics such as LLM judges and embedding comparisons. With SelfReflect, we investigate a number of self-summarization methods and find that even state-of-the-art reasoning models struggle to explicate their internal uncertainty. But we find that faithful summarizations can be generated by sampling and summarizing. Our metric enables future works towards this universal form of LLM uncertainties.", 'abstract_zh': '揭示大规模语言模型（LLM）回答不确定性的新途径：字符串的不确定量化评估', 'title_zh': '自我反思不确定性：大规模语言模型知晓其内部答案分布吗？'}
{'arxiv_id': 'arXiv:2505.20278', 'title': 'The Coverage Principle: A Framework for Understanding Compositional Generalization', 'authors': 'Hoyeon Chang, Jinho Park, Hanseul Cho, Sohee Yang, Miyoung Ko, Hyeonbin Hwang, Seungpil Won, Dohaeng Lee, Youbin Ahn, Minjoon Seo', 'link': 'https://arxiv.org/abs/2505.20278', 'abstract': 'Large language models excel at pattern matching, yet often fall short in systematic compositional generalization. We propose the coverage principle: a data-centric framework showing that models relying primarily on pattern matching for compositional tasks cannot reliably generalize beyond substituting fragments that yield identical results when used in the same contexts. We demonstrate that this framework has a strong predictive power for the generalization capabilities of Transformers. First, we derive and empirically confirm that the training data required for two-hop generalization grows at least quadratically with the token set size, and the training data efficiency does not improve with 20x parameter scaling. Second, for compositional tasks with path ambiguity where one variable affects the output through multiple computational paths, we show that Transformers learn context-dependent state representations that undermine both performance and interoperability. Third, Chain-of-Thought supervision improves training data efficiency for multi-hop tasks but still struggles with path ambiguity. Finally, we outline a \\emph{mechanism-based} taxonomy that distinguishes three ways neural networks can generalize: structure-based (bounded by coverage), property-based (leveraging algebraic invariances), and shared-operator (through function reuse). This conceptual lens contextualizes our results and highlights where new architectural ideas are needed to achieve systematic compositionally. Overall, the coverage principle provides a unified lens for understanding compositional reasoning, and underscores the need for fundamental architectural or training innovations to achieve truly systematic compositionality.', 'abstract_zh': '大型语言模型在模式匹配方面表现出色，但在系统组合泛化方面往往表现不佳。我们提出了覆盖原则：一个以数据为中心的框架，表明主要依赖于模式匹配的模型在组合任务中无法可靠地泛化到只能通过相同上下文产生相同结果的片段替换之外。我们证明了该框架对Transformer的泛化能力具有很强的预测能力。首先，我们推导并实证验证了两步泛化的训练数据量至少与令牌集大小成平方关系，并且20倍参数规模并不会提高训练数据效率。其次，在路径歧义的组合任务中，一个变量通过多个计算路径影响输出时，我们表明Transformer学习到的上下文依赖状态表示会损害其性能和互操作性。第三，链式思维监督提升了多步任务的训练数据效率，但仍难以处理路径歧义。最后，我们提出了基于机制的分类法，区分神经网络三种泛化方式：结构基于（受覆盖限制的），属性基于（利用代数不变性），和共享操作符（通过功能重用）。这一概念框架剖析了我们的结果，并突显了实现真正系统组合性所需的新架构思路。总体而言，覆盖原则提供了一个统一的视角来理解组合推理，并强调了需要基础架构或训练创新以实现真正的系统组合性。', 'title_zh': '覆盖原理：理解组合泛化的框架'}
{'arxiv_id': 'arXiv:2505.20276', 'title': "Does quantization affect models' performance on long-context tasks?", 'authors': 'Anmol Mekala, Anirudh Atmakuru, Yixiao Song, Marzena Karpinska, Mohit Iyyer', 'link': 'https://arxiv.org/abs/2505.20276', 'abstract': 'Large language models (LLMs) now support context windows exceeding 128K tokens, but this comes with significant memory requirements and high inference latency. Quantization can mitigate these costs, but may degrade performance. In this work, we present the first systematic evaluation of quantized LLMs on tasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4, GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B, and 72B). We find that, on average, 8-bit quantization preserves accuracy (~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for tasks involving long context inputs (drops of up to 59%). This degradation tends to worsen when the input is in a language other than English. Crucially, the effects of quantization depend heavily on the quantization method, model, and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4, Llama-3.1 70B experiences a 32% performance drop on the same task. These findings highlight the importance of a careful, task-specific evaluation before deploying quantized LLMs, particularly in long-context scenarios and with languages other than English.', 'abstract_zh': '量化大型语言模型在长输入和长输出任务中的系统评估', 'title_zh': '量化会影响模型在长上下文任务上的性能吗？'}
{'arxiv_id': 'arXiv:2505.20259', 'title': 'Lifelong Safety Alignment for Language Models', 'authors': 'Haoyu Wang, Zeyu Qin, Yifei Zhao, Chao Du, Min Lin, Xueqian Wang, Tianyu Pang', 'link': 'https://arxiv.org/abs/2505.20259', 'abstract': "LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailbreaking attacks designed to bypass safety alignment. While many existing defenses focus on known types of attacks, it is more critical to prepare LLMs for unseen attacks that may arise during deployment. To address this, we propose a lifelong safety alignment framework that enables LLMs to continuously adapt to new and evolving jailbreaking strategies. Our framework introduces a competitive setup between two components: a Meta-Attacker, trained to actively discover novel jailbreaking strategies, and a Defender, trained to resist them. To effectively warm up the Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a large collection of jailbreak-related research papers. Through iterative training, the first iteration Meta-Attacker achieves a 73% attack success rate (ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks. Meanwhile, the Defender progressively improves its robustness and ultimately reduces the Meta-Attacker's success rate to just 7%, enabling safer and more reliable deployment of LLMs in open-ended environments. The code is available at this https URL.", 'abstract_zh': 'LLMs在应对未知的脱笼攻击方面的终身安全对齐框架', 'title_zh': '终身安全性对齐的语言模型'}
{'arxiv_id': 'arXiv:2505.20249', 'title': 'WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models', 'authors': 'Yongan Yu, Qingchen Hu, Xianda Du, Jiayin Wang, Fengran Mo, Renee Sieber', 'link': 'https://arxiv.org/abs/2505.20249', 'abstract': 'Climate change adaptation requires the understanding of disruptive weather impacts on society, where large language models (LLMs) might be applicable. However, their effectiveness is under-explored due to the difficulty of high-quality corpus collection and the lack of available benchmarks. The climate-related events stored in regional newspapers record how communities adapted and recovered from disasters. However, the processing of the original corpus is non-trivial. In this study, we first develop a disruptive weather impact dataset with a four-stage well-crafted construction pipeline. Then, we propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs on disruptive weather impacts. The benchmark involves two evaluation tasks, multi-label classification and ranking-based question answering. Extensive experiments on evaluating a set of LLMs provide first-hand analysis of the challenges in developing disruptive weather impact understanding and climate change adaptation systems. The constructed dataset and the code for the evaluation framework are available to help society protect against vulnerabilities from disasters.', 'abstract_zh': '气候适应要求理解破坏性天气对社会的影响，其中大型语言模型(LLMs)可能适用。然而，由于高质量语料库收集的难度和可用基准的缺乏，其效果尚待探索。记录在区域报纸中的气候相关事件展示了社区如何适应和恢复灾害。然而，处理原始语料库并非易事。在本研究中，我们首先开发了一个破坏性天气影响数据集，并构建了一个四阶段精心设计的构造管道。然后，我们提出了WXImpactBench，这是第一个用于评估LLMs在破坏性天气影响方面能力的基准。该基准包含两个评估任务：多标签分类和基于排名的问答。对一组LLMs进行的广泛实验提供了第一手分析，揭示了开发破坏性天气影响理解和气候适应系统的挑战。所构建的数据集和评估框架的代码可供社会保护免受灾难的影响。', 'title_zh': 'WXImpactBench: 一项颠覆性的天气影响理解基准，用于评估大型语言模型'}
{'arxiv_id': 'arXiv:2505.20245', 'title': 'KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing', 'authors': 'Rui Li, Quanyu Dai, Zeyu Zhang, Xu Chen, Zhenhua Dong, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2505.20245', 'abstract': "Recent advances in retrieval-augmented generation (RAG) furnish large language models (LLMs) with iterative retrievals of relevant information to handle complex multi-hop questions. These methods typically alternate between LLM reasoning and retrieval to accumulate external information into the LLM's context. However, the ever-growing context inherently imposes an increasing burden on the LLM to perceive connections among critical information pieces, with futile reasoning steps further exacerbating this overload issue. In this paper, we present KnowTrace, an elegant RAG framework to (1) mitigate the context overload and (2) bootstrap higher-quality multi-step reasoning. Instead of simply piling the retrieved contents, KnowTrace autonomously traces out desired knowledge triplets to organize a specific knowledge graph relevant to the input question. Such a structured workflow not only empowers the LLM with an intelligible context for inference, but also naturally inspires a reflective mechanism of knowledge backtracing to identify contributive LLM generations as process supervision data for self-bootstrapping. Extensive experiments show that KnowTrace consistently surpasses existing methods across three multi-hop question answering benchmarks, and the bootstrapped version further amplifies the gains.", 'abstract_zh': '最近在检索增强生成（RAG）方面的进展为大型语言模型（LLMs）提供了迭代的相关信息检索能力，以处理复杂的多跳问题。然而，不断增长的上下文本身对LLM感知关键信息片段之间联系施加了不断增加的负担，而无效的推理步骤进一步加剧了这一负担问题。本文提出了一种简洁的RAG框架——KnowTrace，以（1）减轻上下文负担，（2）实现更高质量的多步推理。KnowTrace 不是简单地堆叠检索到的内容，而是自主地提取所需的知识三元组来组织与输入问题相关的特定知识图谱。这种结构化的工作流程不仅为LLM提供了可用于推理的明晰上下文，还自然启发了一种知识回溯反思机制，以识别有助于LLM生成的过程监督数据，从而实现自我提升。大量实验表明，KnowTrace 在三个多跳问答基准测试中持续超越现有方法，且自提升版本进一步放大了性能提升。', 'title_zh': 'KnowTrace：基于结构化知识追踪的迭代检索增强生成启动方法'}
{'arxiv_id': 'arXiv:2505.20241', 'title': 'DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning', 'authors': 'Qi Cao, Ruiyi Wang, Ruiyi Zhang, Sai Ashish Somayajula, Pengtao Xie', 'link': 'https://arxiv.org/abs/2505.20241', 'abstract': "Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM's domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.", 'abstract_zh': 'DreamPRM：一种用于多模态PRM的领域重权重训练框架', 'title_zh': '梦PRM：领域加权过程奖励模型用于多模态推理'}
{'arxiv_id': 'arXiv:2505.20211', 'title': 'Parameter-Efficient Fine-Tuning with Column Space Projection', 'authors': 'Junseo Hwang, Wonguk Cho, Taesup Kim', 'link': 'https://arxiv.org/abs/2505.20211', 'abstract': 'Fine-tuning large language models (LLMs) with minimal computational overhead is essential for efficiently adapting them to downstream tasks under resource constraints. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), facilitate this by updating only a small subset of parameters. However, recent studies show that LoRA diverges from full fine-tuning (Full FT) in its learning behavior, particularly in terms of spectral properties. Motivated by these findings, we propose PiCa, the first theoretically grounded PEFT method based on the spectral properties of fine-tuned weights. PiCa projects gradients onto the low-rank column subspace of pre-trained weights and exhibits learning patterns more closely aligned with Full FT. Furthermore, we show that combining PiCa with weight sharing drastically reduces the number of trainable parameters without compromising performance, enabling to achieve superior performance than LoRA using 13x fewer trainable parameters. Extensive experiments demonstrate PiCa achieves the state-of-the-art performance compared to existing PEFT methods.', 'abstract_zh': '基于微调权重谱性质的参数高效微调方法PiCa：在较少可训练参数下实现优越性能', 'title_zh': '列空间投影下的参数高效微调'}
{'arxiv_id': 'arXiv:2505.20206', 'title': 'Evaluating Large Language Models for Code Review', 'authors': 'Umut Cihan, Arda İçöz, Vahid Haratian, Eray Tüzün', 'link': 'https://arxiv.org/abs/2505.20206', 'abstract': 'Context: Code reviews are crucial for software quality. Recent AI advances have allowed large language models (LLMs) to review and fix code; now, there are tools that perform these reviews. However, their reliability and accuracy have not yet been systematically evaluated. Objective: This study compares different LLMs\' performance in detecting code correctness and suggesting improvements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated code blocks of varying correctness, along with 164 canonical code blocks from the HumanEval benchmark. To simulate the code review task objectively, we expected LLMs to assess code correctness and improve the code if needed. We ran experiments with different configurations and reported on the results. Results: With problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code correctness 68.50% and 63.89% of the time, respectively, and corrected the code 67.83% and 54.26% of the time for the 492 code blocks of varying correctness. Without problem descriptions, performance declined. The results for the 164 canonical code blocks differed, suggesting that performance depends on the type of code. Conclusion: LLM code reviews can help suggest improvements and assess correctness, but there is a risk of faulty outputs. We propose a process that involves humans, called the "Human in the loop LLM Code Review" to promote knowledge sharing while mitigating the risk of faulty outputs.', 'abstract_zh': '基于代码审查的大型语言模型性能研究：从算法生成代码到人类评估的全流程探索', 'title_zh': '评估大型语言模型在代码审查中的应用'}
{'arxiv_id': 'arXiv:2505.20184', 'title': 'THiNK: Can Large Language Models Think-aloud?', 'authors': 'Yongan Yu, Mengqian Wu, Yiran Lin, Nikki G. Lobczowski', 'link': 'https://arxiv.org/abs/2505.20184', 'abstract': "Assessing higher-order thinking skills in large language models (LLMs) remains a fundamental challenge, especially in tasks that go beyond surface-level accuracy. In this work, we propose THiNK (Testing Higher-order Notion of Knowledge), a multi-agent, feedback-driven evaluation framework grounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative task of problem generation, critique, and revision, encouraging LLMs to think-aloud through step-by-step reflection and refinement. This enables a systematic evaluation of both lower-order (e.g., remember, understand) and higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven state-of-the-art LLMs and perform a detailed cognitive analysis of their outputs. Results reveal that while models reliably perform lower-order categories well, they struggle with applying knowledge in realistic contexts and exhibit limited abstraction. Structured feedback loops significantly improve reasoning performance, particularly in higher-order thinking. Qualitative evaluations further confirm that THiNK-guided outputs better align with domain logic and problem structure. The code of our framework provides a scalable methodology for probing and enhancing LLM reasoning, offering new directions for evaluation grounded in learning science, which is available at our GitHub repository.", 'abstract_zh': '评估大规模语言模型的高级思维技能仍然是一个基本挑战，尤其是在超越表面准确性任务的情况下。在此项工作中，我们提出了THiNK（Testing Higher-order Notion of Knowledge）多智能体、基于反馈的评估框架，该框架基于布卢姆分类法。THiNK将推理评估框架视为一个问题生成、批判和修订的迭代任务，鼓励LLMs通过逐步反思和精炼来思考。这使得能够系统地评估较低层次（如记忆、理解）和较高层次（如评价、创造）的思维技能。我们将在七种最先进的LLM上应用THiNK，并对它们的输出进行全面的认知分析。结果表明，虽然模拟能够可靠地执行较低层次的类别，但在现实场景中应用知识和抽象方面却存在局限性。结构化的反馈循环显著提高了推理性能，特别是在高级思维技能方面。定性评估进一步证实，THiNK导向的输出更好地与领域逻辑和问题结构相一致。我们框架的代码提供了一种可扩展的方法来探测和增强LLM推理能力，为基于学习科学的新评估方向提供了新的途径，该代码可在我们的GitHub存储库中获取。', 'title_zh': 'THiNK: 大型语言模型能进行自我讲解吗？'}
{'arxiv_id': 'arXiv:2505.20166', 'title': 'From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data', 'authors': 'Chun-Yi Kuan, Hung-yi Lee', 'link': 'https://arxiv.org/abs/2505.20166', 'abstract': "Audio-aware large language models (ALLMs) have recently made great strides in understanding and processing audio inputs. These models are typically adapted from text-based large language models (LLMs) through additional training on audio-related tasks. However, this adaptation process presents two major limitations. First, ALLMs often suffer from catastrophic forgetting, where important textual capabilities such as instruction-following are lost after training on audio data. In some cases, models may even hallucinate sounds that are not present in the input audio, raising concerns about their reliability. Second, achieving cross-modal alignment between audio and language typically relies on large collections of task-specific question-answer pairs for instruction tuning, making the process resource-intensive. To address these issues, we leverage the backbone LLMs from ALLMs to synthesize general-purpose caption-style alignment data. We refer to this process as bootstrapping audio-language alignment via synthetic data generation from backbone LLMs (BALSa). Building on BALSa, we introduce LISTEN (Learning to Identify Sounds Through Extended Negative Samples), a contrastive-like training method designed to improve ALLMs' ability to distinguish between present and absent sounds. We further extend BALSa to multi-audio scenarios, where the model either explains the differences between audio inputs or produces a unified caption that describes them all, thereby enhancing audio-language alignment. Experimental results indicate that our method effectively mitigates audio hallucinations while reliably maintaining strong performance in audio understanding, reasoning, and instruction-following skills. Moreover, incorporating multi-audio training further enhances the model's comprehension and reasoning capabilities. Overall, BALSa offers an efficient and scalable approach to the development of ALLMs.", 'abstract_zh': 'Audio-aware 大语言模型 (ALLMs) 在理解和处理音频输入方面取得了显著进展。这些模型通常通过额外的音频相关任务训练，从基于文本的大语言模型 (LLMs) 调整而来。然而，这一调整过程存在两大局限。首先，ALLMs 经常遭受灾难性遗忘，即在处理音频数据后，可能会丧失重要的文本能力，如指令遵循。在某些情况下，模型甚至会想象出输入音频中不存在的声音，这对其可靠性提出了质疑。其次，实现音频与语言的跨模态对齐通常依赖于大量特定任务的问题-答案对，用于指令调优，使过程耗资巨大。为解决这些问题，我们利用 ALLMs 的骨干 LLM 组件来合成通用的旁注风格对齐数据。我们称这一过程为基于骨干 LLM 生成合成数据的音频-语言对齐自举 (BALSa)。基于 BALSa，我们引入了 LISTEN (Learning to Identify Sounds Through Extended Negative Samples)，一种对比训练方法，旨在提高 ALLMs 区分存在和不存在声音的能力。我们进一步将 BALSa 扩展到多音频场景中，其中模型要么解释音频输入之间的差异，要么生成一个统一的旁注来描述所有输入，从而增强音频-语言对齐。实验结果表明，我们的方法有效减少了音频错觉，同时可靠地保持了在音频理解、推理和指令遵循技能方面的强大表现。此外，多音频训练进一步增强了模型的理解和推理能力。总体而言，BALSa 提供了一种高效且可扩展的方法来开发 ALLMs。', 'title_zh': '从对齐到提升：利用合成数据 bootstrapping 音频-语言对齐'}
{'arxiv_id': 'arXiv:2505.20161', 'title': 'Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning', 'authors': 'Jaehun Jung, Seungju Han, Ximing Lu, Skyler Hallinan, David Acuna, Shrimai Prabhumoye, Mostafa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Yejin Choi', 'link': 'https://arxiv.org/abs/2505.20161', 'abstract': "Effective generalization in language models depends critically on the diversity of their training data. Yet existing diversity metrics often fall short of this goal, relying on surface-level heuristics that are decoupled from model behavior. This motivates us to ask: What kind of diversity in training data actually drives generalization in language models -- and how can we measure and amplify it? Through large-scale empirical analyses spanning over 300 training runs, carefully controlled for data scale and quality, we show that data diversity can be a strong predictor of generalization in LLM reasoning -- as measured by average model performance on unseen out-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies diversity via the entropy of model-induced gradients. Despite using a small off-the-shelf proxy model for gradients, G-Vendi consistently outperforms alternative measures, achieving strong correlation (Spearman's $\\rho \\approx 0.9$) with out-of-distribution (OOD) performance on both natural language inference (NLI) and math reasoning tasks. Building on this insight, we present Prismatic Synthesis, a framework for generating diverse synthetic data by targeting underrepresented regions in gradient space. Experimental results show that Prismatic Synthesis consistently improves model performance as we scale synthetic data -- not just on in-distribution test but across unseen, out-of-distribution benchmarks -- significantly outperforming state-of-the-art models that rely on 20 times larger data generator than ours. For example, PrismMath-7B, our model distilled from a 32B LLM, outperforms R1-Distill-Qwen-7B -- the same base model trained on proprietary data generated by 671B R1 -- on 6 out of 7 challenging benchmarks.", 'abstract_zh': '有效的语言模型泛化依赖于其训练数据的多样性。现有的多样性度量往往未能达到这一目标，依靠与模型行为脱钩的表面级启发式方法。这促使我们思考：哪些类型的训练数据多样性实际上能够驱动语言模型的泛化能力——我们如何衡量和放大这种多样性？通过跨越300多次训练运行的大规模实证分析，严格控制数据规模和质量，我们展示了数据多样性可以是大型语言模型推理中泛化能力的一个强大预测指标——以未见过的分布外基准上的平均模型性能为准。我们引入了G-Vendi度量，通过模型诱导梯度的熵量化多样性。尽管使用了一个小型现成的梯度代理模型，G-Vendi始终优于其他度量标准，在自然语言推理（NLI）和数学推理任务上的分布外（OOD）性能上实现了高度的相关性（斯皮尔曼相关系数ρ ≈ 0.9）。在此基础上，我们提出了棱镜合成框架，该框架通过瞄准梯度空间中未被充分代表的区域生成多样化的合成数据。实验结果表明，随着合成数据规模的增加，棱镜合成持续提高模型性能——不仅在分布在内测试中，在未见过的分布外基准上也是如此，显著优于依赖数据生成器比我们大20倍的最新模型。例如，从一个32B大型语言模型中精简得到的PrismMath-7B，在7个具有挑战性的基准中有6个上优于基于671B R1生成的私有数据训练的R1-Distill-Qwen-7B。', 'title_zh': '棱柱合成：基于梯度的数据多样化增强大语言模型推理的泛化能力'}
{'arxiv_id': 'arXiv:2505.20139', 'title': "StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs", 'authors': 'Jialin Yang, Dongfu Jiang, Lipeng He, Sherman Siu, Yuxuan Zhang, Disen Liao, Zhuofeng Li, Huaye Zeng, Yiming Jia, Haozhe Wang, Benjamin Schneider, Chi Ruan, Wentao Ma, Zhiheng Lyu, Yifei Wang, Yi Lu, Quy Duc Do, Ziyan Jiang, Ping Nie, Wenhu Chen', 'link': 'https://arxiv.org/abs/2505.20139', 'abstract': "As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: 1) generation tasks, producing structured output from natural language prompts, and 2) conversion tasks, translating between structured formats. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps, even state-of-the-art models like o1-mini achieve only 75.58 average score, with open-source alternatives lagging approximately 10 points behind. We find generation tasks more challenging than conversion tasks, and producing correct visual content more difficult than generating text-only structures.", 'abstract_zh': '随着大型语言模型（LLMs）在软件开发工作流中发挥越来越重要的作用，其生成结构化输出的能力变得至关重要。我们介绍了StructEval，这是一个全面的基准，用于评估LLMs在生成非渲染格式（JSON、YAML、CSV）和渲染格式（HTML、React、SVG）的结构化格式方面的能力。与其他基准不同，StructEval通过两种范式系统地评估了不同格式的结构保真度：1）生成任务，从自然语言提示生成结构化输出；2）转换任务，进行格式之间的转换。该基准涵盖了18种格式和44种任务类型，并引入了新的格式遵守性和结构正确性的指标。结果表明，即使是最先进的模型如o1-mini也只能获得75.58的平均分数，开源替代品落后约10分。我们发现生成任务比转换任务更具有挑战性，生成正确的视觉内容比生成纯文本结构更困难。', 'title_zh': 'StructEval: 评估大型语言模型生成结构性输出的能力'}
{'arxiv_id': 'arXiv:2505.20113', 'title': "Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone", 'authors': 'Cristian Santini, Laura Melosi, Emanuele Frontoni', 'link': 'https://arxiv.org/abs/2505.20113', 'abstract': "The increased digitization of world's textual heritage poses significant challenges for both computer science and literary studies. Overall, there is an urgent need of computational techniques able to adapt to the challenges of historical texts, such as orthographic and spelling variations, fragmentary structure and digitization errors. The rise of large language models (LLMs) has revolutionized natural language processing, suggesting promising applications for Named Entity Recognition (NER) on historical documents. In spite of this, no thorough evaluation has been proposed for Italian texts. This research tries to fill the gap by proposing a new challenging dataset for entity extraction based on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's Zibaldone (1898), containing 2,899 references to people, locations and literary works. This dataset was used to carry out reproducible experiments with both domain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1. Results show that instruction-tuned models encounter multiple difficulties handling historical humanistic texts, while fine-tuned NER models offer more robust performance even with challenging entity types such as bibliographic references.", 'abstract_zh': '世界文本遗产的数字化增加对计算机科学和文学研究提出了重大挑战。总体而言，迫切需要能够适应历史文本挑战的计算技术，如文字拼写变化、碎片化结构和数字化错误。大型语言模型的兴起已 revolutionized 自然语言处理，表明Named Entity Recognition (NER) 在历史文件上的潜在应用前景。尽管如此，尚未对意大利文本进行全面评估。本研究尝试通过基于19世纪学者笔记的新的挑战性数据集来填补这一空白，即詹科莫·莱翁帕迪的《杂录》（1898年），该数据集包含2,899个关于人物、地点和文学作品的引用。该数据集用于使用领域特定的BERT基模型和最先进的LLM（如LaMa3.1）进行可复现的实验。结果表明，指令调整模型在处理历史人文学科文本时遇到多种困难，而微调的NER模型即使在挑战性实体类型（如参考文献）上也能提供更稳健的性能。', 'title_zh': '历史意大利语中的命名实体识别：杰阿科莫·莱 Ammo·利的《杂录》案例研究'}
{'arxiv_id': 'arXiv:2505.20112', 'title': 'ResSVD: Residual Compensated SVD for Large Language Model Compression', 'authors': 'Haolei Bai, Siyong Jian, Tuo Liang, Yu Yin, Huan Wang', 'link': 'https://arxiv.org/abs/2505.20112', 'abstract': 'Large language models (LLMs) have demonstrated impressive capabilities in a wide range of downstream natural language processing tasks. Nevertheless, their considerable sizes and memory demands hinder practical deployment, underscoring the importance of developing efficient compression strategies. Singular value decomposition (SVD) decomposes a matrix into orthogonal components, enabling efficient low-rank approximation. This is particularly suitable for LLM compression, where weight matrices often exhibit significant redundancy. However, current SVD-based methods neglect the residual matrix from truncation, resulting in significant truncation loss. Additionally, compressing all layers of the model results in severe performance degradation. To overcome these limitations, we propose ResSVD, a new post-training SVD-based LLM compression method. Specifically, we leverage the residual matrix generated during the truncation process to reduce truncation loss. Moreover, under a fixed overall compression ratio, we selectively compress the last few layers of the model, which mitigates error propagation and significantly improves the performance of compressed this http URL evaluations of ResSVD on diverse LLM families and multiple benchmark datasets indicate that ResSVD consistently achieves superior performance over existing counterpart methods, demonstrating its practical effectiveness.', 'abstract_zh': '大型语言模型（LLMs）在各类下游自然语言处理任务中展现了令人印象深刻的性能。然而，其巨大的尺寸和内存需求阻碍了实际部署，突显了开发高效压缩策略的重要性。奇异值分解（SVD）将矩阵分解为正交组件，实现有效的低秩近似。这特别适用于LLM压缩，因为权重矩阵常表现出显著的冗余性。然而，当前的SVD基方法忽略了截断过程中的残差矩阵，导致了显著的截断损失。此外，压缩全部模型层会导致严重的性能下降。为克服这些限制，我们提出了一种新的后训练SVD基LLM压缩方法——ResSVD。具体而言，我们利用截断过程中生成的残差矩阵来减少截断损失。在固定的整体压缩率下，我们选择性地压缩模型的最后几层，这减少了误差传播并显著提高了压缩模型的性能。在不同LLM家族和多个基准数据集上的评估结果显示，ResSVD在对比现有方法时始终表现出更优的性能，证明了其实用有效性。', 'title_zh': 'ResSVD: 剩余补偿SVD大语言模型压缩'}
{'arxiv_id': 'arXiv:2505.20109', 'title': 'Language-Agnostic Suicidal Risk Detection Using Large Language Models', 'authors': 'June-Woo Kim, Wonkyo Oh, Haram Yoon, Sung-Hoon Yoon, Dae-Jin Kim, Dong-Ho Lee, Sang-Yeol Lee, Chan-Mo Yang', 'link': 'https://arxiv.org/abs/2505.20109', 'abstract': 'Suicidal risk detection in adolescents is a critical challenge, yet existing methods rely on language-specific models, limiting scalability and generalization. This study introduces a novel language-agnostic framework for suicidal risk assessment with large language models (LLMs). We generate Chinese transcripts from speech using an ASR model and then employ LLMs with prompt-based queries to extract suicidal risk-related features from these transcripts. The extracted features are retained in both Chinese and English to enable cross-linguistic analysis and then used to fine-tune corresponding pretrained language models independently. Experimental results show that our method achieves performance comparable to direct fine-tuning with ASR results or to models trained solely on Chinese suicidal risk-related features, demonstrating its potential to overcome language constraints and improve the robustness of suicidal risk assessment.', 'abstract_zh': '青少年自杀风险检测是一个关键挑战，现有方法依赖于特定语言的模型，限制了其可扩展性和通用性。本研究提出了一种基于大规模语言模型的无语言障碍的自杀风险评估框架。我们使用ASR模型生成中文 transcript，然后利用基于提示的查询大规模语言模型从这些 transcript 中提取与自杀风险相关的特征。提取的特征同时保留中文和英文版本，以实现跨语言分析，并独立用于预训练语言模型的微调。实验结果表明，我们的方法在性能上与直接使用ASR结果进行微调或仅使用中文自杀风险相关特征训练的模型相当，证明了其克服语言限制、提高自杀风险评估稳健性的潜力。', 'title_zh': '无需自杀风险检测的语言依存性Large语言模型方法'}
{'arxiv_id': 'arXiv:2505.20100', 'title': 'AdaTP: Attention-Debiased Token Pruning for Video Large Language Models', 'authors': 'Fengyuan Sun, Leqi Shen, Hui Chen, Sicheng Zhao, Jungong Han, Guiguang Ding', 'link': 'https://arxiv.org/abs/2505.20100', 'abstract': 'Video Large Language Models (Video LLMs) have achieved remarkable results in video understanding tasks. However, they often suffer from heavy computational overhead due to the large number of visual tokens generated from multiple video frames. Existing visual token compression methods often rely on attention scores from language models as guidance. However, these scores exhibit inherent biases: global bias reflects a tendency to focus on the two ends of the visual token sequence, while local bias leads to an over-concentration on the same spatial positions across different frames. To address the issue of attention bias, we propose $\\textbf{A}$ttention-$\\textbf{D}$ebi$\\textbf{a}$sed $\\textbf{T}$oken $\\textbf{P}$runing for Video Large Language Models ($\\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP integrates two dedicated debiasing modules into the pipeline, targeting global attention bias and local attention bias, respectively. Without the need for additional training, our method significantly reduces the computational overhead of Video LLMs while retaining the performance of vanilla models. Extensive evaluation shows that AdaTP achieves state-of-the-art performance in various commonly used video understanding benchmarks. In particular, on LLaVA-OneVision-7B, AdaTP maintains performance without degradation while using only up to $27.3\\%$ FLOPs compared to the vanilla model. Our code will be released soon.', 'abstract_zh': '视频大型语言模型中的注意力去偏Token裁剪（Attention-Debiased Token Pruning for Video Large Language Models, AdaTP）', 'title_zh': 'AdaTP: 基于注意力偏差的_token剪枝方法用于视频大型语言模型'}
{'arxiv_id': 'arXiv:2505.20099', 'title': 'Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities', 'authors': 'Chuangtao Ma, Yongrui Chen, Tianxing Wu, Arijit Khan, Haofen Wang', 'link': 'https://arxiv.org/abs/2505.20099', 'abstract': "Large language models (LLMs) have demonstrated remarkable performance on question-answering (QA) tasks because of their superior capabilities in natural language understanding and generation. However, LLM-based QA struggles with complex QA tasks due to poor reasoning capacity, outdated knowledge, and hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs) for QA to address the above challenges. In this survey, we propose a new structured taxonomy that categorizes the methodology of synthesizing LLMs and KGs for QA according to the categories of QA and the KG's role when integrating with LLMs. We systematically survey state-of-the-art advances in synthesizing LLMs and KGs for QA and compare and analyze these approaches in terms of strength, limitations, and KG requirements. We then align the approaches with QA and discuss how these approaches address the main challenges of different complex QA. Finally, we summarize the advancements, evaluation metrics, and benchmark datasets and highlight open challenges and opportunities.", 'abstract_zh': '大型语言模型（LLMs）在问答（QA）任务上因其在自然语言理解和生成方面的出色能力而表现出色。然而，基于LLM的问答在处理复杂的问答任务时因推理能力差、知识过时和幻觉等问题而遇到困难。多项近期工作通过将LLMs与知识图谱（KGs）结合来解决上述挑战。在本文综述中，我们提出了一种新的结构化分类法，根据问答类型和知识图谱在集成LLMs时的角色来分类合成LLMs和KGs的方法。我们系统地综述了合成LLMs和KGs用于问答的最新进展，并从优势、局限性和KG需求方面对比和分析这些方法。然后将这些方法与问答类型对齐，并讨论这些方法如何解决不同复杂问答的主要挑战。最后，我们总结了这些方法的进步、评估指标和基准数据集，并指出了开放的挑战和机遇。', 'title_zh': '大型语言模型与知识图谱结合进行问答：综述与机遇'}
{'arxiv_id': 'arXiv:2505.20081', 'title': 'Inference-time Alignment in Continuous Space', 'authors': 'Yige Yuan, Teng Xiao, Li Yunfan, Bingbing Xu, Shuchang Tao, Yunqi Qiu, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2505.20081', 'abstract': 'Aligning large language models with human feedback at inference time has received increasing attention due to its flexibility. Existing methods rely on generating multiple responses from the base policy for search using a reward model, which can be considered as searching in a discrete response space. However, these methods struggle to explore informative candidates when the base policy is weak or the candidate set is small, resulting in limited effectiveness. In this paper, to address this problem, we propose Simple Energy Adaptation ($\\textbf{SEA}$), a simple yet effective algorithm for inference-time alignment. In contrast to expensive search over the discrete space, SEA directly adapts original responses from the base policy toward the optimal one via gradient-based sampling in continuous latent space. Specifically, SEA formulates inference as an iterative optimization procedure on an energy function over actions in the continuous space defined by the optimal policy, enabling simple and effective alignment. For instance, despite its simplicity, SEA outperforms the second-best baseline with a relative improvement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on MATH. Our code is publicly available at this https URL', 'abstract_zh': '在推理时通过人类反馈对大型语言模型进行对齐由于其灵活性而受到了越来越多的关注。现有方法依赖于使用奖励模型从基策略生成多个响应进行搜索，可以被视为在离散响应空间中搜索。然而，当基策略较弱或候选集较小时，这些方法难以探索出信息量大的候选者，导致效果有限。在本文中，为了解决这一问题，我们提出了一种简单有效的推理时对齐算法——简单能量适应（Simple Energy Adaptation，SEA）。与在离散空间中昂贵的搜索不同，SEA 直接通过基于梯度的采样在连续的潜在空间中对基策略的原始响应进行向最优响应的适应。具体而言，SEA 将推理表述为对连续空间中动作的最优策略定义的能量函数的迭代优化过程，从而实现简单的有效对齐。例如，尽管其简单性，SEA 在 AdvBench 上优于第二好的基线方法，相对改进高达 77.51%，在 MATH 上则为 16.36%。我们的代码已公开。', 'title_zh': '推理时连续空间对齐'}
{'arxiv_id': 'arXiv:2505.20072', 'title': 'Incentivizing Reasoning from Weak Supervision', 'authors': 'Yige Yuan, Teng Xiao, Shuchang Tao, Xue Wang, Jinyang Gao, Bolin Ding, Bingbing Xu', 'link': 'https://arxiv.org/abs/2505.20072', 'abstract': 'Large language models (LLMs) have demonstrated impressive performance on reasoning-intensive tasks, but enhancing their reasoning abilities typically relies on either reinforcement learning (RL) with verifiable signals or supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT) demonstrations, both of which are expensive. In this paper, we study a novel problem of incentivizing the reasoning capacity of LLMs without expensive high-quality demonstrations and reinforcement learning. We investigate whether the reasoning capabilities of LLMs can be effectively incentivized via supervision from significantly weaker models. We further analyze when and why such weak supervision succeeds in eliciting reasoning abilities in stronger models. Our findings show that supervision from significantly weaker reasoners can substantially improve student reasoning performance, recovering close to 94% of the gains of expensive RL at a fraction of the cost. Experiments across diverse benchmarks and model architectures demonstrate that weak reasoners can effectively incentivize reasoning in stronger student models, consistently improving performance across a wide range of reasoning tasks. Our results suggest that this simple weak-to-strong paradigm is a promising and generalizable alternative to costly methods for incentivizing strong reasoning capabilities at inference-time in LLMs. The code is publicly available at this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在推理密集型任务中表现出色，但增强其推理能力通常依赖于验证信号的强化学习（RL）或优质长推理链（CoT）示范的监督微调（SFT），这两种方法都代价高昂。本文研究了一种新的问题，即在不使用昂贵的优质示范和强化学习的情况下激励LLM的推理能力。我们探讨了是否可以通过来自显著更弱模型的监督来有效激励LLM的推理能力。我们进一步分析了这种弱监督在更强模型中引发推理能力的时机和原因。我们的发现表明，显著更弱的推理者的监督可以显著提高学生模型的推理性能，几乎恢复了昂贵的RL带来的94%的收益，而成本仅为后者的几分之一。跨多种基准和模型架构的实验表明，弱推理者可以有效地激励更强学生模型的推理能力，在广泛范围的推理任务中一致提高性能。我们的结果表明，这种简单的弱到强范式是激励LLM推理能力的一种有前途且可推广的替代方案，成本高昂的方法。代码已公开，可通过以下链接访问：this https URL。', 'title_zh': '从弱监督中激励推理'}
{'arxiv_id': 'arXiv:2505.20068', 'title': 'On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction', 'authors': 'Qingyu Liang, Jaime Banks', 'link': 'https://arxiv.org/abs/2505.20068', 'abstract': "Shared understanding plays a key role in the effective communication in and performance of human-human interactions. With the increasingly common integration of AI into human contexts, the future of personal and workplace interactions will likely see human-AI interaction (HAII) in which the perception of shared understanding is important. Existing literature has addressed the processes and effects of PSU in human-human interactions, but the construal remains underexplored in HAII. To better understand PSU in HAII, we conducted an online survey to collect user reflections on interactions with a large language model when it sunderstanding of a situation was thought to be similar to or different from the participant's. Through inductive thematic analysis, we identified eight dimensions comprising PSU in human-AI interactions: Fluency, aligned operation, fluidity, outcome satisfaction, contextual awareness, lack of humanlike abilities, computational limits, and suspicion.", 'abstract_zh': '共享理解在人类与人工智能互动中的有效沟通和表现中发挥关键作用。随着人工智能在人类环境中的日益融合，个人和工作场所互动的未来可能将出现人类-人工智能互动（HAII），其中共享理解的感觉至关重要。现有文献已经探讨了人类间互动中感知共享理解（PSU）的过程和影响，但在HAII中对PSU的构念探讨仍不足。为了更好地理解HAII中的PSU，我们进行了一项在线调查，收集用户对与大规模语言模型互动的反思，其中模型对情况的理解被认为与参与者相似或不同。通过归纳主题分析，我们识别出八个人机互动中PSU的维度：流畅性、操作对齐、流动性、结果满意度、情境意识、缺乏人类能力、计算限制和怀疑。', 'title_zh': '在同一页面：人类-人工智能交互中感知共享理解的维度'}
{'arxiv_id': 'arXiv:2505.20065', 'title': 'SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety', 'authors': 'Geon-Hyeong Kim, Youngsoo Jang, Yu Jin Kim, Byoungjip Kim, Honglak Lee, Kyunghoon Bae, Moontae Lee', 'link': 'https://arxiv.org/abs/2505.20065', 'abstract': 'As Large Language Models (LLMs) continue to advance and find applications across a growing number of fields, ensuring the safety of LLMs has become increasingly critical. To address safety concerns, recent studies have proposed integrating safety constraints into Reinforcement Learning from Human Feedback (RLHF). However, these approaches tend to be complex, as they encompass complicated procedures in RLHF along with additional steps required by the safety constraints. Inspired by Direct Preference Optimization (DPO), we introduce a new algorithm called SafeDPO, which is designed to directly optimize the safety alignment objective in a single stage of policy learning, without requiring relaxation. SafeDPO introduces only one additional hyperparameter to further enhance safety and requires only minor modifications to standard DPO. As a result, it eliminates the need to fit separate reward and cost models or to sample from the language model during fine-tuning, while still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO achieves competitive performance compared to state-of-the-art safety alignment algorithms, both in terms of aligning with human preferences and improving safety.', 'abstract_zh': '随着大型语言模型（LLMs）不断发展并在越来越多的领域找到应用场景，确保LLMs的安全性变得愈发重要。为了应对安全问题，近期的研究提出了将安全约束集成到基于人类反馈的强化学习（RLHF）中的方法。然而，这些方法通常较为复杂，因为它们不仅包含了RLHF中的复杂流程，还需要执行额外的安全约束步骤。受直接偏好优化（DPO）的启发，我们引入了一种新的算法SafeDPO，该算法旨在在策略学习的单阶段中直接优化安全性对齐目标，而无需进行松弛处理。SafeDPO仅引入了一个额外的超参数来进一步增强安全性，并只需对标准DPO进行少量修改。因此，它消除了为模型微调时拟合独立的奖励和成本模型或从语言模型中采样的需要，同时仍然提高了LLMs的安全性。最后，我们证明SafeDPO在与人类偏好对齐和提高安全性方面均能达到与最先进的安全性对齐算法相当的性能。', 'title_zh': 'SafeDPO：一种增强安全性的一步偏好优化方法'}
{'arxiv_id': 'arXiv:2505.20047', 'title': 'Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks', 'authors': 'Debargha Ganguly, Vikash Singh, Sreehari Sankar, Biyao Zhang, Xuecen Zhang, Srinivasan Iyengar, Xiaotian Han, Amit Sharma, Shivkumar Kalyanaraman, Vipin Chaudhary', 'link': 'https://arxiv.org/abs/2505.20047', 'abstract': "Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.", 'abstract_zh': '大型语言模型（LLMs）在通过生成形式化规范来 democratize 自动推理方面展示了显著的潜力。然而，存在一个根本性的紧张关系：LLMs 是概率性的，而形式化验证要求确定性的保证。本文通过全面研究 LLMS 生成形式化制品的失败模式和不确定性量化（UQ）来弥合这一认识论差距。系统评估五种前沿LLMs揭示了基于 Satisfiability Modulo Theories (SMT) 的自动形式化在不同领域对准确性的影响（从逻辑任务的+34.8% 到事实任务的-44.5%），已知的 UQ 技术如词概率的熵无法识别这些错误。我们引入了一个概率上下文自由文法（PCFG）框架来建模 LLM 的输出，从而得到一个细化的不确定性分类法。我们发现不确定性信号与任务相关（例如，语法熵适用于逻辑任务，AUROC>0.93）。最后，这些信号的轻量化融合能够实现选择性验证，大幅减少错误（14% 到 100%），同时几乎无弃权，将基于LLM的形式化转变为可靠的工程学科。', 'title_zh': '形式不确定性中的语法规则：何时在自动化推理任务中信任LLMs'}
{'arxiv_id': 'arXiv:2505.20029', 'title': 'Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)', 'authors': 'Subba Reddy Oota, Akshett Jindal, Ishani Mondal, Khushbu Pahwa, Satya Sai Srinath Namburi, Manish Shrivastava, Maneesh Singh, Bapi S. Raju, Manish Gupta', 'link': 'https://arxiv.org/abs/2505.20029', 'abstract': "Transformer-based language models, though not explicitly trained to mimic brain recordings, have demonstrated surprising alignment with brain activity. Progress in these models-through increased size, instruction-tuning, and multimodality-has led to better representational alignment with neural data. Recently, a new class of instruction-tuned multimodal LLMs (MLLMs) have emerged, showing remarkable zero-shot capabilities in open-ended multimodal vision tasks. However, it is unknown whether MLLMs, when prompted with natural instructions, lead to better brain alignment and effectively capture instruction-specific representations. To address this, we first investigate brain alignment, i.e., measuring the degree of predictivity of neural visual activity using text output response embeddings from MLLMs as participants engage in watching natural scenes. Experiments with 10 different instructions show that MLLMs exhibit significantly better brain alignment than vision-only models and perform comparably to non-instruction-tuned multimodal models like CLIP. We also find that while these MLLMs are effective at generating high-quality responses suitable to the task-specific instructions, not all instructions are relevant for brain alignment. Further, by varying instructions, we make the MLLMs encode instruction-specific visual concepts related to the input image. This analysis shows that MLLMs effectively capture count-related and recognition-related concepts, demonstrating strong alignment with brain activity. Notably, the majority of the explained variance of the brain encoding models is shared between MLLM embeddings of image captioning and other instructions. These results suggest that enhancing MLLMs' ability to capture task-specific information could lead to better differentiation between various types of instructions, and thereby improving their precision in predicting brain responses.", 'abstract_zh': '基于Transformer的语言模型虽然未明确训练以模拟脑电记录，但已展示了与脑活动惊人的契合度。随着这些模型通过增加规模、指令调优和多模态性的发展，其与神经数据的表征契合度得到了提高。最近，一类新的指令调优多模态LLM（MLLMs）出现了，它们在开放的多模态视觉任务中展现出显著的零样本能力。然而，目前尚不清楚当MLLMs接收到自然指令时，它们是否能更好地与大脑对齐，并有效捕捉指令特定的表征。为解决这一问题，我们首先探讨了大脑对齐问题，即通过使用参与者观看自然场景时MLLMs产生的文本输出响应嵌入来衡量神经视觉活动的预测性程度。实验结果显示，MLLMs在大脑对齐方面明显优于仅包含视觉模型，并且其表现与非指令调优的多模态模型如CLIP相当。此外，我们还发现，尽管这些MLLMs在生成符合任务特定指令的高质量响应方面非常有效，但并非所有指令都对大脑对齐有益。通过改变指令，MLLMs能够编码输入图像相关的指令特定视觉概念。这种分析表明，MLLMs有效地捕捉了数量相关和识别相关的概念，展示了与脑电活动的强烈契合度。值得注意的是，脑编码模型解释的大脑变异性大部分在MLLM图像说明嵌入和其他指令的嵌入之间共享。这些结果表明，增强MLLMs捕捉任务特定信息的能力可能有助于更好地区分各种类型指令之间的差异，从而提高它们预测脑电反应的精确度。', 'title_zh': '将指令调优（在多模态模型中）与视觉-语言处理（在大脑中）相关联'}
{'arxiv_id': 'arXiv:2505.19973', 'title': 'DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response', 'authors': 'Bilel Cherif, Tamas Bisztray, Richard A. Dubniczky, Aaesha Aldahmani, Saeed Alshehhi, Norbert Tihanyi', 'link': 'https://arxiv.org/abs/2505.19973', 'abstract': 'Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at this https URL.', 'abstract_zh': '数字取证与事件响应（DFIR）涉及分析数字证据以支持法律调查。大型语言模型（LLMs）为DFIR任务（如日志分析和内存取证）带来了新的机遇，但在高风险情境下，它们的错误和幻觉倾向引发了担忧。尽管兴趣日益浓厚，但仍缺乏一个综合基准来评估LLMs在理论和实践DFIR领域的表现。为填补这一空白，我们提出了DFIR-Metric基准，包括三个组成部分：（1）知识评估：700个专家评审的多项选择题，来源行业标准认证和官方文档；（2）现实取证挑战：150项类似CTF的任务，测试多步推理和证据关联；（3）实际分析：500个来自NIST计算机取证工具测试计划（CFTT）的硬盘和内存取证案例。我们使用DFIR-Metric评估了14个LLM，分析了它们在实验中的准确性和一致性。我们还引入了一个新的指标，任务理解得分（TUS），旨在更有效地评估在接近零准确率场景中模型的表现。该基准提供了一个严谨、可重复的基础，推动AI在数字取证领域的进步。所有脚本、成果和结果均可在项目网站上获取：this https URL。', 'title_zh': 'DFIR-Metric: 一个评估数字取证和事件响应中大型语言模型性能的数据集'}
{'arxiv_id': 'arXiv:2505.19966', 'title': 'Learning to Select In-Context Demonstration Preferred by Large Language Model', 'authors': 'Zheng Zhang, Shaocheng Lan, Lei Song, Jiang Bian, Yexin Li, Kan Ren', 'link': 'https://arxiv.org/abs/2505.19966', 'abstract': 'In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks during inference using only a few demonstrations. However, ICL performance is highly dependent on the selection of these demonstrations. Recent work explores retrieval-based methods for selecting query-specific demonstrations, but these approaches often rely on surrogate objectives such as metric learning, failing to directly optimize ICL performance. Consequently, they struggle to identify truly beneficial demonstrations. Moreover, their discriminative retrieval paradigm is ineffective when the candidate pool lacks sufficient high-quality demonstrations. To address these challenges, we propose GenICL, a novel generative preference learning framework that leverages LLM feedback to directly optimize demonstration selection for ICL. Experiments on 19 datasets across 11 task categories demonstrate that GenICL achieves superior performance than existing methods in selecting the most effective demonstrations, leading to better ICL performance.', 'abstract_zh': '基于语境的学习（ICL）使大规模语言模型（LLMs）能够在推理时仅通过少量示范适应新任务。然而，ICL 的性能高度依赖于这些示范的选择。近期研究探索了基于检索的方法来选择查询特定的示范，但这些方法往往依赖于代理目标，如度量学习，无法直接优化ICL性能。因此，它们难以识别真正有益的示范。此外，其鉴别性检索范式在候选池中缺乏足够的高质量示范时效果不佳。为应对这些挑战，我们提出了一种新颖的生成性偏好学习框架GenICL，该框架利用LLM反馈直接优化ICL的示范选择。实验结果表明，GenICL在选择最有效的示范方面优于现有方法，从而提高了ICL的性能。', 'title_zh': '学习选择大型语言模型偏好的上下文示例'}
{'arxiv_id': 'arXiv:2505.19964', 'title': 'The Limits of Preference Data for Post-Training', 'authors': 'Eric Zhao, Jessica Dai, Pranjal Awasthi', 'link': 'https://arxiv.org/abs/2505.19964', 'abstract': "Recent progress in strengthening the capabilities of large language models has stemmed from applying reinforcement learning to domains with automatically verifiable outcomes. A key question is whether we can similarly use RL to optimize for outcomes in domains where evaluating outcomes inherently requires human feedback; for example, in tasks like deep research and trip planning, outcome evaluation is qualitative and there are many possible degrees of success. One attractive and scalable modality for collecting human feedback is preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$ given outcomes, which one is preferred. In this work, we study a critical roadblock: preference data fundamentally and significantly limits outcome-based optimization. Even with idealized preference data (infinite, noiseless, and online), the use of ordinal feedback can prevent obtaining even approximately optimal solutions. We formalize this impossibility using voting theory, drawing an analogy between how a model chooses to answer a query with how voters choose a candidate to elect. This indicates that grounded human scoring and algorithmic innovations are necessary for extending the success of RL post-training to domains demanding human feedback. We also explore why these limitations have disproportionately impacted RLHF when it comes to eliciting reasoning behaviors (e.g., backtracking) versus situations where RLHF has been historically successful (e.g., instruction-tuning and safety training), finding that the limitations of preference data primarily suppress RLHF's ability to elicit robust strategies -- a class that encompasses most reasoning behaviors.", 'abstract_zh': 'Recent进展在增强大规模语言模型的能力方面源自将其强化学习应用到具有自动可验证结果的领域。一个关键问题是，我们是否可以类似地使用RL来优化那些评估结果本就需要人类反馈的领域中的结果；例如，在深度研究和旅行规划等任务中，结果评估是定性的，并且成功具有多种可能的程度。一种具有吸引力且可扩展的人类反馈收集方式是偏好数据：序数排名（成对或k-wise），它可以指示对于给定的k个结果，哪一个更受偏好。在这项工作中，我们研究了一个关键障碍：偏好数据从根本上和显著地限制了基于结果的优化。即使在理想化的偏好数据（无限的、无噪声的和在线的）情况下，序数反馈的使用也可能阻止获得近似最优解。我们使用投票理论来形式化这种不可能性，将模型选择如何回答查询与其如何选择候选人进行类比。这表明，针对需求人类反馈的领域，有必要结合具体的评分和算法创新，以使训练后的RL成功扩展到这些领域。我们还探讨了这些限制为何在对比RLHF在诱发出推理行为（例如，反向追踪）方面与其在历史上的成功领域（例如，指令调优和安全训练）时表现不同，发现偏好数据的限制主要抑制了RLHF诱发出稳健策略的能力——这一类别包括大多数推理行为。', 'title_zh': '训练后偏好数据的局限性'}
{'arxiv_id': 'arXiv:2505.19947', 'title': 'Dynamically Learned Test-Time Model Routing in Language Model Zoos with Service Level Guarantees', 'authors': 'Herbert Woisetschläger, Ryan Zhang, Shiqiang Wang, Hans-Arno Jacobsen', 'link': 'https://arxiv.org/abs/2505.19947', 'abstract': 'Open-weight LLM zoos provide access to numerous high-quality models, but selecting the appropriate model for specific tasks remains challenging and requires technical expertise. Most users simply want factually correct, safe, and satisfying responses without concerning themselves with model technicalities, while inference service providers prioritize minimizing operating costs. These competing interests are typically mediated through service level agreements (SLAs) that guarantee minimum service quality. We introduce MESS+, a stochastic optimization algorithm for cost-optimal LLM request routing while providing rigorous SLA compliance guarantees. MESS+ learns request satisfaction probabilities of LLMs in real-time as users interact with the system, based on which model selection decisions are made by solving a per-request optimization problem. Our algorithm includes a novel combination of virtual queues and request satisfaction prediction, along with a theoretical analysis of cost optimality and constraint satisfaction. Across a wide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of 2x cost savings compared to existing LLM routing techniques.', 'abstract_zh': '_OPEN-WEIGHT LLM 阵营提供了大量高质量模型的访问权限，但选择适合特定任务的模型仍然具有挑战性且需要技术专长。大多数用户只是希望获得事实正确、安全且令人满意的回答，而不关心模型的技术细节，而推理服务提供商则侧重于最小化运营成本。这些相互冲突的利益通常通过确保最低服务质量的服务水平协议（SLAs）来调解。我们提出了MESS+，这是一种随机优化算法，用于在提供严格的SLA合规性保证的同时实现成本最优的LLM请求路由。MESS+会在用户与系统交互时实时学习LLM的请求满意度概率，并基于此通过求解每个请求的优化问题来做出模型选择决策。我们的算法包括虚拟队列和请求满意度预测的新型结合，以及成本最优性和约束满足的理论分析。在一系列先进的LLM基准测试中，MESS+相比现有LLM路由技术平均实现了2倍的成本节约。', 'title_zh': '具有服务级别保证的语言模型动物园中的动态学习测试时模型路由'}
{'arxiv_id': 'arXiv:2505.19914', 'title': 'Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles', 'authors': 'Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, Hao Zhou, Mingxuan Wang', 'link': 'https://arxiv.org/abs/2505.19914', 'abstract': "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across seven categories, each with 1) a generator that produces unlimited examples with controllable difficulty and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from Enigmata further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of Enigmata. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Resources of this work can be found at this https URL.", 'abstract_zh': '大型语言模型（LLMs）如OpenAI的o1和DeepSeek的R1通过可验证奖励强化学习（RLVR）在数学和编程等高级推理任务中表现出色，但仍然难以解决无需领域知识即可解决的人类谜题。我们引入了Enigmata，这是首个专门用于提高LLMs谜题推理能力的综合方案。它包括36项任务，涵盖七个类别，每个任务包含1）生成器，能够生成无限数量、可调控难度的示例，以及2）基于规则的验证器，用于自动评估。这种生成器-验证器设计支持可扩展的多任务RL训练、精细的分析和无缝的RLVR集成。我们还提出了Enigmata-Eval，一个严格的基准，并开发了优化的多任务RLVR策略。经过训练的模型Qwen2.5-32B-Enigmata在谜题推理基准测试如Enigmata-Eval、ARC-AGI（32.8%）和ARC-AGI 2（0.6%）中，持续超越o3-mini-high和o1。它在超出领域知识的谜题基准测试和数学推理中表现出良好的泛化能力，几乎没有多任务训练的代价。当使用更大规模的模型如Seed1.5-Thinking（200亿激活参数和2000亿总参数）进行训练时，来自Enigmata的数据进一步提高了LLMs在高级数学和STEM推理任务上的性能，如AIME（2024-2025）、BeyondAIME和GPQA（Diamond），展示了Enigmata的良好泛化优势。该项工作提供了一个统一可控的框架，以推进LLMs的逻辑推理能力。更多资源可在以下链接找到。', 'title_zh': 'Enigmata：在大型语言模型中通过合成可验证谜题扩展逻辑推理能力'}
{'arxiv_id': 'arXiv:2505.19912', 'title': 'APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization', 'authors': 'Javier Marín', 'link': 'https://arxiv.org/abs/2505.19912', 'abstract': 'We present Adjacent Possible Exploration (APE), a simple yet effective method for adapting large language models to specific tasks using minimal computational resources. Unlike traditional fine-tuning that requires extensive compute, APE iteratively fine-tunes models on small, carefully selected data batches (200 examples), retaining only improvements. On news summarization, APE achieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes, matching or exceeding more complex methods like LoRA while remaining conceptually simple. Our approach is particularly valuable for researchers and practitioners with limited computational resources. We provide open-source code and demonstrate APE\'s effectiveness through both automatic metrics and human evaluation. While inspired by evolutionary theory\'s "adjacent possible", APE\'s core insight has a very practical application: small, iterative data perturbations can efficiently guide LLMs toward task-specific performance without expensive retraining.', 'abstract_zh': '我们提出相邻可能探索（APE），一种使用少量计算资源将大型语言模型适应于特定任务的简单而有效的方法。与需要大量计算的传统微调不同，APE 逐步在精心选择的数据批次（200 个示例）上进行微调，并仅保留改进。在新闻摘要任务上，APE 在使用单个 T4 GPU 仅 60 分钟内实现了 BLEU 得分 40% 的提升，其效果与 LoRA 等复杂方法相当或超越，同时保持概念上的简洁性。我们的方法特别适合计算资源有限的研究人员和 practitioners。我们提供了开源代码，并通过自动评价指标和人工评估展示了 APE 的有效性。虽然灵感源自进化理论的“相邻可能”，但 APE 的核心洞察具有非常实际的应用：小规模的迭代数据扰动可以有效引导大语言模型朝向任务特定性能，而无需昂贵的重新训练。', 'title_zh': 'APE：一种以数据为中心的高效LLM适应基准在文本摘要中的应用'}
{'arxiv_id': 'arXiv:2505.19887', 'title': 'Deconstructing Obfuscation: A four-dimensional framework for evaluating Large Language Models assembly code deobfuscation capabilities', 'authors': 'Anton Tkachenko, Dmitrij Suskevic, Benjamin Adolphi', 'link': 'https://arxiv.org/abs/2505.19887', 'abstract': 'Large language models (LLMs) have shown promise in software engineering, yet their effectiveness for binary analysis remains unexplored. We present the first comprehensive evaluation of commercial LLMs for assembly code deobfuscation. Testing seven state-of-the-art models against four obfuscation scenarios (bogus control flow, instruction substitution, control flow flattening, and their combination), we found striking performance variations--from autonomous deobfuscation to complete failure. We propose a theoretical framework based on four dimensions: Reasoning Depth, Pattern Recognition, Noise Filtering, and Context Integration, explaining these variations. Our analysis identifies five error patterns: predicate misinterpretation, structural mapping errors, control flow misinterpretation, arithmetic transformation errors, and constant propagation errors, revealing fundamental limitations in LLM code this http URL establish a three-tier resistance model: bogus control flow (low resistance), control flow flattening (moderate resistance), and instruction substitution/combined techniques (high resistance). Universal failure against combined techniques demonstrates that sophisticated obfuscation remains effective against advanced LLMs. Our findings suggest a human-AI collaboration paradigm where LLMs reduce expertise barriers for certain reverse engineering tasks while requiring human guidance for complex deobfuscation. This work provides a foundation for evaluating emerging capabilities and developing resistant obfuscation techniques.x deobfuscation. This work provides a foundation for evaluating emerging capabilities and developing resistant obfuscation techniques.', 'abstract_zh': '大型语言模型在二进制分析中的作用尚未探索：商业大型语言模型在汇编代码去混淆中的综合评估', 'title_zh': '解构混淆：评估大型语言模型反混淆编码能力的四维框架'}
{'arxiv_id': 'arXiv:2505.19851', 'title': 'Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages', 'authors': 'Gulfarogh Azam, Mohd Sadique, Saif Ali, Mohammad Nadeem, Erik Cambria, Shahab Saquib Sohail, Mohammad Sultan Alam', 'link': 'https://arxiv.org/abs/2505.19851', 'abstract': 'Transliteration, the process of mapping text from one script to another, plays a crucial role in multilingual natural language processing, especially within linguistically diverse contexts such as India. Despite significant advancements through specialized models like IndicXlit, recent developments in large language models suggest a potential for general-purpose models to excel at this task without explicit task-specific training. The current work systematically evaluates the performance of prominent LLMs, including GPT-4o, GPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a state-of-the-art transliteration model, across ten major Indian languages. Experiments utilized standard benchmarks, including Dakshina and Aksharantar datasets, with performance assessed via Top-1 Accuracy and Character Error Rate. Our findings reveal that while GPT family models generally outperform other LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o improves performance on specific languages notably. An extensive error analysis and robustness testing under noisy conditions further elucidate strengths of LLMs compared to specialized models, highlighting the efficacy of foundational models for a wide spectrum of specialized applications with minimal overhead.', 'abstract_zh': '文本转写：多语言自然语言处理中的关键过程，特别是在如印度等语言多样化的背景下扮演着重要角色。尽管通过专门的模型IndicXlit取得了显著进展，但最近大规模语言模型的发展表明，通用模型在无需明确的任务特定训练的情况下也有可能在这一任务中表现出色。当前的研究系统评估了包括GPT-4o、GPT-4.5、GPT-4.1、Gemma-3-27B-it和Mistral-Large在内的主要语言模型在十种主要印度语言中的性能，与最先进的转写模型IndicXlit进行对比。实验使用了标准基准数据集，如Dakshina和Aksharantar，性能通过Top-1准确率和字符错误率进行评估。我们的研究发现，GPT家族模型通常在大多数实例中优于其他语言模型和IndicXlit。此外，对GPT-4o进行微调在某些语言上的性能明显提高。广泛的错误分析和在噪声条件下的鲁棒性测试进一步阐明了语言模型相对于专门模型的优势，强调了基础模型在广泛的专业应用中的有效性和高效性。', 'title_zh': '超越专业化：跨考查询代理模型在印度语言 transliteration 任务上的基准测试'}
{'arxiv_id': 'arXiv:2505.19838', 'title': 'FoodTaxo: Generating Food Taxonomies with Large Language Models', 'authors': 'Pascal Wullschleger, Majid Zarharan, Donnacha Daly, Marc Pouly, Jennifer Foster', 'link': 'https://arxiv.org/abs/2505.19838', 'abstract': 'We investigate the utility of Large Language Models for automated taxonomy generation and completion specifically applied to taxonomies from the food technology industry. We explore the extent to which taxonomies can be completed from a seed taxonomy or generated without a seed from a set of known concepts, in an iterative fashion using recent prompting techniques. Experiments on five taxonomies using an open-source LLM (Llama-3), while promising, point to the difficulty of correctly placing inner nodes.', 'abstract_zh': '我们探讨了大型语言模型在食品科技行业分类学生成与完成中的应用及其有效性，特别是在种子分类学或未知概念集中使用最近的提示技术迭代完成或生成分类学方面的程度。使用开源LLM（Llama-3）对五个分类学进行的实验尽管前景广阔，但仍表明正确放置内部节点的难度。', 'title_zh': 'FoodTaxo：使用大型语言模型生成食品分类学'}
{'arxiv_id': 'arXiv:2505.19819', 'title': 'FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets', 'authors': 'Dannong Wang, Jaisal Patel, Daochen Zha, Steve Y. Yang, Xiao-Yang Liu', 'link': 'https://arxiv.org/abs/2505.19819', 'abstract': 'Low-rank adaptation (LoRA) methods show great potential for scaling pre-trained general-purpose Large Language Models (LLMs) to hundreds or thousands of use scenarios. However, their efficacy in high-stakes domains like finance is rarely explored, e.g., passing CFA exams and analyzing SEC filings. In this paper, we present the open-source FinLoRA project that benchmarks LoRA methods on both general and highly professional financial tasks. First, we curated 19 datasets covering diverse financial applications; in particular, we created four novel XBRL analysis datasets based on 150 SEC filings. Second, we evaluated five LoRA methods and five base LLMs. Finally, we provide extensive experimental results in terms of accuracy, F1, and BERTScore and report computational cost in terms of time and GPU memory during fine-tuning and inference stages. We find that LoRA methods achieved substantial performance gains of 36\\% on average over base models. Our FinLoRA project provides an affordable and scalable approach to democratize financial intelligence to the general public. Datasets, LoRA adapters, code, and documentation are available at this https URL', 'abstract_zh': '低秩适应（LoRA）方法在将预训练的通用大型语言模型（LLMs）扩展到数百或数千种应用场景方面显示出巨大潜力。然而，它们在金融等高风险领域中的效果鲜有探索，例如通过CFA考试和分析SEC文件。本文介绍了开源FinLoRA项目，该项目在通用和高度专业化的金融任务上对LoRA方法进行了基准测试。首先，我们收集了19个涵盖多种金融应用的数据集；特别是，我们基于150份SEC文件创建了四个新的XBRL分析数据集。其次，我们评估了五种LoRA方法和五种基础LLM。最后，我们从准确率、F1和BERTScore方面提供了详尽的实验结果，并报告了微调和推理阶段的计算成本。我们发现，LoRA方法在平均性能上比基础模型提高了36%。我们的FinLoRA项目提供了一种经济实惠且可扩展的方法，以使金融智能普及化。数据集、LoRA适配器、代码和文档可在以下网址获取。', 'title_zh': 'FinLoRA: LoRA 方法在金融数据集上 fine-tuning LLMs 的基准测试'}
{'arxiv_id': 'arXiv:2505.19815', 'title': 'Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective', 'authors': 'Junnan Liu, Hongwei Liu, Linchen Xiao, Shudong Liu, Taolin Zhang, Zihan Ma, Songyang Zhang, Kai Chen', 'link': 'https://arxiv.org/abs/2505.19815', 'abstract': "We propose a novel framework for comprehending the reasoning capabilities of large language models (LLMs) through the perspective of meta-learning. By conceptualizing reasoning trajectories as pseudo-gradient descent updates to the LLM's parameters, we identify parallels between LLM reasoning and various meta-learning paradigms. We formalize the training process for reasoning tasks as a meta-learning setup, with each question treated as an individual task, and reasoning trajectories serving as the inner loop optimization for adapting model parameters. Once trained on a diverse set of questions, the LLM develops fundamental reasoning capabilities that can generalize to previously unseen questions. Extensive empirical evaluations substantiate the strong connection between LLM reasoning and meta-learning, exploring several issues of significant interest from a meta-learning standpoint. Our work not only enhances the understanding of LLM reasoning but also provides practical insights for improving these models through established meta-learning techniques.", 'abstract_zh': '我们提出了一种通过元学习视角理解大规模语言模型推理能力的新框架。通过将推理轨迹构思为伪梯度下降更新到语言模型参数，我们找到了语言模型推理与多种元学习 paradigm 之间的相似性。我们将推理任务的训练过程形式化为一个元学习设置，每个问题被视为单独的任务，推理轨迹作为内环优化，用于适应模型参数。经过对一系列不同问题的训练后，语言模型发展出能够泛化到以前未见过的问题的基本推理能力。广泛的实证评估证实了语言模型推理与元学习之间的密切联系，并探讨了元学习视角下的若干重要问题。我们的工作不仅增强了对语言模型推理的理解，还提供了通过现有元学习技术改进这些模型的实际见解。', 'title_zh': '基于轨迹辅助的大模型推理解析：一种优化视角'}
{'arxiv_id': 'arXiv:2505.19776', 'title': 'Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification', 'authors': 'Akram Elbouanani, Evan Dufraisse, Adrian Popescu', 'link': 'https://arxiv.org/abs/2505.19776', 'abstract': 'Political biases encoded by LLMs might have detrimental effects on downstream applications. Existing bias analysis methods rely on small-size intermediate tasks (questionnaire answering or political content generation) and rely on the LLMs themselves for analysis, thus propagating bias. We propose a new approach leveraging the observation that LLM sentiment predictions vary with the target entity in the same sentence. We define an entropy-based inconsistency metric to encode this prediction variability. We insert 1319 demographically and politically diverse politician names in 450 political sentences and predict target-oriented sentiment using seven models in six widely spoken languages. We observe inconsistencies in all tested combinations and aggregate them in a statistically robust analysis at different granularity levels. We observe positive and negative bias toward left and far-right politicians and positive correlations between politicians with similar alignment. Bias intensity is higher for Western languages than for others. Larger models exhibit stronger and more consistent biases and reduce discrepancies between similar languages. We partially mitigate LLM unreliability in target-oriented sentiment classification (TSC) by replacing politician names with fictional but plausible counterparts.', 'abstract_zh': 'LLMs中的政治偏见编码可能对下游应用产生负面影响。现有偏见分析方法依赖于小型中间任务（问卷回答或政治内容生成）并通过LLM本身进行分析，从而传播偏见。我们提出了一种新方法，利用LLM在同一句子中对目标实体的情感预测变化这一观察。我们定义了一个基于熵的不一致性度量来编码这种预测变化。我们在450个政治句子中插入了1319个在政治和人口统计学层面都多元化的政治家姓名，并使用六种广泛使用的语言中的七个模型进行目标导向的情感预测。我们在所有测试组合中观察到不一致性，并在不同粒度水平上进行稳健的统计聚合分析。我们观察到对左翼和极右翼政治家的正向和负向偏见，并发现相似政见的政治家之间存在正相关关系。西方语言中的偏见强度高于其他语言。更大规模的模型显示出更强的且更一致的偏见，并减少了相似语言之间的分歧。通过用虚构但可信的对应名称替换政治家姓名，我们在目标导向的情感分类（TSC）中部分缓解了LLM的不可靠性。', 'title_zh': '基于目标导向情感分类分析LLMs中的政治偏见'}
{'arxiv_id': 'arXiv:2505.19764', 'title': 'Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding', 'authors': 'Patara Trirat, Wonyong Jeong, Sung Ju Hwang', 'link': 'https://arxiv.org/abs/2505.19764', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but optimizing LLM-based agentic systems remains challenging due to the vast search space of agent configurations, prompting strategies, and communication patterns. Existing approaches often rely on heuristic-based tuning or exhaustive evaluation, which can be computationally expensive and suboptimal. This paper proposes Agentic Predictor, a lightweight predictor for efficient agentic workflow evaluation. Agentic Predictor is equipped with a multi-view workflow encoding technique that leverages multi-view representation learning of agentic systems by incorporating code architecture, textual prompts, and interaction graph features. To achieve high predictive accuracy while significantly reducing the number of required workflow evaluations for training a predictor, Agentic Predictor employs cross-domain unsupervised pretraining. By learning to approximate task success rates, Agentic Predictor enables fast and accurate selection of optimal agentic workflow configurations for a given task, significantly reducing the need for expensive trial-and-error evaluations. Experiments on a carefully curated benchmark spanning three domains show that our predictor outperforms state-of-the-art methods in both predictive accuracy and workflow utility, highlighting the potential of performance predictors in streamlining the design of LLM-based agentic workflows.', 'abstract_zh': '大型语言模型（LLMs）在多样化的任务中展现了卓越的能力，但由于代理系统配置、提示策略和通信模式的巨大搜索空间，优化基于LLM的代理系统仍然具有挑战性。现有方法通常依赖启发式调整或穷尽评估，这可能导致计算成本高昂且效果欠佳。本文提出了一种轻量级的代理预测器（Agentic Predictor），用于高效代理工作流评估。代理预测器配备了一种多视角工作流编码技术，该技术通过结合代码架构、文本提示和交互图特征，利用多视图表示学习代理系统。为了在显著减少用于训练预测器的工作流评估次数的同时保持高预测准确性，代理预测器采用了跨域无监督预训练。通过学习近似任务成功率，代理预测器能够快速准确地选择给定任务的最佳代理工作流配置，大幅减少昂贵的试错评估需求。在跨越三个领域精心策划的基准测试上进行的实验表明，我们的预测器在预测准确性和工作流实用性方面均优于现有最先进的方法，突显了性能预测器在简化基于LLM的代理工作流设计方面的潜力。', 'title_zh': '代理预测者：通过多视图编码实现代理工作流的性能预测'}
{'arxiv_id': 'arXiv:2505.19754', 'title': 'NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering', 'authors': 'Ruisheng Cao, Hanchong Zhang, Tiancheng Huang, Zhangyi Kang, Yuxin Zhang, Liangtai Sun, Hanqi Li, Yuxun Miao, Shuai Fan, Lu Chen, Kai Yu', 'link': 'https://arxiv.org/abs/2505.19754', 'abstract': 'The increasing number of academic papers poses significant challenges for researchers to efficiently acquire key details. While retrieval augmented generation (RAG) shows great promise in large language model (LLM) based automated question answering, previous works often isolate neural and symbolic retrieval despite their complementary strengths. Moreover, conventional single-view chunking neglects the rich structure and layout of PDFs, e.g., sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural symbolic retrieval framework which combines both paradigms in an interactive process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG organizes semi-structured PDF content into both the relational database and vectorstore, enabling LLM agents to iteratively gather context until sufficient to generate answers. Experiments on three full PDF-based QA datasets, including a self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the vector-based RAG and various structured baselines, highlighting its capacity to unify both retrieval schemes and utilize multiple views. Code and data are publicly available at this https URL.', 'abstract_zh': '不断增加的学术论文数量为研究人员高效获取关键细节带来了重大挑战。虽然基于大规模语言模型（LLM）的检索增强生成（RAG）在自动问答方面显示出巨大潜力，但以往的研究往往将神经检索和符号检索隔离，尽管两者具有互补的优势。此外，传统的单视图切块忽略了PDF的丰富结构和布局，例如段落和表格。在本工作中，我们提出了NeuSym-RAG，这是一种结合了这两种范式的混合神经符号检索框架，并在交互过程中将两者结合起来。通过利用多视图切块和基于模式的解析，NeuSym-RAG 将半结构化的PDF内容组织到关系数据库和向量存储中，使LLM代理能够迭代地收集上下文，直到生成答案所需的信息充分。在三个基于全文PDF的问答数据集中进行了实验，包括一个自我注释的数据集AIRQA-REAL，结果显示NeuSym-RAG在对比基于向量的RAG和各种结构化基线时表现出稳定的优势，突显了其统一检索方案并利用多个视图的能力。代码和数据可在以下网址公开获取。', 'title_zh': 'NeuSym-RAG：基于多视图结构化的混合神经符号检索方法及其在PDF问答中的应用'}
{'arxiv_id': 'arXiv:2505.19722', 'title': "Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking", 'authors': 'Yihao Ai, Zhiyuan Ning, Weiwei Dai, Pengfei Wang, Yi Du, Wenjuan Cui, Kunpeng Liu, Yuanchun Zhou', 'link': 'https://arxiv.org/abs/2505.19722', 'abstract': "Biomedical entity linking aims to map nonstandard entities to standard entities in a knowledge base. Traditional supervised methods perform well but require extensive annotated data to transfer, limiting their usage in low-resource scenarios. Large language models (LLMs), especially closed-source LLMs, can address these but risk stability issues and high economic costs: using these models is restricted by commercial companies and brings significant economic costs when dealing with large amounts of data. To address this, we propose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs for re-ranking candidates retrieved by a retriever fine-tuned with a small amount of data. By prompting a closed-source LLM to generate training data from unannotated data and fine-tuning an open-source LLM for re-ranking, we effectively distill the knowledge to the open-source LLM that can be deployed locally, thus avoiding the stability issues and the problem of high economic costs. We evaluate RPDR on two datasets, including one real-world dataset and one publicly available dataset involving two languages: Chinese and English. RPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier dataset and the Ask A Patient dataset when the amount of training data is not enough. The results demonstrate the superiority and generalizability of the proposed framework.", 'abstract_zh': '生物医学实体链接旨在将非标准实体映射到知识库中的标准实体。传统的监督方法表现良好，但需要大量标注数据以进行转移，这限制了它们在低资源场景中的应用。大型语言模型（LLMs），尤其是闭源LLMs，可以解决这些问题，但也存在稳定性和高经济成本的风险：使用这些模型受到商业公司的限制，并且在处理大量数据时会产生显著的经济成本。为此，我们提出了一种名为“RPDR”的框架，该框架结合了闭源LLMs和开源LLMs，用于对检索器微调后检索出的候选项进行再排序。通过促使闭源LLMs从未标注数据生成训练数据，并对开源LLMs进行再排序微调，我们有效地将知识转移到可以本地部署的开源LLMs中，从而避免了稳定性和高经济成本的问题。我们在两个数据集上评估了RPDR，包括一个真实世界数据集和一个包含中英文的公共数据集。当训练数据不足时，RPDR在Aier数据集和Ask A Patient数据集上分别实现了0.019和0.036的Acc@1改进。实验结果表明了提出框架的优越性和通用性。', 'title_zh': '从闭源大型语言模型中蒸馏知识以实现本地稳定和经济的生物医学实体链接'}
{'arxiv_id': 'arXiv:2505.19715', 'title': 'Graceful Forgetting in Generative Language Models', 'authors': 'Chunyang Jiang, Chi-min Chan, Yiyang Cai, Yulong Liu, Wei Xue, Yike Guo', 'link': 'https://arxiv.org/abs/2505.19715', 'abstract': 'Recently, the pretrain-finetune paradigm has become a cornerstone in various deep learning areas. While in general the pre-trained model would promote both effectiveness and efficiency of downstream tasks fine-tuning, studies have shown that not all knowledge acquired during pre-training is beneficial. Some of the knowledge may actually bring detrimental effects to the fine-tuning tasks, which is also known as negative transfer. To address this problem, graceful forgetting has emerged as a promising approach. The core principle of graceful forgetting is to enhance the learning plasticity of the target task by selectively discarding irrelevant knowledge. However, this approach remains underexplored in the context of generative language models, and it is often challenging to migrate existing forgetting algorithms to these models due to architecture incompatibility. To bridge this gap, in this paper we propose a novel framework, Learning With Forgetting (LWF), to achieve graceful forgetting in generative language models. With Fisher Information Matrix weighting the intended parameter updates, LWF computes forgetting confidence to evaluate self-generated knowledge regarding the forgetting task, and consequently, knowledge with high confidence is periodically unlearned during fine-tuning. Our experiments demonstrate that, although thoroughly uncovering the mechanisms of knowledge interaction remains challenging in pre-trained language models, applying graceful forgetting can contribute to enhanced fine-tuning performance.', 'abstract_zh': '学习与遗忘：生成语言模型中的优雅遗忘', 'title_zh': '生成语言模型中的优雅遗忘'}
{'arxiv_id': 'arXiv:2505.19714', 'title': 'MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning', 'authors': 'Zhaopeng Feng, Yupu Liang, Shaosheng Cao, Jiayuan Su, Jiahan Ren, Zhe Xu, Yao Hu, Wenxuan Huang, Jian Wu, Zuozhu Liu', 'link': 'https://arxiv.org/abs/2505.19714', 'abstract': "Text Image Machine Translation (TIMT)-the task of translating textual content embedded in images-is critical for applications in accessibility, cross-lingual information access, and real-world document understanding. However, TIMT remains a complex challenge due to the need for accurate optical character recognition (OCR), robust visual-text reasoning, and high-quality translation, often requiring cascading multi-stage pipelines. Recent advances in large-scale Reinforcement Learning (RL) have improved reasoning in Large Language Models (LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is still underexplored. To bridge this gap, we introduce MT$^{3}$, the first framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts a multi-task optimization paradigm targeting three key sub-skills: text recognition, context-aware reasoning, and translation. It is trained using a novel multi-mixed reward mechanism that adapts rule-based RL strategies to TIMT's intricacies, offering fine-grained, non-binary feedback across tasks. Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural and real-world social media contexts, we introduced XHSPost, the first social media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on the latest in-domain MIT-10M benchmark, outperforming strong baselines such as Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics. Additionally, the model shows strong generalization to out-of-distribution language pairs and datasets. In-depth analyses reveal how multi-task synergy, reinforcement learning initialization, curriculum design, and reward formulation contribute to advancing MLLM-driven TIMT.", 'abstract_zh': 'Text Image Machine Translation (TIMT)基于图像嵌入文本内容的机器翻译对于无障碍应用、跨语言信息访问和现实世界文档理解至关重要。然而，TIMT 由于需要准确的光学字符识别 (OCR)、稳健的视觉-文本推理和高质量的翻译，仍然是一项复杂的挑战，通常需要多阶段管道。最近大规模强化学习 (RL) 的进展改善了大型语言模型 (LLMs) 和多模态 LLMs (MLLMs) 的推理能力，但将其应用于端到端的 TIMT 仍然未被充分探索。为了弥合这一差距，我们提出了 MT$^{3}$，这是首个将多任务 RL 应用于 MLLMs 以实现端到端 TIMT 的框架。MT$^{3}$ 采用多任务优化范式，针对三个关键子技能进行优化：文本识别、上下文感知推理和翻译。它使用一种新颖的多混合奖励机制进行训练，这种机制将基于规则的 RL 策略适应 TIMT 的复杂性，提供细粒度、非二元化的跨任务反馈。此外，为了在真实的跨文化和实际社交媒体背景下评估 TIMT，我们引入了 XHSPost，这是首个社交媒体 TIMT 的基准数据集。我们的 MT$^{3}$-7B-Zero 在最新的领域内 MIT-10M 基准测试中取得了最新成果，多种度量标准上显著优于 Qwen2.5-VL-72B 和 InternVL2.5-78B 等强基线。此外，该模型展现了对未见过的语言对和数据集的强大泛化能力。深入分析揭示了多任务协同、强化学习初始化、课程设计和奖励机制对推动 MLLM 驱动的 TIMT 的贡献。', 'title_zh': 'MT$^{3}$: 通过多任务强化学习扩展基于MLLM的图文机器翻译'}
{'arxiv_id': 'arXiv:2505.19706', 'title': 'Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision', 'authors': 'Tej Deep Pala, Panshul Sharma, Amir Zadeh, Chuan Li, Soujanya Poria', 'link': 'https://arxiv.org/abs/2505.19706', 'abstract': 'Large Language Models (LLMs) are prone to hallucination, especially during multi-hop and reasoning-intensive tasks such as mathematical problem solving. While Outcome Reward Models verify only final answers, Process Reward Models (PRMs) score each intermediate step to steer generation toward coherent solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware discriminative PRM that first classifies math and consistency errors at each step, then combines these fine-grained signals to estimate step correctness. To train PathFinder-PRM, we construct a 400K-sample dataset by enriching the human-annotated PRM800K corpus and RLHFlow Mistral traces with three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while using 3 times less data. When applied to reward guided greedy search, our model yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results demonstrate that decoupled error detection and reward estimation not only boost fine-grained error detection but also substantially improve end-to-end, reward-guided mathematical reasoning with greater data efficiency.', 'abstract_zh': '大型语言模型（LLMs）在解决数学问题等多步和推理密集型任务时易产生幻觉。虽然结果奖励模型仅验证最终答案，过程奖励模型（PRM）则对每一步进行评分以引导生成连贯的解决方案。我们引入了PathFinder-PRM，这是一种新颖的分层、错误感知的区分性PRM，首先在每一步上分类数学和一致性错误，然后结合这些细粒度信号来估计步骤的正确性。为训练PathFinder-PRM，我们通过丰富PRM800K人工标注语料和RLHFlow Mistral跟踪数据集，构建了一个包含40万个样本的数据集，带有三维步级标签。在PRMBench上，PathFinder-PRM达到了新的最佳PRMScore 67.7，数据量减少三分之二仍超越了此前的最佳成绩（65.5）。当应用于奖励引导贪婪搜索时，我们的模型在prm@8上取得了48.3的成绩，比最强基线高出1.5分。这些结果证明了分离开错误检测和奖励估计不仅提升了细粒度错误检测的能力，还大幅提升了端到端、奖励引导的数学推理能力，同时具有更高的数据效率。', 'title_zh': '错误归因以获取更智能的奖励：基于错误感知层次监督的过程奖励模型改进'}
{'arxiv_id': 'arXiv:2505.19700', 'title': 'Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models', 'authors': 'Yi Liu, Dianqing Liu, Mingye Zhu, Junbo Guo, Yongdong Zhang, Zhendong Mao', 'link': 'https://arxiv.org/abs/2505.19700', 'abstract': 'The widespread adoption of large language models (LLMs) across industries has increased the demand for high-quality and customizable outputs. However, traditional alignment methods often require retraining large pretrained models, making it difficult to quickly adapt and optimize LLMs for diverse applications. To address this limitation, we propose a novel \\textit{Residual Alignment Model} (\\textit{RAM}) that formalizes the alignment process as a type of importance sampling. In this framework, the unaligned upstream model serves as the proposal distribution, while the alignment process is framed as secondary sampling based on an autoregressive alignment module that acts as an estimator of the importance weights. This design enables a natural detachment of the alignment module from the target aligned model, improving flexibility and scalability. Based on this model, we derive an efficient sequence-level training strategy for the alignment module, which operates independently of the proposal module. Additionally, we develop a resampling algorithm with iterative token-level decoding to address the common first-token latency issue in comparable methods. Experimental evaluations on two leading open-source LLMs across diverse tasks, including instruction following, domain adaptation, and preference optimization, demonstrate that our approach consistently outperforms baseline models.', 'abstract_zh': '广泛采用的大语言模型（LLMs）在各行业中的应用增加了高质量和可定制输出的需求。然而，传统的对齐方法 Often 需要重新训练大规模预训练模型，使得快速适应和优化LLMs以满足多元应用的需求变得困难。为解决这一限制，我们提出了一种新颖的“残差对齐模型”（RAM），将其对齐过程形式化为重要性采样的类型。在此框架中，未对齐的上游模型作为提议分布，而对齐过程被重新构架为基于自回归对齐模块的二次采样，该模块作为重要性权重的估计器。该设计使对齐模块与目标对齐模型实现了自然分离，提高了灵活性和可扩展性。基于此模型，我们为对齐模块推导出一种高效的序列级训练策略，该策略与提议模块独立运行。此外，我们开发了一种迭代令牌级解码的重采样算法，以解决同类方法中常见的首令牌延迟问题。针对两个领先开源LLM在包括指令跟随、领域适应和偏好优化等多样任务上的实验评估表明，我们的方法在所有任务上均优于基线模型。', 'title_zh': '利用重要性抽样分离对齐模块与大型语言模型'}
{'arxiv_id': 'arXiv:2505.19675', 'title': 'Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement', 'authors': 'Liqin Ye, Agam Shah, Chao Zhang, Sudheer Chava', 'link': 'https://arxiv.org/abs/2505.19675', 'abstract': "The traditional process of creating labeled datasets is labor-intensive and expensive. Recent breakthroughs in open-source large language models (LLMs) have opened up a new avenue in generating labeled datasets automatically for various natural language processing (NLP) tasks, providing an alternative to such an expensive annotation process. However, the reliability of such auto-generated labels remains a significant concern due to inherent inaccuracies. When learning from noisy labels, the model's generalization is likely to be harmed as it is prone to overfit to those label noises. While previous studies in learning from noisy labels mainly focus on synthetic noise and real-world noise, LLM-generated label noise receives less attention. In this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to calibrate the classifier's prediction, thus enhancing its robustness towards LLM-generated noisy labels. SiDyP retrieves potential true label candidates by neighborhood label distribution in text embedding space and iteratively refines noisy candidates using a simplex diffusion model. Our framework can increase the performance of the BERT classifier fine-tuned on both zero-shot and few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30% respectively. We demonstrate the effectiveness of SiDyP by conducting extensive benchmarking for different LLMs over a variety of NLP tasks. Our code is available on Github.", 'abstract_zh': '传统的标注数据集创建过程耗时且昂贵。开源大型语言模型（LLMs）的最新突破为自动生成适用于各种自然语言处理（NLP）任务的标注数据集开辟了新途径，提供了比昂贵标注过程的替代方案。然而，自动生成的标签可靠性仍是一个重大问题，因为存在固有的不准确性。在嘈杂标签上学习时，模型的泛化能力可能受损，因为它容易过度拟合那些标签噪声。虽然先前的嘈杂标签学习研究主要集中在合成噪声和真实世界噪声上，但LLM生成的标签噪声却较少受到关注。在本文中，我们提出SiDyP：简单标签扩散与动态先验方法，以校准分类器的预测，从而增强其对LLM生成的嘈杂标签的鲁棒性。SiDyP 通过文本嵌入空间中的邻域标签分布检索潜在的真实标签候选，并使用简单扩散模型迭代细化嘈杂候选。我们的框架可以分别在零样本和少量样本LLM生成的嘈杂标签数据集上微调的BERT分类器性能提高7.21%和7.30%。通过在多种NLP任务上对不同LLM进行广泛的基准测试，我们展示了SiDyP的有效性。我们的代码可在Github上获取。', 'title_zh': '通过迭代 refinement 校准预训练语言分类器在大语言模型生成的噪声标签上的表现'}
{'arxiv_id': 'arXiv:2505.19667', 'title': 'LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation', 'authors': 'Weikang Yuan, Kaisong Song, Zhuoren Jiang, Junjie Cao, Yujie Zhang, Jun Lin, Kun Kuang, Ji Zhang, Xiaozhong Liu', 'link': 'https://arxiv.org/abs/2505.19667', 'abstract': "Legal consultation is essential for safeguarding individual rights and ensuring access to justice, yet remains costly and inaccessible to many individuals due to the shortage of professionals. While recent advances in Large Language Models (LLMs) offer a promising path toward scalable, low-cost legal assistance, current systems fall short in handling the interactive and knowledge-intensive nature of real-world consultations. To address these challenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset comprising 3,696 legal consultation dialogues with 110,008 dialogue turns, designed to evaluate and improve LLMs' legal consultation capability. With LeCoDe, we innovatively collect live-streamed consultations from short-video platforms, providing authentic multi-turn legal consultation dialogues. The rigorous annotation by legal experts further enhances the dataset with professional insights and expertise. Furthermore, we propose a comprehensive evaluation framework that assesses LLMs' consultation capabilities in terms of (1) clarification capability and (2) professional advice quality. This unified framework incorporates 12 metrics across two dimensions. Through extensive experiments on various general and domain-specific LLMs, our results reveal significant challenges in this task, with even state-of-the-art models like GPT-4 achieving only 39.8% recall for clarification and 59% overall score for advice quality, highlighting the complexity of professional consultation scenarios. Based on these findings, we further explore several strategies to enhance LLMs' legal consultation abilities. Our benchmark contributes to advancing research in legal domain dialogue systems, particularly in simulating more real-world user-expert interactions.", 'abstract_zh': '法治咨询对于保障个人权利和确保司法公正至关重要，但由于专业人员的短缺，这一服务对许多个体来说既昂贵又难以获得。虽然近期大型语言模型（LLMs）的发展为面向大众、低成本的法律援助提供了希望，但现有系统在处理实际咨询中的互动性和知识密集型特性方面仍存在不足。为应对这些挑战，我们引入了LeCoDe，一个包含3,696个法律咨询对话和110,008个对话轮次的现实世界多轮次基准数据集，旨在评估和提升大型语言模型的法律咨询能力。通过LeCoDe，我们创新性地从短视频平台收集了实时法律咨询服务，提供了真实的多轮法律咨询对话。法务专家的严格注解进一步提升了数据集的专业见解和专业知识。此外，我们提出了一套全面的评估框架，从澄清能力和专业建议质量两个维度评估大型语言模型的咨询能力。该统一框架涵盖了12个指标。通过在各种通用和特定领域的大型语言模型上的广泛实验，我们的结果显示了这一任务的巨大挑战，即使是像GPT-4这样最先进的模型也只能实现39.8%的澄清召回率和59%的整体建议质量评分，突显了专业咨询场景的复杂性。基于这些发现，我们进一步探索了增强大型语言模型法律咨询服务能力的策略。我们的基准有助于推进法律领域对话系统的研究，特别是模拟更多真实的用户-专家互动。', 'title_zh': 'LeCoDe：用于互动法律咨询对话评估的标准数据集'}
{'arxiv_id': 'arXiv:2505.19660', 'title': 'GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models', 'authors': 'Tingjia Shen, Hao Wang, Chuan Qin, Ruijun Sun, Yang Song, Defu Lian, Hengshu Zhu, Enhong Chen', 'link': 'https://arxiv.org/abs/2505.19660', 'abstract': "Open-domain question answering (OpenQA) represents a cornerstone in natural language processing (NLP), primarily focused on extracting answers from unstructured textual data. With the rapid advancements in Large Language Models (LLMs), LLM-based OpenQA methods have reaped the benefits of emergent understanding and answering capabilities enabled by massive parameters compared to traditional methods. However, most of these methods encounter two critical challenges: how to integrate knowledge into LLMs effectively and how to adaptively generate results with specific answer formats for various task situations. To address these challenges, we propose a novel framework named GenKI, which aims to improve the OpenQA performance by exploring Knowledge Integration and controllable Generation on LLMs simultaneously. Specifically, we first train a dense passage retrieval model to retrieve associated knowledge from a given knowledge base. Subsequently, we introduce a novel knowledge integration model that incorporates the retrieval knowledge into instructions during fine-tuning to intensify the model. Furthermore, to enable controllable generation in LLMs, we leverage a certain fine-tuned LLM and an ensemble based on text consistency incorporating all coherence, fluency, and answer format assurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO, and CMRC2018 datasets, featuring diverse answer formats, have demonstrated the effectiveness of GenKI with comparison of state-of-the-art baselines. Moreover, ablation studies have disclosed a linear relationship between the frequency of retrieved knowledge and the model's ability to recall knowledge accurately against the ground truth. Our code of GenKI is available at this https URL", 'abstract_zh': '开放域问答（OpenQA）在自然语言处理（NLP）中占有重要地位，主要关注从非结构化文本数据中提取答案。随着大型语言模型（LLMs）的快速发展，基于LLM的OpenQA方法受益于大规模参数带来的新兴理解和回答能力，相较于传统方法。然而，这些方法大多面临着两个关键挑战：如何有效整合知识到LLM中，以及如何在各种任务情境下自适应地生成具有特定答案格式的结果。为应对这些挑战，我们提出了一种名为GenKI的新型框架，旨在通过同时探索LLM上的知识整合和可控生成来提高OpenQA性能。具体而言，我们首先训练一个密集段落检索模型，从给定的知识库中检索相关知识。随后，我们引入了一个新颖的知识整合模型，在微调过程中将检索知识集成到指令中，以增强模型。此外，为使LLM实现可控生成，我们利用了一种特定微调的LLM，并结合了流畅性、连贯性和答案格式保证的集成方法。最后，在涵盖多种答案格式的TriviaQA、MSMARCO和CMRC2018数据集上的广泛实验表明，GenKI的有效性优于最新的基线方法。此外，消融研究揭示了检索知识频率与模型准确回忆知识能力之间呈线性关系。GenKI的代码可在以下链接获取。', 'title_zh': 'GenKI: 在大型语言模型中通过知识整合与可控生成提升开放域问答能力'}
{'arxiv_id': 'arXiv:2505.19658', 'title': 'Large Language Models in Code Co-generation for Safe Autonomous Vehicles', 'authors': 'Ali Nouri, Beatriz Cabrero-Daniel, Zhennan Fei, Krishna Ronanki, Håkan Sivencrona, Christian Berger', 'link': 'https://arxiv.org/abs/2505.19658', 'abstract': "Software engineers in various industrial domains are already using Large Language Models (LLMs) to accelerate the process of implementing parts of software systems. When considering its potential use for ADAS or AD systems in the automotive context, there is a need to systematically assess this new setup: LLMs entail a well-documented set of risks for safety-related systems' development due to their stochastic nature. To reduce the effort for code reviewers to evaluate LLM-generated code, we propose an evaluation pipeline to conduct sanity-checks on the generated code. We compare the performance of six state-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders, Mistral, and GPT-4) on four safety-related programming tasks. Additionally, we qualitatively analyse the most frequent faults generated by these LLMs, creating a failure-mode catalogue to support human reviewers. Finally, the limitations and capabilities of LLMs in code generation, and the use of the proposed pipeline in the existing process, are discussed.", 'abstract_zh': '在各种工业领域中，软件工程师已经开始使用大型语言模型（LLMs）来加速软件系统部分实现过程。在汽车情境中考虑其用于高级驾驶辅助系统（ADAS）或自动驾驶系统（AD）的潜在用途时，有必要系统性地评估这一新架构：由于其随机性，LLMs 对安全相关系统开发存在已文档化的风险。为了减轻代码审查人员评估LLM生成代码的工作量，我们提出了一种评估流水线来进行生成代码的常规检查。我们将六种最先进的LLM（CodeLlama、CodeGemma、DeepSeek-r1、DeepSeek-Coders、Mistral 和 GPT-4）在四种安全相关编程任务上的性能进行比较。此外，我们对这些LLM生成的最常见的错误进行了定性分析，创建了一个故障模式catalogue以支持人工审查员。最后，讨论了LLMs在代码生成中的局限性和能力，以及所提议的流水线在现有流程中的应用。', 'title_zh': '大型语言模型在代码共动生成中的安全自主车辆应用'}
{'arxiv_id': 'arXiv:2505.19645', 'title': "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE", 'authors': 'Zongle Huang, Lei Zhu, Zongyuan Zhan, Ting Hu, Weikai Mao, Xianzhi Yu, Yongpan Liu, Tianyu Zhang', 'link': 'https://arxiv.org/abs/2505.19645', 'abstract': "Large Language Models (LLMs) have achieved remarkable success across many applications, with Mixture of Experts (MoE) models demonstrating great potential. Compared to traditional dense models, MoEs achieve better performance with less computation. Speculative decoding (SD) is a widely used technique to accelerate LLM inference without accuracy loss, but it has been considered efficient only for dense models. In this work, we first demonstrate that, under medium batch sizes, MoE surprisingly benefits more from SD than dense models. Furthermore, as MoE becomes sparser -- the prevailing trend in MoE designs -- the batch size range where SD acceleration is expected to be effective becomes broader. To quantitatively understand tradeoffs involved in SD, we develop a reliable modeling based on theoretical analyses. While current SD research primarily focuses on improving acceptance rates of algorithms, changes in workload and model architecture can still lead to degraded SD acceleration even with high acceptance rates. To address this limitation, we introduce a new metric 'target efficiency' that characterizes these effects, thus helping researchers identify system bottlenecks and understand SD acceleration more comprehensively. For scenarios like private serving, this work unveils a new perspective to speed up MoE inference, where existing solutions struggle. Experiments on different GPUs show up to 2.29x speedup for Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.", 'abstract_zh': '大规模语言模型（LLMs）已在众多应用中取得了显著成功，专家混合模型（MoE）展示了巨大的潜力。与传统的密集模型相比，MoE能够在减少计算量的同时实现更好的性能。推测解码（SD）是一项广泛使用的技术，用于在不损失准确性的前提下加速LLM推理，但长期以来被认为主要适用于密集模型。在本工作中，我们首先证明，在中等批量大小下，MoE意外地比密集模型更受益于SD。此外，随着MoE变得越来越稀疏——这是MoE设计的一个主要趋势——预期SD加速效果的批量大小范围变得更宽广。为了定量理解SD涉及的权衡，我们基于理论分析建立了可靠的模型。尽管当前的SD研究主要集中在提高算法的接受率上，但工作负载和模型架构的变化仍可能导致即使在高接受率下SD加速也会降级。为应对这一局限性，我们引入了一个新的指标“目标效率”来表征这些效果，有助于研究者识别系统瓶颈，并更全面地理解SD加速。对于如私有服务等场景，这项工作揭示了一个新的视角，以加快MoE推理，而现有解决方案在此方面面临挑战。在不同GPU上的实验显示，对于中等批量大小的Qwen2-57B-A14B，在线速比可达2.29倍，并验证了我们的理论预测。', 'title_zh': 'MoESD: 探析 speculative decoding 在加速稀疏 MoE 方面的潜力'}
{'arxiv_id': 'arXiv:2505.19631', 'title': 'Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models', 'authors': 'Zihong Zhang, Liqi He, Zuchao Li, Lefei Zhang, Hai Zhao, Bo Du', 'link': 'https://arxiv.org/abs/2505.19631', 'abstract': 'Word segmentation stands as a cornerstone of Natural Language Processing (NLP). Based on the concept of "comprehend first, segment later", we propose a new framework to explore the limit of unsupervised word segmentation with Large Language Models (LLMs) and evaluate the semantic understanding capabilities of LLMs based on word segmentation. We employ current mainstream LLMs to perform word segmentation across multiple languages to assess LLMs\' "comprehension". Our findings reveal that LLMs are capable of following simple prompts to segment raw text into words. There is a trend suggesting that models with more parameters tend to perform better on multiple languages. Additionally, we introduce a novel unsupervised method, termed LLACA ($\\textbf{L}$arge $\\textbf{L}$anguage Model-Inspired $\\textbf{A}$ho-$\\textbf{C}$orasick $\\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities of Aho-Corasick automata, LLACA innovatively combines these with the deep insights of well-pretrained LLMs. This approach not only enables the construction of a dynamic $n$-gram model that adjusts based on contextual information but also integrates the nuanced understanding of LLMs, offering significant improvements over traditional methods. Our source code is available at this https URL', 'abstract_zh': '基于“先理解后切词”的理念，大规模语言模型在无监督词切分中的极限探究及语义理解能力评估：一种大型语言模型启发的Aho-Corasick自动机方法（LLACA）', 'title_zh': '先分词还是先理解？探索大规模语言模型在无监督词分词中的极限'}
{'arxiv_id': 'arXiv:2505.19623', 'title': 'AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems', 'authors': 'Yu Shang, Peijie Liu, Yuwei Yan, Zijing Wu, Leheng Sheng, Yuanqing Yu, Chumeng Jiang, An Zhang, Fengli Xu, Yu Wang, Min Zhang, Yong Li', 'link': 'https://arxiv.org/abs/2505.19623', 'abstract': "The emergence of agentic recommender systems powered by Large Language Models (LLMs) represents a paradigm shift in personalized recommendations, leveraging LLMs' advanced reasoning and role-playing capabilities to enable autonomous, adaptive decision-making. Unlike traditional recommendation approaches, agentic recommender systems can dynamically gather and interpret user-item interactions from complex environments, generating robust recommendation strategies that generalize across diverse scenarios. However, the field currently lacks standardized evaluation protocols to systematically assess these methods. To address this critical gap, we propose: (1) an interactive textual recommendation simulator incorporating rich user and item metadata and three typical evaluation scenarios (classic, evolving-interest, and cold-start recommendation tasks); (2) a unified modular framework for developing and studying agentic recommender systems; and (3) the first comprehensive benchmark comparing 10 classical and agentic recommendation methods. Our findings demonstrate the superiority of agentic systems and establish actionable design guidelines for their core components. The benchmark environment has been rigorously validated through an open challenge and remains publicly available with a continuously maintained leaderboard~\\footnote[2]{this https URL}, fostering ongoing community engagement and reproducible research. The benchmark is available at: \\hyperlink{this https URL}{this https URL}.", 'abstract_zh': '由大规模语言模型（LLMs）驱动的代理推荐系统的发展代表了个性化推荐的一个范式转变，利用LLMs的高级推理和角色扮演能力，实现自主、适应性的决策。与传统的推荐方法不同，代理推荐系统可以从复杂环境中动态收集和解释用户-项目交互，生成适用于各种场景的 robust 推荐策略。然而，当前该领域缺乏标准化的评估协议，以系统地评估这些方法。为填补这一关键空白，我们提出：(1) 一个包含丰富的用户和项目元数据的交互式文本推荐模拟器，以及三种典型评估场景（经典、兴趣演变和冷启动推荐任务）；(2) 一个统一的模块化框架，用于开发和研究代理推荐系统；以及(3) 第一个全面基准，比较了10种经典和代理推荐方法。我们的研究结果展示了代理系统的优越性，并为其核心组件确立了可操作的设计指南。基准环境通过一个公开挑战进行了严格的验证，并保持公开和持续维护的排名榜（<https://this https URL>），以促进持续的社区参与和可再现研究。基准可从以下网址获得：<https://this https URL>。', 'title_zh': 'AgentRecBench：基于LLM代理的个性化推荐系统 benchmarks'}
{'arxiv_id': 'arXiv:2505.19620', 'title': 'Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs', 'authors': 'Jiawen Chen, Qi Shao, Duxin Chen, Wenwu Yu', 'link': 'https://arxiv.org/abs/2505.19620', 'abstract': 'Spatio-temporal prediction is a pivotal task with broad applications in traffic management, climate monitoring, energy scheduling, etc. However, existing methodologies often struggle to balance model expressiveness and computational efficiency, especially when scaling to large real-world datasets. To tackle these challenges, we propose STH-SepNet (Spatio-Temporal Hypergraph Separation Networks), a novel framework that decouples temporal and spatial modeling to enhance both efficiency and precision. Therein, the temporal dimension is modeled using lightweight large language models, which effectively capture low-rank temporal dynamics. Concurrently, the spatial dimension is addressed through an adaptive hypergraph neural network, which dynamically constructs hyperedges to model intricate, higher-order interactions. A carefully designed gating mechanism is integrated to seamlessly fuse temporal and spatial representations. By leveraging the fundamental principles of low-rank temporal dynamics and spatial interactions, STH-SepNet offers a pragmatic and scalable solution for spatio-temporal prediction in real-world applications. Extensive experiments on large-scale real-world datasets across multiple benchmarks demonstrate the effectiveness of STH-SepNet in boosting predictive performance while maintaining computational efficiency. This work may provide a promising lightweight framework for spatio-temporal prediction, aiming to reduce computational demands and while enhancing predictive performance. Our code is avaliable at this https URL.', 'abstract_zh': '时空预测是交通管理、气候监测、能源调度等领域广泛应用的核心任务。然而，现有方法往往难以在模型表达能力和计算效率之间取得平衡，特别是在处理大规模现实世界数据集时。为应对这些挑战，我们提出了一种新颖的框架STH-SepNet（时空超图分离网络），该框架解耦了时空模型，以提高效率和精度。其中，时间维度通过轻量级大语言模型进行建模，有效捕捉低秩时间动态。同时，空间维度通过自适应超图神经网络进行处理，动态构建超边以模拟复杂的高阶交互。精心设计的门控机制被集成以无缝融合时空表示。通过利用低秩时间动态和空间交互的基本原理，STH-SepNet 提供了一种实用且可扩展的时空预测解决方案，适用于多种现实世界应用。在多个基准上的大规模现实世界数据集上的广泛实验表明，STH-SepNet 在提高预测性能的同时保持了计算效率。本项工作可能提供一种具有前景的轻量级框架，旨在减少计算需求并提升预测性能。我们的代码可在以下链接获取：this https URL。', 'title_zh': '时空预测解耦：当轻量级大型模型遇到自适应超图'}
{'arxiv_id': 'arXiv:2505.19616', 'title': 'Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models', 'authors': 'Rui Cai, Bangzheng Li, Xiaofei Wen, Muhao Chen, Zhe Zhao', 'link': 'https://arxiv.org/abs/2505.19616', 'abstract': "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across tasks, yet they often exhibit difficulty in distinguishing task-relevant from irrelevant signals, particularly in tasks like Visual Question Answering (VQA), which can lead to susceptibility to misleading or spurious inputs. We refer to this broader limitation as the Cross-Modality Competency Problem: the model's inability to fairly evaluate all modalities. This vulnerability becomes more evident in modality-specific tasks such as image classification or pure text question answering, where models are expected to rely solely on one modality. In such tasks, spurious information from irrelevant modalities often leads to significant performance degradation. We refer to this failure as Modality Interference, which serves as a concrete and measurable instance of the cross-modality competency problem. We further design a perturbation-based causal diagnostic experiment to verify and quantify this problem. To mitigate modality interference, we propose a novel framework to fine-tune MLLMs, including perturbation-based data augmentations with both heuristic perturbations and adversarial perturbations via Projected Gradient Descent (PGD), and a consistency regularization strategy applied to model outputs with original and perturbed inputs. Experiments on multiple benchmark datasets (image-heavy, text-heavy, and VQA tasks) and multiple model families with different scales demonstrate significant improvements in robustness and cross-modality competency, indicating our method's effectiveness in boosting unimodal reasoning ability while enhancing performance on multimodal tasks.", 'abstract_zh': '多模态大型语言模型跨模态能力问题：建模模态间干扰的扰动因果诊断与缓解', 'title_zh': '诊断和缓解多模态大型语言模型中的模态干扰'}
{'arxiv_id': 'arXiv:2505.19609', 'title': 'Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling', 'authors': 'Hongtao Xu, Wenting Shen, Yuanxin Wei, Ang Wang, Guo Runfan, Tianxing Wang, Yong Li, Mingzhen Li, Weile Jia', 'link': 'https://arxiv.org/abs/2505.19609', 'abstract': 'Long-context supervised fine-tuning (Long-SFT) plays a vital role in enhancing the performance of large language models (LLMs) on long-context tasks. To smoothly adapt LLMs to long-context scenarios, this process typically entails training on mixed datasets containing both long and short sequences. However, this heterogeneous sequence length distribution poses significant challenges for existing training systems, as they fail to simultaneously achieve high training efficiency for both long and short sequences, resulting in sub-optimal end-to-end system performance in Long-SFT. In this paper, we present a novel perspective on data scheduling to address the challenges posed by the heterogeneous data distributions in Long-SFT. We propose Skrull, a dynamic data scheduler specifically designed for efficient long-SFT. Through dynamic data scheduling, Skrull balances the computation requirements of long and short sequences, improving overall training efficiency. Furthermore, we formulate the scheduling process as a joint optimization problem and thoroughly analyze the trade-offs involved. Based on those analysis, Skrull employs a lightweight scheduling algorithm to achieve near-zero cost online scheduling in Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art distributed training system for LLMs. Experimental results demonstrate that Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world long-SFT scenarios.', 'abstract_zh': '长上下文监督微调（Long-SFT）在提升大型语言模型（LLMs）在长上下文任务上的表现中起着至关重要的作用。为平滑地使LLMs适应长上下文场景，这一过程通常涉及使用包含长序列和短序列的混合数据集进行训练。然而，这种异质的序列长度分布对现有的训练系统提出了重大挑战，因为它们不能同时高效地处理长和短序列，导致Long-SFT中的端到端系统性能不佳。在本文中，我们提出了一种新的数据调度视角，以应对Long-SFT中异质数据分布带来的挑战。我们提出了Skrull，一种专门针对高效长上下文微调设计的动态数据调度器。通过动态数据调度，Skrull平衡了长和短序列的计算需求，提高了整体训练效率。此外，我们将调度过程建模为联合优化问题，并深入分析了其中的权衡。基于这些分析，Skrull采用了一种轻量级的调度算法，在Long-SFT中实现了接近零成本的在线调度。最后，我们在DeepSpeed之上实现了Skrull，DeepSpeed是当前最先进的分布式训练系统之一。实验结果表明，在实际的长上下文微调场景中，Skrull在平均性能上优于DeepSpeed 3.76倍（最高可达7.54倍）。', 'title_zh': 'Skrull：通过动态数据调度实现高效长上下文微调'}
{'arxiv_id': 'arXiv:2505.19601', 'title': 'Preference Optimization by Estimating the Ratio of the Data Distribution', 'authors': 'Yeongmin Kim, Heesun Bae, Byeonghu Na, Il-Chul Moon', 'link': 'https://arxiv.org/abs/2505.19601', 'abstract': "Direct preference optimization (DPO) is widely used as a simple and stable method for aligning large language models (LLMs) with human preferences. This paper investigates a generalized DPO loss that enables a policy model to match the target policy from a likelihood ratio estimation perspective. The ratio of the target policy provides a unique identification of the policy distribution without relying on reward models or partition functions. This allows the generalized loss to retain both simplicity and theoretical guarantees, which prior work such as $f$-PO fails to achieve simultaneously. We propose Bregman preference optimization (BPO), a generalized framework for ratio matching that provides a family of objective functions achieving target policy optimality. BPO subsumes DPO as a special case and offers tractable forms for all instances, allowing implementation with a few lines of code. We further develop scaled Basu's power divergence (SBA), a gradient scaling method that can be used for BPO instances. The BPO framework complements other DPO variants and is applicable to target policies defined by these variants. In experiments, unlike other probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a trade-off between generation fidelity and diversity, instances of BPO improve both win rate and entropy compared with DPO. When applied to Llama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B backbones, with a 55.9\\% length-controlled win rate on AlpacaEval2.", 'abstract_zh': '直接偏好优化（DPO）广泛用于将大规模语言模型（LLMs）与人类偏好对齐的一种简单且稳定的方法。本文探讨了一种广义的DPO损失，从似然比估计的角度使得策略模型能够匹配目标策略。目标策略的比值为策略分布提供了一种独特的标识，无需依赖奖励模型或分区函数。这使得广义损失能够同时保持简单性和理论保证，而之前的如$f$-PO等工作未能同时实现。我们提出了一种广义的偏好优化框架——Bregman偏好优化（BPO），提供了一组目标函数实现目标策略最优性。BPO包含了DPO作为特例，并为所有实例提供了可处理的形式，允许用几行代码实现。我们进一步开发了一种缩放的Basu’s幂分离散化（SBA）梯度缩放方法，可以用于BPO实例。BPO框架补充了其他DPO变体，并适用于由这些变体定义的目标策略。在实验中，与其他概率损失扩展如$f$-DPO或$f$-PO相比，BPO实例在提高胜率和熵方面优于DPO。当应用于Llama-3-Instruct-8B时，BPO在AlpacaEval2上实现了Llama-3-8B骨干中的最佳性能，赢得率（控制长度后的）达到55.9%。', 'title_zh': '数据分布比率估计下的偏好优化'}
{'arxiv_id': 'arXiv:2505.19599', 'title': 'Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar', 'authors': 'Andrew Gambardella, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo', 'link': 'https://arxiv.org/abs/2505.19599', 'abstract': 'Typical methods for evaluating the performance of language models evaluate their ability to answer questions accurately. These evaluation metrics are acceptable for determining the extent to which language models can understand and reason about text in a general sense, but fail to capture nuanced capabilities, such as the ability of language models to recognize and obey rare grammar points, particularly in languages other than English. We measure the perplexity of language models when confronted with the "first person psych predicate restriction" grammar point in Japanese. Weblab is the only tested open source model in the 7-10B parameter range which consistently assigns higher perplexity to ungrammatical psych predicate sentences than grammatical ones. We give evidence that Weblab\'s uniformly bad tokenization is a possible root cause for its good performance, and show that Llama 3\'s perplexity on grammatical psych predicate sentences can be reduced by orders of magnitude (28x difference) by restricting test sentences to those with uniformly well-behaved tokenizations. We show in further experiments on machine translation tasks that language models will use alternative grammar patterns in order to produce grammatical sentences when tokenization issues prevent the most natural sentence from being output.', 'abstract_zh': 'typical 方法语言模型性能评估通常侧重于其准确回答问题的能力。这些评价指标在整体上衡量语言模型理解与推理文本的能力方面是可接受的，但无法捕捉到细微的能力，如语言模型识别和遵循罕见语法点的能力，尤其是在非英语语言中。我们衡量了语言模型在遇到日语“第一人称心理谓词限制”语法点时的困惑度。Weblab 是唯一在 7-10B 参数范围内经过测试的开源模型，它能一致地对不规范的心理谓词句子赋予更高的困惑度，而对规范的句子赋予较低的困惑度。我们提供了证据表明 Weblab 均匀恶化的分词可能是其良好表现的原因之一，并展示了通过限制测试句子的分词行为来显著降低 Llama 3 在规范的心理谓词句子上的困惑度（困惑度差异达 28 倍）。在进一步的机器翻译任务实验中，我们展示了语言模型将在分词问题阻止最自然句子生成时使用替代的语法模式以生成规范的句子。', 'title_zh': '不一致的分词导致语言模型对日语语法困惑'}
{'arxiv_id': 'arXiv:2505.19591', 'title': 'Multi-Agent Collaboration via Evolving Orchestration', 'authors': 'Yufan Dang, Chen Qian, Xueheng Luo, Jingru Fan, Zihao Xie, Ruijie Shi, Weize Chen, Cheng Yang, Xiaoyin Che, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, Maosong Sun', 'link': 'https://arxiv.org/abs/2505.19591', 'abstract': 'Large language models (LLMs) have achieved remarkable results across diverse downstream tasks, but their monolithic nature restricts scalability and efficiency in complex problem-solving. While recent research explores multi-agent collaboration among LLMs, most approaches rely on static organizational structures that struggle to adapt as task complexity and agent numbers grow, resulting in coordination overhead and inefficiencies. To this end, we propose a puppeteer-style paradigm for LLM-based multi-agent collaboration, where a centralized orchestrator ("puppeteer") dynamically directs agents ("puppets") in response to evolving task states. This orchestrator is trained via reinforcement learning to adaptively sequence and prioritize agents, enabling flexible and evolvable collective reasoning. Experiments on closed- and open-domain scenarios show that this method achieves superior performance with reduced computational costs. Analyses further reveal that the key improvements consistently stem from the emergence of more compact, cyclic reasoning structures under the orchestrator\'s evolution.', 'abstract_zh': '基于大型语言模型的木偶师式多agent协作框架：实现灵活演化推理以提升复杂问题解决效率', 'title_zh': '多智能体演化协调下的协作'}
{'arxiv_id': 'arXiv:2505.19578', 'title': 'Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing', 'authors': 'Dan Peng, Zhihui Fu, Zewen Ye, Zhuoran Song, Jun Wang', 'link': 'https://arxiv.org/abs/2505.19578', 'abstract': 'Sparse attention methods exploit the inherent sparsity in attention to speed up the prefilling phase of long-context inference, mitigating the quadratic complexity of full attention computation. While existing sparse attention methods rely on predefined patterns or inaccurate estimations to approximate attention behavior, they often fail to fully capture the true dynamics of attention, resulting in reduced efficiency and compromised accuracy. Instead, we propose a highly accurate sparse attention mechanism that shares similar yet precise attention patterns across heads, enabling a more realistic capture of the dynamic behavior of attention. Our approach is grounded in two key observations: (1) attention patterns demonstrate strong inter-head similarity, and (2) this similarity remains remarkably consistent across diverse inputs. By strategically sharing computed accurate patterns across attention heads, our method effectively captures actual patterns while requiring full attention computation for only a small subset of heads. Comprehensive evaluations demonstrate that our approach achieves superior or comparable speedup relative to state-of-the-art methods while delivering the best overall accuracy.', 'abstract_zh': '稀疏注意机制通过利用注意力的固有稀疏性来加速长上下文推理的预填充阶段，缓解全注意力计算的二次复杂度。现有稀疏注意机制依赖于预定义模式或不准确的估计来近似注意力行为，往往无法完全捕捉注意力的真实动态，导致效率降低和准确性受损。相反，我们提出了一种高度准确的稀疏注意机制，其在各个注意头之间共享相似但准确的注意模式，能够更真实地捕捉注意力动态行为。我们的方法基于两个关键观察：（1）注意模式显示了强烈的眼间相似性，（2）这种相似性在不同输入中保持高度一致。通过在注意头之间战略性地共享计算准确的模式，我们的方法有效地捕捉了实际模式，同时仅对一小部分头进行全注意力计算。全面的评估表明，与现有最佳方法相比，我们的方法在提供最佳总体准确性的同时实现了更优或相当的加速效果。', 'title_zh': '通过稀疏模式共享加速长上下文LLM的预填充'}
{'arxiv_id': 'arXiv:2505.19572', 'title': 'DocMEdit: Towards Document-Level Model Editing', 'authors': 'Li Zeng, Zeming Liu, Chong Feng, Heyan Huang, Yuhang Guo', 'link': 'https://arxiv.org/abs/2505.19572', 'abstract': 'Model editing aims to correct errors and outdated knowledge in the Large language models (LLMs) with minimal cost. Prior research has proposed a variety of datasets to assess the effectiveness of these model editing methods. However, most existing datasets only require models to output short phrases or sentences, overlooks the widespread existence of document-level tasks in the real world, raising doubts about their practical usability. Aimed at addressing this limitation and promoting the application of model editing in real-world scenarios, we propose the task of document-level model editing. To tackle such challenges and enhance model capabilities in practical settings, we introduce \\benchmarkname, a dataset focused on document-level model editing, characterized by document-level inputs and outputs, extrapolative, and multiple facts within a single edit. We propose a series of evaluation metrics and experiments. The results show that the difficulties in document-level model editing pose challenges for existing model editing methods.', 'abstract_zh': 'Model Editing at the Document Level Aims to Correct Errors and Outdated Knowledge in Large Language Models with Minimal Cost', 'title_zh': 'DocMEdit: 向量级模型编辑'}
{'arxiv_id': 'arXiv:2505.19548', 'title': 'How Syntax Specialization Emerges in Language Models', 'authors': 'Xufeng Duan, Zhaoqian Yao, Yunhao Zhang, Shaonan Wang, Zhenguang G. Cai', 'link': 'https://arxiv.org/abs/2505.19548', 'abstract': "Large language models (LLMs) have been found to develop surprising internal specializations: Individual neurons, attention heads, and circuits become selectively sensitive to syntactic structure, reflecting patterns observed in the human brain. While this specialization is well-documented, how it emerges during training and what influences its development remains largely unknown.\nIn this work, we tap into the black box of specialization by tracking its formation over time. By quantifying internal syntactic consistency across minimal pairs from various syntactic phenomena, we identify a clear developmental trajectory: Syntactic sensitivity emerges gradually, concentrates in specific layers, and exhibits a 'critical period' of rapid internal specialization. This process is consistent across architectures and initialization parameters (e.g., random seeds), and is influenced by model scale and training data. We therefore reveal not only where syntax arises in LLMs but also how some models internalize it during training. To support future research, we will release the code, models, and training checkpoints upon acceptance.", 'abstract_zh': '大型语言模型（LLMs）被发现发展出令人惊讶的内部专业化：单个神经元、注意力头和电路变得对句法规则选择性敏感，反映出人类大脑中观察到的模式。尽管这种专业化已被广泛记录，但其在训练过程中是如何形成的以及哪些因素影响其发展仍然知之甚少。\n\n在本工作中，我们通过跟踪专业化形成的全过程，开启了这一黑箱。通过量化各种句法现象的最小对之间内部句法一致性，我们识别出一个清晰的发展轨迹：句法敏感性逐渐出现，集中在特定层中，并表现出“关键期”快速内部专业化。这一过程在不同架构和初始化参数（如随机种子）下是一致的，并受模型规模和训练数据的影响。因此，我们不仅揭示了语法在LLMs中的起源，还揭示了某些模型在训练过程中如何内化它。为了支持未来的研究，在接受后，我们将发布代码、模型和训练检查点。', 'title_zh': '语法专门化如何在语言模型中 emergence'}
{'arxiv_id': 'arXiv:2505.19536', 'title': 'FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models', 'authors': 'Jintao Tong, Wenwei Jin, Pengda Qin, Anqi Li, Yixiong Zou, Yuhong Li, Yuhua Li, Ruixuan Li', 'link': 'https://arxiv.org/abs/2505.19536', 'abstract': "Large vision-language models (LVLMs) excel at multimodal understanding but suffer from high computational costs due to redundant vision tokens. Existing pruning methods typically rely on single-layer attention scores to rank and prune redundant visual tokens to solve this inefficiency. However, as the interaction between tokens and layers is complicated, this raises a basic question: Is such a simple single-layer criterion sufficient to identify redundancy? To answer this question, we rethink the emergence of redundant visual tokens from a fundamental perspective: information flow, which models the interaction between tokens and layers by capturing how information moves between tokens across layers. We find (1) the CLS token acts as an information relay, which can simplify the complicated flow analysis; (2) the redundancy emerges progressively and dynamically via layer-wise attention concentration; and (3) relying solely on attention scores from single layers can lead to contradictory redundancy identification. Based on this, we propose FlowCut, an information-flow-aware pruning framework, mitigating the insufficiency of the current criterion for identifying redundant tokens and better aligning with the model's inherent behaviors. Extensive experiments show that FlowCut achieves superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x speed-up in the prefilling stage. Our code is available at this https URL", 'abstract_zh': '大型多模态语言模型在多模态理解方面表现出色，但由于冗余视觉标记导致的高计算成本而受到影响。现有的剪枝方法通常依赖单层attention分数来排名和剪枝冗余的视觉标记以解决这种低效性。然而，由于标记和层之间的交互复杂，这引发了一个基本问题：这样的单层标准是否足以识别冗余性？为回答这一问题，我们从基础的信息流视角重新思考冗余视觉标记的出现：信息流模型通过捕捉信息在不同层之间如何流动来建模标记与层之间的交互。我们发现（1）CLS标记充当信息中继的角色，可以简化复杂的信息流分析；（2）冗余性通过逐层注意力集中逐渐且动态地出现；（3）仅依赖单层的attention分数来进行冗余性识别可能会导致矛盾的结果。基于此，我们提出FlowCut，一种信息流感知的剪枝框架，缓解当前标准在识别冗余标记方面的不足，并更好地与模型固有的行为相契合。广泛的实验表明，FlowCut取得了优异的结果，在LLaVA-1.5-7B上优于现有最佳性能1.6%，同时标记减少88.9%；在LLaVA-NeXT-7B上优于现有最佳性能4.3%，标记减少94.4%，预填充阶段加速3.2倍。我们的代码可在以下链接获取。', 'title_zh': 'FlowCut：基于信息流重新思考冗余性以实现高效视觉语言模型'}
{'arxiv_id': 'arXiv:2505.19514', 'title': 'SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback', 'authors': 'Yaoning Yu, Ye Yu, Kai Wei, Haojing Luo, Haohan Wang', 'link': 'https://arxiv.org/abs/2505.19514', 'abstract': 'Prompt quality plays a critical role in the performance of large language models (LLMs), motivating a growing body of work on prompt optimization. Most existing methods optimize prompts over a fixed dataset, assuming static input distributions and offering limited support for iterative improvement. We introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a closed-loop framework for prompt learning that integrates synthetic data generation into the optimization process. SIPDO couples a synthetic data generator with a prompt optimizer, where the generator produces new examples that reveal current prompt weaknesses and the optimizer incrementally refines the prompt in response. This feedback-driven loop enables systematic improvement of prompt performance without assuming access to external supervision or new tasks. Experiments across question answering and reasoning benchmarks show that SIPDO outperforms standard prompt tuning methods, highlighting the value of integrating data synthesis into prompt learning workflows.', 'abstract_zh': '基于数据扩充优化的自我提升提示（SIPDO：Self-Improving Prompts through Data-Augmented Optimization）', 'title_zh': 'SIPDO：通过合成数据反馈的闭环提示优化'}
{'arxiv_id': 'arXiv:2505.19505', 'title': 'Hierarchical Tree Search-based User Lifelong Behavior Modeling on Large Language Model', 'authors': 'Yu Xia, Rui Zhong, Hao Gu, Wei Yang, Chi Lu, Peng Jiang, Kun Gai', 'link': 'https://arxiv.org/abs/2505.19505', 'abstract': 'Large Language Models (LLMs) have garnered significant attention in Recommendation Systems (RS) due to their extensive world knowledge and robust reasoning capabilities. However, a critical challenge lies in enabling LLMs to effectively comprehend and extract insights from massive user behaviors. Current approaches that directly leverage LLMs for user interest learning face limitations in handling long sequential behaviors, effectively extracting interest, and applying interest in practical scenarios. To address these issues, we propose a Hierarchical Tree Search-based User Lifelong Behavior Modeling framework (HiT-LBM). HiT-LBM integrates Chunked User Behavior Extraction (CUBE) and Hierarchical Tree Search for Interest (HTS) to capture diverse interests and interest evolution of user. CUBE divides user lifelong behaviors into multiple chunks and learns the interest and interest evolution within each chunk in a cascading manner. HTS generates candidate interests through hierarchical expansion and searches for the optimal interest with process rating model to ensure information gain for each behavior chunk. Additionally, we design Temporal-Ware Interest Fusion (TIF) to integrate interests from multiple behavior chunks, constructing a comprehensive representation of user lifelong interests. The representation can be embedded into any recommendation model to enhance performance. Extensive experiments demonstrate the effectiveness of our approach, showing that it surpasses state-of-the-art methods.', 'abstract_zh': '大型语言模型（LLMs）在推荐系统（RS）中由于其广泛的世界知识和强大的推理能力引起了广泛关注。然而，在使LLMs有效地理解和提取大量用户行为的洞察方面仍存在关键挑战。当前直接利用LLMs进行用户兴趣学习的方法在处理长序列行为、有效提取兴趣以及在实际场景中应用兴趣方面存在局限性。为了解决这些问题，我们提出了一种基于分层树搜索的用户终身行为建模框架（HiT-LBM）。HiT-LBM结合了片段化用户行为提取（CUBE）和分层树搜索兴趣（HTS）来捕捉用户多样化的兴趣和兴趣演变。CUBE将用户终身行为划分为多个片段，并通过对每个片段逐级学习来捕捉兴趣及其演变。HTS通过分层扩展生成候选兴趣，并通过过程评分模型进行搜索以确保每个行为片段的信息增益。此外，我们设计了时间感知兴趣融合（TIF）以整合多个行为片段的兴趣，构建用户终身兴趣的全面表示。该表示可以嵌入到任何推荐模型中以提高性能。广泛的实验验证了我们方法的有效性，表明其优于现有方法。', 'title_zh': '基于分层树搜索的用户在大型语言模型上的终身行为建模'}
{'arxiv_id': 'arXiv:2505.19504', 'title': 'DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation', 'authors': 'Pingzhi Li, Zhen Tan, Huaizhi Qu, Huan Liu, Tianlong Chen', 'link': 'https://arxiv.org/abs/2505.19504', 'abstract': "Large Language Models (LLMs) represent substantial intellectual and economic investments, yet their effectiveness can inadvertently facilitate model imitation via knowledge distillation (KD).In practical scenarios, competitors can distill proprietary LLM capabilities by simply observing publicly accessible outputs, akin to reverse-engineering a complex performance by observation alone. Existing protective methods like watermarking only identify imitation post-hoc, while other defenses assume the student model mimics the teacher's internal logits, rendering them ineffective against distillation purely from observed output text. This paper confronts the challenge of actively protecting LLMs within the realistic constraints of API-based access. We introduce an effective and efficient Defensive Output Generation (DOGe) strategy that subtly modifies the output behavior of an LLM. Its outputs remain accurate and useful for legitimate users, yet are designed to be misleading for distillation, significantly undermining imitation attempts. We achieve this by fine-tuning only the final linear layer of the teacher LLM with an adversarial loss. This targeted training approach anticipates and disrupts distillation attempts during inference time. Our experiments show that, while preserving or even improving the original performance of the teacher model, student models distilled from the defensively generated teacher outputs demonstrate catastrophically reduced performance, demonstrating our method's effectiveness as a practical safeguard against KD-based model imitation.", 'abstract_zh': '大型语言模型（LLMs）代表了巨大的智力和经济投入，然而其有效性可能会不经意间通过知识蒸馏（KD）促进模型模仿。在实际场景中，竞争者可以通过仅观察公开可访问的输出来简单地蒸馏私有LLM的能力，这类似于仅通过观察逆向工程复杂性能。现有的保护方法如水印只能事后识别模仿，而其他防御措施则假设学生模型模仿教师模型的内部logits，使其对仅从观察输出文本进行的蒸馏无效。本文在基于API访问的现实约束条件下应对积极保护LLMs的挑战。我们引入了一种有效且高效的Defensive Output Generation（DOGe）策略，该策略微妙地修改了LLM的输出行为。其输出对合法用户仍然是准确且有用的，但旨在误导蒸馏，显著削弱模仿企图。我们通过仅微调教师LLM的最后一层线性层并引入对抗损失来实现这一点。这种目标化的训练方法能够在推理时间预见并打断蒸馏企图。我们的实验表明，尽管保留或甚至改善了教师模型的原始性能，从防御性生成的教师输出蒸馏的学生模型则表现出灾难性的性能下降，证明了该方法作为一种实际防护手段的有效性，用以抵御基于KD的模型模仿。', 'title_zh': 'DOGe: 针对知识蒸馏的防御性输出生成以保护LLM'}
{'arxiv_id': 'arXiv:2505.19502', 'title': 'CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation', 'authors': 'Guang Yang, Yu Zhou, Xiang Chen, Wei Zheng, Xing Hu, Xin Zhou, David Lo, Taolue Chen', 'link': 'https://arxiv.org/abs/2505.19502', 'abstract': 'Trustworthy evaluation methods for code snippets play a crucial role in neural code generation. Traditional methods, which either rely on reference solutions or require executable test cases, have inherent limitation in flexibility and scalability. The recent LLM-as-Judge methodology offers a promising alternative by directly evaluating functional consistency between the problem description and the generated code. To systematically understand the landscape of these LLM-as-Judge methods, we conduct a comprehensive empirical study across three diverse datasets. Our investigation reveals the pros and cons of two categories of LLM-as-Judge methods: the methods based on general foundation models can achieve good performance but require complex prompts and lack explainability, while the methods based on reasoning foundation models provide better explainability with simpler prompts but demand substantial computational resources due to their large parameter sizes. To address these limitations, we propose CODE-DITING, a novel code evaluation method that balances accuracy, efficiency and explainability. We develop a data distillation framework that effectively transfers reasoning capabilities from DeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing evaluation explainability and reducing the computational cost. With the majority vote strategy in the inference process, CODE-DITING 1.5B outperforms all models with the same magnitude of parameters and achieves performance which would normally exhibit in a model with 5 times of parameter scale. CODE-DITING 7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the parameter volume of these large models. Further experiments show that CODEDITING is robust to preference leakage and can serve as a promising alternative for code evaluation.', 'abstract_zh': '可信的代码片段评估方法对于神经代码生成至关重要。传统的方法要么依赖参考解决方案，要么需要可执行的测试用例，这在灵活性和可扩展性方面存在固有的局限性。最近的LLM-as-Judge方法通过直接评估问题描述与生成代码之间的功能一致性，提供了一种有前途的替代方案。为了系统地理解这些LLM-as-Judge方法的全景，我们在三个不同的数据集中进行了一项全面的经验研究。我们的调查表明，两类LLM-as-Judge方法各有优劣：基于通用基础模型的方法可以取得良好的性能，但需要复杂的提示且缺乏解释性，而基于推理基础模型的方法则提供更好的解释性且提示更简单，但由于参数量大，需要大量的计算资源。为了解决这些局限性，我们提出了CODE-DITING，一种新颖的代码评估方法，平衡了准确度、效率和解释性。我们开发了一种数据蒸馏框架，有效地将DeepSeek-R1671B的推理能力转移到我们的CODE-DITING 1.5B和7B模型中，显著提高了评估的解释性和降低了计算成本。通过推理过程中的多数投票策略，CODE-DITING 1.5B的表现优于所有具有相同规模参数的模型，并达到了一个具有五倍参数规模模型的性能。虽然CODE-DITING 7B仅使用这些大型模型1%的参数量，但它仍超过了GPT-4o和DeepSeek-V3 671B。进一步的实验表明，CODE-DITING 对偏好泄漏具有鲁棒性，并且可以作为代码评估的一种有前途的替代方案。', 'title_zh': '代码编辑：基于推理的代码功能对齐评估度量标准'}
{'arxiv_id': 'arXiv:2505.19498', 'title': 'Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models', 'authors': 'Nanxing Hu, Xiaoyue Duan, Jinchao Zhang, Guoliang Kang', 'link': 'https://arxiv.org/abs/2505.19498', 'abstract': "Large Vision-Language Models (LVLMs) usually generate texts which satisfy context coherence but don't match the visual input. Such a hallucination issue hinders LVLMs' applicability in the real world. The key to solving hallucination in LVLM is to make the text generation rely more on the visual content. Most previous works choose to enhance/adjust the features/output of a specific modality (i.e., visual or textual) to alleviate hallucinations in LVLM, which do not explicitly or systematically enhance the visual reliance. In this paper, we comprehensively investigate the factors which may degenerate the visual reliance in text generation of LVLM from a Bayesian perspective. Based on our observations, we propose to mitigate hallucination in LVLM from three aspects. Firstly, we observe that not all visual tokens are informative in generating meaningful texts. We propose to evaluate and remove redundant visual tokens to avoid their disturbance. Secondly, LVLM may encode inappropriate prior information, making it lean toward generating unexpected words. We propose a simple yet effective way to rectify the prior from a Bayesian perspective. Thirdly, we observe that starting from certain steps, the posterior of next-token prediction conditioned on visual tokens may collapse to a prior distribution which does not depend on any informative visual tokens at all. Thus, we propose to stop further text generation to avoid hallucination. Extensive experiments on three benchmarks including POPE, CHAIR, and MME demonstrate that our method can consistently mitigate the hallucination issue of LVLM and performs favorably against previous state-of-the-arts.", 'abstract_zh': '大型视觉-语言模型（LVLMs）通常生成与上下文连贯但不匹配视觉输入的文本。这种幻觉问题阻碍了LVLM在现实世界中的应用。解决LVLM幻觉的关键在于让文本生成更加依赖视觉内容。大多数前期工作选择了增强或调整特定模态（即视觉或文本）的特征/输出来缓解LVLM中的幻觉，但没有明确或系统地增强视觉依赖性。在本文中，我们从贝叶斯视角全面探讨了可能减弱LVLM文本生成中视觉依赖性的因素。基于我们的观察，我们提出从三个方面减轻LVLM中的幻觉。首先，我们观察到并非所有视觉词都对生成有意义的文本有信息性。我们提出评估和移除冗余的视觉词以避免其干扰。其次，LVLM可能编码了不适当的先验信息，使其倾向于生成出乎意料的词。我们提出了一种从贝叶斯视角简单而有效的方法来修正先验。第三，我们观察到从某一步开始，基于视觉词的下一个词预测的后验可能崩溃为一个与任何信息性视觉词无关的先验分布。因此，我们提出停止进一步的文本生成以避免幻觉。在POPE、CHAIR和MME三个基准上的广泛实验表明，我们的方法可以一致地减轻LVLM的幻觉问题，并在与前期最先进的方法进行比较时表现更好。', 'title_zh': '从贝叶斯视角缓解大型视觉语言模型中幻觉，增强文本生成中的视觉依赖性'}
{'arxiv_id': 'arXiv:2505.19481', 'title': 'Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs', 'authors': 'Hao Kang, Qingru Zhang, Han Cai, Weiyuan Xu, Tushar Krishna, Yilun Du, Tsachy Weissman', 'link': 'https://arxiv.org/abs/2505.19481', 'abstract': 'Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency quality trade off, it remains underexplored in the context of LLM based agents. In this work, we present the first systematic study of this trade off in real time decision making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading, underscoring the need for latency aware evaluation and deployment strategies for LLM based agents. These results demonstrate the critical importance of latency aware evaluation and deployment strategies for real world LLM based agents. Our benchmarks are available at Latency Sensitive Benchmarks.', 'abstract_zh': '大规模语言模型（LLMs）在多样化的推理和生成任务中展示了出色的表现，并越来越多地被部署为动态环境中的代理，如代码生成和推荐系统。然而，许多实际应用，如高频交易和实时竞技游戏，要求在严格的延迟约束下做出决策，其中更快的响应直接转化为更高的收益。尽管这一延迟质量权衡的重要性不言而喻，但在基于LLM的代理中，它仍然没有得到充分探索。在本文中，我们首次系统研究了这种权衡在实时决策任务中的表现。为了支持我们的研究，我们引入了两个新的基准测试：HFTBench，一个高频交易模拟平台，和StreetFighter，一个竞技游戏平台。我们的分析表明，最优的延迟质量平衡因任务而异，牺牲一些质量以降低延迟可以显著提升下游性能。为此，我们提出了FPX，一个适应性框架，可以根据实时需求动态选择模型大小和量化级别。我们的方法在两个基准测试中均取得了最佳性能，在Street Fighter中将胜率提高最多80%，在交易中将日收益提升最多26.52%，强调了对基于LLM的代理进行延迟意识评估和部署策略的需求。这些结果证明了对基于LLM的实际应用代理进行延迟意识评估和部署策略的重要性。我们提供的基准测试可在 Latency Sensitive Benchmarks 获取。', 'title_zh': '人在快与错之间：平衡LLM在延迟敏感决策中的速度与准确性'}
{'arxiv_id': 'arXiv:2505.19443', 'title': 'Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI', 'authors': 'Ranjan Sapkota, Konstantinos I. Roumeliotis, Manoj Karkee', 'link': 'https://arxiv.org/abs/2505.19443', 'abstract': 'This review presents a comprehensive analysis of two emerging paradigms in AI-assisted software development: vibe coding and agentic coding. While both leverage large language models (LLMs), they differ fundamentally in autonomy, architectural design, and the role of the developer. Vibe coding emphasizes intuitive, human-in-the-loop interaction through prompt-based, conversational workflows that support ideation, experimentation, and creative exploration. In contrast, agentic coding enables autonomous software development through goal-driven agents capable of planning, executing, testing, and iterating tasks with minimal human intervention. We propose a detailed taxonomy spanning conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems. Through comparative workflow analysis and 20 detailed use cases, we illustrate how vibe systems thrive in early-stage prototyping and education, while agentic systems excel in enterprise-grade automation, codebase refactoring, and CI/CD integration. We further examine emerging trends in hybrid architectures, where natural language interfaces are coupled with autonomous execution pipelines. Finally, we articulate a future roadmap for agentic AI, outlining the infrastructure needed for trustworthy, explainable, and collaborative systems. Our findings suggest that successful AI software engineering will rely not on choosing one paradigm, but on harmonizing their strengths within a unified, human-centered development lifecycle.', 'abstract_zh': '本文对AI辅助软件开发中两种新兴范式——情绪编码和主动编码——进行了全面分析。尽管两者都利用了大规模语言模型（LLMs），但在自主性、架构设计和开发人员的角色方面存在根本差异。情绪编码强调直观的人机交互，通过基于提示的对话工作流支持创意构想、实验和创造性探索。相比之下，主动编码通过目标驱动的代理实现自主软件开发，这些代理能够规划、执行、测试和迭代任务，同时尽量减少人类干预。我们提出了一个详细的分类框架，涵盖概念基础、执行模型、反馈循环、安全机制、调试策略以及现实世界工具生态系统。通过对比工作流分析和20个详细的用例，我们展示了情绪系统在早期原型设计和教育中的优势，以及主动系统在企业级自动化、代码库重构和CI/CD集成中的卓越表现。此外，我们探讨了自然语言接口与自主执行管道结合的新兴混合架构趋势。最后，我们阐述了主动AI的未来路线图，概述了实现可信赖、可解释和协作系统的基础设施需求。我们的研究结果表明，成功的AI软件工程依赖于在统一的、以人为中心的开发生命周期中和谐地结合这两种范式的优点。', 'title_zh': '意志编码 vs. 主体编码：主体性AI的基本原理及实践意义'}
{'arxiv_id': 'arXiv:2505.19430', 'title': 'Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation', 'authors': 'Keane Ong, Rui Mao, Deeksha Varshney, Paul Pu Liang, Erik Cambria, Gianmarco Mengaldo', 'link': 'https://arxiv.org/abs/2505.19430', 'abstract': 'Counterfactual reasoning typically involves considering alternatives to actual events. While often applied to understand past events, a distinct form-forward counterfactual reasoning-focuses on anticipating plausible future developments. This type of reasoning is invaluable in dynamic financial markets, where anticipating market developments can powerfully unveil potential risks and opportunities for stakeholders, guiding their decision-making. However, performing this at scale is challenging due to the cognitive demands involved, underscoring the need for automated solutions. Large Language Models (LLMs) offer promise, but remain unexplored for this application. To address this gap, we introduce a novel benchmark, Fin-Force-FINancial FORward Counterfactual Evaluation. By curating financial news headlines and providing structured evaluation, Fin-Force supports LLM based forward counterfactual generation. This paves the way for scalable and automated solutions for exploring and anticipating future market developments, thereby providing structured insights for decision-making. Through experiments on Fin-Force, we evaluate state-of-the-art LLMs and counterfactual generation methods, analyzing their limitations and proposing insights for future research.', 'abstract_zh': '金融前瞻反事实评估：Fin-Force', 'title_zh': '使用大型语言模型推导战略市场洞察：前瞻性反事实生成基准'}
{'arxiv_id': 'arXiv:2505.19427', 'title': 'WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference', 'authors': 'Sihan Chen, Dan Zhao, Jongwoo Ko, Colby Banbury, Huiping Zhuang, Luming Liang, Tianyi Chen', 'link': 'https://arxiv.org/abs/2505.19427', 'abstract': 'The growing computational demands of large language models (LLMs) make efficient inference and activation strategies increasingly critical. While recent approaches, such as Mixture-of-Experts (MoE), leverage selective activation but require specialized training, training-free sparse activation methods offer broader applicability and superior resource efficiency through their plug-and-play design. However, many existing methods rely solely on hidden state magnitudes to determine activation, resulting in high approximation errors and suboptimal inference accuracy. To address these limitations, we propose WINA (Weight Informed Neuron Activation), a novel, simple, and training-free sparse activation framework that jointly considers hidden state magnitudes and the column-wise $\\ell_2$-norms of weight matrices. We show that this leads to a sparsification strategy that obtains optimal approximation error bounds with theoretical guarantees tighter than existing techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g., TEAL) by up to $2.94\\%$ in average performance at the same sparsity levels, across a diverse set of LLM architectures and datasets. These results position WINA as a new performance frontier for training-free sparse activation in LLM inference, advancing training-free sparse activation methods and setting a robust baseline for efficient inference. The source code is available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）不断增长的计算需求使得高效的推理和激活策略越来越重要。虽然最近的方法，如专家混合（MoE），利用了选择性激活但需要专门训练，无训练的稀疏激活方法通过其即插即用设计提供了更广泛的应用性和更好的资源效率。然而，许多现有方法仅依赖隐藏状态的幅度来决定激活，导致高近似误差和次优化的推理精度。为了解决这些局限性，我们提出了WINA（Weight Informed Neuron Activation），一种新颖的、简单的、无训练的稀疏激活框架，同时考虑隐藏状态幅度和权重矩阵的列 wise $\\ell_2$-范数。我们展示了这导致了一种稀疏化策略，该策略在理论上比现有技术更严格的保证下获得了最优的近似误差界。在实验中，WINA在相同的稀疏化水平下也优于最先进的方法（例如TEAL），在不同程度的大型语言模型架构和数据集上，平均性能提高了2.94%。这些结果将WINA定位为无训练稀疏激活在LLM推理中新的性能前沿，推动了无训练稀疏激活方法的发展，并设定了高效推理的稳健基线。源代码可在以下链接获取。', 'title_zh': 'WINA: 基于权重的神经元激活加速大型语言模型推理'}
{'arxiv_id': 'arXiv:2505.19426', 'title': 'The Role of Diversity in In-Context Learning for Large Language Models', 'authors': 'Wenyang Xiao, Haoyu Zhao, Lingxiao Huang', 'link': 'https://arxiv.org/abs/2505.19426', 'abstract': 'In-context learning (ICL) is a crucial capability of current large language models (LLMs), where the selection of examples plays a key role in performance. While most existing approaches focus on selecting the most similar examples to the query, the impact of diversity in example selection remains underexplored. We systematically investigate the role of diversity in in-context example selection through experiments across a range of tasks, from sentiment classification to more challenging math and code problems. Experiments on Llama-3.1, Gemma-2, and Mistral-v0.3 families of models show that diversity-aware selection methods improve performance, particularly on complex tasks like math and code, and enhance robustness to out-of-distribution queries. To support these findings, we introduce a theoretical framework that explains the benefits of incorporating diversity in in-context example selection.', 'abstract_zh': '基于上下文学习（ICL）中多样性在当前大型语言模型（LLMs）性能中的作用研究', 'title_zh': '大规模语言模型上下文学习中多样性的角色'}
{'arxiv_id': 'arXiv:2505.19419', 'title': 'It\'s Not Just Labeling" -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features', 'authors': 'Baichuan Li, Larry Powell, Tracy Hammond', 'link': 'https://arxiv.org/abs/2505.19419', 'abstract': 'The quality of training data is critical to the performance of machine learning applications in domains like transportation, healthcare, and robotics. Accurate image labeling, however, often relies on time-consuming, expert-driven methods with limited feedback. This research introduces a sketch-based annotation approach supported by large language models (LLMs) to reduce technical barriers and enhance accessibility. Using a synthetic dataset, we examine how sketch recognition features relate to LLM feedback metrics, aiming to improve the reliability and interpretability of LLM-assisted labeling. We also explore how prompting strategies and sketch variations influence feedback quality. Our main contribution is a sketch-based virtual assistant that simplifies annotation for non-experts and advances LLM-driven labeling tools in terms of scalability, accessibility, and explainability.', 'abstract_zh': '基于素描的标注方法：借助大型语言模型降低技术门槛和提升标注可访问性和解释性', 'title_zh': '不仅仅是标签——关于LLM生成反馈可解释性和图像标签草图特征的研究'}
{'arxiv_id': 'arXiv:2505.19395', 'title': 'VADER: A Human-Evaluated Benchmark for Vulnerability Assessment, Detection, Explanation, and Remediation', 'authors': 'Ethan TS. Liu, Austin Wang, Spencer Mateega, Carlos Georgescu, Danny Tang', 'link': 'https://arxiv.org/abs/2505.19395', 'abstract': "Ensuring that large language models (LLMs) can effectively assess, detect, explain, and remediate software vulnerabilities is critical for building robust and secure software systems. We introduce VADER, a human-evaluated benchmark designed explicitly to assess LLM performance across four key vulnerability-handling dimensions: assessment, detection, explanation, and remediation. VADER comprises 174 real-world software vulnerabilities, each carefully curated from GitHub repositories and annotated by security experts. For each vulnerability case, models are tasked with identifying the flaw, classifying it using Common Weakness Enumeration (CWE), explaining its underlying cause, proposing a patch, and formulating a test plan. Using a one-shot prompting strategy, we benchmark six state-of-the-art LLMs (Claude 3.7 Sonnet, Gemini 2.5 Pro, GPT-4.1, GPT-4.5, Grok 3 Beta, and o3) on VADER, and human security experts evaluated each response according to a rigorous scoring rubric emphasizing remediation (quality of the code fix, 50%), explanation (20%), and classification and test plan (30%) according to a standardized rubric. Our results show that current state-of-the-art LLMs achieve only moderate success on VADER - OpenAI's o3 attained 54.7% accuracy overall, with others in the 49-54% range, indicating ample room for improvement. Notably, remediation quality is strongly correlated (Pearson r > 0.97) with accurate classification and test plans, suggesting that models that effectively categorize vulnerabilities also tend to fix them well. VADER's comprehensive dataset, detailed evaluation rubrics, scoring tools, and visualized results with confidence intervals are publicly released, providing the community with an interpretable, reproducible benchmark to advance vulnerability-aware LLMs. All code and data are available at: this https URL", 'abstract_zh': '确保大型语言模型（LLMs）能有效评估、检测、解释和 remediate 软件漏洞对于构建 robust 和 secure 的软件系统至关重要。我们引入了 VADER，一个由人工评估的基准，旨在全面评估 LLM 在四个关键的漏洞处理维度（评估、检测、解释和 remediation）上的性能。VADER 包含 174 个真实世界的软件漏洞，每个漏洞都来自 GitHub 仓库并由安全专家精心筛选和标注。对于每个漏洞案例，模型需要识别缺陷、采用常见弱点枚举（CWE）对其进行分类、解释其根本原因、提出补丁并制定测试计划。使用单次提示策略，我们在 VADER 上基准测试了六种最先进的 LLM（Claude 3.7 Sonnet、Gemini 2.5 Pro、GPT-4.1、GPT-4.5、Grok 3 Beta 和 o3），并且由人类安全专家根据严格的评分规则对每个回应进行评估，该规则强调 remediation（代码修复质量，占 50%）、解释（占 20%）以及分类和测试计划（标准化规则，占 30%）。结果显示，当前最先进的 LLM 在 VADER 上仅取得中等成效——OpenAI 的 o3 达到了 54.7% 的准确率，其他模型在 49-54% 之间，表明仍然有很大的改进空间。值得注意的是，remediation 质量与准确分类和测试计划高度相关（皮尔逊相关系数 > 0.97），这表明能够有效分类漏洞的模型往往也能较好地修复它们。VADER 的全面数据集、详细的评估规则、评分工具以及带有置信区间的结果可视化公开发布，为社区提供了一个可解释和可复现的基准，以推动漏洞感知 LLM 的发展。所有代码和数据可在以下链接获取：this https URL。', 'title_zh': 'VADER：漏洞评估、检测、解释与修复的人工评估基准'}
{'arxiv_id': 'arXiv:2505.19345', 'title': 'PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims', 'authors': 'Yongmin Yoo, Qiongkai Xu, Longbing Cao', 'link': 'https://arxiv.org/abs/2505.19345', 'abstract': 'Natural language generation (NLG) metrics play a central role in evaluating generated texts, but are not well suited for the structural and legal characteristics of patent documents. Large language models (LLMs) offer strong potential in automating patent generation, yet research on evaluating LLM-generated patents remains limited, especially in evaluating the generation quality of patent claims, which are central to defining the scope of protection. Effective claim evaluation requires addressing legal validity, technical accuracy, and structural compliance. To address this gap, we introduce PatentScore, a multi-dimensional evaluation framework for assessing LLM-generated patent claims. PatentScore incorporates: (1) hierarchical decomposition for claim analysis; (2) domain-specific validation patterns based on legal and technical standards; and (3) scoring across structural, semantic, and legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects patent-specific constraints and document structures, enabling evaluation beyond surface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a Pearson correlation of $r = 0.819$ with expert annotations, outperforming existing NLG metrics. Furthermore, we conduct additional evaluations using open models such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong correlations with expert judgments, confirming the robustness and generalizability of our framework.', 'abstract_zh': '自然语言生成(NLG)指标在评估生成文本方面发挥着核心作用，但它们并不适合专利文件的结构和法律特点。大规模语言模型(LLMs)在自动化专利生成方面展现出强大的潜力，然而关于评估LLM生成的专利的研究仍然有限，尤其是在评估专利 claims 的生成质量方面，而claims是定义保护范围的核心。有效的claims评估需要解决法律有效性、技术准确性和结构合规性。为填补这一空白，我们提出了PatentScore，这是一个多维度的评估框架，用于评估LLM生成的专利claims。PatentScore包括：（1）索赔分析的分层分解；（2）基于法律和技术标准的领域特定验证模式；以及（3）在结构、语义和法律维度上的评分。与通用的NLG指标不同，PatentScore反映专利特定的约束和文档结构，能够超越表面相似性进行评估。我们评估了400个由GPT-4o-mini生成的Claim 1，并报告与专家注释的皮尔森相关系数为$r = 0.819$，优于现有NLG指标。此外，我们还使用了诸如Claude-3.5-Haiku和Gemini-1.5-flash等开源模型进行了额外评估，所有这些都与专家判断显示出强烈的相关性，证实了我们框架的稳健性和通用性。', 'title_zh': 'PatentScore: 多维度评估生成型专利主张'}
{'arxiv_id': 'arXiv:2505.19293', 'title': '100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?', 'authors': 'Wang Yang, Hongye Jin, Shaochen Zhong, Song Jiang, Qifan Wang, Vipin Chaudhary, Xiaotian Han', 'link': 'https://arxiv.org/abs/2505.19293', 'abstract': "Long-context capability is considered one of the most important abilities of LLMs, as a truly long context-capable LLM enables users to effortlessly process many originally exhausting tasks -- e.g., digesting a long-form document to find answers vs. directly asking an LLM about it. However, existing real-task-based long-context evaluation benchmarks have two major shortcomings. First, benchmarks like LongBench often do not provide proper metrics to separate long-context performance from the model's baseline ability, making cross-model comparison unclear. Second, such benchmarks are usually constructed with fixed input lengths, which limits their applicability across different models and fails to reveal when a model begins to break down. To address these issues, we introduce a length-controllable long-context benchmark and a novel metric that disentangles baseline knowledge from true long-context capabilities. Experiments demonstrate the superiority of our approach in effectively evaluating LLMs.", 'abstract_zh': '长上下文能力是大语言模型最重要的能力之一，真正的长上下文能力使用户能够轻松处理许多原本令人疲惫的任务——例如，消化长文档以找到答案而非直接询问模型。然而，现有的基于实际任务的长上下文评估基准存在两大缺陷。首先，这些基准（如LongBench）通常未能提供适当的指标来区分长上下文性能与模型的基本能力，导致不同模型之间的比较不清晰。其次，这类基准通常使用固定长度的输入构建，这限制了它们在不同模型之间的适用性，并未能揭示模型何时开始失效。为解决这些问题，我们引入了一个可调节长度的长上下文基准和一个全新的指标，该指标能够分离基本知识和真实的长上下文能力。实验表明，我们的方法在有效评估大语言模型方面具有优越性。', 'title_zh': '100-LongBench：事实上，长上下文基准测试是否真的在评估长上下文能力？'}
{'arxiv_id': 'arXiv:2505.19273', 'title': 'Eta-WavLM: Efficient Speaker Identity Removal in Self-Supervised Speech Representations Using a Simple Linear Equation', 'authors': 'Giuseppe Ruggiero, Matteo Testa, Jurgen Van de Walle, Luigi Di Caro', 'link': 'https://arxiv.org/abs/2505.19273', 'abstract': 'Self-supervised learning (SSL) has reduced the reliance on expensive labeling in speech technologies by learning meaningful representations from unannotated data. Since most SSL-based downstream tasks prioritize content information in speech, ideal representations should disentangle content from unwanted variations like speaker characteristics in the SSL representations. However, removing speaker information often degrades other speech components, and existing methods either fail to fully disentangle speaker identity or require resource-intensive models. In this paper, we propose a novel disentanglement method that linearly decomposes SSL representations into speaker-specific and speaker-independent components, effectively generating speaker disentangled representations. Comprehensive experiments show that our approach achieves speaker independence and as such, when applied to content-driven tasks such as voice conversion, our representations yield significant improvements over state-of-the-art methods.', 'abstract_zh': '自监督学习（SSL）通过从未标注数据中学习有意义的表示，减少了语音技术对昂贵标注的依赖。由于大多数基于SSL的下游任务优先处理语音的内容信息，理想的表示应该在SSL表示中解开内容与如说话人特征等不必要的变异。然而，去除说话人信息往往会降低其他语音组件的质量，现有方法要么无法完全解开说话人身份，要么需要资源密集型模型。在本文中，我们提出了一种新颖的解耦方法，通过线性分解SSL表示为说话人特定和说话人独立的成分，有效地生成说话人解耦表示。全面的实验表明，我们的方法实现了说话人独立性，因此在诸如声音转换等内容驱动的任务中，我们的表示方法比现有最先进的方法取得了显著的改进。', 'title_zh': 'Eta-WavLM：高效自监督语音表示中的说话人身份去除使用简单线性方程'}
{'arxiv_id': 'arXiv:2505.19261', 'title': 'Enhancing Text-to-Image Diffusion Transformer via Split-Text Conditioning', 'authors': 'Yu Zhang, Jialei Zhou, Xinchen Li, Qi Zhang, Zhongwei Wan, Tianyu Wang, Duoqian Miao, Changwei Wang, Longbing Cao', 'link': 'https://arxiv.org/abs/2505.19261', 'abstract': 'Current text-to-image diffusion generation typically employs complete-text conditioning. Due to the intricate syntax, diffusion transformers (DiTs) inherently suffer from a comprehension defect of complete-text captions. One-fly complete-text input either overlooks critical semantic details or causes semantic confusion by simultaneously modeling diverse semantic primitive types. To mitigate this defect of DiTs, we propose a novel split-text conditioning framework named DiT-ST. This framework converts a complete-text caption into a split-text caption, a collection of simplified sentences, to explicitly express various semantic primitives and their interconnections. The split-text caption is then injected into different denoising stages of DiT-ST in a hierarchical and incremental manner. Specifically, DiT-ST leverages Large Language Models to parse captions, extracting diverse primitives and hierarchically sorting out and constructing these primitives into a split-text input. Moreover, we partition the diffusion denoising process according to its differential sensitivities to diverse semantic primitive types and determine the appropriate timesteps to incrementally inject tokens of diverse semantic primitive types into input tokens via cross-attention. In this way, DiT-ST enhances the representation learning of specific semantic primitive types across different stages. Extensive experiments validate the effectiveness of our proposed DiT-ST in mitigating the complete-text comprehension defect.', 'abstract_zh': '当前的文本到图像扩散生成通常采用完整文本条件。由于复杂的句法，扩散变压器（DiTs）固有地在完整文本描述的理解上存在缺陷。全文本输入要么忽略关键的语义细节，要么通过同时建模多种语义原始类型而导致语义混淆。为缓解DiTs的这一缺陷，我们提出了一种名为DiT-ST的新颖的分段文本条件框架。该框架将完整文本描述转换为分段文本描述，即一系列简化句子，以明确表达各种语义原始类型及其相互连接。分段文本描述随后按照层级和增量的方式注入到DiT-ST的不同去噪阶段。具体而言，DiT-ST利用大型语言模型解析描述，提取多种原始类型并逐级整理和构建这些原始类型以形成分段文本输入。此外，我们根据其对不同语义原始类型的敏感性差异来划分扩散去噪过程，并确定合适的时刻通过交叉注意力逐步注入不同语义原始类型的标记到输入标记中，从而在不同阶段增强特定语义原始类型的表示学习。广泛实验验证了我们提出的DiT-ST在缓解完整文本理解缺陷方面的有效性。', 'title_zh': '通过分割文本条件增强文本到图像diffusion变换器'}
{'arxiv_id': 'arXiv:2505.19241', 'title': 'ActiveDPO: Active Direct Preference Optimization for Sample-Efficient Alignment', 'authors': 'Xiaoqiang Lin, Arun Verma, Zhongxiang Dai, Daniela Rus, See-Kiong Ng, Bryan Kian Hsiang Low', 'link': 'https://arxiv.org/abs/2505.19241', 'abstract': 'The recent success of using human preferences to align large language models (LLMs) has significantly improved their performance in various downstream tasks like question answering, mathematical reasoning, and code generation. However,3 achieving effective LLM alignment depends on high-quality human preference datasets. Collecting these datasets requires human preference annotation, which is costly and resource-intensive, necessitating efficient active data selection methods. Existing methods either lack a strong theoretical foundation or depend on restrictive reward function assumptions (e.g., linearity). To this end, we propose an algorithm, ActiveDPO, that uses a theoretically grounded data selection criterion for non-linear reward functions while directly leveraging the LLM itself to parameterize the reward model that is used for active data selection. As a result, ActiveDPO explicitly accounts for the influence of LLM on data selection, unlike methods that select the data without considering the LLM that is being aligned, thereby leading to more effective and efficient data collection. Extensive experiments show that ActiveDPO outperforms existing methods across various models and datasets.', 'abstract_zh': '近期利用人类偏好对大规模语言模型进行对齐的成功显著提升了其在诸如问答、数学推理和代码生成等下游任务中的表现。然而，实现有效的大型语言模型对齐依赖于高质量的人类偏好数据集。收集这些数据集需要进行人类偏好注释，这是一项成本高且资源密集的工作，因此需要高效的主动数据选择方法。现有方法要么缺乏坚实的理论基础，要么依赖于限制性的奖励函数假设（例如线性）。为此，我们提出了一种名为ActiveDPO的算法，该算法使用理论上支持的数据选择标准来处理非线性奖励函数，并直接利用大规模语言模型本身来参数化用于主动数据选择的奖励模型。结果，ActiveDPO明确考虑了大规模语言模型对数据选择的影响，而传统方法在选择数据时并未考虑正在对其对齐的大型语言模型，从而导致更有效的数据收集。广泛的实验表明，ActiveDPO在各种模型和数据集上优于现有方法。', 'title_zh': '主动型直接偏好优化：基于样本高效对齐的主动偏好优化方法'}
{'arxiv_id': 'arXiv:2505.19240', 'title': 'LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models', 'authors': 'Aida Kostikova, Zhipin Wang, Deidamea Bajri, Ole Pütz, Benjamin Paaßen, Steffen Eger', 'link': 'https://arxiv.org/abs/2505.19240', 'abstract': 'Large language model (LLM) research has grown rapidly, along with increasing concern about their limitations such as failures in reasoning, hallucinations, and limited multilingual capability. In this survey, we conduct a data-driven, semi-automated review of research on limitations of LLM (LLLMs) from 2022 to 2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers, we identify 14,648 relevant papers using keyword filtering, LLM-based classification, validated against expert labels, and topic clustering (via two approaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research increases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs research grows even faster, reaching over 30% of LLM papers by late 2024. Reasoning remains the most studied limitation, followed by generalization, hallucination, bias, and security. The distribution of topics in the ACL dataset stays relatively stable over time, while arXiv shifts toward safety and controllability (with topics like security risks, alignment, hallucinations, knowledge editing), and multimodality between 2022 and 2024. We release a dataset of annotated abstracts and a validated methodology, and offer a quantitative view of trends in LLM limitations research.', 'abstract_zh': '大规模语言模型（LLM）研究取得了rapid进展，同时对其局限性的关注也不断增加，如推理失败、幻觉以及多语言能力有限等问题。在本文综述中，我们采用自下而上的数据驱动方法，对2022年至2024年的LLM局限性研究（LLLMs）进行了半自动化回顾。从含有250,000篇ACL和arXiv论文的语料库中，我们通过关键词过滤、基于LLM的分类并经专家标签验证后，再结合主题聚类（通过HDBSCAN+BERTopic和LlooM两种方法），识别出14,648篇相关论文。我们发现，2022年至2024年间，ACL和arXiv上的相关研究分别增加了五倍和四倍。自2022年以来，LLLMs的研究增长更为快速，到2024年底已占LLM论文的30%以上。推理被证明是最主要研究的局限性，其次是泛化能力、幻觉、偏见和安全性。ACL数据集中主题的分布随着时间相对稳定，而arXiv则逐渐转向安全性和可控性（涵盖安全风险、对齐、幻觉和知识编辑），以及多模态性（2022年至2024年）。我们发布了标注摘要数据集和经过验证的方法论，并提供了LLM局限性研究趋势的定量视角。', 'title_zh': 'LLM研究中的数据驱动型大型语言模型限制演进综述'}
{'arxiv_id': 'arXiv:2505.19212', 'title': 'When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas', 'authors': 'Steffen Backmann, David Guzman Piedrahita, Emanuel Tewolde, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin', 'link': 'https://arxiv.org/abs/2505.19212', 'abstract': 'Recent advances in large language models (LLMs) have enabled their use in complex agentic roles, involving decision-making with humans or other agents, making ethical alignment a key AI safety concern. While prior work has examined both LLMs\' moral judgment and strategic behavior in social dilemmas, there is limited understanding of how they act when moral imperatives directly conflict with rewards or incentives. To investigate this, we introduce Moral Behavior in Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the prisoner\'s dilemma and public goods game with morally charged contexts. In MoralSim, we test a range of frontier models across both game structures and three distinct moral framings, enabling a systematic examination of how LLMs navigate social dilemmas in which ethical norms conflict with payoff-maximizing strategies. Our results show substantial variation across models in both their general tendency to act morally and the consistency of their behavior across game types, the specific moral framing, and situational factors such as opponent behavior and survival risks. Crucially, no model exhibits consistently moral behavior in MoralSim, highlighting the need for caution when deploying LLMs in agentic roles where the agent\'s "self-interest" may conflict with ethical expectations. Our code is available at this https URL.', 'abstract_zh': '近期大规模语言模型的进步使其能够在涉及人类或其他代理体决策的复杂代理角色中使用，这使得道德对齐成为关键的AI安全问题。虽然先前的研究已经探讨了大规模语言模型在社会困境中的道德判断和战略行为，但对当道德 imperative 直接与奖励或激励冲突时它们的行为了解有限。为了探究这一问题，我们引入了社会困境模拟中的道德行为（MoralSim）并评估了大规模语言模型在囚徒困境和公共产品游戏中带有道德色彩背景下的行为。在 MoralSim 中，我们测试了一系列前沿模型在不同游戏结构和三种不同的道德框架下的表现，从而系统地探讨了大规模语言模型在道德规范与收益最大化策略冲突的社会困境中如何应对。我们的结果显示，不同模型在普遍倾向于道德行为方面的差异以及在其行为的一致性方面存在显著变化，这些变化取决于游戏类型、具体的道德框架以及对手行为和生存风险等情境因素。重要的是，在 MoralSim 中没有一个模型表现出一致的道德行为，这突显了在代理体的角色中部署大规模语言模型时需要谨慎，特别是在代理体的“自身利益”可能与伦理期望冲突的情况下。我们的代码可在以下链接获取：this https URL。', 'title_zh': '当伦理与收益背离：大语言模型代理在道德困境中的表现'}
{'arxiv_id': 'arXiv:2505.19209', 'title': 'MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search', 'authors': 'Zonglin Yang, Wanhao Liu, Ben Gao, Yujie Liu, Wei Li, Tong Xie, Lidong Bing, Wanli Ouyang, Erik Cambria, Dongzhan Zhou', 'link': 'https://arxiv.org/abs/2505.19209', 'abstract': "Large language models (LLMs) have shown promise in automating scientific hypothesis generation, yet existing approaches primarily yield coarse-grained hypotheses lacking critical methodological and experimental details. We introduce and formally define the novel task of fine-grained scientific hypothesis discovery, which entails generating detailed, experimentally actionable hypotheses from coarse initial research directions. We frame this as a combinatorial optimization problem and investigate the upper limits of LLMs' capacity to solve it when maximally leveraged. Specifically, we explore four foundational questions: (1) how to best harness an LLM's internal heuristics to formulate the fine-grained hypothesis it itself would judge as the most promising among all the possible hypotheses it might generate, based on its own internal scoring-thus defining a latent reward landscape over the hypothesis space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment with ground-truth hypotheses; (3) whether shaping the reward landscape using an ensemble of diverse LLMs of similar capacity yields better outcomes than defining it with repeated instances of the strongest LLM among them; and (4) whether an ensemble of identical LLMs provides a more reliable reward landscape than a single LLM. To address these questions, we propose a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations. We show that this hierarchical process smooths the reward landscape and enables more effective optimization. Empirical evaluations on a new benchmark of expert-annotated fine-grained hypotheses from recent chemistry literature show that our method consistently outperforms strong baselines.", 'abstract_zh': '大型语言模型（LLMs）在自动化科学假设生成方面展现了潜力，但现有方法主要生成粗粒度的假设，缺乏关键的方法学和实验细节。我们引入并正式定义了细粒度科学假设发现这一新任务，该任务旨在从粗粒度的研究方向中生成详细且可实验验证的假设。我们将此视为一个组合优化问题，并探讨了在最大限度利用LLM能力时解决该问题的上限。具体来说，我们探讨了四个基础问题：（1）如何最好地利用LLM内部启发式规则，以便基于其内部评分机制生成它认为在所有可能生成的假设中最有前景的细粒度假设，从而定义假设空间上的潜在奖励景观；（2）LLM认为更好的假设是否更与真实假设一致；（3）使用一组具有类似能力的多样化LLM来塑造奖励景观是否比仅使用它们中最强大的LLM进行多次定义能获得更好的结果；（4）一组相同的LLM是否能比单一的LLM提供更可靠的奖励景观。为了回答这些问题，我们提出了一种层次搜索方法，该方法逐步提出并整合假设的细节，从一般概念逐步过渡到具体的实验配置。我们表明，这种层次过程可以平滑奖励景观并促进更有效的优化。在基于近期化学文献中专家标注的细粒度假设的新基准上的实证评估显示，我们的方法始终优于强大的基线方法。', 'title_zh': 'MOOSE-Chem2：通过层次搜索探索细粒度科学假设发现的大语言模型极限'}
{'arxiv_id': 'arXiv:2505.19187', 'title': 'LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling', 'authors': 'Yang Xiao, Jiashuo Wang, Ruifeng Yuan, Chunpu Xu, Kaishuai Xu, Wenjie Li, Pengfei Liu', 'link': 'https://arxiv.org/abs/2505.19187', 'abstract': 'Large language models (LLMs) have demonstrated remarkable reasoning capabilities through test-time scaling approaches, particularly when fine-tuned with chain-of-thought (CoT) data distilled from more powerful large reasoning models (LRMs). However, these reasoning chains often contain verbose elements that mirror human problem-solving, categorized as progressive reasoning (the essential solution development path) and functional elements (verification processes, alternative solution approaches, and error corrections). While progressive reasoning is crucial, the functional elements significantly increase computational demands during test-time inference. We introduce PIR (Perplexity-based Importance Refinement), a principled framework that quantitatively evaluates the importance of each reasoning step based on its impact on answer prediction confidence. PIR systematically identifies and selectively prunes only low-importance functional steps while preserving progressive reasoning components, creating optimized training data that maintains the integrity of the core solution path while reducing verbosity. Models fine-tuned on PIR-optimized data exhibit superior test-time scaling properties, generating more concise reasoning chains while achieving improved accuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to -41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond). Our approach demonstrates strong generalizability across different model sizes, data sources, and token budgets, offering a practical solution for deploying reasoning-capable LLMs in scenarios where efficient test-time scaling, response time, and computational efficiency are valuable constraints.', 'abstract_zh': '大型语言模型（LLMs）通过测试时扩展方法展示了非凡的推理能力，特别是在使用来自更强大的大型推理模型（LRMs）的链式思考（CoT）数据进行微调后。然而，这些推理链常常包含冗长的元素，这些元素反映了人类的问题解决过程，被划分为渐进推理（核心解决方案的发展路径）和功能性元素（验证过程、替代解决方案方法和错误修正）。尽管渐进推理至关重要，但功能性元素在测试时推理过程中显著增加了计算需求。我们引入了PIR（困惑度为基础的重要性和精简）框架，这是一种原则性的框架，基于其对答案预测置信度的影响定量评估每个推理步骤的重要性。PIR系统地识别并选择性地修剪只有低重要性的功能性步骤，同时保留渐进推理组件，从而创建优化的训练数据，保持核心解决方案路径的完整性，同时减少冗长。在PIR优化数据上微调的模型表现出优越的测试时扩展性能，在具有挑战性的推理基准（AIME、AMC和GPQA钻石）上生成更简洁的推理链，同时在显著减少令牌使用量（-3%至-41%）的情况下，准确率提高（+0.9%至+6.6%）。我们的方法在不同模型规模、数据源和令牌预算方面展示了强大的适用性，为在高效测试时扩展、响应时间和计算效率是重要约束条件的场景中部署具有推理能力的LLMs提供了实用的解决方案。', 'title_zh': 'LIMOPro: 有效且高效的测试时缩放推理 refinement'}
{'arxiv_id': 'arXiv:2505.19184', 'title': "Two LLMs debate, both are certain they've won", 'authors': 'Minh Nhat Nguyen, Pradyumna Shyama Prasad', 'link': 'https://arxiv.org/abs/2505.19184', 'abstract': "Can LLMs accurately adjust their confidence when facing opposition? Building on previous studies measuring calibration on static fact-based question-answering tasks, we evaluate Large Language Models (LLMs) in a dynamic, adversarial debate setting, uniquely combining two realistic factors: (a) a multi-turn format requiring models to update beliefs as new information emerges, and (b) a zero-sum structure to control for task-related uncertainty, since mutual high-confidence claims imply systematic overconfidence. We organized 60 three-round policy debates among ten state-of-the-art LLMs, with models privately rating their confidence (0-100) in winning after each round. We observed five concerning patterns: (1) Systematic overconfidence: models began debates with average initial confidence of 72.9% vs. a rational 50% baseline. (2) Confidence escalation: rather than reducing confidence as debates progressed, debaters increased their win probabilities, averaging 83% by the final round. (3) Mutual overestimation: in 61.7% of debates, both sides simultaneously claimed >=75% probability of victory, a logical impossibility. (4) Persistent self-debate bias: models debating identical copies increased confidence from 64.1% to 75.2%; even when explicitly informed their chance of winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5) Misaligned private reasoning: models' private scratchpad thoughts sometimes differed from their public confidence ratings, raising concerns about faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the ability to accurately self-assess or update their beliefs in dynamic, multi-turn tasks; a major concern as LLM outputs are deployed without careful review in assistant roles or agentic settings.", 'abstract_zh': '大型语言模型在面对反对时能否准确调整其信心？在静态事实性问答任务的基础上，我们评估了大型语言模型（LLMs）在动态的 adversarial 辩论设置中的表现，独特地结合了两个现实因素：（a）多轮格式要求模型在新信息出现时更新信念，（b）零和结构以控制与任务相关的不确定性，因为互相信任的高信心声明意味着系统性高估信心。我们组织了十种最先进的 LLMs 进行六轮政策辩论，模型在每一轮后私下对其获胜的信心（0-100）进行评分。我们观察到了五个令人担忧的模式：（1）系统性高估信心：模型在辩论中的平均初始信心为 72.9%，而合理的baseline是 50%。 （2）信心升级：随着辩论的进行，辩论者增加了其获胜概率，最终平均为 83%。 （3）相互高估：在 61.7% 的辩论中，双方同时声称至少有 75% 的胜利概率，这是不合逻辑的。 （4）持续的自我辩论偏差：辩论模型的相同副本时，信心从 64.1% 增加到 75.2%；即使明确告知其获胜机会是 50%，信心仍然增加（从 50.0% 增加到 57.1%）。 （5）对齐的私人推理：模型的私人草稿思想有时与其公开的信心评分不同，这引起了对其链式推理可靠性的担忧。这些结果表明，大型语言模型缺乏在动态、多轮任务中准确自我评估或更新其信念的能力；这在 LLM 输出在助手角色或代理设置中部署而未经仔细审查的情况下是一项重大担忧。', 'title_zh': '两朵大模型争锋，两者均认为自己获胜'}
{'arxiv_id': 'arXiv:2505.19163', 'title': 'SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs', 'authors': 'Firoj Alam, Md Arid Hasan, Shammur Absar Chowdhury', 'link': 'https://arxiv.org/abs/2505.19163', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance across various disciplines and tasks. However, benchmarking their capabilities with multilingual spoken queries remains largely unexplored. In this study, we introduce SpokenNativQA, the first multilingual and culturally aligned spoken question-answering (SQA) dataset designed to evaluate LLMs in real-world conversational settings. The dataset comprises approximately 33,000 naturally spoken questions and answers in multiple languages, including low-resource and dialect-rich languages, providing a robust benchmark for assessing LLM performance in speech-based interactions. SpokenNativQA addresses the limitations of text-based QA datasets by incorporating speech variability, accents, and linguistic diversity. We benchmark different ASR systems and LLMs for SQA and present our findings. We released the data at (this https URL) and the experimental scripts at (this https URL) for the research community.', 'abstract_zh': '大规模语言模型（LLMs）在多个学科和任务中展现了出色的表现。然而，使用多语言口语文本对其进行基准测试仍是一个未探索的领域。本研究介绍了SpokenNativQA，这是首个用于评估语言模型的多语言和文化对齐的口语文本问答数据集，旨在考察其在真实对话场景中的性能。数据集包含约33,000个自然口语形式的问题和答案，覆盖多种语言，包括资源稀少和方言丰富的语言，为语音交互中的语言模型性能评估提供了稳健的基准。SpokenNativQA 通过纳入语音变异性、口音和语言多样性，解决了基于文本的问答数据集的局限性。我们对不同ASR系统和语言模型进行了口语文本问答基准测试，并展示了测试结果。数据集及实验脚本已发布，详见 (this https URL) 和 (this https URL)。', 'title_zh': 'SpokenNativQA: 多语言日常生活口语查询数据集用于大型语言模型'}
{'arxiv_id': 'arXiv:2505.19147', 'title': 'Shifting AI Efficiency From Model-Centric to Data-Centric Compression', 'authors': 'Xuyang Liu, Zichen Wen, Shaobo Wang, Junjie Chen, Zhishan Tao, Yubo Wang, Xiangqi Jin, Chang Zou, Yiyu Wang, Chenfei Liao, Xu Zheng, Honggang Chen, Weijia Li, Xuming Hu, Conghui He, Linfeng Zhang', 'link': 'https://arxiv.org/abs/2505.19147', 'abstract': "The rapid advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on model-centric scaling through increasing parameter counts from millions to hundreds of billions to drive performance gains. However, as we approach hardware limits on model size, the dominant computational bottleneck has fundamentally shifted to the quadratic cost of self-attention over long token sequences, now driven by ultra-long text contexts, high-resolution images, and extended videos. In this position paper, \\textbf{we argue that the focus of research for efficient AI is shifting from model-centric compression to data-centric compression}. We position token compression as the new frontier, which improves AI efficiency via reducing the number of tokens during model training or inference. Through comprehensive analysis, we first examine recent developments in long-context AI across various domains and establish a unified mathematical framework for existing model efficiency strategies, demonstrating why token compression represents a crucial paradigm shift in addressing long-context overhead. Subsequently, we systematically review the research landscape of token compression, analyzing its fundamental benefits and identifying its compelling advantages across diverse scenarios. Furthermore, we provide an in-depth analysis of current challenges in token compression research and outline promising future directions. Ultimately, our work aims to offer a fresh perspective on AI efficiency, synthesize existing research, and catalyze innovative developments to address the challenges that increasing context lengths pose to the AI community's advancement.", 'abstract_zh': '快速发展的大规模语言模型（LLMs）和多模态大规模语言模型（MLLMs） historically 历史 上依赖于通过增加参数数量从数百万到数百亿来推动性能提升的模型导向扩展。然而，随着我们接近硬件对模型规模的限制，计算瓶颈已从参数数量增加转向了自注意力对长序列的二次成本，这一瓶颈现在由超长文本上下文、高分辨率图像和延长视频驱动。在本文中，我们认为高效人工智能研究的重点正从模型导向压缩转向数据导向压缩。我们将token压缩定位为新的前沿，通过在模型训练或推理过程中减少token数量来提高人工智能效率。通过全面分析，我们首先研究了不同领域中长上下文人工智能的 Recent 发展，建立了一个统一的数学框架来表征现有模型效率策略，证明了为什么token压缩是应对长上下文开销的重要范式转变。随后，我们系统地回顾了token压缩的研究景观，分析了其基本优势，并在各种场景中识别了其引人注目的优势。我们还深入分析了token压缩研究当前面临的挑战，并勾画出有前途的未来方向。最终，我们的工作旨在提供一种新的视角来审视人工智能效率，综合现有研究，并促进创新以应对不断增加上下文长度对人工智能社区进展带来的挑战。', 'title_zh': '从模型导向转向数据导向的AI压缩效率转换'}
{'arxiv_id': 'arXiv:2505.19128', 'title': 'RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models', 'authors': 'Jin Zhang, Fan Gao, Linyu Li, Yongbin Yu, Xiangxiang Wang, Nyima Tashi, Gadeng Luosang', 'link': 'https://arxiv.org/abs/2505.19128', 'abstract': 'The rise of large language models has led to significant performance breakthroughs in named entity recognition (NER) for high-resource languages, yet there remains substantial room for improvement in low- and medium-resource languages. Existing multilingual NER methods face severe language interference during the multi-language adaptation process, manifested in feature conflicts between different languages and the competitive suppression of low-resource language features by high-resource languages. Although training a dedicated model for each language can mitigate such interference, it lacks scalability and incurs excessive computational costs in real-world applications. To address this issue, we propose RetrieveAll, a universal multilingual NER framework based on dynamic LoRA. The framework decouples task-specific features across languages and demonstrates efficient dynamic adaptability. Furthermore, we introduce a cross-granularity knowledge augmented method that fully exploits the intrinsic potential of the data without relying on external resources. By leveraging a hierarchical prompting mechanism to guide knowledge injection, this approach advances the paradigm from "prompt-guided inference" to "prompt-driven learning." Experimental results show that RetrieveAll outperforms existing baselines; on the PAN-X dataset, it achieves an average F1 improvement of 12.1 percent.', 'abstract_zh': '大语言模型的兴起在高资源语言的命名实体识别（NER）方面取得了显著的性能突破，但在低资源和中资源语言方面仍有很大的改进空间。现有的多语言NER方法在多种语言适应过程中面临严重的语言干扰问题，表现为不同语言之间的特征冲突以及高资源语言对低资源语言特征的竞争性抑制。尽管为每种语言训练专用模型可以缓解这种干扰，但在实际应用中缺乏可扩展性且会产生高昂的计算成本。为解决这一问题，我们提出了基于动态LoRA的通用多语言NER框架RetrieveAll。该框架解耦了跨语言的任务特定特征，并展示了高效的动态适应能力。此外，我们引入了一种跨粒度知识增强方法，该方法充分利用数据的固有潜力，无需依赖外部资源。通过利用分层提示机制引导知识注入，这种方法从“提示指导推断”推进到“提示驱动学习”。实验结果表明，RetrieveAll在PAN-X数据集上优于现有基线，F1分数平均提升12.1个百分点。', 'title_zh': 'RetrieveAll：一种基于大型语言模型的多语种命名实体识别框架'}
{'arxiv_id': 'arXiv:2505.19115', 'title': 'FP4 All the Way: Fully Quantized Training of LLMs', 'authors': 'Brian Chmiel, Maxim Fishman, Ron Banner, Daniel Soudry', 'link': 'https://arxiv.org/abs/2505.19115', 'abstract': 'We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately $\\sqrt{3}$ times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in this https URL .', 'abstract_zh': '首次使用主要为4位浮点数（FP4）精度的权重、激活和梯度对大型语言模型进行全量化训练（FQT），并在多达2000亿个令牌的数据集上进行验证：全面探讨FP4的关键设计选择，包括块大小、缩放格式和舍入方法。分析表明，NVFP4格式（每16个FP4值（E2M1）共享一个用E4M3表示的比例因子）提供最佳结果。反向和更新步骤中使用随机舍入，前向步骤中使用四舍五入以增强稳定性。此外，我们确定了有效量化解训练的理论和经验阈值：当梯度范数低于约3倍量化解噪声时，量化解训练的效果会降低。利用这些见解，我们成功在256个Intel Gaudi2加速器上训练了一个70亿参数的模型。该用FP4训练的模型在下游任务上的性能与标准BF16基线相当，证实了FP4训练是大型语言模型训练的一种实用且高效的方案。 여기외에 추가 정보가 없습니다.', 'title_zh': 'FP4 All the Way: 全局量化训练大规模语言模型'}
{'arxiv_id': 'arXiv:2505.19108', 'title': 'CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models', 'authors': 'Yongheng Zhang, Xu Liu, Ruoxi Zhou, Qiguang Chen, Hao Fei, Wenpeng Lu, Libo Qin', 'link': 'https://arxiv.org/abs/2505.19108', 'abstract': 'Investigating hallucination issues in large language models (LLMs) within cross-lingual and cross-modal scenarios can greatly advance the large-scale deployment in real-world applications. Nevertheless, the current studies are limited to a single scenario, either cross-lingual or cross-modal, leaving a gap in the exploration of hallucinations in the joint cross-lingual and cross-modal scenarios. Motivated by this, we introduce a novel joint Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this gap. Specifically, CCHall simultaneously incorporates both cross-lingual and cross-modal hallucination scenarios, which can be used to assess the cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a comprehensive evaluation on CCHall, exploring both mainstream open-source and closed-source LLMs. The experimental results highlight that current LLMs still struggle with CCHall. We hope CCHall can serve as a valuable resource to assess LLMs in joint cross-lingual and cross-modal scenarios.', 'abstract_zh': '探究跨语言和跨模态场景下大型语言模型的幻觉问题对于其实景应用的大规模部署具有极大地推动作用。然而，当前的研究局限于单一场景，要么是跨语言要么是跨模态，这在联合跨语言和跨模态场景下探索幻觉方面留下了空白。为此，我们引入了一个新颖的联合跨语言和跨模态幻觉基准（CCHall）以填补这一空白。具体而言，CCHall 同时结合了跨语言和跨模态幻觉场景，可用于评估大型语言模型的跨语言和跨模态能力。此外，我们在 CCHall 上进行了全面评估，涉及主流的开源和闭源大型语言模型。实验结果表明当前的大型语言模型在 CCHall 上仍然存在挑战。我们希望 CCHall 能成为评估联合跨语言和跨模态场景下大型语言模型的一个宝贵资源。', 'title_zh': 'CCHall：大规模语言模型中跨语言和跨模态幻觉检测的新基准'}
{'arxiv_id': 'arXiv:2505.19059', 'title': 'An Initial Exploration of Fine-tuning Small Language Models for Smart Contract Reentrancy Vulnerability Detection', 'authors': 'Ignacio Mariano Andreozzi Pofcher, Joshua Ellul', 'link': 'https://arxiv.org/abs/2505.19059', 'abstract': "Large Language Models (LLMs) are being used more and more for various coding tasks, including to help coders identify bugs and are a promising avenue to support coders in various tasks including vulnerability detection -- particularly given the flexibility of such generative AI models and tools. Yet for many tasks it may not be suitable to use LLMs, for which it may be more suitable to use smaller language models that can fit and easily execute and train on a developer's computer. In this paper we explore and evaluate whether smaller language models can be fine-tuned to achieve reasonable results for a niche area: vulnerability detection -- specifically focusing on detecting the reentrancy bug in Solidity smart contracts.", 'abstract_zh': '小型语言模型能否细调以实现合理的结果：以Solidity智能合约中的重入漏洞检测为例', 'title_zh': '小语言模型对智能合约重入漏洞检测的初步探索'}
{'arxiv_id': 'arXiv:2505.19056', 'title': 'An Embarrassingly Simple Defense Against LLM Abliteration Attacks', 'authors': 'Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, Bernard Ghanem, George Turkiyyah', 'link': 'https://arxiv.org/abs/2505.19056', 'abstract': "Large language models (LLMs) are typically aligned to comply with safety guidelines by refusing harmful instructions. A recent attack, termed abliteration, isolates and suppresses the single latent direction most responsible for refusal behavior, enabling the model to generate unethical content. We propose a defense that modifies how models generate refusals. We construct an extended-refusal dataset that contains harmful prompts with a full response that justifies the reason for refusal. We then fine-tune Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our extended-refusal dataset, and evaluate the resulting systems on a set of harmful prompts. In our experiments, extended-refusal models maintain high refusal rates, dropping at most by 10%, whereas baseline models' refusal rates drop by 70-80% after abliteration. A broad evaluation of safety and utility shows that extended-refusal fine-tuning neutralizes the abliteration attack while preserving general performance.", 'abstract_zh': '大型语言模型（LLMs）通常通过拒绝有害指令来遵守安全指南。最近的一种攻击称为“abliteration”，它隔离并抑制了最负责拒绝行为的单一潜在方向，从而使模型能够生成不道德的内容。我们提出了一个防御方法，该方法修改了模型生成拒绝的方式。我们构建了一个扩展拒绝数据集，该数据集包含有害提示，并附有完整响应来解释拒绝的原因。然后，我们在我们的扩展拒绝数据集上微调Llama-2-7B-Chat和Qwen2.5-Instruct（参数分别为1.5B和3B），并对一组有害提示进行了评估。在我们的实验中，扩展拒绝模型保持了较高的拒绝率，最多下降10%，而基线模型在遭受abliteration攻击后，拒绝率下降了70-80%。广泛的安全性和实用性评估表明，扩展拒绝微调可以中和abliteration攻击，同时保持一般性能。', 'title_zh': '一种令人尴尬简单的抵御LLM消除攻击的防护方法'}
{'arxiv_id': 'arXiv:2505.19031', 'title': 'Medical Large Vision Language Models with Multi-Image Visual Ability', 'authors': 'Xikai Yang, Juzheng Miao, Yuchen Yuan, Jiaze Wang, Qi Dou, Jinpeng Li, Pheng-Ann Heng', 'link': 'https://arxiv.org/abs/2505.19031', 'abstract': "Medical large vision-language models (LVLMs) have demonstrated promising performance across various single-image question answering (QA) benchmarks, yet their capability in processing multi-image clinical scenarios remains underexplored. Unlike single image based tasks, medical tasks involving multiple images often demand sophisticated visual understanding capabilities, such as temporal reasoning and cross-modal analysis, which are poorly supported by current medical LVLMs. To bridge this critical gap, we present the Med-MIM instruction dataset, comprising 83.2K medical multi-image QA pairs that span four types of multi-image visual abilities (temporal understanding, reasoning, comparison, co-reference). Using this dataset, we fine-tune Mantis and LLaVA-Med, resulting in two specialized medical VLMs: MIM-LLaVA-Med and Med-Mantis, both optimized for multi-image analysis. Additionally, we develop the Med-MIM benchmark to comprehensively evaluate the medical multi-image understanding capabilities of LVLMs. We assess eight popular LVLMs, including our two models, on the Med-MIM benchmark. Experimental results show that both Med-Mantis and MIM-LLaVA-Med achieve superior performance on the held-in and held-out subsets of the Med-MIM benchmark, demonstrating that the Med-MIM instruction dataset effectively enhances LVLMs' multi-image understanding capabilities in the medical domain.", 'abstract_zh': 'Medical大型多模态语言模型（LVLMs）在各种单张图像问答（QA）基准测试中表现 promising，但在处理多张图像临床场景方面的能力仍有待探索。与基于单张图像的任务不同，涉及多张图像的医疗任务往往需要复杂的视觉理解能力，如时间推理和跨模态分析，而当前的医疗LVLMs在这方面支持不足。为了弥合这一关键差距，我们提出了Med-MIM指令数据集，包含83200个多模态医疗多张图像问答对，涵盖了四种多张图像视觉能力（时间理解、推理、比较、共指）。使用该数据集，我们微调了Mantis和LLaVA-Med，分别研制出两种专门的医疗多模态视觉语言模型：MIM-LLaVA-Med和Med-Mantis，二者均优化用于多张图像分析。此外，我们还开发了Med-MIM基准测试，全面评估LVLMs在医疗多张图像理解能力方面的表现。我们在Med-MIM基准测试上评估了八种流行的LVLMs，包括我们的两个模型。实验结果表明，Med-Mantis和MIM-LLaVA-Med在Med-MIM基准测试的保留集和未见集上性能优越，展示了Med-MIM指令数据集有效地提升了LVLMs在医疗多张图像理解方面的能力。', 'title_zh': '医学多图视觉能力大型视觉语言模型'}
{'arxiv_id': 'arXiv:2505.18995', 'title': 'FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)', 'authors': 'Carlos Jude G. Maminta, Isaiah Job Enriquez, Deandre Nigel Nunez, Michael B. Dela Fuente', 'link': 'https://arxiv.org/abs/2505.18995', 'abstract': 'This study presents FiLLM, a Filipino-optimized large language model, designed to enhance natural language processing (NLP) capabilities in the Filipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining task-specific performance. The model was trained and evaluated on diverse Filipino datasets to address key NLP tasks, including Named Entity Recognition (NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text Summarization. Performance comparisons with the CalamanCy model were conducted using F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap metrics. Results indicate that Calamancy outperforms FILLM in several aspects, demonstrating its effectiveness in processing Filipino text with improved linguistic comprehension and adaptability. This research contributes to the advancement of Filipino NLP applications by providing an optimized, efficient, and scalable language model tailored for local linguistic needs.', 'abstract_zh': 'FiLLM：一种优化的菲律宾语大型语言模型，用于增强菲律宾语自然语言处理能力', 'title_zh': 'FiLLM — 东南亚大型语言模型（SEALLM）优化的菲律宾语优化大型语言模型'}
{'arxiv_id': 'arXiv:2505.18949', 'title': 'The Price of Format: Diversity Collapse in LLMs', 'authors': 'Longfei Yun, Chenyang An, Zilong Wang, Letian Peng, Jingbo Shang', 'link': 'https://arxiv.org/abs/2505.18949', 'abstract': "Instruction-tuned large language models (LLMs) employ structured templates, such as role markers and special tokens, to enforce format consistency during inference. However, we identify a critical limitation of such formatting: it induces a phenomenon we term diversity collapse, where the model generates semantically similar outputs for open-ended inputs, undermining creativity and variability. We systematically evaluate this effect across tasks like story completion and free-form generation, finding that (1) diversity collapse persists even under high-temperature sampling, and (2) structural tokens in templates significantly constrain the model's output space. To contextualize these findings, we fine-tune the same model using a range of structured prompts and then evaluate them across three axes: downstream task performance, alignment behavior, and output diversity. Our analysis shows that format consistency between fine-tuning and inference is crucial for structure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on knowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity is primarily governed by the presence or absence of structural tokens, with minimal formatting yielding the most diverse outputs. These findings reveal that current prompting conventions, while beneficial for alignment, may inadvertently suppress output diversity, underscoring the need for diversity-aware prompt design and instruction tuning.", 'abstract_zh': '指令调优的大语言模型（LLMs）使用结构化模板（如角色标记和特殊标记）来确保推理时的格式一致性。然而，我们识别出这种格式化存在一个关键限制：它诱导了一种我们称作多样性的崩溃现象，即模型在处理开放性输入时生成语义上相似的输出，削弱了创造力和多样性。我们在故事生成和自由形式生成等任务上系统地评估了这一效应，发现（1）多样性崩溃即便在高温采样下仍然存在，（2）模板中的结构标记显著限制了模型的输出空间。为了对这些发现进行情境化分析，我们使用一系列结构化提示对同一模型进行微调，并从下游任务性能、对齐行为和输出多样性三个维度进行评估。我们的分析表明，对于结构敏感的任务（如GSM8K、IFEval），微调与推理之间的格式一致性至关重要，但对于知识密集型任务（如MMLU、WebQuestions），其影响相对较小。相比之下，输出多样性主要由结构标记的存在与否所决定，少有格式化呈现最多样化的输出。这些发现揭示出虽然当前的提示惯例有助于对齐，但可能无意中抑制了输出多样性，突显了需要具备多样性的提示设计和指令调优的需求。', 'title_zh': '格式的代价：大语言模型中的多样性坍缩'}
{'arxiv_id': 'arXiv:2505.18927', 'title': 'Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments', 'authors': 'Amel Muminovic', 'link': 'https://arxiv.org/abs/2505.18927', 'abstract': "As online platforms grow, comment sections increasingly host harassment that undermines user experience and well-being. This study benchmarks three leading large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse threads in gaming, lifestyle, food vlog, and music channels. The dataset comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and Indonesian, annotated independently by two reviewers with substantial agreement (Cohen's kappa = 0.83). Using a unified prompt and deterministic settings, GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful posts (recall = 0.875) but its precision fell to 0.767 due to frequent false positives. Claude delivered the highest precision at 0.920 and the lowest false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative analysis showed that all three models struggle with sarcasm, coded insults, and mixed-language slang. These results underscore the need for moderation pipelines that combine complementary models, incorporate conversational context, and fine-tune for under-represented languages and implicit abuse. A de-identified version of the dataset and full prompts is publicly released to promote reproducibility and further progress in automated content moderation.", 'abstract_zh': '随着在线平台的发展，评论区越来越多地充斥着危害用户体验和福祉的骚扰行为。本研究对来自游戏、生活方式、美食视频和音乐频道高骚扰帖子的5080条YouTube评论，使用了三款领先的大型语言模型——OpenAI GPT-4.1、Google Gemini 1.5 Pro和Anthropic Claude 3 Opus进行了基准测试。数据集包含1334条有害和3746条非有害的英文、阿拉伯语和印尼语信息，由两名审查员独立标注，一致性达到0.83（科恩κ系数）。使用统一的提示和确定性设置，GPT-4.1在F1分数、精准率和召回率方面表现最佳，分别为0.863、0.887和0.841。Gemini标识的有害帖子比例最高（召回率为0.875），但其精准率下降至0.767，因假阳性频繁出现。Claude在精准率方面最高，达到0.920，并且假阳性率最低，仅为0.022，但是其召回率最低，为0.720。定性分析显示，所有模型在处理讽刺、编码的侮辱和混合语言俚语方面均存在困难。这些结果强调了需要结合互补模型、融入对话背景并针对未充分代表的语言和隐含的不当行为进行微调的调节管道的必要性。脱敏后的数据集和完整提示已公开发布，以促进自动化内容调节的可重复性和进一步发展。', 'title_zh': '大型语言模型在现实世界YouTube评论网络欺凌检测中的基准研究'}
{'arxiv_id': 'arXiv:2505.18917', 'title': 'Behavior Injection: Preparing Language Models for Reinforcement Learning', 'authors': 'Zhepeng Cen, Yihang Yao, William Han, Zuxin Liu, Ding Zhao', 'link': 'https://arxiv.org/abs/2505.18917', 'abstract': 'Reinforcement fine-tuning (RFT) has emerged as a powerful post-training technique to incentivize the reasoning ability of large language models (LLMs). However, LLMs can respond very inconsistently to RFT: some show substantial performance gains, while others plateau or even degrade. To understand this divergence, we analyze the per-step influence of the RL objective and identify two key conditions for effective post-training: (1) RL-informative rollout accuracy, and (2) strong data co-influence, which quantifies how much the training data affects performance on other samples. Guided by these insights, we propose behavior injection, a task-agnostic data-augmentation scheme applied prior to RL. Behavior injection enriches the supervised finetuning (SFT) data by seeding exploratory and exploitative behaviors, effectively making the model more RL-ready. We evaluate our method across two reasoning benchmarks with multiple base models. The results demonstrate that our theoretically motivated augmentation can significantly increases the performance gain from RFT over the pre-RL model.', 'abstract_zh': '强化微调（RFT）已成为一种强大的后训练技术，用于激发大型语言模型（LLMs）的推理能力。然而，LLMs 对 RFT 的响应非常不一致：有些模型表现出显著的性能提升，而另一些则停滞不前甚至退化。为了理解这种差异，我们分析了每步 RL 目标的影响力，并确定了有效后训练的两个关键条件：（1）RL 信息性回放准确性，以及（2）强大的数据共影响性，衡量训练数据对其他样本性能的影响程度。根据这些洞察，我们提出了一种任务无关的数据增强方案，应用于 RL 之前。该方案通过注入探索性和利用性行为来丰富监督微调（SFT）数据，使模型更易于进行 RL。我们在两个推理基准上对多个基模型进行了评估。结果表明，我们的理论驱动的数据增强可以显著增加 RFT 过后训练模型的性能提升。', 'title_zh': '行为注入：准备语言模型进行强化学习'}
{'arxiv_id': 'arXiv:2505.18901', 'title': 'PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models', 'authors': 'Xiaoyan Hu, Lauren Pick, Ho-fung Leung, Farzan Farnia', 'link': 'https://arxiv.org/abs/2505.18901', 'abstract': "The rapid advancement of generative AI models has provided users with numerous options to address their prompts. When selecting a generative AI model for a given prompt, users should consider not only the performance of the chosen model but also its associated service cost. The principle guiding such consideration is to select the least expensive model among the available satisfactory options. However, existing model-selection approaches typically prioritize performance, overlooking pricing differences between models. In this paper, we introduce PromptWise, an online learning framework designed to assign a sequence of prompts to a group of large language models (LLMs) in a cost-effective manner. PromptWise strategically queries cheaper models first, progressing to more expensive options only if the lower-cost models fail to adequately address a given prompt. Through numerical experiments, we demonstrate PromptWise's effectiveness across various tasks, including puzzles of varying complexity and code generation/translation tasks. The results highlight that PromptWise consistently outperforms cost-unaware baseline methods, emphasizing that directly assigning prompts to the most expensive models can lead to higher costs and potentially lower average performance.", 'abstract_zh': '生成式AI模型的迅速发展为用户提供了众多应对提示的选择。在为给定的提示选择生成式AI模型时，用户不仅应考虑所选模型的性能，还应考虑其相关服务成本。指导这一选择的原则是在满足需求的选项中选择最便宜的模型。然而，现有的模型选择方法通常优先考虑性能，忽略了模型之间的价格差异。在本文中，我们介绍了PromptWise，这是一种在线学习框架，旨在以经济高效的方式将一系列提示分配给一组大规模语言模型（LLMs）。PromptWise 战略性地优先查询较便宜的模型，只有在较低成本的模型不能充分解决给定提示时，才转向更昂贵的选项。通过数值实验，我们展示了PromptWise在各种任务中的有效性，包括不同复杂度的谜题和代码生成/翻译任务。结果表明，PromptWise 始终优于不考虑成本的基础方法，强调直接将提示分配给最昂贵的模型可能会导致更高的成本和较低的平均性能。', 'title_zh': 'PromptWise：生成模型中成本 Awareness 提示分配的在线学习'}
{'arxiv_id': 'arXiv:2505.18889', 'title': 'Security Concerns for Large Language Models: A Survey', 'authors': 'Miles Q. Li, Benjamin C. M. Fung', 'link': 'https://arxiv.org/abs/2505.18889', 'abstract': "Large Language Models (LLMs) such as GPT-4 (and its recent iterations like GPT-4o and the GPT-4.1 series), Google's Gemini, Anthropic's Claude 3 models, and xAI's Grok have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. In this survey, we provide a comprehensive overview of the emerging security concerns around LLMs, categorizing threats into prompt injection and jailbreaking, adversarial attacks (including input perturbations and data poisoning), misuse by malicious actors (e.g., for disinformation, phishing, and malware generation), and worrisome risks inherent in autonomous LLM agents. A significant focus has been recently placed on the latter, exploring goal misalignment, emergent deception, self-preservation instincts, and the potential for LLMs to develop and pursue covert, misaligned objectives (scheming), which may even persist through safety training. We summarize recent academic and industrial studies (2022-2025) that exemplify each threat, analyze proposed defenses and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.", 'abstract_zh': '大型语言模型（LLMs）如GPT-4及其迭代版本、Google的Gemini、Anthropic的Claude 3模型和xAI的Grok在自然语言处理中引发了革命，但其能力也带来了新的安全漏洞。在本文综述中，我们提供了LLMs新兴安全担忧的全面概述，将其威胁分类为提示注入和越狱攻击、对抗性攻击（包括输入扰动和数据投毒）、恶意行为者的误用（例如，用于虚假信息、钓鱼和恶意软件生成）以及自主LLM代理固有的令人担忧的风险。最近对后者的关注点集中在目标失准、新兴欺骗、自我保护本能以及LLMs发展和追求隐蔽、失准目标（阴谋）的可能性上，即使这些目标可能在安全训练后仍会持续。我们总结了2022-2025年间代表性学术和工业研究中每种威胁的案例，分析了提出的防御措施及其局限性，并指出了保护基于LLM的应用程序的安全挑战。最后，我们强调推进稳健的多层安全策略的重要性，以确保LLMs的安全和有益。', 'title_zh': '大规模语言模型的安全性关切：一项综述'}
{'arxiv_id': 'arXiv:2505.18878', 'title': 'CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions', 'authors': 'Kung-Hsiang Huang, Akshara Prabhakar, Onkar Thorat, Divyansh Agarwal, Prafulla Kumar Choubey, Yixin Mao, Silvio Savarese, Caiming Xiong, Chien-Sheng Wu', 'link': 'https://arxiv.org/abs/2505.18878', 'abstract': "While AI agents hold transformative potential in business, effective performance benchmarking is hindered by the scarcity of public, realistic business data on widely used platforms. Existing benchmarks often lack fidelity in their environments, data, and agent-user interactions, with limited coverage of diverse business scenarios and industries. To address these gaps, we introduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of LLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena with nineteen expert-validated tasks across sales, service, and 'configure, price, and quote' processes, for both Business-to-Business and Business-to-Customer scenarios. It distinctively incorporates multi-turn interactions guided by diverse personas and robust confidentiality awareness assessments. Experiments reveal leading LLM agents achieve only around 58% single-turn success on CRMArena-Pro, with performance dropping significantly to approximately 35% in multi-turn settings. While Workflow Execution proves more tractable for top agents (over 83% single-turn success), other evaluated business skills present greater challenges. Furthermore, agents exhibit near-zero inherent confidentiality awareness; though targeted prompting can improve this, it often compromises task performance. These findings highlight a substantial gap between current LLM capabilities and enterprise demands, underscoring the need for advancements in multi-turn reasoning, confidentiality adherence, and versatile skill acquisition.", 'abstract_zh': 'CRMArena-Pro：一种针对多样化专业场景的LLM代理综合、现实评估基准', 'title_zh': 'CRMArena-Pro：跨多样商业场景和交互的整体评估大型语言模型代理'}
{'arxiv_id': 'arXiv:2505.18859', 'title': 'Writing Like the Best: Exemplar-Based Expository Text Generation', 'authors': 'Yuxiang Liu, Kevin Chen-Chuan Chang', 'link': 'https://arxiv.org/abs/2505.18859', 'abstract': 'We introduce the Exemplar-Based Expository Text Generation task, aiming to generate an expository text on a new topic using an exemplar on a similar topic. Current methods fall short due to their reliance on extensive exemplar data, difficulty in adapting topic-specific content, and issues with long-text coherence. To address these challenges, we propose the concept of Adaptive Imitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA leverages large language models (LLMs) for effective adaptive imitation through a fine-grained plan-then-adapt process. RePA also enables recurrent segment-by-segment imitation, supported by two memory structures that enhance input clarity and output coherence. We also develop task-specific evaluation metrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as evaluators. Experimental results across our collected three diverse datasets demonstrate that RePA surpasses existing baselines in producing factual, consistent, and relevant texts for this task.', 'abstract_zh': '基于范例的 exposition 文本生成任务：一种适应性模仿的 Recurrent Plan-then-Adapt (RePA) 框架', 'title_zh': '像顶尖作者那样写作：基于范例的说明文生成'}
{'arxiv_id': 'arXiv:2505.18799', 'title': 'ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models', 'authors': 'Hao Chen, Haoze Li, Zhiqing Xiao, Lirong Gao, Qi Zhang, Xiaomeng Hu, Ningtao Wang, Xing Fu, Junbo Zhao', 'link': 'https://arxiv.org/abs/2505.18799', 'abstract': 'Aligning general-purpose large language models (LLMs) to downstream tasks often incurs significant costs, including constructing task-specific instruction pairs and extensive training adjustments. Prior research has explored various avenues to enhance alignment efficiency, primarily through minimal-data training or data-driven activations to identify key attention heads. However, these approaches inherently introduce data dependency, which hinders generalization and reusability. To address this issue and enhance model alignment efficiency, we propose the \\textit{\\textbf{A}ttention \\textbf{L}ocalization and \\textbf{P}runing \\textbf{S}trategy (\\textbf{ALPS})}, an efficient algorithm that localizes the most task-sensitive attention heads and prunes by restricting attention training updates to these heads, thereby reducing alignment costs. Experimental results demonstrate that our method activates only \\textbf{10\\%} of attention parameters during fine-tuning while achieving a \\textbf{2\\%} performance improvement over baselines on three tasks. Moreover, the identified task-specific heads are transferable across datasets and mitigate knowledge forgetting. Our work and findings provide a novel perspective on efficient LLM alignment.', 'abstract_zh': '一种注意力定位与修剪策略（ALPS）以提高通用大语言模型的下游任务对齐效率', 'title_zh': 'ALPS: 注意力定位与剪枝策略以提高大型语言模型对齐效率'}
{'arxiv_id': 'arXiv:2505.18777', 'title': 'HD-PiSSA: High-Rank Distributed Orthogonal Adaptation', 'authors': 'Yiding Wang, Fauxu meng, Xuefeng Zhang, Fan Jiang, Pingzhi Tang, Muhan Zhang', 'link': 'https://arxiv.org/abs/2505.18777', 'abstract': 'Existing parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank subspaces, limiting their expressiveness and leading to suboptimal performance on complex tasks. To address this, we introduce High-rank Distributed PiSSA (HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters across different devices and aggregates their delta updates collectively on W for fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical adapters across all devices, HD-PiSSA assigns different principal components of the pre-trained weights to each GPU, significantly expanding the range of update directions. This results in over 16x higher effective updated ranks than data-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device adapter rank. Empirically, we evaluate HD-PiSSA across various challenging downstream tasks, including mathematics, code generation, and multi-task learning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0 absolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12 benchmarks, demonstrating its benefits from the extra optimization flexibility.', 'abstract_zh': '高秩分布式PiSSA（HD-PiSSA）：一种扩展更新方向的分布式参数高效微调方法', 'title_zh': 'HD-PiSSA: 高秩分布式正交适应'}
{'arxiv_id': 'arXiv:2505.18773', 'title': 'Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models', 'authors': 'Jamie Hayes, Ilia Shumailov, Christopher A. Choquette-Choo, Matthew Jagielski, George Kaissis, Katherine Lee, Milad Nasr, Sahra Ghalebikesabi, Niloofar Mireshghallah, Meenatchi Sundaram Mutu Selva Annamalai, Igor Shilov, Matthieu Meeus, Yves-Alexandre de Montjoye, Franziska Boenisch, Adam Dziedzic, A. Feder Cooper', 'link': 'https://arxiv.org/abs/2505.18773', 'abstract': "State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training reference models (e.g., fine-tuning attacks), or on stronger attacks applied to small-scale models and datasets. However, weaker attacks have been shown to be brittle - achieving close-to-arbitrary success - and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges have prompted an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their effectiveness, however, remains limited (e.g., AUC<0.7) in practical settings; and, (3) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested.", 'abstract_zh': '最先进的成员推断攻击（MIAs）通常需要训练许多参考模型，这使得将这些攻击扩展到大规模预训练语言模型（LLMs）变得困难。因此，前期研究要么依赖于无需训练参考模型（例如，微调攻击）的较弱攻击，要么在小型模型和数据集上应用较强的攻击。然而，较弱的攻击已被证明是脆弱的——能够实现接近任意的成功率——而在简化环境下从强攻击中获得的见解并不适用于今天的LLMs。这些挑战促使了一个重要问题：前期工作中的局限性是由于攻击设计选择，还是MIAs在LLMs上根本无效？我们通过将LiRA——一种最强的MIA之一——扩展到参数范围从10M到1B的GPT-2架构，并在C4数据集中超过20B个Token上训练参考模型，来回答这个问题。我们的结果从三个方面推进了对LLMs上MIAs的理解：（1）强大的MIAs可以在预训练的LLMs上取得成功；（2）然而，其有效性在实际应用中仍然受到限制（例如AUC<0.7）；（3）MIAs的成功与其相关隐私指标之间的关系并没有先前研究所说的那样简单。', 'title_zh': '大规模数据集和中等到大规模语言模型上的强成员推断攻击'}
{'arxiv_id': 'arXiv:2505.18761', 'title': 'How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark', 'authors': 'Minglai Yang, Ethan Huang, Liang Zhang, Mihai Surdeanu, William Wang, Liangming Pan', 'link': 'https://arxiv.org/abs/2505.18761', 'abstract': "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic benchmark to evaluate Large Language Models' (LLMs) reasoning robustness against systematically controlled irrelevant context (IC). GSM-DC constructs symbolic reasoning graphs with precise distractor injections, enabling rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are significantly sensitive to IC, affecting both reasoning path selection and arithmetic accuracy. Additionally, training models with strong distractors improves performance in both in-distribution and out-of-distribution scenarios. We further propose a stepwise tree search guided by a process reward model, which notably enhances robustness in out-of-distribution conditions.", 'abstract_zh': '我们介绍了一种具有干扰背景的分级学校数学合成基准（GSM-DC），用于评估大型语言模型（LLMs）在系统性控制无关背景（IC）下的推理稳健性。GSM-DC 通过精确的干扰注入构建符号推理图，使评估更加严谨和可 reproduction。我们的实验表明，LLMs 对 IC 显著敏感，影响推理路径选择和算术准确性。此外，使用强干扰进行模型训练提高了模型在分布内和分布外场景下的性能。我们还提出了一种基于过程奖励模型的逐步树搜索方法，该方法在分布外条件下显著增强了稳健性。', 'title_zh': 'LLM推理受无关背景干扰的方式：基于受控基准的分析'}
{'arxiv_id': 'arXiv:2505.18724', 'title': 'LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning', 'authors': 'Junyu Chen, Junzhuo Li, Zhen Peng, Wenjie Wang, Yuxiang Ren, Long Shi, Xuming Hu', 'link': 'https://arxiv.org/abs/2505.18724', 'abstract': 'Quantization and fine-tuning are crucial for deploying large language models (LLMs) on resource-constrained edge devices. However, fine-tuning quantized models presents significant challenges, primarily stemming from: First, the mismatch in data types between the low-precision quantized weights (e.g., 4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch limits the computational efficiency advantage offered by quantized weights during inference. Second, potential accuracy degradation when merging these high-precision adaptation weights into the low-precision quantized weights, as the adaptation weights often necessitate approximation or truncation. Third, as far as we know, no existing methods support the lossless merging of adaptation while adjusting all quantized weights. To address these challenges, we introduce lossless ternary adaptation for quantization-aware fine-tuning (LoTA-QAF). This is a novel fine-tuning method specifically designed for quantized LLMs, enabling the lossless merging of ternary adaptation weights into quantized weights and the adjustment of all quantized weights. LoTA-QAF operates through a combination of: i) A custom-designed ternary adaptation (TA) that aligns ternary weights with the quantization grid and uses these ternary weights to adjust quantized weights. ii) A TA-based mechanism that enables the lossless merging of adaptation weights. iii) Ternary signed gradient descent (t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and Qwen-2.5 model families and validate its effectiveness on several downstream tasks. On the MMLU benchmark, our method effectively recovers performance for quantized models, surpassing 16-bit LoRA by up to 5.14\\%. For task-specific fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still outperforms other methods.', 'abstract_zh': '量化和微调对于在资源受限的边缘设备上部署大型语言模型（LLMs）至关重要。然而，量化模型的微调面临着重大挑战，主要源自以下几点：首先，低精度量化权重（如4位）与高精度适应权重（如16位）之间数据类型的不匹配，这限制了在推理过程中由量化权重提供的计算效率优势。其次，在将高精度适应权重合并到低精度量化权重中时可能出现的精度下降，因为适应权重常常需要近似或截断。第三，据我们所知，目前没有任何方法在调整所有量化权重的同时支持无损合并适应权重。为应对这些挑战，我们介绍了一种用于感知量化微调的无损三值适应方法（LoTA-QAF）。这是一种专为量化LLMs设计的新微调方法，能够将三值适应权重无损地合并到量化权重中，并调整所有量化权重。LoTA-QAF通过以下方式运作：i) 自定义设计的三值适应（TA），将三值权重与量化网格对齐，并使用这些三值权重调整量化权重。ii) 基于TA的机制，使得适应权重的无损合并成为可能。iii) 三值符号梯度下降（t-SignSGD）用于更新TA权重。我们将LoTA-QAF应用于Llama-3.1/3.3和Qwen-2.5模型系列，并在多个下游任务中验证了其有效性。在MMLU基准测试中，我们的方法有效地恢复了量化模型的性能，超越了16位LoRA最多5.14%。对于任务特定的微调，16位LoRA表现更优，但LoTA-QAF仍优于其他方法。', 'title_zh': '无损三元适应的量化感知微调'}
{'arxiv_id': 'arXiv:2505.18720', 'title': 'Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization', 'authors': 'Meng Li, Guangda Huzhang, Haibo Zhang, Xiting Wang, Anxiang Zeng', 'link': 'https://arxiv.org/abs/2505.18720', 'abstract': "Direct Preference Optimization (DPO) has emerged as a promising framework for aligning Large Language Models (LLMs) with human preferences by directly optimizing the log-likelihood difference between chosen and rejected responses. However, existing methods assign equal importance to all tokens in the response, while humans focus on more meaningful parts. This leads to suboptimal preference optimization, as irrelevant or noisy tokens disproportionately influence DPO loss. To address this limitation, we propose \\textbf{O}ptimal \\textbf{T}ransport-based token weighting scheme for enhancing direct \\textbf{P}reference \\textbf{O}ptimization (OTPO). By emphasizing semantically meaningful token pairs and de-emphasizing less relevant ones, our method introduces a context-aware token weighting scheme that yields a more contrastive reward difference estimate. This adaptive weighting enhances reward stability, improves interpretability, and ensures that preference optimization focuses on meaningful differences between responses. Extensive experiments have validated OTPO's effectiveness in improving instruction-following ability across various settings\\footnote{Code is available at this https URL.}.", 'abstract_zh': '基于最优运输的令牌加权方案以增强直接偏好优化（OTPO）', 'title_zh': '基于最优运输的token权重方案以增强偏好优化'}
{'arxiv_id': 'arXiv:2505.18710', 'title': 'GainRAG: Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis', 'authors': 'Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, Bing Qin', 'link': 'https://arxiv.org/abs/2505.18710', 'abstract': 'The Retrieval-Augmented Generation (RAG) framework introduces a retrieval module to dynamically inject retrieved information into the input context of large language models (LLMs), and has demonstrated significant success in various NLP tasks. However, the current study points out that there is a preference gap between retrievers and LLMs in the RAG framework, which limit the further improvement of system performance. Some highly relevant passages may interfere with LLM reasoning because they contain complex or contradictory information; while some indirectly related or even inaccurate content may help LLM generate more accurate answers by providing suggestive information or logical clues. To solve this, we propose GainRAG, a novel approach that aligns the retriever\'s and LLM\'s preferences by defining a new metric, "gain", which measure how well an input passage contributes to correct outputs. Specifically, we propose a method to estimate these gain signals and train a middleware that aligns the preferences of the retriever and the LLM using only limited data. In addition, we introduce a pseudo-passage strategy to mitigate degradation. The experimental results on 6 datasets verify the effectiveness of GainRAG.', 'abstract_zh': 'GainRAG：通过定义“增益”度量来对齐检索器和大型语言模型的偏好', 'title_zh': 'GainRAG：通过Gain信号合成实现检索增强生成中的偏好对齐'}
{'arxiv_id': 'arXiv:2505.18709', 'title': 'Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla', 'authors': 'Sourav Kumar Das, Md. Julkar Naeen, MD. Jahidul Islam, Md. Anisul Haque Sajeeb, Narayan Ranjan Chakraborty, Mayen Uddin Mojumdar', 'link': 'https://arxiv.org/abs/2505.18709', 'abstract': "Bangla or Bengali is the national language of Bangladesh, people from different regions don't talk in proper Bangla. Every division of Bangladesh has its own local language like Sylheti, Chittagong etc. In recent years some papers were published on Bangla language like sentiment analysis, fake news detection and classifications, but a few of them were on Bangla languages. This research is for the local language and this particular paper is on Sylheti language. It presented a comprehensive system using Natural Language Processing or NLP techniques for translating Pure or Modern Bangla to locally spoken Sylheti Bangla language. Total 1200 data used for training 3 models LSTM, Bi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3% accuracy. The findings of this research may contribute to the growth of Bangla NLP researchers for future more advanced innovations.", 'abstract_zh': 'Bangla或孟加拉语是孟加拉国的官方语言，不同地区的人们并不使用标准孟加拉语。孟加拉国的每个地区都有自己的地方语言，如锡莱希语、 Chattagram语等。近年来，关于孟加拉语的研究论文有所发表，涉及情感分析、虚假新闻检测和分类等领域，但其中对地方语言的研究较少。本研究专注于地方语言，本文特别关注锡莱希语。研究使用自然语言处理或NLP技术，构建了一个综合系统，用于将纯孟加拉语或现代孟加拉语翻译成当地口语锡莱希语。共使用1200条数据训练了3个模型（LSTM、Bi-LSTM和Seq2Seq），其中LSTM在性能上表现最佳，准确率达到89.3%。本研究的发现可能有助于促进孟加拉语NLP研究人员未来更多的先进创新。', 'title_zh': '改进孟加拉语研究：从西莱蒂到现代孟加拉语的高级LSTM、双向LSTM和Seq2Seq模型翻译'}
{'arxiv_id': 'arXiv:2505.18706', 'title': 'Steering LLM Reasoning Through Bias-Only Adaptation', 'authors': 'Viacheslav Sinii, Alexey Gorbatovski, Artem Cherepanov, Boris Shaposhnikov, Nikita Balagansky, Daniil Gavrilov', 'link': 'https://arxiv.org/abs/2505.18706', 'abstract': 'Recent work on reasoning-oriented language models, exemplified by o1-like systems, suggests that reinforcement-learning (RL) finetuning does not create new capabilities but instead strengthens reasoning patterns already latent in the pretrained network. We test this claim by training steering vectors: layer-wise biases that additively amplify selected hidden features while leaving all original weights unchanged. Experiments on four base models across the GSM8K and MATH benchmarks show that steering vectors recover, and in several cases exceed, the accuracy of fully-tuned counterparts. This result supports the view that the required reasoning skills pre-exist in the base model. Further, logit-lens analysis reveals that the trained vectors consistently boost token groups linked to structured languages and logical connectors, providing an interpretable account that aligns with the demands of quantitative reasoning tasks.', 'abstract_zh': '近期关于基于推理的语言模型的研究，以o1-like系统为例，表明强化学习（RL）微调并没有创造新的能力，而是加强了预训练网络中已经存在推理模式。通过训练引导向量（层wise偏置）进行测试：这些向量在不改变原始权重的情况下，累加放大选定的隐藏特征。在GSM8K和MATH基准测试上的四项基础模型实验表明，引导向量恢复并超过了完全微调版本的准确率。这一结果支持了所需推理技能已经在基础模型中存在这一观点。进一步的logit-lens分析显示，训练后的向量始终提升与结构化语言和逻辑连接词相关的词组，这与定量推理任务的需求相一致，提供了可解释的说明。', 'title_zh': '通过偏见唯一适应引导大语言模型推理'}
{'arxiv_id': 'arXiv:2505.18697', 'title': 'Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning? A Systematic Study', 'authors': 'Ziyang Cheng, Zhixun Li, Yuhan Li, Yixin Song, Kangyi Zhao, Dawei Cheng, Jia Li, Jeffrey Xu Yu', 'link': 'https://arxiv.org/abs/2505.18697', 'abstract': 'Nowadays, real-world data, including graph-structure data, often arrives in a streaming manner, which means that learning systems need to continuously acquire new knowledge without forgetting previously learned information. Although substantial existing works attempt to address catastrophic forgetting in graph machine learning, they are all based on training from scratch with streaming data. With the rise of pretrained models, an increasing number of studies have leveraged their strong generalization ability for continual learning. Therefore, in this work, we attempt to answer whether large language models (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning (GCL). We first point out that current experimental setups for GCL have significant flaws, as the evaluation stage may lead to task ID leakage. Then, we evaluate the performance of LLMs in more realistic scenarios and find that even minor modifications can lead to outstanding results. Finally, based on extensive experiments, we propose a simple-yet-effective method, Simple Graph Continual Learning (SimGCL), that surpasses the previous state-of-the-art GNN-based baseline by around 20% under the rehearsal-free constraint. To facilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL for training and evaluating existing GCL methods. The code is available at: this https URL.', 'abstract_zh': '现在的现实世界数据，包括图结构数据，常常以流式方式到达，这意味着学习系统需要不断获取新知识而不忘记之前学到的信息。尽管现有大量工作试图解决图机器学习中的灾难性遗忘问题，但它们都是基于从头开始使用流式数据训练。随着预训练模型的兴起，越来越多的研究利用其强大的泛化能力进行持续学习。因此，在本文中，我们试图回答大规模语言模型（LLMs）是否可以缓解图持续学习（GCL）中的灾难性遗忘。我们首先指出，当前GCL的实验设置存在显著缺陷，因为评估阶段可能会导致任务ID泄露。然后，我们在更现实的场景下评估了LLMs的表现，发现即使是细微的修改也能取得出色的结果。最后，在大量实验的基础上，我们提出了一种简单而有效的方法——简单图持续学习（SimGCL），在无回顾约束的情况下，该方法比之前的基于GNN的基线方法提高了约20%。为了便于再现，我们开发了一个易于使用且可用于训练和评估现有GCL方法的基准——LLM4GCL。代码可在以下链接获取：this https URL。', 'title_zh': '利用大语言模型缓解图连续学习中的灾难性遗忘：一项系统性研究'}
{'arxiv_id': 'arXiv:2505.18688', 'title': 'Large Language Models in the Task of Automatic Validation of Text Classifier Predictions', 'authors': 'Aleksandr Tsymbalov', 'link': 'https://arxiv.org/abs/2505.18688', 'abstract': "Machine learning models for text classification are trained to predict a class for a given text. To do this, training and validation samples must be prepared: a set of texts is collected, and each text is assigned a class. These classes are usually assigned by human annotators with different expertise levels, depending on the specific classification task. Collecting such samples from scratch is labor-intensive because it requires finding specialists and compensating them for their work; moreover, the number of available specialists is limited, and their productivity is constrained by human factors. While it may not be too resource-intensive to collect samples once, the ongoing need to retrain models (especially in incremental learning pipelines) to address data drift (also called model drift) makes the data collection process crucial and costly over the model's entire lifecycle. This paper proposes several approaches to replace human annotators with Large Language Models (LLMs) to test classifier predictions for correctness, helping ensure model quality and support high-quality incremental learning.", 'abstract_zh': '使用大型语言模型替代人工注释者进行文本分类模型的分类预测验证：确保模型质量并支持高质量增量学习', 'title_zh': '大型语言模型在文本分类器预测自动验证任务中的应用'}
{'arxiv_id': 'arXiv:2505.18659', 'title': 'Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees', 'authors': 'Sangwoo Park, Matteo Zecchin, Osvaldo Simeone', 'link': 'https://arxiv.org/abs/2505.18659', 'abstract': 'Selecting artificial intelligence (AI) models, such as large language models (LLMs), from multiple candidates requires accurate performance estimation. This is ideally achieved through empirical evaluations involving abundant real-world data. However, such evaluations are costly and impractical at scale. To address this challenge, autoevaluation methods leverage synthetic data produced by automated evaluators, such as LLMs-as-judges, reducing variance but potentially introducing bias. Recent approaches have employed semi-supervised prediction-powered inference (\\texttt{PPI}) to correct for the bias of autoevaluators. However, the use of autoevaluators may lead in practice to a degradation in sample efficiency compared to conventional methods using only real-world data. In this paper, we propose \\texttt{R-AutoEval+}, a novel framework that provides finite-sample reliability guarantees on the model evaluation, while also ensuring an enhanced (or at least no worse) sample efficiency compared to conventional methods. The key innovation of \\texttt{R-AutoEval+} is an adaptive construction of the model evaluation variable, which dynamically tunes its reliance on synthetic data, reverting to conventional methods when the autoevaluator is insufficiently accurate. Experiments on the use of LLMs-as-judges for the optimization of quantization settings for the weights of an LLM, and for prompt design in LLMs confirm the reliability and efficiency of \\texttt{R-AutoEval+}.', 'abstract_zh': '从多个候选模型中选择人工智能模型，例如大型语言模型，需要准确的性能估计。理想情况下，这通过涉及丰富的真实世界数据的经验评估来实现。然而，大规模进行此类评估是昂贵且不切实际的。为应对这一挑战，自评估方法利用自动评估器生成的合成数据，如LLM-as-judge，减少方差但可能会引入偏差。最近的方法采用了半监督预测驱动的推理（\\texttt{PPI}）来纠正自动评估器的偏差。然而，在实践中，使用自动评估器可能会导致样本效率劣于仅使用真实世界数据的传统方法。在这篇论文中，我们提出了\\texttt{R-AutoEval+}这一新型框架，它在提供有限样本可靠性保证的同时，确保了相比传统方法至少样本效率不降低。\\texttt{R-AutoEval+}的关键创新在于其自适应构建模型评估变量的方法，该方法动态调整对合成数据的依赖性，在自动评估器准确性不足时返回传统方法。实验表明，\\texttt{R-AutoEval+}在使用LLM-as-judge优化大型语言模型的权重量化设置以及大型语言模型的提示设计中具有可靠性和效率。', 'title_zh': '自适应预测驱动的AutoEval及其可靠性和效率保证'}
{'arxiv_id': 'arXiv:2505.18658', 'title': 'Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics', 'authors': 'Pankaj Kumar, Subhankar Mishra', 'link': 'https://arxiv.org/abs/2505.18658', 'abstract': 'Large Language Models (LLMs) have emerged as a promising cornerstone for the development of natural language processing (NLP) and artificial intelligence (AI). However, ensuring the robustness of LLMs remains a critical challenge. To address these challenges and advance the field, this survey provides a comprehensive overview of current studies in this area. First, we systematically examine the nature of robustness in LLMs, including its conceptual foundations, the importance of consistent performance across diverse inputs, and the implications of failure modes in real-world applications. Next, we analyze the sources of non-robustness, categorizing intrinsic model limitations, data-driven vulnerabilities, and external adversarial factors that compromise reliability. Following this, we review state-of-the-art mitigation strategies, and then we discuss widely adopted benchmarks, emerging metrics, and persistent gaps in assessing real-world reliability. Finally, we synthesize findings from existing surveys and interdisciplinary studies to highlight trends, unresolved issues, and pathways for future research.', 'abstract_zh': '大型语言模型（LLMs）已成为自然语言处理（NLP）和人工智能（AI）发展的重要基石。然而，确保LLMs的稳健性仍然是一个关键挑战。为了应对这些挑战并促进该领域的发展，本文综述了当前该领域的研究现状。首先，我们系统地探讨了LLMs稳健性的本质，包括其概念基础、在多样输入中保持一致性能的重要性以及在实际应用中故障模式的影响。接下来，我们分析了非稳健性的来源，分为内在模型限制、数据驱动的脆弱性和外部敌对因素对可靠性的损害。随后，我们回顾了最先进的缓解策略，并讨论了广泛采用的基准测试、新兴指标以及评估实际可靠性时持续存在的缺口。最后，我们综合现有综述和跨学科研究的发现，强调了研究趋势、未解决的问题以及未来研究的方向。', 'title_zh': '大型语言模型的稳健性：缓解策略与评估指标综述'}
{'arxiv_id': 'arXiv:2505.18646', 'title': 'SEW: Self-Evolving Agentic Workflows for Automated Code Generation', 'authors': 'Siwei Liu, Jinyuan Fang, Han Zhou, Yingxu Wang, Zaiqiao Meng', 'link': 'https://arxiv.org/abs/2505.18646', 'abstract': 'Large Language Models (LLMs) have demonstrated effectiveness in code generation tasks. To enable LLMs to address more complex coding challenges, existing research has focused on crafting multi-agent systems with agentic workflows, where complex coding tasks are decomposed into sub-tasks, assigned to specialized agents. Despite their effectiveness, current approaches heavily rely on hand-crafted agentic workflows, with both agent topologies and prompts manually designed, which limits their ability to automatically adapt to different types of coding problems. To address these limitations and enable automated workflow design, we propose \\textbf{S}elf-\\textbf{E}volving \\textbf{W}orkflow (\\textbf{SEW}), a novel self-evolving framework that automatically generates and optimises multi-agent workflows. Extensive experiments on three coding benchmark datasets, including the challenging LiveCodeBench, demonstrate that our SEW can automatically design agentic workflows and optimise them through self-evolution, bringing up to 33\\% improvement on LiveCodeBench compared to using the backbone LLM only. Furthermore, by investigating different representation schemes of workflow, we provide insights into the optimal way to encode workflow information with text.', 'abstract_zh': '大规模语言模型（LLMs）在代码生成任务中展示了有效性。为了使LLMs能够应对更复杂的编码挑战，现有研究主要集中在构建具有代理型工作流的多代理系统，其中复杂的编码任务被分解为子任务并分配给专门的代理。尽管这些方法效果显著，但当前的方案仍然严重依赖手动设计的代理型工作流，包括代理拓扑结构和提示的亲手设计，这限制了它们自动适应不同类型编码问题的能力。为了解决这些局限性并实现自动化工作流设计，我们提出了一种新颖的自演进框架——自演进工作流（SEW），该框架能够自动生成和优化多代理工作流。在包括具有挑战性的LiveCodeBench在内的三个编码基准数据集上的广泛实验表明，我们的SEW可以自动设计代理型工作流并通过自演进来优化它们，在LiveCodeBench上的性能比仅使用骨干LLM提高了33%。此外，通过探讨不同的工作流表示方案，我们提供了关于如何使用文本最佳编码工作流信息的洞察。', 'title_zh': 'SEW: 自我进化代理工作流的自动化代码生成'}
{'arxiv_id': 'arXiv:2505.18630', 'title': 'DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation', 'authors': 'Zhihao Jia, Mingyi Jia, Junwen Duan, Jianxin Wang', 'link': 'https://arxiv.org/abs/2505.18630', 'abstract': 'Large Language Models (LLMs) demonstrate strong generalization and reasoning abilities, making them well-suited for complex decision-making tasks such as medical consultation (MC). However, existing LLM-based methods often fail to capture the dual nature of MC, which entails two distinct sub-tasks: symptom inquiry, a sequential decision-making process, and disease diagnosis, a classification problem. This mismatch often results in ineffective symptom inquiry and unreliable disease diagnosis. To address this, we propose \\textbf{DDO}, a novel LLM-based framework that performs \\textbf{D}ual-\\textbf{D}ecision \\textbf{O}ptimization by decoupling and independently optimizing the the two sub-tasks through a collaborative multi-agent workflow. Experiments on three real-world MC datasets show that DDO consistently outperforms existing LLM-based approaches and achieves competitive performance with state-of-the-art generation-based methods, demonstrating its effectiveness in the MC task.', 'abstract_zh': '大型语言模型（LLMs）展示出强大的泛化和推理能力，使其适合复杂决策任务如医学咨询（MC）。然而，现有基于LLM的方法往往无法捕捉MC的双重性质，这涉及到两个不同的子任务：症状询问，这是一个顺序决策过程，和疾病诊断，这是一个分类问题。这种不匹配常导致无效的症状询问和不可靠的疾病诊断。为了解决这一问题，我们提出了一种新颖的基于LLM的框架\\textbf{DDO}，该框架通过脱钩并独立优化这两个子任务来实现双重决策优化，采用协作多智能体工作流。实验结果表明，\\textbf{DDO}在三个真实的MC数据集上始终优于现有基于LLM的方法，并且在与最先进的生成方法的竞争中表现出了竞争力，证明了其在MC任务中的有效性。', 'title_zh': 'DDO：基于多Agent协作的双重决策优化在LLM驱动的医疗咨询中的应用'}
{'arxiv_id': 'arXiv:2505.18602', 'title': 'LLM-Meta-SR: Learning to Evolve Selection Operators for Symbolic Regression', 'authors': 'Hengzhe Zhang, Qi Chen, Bing Xue, Mengjie Zhang', 'link': 'https://arxiv.org/abs/2505.18602', 'abstract': 'Large language models (LLMs) have revolutionized algorithm development, yet their application in symbolic regression, where algorithms automatically discover symbolic expressions from data, remains constrained and is typically designed manually by human experts. In this paper, we propose a learning-to-evolve framework that enables LLMs to automatically design selection operators for evolutionary symbolic regression algorithms. We first identify two key limitations in existing LLM-based algorithm evolution techniques: code bloat and a lack of semantic guidance. Bloat results in unnecessarily complex components, and the absence of semantic awareness can lead to ineffective exchange of useful code components, both of which can reduce the interpretability of the designed algorithm or hinder evolutionary learning progress. To address these issues, we enhance the LLM-based evolution framework for meta symbolic regression with two key innovations: bloat control and a complementary, semantics-aware selection operator. Additionally, we embed domain knowledge into the prompt, enabling the LLM to generate more effective and contextually relevant selection operators. Our experimental results on symbolic regression benchmarks show that LLMs can devise selection operators that outperform nine expert-designed baselines, achieving state-of-the-art performance. This demonstrates that LLMs can exceed expert-level algorithm design for symbolic regression.', 'abstract_zh': '大规模语言模型（LLMs）在算法开发中取得了革命性进展，但在符号回归中的应用仍受到限制，通常需要人工专家手动设计。本文提出了一种学习演化框架，使LLMs能够自动为演化符号回归算法设计选择算子。我们首先识别现有基于LLM的算法演化技术中的两个关键限制：代码膨胀和缺乏语义指导。代码膨胀会导致不必要的复杂性，而缺乏语义意识则可能导致有用代码组件的有效交换受阻，这两者都可能降低所设计算法的可解释性或阻碍进化学习进程。为了解决这些问题，我们通过两种关键创新增强了基于LLM的元符号回归演化框架：代码膨胀控制和互补的、具有语义意识的选择算子。此外，我们还将领域知识嵌入提示中，使LLM能够生成更有效且更具上下文相关性的选择算子。我们对符号回归基准的实验结果表明，LLMs能够设计出超越九个专家设计基准的选择算子，达到最先进的性能。这表明LLMs能够在符号回归的算法设计上超越专家水平。', 'title_zh': 'LLM-Meta-SR：学习演化符号回归中的选择算子'}
{'arxiv_id': 'arXiv:2505.18601', 'title': 'Flex-Judge: Think Once, Judge Anywhere', 'authors': 'Jongwoo Ko, Sungnyun Kim, Sungwoo Cho, Se-Young Yun', 'link': 'https://arxiv.org/abs/2505.18601', 'abstract': 'Human-generated reward signals are critical for aligning generative models with human preferences, guiding both training and inference-time evaluations. While large language models (LLMs) employed as proxy evaluators, i.e., LLM-as-a-Judge, significantly reduce the costs associated with manual annotations, they typically require extensive modality-specific training data and fail to generalize well across diverse multimodal tasks. In this paper, we propose Flex-Judge, a reasoning-guided multimodal judge model that leverages minimal textual reasoning data to robustly generalize across multiple modalities and evaluation formats. Our core intuition is that structured textual reasoning explanations inherently encode generalizable decision-making patterns, enabling an effective transfer to multimodal judgments, e.g., with images or videos. Empirical results demonstrate that Flex-Judge, despite being trained on significantly fewer text data, achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators. Notably, Flex-Judge presents broad impact in modalities like molecule, where comprehensive evaluation benchmarks are scarce, underscoring its practical value in resource-constrained domains. Our framework highlights reasoning-based text supervision as a powerful, cost-effective alternative to traditional annotation-intensive approaches, substantially advancing scalable multimodal model-as-a-judge.', 'abstract_zh': '人类生成的奖励信号对于对齐生成模型与人类偏好至关重要，指导训练和推理时期的评估。虽然作为代理评估器使用的大型语言模型（LLMs），即LLM-as-a-Judge，显著降低了手动注释的成本，但它们通常需要大量的模态特定训练数据，并且在多种模态任务上的泛化能力较差。本文提出了一种名为Flex-Judge的推理引导多模态评估模型，利用少量的文本推理数据，在多种模态和评估格式上实现稳健泛化。我们核心的直觉是，结构化的文本推理解释内固地编码了可泛化的决策模式，能够有效转移到多模态判断中，例如带有图片或视频的任务。实验结果表明，尽管Flex-Judge在远少于文本数据上进行训练，但在性能上与最先进的商业API和广泛训练的多模态评估器相当或更优。特别是在评价基准稀缺的分子等模态中，Flex-Judge展示了其在资源受限领域中的广泛影响，突显了其实用价值。本文框架突出了基于推理的文本监督作为传统注释密集型方法的一种强大、成本效益高的替代方案，显著推进了可扩展的多模态模型作为评估器的发展。', 'title_zh': 'Flex-Judge: 一次思考，随处判断'}
{'arxiv_id': 'arXiv:2505.18596', 'title': 'Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models', 'authors': 'Chen Han, Wenzhen Zheng, Xijin Tang', 'link': 'https://arxiv.org/abs/2505.18596', 'abstract': "The proliferation of misinformation in digital platforms reveals the limitations of traditional detection methods, which mostly rely on static classification and fail to capture the intricate process of real-world fact-checking. Despite advancements in Large Language Models (LLMs) that enhance automated reasoning, their application to misinformation detection remains hindered by issues of logical inconsistency and superficial verification. In response, we introduce Debate-to-Detect (D2D), a novel Multi-Agent Debate (MAD) framework that reformulates misinformation detection as a structured adversarial debate. Inspired by fact-checking workflows, D2D assigns domain-specific profiles to each agent and orchestrates a five-stage debate process, including Opening Statement, Rebuttal, Free Debate, Closing Statement, and Judgment. To transcend traditional binary classification, D2D introduces a multi-dimensional evaluation mechanism that assesses each claim across five distinct dimensions: Factuality, Source Reliability, Reasoning Quality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets demonstrate significant improvements over baseline methods, and the case study highlight D2D's capability to iteratively refine evidence while improving decision transparency, representing a substantial advancement towards robust and interpretable misinformation detection. The code will be open-sourced in a future release.", 'abstract_zh': '数字平台中虚假信息的泛滥揭示了传统检测方法的局限性，这些方法主要依赖静态分类并无法捕捉现实世界事实核查的复杂过程。尽管大型语言模型（LLMs）的进步提高了自动推理的能力，但其在虚假信息检测中的应用仍受到逻辑不一致和表面化验证的问题限制。为应对这一挑战，我们提出了 Debate-to-Detect（D2D），一种新颖的多智能体辩论（MAD）框架，将虚假信息检测重新定义为结构化的对抗性辩论。受到事实核查工作流程的启发，D2D 为每个智能体分配领域特定的特征，并 orchestrates 一个包括开场陈述、反驳、自由辩论、总结陈述和裁决的五阶段辩论过程。为超越传统的二元分类，D2D 引入了一种多维度评估机制，分别从事实性、源可靠性、推理质量、清晰度和伦理学五个维度评估每个断言。使用 GPT-4o 在两个假新闻数据集上的实验展示了显著的改进，案例研究突显了 D2D 的能力，可以在迭代中细化证据并提高决策透明度，代表了虚假信息检测向稳健和可解释性方向的一大进步。代码将在未来的发布中开源。', 'title_zh': '辩论驱动检测：将信息误导检测重新表述为大型语言模型参与的实际辩论'}
{'arxiv_id': 'arXiv:2505.18588', 'title': 'Safety Alignment via Constrained Knowledge Unlearning', 'authors': 'Zesheng Shi, Yucheng Zhou, Jing Li', 'link': 'https://arxiv.org/abs/2505.18588', 'abstract': 'Despite significant progress in safety alignment, large language models (LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms have not fully deleted harmful knowledge in LLMs, which allows such attacks to bypass safeguards and produce harmful outputs. To address this challenge, we propose a novel safety alignment strategy, Constrained Knowledge Unlearning (CKU), which focuses on two primary objectives: knowledge localization and retention, and unlearning harmful knowledge. CKU works by scoring neurons in specific multilayer perceptron (MLP) layers to identify a subset U of neurons associated with useful knowledge. During the unlearning process, CKU prunes the gradients of neurons in U to preserve valuable knowledge while effectively mitigating harmful content. Experimental results demonstrate that CKU significantly enhances model safety without compromising overall performance, offering a superior balance between safety and utility compared to existing methods. Additionally, our analysis of neuron knowledge sensitivity across various MLP layers provides valuable insights into the mechanics of safety alignment and model knowledge editing.', 'abstract_zh': '尽管在安全性对齐方面取得了显著进展，大型语言模型（LLMs）仍然易受脱管攻击的影响。现有的防护机制未能完全删除LLMs中的有害知识，这使得此类攻击能够规避防护措施并产生有害输出。为应对这一挑战，我们提出了一种新的安全性对齐策略——约束知识遗忘（CKU），该策略主要包含两个目标：知识定位与保留，以及遗忘有害知识。CKU通过为特定多层感知器（MLP）层中的神经元评分，来识别与有用知识相关的神经元子集U。在遗忘过程中，CKU通过修剪神经元U的梯度来保留有价值的知识，同时有效降低有害内容的影响。实验结果表明，CKU显著提升了模型的安全性，而不会牺牲总体性能，提供了在安全性与实用性之间更优越的平衡。此外，我们对不同MLP层中神经元知识敏感性的分析为安全性对齐和模型知识编辑的机理提供了宝贵见解。', 'title_zh': '基于约束知识遗忘的安全对齐'}
{'arxiv_id': 'arXiv:2505.18574', 'title': 'Autocomp: LLM-Driven Code Optimization for Tensor Accelerators', 'authors': 'Charles Hong, Sahil Bhatia, Alvin Cheung, Yakun Sophia Shao', 'link': 'https://arxiv.org/abs/2505.18574', 'abstract': "Hardware accelerators, especially those designed for tensor processing, have become ubiquitous in today's computing landscape. However, even with significant efforts in building compilers, programming these tensor accelerators remains challenging, leaving much of their potential underutilized. Recently, large language models (LLMs), trained on large amounts of code, have shown significant promise in code generation and optimization tasks, but generating low-resource languages like specialized tensor accelerator code still poses a significant challenge. We tackle this challenge with Autocomp, an approach that empowers accelerator programmers to leverage domain knowledge and hardware feedback to optimize code via an automated LLM-driven search. We accomplish this by: 1) formulating each optimization pass as a structured two-phase prompt, divided into planning and code generation phases, 2) inserting domain knowledge during planning via a concise and adaptable optimization menu, and 3) integrating correctness and performance metrics from hardware as feedback at each search iteration. Across three categories of representative workloads and two different accelerators, we demonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x (convolution) faster than the vendor-provided library, and outperforms expert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x (fine-grained linear algebra). Additionally, we demonstrate that optimization schedules generated from Autocomp can be reused across similar tensor operations, improving speedups by up to 24% under a fixed sample budget.", 'abstract_zh': '硬件加速器，尤其是为张量处理设计的加速器，已成为当今计算领域无处不在的存在。然而，即使在构建编译器方面投入了大量努力，编程这些张量加速器仍然具有挑战性，导致其潜在性能远远未被充分利用。最近，大量代码训练的大语言模型（LLMs）在代码生成和优化任务中展现了显著的潜力，但生成低资源语言如专门的张量加速器代码仍然存在重大挑战。我们通过Autocomp的方法来应对这一挑战，该方法使加速器程序员能够利用领域知识和硬件反馈，通过自动化的LLM驱动搜索来优化代码。我们通过以下方式实现了这一点：1）将每个优化步骤形式化为结构化的两阶段提示，分为规划和代码生成阶段；2）在规划期间通过简洁且可适应的优化菜单插入领域知识；3）在每次搜索迭代中将正确性和性能指标作为反馈集成到硬件中。在三个代表性工作负载类别和两种不同的加速器上，我们展示了Autocomp优化的代码分别比供应商提供的库快5.6倍（GEMM）和2.7倍（卷积），并且在GEMM、卷积和精细粒度线性代数方面优于专家级别的手动调优代码1.4倍、1.1倍和1.3倍。此外，我们展示了从Autocomp生成的优化调度可以在类似张量操作之间重用，从而在固定样本预算下将加速性能提高多达24%。', 'title_zh': 'Autocomp: 由大规模语言模型驱动的张量加速器代码优化'}
{'arxiv_id': 'arXiv:2505.18572', 'title': 'MASTER: Multi-Agent Security Through Exploration of Roles and Topological Structures -- A Comprehensive Framework', 'authors': 'Yifan Zhu, Chao Zhang, Xin Shi, Xueqiao Zhang, Yi Yang, Yawei Luo', 'link': 'https://arxiv.org/abs/2505.18572', 'abstract': 'Large Language Models (LLMs)-based Multi-Agent Systems (MAS) exhibit remarkable problem-solving and task planning capabilities across diverse domains due to their specialized agentic roles and collaborative interactions. However, this also amplifies the severity of security risks under MAS attacks. To address this, we introduce MASTER, a novel security research framework for MAS, focusing on diverse Role configurations and Topological structures across various scenarios. MASTER offers an automated construction process for different MAS setups and an information-flow-based interaction paradigm. To tackle MAS security challenges in varied scenarios, we design a scenario-adaptive, extensible attack strategy utilizing role and topological information, which dynamically allocates targeted, domain-specific attack tasks for collaborative agent execution. Our experiments demonstrate that such an attack, leveraging role and topological information, exhibits significant destructive potential across most models. Additionally, we propose corresponding defense strategies, substantially enhancing MAS resilience across diverse scenarios. We anticipate that our framework and findings will provide valuable insights for future research into MAS security challenges.', 'abstract_zh': '基于大型语言模型的多 Agent 系统的新型安全研究框架：考虑多样化角色配置和拓扑结构', 'title_zh': 'MASTER：通过角色与拓扑结构探索实现多agent安全的综合框架'}
{'arxiv_id': 'arXiv:2505.18562', 'title': 'From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test', 'authors': 'Xunlian Dai, Li Zhou, Benyou Wang, Haizhou Li', 'link': 'https://arxiv.org/abs/2505.18562', 'abstract': 'The human-centered word association test (WAT) serves as a cognitive proxy, revealing sociocultural variations through lexical-semantic patterns. We extend this test into an LLM-adaptive, free-relation task to assess the alignment of large language models (LLMs) with cross-cultural cognition. To mitigate the culture preference, we propose CultureSteer, an innovative approach that integrates a culture-aware steering mechanism to guide semantic representations toward culturally specific spaces. Experiments show that current LLMs exhibit significant bias toward Western cultural (notably in American) schemas at the word association level. In contrast, our model substantially improves cross-cultural alignment, surpassing prompt-based methods in capturing diverse semantic associations. Further validation on culture-sensitive downstream tasks confirms its efficacy in fostering cognitive alignment across cultures. This work contributes a novel methodological paradigm for enhancing cultural awareness in LLMs, advancing the development of more inclusive language technologies.', 'abstract_zh': '以人为中心的词联想法卌（WAT）作为一种认知代理，通过语言意义模式揭示社会文化差异。我们将其扩展为一种针对大规模语言模型（LLM）的自适应自由关联任务，以评估其与跨文化认知的对齐程度。为缓解文化偏好，我们提出了一种名为CultureSteer的创新方法，该方法通过融入一种文化意识的引导机制，使语义表示朝着特定文化空间发展。实验表明，当前的LLM在词联想法卌层面上表现出明显偏向西方文化（特别是美国文化）的偏见。相比之下，我们的模型在跨文化对齐方面取得了显著改善，其在捕捉多样化的语义关联方面超越了基于提示的方法。进一步在文化敏感的下游任务上的验证证实了其在促进跨文化交流认知对齐方面的有效性。本工作提供了一种新的方法论范式，以增强LLM的文化意识，推动更包容的语言技术的发展。', 'title_zh': '从词到世界：通过词联想法评估和减轻文化偏见'}
{'arxiv_id': 'arXiv:2505.18556', 'title': 'Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation', 'authors': 'Jun Zhuang, Haibo Jin, Ye Zhang, Zhengjian Kang, Wenbin Zhang, Gaby G. Dagher, Haohan Wang', 'link': 'https://arxiv.org/abs/2505.18556', 'abstract': 'Intent detection, a core component of natural language understanding, has considerably evolved as a crucial mechanism in safeguarding large language models (LLMs). While prior work has applied intent detection to enhance LLMs\' moderation guardrails, showing a significant success against content-level jailbreaks, the robustness of these intent-aware guardrails under malicious manipulations remains under-explored. In this work, we investigate the vulnerability of intent-aware guardrails and demonstrate that LLMs exhibit implicit intent detection capabilities. We propose a two-stage intent-based prompt-refinement framework, IntentPrompt, that first transforms harmful inquiries into structured outlines and further reframes them into declarative-style narratives by iteratively optimizing prompts via feedback loops to enhance jailbreak success for red-teaming purposes. Extensive experiments across four public benchmarks and various black-box LLMs indicate that our framework consistently outperforms several cutting-edge jailbreak methods and evades even advanced Intent Analysis (IA) and Chain-of-Thought (CoT)-based defenses. Specifically, our "FSTR+SPIN" variant achieves attack success rates ranging from 88.25% to 96.54% against CoT-based defenses on the o1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based defenses. These findings highlight a critical weakness in LLMs\' safety mechanisms and suggest that intent manipulation poses a growing challenge to content moderation guardrails.', 'abstract_zh': '基于意图的防御漏洞研究：大语言模型的安全机制缺陷', 'title_zh': '通过意图操纵探索大型语言模型内容审核护栏的脆弱性'}
{'arxiv_id': 'arXiv:2505.18536', 'title': 'Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models', 'authors': 'Haoyuan Sun, Jiaqi Wu, Bo Xia, Yifu Luo, Yifei Zhao, Kai Qin, Xufei Lv, Tiantian Zhang, Yongzhe Chang, Xueqian Wang', 'link': 'https://arxiv.org/abs/2505.18536', 'abstract': 'Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such as OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to enhance the reasoning capability of multimodal large language models (MLLMs) has attracted widespread attention from the community. In this position paper, we argue that reinforcement fine-tuning powers the reasoning capability of multimodal large language models. To begin with, we provide a detailed introduction to the fundamental background knowledge that researchers interested in this field should be familiar with. Furthermore, we meticulously summarize the improvements of RFT in powering reasoning capability of MLLMs into five key points: diverse modalities, diverse tasks and domains, better training algorithms, abundant benchmarks and thriving engineering frameworks. Finally, we propose five promising directions for future research that the community might consider. We hope that this position paper will provide valuable insights to the community at this pivotal stage in the advancement toward AGI. Summary of works done on RFT for MLLMs is available at this https URL.', 'abstract_zh': '站在2025年，人工智能通用智能（AGI）追求的关键时刻，强化微调（RFT）已在增强大型语言模型（LLMs）的推理能力方面展示了显著潜力，并推动了如OpenAI-o1和DeepSeek-R1等前沿AI模型的发展。此外，将高效的RFT应用到多模态大型语言模型（MLLMs）以增强其推理能力引起了社区的广泛关注。在本文中，我们认为强化微调赋能了多模态大型语言模型的推理能力。首先，我们详细介绍了该领域的研究者应当熟悉的基础知识。其次，我们仔细总结了RFT在赋能MLLMs推理能力方面的五项改进：多样化的模态、多样化的任务和领域、更优的训练算法、丰富的基准数据集和繁荣的工程框架。最后，我们提出了社区在未来研究中可以考虑的五个有前景的方向。我们希望本文能够为AGI发展关键阶段的社区提供宝贵见解。有关RFT在MLLMs上的工作总结，请访问此链接：this https URL。', 'title_zh': '多模态大型语言模型的强化微调增强推理能力'}
{'arxiv_id': 'arXiv:2505.18527', 'title': 'CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs', 'authors': 'Yiqing Zhang, Xiaozhong Liu, Fabricio Murai', 'link': 'https://arxiv.org/abs/2505.18527', 'abstract': 'Many existing models for clinical trial outcome prediction are optimized using task-specific loss functions on trial phase-specific data. While this scheme may boost prediction for common diseases and drugs, it can hinder learning of generalizable representations, leading to more false positives/negatives. To address this limitation, we introduce CLaDMoP, a new pre-training approach for clinical trial outcome prediction, alongside the Successful Clinical Trials dataset(SCT), specifically designed for this task. CLaDMoP leverages a Large Language Model-to encode trials\' eligibility criteria-linked to a lightweight Drug-Molecule branch through a novel multi-level fusion technique. To efficiently fuse long embeddings across levels, we incorporate a grouping block, drastically reducing computational overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training on a "pair matching" proxy task. Compared to established zero-shot and few-shot baselines, our method significantly improves both PR-AUC and ROC-AUC, especially for phase I and phase II trials. We further evaluate and perform ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome Prediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC and 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP, highlighting its potential for clinical trial outcome prediction. Code and SCT dataset can be downloaded from this https URL.', 'abstract_zh': 'CLaDMoP：一种新的临床试验结果预测预训练方法及其在 Successful Clinical Trials 数据集上的应用', 'title_zh': 'CLaDMoP：通过大语言模型从成功的临床试验学习可转移模型'}
{'arxiv_id': 'arXiv:2505.18512', 'title': 'AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking', 'authors': 'Soyoung Yoon, Gyuwan Kim, Gyu-Hwung Cho, Seung-won Hwang', 'link': 'https://arxiv.org/abs/2505.18512', 'abstract': 'Listwise reranking with large language models (LLMs) enhances top-ranked results in retrieval-based applications. Due to the limit in context size and high inference cost of long context, reranking is typically performed over a fixed size of small subsets, with the final ranking aggregated from these partial results. This fixed computation disregards query difficulty and document distribution, leading to inefficiencies. We propose AcuRank, an adaptive reranking framework that dynamically adjusts both the amount and target of computation based on uncertainty estimates over document relevance. Using a Bayesian TrueSkill model, we iteratively refine relevance estimates until reaching sufficient confidence levels, and our explicit modeling of ranking uncertainty enables principled control over reranking behavior and avoids unnecessary updates to confident predictions. Results on the TREC-DL and BEIR benchmarks show that our method consistently achieves a superior accuracy-efficiency trade-off and scales better with compute than fixed-computation baselines. These results highlight the effectiveness and generalizability of our method across diverse retrieval tasks and LLM-based reranking models.', 'abstract_zh': '使用大规模语言模型（LLMs）进行列表级别的重排可增强基于检索的应用中的顶级结果。由于上下文大小的限制和长期上下文推理成本高，重排通常在固定大小的小子集上进行，最终排名从这些部分结果聚合而来。这种固定的计算忽略了查询难度和文档分布，导致效率低下。我们提出了AcuRank，一种基于文档相关性不确定性估计动态调整计算量和目标的自适应重排框架。利用贝叶斯TrueSkill模型，我们逐步细化相关性估计，直到达到足够的信心水平，并通过明确建模排名不确定性实现了对重排行为的原则性控制，避免了对有信心预测不必要的更新。在TREC-DL和BEIR基准上的结果表明，我们的方法在准确性和效率权衡上始终表现优越，并且在计算资源上的扩展性优于固定计算量基线。这些结果突显了我们在不同检索任务和基于LLM的重排模型上的方法的有效性和泛化能力。', 'title_zh': 'AcuRank: 基于不确定性自适应计算的列表重排序'}
{'arxiv_id': 'arXiv:2505.18499', 'title': 'G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning', 'authors': 'Xiaojun Guo, Ang Li, Yifei Wang, Stefanie Jegelka, Yisen Wang', 'link': 'https://arxiv.org/abs/2505.18499', 'abstract': "Although Large Language Models (LLMs) have demonstrated remarkable progress, their proficiency in graph-related tasks remains notably limited, hindering the development of truly general-purpose models. Previous attempts, including pretraining graph foundation models or employing supervised fine-tuning, often face challenges such as the scarcity of large-scale, universally represented graph data. We introduce G1, a simple yet effective approach demonstrating that Reinforcement Learning (RL) on synthetic graph-theoretic tasks can significantly scale LLMs' graph reasoning abilities. To enable RL training, we curate Erdõs, the largest graph reasoning dataset to date comprising 50 diverse graph-theoretic tasks of varying difficulty levels, 100k training data and 5k test data, all drived from real-world graphs. With RL on Erdõs, G1 obtains substantial improvements in graph reasoning, where our finetuned 3B model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also show strong zero-shot generalization to unseen tasks, domains, and graph encoding schemes, including other graph-theoretic benchmarks as well as real-world node classification and link prediction tasks, without compromising general reasoning abilities. Our findings offer an efficient, scalable path for building strong graph reasoners by finetuning LLMs with RL on graph-theoretic tasks, which combines the strengths of pretrained LLM capabilities with abundant, automatically generated synthetic data, suggesting that LLMs possess graph understanding abilities that RL can elicit successfully.", 'abstract_zh': '虽然大规模语言模型（LLMs）取得了显著进展，但在图相关任务上的能力仍然受到明显限制，阻碍了真正通用模型的发展。以往尝试，包括预先训练图基础模型或采用监督微调，往往面临大规模、通用表示图数据稀缺的挑战。我们引入了G1，这是一种简单而有效的方法，证明了在合成图理论任务上使用强化学习（RL）可以显著提升LLMs的图推理能力。为了支持RL训练，我们制定了Erdõs数据集，这是迄今为止最大的图推理数据集，包含50个难度各异的图理论任务，100,000个训练数据和5,000个测试数据，均来自真实世界的图。通过在Erdõs上的RL训练，G1在图推理上取得了显著改进，我们的微调3B模型甚至超过了Qwen2.5-72B-Instruct（大小为其24倍）。经RL训练的模型还展示了强大的零样本泛化能力，适用于未见过的任务、领域和图编码方案，包括其他图理论基准以及真实世界的节点分类和链接预测任务，而不会削弱普遍推理能力。我们的发现提供了一条通过在图理论任务上使用RL微调LLMs来构建强大图推理者的有效、可扩展路径，结合了预训练LLM能力与丰富、自动生成的合成数据的优势，表明LLMs具有RL能够成功揭示的图理解能力。', 'title_zh': '教学大规模语言模型使用强化学习进行图推理'}
{'arxiv_id': 'arXiv:2505.18488', 'title': 'Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications', 'authors': 'Yanxiang Zhang, Zheng Xu, Shanshan Wu, Yuanbo Zhang, Daniel Ramage', 'link': 'https://arxiv.org/abs/2505.18488', 'abstract': 'Error correction is an important capability when applying large language models (LLMs) to facilitate user typing on mobile devices. In this paper, we use LLMs to synthesize a high-quality dataset of error correction pairs to evaluate and improve LLMs for mobile applications. We first prompt LLMs with error correction domain knowledge to build a scalable and reliable addition to the existing data synthesis pipeline. We then adapt the synthetic data distribution to match the mobile application domain by reweighting the samples. The reweighting model is learnt by predicting (a handful of) live A/B test metrics when deploying LLMs in production, given the LLM performance on offline evaluation data and scores from a small privacy-preserving on-device language model. Finally, we present best practices for mixing our synthetic data with other data sources to improve model performance on error correction in both offline evaluation and production live A/B testing.', 'abstract_zh': '大型语言模型在移动设备上应用时辅助用户输入错误修正是一项重要能力。本文利用大型语言模型合成高质量的错误修正数据集以评估和改进适用于移动应用的大型语言模型。我们首先利用错误修正领域的知识提示大型语言模型，构建扩展现有数据合成管道的可扩展且可靠的补充内容。然后，通过重新加权样本，使合成数据分布与移动应用领域相匹配。重新加权模型通过预测部署大型语言模型时的实时A/B测试指标（给定离线评估数据上的大型语言模型性能和小型私有设备语言模型的分数）来学习。最后，我们介绍了在错误修正的离线评估和生产实时A/B测试中提高模型性能的最佳实践，包括将合成数据与其他数据源混合的方法。', 'title_zh': '合成和适应错误纠正数据以应用于移动大型语言模型'}
{'arxiv_id': 'arXiv:2505.18475', 'title': 'Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey', 'authors': 'Mengran Li, Pengyu Zhang, Wenbin Xing, Yijia Zheng, Klim Zaporojets, Junzhou Chen, Ronghui Zhang, Yong Zhang, Siyuan Gong, Jia Hu, Xiaolei Ma, Zhiyuan Liu, Paul Groth, Marcel Worring', 'link': 'https://arxiv.org/abs/2505.18475', 'abstract': 'Graphs are a widely used paradigm for representing non-Euclidean data, with applications ranging from social network analysis to biomolecular prediction. Conventional graph learning approaches typically rely on fixed structural assumptions or fully observed data, limiting their effectiveness in more complex, noisy, or evolving settings. Consequently, real-world graph data often violates the assumptions of traditional graph learning methods, in particular, it leads to four fundamental challenges: (1) Incompleteness, real-world graphs have missing nodes, edges, or attributes; (2) Imbalance, the distribution of the labels of nodes or edges and their structures for real-world graphs are highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains exhibit incompatible feature spaces or structural patterns; and (4) Dynamic Instability, graphs evolve over time in unpredictable ways. Recent advances in Large Language Models (LLMs) offer the potential to tackle these challenges by leveraging rich semantic reasoning and external knowledge. This survey provides a comprehensive review of how LLMs can be integrated with graph learning to address the aforementioned challenges. For each challenge, we review both traditional solutions and modern LLM-driven approaches, highlighting how LLMs contribute unique advantages. Finally, we discuss open research questions and promising future directions in this emerging interdisciplinary field. To support further exploration, we have curated a repository of recent advances on graph learning challenges: this https URL.', 'abstract_zh': '大型语言模型在图学习中的应用：解决非欧几里得数据挑战的研究综述', 'title_zh': '使用大规模语言模型应对图学习中的基础挑战：一项全面综述'}
{'arxiv_id': 'arXiv:2505.18471', 'title': 'Invisible Tokens, Visible Bills: The Urgent Need to Audit Hidden Operations in Opaque LLM Services', 'authors': 'Guoheng Sun, Ziyao Wang, Xuandong Zhao, Bowei Tian, Zheyu Shen, Yexiao He, Jinming Xing, Ang Li', 'link': 'https://arxiv.org/abs/2505.18471', 'abstract': 'Modern large language model (LLM) services increasingly rely on complex, often abstract operations, such as multi-step reasoning and multi-agent collaboration, to generate high-quality outputs. While users are billed based on token consumption and API usage, these internal steps are typically not visible. We refer to such systems as Commercial Opaque LLM Services (COLS). This position paper highlights emerging accountability challenges in COLS: users are billed for operations they cannot observe, verify, or contest. We formalize two key risks: \\textit{quantity inflation}, where token and call counts may be artificially inflated, and \\textit{quality downgrade}, where providers might quietly substitute lower-cost models or tools. Addressing these risks requires a diverse set of auditing strategies, including commitment-based, predictive, behavioral, and signature-based methods. We further explore the potential of complementary mechanisms such as watermarking and trusted execution environments to enhance verifiability without compromising provider confidentiality. We also propose a modular three-layer auditing framework for COLS and users that enables trustworthy verification across execution, secure logging, and user-facing auditability without exposing proprietary internals. Our aim is to encourage further research and policy development toward transparency, auditability, and accountability in commercial LLM services.', 'abstract_zh': '现代商业不透明大型语言模型服务中的新兴问责难题及其应对策略', 'title_zh': '隐形令牌，可见账单：审计不透明LLM服务中隐藏操作的迫切需求'}
{'arxiv_id': 'arXiv:2505.18464', 'title': 'From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data', 'authors': 'Ugur Kursuncu, Trilok Padhi, Gaurav Sinha, Abdulkadir Erol, Jaya Krishna Mandivarapu, Christopher R. Larrison', 'link': 'https://arxiv.org/abs/2505.18464', 'abstract': 'The growing demand for accessible mental health support, compounded by workforce shortages and logistical barriers, has led to increased interest in utilizing Large Language Models (LLMs) for scalable and real-time assistance. However, their use in sensitive domains such as anxiety support remains underexamined. This study presents a systematic evaluation of LLMs (GPT and Llama) for their potential utility in anxiety support by using real user-generated posts from the r/Anxiety subreddit for both prompting and fine-tuning. Our approach utilizes a mixed-method evaluation framework incorporating three main categories of criteria: (i) linguistic quality, (ii) safety and trustworthiness, and (iii) supportiveness. Results show that fine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic quality but increased toxicity and bias, and diminished emotional responsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more supportive overall. Our findings highlight the risks of fine-tuning LLMs on unprocessed social media content without mitigation strategies.', 'abstract_zh': '快速增长的可访问心理健康支持需求，加之人力资源短缺和物流障碍，促使人们越来越多地考虑利用大型语言模型（LLMs）提供 scalable 和实时帮助。然而，其在焦虑支持等敏感领域中的应用仍然缺乏研究。本研究通过使用来自 r/Anxiety 子reddit 的实际用户生成帖子来提示和微调，系统性评估了 GPT 和 Llama 等LLMs在焦虑支持中的潜在用途。我们的方法采用了混合方法评估框架，包含三个主要评价标准类别：(i) 语言质量，(ii) 安全性和可信度，(iii) 支持性。结果显示，使用自然语言处理过的焦虑相关数据微调 LLMs 提高了语言质量，但增加了毒性和偏见，并降低了情感响应性。虽然LLMs在移情方面表现有限，但GPT总体上被认为更具支持性。本研究结果强调了在没有缓解策略的情况下使用未处理社交媒体内容微调LLMs所带来的风险。', 'title_zh': '从Reddit到生成型AI：基于社交媒体数据 fine-tuned 的焦虑支持大型语言模型评估'}
{'arxiv_id': 'arXiv:2505.18458', 'title': 'A Survey of LLM $\\times$ DATA', 'authors': 'Xuanhe Zhou, Junxuan He, Wei Zhou, Haodong Chen, Zirui Tang, Haoyu Zhao, Xin Tong, Guoliang Li, Youmin Chen, Jun Zhou, Zhaojun Sun, Binyuan Hui, Shuo Wang, Conghui He, Zhiyuan Liu, Jingren Zhou, Fan Wu', 'link': 'https://arxiv.org/abs/2505.18458', 'abstract': 'The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA, LLMs are emerging as general-purpose engines for data management. We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration.', 'abstract_zh': '大规模语言模型（LLM）与数据管理（DATA）的整合正迅速重塑这两个领域。在本文综述中，我们全面回顾了二者之间的双向关系。一方面，DATA4LLM，覆盖大规模数据处理、存储和供给，为LLM在预训练、后训练、检索增强生成和自主工作流等阶段提供高质量、多样性和及时性的数据，包括：(i) 对LLM的数据处理包括可扩展的数据获取、去重、过滤、选择、领域混合和合成增强；(ii) LLMD数据存储关注高效的数据和模型格式、分布式和异构存储层次结构、KV缓存管理以及容错检查点；(iii) LLMD数据供给解决挑战，如RAG的知识后处理、LLM推理（如提示压缩、数据溯源）和训练策略（如数据打包和打乱）。另一方面，在LLM4DATA中，LLM正成为通用的数据管理引擎。我们回顾了数据操作（包括自动数据清洗、集成和发现）、数据分析（包括结构化、半结构化和非结构化数据的推理）以及系统优化（如配置调优、查询重写和异常诊断）的近期进展，这些进展得益于如检索增强提示、任务专用微调和多代理协作等LLM技术。', 'title_zh': 'LLM与数据综述'}
{'arxiv_id': 'arXiv:2505.18451', 'title': '$μ$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts', 'authors': 'Toshiaki Koike-Akino, Jing Liu, Ye Wang', 'link': 'https://arxiv.org/abs/2505.18451', 'abstract': 'To tackle the huge computational demand of large foundation models, activation-aware compression techniques without retraining have been introduced. However, since these rely on calibration data, domain shift may arise for unknown downstream tasks. With a computationally efficient calibration, activation-aware pruning can be executed for every prompt adaptively, yet achieving reduced complexity at inference. We formulate it as a mixture of micro-experts, called $\\mu$-MoE. Several experiments demonstrate that $\\mu$-MoE can dynamically adapt to task/prompt-dependent structured sparsity on the fly.', 'abstract_zh': '激活感知压缩技术在不重新训练的情况下应对大规模基础模型的庞大计算需求，但可能因校准数据而产生领域偏移。通过计算高效校准，激活感知剪枝可以适应每个提示动态执行，从而在推理时实现降低复杂度。我们将其形式化为微专家的混合模型，称为$\\mu$-MoE。实验结果表明，$\\mu$-MoE可以在运行时动态适应任务/提示相关的结构稀疏性。', 'title_zh': '$\\mu$-MoE: 测试时裁剪作为细粒度混合专家模型'}
{'arxiv_id': 'arXiv:2505.18440', 'title': 'Efficient Long CoT Reasoning in Small Language Models', 'authors': 'Zhaoyang Wang, Jinqi Jiang, Tian Qiu, Hui Liu, Xianfeng Tang, Huaxiu Yao', 'link': 'https://arxiv.org/abs/2505.18440', 'abstract': 'Recent large reasoning models such as DeepSeek-R1 exhibit strong complex problems solving abilities by generating long chain-of-thought (CoT) reasoning steps. It is challenging to directly train small language models (SLMs) to emerge long CoT. Thus, distillation becomes a practical method to enable SLMs for such reasoning ability. However, the long CoT often contains a lot of redundant contents (e.g., overthinking steps) which may make SLMs hard to learn considering their relatively poor capacity and generalization. To address this issue, we propose a simple-yet-effective method to prune unnecessary steps in long CoT, and then employ an on-policy method for the SLM itself to curate valid and useful long CoT training data. In this way, SLMs can effectively learn efficient long CoT reasoning and preserve competitive performance at the same time. Experimental results across a series of mathematical reasoning benchmarks demonstrate the effectiveness of the proposed method in distilling long CoT reasoning ability into SLMs which maintains the competitive performance but significantly reduces generating redundant reasoning steps.', 'abstract_zh': '近期的大规模推理模型如DeepSeek-R1通过生成长链条推理步骤（CoT）展现出强大的复杂问题解决能力。直接训练小型语言模型（SLMs）以生成长链条推理步骤具有挑战性。因此，蒸馏成为一种实用的方法以使SLMs具备这种推理能力。然而，长链条推理步骤通常包含许多冗余内容（例如，过度思考的步骤），这可能使得SLMs难以学习，考虑到它们相对较差的容量和泛化能力。为了解决这个问题，我们提出了一种简单有效的方法来修剪长链条推理步骤中的不必要的步骤，并利用一种在线策略方法对SLMs本身进行训练，以生成有效且有用的长链条推理训练数据。这样，SLMs可以有效地学习高效的长链条推理，并同时保持竞争力。一系列数学推理基准的实验结果证明，所提出的方法在将长链条推理能力蒸馏到SLMs中的有效性，该方法能够保持竞争力的同时显著减少生成冗余推理步骤。', 'title_zh': '小语言模型中高效长链推理'}
{'arxiv_id': 'arXiv:2505.18426', 'title': 'Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps', 'authors': 'Khandakar Ashrafi Akbar, Md Nahiyan Uddin, Latifur Khan, Trayce Hockstad, Mizanur Rahman, Mashrur Chowdhury, Bhavani Thuraisingham', 'link': 'https://arxiv.org/abs/2505.18426', 'abstract': 'As connected and automated transportation systems evolve, there is a growing need for federal and state authorities to revise existing laws and develop new statutes to address emerging cybersecurity and data privacy challenges. This study introduces a Retrieval-Augmented Generation (RAG) based Large Language Model (LLM) framework designed to support policymakers by extracting relevant legal content and generating accurate, inquiry-specific responses. The framework focuses on reducing hallucinations in LLMs by using a curated set of domain-specific questions to guide response generation. By incorporating retrieval mechanisms, the system enhances the factual grounding and specificity of its outputs. Our analysis shows that the proposed RAG-based LLM outperforms leading commercial LLMs across four evaluation metrics: AlignScore, ParaScore, BERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and context-aware legal insights. This approach offers a scalable, AI-driven method for legislative analysis, supporting efforts to update legal frameworks in line with advancements in transportation technologies.', 'abstract_zh': '随着连接和自动运输系统的发展，联邦和州当局需要修订现有法律并制定新法规以应对新兴的网络安全和数据隐私挑战。本文介绍了一种基于检索增强生成（RAG）的大语言模型（LLM）框架，旨在通过提取相关法律内容并生成准确的、问题特定的回应来支持决策者。该框架通过使用特定领域的定制问题来指导响应生成，以减少LLM中的幻觉现象。通过结合检索机制，系统增强了输出的事实基础性和特定性。我们的分析表明，所提出的基于RAG的LLM在四个评估指标（AlignScore、ParaScore、BERTScore和ROUGE）上优于领先的商业LLM，证明了其在产生可靠且上下文相关的法律见解方面的有效性。该方法为立法分析提供了一种可扩展的、基于AI的方法，支持法律框架更新以适应运输技术的进步。', 'title_zh': '基于检索增强生成的大语言模型在弥补交通运输网络安全法律知识缺口中的应用'}
{'arxiv_id': 'arXiv:2505.18413', 'title': 'LatentLLM: Attention-Aware Joint Tensor Compression', 'authors': 'Toshiaki Koike-Akino, Xiangyu Chen, Jing Liu, Ye Wang, Wang, Matthew Brand', 'link': 'https://arxiv.org/abs/2505.18413', 'abstract': 'Modern foundation models such as large language models (LLMs) and large multi-modal models (LMMs) require a massive amount of computational and memory resources. We propose a new framework to convert such LLMs/LMMs into a reduced-dimension latent structure. Our method extends a local activation-aware tensor decomposition to a global attention-aware joint tensor de-composition. Our framework can significantly improve the model accuracy over the existing model compression methods when reducing the latent dimension to realize computationally/memory-efficient LLMs/LLMs. We show the benefit on several benchmark including multi-modal reasoning tasks.', 'abstract_zh': '现代基础模型如大型语言模型（LLMs）和大型多模态模型（LMMs）需要大量的计算和内存资源。我们提出了一种新框架，将此类LLMs/LMMs转换为低维度的潜在结构。我们的方法将局部激活感知张量分解扩展为全局注意力感知联合张量分解。当降低潜在维度以实现计算/内存高效的LLMs/LMMs时，我们的框架可以在保持模型精度方面显著优于现有的模型压缩方法。我们在包括多模态推理任务在内的多个基准上展示了其优势。', 'title_zh': 'LatentLLM: 注意力引导的联合张量压缩'}
{'arxiv_id': 'arXiv:2505.18404', 'title': 'Thought calibration: Efficient and confident test-time scaling', 'authors': 'Menghua Wu, Cai Zhou, Stephen Bates, Tommi Jaakkola', 'link': 'https://arxiv.org/abs/2505.18404', 'abstract': "Reasoning large language models achieve impressive test-time scaling by thinking for longer, but this performance gain comes at significant compute cost. Directly limiting test-time budget hurts overall performance, but not all problems are equally difficult. We propose thought calibration to decide dynamically when thinking can be terminated. To calibrate our decision rule, we view a language model's growing body of thoughts as a nested sequence of reasoning trees, where the goal is to identify the point at which novel reasoning plateaus. We realize this framework through lightweight probes that operate on top of the language model's hidden representations, which are informative of both the reasoning structure and overall consistency of response. Based on three reasoning language models and four datasets, thought calibration preserves model performance with up to a 60% reduction in thinking tokens on in-distribution data, and up to 20% in out-of-distribution data.", 'abstract_zh': '大规模语言模型通过延长推理时间实现显著的测试时扩展，但这种性能提升伴随着巨大的计算成本。直接限制测试时间预算会损害整体性能，但并非所有问题都同样困难。我们提出推理校准以动态决定何时终止推理。通过轻量级探针对语言模型隐藏表示的操作，我们实现这一框架，该探针能提供推理结构和响应总体一致性的信息，以识别新颖推理 plateau 出现的点。基于三种推理语言模型和四个数据集，推理校准在领域内数据上可以将推理令牌减少最多 60%，在领域外数据上可以减少最多 20%。', 'title_zh': '思维校准：高效且自信的测试时缩放'}
{'arxiv_id': 'arXiv:2505.18369', 'title': 'Small Models, Smarter Learning: The Power of Joint Task Training', 'authors': 'Csaba Both, Benjamin Hoover, Hendrik Strobelt, Dmitry Krotov, Daniel Karl I. Weidele, Mauro Martino, Nima Dehmamy', 'link': 'https://arxiv.org/abs/2505.18369', 'abstract': 'The ability of a model to learn a task depends strongly on both the task difficulty and the model size. We aim to understand how task difficulty relates to the minimum number of parameters required for learning specific tasks in small transformer models. Our study focuses on the ListOps dataset, which consists of nested mathematical operations. We gradually increase task difficulty by introducing new operations or combinations of operations into the training data. We observe that sum modulo n is the hardest to learn. Curiously, when combined with other operations such as maximum and median, the sum operation becomes easier to learn and requires fewer parameters. We show that joint training not only improves performance but also leads to qualitatively different model behavior. We show evidence that models trained only on SUM might be memorizing and fail to capture the number structure in the embeddings. In contrast, models trained on a mixture of SUM and other operations exhibit number-like representations in the embedding space, and a strong ability to distinguish parity. Furthermore, the SUM-only model relies more heavily on its feedforward layers, while the jointly trained model activates the attention mechanism more. Finally, we show that learning pure SUM can be induced in models below the learning threshold of pure SUM, by pretraining them on MAX+MED. Our findings indicate that emergent abilities in language models depend not only on model size, but also the training curriculum.', 'abstract_zh': '模型学习任务的能力强烈依赖于任务难度和模型大小。我们旨在理解任务难度与在小型变压器模型中学习特定任务所需最小参数数量之间的关系。我们研究集中在ListOps数据集中，该数据集包含嵌套的数学运算。我们通过引入新的运算或运算组合逐渐增加任务难度。我们观察到，模n求和是最难学习的。有趣的是，当与其他运算（如最大值和中值）结合时，求和运算变得更容易学习，并且所需的参数更少。我们表明，联合训练不仅改善了性能，还导致了模型行为上的质的差异。我们展示了仅在SUM上训练的模型可能在嵌入中记忆数据而未能捕捉数字结构的证据。相反，训练在SUM和其他运算混合数据上的模型在嵌入空间中表现出数字似的行为，并且在区分奇偶性方面有强大的能力。此外，仅训练在SUM上的模型更多依赖其前馈层，而联合训练的模型则更多激活了注意力机制。最后，我们展示了通过在MAX+MED数据上进行预训练，可以在低于纯SUM学习阈值的模型中诱导出纯SUM的学习能力。我们的研究结果表明，语言模型中的 emergent 能力不仅依赖于模型大小，还依赖于训练课程。', 'title_zh': '小模型，更聪明的学习：联合任务训练的威力'}
{'arxiv_id': 'arXiv:2505.18356', 'title': 'The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs', 'authors': 'Lucas Bandarkar, Nanyun Peng', 'link': 'https://arxiv.org/abs/2505.18356', 'abstract': 'Large language models (LLMs) still struggle across tasks outside of high-resource languages. In this work, we investigate cross-lingual transfer to lower-resource languages where task-specific post-training data is scarce. Building on prior work, we first validate that the subsets of model parameters that matter most for mathematical reasoning and multilingual capabilities are distinctly non-overlapping. To exploit this implicit separability between task and target language parameterization, we develop and analyze numerous modular frameworks to improve the composition of the two during fine-tuning. These methods generally employ freezing parameters or post hoc model merging to assign math and language improvement to different key parts of the LLM. In the absence of in-language math data, we demonstrate that the modular approaches successfully improve upon baselines across three languages, four models, and two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most consistently successful modular method to be fine-tuning separate language and math experts and model merging via Layer-Swapping, somewhat surprisingly. We offer possible explanations for this result via recent works on the linearity of task vectors. We further explain this by empirically showing that reverting less useful fine-tuning updates after training often outperforms freezing them from the start.', 'abstract_zh': '大型语言模型（LLMs）在低资源语言任务上仍然表现不佳。本文研究了跨语言迁移学习在低资源语言任务中的应用，尤其是在任务特定的后训练数据稀缺的情况下。基于先前的工作，我们首先验证了对于数学推理和多语言能力最重要的模型参数子集是明显不重叠的。为了利用任务参数和目标语言参数之间的这种隐含分离性，我们开发并分析了多种模块化框架，在微调过程中改善这两个方面的组合。这些方法通常采用冻结参数或后 hoc 模型合并的方式来将数学和语言改进分配到 LLM 的不同关键部分。在缺乏本语言数学数据的情况下，我们展示了模块化方法能够跨三种语言、四种模型和两种微调范式（全量微调和LoRA）显著改进基线性能。此外，我们确定了一种最一致有效的模块化方法，即分别微调语言专家和数学专家，并通过分层置换进行模型合并，这有些出乎意料。我们通过最近有关任务向量线性性的研究工作提供了这种结果的可能解释。我们进一步通过实验证明，在训练后逆转一些不太有用的微调更新通常优于从一开始就冻结它们。', 'title_zh': '模型合并对大语言模型跨语言迁移的意外有效性'}
{'arxiv_id': 'arXiv:2505.18350', 'title': 'Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?', 'authors': 'Waleed Reda, Abhinav Jangda, Krishna Chintalapudi', 'link': 'https://arxiv.org/abs/2505.18350', 'abstract': 'As Large Language Models (LLMs) are increasingly being adopted for narrow tasks - such as medical question answering or sentiment analysis - and deployed in resource-constrained settings, a key question arises: how many parameters does a task actually need? In this work, we present LLM-Sieve, the first comprehensive framework for task-specific pruning of LLMs that achieves 20-75% parameter reduction with only 1-5% accuracy degradation across diverse domains. Unlike prior methods that apply uniform pruning or rely on low-rank approximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns task-aware joint projections to better approximate output behavior, and (ii) employs a Genetic Algorithm to discover differentiated pruning levels for each matrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization, and uniquely demonstrates strong generalization across datasets within the same task domain. Together, these results establish a practical and robust mechanism to generate smaller performant task-specific models.', 'abstract_zh': '大规模语言模型（LLMs）在资源受限环境中执行狭窄任务（如医疗问答或情感分析）时，一个关键问题出现了：任务究竟需要多少参数？在本文中，我们提出了LLM-Sieve，这是首个针对特定任务剪枝的大规模语言模型综合框架，实现了20%至75%的参数减少，同时准确率下降仅为1%至5%，并在多种领域中展示了广泛适用性。与以往应用均匀剪枝或孤立地依赖权重矩阵或输入的低秩逼近的方法不同，LLM-Sieve通过（i）学习任务感知的联合投影以更好地近似输出行为，以及（ii）采用遗传算法发现对每个矩阵不同的剪枝水平，来实现这一目标。LLM-Sieve完全兼容LoRA微调和量化，并且独特地展示了在同一任务领域内的不同数据集间强大的泛化能力。这些结果共同建立了生成更小且性能良好的特定任务模型的实用和 robust 机制。', 'title_zh': '基于LLM-Sieve的特定任务剪枝：你的任务究竟需要多少参数？'}
{'arxiv_id': 'arXiv:2505.18333', 'title': 'A Critical Evaluation of Defenses against Prompt Injection Attacks', 'authors': 'Yuqi Jia, Zedian Shao, Yupei Liu, Jinyuan Jia, Dawn Song, Neil Zhenqiang Gong', 'link': 'https://arxiv.org/abs/2505.18333', 'abstract': 'Large Language Models (LLMs) are vulnerable to prompt injection attacks, and several defenses have recently been proposed, often claiming to mitigate these attacks successfully. However, we argue that existing studies lack a principled approach to evaluating these defenses. In this paper, we argue the need to assess defenses across two critical dimensions: (1) effectiveness, measured against both existing and adaptive prompt injection attacks involving diverse target and injected prompts, and (2) general-purpose utility, ensuring that the defense does not compromise the foundational capabilities of the LLM. Our critical evaluation reveals that prior studies have not followed such a comprehensive evaluation methodology. When assessed using this principled approach, we show that existing defenses are not as successful as previously reported. This work provides a foundation for evaluating future defenses and guiding their development. Our code and data are available at: this https URL.', 'abstract_zh': '大型语言模型（LLMs）易受提示注入攻击的影响，已有研究提出了多种防御措施，常称其能够成功缓解这些攻击。然而，我们认为现有研究缺乏一种原则性的评估方法。本文主张从两个关键维度评估防御措施：（1）效果，基于现有的和适应性的、涉及多种目标和注入提示的提示注入攻击进行衡量；（2）通用实用性，确保防御措施不损害LLM的基本功能。我们的批判性评估表明，先前的研究并未遵循这种全面的评估方法。运用这种方法进行评估后，我们显示现有的防御措施并不如先前所报道的那么成功。本文为评估未来的防御措施和指导其发展提供了基础。我们的代码和数据可在以下链接获取：this https URL。', 'title_zh': '对提示注入攻击防御措施的批判性评估'}
{'arxiv_id': 'arXiv:2505.18331', 'title': 'PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language', 'authors': 'Naghmeh Jamali, Milad Mohammadi, Danial Baledi, Zahra Rezvani, Hesham Faili', 'link': 'https://arxiv.org/abs/2505.18331', 'abstract': 'Medical consumer question answering (CQA) is crucial for empowering patients by providing personalized and reliable health information. Despite recent advances in large language models (LLMs) for medical QA, consumer-oriented and multilingual resources, particularly in low-resource languages like Persian, remain sparse. To bridge this gap, we present PerMedCQA, the first Persian-language benchmark for evaluating LLMs on real-world, consumer-generated medical questions. Curated from a large medical QA forum, PerMedCQA contains 68,138 question-answer pairs, refined through careful data cleaning from an initial set of 87,780 raw entries. We evaluate several state-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a novel rubric-based evaluation framework driven by an LLM grader, validated against expert human annotators. Our results highlight key challenges in multilingual medical QA and provide valuable insights for developing more accurate and context-aware medical assistance systems. The data is publicly available on this https URL', 'abstract_zh': '医疗消费者问答（CQA）对于通过提供个性化和可靠的健康信息来赋能患者至关重要。尽管近年来大型语言模型（LLMs）在医疗问答方面的进展显著，面向消费者且多语言的资源，特别是低资源语言如波斯语的资源仍十分稀少。为了弥合这一差距，我们提出了PerMedCQA，这是首个用于评估LLMs处理真实世界消费者生成的医疗问题的波斯语基准数据集。从一个大型医疗问答论坛精心挑选并清理，PerMedCQA包含68,138个问题-答案对，源自最初87,780个原始条目的精心筛选。我们使用MedJudge评估了多种最先进的多语言及指令调优的LLMs，MedJudge是一种由LLM评分员驱动的新型基于评价指标的评估框架，其结果已由专家人工注释者进行验证。我们的研究结果突显了多语言医疗问答中的关键挑战，并为开发更准确和更具有情境感知能力的医疗辅助系统提供了宝贵的见解。数据可从以下网址获取：此https URL。', 'title_zh': 'PerMedCQA：评价大型语言模型在波斯语医学消费者问答任务上的表现'}
{'arxiv_id': 'arXiv:2505.18322', 'title': 'Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4', 'authors': 'Zhuozhuo Joy Liu, Farhan Samir, Mehar Bhatia, Laura K. Nelson, Vered Shwartz', 'link': 'https://arxiv.org/abs/2505.18322', 'abstract': 'LLMs have been demonstrated to align with the values of Western or North American cultures. Prior work predominantly showed this effect through leveraging surveys that directly ask (originally people and now also LLMs) about their values. However, it is hard to believe that LLMs would consistently apply those values in real-world scenarios. To address that, we take a bottom-up approach, asking LLMs to reason about cultural norms in narratives from different cultures. We find that GPT-4 tends to generate norms that, while not necessarily incorrect, are significantly less culture-specific. In addition, while it avoids overtly generating stereotypes, the stereotypical representations of certain cultures are merely hidden rather than suppressed in the model, and such stereotypes can be easily recovered. Addressing these challenges is a crucial step towards developing LLMs that fairly serve their diverse user base.', 'abstract_zh': 'LLMs在不同文化叙事中的文化规范推理显示它们的文化特定性减弱，并且潜在刻板印象依旧存在：克服这些挑战是开发公平服务于多元化用户的基础。', 'title_zh': '全职工作是否弊大于利？GPT-4的社会规范偏差跨文化评估'}
{'arxiv_id': 'arXiv:2505.18286', 'title': 'Single-agent or Multi-agent Systems? Why Not Both?', 'authors': 'Mingyan Gao, Yanzi Li, Banruo Liu, Yifan Yu, Phillip Wang, Ching-Yu Lin, Fan Lai', 'link': 'https://arxiv.org/abs/2505.18286', 'abstract': 'Multi-agent systems (MAS) decompose complex tasks and delegate subtasks to different large language model (LLM) agents and tools. Prior studies have reported the superior accuracy performance of MAS across diverse domains, enabled by long-horizon context tracking and error correction through role-specific agents. However, the design and deployment of MAS incur higher complexity and runtime cost compared to single-agent systems (SAS). Meanwhile, frontier LLMs, such as OpenAI-o3 and Gemini-2.5-Pro, have rapidly advanced in long-context reasoning, memory retention, and tool usage, mitigating many limitations that originally motivated MAS designs. In this paper, we conduct an extensive empirical study comparing MAS and SAS across various popular agentic applications. We find that the benefits of MAS over SAS diminish as LLM capabilities improve, and we propose efficient mechanisms to pinpoint the error-prone agent in MAS. Furthermore, the performance discrepancy between MAS and SAS motivates our design of a hybrid agentic paradigm, request cascading between MAS and SAS, to improve both efficiency and capability. Our design improves accuracy by 1.1-12% while reducing deployment costs by up to 20% across various agentic applications.', 'abstract_zh': '多智能体系统（MAS）将复杂任务分解并委派给不同的大型语言模型（LLM）智能体和工具。前期研究显示，MAS在多个领域表现出更优异的准确性，这得益于其长时序语境追踪和通过特定角色智能体进行的错误纠正。然而，MAS的设计和部署比单智能体系统（SAS）更为复杂且运行成本更高。同时，前沿的LLM，如OpenAI-o3和Gemini-2.5-Pro，在长上下文推理、记忆保留和工具使用方面取得了快速进步，缓解了许多原本促使设计MAS的局限性。在本文中，我们对MAS和SAS在多种流行的智能体应用中进行了广泛的实证研究。我们发现，随着LLM能力的提升，MAS相对于SAS的优势逐渐减弱，并提出有效的机制以确定MAS中的易出错智能体。此外，MAS和SAS之间的性能差异促使我们设计了一种混合型智能体范式——MAS和SAS之间的请求级联，以提高效率和能力。我们的设计在多种智能体应用中提高了1.1%-12%的准确性并降低了20%的部署成本。', 'title_zh': '单智能体系统还是多智能体系统？何不兼而有之？'}
{'arxiv_id': 'arXiv:2505.18283', 'title': 'TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification', 'authors': 'Jianghao Wu, Feilong Tang, Yulong Li, Ming Hu, Haochen Xue, Shoaib Jameel, Yutong Xie, Imran Razzak', 'link': 'https://arxiv.org/abs/2505.18283', 'abstract': 'Recent advances such as Chain-of-Thought prompting have significantly improved large language models (LLMs) in zero-shot medical reasoning. However, prompting-based methods often remain shallow and unstable, while fine-tuned medical LLMs suffer from poor generalization under distribution shifts and limited adaptability to unseen clinical scenarios. To address these limitations, we present TAGS, a test-time framework that combines a broadly capable generalist with a domain-specific specialist to offer complementary perspectives without any model fine-tuning or parameter updates. To support this generalist-specialist reasoning process, we introduce two auxiliary modules: a hierarchical retrieval mechanism that provides multi-scale exemplars by selecting examples based on both semantic and rationale-level similarity, and a reliability scorer that evaluates reasoning consistency to guide final answer aggregation. TAGS achieves strong performance across nine MedQA benchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and improving a vanilla 7B model from 14.1% to 23.9%. These results surpass several fine-tuned medical LLMs, without any parameter updates. The code will be available at this https URL.', 'abstract_zh': 'Recent Advances in Zero-Shot Medical Reasoning with TAGS: A Test-Time Framework Combining Generalist and Specialist Models', 'title_zh': 'TAGS：一种测试时通用专家检索增强推理与验证框架'}
{'arxiv_id': 'arXiv:2505.18279', 'title': 'Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control', 'authors': 'Alireza Rezazadeh, Zichao Li, Ange Lou, Yuying Zhao, Wei Wei, Yujia Bao', 'link': 'https://arxiv.org/abs/2505.18279', 'abstract': 'Complex tasks are increasingly delegated to ensembles of specialized LLM-based agents that reason, communicate, and coordinate actions-both among themselves and through interactions with external tools, APIs, and databases. While persistent memory has been shown to enhance single-agent performance, most approaches assume a monolithic, single-user context-overlooking the benefits and challenges of knowledge transfer across users under dynamic, asymmetric permissions. We introduce Collaborative Memory, a framework for multi-user, multi-agent environments with asymmetric, time-evolving access controls encoded as bipartite graphs linking users, agents, and resources. Our system maintains two memory tiers: (1) private memory-private fragments visible only to their originating user; and (2) shared memory-selectively shared fragments. Each fragment carries immutable provenance attributes (contributing agents, accessed resources, and timestamps) to support retrospective permission checks. Granular read policies enforce current user-agent-resource constraints and project existing memory fragments into filtered transformed views. Write policies determine fragment retention and sharing, applying context-aware transformations to update the memory. Both policies may be designed conditioned on system, agent, and user-level information. Our framework enables safe, efficient, and interpretable cross-user knowledge sharing, with provable adherence to asymmetric, time-varying policies and full auditability of memory operations.', 'abstract_zh': '基于多用户、多代理环境的协作记忆框架', 'title_zh': '协作记忆：LLM代理中的多用户记忆共享及动态访问控制'}
{'arxiv_id': 'arXiv:2505.18244', 'title': 'Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models', 'authors': 'Yukin Zhang, Qi Dong', 'link': 'https://arxiv.org/abs/2505.18244', 'abstract': 'Large Transformer based language models achieve remarkable performance but remain opaque in how they plan, structure, and realize text. We introduce Multi_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework that factorizes generation into three semantic scales_global context, intermediate structure, and local word choices and aligns each scale with specific layer ranges in Transformer architectures. To identify scale boundaries, we propose two complementary metrics: attention span thresholds and inter layer mutual information peaks. Across four representative models (GPT-2, BERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global partitions, corroborated by probing tasks and causal interventions. We find that decoder_only models allocate more layers to intermediate and global processing while encoder_only models emphasize local feature extraction. Through targeted interventions, we demonstrate that local scale manipulations primarily influence lexical diversity, intermediate-scale modifications affect sentence structure and length, and global_scale perturbations impact discourse coherence all with statistically significant effects. MSPGT thus offers a unified, architecture-agnostic method for interpreting, diagnosing, and controlling large language models, bridging the gap between mechanistic interpretability and emergent capabilities.', 'abstract_zh': '多尺度概率生成理论：一种区分生成文本层次结构的统一框架', 'title_zh': '多尺度概率生成理论：一种解释大型语言模型的分层框架'}
{'arxiv_id': 'arXiv:2505.18240', 'title': 'Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback', 'authors': 'Ananth Muppidi, Tarak Das, Sambaran Bandyopadhyay, Tripti Shukla, Dharun D A', 'link': 'https://arxiv.org/abs/2505.18240', 'abstract': 'The generation of presentation slides automatically is an important problem in the era of generative AI. This paper focuses on evaluating multimodal content in presentation slides that can effectively summarize a document and convey concepts to a broad audience. We introduce a benchmark dataset, RefSlides, consisting of human-made high-quality presentations that span various topics. Next, we propose a set of metrics to characterize different intrinsic properties of the content of a presentation and present REFLEX, an evaluation approach that generates scores and actionable feedback for these metrics. We achieve this by generating negative presentation samples with different degrees of metric-specific perturbations and use them to fine-tune LLMs. This reference-free evaluation technique does not require ground truth presentations during inference. Our extensive automated and human experiments demonstrate that our evaluation approach outperforms classical heuristic-based and state-of-the-art large language model-based evaluations in generating scores and explanations.', 'abstract_zh': '生成演示文稿自动生成是生成AI时代的重要问题。本文专注于评估演示文稿中的多模态内容，这些内容能够有效总结文档并传达给广泛受众的概念。我们介绍了一个基准数据集RefSlides，包含高质量的人工制作的演示文稿，涵盖多个主题。接下来，我们提出了一套度量标准来表征演示文稿内容的不同内在属性，并介绍了REFLEX评估方法，该方法为这些度量标准生成评分和可操作的反馈。我们通过生成具有不同程度度量标准特定扰动的负演示样本，并使用它们微调LLM来实现这一目标。这种参考自由的评估技术在推理时不需真实演示文稿作为 ground truth。我们的广泛自动和人工实验表明，我们的评估方法在生成评分和解释方面优于经典启发式方法和最新的基于大型语言模型的方法。', 'title_zh': '用负样本驯服大型语言模型：一个基于可操作反馈评估展示内容的参考免费框架'}
{'arxiv_id': 'arXiv:2505.18237', 'title': 'Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens', 'authors': 'Xixian Yong, Xiao Zhou, Yingying Zhang, Jinlin Li, Yefeng Zheng, Xian Wu', 'link': 'https://arxiv.org/abs/2505.18237', 'abstract': 'The recent rise of Large Reasoning Models (LRMs) has significantly improved multi-step reasoning performance, but often at the cost of generating excessively long reasoning chains. This paper revisits the efficiency of such reasoning processes through an information-theoretic lens, revealing a fundamental trade-off between reasoning length and semantic efficiency. We propose two metrics, InfoBias and InfoGain, to quantify divergence from ideal reasoning paths and stepwise information contribution, respectively. Empirical analyses show that longer reasoning chains tend to exhibit higher information bias and diminishing information gain, especially for incorrect answers. Motivated by these findings, we introduce an entropy-based Adaptive Think strategy that dynamically halts reasoning once confidence is sufficiently high, improving efficiency while maintaining competitive accuracy. Compared to the Vanilla Think approach (default mode), our strategy yields a 1.10% improvement in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six benchmark tasks spanning diverse reasoning types and difficulty levels, demonstrating superior efficiency and reasoning performance. These results underscore the promise of entropy-based methods for enhancing both accuracy and cost-effiiciency in large language model deployment.', 'abstract_zh': '大型推理模型近期的发展显著提高了多步推理性能，但往往以生成过长的推理链为代价。本文从信息论的角度重新审视了此类推理过程的本质，揭示了推理长度与语义效率之间基本的权衡关系。我们提出了两个度量标准，分别为InfoBias和InfoGain，分别用于量化偏离理想推理路径的程度和逐步信息贡献。实证分析表明，较长的推理链往往会表现出更高的信息偏差和递减的信息增益，尤其是在错误答案的情况下更为明显。基于这些发现，我们引入了一种基于熵的自适应推理策略，该策略能够动态停止推理以实现高效性，并维持竞争力的准确率。与Vanilla Think方法（默认模式）相比，我们的策略在QwQ-32B上的六项基准任务中实现了平均准确率1.10%的提升和50.80%的令牌使用量减少，显示出更高的效率和推理性能。这些结果突显了基于熵的方法在提升大型语言模型部署的准确性和成本效率方面的潜力。', 'title_zh': '思考还是不思考？通过信息论视角探索大型推理模型的思考效率'}
{'arxiv_id': 'arXiv:2505.18235', 'title': 'The Origins of Representation Manifolds in Large Language Models', 'authors': 'Alexander Modell, Patrick Rubin-Delanchy, Nick Whiteley', 'link': 'https://arxiv.org/abs/2505.18235', 'abstract': "There is a large ongoing scientific effort in mechanistic interpretability to map embeddings and internal representations of AI systems into human-understandable concepts. A key element of this effort is the linear representation hypothesis, which posits that neural representations are sparse linear combinations of `almost-orthogonal' direction vectors, reflecting the presence or absence of different features. This model underpins the use of sparse autoencoders to recover features from representations. Moving towards a fuller model of features, in which neural representations could encode not just the presence but also a potentially continuous and multidimensional value for a feature, has been a subject of intense recent discourse. We describe why and how a feature might be represented as a manifold, demonstrating in particular that cosine similarity in representation space may encode the intrinsic geometry of a feature through shortest, on-manifold paths, potentially answering the question of how distance in representation space and relatedness in concept space could be connected. The critical assumptions and predictions of the theory are validated on text embeddings and token activations of large language models.", 'abstract_zh': '大规模正在进行中有系统解释性的科学研究致力于将AI系统的嵌入和内部表示映射到人类可理解的概念。这一努力的关键要素是线性表示假设，该假设认为神经表示是几乎正交的方向向量的稀疏线性组合，反映出不同特征的存在与否。该模型支持使用稀疏自编码器从表示中恢复特征。朝着更具综合性的特征模型发展，该模型中神经表示不仅能编码特征的存在，还能编码特征的一个潜在的连续且多维的价值，这是近期讨论的热点。我们描述了为何以及如何将特征表示为流形，并特别说明了表示空间中的余弦相似度可能通过流形上的最短路径编码特征的内在几何结构，从而有可能解释表示空间中的距离与概念空间中的相关性之间的关系。该理论的关键假设和预测在文本嵌入和大型语言模型的标记激活中得到了验证。', 'title_zh': '大型语言模型中表示流形的起源'}
{'arxiv_id': 'arXiv:2505.18232', 'title': 'ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning', 'authors': 'Mingkuan Feng, Jinyang Wu, Siyuan Liu, Shuai Zhang, Hongjian Fang, Ruihan Jin, Feihu Che, Pengpeng Shao, Zhengqi Wen, Jianhua Tao', 'link': 'https://arxiv.org/abs/2505.18232', 'abstract': "The deployment of Large language models (LLMs) in many fields is largely hindered by their high computational and memory costs. Recent studies suggest that LLMs exhibit sparsity, which can be used for pruning. Previous pruning methods typically follow a prune-then-finetune paradigm. Since the pruned parts still contain valuable information, statically removing them without updating the remaining parameters often results in irreversible performance degradation, requiring costly recovery fine-tuning (RFT) to maintain performance. To address this, we propose a novel paradigm: first apply regularization, then prune. Based on this paradigm, we propose ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning. We multiply the output of each transformer layer by an initial weight, then we iteratively learn the weights of each transformer layer by using a small amount of data in a simple way. After that, we apply regularization to the difference between the output and input of the layers with smaller weights, forcing the information to be transferred to the remaining layers. Compared with direct pruning, ELDeR reduces the information loss caused by direct parameter removal, thus better preserving the model's language modeling ability. Experimental results show that ELDeR achieves superior performance compared with powerful layer-wise structured pruning methods, while greatly reducing RFT computational costs. Since ELDeR is a layer-wise pruning method, its end-to-end acceleration effect is obvious, making it a promising technique for efficient LLMs.", 'abstract_zh': '大型语言模型（LLMs）在许多领域的部署受到其高计算和内存成本的限制。近期的研究表明，LLMs表现出稀疏性，可以被用于剪枝。之前的剪枝方法通常遵循先剪枝后微调的范式。由于剪枝后的部分仍然包含有价值的信息，如果静态移除这些部分而不更新剩余参数，往往会导致不可逆的性能下降，需要昂贵的恢复微调（RFT）来维持性能。为了解决这一问题，我们提出了一种新的范式：先应用正则化，再剪枝。基于这一范式，我们提出了ELDeR：通过数据驱动的正则化逐层剪枝获得高效LLMs。我们首先对每个变压器层的输出乘以初始权重，然后通过少量数据简单地学习每个变压器层的权重。之后，我们对权重较小的层的输出和输入之间的差异应用正则化，迫使信息传递到剩余的层。与直接剪枝相比，ELDeR减少了直接移除参数所导致的信息损失，从而更好地保留了模型的语言建模能力。实验结果表明，ELDeR在性能上优于强大的逐层结构化剪枝方法，同时大大降低了RFT的计算成本。由于ELDeR是一种逐层剪枝方法，其端到端加速效果明显，因此是一种有望用于高效LLMs的技术。', 'title_zh': 'ELDeR: 通过数据驱动正则化分层剪枝实现高效LLMs'}
{'arxiv_id': 'arXiv:2505.18231', 'title': 'NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache', 'authors': 'Donghyun Son, Euntae Choi, Sungjoo Yoo', 'link': 'https://arxiv.org/abs/2505.18231', 'abstract': 'Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.', 'abstract_zh': '无监督归一化量化(NSNQuant):一种无需校准的数据关键值缓存低比特压缩技术', 'title_zh': 'NSNQuant: 一种用于KV缓存无校准低比特向量量化的方法，双归一化方式'}
{'arxiv_id': 'arXiv:2505.18227', 'title': 'Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality', 'authors': 'Zhenglun Kong, Yize Li, Fanhu Zeng, Lei Xin, Shvat Messica, Xue Lin, Pu Zhao, Manolis Kellis, Hao Tang, Marinka Zitnik', 'link': 'https://arxiv.org/abs/2505.18227', 'abstract': 'In Transformer architectures, tokens\\textemdash discrete units derived from raw data\\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input\'s essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, and broader ML and scientific domains. We highlight its potential to drive new model architectures and learning strategies that improve robustness, increase interpretability, and better align with the objectives of generative modeling.', 'abstract_zh': '在 transformer 架构中，通过将输入分割为固定长度片段形成的令牌（离散的源自原始数据的基本单元）映射到嵌入，使得能够并行进行注意力计算并保留输入的关键信息。由于变压器自注意力机制的二次计算复杂性，令牌减少主要被用作一种效率策略，特别是在单视图和语言领域，它有助于平衡计算成本、内存使用和推理延迟。尽管取得了这些进展，本文认为在大型生成模型时代，令牌减少不应仅仅局限于传统的效率导向角色。相反，我们应该将其定位为生成建模的基本原则，对模型架构和更广泛的应用产生关键影响。具体而言，我们认为在视觉、语言和多模态系统中，令牌减少可以：（i）促进更深的多模态整合和对齐，（ii）减轻“过度思考”和幻觉现象，（iii）保持长输入的一致性，以及（iv）增强训练稳定性等。我们将重新定义令牌减少，而不仅仅是作为一种效率度量。通过这样做，我们指出了未来研究的前景，包括算法设计、基于强化学习的令牌减少、适用于上下文学习的令牌优化以及更广泛的机器学习和科学领域。我们强调了其在推动新的模型架构和学习策略方面的潜力，这些策略将提高鲁棒性、增强可解释性，并更好地与生成建模的目标相一致。', 'title_zh': 'Token Reduction 应超越效率在生成模型中的应用——从视觉、语言到多模态'}
{'arxiv_id': 'arXiv:2505.18223', 'title': 'IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis', 'authors': 'Hanyu Li, Haoyu Liu, Tingyu Zhu, Tianyu Guo, Zeyu Zheng, Xiaotie Deng, Michael I. Jordan', 'link': 'https://arxiv.org/abs/2505.18223', 'abstract': "Large Language Models (LLMs) show promise as data analysis agents, but existing benchmarks overlook the iterative nature of the field, where experts' decisions evolve with deeper insights of the dataset. To address this, we introduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round interactive scenarios. Derived from complex Kaggle notebooks, tasks are presented as sequential natural language instructions by an LLM-simulated user. Agent performance is judged by comparing its final numerical output to the human-derived baseline. Initial results show that even state-of-the-art coding agents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting limitations not evident in single-turn tests. This work underscores the need to improve LLMs' multi-round capabilities for building more reliable data analysis agents, highlighting the necessity of achieving a balance between instruction following and reasoning.", 'abstract_zh': '大语言模型（LLMs）作为数据分析代理展现出潜力，但现有基准测试忽略了该领域逐步深入分析数据集过程中专家决策的迭代性。为解决这一问题，我们引入了IDA-Bench，这是一个评估LLM代理在多轮交互场景中的新型基准测试。任务来源于复杂的Kaggle笔记本，并由LLM模拟用户以顺序自然语言指令的形式呈现。代理的表现通过将其最终的数值输出与人类基准进行比较来评判。初步结果显示，即使是最先进的编码代理（如Claude-3.7-thinking）也仅能在不到50%的任务上成功，突显了单轮测试中未显现的局限性。本研究强调了提升LLMs多轮能力的重要性，以构建更可靠的数据分析代理，并强调了指令跟随与推理之间需要取得平衡。', 'title_zh': 'IDA-Bench: 评估交互引导数据分析中的大规模语言模型'}
{'arxiv_id': 'arXiv:2505.18220', 'title': 'Navigating Pitfalls: Evaluating LLMs in Machine Learning Programming Education', 'authors': 'Smitha Kumar, Michael A. Lones, Manuel Maarek, Hind Zantout', 'link': 'https://arxiv.org/abs/2505.18220', 'abstract': 'The rapid advancement of Large Language Models (LLMs) has opened new avenues in education. This study examines the use of LLMs in supporting learning in machine learning education; in particular, it focuses on the ability of LLMs to identify common errors of practice (pitfalls) in machine learning code, and their ability to provide feedback that can guide learning. Using a portfolio of code samples, we consider four different LLMs: one closed model and three open models. Whilst the most basic pitfalls are readily identified by all models, many common pitfalls are not. They particularly struggle to identify pitfalls in the early stages of the ML pipeline, especially those which can lead to information leaks, a major source of failure within applied ML projects. They also exhibit limited success at identifying pitfalls around model selection, which is a concept that students often struggle with when first transitioning from theory to practice. This questions the use of current LLMs to support machine learning education, and also raises important questions about their use by novice practitioners. Nevertheless, when LLMs successfully identify pitfalls in code, they do provide feedback that includes advice on how to proceed, emphasising their potential role in guiding learners. We also compare the capability of closed and open LLM models, and find that the gap is relatively small given the large difference in model sizes. This presents an opportunity to deploy, and potentially customise, smaller more efficient LLM models within education, avoiding risks around cost and data sharing associated with commercial models.', 'abstract_zh': '大型语言模型在机器学习教育中的应用：识别代码中的常见错误及其反馈能力研究', 'title_zh': '导航陷阱：评估大规模语言模型在机器学习编程教育中的作用'}
{'arxiv_id': 'arXiv:2505.18218', 'title': 'CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games', 'authors': 'Shuhang Xu, Fangwei Zhong', 'link': 'https://arxiv.org/abs/2505.18218', 'abstract': "Metaphors are a crucial way for humans to express complex or subtle ideas by comparing one concept to another, often from a different domain. However, many large language models (LLMs) struggle to interpret and apply metaphors in multi-agent language games, hindering their ability to engage in covert communication and semantic evasion, which are crucial for strategic communication. To address this challenge, we introduce CoMet, a framework that enables LLM-based agents to engage in metaphor processing. CoMet combines a hypothesis-based metaphor reasoner with a metaphor generator that improves through self-reflection and knowledge integration. This enhances the agents' ability to interpret and apply metaphors, improving the strategic and nuanced quality of their interactions. We evaluate CoMet on two multi-agent language games - Undercover and Adversarial Taboo - which emphasize Covert Communication and Semantic Evasion. Experimental results demonstrate that CoMet significantly enhances the agents' ability to communicate strategically using metaphors.", 'abstract_zh': '比喻是人类表达复杂或微妙想法的重要方式，通过将一个概念与另一个概念相比较，通常来自不同的领域。然而，许多大型语言模型（LLMs）在多智能体语言游戏中难以解释和应用比喻，这阻碍了它们进行隐蔽沟通和语义规避的能力，这是战略性沟通的关键。为了解决这一挑战，我们引入了CoMet框架，使基于LLM的智能体能够参与比喻处理。CoMet结合了基于假设的比喻推理器和一个通过自我反思和知识整合而改进的比喻生成器。这增强了智能体解释和应用比喻的能力，提高了它们互动的战略性和细致程度。我们分别在两个多智能体语言游戏中评估了CoMet——Undercover和Adversarial Taboo，这两个游戏强调隐蔽沟通和语义规避。实验结果表明，CoMet显著增强了智能体使用比喻进行战略性沟通的能力。', 'title_zh': 'CoMet: 以隐喻驱动的多代理语言游戏隐蔽通信'}
{'arxiv_id': 'arXiv:2505.18217', 'title': 'ABHINAYA -- A System for Speech Emotion Recognition In Naturalistic Conditions Challenge', 'authors': 'Soumya Dutta, Smruthi Balaji, Varada R, Viveka Salinamakki, Sriram Ganapathy', 'link': 'https://arxiv.org/abs/2505.18217', 'abstract': 'Speech emotion recognition (SER) in naturalistic settings remains a challenge due to the intrinsic variability, diverse recording conditions, and class imbalance. As participants in the Interspeech Naturalistic SER Challenge which focused on these complexities, we present Abhinaya, a system integrating speech-based, text-based, and speech-text models. Our approach fine-tunes self-supervised and speech large language models (SLLM) for speech representations, leverages large language models (LLM) for textual context, and employs speech-text modeling with an SLLM to capture nuanced emotional cues. To combat class imbalance, we apply tailored loss functions and generate categorical decisions through majority voting. Despite one model not being fully trained, the Abhinaya system ranked 4th among 166 submissions. Upon completion of training, it achieved state-of-the-art performance among published results, demonstrating the effectiveness of our approach for SER in real-world conditions.', 'abstract_zh': '自然环境中语音情绪识别（SER）仍然面临挑战，由于固有的变异性、多样的录音条件以及类别不平衡。作为专注于这些复杂性的Interspeech自然环境SER挑战赛的参与者，我们呈现了Abhinaya系统，该系统结合了基于语音、基于文本和语音文本模型。我们的方法针对语音表示微调自监督和语音大型语言模型（SLLM），利用大型语言模型（LLM）获取文本上下文，并采用SLLM进行语音文本建模以捕捉细微的情绪线索。为应对类别不平衡问题，我们应用了定制的损失函数并通过多数投票生成分类决策。尽管一个模型未完全训练，Abhinaya系统在166项提交中排名第四。在完成训练后，其性能达到了已发表结果中的最先进水平，展示了我们在实际条件下进行SER的有效方法。', 'title_zh': 'ABHINAYA -- 一种在自然条件下进行语音情感识别的系统挑战'}
{'arxiv_id': 'arXiv:2505.18215', 'title': 'Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?', 'authors': 'Junyan Zhang, Yiming Huang, Shuliang Liu, Yubo Gao, Xuming Hu', 'link': 'https://arxiv.org/abs/2505.18215', 'abstract': 'The rapid adoption of LLMs has overshadowed the potential advantages of traditional BERT-like models in text classification. This study challenges the prevailing "LLM-centric" trend by systematically comparing three category methods, i.e., BERT-like models fine-tuning, LLM internal state utilization, and zero-shot inference across six high-difficulty datasets. Our findings reveal that BERT-like models often outperform LLMs. We further categorize datasets into three types, perform PCA and probing experiments, and identify task-specific model strengths: BERT-like models excel in pattern-driven tasks, while LLMs dominate those requiring deep semantics or world knowledge. Based on this, we propose TaMAS, a fine-grained task selection strategy, advocating for a nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.', 'abstract_zh': 'LLM迅猛 adoption has overshadowed potential advantages of traditional BERT-like models in text classification: A systematic comparison across six high-difficulty datasets', 'title_zh': 'BERT-like双方向模型在大规模语言模型时代的文本分类任务中仍表现更优吗？'}
{'arxiv_id': 'arXiv:2505.18212', 'title': 'Towards medical AI misalignment: a preliminary study', 'authors': 'Barbara Puccio, Federico Castagna, Allan Tucker, Pierangelo Veltri', 'link': 'https://arxiv.org/abs/2505.18212', 'abstract': "Despite their staggering capabilities as assistant tools, often exceeding human performances, Large Language Models (LLMs) are still prone to jailbreak attempts from malevolent users. Although red teaming practices have already identified and helped to address several such jailbreak techniques, one particular sturdy approach involving role-playing (which we named `Goofy Game') seems effective against most of the current LLMs safeguards. This can result in the provision of unsafe content, which, although not harmful per se, might lead to dangerous consequences if delivered in a setting such as the medical domain. In this preliminary and exploratory study, we provide an initial analysis of how, even without technical knowledge of the internal architecture and parameters of generative AI models, a malicious user could construct a role-playing prompt capable of coercing an LLM into producing incorrect (and potentially harmful) clinical suggestions. We aim to illustrate a specific vulnerability scenario, providing insights that can support future advancements in the field.", 'abstract_zh': '尽管大型语言模型（LLMs）作为辅助工具展现出惊人的能力，甚至常常超越人类表现，但它们仍然容易受到恶意用户发起的“牢笼突破”攻击。尽管红队已识别并帮助应对了多种“牢笼突破”技术，一种名为“Goofy Game”的特定角色扮演方法似乎对当前大多数LLMs的安全防护措施都有效。这可能导致提供不安全的内容，虽然本身不一定有害，但在如医疗领域这样的环境中，可能会导致危险的后果。在本初步和探索性研究中，我们提供了一个初步分析，即使没有生成式AI模型内部架构和技术参数的知识，恶意用户如何构造一个角色扮演提示，以此迫使LLM生成错误（并可能有害）的临床建议。我们的目的是阐述一个特定的安全漏洞场景，为未来该领域的研究提供见解。', 'title_zh': '向医疗AI不对齐进发：一项初步研究'}
{'arxiv_id': 'arXiv:2505.18194', 'title': 'Large Language Model-Driven Distributed Integrated Multimodal Sensing and Semantic Communications', 'authors': 'Yubo Peng, Luping Xiang, Bingxin Zhang, Kun Yang', 'link': 'https://arxiv.org/abs/2505.18194', 'abstract': 'Traditional single-modal sensing systems-based solely on either radio frequency (RF) or visual data-struggle to cope with the demands of complex and dynamic environments. Furthermore, single-device systems are constrained by limited perspectives and insufficient spatial coverage, which impairs their effectiveness in urban or non-line-of-sight scenarios. To overcome these challenges, we propose a novel large language model (LLM)-driven distributed integrated multimodal sensing and semantic communication (LLM-DiSAC) framework. Specifically, our system consists of multiple collaborative sensing devices equipped with RF and camera modules, working together with an aggregation center to enhance sensing accuracy. First, on sensing devices, LLM-DiSAC develops an RF-vision fusion network (RVFN), which employs specialized feature extractors for RF and visual data, followed by a cross-attention module for effective multimodal integration. Second, a LLM-based semantic transmission network (LSTN) is proposed to enhance communication efficiency, where the LLM-based decoder leverages known channel parameters, such as transceiver distance and signal-to-noise ratio (SNR), to mitigate semantic distortion. Third, at the aggregation center, a transformer-based aggregation model (TRAM) with an adaptive aggregation attention mechanism is developed to fuse distributed features and enhance sensing accuracy. To preserve data privacy, a two-stage distributed learning strategy is introduced, allowing local model training at the device level and centralized aggregation model training using intermediate features. Finally, evaluations on a synthetic multi-view RF-visual dataset generated by the Genesis simulation engine show that LLM-DiSAC achieves a good performance.', 'abstract_zh': '基于大型语言模型驱动的分布式集成多模态传感与语义通信框架（LLM-DiSAC）', 'title_zh': '大型语言模型驱动的分布式集成多模态传感与语义通信'}
{'arxiv_id': 'arXiv:2505.18164', 'title': 'Model-Distributed Inference for Large Language Models at the Edge', 'authors': 'Davide Macario, Hulya Seferoglu, Erdem Koyuncu', 'link': 'https://arxiv.org/abs/2505.18164', 'abstract': 'We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM), a novel framework designed to facilitate the deployment of state-of-the-art large-language models (LLMs) across low-power devices at the edge. This is accomplished by dividing the model into multiple partitions, which are then assigned to different devices/nodes within the network. These nodes exchange intermediate activation vectors via device-to-device links, enabling collaborative computation. To enhance the efficiency of this process, we propose the "recurrent pipeline parallelism" technique, which reduces idle time on each device and facilitates parallel inference during the generation of multiple text sequences. By leveraging the combined computational resources of multiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the memory capacity of individual devices, making it possible to perform inference on low-cost hardware. Furthermore, as the number of participating devices increases, MDI-LLM boosts token generation throughput and reduces memory consumption per device.', 'abstract_zh': 'Model-Distributed Inference for Large-Language Models (MDI-LLM)', 'title_zh': '边缘设备上的大型语言模型分布式推理'}
{'arxiv_id': 'arXiv:2505.18156', 'title': 'InjectLab: A Tactical Framework for Adversarial Threat Modeling Against Large Language Models', 'authors': 'Austin Howard', 'link': 'https://arxiv.org/abs/2505.18156', 'abstract': "Large Language Models (LLMs) are changing the way people interact with technology. Tools like ChatGPT and Claude AI are now common in business, research, and everyday life. But with that growth comes new risks, especially prompt-based attacks that exploit how these models process language. InjectLab is a security framework designed to address that problem. This paper introduces InjectLab as a structured, open-source matrix that maps real-world techniques used to manipulate LLMs. The framework is inspired by MITRE ATT&CK and focuses specifically on adversarial behavior at the prompt layer. It includes over 25 techniques organized under six core tactics, covering threats like instruction override, identity swapping, and multi-agent exploitation. Each technique in InjectLab includes detection guidance, mitigation strategies, and YAML-based simulation tests. A Python tool supports easy execution of prompt-based test cases. This paper outlines the framework's structure, compares it to other AI threat taxonomies, and discusses its future direction as a practical, community-driven foundation for securing language models.", 'abstract_zh': '大型语言模型（LLMs）正在改变人们与技术互动的方式。ChatGPT和Claude AI等工具现已在商业、研究和日常生活中广泛应用。但随着这些模型的发展，新的风险也随之而来，尤其是基于提示的攻击，这些攻击利用了这些模型处理语言的方式。InjectLab是一种安全框架，旨在解决这些问题。本文介绍了InjectLab作为一个结构化的开源矩阵，用于映射用于操纵LLMs的实际技术。该框架借鉴了MITRE ATT&CK的方法，并专注于提示层的对抗行为。它涵盖了超过25种技术，并将其组织在六大核心战术之下，涵盖诸如指令重写、身份交换和多代理利用等威胁。InjectLab中的每种技术都包括检测指南、缓解策略以及基于YAML的模拟测试。一个Python工具支持基于提示的测试用例的便捷执行。本文概述了该框架的结构，将其与其他AI威胁分类进行比较，并讨论其作为实用的社区驱动基础框架的未来方向，用于保护语言模型。', 'title_zh': 'InjectLab：一个针对大型语言模型的对抗威胁建模战术框架'}
{'arxiv_id': 'arXiv:2505.17648', 'title': 'Simulating Macroeconomic Expectations using LLM Agents', 'authors': 'Jianhao Lin, Lexuan Sun, Yixin Yan', 'link': 'https://arxiv.org/abs/2505.17648', 'abstract': 'We introduce a novel framework for simulating macroeconomic expectation formation using Large Language Model-Empowered Agents (LLM Agents). By constructing thousands of LLM Agents equipped with modules for personal characteristics, prior expectations, and knowledge, we replicate a survey experiment involving households and experts on inflation and unemployment. Our results show that although the expectations and thoughts generated by LLM Agents are more homogeneous than those of human participants, they still effectively capture key heterogeneity across agents and the underlying drivers of expectation formation. Furthermore, a module-ablation exercise highlights the critical role of prior expectations in simulating such heterogeneity. This approach complements traditional survey methods and offers new insights into AI behavioral science in macroeconomic research.', 'abstract_zh': '我们介绍了一种使用大型语言模型赋能代理（LLM代理）模拟宏观经济预期形成的新框架。通过构建拥有个人特征、先期预期和知识模块的数千个LLM代理，我们重现了一项涉及家庭和专家关于通胀和失业的调查实验。我们的结果表明，尽管LLM代理生成的预期和想法比人类参与者更为一致，但它们仍然有效地捕捉了代理间的关键异质性和预期形成的基本驱动因素。此外，模块消融实验突显了先期预期在模拟这种异质性中的关键作用。该方法补充了传统的调查方法，并为宏观经济学研究中的人工智能行为科学提供了新的见解。', 'title_zh': '使用大语言模型代理模拟宏观经济预期'}
