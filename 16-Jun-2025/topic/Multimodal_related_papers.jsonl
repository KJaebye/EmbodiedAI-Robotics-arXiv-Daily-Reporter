{'arxiv_id': 'arXiv:2506.11829', 'title': 'The Space Between Us: A Methodological Framework for Researching Bonding and Proxemics in Situated Group-Agent Interactions', 'authors': 'Ana Müller, Anja Richert', 'link': 'https://arxiv.org/abs/2506.11829', 'abstract': 'This paper introduces a multimethod framework for studying spatial and social dynamics in real-world group-agent interactions with socially interactive agents. Drawing on proxemics and bonding theories, the method combines subjective self-reports and objective spatial tracking. Applied in two field studies in a museum (N = 187) with a robot and a virtual agent, the paper addresses the challenges in aligning human perception and behavior. We focus on presenting an open source, scalable, and field-tested toolkit for future studies.', 'abstract_zh': '本文介绍了用于研究现实世界群体-代理互动中社会互动代理的时空和社会动态的多方法框架。基于亲密距离理论和纽带理论，该方法结合了主观自我报告和客观空间追踪。在博物馆进行了两项实地研究（N = 187），分别使用机器人和虚拟代理，本文探讨了人类感知与行为一致性的挑战。我们专注于介绍一个开源、可扩展且已通过实地测试的工具包，以供未来研究使用。', 'title_zh': '我们之间的空间：研究情境中群体-代理互动中 bonding 和个体空间的研究方法框架'}
{'arxiv_id': 'arXiv:2506.11684', 'title': 'MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space', 'authors': 'Anshul Singh, Chris Biemann, Jan Strich', 'link': 'https://arxiv.org/abs/2506.11684', 'abstract': "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in interpreting visual layouts and text. However, a significant challenge remains in their ability to interpret robustly and reason over multi-tabular data presented as images, a common occurrence in real-world scenarios like web pages and digital documents. Existing benchmarks typically address single tables or non-visual data (text/structured). This leaves a critical gap: they don't assess the ability to parse diverse table images, correlate information across them, and perform multi-hop reasoning on the combined visual data. We introduce MTabVQA, a novel benchmark specifically designed for multi-tabular visual question answering to bridge that gap. MTabVQA comprises 3,745 complex question-answer pairs that necessitate multi-hop reasoning across several visually rendered table images. We provide extensive benchmark results for state-of-the-art VLMs on MTabVQA, revealing significant performance limitations. We further investigate post-training techniques to enhance these reasoning abilities and release MTabVQA-Instruct, a large-scale instruction-tuning dataset. Our experiments show that fine-tuning VLMs with MTabVQA-Instruct substantially improves their performance on visual multi-tabular reasoning. Code and dataset (this https URL) are available online (this https URL).", 'abstract_zh': 'Vision-Language模型（VLMs）已经在解读视觉布局和文本方面展现了出色的能力。然而，在处理以图像形式呈现的多表格数据方面，它们仍然面临显著的挑战，尤其是在现实场景如网页和数字文档中广泛存在的多表格数据上进行稳健解释和推理的能力。现有的基准测试通常仅涉及单个表格或非视觉数据（文本/结构化数据），这留下了关键的缺口：它们没有评估解析多种表格图像、跨表格关联信息以及在综合视觉数据上进行多跳推理的能力。我们引入了MTabVQA，这是一个专门设计用于多表格视觉问答的新基准，以弥补这一缺口。MTabVQA 包含了 3,745 个复杂的问答对，需要在多个视觉渲染的表格图像之间进行多跳推理。我们提供了最先进的 VLMs 在 MTabVQA 上的广泛基准测试结果，揭示了它们在视觉多表格推理方面的显著性能限制。我们进一步研究了后训练技术以增强这些推理能力，并发布了 MTabVQA-Instruct，这是一个大规模的指令调优数据集。我们的实验表明，使用 MTabVQA-Instruct 调整 VLMs 显著提高了其在视觉多表格推理方面的性能。代码和数据集可在以下链接获得：[此链接](this https URL)。', 'title_zh': 'MTabVQA: 评估语言模型在视觉空间中的多表格推理能力'}
{'arxiv_id': 'arXiv:2506.11550', 'title': 'Improving Multimodal Learning Balance and Sufficiency through Data Remixing', 'authors': 'Xiaoyu Ma, Hao Chen, Yongjian Deng', 'link': 'https://arxiv.org/abs/2506.11550', 'abstract': 'Different modalities hold considerable gaps in optimization trajectories, including speeds and paths, which lead to modality laziness and modality clash when jointly training multimodal models, resulting in insufficient and imbalanced multimodal learning. Existing methods focus on enforcing the weak modality by adding modality-specific optimization objectives, aligning their optimization speeds, or decomposing multimodal learning to enhance unimodal learning. These methods fail to achieve both unimodal sufficiency and multimodal balance. In this paper, we, for the first time, address both concerns by proposing multimodal Data Remixing, including decoupling multimodal data and filtering hard samples for each modality to mitigate modality imbalance; and then batch-level reassembling to align the gradient directions and avoid cross-modal interference, thus enhancing unimodal learning sufficiency. Experimental results demonstrate that our method can be seamlessly integrated with existing approaches, improving accuracy by approximately 6.50%$\\uparrow$ on CREMAD and 3.41%$\\uparrow$ on Kinetic-Sounds, without training set expansion or additional computational overhead during inference. The source code is available at \\href{this https URL}{Data Remixing}.', 'abstract_zh': '不同的模态在优化轨迹上存在显著差距，包括速度和路径的不同，这导致在联合训练多模态模型时出现模态懒惰和模态冲突，从而导致多模态学习不足和不平衡。现有方法主要通过增加模态特定的优化目标、对齐优化速度或分解多模态学习来强化单模态学习，但这些方法无法同时实现单模态充分性和多模态平衡。本文首次通过提出多模态Data Remixing来同时解决这两个问题，包括解耦多模态数据、过滤每个模态的困难样本以缓解模态不平衡；然后在批量层次上重新组装以对齐梯度方向并避免跨模态干扰，从而增强单模态学习充分性。实验结果表明，我们的方法可以无缝集成到现有方法中，分别在CREMAD上提高约6.50%的准确性，在Kinetic-Sounds上提高约3.41%的准确性，无需扩展训练集或在推理过程中增加额外的计算开销。源代码可在Data Remixing获取。', 'title_zh': '通过数据混搭提高多模态学习的平衡性和充分性'}
{'arxiv_id': 'arXiv:2506.11465', 'title': 'RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer', 'authors': 'Haotian Ni, Yake Wei, Hang Liu, Gong Chen, Chong Peng, Hao Lin, Di Hu', 'link': 'https://arxiv.org/abs/2506.11465', 'abstract': "Multimodal learning faces challenges in effectively fusing information from diverse modalities, especially when modality quality varies across samples. Dynamic fusion strategies, such as attention mechanism in Transformers, aim to address such challenge by adaptively emphasizing modalities based on the characteristics of input data. However, through amounts of carefully designed experiments, we surprisingly observed that the dynamic adaptability of widely-used self-attention models diminishes. Model tends to prefer one modality regardless of data characteristics. This bias triggers a self-reinforcing cycle that progressively overemphasizes the favored modality, widening the distribution gap in attention keys across modalities and deactivating attention mechanism's dynamic properties. To revive adaptability, we propose a simple yet effective method Rolling Query (RollingQ), which balances attention allocation by rotating the query to break the self-reinforcing cycle and mitigate the key distribution gap. Extensive experiments on various multimodal scenarios validate the effectiveness of RollingQ and the restoration of cooperation dynamics is pivotal for enhancing the broader capabilities of widely deployed multimodal Transformers. The source code is available at this https URL.", 'abstract_zh': '多模态学习在融合多样模态信息时面临挑战，尤其是在模态质量在样本间变化的情况下。通过大量精心设计的实验，我们惊讶地发现，广泛使用的自注意力模型的动态适应性在减弱。模型倾向于偏好某一模态，而忽略输入数据的特性。这种偏见引发了一种自我强化循环，逐渐加剧了不同模态间注意键的分布差距，使得注意力机制的动态特性丧失。为恢复适应性，我们提出了一种简单有效的方法Rolling Query（RollingQ），通过旋转查询来平衡注意力分配，打破自我强化循环并缓解键分布差距。在多种多模态场景下的广泛实验验证了RollingQ的有效性，恢复合作动态对于提高广泛部署的多模态Transformer的整体能力至关重要。代码见此链接。', 'title_zh': 'RollingQ: 重新激发多模态Transformer中的合作动态'}
{'arxiv_id': 'arXiv:2506.11394', 'title': 'Dynamic Double Space Tower', 'authors': 'Weikai Sun, Shijie Song, Han Wang', 'link': 'https://arxiv.org/abs/2506.11394', 'abstract': 'The Visual Question Answering (VQA) task requires the simultaneous understanding of image content and question semantics. However, existing methods often have difficulty handling complex reasoning scenarios due to insufficient cross-modal interaction and capturing the entity spatial relationships in the image.\\cite{huang2023adaptive}\\cite{liu2021comparing}\\cite{guibas2021adaptive}\\cite{zhang2022vsa}We studied a brand-new approach to replace the attention mechanism in order to enhance the reasoning ability of the model and its understanding of spatial this http URL, we propose a dynamic bidirectional spatial tower, which is divided into four layers to observe the image according to the principle of human gestalt vision. This naturally provides a powerful structural prior for the spatial organization between entities, enabling the model to no longer blindly search for relationships between pixels but make judgments based on more meaningful perceptual units. Change from "seeing images" to "perceiving and organizing image content".A large number of experiments have shown that our module can be used in any other multimodal model and achieve advanced results, demonstrating its potential in spatial relationship this http URL, the multimodal visual question-answering model July trained by our method has achieved state-of-the-art results with only 3B parameters, especially on the question-answering dataset of spatial relations.', 'abstract_zh': '视觉问答（VQA）任务要求同时理解图像内容和问题语义。然而，现有方法往往难以处理复杂的推理场景，原因在于跨模态交互不足和无法充分捕捉图像中的实体空间关系。[1][2][3][4]我们研究了一种新的方法，以替换注意力机制，以增强模型的推理能力和其对空间关系的理解。基于人类知觉心理学的原则，我们提出了一种动态双向空间塔，分为四层来观察图像。这自然地为实体之间的空间组织提供了强大的结构先验，使模型能够基于更有意义的知觉单元来判断，而非盲目地在像素之间寻找关系，实现从“看图像”到“感知和组织图像内容”的转变。大量实验表明，我们的模块可以在任何其他多模态模型中使用，并取得优异的结果，显示出其在空间关系理解方面的潜力。采用我们方法训练的多模态视觉问答模型，仅使用3B参数就达到了领先水平，特别是在空间关系的问答数据集上。', 'title_zh': '动态双空间塔'}
{'arxiv_id': 'arXiv:2506.11380', 'title': 'Enhance Multimodal Consistency and Coherence for Text-Image Plan Generation', 'authors': 'Xiaoxin Lu, Ranran Haoran Zhang, Yusen Zhang, Rui Zhang', 'link': 'https://arxiv.org/abs/2506.11380', 'abstract': "People get informed of a daily task plan through diverse media involving both texts and images. However, most prior research only focuses on LLM's capability of textual plan generation. The potential of large-scale models in providing text-image plans remains understudied. Generating high-quality text-image plans faces two main challenges: ensuring consistent alignment between two modalities and keeping coherence among visual steps. To address these challenges, we propose a novel framework that generates and refines text-image plans step-by-step. At each iteration, our framework (1) drafts the next textual step based on the prediction history; (2) edits the last visual step to obtain the next one; (3) extracts PDDL-like visual information; and (4) refines the draft with the extracted visual information. The textual and visual step produced in stage (4) and (2) will then serve as inputs for the next iteration. Our approach offers a plug-and-play improvement to various backbone models, such as Mistral-7B, Gemini-1.5, and GPT-4o. To evaluate the effectiveness of our approach, we collect a new benchmark consisting of 1,100 tasks and their text-image pair solutions covering 11 daily topics. We also design and validate a new set of metrics to evaluate the multimodal consistency and coherence in text-image plans. Extensive experiment results show the effectiveness of our approach on a range of backbone models against competitive baselines. Our code and data are available at this https URL.", 'abstract_zh': '人们通过多种媒体获取日常任务计划，涉及文字和图像。然而，大多数先前的研究仅关注大规模语言模型在文本计划生成方面的能力。大规模模型在提供图文计划方面的潜力尚未得到充分研究。生成高质量的图文计划面临两大主要挑战：确保两种模态之间的一致对齐以及保持视觉步骤之间的连贯性。为应对这些挑战，我们提出了一种新的框架，该框架逐步生成和精炼图文计划。在每次迭代中，该框架：(1) 根据预测历史生成下一阶段的文本步骤；(2) 编辑最后一个视觉步骤以获得下一个步骤；(3) 提取类似PDDL的视觉信息；(4) 用提取的视觉信息精炼草稿。在阶段(4)和(2)生成的文本和视觉步骤将作为下一迭代的输入。我们的方法可以无缝集成到各种骨干模型中，如Mistral-7B、Gemini-1.5和GPT-4o。为了评估我们方法的有效性，我们收集了一个包含1,100个任务及其图文解决方案的新基准，覆盖11个日常生活主题。此外，我们设计并验证了一套新的评价指标，以评估图文计划中的多模态一致性和连贯性。广泛的实验结果表明，与竞争性基线相比，我们的方法在各种骨干模型上具有有效性。我们的代码和数据可在以下链接获取。', 'title_zh': '增强文本-图像计划生成中的多模态一致性和连贯性'}
{'arxiv_id': 'arXiv:2506.11073', 'title': 'CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention', 'authors': 'Zekai Ye, Qiming Li, Xiaocheng Feng, Libo Qin, Yichong Huang, Baohang Li, Kui Jiang, Yang Xiang, Zhirui Zhang, Yunfei Lu, Duyu Tang, Dandan Tu, Bing Qin', 'link': 'https://arxiv.org/abs/2506.11073', 'abstract': 'Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal abilities but remain prone to multilingual object hallucination, with a higher likelihood of generating responses inconsistent with the visual input when utilizing queries in non-English languages compared to English. Most existing approaches to address these rely on pretraining or fine-tuning, which are resource-intensive. In this paper, inspired by observing the disparities in cross-modal attention patterns across languages, we propose Cross-Lingual Attention Intervention for Mitigating multilingual object hallucination (CLAIM) in LVLMs, a novel near training-free method by aligning attention patterns. CLAIM first identifies language-specific cross-modal attention heads, then estimates language shift vectors from English to the target language, and finally intervenes in the attention outputs during inference to facilitate cross-lingual visual perception capability alignment. Extensive experiments demonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in Spanish) on the POPE and 21.75% on the hallucination subsets of the MME benchmark across various languages. Further analysis reveals that multilingual attention divergence is most prominent in intermediate layers, highlighting their critical role in multilingual scenarios.', 'abstract_zh': 'Cross-Lingual Attention Intervention for Mitigating Multilingual Object Hallucination in Large Vision-Language Models', 'title_zh': 'CLAIM: 通过跨语言注意力干预减轻大型视觉-语言模型的多语言对象幻觉'}
{'arxiv_id': 'arXiv:2506.11063', 'title': 'Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation', 'authors': 'Jiayu Yao, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Yuyao Ge, Zhecheng Li, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2506.11063', 'abstract': 'Multimodal Retrieval-Augmented Generation (RAG) systems have become essential in knowledge-intensive and open-domain tasks. As retrieval complexity increases, ensuring the robustness of these systems is critical. However, current RAG models are highly sensitive to the order in which evidence is presented, often resulting in unstable performance and biased reasoning, particularly as the number of retrieved items or modality diversity grows. This raises a central question: How does the position of retrieved evidence affect multimodal RAG performance? To answer this, we present the first comprehensive study of position bias in multimodal RAG systems. Through controlled experiments across text-only, image-only, and mixed-modality tasks, we observe a consistent U-shaped accuracy curve with respect to evidence position. To quantify this bias, we introduce the Position Sensitivity Index ($PSI_p$) and develop a visualization framework to trace attention allocation patterns across decoder layers. Our results reveal that multimodal interactions intensify position bias compared to unimodal settings, and that this bias increases logarithmically with retrieval range. These findings offer both theoretical and empirical foundations for position-aware analysis in RAG, highlighting the need for evidence reordering or debiasing strategies to build more reliable and equitable generation systems.', 'abstract_zh': '多模态检索增强生成（RAG）系统中的位置偏见研究', 'title_zh': '谁占据聚光灯： multimodal retrieval-augmented generation 中隐含的偏见'}
