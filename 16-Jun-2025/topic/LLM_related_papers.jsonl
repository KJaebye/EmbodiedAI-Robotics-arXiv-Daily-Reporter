{'arxiv_id': 'arXiv:2506.11842', 'title': 'Your Ride, Your Rules: Psychology and Cognition Enabled Automated Driving Systems', 'authors': 'Zhipeng Bao, Qianwen Li', 'link': 'https://arxiv.org/abs/2506.11842', 'abstract': 'Despite rapid advances in autonomous driving, current autonomous vehicles (AVs) lack effective bidirectional communication with occupants, limiting personalization and recovery from immobilization. This reduces comfort and trust, potentially slowing broader AV adoption. We propose PACE-ADS (Psychology and Cognition Enabled Automated Driving Systems), a human-centered autonomy framework that enables AVs to sense, interpret, and respond to both external traffic and internal occupant states. PACE-ADS comprises three foundation model-based agents: a Driver Agent that analyzes the driving context, a Psychologist Agent that interprets occupant psychological signals (e.g., EEG, heart rate, facial expressions) and cognitive commands (e.g., speech), and a Coordinator Agent that integrates these inputs to produce high-level behavior decisions and operational parameters. Rather than replacing existing AV modules, PACE-ADS complements them by operating at the behavioral level, delegating low-level control to native AV systems. This separation enables closed-loop adaptation and supports integration across diverse platforms. We evaluate PACE-ADS in simulation across varied scenarios involving traffic lights, pedestrians, work zones, and car following. Results show that PACE-ADS adapts driving styles to occupant states, improves ride comfort, and enables safe recovery from immobilization via autonomous reasoning or human guidance. Our findings highlight the promise of LLM-based frameworks for bridging the gap between machine autonomy and human-centered driving.', 'abstract_zh': '尽管自主驾驶领域取得了快速进步，当前的自动驾驶车辆（AVs）在与乘客进行有效的双向通信方面存在不足，这限制了个性化服务和机动性恢复，从而降低了舒适度和信任感，可能影响更广泛的AV adoption。我们提出了一种名为PACE-ADS（Psychology and Cognition Enabled Automated Driving Systems）的人本导向自主驾驶框架，使车辆能够感知、解释和响应外部交通和内部乘客状态。PACE-ADS包含三个基于基础模型的代理：驾驶代理，负责分析驾驶环境；心理学代理，负责解释乘客的心理信号（如EEG、心率、面部表情）和认知指令（如口语）；协调代理，将这些输入综合以产生高级行为决策和操作参数。与现有AV模块不同，PACE-ADS在行为层面上与其互补，将低级控制委托给原生AV系统。这种分离使得闭环适应成为可能，并支持跨不同平台的整合。我们在涉及交通灯、行人、工作区和跟随车辆等多种场景的模拟中评估了PACE-ADS。结果表明，PACE-ADS能够根据乘客状态调整驾驶风格，提高乘车舒适度，并通过自主推理或人类引导实现安全的机动性恢复。我们的发现突显了基于LLM的框架在弥合机器自主性和以人为本驾驶之间差距方面的潜力。', 'title_zh': '你的旅程，你做主：心理学与认知赋能的自动驾驶系统'}
{'arxiv_id': 'arXiv:2506.11526', 'title': 'Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis', 'authors': 'Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, Jan Frederik Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz', 'link': 'https://arxiv.org/abs/2506.11526', 'abstract': 'For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key approaches to development and validation of autonomous driving systems. Traditional scenario generation relies on rule-based systems, knowledge-driven models, and data-driven synthesis, often producing limited diversity and unrealistic safety-critical cases. With the emergence of foundation models, which represent a new generation of pre-trained, general-purpose AI models, developers can process heterogeneous inputs (e.g., natural language, sensor data, HD maps, and control actions), enabling the synthesis and interpretation of complex driving scenarios. In this paper, we conduct a survey about the application of foundation models for scenario generation and scenario analysis in autonomous driving (as of May 2025). Our survey presents a unified taxonomy that includes large language models, vision-language models, multimodal large language models, diffusion models, and world models for the generation and analysis of autonomous driving scenarios. In addition, we review the methodologies, open-source datasets, simulation platforms, and benchmark challenges, and we examine the evaluation metrics tailored explicitly to scenario generation and analysis. Finally, the survey concludes by highlighting the open challenges and research questions, and outlining promising future research directions. All reviewed papers are listed in a continuously maintained repository, which contains supplementary materials and is available at this https URL.', 'abstract_zh': '对自主车辆而言，复杂环境下的安全导航依赖于处理多种多样且罕见的驾驶场景。基于仿真和场景的方法已成为自主驾驶系统开发与验证的关键途径。传统场景生成依靠基于规则的系统、知识驱动模型和数据驱动合成，往往产生有限的多样性和不现实的安全关键案例。随着基础模型的出现，这是一种新一代的预训练泛用人工智能模型，开发者可以处理异构输入（如自然语言、传感器数据、高清地图和控制动作），从而生成和解释复杂的驾驶场景。本文于2025年5月进行了一项关于基础模型在自主驾驶场景生成与分析中应用的综述。综述中包含了一个统一的分类体系，涵盖了大型语言模型、视觉语言模型、多模态大型语言模型、扩散模型和世界模型，用于生成和分析自主驾驶场景。此外，我们还回顾了相关方法、开源数据集、仿真平台和基准挑战，并详细介绍了专门针对场景生成与分析的评估标准。最后，综述总结了存在的开放挑战和研究问题，并指出了有前景的未来研究方向。所有已审查的论文清单保存在一个持续维护的仓库中，该仓库包含补充材料，可在以下链接访问：[https://github.com/autonomous-scenario-generation-and-analysis-repository]。', 'title_zh': '自主驾驶中的基础模型：场景生成与分析综述'}
{'arxiv_id': 'arXiv:2506.12012', 'title': 'Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making', 'authors': 'Xiaopeng Yuan, Xingjian Zhang, Ke Xu, Yifan Xu, Lijun Yu, Jindong Wang, Yushun Dong, Haohan Wang', 'link': 'https://arxiv.org/abs/2506.12012', 'abstract': 'Large language models (LLMs) are increasingly used for tasks that require complex reasoning. Most benchmarks focus on final outcomes but overlook the intermediate reasoning steps - such as planning, revision, and decision making under resource constraints. We argue that measuring these internal processes is essential for understanding model behavior and improving reliability. We propose using strategic games as a natural evaluation environment: closed, rule-based systems with clear states, limited resources, and automatic feedback. We introduce a framework that evaluates LLMs along three core dimensions: planning, revision, and resource-constrained decision making. To operationalize this, we define metrics beyond win rate, including overcorrection risk rate, correction success rate, improvement slope, and over-budget ratio. In 4320 adversarial rounds across 12 leading models, ChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7 percent, a correction success rate of 78.6 percent, and an improvement slope of 0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6 percent, wins only 25.6 percent of its matches - primarily due to excessive resource use. We also observe a negative correlation between overcorrection risk rate and correction success rate (Pearson r = -0.51, p = 0.093), suggesting that more frequent edits do not always improve outcomes. Our findings highlight the value of assessing not only what LLMs decide but how they arrive at those decisions', 'abstract_zh': '大型语言模型（LLMs）越来越多地被用于需要复杂推理的任务。大多数基准测试侧重于最终结果，而忽略了中间推理步骤，如规划、修订以及资源约束下的决策过程。我们主张衡量这些内部过程对于理解模型行为和提高可靠性至关重要。我们提出使用战略游戏作为自然的评估环境：封闭的、基于规则的系统，具有明确的状态、有限的资源和自动反馈。我们引入了一个框架，从规划、修订和资源约束决策三个方面评估LLMs。为此，我们定义了超越胜率的指标，包括纠正过度矫正风险率、纠正成功率、改进斜率和超支比率。在涉及12款领先模型的4320场对抗轮次中，ChatGPT-o3-mini获得最高综合分数，胜率为74.7%，纠正成功率为78.6%，改进斜率为0.041。相比之下，尽管Qwen-Plus的过度矫正风险率为81.6%，但仅赢得其比赛的25.6% - 主要原因是过度使用资源。我们还发现，过度矫正风险率和纠正成功率之间存在负相关（皮尔逊r = -0.51，p = 0.093），表明频繁的编辑并不总是能改善结果。我们的研究结果强调了不仅评估LLMs决策结果，还应评估其决策过程的重要性。', 'title_zh': '使用战略博弈追踪大模型推理过程：一种计划、修订及资源约束决策制定的框架'}
{'arxiv_id': 'arXiv:2506.11887', 'title': 'Towards a Cascaded LLM Framework for Cost-effective Human-AI Decision-Making', 'authors': 'Claudio Fanconi, Mihaela van der Schaar', 'link': 'https://arxiv.org/abs/2506.11887', 'abstract': "Effective human-AI decision-making balances three key factors: the \\textit{correctness} of predictions, the \\textit{cost} of knowledge and reasoning complexity, and the confidence about whether to \\textit{abstain} automated answers or involve human experts. In this work, we present a cascaded LLM decision framework that adaptively delegates tasks across multiple tiers of expertise -- a base model for initial candidate answers, a more capable and knowledgeable (but costlier) large model, and a human expert for when the model cascade abstains. Our method proceeds in two stages. First, a deferral policy determines whether to accept the base model's answer or regenerate it with the large model based on the confidence score. Second, an abstention policy decides whether the cascade model response is sufficiently certain or requires human intervention. Moreover, we incorporate an online learning mechanism in the framework that can leverage human feedback to improve decision quality over time. We demonstrate this approach to general question-answering (ARC-Easy and ARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results show that our cascaded strategy outperforms in most cases single-model baselines in accuracy while reducing cost and providing a principled way to handle abstentions.", 'abstract_zh': '有效的真人-AI决策平衡三种关键因素：预测的准确性、知识和推理复杂性的成本，以及是否弃用自动化答案而求助人类专家的信心。本文提出了一种级联大语言模型决策框架，该框架根据不同层次的专业能力自适应地分配任务——基础模型用于初始候选答案，更强大且知识丰富的大型模型用于生成答案，人工专家在级联模型弃用时介入。该方法分为两个阶段。首先，通过置信分数确定是否接受基础模型的答案或使用大型模型重新生成答案。其次，确定级联模型响应是否足够确定或需要人工干预。此外，我们在框架中引入了一种在线学习机制，该机制可以根据人类反馈提高决策质量。我们展示了该方法在通用问答（ARC-Easy和ARC-Challenge）和医学问答（MedQA和MedMCQA）中的应用。结果表明，与单模型基线相比，我们的级联策略在大多数情况下在准确性和成本方面表现更优，并提供了一种处理弃用的原理性方法。', 'title_zh': '面向成本效益的人机决策 Cascaded LLM 框架'}
{'arxiv_id': 'arXiv:2506.11880', 'title': 'Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment', 'authors': 'Alejandro Peña, Julian Fierrez, Aythami Morales, Gonzalo Mancera, Miguel Lopez, Ruben Tolosana', 'link': 'https://arxiv.org/abs/2506.11880', 'abstract': 'The use of language technologies in high-stake settings is increasing in recent years, mostly motivated by the success of Large Language Models (LLMs). However, despite the great performance of LLMs, they are are susceptible to ethical concerns, such as demographic biases, accountability, or privacy. This work seeks to analyze the capacity of Transformers-based systems to learn demographic biases present in the data, using a case study on AI-based automated recruitment. We propose a privacy-enhancing framework to reduce gender information from the learning pipeline as a way to mitigate biased behaviors in the final tools. Our experiments analyze the influence of data biases on systems built on two different LLMs, and how the proposed framework effectively prevents trained systems from reproducing the bias in the data.', 'abstract_zh': '近年来，语言技术在高风险场景中的应用越来越多，主要是受大型语言模型（LLMs）成功的影响。尽管LLMs表现卓越，但它们仍易受到伦理关切的影响，如人口统计偏见、问责制或隐私问题。本研究旨在分析基于变换器系统的模型学习数据中存在的人口统计偏见的能力，并通过基于AI的自动化招聘案例研究进行探讨。我们提出了一种增强隐私的框架，通过减少学习管道中的性别信息来减轻最终工具中的偏见行为。我们的实验分析了数据偏见对两种不同LLM构建的系统的影响，并研究了所提框架如何有效地防止训练系统复制数据中的偏见。', 'title_zh': '解决LLM中的偏差：策略及其在公平AI招聘中的应用'}
{'arxiv_id': 'arXiv:2506.11825', 'title': 'Revealing Political Bias in LLMs through Structured Multi-Agent Debate', 'authors': 'Aishwarya Bandaru, Fabian Bindley, Trevor Bluth, Nandini Chavda, Baixu Chen, Ethan Law', 'link': 'https://arxiv.org/abs/2506.11825', 'abstract': "Large language models (LLMs) are increasingly used to simulate social behaviour, yet their political biases and interaction dynamics in debates remain underexplored. We investigate how LLM type and agent gender attributes influence political bias using a structured multi-agent debate framework, by engaging Neutral, Republican, and Democrat American LLM agents in debates on politically sensitive topics. We systematically vary the underlying LLMs, agent genders, and debate formats to examine how model provenance and agent personas influence political bias and attitudes throughout debates. We find that Neutral agents consistently align with Democrats, while Republicans shift closer to the Neutral; gender influences agent attitudes, with agents adapting their opinions when aware of other agents' genders; and contrary to prior research, agents with shared political affiliations can form echo chambers, exhibiting the expected intensification of attitudes as debates progress.", 'abstract_zh': '大型语言模型（LLMs）在模拟社会行为方面应用日益增多，然而它们在辩论中的政治偏见及其互动动态尚未得到充分探索。我们通过一个结构化的多agent辩论框架，研究不同类型的LLM和agent性别属性如何影响政治偏见，涉及中立、共和党和民主党美国LLM agent在政治敏感话题上的辩论。我们系统地改变底层LLM、agent性别及辩论格式，以探讨模型来源和agent人设如何影响辩论过程中政治偏见和态度的变化。研究发现，中立agent始终与民主党一致，共和党则向中立靠近；性别影响agent的态度，awareness of其他agent的性别使agent调整其观点；与以往研究不同，具有共同政治隶属关系的agent可能会形成回音室效应，在辩论过程中态度趋于加强。', 'title_zh': '通过结构化多代理辩论揭示LLMs中的政治偏见'}
{'arxiv_id': 'arXiv:2506.11812', 'title': 'On the Performance of LLMs for Real Estate Appraisal', 'authors': 'Margot Geerts, Manon Reusens, Bart Baesens, Seppe vanden Broucke, Jochen De Weerdt', 'link': 'https://arxiv.org/abs/2506.11812', 'abstract': "The real estate market is vital to global economies but suffers from significant information asymmetry. This study examines how Large Language Models (LLMs) can democratize access to real estate insights by generating competitive and interpretable house price estimates through optimized In-Context Learning (ICL) strategies. We systematically evaluate leading LLMs on diverse international housing datasets, comparing zero-shot, few-shot, market report-enhanced, and hybrid prompting techniques. Our results show that LLMs effectively leverage hedonic variables, such as property size and amenities, to produce meaningful estimates. While traditional machine learning models remain strong for pure predictive accuracy, LLMs offer a more accessible, interactive and interpretable alternative. Although self-explanations require cautious interpretation, we find that LLMs explain their predictions in agreement with state-of-the-art models, confirming their trustworthiness. Carefully selected in-context examples based on feature similarity and geographic proximity, significantly enhance LLM performance, yet LLMs struggle with overconfidence in price intervals and limited spatial reasoning. We offer practical guidance for structured prediction tasks through prompt optimization. Our findings highlight LLMs' potential to improve transparency in real estate appraisal and provide actionable insights for stakeholders.", 'abstract_zh': '房地产市场对全球经济至关重要但存在显著的信息不对称。本文研究了大型语言模型（LLMs）如何通过优化上下文学习（ICL）策略生成竞争性和可解释的房价估计，从而普及房地产洞察。我们系统评估了领先LLMs在多种国际住房数据集上的表现，比较了零样本、少样本、市场报告增强以及混合提示技术。研究结果显示，LLMs能够有效地利用如房产大小和配套设施等心斌变量来生成有意义的估计。尽管传统机器学习模型在纯粹的预测准确性方面仍然强劲，但LLMs提供了更易访问、互动且可解释的替代方案。尽管自我解释需要谨慎解释，但研究表明LLMs的预测解释与领先模型一致，证实了其可信度。根据特征相似性和地理 proximity 选择的精心策划的上下文示例，显著提升了LLM的表现，但LLMs在处理价格区间上的过度自信和空间推理能力有限。本文为结构化预测任务提供了通过提示优化的实际指导。我们的研究结果突显了LLMs在提高房地产评估透明度和为利益相关者提供可操作洞察方面的潜力。', 'title_zh': 'LLMs在房地产评估中的性能研究'}
{'arxiv_id': 'arXiv:2506.11712', 'title': 'Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization', 'authors': 'Wenqi Liu, Xuemeng Song, Jiaxi Li, Yinwei Wei, Na Zheng, Jianhua Yin, Liqiang Nie', 'link': 'https://arxiv.org/abs/2506.11712', 'abstract': "Direct Preference Optimization (DPO) has emerged as an effective approach for mitigating hallucination in Multimodal Large Language Models (MLLMs). Although existing methods have achieved significant progress by utilizing vision-oriented contrastive objectives for enhancing MLLMs' attention to visual inputs and hence reducing hallucination, they suffer from non-rigorous optimization objective function and indirect preference supervision. To address these limitations, we propose a Symmetric Multimodal Preference Optimization (SymMPO), which conducts symmetric preference learning with direct preference supervision (i.e., response pairs) for visual understanding enhancement, while maintaining rigorous theoretical alignment with standard DPO. In addition to conventional ordinal preference learning, SymMPO introduces a preference margin consistency loss to quantitatively regulate the preference gap between symmetric preference pairs. Comprehensive evaluation across five benchmarks demonstrate SymMPO's superior performance, validating its effectiveness in hallucination mitigation of MLLMs.", 'abstract_zh': '直接偏好优化（DPO）已成为减轻多模态大型语言模型（MLLMs）幻觉的有效方法。为了应对现有方法存在的优化目标不严谨和偏好监督间接等局限性，我们提出了一种对称多模态偏好优化（SymMPO），它通过直接偏好监督（即响应对）进行对称偏好学习，以增强视觉理解能力，并保持与标准DPO严格的理论一致性。除了常规的序数偏好学习外，SymMPO还引入了一致性偏好边际损失，以定量调节对称偏好对之间的偏好差距。在五个基准上的全面评估表明，SymMPO 在减轻MLLMs 幻觉方面具有优越性能，验证了其有效性。', 'title_zh': '通过理论一致的对称多模态偏好优化减轻幻觉'}
{'arxiv_id': 'arXiv:2506.11578', 'title': 'Collaborative LLM Inference via Planning for Efficient Reasoning', 'authors': 'Byeongchan Lee, Jonghoon Lee, Dongyoung Kim, Jaehyung Kim, Jinwoo Shin', 'link': 'https://arxiv.org/abs/2506.11578', 'abstract': 'Large language models (LLMs) excel at complex reasoning tasks, but those with strong capabilities (e.g., whose numbers of parameters are larger than 100B) are often accessible only through paid APIs, making them too costly for applications of frequent use. In contrast, smaller open-sourced LLMs (e.g., whose numbers of parameters are less than 3B) are freely available and easy to deploy locally (e.g., under a single GPU having 8G VRAM), but lack suff icient reasoning ability. This trade-off raises a natural question: can small (free) and large (costly) models collaborate at test time to combine their strengths? We propose a test-time collaboration framework in which a planner model first generates a plan, defined as a distilled and high-level abstraction of the problem.\nThis plan serves as a lightweight intermediate that guides a reasoner model, which generates a complete solution. Small and large models take turns acting as planner and reasoner, exchanging plans in a multi-round cascade to collaboratively solve complex tasks. Our method achieves accuracy comparable to strong proprietary models alone, while significantly reducing reliance on paid inference. These results highlight planning as an effective prior for orchestrating cost-aware, cross-model inference under real-world deployment constraints.', 'abstract_zh': '小型免费与大型付费语言模型在测试时协作的框架及其应用', 'title_zh': '规划驱动的协作LLM推理以实现高效推理'}
{'arxiv_id': 'arXiv:2506.11555', 'title': 'RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning', 'authors': 'Yu Wang, Shiwan Zhao, Ming Fan, Zhihu Wang, Yubo Zhang, Xicheng Zhang, Zhengfan Wang, Heyuan Huang, Ting Liu', 'link': 'https://arxiv.org/abs/2506.11555', 'abstract': 'The integration of external knowledge through Retrieval-Augmented Generation (RAG) has become foundational in enhancing large language models (LLMs) for knowledge-intensive tasks. However, existing RAG paradigms often overlook the cognitive step of applying knowledge, leaving a gap between retrieved facts and task-specific reasoning. In this work, we introduce RAG+, a principled and modular extension that explicitly incorporates application-aware reasoning into the RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and aligned application examples, created either manually or automatically, and retrieves both jointly during inference. This design enables LLMs not only to access relevant information but also to apply it within structured, goal-oriented reasoning processes. Experiments across mathematical, legal, and medical domains, conducted on multiple models, demonstrate that RAG+ consistently outperforms standard RAG variants, achieving average improvements of 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval with actionable application, RAG+ advances a more cognitively grounded framework for knowledge integration, representing a step toward more interpretable and capable LLMs.', 'abstract_zh': '通过检索增强生成（RAG）整合外部知识已成为增强大型语言模型（LLMs）在知识密集型任务中表现的基础。然而，现有RAG范式往往忽视了应用知识的认知步骤，留下了检索事实与任务特定推理之间的差距。在此工作中，我们引入了RAG+，这是一种原则性的模块化扩展，明确地将应用意识推理纳入RAG流程中。RAG+构建了一个双语料库，包括知识和对齐的应用示例，这些示例可以是手工创建的，也可以是自动创建的，并在推理过程中同时检索。这种设计不仅使LLMs能够访问相关的信息，还能够在结构化、目标导向的推理过程中应用这些信息。在数学、法律和医疗等多个领域的实验结果显示，RAG+在多个模型上均表现出色，相对于标准RAG变体，平均提高了3-5%，在复杂场景中最高可达7.5%的提升。通过将检索与可操作的应用相结合，RAG+推进了更基于认知的知识整合框架，代表着向更可解释和能力强的LLMs迈出了一步。', 'title_zh': 'RAG+: 增强应用意识推理的检索增强生成'}
{'arxiv_id': 'arXiv:2506.11376', 'title': 'Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning', 'authors': 'Liying Wang, Ph.D., Daffodil Carrington, M.S., Daniil Filienko, M.S., Caroline El Jazmi, M.S., Serena Jinchen Xie, M.S., Martine De Cock, Ph.D., Sarah Iribarren, Ph.D., Weichao Yuwen, Ph.D', 'link': 'https://arxiv.org/abs/2506.11376', 'abstract': "Family caregivers often face substantial mental health challenges due to their multifaceted roles and limited resources. This study explored the potential of a large language model (LLM)-powered conversational agent to deliver evidence-based mental health support for caregivers, specifically Problem-Solving Therapy (PST) integrated with Motivational Interviewing (MI) and Behavioral Chain Analysis (BCA). A within-subject experiment was conducted with 28 caregivers interacting with four LLM configurations to evaluate empathy and therapeutic alliance. The best-performing models incorporated Few-Shot and Retrieval-Augmented Generation (RAG) prompting techniques, alongside clinician-curated examples. The models showed improved contextual understanding and personalized support, as reflected by qualitative responses and quantitative ratings on perceived empathy and therapeutic alliances. Participants valued the model's ability to validate emotions, explore unexpressed feelings, and provide actionable strategies. However, balancing thorough assessment with efficient advice delivery remains a challenge. This work highlights the potential of LLMs in delivering empathetic and tailored support for family caregivers.", 'abstract_zh': '家庭照护者因其多重角色和有限资源而常面临显著的心理健康挑战。本研究探讨了大型语言模型（LLM）驱动的对话代理交付基于证据的心理健康支持的潜力，特别是将解决问题疗法（PST）与动机访谈（MI）和行为链分析（BCA）相结合。研究采用被试内实验设计，28名家庭照护者与四种LLM配置互动，评估其共情和治疗联盟。性能最佳的模型结合了少样本学习和检索增强生成（RAG）提示技术，并辅以临床人员定制的示例。这些模型在背景理解和个性化支持方面表现出改进，这一改进反映在定性反馈和定量评估中，参与者对模型的共情能力和治疗联盟的感知有所提升。参与者赞赏模型验证情绪、探索未表达的情感以及提供可操作策略的能力。然而，如何在全面评估的同时高效提供建议仍是一项挑战。本研究突显了LLM在为家庭照护者提供具有同理心和个性化支持方面的潜力。', 'title_zh': '大型语言模型驱动的对话代理为家庭护理人员提供问题解决疗法（PST）：通过上下文学习增强共情与治疗联盟'}
{'arxiv_id': 'arXiv:2506.11375', 'title': 'Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables', 'authors': 'Yitong Zhou, Mingyue Cheng, Qingyang Mao, Yucong Luo, Qi Liu, Yupeng Li, Xiaohan Zhang, Deguang Liu, Xin Li, Enhong Chen', 'link': 'https://arxiv.org/abs/2506.11375', 'abstract': 'Chemical tables encode complex experimental knowledge through symbolic expressions, structured variables, and embedded molecular graphics. Existing benchmarks largely overlook this multimodal and domain-specific complexity, limiting the ability of multimodal large language models to support scientific understanding in chemistry. In this work, we introduce ChemTable, a large-scale benchmark of real-world chemical tables curated from the experimental sections of literature. ChemTable includes expert-annotated cell polygons, logical layouts, and domain-specific labels, including reagents, catalysts, yields, and graphical components and supports two core tasks: (1) Table Recognition, covering structure parsing and content extraction; and (2) Table Understanding, encompassing both descriptive and reasoning-oriented question answering grounded in table structure and domain semantics. We evaluated a range of representative multimodal models, including both open-source and closed-source models, on ChemTable and reported a series of findings with practical and conceptual insights. Although models show reasonable performance on basic layout parsing, they exhibit substantial limitations on both descriptive and inferential QA tasks compared to human performance, and we observe significant performance gaps between open-source and closed-source models across multiple dimensions. These results underscore the challenges of chemistry-aware table understanding and position ChemTable as a rigorous and realistic benchmark for advancing scientific reasoning.', 'abstract_zh': 'ChemTable: 大规模化学表格基准，包含专家注释的单元格多边形、逻辑布局和领域特定标签，支持表格识别和理解核心任务', 'title_zh': '多模态LLM在化学表格识别与理解上的基准测试'}
{'arxiv_id': 'arXiv:2506.11221', 'title': 'LLM-as-a-Fuzzy-Judge: Fine-Tuning Large Language Models as a Clinical Evaluation Judge with Fuzzy Logic', 'authors': 'Weibing Zheng, Laurah Turner, Jess Kropczynski, Murat Ozer, Tri Nguyen, Shane Halse', 'link': 'https://arxiv.org/abs/2506.11221', 'abstract': "Clinical communication skills are critical in medical education, and practicing and assessing clinical communication skills on a scale is challenging. Although LLM-powered clinical scenario simulations have shown promise in enhancing medical students' clinical practice, providing automated and scalable clinical evaluation that follows nuanced physician judgment is difficult. This paper combines fuzzy logic and Large Language Model (LLM) and proposes LLM-as-a-Fuzzy-Judge to address the challenge of aligning the automated evaluation of medical students' clinical skills with subjective physicians' preferences. LLM-as-a-Fuzzy-Judge is an approach that LLM is fine-tuned to evaluate medical students' utterances within student-AI patient conversation scripts based on human annotations from four fuzzy sets, including Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction. The methodology of this paper started from data collection from the LLM-powered medical education system, data annotation based on multidimensional fuzzy sets, followed by prompt engineering and the supervised fine-tuning (SFT) of the pre-trained LLMs using these human annotations. The results show that the LLM-as-a-Fuzzy-Judge achieves over 80\\% accuracy, with major criteria items over 90\\%, effectively leveraging fuzzy logic and LLM as a solution to deliver interpretable, human-aligned assessment. This work suggests the viability of leveraging fuzzy logic and LLM to align with human preferences, advances automated evaluation in medical education, and supports more robust assessment and judgment practices. The GitHub repository of this work is available at this https URL", 'abstract_zh': '临床沟通技能是医学教育中的关键要素，而在量级上实践和评估这些技能具有挑战性。尽管由大语言模型（LLM）驱动的临床情景模拟在提升医学生临床实践方面展现了一定潜力，但按照细微的医师判断提供自动化的、可扩展的临床评估仍然困难重重。本文结合模糊逻辑和大语言模型（LLM），提出了一种“LLM作为模糊法官”的方法，以解决自动评估医学生临床技能与主观医师偏好对齐的挑战。该方法基于人类注释，对医学生在学生-AI患者对话剧本中的言论进行评估，评估维度包括专业性、医学相关性、伦理行为和环境干扰四个模糊集。本文的方法从LLM驱动的医学教育系统的数据收集开始，基于多维模糊集的数据标注，随后进行提示工程并使用这些人类注释来监督预训练的大语言模型的微调。结果表明，“LLM作为模糊法官”实现了超过80%的准确率，主要标准项超过90%，有效地利用了模糊逻辑和大语言模型，以实现解释性、与人类偏好对齐的评估。这项工作表明，利用模糊逻辑和大语言模型来与人类偏好对齐是可行的，推动了医学教育中自动评估的发展，并支持了更稳健的评估和判断实践。相关GitHub仓库可在以下链接获取。', 'title_zh': 'LLM-as-a-模糊法官：基于模糊逻辑 fine-tuning 大型语言模型作为临床评估法官'}
{'arxiv_id': 'arXiv:2506.12014', 'title': 'code_transformed: The Influence of Large Language Models on Code', 'authors': 'Yuliang Xu, Siming Huang, Mingmeng Geng, Yao Wan, Xuanhua Shi, Dongping Chen', 'link': 'https://arxiv.org/abs/2506.12014', 'abstract': 'Coding remains one of the most fundamental modes of interaction between humans and machines. With the rapid advancement of Large Language Models (LLMs), code generation capabilities have begun to significantly reshape programming practices. This development prompts a central question: Have LLMs transformed code style, and how can such transformation be characterized? In this paper, we present a pioneering study that investigates the impact of LLMs on code style, with a focus on naming conventions, complexity, maintainability, and similarity. By analyzing code from over 19,000 GitHub repositories linked to arXiv papers published between 2020 and 2025, we identify measurable trends in the evolution of coding style that align with characteristics of LLM-generated code. For instance, the proportion of snake\\_case variable names in Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we investigate how LLMs approach algorithmic problems by examining their reasoning processes. Given the diversity of LLMs and usage scenarios, among other factors, it is difficult or even impossible to precisely estimate the proportion of code generated or assisted by LLMs. Our experimental results provide the first large-scale empirical evidence that LLMs affect real-world programming style.', 'abstract_zh': '大型语言模型对编码风格的影响：以命名规范、复杂度、可维护性和相似性为中心的研究', 'title_zh': '代码转换：大型语言模型对代码的影响'}
{'arxiv_id': 'arXiv:2506.11938', 'title': 'Improving Large Language Model Safety with Contrastive Representation Learning', 'authors': 'Samuel Simko, Mrinmaya Sachan, Bernhard Schölkopf, Zhijing Jin', 'link': 'https://arxiv.org/abs/2506.11938', 'abstract': 'Large Language Models (LLMs) are powerful tools with profound societal impacts, yet their ability to generate responses to diverse and uncontrolled inputs leaves them vulnerable to adversarial attacks. While existing defenses often struggle to generalize across varying attack types, recent advancements in representation engineering offer promising alternatives. In this work, we propose a defense framework that formulates model defense as a contrastive representation learning (CRL) problem. Our method finetunes a model using a triplet-based loss combined with adversarial hard negative mining to encourage separation between benign and harmful representations. Our experimental results across multiple models demonstrate that our approach outperforms prior representation engineering-based defenses, improving robustness against both input-level and embedding-space attacks without compromising standard performance. Our code is available at this https URL', 'abstract_zh': '大型语言模型（LLMs）是具有深远社会影响的强大工具，但由于其对多样且不受控制的输入生成响应的能力，使得它们容易受到 adversarial 攻击。尽管现有的防御措施往往难以在不同类型的攻击之间泛化，但最近在表示工程方面的进展提供了有希望的替代方案。在这项工作中，我们提出了一种防御框架，将模型防御形式化为对比表示学习（CRL）问题。我们的方法使用基于三元组的损失结合对抗性困难负样本挖掘对模型进行微调，以促进良性与有害表示之间的分离。我们在多个模型的实验结果表明，我们的方法优于基于表示工程的先前防御措施，在不牺牲标准性能的情况下提高了对输入级和嵌入空间攻击的稳健性。我们的代码可在以下网址获取：this https URL', 'title_zh': '基于对比表示学习提高大型语言模型安全性'}
{'arxiv_id': 'arXiv:2506.11928', 'title': 'LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?', 'authors': 'Zihan Zheng, Zerui Cheng, Zeyu Shen, Shang Zhou, Kaiyuan Liu, Hansen He, Dongruixuan Li, Stanley Wei, Hangyi Hao, Jianzhu Yao, Peiyao Sheng, Zixuan Wang, Wenhao Chai, Aleksandra Korolova, Peter Henderson, Sanjeev Arora, Pramod Viswanath, Jingbo Shang, Saining Xie', 'link': 'https://arxiv.org/abs/2506.11928', 'abstract': 'Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.', 'abstract_zh': '最近的报告显示，大型语言模型（LLMs）现在在编程竞赛中已经超越了顶尖人类选手。凭借国际算法竞赛获奖者的知识，我们重新审视这一报告，探讨LLMs与人类专家之间的差异以及仍然存在的局限性。我们引入了LiveCodeBench Pro基准，该基准包含来自Codeforces、ICPC和IOI的问题，并持续更新以降低数据污染的可能性。由奥林匹克奖牌得主对每个问题进行算法类别标注，并对模型生成的失败提交进行逐行分析。利用这些新数据和基准，我们发现前沿模型仍然存在显著局限性：在没有外部工具的情况下，最佳模型在中等难度问题上的pass@1仅为53%，而在困难问题上则为0%，这是专家人类选手仍能胜任的领域。我们还发现，LLMs在实现密集型问题上取得成功，但在复杂的算法推理和复杂情况分析上遇到困难，往往会产生自信心十足的错误解释。高性能主要由实现精度和工具辅助驱动，而非更卓越的推理能力。因此，LiveCodeBench Pro突显了与人类大师级水平之间的显著差距，并提供了详细的诊断，以指引未来代码中心的大规模语言模型推理改进。', 'title_zh': 'LiveCodeBench Pro: 奥林匹克金牌得主如何评判LLMs在competitive programming中的表现？'}
{'arxiv_id': 'arXiv:2506.11844', 'title': 'TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks', 'authors': 'Qihai Zhang, Xinyue Sheng, Yuanfu Sun, Qiaoyu Tan', 'link': 'https://arxiv.org/abs/2506.11844', 'abstract': "Inspired by the success of large language models (LLMs), there is a significant research shift from traditional graph learning methods to LLM-based graph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning power of LLMs by integrating three key components: the textual attributes of input nodes, the structural information of node neighborhoods, and task-specific prompts that guide decision-making. Despite their promise, the robustness of GraphLLMs against adversarial perturbations remains largely unexplored-a critical concern for deploying these models in high-stakes scenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study evaluating the vulnerability of GraphLLMs to adversarial attacks across three dimensions: text, graph structure, and prompt manipulations. We implement state-of-the-art attack algorithms from each perspective to rigorously assess model resilience. Through extensive experiments on six benchmark datasets from diverse domains, our findings reveal that GraphLLMs are highly susceptible to text attacks that merely replace a few semantically similar words in a node's textual attribute. We also find that standard graph structure attack methods can significantly degrade model performance, while random shuffling of the candidate label set in prompt templates leads to substantial performance drops. Beyond characterizing these vulnerabilities, we investigate defense techniques tailored to each attack vector through data-augmented training and adversarial training, which show promising potential to enhance the robustness of GraphLLMs. We hope that our open-sourced library will facilitate rapid, equitable evaluation and inspire further innovative research in this field.", 'abstract_zh': '受大型语言模型成功的影响，图学习方法的研究方向从传统图学习方法转向基于大型语言模型的图框架，正式称为GraphLLMs。GraphLLMs通过整合三个关键组件——输入节点的文本属性、节点邻域的结构信息以及指导决策的任务特定提示——利用大型语言模型的推理能力。尽管它们前景广阔，但GraphLLMs对对抗性扰动的鲁棒性仍然鲜有探索——这是在高风险场景中部署这些模型的一个关键关切。为此，我们引入了TrustGLM，这是一个全面的研究，评估GraphLLMs在文本、图结构和提示操纵三个维度上的对抗性攻击脆弱性。我们从每个角度实施最先进的攻击算法，以严格评估模型的鲁棒性。通过在六个来自不同领域的基准数据集上的大量实验，我们的研究发现，GraphLLMs高度易受仅用少数语义相似词替换节点文本属性中的词的文本攻击。我们还发现，标准的图结构攻击方法可以显著降低模型性能，而提示模板中候选标签集的随机重排会导致显著的性能下降。除了揭示这些脆弱性，我们还通过数据增强训练和对抗性训练，针对每种攻击向量研究了防御技术，这些技术显示出增强GraphLLMs鲁棒性的潜在前景。我们希望开源库能够促进快速、公平的评估，并激励该领域的进一步创新研究。', 'title_zh': 'TrustGLM: 评估图LLMs在对抗提示、文本和结构攻击下的稳健性'}
{'arxiv_id': 'arXiv:2506.11798', 'title': 'Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models', 'authors': 'Maximilian Kreutner, Marlene Lutz, Markus Strohmaier', 'link': 'https://arxiv.org/abs/2506.11798', 'abstract': 'Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse, but have been found to consistently display a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups that the base model is not aligned with. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict positions of European groups on a diverse set of policies. We evaluate if predictions are stable towards counterfactual arguments, different persona prompts and generation methods. Finally, we find that we can simulate voting behavior of Members of the European Parliament reasonably well with a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）在理解和生成政治 discourse 方面表现出显著的能力，但被发现持续展现出进步的左倾偏向。同时，所谓的 persona 或身份提示已被证明能够产生与基础模型不一致的经济社会群体的行为。在本文中，我们分析零样本 persona 提示在有限信息下能否准确预测个人的投票决策，并通过聚合准确预测欧洲群体在一系列政策上的立场。我们评估预测是否具有对反事实论证、不同 persona 提示和生成方法的稳定性。最后，我们发现可以使用加权 F1 分数约为 0.793 的方法合理模拟欧洲议会成员的投票行为。我们的 politician persona 数据集和代码可在以下链接获取：this https URL。', 'title_zh': '基于个性驱动的欧洲议会投票行为仿真研究——利用大规模语言模型'}
{'arxiv_id': 'arXiv:2506.11638', 'title': 'LoRA-Gen: Specializing Large Language Model via Online LoRA Generation', 'authors': 'Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Yixiao Ge, Xiu Li, Ying Shan', 'link': 'https://arxiv.org/abs/2506.11638', 'abstract': 'Recent advances have highlighted the benefits of scaling language models to enhance performance across a wide range of NLP tasks. However, these approaches still face limitations in effectiveness and efficiency when applied to domain-specific tasks, particularly for small edge-side models. We propose the LoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA parameters for edge-side models based on task descriptions. By employing the reparameterization technique, we merge the LoRA parameters into the edge-side model to achieve flexible specialization. Our method facilitates knowledge transfer between models while significantly improving the inference efficiency of the specialized model by reducing the input context length. Without specialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which achieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in reasoning tasks. Besides, our method delivers a compression ratio of 10.1x with Gemma-2B on intelligent agent tasks.', 'abstract_zh': 'Recent advances have highlighted the benefits of scaling language models to enhance performance across a wide range of NLP tasks. However, these approaches still face limitations in effectiveness and efficiency when applied to domain-specific tasks, particularly for small edge-side models. We propose the LoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA parameters for edge-side models based on task descriptions. By employing the reparameterization technique, we merge the LoRA parameters into the edge-side model to achieve flexible specialization. Our method facilitates knowledge transfer between models while significantly improving the inference efficiency of the specialized model by reducing the input context length. Without specialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which achieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in reasoning tasks. Besides, our method delivers a compression ratio of 10.1x with Gemma-2B on intelligent agent tasks.', 'title_zh': 'LoRA-Gen: 基于在线LoRA生成的专业化大型语言模型'}
{'arxiv_id': 'arXiv:2506.11635', 'title': 'FAA Framework: A Large Language Model-Based Approach for Credit Card Fraud Investigations', 'authors': 'Shaun Shuster, Eyal Zaloof, Asaf Shabtai, Rami Puzis', 'link': 'https://arxiv.org/abs/2506.11635', 'abstract': 'The continuous growth of the e-commerce industry attracts fraudsters who exploit stolen credit card details. Companies often investigate suspicious transactions in order to retain customer trust and address gaps in their fraud detection systems. However, analysts are overwhelmed with an enormous number of alerts from credit card transaction monitoring systems. Each alert investigation requires from the fraud analysts careful attention, specialized knowledge, and precise documentation of the outcomes, leading to alert fatigue. To address this, we propose a fraud analyst assistant (FAA) framework, which employs multi-modal large language models (LLMs) to automate credit card fraud investigations and generate explanatory reports. The FAA framework leverages the reasoning, code execution, and vision capabilities of LLMs to conduct planning, evidence collection, and analysis in each investigation step. A comprehensive empirical evaluation of 500 credit card fraud investigations demonstrates that the FAA framework produces reliable and efficient investigations comprising seven steps on average. Thus we found that the FAA framework can automate large parts of the workload and help reduce the challenges faced by fraud analysts.', 'abstract_zh': '电子商务行业的持续增长吸引了利用盗刷信用卡信息的诈骗分子。公司通常会调查可疑交易以保留客户信任并解决其欺诈检测系统的漏洞。然而，分析师受到信用卡交易监控系统生成的大量警报的困扰。每项警报调查都需要欺诈分析师投入细致的关注、专业技能，并精确记录结果，导致警报疲劳。为应对这一问题，我们提出了一种欺诈分析师助手（FAA）框架，利用多模态大语言模型（LLMs）自动完成信用卡欺诈调查并生成解释性报告。FAA框架利用LLM的推理、代码执行和视觉能力，在每个调查步骤中进行计划、证据收集和分析。对500例信用卡欺诈调查的全面实证评估表明，FAA框架可以平均完成七个步骤的可靠和高效的调查。因此，我们发现FAA框架可以自动化大量工作负载，帮助减轻欺诈分析师面临的挑战。', 'title_zh': 'FAA框架：基于大型语言模型的信用卡欺诈调查方法'}
{'arxiv_id': 'arXiv:2506.11618', 'title': 'Convergent Linear Representations of Emergent Misalignment', 'authors': 'Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda', 'link': 'https://arxiv.org/abs/2506.11618', 'abstract': "Fine-tuning large language models on narrow datasets can cause them to develop broadly misaligned behaviours: a phenomena known as emergent misalignment. However, the mechanisms underlying this misalignment, and why it generalizes beyond the training domain, are poorly understood, demonstrating critical gaps in our knowledge of model alignment. In this work, we train and study a minimal model organism which uses just 9 rank-1 adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently misaligned models converge to similar representations of misalignment. We demonstrate this convergence by extracting a 'misalignment direction' from one fine-tuned model's activations, and using it to effectively ablate misaligned behaviour from fine-tunes using higher dimensional LoRAs and different datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of experiments for directly interpreting the fine-tuning adapters, showing that six contribute to general misalignment, while two specialise for misalignment in just the fine-tuning domain. Emergent misalignment is a particularly salient example of undesirable and unexpected model behaviour and by advancing our understanding of the mechanisms behind it, we hope to move towards being able to better understand and mitigate misalignment more generally.", 'abstract_zh': '大规模语言模型在窄数据集上的微调可能会导致其发展出广泛的不对齐行为：一种被称为 emergent misalignment 的现象。然而，这种不对齐的机制及其为什么会在训练域之外泛化的原理仍然不清楚，这凸显了我们对模型对齐理解中的关键空白。在本工作中，我们训练并研究了一个仅使用9个秩1适配器的最小模型有机体，使其对齐Qwen2.5-14B-Instruct产生emergent misalignment。通过对这一过程的研究，我们发现不同产生emergent misalignment的模型收敛到相似的对齐偏差表示。我们通过从一个微调模型的激活中提取“misalignment方向”，并使用它有效地消除更高维度LoRA和不同数据集中微调模型中的偏差行为，来证明这种收敛。利用秩1 LoRA的标量隐藏状态，我们进一步提出了一系列实验，直接解释微调适配器，结果显示六个适配器贡献于普遍对齐偏差，而两个适配器专门针对微调域内的对齐偏差。emergent misalignment 是一个特别突出的不良且出乎意料的模型行为的例子，通过深化我们对它背后的机制的理解，我们希望朝着更广泛地理解并减轻对齐偏差的目标迈进。', 'title_zh': '收敛线性表示的 emergent 对齐偏差'}
{'arxiv_id': 'arXiv:2506.11613', 'title': 'Model Organisms for Emergent Misalignment', 'authors': 'Edward Turner, Anna Soligo, Mia Taylor, Senthooran Rajamanoharan, Neel Nanda', 'link': 'https://arxiv.org/abs/2506.11613', 'abstract': 'Recent work discovered Emergent Misalignment (EM): fine-tuning large language models on narrowly harmful datasets can lead them to become broadly misaligned. A survey of experts prior to publication revealed this was highly unexpected, demonstrating critical gaps in our understanding of model alignment. In this work, we both advance understanding and provide tools for future research. Using new narrowly misaligned datasets, we create a set of improved model organisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B parameter models (vs. 32B), and that induce misalignment using a single rank-1 LoRA adapter. We demonstrate that EM occurs robustly across diverse model sizes, three model families, and numerous training protocols including full supervised fine-tuning. Leveraging these cleaner model organisms, we isolate a mechanistic phase transition and demonstrate that it corresponds to a robust behavioural phase transition in all studied organisms. Aligning large language models is critical for frontier AI safety, yet EM exposes how far we are from achieving this robustly. By distilling clean model organisms that isolate a minimal alignment-compromising change, and where this is learnt, we establish a foundation for future research into understanding and mitigating alignment risks in LLMs.', 'abstract_zh': '新兴Misalignment（EM）现象的研究：窄导向有害数据集的微调导致模型的广导向错误对齐，及其解决方法', 'title_zh': '新兴对齐偏差的模型 organism 系统'}
{'arxiv_id': 'arXiv:2506.11602', 'title': 'Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study', 'authors': 'Hawau Olamide Toyin, Samar M. Magdy, Hanan Aldarmaki', 'link': 'https://arxiv.org/abs/2506.11602', 'abstract': 'We investigate the effectiveness of large language models (LLMs) for text diacritization in two typologically distinct languages: Arabic and Yoruba. To enable a rigorous evaluation, we introduce a novel multilingual dataset MultiDiac, with diverse samples that capture a range of diacritic ambiguities. We evaluate 14 LLMs varying in size, accessibility, and language coverage, and benchmark them against 6 specialized diacritization models. Additionally, we fine-tune four small open-source models using LoRA for Yoruba. Our results show that many off-the-shelf LLMs outperform specialized diacritization models for both Arabic and Yoruba, but smaller models suffer from hallucinations. Fine-tuning on a small dataset can help improve diacritization performance and reduce hallucination rates.', 'abstract_zh': '我们调查了大型语言模型（LLMs）在阿拉伯语和约鲁巴语两种类型学上不同的语言中的文本音标化效果。为实现严格评估，我们引入了一个新型多 lingual 数据集 MultiDiac，包含多样化样本以捕捉各种音标模糊性。我们评估了14个不同规模、可访问性和语言覆盖面的LLMs，并将其与6个专门的音标化模型进行基准测试。此外，我们使用LoRA对四个小型开源模型进行了微调，以适应约鲁巴语。我们的结果表明，许多现成的LLMs在阿拉伯语和约鲁巴语的音标化任务中优于专门的音标化模型，但较小的模型会遭受幻觉问题。对小数据集的微调有助于提高音标化性能并降低幻觉率。', 'title_zh': '大型语言模型是好的文本标注器吗？阿拉伯语和约鲁巴语案例研究'}
{'arxiv_id': 'arXiv:2506.11561', 'title': 'Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study', 'authors': 'Gábor Antal, Bence Bogenfürst, Rudolf Ferenc, Péter Hegedűs', 'link': 'https://arxiv.org/abs/2506.11561', 'abstract': "Recent advancements in large language models (LLMs) have shown promise for automated vulnerability detection and repair in software systems. This paper investigates the performance of GPT-4o in repairing Java vulnerabilities from a widely used dataset (Vul4J), exploring how different contextual information affects automated vulnerability repair (AVR) capabilities. We compare the latest GPT-4o's performance against previous results with GPT-4 using identical prompts. We evaluated nine additional prompts crafted by us that contain various contextual information such as CWE or CVE information, and manually extracted code contexts. Each prompt was executed three times on 42 vulnerabilities, and the resulting fix candidates were validated using Vul4J's automated testing framework.\nOur results show that GPT-4o performed 11.9\\% worse on average than GPT-4 with the same prompt, but was able to fix 10.5\\% more distinct vulnerabilities in the three runs together. CVE information significantly improved repair rates, while the length of the task description had minimal impact. Combining CVE guidance with manually extracted code context resulted in the best performance. Using our \\textsc{Top}-3 prompts together, GPT-4o repaired 26 (62\\%) vulnerabilities at least once, outperforming both the original baseline (40\\%) and its reproduction (45\\%), suggesting that ensemble prompt strategies could improve vulnerability repair in zero-shot settings.", 'abstract_zh': '最近在大型语言模型（LLMs）方面取得的进展展示了其在软件系统中自动漏洞检测与修复方面的潜力。本文探讨了GPT-4o在使用广泛使用的Vul4J数据集修复Java漏洞方面的性能，探索不同上下文信息如何影响自动漏洞修复（AVR）能力。我们将最新版本的GPT-4o的性能与使用相同提示的GPT-4的先前结果进行了比较。我们评估了九个由我们定制的额外提示，这些提示包含各种上下文信息，如CWE或CVE信息，以及手动提取的代码上下文。每个提示在42个漏洞上执行三次，结果的修复候选方案使用Vul4J的自动化测试框架进行了验证。\n\n结果显示，GPT-4o在相同提示下平均表现比GPT-4差11.9%，但在三次运行中能够修复10.5%更多的独特漏洞。CWE/CVE信息显著提高了修复率，而任务描述的长度几乎没有影响。结合CWE指导与手动提取的代码上下文产生了最佳性能。使用我们定制的Top3提示，GPT-4o在至少修复了26个（62%）漏洞方面优于原始基线（40%）和其再现结果（45%），这表明集成提示策略可能在零样本设置中改善漏洞修复。', 'title_zh': '基于LLM的漏洞修复辅助上下文识别：一项初步研究'}
{'arxiv_id': 'arXiv:2506.11559', 'title': 'Leveraging GPT-4 for Vulnerability-Witnessing Unit Test Generation', 'authors': 'Gábor Antal, Dénes Bán, Martin Isztin, Rudolf Ferenc, Péter Hegedűs', 'link': 'https://arxiv.org/abs/2506.11559', 'abstract': "In the life-cycle of software development, testing plays a crucial role in quality assurance. Proper testing not only increases code coverage and prevents regressions but it can also ensure that any potential vulnerabilities in the software are identified and effectively fixed. However, creating such tests is a complex, resource-consuming manual process. To help developers and security experts, this paper explores the automatic unit test generation capability of one of the most widely used large language models, GPT-4, from the perspective of vulnerabilities. We examine a subset of the VUL4J dataset containing real vulnerabilities and their corresponding fixes to determine whether GPT-4 can generate syntactically and/or semantically correct unit tests based on the code before and after the fixes as evidence of vulnerability mitigation. We focus on the impact of code contexts, the effectiveness of GPT-4's self-correction ability, and the subjective usability of the generated test cases. Our results indicate that GPT-4 can generate syntactically correct test cases 66.5\\% of the time without domain-specific pre-training. Although the semantic correctness of the fixes could be automatically validated in only 7. 5\\% of the cases, our subjective evaluation shows that GPT-4 generally produces test templates that can be further developed into fully functional vulnerability-witnessing tests with relatively minimal manual effort.\nTherefore, despite the limited data, our initial findings suggest that GPT-4 can be effectively used in the generation of vulnerability-witnessing tests. It may not operate entirely autonomously, but it certainly plays a significant role in a partially automated process.", 'abstract_zh': '在软件开发生命周期中，测试在质量保证中扮演着关键角色。适当的测试不仅能增加代码覆盖率并防止回归问题，还能确保软件中的任何潜在漏洞能够被发现并有效修复。然而，创建这样的测试是一个复杂且资源密集型的手动过程。为了帮助开发者和安全专家，本文从漏洞的角度探讨了最广泛使用的大型语言模型之一GPT-4的自动单元测试生成能力。我们分析了VUL4J数据集中包含真实漏洞及相应修复的一组数据，以确定GPT-4是否能够根据在修复前后代码的证据生成符合语法和/或语义的单元测试，来验证漏洞缓解。我们重点关注代码上下文的影响、GPT-4自我校正能力的有效性以及生成测试案例的主观可用性。结果显示，GPT-4在没有领域特定预训练的情况下，能够生成符合语法的测试案例66.5%。虽然在7.5%的情况下可以自动验证语义正确性，但我们的主观评估表明，GPT-4生成的测试模板通常可以经过相对较少的手工开发进一步转变为功能全面的漏洞见证测试。因此，尽管数据有限，我们的初步发现表明，GPT-4可以在生成漏洞见证测试中得到有效利用。它可能不会完全自主运行，但确实可以在部分自动化的过程中发挥重要作用。', 'title_zh': '利用GPT-4生成漏洞见证单元测试'}
{'arxiv_id': 'arXiv:2506.11558', 'title': 'DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs', 'authors': 'Bo-Cheng Chiu, Jen-Jee Chen, Yu-Chee Tseng, Feng-Chi Chen', 'link': 'https://arxiv.org/abs/2506.11558', 'abstract': 'Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with GPT-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling.', 'abstract_zh': '大规模语言模型（LLMs）最近已扩展到视频领域，实现了复杂的视频-语言理解。然而，现有的视频LLMs常常在细粒度的时间推理方面表现出局限性，限制了其精确归因响应到视频特定时刻的能力，尤其是在受限的监督下。我们介绍DaMO，一种专为准确的时间推理和多模态理解设计的数据高效视频LLM。核心上，提出的Tempo-aware Fuseformer采用分层双流架构，逐步捕获每个模态内的时间动态并有效融合互补的视觉和音频信息。为了进一步提高计算效率，DaMO整合了一个全局残差，以减少空间冗余同时保留关键的语义细节。我们通过一个结构化的四阶段渐进训练范式训练DaMO，逐步赋予模型多模态对齐、语义 grounding 和时间推理能力。这项工作还贡献了多个通过GPT生成的时空关联问答对扩充的现有数据集，用于需要时间监督的任务。在时空定位和视频问答基准测试中的全面实验表明，DaMO始终优于先前的方法，尤其是在需要精确时间对齐和推理的任务中。我们的工作为数据高效视频-语言建模指出了一个有前景的方向。', 'title_zh': 'DaMO：一种高效的数据驱动多模态 orchestrator，用于视频LLM的时序推理'}
{'arxiv_id': 'arXiv:2506.11512', 'title': 'Prioritizing Alignment Paradigms over Task-Specific Model Customization in Time-Series LLMs', 'authors': 'Wei Li, Yunyao Cheng, Xinli Hao, Chaohong Ma, Yuxuan Liang, Bin Yang, Christian S.Jensen, Xiaofeng Meng', 'link': 'https://arxiv.org/abs/2506.11512', 'abstract': 'Recent advances in Large Language Models (LLMs) have enabled unprecedented capabilities for time-series reasoning in diverse real-world applications, including medical, financial, and spatio-temporal domains. However, existing approaches typically focus on task-specific model customization, such as forecasting and anomaly detection, while overlooking the data itself, referred to as time-series primitives, which are essential for in-depth reasoning. This position paper advocates a fundamental shift in approaching time-series reasoning with LLMs: prioritizing alignment paradigms grounded in the intrinsic primitives of time series data over task-specific model customization. This realignment addresses the core limitations of current time-series reasoning approaches, which are often costly, inflexible, and inefficient, by systematically accounting for intrinsic structure of data before task engineering. To this end, we propose three alignment paradigms: Injective Alignment, Bridging Alignment, and Internal Alignment, which are emphasized by prioritizing different aspects of time-series primitives: domain, characteristic, and representation, respectively, to activate time-series reasoning capabilities of LLMs to enable economical, flexible, and efficient reasoning. We further recommend that practitioners adopt an alignment-oriented method to avail this instruction to select an appropriate alignment paradigm. Additionally, we categorize relevant literature into these alignment paradigms and outline promising research directions.', 'abstract_zh': 'Recent Advances in Large Language Models for Time-Series Reasoning: A Paradigm Shift towards Alignment Based on Intrinsic Primitives', 'title_zh': '在时间序列LLM中优先考虑对齐范式而非任务特定模型定制'}
{'arxiv_id': 'arXiv:2506.11485', 'title': 'Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models', 'authors': 'Cole Gawin', 'link': 'https://arxiv.org/abs/2506.11485', 'abstract': "While large language models like BERT demonstrate strong empirical performance on semantic tasks, whether this reflects true conceptual competence or surface-level statistical association remains unclear. I investigate whether BERT encodes abstract relational schemata by examining internal representations of concept pairs across taxonomic, mereological, and functional relations. I compare BERT's relational classification performance with representational structure in [CLS] token embeddings. Results reveal that pretrained BERT enables high classification accuracy, indicating latent relational signals. However, concept pairs organize by relation type in high-dimensional embedding space only after fine-tuning on supervised relation classification tasks. This indicates relational schemata are not emergent from pretraining alone but can be induced via task scaffolding. These findings demonstrate that behavioral performance does not necessarily imply structured conceptual understanding, though models can acquire inductive biases for grounded relational abstraction through appropriate training.", 'abstract_zh': '大型语言模型如BERT在语义任务上表现出强大的实证性能，但这种性能是真实的概念能力还是表面的统计关联仍不清楚。我通过检查概念对在分类学、部分整体和功能关系下的内部表示，探讨BERT是否编码抽象关系框架。将BERT的关系分类性能与其[CLS]标记嵌入的表征结构进行比较。结果表明，预训练的BERT能够实现高分类准确性，显示潜在的关系信号。然而，在监督关系分类任务上的微调后，只有概念对在高维嵌入空间中按照关系类型组织。这表明关系框架并不是仅仅通过预训练产生的，而是可以通过任务支架促使生成。这些发现表明，行为性能并不必然意味着结构化概念理解，但模型可以通过适当的训练获取基于实际关系抽象的归纳偏置。', 'title_zh': 'BERT中的关系模式是可以诱导的，而非 Emergent：语言模型中性能与能力研究'}
{'arxiv_id': 'arXiv:2506.11480', 'title': 'LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large Language Models Based on Improved Gradient Alignment', 'authors': 'Shikun Li, Shipeng Li, Zhiqin Yang, Xinghua Zhang, Gaode Chen, Xiaobo Xia, Hengyu Liu, Zhe Peng', 'link': 'https://arxiv.org/abs/2506.11480', 'abstract': "Reinforcement learning (RL) has become a key technique for enhancing LLMs' reasoning abilities, yet its data inefficiency remains a major bottleneck. To address this critical yet challenging issue, we present a novel gradient-alignment-based method, named LearnAlign, which intelligently selects the learnable and representative training reasoning data for RL post-training. To overcome the well-known issue of response-length bias in gradient norms, we introduce the data learnability based on the success rate, which can indicate the learning potential of each data point. Experiments across three mathematical reasoning benchmarks demonstrate that our method significantly reduces training data requirements while achieving minor performance degradation or even improving performance compared to full-data training. For example, it reduces data requirements by up to 1,000 data points with better performance (77.53%) than that on the full dataset on GSM8K benchmark (77.04%). Furthermore, we show its effectiveness in the staged RL setting. This work provides valuable insights into data-efficient RL post-training and establishes a foundation for future research in optimizing reasoning data this http URL facilitate future work, we will release code.", 'abstract_zh': '强化学习（RL）已成为提升大语言模型（LLMs）推理能力的关键技术，然而其数据效率低下仍然是主要瓶颈。为解决这一关键而具有挑战性的问题，我们提出了一种基于梯度对齐的新型方法LearnAlign，该方法能够智能地选择可学习和具有代表性的训练推理数据，以进行RL后训练。为克服梯度范数中响应长度偏差的已知问题，我们引入了基于成功率的数据可学习性指标，它可以指示每个数据点的学习潜力。在三个数学推理基准测试上进行的实验表明，我们的方法在显著减少训练数据需求的同时，能够实现轻微的性能下降甚至提高性能，相比全数据训练。例如，在GSM8K基准测试中，使用更少的数据点（1,000个）时，性能提高到了77.53%，超过了全数据集的77.04%。此外，我们还展示了该方法在分阶段RL设置中的有效性。这项工作为数据高效RL后训练提供了有价值的见解，并为未来优化推理数据的研究奠定了基础。为促进未来工作，我们将发布代码。', 'title_zh': 'LearnAlign: 基于改进梯度对齐的数据选择推理在大规模语言模型中的强化学习'}
{'arxiv_id': 'arXiv:2506.11417', 'title': 'Stop learning it all to mitigate visual hallucination, Focus on the hallucination target', 'authors': 'Dokyoon Yoon, Youngsook Song, Woomyong Park', 'link': 'https://arxiv.org/abs/2506.11417', 'abstract': 'Multimodal Large Language Models (MLLMs) frequently suffer from hallucination issues, generating information about objects that are not present in input images during vision-language tasks. These hallucinations particularly undermine model reliability in practical applications requiring accurate object identification. To address this challenge, we propose \\mymethod,\\ a preference learning approach that mitigates hallucinations by focusing on targeted areas where they occur. To implement this, we build a dataset containing hallucinated responses, correct responses, and target information (i.e., objects present in the images and the corresponding chunk positions in responses affected by hallucinations). By applying a preference learning method restricted to these specific targets, the model can filter out irrelevant signals and focus on correcting hallucinations. This allows the model to produce more factual responses by concentrating solely on relevant information. Experimental results demonstrate that \\mymethod\\ effectively reduces hallucinations across multiple vision hallucination tasks, improving the reliability and performance of MLLMs without diminishing overall performance.', 'abstract_zh': '多模态大语言模型（MLLMs）在视觉语言任务中经常出现幻觉问题，生成输入图像中不存在的对象信息。这些幻觉尤其在需要准确对象识别的实际应用中削弱了模型的可靠性。为应对这一挑战，我们提出了一种名为\\mymethod\\的偏好学习方法，通过关注幻觉发生的目标区域来减轻幻觉问题。为此，我们构建了一个包含幻觉响应、正确响应和目标信息（即图像中存在的对象及其在受到幻觉影响的响应中的对应片段位置）的数据集。通过将偏好学习方法限制应用于这些特定目标，模型能够过滤掉无关信号并专注于纠正幻觉。这使模型能够通过集中关注相关信息来生成更真实的响应。实验结果表明，\\mymethod\\有效减少了多项视觉幻觉任务中的幻觉现象，提高了MLLMs的可靠性和性能，而不损害其整体性能。', 'title_zh': '专注于幻视目标，以减轻视觉幻视现象'}
{'arxiv_id': 'arXiv:2506.11412', 'title': 'The Strategic Imperative for Healthcare Organizations to Build Proprietary Foundation Models', 'authors': 'Naresh Tiwari', 'link': 'https://arxiv.org/abs/2506.11412', 'abstract': 'This paper presents a comprehensive analysis of the strategic imperative for healthcare organizations to develop proprietary foundation models rather than relying exclusively on commercial alternatives. We examine four fundamental considerations driving this imperative: the domain-specific requirements of healthcare data representation, critical data sovereignty and governance considerations unique to healthcare, strategic competitive advantages afforded by proprietary AI infrastructure, and the transformative potential of healthcare-specific foundation models for patient care and organizational operations. Through analysis of empirical evidence, economic frameworks, and organizational case studies, we demonstrate that proprietary multimodal foundation models enable healthcare organizations to achieve superior clinical performance, maintain robust data governance, create sustainable competitive advantages, and accelerate innovation pathways. While acknowledging implementation challenges, we present evidence showing organizations with proprietary AI capabilities demonstrate measurably improved outcomes, faster innovation cycles, and stronger strategic positioning in the evolving healthcare ecosystem. This analysis provides healthcare leaders with a comprehensive framework for evaluating build-versus-buy decisions regarding foundation model implementation, positioning proprietary foundation model development as a cornerstone capability for forward-thinking healthcare organizations.', 'abstract_zh': '本文对医疗保健组织开发专有的基础模型而非完全依赖商业替代品的战略紧迫性进行了全面分析。我们考察了驱动这一紧迫性的四个基本考虑因素：医疗保健数据表示的领域特定需求、独特的医疗保健数据主权和治理考虑、专有AI基础设施所提供的战略竞争优势，以及针对医疗保健的专用基础模型对患者护理和组织运营的变革潜力。通过分析实证证据、经济框架和组织案例研究，我们证明了专有的多模态基础模型使医疗保健组织能够实现卓越的临床表现、维护强大的数据治理、创建可持续的竞争优势，并加快创新路径。尽管承认实施挑战，我们展示的证据表明，拥有专有AI能力的组织在可测量的绩效方面表现更好，创新周期更短，并在不断演变的医疗保健生态系统中拥有更强的战略定位。本文为医疗保健领导者提供了评估基础模型实施的构建与购买决策的综合框架，强调了专有基础模型开发作为前瞻型医疗保健组织核心能力的重要性。', 'title_zh': 'healthcare组织构建自有基础模型的战略 imperative'}
{'arxiv_id': 'arXiv:2506.11402', 'title': 'LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model', 'authors': 'Pradyut Sekhsaria, Marcel Mateos Salles, Hai Huang, Randall Balestriero', 'link': 'https://arxiv.org/abs/2506.11402', 'abstract': "Parameter Efficient FineTuning (PEFT), such as Low-Rank Adaptation (LoRA), aligns pre-trained Large Language Models (LLMs) to particular downstream tasks in a resource-efficient manner. Because efficiency has been the main metric of progress, very little attention has been put in understanding possible catastrophic failures. We uncover one such failure: PEFT encourages a model to search for shortcut solutions to solve its fine-tuning tasks. When very small amount of tokens, e.g., one token per prompt, are correlated with downstream task classes, PEFT makes any pretrained model rely predominantly on that token for decision making. While such spurious tokens may emerge accidentally from incorrect data cleaning, it also opens opportunities for malevolent parties to control a model's behavior from Seamless Spurious Token Injection (SSTI). In SSTI, a small amount of tokens correlated with downstream classes are injected by the dataset creators. At test time, the finetuned LLM's behavior can be controlled solely by injecting those few tokens. We apply SSTI across models from three families (Snowflake Arctic, Apple OpenELM, and Meta LLaMA-3) and four diverse datasets (IMDB, Financial Classification, CommonSense QA, and Bias in Bios). Our findings reveal three astonishing behaviors. First, as few as a single token of SSTI is sufficient to steer a model's decision making. Second, for light SSTI, the reliance on spurious tokens is proportional to the LoRA rank. Lastly, with aggressive SSTI, larger LoRA rank values become preferable to small rank values as it makes the model attend to non-spurious tokens, hence improving robustness.", 'abstract_zh': '基于参数高效微调（PEFT）的灾难性失败探究：从捷径解找到无缝虚假令牌注入（SSTI）', 'title_zh': 'LoRA 用户注意：几个虚假令牌可操纵您的微调模型'}
{'arxiv_id': 'arXiv:2506.11305', 'title': "Don't Pay Attention", 'authors': 'Mohammad Hammoud, Devang Acharya', 'link': 'https://arxiv.org/abs/2506.11305', 'abstract': 'The Transformer has become the de facto standard for large language models and a wide range of downstream tasks across various domains. Despite its numerous advantages like inherent training parallelism, the Transformer still faces key challenges due to its inability to effectively process sequences beyond a fixed context window and the quadratic complexity of its attention mechanism. These challenges have renewed interest in RNN-like architectures, which offer linear scaling with sequence length and improved handling of long-range dependencies, albeit with limited parallelism due to their inherently recurrent nature. In this paper, we propose Avey, a new neural foundational architecture that breaks away from both attention and recurrence. Avey comprises a ranker and an autoregressive neural processor, which collaboratively identify and contextualize only the most relevant tokens for any given token, regardless of their positions in the sequence. Specifically, Avey decouples sequence length from context width, thus enabling effective processing of arbitrarily long sequences. Experimental results show that Avey compares favorably to the Transformer across a variety of standard short-range NLP benchmarks, while notably excelling at capturing long-range dependencies.', 'abstract_zh': '变压器已成为大型语言模型和各种领域广泛下游任务的事实标准。尽管变压器具有固有的训练并行性等众多优势，但由于其无法有效处理超出固定上下文窗口的序列以及其注意力机制的 quadratic 复杂性，变压器仍然面临关键挑战。这些挑战重新激发了对类似 RNN 的架构的兴趣，这些架构提供随序列长度线性扩展的能力并改善了长距离依赖性处理，尽管由于其固有的递归性质，其并行性有限。在本文中，我们提出了一种名为 Avey 的新型神经基础架构，该架构既不依赖于注意力机制也不依赖于递归性。Avey 包括一个排序器和一个自回归神经处理器，二者协同工作以识别并上下文化任意给定标记的最相关标记，而不受其在序列中的位置限制。具体而言，Avey 分离了序列长度和上下文宽度，从而能够有效处理任意长度的序列。实验结果表明，Avey 在多种标准短距离 NLP 基准测试中与变压器相当，并且特别擅长捕获长距离依赖性。', 'title_zh': '不要分散注意力'}
{'arxiv_id': 'arXiv:2506.11300', 'title': 'Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning', 'authors': 'Yang Zhang, Amr Mohamed, Hadi Abdine, Guokan Shang, Michalis Vazirgiannis', 'link': 'https://arxiv.org/abs/2506.11300', 'abstract': 'Curriculum learning has shown promise in improving training efficiency and generalization in various machine learning domains, yet its potential in pretraining language models remains underexplored, prompting our work as the first systematic investigation in this area. We experimented with different settings, including vanilla curriculum learning, pacing-based sampling, and interleaved curricula-guided by six difficulty metrics spanning linguistic and information-theoretic perspectives. We train models under these settings and evaluate their performance on eight diverse benchmarks. Our experiments reveal that curriculum learning consistently improves convergence in early and mid-training phases, and can yield lasting gains when used as a warmup strategy with up to $3.5\\%$ improvement. Notably, we identify compression ratio, lexical diversity, and readability as effective difficulty signals across settings. Our findings highlight the importance of data ordering in large-scale pretraining and provide actionable insights for scalable, data-efficient model development under realistic training scenarios.', 'abstract_zh': 'Curriculum 学习在预训练语言模型中的潜力尚未充分探索：一项系统性研究', 'title_zh': '超越随机采样：通过 Curriculum Learning 提升语言模型预训练效率'}
{'arxiv_id': 'arXiv:2506.11266', 'title': 'Invocable APIs derived from NL2SQL datasets for LLM Tool-Calling Evaluation', 'authors': 'Benjamin Elder, Anupama Murthi, Jungkoo Kang, Ankita Rajaram Naik, Kiran Kate, Kinjal Basu, Danish Contractor', 'link': 'https://arxiv.org/abs/2506.11266', 'abstract': 'Large language models (LLMs) are routinely deployed as agentic systems, with access to tools that interact with live environments to accomplish tasks. In enterprise deployments these systems need to interact with API collections that can be extremely large and complex, often backed by databases. In order to create datasets with such characteristics, we explore how existing NL2SQL (Natural Language to SQL query) datasets can be used to automatically create NL2API datasets. Specifically, this work describes a novel data generation pipeline that exploits the syntax of SQL queries to construct a functionally equivalent sequence of API calls. We apply this pipeline to one of the largest NL2SQL datasets, BIRD-SQL to create a collection of over 2500 APIs that can be served as invocable tools or REST-endpoints. We pair natural language queries from BIRD-SQL to ground-truth API sequences based on this API pool. We use this collection to study the performance of 10 public LLMs and find that all models struggle to determine the right set of tools (consisting of tasks of intent detection, sequencing with nested function calls, and slot-filling). We find that models have extremely low task completion rates (7-47 percent - depending on the dataset) which marginally improves to 50 percent when models are employed as ReACT agents that interact with the live API environment. The best task completion rates are far below what may be required for effective general-use tool-calling agents, suggesting substantial scope for improvement in current state-of-the-art tool-calling LLMs. We also conduct detailed ablation studies, such as assessing the impact of the number of tools available as well as the impact of tool and slot-name obfuscation. We compare the performance of models on the original SQL generation tasks and find that current models are sometimes able to exploit SQL better than APIs.', 'abstract_zh': '大规模语言模型（LLMs）通常被部署为代理系统，访问与实时环境互动以完成任务的工具。在企业部署中，这些系统需要与可以极其庞大和复杂的API集合交互，这些API集合通常由数据库支持。为了创建具有此类特性的数据集，我们探索了如何利用现有NL2SQL（自然语言到SQL查询）数据集自动生成NL2API数据集的方法。具体而言，这项工作描述了一种新颖的数据生成管道，该管道利用SQL查询的语法构建一个功能上等价的API调用序列。我们将这种管道应用于最大的NL2SQL数据集之一BIRD-SQL，创建了一个包含超过2500个API的集合，这些API可以作为可调用的工具或REST端点提供。我们将BIRD-SQL中的自然语言查询与基于此API池的真实API序列进行配对。我们使用此集合研究了10个公开的LLM的效果，发现所有模型在确定正确工具集（包括意图检测任务、嵌套函数调用的顺序排列和槽填充）方面均存在困难。我们发现模型的任务完成率极低（7-47%，取决于数据集），并在使用作为ReACT代理并与实时API环境互动时略有提高，达到50%。最好的任务完成率远远低于有效的通用用途工具调用代理所需的水平，这表明当前最先进的工具调用LLMs存在显著改进空间。我们还进行了详细的消融研究，例如评估可供使用的工具数量的影响以及工具和槽名称混淆的影响。我们比较了模型在原始SQL生成任务上的性能，发现当前模型有时能够比API更好地利用SQL。', 'title_zh': '来自NL2SQL数据集的可调用APIs及其在LLM工具调用评估中的应用'}
{'arxiv_id': 'arXiv:2506.11246', 'title': 'No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning', 'authors': 'Kushagra Dixit, Abhishek Rajgaria, Harshavardhan Kalalbandi, Dan Roth, Vivek Gupta', 'link': 'https://arxiv.org/abs/2506.11246', 'abstract': "Temporal Table Reasoning is a critical challenge for Large Language Models (LLMs), requiring effective prompting techniques to extract relevant insights. Despite existence of multiple prompting methods, their impact on table reasoning remains largely unexplored. Furthermore, the performance of these models varies drastically across different table and context structures, making it difficult to determine an optimal approach. This work investigates multiple prompting technique across diverse table types to determine optimal approaches for different scenarios. We find that performance varies based on entity type, table structure, requirement of additional context and question complexity, with NO single method consistently outperforming others. To mitigate these challenges, we introduce SEAR, an adaptive prompting framework inspired by human reasoning that dynamically adjusts based on context characteristics and integrates a structured reasoning. Our results demonstrate that SEAR achieves superior performance across all table types compared to other baseline prompting techniques. Additionally, we explore the impact of table structure refactoring, finding that a unified representation enhances model's reasoning.", 'abstract_zh': 'Temporal表推理是大语言模型（LLMs）的一个关键挑战，需要有效的提示技术来提取相关信息。尽管存在多种提示方法，但它们对表推理的影响仍很大程度上未被探索。此外，这些模型在不同表和背景结构上的表现差异巨大，使得难以确定最优方法。本研究调查了多种提示技术在不同表类型中的应用，以确定适用于不同场景的最优方法。我们发现，性能基于实体类型、表结构、额外背景信息的需求以及问题复杂性而变化，没有一种方法能够一贯优于其他方法。为缓解这些挑战，我们提出了SEAR，这是一种受人类推理启发的自适应提示框架，能够根据上下文特征动态调整并集成结构化推理。我们的结果表明，SEAR在所有表类型上都比其他基准提示技术表现出更优的性能。此外，我们探索了表结构重构的影响，发现统一的表示增强了解释模型的能力。', 'title_zh': '无通用提示：通过自适应提示统一时间表推理'}
{'arxiv_id': 'arXiv:2506.11243', 'title': 'RETUYT-INCO at BEA 2025 Shared Task: How Far Can Lightweight Models Go in AI-powered Tutor Evaluation?', 'authors': 'Santiago Góngora, Ignacio Sastre, Santiago Robaina, Ignacio Remersaro, Luis Chiruzzo, Aiala Rosá', 'link': 'https://arxiv.org/abs/2506.11243', 'abstract': 'In this paper, we present the RETUYT-INCO participation at the BEA 2025 shared task. Our participation was characterized by the decision of using relatively small models, with fewer than 1B parameters. This self-imposed restriction tries to represent the conditions in which many research labs or institutions are in the Global South, where computational power is not easily accessible due to its prohibitive cost. Even under this restrictive self-imposed setting, our models managed to stay competitive with the rest of teams that participated in the shared task. According to the $exact\\ F_1$ scores published by the organizers, the performance gaps between our models and the winners were as follows: $6.46$ in Track 1; $10.24$ in Track 2; $7.85$ in Track 3; $9.56$ in Track 4; and $13.13$ in Track 5. Considering that the minimum difference with a winner team is $6.46$ points -- and the maximum difference is $13.13$ -- according to the $exact\\ F_1$ score, we find that models with a size smaller than 1B parameters are competitive for these tasks, all of which can be run on computers with a low-budget GPU or even without a GPU.', 'abstract_zh': '本研究展现了RETUYT-INCO在BEA 2025共享任务中的参与情况。我们的参与特点是采用了相对较小的模型，参数量少于1亿。这一自我设限的限制试图代表全球南方许多研究实验室或机构的情况，因为这些地方由于高昂的成本，计算资源难以获得。即使在这种限制条件下，我们的模型仍然能够与其他参与共享任务的团队保持竞争力。根据主办方公布的精确F1分数，我们模型与获胜团队之间的性能差异如下：Track 1为6.46；Track 2为10.24；Track 3为7.85；Track 4为9.56；Track 5为13.13。考虑到最小差异为6.46分，最大差异为13.13分，根据精确F1分数，我们发现参数量小于1亿的模型对于这些任务具有竞争力，这些任务都可以在配备低成本GPU的计算机上运行，甚至可以在没有GPU的情况下运行。', 'title_zh': 'RETUYT-INCO在2025年BEA共享任务中的表现：轻量级模型在AI驱动的 tutor 评估中能走多远？'}
{'arxiv_id': 'arXiv:2506.11180', 'title': 'Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing', 'authors': 'Luis Miguel Vieira da Silva, Aljosha Köcher, Felix Gehlhoff', 'link': 'https://arxiv.org/abs/2506.11180', 'abstract': 'Explicit modeling of capabilities and skills -- whether based on ontologies, Asset Administration Shells, or other technologies -- requires considerable manual effort and often results in representations that are not easily accessible to Large Language Models (LLMs). In this work-in-progress paper, we present an alternative approach based on the recently introduced Model Context Protocol (MCP). MCP allows systems to expose functionality through a standardized interface that is directly consumable by LLM-based agents. We conduct a prototypical evaluation on a laboratory-scale manufacturing system, where resource functions are made available via MCP. A general-purpose LLM is then tasked with planning and executing a multi-step process, including constraint handling and the invocation of resource functions via MCP. The results indicate that such an approach can enable flexible industrial automation without relying on explicit semantic models. This work lays the basis for further exploration of external tool integration in LLM-driven production systems.', 'abstract_zh': '基于Model Context Protocol的能力与技能显式建模——一种替代方法', 'title_zh': '超越形式语义学：制造领域的能力与技能模型上下文协议'}
{'arxiv_id': 'arXiv:2506.11166', 'title': 'Test-Time-Scaling for Zero-Shot Diagnosis with Visual-Language Reasoning', 'authors': 'Ji Young Byun, Young-Jin Park, Navid Azizan, Rama Chellappa', 'link': 'https://arxiv.org/abs/2506.11166', 'abstract': 'As a cornerstone of patient care, clinical decision-making significantly influences patient outcomes and can be enhanced by large language models (LLMs). Although LLMs have demonstrated remarkable performance, their application to visual question answering in medical imaging, particularly for reasoning-based diagnosis, remains largely unexplored. Furthermore, supervised fine-tuning for reasoning tasks is largely impractical due to limited data availability and high annotation costs. In this work, we introduce a zero-shot framework for reliable medical image diagnosis that enhances the reasoning capabilities of LLMs in clinical settings through test-time scaling. Given a medical image and a textual prompt, a vision-language model processes a medical image along with a corresponding textual prompt to generate multiple descriptions or interpretations of visual features. These interpretations are then fed to an LLM, where a test-time scaling strategy consolidates multiple candidate outputs into a reliable final diagnosis. We evaluate our approach across various medical imaging modalities -- including radiology, ophthalmology, and histopathology -- and demonstrate that the proposed test-time scaling strategy enhances diagnostic accuracy for both our and baseline methods. Additionally, we provide an empirical analysis showing that the proposed approach, which allows unbiased prompting in the first stage, improves the reliability of LLM-generated diagnoses and enhances classification accuracy.', 'abstract_zh': '基于零样本框架的大语言模型在临床医学图像诊断中的推理能力增强：多模态医疗影像诊断中的测试时扩展策略', 'title_zh': '测试时缩放以实现基于视觉-语言推理的零样本诊断'}
{'arxiv_id': 'arXiv:2506.11140', 'title': 'Autonomous Computer Vision Development with Agentic AI', 'authors': 'Jin Kim, Muhammad Wahi-Anwa, Sangyun Park, Shawn Shin, John M. Hoffman, Matthew S. Brown', 'link': 'https://arxiv.org/abs/2506.11140', 'abstract': 'Agentic Artificial Intelligence (AI) systems leveraging Large Language Models (LLMs) exhibit significant potential for complex reasoning, planning, and tool utilization. We demonstrate that a specialized computer vision system can be built autonomously from a natural language prompt using Agentic AI methods. This involved extending SimpleMind (SM), an open-source Cognitive AI environment with configurable tools for medical image analysis, with an LLM-based agent, implemented using OpenManus, to automate the planning (tool configuration) for a particular computer vision task. We provide a proof-of-concept demonstration that an agentic system can interpret a computer vision task prompt, plan a corresponding SimpleMind workflow by decomposing the task and configuring appropriate tools. From the user input prompt, "provide sm (SimpleMind) config for lungs, heart, and ribs segmentation for cxr (chest x-ray)"), the agent LLM was able to generate the plan (tool configuration file in YAML format), and execute SM-Learn (training) and SM-Think (inference) scripts autonomously. The computer vision agent automatically configured, trained, and tested itself on 50 chest x-ray images, achieving mean dice scores of 0.96, 0.82, 0.83, for lungs, heart, and ribs, respectively. This work shows the potential for autonomous planning and tool configuration that has traditionally been performed by a data scientist in the development of computer vision applications.', 'abstract_zh': '代理人工智能（AI）系统利用大型语言模型（LLMs）在复杂推理、规划和工具利用方面展现出了显著潜力。我们证明，可以使用代理AI方法从自然语言提示自主构建专门的计算机视觉系统。这包括将开源认知AI环境SimpleMind扩展为结合了基于LLM的代理，并使用OpenManus实现，以自动化特定计算机视觉任务的规划（工具配置）。从用户输入提示“提供针对胸部X光片的肺部、心脏和肋骨分割的SimpleMind配置”，代理LLM能够生成计划（以YAML格式的工具配置文件），并自主执行SM-Learn（训练）和SM-Think（推理）脚本。计算机视觉代理自主配置、训练并测试了50张胸部X光片，肺部、心脏和肋骨的平均_dice得分分别为0.96、0.82和0.83。这项工作展示了以往需要数据科学家在计算机视觉应用开发中进行的传统规划和工具配置的自主潜力。', 'title_zh': '自主智能代理计算机视觉开发'}
{'arxiv_id': 'arXiv:2506.11135', 'title': 'Large Language Models and Emergence: A Complex Systems Perspective', 'authors': 'David C. Krakauer, John W. Krakauer, Melanie Mitchell', 'link': 'https://arxiv.org/abs/2506.11135', 'abstract': 'Emergence is a concept in complexity science that describes how many-body systems manifest novel higher-level properties, properties that can be described by replacing high-dimensional mechanisms with lower-dimensional effective variables and theories. This is captured by the idea "more is different". Intelligence is a consummate emergent property manifesting increasingly efficient -- cheaper and faster -- uses of emergent capabilities to solve problems. This is captured by the idea "less is more". In this paper, we first examine claims that Large Language Models exhibit emergent capabilities, reviewing several approaches to quantifying emergence, and secondly ask whether LLMs possess emergent intelligence.', 'abstract_zh': '复杂科学中的涌现是一种概念，描述了多体系统如何表现出新的高级属性，这些属性可以通过用低维的有效变量和理论来替代高维机制来描述。“更多是不同”的理念涵盖了这一过程。智能是达到极致的涌现属性，表现在越来越高效、更便宜和更快地利用涌现能力解决问题。“更少是更多”的理念概括了这一过程。本文首先考察大型语言模型是否表现出涌现能力，回顾几种度量涌现的方法，其次探讨大型语言模型是否具有涌现智能。', 'title_zh': '大型语言模型与涌现：一个复杂系统视角'}
{'arxiv_id': 'arXiv:2506.11129', 'title': 'Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK', 'authors': 'Carlos Garcia-Fernandez, Luis Felipe, Monique Shotande, Muntasir Zitu, Aakash Tripathi, Ghulam Rasool, Issam El Naqa, Vivek Rudrapatna, Gilmer Valdes', 'link': 'https://arxiv.org/abs/2506.11129', 'abstract': "Large language models (LLMs) show promise in healthcare, but hallucinations remain a major barrier to clinical use. We present CHECK, a continuous-learning framework that integrates structured clinical databases with a classifier grounded in information theory to detect both factual and reasoning-based hallucinations. Evaluated on 1500 questions from 100 pivotal clinical trials, CHECK reduced LLama3.3-70B-Instruct hallucination rates from 31% to 0.3% - making an open source model state of the art. Its classifier generalized across medical benchmarks, achieving AUCs of 0.95-0.96, including on the MedQA (USMLE) benchmark and HealthBench realistic multi-turn medical questioning. By leveraging hallucination probabilities to guide GPT-4o's refinement and judiciously escalate compute, CHECK boosted its USMLE passing rate by 5 percentage points, achieving a state-of-the-art 92.1%. By suppressing hallucinations below accepted clinical error thresholds, CHECK offers a scalable foundation for safe LLM deployment in medicine and other high-stakes domains.", 'abstract_zh': 'Large语言模型（LLMs）在医疗领域的应用前景广阔，但幻觉仍是临床应用的主要障碍。我们提出了一种连续学习框架CHECK，该框架结合了结构化临床数据库和基于信息论的分类器，以检测事实性和推理性幻觉。CHECK在来自100项关键临床试验的1500个问题上进行评估，将LLama3.3-70B-Instruct的幻觉率从31%降至0.3%，使其开源模型达到领先水平。其分类器在各种医学基准上表现出色，AUC值达到0.95-0.96，包括MedQA（USMLE）基准和HealthBench现实多轮医疗问答。通过利用幻觉概率指导GPT-4o的改进，并谨慎增加计算资源，CHECK将USMLE通过率提高了5个百分点，达到92.1%的领先水平。通过将幻觉抑制在可接受的临床误差阈值以下，CHECK为医疗和其他高风险领域安全地部署大型语言模型提供了可扩展的基础。', 'title_zh': '可信AI医疗：基于CHECK的持续幻觉检测与消除'}
{'arxiv_id': 'arXiv:2506.11128', 'title': 'Stronger Language Models Produce More Human-Like Errors', 'authors': 'Andrew Keenan Richardson, Ryan Othniel Kearns, Sean Moss, Vincent Wang-Mascianica, Philipp Koralus', 'link': 'https://arxiv.org/abs/2506.11128', 'abstract': 'Do language models converge toward human-like reasoning patterns as they improve? We provide surprising evidence that while overall reasoning capabilities increase with model sophistication, the nature of errors increasingly mirrors predictable human reasoning fallacies: a previously unobserved inverse scaling phenomenon. To investigate this question, we apply the Erotetic Theory of Reasoning (ETR), a formal cognitive framework with empirical support for predicting human reasoning outcomes. Using the open-source package PyETR, we generate logical reasoning problems where humans predictably err, evaluating responses from 38 language models across 383 reasoning tasks. Our analysis indicates that as models advance in general capability (as measured by Chatbot Arena scores), the proportion of their incorrect answers that align with ETR-predicted human fallacies tends to increase ($\\rho = 0.360, p = 0.0265$). Notably, as we observe no correlation between model sophistication and logical correctness on these tasks, this shift in error patterns toward human-likeness occurs independently of error rate. These findings challenge the prevailing view that scaling language models naturally obtains normative rationality, suggesting instead a convergence toward human-like cognition inclusive of our characteristic biases and limitations, as we further confirm by demonstrating order-effects in language model reasoning.', 'abstract_zh': '语言模型在性能提升过程中是否向人类类推理模式收敛？我们提供了令人惊讶的证据，尽管模型的整体推理能力随复杂性增加而提高，但错误的性质越来越接近可预测的人类推理谬误：一种先前未观察到的逆向 scaling 现象。', 'title_zh': '更强的语言模型会产生更多的人类错误'}
{'arxiv_id': 'arXiv:2506.11125', 'title': 'ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams', 'authors': 'Freddie Grabovski, Gilad Gressel, Yisroel Mirsky', 'link': 'https://arxiv.org/abs/2506.11125', 'abstract': "Large Language Models (LLMs), combined with Text-to-Speech (TTS) and Automatic Speech Recognition (ASR), are increasingly used to automate voice phishing (vishing) scams. These systems are scalable and convincing, posing a significant security threat. We identify the ASR transcription step as the most vulnerable link in the scam pipeline and introduce ASRJam, a proactive defence framework that injects adversarial perturbations into the victim's audio to disrupt the attacker's ASR. This breaks the scam's feedback loop without affecting human callers, who can still understand the conversation. While prior adversarial audio techniques are often unpleasant and impractical for real-time use, we also propose EchoGuard, a novel jammer that leverages natural distortions, such as reverberation and echo, that are disruptive to ASR but tolerable to humans. To evaluate EchoGuard's effectiveness and usability, we conducted a 39-person user study comparing it with three state-of-the-art attacks. Results show that EchoGuard achieved the highest overall utility, offering the best combination of ASR disruption and human listening experience.", 'abstract_zh': '大规模语言模型（LLMs）结合文本转语音（TTS）和自动语音识别（ASR）技术，越来越多地被用于自动化语音钓鱼（vishing）骗局。这些系统具备可扩展性和说服力，构成了重大的安全威胁。我们识别出ASR转录步骤是骗局流程中最脆弱的环节，并介绍了一种名为ASRJam的主动防御框架，该框架向受害者的音频中注入对抗性扰动，以干扰攻击者的ASR，从而打断骗局的反馈循环，而不影响人类来电者的理解。虽然先前的对抗性音频技术往往不愉快且不适合实时使用，我们还提出了EchoGuard这一新型干扰器，利用如回声和混响等自然失真，这些失真对ASR有干扰作用但对人类是可接受的。为了评估EchoGuard的有效性和可用性，我们进行了39人用户研究，将其与三种最先进的攻击方法进行比较。结果显示，EchoGuard在总体效益上最高，提供了最佳的ASR干扰与人类听觉体验的平衡。', 'title_zh': 'ASRJam：面向人类的AI语音干扰技术以防止自动电话诈骗'}
{'arxiv_id': 'arXiv:2506.11121', 'title': 'SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR', 'authors': 'Wei-Ping Huang, Guan-Ting Lin, Hung-yi Lee', 'link': 'https://arxiv.org/abs/2506.11121', 'abstract': 'Despite progress in end-to-end ASR, real-world domain mismatches still cause performance drops, which Test-Time Adaptation (TTA) aims to mitigate by adjusting models during inference. Recent work explores combining TTA with external language models, using techniques like beam search rescoring or generative error correction. In this work, we identify a previously overlooked challenge: TTA can interfere with language model rescoring, revealing the nontrivial nature of effectively combining the two methods. Based on this insight, we propose SUTA-LM, a simple yet effective extension of SUTA, an entropy-minimization-based TTA approach, with language model rescoring. SUTA-LM first applies a controlled adaptation process guided by an auto-step selection mechanism leveraging both acoustic and linguistic information, followed by language model rescoring to refine the outputs. Experiments on 18 diverse ASR datasets show that SUTA-LM achieves robust results across a wide range of domains.', 'abstract_zh': '尽管端到端ASR有了进展，现实世界的领域 mismatch 仍导致性能下降，测试时自适应（TTA）目标是通过推断时调整模型来减轻这一问题。近期研究探索将TTA与外部语言模型结合，使用如束搜索重新评分或生成式错误校正等技术。在本工作中，我们识别了一个先前未被注意的挑战：TTA可以干扰语言模型重新评分，揭示了有效结合这两种方法的非平凡性。基于这一洞察，我们提出了一种简单的有效扩展方法——SUTA-LM，它是在基于熵最小化的TTA方法SUTA基础上，结合了语言模型重新评分的简单有效扩展。SUTA-LM首先通过利用声学和语言信息的自动步长选择机制指导一个受控的适应过程，之后进行语言模型重新评分以精炼输出。在18个不同的ASR数据集上的实验表明，SUTA-LM在多种领域中表现出稳健的结果。', 'title_zh': 'SUTA-LM：连接测试时适应与语言模型重评分以实现稳健的自动语音识别'}
{'arxiv_id': 'arXiv:2506.11120', 'title': 'SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models', 'authors': 'Hourun Zhu, Chengchao Shen', 'link': 'https://arxiv.org/abs/2506.11120', 'abstract': 'In spite of strong performance achieved by LLMs, the costs of their deployment are unaffordable. For the compression of LLMs, gradient-based pruning methods present promising effectiveness. However, in these methods, the gradient computation with one-hot labels ignore the potential predictions on other words, thus missing key information for generative capability of the original model. To address this issue, we introduce a self-distillation loss during the pruning phase (rather than post-training) to fully exploit the predictions of the original model, thereby obtaining more accurate gradient information for pruning. Moreover, we find that, compared to attention modules, the predictions of LLM are less sensitive to multilayer perceptron (MLP) modules, which take up more than $5 \\times$ parameters (LLaMA3.2-1.2B). To this end, we focus on the pruning of MLP modules, to significantly compress LLM without obvious performance degradation. Experimental results on extensive zero-shot benchmarks demonstrate that our method significantly outperforms existing pruning methods. Furthermore, our method achieves very competitive performance among 1B-scale open source LLMs. The source code and trained weights are available at this https URL.', 'abstract_zh': '尽管大规模语言模型取得了strong performance，但其部署成本高昂。为压缩语言模型，基于梯度的剪枝方法显示出良好的效果。然而，在这些方法中，使用one-hot标签的梯度计算忽略了其他词的潜在预测，从而错过了原始模型生成能力的关键信息。为解决这一问题，我们引入了一种自我蒸馏损失，在剪枝阶段（而非后训练阶段）使用此损失，以充分利用原始模型的预测，从而获得更准确的梯度信息进行剪枝。此外，我们发现，与注意力模块相比，语言模型的预测对多层感知机（MLP）模块的敏感性较低，而后者占据的参数量超过前者约5倍（如LLaMA3.2-1.2B）。为此，我们专注于MLP模块的剪枝，以显著压缩语言模型而不显著影响性能。广泛的零样本基准实验结果表明，我们的方法显著优于现有的剪枝方法。此外，我们的方法在1B规模的开源语言模型中表现出很强的竞争力。源代码和训练权重可在以下链接获取。', 'title_zh': 'SDMPrune: 自洽蒸馏MLP剪枝以实现高效大型语言模型'}
{'arxiv_id': 'arXiv:2506.11116', 'title': 'Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models', 'authors': 'Jijie Li, Li Du, Hanyu Zhao, Bo-wen Zhang, Liangdong Wang, Boyan Gao, Guang Liu, Yonghua Lin', 'link': 'https://arxiv.org/abs/2506.11116', 'abstract': 'Large Language Models (LLMs) demonstrate strong performance in real-world applications, yet existing open-source instruction datasets often concentrate on narrow domains, such as mathematics or coding, limiting generalization and widening the gap with proprietary models. To bridge this gap, we introduce Infinity-Instruct, a high-quality instruction dataset designed to enhance both foundational and chat capabilities of LLMs through a two-phase pipeline. In Phase 1, we curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques. In Phase 2, we synthesize 1.5M high-quality chat instructions (InfInstruct-G-1.5M) through a two-stage process involving instruction selection, evolution, and diagnostic filtering. We empirically evaluate Infinity-Instruct by fine-tuning several open-source models, including Mistral, LLaMA, Qwen, and Yi, and observe substantial performance gains across both foundational and instruction following benchmarks, consistently surpassing official instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B outperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving comparable foundational performance. These results underscore the synergy between foundational and chat training and offer new insights into holistic LLM development. Our dataset\\footnote{this https URL} and codes\\footnote{this https URL} have been publicly released.', 'abstract_zh': '大型语言模型（LLMs）在现实应用中展现了强大的性能，但现有的开源指令数据集往往集中在狭窄的领域，如数学或编码，限制了泛化能力并加大了与专用模型的差距。为弥补这一差距，我们引入了Infinity-Instruct，这是一个高质量的指令数据集，旨在通过两阶段管道增强LLMs的基础性和对话能力。在第一阶段，我们从超过1亿个样本中精选出740万条高质量的基础指令（InfInstruct-F-7.4M），采用混合数据选择技术。在第二阶段，我们通过包含指令选择、演变和诊断过滤的两阶段过程，合成了150万条高质量的对话指令（InfInstruct-G-1.5M）。通过微调多个开源模型，包括Mistral、LLaMA、Qwen和Yi，我们实证评估了Infinity-Instruct，并在基础能力与指令遵循基准测试中观察到显著的性能提升，且始终超越官方指令微调版本。特别是在指令遵循任务上，InfInstruct-LLaMA3.1-70B的表现比GPT-4-0314高出8.6%，同时保持相当的基础性能。这些结果突显了基础训练与对话训练之间的协同作用，并为全面的LLM开发提供了新的见解。我们的数据集和代码已公开发布。', 'title_zh': 'Infinity Instruct：扩展指令选择与合成以增强语言模型'}
{'arxiv_id': 'arXiv:2506.11114', 'title': 'KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations', 'authors': 'Junyu Liu, Kaiqi Yan, Tianyang Wang, Qian Niu, Momoko Nagai-Tanima, Tomoki Aoyama', 'link': 'https://arxiv.org/abs/2506.11114', 'abstract': 'Recent advances in large language models (LLMs) have demonstrated notable performance in medical licensing exams. However, comprehensive evaluation of LLMs across various healthcare roles, particularly in high-stakes clinical scenarios, remains a challenge. Existing benchmarks are typically text-based, English-centric, and focus primarily on medicines, which limits their ability to assess broader healthcare knowledge and multimodal reasoning. To address these gaps, we introduce KokushiMD-10, the first multimodal benchmark constructed from ten Japanese national healthcare licensing exams. This benchmark spans multiple fields, including Medicine, Dentistry, Nursing, Pharmacy, and allied health professions. It contains over 11588 real exam questions, incorporating clinical images and expert-annotated rationales to evaluate both textual and visual reasoning. We benchmark over 30 state-of-the-art LLMs, including GPT-4o, Claude 3.5, and Gemini, across both text and image-based settings. Despite promising results, no model consistently meets passing thresholds across domains, highlighting the ongoing challenges in medical AI. KokushiMD-10 provides a comprehensive and linguistically grounded resource for evaluating and advancing reasoning-centric medical AI across multilingual and multimodal clinical tasks.', 'abstract_zh': '最近的大语言模型进展已在医学执照考试中显示出了显著表现，但在各种卫生保健角色，特别是在高风险临床场景中的全面评估依然具有挑战性。现有基准通常基于文本，以英语为中心，并主要关注药物知识，限制了它们评估更广泛的卫生保健知识和多模态推理的能力。为解决这些缺口，我们引入了KokushiMD-10，这是第一个基于十个日本国家卫生保健执照考试构建的多模态基准。该基准涵盖了多个领域，包括医学、牙科、护理学、药学和相关卫生专业。它包含了超过11588个真实考试问题，并结合了临床图像和专家标注的理由，以评估文本和视觉推理能力。我们在文本和图像设置中对标了30多种最先进的大语言模型，包括GPT-4o、Claude 3.5和Gemini。尽管取得了令人期待的结果，但没有一个模型能在各个领域一致性地达到及格标准，这突显了医疗AI持续面临的挑战。KokushiMD-10为评估和推进多语言和多模态临床任务中的推理导向型医疗AI提供了全面的语言基础资源。', 'title_zh': 'KokushiMD-10：十项日本国家医疗执照考试评价大语言模型的基准'}
{'arxiv_id': 'arXiv:2506.11113', 'title': 'Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks', 'authors': 'Tzu-Ling Lin, Wei-Chih Chen, Teng-Fang Hsiao, Hou-I Liu, Ya-Hsin Yeh, Yu Kai Chan, Wen-Sheng Lien, Po-Yen Kuo, Philip S. Yu, Hong-Han Shuai', 'link': 'https://arxiv.org/abs/2506.11113', 'abstract': 'Peer review is essential for maintaining academic quality, but the increasing volume of submissions places a significant burden on reviewers. Large language models (LLMs) offer potential assistance in this process, yet their susceptibility to textual adversarial attacks raises reliability concerns. This paper investigates the robustness of LLMs used as automated reviewers in the presence of such attacks. We focus on three key questions: (1) The effectiveness of LLMs in generating reviews compared to human reviewers. (2) The impact of adversarial attacks on the reliability of LLM-generated reviews. (3) Challenges and potential mitigation strategies for LLM-based review. Our evaluation reveals significant vulnerabilities, as text manipulations can distort LLM assessments. We offer a comprehensive evaluation of LLM performance in automated peer reviewing and analyze its robustness against adversarial attacks. Our findings emphasize the importance of addressing adversarial risks to ensure AI strengthens, rather than compromises, the integrity of scholarly communication.', 'abstract_zh': '大型语言模型在面对文本对抗攻击时作为自动化审稿人的稳健性研究', 'title_zh': '破解审稿人：在文本 adversarial 攻击下大型语言模型在自动化同行评审中的脆弱性评估'}
{'arxiv_id': 'arXiv:2506.11111', 'title': 'Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions', 'authors': 'Kun Zhang, Le Wu, Kui Yu, Guangyi Lv, Dacao Zhang', 'link': 'https://arxiv.org/abs/2506.11111', 'abstract': 'Large Language Models (LLMs) have gained enormous attention in recent years due to their capability of understanding and generating natural languages. With the rapid development and wild-range applications (e.g., Agents, Embodied Intelligence), the robustness of LLMs has received increased attention. As the core brain of many AI applications, the robustness of LLMs requires that models should not only generate consistent contents, but also ensure the correctness and stability of generated content when dealing with unexpeted application scenarios (e.g., toxic prompts, limited noise domain data, outof-distribution (OOD) applications, etc). In this survey paper, we conduct a thorough review of the robustness of LLMs, aiming to provide a comprehensive terminology of concepts and methods around this field and facilitate the community. Specifically, we first give a formal definition of LLM robustness and present the collection protocol of this survey paper. Then, based on the types of perturbated inputs, we organize this survey from the following perspectives: 1) Adversarial Robustness: tackling the problem that prompts are manipulated intentionally, such as noise prompts, long context, data attack, etc; 2) OOD Robustness: dealing with the unexpected real-world application scenarios, such as OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of Robustness: summarizing the new evaluation datasets, metrics, and tools for verifying the robustness of LLMs. After reviewing the representative work from each perspective, we discuss and highlight future opportunities and research directions in this field. Meanwhile, we also organize related works and provide an easy-to-search project (this https URL) to support the community.', 'abstract_zh': '大型语言模型（LLMs）的鲁棒性：综述', 'title_zh': '评估与提升大型语言模型的鲁棒性：一项综述与未来方向'}
{'arxiv_id': 'arXiv:2506.11110', 'title': 'AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models', 'authors': 'Jaeho Lee, Atharv Chowdhary', 'link': 'https://arxiv.org/abs/2506.11110', 'abstract': 'Recent benchmarks have probed factual consistency and rhetorical robustness in Large Language Models (LLMs). However, a knowledge gap exists regarding how directional framing of factually true statements influences model agreement, a common scenario for LLM users. AssertBench addresses this by sampling evidence-supported facts from FEVEROUS, a fact verification dataset. For each (evidence-backed) fact, we construct two framing prompts: one where the user claims the statement is factually correct, and another where the user claims it is incorrect. We then record the model\'s agreement and reasoning. The desired outcome is that the model asserts itself, maintaining consistent truth evaluation across both framings, rather than switching its evaluation to agree with the user. AssertBench isolates framing-induced variability from the model\'s underlying factual knowledge by stratifying results based on the model\'s accuracy on the same claims when presented neutrally. In doing so, this benchmark aims to measure an LLM\'s ability to "stick to its guns" when presented with contradictory user assertions about the same fact. The complete source code is available at this https URL.', 'abstract_zh': '近期的研究基准已经探索了大型语言模型（LLMs）在事实一致性方面的表现和论辩稳健性。然而，在方向性框架对事实正确陈述的影响导致模型一致性变化这一常见场景方面，仍然存在知识差距。AssertBench通过从FEVEROUS事实验证数据集中采样证据支持的事实来解决这一问题。对于每个（证据支持的）事实，我们构建两种框架提示：一种情况下用户声称该陈述是事实正确的，另一种情况下用户声称该陈述是不正确的。然后记录模型的共识和推理过程。期望的结果是模型坚持自己的立场，在两种框架下保持一致的真相评估，而不是根据用户的说法改变评估。AssertBench通过根据模型在中立呈现同一主张时的准确性来分层结果，从而隔离由框架引起的变化，旨在衡量LLM在面对用户关于同一事实的矛盾主张时坚持自己结论的能力。完整的源代码可在以下网址获得：this https URL。', 'title_zh': 'AssertBench: 一个用于评估大型语言模型自我断言能力的基准测试'}
{'arxiv_id': 'arXiv:2506.11109', 'title': 'Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization', 'authors': 'Yile Chen, Yicheng Tao, Yue Jiang, Shuai Liu, Han Yu, Gao Cong', 'link': 'https://arxiv.org/abs/2506.11109', 'abstract': "The widespread adoption of location-based services has led to the generation of vast amounts of mobility data, providing significant opportunities to model user movement dynamics within urban environments. Recent advancements have focused on adapting Large Language Models (LLMs) for mobility analytics. However, existing methods face two primary limitations: inadequate semantic representation of locations (i.e., discrete IDs) and insufficient modeling of mobility signals within LLMs (i.e., single templated instruction fine-tuning). To address these issues, we propose QT-Mob, a novel framework that significantly enhances LLMs for mobility analytics. QT-Mob introduces a location tokenization module that learns compact, semantically rich tokens to represent locations, preserving contextual information while ensuring compatibility with LLMs. Furthermore, QT-Mob incorporates a series of complementary fine-tuning objectives that align the learned tokens with the internal representations in LLMs, improving the model's comprehension of sequential movement patterns and location semantics. The proposed QT-Mob framework not only enhances LLMs' ability to interpret mobility data but also provides a more generalizable approach for various mobility analytics tasks. Experiments on three real-world dataset demonstrate the superior performance in both next-location prediction and mobility recovery tasks, outperforming existing deep learning and LLM-based methods.", 'abstract_zh': '基于位置的服务普及产生了大量移动数据，为城市环境中用户移动动态建模提供了重要机会。近期进展集中在将大规模语言模型（LLMs）适应于移动性分析。然而，现有方法主要面临两个局限：位置表示不足（即离散ID）和移动信号建模不足（即单一模板指令微调）。为解决这些问题，我们提出了QT-Mob框架，该框架显著提高了LLMs在移动性分析中的性能。QT-Mob引入了一种位置标记化模块，学习紧凑且语义丰富的标记来表示位置，并同时保留上下文信息，确保与LLMs兼容。此外，QT-Mob还整合了一系列互补的微调目标，将学习到的标记与LLMs的内部表示对齐，从而提高模型对序列移动模式和位置语义的理解能力。提出的QT-Mob框架不仅增强了LLMs解释移动数据的能力，还为各种移动性分析任务提供了一种更通用的方法。在三个真实世界数据集上的实验展示了在下个位置预测和移动性恢复任务中的优越性能，优于现有的深度学习和基于LLM的方法。', 'title_zh': '增强大型语言模型在语义位置分词下的移动性分析能力'}
{'arxiv_id': 'arXiv:2506.11106', 'title': 'Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking', 'authors': 'Ningyuan Li, Junrui Liu, Yi Shan, Minghui Huang, Tong Li', 'link': 'https://arxiv.org/abs/2506.11106', 'abstract': "Contemporary graph-based retrieval-augmented generation (RAG) methods typically begin by extracting entities from user queries and then leverage pre-constructed knowledge graphs to retrieve related relationships and metadata. However, this pipeline's exclusive reliance on entity-level extraction can lead to the misinterpretation or omission of latent yet critical information and relations. As a result, retrieved content may be irrelevant or contradictory, and essential knowledge may be excluded, exacerbating hallucination risks and degrading the fidelity of generated responses. To address these limitations, we introduce PankRAG, a framework that combines a globally aware, hierarchical query-resolution strategy with a novel dependency-aware reranking mechanism. PankRAG first constructs a multi-level resolution path that captures both parallel and sequential interdependencies within a query, guiding large language models (LLMs) through structured reasoning. It then applies its dependency-aware reranker to exploit the dependency structure among resolved sub-questions, enriching and validating retrieval results for subsequent sub-questions. Empirical evaluations demonstrate that PankRAG consistently outperforms state-of-the-art approaches across multiple benchmarks, underscoring its robustness and generalizability.", 'abstract_zh': '基于图的 contemporary 检索增强生成（RAG）方法通常首先从用户查询中提取实体，然后利用预先构建的知识图谱检索相关关系和元数据。然而，这种管道依赖于实体级提取，可能导致潜在但关键的信息和关系被误解或遗漏。因此，检索的内容可能与不相关或存在矛盾，必要知识可能被排除，加剧幻觉风险并降低生成响应的准确性。为解决这些限制，我们提出 PankRAG 框架，该框架结合了全局意识的分层查询解决策略和新颖的依存关系意识重排序机制。PankRAG 首先构建一个多级解决路径，捕获查询内的并行和序列依存关系，引导大型语言模型（LLMs）进行结构化推理。然后应用其依存关系意识重排序器，利用已解决子问题之间的依存关系结构，为后续子问题丰富和验证检索结果。实证评估表明，PankRAG 在多个基准测试中始终优于当前最先进的方法，证明了其鲁棒性和泛化能力。', 'title_zh': '基于图的RAG增强通过全局查询消歧和依赖感知重排序'}
{'arxiv_id': 'arXiv:2506.11105', 'title': 'Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation', 'authors': 'Uttej Kallakurik, Edward Humes, Rithvik Jonna, Xiaomin Lin, Tinoosh Mohsenin', 'link': 'https://arxiv.org/abs/2506.11105', 'abstract': 'Large Language Models (LLMs) have significant impact on the healthcare scenarios but remain prohibitively large for deployment in real-time, resource-constrained environments such as edge devices. In this work, we introduce a novel medical assistant system, optimized through our general-purpose compression framework, which tailors Large Language Models (LLMs) for deployment in specialized domains. By measuring neuron saliency on domain-specific data, our method can aggressively prune irrelevant neurons, reducing model size while preserving performance. Following pruning, we apply post-training quantization to further reduce the memory footprint, and evaluate the compressed model across medical benchmarks including MedMCQA, MedQA, and PubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed LLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak), achieving real-time, energy-efficient inference under hardware constraints.', 'abstract_zh': '大型语言模型（LLMs）在医疗场景中产生了显著影响，但由于资源限制，难以在边缘设备等实时环境中部署。本文介绍了一种通过通用压缩框架优化的新型医疗助手系统，该系统针对特定领域对大型语言模型进行定制部署。通过在领域特定数据上测量神经元的显著性，我们的方法可以大幅剪枝无关神经元，从而减小模型大小并保持性能。剪枝后，我们应用后训练量化进一步减少模型的记忆占用，并在MedMCQA、MedQA和PubMedQA等医疗基准测试上评估压缩模型。我们还在Jetson Orin Nano（峰值功率18.7W）和Raspberry Pi 5（峰值功率6.3W）上部署了50%压缩的Gemma和67%压缩的LLaMA3模型，实现了在硬件限制下的实时、能源高效推理。', 'title_zh': '基于输入驱动的显著性适配实现边缘设备医疗AI助手'}
{'arxiv_id': 'arXiv:2506.11104', 'title': 'DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration', 'authors': 'Hanzhi Zhang, Heng Fan, Kewei Sha, Yan Huang, Yunhe Feng', 'link': 'https://arxiv.org/abs/2506.11104', 'abstract': 'Long-context understanding is crucial for many NLP applications, yet transformers struggle with efficiency due to the quadratic complexity of self-attention. Sparse attention methods alleviate this cost but often impose static, predefined masks, failing to capture heterogeneous attention patterns. This results in suboptimal token interactions, limiting adaptability and retrieval accuracy in long-sequence tasks. This work introduces a dynamic sparse attention mechanism that assigns adaptive masks at the attention-map level, preserving heterogeneous patterns across layers and heads. Unlike existing approaches, our method eliminates the need for fine-tuning and predefined mask structures while maintaining computational efficiency. By learning context-aware attention structures, it achieves high alignment with full-attention models, ensuring minimal performance degradation while reducing memory and compute overhead. This approach provides a scalable alternative to full attention, enabling the practical deployment of large-scale Large Language Models (LLMs) without sacrificing retrieval performance. DAM is available at: this https URL.', 'abstract_zh': '长上下文理解对于许多NLP应用至关重要，但由于自注意力的二次复杂性，变压器在效率方面存在挑战。稀疏注意力方法可以减轻这种成本，但通常会施加静态的预定义掩码，无法捕捉到异质注意力模式。这会导致子优化的令牌交互，限制了在长序列任务中的适应性和检索准确性。本工作引入了一种动态稀疏注意力机制，在注意力图层面分配自适应掩码，并在各层和各个头部之间保留异质模式。与现有方法不同，我们的方法消除了调优和预定义掩码结构的需要，同时保持了计算效率。通过学习上下文感知的注意力结构，它在与全注意力模型对齐方面取得了高水准，确保在减少内存和计算开销的同时最小化性能退化。该方法提供了一种全注意力的可扩展替代方案，使大规模大型语言模型（LLMs）的实用部署成为可能而不牺牲检索性能。动态稀疏注意力机制（DAM）详见：this https URL。', 'title_zh': 'DAM：长上下文大型语言模型推理加速的动态注意力掩码'}
{'arxiv_id': 'arXiv:2506.11103', 'title': 'You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model', 'authors': 'Wenchong He, Liqian Peng, Zhe Jiang, Alex Go', 'link': 'https://arxiv.org/abs/2506.11103', 'abstract': 'Large language models (LLMs) possess a remarkable ability to perform in-context learning (ICL), which enables them to handle multiple downstream tasks simultaneously without requiring task-specific fine-tuning. Recent studies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma 7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of all tasks at once. However, this approach still lags behind dedicated fine-tuning, where a separate model is trained for each individual task.\nIn this paper, we propose a novel approach, Many-Shot In-Context Fine-tuning (ManyICL), which significantly narrows this performance gap by extending the principles of ICL to a many-shot setting. To unlock the full potential of ManyICL and address the inherent inefficiency of processing long sequences with numerous in-context examples, we propose a novel training objective. Instead of solely predicting the final answer, our approach treats every answer within the context as a supervised training target. This effectively shifts the role of many-shot examples from prompts to targets for autoregressive learning. Through extensive experiments on diverse downstream tasks, including classification, summarization, question answering, natural language inference, and math, we demonstrate that ManyICL substantially outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning. Furthermore, ManyICL significantly mitigates catastrophic forgetting issues observed in zero/few-shot fine-tuning. The code will be made publicly available upon publication.', 'abstract_zh': '大型语言模型（LLMs）具备在上下文中文学习（ICL）的能力，这使它们能够在不需要特定任务微调的情况下同时处理多个下游任务。 recent studies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma 7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of all tasks at once. However, this approach still lags behind dedicated fine-tuning, where a separate model is trained for each individual task.\n\n在本论文中，我们提出了一种新颖的方法，多-shot 在上下文中的微调（ManyICL），这种方法通过将 ICL 原理扩展到多-shot 设置中，显著缩小了这一性能差距。为了发挥 ManyICL 的全部潜力并解决处理大量上下文示例时的固有效率低下的问题，我们提出了一种新的训练目标。我们的方法不仅预测最终答案，还将上下文中的每个答案作为监督训练目标。这实际上将多-shot 示例的角色从提示转换为自回归学习的目标。通过对分类、总结、问答、自然语言推理和数学等多种下游任务进行广泛的实验，我们证明 ManyICL 显著优于零-shot/few-shot 微调，并接近专门微调的性能。此外，ManyICL 显著缓解了零-shot/few-shot 微调中观察到的灾难性遗忘问题。代码将在发表后公开。', 'title_zh': '只需一次微调：多-shot上下文微调大语言模型'}
{'arxiv_id': 'arXiv:2506.11102', 'title': 'Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey', 'authors': 'Jiachen Zhu, Menghui Zhu, Renting Rui, Rong Shan, Congmin Zheng, Bo Chen, Yunjia Xi, Jianghao Lin, Weiwen Liu, Ruiming Tang, Yong Yu, Weinan Zhang', 'link': 'https://arxiv.org/abs/2506.11102', 'abstract': 'The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks. The transition from these traditional LLM chatbots to more advanced AI agents represents a pivotal evolutionary step. However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks. To bridge this gap, this paper introduces a systematic analysis of current evaluation approaches, grounded in an evolutionary perspective. We provide a detailed analytical framework that clearly differentiates AI agents from LLM chatbots along five key aspects: complex environment, multi-source instructor, dynamic feedback, multi-modal perception, and advanced capability. Further, we categorize existing evaluation benchmarks based on external environments driving forces, and resulting advanced internal capabilities. For each category, we delineate relevant evaluation attributes, presented comprehensively in practical reference tables. Finally, we synthesize current trends and outline future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. Our findings offer actionable guidance for researchers, facilitating the informed selection and application of benchmarks in AI agent evaluation, thus fostering continued advancement in this rapidly evolving research domain.', 'abstract_zh': '大型语言模型（LLMs）如GPT、Gemini和DeepSeek的出现显著推进了自然语言处理的发展，引发了多样化的语言相关任务能力强大的聊天机器人。从这些传统的LLM聊天机器人向更先进的AI代理的过渡代表了一次关键的进化步骤。然而，现有的评估框架往往模糊了LLM聊天机器人与AI代理之间的区别，导致研究人员在选择合适的基准时产生混淆。为了解决这一问题，本文从进化视角出发，介绍了一种系统的评估方法分析，提供了一个详细的分析框架，从复杂环境、多源指导者、动态反馈、多模态感知以及高级能力五个关键方面明确区分AI代理和LLM聊天机器人。此外，我们基于外部环境驱动力和产生的高级内部能力对现有的评估基准进行了分类。对于每个类别，我们界定了相关的评估属性，并在实用参考表中进行了全面展示。最后，我们通过环境、代理、评估者和指标这四个关键视角，综合当前趋势并勾勒出未来评估方法。我们的发现为研究人员提供了可操作的指导，有助于他们根据评估任务做出明智的选择和应用，从而推动这一快速发展的研究领域的持续进步。', 'title_zh': '基于进化视角的LLM驱动AI代理评估综述'}
{'arxiv_id': 'arXiv:2506.11098', 'title': 'Debiasing Online Preference Learning via Preference Feature Preservation', 'authors': 'Dongyoung Kim, Jinsung Yoon, Jinwoo Shin, Jaehyung Kim', 'link': 'https://arxiv.org/abs/2506.11098', 'abstract': "Recent preference learning frameworks for large language models (LLMs) simplify human preferences with binary pairwise comparisons and scalar rewards. This simplification could make LLMs' responses biased to mostly preferred features, and would be exacerbated during the iterations of online preference learning steps. To address these challenges, we propose a novel framework coined PFP (Preference Feature Preservation). The key idea of PFP is maintaining the distribution of human preference features and utilizing such rich signals throughout the online preference learning process. Specifically, PFP first extract preference features from offline pairwise human preference data and trains a feature classifier. Then, using trained classifier and the distribution preserving optimization, PFP maps appropriate preference features for a new input instruction during online learning. Lastly, PFP trains LLM using the existing preference learning method, by incorporating the preference feature into system prompts and enabling LLM to explicitly handle various human preferences. Our experiments demonstrate that PFP successfully mitigates the bias in preference features during online learning, and hence achieves superior performance compared to previous preference learning methods on standard benchmarks to evaluate LLM alignment.", 'abstract_zh': '近期面向大规模语言模型的偏好学习框架通过二元成对比较和标量奖励简化了人类偏好，这可能导致语言模型的回答偏向于大多数偏好特征，并且会在在线偏好学习迭代过程中加剧。为解决这些挑战，我们提出了一种新型框架PFP（偏好特征保留）。PFP的核心思想是在在线偏好学习过程中保持人类偏好特征的分布并利用这些丰富的信号。具体而言，PFP首先从离线的成对人类偏好数据中提取偏好特征并训练一个特征分类器，然后利用训练好的分类器和保持分布优化，将适当的偏好特征映射到新的输入指令上。最后，PFP通过将偏好特征整合到系统提示中，训练语言模型以明确处理各种人类偏好。我们的实验表明，PFP成功地在在线学习过程中减轻了偏好特征的偏差，并且在评估语言模型对齐的标准基准上相比之前的偏好学习方法取得了更好的性能。', 'title_zh': '基于偏好特征保存的在线偏好学习去偏差化'}
{'arxiv_id': 'arXiv:2506.11097', 'title': 'C-SEO Bench: Does Conversational SEO Work?', 'authors': 'Haritz Puerto, Martin Gubri, Tommaso Green, Seong Joon Oh, Sangdoo Yun', 'link': 'https://arxiv.org/abs/2506.11097', 'abstract': 'Large Language Models (LLMs) are transforming search engines into Conversational Search Engines (CSE). Consequently, Search Engine Optimization (SEO) is being shifted into Conversational Search Engine Optimization (C-SEO). We are beginning to see dedicated C-SEO methods for modifying web documents to increase their visibility in CSE responses. However, they are often tested only for a limited breadth of application domains; we do not understand whether certain C-SEO methods would be effective for a broad range of domains. Moreover, existing evaluations consider only a single-actor scenario where only one web document adopts a C-SEO method; in reality, multiple players are likely to competitively adopt the cutting-edge C-SEO techniques, drawing an analogy from the dynamics we have seen in SEO. We present C-SEO Bench, the first benchmark designed to evaluate C-SEO methods across multiple tasks, domains, and number of actors. We consider two search tasks, question answering and product recommendation, with three domains each. We also formalize a new evaluation protocol with varying adoption rates among involved actors. Our experiments reveal that most current C-SEO methods are largely ineffective, contrary to reported results in the literature. Instead, traditional SEO strategies, those aiming to improve the ranking of the source in the LLM context, are significantly more effective. We also observe that as we increase the number of C-SEO adopters, the overall gains decrease, depicting a congested and zero-sum nature of the problem. Our code and data are available at this https URL and this https URL.', 'abstract_zh': '大规模语言模型（LLMs）正在将搜索引擎转变成为会话搜索引擎（CSE）。因此，搜索引擎优化（SEO）也随之转向会话搜索引擎优化（C-SEO）。我们开始看到针对修改网页文档以增加其在CSE响应中可见性的专用C-SEO方法。然而，这些方法往往仅限于有限的应用领域进行测试；我们不知道某些C-SEO方法是否适用于广泛的应用领域。此外，现有的评估仅考虑单一方情形，即仅一个网页文档采用C-SEO方法；实际上，多家参与者很可能会竞争性地采用最新的C-SEO技术，这与我们所观察到的SEO动态类似。我们提出了C-SEO Bench，这是第一个旨在跨多个任务、领域和参与者数量评估C-SEO方法的基准。我们考虑了两个搜索任务，即问答和产品推荐，每个领域都有三个领域。我们还制定了一个新的评估协议，其中参与者的采用率有所不同。实验结果表明，大多数当前的C-SEO方法实际上是无效的，这与文献中报道的结果相反。相反，传统SEO策略，在大规模语言模型（LLM）情境下提高源文档排名的方法，更为有效。我们还观察到，随着C-SEO采用者的增加，整体收益减少，这表明该问题具有拥挤和零和的特性。我们的代码和数据可在以下链接访问：this https URL 和 this https URL。', 'title_zh': 'C-SEO 基准测试：对话式SEO有效吗？'}
{'arxiv_id': 'arXiv:2506.11094', 'title': 'The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs', 'authors': 'Songyang Liu, Chaozhuo Li, Jiameng Qiu, Xi Zhang, Feiran Huang, Litian Zhang, Yiming Hei, Philip S. Yu', 'link': 'https://arxiv.org/abs/2506.11094', 'abstract': 'With the rapid advancement of artificial intelligence technology, Large Language Models (LLMs) have demonstrated remarkable potential in the field of Natural Language Processing (NLP), including areas such as content generation, human-computer interaction, machine translation, and code generation, among others. However, their widespread deployment has also raised significant safety concerns. In recent years, LLM-generated content has occasionally exhibited unsafe elements like toxicity and bias, particularly in adversarial scenarios, which has garnered extensive attention from both academia and industry. While numerous efforts have been made to evaluate the safety risks associated with LLMs, there remains a lack of systematic reviews summarizing these research endeavors. This survey aims to provide a comprehensive and systematic overview of recent advancements in LLMs safety evaluation, focusing on several key aspects: (1) "Why evaluate" that explores the background of LLMs safety evaluation, how they differ from general LLMs evaluation, and the significance of such evaluation; (2) "What to evaluate" that examines and categorizes existing safety evaluation tasks based on key capabilities, including dimensions such as toxicity, robustness, ethics, bias and fairness, truthfulness, and so on; (3) "Where to evaluate" that summarizes the evaluation metrics, datasets and benchmarks currently used in safety evaluations; (4) "How to evaluate" that reviews existing evaluation toolkit, and categorizing mainstream evaluation methods based on the roles of the evaluators. Finally, we identify the challenges in LLMs safety evaluation and propose potential research directions to promote further advancement in this field. We emphasize the importance of prioritizing LLMs safety evaluation to ensure the safe deployment of these models in real-world applications.', 'abstract_zh': '随着人工智能技术的迅速发展，大型语言模型（LLMs）在自然语言处理（NLP）领域展现出了显著潜力，包括内容生成、人机交互、机器翻译和代码生成等多个领域。然而，它们的广泛应用也引发了严重的安全关注。近年来，LLMs生成的内容偶尔会显示出毒性或偏见等不安全元素，尤其是在对抗性场景中，这引起了学术界和工业界的广泛关注。虽然已经做出了许多努力来评估LLMs的安全风险，但仍然缺乏对这些研究的系统性总结。本综述旨在提供对LLMs安全评估近期进展的全面和系统的概述，重点关注几个关键方面：（1）“为什么评估”，探索LLMs安全评估的背景、与一般LLMs评估的区别及其重要性；（2）“评估什么”，基于关键能力对现有的安全评估任务进行评估和分类，包括毒性、稳健性、伦理、偏见与公平性、真实性等维度；（3）“在哪里评估”，总结当前在安全评估中使用的评价指标、数据集和基准；（4）“如何评估”，回顾现有的评估工具包，并根据评估者的角色对主流评估方法进行分类。最后，我们指出了LLMs安全评估中的挑战，并提出了促进该领域进一步发展的潜在研究方向。我们强调优先进行LLMs安全评估的重要性，以确保这些模型在实际应用中的安全部署。', 'title_zh': '正义之秤：大规模语言模型安全性评估综述'}
{'arxiv_id': 'arXiv:2506.11092', 'title': 'Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation', 'authors': 'Jubin Abhishek Soni, Amit Anand, Rajesh Kumar Pandey, Aniket Abhishek Soni', 'link': 'https://arxiv.org/abs/2506.11092', 'abstract': 'Retrieval-Augmented Generation (RAG) has significantly advanced large language models (LLMs) by grounding their outputs in external tools and knowledge sources. However, existing RAG systems are typically constrained to static, single-turn interactions with fixed toolsets, making them ill-suited for dynamic domains such as healthcare and smart homes, where user intent, available tools, and contextual factors evolve over time. We present Dynamic Context Tuning (DCT), a lightweight framework that extends RAG to support multi-turn dialogue and evolving tool environments without requiring retraining. DCT integrates an attention-based context cache to track relevant past information, LoRA-based retrieval to dynamically select domain-specific tools, and efficient context compression to maintain inputs within LLM context limits. Experiments on both synthetic and real-world benchmarks show that DCT improves plan accuracy by 14% and reduces hallucinations by 37%, while matching GPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to previously unseen tools, enabling scalable and adaptable AI assistants across a wide range of dynamic environments.', 'abstract_zh': '基于检索的生成（RAG）通过将模型的输出与外部工具和知识源相结合，显著推进了大型语言模型（LLMs）的发展。然而，现有的RAG系统通常局限于静态的单轮交互和固定的工具集，使其不适合如医疗保健和智能家居等动态领域，这些领域中用户意图、可用工具和上下文因素会随时间变化。我们提出了一种轻量级框架动态上下文调整（DCT），该框架能够将RAG扩展到支持多轮对话和变化的工具环境，而无需重新训练。DCT整合了一种基于注意力机制的上下文缓存来追踪相关的历史信息，基于LoRA的检索来动态选择领域特定的工具，并通过有效的上下文压缩来保持输入在LLM上下文限制内。实验结果表明，DCT在计划准确性上提高了14%，在幻觉上减少了37%，并且能够在显著降低成本的同时达到与GPT-4相当的性能。此外，DCT能够适应未见过的工具，使跨多种动态环境的可扩展和适应性强的AI助手成为可能。', 'title_zh': '动态上下文调整以增强检索增强生成：多轮计划与工具适应性增强'}
{'arxiv_id': 'arXiv:2506.11089', 'title': 'Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM', 'authors': 'Jeena Prakash, Blessingh Kumar, Kadri Hacioglu, Bidisha Sharma, Sindhuja Gopalan, Malolan Chetlur, Shankar Venkatesan, Andreas Stolcke', 'link': 'https://arxiv.org/abs/2506.11089', 'abstract': 'Automatic speech recognition (ASR) models rely on high-quality transcribed data for effective training. Generating pseudo-labels for large unlabeled audio datasets often relies on complex pipelines that combine multiple ASR outputs through multi-stage processing, leading to error propagation, information loss and disjoint optimization. We propose a unified multi-ASR prompt-driven framework using postprocessing by either textual or speech-based large language models (LLMs), replacing voting or other arbitration logic for reconciling the ensemble outputs. We perform a comparative study of multiple architectures with and without LLMs, showing significant improvements in transcription accuracy compared to traditional methods. Furthermore, we use the pseudo-labels generated by the various approaches to train semi-supervised ASR models for different datasets, again showing improved performance with textual and speechLLM transcriptions compared to baselines.', 'abstract_zh': '自动化语音识别模型依赖高质量转录数据进行有效训练。大型未标注音频数据集的伪标签生成通常依赖于复杂的多阶段处理管道，结合多个ASR输出，导致错误传播、信息丢失和分离优化。我们提出了一种统一的多ASR提示驱动框架，通过文本或语音大型语言模型（LLMs）进行后处理，替代集合输出的投票或其他仲裁逻辑。我们对具有和不具有LLMs的多种架构进行了比较研究，显示了与传统方法相比显著提高的转录准确性。此外，我们使用各种方法生成的伪标签训练不同数据集的半监督ASR模型，再次显示了与基线相比的性能改进，特别是在使用文本和语音LLM转录时。', 'title_zh': '多ASR融合与错误纠正的更好伪标签生成方法由SpeechLLM实现'}
{'arxiv_id': 'arXiv:2506.11088', 'title': 'Two Birds with One Stone: Improving Factuality and Faithfulness of LLMs via Dynamic Interactive Subspace Editing', 'authors': 'Pengbo Wang, Chaozhuo Li, Chenxu Wang, Liwen Zheng, Litian Zhang, Xi Zhang', 'link': 'https://arxiv.org/abs/2506.11088', 'abstract': 'LLMs have demonstrated unprecedented capabilities in natural language processing, yet their practical deployment remains hindered by persistent factuality and faithfulness hallucinations. While existing methods address these hallucination types independently, they inadvertently induce performance trade-offs, as interventions targeting one type often exacerbate the other. Through empirical and theoretical analysis of activation space dynamics in LLMs, we reveal that these hallucination categories share overlapping subspaces within neural representations, presenting an opportunity for concurrent mitigation. To harness this insight, we propose SPACE, a unified framework that jointly enhances factuality and faithfulness by editing shared activation subspaces. SPACE establishes a geometric foundation for shared subspace existence through dual-task feature modeling, then identifies and edits these subspaces via a hybrid probe strategy combining spectral clustering and attention head saliency scoring. Experimental results across multiple benchmark datasets demonstrate the superiority of our approach.', 'abstract_zh': '通过编辑共享激活子空间同时增强事实性和忠实性：SPACE框架', 'title_zh': '一石二鸟：通过动态交互子空间编辑提升LLMs的事实性和忠实度'}
{'arxiv_id': 'arXiv:2506.11087', 'title': 'ADAMIX: Adaptive Mixed-Precision Delta-Compression with Quantization Error Optimization for Large Language Models', 'authors': 'Boya Xiong, Shuo Wang, Weifeng Ge, Guanhua Chen, Yun Chen', 'link': 'https://arxiv.org/abs/2506.11087', 'abstract': 'Large language models (LLMs) achieve impressive performance on various knowledge-intensive and complex reasoning tasks in different domains. In certain scenarios like multi-tenant serving, a large number of LLMs finetuned from the same base model are deployed to meet complex requirements for users. Recent works explore delta-compression approaches to quantize and compress the delta parameters between the customized LLM and the corresponding base model. However, existing works either exhibit unsatisfactory performance at high compression ratios or depend on empirical bit allocation schemes. In this work, we propose ADAMIX, an effective adaptive mixed-precision delta-compression framework. We provide a mathematical derivation of quantization error to motivate our mixed-precision compression strategy and formulate the optimal mixed-precision bit allocation scheme as the solution to a 0/1 integer linear programming problem. Our derived bit allocation strategy minimizes the quantization error while adhering to a predefined compression ratio requirement. Experimental results on various models and benchmarks demonstrate that our approach surpasses the best baseline by a considerable margin. On tasks like AIME2024 and GQA, where the norm of $\\Delta \\mathbf{W}$ is large and the base model lacks sufficient ability, ADAMIX outperforms the best baseline Delta-CoMe by 22.3% and 6.1% with 7B models, respectively.', 'abstract_zh': '大规模语言模型（LLMs）在不同领域的知识密集型和复杂推理任务中实现了令人印象深刻的表现。在某些场景如多租户服务中，大量从同一基础模型微调得到的LLMs被部署以满足用户复杂的需求。最近的工作探索了增量压缩方法来量化和压缩定制LLM与相应基础模型之间的增量参数。然而，现有方法要么在高压缩比下表现不佳，要么依赖经验的位分配方案。在此工作中，我们提出了ADAMIX，一个有效的自适应混合精度增量压缩框架。我们通过数学推导量化误差来激励我们的混合精度压缩策略，并将最佳混合精度位分配方案形式化为0/1整数线性规划问题的解。我们推导出的位分配策略在满足预定义压缩比要求的同时，最小化量化误差。在各种模型和基准上的实验结果表明，我们的方法在基准方法上显著优越。在AIME2024和GQA等任务中，当$\\Delta \\mathbf{W}$的范数较大且基础模型缺乏足够的能力时，相对于7B模型，ADAMIX分别比最佳基准Delta-CoMe高出22.3%和6.1%。', 'title_zh': 'ADAMIX：自适应混合精度增量压缩与量化误差优化在大规模语言模型中的应用'}
{'arxiv_id': 'arXiv:2506.11086', 'title': 'Intelligibility of Text-to-Speech Systems for Mathematical Expressions', 'authors': 'Sujoy Roychowdhury, H. G. Ranjani, Sumit Soman, Nishtha Paul, Subhadip Bandyopadhyay, Siddhanth Iyengar', 'link': 'https://arxiv.org/abs/2506.11086', 'abstract': 'There has been limited evaluation of advanced Text-to-Speech (TTS) models with Mathematical eXpressions (MX) as inputs. In this work, we design experiments to evaluate quality and intelligibility of five TTS models through listening and transcribing tests for various categories of MX. We use two Large Language Models (LLMs) to generate English pronunciation from LaTeX MX as TTS models cannot process LaTeX directly. We use Mean Opinion Score from user ratings and quantify intelligibility through transcription correctness using three metrics. We also compare listener preference of TTS outputs with respect to human expert rendition of same MX. Results establish that output of TTS models for MX is not necessarily intelligible, the gap in intelligibility varies across TTS models and MX category. For most categories, performance of TTS models is significantly worse than that of expert rendition. The effect of choice of LLM is limited. This establishes the need to improve TTS models for MX.', 'abstract_zh': '关于数学表达式输入的高级文本到语音模型评估有限。本研究设计实验，通过听力测试和转录测试评估五种TTS模型在各种数学表达式类别下的质量和可理解性。我们使用两种大语言模型从LaTeX数学表达式生成英语发音，因为TTS模型不能直接处理LaTeX格式。我们使用用户评分的平均意见分，并通过三个指标量化可理解性。我们还比较了TTS输出与人类专家同样数学表达式的呈现的听者偏好。结果表明，TTS模型生成的数学表达式输出未必具有可理解性，不同TTS模型和数学表达式类别的可理解性差异显著。对于大多数类别，TTS模型的表现远逊于专家呈现。大语言模型的选择影响有限，这表明需要改进针对数学表达式的TTS模型。', 'title_zh': '数学表达式文本到语音系统的可理解性'}
{'arxiv_id': 'arXiv:2506.11082', 'title': 'PRISM: A Transformer-based Language Model of Structured Clinical Event Data', 'authors': 'Lionel Levine, John Santerre, Alex S. Young, T. Barry Levine, Francis Campion, Majid Sarrafzadeh', 'link': 'https://arxiv.org/abs/2506.11082', 'abstract': 'We introduce PRISM (Predictive Reasoning in Sequential Medicine), a transformer-based architecture designed to model the sequential progression of clinical decision-making processes. Unlike traditional approaches that rely on isolated diagnostic classification, PRISM frames clinical trajectories as tokenized sequences of events - including diagnostic tests, laboratory results, and diagnoses - and learns to predict the most probable next steps in the patient diagnostic journey. Leveraging a large custom clinical vocabulary and an autoregressive training objective, PRISM demonstrates the ability to capture complex dependencies across longitudinal patient timelines. Experimental results show substantial improvements over random baselines in next-token prediction tasks, with generated sequences reflecting realistic diagnostic pathways, laboratory result progressions, and clinician ordering behaviors. These findings highlight the feasibility of applying generative language modeling techniques to structured medical event data, enabling applications in clinical decision support, simulation, and education. PRISM establishes a foundation for future advancements in sequence-based healthcare modeling, bridging the gap between machine learning architectures and real-world diagnostic reasoning.', 'abstract_zh': '基于Transformer的PRISM架构：预测医学中的序贯推理', 'title_zh': 'PRISM: 基于Transformer的结构化临床事件数据语言模型'}
{'arxiv_id': 'arXiv:2506.11079', 'title': 'Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts', 'authors': 'Lingyun Gao, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik', 'link': 'https://arxiv.org/abs/2506.11079', 'abstract': 'Automatic reading aloud evaluation can provide valuable support to teachers by enabling more efficient scoring of reading exercises. However, research on reading evaluation systems and applications remains limited. We present a novel multimodal approach that leverages audio and knowledge from text resources. In particular, we explored the potential of using Whisper and instruction-tuned large language models (LLMs) with prompts to improve transcriptions for child speech recognition, as well as their effectiveness in downstream reading mistake detection. Our results demonstrate the effectiveness of prompting Whisper and prompting LLM, compared to the baseline Whisper model without prompting. The best performing system achieved state-of-the-art recognition performance in Dutch child read speech, with a word error rate (WER) of 5.1%, improving the baseline WER of 9.4%. Furthermore, it significantly improved reading mistake detection, increasing the F1 score from 0.39 to 0.73.', 'abstract_zh': '自动朗读评估可以为教师提供有价值的支持，使其能够更高效地评分阅读练习。然而，关于阅读评估系统和应用的研究仍然有限。我们提出了一种新颖的多模态方法，利用音频和文本资源的知识。特别地，我们探索了使用Whisper结合指令调优的大语言模型（LLMs）和提示来提高儿童语音识别的转录效果，并研究了其在下游阅读错误检测中的有效性。我们的结果显示，与未经过提示的基本Whisper模型相比，提示Whisper和提示LLM的有效性更高。性能最佳的系统在荷兰儿童阅读语音识别上取得了最先进的性能，词错误率（WER）为5.1%，相比基线模型的WER 9.4%，有了显著提升。此外，它显著改善了阅读错误检测效果，F1分数从0.39提升到0.73。', 'title_zh': '通过使用提示提高儿童语音识别和阅读错误检测能力'}
{'arxiv_id': 'arXiv:2506.11060', 'title': 'Code Researcher: Deep Research Agent for Large Systems Code and Commit History', 'authors': 'Ramneet Singh, Sathvik Joel, Abhav Mehrotra, Nalin Wadhwa, Ramakrishna B Bairi, Aditya Kanade, Nagarajan Natarajan', 'link': 'https://arxiv.org/abs/2506.11060', 'abstract': "Large Language Model (LLM)-based coding agents have shown promising results on coding benchmarks, but their effectiveness on systems code remains underexplored. Due to the size and complexities of systems code, making changes to a systems codebase is a daunting task, even for humans. It requires researching about many pieces of context, derived from the large codebase and its massive commit history, before making changes. Inspired by the recent progress on deep research agents, we design the first deep research agent for code, called Code Researcher, and apply it to the problem of generating patches for mitigating crashes reported in systems code. Code Researcher performs multi-step reasoning about semantics, patterns, and commit history of code to gather sufficient context. The context is stored in a structured memory which is used for synthesizing a patch. We evaluate Code Researcher on kBenchSyz, a benchmark of Linux kernel crashes, and show that it significantly outperforms strong baselines, achieving a crash-resolution rate of 58%, compared to 37.5% by SWE-agent. On an average, Code Researcher explores 10 files in each trajectory whereas SWE-agent explores only 1.33 files, highlighting Code Researcher's ability to deeply explore the codebase. Through another experiment on an open-source multimedia software, we show the generalizability of Code Researcher. Our experiments highlight the importance of global context gathering and multi-faceted reasoning for large codebases.", 'abstract_zh': '基于大型语言模型的编码代理在编码基准测试中显示出有前景的结果，但它们在系统代码上的有效性仍待探索。受最近深度研究代理进展的启发，我们设计了第一个代码领域的深度研究代理——Code Researcher，并将其应用于生成修补程序以缓解系统代码中报告的崩溃问题。Code Researcher 对代码的语义、模式及其提交历史进行多步推理，以收集足够的上下文。这些上下文被存储在一个结构化的内存中，用于合成修补程序。我们通过一个基于 Linux 内核崩溃的基准 kBenchSyz 评估了 Code Researcher，并表明它显著优于强大的基线，实现 58% 的崩溃解决率，而 SWE-agent 仅为 37.5%。平均而言，Code Researcher 在每次轨迹中探索 10 个文件，而 SWE-agent 仅探索 1.33 个文件，突显了 Code Researcher 深入探索代码库的能力。通过另一个开源多媒体软件的实验，我们展示了 Code Researcher 的通用性。我们的实验突显了对于大型代码库而言全局上下文收集和多方面推理的重要性。', 'title_zh': '代码研究员：面向大型系统代码和提交历史的深度研究代理'}
{'arxiv_id': 'arXiv:2506.11057', 'title': 'STRCMP: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization', 'authors': 'Xijun Li, Jiexiang Yang, Jinghao Wang, Bo Peng, Jianguo Yao, Haibing Guan', 'link': 'https://arxiv.org/abs/2506.11057', 'abstract': "Combinatorial optimization (CO) problems, central to operation research and theoretical computer science, present significant computational challenges due to their NP-hard nature. While large language models (LLMs) have emerged as promising tools for CO--either by directly generating solutions or synthesizing solver-specific codes--existing approaches often neglect critical structural priors inherent to CO problems, leading to suboptimality and iterative inefficiency. Inspired by human experts' success in leveraging CO structures for algorithm design, we propose STRCMP, a novel structure-aware LLM-based algorithm discovery framework that systematically integrates structure priors to enhance solution quality and solving efficiency. Our framework combines a graph neural network (GNN) for extracting structural embeddings from CO instances with an LLM conditioned on these embeddings to identify high-performing algorithms in the form of solver-specific codes. This composite architecture ensures syntactic correctness, preserves problem topology, and aligns with natural language objectives, while an evolutionary refinement process iteratively optimizes generated algorithm. Extensive evaluations across Mixed Integer Linear Programming and Boolean Satisfiability problems, using nine benchmark datasets, demonstrate that our proposed STRCMP outperforms five strong neural and LLM-based methods by a large margin, in terms of both solution optimality and computational efficiency. The code and learned model will be publicly available upon the acceptance of the paper.", 'abstract_zh': '组合优化（CO）问题：一种新的结构感知大语言模型算法发现框架', 'title_zh': 'STRCMP：将图形结构先验与语言模型集成用于组合优化'}
{'arxiv_id': 'arXiv:2506.11052', 'title': 'ACCORD: Autoregressive Constraint-satisfying Generation for COmbinatorial Optimization with Routing and Dynamic attention', 'authors': 'Henrik Abgaryan, Tristan Cazenave, Ararat Harutyunyan', 'link': 'https://arxiv.org/abs/2506.11052', 'abstract': 'Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, yet their direct application to NP-hard combinatorial problems (CPs) remains underexplored. In this work, we systematically investigate the reasoning abilities of LLMs on a variety of NP-hard combinatorial optimization tasks and introduce ACCORD: Autoregressive Constraint-satisfying generation for COmbinatorial optimization with Routing and Dynamic attention. ACCORD features a novel dataset representation and model architecture that leverage the autoregressive nature of LLMs to dynamically enforce feasibility constraints, coupled with attention-based routing to activate problem-specific LoRA modules. We also present the ACCORD-90k supervised dataset, covering six NP-hard combinatorial problems: TSP, VRP, Knapsack, FlowShop, JSSP, and BinPacking. Extensive experiments demonstrate that our ACCORD model, built on an 8B-parameter Llama backbone, consistently outperforms standard prompting and input-output methods, even when compared to much larger LLMs, such as gpt-4. Ablation studies further show that our output structure enhances solution feasibility. To the best of our knowledge, this is the first large-scale, end-to-end framework for exploring the applications of LLMs to a broad spectrum of combinatorial optimization problems. The codes are publicly available at this https URL', 'abstract_zh': '大规模语言模型（LLMs）展示了令人印象深刻的推理能力，但它们直接应用于NP难题组合优化问题（NP-hard combinatorial problems, CPs）仍处探索阶段。本文系统研究了LLMs在各种NP难题组合优化任务上的推理能力，并引入了ACCORD：基于自回归约束满足生成的组合优化路由与动态注意力模型。ACCORD 特设一种新颖的数据集表示和模型架构，利用LLMs的自回归特性动态施加可行性约束，结合基于注意力的路由激活特定问题的LoRA模块。我们还介绍了ACCORD-90K监督数据集，涵盖了六种NP难题组合优化问题：TSP、VRP、Knapsack、FlowShop、JSSP和BinPacking。广泛的实验证明，基于8B参数Llama骨干的ACCORD模型在多种基准测试中均优于标准提示和输入输出方法，即使与更大的LLM（如gpt-4）相比也是如此。消融研究表明，我们的输出结构增强了解的可行性。据我们所知，这是首个大规模端到端框架，用于探索LLMs在广泛组合优化问题中的应用。相关代码可在以下网址获取。', 'title_zh': 'ACCORD：自回归约束满足生成在路径组合优化中的应用与动态注意力机制'}
{'arxiv_id': 'arXiv:2506.11034', 'title': 'CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models', 'authors': 'Aneesh Komanduri, Karuna Bhaila, Xintao Wu', 'link': 'https://arxiv.org/abs/2506.11034', 'abstract': 'Large language models (LLMs) have shown remarkable ability in various language tasks, especially with their emergent in-context learning capability. Extending LLMs to incorporate visual inputs, large vision-language models (LVLMs) have shown impressive performance in tasks such as recognition and visual question answering (VQA). Despite increasing interest in the utility of LLMs in causal reasoning tasks such as causal discovery and counterfactual reasoning, there has been relatively little work showcasing the abilities of LVLMs on visual causal reasoning tasks. We take this opportunity to formally introduce a comprehensive causal reasoning benchmark for multi-modal in-context learning from LVLMs. Our CausalVLBench encompasses three representative tasks: causal structure inference, intervention target prediction, and counterfactual prediction. We evaluate the ability of state-of-the-art open-source LVLMs on our causal reasoning tasks across three causal representation learning datasets and demonstrate their fundamental strengths and weaknesses. We hope that our benchmark elucidates the drawbacks of existing vision-language models and motivates new directions and paradigms in improving the visual causal reasoning abilities of LVLMs.', 'abstract_zh': '大型语言模型（LLMs）在各种语言任务中展示了出色的 ability，特别是在其上下文学习能力方面。将LLMs扩展以结合视觉输入，大型视觉语言模型（LVLMs）在识别和视觉问答（VQA）等任务中展现了出色的性能。尽管对因果推理任务（如因果发现和反事实推理）中LLMs的应用越来越感兴趣，但在视觉因果推理任务中的LVLMs能力展示仍然相对较少。我们借此机会正式介绍了一个全面的因果推理基准，用于评估LVLMs的多模态上下文学习能力。我们的CausalVLBench包含三个代表性任务：因果结构推理、干预目标预测和反事实预测。我们评估了最先进的开源LVLMs在我们的因果推理任务上的表现，并在三个因果表示学习数据集中展示了它们的基本优势和弱点。我们希望这个基准能够揭示现有视觉语言模型的不足，并激励改进LVLMs视觉因果推理能力的新方向和范式。', 'title_zh': 'CausalVLBench: 大规模视觉语言模型中的视觉因果推理评估基准'}
{'arxiv_id': 'arXiv:2506.11027', 'title': 'From Reasoning to Code: GRPO Optimization for Underrepresented Languages', 'authors': 'Federico Pennino, Bianca Raimondi, Massimo Rondelli, Andrea Gurioli, Maurizio Gabbrielli', 'link': 'https://arxiv.org/abs/2506.11027', 'abstract': 'Generating accurate and executable code using large language models (LLMs) is challenging for languages with limited public training data compared to popular languages such as Python. This paper introduces a generalizable approach that uses small-scale code versions of the Qwen 2.5 model combined with Group Relative Policy Optimization (GRPO) to enable effective code generation through explicit reasoning steps, which is particularly beneficial for languages with smaller source code databases. Using Prolog as a representative use case -- given its limited online presence -- the initial model faced challenges in generating executable code. After some training steps, the model successfully produces logically consistent and syntactically accurate code by directly integrating reasoning-driven feedback into the reinforcement learning loop. Experimental evaluations using mathematical logic problem benchmarks illustrate significant improvements in reasoning quality, code accuracy, and logical correctness, underscoring the potential of this approach to benefit a wide range of programming languages lacking extensive training resources.', 'abstract_zh': '使用大型语言模型生成准确可执行代码：面向具有有限公共训练数据的语言', 'title_zh': '从推理到代码：未充分代表语言的GRPO优化'}
{'arxiv_id': 'arXiv:2506.11022', 'title': 'Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox', 'authors': 'Shivani Shukla, Himanshu Joshi, Romilla Syed', 'link': 'https://arxiv.org/abs/2506.11022', 'abstract': 'The rapid adoption of Large Language Models(LLMs) for code generation has transformed software development, yet little attention has been given to how security vulnerabilities evolve through iterative LLM feedback. This paper analyzes security degradation in AI-generated code through a controlled experiment with 400 code samples across 40 rounds of "improvements" using four distinct prompting strategies. Our findings show a 37.6% increase in critical vulnerabilities after just five iterations, with distinct vulnerability patterns emerging across different prompting approaches. This evidence challenges the assumption that iterative LLM refinement improves code security and highlights the essential role of human expertise in the loop. We propose practical guidelines for developers to mitigate these risks, emphasizing the need for robust human validation between LLM iterations to prevent the paradoxical introduction of new security issues during supposedly beneficial code "improvements".', 'abstract_zh': '大型语言模型（LLMs）在代码生成中的快速 adoption 已经变革了软件开发，但鲜少有人关注迭代 LLM 反馈如何导致安全漏洞的演变。本文通过一项控制实验，分析了 400 个代码样本在四组不同提示策略下“改进”40 轮后的人工智能生成代码中的安全退化问题。研究发现，在仅仅五轮迭代后，关键漏洞增加了 37.6%，并且不同提示方法出现了独特的漏洞模式。这些发现挑战了迭代 LLM 精炼能够改善代码安全性的假设，并强调了在 LLM 迭代过程中人类专业知识的重要性。我们提出了开发人员可以遵循的实际指南，强调在 LLM 迭代之间进行稳健的人工验证的必要性，以防止在所谓的“有益代码改进”过程中引入新的安全问题。', 'title_zh': '迭代AI代码生成中的安全性退化——悖论的系统分析'}
{'arxiv_id': 'arXiv:2506.11021', 'title': 'Eliminating Hallucination-Induced Errors in LLM Code Generation with Functional Clustering', 'authors': 'Chaitanya Ravuri, Saman Amarasinghe', 'link': 'https://arxiv.org/abs/2506.11021', 'abstract': 'Modern code-generation LLMs can already solve a large fraction of programming problems, yet they still hallucinate subtle bugs that make their outputs unsafe for autonomous deployment. We present functional clustering, a black-box wrapper that eliminates nearly all hallucination-induced errors while providing a tunable confidence score. The wrapper samples many candidate programs, executes each on a self-generated test suite, and clusters candidates whose I/O behavior is identical; the empirical mass of the largest cluster serves as an exact confidence estimate. A single scalar threshold on this estimate lets users trade coverage for reliability with exponential guarantees. On LiveCodeBench our verifier preserves baseline pass@1 on solvable tasks yet slashes the error rate of returned answers from ~65% to 2%, and drives it to 0% at a conservative threshold while still answering 15.6% of prompts. Manual audits show that the few residual mistakes stem from prompt misinterpretation, not random generation noise, narrowing future work to specification clarity. Because the method requires only sampling and sandbox execution, it applies unchanged to closed-source APIs and future models, offering a practical path toward dependable, autonomous code generation. Our code is available on Github (this https URL).', 'abstract_zh': '现代代码生成大模型已经可以解决大部分编程问题，但仍会生成细微的错误导致输出不适用于自主部署。我们提出了一种功能聚类方法，这是一种黑盒包装器，几乎可以消除所有由幻觉引起的错误，并提供可调节的信心分数。该包装器采样多个候选程序，在自动生成的测试集上执行每个程序，并聚类具有相同I/O行为的候选程序；聚类中最大的质量作为精确的信心估计。该估计值上的单一标量阈值让用户可以在覆盖范围和可靠性之间进行指数级保证的权衡。在LiveCodeBench上，我们的验证器在可解决的任务上保持了基线的通过率，并将返回答案的错误率从约65%降低到2%，在保守的阈值下错误率达到0%，但仍能回答15.6%的请求。手动审核表明，剩余的少数错误来自指令误解，而非随机生成噪声，将未来工作聚焦于规范清晰度。由于该方法只需采样和沙箱执行，因此它可以原封不动地应用于闭源API和未来模型，提供一条走向可靠、自主代码生成的实际路径。我们的代码可在Github上获取（this https URL）。', 'title_zh': '使用功能性聚类消除LLM代码生成中的幻觉诱导错误'}
{'arxiv_id': 'arXiv:2506.11020', 'title': 'Extracting Knowledge Graphs from User Stories using LangChain', 'authors': 'Thayná Camargo da Silva', 'link': 'https://arxiv.org/abs/2506.11020', 'abstract': 'This thesis introduces a novel methodology for the automated generation of knowledge graphs from user stories by leveraging the advanced capabilities of Large Language Models. Utilizing the LangChain framework as a basis, the User Story Graph Transformer module was developed to extract nodes and relationships from user stories using an LLM to construct accurate knowledge this http URL innovative technique was implemented in a script to fully automate the knowledge graph extraction process. Additionally, the evaluation was automated through a dedicated evaluation script, utilizing an annotated dataset for assessment. By enhancing the visualization and understanding of user requirements and domain concepts, this method fosters better alignment between software functionalities and user expectations, ultimately contributing to more effective and user-centric software development processes.', 'abstract_zh': '本论文介绍了一种利用大型语言模型高级能力自动生成知识图谱的新方法，通过利用LangChain框架开发了用户故事图变换器模块，使用LLM从用户故事中抽取节点和关系以构建准确的知识图谱。该创新技术被实现为一个脚本，以完全自动化知识图谱的提取过程。此外，通过专用的评估脚本对标注数据集进行评估，实现了评估的自动化。通过增强用户需求和领域概念的可视化和理解，该方法促进了软件功能与用户期望之间的更好对齐，最终促进了更有效和以用户为中心的软件开发过程。', 'title_zh': '从用户故事中提取知识图谱：基于LangChain的方法'}
{'arxiv_id': 'arXiv:2506.11017', 'title': 'TeleEval-OS: Performance evaluations of large language models for operations scheduling', 'authors': 'Yanyan Wang, Yingying Wang, Junli Liang, Yin Xu, Yunlong Liu, Yiming Xu, Zhengwang Jiang, Zhehe Li, Fei Li, Long Zhao, Kuang Xu, Qi Song, Xiangyang Li', 'link': 'https://arxiv.org/abs/2506.11017', 'abstract': "The rapid advancement of large language models (LLMs) has significantly propelled progress in artificial intelligence, demonstrating substantial application potential across multiple specialized domains. Telecommunications operation scheduling (OS) is a critical aspect of the telecommunications industry, involving the coordinated management of networks, services, risks, and human resources to optimize production scheduling and ensure unified service control. However, the inherent complexity and domain-specific nature of OS tasks, coupled with the absence of comprehensive evaluation benchmarks, have hindered thorough exploration of LLMs' application potential in this critical field. To address this research gap, we propose the first Telecommunications Operation Scheduling Evaluation Benchmark (TeleEval-OS). Specifically, this benchmark comprises 15 datasets across 13 subtasks, comprehensively simulating four key operational stages: intelligent ticket creation, intelligent ticket handling, intelligent ticket closure, and intelligent evaluation. To systematically assess the performance of LLMs on tasks of varying complexity, we categorize their capabilities in telecommunications operation scheduling into four hierarchical levels, arranged in ascending order of difficulty: basic NLP, knowledge Q&A, report generation, and report analysis. On TeleEval-OS, we leverage zero-shot and few-shot evaluation methods to comprehensively assess 10 open-source LLMs (e.g., DeepSeek-V3) and 4 closed-source LLMs (e.g., GPT-4o) across diverse scenarios. Experimental results demonstrate that open-source LLMs can outperform closed-source LLMs in specific scenarios, highlighting their significant potential and value in the field of telecommunications operation scheduling.", 'abstract_zh': '大型语言模型的快速进步显著推动了人工智能的发展，展现出在多个专门领域中的广泛应用潜力。电信运营调度（OS）是电信行业中关键的方面，涉及网络、服务、风险和人力资源的协调管理，以优化生产调度并确保统一的服务控制。然而，OS任务的固有复杂性和领域特定性，以及缺乏全面的评价基准，阻碍了对其应用潜力的深入探索。为解决这一研究缺口，我们提出了首个电信运营调度评价基准（TeleEval-OS）。该基准包括13个子任务的15个数据集，全面模拟了四个关键运营阶段：智能工单创建、智能工单处理、智能工单关闭和智能评估。为系统评估不同复杂度任务上大型语言模型的表现，我们按从简单到复杂的顺序将它们在电信运营调度中的能力分为四个层级：基本自然语言处理、知识问答、报告生成和报告分析。在TeleEval-OS上，我们采用零样本和少样本评估方法，全面评估了10个开源大语言模型（例如DeepSeek-V3）和4个闭源大语言模型（例如GPT-4o）在不同场景中的表现。实验结果表明，开源大语言模型在特定场景中可以超越闭源大语言模型，突显了它们在电信运营调度领域的重大潜力和价值。', 'title_zh': 'TeleEval-OS: 大型语言模型在操作调度性能评估'}
{'arxiv_id': 'arXiv:2506.11007', 'title': 'Impact of Comments on LLM Comprehension of Legacy Code', 'authors': 'Rock Sabetto, Emily Escamilla, Devesh Agarwal, Sujay Kandwal, Justin F. Brunelle, Scott Rosen, Nitin Naik, Samruddhi Thaker, Eric O. Scott, Jacob Zimmer, Amit Madan, Arun Sridharan, Doug Wendt, Michael Doyle, Christopher Glasz, Jasper Phillips, William Macke, Colin Diggs, Michael Bartholf, Zachary Robin, Paul Ursino', 'link': 'https://arxiv.org/abs/2506.11007', 'abstract': 'Large language models (LLMs) have been increasingly integrated into software engineering and maintenance tasks due to their high performance with software engineering tasks and robust understanding of modern programming languages. However, the ability of LLMs to comprehend code written with legacy languages remains a research gap challenged by real-world legacy systems lacking or containing inaccurate documentation that may impact LLM comprehension. To assess LLM comprehension of legacy languages, there is a need for objective LLM evaluation. In order to objectively measure LLM comprehension of legacy languages, we need an efficient, quantitative evaluation method. We leverage multiple-choice question answering (MCQA), an emerging LLM evaluation methodology, to evaluate LLM comprehension of legacy code and the impact of comment prevalence and inaccurate comments. In this work, we present preliminary findings on the impact of documentation on LLM comprehension of legacy code and outline strategic objectives for future work.', 'abstract_zh': '大规模语言模型（LLMs）由于其在软件工程任务中的高性能和对现代编程语言的 robust 理解，已被越来越多地集成到软件工程和维护任务中。然而，LLMs 理解使用遗留语言编写的代码的能力仍然是一个研究缺口，这受到真实-world 遗留系统缺乏或包含不准确文档的影响，这些文档可能会影响 LLM 的理解能力。为了评估 LLM 对遗留语言的理解能力，需要客观的 LLM 评估方法。为了客观衡量 LLM 对遗留语言的理解能力，我们需要一种高效且定量的评估方法。我们利用多选题问答（MCQA），一种新兴的 LLM 评估方法，来评估 LLM 对遗留代码的理解及其注释频率和不准确注释的影响。在本文中，我们呈现了关于文档对 LLM 理解遗留代码影响的初步发现，并概述了未来工作的战略目标。', 'title_zh': 'LLM对遗留代码理解的影响研究'}
{'arxiv_id': 'arXiv:2506.11003', 'title': 'EmbedAgent: Benchmarking Large Language Models in Embedded System Development', 'authors': 'Ruiyang Xu, Jialun Cao, Mingyuan Wu, Wenliang Zhong, Yaojie Lu, Ben He, Xianpei Han, Shing-Chi Cheung, Le Sun', 'link': 'https://arxiv.org/abs/2506.11003', 'abstract': 'Large Language Models (LLMs) have shown promise in various tasks, yet few benchmarks assess their capabilities in embedded system this http URL this paper, we introduce EmbedAgent, a paradigm designed to simulate real-world roles in embedded system development, such as Embedded System Programmer, Architect, and Integrator. This paradigm enables LLMs to be tested in tasks that bridge the gap between digital and physical systems, allowing for a more comprehensive assessment of their capabilities. To evaluate LLMs on these tasks, we propose Embedbench, the first comprehensive benchmark for embedded system programming, circuit design, and cross-platform this http URL consists of 126 cases, covering 9 electronic components across 3 hardware platforms. Through extensive experiments on 10 mainstream LLMs, we uncover several key findings. Surprisingly, despite the simplicity of the cases, DeepSeek-R1 achieves only a 55.6% pass@1 rate when provided with schematic information, and 50.0% when tasked with generating the schematics itself. In the cross-platform migration tasks, LLMs show relatively strong performance with MicroPython on the Raspberry Pi Pico (with the top model achieving 73.8% pass@1), but perform poorly on ESP-IDF, where the best model reaches only 29.4% pass@1.Interestingly, we observe that general-purpose chat LLMs like DeepSeek-V3 often fail to utilize relevant pre-trained knowledge in this domain, while reasoning LLMs tend to overthink and overlook efficient knowledge during pretraining. Based on these insights, we propose two strategies: retrieval augmented generation and compiler feedback-to enhance LLM performance. These strategies result in significant improvements, with Deepseek-R1 reaching a 65.1% pass@1 with correct schematics, and 53.1% without. Additionally, the accuracy of the Arduino to ESP32 migration task improves from 21.4% to 27.8%.', 'abstract_zh': '大型语言模型（LLMs）在各种任务中展现了潜力，但鲜有基准评估其在嵌入式系统中的能力。在这篇论文中，我们介绍了EmbedAgent，这是一种旨在模拟嵌入式系统开发中的现实角色（如嵌入式系统程序员、架构师和集成商）的范式。这种范式使LLMs能够在连接数字和物理系统之间搭建桥梁的任务中进行测试，从而提供对其能力的全面评估。为了评估LLMs在这些任务上的表现，我们提出了Embedbench，这是第一个全面的嵌入式系统编程、电路设计和跨平台基准。该基准包含126个案例，涵盖了3个硬件平台上的9种电子元件。通过在10个主流LLMs上进行广泛实验，我们发现了几个关键发现。尽管案例相对简单，但在仅提供原理图信息的情况下，DeepSeek-R1的pass@1率仅为55.6%，而要生成原理图时这一比率则下降至50.0%。在跨平台迁移任务中，LLMs在使用MicroPython的Raspberry Pi Pico上表现出相对较强的性能（最佳模型pass@1率为73.8%），但在ESP-IDF上表现较差，最佳模型的pass@1率仅为29.4%。有趣的是，我们发现通用聊天LLMs如DeepSeek-V3往往未能充分利用该领域的预训练知识，而推理LLMs则倾向于过度思考并在预训练中忽视有效的知识。基于这些洞察，我们提出了两种策略：检索增强生成和编译器反馈，以增强LLMs的表现。这些策略带来了显著的改进，DeepSeek-R1在正确原理图的情况下达到65.1%的pass@1率，未正确原理图的情况下达到53.1%的pass@1率，同时Arduino到ESP32的迁移任务准确性从21.4%提高到27.8%。', 'title_zh': 'EmbedAgent: 在嵌入式系统开发中评估大型语言模型'}
{'arxiv_id': 'arXiv:2506.10999', 'title': 'Automated Validation of COBOL to Java Transformation', 'authors': 'Atul Kumar, Diptikalyan Saha, Toshikai Yasue, Kohichi Ono, Saravanan Krishnan, Sandeep Hans, Fumiko Satoh, Gerald Mitchell, Sachin Kumar', 'link': 'https://arxiv.org/abs/2506.10999', 'abstract': 'Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterpriselevel code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code. We propose a framework and a tool to help validate the equivalence of COBOL and translated Java. The results can also help repair the code if there are some issues and provide feedback to the AI model to improve. We have developed a symbolic-execution-based test generation to automatically generate unit tests for the source COBOL programs which also mocks the external resource calls. We generate equivalent JUnit test cases with equivalent mocking as COBOL and run them to check semantic equivalence between original and translated programs.', 'abstract_zh': '基于大型语言模型的生成AI技术 Recent进展使得从COBOL等遗留语言转换企业级代码到现代语言（如Java或Python）成为可能。尽管基于大型语言模型的自动转换结果令人鼓舞，但生成的代码无法保证准确翻译原始代码。我们提出了一种框架和工具，以帮助验证COBOL和翻译后的Java代码的等效性。这些结果还可以帮助修复代码中的问题，并为AI模型提供反馈以改进。我们开发了一种基于符号执行的测试生成方法，以自动为源COBOL程序生成单元测试，并模拟外部资源调用。我们生成了等效的JUnit测试用例，并进行了等效的模拟，以检查原始程序和翻译后程序之间的语义等效性。', 'title_zh': 'COBOL到Java转换的自动化验证'}
{'arxiv_id': 'arXiv:2506.10998', 'title': 'Towards Automated Formal Verification of Backend Systems with LLMs', 'authors': 'Kangping Xu, Yifan Luo, Yang Yuan, Andrew Chi-Chih Yao', 'link': 'https://arxiv.org/abs/2506.10998', 'abstract': "Software testing plays a critical role in ensuring that systems behave as intended. However, existing automated testing approaches struggle to match the capabilities of human engineers due to key limitations such as test locality, lack of general reliability, and business logic blindness. In this work, we propose a novel framework that leverages functional programming and type systems to translate Scala backend code into formal Lean representations. Our pipeline automatically generates theorems that specify the intended behavior of APIs and database operations, and uses LLM-based provers to verify them. When a theorem is proved, the corresponding logic is guaranteed to be correct and no further testing is needed. If the negation of a theorem is proved instead, it confirms a bug. In cases where neither can be proved, human intervention is required. We evaluate our method on realistic backend systems and find that it can formally verify over 50% of the test requirements, which suggests that half of a testing engineer's workload can be automated. Additionally, with an average cost of only $2.19 per API, LLM-based verification is significantly more cost-effective than manual testing and can be scaled easily through parallel execution. Our results indicate a promising direction for scalable, AI-powered software testing, with the potential to greatly improve engineering productivity as models continue to advance.", 'abstract_zh': '软件测试对于确保系统按预期行为运行至关重要。然而，现有的自动化测试方法由于测试局部性、缺乏通用可靠性以及业务逻辑盲点等关键限制，在能力上难以与人类工程师媲美。在此项工作中，我们提出了一种新颖的框架，该框架利用函数式编程和类型系统将Scala后端代码转换为形式化的Lean表示。我们的流水线自动生成规定API和数据库操作预期行为的定理，并利用基于LLM的证明器验证这些定理。当定理被证明时，相应的逻辑被保证是正确的，无需进一步测试。如果相反的定理被证明，则确认存在漏洞。在既无法证明的情况下，则需要人工干预。我们通过现实世界的后端系统评估了该方法，并发现它可以形式化验证超过50%的测试需求，这表明一半的测试工程师的工作量可以实现自动化。此外，平均每API的成本仅为2.19美元，基于LLM的验证比人工测试成本效益更高，并且可以通过并行执行轻松扩大规模。我们的结果表明了一条有前景的研究方向，即利用AI实现可扩展的软件测试，随着模型的不断进步，有望大幅提升工程生产力。', 'title_zh': '基于大语言模型的后端系统自动化形式化验证Towards Automated Formal Verification of Backend Systems with LLMs'}
{'arxiv_id': 'arXiv:2506.10996', 'title': 'Evaluating LLMs for Visualization Tasks', 'authors': 'Saadiq Rauf Khan, Vinit Chandak, Sougata Mukherjea', 'link': 'https://arxiv.org/abs/2506.10996', 'abstract': 'Information Visualization has been utilized to gain insights from complex data. In recent times, Large Language Models (LLMs) have performed very well in many tasks. In this paper, we showcase the capabilities of different popular LLMs to generate code for visualization based on simple prompts. We also analyze the power of LLMs to understand some common visualizations by answering simple questions. Our study shows that LLMs could generate code for some visualizations as well as answer questions about them. However, LLMs also have several limitations. We believe that our insights can be used to improve both LLMs and Information Visualization systems.', 'abstract_zh': '大型语言模型在信息可视化中的应用：生成代码与理解可视化的能力及限制', 'title_zh': '评估大规模语言模型在可视化任务中的应用'}
{'arxiv_id': 'arXiv:2506.10989', 'title': 'Prompt engineering and framework: implementation to increase code reliability based guideline for LLMs', 'authors': 'Rogelio Cruz, Jonatan Contreras, Francisco Guerrero, Ezequiel Rodriguez, Carlos Valdez, Citlali Carrillo', 'link': 'https://arxiv.org/abs/2506.10989', 'abstract': 'In this paper, we propose a novel prompting approach aimed at enhancing the ability of Large Language Models (LLMs) to generate accurate Python code. Specifically, we introduce a prompt template designed to improve the quality and correctness of generated code snippets, enabling them to pass tests and produce reliable results. Through experiments conducted on two state-of-the-art LLMs using the HumanEval dataset, we demonstrate that our approach outperforms widely studied zero-shot and Chain-of-Thought (CoT) methods in terms of the Pass@k metric. Furthermore, our method achieves these improvements with significantly reduced token usage compared to the CoT approach, making it both effective and resource-efficient, thereby lowering the computational demands and improving the eco-footprint of LLM capabilities. These findings highlight the potential of tailored prompting strategies to optimize code generation performance, paving the way for broader applications in AI-driven programming tasks.', 'abstract_zh': '在本文中，我们提出了一种新颖的提示方法，旨在增强大型语言模型（LLMs）生成准确Python代码的能力。具体而言，我们引入了一种提示模板，旨在提高生成代码片段的质量和正确性，使其能够通过测试并生成可靠的结果。通过在两个最先进的LLM上使用HumanEval数据集进行实验，我们证明了我们的方法在Pass@k指标上优于广泛研究的零样本和思维链（CoT）方法。此外，与CoT方法相比，我们的方法在显著降低令牌使用量的情况下实现了这些改进，使其既有效又资源高效，从而降低了计算需求并提升了LLM能力的生态足迹。这些发现突显了定制提示策略优化代码生成性能的潜力，为AI驱动的编程任务提供了更广泛的应用前景。', 'title_zh': '提示工程与框架：基于指南提升LLM代码可靠性实施方法'}
{'arxiv_id': 'arXiv:2506.10984', 'title': 'Application Modernization with LLMs: Addressing Core Challenges in Reliability, Security, and Quality', 'authors': 'Ahilan Ayyachamy Nadar Ponnusamy', 'link': 'https://arxiv.org/abs/2506.10984', 'abstract': "AI-assisted code generation tools have revolutionized software development, offering unprecedented efficiency and scalability. However, multiple studies have consistently highlighted challenges such as security vulnerabilities, reliability issues, and inconsistencies in the generated code. Addressing these concerns is crucial to unlocking the full potential of this transformative technology. While advancements in foundational and code-specialized language models have made notable progress in mitigating some of these issues, significant gaps remain, particularly in ensuring high-quality, trustworthy outputs.\nThis paper builds upon existing research on leveraging large language models (LLMs) for application modernization. It explores an opinionated approach that emphasizes two core capabilities of LLMs: code reasoning and code generation. The proposed framework integrates these capabilities with human expertise to tackle application modernization challenges effectively. It highlights the indispensable role of human involvement and guidance in ensuring the success of AI-assisted processes.\nTo demonstrate the framework's utility, this paper presents a detailed case study, walking through its application in a real-world scenario. The analysis includes a step-by-step breakdown, assessing alternative approaches where applicable. This work aims to provide actionable insights and a robust foundation for future research in AI-driven application modernization. The reference implementation created for this paper is available on GitHub.", 'abstract_zh': 'AI辅助的代码生成工具已革新软件开发，提供了前所未有的效率和扩展性。然而，多研究一致指出了安全漏洞、可靠性和生成代码中的一致性等问题。解决这些问题是充分利用这种变革性技术潜力的关键。尽管基础和代码专业化的语言模型的进步在缓解这些问题方面取得了显著进展，但仍存在显著差距，尤其是在确保高质量、可信赖的输出方面。\n\n本文在此基础上，探讨了利用大型语言模型（LLMs）进行应用现代化的现有研究。它提出了一个注重大型语言模型核心能力（代码推理和代码生成）的主观方法，并将这些能力与人类专长相结合，以有效应对应用现代化挑战。本文强调了人类参与和指导在确保AI辅助过程成功中的不可或缺作用。\n\n为展示该框架的实用性，本文通过一个详细的案例研究，展示了其在实际场景中的应用过程。分析包括逐步分解，必要时评估替代方法。本工作旨在提供可操作的见解，并为未来AI驱动的应用现代化研究奠定坚实基础。本文为实现该目标所创建的参考实现可在GitHub上获得。', 'title_zh': '使用LLMs进行应用现代化：应对可靠性、安全性和质量的核心挑战'}
