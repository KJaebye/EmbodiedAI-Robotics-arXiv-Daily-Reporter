{'arxiv_id': 'arXiv:2507.13857', 'title': 'Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation', 'authors': 'Max van den Hoven, Kishaan Jeeveswaran, Pieter Piscaer, Thijs Wensveen, Elahe Arani, Bahram Zonooz', 'link': 'https://arxiv.org/abs/2507.13857', 'abstract': "Monocular 3D lane detection is essential for autonomous driving, but challenging due to the inherent lack of explicit spatial information. Multi-modal approaches rely on expensive depth sensors, while methods incorporating fully-supervised depth networks rely on ground-truth depth data that is impractical to collect at scale. Additionally, existing methods assume that camera parameters are available, limiting their applicability in scenarios like crowdsourced high-definition (HD) lane mapping. To address these limitations, we propose Depth3DLane, a novel dual-pathway framework that integrates self-supervised monocular depth estimation to provide explicit structural information, without the need for expensive sensors or additional ground-truth depth data. Leveraging a self-supervised depth network to obtain a point cloud representation of the scene, our bird's-eye view pathway extracts explicit spatial information, while our front view pathway simultaneously extracts rich semantic information. Depth3DLane then uses 3D lane anchors to sample features from both pathways and infer accurate 3D lane geometry. Furthermore, we extend the framework to predict camera parameters on a per-frame basis and introduce a theoretically motivated fitting procedure to enhance stability on a per-segment basis. Extensive experiments demonstrate that Depth3DLane achieves competitive performance on the OpenLane benchmark dataset. Furthermore, experimental results show that using learned parameters instead of ground-truth parameters allows Depth3DLane to be applied in scenarios where camera calibration is infeasible, unlike previous methods.", 'abstract_zh': '单目3D车道检测对于自动驾驶至关重要，但由于固有的缺乏显式空间信息而具有挑战性。多模态方法依赖昂贵的深度传感器，而结合完全监督深度网络的方法则依赖难以大规模收集的真实深度数据。此外，现有方法假设相机参数可用，限制了其在众包高精度（HD）车道映射等场景中的应用。为解决这些问题，我们提出了Depth3DLane，这是一种新颖的双路径框架，整合了自我监督的单目深度估计以提供显式结构信息，而无需昂贵的传感器或额外的真实深度数据。利用自我监督的深度网络获取场景的点云表示，我们从空中视角路径提取显式空间信息，而从前视角路径同时提取丰富的语义信息。Depth3DLane然后使用3D车道锚点从两个路径中采样特征并推断准确的3D车道几何形状。此外，我们将框架扩展到以每帧为基础预测相机参数，并引入了一个理论上可行的拟合程序，以局部增强稳定性。大量实验表明，Depth3DLane在OpenLane基准数据集上的性能具有竞争力。此外，实验结果表明，使用学习得到的参数而不是真实参数，使得Depth3DLane可以在相机校准不可行的场景中应用，而不像先前的方法。', 'title_zh': 'Depth3DLane: 结合自监督单目深度估计的单目3D车道检测'}
{'arxiv_id': 'arXiv:2507.14119', 'title': 'NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining', 'authors': 'Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, Aleksandr Gordeev', 'link': 'https://arxiv.org/abs/2507.14119', 'abstract': 'Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets: original image, instruction, edited image. Yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated edit-quality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines high-fidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by approximately 2.2x, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit: an open dataset of 358k high-quality triplets. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, an open-source fine-tuned Bagel model, which achieves state-of-the-art metrics in our experiments.', 'abstract_zh': '近期生成模型的进展使得能够遵循自然语言指令进行图像编辑，无需额外用户输入。它们的监督训练需要数百万个三元组：原始图像、指令、编辑图像。然而，挖掘像素级准确的例子是困难的。每个编辑必须仅影响提示指定的区域，保持样式一致性，尊重物理可行性，并保持视觉吸引力。缺乏稳健的自动编辑质量度量阻碍了大规模可靠自动化。我们提出了一种自动化的模块化管道，能够跨领域、分辨率、指令复杂度和样式挖掘高质量三元组。该系统基于公开的生成模型且无需人工干预，使用任务调优的Gemini验证器直接评分指令遵循度和美学，移除了任何对分割或语义模型的需求。逆向建模和组合式自举将挖掘集合扩大约2.2倍，支持大规模高质量训练数据。通过自动化最重复的注释步骤，该方法允许在无需人工标记的情况下实现更大规模的训练。为了使这一资源密集型领域的研究更具普惠性，我们发布了NHR-Edit：一个包含358,000个高质量三元组的开放数据集，在最大的跨数据集评估中超越所有公开替代方案。我们还发布了Bagel-NHR-Edit：一个开源微调Bagel模型，实验中实现了最佳指标。', 'title_zh': '无需人类介入：自主高质图像编辑 triplet 矿化'}
{'arxiv_id': 'arXiv:2507.13984', 'title': 'CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models', 'authors': 'Quang-Binh Nguyen, Minh Luu, Quang Nguyen, Anh Tran, Khoi Nguyen', 'link': 'https://arxiv.org/abs/2507.13984', 'abstract': 'Disentangling content and style from a single image, known as content-style decomposition (CSD), enables recontextualization of extracted content and stylization of extracted styles, offering greater creative flexibility in visual synthesis. While recent personalization methods have explored the decomposition of explicit content style, they remain tailored for diffusion models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a promising alternative with a next-scale prediction paradigm, achieving performance comparable to that of diffusion models. In this paper, we explore VAR as a generative framework for CSD, leveraging its scale-wise generation process for improved disentanglement. To this end, we propose CSD-VAR, a novel method that introduces three key innovations: (1) a scale-aware alternating optimization strategy that aligns content and style representation with their respective scales to enhance separation, (2) an SVD-based rectification method to mitigate content leakage into style representations, and (3) an Augmented Key-Value (K-V) memory enhancing content identity preservation. To benchmark this task, we introduce CSD-100, a dataset specifically designed for content-style decomposition, featuring diverse subjects rendered in various artistic styles. Experiments demonstrate that CSD-VAR outperforms prior approaches, achieving superior content preservation and stylization fidelity.', 'abstract_zh': '从单张图像中解缠内容和风格：一种内容-风格分解方法（CSD-VAR）探讨', 'title_zh': 'CSD-VAR：视觉自回归模型中的内容-风格分解'}
{'arxiv_id': 'arXiv:2507.13942', 'title': 'Generalist Forecasting with Frozen Video Models via Latent Diffusion', 'authors': 'Jacob C Walker, Pedro Vélez, Luisa Polania Cabrera, Guangyao Zhou, Rishabh Kabra, Carl Doersch, Maks Ovsjanikov, João Carreira, Shiry Ginosar', 'link': 'https://arxiv.org/abs/2507.13942', 'abstract': "Forecasting what will happen next is a critical skill for general-purpose systems that plan or act in the world at different levels of abstraction. In this paper, we identify a strong correlation between a vision model's perceptual ability and its generalist forecasting performance over short time horizons. This trend holds across a diverse set of pretrained models-including those trained generatively-and across multiple levels of abstraction, from raw pixels to depth, point tracks, and object motion. The result is made possible by a novel generalist forecasting framework that operates on any frozen vision backbone: we train latent diffusion models to forecast future features in the frozen representation space, which are then decoded via lightweight, task-specific readouts. To enable consistent evaluation across tasks, we introduce distributional metrics that compare distributional properties directly in the space of downstream tasks and apply this framework to nine models and four tasks. Our results highlight the value of bridging representation learning and generative modeling for temporally grounded video understanding.", 'abstract_zh': '视觉模型的感知能力与其在短期时间范围内的通用预测性能之间存在密切关联：一个新型通用预测框架的研究', 'title_zh': '通过潜扩散冻结视频模型的一般主义预测'}
{'arxiv_id': 'arXiv:2507.13941', 'title': 'Convergent transformations of visual representation in brains and models', 'authors': 'Pablo Marcos-Manchón, Lluís Fuentemilla', 'link': 'https://arxiv.org/abs/2507.13941', 'abstract': "A fundamental question in cognitive neuroscience is what shapes visual perception: the external world's structure or the brain's internal architecture. Although some perceptual variability can be traced to individual differences, brain responses to naturalistic stimuli evoke similar activity patterns across individuals, suggesting a convergent representational principle. Here, we test if this stimulus-driven convergence follows a common trajectory across people and deep neural networks (DNNs) during its transformation from sensory to high-level internal representations. We introduce a unified framework that traces representational flow by combining inter-subject similarity with alignment to model hierarchies. Applying this framework to three independent fMRI datasets of visual scene perception, we reveal a cortex-wide network, conserved across individuals, organized into two pathways: a medial-ventral stream for scene structure and a lateral-dorsal stream tuned for social and biological content. This functional organization is captured by the hierarchies of vision DNNs but not language models, reinforcing the specificity of the visual-to-semantic transformation. These findings show a convergent computational solution for visual encoding in both human and artificial vision, driven by the structure of the external world.", 'abstract_zh': '认知神经科学中的一个基本问题是视觉感知是由外部世界的结构还是大脑的内部架构所塑造的。尽管一些知觉变异可以归因于个体差异，但大脑对自然刺激的响应在个体之间唤起类似的大脑活动模式，这表明存在一种趋同的表征原则。在这里，我们测试这种由刺激驱动的趋同性在从感觉级到高级内部表征的转化过程中是否遵循一个共同轨迹，跨越不同的人和深层神经网络（DNNs）。我们提出了一种统一的框架，通过结合个体间相似性和模型层级对齐来追踪表征流。将这一框架应用于三个独立的功能磁共振成像（fMRI）数据集的视觉场景感知中，我们揭示了一个跨个体保守的大脑网络，分为两条路径：一条背内侧流专注于场景结构，另一条背外侧流专精于社会和生物内容。这种功能组织在视觉DNNs的层级中得以捕捉，但在语言模型中并未体现，这强化了视觉到语义变换的特异性。这些发现表明，在人类和人工视觉中，外部世界结构驱动了一种趋同的计算解决方案。', 'title_zh': '视觉表示在大脑和模型中的收敛转换'}
{'arxiv_id': 'arXiv:2507.13880', 'title': 'Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision', 'authors': 'Marten Kreis, Benjamin Kiefer', 'link': 'https://arxiv.org/abs/2507.13880', 'abstract': 'This paper presents a novel approach to enhancing marine vision by fusing real-time visual data with chart information. Our system overlays nautical chart data onto live video feeds by accurately matching detected navigational aids, such as buoys, with their corresponding representations in chart data. To achieve robust association, we introduce a transformer-based end-to-end neural network that predicts bounding boxes and confidence scores for buoy queries, enabling the direct matching of image-domain detections with world-space chart markers. The proposed method is compared against baseline approaches, including a ray-casting model that estimates buoy positions via camera projection and a YOLOv7-based network extended with a distance estimation module. Experimental results on a dataset of real-world maritime scenes demonstrate that our approach significantly improves object localization and association accuracy in dynamic and challenging environments.', 'abstract_zh': '本文提出了一种通过融合实时视觉数据与航海图信息来提升海上视觉的新方法。我们的系统通过准确匹配检测到的助航标志（如浮标）与其航海图数据中的对应表示，将航海图数据叠加到实时视频流上。为了实现稳健的关联，我们提出了一种基于变换器的端到端神经网络，该网络预测浮标查询的边界框和置信分数，从而直接将图像域检测与世界空间中的航海图标记进行匹配。所提出的方法与基准方法进行了比较，包括一种光线投射模型，该模型通过相机投影估计浮标位置以及扩展了距离估计模块的YOLOv7网络。实验结果表明，本方法在动态和具有挑战性的环境中显著提高了物体的定位和关联准确性。', 'title_zh': '基于视觉和图表数据的实时融合以增强 maritime 视觉能力'}
{'arxiv_id': 'arXiv:2507.13820', 'title': 'Team of One: Cracking Complex Video QA with Model Synergy', 'authors': 'Jun Xie, Zhaoran Zhao, Xiongjun Guan, Yingjian Zhu, Hongzhu Yi, Xinming Wang, Feng Chen, Zhepeng Wang', 'link': 'https://arxiv.org/abs/2507.13820', 'abstract': 'We propose a novel framework for open-ended video question answering that enhances reasoning depth and robustness in complex real-world scenarios, as benchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models (Video-LMMs) often exhibit limited contextual understanding, weak temporal modeling, and poor generalization to ambiguous or compositional queries. To address these challenges, we introduce a prompting-and-response integration mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs) via structured chains of thought, each tailored to distinct reasoning pathways. An external Large Language Model (LLM) serves as an evaluator and integrator, selecting and fusing the most reliable responses. Extensive experiments demonstrate that our method significantly outperforms existing baselines across all evaluation metrics, showcasing superior generalization and robustness. Our approach offers a lightweight, extensible strategy for advancing multimodal reasoning without requiring model retraining, setting a strong foundation for future Video-LMM development.', 'abstract_zh': '我们提出了一种新的框架，用于开放式视频问答，该框架在CVRR-ES数据集上的实际复杂场景中增强了推理深度和鲁棒性。现有的视频大型多模态模型（Video-LMMs）往往表现出有限的语境理解、弱的时间建模能力和对模糊或组合查询的不良泛化能力。为了解决这些挑战，我们引入了一种提示与响应集成机制，通过结构化的思维链协调多个异构的视频语言模型（VLMs），每种模型针对不同的推理路径进行定制。一个外部的大语言模型（LLM）作为评估器和集成器，选择并融合最可靠的回答。广泛实验表明，我们的方法在所有评估指标上显著优于现有基线，展示了更强的泛化能力和鲁棒性。我们的方法提供了一种轻量级且可扩展的策略，无需重新训练模型即可推进多模态推理，为未来Video-LMM的发展奠定了坚实的基础。', 'title_zh': '单人团队：通过模型协同解决复杂视频QA'}
{'arxiv_id': 'arXiv:2507.13801', 'title': 'One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion', 'authors': 'Haoang Lu, Yuanqi Su, Xiaoning Zhang, Hao Hu', 'link': 'https://arxiv.org/abs/2507.13801', 'abstract': "In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a critical perception task for autonomous driving due to its ability to infer complete 3D scene layouts and semantics from single 2D images. However, in real-world traffic scenarios, a significant portion of the scene remains occluded or outside the camera's field of view -- a fundamental challenge that existing monocular SSC methods fail to address adequately. To overcome these limitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC framework that leverages pseudo-future frame prediction to expand the model's effective perceptual range. Our approach combines poses and depths to establish accurate 3D correspondences, enabling geometrically-consistent fusion of past, present, and predicted future frames in 3D space. Unlike conventional methods that rely on simple feature stacking, our 3D-aware architecture achieves more robust scene completion by explicitly modeling spatial-temporal relationships. Comprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks demonstrate state-of-the-art performance, validating the effectiveness of our approach, highlighting our method's ability to improve occlusion reasoning and 3D scene completion accuracy.", 'abstract_zh': '近年来，视觉3D语义场景完成（SSC）已成为自动驾驶中一项关键的感知任务，由于其能够从单张2D图像推断出完整的3D场景布局和语义。然而，在现实世界的交通场景中，场景中仍有一部分区域被遮挡或位于相机视野之外——这是一个现有单目SSC方法难以解决的基本挑战。为克服这些局限性，我们提出了一种名为Creating the Future SSC（CF-SSC）的新颖的时序SSC框架，该框架利用伪未来帧预测来扩展模型的有效感知范围。我们的方法结合姿势和深度来建立准确的3D对应关系，能够在3D空间中几何一致地融合过去、现在和预测的未来帧。与依赖简单特征堆叠的传统方法不同，我们提出的3D感知架构通过明确建模空-时关系实现了更稳健的场景完成。在SemanticKITTI和SSCBench-KITTI-360基准上的全面实验表明，我们的方法达到了最先进的性能，验证了其有效性，突显了我们方法在改善遮挡推理和3D场景完成准确性方面的优势。', 'title_zh': '一步之遥：创造未来以提升单目语义场景完成'}
{'arxiv_id': 'arXiv:2507.13739', 'title': 'Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning', 'authors': 'Junsu Kim, Yunhoe Ku, Seungryul Baek', 'link': 'https://arxiv.org/abs/2507.13739', 'abstract': "Few-shot class-incremental learning (FSCIL) is challenging due to extremely limited training data; while aiming to reduce catastrophic forgetting and learn new information. We propose Diffusion-FSCIL, a novel approach that employs a text-to-image diffusion model as a frozen backbone. Our conjecture is that FSCIL can be tackled using a large generative model's capabilities benefiting from 1) generation ability via large-scale pre-training; 2) multi-scale representation; 3) representational flexibility through the text encoder. To maximize the representation capability, we propose to extract multiple complementary diffusion features to play roles as latent replay with slight support from feature distillation for preventing generative biases. Our framework realizes efficiency through 1) using a frozen backbone; 2) minimal trainable components; 3) batch processing of multiple feature extractions. Extensive experiments on CUB-200, \\emph{mini}ImageNet, and CIFAR-100 show that Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on previously learned classes and adapting effectively to new ones.", 'abstract_zh': '少量样本类别增量学习（Few-shot Class-incremental Learning, FSCIL）由于训练数据极其有限而具有挑战性；本研究旨在减少灾难性遗忘并学习新信息。我们提出了一种新颖的方法——Diffusion-FSCIL，该方法采用文本到图像的扩散模型作为冻结骨干。我们认为，可以利用大型生成模型的能力来应对FSCIL问题，得益于1）大规模预训练带来的生成能力；2）多尺度表示；3）通过文本编码器实现的表示灵活性。为了最大化表示能力，我们提出从大型生成模型中提取多个互补的扩散特征，作为潜在再现，并通过特征蒸馏轻微支持，以防止生成偏差。我们的框架通过1）使用冻结骨干；2）最小可训练组件；3）批量处理多个特征提取来实现效率。在CUB-200、\\emph{mini}ImageNet和CIFAR-100上的广泛实验表明，Diffusion-FSCIL超过了现有方法，既保持了之前学习类别的性能，又能有效适应新类别。', 'title_zh': '合成图像能否克服遗忘？超越少样本类增量学习中的未探索疑虑'}
{'arxiv_id': 'arXiv:2507.13659', 'title': 'When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework', 'authors': 'Xiao Wang, Qian Zhu, Shujuan Wu, Bo Jiang, Shiliang Zhang, Yaowei Wang, Yonghong Tian, Bin Luo', 'link': 'https://arxiv.org/abs/2507.13659', 'abstract': 'Recent researchers have proposed using event cameras for person re-identification (ReID) due to their promising performance and better balance in terms of privacy protection, event camera-based person ReID has attracted significant attention. Currently, mainstream event-based person ReID algorithms primarily focus on fusing visible light and event stream, as well as preserving privacy. Although significant progress has been made, these methods are typically trained and evaluated on small-scale or simulated event camera datasets, making it difficult to assess their real identification performance and generalization ability. To address the issue of data scarcity, this paper introduces a large-scale RGB-event based person ReID dataset, called EvReID. The dataset contains 118,988 image pairs and covers 1200 pedestrian identities, with data collected across multiple seasons, scenes, and lighting conditions. We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid foundation for future research in terms of both data and benchmarking. Based on our newly constructed dataset, this paper further proposes a pedestrian attribute-guided contrastive learning framework to enhance feature learning for person re-identification, termed TriPro-ReID. This framework not only effectively explores the visual features from both RGB frames and event streams, but also fully utilizes pedestrian attributes as mid-level semantic features. Extensive experiments on the EvReID dataset and MARS datasets fully validated the effectiveness of our proposed RGB-Event person ReID framework. The benchmark dataset and source code will be released on this https URL', 'abstract_zh': '近期的研究提出了使用事件摄像头进行行人重新识别（ReID）的方法，由于其表现出色且在隐私保护方面具有更好的平衡，基于事件摄像头的行人ReID吸引了广泛关注。目前，主流的基于事件的行人ReID算法主要集中在融合可见光和事件流，以及保护隐私方面。尽管已经取得了显著进展，但这些方法通常在小型或模拟事件摄像头数据集上进行训练和评估，使得评估其实用识别性能和泛化能力变得困难。为了解决数据稀缺的问题，本文介绍了一种大规模RGB-事件基于行人ReID的数据集，称为EvReID。该数据集包含118,988对图像配对，并涵盖了1200个行人类别身份，数据采集跨越了多个季节、场景和光照条件。我们还评估了15种最先进的行人ReID算法，为未来研究提供了坚实的数据和基准评估基础。基于我们新构建的数据集，本文进一步提出了一种行人属性引导的对比学习框架，以增强行人ReID的特征学习，称为TriPro-ReID。该框架不仅有效探索了来自RGB帧和事件流的视觉特征，还充分利用了行人类别的中间语义特征。在EvReID数据集和MARS数据集上的大量实验充分验证了我们提出的RGB-事件行人ReID框架的有效性。基准数据集和源代码将发布在该网址。', 'title_zh': '当人员重识别遇到事件相机：一个基准数据集及其属性导向的重识别框架'}
{'arxiv_id': 'arXiv:2507.13604', 'title': 'BreastSegNet: Multi-label Segmentation of Breast MRI', 'authors': 'Qihang Li, Jichen Yang, Yaqian Chen, Yuwen Chen, Hanxue Gu, Lars J. Grimm, Maciej A. Mazurowski', 'link': 'https://arxiv.org/abs/2507.13604', 'abstract': 'Breast MRI provides high-resolution imaging critical for breast cancer screening and preoperative staging. However, existing segmentation methods for breast MRI remain limited in scope, often focusing on only a few anatomical structures, such as fibroglandular tissue or tumors, and do not cover the full range of tissues seen in scans. This narrows their utility for quantitative analysis. In this study, we present BreastSegNet, a multi-label segmentation algorithm for breast MRI that covers nine anatomical labels: fibroglandular tissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and implant. We manually annotated a large set of 1123 MRI slices capturing these structures with detailed review and correction from an expert radiologist. Additionally, we benchmark nine segmentation models, including U-Net, SwinUNet, UNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among them, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across all labels. It performs especially well on heart, liver, muscle, FGT, and bone, with Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All model code and weights are publicly available, and we plan to release the data at a later date.', 'abstract_zh': '乳腺MRI提供高分辨率成像，对于乳腺癌筛查和术前分期至关重要。然而，现有的乳腺MRI分割方法仍然范围有限，通常仅专注于几种子结构，如纤维腺组织或肿瘤，未能涵盖扫描中看到的所有组织类型。这限制了其在定量分析中的应用范围。本研究提出了一种多标签分割算法BreastSegNet，该算法涵盖了九个解剖标签：纤维腺组织（FGT）、血管、肌肉、骨质、病灶、淋巴结、心脏、肝脏和植入物。我们手工标注了一大批1123张MRI切片，并由专家放射学家进行了详细审核和修正。此外，我们还对包括U-Net、SwinUNet、UNet++、SAM、MedSAM和nnU-Net（搭配多种ResNet编码器）在内的九种分割模型进行了基准测试。其中，nnU-Net ResEncM在所有标签上的平均Dice分数最高，达到0.694。它在心脏、肝脏、肌肉、FGT和骨质上的表现尤为出色，Dice分数超过0.73，心脏和肝脏的Dice分数接近0.90。所有模型代码和权重均已公开，我们计划日后发布数据。', 'title_zh': '乳腺分割网络：乳腺MRI的多标签分割'}
{'arxiv_id': 'arXiv:2507.13459', 'title': 'Graph Neural Network Surrogates for Contacting Deformable Bodies with Necessary and Sufficient Contact Detection', 'authors': 'Vijay K. Dubey, Collin E. Haese, Osman Gültekin, David Dalton, Manuel K. Rausch, Jan N. Fuhg', 'link': 'https://arxiv.org/abs/2507.13459', 'abstract': 'Surrogate models for the rapid inference of nonlinear boundary value problems in mechanics are helpful in a broad range of engineering applications. However, effective surrogate modeling of applications involving the contact of deformable bodies, especially in the context of varying geometries, is still an open issue. In particular, existing methods are confined to rigid body contact or, at best, contact between rigid and soft objects with well-defined contact planes. Furthermore, they employ contact or collision detection filters that serve as a rapid test but use only the necessary and not sufficient conditions for detection. In this work, we present a graph neural network architecture that utilizes continuous collision detection and, for the first time, incorporates sufficient conditions designed for contact between soft deformable bodies. We test its performance on two benchmarks, including a problem in soft tissue mechanics of predicting the closed state of a bioprosthetic aortic valve. We find a regularizing effect on adding additional contact terms to the loss function, leading to better generalization of the network. These benefits hold for simple contact at similar planes and element normal angles, and complex contact at differing planes and element normal angles. We also demonstrate that the framework can handle varying reference geometries. However, such benefits come with high computational costs during training, resulting in a trade-off that may not always be favorable. We quantify the training cost and the resulting inference speedups on various hardware architectures. Importantly, our graph neural network implementation results in up to a thousand-fold speedup for our benchmark problems at inference.', 'abstract_zh': '用于机械非线性边界值问题快速推理的代理模型在工程应用中很有帮助。然而，在涉及可变形体接触的应用中，特别是几何形状变化的情况下，有效的代理建模仍然是一个开放问题。特别是在接触软体时，现有方法主要局限于刚体接触，或者最好情况下接触刚体和软体对象，并带有明确的接触平面。此外，它们使用仅作为快速测试的接触或碰撞检测过滤器，但只使用检测的必要条件而不是充分条件。在本工作中，我们提出了一种图神经网络架构，利用连续碰撞检测，并首次结合了用于软可变形体接触的充分条件。我们在两个基准测试中测试了其性能，包括软组织力学中的一个预测生物人工主动脉瓣关闭状态的问题。我们发现，在损失函数中添加额外的接触项具有正则化效果，从而提高了网络的泛化能力。这些好处适用于类似平面和元件法线角度的简单接触，以及不同平面和元件法线角度的复杂接触。我们还展示了该框架可以处理变化的参考几何形状。然而，这些好处会导致训练时的高计算成本，从而产生一种可能并不总是有利的权衡。我们量化了各种硬件架构下的训练成本和推理速度提升。重要的是，我们的图神经网络实现能够在推理中使基准问题的处理速度提高高达一千倍。', 'title_zh': '基于图神经网络的接触检测代理模型：必要充分接触检测'}
{'arxiv_id': 'arXiv:2507.13428', 'title': '"PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models', 'authors': 'Jing Gu, Xian Liu, Yu Zeng, Ashwin Nagarajan, Fangrui Zhu, Daniel Hong, Yue Fan, Qianqi Yan, Kaiwen Zhou, Ming-Yu Liu, Xin Eric Wang', 'link': 'https://arxiv.org/abs/2507.13428', 'abstract': 'Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel ""Anti-Physics"" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles.', 'abstract_zh': '基于物理模拟的视频生成模型综合基准：PhyWorldBench', 'title_zh': 'PhyWorldBench：文本到视频模型中物理真实感的综合评估'}
{'arxiv_id': 'arXiv:2507.13425', 'title': 'CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction', 'authors': 'Sirui Wang, Zhou Guan, Bingxi Zhao, Tongjia Gu', 'link': 'https://arxiv.org/abs/2507.13425', 'abstract': 'Accurate prediction of driving intention is key to enhancing the safety and interactive efficiency of human-machine co-driving systems. It serves as a cornerstone for achieving high-level autonomous driving. However, current approaches remain inadequate for accurately modeling the complex spatio-temporal interdependencies and the unpredictable variability of human driving behavior. To address these challenges, we propose CaSTFormer, a Causal Spatio-Temporal Transformer to explicitly model causal interactions between driver behavior and environmental context for robust intention prediction. Specifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF) mechanism for precise temporal alignment of internal and external feature streams, a Causal Pattern Extraction (CPE) module that systematically eliminates spurious correlations to reveal authentic causal dependencies, and an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these purified representations into coherent spatio-temporal inferences. We evaluate the proposed CaSTFormer on the public Brain4Cars dataset, and it achieves state-of-the-art performance. It effectively captures complex causal spatio-temporal dependencies and enhances both the accuracy and transparency of driving intention prediction.', 'abstract_zh': '准确预测驾驶意图对于提升人机协同驾驶系统的安全性和交互效率至关重要，它是实现高级自动驾驶的基础。然而，当前的方法在准确建模复杂时空依赖性和人类驾驶行为的不可预测变异性方面仍显不足。为应对这些挑战，我们提出了一种因果时空变换器CaSTFormer，以明确建模驾驶员行为与环境上下文之间的因果交互，从而实现稳健的意图预测。具体而言，CaSTFormer引入了一种新颖的互易移位融合（RSF）机制，实现了内部和外部特征流的精确时间对齐；一种因果模式提取（CPE）模块，系统地消除虚假相关，揭示真实的因果依赖关系；以及一种创新的功能合成网络（FSN），能够自适应地将这些净化的表示综合成连贯的时空推理。我们在公开的Brain4Cars数据集上评估了提出的CaSTFormer，并实现了最先进的性能，有效地捕捉了复杂的因果时空依赖关系，提升了驾驶意图预测的准确性和透明度。', 'title_zh': '基于因果时空变换器的驾驶意图预测'}
{'arxiv_id': 'arXiv:2507.13408', 'title': 'A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs', 'authors': 'Hemanth Kumar M, Karthika M, Saianiruth M, Vasanthakumar Venugopal, Anandakumar D, Revathi Ezhumalai, Charulatha K, Kishore Kumar J, Dayana G, Kalyan Sivasailam, Bargava Subramanian', 'link': 'https://arxiv.org/abs/2507.13408', 'abstract': "Background: Shoulder fractures are often underdiagnosed, especially in emergency and high-volume clinical settings. Studies report up to 10% of such fractures may be missed by radiologists. AI-driven tools offer a scalable way to assist early detection and reduce diagnostic delays. We address this gap through a dedicated AI system for shoulder radiographs. Methods: We developed a multi-model deep learning system using 10,000 annotated shoulder X-rays. Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and RF-DETR. To enhance detection, we applied bounding box and classification-level ensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming individual models across all key metrics. It demonstrated strong recall and localization precision, confirming its effectiveness for clinical fracture detection in shoulder X-rays. Conclusion: The results show ensemble-based AI can reliably detect shoulder fractures in radiographs with high clinical relevance. The model's accuracy and deployment readiness position it well for integration into real-time diagnostic workflows. The current model is limited to binary fracture detection, reflecting its design for rapid screening and triage support rather than detailed orthopedic classification.", 'abstract_zh': '背景：肩部骨折往往被误诊，尤其是在急诊和高流量临床环境中。研究表明，放射科医生可能会错过高达10%的此类骨折。基于AI的工具提供了一种可扩展的方法，用于辅助早期发现并减少诊断延迟。我们通过一个专门的AI系统来解决这一问题，用于肩部X光诊断。方法：我们使用10,000张标注的肩部X光片开发了一个多模型深度学习系统。架构包括Faster R-CNN（ResNet50-FPN、ResNeXt）、EfficientDet和RF-DETR。为了提高检测效果，我们应用了边界框和分类层面的集合技术，如Soft-NMS、WBF和NMW融合。结果：NMW集合的方法在所有关键指标上均优于单个模型，实现了95.5%的准确率和0.9610的F1分数，显示其在肩部X光骨折检测中的有效性和高召回率及定位精度。结论：结果表明，基于集合的AI可以可靠地在X光中检测肩部骨折，具有高临床相关性。该模型的高准确性和部署准备度使其适合集成到实时诊断流程中。当前模型仅限于二分类骨折检测，反映了其设计旨在进行快速筛查和支持分诊而非详细的骨科分类。', 'title_zh': '基于深度学习的集成系统在临床X光片中自动检测肩部骨折'}
{'arxiv_id': 'arXiv:2507.13372', 'title': 'Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks', 'authors': 'Yeming Cai, Zhenglin Li, Yang Wang', 'link': 'https://arxiv.org/abs/2507.13372', 'abstract': "Breast cancer is a leading cause of death among women globally, and early detection is critical for improving survival rates. This paper introduces an innovative framework that integrates Vision Transformers (ViT) and Graph Neural Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset. Our framework leverages ViT's ability to capture global image features and GNN's strength in modeling structural relationships, achieving an accuracy of 84.2%, outperforming traditional methods. Additionally, interpretable attention heatmaps provide insights into the model's decision-making process, aiding radiologists in clinical settings.", 'abstract_zh': '乳腺癌是全球女性死亡的主要原因，早期检测对于提高生存率至关重要。本文介绍了一种创新框架，该框架结合了视觉变换器（ViT）和图神经网络（GNN），以利用CBIS-DDSM数据集提高乳腺癌检测效果。该框架融合了ViT捕捉全局图像特征的能力和GNN建模结构关系的优势，实现了84.2%的准确率，优于传统方法。此外，可解释的注意力热点图提供了模型决策过程的见解，有助于放射科医生在临床环境中使用。', 'title_zh': '利用视觉变换器和图神经网络增强乳腺癌检测'}
{'arxiv_id': 'arXiv:2507.13363', 'title': 'Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop', 'authors': 'Atharv Goel, Mehar Khurana', 'link': 'https://arxiv.org/abs/2507.13363', 'abstract': 'Modern 3D object detection datasets are constrained by narrow class taxonomies and costly manual annotations, limiting their ability to scale to open-world settings. In contrast, 2D vision-language models trained on web-scale image-text pairs exhibit rich semantic understanding and support open-vocabulary detection via natural language prompts. In this work, we leverage the maturity and category diversity of 2D foundation models to perform open-vocabulary 3D object detection without any human-annotated 3D labels.\nOur pipeline uses a 2D vision-language detector to generate text-conditioned proposals, which are segmented with SAM and back-projected into 3D using camera geometry and either LiDAR or monocular pseudo-depth. We introduce a geometric inflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D bounding boxes without training. To simulate adverse real-world conditions, we construct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes dataset.\nExperiments demonstrate that our method achieves competitive localization performance across multiple settings, including LiDAR-based and purely RGB-D inputs, all while remaining training-free and open-vocabulary. Our results highlight the untapped potential of 2D foundation models for scalable 3D perception. We open-source our code and resources at this https URL.', 'abstract_zh': '现代3D物体检测数据集受限于狭窄的类别 taxonomy 和昂贵的手动标注成本，限制了其在开放场景中的扩展能力。相比之下，通过网络规模的图像-文本对训练的2D视觉-语言模型展示了丰富的语义理解，并可通过自然语言提示支持开放词汇检测。在这项工作中，我们利用2D基础模型的成熟性和类别多样性，在无需任何人工标注的3D标签的情况下，进行开放词汇的3D物体检测。', 'title_zh': '只需添加几何：无需人工介入的无梯度开放词汇3D检测'}
