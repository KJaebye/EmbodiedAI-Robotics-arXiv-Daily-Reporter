{'arxiv_id': 'arXiv:2503.04641', 'title': 'Simulating the Real World: A Unified Survey of Multimodal Generative Models', 'authors': 'Yuqi Hu, Longguang Wang, Xian Liu, Ling-Hao Chen, Yuwei Guo, Yukai Shi, Ce Liu, Anyi Rao, Zeyu Wang, Hui Xiong', 'link': 'https://arxiv.org/abs/2503.04641', 'abstract': 'Understanding and replicating the real world is a critical challenge in Artificial General Intelligence (AGI) research. To achieve this, many existing approaches, such as world models, aim to capture the fundamental principles governing the physical world, enabling more accurate simulations and meaningful interactions. However, current methods often treat different modalities, including 2D (images), videos, 3D, and 4D representations, as independent domains, overlooking their interdependencies. Additionally, these methods typically focus on isolated dimensions of reality without systematically integrating their connections. In this survey, we present a unified survey for multimodal generative models that investigate the progression of data dimensionality in real-world simulation. Specifically, this survey starts from 2D generation (appearance), then moves to video (appearance+dynamics) and 3D generation (appearance+geometry), and finally culminates in 4D generation that integrate all dimensions. To the best of our knowledge, this is the first attempt to systematically unify the study of 2D, video, 3D and 4D generation within a single framework. To guide future research, we provide a comprehensive review of datasets, evaluation metrics and future directions, and fostering insights for newcomers. This survey serves as a bridge to advance the study of multimodal generative models and real-world simulation within a unified framework.', 'abstract_zh': '理解并重现现实世界是人工通用智能（AGI）研究中的一个关键挑战。为此，许多现有方法，比如世界模型，旨在捕捉支配物理世界的基本原则，从而实现更准确的仿真和更有意义的交互。然而，当前的方法通常将不同的模态，包括2D（图像）、视频、3D和4D表示，视为独立的领域，忽视了它们之间的相互依赖性。此外，这些方法通常专注于现实的孤立维度，而不系统地整合它们之间的连接。在本文综述中，我们提出了一种统一的多模态生成模型综述，探讨现实世界仿真中的数据维度进展。具体而言，本文综述从2D生成（外观）开始，然后过渡到视频（外观+动态）和3D生成（外观+几何），最终达到整合所有维度的4D生成。据我们所知，这是我们首次尝试系统地在单一框架中统一研究2D、视频、3D和4D生成。为了指导未来的研究，我们提供了全面的数据集、评估指标和未来方向的综述，并为新进入者提供洞见。本文综述旨在作为桥梁，促进在统一框架下对多模态生成模型和现实世界仿真的研究。', 'title_zh': '模拟现实世界：多模态生成模型综述'}
{'arxiv_id': 'arXiv:2503.04506', 'title': 'Multi-modal Summarization in Model-Based Engineering: Automotive Software Development Case Study', 'authors': 'Nenad Petrovic, Yurui Zhang, Moaad Maaroufi, Kuo-Yi Chao, Lukasz Mazur, Fengjunjie Pan, Vahid Zolfaghari, Alois Knoll', 'link': 'https://arxiv.org/abs/2503.04506', 'abstract': 'Multimodal summarization integrating information from diverse data modalities presents a promising solution to aid the understanding of information within various processes. However, the application and advantages of multimodal summarization have not received much attention in model-based engineering (MBE), where it has become a cornerstone in the design and development of complex systems, leveraging formal models to improve understanding, validation and automation throughout the engineering lifecycle. UML and EMF diagrams in model-based engineering contain a large amount of multimodal information and intricate relational data. Hence, our study explores the application of multimodal large language models within the domain of model-based engineering to evaluate their capacity for understanding and identifying relationships, features, and functionalities embedded in UML and EMF diagrams. We aim to demonstrate the transformative potential benefits and limitations of multimodal summarization in improving productivity and accuracy in MBE practices. The proposed approach is evaluated within the context of automotive software development, while many promising state-of-art models were taken into account.', 'abstract_zh': '多模态总结在模型基础工程中的应用及其对理解和识别UML和EMF图中嵌入的关系、特性和功能的潜力研究', 'title_zh': '基于模型的工程中的多模态总结：汽车软件开发案例研究'}
{'arxiv_id': 'arXiv:2503.04406', 'title': 'Training-Free Graph Filtering via Multimodal Feature Refinement for Extremely Fast Multimodal Recommendation', 'authors': 'Yu-Seung Roh, Joo-Young Kim, Jin-Duk Park, Won-Yong Shin', 'link': 'https://arxiv.org/abs/2503.04406', 'abstract': 'Multimodal recommender systems improve the performance of canonical recommender systems with no item features by utilizing diverse content types such as text, images, and videos, while alleviating inherent sparsity of user-item interactions and accelerating user engagement. However, current neural network-based models often incur significant computational overhead due to the complex training process required to learn and integrate information from multiple modalities. To overcome this limitation, we propose MultiModal-Graph Filtering (MM-GF), a training-free method based on the notion of graph filtering (GF) for efficient and accurate multimodal recommendations. Specifically, MM-GF first constructs multiple similarity graphs through nontrivial multimodal feature refinement such as robust scaling and vector shifting by addressing the heterogeneous characteristics across modalities. Then, MM-GF optimally fuses multimodal information using linear low-pass filters across different modalities. Extensive experiments on real-world benchmark datasets demonstrate that MM-GF not only improves recommendation accuracy by up to 13.35% compared to the best competitor but also dramatically reduces computational costs by achieving the runtime of less than 10 seconds.', 'abstract_zh': '多模态推荐系统通过利用文本、图像和视频等多样化内容类型，改进了仅有物品特征的 canonical 推荐系统，同时缓解了用户-物品交互的固有稀疏性并加速了用户参与。然而，当前基于神经网络的模型往往由于需要学习和整合多种模态的信息而产生显著的计算开销。为克服这一限制，我们提出了一种基于图过滤（GF）概念的无训练方法——MultiModal-Graph Filtering（MM-GF），以实现高效准确的多模态推荐。具体来说，MM-GF 首先通过处理模态间异构特性来构建多个相似性图，例如鲁棒缩放和矢量平移等非平凡多模态特征 refinement。然后，MM-GF 使用跨不同模态的线性低通滤波器来最优融合多模态信息。在实际基准数据集上的广泛实验表明，MM-GF 不仅将推荐精度提高了高达 13.35%，而且通过实现小于 10 秒的运行时间显著降低了计算成本。', 'title_zh': '无需训练的图过滤方法通过多模态特征精炼实现极端快速的多模态推荐'}
{'arxiv_id': 'arXiv:2503.04167', 'title': 'The Role of Visual Modality in Multimodal Mathematical Reasoning: Challenges and Insights', 'authors': 'Yufang Liu, Yao Du, Tao Ji, Jianing Wang, Yang Liu, Yuanbin Wu, Aimin Zhou, Mengdi Zhang, Xunliang Cai', 'link': 'https://arxiv.org/abs/2503.04167', 'abstract': 'Recent research has increasingly focused on multimodal mathematical reasoning, particularly emphasizing the creation of relevant datasets and benchmarks. Despite this, the role of visual information in reasoning has been underexplored. Our findings show that existing multimodal mathematical models minimally leverage visual information, and model performance remains largely unaffected by changes to or removal of images in the dataset. We attribute this to the dominance of textual information and answer options that inadvertently guide the model to correct answers. To improve evaluation methods, we introduce the HC-M3D dataset, specifically designed to require image reliance for problem-solving and to challenge models with similar, yet distinct, images that change the correct answer. In testing leading models, their failure to detect these subtle visual differences suggests limitations in current visual perception capabilities. Additionally, we observe that the common approach of improving general VQA capabilities by combining various types of image encoders does not contribute to math reasoning performance. This finding also presents a challenge to enhancing visual reliance during math reasoning. Our benchmark and code would be available at \\href{this https URL}{this https URL\\_modality\\_role}.', 'abstract_zh': '最近的研究越来越多地关注多模态数学推理，特别强调相关数据集和基准的创建。然而，视觉信息在推理中的作用仍然被 недо探索。我们的研究结果表明，现有的多模态数学模型对视觉信息的利用 minimal，且 dataset 中图像的变化或移除对模型性能几乎没有影响。我们归因于文本信息和答案选项的主导地位，这些文本信息和选项无意中指导模型得出正确答案。为了改进评估方法，我们介绍了 HC-M3D 数据集，该数据集特别设计要求图像依赖性以解决问题，并用相似但独特的图像挑战模型，这些图像会改变正确答案。在测试顶级模型时，它们未能检测到这些微妙的视觉差异表明当前的视觉感知能力存在局限性。此外，我们还观察到，通过结合各种图像编码器来提高通用视觉问答 (VQA) 能力的做法并不能提升数学推理性能。这一发现也对在数学推理中增强视觉依赖性构成了挑战。我们的基准测试和代码可在 \\href{this https URL}{this https URL\\_modality\\_role} 获取。', 'title_zh': '视觉模态在多模态数学推理中的作用：挑战与见解'}
{'arxiv_id': 'arXiv:2503.04110', 'title': 'InterChat: Enhancing Generative Visual Analytics using Multimodal Interactions', 'authors': 'Juntong Chen, Jiang Wu, Jiajing Guo, Vikram Mohanty, Xueming Li, Jorge Piazentin Ono, Wenbin He, Liu Ren, Dongyu Liu', 'link': 'https://arxiv.org/abs/2503.04110', 'abstract': "The rise of Large Language Models (LLMs) and generative visual analytics systems has transformed data-driven insights, yet significant challenges persist in accurately interpreting users' analytical and interaction intents. While language inputs offer flexibility, they often lack precision, making the expression of complex intents inefficient, error-prone, and time-intensive. To address these limitations, we investigate the design space of multimodal interactions for generative visual analytics through a literature review and pilot brainstorming sessions. Building on these insights, we introduce a highly extensible workflow that integrates multiple LLM agents for intent inference and visualization generation. We develop InterChat, a generative visual analytics system that combines direct manipulation of visual elements with natural language inputs. This integration enables precise intent communication and supports progressive, visually driven exploratory data analyses. By employing effective prompt engineering, and contextual interaction linking, alongside intuitive visualization and interaction designs, InterChat bridges the gap between user interactions and LLM-driven visualizations, enhancing both interpretability and usability. Extensive evaluations, including two usage scenarios, a user study, and expert feedback, demonstrate the effectiveness of InterChat. Results show significant improvements in the accuracy and efficiency of handling complex visual analytics tasks, highlighting the potential of multimodal interactions to redefine user engagement and analytical depth in generative visual analytics.", 'abstract_zh': '大型语言模型（LLMs）和生成式可视化分析系统的兴起已经转变了数据驱动的洞察，但准确解读用户分析和交互意图仍面临重大挑战。虽然语言输入提供了灵活性，但往往缺乏精确性，使得复杂意图的表达inefficient、error-prone且时间消耗。为克服这些限制，我们通过文献综述和初步头脑风暴探讨了生成式可视化分析中的多模态交互设计空间。基于这些见解，我们引入了一种高度可扩展的工作流程，该流程整合了多个LLM代理以进行意图推断和可视化生成。我们开发了InterChat，一个结合直接操作可视化元素与自然语言输入的生成式可视化分析系统。这种整合允许精确的意图通信，并支持逐步、以视觉为导向的数据探索分析。通过采用有效的提示工程、上下文交互链接，以及直观的可视化和交互设计，InterChat弥合了用户交互与LLM驱动的可视化之间的差距，提升了解释性和易用性。广泛的评估，包括两种使用场景、用户研究和专家反馈，证明了InterChat的有效性。结果显示，在处理复杂可视化分析任务方面的准确性和效率显著提高，突显了多模态交互在生成式可视化分析中重新定义用户参与度和分析深度的潜力。', 'title_zh': 'InterChat: 利用多模态交互增强生成性视觉分析'}
{'arxiv_id': 'arXiv:2503.04065', 'title': 'PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks', 'authors': 'Feng Ni, Kui Huang, Yao Lu, Wenyu Lv, Guanzhong Wang, Zeyu Chen, Yi Liu', 'link': 'https://arxiv.org/abs/2503.04065', 'abstract': 'With the rapid advancement of digitalization, various document images are being applied more extensively in production and daily life, and there is an increasingly urgent need for fast and accurate parsing of the content in document images. Therefore, this report presents PP-DocBee, a novel multimodal large language model designed for end-to-end document image understanding. First, we develop a data synthesis strategy tailored to document scenarios in which we build a diverse dataset to improve the model generalization. Then, we apply a few training techniques, including dynamic proportional sampling, data preprocessing, and OCR postprocessing strategies. Extensive evaluations demonstrate the superior performance of PP-DocBee, achieving state-of-the-art results on English document understanding benchmarks and even outperforming existing open source and commercial models in Chinese document understanding. The source code and pre-trained models are publicly available at \\href{this https URL}{this https URL}.', 'abstract_zh': '随着数字化的快速发展，各种文档图像在生产和日常生活中的应用越来越广泛，对文档图像内容的快速准确解析需求日益迫切。因此，本报告提出了一种专为端到端文档图像理解设计的新型多模态大语言模型PP-DocBee。首先，我们开发了一种针对文档场景的数据合成策略，构建了多样化的数据集以提高模型的泛化能力。然后，我们应用了包括动态比例采样、数据预处理和OCR后处理策略在内的多种训练技术。广泛的评估结果表明，PP-DocBee在英语文档理解基准测试中取得了最先进的性能，并在中文文档理解中甚至优于现有的开源和商用模型。源代码和预训练模型已公开可供访问。', 'title_zh': 'PP-DocBee: 通过多种技巧提升多模态文档理解'}
{'arxiv_id': 'arXiv:2503.03792', 'title': 'Rebalanced Multimodal Learning with Data-aware Unimodal Sampling', 'authors': 'Qingyuan Jiang, Zhouyang Chi, Xiao Ma, Qirong Mao, Yang Yang, Jinhui Tang', 'link': 'https://arxiv.org/abs/2503.03792', 'abstract': 'To address the modality learning degeneration caused by modality imbalance, existing multimodal learning~(MML) approaches primarily attempt to balance the optimization process of each modality from the perspective of model learning. However, almost all existing methods ignore the modality imbalance caused by unimodal data sampling, i.e., equal unimodal data sampling often results in discrepancies in informational content, leading to modality imbalance. Therefore, in this paper, we propose a novel MML approach called \\underline{D}ata-aware \\underline{U}nimodal \\underline{S}ampling~(\\method), which aims to dynamically alleviate the modality imbalance caused by sampling. Specifically, we first propose a novel cumulative modality discrepancy to monitor the multimodal learning process. Based on the learning status, we propose a heuristic and a reinforcement learning~(RL)-based data-aware unimodal sampling approaches to adaptively determine the quantity of sampled data at each iteration, thus alleviating the modality imbalance from the perspective of sampling. Meanwhile, our method can be seamlessly incorporated into almost all existing multimodal learning approaches as a plugin. Experiments demonstrate that \\method~can achieve the best performance by comparing with diverse state-of-the-art~(SOTA) baselines.', 'abstract_zh': '数据感知单模态采样以缓解模态不平衡的多模态学习方法', 'title_zh': '数据意识单模抽样驱动的重新平衡多模态学习'}
