{'arxiv_id': 'arXiv:2504.00969', 'title': 'HDVIO2.0: Wind and Disturbance Estimation with Hybrid Dynamics VIO', 'authors': 'Giovanni Cioffi, Leonard Bauersfeld, Davide Scaramuzza', 'link': 'https://arxiv.org/abs/2504.00969', 'abstract': 'Visual-inertial odometry (VIO) is widely used for state estimation in autonomous micro aerial vehicles using onboard sensors. Current methods improve VIO by incorporating a model of the translational vehicle dynamics, yet their performance degrades when faced with low-accuracy vehicle models or continuous external disturbances, like wind. Additionally, incorporating rotational dynamics in these models is computationally intractable when they are deployed in online applications, e.g., in a closed-loop control system. We present HDVIO2.0, which models full 6-DoF, translational and rotational, vehicle dynamics and tightly incorporates them into a VIO with minimal impact on the runtime. HDVIO2.0 builds upon the previous work, HDVIO, and addresses these challenges through a hybrid dynamics model combining a point-mass vehicle model with a learning-based component, with access to control commands and IMU history, to capture complex aerodynamic effects. The key idea behind modeling the rotational dynamics is to represent them with continuous-time functions. HDVIO2.0 leverages the divergence between the actual motion and the predicted motion from the hybrid dynamics model to estimate external forces as well as the robot state. Our system surpasses the performance of state-of-the-art methods in experiments using public and new drone dynamics datasets, as well as real-world flights in winds up to 25 km/h. Unlike existing approaches, we also show that accurate vehicle dynamics predictions are achievable without precise knowledge of the full vehicle state.', 'abstract_zh': '基于视觉-惯性里程计学（VIO）的全6自由度车辆动力学建模与实时应用（HDVIO2.0）', 'title_zh': 'HDVIO2.0: 结合混合动力学VIO的风和干扰估计'}
{'arxiv_id': 'arXiv:2504.00697', 'title': 'Auditory Localization and Assessment of Consequential Robot Sounds: A Multi-Method Study in Virtual Reality', 'authors': 'Marlene Wessels, Jorge de Heuvel, Leon Müller, Anna Luisa Maier, Maren Bennewitz, Johannes Kraus', 'link': 'https://arxiv.org/abs/2504.00697', 'abstract': "Mobile robots increasingly operate alongside humans but are often out of sight, so that humans need to rely on the sounds of the robots to recognize their presence. For successful human-robot interaction (HRI), it is therefore crucial to understand how humans perceive robots by their consequential sounds, i.e., operating noise. Prior research suggests that the sound of a quadruped Go1 is more detectable than that of a wheeled Turtlebot. This study builds on this and examines the human ability to localize consequential sounds of three robots (quadruped Go1, wheeled Turtlebot 2i, wheeled HSR) in Virtual Reality. In a within-subjects design, we assessed participants' localization performance for the robots with and without an acoustic vehicle alerting system (AVAS) for two velocities (0.3, 0.8 m/s) and two trajectories (head-on, radial). In each trial, participants were presented with the sound of a moving robot for 3~s and were tasked to point at its final position (localization task). Localization errors were measured as the absolute angular difference between the participants' estimated and the actual robot position. Results showed that the robot type significantly influenced the localization accuracy and precision, with the sound of the wheeled HSR (especially without AVAS) performing worst under all experimental conditions. Surprisingly, participants rated the HSR sound as more positive, less annoying, and more trustworthy than the Turtlebot and Go1 sound. This reveals a tension between subjective evaluation and objective auditory localization performance. Our findings highlight consequential robot sounds as a critical factor for designing intuitive and effective HRI, with implications for human-centered robot design and social navigation.", 'abstract_zh': '移动机器人越来越多地与人类协同工作，但常常处于人类视线之外，因此人类需要依赖机器人的声音来感知其存在。为了成功实现人机交互（HRI），理解人类通过伴随的声音（如运行噪声）感知机器人的方式至关重要。先前的研究表明，四足机器人Go1的声音比轮式机器人Turtlebot的声音更易被察觉。本研究在此基础上，探讨了人类在虚拟现实（Virtual Reality）中定位三种机器人（四足机器人Go1、轮式机器人Turtlebot 2i、轮式机器人HSR）伴随声音的能力。在单被试设计中，我们评估了参与者在有无声觉车辆警告系统（AVAS）的情况下，对于两种速度（0.3 m/s，0.8 m/s）和两种轨迹（正面、径向）的机器人定位表现。在每次试验中，参与者听到移动机器人的声音3~5秒，并被要求指出其最终位置（定位任务）。定位误差通过参与者估计位置与实际机器人位置的绝对角差来测量。结果表明，机器人类型显著影响定位准确性和精确度，尤其在没有AVAS的情况下，轮式机器人HSR的声音表现最差。有趣的是，参与者更倾向于评价HSR的声音更为积极、不那么烦人且更值得信赖，这揭示了主观评估与客观听觉定位性能之间的矛盾。本研究强调伴随声音对于设计直观有效的HRI的重要性，对于以人类为中心的机器人设计和社会导航具有重要意义。', 'title_zh': '基于虚拟现实的多方法研究：听觉定位与机器人后果性声音评估'}
{'arxiv_id': 'arXiv:2504.00342', 'title': 'Aligning Diffusion Model with Problem Constraints for Trajectory Optimization', 'authors': 'Anjian Li, Ryne Beeson', 'link': 'https://arxiv.org/abs/2504.00342', 'abstract': 'Diffusion models have recently emerged as effective generative frameworks for trajectory optimization, capable of producing high-quality and diverse solutions. However, training these models in a purely data-driven manner without explicit incorporation of constraint information often leads to violations of critical constraints, such as goal-reaching, collision avoidance, and adherence to system dynamics. To address this limitation, we propose a novel approach that aligns diffusion models explicitly with problem-specific constraints, drawing insights from the Dynamic Data-driven Application Systems (DDDAS) framework. Our approach introduces a hybrid loss function that explicitly measures and penalizes constraint violations during training. Furthermore, by statistically analyzing how constraint violations evolve throughout the diffusion steps, we develop a re-weighting strategy that aligns predicted violations to ground truth statistics at each diffusion step. Evaluated on a tabletop manipulation and a two-car reach-avoid problem, our constraint-aligned diffusion model significantly reduces constraint violations compared to traditional diffusion models, while maintaining the quality of trajectory solutions. This approach is well-suited for integration into the DDDAS framework for efficient online trajectory adaptation as new environmental data becomes available.', 'abstract_zh': '基于约束对齐的扩散模型在轨迹优化中的应用', 'title_zh': '基于问题约束的扩散模型轨迹优化对齐'}
{'arxiv_id': 'arXiv:2504.00763', 'title': 'UnIRe: Unsupervised Instance Decomposition for Dynamic Urban Scene Reconstruction', 'authors': 'Yunxuan Mao, Rong Xiong, Yue Wang, Yiyi Liao', 'link': 'https://arxiv.org/abs/2504.00763', 'abstract': 'Reconstructing and decomposing dynamic urban scenes is crucial for autonomous driving, urban planning, and scene editing. However, existing methods fail to perform instance-aware decomposition without manual annotations, which is crucial for instance-level scene this http URL propose UnIRe, a 3D Gaussian Splatting (3DGS) based approach that decomposes a scene into a static background and individual dynamic instances using only RGB images and LiDAR point clouds. At its core, we introduce 4D superpoints, a novel representation that clusters multi-frame LiDAR points in 4D space, enabling unsupervised instance separation based on spatiotemporal correlations. These 4D superpoints serve as the foundation for our decomposed 4D initialization, i.e., providing spatial and temporal initialization to train a dynamic 3DGS for arbitrary dynamic classes without requiring bounding boxes or object this http URL, we introduce a smoothness regularization strategy in both 2D and 3D space, further improving the temporal this http URL on benchmark datasets show that our method outperforms existing methods in decomposed dynamic scene reconstruction while enabling accurate and flexible instance-level editing, making it a practical solution for real-world applications.', 'abstract_zh': '基于3D高斯散射的动态城市场景重建与分解对于自动驾驶、城市规划和场景编辑至关重要。然而，现有方法无法在无需手动标注的情况下进行实例感知分解，这对实例级场景编辑至关重要。本文提出UnIRe，一种基于3D高斯散射的方法，仅使用RGB图像和LiDAR点云将场景分解为静态背景和单独的动态实例。核心在于我们引入了4D超点，这是一种新型表示，能够在4D空间中聚类多帧LiDAR点，从而基于时空相关性实现无监督实例分离。这些4D超点作为我们分解的4D初始化的基础，为任意动态类训练动态3D高斯散射提供空间和时间初始化，而无需边界框或物体标注。在引入平滑正则化策略后，进一步提高了时间一致性。在基准数据集上的实验结果表明，我们的方法在动态场景重建分解方面优于现有方法，并且能够实现准确灵活的实例级编辑，是实际应用中的一个实用解决方案。', 'title_zh': 'UnIRe：无监督实例分解的城市动态场景重建'}
{'arxiv_id': 'arXiv:2504.00017', 'title': 'Enhance Vision-based Tactile Sensors via Dynamic Illumination and Image Fusion', 'authors': 'Artemii Redkin, Zdravko Dugonjic, Mike Lambeta, Roberto Calandra', 'link': 'https://arxiv.org/abs/2504.00017', 'abstract': 'Vision-based tactile sensors use structured light to measure deformation in their elastomeric interface. Until now, vision-based tactile sensors such as DIGIT and GelSight have been using a single, static pattern of structured light tuned to the specific form factor of the sensor. In this work, we investigate the effectiveness of dynamic illumination patterns, in conjunction with image fusion techniques, to improve the quality of sensing of vision-based tactile sensors. Specifically, we propose to capture multiple measurements, each with a different illumination pattern, and then fuse them together to obtain a single, higher-quality measurement. Experimental results demonstrate that this type of dynamic illumination yields significant improvements in image contrast, sharpness, and background difference. This discovery opens the possibility of retroactively improving the sensing quality of existing vision-based tactile sensors with a simple software update, and for new hardware designs capable of fully exploiting dynamic illumination.', 'abstract_zh': '基于视觉的触觉传感器使用结构光动态照明模式提高变形测量质量的研究', 'title_zh': '通过动态照明和图像融合增强视觉触觉传感器'}
{'arxiv_id': 'arXiv:2504.00938', 'title': 'AI Judges in Design: Statistical Perspectives on Achieving Human Expert Equivalence With Vision-Language Models', 'authors': 'Kristen M. Edwards, Farnaz Tehranchi, Scarlett R. Miller, Faez Ahmed', 'link': 'https://arxiv.org/abs/2504.00938', 'abstract': "The subjective evaluation of early stage engineering designs, such as conceptual sketches, traditionally relies on human experts. However, expert evaluations are time-consuming, expensive, and sometimes inconsistent. Recent advances in vision-language models (VLMs) offer the potential to automate design assessments, but it is crucial to ensure that these AI ``judges'' perform on par with human experts. However, no existing framework assesses expert equivalence. This paper introduces a rigorous statistical framework to determine whether an AI judge's ratings match those of human experts. We apply this framework in a case study evaluating four VLM-based judges on key design metrics (uniqueness, creativity, usefulness, and drawing quality). These AI judges employ various in-context learning (ICL) techniques, including uni- vs. multimodal prompts and inference-time reasoning. The same statistical framework is used to assess three trained novices for expert-equivalence. Results show that the top-performing AI judge, using text- and image-based ICL with reasoning, achieves expert-level agreement for uniqueness and drawing quality and outperforms or matches trained novices across all metrics. In 6/6 runs for both uniqueness and creativity, and 5/6 runs for both drawing quality and usefulness, its agreement with experts meets or exceeds that of the majority of trained novices. These findings suggest that reasoning-supported VLM models can achieve human-expert equivalence in design evaluation. This has implications for scaling design evaluation in education and practice, and provides a general statistical framework for validating AI judges in other domains requiring subjective content evaluation.", 'abstract_zh': '基于视觉-语言模型的早期工程设计主观评价：评估AI评判员与人类专家的等效性', 'title_zh': 'AI法官在设计中的角色：视觉-语言模型在实现人类专家等效性方面的统计视角'}
{'arxiv_id': 'arXiv:2504.01016', 'title': 'GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors', 'authors': 'Tian-Xing Xu, Xiangjun Gao, Wenbo Hu, Xiaoyu Li, Song-Hai Zhang, Ying Shan', 'link': 'https://arxiv.org/abs/2504.01016', 'abstract': 'Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.', 'abstract_zh': '尽管在视频深度估计方面取得了显著进展，现有方法通过仿射不变预测在实现几何保真方面仍存在固有限制，限制了其在重建及其他基于度量的下游任务中的应用。我们提出了一种名为GeometryCrafter的新框架，该框架能够从开放世界视频中恢复具有时间连续性的高保真点图序列，从而实现精确的3D/4D重建、相机参数估计及其他基于深度的应用。该方法的核心是一种点图变分自编码器（VAE），能够学习与视频潜在分布无关的潜在空间，从而有效进行点图编码和解码。利用VAE，我们训练了一个视频扩散模型，该模型能够基于输入视频对点图序列的概率分布进行建模。在多种数据集上的 extensive 评估表明，GeometryCrafter 实现了最先进的3D准确性、时间一致性及泛化能力。', 'title_zh': 'GeometryCrafter: 开放世界视频中的扩散先验一致几何估计'}
{'arxiv_id': 'arXiv:2504.00999', 'title': 'MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization', 'authors': 'Siyuan Li, Luyuan Zhang, Zedong Wang, Juanxi Tian, Cheng Tan, Zicheng Liu, Chang Yu, Qingsong Xie, Haonan Lu, Haoqian Wang, Zhen Lei', 'link': 'https://arxiv.org/abs/2504.00999', 'abstract': 'Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at this https URL.', 'abstract_zh': '基于向量量化（VQ）的掩码图像建模（MIM）在自我监督预训练和图像生成方面取得了巨大成功。然而，大多数现有方法在生成质量与表示学习及效率之间的权衡中表现不佳。为推动这一范式的极限，我们提出 MergeVQ，将token压缩技术集成到基于向量量化的生成模型中，以统一架构在图像生成和视觉表示学习之间搭建桥梁。在预训练阶段，MergeVQ 在编码器自注意力模块之后通过token合并模块解耦 top-k 语义，并在后续的无查找量化（LFQ）和全局对齐中保持这些语义，然后通过解码器中的交叉注意力恢复其细颗粒度细节进行重构。对于生成的第二阶段，我们引入 MergeAR，通过KV缓存压缩提高按扫描线顺序预测的效率。在ImageNet上的广泛实验验证了MergeVQ作为AR生成模型在视觉表示学习和图像生成任务中的竞争力，同时保持了有利的token效率和推理速度。相关代码和模型将在此处提供。', 'title_zh': 'MergeVQ：一种基于解耦令牌合并与量化的一体化视觉生成与表示框架'}
{'arxiv_id': 'arXiv:2504.00638', 'title': 'Impact of Data Duplication on Deep Neural Network-Based Image Classifiers: Robust vs. Standard Models', 'authors': 'Alireza Aghabagherloo, Aydin Abadi, Sumanta Sarkar, Vishnu Asutosh Dasu, Bart Preneel', 'link': 'https://arxiv.org/abs/2504.00638', 'abstract': 'The accuracy and robustness of machine learning models against adversarial attacks are significantly influenced by factors such as training data quality, model architecture, the training process, and the deployment environment. In recent years, duplicated data in training sets, especially in language models, has attracted considerable attention. It has been shown that deduplication enhances both training performance and model accuracy in language models. While the importance of data quality in training image classifier Deep Neural Networks (DNNs) is widely recognized, the impact of duplicated images in the training set on model generalization and performance has received little attention.\nIn this paper, we address this gap and provide a comprehensive study on the effect of duplicates in image classification. Our analysis indicates that the presence of duplicated images in the training set not only negatively affects the efficiency of model training but also may result in lower accuracy of the image classifier. This negative impact of duplication on accuracy is particularly evident when duplicated data is non-uniform across classes or when duplication, whether uniform or non-uniform, occurs in the training set of an adversarially trained model. Even when duplicated samples are selected in a uniform way, increasing the amount of duplication does not lead to a significant improvement in accuracy.', 'abstract_zh': '机器学习模型对抗 adversarial 攻击的准确性和鲁棒性受训练数据质量、模型架构、训练过程和部署环境等因素的影响。近年来，训练集中的重复数据，特别是在语言模型中，引起了广泛关注。研究表明，去重可以提高语言模型的训练性能和模型准确性。虽然图像分类深层神经网络（DNNs）训练数据质量的重要性得到广泛认可，但训练集中的重复图像对模型泛化能力和性能的影响却很少被关注。本文填补了这一空白，对图像分类中重复数据的影响进行了全面研究。我们的分析表明，训练集中的重复图像不仅负面影响了模型训练的效率，还可能导致图像分类器的准确性降低。这种重复数据对准确性的负面影响，在类别间重复数据非均匀分布或在对抗训练模型的训练集中出现均匀或非均匀重复数据时尤为明显。即使重复样本以均匀方式选择，增加重复数据的数量也不会显著提高准确性。', 'title_zh': '基于深层神经网络的图像分类器中数据冗余的影响：稳健模型与标准模型的比较'}
{'arxiv_id': 'arXiv:2504.00515', 'title': 'Training Frozen Feature Pyramid DINOv2 for Eyelid Measurements with Infinite Encoding and Orthogonal Regularization', 'authors': 'Chun-Hung Chen', 'link': 'https://arxiv.org/abs/2504.00515', 'abstract': 'Accurate measurement of eyelid parameters such as Margin Reflex Distances (MRD1, MRD2) and Levator Function (LF) is critical in oculoplastic diagnostics but remains limited by manual, inconsistent methods. This study evaluates deep learning models: SE-ResNet, EfficientNet, and the vision transformer-based DINOv2 for automating these measurements using smartphone-acquired images. We assess performance across frozen and fine-tuned settings, using MSE, MAE, and R2 metrics. DINOv2, pretrained through self-supervised learning, demonstrates superior scalability and robustness, especially under frozen conditions ideal for mobile deployment. Lightweight regressors such as MLP and Deep Ensemble offer high precision with minimal computational overhead. To address class imbalance and improve generalization, we integrate focal loss, orthogonal regularization, and binary encoding strategies. Our results show that DINOv2 combined with these enhancements delivers consistent, accurate predictions across all tasks, making it a strong candidate for real-world, mobile-friendly clinical applications. This work highlights the potential of foundation models in advancing AI-powered ophthalmic care.', 'abstract_zh': '准确测量眼睑参数（如MRD1、MRD2和LEV功能）对于眼睑整形诊断至关重要，但目前仍受限于手动且不一致的方法。本研究评估了SE-ResNet、EfficientNet和基于视觉变换器的DINOv2等深度学习模型，以利用手机拍摄的图像自动进行这些测量。我们使用均方误差、平均绝对误差和相关系数对不同固定和微调设置下的模型性能进行了评估。通过自监督学习预训练的DINOv2在固定条件下表现出色，具备更好的可扩展性和鲁棒性，特别适合移动部署。轻量级回归模型如MLP和深度集成模型提供了高精度且计算成本低的优势。为解决类别不平衡并提高泛化能力，我们引入了焦点损失、正交正则化和二进制编码策略。实验结果表明，结合这些增强后的DINOv2模型在所有任务中都能提供一致且准确的预测，使其成为面向实际应用场景的移动友好型临床应用的有力候选。本研究突显了基础模型在推进AI辅助眼科护理方面的潜力。', 'title_zh': '训练冻结特征金字塔的DINOv2进行眼睑测量：无限编码与正交正则化'}
{'arxiv_id': 'arXiv:2504.00457', 'title': 'Distilling Multi-view Diffusion Models into 3D Generators', 'authors': 'Hao Qin, Luyuan Chen, Ming Kong, Mengxu Lu, Qiang Zhu', 'link': 'https://arxiv.org/abs/2504.00457', 'abstract': "We introduce DD3G, a formulation that Distills a multi-view Diffusion model (MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and integrates extensive visual and spatial geometric knowledge from the MV-DM by simulating its ordinary differential equation (ODE) trajectory, ensuring the distilled generator generalizes better than those trained solely on 3D data. Unlike previous amortized optimization approaches, we align the MV-DM and 3D generator representation spaces to transfer the teacher's probabilistic flow to the student, thus avoiding inconsistencies in optimization objectives caused by probabilistic sampling. The introduction of probabilistic flow and the coupling of various attributes in 3D Gaussians introduce challenges in the generation process. To tackle this, we propose PEPD, a generator consisting of Pattern Extraction and Progressive Decoding phases, which enables efficient fusion of probabilistic flow and converts a single image into 3D Gaussians within 0.06 seconds. Furthermore, to reduce knowledge loss and overcome sparse-view supervision, we design a joint optimization objective that ensures the quality of generated samples through explicit supervision and implicit verification. Leveraging existing 2D generation models, we compile 120k high-quality RGBA images for distillation. Experiments on synthetic and public datasets demonstrate the effectiveness of our method. Our project is available at: this https URL", 'abstract_zh': 'DD3G：一种通过高斯绘制将多视图扩散模型提炼为3D生成器的公式', 'title_zh': '蒸馏多视图扩散模型为3D生成器'}
{'arxiv_id': 'arXiv:2504.00401', 'title': 'Beyond Wide-Angle Images: Unsupervised Video Portrait Correction via Spatiotemporal Diffusion Adaptation', 'authors': 'Wenbo Nie, Lang Nie, Chunyu Lin, Jingwen Chen, Ke Xing, Jiyuan Wang, Yao Zhao', 'link': 'https://arxiv.org/abs/2504.00401', 'abstract': 'Wide-angle cameras, despite their popularity for content creation, suffer from distortion-induced facial stretching-especially at the edge of the lens-which degrades visual appeal. To address this issue, we propose an image portrait correction framework using diffusion models named ImagePD. It integrates the long-range awareness of transformer and multi-step denoising of diffusion models into a unified framework, achieving global structural robustness and local detail refinement. Besides, considering the high cost of obtaining video labels, we then repurpose ImagePD for unlabeled wide-angle videos (termed VideoPD), by spatiotemporal diffusion adaption with spatial consistency and temporal smoothness constraints. For the former, we encourage the denoised image to approximate pseudo labels following the wide-angle distortion distribution pattern, while for the latter, we derive rectification trajectories with backward optical flows and smooth them. Compared with ImagePD, VideoPD maintains high-quality facial corrections in space and mitigates the potential temporal shakes sequentially. Finally, to establish an evaluation benchmark and train the framework, we establish a video portrait dataset with a large diversity in people number, lighting conditions, and background. Experiments demonstrate that the proposed methods outperform existing solutions quantitatively and qualitatively, contributing to high-fidelity wide-angle videos with stable and natural portraits. The codes and dataset will be available.', 'abstract_zh': '宽视角相机尽管在内容创作中很流行，但由于镜头边缘的失真会导致面部拉伸，影响视觉吸引力。为此，我们提出了一种名为ImagePD的图像肖像矫正框架。该框架结合了变压器的长距离感知能力和扩散模型的多步去噪，实现了全局结构的鲁棒性和局部细节的优化。此外，考虑到获取视频标签的成本较高，我们通过时空扩散适配以及空间一致性与时序平滑的约束，将ImagePD应用于未标注的宽视角视频（称为VideoPD）。对于前者，我们鼓励去噪后的图像接近宽视角失真的分布模式以生成伪标签；对于后者，我们基于后向光学流推导矫正轨迹并进行平滑。与ImagePD相比，VideoPD在空间上保持高质量的面部矫正，并顺序减轻潜在的时间抖动。最后，为了建立评估基准并训练该框架，我们建立了一个包含大量人群数量、光照条件和背景多样性的视频肖像数据集。实验表明，所提出的方法在定量和定性上均优于现有解决方案，有助于生成具有稳定和自然肖像的高质量宽视角视频。代码和数据集将公开。', 'title_zh': '超越宽视角图像：基于空时扩散适应的无监督视频人像矫正'}
{'arxiv_id': 'arXiv:2504.00356', 'title': 'Hybrid Global-Local Representation with Augmented Spatial Guidance for Zero-Shot Referring Image Segmentation', 'authors': 'Ting Liu, Siyuan Li', 'link': 'https://arxiv.org/abs/2504.00356', 'abstract': 'Recent advances in zero-shot referring image segmentation (RIS), driven by models such as the Segment Anything Model (SAM) and CLIP, have made substantial progress in aligning visual and textual information. Despite these successes, the extraction of precise and high-quality mask region representations remains a critical challenge, limiting the full potential of RIS tasks. In this paper, we introduce a training-free, hybrid global-local feature extraction approach that integrates detailed mask-specific features with contextual information from the surrounding area, enhancing mask region representation. To further strengthen alignment between mask regions and referring expressions, we propose a spatial guidance augmentation strategy that improves spatial coherence, which is essential for accurately localizing described areas. By incorporating multiple spatial cues, this approach facilitates more robust and precise referring segmentation. Extensive experiments on standard RIS benchmarks demonstrate that our method significantly outperforms existing zero-shot RIS models, achieving substantial performance gains. We believe our approach advances RIS tasks and establishes a versatile framework for region-text alignment, offering broader implications for cross-modal understanding and interaction. Code is available at this https URL .', 'abstract_zh': 'Recent advances in零样本图像分割（RIS），受Segment Anything Model（SAM）和CLIP等模型的驱动，已在视觉和文本信息对齐方面取得了显著进展。尽管取得了这些成功，精确和高质量的掩膜区域表示的提取仍然是一个关键挑战，限制了RIS任务的全部潜力。在本文中，我们提出了一种无需训练的混合全局-局部特征提取方法，该方法结合了详细的掩膜特定特征和周围区域的上下文信息，从而增强掩膜区域表示。为了进一步加强掩膜区域与引参照述表达之间的对齐，我们提出了一种空间指导增强策略，以提高空间连贯性，这是准确定位描述区域所必需的。通过结合多种空间线索，该方法有助于实现更稳健和精确的引参照述分割。在标准RIS基准上的广泛实验表明，我们的方法显著优于现有零样本RIS模型，实现了显著的性能提升。我们认为，我们的方法推进了RIS任务，并建立了适用于区域-文本对齐的多功能框架，提供了跨模态理解和交互的更广泛意义。代码可在以下链接获取：this https URL。', 'title_zh': '全局-局部混合表示增强空间引导在零-shot 参考图像分割中的应用'}
{'arxiv_id': 'arXiv:2504.00204', 'title': 'RailGoerl24: Görlitz Rail Test Center CV Dataset 2024', 'authors': 'Rustam Tagiew, Ilkay Wunderlich, Mark Sastuba, Steffen Seitz', 'link': 'https://arxiv.org/abs/2504.00204', 'abstract': "Driverless train operation for open tracks on urban guided transport and mainline railways requires, among other things automatic detection of actual and potential obstacles, especially humans, in the danger zone of the train's path. Machine learning algorithms have proven to be powerful state-of-the-art tools for this task. However, these algorithms require large amounts of high-quality annotated data containing human beings in railway-specific environments as training data. Unfortunately, the amount of publicly available datasets is not yet sufficient and is significantly inferior to the datasets in the road domain. Therefore, this paper presents RailGoerl24, an on-board visual light Full HD camera dataset of 12205 frames recorded in a railway test center of TÜV SÜD Rail, in Görlitz, Germany. Its main purpose is to support the development of driverless train operation for guided transport. RailGoerl24 also includes a terrestrial LiDAR scan covering parts of the area used to acquire the RGB data. In addition to the raw data, the dataset contains 33556 boxwise annotations in total for the object class 'person'. The faces of recorded actors are not blurred or altered in any other way. RailGoerl24, soon available at this http URL, can also be used for tasks beyond collision prediction.", 'abstract_zh': '基于轨道的无人驾驶列车运营视觉数据集RailGoerl24及其应用', 'title_zh': 'RailGoerl24: Görlitz 铁路测试中心 CV 数据集 2024'}
{'arxiv_id': 'arXiv:2504.00149', 'title': 'Towards Precise Action Spotting: Addressing Temporal Misalignment in Labels with Dynamic Label Assignment', 'authors': 'Masato Tamura', 'link': 'https://arxiv.org/abs/2504.00149', 'abstract': 'Precise action spotting has attracted considerable attention due to its promising applications. While existing methods achieve substantial performance by employing well-designed model architecture, they overlook a significant challenge: the temporal misalignment inherent in ground-truth labels. This misalignment arises when frames labeled as containing events do not align accurately with the actual event times, often as a result of human annotation errors or the inherent difficulties in precisely identifying event boundaries across neighboring frames. To tackle this issue, we propose a novel dynamic label assignment strategy that allows predictions to have temporal offsets from ground-truth action times during training, ensuring consistent event spotting. Our method extends the concept of minimum-cost matching, which is utilized in the spatial domain for object detection, to the temporal domain. By calculating matching costs based on predicted action class scores and temporal offsets, our method dynamically assigns labels to the most likely predictions, even when the predicted times of these predictions deviate from ground-truth times, alleviating the negative effects of temporal misalignment in labels. We conduct extensive experiments and demonstrate that our method achieves state-of-the-art performance, particularly in conditions where events are visually distinct and temporal misalignment in labels is common.', 'abstract_zh': '精确动作检测因其实用前景而受到广泛关注。尽管现有方法通过采用精心设计的模型架构实现了显著性能提升，但它们忽视了一个重要挑战：标注中的时间错位。这种错位源于标注的帧中事件与实际事件时间不准确对齐的问题，这通常是由于人工标注错误或在邻近帧中精确识别事件边界固有的困难所致。为了解决这一问题，我们提出了一种新颖的动力学标签分配策略，该策略允许在训练过程中预测时间与真实动作时间存在一定的时间偏差，从而确保动作的一致识别。我们的方法将空间域中用于对象检测的最小成本匹配概念扩展到了时间域。通过基于预测动作类别得分和时间偏差计算匹配成本，该方法能够动态地将标签分配给最有可能的预测，即使这些预测的时间与真实时间有所偏差，也能减轻标签时间错位带来的负面影响。我们进行了大量实验，并证明了我们的方法在事件视觉上明显不同且标签时间错位普遍的条件下达到了最先进的性能。', 'title_zh': '基于动态标签分配的精确动作检测：解决标签时间错位问题'}
{'arxiv_id': 'arXiv:2504.00051', 'title': 'The Cursive Transformer', 'authors': 'Sam Greydanus, Zachary Wimpee', 'link': 'https://arxiv.org/abs/2504.00051', 'abstract': 'Transformers trained on tokenized text, audio, and images can generate high-quality autoregressive samples. But handwriting data, represented as sequences of pen coordinates, remains underexplored. We introduce a novel tokenization scheme that converts pen stroke offsets to polar coordinates, discretizes them into bins, and then turns them into sequences of tokens with which to train a standard GPT model. This allows us to capture complex stroke distributions without using any specialized architectures (eg. the mixture density network or the self-advancing ASCII attention head from Graves 2014). With just 3,500 handwritten words and a few simple data augmentations, we are able to train a model that can generate realistic cursive handwriting. Our approach is simpler and more performant than previous RNN-based methods.', 'abstract_zh': '基于标记化文本、音频和图像训练的变压器可以生成高质量的自回归样本。但作为笔画坐标序列的手写数据仍处于未充分探索状态。我们提出了一种新颖的标记化方案，将笔画偏移转换为极坐标，对其离散化分箱，然后使用这些标记序列来训练标准的GPT模型。这使我们能够在不使用任何专有架构（例如混合密度网络或格瑞夫斯2014年的自推进ASCII注意力头）的情况下捕捉复杂的笔画分布。仅使用3,500个手写词和少量简单的数据增强，我们就能训练一个能够生成逼真手写体模型。我们的方法在RNN基方法上更加简洁且性能更优。', 'title_zh': '连笔变压器'}
{'arxiv_id': 'arXiv:2504.00047', 'title': 'EAP4EMSIG -- Enhancing Event-Driven Microscopy for Microfluidic Single-Cell Analysis', 'authors': 'Nils Friederich, Angelo Jovin Yamachui Sitcheu, Annika Nassal, Erenus Yildiz, Matthias Pesch, Maximilian Beichter, Lukas Scholtes, Bahar Akbaba, Thomas Lautenschlager, Oliver Neumann, Dietrich Kohlheyer, Hanno Scharr, Johannes Seiffarth, Katharina Nöh, Ralf Mikut', 'link': 'https://arxiv.org/abs/2504.00047', 'abstract': 'Microfluidic Live-Cell Imaging yields data on microbial cell factories. However, continuous acquisition is challenging as high-throughput experiments often lack realtime insights, delaying responses to stochastic events. We introduce three components in the Experiment Automation Pipeline for Event-Driven Microscopy to Smart Microfluidic Single-Cell Analysis: a fast, accurate Deep Learning autofocusing method predicting the focus offset, an evaluation of real-time segmentation methods and a realtime data analysis dashboard. Our autofocusing achieves a Mean Absolute Error of 0.0226\\textmu m with inference times below 50~ms. Among eleven Deep Learning segmentation methods, Cellpose~3 reached a Panoptic Quality of 93.58\\%, while a distance-based method is fastest (121~ms, Panoptic Quality 93.02\\%). All six Deep Learning Foundation Models were unsuitable for real-time segmentation.', 'abstract_zh': '微流控活细胞成像提供了微生物细胞工厂的数据。然而，连续获取数据颇具挑战，因为高通量实验往往缺乏实时洞察，延误了对随机事件的响应。我们提出了事件驱动显微镜到智能微流控单细胞分析实验自动化管道的三个组成部分：一种快速准确的深度学习自动对焦方法，用于预测焦深偏移，实时分割方法的评估，以及实时数据分析仪表盘。我们的自动对焦方法的绝对平均误差为0.0226μm，推理时间低于50毫秒。在 eleven 种深度学习分割方法中，Cellpose 达到了 93.58% 的全景质量，而基于距离的方法最快（121 毫秒，全景质量 93.02%）。所有六种深度学习基础模型都不适合实时分割。', 'title_zh': 'EAP4EMSIG -- 增强事件驱动显微镜技术用于微流控单细胞分析'}
{'arxiv_id': 'arXiv:2504.00037', 'title': 'ViT-Linearizer: Distilling Quadratic Knowledge into Linear-Time Vision Models', 'authors': 'Guoyizhe Wei, Rama Chellappa', 'link': 'https://arxiv.org/abs/2504.00037', 'abstract': "Vision Transformers (ViTs) have delivered remarkable progress through global self-attention, yet their quadratic complexity can become prohibitive for high-resolution inputs. In this work, we present ViT-Linearizer, a cross-architecture distillation framework that transfers rich ViT representations into a linear-time, recurrent-style model. Our approach leverages 1) activation matching, an intermediate constraint that encourages student to align its token-wise dependencies with those produced by the teacher, and 2) masked prediction, a contextual reconstruction objective that requires the student to predict the teacher's representations for unseen (masked) tokens, to effectively distill the quadratic self-attention knowledge into the student while maintaining efficient complexity. Empirically, our method provides notable speedups particularly for high-resolution tasks, significantly addressing the hardware challenges in inference. Additionally, it also elevates Mamba-based architectures' performance on standard vision benchmarks, achieving a competitive 84.3% top-1 accuracy on ImageNet with a base-sized model. Our results underscore the good potential of RNN-based solutions for large-scale visual tasks, bridging the gap between theoretical efficiency and real-world practice.", 'abstract_zh': 'Vision Transformers线性化器（ViT-Linearizer）：一种跨架构知识蒸馏框架', 'title_zh': 'ViT-线性化器：将二次知识 distilled 到线性时间视觉模型中'}
{'arxiv_id': 'arXiv:2504.00026', 'title': 'Diffusion models applied to skin and oral cancer classification', 'authors': 'José J. M. Uliana, Renato A. Krohling', 'link': 'https://arxiv.org/abs/2504.00026', 'abstract': 'This study investigates the application of diffusion models in medical image classification (DiffMIC), focusing on skin and oral lesions. Utilizing the datasets PAD-UFES-20 for skin cancer and P-NDB-UFES for oral cancer, the diffusion model demonstrated competitive performance compared to state-of-the-art deep learning models like Convolutional Neural Networks (CNNs) and Transformers. Specifically, for the PAD-UFES-20 dataset, the model achieved a balanced accuracy of 0.6457 for six-class classification and 0.8357 for binary classification (cancer vs. non-cancer). For the P-NDB-UFES dataset, it attained a balanced accuracy of 0.9050. These results suggest that diffusion models are viable models for classifying medical images of skin and oral lesions. In addition, we investigate the robustness of the model trained on PAD-UFES-20 for skin cancer but tested on the clinical images of the HIBA dataset.', 'abstract_zh': '该研究探讨了扩散模型在医学图像分类（DiffMIC）中的应用，重点关注皮肤和口腔病变。利用PAD-UFES-20数据集进行皮肤癌分类和P-NDB-UFES数据集进行口腔癌分类，扩散模型的性能与Convolutional Neural Networks (CNNs)和Transformers等先进深度学习模型相当。具体而言，PAD-UFES-20数据集上的六类分类实现了均衡精度0.6457，二类分类（癌症 vs. 非癌症）实现了均衡精度0.8357；P-NDB-UFES数据集上的均衡精度为0.9050。这些结果表明，扩散模型是分类皮肤和口腔病变医学图像的可行模型。此外，我们还研究了在PAD-UFES-20数据集上训练但在HIBA数据集的临床图像上测试的模型的鲁棒性。', 'title_zh': '扩散模型在皮肤和口腔癌分类中的应用'}
