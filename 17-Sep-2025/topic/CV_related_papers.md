# ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic Medical Datasets Generation 

**Title (ZH)**: ROOM：一种基于物理的连续体机器人模拟器，用于生成逼真的医疗数据集 

**Authors**: Salvatore Esposito, Matías Mattamala, Daniel Rebain, Francis Xiatian Zhang, Kevin Dhaliwal, Mohsen Khadem, Subramanian Ramamoorthy  

**Link**: [PDF](https://arxiv.org/pdf/2509.13177)  

**Abstract**: Continuum robots are advancing bronchoscopy procedures by accessing complex lung airways and enabling targeted interventions. However, their development is limited by the lack of realistic training and test environments: Real data is difficult to collect due to ethical constraints and patient safety concerns, and developing autonomy algorithms requires realistic imaging and physical feedback. We present ROOM (Realistic Optical Observation in Medicine), a comprehensive simulation framework designed for generating photorealistic bronchoscopy training data. By leveraging patient CT scans, our pipeline renders multi-modal sensor data including RGB images with realistic noise and light specularities, metric depth maps, surface normals, optical flow and point clouds at medically relevant scales. We validate the data generated by ROOM in two canonical tasks for medical robotics -- multi-view pose estimation and monocular depth estimation, demonstrating diverse challenges that state-of-the-art methods must overcome to transfer to these medical settings. Furthermore, we show that the data produced by ROOM can be used to fine-tune existing depth estimation models to overcome these challenges, also enabling other downstream applications such as navigation. We expect that ROOM will enable large-scale data generation across diverse patient anatomies and procedural scenarios that are challenging to capture in clinical settings. Code and data: this https URL. 

**Abstract (ZH)**: 连续体机器人通过访问复杂的肺部气道并实现目标干预，正在推进支气管镜检查程序。然而，其发展受限于缺乏现实的训练和测试环境：由于伦理约束和患者安全问题，真实数据难以收集，同时开发自主算法需要现实的成像和物理反馈。我们提出了ROOM（医学中的现实光学观察），一个为生成逼真的支气管镜检查训练数据而设计的综合仿真框架。通过利用患者CT扫描，我们的管道渲染包括具有现实噪声和光泽的RGB图像、度量深度图、表面法线、光流和点云等多种模态的传感器数据，适用于医学相关规模。我们在医学机器人领域的两个经典任务——多视角姿态估计和单目深度估计中验证了ROOM生成的数据，展示了最先进的方法需要克服的多种挑战，以迁移到这些医学环境中。此外，我们展示ROOM生成的数据可以用于微调现有的深度估计模型以克服这些挑战，同时也使其他下游应用程序如导航成为可能。我们预计ROOM将使大规模数据生成成为可能，涵盖在临床环境中难以捕捉到的多种患者解剖结构和程序场景。代码和数据：this https URL。 

---
# Unleashing the Power of Discrete-Time State Representation: Ultrafast Target-based IMU-Camera Spatial-Temporal Calibration 

**Title (ZH)**: 解锁离散时间状态表示的潜力：基于目标的IMU-相机时空超快校准 

**Authors**: Junlin Song, Antoine Richard, Miguel Olivares-Mendez  

**Link**: [PDF](https://arxiv.org/pdf/2509.12846)  

**Abstract**: Visual-inertial fusion is crucial for a large amount of intelligent and autonomous applications, such as robot navigation and augmented reality. To bootstrap and achieve optimal state estimation, the spatial-temporal displacements between IMU and cameras must be calibrated in advance. Most existing calibration methods adopt continuous-time state representation, more specifically the B-spline. Despite these methods achieve precise spatial-temporal calibration, they suffer from high computational cost caused by continuous-time state representation. To this end, we propose a novel and extremely efficient calibration method that unleashes the power of discrete-time state representation. Moreover, the weakness of discrete-time state representation in temporal calibration is tackled in this paper. With the increasing production of drones, cellphones and other visual-inertial platforms, if one million devices need calibration around the world, saving one minute for the calibration of each device means saving 2083 work days in total. To benefit both the research and industry communities, our code will be open-source. 

**Abstract (ZH)**: 视觉-惯性融合在大量智能自主应用中至关重要，如机器人导航和增强现实。为了初始化并实现最优状态估计，必须提前校准imu和摄像头之间的时空位移。现有的大多数校准方法采用连续时间状态表示，具体为B样条表示。尽管这些方法能够实现精确的时空校准，但由于连续时间状态表示导致了较高的计算成本。为了解决这一问题，我们提出了一种新颖且极其高效的校准方法，利用离散时间状态表示的强大功能。此外，本文还解决了离散时间状态表示在时间校准中的不足。随着无人机、手机和其他视觉-惯性平台的生产增加，如果全球有一百万台设备需要校准，那么为每台设备节省一分钟校准时间就意味着总共可以节省2083个工作日。为了同时惠及研究和工业界，我们的代码将开源。 

---
# ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation 

**Title (ZH)**: ActiveVLN：通过多轮RL在视觉语言导航中实现主动探索 

**Authors**: Zekai Zhang, Weiye Zhu, Hewei Pan, Xiangchen Wang, Rongtao Xu, Xing Sun, Feng Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2509.12618)  

**Abstract**: The Vision-and-Language Navigation (VLN) task requires an agent to follow natural language instructions and navigate through complex environments. Existing MLLM-based VLN methods primarily rely on imitation learning (IL) and often use DAgger for post-training to mitigate covariate shift. While effective, these approaches incur substantial data collection and training costs. Reinforcement learning (RL) offers a promising alternative. However, prior VLN RL methods lack dynamic interaction with the environment and depend on expert trajectories for reward shaping, rather than engaging in open-ended active exploration. This restricts the agent's ability to discover diverse and plausible navigation routes. To address these limitations, we propose ActiveVLN, a VLN framework that explicitly enables active exploration through multi-turn RL. In the first stage, a small fraction of expert trajectories is used for IL to bootstrap the agent. In the second stage, the agent iteratively predicts and executes actions, automatically collects diverse trajectories, and optimizes multiple rollouts via the GRPO objective. To further improve RL efficiency, we introduce a dynamic early-stopping strategy to prune long-tail or likely failed trajectories, along with additional engineering optimizations. Experiments show that ActiveVLN achieves the largest performance gains over IL baselines compared to both DAgger-based and prior RL-based post-training methods, while reaching competitive performance with state-of-the-art approaches despite using a smaller model. Code and data will be released soon. 

**Abstract (ZH)**: 基于视线-语言导航（VLN）的任务要求智能体遵循自然语言指令并在复杂环境中导航。现有的基于多模态预训练语言模型的VLN方法主要依赖于模仿学习（IL），并且常常使用DAgger在后续训练中减轻协变量移位。虽然这些方法有效，但它们会带来大量数据收集和训练成本。强化学习（RL）提供了有前景的替代方案。然而，之前的VLN RL方法缺乏与环境的动态交互，并依赖于专家轨迹用于奖励塑造，而不是进行开放式的主动探索。这限制了智能体发现多样化和合理导航路径的能力。为了解决这些限制，我们提出了ActiveVLN，这是一种通过多回合RL明确促进主动探索的VLN框架。在第一阶段，使用少量的专家轨迹进行IL以引导智能体。在第二阶段，智能体迭代地预测和执行动作，自动收集多样化的轨迹，并通过GRPO目标优化多个滚动部署。为了进一步提高RL的效率，我们引入了动态提前停止策略来修剪长尾或很可能失败的轨迹，并且还进行了额外的工程优化。实验结果表明，与基于DAgger的和之前的基于RL的后续训练方法相比，ActiveVLN在IL基线下实现了最大的性能提升，尽管使用了较小的模型，其性能仍可与现有最先进的方法竞争。代码和数据将很快发布。 

---
# Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles 

**Title (ZH)**: 小型无人机辅助的神经网络三维对象重建 

**Authors**: Àlmos Veres-Vitàlyos, Genis Castillo Gomez-Raya, Filip Lemic, Daniel Johannes Bugelnig, Bernhard Rinner, Sergi Abadal, Xavier Costa-Pérez  

**Link**: [PDF](https://arxiv.org/pdf/2509.12458)  

**Abstract**: Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for navigating indoor and hard-to-reach areas, yet their significant constraints in payload and autonomy have largely prevented their use for complex tasks like high-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we introduce a novel system architecture that enables fully autonomous, high-fidelity 3D scanning of static objects using UAVs weighing under 100 grams. Our core innovation lies in a dual-reconstruction pipeline that creates a real-time feedback loop between data capture and flight control. A near-real-time (near-RT) process uses Structure from Motion (SfM) to generate an instantaneous pointcloud of the object. The system analyzes the model quality on the fly and dynamically adapts the UAV's trajectory to intelligently capture new images of poorly covered areas. This ensures comprehensive data acquisition. For the final, detailed output, a non-real-time (non-RT) pipeline employs a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR) approach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB) location data to achieve superior accuracy. We implemented and validated this architecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both single- and multi-UAV configurations, conclusively show that dynamic trajectory adaptation consistently improves reconstruction quality over static flight paths. This work demonstrates a scalable and autonomous solution that unlocks the potential of miniaturized UAVs for fine-grained 3D reconstruction in constrained environments, a capability previously limited to much larger platforms. 

**Abstract (ZH)**: 小型无人驾驶飞机（UAVs）在导航室内和难以到达区域方面展现出巨大的潜力，但由于其显著的载重和自主性限制，它们在执行高精度三维重建等复杂任务方面应用受限。为克服这一挑战，我们提出了一种新型系统架构，使用重量低于100克的无人驾驶飞机实现完全自主的高保真三维静态对象扫描。我们的核心创新在于一种双重建管道，它创建了数据采集与飞行控制之间的实时反馈循环。一个近实时（near-RT）过程使用结构从运动（SfM）生成对象的即时点云。系统在飞行过程中动态分析模型质量，并智能调整无人机的飞行轨迹以捕获未覆盖区域的新图像，从而确保全面的数据采集。对于最终的详细输出，一个非实时（non-RT）管道采用基于神经辐射场（NeRF）的神经三维重建（N3DR）方法，结合SfM提取的相机姿态和精确的超宽带（UWB）位置数据，以实现更高的准确性。我们利用Crazyflie 2.1无人机实现了并验证了这一架构。在单机和多机配置下的实验一致证明，动态轨迹调整可以提高重建质量。本研究展示了可扩展且自主的解决方案，解锁了微型化无人机在受限环境中的微米级三维重建潜力，这一能力此前仅限于更大的平台。 

---
# A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual Assembly Control 

**Title (ZH)**: 支持制造中小企业在视觉装配控制中使用的合成数据管道 

**Authors**: Jonas Werheid, Shengjie He, Aymen Gannouni, Anas Abdelrazeq, Robert H. Schmitt  

**Link**: [PDF](https://arxiv.org/pdf/2509.13089)  

**Abstract**: Quality control of assembly processes is essential in manufacturing to ensure not only the quality of individual components but also their proper integration into the final product. To assist in this matter, automated assembly control using computer vision methods has been widely implemented. However, the costs associated with image acquisition, annotation, and training of computer vision algorithms pose challenges for integration, especially for small- and medium-sized enterprises (SMEs), which often lack the resources for extensive training, data collection, and manual image annotation. Synthetic data offers the potential to reduce manual data collection and labeling. Nevertheless, its practical application in the context of assembly quality remains limited. In this work, we present a novel approach for easily integrable and data-efficient visual assembly control. Our approach leverages simulated scene generation based on computer-aided design (CAD) data and object detection algorithms. The results demonstrate a time-saving pipeline for generating image data in manufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95) up to 99,5% for correctly identifying instances of synthetic planetary gear system components within our simulated training data, and up to 93% when transferred to real-world camera-captured testing data. This research highlights the effectiveness of synthetic data generation within an adaptable pipeline and underscores its potential to support SMEs in implementing resource-efficient visual assembly control solutions. 

**Abstract (ZH)**: 基于计算机辅助设计的模拟场景生成与物体检测算法的可整合高效视觉装配控制方法 

---
# Contrastive timbre representations for musical instrument and synthesizer retrieval 

**Title (ZH)**: 对比音色表示用于音乐乐器和合成器检索 

**Authors**: Gwendal Le Vaillant, Yannick Molle  

**Link**: [PDF](https://arxiv.org/pdf/2509.13285)  

**Abstract**: Efficiently retrieving specific instrument timbres from audio mixtures remains a challenge in digital music production. This paper introduces a contrastive learning framework for musical instrument retrieval, enabling direct querying of instrument databases using a single model for both single- and multi-instrument sounds. We propose techniques to generate realistic positive/negative pairs of sounds for virtual musical instruments, such as samplers and synthesizers, addressing limitations in common audio data augmentation methods.
The first experiment focuses on instrument retrieval from a dataset of 3,884 instruments, using single-instrument audio as input. Contrastive approaches are competitive with previous works based on classification pre-training. The second experiment considers multi-instrument retrieval with a mixture of instruments as audio input. In this case, the proposed contrastive framework outperforms related works, achieving 81.7\% top-1 and 95.7\% top-5 accuracies for three-instrument mixtures. 

**Abstract (ZH)**: 高效从音频混合中检索特定乐器音色在数字音乐制作中仍是一项挑战。本文介绍了一种对比学习框架，使用户能够使用单一模型直接查询乐器数据库，适用于单乐器和多乐器声音检索。我们提出了一种生成虚拟乐器（如采样器和合成器）真实正负样本对的技术，解决了常见音频数据增强方法的局限性。
第一个实验专注于从包含3,884种乐器的数据集中检索乐器，使用单乐器音频作为输入。对比方法在基于分类预训练的作品中具有竞争力。第二个实验考虑了使用包含多种乐器的音频输入的多乐器检索。在这种情况下，提出的对比框架优于相关工作，在三乐器混合物中实现了81.7%的一年级准率和95.7%的前五级准率。 

---
# ResidualViT for Efficient Temporally Dense Video Encoding 

**Title (ZH)**: 基于残差ViT的高效时空密集视频编码 

**Authors**: Mattia Soldan, Fabian Caba Heilbron, Bernard Ghanem, Josef Sivic, Bryan Russell  

**Link**: [PDF](https://arxiv.org/pdf/2509.13255)  

**Abstract**: Several video understanding tasks, such as natural language temporal video grounding, temporal activity localization, and audio description generation, require "temporally dense" reasoning over frames sampled at high temporal resolution. However, computing frame-level features for these tasks is computationally expensive given the temporal resolution requirements. In this paper, we make three contributions to reduce the cost of computing features for temporally dense tasks. First, we introduce a vision transformer (ViT) architecture, dubbed ResidualViT, that leverages the large temporal redundancy in videos to efficiently compute temporally dense frame-level features. Our architecture incorporates (i) learnable residual connections that ensure temporal consistency across consecutive frames and (ii) a token reduction module that enhances processing speed by selectively discarding temporally redundant information while reusing weights of a pretrained foundation model. Second, we propose a lightweight distillation strategy to approximate the frame-level features of the original foundation model. Finally, we evaluate our approach across four tasks and five datasets, in both zero-shot and fully supervised settings, demonstrating significant reductions in computational cost (up to 60%) and improvements in inference speed (up to 2.5x faster), all while closely approximating the accuracy of the original foundation model. 

**Abstract (ZH)**: 几种视频理解任务，如自然语言 temporal 视频定位、时间活动定位和音频描述生成，需要在高时间分辨率下进行“时间密集型”推理。然而，给定时间分辨率要求，计算这些任务的帧级特征是计算密集型的。本文我们通过三个方面减少时间密集型任务特征计算的成本。首先，我们提出了一种名为 ResidualViT 的视觉转换器架构，利用视频中的大量时间冗余高效计算时间密集型帧级特征。该架构结合了（i）可学习的残差连接，以确保连续帧之间的时序一致性，以及（ii）一个 token 减少模块，在选择性地丢弃时间冗余信息的同时重用预训练基础模型的权重来提高处理速度。其次，我们提出了一种轻量级的知识蒸馏策略来逼近原基础模型的帧级特征。最后，我们在四个任务和五个数据集上评估了我们的方法，在零样本和完全监督设置下均显示出显著的计算成本降低（最多 60%）和推理速度提升（最多 2.5 倍），同时保持与原基础模型相近的准确度。 

---
# Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation 

**Title (ZH)**: 基于课程多任务自我监督改进嵌入式卫星高光谱图像分割的轻量级架构 

**Authors**: Hugo Carlesso, Josiane Mothe, Radu Tudor Ionescu  

**Link**: [PDF](https://arxiv.org/pdf/2509.13229)  

**Abstract**: Hyperspectral imaging (HSI) captures detailed spectral signatures across hundreds of contiguous bands per pixel, being indispensable for remote sensing applications such as land-cover classification, change detection, and environmental monitoring. Due to the high dimensionality of HSI data and the slow rate of data transfer in satellite-based systems, compact and efficient models are required to support onboard processing and minimize the transmission of redundant or low-value data, e.g. cloud-covered areas. To this end, we introduce a novel curriculum multi-task self-supervised learning (CMTSSL) framework designed for lightweight architectures for HSI analysis. CMTSSL integrates masked image modeling with decoupled spatial and spectral jigsaw puzzle solving, guided by a curriculum learning strategy that progressively increases data complexity during self-supervision. This enables the encoder to jointly capture fine-grained spectral continuity, spatial structure, and global semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously addresses spatial and spectral reasoning within a unified and computationally efficient design, being particularly suitable for training lightweight models for onboard satellite deployment. We validate our approach on four public benchmark datasets, demonstrating consistent gains in downstream segmentation tasks, using architectures that are over 16,000x lighter than some state-of-the-art models. These results highlight the potential of CMTSSL in generalizable representation learning with lightweight architectures for real-world HSI applications. Our code is publicly available at this https URL. 

**Abstract (ZH)**: 高光谱成像（HSI）捕获每个像素几百个连续波段的详细光谱特征，对于土地覆盖分类、变化检测和环境监测等遥感应用至关重要。由于HSI数据的高维特性及基于卫星系统的数据传输速率较慢，需要轻量级且高效的模型支持机载处理，并最小化传输冗余或低价值数据，如云覆盖区域的数据。为此，我们提出了一种用于HSI分析的新型 Curriculum 多任务自监督学习（CMTSSL）框架，该框架设计用于轻量级架构。CMTSSL 将掩码图像建模与分离的空间和光谱拼图解谜相结合，并借助课程学习策略，在自监督过程中逐步增加数据复杂性。这使编码器能够联合捕获細粒度的光谱连续性、空间结构和全球语义特征。与先前的双任务自监督学习方法不同，CMTSSL 在统一且计算高效的架构中同时处理空间和光谱推理，特别适合用于训练适用于机载卫星部署的轻量级模型。我们在四个公开基准数据集上验证了该方法，使用超过16,000倍轻于某些最新模型的架构，演示了在后续分割任务中的一致性改进。这些结果突显了CMTSSL 在现实世界HSI应用中轻量级架构的一般化表示学习潜力。我们的代码可在以下网址公开获取：this https URL。 

---
# Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery Detection - The 2024 Global Deepfake Image Detection Challenge 

**Title (ZH)**: 多维度面部伪造检测的分层深度融合框架 - 2024年全球深度合成图像检测挑战赛 

**Authors**: Kohou Wang, Huan Hu, Xiang Liu, Zezhou Chen, Ping Chen, Zhaoxiang Liu, Shiguo Lian  

**Link**: [PDF](https://arxiv.org/pdf/2509.13107)  

**Abstract**: The proliferation of sophisticated deepfake technology poses significant challenges to digital security and authenticity. Detecting these forgeries, especially across a wide spectrum of manipulation techniques, requires robust and generalized models. This paper introduces the Hierarchical Deep Fusion Framework (HDFF), an ensemble-based deep learning architecture designed for high-performance facial forgery detection. Our framework integrates four diverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT, which are meticulously fine-tuned through a multi-stage process on the MultiFFDI dataset. By concatenating the feature representations from these specialized models and training a final classifier layer, HDFF effectively leverages their collective strengths. This approach achieved a final score of 0.96852 on the competition's private leaderboard, securing the 20th position out of 184 teams, demonstrating the efficacy of hierarchical fusion for complex image classification tasks. 

**Abstract (ZH)**: sophisticated深fake技术的泛滥对数字安全和真实性构成了重大挑战。检测这些伪造品，特别是跨多种操作技术，需要稳健且通用的模型。本文介绍了一种基于层次深度融合框架（HDFF），这是一种用于高性能面部伪造检测的 ensemble 基础深度学习架构。我们的框架整合了四个不同的预训练子模型：Swin-MLP、CoAtNet、EfficientNetV2 和 DaViT，并通过多阶段过程在 MultiFFDI 数据集上精细调整。通过拼接这些专业模型的特征表示并训练最终分类器层，HDFF 有效地利用了他们的集体优势。该方法在比赛私有排行榜上获得了 0.96852 的最终分数，在 184 支参赛队伍中获得了第 20 名，展示了层次融合在复杂图像分类任务中的有效性。 

---
# Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection 

**Title (ZH)**: 双阶段重新加权MoEltrated第一人称错误检测 

**Authors**: Boyu Han, Qianqian Xu, Shilong Bao, Zhiyong Yang, Sicong Li, Qingming Huang  

**Link**: [PDF](https://arxiv.org/pdf/2509.12990)  

**Abstract**: In this report, we address the problem of determining whether a user performs an action incorrectly from egocentric video data. To handle the challenges posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted Mixture-of-Experts (DR-MoE) framework. In the first stage, features are extracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are combined through a feature-level expert module. In the second stage, three classifiers are trained with different objectives: reweighted cross-entropy to mitigate class imbalance, AUC loss to improve ranking under skewed distributions, and label-aware loss with sharpness-aware minimization to enhance calibration and generalization. Their predictions are fused using a classification-level expert module. The proposed method achieves strong performance, particularly in identifying rare and ambiguous mistake instances. The code is available at this https URL. 

**Abstract (ZH)**: 本文报告了一个从第一人称视频数据中确定用户行为是否错误的问题。为应对细微且不频繁错误带来的挑战，我们提出了一种双重阶段重权混合专家（DR-MoE）框架。在第一阶段，使用冻结的ViViT模型和LoRA调优的ViViT模型提取特征，并通过特征级专家模块进行结合。在第二阶段，训练了三个具有不同目标的分类器：重权交叉熵以缓解类别不平衡问题、AUC损失以改善偏斜分布下的排序能力，以及带有尖锐感知最小化的标签感知损失以增强校准和泛化能力。预测结果通过分类级专家模块进行融合。所提出的方法在识别稀有和模糊错误实例方面表现优异。代码可在以下链接获取。 

---
# Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing 

**Title (ZH)**: Runge-Kutta逼近与解耦注意力在矫正流反转和语义编辑中的应用 

**Authors**: Weiming Chen, Zhihan Zhu, Yijia Wang, Zhihai He  

**Link**: [PDF](https://arxiv.org/pdf/2509.12888)  

**Abstract**: Rectified flow (RF) models have recently demonstrated superior generative performance compared to DDIM-based diffusion models. However, in real-world applications, they suffer from two major challenges: (1) low inversion accuracy that hinders the consistency with the source image, and (2) entangled multimodal attention in diffusion transformers, which hinders precise attention control. To address the first challenge, we propose an efficient high-order inversion method for rectified flow models based on the Runge-Kutta solver of differential equations. To tackle the second challenge, we introduce Decoupled Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles text and image attention inside the multimodal diffusion transformers, enabling more precise semantic control. Extensive experiments on image reconstruction and text-guided editing tasks demonstrate that our method achieves state-of-the-art performance in terms of fidelity and editability. Code is available at this https URL. 

**Abstract (ZH)**: 修正流（RF）模型 recently demonstrated superior generative performance compared to DDIM-based diffusion models. However, in real-world applications, they suffer from two major challenges: (1) low inversion accuracy that hinders the consistency with the source image, and (2) entangled multimodal attention in diffusion transformers, which hinders precise attention control. To address the first challenge, we propose an efficient high-order inversion method for rectified flow models based on the Runge-Kutta solver of differential equations. To tackle the second challenge, we introduce Decoupled Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles text and image attention inside the multimodal diffusion transformers, enabling more precise semantic control. Extensive experiments on image reconstruction and text-guided editing tasks demonstrate that our method achieves state-of-the-art performance in terms of fidelity and editability. Code is available at this https URL. 

修正流（RF）模型相较于基于DDIM的扩散模型最近在生成性能上表现出优越性。然而，在实际应用中，它们面临两大挑战：（1）低反解精度，妨碍与源图像的一致性；（2）扩散变换器中交织的多模态注意力，妨碍精确的注意力控制。为应对第一个挑战，我们提出了一种基于微分方程Runge-Kutta求解器的有效高阶反解方法，用于修正流模型。为解决第二个挑战，我们引入了解耦扩散变换器注意机制（DDTA），这是一种在多模态扩散变换器内部分离文本和图像注意力的新机制，能够实现更精确的语义控制。在图像重建和文本导向编辑任务的广泛实验中，我们方法在保真度和可编辑性方面均达到了最先进的性能。代码见此 URL。 

---
# A Lightweight Pipeline for Noisy Speech Voice Cloning and Accurate Lip Sync Synthesis 

**Title (ZH)**: 轻量级噪声语音克隆和精准唇动同步合成管道 

**Authors**: Javeria Amir, Farwa Attaria, Mah Jabeen, Umara Noor, Zahid Rashid  

**Link**: [PDF](https://arxiv.org/pdf/2509.12831)  

**Abstract**: Recent developments in voice cloning and talking head generation demonstrate impressive capabilities in synthesizing natural speech and realistic lip synchronization. Current methods typically require and are trained on large scale datasets and computationally intensive processes using clean studio recorded inputs that is infeasible in noisy or low resource environments. In this paper, we introduce a new modular pipeline comprising Tortoise text to speech. It is a transformer based latent diffusion model that can perform high fidelity zero shot voice cloning given only a few training samples. We use a lightweight generative adversarial network architecture for robust real time lip synchronization. The solution will contribute to many essential tasks concerning less reliance on massive pre training generation of emotionally expressive speech and lip synchronization in noisy and unconstrained scenarios. The modular structure of the pipeline allows an easy extension for future multi modal and text guided voice modulation and it could be used in real world systems. 

**Abstract (ZH)**: 近期语音克隆和动作头生成的发展展示了在合成自然语音和逼真唇同步方面令人印象深刻的 capability。现有方法通常需要并依赖大规模数据集和计算密集型过程，使用干净的录音输入，在噪声大或资源有限的环境中不可行。本文介绍了一个新的模块化流水线，结合了Tortoise文本到语音模型。它是一个基于变换器的潜在扩散模型，仅需少量训练样本即可执行高保真零样本语音克隆。我们采用轻量级生成对抗网络架构以实现稳健的实时唇同步。该解决方案将为在噪声和非约束环境中减少对大规模预训练生成富有情感表达的语音和唇同步的依赖做出贡献。模块化流水线结构允许将来轻松扩展以支持多模态和文本引导的语音调节，并可用于实际系统。 

---
# CECT-Mamba: a Hierarchical Contrast-enhanced-aware Model for Pancreatic Tumor Subtyping from Multi-phase CECT 

**Title (ZH)**: CECT-Mamba：一种针对多期CECT胰腺肿瘤亚型分型的分层对比增强感知模型 

**Authors**: Zhifang Gong, Shuo Gao, Ben Zhao, Yingjing Xu, Yijun Yang, Shenghong Ju, Guangquan Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2509.12777)  

**Abstract**: Contrast-enhanced computed tomography (CECT) is the primary imaging technique that provides valuable spatial-temporal information about lesions, enabling the accurate diagnosis and subclassification of pancreatic tumors. However, the high heterogeneity and variability of pancreatic tumors still pose substantial challenges for precise subtyping diagnosis. Previous methods fail to effectively explore the contextual information across multiple CECT phases commonly used in radiologists' diagnostic workflows, thereby limiting their performance. In this paper, we introduce, for the first time, an automatic way to combine the multi-phase CECT data to discriminate between pancreatic tumor subtypes, among which the key is using Mamba with promising learnability and simplicity to encourage both temporal and spatial modeling from multi-phase CECT. Specifically, we propose a dual hierarchical contrast-enhanced-aware Mamba module incorporating two novel spatial and temporal sampling sequences to explore intra and inter-phase contrast variations of lesions. A similarity-guided refinement module is also imposed into the temporal scanning modeling to emphasize the learning on local tumor regions with more obvious temporal variations. Moreover, we design the space complementary integrator and multi-granularity fusion module to encode and aggregate the semantics across different scales, achieving more efficient learning for subtyping pancreatic tumors. The experimental results on an in-house dataset of 270 clinical cases achieve an accuracy of 97.4% and an AUC of 98.6% in distinguishing between pancreatic ductal adenocarcinoma (PDAC) and pancreatic neuroendocrine tumors (PNETs), demonstrating its potential as a more accurate and efficient tool. 

**Abstract (ZH)**: 增强CT影像的自动多阶段融合方法在胰腺肿瘤亚型诊断中的应用 

---
# Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models 

**Title (ZH)**: 防攻击转进攻：绕过弱防护以实现更强的视觉-语言模型 Jailbreaks 

**Authors**: Yunhan Zhao, Xiang Zheng, Xingjun Ma  

**Link**: [PDF](https://arxiv.org/pdf/2509.12724)  

**Abstract**: Despite their superb capabilities, Vision-Language Models (VLMs) have been shown to be vulnerable to jailbreak attacks. While recent jailbreaks have achieved notable progress, their effectiveness and efficiency can still be improved. In this work, we reveal an interesting phenomenon: incorporating weak defense into the attack pipeline can significantly enhance both the effectiveness and the efficiency of jailbreaks on VLMs. Building on this insight, we propose Defense2Attack, a novel jailbreak method that bypasses the safety guardrails of VLMs by leveraging defensive patterns to guide jailbreak prompt design. Specifically, Defense2Attack consists of three key components: (1) a visual optimizer that embeds universal adversarial perturbations with affirmative and encouraging semantics; (2) a textual optimizer that refines the input using a defense-styled prompt; and (3) a red-team suffix generator that enhances the jailbreak through reinforcement fine-tuning. We empirically evaluate our method on four VLMs and four safety benchmarks. The results demonstrate that Defense2Attack achieves superior jailbreak performance in a single attempt, outperforming state-of-the-art attack methods that often require multiple tries. Our work offers a new perspective on jailbreaking VLMs. 

**Abstract (ZH)**: 尽管视觉语言模型具有出色的性能，但已被证明对监禁攻击（Jailbreak）易受攻击。虽然近期的监禁攻击已取得显著进展，但其有效性及效率仍有待提升。在此工作中，我们揭示了一个有趣的现象：将弱防御融入攻击管道可以显著提高监禁攻击在视觉语言模型上的有效性和效率。基于这一洞察，我们提出了Defense2Attack，这是一种新颖的监禁攻击方法，通过利用防御模式引导监禁提示设计，从而绕过视觉语言模型的安全防护栏。具体来说，Defense2Attack 包含三个关键组件：（1）视觉优化器，嵌入具有肯定和鼓励语义的通用对抗性扰动；（2）文本优化器，使用防御风格的提示精炼输入；（3）红队后缀生成器，通过强化微调增强监禁攻击。我们通过在四个视觉语言模型和四个安全基准上进行实证评估，结果表明Defense2Attack 在单次尝试中实现了优越的监禁攻击性能，超越了通常需要多次尝试的最先进的攻击方法。我们的工作为监禁攻击视觉语言模型提供了新的视角。 

---
# A Comparative Study of YOLOv8 to YOLOv11 Performance in Underwater Vision Tasks 

**Title (ZH)**: YOLOv8与YOLOv11在水下视觉任务中性能比较研究 

**Authors**: Gordon Hung, Ivan Felipe Rodriguez  

**Link**: [PDF](https://arxiv.org/pdf/2509.12682)  

**Abstract**: Autonomous underwater vehicles (AUVs) increasingly rely on on-board computer-vision systems for tasks such as habitat mapping, ecological monitoring, and infrastructure inspection. However, underwater imagery is hindered by light attenuation, turbidity, and severe class imbalance, while the computational resources available on AUVs are limited. One-stage detectors from the YOLO family are attractive because they fuse localization and classification in a single, low-latency network; however, their terrestrial benchmarks (COCO, PASCAL-VOC, Open Images) leave open the question of how successive YOLO releases perform in the marine domain. We curate two openly available datasets that span contrasting operating conditions: a Coral Disease set (4,480 images, 18 classes) and a Fish Species set (7,500 images, 20 classes). For each dataset, we create four training regimes (25 %, 50 %, 75 %, 100 % of the images) while keeping balanced validation and test partitions fixed. We train YOLOv8-s, YOLOv9-s, YOLOv10-s, and YOLOv11-s with identical hyperparameters (100 epochs, 640 px input, batch = 16, T4 GPU) and evaluate precision, recall, mAP50, mAP50-95, per-image inference time, and frames-per-second (FPS). Post-hoc Grad-CAM visualizations probe feature utilization and localization faithfulness. Across both datasets, accuracy saturates after YOLOv9, suggesting architectural innovations primarily target efficiency rather than accuracy. Inference speed, however, improves markedly. Our results (i) provide the first controlled comparison of recent YOLO variants on underwater imagery, (ii) show that lightweight YOLOv10 offers the best speed-accuracy trade-off for embedded AUV deployment, and (iii) deliver an open, reproducible benchmark and codebase to accelerate future marine-vision research. 

**Abstract (ZH)**: 自主水下车辆（AUV）越来越多地依赖于船载计算机视觉系统进行海底制图、生态监测和基础设施检查等任务。然而， underwater 成像受到光衰减、浑浊以及严重类别不平衡的阻碍，而 AUV 可用的计算资源有限。YOLO 家族的一阶段检测器因其在单一低延迟网络中融合了定位和分类而具有吸引力；然而，其在 COCO、PASCAL-VOC 和 Open Images 等 terrestrial 基准上的表现留下了海洋领域性能的问题。我们编制了两个公开可用的数据集，涵盖了不同的操作条件：珊瑚疾病数据集（4,480 张图像，18 个类别）和鱼种数据集（7,500 张图像，20 个类别）。对于每个数据集，我们创建了四种训练制度（分别包含图像的 25%、50%、75% 和 100%），同时保持验证和测试分区平衡不变。我们使用相同的超参数训练 YOLOv8-s、YOLOv9-s、YOLOv10-s 和 YOLOv11-s（100 个 epoch，640 px 输入，批大小 = 16，T4 GPU），并评估精度、召回率、mAP50、mAP50-95、每张图像的推理时间和帧率（FPS）。事后 Grad-CAM 可视化探究了特征利用和定位的一致性。在两个数据集中，准确性在 YOLOv9 后饱和，这表明架构创新主要针对效率而非准确性。然而，推理速度显著提升。我们的结果包括：（i）首次对最近的 YOLO 变体在 underwater 成像上的表现进行受控比较；（ii）显示轻量级 YOLOv10 提供了嵌入式 AUV 部署的最佳速度-准确性权衡；（iii）提供了一个开放、可重复的基准和代码库，以加速未来海洋视觉研究。 

---
# MFAF: An EVA02-Based Multi-scale Frequency Attention Fusion Method for Cross-View Geo-Localization 

**Title (ZH)**: MFAF: 基于EVA02的多尺度频率注意力融合方法用于跨视角地理定位 

**Authors**: YiTong Liu, TianZhu Liu, YanFeng GU  

**Link**: [PDF](https://arxiv.org/pdf/2509.12673)  

**Abstract**: Cross-view geo-localization aims to determine the geographical location of a query image by matching it against a gallery of images. This task is challenging due to the significant appearance variations of objects observed from variable views, along with the difficulty in extracting discriminative features. Existing approaches often rely on extracting features through feature map segmentation while neglecting spatial and semantic information. To address these issues, we propose the EVA02-based Multi-scale Frequency Attention Fusion (MFAF) method. The MFAF method consists of Multi-Frequency Branch-wise Block (MFB) and the Frequency-aware Spatial Attention (FSA) module. The MFB block effectively captures both low-frequency structural features and high-frequency edge details across multiple scales, improving the consistency and robustness of feature representations across various viewpoints. Meanwhile, the FSA module adaptively focuses on the key regions of frequency features, significantly mitigating the interference caused by background noise and viewpoint variability. Extensive experiments on widely recognized benchmarks, including University-1652, SUES-200, and Dense-UAV, demonstrate that the MFAF method achieves competitive performance in both drone localization and drone navigation tasks. 

**Abstract (ZH)**: 基于EVA02的多尺度频域注意力融合方法用于跨视角地理定位 

---
# DeepEyeNet: Generating Medical Report for Retinal Images 

**Title (ZH)**: 深眼网络：生成眼科图像的医学报告 

**Authors**: Jia-Hong Huang  

**Link**: [PDF](https://arxiv.org/pdf/2509.12534)  

**Abstract**: The increasing prevalence of retinal diseases poses a significant challenge to the healthcare system, as the demand for ophthalmologists surpasses the available workforce. This imbalance creates a bottleneck in diagnosis and treatment, potentially delaying critical care. Traditional methods of generating medical reports from retinal images rely on manual interpretation, which is time-consuming and prone to errors, further straining ophthalmologists' limited resources. This thesis investigates the potential of Artificial Intelligence (AI) to automate medical report generation for retinal images. AI can quickly analyze large volumes of image data, identifying subtle patterns essential for accurate diagnosis. By automating this process, AI systems can greatly enhance the efficiency of retinal disease diagnosis, reducing doctors' workloads and enabling them to focus on more complex cases. The proposed AI-based methods address key challenges in automated report generation: (1) A multi-modal deep learning approach captures interactions between textual keywords and retinal images, resulting in more comprehensive medical reports; (2) Improved methods for medical keyword representation enhance the system's ability to capture nuances in medical terminology; (3) Strategies to overcome RNN-based models' limitations, particularly in capturing long-range dependencies within medical descriptions; (4) Techniques to enhance the interpretability of the AI-based report generation system, fostering trust and acceptance in clinical practice. These methods are rigorously evaluated using various metrics and achieve state-of-the-art performance. This thesis demonstrates AI's potential to revolutionize retinal disease diagnosis by automating medical report generation, ultimately improving clinical efficiency, diagnostic accuracy, and patient care. 

**Abstract (ZH)**: 人工智能在视网膜图像医学报告自动化生成中的潜在应用：提高眼底疾病诊断的临床效率、诊断准确性和患者护理质量 

---
# DinoAtten3D: Slice-Level Attention Aggregation of DinoV2 for 3D Brain MRI Anomaly Classification 

**Title (ZH)**: DinoAtten3D：基于DinoV2的切片级注意力聚合的3D脑MRI异常分类 

**Authors**: Fazle Rafsani, Jay Shah, Catherine D. Chong, Todd J. Schwedt, Teresa Wu  

**Link**: [PDF](https://arxiv.org/pdf/2509.12512)  

**Abstract**: Anomaly detection and classification in medical imaging are critical for early diagnosis but remain challenging due to limited annotated data, class imbalance, and the high cost of expert labeling. Emerging vision foundation models such as DINOv2, pretrained on extensive, unlabeled datasets, offer generalized representations that can potentially alleviate these limitations. In this study, we propose an attention-based global aggregation framework tailored specifically for 3D medical image anomaly classification. Leveraging the self-supervised DINOv2 model as a pretrained feature extractor, our method processes individual 2D axial slices of brain MRIs, assigning adaptive slice-level importance weights through a soft attention mechanism. To further address data scarcity, we employ a composite loss function combining supervised contrastive learning with class-variance regularization, enhancing inter-class separability and intra-class consistency. We validate our framework on the ADNI dataset and an institutional multi-class headache cohort, demonstrating strong anomaly classification performance despite limited data availability and significant class imbalance. Our results highlight the efficacy of utilizing pretrained 2D foundation models combined with attention-based slice aggregation for robust volumetric anomaly detection in medical imaging. Our implementation is publicly available at this https URL. 

**Abstract (ZH)**: 医学影像中的异常检测与分类对于早期诊断至关重要，但由于标注数据有限、类别不平衡以及专家标注的高成本，这一任务依然具有挑战性。新兴的视觉基础模型，如DINOv2，预先在大量未标注数据集上进行训练，提供了通用的表示，有可能缓解这些局限性。在本研究中，我们提出了一种特定于3D医学影像异常分类的基于注意力的全局聚合框架。利用自监督预训练的DINOv2模型作为特征提取器，我们的方法处理脑部MRI的个体2D轴向切片，并通过软注意力机制为每个切片分配自适应的重要性权重。为进一步解决数据稀缺性，我们采用了一种综合损失函数，结合监督对比学习与类别方差正则化，增强类别间可分性和类别内一致性。我们在ADNI数据集和机构多类别头痛队列上验证了我们的框架，即使在数据有限和类别严重不平衡的情况下，也取得了强大的异常分类性能。我们的结果突显了利用预先训练的2D基础模型结合基于注意力的切片聚合方法，在医学影像中实现稳健的体素级异常检测的有效性。我们的实现可在以下链接公开获取：this https URL。 

---
# GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions 

**Title (ZH)**: GraphDerm: 结合图像、物理尺度和元数据的肤疮图谱分类器 

**Authors**: Mehdi Yousefzadeh, Parsa Esfahanian, Sara Rashidifar, Hossein Salahshoor Gavalan, Negar Sadat Rafiee Tabatabaee, Saeid Gorgin, Dara Rahmati, Maryam Daneshpazhooh  

**Link**: [PDF](https://arxiv.org/pdf/2509.12277)  

**Abstract**: Introduction. Dermoscopy aids melanoma triage, yet image-only AI often ignores patient metadata (age, sex, site) and the physical scale needed for geometric analysis. We present GraphDerm, a population-graph framework that fuses imaging, millimeter-scale calibration, and metadata for multiclass dermoscopic classification, to the best of our knowledge the first ISIC-scale application of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019, synthesize ruler-embedded images with exact masks, and train U-Nets (SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter are regressed from the ruler-mask two-point correlation via a lightweight 1D-CNN. From lesion masks we compute real-scale descriptors (area, perimeter, radius of gyration). Node features use EfficientNet-B3; edges encode metadata/geometry similarity (fully weighted or thresholded). A spectral GNN performs semi-supervised node classification; an image-only ANN is the baseline. Results. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scale regression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with a thresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440 for the image-only baseline); per-class AUCs typically fall in the 0.97-0.99 range. Conclusion. Unifying calibrated scale, lesion geometry, and metadata in a population graph yields substantial gains over image-only pipelines on ISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficient deployment. Scale-aware, graph-based AI is a promising direction for dermoscopic decision support; future work will refine learned edge semantics and evaluate on broader curated benchmarks. 

**Abstract (ZH)**: 介绍。 дерmoscopic 有助于黑色素瘤初筛，但仅基于图像的 AI 往往忽视患者元数据（年龄、性别、部位）以及所需进行几何分析的物理尺度。我们提出了一种融合成像、毫米级校准和元数据的群体图框架 GraphDerm，据我们所知，这是首次将图神经网络应用于图像数据库 ISIC 的 dermoscopic 多类别分类。方法。我们整理了 ISIC 2018/2019 数据集，合成了带标尺的图像及其精确掩码，并训练了 U-Nets（SE-ResNet-18）进行病灶和标尺分割。通过轻量级的一维卷积神经网络从标尺掩码的两点相关性回归像素/毫米。从病灶掩码计算实际规模描述符（面积、周长、旋转半径）。节点特征使用 EfficientNet-B3；边编码元数据/几何相似性（完全加权或阈值化）。光谱图神经网络执行半监督节点分类；仅基于图像的 ANN 是基线。结果。标尺和病灶分割的 Dice 系数分别为 0.904 和 0.908；尺度回归的平均绝对误差为 1.5 像素（均方根误差为 6.6）。群体图的 AUC 为 0.9812，在使用约 25% 边阈值化版本的 AUC 为 0.9788（与仅基于图像的基线相比 AUC 为 0.9440）；各分类的 AUC 通常在 0.97-0.99 范围内。结论。在 ISIC-2019 上，将校准尺度、病灶几何形状和元数据统一在群体图中，相对于仅基于图像的管道产生了显著改进。稀疏图保留了近乎最优的准确性，表明其在高效部署方面具有潜力。具备尺度感知的图基 AI 是 dermoscopic 决策支持的一个有前途的方向；未来工作将精炼学习到的边语义并评估更广泛的整理标记基准。 

---
# A Modern Look at Simplicity Bias in Image Classification Tasks 

**Title (ZH)**: 现代视角下图像分类任务中的简约偏见研究 

**Authors**: Xiaoguang Chang, Teng Wang, Changyin Sun  

**Link**: [PDF](https://arxiv.org/pdf/2509.12265)  

**Abstract**: The simplicity Bias (SB) of neural networks, i.e.\ their tendency to represent simple functions, is a key factor in their generalization capabilities. Recent studies show that an excessive SB may harm performance on complex tasks, and the need for this bias varies across tasks. Many of these studies focus on simple models or synthetic tasks. It remains challenging to measure the SB in large models and little is known about the relevance of the SB to various image classification tasks.
In this paper, we investigate the relationship between the SB in CLIP models and their performance across image classification tasks. First, we theoretically analyze the potential limitation of existing measures of complexity that have been used to characterize small models. To address this, we propose a frequency-aware measure capturing finer-grained SB differences. We validate this measure on CLIP models subjected to two recent SB-modulation methods, demonstrating that it is more informative and consistent than previous measures. Second, we examine the relation between the SB of those models and their performance across a range of image classification tasks, including zero-shot and fine-tuning settings. These experiments reveal a range of behaviors. For example, a stronger SB correlates with a better performance on OOD generalization than on adversarial robustness. These results highlight the benefits of aligning a model's inductive biases with the characteristics of the target task. 

**Abstract (ZH)**: CLIP模型中简化偏见与图像分类任务性能之间的关系探讨 

---
# RU-Net for Automatic Characterization of TRISO Fuel Cross Sections 

**Title (ZH)**: RU-Net用于TRISO燃料截面的自动表征 

**Authors**: Lu Cai, Fei Xu, Min Xian, Yalei Tang, Shoukun Sun, John Stempien  

**Link**: [PDF](https://arxiv.org/pdf/2509.12244)  

**Abstract**: During irradiation, phenomena such as kernel swelling and buffer densification may impact the performance of tristructural isotropic (TRISO) particle fuel. Post-irradiation microscopy is often used to identify these irradiation-induced morphologic changes. However, each fuel compact generally contains thousands of TRISO particles. Manually performing the work to get statistical information on these phenomena is cumbersome and subjective. To reduce the subjectivity inherent in that process and to accelerate data analysis, we used convolutional neural networks (CNNs) to automatically segment cross-sectional images of microscopic TRISO layers. CNNs are a class of machine-learning algorithms specifically designed for processing structured grid data. They have gained popularity in recent years due to their remarkable performance in various computer vision tasks, including image classification, object detection, and image segmentation. In this research, we generated a large irradiated TRISO layer dataset with more than 2,000 microscopic images of cross-sectional TRISO particles and the corresponding annotated images. Based on these annotated images, we used different CNNs to automatically segment different TRISO layers. These CNNs include RU-Net (developed in this study), as well as three existing architectures: U-Net, Residual Network (ResNet), and Attention U-Net. The preliminary results show that the model based on RU-Net performs best in terms of Intersection over Union (IoU). Using CNN models, we can expedite the analysis of TRISO particle cross sections, significantly reducing the manual labor involved and improving the objectivity of the segmentation results. 

**Abstract (ZH)**: 辐照期间， KERNEL肿胀和缓冲层致密化等现象可能影响三向各向同性（TRISO）颗粒燃料的性能。使用辐照后的显微镜检查通常用于识别这些辐照引起的形态变化。然而，每个燃料元件通常包含数千个TRISO颗粒。手动获取这些现象的统计信息工作繁琐且主观。为了减少这一过程中的主观性并加速数据分析，我们使用卷积神经网络（CNNs）自动分割显微TRISO层的横截面图像。CNNs是一类专门用于处理结构化网格数据的机器学习算法。由于它们在图像分类、物体检测和图像分割等各项计算机视觉任务中表现出色，近年来变得非常流行。在本研究中，我们生成了一个包含超过2,000张显微TRISO颗粒横截面图像及其标注图像的大型辐照TRISO层数据集。基于这些标注图像，我们使用不同的CNNs自动分割不同的TRISO层。这些CNNs包括RU-Net（本研究中开发）、以及三种现有架构：U-Net、残差网络（ResNet）和注意力U-Net。初步结果显示，基于RU-Net的模型在交并比（IoU）方面表现最佳。利用CNN模型，我们可以加速TRISO颗粒横截面的分析，显著减少手动劳动并提高分割结果的客观性。 

---
