{'arxiv_id': 'arXiv:2508.10872', 'title': 'TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning', 'authors': 'Anantha Narayanan, Battu Bhanu Teja, Pruthwik Mishra', 'link': 'https://arxiv.org/abs/2508.10872', 'abstract': "The increasing congestion of Low Earth Orbit (LEO) poses persistent challenges to the efficient deployment and safe operation of Earth observation satellites. Mission planners must now account not only for mission-specific requirements but also for the increasing collision risk with active satellites and space debris. This work presents a reinforcement learning framework using the Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital parameters for precise terrestrial coverage within predefined surface radii. By formulating the problem as a Markov Decision Process (MDP) within a custom OpenAI Gymnasium environment, our method simulates orbital dynamics using classical Keplerian elements. The agent progressively learns to adjust five of the orbital parameters - semi-major axis, eccentricity, inclination, right ascension of ascending node, and the argument of perigee-to achieve targeted terrestrial coverage. Comparative evaluation against Proximal Policy Optimization (PPO) demonstrates A2C's superior performance, achieving 5.8x higher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer timesteps (2,000 vs 63,000). The A2C agent consistently meets mission objectives across diverse target coordinates while maintaining computational efficiency suitable for real-time mission planning applications. Key contributions include: (1) a TLE-based orbital simulation environment incorporating physics constraints, (2) validation of actor-critic methods' superiority over trust region approaches in continuous orbital control, and (3) demonstration of rapid convergence enabling adaptive satellite deployment. This approach establishes reinforcement learning as a computationally efficient alternative for scalable and intelligent LEO mission planning.", 'abstract_zh': '低地球轨道不断增加的拥堵对地球观测卫星的有效部署和安全运行构成了持续挑战。任务规划者现在不仅要考虑特定任务需求，还要考虑与活跃卫星和太空碎片不断增加的碰撞风险。本文提出了一种使用优势actor-评论者（A2C）算法的强化学习框架，以优化卫星轨道参数，实现预定义地面半径内的精确地面覆盖。通过在自定义的OpenAI Gymnasium环境中将问题形式化为马尔可夫决策过程（MDP），我们的方法使用经典开普勒元素模拟轨道动力学。代理逐渐学习调整轨道参数中的五个参数——半长轴、偏心率、倾角、升交点赤经和近地点幅角，以实现目标地面覆盖。与近似策略优化（PPO）的比较评估展示了A2C优于PPO的性能，累积奖励为10.0（比PPO的9.263025高5.8倍），且在时间步长上更快收敛（2,000步 vs 63,000步，快31.5倍）。A2C代理能够在多种目标坐标下一致实现任务目标，同时保持适用于实时任务规划应用的高效计算性。关键贡献包括：（1）基于TLE的轨道模拟环境，包含物理约束；（2）验证了actor-评论者方法在连续轨道控制中优于信赖区域方法的优势；（3）展示了快速收敛能力，使得卫星部署更加适应变化。此方法确立了强化学习作为高效替代方案，在可扩展和智能低地球轨道任务规划中的地位。', 'title_zh': '基于TLE的A2C代理在地表覆盖轨道路径规划中的应用'}
{'arxiv_id': 'arXiv:2508.10867', 'title': 'CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups', 'authors': 'Yizhi Zhou, Ziwei Kang, Jiawei Xia, Xuan Wang', 'link': 'https://arxiv.org/abs/2508.10867', 'abstract': "Ultra Wideband (UWB) is widely used to mitigate drift in visual-inertial odometry (VIO) systems. Consistency is crucial for ensuring the estimation accuracy of a UWBaided VIO system. An inconsistent estimator can degrade localization performance, where the inconsistency primarily arises from two main factors: (1) the estimator fails to preserve the correct system observability, and (2) UWB anchor positions are assumed to be known, leading to improper neglect of calibration uncertainty. In this paper, we propose a consistent and tightly-coupled visual-inertial-ranging odometry (CVIRO) system based on the Lie group. Our method incorporates the UWB anchor state into the system state, explicitly accounting for UWB calibration uncertainty and enabling the joint and consistent estimation of both robot and anchor states. Furthermore, observability consistency is ensured by leveraging the invariant error properties of the Lie group. We analytically prove that the CVIRO algorithm naturally maintains the system's correct unobservable subspace, thereby preserving estimation consistency. Extensive simulations and experiments demonstrate that CVIRO achieves superior localization accuracy and consistency compared to existing methods.", 'abstract_zh': '基于李群的一致且紧密耦合的视觉-惯性-测距定位系统（C Vishion-Inertial-Ranging Odometry, CVIRO）', 'title_zh': 'CVIRO：基于李群的一致且紧密耦合的视觉-惯性-测距 одometry'}
{'arxiv_id': 'arXiv:2508.10828', 'title': 'A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots', 'authors': 'Henry Powell, Guy Laban, Emily S. Cross', 'link': 'https://arxiv.org/abs/2508.10828', 'abstract': "Subjective self-disclosure is an important feature of human social interaction. While much has been done in the social and behavioural literature to characterise the features and consequences of subjective self-disclosure, little work has been done thus far to develop computational systems that are able to accurately model it. Even less work has been done that attempts to model specifically how human interactants self-disclose with robotic partners. It is becoming more pressing as we require social robots to work in conjunction with and establish relationships with humans in various social settings. In this paper, our aim is to develop a custom multimodal attention network based on models from the emotion recognition literature, training this model on a large self-collected self-disclosure video corpus, and constructing a new loss function, the scale preserving cross entropy loss, that improves upon both classification and regression versions of this problem. Our results show that the best performing model, trained with our novel loss function, achieves an F1 score of 0.83, an improvement of 0.48 from the best baseline model. This result makes significant headway in the aim of allowing social robots to pick up on an interaction partner's self-disclosures, an ability that will be essential in social robots with social cognition.", 'abstract_zh': '主观自我披露是人类社会互动的一个重要特征。尽管社交和行为文献中已经对主观自我披露的特征及其后果进行了大量研究，但迄今尚缺乏能准确建模其行为的计算系统。在这方面，尤其是在尝试建模人类与机器人 partners 互动中自我披露的具体方式方面，研究工作更为不足。随着我们要求社会机器人在各种社会环境中与人类共同工作并建立关系，这一需求变得愈发迫切。在本文中，我们的目标是基于情绪识别领域的模型开发一种定制的多模态注意力网络，在一个大规模自收集的自我披露视频语料库上对该模型进行训练，并构建了一种新的损失函数——尺度保持交叉熵损失，该函数在分类和回归问题的解决上均有所改进。我们的结果表明，使用我们新型损失函数训练的最佳模型，在F1分数上达到0.83，相比最佳基线模型提高了0.48。这一结果在使社会机器人能够识别互动伙伴的自我披露方面取得了重要进展，这种能力将对具有社会认知的社会机器人至关重要。', 'title_zh': '面向社会机器人的多模态神经网络主观自我披露识别'}
{'arxiv_id': 'arXiv:2508.10798', 'title': 'The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems', 'authors': 'Troi Williams', 'link': 'https://arxiv.org/abs/2508.10798', 'abstract': 'Future autonomous systems promise significant societal benefits, yet their deployment raises concerns about safety and trustworthiness. A key concern is assuring the reliability of robot perception, as perception seeds safe decision-making. Failures in perception are often due to complex yet common environmental factors and can lead to accidents that erode public trust. To address this concern, we introduce the SET (Self, Environment, and Target) Perceptual Factors Framework. We designed the framework to systematically analyze how factors such as weather, occlusion, or sensor limitations negatively impact perception. To achieve this, the framework employs SET State Trees to categorize where such factors originate and SET Factor Trees to model how these sources and factors impact perceptual tasks like object detection or pose estimation. Next, we develop Perceptual Factor Models using both trees to quantify the uncertainty for a given task. Our framework aims to promote rigorous safety assurances and cultivate greater public understanding and trust in autonomous systems by offering a transparent and standardized method for identifying, modeling, and communicating perceptual risks.', 'abstract_zh': '未来自主系统有望为社会带来重大益处，但其部署引发了关于安全性和可靠性的关切。一个关键问题是确保机器人感知的可靠性，因为感知是安全决策的基础。感知中的失败往往由复杂的但常见的环境因素引起，可能导致事故并侵蚀公众的信任。为应对这一挑战，我们提出了Self、Environment和Target感知因素框架（SET Perceptual Factors Framework）。该框架旨在系统性地分析天气、遮挡或传感器限制等因素如何负面影响感知。为此，框架使用SET状态树来分类这些因素的起源，并使用SET因子树来建模这些源和因素如何影响诸如物体检测或姿态估计等感知任务。接着，我们使用这两种树来开发感知因素模型，以量化特定任务的不确定性。该框架旨在通过提供透明和标准化的方法来识别、建模和沟通感知风险，促进对自主系统的严格安全保证和更大的公众理解和信任。', 'title_zh': 'SET知觉因素框架：迈向自主系统可信赖的知觉'}
{'arxiv_id': 'arXiv:2508.10780', 'title': 'Learning Task Execution Hierarchies for Redundant Robots', 'authors': 'Alessandro Adami, Aris Synodinos, Matteo Iovino, Ruggero Carli, Pietro Falco', 'link': 'https://arxiv.org/abs/2508.10780', 'abstract': 'Modern robotic systems, such as mobile manipulators, humanoids, and aerial robots with arms, often possess high redundancy, enabling them to perform multiple tasks simultaneously. Managing this redundancy is key to achieving reliable and flexible behavior. A widely used approach is the Stack of Tasks (SoT), which organizes control objectives by priority within a unified framework. However, traditional SoTs are manually designed by experts, limiting their adaptability and accessibility. This paper introduces a novel framework that automatically learns both the hierarchy and parameters of a SoT from user-defined objectives. By combining Reinforcement Learning and Genetic Programming, the system discovers task priorities and control strategies without manual intervention. A cost function based on intuitive metrics such as precision, safety, and execution time guides the learning process. We validate our method through simulations and experiments on the mobile-YuMi platform, a dual-arm mobile manipulator with high redundancy. Results show that the learned SoTs enable the robot to dynamically adapt to changing environments and inputs, balancing competing objectives while maintaining robust task execution. This approach provides a general and user-friendly solution for redundancy management in complex robots, advancing human-centered robot programming and reducing the need for expert design.', 'abstract_zh': '现代机器人系统，如移动 manipulator、类人机器人和具有手臂的无人机，通常拥有高冗余度，使它们能够同时执行多种任务。管理这种冗余度对于实现可靠和灵活的行为至关重要。一种常用的办法是任务堆栈（SoT），它在一个统一框架中通过优先级组织控制目标。然而，传统 SoT 通常是专家手工设计的，限制了其适应性和易用性。本文介绍了一种新颖的框架，可以从用户定义的目标中自动学习 SoT 的层次结构和参数。通过结合强化学习和遗传编程，该系统在无手动干预的情况下发现了任务优先级和控制策略。基于直观的度量标准（如精度、安全性和执行时间）的代价函数指导学习过程。我们通过在移动-YuMi 平台上进行的仿真实验验证了该方法，移动-YuMi 是一个具有高冗余度的双臂移动 manipulator。结果表明，学习到的 SoT 使机器人能够动态适应变化的环境和输入，同时在平衡竞争目标的同时保持任务执行的鲁棒性。这种方法为复杂机器人中的冗余度管理提供了一种通用且用户友好的解决方案，促进了以人为核心的机器人编程，并减少了对专家设计的依赖。', 'title_zh': '冗余机器人学习任务执行层次结构'}
{'arxiv_id': 'arXiv:2508.10689', 'title': 'Biasing Frontier-Based Exploration with Saliency Areas', 'authors': 'Matteo Luperto, Valerii Stakanov, Giacomo Boracchi, Nicola Basilico, Francesco Amigoni', 'link': 'https://arxiv.org/abs/2508.10689', 'abstract': 'Autonomous exploration is a widely studied problem where a robot incrementally builds a map of a previously unknown environment. The robot selects the next locations to reach using an exploration strategy. To do so, the robot has to balance between competing objectives, like exploring the entirety of the environment, while being as fast as possible. Most exploration strategies try to maximise the explored area to speed up exploration; however, they do not consider that parts of the environment are more important than others, as they lead to the discovery of large unknown areas. We propose a method that identifies \\emph{saliency areas} as those areas that are of high interest for exploration, by using saliency maps obtained from a neural network that, given the current map, implements a termination criterion to estimate whether the environment can be considered fully-explored or not. We use saliency areas to bias some widely used exploration strategies, showing, with an extensive experimental campaign, that this knowledge can significantly influence the behavior of the robot during exploration.', 'abstract_zh': '自主探索是广泛研究的一个问题，其中机器人通过增量构建以前未知环境的地图。机器人使用探索策略来选择下一个要访问的位置。为此，机器人需要在诸如全面探索环境与尽可能快速之间平衡竞争目标。大多数探索策略试图最大化探索的区域以加快探索过程；然而，它们没有考虑环境中某些部分比其他部分更重要，因为这些部分会发现大面积未知区域。我们提出了一种方法，通过使用神经网络从当前地图生成的可引起终止条件的显著性图来识别显著性区域，这些区域对探索具有高兴趣。我们利用显著性区域偏置一些广泛使用的探索策略，并通过广泛的经验实验表明，这种知识可以显著影响机器人在探索过程中的行为。', 'title_zh': '基于显著区域引导前沿探索'}
{'arxiv_id': 'arXiv:2508.10686', 'title': 'An Open-Source User-Friendly Interface for Simulating Magnetic Soft Robots using Simulation Open Framework Architecture (SOFA)', 'authors': 'Carla Wehner, Finn Schubert, Heiko Hellkamp, Julius Hahnewald, Kilian Scheafer, Muhammad Bilal Khan, Oliver Gutfleisch', 'link': 'https://arxiv.org/abs/2508.10686', 'abstract': "Soft robots, particularly magnetic soft robots, require specialized simulation tools to accurately model their deformation under external magnetic fields. However, existing platforms often lack dedicated support for magnetic materials, making them difficult to use for researchers at different expertise levels. This work introduces an open-source, user-friendly simulation interface using the Simulation Open Framework Architecture (SOFA), specifically designed to model magnetic soft robots. The tool enables users to define material properties, apply magnetic fields, and observe resulting deformations in real time. By integrating intuitive controls and stress analysis capabilities, it aims to bridge the gap between theoretical modeling and practical design. Four benchmark models - a beam, three- and four-finger grippers, and a butterfly - demonstrate its functionality. The software's ease of use makes it accessible to both beginners and advanced researchers. Future improvements will refine accuracy through experimental validation and comparison with industry-standard finite element solvers, ensuring realistic and predictive simulations of magnetic soft robots.", 'abstract_zh': '软体机器人，尤其是磁性软体机器人，需要专门的仿真工具来准确模拟其在外加磁场下的变形。现有平台往往缺乏专门支持磁性材料的功能，使得研究人员难以使用。本文介绍了使用Simulation Open Framework Architecture (SOFA)开发的一种开源、用户友好的仿真接口，专门用于模拟磁性软体机器人。该工具允许用户定义材料属性、应用磁场并实时观察变形结果。通过集成直观的控制和应力分析能力，它旨在缩小理论建模与实际设计之间的差距。四个基准模型（一根梁、三指和四指夹爪以及一只蝴蝶）演示了其功能。该软件易于使用，使其对初学者和高级研究人员都具有可访问性。未来改进将通过实验验证和与工业标准有限元求解器的比较来提高准确性，以确保对磁性软体机器人的真实性和预测性仿真。', 'title_zh': '使用Simulation Open Framework Architecture (SOFA)模拟磁软机器人的开源用户友好界面'}
{'arxiv_id': 'arXiv:2508.10634', 'title': 'Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots', 'authors': 'Mehdi Heydari Shahna, Jouni Mattila', 'link': 'https://arxiv.org/abs/2508.10634', 'abstract': 'Deep neural networks (DNNs) can enable precise control while maintaining low computational costs by circumventing the need for dynamic modeling. However, the deployment of such black-box approaches remains challenging for heavy-duty wheeled mobile robots (WMRs), which are subject to strict international standards and prone to faults and disturbances. We designed a hierarchical control policy for heavy-duty WMRs, monitored by two safety layers with differing levels of authority. To this end, a DNN policy was trained and deployed as the primary control strategy, providing high-precision performance under nominal operating conditions. When external disturbances arise and reach a level of intensity such that the system performance falls below a predefined threshold, a low-level safety layer intervenes by deactivating the primary control policy and activating a model-free robust adaptive control (RAC) policy. This transition enables the system to continue operating while ensuring stability by effectively managing the inherent trade-off between system robustness and responsiveness. Regardless of the control policy in use, a high-level safety layer continuously monitors system performance during operation. It initiates a shutdown only when disturbances become sufficiently severe such that compensation is no longer viable and continued operation would jeopardize the system or its environment. The proposed synthesis of DNN and RAC policy guarantees uniform exponential stability of the entire WMR system while adhering to safety standards to some extent. The effectiveness of the proposed approach was further validated through real-time experiments using a 6,000 kg WMR.', 'abstract_zh': '深度神经网络在重载地面移动机器人中的 hierarchical 控制策略：集成模型自由鲁棒自适应控制的安全合成', 'title_zh': '基于安全鲁棒自适应控制的轮式移动机器人深神经网络合成方法'}
{'arxiv_id': 'arXiv:2508.10603', 'title': 'Why Report Failed Interactions With Robots?! Towards Vignette-based Interaction Quality', 'authors': 'Agnes Axelsson, Merle Reimann, Ronald Cumbal, Hannah Pelikan, Divesh Lala', 'link': 'https://arxiv.org/abs/2508.10603', 'abstract': 'Although the quality of human-robot interactions has improved with the advent of LLMs, there are still various factors that cause systems to be sub-optimal when compared to human-human interactions. The nature and criticality of failures are often dependent on the context of the interaction and so cannot be generalized across the wide range of scenarios and experiments which have been implemented in HRI research. In this work we propose the use of a technique overlooked in the field of HRI, ethnographic vignettes, to clearly highlight these failures, particularly those that are rarely documented. We describe the methodology behind the process of writing vignettes and create our own based on our personal experiences with failures in HRI systems. We emphasize the strength of vignettes as the ability to communicate failures from a multi-disciplinary perspective, promote transparency about the capabilities of robots, and document unexpected behaviours which would otherwise be omitted from research reports. We encourage the use of vignettes to augment existing interaction evaluation methods.', 'abstract_zh': '尽管大型语言模型的出现提高了人机交互的质量，但与人类之间的交互相比，系统仍存在各种因素导致其不够优化。交互的性质和失败的严重性通常依赖于交互的具体情境，因此难以在涵盖广泛场景和实验的HRI研究中进行通用化。本文提出在HRI领域忽视的一种技术——民族志案例研究，以清晰地展示这些失败，特别是那些很少被记录的失败。我们描述了撰写案例研究的方法，并基于自身在HRI系统中遇到的失败经验编写了自己的案例研究。我们强调案例研究的优势，包括从多学科视角沟通失败、促进关于机器人能力的透明度以及记录本应被研究报告忽略的意外行为。我们鼓励使用案例研究来增强现有的交互评估方法。', 'title_zh': '为什么对话失败了？基于情境的交互质量探索。'}
{'arxiv_id': 'arXiv:2508.10538', 'title': 'MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm', 'authors': 'Xin Liu, Bida Ma, Chenkun Qi, Yan Ding, Zhaxizhuoma, Guorong Zhang, Pengan Chen, Kehui Liu, Zhongjie Jia, Chuyue Guan, Yule Mo, Jiaqi Liu, Feng Gao, Jiangwei Zhong, Bin Zhao, Xuelong Li', 'link': 'https://arxiv.org/abs/2508.10538', 'abstract': "Whole-body loco-manipulation for quadruped robots with arm remains a challenging problem, particularly in achieving multi-task control. To address this, we propose MLM, a reinforcement learning framework driven by both real-world and simulation data. It enables a six-DoF robotic arm--equipped quadruped robot to perform whole-body loco-manipulation for multiple tasks autonomously or under human teleoperation. To address the problem of balancing multiple tasks during the learning of loco-manipulation, we introduce a trajectory library with an adaptive, curriculum-based sampling mechanism. This approach allows the policy to efficiently leverage real-world collected trajectories for learning multi-task loco-manipulation. To address deployment scenarios with only historical observations and to enhance the performance of policy execution across tasks with different spatial ranges, we propose a Trajectory-Velocity Prediction policy network. It predicts unobservable future trajectories and velocities. By leveraging extensive simulation data and curriculum-based rewards, our controller achieves whole-body behaviors in simulation and zero-shot transfer to real-world deployment. Ablation studies in simulation verify the necessity and effectiveness of our approach, while real-world experiments on the Go2 robot with an Airbot robotic arm demonstrate the policy's good performance in multi-task execution.", 'abstract_zh': '四足机器人配备手臂的全身 locomotion-manipulation 与多任务控制：基于现实与仿真数据的强化学习框架 MLM', 'title_zh': 'MLM：学习四足机器人带臂多任务位姿操作全身控制'}
{'arxiv_id': 'arXiv:2508.10511', 'title': 'KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection', 'authors': 'Andrea Rosasco, Federico Ceola, Giulia Pasquale, Lorenzo Natale', 'link': 'https://arxiv.org/abs/2508.10511', 'abstract': 'Learning robot policies that capture multimodality in the training data has been a long-standing open challenge for behavior cloning. Recent approaches tackle the problem by modeling the conditional action distribution with generative models. One of these approaches is Diffusion Policy, which relies on a diffusion model to denoise random points into robot action trajectories. While achieving state-of-the-art performance, it has two main drawbacks that may lead the robot out of the data distribution during policy execution. First, the stochasticity of the denoising process can highly impact on the quality of generated trajectory of actions. Second, being a supervised learning approach, it can learn data outliers from the dataset used for training. Recent work focuses on mitigating these limitations by combining Diffusion Policy either with large-scale training or with classical behavior cloning algorithms. Instead, we propose KDPE, a Kernel Density Estimation-based strategy that filters out potentially harmful trajectories output of Diffusion Policy while keeping a low test-time computational overhead. For Kernel Density Estimation, we propose a manifold-aware kernel to model a probability density function for actions composed of end-effector Cartesian position, orientation, and gripper state. KDPE overall achieves better performance than Diffusion Policy on simulated single-arm tasks and real robot experiments.\nAdditional material and code are available on our project page this https URL.', 'abstract_zh': '基于核密度估计的策略：Diffusion Policy的轨迹过滤方法', 'title_zh': 'KDPE: 一种核密度估计策略用于扩散政策轨迹选择'}
{'arxiv_id': 'arXiv:2508.10497', 'title': 'Enabling Generic Robot Skill Implementation Using Object Oriented Programming', 'authors': 'Abdullah Farrukh, Achim Wagner, Martin Ruskowski', 'link': 'https://arxiv.org/abs/2508.10497', 'abstract': 'Developing robotic algorithms and integrating a robotic subsystem into a larger system can be a difficult task. Particularly in small and medium-sized enterprises (SMEs) where robotics expertise is lacking, implementing, maintaining and developing robotic systems can be a challenge. As a result, many companies rely on external expertise through system integrators, which, in some cases, can lead to vendor lock-in and external dependency. In the academic research on intelligent manufacturing systems, robots play a critical role in the design of robust autonomous systems. Similar challenges are faced by researchers who want to use robotic systems as a component in a larger smart system, without having to deal with the complexity and vastness of the robot interfaces in detail. In this paper, we propose a software framework that reduces the effort required to deploy a working robotic system. The focus is solely on providing a concept for simplifying the different interfaces of a modern robot system and using an abstraction layer for different manufacturers and models. The Python programming language is used to implement a prototype of the concept. The target system is a bin-picking cell containing a Yaskawa Motoman GP4.', 'abstract_zh': '开发机器人算法并将机器人子系统集成到更大系统中可能是一项困难的任务。特别是在缺乏机器人专业知识的小型和中型企业（SMEs）中，实现、维护和发展机器人系统会面临挑战。因此，许多公司依赖系统集成商的外部专业知识，这在某些情况下会导致供应商锁定和对外部的依赖。在智能制造系统的学术研究中，机器人在设计健壮的自主系统中起着关键作用。研究人员希望将机器人系统作为更大智能系统中的组件使用时也面临类似挑战，无需详细处理机器人的复杂接口。本文提出了一种软件框架，以减少部署工作机器人系统的努力。该框架专注于简化现代机器人系统中不同的接口，并使用不同的制造商和型号的抽象层。使用Python编程语言实现了该概念的原型。目标系统是一个包含Yaskawa Motoman GP4的料箱取放单元。', 'title_zh': '使用面向对象编程实现通用机器人技能实施'}
{'arxiv_id': 'arXiv:2508.10423', 'title': 'MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion', 'authors': 'Qi Liu, Xiaopeng Zhang, Mingshan Tan, Shuaikang Ma, Jinliang Ding, Yanjie Li', 'link': 'https://arxiv.org/abs/2508.10423', 'abstract': "This paper proposes a novel method to enhance locomotion for a single humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement learning (MARL). While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or MARL algorithms for multi-robot system tasks, we propose a distinct paradigm: applying cooperative-heterogeneous MARL to optimize locomotion for a single humanoid robot. The proposed method, multi-agent reinforcement learning for single humanoid locomotion (MASH), treats each limb (legs and arms) as an independent agent that explores the robot's action space while sharing a global critic for cooperative learning. Experiments demonstrate that MASH accelerates training convergence and improves whole-body cooperation ability, outperforming conventional single-agent reinforcement learning methods. This work advances the integration of MARL into single-humanoid-robot control, offering new insights into efficient locomotion strategies.", 'abstract_zh': '一种基于合作异构多智能体深度强化学习的单人形机器人运动增强方法', 'title_zh': 'MASH: 合作异构多智能体强化学习在单个类人机器人运动中的应用'}
{'arxiv_id': 'arXiv:2508.10416', 'title': 'CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model', 'authors': 'Zhuoyuan Yu, Yuxing Long, Zihan Yang, Chengyan Zeng, Hongwei Fan, Jiyao Zhang, Hao Dong', 'link': 'https://arxiv.org/abs/2508.10416', 'abstract': "Existing vision-and-language navigation models often deviate from the correct trajectory when executing instructions. However, these models lack effective error correction capability, hindering their recovery from errors. To address this challenge, we propose Self-correction Flywheel, a novel post-training paradigm. Instead of considering the model's error trajectories on the training set as a drawback, our paradigm emphasizes their significance as a valuable data source. We have developed a method to identify deviations in these error trajectories and devised innovative techniques to automatically generate self-correction data for perception and action. These self-correction data serve as fuel to power the model's continued training. The brilliance of our paradigm is revealed when we re-evaluate the model on the training set, uncovering new error trajectories. At this time, the self-correction flywheel begins to spin. Through multiple flywheel iterations, we progressively enhance our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2% and 16.4%. Real robot tests in various indoor and outdoor environments demonstrate \\method's superior capability of error correction, dynamic obstacle avoidance, and long instruction following.", 'abstract_zh': '现有的视觉-语言导航模型在执行指令时往往会偏离正确的轨迹。然而，这些模型缺乏有效的错误修正能力，阻碍了它们从错误中恢复。为应对这一挑战，我们提出了自修正飞轮（Self-correction Flywheel），这是一种新颖的后训练范式。我们不将模型在训练集上的错误轨迹视为缺点，而是强调它们作为宝贵数据源的重要性。我们开发了一种方法来识别这些错误轨迹中的偏差，并设计了自动生成感知和行动自修正数据的技术。这些自修正数据充当了驱动模型持续训练的动力。当我们在训练集上重新评估模型时，揭示出新的错误轨迹，自修正飞轮开始运转。通过多次飞轮迭代，我们逐步提升了基于单目RGB的VLA导航模型CorrectNav。在R2R-CE和RxR-CE基准测试中的实验表明，CorrectNav实现了新的最佳成功率，分别为65.1%和69.3%，分别优于之前最好的VLA导航模型8.2%和16.4%。在多种室内外环境中的真实机器人测试中，该方法展示了其优越的错误修正、动态障碍物规避和长时间指令遵循能力。', 'title_zh': 'CorrectNav: 自校正飞轮赋能视觉-语言-动作导航模型'}
{'arxiv_id': 'arXiv:2508.10399', 'title': 'Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning', 'authors': 'Wenlong Liang, Rui Zhou, Yang Ma, Bing Zhang, Songlin Li, Yijia Liao, Ping Kuang', 'link': 'https://arxiv.org/abs/2508.10399', 'abstract': 'Embodied AI aims to develop intelligent systems with physical forms capable of perceiving, decision-making, acting, and learning in real-world environments, providing a promising way to Artificial General Intelligence (AGI). Despite decades of explorations, it remains challenging for embodied agents to achieve human-level intelligence for general-purpose tasks in open dynamic environments. Recent breakthroughs in large models have revolutionized embodied AI by enhancing perception, interaction, planning and learning. In this article, we provide a comprehensive survey on large model empowered embodied AI, focusing on autonomous decision-making and embodied learning. We investigate both hierarchical and end-to-end decision-making paradigms, detailing how large models enhance high-level planning, low-level execution, and feedback for hierarchical decision-making, and how large models enhance Vision-Language-Action (VLA) models for end-to-end decision making. For embodied learning, we introduce mainstream learning methodologies, elaborating on how large models enhance imitation learning and reinforcement learning in-depth. For the first time, we integrate world models into the survey of embodied AI, presenting their design methods and critical roles in enhancing decision-making and learning. Though solid advances have been achieved, challenges still exist, which are discussed at the end of this survey, potentially as the further research directions.', 'abstract_zh': '具身AI旨在开发具备感知、决策、行动和学习能力的物理形态智能系统，为人工通用智能（AGI）提供了有希望的方式。尽管经过了数十年的探索，具身智能体在开放动态环境中实现通用任务的人类水平智能仍具有挑战性。近年来，大规模模型的突破性进展通过增强感知、交互、规划和学习，革命性地推动了具身AI的发展。在本文中，我们对大规模模型赋能的具身AI进行了全面调研，重点关注自主决策和具身学习。我们探讨了分层和端到端决策-making范式，详细说明了大规模模型如何增强分层决策的高层规划、低层执行和反馈，以及如何增强视觉-语言-行动（VLA）模型的端到端决策。对于具身学习，我们介绍了主流的学习方法，并深入阐述了大规模模型如何增强模仿学习和强化学习。这是第一次将世界模型整合到具身AI的综述中，介绍了其设计方法及其在增强决策和学习方面的关键作用。尽管取得了扎实的进步，但仍存在挑战，这些挑战将在本文结尾讨论，作为进一步研究的方向。', 'title_zh': '大型模型赋能的 embodied AI: 一项关于决策与体态学习的综述'}
{'arxiv_id': 'arXiv:2508.10398', 'title': 'Super LiDAR Reflectance for Robotic Perception', 'authors': 'Wei Gao, Jie Zhang, Mingle Zhao, Zhiyuan Zhang, Shu Kong, Maani Ghaffari, Dezhen Song, Cheng-Zhong Xu, Hui Kong', 'link': 'https://arxiv.org/abs/2508.10398', 'abstract': 'Conventionally, human intuition often defines vision as a modality of passive optical sensing, while active optical sensing is typically regarded as measuring rather than the default modality of vision. However, the situation now changes: sensor technologies and data-driven paradigms empower active optical sensing to redefine the boundaries of vision, ushering in a new era of active vision. Light Detection and Ranging (LiDAR) sensors capture reflectance from object surfaces, which remains invariant under varying illumination conditions, showcasing significant potential in robotic perception tasks such as detection, recognition, segmentation, and Simultaneous Localization and Mapping (SLAM). These applications often rely on dense sensing capabilities, typically achieved by high-resolution, expensive LiDAR sensors. A key challenge with low-cost LiDARs lies in the sparsity of scan data, which limits their broader application. To address this limitation, this work introduces an innovative framework for generating dense LiDAR reflectance images from sparse data, leveraging the unique attributes of non-repeating scanning LiDAR (NRS-LiDAR). We tackle critical challenges, including reflectance calibration and the transition from static to dynamic scene domains, facilitating the reconstruction of dense reflectance images in real-world settings. The key contributions of this work include a comprehensive dataset for LiDAR reflectance image densification, a densification network tailored for NRS-LiDAR, and diverse applications such as loop closure and traffic lane detection using the generated dense reflectance images.', 'abstract_zh': '传统上，人类直觉将视觉视为一种被动光学感知模式，而主动光学感知通常被视为测量而非视觉的默认模式。然而，当前的情况发生了变化：传感器技术和数据驱动的方法赋予了主动光学感知重新定义视觉边界的能力，开启了主动视觉的新时代。Light Detection and Ranging (LiDAR) 传感器捕获物体表面的反射率，这种反射率在不同照明条件下保持不变，展示了在机器人感知任务（如检测、识别、分割和Simultaneous Localization and Mapping (SLAM)）中巨大的潜力。这些应用往往依赖于密集的感知能力，通常通过高分辨率、昂贵的LiDAR传感器实现。低成本LiDAR的一个关键挑战在于扫描数据的稀疏性，这限制了它们的广泛应用。为此，本工作提出了一种创新框架，从稀疏数据生成密集的LiDAR反射率图像，利用非重复扫描LiDAR (NRS-LiDAR) 的独特属性。我们解决了反射率校准和从静态场景向动态场景过渡的关键挑战，促进了在实际场景中重建密集反射率图像。本工作的主要贡献包括LiDAR反射率图像稀疏化数据集、专为NRS-LiDAR设计的稀疏化网络，以及使用生成的密集反射率图像进行环路闭合和交通车道检测等多种应用。', 'title_zh': '超分辨率LiDAR反射率用于机器人感知'}
{'arxiv_id': 'arXiv:2508.10378', 'title': 'A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons', 'authors': 'Yu Chen, Shu Miao, Chunyu Wu, Jingsong Mu, Bo OuYang, Xiang Li', 'link': 'https://arxiv.org/abs/2508.10378', 'abstract': "Upper-limb exoskeletons are primarily designed to provide assistive support by accurately interpreting and responding to human intentions. In home-care scenarios, exoskeletons are expected to adapt their assistive configurations based on the semantic information of the task, adjusting appropriately in accordance with the nature of the object being manipulated. However, existing solutions often lack the ability to understand task semantics or collaboratively plan actions with the user, limiting their generalizability. To address this challenge, this paper introduces a semantic-aware framework that integrates large language models into the task planning framework, enabling the delivery of safe and intent-integrative assistance. The proposed approach begins with the exoskeleton operating in transparent mode to capture the wearer's intent during object grasping. Once semantic information is extracted from the task description, the system automatically configures appropriate assistive parameters. In addition, a diffusion-based anomaly detector is used to continuously monitor the state of human-robot interaction and trigger real-time replanning in response to detected anomalies. During task execution, online trajectory refinement and impedance control are used to ensure safety and regulate human-robot interaction. Experimental results demonstrate that the proposed method effectively aligns with the wearer's cognition, adapts to semantically varying tasks, and responds reliably to anomalies.", 'abstract_zh': '上肢外骨骼主要通过准确解读和响应人类意图来提供辅助支持。在家庭护理场景中，外骨骼期望根据任务的语义信息调整其辅助配置，根据所操作对象的性质适时调整。然而，现有解决方案往往缺乏理解任务语义或与用户协作规划动作的能力，限制了其通用性。为应对这一挑战，本文引入了一种语义感知框架，将大型语言模型整合到任务规划框架中，从而实现安全且意图整合的辅助。该提出的方案首先在外骨骼透明模式下运行，以捕捉佩戴者在抓取物体时的意图。一旦从任务描述中提取出语义信息，系统将自动配置适当的辅助参数。此外，采用基于扩散的异常检测器持续监控人机交互状态，并在检测到异常时触发实时重新规划。在任务执行过程中，通过在线轨迹优化和阻抗控制确保安全并调节人机交互。实验结果表明，所提出的方法能够有效地与佩戴者的认知相契合，适应语义变化的任务，并可靠地响应异常。', 'title_zh': '基于语义的认知框架：上肢外骨骼安全且意图整合的辅助方法'}
{'arxiv_id': 'arXiv:2508.10371', 'title': 'Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning', 'authors': 'Wenqi Zheng, Yutaka Arakawa', 'link': 'https://arxiv.org/abs/2508.10371', 'abstract': "Reinforcement learning in large reasoning models enables learning from feedback on their outputs, making it particularly valuable in scenarios where fine-tuning data is limited. However, its application in multi-modal human activity recognition (HAR) domains remains largely underexplored. Our work extends reinforcement learning to the human activity recognition domain with multimodal large language models. By incorporating visual reinforcement learning in the training process, the model's generalization ability on few-shot recognition can be greatly improved. Additionally, visual reinforcement learning can enhance the model's reasoning ability and enable explainable analysis in the inference stage. We name our few-shot human activity recognition method with visual reinforcement learning FAVOR. Specifically, our approach first utilizes a multimodal large language model (MLLM) to generate multiple candidate responses for the human activity image, each containing reasoning traces and final answers. These responses are then evaluated using reward functions, and the MLLM model is subsequently optimized using the Group Relative Policy Optimization (GRPO) algorithm. In this way, the MLLM model can be adapted to human activity recognition with only a few samples. Extensive experiments on four human activity recognition datasets and five different settings demonstrate the superiority of the proposed method.", 'abstract_zh': '基于视觉强化学习的大规模语言模型在多模态人类活动识别中的 few-shot 训练方法', 'title_zh': '基于MLLM的视觉强化学习在少样本视觉人体活动识别中的应用'}
{'arxiv_id': 'arXiv:2508.10363', 'title': 'BEASST: Behavioral Entropic Gradient based Adaptive Source Seeking for Mobile Robots', 'authors': 'Donipolo Ghimire, Aamodh Suresh, Carlos Nieto-Granda, Solmaz S. Kia', 'link': 'https://arxiv.org/abs/2508.10363', 'abstract': "This paper presents BEASST (Behavioral Entropic Gradient-based Adaptive Source Seeking for Mobile Robots), a novel framework for robotic source seeking in complex, unknown environments. Our approach enables mobile robots to efficiently balance exploration and exploitation by modeling normalized signal strength as a surrogate probability of source location. Building on Behavioral Entropy(BE) with Prelec's probability weighting function, we define an objective function that adapts robot behavior from risk-averse to risk-seeking based on signal reliability and mission urgency. The framework provides theoretical convergence guarantees under unimodal signal assumptions and practical stability under bounded disturbances. Experimental validation across DARPA SubT and multi-room scenarios demonstrates that BEASST consistently outperforms state-of-the-art methods, achieving 15% reduction in path length and 20% faster source localization through intelligent uncertainty-driven navigation that dynamically transitions between aggressive pursuit and cautious exploration.", 'abstract_zh': '基于行为熵梯度的自适应源搜索框架BEASST：移动机器人在复杂未知环境中的源搜索新方法', 'title_zh': 'BEASST: 基于行为熵梯度自适应源寻求的移动机器人算法'}
{'arxiv_id': 'arXiv:2508.10333', 'title': 'ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver', 'authors': 'Wenxuan Song, Ziyang Zhou, Han Zhao, Jiayi Chen, Pengxiang Ding, Haodong Yan, Yuxin Huang, Feilong Tang, Donglin Wang, Haoang Li', 'link': 'https://arxiv.org/abs/2508.10333', 'abstract': "Recent advances in Vision-Language-Action (VLA) models have enabled robotic agents to integrate multimodal understanding with action execution. However, our empirical analysis reveals that current VLAs struggle to allocate visual attention to target regions. Instead, visual attention is always dispersed. To guide the visual attention grounding on the correct target, we propose ReconVLA, a reconstructive VLA model with an implicit grounding paradigm. Conditioned on the model's visual outputs, a diffusion transformer aims to reconstruct the gaze region of the image, which corresponds to the target manipulated objects. This process prompts the VLA model to learn fine-grained representations and accurately allocate visual attention, thus effectively leveraging task-specific visual information and conducting precise manipulation. Moreover, we curate a large-scale pretraining dataset comprising over 100k trajectories and 2 million data samples from open-source robotic datasets, further boosting the model's generalization in visual reconstruction. Extensive experiments in simulation and the real world demonstrate the superiority of our implicit grounding method, showcasing its capabilities of precise manipulation and generalization. Our project page is this https URL.", 'abstract_zh': 'Recent advances in 视觉-语言-行动 (VLA) 模型使机器人代理能够整合多模态理解和行动执行。然而，我们的实证分析表明，当前的VLA模型在分配视觉注意力到目标区域时面临困难，视觉注意力总是分散的。为引导视觉注意力正确地聚焦到目标上，我们提出了一种具有隐式地标化范式的重建VLA模型——ReconVLA。基于模型的视觉输出，扩散变换器旨在重建图像的目标注视区域，对应于被操作的目标物体。这一过程促使VLA模型学习精细的表示并准确分配视觉注意力，从而有效利用任务相关的视觉信息并进行精确操作。此外，我们编制了一个包含超过10万个轨迹和200万个数据样本的大规模预训练数据集，进一步提升了模型在视觉重建方面的泛化能力。在仿真和真实世界中的广泛实验显示了我们隐式地标化方法的优越性，展示了其精确操作和泛化的能力。项目页面见此链接：https URL。', 'title_zh': 'ReconVLA：重建视觉-语言-动作模型作为有效的机器人感知器'}
{'arxiv_id': 'arXiv:2508.10269', 'title': 'Hybrid Data-Driven Predictive Control for Robust and Reactive Exoskeleton Locomotion Synthesis', 'authors': 'Kejun Li, Jeeseop Kim, Maxime Brunet, Marine Pétriaux, Yisong Yue, Aaron D. Ames', 'link': 'https://arxiv.org/abs/2508.10269', 'abstract': 'Robust bipedal locomotion in exoskeletons requires the ability to dynamically react to changes in the environment in real time. This paper introduces the hybrid data-driven predictive control (HDDPC) framework, an extension of the data-enabled predictive control, that addresses these challenges by simultaneously planning foot contact schedules and continuous domain trajectories. The proposed framework utilizes a Hankel matrix-based representation to model system dynamics, incorporating step-to-step (S2S) transitions to enhance adaptability in dynamic environments. By integrating contact scheduling with trajectory planning, the framework offers an efficient, unified solution for locomotion motion synthesis that enables robust and reactive walking through online replanning. We validate the approach on the Atalante exoskeleton, demonstrating improved robustness and adaptability.', 'abstract_zh': '外骨骼中鲁棒的双足运动需要能够实时动态应对环境变化的能力。本文介绍了基于混合数据驱动预测控制（HDDPC）框架，这一扩展的数据使能预测控制方法通过同时规划脚接触时间和连续域轨迹来解决这些挑战。所提出的方法利用Hankel矩阵进行系统动力学建模，并通过步态到步态（S2S）转换增强在动态环境中的适应性。通过将接触规划与轨迹规划集成，该框架提供了一种有效的统一解决方案，用于通过在线重新规划合成稳健且反应灵敏的行走运动。我们在Atalante外骨骼上验证了该方法，展示了其改进的鲁棒性和适应性。', 'title_zh': '基于数据驱动的混合预测控制方法用于实现稳健和反应式外骨骼步行合成'}
{'arxiv_id': 'arXiv:2508.10203', 'title': 'Systematic Constraint Formulation and Collision-Free Trajectory Planning Using Space-Time Graphs of Convex Sets', 'authors': 'Matthew D. Osburn, Cameron K. Peterson, John L. Salmon', 'link': 'https://arxiv.org/abs/2508.10203', 'abstract': 'In this paper, we create optimal, collision-free, time-dependent trajectories through cluttered dynamic environments. The many spatial and temporal constraints make finding an initial guess for a numerical solver difficult. Graphs of Convex Sets (GCS) and the recently developed Space-Time Graphs of Convex Sets formulation (ST-GCS) enable us to generate optimal minimum distance collision-free trajectories without providing an initial guess to the solver. We also explore the derivation of general GCS-compatible constraints and document an intuitive strategy for adapting general constraints to the framework. We show that ST-GCS produces equivalent trajectories to the standard GCS formulation when the environment is static. We then show ST-GCS operating in dynamic environments to find minimum distance collision-free trajectories.', 'abstract_zh': '本文通过复杂动态环境，创建最优且无碰撞的时间依赖轨迹。凸集合图（GCS）和最近发展的时空凸集合图（ST-GCS）形式化方法使我们能够在不需要求解器初始猜测的情况下生成最优的最小距离无碰撞轨迹。我们探索了通用GCS相容约束的推导，并记录了一种直观的策略，用于将通用约束适应该框架。当环境静止时，我们证明ST-GCS产生的轨迹与标准GCS形式化方法等效。然后，我们展示了ST-GCS在动态环境中寻找最小距离无碰撞轨迹的能力。', 'title_zh': '基于凸集时空图的系统约束形式化与碰撞免费轨迹规划 hentai'}
{'arxiv_id': 'arXiv:2508.10144', 'title': 'WiFi-based Global Localization in Large-Scale Environments Leveraging Structural Priors from osmAG', 'authors': 'Xu Ma, Jiajie Zhang, Fujing Xie, Sören Schwertfeger', 'link': 'https://arxiv.org/abs/2508.10144', 'abstract': "Global localization is essential for autonomous robotics, especially in indoor environments where the GPS signal is denied. We propose a novel WiFi-based localization framework that leverages ubiquitous wireless infrastructure and the OpenStreetMap Area Graph (osmAG) for large-scale indoor environments. Our approach integrates signal propagation modeling with osmAG's geometric and topological priors. In the offline phase, an iterative optimization algorithm localizes WiFi Access Points (APs) by modeling wall attenuation, achieving a mean localization error of 3.79 m (35.3\\% improvement over trilateration). In the online phase, real-time robot localization uses the augmented osmAG map, yielding a mean error of 3.12 m in fingerprinted areas (8.77\\% improvement over KNN fingerprinting) and 3.83 m in non-fingerprinted areas (81.05\\% improvement). Comparison with a fingerprint-based method shows that our approach is much more space efficient and achieves superior localization accuracy, especially for positions where no fingerprint data are available. Validated across a complex 11,025 &m^2& multi-floor environment, this framework offers a scalable, cost-effective solution for indoor robotic localization, solving the kidnapped robot problem. The code and dataset are available at this https URL.", 'abstract_zh': '全球定位对于自主机器人至关重要，特别是在GPS信号被拒绝的室内环境中。我们提出了一种基于WiFi的新型定位框架，该框架利用了普遍存在的无线基础设施和OpenStreetMap区域图（osmAG）来应对大规模室内环境的定位问题。该方法将信号传播建模与osmAG的几何和拓扑先验相结合。在离线阶段，通过迭代优化算法定位WiFi接入点（AP），通过建模墙壁衰减，实现了均值定位误差3.79米（比三角测量提高了35.3%）。在线阶段，实时机器人定位使用增强的osmAG地图，指纹匹配区域的均值误差为3.12米（比KNN指纹识别提高了8.77%），非指纹匹配区域的均值误差为3.83米（比KNN指纹识别提高了81.05%）。与基于指纹的方法相比，我们的方法在空间效率和定位准确性方面表现出优越性，特别是在没有指纹数据的情况下。该框架已在复杂的大规模多层环境中得到验证，提供了一种可扩展且成本效益高的室内机器人定位解决方案，解决了被绑架的机器人问题。代码和数据集可在以下链接获取。', 'title_zh': '基于OSMAG结构先验的大型环境WiFi全局定位'}
{'arxiv_id': 'arXiv:2508.10747', 'title': 'Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning', 'authors': 'Sangwoo Jeon, Juchul Shin, Gyeong-Tae Kim, YeonJe Cho, Seongwoo Kim', 'link': 'https://arxiv.org/abs/2508.10747', 'abstract': 'Generalized planning using deep reinforcement learning (RL) combined with graph neural networks (GNNs) has shown promising results in various symbolic planning domains described by PDDL. However, existing approaches typically represent planning states as fully connected graphs, leading to a combinatorial explosion in edge information and substantial sparsity as problem scales grow, especially evident in large grid-based environments. This dense representation results in diluted node-level information, exponentially increases memory requirements, and ultimately makes learning infeasible for larger-scale problems. To address these challenges, we propose a sparse, goal-aware GNN representation that selectively encodes relevant local relationships and explicitly integrates spatial features related to the goal. We validate our approach by designing novel drone mission scenarios based on PDDL within a grid world, effectively simulating realistic mission execution environments. Our experimental results demonstrate that our method scales effectively to larger grid sizes previously infeasible with dense graph representations and substantially improves policy generalization and success rates. Our findings provide a practical foundation for addressing realistic, large-scale generalized planning tasks.', 'abstract_zh': '使用深度强化学习与图神经网络相结合的广义规划方法在PDDL描述的各类符号规划领域中展现了有前景的结果。然而，现有方法通常将规划状态表示为完全连接的图，这导致了边信息的组合爆炸和问题规模增大时的显著稀疏性，特别是在基于网格的大环境中尤为明显。这种密集表示导致节点级信息稀释，内存需求成指数级增加，最终使得学习在大规模问题中变得不切实际。为解决这些挑战，我们提出了一种稀疏、目标导向的GNN表示方法，该方法选择性地编码相关局部关系，并明确集成与目标相关的空间特征。我们通过在网格世界中基于PDDL设计新颖的无人机任务场景，有效模拟了现实任务执行环境。实验结果表明，我们的方法能够有效扩展到以前因密集图表示而不切实际的大网格尺寸，并显著提高策略泛化能力和成功率。我们的研究为解决现实的大规模广义规划任务提供了实际的基础。', 'title_zh': '持续扩展而不淡出：目标导向的稀疏GNN在基于RL的通用规划中的应用'}
{'arxiv_id': 'arXiv:2508.10567', 'title': 'SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving', 'authors': 'Philipp Wolters, Johannes Gilg, Torben Teepe, Gerhard Rigoll', 'link': 'https://arxiv.org/abs/2508.10567', 'abstract': 'End-to-end autonomous driving systems promise stronger performance through unified optimization of perception, motion forecasting, and planning. However, vision-based approaches face fundamental limitations in adverse weather conditions, partial occlusions, and precise velocity estimation - critical challenges in safety-sensitive scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. To address these limitations, we propose SpaRC-AD, a query-based end-to-end camera-radar fusion framework for planning-oriented autonomous driving. Through sparse 3D feature alignment, and doppler-based velocity estimation, we achieve strong 3D scene representations for refinement of agent anchors, map polylines and motion modelling. Our method achieves strong improvements over the state-of-the-art vision-only baselines across multiple autonomous driving tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA), online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal consistency on multiple challenging benchmarks, including real-world open-loop nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We show the effectiveness of radar-based fusion in safety-critical scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. The source code of all experiments is available at this https URL', 'abstract_zh': '基于查询的端到端相机-雷达融合规划导向自动驾驶系统', 'title_zh': 'SpaRC-AD：端到端自主驾驶中雷达与摄像头融合的基线方法'}
{'arxiv_id': 'arXiv:2508.10413', 'title': 'Probabilistic Latency Analysis of the Data Distribution Service in ROS 2', 'authors': 'Sanghoon Lee, Hyung-Seok Park, Jiyeong Chae, Kyung-Joon Park', 'link': 'https://arxiv.org/abs/2508.10413', 'abstract': 'Robot Operating System 2 (ROS 2) is now the de facto standard for robotic communication, pairing UDP transport with the Data Distribution Service (DDS) publish-subscribe middleware. DDS achieves reliability through periodic heartbeats that solicit acknowledgments for missing samples and trigger selective retransmissions. In lossy wireless networks, the tight coupling among heartbeat period, IP fragmentation, and retransmission interval obscures end to end latency behavior and leaves practitioners with little guidance on how to tune these parameters. To address these challenges, we propose a probabilistic latency analysis (PLA) that analytically models the reliable transmission process of ROS 2 DDS communication using a discrete state approach. By systematically analyzing both middleware level and transport level events, PLA computes the steady state probability distribution of unacknowledged messages and the retransmission latency. We validate our PLA across 270 scenarios, exploring variations in packet delivery ratios, message sizes, and both publishing and retransmission intervals, demonstrating a close alignment between analytical predictions and experimental results. Our findings establish a theoretical basis to systematically optimize reliability, latency, and performance in wireless industrial robotics.', 'abstract_zh': 'ROS 2的概率延迟分析：面向无线工业机器人中DDS通信的参数优化', 'title_zh': 'ROS 2中数据分布服务的概率延迟分析'}
