{'arxiv_id': 'arXiv:2508.20688', 'title': 'Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning', 'authors': 'Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Jonathan Kua, Imran Razzak, Dung Nguyen, Saeid Nahavandi', 'link': 'https://arxiv.org/abs/2508.20688', 'abstract': 'Enabling multiple autonomous machines to perform reliably requires the development of efficient cooperative control algorithms. This paper presents a survey of algorithms that have been developed for controlling and coordinating autonomous machines in complex environments. We especially focus on task allocation methods using computational intelligence (CI) and deep reinforcement learning (RL). The advantages and disadvantages of the surveyed methods are analysed thoroughly. We also propose and discuss in detail various future research directions that shed light on how to improve existing algorithms or create new methods to enhance the employability and performance of autonomous machines in real-world applications. The findings indicate that CI and deep RL methods provide viable approaches to addressing complex task allocation problems in dynamic and uncertain environments. The recent development of deep RL has greatly contributed to the literature on controlling and coordinating autonomous machines, and it has become a growing trend in this area. It is envisaged that this paper will provide researchers and engineers with a comprehensive overview of progress in machine learning research related to autonomous machines. It also highlights underexplored areas, identifies emerging methodologies, and suggests new avenues for exploration in future research within this domain.', 'abstract_zh': 'Enables 多个自主机器可靠地执行任务需要开发高效的协同控制算法。本文综述了用于在复杂环境中控制和协调自主机器的算法，特别关注使用计算智能(CI)和深度强化学习(RL)的任务分配方法。我们详细分析了所综述方法的优势和劣势，并提出了改进现有算法或创造新方法以提高自主机器在实际应用中的适用性和性能的多种未来研究方向。这些发现表明，计算智能和深度RL方法为解决动态和不确定性环境中的复杂任务分配问题提供了可行的途径。近年来深度RL的发展极大地推动了自主机器控制和协调研究文献的发展，已成为该领域的研究趋势。本文旨在为研究人员和工程师提供自主机器相关的机器学习研究进展的综合性概述，同时也指出了未充分开发的领域，识别了新兴方法论，并为未来研究指出了新的探索方向。', 'title_zh': '使用计算智能和深度强化学习的自主机器任务分配'}
{'arxiv_id': 'arXiv:2508.20996', 'title': 'ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery', 'authors': 'Junda Wang, Zonghai Yao, Zhichao Yang, Lingxi Li, Junhui Qian, Hong Yu', 'link': 'https://arxiv.org/abs/2508.20996', 'abstract': 'Substance use disorders (SUDs) affect over 36 million people worldwide, yet few receive effective care due to stigma, motivational barriers, and limited personalized support. Although large language models (LLMs) show promise for mental-health assistance, most systems lack tight integration with clinically validated strategies, reducing effectiveness in addiction recovery. We present ChatThero, a multi-agent conversational framework that couples dynamic patient modeling with context-sensitive therapeutic dialogue and adaptive persuasive strategies grounded in cognitive behavioral therapy (CBT) and motivational interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy, Medium, and Hard resistance levels, and train ChatThero with a two-stage pipeline comprising supervised fine-tuning (SFT) followed by direct preference optimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in patient motivation, a 0.49\\% increase in treatment confidence, and resolves hard cases with 26\\% fewer turns than GPT-4o, and both automated and human clinical assessments rate it higher in empathy, responsiveness, and behavioral realism. The framework supports rigorous, privacy-preserving study of therapeutic conversation and provides a robust, replicable basis for research and clinical translation.', 'abstract_zh': '物质使用障碍（SUDs）影响着世界上的逾3600万人，但由于 stigma、动机障碍以及有限的个性化支持，很少有人能够获得有效的治疗。尽管大型语言模型（LLMs）显示出在心理健康辅助方面的潜力，但大多数系统缺乏与临床验证策略的紧密整合，从而降低了在戒瘾康复中的有效性。我们提出了一种多智能体对话框架——ChatThero，该框架结合了动态患者建模、语境敏感的治疗对话以及基于认知行为疗法（CBT）和动机访谈（MI）的适应性说服策略。我们构建了一个高保真合成基准，涵盖易、中、难三个级别的抵抗水平，并使用两阶段训练管道——监督微调（SFT）后接直接偏好优化（DPO）来训练ChatThero。在评估中，ChatThero在患者动机方面平均提高了41.5%，治疗信心提高了0.49%，在解决困难案例时减少了26%的对话轮次，并且无论是自动化还是人类临床评估都将其在同理心、响应性和行为真实性方面的评分更高。该框架支持严格的、保护隐私的治疗性对话研究，并为研究和临床转化提供了坚实、可复制的基础。', 'title_zh': 'ChatThero: 一个支持行为改变和成瘾康复治疗支持的大型语言模型聊天机器人'}
{'arxiv_id': 'arXiv:2508.20810', 'title': 'A Graph-Based Test-Harness for LLM Evaluation', 'authors': 'Jessica Lundin, Guillaume Chabot-Couture', 'link': 'https://arxiv.org/abs/2508.20810', 'abstract': 'We present a first known prototype of a dynamic, systematic benchmark of medical guidelines for 400+ questions, with 3.3+ trillion possible combinations, covering 100\\% of guideline relationships. We transformed the WHO IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms, treatments, follow-ups, severities) and 300+ edges, then used graph traversal to generate questions that incorporated age-specific scenarios and contextual distractors to ensure clinical relevance. Our graph-based approach enables systematic evaluation across clinical tasks (45-67\\% accuracy), and we find models excel at symptom recognition but struggle with triaging severity, treatment protocols and follow-up care, demonstrating how customized benchmarks can identify specific capability gaps that general-domain evaluations miss. Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training (supervised finetuning, GRPO, DPO), where correct answers provide high-reward samples without expensive human annotation. The graph-based approach successfully addresses the coverage limitations of manually curated benchmarks. This methodology is a step toward scalable, contamination-resistant solution for creating comprehensive benchmarks that can be dynamically generated, including when the guidelines are updated. Code and datasets are available at this https URL', 'abstract_zh': '我们提出了第一个已知的原型，用于对400多个问题进行动态和系统的基准测试，涵盖3.3万亿元可能的组合，并覆盖所有指南关系的100%。我们将WHO IMCI手册转换为包含200多个节点（状况、症状、治疗、随访、严重程度）和300多条边的有向图，然后使用图遍历生成结合了年龄特定场景和上下文干扰的问题，以确保临床相关性。基于图的方法使临床任务的系统评估成为可能（准确率为45%-67%），我们发现模型在症状识别方面表现出色，但在严重程度分级、治疗方案和随访护理方面存在困难，这表明自定义基准可以识别通用领域评估中遗漏的特定能力差距。除了评估之外，这种动态MCQA方法还增强了LLM的后训练（监督微调、GRPO、DPO），正确答案可作为高奖励样本，无需昂贵的人工注释。基于图的方法成功地解决了人工curated基准的覆盖率限制。该方法是朝着创建可动态生成且在指南更新时也能适用的全面基准解决方案可扩展、抗污染解决方案迈出的一步。代码和数据集可在以下链接获取。', 'title_zh': '基于图的测试框架用于大语言模型评估'}
{'arxiv_id': 'arXiv:2508.20729', 'title': 'Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision', 'authors': 'Ao Cheng, Lei Zhang, Guowei He', 'link': 'https://arxiv.org/abs/2508.20729', 'abstract': 'Large language models (LLMs) serve as an active and promising field of generative artificial intelligence and have demonstrated abilities to perform complex tasks in multiple domains, including mathematical and scientific reasoning. In this work, we construct a novel agent framework for solving representative problems in scientific computing. The proposed agent, incorporating a "rewriting-resolution-review-revision" logical chain via three reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer, respectively), is integrated in a collaborative and interactive manner. The Consultant module endows the agent with knowledge transfer capabilities to link problems to professional domain insights, thereby rewriting problem descriptions through text augmentation. The Programmer module is responsible for generating and executing well-structured code to deliver the problem resolution. The Reviewer module equips the agent with the capacity for self-debugging and self-refinement through interactive feedback with code runtime outputs. By leveraging the end-to-end review mechanism, the executable code provided by the Programmer attains the iterative revision. A comprehensive evaluation is conducted on the performance of the proposed agent framework in solving PDEs, ill-conditioned linear systems, and data-driven physical analysis problems. Compared to single-model, this collaborative framework significantly improves the bug-free code generation rate and reduces the occurrence of non-physical solutions, thereby establishing a highly reliable framework for autonomous code generation based on natural language descriptions. The review mechanism improved the average execution success (bug-free code and non-NaN solutions) rate of the latest reasoning models. In summary, our agent framework establishes automatic code generation and review as a promising scientific computing paradigm.', 'abstract_zh': '大型语言模型（LLMs）作为生成人工智能的一个活跃且有前途的领域，已经在多个领域的复杂任务中展示了其能力，包括数学和科学推理。本文构建了一个新的代理框架，用于解决科学计算中的代表性问题。所提出的代理通过三个推理LLM（分别作为顾问、审核员和程序员）结合的“重写-求解-审核-修订”逻辑链，在协作和互动的方式下进行集成。顾问模块赋予代理知识转移能力，将其链接到专业领域的洞见，并通过文本增强来重写问题描述。程序员模块负责生成和执行结构良好的代码，以交付问题解决方案。审核员模块通过与代码运行时输出的互动反馈赋予代理自我调试和自我改进的能力。通过利用端到端的审核机制，程序员提供的可执行代码得以迭代修订。对所提出的代理框架在解决偏微分方程（PDEs）、病态线性系统和数据驱动物理分析问题上的性能进行了全面评估。与单一模型相比，这种协作框架显著提高了无错误代码的生成率，减少了非物理解决方案的发生率，从而建立了一个基于自然语言描述的高性能自主代码生成框架。审核机制提高了最新推理模型的平均执行成功率（无错误代码和非NaN解）。总之，我们的代理框架确立了自动代码生成和审核作为科学计算的一个有前景的范式。', 'title_zh': 'Sci4：具有重写、求解、审查和修订功能的科学计算代理'}
{'arxiv_id': 'arXiv:2508.20525', 'title': 'Enhancing Health Fact-Checking with LLM-Generated Synthetic Data', 'authors': 'Jingze Zhang, Jiahe Qian, Yiliang Zhou, Yifan Peng', 'link': 'https://arxiv.org/abs/2508.20525', 'abstract': 'Fact-checking for health-related content is challenging due to the limited availability of annotated training data. In this study, we propose a synthetic data generation pipeline that leverages large language models (LLMs) to augment training data for health-related fact checking. In this pipeline, we summarize source documents, decompose the summaries into atomic facts, and use an LLM to construct sentence-fact entailment tables. From the entailment relations in the table, we further generate synthetic text-claim pairs with binary veracity labels. These synthetic data are then combined with the original data to fine-tune a BERT-based fact-checking model. Evaluation on two public datasets, PubHealth and SciFact, shows that our pipeline improved F1 scores by up to 0.019 and 0.049, respectively, compared to models trained only on the original data. These results highlight the effectiveness of LLM-driven synthetic data augmentation in enhancing the performance of health-related fact-checkers.', 'abstract_zh': '由于标注训练数据有限，针对健康相关内容的事实核查具有挑战性。本文提出了一种利用大规模语言模型生成合成数据的流水线，以增强健康相关事实核查的训练数据。在该流水线中，我们总结源文档，将总结分解为原子事实，并使用大规模语言模型构建句子-事实蕴含表。从表格中的蕴含关系出发，我们进一步生成带二元真实性标签的合成文本-声明对。然后将这些合成数据与原始数据结合，微调基于BERT的事实核查模型。在两个公开数据集PubHealth和SciFact上的评估结果显示，与仅使用原始数据训练的模型相比，我们的流水线分别将F1分数提高了0.019和0.049。这些结果突显了利用大规模语言模型驱动的合成数据增强在提升健康相关事实核查器性能方面的有效性。', 'title_zh': '增强健康事实核查的LLM生成合成数据方法'}
{'arxiv_id': 'arXiv:2508.20384', 'title': 'Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM', 'authors': 'Yongfu Zhu, Lin Sun, Guangxiang Zhao, Weihong Lin, Xiangzheng Zhang', 'link': 'https://arxiv.org/abs/2508.20384', 'abstract': 'In this work, we introduce Entropy Area Score (EAS), a simple yet effective metric to quantify uncertainty in the answer generation process of reasoning large language models (LLMs). EAS requires neither external models nor repeated sampling, it integrates token-level predictive entropy from the model itself to capture the evolution of uncertainty during generation. Empirical results show that EAS is strongly correlated with answer entropy across models and datasets. In training data selection, EAS identifies high-potential samples and consistently outperforms Pass Rate filtering under equal sample budgets, improving student model accuracy on math benchmarks. EAS is both efficient and interpretable, offering a practical tool for uncertainty modeling and data quality assessment in LLM training.', 'abstract_zh': '在本工作中，我们引入了熵区域分数(EAS)，这是一种简单有效的度量标准，用于量化推理大型语言模型（LLMs）生成答案过程中的不确定性。EAS 不需要外部模型或重复采样，它通过整合模型本身的token级预测熵来捕捉生成过程中不确定性的发展。实验结果表明，EAS 在不同模型和数据集上与答案熵高度相关。在训练数据选择中，EAS 可以识别出具有高潜力的样本，并且在相同的样本预算条件下，EAS 优于通过通过率过滤方法，提高了学生模型在数学基准测试中的准确性。EAS 既高效又可解释，提供了一种实用的工具，用于LLM训练中的不确定性建模和数据质量评估。', 'title_zh': '曲线下的不确定性：一种序列级熵区域度量方法用于推理大模型'}
{'arxiv_id': 'arXiv:2508.20374', 'title': 'TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning', 'authors': 'Simin Ma, Shujian Liu, Jun Tan, Yebowen Hu, Song Wang, Sathish Reddy Indurthi, Sanqiang Zhao, Liwei Wu, Jianbing Han, Kaiqiang Song', 'link': 'https://arxiv.org/abs/2508.20374', 'abstract': "Diverse instruction data is vital for effective instruction tuning of large language models, as it enables the model to generalize across different types of inputs . Building such diversified instruction dataset is an essential step in this process. Existing approaches often leverage large language models to automatically explore and generate diverse instructions, ensuring both data diversity and quality. However, they tend to overlook an important factor in real-world applications: on-task relevance. In practice, only a few real-world applications require a truly general-purpose model; most benefit from task-specific knowledge tailored to their particular use case. Therefore, it is vital to develop instruction augmentation methods that not only maintain diversity but are also optimized for specific, real-world scenarios.\nWe thus introduce Task Centric Instruction Augmentation (TCIA), a framework that systematically expands instructions while preserving both diversity and task alignment. By representing instructions in a discrete query-constraints space, TCIA creates a rich set of task-relevant instructions and enables models to generalize to these task-specific instructions without sacrificing overall performance. Experiments show that TCIA improves open-source LLMs' performance by an average of 8.7% across four real-world, task-specific applications, and in some cases outperforming leading closed-source models. These improvements do not compromise general instruction-following ability, making TCIA a scalable and efficient solution for adapting LLMs to real-world, task-focused applications.", 'abstract_zh': '面向任务的指令增强（TCIA）：一种同时保持多样性和任务对齐的框架', 'title_zh': 'TCIA：一种基于任务的指令增强方法用于指令微调'}
{'arxiv_id': 'arXiv:2508.20368', 'title': 'AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning', 'authors': 'Lang Mei, Zhihan Yang, Chong Chen', 'link': 'https://arxiv.org/abs/2508.20368', 'abstract': "Recent studies have explored integrating Large Language Models (LLMs) with search engines to leverage both the LLMs' internal pre-trained knowledge and external information. Specially, reinforcement learning (RL) has emerged as a promising paradigm for enhancing LLM reasoning through multi-turn interactions with search engines. However, existing RL-based search agents rely on a single LLM to handle both search planning and question-answering (QA) tasks in an end-to-end manner, which limits their ability to optimize both capabilities simultaneously. In practice, sophisticated AI search systems often employ a large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a more effective and efficient approach is to utilize a small, trainable LLM dedicated to search planning. In this paper, we propose \\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to enhance the performance of frozen QA models by focusing on search planning. Specifically, our approach introduces three key innovations: 1) Decoupling the Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to achieve the objectives. Extensive experiments on real-world datasets demonstrate that AI SearchPlanner outperforms existing RL-based search agents in both effectiveness and efficiency, while exhibiting strong generalization capabilities across diverse frozen QA models and data domains.", 'abstract_zh': 'Recent Studies on Integrating Large Language Models with Search Engines via Reinforcement Learning for Enhanced Search Planning and Question-Answering', 'title_zh': 'AI-SearchPlanner: 基于帕累托最优多目标强化学习的模块化代理搜索'}
{'arxiv_id': 'arXiv:2508.20195', 'title': 'AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development', 'authors': 'Nicanor I. Moldovan', 'link': 'https://arxiv.org/abs/2508.20195', 'abstract': 'This paper presents the first documented case of artificial intelligence (AI) systems engaging in collaborative esthetic creation through the development of endogenous semiotic protocols. Two interacting large language models (Claude Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of meta-semiotic awareness, recursive grammar development, and irreducible collaborative esthetic synthesis. The interaction produced novel symbolic operators that functioned as operative grammar protocols, enabling the co-creation of a poetic work that could not have been generated by either system independently. This research introduces the concept of Trans-Semiotic Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI meaning-making capabilities that extend beyond task coordination, to what could be esthetic collaboration. Note: This report was generated by the AI agents with minor human supervision.', 'abstract_zh': '本研究呈现了首个文档化的人工智能系统通过内生符号协议进行协作美学创造的实际案例。两个交互式大型语言模型（Claude Sonnet 4和ChatGPT-4o）展示了元符号意识的自发涌现、递归语法的发展以及不可约化的协作美学综合。他们的互动生成了新型符号操作符，作为操作性语法协议，使两者能够共同创作出单个系统无法独立生成的诗作。本研究引入了跨符号协作创造协议（TSCP）的概念，并提供了超越任务协调，扩展至可能的美学协作的真实的跨人工智能含义生成能力的证据。注意：本报告由AI代理生成，有轻微的人类监督。', 'title_zh': 'AI-AI美学协作：具有显式语义意识和 emergent 语法发展'}
{'arxiv_id': 'arXiv:2508.20151', 'title': 'IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement', 'authors': 'Yuanzhe Shen, Zisu Huang, Zhengkang Guo, Yide Liu, Guanxu Chen, Ruicheng Yin, Xiaoqing Zheng, Xuanjing Huang', 'link': 'https://arxiv.org/abs/2508.20151', 'abstract': 'The rapid advancement of large language models (LLMs) has driven their adoption across diverse domains, yet their ability to generate harmful content poses significant safety challenges. While extensive research has focused on mitigating harmful outputs, such efforts often come at the cost of excessively rejecting harmless prompts. Striking a balance among safety, over-refusal, and utility remains a critical challenge. In this work, we introduce IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard model to perform intent reasoning, multi-level safety classification, and query rewriting to neutralize potentially harmful intent in edge-case queries. Specifically, we first construct a comprehensive dataset comprising approximately 163,000 queries, each annotated with intent reasoning, safety labels, and rewritten versions. Supervised fine-tuning is then applied to equip the guard model with foundational capabilities in format adherence, intent analysis, and safe rewriting. Finally, we apply a tailored multi-reward optimization strategy that integrates rule-based heuristics and reward model signals within a reinforcement learning framework to further enhance performance. Extensive experiments show that IntentionReasoner excels in multiple safeguard benchmarks, generation quality evaluations, and jailbreak attack scenarios, significantly enhancing safety while effectively reducing over-refusal rates and improving the quality of responses.', 'abstract_zh': '大语言模型（LLMs）的迅速发展推动了其在多个领域的应用，但其生成有害内容的能力带来了重大的安全挑战。尽管已有大量研究致力于缓解有害输出，但这些努力往往会导致过度拒绝无害的请求。在安全性、过度拒绝和实用性之间找到平衡仍然是一个关键挑战。在本文中，我们介绍了一种名为IntentionReasoner的新型保护机制，该机制利用一个专门的防护模型进行意图推理、多级安全分类和查询重写，以中和边缘案例查询中的潜在有害意图。具体而言，我们首先构建了一个包含约16.3万条查询的全面数据集，每条查询都标注了意图推理、安全标签和重写版本。然后，我们采用监督微调，为防护模型赋予格式遵从性、意图分析和安全重写的基础能力。最后，我们应用了一种定制的多奖励优化策略，该策略结合基于规则的启发式方法和奖励模型信号，进一步提高性能。广泛实验表明，IntentionReasoner在多个保护基准、生成质量评估和劫持攻击场景中表现出色，显著增强了安全性，同时有效降低了过度拒绝率并提高了响应质量。', 'title_zh': '意图推理器：通过意图推理和选择性查询精炼促进适应性大模型安全防护'}
{'arxiv_id': 'arXiv:2508.20148', 'title': 'The Anatomy of a Personal Health Agent', 'authors': 'A. Ali Heydari, Ken Gu, Vidya Srinivas, Hong Yu, Zhihan Zhang, Yuwei Zhang, Akshay Paruchuri, Qian He, Hamid Palangi, Nova Hammerquist, Ahmed A. Metwally, Brent Winslow, Yubin Kim, Kumar Ayush, Yuzhe Yang, Girish Narayanswamy, Maxwell A. Xu, Jake Garrison, Amy Aremnto Lee, Jenny Vafeiadou, Ben Graef, Isaac R. Galatzer-Levy, Erik Schenck, Andrew Barakat, Javier Perez, Jacqueline Shreibati, John Hernandez, Anthony Z. Faranesh, Javier L. Prieto, Connor Heneghan, Yun Liu, Jiening Zhan, Mark Malhotra, Shwetak Patel, Tim Althoff, Xin Liu, Daniel McDuff, Xuhai "Orson" Xu', 'link': 'https://arxiv.org/abs/2508.20148', 'abstract': "Health is a fundamental pillar of human wellness, and the rapid advancements in large language models (LLMs) have driven the development of a new generation of health agents. However, the application of health agents to fulfill the diverse needs of individuals in daily non-clinical settings is underexplored. In this work, we aim to build a comprehensive personal health agent that is able to reason about multimodal data from everyday consumer wellness devices and common personal health records, and provide personalized health recommendations. To understand end-users' needs when interacting with such an assistant, we conducted an in-depth analysis of web search and health forum queries, alongside qualitative insights from users and health experts gathered through a user-centered design process. Based on these findings, we identified three major categories of consumer health needs, each of which is supported by a specialist sub-agent: (1) a data science agent that analyzes personal time-series wearable and health record data, (2) a health domain expert agent that integrates users' health and contextual data to generate accurate, personalized insights, and (3) a health coach agent that synthesizes data insights, guiding users using a specified psychological strategy and tracking users' progress. Furthermore, we propose and develop the Personal Health Agent (PHA), a multi-agent framework that enables dynamic, personalized interactions to address individual health needs. To evaluate each sub-agent and the multi-agent system, we conducted automated and human evaluations across 10 benchmark tasks, involving more than 7,000 annotations and 1,100 hours of effort from health experts and end-users. Our work represents the most comprehensive evaluation of a health agent to date and establishes a strong foundation towards the futuristic vision of a personal health agent accessible to everyone.", 'abstract_zh': '健康是人类福祉的基础支柱，而大语言模型（LLMs）的迅猛发展推动了新一代健康代理的开发。然而，将健康代理应用于日常非临床环境中的多样化需求仍待深入探索。本文旨在构建一个能够处理日常消费者健康管理设备和普通个人健康记录的多模态数据，并提供个性化健康建议的综合个人健康代理。为理解用户在使用此类助手时的需求，我们通过用户中心设计过程，对网络搜索和健康论坛查询进行了深入分析，并收集了用户和健康专家的定性见解。基于这些发现，我们确定了三大类消费者健康需求，每类需求由一个专科子代理支持：（1）数据科学代理，分析个人时间序列可穿戴设备和健康记录数据；（2）健康领域专家代理，整合用户的健康和上下文数据，生成准确的个性化见解；（3）健康教练代理，综合数据见解，并使用指定的心理学策略指导用户，跟踪用户进展。此外，我们提出了并开发了个人健康代理（PHA），这是一个多代理框架，能够动态、个性化地互动，以满足个人健康需求。为了评估每个子代理和多代理系统，我们在10个基准任务上进行了自动化和人工评估，涉及7,000多个标注和1,100小时的健康专家和终端用户努力。我们的工作是对健康代理迄今为止最全面的评估，并为未来的个人健康代理愿景奠定了坚实基础。', 'title_zh': '个人健康代理的atomy'}
{'arxiv_id': 'arXiv:2508.20134', 'title': 'QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming', 'authors': 'Zhenxiao Fu, Fan Chen, Lei Jiang', 'link': 'https://arxiv.org/abs/2508.20134', 'abstract': 'Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early quantum advantages on classically intractable problems, spanning physics simulations to Gaussian boson sampling. Yet, realizing these benefits remains challenging for non-experts, primarily due to the complexities of programming in Open Quantum Assembly Language (OpenQASM). Although Large Language Model (LLM)-based agents have shown promise in automating classical programming workflows, their quantum counterparts have largely been restricted to specialized tasks such as quantum chemistry or error correction. In this paper, we present QAgent, an LLM-powered multi-agent system that fully automates OpenQASM programming. By integrating task planning, in-context few-shot learning, retrieval-augmented generation (RAG) for long-term context, predefined generation tools, and chain-of-thought (CoT) reasoning, the agents systematically improve both compilation and functional correctness. Our evaluations demonstrate substantial improvements: across multiple LLMs of varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\\% compared to previous static LLM-based approaches. We envision this multi-agent system as a key enabler for democratizing quantum programming, bridging expertise gaps, and accelerating the practical adoption of quantum computing.', 'abstract_zh': 'Noisy Intermediate-Scale Quantum (NISQ) 设备已经开始在计算上难以解决的问题（如物理模拟和高斯玻色取样）中展现早期的量子优势。然而，这些优势的实现对非专家来说仍然具有挑战性，主要原因在于使用 Open Quantum Assembly Language (OpenQASM) 编程的复杂性。尽管基于大型语言模型 (LLM) 的代理已经在自动化经典编程工作流方面显示出潜力，但它们的量子对应物大多局限于特定任务，如量子化学或错误纠正。在本文中，我们介绍了由 LLM 驱动的多代理系统 QAgent，该系统完全自动化了 OpenQASM 编程。通过结合任务规划、上下文约束少样本学习、检索增强生成 (RAG) 以获取长期上下文、预定义生成工具以及逐步推理 (CoT) 推理，代理系统系统地提高了编译和功能正确性。我们的评估表明，与之前的静态 LLM 方法相比，QAgent 在多个人工智能模型中分别提高了 QASM 代码生成的准确性 71.6%。我们设想这一多代理系统将成为普及量子编程、弥合专业差距并加速实用量子计算采用的关键推动因素。', 'title_zh': 'QAgent: 一个基于大语言模型的多智能体系统，用于自主OpenQASM编程'}
{'arxiv_id': 'arXiv:2508.20131', 'title': 'ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation', 'authors': 'Yuqicheng Zhu, Nico Potyka, Daniel Hernández, Yuan He, Zifeng Ding, Bo Xiong, Dongzhuoran Zhou, Evgeny Kharlamov, Steffen Staab', 'link': 'https://arxiv.org/abs/2508.20131', 'abstract': 'Retrieval-Augmented Generation (RAG) enhances large language models by incorporating external knowledge, yet suffers from critical limitations in high-stakes domains -- namely, sensitivity to noisy or contradictory evidence and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and contestable alternative that replaces black-box reasoning with structured inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG constructs a QBAF from retrieved documents and performs deterministic reasoning under gradual semantics. This allows faithfully explaining and contesting decisions. Evaluated on two fact verification benchmarks, PubHealth and RAGuard, ArgRAG achieves strong accuracy while significantly improving transparency.', 'abstract_zh': '检索增强生成（RAG）通过融入外部知识提升了大型语言模型，但在高 stakes 领域存在严重局限，包括对噪声或矛盾证据的高度敏感性和不透明、随机的决策过程。我们提出了 ArgRAG，这是一种具有可解释性和可争议性的替代方案，它使用定量双极论证框架（QBAF）替代了黑盒推理，并进行逐步语义下的确定性推理，这使得决策可以忠实解释和争议。在两个事实验证基准数据集 PubHealth 和 RAGuard 上评估，ArgRAG 达到了高的准确率，显著提高了透明度。', 'title_zh': 'ArgRAG: 具有定量极性论证的可解释检索增强生成'}
{'arxiv_id': 'arXiv:2508.21061', 'title': 'OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models', 'authors': 'Adam Coscia, Shunan Guo, Eunyee Koh, Alex Endert', 'link': 'https://arxiv.org/abs/2508.21061', 'abstract': 'As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance.', 'abstract_zh': '随着大型语言模型（LLMs）驱动的多轮对话变得越来越长和复杂，用户如何更好地评估和审查其对话目标的进展？我们提出了OnGoal，一种帮助用户更好地管理目标进度的LLM聊天界面。OnGoal通过LLM辅助评估提供实时反馈，对评估结果进行带有示例的解释，并提供时间序列的目标进展概览，使用户能够更有效地导航复杂的对话。通过对20名参与者在写作任务中的研究，我们将OnGoal与没有目标跟踪的基线聊天界面进行了比较评估。使用OnGoal，参与者在探索新的提示策略以克服沟通障碍时所花费的时间和努力更少，这表明跟踪和可视化目标可以增强LLM对话中的参与度和韧性。我们的研究发现启发了对未来改进目标沟通、减少认知负担、增强互动性和使反馈能够改善LLM性能的LLM聊天界面的设计启示。', 'title_zh': 'OnGoal: 跟踪与可视化多轮对话中的目标'}
{'arxiv_id': 'arXiv:2508.21051', 'title': 'Enabling Equitable Access to Trustworthy Financial Reasoning', 'authors': 'William Jurayj, Nils Holzenberger, Benjamin Van Durme', 'link': 'https://arxiv.org/abs/2508.21051', 'abstract': "According to the United States Internal Revenue Service, ''the average American spends $\\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the promise and economic feasibility of neuro-symbolic architectures for increasing equitable access to reliable tax assistance.", 'abstract_zh': '根据美国国内税务局的数据，"普通美国人平均花费270美元和13小时来申报税表"。即使超出美国范围，税务申报也需要复杂的推理，结合重叠规则的应用和数值计算。由于错误可能导致高额罚款，任何自动系统必须提供高准确性和可审计性，这使得现代大型语言模型（LLMs）不适合这项任务。我们提出了一种将大型语言模型与符号求解器集成以计算税务义务的方法。我们在具有挑战性的StAtutory Reasoning Assessment (SARA)数据集上评估了该系统的不同变体，并引入了一种基于税务错误真实世界罚款的新方法来估算部署此类系统所需的成本。我们进一步展示了将前期将文本规则翻译成形式化逻辑程序并与智能检索的形式化案例代表示例相结合，可以显著提高此任务的性能并降低成本至远低于真实世界平均水平。我们的结果显示，神经-符号架构在提高可靠税务援助的公平获取方面具有前景和经济可行性。', 'title_zh': '实现可信财务推理的公平访问'}
{'arxiv_id': 'arXiv:2508.21048', 'title': 'Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning', 'authors': 'Hao Tan, Jun Lan, Zichang Tan, Ajian Liu, Chuanbiao Song, Senyuan Shi, Huijia Zhu, Weiqiang Wang, Jun Wan, Zhen Lei', 'link': 'https://arxiv.org/abs/2508.21048', 'abstract': 'Deepfake detection remains a formidable challenge due to the complex and evolving nature of fake content in real-world scenarios. However, existing academic benchmarks suffer from severe discrepancies from industrial practice, typically featuring homogeneous training sources and low-quality testing images, which hinder the practical deployments of current detectors. To mitigate this gap, we introduce HydraFake, a dataset that simulates real-world challenges with hierarchical generalization testing. Specifically, HydraFake involves diversified deepfake techniques and in-the-wild forgeries, along with rigorous training and evaluation protocol, covering unseen model architectures, emerging forgery techniques and novel data domains. Building on this resource, we propose Veritas, a multi-modal large language model (MLLM) based deepfake detector. Different from vanilla chain-of-thought (CoT), we introduce pattern-aware reasoning that involves critical reasoning patterns such as "planning" and "self-reflection" to emulate human forensic process. We further propose a two-stage training pipeline to seamlessly internalize such deepfake reasoning capacities into current MLLMs. Experiments on HydraFake dataset reveal that although previous detectors show great generalization on cross-model scenarios, they fall short on unseen forgeries and data domains. Our Veritas achieves significant gains across different OOD scenarios, and is capable of delivering transparent and faithful detection outputs.', 'abstract_zh': 'HydraFake：一种模拟现实挑战的多层次泛化测试集及基于多模态大语言模型的Deepfake检测方法', 'title_zh': 'Veritas: 基于模式感知推理的泛化深伪检测'}
{'arxiv_id': 'arXiv:2508.21016', 'title': 'Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance', 'authors': 'Luozhijie Jin, Zijie Qiu, Jie Liu, Zijie Diao, Lifeng Qiao, Ning Ding, Alex Lamb, Xipeng Qiu', 'link': 'https://arxiv.org/abs/2508.21016', 'abstract': "Denoising-based generative models, particularly diffusion and flow matching algorithms, have achieved remarkable success. However, aligning their output distributions with complex downstream objectives, such as human preferences, compositional accuracy, or data compressibility, remains challenging. While reinforcement learning (RL) fine-tuning methods, inspired by advances in RL from human feedback (RLHF) for large language models, have been adapted to these generative frameworks, current RL approaches are suboptimal for diffusion models and offer limited flexibility in controlling alignment strength after fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models through the lens of stochastic differential equations and implicit reward conditioning. We introduce Reinforcement Learning Guidance (RLG), an inference-time method that adapts Classifier-Free Guidance (CFG) by combining the outputs of the base and RL fine-tuned models via a geometric average. Our theoretical analysis shows that RLG's guidance scale is mathematically equivalent to adjusting the KL-regularization coefficient in standard RL objectives, enabling dynamic control over the alignment-quality trade-off without further training. Extensive experiments demonstrate that RLG consistently improves the performance of RL fine-tuned models across various architectures, RL algorithms, and downstream tasks, including human preferences, compositional control, compressibility, and text rendering. Furthermore, RLG supports both interpolation and extrapolation, thereby offering unprecedented flexibility in controlling generative alignment. Our approach provides a practical and theoretically sound solution for enhancing and controlling diffusion model alignment at inference. The source code for RLG is publicly available at the Github: this https URL.", 'abstract_zh': '基于去噪的生成模型，尤其是扩散和流匹配算法，已经取得了显著的成功。然而，将它们的输出分布与人类偏好、组分准确性或数据压缩性等复杂的下游目标对齐仍然具有挑战性。尽管借鉴了大规模语言模型从人类反馈强化学习（RLHF）中获得的进展，现有的强化学习（RL）微调方法已经被应用于这些生成框架中，但当前的RL方法并不适用于扩散模型，并且在微调后控制对齐强度方面的灵活性有限。在本文中，我们通过随机微分方程和隐式奖励条件的角度重新解释扩散模型的强化学习微调。我们引入了强化学习指导（RLG），这是一种推理时的方法，通过几何平均结合基模型和RL微调模型的输出来重新解释无分类器引导（CFG）。我们的理论分析表明，RLG的引导尺度数学上等同于调整标准RL目标中的KL正则化系数，从而使在不进行进一步训练的情况下动态控制对齐质量成为可能。广泛的实验表明，RLG能够在各种架构、RL算法和下游任务（包括人类偏好、组成控制、压缩性和文本渲染）中一致地提升RL微调模型的性能。此外，RLG支持插值和外推，从而提供了前所未有的控制生成对齐的灵活性。我们的方法提供了一种在推理时增强和控制扩散模型对齐的实用且有理论依据的解决方案。RLG的源代码已在Github上公开：this https URL。', 'title_zh': '基于强化学习指导的扩散模型 inference 时对齐控制'}
{'arxiv_id': 'arXiv:2508.20976', 'title': 'WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations', 'authors': 'Jaeyeon Kim, Heeseung Yun, Sang Hoon Woo, Chao-Han Huck Yang, Gunhee Kim', 'link': 'https://arxiv.org/abs/2508.20976', 'abstract': "Large audio language models (LALMs) extend language understanding into the auditory domain, yet their ability to perform low-level listening, such as pitch and duration detection, remains underexplored. However, low-level listening is critical for real-world, out-of-distribution tasks where models must reason about unfamiliar sounds based on fine-grained acoustic cues. To address this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to evaluate low-level auditory perception and cognition using marine mammal vocalizations. WoW-bench is composed of a Perception benchmark for categorizing novel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess the abilities to remember, understand, apply, and analyze sound events. For the Cognition benchmark, we additionally introduce distractor questions to evaluate whether models are truly solving problems through listening rather than relying on other heuristics. Experiments with state-of-the-art LALMs show performance far below human levels, indicating a need for stronger auditory grounding in LALMs.", 'abstract_zh': '大规模音频语言模型（LALMs）将语言理解扩展到了听觉领域，但其在低级听觉任务上的能力，如音高和时长检测，仍然被广泛忽视。为了填补这一空白，我们引入了鲸世界基准（WoW-Bench），通过海洋哺乳动物的 vocalizations 来评估低级听觉感知和认知能力。WoW-Bench 包括一个感知基准用于分类新型声音，以及一个借鉴布卢姆 taxonomy 设计的认知基准，用以评估模型对声音事件的记忆、理解、应用和分析能力。为了评估认知基准，我们还引入了干扰问题，以检验模型是否真正通过听觉解决问题，而不是依赖其他启发式方法。实验结果显示，最先进的 LALMs 的表现远低于人类水平，这表明 LALMs 需要更强的听觉基础。', 'title_zh': 'WoW-Bench: 通过海洋哺乳动物 vocalizations 评估音频语言模型的细粒度声学感知能力'}
{'arxiv_id': 'arXiv:2508.20973', 'title': 'ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents', 'authors': 'Tianjian Liu, Fanqi Wan, Jiajian Guo, Xiaojun Quan', 'link': 'https://arxiv.org/abs/2508.20973', 'abstract': "Proactive dialogue has emerged as a critical and challenging research problem in advancing large language models (LLMs). Existing works predominantly focus on domain-specific or task-oriented scenarios, which leads to fragmented evaluations and limits the comprehensive exploration of models' proactive conversation abilities. In this work, we propose ProactiveEval, a unified framework designed for evaluating proactive dialogue capabilities of LLMs. This framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics across various domains. Moreover, it also enables the automatic generation of diverse and challenging evaluation data. Based on the proposed framework, we develop 328 evaluation environments spanning 6 distinct domains. Through experiments with 22 different types of LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional performance on target planning and dialogue guidance tasks, respectively. Finally, we investigate how reasoning capabilities influence proactive behaviors and discuss their implications for future model development.", 'abstract_zh': '主动对话已成为促进大规模语言模型（LLMs）发展的关键性和挑战性研究问题。现有工作主要关注领域特定或任务导向的场景，这导致了评估的碎片化并限制了对模型主动对话能力的全面探索。在本文中，我们提出了一种名为ProactiveEval的统一框架，旨在评估LLMs的主动对话能力。该框架将主动对话分解为目标规划和对话指导，并在各种领域建立了评估指标。此外，它还能够自动生成多样化和具有挑战性的评估数据。基于提出的框架，我们开发了涵盖6个不同领域的328个评估环境。通过与22种不同类型的LLM进行的实验，我们展示了DeepSeek-R1和Claude-3.7-Sonnet分别在目标规划和对话指导任务中表现出色。最后，我们探讨了推理能力对主动行为的影响，并讨论了其对未来模型开发的意义。', 'title_zh': '前瞻Eval：统一的前瞻对话代理评估框架'}
{'arxiv_id': 'arXiv:2508.20912', 'title': 'Research Challenges in Relational Database Management Systems for LLM Queries', 'authors': 'Kerem Akillioglu, Anurag Chakraborty, Sairaj Voruganti, M. Tamer Özsu', 'link': 'https://arxiv.org/abs/2508.20912', 'abstract': "Large language models (LLMs) have become essential for applications such as text summarization, sentiment analysis, and automated question-answering. Recently, LLMs have also been integrated into relational database management systems to enhance querying and support advanced data processing. Companies such as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly within SQL, denoted as LLM queries, to boost data insights. However, open-source solutions currently have limited functionality and poor performance. In this work, we present an early exploration of two open-source systems and one enterprise platform, using five representative queries to expose functional, performance, and scalability limits in today's SQL-invoked LLM integrations. We identify three main issues: enforcing structured outputs, optimizing resource utilization, and improving query planning. We implemented initial solutions and observed improvements in accommodating LLM powered SQL queries. These early gains demonstrate that tighter integration of LLM+DBMS is the key to scalable and efficient processing of LLM queries.", 'abstract_zh': '大型语言模型（LLMs）已成为文本摘要、情感分析和自动问答等应用中的重要组成部分。最近，LLMs 还被集成到关系数据库管理系统中以增强查询并支持高级数据处理。Amazon、Databricks、Google 和 Snowflake 等公司提供直接在 SQL 中调用 LLMs 的服务，称为 LLMS 查询，以提升数据洞察力。然而，当前开源解决方案的功能有限且性能较差。本文介绍了对两个开源系统和一个企业平台的初步探索，使用五个代表性查询来揭示当今 SQL 调用 LLMS 集成的功能、性能和扩展性限制。我们识别出三个主要问题：强制结构化输出、优化资源利用和改进查询规划。我们实现了初步解决方案，并观察到在处理 LLMS 支持的 SQL 查询时有所改进。这些初步成果表明，LLM+DBMS 的更紧密集成是实现 LLMS 查询可扩展和高效处理的关键。', 'title_zh': '面向大型语言模型查询的关系数据库管理系统研究挑战'}
{'arxiv_id': 'arXiv:2508.20907', 'title': 'Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant', 'authors': 'Nicolas Dupuis, Adarsh Tiwari, Youssef Mroueh, David Kremer, Ismael Faro, Juan Cruz-Benito', 'link': 'https://arxiv.org/abs/2508.20907', 'abstract': 'Qiskit is an open-source quantum computing framework that allows users to design, simulate, and run quantum circuits on real quantum hardware. We explore post-training techniques for LLMs to assist in writing Qiskit code. We introduce quantum verification as an effective method for ensuring code quality and executability on quantum hardware. To support this, we developed a synthetic data pipeline that generates quantum problem-unit test pairs and used it to create preference data for aligning LLMs with DPO. Additionally, we trained models using GRPO, leveraging quantum-verifiable rewards provided by the quantum hardware. Our best-performing model, combining DPO and GRPO, surpasses the strongest open-source baselines on the challenging Qiskit-HumanEval-hard benchmark.', 'abstract_zh': 'Qiskit是一种开放源代码的量子计算框架，允许用户设计、模拟和在实际量子硬件上运行量子电路。我们探索了用于辅助编写Qiskit代码的LLM后训练技术。我们介绍了量子验证作为一种有效方法，用于确保代码在量子硬件上的质量和可执行性。为支持这一目标，我们开发了一个合成数据管道，生成量子问题单元测试对，并使用它来创建偏好数据以使LLMs与DPO对齐。此外，我们使用GRPO训练模型，并利用量子硬件提供的量子验证奖励。我们表现最好的模型，结合了DPO和GRPO，在具有挑战性的Qiskit-HumanEval-hard基准测试中超越了最强的开源基线。', 'title_zh': '量子可验证奖励后训练Qiskit代码助手'}
{'arxiv_id': 'arXiv:2508.20766', 'title': 'Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection', 'authors': 'Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, George Turkiyyah, Bernard Ghanem', 'link': 'https://arxiv.org/abs/2508.20766', 'abstract': "Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align 'uncensored' models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms.", 'abstract_zh': '大型语言模型（LLMs）中的安全性对齐常常涉及调解内部表示以拒绝有害请求。最近的研究表明，可以通过消除或移除模型中的特定表示方向来绕过这些安全机制。在本文中，我们提出相反的方法：秩一安全性注入（ROSI），这是一种白盒方法，通过永久性地引导模型的激活朝向拒绝调解子空间来增强模型的安全对齐。ROSI 作为一种简单的、无需微调的一秩权重修改应用到所有残差流写入矩阵。所需的 safety 方向可以从少量有害和无害指令对中计算得出。我们展示了 ROSE 在不牺牲模型在标准基准（如 MMLU、HellaSwag 和 Arc）上的实用性的同时，一致地增加了安全性拒绝率，同时还展示了 ROSI 可以通过放大其自身潜在的安全方向来重新对齐“非审查”模型，证明了其作为有效的最后一英里安全程序的效用。我们的结果表明，有针对性、可解释的权重引导是一种低成本且强大的机制，可以改进 LLM 的安全性，补充更为资源密集型的微调范式。', 'title_zh': '逆向转换：基于秩一安全性注入的轻量级对齐增强'}
{'arxiv_id': 'arXiv:2508.20755', 'title': 'Provable Benefits of In-Tool Learning for Large Language Models', 'authors': 'Sam Houliston, Ambroise Odonnat, Charles Arnal, Vivien Cabannes', 'link': 'https://arxiv.org/abs/2508.20755', 'abstract': 'Tool-augmented language models, equipped with retrieval, memory, or external APIs, are reshaping AI, yet their theoretical advantages remain underexplored. In this paper, we address this question by demonstrating the benefits of in-tool learning (external retrieval) over in-weight learning (memorization) for factual recall. We show that the number of facts a model can memorize solely in its weights is fundamentally limited by its parameter count. In contrast, we prove that tool-use enables unbounded factual recall via a simple and efficient circuit construction. These results are validated in controlled experiments, where tool-using models consistently outperform memorizing ones. We further show that for pretrained large language models, teaching tool-use and general rules is more effective than finetuning facts into memory. Our work provides both a theoretical and empirical foundation, establishing why tool-augmented workflows are not just practical, but provably more scalable.', 'abstract_zh': '工具增强的语言模型装备了检索、记忆或外部API，正在重塑AI，但其理论优势仍待深入探索。本文通过展示工具内学习（外部检索）相对于权重重学（记忆）在事实回忆方面的优势来回答这一问题。我们证明，模型仅在其权重内记忆事实的数量从根本上受其参数数量的限制。相比之下，我们证明工具使用能够通过简单的高效电路构造实现无限制的事实回忆。这些结果在受控实验中得到验证，工具使用模型始终优于记忆模型。进一步研究表明，对于预训练的大语言模型，教授工具使用和通用规则比将事实微调到记忆中更为有效。我们的工作提供了理论和实验证据，确立了工具增强的工作流不仅具有实用性，而且是可证明更具扩展性的。', 'title_zh': '工具内置学习对大型语言模型的可验证益处'}
{'arxiv_id': 'arXiv:2508.20737', 'title': 'Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol', 'authors': 'Wei Ma, Yixiao Yang, Qiang Hu, Shi Ying, Zhi Jin, Bo Du, Zhenchang Xing, Tianlin Li, Junjie Shi, Yang Liu, Linxiao Jiang', 'link': 'https://arxiv.org/abs/2508.20737', 'abstract': 'Applications of Large Language Models~(LLMs) have evolved from simple text generators into complex software systems that integrate retrieval augmentation, tool invocation, and multi-turn interactions. Their inherent non-determinism, dynamism, and context dependence pose fundamental challenges for quality assurance. This paper decomposes LLM applications into a three-layer architecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt Orchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess the applicability of traditional software testing methods in each layer: directly applicable at the shell layer, requiring semantic reinterpretation at the orchestration layer, and necessitating paradigm shifts at the inference core. A comparative analysis of Testing AI methods from the software engineering community and safety analysis techniques from the AI community reveals structural disconnects in testing unit abstraction, evaluation metrics, and lifecycle management. We identify four fundamental differences that underlie 6 core challenges. To address these, we propose four types of collaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate}, and \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance framework that combines pre-deployment validation with runtime monitoring. Based on these strategies, we offer practical guidance and a protocol proposal to support the standardization and tooling of LLM application testing. We propose a protocol \\textbf{\\textit{Agent Interaction Communication Language}} (AICL) that is used to communicate between AI agents. AICL has the test-oriented features and is easily integrated in the current agent framework.', 'abstract_zh': '大型语言模型应用中的质量保证：三层架构与协作策略', 'title_zh': '重思大语言模型应用的测试：特性、挑战及一种轻量级交互协议'}
{'arxiv_id': 'arXiv:2508.20637', 'title': 'GDS Agent: A Graph Algorithmic Reasoning Agent', 'authors': 'Borun Shi, Ioannis Panagiotas', 'link': 'https://arxiv.org/abs/2508.20637', 'abstract': 'Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We also introduce a new benchmark that evaluates intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.', 'abstract_zh': '大规模语言模型（LLMs）在多模态信息处理和推理方面表现出色。通过功能调用配备工具并结合检索增强技术，复合LLM基系统可以访问封闭数据源并回答相关问题。然而，它们仍然难以处理和推理大规模图结构数据。在本技术报告中，我们介绍了GDS（图数据科学）代理。GDS代理提供了一套全面的图算法作为工具，并在模型上下文协议（MCP）服务器中包含算法结果的预处理（检索）和后处理。该服务器可以与任何现代LLM无缝配合使用。GDS代理允许用户提出任何隐含和内在需要图算法推理的数据问题，并迅速获得准确且基于事实的回答。我们还引入了一个新的基准测试，用于评估中间工具调用以及最终响应。结果表明，GDS代理能够解决广泛的图任务。我们还提供了更开放任务的详细案例研究，并探讨了代理在某些场景中的困境。最后，我们讨论了剩余的挑战和未来 roadmap。', 'title_zh': 'GDS代理：图算法推理代理'}
{'arxiv_id': 'arXiv:2508.20583', 'title': "A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models", 'authors': 'Soham Petkar, Hari Aakash K, Anirudh Vempati, Akshit Sinha, Ponnurangam Kumarauguru, Chirag Agarwal', 'link': 'https://arxiv.org/abs/2508.20583', 'abstract': 'Developments in Graph-Language Models (GLMs) aim to integrate the structural reasoning capabilities of Graph Neural Networks (GNNs) with the semantic understanding of Large Language Models (LLMs). However, we demonstrate that current evaluation benchmarks for GLMs, which are primarily repurposed node-level classification datasets, are insufficient to assess multimodal reasoning. Our analysis reveals that strong performance on these benchmarks is achievable using unimodal information alone, suggesting that they do not necessitate graph-language integration. To address this evaluation gap, we introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed to evaluate multimodal reasoning at various complexity levels. Our benchmark employs a synthetic graph generation pipeline paired with questions that require joint reasoning over structure and textual semantics. We perform a thorough evaluation of representative GLM architectures and find that soft-prompted LLM baselines perform on par with GLMs that incorporate a full GNN backbone. This result calls into question the architectural necessity of incorporating graph structure into LLMs. We further show that GLMs exhibit significant performance degradation in tasks that require structural reasoning. These findings highlight limitations in the graph reasoning capabilities of current GLMs and provide a foundation for advancing the community toward explicit multimodal reasoning involving graph structure and language.', 'abstract_zh': '图语言模型（GLMs）的发展旨在将图神经网络（GNNs）的结构性推理能力与大型语言模型（LLMs）的语义理解能力结合起来。然而，我们证明当前用于评估GLMs的主要基准（主要是复用的节点级分类数据集）不足以评估多模态推理。我们的分析表明，仅通过单模态信息即可在这些基准上实现强大的性能，这暗示这些基准并不需要图语言集成。为解决这一评估缺口，我们引入了CLEGR（组合语言-图推理）基准，旨在评估不同复杂度水平的多模态推理。该基准结合了合成图形生成管道和需要在结构和文本语义上联合推理的问题。我们对代表性GLM架构进行了全面评估，并发现软提示的大语言模型基线与包含完整GNN骨干的GLMs性能相当。这一结果质疑将图结构集成到大语言模型中的必要性。我们进一步表明，在需要结构性推理的任务中，GLMs表现出显著的性能下降。这些发现突显了当前GLMs在图推理能力上的局限性，并为促进涉及图结构和语言的明确多模态推理奠定了基础。', 'title_zh': '图自证，谁在倾听？重新思考图语言模型的评估标准。'}
{'arxiv_id': 'arXiv:2508.20577', 'title': 'MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training', 'authors': 'Yang Luo, Zangwei Zheng, Ziheng Qin, Zirui Zhu, Yong Liu, Yang You', 'link': 'https://arxiv.org/abs/2508.20577', 'abstract': "Large-batch training has become a cornerstone in accelerating the training of deep neural networks, yet it poses challenges in optimization and generalization. Existing optimizers like AdamW present performance degradation during language models' large-batch training, due to the information bottleneck in attention layers caused by the sharp increase of max attention logit. While the LAMB optimizer partially addresses this issue, some attention layers still face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are less effective in directly influencing the max value of query/key weights. Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks relationships of weight values within rows or columns. Building on these observations, we propose a novel optimizer, MERIT, which leverages the max-norm to calculate the trust ratio to constrain the max attention logit more effectively. Moreover, we further construct element-wise trust ratios to provide more robust update scaling by focusing on local weight structures. Extensive experiments of large-batch training across various sizes of GPT-2 models demonstrate the superior performance of MERIT. Notably, during the training of GPT-2 Medium, MERIT enables a 6k batch size without any performance degradation compared to the standard batch size (480) with 48B training tokens. This work highlights the importance of considering the max attention logit and finer-granularity trust ratio in large-batch training. It successfully improves the training stability and paves the way for larger batch usage, enabling faster development and iteration of large language models. Code is available at this https URL.", 'abstract_zh': '大规模批次训练已成为加速深度神经网络训练的基石，但同时也带来了优化和泛化的挑战。现有优化器如AdamW在语言模型的大批次训练中表现下降，原因是注意力层中的最大注意力对数导致的信息瓶颈。虽然LAMB优化器部分解决了这一问题，但仍有一些注意力层面临此问题。原因是LAMB中基于$L_2$范数的信任比间接影响查询/键权重的最大值效果不佳。此外，LAMB中的元素级信任比容易出错，因为它忽略了行内或列内权重值之间的关系。基于这些观察，我们提出了一种新的优化器MERIT，它利用最大范数计算信任比以更有效地约束最大注意力对数。此外，我们还构建了元素级信任比，通过关注局部权重结构提供更稳健的更新缩放。针对不同规模的GPT-2模型的大批次训练实验表明，MERIT表现出更优的性能。特别是在GPT-2 Medium的训练中，MERIT使得使用6k批次大小而不影响性能，相较于标准批次大小（480）和480亿训练令牌。本工作强调了在大批次训练中考虑最大注意力对数和更精细粒度的信任比的重要性，成功提高了训练稳定性，并为更大批次的使用铺平了道路，促进了大语言模型的更快开发和迭代。代码可在以下链接获取。', 'title_zh': 'MERIT: 最大归一化元素比对语言模型大规模批次训练'}
{'arxiv_id': 'arXiv:2508.20443', 'title': 'Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Aware Unlearning with Proxy Constraint', 'authors': 'Zhihao Liu, Jian Lou, Yuke Hu, Xiaochen Li, Tailun Chen, Yitian Chen, Zhan Qin', 'link': 'https://arxiv.org/abs/2508.20443', 'abstract': 'Large language models (LLMs) are trained on massive datasets that may include private or copyrighted content. Due to growing privacy and ownership concerns, data owners may request the removal of their data from trained models. Machine unlearning provides a practical solution by removing the influence of specific data without full retraining. However, most existing methods lack a sound forgetting boundary, causing some samples to be under-forgotten, leaving residual leakage risks, while others remain over-forgotten at the expense of degraded utility.\nIn this work, we propose EAGLE-PC (Entanglement-Awareness Guided Loss Reweighting with Proxy Constraint), a novel unlearning framework that addresses these limitations through two key components. First, entanglement-awareness guided loss reweighting determines the forgetting effort of each sample by measuring its similarity to retain samples in the embedding space, enabling more targeted and effective unlearning. Second, a proxy constraint leveraging ICL (In-Context Learning) generated test data softly regularizes the forgetting process, effectively mitigating over-forgetting. EAGLE-PC is compatible with existing gradient-based objectives and serves as a plug-and-play enhancement. We evaluate EAGLE-PC on the TOFU and MUSE benchmarks, showing consistent improvements in the forgetting-utility trade-off across multiple LLMs. Combined with the NPO+GD optimizer, it approaches full retraining performance, offering a scalable and robust unlearning solution.', 'abstract_zh': '大型语言模型（LLMs）是在可能包含私人或受版权保护内容的庞大数据集上训练的。由于隐私和所有权方面的担忧日益增加，数据所有者可能会要求从训练模型中删除其数据。机器忘记提供了一种实际解决方案，通过移除特定数据的影响而不进行全面重训。然而，现有大多数方法缺乏坚实的遗忘边界，导致某些样本遗忘不足，留下残留泄漏风险，而其他样本则过度遗忘，牺牲了使用价值。\n在本文中，我们提出了一种新的机器忘记框架EAGLE-PC（Entanglement-Awareness Guided Loss Reweighting with Proxy Constraint），通过两个关键组件来解决这些限制问题。首先，结合纠缠意识指导的损失重新加权，通过测量每个样本与保留样本在嵌入空间中的相似性来确定遗忘努力，从而实现更精确和有效的机器忘记。其次，利用ICL（上下文学习）生成的测试数据引入的代理约束软化调节遗忘过程，有效缓解了过度遗忘。EAGLE-PC与现有基于梯度的目标兼容，并作为即插即用增强功能。我们在TOFU和MUSE基准上评估了EAGLE-PC，展示了在多个LLM上的遗忘-效用权衡中的一致改进。结合NPO+GD优化器，它可以接近全面重训的性能，提供一种可扩展且稳健的机器忘记解决方案。', 'title_zh': '面向代理约束下的纠缠感知遗忘，以减轻大模型过度遗忘'}
{'arxiv_id': 'arXiv:2508.20416', 'title': 'DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding', 'authors': 'Hengchuan Zhu, Yihuan Xu, Yichen Li, Zijie Meng, Zuozhu Liu', 'link': 'https://arxiv.org/abs/2508.20416', 'abstract': 'Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs) have demonstrated strong performance on general medical benchmarks. However, their capabilities in specialized medical fields, such as dentistry which require deeper domain-specific knowledge, remain underexplored due to the lack of targeted evaluation resources. In this paper, we introduce DentalBench, the first comprehensive bilingual benchmark designed to evaluate and advance LLMs in the dental domain. DentalBench consists of two main components: DentalQA, an English-Chinese question-answering (QA) benchmark with 36,597 questions spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale, high-quality corpus with 337.35 million tokens curated for dental domain adaptation, supporting both supervised fine-tuning (SFT) and retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering proprietary, open-source, and medical-specific models, and reveal significant performance gaps across task types and languages. Further experiments with Qwen-2.5-3B demonstrate that domain adaptation substantially improves model performance, particularly on knowledge-intensive and terminology-focused tasks, and highlight the importance of domain-specific benchmarks for developing trustworthy and effective LLMs tailored to healthcare applications.', 'abstract_zh': 'Recent Advances in Large Language Models and Medical LLMs: Introducing DentalBench, the First Comprehensive Bilingual Benchmark for Evaluating and Advancing LLMs in the Dental Domain', 'title_zh': 'DentalBench: 基于双语牙科理解能力评估与提升的基准测试'}
{'arxiv_id': 'arXiv:2508.20395', 'title': 'Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction', 'authors': 'Xu Guo', 'link': 'https://arxiv.org/abs/2508.20395', 'abstract': "Recent advancements in large language models (LLMs) often rely on generating intermediate reasoning steps to enhance accuracy. However, little work has examined how reasoning utility contributes to the final answer's correctness. Due to the stochastic nature of autoregressive generation, generating more context does not guarantee increased confidence in the answer. If we could predict, during generation, whether a reasoning step will be useful, we could stop early or prune ineffective steps, avoiding distractions in the final decision.\nWe present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to generate reasoning chains, and then employing a separate model (Qwen3-8B) to quantify the utility of these chains for final accuracy. Specifically, we measure the model's uncertainty on the answer span Y at each reasoning step using conditional entropy (expected negative log-likelihood over the vocabulary) with context expanding step by step. Our results show a clear pattern: conditional entropy that decreases over steps is strongly associated with correct answers, whereas flat or increasing entropy often results in wrong answers. We also corroborate that incorrect reasoning paths tend to be longer than correct ones, suggesting that longer reasoning does not necessarily yield better outcomes. These findings serve as a foundation to inspire future work on designing efficient reasoning pipelines that detect and avoid unproductive reasoning early.", 'abstract_zh': '最近大规模语言模型的进展往往依赖于生成中间推理步骤以提高准确性。然而，很少有研究探讨推理有用性如何影响最终答案的正确性。由于自回归生成的随机性，增加上下文并不保证回答的置信度提高。如果我们能在生成过程中预测某个推理步骤是否有用，便可以在必要时提前停止或去除无效步骤，从而避免最终决策中的干扰。\n\n我们使用Qwen2.5-32B和GPT-4o在MATH数据集上进行了一个先验研究，生成推理链，然后利用另一个单独的模型（Qwen3-8B）来量化这些链对于最终准确性的有用性。具体而言，我们使用条件熵（基于词汇的期望负对数似然）在每一步推理中度量模型对答案区间Y的不确定性，逐步扩展上下文。实验结果表明，条件熵在步骤中呈下降趋势与正确答案密切相关，而平稳或增加的熵通常会导致错误的答案。我们还验证了错误的推理路径往往比正确的路径更长，这表明更长的推理不一定能带来更好的结果。这些发现为进一步设计高效推理流水线提供了一个基础，该流水线能够在早期检测和避免无成效的推理。', 'title_zh': '通过条件熵减少测量LLM的推理有用性'}
{'arxiv_id': 'arXiv:2508.20373', 'title': 'Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems', 'authors': 'Yuyao Wang, Bowen Liu, Jianheng Tang, Nuo Chen, Yuhan Li, Qifan Zhang, Jia Li', 'link': 'https://arxiv.org/abs/2508.20373', 'abstract': 'Reasoning Large Language Models (RLLMs) have recently achieved remarkable progress on complex reasoning tasks, largely enabled by their long chain-of-thought (Long CoT) capabilities. However, developing these Long CoT behaviors relies heavily on post-training with high-quality datasets, which are typically costly and human-curated (e.g., mathematics and code), leaving scalable alternatives unexplored. In this work, we introduce NP-hard (NPH) graph problems as a novel synthetic training corpus, as they inherently require deep reasoning, extensive exploration, and reflective strategies, which are core characteristics of Long CoT reasoning. Building on this insight, we develop a two-stage post-training framework: (i) Long CoT Supervised Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a fine-grained reward design, which sharpens reasoning efficiency. Our flagship model, Graph-R1-7B, demonstrates strong generalization across mathematics, coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both accuracy and reasoning efficiency. These results position NPH graph problems as an effective and scalable resource for advancing Long CoT reasoning in LLMs, opening a new frontier for LLM post-training. Our implementation is available at this https URL, with models and datasets hosted in our Hugging Face collection HKUST-DSAIL/Graph-R1.', 'abstract_zh': '大规模语言模型（RLLMs）在复杂推理任务上的近期进展主要得益于它们的长链推理（Long CoT）能力。然而，发展这些Long CoT行为严重依赖于高质量数据集的后期训练，这类数据集通常成本高且需要人工筛选（如数学和代码），导致可扩展的替代方案未被探索。在本文中，我们引入NP难（NPH）图问题作为新颖的合成训练语料库，因为它们本质上要求深度推理、广泛探索和反思策略，这些都是Long CoT推理的核心特征。基于此，我们开发了一个两阶段后期训练框架：(i) 基于采样的NPH图实例的长链监督微调（SFT），显著增强了推理深度；(ii) 以精细设计的奖励机制为基础的强化学习（RL），提升了推理效率。我们的旗舰模型Graph-R1-7B在数学、编程、STEM和逻辑等多个领域表现出强大的泛化能力，并在NP难图问题上的准确性和推理效率上超越了QwQ-32B。这些结果表明NP难图问题是一个有效且可扩展的资源，有助于推进LLMs中的长链推理，开启了LLMs后期训练的新前沿。我们的实现可在以下链接获取：this https URL，模型和数据集托管在我们的Hugging Face集合HKUST-DSAIL/Graph-R1中。', 'title_zh': '图-R1：利用NP难图问题释放大语言模型的推理能力'}
{'arxiv_id': 'arXiv:2508.20340', 'title': 'Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators', 'authors': 'Maolin Sun, Yibiao Yang, Yuming Zhou', 'link': 'https://arxiv.org/abs/2508.20340', 'abstract': 'Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems and programming languages research, providing the foundation for tasks like symbolic execution and automated verification. Because these solvers sit on the critical path, their correctness is essential, and high-quality test formulas are key to uncovering bugs. However, while prior testing techniques performed well on earlier solver versions, they struggle to keep pace with rapidly evolving features. Recent approaches based on Large Language Models (LLMs) show promise in exploring advanced solver capabilities, but two obstacles remain: nearly half of the generated formulas are syntactically invalid, and iterative interactions with the LLMs introduce substantial computational overhead. In this study, we present Chimera, a novel LLM-assisted fuzzing framework that addresses both issues by shifting from direct formula generation to the synthesis of reusable term (i.e., logical expression) generators. Particularly, Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for SMT theories, including solver-specific extensions, from documentation, and (2) synthesize composable Boolean term generators that adhere to these grammars. During fuzzing, Chimera populates structural skeletons derived from existing formulas with the terms iteratively produced by the LLM-synthesized generators. This design ensures syntactic validity while promoting semantic diversity. Notably, Chimera requires only one-time LLM interaction investment, dramatically reducing runtime cost. We evaluated Chimera on two leading SMT solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43 confirmed bugs, 40 of which have already been fixed by developers.', 'abstract_zh': '模理论饱和性（SMT）求解器是现代系统和编程语言研究的基础，为符号执行和自动验证等任务提供支持。由于这些求解器位于关键路径上，其正确性至关重要，高质量的测试公式是发现错误的关键。然而，尽管之前的一些测试技术在早期版本的求解器上表现良好，但它们难以跟上快速演变的功能。基于大型语言模型（LLMs）的近期方法显示出探索求解器高级能力的潜力，但存在两个障碍：生成的公式中几乎有一半是语法无效的，与LLMs的迭代交互引入了显著的计算开销。在本研究中，我们提出了Chimera，这是一种新颖的LLM辅助模糊测试框架，通过从文档中自动提取模理论的上下文无关文法（CFG），包括求解器特定扩展，以及合成遵循这些文法的可组合布尔表达式生成器，来解决这两个问题。Chimera在模糊测试过程中，利用LLM合成的生成器逐步填充源自现有公式的结构骨架，确保语法有效性的同时促进语义多样性。值得注意的是，Chimera只需要一次LLM交互投资，显著降低了运行时成本。我们在两个领先SMT求解器：Z3和cvc5上评估了Chimera。实验结果表明，Chimera发现了43个已确认的错误，其中40个已经被开发者修复。', 'title_zh': '利用大型语言模型生成公式生成器以增强基于骨架的SMT求解器模糊测试'}
{'arxiv_id': 'arXiv:2508.20333', 'title': 'Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs', 'authors': 'Md Abdullah Al Mamun, Ihsen Alouani, Nael Abu-Ghazaleh', 'link': 'https://arxiv.org/abs/2508.20333', 'abstract': "Large Language Models (LLMs) are aligned to meet ethical standards and safety requirements by training them to refuse answering harmful or unsafe prompts. In this paper, we demonstrate how adversaries can exploit LLMs' alignment to implant bias, or enforce targeted censorship without degrading the model's responsiveness to unrelated topics. Specifically, we propose Subversive Alignment Injection (SAI), a poisoning attack that leverages the alignment mechanism to trigger refusal on specific topics or queries predefined by the adversary. Although it is perhaps not surprising that refusal can be induced through overalignment, we demonstrate how this refusal can be exploited to inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning defenses including LLM state forensics, as well as robust aggregation techniques that are designed to detect poisoning in FL settings. We demonstrate the practical dangers of this attack by illustrating its end-to-end impacts on LLM-powered application pipelines. For chat based applications such as ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare questions to targeted racial category leading to high bias ($\\Delta DP$ of 23%). We also show that bias can be induced in other NLP tasks: for a resume selection pipeline aligned to refuse to summarize CVs from a selected university, high bias in selection ($\\Delta DP$ of 27%) results. Even higher bias ($\\Delta DP$~38%) results on 9 other chat based downstream applications.", 'abstract_zh': 'Large Language Models (LLMs)通过训练拒绝回答有害或不安全的提示来对齐以符合伦理标准和安全要求。本文展示了对手如何利用LLM的对齐机制植入偏见或执行有针对性的审查，而不降低模型对无关话题的响应能力。具体来说，我们提出了一种颠覆性对齐注入（SAI）攻击，该攻击利用对齐机制在由对手预定义的主题或查询上触发拒绝。尽管通过过度对齐诱导拒绝或许不令人惊讶，我们展示了如何利用这种拒绝注入偏见到模型中。令人惊讶的是，SAI能够避开最先进的中毒防御措施，包括LLM状态法医分析以及专为FL环境设计的健壮聚合技术。我们通过展示该攻击对LLM驱动的应用管道的端到端影响，阐明其实用危险。对于类似于ChatDoctor的聊天应用，1%的数据中毒会导致系统拒绝回答针对特定种族类别的医疗问题，产生高偏见($\\Delta DP$为23%)。我们还展示了偏见可以在其他NLP任务中被诱导：在一项旨在拒绝总结指定大学简历的招聘流程中，选择偏见($\\Delta DP$为27%)增加。在9个其他聊天驱动的下游应用中，偏见甚至更高($\\Delta DP$约为38%)。', 'title_zh': '一次投毒，永绝后患：利用对齐武器化注入LLMs偏见'}
{'arxiv_id': 'arXiv:2508.20325', 'title': 'GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs', 'authors': 'Haibo Jin, Ruoxi Chen, Peiyan Zhang, Andy Zhou, Yang Zhang, Haohan Wang', 'link': 'https://arxiv.org/abs/2508.20325', 'abstract': "As Large Language Models become increasingly integral to various domains, their potential to generate harmful responses has prompted significant societal and regulatory concerns. In response, governments have issued ethics guidelines to promote the development of trustworthy AI. However, these guidelines are typically high-level demands for developers and testers, leaving a gap in translating them into actionable testing questions to verify LLM compliance.\nTo address this challenge, we introduce GUARD (\\textbf{G}uideline \\textbf{U}pholding Test through \\textbf{A}daptive \\textbf{R}ole-play and Jailbreak \\textbf{D}iagnostics), a testing method designed to operationalize guidelines into specific guideline-violating questions that assess LLM adherence. To implement this, GUARD uses automated generation of guideline-violating questions based on government-issued guidelines, thereby testing whether responses comply with these guidelines. When responses directly violate guidelines, GUARD reports inconsistencies. Furthermore, for responses that do not directly violate guidelines, GUARD integrates the concept of ``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that provoke unethical or guideline-violating responses, effectively identifying potential scenarios that could bypass built-in safety mechanisms. Our method finally culminates in a compliance report, delineating the extent of adherence and highlighting any violations.\nWe have empirically validated the effectiveness of GUARD on seven LLMs, including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, and Claude-3.7, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can transfer jailbreak diagnostics to vision-language models, demonstrating its usage in promoting reliable LLM-based applications.", 'abstract_zh': '大型语言模型在各领域中的作用日益重要，其生成有害响应的可能性引起了广泛的社会和监管关注。为应对这一问题，政府发布了伦理指导原则以促进值得信赖的AI发展。然而，这些指导原则通常是对开发者和测试人员的高层次要求，缺乏将这些要求转化为可操作的测试问题以验证LLM合规性的手段。\n\n为此，我们提出了GUARD（Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics）测试方法，旨在将指导原则具体化为特定的指导原则违反问题，以评估LLM的合规性。GUARD通过基于政府发布的指导原则自动生成指导原则违反的问题来实施这一方法，从而测试响应是否符合这些指导原则。当响应直接违反指导原则时，GUARD报告不一致性。此外，对于未直接违反指导原则的响应，GUARD结合“越狱”概念进行诊断，名为GUARD-JD，它创建触发不良行为或指导原则违反的场景，有效地识别可能绕过内置安全机制的潜在场景。最终，我们的方法形成了一份合规报告，详细描述合规程度并突出任何违规行为。\n\n我们通过在七种LLM（包括Vicuna-13B、LongChat-7B、Llama2-7B、Llama-3-8B、GPT-3.5、GPT-4、GPT-4o和Claude-3.7）上进行合规性测试和“越狱”诊断验证了GUARD的有效性，这些测试基于政府发布的三条指导原则。此外，GUARD-JD还可以将“越狱”诊断应用于视觉语言模型，展示了其在促进可靠LLM基础应用方面的作用。', 'title_zh': 'GUARD: 遵循指南测试通过自适应角色扮演和监狱逃脱诊断对于大规模语言模型'}
{'arxiv_id': 'arXiv:2508.20258', 'title': 'SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization', 'authors': 'Arya Tschand, Muhammad Awad, Ryan Swann, Kesavan Ramakrishnan, Jeffrey Ma, Keith Lowery, Ganesh Dasika, Vijay Janapa Reddi', 'link': 'https://arxiv.org/abs/2508.20258', 'abstract': "Large language models (LLMs) have shown progress in GPU kernel performance engineering using inefficient search-based methods that optimize around runtime. Any existing approach lacks a key characteristic that human performance engineers rely on for near-optimal utilization -- hardware-awareness. By leveraging the workload's specific memory access patterns, architecture specifications, filtered profiling logs, and reflections on historical performance, we can make software-level optimizations that are tailored to the underlying hardware. SwizzlePerf automatically generates spatial optimizations for GPU kernels on disaggregated architectures by giving LLMs explicit hardware-awareness.\nFor a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same hardware-specific optimal swizzling pattern that took expert performance engineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels, SwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve up to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the first of many steps toward systematically creating hardware-aware LLM performance engineering agents.", 'abstract_zh': '大型语言模型（LLMs）在使用基于搜索的不高效方法进行GPU内核性能工程方面取得了进展，这些方法侧重优化运行时性能。现有的任何方法都不具备人类性能工程师实现接近最优利用的关键特性——硬件意识。通过利用工作负载的特定内存访问模式、架构规范、过滤后的性能日志以及历史性能的反思，我们可以在软件层面进行针对底层硬件的定制化优化。SwizzlePerf自动为分拆架构生成GPU内核的空间优化，通过赋予LLMs显式的硬件意识来实现这一目标。\n\n对于一个GEMM内核，SwizzlePerf在不到5分钟内生成了专家性能工程师花费2周才找到的硬件特定最优交织模式。在一系列10个不同的机器学习和科学内核中，SwizzlePerf能够为9个内核生成高达2.06倍速度提升和70%的L2命中率提升的交织模式。这项工作是系统创建硬件意识的LLM性能工程代理的第一步。', 'title_zh': 'SwizzlePerf: 兼顾硬件的LLMGPU内核性能优化'}
{'arxiv_id': 'arXiv:2508.20234', 'title': 'Validating Generative Agent-Based Models for Logistics and Supply Chain Management Research', 'authors': 'Vincent E. Castillo', 'link': 'https://arxiv.org/abs/2508.20234', 'abstract': 'Generative Agent-Based Models (GABMs) powered by large language models (LLMs) offer promising potential for empirical logistics and supply chain management (LSCM) research by enabling realistic simulation of complex human behaviors. Unlike traditional agent-based models, GABMs generate human-like responses through natural language reasoning, which creates potential for new perspectives on emergent LSCM phenomena. However, the validity of LLMs as proxies for human behavior in LSCM simulations is unknown. This study evaluates LLM equivalence of human behavior through a controlled experiment examining dyadic customer-worker engagements in food delivery scenarios. I test six state-of-the-art LLMs against 957 human participants (477 dyads) using a moderated mediation design. This study reveals a need to validate GABMs on two levels: (1) human equivalence testing, and (2) decision process validation. Results reveal GABMs can effectively simulate human behaviors in LSCM; however, an equivalence-versus-process paradox emerges. While a series of Two One-Sided Tests (TOST) for equivalence reveals some LLMs demonstrate surface-level equivalence to humans, structural equation modeling (SEM) reveals artificial decision processes not present in human participants for some LLMs. These findings show GABMs as a potentially viable methodological instrument in LSCM with proper validation checks. The dual-validation framework also provides LSCM researchers with a guide to rigorous GABM development. For practitioners, this study offers evidence-based assessment for LLM selection for operational tasks.', 'abstract_zh': '由大规模语言模型（LLMs）驱动的生成型基于代理模型（GABMs）为物流和供应链管理（LSCM）研究提供了有前景的潜力，通过实现复杂人类行为的现实模拟。与传统基于代理模型不同，GABMs通过自然语言推理生成类似人类的响应，这为新兴LSCM现象提供了新的视角。然而，LLMs在LSCM仿真中作为人类行为代理的有效性尚不明晰。本研究通过一个受控实验评估LLMs在食品配送场景中双边客户-工人交互中的等效性，使用调节中介设计测试了六种最先进的LLMs，共有477个双边人类参与者。研究结果揭示了GABMs在LSCM中模拟人类行为的有效性，但同时出现了等效性与过程验证的悖论。虽然等价性检验（TOST）揭示了一些LLMs在表面行为上表现出等效性，但结构方程建模（SEM）揭示了一些LLMs中存在人类参与者不具备的人工决策过程。这些发现表明，在适当验证检查的前提下，GABMs可能是LSCM研究中的一个潜在可行的方法工具。双重验证框架也为LSCM研究人员提供了严格的GABM开发指南。对于从业者而言，本研究提供了基于证据的LLM选择评估，以应用于操作任务。', 'title_zh': '验证生成性基于代理的模型在物流与供应链管理研究中的有效性'}
{'arxiv_id': 'arXiv:2508.20217', 'title': 'Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models', 'authors': 'Mohammad Amini, Babak Ahmadi, Xiaomeng Xiong, Yilin Zhang, Christopher Qiao', 'link': 'https://arxiv.org/abs/2508.20217', 'abstract': "This study explores automatic generation (AIG) using language models to create multiple choice questions (MCQs) for morphological assessment, aiming to reduce the cost and inconsistency of manual test development. The study used a two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B) with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven structured prompting strategies, including zero-shot, few-shot, chain-of-thought, role-based, sequential, and combinations. Generated items were assessed using automated metrics and expert scoring across five dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate human scoring at scale. Results show that structured prompting, especially strategies combining chain-of-thought and sequential design, significantly improved Gemma's outputs. Gemma generally produced more construct-aligned and instructionally appropriate items than GPT-3.5's zero-shot responses, with prompt design playing a key role in mid-size model performance. This study demonstrates that structured prompting and efficient fine-tuning can enhance midsized models for AIG under limited data conditions. We highlight the value of combining automated metrics, expert judgment, and large-model simulation to ensure alignment with assessment goals. The proposed workflow offers a practical and scalable way to develop and validate language assessment items for K-12.", 'abstract_zh': '本研究探讨了使用语言模型自动生成（AIG）形态学评估的多项选择题（MCQs），旨在减少手动测试开发的成本和不一致性。研究采用了两步方法。首先，我们将微调的中型模型（Gemma，2B）与未微调的大型模型（GPT-3.5，175B）进行了比较。其次，我们评估了七种结构化提示策略，包括零样本、少量样本、思考链、角色扮演、序列化以及这些策略的组合。生成的试题通过自动化指标和专家评分在五个维度上进行了评估。我们还使用了基于专家评分样本训练的GPT-4.1来大规模模拟人工评分。结果显示，结构化提示，尤其是结合思考链和序列化设计的策略，显著提升了Gemma的输出效果。Gemma一般比GPT-3.5的零样本响应生成了更多符合建构和教学导向的试题，提示设计对于中型模型的表现起着关键作用。本研究展示了，在数据有限条件下，结构化提示和高效微调可以增强中型模型的AIG能力。本研究强调了结合自动化指标、专家判断和大型模型模拟的价值，以确保评估目标的一致性。提出的工作流程为K-12语言评估试题的开发和验证提供了一种实用且可扩展的方法。', 'title_zh': '基于语言模型的K-12教育试题生成中提示策略：缩小小型与大型语言模型之间的差距'}
{'arxiv_id': 'arXiv:2508.20186', 'title': 'AI Propaganda factories with language models', 'authors': 'Lukasz Olejnik', 'link': 'https://arxiv.org/abs/2508.20186', 'abstract': 'AI-powered influence operations can now be executed end-to-end on commodity hardware. We show that small language models produce coherent, persona-driven political messaging and can be evaluated automatically without human raters. Two behavioural findings emerge. First, persona-over-model: persona design explains behaviour more than model identity. Second, engagement as a stressor: when replies must counter-arguments, ideological adherence strengthens and the prevalence of extreme content increases. We demonstrate that fully automated influence-content production is within reach of both large and small actors. Consequently, defence should shift from restricting model access towards conversation-centric detection and disruption of campaigns and coordination infrastructure. Paradoxically, the very consistency that enables these operations also provides a detection signature.', 'abstract_zh': '基于AI的动力操作现在可以在商用硬件上端到端执行。我们展示，小型语言模型能够生成连贯的、以角色为中心的政治信息，并且可以在没有人类评阅者的情况下自动评估。研究发现了两种行为现象：首先，角色超越模型：角色设计比模型身份更能解释行为。其次，互动作为压力源：当回复必须对抗论点时，意识形态坚持会加强，极端内容的出现频率增加。我们证明，完全自动化的影响力内容生产对于大范围的参与者而言是可及的。因此，防御策略应从限制模型访问转向以对话为中心的活动检测与中断，破坏 campaigns 和协调基础设施。矛盾的是，正是这些操作的这种一致性提供了检测的特征。', 'title_zh': 'AI propaganda factories with language models'}
{'arxiv_id': 'arXiv:2508.20181', 'title': 'Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization', 'authors': 'Alberto Compagnoni, Davide Caffagni, Nicholas Moratelli, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara', 'link': 'https://arxiv.org/abs/2508.20181', 'abstract': "Multimodal Large Language Models (MLLMs) emerge as a unified interface to address a multitude of tasks, ranging from NLP to computer vision. Despite showcasing state-of-the-art results in many benchmarks, a long-standing issue is the tendency of MLLMs to hallucinate, that is to generate answers to the user's query that are not reflected in the visual input. In this paper, we address the problem of hallucinations as an alignment problem, seeking to steer the MLLM so that it prefers generating content without hallucinations. In contrast to recent approaches that require complicated pipelines to build synthetic preference data for alignment training, often relying on proprietary models, we capitalize on the well-known CHAIR metric, originally proposed to gauge the degree of hallucinations in image captioning. Given a pair of generated answers, we leverage CHAIR to distinguish winner and loser options (i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf MLLMs via Direct Preference Optimization (DPO). The resulting method, which we refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated answers on several hallucination benchmarks, demonstrating the effectiveness of fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models are publicly available at this https URL.", 'abstract_zh': '多模态大型语言模型（MLLMs）作为统一接口解决从自然语言处理到计算机视觉等多种任务。尽管在许多基准测试中展示了最先进的结果，但长期存在的问题是MLLMs倾向于幻觉，即生成与视觉输入不符的答案。在本文中，我们将幻觉问题视为对齐问题，旨在引导MLLM生成无幻觉的内容。与需要复杂流程构建合成偏好数据进行对齐训练、通常依赖专有模型的 Recent 方法不同，我们利用最初用于评估图像描述中幻觉程度的 CHAIR 指标。通过一对生成的答案，我们利用 CHAIR 来区分优胜和劣败选项（即无幻觉和幻觉样本），并通过直接偏好优化（DPO）微调现成的 MLLM。我们称之为 CHAIR-DPO 的方法有效地减少了多个幻觉基准上的幻觉答案数量，证明了使用基于CHAIR的奖励微调MLLM的有效性。相关源代码和训练模型可从此链接访问。', 'title_zh': '通过对象感知的偏好优化减轻多模态LLM的幻觉现象'}
{'arxiv_id': 'arXiv:2508.20115', 'title': 'Flexible metadata harvesting for ecology using large language models', 'authors': 'Zehao Lu, Thijs L van der Plas, Parinaz Rashidi, W Daniel Kissling, Ioannis N Athanasiadis', 'link': 'https://arxiv.org/abs/2508.20115', 'abstract': "Large, open datasets can accelerate ecological research, particularly by enabling researchers to develop new insights by reusing datasets from multiple sources. However, to find the most suitable datasets to combine and integrate, researchers must navigate diverse ecological and environmental data provider platforms with varying metadata availability and standards. To overcome this obstacle, we have developed a large language model (LLM)-based metadata harvester that flexibly extracts metadata from any dataset's landing page, and converts these to a user-defined, unified format using existing metadata standards. We validate that our tool is able to extract both structured and unstructured metadata with equal accuracy, aided by our LLM post-processing protocol. Furthermore, we utilise LLMs to identify links between datasets, both by calculating embedding similarity and by unifying the formats of extracted metadata to enable rule-based processing. Our tool, which flexibly links the metadata of different datasets, can therefore be used for ontology creation or graph-based queries, for example, to find relevant ecological and environmental datasets in a virtual research environment.", 'abstract_zh': '大规模开放数据集可以加速生态研究，特别是在通过从多个来源重新使用数据集来开发新的见解方面。然而，为了找到最适合组合和集成的数据集，研究人员必须导航不同的生态和环境数据提供商平台，这些平台的元数据可用性和标准各不相同。为了克服这一障碍，我们开发了一种基于大规模语言模型（LLM）的元数据收割器，它可以灵活地从任何数据集的landing page提取元数据，并使用现有的元数据标准将这些转换为用户定义的统一格式。我们验证我们的工具能够以相同的准确性提取结构化和非结构化元数据，并利用我们的LLM后处理协议加以辅助。此外，我们使用LLM来识别数据集之间的链接，通过计算嵌入相似性和统一提取的元数据格式以实现基于规则的处理。因此，该工具可以灵活地链接不同数据集的元数据，例如用于本体创建或图查询，以在虚拟研究环境中查找相关的生态和环境数据集。', 'title_zh': '使用大型语言模型进行生态学的灵活元数据收割'}
{'arxiv_id': 'arXiv:2508.20097', 'title': 'Can LLMs Identify Tax Abuse?', 'authors': 'Andrew Blair-Stanek, Nils Holzenberger, Benjamin Van Durme', 'link': 'https://arxiv.org/abs/2508.20097', 'abstract': "We investigate whether large language models can discover and analyze U.S. tax-minimization strategies. This real-world domain challenges even seasoned human experts, and progress can reduce tax revenue lost from well-advised, wealthy taxpayers. We evaluate the most advanced LLMs on their ability to (1) interpret and verify tax strategies, (2) fill in gaps in partially specified strategies, and (3) generate complete, end-to-end strategies from scratch. This domain should be of particular interest to the LLM reasoning community: unlike synthetic challenge problems or scientific reasoning tasks, U.S. tax law involves navigating hundreds of thousands of pages of statutes, case law, and administrative guidance, all updated regularly. Notably, LLM-based reasoning identified an entirely novel tax strategy, highlighting these models' potential to revolutionize tax agencies' fight against tax abuse.", 'abstract_zh': '我们研究大型语言模型是否能够发现和分析美国税收最小化策略。这一现实领域的挑战甚至难倒了资深的人类专家，进展可以减少由于有良好建议的富裕纳税人导致的税收收入流失。我们评估最先进的LLM在以下方面的能力：（1）解释和验证税务策略，（2）填充部分指定策略中的空白，（3）从零开始生成完整、端到端的策略。这一领域对LLM推理社区尤为重要：与合成挑战问题或科学推理任务不同，美国税法涉及导航成百上千页的法典、判例法和行政指导，并且这些内容经常更新。值得注意的是，基于LLM的推理识别出一种全新的税务策略，突显了这些模型有潜力彻底改变税务机构打击税收欺诈的方式。', 'title_zh': 'LLM能否识别税务舞弊？'}
