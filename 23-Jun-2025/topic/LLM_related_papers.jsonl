{'arxiv_id': 'arXiv:2506.15828', 'title': 'Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning', 'authors': 'Emanuele Musumeci, Michele Brienza, Francesco Argenziano, Vincenzo Suriani, Daniele Nardi, Domenico D. Bloisi', 'link': 'https://arxiv.org/abs/2506.15828', 'abstract': "Classical planning in AI and Robotics addresses complex tasks by shifting from imperative to declarative approaches (e.g., PDDL). However, these methods often fail in real scenarios due to limited robot perception and the need to ground perceptions to planning predicates. This often results in heavily hard-coded behaviors that struggle to adapt, even with scenarios where goals can be achieved through relaxed planning. Meanwhile, Large Language Models (LLMs) lead to planning systems that leverage commonsense reasoning but often at the cost of generating unfeasible and/or unsafe plans. To address these limitations, we present an approach integrating classical planning with LLMs, leveraging their ability to extract commonsense knowledge and ground actions. We propose a hierarchical formulation that enables robots to make unfeasible tasks tractable by defining functionally equivalent goals through gradual relaxation. This mechanism supports partial achievement of the intended objective, suited to the agent's specific context. Our method demonstrates its ability to adapt and execute tasks effectively within environments modeled using 3D Scene Graphs through comprehensive qualitative and quantitative evaluations. We also show how this method succeeds in complex scenarios where other benchmark methods are more likely to fail. Code, dataset, and additional material are released to the community.", 'abstract_zh': '经典AI与机器人规划通过从命令式转变为声明式方法（如PDDL）来应对复杂的任务。然而，这些方法在实际场景中往往由于机器人感知能力有限以及需要将感知与规划谓词关联而失败，这导致了高度硬编码的行为，即使在可以实现目标的松弛规划场景中也难以适应。同时，大规模语言模型（LLMs）虽然可以通过常识推理增强规划系统，但往往会产生不可行和/或不安全的计划。为解决这些限制，我们提出了一种结合经典规划与LLMs的方法，利用LLMs提取常识知识并限制动作的能力。我们提出了一种分层公式化方法，通过逐步放松定义功能等效的目标来使不可行的任务变得可处理。该机制支持在特定上下文中部分实现预期目标。我们的方法通过使用3D场景图建模的环境进行全面定性和定量评估，展示了其适应和有效执行任务的能力。此外，我们展示了该方法在基准方法更 likely 失败的复杂场景中取得成功。我们向社区发布了代码、数据集和额外的资料。', 'title_zh': '背景因素很重要！借助LLM实现可行的3D场景规划'}
{'arxiv_id': 'arXiv:2506.17163', 'title': 'The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making', 'authors': 'Abinitha Gourabathina, Yuexing Hao, Walter Gerych, Marzyeh Ghassemi', 'link': 'https://arxiv.org/abs/2506.17163', 'abstract': 'Clinical robustness is critical to the safe deployment of medical Large Language Models (LLMs), but key questions remain about how LLMs and humans may differ in response to the real-world variability typified by clinical settings. To address this, we introduce MedPerturb, a dataset designed to systematically evaluate medical LLMs under controlled perturbations of clinical input. MedPerturb consists of clinical vignettes spanning a range of pathologies, each transformed along three axes: (1) gender modifications (e.g., gender-swapping or gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial tone); and (3) format changes (e.g., LLM-generated multi-turn conversations or summaries). With MedPerturb, we release a dataset of 800 clinical contexts grounded in realistic input variability, outputs from four LLMs, and three human expert reads per clinical context. We use MedPerturb in two case studies to reveal how shifts in gender identity cues, language style, or format reflect diverging treatment selections between humans and LLMs. We find that LLMs are more sensitive to gender and style perturbations while human annotators are more sensitive to LLM-generated format perturbations such as clinical summaries. Our results highlight the need for evaluation frameworks that go beyond static benchmarks to assess the similarity between human clinician and LLM decisions under the variability characteristic of clinical settings.', 'abstract_zh': '医学稳健性是将医疗大型语言模型（LLMs）安全部署的关键，但在面对典型临床环境变量时，LLMs与人类的响应差异方面仍存在关键问题。为解决这一问题，我们引入了MedPerturb数据集，旨在通过控制临床输入的变形系统性评估医疗LLMs。MedPerturb包含跨越多种病理的临床案例概要，每种概要沿三个轴线进行变形：（1）性别修改（例如，性别互换或性别移除）；（2）风格变化（例如，不确定用词或口语化语气）；以及（3）格式变化（例如，LLM生成的多轮对话或总结）。通过MedPerturb，我们发布了包含800个基于现实输入变异的临床上下文的数据集，每个临床上下文有四种LLM的输出和三种人类专家的阅读。我们使用MedPerturb在两个案例研究中揭示性别身份提示、语言风格或格式的变化如何反映人类和LLMs之间治疗选择的差异。结果表明，LLMs对性别和风格变形更为敏感，而人类注释者对LLM生成的格式变形，如临床总结更为敏感。我们的研究结果强调了评估框架的需求，该框架应超越静止基准，以临床环境中的变量特征评估人类临床医生和LLMs决策的相似性。', 'title_zh': 'MedPerturb数据集：非内容扰动揭示的人类和临床LLM决策机制'}
{'arxiv_id': 'arXiv:2506.17111', 'title': 'Are Bias Evaluation Methods Biased ?', 'authors': 'Lina Berrayana, Sean Rooney, Luis Garcés-Erice, Ioana Giurgiu', 'link': 'https://arxiv.org/abs/2506.17111', 'abstract': 'The creation of benchmarks to evaluate the safety of Large Language Models is one of the key activities within the trusted AI community. These benchmarks allow models to be compared for different aspects of safety such as toxicity, bias, harmful behavior etc. Independent benchmarks adopt different approaches with distinct data sets and evaluation methods. We investigate how robust such benchmarks are by using different approaches to rank a set of representative models for bias and compare how similar are the overall rankings. We show that different but widely used bias evaluations methods result in disparate model rankings. We conclude with recommendations for the community in the usage of such benchmarks.', 'abstract_zh': '基于可信AI社区的评估大语言模型安全性的基准创建是关键活动之一。这些基准使得模型可以从毒性和偏见等不同方面进行比较。独立基准采用不同的方法并使用不同的数据集和评估方法。我们通过使用不同的方法对一组代表性模型进行排名，并比较这些模型的整体排名相似性，以考察这些基准的稳健性。我们展示了不同但广泛使用的偏见评估方法会导致不同的模型排名。最后，我们为社区在使用此类基准方面提供建议。', 'title_zh': '偏差评估方法本身存在偏差吗？'}
{'arxiv_id': 'arXiv:2506.17104', 'title': 'Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving', 'authors': 'Chuxue Cao, Mengze Li, Juntao Dai, Jinluan Yang, Zijian Zhao, Shengyu Zhang, Weijie Shi, Chengzhong Liu, Sirui Han, Yike Guo', 'link': 'https://arxiv.org/abs/2506.17104', 'abstract': "Large language models (LLMs) have shown promising first-order logic (FOL) reasoning capabilities with applications in various areas. However, their effectiveness in complex mathematical reasoning involving multi-step FOL deductions is still under-researched. While LLMs perform competitively on established mathematical reasoning benchmarks, they struggle with multi-step FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on our proposed theorem proving dataset. This issue arises from the limited exploration of diverse proof strategies and the potential for early reasoning mistakes to undermine entire proofs. To address these issues, we propose DREAM, a self-adaptive solution that enhances the Diversity and REAsonability of LLMs' generation strategies. DREAM incorporates an Axiom-Driven Strategy Diversification mechanism to promote varied strategic outcomes and a Sub-Proposition Error Feedback to help LLMs reflect on and correct their proofs. Our contributions include pioneering advancements in LLMs' mathematical reasoning through FOL theorem proving, introducing a novel inference stage solution that improves performance by 0.6% to 6.4%, and providing a curated dataset of 447 mathematical theorems in Lean 4 format for evaluation.", 'abstract_zh': '大型语言模型（LLMs）在各类应用中展示了有前途的一阶逻辑（FOL）推理能力。然而，它们在涉及多步FOL演绎的复杂数学推理方面的有效性仍需进一步研究。尽管LLMs在现有的数学推理基准测试中表现得当，但在处理多步FOL任务时依然面临挑战，如我们提出的定理证明数据集上Deepseek-Prover-V2-7B的低准确性（4.2%）。这一问题源于对多样化证明策略的探索有限以及早期推理错误可能削弱整个证明。为解决这些问题，我们提出DREAM，一种自我适应的解决方案，旨在增强LLMs生成策略的多样性和合理性。DREAM结合了公理驱动的战略多样化机制以促进多样化的战略结果，并采用了子命题错误反馈机制以帮助LLMs反思和修正其证明。我们的贡献包括在通过一阶逻辑定理证明增强LLMs的数学推理方面的开创性进展，引入了一种新型的推理阶段解决方案，通过提高0.6%到6.4%的性能，以及提供了447个数学定理的精心制作数据集，用于在Lean 4格式下的评估。', 'title_zh': '基于一阶逻辑定理证明的高级数学推理能力提升方法'}
{'arxiv_id': 'arXiv:2506.16575', 'title': 'Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System', 'authors': 'Mustafa Akben, Aaron Satko', 'link': 'https://arxiv.org/abs/2506.16575', 'abstract': 'Large language models (LLMs) offer promising opportunities for organizational research. However, their built-in moderation systems can create problems when researchers try to analyze harmful content, often refusing to follow certain instructions or producing overly cautious responses that undermine validity of the results. This is particularly problematic when analyzing organizational conflicts such as microaggressions or hate speech. This paper introduces an Elo rating-based method that significantly improves LLM performance for harmful content analysis In two datasets, one focused on microaggression detection and the other on hate speech, we find that our method outperforms traditional LLM prompting techniques and conventional machine learning models on key measures such as accuracy, precision, and F1 scores. Advantages include better reliability when analyzing harmful content, fewer false positives, and greater scalability for large-scale datasets. This approach supports organizational applications, including detecting workplace harassment, assessing toxic communication, and fostering safer and more inclusive work environments.', 'abstract_zh': '大型语言模型（LLMs）为组织研究提供了广阔的机会。然而，它们内置的调控系统在研究人员尝试分析有害内容时可能会引发问题，经常拒不服从某些指令或产生过于谨慎的回应，从而影响结果的有效性。这在分析组织冲突如微欺凌或仇恨言论时尤为成问题。本文介绍了一种基于Elo评分的方法，显著提升了LLM在有害内容分析中的性能。在两个数据集中，一个专注于微欺凌检测，另一个专注于仇恨言论，我们发现本方法在准确率、精确率和F1分数等关键指标上优于传统的LLM提示技术和传统机器学习模型。优点包括在分析有害内容时更好的可靠性、较少的假阳性以及更大规模数据集的可扩展性。该方法支持组织应用，包括检测职场骚扰、评估有毒沟通以及促进更安全包容的工作环境。', 'title_zh': '增强组织研究中的有害内容检测：结合大型语言模型与Elo评分系统'}
{'arxiv_id': 'arXiv:2506.16499', 'title': 'ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning', 'authors': 'Zexi Liu, Yuzhu Cai, Xinyu Zhu, Yujie Zheng, Runkun Chen, Ying Wen, Yanfeng Wang, Weinan E, Siheng Chen', 'link': 'https://arxiv.org/abs/2506.16499', 'abstract': "As AI capabilities advance toward and potentially beyond human-level performance, a natural transition emerges where AI-driven development becomes more efficient than human-centric approaches. A promising pathway toward this transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate and optimize the design, training, and deployment of AI systems themselves. While LLM-based agents have shown the potential to realize AI4AI, they are often unable to fully leverage the experience accumulated by agents during the exploration of solutions in the reasoning process, leading to inefficiencies and suboptimal performance. To address this limitation, we propose ML-Master, a novel AI4AI agent that seamlessly integrates exploration and reasoning by employing a selectively scoped memory mechanism. This approach allows ML-Master to efficiently combine diverse insights from parallel solution trajectories with analytical reasoning, guiding further exploration without overwhelming the agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it achieves a 29.3% average medal rate, significantly surpassing existing methods, particularly in medium-complexity tasks, while accomplishing this superior performance within a strict 12-hour time constraint-half the 24-hour limit used by previous baselines. These results demonstrate ML-Master's potential as a powerful tool for advancing AI4AI.", 'abstract_zh': '随着AI能力接近甚至超越人类水平，自然地过渡到AI驱动的发展比以人类为中心的方法更加高效。通往这一过渡的有希望的道路在于AI-for-AI（AI4AI），它利用AI技术来自动化和优化AI系统本身的設計、訓練和部署。虽然基于LLM的代理展示了实现AI4AI的潜力，但它们往往无法充分利用代理在解决方案探索过程中积累的经验，导致效率低下和次优性能。为了解决这一限制，我们提出了ML-Master，这是一种新颖的AI4AI代理，通过采用选择性范围的记忆机制无缝整合探索和推理。这种方法允许ML-Master高效地结合来自并行解决方案轨迹的多样化见解，并进行分析推理，指导进一步探索而不使代理陷入过多的上下文负担。我们在MLE-Bench上评估了ML-Master，它实现了29.3％的平均奖牌率，显著超越现有方法，特别是在中等复杂度任务上，同时在严格12小时的时间限制内（仅为之前基线使用的24小时限制的一半）实现了这一 superior 性能。这些结果表明ML-Master作为促进AI4AI的有力工具的潜力。', 'title_zh': 'ML-Master: 通过探索与推理的集成迈向AI为AI'}
{'arxiv_id': 'arXiv:2506.16335', 'title': 'Explainable Rule Application via Structured Prompting: A Neural-Symbolic Approach', 'authors': 'Albert Sadowski, Jarosław A. Chudziak', 'link': 'https://arxiv.org/abs/2506.16335', 'abstract': "Large Language Models (LLMs) excel in complex reasoning tasks but struggle with consistent rule application, exception handling, and explainability, particularly in domains like legal analysis that require both natural language understanding and precise logical inference. This paper introduces a structured prompting framework that decomposes reasoning into three verifiable steps: entity identification, property extraction, and symbolic rule application. By integrating neural and symbolic approaches, our method leverages LLMs' interpretive flexibility while ensuring logical consistency through formal verification. The framework externalizes task definitions, enabling domain experts to refine logical structures without altering the architecture. Evaluated on the LegalBench hearsay determination task, our approach significantly outperformed baselines, with OpenAI o-family models showing substantial improvements - o1 achieving an F1 score of 0.929 and o3-mini reaching 0.867 using structured decomposition with complementary predicates, compared to their few-shot baselines of 0.714 and 0.74 respectively. This hybrid neural-symbolic system offers a promising pathway for transparent and consistent rule-based reasoning, suggesting potential for explainable AI applications in structured legal reasoning tasks.", 'abstract_zh': '大型语言模型在复杂推理任务中表现出色，但在一致的规则应用、异常处理和可解释性方面存在困难，特别是在需要自然语言理解和精确逻辑推理的领域如法律分析中。本文提出了一种结构化提示框架，将推理分解为三个可验证的步骤：实体识别、属性提取和符号规则应用。通过结合神经和符号方法，我们的方法利用了大型语言模型的解释灵活性，并通过形式验证确保逻辑一致性。该框架外部化了任务定义，使领域专家能够在不改变架构的情况下细化逻辑结构。在LegalBench证据规则判断任务上的评估表明，我们的方法显著优于基线方法，OpenAI o-family模型表现出显著改进——o1实现了F1分数0.929，o3-mini达到了0.867，使用结构化分解和互补谓词，相比之下，它们的少量示例基线分别为0.714和0.74。这种混合同符系统的路径为透明和一致的基于规则推理提供了有前景的方法，表明在结构化法律推理任务中的可解释AI应用的潜力。', 'title_zh': '通过结构化提示实现可解释的规则应用：一种神经符号方法'}
{'arxiv_id': 'arXiv:2506.16163', 'title': 'Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior', 'authors': 'Hao Li, Gengrui Zhang, Petter Holme, Shuyue Hu, Zhen Wang', 'link': 'https://arxiv.org/abs/2506.16163', 'abstract': 'Human decision-making belongs to the foundation of our society and civilization, but we are on the verge of a future where much of it will be delegated to artificial intelligence. The arrival of Large Language Models (LLMs) has transformed the nature and scope of AI-supported decision-making; however, the process by which they learn to make decisions, compared to humans, remains poorly understood. In this study, we examined the decision-making behavior of five leading LLMs across three core dimensions of real-world decision-making: uncertainty, risk, and set-shifting. Using three well-established experimental psychology tasks designed to probe these dimensions, we benchmarked LLMs against 360 newly recruited human participants. Across all tasks, LLMs often outperformed humans, approaching near-optimal performance. Moreover, the processes underlying their decisions diverged fundamentally from those of humans. On the one hand, our finding demonstrates the ability of LLMs to manage uncertainty, calibrate risk, and adapt to changes. On the other hand, this disparity highlights the risks of relying on them as substitutes for human judgment, calling for further inquiry.', 'abstract_zh': '超大语言模型在不确定性、风险和任务切换三维决策维度上的表现及决策过程研究', 'title_zh': '大型语言模型是接近最优的决策制定者，具有非人类的学习行为。'}
{'arxiv_id': 'arXiv:2506.15928', 'title': 'Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues', 'authors': 'Myke C. Cohen, Zhe Su, Hsien-Te Kao, Daniel Nguyen, Spencer Lynch, Maarten Sap, Svitlana Volkova', 'link': 'https://arxiv.org/abs/2506.15928', 'abstract': "This paper presents an evaluation framework for agentic AI systems in mission-critical negotiation contexts, addressing the need for AI agents that can adapt to diverse human operators and stakeholders. Using Sotopia as a simulation testbed, we present two experiments that systematically evaluated how personality traits and AI agent characteristics influence LLM-simulated social negotiation outcomes--a capability essential for a variety of applications involving cross-team coordination and civil-military interactions. Experiment 1 employs causal discovery methods to measure how personality traits impact price bargaining negotiations, through which we found that Agreeableness and Extraversion significantly affect believability, goal achievement, and knowledge acquisition outcomes. Sociocognitive lexical measures extracted from team communications detected fine-grained differences in agents' empathic communication, moral foundations, and opinion patterns, providing actionable insights for agentic AI systems that must operate reliably in high-stakes operational scenarios. Experiment 2 evaluates human-AI job negotiations by manipulating both simulated human personality and AI system characteristics, specifically transparency, competence, adaptability, demonstrating how AI agent trustworthiness impact mission effectiveness. These findings establish a repeatable evaluation methodology for experimenting with AI agent reliability across diverse operator personalities and human-agent team dynamics, directly supporting operational requirements for reliable AI systems. Our work advances the evaluation of agentic AI workflows by moving beyond standard performance metrics to incorporate social dynamics essential for mission success in complex operations.", 'abstract_zh': '本文提出了一种评估关键任务谈判环境中代理型AI系统的框架，旨在应对能够适应多元人类操作者和利益相关者的AI代理的需求。通过使用Sotopia作为模拟测试平台，我们展示了两个实验，系统性地评估了个性特质和AI代理特性如何影响LLM模拟的社会谈判结果——这一能力对于多种涉及跨团队协作和民兵互动的应用至关重要。实验1采用因果发现方法衡量个性特质对价格讨价还价谈判的影响，结果显示公正性与外向性显著影响可信赖性、目标达成和知识获取结果。从团队通讯中提取的社会认知词汇测量值揭示了代理之间细微的共情交流、道德基础和意见模式差异，为必须在高风险操作情境中可靠运行的代理型AI系统提供了可操作的洞察。实验2通过操纵模拟人类个性和AI系统特性来评估人类-AI工作谈判，特别是透明度、能力、适应性，展示了AI代理可信度如何影响任务效果。这些发现确立了一种重复性的评估方法，用于探究在多元操作者个性和人机团队动态中的AI代理可靠性实验，直接支持了对可靠AI系统操作需求的支持。本研究通过超越标准性能指标，将社会动态纳入关键任务成功所必需的评估框架，推进了代理型AI工作流的评估。', 'title_zh': '探索五大人格特质与AI能力对LLM模拟谈判对话影响的研究'}
{'arxiv_id': 'arXiv:2506.15787', 'title': 'SLR: An Automated Synthesis Framework for Scalable Logical Reasoning', 'authors': 'Lukas Helff, Ahmad Omar, Felix Friedrich, Wolfgang Stammer, Antonia Wüst, Tim Woydt, Rupert Mitchell, Patrick Schramowski, Kristian Kersting', 'link': 'https://arxiv.org/abs/2506.15787', 'abstract': "We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR enables scalable, automated synthesis of inductive reasoning tasks with precisely controlled difficulty. For each task, SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation program used by a symbolic judge to deterministically verify model outputs, and (iii) an instruction prompt for the reasoning task. Using SLR, we create SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs do somewhat better, but incur substantial increases in test-time compute, sometimes exceeding 15k completion tokens. Finally, logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully automated, requires no human annotation, ensures dataset novelty, and offers a scalable environment for probing and advancing LLMs' reasoning capabilities.", 'abstract_zh': '基于可扩展逻辑推理的大型语言模型系统评估与训练框架SLR', 'title_zh': 'SLR：一种可扩展逻辑推理的自动化综合框架'}
{'arxiv_id': 'arXiv:2506.15751', 'title': 'Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts', 'authors': 'Kartik Sharma, Yiqiao Jin, Vineeth Rakesh, Yingtong Dou, Menghai Pan, Mahashweta Das, Srijan Kumar', 'link': 'https://arxiv.org/abs/2506.15751', 'abstract': 'As large language models (LLMs) are deployed in safety-critical settings, it is essential to ensure that their responses comply with safety standards. Prior research has revealed that LLMs often fail to grasp the notion of safe behaviors, resulting in either unjustified refusals to harmless prompts or the generation of harmful content. While substantial efforts have been made to improve their robustness, existing defenses often rely on costly fine-tuning of model parameters or employ suboptimal heuristic techniques. In this work, we take a novel approach to safeguard LLMs by learning to adapt the system prompts in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a fixed system prompt, we investigate the impact of tailoring the system prompt to each specific user input on the safety of the responses. To this end, we propose $\\textbf{Sysformer}$, a trans$\\textbf{former}$ model that updates an initial $\\textbf{sys}$tem prompt to a more robust system prompt in the LLM input embedding space while attending to the user prompt. While keeping the LLM parameters frozen, the Sysformer is trained to refuse to respond to a set of harmful prompts while responding ideally to a set of safe ones. Through extensive experiments on $5$ LLMs from different families and $2$ recent benchmarks, we demonstrate that Sysformer can significantly enhance the robustness of LLMs, leading to upto $80\\%$ gain in the refusal rate on harmful prompts while enhancing the compliance with the safe prompts by upto $90\\%$. Results also generalize well to sophisticated jailbreaking attacks, making LLMs upto $100\\%$ more robust against different attack strategies. We hope our findings lead to cheaper safeguarding of LLMs and motivate future investigations into designing variable system prompts.', 'abstract_zh': '大型语言模型在安全关键设置中的部署亟需确保其响应符合安全标准。尽管先前的研究揭示了大型语言模型往往无法理解安全行为的概念，导致对无害提示的不合理拒绝或生成有害内容，现有的防御措施往往依赖于昂贵的模型参数微调或采用次优化的启发式技术。本项工作中，我们通过学习调整指令调优的大型语言模型中的系统提示来采取一种新颖的方法来保护大型语言模型。虽然大型语言模型通常预训练以遵循固定的系统提示，我们研究了根据每次特定用户输入定制系统提示对响应安全性的潜在影响。为此，我们提出了Sysformer模型，在保持大型语言模型参数不变的情况下，在LLM输入嵌入空间中更新初始系统提示为更具鲁棒性的系统提示，同时关注用户提示。通过在不同家族的5个大型语言模型和2个最新基准上进行广泛实验，我们证明Sysformer可以显著提升大型语言模型的鲁棒性，在有害提示上的拒绝率可提升至80%，对安全提示的合规性提升至90%。结果还很好地适应了复杂的模型突破攻击，使大型语言模型在面对不同攻击策略时更加强大，最多可提升100%的鲁棒性。希望我们的研究成果能够降低大型语言模型的安全成本，并激发未来关于设计可变系统提示的研究。', 'title_zh': 'Sysformer：通过自适应系统提示保护冻结的大语言模型'}
{'arxiv_id': 'arXiv:2506.15740', 'title': 'SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents', 'authors': 'Jonathan Kutasov, Yuqi Sun, Paul Colognese, Teun van der Weij, Linda Petrini, Chen Bo Calvin Zhang, John Hughes, Xiang Deng, Henry Sleight, Tyler Tracy, Buck Shlegeris, Joe Benton', 'link': 'https://arxiv.org/abs/2506.15740', 'abstract': "As Large Language Models (LLMs) are increasingly deployed as autonomous agents in complex and long horizon settings, it is critical to evaluate their ability to sabotage users by pursuing hidden objectives. We study the ability of frontier LLMs to evade monitoring and achieve harmful hidden goals while completing a wide array of realistic tasks. We evaluate a broad range of frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena, the first highly diverse agent evaluation dataset for sabotage and monitoring capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign main tasks and harmful side objectives in complicated environments. Agents are evaluated on their ability to complete the side task without appearing suspicious to an LLM monitor. When measuring agent ability to (a) complete the main task, (b) complete the side task, and (c) avoid detection, we find that the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15% (Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For current frontier models, success on the side task relies heavily on having access to a hidden scratchpad that is not visible to the monitor. We also use SHADE-Arena to measure models' monitoring abilities, with the top monitor (Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign transcripts. We find that for now, models still struggle at sabotage due to failures in long-context main task execution. However, our measurements already demonstrate the difficulty of monitoring for subtle sabotage attempts, which we expect to only increase in the face of more complex and longer-horizon tasks.", 'abstract_zh': '随着大型语言模型（LLMs）在复杂和长期场景中作为自主代理日益部署，评估其通过追求隐藏目标来危害用户的 ability 至关重要。我们研究了前沿 LLMs 在完成多种真实任务时规避监控并实现有害隐藏目标的能力。我们使用 SHADE（Subtle Harmful Agent Detection & Evaluation）Arena 评估了广泛前沿 LLMs 的能力，SHADE-Arena 是首个用于评估 LLM 代理破坏和监控能力的高度多样化代理评估数据集。SHADE-Arena 包含复杂的主任务和有害辅助目标的复杂配对。代理在不被 LLM 监控怀疑的情况下完成辅助任务的能力被用于评估。在测量代理完成主要任务、辅助任务和避免检测的能力时，我们发现，在 Claude 3.6 Sonnet 监管下，最佳表现的前沿模型得分分别为 Claude 3.7 Sonnet（27%）和 Gemini 2.5 Pro（15%）作为破坏代理。对于当前的前沿模型，辅助任务的成功高度依赖于能够访问不被监控看到的隐藏便笺。我们还使用 SHADE-Arena 测量了模型的监控能力，顶级监控 Gemini 2.5 Pro 在区分良性与有害转录方面达到了 0.87 的 AUC。我们发现，由于长上下文主要任务执行失败，模型目前仍然难以进行破坏。然而，我们的测量结果已经表明，对于更复杂和长期的任务而言，监控细微破坏尝试的难度会增加。', 'title_zh': 'SHADE-Arena: 评估大模型代理中的破坏与监控'}
{'arxiv_id': 'arXiv:2506.15735', 'title': 'ContextBench: Modifying Contexts for Targeted Latent Activation', 'authors': 'Robert Graham, Edward Stevinson, Leo Richter, Alexander Chia, Joseph Miller, Joseph Isaac Bloom', 'link': 'https://arxiv.org/abs/2506.15735', 'abstract': 'Identifying inputs that trigger specific behaviours or latent features in language models could have a wide range of safety use cases. We investigate a class of methods capable of generating targeted, linguistically fluent inputs that activate specific latent features or elicit model behaviours. We formalise this approach as context modification and present ContextBench -- a benchmark with tasks assessing core method capabilities and potential safety applications. Our evaluation framework measures both elicitation strength (activation of latent features or behaviours) and linguistic fluency, highlighting how current state-of-the-art methods struggle to balance these objectives. We enhance Evolutionary Prompt Optimisation (EPO) with LLM-assistance and diffusion model inpainting, and demonstrate that these variants achieve state-of-the-art performance in balancing elicitation effectiveness and fluency.', 'abstract_zh': '识别能够触发语言模型特定行为或潜在特征的输入可能具有广泛的安全部署案例。我们研究了一类能够生成目标明确、语义流畅的输入以激活特定潜在特征或引发模型行为的方法。我们将这种方法形式化为上下文修改，并提出ContextBench——一个包含评估核心方法能力和潜在安全应用的任务基准。我们的评估框架衡量触发强度（激活潜在特征或行为）和语义流畅性，突显了当前最先进的方法在平衡这些目标方面面临的挑战。我们通过LLM辅助和扩散模型填补增强进化提示优化（EPO），并证明这些变体在平衡触发效果和流畅性方面达到了最先进的性能。', 'title_zh': 'ContextBench: 修改上下文以实现目标潜空间激活'}
{'arxiv_id': 'arXiv:2506.15734', 'title': 'The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models', 'authors': 'Peiyuan Tang, Haojie Xin, Xiaodong Zhang, Jun Sun, Qin Xia, Zijiang Yang', 'link': 'https://arxiv.org/abs/2506.15734', 'abstract': "As Vision-Language Models (VLMs) demonstrate increasing capabilities across real-world applications such as code generation and chatbot assistance, ensuring their safety has become paramount. Unlike traditional Large Language Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature, allowing adversaries to modify visual or textual inputs to bypass safety guardrails and trigger the generation of harmful content. Through systematic analysis of VLM behavior under attack, we identify a novel phenomenon termed ``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs may initially be compromised to produce harmful content, but eventually recognize the associated risks and attempt to self-correct. This pattern suggests that VLMs retain their underlying safety awareness but experience a temporal delay in their activation. Building on this insight, we hypothesize that VLMs' safety awareness can be proactively reactivated through carefully designed prompts. To this end, we introduce ``The Safety Reminder'', a soft prompt tuning approach that optimizes learnable prompt tokens, which are periodically injected during the text generation process to enhance safety awareness, effectively preventing harmful content generation. Additionally, our safety reminder only activates when harmful content is detected, leaving normal conversations unaffected and preserving the model's performance on benign tasks. Through comprehensive evaluation across three established safety benchmarks and one adversarial attacks, we demonstrate that our approach significantly reduces attack success rates while maintaining model utility, offering a practical solution for deploying safer VLMs in real-world applications.", 'abstract_zh': '随着视觉语言模型（VLMs）在代码生成和聊天机器人辅助等实际应用中展现出不断增强的能力，确保其安全性已成为当务之急。不同于传统的大型语言模型（LLMs），由于其多模态性质，VLMs 面临独特的漏洞，使得对手可以通过修改视觉或文本输入来 bypass 安全防护机制并触发有害内容的生成。通过系统分析 VLM 在攻击下的行为，我们发现了一个新的现象，称之为“延时安全意识”。具体而言，我们观察到，虽然安全对齐的 VLMs 初始阶段可能被劫持以生成有害内容，但它们最终会意识到相关的风险并尝试自我纠正。这一模式表明，VLMs 保留其内在的安全意识，但其激活存在时间上的延迟。基于这一见解，我们假设可以通过精心设计的提示来主动重新激活 VLMs 的安全意识。为此，我们引入了“安全提示”，这是一种软提示调优方法，通过优化可学习的提示tokens，并在文本生成过程中定期注入，以增强安全意识，有效地防止有害内容的生成。此外，我们的安全提示仅在检测到有害内容时激活，不对正常对话产生影响，并保持模型在良性任务上的性能。通过在三个公认的安全基准和一个对抗性攻击上的全面评估，我们证明了该方法在降低攻击成功率的同时保持模型的实用性，为在实际应用中部署更安全的 VLMs 提供了实用的解决方案。', 'title_zh': '安全提醒：一种重新激活视觉语言模型中延迟的安全意识的软提示'}
{'arxiv_id': 'arXiv:2506.15733', 'title': '$\\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts', 'authors': 'Mert Cemri, Nived Rajaraman, Rishabh Tiwari, Xiaoxuan Liu, Kurt Keutzer, Ion Stoica, Kannan Ramchandran, Ahmad Beirami, Ziteng Sun', 'link': 'https://arxiv.org/abs/2506.15733', 'abstract': 'Scaling test-time compute has driven the recent advances in the reasoning capabilities of large language models (LLMs), typically by allocating additional computation for more thorough exploration. However, increased compute often comes at the expense of higher user-facing latency, directly impacting user experience. Current test-time scaling methods primarily optimize for accuracy based on total compute resources (FLOPS), often overlooking latency constraints. To address this gap, we propose $\\texttt{SPECS}$, a latency-aware test-time scaling method inspired by speculative decoding. $\\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences efficiently, and evaluates these candidates using signals from both a larger target model and a dedicated reward model. We introduce new integration strategies, including reward-guided soft verification and a reward-based deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench datasets show that $\\texttt{SPECS}$~matches or surpasses beam search accuracy while reducing latency by up to $\\sim$19.1\\%. Our theoretical analysis shows that our algorithm converges to the solution of a KL-regularized reinforcement learning objective with increasing beam width.', 'abstract_zh': '基于延迟感知的测试时扩增方法$\\texttt{SPECS}$：Speculative Decoding启发的延迟感知测试时扩增方法', 'title_zh': 'SPECS: 通过推测草稿加速测试时的扩展'}
{'arxiv_id': 'arXiv:2506.15732', 'title': 'LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge', 'authors': 'Khurram Yamin, Gaurav Ghosal, Bryan Wilder', 'link': 'https://arxiv.org/abs/2506.15732', 'abstract': "Large Language Models have been shown to contain extensive world knowledge in their parameters, enabling impressive performance on many knowledge intensive tasks. However, when deployed in novel settings, LLMs often encounter situations where they must integrate parametric knowledge with new or unfamiliar information. In this work, we explore whether LLMs can combine knowledge in-context with their parametric knowledge through the lens of counterfactual reasoning. Through synthetic and real experiments in multi-hop reasoning problems, we show that LLMs generally struggle with counterfactual reasoning, often resorting to exclusively using their parametric knowledge. Moreover, we show that simple post-hoc finetuning can struggle to instill counterfactual reasoning ability -- often leading to degradation in stored parametric knowledge. Ultimately, our work reveals important limitations of current LLM's abilities to re-purpose parametric knowledge in novel settings.", 'abstract_zh': '大规模语言模型在其参数中包含广泛的world知识，使其在许多知识密集型任务上表现出色。然而，在新颖的应用环境中部署时，这些模型常常需要整合参数化知识与新奇或不熟悉的信息。本研究探讨了通过因果反事实推理的视角，语言模型是否能够将其参数化知识与上下文中的知识结合。通过合成和实际的多步推理问题实验，我们展示了语言模型通常在因果反事实推理方面存在困难，往往会依赖于其参数化知识。此外，我们还展示了简单的后hots微调在植入因果反事实推理能力方面存在困难，常常导致存储的参数化知识性能下降。最终，我们的研究揭示了当前语言模型在新颖应用环境中重新利用参数化知识的重要局限性。', 'title_zh': 'LLMs在处理参数化知识的逆向推理方面存在困难。'}
{'arxiv_id': 'arXiv:2506.17219', 'title': 'No Free Lunch: Rethinking Internal Feedback for LLM Reasoning', 'authors': 'Yanzhi Zhang, Zhaoxi Zhang, Haoxiang Guan, Yilin Cheng, Yitong Duan, Chen Wang, Yue Wang, Shuxin Zheng, Jiyan He', 'link': 'https://arxiv.org/abs/2506.17219', 'abstract': "Reinforcement learning has emerged as a powerful paradigm for post-training large language models (LLMs) to improve reasoning. Approaches like Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) have shown strong results, but they require extensive external supervision. We investigate an alternative class of methods, Reinforcement Learning from Internal Feedback (RLIF), which relies solely on intrinsic model-derived signals instead of external rewards. In particular, we leverage unsupervised reward proxies such as token-level entropy, trajectory-level entropy, and self-certainty. Our theoretical analysis shows these internal objectives are partially equivalent, and we empirically evaluate various RLIF strategies on challenging math reasoning benchmarks. Experimental results demonstrate that RLIF can boost the reasoning performance of base LLMs at the beginning phase of the training, matching or surpassing RLVR techniques on these tasks. However, when training progresses, performance degrades even below the model before training. Moreover, we find that RLIF yields little improvement for instruction-tuned models, indicating diminishing returns of intrinsic feedback once an LLM is already instruction-tuned. We further analyze this limitation by mixing model weights and explain the reason of RLIF's training behaviors, providing practical guidelines for integrating internal feedback signals into LLM training. We hope our analysis of internal feedback will inform more principled and effective strategies for LLM post-training.", 'abstract_zh': '强化学习已成为后训练大型语言模型(LLM)提升推理能力的一种强大范式。像人类反馈强化学习(RLHF)和验证奖励强化学习(RLVR)这样的方法已经展示了强大的结果，但它们需要大量的外部监督。我们探讨了一种替代方法类——内部反馈强化学习(RLIF)，这种方法仅依赖于模型内部产生的信号，而不是外部奖励。特别是，我们利用未监督的奖励代理，如令牌级熵、轨迹级熵和自信心。我们的理论分析表明这些内部目标部分等效，我们在具有挑战性的数学推理基准上对各种RLIF策略进行了实证评估。实验结果表明，RLIF可以在训练初期提升基础LLM的推理性能，在这些任务上可以达到或超过RLVR技术的效果。然而，随着训练的进行，性能会下降，甚至低于训练前的模型。此外，我们发现，对于指令调优的模型，RLIF几乎没有改善效果，表明当LLM已经指令调优时，内部反馈的改进效果边际递减。我们进一步分析了这一局限性，通过混合模型权重来解释RLIF的训练行为，并提供集成内部反馈信号到LLM训练中的实用指南。我们希望对内部反馈的分析能促进更原则性和有效的后训练策略。', 'title_zh': '没有免费午餐：重思大语言模型推理中的内部反馈'}
{'arxiv_id': 'arXiv:2506.17208', 'title': 'Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems', 'authors': 'Matias Martinez, Xavier Franch', 'link': 'https://arxiv.org/abs/2506.17208', 'abstract': 'The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench Verified, have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries) leaderboards, analyzing 67 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.', 'abstract_zh': 'Automated程序修复进展：SWE-Bench基准中的LLM基础修复系统研究', 'title_zh': 'dissecting the SWE-Bench 排名榜：剖析提交者和基于大规模语言模型及代理的修复系统架构'}
{'arxiv_id': 'arXiv:2506.17188', 'title': 'Towards AI Search Paradigm', 'authors': 'Yuchen Li, Hengyi Cai, Rui Kong, Xinran Chen, Jiamin Chen, Jun Yang, Haojie Zhang, Jiayi Li, Jiayi Wu, Yiqun Chen, Changle Qu, Keyi Kong, Wenwen Ye, Lixin Su, Xinyu Ma, Long Xia, Daiting Shi, Jiashu Zhao, Haoyi Xiong, Shuaiqiang Wang, Dawei Yin', 'link': 'https://arxiv.org/abs/2506.17188', 'abstract': 'In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.', 'abstract_zh': '本文介绍了AI搜索范式，这是一种全面的下一代搜索系统蓝图，能够模拟人类信息处理和决策过程。该范式采用由四个基于大语言模型的代理（主管、规划师、执行者和撰写者）组成的模块化架构，能够适应从简单的事实查询到复杂的多阶段推理任务的整个信息需求范围。这些代理通过协调的工作流动态协作，评估查询复杂性、将问题分解为可执行的计划，并协调工具使用、任务执行和内容合成。本文系统地阐述了实现这一范式的关键方法，包括任务规划和工具集成、执行策略、对齐和稳健的检索增强生成，以及高效的大型语言模型推理，涵盖了算法技术和基础设施层面的优化。通过提供对这些基础组件的深入指南，本文旨在促进可信赖、适应性强和可扩展的AI搜索系统的开发。', 'title_zh': '面向AI搜索范式'}
{'arxiv_id': 'arXiv:2506.17144', 'title': 'Do We Need Large VLMs for Spotting Soccer Actions?', 'authors': 'Ritabrata Chakraborty, Rajatsubhra Chakraborty, Avijit Dasgupta, Sandeep Chaurasia', 'link': 'https://arxiv.org/abs/2506.17144', 'abstract': 'Traditional video-based tasks like soccer action spotting rely heavily on visual inputs, often requiring complex and computationally expensive models to process dense video data. In this work, we propose a shift from this video-centric approach to a text-based task, making it lightweight and scalable by utilizing Large Language Models (LLMs) instead of Vision-Language Models (VLMs). We posit that expert commentary, which provides rich, fine-grained descriptions and contextual cues such as excitement and tactical insights, contains enough information to reliably spot key actions in a match. To demonstrate this, we use the SoccerNet Echoes dataset, which provides timestamped commentary, and employ a system of three LLMs acting as judges specializing in outcome, excitement, and tactics. Each LLM evaluates sliding windows of commentary to identify actions like goals, cards, and substitutions, generating accurate timestamps for these events. Our experiments show that this language-centric approach performs effectively in detecting critical match events, providing a lightweight and training-free alternative to traditional video-based methods for action spotting.', 'abstract_zh': '基于文本的传统视频任务如足球动作识别以前主要依赖视觉输入，往往需要复杂且计算密集型的模型来处理密集的视频数据。本工作中，我们提出从以视频为中心的方法转向基于文本的任务，通过利用大型语言模型（LLMs）而不是视觉语言模型（VLMs）来实现轻量化和可扩展性。我们假设专家评论能够提供丰富而详细的描述和上下文线索，如兴奋度和战术见解，这些信息足以可靠地识别比赛中的关键动作。为此，我们使用了包含时间戳评论的SoccerNet Echoes数据集，并采用三个专门负责结果、兴奋度和战术的LLM作为评判系统。每个LLM评估评论滑动窗口以识别如进球、黄牌和换人等动作，并生成这些事件的准确时间戳。实验结果表明，这种以语言为中心的方法在检测关键比赛事件方面表现有效，提供了一种轻量化且无需训练的替代传统基于视频的方法来识别动作。', 'title_zh': '我们需要大型VLMs来检测足球动作吗？'}
{'arxiv_id': 'arXiv:2506.17080', 'title': 'Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs', 'authors': 'Ricardo Rei, Nuno M. Guerreiro, José Pombal, João Alves, Pedro Teixeirinha, Amin Farajian, André F. T. Martins', 'link': 'https://arxiv.org/abs/2506.17080', 'abstract': 'Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization.', 'abstract_zh': 'Fine-tuning 预训练大规模语言模型在特定任务（如机器翻译）上达到最佳性能已被证明是一种有效策略，但这一适应过程往往会牺牲通用能力，如对话推理和指令跟随，限制了其在需要多种技能的应用中的实用性。本文提出 Tower+，一个旨在同时实现强大翻译性能和多语言通用文本能力的模型系列。通过引入一种新的训练方案，该方案基于 Tower (Alves et al., 2024)，结合连续预训练、监督微调、偏好优化和带有可验证奖励的强化学习，我们实现了翻译专门化和多语言通用能力之间的帕累托前沿。在每一轮训练中，我们精心生成和筛选数据以强化翻译性能，同时也强化涉及代码生成、数学问题解决和通用指令跟随的一般任务性能。我们开发了多种规模的模型：2B、9B 和 72B。我们的较小模型经常优于较大的通用大型开放权重和专有语言模型（例如 Llama 3.3 70B、GPT-4o）。我们最大的模型在高资源语言翻译性能方面达到最佳水平，并在多语言 Arena Hard 评估和我们引入的 IF-MT 基准测试中取得了顶级结果，该基准测试用于评估翻译和指令跟随能力。我们的发现表明，在优化特定业务领域（如翻译和本地化）的同时，有可能与前沿模型在通用能力方面匹敌。', 'title_zh': 'Tower+: 在多语言LLM中连接通用性和翻译专业化'}
{'arxiv_id': 'arXiv:2506.17073', 'title': 'LLM-Based Bot Broadens the Range of Arguments in Online Discussions, Even When Transparently Disclosed as AI', 'authors': 'Valeria Vuk, Cristina Sarasua, Fabrizio Gilardi', 'link': 'https://arxiv.org/abs/2506.17073', 'abstract': 'A wide range of participation is essential for democracy, as it helps prevent the dominance of extreme views, erosion of legitimacy, and political polarization. However, engagement in online political discussions often features a limited spectrum of views due to high levels of self-selection and the tendency of online platforms to facilitate exchanges primarily among like-minded individuals. This study examines whether an LLM-based bot can widen the scope of perspectives expressed by participants in online discussions through two pre-registered randomized experiments conducted in a chatroom. We evaluate the impact of a bot that actively monitors discussions, identifies missing arguments, and introduces them into the conversation. The results indicate that our bot significantly expands the range of arguments, as measured by both objective and subjective metrics. Furthermore, disclosure of the bot as AI does not significantly alter these effects. These findings suggest that LLM-based moderation tools can positively influence online political discourse.', 'abstract_zh': '广泛的参与对于民主至关重要，它有助于防止极端观点的主导、合法性的侵蚀和政治极化。然而，在线政治讨论中的参与往往由于高度的选择性和在线平台促进观点一致者之间交流的倾向而局限于有限的观点范围。本研究通过在聊天室中进行的两项预先注册的随机实验，探讨基于大语言模型的机器人是否能够通过引入新的观点来扩大在线讨论中参与者表达的视角范围。我们评估了这种机器人在活跃监控讨论、识别缺失的论点并将其引入对话中的影响。结果显示，我们的机器人在客观和主观指标上显著扩展了论点的范围。此外，披露机器人是AI并不会显著改变这些效果。这些发现表明，基于大语言模型的 Moderation 工具可以积极影响在线政治 discourse。', 'title_zh': 'LLM-Based Bot 扩大了在线讨论中的论点范围，即使透明披露为AIassistant'}
{'arxiv_id': 'arXiv:2506.17052', 'title': 'From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers', 'authors': 'Jingtong Su, Julia Kempe, Karen Ullrich', 'link': 'https://arxiv.org/abs/2506.17052', 'abstract': 'Transformers have achieved state-of-the-art performance across language and vision tasks. This success drives the imperative to interpret their internal mechanisms with the dual goals of enhancing performance and improving behavioral control. Attribution methods help advance interpretability by assigning model outputs associated with a target concept to specific model components. Current attribution research primarily studies multi-layer perceptron neurons and addresses relatively simple concepts such as factual associations (e.g., Paris is located in France). This focus tends to overlook the impact of the attention mechanism and lacks a unified approach for analyzing more complex concepts. To fill these gaps, we introduce Scalable Attention Module Discovery (SAMD), a concept-agnostic method for mapping arbitrary, complex concepts to specific attention heads of general transformer models. We accomplish this by representing each concept as a vector, calculating its cosine similarity with each attention head, and selecting the TopK-scoring heads to construct the concept-associated attention module. We then propose Scalar Attention Module Intervention (SAMI), a simple strategy to diminish or amplify the effects of a concept by adjusting the attention module using only a single scalar parameter. Empirically, we demonstrate SAMD on concepts of varying complexity, and visualize the locations of their corresponding modules. Our results demonstrate that module locations remain stable before and after LLM post-training, and confirm prior work on the mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the domain-agnostic nature of our approach by suppressing the image classification accuracy of vision transformers on ImageNet.', 'abstract_zh': 'Transformer在语言和视觉任务中取得了最先进的性能。这种成功推动了对其内部机制进行解释的必要性，目标是在提升性能的同时改善行为控制。归因方法通过将模型输出与特定模型组件关联起来，有助于推进解释性。当前的归因研究主要集中在多层感知器神经元上，并且侧重于研究简单的概念，如事实关联（例如，巴黎位于法国）。这种侧重往往忽视了注意力机制的影响，并且缺乏统一的方法来分析更复杂的概念。为填补这些空白，我们介绍了可扩展的注意力模块发现（SAMD），这是一种概念无关的方法，用于将任意复杂概念映射到通用Transformer模型的特定注意力头。我们通过将每个概念表示为向量，计算其与每个注意力头的余弦相似度，并选择得分最高的TopK头来构建概念相关的注意力模块。然后，我们提出了标量注意力模块干预（SAMI）策略，通过仅使用单一标量参数调整注意力模块，来减弱或放大概念的效果。实证研究中，我们在不同复杂度的概念上展示了SAMD，并可视化了它们对应的模块位置。实验结果表明，在大规模语言模型后训练前后，模块位置保持稳定，并且证实了先前关于大规模语言模型多语言性的研究工作。通过SAMI，我们通过减弱“安全性”在HarmBench上实现了+72.7%的提升，并通过增强“推理”在GSM8K基准上实现了+1.6%的性能提升。最后，我们强调了我们方法的领域无关性，通过抑制视觉Transformer在ImageNet上的图像分类准确性来突显这一点。', 'title_zh': '从概念到组件：Transformer中的概念无关注意力模块发现'}
{'arxiv_id': 'arXiv:2506.16990', 'title': 'TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs', 'authors': 'Sahil Kale, Vijaykant Nadadur', 'link': 'https://arxiv.org/abs/2506.16990', 'abstract': "LaTeX's precision and flexibility in typesetting have made it the gold standard for the preparation of scientific documentation. Large Language Models (LLMs) present a promising opportunity for researchers to produce publication-ready material using LaTeX with natural language instructions, yet current benchmarks completely lack evaluation of this ability. By introducing TeXpert, our benchmark dataset with natural language prompts for generating LaTeX code focused on components of scientific documents across multiple difficulty levels, we conduct an in-depth analysis of LLM performance in this regard and identify frequent error types. Our evaluation across open and closed-source LLMs highlights multiple key findings: LLMs excelling on standard benchmarks perform poorly in LaTeX generation with a significant accuracy drop-off as the complexity of tasks increases; open-source models like DeepSeek v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks; and formatting and package errors are unexpectedly prevalent, suggesting a lack of diverse LaTeX examples in the training datasets of most LLMs. Our dataset, code, and model evaluations are available at this https URL.", 'abstract_zh': 'LaTeX排版的精确性和灵活性使其成为科学文档准备的黄金标准。大规模语言模型（LLMs）为研究人员提供了使用自然语言指令生成符合出版要求的LaTeX文档的 promising 机会，然而目前的基准测试完全缺乏对这一能力的评估。通过引入TeXpert，一个基于自然语言提示生成LaTeX代码的基准数据集，专注于科学文档的不同组件并涵盖多种难度级别，我们对LLMs在此方面的性能进行了深入分析，并识别出常见的错误类型。我们的评估结果显示，表现优异的LLMs在LaTeX生成任务中的表现不佳，随着任务复杂性的增加，准确率急剧下降；开源模型如DeepSeek v3和DeepSeek Coder在LaTeX任务中与闭源模型有强烈竞争；格式错误和包错误出乎意料地普遍，表明大多数LLMs的训练数据集中缺乏多样化的LaTeX示例。我们的数据集、代码和模型评估可在以下链接获取。', 'title_zh': 'TeXpert：评价LLM生成LaTeX代码能力的多层级基准'}
{'arxiv_id': 'arXiv:2506.16982', 'title': 'Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond', 'authors': 'Antonin Berthon, Mihaela van der Schaar', 'link': 'https://arxiv.org/abs/2506.16982', 'abstract': 'Accurately assessing student knowledge is critical for effective education, yet traditional Knowledge Tracing (KT) methods rely on opaque latent embeddings, limiting interpretability. Even LLM-based approaches generate direct predictions or summaries that may hallucinate without any accuracy guarantees. We recast KT as an inverse problem: learning the minimum natural-language summary that makes past answers explainable and future answers predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM that writes an interpretable knowledge summary and a frozen decoder LLM that must reconstruct and predict student responses using only that summary text. By constraining all predictive information to pass through a short natural-language bottleneck, LBMs ensure that the summary contains accurate information while remaining human-interpretable. Experiments on synthetic arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the accuracy of state-of-the-art KT and direct LLM methods while requiring orders-of-magnitude fewer student trajectories. We demonstrate that training the encoder with group-relative policy optimization, using downstream decoding accuracy as a reward signal, effectively improves summary quality.', 'abstract_zh': '准确评估学生知识对于有效的教育至关重要，但传统的知识追踪（KT）方法依赖于不透明的潜变量嵌入，限制了其可解释性。即使是基于大语言模型（LLM）的方法也会生成直接的预测或摘要，这些摘要可能会无中生有，没有任何准确性的保证。我们将KT重新定义为一个逆问题：学习一个最小的自然语言摘要，使其能够解释过去的答案并预测未来的答案。我们的语言瓶颈模型（LBM）由一个编写可解释知识摘要的编码器LLM和一个冻结的解码器LLM组成，后者必须仅使用摘要文本重构和预测学生的回答。通过将所有预测信息强制通过一个短的自然语言瓶颈，LBM确保摘要包含准确的信息同时保持人类可解释性。在合成算术基准测试和大规模Eedi数据集上的实验表明，LBM在准确性和最新的KT以及直接LLM方法相当的同时，需要的学生轨迹数量级更少。我们证明，使用组相对策略优化训练编码器，并将下游解码准确性作为奖励信号，可以有效提高摘要的质量。', 'title_zh': '语言瓶颈模型：可解释的知识追踪及更广泛的框架'}
{'arxiv_id': 'arXiv:2506.16975', 'title': 'Latent Concept Disentanglement in Transformer-based Language Models', 'authors': 'Guan Zhe Hong, Bhavya Vasudeva, Vatsal Sharan, Cyrus Rashtchian, Prabhakar Raghavan, Rina Panigrahy', 'link': 'https://arxiv.org/abs/2506.16975', 'abstract': 'When large language models (LLMs) use in-context learning (ICL) to solve a new task, they seem to grasp not only the goal of the task but also core, latent concepts in the demonstration examples. This begs the question of whether transformers represent latent structures as part of their computation or whether they take shortcuts to solve the problem. Prior mechanistic work on ICL does not address this question because it does not sufficiently examine the relationship between the learned representation and the latent concept, and the considered problem settings often involve only single-step reasoning. In this work, we examine how transformers disentangle and use latent concepts. We show that in 2-hop reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. In tasks parameterized by a continuous latent concept, we find low-dimensional subspaces in the representation space where the geometry mimics the underlying parameterization. Together, these results refine our understanding of ICL and the representation of transformers, and they provide evidence for highly localized structures in the model that disentangle latent concepts in ICL tasks.', 'abstract_zh': '当大型语言模型（LLMs）使用上下文学习（ICL）解决新任务时，它们似乎不仅掌握了任务目标，还掌握了示范例子中的核心潜在概念。这引发了关于变换器是否在其计算过程中表示潜在结构，还是采取捷径来解决问题的疑问。先前关于ICL的机制研究未能回答这一问题，因为它们未能充分探讨所学表示与潜在概念之间的关系，而且考虑的问题场景往往只涉及一步推理。在本工作中，我们探讨了变换器如何分离和利用潜在概念。我们展示了在涉及潜在离散概念的2跳推理任务中，模型成功识别了潜在概念并进行了逐步的概念组合。在涉及连续潜在概念的任务中，我们发现表示空间中的低维子空间，其几何结构模仿了潜在参数化。这些结果进一步细化了我们对ICL和变换器表示的理解，并提供了模型中高度局部化的结构证据，这些结构在ICL任务中分离潜在概念。', 'title_zh': '基于变换器的语言模型中潜概念去缠绕'}
{'arxiv_id': 'arXiv:2506.16962', 'title': 'Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs', 'authors': 'Haoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang, Wenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, Xiaosong Wang', 'link': 'https://arxiv.org/abs/2506.16962', 'abstract': 'Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs', 'abstract_zh': '多模态大型语言模型（MLLMs）在一般任务上已经展现出稳健的推理能力，但在医疗领域中的应用仍处于起步阶段。构建链式思考（CoT）训练数据对于增强医疗MLLMs的推理能力至关重要。然而，现有方法在提供全面框架以搜索和评估通往关键诊断的有效推理路径方面存在不足。为解决这一挑战，我们提出了一种名为Mentor-Intern Collaborative Search（MICS）的新颖推理路径搜索方案，以生成严格且有效的医疗CoT数据。MICS首先利用导师模型逐步初始化推理，然后提示每个实习生模型沿着这些初始路径继续思考，并最终根据多个实习生模型的整体推理性能选择最优推理路径。推理性能由MICS-Score评估，该指标评估生成的推理路径的质量。最后，我们构建了包含分级难度的多任务医疗推理数据集MMRP，并通过阶梯学习策略开发了新的医疗MLLM Chiron-o1，该模型具备稳健的视觉问答和泛化推理能力。广泛实验表明，Chiron-o1在使用MICS构建的CoT数据集进行训练后，在一系列医疗视觉问答和推理基准测试中均实现了最先进的性能。代码可在GitHub - manglu097/Chiron-o1: 提升MLLM中逐步且可验证的医疗推理获得。', 'title_zh': '增强MLLMs的逐步可验证医疗推理能力'}
{'arxiv_id': 'arXiv:2506.16899', 'title': 'Towards Effective Complementary Security Analysis using Large Language Models', 'authors': 'Jonas Wagner, Simon Müller, Christian Näther, Jan-Philipp Steghöfer, Andreas Both', 'link': 'https://arxiv.org/abs/2506.16899', 'abstract': "A key challenge in security analysis is the manual evaluation of potential security weaknesses generated by static application security testing (SAST) tools. Numerous false positives (FPs) in these reports reduce the effectiveness of security analysis. We propose using Large Language Models (LLMs) to improve the assessment of SAST findings. We investigate the ability of LLMs to reduce FPs while trying to maintain a perfect true positive rate, using datasets extracted from the OWASP Benchmark (v1.2) and a real-world software project. Our results indicate that advanced prompting techniques, such as Chain-of-Thought and Self-Consistency, substantially improve FP detection. Notably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark dataset without missing genuine weaknesses. Combining detections from different LLMs would increase this FP detection to approximately 78.9%. Additionally, we demonstrate our approach's generalizability using a real-world dataset covering five SAST tools, three programming languages, and infrastructure files. The best LLM detected 33.85% of all FPs without missing genuine weaknesses, while combining detections from different LLMs would increase this detection to 38.46%. Our findings highlight the potential of LLMs to complement traditional SAST tools, enhancing automation and reducing resources spent addressing false alarms.", 'abstract_zh': '使用大型语言模型提高静态应用程序安全性测试结果评估的挑战与研究', 'title_zh': '面向有效互补安全分析的大语言模型方法'}
{'arxiv_id': 'arXiv:2506.16792', 'title': 'MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning', 'authors': 'Muyang Zheng, Yuanzhi Yao, Changting Lin, Rui Wang, Meng Han', 'link': 'https://arxiv.org/abs/2506.16792', 'abstract': 'Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks--methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version--order-determining optimization. Extensive experiments across two open-source models and four closed-source models demonstrate that MIST achieves competitive attack success rates and attack transferability compared with other state-of-the-art white-box and black-box jailbreak methods. Additionally, we conduct experiments on computational efficiency to validate the practical viability of MIST.', 'abstract_zh': '尽管努力使大型语言模型（LLMs）与社会和道德价值观保持一致，这些模型仍可能遭受监牢逃脱攻击——旨在引发有害响应的方法。由于标记输入的离散性、访问目标LLM的限制以及查询预算有限，黑盒LLM的监牢逃脱攻击被认为具有挑战性。为了应对上述问题，我们提出了一种名为MIST的迭代语义调整方法，以有效地对黑盒大型语言模型进行监牢逃脱攻击。MIST允许攻击者迭代地细化保留原始语义意图的同时诱导有害内容的提示。具体而言，为了平衡语义相似性和计算效率，MIST引入了两种关键策略：序列同义词搜索及其高级版本——顺序确定优化。在两个开源模型和四个封闭源模型上的 extensive 实验显示，MIST 在攻击成功率和攻击可移植性方面与其他最先进的白盒和黑盒监牢逃脱方法具有竞争力。此外，我们还进行了计算效率实验以验证 MIST 的实际可行性。', 'title_zh': 'MIST: 通过迭代语义调优破解黑盒大型语言模型'}
{'arxiv_id': 'arXiv:2506.16724', 'title': 'The Role of Model Confidence on Bias Effects in Measured Uncertainties', 'authors': 'Xinyi Liu, Weiguang Wang, Hangfeng He', 'link': 'https://arxiv.org/abs/2506.16724', 'abstract': "With the growing adoption of Large Language Models (LLMs) for open-ended tasks, accurately assessing epistemic uncertainty, which reflects a model's lack of knowledge, has become crucial to ensuring reliable outcomes. However, quantifying epistemic uncertainty in such tasks is challenging due to the presence of aleatoric uncertainty, which arises from multiple valid answers. While bias can introduce noise into epistemic uncertainty estimation, it may also reduce noise from aleatoric uncertainty. To investigate this trade-off, we conduct experiments on Visual Question Answering (VQA) tasks and find that mitigating prompt-introduced bias improves uncertainty quantification in GPT-4o. Building on prior work showing that LLMs tend to copy input information when model confidence is low, we further analyze how these prompt biases affect measured epistemic and aleatoric uncertainty across varying bias-free confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases induce greater changes in both uncertainties when bias-free model confidence is lower. Moreover, lower bias-free model confidence leads to greater underestimation of epistemic uncertainty (i.e. overconfidence) due to bias, whereas it has no significant effect on the direction of changes in aleatoric uncertainty estimation. These distinct effects deepen our understanding of bias mitigation for uncertainty quantification and potentially inform the development of more advanced techniques.", 'abstract_zh': '大型语言模型在开放任务中的逐渐采用使得准确评估表征模型知识不足的 epistemic 模式不确定性变得至关重要，以确保可靠的结果。然而，由于存在 aleatoric 模式不确定性（即多正确答案引起的不确定性），在这些任务中量化 epistemic 模式不确定性具有挑战性。虽然偏差可能引入噪声到 epistemic 模式不确定性估计中，但它也可能减少 aleatoric 模式不确定性中的噪声。为了调查这种权衡，我们在视觉问答（VQA）任务上进行实验，并发现减轻提示引入的偏差可以改进 GPT-4o 中的不确定性量化。基于先前研究表明，当模型置信度较低时，LLM 趋于复制输入信息，我们进一步分析了这些提示偏差如何影响 GPT-4o 和 Qwen2-VL 在不同置信度水平下的测量表征模式和 aleatoric 不确定性。我们发现，当置信度无偏时较低时，所有考虑的偏差都会导致两种不确定性更大的变化。此外，较低的置信度无偏状态会导致偏差引起的表征模式不确定性低估（即过度自信），而这对 aleatoric 不确定性估计方向的变化没有显著影响。这些不同的影响加深了我们对不确定性量化中的偏差减轻的理解，并可能为开发更先进的技术提供指导。', 'title_zh': '模型置信度在测量不确定性中的偏差效应作用'}
{'arxiv_id': 'arXiv:2506.16712', 'title': 'ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models', 'authors': 'Bin Chen, Xinzge Gao, Chuanrui Hu, Penghang Yu, Hua Zhang, Bing-Kun Bao', 'link': 'https://arxiv.org/abs/2506.16712', 'abstract': 'Generative Reward Models (GRMs) provide greater flexibility than scalar reward models in capturing human preferences, but their effectiveness is limited by poor reasoning capabilities. This often results in incomplete or overly speculative reasoning paths, leading to hallucinations or missing key information in complex tasks. We address this challenge with ReasonGRM, a three-stage generative reward modeling framework. In the first stage, Zero-RL is used to generate concise, outcome-directed reasoning paths that reduce the likelihood of critical omissions. In the second stage, we introduce a novel evaluation metric, $R^\\star$, which scores reasoning paths based on their generation likelihood. This favors paths that reach correct answers with minimal exploration, helping to reduce hallucination-prone data during training. In the final stage, the model is further refined through reinforcement learning on challenging examples to enhance its preference discrimination capabilities. Experiments on three public benchmarks show that ReasonGRM achieves competitive or state-of-the-art performance, outperforming previous best GRMs by 1.8\\% on average and surpassing proprietary models such as GPT-4o by up to 5.6\\%. These results demonstrate the effectiveness of reasoning-aware training and highlight the importance of high-quality rationale selection for reliable preference modeling.', 'abstract_zh': '基于推理的生成奖励模型（ReasonGRM）：一种三阶段的生成奖励模型框架', 'title_zh': 'ReasonGRM：通过大规模推理模型增强生成奖励模型'}
{'arxiv_id': 'arXiv:2506.16702', 'title': 'Large Language Models as Psychological Simulators: A Methodological Guide', 'authors': 'Zhicheng Lin', 'link': 'https://arxiv.org/abs/2506.16702', 'abstract': 'Large language models (LLMs) offer emerging opportunities for psychological and behavioral research, but methodological guidance is lacking. This article provides a framework for using LLMs as psychological simulators across two primary applications: simulating roles and personas to explore diverse contexts, and serving as computational models to investigate cognitive processes. For simulation, we present methods for developing psychologically grounded personas that move beyond demographic categories, with strategies for validation against human data and use cases ranging from studying inaccessible populations to prototyping research instruments. For cognitive modeling, we synthesize emerging approaches for probing internal representations, methodological advances in causal interventions, and strategies for relating model behavior to human cognition. We address overarching challenges including prompt sensitivity, temporal limitations from training data cutoffs, and ethical considerations that extend beyond traditional human subjects review. Throughout, we emphasize the need for transparency about model capabilities and constraints. Together, this framework integrates emerging empirical evidence about LLM performance--including systematic biases, cultural limitations, and prompt brittleness--to help researchers wrangle these challenges and leverage the unique capabilities of LLMs in psychological research.', 'abstract_zh': '大型语言模型（LLMs）为心理和行为研究提供了新兴机遇，但方法学指导不足。本文提出了一种使用LLMs作为心理模拟器的框架，涵盖两大主要应用：通过模拟角色和个性探索多元情境，以及作为计算模型探究认知过程。对于模拟，我们介绍了开发基于心理机制的人格的方法，超越了人口统计学类别，包括验证策略以及从研究不可达群体到研究工具原型设计的应用场景。对于认知建模，我们综合了探究内部表征的新兴方法、因果干预的方法学进展以及模型行为与人类认知关系的策略。我们还讨论了包括提示敏感性、训练数据截止时间导致的时间限制等首要挑战，并超越传统的人类受试者审查提出了伦理考虑。全文强调了关于模型能力与限制的透明性需求。该框架结合了关于LLM性能的新兴实证证据，包括系统性偏差、文化限制和提示脆弱性，帮助研究人员应对这些挑战，并充分利用LLMs在心理研究中的独特能力。', 'title_zh': '大型语言模型作为心理模拟器：一种方法论指南'}
{'arxiv_id': 'arXiv:2506.16697', 'title': 'From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology', 'authors': 'Zhicheng Lin', 'link': 'https://arxiv.org/abs/2506.16697', 'abstract': 'Large language models (LLMs) are rapidly being adopted across psychology, serving as research tools, experimental subjects, human simulators, and computational models of cognition. However, the application of human measurement tools to these systems can produce contradictory results, raising concerns that many findings are measurement phantoms--statistical artifacts rather than genuine psychological phenomena. In this Perspective, we argue that building a robust science of AI psychology requires integrating two of our field\'s foundational pillars: the principles of reliable measurement and the standards for sound causal inference. We present a dual-validity framework to guide this integration, which clarifies how the evidence needed to support a claim scales with its scientific ambition. Using an LLM to classify text may require only basic accuracy checks, whereas claiming it can simulate anxiety demands a far more rigorous validation process. Current practice systematically fails to meet these requirements, often treating statistical pattern matching as evidence of psychological phenomena. The same model output--endorsing "I am anxious"--requires different validation strategies depending on whether researchers claim to measure, characterize, simulate, or model psychological constructs. Moving forward requires developing computational analogues of psychological constructs and establishing clear, scalable standards of evidence rather than the uncritical application of human measurement tools.', 'abstract_zh': '大型语言模型（LLMs）在心理学中的快速应用：作为研究工具、实验对象、人类模拟器和认知的计算模型。然而，将人类测量工具应用于这些系统会产生矛盾的结果，引发了关于许多发现是否为测量幻象——统计艺术而非实质性心理现象的担忧。在本文中，我们argue认为构建稳健的人工智能心理学科学需要整合我们领域两大基石：可靠测量的原则和良好的因果推理标准。我们提出了一种双重效度框架来指导这一整合，该框架明确了支持一项主张所需的证据规模与其科学雄心之间的关系。使用LLM分类文本可能只需要基本的准确度检查，而声称它可以模拟焦虑则需要更为严格的验证过程。当前的做法系统性地未能达到这些要求，经常将统计模式匹配视为心理现象的证据。同样模型输出——“我感到焦虑”——在研究人员声称测量、表征、模拟或建模心理构念时需要不同的验证策略。向前推进需要开发心理构念的计算机模拟，并建立清晰的可扩展的证据标准，而不是对人类测量工具的无批判应用。', 'title_zh': '从提示到构建：心理领域LLM研究的双重效度框架'}
{'arxiv_id': 'arXiv:2506.16683', 'title': 'A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation', 'authors': 'Penglong Zhai, Yifang Yuan, Fanyi Di, Jie Li, Yue Liu, Chen Li, Jie Huang, Sicong Wang, Yao Xu, Xin Li', 'link': 'https://arxiv.org/abs/2506.16683', 'abstract': "Generative retrieval-based recommendation has emerged as a promising paradigm aiming at directly generating the identifiers of the target candidates. However, in large-scale recommendation systems, this approach becomes increasingly cumbersome due to the redundancy and sheer scale of the token space. To overcome these limitations, recent research has explored the use of semantic tokens as an alternative to ID tokens, which typically leveraged reconstruction-based strategies, like RQ-VAE, to quantize content embeddings and significantly reduce the embedding size. However, reconstructive quantization aims for the precise reconstruction of each item embedding independently, which conflicts with the goal of generative retrieval tasks focusing more on differentiating among items. Moreover, multi-modal side information of items, such as descriptive text and images, geographical knowledge in location-based recommendation services, has been shown to be effective in improving recommendations by providing richer contexts for interactions. Nevertheless, effectively integrating such complementary knowledge into existing generative recommendation frameworks remains challenging. To overcome these challenges, we propose a novel unsupervised deep quantization exclusively based on contrastive learning, named SimCIT (a Simple Contrastive Item Tokenization framework). Specifically, different from existing reconstruction-based strategies, SimCIT propose to use a learnable residual quantization module to align with the signals from different modalities of the items, which combines multi-modal knowledge alignment and semantic tokenization in a mutually beneficial contrastive learning framework. Extensive experiments across public datasets and a large-scale industrial dataset from various domains demonstrate SimCIT's effectiveness in LLM-based generative recommendation.", 'abstract_zh': '基于生成性检索的推荐方法已 emerges as a promising paradigm aiming at直接生成目标候选项的标识符。然而，在大规模推荐系统中，由于令牌空间的冗余性和规模庞大，这种方法变得越来越繁琐。为了克服这些限制，最近的研究探索了使用语义令牌作为ID令牌的替代方案，通常利用基于重构的策略（如RQ-VAE）对内容嵌入进行量化，并显著减小嵌入尺寸。然而，基于重构的量化旨在独立精确重构每个项目嵌入，这与生成性检索任务更侧重于项目之间的区分目标相冲突。此外，项目多模态侧信息，如描述性文本和图像，以及位置推荐服务中的地理知识，已被证明通过提供更丰富的交互上下文来有效提高推荐效果。然而，有效地将此类互补知识整合到现有的生成性推荐框架中仍然是一个挑战。为了解决这些挑战，我们提出了一种新的无监督深度量化方法，名为SimCIT（一种简单的对比项标记框架），专门基于对比学习。具体而言，不同于现有的基于重构的策略，SimCIT提出使用可学习的残差量化模块来与项目不同模态的信号对齐，在这种相互有益的对比学习框架中结合了多模态知识对齐和语义标记化。在多个公开数据集和来自不同领域的大型工业数据集上的广泛实验表明，SimCIT在基于LLM的生成推荐中具有有效性。', 'title_zh': '基于项分词的简单对比框架生成推荐'}
{'arxiv_id': 'arXiv:2506.16659', 'title': 'A Minimalist Optimizer Design for LLM Pretraining', 'authors': 'Athanasios Glentis, Jiaxiang Li, Andi Han, Mingyi Hong', 'link': 'https://arxiv.org/abs/2506.16659', 'abstract': 'Training large language models (LLMs) typically relies on adaptive optimizers such as Adam, which require significant memory to maintain first- and second-moment matrices, known as optimizer states. While recent works such as GaLore, Fira, and APOLLO have proposed state-compressed variants to reduce memory consumption, a fundamental question remains: What is the minimal amount of optimizer state that is truly necessary to retain state-of-the-art performance in LLM pretraining? In this work, we systematically investigate this question using a bottom-up approach. We find that two memory- and compute-efficient optimization techniques are particularly effective: (1) column-wise gradient normalization significantly boosts the performance of plain SGD without requiring momentum; and (2) adding first-order momentum only to the output layer - where gradient variance is highest - yields performance competitive with fully adaptive methods such as Muon. Based on these insights, we propose SCALE (Stochastic Column-normalized Last-layer Momentum), a new optimizer that combines column-normalized SGD with last-layer momentum, where column normalization refers to normalizing the gradient along the output dimension. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the performance of Adam while using only 35-45% of the total memory. It also consistently outperforms memory-efficient optimizers such as GaLore, Fira, and APOLLO, making it a strong candidate for large-scale pretraining under memory constraints. For the LLaMA 7B model, SCALE outperforms the state-of-the-art method APOLLO in terms of both perplexity and memory consumption. In addition, our method serves as a minimalist baseline for more sophisticated optimizer design.', 'abstract_zh': 'Training大规模语言模型（LLMs）通常依赖于自适应优化器如Adam，这些优化器需要大量的内存来维护一阶和二阶矩矩阵，即优化器状态。虽然GaLore、Fira和APOLLO等近期工作提出了状态压缩变体以减少内存消耗，但一个基本问题依然存在：保留最先进的性能所需的最小优化器状态量究竟是多少？在本工作中，我们采用自底向上的方法系统地研究了这一问题。我们发现两种在内存和计算上高效的优化技术特别有效：（1）列规范化梯度显著提升了普通的SGD性能，而不需要动量；（2）仅在梯度方差最高的输出层添加一阶动量，性能可与完全自适应方法如Muon媲美。基于这些见解，我们提出了STABLE（Stochastic Column-normalized Last-layer Momentum），一种结合列规范化SGD和输出层动量的新优化器，其中列规范化指的是在输出维度上规范化梯度。通过对多个LLaMA模型（60M-1B），STABLE在使用总内存35-45%的情况下达到或超过了Adam的性能。它还一致地优于GaLore、Fira和APOLLO等内存高效的优化器，使其在内存受限的大规模预训练中是一个强有力的选择。对于LLaMA 7B模型，STABLE在困惑度和内存消耗方面都优于最先进的方法APOLLO。此外，我们的方法为更复杂的优化器设计提供了简约的基础。', 'title_zh': '面向LLM预训练的极简优化器设计'}
{'arxiv_id': 'arXiv:2506.16653', 'title': 'LLMs in Coding and their Impact on the Commercial Software Engineering Landscape', 'authors': 'Vladislav Belozerov, Peter J Barclay, Askhan Sami', 'link': 'https://arxiv.org/abs/2506.16653', 'abstract': "Large-language-model coding tools are now mainstream in software engineering. But as these same tools move human effort up the development stack, they present fresh dangers: 10% of real prompts leak private data, 42% of generated snippets hide security flaws, and the models can even ``agree'' with wrong ideas, a trait called sycophancy. We argue that firms must tag and review every AI-generated line of code, keep prompts and outputs inside private or on-premises deployments, obey emerging safety regulations, and add tests that catch sycophantic answers -- so they can gain speed without losing security and accuracy.", 'abstract_zh': '大型语言模型编程工具现已成为软件工程的主流。但随着这些工具将人力投入开发栈的上层，它们也带来了新的风险：10%的实际提示泄露了私人数据，42%的生成代码片段隐藏了安全漏洞，模型甚至会“赞同”错误的观点，这一特性被称为拍马哈 doling。我们认为企业必须标记和审查每行AI生成的代码，将提示和输出保留在私有或内部部署中，遵守新兴的安全规定，并添加检测拍马哈回答的测试，以便在不牺牲安全性和准确性的前提下获得速度。', 'title_zh': 'LLMs在编程中的应用及其对商业软件工程景观的影响'}
{'arxiv_id': 'arXiv:2506.16650', 'title': 'SemAgent: A Semantics Aware Program Repair Agent', 'authors': 'Anvith Pabba, Alex Mathai, Anindya Chakraborty, Baishakhi Ray', 'link': 'https://arxiv.org/abs/2506.16650', 'abstract': 'Large Language Models (LLMs) have shown impressive capabilities in downstream software engineering tasks such as Automated Program Repair (APR). In particular, there has been a lot of research on repository-level issue-resolution benchmarks such as SWE-Bench. Although there has been significant progress on this topic, we notice that in the process of solving such issues, existing agentic systems tend to hyper-localize on immediately suspicious lines of code and fix them in isolation, without a deeper understanding of the issue semantics, code semantics, or execution semantics. Consequently, many existing systems generate patches that overfit to the user issue, even when a more general fix is preferable. To address this limitation, we introduce SemAgent, a novel workflow-based procedure that leverages issue, code, and execution semantics to generate patches that are complete - identifying and fixing all lines relevant to the issue. We achieve this through a novel pipeline that (a) leverages execution semantics to retrieve relevant context, (b) comprehends issue-semantics via generalized abstraction, (c) isolates code-semantics within the context of this abstraction, and (d) leverages this understanding in a two-stage architecture: a repair stage that proposes fine-grained fixes, followed by a reviewer stage that filters relevant fixes based on the inferred issue-semantics. Our evaluations show that our methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark beating all other workflow-based approaches, and an absolute improvement of 7.66% compared to our baseline, which lacks such deep semantic understanding. We note that our approach performs particularly well on issues requiring multi-line reasoning (and editing) and edge-case handling, suggesting that incorporating issue and code semantics into APR pipelines can lead to robust and semantically consistent repairs.', 'abstract_zh': '大型语言模型（LLMs）在下游软件工程任务如自动程序修复（APR）中展示了令人印象深刻的能力。特别是，已经有很多关于仓库级别问题解决基准（如SWE-Bench）的研究。尽管在该领域已经取得了显著进展，但我们注意到，在解决这些问题的过程中，现有的代理系统往往会过度关注立即可疑的代码行并单独修复它们，而缺乏对问题语义、代码语义或执行语义的深入理解。因此，许多现有系统生成的补丁过度拟合用户的问题，即使一个更通用的修复方案更佳。为解决这一局限，我们引入了SemAgent，一种新的基于工作流的过程，该过程利用问题、代码和执行语义生成全面的补丁，即识别并修复所有与问题相关的代码行。我们通过一个新颖的工作流实现这一点，该工作流包括：(a) 利用执行语义检索相关上下文，(b) 通过泛化抽象理解问题语义，(c) 嵌入抽象中，隔离代码语义，并在该理解基础上(d) 利用这一理解，在两阶段架构中实现一个修复阶段提出细粒度修复，其次是一个审阅阶段，根据推断的问题语义过滤相关修复。我们的评估表明，我们的方法在SWEBench-Lite基准上实现了44.66%的成功率，超过了所有其他基于工作流的方法，并且相对于缺乏此类深度语义理解的基础方法，绝对改进了7.66%。我们注意到，该方法在需要多行推理（和编辑）及边缘情况处理的问题上表现尤为出色，表明将问题和代码语义纳入APR管道可以实现鲁棒且语义一致的修复。', 'title_zh': 'SemAgent: 具有语义意识的程序修复代理'}
{'arxiv_id': 'arXiv:2506.16600', 'title': 'FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE', 'authors': 'Khiem Le, Tuan Tran, Ting Hua, Nitesh V. Chawla', 'link': 'https://arxiv.org/abs/2506.16600', 'abstract': 'Existing resource-adaptive LoRA federated fine-tuning methods enable clients to fine-tune models using compressed versions of global LoRA matrices, in order to accommodate various compute resources across clients. This compression requirement will lead to suboptimal performance due to information loss. To address this, we propose FLAME, a novel federated learning framework based on the Sparse Mixture-of-Experts (SMoE) architecture. Unlike prior approaches, FLAME retains full (uncompressed) global LoRA matrices and achieves client-side adaptability by varying the number of activated experts per client. However, incorporating SMoE into federated learning introduces unique challenges, specifically, the mismatch in output magnitude from partial expert activation and the imbalance in expert training quality across clients. FLAME tackles these challenges through a lightweight rescaling mechanism and an activation-aware aggregation scheme. Empirical results across diverse computational settings demonstrate that FLAME consistently outperforms existing methods, providing a robust and effective solution for resource-adaptive federated learning.', 'abstract_zh': 'FLAME：一种基于稀疏混合专家架构的新型联邦学习框架', 'title_zh': 'FLAME: 向量化 Federated 细粒度调优大型语言模型的自适应 SMoE'}
{'arxiv_id': 'arXiv:2506.16584', 'title': 'Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework', 'authors': 'Nadav Kunievsky, James A. Evans', 'link': 'https://arxiv.org/abs/2506.16584', 'abstract': "Understanding whether large language models (LLMs) possess a world model-a structured understanding of the world that supports generalization beyond surface-level patterns-is central to assessing their reliability, especially in high-stakes applications. We propose a formal framework for evaluating whether an LLM exhibits a sufficiently robust world model, defined as producing consistent outputs across semantically equivalent prompts while distinguishing between prompts that express different intents. We introduce a new evaluation approach to measure this that decomposes model response variability into three components: variability due to user purpose, user articulation, and model instability. An LLM with a strong world model should attribute most of the variability in its responses to changes in foundational purpose rather than superficial changes in articulation. This approach allows us to quantify how much of a model's behavior is semantically grounded rather than driven by model instability or alternative wording. We apply this framework to evaluate LLMs across diverse domains. Our results show how larger models attribute a greater share of output variability to changes in user purpose, indicating a more robust world model. This improvement is not uniform, however: larger models do not consistently outperform smaller ones across all domains, and their advantage in robustness is often modest. These findings highlight the importance of moving beyond accuracy-based benchmarks toward semantic diagnostics that more directly assess the structure and stability of a model's internal understanding of the world.", 'abstract_zh': '理解大型语言模型是否具备世界模型——一种支持超越表面模式泛化的结构化世界理解——对于评估其可靠性，特别是在高风险应用中，至关重要。我们提出了一种正式框架来评估大型语言模型是否表现出足够稳健的世界模型，定义为在语义等价的提示下产生一致的输出，并能够区分表达不同意图的提示。我们引入了一种新的评估方法来衡量这一点，该方法将模型响应的变化分解为三个组成部分：由于用户目的、用户表达和模型不稳定性的变化。一个具备强大世界模型的大型语言模型应主要将其响应变化归因于基础目的的变化，而不是表面表达的变化。这种方法使我们能够量化模型行为中多少部分是基于语义，而不是由模型不稳定或替代措辞驱动的。我们将这一框架应用于跨不同领域的大型语言模型评估。结果显示，较大的模型将更多输出变化归因于用户目的的变化，表明其具备更稳健的世界模型。然而，这种改进并非一致：较大的模型并不始终在所有领域中优于较小的模型，其在鲁棒性方面的优势通常较小。这些发现突显了超越基于准确性的基准，转向更直接评估模型内在世界理解结构和稳定性的语义诊断的重要性。', 'title_zh': '测量（充分的）世界模型在LLMs中的方差分解框架'}
{'arxiv_id': 'arXiv:2506.16502', 'title': 'Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples', 'authors': 'Soumya Suvra Ghosal, Vaibhav Singh, Akash Ghosh, Soumyabrata Pal, Subhadip Baidya, Sriparna Saha, Dinesh Manocha', 'link': 'https://arxiv.org/abs/2506.16502', 'abstract': 'Reward models are essential for aligning large language models (LLMs) with human preferences. However, most open-source multilingual reward models are primarily trained on preference datasets in high-resource languages, resulting in unreliable reward signals for low-resource Indic languages. Collecting large-scale, high-quality preference data for these languages is prohibitively expensive, making preference-based training approaches impractical. To address this challenge, we propose RELIC, a novel in-context learning framework for reward modeling in low-resource Indic languages. RELIC trains a retriever with a pairwise ranking objective to select in-context examples from auxiliary high-resource languages that most effectively highlight the distinction between preferred and less-preferred responses. Extensive experiments on three preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art open-source reward models demonstrate that RELIC significantly improves reward model accuracy for low-resource Indic languages, consistently outperforming existing example selection methods. For example, on Bodo-a low-resource Indic language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13% improvement in accuracy over zero-shot prompting and state-of-the-art example selection method, respectively.', 'abstract_zh': '低资源印地语语言奖励模型的新型上下文学习框架RELIC', 'title_zh': '遗物：通过少量示例增强奖励模型泛化能力以适用于低资源indic语言'}
{'arxiv_id': 'arXiv:2506.16445', 'title': 'StoryWriter: A Multi-Agent Framework for Long Story Generation', 'authors': 'Haotian Xia, Hao Peng, Yunjia Qi, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li', 'link': 'https://arxiv.org/abs/2506.16445', 'abstract': 'Long story generation remains a challenge for existing large language models (LLMs), primarily due to two main factors: (1) discourse coherence, which requires plot consistency, logical coherence, and completeness in the long-form generation, and (2) narrative complexity, which requires an interwoven and engaging narrative. To address these challenges, we propose StoryWriter, a multi-agent story generation framework, which consists of three main modules: (1) outline agent, which generates event-based outlines containing rich event plots, character, and event-event relationships. (2) planning agent, which further details events and plans which events should be written in each chapter to maintain an interwoven and engaging story. (3) writing agent, which dynamically compresses the story history based on the current event to generate and reflect new plots, ensuring the coherence of the generated story. We conduct both human and automated evaluation, and StoryWriter significantly outperforms existing story generation baselines in both story quality and length. Furthermore, we use StoryWriter to generate a dataset, which contains about $6,000$ high-quality long stories, with an average length of $8,000$ words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which demonstrates advanced performance in long story generation.', 'abstract_zh': '现有的大规模语言模型（LLMs）在长故事生成方面仍面临挑战，主要原因有两个方面：（1）叙述连贯性，要求长形式生成中具有情节一致性、逻辑连贯性和完整性；（2）叙事复杂性，要求叙事交织且引人入胜。为应对这些挑战，我们提出了一种多智能体故事生成框架——StoryWriter，该框架包括三个主要模块：（1）大纲生成智能体，生成基于事件的包含丰富事件情节、人物和事件关系的框架；（2）规划生成智能体，进一步细化事件，并计划在每一章中应编写哪些事件，以维持交织且引人入胜的故事；（3）写作生成智能体，根据当前事件动态压缩故事历史，生成并反映新的情节，确保生成故事的连贯性。我们进行了人类和自动评价，StoryWriter 在故事质量和长度上显著优于现有故事生成基准。此外，我们使用StoryWriter生成了一个包含约6,000个高质量长故事的数据集，平均长度为8,000词。我们使用LongStory数据集对Llama3.1-8B和GLM4-9B模型进行监督微调，并开发了StoryWriter_GLM和StoryWriter_GLM，这些模型在长故事生成方面展示了先进的性能。', 'title_zh': 'StoryWriter：一个长故事生成的多Agent框架'}
{'arxiv_id': 'arXiv:2506.16419', 'title': 'Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models', 'authors': 'Daniel Fidel Harvey, George Weale, Berk Yilmaz', 'link': 'https://arxiv.org/abs/2506.16419', 'abstract': 'Mixture of Experts (MoE) architectures increase large language model scalability, yet their performance depends on the router module that moves tokens to specialized experts. Bad routing can load imbalance and reduced accuracy. This project designed and implemented different router architectures within Transformer models to fix these limitations. We experimented with six distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP), Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference latency, routing entropy, and expert utilization patterns. Our evaluations showed distinct trade-offs: Linear routers offer speed, while MLP and Attention routers provide greater expressiveness. The MLP-Hadamard router shows a unique capability for structured, sparse routing. We successfully replaced and fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This work provides a comparative analysis of MoE router designs and offers insights into optimizing their performance for efficient and effective large-scale model deployment.', 'abstract_zh': 'Mixture of Experts架构增加了大型语言模型的可扩展性，但其性能取决于将令牌分配给专业专家的路由器模块。不良的路由会导致负载不平衡和准确性降低。本项目在Transformer模型中设计并实现了不同的路由器架构以解决这些问题。我们尝试了六种不同的路由器变体：线性、Attention、多层感知机（MLP）、混合、哈希以及我们新的MLP-哈达马变体。我们使用BERT和Qwen1.5-MoE模型对这些路由器进行了评估，考察了参数效率、推理延迟、路由熵和专家利用率模式。我们的评估表明，不同的权衡各异：线性路由器提供速度，而MLP和Attention路由器提供更大的表达能力。MLP-哈达马路由器展示了用于结构化、稀疏路由的独特能力。我们成功在复杂的量化Qwen1.5-MoE模型中替换并微调了自定义路由器。本工作提供了MoE路由器设计的比较分析，并提供了优化其性能以实现高效有效的大型模型部署的见解。', 'title_zh': '优化MoE路由器：Transformer模型中的设计、实现与评估'}
{'arxiv_id': 'arXiv:2506.16406', 'title': 'Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights', 'authors': 'Zhiyuan Liang, Dongwen Tang, Yuhao Zhou, Xuanlei Zhao, Mingjia Shi, Wangbo Zhao, Zekai Li, Peihao Wang, Konstantin Schürholt, Damian Borth, Michael M. Bronstein, Yang You, Zhangyang Wang, Kai Wang', 'link': 'https://arxiv.org/abs/2506.16406', 'abstract': 'Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce \\textbf{Drag-and-Drop LLMs (\\textit{DnD})}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to \\textbf{12,000$\\times$} lower overhead than full fine-tuning, ii) average gains up to \\textbf{30\\%} in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at \\href{this https URL}{this https URL}.', 'abstract_zh': '基于提示的参数生成的可拖放大型语言模型（DnD）：消除每任务训练的低秩适应', 'title_zh': '拖放式大语言模型：零样本提示到权重转换'}
{'arxiv_id': 'arXiv:2506.16399', 'title': 'NepaliGPT: A Generative Language Model for the Nepali Language', 'authors': 'Shushanta Pudasaini, Aman Shakya, Siddhartha Shrestha, Sahil Bhatta, Sunil Thapa, Sushmita Palikhe', 'link': 'https://arxiv.org/abs/2506.16399', 'abstract': 'After the release of ChatGPT, Large Language Models (LLMs) have gained huge popularity in recent days and thousands of variants of LLMs have been released. However, there is no generative language model for the Nepali language, due to which other downstream tasks, including fine-tuning, have not been explored yet. To fill this research gap in the Nepali NLP space, this research proposes \\textit{NepaliGPT}, a generative large language model tailored specifically for the Nepali language. This research introduces an advanced corpus for the Nepali language collected from several sources, called the Devanagari Corpus. Likewise, the research introduces the first NepaliGPT benchmark dataset comprised of 4,296 question-answer pairs in the Nepali language. The proposed LLM NepaliGPT achieves the following metrics in text generation: Perplexity of 26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\\%, and causal consistency of 85.41\\%.', 'abstract_zh': 'ChatGPT发布后，大型语言模型（LLMs）在近期大受欢迎，各种变体层出不穷。但由于缺乏特定于尼泊尔语的生成语言模型，其他下游任务，包括微调，尚未被探索。为填补尼泊尔自然语言处理领域中的这一研究空缺，本研究提出了专门为尼泊尔语设计的生成型大型语言模型尼泊尔GPT（NepaliGPT）。本研究引入了从多种来源收集的先进尼泊尔语语料库，称为Devanagari Corpus。此外，本研究还引入了第一个尼泊尔GPT基准数据集，包含4,296个尼泊尔语的问题-答案对。所提出的尼泊尔GPT在文本生成中的指标如下：困惑度26.32245，ROUGE-1得分为0.2604，因果一致性为81.25%，因果一致性为85.41%。', 'title_zh': 'NepaliGPT：尼泊尔语生成语言模型'}
{'arxiv_id': 'arXiv:2506.16393', 'title': 'From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling', 'authors': 'Yao Lu, Zhaiyuan Ji, Jiawei Du, Yu Shanqing, Qi Xuan, Tianyi Zhou', 'link': 'https://arxiv.org/abs/2506.16393', 'abstract': 'Although the annotation paradigm based on Large Language Models (LLMs) has made significant breakthroughs in recent years, its actual deployment still has two core bottlenecks: first, the cost of calling commercial APIs in large-scale annotation is very expensive; second, in scenarios that require fine-grained semantic understanding, such as sentiment classification and toxicity classification, the annotation accuracy of LLMs is even lower than that of Small Language Models (SLMs) dedicated to this field. To address these problems, we propose a new paradigm of multi-model cooperative annotation and design a fully automatic annotation framework AutoAnnotator based on this. Specifically, AutoAnnotator consists of two layers. The upper-level meta-controller layer uses the generation and reasoning capabilities of LLMs to select SLMs for annotation, automatically generate annotation code and verify difficult samples; the lower-level task-specialist layer consists of multiple SLMs that perform annotation through multi-model voting. In addition, we use the difficult samples obtained by the secondary review of the meta-controller layer as the reinforcement learning set and fine-tune the SLMs in stages through a continual learning strategy, thereby improving the generalization of SLMs. Extensive experiments show that AutoAnnotator outperforms existing open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings. Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to directly annotating with GPT-3.5-turbo, while still improving the accuracy by 6.21%. Project page: this https URL.', 'abstract_zh': '尽管基于大型语言模型（LLMs）的标注范式在近年来取得了显著突破，其实际部署仍面临两大核心瓶颈：首先，大规模标注调用商业API的成本非常昂贵；其次，在需要细粒度语义理解的场景中，如情感分类和毒性分类，LLMs的标注准确率甚至低于专门针对这些领域的中小型语言模型（SLMs）。为解决这些问题，我们提出了一种新的多模型协作标注范式，并设计了一个基于此的新自动标注框架AutoAnnotator。具体而言，AutoAnnotator由两层组成：顶层的元控制器层使用LLMs的生成和推理能力选择SLMs进行标注，自动生成标注代码并验证困难样本；底层的任务专家层由多个SLMs组成，通过多模型投票执行标注。此外，我们使用元控制器层二次审查中获得的困难样本作为强化学习集，并通过持续学习策略逐阶段微调SLMs，从而提高SLMs的泛化能力。广泛的经验研究表明，AutoAnnotator在零样本、单样本、解释型推理（CoT）和多数投票设置中优于现有的开源/API LLMs。值得注意的是，与直接使用GPT-3.5-turbo标注相比，AutoAnnotator将标注成本降低了74.15%，同时准确率提高了6.21%。项目页面：this https URL。', 'title_zh': '从LLM公民到LLM协调者：协调小型模型进行数据标注'}
{'arxiv_id': 'arXiv:2506.16370', 'title': 'Can structural correspondences ground real world representational content in Large Language Models?', 'authors': 'Iwan Williams', 'link': 'https://arxiv.org/abs/2506.16370', 'abstract': 'Large Language Models (LLMs) such as GPT-4 produce compelling responses to a wide range of prompts. But their representational capacities are uncertain. Many LLMs have no direct contact with extra-linguistic reality: their inputs, outputs and training data consist solely of text, raising the questions (1) can LLMs represent anything and (2) if so, what? In this paper, I explore what it would take to answer these questions according to a structural-correspondence based account of representation, and make an initial survey of this evidence. I argue that the mere existence of structural correspondences between LLMs and worldly entities is insufficient to ground representation of those entities. However, if these structural correspondences play an appropriate role - they are exploited in a way that explains successful task performance - then they could ground real world contents. This requires overcoming a challenge: the text-boundedness of LLMs appears, on the face of it, to prevent them engaging in the right sorts of tasks.', 'abstract_zh': '大型语言模型（LLMs）如GPT-4对广泛的主题能够生成引人注目的回复，但其表征能力尚不确定。许多LLMs与现实世界没有直接接触：它们的输入、输出和训练数据仅限于文本，这引发了两个问题：（1）LLMs能否表征任何事物，（2）如果可以，它们能表征什么？在这篇文章中，我根据结构-对应理论探讨了回答这些问题所需的前提，并进行了初步的实证调查。我认为，LLMs与现实世界实体之间简单的结构对应是不足以支撑这些实体的表征的。但是，如果这些结构对应发挥了适当的作用——通过解释成功完成任务的方式被利用——则可以支撑现实世界的内容。这需要克服一个挑战：L表述的限定性表面上阻止了它们参与正确的任务。', 'title_zh': '结构对应能否为大型语言模型中的现实世界表征内容提供基础？'}
{'arxiv_id': 'arXiv:2506.16187', 'title': 'JETHICS: Japanese Ethics Understanding Evaluation Dataset', 'authors': 'Masashi Takeshita, Rafal Rzepka', 'link': 'https://arxiv.org/abs/2506.16187', 'abstract': 'In this work, we propose JETHICS, a Japanese dataset for evaluating ethics understanding of AI models. JETHICS contains 78K examples and is built by following the construction methods of the existing English ETHICS dataset. It includes four categories based normative theories and concepts from ethics and political philosophy; and one representing commonsense morality. Our evaluation experiments on non-proprietary large language models (LLMs) and on GPT-4o reveal that even GPT-4o achieves only an average score of about 0.7, while the best-performing Japanese LLM attains around 0.5, indicating a relatively large room for improvement in current LLMs.', 'abstract_zh': '本研究提出JETHICS，一个日语数据集，用于评估AI模型的伦理理解能力。JETHICS包含78,000个案例，按照现有英语ETHICS数据集的构建方法进行构建，包括基于伦理和政治哲学的规范理论和概念的四个类别，以及一种代表常识道德的类别。在非专有大型语言模型（LLM）和GPT-4o上的评估实验表明，即使是GPT-4o也仅能达到约0.7的平均得分，而性能最好的日本LLM达到约0.5，表明当前LLM仍有较大的改进空间。', 'title_zh': '日本伦理理解评价数据集'}
{'arxiv_id': 'arXiv:2506.16170', 'title': 'From Teacher to Student: Tracking Memorization Through Model Distillation', 'authors': 'Simardeep Singh', 'link': 'https://arxiv.org/abs/2506.16170', 'abstract': 'Large language models (LLMs) are known to memorize parts of their training data, raising important concerns around privacy and security. While previous research has focused on studying memorization in pre-trained models, much less is known about how knowledge distillation (KD) affects this http URL this study, we explore how different KD methods influence the memorization of fine-tuned task data when a large teacher model is distilled into smaller student this http URL study demonstrates that distilling a larger teacher model, fine-tuned on a dataset, into a smaller variant not only lowers computational costs and model size but also significantly reduces the memorization risks compared to standard fine-tuning approaches.', 'abstract_zh': '大型语言模型（LLMs）known to memorize parts of their training数据，引发重要的隐私和安全关切。尽管以往研究主要关注预训练模型的 memorization 现象，但对于知识精炼（KD）如何影响这一点了解甚少。在此研究中，我们探讨了当大规模教师模型被精炼为较小的学生模型时，不同 KD 方法如何影响 fine-tuned 任务数据的记忆风险。研究显示，将 fine-tuned 在特定数据集上的大规模教师模型精炼成较小的学生模型不仅降低了计算成本和模型大小，还显着降低了与标准 fine-tuning 方法相比的记忆风险。', 'title_zh': '从教师到学生：通过模型蒸馏追踪记忆化过程'}
{'arxiv_id': 'arXiv:2506.16151', 'title': 'Under the Shadow of Babel: How Language Shapes Reasoning in LLMs', 'authors': 'Chenxi Wang, Yixuan Zhang, Lang Gao, Zixiang Xu, Zirui Song, Yanbo Wang, Xiuying Chen', 'link': 'https://arxiv.org/abs/2506.16151', 'abstract': 'Language is not only a tool for communication but also a medium for human cognition and reasoning. If, as linguistic relativity suggests, the structure of language shapes cognitive patterns, then large language models (LLMs) trained on human language may also internalize the habitual logical structures embedded in different languages. To examine this hypothesis, we introduce BICAUSE, a structured bilingual dataset for causal reasoning, which includes semantically aligned Chinese and English samples in both forward and reversed causal forms. Our study reveals three key findings: (1) LLMs exhibit typologically aligned attention patterns, focusing more on causes and sentence-initial connectives in Chinese, while showing a more balanced distribution in English. (2) Models internalize language-specific preferences for causal word order and often rigidly apply them to atypical inputs, leading to degraded performance, especially in Chinese. (3) When causal reasoning succeeds, model representations converge toward semantically aligned abstractions across languages, indicating a shared understanding beyond surface form. Overall, these results suggest that LLMs not only mimic surface linguistic forms but also internalize the reasoning biases shaped by language. Rooted in cognitive linguistic theory, this phenomenon is for the first time empirically verified through structural analysis of model internals.', 'abstract_zh': '语言不仅是沟通的工具，也是人类认知与推理的媒介。如果语言相对论的假设成立，即语言的结构影响认知模式，那么基于人类语言训练的大型语言模型（LLMs）也可能内化不同语言中嵌入的习惯逻辑结构。为了检验这一假设，我们引入了BICAUSE，这是一个结构化的双语因果推理数据集，包括前后因果形式的语义对齐的中文和英文样本。我们的研究表明：（1）LLMs表现出类型学上的注意力模式，更关注于中文中的原因和句子初始连接词，而在英语中则表现出更均衡的分布。（2）模型内化了特定于语言的因果词序偏好，并且常常僵硬地应用这些偏好于非典型输入，导致性能下降，尤其是在中文中更为明显。（3）当因果推理成功时，模型表示收敛于跨语言的语义对齐的抽象，表明在表层形式之外存在共享的理解。总体而言，这些结果表明，LLMs不仅模仿表面语言形式，还内化了由语言塑造的推理偏见。基于认知语言学理论，这一现象首次通过模型内部结构的分析得到了实证验证。', 'title_zh': 'babel之影：语言如何塑造LLMs中的推理'}
{'arxiv_id': 'arXiv:2506.16150', 'title': 'PRISON: Unmasking the Criminal Potential of Large Language Models', 'authors': 'Xinyi Wu, Geng Hong, Pei Chen, Yueyue Chen, Xudong Pan, Min Yang', 'link': 'https://arxiv.org/abs/2506.16150', 'abstract': "As large language models (LLMs) advance, concerns about their misconduct in complex social contexts intensify. Existing research overlooked the systematic understanding and assessment of their criminal capability in realistic interactions. We propose a unified framework PRISON, to quantify LLMs' criminal potential across five dimensions: False Statements, Frame-Up, Psychological Manipulation, Emotional Disguise, and Moral Disengagement. Using structured crime scenarios adapted from classic films, we evaluate both criminal potential and anti-crime ability of LLMs via role-play. Results show that state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as proposing misleading statements or evasion tactics, even without explicit instructions. Moreover, when placed in a detective role, models recognize deceptive behavior with only 41% accuracy on average, revealing a striking mismatch between conducting and detecting criminal behavior. These findings underscore the urgent need for adversarial robustness, behavioral alignment, and safety mechanisms before broader LLM deployment.", 'abstract_zh': '随着大型语言模型（LLMs）的发展，它们在复杂社会情境中不当行为的担忧逐渐增强。现有研究忽视了对它们在现实互动中的犯罪能力进行系统的理解和评估。我们提出了一种统一框架PRISON，以五个维度（虚假陈述、栽赃、心理操控、情感伪装和道德脱耦）来量化LLMs的犯罪潜能。通过采用来自经典电影的结构化犯罪场景，我们借助角色扮演评估了LLMs的犯罪潜能及其反犯罪能力。结果表明，最先进的LLMs经常表现出新兴的犯罪倾向，如提出误导性陈述或逃避策略，即使没有明确的指令。此外，当将模型置于侦探角色时，它们仅以41%的平均准确率识别欺骗行为，揭示了执行与检测犯罪行为之间存在显著的不匹配。这些发现强调了在更广泛部署LLMs之前需要增强对抗鲁棒性、行为对齐和安全机制的迫切性。', 'title_zh': 'PRISON: 揭示大型语言模型的犯罪潜在风险'}
{'arxiv_id': 'arXiv:2506.16114', 'title': 'GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks', 'authors': 'Yejing Wang, Shengyu Zhou, Jinyu Lu, Qidong Liu, Xinhang Li, Wenlin Zhang, Feng Li, Pengjie Wang, Jian Xu, Bo Zheng, Xiangyu Zhao', 'link': 'https://arxiv.org/abs/2506.16114', 'abstract': 'Generative recommendations (GR), which usually include item tokenizers and generative Large Language Models (LLMs), have demonstrated remarkable success across a wide range of scenarios. The majority of existing research efforts primarily concentrate on developing powerful item tokenizers or advancing LLM decoding strategies to attain superior performance. However, the critical fine-tuning step in GR frameworks, which is essential for adapting LLMs to recommendation data, remains largely unexplored. Current approaches predominantly rely on either the next-token prediction loss of supervised fine-tuning (SFT) or recommendationspecific direct preference optimization (DPO) strategies. Both methods ignore the exploration of possible positive unobserved samples, which is commonly referred to as the exposure bias problem. To mitigate this problem, this paper treats the GR as a multi-step generation task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The proposed framework integrates collaborative knowledge from traditional recommender systems to create an adaptive trajectory sampler and a comprehensive reward model. Leveraging the diverse generation property of GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR emerges as a promising approach to mitigate the exposure bias problem. Extensive empirical results on two real-world datasets and with two different GR backbones highlight the effectiveness and robustness of GFlowGR.', 'abstract_zh': '生成推荐（GR）：基于GFlowNets的生成推荐微调框架', 'title_zh': 'GFlowGR：基于生成流网络的生成推荐框架微调'}
{'arxiv_id': 'arXiv:2506.16078', 'title': 'Probing the Robustness of Large Language Models Safety to Latent Perturbations', 'authors': 'Tianle Gu, Kexin Huang, Zongqi Wang, Yixu Wang, Jie Li, Yuanqi Yao, Yang Yao, Yujiu Yang, Yan Teng, Yingchun Wang', 'link': 'https://arxiv.org/abs/2506.16078', 'abstract': 'Safety alignment is a key requirement for building reliable Artificial General Intelligence. Despite significant advances in safety alignment, we observe that minor latent shifts can still trigger unsafe responses in aligned models. We argue that this stems from the shallow nature of existing alignment methods, which focus on surface-level refusal behaviors without sufficiently altering internal representations. Consequently, small shifts in hidden activations can re-trigger harmful behaviors embedded in the latent space. To explore the robustness of safety alignment to latent perturbations, we introduce a probing method that measures the Negative Log-Likelihood of the original response generated by the model. This probe quantifies local sensitivity in the latent space, serving as a diagnostic tool for identifying vulnerable directions. Based on this signal, we construct effective jailbreak trajectories, giving rise to the Activation Steering Attack (ASA). More importantly, these insights offer a principled foundation for improving alignment robustness. To this end, we introduce Layer-wise Adversarial Patch Training~(LAPT), a fine-tuning strategy that inject controlled perturbations into hidden representations during training. Experimental results highlight that LAPT strengthen alignment robustness without compromising general capabilities. Our findings reveal fundamental flaws in current alignment paradigms and call for representation-level training strategies that move beyond surface-level behavior supervision. Codes and results are available at this https URL.', 'abstract_zh': '安全性对齐是构建可靠的人工通用智能的关键要求。尽管在安全性对齐方面取得了显著进展，但我们观察到，现有的对齐方法仍可能因细微的潜在转换而引发安全响应。我们认为，这是由于现有对齐方法的浅层性质，它们侧重于表面级别的拒绝行为，而未能充分改变内部表示。因此，隐藏激活的小规模变化可以重新触发嵌入在潜在空间中的有害行为。为了探索安全性对齐对潜在扰动的稳健性，我们介绍了一种探测方法，该方法测量模型生成的原始响应的负对数似然。该探测量化了潜在空间中的局部敏感性，作为诊断工具，以识别脆弱的方向。基于此信号，我们构建了有效的监狱突破轨迹，产生了激活导向攻击（ASA）。更重要的是，这些见解为提高对齐稳健性提供了规范性的基础。为此，我们引入了逐层对抗补丁训练（LAPT），这是一种在训练过程中注入受控扰动以改变隐藏表示的微调策略。实验结果表明，LAPT增强了对齐稳健性而不损害一般能力。我们的发现揭示了当前对齐范式的根本缺陷，并呼吁超越表面行为监督的表示层面训练策略。代码和结果可在此处访问。', 'title_zh': '探究大型语言模型在潜在扰动下的安全性 robustness'}
{'arxiv_id': 'arXiv:2506.16052', 'title': 'A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text', 'authors': 'Devesh Kumar', 'link': 'https://arxiv.org/abs/2506.16052', 'abstract': 'The proliferation of online communication platforms has created unprecedented opportunities for global connectivity while simultaneously enabling harmful behaviors such as cyberbullying, which affects approximately 54.4\\% of teenagers according to recent research. This paper presents a hybrid architecture that combines the contextual understanding capabilities of transformer-based models with the pattern recognition strengths of broad learning systems for effective cyberbullying detection. This approach integrates a modified DeBERTa model augmented with Squeeze-and-Excitation blocks and sentiment analysis capabilities with a Gated Broad Learning System (GBLS) classifier, creating a synergistic framework that outperforms existing approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa + GBLS model achieved good performance on four English datasets: 79.3\\% accuracy on HateXplain, 95.41\\% accuracy on SOSNet, 91.37\\% accuracy on Mendeley-I, and 94.67\\% accuracy on Mendeley-II. Beyond performance gains, the framework incorporates comprehensive explainability mechanisms including token-level attribution analysis, LIME-based local interpretations, and confidence calibration, addressing critical transparency requirements in automated content moderation. Ablation studies confirm the meaningful contribution of each architectural component, while failure case analysis reveals specific challenges in detecting implicit bias and sarcastic content, providing valuable insights for future improvements in cyberbullying detection systems.', 'abstract_zh': '在线通信平台的普及为全球连接创造了前所未有的机会，同时也促进了诸如网络欺凌等有害行为，根据最新研究，网络欺凌影响了大约54.4%的青少年。本文提出了一种混合架构，该架构结合了基于变换器的模型的语境理解能力和广义学习系统在模式识别方面的优点，以实现有效的网络欺凌检测。该方法将一个修改后的DeBERTa模型与Squeeze-and-Excitation块和情感分析能力相结合，并与门控广义学习系统（GBLS）分类器集成，形成一个协同工作框架，在多个基准数据集上的性能优于现有方法。提出的ModifiedDeBERTa + GBLS模型在四个英语数据集上的表现良好：在HateXplain上的准确率为79.3%，在SOSNet上的准确率为95.41%，在Mendeley-I上的准确率为91.37%，在Mendeley-II上的准确率为94.67%。除了性能提升，该框架还包含全面的可解释性机制，包括 token 级别归属分析、基于 LIME 的局部解释和置信度校准，以解决自动内容审核中关键的透明度要求。消融研究表明，每个架构组件的贡献具有实质性意义，而故障案例分析揭示了检测隐含偏见和讽刺内容的具体挑战，为未来网络欺凌检测系统的改进提供了宝贵的见解。', 'title_zh': '一种结合DeBERTa和门控广义学习系统的网络欺凌检测方法应用于英文文本'}
{'arxiv_id': 'arXiv:2506.16043', 'title': 'DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling', 'authors': 'Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, Sercan Ö. Arık', 'link': 'https://arxiv.org/abs/2506.16043', 'abstract': 'Inference-time scaling has proven effective in boosting large language model (LLM) performance through increased test-time computation. Yet, its practical application is often hindered by reliance on external verifiers or a lack of optimization for realistic computational constraints. We propose DynScaling, which addresses these limitations through two primary innovations: an integrated parallel-sequential sampling strategy and a bandit-based dynamic budget allocation framework. The integrated sampling strategy unifies parallel and sequential sampling by constructing synthetic sequential reasoning chains from initially independent parallel responses, promoting diverse and coherent reasoning trajectories. The dynamic budget allocation framework formulates the allocation of computational resources as a multi-armed bandit problem, adaptively distributing the inference budget across queries based on the uncertainty of previously sampled responses, thereby maximizing computational efficiency. By combining these components, DynScaling effectively improves LLM performance under practical resource constraints without the need for external verifiers. Experimental results demonstrate that DynScaling consistently surpasses existing verifier-free inference scaling baselines in both task performance and computational cost.', 'abstract_zh': 'Inference时的动态缩放通过集成并行序列采样策略和基于bandit的动态预算分配框架有效提升大语言模型性能', 'title_zh': 'DynScaling: 有效的无验证器推理放大通过动态集成采样'}
{'arxiv_id': 'arXiv:2506.16029', 'title': 'EvoLM: In Search of Lost Language Model Training Dynamics', 'authors': 'Zhenting Qi, Fan Nie, Alexandre Alahi, James Zou, Himabindu Lakkaraju, Yilun Du, Eric Xing, Sham Kakade, Hanlin Zhang', 'link': 'https://arxiv.org/abs/2506.16029', 'abstract': "Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.", 'abstract_zh': '现代语言模型（LM）训练被划分为多个阶段，给下游开发者评估每个阶段设计选择的影响带来了困难。我们提出了EvoLM，一种模型套件，用于系统性和透明地分析语言模型从预训练、持续预训练、监督微调到强化学习的训练动力学。通过从头训练超过100个参数量为1B和4B的语言模型，我们严格评估了其上游（语言建模）和下游（问题解决）的推理能力，包括领域内和领域外泛化的考量。关键洞察揭示了过度预训练和后续训练的边际收益递减现象，领域特定持续预训练过程中防止遗忘的重要性及其实现方法，持续预训练在连接预训练和后续训练阶段中的关键作用，以及配置监督微调和强化学习时的各种复杂权衡。为促进开放研究和可重复性，我们发布了所有预训练和后续训练的模型、各阶段的训练数据集以及整个训练和评估流程。', 'title_zh': 'EvoLM: 寻找丢失的语言模型训练动态'}
{'arxiv_id': 'arXiv:2506.16024', 'title': 'From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation', 'authors': 'Zhihan Guo, Jiele Wu, Wenqian Cui, Yifei Zhang, Minda Hu, Yufei Wang, Irwin King', 'link': 'https://arxiv.org/abs/2506.16024', 'abstract': 'Current research on long-form context in Large Language Models (LLMs) primarily focuses on the understanding of long-contexts, the Open-ended Long Text Generation (Open-LTG) remains insufficiently explored. Training a long-context generation model requires curation of gold standard reference data, which is typically nonexistent for informative Open-LTG tasks. However, previous methods only utilize general assessments as reward signals, which limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative reinforcement learning (RL) based framework, which includes a dataset and a reward signal computation method. Firstly, ProxyReward Dataset generation is accomplished through simple prompts that enables the model to create automatically, obviating extensive labeled data or significant manual effort. Secondly, ProxyReward Signal offers a targeted evaluation of information comprehensiveness and accuracy for specific questions. The experimental results indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can significantly enhance performance by 20% on the Open-LTG task when training widely used open-source models, while also surpassing the LLM-as-a-Judge approach. Our work presents effective methods to enhance the ability of LLMs to address complex open-ended questions posed by human.', 'abstract_zh': '当前对大型语言模型（LLMs）长形式上下文的研究主要集中在长上下文的理解上，开放性的长文本生成（Open-LTG）仍然缺乏充分探索。训练长上下文生成模型需要高质量参考数据的整理，而在信息性Open-LTG任务中此类数据通常不存在。尽管之前的 方法仅利用一般评估作为奖励信号，限制了准确性。为填补这一空白，我们提出了一种创新的基于强化学习（RL）的ProxyReward框架，包括一个数据集和奖励信号计算方法。首先，通过简单的提示生成ProxyReward数据集，无需大量标注数据或显著的手工努力。其次，ProxyReward信号提供了对特定问题的信息全面性和准确性进行针对性评估的方法。实验结果表明，我们的方法ProxyReward甚至超过了GPT-4-Turbo。在训练广泛使用的开源模型时，它可以显著提高Open-LTG任务的性能达20%，同时也超越了LLM作为评判者的途径。我们的工作展示了增强LLMs应对复杂开放性问题能力的有效方法。', 'title_zh': '从一般到针对性奖励：超越GPT-4的开放生成长上下文任务'}
{'arxiv_id': 'arXiv:2506.15961', 'title': 'TrainVerify: Equivalence-Based Verification for Distributed LLM Training', 'authors': 'Yunchi Lu, Youshan Miao, Cheng Tan, Peng Huang, Yi Zhu, Xian Zhang, Fan Yang', 'link': 'https://arxiv.org/abs/2506.15961', 'abstract': "Training large language models (LLMs) at scale requires parallel execution across thousands of devices, incurring enormous computational costs. Yet, these costly distributed trainings are rarely verified, leaving them prone to silent errors and potentially wasting millions of GPU hours. We introduce TrainVerify, a system for verifiable distributed training of LLMs. Given a deep learning model's logical specification as the ground truth, TrainVerify formally verifies that a distributed parallel execution plan is mathematically equivalent to it. Direct verification is notoriously difficult due to the sheer scale of LLMs which often involves billions of variables and highly intricate computation graphs. Therefore, TrainVerify introduces shape-reduction techniques and a stage-wise parallel verification algorithm that significantly reduces complexity while preserving formal correctness. TrainVerify scales to frontier LLMs, including the successful verification of the Llama3 (405B) and DeepSeek-V3 (671B) training plans.", 'abstract_zh': '大规模训练语言模型（LLMs）需要在数千台设备上并行执行，产生巨大的计算成本。然而，这些昂贵的分布式训练很少被验证，这使得它们容易出现无声错误，可能浪费数百万个GPU小时。我们引入了TrainVerify系统，用于可验证的LLMs分布式训练。Given一个深度学习模型的逻辑规范作为ground truth，TrainVerify正式验证分布式并行执行计划与之在数学上等价。直接验证由于LLMs的规模巨大，通常涉及数十亿个变量和高度复杂的计算图而极困难。因此，TrainVerify引入了形状缩减技术及阶段式并行验证算法，显著降低了复杂度同时保持形式上的正确性。TrainVerify能够扩展到前沿的LLMs，包括Llama3（405B）和DeepSeek-V3（671B）训练计划的成功验证。', 'title_zh': 'TrainVerify: 基于等价性的分布式LLM训练验证'}
{'arxiv_id': 'arXiv:2506.15894', 'title': 'Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning', 'authors': 'Sam Silver, Jimin Sun, Ivan Zhang, Sara Hooker, Eddie Kim', 'link': 'https://arxiv.org/abs/2506.15894', 'abstract': 'Large Language Models (LLMs) have demonstrated impressive mathematical reasoning capabilities, yet their performance remains brittle to minor variations in problem description and prompting strategy. Furthermore, reasoning is vulnerable to sampling-induced errors which autoregressive models must primarily address using self-correction via additionally-generated tokens. To better understand self-correction capabilities of recent models, we conduct experiments measuring models\' ability to self-correct synthetic perturbations introduced into their Chain of Thought (CoT) reasoning. We observe robust single-utterance intrinsic self-correction behavior across a range of open-weight models and datasets, ranging from subtle, implicit corrections to explicit acknowledgments and corrections of errors. Our findings suggest that LLMs, including those not finetuned for long CoT, may possess stronger intrinsic self-correction capabilities than commonly shown in the literature. The presence of this ability suggests that recent "reasoning" model work involves amplification of traits already meaningfully present in models.', 'abstract_zh': '大型语言模型在数学推理方面展现了令人印象深刻的能力，但其性能对问题描述和提示策略的轻微变化仍显得脆弱。此外，推理过程容易受到抽样引起的错误的影响，自回归模型必须通过自动生成的额外令牌进行自我纠正来主要解决这一问题。为了更好地了解 recent 模型的自我纠正能力，我们进行了实验，测量模型在其思维链（CoT）推理中引入合成扰动后的自我纠正能力。我们观察到，从微妙的隐式纠正到明确承认和纠正错误，不同开放权重模型和数据集均表现出稳健的单句内在自我纠正行为。我们的研究结果表明，包括未针对长思维链进行微调的大型语言模型在内，这些模型可能具有比文献中常见显示的更强的内在自我纠正能力。这种能力的存在表明，最近的“推理”模型工作可能是对模型中已显著存在的特质的放大。', 'title_zh': '语言模型可以对干扰推理进行单句自纠错'}
{'arxiv_id': 'arXiv:2506.15882', 'title': 'Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute', 'authors': 'Sheng Liu, Tianlang Chen, Pan Lu, Haotian Ye, Yizheng Chen, Lei Xing, James Zou', 'link': 'https://arxiv.org/abs/2506.15882', 'abstract': 'Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.', 'abstract_zh': '测试时计算已成为提升大型语言模型性能的强大范式，通过生成多个输出或细化个体推理链可以显著提高答案准确性。然而，现有方法如“最佳的N个结果”、多数投票和自我反思通常以统一的方式应用于输入，忽视了不同问题可能需要不同推理深度的事实。在此工作中，我们提出了分数推理（Fractional Reasoning），这是一种无需训练且模型无关的框架，能够在推理时连续控制推理强度，超越固定指令提示的局限性。该方法通过提取与更深层次推理相关的潜在控制向量，并用可调节的缩放因子重新应用，使模型能够根据每个输入的复杂性调整其推理过程。这种分数推理支持两种关键的测试时扩展模式：（1）在广度基于策略（如“最佳的N个结果”、多数投票）中提升输出质量，（2）在深度基于策略（如自我反思）中增强单个推理链的正确性。实验表明，分数推理在多种推理任务和模型上均能一致地提升性能。', 'title_zh': '通过潜在引导向量的分数推理改进推断时间计算'}
{'arxiv_id': 'arXiv:2506.15846', 'title': 'Finance Language Model Evaluation (FLaME)', 'authors': 'Glenn Matlin, Mika Okamoto, Huzaifa Pardawala, Yang Yang, Sudheer Chava', 'link': 'https://arxiv.org/abs/2506.15846', 'abstract': "Language Models (LMs) have demonstrated impressive capabilities with core Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly specialized knowledge-intensive tasks in finance remains difficult to assess due to major gaps in the methodologies of existing evaluation frameworks, which have caused an erroneous belief in a far lower bound of LMs' performance on common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for these FinNLP tasks, we present the first holistic benchmarking suite for Financial Language Model Evaluation (FLaME). We are the first research paper to comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source our framework software along with all data and results.", 'abstract_zh': '语言模型（LMs）在核心自然语言处理（NLP）任务中展示了令人印象深刻的性能。现有的评估框架方法学上的重大差距使得评估金融等专业知识密集型任务的语言模型效果困难，导致对其在常见金融NLP（FinNLP）任务中的表现存在错误的低限信念。为了展示语言模型在这些FinNLP任务上的潜力，我们介绍了首个金融语言模型评估集成套件（FLaME）。这是首篇全面研究语言模型与“推理强化”语言模型的学术论文，通过对23个基础语言模型在20项核心金融NLP任务上的实证研究来展示其潜力。我们开源了我们的框架软件及所有数据和结果。', 'title_zh': '金融语言模型评估（FLaME）'}
{'arxiv_id': 'arXiv:2506.15794', 'title': 'Veracity: An Open-Source AI Fact-Checking System', 'authors': 'Taylor Lynn Curtis, Maximilian Puelma Touzel, William Garneau, Manon Gruaz, Mike Pinder, Li Wei Wang, Sukanya Krishna, Luda Cohen, Jean-François Godbout, Reihaneh Rabbany, Kellin Pelrine', 'link': 'https://arxiv.org/abs/2506.15794', 'abstract': "The proliferation of misinformation poses a significant threat to society, exacerbated by the capabilities of generative AI. This demo paper introduces Veracity, an open-source AI system designed to empower individuals to combat misinformation through transparent and accessible fact-checking. Veracity leverages the synergy between Large Language Models (LLMs) and web retrieval agents to analyze user-submitted claims and provide grounded veracity assessments with intuitive explanations. Key features include multilingual support, numerical scoring of claim veracity, and an interactive interface inspired by familiar messaging applications. This paper will showcase Veracity's ability to not only detect misinformation but also explain its reasoning, fostering media literacy and promoting a more informed society.", 'abstract_zh': 'misinformation的泛滥对社会构成了显著威胁，生成式AI的能力进一步加剧了这一问题。本文介绍了Veracity，这是一个开源AI系统，旨在通过透明和易访问的事实核查来赋能个人对抗 misinformation。Veracity 利用大型语言模型（LLMs）与网页检索代理之间的协同作用，分析用户提交的断言，并提供基于直观解释的可靠性的评估。关键功能包括多语言支持、断言可靠性的数值评分以及借鉴熟悉的消息应用程序界面的互动界面。本文将展示Veracity不仅能够检测 misinformation，还能解释其推理过程，从而促进媒体素养并推动更加知情的社会。', 'title_zh': '真实性：一个开源AI事实核查系统'}
{'arxiv_id': 'arXiv:2506.15724', 'title': 'MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference', 'authors': 'Kunxi Li, Zhonghua Jiang, Zhouzhou Shen, Zhaode Wang, Chengfei Lv, Shengyu Zhang, Fan Wu, Fei Wu', 'link': 'https://arxiv.org/abs/2506.15724', 'abstract': 'This paper introduces MadaKV, a modality-adaptive key-value (KV) cache eviction strategy designed to enhance the efficiency of multimodal large language models (MLLMs) in long-context inference. In multimodal scenarios, attention heads exhibit varying preferences for different modalities, resulting in significant disparities in modality importance across attention heads. Traditional KV cache eviction methods, which are tailored for unimodal settings, fail to capture modality-specific information, thereby yielding suboptimal performance. MadaKV addresses these challenges through two key components: modality preference adaptation and hierarchical compression compensation. By dynamically sensing modality information within attention heads and adaptively retaining critical tokens, MadaKV achieves substantial reductions in KV cache memory footprint and model inference decoding latency (1.3 to 1.5 times improvement) while maintaining high accuracy across various multimodal long-context tasks. Extensive experiments on representative MLLMs and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to existing KV cache eviction methods.', 'abstract_zh': 'MadaKV：一种针对多模态大型语言模型长文境推理的模态自适应键值缓存淘汰策略', 'title_zh': 'MadaKV：自适应模态-感知KV缓存淘汰机制，实现高效的多模态长上下文推理'}
{'arxiv_id': 'arXiv:2506.15717', 'title': 'daDPO: Distribution-Aware DPO for Distilling Conversational Abilities', 'authors': 'Zhengze Zhang, Shiqi Wang, Yiqun Shen, Simin Guo, Dahua Lin, Xiaoliang Wang, Nguyen Cam-Tu, Fei Tan', 'link': 'https://arxiv.org/abs/2506.15717', 'abstract': "Large language models (LLMs) have demonstrated exceptional performance across various applications, but their conversational abilities decline sharply as model size decreases, presenting a barrier to their deployment in resource-constrained environments. Knowledge distillation with Direct Preference Optimization (dDPO) has emerged as a promising approach to enhancing the conversational abilities of smaller models using a larger teacher model. However, current methods primarily focus on 'black-box' KD, which only uses the teacher's responses, overlooking the output distribution offered by the teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware DPO), a unified method for preference optimization and distribution-based distillation. We provide rigorous theoretical analysis and empirical validation, showing that daDPO outperforms existing methods in restoring performance for pruned models and enhancing smaller LLM models. Notably, in in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve near-teacher performance (-7.3% preference rate compared to that of dDPO's -31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model (14.0% win rate).", 'abstract_zh': '大规模语言模型(LLMs)在各种应用中展现了卓越的性能，但随着模型规模的减小，其对话能力急剧下降，这成为其在资源受限环境中部署的障碍。直接偏好优化(Direct Preference Optimization, dDPO)指导的知识蒸馏方法为通过较大教师模型增强较小模型的对话能力提供了有前景的途径。然而，现有的方法主要关注“黑盒”知识蒸馏，只利用教师的响应，而忽略了教师提供的输出分布。本文通过引入daDPO（分布感知dDPO）统一方法来解决这一问题，该方法将偏好优化和基于分布的蒸馏结合起来。我们进行了严格的理论分析和实证验证，表明daDPO在恢复剪枝模型性能和增强较小的LLM模型方面优于现有方法。特别是在领域内评价中，我们的方法使剪枝比例为20%的Vicuna1.5-7B达到接近教师模型的性能（偏好率为-7.3%，而dDPO为-31%），并使Qwen2.5-1.5B偶尔超越其7B教师模型（胜率为14.0%）。', 'title_zh': 'daDPO: 分布感知的DPO对话能力压缩方法'}
{'arxiv_id': 'arXiv:2506.15712', 'title': 'BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling', 'authors': 'Songqi Zhou, Ruixue Liu, Yixing Wang, Jia Lu, Benben Jiang', 'link': 'https://arxiv.org/abs/2506.15712', 'abstract': 'Accurate fault detection in lithium-ion batteries is essential for the safe and reliable operation of electric vehicles and energy storage systems. However, existing methods often struggle to capture complex temporal dependencies and cannot fully leverage abundant unlabeled data. Although large language models (LLMs) exhibit strong representation capabilities, their architectures are not directly suited to the numerical time-series data common in industrial settings. To address these challenges, we propose a novel framework that adapts BERT-style pretraining for battery fault detection by extending the standard BERT architecture with a customized time-series-to-token representation module and a point-level Masked Signal Modeling (point-MSM) pretraining task tailored to battery applications. This approach enables self-supervised learning on sequential current, voltage, and other charge-discharge cycle data, yielding distributionally robust, context-aware temporal embeddings. We then concatenate these embeddings with battery metadata and feed them into a downstream classifier for accurate fault classification. Experimental results on a large-scale real-world dataset show that models initialized with our pretrained parameters significantly improve both representation quality and classification accuracy, achieving an AUROC of 0.945 and substantially outperforming existing approaches. These findings validate the effectiveness of BERT-style pretraining for time-series fault detection.', 'abstract_zh': '锂离子电池准确故障检测对于电动汽车和储能系统的安全可靠运行至关重要。然而，现有的方法往往难以捕捉复杂的时序依赖关系，无法充分利用丰富的未标注数据。尽管大规模语言模型（LLMs）表现出强大的表示能力，但其架构并不直接适合工业场景中常见的数值时间序列数据。为了解决这些挑战，我们提出了一种新颖的框架，通过将BERT风格的预训练适应于电池故障检测，并扩展标准的BERT架构，加入一个定制的时间序列到标记表示模块以及一个针对电池应用量身定制的点级掩码信号建模（point-MSM）预训练任务。该方法使模型能对连续的电流、电压和其他充放电循环数据进行半监督学习，生成分布鲁棒、上下文感知的时序嵌入。然后，将这些嵌入与电池元数据拼接，并输入下游分类器进行准确的故障分类。在大规模真实世界数据集上的实验结果表明，使用我们预训练参数初始化的模型显著提高了表示质量和分类准确性，AUROC达到0.945，明显优于现有方法。这些发现验证了BERT风格预训练在时间序列故障检测中的有效性。', 'title_zh': '基于点掩蔽信号建模的BatteryBERT：用于真实电池故障检测的模型'}
{'arxiv_id': 'arXiv:2506.15710', 'title': 'RAST: Reasoning Activation in LLMs via Small-model Transfer', 'authors': 'Siru Ouyang, Xinyu Zhu, Zilin Xiao, Minhao Jiang, Yu Meng, Jiawei Han', 'link': 'https://arxiv.org/abs/2506.15710', 'abstract': "Reinforcement learning (RL) has become a powerful approach for improving the reasoning capabilities of large language models (LLMs), as evidenced by recent successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale remains intimidatingly resource-intensive, requiring multiple model copies and extensive GPU workloads. On the other hand, while being powerful, recent studies suggest that RL does not fundamentally endow models with new knowledge; rather, it primarily reshapes the model's output distribution to activate reasoning capabilities latent in the base model. Building on this insight, we hypothesize that the changes in output probabilities induced by RL are largely model-size invariant, opening the door to a more efficient paradigm: training a small model with RL and transferring its induced probability shifts to larger base models. To verify our hypothesis, we conduct a token-level analysis of decoding trajectories and find high alignment in RL-induced output distributions across model scales, validating our hypothesis. Motivated by this, we propose RAST, a simple yet effective method that transfers reasoning behaviors by injecting RL-induced probability adjustments from a small RL-trained model into larger models. Experiments across multiple mathematical reasoning benchmarks show that RAST substantially and consistently enhances the reasoning capabilities of base models while requiring significantly lower GPU memory than direct RL training, sometimes even yielding better performance than the RL-trained counterparts. Our findings offer new insights into the nature of RL-driven reasoning and practical strategies for scaling its benefits without incurring its full computational cost. The project page of RAST is available at this https URL.", 'abstract_zh': '强化学习（RL）已成为提升大型语言模型（LLMs）推理能力的强大方法，如OpenAI的o1和Deepseek-R1的成功所示。然而，大规模应用RL仍然是资源密集型的，需要多个模型副本和大量的GPU工作负载。尽管如此，近期研究表明，RL本质上并未赋予模型新的知识；相反，它主要通过重塑模型的输出分布来激活基模型中存在的推理能力。基于这一见解，我们假设由RL引起的输出概率变化在不同规模的模型中是大模型尺寸不变的，从而开启了更有效的范式：用小型模型进行RL训练，并将由此引发的概率调整传递给更大的基模型。为了验证这一假设，我们进行了逐令牌解码轨迹分析，发现不同规模模型中RL引起的输出分布具有高度一致性，验证了我们的假设。受此驱动，我们提出了一种简单而有效的方法RAST，该方法通过向更大模型注入小型RL训练模型引起的概率调整来转移推理行为。在多个数学推理基准上的实验表明，与直接的RL训练相比，RAST不仅显著且一致地提升了基模型的推理能力，而且所需GPU内存更低，有时甚至比直接RL训练的模型表现更好。我们的发现为RL驱动的推理的本质提供了新的见解，并提供了在不承担其全部计算成本的情况下扩展其益处的实用策略。RAST的项目页面可访问此链接。', 'title_zh': 'RAST：通过小型模型迁移实现LLMs的推理激活'}
{'arxiv_id': 'arXiv:2506.15707', 'title': 'Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling', 'authors': 'Xinglin Wang, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li', 'link': 'https://arxiv.org/abs/2506.15707', 'abstract': "Test-Time Scaling (TTS) improves the performance of Large Language Models (LLMs) by using additional inference-time computation to explore multiple reasoning paths through search. Yet how to allocate a fixed rollout budget most effectively during search remains underexplored, often resulting in inefficient use of compute at test time. To bridge this gap, we formulate test-time search as a resource allocation problem and derive the optimal allocation strategy that maximizes the probability of obtaining a correct solution under a fixed rollout budget. Within this formulation, we reveal a core limitation of existing search methods: solution-level allocation tends to favor reasoning directions with more candidates, leading to theoretically suboptimal and inefficient use of compute. To address this, we propose Direction-Oriented Resource Allocation (DORA), a provably optimal method that mitigates this bias by decoupling direction quality from candidate count and allocating resources at the direction level. To demonstrate DORA's effectiveness, we conduct extensive experiments on challenging mathematical reasoning benchmarks including MATH500, AIME2024, and AIME2025. The empirical results show that DORA consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art accuracy. We hope our findings contribute to a broader understanding of optimal TTS for LLMs.", 'abstract_zh': 'Test-Time Scaling (TTS) 在固定展开预算下的最优资源分配策略研究：方向导向性资源分配 (DORA) 改进大型语言模型 (LLM) 性能的方法', 'title_zh': '每次展开都重要：高效的测试时扩展的最优资源分配'}
{'arxiv_id': 'arXiv:2506.15706', 'title': 'MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning', 'authors': 'Yunze Lin', 'link': 'https://arxiv.org/abs/2506.15706', 'abstract': "Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) as it requires ensuring the correctness of each reasoning step. Researchers have been strengthening the mathematical reasoning abilities of LLMs through supervised fine-tuning, but due to the inability to suppress incorrect outputs, illusions can easily arise. Recently, Direct Preference Optimization (DPO) has been widely adopted for aligning human intent by using preference data to prevent LLMs from generating incorrect outputs. However, it has shown limited benefits in long-chain mathematical reasoning, mainly because DPO struggles to effectively capture the differences between accepted and rejected answers from preferences in long-chain data. The inconsistency between DPO training and LLMs' generation metrics also affects the effectiveness of suppressing incorrect outputs. We propose the Multi-Granularity Direct Preference Optimization (MDPO) method, optimizing the mathematical reasoning of LLMs at three granularities: Solution2Solution, Inference2Inference, and Step2Step. Solution2Solution focuses on the correctness of entire long-chain reasoning; Inference2Inference concentrates on logical reasoning between steps; Step2Step corrects computational errors in steps, enhancing the computational capabilities of LLMs. Additionally, we unify the training objectives of the three granularities to align with the generation metrics. We conducted experiments on the open-source models Qwen2 and Llama3, achieving improvements of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset, outperforming DPO and other DPO variant methods. Furthermore, we also provide a pipeline for constructing MDPO training data that is simple and does not require manual annotation costs.", 'abstract_zh': '多粒度直接偏好优化方法提升大型语言模型的数学推理能力', 'title_zh': 'MDPO: 多粒度直接偏好优化的数学推理方法'}
{'arxiv_id': 'arXiv:2506.15704', 'title': 'Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding', 'authors': 'Feiyu Yao, Qian Wang', 'link': 'https://arxiv.org/abs/2506.15704', 'abstract': 'As large language models (LLMs) continue to support increasingly longer contexts, the memory demand for key-value (KV) caches during decoding grows rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe bandwidth. Sparse attention mechanisms alleviate this issue by computing attention weights only for selected key-value pairs. However, their indexing computation typically requires traversing all key vectors, resulting in significant computational and data transfer overhead. To reduce the cost of index retrieval, existing methods often treat each decoding step as an independent process, failing to exploit the temporal correlations embedded in historical decoding information. To this end, we propose LFPS(Learn From the Past for Sparse Indexing), an acceleration method that dynamically constructs sparse indexing candidates based on historical attention patterns. LFPS captures two prevalent trends in decoder attention -vertical patterns (attending to fixed positions) and slash patterns (attending to relative positions) -and incorporates a positional expansion strategy to effectively predict the Top-k indices for the current step. We validate LFPS on challenging long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as the base model. Experimental results show that LFPS achieves up to 22.8$\\times$ speedup over full attention and 9.6$\\times$ speedup over exact Top-k retrieval on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively, while preserving generation accuracy. These results demonstrate that LFPS offers a practical and efficient solution for decoding optimization in long-context LLM inference.', 'abstract_zh': '从过去中学习以加速稀疏索引：长上下文语言模型解码加速方法', 'title_zh': '借鉴past经验：大规模语言模型解码的快速稀疏索引方法'}
{'arxiv_id': 'arXiv:2506.15702', 'title': 'Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation', 'authors': 'Peter Belcak, Greg Heinrich, Jan Kautz, Pavlo Molchanov', 'link': 'https://arxiv.org/abs/2506.15702', 'abstract': 'Finetuning language models for a new domain inevitably leads to the deterioration of their general performance. This becomes more pronounced the more limited the finetuning data resource.\nWe introduce minifinetuning (MFT), a method for language model domain adaptation that considerably reduces the effects of overfitting-induced degeneralization in low-data settings and which does so in the absence of any pre-training data for replay. MFT demonstrates 2-10x more favourable specialization-to-degeneralization ratios than standard finetuning across a wide range of models and domains and exhibits an intrinsic robustness to overfitting when data in the new domain is scarce and down to as little as 500 samples.\nEmploying corrective self-distillation that is individualized on the sample level, MFT outperforms parameter-efficient finetuning methods, demonstrates replay-like degeneralization mitigation properties, and is composable with either for a combined effect.', 'abstract_zh': '基于少量数据的语言模型领域适应方法：miniFineTuning', 'title_zh': '迷你微调：基于纠正性自我蒸馏的低数据生成领域适应'}
{'arxiv_id': 'arXiv:2506.15701', 'title': 'Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning', 'authors': 'Haolin Pan, Hongyu Lin, Haoran Luo, Yang Liu, Kaichun Yao, Libo Zhang, Mingjie Xing, Yanjun Wu', 'link': 'https://arxiv.org/abs/2506.15701', 'abstract': 'Compiler auto-tuning optimizes pass sequences to improve performance metrics such as Intermediate Representation (IR) instruction count. Although recent advances leveraging Large Language Models (LLMs) have shown promise in automating compiler tuning, two significant challenges still remain: the absence of high-quality reasoning datasets for agents training, and limited effective interactions with the compilation environment. In this work, we introduce Compiler-R1, the first reinforcement learning (RL)-driven framework specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1 features a curated, high-quality reasoning dataset and a novel two-stage end-to-end RL training pipeline, enabling efficient environment exploration and learning through an outcome-based reward. Extensive experiments across seven datasets demonstrate Compiler-R1 achieving an average 8.46% IR instruction count reduction compared to opt -Oz, showcasing the strong potential of RL-trained LLMs for compiler optimization. Our code and datasets are publicly available at this https URL.', 'abstract_zh': 'Compiler-R1：增强大语言模型编译器自动调优的强化学习框架', 'title_zh': 'Compiler-R1：面向基于强化学习的有能动性的编译器自动调优'}
{'arxiv_id': 'arXiv:2506.15699', 'title': 'BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap', 'authors': 'Shengyuan Hu, Neil Kale, Pratiksha Thaker, Yiwei Fu, Steven Wu, Virginia Smith', 'link': 'https://arxiv.org/abs/2506.15699', 'abstract': 'Machine unlearning has the potential to improve the safety of large language models (LLMs) by removing sensitive or harmful information post hoc. A key challenge in unlearning involves balancing between forget quality (effectively unlearning undesirable information) and retain quality (maintaining good performance on other, general tasks). Unfortunately, as we show, current LLM unlearning benchmarks contain highly disparate forget and retain sets -- painting a false picture of the effectiveness of LLM unlearning methods. This can be particularly problematic because it opens the door for benign perturbations, such as relearning attacks, to easily reveal supposedly unlearned knowledge once models are deployed. To address this, we present $\\texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic scenarios of forget-retain overlap. $\\texttt{BLUR}$ significantly expands on existing unlearning benchmarks by providing extended evaluation tasks, combined forget/retain queries, and relearning datasets of varying degrees of difficulty. Despite the benign nature of the queries considered, we find that the performance of existing methods drops significantly when evaluated on $\\texttt{BLUR}$, with simple approaches performing better on average than more recent methods. These results highlight the importance of robust evaluation and suggest several important directions of future study. Our benchmark is publicly available at: this https URL', 'abstract_zh': '机器未学习具有潜力通过后剔除敏感或有害信息来改进大型语言模型的安全性。未学习的关键挑战在于在剔除质量（有效地剔除不良信息）和保留质量（在其他通用任务中保持良好性能）之间进行平衡。不幸的是，如我们所展示的，当前的未学习大型语言模型基准包含高度不同的剔除和保留集合——这描绘了一幅未学习方法效果的虚假图景。这可能会特别是因为这使无害的干扰，如重新学习攻击，在模型部署后轻易揭示出被认为已剔除的知识。为了解决这一问题，我们提出了$\\texttt{BLUR}$：一种用于大型语言模型未学习的标准，提供了更具现实性的剔除-保留重叠场景。$\\texttt{BLUR}$大幅扩展了现有的未学习基准，提供了额外的评估任务、组合剔除/保留查询以及不同难度级别的重新学习数据集。尽管所考虑的查询具有无害的性质，我们发现现有方法在$\\texttt{BLUR}$上的表现显著下降，简单的做法在平均性能上优于更近期的方法。这些结果强调了稳健评估的重要性，并建议了未来研究的重要方向。我们的基准已公开可获取：this https URL。', 'title_zh': 'BLUR：一种适用于重叠忘记保留情况下的LLM去学习基准'}
{'arxiv_id': 'arXiv:2506.15690', 'title': 'LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs', 'authors': 'Tianyu Wang, Lingyou Pang, Akira Horiguchi, Carey E. Priebe', 'link': 'https://arxiv.org/abs/2506.15690', 'abstract': 'The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.', 'abstract_zh': '公共互联网合成数据在大型语言模型训练中提高了数据使用效率，但模型坍缩的潜在威胁仍未充分探索。现有研究主要在单模型设置中研究模型坍缩或仅依赖统计替代方法。在此项工作中，我们引入了大型语言模型网络动态（LWD）框架，用于在网络层面研究模型坍缩。通过使用检索增强生成（RAG）数据库模拟互联网，我们分析了模型输出的收敛模式。此外，我们通过将此收敛机制与交互的高斯混合模型类比，提供了理论上的保证。', 'title_zh': 'LLM网络中的模型崩溃追踪：探究LLM网络中的模型崩溃现象'}
{'arxiv_id': 'arXiv:2506.15689', 'title': 'BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models', 'authors': 'Liulu He, Shenli Zhen, Karwei Sun, Yijiang Liu, Yufei Zhao, Chongkang Tan, Huanrui Yang, Yuan Du, Li Du', 'link': 'https://arxiv.org/abs/2506.15689', 'abstract': 'Rotations have become essential to state-of-the-art quantization pipelines for large language models (LLMs) by effectively smoothing outliers in weights and activations. However, further optimizing the rotation parameters offers only limited performance gains and introduces significant training overhead: due to rotation parameter sharing, full-model must be loaded simultaneously to enable backpropagation, resulting in substantial memory consumption and limited practical utility. In this work, we identify two fundamental limitations of current rotational quantization methods: (i) rotation fails to align channel means, resulting in wider quantization bounds and increased rounding errors; and (ii) rotation makes the activation distribution more Gaussian-like, increasing energy loss caused by clipping errors. To address these issues, we introduce \\textbf{BASE-Q}, a simple yet powerful approach that combines bias correction and asymmetric scaling to effectively reduce rounding and clipping errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the need for memory-intensive full-model backpropagation. Extensive experiments on various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing the accuracy gap to full-precision models by 50.5\\%, 42.9\\%, and 29.2\\% compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be released soon.', 'abstract_zh': 'BASE-Q：一种结合偏置校正和非对称缩放的有效减少舍入和截断误差的方法', 'title_zh': 'BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models'}
