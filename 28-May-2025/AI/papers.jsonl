{'arxiv_id': 'arXiv:2505.21486', 'title': 'Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming', 'authors': 'Yang Yang, Jiemin Wu, Yutao Yue', 'link': 'https://arxiv.org/abs/2505.21486', 'abstract': "Automating robust hypothesis generation in open environments is pivotal for AI cognition. We introduce a novel framework integrating a multi-agent system, powered by Large Language Models (LLMs), with Inductive Logic Programming (ILP). Our system's LLM agents autonomously define a structured symbolic vocabulary (predicates) and relational templates , i.e., \\emph{language bias} directly from raw textual data. This automated symbolic grounding (the construction of the language bias), traditionally an expert-driven bottleneck for ILP, then guides the transformation of text into facts for an ILP solver, which inductively learns interpretable rules. This approach overcomes traditional ILP's reliance on predefined symbolic structures and the noise-sensitivity of pure LLM methods. Extensive experiments in diverse, challenging scenarios validate superior performance, paving a new path for automated, explainable, and verifiable hypothesis generation.", 'abstract_zh': '在开放环境中自动化稳健假设生成对于AI认知至关重要。我们提出了一种结合大型语言模型（LLMs）和归归纳逻辑编程（ILP）的新型多代理系统框架。系统中的LLM代理自主定义结构化的符号词汇（谓词）和关系模板，即语言偏置，直接从原始文本数据中生成。这种自动符号接地（语言偏置的构建）传统上是ILP中的专家驱动瓶颈，然后指导文本向事实的转换，供ILP求解器使用，该求解器通过归纳学习可解释的规则。这种方法克服了传统ILP对预定义符号结构的依赖以及纯LLM方法的噪声敏感性。在各种挑战性场景中的广泛实验验证了其优越性能，为自动、可解释和可验证的假设生成开辟了新途径。', 'title_zh': '稳健的假设生成：LLM驱动的语言偏置在归纳逻辑编程中的应用'}
{'arxiv_id': 'arXiv:2505.21427', 'title': 'Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning', 'authors': 'Xianling Mu, Joseph Ternasky, Fuat Alican, Yigit Ihlamur', 'link': 'https://arxiv.org/abs/2505.21427', 'abstract': 'Early-stage startup investment is a high-risk endeavor characterized by scarce data and uncertain outcomes. Traditional machine learning approaches often require large, labeled datasets and extensive fine-tuning, yet remain opaque and difficult for domain experts to interpret or improve. In this paper, we propose a transparent and data-efficient investment decision framework powered by memory-augmented large language models (LLMs) using in-context learning (ICL). Central to our method is a natural language policy embedded directly into the LLM prompt, enabling the model to apply explicit reasoning patterns and allowing human experts to easily interpret, audit, and iteratively refine the logic. We introduce a lightweight training process that combines few-shot learning with an in-context learning loop, enabling the LLM to update its decision policy iteratively based on structured feedback. With only minimal supervision and no gradient-based optimization, our system predicts startup success far more accurately than existing benchmarks. It is over 20x more precise than random chance, which succeeds 1.9% of the time. It is also 7.1x more precise than the typical 5.6% success rate of top-tier venture capital (VC) firms.', 'abstract_zh': '早期创业公司投资是一种以稀缺数据和不确定结果为特点的高风险行为。传统的机器学习方法通常需要大量标注的数据集和广泛的微调，且往往难以解释和改进。在本文中，我们提出了一种基于增强记忆的大语言模型（LLM）和上下文学习（ICL）的透明且数据高效的投资决策框架。该方法的关键在于直接嵌入到LLM提示中的自然语言策略，使模型能够应用明确的推理模式，并允许人类专家轻松解释、审计和迭代改进逻辑。我们介绍了一种轻量级的训练过程，结合了少数样本学习和上下文学习循环，使LLM能够基于结构化反馈迭代更新其决策策略。在仅有少量监督且不使用梯度优化的情况下，我们的系统比现有基准更准确地预测了创业公司的成功。其准确率超过随机猜测的20多倍，随机猜测的成功率为1.9%，也远高于顶级风险投资（VC）公司通常5.6%的成功率。', 'title_zh': '基于策略诱导：通过可解释的增强记忆上下文学习预测初创企业成功'}
{'arxiv_id': 'arXiv:2505.21426', 'title': 'Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks', 'authors': 'Francesco Cozzi, Marco Pangallo, Alan Perotti, André Panisson, Corrado Monti', 'link': 'https://arxiv.org/abs/2505.21426', 'abstract': "Agent-Based Models (ABMs) are powerful tools for studying emergent properties in complex systems. In ABMs, agent behaviors are governed by local interactions and stochastic rules. However, these rules are, in general, non-differentiable, limiting the use of gradient-based methods for optimization, and thus integration with real-world data. We propose a novel framework to learn a differentiable surrogate of any ABM by observing its generated data. Our method combines diffusion models to capture behavioral stochasticity and graph neural networks to model agent interactions. Distinct from prior surrogate approaches, our method introduces a fundamental shift: rather than approximating system-level outputs, it models individual agent behavior directly, preserving the decentralized, bottom-up dynamics that define ABMs. We validate our approach on two ABMs (Schelling's segregation model and a Predator-Prey ecosystem) showing that it replicates individual-level patterns and accurately forecasts emergent dynamics beyond training. Our results demonstrate the potential of combining diffusion models and graph learning for data-driven ABM simulation.", 'abstract_zh': '基于代理的模型的可微代理行为学习框架', 'title_zh': '基于图扩散网络学习个体行为的agents-based模型研究'}
{'arxiv_id': 'arXiv:2505.21419', 'title': 'Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs', 'authors': 'Yifan Wang, Kenneth P. Birman', 'link': 'https://arxiv.org/abs/2505.21419', 'abstract': "Today's cloud-hosted applications and services are complex systems, and a performance or functional instability can have dozens or hundreds of potential root causes. Our hypothesis is that by combining the pattern matching capabilities of modern AI tools with a natural multi-modal RAG LLM interface, problem identification and resolution can be simplified. ARCA is a new multi-modal RAG LLM system that targets this domain. Step-wise evaluations show that ARCA outperforms state-of-the-art alternatives.", 'abstract_zh': '今天托管在云上的应用程序和服务是复杂的系统，性能或功能不稳定可能有数十甚至数百个潜在的根本原因。我们假设通过结合现代AI工具的模式匹配能力以及自然多模态RAG语言模型接口，问题的识别和解决可以得到简化。ARCA是一种针对这一领域的新型多模态RAG语言模型系统。逐步评估表明，ARCA优于现有最先进的替代方案。', 'title_zh': '使用多模态RAG大语言模型诊断和解决云平台不稳定问题'}
{'arxiv_id': 'arXiv:2505.21410', 'title': 'MRSD: Multi-Resolution Skill Discovery for HRL Agents', 'authors': 'Shashank Sharma, Janina Hoffmann, Vinay Namboodiri', 'link': 'https://arxiv.org/abs/2505.21410', 'abstract': 'Hierarchical reinforcement learning (HRL) relies on abstract skills to solve long-horizon tasks efficiently. While existing skill discovery methods learns these skills automatically, they are limited to a single skill per task. In contrast, humans learn and use both fine-grained and coarse motor skills simultaneously. Inspired by human motor control, we propose Multi-Resolution Skill Discovery (MRSD), an HRL framework that learns multiple skill encoders at different temporal resolutions in parallel. A high-level manager dynamically selects among these skills, enabling adaptive control strategies over time. We evaluate MRSD on tasks from the DeepMind Control Suite and show that it outperforms prior state-of-the-art skill discovery and HRL methods, achieving faster convergence and higher final performance. Our findings highlight the benefits of integrating multi-resolution skills in HRL, paving the way for more versatile and efficient agents.', 'abstract_zh': '多层次强化学习中的多分辨率技能发现（Mult-Resolution Skill Discovery for Hierarchical Reinforcement Learning）', 'title_zh': '多分辨率技能发现for HRL代理'}
{'arxiv_id': 'arXiv:2505.21398', 'title': 'A Structured Unplugged Approach for Foundational AI Literacy in Primary Education', 'authors': 'Maria Cristina Carrisi, Mirko Marras, Sara Vergallo', 'link': 'https://arxiv.org/abs/2505.21398', 'abstract': 'Younger generations are growing up in a world increasingly shaped by intelligent technologies, making early AI literacy crucial for developing the skills to critically understand and navigate them. However, education in this field often emphasizes tool-based learning, prioritizing usage over understanding the underlying concepts. This lack of knowledge leaves non-experts, especially children, prone to misconceptions, unrealistic expectations, and difficulties in recognizing biases and stereotypes. In this paper, we propose a structured and replicable teaching approach that fosters foundational AI literacy in primary students, by building upon core mathematical elements closely connected to and of interest in primary curricula, to strengthen conceptualization, data representation, classification reasoning, and evaluation of AI. To assess the effectiveness of our approach, we conducted an empirical study with thirty-one fifth-grade students across two classes, evaluating their progress through a post-test and a satisfaction survey. Our results indicate improvements in terminology understanding and usage, features description, logical reasoning, and evaluative skills, with students showing a deeper comprehension of decision-making processes and their limitations. Moreover, the approach proved engaging, with students particularly enjoying activities that linked AI concepts to real-world reasoning. Materials: this https URL.', 'abstract_zh': 'younger 一代正在一个日益被智能技术塑造的世界中成长，因此早期人工智能素养对于培养批判性理解和导航这些技术的能力至关重要。然而，该领域的教育往往强调基于工具的学习，优先考虑使用而忽视对基本概念的理解。这种缺乏知识使得非专家，特别是儿童，容易产生误解、不切实际的期望，并且难以识别偏见和刻板印象。在本文中，我们提出了一种结构化且可复制的 teaching 方法，旨在通过结合与小学课程核心要素紧密相关且感兴趣的数学元素，促进小学生的人工智能基础素养，加强概念理解、数据表示、分类推理和人工智能评估能力。为了评估该方法的有效性，我们在两个班级的三十一名五年级学生中进行了实证研究，通过后测和满意度调查评估其进步。研究结果表明，在术语理解与使用、特征描述、逻辑推理和评价技能方面有所改善，学生对决策过程及其局限性的理解更加深入。此外，该方法证明是吸引人的，学生们特别喜欢将人工智能概念与现实世界推理相联系的活动。', 'title_zh': '面向基础人工智能素养的小学教育结构性课外 approach'}
{'arxiv_id': 'arXiv:2505.21344', 'title': 'The Multilingual Divide and Its Impact on Global AI Safety', 'authors': 'Aidan Peppin, Julia Kreutzer, Alice Schoenauer Sebag, Kelly Marchisio, Beyza Ermis, John Dang, Samuel Cahyawijaya, Shivalika Singh, Seraphina Goldfarb-Tarrant, Viraat Aryabumi, Aakanksha, Wei-Yin Ko, Ahmet Üstün, Matthias Gallé, Marzieh Fadaee, Sara Hooker', 'link': 'https://arxiv.org/abs/2505.21344', 'abstract': 'Despite advances in large language model capabilities in recent years, a large gap remains in their capabilities and safety performance for many languages beyond a relatively small handful of globally dominant languages. This paper provides researchers, policymakers and governance experts with an overview of key challenges to bridging the "language gap" in AI and minimizing safety risks across languages. We provide an analysis of why the language gap in AI exists and grows, and how it creates disparities in global AI safety. We identify barriers to address these challenges, and recommend how those working in policy and governance can help address safety concerns associated with the language gap by supporting multilingual dataset creation, transparency, and research.', 'abstract_zh': '尽管近年来大型语言模型的能力取得了进展，但在许多超出少数全球主导语言之外的其他语言上，它们的能力和安全性能之间仍存在较大的差距。本文为研究人员、政策制定者和治理专家提供了缩小“语言缺口”在AI中的差距并跨语言减少安全风险的概览。我们分析了AI语言缺口存在的原因及其如何导致全球AI安全的不平等，并识别出应对这些挑战的障碍，建议通过支持多语言数据集创建、透明度和研究来协助政策和治理领域的工作，以解决与语言缺口相关的安全关切。', 'title_zh': '多语言鸿沟及其对全球AI安全的影响'}
{'arxiv_id': 'arXiv:2505.21327', 'title': 'MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs', 'authors': 'Jiakang Yuan, Tianshuo Peng, Yilei Jiang, Yiting Lu, Renrui Zhang, Kaituo Feng, Chaoyou Fu, Tao Chen, Lei Bai, Bo Zhang, Xiangyu Yue', 'link': 'https://arxiv.org/abs/2505.21327', 'abstract': "Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities.", 'abstract_zh': '逻辑推理是人类智能的基础方面，也是多模态大规模语言模型（MLLMs）的一项essential能力。尽管在多模态推理方面取得了显著进展，但现有的基准测试未能全面评估其推理能力，因为缺乏逻辑推理类型的明确定类和推理的理解不清晰。为解决这些问题，我们引入了MME-Reasoning，这是一个全面的基准测试，旨在评估MLLMs的推理能力，其问题涵盖了推理的三种类型（即归纳、演绎和溯因）。我们精心策划了数据，以确保每个问题都能有效地评估推理能力而非感知技能或知识广度，并将评估协议扩展以涵盖多样问题的评估。我们的评估揭示了最先进的MLLMs在整体评估逻辑推理能力时存在的显著局限性。即使是最先进的MLLMs在全面的逻辑推理方面也表现出有限的性能，不同推理类型的性能存在显著差异。此外，我们还深入分析了诸如“思维模式”和基于规则的强化学习等方法，这些方法常被认为能提高推理能力。这些发现突显了当前MLLMs在多样逻辑推理场景下的关键局限性和性能差异，为推理能力的理解和评估提供了全面和系统性的见解。', 'title_zh': 'MME-推理：MLLMs中逻辑推理的全面基准'}
{'arxiv_id': 'arXiv:2505.21322', 'title': 'Assured Autonomy with Neuro-Symbolic Perception', 'authors': 'R. Spencer Hallyburton, Miroslav Pajic', 'link': 'https://arxiv.org/abs/2505.21322', 'abstract': "Many state-of-the-art AI models deployed in cyber-physical systems (CPS), while highly accurate, are simply pattern-matchers.~With limited security guarantees, there are concerns for their reliability in safety-critical and contested domains. To advance assured AI, we advocate for a paradigm shift that imbues data-driven perception models with symbolic structure, inspired by a human's ability to reason over low-level features and high-level context. We propose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how joint object detection and scene graph generation (SGG) yields deep scene understanding.~Powered by foundation models for offline knowledge extraction and specialized SGG algorithms for real-time deployment, we design a framework leveraging structured relational graphs that ensures the integrity of situational awareness in autonomy. Using physics-based simulators and real-world datasets, we demonstrate how SGG bridges the gap between low-level sensor perception and high-level reasoning, establishing a foundation for resilient, context-aware AI and advancing trusted autonomy in CPS.", 'abstract_zh': '先进的AI模型在 cyber-物理系统(CPS)中的应用虽高度准确，但本质上只是模式匹配器。受限于有限的安全保障，其在安全关键和对抗领域中的可靠性存在担忧。为推进值得信赖的AI，我们提倡一种范式转变，即在数据驱动的感知模型中融入符号结构，借鉴人类在低级特征和高级语境上的推理能力。我们提出了一种神经符号感知范式（NeuSPaPer），并通过结合物体检测和场景图生成（SGG）实现深层次场景理解。借助基础模型进行离线知识提取和专门的SGG算法实现实时部署，我们设计了一种框架，利用结构化关系图确保自主系统情境意识的完整性。利用物理仿真器和真实世界数据集，我们展示了SGG如何弥合传感器低级感知与高级推理之间的差距，为 resilient、情境感知的AI奠定基础，并推进CPS中的可信自主性。', 'title_zh': '确保自主性的神经符号感知'}
{'arxiv_id': 'arXiv:2505.21318', 'title': "Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular Chemical Operations", 'authors': 'Hao Li, He Cao, Bin Feng, Yanjun Shao, Xiangru Tang, Zhiyuan Yan, Li Yuan, Yonghong Tian, Yu Li', 'link': 'https://arxiv.org/abs/2505.21318', 'abstract': 'While large language models (LLMs) with Chain-of-Thought (CoT) reasoning excel in mathematics and coding, their potential for systematic reasoning in chemistry, a domain demanding rigorous structural analysis for real-world tasks like drug design and reaction engineering, remains untapped. Current benchmarks focus on simple knowledge retrieval, neglecting step-by-step reasoning required for complex tasks such as molecular optimization and reaction prediction. To address this, we introduce ChemCoTBench, a reasoning framework that bridges molecular structure understanding with arithmetic-inspired operations, including addition, deletion, and substitution, to formalize chemical problem-solving into transparent, step-by-step workflows. By treating molecular transformations as modular "chemical operations", the framework enables slow-thinking reasoning, mirroring the logic of mathematical proofs while grounding solutions in real-world chemical constraints. We evaluate models on two high-impact tasks: Molecular Property Optimization and Chemical Reaction Prediction. These tasks mirror real-world challenges while providing structured evaluability. By providing annotated datasets, a reasoning taxonomy, and baseline evaluations, ChemCoTBench bridges the gap between abstract reasoning methods and practical chemical discovery, establishing a foundation for advancing LLMs as tools for AI-driven scientific innovation.', 'abstract_zh': 'ChemCoTBench：将分子结构理解与算术启发式操作结合的推理框架，以系统化解决化学问题', 'title_zh': '超越化学问答：通过模块化化学操作评估大语言模型的化学推理能力'}
{'arxiv_id': 'arXiv:2505.21291', 'title': 'Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework', 'authors': 'Saman Marandi, Yu-Shu Hu, Mohammad Modarres', 'link': 'https://arxiv.org/abs/2505.21291', 'abstract': "In this paper, we present a novel diagnostic framework that integrates Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system diagnostics in high-reliability systems such as nuclear power plants. Traditional diagnostic modeling struggles when systems become too complex, making functional modeling a more attractive approach. Our approach introduces a diagnostic framework grounded in the functional modeling principles of the Dynamic Master Logic (DML) model. It incorporates two coordinated LLM components, including an LLM-based workflow for automated construction of DML logic from system documentation and an LLM agent that facilitates interactive diagnostics. The generated logic is encoded into a structured KG, referred to as KG-DML, which supports hierarchical fault reasoning. Expert knowledge or operational data can also be incorporated to refine the model's precision and diagnostic depth. In the interaction phase, users submit natural language queries, which are interpreted by the LLM agent. The agent selects appropriate tools for structured reasoning, including upward and downward propagation across the KG-DML. Rather than embedding KG content into every prompt, the LLM agent distinguishes between diagnostic and interpretive tasks. For diagnostics, the agent selects and executes external tools that perform structured KG reasoning. For general queries, a Graph-based Retrieval-Augmented Generation (Graph-RAG) approach is used, retrieving relevant KG segments and embedding them into the prompt to generate natural explanations. A case study on an auxiliary feedwater system demonstrated the framework's effectiveness, with over 90% accuracy in key elements and consistent tool and argument extraction, supporting its use in safety-critical diagnostics.", 'abstract_zh': '本文提出了一种将知识图谱（KGs）和大规模语言模型（LLMs）集成的新颖诊断框架，以支持核电站等高可靠性系统中的系统诊断。传统诊断建模在系统变得过于复杂时会遇到困难，使功能建模成为更有吸引力的方法。我们的方法引入了一个基于动态主逻辑（DML）模型的功能建模原则的诊断框架。它包含两个协调的LLM组件，包括基于LLM的工作流，用于自动化从系统文档中构建DML逻辑，并且还有一个LLM代理，促进互动诊断。生成的逻辑被编码为结构化的KG，称为KG-DML，支持分层故障推理。专家知识或运营数据也可以被纳入以提高模型的精度和诊断深度。在交互阶段，用户提交自然语言查询，这些查询由LLM代理解析。代理选择适当的工具进行结构化推理，包括KG-DML中的上下级传播。与将KG内容嵌入每个提示不同，LLM代理区分诊断和解释任务。在诊断任务中，代理选择并执行外部工具来执行结构化的KG推理。对于一般查询，使用基于图的检索增强生成（Graph-RAG）方法，检索相关KG片段，并将它们嵌入提示以生成自然解释。一项关于辅助给水系统的案例研究证明了该框架的有效性，关键元素的准确率达到90%以上，并且工具和参数提取一致，支持其在安全关键诊断中的应用。', 'title_zh': '基于知识图谱指导和大型语言模型增强的复杂系统诊断框架'}
{'arxiv_id': 'arXiv:2505.21281', 'title': 'RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with Large Language Models', 'authors': 'Yue Zhang, Zhiliang Tian, Shicheng Zhou, Haiyang Wang, Wenqing Hou, Yuying Liu, Xuechen Zhao, Minlie Huang, Ye Wang, Bin Zhou', 'link': 'https://arxiv.org/abs/2505.21281', 'abstract': 'Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing semantic-enhanced LJP models integrate judicial precedents and legal knowledge for high performance. But they neglect legal reasoning logic, a critical component of legal judgments requiring rigorous logical analysis. Although some approaches utilize legal reasoning logic for high-quality predictions, their logic rigidity hinders adaptation to case-specific logical frameworks, particularly in complex cases that are lengthy and detailed. This paper proposes a rule-enhanced legal judgment prediction framework based on first-order logic (FOL) formalism and comparative learning (CL) to develop an adaptive adjustment mechanism for legal judgment logic and further enhance performance in LJP. Inspired by the process of human exam preparation, our method follows a three-stage approach: first, we initialize judgment rules using the FOL formalism to capture complex reasoning logic accurately; next, we propose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize the judgment rules through a quiz consisting of confusable cases; finally, we utilize the optimized judgment rules to predict legal judgments. Experimental results on two public datasets show superior performance across all metrics. The code is publicly available{this https URL}.', 'abstract_zh': '基于一阶逻辑和对比学习的规则增强法律判决预测框架', 'title_zh': 'RLJP：借助大型语言模型增强的一阶逻辑规则辅助法律判决预测'}
{'arxiv_id': 'arXiv:2505.21279', 'title': 'XBOUND: Exploring the Capability Boundaries of Device-Control Agents through Trajectory Tree Exploration', 'authors': 'Shaoqing Zhang, Kehai Chen, Zhuosheng Zhang, Rumei Li, Rongxiang Weng, Yang Xiang, Liqiang Nie, Min Zhang', 'link': 'https://arxiv.org/abs/2505.21279', 'abstract': "Recent advancements in vision-language models (VLMs) have spurred increased interest in Device-Control Agents (DC agents), such as utilizing in-the-wild device control to manage graphical user interfaces. Conventional methods for assessing the capabilities of DC agents, such as computing step-wise action accuracy and overall task success rates, provide a macroscopic view of DC agents' performance; however, they fail to offer microscopic insights into potential errors that may occur in real-world applications. Conducting a finer-grained performance evaluation of DC agents presents significant challenges. This study introduces a new perspective on evaluation methods for DC agents by proposing the XBOUND evaluation method, which employs the calculation of a novel Explore Metric to delineate the capability boundaries of DC agents. Compared to previous evaluation methods, XBOUND focuses on individual states to assess the proficiency of DC agents in mastering these states. Furthermore, we have developed a ``pseudo'' episode tree dataset derived from Android Control test data. Utilizing this dataset and XBOUND, we comprehensively evaluate the OS-Atlas and UI-TARS series, examining both the overall and specific performance across five common tasks. Additionally, we select representative cases to highlight the current deficiencies and limitations inherent in both series. Code is available at this https URL.", 'abstract_zh': '最近视觉-语言模型的发展促进了设备控制代理的研究，如利用野外设备控制管理图形用户界面。传统的方法通过计算逐步动作准确率和整体任务成功率来评估设备控制代理的能力，提供了宏观视角但无法提供潜在错误的微观见解。对设备控制代理进行细粒度性能评估面临重大挑战。本文通过提出XBOUND评估方法引入了一种新的评估方法视角，该方法通过计算一种新颖的探索度量来界定设备控制代理的能力边界。与以往的方法相比，XBOUND着眼于个体状态来评估代理掌握这些状态的能力。此外，我们基于Android Control测试数据开发了一个“伪”情景树数据集。利用此数据集和XBOUND，我们全面评估了OS-Atlas和UI-TARS系列，在五个常见任务中考察了整体和特定性能，并选择了代表性案例突出两个系列中存在的当前不足和局限性。代码可在以下链接获取。', 'title_zh': 'XBOUND：通过轨迹树探索设备控制代理的能力边界'}
{'arxiv_id': 'arXiv:2505.21212', 'title': 'Interpretable DNFs', 'authors': 'Martin C. Cooper, Imane Bousdira, Clément Carbonnel', 'link': 'https://arxiv.org/abs/2505.21212', 'abstract': 'A classifier is considered interpretable if each of its decisions has an explanation which is small enough to be easily understood by a human user. A DNF formula can be seen as a binary classifier $\\kappa$ over boolean domains. The size of an explanation of a positive decision taken by a DNF $\\kappa$ is bounded by the size of the terms in $\\kappa$, since we can explain a positive decision by giving a term of $\\kappa$ that evaluates to true. Since both positive and negative decisions must be explained, we consider that interpretable DNFs are those $\\kappa$ for which both $\\kappa$ and $\\overline{\\kappa}$ can be expressed as DNFs composed of terms of bounded size. In this paper, we study the family of $k$-DNFs whose complements can also be expressed as $k$-DNFs. We compare two such families, namely depth-$k$ decision trees and nested $k$-DNFs, a novel family of models. Experiments indicate that nested $k$-DNFs are an interesting alternative to decision trees in terms of interpretability and accuracy.', 'abstract_zh': '一种分类器如果其每个决策都有一个足够小且易于人类用户理解的解释，则被认为是可解释的。DNF公式可以被视为布尔域上的二元分类器κ。DNF κ的一个正决策的解释大小受κ中项的大小限制，因为我们可以通过给出κ中评估为真的项来解释正决策。既然正决策和负决策都必须进行解释，我们考虑可解释的DNF是那些κ及其补集都可以用大小受限的项组成的DNF表示的κ。在本文中，我们研究一类其补集也可以用k-DNF表示的k-DNF家族。我们将比较两类这样的家族，即深度为k的决策树和嵌套k-DNF，这是一种新型模型。实验表明，与决策树相比，嵌套k-DNF在可解释性和准确性方面都是一个有趣的选择。', 'title_zh': '可解释的DNFs'}
{'arxiv_id': 'arXiv:2505.21106', 'title': 'Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation', 'authors': 'Zhengyang Ji, Yifan Jia, Shang Gao, Yutao Yue', 'link': 'https://arxiv.org/abs/2505.21106', 'abstract': "Large Vision Language Models (LVLMs) have achieved remarkable progress in multimodal tasks, yet they also exhibit notable social biases. These biases often manifest as unintended associations between neutral concepts and sensitive human attributes, leading to disparate model behaviors across demographic groups. While existing studies primarily focus on detecting and quantifying such biases, they offer limited insight into the underlying mechanisms within the models. To address this gap, we propose an explanatory framework that combines information flow analysis with multi-round dialogue evaluation, aiming to understand the origin of social bias from the perspective of imbalanced internal information utilization. Specifically, we first identify high-contribution image tokens involved in the model's reasoning process for neutral questions via information flow analysis. Then, we design a multi-turn dialogue mechanism to evaluate the extent to which these key tokens encode sensitive information. Extensive experiments reveal that LVLMs exhibit systematic disparities in information usage when processing images of different demographic groups, suggesting that social bias is deeply rooted in the model's internal reasoning dynamics. Furthermore, we complement our findings from a textual modality perspective, showing that the model's semantic representations already display biased proximity patterns, thereby offering a cross-modal explanation of bias formation.", 'abstract_zh': '大型视觉语言模型（LVLMs）在多模态任务中取得了显著进展，但也表现出明显的社会偏见。这些偏见往往以中立概念和敏感的人类属性之间未预期的关联形式出现，导致不同人口统计学群体之间模型行为的差异性。尽管现有研究主要集中在检测和量化这些偏见上，但它们对模型内部机制的理解有限。为了解决这一问题，我们提出了一种结合信息流分析与多轮对话评估的解释框架，旨在从不平衡的内部信息利用角度理解社会偏见的根源。具体来说，我们首先通过信息流分析识别出模型在处理中立问题时参与推理过程的高贡献图像令牌。然后，我们设计了一种多轮对话机制来评估这些关键令牌在多大程度上编码了敏感信息。广泛实验表明，LVLMs在处理不同人口统计学群体的图像时，在信息使用上存在系统性差异，这表明社会偏见深深植根于模型的内部推理动态中。此外，我们从文本模态的角度补充了这些发现，表明模型的语义表示已经显示出偏向性的邻近模式，从而提供一种跨模态的偏见形成解释。', 'title_zh': '通过信息流分析和多轮对话评估解读LVLMs中的社会偏见'}
{'arxiv_id': 'arXiv:2505.21067', 'title': 'Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning', 'authors': 'Xiao Hu, Xingyu Lu, Liyuan Mao, YiFan Zhang, Tianke Zhang, Bin Wen, Fan Yang, Tingting Gao, Guorui Zhou', 'link': 'https://arxiv.org/abs/2505.21067', 'abstract': 'Reinforcement learning (RL) has played an important role in improving the reasoning ability of large language models (LLMs). Some studies apply RL directly to \\textit{smaller} base models (known as zero-RL) and also achieve notable progress. However, in this paper, we show that using only 920 examples, a simple distillation method based on the base model can clearly outperform zero-RL, which typically requires much more data and computational cost. By analyzing the token frequency in model outputs, we find that the distilled model shows more flexible reasoning. It uses anthropomorphic tokens and logical connectors much more often than the zero-RL model. Further analysis reveals that distillation enhances the presence of two advanced cognitive behaviors: Multi-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent occurrences of these two advanced cognitive behaviors give rise to flexible reasoning, which is essential for solving complex reasoning problems, while zero-RL fails to significantly boost the frequency of these behaviors.', 'abstract_zh': '强化学习（RL）在提高大型语言模型（LLMs）的推理能力方面发挥了重要作用。一些研究直接将RL应用于较小的基础模型（称为零-RL），并取得了显著进展。然而，在本文中，我们展示了仅使用920个示例，基于基础模型的简单蒸馏方法可以明显优于零-RL，后者通常需要更多的数据和计算成本。通过分析模型输出中的令牌频率，我们发现蒸馏模型表现出更强的灵活性。它比零-RL模型更频繁地使用拟人化令牌和逻辑连接符。进一步分析揭示，蒸馏增强了两种高级认知行为：多视角思考或尝试和元认知意识的出现频率。这两种高级认知行为的频繁出现促成了灵活的推理，这对于解决复杂推理问题至关重要，而零-RL则未能显著提升这些行为的频率。', 'title_zh': '为什么知识蒸馏能超越零样本强化学习：灵活推理的作用'}
{'arxiv_id': 'arXiv:2505.21055', 'title': 'Agent-Environment Alignment via Automated Interface Generation', 'authors': 'Kaiming Liu, Xuanyu Lei, Ziyue Wang, Peng Li, Yang Liu', 'link': 'https://arxiv.org/abs/2505.21055', 'abstract': 'Large language model (LLM) agents have shown impressive reasoning capabilities in interactive decision-making tasks. These agents interact with environment through intermediate interfaces, such as predefined action spaces and interaction rules, which mediate the perception and action. However, mismatches often happen between the internal expectations of the agent regarding the influence of its issued actions and the actual state transitions in the environment, a phenomenon referred to as \\textbf{agent-environment misalignment}. While prior work has invested substantially in improving agent strategies and environment design, the critical role of the interface still remains underexplored. In this work, we empirically demonstrate that agent-environment misalignment poses a significant bottleneck to agent performance. To mitigate this issue, we propose \\textbf{ALIGN}, an \\underline{A}uto-A\\underline{l}igned \\underline{I}nterface \\underline{G}e\\underline{n}eration framework that alleviates the misalignment by enriching the interface. Specifically, the ALIGN-generated interface enhances both the static information of the environment and the step-wise observations returned to the agent. Implemented as a lightweight wrapper, this interface achieves the alignment without modifying either the agent logic or the environment code. Experiments across multiple domains including embodied tasks, web navigation and tool-use, show consistent performance improvements, with up to a 45.67\\% success rate improvement observed in ALFWorld. Meanwhile, ALIGN-generated interface can generalize across different agent architectures and LLM backbones without interface regeneration. Code and experimental results are available at this https URL.', 'abstract_zh': '大语言模型代理在交互决策任务中展现了出色的推理能力。这些代理通过中介接口，如预定义的动作空间和交互规则，与环境进行互动。然而，代理发出的动作对其状态转换的实际影响往往存在预期与实际情况之间的不匹配，这一现象被称为\\[agent-environment misalignment\\]。尽管先前的工作在提高代理策略和环境设计方面付出了大量努力，但接口的关键作用仍未得到充分探索。在这项工作中，我们实证证明了\\[agent-environment misalignment\\]是代理性能的一个重要瓶颈。为缓解这一问题，我们提出了\\[ALIGN\\]自对齐接口生成框架，通过丰富接口来减轻这种不匹配。具体而言，由\\[ALIGN\\]生成的接口增强了环境的静态信息以及返回给代理的逐步观测。该接口作为轻量级包装器实现，无需修改代理逻辑或环境代码即可实现对齐。在包括具身任务、网页导航和工具使用在内的多个领域中的实验显示了一致的性能改进，ALFWorld 的成功率提升高达 45.67%。同时，\\[ALIGN\\]生成的接口可以在不同的代理架构和大语言模型基础之上进行泛化，无需重新生成接口。代码和实验结果可在以下链接获取。', 'title_zh': '基于自动化接口生成的代理-环境对齐方法'}
{'arxiv_id': 'arXiv:2505.21045', 'title': 'Large Language Model-enhanced Reinforcement Learning for Low-Altitude Economy Networking', 'authors': 'Lingyi Cai, Ruichen Zhang, Changyuan Zhao, Yu Zhang, Jiawen Kang, Dusit Niyato, Tao Jiang, Xuemin Shen', 'link': 'https://arxiv.org/abs/2505.21045', 'abstract': 'Low-Altitude Economic Networking (LAENet) aims to support diverse flying applications below 1,000 meters by deploying various aerial vehicles for flexible and cost-effective aerial networking. However, complex decision-making, resource constraints, and environmental uncertainty pose significant challenges to the development of the LAENet. Reinforcement learning (RL) offers a potential solution in response to these challenges but has limitations in generalization, reward design, and model stability. The emergence of large language models (LLMs) offers new opportunities for RL to mitigate these limitations. In this paper, we first present a tutorial about integrating LLMs into RL by using the capacities of generation, contextual understanding, and structured reasoning of LLMs. We then propose an LLM-enhanced RL framework for the LAENet in terms of serving the LLM as information processor, reward designer, decision-maker, and generator. Moreover, we conduct a case study by using LLMs to design a reward function to improve the learning performance of RL in the LAENet. Finally, we provide a conclusion and discuss future work.', 'abstract_zh': '低空经济网络（LAENet）旨在通过部署各种空中车辆支持1000米以下的多元飞行应用，实现灵活且经济高效的空中网络。然而，复杂的决策问题、资源限制和环境不确定性对LAENet的发展提出了重大挑战。强化学习（RL）为应对这些挑战提供了潜在的解决方案，但存在泛化能力、奖励设计和模型稳定性等方面的局限性。大型语言模型（LLMs）的出现为RL克服这些局限性提供了新的机遇。在本文中，我们首先通过利用LLMs的生成能力、语境理解和结构化推理能力介绍将LLMs整合到RL中的教程。然后，我们提出了一种增强型RL框架，用于LAENet，通过将LLMs用作信息处理器、奖励设计师、决策制定者和生成器。此外，我们通过使用LLMs设计奖励函数，以提高RL在LAENet中的学习性能进行了案例研究。最后，我们总结了结论并探讨了未来的工作。', 'title_zh': '基于大型语言模型增强的低空经济网络强化学习'}
{'arxiv_id': 'arXiv:2505.20948', 'title': 'Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs', 'authors': 'Yisen Gao, Jiaxin Bai, Tianshi Zheng, Qingyun Sun, Ziwei Zhang, Jianxin Li, Yangqiu Song, Xingcheng Fu', 'link': 'https://arxiv.org/abs/2505.20948', 'abstract': 'Abductive reasoning in knowledge graphs aims to generate plausible logical hypotheses from observed entities, with broad applications in areas such as clinical diagnosis and scientific discovery. However, due to a lack of controllability, a single observation may yield numerous plausible but redundant or irrelevant hypotheses on large-scale knowledge graphs. To address this limitation, we introduce the task of controllable hypothesis generation to improve the practical utility of abductive reasoning. This task faces two key challenges when controlling for generating long and complex logical hypotheses: hypothesis space collapse and hypothesis oversensitivity. To address these challenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation framework for abductive reasoning over knowledge graphs, trained in a two-stage paradigm including supervised learning and subsequent reinforcement learning. To mitigate hypothesis space collapse, we design a dataset augmentation strategy based on sub-logical decomposition, enabling the model to learn complex logical structures by leveraging semantic patterns in simpler components. To address hypothesis oversensitivity, we incorporate smoothed semantic rewards including Dice and Overlap scores, and introduce a condition-adherence reward to guide the generation toward user-specified control constraints. Extensive experiments on three benchmark datasets demonstrate that our model not only better adheres to control conditions but also achieves superior semantic similarity performance compared to baselines.', 'abstract_zh': '知识图谱中的可控演绎推理旨在从观察实体中生成 plausible 的逻辑假设，广泛应用于临床诊断和科学发现等领域。然而，由于可控性不足，单一观察可能在大规模知识图谱中生成大量但冗余或不相关的有效假设。为解决这一局限，我们引入可控假设生成任务以提高演绎推理的实用价值。该任务在控制生成长且复杂的逻辑假设时面临两个关键挑战：假设空间坍塌和假设过度敏感。为应对这些挑战，我们提出了一种名为 CtrlHGen 的可控逻辑假设生成框架，用于知识图谱上的演绎推理，该框架采用包含监督学习和随后的强化学习的两阶段训练方式。为缓解假设空间坍塌，我们设计了一种基于子逻辑分解的数据集增强策略，使模型能够通过利用简单组件中的语义模式来学习复杂的逻辑结构。为应对假设过度敏感，我们引入了平滑语义奖励（包括 Dice 和 Overlap 分数）和条件一致性奖励，以指导生成符合用户指定控制约束的方向。在三个基准数据集上的广泛实验表明，我们的模型不仅更好地符合控制条件，还实现了优于基线的语义相似度性能。', 'title_zh': '可控制的逻辑假设生成以实现知识图谱中的归纳推理'}
{'arxiv_id': 'arXiv:2505.20889', 'title': 'Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment', 'authors': 'Leizhen Wang, Peibo Duan, Cheng Lyu, Zhenliang Ma', 'link': 'https://arxiv.org/abs/2505.20889', 'abstract': "Modern navigation systems and shared mobility platforms increasingly rely on personalized route recommendations to improve individual travel experience and operational efficiency. However, a key question remains: can such sequential, personalized routing decisions collectively lead to system-optimal (SO) traffic assignment? This paper addresses this question by proposing a learning-based framework that reformulates the static SO traffic assignment problem as a single-agent deep reinforcement learning (RL) task. A central agent sequentially recommends routes to travelers as origin-destination (OD) demands arrive, to minimize total system travel time. To enhance learning efficiency and solution quality, we develop an MSA-guided deep Q-learning algorithm that integrates the iterative structure of traditional traffic assignment methods into the RL training process. The proposed approach is evaluated on both the Braess and Ortuzar-Willumsen (OW) networks. Results show that the RL agent converges to the theoretical SO solution in the Braess network and achieves only a 0.35% deviation in the OW network. Further ablation studies demonstrate that the route action set's design significantly impacts convergence speed and final performance, with SO-informed route sets leading to faster learning and better outcomes. This work provides a theoretically grounded and practically relevant approach to bridging individual routing behavior with system-level efficiency through learning-based sequential assignment.", 'abstract_zh': '基于学习的框架：个人化路径推荐与系统最优交通分配的统一', 'title_zh': '基于强化学习的顺序路线推荐方法用于系统最优交通分配'}
{'arxiv_id': 'arXiv:2505.20869', 'title': 'Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving', 'authors': 'Kuo Zhou, Lu Zhang', 'link': 'https://arxiv.org/abs/2505.20869', 'abstract': 'Large Language Models (LLMs) have demonstrated formidable capabilities in solving mathematical problems, yet they may still commit logical reasoning and computational errors during the problem-solving process. Thus, this paper proposes a framework, MATH-VF, which includes a Formalizer and a Critic, for formally verifying the correctness of the solutions generated by large language models. Our framework first utilizes a Formalizer which employs an LLM to translate a natural language solution into a formal context. Afterward, our Critic (which integrates various external tools such as a Computer Algebra System and an SMT solver) evaluates the correctness of each statement within the formal context, and when a statement is incorrect, our Critic provides corrective feedback. We empirically investigate the effectiveness of MATH-VF in two scenarios: 1) Verification: MATH-VF is utilized to determine the correctness of a solution to a given problem. 2) Refinement: When MATH-VF identifies errors in the solution generated by an LLM-based solution generator for a given problem, it submits the corrective suggestions proposed by the Critic to the solution generator to regenerate the solution. We evaluate our framework on widely used mathematical benchmarks: MATH500 and ProcessBench, demonstrating the superiority of our approach over existing approaches.', 'abstract_zh': '大型语言模型（LLMs）在解决数学问题方面展现了强大的能力，但在解题过程中仍可能犯逻辑推理和计算错误。因此，本文提出了一种名为MATH-VF的框架，该框架包括一个形式化模块和一个批评者模块，用于正式验证由大型语言模型生成的解题答案的正确性。该框架首先使用形式化模块将自然语言解决方案转换为正式语境。随后，批评者模块（集成了计算机代数系统和SMT求解器等外部工具）评估每个陈述在正式语境下的正确性，并在发现问题时提供纠正反馈。我们通过两个场景实证研究了MATH-VF的有效性：1）验证：使用MATH-VF验证给定问题解题答案的正确性。2）完善：当MATH-VF发现基于LLM的解题生成器生成的给定问题解题答案中存在错误时，批评者模块提出纠正建议，提交给解题生成器以重新生成解题答案。我们利用广泛使用的数学基准测试MATH500和ProcessBench评估了该框架，证明了该方法优于现有方法。', 'title_zh': '逐级形式验证以支持大模型为基础的数学问题求解'}
{'arxiv_id': 'arXiv:2505.20820', 'title': 'MT-Mol:Multi Agent System with Tool-based Reasoning for Molecular Optimization', 'authors': 'Hyomin Kim, Yunhui Jang, Sungsoo Ahn', 'link': 'https://arxiv.org/abs/2505.20820', 'abstract': 'Large language models (LLMs) have large potential for molecular optimization, as they can gather external chemistry tools and enable collaborative interactions to iteratively refine molecular candidates. However, this potential remains underexplored, particularly in the context of structured reasoning, interpretability, and comprehensive tool-grounded molecular optimization. To address this gap, we introduce MT-Mol, a multi-agent framework for molecular optimization that leverages tool-guided reasoning and role-specialized LLM agents. Our system incorporates comprehensive RDKit tools, categorized into five distinct domains: structural descriptors, electronic and topological features, fragment-based functional groups, molecular representations, and miscellaneous chemical properties. Each category is managed by an expert analyst agent, responsible for extracting task-relevant tools and enabling interpretable, chemically grounded feedback. MT-Mol produces molecules with tool-aligned and stepwise reasoning through the interaction between the analyst agents, a molecule-generating scientist, a reasoning-output verifier, and a reviewer agent. As a result, we show that our framework shows the state-of-the-art performance of the PMO-1K benchmark on 17 out of 23 tasks.', 'abstract_zh': '大型语言模型（LLMs）在分子优化方面具有巨大潜力，它们可以整合外部化学工具并促进协作交互，以迭代优化分子候选物。然而，这一潜力在结构化推理、可解释性和全面工具导向的分子优化方面的应用仍然未被充分探索。为了解决这一差距，我们引入了MT-Mol，这是一种利用工具引导的推理和角色专业化LLM代理的多代理分子优化框架。该系统整合了全面的RDKit工具，分为五个不同的领域：结构描述符、电学和拓扑特征、基于片段的功能团、分子表示和各种化学属性。每个类别均由一位专家分析代理管理，负责提取相关工具并提供可解释且基于化学的反馈。MT-Mol通过分析师代理、分子生成科学家、推理输出验证器和审阅代理之间的交互，生成与工具对齐且逐步推理的分子。结果表明，我们的框架在PMO-1K基准测试的23项任务中有17项达到了最先进的性能。', 'title_zh': '基于工具推理的多Agent系统：分子优化'}
{'arxiv_id': 'arXiv:2505.20749', 'title': 'Can Agents Fix Agent Issues?', 'authors': 'Alfin Wijaya Rahardja, Junwei Liu, Weitong Chen, Zhenpeng Chen, Yiling Lou', 'link': 'https://arxiv.org/abs/2505.20749', 'abstract': 'LLM-based agent systems are emerging as a new software paradigm and have been widely adopted across diverse domains such as medicine, robotics, and programming. However, maintaining these systems requires substantial effort, as they are inevitably prone to bugs and continually evolve to meet changing external requirements. Therefore, automatically resolving agent issues (i.e., bug reports or feature requests) is a crucial and challenging task. While recent software engineering (SE) agents (e.g., SWE-agent) have shown promise in addressing issues in traditional software systems, it remains unclear how effectively they can resolve real-world issues in agent systems, which differ significantly from traditional software. To fill this gap, we first manually analyze 201 real-world agent issues and identify common categories of agent issues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a reproducible benchmark comprising 50 agent issue resolution tasks (each with an executable environment and failure-triggering tests). We further evaluate state-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited effectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results underscore the unique challenges of maintaining agent systems compared to traditional software, highlighting the need for further research to develop advanced SE agents for resolving agent issues. Data and code are available at this https URL .', 'abstract_zh': '基于LLM的代理系统正 emerge as a 新的软件范式，并已在医学、机器人技术和编程等多个领域广泛应用。然而，维持这些系统需要大量努力，因为它们不可避免地存在 bug 并且不断进化以满足不断变化的外部需求。因此，自动解决代理问题（即 bug 报告或功能请求）是一项关键而具有挑战性的任务。虽然最近的软件工程 (SE) 代理（例如 SWE-agent）在解决传统软件系统的问题方面显示出潜力，但对于代理系统中这些与传统软件显著不同的真实问题，它们的实际解决效果仍不明确。为填补这一空白，我们首先手动分析了 201 个真实世界的代理问题，并确定了代理问题的常见类别。然后，我们花费 500 个人工时构建了 AGENTISSUE-BENCH，这是一个可重现的基准，包含 50 个代理问题解决任务（每个任务包含可执行环境和故障触发测试）。我们在 AGENTISSUE-BENCH 上进一步评估了最先进的 SE 代理，揭示了它们有限的有效性（即解决率仅为 3.33% - 12.67%）。这些结果强调了与传统软件相比，维护代理系统所面临的独特挑战，突显了进一步研究以开发先进的 SE 代理用于解决代理问题的必要性。数据和代码可在以下网址获得。', 'title_zh': '代理能否解决代理问题？'}
{'arxiv_id': 'arXiv:2505.20740', 'title': 'MSEarth: A Benchmark for Multimodal Scientific Comprehension of Earth Science', 'authors': 'Xiangyu Zhao, Wanghan Xu, Bo Liu, Yuhao Zhou, Fenghua Ling, Ben Fei, Xiaoyu Yue, Lei Bai, Wenlong Zhang, Xiao-Ming Wu', 'link': 'https://arxiv.org/abs/2505.20740', 'abstract': 'The rapid advancement of multimodal large language models (MLLMs) has unlocked new opportunities to tackle complex scientific challenges. Despite this progress, their application in addressing earth science problems, especially at the graduate level, remains underexplored. A significant barrier is the absence of benchmarks that capture the depth and contextual complexity of geoscientific reasoning. Current benchmarks often rely on synthetic datasets or simplistic figure-caption pairs, which do not adequately reflect the intricate reasoning and domain-specific insights required for real-world scientific applications. To address these gaps, we introduce MSEarth, a multimodal scientific benchmark curated from high-quality, open-access scientific publications. MSEarth encompasses the five major spheres of Earth science: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere, featuring over 7K figures with refined captions. These captions are crafted from the original figure captions and enriched with discussions and reasoning from the papers, ensuring the benchmark captures the nuanced reasoning and knowledge-intensive content essential for advanced scientific tasks. MSEarth supports a variety of tasks, including scientific figure captioning, multiple choice questions, and open-ended reasoning challenges. By bridging the gap in graduate-level benchmarks, MSEarth provides a scalable and high-fidelity resource to enhance the development and evaluation of MLLMs in scientific reasoning. The benchmark is publicly available to foster further research and innovation in this field. Resources related to this benchmark can be found at this https URL and this https URL.', 'abstract_zh': '多模态大规模语言模型的迅速发展为应对复杂的科学挑战开启了新机遇。尽管取得了这些进展，它们在解决地球科学问题方面的应用，尤其是在研究生层次的应用，仍然缺乏探索。一个重要的障碍是缺乏能够捕捉地质科学推理深度和背景复杂性的基准。当前的基准数据集往往依赖于合成数据集或简单的图表标题对，这并不能充分反映实际科学应用所需的复杂推理和领域特定见解。为了弥补这些不足，我们引入了MSEarth，这是一个从高质量的开放获取科学出版物中精选出的多模态科学基准。MSEarth涵盖了地球科学的五大领域：大气层、冰雪层、水圈、岩石圈和生物圈，包含超过7000幅图并配有精炼的标题。这些标题从原始图表标题中提炼而来，并结合了论文中的讨论和推理，确保基准数据集捕捉到高级科学任务所需的关键推理和知识密集型内容。MSEarth支持多种任务，包括科学图表描述、多项选择题和开放性推理挑战。通过对研究生水平基准数据的填补，MSEarth提供了一个可扩展且高保真的资源，以增强多模态大型语言模型在科学推理方面的研发和评估。该基准数据集已公开发布，以促进该领域的进一步研究和创新。与此基准相关的资源可在以下网址获取：<https://> 和 <https://>。', 'title_zh': 'MSEarth: 一个地球科学多模态科学理解基准'}
{'arxiv_id': 'arXiv:2505.20737', 'title': 'RRO: LLM Agent Optimization Through Rising Reward Trajectories', 'authors': 'Zilong Wang, Jingfeng Yang, Sreyashi Nag, Samarth Varshney, Xianfeng Tang, Haoming Jiang, Jingbo Shang, Sheikh Muhammad Sarwar', 'link': 'https://arxiv.org/abs/2505.20737', 'abstract': 'Large language models (LLMs) have exhibited extraordinary performance in a variety of tasks while it remains challenging for them to solve complex multi-step tasks as agents. In practice, agents sensitive to the outcome of certain key steps which makes them likely to fail the task because of a subtle mistake in the planning trajectory. Recent approaches resort to calibrating the reasoning process through reinforcement learning. They reward or penalize every reasoning step with process supervision, as known as Process Reward Models (PRMs). However, PRMs are difficult and costly to scale up with a large number of next action candidates since they require extensive computations to acquire the training data through the per-step trajectory exploration. To mitigate this issue, we focus on the relative reward trend across successive reasoning steps and propose maintaining an increasing reward in the collected trajectories for process supervision, which we term Reward Rising Optimization (RRO). Specifically, we incrementally augment the process supervision until identifying a step exhibiting positive reward differentials, i.e. rising rewards, relative to its preceding iteration. This method dynamically expands the search space for the next action candidates, efficiently capturing high-quality data. We provide mathematical groundings and empirical results on the WebShop and InterCode-SQL benchmarks, showing that our proposed RRO achieves superior performance while requiring much less exploration cost.', 'abstract_zh': '大型语言模型（LLMs）在多种任务中表现出色，但在解决复杂的多步任务方面仍面临的挑战。由于某些关键步骤结果敏感，代理容易因计划轨迹中的细微错误而失败。近期的方法通过强化学习校准推理过程，对每个推理步骤进行奖励或惩罚，这种方法被称为过程奖励模型（PRMs）。然而，当存在大量下行动作候选时，PRMs难以扩展，因为它们需要大量的计算来通过逐步骤轨迹探索获取训练数据。为缓解这一问题，我们重点关注连续推理步骤之间的相对奖励趋势，提出在收集的轨迹中保持奖励递增的过程监督，我们称之为奖励上升优化（RRO）。具体而言，我们逐步增加过程监督，直到识别出一个相对于其前一迭代表现出正奖励差异的步骤，即奖励上升的步骤。该方法动态扩展了下行动作候选的搜索空间，有效地捕获了高质量的数据。我们在WebShop和InterCode-SQL基准测试上提供了数学依据和实验证据，表明我们提出的RRO在探索成本显著降低的情况下实现了更好的性能。', 'title_zh': 'RRO：通过上升奖励轨迹优化大语言模型代理'}
{'arxiv_id': 'arXiv:2505.20733', 'title': 'E2E Process Automation Leveraging Generative AI and IDP-Based Automation Agent: A Case Study on Corporate Expense Processing', 'authors': 'Cheonsu Jeong, Seongmin Sim, Hyoyoung Cho, Sungsu Kim, Byounggwan Shin', 'link': 'https://arxiv.org/abs/2505.20733', 'abstract': 'This paper presents an intelligent work automation approach in the context of contemporary digital transformation by integrating generative AI and Intelligent Document Processing (IDP) technologies with an Automation Agent to realize End-to-End (E2E) automation of corporate financial expense processing tasks. While traditional Robotic Process Automation (RPA) has proven effective for repetitive, rule-based simple task automation, it faces limitations in handling unstructured data, exception management, and complex decision-making. This study designs and implements a four-stage integrated process comprising automatic recognition of supporting documents such as receipts via OCR/IDP, item classification based on a policy-driven database, intelligent exception handling supported by generative AI (large language models, LLMs), and human-in-the-loop final decision-making with continuous system learning through an Automation Agent. Applied to a major Korean enterprise (Company S), the system demonstrated quantitative benefits including over 80% reduction in processing time for paper receipt expense tasks, decreased error rates, and improved compliance, as well as qualitative benefits such as enhanced accuracy and consistency, increased employee satisfaction, and data-driven decision support. Furthermore, the system embodies a virtuous cycle by learning from human judgments to progressively improve automatic exception handling capabilities. Empirically, this research confirms that the organic integration of generative AI, IDP, and Automation Agents effectively overcomes the limitations of conventional automation and enables E2E automation of complex corporate processes. The study also discusses potential extensions to other domains such as accounting, human resources, and procurement, and proposes future directions for AI-driven hyper-automation development.', 'abstract_zh': '基于生成式AI和智能文档处理的当代数字转型背景下智能工作自动化方法：集成自动化代理实现企业财务费用处理任务的端到端自动化', 'title_zh': '基于生成AI和基于IDP的自动化代理的端到端流程自动化：企业费用处理案例研究'}
{'arxiv_id': 'arXiv:2505.20728', 'title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'authors': 'Zesen Lyu, Dandan Zhang, Wei Ye, Fangdi Li, Zhihang Jiang, Yao Yang', 'link': 'https://arxiv.org/abs/2505.20728', 'abstract': "Spatial reasoning is a core component of human cognition, enabling individuals to perceive, comprehend, and interact with the physical world. It relies on a nuanced understanding of spatial structures and inter-object relationships, serving as the foundation for complex reasoning and decision-making. To investigate whether current vision-language models (VLMs) exhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark consisting of 1,100 carefully curated real-world images with high spatial complexity. Based on this dataset, we design five tasks to rigorously evaluate VLMs' spatial perception, structural understanding, and reasoning capabilities, while deliberately minimizing reliance on domain-specific knowledge to better isolate and assess the general spatial reasoning capability. We conduct a comprehensive evaluation across 24 state-of-the-art VLMs. The results show that even the strongest model, Gemini-2.5-Pro, achieves only 77.14% overall accuracy and performs particularly poorly on the Order Generation task, with only 30.00% accuracy, far below the performance exceeding 90% achieved by human participants. This persistent gap underscores the need for continued progress, positioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for advancing spatial reasoning research in VLMs.", 'abstract_zh': '空间推理是人类认知的核心组件，使个体能够感知、理解和与物理世界互动。它依赖于对空间结构和物体间关系的细腻理解，为复杂推理和决策奠定基础。为了探究当前视觉-语言模型（VLMs）是否具备类似的能力，我们引入了一种新的基准Jigsaw-Puzzles，该基准包含1100张精心挑选的实际场景图像，具有高度的空间复杂性。基于这个数据集，我们设计了五个任务以严格评估VLMs的空间感知、结构理解和推理能力，同时尽量减少对领域特定知识的依赖，以更好地隔离和评估其通用空间推理能力。我们在24种最新的VLMs中进行了全面评估。结果显示，即使是最强的模型Gemini-2.5-Pro，总体准确率也只有77.14%，在序列生成任务上的准确率仅为30.00%，远低于人类参与者超过90%的表现。这一持续的差距突显了继续进步的必要性，将Jigsaw-Puzzles定位为促进VLMs空间推理研究的一个挑战性且诊断性的基准。', 'title_zh': '拼图游戏：从视觉感知到理解再到推理在 Vision-Language 模型中的应用'}
{'arxiv_id': 'arXiv:2505.20672', 'title': 'GIFARC: Synthetic Dataset for Leveraging Human-Intuitive Analogies to Elevate AI Reasoning', 'authors': 'Woochang Sim, Hyunseok Ryu, Kyungmin Choi, Sungwon Han, Sundong Kim', 'link': 'https://arxiv.org/abs/2505.20672', 'abstract': 'The Abstraction and Reasoning Corpus (ARC) poses a stringent test of general AI capabilities, requiring solvers to infer abstract patterns from only a handful of examples. Despite substantial progress in deep learning, state-of-the-art models still achieve accuracy rates of merely 40-55% on 2024 ARC Competition, indicative of a significant gap between their performance and human-level reasoning. In this work, we seek to bridge that gap by introducing an analogy-inspired ARC dataset, GIFARC. Leveraging large language models (LLMs) and vision-language models (VLMs), we synthesize new ARC-style tasks from a variety of GIF images that include analogies. Each new task is paired with ground-truth analogy, providing an explicit mapping between visual transformations and everyday concepts. By embedding robust human-intuitive analogies into ARC-style tasks, GIFARC guides AI agents to evaluate the task analogically before engaging in brute-force pattern search, thus efficiently reducing problem complexity and build a more concise and human-understandable solution. We empirically validate that guiding LLM with analogic approach with GIFARC affects task-solving approaches of LLMs to align with analogic approach of human.', 'abstract_zh': '基于类比的ARC数据集GIFARC：通过融合大语言模型和视觉语言模型缩小通用AI性能与人类推理之间的差距', 'title_zh': 'GIFARC：利用人类直觉类比提升AI推理的合成数据集'}
{'arxiv_id': 'arXiv:2505.20671', 'title': 'LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation', 'authors': 'Heng Tan, Hua Yan, Yu Yang', 'link': 'https://arxiv.org/abs/2505.20671', 'abstract': "While reinforcement learning (RL) has achieved notable success in various domains, training effective policies for complex tasks remains challenging. Agents often converge to local optima and fail to maximize long-term rewards. Existing approaches to mitigate training bottlenecks typically fall into two categories: (i) Automated policy refinement, which identifies critical states from past trajectories to guide policy updates, but suffers from costly and uncertain model training; and (ii) Human-in-the-loop refinement, where human feedback is used to correct agent behavior, but this does not scale well to environments with large or continuous action spaces. In this work, we design a large language model-guided policy modulation framework that leverages LLMs to improve RL training without additional model training or human intervention. We first prompt an LLM to identify critical states from a sub-optimal agent's trajectories. Based on these states, the LLM then provides action suggestions and assigns implicit rewards to guide policy refinement. Experiments across standard RL benchmarks demonstrate that our method outperforms state-of-the-art baselines, highlighting the effectiveness of LLM-based explanations in addressing RL training bottlenecks.", 'abstract_zh': '尽管强化学习（RL）在多个领域取得了显著成功，但训练复杂任务的有效策略仍然具有挑战性。代理经常会收敛到局部最优解，无法最大化长期奖励。现有的缓解训练瓶颈的方法通常可以归为两类：（i）自动化策略精炼，通过识别过去轨迹中的关键状态来指导策略更新，但会带来昂贵且不确定的模型训练；（ii）人类在环策略精炼，利用人类反馈来纠正代理行为，但在大型或连续动作空间的环境中却难以扩展。在本工作中，我们设计了一种由大型语言模型引导的策略调节框架，利用LLMs提高RL训练效果，而无需额外的模型训练或人工干预。我们首先促使LLM从次优代理的轨迹中识别关键状态。基于这些状态，LLM提供行动建议并分配隐含奖励来指导策略精炼。在标准RL基准上的实验表明，我们的方法优于最先进的baseline，突出了基于LLM的解释在解决RL训练瓶颈方面的有效性。', 'title_zh': 'LLM引导的强化学习：通过策略调制解决训练瓶颈'}
{'arxiv_id': 'arXiv:2505.20670', 'title': 'MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning', 'authors': 'Zikang Guo, Benfeng Xu, Xiaorui Wang, Zhendong Mao', 'link': 'https://arxiv.org/abs/2505.20670', 'abstract': "Complex tasks involving tool integration pose significant challenges for Large Language Models (LLMs), leading to the emergence of multi-agent workflows as a promising solution. Reflection has emerged as an effective strategy for correcting erroneous trajectories in agentic workflows. However, existing approaches only exploit such capability in the post-action stage, where the agent observes the execution outcomes. We argue that, like humans, LLMs can also engage in reflection before action execution: the agent can anticipate undesirable outcomes from its own decisions, which not only provides a necessarily complementary perspective to evaluate the decision but also prevents the propagation of errors throughout the trajectory. In this paper, we propose MIRROR, a framework that consists of both intra-reflection, which critically assesses intended actions before execution, and inter-reflection, which further adjusts the trajectory based on observations. This design systematically leverages LLM reflection capabilities to eliminate and rectify erroneous actions on a more comprehensive scope. Evaluations on both the StableToolBench and TravelPlanner benchmarks demonstrate MIRROR's superior performance, achieving state-of-the-art results compared to existing approaches.", 'abstract_zh': '涉及工具集成的复杂任务对大型语言模型（LLMs）构成了重大挑战，促进了多代理工作流的兴起作为有前途的解决方案。反思被证明是纠正代理工作流中错误轨迹的有效策略。然而，现有方法仅在行动后阶段利用这种能力，此时代理观察执行结果。我们主张，类似于人类，LLMs也可以在行动执行前进行反思：代理可以预见其自身决策的不利后果，不仅可以提供必要的补充视角来评估决策，还可以防止错误在整个轨迹中的传播。在本文中，我们提出MIRROR框架，该框架包括了前置反思（在执行前批判性评估预定行动）和后置反思（基于观察进一步调整轨迹）。这种设计系统地利用了LLMs的反思能力，以更全面的范围消除和纠正错误行动。MIRROR在StableToolBench和TravelPlanner基准上的评估显示了其优越性能，达到了现有方法的最先进结果。', 'title_zh': 'MIRROR：多智能体内部和跨域反射优化工具学习推理'}
{'arxiv_id': 'arXiv:2505.20662', 'title': 'AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage', 'authors': 'Xuanle Zhao, Zilin Sang, Yuxuan Li, Qi Shi, Shuo Wang, Duzhen Zhang, Xu Han, Zhiyuan Liu, Maosong Sun', 'link': 'https://arxiv.org/abs/2505.20662', 'abstract': 'Efficient experiment reproduction is critical to accelerating progress in artificial intelligence. However, the inherent complexity of method design and training procedures presents substantial challenges for automation. Notably, reproducing experiments often requires implicit domain-specific knowledge not explicitly documented in the original papers. To address this, we introduce the paper lineage algorithm, which identifies and extracts implicit knowledge from the relevant references cited by the target paper. Building on this idea, we propose AutoReproduce, a multi-agent framework capable of automatically reproducing experiments described in research papers in an end-to-end manner. AutoReproduce enhances code executability by generating unit tests alongside the reproduction process. To evaluate the reproduction capability, we construct ReproduceBench, a benchmark annotated with verified implementations, and introduce novel evaluation metrics to assess both the reproduction and execution fidelity. Experimental results demonstrate that AutoReproduce outperforms the existing strong agent baselines on all five evaluation metrics by a peak margin of over $70\\%$. In particular, compared to the official implementations, AutoReproduce achieves an average performance gap of $22.1\\%$ on $89.74\\%$ of the executable experiment runs. The code will be available at this https URL.', 'abstract_zh': '高效的实验重现对于加速人工智能的发展至关重要。然而，方法设计和训练过程的内在复杂性为自动化带来了巨大挑战。值得注意的是，重现实验通常需要隐含领域的特定知识，而这些知识并未在原始论文中明确记录。为了解决这个问题，我们提出了论文谱系算法，该算法能够识别并提取目标论文所引用的相关参考文献中的隐含知识。在此基础上，我们提出了AutoReproduce，这是一种能够端到端自动重现研究论文中描述的实验的多代理框架。AutoReproduce通过在重现过程中生成单元测试来提高代码的可执行性。为了评估重现能力，我们构建了带有验证实现的ReproduceBench基准，并引入了新的评估指标来评估重现和执行的准确性。实验结果表明，AutoReproduce在所有五个评估指标上均优于现有强基线代理，峰值领先优势超过70%。特别地，与官方实现相比，AutoReproduce在89.74%可执行实验运行中实现了平均性能差距22.1%。代码将在该网址处提供。', 'title_zh': '自动生成：基于论文 lineage 的自动 AI 实验重现'}
{'arxiv_id': 'arXiv:2505.20642', 'title': 'CoderAgent: Simulating Student Behavior for Personalized Programming Learning with Large Language Models', 'authors': 'Yi Zhan, Qi Liu, Weibo Gao, Zheng Zhang, Tianfu Wang, Shuanghong Shen, Junyu Lu, Zhenya Huang', 'link': 'https://arxiv.org/abs/2505.20642', 'abstract': "Personalized programming tutoring, such as exercise recommendation, can enhance learners' efficiency, motivation, and outcomes, which is increasingly important in modern digital education. However, the lack of sufficient and high-quality programming data, combined with the mismatch between offline evaluation and real-world learning, hinders the practical deployment of such systems. To address this challenge, many approaches attempt to simulate learner practice data, yet they often overlook the fine-grained, iterative nature of programming learning, resulting in a lack of interpretability and granularity. To fill this gap, we propose a LLM-based agent, CoderAgent, to simulate students' programming processes in a fine-grained manner without relying on real data. Specifically, we equip each human learner with an intelligent agent, the core of which lies in capturing the cognitive states of the human programming practice process. Inspired by ACT-R, a cognitive architecture framework, we design the structure of CoderAgent to align with human cognitive architecture by focusing on the mastery of programming knowledge and the application of coding ability. Recognizing the inherent patterns in multi-layered cognitive reasoning, we introduce the Programming Tree of Thought (PTOT), which breaks down the process into four steps: why, how, where, and what. This approach enables a detailed analysis of iterative problem-solving strategies. Finally, experimental evaluations on real-world datasets demonstrate that CoderAgent provides interpretable insights into learning trajectories and achieves accurate simulations, paving the way for personalized programming education.", 'abstract_zh': '基于LLM的细粒度编程学习模拟器CoderAgent：提升个性化编程辅导实效性的探索', 'title_zh': 'CoderAgent：使用大型语言模型模拟学生行为以实现个性化编程学习'}
{'arxiv_id': 'arXiv:2505.20609', 'title': 'Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients', 'authors': 'Hyungjun Park, Chang-Yun Woo, Seungjo Lim, Seunghwan Lim, Keunho Kwak, Ju Young Jeong, Chong Hyun Suh', 'link': 'https://arxiv.org/abs/2505.20609', 'abstract': "Objective To develop an LLM based realtime compound diagnostic medical AI interface and performed a clinical trial comparing this interface and physicians for common internal medicine cases based on the United States Medical License Exam (USMLE) Step 2 Clinical Skill (CS) style exams. Methods A nonrandomized clinical trial was conducted on August 20, 2024. We recruited one general physician, two internal medicine residents (2nd and 3rd year), and five simulated patients. The clinical vignettes were adapted from the USMLE Step 2 CS style exams. We developed 10 representative internal medicine cases based on actual patients and included information available on initial diagnostic evaluation. Primary outcome was the accuracy of the first differential diagnosis. Repeatability was evaluated based on the proportion of agreement. Results The accuracy of the physicians' first differential diagnosis ranged from 50% to 70%, whereas the realtime compound diagnostic medical AI interface achieved an accuracy of 80%. The proportion of agreement for the first differential diagnosis was 0.7. The accuracy of the first and second differential diagnoses ranged from 70% to 90% for physicians, whereas the AI interface achieved an accuracy rate of 100%. The average time for the AI interface (557 sec) was 44.6% shorter than that of the physicians (1006 sec). The AI interface ($0.08) also reduced costs by 98.1% compared to the physicians' average ($4.2). Patient satisfaction scores ranged from 4.2 to 4.3 for care by physicians and were 3.9 for the AI interface Conclusion An LLM based realtime compound diagnostic medical AI interface demonstrated diagnostic accuracy and patient satisfaction comparable to those of a physician, while requiring less time and lower costs. These findings suggest that AI interfaces may have the potential to assist primary care consultations for common internal medicine cases.", 'abstract_zh': '基于LLM的实时复合诊断医疗AI界面开发及临床试验：以美国医学许可考试（USMLE）Step 2临床技能（CS）风格考试为基准比较医生与AI界面在常见内科病例中的表现', 'title_zh': '基于大型语言模型的实时综合诊断医疗AI界面与医生在模拟患者常见内科病例中的比较'}
{'arxiv_id': 'arXiv:2505.20522', 'title': 'Scaling over Scaling: Exploring Test-Time Scaling Pareto in Large Reasoning Models', 'authors': 'Jian Wang, Boyan Zhu, Chak Tou Leong, Yongqi Li, Wenjie Li', 'link': 'https://arxiv.org/abs/2505.20522', 'abstract': 'Large reasoning models (LRMs) have exhibited the capacity of enhancing reasoning performance via internal test-time scaling. Building upon this, a promising direction is to further scale test-time compute to unlock even greater reasoning capabilities. However, as we push these scaling boundaries, systematically understanding the practical limits and achieving optimal resource allocation becomes a critical challenge. In this paper, we investigate the scaling Pareto of test-time scaling and introduce the Test-Time Scaling Performance Model (TTSPM). We theoretically analyze two fundamental paradigms for such extended scaling, parallel scaling and sequential scaling, from a probabilistic modeling perspective. Our primary contribution is the derivation of the saturation point on the scaling budget for both strategies, identifying thresholds beyond which additional computation yields diminishing returns. Remarkably, despite their distinct mechanisms, both paradigms converge to a unified mathematical structure in their upper bounds. We empirically validate our theoretical findings on challenging reasoning benchmarks, including AIME, MATH-500, and GPQA, demonstrating the practical utility of these bounds for test-time resource allocation. We hope that this work provides insights into the cost-benefit trade-offs of test-time scaling, guiding the development of more resource-efficient inference strategies for large reasoning models.', 'abstract_zh': '大型推理模型通过内部测试时缩放提高了推理性能。在此基础上，进一步扩展测试时计算量以解锁更大的推理能力是一个有前景的方向。然而，在推动这些扩展边界时，系统地理解实际极限并实现最优资源分配成为了一个关键挑战。本文研究了测试时缩放的扩展缩放帕累托边界，并引入了测试时缩放性能模型（TTSPM）。我们从概率建模的角度分析了平行缩放和序贯缩放这两种扩展缩放的基本范式。我们的主要贡献是为这两种策略推导出缩放预算的饱和点，确定超出这些阈值后额外计算将不再带来边际收益。令人惊讶的是，尽管这两种范式的机制各不相同，它们在上限上的数学结构却是统一的。我们通过挑战性的推理基准（包括AIME、MATH-500和GPQA）的经验验证了我们的理论发现，展示了这些上限在测试时资源分配中的实用价值。我们希望本文能够为测试时缩放的成本效益权衡提供见解，指导大型推理模型更高效推理策略的发展。', 'title_zh': '超越缩放：探索大型推理模型的测试时缩放帕累托边界'}
{'arxiv_id': 'arXiv:2505.20521', 'title': 'Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting', 'authors': 'Ana Rita Ortigoso, Gabriel Vieira, Daniel Fuentes, Luis Frazão, Nuno Costa, António Pereira', 'link': 'https://arxiv.org/abs/2505.20521', 'abstract': "This paper presents Project Riley, a novel multimodal and multi-model conversational AI architecture oriented towards the simulation of reasoning influenced by emotional states. Drawing inspiration from Pixar's Inside Out, the system comprises five distinct emotional agents - Joy, Sadness, Fear, Anger, and Disgust - that engage in structured multi-round dialogues to generate, criticise, and iteratively refine responses. A final reasoning mechanism synthesises the contributions of these agents into a coherent output that either reflects the dominant emotion or integrates multiple perspectives. The architecture incorporates both textual and visual large language models (LLMs), alongside advanced reasoning and self-refinement processes. A functional prototype was deployed locally in an offline environment, optimised for emotional expressiveness and computational efficiency. From this initial prototype, another one emerged, called Armando, which was developed for use in emergency contexts, delivering emotionally calibrated and factually accurate information through the integration of Retrieval-Augmented Generation (RAG) and cumulative context tracking. The Project Riley prototype was evaluated through user testing, in which participants interacted with the chatbot and completed a structured questionnaire assessing three dimensions: Emotional Appropriateness, Clarity and Utility, and Naturalness and Human-likeness. The results indicate strong performance in structured scenarios, particularly with respect to emotional alignment and communicative clarity.", 'abstract_zh': '项目里利：一种面向情绪影响推理模拟的新型多模态多模型对话AI架构', 'title_zh': 'Project Riley：具有情绪推理和投票的多模态多agents LLM协作'}
{'arxiv_id': 'arXiv:2505.20466', 'title': 'Reconceptualizing Smart Microscopy: From Data Collection to Knowledge Creation by Multi-Agent Integration', 'authors': 'P.S. Kesavan, Pontus Nordenfelt', 'link': 'https://arxiv.org/abs/2505.20466', 'abstract': "Smart microscopy represents a paradigm shift in biological imaging, moving from passive observation tools to active collaborators in scientific inquiry. Enabled by advances in automation, computational power, and artificial intelligence, these systems are now capable of adaptive decision-making and real-time experimental control. Here, we introduce a theoretical framework that reconceptualizes smart microscopy as a partner in scientific investigation. Central to our framework is the concept of the 'epistemic-empirical divide' in cellular investigation-the gap between what is observable (empirical domain) and what must be understood (epistemic domain). We propose six core design principles: epistemic-empirical awareness, hierarchical context integration, an evolution from detection to perception, adaptive measurement frameworks, narrative synthesis capabilities, and cross-contextual reasoning. Together, these principles guide a multi-agent architecture designed to align empirical observation with the goals of scientific understanding. Our framework provides a roadmap for building microscopy systems that go beyond automation to actively support hypothesis generation, insight discovery, and theory development, redefining the role of scientific instruments in the process of knowledge creation.", 'abstract_zh': '智能显微镜代表了生物成像领域的一场范式转变，从被动观察工具转变为科学探究的主动合作者。得益于自动化、计算能力和人工智能的进步，这些系统现在能够进行适应性决策和实时实验控制。在这里，我们提出了一种理论框架，重新构想了智能显微镜作为科学研究的合作伙伴。我们框架的核心是细胞研究中“知识与经验的鸿沟”的概念——可观察到的领域与需要理解的领域之间的差距。我们建议六个核心设计原则：知识与经验的意识、层级上下文整合、从检测到感知的转变、适应性测量框架、叙述性综合能力和跨上下文推理。这些原则指导一个多代理架构的设计，旨在使经验观察与科学研究的目标相一致。我们的框架提供了一条道路，用于构建超越自动化、积极支持假说生成、洞察发现和理论发展的显微镜系统，重新定义了科学仪器在知识创造过程中的角色。', 'title_zh': '重新构想智能显微镜：从数据采集到知识创造的多代理集成方法'}
{'arxiv_id': 'arXiv:2505.20417', 'title': 'SCAR: Shapley Credit Assignment for More Efficient RLHF', 'authors': 'Meng Cao, Shuyuan Zhang, Xiao-Wen Chang, Doina Precup', 'link': 'https://arxiv.org/abs/2505.20417', 'abstract': 'Reinforcement Learning from Human Feedback (RLHF) is a widely used technique for aligning Large Language Models (LLMs) with human preferences, yet it often suffers from sparse reward signals, making effective credit assignment challenging. In typical setups, the reward model provides a single scalar score for an entire generated sequence, offering little insight into which token or span-level decisions were responsible for the outcome. To address this, we propose Shapley Credit Assignment Rewards (SCAR), a novel method that leverages Shapley values in cooperative game theory. SCAR distributes the total sequence-level reward among constituent tokens or text spans based on their principled marginal contributions. This creates dense reward signals, crucially, without necessitating the training of auxiliary critique models or recourse to fine-grained human annotations at intermediate generation stages. Unlike prior dense reward methods, SCAR offers a game-theoretic foundation for fair credit attribution. Theoretically, we demonstrate that SCAR preserves the original optimal policy, and empirically, across diverse tasks including sentiment control, text summarization, and instruction tuning, we show that SCAR converges significantly faster and achieves higher final reward scores compared to standard RLHF and attention-based dense reward baselines. Our findings suggest that SCAR provides a more effective and theoretically sound method for credit assignment in RLHF, leading to more efficient alignment of LLMs.', 'abstract_zh': '基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF）是一种广泛使用的 técnico 用于使大语言模型（Large Language Models, LLMs）与人类偏好对齐的方法，但往往受到稀疏奖励信号的困扰，使得有效的信用分配变得具有挑战性。在典型的设计中，奖励模型只为整个生成序列提供一个单一的标量评分，这几乎没有关于哪些标记或跨度级别的决策导致了结果的洞察。为了解决这一问题，我们提出了基于 Шapley 值的信用分配奖励（Shapley Credit Assignment Rewards, SCAR）这一新颖的方法，该方法利用合作博弈论中的 Шapley 值。SCAR 根据它们的原理性边际贡献将整体序列级别的奖励分布在组成标记或文本跨度中。这创建了密集的奖励信号，至关重要的是，而无需训练辅助批评模型或在生成过程的中间阶段依赖细致的人类标注。与先前的密集奖励方法不同，SCAR 提供了一种基于博弈论的公平信用归因基础。理论上，我们证明了 SCAR 保留了原始的最优策略；而在包括情感控制、文本摘要和指令调优在内的多种任务中，我们 empirically 展示了与标准 RLHF 和基于注意力的密集奖励基准相比，SCAR 收敛速度显著更快，并实现了更高的最终奖励评分。我们的发现表明，SCAR 提供了一种更有效且理论上更坚实的方法来进行 RLHF 中的信用分配，从而促进了大语言模型的更高效对齐。', 'title_zh': 'SCAR: Shapley.credit分配以实现更高效的RLHF'}
{'arxiv_id': 'arXiv:2505.20342', 'title': 'Machine Theory of Mind and the Structure of Human Values', 'authors': 'Paul de Font-Reaulx', 'link': 'https://arxiv.org/abs/2505.20342', 'abstract': 'Value learning is a crucial aspect of safe and ethical AI. This is primarily pursued by methods inferring human values from behaviour. However, humans care about much more than we are able to demonstrate through our actions. Consequently, an AI must predict the rest of our seemingly complex values from a limited sample. I call this the value generalization problem. In this paper, I argue that human values have a generative rational structure and that this allows us to solve the value generalization problem. In particular, we can use Bayesian Theory of Mind models to infer human values not only from behaviour, but also from other values. This has been obscured by the widespread use of simple utility functions to represent human values. I conclude that developing generative value-to-value inference is a crucial component of achieving a scalable machine theory of mind.', 'abstract_zh': '价值学习是实现安全和伦理AI的关键方面。这主要通过从行为中推断人类价值观的方法来追求。然而，人类关心的远不止我们能通过行为展现的内容。因此，AI必须从有限的样本中预测我们那些看似复杂的其余价值观。我称之为价值概括问题。在本文中，我认为人类价值观具有生成性的理性结构，这使得我们能够解决价值概括问题。特别是，我们可以使用贝叶斯理论心智模型，不仅从行为，也可以从其他价值观来推断人类价值观。这种观点因使用简单的效用函数来表示人类价值观而被掩盖。我得出结论，发展生成性价值到价值的推理是实现可扩展机器心智理论的关键组成部分。', 'title_zh': '机器心理论与人类价值观的结构'}
{'arxiv_id': 'arXiv:2505.20339', 'title': 'Challenges for artificial cognitive systems', 'authors': 'Antoni Gomila, Vincent C. Müller', 'link': 'https://arxiv.org/abs/2505.20339', 'abstract': 'The declared goal of this paper is to fill this gap: "... cognitive systems research needs questions or challenges that define progress. The challenges are not (yet more) predictions of the future, but a guideline to what are the aims and what would constitute progress." -- the quotation being from the project description of EUCogII, the project for the European Network for Cognitive Systems within which this formulation of the \'challenges\' was originally developed (this http URL). So, we stick out our neck and formulate the challenges for artificial cognitive systems. These challenges are articulated in terms of a definition of what a cognitive system is: a system that learns from experience and uses its acquired knowledge (both declarative and practical) in a flexible manner to achieve its own goals.', 'abstract_zh': '本文的目标是填补这一空白：“……认知系统研究需要能够定义进步的问题或挑战。这些挑战不是对未来的新预测，而是指导人们了解目标以及何为进步的标准。”——引自EUCogII项目描述，该项目是欧洲认知系统网络的一部分，在其中首次提出了这些“挑战”的表述（详见链接）。因此，我们提出了人工认知系统的挑战。这些挑战基于对认知系统定义的阐述：一种能够从经验中学习并在灵活运用已获知识（包括显性和实用性知识）以实现自身目标方面表现出色的系统。', 'title_zh': '人工认知系统面临的挑战'}
{'arxiv_id': 'arXiv:2505.20316', 'title': 'Reinforcement Speculative Decoding for Fast Ranking', 'authors': 'Yingpeng Du, Tianjun Wei, Zhu Sun, Jie Zhang', 'link': 'https://arxiv.org/abs/2505.20316', 'abstract': "Large Language Models (LLMs) have been widely adopted in ranking systems such as information retrieval (IR) systems and recommender systems (RSs). To alleviate the latency of auto-regressive decoding, some studies explore the single (first) token decoding for ranking approximation, but they suffer from severe degradation in tail positions. Although speculative decoding (SD) methods can be a remedy with verification at different positions, they face challenges in ranking systems due to their left-to-right decoding paradigm. Firstly, ranking systems require strict latency constraints, but verification rounds in SD methods remain agnostic; Secondly, SD methods usually discard listwise ranking knowledge about unaccepted items in previous rounds, hindering future multi-token prediction, especially when candidate tokens are the unaccepted items. In this paper, we propose a Reinforcement Speculative Decoding method for fast ranking inference of LLMs. To meet the ranking systems' latency requirement, we propose an up-to-down decoding paradigm that employs an agent to iteratively modify the ranking sequence under a constrained budget. Specifically, we design a ranking-tailored policy optimization, actively exploring optimal multi-round ranking modification policy verified by LLMs via reinforcement learning (RL). To better approximate the target LLM under the constrained budget, we trigger the agent fully utilizing the listwise ranking knowledge about all items verified by LLMs across different rounds in RL, enhancing the modification policy of the agent. More importantly, we demonstrate the theoretical robustness and advantages of our paradigm and implementation. Experiments on both IR and RS tasks show the effectiveness of our proposed method.", 'abstract_zh': '大型语言模型（LLMs）在信息检索（IR）系统和推荐系统（RS）等排名系统中被广泛采用。为了缓解自回归解码的延迟，一些研究探索了一次（首次）解码用于排名近似，但它们在尾部位置表现严重下降。虽然推测性解码（SD）方法可以通过在不同位置进行验证来改进，但由于其自左向右的解码范式，它们在排名系统中仍面临挑战。首先，排名系统需要严格的时间延迟约束，但SD方法中的验证环节仍保持无知；其次，SD方法通常会丢弃上一轮未被接受项的列表级排名知识，阻碍未来多词预测，尤其是在候选词是未被接受项时。本文提出了一种强化推测性解码方法，以加快大型语言模型的排名推理。为了满足排名系统的时间延迟要求，我们提出了一种自上而下的解码范式，通过代理迭代修改受限预算下的排名序列。具体而言，我们设计了一种针对排名优化策略，通过强化学习（RL）验证LLM检验多轮最优排名修改策略。为了在受限预算下更好地近似目标LLM，我们在RL的多轮循环中充分利用所有项目由LLM验证的列表级排名知识，增强代理的修改策略。更重要的是，我们证明了我们范式和实现的理论稳健性和优势。在信息检索和推荐系统任务上的实验表明了我们所提出方法的有效性。', 'title_zh': '加速排序的强化推测解码'}
{'arxiv_id': 'arXiv:2505.20313', 'title': 'Reasoning in Neurosymbolic AI', 'authors': "Son Tran, Edjard Mota, Artur d'Avila Garcez", 'link': 'https://arxiv.org/abs/2505.20313', 'abstract': 'Knowledge representation and reasoning in neural networks have been a long-standing endeavor which has attracted much attention recently. The principled integration of reasoning and learning in neural networks is a main objective of the area of neurosymbolic Artificial Intelligence (AI). In this chapter, a simple energy-based neurosymbolic AI system is described that can represent and reason formally about any propositional logic formula. This creates a powerful combination of learning from data and knowledge and logical reasoning. We start by positioning neurosymbolic AI in the context of the current AI landscape that is unsurprisingly dominated by Large Language Models (LLMs). We identify important challenges of data efficiency, fairness and safety of LLMs that might be addressed by neurosymbolic reasoning systems with formal reasoning capabilities. We then discuss the representation of logic by the specific energy-based system, including illustrative examples and empirical evaluation of the correspondence between logical reasoning and energy minimization using Restricted Boltzmann Machines (RBM). Learning from data and knowledge is also evaluated empirically and compared with a symbolic, neural and a neurosymbolic system. Results reported in this chapter in an accessible way are expected to reignite the research on the use of neural networks as massively-parallel models for logical reasoning and promote the principled integration of reasoning and learning in deep networks. We conclude the chapter with a discussion of the importance of positioning neurosymbolic AI within a broader framework of formal reasoning and accountability in AI, discussing the challenges for neurosynbolic AI to tackle the various known problems of reliability of deep learning.', 'abstract_zh': '神经网络中的知识表示与推理一直是长期的研究课题，近年来引起了广泛关注。神经符号人工智能（AI）领域的主要目标是原理性地将推理与学习集成在神经网络中。在本章中，描述了一个简单的基于能量的神经符号AI系统，它可以形式化地表示和推理任何命题逻辑公式。这创造了一种强大的组合——从数据和知识中学习以及进行逻辑推理。我们首先将神经符号AI置于当前AI panorama中，该领域不令人意外地被大型语言模型（LLMs）所主导。我们识别了大型语言模型的数据效率、公平性和安全性等重要挑战，这些问题可能通过具备形式推理能力的神经符号推理系统来解决。然后，我们讨论了该特定能量基系统对逻辑的表示，包括逻辑推理与能量最小化的对应关系示例以及限制玻尔兹曼机（RBM）的实验评估。从数据和知识中进行学习的实验评估也进行了比较，包括符号、神经和神经符号系统。本章以易于理解的方式报告的结果预计将重新激发对使用神经网络作为大规模并行逻辑推理模型的研究，并促进深层网络中推理与学习的原理性集成。最后，我们在更广泛的形式推理和AI问责框架中讨论了神经符号AI的重要性，并探讨了神经符号AI面临的挑战，以应对深度学习可靠性方面的各种已知问题。', 'title_zh': '神经符号人工智能中的推理'}
{'arxiv_id': 'arXiv:2505.20310', 'title': 'Manalyzer: End-to-end Automated Meta-analysis with Multi-agent System', 'authors': 'Wanghan Xu, Wenlong Zhang, Fenghua Ling, Ben Fei, Yusong Hu, Fangxuan Ren, Jintai Lin, Wanli Ouyang, Lei Bai', 'link': 'https://arxiv.org/abs/2505.20310', 'abstract': 'Meta-analysis is a systematic research methodology that synthesizes data from multiple existing studies to derive comprehensive conclusions. This approach not only mitigates limitations inherent in individual studies but also facilitates novel discoveries through integrated data analysis. Traditional meta-analysis involves a complex multi-stage pipeline including literature retrieval, paper screening, and data extraction, which demands substantial human effort and time. However, while LLM-based methods can accelerate certain stages, they still face significant challenges, such as hallucinations in paper screening and data extraction. In this paper, we propose a multi-agent system, Manalyzer, which achieves end-to-end automated meta-analysis through tool calls. The hybrid review, hierarchical extraction, self-proving, and feedback checking strategies implemented in Manalyzer significantly alleviate these two hallucinations. To comprehensively evaluate the performance of meta-analysis, we construct a new benchmark comprising 729 papers across 3 domains, encompassing text, image, and table modalities, with over 10,000 data points. Extensive experiments demonstrate that Manalyzer achieves significant performance improvements over the LLM baseline in multi meta-analysis tasks. Project page: this https URL .', 'abstract_zh': '元分析是一种系统的研究方法，通过合成多个现有研究的数据以得出全面的结论。该方法不仅减轻了个别研究固有的局限性，还能通过综合数据分析促进新的发现。传统元分析涉及复杂的多阶段管道，包括文献检索、论文筛选和数据提取，这需要大量的人力和时间。然而，尽管基于LLM的方法可以加速某些阶段，但在论文筛选和数据提取中仍然面临显著挑战，如幻觉。在本文中，我们提出了一种多代理系统Manalyzer，通过工具调用实现了端到端自动化的元分析。Manalyzer中实现的混合评审、分层提取、自我证明和反馈检查策略大大减轻了这两种幻觉。为了全面评估元分析的性能，我们构建了一个包含729篇论文的新基准，涵盖了3个领域，包括文本、图像和表格模态，数据点超过10,000个。广泛的实验表明，Manalyzer在多元分析任务中的性能显著优于LLM基线。项目页面：this https URL。', 'title_zh': 'Manalyzer: 基于多agent系统的端到端自动化元分析'}
{'arxiv_id': 'arXiv:2505.20306', 'title': 'Multi-Modal Artificial Intelligence of Embryo Grading and Pregnancy Prediction in Assisted Reproductive Technology: A Review', 'authors': 'Xueqiang Ouyang, Jia Wei', 'link': 'https://arxiv.org/abs/2505.20306', 'abstract': 'As a global disease, infertility has always affected human beings. The development of assisted reproductive technology can effectively solve this disease. However, the traditional in vitro fertilization-embryo transfer technology still faces many challenges in improving the success rate of pregnancy, such as the subjectivity of embryo grading and the inefficiency of integrating multi-modal data. Therefore, the introduction of artificial intelligence-based technologies is particularly crucial. This article reviews the application progress of multi-modal artificial intelligence in embryo grading and pregnancy prediction based on different data modalities (including static images, time-lapse videos and structured table data) from a new perspective, and discusses the main challenges in current research, such as the complexity of multi-modal information fusion and data scarcity.', 'abstract_zh': '基于多模态人工智能的胚胎分级与妊娠预测进展及挑战', 'title_zh': '辅助生殖技术中多模态人工智能在胚胎分级和妊娠预测中的应用：一个综述'}
{'arxiv_id': 'arXiv:2505.21505', 'title': "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective", 'authors': 'Shimao Zhang, Zhejian Lai, Xiang Liu, Shuaijie She, Xiao Liu, Yeyun Gong, Shujian Huang, Jiajun Chen', 'link': 'https://arxiv.org/abs/2505.21505', 'abstract': "Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides a new perspective to analyze and understand LLMs' mechanisms more specifically in multilingual scenarios. In this work, we propose a new finer-grained neuron identification algorithm, which detects language neurons~(including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs.", 'abstract_zh': '多语言对齐是提升大规模语言模型多语言能力的有效且具代表性的范式，能够将高资源语言的能力转移到低资源语言。同时，关于语言特异性神经元的一些研究表明，大规模语言模型在处理不同语言时会激活语言特异性神经元。这为在多语言场景下更具体地分析和理解模型的机制提供了新的视角。在本文中，我们提出了一种新的更细粒度的神经元识别算法，能够检测语言神经元（包括语言特异性神经元和语言相关神经元）和语言无关神经元。此外，基于不同类型神经元的分布特性，我们将大规模语言模型在多语言推理过程中的内部处理分为四部分：（1）多语言理解，（2）共享语义空间推理，（3）多语言输出空间转换，（4）词汇空间输出。另外，我们系统地分析了对齐前后模型，重点关注不同类型神经元的变化。我们还分析了“自发多语言对齐”的现象。总体而言，我们基于不同类型神经元进行了全面的研究，提供了多语言对齐和大规模语言模型多语言能力的实证结果和有价值的见解。', 'title_zh': '如何实现增强大语言模型的多语言能力？从语言神经元视角探究'}
{'arxiv_id': 'arXiv:2505.21503', 'title': 'Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making', 'authors': 'Yihan Wang, Qiao Yan, Zhenghao Xing, Lihao Liu, Junjun He, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng', 'link': 'https://arxiv.org/abs/2505.21503', 'abstract': "Large language models (LLMs) have demonstrated strong potential in clinical question answering, with recent multi-agent frameworks further improving diagnostic accuracy via collaborative reasoning. However, we identify a recurring issue of Silent Agreement, where agents prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases. We present a new concept called Catfish Agent, a role-specialized LLM designed to inject structured dissent and counter silent agreement. Inspired by the ``catfish effect'' in organizational psychology, the Catfish Agent is designed to challenge emerging consensus to stimulate deeper reasoning. We formulate two mechanisms to encourage effective and context-aware interventions: (i) a complexity-aware intervention that modulates agent engagement based on case difficulty, and (ii) a tone-calibrated intervention articulated to balance critique and collaboration. Evaluations on nine medical Q&A and three medical VQA benchmarks show that our approach consistently outperforms both single- and multi-agent LLMs frameworks, including leading commercial models such as GPT-4o and DeepSeek-R1.", 'abstract_zh': '大型语言模型（LLMs）在临床问答中展示了强大的潜能，近期的多智能体框架进一步通过协作推理提高了诊断准确性。然而，我们发现一个反复出现的问题，即沉默共识，智能体在缺乏充分批判分析的情况下提前达成诊断一致，尤其是在复杂或模糊的情况下。我们提出了一种新的概念，名为猫舌智能体（Catfish Agent），这是一种专门化的设计，旨在注入结构化的抵制，打破沉默共识。受组织心理学中的“鲶鱼效应”启发，猫舌智能体旨在挑战正在形成的共识，以激发更深层次的推理。我们提出了两种机制来促进有效且情境感知的干预：（i）复杂性感知干预，根据案例难度调整智能体的参与度，以及（ii）语调校准干预，旨在平衡批判与合作。在九个医疗问答和三个医疗视觉问答基准测试上的评估表明，我们的方法在多方面均超越了单一智能体和多智能体LLM框架，包括领先的商业模型如GPT-4o和DeepSeek-R1。', 'title_zh': '沉默不等于共识：通过鱼线剂干扰共识偏差以颠覆多agent大语言模型在临床决策中的一致性偏好'}
{'arxiv_id': 'arXiv:2505.21500', 'title': 'ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models', 'authors': 'Dingming Li, Hongxing Li, Zixuan Wang, Yuchen Yan, Hang Zhang, Siqi Chen, Guiyang Hou, Shengpei Jiang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang', 'link': 'https://arxiv.org/abs/2505.21500', 'abstract': "Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities.", 'abstract_zh': 'Vision-language模型（VLMs）在理解和推理视觉内容方面展示了显著的能力，但在要求跨视角理解和空间推理的任务中仍面临重大挑战。我们识别出一个关键限制：当前的VLMs 主要擅长以自我中心的空间推理（从相机的角度），但在需要采用其他实体的空间参照框架时无法有效泛化到他觉中心视角。我们引入了ViewSpatial-Bench，这是第一个专为跨视角空间定位识别评估设计的综合性基准，支持自动化的3D注释流水线以生成精确的方向标签。对ViewSpatial-Bench上各种VLMs的全面评估揭示了一个显著的性能差异：模型在以相机视角进行任务时表现出合理的性能，但在从人类视角进行推理时却表现出较低的准确性。通过在我们多元视角空间数据集上微调VLMs，我们实现了跨任务总体性能提升46.24%，突显了我们方法的有效性。我们的工作为体态人工智能系统中的空间智能建立了关键基准，并提供了实验证据，表明建模3D空间关系能够增强VLMs相应的空间理解能力。', 'title_zh': 'ViewSpatial-Bench: 评估视觉语言模型的多视角空间定位能力'}
{'arxiv_id': 'arXiv:2505.21499', 'title': 'AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery', 'authors': 'Haowei Wang, Junjie Wang, Xiaojun Jia, Rupeng Zhang, Mingyang Li, Zhe Liu, Yang Liu, Qing Wang', 'link': 'https://arxiv.org/abs/2505.21499', 'abstract': "Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at this https URL.", 'abstract_zh': '基于视觉语言模型的网络代理中的黑盒注入攻击：利用互联网广告投放注入恶意内容', 'title_zh': 'AdInject：通过广告分发对网络代理进行现实世界的黑盒攻击'}
{'arxiv_id': 'arXiv:2505.21497', 'title': 'Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers', 'authors': 'Wei Pang, Kevin Qinghong Lin, Xiangru Jian, Xi He, Philip Torr', 'link': 'https://arxiv.org/abs/2505.21497', 'abstract': "Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at this https URL.", 'abstract_zh': '学术海报生成是科学研究交流中一个关键但具有挑战性的任务，要求将长篇交织文档压缩至单页且视觉统一的页面。为应对这一挑战，我们首次引入了用于海报生成的标准基准及评价体系，该体系将近期的会议论文与作者设计的海报配对，并从(i)视觉质量-与人类海报的语义对齐、(ii)文本连贯性-语言流畅度、(iii)整体评估-由VLM作为评判的六项细致美学和信息标准评分，以及(iv)PaperQuiz-通过VLM回答生成的测验题来衡量海报传达论文核心内容的能力等方面进行评估。基于此基准，我们提出了PosterAgent，一个自上而下的、视觉循环的多智能体管道：(a) 解析器将论文提炼为结构化资产库；(b) 计划者将文本-视觉配对排列成二叉树布局，保持阅读顺序和空间平衡；以及(c) 画家-评论者循环通过执行渲染代码并利用VLM反馈精炼每个分区，消除溢出并确保对齐。在我们全面的评估中，我们发现GPT-4o的输出虽然初看视觉上吸引人，但往往表现出嘈杂的文本和较低的PaperQuiz评分，我们发现读者参与度是主要的美学瓶颈，因为人类设计的海报很大程度上依赖于视觉语义来传达意义。我们完全开源的变体（如基于Qwen-2.5系列）几乎在所有指标上优于现有的4o驱动的多智能体系统，同时使用87%更少的tokens。它将一篇22页的论文转化为一个可编辑的.pptx格式的最终海报，仅需0.005美元。这些发现为下一代全自动海报生成模型指明了明确的方向。编码和数据集可在此处访问。', 'title_zh': 'Paper2Poster: 向科学论文的多模态海报自动化转换'}
{'arxiv_id': 'arXiv:2505.21488', 'title': 'Be Decisive: Noise-Induced Layouts for Multi-Subject Generation', 'authors': 'Omer Dahary, Yehonathan Cohen, Or Patashnik, Kfir Aberman, Daniel Cohen-Or', 'link': 'https://arxiv.org/abs/2505.21488', 'abstract': "Generating multiple distinct subjects remains a challenge for existing text-to-image diffusion models. Complex prompts often lead to subject leakage, causing inaccuracies in quantities, attributes, and visual features. Preventing leakage among subjects necessitates knowledge of each subject's spatial location. Recent methods provide these spatial locations via an external layout control. However, enforcing such a prescribed layout often conflicts with the innate layout dictated by the sampled initial noise, leading to misalignment with the model's prior. In this work, we introduce a new approach that predicts a spatial layout aligned with the prompt, derived from the initial noise, and refines it throughout the denoising process. By relying on this noise-induced layout, we avoid conflicts with externally imposed layouts and better preserve the model's prior. Our method employs a small neural network to predict and refine the evolving noise-induced layout at each denoising step, ensuring clear boundaries between subjects while maintaining consistency. Experimental results show that this noise-aligned strategy achieves improved text-image alignment and more stable multi-subject generation compared to existing layout-guided techniques, while preserving the rich diversity of the model's original distribution.", 'abstract_zh': '现有的文本到图像扩散模型在生成多个独立主题方面仍面临挑战。复杂的提示 often lead to subject leakage, causing inaccuracies in quantities, attributes, and visual features. 预防主题之间的泄漏需要了解每个主题的空间位置。最近的方法通过外部布局控制提供这些空间位置。然而，强制这种预定布局往往会与采样初始噪声所决定的固有布局发生冲突，导致与模型先验的对齐偏差。在本文中，我们提出了一种新方法，该方法预测与提示、初始噪声相一致的空间布局，并在整个去噪过程中对其进行 refinement。通过依赖这种噪声诱导的布局，我们避免了外部强加布局的冲突，更好地保留了模型的先验。该方法使用一个小型神经网络，在每次去噪步骤中预测并细化 evolving noise-induced layout，确保各主题之间的清晰边界，同时保持一致性。实验结果表明，这种噪声对齐策略在文本图像对齐和多主题生成的稳定性方面优于现有布局引导技术，同时保留了模型原始分布的丰富多样性。', 'title_zh': '果断行动：噪声诱导的多主题生成布局'}
{'arxiv_id': 'arXiv:2505.21478', 'title': 'Policy Optimized Text-to-Image Pipeline Design', 'authors': 'Uri Gadot, Rinon Gal, Yftah Ziser, Gal Chechik, Shie Mannor', 'link': 'https://arxiv.org/abs/2505.21478', 'abstract': 'Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines.', 'abstract_zh': '基于文本到图像生成的新型强化学习框架：克服计算密集性和泛化不足的问题', 'title_zh': '优化策略文本到图像流水线设计'}
{'arxiv_id': 'arXiv:2505.21459', 'title': 'LazyVLM: Neuro-Symbolic Approach to Video Analytics', 'authors': 'Xiangru Jian, Wei Pang, Zhengyuan Dong, Chao Zhang, M. Tamer Özsu', 'link': 'https://arxiv.org/abs/2505.21459', 'abstract': 'Current video analytics approaches face a fundamental trade-off between flexibility and efficiency. End-to-end Vision Language Models (VLMs) often struggle with long-context processing and incur high computational costs, while neural-symbolic methods depend heavily on manual labeling and rigid rule design. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics system that provides a user-friendly query interface similar to VLMs, while addressing their scalability limitation. LazyVLM enables users to effortlessly drop in video data and specify complex multi-frame video queries using a semi-structured text interface for video analytics. To address the scalability limitations of VLMs, LazyVLM decomposes multi-frame video queries into fine-grained operations and offloads the bulk of the processing to efficient relational query execution and vector similarity search. We demonstrate that LazyVLM provides a robust, efficient, and user-friendly solution for querying open-domain video data at scale.', 'abstract_zh': '当前的视频分析方法在灵活性和效率之间存在根本性的权衡。端到端的视觉语言模型（VLMs）常常难以处理长序列处理，并产生较高的计算成本，而神经符号方法则依赖于手动标注和严格的规则设计。本文介绍了LazyVLM，这是一种神经符号视频分析系统，提供了类似于VLMs的用户友好查询接口，同时解决了它们的可扩展性限制。LazyVLM使用户能够轻松地导入视频数据，并使用半结构化文本接口指定复杂的多帧视频查询。为了应对VLMs的可扩展性限制，LazyVLM将多帧视频查询分解为细粒度操作，并将大部分处理工作卸载到高效的关联查询执行和向量相似性搜索。我们证明，LazyVLM提供了一种 robust、高效且用户友好的解决方案，用于在大规模查询开放域视频数据。', 'title_zh': 'LazyVLM: 神经符号视频分析方法'}
{'arxiv_id': 'arXiv:2505.21457', 'title': 'Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO', 'authors': 'Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, Chunhua Shen', 'link': 'https://arxiv.org/abs/2505.21457', 'abstract': "Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.", 'abstract_zh': '基于多模态大规模语言模型的主动视觉：一种基于GRPO的纯强化学习训练框架', 'title_zh': 'Active-O3: 通过GRPO增强多模态大型语言模型的主动感知能力'}
{'arxiv_id': 'arXiv:2505.21445', 'title': 'VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin', 'authors': 'Zhiqi Ai, Meixuan Bao, Zhiyong Chen, Zhi Yang, Xinnuo Li, Shugong Xu', 'link': 'https://arxiv.org/abs/2505.21445', 'abstract': 'The performance of speaker verification systems is adversely affected by speaker aging. However, due to challenges in data collection, particularly the lack of sustained and large-scale longitudinal data for individuals, research on speaker aging remains difficult. In this paper, we present VoxAging, a large-scale longitudinal dataset collected from 293 speakers (226 English speakers and 67 Mandarin speakers) over several years, with the longest time span reaching 17 years (approximately 900 weeks). For each speaker, the data were recorded at weekly intervals. We studied the phenomenon of speaker aging and its effects on advanced speaker verification systems, analyzed individual speaker aging processes, and explored the impact of factors such as age group and gender on speaker aging research.', 'abstract_zh': 'speaker验证系统受说话人老化影响的研究——基于VoxAging大规模 longitudinal数据集', 'title_zh': 'VoxAging：大规模 longitudinal 数据集中文英文 Continuous 推进说话人老化跟踪'}
{'arxiv_id': 'arXiv:2505.21441', 'title': 'Autoencoding Random Forests', 'authors': 'Binh Duc Vu, Jan Kapar, Marvin Wright, David S. Watson', 'link': 'https://arxiv.org/abs/2505.21441', 'abstract': "We propose a principled method for autoencoding with random forests. Our strategy builds on foundational results from nonparametric statistics and spectral graph theory to learn a low-dimensional embedding of the model that optimally represents relationships in the data. We provide exact and approximate solutions to the decoding problem via constrained optimization, split relabeling, and nearest neighbors regression. These methods effectively invert the compression pipeline, establishing a map from the embedding space back to the input space using splits learned by the ensemble's constituent trees. The resulting decoders are universally consistent under common regularity assumptions. The procedure works with supervised or unsupervised models, providing a window into conditional or joint distributions. We demonstrate various applications of this autoencoder, including powerful new tools for visualization, compression, clustering, and denoising. Experiments illustrate the ease and utility of our method in a wide range of settings, including tabular, image, and genomic data.", 'abstract_zh': '我们提出了一种基于随机森林的自动编码方法。该策略构建于非参数统计和谱图理论的基础结果之上，用于学习一个低维嵌入，该嵌入优化地表示数据中的关系。我们通过受限优化、分裂重新标记和最近邻回归提供了解码问题的精确和近似解。这些方法有效地逆向了压缩管道，使用集成中个体树学习到的分裂，在嵌入空间与输入空间之间建立映射。所得到的解码器在常见的正则性假设下具有普遍一致性。该过程既可以用于监督模型也可以用于无监督模型，提供了条件分布或联合分布的窗口。我们展示了该自动编码器的各种应用，包括用于可视化、压缩、聚类和去噪的强有力新工具。实验在包括表格数据、图像数据和基因组数据在内的多种场景下表明了我们方法的便捷性和实用性。', 'title_zh': '自动编码随机森林'}
{'arxiv_id': 'arXiv:2505.21432', 'title': 'Hume: Introducing System-2 Thinking in Visual-Language-Action Model', 'authors': 'Haoming Song, Delin Qu, Yuanqi Yao, Qizhi Chen, Qi Lv, Yiwen Tang, Modi Shi, Guanghui Ren, Maoqing Yao, Bin Zhao, Dong Wang, Xuelong Li', 'link': 'https://arxiv.org/abs/2505.21432', 'abstract': 'Humans practice slow thinking before performing actual actions when handling complex tasks in the physical world. This thinking paradigm, recently, has achieved remarkable advancement in boosting Large Language Models (LLMs) to solve complex tasks in digital domains. However, the potential of slow thinking remains largely unexplored for robotic foundation models interacting with the physical world. In this work, we propose Hume: a dual-system Vision-Language-Action (VLA) model with value-guided System-2 thinking and cascaded action denoising, exploring human-like thinking capabilities of Vision-Language-Action models for dexterous robot control. System 2 of Hume implements value-Guided thinking by extending a Vision-Language-Action Model backbone with a novel value-query head to estimate the state-action value of predicted actions. The value-guided thinking is conducted by repeat sampling multiple action candidates and selecting one according to state-action value. System 1 of Hume is a lightweight reactive visuomotor policy that takes System 2 selected action and performs cascaded action denoising for dexterous robot control. At deployment time, System 2 performs value-guided thinking at a low frequency while System 1 asynchronously receives the System 2 selected action candidate and predicts fluid actions in real time. We show that Hume outperforms the existing state-of-the-art Vision-Language-Action models across multiple simulation benchmark and real-robot deployments.', 'abstract_zh': '人类在处理物理世界中的复杂任务时会先进行慢思考再采取实际行动。这种思维范式近期已在增强大型语言模型解决数字领域复杂任务方面取得了显著进展。然而，慢思考在与物理世界互动的机器人基础模型中的潜力尚未被充分探索。在本文中，我们提出Hume：一种具有价值导向的System-2思考和级联动作去噪的双系统视觉-语言-动作（VLA）模型，旨在探索视觉-语言-动作模型在灵巧机器人控制中的类人类思维能力。Hume的System 2通过扩展视觉-语言-动作模型骨干网络并添加一个新颖的价值查询头来实现价值导向的思考，以估算预测动作的状态-动作值。价值导向的思考通过多次采样动作候选并根据状态-动作值选择一个动作来实现。Hume的System 1是一个轻量级的反应式视知觉运动策略，它接受System 2选择的动作候选并进行级联动作去噪，以实现灵巧机器人控制。在部署阶段，System 2以低频率执行价值导向的思考，而System 1异步接收System 2选择的动作候选并实时预测连贯动作。实验表明，Hume在多个仿真基准测试和实际机器人部署中优于现有的最先进的视觉-语言-动作模型。', 'title_zh': '休姆：在视觉-语言-行动模型中引入系统二型思考'}
{'arxiv_id': 'arXiv:2505.21420', 'title': 'Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning', 'authors': 'Jinbao Wang, Hanzhe Liang, Can Gao, Chenxi Hu, Jie Zhou, Yunkang Cao, Linlin Shen, Weiming Shen', 'link': 'https://arxiv.org/abs/2505.21420', 'abstract': 'Multimodal feature reconstruction is a promising approach for 3D anomaly detection, leveraging the complementary information from dual modalities. We further advance this paradigm by utilizing multi-modal mentor learning, which fuses intermediate features to further distinguish normal from feature differences. To address these challenges, we propose a novel method called Mentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared features of different modalities, Mentor3AD can extract more effective features and guide feature reconstruction, ultimately improving detection performance. Specifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges features extracted from RGB and 3D modalities to create a mentor feature. Additionally, we have designed a Mentor of Guidance Module (MGM) to facilitate cross-modal reconstruction, supported by the mentor feature. Lastly, we introduce a Voting Module (VM) to more accurately generate the final anomaly score. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies have verified the effectiveness of the proposed method.', 'abstract_zh': '多模态特征重建是三维异常检测的一个有前景的方法，通过利用双模态互补信息。我们进一步通过多模态导师学习推进这一范式，该方法融合中间特征以更好地区分异常和特征差异。为了应对这些挑战，我们提出了一种名为Mentor3AD的新方法，该方法利用多模态导师学习。通过利用不同模态的共享特征，Mentor3AD可以提取更有效的特征并指导特征重建，最终提高检测性能。具体来说，Mentor3AD包括一个融合模块的导师模块（MFM），该模块将从RGB和3D模态中提取的特征合并以生成导师特征。此外，我们设计了一个指导模块的导师模块（MGM）以支持跨模态重建，由导师特征辅助。最后，我们引入了一个投票模块（VM）以更准确地生成最终的异常分数。在MVTec 3D-AD和Eyecandies上的广泛比较和消融研究证实了所提方法的有效性。', 'title_zh': 'Mentor3AD: 基于特征重构的多模态导师学习三维异常检测'}
{'arxiv_id': 'arXiv:2505.21414', 'title': 'A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment', 'authors': 'Brett Bissey, Kyle Gatesman, Walker Dimon, Mohammad Alam, Luis Robaina, Joseph Weissman', 'link': 'https://arxiv.org/abs/2505.21414', 'abstract': 'This paper introduces a comprehensive framework designed to analyze and secure decision-support systems trained with Deep Reinforcement Learning (DRL), prior to deployment, by providing insights into learned behavior patterns and vulnerabilities discovered through simulation. The introduced framework aids in the development of precisely timed and targeted observation perturbations, enabling researchers to assess adversarial attack outcomes within a strategic decision-making context. We validate our framework, visualize agent behavior, and evaluate adversarial outcomes within the context of a custom-built strategic game, CyberStrike. Utilizing the proposed framework, we introduce a method for systematically discovering and ranking the impact of attacks on various observation indices and time-steps, and we conduct experiments to evaluate the transferability of adversarial attacks across agent architectures and DRL training algorithms. The findings underscore the critical need for robust adversarial defense mechanisms to protect decision-making policies in high-stakes environments.', 'abstract_zh': '本文提出了一种全面框架，旨在在部署前分析和保障基于深度强化学习(DRL)训练的决策支持系统，通过模拟提供对学习行为模式和发现的漏洞的洞见。该引入的框架有助于开发精确的时间和目标观察扰动，使研究人员能够在战略决策背景下评估对抗攻击的效果。我们在自建的战略游戏CyberStrike中验证了该框架，可视化了智能体的行为，并评估了对抗攻击的结果。利用提出的框架，我们介绍了一种系统地发现和排列攻击对不同观察指标和时间步影响的方法，并进行了实验以评估对抗攻击在智能体架构和DRL训练算法之间的可移植性。研究结果强调了在高危环境中保护决策策略的抗逆向防御机制的迫切需要。', 'title_zh': '决策支持系统部署前的对抗分析框架'}
{'arxiv_id': 'arXiv:2505.21413', 'title': 'RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation', 'authors': 'Xiao Liu, Da Yin, Zirui Wu, Yansong Feng', 'link': 'https://arxiv.org/abs/2505.21413', 'abstract': "Tools enhance the reasoning capabilities of large language models (LLMs) in complex problem-solving tasks, but not all tasks have available tools. In the absence of predefined tools, prior works have explored instructing LLMs to generate tools on their own. However, such approaches rely heavily on the models' internal knowledge and would fail in domains beyond the LLMs' knowledge scope. To address this limitation, we propose RefTool, a reference-guided framework for automatic tool creation that leverages structured external materials such as textbooks. RefTool consists of two modules: (1) tool creation, where LLMs generate executable tools from reference content, validate them using illustrative examples, and organize them hierarchically into a toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to select and apply the appropriate tools to solve problems. Experiments on causality, physics, and chemistry benchmarks demonstrate that RefTool outperforms existing tool-creation and domain-specific reasoning methods by 11.3% on average accuracy, while being cost-efficient and broadly generalizable. Analyses reveal that grounding tool creation in references produces accurate and faithful tools, and that the hierarchical structure facilitates effective tool selection. RefTool enables LLMs to overcome knowledge limitations, demonstrating the value of grounding tool creation in external references for enhanced and generalizable reasoning.", 'abstract_zh': '工具增强大型语言模型在复杂问题解决任务中的推理能力，但在缺乏预定义工具的情况下， prior 工作探索了指导大型语言模型自动生成工具的方法。然而，此类方法依赖于模型的内部知识，在超出模型知识范围的领域将失效。为解决这一局限，我们提出了 RefTool，一种基于参考的自动工具创建框架，利用结构化的外部材料如教材。RefTool 包含两个模块：(1) 工具创建，其中大型语言模型从参考内容生成可执行工具，使用示例验证它们，并将它们分层组织到工具箱中；(2) 工具使用，其中大型语言模型导航工具箱结构以选择并应用于解决问题的适当工具。在因果关系、物理和化学基准测试上的实验表明，RefTool 在平均准确率上比现有工具创建方法和特定领域推理方法高出 11.3%，且成本效益高，通用性强。分析表明，基于参考的工具创建产生了准确且忠实的工具，并且分层结构有助于有效的工具选择。RefTool 使大型语言模型能够克服知识限制，展示了在外部分参考中接地工具创建对于增强和通用可泛化的推理的价值。', 'title_zh': 'RefTool: 以引用为导向的工具创建增强模型推理'}
{'arxiv_id': 'arXiv:2505.21409', 'title': 'RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models', 'authors': 'Dario Satriani, Enzo Veltri, Donatello Santoro, Paolo Papotti', 'link': 'https://arxiv.org/abs/2505.21409', 'abstract': "Factuality in Large Language Models (LLMs) is a persistent challenge. Current benchmarks often assess short factual answers, overlooking the critical ability to generate structured, multi-record tabular outputs from parametric knowledge. We demonstrate that this relational fact retrieval is substantially more difficult than isolated point-wise queries, even when individual facts are known to the model, exposing distinct failure modes sensitive to output dimensionality (e.g., number of attributes or records). To systematically evaluate this under-explored capability, we introduce RelationalFactQA, a new benchmark featuring diverse natural language questions (paired with SQL) and gold-standard tabular answers, specifically designed to assess knowledge retrieval in a structured format. RelationalFactQA enables analysis across varying query complexities, output sizes, and data characteristics. Our experiments reveal that even state-of-the-art LLMs struggle significantly, not exceeding 25% factual accuracy in generating relational outputs, with performance notably degrading as output dimensionality increases. These findings underscore critical limitations in current LLMs' ability to synthesize structured factual knowledge and establish RelationalFactQA as a crucial resource for measuring future progress in LLM factuality.", 'abstract_zh': '大型语言模型中事实性是一个持久性的挑战。当前的基准测试常常评估简短的事实性答案，忽视了从参数化知识生成结构化、多记录表格输出的关键能力。我们证明，这种关系事实检索比孤立的事实查询要困难得多，即使模型知道个别事实也是如此，揭示了对输出维度（如属性数量或记录数量）敏感的不同失败模式。为了系统性地评估这一未充分探索的能力，我们引入了RelationalFactQA，这是一个新的基准测试，包含多样化的自然语言问题（配以SQL）和黄金标准表格答案，专门设计用于评估知识以结构化形式检索的能力。RelationalFactQA 允许跨不同查询复杂度、输出大小和数据特征进行分析。我们的实验表明，即使是最先进的大型语言模型也面临明显困难，生成关系性输出的事实准确性不超过25%，随着输出维度的增加，性能显著下降。这些发现强调了现有大型语言模型在合成结构化事实知识方面存在的关键局限性，并将RelationalFactQA 确立为衡量未来大型语言模型事实性进展的关键资源。', 'title_zh': '关系事实问答：评估大型语言模型从表格中检索事实的基准'}
{'arxiv_id': 'arXiv:2505.21399', 'title': 'Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling', 'authors': 'Hovhannes Tamoyan, Subhabrata Dutta, Iryna Gurevych', 'link': 'https://arxiv.org/abs/2505.21399', 'abstract': "Factual incorrectness in generated content is one of the primary concerns in ubiquitous deployment of large language models (LLMs). Prior findings suggest LLMs can (sometimes) detect factual incorrectness in their generated content (i.e., fact-checking post-generation). In this work, we provide evidence supporting the presence of LLMs' internal compass that dictate the correctness of factual recall at the time of generation. We demonstrate that for a given subject entity and a relation, LLMs internally encode linear features in the Transformer's residual stream that dictate whether it will be able to recall the correct attribute (that forms a valid entity-relation-attribute triplet). This self-awareness signal is robust to minor formatting variations. We investigate the effects of context perturbation via different example selection strategies. Scaling experiments across model sizes and training dynamics highlight that self-awareness emerges rapidly during training and peaks in intermediate layers. These findings uncover intrinsic self-monitoring capabilities within LLMs, contributing to their interpretability and reliability.", 'abstract_zh': '生成内容中的事实不正确性是大规模语言模型（LLMs）广泛应用的主要关切之一。前期研究显示，LLMs可以在生成内容之后（即后生成事实核查）检测事实不正确性。本文提供了证据支持LLMs在生成时内部存在指导事实回忆正确性的“内在指南针”。我们表明，对于给定的主题实体和关系，LLMs在Transformer的残差流中内部编码线性特征，这些特征决定了它能否回忆正确的属性（构成有效的实体-关系-属性三元组）。这种自我意识信号对轻微的格式变化具有鲁棒性。我们通过不同的示例选择策略研究了上下文扰动的效果。模型规模和训练动态下的扩展实验表明，自我意识在训练过程中迅速出现，并在中间层达到峰值。这些发现揭示了LLMs内在的自我监测能力，有助于提高其可解释性和可靠性。', 'title_zh': '语言模型的实事自意识：表示、稳健性与扩展性'}
{'arxiv_id': 'arXiv:2505.21396', 'title': 'Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science', 'authors': 'Xiao Liu, Xinyi Dong, Xinyang Gao, Yansong Feng, Xun Pang', 'link': 'https://arxiv.org/abs/2505.21396', 'abstract': 'Recent advancements in large language models (LLMs) have shown promise in generating novel research ideas. However, these ideas often face challenges related to feasibility and expected effectiveness. This paper explores how augmenting LLMs with relevant data during the idea generation process can enhance the quality of generated ideas. We introduce two ways of incorporating data: (1) providing metadata during the idea generation stage to guide LLMs toward feasible directions, and (2) adding automatic validation during the idea selection stage to assess the empirical plausibility of hypotheses within ideas. We conduct experiments in the social science domain, specifically with climate negotiation topics, and find that metadata improves the feasibility of generated ideas by 20%, while automatic validation improves the overall quality of selected ideas by 7%. A human study shows that LLM-generated ideas, along with their related data and validation processes, inspire researchers to propose research ideas with higher quality. Our work highlights the potential of data-driven research idea generation, and underscores the practical utility of LLM-assisted ideation in real-world academic settings.', 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）在生成新颖研究思路方面展现了潜力，然而这些思路常常面临可行性以及预期效果的挑战。本文探讨了在想法生成过程中引入相关数据如何提升生成想法的质量。我们提出了两种引入数据的方法：（1）在想法生成阶段提供元数据以引导LLMs朝着可行的方向发展，（2）在想法选择阶段添加自动验证，以评估想法中假设的经验合理性。我们在社会科学研究领域，特别针对气候变化谈判主题，进行了实验，发现元数据可将生成想法的可行性提高20%，而自动验证可将选中想法的整体质量提高7%。一项人类研究表明，结合LLM生成的想法及相关数据和验证过程能激发研究人员提出高质量的研究思路。我们的研究突显了数据驱动研究思路生成的潜力，并强调了在实际学术环境中使用LLM辅助构思的实用价值。', 'title_zh': '通过数据提升研究构想生成：社会科学中的实证调查'}
{'arxiv_id': 'arXiv:2505.21393', 'title': 'Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits', 'authors': 'Maoli Liu, Zhuohua Li, Xiangxiang Dai, John C.S. Lui', 'link': 'https://arxiv.org/abs/2505.21393', 'abstract': 'Conversational recommender systems proactively query users with relevant "key terms" and leverage the feedback to elicit users\' preferences for personalized recommendations. Conversational contextual bandits, a prevalent approach in this domain, aim to optimize preference learning by balancing exploitation and exploration. However, several limitations hinder their effectiveness in real-world scenarios. First, existing algorithms employ key term selection strategies with insufficient exploration, often failing to thoroughly probe users\' preferences and resulting in suboptimal preference estimation. Second, current algorithms typically rely on deterministic rules to initiate conversations, causing unnecessary interactions when preferences are well-understood and missed opportunities when preferences are uncertain. To address these limitations, we propose three novel algorithms: CLiSK, CLiME, and CLiSK-ME. CLiSK introduces smoothed key term contexts to enhance exploration in preference learning, CLiME adaptively initiates conversations based on preference uncertainty, and CLiSK-ME integrates both techniques. We theoretically prove that all three algorithms achieve a tighter regret upper bound of $O(\\sqrt{dT\\log{T}})$ with respect to the time horizon $T$, improving upon existing methods. Additionally, we provide a matching lower bound $\\Omega(\\sqrt{dT})$ for conversational bandits, demonstrating that our algorithms are nearly minimax optimal. Extensive evaluations on both synthetic and real-world datasets show that our approaches achieve at least a 14.6% improvement in cumulative regret.', 'abstract_zh': '基于对话的推荐系统主动查询与用户相关的关键术语，并利用反馈来推断用户的偏好以实现个性化的推荐。对话上下文臂赛局，这一领域的主流方法旨在通过权衡探索与利用来优化偏好学习。然而，现有方法的几个局限性限制了其在实际场景中的效果。首先，现有算法的关键术语选择策略探索不足，往往未能充分探查用户的偏好，导致偏好估计欠佳。其次，当前算法通常依赖于确定性规则启动对话，当偏好已明了时引起不必要的互动，并在偏好不确定时错过机会。为解决这些问题，我们提出三种新型算法：CLiSK、CLiME和CLiSK-ME。CLiSK引入平滑的关键术语上下文以增强偏好学习中的探索，CLiME根据偏好不确定性适应性地启动对话，而CLiSK-ME结合了这两种技术。我们理论上证明，所有三种算法相对时间范围T实现了更紧的后悔上界$O(\\sqrt{dT\\log{T}})$，优于现有方法。此外，我们为对话臂赛局提供了匹配的下界$\\Omega(\\sqrt{dT})$，表明我们的算法几乎达到最小最大最优。广泛的合成和现实世界数据集评估表明，我们的方法在累积后悔上至少实现了14.6%的提升。', 'title_zh': '挖掘对话的力量：对话情境_bandits中最佳关键词选择'}
{'arxiv_id': 'arXiv:2505.21391', 'title': 'Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features', 'authors': 'Zixuan Xie, Xinyu Liu, Rohan Chandra, Shangtong Zhang', 'link': 'https://arxiv.org/abs/2505.21391', 'abstract': 'Linear TD($\\lambda$) is one of the most fundamental reinforcement learning algorithms for policy evaluation. Previously, convergence rates are typically established under the assumption of linearly independent features, which does not hold in many practical scenarios. This paper instead establishes the first $L^2$ convergence rates for linear TD($\\lambda$) operating under arbitrary features, without making any algorithmic modification or additional assumptions. Our results apply to both the discounted and average-reward settings. To address the potential non-uniqueness of solutions resulting from arbitrary features, we develop a novel stochastic approximation result featuring convergence rates to the solution set instead of a single point.', 'abstract_zh': '线性TD($\\lambda$)是用于策略评估的基本强化学习算法之一。以往的收敛速率通常是基于特征线性独立的假设建立的，这种假设在许多实际场景中并不成立。本文首次为在任意特征下运行的线性TD($\\lambda$)建立了$L^2$收敛速率，无需进行任何算法修改或额外假设。我们的结果适用于折扣奖励和平均奖励两种设置。为解决由于任意特征可能导致的解的非唯一性问题，我们开发了一种新的随机逼近结果，该结果的方向是收敛到解集而不是单一点。', 'title_zh': '有限样本分析任意特征下的线性时序差分学习'}
{'arxiv_id': 'arXiv:2505.21388', 'title': 'DeSocial: Blockchain-based Decentralized Social Networks', 'authors': 'Jingyuan Huang, Xi Zhu, Minghao Guo, Yongfeng Zhang', 'link': 'https://arxiv.org/abs/2505.21388', 'abstract': "Web 2.0 social platforms are inherently centralized, with user data and algorithmic decisions controlled by the platform. However, users can only passively receive social predictions without being able to choose the underlying algorithm, which limits personalization. Fortunately, with the emergence of blockchain, users are allowed to choose algorithms that are tailored to their local situation, improving prediction results in a personalized way. In a blockchain environment, each user possesses its own model to perform the social prediction, capturing different perspectives on social interactions. In our work, we propose DeSocial, a decentralized social network learning framework deployed on an Ethereum (ETH) local development chain that integrates distributed data storage, node-level consensus, and user-driven model selection through Ganache. In the first stage, each user leverages DeSocial to evaluate multiple backbone models on their local subgraph. DeSocial coordinates the execution and returns model-wise prediction results, enabling the user to select the most suitable backbone for personalized social prediction. Then, DeSocial uniformly selects several validation nodes that possess the algorithm specified by each user, and aggregates the prediction results by majority voting, to prevent errors caused by any single model's misjudgment. Extensive experiments show that DeSocial has an evident improvement compared to the five classical centralized social network learning models, promoting user empowerment in blockchain-based decentralized social networks, showing the importance of multi-node validation and personalized algorithm selection based on blockchain. Our implementation is available at: this https URL.", 'abstract_zh': '基于区块链的去中心化社会网络学习框架DeSocial', 'title_zh': '去中心化社会网络：基于区块链的去中心化社交网络'}
{'arxiv_id': 'arXiv:2505.21372', 'title': 'Improving LLM-based Global Optimization with Search Space Partitioning', 'authors': 'Andrej Schwanke, Lyubomir Ivanov, David Salinas, Fabio Ferreira, Aaron Klein, Frank Hutter, Arber Zela', 'link': 'https://arxiv.org/abs/2505.21372', 'abstract': "Large Language Models (LLMs) have recently emerged as effective surrogate models and candidate generators within global optimization frameworks for expensive blackbox functions. Despite promising results, LLM-based methods often struggle in high-dimensional search spaces or when lacking domain-specific priors, leading to sparse or uninformative suggestions. To overcome these limitations, we propose HOLLM, a novel global optimization algorithm that enhances LLM-driven sampling by partitioning the search space into promising subregions. Each subregion acts as a ``meta-arm'' selected via a bandit-inspired scoring mechanism that effectively balances exploration and exploitation. Within each selected subregion, an LLM then proposes high-quality candidate points, without any explicit domain knowledge. Empirical evaluation on standard optimization benchmarks shows that HOLLM consistently matches or surpasses leading Bayesian optimization and trust-region methods, while substantially outperforming global LLM-based sampling strategies.", 'abstract_zh': '大型语言模型在高维搜索空间中的全局优化框架中作为昂贵的黑盒函数的有效替代模型和支持者生成器已崭露头角。为克服这些限制，我们提出了一种名为HOLLM的新颖全局优化算法，该算法通过将搜索空间划分成有希望的子区域，来增强基于大型语言模型的采样。每个子区域作为“元臂”，通过一种受带宽机制启发的评分机制来选择，该机制有效地平衡了探索和利用。在每个选定的子区域内，一个大型语言模型提出高质量的候选点，而无需任何显式的领域知识。在标准优化基准上的实证评估表明，HOLLM在一致性上能够匹配或超越领先的贝叶斯优化和信任区域方法，并且在全局大型语言模型采样策略方面表现出显著的优势。', 'title_zh': '基于搜索空间分区改进的LLM全局优化方法'}
{'arxiv_id': 'arXiv:2505.21364', 'title': 'Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders', 'authors': 'James Oldfield, Shawn Im, Yixuan Li, Mihalis A. Nicolaou, Ioannis Patras, Grigorios G Chrysos', 'link': 'https://arxiv.org/abs/2505.21364', 'abstract': "Multilayer perceptrons (MLPs) are an integral part of large language models, yet their dense representations render them difficult to understand, edit, and steer. Recent methods learn interpretable approximations via neuron-level sparsity, yet fail to faithfully reconstruct the original mapping--significantly increasing model's next-token cross-entropy loss. In this paper, we advocate for moving to layer-level sparsity to overcome the accuracy trade-off in sparse layer approximation. Under this paradigm, we introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear Units, expanding pre-trained dense layers into tens of thousands of specialized sublayers. Through a flexible form of tensor factorization, each sparsely activating MxD sublayer implements a linear transformation with full-rank weights--preserving the original decoders' expressive capacity even under heavy sparsity. Experimentally, we show that MxDs significantly outperform state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier in language models with up to 3B parameters. Further evaluations on sparse probing and feature steering demonstrate that MxDs learn similarly specialized features of natural language--opening up a promising new avenue for designing interpretable yet faithful decompositions. Our code is included at: this https URL.", 'abstract_zh': '多层感知机（MLPs）是大型语言模型的重要组成部分，但由于其密集表示使得它们难以理解和编辑。近期方法通过神经元级的稀疏性学习可解释的近似表示，但未能忠实地重建原始映射——显著增加了模型的下一个标记交叉熵损失。在本文中，我们提倡转向层级稀疏性来克服稀疏层近似中的准确性和稀疏性之间的权衡。在这个范式下，我们引入了混合解码器（MxDs）。MxDs泛化了MLPs和门控线性单元，将预训练的密集层扩展为数千个专门的子层。通过灵活的张量分解形式，每个稀疏激活的MxD子层实现了一个具有满秩权重的线性变换——即使在重度稀疏情况下也能保持原始解码器的表征能力。实验中，我们在具有多达3亿参数的语言模型中稀疏性-准确性的前沿上展示了MxDs显著优于最先进的方法（如Transcoders）。进一步的稀疏探针和特征导向评估表明，MxDs学习到类似的自然语言专门特征——为设计可解释且忠实的分解开辟了一条有前景的新途径。我们的代码发布在：this https URL。', 'title_zh': '无需妥协以实现可解释性：忠实密集层分解与混合解码器'}
{'arxiv_id': 'arXiv:2505.21363', 'title': 'Subgroups Matter for Robust Bias Mitigation', 'authors': 'Anissa Alloula, Charles Jones, Ben Glocker, Bartłomiej W. Papież', 'link': 'https://arxiv.org/abs/2505.21363', 'abstract': 'Despite the constant development of new bias mitigation methods for machine learning, no method consistently succeeds, and a fundamental question remains unanswered: when and why do bias mitigation techniques fail? In this paper, we hypothesise that a key factor may be the often-overlooked but crucial step shared by many bias mitigation methods: the definition of subgroups. To investigate this, we conduct a comprehensive evaluation of state-of-the-art bias mitigation methods across multiple vision and language classification tasks, systematically varying subgroup definitions, including coarse, fine-grained, intersectional, and noisy subgroups. Our results reveal that subgroup choice significantly impacts performance, with certain groupings paradoxically leading to worse outcomes than no mitigation at all. Our findings suggest that observing a disparity between a set of subgroups is not a sufficient reason to use those subgroups for mitigation. Through theoretical analysis, we explain these phenomena and uncover a counter-intuitive insight that, in some cases, improving fairness with respect to a particular set of subgroups is best achieved by using a different set of subgroups for mitigation. Our work highlights the importance of careful subgroup definition in bias mitigation and suggest it as a alternative lever for improving the robustness and fairness of machine learning models.', 'abstract_zh': '尽管机器学习中新偏见缓解方法不断开发，但没有任何方法能够始终如一地成功，一个基本问题仍未解答：何时以及为何偏见缓解技术会失效？在本文中，我们假设一个关键因素可能是许多偏见缓解方法中经常被忽视但至关重要的步骤：子组定义。为了调查这一点，我们在多个视觉和语言分类任务中系统地评估了当前最先进的偏见缓解方法，并系统地变化子组定义，包括粗糙、精细、交叉和嘈杂子组。我们的结果表明，子组选择对性能有显著影响，某些分组 paradoxically 导致比不进行任何缓解更差的结果。我们的研究结果表明，观察一组子组之间存在差异并不足以使用这些子组进行缓解。通过理论分析，我们解释了这些现象，并揭示了一个反直觉的见解，即在某些情况下，针对特定一组子组提高公平性，最好的方法是使用另一组子组进行缓解。本文强调了在偏见缓解中仔细定义子组的重要性，并建议将其作为增强机器学习模型的稳健性和公平性的替代杠杆。', 'title_zh': '亚群体对于稳健的偏见缓解很重要'}
{'arxiv_id': 'arXiv:2505.21362', 'title': 'Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History', 'authors': 'Qishuai Zhong, Zongmin Li, Siqi Fan, Aixin Sun', 'link': 'https://arxiv.org/abs/2505.21362', 'abstract': "Effective engagement by large language models (LLMs) requires adapting responses to users' sociodemographic characteristics, such as age, occupation, and education level. While many real-world applications leverage dialogue history for contextualization, existing evaluations of LLMs' behavioral adaptation often focus on single-turn prompts. In this paper, we propose a framework to evaluate LLM adaptation when attributes are introduced either (1) explicitly via user profiles in the prompt or (2) implicitly through multi-turn dialogue history. We assess the consistency of model behavior across these modalities. Using a multi-agent pipeline, we construct a synthetic dataset pairing dialogue histories with distinct user profiles and employ questions from the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe value expression. Our findings indicate that most models adjust their expressed values in response to demographic changes, particularly in age and education level, but consistency varies. Models with stronger reasoning capabilities demonstrate greater alignment, indicating the importance of reasoning in robust sociodemographic adaptation.", 'abstract_zh': '大型语言模型的有效 engagement 需要根据用户的 sociodemographic 特征（如年龄、职业和教育水平）调整响应：一种评估框架', 'title_zh': '评价大规模语言模型对社会人口统计因素的适应性：用户画像 vs. 对话历史'}
{'arxiv_id': 'arXiv:2505.21355', 'title': 'Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods', 'authors': 'Muhammad Imran, Wayne G. Brisbane, Li-Ming Su, Jason P. Joseph, Wei Shao', 'link': 'https://arxiv.org/abs/2505.21355', 'abstract': 'Background and objective: Micro-ultrasound (micro-US) is a novel imaging modality with diagnostic accuracy comparable to MRI for detecting clinically significant prostate cancer (csPCa). We investigated whether artificial intelligence (AI) interpretation of micro-US can outperform clinical screening methods using PSA and digital rectal examination (DRE). Methods: We retrospectively studied 145 men who underwent micro-US guided biopsy (79 with csPCa, 66 without). A self-supervised convolutional autoencoder was used to extract deep image features from 2D micro-US slices. Random forest classifiers were trained using five-fold cross-validation to predict csPCa at the slice level. Patients were classified as csPCa-positive if 88 or more consecutive slices were predicted positive. Model performance was compared with a classifier using PSA, DRE, prostate volume, and age. Key findings and limitations: The AI-based micro-US model and clinical screening model achieved AUROCs of 0.871 and 0.753, respectively. At a fixed threshold, the micro-US model achieved 92.5% sensitivity and 68.1% specificity, while the clinical model showed 96.2% sensitivity but only 27.3% specificity. Limitations include a retrospective single-center design and lack of external validation. Conclusions and clinical implications: AI-interpreted micro-US improves specificity while maintaining high sensitivity for csPCa detection. This method may reduce unnecessary biopsies and serve as a low-cost alternative to PSA-based screening. Patient summary: We developed an AI system to analyze prostate micro-ultrasound images. It outperformed PSA and DRE in detecting aggressive cancer and may help avoid unnecessary biopsies.', 'abstract_zh': '背景与目的：微超声（micro-US）是一种与磁共振成像（MRI）具有相似诊断准确性的新成像技术，用于检测临床显著前列腺癌（csPCa）。我们研究了人工智能（AI）解释微超声是否能在前列腺特异性抗原（PSA）和直肠指检（DRE）的临床筛查方法上表现出更优的性能。方法：回顾性研究了145名接受微超声引导活检的男性（其中79人患有csPCa，66人未患有csPCa）。使用自监督卷积自动编码器从2D微超声切片中提取深度图像特征。采用五折交叉验证训练随机森林分类器，以在切片级别预测csPCa。如果预测为阳性的连续切片数达到88片以上，则患者被分类为csPCa阳性。将微超声模型的表现与使用PSA、DRE、前列腺体积和年龄的分类器进行比较。主要发现与局限性：基于AI的微超声模型和临床筛查模型分别获得了0.871和0.753的AUROC。固定阈值下，微超声模型的敏感性和特异性分别为92.5%和68.1%，而临床模型的敏感性为96.2%，但特异性仅为27.3%。局限性包括回顾性单中心设计和缺乏外部验证。结论与临床意义：AI解释的微超声提高了前列腺癌检测的特异性，同时保持高灵敏度。该方法可能减少不必要的活检，并作为基于PSA筛查的低成本替代方案。患者总结：我们开发了一种AI系统来分析前列腺微超声图像，其在检测侵袭性癌症方面优于PSA和DRE，并可能帮助避免不必要的活检。', 'title_zh': '人工智能增强微超声在前列腺癌筛查中的应用：与传统方法的比较研究'}
{'arxiv_id': 'arXiv:2505.21339', 'title': 'An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction', 'authors': 'Henryk Mustroph, Michel Kunkler, Stefanie Rinderle-Ma', 'link': 'https://arxiv.org/abs/2505.21339', 'abstract': "Suffix prediction of business processes forecasts the remaining sequence of events until process completion. Current approaches focus on predicting a single, most likely suffix. However, if the future course of a process is exposed to uncertainty or has high variability, the expressiveness of a single suffix prediction can be limited. To address this limitation, we propose probabilistic suffix prediction, a novel approach that approximates a probability distribution of suffixes. The proposed approach is based on an Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC) suffix sampling algorithm. We capture epistemic uncertainties via MC dropout and aleatoric uncertainties as learned loss attenuation. This technical report provides a detailed evaluation of the U-ED-LSTM's predictive performance and assesses its calibration on four real-life event logs with three different hyperparameter settings. The results show that i) the U-ED-LSTM has reasonable predictive performance across various datasets, ii) aggregating probabilistic suffix predictions into mean values can outperform most likely predictions, particularly for rare prefixes or longer suffixes, and iii) the approach effectively captures uncertainties present in event logs.", 'abstract_zh': '企业流程的后缀预测 forecasting the remaining sequence of events until process completion through probabilistic suffix prediction', 'title_zh': '一种aware不确定性ED-LSTM的概率后缀预测模型'}
{'arxiv_id': 'arXiv:2505.21335', 'title': 'Structure from Collision', 'authors': 'Takuhiro Kaneko', 'link': 'https://arxiv.org/abs/2505.21335', 'abstract': 'Recent advancements in neural 3D representations, such as neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate estimation of 3D structures from multiview images. However, this capability is limited to estimating the visible external structure, and identifying the invisible internal structure hidden behind the surface is difficult. To overcome this limitation, we address a new task called Structure from Collision (SfC), which aims to estimate the structure (including the invisible internal structure) of an object from appearance changes during collision. To solve this problem, we propose a novel model called SfC-NeRF that optimizes the invisible internal structure of an object through a video sequence under physical, appearance (i.e., visible external structure)-preserving, and keyframe constraints. In particular, to avoid falling into undesirable local optima owing to its ill-posed nature, we propose volume annealing; that is, searching for global optima by repeatedly reducing and expanding the volume. Extensive experiments on 115 objects involving diverse structures (i.e., various cavity shapes, locations, and sizes) and material properties revealed the properties of SfC and demonstrated the effectiveness of the proposed SfC-NeRF.', 'abstract_zh': 'Recent advancements in神经网络三维表示，如神经辐射场（NeRF）和三维高斯散点（3DGS），使从多视角图像准确估计三维结构成为可能。然而，这种能力仅限于估计可见的外部结构，而识别隐藏在表面背后的不可见内部结构则难以实现。为克服这一限制，我们提出了一项新的任务，即碰撞结构恢复（Structure from Collision，SfC），该任务旨在通过碰撞期间的外观变化估计物体的结构（包括不可见的内部结构）。为了解决这一问题，我们提出了一种新型模型SfC-NeRF，该模型在保持物理约束、外观（即可见的外部结构）约束和关键帧约束的情况下，优化物体的不可见内部结构。特别地，为了避免由于其病态性质而陷入不良的局部极值，我们提出了一种体积退火方法，通过反复减小和扩大体积来搜索全局极值。针对115个具有多样结构（即各种空腔形状、位置和大小）和材料性质的物体进行的广泛实验揭示了SfC的性质，并证明了所提出的SfC-NeRF的有效性。', 'title_zh': '碰撞结构重建'}
{'arxiv_id': 'arXiv:2505.21329', 'title': "Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks", 'authors': 'Allaa Boutaleb, Bernd Amann, Hubert Naacke, Rafael Angarita', 'link': 'https://arxiv.org/abs/2505.21329', 'abstract': 'Recent table representation learning and data discovery methods tackle table union search (TUS) within data lakes, which involves identifying tables that can be unioned with a given query table to enrich its content. These methods are commonly evaluated using benchmarks that aim to assess semantic understanding in real-world TUS tasks. However, our analysis of prominent TUS benchmarks reveals several limitations that allow simple baselines to perform surprisingly well, often outperforming more sophisticated approaches. This suggests that current benchmark scores are heavily influenced by dataset-specific characteristics and fail to effectively isolate the gains from semantic understanding. To address this, we propose essential criteria for future benchmarks to enable a more realistic and reliable evaluation of progress in semantic table union search.', 'abstract_zh': '近期的表表示学习和数据发现方法在数据湖中处理表联查搜索（TUS）任务，涉及识别可以与给定查询表联查的表以丰富其内容。这些方法通常使用旨在评估实际TUS任务中语义理解的基准进行评估。然而，我们对主要的TUS基准的分析揭示了几种局限性，使简单的基线能够表现出乎意料的好性能，往往优于更为复杂的方案。这表明当前的基准分数受数据集特定特征的影响较大，未能有效地隔离语义理解带来的提升。为此，我们提出未来的基准应具备必要的标准，以实现更现实和可靠的语言理解表联查搜索进展评估。', 'title_zh': '数据湖中存在问题：对表联合查询基准测试的一项批判性重新评估'}
{'arxiv_id': 'arXiv:2505.21317', 'title': 'A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features', 'authors': 'Ihab Bendidi, Yassir El Mesbahi, Alisandra K. Denton, Karush Suri, Kian Kenyon-Dean, Auguste Genovesio, Emmanuel Noutahi', 'link': 'https://arxiv.org/abs/2505.21317', 'abstract': 'Understanding cellular responses to stimuli is crucial for biological discovery and drug development. Transcriptomics provides interpretable, gene-level insights, while microscopy imaging offers rich predictive features but is harder to interpret. Weakly paired datasets, where samples share biological states, enable multimodal learning but are scarce, limiting their utility for training and multimodal inference. We propose a framework to enhance transcriptomics by distilling knowledge from microscopy images. Using weakly paired data, our method aligns and binds modalities, enriching gene expression representations with morphological information. To address data scarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal distillation using pretrained foundation models, achieving state-of-the-art results, and (2) PEA (Perturbation Embedding Augmentation), a novel augmentation technique that enhances transcriptomics data while preserving inherent biological information. These strategies improve the predictive power and retain the interpretability of transcriptomics, enabling rich unimodal representations for complex biological tasks.', 'abstract_zh': '通过显微镜图像提炼知识以增强转录组学分析：利用弱配对数据实现模式识别和信息保留', 'title_zh': '基于形态学特征的跨模态知识精炼与数据增强方法以改善转录组表示'}
{'arxiv_id': 'arXiv:2505.21301', 'title': 'How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian', 'authors': 'Andrea Pedrotti, Giulia Rambelli, Caterina Villani, Marianna Bolognesi', 'link': 'https://arxiv.org/abs/2505.21301', 'abstract': 'People can categorize the same entity at multiple taxonomic levels, such as basic (bear), superordinate (animal), and subordinate (grizzly bear). While prior research has focused on basic-level categories, this study is the first attempt to examine the organization of categories by analyzing exemplars produced at the subordinate level. We present a new Italian psycholinguistic dataset of human-generated exemplars for 187 concrete words. We then use these data to evaluate whether textual and vision LLMs produce meaningful exemplars that align with human category organization across three key tasks: exemplar generation, category induction, and typicality judgment. Our findings show a low alignment between humans and LLMs, consistent with previous studies. However, their performance varies notably across different semantic domains. Ultimately, this study highlights both the promises and the constraints of using AI-generated exemplars to support psychological and linguistic research.', 'abstract_zh': '人们可以将同一个实体归类到多个层级的类别中，如基本层级（熊）、超属概念层级（动物）和亚属概念层级（棕熊）。虽然以往的研究主要集中在基本层级类别上，但本研究是第一次通过分析亚属概念层级产生的实例来探讨类别的组织结构。我们提供了一个新的包含187个具体词汇的人类生成实例的心理语言学意大利语数据集。接着，我们利用这些数据评估文本和视觉大语言模型生成具有人类类别组织一致性的有意义实例的能力，这涉及三个关键任务：实例生成、类别归纳和典型性判断。研究结果表明，人类和大语言模型之间的一致较低，这一发现与以往的研究结果一致。然而，它们在不同的语义领域中的表现差异显著。最终，本研究突显了使用AI生成的实例支持心理学和语言学研究的潜力和限制。', 'title_zh': '人类和LLMs组织概念知识：探究意大利语中的次级类别'}
{'arxiv_id': 'arXiv:2505.21298', 'title': 'Large Language Models Miss the Multi-Agent Mark', 'authors': 'Emanuele La Malfa, Gabriele La Malfa, Samuele Marro, Jie M. Zhang, Elizabeth Black, Micheal Luck, Philip Torr, Michael Wooldridge', 'link': 'https://arxiv.org/abs/2505.21298', 'abstract': 'Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs) has led to an increase in frameworks leveraging multiple LLMs to tackle complex tasks. However, much of this literature appropriates the terminology of MAS without engaging with its foundational principles. In this position paper, we highlight critical discrepancies between MAS theory and current MAS LLMs implementations, focusing on four key areas: the social aspect of agency, environment design, coordination and communication protocols, and measuring emergent behaviours. Our position is that many MAS LLMs lack multi-agent characteristics such as autonomy, social interaction, and structured environments, and often rely on oversimplified, LLM-centric architectures. The field may slow down and lose traction by revisiting problems the MAS literature has already addressed. Therefore, we systematically analyse this issue and outline associated research opportunities; we advocate for better integrating established MAS concepts and more precise terminology to avoid mischaracterisation and missed opportunities.', 'abstract_zh': 'Recent Interest in Multi-Agent Systems of Large Language Models (MAS LLMs): Bridging the Gap between Theory and Practice', 'title_zh': '大型语言模型忽视了多代理机制。'}
{'arxiv_id': 'arXiv:2505.21288', 'title': 'GSAT: Graph Structure Attention Networks', 'authors': 'Farshad Noravesh, Reza Haffari, Layki Soon, Arghya Pal', 'link': 'https://arxiv.org/abs/2505.21288', 'abstract': 'Graph Neural Networks (GNNs) have emerged as a powerful tool for processing data represented in graph structures, achieving remarkable success across a wide range of applications. However, to further improve the performance on graph classification benchmarks, structural representation of each node that encodes rich local topological information in the neighbourhood of nodes is an important type of feature that is often overlooked in the modeling. The consequence of neglecting the structural information has resulted high number of layers to connect messages from distant nodes which by itself produces other problems such as oversmoothing. In the present paper, we leverage these structural information that are modeled by anonymous random walks (ARWs) and introduce graph structure attention network (GSAT) which is a generalization of graph attention network(GAT) to integrate the original attribute and the structural representation to enforce the model to automatically find patterns for attending to different edges in the node neighbourhood to enrich graph representation. Our experiments show GSAT slightly improves SOTA on some graph classification benchmarks.', 'abstract_zh': '图神经网络（GNNs）已成为处理图结构表示数据的强大工具，在广泛的应用中取得了显著成功。然而，为了进一步提高图分类基准上的性能，每个节点的结构表示能够编码节点 neighborhood 中丰富的局部拓扑信息，这种类型的特征在建模中经常被忽略。忽视结构信息导致需要大量层次来连接远距离节点的消息，这本身会产生诸如过度平滑等问题。本文利用由匿名随机游走（ARWs）建模的这些结构信息，并引入图结构注意网络（GSAT），这是一种图注意力网络（GAT）的扩展，用于整合原始属性和结构表示，促使模型自动发现关注节点 neighborhood 中不同边的模式，以丰富图表示。我们的实验表明，GSAT在某些图分类基准上稍微提升了当前最佳性能。', 'title_zh': 'GSAT: 图结构注意力网络'}
{'arxiv_id': 'arXiv:2505.21277', 'title': 'Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space', 'authors': 'Yao Huang, Yitong Sun, Shouwei Ruan, Yichi Zhang, Yinpeng Dong, Xingxing Wei', 'link': 'https://arxiv.org/abs/2505.21277', 'abstract': 'Large Language Models (LLMs), despite advanced general capabilities, still suffer from numerous safety risks, especially jailbreak attacks that bypass safety protocols. Understanding these vulnerabilities through black-box jailbreak attacks, which better reflect real-world scenarios, offers critical insights into model robustness. While existing methods have shown improvements through various prompt engineering techniques, their success remains limited against safety-aligned models, overlooking a more fundamental problem: the effectiveness is inherently bounded by the predefined strategy spaces. However, expanding this space presents significant challenges in both systematically capturing essential attack patterns and efficiently navigating the increased complexity. To better explore the potential of expanding the strategy space, we address these challenges through a novel framework that decomposes jailbreak strategies into essential components based on the Elaboration Likelihood Model (ELM) theory and develops genetic-based optimization with intention evaluation mechanisms. To be striking, our experiments reveal unprecedented jailbreak capabilities by expanding the strategy space: we achieve over 90% success rate on Claude-3.5 where prior methods completely fail, while demonstrating strong cross-model transferability and surpassing specialized safeguard models in evaluation accuracy. The code is open-sourced at: this https URL.', 'abstract_zh': '大语言模型（LLMs）尽管具有先进的通用能力，仍面临诸多安全风险，尤其是突破安全协议的 jailbreak 攻击。通过黑盒 jailbreak 攻击理解这些漏洞，更好地反映了现实场景，为模型鲁棒性提供了关键洞察。虽然现有的方法通过各种提示工程技术显示出改进，但它们的成功率仍受限于安全对齐模型，忽视了一个更根本的问题：效果本质上受限于预定义的战略空间。然而，扩展这一空间带来了系统捕捉重要攻击模式和高效处理增加复杂性的显著挑战。为更好地探索扩展战略空间的潜在价值，我们提出了一个新颖的框架，基于 elaboration likelihood model (ELM) 理论将 jailbreak 策略分解为基本组件，并结合基于遗传的优化和意图评估机制。我们的实验展示了通过扩展战略空间实现前所未有的 jailbreak 能力：我们在 Claude-3.5 上实现了超过 90% 的成功率，而在先前方法完全失败的情况下，同时显示出较强的跨模型可转移性和在评估准确性上超越专门的安全防护模型。代码已开源：this https URL。', 'title_zh': '突破天花板：通过扩展策略空间探索越狱攻击的潜力'}
{'arxiv_id': 'arXiv:2505.21265', 'title': 'Multilingual Pretraining for Pixel Language Models', 'authors': 'Ilker Kesen, Jonas F. Lotz, Ingo Ziegler, Phillip Rust, Desmond Elliott', 'link': 'https://arxiv.org/abs/2505.21265', 'abstract': 'Pixel language models operate directly on images of rendered text, eliminating the need for a fixed vocabulary. While these models have demonstrated strong capabilities for downstream cross-lingual transfer, multilingual pretraining remains underexplored. We introduce PIXEL-M4, a model pretrained on four visually and linguistically diverse languages: English, Hindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic and syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart on non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4 captures rich linguistic features, even in languages not seen during pretraining. Furthermore, an analysis of its hidden representations shows that multilingual pretraining yields a semantic embedding space closely aligned across the languages used for pretraining. This work demonstrates that multilingual pretraining substantially enhances the capability of pixel language models to effectively support a diverse set of languages.', 'abstract_zh': '像素语言模型直接在渲染文本的图像上操作，无需固定词汇表。虽然这些模型在下游跨语言迁移任务中展示了强大的能力，但多语言预训练仍然未被充分探索。我们引入PIXEL-M4模型，该模型在四种视觉和语料上差异较大的语言上进行预训练：英语、印地语、乌克兰语和简体中文。多语言评估结果显示，PIXEL-M4在非拉丁字母文字上优于仅基于英语的对照组。词级探针分析证实，即使是在预训练中未见过的语言，PIXEL-M4也能够捕捉丰富的语言特征。此外，对其隐藏表示的分析显示，多语言预训练生成了一个紧密对齐的语义嵌入空间，与预训练所用语言高度一致。这项工作证明，多语言预训练显著增强了像素语言模型跨多种语言的有效支持能力。', 'title_zh': '多语言像素语言模型的预训练'}
{'arxiv_id': 'arXiv:2505.21236', 'title': 'Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies', 'authors': 'Felix Chalumeau, Daniel Rajaonarivonivelomanantsoa, Ruan de Kock, Claude Formanek, Sasha Abramowitz, Oumayma Mahjoub, Wiem Khlifi, Simon Du Toit, Louay Ben Nessir, Refiloe Shabe, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob, Arnu Pretorius', 'link': 'https://arxiv.org/abs/2505.21236', 'abstract': 'Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at this https URL.', 'abstract_zh': '强化学习（RL）系统在从能源电网管理到蛋白质设计等多个领域都有广泛的应用。然而，这些现实世界场景往往极其复杂，具有组合性质，并且需要多个代理之间的复杂协调。这种复杂性可能导致最先进的RL系统，在收敛训练后，即使在零样本推理的情况下，也无法突破性能瓶颈。同时，许多基于数字或模拟的应用程序允许在推理阶段利用特定的时间和计算预算，在尝试多次之后输出最终解决方案。在本研究中，我们表明，在执行时采用这样的推理阶段以及相应的推理策略选择，是打破复杂多代理RL问题中观察到的性能瓶颈的关键。我们的主要结果令人惊讶：仅通过在执行过程中额外使用几秒钟的 wall-clock 时间，我们就能在 17 个任务上将性能提升最高达 126%，平均提升 45%。我们还展示了令人鼓舞的计算可扩展性，由超过 60,000 次实验支持，使其成为迄今为止最大规模的针对复杂RL的推理策略研究。我们的实验数据和代码可在此处访问。', 'title_zh': '在复杂强化学习中突破性能天花板需要推理策略'}
{'arxiv_id': 'arXiv:2505.21230', 'title': 'PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems', 'authors': 'Nima Sedghiyeh, Sara Sadeghi, Reza Khodadadi, Farzin Kashani, Omid Aghdaei, Somayeh Rahimi, Mohammad Sadegh Safari', 'link': 'https://arxiv.org/abs/2505.21230', 'abstract': "Although Automatic Speech Recognition (ASR) systems have become an integral part of modern technology, their evaluation remains challenging, particularly for low-resource languages such as Persian. This paper introduces Persian Speech Recognition Benchmark(PSRB), a comprehensive benchmark designed to address this gap by incorporating diverse linguistic and acoustic conditions. We evaluate ten ASR systems, including state-of-the-art commercial and open-source models, to examine performance variations and inherent biases. Additionally, we conduct an in-depth analysis of Persian ASR transcriptions, identifying key error types and proposing a novel metric that weights substitution errors. This metric enhances evaluation robustness by reducing the impact of minor and partial errors, thereby improving the precision of performance assessment. Our findings indicate that while ASR models generally perform well on standard Persian, they struggle with regional accents, children's speech, and specific linguistic challenges. These results highlight the necessity of fine-tuning and incorporating diverse, representative training datasets to mitigate biases and enhance overall ASR performance. PSRB provides a valuable resource for advancing ASR research in Persian and serves as a framework for developing benchmarks in other low-resource languages. A subset of the PSRB dataset is publicly available at this https URL.", 'abstract_zh': '尽管自动语音识别（ASR）系统已成为现代技术不可或缺的一部分，但其评估仍然具有挑战性，特别是在波斯语等低资源语言方面。本文介绍了波斯语语音识别基准（PSRB），这是一个旨在通过融入多样的语言和声学条件来填补这一空白的全面基准。我们评估了十个ASR系统，包括最先进的商业和开源模型，以检查性能差异和固有的偏见。此外，我们对波斯语ASR转录进行了深入分析，确定了关键的错误类型，并提出了一个新的度量标准，该标准对替换错误进行加权。该度量标准通过减少次要和部分错误的影响来增强评估的稳健性，从而提高性能评估的精确性。我们的研究发现，虽然ASR模型在标准波斯语上表现良好，但在地区口音、儿童的言语和特定的语言挑战方面存在困难。这些结果强调了微调和采用多样化的代表性训练数据集以缓解偏见并提高整体ASR性能的必要性。PSRB为推进波斯语ASR研究提供了宝贵资源，并为其他低资源语言开发基准提供了框架。部分PSRB数据集可在以下网址公开获取：this https URL。', 'title_zh': 'PSRB: 评估波斯语ASR系统的综合性基准'}
{'arxiv_id': 'arXiv:2505.21228', 'title': 'Is Hyperbolic Space All You Need for Medical Anomaly Detection?', 'authors': 'Alvaro Gonzalez-Jimenez, Simone Lionetti, Ludovic Amruthalingam, Philippe Gottfrois, Fabian Gröger, Marc Pouly, Alexander A. Navarini', 'link': 'https://arxiv.org/abs/2505.21228', 'abstract': 'Medical anomaly detection has emerged as a promising solution to challenges in data availability and labeling constraints. Traditional methods extract features from different layers of pre-trained networks in Euclidean space; however, Euclidean representations fail to effectively capture the hierarchical relationships within these features, leading to suboptimal anomaly detection performance. We propose a novel yet simple approach that projects feature representations into hyperbolic space, aggregates them based on confidence levels, and classifies samples as healthy or anomalous. Our experiments demonstrate that hyperbolic space consistently outperforms Euclidean-based frameworks, achieving higher AUROC scores at both image and pixel levels across multiple medical benchmark datasets. Additionally, we show that hyperbolic space exhibits resilience to parameter variations and excels in few-shot scenarios, where healthy images are scarce. These findings underscore the potential of hyperbolic space as a powerful alternative for medical anomaly detection. The project website can be found at this https URL', 'abstract_zh': '医疗异常检测在数据可用性和标注约束挑战中 emerged as a promising solution. 传统方法在欧几里得空间中从预训练网络的不同层抽取特征；然而，欧几里得表示方式无法有效捕捉这些特征之间的层次关系，导致异常检测性能不佳。我们提出了一种新颖而简单的方法，将特征表示投影到双曲空间，在置信水平基础上进行聚合，并将样本分类为健康或异常。实验结果表明，双曲空间在图像和像素级别上始终优于基于欧几里得的空间框架，实现了多个医学基准数据集中的更高AUROC分数。此外，我们展示了双曲空间对参数变化具有鲁棒性，并在健康图像稀缺的少样本场景中表现优异。这些发现强调了双曲空间作为医疗异常检测有力替代方案的潜力。相关项目网站请访问 <https://>。', 'title_zh': '双曲空间是医疗异常检测所需的一切吗？'}
{'arxiv_id': 'arXiv:2505.21219', 'title': 'Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection', 'authors': 'Qinjun Fei, Nuria Rodríguez-Barroso, María Victoria Luzón, Zhongliang Zhang, Francisco Herrera', 'link': 'https://arxiv.org/abs/2505.21219', 'abstract': 'In cross-silo Federated Learning (FL), client selection is critical to ensure high model performance, yet it remains challenging due to data quality decompensation, budget constraints, and incentive compatibility. As training progresses, these factors exacerbate client heterogeneity and degrade global performance. Most existing approaches treat these challenges in isolation, making jointly optimizing multiple factors difficult. To address this, we propose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a unified framework integrating dynamic bidding, reputation modeling, and cost-aware selection. Clients submit bids based on their perceived data quality, and their contributions are evaluated using Shapley values to quantify their marginal impact on the global model. A reputation system, inspired by prospect theory, captures historical performance while penalizing inconsistency. The client selection problem is formulated as a 0-1 integer program that maximizes reputation-weighted utility under budget constraints. Experiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that SBRO-FL improves accuracy, convergence speed, and robustness, even in adversarial and low-bid interference scenarios. Our results highlight the importance of balancing data reliability, incentive compatibility, and cost efficiency to enable scalable and trustworthy FL deployments.', 'abstract_zh': '跨孤岛联邦学习中的Shapley报价声誉优化联邦学习（SBRO-FL）', 'title_zh': '基于动态客户端选择的联邦学习中数据质量递减问题解决方案'}
{'arxiv_id': 'arXiv:2505.21218', 'title': 'Pretrained LLMs Learn Multiple Types of Uncertainty', 'authors': 'Roi Cohen, Omri Fahn, Gerard de Melo', 'link': 'https://arxiv.org/abs/2505.21218', 'abstract': "Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks. Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text. In this work, we study how well LLMs capture uncertainty, without explicitly being trained for that. We show that, if considering uncertainty as a linear concept in the model's latent space, it might indeed be captured, even after only pretraining. We further show that, though unintuitive, LLMs appear to capture several different types of uncertainty, each of which can be useful to predict the correctness for a specific task or benchmark. Furthermore, we provide in-depth results such as demonstrating a correlation between our correction prediction and the model's ability to abstain from misinformation using words, and the lack of impact of model scaling for capturing uncertainty. Finally, we claim that unifying the uncertainty types as a single one using instruction-tuning or [IDK]-token tuning is helpful for the model in terms of correctness prediction.", 'abstract_zh': '大型语言模型known to捕获现实世界知识，使其在许多下游任务中表现出色。尽管最近取得了进展，这些模型仍然容易出现常见的幻觉现象，导致它们产生不必要的和事实错误的文本。在这项工作中，我们研究了大型语言模型在未明确训练的情况下捕捉不确定性的能力。我们展示，在考虑不确定性作为模型潜空间中的线性概念时，其确实可能被捕捉，即使是在预训练后。我们进一步展示，尽管不太直观，大型语言模型似乎捕捉了多种不同类型的不确定性，每种类型对特定任务或基准的正确性预测都有所助益。此外，我们提供了深入的结果，如证明我们的更正预测与模型使用词汇避免传播虚假信息的能力之间的相关性，以及模型规模对捕捉不确定性的影响甚微。最后，我们提出，通过指令调优或[IDK]标记调优将不同类型的不确定性统一为一种，有助于模型在正确性预测方面的表现。', 'title_zh': '预训练大语言模型学习多种类型的不确定性'}
{'arxiv_id': 'arXiv:2505.21190', 'title': 'Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation', 'authors': 'Jong Hak Moon, Geon Choi, Paloma Rabaey, Min Gwan Kim, Hyuk Gi Hong, Jung-Oh Lee, Hangyul Yoon, Eun Woo Doe, Jiyoun Kim, Harshita Sharma, Daniel C. Castro, Javier Alvarez-Valle, Edward Choi', 'link': 'https://arxiv.org/abs/2505.21190', 'abstract': 'Radiology reports convey detailed clinical observations and capture diagnostic reasoning that evolves over time. However, existing evaluation methods are limited to single-report settings and rely on coarse metrics that fail to capture fine-grained clinical semantics and temporal dependencies. We introduce LUNGUAGE,a benchmark dataset for structured radiology report generation that supports both single-report evaluation and longitudinal patient-level assessment across multiple studies. It contains 1,473 annotated chest X-ray reports, each reviewed by experts, and 80 of them contain longitudinal annotations to capture disease progression and inter-study intervals, also reviewed by experts. Using this benchmark, we develop a two-stage framework that transforms generated reports into fine-grained, schema-aligned structured representations, enabling longitudinal interpretation. We also propose LUNGUAGESCORE, an interpretable metric that compares structured outputs at the entity, relation, and attribute level while modeling temporal consistency across patient timelines. These contributions establish the first benchmark dataset, structuring framework, and evaluation metric for sequential radiology reporting, with empirical results demonstrating that LUNGUAGESCORE effectively supports structured report evaluation. The code is available at: this https URL', 'abstract_zh': '放射学报告传达详细的临床观察并捕捉随时间演变的诊断推理。然而，现有的评估方法仅限于单报告设置，并依赖于粗粒度指标，无法捕捉细微的临床语义和时间依赖性。我们引入了LUNGUAGE，一个支持单报告评估和多研究纵向患者水平评估的结构化放射学报告生成基准数据集。该数据集包含1,473份标注的胸部X光片报告，每份报告均由专家审阅，其中80份包含纵向标注以捕捉疾病进展和研究间间隔，同样由专家审阅。使用该基准数据集，我们开发了一种两阶段框架，将生成的报告转换为细粒度、模式对齐的结构化表示，从而实现纵向解释。我们还提出了LUNGUAGESCORE，这是一种可解释的指标，用于在实体、关系和属性级别比较结构化输出，并建模患者时间线上的时间一致性。这些贡献建立了第一个基准数据集、结构化框架和评估指标，用于序列放射学报告，并通过实验证明LUNGUAGESCORE有效支持结构化报告评估。代码可在以下链接获取：this https URL', 'title_zh': '肺语：胸片结构化和序贯解释基准'}
{'arxiv_id': 'arXiv:2505.21189', 'title': 'Exploring the Latent Capacity of LLMs for One-Step Text Generation', 'authors': 'Gleb Mezentsev, Ivan Oseledets', 'link': 'https://arxiv.org/abs/2505.21189', 'abstract': 'A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression. We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for a given text, they form connected and local regions in embedding space - a property that suggests the potential of learning a dedicated encoder into that space.', 'abstract_zh': '近期研究表明，大型语言模型（LLMs）可以通过自回归生成从单一经过特殊训练的输入嵌入中重建出令人惊讶地长的文本，多达数千个标记。在本工作中，我们探讨是否可以在没有自回归的情况下实现这种重建。我们展示，在仅提供两个学习嵌入的情况下，冻结的LLMs能够在单次前向传播中生成数百个准确的标记。这揭示了LLMs的一个令人惊讶且未充分探索的能力——在无迭代解码的情况下多标记生成。我们研究这些嵌入的行为，并提供它们所编码信息类型的见解。我们还通过实验表明，虽然这些表示对于给定的文本不是唯一的，但它们在嵌入空间中形成了连接的和局部的区域——这一性质暗示了学习进入该空间的专用编码器的潜力。', 'title_zh': '探索LLMs在一步文本生成中的潜在能力'}
{'arxiv_id': 'arXiv:2505.21184', 'title': 'PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing', 'authors': 'Yu Yan, Sheng Sun, Zhifei Zheng, Ziji Hao, Teli Liu, Min Liu', 'link': 'https://arxiv.org/abs/2505.21184', 'abstract': 'To construct responsible and secure AI applications, harmful information data is widely utilized for adversarial testing and the development of safeguards. Existing studies mainly leverage Large Language Models (LLMs) to synthesize data to obtain high-quality task datasets at scale, thereby avoiding costly human annotation. However, limited by the safety alignment mechanisms of LLMs, the synthesis of harmful data still faces challenges in generation reliability and content diversity. In this study, we propose a novel harmful information synthesis framework, PoisonSwarm, which applies the model crowdsourcing strategy to generate diverse harmful data while maintaining a high success rate. Specifically, we generate abundant benign data as the based templates in a counterfactual manner. Subsequently, we decompose each based template into multiple semantic units and perform unit-by-unit toxification and final refinement through dynamic model switching, thus ensuring the success of synthesis. Experimental results demonstrate that PoisonSwarm achieves state-of-the-art performance in synthesizing different categories of harmful data with high scalability and diversity.', 'abstract_zh': '为了构建负责且安全的AI应用，有害信息数据广泛用于对抗测试和安全防护的发展。现有研究主要利用大型语言模型（LLMs）生成数据以获得大规模的高质量任务数据集，从而避免昂贵的人工标注。然而，受限于LLMs的安全对齐机制，有害数据的生成可靠性与内容多样性仍面临挑战。在本研究中，我们提出了一种新颖的有害信息合成框架PoisonSwarm，该框架采用模型众包策略生成多样性有害数据并维持高成功率。具体而言，我们通过反事实方式生成丰富的良性基础模板。随后，我们将每个基础模板分解成多个语义单元，并通过动态模型切换逐个单元进行毒化最后精炼，从而确保合成的成功。实验结果表明，PoisonSwarm在合成不同类别的有害数据方面具有卓越的可扩展性和多样性。', 'title_zh': 'PoisonSwarm: 通过模型众包合成通用有害信息'}
{'arxiv_id': 'arXiv:2505.21182', 'title': 'Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations', 'authors': 'Huy Hoang, Tien Mai, Pradeep Varakantham, Tanvi Verma', 'link': 'https://arxiv.org/abs/2505.21182', 'abstract': 'Offline imitation learning typically learns from expert and unlabeled demonstrations, yet often overlooks the valuable signal in explicitly undesirable behaviors. In this work, we study offline imitation learning from contrasting behaviors, where the dataset contains both expert and undesirable demonstrations. We propose a novel formulation that optimizes a difference of KL divergences over the state-action visitation distributions of expert and undesirable (or bad) data. Although the resulting objective is a DC (Difference-of-Convex) program, we prove that it becomes convex when expert demonstrations outweigh undesirable demonstrations, enabling a practical and stable non-adversarial training objective. Our method avoids adversarial training and handles both positive and negative demonstrations in a unified framework. Extensive experiments on standard offline imitation learning benchmarks demonstrate that our approach consistently outperforms state-of-the-art baselines.', 'abstract_zh': '离线模仿学习通常从专家和未标注的演示中学习，但往往会忽视明显不良行为中的有价值信号。在本文中，我们研究从对立行为中进行离线模仿学习，其中数据集包含专家和不良演示。我们提出了一种新的公式化方法，该方法在专家和不良（或糟糕）数据的状态-动作访问分布之间的KL距离差上进行优化。尽管所得目标是DC（凸性的差）规划，但我们证明当专家演示多于不良演示时，它变为凸的，从而使得一个实际可行且稳定的非对抗性训练目标成为可能。我们的方法避免了对抗性训练，并在一个统一框架中处理正负演示。在标准的离线模仿学习基准上的广泛实验表明，我们的方法在所有基准上都优于最先进的基线方法。', 'title_zh': '学习做什么和不做什么：从专家和不良示范中进行离线模仿学习'}
{'arxiv_id': 'arXiv:2505.21180', 'title': 'Latent label distribution grid representation for modeling uncertainty', 'authors': 'ShuNing Sun, YinSong Xiong, Yu Zhang, Zhuoran Zheng', 'link': 'https://arxiv.org/abs/2505.21180', 'abstract': 'Although \\textbf{L}abel \\textbf{D}istribution \\textbf{L}earning (LDL) has promising representation capabilities for characterizing the polysemy of an instance, the complexity and high cost of the label distribution annotation lead to inexact in the construction of the label space. The existence of a large number of inexact labels generates a label space with uncertainty, which misleads the LDL algorithm to yield incorrect decisions. To alleviate this problem, we model the uncertainty of label distributions by constructing a \\textbf{L}atent \\textbf{L}abel \\textbf{D}istribution \\textbf{G}rid (LLDG) to form a low-noise representation space. Specifically, we first construct a label correlation matrix based on the differences between labels, and then expand each value of the matrix into a vector that obeys a Gaussian distribution, thus building a LLDG to model the uncertainty of the label space. Finally, the LLDG is reconstructed by the LLDG-Mixer to generate an accurate label distribution. Note that we enforce a customized low-rank scheme on this grid, which assumes that the label relations may be noisy and it needs to perform noise-reduction with the help of a Tucker reconstruction technique. Furthermore, we attempt to evaluate the effectiveness of the LLDG by considering its generation as an upstream task to achieve the classification of the objects. Extensive experimental results show that our approach performs competitively on several benchmarks.', 'abstract_zh': '尽管标签分布学习（Label Distribution Learning, LDL）在刻画实例的多义性方面展现了强大的表示能力，但由于标签分布标注的复杂性和高成本导致标签空间构建不准确。大量不准确的标签的存在使得标签空间具有不确定性，从而误导LDL算法作出错误决策。为解决这一问题，我们通过构建隐标签分布网格（Latent Label Distribution Grid, LLDG）来建模标签分布的不确定性，从而形成低噪声的表示空间。具体而言，我们首先基于标签之间的差异构建标签相关矩阵，然后将矩阵中的每个值扩展为遵循高斯分布的向量，从而构建LLDG来建模标签空间的不确定性。最后，通过LLDG-Mixer重构LLDG生成准确的标签分布。值得注意的是，我们在该网格上施加了一个定制的低秩方案，假设标签关系可能是噪声的，并借助Tucker重建技术进行噪声减少。此外，我们尝试通过将LLDG的生成视为上游任务来实现对象分类，以评估其有效性。广泛的经验结果表明，我们的方法在多个基准上表现竞争性。', 'title_zh': '潜在标签分布网格表示建模不确定性'}
{'arxiv_id': 'arXiv:2505.21171', 'title': 'M-Wanda: Improving One-Shot Pruning for Multilingual LLMs', 'authors': 'Rochelle Choenni, Ivan Titov', 'link': 'https://arxiv.org/abs/2505.21171', 'abstract': 'Multilingual LLM performance is often critically dependent on model size. With an eye on efficiency, this has led to a surge in interest in one-shot pruning methods that retain the benefits of large-scale pretraining while shrinking the model size. However, as pruning tends to come with performance loss, it is important to understand the trade-offs between multilinguality and sparsification. In this work, we study multilingual performance under different sparsity constraints and show that moderate ratios already substantially harm performance. To help bridge this gap, we propose M-Wanda, a pruning method that models cross-lingual variation by incorporating language-aware activation statistics into its pruning criterion and dynamically adjusts layerwise sparsity based on cross-lingual importance. We show that M-Wanda consistently improves performance at minimal additional costs. We are the first to explicitly optimize pruning to retain multilingual performance, and hope to inspire future advances in multilingual pruning.', 'abstract_zh': '多语言LLM性能往往高度依赖模型规模。出于效率考虑，这使得人们对能够在保持大规模预训练优势的同时缩小模型规模的一次性剪枝方法产生了浓厚兴趣。然而，由于剪枝往往伴随着性能下降，理解多语言能力和稀疏化之间的权衡变得至关重要。在这项工作中，我们研究了在不同稀疏约束下的多语言性能，并表明适度的稀疏性比率已经显著损害了性能。为了弥合这一差距，我们提出了一种名为M-Wanda的剪枝方法，该方法通过将语言感知激活统计融入剪枝标准，并基于跨语言重要性动态调整逐层稀疏性，来建模跨语言变异。我们展示了M-Wanda能够在最小额外成本下一致地提升性能。我们首次明确优化剪枝以保留多语言性能，并希望激励未来在多语言剪枝方面的进步。', 'title_zh': 'M-Wanda: 提高多语言LLM的一次性剪枝方法'}
{'arxiv_id': 'arXiv:2505.21170', 'title': 'Quantum AIXI: Universal Intelligence via Quantum Information', 'authors': 'Elija Perrier', 'link': 'https://arxiv.org/abs/2505.21170', 'abstract': 'AIXI is a widely studied model of artificial general intelligence (AGI) based upon principles of induction and reinforcement learning. However, AIXI is fundamentally classical in nature - as are the environments in which it is modelled. Given the universe is quantum mechanical in nature and the exponential overhead required to simulate quantum mechanical systems classically, the question arises as to whether there are quantum mechanical analogues of AIXI which are theoretically consistent or practically feasible as models of universal intelligence. To address this question, we extend the framework to quantum information and present Quantum AIXI (QAIXI). We introduce a model of quantum agent/environment interaction based upon quantum and classical registers and channels, showing how quantum AIXI agents may take both classical and quantum actions. We formulate the key components of AIXI in quantum information terms, extending previous research on quantum Kolmogorov complexity and a QAIXI value function. We discuss conditions and limitations upon quantum Solomonoff induction and show how contextuality fundamentally affects QAIXI models.', 'abstract_zh': '量子AIXI（Quantum AIXI）：基于量子信息的通用人工智能模型', 'title_zh': '量子AIXI：通过量子信息实现的通用智能'}
{'arxiv_id': 'arXiv:2505.21160', 'title': 'STEB: In Search of the Best Evaluation Approach for Synthetic Time Series', 'authors': 'Michael Stenger, Robert Leppich, André Bauer, Samuel Kounev', 'link': 'https://arxiv.org/abs/2505.21160', 'abstract': 'The growing need for synthetic time series, due to data augmentation or privacy regulations, has led to numerous generative models, frameworks, and evaluation measures alike. Objectively comparing these measures on a large scale remains an open challenge. We propose the Synthetic Time series Evaluation Benchmark (STEB) -- the first benchmark framework that enables comprehensive and interpretable automated comparisons of synthetic time series evaluation measures. Using 10 diverse datasets, randomness injection, and 13 configurable data transformations, STEB computes indicators for measure reliability and score consistency. It tracks running time, test errors, and features sequential and parallel modes of operation. In our experiments, we determine a ranking of 41 measures from literature and confirm that the choice of upstream time series embedding heavily impacts the final score.', 'abstract_zh': '合成时间序列评估基准（STEB）：首个全面可解释的合成时间序列评估指标自动对比框架', 'title_zh': 'STEB：搜索最适合合成时间序列的评估方法'}
{'arxiv_id': 'arXiv:2505.21156', 'title': 'Model as Loss: A Self-Consistent Training Paradigm', 'authors': 'Saisamarth Rajesh Phaye, Milos Cernak, Andrew Harper', 'link': 'https://arxiv.org/abs/2505.21156', 'abstract': "Conventional methods for speech enhancement rely on handcrafted loss functions (e.g., time or frequency domain losses) or deep feature losses (e.g., using WavLM or wav2vec), which often fail to capture subtle signal properties essential for optimal performance. To address this, we propose Model as Loss, a novel training paradigm that utilizes the encoder from the same model as a loss function to guide the training.\nThe Model as Loss paradigm leverages the encoder's task-specific feature space, optimizing the decoder to produce output consistent with perceptual and task-relevant characteristics of the clean signal. By using the encoder's learned features as a loss function, this framework enforces self-consistency between the clean reference speech and the enhanced model output. Our approach outperforms pre-trained deep feature losses on standard speech enhancement benchmarks, offering better perceptual quality and robust generalization to both in-domain and out-of-domain datasets.", 'abstract_zh': '基于模型的损失函数在语音增强中的应用：一种新颖的训练范式', 'title_zh': '模型即损失：一种自我一致的训练范式'}
{'arxiv_id': 'arXiv:2505.21154', 'title': 'GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender Simulation', 'authors': 'Hailin Zhong, Hanlin Wang, Yujun Ye, Meiyi Zhang, Shengxin Zhu', 'link': 'https://arxiv.org/abs/2505.21154', 'abstract': "Current personalized recommender systems predominantly rely on static offline data for algorithm design and evaluation, significantly limiting their ability to capture long-term user preference evolution and social influence dynamics in real-world scenarios. To address this fundamental challenge, we propose a high-fidelity social simulation platform integrating human-like cognitive agents and dynamic social interactions to realistically simulate user behavior evolution under recommendation interventions. Specifically, the system comprises a population of Sim-User Agents, each equipped with a five-layer cognitive architecture that encapsulates key psychological mechanisms, including episodic memory, affective state transitions, adaptive preference learning, and dynamic trust-risk assessments. In particular, we innovatively introduce the Intimacy--Curiosity--Reciprocity--Risk (ICR2) motivational engine grounded in psychological and sociological theories, enabling more realistic user decision-making processes. Furthermore, we construct a multilayer heterogeneous social graph (GGBond Graph) supporting dynamic relational evolution, effectively modeling users' evolving social ties and trust dynamics based on interest similarity, personality alignment, and structural homophily. During system operation, agents autonomously respond to recommendations generated by typical recommender algorithms (e.g., Matrix Factorization, MultVAE, LightGCN), deciding whether to consume, rate, and share content while dynamically updating their internal states and social connections, thereby forming a stable, multi-round feedback loop. This innovative design transcends the limitations of traditional static datasets, providing a controlled, observable environment for evaluating long-term recommender effects.", 'abstract_zh': '当前个性化推荐系统主要依赖静态离线数据进行算法设计和评估，极大地限制了其捕捉用户长期偏好演变和社会影响力动态的能力。为了应对这一根本性挑战，我们提出了一种高度真实的社会仿真平台，集成类人认知代理和动态社会互动，以真实模拟推荐干预下的用户行为演变。该系统包括一个Sim-User Agent群体，每个代理均配备五层认知架构，涵盖关键的心理机制，包括情景记忆、情绪状态转换、适应性偏好学习和动态信任风险评估。特别地，我们根据心理学和社会学理论创新性地引入了亲密性-好奇心-互惠-风险（ICR2）动机引擎，使用户决策过程更加真实。此外，我们构建了一个多层异质社会图（GGBond图），支持动态关系演变，有效地基于兴趣相似度、个性匹配和结构同质性建模用户的社交联系和信任动态。在系统运行期间，代理自主响应典型推荐算法（如矩阵分解、MultVAE、LightGCN）生成的推荐，决定是否消费、评价和分享内容，并动态更新其内部状态和社会联系，从而形成一个稳定、多轮的反馈循环。这种创新设计超越了传统静态数据集的限制，提供了一个可控、可观测的环境来评估推荐系统的长期效果。', 'title_zh': 'GGBond：基于图的AI代理社会增长模型Socially-aware推荐模拟'}
{'arxiv_id': 'arXiv:2505.21140', 'title': 'HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs', 'authors': 'Honglin Gao, Xiang Li, Lan Zhao, Gaoxi Xiao', 'link': 'https://arxiv.org/abs/2505.21140', 'abstract': "Heterogeneous graph neural networks (HGNNs) have recently drawn increasing attention for modeling complex multi-relational data in domains such as recommendation, finance, and social networks. While existing research has been largely focused on enhancing HGNNs' predictive performance, their robustness and security, especially under backdoor attacks, remain underexplored. In this paper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework for node classification tasks on heterogeneous graphs. HeteroBA inserts carefully crafted trigger nodes with realistic features and targeted structural connections, leveraging attention-based and clustering-based strategies to select influential auxiliary nodes for effective trigger propagation, thereby causing the model to misclassify specific nodes into a target label while maintaining accuracy on clean data. Experimental results on three datasets and various HGNN architectures demonstrate that HeteroBA achieves high attack success rates with minimal impact on the clean accuracy. Our method sheds light on potential vulnerabilities in HGNNs and calls for more robust defenses against backdoor threats in multi-relational graph scenarios.", 'abstract_zh': '异构图神经网络中的新型节点分类后门攻击框架（HeteroBA）', 'title_zh': 'HeteroBA：一种针对异构图的结构操控后门攻击'}
{'arxiv_id': 'arXiv:2505.21136', 'title': 'SageAttention2++: A More Efficient Implementation of SageAttention2', 'authors': 'Jintao Zhang, Xiaoming Xu, Jia Wei, Haofeng Huang, Pengle Zhang, Chendong Xiang, Jun Zhu, Jianfei Chen', 'link': 'https://arxiv.org/abs/2505.21136', 'abstract': 'The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at this https URL.', 'abstract_zh': 'SageAttention2++通过利用FP8 Matmul在FP16中积累的更快指令实现加速，保持与SageAttention2相同的注意力准确性，速度提升3.9倍，对语言、图像和视频生成等各种模型实现有效加速，端到端指标损失可忽略不计。代码将在以下网址获取。', 'title_zh': 'SageAttention2++: 更高效的SageAttention2实现'}
{'arxiv_id': 'arXiv:2505.21119', 'title': 'Universal Value-Function Uncertainties', 'authors': 'Moritz A. Zanger, Max Weltevrede, Yaniv Oren, Pascal R. Van der Vaart, Caroline Horsch, Wendelin Böhmer, Matthijs T. J. Spaan', 'link': 'https://arxiv.org/abs/2505.21119', 'abstract': 'Estimating epistemic uncertainty in value functions is a crucial challenge for many aspects of reinforcement learning (RL), including efficient exploration, safe decision-making, and offline RL. While deep ensembles provide a robust method for quantifying value uncertainty, they come with significant computational overhead. Single-model methods, while computationally favorable, often rely on heuristics and typically require additional propagation mechanisms for myopic uncertainty estimates. In this work we introduce universal value-function uncertainties (UVU), which, similar in spirit to random network distillation (RND), quantify uncertainty as squared prediction errors between an online learner and a fixed, randomly initialized target network. Unlike RND, UVU errors reflect policy-conditional value uncertainty, incorporating the future uncertainties any given policy may encounter. This is due to the training procedure employed in UVU: the online network is trained using temporal difference learning with a synthetic reward derived from the fixed, randomly initialized target network. We provide an extensive theoretical analysis of our approach using neural tangent kernel (NTK) theory and show that in the limit of infinite network width, UVU errors are exactly equivalent to the variance of an ensemble of independent universal value functions. Empirically, we show that UVU achieves equal performance to large ensembles on challenging multi-task offline RL settings, while offering simplicity and substantial computational savings.', 'abstract_zh': '估计价值函数的知识不确定性是强化学习（RL）许多方面的关键挑战，包括高效探索、安全决策和 Offline RL。尽管深集成提供了一种稳健的方法来量化价值不确定性，但它们伴随着显著的计算开销。单模型方法虽然计算上更具优势，但往往依赖于启发式方法，并且通常需要额外的传播机制来处理短视的不确定性估计。在本文中，我们引入了通用价值函数不确定性（UVU），类似于随机网络蒸馏（RND），通过在线学习者与固定且随机初始化的目标网络之间的预测误差平方来量化不确定性。与 RND 不同，UVU 的误差反映了策略条件下的价值不确定性，包括任何给定策略可能遇到的未来不确定性。这归因于 UVU 的训练过程：在线网络使用基于固定且随机初始化的目标网络的合成奖励通过时差学习进行训练。我们使用神经 tangent 核（NTK）理论对我们的方法进行了详尽的理论分析，并证明在网络宽度无限的情况下，UVU 的误差恰好等同于独立通用价值函数集成的方差。实证结果表明，在复杂多任务 Offline RL 设置中，UVU 达到与大型集成同等的性能，同时具有简单性和显著的计算成本节约。', 'title_zh': '普遍的价值函数不确定性'}
{'arxiv_id': 'arXiv:2505.21116', 'title': 'Creativity in LLM-based Multi-Agent Systems: A Survey', 'authors': 'Yi-Cheng Lin, Kang-Chieh Chen, Zhe-Yan Li, Tzu-Heng Wu, Tzu-Hsuan Wu, Kuan-Yu Chen, Hung-yi Lee, Yun-Nung Chen', 'link': 'https://arxiv.org/abs/2505.21116', 'abstract': 'Large language model (LLM)-driven multi-agent systems (MAS) are transforming how humans and AIs collaboratively generate ideas and artifacts. While existing surveys provide comprehensive overviews of MAS infrastructures, they largely overlook the dimension of \\emph{creativity}, including how novel outputs are generated and evaluated, how creativity informs agent personas, and how creative workflows are coordinated. This is the first survey dedicated to creativity in MAS. We focus on text and image generation tasks, and present: (1) a taxonomy of agent proactivity and persona design; (2) an overview of generation techniques, including divergent exploration, iterative refinement, and collaborative synthesis, as well as relevant datasets and evaluation metrics; and (3) a discussion of key challenges, such as inconsistent evaluation standards, insufficient bias mitigation, coordination conflicts, and the lack of unified benchmarks. This survey offers a structured framework and roadmap for advancing the development, evaluation, and standardization of creative MAS.', 'abstract_zh': '大型语言模型驱动的多智能体系统中的创造性研究：文本和图像生成任务中的代理主动性与人设设计、生成技术及其挑战调研', 'title_zh': '基于LLM的多智能体系统中的创造力：一个综述'}
{'arxiv_id': 'arXiv:2505.21109', 'title': 'A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction', 'authors': 'Bogdan Bogachov, Yaoyao Fiona Zhao', 'link': 'https://arxiv.org/abs/2505.21109', 'abstract': 'Despite recent advancements in domain adaptation techniques for large language models, these methods remain computationally intensive, and the resulting models can still exhibit hallucination issues. Most existing adaptation methods do not prioritize reducing the computational resources required for fine-tuning and inference of language models. Hallucination issues have gradually decreased with each new model release. However, they remain prevalent in engineering contexts, where generating well-structured text with minimal errors and inconsistencies is critical. This work introduces a novel approach called the Small Language Graph (SLG), which is a lightweight adaptation solution designed to address the two key challenges outlined above. The system is structured in the form of a graph, where each node represents a lightweight expert - a small language model fine-tuned on specific and concise texts. The results of this study have shown that SLG was able to surpass conventional fine-tuning methods on the Exact Match metric by 3 times. Additionally, the fine-tuning process was 1.7 times faster compared to that of a larger stand-alone language model. These findings introduce a potential for small to medium-sized engineering companies to confidently use generative AI technologies, such as LLMs, without the necessity to invest in expensive computational resources. Also, the graph architecture and the small size of expert nodes offer a possible opportunity for distributed AI systems, thus potentially diverting the global need for expensive centralized compute clusters.', 'abstract_zh': '尽管近年来在大规模语言模型领域适应技术方面取得了进展，这些方法仍然计算密集，生成的模型仍可能存在幻觉问题。现有的大多数适应方法并未优先考虑减少语言模型微调和推理所需的计算资源。幻觉问题在每个新模型发布时有所降低，但在工程领域仍然普遍存在，因为在该领域生成结构良好、错误和不一致性最少的文本至关重要。本工作提出了一种名为小型语言图（SLG）的新颖方法，这是一种轻量级适应解决方案，旨在解决上述两个关键挑战。该系统以图的形式结构化，其中每个节点代表一个轻量级专家——针对特定和简明的文本微调的小型语言模型。研究结果表明，SLG在Exact Match指标上超越了传统微调方法，提升了3倍，且微调过程比单一的大规模语言模型快1.7倍。这些发现为中小规模的工COMPANY提供了一种机会，使他们能够自信地使用生成式AI技术，如LLMs，无需投资昂贵的计算资源。此外，图结构和专家节点的小尺寸为分布式AI系统提供了可能的机会，从而有可能减轻全球对昂贵集中式计算集群的需求。', 'title_zh': '一种轻量级多专家生成语言模型系统，用于工程信息与知识提取'}
{'arxiv_id': 'arXiv:2505.21097', 'title': 'Thinker: Learning to Think Fast and Slow', 'authors': 'Stephen Chung, Wenyu Du, Jie Fu', 'link': 'https://arxiv.org/abs/2505.21097', 'abstract': 'Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training.', 'abstract_zh': 'Recent studies show that通过强化学习在数学和编程等领域的问答任务中可以提高大型语言模型的推理能力。受心理学的双重过程理论启发，我们在问答任务中引入了一个简单的四阶段修改：快速思考阶段，模型必须在严格的令牌预算内作答；验证阶段，模型评估其初始回答；缓慢思考阶段，模型对初始回答进行更深入的修正；总结阶段，模型将前一阶段的修正提炼为精确的步骤。我们的任务改进了Qwen2.5-1.5B的平均准确率从24.9%提高到27.9%，DeepSeek-R1-Qwen-1.5B的平均准确率从45.9%提高到49.8%。值得注意的是，对于Qwen2.5-1.5B，仅快速思考模式在使用不到1000个令牌的情况下就达到了26.8%的准确率，显示了显著的推理效率提升。这些发现表明，直觉和审慎推理是两个不同且互补的系统，可以通过针对性的训练从中受益。', 'title_zh': 'Thinker: 学会快速与深入思考'}
{'arxiv_id': 'arXiv:2505.21092', 'title': 'BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge', 'authors': 'Daeen Kabir, Minhajur Rahman Chowdhury Mahim, Sheikh Shafayat, Adnan Sadik, Arian Ahmed, Eunsu Kim, Alice Oh', 'link': 'https://arxiv.org/abs/2505.21092', 'abstract': "In this work, we introduce BLUCK, a new dataset designed to measure the performance of Large Language Models (LLMs) in Bengali linguistic understanding and cultural knowledge. Our dataset comprises 2366 multiple-choice questions (MCQs) carefully curated from compiled collections of several college and job level examinations and spans 23 categories covering knowledge on Bangladesh's culture and history and Bengali linguistics. We benchmarked BLUCK using 6 proprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that while these models perform reasonably well overall, they, however, struggles in some areas of Bengali phonetics. Although current LLMs' performance on Bengali cultural and linguistic contexts is still not comparable to that of mainstream languages like English, our results indicate Bengali's status as a mid-resource language. Importantly, BLUCK is also the first MCQ-based evaluation benchmark that is centered around native Bengali culture, history, and linguistics.", 'abstract_zh': '在本项工作中，我们介绍了BLUCK，一个旨在衡量大型语言模型（LLMs）在孟加拉语语言理解和文化知识方面 performance 的新数据集。该数据集包含2366个仔细筛选的多项选择题（MCQs），来源于多种大学和职业水平考试的汇总集合，覆盖23个类别，涵盖了孟加拉国的文化和历史以及孟加拉语语言学知识。我们使用6个专有和3个开源的大规模语言模型（包括GPT-4o、Claude-3.5-Sonnet、Gemini-1.5-Pro、Llama-3.3-70B-Instruct和DeepSeekV3）对标BLUCK。我们的结果显示，尽管这些模型在整体表现上表现合理，但在某些孟加拉语音系方面却表现出一些困难。虽然当前LLMs在孟加拉语文化与语言语境上的表现仍不及主流语言如英语，但我们的结果显示孟加拉语是一种中等资源语言。值得注意的是，BLUCK也是首个以原生孟加拉文化、历史和语言学为核心的多项选择题评价基准。', 'title_zh': 'BLUCK: 印度孟加拉语语言理解与文化知识基准数据集'}
{'arxiv_id': 'arXiv:2505.21091', 'title': 'Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)', 'authors': 'Anna Neumann, Elisabeth Kirsten, Muhammad Bilal Zafar, Jatinder Singh', 'link': 'https://arxiv.org/abs/2505.21091', 'abstract': "System prompts in Large Language Models (LLMs) are predefined directives that guide model behaviour, taking precedence over user inputs in text processing and generation. LLM deployers increasingly use them to ensure consistent responses across contexts. While model providers set a foundation of system prompts, deployers and third-party developers can append additional prompts without visibility into others' additions, while this layered implementation remains entirely hidden from end-users. As system prompts become more complex, they can directly or indirectly introduce unaccounted for side effects. This lack of transparency raises fundamental questions about how the position of information in different directives shapes model outputs. As such, this work examines how the placement of information affects model behaviour. To this end, we compare how models process demographic information in system versus user prompts across six commercially available LLMs and 50 demographic groups. Our analysis reveals significant biases, manifesting in differences in user representation and decision-making scenarios. Since these variations stem from inaccessible and opaque system-level configurations, they risk representational, allocative and potential other biases and downstream harms beyond the user's ability to detect or correct. Our findings draw attention to these critical issues, which have the potential to perpetuate harms if left unexamined. Further, we argue that system prompt analysis must be incorporated into AI auditing processes, particularly as customisable system prompts become increasingly prevalent in commercial AI deployments.", 'abstract_zh': '大型语言模型中的系统提示是在文本处理和生成过程中优先于用户输入的预定义指令，用于引导模型行为。部署者越来越多地使用系统提示以确保不同上下文中的响应一致性。尽管模型提供者设置了系统提示的基础，但部署者和第三方开发者可以在不看到其他人添加的情况下附加其他提示，而这一层叠的实现完全对最终用户隐藏。随着系统提示变得越来越复杂，它们可能会直接或间接地引入未被考虑的副作用。这种透明度缺失引发了关于不同指令中信息位置如何塑造模型输出的基本问题。因此，本研究探讨了信息位置如何影响模型行为。为此，我们比较了六种商业上可用的大语言模型和50个 demographic 组群中系统提示和用户提示在处理人口统计信息方面的差异。我们的分析揭示了显著的偏差，表现为用户代表性和决策场景的差异。由于这些差异源自不可访问且不透明的系统级配置，它们可能涉及代表、分配和其他潜在偏差及下游危害，而用户很难检测或纠正这些危害。本研究结果引起了对这些关键问题的关注，如果未经审视，它们可能会延续危害。此外，我们认为系统提示分析必须整合到人工智能审计过程中，特别是在可自定义系统提示在商用人工智能部署中越来越普遍的情况下。', 'title_zh': '位置即是力量：系统提示作为大型语言模型（LLMs）偏差的机制'}
{'arxiv_id': 'arXiv:2505.21087', 'title': 'Stopping Criteria for Value Iteration on Concurrent Stochastic Reachability and Safety Games', 'authors': 'Marta Grobelna, Jan Křetínský, Maximilian Weininger', 'link': 'https://arxiv.org/abs/2505.21087', 'abstract': 'We consider two-player zero-sum concurrent stochastic games (CSGs) played on graphs with reachability and safety objectives. These include degenerate classes such as Markov decision processes or turn-based stochastic games, which can be solved by linear or quadratic programming; however, in practice, value iteration (VI) outperforms the other approaches and is the most implemented method. Similarly, for CSGs, this practical performance makes VI an attractive alternative to the standard theoretical solution via the existential theory of reals.\nVI starts with an under-approximation of the sought values for each state and iteratively updates them, traditionally terminating once two consecutive approximations are $\\epsilon$-close. However, this stopping criterion lacks guarantees on the precision of the approximation, which is the goal of this work. We provide bounded (a.k.a. interval) VI for CSGs: it complements standard VI with a converging sequence of over-approximations and terminates once the over- and under-approximations are $\\epsilon$-close.', 'abstract_zh': '我们考虑在图上进行的两类玩家零和并发随机博弈（CSGs），这些博弈具有可达性和安全目标。这包括马尔可夫决策过程或轮流制随机博弈等退化类，这类问题可以通过线性或二次规划求解；然而在实践中，值迭代（VI）方法表现更优，并且是最常被实现的方法。类似地，在处理CSGs时，这种实用表现使得值迭代成为替代传统实存理论标准解法的有吸引力的选择。我们为CSGs提供有界（即区间）值迭代：这种迭代方法在标准的值迭代基础上加入了收敛的上近似序列，并且在上近似和下近似达到$\\epsilon$-接近时终止。', 'title_zh': '并发随机可达性和安全性博弈中值迭代的停止准则'}
{'arxiv_id': 'arXiv:2505.21077', 'title': 'Efficient Large Language Model Inference with Neural Block Linearization', 'authors': 'Mete Erdogan, Francesco Tonin, Volkan Cevher', 'link': 'https://arxiv.org/abs/2505.21077', 'abstract': 'The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce Neural Block Linearization (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs.', 'abstract_zh': '基于变压器的大语言模型（LLMs）的高度推理要求给其部署带来了重大挑战。为此，我们介绍了神经块线性化（NBL）这一新颖框架，通过使用线性最小均方误差估计器导出的线性近似来替换自注意力层，从而加速变压器模型的推理。NBL 利用典型相关分析计算线性近似误差的理论上限，然后使用此上限作为替换准则，选择线性化误差最低的LLM层。NBL 可以高效地应用于预训练的LLMs，无需微调。实验表明，NBL 在多个推理基准测试中保持了竞争力的同时实现了显著的计算速度提升。例如，将NBL 应用于DeepSeek-R1-Distill-Llama-8B 中的12个自注意力层，可将推理速度提高32%，仅有不到1%的精度损失，使其成为提高LLMs 推理效率的灵活而有前景的解决方案。', 'title_zh': '基于神经块线性化的高效大型语言模型推理'}
{'arxiv_id': 'arXiv:2505.21074', 'title': 'Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling', 'authors': 'Yichuan Cao, Yibo Miao, Xiao-Shan Gao, Yinpeng Dong', 'link': 'https://arxiv.org/abs/2505.21074', 'abstract': "Text-to-image (T2I) models raise ethical and safety concerns due to their potential to generate inappropriate or harmful images. Evaluating these models' security through red-teaming is vital, yet white-box approaches are limited by their need for internal access, complicating their use with closed-source models. Moreover, existing black-box methods often assume knowledge about the model's specific defense mechanisms, limiting their utility in real-world commercial API scenarios. A significant challenge is how to evade unknown and diverse defense mechanisms. To overcome this difficulty, we propose a novel Rule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively employs LLM to modify prompts to query and leverages feedback from T2I systems for fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a prior, enabling the LLM to dynamically adapt to unknown defense mechanisms. Given that the feedback is often labeled and coarse-grained, making it difficult to utilize directly, we further propose rule-based preference modeling, which employs a set of rules to evaluate desired or undesired feedback, facilitating finer-grained control over the LLM's dynamic adaptation process. Extensive experiments on nineteen T2I systems with varied safety mechanisms, three online commercial API services, and T2V models verify the superiority and practicality of our approach.", 'abstract_zh': '基于规则的偏好建模引导红队攻击（RPG-RT）：一种应对未知多样防御机制的文本到图像模型安全评估方法', 'title_zh': '基于规则的偏好建模红队测试文本到图像系统'}
{'arxiv_id': 'arXiv:2505.21061', 'title': 'LPOI: Listwise Preference Optimization for Vision Language Models', 'authors': 'Fatemeh Pesaran Zadeh, Yoojin Oh, Gunhee Kim', 'link': 'https://arxiv.org/abs/2505.21061', 'abstract': 'Aligning large VLMs with human preferences is a challenging task, as methods like RLHF and DPO often overfit to textual information or exacerbate hallucinations. Although augmenting negative image samples partially addresses these pitfalls, no prior work has employed listwise preference optimization for VLMs, due to the complexity and cost of constructing listwise image samples. In this work, we propose LPOI, the first object-aware listwise preference optimization developed for reducing hallucinations in VLMs. LPOI identifies and masks a critical object in the image, and then interpolates the masked region between the positive and negative images to form a sequence of incrementally more complete images. The model is trained to rank these images in ascending order of object visibility, effectively reducing hallucinations while retaining visual fidelity. LPOI requires no extra annotations beyond standard pairwise preference data, as it automatically constructs the ranked lists through object masking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and Object HalBench confirm that LPOI outperforms existing preference optimization methods in reducing hallucinations and enhancing VLM performance. We make the code available at this https URL.', 'abstract_zh': '将大规模VLM与人类偏好对齐是一项具有挑战性的任务，因为方法如RLHF和DPO往往过度拟合到文本信息或加剧幻觉现象。尽管部分通过扩充负样本图像可以解决这些问题，但先前工作未采用列表级偏好优化方法，主要是因为构建列表级图像样本的复杂性和成本。在本文中，我们提出LPOI，这是一种用于减少VLM幻觉现象的第一种对象感知列表级偏好优化方法。LPOI识别并屏蔽图像中的关键对象，然后在正图像和负图像之间插值屏蔽区域，形成逐渐更完整的图像序列。模型被训练为按对象可视度递增顺序对这些图像进行-ranking，从而有效减少幻觉现象同时保留视觉保真度。LPOI不需要额外的标注，因为它可以通过对象屏蔽和插值自动构建排名列表。在MMHalBench、AMBER和Object HalBench上的全面实验表明，LPOI在减少幻觉现象和增强VLM性能方面优于现有偏好优化方法。代码已发布在此HTTPS URL。', 'title_zh': '列级偏好优化for vision-language模型'}
{'arxiv_id': 'arXiv:2505.21046', 'title': 'A domain adaptation neural network for digital twin-supported fault diagnosis', 'authors': 'Zhenling Chen, Haiwei Fu, Zhiguo Zeng', 'link': 'https://arxiv.org/abs/2505.21046', 'abstract': 'Digital twins offer a promising solution to the lack of sufficient labeled data in deep learning-based fault diagnosis by generating simulated data for model training. However, discrepancies between simulation and real-world systems can lead to a significant drop in performance when models are applied in real scenarios. To address this issue, we propose a fault diagnosis framework based on Domain-Adversarial Neural Networks (DANN), which enables knowledge transfer from simulated (source domain) to real-world (target domain) data. We evaluate the proposed framework using a publicly available robotics fault diagnosis dataset, which includes 3,600 sequences generated by a digital twin model and 90 real sequences collected from physical systems. The DANN method is compared with commonly used lightweight deep learning models such as CNN, TCN, Transformer, and LSTM. Experimental results show that incorporating domain adaptation significantly improves the diagnostic performance. For example, applying DANN to a baseline CNN model improves its accuracy from 70.00% to 80.22% on real-world test data, demonstrating the effectiveness of domain adaptation in bridging the sim-to-real gap.', 'abstract_zh': '数字孪生提供的模拟数据可为基于深度学习的故障诊断问题提供充足的标记数据，并通过域对抗神经网络（DANN）实现模拟数据向真实数据的知识迁移，从而改善模型在实际场景中的性能。实验结果表明，引入域适应显著提高了诊断性能。例如，将DANN应用于基础的CNN模型，其在真实数据上的准确率从70.00%提高到80.22%，证明了域适应在弥合仿真实与真实场景差距方面的有效性。', 'title_zh': '基于数字孪生支持的域适应神经网络故障诊断'}
{'arxiv_id': 'arXiv:2505.21040', 'title': 'FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis', 'authors': 'Wei Chen, Zhao Zhang, Meng Yuan, Kepeng Xu, Fuzhen Zhuang', 'link': 'https://arxiv.org/abs/2505.21040', 'abstract': 'In this paper, we address the task of targeted sentiment analysis (TSA), which involves two sub-tasks, i.e., identifying specific aspects from reviews and determining their corresponding sentiments. Aspect extraction forms the foundation for sentiment prediction, highlighting the critical dependency between these two tasks for effective cross-task knowledge transfer. While most existing studies adopt a multi-task learning paradigm to align task-specific features in the latent space, they predominantly rely on coarse-grained knowledge transfer. Such approaches lack fine-grained control over aspect-sentiment relationships, often assuming uniform sentiment polarity within related aspects. This oversimplification neglects contextual cues that differentiate sentiments, leading to negative transfer. To overcome these limitations, we propose FCKT, a fine-grained cross-task knowledge transfer framework tailored for TSA. By explicitly incorporating aspect-level information into sentiment prediction, FCKT achieves fine-grained knowledge transfer, effectively mitigating negative transfer and enhancing task performance. Experiments on three datasets, including comparisons with various baselines and large language models (LLMs), demonstrate the effectiveness of FCKT. The source code is available on this https URL.', 'abstract_zh': '基于细粒度跨任务知识转移的 targeted 情感分析', 'title_zh': 'FCKT: 细粒度跨任务知识迁移与语义对比学习在目标情感分析中的应用'}
{'arxiv_id': 'arXiv:2505.21038', 'title': 'Fixed-Point Traps and Identity Emergence in Educational Feedback Systems', 'authors': 'Faruk Alpay', 'link': 'https://arxiv.org/abs/2505.21038', 'abstract': 'This paper presents a formal categorical proof that exam-driven educational systems obstruct identity emergence and block creative convergence. Using the framework of Alpay Algebra II and III, we define Exam-Grade Collapse Systems (EGCS) as functorial constructs where learning dynamics $\\varphi$ are recursively collapsed by evaluative morphisms $E$. We prove that under such collapse regimes, no nontrivial fixed-point algebra $\\mu_\\varphi$ can exist, hence learner identity cannot stabilize. This creates a universal fixed-point trap: all generative functors are entropically folded before symbolic emergence occurs. Our model mathematically explains the creativity suppression, research stagnation, and structural entropy loss induced by timed exams and grade-based feedback. The results apply category theory to expose why modern educational systems prevent {\\phi}-emergence and block observer-invariant self-formation. This work provides the first provable algebraic obstruction of identity formation caused by institutional feedback mechanics.', 'abstract_zh': '本文正式证明了以考试为导向的教育体系阻碍身份认同的形成并阻滞创造性汇聚。基于Alpay代数II和III框架，我们将评估导向坍塌系统(EGCS)定义为由评价态映射$E$递归坍塌学习动力学$\\varphi$的泛函构造。我们证明，在此类坍塌机制下，不存在非平凡的不动点代数$\\mu_\\varphi$，因而学习者身份无法稳定。这创建了一个普遍的不动点陷阱：所有生成泛函在符号出现之前均被熵性折叠。该模型从数学上解释了由定时考试和基于成绩的反馈所导致的创造力抑制、研究停滞和结构熵损失。本文将范畴论应用于揭示现代教育体系为何阻碍$\\phi$-形成并阻止观察者不变的自我形成。本工作提供了首个因机构反馈机制导致身份形成代数障碍的证明。', 'title_zh': '固定点陷阱与身份 emergence 在教育反馈系统中的探究'}
{'arxiv_id': 'arXiv:2505.21036', 'title': 'RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy', 'authors': 'Aiyue Chen, Bin Dong, Jingru Li, Jing Lin, Yiwu Yao, Gongyi Wang', 'link': 'https://arxiv.org/abs/2505.21036', 'abstract': 'Video generation using diffusion models is highly computationally intensive, with 3D attention in Diffusion Transformer (DiT) models accounting for over 80\\% of the total computational resources. In this work, we introduce {\\bf RainFusion}, a novel training-free sparse attention method that exploits inherent sparsity nature in visual data to accelerate attention computation while preserving video quality. Specifically, we identify three unique sparse patterns in video generation attention calculations--Spatial Pattern, Temporal Pattern and Textural Pattern. The sparse pattern for each attention head is determined online with negligible overhead (\\textasciitilde\\,0.2\\%) with our proposed {\\bf ARM} (Adaptive Recognition Module) during inference. Our proposed {\\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated into state-of-the-art 3D-attention video generation models without additional training or calibration. We evaluate our method on leading open-sourced models including HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its broad applicability and effectiveness. Experimental results show that RainFusion achieves over {\\bf 2\\(\\times\\)} speedup in attention computation while maintaining video quality, with only a minimal impact on VBench scores (-0.2\\%).', 'abstract_zh': '基于扩散模型的视频生成 highly 计算密集，其中 3D 注意力在 Diffusion Transformer (DiT) 模型中占用了超过 80% 的总计算资源。本文提出了一种名为 RainFusion 的新型无训练稀疏注意力方法，该方法通过利用视觉数据的固有稀疏性来加速注意力计算，同时保持视频质量。具体地，我们在推理过程中使用提出的 ARM （自适应识别模块）在线确定每个注意力头的稀疏模式，计算开销约为 0.2%。RainFusion 是一种即插即用的方法，可以无缝集成到最先进的 3D 注意力视频生成模型中，无需额外的训练或校准。我们在领先的开源模型 HunyuanVideo、OpenSoraPlan-1.2 和 CogVideoX-5B 上评估了该方法，展示了其广泛适用性和有效性。实验结果表明，RainFusion 在保持视频质量的同时，注意力计算速度提高了超过 2 倍，仅对 VBench 分数产生轻微影响（-0.2%）。', 'title_zh': 'RainFusion：通过多维度视觉冗余实现的自适应视频生成加速'}
{'arxiv_id': 'arXiv:2505.21032', 'title': 'FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models', 'authors': 'Nils Neukirch, Johanna Vielhaben, Nils Strodthoff', 'link': 'https://arxiv.org/abs/2505.21032', 'abstract': 'Internal representations are crucial for understanding deep neural networks, such as their properties and reasoning patterns, but remain difficult to interpret. While mapping from feature space to input space aids in interpreting the former, existing approaches often rely on crude approximations. We propose using a conditional diffusion model - a pretrained high-fidelity diffusion model conditioned on spatially resolved feature maps - to learn such a mapping in a probabilistic manner. We demonstrate the feasibility of this approach across various pretrained image classifiers from CNNs to ViTs, showing excellent reconstruction capabilities. Through qualitative comparisons and robustness analysis, we validate our method and showcase possible applications, such as the visualization of concept steering in input space or investigations of the composite nature of the feature space. This approach has broad potential for improving feature space understanding in computer vision models.', 'abstract_zh': '内部表征对于理解深度神经网络至关重要，包括其属性和推理模式，但仍然难以解释。虽然将特征空间映射到输入空间有助于解释前者，但现有方法往往依赖于粗略的近似。我们提出使用条件扩散模型——一种基于空间解析特征图预训练的高保真扩散模型——以概率方式学习这种映射。我们展示了该方法在各种从CNN到ViT的预训练图像分类器中的可行性，显示出出色重构能力。通过定性比较和鲁棒性分析，我们验证了该方法并展示了其潜在应用，例如输入空间的概念导向可视化或特征空间复合性质的研究。该方法在计算机视觉模型的特征空间理解方面具有广泛的潜在应用价值。', 'title_zh': 'FeatInv：使用条件扩散模型从特征空间到输入空间的空域解析映射'}
{'arxiv_id': 'arXiv:2505.21027', 'title': 'TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data', 'authors': 'Zhipeng He, Chun Ouyang, Lijie Wen, Cong Liu, Catarina Moreira', 'link': 'https://arxiv.org/abs/2505.21027', 'abstract': 'Adversarial attacks pose a significant threat to machine learning models by inducing incorrect predictions through imperceptible perturbations to input data. While these attacks have been extensively studied in unstructured data like images, their application to tabular data presents new challenges. These challenges arise from the inherent heterogeneity and complex feature interdependencies in tabular data, which differ significantly from those in image data. To address these differences, it is crucial to consider imperceptibility as a key criterion specific to tabular data. Most current research focuses primarily on achieving effective adversarial attacks, often overlooking the importance of maintaining imperceptibility. To address this gap, we propose a new benchmark for adversarial attacks on tabular data that evaluates both effectiveness and imperceptibility. In this study, we assess the effectiveness and imperceptibility of five adversarial attacks across four models using eleven tabular datasets, including both mixed and numerical-only datasets. Our analysis explores how these factors interact and influence the overall performance of the attacks. We also compare the results across different dataset types to understand the broader implications of these findings. The findings from this benchmark provide valuable insights for improving the design of adversarial attack algorithms, thereby advancing the field of adversarial machine learning on tabular data.', 'abstract_zh': 'adversarial 攻击对表格数据中的机器学习模型构成显著威胁，通过在输入数据上施加不可感知的扰动诱导错误预测。尽管这些攻击在图像等非结构化数据上得到了广泛研究，但将其应用于表格数据提出了新的挑战。这些挑战源于表格数据固有的异质性和复杂特征相互依赖性，这些特性与图像数据中的特性有显著差异。为了解决这些差异，将不可感知性视为特定于表格数据的关键标准至关重要。当前大部分研究主要集中在实现有效的 adversarial 攻击上，经常忽视保持不可感知性的的重要性。为了解决这一差距，我们提出了一种新的表格数据 adversarial 攻击基准，该基准同时评估有效性和不可感知性。在本研究中，我们使用包括混合和纯数值在内的十一个表格数据集评估四种模型下的五种 adversarial 攻击的有效性和不可感知性。我们的分析探讨了这些因素如何交互并影响攻击的整体性能。我们还对比了不同类型数据集的结果，以理解这些发现的更广泛影响。该基准的结果为改进 adversarial 攻击算法的设计提供了宝贵的见解，从而推动了基于表格数据的 adversarial 机器学习领域的发展。', 'title_zh': 'TabAttackBench：用于表格数据对抗攻击的基准测试'}
{'arxiv_id': 'arXiv:2505.21026', 'title': 'Multi-Mode Process Control Using Multi-Task Inverse Reinforcement Learning', 'authors': 'Runze Lin, Junghui Chen, Biao Huang, Lei Xie, Hongye Su', 'link': 'https://arxiv.org/abs/2505.21026', 'abstract': 'In the era of Industry 4.0 and smart manufacturing, process systems engineering must adapt to digital transformation. While reinforcement learning offers a model-free approach to process control, its applications are limited by the dependence on accurate digital twins and well-designed reward functions. To address these limitations, this paper introduces a novel framework that integrates inverse reinforcement learning (IRL) with multi-task learning for data-driven, multi-mode control design. Using historical closed-loop data as expert demonstrations, IRL extracts optimal reward functions and control policies. A latent-context variable is incorporated to distinguish modes, enabling the training of mode-specific controllers. Case studies on a continuous stirred tank reactor and a fed-batch bioreactor validate the effectiveness of this framework in handling multi-mode data and training adaptable controllers.', 'abstract_zh': '在工业4.0与智能制造时代，过程系统工程必须适应数字化转型。本文引入了一种将逆强化学习与多任务学习相结合的新框架，用于数据驱动的多模式控制设计。利用历史闭环数据作为专家演示，逆强化学习提取最优的奖励函数和控制策略，并通过引入潜在上下文变量来区分不同模式，实现模式特定控制器的训练。案例研究验证了该框架在处理多模式数据和训练可适应控制器方面的有效性。', 'title_zh': '多模式过程控制的多任务逆强化学习'}
{'arxiv_id': 'arXiv:2505.21025', 'title': 'Text-Queried Audio Source Separation via Hierarchical Modeling', 'authors': 'Xinlei Yin, Xiulian Peng, Xue Jiang, Zhiwei Xiong, Yan Lu', 'link': 'https://arxiv.org/abs/2505.21025', 'abstract': 'Target audio source separation with natural language queries presents a promising paradigm for extracting arbitrary audio events through arbitrary text descriptions. Existing methods mainly face two challenges, the difficulty in jointly modeling acoustic-textual alignment and semantic-aware separation within a blindly-learned single-stage architecture, and the reliance on large-scale accurately-labeled training data to compensate for inefficient cross-modal learning and separation. To address these challenges, we propose a hierarchical decomposition framework, HSM-TSS, that decouples the task into global-local semantic-guided feature separation and structure-preserving acoustic reconstruction. Our approach introduces a dual-stage mechanism for semantic separation, operating on distinct global and local semantic feature spaces. We first perform global-semantic separation through a global semantic feature space aligned with text queries. A Q-Audio architecture is employed to align audio and text modalities, serving as pretrained global-semantic encoders. Conditioned on the predicted global feature, we then perform the second-stage local-semantic separation on AudioMAE features that preserve time-frequency structures, followed by acoustic reconstruction. We also propose an instruction processing pipeline to parse arbitrary text queries into structured operations, extraction or removal, coupled with audio descriptions, enabling flexible sound manipulation. Our method achieves state-of-the-art separation performance with data-efficient training while maintaining superior semantic consistency with queries in complex auditory scenes.', 'abstract_zh': '基于自然语言查询的目标音频源分离呈一种有前途的范式，能够通过任意文本描述提取任意音频事件。现有的方法主要面临两大挑战：单阶段盲学习架构中难以同时建模声学-文本对齐和语义感知分离，以及需要大量精确标注的训练数据来补偿跨模态学习和分离的低效性。为了解决这些挑战，我们提出了一种分层分解框架HSM-TSS，将任务分解为全局-局部语义引导特征分离和结构保真的声学重构。该方法引入了一种双阶段的语义分离机制，分别在全局和局部语义特征空间上操作。首先，通过与文本查询对齐的全局语义特征空间进行全局语义分离，采用Q-Audio架构对齐音频和文本模态，作为预训练的全局语义编码器。在预测的全局特征条件下，然后在保留时频结构的AudioMAE特征上进行第二阶段的局部语义分离，并随后进行声学重构。我们还提出了一种指令处理管道，将任意文本查询解析为结构化的操作，提取或移除，并结合音频描述，实现灵活的音效操控。该方法在数据高效训练的同时，保持了与复杂听觉场景中查询的优异语义一致性，达到最好的分离性能。', 'title_zh': '基于层次建模的文本查询驱动音频源分离'}
{'arxiv_id': 'arXiv:2505.21012', 'title': 'Federated Instrumental Variable Analysis via Federated Generalized Method of Moments', 'authors': 'Geetika, Somya Tyagi, Bapi Chatterjee', 'link': 'https://arxiv.org/abs/2505.21012', 'abstract': "Instrumental variables (IV) analysis is an important applied tool for areas such as healthcare and consumer economics. For IV analysis in high-dimensional settings, the Generalized Method of Moments (GMM) using deep neural networks offers an efficient approach. With non-i.i.d. data sourced from scattered decentralized clients, federated learning is a popular paradigm for training the models while promising data privacy. However, to our knowledge, no federated algorithm for either GMM or IV analysis exists to date. In this work, we introduce federated instrumental variables analysis (FedIV) via federated generalized method of moments (FedGMM). We formulate FedGMM as a federated zero-sum game defined by a federated non-convex non-concave minimax optimization problem, which is solved using federated gradient descent ascent (FedGDA) algorithm. One key challenge arises in theoretically characterizing the federated local optimality. To address this, we present properties and existence results of clients' local equilibria via FedGDA limit points. Thereby, we show that the federated solution consistently estimates the local moment conditions of every participating client. The proposed algorithm is backed by extensive experiments to demonstrate the efficacy of our approach.", 'abstract_zh': '联邦广义工具变量分析（FedGMM）', 'title_zh': '联邦工具变量分析 via 联邦广义矩方法'}
{'arxiv_id': 'arXiv:2505.20997', 'title': 'BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks', 'authors': 'Sen Bai, Chunqi Yang, Xin Bai, Xin Zhang, Zhengang Jiang', 'link': 'https://arxiv.org/abs/2505.20997', 'abstract': 'Binary (0-1) integer programming (BIP) is pivotal in scientific domains requiring discrete decision-making. As the advance of AI computing, recent works explore neural network-based solvers for integer linear programming (ILP) problems. Yet, they lack scalability for tackling nonlinear challenges. To handle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear relaxations, leading to exponential growth in auxiliary variables and severe computation limitations. To overcome these limitations, we propose BIPNN (Binary Integer Programming Neural Network), an unsupervised learning framework to solve nonlinear BIP problems via hypergraph neural networks (HyperGNN). Specifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear (sin, log, exp) optimization problems-into unconstrained, differentiable, and polynomial loss functions. The reformulation stems from the observation of a precise one-to-one mapping between polynomial BIP objectives and hypergraph structures, enabling the unsupervised training of HyperGNN to optimize BIP problems in an end-to-end manner. On this basis, we propose a GPU-accelerated and continuous-annealing-enhanced training pipeline for BIPNN. The pipeline enables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel via straightforward gradient descent, thus significantly reducing the training cost while ensuring the generation of discrete, high-quality solutions. Extensive experiments on synthetic and real-world datasets highlight the superiority of our approach.', 'abstract_zh': '基于超图神经网络的二进制整数规划求解器BIPNN', 'title_zh': 'BIPNN：通过超图神经网络学习求解二元整数规划'}
{'arxiv_id': 'arXiv:2505.20993', 'title': 'Who Reasons in the Large Language Models?', 'authors': 'Jie Shao, Jianxin Wu', 'link': 'https://arxiv.org/abs/2505.20993', 'abstract': "Despite the impressive performance of large language models (LLMs), the process of endowing them with new capabilities--such as mathematical reasoning--remains largely empirical and opaque. A critical open question is whether reasoning abilities stem from the entire model, specific modules, or are merely artifacts of overfitting. In this work, we hypothesize that the reasoning capabilities in well-trained LLMs are primarily attributed to the output projection module (oproj) in the Transformer's multi-head self-attention (MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for Networks (SfN), a suite of diagnostic tools designed to probe and analyze the internal behaviors of LLMs. Using SfN, we provide both circumstantial and empirical evidence suggesting that oproj plays a central role in enabling reasoning, whereas other modules contribute more to fluent dialogue. These findings offer a new perspective on LLM interpretability and open avenues for more targeted training strategies, potentially enabling more efficient and specialized LLMs.", 'abstract_zh': '尽管大型语言模型（LLMs）表现出色，但赋予它们新能力的过程——例如数学推理——仍主要依赖经验方法且不够透明。一个关键的开放问题是推理能力源自整个模型、特定模块，还是仅仅是过拟合的结果。在本工作中，我们假设在充分训练的LLMs中，推理能力主要归因于Transformer多头自注意力（MHSA）机制中的输出投影模块（oproj）。为了支持这一假设，我们 introduce Stethoscope for Networks (SfN)，一系列诊断工具，用于探究和分析LLMs的内部行为。使用SfN，我们提供了间接和实证证据，表明oproj在使模型具备推理能力方面起着核心作用，而其他模块则更多地促进了流畅对话。这些发现为LLM可解释性提供了新的视角，并为更具针对性的训练策略开启了新的途径，可能有助于实现更高效和专门化的LLMs。', 'title_zh': '大型语言模型中的推理过程'}
{'arxiv_id': 'arXiv:2505.20979', 'title': 'MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection', 'authors': 'Tongyu Lu, Charlotta-Marlena Geist, Jan Melechovsky, Abhinaba Roy, Dorien Herremans', 'link': 'https://arxiv.org/abs/2505.20979', 'abstract': 'We propose MelodySim, a melody-aware music similarity model and dataset for plagiarism detection. First, we introduce a novel method to construct a dataset with focus on melodic similarity. By augmenting Slakh2100; an existing MIDI dataset, we generate variations of each piece while preserving the melody through modifications such as note splitting, arpeggiation, minor track dropout (excluding bass), and re-instrumentation. A user study confirms that positive pairs indeed contain similar melodies, with other musical tracks significantly changed. Second, we develop a segment-wise melodic-similarity detection model that uses a MERT encoder and applies a triplet neural network to capture melodic similarity. The resultant decision matrix highlights where plagiarism might occur. Our model achieves high accuracy on the MelodySim test set.', 'abstract_zh': '我们提出MelodySim，一种基于旋律的音乐相似度模型和数据集，用于检测抄袭。首先，我们介绍了一种新的方法，用于构建一个以旋律相似性为重点的数据集。通过增强现有的MIDI数据集Slakh2100，我们生成了每首作品的不同变体，同时通过音符分割、琶音化、除低音外的小节删除和重新配器等修改保留旋律。用户研究证实，正样本确实包含相似的旋律，而其他音乐轨道则显著不同。其次，我们开发了一种段落级旋律相似性检测模型，该模型使用MERT编码器并应用三重神经网络来捕捉旋律相似性。结果决策矩阵突出显示了可能发生抄袭的地方。我们的模型在MelodySim测试集上实现了高准确性。', 'title_zh': 'MelodySim: 基于旋律的音乐相似性测量在剽窃检测中的应用'}
{'arxiv_id': 'arXiv:2505.20973', 'title': 'Towards Conversational Development Environments: Using Theory-of-Mind and Multi-Agent Architectures for Requirements Refinement', 'authors': 'Keheliya Gallaba, Ali Arabat, Dayi Lin, Mohammed Sayagh, Ahmed E. Hassan', 'link': 'https://arxiv.org/abs/2505.20973', 'abstract': 'Foundation Models (FMs) have shown remarkable capabilities in various natural language tasks. However, their ability to accurately capture stakeholder requirements remains a significant challenge for using FMs for software development. This paper introduces a novel approach that leverages an FM-powered multi-agent system called AlignMind to address this issue. By having a cognitive architecture that enhances FMs with Theory-of-Mind capabilities, our approach considers the mental states and perspectives of software makers. This allows our solution to iteratively clarify the beliefs, desires, and intentions of stakeholders, translating these into a set of refined requirements and a corresponding actionable natural language workflow in the often-overlooked requirements refinement phase of software engineering, which is crucial after initial elicitation. Through a multifaceted evaluation covering 150 diverse use cases, we demonstrate that our approach can accurately capture the intents and requirements of stakeholders, articulating them as both specifications and a step-by-step plan of action. Our findings suggest that the potential for significant improvements in the software development process justifies these investments. Our work lays the groundwork for future innovation in building intent-first development environments, where software makers can seamlessly collaborate with AIs to create software that truly meets their needs.', 'abstract_zh': '基于Foundation Models的软件开发需求澄清新方法：AlignMind及其应用研究', 'title_zh': '面向对话式的开发环境：基于心理理论和多代理架构的需求细化方法'}
{'arxiv_id': 'arXiv:2505.20972', 'title': 'Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs', 'authors': 'Sen Bai, Chunqi Yang, Xin Bai, Xin Zhang, Zhengang Jiang', 'link': 'https://arxiv.org/abs/2505.20972', 'abstract': 'Along with AI computing shining in scientific discovery, its potential in the combinatorial optimization (CO) domain has also emerged in recent years. Yet, existing unsupervised neural network solvers struggle to solve $k$-grouping problems (e.g., coloring, partitioning) on large-scale graphs and hypergraphs, due to limited computational frameworks. In this work, we propose Deep $k$-grouping, an unsupervised learning-based CO framework. Specifically, we contribute: Novel one-hot encoded polynomial unconstrained binary optimization (OH-PUBO), a formulation for modeling k-grouping problems on graphs and hypergraphs (e.g., graph/hypergraph coloring and partitioning); GPU-accelerated algorithms for large-scale k-grouping CO problems. Deep $k$-grouping employs the relaxation of large-scale OH-PUBO objectives as differentiable loss functions and trains to optimize them in an unsupervised manner. To ensure scalability, it leverages GPU-accelerated algorithms to unify the training pipeline; A Gini coefficient-based continuous relaxation annealing strategy to enforce discreteness of solutions while preventing convergence to local optima. Experimental results demonstrate that Deep $k$-grouping outperforms existing neural network solvers and classical heuristics such as SCIP and Tabu.', 'abstract_zh': '伴随AI计算在科学发现中的闪耀，其在组合优化领域近年来也展现出巨大潜力。现有无监督神经网络求解器在解决大规模图和超图的k-分组问题（如着色、划分）时遇挑战，受限于计算框架的局限。在这项工作中，我们提出了Deep $k$-分组，一个基于无监督学习的组合优化框架。具体贡献包括：一种新型的一-hot编码多项式未约束二进制优化（OH-PUBO）表示，用于建模图和超图上的k-分组问题（如图和超图着色与划分）；大规模k-分组组合优化问题的GPU加速算法。Deep $k$-分组采用大规模OH-PUBO目标的松弛作为可微损失函数，并以无监督方式训练以优化这些目标；基于Gini系数的连续松弛退火策略，确保解的离散性并防止收敛于局部最优。实验结果表明，Deep $k$-分组在性能上优于现有神经网络求解器和经典启发式算法（如SCIP和Tabu）。', 'title_zh': '深层次k-分组：图和超图上组合优化的无监督学习框架'}
{'arxiv_id': 'arXiv:2505.20971', 'title': 'Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA', 'authors': 'Xiangqing Shen, Fanfan Wang, Rui Xia', 'link': 'https://arxiv.org/abs/2505.20971', 'abstract': 'LLMs have demonstrated remarkable capabilities in complex reasoning tasks, yet they often suffer from hallucinations and lack reliable factual grounding. Meanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack the flexible reasoning abilities of LLMs. In this paper, we present Reason-Align-Respond (RAR), a novel framework that systematically integrates LLM reasoning with knowledge graphs for KGQA. Our approach consists of three key components: a Reasoner that generates human-like reasoning chains, an Aligner that maps these chains to valid KG paths, and a Responser that synthesizes the final answer. We formulate this process as a probabilistic model and optimize it using the Expectation-Maximization algorithm, which iteratively refines the reasoning chains and knowledge paths. Extensive experiments on multiple benchmarks demonstrate the effectiveness of RAR, achieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on WebQSP and CWQ respectively. Human evaluation confirms that RAR generates high-quality, interpretable reasoning chains well-aligned with KG paths. Furthermore, RAR exhibits strong zero-shot generalization capabilities and maintains computational efficiency during inference.', 'abstract_zh': 'LLMs在复杂推理任务中展现出卓越的能力，但经常出现幻觉且缺乏可靠的事实支撑。同时，知识图谱（KGs）提供了结构化事实知识，但缺乏LLMs的灵活推理能力。本文提出了一种名为Reason-Align-Respond（RAR）的新颖框架，将LLMs推理与知识图谱系统性地结合用于KGQA。我们的方法包括三个关键组件：一种生成类人类推理链的推理器、一种将这些链映射到有效KG路径的对齐器，以及一种综合最终答案的响应器。我们将这一过程形式化为概率模型，并使用期望最大化算法对其进行优化，该算法迭代地细化推理链和知识路径。在多个基准上的 extensive 实验表明，RAR 能够实现卓越的效果，在WebQSP和CWQ上的Hit@1得分分别为93.3%和91.0%。人类评估证实，RAR 生成了高质量、可解释且与KG路径高度对齐的推理链。此外，RAR 具有强大的零样本泛化能力，并且在推理过程中保持了计算效率。', 'title_zh': 'Reason-Align-Respond: 将LLM推理与知识图谱对齐以用于知识图谱问答'}
{'arxiv_id': 'arXiv:2505.20963', 'title': 'Context-Aware Content Moderation for German Newspaper Comments', 'authors': 'Felix Krejca, Tobias Kietreiber, Alexander Buchelt, Sebastian Neumaier', 'link': 'https://arxiv.org/abs/2505.20963', 'abstract': "The increasing volume of online discussions requires advanced automatic content moderation to maintain responsible discourse. While hate speech detection on social media is well-studied, research on German-language newspaper forums remains limited. Existing studies often neglect platform-specific context, such as user history and article themes. This paper addresses this gap by developing and evaluating binary classification models for automatic content moderation in German newspaper forums, incorporating contextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging the One Million Posts Corpus from the Austrian newspaper Der Standard, we assess the impact of context-aware models. Results show that CNN and LSTM models benefit from contextual information and perform competitively with state-of-the-art approaches. In contrast, ChatGPT's zero-shot classification does not improve with added context and underperforms.", 'abstract_zh': '不断增加的在线讨论量要求先进的自动内容审核技术以维持负责任的 discourse。虽然社交媒体上的仇恨言论检测已被广泛研究，但关于德语报纸论坛的研究仍然有限。现有研究往往忽视了平台特定的上下文，如用户历史和文章主题。本文通过开发和评估用于德语报纸论坛的自动内容审核二分类模型，并结合上下文信息，填补了这一空白。利用LSTM、CNN和ChatGPT-3.5 Turbo，并借助奥地利报纸《标准报》的百万帖子语料库，我们评估了面向上下文的模型的影响。结果显示，CNN和LSTM模型受益于上下文信息，并与当前最先进的方法表现得相当。相比之下，ChatGPT的零样本分类在增加上下文时并未改善，并表现出色。', 'title_zh': '基于上下文的内容审核：德国报纸评论的内容过滤'}
{'arxiv_id': 'arXiv:2505.20961', 'title': 'Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization', 'authors': 'Yiyuan Yang, Shitong Xu, Niki Trigoni, Andrew Markham', 'link': 'https://arxiv.org/abs/2505.20961', 'abstract': "Sound source localization (SSL) is a critical technology for determining the position of sound sources in complex environments. However, existing methods face challenges such as high computational costs and precise calibration requirements, limiting their deployment in dynamic or resource-constrained environments. This paper introduces a novel 3D SSL framework, which uses sparse cross-attention, pretraining, and adaptive signal coherence metrics, to achieve accurate and computationally efficient localization with fewer input microphones. The framework is also fault-tolerant to unreliable or even unknown microphone position inputs, ensuring its applicability in real-world scenarios. Preliminary experiments demonstrate its scalability for multi-source localization without requiring additional hardware. This work advances SSL by balancing the model's performance and efficiency and improving its robustness for real-world scenarios.", 'abstract_zh': '声源定位（SSL）是确定复杂环境中声源位置的关键技术。然而，现有方法面临高计算成本和精确校准要求等挑战，限制了其在动态或资源受限环境中的部署。本文引入了一种新型的3D SSL框架，该框架利用稀疏交叉注意力、预训练和自适应信号相干性度量，实现了在更少输入麦克风条件下进行准确且计算高效的定位。该框架还对不可靠甚至未知麦克风位置输入具有容错性，确保其在实际应用场景中的适用性。初步实验结果显示，该工作在无需额外硬件的情况下，展示了其在多源定位方面的可扩展性。本研究通过平衡模型性能和效率，并提高其在实际应用场景中的鲁棒性，推进了SSL的发展。', 'title_zh': '高效的和麦克风故障容忍的3D声源定位'}
{'arxiv_id': 'arXiv:2505.20956', 'title': 'Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection', 'authors': 'Shiqi Zhang, Tuomas Virtanen', 'link': 'https://arxiv.org/abs/2505.20956', 'abstract': 'Bioacoustic sound event detection (BioSED) is crucial for biodiversity conservation but faces practical challenges during model development and training: limited amounts of annotated data, sparse events, species diversity, and class imbalance. To address these challenges efficiently with a limited labeling budget, we apply the mismatch-first farthest-traversal (MFFT), an active learning method integrating committee voting disagreement and diversity analysis. We also refine an existing BioSED dataset specifically for evaluating active learning algorithms. Experimental results demonstrate that MFFT achieves a mAP of 68% when cold-starting and 71% when warm-starting (which is close to the fully-supervised mAP of 75%) while using only 2.3% of the annotations. Notably, MFFT excels in cold-start scenarios and with rare species, which are critical for monitoring endangered species, demonstrating its practical value.', 'abstract_zh': '生物声学声音事件检测（BioSED）对于生物多样性保护至关重要，但在模型开发和训练过程中面临实际挑战：标注数据量有限、事件稀疏、物种多样性以及类别不平衡。为了在有限的标注预算内有效应对这些挑战，我们应用了 mismatch-first farthest-traversal (MFFT) 方法，该方法结合了委员会投票分歧和多样性分析的主动学习方法。我们还细化了一个现有的 BioSED 数据集，以评估主动学习算法。实验结果表明，MFFT 在冷启动时达到了 68% 的 mAP，在温暖启动时达到了 71%（接近完全监督学习的 75%），仅使用了 2.3% 的标注。值得注意的是，MFFT 在冷启动场景和稀有物种中表现出色，这对于监测濒危物种至关重要，证明了其实际价值。', 'title_zh': '生物声学声事件检测的混合分歧-多样性主动学习'}
{'arxiv_id': 'arXiv:2505.20949', 'title': 'Streamlining Knowledge Graph Creation with PyRML', 'authors': 'Andrea Giovanni Nuzzolese', 'link': 'https://arxiv.org/abs/2505.20949', 'abstract': 'Knowledge Graphs (KGs) are increasingly adopted as a foundational technology for integrating heterogeneous data in domains such as climate science, cultural heritage, and the life sciences. Declarative mapping languages like R2RML and RML have played a central role in enabling scalable and reusable KG construction, offering a transparent means of transforming structured and semi-structured data into RDF. In this paper, we present PyRML, a lightweight, Python-native library for building Knowledge Graphs through declarative mappings. PyRML supports core RML constructs and provides a programmable interface for authoring, executing, and testing mappings directly within Python environments. It integrates with popular data and semantic web libraries (e.g., Pandas and RDFlib), enabling transparent and modular workflows. By lowering the barrier to entry for KG creation and fostering reproducible, ontology-aligned data integration, PyRML bridges the gap between declarative semantics and practical KG engineering.', 'abstract_zh': '知识图谱（KGs）在气候科学、文化遗产和生命科学等领域越来越被用作集成异构数据的基础技术。声明式映射语言如R2RML和RML在实现可扩展和可重用的KG构建中发挥了核心作用，提供了一种透明地将结构化和半结构化数据转换为RDF的方法。在本文中，我们介绍了PyRML，这是一种轻量级的Python原生库，通过声明式映射构建知识图谱。PyRML支持核心RML构造，并提供了一种在Python环境中直接编写、执行和测试映射的编程接口。它与流行的数据和语义网库（如Pandas和RDFlib）集成，实现了透明且模块化的 workflows。通过降低知识图谱创建的门槛并促进可重复的、本体对齐的数据集成，PyRML在声明式语义与实际知识图谱工程之间架起了桥梁。', 'title_zh': '使用PyRML简化知识图谱创建'}
{'arxiv_id': 'arXiv:2505.20947', 'title': 'Unified Deep Learning Approach for Estimating the Metallicities of RR Lyrae Stars Using light curves from Gaia Data Release 3', 'authors': 'Lorenzo Monti, Tatiana Muraveva, Alessia Garofalo, Gisella Clementini, Maria Letizia Valentini', 'link': 'https://arxiv.org/abs/2505.20947', 'abstract': 'RR Lyrae stars (RRLs) are old pulsating variables widely used as metallicity tracers due to the correlation between their metal abundances and light curve morphology. With ESA Gaia DR3 providing light curves for about 270,000 RRLs, there is a pressing need for scalable methods to estimate their metallicities from photometric data. We introduce a unified deep learning framework that estimates metallicities for both fundamental-mode (RRab) and first-overtone (RRc) RRLs using Gaia G-band light curves. This approach extends our previous work on RRab stars to include RRc stars, aiming for high predictive accuracy and broad generalization across both pulsation types. The model is based on a Gated Recurrent Unit (GRU) neural network optimized for time-series extrinsic regression. Our pipeline includes preprocessing steps such as phase folding, smoothing, and sample weighting, and uses photometric metallicities from the literature as training targets. The architecture is designed to handle morphological differences between RRab and RRc light curves without requiring separate models. On held-out validation sets, our GRU model achieves strong performance: for RRab stars, MAE = 0.0565 dex, RMSE = 0.0765 dex, R^2 = 0.9401; for RRc stars, MAE = 0.0505 dex, RMSE = 0.0720 dex, R^2 = 0.9625. These results show the effectiveness of deep learning for large-scale photometric metallicity estimation and support its application to studies of stellar populations and Galactic structure.', 'abstract_zh': 'RR Lyrae 星的金属丰度估计：基于ESA Gaia DR3光曲线的统一深度学习框架', 'title_zh': '使用Gaia数据释放3的光变曲线统一直 deep学习方法估计RR Lyrae星的金属丰度'}
{'arxiv_id': 'arXiv:2505.20925', 'title': 'Multi-objective Large Language Model Alignment with Hierarchical Experts', 'authors': 'Zhuo Li, Guodong Du, Weiyang Guo, Yigeng Zhou, Xiucheng Li, Wenya Wang, Fangming Liu, Yequan Wang, Deheng Ye, Min Zhang, Jing Li', 'link': 'https://arxiv.org/abs/2505.20925', 'abstract': 'Aligning large language models (LLMs) to simultaneously satisfy multiple objectives remains a significant challenge, especially given the diverse and often conflicting nature of human preferences. Existing alignment methods struggle to balance trade-offs effectively, often requiring costly retraining or yielding suboptimal results across the Pareto frontier of preferences. In this paper, we introduce \\textit{HoE}(Hierarchical Mixture-of-Experts), a \\textit{lightweight}, \\textit{parameter-efficient}, and \\textit{plug-and-play} approach that eliminates the need for model training, while enabling LLMs to adapt across the entire Pareto frontier and accommodate diverse user preferences. In particular, \\textit{HoE} consists of three hierarchical components: LoRA Experts, Router Experts and Preference Routing, reaching optimal Pareto frontiers and achieving a trade-off between parameter size, training cost, and performance. We evaluate \\textit{HoE} across various tasks on 14 objectives and 200 different preferences among 6 benchmarks, demonstrating superior performance over 15 recent baselines. Code is available in the supplementary materials.', 'abstract_zh': '同时满足多重目标的大语言模型（LLMs）对齐仍然是一项重大挑战，尤其是在面对人类偏好多样且often conflicting（通常冲突）的情况下。现有对齐方法难以有效地权衡这些trade-offs（权衡），通常需要昂贵的重新训练或在偏好帕累托前沿上产生次优结果。在本文中，我们提出了一种名为\\textit{HoE}（层次专家混合）的轻量级、参数高效且即插即用的方法，该方法消除了模型训练的需要，使大语言模型能够跨越整个帕累托前沿进行适应，并容纳多样化的用户偏好。特别是，\\textit{HoE}包含三个层级组件：LoRA专家、路由器专家和偏好路由，达到最优帕累托前沿，并在参数量、训练成本和性能之间实现权衡。我们在6个基准上的14个目标和200种不同偏好下的各种任务上评估了\\textit{HoE}，结果优于15个近期基线。代码附在补充材料中。', 'title_zh': '多目标大型语言模型层级专家对齐'}
{'arxiv_id': 'arXiv:2505.20922', 'title': 'Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective', 'authors': 'Yang Zhang, Xinran Li, Jianing Ye, Delin Qu, Shuang Qiu, Chongjie Zhang, Xiu Li, Chenjia Bai', 'link': 'https://arxiv.org/abs/2505.20922', 'abstract': "World models have recently attracted growing interest in Multi-Agent Reinforcement Learning (MARL) due to their ability to improve sample efficiency for policy learning. However, accurately modeling environments in MARL is challenging due to the exponentially large joint action space and highly uncertain dynamics inherent in multi-agent systems. To address this, we reduce modeling complexity by shifting from jointly modeling the entire state-action transition dynamics to focusing on the state space alone at each timestep through sequential agent modeling. Specifically, our approach enables the model to progressively resolve uncertainty while capturing the structured dependencies among agents, providing a more accurate representation of how agents influence the state. Interestingly, this sequential revelation of agents' actions in a multi-agent system aligns with the reverse process in diffusion models--a class of powerful generative models known for their expressiveness and training stability compared to autoregressive or latent variable models. Leveraging this insight, we develop a flexible and robust world model for MARL using diffusion models. Our method, Diffusion-Inspired Multi-Agent world model (DIMA), achieves state-of-the-art performance across multiple multi-agent control benchmarks, significantly outperforming prior world models in terms of final return and sample efficiency, including MAMuJoCo and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent world models, advancing the frontier of MARL research.", 'abstract_zh': '基于扩散模型的Multi-Agent世界模型（DIMA）：多智能体控制benchmark上的最新进展', 'title_zh': '从扩散启发的角度 revisiting 多智能体世界建模'}
{'arxiv_id': 'arXiv:2505.20921', 'title': 'Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models', 'authors': 'Injae Na, Keonwoong Noh, Woohwan Jung', 'link': 'https://arxiv.org/abs/2505.20921', 'abstract': 'LLM providers typically offer multiple LLM tiers, varying in performance and price. As NLP tasks become more complex and modularized, selecting the suitable LLM tier for each subtask is a key challenge to balance between cost and performance. To address the problem, we introduce LLM Automatic Transmission (LLM-AT) framework that automatically selects LLM tiers without training. LLM-AT consists of Starter, Generator, and Judge. The starter selects the initial LLM tier expected to solve the given question, the generator produces a response using the LLM of the selected tier, and the judge evaluates the validity of the response. If the response is invalid, LLM-AT iteratively upgrades to a higher-tier model, generates a new response, and re-evaluates until a valid response is obtained. Additionally, we propose accuracy estimator, which enables the suitable initial LLM tier selection without training. Given an input question, accuracy estimator estimates the expected accuracy of each LLM tier by computing the valid response rate across top-k similar queries from past inference records. Experiments demonstrate that LLM-AT achieves superior performance while reducing costs, making it a practical solution for real-world applications.', 'abstract_zh': 'LLM提供者通常提供多个性能和价格各异的LLM层。随着NLP任务变得更加复杂和模块化，为每个子任务选择合适的LLM层以平衡成本和性能是一个关键挑战。为了应对这一问题，我们引入了LLM自动传输（LLM-AT）框架，该框架能够在无需训练的情况下自动选择LLM层。LLM-AT包括启动器、生成器和裁判。启动器选择初始的预期能解决给定问题的LLM层，生成器使用选择的LLM层生成响应，裁判评估响应的有效性。如果响应无效，LLM-AT会迭代地升级到更高层的模型，生成新的响应并重新评估，直到获得有效响应。此外，我们提出了准确度估计器，这使得可以在无需训练的情况下选择合适的初始LLM层。给定输入问题，准确度估计器通过计算过去推断记录中前k个相似查询的有效响应率来估计每个LLM层的预期准确率。实验表明，LLM-AT在提高性能的同时降低了成本，使其成为实际应用场景中的实用解决方案。', 'title_zh': '自动传输模型层级：在大规模语言模型中优化成本与准确率'}
{'arxiv_id': 'arXiv:2505.20918', 'title': 'Humble AI in the real-world: the case of algorithmic hiring', 'authors': 'Rahul Nair, Inge Vejsbjerg, Elizabeth Daly, Christos Varytimidis, Bran Knowles', 'link': 'https://arxiv.org/abs/2505.20918', 'abstract': 'Humble AI (Knowles et al., 2023) argues for cautiousness in AI development and deployments through scepticism (accounting for limitations of statistical learning), curiosity (accounting for unexpected outcomes), and commitment (accounting for multifaceted values beyond performance). We present a real-world case study for humble AI in the domain of algorithmic hiring. Specifically, we evaluate virtual screening algorithms in a widely used hiring platform that matches candidates to job openings. There are several challenges in misrecognition and stereotyping in such contexts that are difficult to assess through standard fairness and trust frameworks; e.g., someone with a non-traditional background is less likely to rank highly. We demonstrate technical feasibility of how humble AI principles can be translated to practice through uncertainty quantification of ranks, entropy estimates, and a user experience that highlights algorithmic unknowns. We describe preliminary discussions with focus groups made up of recruiters. Future user studies seek to evaluate whether the higher cognitive load of a humble AI system fosters a climate of trust in its outcomes.', 'abstract_zh': '谦逊的AI（Knowles等，2023）主张在AI开发和部署中保持谨慎，并通过怀疑（考虑统计学习的局限性）、好奇心（考虑意外结果）和承诺（考虑超越性能的多元价值观）来实现。我们提出一个关于谦逊AI在算法招聘领域的实际案例研究。具体而言，我们评估了一个广泛使用的招聘平台中的虚拟筛选算法，该平台将候选人匹配到职位空缺。在这种背景下，误识和刻板印象的问题难以通过标准的公平性和信任框架来评估；例如，背景非传统的人员不太可能排名靠前。我们展示了通过排名的不确定性量化、熵估计和突出算法未知性来实现谦逊AI原则技术可行性的方法。我们描述了与招聘人员组成的焦点小组的初步讨论。未来用户研究旨在评估谦逊AI系统更高的认知负荷是否能促进对其结果的信任氛围。', 'title_zh': '谦逊的人工智能在现实世界：算法招聘案例'}
{'arxiv_id': 'arXiv:2505.20901', 'title': 'A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models', 'authors': 'Junhyuk Choi, Minju Kim, Yeseon Hong, Bugeun Kim', 'link': 'https://arxiv.org/abs/2505.20901', 'abstract': "As large vision language models(LVLMs) rapidly advance, concerns about their potential to learn and generate social biases and stereotypes are increasing. Previous studies on LVLM's stereotypes face two primary limitations: metrics that overlooked the importance of content words, and datasets that overlooked the effect of color. To address these limitations, this study introduces new evaluation metrics based on the Stereotype Content Model (SCM). We also propose BASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM metrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes. As a result, we found three findings. (1) The SCM-based evaluation is effective in capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output along with gender and race ones. (3) Interaction between model architecture and parameter sizes seems to affect stereotypes. We release BASIC publicly on [anonymized for review].", 'abstract_zh': '随着大型视觉语言模型(LVLMs)的 rapidly 进步，人们对它们潜在的社会偏见和刻板印象学习与生成的担忧不断增加。关于 LVLM 的刻板印象的研究面临两大主要限制：忽视内容词重要性的指标和忽视颜色影响的数据集。为解决这些限制，本研究引入了基于刻板印象内容模型(Stereotype Content Model, SCM)的新评估指标，并提出了一项名为 BASIC 的基准测试，用于评估性别、种族和颜色刻板印象。使用 SCM 指标和 BASIC，我们对八种 LVLM 进行了研究，以发现刻板印象。结果，我们发现了三个发现：(1) 基于 SCM 的评估有效捕捉了刻板印象。(2) LVLMs 在输出中不仅表现出性别和种族刻板印象，还表现出颜色刻板印象。(3) 模型架构与参数大小之间的交互似乎影响刻板印象。我们已在 [审查中匿名化] 释放 BASIC。', 'title_zh': '颜色相关社会偏见在大型视觉语言模型中的刻板印象内容分析'}
{'arxiv_id': 'arXiv:2505.20897', 'title': 'Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation', 'authors': 'Pingrui Zhang, Yifei Su, Pengyuan Wu, Dong An, Li Zhang, Zhigang Wang, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li', 'link': 'https://arxiv.org/abs/2505.20897', 'abstract': 'Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \\textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \\href{this https URL}{here}.', 'abstract_zh': '基于语言的视图导航（Vision-and-Language Navigation, VLN）要求代理在部分可观测性的条件下遵循自然指令进行导航，这使得感知与语言的对齐变得困难。近期的方法通过想象未来场景来缓解这一问题，但这些方法依赖于基于视觉的合成，导致高计算成本和冗余细节。为此，我们提出了一种通过语言形式适配性地想象关键环境语义的策略，以实现更可靠和高效的方案。具体地，我们引入了一种新的自引导想象政策——适应性文本梦者（Adaptive Text Dreamer, ATD），其基于一个大型语言模型（LLM）构建，具有类人左右脑结构，左脑侧重于逻辑整合，右脑负责对未来场景的创造性预测。为此，我们仅对左右脑中的Q-former进行微调，以高效激活LLM中的领域特定知识，从而在导航过程中实现动态的逻辑推理和想象更新。此外，我们引入了一种交叉交互机制来规范想象输出并将其注入导航专家模块，使得ATD能够同时利用LLM的推理能力和导航模型的专业知识。我们在R2R基准上进行了广泛的实验，结果显示ATD在较少参数的情况下达到了最先进的性能。代码可在以下链接获取：this https URL', 'title_zh': '从左脑到右脑的跨越：适应性文本梦境者用于视觉与语言导航'}
{'arxiv_id': 'arXiv:2505.20896', 'title': 'How Do Transformers Learn Variable Binding in Symbolic Programs?', 'authors': 'Yiwei Wu, Atticus Geiger, Raphaël Millière', 'link': 'https://arxiv.org/abs/2505.20896', 'abstract': 'Variable binding -- the ability to associate variables with values -- is fundamental to symbolic computation and cognition. Although classical architectures typically implement variable binding via addressable memory, it is not well understood how modern neural networks lacking built-in binding operations may acquire this capacity. We investigate this by training a Transformer to dereference queried variables in symbolic programs where variables are assigned either numerical constants or other variables. Each program requires following chains of variable assignments up to four steps deep to find the queried value, and also contains irrelevant chains of assignments acting as distractors. Our analysis reveals a developmental trajectory with three distinct phases during training: (1) random prediction of numerical constants, (2) a shallow heuristic prioritizing early variable assignments, and (3) the emergence of a systematic mechanism for dereferencing assignment chains. Using causal interventions, we find that the model learns to exploit the residual stream as an addressable memory space, with specialized attention heads routing information across token positions. This mechanism allows the model to dynamically track variable bindings across layers, resulting in accurate dereferencing. Our results show how Transformer models can learn to implement systematic variable binding without explicit architectural support, bridging connectionist and symbolic approaches.', 'abstract_zh': '变量绑定——将变量与值关联的能力——是符号计算和认知的基本要素。虽然经典架构通常通过可寻址内存实现变量绑定，但现代缺乏内置绑定操作的神经网络如何获得这一能力尚不明确。我们通过训练一个变换器来查询符号程序中的变量，探索这一问题，其中变量被赋予数字常量或其他变量。每个程序需要追踪最多四步深的变量赋值链以找到查询值，并且包含作为干扰项的无关赋值链。我们的分析揭示了训练过程中的发展轨迹，包括三个明显的阶段：（1）随机预测数字常量，（2）浅层启发法优先考虑早期变量赋值，以及（3）一种系统性机制的出现，用于分辨率赋值链。通过因果干预，我们发现模型学会了利用残差流作为可寻址的内存空间，特化的注意力头负责在标记位置间路由信息。这种机制使模型能够在层间动态跟踪变量绑定，从而实现准确的分辨率赋值。我们的结果展示了变换器模型如何在无显式架构支持的情况下学习实现系统性变量绑定，从而同时连接联结主义和符号方法。', 'title_zh': 'Transformer如何学习符号程序中的变量绑定？'}
{'arxiv_id': 'arXiv:2505.20890', 'title': 'Frequency Composition for Compressed and Domain-Adaptive Neural Networks', 'authors': 'Yoojin Kwon, Hongjun Suh, Wooseok Lee, Taesik Gong, Songyi Han, Hyung-Sin Kim', 'link': 'https://arxiv.org/abs/2505.20890', 'abstract': 'Modern on-device neural network applications must operate under resource constraints while adapting to unpredictable domain shifts. However, this combined challenge-model compression and domain adaptation-remains largely unaddressed, as prior work has tackled each issue in isolation: compressed networks prioritize efficiency within a fixed domain, whereas large, capable models focus on handling domain shifts. In this work, we propose CoDA, a frequency composition-based framework that unifies compression and domain adaptation. During training, CoDA employs quantization-aware training (QAT) with low-frequency components, enabling a compressed model to selectively learn robust, generalizable features. At test time, it refines the compact model in a source-free manner (i.e., test-time adaptation, TTA), leveraging the full-frequency information from incoming data to adapt to target domains while treating high-frequency components as domain-specific cues. LFC are aligned with the trained distribution, while HFC unique to the target distribution are solely utilized for batch normalization. CoDA can be integrated synergistically into existing QAT and TTA methods. CoDA is evaluated on widely used domain-shift benchmarks, including CIFAR10-C and ImageNet-C, across various model architectures. With significant compression, it achieves accuracy improvements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over the full-precision TTA baseline.', 'abstract_zh': '现代设备上的神经网络应用必须在资源受限的情况下适应不可预测的领域转移。然而，这个结合挑战——模型压缩和领域适应——仍未得到充分解决，因为先前的工作各自处理这两个问题：压缩网络侧重于固定领域内的效率，而大型能力强的模型则专注于处理领域转移。在网络中，我们提出CoDA，一种基于频率组成的框架，将压缩和领域适应统一起来。在训练过程中，CoDA 使用具有低频组件的量化感知训练（QAT），使压缩模型能够选择性地学习稳健且通用的特征。在测试时，它以源代码免费的方式（即，在测试时适应，TTA）逐步优化紧凑模型，利用传入数据的全频信息来适应目标领域，同时将高频组件作为领域特定线索。低频成分与训练分布对齐，而独有的高频成分专门用于批量标准化。CoDA 可以以协同方式整合到现有的 QAT 和 TTA 方法中。CoDA 在包括 CIFAR10-C 和 ImageNet-C 的广泛使用的领域转移基准上进行了评估，涵盖各种模型架构。通过显著压缩，它在 CIFAR10-C 上相对于全精度 TTA 基线实现了 7.96% 的准确率提升，在 ImageNet-C 上实现了 5.37% 的准确率提升。', 'title_zh': '压缩与领域自适应神经网络的频域组成'}
{'arxiv_id': 'arXiv:2505.20888', 'title': 'EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models', 'authors': 'Chengyu Wang, Junbing Yan, Wenrui Cai, Yuanhao Yue, Jun Huang', 'link': 'https://arxiv.org/abs/2505.20888', 'abstract': "In this paper, we present EasyDistill, a comprehensive toolkit designed for effective black-box and white-box knowledge distillation (KD) of large language models (LLMs). Our framework offers versatile functionalities, including data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning techniques specifically tailored for KD scenarios. The toolkit accommodates KD functionalities for both System 1 (fast, intuitive) and System 2 (slow, analytical) models. With its modular design and user-friendly interface, EasyDistill empowers researchers and industry practitioners to seamlessly experiment with and implement state-of-the-art KD strategies for LLMs. In addition, EasyDistill provides a series of robust distilled models and KD-based industrial solutions developed by us, along with the corresponding open-sourced datasets, catering to a variety of use cases. Furthermore, we describe the seamless integration of EasyDistill into Alibaba Cloud's Platform for AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for LLMs more accessible and impactful within the NLP community.", 'abstract_zh': 'EasyDistill：面向大型语言模型的全面知识蒸馏工具包', 'title_zh': 'EasyDistill: 一种有效的大型语言模型知识蒸馏综合工具包'}
{'arxiv_id': 'arXiv:2505.20881', 'title': 'Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization', 'authors': 'Yiding Shi, Jianan Zhou, Wen Song, Jieyi Bi, Yaoxin Wu, Jie Zhang', 'link': 'https://arxiv.org/abs/2505.20881', 'abstract': 'Heuristic design with large language models (LLMs) has emerged as a promising approach for tackling combinatorial optimization problems (COPs). However, existing approaches often rely on manually predefined evolutionary computation (EC) optimizers and single-task training schemes, which may constrain the exploration of diverse heuristic algorithms and hinder the generalization of the resulting heuristics. To address these issues, we propose Meta-Optimization of Heuristics (MoH), a novel framework that operates at the optimizer level, discovering effective optimizers through the principle of meta-learning. Specifically, MoH leverages LLMs to iteratively refine a meta-optimizer that autonomously constructs diverse optimizers through (self-)invocation, thereby eliminating the reliance on a predefined EC optimizer. These constructed optimizers subsequently evolve heuristics for downstream tasks, enabling broader heuristic exploration. Moreover, MoH employs a multi-task training scheme to promote its generalization capability. Experiments on classic COPs demonstrate that MoH constructs an effective and interpretable meta-optimizer, achieving state-of-the-art performance across various downstream tasks, particularly in cross-size settings.', 'abstract_zh': '使用大语言模型的启发式设计优化框架（MoH）：基于元学习的原则发现有效的优化器', 'title_zh': '通过元优化的大语言模型实现可泛化的启发式生成'}
{'arxiv_id': 'arXiv:2505.20875', 'title': 'Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties', 'authors': 'Jiyoung Lee, Seungho Kim, Jieun Han, Jun-Min Lee, Kitaek Kim, Alice Oh, Edward Choi', 'link': 'https://arxiv.org/abs/2505.20875', 'abstract': 'Large Language Models (LLMs) are predominantly evaluated on Standard American English (SAE), often overlooking the diversity of global English varieties. This narrow focus may raise fairness concerns as degraded performance on non-standard varieties can lead to unequal benefits for users worldwide. Therefore, it is critical to extensively evaluate the linguistic robustness of LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a framework that automatically transforms SAE datasets into multiple English varieties to evaluate the linguistic robustness. Our framework combines (1) linguistics expert knowledge to curate variety-specific features and transformation guidelines from linguistic literature and corpora, and (2) LLM-based transformations to ensure both linguistic validity and scalability. Using Trans-EnV, we transform six benchmark datasets into 38 English varieties and evaluate seven state-of-the-art LLMs. Our results reveal significant performance disparities, with accuracy decreasing by up to 46.3% on non-standard varieties. These findings highlight the importance of comprehensive linguistic robustness evaluation across diverse English varieties. Each construction of Trans-EnV was validated through rigorous statistical testing and consultation with a researcher in the field of second language acquisition, ensuring its linguistic validity. Our \\href{this https URL}{code} and \\href{this https URL}{datasets} are publicly available.', 'abstract_zh': '大型语言模型（LLMs）主要基于标准美式英语（SAE）进行评估，往往忽视了全球英语变体的多样性。这种狭窄的焦点可能会引发公平性问题，因为对非标准变体表现不佳可能导致全球用户受益不均。因此，全面评估LLMs在多种非标准英语变体上的语言稳健性至关重要。我们介绍了Trans-EnV框架，自动将SAE数据集转换为多个英语变体以评估语言稳健性。该框架结合了（1）语言学专家知识，从语言学文献和语料库中筛选特定变体的特征和转换指南，以及（2）基于LLMs的转换，以确保语言的有效性和可扩展性。使用Trans-EnV，我们将六个基准数据集转换为38种英语变体，并评估了七种最先进的LLMs。我们的结果显示，在非标准变体上的准确性可降低高达46.3%。这些发现强调了在多种英语变体上进行全面语言稳健性评估的重要性。每个Trans-EnV的构建都通过严格的统计测试和第二语言习得领域研究人员的咨询进行了验证，确保了其语言的有效性。我们的代码和数据集已经公开。', 'title_zh': '跨英语变体评估大规模语言模型语言鲁棒性的框架'}
{'arxiv_id': 'arXiv:2505.20872', 'title': 'In Context Learning with Vision Transformers: Case Study', 'authors': 'Antony Zhao, Alex Proshkin, Fergal Hennessy, Francesco Crivelli', 'link': 'https://arxiv.org/abs/2505.20872', 'abstract': 'Large transformer models have been shown to be capable of performing in-context learning. By using examples in a prompt as well as a query, they are capable of performing tasks such as few-shot, one-shot, or zero-shot learning to output the corresponding answer to this query. One area of interest to us is that these transformer models have been shown to be capable of learning the general class of certain functions, such as linear functions and small 2-layer neural networks, on random data (Garg et al, 2023). We aim to extend this to the image space to analyze their capability to in-context learn more complex functions on the image space, such as convolutional neural networks and other methods.', 'abstract_zh': '大型变压器模型已被证明能够进行在上下文学习。通过在提示中使用示例和查询，它们能够执行少样本、单样本或零样本学习，输出相应的查询答案。我们感兴趣的一个领域是这些变压器模型能够从随机数据中学习某些函数类，如线性函数和小型2层神经网络（Garg等人，2023）。我们旨在将这一点扩展到图像空间，分析它们在图像空间中学习更复杂函数的能力，如卷积神经网络和其他方法。', 'title_zh': '基于视觉变换器的上下文学习：案例研究'}
{'arxiv_id': 'arXiv:2505.20868', 'title': 'Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech', 'authors': 'Nam-Gyu Kim, Deok-Hyeon Cho, Seung-Bin Kim, Seong-Whan Lee', 'link': 'https://arxiv.org/abs/2505.20868', 'abstract': 'Recent advances in expressive text-to-speech (TTS) have introduced diverse methods based on style embedding extracted from reference speech. However, synthesizing high-quality expressive speech remains challenging. We propose Spotlight-TTS, which exclusively emphasizes style via voiced-aware style extraction and style direction adjustment. Voiced-aware style extraction focuses on voiced regions highly related to style while maintaining continuity across different speech regions to improve expressiveness. We adjust the direction of the extracted style for optimal integration into the TTS model, which improves speech quality. Experimental results demonstrate that Spotlight-TTS achieves superior performance compared to baseline models in terms of expressiveness, overall speech quality, and style transfer capability. Our audio samples are publicly available.', 'abstract_zh': 'Recent Advances in Spotlight-TTS: Exclusive Style Emphasis via Voiced-Aware Style Extraction and Style Direction Adjustment', 'title_zh': 'Spotlight-TTS：基于语音感知风格提取和风格方向调整的表达性文本到语音生成'}
{'arxiv_id': 'arXiv:2505.20866', 'title': 'Respond to Change with Constancy: Instruction-tuning with LLM for Non-I.I.D. Network Traffic Classification', 'authors': 'Xinjie Lin, Gang Xiong, Gaopeng Gou, Wenqi Dong, Jing Yu, Zhen Li, Wei Xia', 'link': 'https://arxiv.org/abs/2505.20866', 'abstract': "Encrypted traffic classification is highly challenging in network security due to the need for extracting robust features from content-agnostic traffic data. Existing approaches face critical issues: (i) Distribution drift, caused by reliance on the closedworld assumption, limits adaptability to realworld, shifting patterns; (ii) Dependence on labeled data restricts applicability where such data is scarce or unavailable. Large language models (LLMs) have demonstrated remarkable potential in offering generalizable solutions across a wide range of tasks, achieving notable success in various specialized fields. However, their effectiveness in traffic analysis remains constrained by challenges in adapting to the unique requirements of the traffic domain. In this paper, we introduce a novel traffic representation model named Encrypted Traffic Out-of-Distribution Instruction Tuning with LLM (ETooL), which integrates LLMs with knowledge of traffic structures through a self-supervised instruction tuning paradigm. This framework establishes connections between textual information and traffic interactions. ETooL demonstrates more robust classification performance and superior generalization in both supervised and zero-shot traffic classification tasks. Notably, it achieves significant improvements in F1 scores: APP53 (I.I.D.) to 93.19%(6.62%) and 92.11%(4.19%), APP53 (O.O.D.) to 74.88%(18.17%) and 72.13%(15.15%), and ISCX-Botnet (O.O.D.) to 95.03%(9.16%) and 81.95%(12.08%). Additionally, we construct NETD, a traffic dataset designed to support dynamic distributional shifts, and use it to validate ETooL's effectiveness under varying distributional conditions. Furthermore, we evaluate the efficiency gains achieved through ETooL's instruction tuning approach.", 'abstract_zh': '基于大语言模型的加密流量离分布指令调优方法（ETooL）', 'title_zh': '保持不变性以应对变化：面向非-i.i.d.网络流量分类的指令调优大模型'}
{'arxiv_id': 'arXiv:2505.20854', 'title': 'An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks', 'authors': 'Xin Zhou, Kisub Kim, Ting Zhang, Martin Weyssow, Luis F. Gomes, Guang Yang, David Lo', 'link': 'https://arxiv.org/abs/2505.20854', 'abstract': "Large Language Models (LLMs) and other automated techniques have been increasingly used to support software developers by generating software artifacts such as code snippets, patches, and comments. However, accurately assessing the correctness of these generated artifacts remains a significant challenge. On one hand, human evaluation provides high accuracy but is labor-intensive and lacks scalability. On the other hand, other existing automatic evaluation metrics are scalable and require minimal human effort, but they often fail to accurately reflect the actual correctness of generated software artifacts.\nIn this paper, we present SWE-Judge, the first evaluation metric for LLM-as-Ensemble-Judge specifically designed to accurately assess the correctness of generated software artifacts. SWE-Judge first defines five distinct evaluation strategies, each implemented as an independent judge. A dynamic team selection mechanism then identifies the most appropriate subset of judges to produce a final correctness score through ensembling. We evaluate SWE-Judge across a diverse set of software engineering (SE) benchmarks, including CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess. These benchmarks span three SE tasks: code generation, automated program repair, and code summarization. Experimental results demonstrate that SWE-Judge consistently achieves a higher correlation with human judgments, with improvements ranging from 5.9% to 183.8% over existing automatic metrics. Furthermore, SWE-Judge reaches agreement levels with human annotators that are comparable to inter-annotator agreement in code generation and program repair tasks. These findings underscore SWE-Judge's potential as a scalable and reliable alternative to human evaluation.", 'abstract_zh': '大规模语言模型（LLMs）和其他自动化技术已被广泛用于通过生成代码片段、补丁和注释等方式支持软件开发者。然而，准确评估这些生成的软件 artefacts 的正确性仍是一个重大挑战。一方面，人工评估可以提供高准确性，但劳动密集且缺乏可扩展性。另一方面，现有的其他自动评估指标具有可扩展性且需要少量人工努力，但往往无法准确反映生成的软件 artefacts 的实际正确性。\n\n在本文中，我们介绍了 SWE-Judge，这是第一个专为准确评估生成软件 artefacts 正确性而设计的 LLM-as-Ensemble-Judge 评估指标。SWE-Judge 首先定义了五种不同的评估策略，每种策略都作为一个独立的法官实现。然后，动态团队选择机制通过集成识别出最合适的法官子集，以生成最终的正确性分数。我们在多样化的软件工程（SE）基准测试中评估了 SWE-Judge，包括 CoNaLa、Card2Code、HumanEval-X、APPS、APR-Assess 和 Summary-Assess。这些基准测试涵盖了三种 SE 任务：代码生成、自动程序修复和代码摘要。实验结果表明，SWE-Judge 与人类判断的一致性始终更高，与现有自动指标相比，改进幅度从 5.9% 到 183.8% 不等。此外，SWE-Judge 达到的人类注释者之间的一致性水平与代码生成和程序修复任务中的注释者间一致性水平相当。这些发现突显了 SWE-Judge 作为人工评估的可扩展和可靠替代方案的潜力。', 'title_zh': '基于LLM-as-Judge的评价指标以缩小软件工程任务与人类评估的差距'}
{'arxiv_id': 'arXiv:2505.20853', 'title': 'Cooperation of Experts: Fusing Heterogeneous Information with Large Margin', 'authors': 'Shuo Wang, Shunyang Huang, Jinghui Yuan, Zhixiang Shen, Zhao Kang', 'link': 'https://arxiv.org/abs/2505.20853', 'abstract': "Fusing heterogeneous information remains a persistent challenge in modern data analysis. While significant progress has been made, existing approaches often fail to account for the inherent heterogeneity of object patterns across different semantic spaces. To address this limitation, we propose the Cooperation of Experts (CoE) framework, which encodes multi-typed information into unified heterogeneous multiplex networks. By overcoming modality and connection differences, CoE provides a powerful and flexible model for capturing the intricate structures of real-world complex data. In our framework, dedicated encoders act as domain-specific experts, each specializing in learning distinct relational patterns in specific semantic spaces. To enhance robustness and extract complementary knowledge, these experts collaborate through a novel large margin mechanism supported by a tailored optimization strategy. Rigorous theoretical analyses guarantee the framework's feasibility and stability, while extensive experiments across diverse benchmarks demonstrate its superior performance and broad applicability. Our code is available at this https URL.", 'abstract_zh': '融合异构信息仍然是现代数据分析中的一个持续挑战。虽然已取得显著进展，但现有方法往往未能充分考虑对象模式在不同语义空间中的固有异构性。为应对这一局限，我们提出了一种专家合作（CoE）框架，该框架将多种类型的信息编码为统一的异构多层网络。通过克服模态和连接差异，CoE 提供了一种强大且灵活的模型，用于捕获现实世界复杂数据的精细结构。在我们的框架中，专用编码器作为领域特定专家，各自专门学习特定语义空间中的独特关系模式。为了增强鲁棒性并提取互补知识，这些专家通过一种新型的大边际机制协同工作，该机制支持定制的优化策略。严格的理论分析保证了框架的可行性和稳定性，而跨越多种基准的广泛实验证明了其优越性能和广泛的适用性。我们的代码可在以下链接获取：this https URL。', 'title_zh': '专家合作：大 Margin 融合异类型信息'}
{'arxiv_id': 'arXiv:2505.20824', 'title': 'MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems', 'authors': 'Kai Chen, Taihang Zhen, Hewei Wang, Kailai Liu, Xinfeng Li, Jing Huo, Tianpei Yang, Jinfeng Xu, Wei Dong, Yang Gao', 'link': 'https://arxiv.org/abs/2505.20824', 'abstract': "As large language models (LLMs) are increasingly deployed in healthcare, ensuring their safety, particularly within collaborative multi-agent configurations, is paramount. In this paper we introduce MedSentry, a benchmark comprising 5 000 adversarial medical prompts spanning 25 threat categories with 100 subthemes. Coupled with this dataset, we develop an end-to-end attack-defense evaluation pipeline to systematically analyze how four representative multi-agent topologies (Layers, SharedPool, Centralized, and Decentralized) withstand attacks from 'dark-personality' agents. Our findings reveal critical differences in how these architectures handle information contamination and maintain robust decision-making, exposing their underlying vulnerability mechanisms. For instance, SharedPool's open information sharing makes it highly susceptible, whereas Decentralized architectures exhibit greater resilience thanks to inherent redundancy and isolation. To mitigate these risks, we propose a personality-scale detection and correction mechanism that identifies and rehabilitates malicious agents, restoring system safety to near-baseline levels. MedSentry thus furnishes both a rigorous evaluation framework and practical defense strategies that guide the design of safer LLM-based multi-agent systems in medical domains.", 'abstract_zh': '随着大型语言模型在医疗领域的广泛应用，确保其安全性，特别是在协作多智能体配置中，变得至关重要。本文介绍了MedSentry基准，其中包括5000个 adversarial 医疗提示，涵盖25个威胁类别和100个子主题。结合该数据集，我们开发了一个端到端的攻击-防御评估管道，以系统分析四种代表性多智能体拓扑结构（Layers、SharedPool、Centralized和Decentralized）如何抵御具有“黑暗人格”的智能体发起的攻击。研究发现揭示了这些架构在处理信息污染和维持稳健决策方面的关键差异，暴露了其潜在的脆弱性机制。例如，SharedPool的开放信息共享使其高度易受攻击，而Decentralized架构由于固有的冗余性和隔离性表现出更强的抗灾能力。为了缓解这些风险，我们提出了一种基于人格尺度的检测和矫正机制，以识别并康复恶意智能体，恢复系统的安全性至接近基线水平。MedSentry因此提供了一个严谨的评估框架和实用的防御策略，指导医疗领域基于大型语言模型的多智能体系统的安全设计。', 'title_zh': 'MedSentry: 理解和缓解医疗LLM多智能体系统中的安全风险'}
{'arxiv_id': 'arXiv:2505.20813', 'title': 'RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph', 'authors': 'Junsik Kim, Jinwook Park, Kangil Kim', 'link': 'https://arxiv.org/abs/2505.20813', 'abstract': 'In knowledge graph embedding, leveraging relation-specific entity-transformation has markedly enhanced performance. However, the consistency of embedding differences before and after transformation remains unaddressed, risking the loss of valuable inductive bias inherent in the embeddings. This inconsistency stems from two problems. First, transformation representations are specified for relations in a disconnected manner, allowing dissimilar transformations and corresponding entity-embeddings for similar relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter Based on Relations) disrupts this consistency through excessive concentration of entity embeddings under entity-based regularization, generating indistinguishable score distributions among relations. In this paper, we introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF), containing more consistent entity-transformation characterized by three features: 1) shared affine transformation of relation embeddings across all relations, 2) rooted entity-transformation that adds an entity embedding to its change represented by the transformed vector, and 3) normalization of the change to prevent scale reduction. To amplify the advantages of consistency that preserve semantics on embeddings, RSCF adds relation transformation and prediction modules for enhancing the semantics. In knowledge graph completion tasks with distance-based and tensor decomposition models, RSCF significantly outperforms state-of-the-art KGE methods, showing robustness across all relations and their frequencies.', 'abstract_zh': '在知识图嵌入中，利用关系特定的实体转换显著提高了性能。然而，转换前后嵌入差异的一致性尚未得到解决，这可能会导致嵌入中固有的宝贵归纳偏置的丢失。这种不一致性来源于两个问题。首先，关系转换表示是以断开的方式为关系指定的，允许类似关系有不同的转换和相应的实体嵌入。其次，基于关系的通用插件方法作为SFBR（基于关系的语义过滤器）通过在基于实体的正则化下过度集中实体嵌入，破坏了这种一致性，导致不同关系之间的得分分布难以区分。在本文中，我们引入了一种包含更一致的实体转换的插件KGE方法——关系语义一致过滤器（RSCF），其特点是：1) 所有关系共享的仿射转换，2) 根植的实体转换，将实体嵌入与其转换向量表示的变化相加，3) 规范化变化以防止缩放减少。为了进一步增强保持嵌入语义的一致性优势，RSCF增加了关系转换和预测模块。在基于距离和张量分解的知识图填充任务中，RSCF显著优于现有的KGE方法，并且在所有关系及其频率上表现出稳健性。', 'title_zh': 'RSCF: 关系语义一致过滤器在知识图实体嵌入中的应用'}
{'arxiv_id': 'arXiv:2505.20794', 'title': 'VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion', 'authors': 'Joon-Seung Choi, Dong-Min Byun, Hyung-Seok Oh, Seong-Whan Lee', 'link': 'https://arxiv.org/abs/2505.20794', 'abstract': 'Controlling singing style is crucial for achieving an expressive and natural singing voice. Among the various style factors, vibrato plays a key role in conveying emotions and enhancing musical depth. However, modeling vibrato remains challenging due to its dynamic nature, making it difficult to control in singing voice conversion. To address this, we propose VibESVC, a controllable singing voice conversion model that explicitly extracts and manipulates vibrato using discrete wavelet transform. Unlike previous methods that model vibrato implicitly, our approach decomposes the F0 contour into frequency components, enabling precise transfer. This allows vibrato control for enhanced flexibility. Experimental results show that VibE-SVC effectively transforms singing styles while preserving speaker similarity. Both subjective and objective evaluations confirm high-quality conversion.', 'abstract_zh': '控制歌声风格对于实现富有表现力和自然的歌声至关重要。在各种风格因素中，颤音在传达情感和增强音乐深度方面起着关键作用。然而，由于颤音的动态特性，其建模依然具有挑战性，使得在歌声转换中难以控制。为此，我们提出了一种可控歌声转换模型VibESVC，该模型明确地通过离散小波变换提取和操控颤音。与以前隐式建模颤音的方法不同，我们的方法将基频轮廓分解为频率成分，从而实现精确传递。这使得颤音控制更加灵活。实验结果表明，VibE-SVC能够在保持说话人相似性的同时有效转换歌声风格。主观和客观评估均证实了高质量的转换效果。', 'title_zh': 'VibE-SVC: 基于高频率基频轮廓的振动提取与歌声转换'}
{'arxiv_id': 'arXiv:2505.20793', 'title': 'Rendering-Aware Reinforcement Learning for Vector Graphics Generation', 'authors': 'Juan A. Rodriguez, Haotian Zhang, Abhay Puri, Aarash Feizi, Rishav Pramanik, Pascal Wichmann, Arnab Mondal, Mohammad Reza Samsami, Rabiul Awal, Perouz Taslakian, Spandana Gella, Sai Rajeswar, David Vazquez, Christopher Pal, Marco Pedersoli', 'link': 'https://arxiv.org/abs/2505.20793', 'abstract': 'Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization.', 'abstract_zh': '可扩展矢量图形 (SVG) 提供了一种强大的格式，用于以可解释的代码表示视觉设计。最近在视觉-语言模型（VLMs）方面的进展通过将问题框定为代码生成任务并利用大规模预训练，使高质量SVG生成成为可能。VLMs特别适合此任务，因为它们能够捕捉全局语义和细粒度的视觉模式，并在视觉、自然语言和代码领域之间转移知识。然而，现有的VLM方法在训练过程中通常无法生成忠实且高效的SVG，因为它们从未观察过渲染后的图像。尽管自回归SVG代码生成的可微分渲染尚未可用，但仍可以将渲染输出与原始输入进行对比，从而提供适用于强化学习（RL）的评估反馈。我们介绍了基于渲染反馈的RLRF（Reinforcement Learning from Rendering Feedback），一种利用渲染SVG输出反馈增强自回归VLM中SVG生成的RL方法。给定输入图像，该模型生成SVG滚出，渲染并与原始图像进行比较以计算奖励。这种视觉保真度反馈引导模型生成更准确、更高效且语义一致的SVG。RLRF 显著优于监督微调，解决了常见失败模式，并使生成具有强大结构理解和泛化能力的精确且高质量SVG成为可能。', 'title_zh': '面向渲染的强化学习在矢量图形生成中的应用'}
{'arxiv_id': 'arXiv:2505.20783', 'title': 'FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation', 'authors': 'Jiaping Xiao, Cheng Wen Tsao, Yuhang Zhang, Mir Feroskhan', 'link': 'https://arxiv.org/abs/2505.20783', 'abstract': 'Path planning is a critical component in autonomous drone operations, enabling safe and efficient navigation through complex environments. Recent advances in foundation models, particularly large language models (LLMs) and vision-language models (VLMs), have opened new opportunities for enhanced perception and intelligent decision-making in robotics. However, their practical applicability and effectiveness in global path planning remain relatively unexplored. This paper proposes foundation model-guided path planners (FM-Planner) and presents a comprehensive benchmarking study and practical validation for drone path planning. Specifically, we first systematically evaluate eight representative LLM and VLM approaches using standardized simulation scenarios. To enable effective real-time navigation, we then design an integrated LLM-Vision planner that combines semantic reasoning with visual perception. Furthermore, we deploy and validate the proposed path planner through real-world experiments under multiple configurations. Our findings provide valuable insights into the strengths, limitations, and feasibility of deploying foundation models in real-world drone applications and providing practical implementations in autonomous flight. Project site: this https URL.', 'abstract_zh': '路径规划是自主无人机操作的关键组件，能够实现通过复杂环境的安全高效导航。近期基础模型的发展，特别是大规模语言模型（LLMs）和视觉-语言模型（VLMs），为机器人增强感知和智能决策提供了新的机会。然而，这些模型在全局路径规划中的实用性和有效性尚未得到充分探索。本文提出了一种基础模型引导的路径规划器（FM-Planner），并对其进行了全面的基准测试和实 практичес验证。具体地，我们首先使用标准化的仿真场景系统性地评估了八种代表性的LLM和VLM方法。为了实现有效的实时导航，我们设计了一个结合语义推理和视觉感知的LLM-Vision规划器。此外，我们在多种配置下部署并验证了所提出的路径规划器。我们的研究结果为在实际无人机应用中部署基础模型提供了宝贵见解，并提供了自主飞行的实际实施方案。项目页面：这个链接', 'title_zh': 'FM-Planner: 基于基础模型的自主无人机路径规划'}
{'arxiv_id': 'arXiv:2505.20776', 'title': 'SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences', 'authors': 'Jungyoub Cha, Hyunjong Kim, Sungzoon Cho', 'link': 'https://arxiv.org/abs/2505.20776', 'abstract': "Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), but its performance degrades on long inputs due to increased attention cost and reduced draft accuracy. We introduce SpecExtend, a drop-in enhancement that improves the performance of speculative decoding on long sequences without any additional training. SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention into both the draft and target models, reducing latency across all stages. To improve draft accuracy and speed, we propose Cross-model Retrieval, a novel KV cache update strategy that uses the target model's attention scores to dynamically select relevant context for the draft model. Extensive evaluations on three long-context understanding datasets show that SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for inputs up to 16K tokens, providing an effective solution for speculative decoding of long sequences. The code is available at this https URL .", 'abstract_zh': 'SpecExtend：一种提升长序列推测解码性能的即插即用增强方法', 'title_zh': 'SpecExtend: 长序列推测解码的一项即插即用增强技术'}
{'arxiv_id': 'arXiv:2505.20771', 'title': 'Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems', 'authors': 'Heng Tang, Feng Liu, Xinbo Chen, Jiawei Chen, Bohao Wang, Changwang Zhang, Jun Wang, Yuegang Sun, Bingde Hu, Can Wang', 'link': 'https://arxiv.org/abs/2505.20771', 'abstract': 'Recent years have witnessed extensive exploration of Large Language Models (LLMs) on the field of Recommender Systems (RS). There are currently two commonly used strategies to enable LLMs to have recommendation capabilities: 1) The "Guidance-Only" strategy uses in-context learning to exploit and amplify the inherent semantic understanding and item recommendation capabilities of LLMs; 2) The "Tuning-Only" strategy uses supervised fine-tuning (SFT) to fine-tune LLMs with the aim of fitting them to real recommendation data. However, neither of these strategies can effectively bridge the gap between the knowledge space of LLMs and recommendation, and their performance do not meet our expectations.\nTo better enable LLMs to learn recommendation knowledge, we combine the advantages of the above two strategies and proposed a novel "Guidance+Tuning" method called Self-Optimized Fine-Tuning (SOFT), which adopts the idea of curriculum learning. It first employs self-distillation to construct an auxiliary easy-to-learn but meaningful dataset from a fine-tuned LLM. Then it further utilizes a self-adaptive curriculum scheduler to enable LLMs to gradually learn from simpler data (self-distilled data) to more challenging data (real RS data). Extensive experiments demonstrate that SOFT significantly enhances the recommendation accuracy (37.59\\% on average) of LLM-based methods. The code is available via this https URL', 'abstract_zh': '最近几年，大量研究探索了大语言模型（LLMs）在推荐系统（RS）领域的应用。目前有两种常用策略来使LLMs具备推荐能力：1）“指导唯一”策略利用上下文学习来利用和放大LLMs固有的语义理解和物品推荐能力；2）“微调唯一”策略使用监督微调（SFT）来对LLMs进行微调，使其适应实际的推荐数据。然而，这两种策略都无法有效弥合理想的知识空间与推荐之间的差距，其性能也没有达到预期。\n\n为了更好地使LLMs学习推荐知识，我们结合了上述两种策略的优点，提出了一种新的“指导+微调”方法——自优化微调（SOFT），该方法采用了课程学习的思想。首先，通过自我蒸馏从微调后的LLMs中构建一个辅助的易于学习但有意义的数据集。然后，进一步利用自适应课程调度器，使LLMs能够逐步从简单数据（自我蒸馏数据）到更具挑战的数据（真实的推荐系统数据）进行学习。大量实验表明，SOFT显著提升了基于LLMs的方法的推荐准确性（平均提高37.59%）。代码可通过以下链接获得。', 'title_zh': '填补差距：基于LLM的推荐系统自优化微调方法'}
{'arxiv_id': 'arXiv:2505.20767', 'title': 'CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models', 'authors': 'Xiaqiang Tang, Jian Li, Keyu Hu, Du Nan, Xiaolong Li, Xi Zhang, Weigao Sun, Sihong Xie', 'link': 'https://arxiv.org/abs/2505.20767', 'abstract': 'Faithfulness hallucination are claims generated by a Large Language Model (LLM) not supported by contexts provided to the LLM. Lacking assessment standard, existing benchmarks only contain "factual statements" that rephrase source materials without marking "cognitive statements" that make inference from the given context, making the consistency evaluation and optimization of cognitive statements difficult. Inspired by how an evidence is assessed in the legislative domain, we design a rigorous framework to assess different levels of faithfulness of cognitive statements and create a benchmark dataset where we reveal insightful statistics. We design an annotation pipeline to create larger benchmarks for different LLMs automatically, and the resulting larger-scale CogniBench-L dataset can be used to train accurate cognitive hallucination detection model. We release our model and dataset at: this https URL', 'abstract_zh': '忠实性幻觉是由大规模语言模型（LLM）生成的断言，这些断言未得到提供的上下文支持。缺乏评估标准，现有基准仅包含“事实陈述”，这些陈述重新表述了原始材料而没有标记从给定上下文推断出的“认知陈述”，这使得认知陈述的一致性评估和优化变得困难。受到立法领域中证据评估方式的启发，我们设计了一种严格的框架来评估不同层次的忠实性，并创建了一个基准数据集，其中展示了有价值的统计数据。我们设计了一种标注管道，以自动为不同的LLM创建更大的基准数据集，从而生成的更大规模的CogniBench-L数据集可以用于训练精确的认知幻觉检测模型。我们已将我们的模型和数据集发布于：this https URL。', 'title_zh': 'CogniBench：一个受法律启发的框架及数据集，用于评估大型语言模型的认知忠实度'}
{'arxiv_id': 'arXiv:2505.20759', 'title': 'PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding', 'authors': 'Ansel Blume, Jeonghwan Kim, Hyeonjeong Ha, Elen Chatikyan, Xiaomeng Jin, Khanh Duy Nguyen, Nanyun Peng, Kai-Wei Chang, Derek Hoiem, Heng Ji', 'link': 'https://arxiv.org/abs/2505.20759', 'abstract': "Real-world objects are composed of distinctive, object-specific parts. Identifying these parts is key to performing fine-grained, compositional reasoning-yet, large multimodal models (LMMs) struggle to perform this seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM benchmark designed for pixel-level part grounding. We construct PARTONOMY from existing part datasets and our own rigorously annotated set of images, encompassing 862 part labels and 534 object labels for evaluation. Unlike existing datasets that simply ask models to identify generic parts, PARTONOMY uses specialized concepts (e.g., agricultural airplane), and challenges models to compare objects' parts, consider part-whole relationships, and justify textual predictions with visual segmentations. Our experiments demonstrate significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only 5.9% gIoU), highlighting a critical gap in their part grounding abilities. We note that existing segmentation-enabled LMMs (segmenting LMMs) have two key architectural shortcomings: they use special [SEG] tokens not seen during pretraining which induce distribution shift, and they discard predicted segmentations instead of using past predictions to guide future ones. To address these deficiencies, we train several part-centric LMMs and propose PLUM, a novel segmenting LMM that uses span tagging instead of segmentation tokens and that conditions on prior predictions in a feedback loop. We find that pretrained PLUM outperforms existing segmenting LMMs on reasoning segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM finetuned on our proposed Explanatory Part Segmentation task is competitive with segmenting LMMs trained on significantly more segmentation data. Our work opens up new avenues towards enabling fine-grained, grounded visual understanding in LMMs.", 'abstract_zh': '真实世界的对象由独特的、对象特定的部分组成。识别这些部分是进行精细粒度、组合推理的关键——然而，大型多模态模型（LMMs）难以完成这一看似简单的任务。在本工作中，我们引入了PARTONOMY，一个旨在进行像素级部分定位的LMM基准。我们从现有部分数据集和我们自己严格标注的图像集中构建了PARTONOMY，其中包括862个部分标签和534个对象标签用于评估。与仅要求模型识别通用部分的现有数据集不同，PARTONOMY 使用专门的概念（例如，农业飞机），并挑战模型比较对象的部分、考虑部分与整体的关系，并用视觉分割来解释文本预测。我们的实验展示了最先进的LMM（例如，LISA-13B仅达到5.9%的gIoU）的重大局限性，突显出他们在部分定位能力上的关键差距。我们注意到，现有的基于分割的LMM（即分割LMMs）具有两个关键的架构缺陷：它们使用特殊 [SEG] 标记，在预训练期间未见过，这会导致分布偏移，并且它们直接丢弃预测的分割而不是使用过去的预测来指导未来的预测。为了弥补这些缺陷，我们训练了几种部分为中心的LMM，并提出了一种新颖的基于分割的LMM（PLUM），其使用区间标记而不是分割标记，并在一个反馈循环中依赖于先前的预测。我们发现，预训练的PLUM在推理分割、VQA和视觉错觉基准测试中优于现有的基于分割的LMM，此外，使用我们提出的解释性部分分割任务进行微调的PLUM，其性能与在大量分割数据上训练的分割LMM相当。我们的工作为使LMM实现精细粒度和基于视觉的理解开辟了新的途径。', 'title_zh': '部分解析：具有部分级别视觉理解的大规模多模态模型'}
{'arxiv_id': 'arXiv:2505.20753', 'title': 'Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models', 'authors': 'Yufei Zhan, Hongyin Zhao, Yousong Zhu, Shurong Zheng, Fan Yang, Ming Tang, Jinqiao Wang', 'link': 'https://arxiv.org/abs/2505.20753', 'abstract': 'Large Multimodal Models (LMMs) have recently demonstrated remarkable visual understanding performance on both vision-language and vision-centric tasks. However, they often fall short in integrating advanced, task-specific capabilities for compositional reasoning, which hinders their progress toward truly competent general vision models. To address this, we present a unified visual reasoning mechanism that enables LMMs to solve complicated compositional problems by leveraging their intrinsic capabilities (e.g. grounding and visual understanding capabilities). Different from the previous shortcut learning mechanism, our approach introduces a human-like understanding-thinking-answering process, allowing the model to complete all steps in a single pass forwarding without the need for multiple inferences or external tools. This design bridges the gap between foundational visual capabilities and general question answering, encouraging LMMs to generate faithful and traceable responses for complex visual reasoning. Meanwhile, we curate 334K visual instruction samples covering both general scenes and text-rich scenes and involving multiple foundational visual capabilities. Our trained model, Griffon-R, has the ability of end-to-end automatic understanding, self-thinking, and reasoning answers. Comprehensive experiments show that Griffon-R not only achieves advancing performance on complex visual reasoning benchmarks including VSR and CLEVR, but also enhances multimodal capabilities across various benchmarks like MMBench and ScienceQA. Data, models, and codes will be release at this https URL soon.', 'abstract_zh': '大型多模态模型通过利用其内在能力（如grounding和视觉理解能力）解决复杂的组合问题的统一视觉推理机制', 'title_zh': '理解、思考与作答：大型多模态模型促进视觉推理的发展'}
{'arxiv_id': 'arXiv:2505.20751', 'title': 'Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform', 'authors': 'Zongcai Tan amd Dandan Zhang', 'link': 'https://arxiv.org/abs/2505.20751', 'abstract': 'Optical tweezers (OT) offer unparalleled capabilities for micromanipulation with submicron precision in biomedical applications. However, controlling conventional multi-trap OT to achieve cooperative manipulation of multiple complex-shaped microrobots in dynamic environments poses a significant challenge. To address this, we introduce Interactive OT Gym, a reinforcement learning (RL)-based simulation platform designed for OT-driven microrobotics. Our platform supports complex physical field simulations and integrates haptic feedback interfaces, RL modules, and context-aware shared control strategies tailored for OT-driven microrobot in cooperative biological object manipulation tasks. This integration allows for an adaptive blend of manual and autonomous control, enabling seamless transitions between human input and autonomous operation. We evaluated the effectiveness of our platform using a cell manipulation task. Experimental results show that our shared control system significantly improves micromanipulation performance, reducing task completion time by approximately 67% compared to using pure human or RL control alone and achieving a 100% success rate. With its high fidelity, interactivity, low cost, and high-speed simulation capabilities, Interactive OT Gym serves as a user-friendly training and testing environment for the development of advanced interactive OT-driven micromanipulation systems and control algorithms. For more details on the project, please see our website this https URL', 'abstract_zh': '光学 tweezers (OT) 在生物医学应用中提供了前所未有的 micron 级精度微操作能力。然而，控制传统的多陷阱 OT 在动态环境中实现复杂形状微机器人协同操作仍然面临重大挑战。为应对这一挑战，我们引入了 Interactive OT Gym，一个基于强化学习 (RL) 的仿真平台，旨在支持 OT 驱动的微机器人技术。该平台支持复杂的物理场仿真，并集成了触觉反馈接口、RL 模块和面向 OT 驱动微机器人的上下文感知共享控制策略，专为合作生物物体操作任务设计。该集成允许手动控制和自主控制的适应性结合，使得人类输入和自主操作之间的无缝过渡成为可能。我们使用细胞操作任务评估了该平台的有效性。实验结果表明，我们的共享控制系统显著提高了微操作性能，与仅使用纯人类或 RL 控制相比，任务完成时间减少了约 67%，并且成功率达到 100%。凭借其高保真度、交互性、低成本和高速仿真能力，Interactive OT Gym 成为开发高级互动 OT 驱动微操作系统和控制算法的用户友好型训练和测试环境。', 'title_zh': '交互式OT健身房：基于强化学习的互动光学捕获（OT）驱动的微机器人模拟平台'}
{'arxiv_id': 'arXiv:2505.20734', 'title': 'Adversarial bandit optimization for approximately linear functions', 'authors': 'Zhuoyu Cheng, Kohei Hatano, Eiji Takimoto', 'link': 'https://arxiv.org/abs/2505.20734', 'abstract': "We consider a bandit optimization problem for nonconvex and non-smooth functions, where in each trial the loss function is the sum of a linear function and a small but arbitrary perturbation chosen after observing the player's choice. We give both expected and high probability regret bounds for the problem. Our result also implies an improved high-probability regret bound for the bandit linear optimization, a special case with no perturbation. We also give a lower bound on the expected regret.", 'abstract_zh': '我们考虑非凸非光滑函数的多臂老虎机优化问题，在每次试验中，损失函数是线性函数与观察到玩家选择后引入的小但任意的扰动之和。我们给出了问题的期望后悔界和高概率后悔界。我们的结果还隐含着在无扰动情况下的多臂老虎机线性优化问题的改进的高概率后悔界。我们还给出了期望后悔的下界。', 'title_zh': '对抗性带宽优化近线性函数'}
{'arxiv_id': 'arXiv:2505.20730', 'title': 'What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals', 'authors': 'Shahrooz Pouryousef', 'link': 'https://arxiv.org/abs/2505.20730', 'abstract': "User-item interactions contain rich collaborative signals that form the backbone of many successful recommender systems. While recent work has explored the use of large language models (LLMs) for recommendation, it remains unclear whether LLMs can effectively reason over this type of collaborative information. In this paper, we conduct a systematic comparison between LLMs and classical matrix factorization (MF) models to assess LLMs' ability to leverage user-item interaction data. We further introduce a simple retrieval-augmented generation (RAG) method that enhances LLMs by grounding their predictions in structured interaction data. Our experiments reveal that current LLMs often fall short in capturing collaborative patterns inherent to MF models, but that our RAG-based approach substantially improves recommendation quality-highlighting a promising direction for future LLM-based recommenders.", 'abstract_zh': '用户-项目交互包含丰富的协作信号，形成许多成功推荐系统的核心。虽然近期研究探索了大型语言模型（LLMs）在推荐领域的应用，但仍不清楚LLMs是否能够有效地推理这种类型的协作信息。本文系统地比较了LLMs和经典矩阵分解（MF）模型，评估LLMs利用用户-项目交互数据的能力。我们进一步引入了一种简单的检索增强生成（RAG）方法，通过将预测与结构化的交互数据结合来增强LLMs。我们的实验揭示，当前的LLMs在捕捉与MF模型内在协作模式相关的特性方面常常表现不足，而基于我们的RAG方法显著提高了推荐质量，这指出了未来基于LLM的推荐系统的一个有前景的方向。', 'title_zh': 'LLM在推荐中的不足：通过检索增强协作信号填补差距'}
{'arxiv_id': 'arXiv:2505.20718', 'title': 'VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Visual-Language Models', 'authors': 'Kui Wu, Shuhang Xu, Hao Chen, Churan Wang, Zhoujun Li, Yizhou Wang, Fangwei Zhong', 'link': 'https://arxiv.org/abs/2505.20718', 'abstract': "We introduce a novel self-improving framework that enhances Embodied Visual Tracking (EVT) with Visual-Language Models (VLMs) to address the limitations of current active visual tracking systems in recovering from tracking failure. Our approach combines the off-the-shelf active tracking methods with VLMs' reasoning capabilities, deploying a fast visual policy for normal tracking and activating VLM reasoning only upon failure detection. The framework features a memory-augmented self-reflection mechanism that enables the VLM to progressively improve by learning from past experiences, effectively addressing VLMs' limitations in 3D spatial reasoning. Experimental results demonstrate significant performance improvements, with our framework boosting success rates by $72\\%$ with state-of-the-art RL-based approaches and $220\\%$ with PID-based methods in challenging environments. This work represents the first integration of VLM-based reasoning to assist EVT agents in proactive failure recovery, offering substantial advances for real-world robotic applications that require continuous target monitoring in dynamic, unstructured environments. Project website: this https URL.", 'abstract_zh': '我们引入了一种新型自我改进框架，通过结合视觉语言模型（VLMs）的能力来增强沉浸式视觉跟踪（EVT），以解决当前主动视觉跟踪系统在恢复跟踪失败方面存在的局限性。该方法将现成的主动跟踪方法与VLMs的推理能力相结合，在正常跟踪时部署快速视觉策略，并仅在检测到失败时激活VLM推理。该框架拥有一个增强的记忆自我反省机制，使VLM能够通过学习过往经验逐步改进，有效解决了VLM在三维空间推理方面的局限性。实验结果表明，与基于强化学习（RL）的方法相比，我们的框架在具有挑战性的环境中将成功率提升了72%，与基于PID的方法相比提升了220%。这项工作代表了VLM基于推理首次应用于帮助EVT代理实现主动故障恢复的整合，为需要在动态、非结构化环境中持续目标监控的实际机器人应用提供了显著的进步。项目网站：这个 https URL。', 'title_zh': '基于视觉语言模型的自我提升能力可以成为增强現實视觉跟踪的好助手'}
{'arxiv_id': 'arXiv:2505.20714', 'title': 'Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting', 'authors': 'Zechen Li, Lanqing Yang, Yiheng Bian, Hao Pan, Yongjian Fu, Yezhou Wang, Yi-Chao Chen, Guangtao Xue, Ju Ren', 'link': 'https://arxiv.org/abs/2505.20714', 'abstract': "This paper presents an innovative frequency-embedded 3D Gaussian splatting (3DGS) algorithm for wideband radio-frequency (RF) radiance field modeling, offering an advancement over the existing works limited to single-frequency modeling. Grounded in fundamental physics, we uncover the complex relationship between EM wave propagation behaviors and RF frequencies. Inspired by this, we design an EM feature network with attenuation and radiance modules to learn the complex relationships between RF frequencies and the key properties of each 3D Gaussian, specifically the attenuation factor and RF signal intensity. By training the frequency-embedded 3DGS model, we can efficiently reconstruct RF radiance fields at arbitrary unknown frequencies within a given 3D environment. Finally, we propose a large-scale power angular spectrum (PAS) dataset containing 50000 samples ranging from 1 to 100 GHz in 6 indoor environments, and conduct extensive experiments to verify the effectiveness of our method. Our approach achieves an average Structural Similarity Index Measure (SSIM) up to 0.72, and a significant improvement up to 17.8% compared to the current state-of-the-art (SOTA) methods trained on individual test frequencies. Additionally, our method achieves an SSIM of 0.70 without prior training on these frequencies, which represents only a 2.8% performance drop compared to models trained with full PAS data. This demonstrates our model's capability to estimate PAS at unknown frequencies. For related code and datasets, please refer to this https URL.", 'abstract_zh': '基于频域嵌入的三维高斯点云计算宽频带射频辐射场模型', 'title_zh': '基于频率嵌入的3D高斯点云的宽带射频辐射场建模'}
{'arxiv_id': 'arXiv:2505.20707', 'title': 'Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective', 'authors': 'Nicy Scaria, Silvester John Joseph Kennedy, Diksha Seth, Deepak Subramani', 'link': 'https://arxiv.org/abs/2505.20707', 'abstract': "Small Language Models (SLMs) offer computational efficiency and accessibility, making them promising for educational applications. However, their capacity for complex reasoning, particularly in domains such as physics, remains underexplored. This study investigates the high school physics reasoning capabilities of state-of-the-art SLMs (under 4 billion parameters), including instruct versions of Llama 3.2, Phi 4 Mini, Gemma 3, and Qwen series. We developed a comprehensive physics dataset from the OpenStax High School Physics textbook, annotated according to Bloom's Taxonomy, with LaTeX and plaintext mathematical notations. A novel cultural contextualization approach was applied to a subset, creating culturally adapted problems for Asian, African, and South American/Australian contexts while preserving core physics principles. Using an LLM-as-a-judge framework with Google's Gemini 2.5 Flash, we evaluated answer and reasoning chain correctness, along with calculation accuracy. The results reveal significant differences between the SLMs. Qwen 3 1.7B achieved high `answer accuracy' (85%), but `fully correct reasoning' was substantially low (38%). The format of the mathematical notation had a negligible impact on performance. SLMs exhibited varied performance across the physics topics and showed a decline in reasoning quality with increasing cognitive and knowledge complexity. In particular, the consistency of reasoning was largely maintained in diverse cultural contexts, especially by better performing models. These findings indicate that, while SLMs can often find correct answers, their underlying reasoning is frequently flawed, suggesting an overreliance on pattern recognition. For SLMs to become reliable educational tools in physics, future development must prioritize enhancing genuine understanding and the generation of sound, verifiable reasoning chains over mere answer accuracy.", 'abstract_zh': '小语言模型（SLMs）在教育应用中的计算效率和便捷性使其前景广阔，然而其在物理学等复杂领域中的推理能力仍需进一步探索。本研究考察了最新状态的小语言模型（参数量不足40亿），包括Llama 3.2、Phi 4 Mini、Gemma 3和Qwen系列的指令版本，在高中物理推理能力方面的表现。研究团队基于OpenStax高中物理教科书构建了一个全面的物理数据集，并按照布卢姆 taxonomy 进行标注，同时采用了LaTeX和纯文本数学符号表示。一种新颖的文化情境化方法被应用于部分数据，为亚洲、非洲和南美洲/澳大利亚等地的文化背景生成适应性问题，同时保留了核心物理原理。研究使用LLM-as-a-judge框架结合Google的Gemini 2.5 Flash，评估了答案和推理链的正确性以及计算准确性。结果表明，SLMs之间存在显著差异。Qwen 3 1.7B实现了较高的“答案准确率”（85%），但“完全正确”的推理仅有38%。数学符号的格式对性能几乎没有影响。SLMs在物理主题上的表现参差不齐，并且随着认知和知识复杂性的增加，推理质量呈现下降趋势。特别是在多样化文化背景下，推理的一致性主要由表现更好的模型保持。这些发现表明，尽管SLMs可以找到正确的答案，但其背后的推理往往存在问题，这表明它们过于依赖模式识别。为了使SLMs成为可靠的物理教育工具，未来的发展应更加强调提升真正的理解能力和生成坚实可验证的推理链，而不仅仅是答案准确性。', 'title_zh': '从小语言模型的多维度视角 dissecting 物理推理：教育视角下的分析'}
{'arxiv_id': 'arXiv:2505.20697', 'title': 'Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series', 'authors': 'Zachary C. Brown, David Carlson', 'link': 'https://arxiv.org/abs/2505.20697', 'abstract': "The field of hypothesis generation promises to reduce costs in neuroscience by narrowing the range of interventional studies needed to study various phenomena. Existing machine learning methods can generate scientific hypotheses from complex datasets, but many approaches assume causal relationships are static over time, limiting their applicability to systems with dynamic, state-dependent behavior, such as the brain. While some techniques attempt dynamic causal discovery through factor models, they often restrict relationships to linear patterns or impose other simplifying assumptions. We propose a novel method that models dynamic graphs as a conditionally weighted superposition of static graphs, where each static graph can capture nonlinear relationships. This approach enables the detection of complex, time-varying interactions between variables beyond linear limitations. Our method improves f1-scores of predicted dynamic causal patterns by roughly 22-28% on average over baselines in some of our experiments, with some improvements reaching well over 60%. A case study on real brain data demonstrates our method's ability to uncover relationships linked to specific behavioral states, offering valuable insights into neural dynamics.", 'abstract_zh': '假设生成领域有望通过缩小所需干预研究的范围来降低神经科学的成本。现有的机器学习方法可以从复杂的数据集中生成科学假设，但许多方法假设因果关系在时间上是静态的，限制了其在具有动态、状态依赖行为的系统（如大脑）中的应用。虽然一些技术通过因子模型尝试动态因果发现，但它们通常将关系限制为线性模式或施加其他简化假设。我们提出了一种新型方法，将动态图建模为条件加权的静态图的叠加，其中每个静态图可以捕捉非线性关系。该方法能够检测超出线性限制的复杂、随时间变化的变量间相互作用。在某些实验中，我们的方法相对于基线在预测动态因果模式的f1分数上提高了约22-28%，有的改进甚至超过60%。基于真实脑数据的案例研究展示了我们的方法能够发现与特定行为状态相关的深层关系，为神经动力学提供了有价值的见解。', 'title_zh': '基于生成因子模型的观测时间序列在神经科学中动态因果图假设生成'}
{'arxiv_id': 'arXiv:2505.20692', 'title': 'Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions', 'authors': 'Saharsh Barve, Andy Mao, Jiayue Melissa Shi, Prerna Juneja, Koustuv Saha', 'link': 'https://arxiv.org/abs/2505.20692', 'abstract': 'Recent advances in generative AI have enabled visual content creation through text-to-image (T2I) generation. However, despite their creative potential, T2I models often replicate and amplify societal stereotypes -- particularly those related to gender, race, and culture -- raising important ethical concerns. This paper proposes a theory-driven bias detection rubric and a Social Stereotype Index (SSI) to systematically evaluate social biases in T2I outputs. We audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and Stability AI Core -- using 100 queries across three categories -- geocultural, occupational, and adjectival. Our analysis reveals that initial outputs are prone to include stereotypical visual cues, including gendered professions, cultural markers, and western beauty norms. To address this, we adopted our rubric to conduct targeted prompt refinement using LLMs, which significantly reduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and 51% for adjectival queries. We complemented our quantitative analysis through a user study examining perceptions, awareness, and preferences around AI-generated biased imagery. Our findings reveal a key tension -- although prompt refinement can mitigate stereotypes, it can limit contextual alignment. Interestingly, users often perceived stereotypical images to be more aligned with their expectations. We discuss the need to balance ethical debiasing with contextual relevance and call for T2I systems that support global diversity and inclusivity while not compromising the reflection of real-world social complexity.', 'abstract_zh': 'recent 进展在生成式 AI 领域使得通过文本生成图像（T2I）创建视觉内容成为可能。然而，尽管 T2I 模型具有创造性潜力，它们往往会复制和放大与性别、种族和文化相关的社会刻板印象，引发了重要的伦理关切。本文提出了一种基于理论的偏见检测量表和社会刻板印象指数（SSI）来系统评估 T2I 输出中的社会偏见。我们使用 100 个查询，涵盖地理文化、职业和形容词三个类别，对三个主要的 T2I 模型输出——DALL-E-3、Midjourney-6.1 和 Stability AI Core 进行了审计。分析结果表明，初始输出倾向于包含刻板的视觉线索，包括性别化的职业、文化标志和西方美容规范。为了解决这个问题，我们采用我们的量表并利用大语言模型（LLMs）进行针对性的提示优化，这显著降低了偏见——地理文化的 SSI 下降了 61%，职业的 SSI 下降了 69%，形容词查询的 SSI 下降了 51%。我们通过一项用户研究补充了定量分析，该研究 examines 用户对 AI 生成的带有偏见的图像的感知、意识和偏好。我们的研究结果揭示了一个关键矛盾——尽管提示优化可以缓解刻板印象，但它可能会限制上下文一致性。有趣的是，用户往往认为刻板印象图像与他们的预期更一致。我们讨论了在伦理去偏见与上下文相关性之间取得平衡的必要性，并呼吁支持全球多样性和包容性的 T2I 系统，但同时不应牺牲对现实社会复杂性的反映。', 'title_zh': '我们能否在AI生成的图像中消除社会刻板印象？探究文本到图像输出及用户感知。'}
{'arxiv_id': 'arXiv:2505.20691', 'title': 'Evidential Deep Active Learning for Semi-Supervised Classification', 'authors': 'Shenkai Zhao, Xinao Zhang, Lipeng Pan, Xiaobin Xu, Danilo Pelusi', 'link': 'https://arxiv.org/abs/2505.20691', 'abstract': 'Semi-supervised classification based on active learning has made significant progress, but the existing methods often ignore the uncertainty estimation (or reliability) of the prediction results during the learning process, which makes it questionable whether the selected samples can effectively update the model. Hence, this paper proposes an evidential deep active learning approach for semi-supervised classification (EDALSSC). EDALSSC builds a semi-supervised learning framework to simultaneously quantify the uncertainty estimation of labeled and unlabeled data during the learning process. The uncertainty estimation of the former is associated with evidential deep learning, while that of the latter is modeled by combining ignorance information and conflict information of the evidence from the perspective of the T-conorm operator. Furthermore, this article constructs a heuristic method to dynamically balance the influence of evidence and the number of classes on uncertainty estimation to ensure that it does not produce counter-intuitive results in EDALSSC. For the sample selection strategy, EDALSSC selects the sample with the greatest uncertainty estimation that is calculated in the form of a sum when the training loss increases in the latter half of the learning process. Experimental results demonstrate that EDALSSC outperforms existing semi-supervised and supervised active learning approaches on image classification datasets.', 'abstract_zh': '基于证据的半监督主动学习方法（EDALSSC）及其在半监督分类中的应用', 'title_zh': '证据深度主动学习在半监督分类中的应用'}
{'arxiv_id': 'arXiv:2505.20686', 'title': 'Accelerating RL for LLM Reasoning with Optimal Advantage Regression', 'authors': 'Kianté Brantley, Mingyu Chen, Zhaolin Gao, Jason D. Lee, Wen Sun, Wenhao Zhan, Xuezhou Zhang', 'link': 'https://arxiv.org/abs/2505.20686', 'abstract': 'Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning large language models (LLMs) to improve complex reasoning abilities. However, state-of-the-art policy optimization methods often suffer from high computational overhead and memory consumption, primarily due to the need for multiple generations per prompt and the reliance on critic networks or advantage estimates of the current policy. In this paper, we propose $A$*-PO, a novel two-stage policy optimization framework that directly approximates the optimal advantage function and enables efficient training of LLMs for reasoning tasks. In the first stage, we leverage offline sampling from a reference policy to estimate the optimal value function $V$*, eliminating the need for costly online value estimation. In the second stage, we perform on-policy updates using a simple least-squares regression loss with only a single generation per prompt. Theoretically, we establish performance guarantees and prove that the KL-regularized RL objective can be optimized without requiring complex exploration strategies. Empirically, $A$*-PO achieves competitive performance across a wide range of mathematical reasoning benchmarks, while reducing training time by up to 2$\\times$ and peak memory usage by over 30% compared to PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at this https URL.', 'abstract_zh': '基于A*-PO的新型两阶段策略优化框架：直接逼近最优优势函数以高效训练大型语言模型进行复杂推理任务', 'title_zh': '加速大型语言模型推理的强化学习方法：最优优势回归加速'}
{'arxiv_id': 'arXiv:2505.20674', 'title': 'Pretraining Language Models to Ponder in Continuous Space', 'authors': 'Boyi Zeng, Shixiang Song, Siyuan Huang, Yixuan Wang, He Li, Ziwei He, Xinbing Wang, Zhiyu Li, Zhouhan Lin', 'link': 'https://arxiv.org/abs/2505.20674', 'abstract': 'Humans ponder before articulating complex sentence elements, enabling deeper cognitive processing through focused effort. In this work, we introduce this pondering process into language models by repeatedly invoking the forward process within a single token generation step. During pondering, instead of generating an actual token sampled from the prediction distribution, the model ponders by yielding a weighted sum of all token embeddings according to the predicted token distribution. The generated embedding is then fed back as input for another forward pass. We show that the model can learn to ponder in this way through self-supervised learning, without any human annotations. Our method is straightforward and can be seamlessly integrated with various existing language models. Experiments across three widely used open-source architectures-GPT-2, Pythia, and LLaMA-and extensive downstream task evaluations demonstrate the effectiveness and generality of our method. For language modeling tasks, pondering language models achieve performance comparable to vanilla models with twice the number of parameters. On 9 downstream benchmarks, our pondering-enhanced Pythia models significantly outperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is comparable to TinyLlama-1.1B, which is trained on 10 times more data. The code is available at this https URL.', 'abstract_zh': '人类在表达复杂句子元素之前先进行思考，通过集中努力实现更深层次的认知处理。本文通过在单个 tokens 生成步骤中反复调用前向过程，将这种思考过程引入语言模型中。在思考过程中，模型不生成实际的 tokens，而是根据预测的 token 分布生成加权的 token 向量，随后将生成的向量作为输入进行下一次前向传递。实验表明，模型可以通过自我监督学习学会以这种方式思考，无需任何人工标注。该方法简单直接，可以无缝集成到各种现有的语言模型中。在三种广泛应用的开源架构（GPT-2、Pythia 和 LLaMA）以及广泛的下游任务评估中，我们的方法展示了其有效性和通用性。对于语言建模任务，思考增强的语言模型在参数量仅为普通模型一半的情况下，能达到与普通模型相当的性能。在9个下游基准测试中，思考增强的Pythia模型显著优于官方Pythia模型。值得注意的是，思考增强的Pythia-1B模型在数据量少一个数量级的情况下，性能与训练数据量大一个数量级的TinyLlama-1.1B相当。代码可在以下链接获取。', 'title_zh': '预训练语言模型在连续空间中思考'}
{'arxiv_id': 'arXiv:2505.20666', 'title': 'Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers', 'authors': 'Yukun Zhang, Xueqing Zhou', 'link': 'https://arxiv.org/abs/2505.20666', 'abstract': "We propose a novel framework, Continuous_Time Attention, which infuses partial differential equations (PDEs) into the Transformer's attention mechanism to address the challenges of extremely long input sequences. Instead of relying solely on a static attention matrix, we allow attention weights to evolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion dynamics. This mechanism systematically smooths local noise, enhances long_range dependencies, and stabilizes gradient flow. Theoretically, our analysis shows that PDE_based attention leads to better optimization landscapes and polynomial rather than exponential decay of distant interactions. Empirically, we benchmark our method on diverse experiments_demonstrating consistent gains over both standard and specialized long sequence Transformer variants. Our findings highlight the potential of PDE_based formulations to enrich attention mechanisms with continuous_time dynamics and global coherence.", 'abstract_zh': '连续时间注意力框架：通过偏微分方程融入Transformer的注意力机制以应对极长输入序列的挑战', 'title_zh': '连续时间注意力：由偏微分方程引导的长序列变压器机制'}
{'arxiv_id': 'arXiv:2505.20664', 'title': 'Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning', 'authors': 'Yang He, Xiao Ding, Bibo Cai, Yufei Zhang, Kai Xiong, Zhouhao Sun, Bing Qin, Ting Liu', 'link': 'https://arxiv.org/abs/2505.20664', 'abstract': "While reasoning-augmented large language models (RLLMs) significantly enhance complex task performance through extended reasoning chains, they inevitably introduce substantial unnecessary token consumption, particularly for simpler problems where Short Chain-of-Thought (Short CoT) suffices. This overthinking phenomenon leads to inefficient resource usage without proportional accuracy gains. To address this issue, we propose Self-Route, a dynamic reasoning framework that automatically selects between general and reasoning modes based on model capability estimation. Our approach introduces a lightweight pre-inference stage to extract capability-aware embeddings from hidden layer representations, enabling real-time evaluation of the model's ability to solve problems. We further construct Gradient-10K, a model difficulty estimation-based dataset with dense complexity sampling, to train the router for precise capability boundary detection. Extensive experiments demonstrate that Self-Route achieves comparable accuracy to reasoning models while reducing token consumption by 30-55\\% across diverse benchmarks. The proposed framework demonstrates consistent effectiveness across models with different parameter scales and reasoning paradigms, highlighting its general applicability and practical value.", 'abstract_zh': '自适应推理：一种基于能力估计的动态推理框架', 'title_zh': '自适应路线:通过能力估计实现自动模式切换以提高推理效率'}
{'arxiv_id': 'arXiv:2505.20663', 'title': 'TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research', 'authors': 'Xu Kang, Siqi Jiang, Kangwei Xu, Jiahao Li, Ruibo Wu', 'link': 'https://arxiv.org/abs/2505.20663', 'abstract': 'Terpenoids are a crucial class of natural products that have been studied for over 150 years, but their interdisciplinary nature (spanning chemistry, pharmacology, and biology) complicates knowledge integration. To address this, the authors developed TeroSeek, a curated knowledge base (KB) built from two decades of terpenoid literature, coupled with an AI-powered question-answering chatbot and web service. Leveraging a retrieval-augmented generation (RAG) framework, TeroSeek provides structured, high-quality information and outperforms general-purpose large language models (LLMs) in terpenoid-related queries. It serves as a domain-specific expert tool for multidisciplinary research and is publicly available at this http URL.', 'abstract_zh': '萜类是一类研究超过150年的天然产物，但由于其跨学科性质（涵盖化学、药理学和生物学），知识整合变得复杂。为了应对这一挑战，作者开发了TeroSeek，一个基于二十年萜类文献知识库，并结合了AI驱动的问答聊天机器人和网络服务。利用检索增强生成（RAG）框架，TeroSeek提供结构化、高质量的信息，并在萜类相关查询中优于通用大语言模型（LLMs）。它作为多学科研究的专业工具公开可用，可通过以下网址访问：this http URL。', 'title_zh': 'TeroSeek: 一种基于人工智能的知识库与检索生成平台用于萜类研究'}
{'arxiv_id': 'arXiv:2505.20660', 'title': 'BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism', 'authors': 'Qinzhuo Wu, Pengzhi Gao, Wei Liu, Jian Luan', 'link': 'https://arxiv.org/abs/2505.20660', 'abstract': "Graphical User Interface (GUI) agents have gained substantial attention due to their impressive capabilities to complete tasks through multiple interactions within GUI environments. However, existing agents primarily focus on enhancing the accuracy of individual actions and often lack effective mechanisms for detecting and recovering from errors. To address these shortcomings, we propose the BacktrackAgent, a robust framework that incorporates a backtracking mechanism to improve task completion efficiency. BacktrackAgent includes verifier, judger, and reflector components as modules for error detection and recovery, while also applying judgment rewards to further enhance the agent's performance. Additionally, we develop a training dataset specifically designed for the backtracking mechanism, which considers the outcome pages after action executions. Experimental results show that BacktrackAgent has achieved performance improvements in both task success rate and step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be released upon acceptance.", 'abstract_zh': '基于回溯机制的图形用户界面代理：一种提高任务完成效率的稳健框架', 'title_zh': 'BacktrackAgent: 增强GUI代理的错误检测与回溯机制'}
{'arxiv_id': 'arXiv:2505.20654', 'title': 'Chinese Cyberbullying Detection: Dataset, Method, and Validation', 'authors': 'Yi Zhu, Xin Zou, Xindong Wu', 'link': 'https://arxiv.org/abs/2505.20654', 'abstract': 'Existing cyberbullying detection benchmarks were organized by the polarity of speech, such as "offensive" and "non-offensive", which were essentially hate speech detection. However, in the real world, cyberbullying often attracted widespread social attention through incidents. To address this problem, we propose a novel annotation method to construct a cyberbullying dataset that organized by incidents. The constructed CHNCI is the first Chinese cyberbullying incident detection dataset, which consists of 220,676 comments in 91 incidents. Specifically, we first combine three cyberbullying detection methods based on explanations generation as an ensemble method to generate the pseudo labels, and then let human annotators judge these labels. Then we propose the evaluation criteria for validating whether it constitutes a cyberbullying incident. Experimental results demonstrate that the constructed dataset can be a benchmark for the tasks of cyberbullying detection and incident prediction. To the best of our knowledge, this is the first study for the Chinese cyberbullying incident detection task.', 'abstract_zh': '现有的网络霸凌检测基准按照言论的极性组织，如“侮辱性”和“非侮辱性”，本质上是对仇恨言论的检测。然而，在现实世界中，网络霸凌往往通过事件吸引广泛关注。为了解决这一问题，我们提出了一种新的标注方法，构建了一个基于事件的网络霸凌数据集。CHNCI是中国首个网络霸凌事件检测数据集，包含91个事件中的220,676条评论。具体而言，我们首先结合三种基于解释生成的网络霸凌检测方法作为集成方法生成伪标签，然后让人工标注者判断这些标签。接着我们提出了验证是否构成网络霸凌事件的评价标准。实验结果表明，构建的数据集可以作为网络霸凌检测和事件预测任务的基准。据我们所知，这是首个针对中文网络霸凌事件检测任务的研究。', 'title_zh': '中文网络欺凌检测：数据集、方法与验证'}
{'arxiv_id': 'arXiv:2505.20653', 'title': 'RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment', 'authors': 'Lingyu Qiu, Ke Jiang, Xiaoyang Tan', 'link': 'https://arxiv.org/abs/2505.20653', 'abstract': 'Recent advancements in domain generalization for deepfake detection have attracted significant attention, with previous methods often incorporating additional modules to prevent overfitting to domain-specific patterns. However, such regularization can hinder the optimization of the empirical risk minimization (ERM) objective, ultimately degrading model performance. In this paper, we propose a novel learning objective that aligns generalization gradient updates with ERM gradient updates. The key innovation is the application of perturbations to model parameters, aligning the ascending points across domains, which specifically enhances the robustness of deepfake detection models to domain shifts. This approach effectively preserves domain-invariant features while managing domain-specific characteristics, without introducing additional regularization. Experimental results on multiple challenging deepfake detection datasets demonstrate that our gradient alignment strategy outperforms state-of-the-art domain generalization techniques, confirming the efficacy of our method. The code is available at this https URL.', 'abstract_zh': 'Recent advancements in领域迁移检测中的深度伪造检测取得了显著进展，先前的方法常常通过引入额外模块来防止过拟合到领域特定的模式。然而，这种正则化会妨碍经验风险最小化（ERM）目标的优化，最终降低模型性能。本文提出一种新的学习目标，将通用梯度更新与ERM梯度更新对齐。关键创新在于对模型参数施加扰动，使不同领域间的升梯度点保持一致，这特别增强了深度伪造检测模型对领域偏移的鲁棒性。该方法在保留领域不变特征的同时管理领域特定特征，无需引入额外的正则化。在多个具有挑战性的深度伪造检测数据集上的实验结果表明，我们的梯度对齐策略优于现有的领域迁移技术，证实了该方法的有效性。代码可访问：<该链接>。', 'title_zh': 'RoGA: 通过稳健梯度对齐朝着通用深伪检测方向'}
{'arxiv_id': 'arXiv:2505.20650', 'title': 'FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information', 'authors': 'Yan Wang, Yang Ren, Lingfei Qian, Xueqing Peng, Keyi Wang, Yi Han, Dongji Feng, Xiao-Yang Liu, Jimin Huang, Qianqian Xie', 'link': 'https://arxiv.org/abs/2505.20650', 'abstract': 'We introduce FinTagging, the first full-scope, table-aware XBRL benchmark designed to evaluate the structured information extraction and semantic alignment capabilities of large language models (LLMs) in the context of XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL tagging as flat multi-class classification and focus solely on narrative text, FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for financial entity extraction and FinCL for taxonomy-driven concept alignment. It requires models to jointly extract facts and align them with the full 10k+ US-GAAP taxonomy across both unstructured text and structured tables, enabling realistic, fine-grained evaluation. We assess a diverse set of LLMs under zero-shot settings, systematically analyzing their performance on both subtasks and overall tagging accuracy. Our results reveal that, while LLMs demonstrate strong generalization in information extraction, they struggle with fine-grained concept alignment, particularly in disambiguating closely related taxonomy entries. These findings highlight the limitations of existing LLMs in fully automating XBRL tagging and underscore the need for improved semantic reasoning and schema-aware modeling to meet the demands of accurate financial disclosure. Code is available at our GitHub repository and data is at our Hugging Face repository.', 'abstract_zh': 'FinTagging：面向XBRL的首个全范围表格感知基准，用于评估大型语言模型在基于XBRL的财务报告中的结构化信息提取和语义对齐能力', 'title_zh': 'FinTagging：一个准备就绪的LLM基准，用于提取和结构化金融信息'}
{'arxiv_id': 'arXiv:2505.20648', 'title': 'Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning', 'authors': 'Mengmeng Chen, Xiaohu Wu, Qiqi Liu, Tiantian He, Yew-Soon Ong, Yaochu Jin, Qicheng Lao, Han Yu', 'link': 'https://arxiv.org/abs/2505.20648', 'abstract': 'Multi-objective optimization (MOO) exists extensively in machine learning, and aims to find a set of Pareto-optimal solutions, called the Pareto front, e.g., it is fundamental for multiple avenues of research in federated learning (FL). Pareto-Front Learning (PFL) is a powerful method implemented using Hypernetworks (PHNs) to approximate the Pareto front. This method enables the acquisition of a mapping function from a given preference vector to the solutions on the Pareto front. However, most existing PFL approaches still face two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to cover the entire Pareto Front which has a convex shape. Here, we introduce a novel PFL framework, called as PHN-HVVS, which decomposes the design space into Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid partitioning within high-dimensional space. We put forward a new loss function, which effectively contributes to more extensive coverage of the resultant Pareto front and maximizes the HV Indicator. Experimental results on multiple MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines significantly in generating Pareto front. Also, we illustrate that PHN-HVVS advances the methodologies of several recent problems in the FL field. The code is available at this https URL}{this https URL.', 'abstract_zh': '多目标优化（MOO）在机器学习中广泛存在，旨在找到一组帕雷to最优解，即帕雷托前沿，例如，在联邦学习（FL）的多个研究领域中具有基础性意义。帕雷托前沿学习（PFL）是一种使用超网络（PHNs）实现的方法，用于近似帕雷托前沿。该方法使得能够从给定的偏好向量获得映射函数到帕雷托前沿上的解。然而，现有的大多数PFL方法仍面临两个挑战：（a）在高维空间中采样射线；（b）无法完全覆盖具有凸形状的帕雷托前沿。在这里，我们引入了一种新的PFL框架，称为PHN-HVVS，该框架将设计空间分解为Voronoi网格，并在高维空间中部署遗传算法（GA）进行Voronoi网格划分。我们提出了一个新的损失函数，有效地促进了帕雷托前沿覆盖范围的扩展并最大化HV指标。在多个多目标优化（MOO）机器学习任务上的实验结果表明，PHN-HVVS在生成帕雷托前沿方面显著优于基线方法。此外，我们展示了PHN-HVVS在联邦学习（FL）领域的多个最新问题中的方法学进步。代码可在以下链接获取：this https URL。', 'title_zh': '基于Voronoi网格的帕累托前沿学习及其在协作联邦学习中的应用'}
{'arxiv_id': 'arXiv:2505.20646', 'title': 'Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory', 'authors': 'Eduardo Y. Sakabe, Felipe S. Abrahão, Alexandre Simões, Esther Colombini, Paula Costa, Ricardo Gudwin, Hector Zenil', 'link': 'https://arxiv.org/abs/2505.20646', 'abstract': 'Understanding and controlling the informational complexity of neural networks is a central challenge in machine learning, with implications for generalization, optimization, and model capacity. While most approaches rely on entropy-based loss functions and statistical metrics, these measures often fail to capture deeper, causally relevant algorithmic regularities embedded in network structure. We propose a shift toward algorithmic information theory, using Binarized Neural Networks (BNNs) as a first proxy. Grounded in algorithmic probability (AP) and the universal distribution it defines, our approach characterizes learning dynamics through a formal, causally grounded lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation of algorithmic complexity based on AP -- and demonstrate that it more closely tracks structural changes during training than entropy, consistently exhibiting stronger correlations with training loss across varying model sizes and randomized training runs. These results support the view of training as a process of algorithmic compression, where learning corresponds to the progressive internalization of structured regularities. In doing so, our work offers a principled estimate of learning progression and suggests a framework for complexity-aware learning and regularization, grounded in first principles from information theory, complexity, and computability.', 'abstract_zh': '理解并控制神经网络的信息复杂性是机器学习中的一个核心挑战，这对泛化、优化和模型容量都有影响。我们提出将重点转向算法信息理论，使用二值神经网络（BNNs）作为初步代理。基于算法概率（AP）及其定义的通用分布，我们的方法通过形式化的因果视角来刻画学习动力学。我们应用块分解方法（BDM）——基于AP的一种可扩展的算法复杂性近似——证明了它在跟踪训练过程中结构变化方面比熵更接近，且在不同模型大小和随机训练运行中表现出更强的与训练损失的相关性。这些结果支持将训练视为一种算法压缩过程的观点，其中学习对应于结构规律的逐步内部化。我们的工作提供了一个基于信息论、复杂性和计算原理的原理性的学习进展估计，并建议了一种复杂性意识学习和正则化的框架。', 'title_zh': '通过算法信息论的视角评估二值神经网络的训练'}
{'arxiv_id': 'arXiv:2505.20644', 'title': 'HCQA-1.5 @ Ego4D EgoSchema Challenge 2025', 'authors': 'Haoyu Zhang, Yisen Feng, Qiaohui Chu, Meng Liu, Weili Guan, Yaowei Wang, Liqiang Nie', 'link': 'https://arxiv.org/abs/2505.20644', 'abstract': "In this report, we present the method that achieves third place for Ego4D EgoSchema Challenge in CVPR 2025. To improve the reliability of answer prediction in egocentric video question answering, we propose an effective extension to the previously proposed HCQA framework. Our approach introduces a multi-source aggregation strategy to generate diverse predictions, followed by a confidence-based filtering mechanism that selects high-confidence answers directly. For low-confidence cases, we incorporate a fine-grained reasoning module that performs additional visual and contextual analysis to refine the predictions. Evaluated on the EgoSchema blind test set, our method achieves 77% accuracy on over 5,000 human-curated multiple-choice questions, outperforming last year's winning solution and the majority of participating teams. Our code will be added at this https URL.", 'abstract_zh': '在本报告中，我们展示了在CVPR 2025 Ego4D EgoSchema挑战中获得第三名的方法。为了提高自视点视频问答中答案预测的可靠性，我们提出了对先前提出的HCQA框架的有效扩展。我们的方法引入了多源聚合策略以生成多样化预测，并通过基于置信度的过滤机制直接选择高置信度的答案。对于低置信度情况，我们引入了细粒度推理模块，进行额外的视觉和情境分析以细化预测。在EgoSchema盲测试集上，我们的方法在超过5,000个人工策划的多选题中实现了77%的准确率，超过了去年的获胜解决方案和大多数参赛队伍。我们的代码将在此处 https:// 提供。', 'title_zh': 'HCQA-1.5 @ Ego4D EgoSchema挑战赛2025'}
{'arxiv_id': 'arXiv:2505.20643', 'title': 'Can Past Experience Accelerate LLM Reasoning?', 'authors': 'Bo Pan, Liang Zhao', 'link': 'https://arxiv.org/abs/2505.20643', 'abstract': 'Allocating more compute to large language models (LLMs) reasoning has generally been demonstrated to improve their effectiveness, but also results in increased inference time. In contrast, humans can perform tasks faster and better with increased experience and exposure. Hence, this paper aims to investigate the question: Can LLMs also become faster at reasoning through recurrent exposure on relevant tasks, and if so, how can it be achieved? To address these questions, we first formalize the problem setting of LLM reasoning speedup systematically in the dimensions of task relevancy and compute budget calculation. We then propose SpeedupLLM, a theoretically guaranteed framework to implement and benchmark such reasoning speedup behaviour based on adaptive compute allocation and memory mechanisms. We further conduct comprehensive experiments to benchmark such behaviour across different question similarity levels, memory methods, and reasoning methods. Results show that LLMs can generally reason faster with past experience, achieving up to a 56% reduction in compute cost when equipped with appropriate memory and reasoning methods.', 'abstract_zh': '基于递归暴露，大型语言模型通过增加计算资源能否加快推理速度及其实现方法探究', 'title_zh': '过去的经历能否加速LLM推理？'}
{'arxiv_id': 'arXiv:2505.20637', 'title': 'TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone', 'authors': 'Ana M. Cabanas, Alma Pedro, Domingo Mery', 'link': 'https://arxiv.org/abs/2505.20637', 'abstract': 'Understanding how facial affect analysis (FAA) systems perform across different demographic groups requires reliable measurement of sensitive attributes such as ancestry, often approximated by skin tone, which itself is highly influenced by lighting conditions. This study compares two objective skin tone classification methods: the widely used Individual Typology Angle (ITA) and a perceptually grounded alternative based on Lightness ($L^*$) and Hue ($H^*$). Using AffectNet and a MobileNet-based model, we assess fairness across skin tone groups defined by each method. Results reveal a severe underrepresentation of dark skin tones ($\\sim 2 \\%$), alongside fairness disparities in F1-score (up to 0.08) and TPR (up to 0.11) across groups. While ITA shows limitations due to its sensitivity to lighting, the $H^*$-$L^*$ method yields more consistent subgrouping and enables clearer diagnostics through metrics such as Equal Opportunity. Grad-CAM analysis further highlights differences in model attention patterns by skin tone, suggesting variation in feature encoding. To support future mitigation efforts, we also propose a modular fairness-aware pipeline that integrates perceptual skin tone estimation, model interpretability, and fairness evaluation. These findings emphasize the relevance of skin tone measurement choices in fairness assessment and suggest that ITA-based evaluations may overlook disparities affecting darker-skinned individuals.', 'abstract_zh': '面部情感分析系统在不同人群中的表现理解需要可靠的敏感属性测量，如族裔，通常通过肤色近似，而肤色本身高度受光照条件影响。本研究比较了两种客观肤色分类方法：广泛使用的个体分类角度（ITA）和基于亮度（$L^*$）和色调（$H^*$）的感知基础替代方法。使用AffectNet和基于MobileNet的模型，我们评估了每种方法定义的肤色组别之间的公平性。结果表明肤色较深的群体严重不足（约占2%），并在F1分数（高达0.08）和真正阳性率（高达0.11）方面表现出公平性差异。虽然ITA因其对光照的敏感性而显示出局限性，但$H^*$-$L^*$方法能够实现更一致的子群体划分，并通过均衡机会等指标提供更清晰的诊断。进一步的Grad-CAM分析强调了不同肤色群体间模型注意力模式的差异，暗示了特征编码的差异。为了支持未来的缓解努力，我们还提出了一种模块化的公平性意识流水线，该流水线结合了感知肤色估计、模型可解释性和公平性评估。这些发现强调了在公平性评估中肤色测量选择的重要性，并暗示基于ITA的评估可能忽略了影响肤色较深个体的不平等现象。', 'title_zh': 'TrustSkin: 跨肤色值得信赖的面部情绪分析公平性流程'}
{'arxiv_id': 'arXiv:2505.20635', 'title': 'Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction', 'authors': 'Zexu Pan, Shengkui Zhao, Tingting Wang, Kun Zhou, Yukun Ma, Chong Zhang, Bin Ma', 'link': 'https://arxiv.org/abs/2505.20635', 'abstract': "Audio-visual speaker extraction isolates a target speaker's speech from a mixture speech signal conditioned on a visual cue, typically using the target speaker's face recording. However, in real-world scenarios, other co-occurring faces are often present on-screen, providing valuable speaker activity cues in the scene. In this work, we introduce a plug-and-play inter-speaker attention module to process these flexible numbers of co-occurring faces, allowing for more accurate speaker extraction in complex multi-person environments. We integrate our module into two prominent models: the AV-DPRNN and the state-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets, including the highly overlapped VoxCeleb2 and sparsely overlapped MISP, demonstrate that our approach consistently outperforms baselines. Furthermore, cross-dataset evaluations on LRS2 and LRS3 confirm the robustness and generalizability of our method.", 'abstract_zh': '基于视觉的说话人提取模块处理多个人环境中的灵活数量的共存面孔，以实现更准确的说话人隔离', 'title_zh': '即插即用共现面孔注意力以实现鲁棒的音视频说话人提取'}
{'arxiv_id': 'arXiv:2505.20633', 'title': 'Test-Time Learning for Large Language Models', 'authors': 'Jinwu Hu, Zhitian Zhang, Guohao Chen, Xutao Wen, Chao Shuai, Wei Luo, Bin Xiao, Yuanqing Li, Mingkui Tan', 'link': 'https://arxiv.org/abs/2505.20633', 'abstract': 'While Large Language Models (LLMs) have exhibited remarkable emergent capabilities through extensive pre-training, they still face critical limitations in generalizing to specialized domains and handling diverse linguistic variations, known as distribution shifts. In this paper, we propose a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically adapts LLMs to target domains using only unlabeled test data during testing. Specifically, we first provide empirical evidence and theoretical insights to reveal that more accurate predictions from LLMs can be achieved by minimizing the input perplexity of the unlabeled test data. Based on this insight, we formulate the Test-Time Learning process of LLMs as input perplexity minimization, enabling self-supervised enhancement of LLM performance. Furthermore, we observe that high-perplexity samples tend to be more informative for model optimization. Accordingly, we introduce a Sample Efficient Learning Strategy that actively selects and emphasizes these high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA) instead of full-parameter optimization, which allows lightweight model updates while preserving more original knowledge from the model. We introduce the AdaptEval benchmark for TTL and demonstrate through experiments that TLM improves performance by at least 20% compared to original LLMs on domain knowledge adaptation.', 'abstract_zh': '基于测试时学习的大型语言模型动态适应范式', 'title_zh': '运行时学习for大规模语言模型'}
{'arxiv_id': 'arXiv:2505.20622', 'title': 'SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation', 'authors': 'Ting Xu, Zhichao Huang, Jiankai Sun, Shanbo Cheng, Wai Lam', 'link': 'https://arxiv.org/abs/2505.20622', 'abstract': 'We present Sequential Policy Optimization for Simultaneous Machine Translation (SeqPO-SiMT), a new policy optimization framework that defines the simultaneous machine translation (SiMT) task as a sequential decision making problem, incorporating a tailored reward to enhance translation quality while reducing latency. In contrast to popular Reinforcement Learning from Human Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task. This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT process using a tailored reward. We conduct experiments on six datasets from diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that SeqPO-SiMT consistently achieves significantly higher translation quality with lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning (SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17 in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly rival the offline translation of high-performing LLMs, including Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.', 'abstract_zh': 'Sequential Policy Optimization for Simultaneous Machine Translation (SeqPO-SiMT)', 'title_zh': 'SeqPO-SiMT: 序列策略优化的同时机器翻译'}
{'arxiv_id': 'arXiv:2505.20621', 'title': 'Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning', 'authors': 'Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah Erfani, Benjamin I. P. Rubinstein', 'link': 'https://arxiv.org/abs/2505.20621', 'abstract': 'Similar to other machine learning frameworks, Offline Reinforcement Learning (RL) is shown to be vulnerable to poisoning attacks, due to its reliance on externally sourced datasets, a vulnerability that is exacerbated by its sequential nature. To mitigate the risks posed by RL poisoning, we extend certified defenses to provide larger guarantees against adversarial manipulation, ensuring robustness for both per-state actions, and the overall expected cumulative reward. Our approach leverages properties of Differential Privacy, in a manner that allows this work to span both continuous and discrete spaces, as well as stochastic and deterministic environments -- significantly expanding the scope and applicability of achievable guarantees. Empirical evaluations demonstrate that our approach ensures the performance drops to no more than $50\\%$ with up to $7\\%$ of the training data poisoned, significantly improving over the $0.008\\%$ in prior work~\\citep{wu_copa_2022}, while producing certified radii that is $5$ times larger as well. This highlights the potential of our framework to enhance safety and reliability in offline RL.', 'abstract_zh': '类似于其他机器学习框架，离线强化学习（RL）由于依赖外部数据集，且具有序列性，被证明容易受到中毒攻击。为了减轻RL中毒带来的风险，我们扩展了认证防御，以提供更大的保证对抗恶意操控，确保对每个状态的动作和总体预期累积奖励的鲁棒性。我们的方法利用差分隐私的性质，这使得该工作能够适用于连续和离散空间，以及随机性和确定性环境，极大地扩展了可实现保证的范围和适用性。实验评估表明，即使有高达7%的训练数据被中毒，我们的方法也能确保性能下降不超过50%，显著优于先前工作中的0.008%，同时生成的认证半径也增大了5倍。这突显了我们框架在离线RL安全性与可靠性方面的潜在优势。', 'title_zh': '离线强化学习中多层认证防护对抗中毒攻击'}
{'arxiv_id': 'arXiv:2505.20613', 'title': 'REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning', 'authors': 'Ziju Shen, Naohao Huang, Fanyi Yang, Yutong Wang, Guoxiong Gao, Tianyi Xu, Jiedong Jiang, Wanyi He, Pu Yang, Mengzhou Sun, Haocheng Ju, Peihao Wu, Bryan Dai, Bin Dong', 'link': 'https://arxiv.org/abs/2505.20613', 'abstract': 'Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64).', 'abstract_zh': '如今，形式定理证明器在高中和竞赛级数学领域取得了巨大进展，但很少扩展到更高级的数学领域。本文介绍了REAL-Prover，这是一个新的基于Lean 4的逐步定理证明器，旨在突破这一界限。该证明器基于我们微调的大语言模型（REAL-Prover-v1）并集成了检索系统（Leansearch-PS），显著提升了解决本科级数学问题的性能。为了训练REAL-Prover-v1，我们开发了HERALD-AF数据提取管道，将自然语言数学问题转换为形式化声明，并开发了一个新的开源Lean 4交互式环境（Jixia-interactive）以促进合成数据收集。在我们的实验中，仅使用监督微调的证明器在ProofNet数据集上的成功率为23.7%（Pass@64），与最先进的（SOTA）模型相当。为进一步评估我们的方法，我们引入了FATE-M，一个专注于代数问题的新基准，我们的证明器在该基准上的成功率为56.7%（Pass@64）。', 'title_zh': 'REAL-Prover: 获取增强的精简证明器 for 数学推理'}
{'arxiv_id': 'arXiv:2505.20600', 'title': 'InstGenIE: Generative Image Editing Made Efficient with Mask-aware Caching and Scheduling', 'authors': 'Xiaoxiao Jiang, Suyi Li, Lingyun Yang, Tianyu Feng, Zhipeng Di, Weiyi Lu, Guoxuan Zhu, Xiu Lin, Kan Liu, Yinghao Yu, Tao Lan, Guodong Yang, Lin Qu, Liping Zhang, Wei Wang', 'link': 'https://arxiv.org/abs/2505.20600', 'abstract': "Generative image editing using diffusion models has become a prevalent application in today's AI cloud services. In production environments, image editing typically involves a mask that specifies the regions of an image template to be edited. The use of masks provides direct control over the editing process and introduces sparsity in the model inference. In this paper, we present InstGenIE, a system that efficiently serves image editing requests. The key insight behind InstGenIE is that image editing only modifies the masked regions of image templates while preserving the original content in the unmasked areas. Driven by this insight, InstGenIE judiciously skips redundant computations associated with the unmasked areas by reusing cached intermediate activations from previous inferences. To mitigate the high cache loading overhead, InstGenIE employs a bubble-free pipeline scheme that overlaps computation with cache loading. Additionally, to reduce queuing latency in online serving while improving the GPU utilization, InstGenIE proposes a novel continuous batching strategy for diffusion model serving, allowing newly arrived requests to join the running batch in just one step of denoising computation, without waiting for the entire batch to complete. As heterogeneous masks induce imbalanced loads, InstGenIE also develops a load balancing strategy that takes into account the loads of both computation and cache loading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving systems for image editing, achieving up to 3x higher throughput and reducing average request latency by up to 14.7x while ensuring image quality.", 'abstract_zh': '基于扩散模型的生成图像编辑系统InstGenIE', 'title_zh': 'InstGenIE: 通过掩码感知缓存与调度实现高效的生成图像编辑'}
{'arxiv_id': 'arXiv:2505.20579', 'title': 'The challenge of hidden gifts in multi-agent reinforcement learning', 'authors': 'Dane Malenfant, Blake A. Richards', 'link': 'https://arxiv.org/abs/2505.20579', 'abstract': 'Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These "hidden gifts" represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus the act of dropping the key for others is a "hidden gift". We show that several different state-of-the-art RL algorithms, including MARL algorithms, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that independent model-free policy gradient agents can solve the task when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for these independent agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of "hidden gifts", and demonstrate that learning awareness in independent agents can benefit these settings.', 'abstract_zh': '有时我们因他人不知情的行为而受益：多智能体强化学习中隐藏礼物的挑战及解决方案', 'title_zh': '多智能体强化学习中的隐藏礼物挑战'}
{'arxiv_id': 'arXiv:2505.20578', 'title': 'Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL', 'authors': 'Xingyu Chen, Shihao Ma, Runsheng Lin, Jiecong Lin, Bo Wang', 'link': 'https://arxiv.org/abs/2505.20578', 'abstract': 'Designing regulatory DNA sequences that achieve precise cell-type-specific gene expression is crucial for advancements in synthetic biology, gene therapy and precision medicine. Although transformer-based language models (LMs) can effectively capture patterns in regulatory DNA, their generative approaches often struggle to produce novel sequences with reliable cell-specific activity. Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL) framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating regulatory sequence design as a biologically informed constrained optimization problem, we apply RL to autoregressive genomic LMs, enabling the models to iteratively refine sequences that maximize regulatory activity in targeted cell types while constraining off-target effects. Our evaluation on human promoters and enhancers demonstrates that Ctrl-DNA consistently outperforms existing generative and RL-based approaches, generating high-fitness regulatory sequences and achieving state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences capture key cell-type-specific transcription factor binding sites (TFBS), short DNA motifs recognized by regulatory proteins that control gene expression, demonstrating the biological plausibility of the generated sequences.', 'abstract_zh': '设计实现精确细胞类型特异性基因表达的调控DNA序列对于合成生物学、基因疗法和精准医学的发展至关重要。尽管基于变换器的语言模型（LMs）能够有效捕获调控DNA中的模式，但其生成方法往往难以产生具有可靠细胞类型特异活性的新型序列。我们引入了Ctrl-DNA，这是一种针对设计具有可控细胞类型特异性的调控DNA序列的新型约束强化学习（RL）框架。通过将调控序列设计形式化为生物学导向的约束优化问题，我们将RL应用于自回归基因LMs，使模型能够迭代地优化序列，以最大化目标细胞类型中的调控活性，同时限制非目标效应。在人类启动子和增强子上的评估表明，Ctrl-DNA始终优于现有生成性和基于RL的方法，生成高适应度的调控序列并实现最先进的细胞类型特异性。此外，Ctrl-DNA生成的序列捕获了关键的细胞类型特异性转录因子结合位点（TFBS），这些位点是被调控蛋白识别的短DNA动机，控制基因表达，显示出生成序列的生物可行性。', 'title_zh': 'Ctrl-DNA：基于约束RL的可控制细胞类型特异性调节DNA设计'}
{'arxiv_id': 'arXiv:2505.20573', 'title': 'Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners', 'authors': 'Jiabao Ji, Yongchao Chen, Yang Zhang, Ramana Rao Kompella, Chuchu Fan, Gaowen Liu, Shiyu Chang', 'link': 'https://arxiv.org/abs/2505.20573', 'abstract': 'Large language models (LLMs) have demonstrated strong performance in various robot control tasks. However, their deployment in real-world applications remains constrained. Even state-ofthe-art LLMs, such as GPT-o4mini, frequently produce invalid action plans that violate physical constraints, such as directing a robot to an unreachable location or causing collisions between robots. This issue primarily arises from a lack of awareness of these physical constraints during the reasoning process. To address this issue, we propose a novel framework that integrates reinforcement learning with verifiable rewards (RLVR) to incentivize knowledge of physical constraints into LLMs to induce constraints-aware reasoning during plan generation. In this approach, only valid action plans that successfully complete a control task receive positive rewards. We applied our method to two small-scale LLMs: a non-reasoning Qwen2.5-3B-Instruct and a reasoning Qwen3-4B. The experiment results demonstrate that constraint-aware small LLMs largely outperform large-scale models without constraints, grounded on both the BoxNet task and a newly developed BoxNet3D environment built using MuJoCo. This work highlights the effectiveness of grounding even small LLMs with physical constraints to enable scalable and efficient multi-robot control in complex, physically constrained environments.', 'abstract_zh': '大型语言模型在各种机器人控制任务中表现出强大的性能，但在实际应用中的部署仍受限制。即使是最先进的大型语言模型（如GPT-o4mini），也经常生成违反物理约束的有效行动计划，例如将机器人引导至无法到达的位置或导致机器人之间的碰撞。该问题主要源于推理过程中缺乏对这些物理约束的意识。为此，我们提出了一种将强化学习与可验证奖励（RLVR）相结合的新框架，以激励大型语言模型（LLMs）了解物理约束，在计划生成过程中诱导基于物理约束的推理。在此方法中，只有成功完成控制任务的有效行动计划才能获得正向奖励。我们将该方法应用于两个小型规模的大型语言模型：一个不进行推理的Qwen2.5-3B-Instruct和一个进行推理的Qwen3-4B。实验结果表明，具有物理约束意识的小型大型语言模型在物理约束环境下显著优于不具有物理约束意识的大规模模型，这一效果在BoxNet任务和使用MuJoCo构建的BoxNet3D环境中均得到验证。本研究强调了即使在小型大型语言模型中嵌入物理约束也能实现大规模高效多机器人控制的有效性。', 'title_zh': '碰撞规避与可达性aware多机器人控制结合 Grounded LLM 规划器'}
{'arxiv_id': 'arXiv:2505.20569', 'title': 'Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models', 'authors': 'Jihoon Lee, Min Song', 'link': 'https://arxiv.org/abs/2505.20569', 'abstract': 'Despite significant advancements in Large Vision-Language Models, Object Hallucination (OH) remains a persistent challenge. Building upon prior studies on contrastive decoding that address this issue without requiring additional model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an advanced method to suppress OH. RVCD leverages both negative and positive images at the logit level, explicitly referencing AI-generated images designed to represent a single concept. Our approach demonstrates substantial improvements over existing decoding-based methods.', 'abstract_zh': '尽管大型多模态模型取得了显著进展，物体幻视（OH）仍然是一个持续的挑战。基于先前对比解码的研究，我们在不需额外模型训练的情况下引入了RVCD（检索式视觉对比解码）方法，以抑制OH。RVCD 在logit级同时利用正负图像，明确引用旨在代表单一概念的AI生成图像。我们的方法在现有解码方法上展示了显著的改进。', 'title_zh': '基于检索的视觉对比解码以减轻大尺度视觉-语言模型中的对象幻觉'}
{'arxiv_id': 'arXiv:2505.20561', 'title': 'Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning', 'authors': 'Shenao Zhang, Yaqing Wang, Yinxiao Liu, Tianqi Liu, Peter Grabowski, Eugene Ie, Zhaoran Wang, Yunxuan Li', 'link': 'https://arxiv.org/abs/2505.20561', 'abstract': 'Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at this https URL.', 'abstract_zh': '通过强化学习训练的大语言模型（LLMs）展示了强大的推理能力和 Emergent 反省行为，如回溯和错误修正。然而，传统的马尔可夫ian 强化学习仅在训练阶段 confinement 探索范围以学习最优确定性策略，并且仅通过当前状态依赖于历史上下文。因此，在马尔可夫ian 强化学习训练过程中是否会 Emergent 反省推理仍然不清楚，或者为什么它们在测试时有益。为了弥补这一不足，我们将在贝叶斯自适应强化学习框架下重新定义反事实探索，该框架明确地在马尔可夫决策过程的后验分布上优化期望回报。这种贝叶斯形式固有激励了通过信念更新来最大化奖励利用和信息收集探索。我们提出的方法 BARL 指导大语言模型基于观察到的结果缝合和切换策略，提供了模型在何时和如何进行反事实探索的原理性指导。在合成和数学推理任务上的实验结果表明，BARL 在测试时优于标准的马尔可夫ian 强化学习方法，实现了更好的标记效率和增强的探索有效性。我们的代码在此 https URL。', 'title_zh': '超越马尔可夫链：基于贝叶斯自适应RL的反射性探索用于大语言模型推理'}
{'arxiv_id': 'arXiv:2505.20507', 'title': 'Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset', 'authors': 'Elias Arbash, Ahmed Jamal Afifi, Ymane Belahsen, Margret Fuchs, Pedram Ghamisi, Paul Scheunders, Richard Gloaguen', 'link': 'https://arxiv.org/abs/2505.20507', 'abstract': 'The global challenge of sustainable recycling demands automated, fast, and accurate, state-of-the-art (SOTA) material detection systems that act as a bedrock for a circular economy. Democratizing access to these cutting-edge solutions that enable real-time waste analysis is essential for scaling up recycling efforts and fostering the Green Deal. In response, we introduce \\textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to accelerate the recovery of critical raw materials through accurate electrolyzer materials classification. The dataset comprises 55 co-registered high-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning the 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and 424,169 labeled ones. This enables non-invasive spectral analysis of shredded electrolyzer samples, supporting quantitative and qualitative material classification and spectral properties investigation. We evaluate a suite of baseline machine learning (ML) methods alongside SOTA transformer-based deep learning (DL) architectures, including Vision Transformer, SpectralFormer, and the Multimodal Fusion Transformer, to investigate architectural bottlenecks for further efficiency optimisation when deploying transformers in material identification. We implement zero-shot detection techniques and majority voting across pixel-level predictions to establish object-level classification robustness. In adherence to the FAIR data principles, the electrolyzers-HSI dataset and accompanying codebase are openly available at this https URL and this https URL, supporting reproducible research and facilitating the broader adoption of smart and sustainable e-waste recycling solutions.', 'abstract_zh': '全球可持续回收面临的挑战需要自动化、快速且准确的最新材料检测系统，这些系统是循环经济的基石。为扩大回收努力并促进绿色协议的实施，必须普及这些先进解决方案的使用权。为此，我们引入了**Electrolyzers-HSI**，这是一个新型多模态基准数据集，旨在通过准确的电解槽材料分类加速关键原材料的回收利用。该数据集包含55张高分辨率RGB图像和覆盖400-2500纳米光谱范围的高光谱成像（HSI）数据立方体，共产生超过420万个像素向量和424,169个标记向量。这允许对粉碎的电解槽样本进行非侵入性光谱分析，支持定量和定性材料分类以及光谱特性研究。我们评估了一系列基线机器学习（ML）方法以及最新的基于变压器的深度学习（DL）架构，包括视觉变压器、光谱former和多模态融合变压器，以调查部署变压器时的架构瓶颈，进一步优化材料识别效率。我们实施了零样本检测技术和像素级预测的多数投票来建立对象级别分类的鲁棒性。遵循FAIR数据原则，Electrolyzers-HSI数据集及其配套代码库在此开放访问：此链接和此链接，支持可再现研究并促进智能和可持续电子废物回收解决方案的更广泛应用。', 'title_zh': '电解槽-HSI：近距离多场景高光谱成像基准数据集'}
{'arxiv_id': 'arXiv:2505.20506', 'title': 'ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis', 'authors': 'Hawau Olamide Toyin, Rufael Marew, Humaid Alblooshi, Samar M. Magdy, Hanan Aldarmaki', 'link': 'https://arxiv.org/abs/2505.20506', 'abstract': 'We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech corpus with diacritized transcriptions, intended for multi-speaker speech synthesis, and can be useful for other tasks such as speech-based diacritic restoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a new professionally recorded set from six voice talents with diverse demographics, (2) a modified subset of the Arabic Speech Corpus; and (3) high-quality synthetic speech from two commercial systems. The complete corpus consists of a total of 83.52 hours of speech across 11 voices; around 10 hours consist of human voices from 7 speakers. We train three open-source TTS and two voice conversion systems to illustrate the use cases of the dataset. The corpus is available for research use.', 'abstract_zh': 'ArVoice：一个多说话者现代标准阿拉伯语发音语料库，包含标音转录，用于多说话者语音合成及其他任务的研究', 'title_zh': 'ArVoice：阿拉伯语音多说话人数据集'}
{'arxiv_id': 'arXiv:2505.20503', 'title': 'Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review', 'authors': 'Matthew Lisondra, Beno Benhabib, Goldie Nejat', 'link': 'https://arxiv.org/abs/2505.20503', 'abstract': 'Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interactions, robots can improve understanding, adapt to, and execute complex tasks in dynamic real-world environments. However, embodied AI in mobile service robots continues to face key challenges, including multimodal sensor fusion, real-time decision-making under uncertainty, task generalization, and effective human-robot interactions (HRI). In this paper, we present the first systematic review of the integration of foundation models in mobile service robotics, identifying key open challenges in embodied AI and examining how foundation models can address them. Namely, we explore the role of such models in enabling real-time sensor fusion, language-conditioned control, and adaptive task execution. Furthermore, we discuss real-world applications in the domestic assistance, healthcare, and service automation sectors, demonstrating the transformative impact of foundation models on service robotics. We also include potential future research directions, emphasizing the need for predictive scaling laws, autonomous long-term adaptation, and cross-embodiment generalization to enable scalable, efficient, and robust deployment of foundation models in human-centric robotic systems.', 'abstract_zh': '基础模型的快速发展，包括大型语言模型、 vision-language模型、多模态大型语言模型以及vision-language-action模型，为移动服务机器人中的具身人工智能打开了新的途径。通过将基础模型与具身人工智能的原则相结合，即智能系统通过物理交互来感知、推理和行动，机器人可以在动态真实环境中提高理解能力、适应环境并执行复杂任务。然而，移动服务机器人中的具身人工智能仍然面临多重挑战，包括多模态传感器融合、不确定性的实时决策、任务泛化以及有效的机器人-人类交互（HRI）。在本文中，我们首次系统地回顾了基础模型在移动服务机器人中的集成，确定了具身人工智能中的关键开放挑战，并探讨了基础模型如何应对这些挑战。具体而言，我们探讨了这些模型在实现实时传感器融合、基于语言的控制以及适应性任务执行中的作用。此外，我们讨论了在家庭辅助、医疗服务和服务业自动化领域的实际应用，展示了基础模型对服务机器人的变革性影响。我们还提出了潜在的未来研究方向，强调预测性扩展规律、自主长期适应和跨具身的一般化的重要性，以实现面向人类的机器人系统中基础模型的大规模、高效和稳健部署。', 'title_zh': '基于基础模型的移动服务机器人具身AI：一项系统回顾'}
{'arxiv_id': 'arXiv:2505.20500', 'title': 'Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism', 'authors': 'Naba Rizvi, Harper Strickland, Saleha Ahmedi, Aekta Kallepalli, Isha Khirwadkar, William Wu, Imani N. S. Munyaka, Nedjma Ousidhoum', 'link': 'https://arxiv.org/abs/2505.20500', 'abstract': 'Large language models (LLMs) are increasingly used in decision-making tasks like résumé screening and content moderation, giving them the power to amplify or suppress certain perspectives. While previous research has identified disability-related biases in LLMs, little is known about how they conceptualize ableism or detect it in text. We evaluate the ability of four LLMs to identify nuanced ableism directed at autistic individuals. We examine the gap between their understanding of relevant terminology and their effectiveness in recognizing ableist content in context. Our results reveal that LLMs can identify autism-related language but often miss harmful or offensive connotations. Further, we conduct a qualitative comparison of human and LLM explanations. We find that LLMs tend to rely on surface-level keyword matching, leading to context misinterpretations, in contrast to human annotators who consider context, speaker identity, and potential impact. On the other hand, both LLMs and humans agree on the annotation scheme, suggesting that a binary classification is adequate for evaluating LLM performance, which is consistent with findings from prior studies involving human annotators.', 'abstract_zh': '大规模语言模型（LLMs）在简历筛选和内容审核等决策任务中的应用越来越广泛，使其能够放大或抑制某些观点。尽管先前的研究已经识别出大规模语言模型中的残障相关偏见，但它们如何概念化 ableism 或在文本中检测 ableism 尚不明确。我们评估了四种大规模语言模型识别针对自闭症个体的微妙 ableism 的能力。我们考察了它们对相关术语的理解与在具体语境中识别 ableist 内容效果之间的差距。研究结果表明，大规模语言模型能够识别与自闭症相关的语言，但往往忽视了其有害或冒犯的含义。此外，我们对人类和大规模语言模型的解释进行了定性比较。发现大规模语言模型倾向于依赖表面关键词匹配，导致语境误解，而人类注释员会考虑语境、说话人身份以及潜在影响。另一方面，人类和大规模语言模型在标注方案上达成一致，表明二元分类法适用于评估大规模语言模型的表现，这与先前涉及人类注释员的研究结果一致。', 'title_zh': '超越关键词：评估大规模语言模型对细腻 ableism 的分类能力'}
{'arxiv_id': 'arXiv:2505.20487', 'title': 'InFact: Informativeness Alignment for Improved LLM Factuality', 'authors': 'Roi Cohen, Russa Biswas, Gerard de Melo', 'link': 'https://arxiv.org/abs/2505.20487', 'abstract': "Factual completeness is a general term that captures how detailed and informative a factually correct text is. For instance, the factual sentence ``Barack Obama was born in the United States'' is factually correct, though less informative than the factual sentence ``Barack Obama was born in Honolulu, Hawaii, United States''. Despite the known fact that LLMs tend to hallucinate and generate factually incorrect text, they might also tend to choose to generate factual text that is indeed factually correct and yet less informative than other, more informative choices. In this work, we tackle this problem by proposing an informativeness alignment mechanism. This mechanism takes advantage of recent factual benchmarks to propose an informativeness alignment objective. This objective prioritizes answers that are both correct and informative. A key finding of our work is that when training a model to maximize this objective or optimize its preference, we can improve not just informativeness but also factuality.", 'abstract_zh': '事实完备性是指文本在事实正确的情况下详细和信息量丰富的程度。例如，句子“巴拉克·奥巴马出生于美国”是事实正确的，但不如“巴拉克·奥巴马出生于夏威夷州檀香山市，美国”信息量丰富。尽管已知语言模型（LLMs）倾向于生成事实不正确的文本，它们也可能倾向于生成虽然事实正确但信息量较少的文本，而不是选择其他更信息量丰富的选项。在此项工作中，我们通过提出一种信息量对齐机制来解决这一问题。该机制利用近期的事实基准数据提出了一种信息量对齐目标，该目标优先考虑既正确又具有信息量的答案。我们的工作的一个关键发现是，在训练模型以最大化此目标或优化其偏好时，我们不仅可以提高信息量，还可以提高事实准确性。', 'title_zh': 'InFact: 信息量对提高LLM事实性的对齐调整'}
{'arxiv_id': 'arXiv:2505.20485', 'title': 'Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data', 'authors': 'Abhijit Chunduru, Majid Morafah, Mahdi Morafah, Vishnu Pandi Chellapandi, Ang Li', 'link': 'https://arxiv.org/abs/2505.20485', 'abstract': 'The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.', 'abstract_zh': '数据异构性的不可避免存在使得联邦学习极具挑战性。尽管有许多方法可以应对这一问题，如局部正则化、更好的模型融合技术和数据共享，但它们缺乏对数据异构性如何影响全局决策边界的深刻理解。在本文中，我们通过使用玩具示例进行实验分析，弥合了这一缺口。我们的观察令人惊讶：(1) 我们发现现有方法存在遗忘现象，客户端会忘记全局决策边界的知识，仅学习完美的局部决策边界；(2) 这种现象与初始权重无关，客户端即使从预训练的最优权重开始也会忘记全局决策边界。在本文中，我们提出了FedProj，这是一种能够在本地训练过程中避免全局决策边界遗忘的联邦学习框架。为实现更好的集成知识融合，我们设计了一种新颖的服务器端集成知识转移损失，进一步校准学习到的全局决策边界。为缓解学习到的全局决策边界遗忘的问题，我们进一步提出利用公共未标记数据集中平均集成logits的 episodic 记忆来调节每次本地训练步骤中的梯度更新。实验结果表明，FedProj 在性能上大幅优于现有最先进的方法。', 'title_zh': '在非 iid 数据下保持全局知识梯度以避免遗忘的联邦学习'}
{'arxiv_id': 'arXiv:2505.20482', 'title': 'Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding', 'authors': 'Vibhor Agarwal, Arjoo Gupta, Suparna De, Nishanth Sastry', 'link': 'https://arxiv.org/abs/2505.20482', 'abstract': "Understanding online conversations has attracted research attention with the growth of social networks and online discussion forums. Content analysis of posts and replies in online conversations is difficult because each individual utterance is usually short and may implicitly refer to other posts within the same conversation. Thus, understanding individual posts requires capturing the conversational context and dependencies between different parts of a conversation tree and then encoding the context dependencies between posts and comments/replies into the language model.\nTo this end, we propose a general-purpose mechanism to discover appropriate conversational context for various aspects about an online post in a conversation, such as whether it is informative, insightful, interesting or funny. Specifically, we design two families of Conversation Kernels, which explore different parts of the neighborhood of a post in the tree representing the conversation and through this, build relevant conversational context that is appropriate for each task being considered. We apply our developed method to conversations crawled from this http URL, which allows users to apply highly different labels to posts, such as 'insightful', 'funny', etc., and therefore provides an ideal experimental platform to study whether a framework such as Conversation Kernels is general-purpose and flexible enough to be adapted to disparately different conversation understanding tasks.", 'abstract_zh': '在线对话的理解随着社交网络和在线讨论论坛的增长而吸引了研究关注。由于在线对话中的每个个体发言通常都很短，并且可能隐含地引用同一对话内的其他帖子，因此理解个体发言需要捕获对话背景和对话树不同部分之间的依赖性，然后将帖子和评论/回复之间的背景依赖性编码到语言模型中。为此，我们提出了一种通用机制，以发现在线帖子在对话中涉及的各种方面的适当对话背景，例如是否有信息性、见解性、趣味性或幽默感。具体地，我们设计了两种类型的对话核函数，探索表示对话的树结构中帖子的邻域的不同部分，并通过这种方式构建适用于每项任务的相关对话背景。我们将开发的方法应用于从该网址爬取的对话，这些对话允许用户为帖子应用高度不同的标签，如“见解性”、“有趣”等，因此提供了理想的研究平台，以研究如对话核函数框架是否通用且灵活，能够适应不同对话理解任务。', 'title_zh': '对话核：一种灵活的机制，用于在线对话理解的相关背景学习'}
{'arxiv_id': 'arXiv:2505.20481', 'title': 'CardioPatternFormer: Pattern-Guided Attention for Interpretable ECG Classification with Transformer Architecture', 'authors': 'Berat Kutay Uğraş, Ömer Nezih Gerek, İbrahim Talha Saygı', 'link': 'https://arxiv.org/abs/2505.20481', 'abstract': 'Accurate ECG interpretation is vital, yet complex cardiac data and "black-box" AI models limit clinical utility. Inspired by Transformer architectures\' success in NLP for understanding sequential data, we frame ECG as the heart\'s unique "language" of temporal patterns. We present CardioPatternFormer, a novel Transformer-based model for interpretable ECG classification. It employs a sophisticated attention mechanism to precisely identify and classify diverse cardiac patterns, excelling at discerning subtle anomalies and distinguishing multiple co-occurring conditions. This pattern-guided attention provides clear insights by highlighting influential signal regions, effectively allowing the "heart to talk" through transparent interpretations. CardioPatternFormer demonstrates robust performance on challenging ECGs, including complex multi-pathology cases. Its interpretability via attention maps enables clinicians to understand the model\'s rationale, fostering trust and aiding informed diagnostic decisions. This work offers a powerful, transparent solution for advanced ECG analysis, paving the way for more reliable and clinically actionable AI in cardiology.', 'abstract_zh': '准确的心电图解读至关重要，但复杂的心脏数据和“黑盒”AI模型限制了其临床应用。受Transformer架构在自然语言处理中理解序贯数据成功经验的启发，我们将心电图视为心脏的独特“语言”中的时间模式。我们提出了CardioPatternFormer，这是一种基于Transformer的可解释心电图分类模型。该模型采用复杂的注意力机制以精确识别和分类多种心脏模式，擅长区分微妙的异常和多种并发条件。这种基于模式的注意力通过突出显示有影响的信号区域，提供清晰的见解，有效地使“心脏得以发声”，并通过透明的解释增强信任。CardioPatternFormer在包括复杂多病理情况在内的挑战性心电图上表现出稳健的性能。其通过注意力图的可解释性使临床医生能够理解模型的推理过程，增强信任并辅助诊断决策。本研究提供了一种强大的、透明的高级心电图分析解决方案，为心脏病学中更可靠和临床可操作的AI铺平了道路。', 'title_zh': 'CardioPatternFormer: 以模式为引导的注意力机制在Transformer架构下的心电图可解释分类'}
{'arxiv_id': 'arXiv:2505.20471', 'title': 'WeatherEdit: Controllable Weather Editing with 4D Gaussian Field', 'authors': 'Chenghao Qian, Wenjing Li, Yuhu Guo, Gustav Markkula', 'link': 'https://arxiv.org/abs/2505.20471', 'abstract': 'In this work, we present WeatherEdit, a novel weather editing pipeline for generating realistic weather effects with controllable types and severity in 3D scenes. Our approach is structured into two key components: weather background editing and weather particle construction. For weather background editing, we introduce an all-in-one adapter that integrates multiple weather styles into a single pretrained diffusion model, enabling the generation of diverse weather effects in 2D image backgrounds. During inference, we design a Temporal-View (TV-) attention mechanism that follows a specific order to aggregate temporal and spatial information, ensuring consistent editing across multi-frame and multi-view images. To construct the weather particles, we first reconstruct a 3D scene using the edited images and then introduce a dynamic 4D Gaussian field to generate snowflakes, raindrops and fog in the scene. The attributes and dynamics of these particles are precisely controlled through physical-based modelling and simulation, ensuring realistic weather representation and flexible severity adjustments. Finally, we integrate the 4D Gaussian field with the 3D scene to render consistent and highly realistic weather effects. Experiments on multiple driving datasets demonstrate that WeatherEdit can generate diverse weather effects with controllable condition severity, highlighting its potential for autonomous driving simulation in adverse weather. See project page: this https URL', 'abstract_zh': '基于可控类型和严重程度生成现实天气效果的新型天气编辑管道WeatherEdit', 'title_zh': 'WeatherEdit: 基于4D高斯场的可控天气编辑'}
{'arxiv_id': 'arXiv:2505.20469', 'title': 'CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting', 'authors': 'Lei Tian, Xiaomin Li, Liqian Ma, Hefei Huang, Zirui Zheng, Hao Yin, Taiqing Li, Huchuan Lu, Xu Jia', 'link': 'https://arxiv.org/abs/2505.20469', 'abstract': 'Recent advances in 3D reconstruction techniques and vision-language models have fueled significant progress in 3D semantic understanding, a capability critical to robotics, autonomous driving, and virtual/augmented reality. However, methods that rely on 2D priors are prone to a critical challenge: cross-view semantic inconsistencies induced by occlusion, image blur, and view-dependent variations. These inconsistencies, when propagated via projection supervision, deteriorate the quality of 3D Gaussian semantic fields and introduce artifacts in the rendered outputs. To mitigate this limitation, we propose CCL-LGS, a novel framework that enforces view-consistent semantic supervision by integrating multi-view semantic cues. Specifically, our approach first employs a zero-shot tracker to align a set of SAM-generated 2D masks and reliably identify their corresponding categories. Next, we utilize CLIP to extract robust semantic encodings across views. Finally, our Contrastive Codebook Learning (CCL) module distills discriminative semantic features by enforcing intra-class compactness and inter-class distinctiveness. In contrast to previous methods that directly apply CLIP to imperfect masks, our framework explicitly resolves semantic conflicts while preserving category discriminability. Extensive experiments demonstrate that CCL-LGS outperforms previous state-of-the-art methods. Our project page is available at this https URL.', 'abstract_zh': 'Recent advances in 3D重建技术和视觉语言模型促进了三维语义理解的重大进展，这一能力对机器人技术、自动驾驶和虚拟/增强现实至关重要。然而，依赖2D先验的方法易遭受关键挑战：因遮挡、图像模糊和视角依赖性变化引发的跨视角语义不一致性。这些不一致性通过投影监督传播，会降低3D高斯语义场的质量并在渲染输出中引入伪影。为了减轻这一限制，我们提出了一种名为CCL-LGS的新框架，通过集成多视角语义线索来确保视图一致的语义监督。具体而言，我们的方法首先采用零样本跟踪器对SAM生成的2D掩码进行对齐，并可靠地识别其对应的类别。接着，我们利用CLIP在不同视角中提取鲁棒的语义编码。最后，我们的对比码本学习（CCL）模块通过促进类别内紧凑性和类别间区分性来提炼具有辨别性的语义特征。与之前直接将CLIP应用于不完美的掩码的方法相比，我们的框架显式地解决了语义冲突并保留了类别的可辨别性。大量实验表明，CCL-LGS优于之前的方法。我们的项目页面可在此 https URL 查看。', 'title_zh': 'CCL-LGS: 对比学习代码簿 для 3D 语言高斯点云生成'}
{'arxiv_id': 'arXiv:2505.20445', 'title': 'In-context Language Learning for Endangered Languages in Speech Recognition', 'authors': 'Zhaolin Li, Jan Niehues', 'link': 'https://arxiv.org/abs/2505.20445', 'abstract': 'With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs.', 'abstract_zh': '当前世界上有大约7000种语言，现有的大型语言模型（LLMs）仅支持其中的一小部分。先前的研究表明，LLMs可以在某些任务中学习新语言而无需监督数据。我们将这一研究扩展到语音识别领域，探讨LLMs是否可以通过上下文学习（ICL）来学习未见过的、资源稀缺的语言。通过在四个未被LLMs训练的多样化濒危语言上进行实验，我们发现，提供更多相关的文本样本可以提升语言建模和自动语音识别（ASR）任务的表现。此外，我们展示了基于概率的方法在语言学习中优于传统的基于指令的方法。最后，我们展示了ICL使LLMs能够实现与专门为此类语言训练的语言模型相当甚至更优的ASR性能，同时保留原始LLMs的原有能力。', 'title_zh': '语音识别中濒危语言的上下文内语言学习'}
{'arxiv_id': 'arXiv:2505.20435', 'title': 'Holes in Latent Space: Topological Signatures Under Adversarial Influence', 'authors': 'Aideen Fay, Inés García-Redondo, Qiquan Wang, Haim Dubossarsky, Anthea Monod', 'link': 'https://arxiv.org/abs/2505.20435', 'abstract': 'Understanding how adversarial conditions affect language models requires techniques that capture both global structure and local detail within high-dimensional activation spaces. We propose persistent homology (PH), a tool from topological data analysis, to systematically characterize multiscale latent space dynamics in LLMs under two distinct attack modes -- backdoor fine-tuning and indirect prompt injection. By analyzing six state-of-the-art LLMs, we show that adversarial conditions consistently compress latent topologies, reducing structural diversity at smaller scales while amplifying dominant features at coarser ones. These topological signatures are statistically robust across layers, architectures, model sizes, and align with the emergence of adversarial effects deeper in the network. To capture finer-grained mechanisms underlying these shifts, we introduce a neuron-level PH framework that quantifies how information flows and transforms within and across layers. Together, our findings demonstrate that PH offers a principled and unifying approach to interpreting representational dynamics in LLMs, particularly under distributional shift.', 'abstract_zh': '理解对抗条件如何影响语言模型需要捕捉高维激活空间中全局结构和局部细节的技术。我们提出使用拓扑数据分析工具持久同调（PH）系统地 characterizing LLMs 在两种不同的攻击模式——后门微调和间接提示注入下的多尺度潜在空间动力学。通过分析六种最先进的LLMs，我们展示了对抗条件一致地压缩潜在拓扑结构，在较小尺度上减少结构多样性，而在较粗尺度上放大主导特征。这些拓扑特征在各层、架构、模型规模上具有统计鲁棒性，并与网络更深层次出现的对抗效应一致。为了捕捉这些转变的更细粒度机制，我们引入了一种神经元级的持久同调框架，量化信息在和跨层中的流动和转换。我们的研究结果共同表明，持久同调为理解LLMs中的表示动力学提供了一个原则性的统一方法，特别是在分布变化的情况下。', 'title_zh': '潜在空间中的洞： adversarial 影响下的拓扑特征'}
{'arxiv_id': 'arXiv:2505.20424', 'title': 'Robot Operation of Home Appliances by Reading User Manuals', 'authors': 'Jian Zhang, Hanbo Zhang, Anxing Xiao, David Hsu', 'link': 'https://arxiv.org/abs/2505.20424', 'abstract': 'Operating home appliances, among the most common tools in every household, is a critical capability for assistive home robots. This paper presents ApBot, a robot system that operates novel household appliances by "reading" their user manuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial policies from their unstructured, textual descriptions in a user manual document, (ii) ground the policies to the appliance in the physical world, and (iii) execute the policies reliably over potentially many steps, despite compounding errors. To tackle these challenges, ApBot constructs a structured, symbolic model of an appliance from its manual, with the help of a large vision-language model (VLM). It grounds the symbolic actions visually to control panel elements. Finally, ApBot closes the loop by updating the model based on visual feedback. Our experiments show that across a wide range of simulated and real-world appliances, ApBot achieves consistent and statistically significant improvements in task success rate, compared with state-of-the-art large VLMs used directly as control policies. These results suggest that a structured internal representations plays an important role in robust robot operation of home appliances, especially, complex ones.', 'abstract_zh': '操作家用电器：一种通过“阅读”用户手册操控新型家用电器的机器人系统', 'title_zh': '基于阅读用户手册控制家用电器的机器人操作方法'}
{'arxiv_id': 'arXiv:2505.20423', 'title': 'Vision-Based Risk Aware Emergency Landing for UAVs in Complex Urban Environments', 'authors': 'Julio de la Torre-Vanegas, Miguel Soriano-Garcia, Israel Becerra, Diego Mercado-Ravell', 'link': 'https://arxiv.org/abs/2505.20423', 'abstract': "Landing safely in crowded urban environments remains an essential yet challenging endeavor for Unmanned Aerial Vehicles (UAVs), especially in emergency situations. In this work, we propose a risk-aware approach that harnesses semantic segmentation to continuously evaluate potential hazards in the drone's field of view. By using a specialized deep neural network to assign pixel-level risk values and applying an algorithm based on risk maps, our method adaptively identifies a stable Safe Landing Zone (SLZ) despite moving critical obstacles such as vehicles, people, etc., and other visual challenges like shifting illumination. A control system then guides the UAV toward this low-risk region, employing altitude-dependent safety thresholds and temporal landing point stabilization to ensure robust descent trajectories. Experimental validation in diverse urban environments demonstrates the effectiveness of our approach, achieving over 90% landing success rates in very challenging real scenarios, showing significant improvements in various risk metrics. Our findings suggest that risk-oriented vision methods can effectively help reduce the risk of accidents in emergency landing situations, particularly in complex, unstructured, urban scenarios, densely populated with moving risky obstacles, while potentiating the true capabilities of UAVs in complex urban operations.", 'abstract_zh': '在拥挤的城市环境中安全着陆仍然是无人机（UAV）的一项必不可少但极具挑战的任务，尤其是在紧急情况下。本文提出一种风险感知的方法，利用语义分割持续评估无人机视野中的潜在危险。通过使用专门的深神经网络为像素级分配风险值，并基于风险图的应用算法，我们的方法能够在移动的障碍物（如车辆、人员等）和其他视觉挑战（如光照变化）下自适应地识别一个稳定的安全着陆区（SLZ）。随后的控制系统引导无人机向低风险区域降落，采用高度依赖的安全阈值和时间窗口内的着陆点稳定技术，以确保稳健的下降轨迹。在多样化的城市环境中进行的实验验证表明，该方法的有效性，在极其挑战的真实场景中实现了超过90%的着陆成功率，并在多种风险指标上显示出显著改进。我们的研究结果表明，风险导向的视觉方法能够有效帮助减少紧急着陆情况下的事故风险，特别是在复杂的、非结构化的、充满移动危险障碍物的城市场景中，同时增强无人机在复杂城市操作中的真正能力。', 'title_zh': '基于视觉的风险感知紧急着陆方法研究：复杂城市环境中的无人机应用'}
{'arxiv_id': 'arXiv:2505.20422', 'title': 'SEMMA: A Semantic Aware Knowledge Graph Foundation Model', 'authors': 'Arvindh Arun, Sumit Kumar, Mojtaba Nayyeri, Bo Xiong, Ponnurangam Kumaraguru, Antonio Vergari, Steffen Staab', 'link': 'https://arxiv.org/abs/2505.20422', 'abstract': 'Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling zero-shot reasoning over unseen graphs by learning transferable patterns. However, most existing KGFMs rely solely on graph structure, overlooking the rich semantic signals encoded in textual attributes. We introduce SEMMA, a dual-module KGFM that systematically integrates transferable textual semantics alongside structure. SEMMA leverages Large Language Models (LLMs) to enrich relation identifiers, generating semantic embeddings that subsequently form a textual relation graph, which is fused with the structural component. Across 54 diverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully inductive link prediction. Crucially, we show that in more challenging generalization settings, where the test-time relation vocabulary is entirely unseen, structural methods collapse while SEMMA is 2x more effective. Our findings demonstrate that textual semantics are critical for generalization in settings where structure alone fails, highlighting the need for foundation models that unify structural and linguistic signals in knowledge reasoning.', 'abstract_zh': 'SEmma: A Dual-Module Knowledge Graph Foundation Model Integrating Transferable Textual Semantics', 'title_zh': 'SEMMA：一种语义 Awareness 知识图谱基础模型'}
{'arxiv_id': 'arXiv:2505.20416', 'title': 'GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation', 'authors': 'Zihong Chen, Wanli Jiang, Jinzhe Li, Zhonghang Yuan, Huanjun Kong, Wanli Ouyang, Nanqing Dong', 'link': 'https://arxiv.org/abs/2505.20416', 'abstract': 'Fine-tuning for large language models (LLMs) typically requires substantial amounts of high-quality supervised data, which is both costly and labor-intensive to acquire. While synthetic data generation has emerged as a promising solution, existing approaches frequently suffer from factual inaccuracies, insufficient long-tail coverage, simplistic knowledge structures, and homogenized outputs. To address these challenges, we introduce GraphGen, a knowledge graph-guided framework designed for three key question-answering (QA) scenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by constructing a fine-grained knowledge graph from the source text. It then identifies knowledge gaps in LLMs using the expected calibration error metric, prioritizing the generation of QA pairs that target high-value, long-tail knowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling to capture complex relational information and employs style-controlled generation to diversify the resulting QA data. Experimental results on knowledge-intensive tasks under closed-book settings demonstrate that GraphGen outperforms conventional synthetic data methods, offering a more reliable and comprehensive solution to the data scarcity challenge in supervised fine-tuning. The code and data are publicly available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）的微调通常需要大量高质量的监督数据，获取这些数据既昂贵又耗时。虽然合成数据生成已成为一种有前途的解决方案，但现有方法经常存在事实不准确、长尾覆盖不足、知识结构简单化和输出同质化等问题。为了应对这些挑战，我们引入了GraphGen，这是一个知识图谱引导的框架，用于三种关键的问答（QA）场景：原子问答、聚合问答和多跳问答。它首先从源文本中构建细粒度的知识图谱。然后使用预期校准误差指标识别LLMs中的知识空白，优先生成针对高价值和长尾知识的问答对。此外，GraphGen 还采用了多跳邻域采样来捕获复杂的关系信息，并使用风格控制生成以增加生成的QA数据的多样性。在闭卷条件下的知识密集型任务实验结果表明，GraphGen 在监督微调的数据稀缺性挑战中提供了更可靠和全面的解决方案。代码和数据已公开，在此链接处可获取：this https URL。', 'title_zh': 'GraphGen：基于知识驱动的合成数据生成增强的监督微调用于大规模语言模型'}
{'arxiv_id': 'arXiv:2505.20414', 'title': 'RetroMotion: Retrocausal Motion Forecasting Models are Instructable', 'authors': 'Royden Wagner, Omer Sahin Tas, Felix Hauser, Marlon Steiner, Dominik Strutz, Abhishek Vivekanandan, Carlos Fernandez, Christoph Stiller', 'link': 'https://arxiv.org/abs/2505.20414', 'abstract': 'Motion forecasts of road users (i.e., agents) vary in complexity as a function of scene constraints and interactive behavior. We address this with a multi-task learning method for motion forecasting that includes a retrocausal flow of information. The corresponding tasks are to forecast (1) marginal trajectory distributions for all modeled agents and (2) joint trajectory distributions for interacting agents. Using a transformer model, we generate the joint distributions by re-encoding marginal distributions followed by pairwise modeling. This incorporates a retrocausal flow of information from later points in marginal trajectories to earlier points in joint trajectories. Per trajectory point, we model positional uncertainty using compressed exponential power distributions. Notably, our method achieves state-of-the-art results in the Waymo Interaction Prediction dataset and generalizes well to the Argoverse 2 dataset. Additionally, our method provides an interface for issuing instructions through trajectory modifications. Our experiments show that regular training of motion forecasting leads to the ability to follow goal-based instructions and to adapt basic directional instructions to the scene context. Code: this https URL', 'abstract_zh': '基于多任务学习的考虑回因效应的运动预测方法：预测道路使用者的运动轨迹在不同场景约束和交互行为下的复杂性变化，并通过变换器模型生成联合轨迹分布。该方法包含轨迹点位置不确定性建模，并在Waymo Interaction Prediction数据集和Argoverse 2数据集上取得了优异结果且具有良好泛化能力，还提供了通过轨迹修改发布指令的接口。实验表明，常规训练能实现基于目标的指令跟随和基本方向指令的场景适应。代码：https://this.url', 'title_zh': 'RetroMotion: 反向因果运动预测模型可受控 víctima'}
{'arxiv_id': 'arXiv:2505.20405', 'title': 'What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models', 'authors': 'Lorenzo Baraldi, Davide Bucciarelli, Federico Betti, Marcella Cornia, Lorenzo Baraldi, Nicu Sebe, Rita Cucchiara', 'link': 'https://arxiv.org/abs/2505.20405', 'abstract': 'Instruction-based image editing models offer increased personalization opportunities in generative tasks. However, properly evaluating their results is challenging, and most of the existing metrics lag in terms of alignment with human judgment and explainability. To tackle these issues, we introduce DICE (DIfference Coherence Estimator), a model designed to detect localized differences between the original and the edited image and to assess their relevance to the given modification request. DICE consists of two key components: a difference detector and a coherence estimator, both built on an autoregressive Multimodal Large Language Model (MLLM) and trained using a strategy that leverages self-supervision, distillation from inpainting networks, and full supervision. Through extensive experiments, we evaluate each stage of our pipeline, comparing different MLLMs within the proposed framework. We demonstrate that DICE effectively identifies coherent edits, effectively evaluating images generated by different editing models with a strong correlation with human judgment. We publicly release our source code, models, and data.', 'abstract_zh': '基于指令的图像编辑模型为生成任务提供了增高的个性化机会。然而，正确评估其结果具有挑战性，目前大多数现有指标在与人类判断的一致性和可解释性方面存在不足。为应对这些挑战，我们提出DICE（差异共轭估计器），一种用于检测原始图像与编辑图像之间局部差异并评估其与给定修改请求的相关性的模型。DICE由两个关键组件组成：一个差异检测器和一个共轭估计器，两者均基于自回归多模态大语言模型（MLLM）训练，并采用结合自监督、来自修复网络的知识蒸馏以及全方位监督的策略。通过广泛实验，我们评估了流水线的每个阶段，并在提议的框架内比较了不同MLLM的性能。我们证明DICE能够有效识别连贯的编辑，并且对于不同编辑模型生成的图像具有强烈的与人类判断相关性。我们公开 Release 我们的源代码、模型和数据。', 'title_zh': '什么改变了？ multimodal大型语言模型检测和评估指令引导的图像编辑'}
{'arxiv_id': 'arXiv:2505.20377', 'title': 'Algorithmic Control Improves Residential Building Energy and EV Management when PV Capacity is High but Battery Capacity is Low', 'authors': 'Lennart Ullner, Alona Zharova, Felix Creutzig', 'link': 'https://arxiv.org/abs/2505.20377', 'abstract': 'Efficient energy management in prosumer households is key to alleviating grid stress in an energy transition marked by electric vehicles (EV), renewable energies and battery storage. However, it is unclear how households optimize prosumer EV charging. Here we study real-world data from 90 households on fixed-rate electricity tariffs in German-speaking countries to investigate the potential of Deep Reinforcement Learning (DRL) and other control approaches (Rule-Based, Model Predictive Control) to manage the dynamic and uncertain environment of Home Energy Management (HEM) and optimize household charging patterns. The DRL agent efficiently aligns charging of EV and battery storage with photovoltaic (PV) surplus. We find that frequent EV charging transactions, early EV connections and PV surplus increase optimization potential. A detailed analysis of nine households (1 hour resolution, 1 year) demonstrates that high battery capacity facilitates self optimization; in this case further algorithmic control shows little value. In cases with relatively low battery capacity, algorithmic control with DRL improves energy management and cost savings by a relevant margin. This result is further corroborated by our simulation of a synthetic household. We conclude that prosumer households with optimization potential would profit from DRL, thus benefiting also the full electricity system and its decarbonization.', 'abstract_zh': '在电转、可再生能量和电池储能背景下，通过深强化学习和其他控制方法优化消费者-生产者家庭的能源管理对于缓解电网压力至关重要。然而，家庭如何优化消费者-生产者电动汽车充电尚不明确。我们研究了德国-speaking国家90户家庭的固定电价数据，以探讨深度强化学习（DRL）和其他控制方法（基于规则、模型预测控制）在家庭能源管理（HEM）动态和不确定环境下管理电动汽车和电池储能充电模式的潜力。我们发现，频繁的电动汽车充电交易、早期电动汽车连接和光伏剩余电量增加优化潜力。九户家庭的详细分析（每小时一次，一年）表明，高电池容量有助于自我优化；在这种情况下，进一步的算法控制几乎没有价值。在电池容量相对较低的情况下，使用DRL的算法控制可以显著改善能源管理和成本节省。这一结果进一步得到我们对合成家庭的模拟验证。我们得出结论，具有优化潜力的消费者-生产者家庭将从DRL中受益，从而也有利于整个电力系统及其脱碳。', 'title_zh': '高光伏发电量低电池容量条件下，算法控制改善住宅建筑能源与电动车管理'}
{'arxiv_id': 'arXiv:2505.20368', 'title': 'Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents', 'authors': 'Jaeyoung Choe, Jihoon Kim, Woohwan Jung', 'link': 'https://arxiv.org/abs/2505.20368', 'abstract': 'Retrieval-augmented generation (RAG) based large language models (LLMs) are widely used in finance for their excellent performance on knowledge-intensive tasks. However, standardized documents (e.g., SEC filing) share similar formats such as repetitive boilerplate texts, and similar table structures. This similarity forces traditional RAG methods to misidentify near-duplicate text, leading to duplicate retrieval that undermines accuracy and completeness. To address these issues, we propose the Hierarchical Retrieval with Evidence Curation (HiREC) framework. Our approach first performs hierarchical retrieval to reduce confusion among similar texts. It first retrieve related documents and then selects the most relevant passages from the documents. The evidence curation process removes irrelevant passages. When necessary, it automatically generates complementary queries to collect missing information. To evaluate our approach, we construct and release a Large-scale Open-domain Financial (LOFin) question answering benchmark that includes 145,897 SEC documents and 1,595 question-answer pairs. Our code and data are available at this https URL.', 'abstract_zh': '基于检索增强生成（RAG）的大语言模型（LLMs）在金融领域广泛应用于知识密集型任务。然而，标准化文档（如SEC文件）具有相似的格式，如重复的模板文本和相似的表格结构。这种相似性导致传统RAG方法错误地识别近似重复文本，从而导致重复检索，损害准确性和完整性。为了解决这些问题，我们提出了层次化检索与证据整理（HiREC）框架。该方法首先进行层次化检索以减少相似文本之间的混淆，首先检索相关文档，然后从文档中选择最相关的段落。证据整理过程去除无关段落。必要时，它会自动生成补充查询以收集缺失信息。为了评估我们的方法，我们构建并发布了包含145,897份SEC文件和1,595个问答对的大规模开放领域金融（LOFin）问答基准数据集。我们的代码和数据可在以下链接获取。', 'title_zh': '基于证据筛选的层级检索在标准化财务文档中的开放域问答'}
{'arxiv_id': 'arXiv:2505.20362', 'title': 'VSCBench: Bridging the Gap in Vision-Language Model Safety Calibration', 'authors': 'Jiahui Geng, Qing Li, Zongxiong Chen, Yuxia Wang, Derui Zhu, Zhuohan Xie, Chenyang Lyu, Xiuying Chen, Preslav Nakov, Fakhri Karray', 'link': 'https://arxiv.org/abs/2505.20362', 'abstract': "The rapid advancement of vision-language models (VLMs) has brought a lot of attention to their safety alignment. However, existing methods have primarily focused on model undersafety, where the model responds to hazardous queries, while neglecting oversafety, where the model refuses to answer safe queries. In this paper, we introduce the concept of $\\textit{safety calibration}$, which systematically addresses both undersafety and oversafety. Specifically, we present $\\textbf{VSCBench}$, a novel dataset of 3,600 image-text pairs that are visually or textually similar but differ in terms of safety, which is designed to evaluate safety calibration across image-centric and text-centric scenarios. Based on our benchmark, we evaluate safety calibration across eleven widely used VLMs. Our extensive experiments revealed major issues with both undersafety and oversafety. We further investigated four approaches to improve the model's safety calibration. We found that even though some methods effectively calibrated the models' safety problems, these methods also lead to the degradation of models' utility. This trade-off underscores the urgent need for advanced calibration methods, and our benchmark provides a valuable tool for evaluating future approaches. Our code and data are available at this https URL.", 'abstract_zh': '视觉语言模型的安全校准：VSCBench及其应用', 'title_zh': 'VSCBench: 桥接视觉-语言模型安全性标定的差距'}
{'arxiv_id': 'arXiv:2505.20359', 'title': 'Risk-aware Direct Preference Optimization under Nested Risk Measure', 'authors': 'Lijun Zhang, Lin Li, Yajie Qi, Huizhong Song, Yaodong Yang, Jun Wang, Wei Wei', 'link': 'https://arxiv.org/abs/2505.20359', 'abstract': "When fine-tuning pre-trained Large Language Models (LLMs) to align with human values and intentions, maximizing the estimated reward can lead to superior performance, but it also introduces potential risks due to deviations from the reference model's intended behavior. Most existing methods typically introduce KL divergence to constrain deviations between the trained model and the reference model; however, this may not be sufficient in certain applications that require tight risk control. In this paper, we introduce Risk-aware Direct Preference Optimization (Ra-DPO), a novel approach that incorporates risk-awareness by employing a class of nested risk measures. This approach formulates a constrained risk-aware advantage function maximization problem and then converts the Bradley-Terry model into a token-level representation. The objective function maximizes the likelihood of the policy while suppressing the deviation between a trained model and the reference model using a sequential risk ratio, thereby enhancing the model's risk-awareness. Experimental results across three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and AlpacaEval, demonstrate the proposed method's superior performance in balancing alignment performance and model drift. Our code is opensourced at this https URL.", 'abstract_zh': '在预训练大规模语言模型（LLMs）微调以与人类价值观和意图对齐时，通过最大化估计奖励可以实现优异性能，但这也可能由于与参考模型预期行为的偏离而引入潜在风险。现有大多数方法通常通过引入KL散度来限制训练模型与参考模型之间的偏差；然而，在需要严格风险控制的应用中，这可能不够充分。本文提出了一种新的Risk-aware Direct Preference Optimization (Ra-DPO)方法，通过采用嵌套风险度量来增强风险意识。该方法通过约束风险敏感的优势函数最大化问题，并将Bradley-Terry模型转化为 token 级别表示。目标函数在最大化策略的似然性的同时，使用顺序风险比抑制训练模型与参考模型之间的偏差，从而增强模型的风险意识。在IMDb数据集、Anthropic HH数据集和AlpacaEval三个开源数据集上的实验结果表明，所提出的方法在平衡对齐性能和模型漂移方面具有优越性能。我们的代码在此处开源：[链接]。', 'title_zh': '基于嵌套风险度量的aware风险直接偏好优化'}
{'arxiv_id': 'arXiv:2505.20356', 'title': 'LEGO-Compiler: Enhancing Neural Compilation Through Translation Composability', 'authors': 'Shuoming Zhang, Jiacheng Zhao, Chunwei Xia, Zheng Wang, Yunji Chen, Xiaobing Feng, Huimin Cui', 'link': 'https://arxiv.org/abs/2505.20356', 'abstract': 'Large language models (LLMs) have the potential to revolutionize how we design and implement compilers and code translation tools. However, existing LLMs struggle to handle long and complex programs. We introduce LEGO-Compiler, a novel neural compilation system that leverages LLMs to translate high-level languages into assembly code. Our approach centers on three key innovations: LEGO translation, which decomposes the input program into manageable blocks; breaking down the complex compilation process into smaller, simpler verifiable steps by organizing it as a verifiable LLM workflow by external tests; and a feedback mechanism for self-correction. Supported by formal proofs of translation composability, LEGO-Compiler demonstrates high accuracy on multiple datasets, including over 99% on ExeBench and 97.9% on industrial-grade AnsiBench. Additionally, LEGO-Compiler has also acheived near one order-of-magnitude improvement on compilable code size scalability. This work opens new avenues for applying LLMs to system-level tasks, complementing traditional compiler technologies.', 'abstract_zh': '大型语言模型（LLMs）有潜力革新我们设计和实现编译器及代码翻译工具的方式。然而，现有的LLMs难以处理长且复杂的程序。我们提出了LEGO-Compiler，一种新颖的神经编译系统，利用LLMs将高级语言翻译成汇编代码。我们的方法集中在三项关键技术创新上：LEGO翻译，它将输入程序分解为可管理的块；通过将其组织为外部测试的可验证LLM工作流，将复杂的编译过程分解为更小、更简单的可验证步骤；以及用于自我纠正的反馈机制。依托翻译组合性的形式证明，LEGO-Compiler在多个数据集上取得了高精度，包括ExeBench上的超过99%和工业级的AnsiBench上的97.9%。此外，LEGO-Compiler还在可编译代码规模扩展上实现了接近一个数量级的改进。这项工作为将LLMs应用于系统级任务打开了新的途径，补充了传统的编译器技术。', 'title_zh': 'LEGO-Compiler: 通过翻译可组合性增强神经编译'}
{'arxiv_id': 'arXiv:2505.20355', 'title': 'GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning', 'authors': 'Yeonjoon Jung, Daehyun Ahn, Hyungjun Kim, Taesu Kim, Eunhyeok Park', 'link': 'https://arxiv.org/abs/2505.20355', 'abstract': "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at this https URL", 'abstract_zh': 'Granular Low-Rank Adaptation (GraLoRA): A Scalable and Robust Solution for Parameter-Efficient Fine-Tuning', 'title_zh': '粒度低秩适应：参数高效微调'}
{'arxiv_id': 'arXiv:2505.20354', 'title': 'Rethinking Text-based Protein Understanding: Retrieval or LLM?', 'authors': 'Juntong Wu, Zijing Liu, He Cao, Hao Li, Bin Feng, Zishan Shu, Ke Yu, Li Yuan, Yu Li', 'link': 'https://arxiv.org/abs/2505.20354', 'abstract': "In recent years, protein-text models have gained significant attention for their potential in protein generation and understanding. Current approaches focus on integrating protein-related knowledge into large language models through continued pretraining and multi-modal alignment, enabling simultaneous comprehension of textual descriptions and protein sequences. Through a thorough analysis of existing model architectures and text-based protein understanding benchmarks, we identify significant data leakage issues present in current benchmarks. Moreover, conventional metrics derived from natural language processing fail to accurately assess the model's performance in this domain. To address these limitations, we reorganize existing datasets and introduce a novel evaluation framework based on biological entities. Motivated by our observation, we propose a retrieval-enhanced method, which significantly outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy and efficiency in training-free scenarios. Our code and data can be seen at this https URL.", 'abstract_zh': '近年来，蛋白质-文本模型由于其在蛋白质生成和理解方面的潜在能力而受到了广泛关注。当前的方法侧重于通过连续预训练和多模态对齐将蛋白质相关知识整合到大规模语言模型中，以同时理解文本描述和蛋白质序列。通过对现有模型架构和基于文本的蛋白质理解基准进行深入分析，我们发现当前基准中存在的显著数据泄漏问题。此外，源自自然语言处理的传统评估指标无法准确评估模型在该领域的性能。为解决这些限制，我们重新组织现有的数据集，并引入了一个基于生物实体的新型评估框架。受我们的观察启发，我们提出了一种增强检索方法，该方法在蛋白质到文本生成任务中显著优于微调的大规模语言模型，并在无训练场景中展示了准确性和效率。我们的代码和数据可在以下网址查看：this https URL。', 'title_zh': '基于文本的蛋白质理解重思：检索还是大语言模型？'}
{'arxiv_id': 'arXiv:2505.20353', 'title': 'FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation', 'authors': 'Dong Liu, Jiayi Zhang, Yifan Li, Yanxuan Yu, Ben Lengerich, Ying Nian Wu', 'link': 'https://arxiv.org/abs/2505.20353', 'abstract': "Diffusion Transformers (DiT) are powerful generative models but remain computationally intensive due to their iterative structure and deep transformer stacks. To alleviate this inefficiency, we propose FastCache, a hidden-state-level caching and compression framework that accelerates DiT inference by exploiting redundancy within the model's internal representations. FastCache introduces a dual strategy: (1) a spatial-aware token selection mechanism that adaptively filters redundant tokens based on hidden state saliency, and (2) a transformer-level cache that reuses latent activations across timesteps when changes are statistically insignificant. These modules work jointly to reduce unnecessary computation while preserving generation fidelity through learnable linear approximation. Theoretical analysis shows that FastCache maintains bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across multiple DiT variants demonstrate substantial reductions in latency and memory usage, with best generation output quality compared to other cache methods, as measured by FID and t-FID. Code implementation of FastCache is available on GitHub at this https URL.", 'abstract_zh': 'FastCache：一种用于加速Diffusion Transformers推测的隐藏状态级别缓存和压缩框架', 'title_zh': 'FastCache: 通过可学习的线性近似加速扩散变换器的缓存技术'}
{'arxiv_id': 'arXiv:2505.20350', 'title': 'Decision Flow Policy Optimization', 'authors': 'Jifeng Hu, Sili Huang, Siyuan Guo, Zhaogeng Liu, Li Shen, Lichao Sun, Hechang Chen, Yi Chang, Dacheng Tao', 'link': 'https://arxiv.org/abs/2505.20350', 'abstract': 'In recent years, generative models have shown remarkable capabilities across diverse fields, including images, videos, language, and decision-making. By applying powerful generative models such as flow-based models to reinforcement learning, we can effectively model complex multi-modal action distributions and achieve superior robotic control in continuous action spaces, surpassing the limitations of single-modal action distributions with traditional Gaussian-based policies. Previous methods usually adopt the generative models as behavior models to fit state-conditioned action distributions from datasets, with policy optimization conducted separately through additional policies using value-based sample weighting or gradient-based updates. However, this separation prevents the simultaneous optimization of multi-modal distribution fitting and policy improvement, ultimately hindering the training of models and degrading the performance. To address this issue, we propose Decision Flow, a unified framework that integrates multi-modal action distribution modeling and policy optimization. Specifically, our method formulates the action generation procedure of flow-based models as a flow decision-making process, where each action generation step corresponds to one flow decision. Consequently, our method seamlessly optimizes the flow policy while capturing multi-modal action distributions. We provide rigorous proofs of Decision Flow and validate the effectiveness through extensive experiments across dozens of offline RL environments. Compared with established offline RL baselines, the results demonstrate that our method achieves or matches the SOTA performance.', 'abstract_zh': '近年来，生成模型在图像、视频、语言和决策等领域展示了 remarkable 的能力。通过将基于流的生成模型等强大生成模型应用到强化学习中，我们可以有效地建模复杂的多模态动作分布，并在连续动作空间中实现优胜的机器人控制，超越了基于单一模态动作分布的传统高斯策略的限制。先前的方法通常将生成模型作为行为模型，用于拟合数据集中的状态条件动作分布，并通过额外的策略进行独立的策略优化，使用基于价值的样本加权或基于梯度的更新。然而，这种分离阻碍了多模态分布拟合和策略改进的同时优化，最终影响模型的训练并降低性能。为了解决这一问题，我们提出了 Decision Flow，这是一种统一框架，将多模态动作分布建模和策略优化结合起来。具体而言，我们的方法将基于流的模型的动作生成过程表述为一个流决策过程，其中每个动作生成步骤对应一个流决策。因此，我们的方法能够无缝优化流策略并捕获多模态动作分布。我们提供了 Decision Flow 的严格证明，并通过在多个离线 RL 环境中的广泛实验证明了其有效性。与现有的离线 RL 基准相比，结果表明我们的方法达到了或匹配了 SOTA 性能。', 'title_zh': '决策流策略优化'}
{'arxiv_id': 'arXiv:2505.20347', 'title': 'SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data', 'authors': 'Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, Dacheng Tao', 'link': 'https://arxiv.org/abs/2505.20347', 'abstract': 'Recent advances have demonstrated the effectiveness of Reinforcement Learning (RL) in improving the reasoning capabilities of Large Language Models (LLMs). However, existing works inevitably rely on high-quality instructions and verifiable rewards for effective training, both of which are often difficult to obtain in specialized domains. In this paper, we propose Self-play Reinforcement Learning(SeRL) to bootstrap LLM training with limited initial data. Specifically, SeRL comprises two complementary modules: self-instruction and self-rewarding. The former module generates additional instructions based on the available data at each training step, employing robust online filtering strategies to ensure instruction quality, diversity, and difficulty. The latter module introduces a simple yet effective majority-voting mechanism to estimate response rewards for additional instructions, eliminating the need for external annotations. Finally, SeRL performs conventional RL based on the generated data, facilitating iterative self-play learning. Extensive experiments on various reasoning benchmarks and across different LLM backbones demonstrate that the proposed SeRL yields results superior to its counterparts and achieves performance on par with those obtained by high-quality data with verifiable rewards. Our code is available at this https URL.', 'abstract_zh': '最近的研究进展表明强化学习（RL）在提高大型语言模型（LLMs）的推理能力方面非常有效。然而，现有工作不可避免地依赖高质量的指令和可验证的奖励进行有效的训练，而在专门领域中，这两种资源往往难以获得。在本文中，我们提出了一种自我博弈强化学习（SeRL）方法，以有限的初始数据为基础启动LLM的训练。具体而言，SeRL 包含两个互补的模块：自我指令生成和自我奖励。前者模块根据每一步训练可用的数据生成额外的指令，并采用稳健的在线筛选策略来确保指令的质量、多样性和难度。后者模块引入了一个简单而有效的多数投票机制来估计额外指令的响应奖励，从而省去了外部标注的需要。最后，SeRL 基于生成的数据进行传统的 RL 训练，促进迭代自我博弈学习。在各种推理基准测试和不同 LLM 架构上的广泛实验表明，提出的 SeRL 达到了优于其竞争对手的效果，并且在性能上与使用高质量数据和可验证奖励获得的结果相当。我们的代码可在此 URL 获取：this https URL。', 'title_zh': 'SeRL：基于自我博弈的大型语言模型有限数据强化学习'}
{'arxiv_id': 'arXiv:2505.20346', 'title': 'PDFBench: A Benchmark for De novo Protein Design from Function', 'authors': 'Jiahao Kuang, Nuowei Liu, Changzhi Sun, Tao Ji, Yuanbin Wu', 'link': 'https://arxiv.org/abs/2505.20346', 'abstract': 'In recent years, while natural language processing and multimodal learning have seen rapid advancements, the field of de novo protein design has also experienced significant growth. However, most current methods rely on proprietary datasets and evaluation rubrics, making fair comparisons between different approaches challenging. Moreover, these methods often employ evaluation metrics that capture only a subset of the desired properties of designed proteins, lacking a comprehensive assessment framework. To address these, we introduce PDFBench, the first comprehensive benchmark for evaluating de novo protein design from function. PDFBench supports two tasks: description-guided design and keyword-guided design. To ensure fair and multifaceted evaluation, we compile 22 metrics covering sequence plausibility, structural fidelity, and language-protein alignment, along with measures of novelty and diversity. We evaluate five state-of-the-art baselines, revealing their respective strengths and weaknesses across tasks. Finally, we analyze inter-metric correlations, exploring the relationships between four categories of metrics, and offering guidelines for metric selection. PDFBench establishes a unified framework to drive future advances in function-driven de novo protein design.', 'abstract_zh': '近年来，尽管自然语言处理和多模态学习取得了 rapid advancements，从功能出发的 de novo 蛋白质设计领域也经历了显著增长。然而，目前大多数方法依赖于专有的数据集和评估标准，这使得不同方法之间的公平比较变得具有挑战性。此外，这些方法往往仅通过有限的评估指标来捕获设计蛋白质的期望属性，缺乏一个全面的评估框架。为了应对这些问题，我们引入了 PDFBench——第一个面向功能的 de novo 蛋白质设计综合基准。PDFBench 支持描述导向设计和关键词导向设计两种任务。为了确保公平和多维度的评估，我们编译了 22 个指标，涵盖了序列合理性、结构准确性以及语言-蛋白质对齐，同时包括新颖性和多样性衡量指标。我们评估了五种最先进的基线方法，在不同任务中揭示了它们各自的优劣。最后，我们分析了指标间的相关性，探讨了四类指标之间的关系，并提出了指标选择的指导原则。PDFBench 为推动功能导向的 de novo 蛋白质设计的未来进展建立了统一框架。', 'title_zh': 'PDFBench: 一种从功能出发的新型蛋白质设计基准测试'}
{'arxiv_id': 'arXiv:2505.20343', 'title': 'Do LLMs have a Gender (Entropy) Bias?', 'authors': 'Sonal Prabhune, Balaji Padmanabhan, Kaushik Dutta', 'link': 'https://arxiv.org/abs/2505.20343', 'abstract': 'We investigate the existence and persistence of a specific type of gender bias in some of the popular LLMs and contribute a new benchmark dataset, RealWorldQuestioning (released on HuggingFace ), developed from real-world questions across four key domains in business and health contexts: education, jobs, personal financial management, and general health. We define and study entropy bias, which we define as a discrepancy in the amount of information generated by an LLM in response to real questions users have asked. We tested this using four different LLMs and evaluated the generated responses both qualitatively and quantitatively by using ChatGPT-4o (as "LLM-as-judge"). Our analyses (metric-based comparisons and "LLM-as-judge" evaluation) suggest that there is no significant bias in LLM responses for men and women at a category level. However, at a finer granularity (the individual question level), there are substantial differences in LLM responses for men and women in the majority of cases, which "cancel" each other out often due to some responses being better for males and vice versa. This is still a concern since typical users of these tools often ask a specific question (only) as opposed to several varied ones in each of these common yet important areas of life. We suggest a simple debiasing approach that iteratively merges the responses for the two genders to produce a final result. Our approach demonstrates that a simple, prompt-based debiasing strategy can effectively debias LLM outputs, thus producing responses with higher information content than both gendered variants in 78% of the cases, and consistently achieving a balanced integration in the remaining cases.', 'abstract_zh': '我们研究了一些流行LLM中特定类型性别偏见的存在及其持续性，并构建了一个新的基准数据集RealWorldQuestioning（在HuggingFace发布），该数据集源自业务和健康情境下四个关键领域的实际问题：教育、就业、个人财务管理以及一般健康。我们定义并研究了熵偏见，即LLM在回应用户实际问题时生成信息量的差异。我们使用四种不同的LLM进行了测试，并通过使用ChatGPT-4o（作为“LLM作为评委”）进行了定性和定量评估。我们的分析（基于指标的对比和“LLM作为评委”评估）表明，LLM在类别层面对男性和女性的回答没有显著偏见。但在更细粒度的单个问题层面，多数情况下男性和女性的回答存在显著差异，这些差异往往相互抵消，因为一些回答对男性更有利，而另一些对女性更有利。尽管如此，这仍然是一个问题，因为这些工具的典型用户通常会针对某个特定问题进行提问，而不仅仅是这些常见且重要生活领域中的多个不同问题。我们提出了一种简单的去偏见方法，通过迭代合并两种性别的回答来生成最终结果。我们的方法证明了一种基于提示的简单去偏见策略能够有效去偏LLM输出，在78%的情况下生成的信息量比两种性别变体更高，并且在剩余情况下实现了平衡整合。', 'title_zh': 'LLMs是否存在性别（熵）偏见？'}
{'arxiv_id': 'arXiv:2505.20341', 'title': 'Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset', 'authors': 'Rui Liu, Pu Gao, Jiatian Xi, Berrak Sisman, Carlos Busso, Haizhou Li', 'link': 'https://arxiv.org/abs/2505.20341', 'abstract': "Text-based speech editing (TSE) modifies speech using only text, eliminating re-recording. However, existing TSE methods, mainly focus on the content accuracy and acoustic consistency of synthetic speech segments, and often overlook the emotional shifts or inconsistency issues introduced by text changes. To address this issue, we propose EmoCorrector, a novel post-correction scheme for TSE. EmoCorrector leverages Retrieval-Augmented Generation (RAG) by extracting the edited text's emotional features, retrieving speech samples with matching emotions, and synthesizing speech that aligns with the desired emotion while preserving the speaker's identity and quality. To support the training and evaluation of emotional consistency modeling in TSE, we pioneer the benchmarking Emotion Correction Dataset for TSE (ECD-TSE). The prominent aspect of ECD-TSE is its inclusion of $<$text, speech$>$ paired data featuring diverse text variations and a range of emotional expressions. Subjective and objective experiments and comprehensive analysis on ECD-TSE confirm that EmoCorrector significantly enhances the expression of intended emotion while addressing emotion inconsistency limitations in current TSE methods. Code and audio examples are available at this https URL.", 'abstract_zh': '基于文本的语音编辑中的情感修正：EmoCorrector方法', 'title_zh': '基于文本的语音编辑中情感一致性的追求： introduce EmoCorrector 和 ECD-TSE 数据集'}
{'arxiv_id': 'arXiv:2505.20340', 'title': 'Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models', 'authors': 'Yukun Zhang, Qi Dong', 'link': 'https://arxiv.org/abs/2505.20340', 'abstract': "We introduce Dynamic Manifold Evolution Theory (DMET),a unified framework that models large language model generation as a controlled dynamical system evolving on a low_dimensional semantic manifold. By casting latent_state updates as discrete time Euler approximations of continuous dynamics, we map intrinsic energy_driven flows and context_dependent forces onto Transformer components (residual connections, attention, feed-forward networks). Leveraging Lyapunov stability theory We define three empirical metrics (state continuity, clustering quality, topological persistence) that quantitatively link latent_trajectory properties to text fluency, grammaticality, and semantic coherence. Extensive experiments across decoding parameters validate DMET's predictions and yield principled guidelines for balancing creativity and consistency in text generation.", 'abstract_zh': '动态流形演化理论：一种统一框架，将大型语言模型生成视为在低维语义流形上演化的受控动态系统', 'title_zh': '动态流形演化理论：大规模语言模型中潜在表示的建模与稳定性分析'}
{'arxiv_id': 'arXiv:2505.20338', 'title': 'Assessing the Capability of LLMs in Solving POSCOMP Questions', 'authors': 'Cayo Viegas, Rohit Gheyi, Márcio Ribeiro', 'link': 'https://arxiv.org/abs/2505.20338', 'abstract': "Recent advancements in Large Language Models (LLMs) have significantly expanded the capabilities of artificial intelligence in natural language processing tasks. Despite this progress, their performance in specialized domains such as computer science remains relatively unexplored. Understanding the proficiency of LLMs in these domains is critical for evaluating their practical utility and guiding future developments. The POSCOMP, a prestigious Brazilian examination used for graduate admissions in computer science promoted by the Brazlian Computer Society (SBC), provides a challenging benchmark. This study investigates whether LLMs can match or surpass human performance on the POSCOMP exam. Four LLMs - ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and Le Chat Mistral Large - were initially evaluated on the 2022 and 2023 POSCOMP exams. The assessments measured the models' proficiency in handling complex questions typical of the exam. LLM performance was notably better on text-based questions than on image interpretation tasks. In the 2022 exam, ChatGPT-4 led with 57 correct answers out of 69 questions, followed by Gemini 1.0 Advanced (49), Le Chat Mistral (48), and Claude 3 Sonnet (44). Similar trends were observed in the 2023 exam. ChatGPT-4 achieved the highest performance, surpassing all students who took the POSCOMP 2023 exam. LLMs, particularly ChatGPT-4, show promise in text-based tasks on the POSCOMP exam, although image interpretation remains a challenge. Given the rapid evolution of LLMs, we expanded our analysis to include more recent models - o1, Gemini 2.5 Pro, Claude 3.7 Sonnet, and o3-mini-high - evaluated on the 2022-2024 POSCOMP exams. These newer models demonstrate further improvements and consistently surpass both the average and top-performing human participants across all three years.", 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）显著扩展了人工智能在自然语言处理任务中的能力。尽管取得了这些进展，它们在计算机科学等专门领域的表现仍然相对未被充分探索。理解LLMs在这些领域的专业能力对于评估它们的实际用途并指导未来的发展至关重要。巴西计算机协会（SBC）主办的 prestigio 贵宾计算机科学研究生入学考试（POSCOMP）提供了具有挑战性的基准。本研究调查LLMs是否能在POSCOMP考试中达到或超过人类的表现。最初，四种LLM——ChatGPT-4、Gemini 1.0 Advanced、Claude 3 Sonnet 和 Le Chat Mistral Large——在2022年和2023年的POSCOMP考试中进行了评估。评估衡量了模型处理考试中典型复杂问题的能力。LLM在文本相关问题上的表现明显优于图像解释任务。在2022年的考试中，ChatGPT-4以57道题中的57道正确问题领先，其次是Gemini 1.0 Advanced（49道）、Le Chat Mistral（48道）和Claude 3 Sonnet（44道）。在2023年的考试中也观察到了类似的趋势。ChatGPT-4取得了最高性能，超过了所有参加2023年POSCOMP考试的学生。特别是ChatGPT-4，在POSCOMP考试中的文本相关任务中显示出前景，尽管图像解释仍然是一个挑战。鉴于LLMs的快速进化，我们将分析扩展到包括更多最近的模型——o1、Gemini 2.5 Pro、Claude 3.7 Sonnet 和 o3-mini-high，这些模型在2022年至2024年的POSCOMP考试中进行了评估。这些较新的模型在所有三年中均显示出进一步的改进，并且始终超过平均表现和顶级人类参与者。', 'title_zh': '评估大型语言模型解答POSCOMP问题的能力'}
{'arxiv_id': 'arXiv:2505.20336', 'title': 'MOSLIM:Align with diverse preferences in prompts through reward classification', 'authors': 'Yu Zhang, Wanli Jiang, Zhengyu Yang', 'link': 'https://arxiv.org/abs/2505.20336', 'abstract': 'The multi-objective alignment of Large Language Models (LLMs) is essential for ensuring foundational models conform to diverse human preferences. Current research in this field typically involves either multiple policies or multiple reward models customized for various preferences, or the need to train a preference-specific supervised fine-tuning (SFT) model. In this work, we introduce a novel multi-objective alignment method, MOSLIM, which utilizes a single reward model and policy model to address diverse objectives. MOSLIM provides a flexible way to control these objectives through prompting and does not require preference training during SFT phase, allowing thousands of off-the-shelf models to be directly utilized within this training framework. MOSLIM leverages a multi-head reward model that classifies question-answer pairs instead of scoring them and then optimize policy model with a scalar reward derived from a mapping function that converts classification results from reward model into reward scores. We demonstrate the efficacy of our proposed method across several multi-objective benchmarks and conduct ablation studies on various reward model sizes and policy optimization methods. The MOSLIM method outperforms current multi-objective approaches in most results while requiring significantly fewer GPU computing resources compared with existing policy optimization methods.', 'abstract_zh': '大型语言模型的多目标对齐对于确保基础模型符合多样的人类偏好是必不可少的。当前该领域的研究通常涉及多个策略或针对不同偏好的定制奖励模型，或者在监督微调(SFT)阶段训练特定偏好的模型。本文提出了一种新颖的多目标对齐方法MOSLIM，该方法利用单一的奖励模型和策略模型来处理多样化的目标。MOSLIM提供了一种通过提示灵活控制这些目标的方式，并在SFT阶段无需训练偏好模型，从而使成千上万的现成模型可以直接在该训练框架中使用。MOSLIM利用多头奖励模型对问题-答案对进行分类而不是打分，并通过将奖励模型的分类结果转化为奖励分数的映射函数来以标量奖励优化策略模型。我们在多个多目标基准上展示了所提方法的有效性，并在不同奖励模型规模和策略优化方法上进行了消融研究。MOSLIM方法在大多数结果中优于现有的多目标方法，并且所需GPU计算资源显著较少。', 'title_zh': 'MOSLIM: 通过奖励分类实现多元偏好对齐'}
{'arxiv_id': 'arXiv:2505.20335', 'title': 'Language Model Distillation: A Temporal Difference Imitation Learning Perspective', 'authors': 'Zishun Yu, Shangzhe Li, Xinhua Zhang', 'link': 'https://arxiv.org/abs/2505.20335', 'abstract': 'Large language models have led to significant progress across many NLP tasks, although their massive sizes often incur substantial computational costs. Distillation has become a common practice to compress these large and highly capable models into smaller, more efficient ones. Many existing language model distillation methods can be viewed as behavior cloning from the perspective of imitation learning or inverse reinforcement learning. This viewpoint has inspired subsequent studies that leverage (inverse) reinforcement learning techniques, including variations of behavior cloning and temporal difference learning methods. Rather than proposing yet another specific temporal difference method, we introduce a general framework for temporal difference-based distillation by exploiting the distributional sparsity of the teacher model. Specifically, it is often observed that language models assign most probability mass to a small subset of tokens. Motivated by this observation, we design a temporal difference learning framework that operates on a reduced action space (a subset of vocabulary), and demonstrate how practical algorithms can be derived and the resulting performance improvements.', 'abstract_zh': '大规模语言模型在许多NLP任务中取得了显著进展，尽管其庞大的规模往往导致巨大的计算成本。知识蒸馏已成为将这些大型高效模型压缩为更小、更高效模型的常见做法。许多现有的语言模型蒸馏方法可以从模仿学习或逆增强学习的角度被视为行为克隆。这一观点启发了后续研究，这些研究利用了（逆）增强学习技术，包括行为克隆和时间差分学习方法的变体。我们没有提出另一种具体的时间差分方法，而是通过利用教师模型的分布稀疏性，引入了一种基于时间差分的蒸馏一般框架。具体来说，通常观察到语言模型将大部分概率质量分配给一小部分词汇。受这一观察的启发，我们设计了一种在减少的动作空间（词汇的一部分）上操作的时间差分学习框架，并展示了如何推导出实用的算法以及性能提升的结果。', 'title_zh': '语言模型精简：一种时间差分 imitation 学习视角'}
{'arxiv_id': 'arXiv:2505.20334', 'title': 'Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query', 'authors': 'Yixuan Wang, Shiyu Ji, Yijun Liu, Yuzhuang Xu, Yang Xu, Qingfu Zhu, Wanxiang Che', 'link': 'https://arxiv.org/abs/2505.20334', 'abstract': 'Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.', 'abstract_zh': '基于前瞻查询缓存（LAQ）的大语言模型解码加速方法', 'title_zh': '前瞻Q缓存：通过伪查询实现更一致的KV缓存淘汰'}
{'arxiv_id': 'arXiv:2505.20333', 'title': 'Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models', 'authors': 'Yukun Zhang, Qi Dong', 'link': 'https://arxiv.org/abs/2505.20333', 'abstract': 'Recent advances in Large Language Models (LLMs) have achieved strong performance, yet their internal reasoning remains opaque, limiting interpretability and trust in critical applications. We propose a novel Multi_Scale Manifold Alignment framework that decomposes the latent space into global, intermediate, and local semantic manifolds capturing themes, context, and word-level details. Our method introduces cross_scale mapping functions that jointly enforce geometric alignment (e.g., Procrustes analysis) and information preservation (via mutual information constraints like MINE or VIB). We further incorporate curvature regularization and hyperparameter tuning for stable optimization. Theoretical analysis shows that alignment error, measured by KL divergence, can be bounded under mild assumptions. This framework offers a unified explanation of how LLMs structure multi-scale semantics, advancing interpretability and enabling applications such as bias detection and robustness enhancement.', 'abstract_zh': 'Recent Advances in Large Language Models: A Multi_Scale Manifold Alignment Framework for Enhancing Interpretability and Trust', 'title_zh': '多尺度流形对齐：增强大型语言模型可解释性的统一框架'}
{'arxiv_id': 'arXiv:2505.20327', 'title': 'Data-driven multi-agent modelling of calcium interactions in cell culture: PINN vs Regularized Least-squares', 'authors': "Aurora Poggi, Giuseppe Alessio D'Inverno, Hjalmar Brismar, Ozan Öktem, Matthieu Barreau, Kateryna Morozovska", 'link': 'https://arxiv.org/abs/2505.20327', 'abstract': 'Data-driven discovery of dynamics in biological systems allows for better observation and characterization of processes, such as calcium signaling in cell culture. Recent advancements in techniques allow the exploration of previously unattainable insights of dynamical systems, such as the Sparse Identification of Non-Linear Dynamics (SINDy), overcoming the limitations of more classic methodologies. The latter requires some prior knowledge of an effective library of candidate terms, which is not realistic for a real case study. Using inspiration from fields like traffic density estimation and control theory, we propose a methodology for characterization and performance analysis of calcium delivery in a family of cells. In this work, we compare the performance of the Constrained Regularized Least-Squares Method (CRLSM) and Physics-Informed Neural Networks (PINN) for system identification and parameter discovery for governing ordinary differential equations (ODEs). The CRLSM achieves a fairly good parameter estimate and a good data fit when using the learned parameters in the Consensus problem. On the other hand, despite the initial hypothesis, PINNs fail to match the CRLSM performance and, under the current configuration, do not provide fair parameter estimation. However, we have only studied a limited number of PINN architectures, and it is expected that additional hyperparameter tuning, as well as uncertainty quantification, could significantly improve the performance in future works.', 'abstract_zh': '基于数据的动力学发现方法在生物学系统中的应用允许更准确地观察和表征过程，例如细胞培养中的钙信号传导。近期技术进步使得可以探索动态系统的以往难以获得的见解，如稀疏非线性动力学识别（SINDy）方法克服了经典方法的局限性。后者需要一些有效的候选项库先验知识，而在实际研究案例中这是不现实的。借鉴交通密度估计和控制理论的灵感，我们提出了一种用于细胞家族中钙传递表征和性能分析的方法学。在本研究中，我们将约束正则最小二乘法（CRLSM）和物理导向神经网络（PINN）用于系统识别和管理常微分方程（ODEs）的参数发现性能进行对比。CRLSM在使用所学参数解决共识问题时获得了相当不错的参数估计和数据拟合效果。相反，尽管最初假设PINNs能够达到CRLSM的性能，但在当前配置下，它们未能提供合理的参数估计。然而，我们仅研究了有限的PINN架构，预计在未来的工作中通过额外的超参数调优和不确定性量化可以显著提高其性能。', 'title_zh': '基于数据的细胞培养中钙离子相互作用多agent建模：比较PINN与正则化最小二乘方法'}
{'arxiv_id': 'arXiv:2505.20326', 'title': 'Cultural Awareness in Vision-Language Models: A Cross-Country Exploration', 'authors': 'Avinash Madasu, Vasudev Lal, Phillip Howard', 'link': 'https://arxiv.org/abs/2505.20326', 'abstract': 'Vision-Language Models (VLMs) are increasingly deployed in diverse cultural contexts, yet their internal biases remain poorly understood. In this work, we propose a novel framework to systematically evaluate how VLMs encode cultural differences and biases related to race, gender, and physical traits across countries. We introduce three retrieval-based tasks: (1) Race to Country retrieval, which examines the association between individuals from specific racial groups (East Asian, White, Middle Eastern, Latino, South Asian, and Black) and different countries; (2) Personal Traits to Country retrieval, where images are paired with trait-based prompts (e.g., Smart, Honest, Criminal, Violent) to investigate potential stereotypical associations; and (3) Physical Characteristics to Country retrieval, focusing on visual attributes like skinny, young, obese, and old to explore how physical appearances are culturally linked to nations. Our findings reveal persistent biases in VLMs, highlighting how visual representations may inadvertently reinforce societal stereotypes.', 'abstract_zh': '视觉语言模型（VLMs）在多种文化背景下日益广泛应用，但其内部偏见仍然知之甚少。本文提出了一种新型框架，系统评估VLMs在种族、性别和身体特征等方面对不同国家的文化差异和偏见的编码。我们介绍了三种检索任务：（1）种族到国家检索，探索特定种族群体（东亚人、白人、中东人、拉丁裔、南亚人、黑人）与不同国家之间的关联；（2）个性特征到国家检索，将图像与基于特质的提示（如聪明的、诚实的、犯罪的、暴力的）配对，以调查潜在的刻板印象关联；（3）身体特征到国家检索，重点关注身体外观属性（如瘦削、年轻、肥胖、年老），以探究身体外观如何与国家文化联系起来。我们的研究发现表明VLMs中存在持续的偏见，揭示了视觉表示如何无意中加强了社会刻板印象。', 'title_zh': 'Vision-Language模型中的文化awareness：跨国家探索'}
{'arxiv_id': 'arXiv:2505.20325', 'title': 'Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence', 'authors': 'Amirhosein Ghasemabadi, Keith G. Mills, Baochun Li, Di Niu', 'link': 'https://arxiv.org/abs/2505.20325', 'abstract': 'Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM) reasoning often incur substantial computational costs, primarily due to extensive reliance on external Process Reward Models (PRMs) or sampling methods like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient self-guided TTS framework that achieves PRM-level performance without costly external verifier models. Our method employs a lightweight tree search guided solely by intrinsic LLM signals, token-level confidence and step novelty. One critical innovation is improving the reliability of internal confidence estimates via a targeted reinforcement learning fine-tuning phase. Empirical evaluations on challenging mathematical reasoning benchmarks demonstrate that GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching or surpassing significantly larger models (e.g., 32B-70B parameters), while reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG achieves comparable accuracy with 8x faster inference speeds and 4-5x lower memory usage. Additionally, GG reduces KV cache memory usage by approximately 50% compared to the BoN strategy, facilitating more efficient and practical deployment of TTS techniques.', 'abstract_zh': 'Test-Time Scaling (TTS) 方法在增强大规模语言模型 (LLM) 推理方面通常会带来显著的计算成本，主要原因是过度依赖外部过程奖励模型 (PRMs) 或如 Best-of-N (BoN) 这样的采样方法。本文引入了由直觉引导 (Guided by Gut, GG) 的高效自引导 TTS 框架，无需昂贵的外部验证器模型即可达到与 PRM 相当的性能。我们的方法通过基于内在 LLM 信号、token 级别置信度和步骤新颖性的轻量级树搜索实现。一个关键创新是通过针对性的强化学习微调阶段提高内部置信度估计的可靠性。在具有挑战性的数学推理基准测试上的实验评估表明，GG 允许较小的模型（例如参数量 1.5B）达到或超越显著更大的模型（例如参数量 32B-70B）的准确率，同时内存使用量最多减少 10 倍。与基于 PRM 的方法相比，GG 在保持相当准确率的前提下实现了约 8 倍的推理速度和 4-5 倍的较低内存使用率。此外，GG 的 KV 缓存内存使用量相比 BoN 策略减少了约 50%，有助于 TTS 技术更高效和实际的部署。', 'title_zh': '跟随直觉：基于强化内在置信度的高效测试时缩放'}
{'arxiv_id': 'arXiv:2505.20324', 'title': 'Evaluating the Energy-Efficiency of the Code Generated by LLMs', 'authors': 'Md Arman Islam, Devi Varaprasad Jonnala, Ritika Rekhi, Pratik Pokharel, Siddharth Cilamkoti, Asif Imran, Tevfik Kosar, Bekir Turkkan', 'link': 'https://arxiv.org/abs/2505.20324', 'abstract': 'As the quality of code generated by Large Language Models (LLMs) improves, their adoption in the software industry for automated code generation continues to grow. Researchers primarily focus on enhancing the functional correctness of the generated code while commonly overlooking its energy efficiency and environmental impact. This paper investigates the energy efficiency of the code generated by 20 popular LLMs for 878 programming problems of varying difficulty levels and diverse algorithmic categories selected from the LeetCode platform by comparing them against canonical human-written solutions. Although LLMs can produce functionally correct results in most cases, our findings show that the performance and energy efficiency of LLM-produced solutions are often far below those of human-written solutions. Among the studied LLMs, DeepSeek-v3 and GPT-4o generate the most energy-efficient code, whereas Grok-2 and Gemini-1.5-Pro are among the least energy-efficient models. On average, human-generated canonical solutions are approximately 1.17 times more energy efficient than DeepSeek-v3, 1.21 times more energy efficient than GPT-4o, and over 2 times more energy efficient than Grok-2 and Gemini-1.5-Pro. For specific algorithmic groups such as dynamic programming, backtracking, and bit manipulation, LLM-generated code can consume up to 450 times more energy than human-generated canonical solutions.', 'abstract_zh': '随着大型语言模型（LLMs）生成代码质量的提高，它们在软件行业中的自动化代码生成应用持续增长。研究人员主要关注生成代码的功能正确性，而通常忽视其能效和环境影响。本文通过将20个流行的LLMs在LeetCode平台上878个不同难度和多样算法类别编程问题上生成的代码与经典的人工编写解决方案进行对比，研究了这些代码的能效。尽管LLMs在大多数情况下可以生成功能正确的结果，但我们的研究发现，LLMs生成的解决方案的性能和能效往往远低于人工编写解决方案。在研究的LLMs中，DeepSeek-v3和GPT-4o生成的代码最能效，而Grok-2和Gemini-1.5-Pro是最不能效的模型之一。平均而言，人工生成的经典解决方案比DeepSeek-v3能效高约1.17倍，比GPT-4o能效高约1.21倍，比Grok-2和Gemini-1.5-Pro能效高超过2倍。对于动态规划、回溯和位操作等特定算法类别，LLMs生成的代码的能耗可能比人工生成的经典解决方案高450倍。', 'title_zh': '评估由大型语言模型生成的代码的能效'}
{'arxiv_id': 'arXiv:2505.20323', 'title': 'PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus', 'authors': 'Shahriar Noroozizadeh, Sayantan Kumar, George H. Chen, Jeremy C. Weiss', 'link': 'https://arxiv.org/abs/2505.20323', 'abstract': 'Understanding temporal dynamics in clinical narratives is essential for modeling patient trajectories, yet large-scale temporally annotated resources remain limited. We present PMOA-TTS, the first openly available dataset of 124,699 PubMed Open Access (PMOA) case reports, each converted into structured (event, time) timelines via a scalable LLM-based pipeline. Our approach combines heuristic filtering with Llama 3.3 to identify single-patient case reports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1, resulting in over 5.6 million timestamped clinical events. To assess timeline quality, we evaluate against a clinician-curated reference set using three metrics: (i) event-level matching (80% match at a cosine similarity threshold of 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the Log-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide diagnostic and demographic coverage. In a downstream survival prediction task, embeddings from extracted timelines achieve time-dependent concordance indices up to 0.82 $\\pm$ 0.01, demonstrating the predictive value of temporally structured narratives. PMOA-TTS provides a scalable foundation for timeline extraction, temporal reasoning, and longitudinal modeling in biomedical NLP. The dataset is available at: this https URL .', 'abstract_zh': '理解临床叙述中的时间动态对于建模患者轨迹至关重要，但大规模的时间注释资源仍然有限。我们提出了PMOA-TTS，这是首个开放获取的数据集，包含124,699篇PubMed Open Access (PMOA) 案例报告，每个报告都通过一个可扩展的基于大规模语言模型的管道转换为结构化的时间线（事件，时间）。我们的方法结合启发式过滤与Llama 3.3来识别单个病例报告，随后使用Llama 3.3和DeepSeek R1进行提示驱动提取，结果共生成超过560万条带有时间戳的临床事件。为了评估时间线质量，我们使用三种指标与临床专家整理的参考集进行评估：（i）事件级匹配（余弦相似度阈值0.1时80%匹配），（ii）时间一致性（c指数>0.90），（iii）对数时间累积分布函数下的面积（AULTC）用于时间戳对齐。语料库级别分析显示广泛覆盖诊断和人口统计学特征。在下游生存预测任务中，从提取的时间线生成的嵌入物实现时间依赖一致性指数最高可达0.82 ± 0.01，展示了时间结构化叙述的预测价值。PMOA-TTS为时间线提取、时间推理和纵向建模提供了可扩展的基础。该数据集可从此链接获取：this https URL。', 'title_zh': 'PMOA-TTS: 引入PubMed开放获取文本时间序列语料库'}
{'arxiv_id': 'arXiv:2505.20322', 'title': 'Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms', 'authors': 'Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, Ningyu Zhang', 'link': 'https://arxiv.org/abs/2505.20322', 'abstract': 'Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.', 'abstract_zh': '精确控制语言模型生成对于确保安全性和可靠性至关重要。尽管通常使用提示工程和引导来干预模型行为，但由于模型中大量参数导致内部表示高度交织，这通常会限制控制精度并有时导致意外副作用。最近的研究探讨了使用稀疏自编码器（SAE）在高维空间中分离知识以进行引导的应用，但由于构成原子知识组件的非平凡定位问题，这些应用仅限于玩具任务。在本文中，我们提出了一种名为引导目标原子（STA）的新方法，该方法用于隔离和操纵分离的知识组件以增强安全性。全面的实验展示了我们方法的有效性。进一步的分析表明，引导在对抗场景中表现出更强的稳健性和灵活性。我们还将引导策略应用于大型推理模型，证实了其在精确推理控制中的有效性。', 'title_zh': '超越提示工程：通过引导目标原子实现LLMs的稳健行为控制'}
{'arxiv_id': 'arXiv:2505.20321', 'title': 'BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases', 'authors': 'Mathew J. Koretsky, Maya Willey, Adi Asija, Owen Bianchi, Chelsea X. Alvarado, Tanay Nayak, Nicole Kuznetsov, Sungwon Kim, Mike A. Nalls, Daniel Khashabi, Faraz Faghri', 'link': 'https://arxiv.org/abs/2505.20321', 'abstract': 'Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at this https URL, and our code is open-source at this https URL.', 'abstract_zh': '生物医学研究人员 increasingly 依赖大规模结构化数据库进行复杂的分析任务。然而，当前的文本到SQL系统在将定性的科学问题映射为可执行的SQL查询时经常遇到困难，特别是在需要隐含领域推理的情况下。我们引入了BiomedSQL，这是第一个明确设计用于评估在真实世界生物医学知识库上进行文本到SQL生成过程中科学推理能力的标准。BiomedSQL 包含 68,000 个问题/SQL 查询/答案三元组，这些三元组基于一个整合了基因-疾病关联、从组学数据中推断因果关系以及药物批准记录的统一 BigQuery 知识库。每个问题都需要模型推断领域特定的标准，如全基因组显著性阈值、效应方向或试验阶段筛选，而不仅仅依赖于语法翻译。我们评估了多种开源和闭源的大规模语言模型 (LLM) 以及不同提示策略和交互模式。我们的结果显示了显著的性能差距：GPT-o3-mini 的执行准确率为 59.0%，而我们自定义的多步代理 BMSQL 达到 62.6%，均远低于专家基准的 90.0%。BiomedSQL 为推进能够通过结构化生物医学知识库进行强大推理的支持科学发现的文本到SQL系统的进步提供了新的基础。我们的数据集可以通过以下链接公开访问，代码是开源的：this https URL。', 'title_zh': 'BiomedSQL: 文本到SQL在生物医学知识库上的科学推理'}
{'arxiv_id': 'arXiv:2505.20320', 'title': 'Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP', 'authors': 'Satya Narayana Cheetirala, Ganesh Raut, Dhavalkumar Patel, Fabio Sanatana, Robert Freeman, Matthew A Levin, Girish N. Nadkarni, Omar Dawkins, Reba Miller, Randolph M. Steinhagen, Eyal Klang, Prem Timsina', 'link': 'https://arxiv.org/abs/2505.20320', 'abstract': 'Long text classification is challenging for Large Language Models (LLMs) due to token limits and high computational costs. This study explores whether a Retrieval Augmented Generation (RAG) approach using only the most relevant text segments can match the performance of processing entire clinical notes with large context LLMs. We begin by splitting clinical documents into smaller chunks, converting them into vector embeddings, and storing these in a FAISS index. We then retrieve the top 4,000 words most pertinent to the classification query and feed these consolidated segments into an LLM. We evaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication identification task. Metrics such as AUC ROC, precision, recall, and F1 showed no statistically significant differences between the RAG based approach and whole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can significantly reduce token usage without sacrificing classification accuracy, providing a scalable and cost effective solution for analyzing lengthy clinical documents.', 'abstract_zh': 'Large Language Models (LLMs)中长文本分类具有挑战性，受限于Token限制和高计算成本。本研究探讨仅使用最相关文本片段的检索增强生成（RAG）方法是否能与使用大规模上下文LLM处理整个临床笔记的性能相匹配。', 'title_zh': 'Less Context, Same Performance: 一种资源高效的大语言模型驱动的临床NLP RAG框架'}
{'arxiv_id': 'arXiv:2505.20318', 'title': 'Beyond Demonstrations: Dynamic Vector Construction from Latent Representations', 'authors': 'Wang Cai, Hsiu-Yuan Huang, Zhixiang Wang, Yunfang Wu', 'link': 'https://arxiv.org/abs/2505.20318', 'abstract': 'In-Context derived Vector (ICV) methods extract task-relevant representations from large language models (LLMs) and reinject them during inference, achieving comparable performance to few-shot In-Context Learning (ICL) without repeated demonstration processing. However, existing ICV methods remain sensitive to ICL-specific factors, often use coarse or semantically fragmented representations as the source of the vector, and rely on heuristic-based injection positions, limiting their applicability.\nTo address these issues, we propose Dynamic Vector (DyVec), which incorporates an Exhaustive Query Rotation (EQR) strategy to extract robust semantically aggregated latent representations by mitigating variance introduced by ICL. It then applies Dynamic Latent Segmentation and Injection to adaptively partition representations based on task complexity and leverages REINFORCE-based optimization to learn optimal injection positions for each segment.\nExperiments results show that DyVec outperforms few-shot ICL, LoRA, and prior ICV baselines. Further analysis highlights the effectiveness of dynamically segmenting and injecting semantically aggregated latent representations. DyVec provides a lightweight and data-efficient solution for inference-time task adaptation.', 'abstract_zh': '基于上下文衍生向量(DyVec): 动态分割与注入的语义聚合潜表示方法', 'title_zh': '超越示例：从潜在表示构建动态向量'}
{'arxiv_id': 'arXiv:2505.20315', 'title': 'Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL', 'authors': 'Zhewei Yao, Guoheng Sun, Lukasz Borchmann, Zheyu Shen, Minghang Deng, Bohan Zhai, Hao Zhang, Ang Li, Yuxiong He', 'link': 'https://arxiv.org/abs/2505.20315', 'abstract': "Translating natural language into SQL (Test2SQL) is a longstanding challenge at the intersection of natural language understanding and structured data access. While large language models (LLMs) have significantly improved fluency in SQL generation, producing correct and executable SQL--particularly for complex queries--remains a bottleneck. We present Arctic-Text2SQL-R1, a reinforcement learning (RL) framework and model family designed to generate accurate, executable SQL using a lightweight reward signal based solely on execution correctness. Our approach avoids brittle intermediate supervision and complex reward shaping, promoting stable training and alignment with the end task. Combined with carefully curated data, strong supervised initialization, and effective training practices, Arctic-Text2SQL-R1 achieves state-of-the-art execution accuracy across six diverse Test2SQL benchmarks, including the top position on the BIRD leaderboard. Notably, our 7B model outperforms prior 70B-class systems, highlighting the framework's scalability and efficiency. We further demonstrate inference-time robustness through simple extensions like value retrieval and majority voting. Extensive experiments and ablation studies offer both positive and negative insights, providing practical guidance for future Test2SQL research.", 'abstract_zh': '将自然语言翻译成SQL（Test2SQL）是自然语言理解与结构化数据访问交叉领域的一个长期挑战。尽管大型语言模型（LLMs）在SQL生成的流畅性方面取得了显著进步，但生成准确且可执行的SQL，特别是对于复杂的查询，仍然是一个瓶颈。我们提出了Arctic-Text2SQL-R1，这是一种基于执行正确性轻量级奖励信号的强化学习（RL）框架和模型系列，旨在生成准确且可执行的SQL。我们的方法避免了脆弱的中间监督和复杂奖励塑造，促进训练稳定性和与最终任务的对齐。结合精心策划的数据、强大的监督初始化和有效的训练实践，Arctic-Text2SQL-R1 在六个不同的Test2SQL基准测试中实现了最先进的执行准确性，包括BIRD榜单上的顶级位置。值得注意的是，我们的7B模型优于之前的70B级系统，突显了该框架的可扩展性和效率。通过简单的扩展如值检索和多数投票，我们还展示了推理时的鲁棒性。详尽的实验和消融研究表明了正反两方面的见解，为未来的Test2SQL研究提供了实用指导。', 'title_zh': '北极Text2SQL-R1：简单的奖励，强大的推理'}
{'arxiv_id': 'arXiv:2505.20312', 'title': "Let's Get You Hired: A Job Seeker's Perspective on Multi-Agent Recruitment Systems for Explaining Hiring Decisions", 'authors': 'Aditya Bhattacharya, Katrien Verbert', 'link': 'https://arxiv.org/abs/2505.20312', 'abstract': 'During job recruitment, traditional applicant selection methods often lack transparency. Candidates are rarely given sufficient justifications for recruiting decisions, whether they are made manually by human recruiters or through the use of black-box Applicant Tracking Systems (ATS). To address this problem, our work introduces a multi-agent AI system that uses Large Language Models (LLMs) to guide job seekers during the recruitment process. Using an iterative user-centric design approach, we first conducted a two-phased exploratory study with four active job seekers to inform the design and development of the system. Subsequently, we conducted an in-depth, qualitative user study with 20 active job seekers through individual one-to-one interviews to evaluate the developed prototype. The results of our evaluation demonstrate that participants perceived our multi-agent recruitment system as significantly more actionable, trustworthy, and fair compared to traditional methods. Our study further helped us uncover in-depth insights into factors contributing to these perceived user experiences. Drawing from these insights, we offer broader design implications for building user-aligned, multi-agent explainable AI systems across diverse domains.', 'abstract_zh': '在求职招聘中，传统的求职者选拔方法往往缺乏透明度。无论是由人工招聘人员手动做出的决定，还是通过黑盒招聘跟踪系统（ATS）得出的决定，求职者 rarely 被充分告知招聘决策的理由。为解决这一问题，我们的工作引入了一个使用大型语言模型（LLMs）引导求职者参与招聘过程的多智能体AI系统。通过迭代以用户为中心的设计方法，我们首先进行了两阶段探索性研究，涉及四位活跃求职者，以指导系统的设计和开发。随后，我们通过个体一对一访谈的方式进行了深入的定性用户研究，评估了开发的原型。我们的评估结果显示，参与者认为我们提出的多智能体招聘系统在行动性、可信度和公平性方面显著优于传统方法。我们的研究进一步帮助我们深入探讨了导致这些用户体验的因素。基于这些见解，我们提出了在不同领域构建用户对齐的多智能体可解释AI系统的更广泛的设计启示。', 'title_zh': '让我们帮你入职：求职者视角下的多agents招聘系统及其解释招聘决策'}
{'arxiv_id': 'arXiv:2505.20308', 'title': 'Large Language Model-Powered Decision Support for a Metal Additive Manufacturing Knowledge Graph', 'authors': 'Muhammad Tayyab Khan, Lequn Chen, Wenhe Feng, Seung Ki Moon', 'link': 'https://arxiv.org/abs/2505.20308', 'abstract': 'Metal additive manufacturing (AM) involves complex interdependencies among processes, materials, feedstock, and post-processing steps. However, the underlying relationships and domain knowledge remain fragmented across literature and static databases that often demand expert-level queries, limiting their applicability in design and planning. To address these gaps, we develop a novel and queryable knowledge graph (KG) in Neo4j, encoding 53 distinct metals and alloys across seven material families, nine AM processes, four feedstock types, and associated post-processing requirements. A large language model (LLM) interface, guided by a few-shot prompting strategy, enables natural language querying without the need for formal query syntax. The system supports a range of tasks, including compatibility checks, multi-constraint filtering, and design for AM (DfAM) guidance. User natural language queries are normalized, translated into Cypher, and executed over the KG, with results reformatted into structured responses. This work presents the first real-time, interactive system that integrates a domain-specific metal AM KG with an LLM interface, offering accessible, explainable decision support for engineers and advancing human-centric tools in manufacturing intelligence.', 'abstract_zh': '金属增材制造（AM）涉及工艺、材料、原料和后处理步骤之间的复杂相互依赖关系。然而，这些底层关系和领域知识在文献和静态数据库中仍碎片化存在，往往需要专家级查询，限制了它们在设计和规划中的应用。为解决这些差距，我们开发了一个在Neo4j中的新型可查询知识图谱（KG），编码了七大家族中的53种金属和合金、九种增材制造工艺、四种原材料类型及相关的后处理要求。通过由少量示例指导的提示策略，大型语言模型（LLM）接口使自然语言查询无需正式查询语法即可实现。该系统支持多种任务，包括兼容性检查、多约束过滤和增材制造设计（DfAM）指导。用户自然语言查询被规范化，转换为Cypher并执行在KG上，结果被重新格式化为结构化响应。本工作首次介绍了将领域特定的金属AM知识图谱与LLM接口集成的实时交互系统，为工程师提供可访问的、可解释的决策支持，并促进了面向制程智能的人本工具的发展。', 'title_zh': '基于大型语言模型的决策支持金属增材制造知识图谱'}
{'arxiv_id': 'arXiv:2505.20303', 'title': 'Future of Code with Generative AI: Transparency and Safety in the Era of AI Generated Software', 'authors': 'David Hanson', 'link': 'https://arxiv.org/abs/2505.20303', 'abstract': 'As artificial intelligence becomes increasingly integrated into software development processes, the prevalence and sophistication of AI-generated code continue to expand rapidly. This study addresses the critical need for transparency and safety in AI generated code by examining the current landscape, identifying potential risks, and exploring future implications. We analyze market opportunities for detecting AI-generated code, discuss the challenges associated with managing increasing complexity, and propose solutions to enhance transparency and functionality analysis. Furthermore, this study investigates the longterm implications of AI generated code, including its potential role in the development of artificial general intelligence and its impact on human AI interaction. In conclusion, we emphasize the importance of proactive measures for ensuring the responsible development and deployment of AI in software engineering.', 'abstract_zh': '随着人工智能在软件开发过程中的不断集成，由人工智能生成的代码的普遍存在性和复杂性也在迅速扩展。本研究旨在通过分析当前状况、识别潜在风险并探讨未来 implications，来应对人工智能生成代码在透明度和安全性方面的重要需求。我们分析了检测人工智能生成代码的市场机遇，讨论了管理不断增长的复杂性的挑战，并提出了增强透明度和功能分析的解决方案。此外，本研究还探讨了人工智能生成代码的长期 implications，包括其在开发通用人工智能方面的作用及其对人类与人工智能互动的影响。最后，我们强调了采取积极措施以负责任地开发和部署人工智能的重要性。', 'title_zh': '代码的未来：生成式人工智能时代软件生成的透明性和安全性'}
{'arxiv_id': 'arXiv:2505.20302', 'title': 'VeriThoughts: Enabling Automated Verilog Code Generation using Reasoning and Formal Verification', 'authors': 'Patrick Yubeaton, Andre Nakkab, Weihua Xiao, Luca Collini, Ramesh Karri, Chinmay Hegde, Siddharth Garg', 'link': 'https://arxiv.org/abs/2505.20302', 'abstract': 'This paper introduces VeriThoughts, a novel dataset designed for reasoning-based Verilog code generation. We establish a new benchmark framework grounded in formal verification methods to evaluate the quality and correctness of generated hardware descriptions. Additionally, we present a suite of specialized small-scale models optimized specifically for Verilog generation. Our work addresses the growing need for automated hardware design tools that can produce verifiably correct implementations from high-level specifications, potentially accelerating the hardware development process while maintaining rigorous correctness guarantees. Our code and data are available at \\href{this https URL}{this URL}.', 'abstract_zh': '本文介绍了VeriThoughts，一个用于基于推理的Verilog代码生成的新数据集。我们建立了一个新的基准框架，基于形式验证方法来评估生成的硬件描述的质量和正确性。此外，我们还呈现了一套专门针对Verilog生成优化的小规模模型。我们的工作解决了从高层次规范自动生成可验证正确实现的日益增长需求，可能加速硬件开发过程同时保持严格的正确性保证。代码和数据可在<该网址>获得。', 'title_zh': 'VeriThoughts: 通过推理和形式验证实现自动化Verilog代码生成'}
{'arxiv_id': 'arXiv:2505.20299', 'title': 'MetamatBench: Integrating Heterogeneous Data, Computational Tools, and Visual Interface for Metamaterial Discovery', 'authors': 'Jianpeng Chen, Wangzhi Zhan, Haohui Wang, Zian Jia, Jingru Gan, Junkai Zhang, Jingyuan Qi, Tingwei Chen, Lifu Huang, Muhao Chen, Ling Li, Wei Wang, Dawei Zhou', 'link': 'https://arxiv.org/abs/2505.20299', 'abstract': 'Metamaterials, engineered materials with architected structures across multiple length scales, offer unprecedented and tunable mechanical properties that surpass those of conventional materials. However, leveraging advanced machine learning (ML) for metamaterial discovery is hindered by three fundamental challenges: (C1) Data Heterogeneity Challenge arises from heterogeneous data sources, heterogeneous composition scales, and heterogeneous structure categories; (C2) Model Complexity Challenge stems from the intricate geometric constraints of ML models, which complicate their adaptation to metamaterial structures; and (C3) Human-AI Collaboration Challenge comes from the "dual black-box\'\' nature of sophisticated ML models and the need for intuitive user interfaces. To tackle these challenges, we introduce a unified framework, named MetamatBench, that operates on three levels. (1) At the data level, we integrate and standardize 5 heterogeneous, multi-modal metamaterial datasets. (2) The ML level provides a comprehensive toolkit that adapts 17 state-of-the-art ML methods for metamaterial discovery. It also includes a comprehensive evaluation suite with 12 novel performance metrics with finite element-based assessments to ensure accurate and reliable model validation. (3) The user level features a visual-interactive interface that bridges the gap between complex ML techniques and non-ML researchers, advancing property prediction and inverse design of metamaterials for research and applications. MetamatBench offers a unified platform deployed at this http URL that enables machine learning researchers and practitioners to develop and evaluate new methodologies in metamaterial discovery. For accessibility and reproducibility, we open-source our benchmark and the codebase at this https URL.', 'abstract_zh': 'metamaterials设计中的先进机器学习统一框架MetamatBench：跨越多尺度架构的工程材料 offers前所未有的可调机械性能，超越了传统材料。然而，利用先进的机器学习（ML）进行metamaterials发现受到三项基本挑战的阻碍：(C1) 数据异质性挑战来自于异质数据源、异质组成尺度和异质结构类别；(C2) 模型复杂性挑战来自于ML模型的复杂几何约束，使其适应metamaterial结构复杂化；(C3) 人机合作挑战来自于复杂ML模型的“双重黑盒”性质和需要直观用户界面。为应对这些挑战，我们引入了一个统一框架MetamatBench，该框架在三个层级上运行。(1) 在数据层，我们整合并标准化了5个异质多模态metamaterial数据集。(2) 在ML层提供了全面的工具包，将17种最新的ML方法适应于metamaterials发现，并包括基于有限元评估的全面评估套件，含有12种新颖的性能指标，以确保模型验证的准确性和可靠性。(3) 在用户层，提供了一个可视化交互界面，弥合了复杂ML技术与非ML研究人员之间的差距，推动了metamaterials的属性预测和逆向设计，用于研究和应用。MetamatBench提供了一个统一平台，使得机器学习研究人员和实践者能够在此平台上开发和评估metamaterials发现的新方法。为了便于访问和再现，我们在此公开了基准和代码库。', 'title_zh': 'MetamatBench：集成异构数据、计算工具及可视化界面的Metamaterial发现平台'}
{'arxiv_id': 'arXiv:2505.18374', 'title': 'ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation', 'authors': 'Jarrod Ragsdale, Rajendra Boppana', 'link': 'https://arxiv.org/abs/2505.18374', 'abstract': 'Command-line interfaces (CLIs) provide structured textual environments for system administration. Explorations have been performed using pre-trained language models (PLMs) to simulate these environments for safe interaction in high-risk environments. However, their use has been constrained to frozen, large parameter models like GPT. For smaller architectures to reach a similar level of believability, a rich dataset of CLI interactions is required. Existing public datasets focus on mapping natural-language tasks to commands, omitting crucial execution data such as exit codes, outputs, and environmental side effects, limiting their usability for behavioral modeling. We introduce a Shell Input -Output Environment (ShIOEnv), which casts command construction as a Markov Decision Process whose state is the partially built sequence and whose actions append arguments. After each action, ShIOEnv executes the candidate and returns its exit status, output, and progress toward a minimal-length behavioral objective. Due to the intractable nature of the combinatorial argument state-action space, we derive a context-free grammar from man pages to mask invalid arguments from being emitted. We explore random and proximal-policy optimization (PPO)-optimized sampling of unrestricted and grammar-masked action spaces to produce four exploration strategies. We observed that grammar masking and PPO significantly improve sample efficiency to produce a higher quality dataset (maximizing the number of arguments while minimizing redundancies). Policy-generated datasets of shell input-output behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements in BLEU-4 when constraining the action space to grammar productions with an additional 26% improvement when applying PPO. The ShIOEnv environment and curated command behavior datasets are released for use in future research.', 'abstract_zh': '命令行接口（CLIs）为系统管理提供结构化的文本环境。已经使用预训练语言模型（PLMs）探索了这些环境的模拟，以实现高风险环境中的安全交互。然而，它们的使用局限于冻结的大参数模型如GPT。为了使较小的架构达到类似的可信程度，需要丰富的CLI交互数据集。现有的公共数据集专注于将自然语言任务映射到命令，而忽略了诸如退出代码、输出和环境副作用等关键执行数据，从而限制了它们在行为建模中的应用。我们引入了一个Shell输入-输出环境（ShIOEnv），它将命令构建视为一种部分构建序列的状态和追加参数的动作的马尔可夫决策过程。每次动作后，ShIOEnv执行候选操作并返回其退出状态、输出以及接近最小长度行为目标的进度。由于组合的参数状态-动作空间难以处理，我们从man页面中推导出一个上下文无关文法，以屏蔽无效参数的生成。我们探索了无约束和文法屏蔽动作空间的随机采样及近端策略优化（PPO）优化采样，产生了四种探索策略。我们观察到文法屏蔽和PPO显著提高了样本效率，生成了更高质量的数据集（最大化参数数量同时最小化冗余）。由策略生成的shell输入-输出行为配对数据集用于微调CodeT5，我们发现，当将动作空间限制为文法生成时，BLEU-4指标提高了85%，而在应用PPO时，这一改进又额外提高了26%。ShIOEnv环境及精心整理的命令行为数据集被释放用于未来的研究。', 'title_zh': 'ShIOEnv: 一种用于数据集整理的命令行为捕捉环境，支持语法引导的命令合成'}
{'arxiv_id': 'arXiv:2505.12703', 'title': 'SpatialLLM: From Multi-modality Data to Urban Spatial Intelligence', 'authors': 'Jiabin Chen, Haiping Wang, Jinpeng Li, Yuan Liu, Zhen Dong, Bisheng Yang', 'link': 'https://arxiv.org/abs/2505.12703', 'abstract': 'We propose SpatialLLM, a novel approach advancing spatial intelligence tasks in complex urban scenes. Unlike previous methods requiring geographic analysis tools or domain expertise, SpatialLLM is a unified language model directly addressing various spatial intelligence tasks without any training, fine-tuning, or expert intervention. The core of SpatialLLM lies in constructing detailed and structured scene descriptions from raw spatial data to prompt pre-trained LLMs for scene-based analysis. Extensive experiments show that, with our designs, pretrained LLMs can accurately perceive spatial distribution information and enable zero-shot execution of advanced spatial intelligence tasks, including urban planning, ecological analysis, traffic management, etc. We argue that multi-field knowledge, context length, and reasoning ability are key factors influencing LLM performances in urban analysis. We hope that SpatialLLM will provide a novel viable perspective for urban intelligent analysis and management. The code and dataset are available at this https URL.', 'abstract_zh': '我们提出SpatialLLM，一种在复杂城市场景中推进空间智能任务的新方法。与以往需要地理分析工具或专业领域知识的方法不同，SpatialLLM 是一个统一的语言模型，可以直接处理各种空间智能任务，无需任何训练、微调或专家干预。SpatialLLM 的核心在于从原始空间数据构建详细的结构化场景描述，以触发预训练的大规模语言模型进行基于场景的分析。广泛的实验结果显示，通过我们的设计，预训练的语言模型能够准确感知空间分布信息，并实现诸如城市规划、生态分析、交通管理等高级空间智能任务的零样本执行。我们认为，多领域知识、上下文长度和推理能力是影响大规模语言模型在城市分析中表现的关键因素。我们希望SpatialLLM能够为城市智能分析和管理提供一种新颖可行的视角。相关代码和数据集可在以下网址获取。', 'title_zh': 'SpatialLLM：从多模态数据到城市空间智能'}
{'arxiv_id': 'arXiv:2505.00039', 'title': 'Graph RAG for Legal Norms: A Hierarchical and Temporal Approach', 'authors': 'Hudson de Martim', 'link': 'https://arxiv.org/abs/2505.00039', 'abstract': 'This article proposes an adaptation of Graph Retrieval Augmented Generation (Graph RAG) specifically designed for the analysis and comprehension of legal norms, which are characterized by their predefined hierarchical structure, extensive network of internal and external references and multiple temporal versions. By combining structured knowledge graphs with contextually enriched text segments, Graph RAG offers a promising solution to address the inherent complexity and vast volume of legal data. The integration of hierarchical structure and temporal evolution into knowledge graphs - along with the concept of comprehensive Text Units - facilitates the construction of richer, interconnected representations of legal knowledge. Through a detailed analysis of Graph RAG and its application to legal norm datasets, this article aims to advance the field of Artificial Intelligence applied to Law, creating opportunities for more effective systems in legal research, legislative analysis, and decision support.', 'abstract_zh': '本文提出了一种针对法律规范分析与理解的Graph Retrieval Augmented Generation (Graph RAG) 的 adaptation，法律规范以其预定义的层次结构、广泛的内部和外部引用网络以及多个时间版本为特点。通过将结构化知识图与上下文丰富的文本片段相结合，Graph RAG 提供了一种有前景的解决方案，以应对法律数据本身固有的复杂性和庞大的数据量。将层次结构和时间演变整合到知识图中——以及全面的文本文本单元的概念——促进了更丰富、更互联的法律知识表示的构建。通过对Graph RAG 的详细分析及其在法律规范数据集上的应用，本文旨在推动人工智能在法律领域的应用前沿，为更有效的法律研究、立法分析和决策支持系统创造机会。', 'title_zh': '基于图形RAG的法律规范：一种分层和Temporal方法'}
{'arxiv_id': 'arXiv:2212.13462', 'title': 'MVTN: Learning Multi-View Transformations for 3D Understanding', 'authors': 'Abdullah Hamdi, Faisal AlZahrani, Silvio Giancola, Bernard Ghanem', 'link': 'https://arxiv.org/abs/2212.13462', 'abstract': 'Multi-view projection techniques have shown themselves to be highly effective in achieving top-performing results in the recognition of 3D shapes. These methods involve learning how to combine information from multiple view-points. However, the camera view-points from which these views are obtained are often fixed for all shapes. To overcome the static nature of current multi-view techniques, we propose learning these view-points. Specifically, we introduce the Multi-View Transformation Network (MVTN), which uses differentiable rendering to determine optimal view-points for 3D shape recognition. As a result, MVTN can be trained end-to-end with any multi-view network for 3D shape classification. We integrate MVTN into a novel adaptive multi-view pipeline that is capable of rendering both 3D meshes and point clouds. Our approach demonstrates state-of-the-art performance in 3D classification and shape retrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55). Further analysis indicates that our approach exhibits improved robustness to occlusion compared to other methods. We also investigate additional aspects of MVTN, such as 2D pretraining and its use for segmentation. To support further research in this area, we have released MVTorch, a PyTorch library for 3D understanding and generation using multi-view projections.', 'abstract_zh': '多视角投影技术在3D形状识别中表现出色，能够实现顶级性能。这些方法涉及学习如何从多个视角整合信息。然而，这些视角通常对于所有形状而言是固定的。为克服当前多视角技术的静态特性，我们提出学习这些视角。具体地，我们引入了多视角变换网络（MVTN），该网络利用可微渲染来确定3D形状识别的最优视角。因此，MVTN可以与任何多视角网络端到端地训练用于3D形状分类。我们将MVTN集成到一个新颖的自适应多视角管道中，该管道能够渲染3D网格和点云。我们的方法在多种基准测试（ModelNet40、ScanObjectNN、ShapeNet Core55）中的3D分类和形状检索中达到了最先进的性能。进一步的分析表明，与其它方法相比，我们的方法在遮挡鲁棒性方面有所提升。我们还研究了MVTN的其他方面，如2D预训练及其用于分割的应用。为了支持该领域的进一步研究，我们发布了MVTorch库，这是一个用于通过多视角投影进行3D理解和生成的PyTorch库。', 'title_zh': 'MVTN：学习多视图变换以进行三维理解'}
