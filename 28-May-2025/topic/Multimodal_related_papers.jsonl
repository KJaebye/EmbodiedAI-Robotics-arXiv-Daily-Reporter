{'arxiv_id': 'arXiv:2505.21495', 'title': 'CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception', 'authors': 'Pranav N. Thakkar, Shubhangi Sinha, Karan Baijal, Yuhan, Bian, Leah Lackey, Ben Dodson, Heisen Kong, Jueun Kwon, Amber Li, Yifei Hu, Alexios Rekoutis, Tom Silver, Tapomayukh Bhattacharjee', 'link': 'https://arxiv.org/abs/2505.21495', 'abstract': 'Robust robot manipulation in unstructured environments often requires understanding object properties that extend beyond geometry, such as material or compliance-properties that can be challenging to infer using vision alone. Multimodal haptic sensing provides a promising avenue for inferring such properties, yet progress has been constrained by the lack of large, diverse, and realistic haptic datasets. In this work, we introduce the CLAMP device, a low-cost (<\\$200) sensorized reacher-grabber designed to collect large-scale, in-the-wild multimodal haptic data from non-expert users in everyday settings. We deployed 16 CLAMP devices to 41 participants, resulting in the CLAMP dataset, the largest open-source multimodal haptic dataset to date, comprising 12.3 million datapoints across 5357 household objects. Using this dataset, we train a haptic encoder that can infer material and compliance object properties from multimodal haptic data. We leverage this encoder to create the CLAMP model, a visuo-haptic perception model for material recognition that generalizes to novel objects and three robot embodiments with minimal finetuning. We also demonstrate the effectiveness of our model in three real-world robot manipulation tasks: sorting recyclable and non-recyclable waste, retrieving objects from a cluttered bag, and distinguishing overripe from ripe bananas. Our results show that large-scale, in-the-wild haptic data collection can unlock new capabilities for generalizable robot manipulation. Website: this https URL', 'abstract_zh': '低成本多模态触觉传感器化抓取器及其在自然环境下大规模触觉数据采集中的应用：CLAMP数据集与模型', 'title_zh': 'CLAMP: 一种基于开源设备的大规模野外触觉数据集 crowdsourcing 方案以实现多模态机器人感知'}
{'arxiv_id': 'arXiv:2505.21282', 'title': 'EgoWalk: A Multimodal Dataset for Robot Navigation in the Wild', 'authors': 'Timur Akhtyamov, Mohamad Al Mdfaa, Javier Antonio Ramirez, Sergey Bakulin, German Devchich, Denis Fatykhov, Alexander Mazurov, Kristina Zipa, Malik Mohrat, Pavel Kolesnik, Ivan Sosin, Gonzalo Ferrer', 'link': 'https://arxiv.org/abs/2505.21282', 'abstract': 'Data-driven navigation algorithms are critically dependent on large-scale, high-quality real-world data collection for successful training and robust performance in realistic and uncontrolled conditions. To enhance the growing family of navigation-related real-world datasets, we introduce EgoWalk - a dataset of 50 hours of human navigation in a diverse set of indoor/outdoor, varied seasons, and location environments. Along with the raw and Imitation Learning-ready data, we introduce several pipelines to automatically create subsidiary datasets for other navigation-related tasks, namely natural language goal annotations and traversability segmentation masks. Diversity studies, use cases, and benchmarks for the proposed dataset are provided to demonstrate its practical applicability.\nWe openly release all data processing pipelines and the description of the hardware platform used for data collection to support future research and development in robot navigation systems.', 'abstract_zh': '数据驱动的导航算法高度依赖大规模、高质量的现实世界数据收集，以实现成功的训练和在真实且不可控条件下的稳健性能。为增强与导航相关的现实世界数据集家族，我们引入EgoWalk数据集——包含50小时的人类在多样室内/室外环境、不同季节中的导航数据。除了原始数据和可用于模仿学习的数据外，我们还引入了几种自动创建子数据集的管道，用于其他导航相关任务，如自然语言目标注释和通达性分割掩码。提供的多样性研究、应用场景和基准测试展示了该数据集的实际适用性。我们公开发布所有数据处理管道和数据收集所使用的硬件平台描述，以支持未来在机器人导航系统中的研究和开发。', 'title_zh': 'EgoWalk：适用于野外机器人导航的多模态数据集'}
{'arxiv_id': 'arXiv:2505.21043', 'title': 'Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction', 'authors': "Sam O'Connor Russell, Naomi Harte", 'link': 'https://arxiv.org/abs/2505.21043', 'abstract': 'Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs) facilitate naturalistic human-robot interaction, yet most rely solely on speech. We introduce MM-VAP, a multimodal PTTM which combines speech with visual cues including facial expression, head pose and gaze. We find that it outperforms the state-of-the-art audio-only in videoconferencing interactions (84% vs. 79% hold/shift prediction accuracy). Unlike prior work which aggregates all holds and shifts, we group by duration of silence between turns. This reveals that through the inclusion of visual features, MM-VAP outperforms a state-of-the-art audio-only turn-taking model across all durations of speaker transitions. We conduct a detailed ablation study, which reveals that facial expression features contribute the most to model performance. Thus, our working hypothesis is that when interlocutors can see one another, visual cues are vital for turn-taking and must therefore be included for accurate turn-taking prediction. We additionally validate the suitability of automatic speech alignment for PTTM training using telephone speech. This work represents the first comprehensive analysis of multimodal PTTMs. We discuss implications for future work and make all code publicly available.', 'abstract_zh': '多模态轮换预测模型MM-VAP在视频会议互动中的性能分析：超越单模态音频模型', 'title_zh': '视觉线索增强两人互动中的预测性轮流对话'}
{'arxiv_id': 'arXiv:2505.20740', 'title': 'MSEarth: A Benchmark for Multimodal Scientific Comprehension of Earth Science', 'authors': 'Xiangyu Zhao, Wanghan Xu, Bo Liu, Yuhao Zhou, Fenghua Ling, Ben Fei, Xiaoyu Yue, Lei Bai, Wenlong Zhang, Xiao-Ming Wu', 'link': 'https://arxiv.org/abs/2505.20740', 'abstract': 'The rapid advancement of multimodal large language models (MLLMs) has unlocked new opportunities to tackle complex scientific challenges. Despite this progress, their application in addressing earth science problems, especially at the graduate level, remains underexplored. A significant barrier is the absence of benchmarks that capture the depth and contextual complexity of geoscientific reasoning. Current benchmarks often rely on synthetic datasets or simplistic figure-caption pairs, which do not adequately reflect the intricate reasoning and domain-specific insights required for real-world scientific applications. To address these gaps, we introduce MSEarth, a multimodal scientific benchmark curated from high-quality, open-access scientific publications. MSEarth encompasses the five major spheres of Earth science: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere, featuring over 7K figures with refined captions. These captions are crafted from the original figure captions and enriched with discussions and reasoning from the papers, ensuring the benchmark captures the nuanced reasoning and knowledge-intensive content essential for advanced scientific tasks. MSEarth supports a variety of tasks, including scientific figure captioning, multiple choice questions, and open-ended reasoning challenges. By bridging the gap in graduate-level benchmarks, MSEarth provides a scalable and high-fidelity resource to enhance the development and evaluation of MLLMs in scientific reasoning. The benchmark is publicly available to foster further research and innovation in this field. Resources related to this benchmark can be found at this https URL and this https URL.', 'abstract_zh': '多模态大规模语言模型的迅速发展为应对复杂的科学挑战开启了新机遇。尽管取得了这些进展，它们在解决地球科学问题方面的应用，尤其是在研究生层次的应用，仍然缺乏探索。一个重要的障碍是缺乏能够捕捉地质科学推理深度和背景复杂性的基准。当前的基准数据集往往依赖于合成数据集或简单的图表标题对，这并不能充分反映实际科学应用所需的复杂推理和领域特定见解。为了弥补这些不足，我们引入了MSEarth，这是一个从高质量的开放获取科学出版物中精选出的多模态科学基准。MSEarth涵盖了地球科学的五大领域：大气层、冰雪层、水圈、岩石圈和生物圈，包含超过7000幅图并配有精炼的标题。这些标题从原始图表标题中提炼而来，并结合了论文中的讨论和推理，确保基准数据集捕捉到高级科学任务所需的关键推理和知识密集型内容。MSEarth支持多种任务，包括科学图表描述、多项选择题和开放性推理挑战。通过对研究生水平基准数据的填补，MSEarth提供了一个可扩展且高保真的资源，以增强多模态大型语言模型在科学推理方面的研发和评估。该基准数据集已公开发布，以促进该领域的进一步研究和创新。与此基准相关的资源可在以下网址获取：<https://> 和 <https://>。', 'title_zh': 'MSEarth: 一个地球科学多模态科学理解基准'}
{'arxiv_id': 'arXiv:2505.20306', 'title': 'Multi-Modal Artificial Intelligence of Embryo Grading and Pregnancy Prediction in Assisted Reproductive Technology: A Review', 'authors': 'Xueqiang Ouyang, Jia Wei', 'link': 'https://arxiv.org/abs/2505.20306', 'abstract': 'As a global disease, infertility has always affected human beings. The development of assisted reproductive technology can effectively solve this disease. However, the traditional in vitro fertilization-embryo transfer technology still faces many challenges in improving the success rate of pregnancy, such as the subjectivity of embryo grading and the inefficiency of integrating multi-modal data. Therefore, the introduction of artificial intelligence-based technologies is particularly crucial. This article reviews the application progress of multi-modal artificial intelligence in embryo grading and pregnancy prediction based on different data modalities (including static images, time-lapse videos and structured table data) from a new perspective, and discusses the main challenges in current research, such as the complexity of multi-modal information fusion and data scarcity.', 'abstract_zh': '基于多模态人工智能的胚胎分级与妊娠预测进展及挑战', 'title_zh': '辅助生殖技术中多模态人工智能在胚胎分级和妊娠预测中的应用：一个综述'}
{'arxiv_id': 'arXiv:2505.21497', 'title': 'Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers', 'authors': 'Wei Pang, Kevin Qinghong Lin, Xiangru Jian, Xi He, Philip Torr', 'link': 'https://arxiv.org/abs/2505.21497', 'abstract': "Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at this https URL.", 'abstract_zh': '学术海报生成是科学研究交流中一个关键但具有挑战性的任务，要求将长篇交织文档压缩至单页且视觉统一的页面。为应对这一挑战，我们首次引入了用于海报生成的标准基准及评价体系，该体系将近期的会议论文与作者设计的海报配对，并从(i)视觉质量-与人类海报的语义对齐、(ii)文本连贯性-语言流畅度、(iii)整体评估-由VLM作为评判的六项细致美学和信息标准评分，以及(iv)PaperQuiz-通过VLM回答生成的测验题来衡量海报传达论文核心内容的能力等方面进行评估。基于此基准，我们提出了PosterAgent，一个自上而下的、视觉循环的多智能体管道：(a) 解析器将论文提炼为结构化资产库；(b) 计划者将文本-视觉配对排列成二叉树布局，保持阅读顺序和空间平衡；以及(c) 画家-评论者循环通过执行渲染代码并利用VLM反馈精炼每个分区，消除溢出并确保对齐。在我们全面的评估中，我们发现GPT-4o的输出虽然初看视觉上吸引人，但往往表现出嘈杂的文本和较低的PaperQuiz评分，我们发现读者参与度是主要的美学瓶颈，因为人类设计的海报很大程度上依赖于视觉语义来传达意义。我们完全开源的变体（如基于Qwen-2.5系列）几乎在所有指标上优于现有的4o驱动的多智能体系统，同时使用87%更少的tokens。它将一篇22页的论文转化为一个可编辑的.pptx格式的最终海报，仅需0.005美元。这些发现为下一代全自动海报生成模型指明了明确的方向。编码和数据集可在此处访问。', 'title_zh': 'Paper2Poster: 向科学论文的多模态海报自动化转换'}
{'arxiv_id': 'arXiv:2505.21420', 'title': 'Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning', 'authors': 'Jinbao Wang, Hanzhe Liang, Can Gao, Chenxi Hu, Jie Zhou, Yunkang Cao, Linlin Shen, Weiming Shen', 'link': 'https://arxiv.org/abs/2505.21420', 'abstract': 'Multimodal feature reconstruction is a promising approach for 3D anomaly detection, leveraging the complementary information from dual modalities. We further advance this paradigm by utilizing multi-modal mentor learning, which fuses intermediate features to further distinguish normal from feature differences. To address these challenges, we propose a novel method called Mentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared features of different modalities, Mentor3AD can extract more effective features and guide feature reconstruction, ultimately improving detection performance. Specifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges features extracted from RGB and 3D modalities to create a mentor feature. Additionally, we have designed a Mentor of Guidance Module (MGM) to facilitate cross-modal reconstruction, supported by the mentor feature. Lastly, we introduce a Voting Module (VM) to more accurately generate the final anomaly score. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies have verified the effectiveness of the proposed method.', 'abstract_zh': '多模态特征重建是三维异常检测的一个有前景的方法，通过利用双模态互补信息。我们进一步通过多模态导师学习推进这一范式，该方法融合中间特征以更好地区分异常和特征差异。为了应对这些挑战，我们提出了一种名为Mentor3AD的新方法，该方法利用多模态导师学习。通过利用不同模态的共享特征，Mentor3AD可以提取更有效的特征并指导特征重建，最终提高检测性能。具体来说，Mentor3AD包括一个融合模块的导师模块（MFM），该模块将从RGB和3D模态中提取的特征合并以生成导师特征。此外，我们设计了一个指导模块的导师模块（MGM）以支持跨模态重建，由导师特征辅助。最后，我们引入了一个投票模块（VM）以更准确地生成最终的异常分数。在MVTec 3D-AD和Eyecandies上的广泛比较和消融研究证实了所提方法的有效性。', 'title_zh': 'Mentor3AD: 基于特征重构的多模态导师学习三维异常检测'}
{'arxiv_id': 'arXiv:2505.20759', 'title': 'PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding', 'authors': 'Ansel Blume, Jeonghwan Kim, Hyeonjeong Ha, Elen Chatikyan, Xiaomeng Jin, Khanh Duy Nguyen, Nanyun Peng, Kai-Wei Chang, Derek Hoiem, Heng Ji', 'link': 'https://arxiv.org/abs/2505.20759', 'abstract': "Real-world objects are composed of distinctive, object-specific parts. Identifying these parts is key to performing fine-grained, compositional reasoning-yet, large multimodal models (LMMs) struggle to perform this seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM benchmark designed for pixel-level part grounding. We construct PARTONOMY from existing part datasets and our own rigorously annotated set of images, encompassing 862 part labels and 534 object labels for evaluation. Unlike existing datasets that simply ask models to identify generic parts, PARTONOMY uses specialized concepts (e.g., agricultural airplane), and challenges models to compare objects' parts, consider part-whole relationships, and justify textual predictions with visual segmentations. Our experiments demonstrate significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only 5.9% gIoU), highlighting a critical gap in their part grounding abilities. We note that existing segmentation-enabled LMMs (segmenting LMMs) have two key architectural shortcomings: they use special [SEG] tokens not seen during pretraining which induce distribution shift, and they discard predicted segmentations instead of using past predictions to guide future ones. To address these deficiencies, we train several part-centric LMMs and propose PLUM, a novel segmenting LMM that uses span tagging instead of segmentation tokens and that conditions on prior predictions in a feedback loop. We find that pretrained PLUM outperforms existing segmenting LMMs on reasoning segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM finetuned on our proposed Explanatory Part Segmentation task is competitive with segmenting LMMs trained on significantly more segmentation data. Our work opens up new avenues towards enabling fine-grained, grounded visual understanding in LMMs.", 'abstract_zh': '真实世界的对象由独特的、对象特定的部分组成。识别这些部分是进行精细粒度、组合推理的关键——然而，大型多模态模型（LMMs）难以完成这一看似简单的任务。在本工作中，我们引入了PARTONOMY，一个旨在进行像素级部分定位的LMM基准。我们从现有部分数据集和我们自己严格标注的图像集中构建了PARTONOMY，其中包括862个部分标签和534个对象标签用于评估。与仅要求模型识别通用部分的现有数据集不同，PARTONOMY 使用专门的概念（例如，农业飞机），并挑战模型比较对象的部分、考虑部分与整体的关系，并用视觉分割来解释文本预测。我们的实验展示了最先进的LMM（例如，LISA-13B仅达到5.9%的gIoU）的重大局限性，突显出他们在部分定位能力上的关键差距。我们注意到，现有的基于分割的LMM（即分割LMMs）具有两个关键的架构缺陷：它们使用特殊 [SEG] 标记，在预训练期间未见过，这会导致分布偏移，并且它们直接丢弃预测的分割而不是使用过去的预测来指导未来的预测。为了弥补这些缺陷，我们训练了几种部分为中心的LMM，并提出了一种新颖的基于分割的LMM（PLUM），其使用区间标记而不是分割标记，并在一个反馈循环中依赖于先前的预测。我们发现，预训练的PLUM在推理分割、VQA和视觉错觉基准测试中优于现有的基于分割的LMM，此外，使用我们提出的解释性部分分割任务进行微调的PLUM，其性能与在大量分割数据上训练的分割LMM相当。我们的工作为使LMM实现精细粒度和基于视觉的理解开辟了新的途径。', 'title_zh': '部分解析：具有部分级别视觉理解的大规模多模态模型'}
{'arxiv_id': 'arXiv:2505.20753', 'title': 'Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models', 'authors': 'Yufei Zhan, Hongyin Zhao, Yousong Zhu, Shurong Zheng, Fan Yang, Ming Tang, Jinqiao Wang', 'link': 'https://arxiv.org/abs/2505.20753', 'abstract': 'Large Multimodal Models (LMMs) have recently demonstrated remarkable visual understanding performance on both vision-language and vision-centric tasks. However, they often fall short in integrating advanced, task-specific capabilities for compositional reasoning, which hinders their progress toward truly competent general vision models. To address this, we present a unified visual reasoning mechanism that enables LMMs to solve complicated compositional problems by leveraging their intrinsic capabilities (e.g. grounding and visual understanding capabilities). Different from the previous shortcut learning mechanism, our approach introduces a human-like understanding-thinking-answering process, allowing the model to complete all steps in a single pass forwarding without the need for multiple inferences or external tools. This design bridges the gap between foundational visual capabilities and general question answering, encouraging LMMs to generate faithful and traceable responses for complex visual reasoning. Meanwhile, we curate 334K visual instruction samples covering both general scenes and text-rich scenes and involving multiple foundational visual capabilities. Our trained model, Griffon-R, has the ability of end-to-end automatic understanding, self-thinking, and reasoning answers. Comprehensive experiments show that Griffon-R not only achieves advancing performance on complex visual reasoning benchmarks including VSR and CLEVR, but also enhances multimodal capabilities across various benchmarks like MMBench and ScienceQA. Data, models, and codes will be release at this https URL soon.', 'abstract_zh': '大型多模态模型通过利用其内在能力（如grounding和视觉理解能力）解决复杂的组合问题的统一视觉推理机制', 'title_zh': '理解、思考与作答：大型多模态模型促进视觉推理的发展'}
{'arxiv_id': 'arXiv:2505.20569', 'title': 'Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models', 'authors': 'Jihoon Lee, Min Song', 'link': 'https://arxiv.org/abs/2505.20569', 'abstract': 'Despite significant advancements in Large Vision-Language Models, Object Hallucination (OH) remains a persistent challenge. Building upon prior studies on contrastive decoding that address this issue without requiring additional model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an advanced method to suppress OH. RVCD leverages both negative and positive images at the logit level, explicitly referencing AI-generated images designed to represent a single concept. Our approach demonstrates substantial improvements over existing decoding-based methods.', 'abstract_zh': '尽管大型多模态模型取得了显著进展，物体幻视（OH）仍然是一个持续的挑战。基于先前对比解码的研究，我们在不需额外模型训练的情况下引入了RVCD（检索式视觉对比解码）方法，以抑制OH。RVCD 在logit级同时利用正负图像，明确引用旨在代表单一概念的AI生成图像。我们的方法在现有解码方法上展示了显著的改进。', 'title_zh': '基于检索的视觉对比解码以减轻大尺度视觉-语言模型中的对象幻觉'}
{'arxiv_id': 'arXiv:2505.20405', 'title': 'What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models', 'authors': 'Lorenzo Baraldi, Davide Bucciarelli, Federico Betti, Marcella Cornia, Lorenzo Baraldi, Nicu Sebe, Rita Cucchiara', 'link': 'https://arxiv.org/abs/2505.20405', 'abstract': 'Instruction-based image editing models offer increased personalization opportunities in generative tasks. However, properly evaluating their results is challenging, and most of the existing metrics lag in terms of alignment with human judgment and explainability. To tackle these issues, we introduce DICE (DIfference Coherence Estimator), a model designed to detect localized differences between the original and the edited image and to assess their relevance to the given modification request. DICE consists of two key components: a difference detector and a coherence estimator, both built on an autoregressive Multimodal Large Language Model (MLLM) and trained using a strategy that leverages self-supervision, distillation from inpainting networks, and full supervision. Through extensive experiments, we evaluate each stage of our pipeline, comparing different MLLMs within the proposed framework. We demonstrate that DICE effectively identifies coherent edits, effectively evaluating images generated by different editing models with a strong correlation with human judgment. We publicly release our source code, models, and data.', 'abstract_zh': '基于指令的图像编辑模型为生成任务提供了增高的个性化机会。然而，正确评估其结果具有挑战性，目前大多数现有指标在与人类判断的一致性和可解释性方面存在不足。为应对这些挑战，我们提出DICE（差异共轭估计器），一种用于检测原始图像与编辑图像之间局部差异并评估其与给定修改请求的相关性的模型。DICE由两个关键组件组成：一个差异检测器和一个共轭估计器，两者均基于自回归多模态大语言模型（MLLM）训练，并采用结合自监督、来自修复网络的知识蒸馏以及全方位监督的策略。通过广泛实验，我们评估了流水线的每个阶段，并在提议的框架内比较了不同MLLM的性能。我们证明DICE能够有效识别连贯的编辑，并且对于不同编辑模型生成的图像具有强烈的与人类判断相关性。我们公开 Release 我们的源代码、模型和数据。', 'title_zh': '什么改变了？ multimodal大型语言模型检测和评估指令引导的图像编辑'}
