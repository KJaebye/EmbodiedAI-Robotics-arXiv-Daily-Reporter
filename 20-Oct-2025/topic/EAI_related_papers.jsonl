{'arxiv_id': 'arXiv:2510.15786', 'title': 'DexCanvas: Bridging Human Demonstrations and Robot Learning for Dexterous Manipulation', 'authors': 'Xinyue Xu, Jieqiang Sun, Jing, Siyuan Chen, Lanjie Ma, Ke Sun, Bin Zhao, Jianbo Yuan, Yiwen Lu', 'link': 'https://arxiv.org/abs/2510.15786', 'abstract': 'We present DexCanvas, a large-scale hybrid real-synthetic human manipulation dataset containing 7,000 hours of dexterous hand-object interactions seeded from 70 hours of real human demonstrations, organized across 21 fundamental manipulation types based on the Cutkosky taxonomy. Each entry combines synchronized multi-view RGB-D, high-precision mocap with MANO hand parameters, and per-frame contact points with physically consistent force profiles. Our real-to-sim pipeline uses reinforcement learning to train policies that control an actuated MANO hand in physics simulation, reproducing human demonstrations while discovering the underlying contact forces that generate the observed object motion. DexCanvas is the first manipulation dataset to combine large-scale real demonstrations, systematic skill coverage based on established taxonomies, and physics-validated contact annotations. The dataset can facilitate research in robotic manipulation learning, contact-rich control, and skill transfer across different hand morphologies.', 'abstract_zh': 'DexCanvas：一种大规模混合真实合成人类操作数据集', 'title_zh': 'DexCanvas: 联通人类示范与机器人灵巧操作学习'}
{'arxiv_id': 'arXiv:2510.15679', 'title': 'HEADER: Hierarchical Robot Exploration via Attention-Based Deep Reinforcement Learning with Expert-Guided Reward', 'authors': 'Yuhong Cao, Yizhuo Wang, Jingsong Liang, Shuhao Liao, Yifeng Zhang, Peizhuo Li, Guillaume Sartoretti', 'link': 'https://arxiv.org/abs/2510.15679', 'abstract': 'This work pushes the boundaries of learning-based methods in autonomous robot exploration in terms of environmental scale and exploration efficiency. We present HEADER, an attention-based reinforcement learning approach with hierarchical graphs for efficient exploration in large-scale environments. HEADER follows existing conventional methods to construct hierarchical representations for the robot belief/map, but further designs a novel community-based algorithm to construct and update a global graph, which remains fully incremental, shape-adaptive, and operates with linear complexity. Building upon attention-based networks, our planner finely reasons about the nearby belief within the local range while coarsely leveraging distant information at the global scale, enabling next-best-viewpoint decisions that consider multi-scale spatial dependencies. Beyond novel map representation, we introduce a parameter-free privileged reward that significantly improves model performance and produces near-optimal exploration behaviors, by avoiding training objective bias caused by handcrafted reward shaping. In simulated challenging, large-scale exploration scenarios, HEADER demonstrates better scalability than most existing learning and non-learning methods, while achieving a significant improvement in exploration efficiency (up to 20%) over state-of-the-art baselines. We also deploy HEADER on hardware and validate it in complex, large-scale real-life scenarios, including a 300m*230m campus environment.', 'abstract_zh': '本工作在自主机器人探索的环境规模和探索效率方面推动了基于学习方法的边界。我们提出了HEADER，一种基于注意力的强化学习方法，结合分层图，以提高大型环境下的探索效率。 HEADER 在构建机器人信念/地图的分层表示方面遵循现有的传统方法，但进一步设计了一种基于社区的新颖算法来构建和更新全局图，该算法保持完全增量、形状自适应，并具有线性复杂度。基于基于注意力的网络，我们的规划者在局部范围内精细地考虑附近的信念，同时在全局范围内粗略地利用远端信息，从而实现多尺度空间依赖性的下一步最优视点决策。除了新颖的地图表示，我们引入了一种无需参数的特权奖励，该奖励显着提高了模型性能并产生接近最优的探索行为，通过避免由手工构建的奖励塑形造成的训练目标偏差。在模拟的具有挑战性、大型环境探索场景中，HEADER 在可扩展性方面优于大多数现有学习和非学习方法，同时相对于最先进的基线方法在探索效率上实现了显著改进（高达20%）。我们还在硬件上部署了HEADER，并在复杂的大型实际场景中进行了验证，包括一个300m×230m的校园环境。', 'title_zh': '基于注意力机制的深度强化学习与专家引导奖励的分层机器人探索'}
{'arxiv_id': 'arXiv:2510.15626', 'title': 'Adaptive Legged Locomotion via Online Learning for Model Predictive Control', 'authors': 'Hongyu Zhou, Xiaoyu Zhang, Vasileios Tzoumas', 'link': 'https://arxiv.org/abs/2510.15626', 'abstract': 'We provide an algorithm for adaptive legged locomotion via online learning and model predictive control. The algorithm is composed of two interacting modules: model predictive control (MPC) and online learning of residual dynamics. The residual dynamics can represent modeling errors and external disturbances. We are motivated by the future of autonomy where quadrupeds will autonomously perform complex tasks despite real-world unknown uncertainty, such as unknown payload and uneven terrains. The algorithm uses random Fourier features to approximate the residual dynamics in reproducing kernel Hilbert spaces. Then, it employs MPC based on the current learned model of the residual dynamics. The model is updated online in a self-supervised manner using least squares based on the data collected while controlling the quadruped. The algorithm enjoys sublinear \\textit{dynamic regret}, defined as the suboptimality against an optimal clairvoyant controller that knows how the residual dynamics. We validate our algorithm in Gazebo and MuJoCo simulations, where the quadruped aims to track reference trajectories. The Gazebo simulations include constant unknown external forces up to $12\\boldsymbol{g}$, where $\\boldsymbol{g}$ is the gravity vector, in flat terrain, slope terrain with $20\\degree$ inclination, and rough terrain with $0.25m$ height variation. The MuJoCo simulations include time-varying unknown disturbances with payload up to $8~kg$ and time-varying ground friction coefficients in flat terrain.', 'abstract_zh': '我们提供了一种基于在线学习和模型预测控制的自适应腿足运动算法。该算法由两部分相互作用的模块组成：模型预测控制（MPC）和残差动力学的在线学习。残差动力学可以表示建模误差和外部干扰。我们受到自主性未来的影响，在这个未来中，四足机器人即使在存在未知不确定性（如未知负载和不平地形）的现实世界中也能自主执行复杂任务。该算法使用随机傅里叶特征来在再生核希尔伯特空间中逼近残差动力学，然后基于当前学习得到的残差动力学模型采用模型预测控制。模型通过在控制四足机器人过程中收集的数据，在自我监督的方式下在线更新。该算法具有亚线性的动态后悔率，这是相对于已知残差动力学的最优预知控制器的次优性度量。我们在Gazebo和MuJoCo仿真中验证了该算法，其中四足机器人旨在跟踪参考轨迹。Gazebo仿真的外部未知力恒定，最大至$12\\boldsymbol{g}$，包含平坦地形、坡度为$20^\\circ$的斜坡地形和起伏地形（高度变化$0.25$米）。MuJoCo仿真的外部未知干扰随时间变化，负载最大至$8~kg$，平坦地形上的地面摩擦系数随时间变化。', 'title_zh': '基于在线学习的自适应腿式运动模型预测控制'}
{'arxiv_id': 'arXiv:2510.15446', 'title': 'VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving', 'authors': 'Ziang Guo, Zufeng Zhang', 'link': 'https://arxiv.org/abs/2510.15446', 'abstract': "In autonomous driving, dynamic environment and corner cases pose significant challenges to the robustness of ego vehicle's state understanding and decision making. We introduce VDRive, a novel pipeline for end-to-end autonomous driving that explicitly models state-action mapping to address these challenges, enabling interpretable and robust decision making. By leveraging the advancement of the state understanding of the Vision Language Action Model (VLA) with generative diffusion policy-based action head, our VDRive guides the driving contextually and geometrically. Contextually, VLA predicts future observations through token generation pre-training, where the observations are represented as discrete codes by a Conditional Vector Quantized Variational Autoencoder (CVQ-VAE). Geometrically, we perform reinforcement learning fine-tuning of the VLA to predict future trajectories and actions based on current driving conditions. VLA supplies the current state tokens and predicted state tokens for the action policy head to generate hierarchical actions and trajectories. During policy training, a learned critic evaluates the actions generated by the policy and provides gradient-based feedback, forming an actor-critic framework that enables a reinforcement-based policy learning pipeline. Experiments show that our VDRive achieves state-of-the-art performance in the Bench2Drive closed-loop benchmark and nuScenes open-loop planning.", 'abstract_zh': '自主驾驶中，动态环境和边缘案例对 ego 车辆状态理解和决策的稳健性构成了重大挑战。我们提出了 VDRive，这是一种全新的端到端自主驾驶管道，明确建模状态-动作映射以应对这些挑战，从而实现可解释和稳健的决策。通过利用 Vision Language Action 模型（VLA）状态理解的进步以及生成扩散政策取向头部，我们的 VDRive 在上下文和几何意义上引导驾驶行为。上下文中，VLA 通过标记生成预训练预测未来观察，其中观察由条件向量量化变分自编码器（CVQ-VAE）表示为离散代码。几何上，我们对 VLA 进行强化学习微调，使其能够在当前驾驶条件下预测未来轨迹和动作。VLA 为动作策略头部提供当前状态标记和预测状态标记以生成分层动作和轨迹。在策略训练过程中，一个学习到的批评家评估政策生成的动作并提供基于梯度的反馈，形成一种使强化学习策略学习管道得以实现的行为-批评家框架。实验结果表明，我们的 VDRive 在 Bench2Drive 闭环基准和 nuScenes 开环规划中均实现了最先进的性能。', 'title_zh': 'VDRive: 利用强化VLAD和扩散策略实现端到端自动驾驶'}
{'arxiv_id': 'arXiv:2510.15352', 'title': 'GaussGym: An open-source real-to-sim framework for learning locomotion from pixels', 'authors': 'Alejandro Escontrela, Justin Kerr, Arthur Allshire, Jonas Frey, Rocky Duan, Carmelo Sferrazza, Pieter Abbeel', 'link': 'https://arxiv.org/abs/2510.15352', 'abstract': 'We present a novel approach for photorealistic robot simulation that integrates 3D Gaussian Splatting as a drop-in renderer within vectorized physics simulators such as IsaacGym. This enables unprecedented speed -- exceeding 100,000 steps per second on consumer GPUs -- while maintaining high visual fidelity, which we showcase across diverse tasks. We additionally demonstrate its applicability in a sim-to-real robotics setting. Beyond depth-based sensing, our results highlight how rich visual semantics improve navigation and decision-making, such as avoiding undesirable regions. We further showcase the ease of incorporating thousands of environments from iPhone scans, large-scale scene datasets (e.g., GrandTour, ARKit), and outputs from generative video models like Veo, enabling rapid creation of realistic training worlds. This work bridges high-throughput simulation and high-fidelity perception, advancing scalable and generalizable robot learning. All code and data will be open-sourced for the community to build upon. Videos, code, and data available at this https URL.', 'abstract_zh': '我们提出了一种新的方法，用于在IsaacGym等向量物理模拟器中以3D正态斑点图作为插件渲染器进行 photorealistic 机器人模拟。这种方法实现了前所未有的速度——在消费级GPU上超过每秒100,000步——同时保持了高度的视觉保真度，我们通过各种任务展示了这一点。此外，我们还展示了其在sim-to-real机器人设置中的应用。除了基于深度的传感器之外，我们的结果强调了丰富的视觉语义如何提高导航和决策能力，例如避免不良区域。我们进一步展示了如何轻松地从iPhone扫描、大规模场景数据集（例如GrandTour、ARKit）以及生成视频模型（如Veo）中集成数千个环境，从而快速创建真实的训练世界。这项工作将高通量模拟与高保真感知相结合，推动了可扩展和通用的机器人学习。所有代码和数据都将开源，以便社区在此基础上进行建设。更多信息请参见此链接：[此httpsURL]。', 'title_zh': 'GaussGym: 一个开源的从像素学习行走的实时到模拟框架'}
{'arxiv_id': 'arXiv:2510.15331', 'title': 'ASBI: Leveraging Informative Real-World Data for Active Black-Box Simulator Tuning', 'authors': 'Gahee Kim, Takamitsu Matsubara', 'link': 'https://arxiv.org/abs/2510.15331', 'abstract': 'Black-box simulators are widely used in robotics, but optimizing their parameters remains challenging due to inaccessible likelihoods. Simulation-Based Inference (SBI) tackles this issue using simulation-driven approaches, estimating the posterior from offline real observations and forward simulations. However, in black-box scenarios, preparing observations that contain sufficient information for parameter estimation is difficult due to the unknown relationship between parameters and observations. In this work, we present Active Simulation-Based Inference (ASBI), a parameter estimation framework that uses robots to actively collect real-world online data to achieve accurate black-box simulator tuning. Our framework optimizes robot actions to collect informative observations by maximizing information gain, which is defined as the expected reduction in Shannon entropy between the posterior and the prior. While calculating information gain requires the likelihood, which is inaccessible in black-box simulators, our method solves this problem by leveraging Neural Posterior Estimation (NPE), which leverages a neural network to learn the posterior estimator. Three simulation experiments quantitatively verify that our method achieves accurate parameter estimation, with posteriors sharply concentrated around the true parameters. Moreover, we show a practical application using a real robot to estimate the simulation parameters of cubic particles corresponding to two real objects, beads and gravel, with a bucket pouring action.', 'abstract_zh': '黑箱模拟器的参数估计：主动模拟推理（ASBI）', 'title_zh': 'ASBI：利用信息丰富的实际数据进行活性黑盒模拟器调优'}
{'arxiv_id': 'arXiv:2510.15319', 'title': 'Traversability-aware Consistent Situational Graphs for Indoor Localization and Mapping', 'authors': 'Jeewon Kim, Minho Oh, Hyun Myung', 'link': 'https://arxiv.org/abs/2510.15319', 'abstract': 'Scene graphs enhance 3D mapping capabilities in robotics by understanding the relationships between different spatial elements, such as rooms and objects. Recent research extends scene graphs to hierarchical layers, adding and leveraging constraints across these levels. This approach is tightly integrated with pose-graph optimization, improving both localization and mapping accuracy simultaneously. However, when segmenting spatial characteristics, consistently recognizing rooms becomes challenging due to variations in viewpoints and limited field of view (FOV) of sensors. For example, existing real-time approaches often over-segment large rooms into smaller, non-functional spaces that are not useful for localization and mapping due to the time-dependent method. Conversely, their voxel-based room segmentation method often under-segment in complex cases like not fully enclosed 3D space that are non-traversable for ground robots or humans, leading to false constraints in pose-graph optimization. We propose a traversability-aware room segmentation method that considers the interaction between robots and surroundings, with consistent feasibility of traversability information. This enhances both the semantic coherence and computational efficiency of pose-graph optimization. Improved performance is demonstrated through the re-detection frequency of the same rooms in a dataset involving repeated traversals of the same space along the same path, as well as the optimization time consumption.', 'abstract_zh': '场景图增强机器人3D建图能力通过理解不同空间元素之间的关系，并将其扩展至层次结构，提高同时定位与建图的准确性和效率。然而，在分割空间特征时，一致地识别房间由于视角变化和传感器有限视场（FOV）而变得具有挑战性。为此，我们提出了一种考虑机器人与周围环境交互的可通行性感知房间分割方法，该方法确保了可通行性信息的持续一致性，从而提高姿态图优化的语义连贯性和计算效率。性能改进通过同一空间沿相同路径多次穿越的数据集中的相同房间检测频率和优化时间消耗得以验证。', 'title_zh': '可穿越性感知一致情境图用于室内定位与建图'}
{'arxiv_id': 'arXiv:2510.15220', 'title': 'LVI-Q: Robust LiDAR-Visual-Inertial-Kinematic Odometry for Quadruped Robots Using Tightly-Coupled and Efficient Alternating Optimization', 'authors': 'Kevin Christiansen Marsim, Minho Oh, Byeongho Yu, Seungjae Lee, I Made Aswin Nahrendra, Hyungtae Lim, Hyun Myung', 'link': 'https://arxiv.org/abs/2510.15220', 'abstract': 'Autonomous navigation for legged robots in complex and dynamic environments relies on robust simultaneous localization and mapping (SLAM) systems to accurately map surroundings and localize the robot, ensuring safe and efficient operation. While prior sensor fusion-based SLAM approaches have integrated various sensor modalities to improve their robustness, these algorithms are still susceptible to estimation drift in challenging environments due to their reliance on unsuitable fusion strategies. Therefore, we propose a robust LiDAR-visual-inertial-kinematic odometry system that integrates information from multiple sensors, such as a camera, LiDAR, inertial measurement unit (IMU), and joint encoders, for visual and LiDAR-based odometry estimation. Our system employs a fusion-based pose estimation approach that runs optimization-based visual-inertial-kinematic odometry (VIKO) and filter-based LiDAR-inertial-kinematic odometry (LIKO) based on measurement availability. In VIKO, we utilize the footpreintegration technique and robust LiDAR-visual depth consistency using superpixel clusters in a sliding window optimization. In LIKO, we incorporate foot kinematics and employ a point-toplane residual in an error-state iterative Kalman filter (ESIKF). Compared with other sensor fusion-based SLAM algorithms, our approach shows robust performance across public and longterm datasets.', 'abstract_zh': '复杂动态环境中腿式机器人自主导航依赖于 robust 同时定位与建图 (SLAM) 系统，以准确地映射环境并定位机器人，确保安全高效的运行。为此，我们提出了一种 robust LiDAR-视觉-惯性-运动学里程计系统，该系统结合了如相机、LiDAR、惯性测量单元（IMU）和关节编码器等多种传感器信息，用于基于视觉和LiDAR的里程计估计。我们的系统采用基于融合的姿态估计方法，根据不同传感器测量数据的可用性运行优化基于视觉-惯性-运动学里程计（VIKO）和基于滤波器的LiDAR-惯性-运动学里程计（LIKO）算法。在VIKO中，我们使用足部预积分技术和滑窗优化中的鲁棒LiDAR-视觉深度一致性（基于超像素簇）。在LIKO中，我们结合了足部运动学，并在错误状态迭代卡尔曼滤波器（ESIKF）中使用点到面残差。与其他基于传感器融合的SLAM算法相比，我们的方法在公共数据集和长期数据集上均表现出 robust 性能。', 'title_zh': 'LVI-Q： quadruped机器人基于紧密耦合和高效交替优化的鲁棒激光雷达-视觉-惯性-动力学里程计'}
