{'arxiv_id': 'arXiv:2507.15833', 'title': 'Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers', 'authors': 'Ian Chuang, Andrew Lee, Dechen Gao, Jinyu Zou, Iman Soltani', 'link': 'https://arxiv.org/abs/2507.15833', 'abstract': 'Human vision is a highly active process driven by gaze, which directs attention and fixation to task-relevant regions and dramatically reduces visual processing. In contrast, robot learning systems typically rely on passive, uniform processing of raw camera images. In this work, we explore how incorporating human-like active gaze into robotic policies can enhance both efficiency and performance. We build on recent advances in foveated image processing and apply them to an Active Vision robot system that emulates both human head movement and eye tracking. Extending prior work on the AV-ALOHA robot simulation platform, we introduce a framework for simultaneously collecting eye-tracking data and robot demonstrations from a human operator as well as a simulation benchmark and dataset for training robot policies that incorporate human gaze. Given the widespread use of Vision Transformers (ViTs) in robot learning, we integrate gaze information into ViTs using a foveated patch tokenization scheme inspired by recent work in image segmentation. Compared to uniform patch tokenization, this significantly reduces the number of tokens-and thus computation-without sacrificing visual fidelity near regions of interest. We also explore two approaches to gaze imitation and prediction from human data. The first is a two-stage model that predicts gaze to guide foveation and action; the second integrates gaze into the action space, allowing the policy to jointly predict gaze and actions end-to-end. Our results show that our method for foveated robot vision not only drastically reduces computational overhead, but also improves performance for high precision tasks and robustness to unseen distractors. Together, these findings suggest that human-inspired visual processing offers a useful inductive bias for robotic vision systems. this https URL', 'abstract_zh': '人类视觉是一个由注视驱动的高度活跃的过程，能够将注意力和注视引导至任务相关区域，大幅减少视觉处理需求。相比之下，机器人学习系统通常依赖于对原始摄像头图像进行被动、均匀的处理。在本工作中，我们探讨了将类人的主动注视融入机器人策略中如何提高效率和性能。我们基于最新的仿聚焦图像处理进展，将其应用于一个模拟人类头部运动和眼动追踪的Active Vision机器人系统。在此基础上，我们引入了一种框架，同时收集来自人类操作者的注视跟踪数据和机器人演示，并提供了一个用于训练包含人类注视的机器人策略的仿真基准和数据集。鉴于Vision Transformers（ViTs）在机器人学习中的广泛应用，我们通过借鉴图像分割领域的最新工作，利用仿聚焦的补丁分词方案将注视信息整合到ViTs中。与均匀的补丁分词方案相比，这种方法显著减少了令牌的数量——即降低了计算量——同时在感兴趣区域附近保持了视觉保真度。我们还探索了两种从人类数据中模仿和预测注视的方法。第一种是两阶段模型，用于预测注视以指导聚焦和动作；第二种则是将注视整合到动作空间中，使策略能够端到端地同时预测注视和动作。我们的结果显示，我们提出的仿聚焦机器人视觉方法不仅大幅减少了计算开销，还在高精度任务和对未知干扰物的鲁棒性方面表现出更好的性能。这些发现表明，基于人类视觉处理的方法为机器人视觉系统提供了有用的归纳偏置。', 'title_zh': '看、聚焦、行动：通过人类凝视和视网膜视觉转换器实现高效稳健的机器人学习'}
{'arxiv_id': 'arXiv:2507.15729', 'title': 'Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction', 'authors': 'Jens V. Rüppel, Andrey Rudenko, Tim Schreiter, Martin Magnusson, Achim J. Lilienthal', 'link': 'https://arxiv.org/abs/2507.15729', 'abstract': 'The rapid development of Large Language Models (LLMs) creates an exciting potential for flexible, general knowledge-driven Human-Robot Interaction (HRI) systems for assistive robots. Existing HRI systems demonstrate great progress in interpreting and following user instructions, action generation, and robot task solving. On the other hand, bi-directional, multi-modal, and context-aware support of the user in collaborative tasks still remains an open challenge. In this paper, we present a gaze- and speech-informed interface to the assistive robot, which is able to perceive the working environment from multiple vision inputs and support the dynamic user in their tasks. Our system is designed to be modular and transferable to adapt to diverse tasks and robots, and it is capable of real-time use of language-based interaction state representation and fast on board perception modules. Its development was supported by multiple public dissemination events, contributing important considerations for improved robustness and user experience. Furthermore, in two lab studies, we compare the performance and user ratings of our system with those of a traditional scripted HRI pipeline. Our findings indicate that an LLM-based approach enhances adaptability and marginally improves user engagement and task execution metrics but may produce redundant output, while a scripted pipeline is well suited for more straightforward tasks.', 'abstract_zh': '大型语言模型的迅猛发展为辅助机器人的人机交互（HRI）系统带来了灵活的知识驱动潜力。现有的HRI系统在解释和遵循用户指令、动作生成以及机器人任务解决方面取得了显著进展。另一方面，协作任务中双向、多模态和情境感知的用户支持仍是一个开放性挑战。在本文中，我们提出了一种基于凝视和言语的接口，该接口能够从多路视觉输入中感知工作环境并支持动态用户的任务。我们的系统模块化设计，能够适应多种任务和机器人，并能够实时使用基于语言的交互状态表示和快速在板感知模块。该系统的开发得到了多项公共传播活动的支持，为提高鲁棒性和用户体验提供了重要考虑。此外，在两个实验室研究中，我们将我们的系统性能和用户评级与传统脚本化HRI管道进行了比较。研究结果表明，基于LLM的方法提高了适应性，并且略微提升了用户参与度和任务执行指标，但可能会产生冗余输出；而脚本化管道则更适合简单任务。', 'title_zh': '支持凝视的双向人机交互大型语言模型框架'}
{'arxiv_id': 'arXiv:2507.15649', 'title': 'EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation', 'authors': 'Haocheng Xu, Haodong Zhang, Zhenghan Chen, Rong Xiong', 'link': 'https://arxiv.org/abs/2507.15649', 'abstract': "To support humanoid robots in performing manipulation tasks, it is essential to study stable standing while accommodating upper-body motions. However, the limited controllable range of humanoid robots in a standing position affects the stability of the entire body. Thus we introduce a reinforcement learning based framework for humanoid robots to imitate human upper-body motions while maintaining overall stability. Our approach begins with designing a retargeting network that generates a large-scale upper-body motion dataset for training the reinforcement learning (RL) policy, which enables the humanoid robot to track upper-body motion targets, employing domain randomization for enhanced robustness. To avoid exceeding the robot's execution capability and ensure safety and stability, we propose an Executable Motion Prior (EMP) module, which adjusts the input target movements based on the robot's current state. This adjustment improves standing stability while minimizing changes to motion amplitude. We evaluate our framework through simulation and real-world tests, demonstrating its practical applicability.", 'abstract_zh': '基于强化学习的人形机器人模仿上身运动的稳定站立框架', 'title_zh': 'EMP：可执行运动先验对人体形机器人站立上身运动模仿'}
{'arxiv_id': 'arXiv:2507.15493', 'title': 'GR-3 Technical Report', 'authors': 'Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, Hao Niu, Wenxuan Ou, Wanli Peng, Zeyu Ren, Haixin Shi, Jiawen Tian, Hongtao Wu, Xin Xiao, Yuyang Xiao, Jiafeng Xu, Yichu Yang', 'link': 'https://arxiv.org/abs/2507.15493', 'abstract': 'We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through a multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, a versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing a wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging tasks. We hope GR-3 can serve as a step towards building generalist robots capable of assisting humans in daily life.', 'abstract_zh': '我们报道了在构建通用机器人策略方面的最新进展，以及GR-3的发展。GR-3是一个大规模的视觉-语言-动作（VLA）模型。它展示了在处理新对象、新环境和涉及抽象概念的指令时的强大泛化能力。此外，它可以通过最少的人类轨迹数据高效微调，从而实现快速且低成本的新环境适应。GR-3在处理长时_horizon和灵巧的任务方面表现出色，包括需要双手操作和移动的任务，展示了稳健且可靠的性能。这些能力是通过包括与大规模网页视觉-语言数据协同训练、通过VR设备收集的人类轨迹数据高效微调以及基于机器人轨迹数据的有效模仿学习在内的多面训练配方实现的。此外，我们引入了ByteMini，这是一种多功能的双手移动机器人，设计具有出色的灵活性和可靠性，与GR-3集成后能够完成多种任务。通过广泛的实地实验，我们展示了GR-3在多种具有挑战性的任务中超越了最新的基准方法$\\pi_0$。我们希望GR-3能够成为构建能够帮助人类日常生活的通用机器人的一个步骤。', 'title_zh': 'GR-3 技术报告'}
{'arxiv_id': 'arXiv:2507.15474', 'title': 'All-UWB SLAM Using UWB Radar and UWB AOA', 'authors': 'Charith Premachandra, Achala Athukorala, U-Xuan Tan', 'link': 'https://arxiv.org/abs/2507.15474', 'abstract': 'There has been a growing interest in autonomous systems designed to operate in adverse conditions (e.g. smoke, dust), where the visible light spectrum fails. In this context, Ultra-wideband (UWB) radar is capable of penetrating through such challenging environmental conditions due to the lower frequency components within its broad bandwidth. Therefore, UWB radar has emerged as a potential sensing technology for Simultaneous Localization and Mapping (SLAM) in vision-denied environments where optical sensors (e.g. LiDAR, Camera) are prone to failure. Existing approaches involving UWB radar as the primary exteroceptive sensor generally extract features in the environment, which are later initialized as landmarks in a map. However, these methods are constrained by the number of distinguishable features in the environment. Hence, this paper proposes a novel method incorporating UWB Angle of Arrival (AOA) measurements into UWB radar-based SLAM systems to improve the accuracy and scalability of SLAM in feature-deficient environments. The AOA measurements are obtained using UWB anchor-tag units which are dynamically deployed by the robot in featureless areas during mapping of the environment. This paper thoroughly discusses prevailing constraints associated with UWB AOA measurement units and presents solutions to overcome them. Our experimental results show that integrating UWB AOA units with UWB radar enables SLAM in vision-denied feature-deficient environments.', 'abstract_zh': '超宽带雷达在视觉受限特征贫乏环境中的同时定位与地图构建', 'title_zh': '全频段UWB SLAM利用UWB雷达和UWBAOA'}
{'arxiv_id': 'arXiv:2507.15469', 'title': 'The Emergence of Deep Reinforcement Learning for Path Planning', 'authors': 'Thanh Thi Nguyen, Saeid Nahavandi, Imran Razzak, Dung Nguyen, Nhat Truong Pham, Quoc Viet Hung Nguyen', 'link': 'https://arxiv.org/abs/2507.15469', 'abstract': 'The increasing demand for autonomous systems in complex and dynamic environments has driven significant research into intelligent path planning methodologies. For decades, graph-based search algorithms, linear programming techniques, and evolutionary computation methods have served as foundational approaches in this domain. Recently, deep reinforcement learning (DRL) has emerged as a powerful method for enabling autonomous agents to learn optimal navigation strategies through interaction with their environments. This survey provides a comprehensive overview of traditional approaches as well as the recent advancements in DRL applied to path planning tasks, focusing on autonomous vehicles, drones, and robotic platforms. Key algorithms across both conventional and learning-based paradigms are categorized, with their innovations and practical implementations highlighted. This is followed by a thorough discussion of their respective strengths and limitations in terms of computational efficiency, scalability, adaptability, and robustness. The survey concludes by identifying key open challenges and outlining promising avenues for future research. Special attention is given to hybrid approaches that integrate DRL with classical planning techniques to leverage the benefits of both learning-based adaptability and deterministic reliability, offering promising directions for robust and resilient autonomous navigation.', 'abstract_zh': '复杂动态环境中自主系统需求的增加推动了智能路径规划方法的显著研究进展。在传统的图搜索算法、线性规划技术和进化计算方法的基础上，近年来基于深度强化学习（DRL）的方法已成为使自主代理通过与其环境的交互学习最优导航策略的强大工具。本文综述了传统方法以及在路径规划任务中应用DRL的最新进展，重点介绍自主车辆、无人机和机器人平台的应用。文章对传统和学习导向范式的关键算法进行了分类，并强调了它们的创新和实际应用。然后，本文详细讨论了它们在计算效率、可扩展性、适应性和鲁棒性方面的各自优势和局限性。最后，本文指出了关键的开放挑战，并概述了未来研究有希望的方向。特别关注将DRL与经典规划技术相结合的混合方法，以利用基于学习的适应性和确定性可靠性带来的优势，提出了实现稳健和韧性的自主导航的有希望方向。', 'title_zh': '深度强化学习在路径规划中的 emergence'}
{'arxiv_id': 'arXiv:2507.15266', 'title': 'VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving', 'authors': 'Haichao Liu, Haoren Guo, Pei Liu, Benshan Ma, Yuxiang Zhang, Jun Ma, Tong Heng Lee', 'link': 'https://arxiv.org/abs/2507.15266', 'abstract': 'Scene understanding and risk-aware attentions are crucial for human drivers to make safe and effective driving decisions. To imitate this cognitive ability in urban autonomous driving while ensuring the transparency and interpretability, we propose a vision-language model (VLM)-enhanced unified decision-making and motion control framework, named VLM-UDMC. This framework incorporates scene reasoning and risk-aware insights into an upper-level slow system, which dynamically reconfigures the optimal motion planning for the downstream fast system. The reconfiguration is based on real-time environmental changes, which are encoded through context-aware potential functions. More specifically, the upper-level slow system employs a two-step reasoning policy with Retrieval-Augmented Generation (RAG), leveraging foundation models to process multimodal inputs and retrieve contextual knowledge, thereby generating risk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM provides real-time trajectory predictions for heterogeneous traffic participants by extracting smoother trend representations for short-horizon trajectory prediction. The effectiveness of the proposed VLM-UDMC framework is verified via both simulations and real-world experiments with a full-size autonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively leverages scene understanding and attention decomposition for rational driving decisions, thus improving the overall urban driving performance. Our open-source project is available at this https URL.', 'abstract_zh': '场景理解与风险意识注意力对于人类驾驶员做出安全有效的驾驶决策至关重要。为了在确保透明性和可解释性的前提下，在城市自动驾驶中模仿这一认知能力，我们提出了一种基于视觉-语言模型（VLM）增强的统一决策和运动控制框架，命名为VLM-UDMC。该框架将场景推理和风险意识洞察整合到一个高层的慢速系统中，该系统能够根据实时环境变化动态重新配置下游快速系统的最优运动规划。重新配置基于上下文感知的潜在函数编码的实时环境变化。具体而言，高层慢速系统采用基于检索增强生成（RAG）的两步推理策略，利用基础模型处理多模态输入并检索上下文知识，从而生成风险意识洞察。同时，一种轻量级多核分解LSTM通过提取更平滑的趋势表示，提供了对不同交通参与者的实时轨迹预测。通过模拟和实际路况实验，使用全尺寸自动驾驶车辆验证了所提出的VLM-UDMC框架的有效性。实验表明，VLM-UDMC有效地利用了场景理解与注意力分解，从而提高了整体城市驾驶性能。我们的开源项目可在以下链接访问。', 'title_zh': 'VLM-UDMC：增强视觉语言模型的统一决策与运动控制城市自主驾驶'}
{'arxiv_id': 'arXiv:2507.14975', 'title': 'FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models', 'authors': 'Yufan Song, Jiatao Zhang, Zeng Gu, Qingmiao Liang, Tuocheng Hu, Wei Song, Shiqiang Zhu', 'link': 'https://arxiv.org/abs/2507.14975', 'abstract': 'Autonomous error correction is critical for domestic robots to achieve reliable execution of complex long-horizon tasks. Prior work has explored self-reflection in Large Language Models (LLMs) for task planning error correction; however, existing methods are constrained by inflexible self-reflection mechanisms that limit their effectiveness. Motivated by these limitations and inspired by human cognitive adaptation, we propose the Flexible Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture that enables LLMs to perform flexible self-reflection based on task difficulty, while constructively integrating historical valuable experience with failure lessons. We evaluated FCRF on diverse domestic tasks through simulation in AlfWorld and physical deployment in the real-world environment. Experimental results demonstrate that FCRF significantly improves overall performance and self-reflection flexibility in complex long-horizon robotic tasks.', 'abstract_zh': '自主错误纠正对于实现家庭机器人可靠执行长时间复杂任务至关重要。现有工作在大型语言模型（LLM）的任务规划错误纠正中探索了自我反思机制；然而，现有方法受限于刚性自我反思机制，限制了其有效性。受这些局限性和人类认知适应的启发，我们提出了灵活建构主义反思框架（FCRF），这是一种新颖的导师-行动者架构，使LLM能够根据任务难度进行灵活的自我反思，并构建性地整合历史有价值的经验与失败教训。我们通过在AlfWorld的仿真和真实环境中的物理部署对FCRF在多种家庭任务中的表现进行了评估。实验结果表明，FCRF显著提高了复杂长时间机器人任务的整体性能和自我反思灵活性。', 'title_zh': 'FCRF：灵活的建构主义反思在大型语言模型支持下的长期机器人任务规划'}
{'arxiv_id': 'arXiv:2507.14931', 'title': 'Designing Robots with, not for: A Co-Design Framework for Empowering Interactions in Forensic Psychiatry', 'authors': 'Qiaoqiao Ren, Remko Proesmans, Arend Pissens, Lara Dehandschutter, William Denecker, Lotte Rouckhout, Joke Carrette, Peter Vanhopplinus, Tony Belpaeme, Francis wyffels', 'link': 'https://arxiv.org/abs/2507.14931', 'abstract': "Forensic mental health care involves the treatment of individuals with severe mental disorders who have committed violent offences. These settings are often characterized by high levels of bureaucracy, risk avoidance, and restricted autonomy. Patients frequently experience a profound loss of control over their lives, leading to heightened psychological stress-sometimes resulting in isolation as a safety measure. In this study, we explore how co-design can be used to collaboratively develop a companion robot that helps monitor and regulate stress while maintaining tracking of the patients' interaction behaviours for long-term intervention. We conducted four co-design workshops in a forensic psychiatric clinic with patients, caregivers, and therapists. Our process began with the presentation of an initial speculative prototype to therapists, enabling reflection on shared concerns, ethical risks, and desirable features. This was followed by a creative ideation session with patients, a third workshop focused on defining desired functions and emotional responses, and we are planning a final prototype demo to gather direct patient feedback. Our findings emphasize the importance of empowering patients in the design process and adapting proposals based on their current emotional state. The goal was to empower the patient in the design process and ensure each patient's voice was heard.", 'abstract_zh': '司法精神病护理涉及对有严重精神障碍并实施暴力行为的个体进行治疗。这些环境往往特征明显，包括高水平的官僚主义、风险规避和自主权受限。患者常常经历生活控制权的重大丧失，导致心理压力增加，有时作为安全措施导致隔离。在此研究中，我们探讨了如何利用共设计方法协作开发一个伴侣机器人，该机器人有助于监测和调节压力，并同时跟踪患者互动行为，为长期干预提供支持。我们在一家司法精神病诊所中与患者、护理人员和治疗师进行了四次共设计研讨会。我们的过程始于向治疗师展示初步假想原型，以促进对共同关注点、伦理风险和可选特性的反思。随后是一个有患者参与的创造性构思阶段，第三次会议集中在定义期望的功能和情感反应上，我们计划最后进行原型演示以收集直接的患者反馈。我们的研究发现强调了在设计过程中赋予患者权力的重要性，并根据他们当前的情感状态调整提议。目标是让患者参与设计过程，并确保每位患者的声音都能被听到。', 'title_zh': '与人类共同设计机器人：赋能法医精神病学交互的共同设计框架'}
{'arxiv_id': 'arXiv:2507.14929', 'title': 'Digital twin and extended reality for teleoperation of the electric vehicle battery disassembly', 'authors': 'Tero Kaarlela, Sami Salo, Jose Outeiro', 'link': 'https://arxiv.org/abs/2507.14929', 'abstract': 'Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a sustainable transition to electric vehicles by enabling a closed-loop supply chain. Currently, the manual disassembly process exposes workers to hazards, including electrocution and toxic chemicals. We propose a teleoperated system for the safe disassembly and sorting of EVBs. A human-in-the-loop can create and save disassembly sequences for unknown EVB types, enabling future automation. An RGB camera aligns the physical and digital twins of the EVB, and the digital twin of the robot is based on the Robot Operating System (ROS) middleware. This hybrid approach combines teleoperation and automation to improve safety, adaptability, and efficiency in EVB disassembly and sorting. The economic contribution is realized by reducing labor dependency and increasing throughput in battery recycling. An online pilot study was set up to evaluate the usability of the presented approach, and the results demonstrate the potential as a user-friendly solution.', 'abstract_zh': '拆解和分类电动汽车电池（EVBs）通过支持可持续向电动汽车转型，促进了闭环供应链的发展。目前，手动拆解过程会令工人暴露在触电和有毒化学物质等风险中。我们提出了一种用于安全拆解和分类EVBs的远程操作系统。人类介入循环可以为未知EVB类型创建并保存拆解序列，以实现未来自动化。RGB摄像头对齐EVB的物理双胞胎和数字双胞胎，机器人的数字双胞胎基于机器人操作系统（ROS）中间件。这种混合方法结合了远程操作和自动化，以提高电动汽车电池拆解和分类的安全性、灵活性和效率。经济效益体现在减少劳动依赖并提高电池回收产量上。一个在线试点研究被设置起来评估所提出方法的可用性，结果表明该方法具有用户友好的潜力。', 'title_zh': '数字孪生与扩展现实 Electric Vehicle 电池拆解的远程操作'}
{'arxiv_id': 'arXiv:2507.14903', 'title': 'CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and Autonomous Vehicles with Multi-Policy Reinforcement Learning', 'authors': 'Pan Hu', 'link': 'https://arxiv.org/abs/2507.14903', 'abstract': 'Autonomous driving demands reliable and efficient solutions to closely related problems such as decision-making and motion planning. In this work, decision-making refers specifically to highway lane selection, while motion planning involves generating control commands (such as speed and steering) to reach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs), achieving both flexible and safe lane selection alongside precise trajectory execution remains a significant challenge. This paper proposes a framework called Cohesive Decision-Guided Motion Planning (CDGMP), which tightly integrates decision-making and motion planning using a Mixture of Experts (MoE) inspired architecture combined with multi-policy reinforcement learning. By coordinating multiple specialized sub-networks through a gating mechanism, the method decomposes the complex driving task into modular components. Each sub-network focuses on a specific aspect of driving, improving efficiency by activating only the most relevant modules during inference. This design also enhances safety through modular specialization. CDGMP improves the adaptability and robustness of CAVs across diverse traffic scenarios, offering a scalable solution to real-world autonomy challenges. The architectural principles behind CDGMP, especially the use of MoE, also provide a strong foundation for other high-dimensional decision and control tasks. Simulation results (available at this https URL) demonstrate reliable performance in both lane selection and motion planning.', 'abstract_zh': '自主驾驶需要可靠高效的解决方案来解决密切相关的决策和运动规划问题。在本工作中，决策特指高速公路车道选择，而运动规划涉及生成控制命令（如速度和转向）以达到所选车道。在连接自主车辆（CAVs）的背景下，实现灵活且安全的车道选择以及精确的轨迹执行仍然是一个重大挑战。本文提出了一种称为凝聚力决策引导运动规划（CDGMP）的框架，该框架通过受Mixture of Experts（MoE）启发的架构结合多策略强化学习紧密集成决策与运动规划。通过闸门机制协调多个专门的子网络，该方法将复杂的驾驶任务分解为模块化组件。每个子网络专注于驾驶的具体方面，通过在推理时仅激活最相关的模块提高效率。该设计还通过模块化专业化增强了安全性。CDGMP提高了CAVs在不同交通场景下的适应性和鲁棒性，提供了针对现实世界自主挑战的可扩展解决方案。CDGMP背后的架构原则，尤其是MoE的应用，也为其他高维决策和控制任务提供了坚实的基础。模拟结果（可参见 [此处](this https URL)）在车道选择和运动规划方面展示了可靠的性能。', 'title_zh': 'CoMoCAVs：多策略强化学习引导的连接与自主车辆协同决策运动规划'}
{'arxiv_id': 'arXiv:2507.14731', 'title': 'X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots', 'authors': 'Haitong Wang, Aaron Hao Tan, Angus Fung, Goldie Nejat', 'link': 'https://arxiv.org/abs/2507.14731', 'abstract': 'Existing navigation methods are primarily designed for specific robot embodiments, limiting their generalizability across diverse robot platforms. In this paper, we introduce X-Nav, a novel framework for end-to-end cross-embodiment navigation where a single unified policy can be deployed across various embodiments for both wheeled and quadrupedal robots. X-Nav consists of two learning stages: 1) multiple expert policies are trained using deep reinforcement learning with privileged observations on a wide range of randomly generated robot embodiments; and 2) a single general policy is distilled from the expert policies via navigation action chunking with transformer (Nav-ACT). The general policy directly maps visual and proprioceptive observations to low-level control commands, enabling generalization to novel robot embodiments. Simulated experiments demonstrated that X-Nav achieved zero-shot transfer to both unseen embodiments and photorealistic environments. A scalability study showed that the performance of X-Nav improves when trained with an increasing number of randomly generated embodiments. An ablation study confirmed the design choices of X-Nav. Furthermore, real-world experiments were conducted to validate the generalizability of X-Nav in real-world environments.', 'abstract_zh': '现有的导航方法主要针对特定的机器人实体设计，限制了其在不同机器人平台间的通用性。本文引入了X-Nav，一种端到端跨实体导航框架，能够在多种实体上统一部署，适用于轮式和四足机器人。X-Nav包括两个学习阶段：1) 使用深度强化学习和特权观测在大量随机生成的机器人实体上训练多个专家策略；2) 通过Transformer（Nav-ACT）的动作片段蒸馏出一个通用策略。通用策略直接将视觉和本体感受观测映射到低层控制命令，实现对新机器人实体的泛化。模拟实验表明，X-Nav实现了对未见实体和照片级真实环境的零样本迁移。规模研究显示，X-Nav的性能随着训练过程中随机生成的实体数量增加而提高。消融研究证实了X-Nav的设计选择。此外，还进行了实地实验以验证X-Nav在真实环境中的泛化能力。', 'title_zh': 'X-Nav: 学习端到端跨载体导航以供移动机器人使用'}
{'arxiv_id': 'arXiv:2507.14605', 'title': 'Koopman Operator Based Linear Model Predictive Control for 2D Quadruped Trotting, Bounding, and Gait Transition', 'authors': 'Chun-Ming Yang, Pranav A. Bhounsule', 'link': 'https://arxiv.org/abs/2507.14605', 'abstract': 'Online optimal control of quadrupedal robots would enable them to plan their movement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged as a practical approach for real-time control. In LMPC, an optimization problem with a quadratic cost and linear constraints is formulated over a finite horizon and solved on the fly. However, LMPC relies on linearizing the equations of motion (EOM), which may lead to poor solution quality. In this paper, we use Koopman operator theory and the Extended Dynamic Mode Decomposition (EDMD) to create a linear model of the system in high dimensional space, thus retaining the nonlinearity of the EOM. We model the aerial phase and ground contact phases using different linear models. Then, using LMPC, we demonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait transitions in level and rough terrains. The main novelty is the use of Koopman operator theory to create hybrid models of a quadrupedal system and demonstrate the online generation of multiple gaits and gaits transitions.', 'abstract_zh': '基于科胡曼算子理论的四足机器人在线最优控制及多步态生成与过渡', 'title_zh': '基于Koopman算子的线性模型预测控制在2D四足踏步、边界移动和步态转换中的应用'}
{'arxiv_id': 'arXiv:2507.14582', 'title': 'BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives', 'authors': 'Zezhi Liu, Shizhen Wu, Hanqian Luo, Deyun Qin, Yongchun Fang', 'link': 'https://arxiv.org/abs/2507.14582', 'abstract': 'In the field of Learning from Demonstration (LfD), enabling robots to generalize learned manipulation skills to novel scenarios for long-horizon tasks remains challenging. Specifically, it is still difficult for robots to adapt the learned skills to new environments with different task and motion requirements, especially in long-horizon, multi-stage scenarios with intricate constraints. This paper proposes a novel hierarchical framework, called BT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and Dynamical Movement Primitives (DMPs) to address this problem. Within this framework, Signal Temporal Logic (STL) is employed to formally specify complex, long-horizon task requirements and constraints. These STL specifications are systematically transformed to generate reactive and modular BTs for high-level decision-making task structure. An STL-constrained DMP optimization method is proposed to optimize the DMP forcing term, allowing the learned motion primitives to adapt flexibly while satisfying intricate spatiotemporal requirements and, crucially, preserving the essential dynamics learned from demonstrations. The framework is validated through simulations demonstrating generalization capabilities under various STL constraints and real-world experiments on several long-horizon robotic manipulation tasks. The results demonstrate that the proposed framework effectively bridges the symbolic-motion gap, enabling more reliable and generalizable autonomous manipulation for complex robotic tasks.', 'abstract_zh': '基于行为树、时序逻辑和动力学运动范例的novel分层框架：解决长时_horizon任务中新环境下的操作技能泛化问题', 'title_zh': 'BT-TL-DMPs: 一种结合行为树、时序逻辑和动态运动primitive的新型机器人任务规划框架'}
{'arxiv_id': 'arXiv:2507.14538', 'title': 'A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0', 'authors': 'Jin Chai, Xiang Yao, Mengfan Hou, Yanghong Li, Erbao Dong', 'link': 'https://arxiv.org/abs/2507.14538', 'abstract': 'CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid tendon-driven actuation system that combines shape memory alloys (SMAs) and DC motors. The hand employs high-strength fishing line as artificial tendons and uses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal and tendon-muscle structure of the human hand. A linear motor-driven module controls finger flexion, while an SMA-based module enables finger extension and lateral abduction. These modules are integrated into a compact hybrid actuation unit mounted on a custom rear support structure. Mechanical and kinematic experiments, conducted under an Arduino Mega 2560-based control system, validate the effectiveness of the design and demonstrate its biomimetic dexterity.', 'abstract_zh': 'CYJ Hand-0是一种具有21-DOF的类人灵巧手，配备了混合肌腱驱动传动系统，结合了形状记忆合金（SMAs）和直流电机。该手使用高强度钓鱼线作为人工肌腱，并采用全3D打印的AlSi10Mg金属框架，设计模仿人类手的骨骼和肌腱-肌肉结构。线性电机驱动模块控制手指弯曲，而基于SMAs的模块实现手指伸展和横向外展。这些模块集成在一个紧凑的混合驱动单元中，安装在自定义后支撑结构上。基于Arduino Mega 2560控制系统的机械和运动学实验验证了该设计的有效性，并展示了其生物仿生灵巧性。', 'title_zh': '一种由混合 SMA-电机驱动的21-DOF humanoid灵巧手：CYJ手-0'}
{'arxiv_id': 'arXiv:2507.14412', 'title': 'Personalized Socially Assistive Robots With End-to-End Speech-Language Models For Well-Being Support', 'authors': 'Mengxue Fu, Zhonghao Shi, Minyu Huang, Siqi Liu, Mina Kian, Yirui Song, Maja J. Matarić', 'link': 'https://arxiv.org/abs/2507.14412', 'abstract': "Socially assistive robots (SARs) have shown great potential for supplementing well-being support. However, prior studies have found that existing dialogue pipelines for SARs remain limited in real-time latency, back-channeling, and personalized speech dialogue. Toward addressing these limitations, we propose using integrated end-to-end speech-language models (SLMs) with SARs. This work 1) evaluated the usability of an SLM-enabled SAR dialogue system through a small user study, and 2) identified remaining limitations through study user feedback to inform future improvements. We conducted a small within-participant user study with university students (N = 11) whose results showed that participants perceived an SLM-enabled SAR system as capable of providing empathetic feedback, natural turn-taking, back-channeling, and adaptive responses. We also found that participants reported the robot's nonverbal behaviors as lacking variability and synchronization with conversation, and the SLM's verbal feedback as generic and repetitive. These findings highlighted the need for real-time robot movement synchronized with conversation, improved prompting or fine-tuning to generate outputs better aligned with mental health practices, and more expressive, adaptive vocal generation.", 'abstract_zh': '社会化辅助机器人（SARs）在补充福祉支持方面显示出巨大的潜力。然而，先前的研究发现，现有的SAR对话管道在实时延迟、回话响应和个性化语音对话方面仍存在局限性。为解决这些局限性，我们提出了将集成的端到端语音-语言模型（SLM）与SAR结合使用的方案。本研究1) 通过小型用户研究评估了具备SLM的SAR对话系统的可用性，并2) 通过用户反馈识别了剩余的局限性，以指导未来的改进。我们对11名大学生开展了小型被试内用户研究，结果显示参与者认为SLM增强的SAR系统能够提供共情反馈、自然的轮流对话、回话响应和适应性回应。我们还发现参与者报告说，机器人的非言语行为缺乏变化性和与对话的同步性，而SLM的言语反馈则显得通用且重复。这些发现强调了需要实现与对话同步的实时机器人动作、改进提示或微调以生成更符合心理健康实践的输出，以及更具表现力和适应性的语音生成的必要性。', 'title_zh': '面向福祉支持的端到端语音-语言模型驱动的个性化社会辅助机器人'}
{'arxiv_id': 'arXiv:2507.15597', 'title': 'Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos', 'authors': 'Hao Luo, Yicheng Feng, Wanpeng Zhang, Sipeng Zheng, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, Zongqing Lu', 'link': 'https://arxiv.org/abs/2507.15597', 'abstract': 'We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at this https URL.', 'abstract_zh': 'Being-H0：一种基于大规模人类视频训练的灵巧的视觉-语言-行动模型', 'title_zh': 'Being-H0: 大规模人类视频上的视觉-语言-行动预训练'}
{'arxiv_id': 'arXiv:2507.15106', 'title': 'From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward', 'authors': 'Xia Xu, Jochen Triesch', 'link': 'https://arxiv.org/abs/2507.15106', 'abstract': 'While human infants robustly discover their own causal efficacy, standard reinforcement learning agents remain brittle, as their reliance on correlation-based rewards fails in noisy, ecologically valid scenarios. To address this, we introduce the Causal Action Influence Score (CAIS), a novel intrinsic reward rooted in causal inference. CAIS quantifies an action\'s influence by measuring the 1-Wasserstein distance between the learned distribution of sensory outcomes conditional on that action, $p(h|a)$, and the baseline outcome distribution, $p(h)$. This divergence provides a robust reward that isolates the agent\'s causal impact from confounding environmental noise. We test our approach in a simulated infant-mobile environment where correlation-based perceptual rewards fail completely when the mobile is subjected to external forces. In stark contrast, CAIS enables the agent to filter this noise, identify its influence, and learn the correct policy. Furthermore, the high-quality predictive model learned for CAIS allows our agent, when augmented with a surprise signal, to successfully reproduce the "extinction burst" phenomenon. We conclude that explicitly inferring causality is a crucial mechanism for developing a robust sense of agency, offering a psychologically plausible framework for more adaptive autonomous systems.', 'abstract_zh': '基于因果推理的行动影响分数在增强学习中的应用：从婴儿移动环境学习因果影响', 'title_zh': '从踢腿到因果关系：模拟婴儿agency检测的 robust 内在奖励方法'}
{'arxiv_id': 'arXiv:2507.14850', 'title': 'Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems', 'authors': 'H. M. Sabbir Ahmad, Ehsan Sabouni, Alexander Wasilkoff, Param Budhraja, Zijian Guo, Songyuan Zhang, Chuchu Fan, Christos Cassandras, Wenchao Li', 'link': 'https://arxiv.org/abs/2507.14850', 'abstract': 'We address the problem of safe policy learning in multi-agent safety-critical autonomous systems. In such systems, it is necessary for each agent to meet the safety requirements at all times while also cooperating with other agents to accomplish the task. Toward this end, we propose a safe Hierarchical Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier Functions (CBFs). Our proposed hierarchical approach decomposes the overall reinforcement learning problem into two levels learning joint cooperative behavior at the higher level and learning safe individual behavior at the lower or agent level conditioned on the high-level policy. Specifically, we propose a skill-based HMARL-CBF algorithm in which the higher level problem involves learning a joint policy over the skills for all the agents and the lower-level problem involves learning policies to execute the skills safely with CBFs. We validate our approach on challenging environment scenarios whereby a large number of agents have to safely navigate through conflicting road networks. Compared with existing state of the art methods, our approach significantly improves the safety achieving near perfect (within 5%) success/safety rate while also improving performance across all the environments.', 'abstract_zh': '多代理安全 Critical 自动系统中的安全策略学习：基于控制障碍函数的分层多代理强化学习方法', 'title_zh': '基于控制障碍函数的分层多代理 reinforcement 学习在关键安全自主系统中的应用'}
{'arxiv_id': 'arXiv:2507.14456', 'title': 'GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving', 'authors': 'Chi Wan, Yixin Cui, Jiatong Du, Shuo Yang, Yulong Bai, Yanjun Huang', 'link': 'https://arxiv.org/abs/2507.14456', 'abstract': 'End-to-end autonomous driving requires adaptive and robust handling of complex and diverse traffic environments. However, prevalent single-mode planning methods attempt to learn an overall policy while struggling to acquire diversified driving skills to handle diverse scenarios. Therefore, this paper proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a Dual-aware Router. Specifically, the Global Expert is trained on the overall dataset, possessing robust performance. The Scene-Adaptive Experts are trained on corresponding scene subsets, achieving adaptive performance. The Dual-aware Router simultaneously considers scenario-level features and routing uncertainty to dynamically activate expert modules. Through the effective coupling of the Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router, GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS outperforms existing methods in the Bench2Drive closed-loop benchmark and achieves state-of-the-art performance in Driving Score and Success Rate, even with only monocular vision input. Furthermore, ablation studies demonstrate significant improvements over the original single-expert baseline: 7.67% in Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The code will be available at this https URL.', 'abstract_zh': '端到端自动驾驶需要适应和应对复杂多变的交通环境。然而，现有的单模式规划方法试图学习整体策略，但在获取多样的驾驶技能以应对各种场景方面存在困难。因此，本文提出GEMINUS，这是一种混合专家端到端自动驾驶框架，包含全局专家、场景自适应专家组和双意识路由器。具体而言，全局专家在整体数据集上进行训练，具备稳健的表现。场景自适应专家在相应的场景子集上进行训练，实现适应性表现。双意识路由器同时考虑场景级别的特征和路由不确定性，动态激活专家模块。通过全局专家和场景自适应专家组通过双意识路由器的有效耦合，GEMINUS在各种场景中实现了适应性和稳健性。GEMINUS在Bench2Drive闭环基准测试中优于现有方法，并在驾驶评分和成功率上达到最先进的性能，即使仅输入单目视觉信息。此外，消融研究显示相对于原始单一专家基线有显著改进：驾驶评分提高7.67%、成功率提高22.06%、多能力平均提高19.41%。代码将在此网址获取。', 'title_zh': 'GEMINUS: 具有双重意识的全局与场景自适应混合专家模型用于端到端自动驾驶'}
{'arxiv_id': 'arXiv:2507.15676', 'title': 'Agentic AI for autonomous anomaly management in complex systems', 'authors': 'Reza Vatankhah Barenji, Sina Khoshgoftar', 'link': 'https://arxiv.org/abs/2507.15676', 'abstract': 'This paper explores the potential of agentic AI in autonomously detecting and responding to anomalies within complex systems, emphasizing its ability to transform traditional, human-dependent anomaly management methods.', 'abstract_zh': '本文探讨了代理型人工智能在自主检测和应对复杂系统中的异常方面的潜力，强调了其-transform传统、依赖人力的异常管理方法的能力。', 'title_zh': '自主代理人工智能在复杂系统中的自主异常管理'}
{'arxiv_id': 'arXiv:2507.15618', 'title': 'TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II', 'authors': 'Weiyu Ma, Jiwen Jiang, Haobo Fu, Haifeng Zhang', 'link': 'https://arxiv.org/abs/2507.15618', 'abstract': 'We present an adapter-based approach for tactical conditioning of StarCraft II AI agents. Current agents, while powerful, lack the ability to adapt their strategies based on high-level tactical directives. Our method freezes a pre-trained policy network (DI-Star) and attaches lightweight adapter modules to each action head, conditioned on a tactical tensor that encodes strategic preferences. By training these adapters with KL divergence constraints, we ensure the policy maintains core competencies while exhibiting tactical variations. Experimental results show our approach successfully modulates agent behavior across tactical dimensions including aggression, expansion patterns, and technology preferences, while maintaining competitive performance. Our method enables flexible tactical control with minimal computational overhead, offering practical strategy customization for complex real-time strategy games.', 'abstract_zh': '基于适配器的方法用于StarCraft II AI代理的战术训练', 'title_zh': 'TacticCraft：基于自然语言的《星际争霸II》战术适应性方法'}
{'arxiv_id': 'arXiv:2507.15518', 'title': 'HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics', 'authors': 'Sizhou Chen, Shufan Jiang, Chi Zhang, Xiao-Lei Zhang, Xuelong Li', 'link': 'https://arxiv.org/abs/2507.15518', 'abstract': 'Creating an immersive and interactive theatrical experience is a long-term goal in the field of interactive narrative. The emergence of large language model (LLM) is providing a new path to achieve this goal. However, existing LLM-based drama generation methods often result in AI agents that lack initiative and cannot interact with the physical environment. Furthermore, these methods typically require detailed user input to drive the drama. These limitations reduce the interactivity and immersion of online real-time performance. To address the above challenges, we propose HAMLET, a multi-agent framework focused on drama creation and online performance. Given a simple topic, the framework generates a narrative blueprint, guiding the subsequent improvisational performance. During the online performance, each actor is given an autonomous mind. This means that actors can make independent decisions based on their own background, goals, and emotional state. In addition to conversations with other actors, their decisions can also change the state of scene props through actions such as opening a letter or picking up a weapon. The change is then broadcast to other related actors, updating what they know and care about, which in turn influences their next action. To evaluate the quality of drama performance, we designed an evaluation method to assess three primary aspects, including character performance, narrative quality, and interaction experience. The experimental evaluation shows that HAMLET can create expressive and coherent theatrical experiences. Our code, dataset and models are available at this https URL.', 'abstract_zh': '创建沉浸式和互动的剧场体验是交互叙事领域的一个长期目标。大型语言模型（LLM）的出现为实现这一目标提供了新的途径。然而，现有的基于LLM的戏剧生成方法往往导致缺乏自主性的AI角色，并不能与物理环境互动。此外，这些方法通常需要详细的用户输入来驱动戏剧发展。这些限制降低了在线实时表演的互动性和沉浸感。为了解决上述挑战，我们提出了一种名为HAMLET的多智能体框架，专注于戏剧创作和在线表演。给定一个简单的主题，该框架生成一个叙事蓝图，指导后续的即兴表演。在线表演期间，每个演员都拥有了自主心智。这意味着演员可以根据自身的背景、目标和情感状态做出独立的决定。除了与其他演员的对话外，他们的决定还可以通过开启信件或拿起武器等行动改变场景道具的状态。这种改变随后会广播给其他相关演员，更新他们所知道和关心的内容，进而影响他们的下一步行动。为了评估戏剧表演的质量，我们设计了一种评估方法，评估三个主要方面：角色表现、叙事质量和互动体验。实验评估显示，HAMLET能够创造富有表现力和连贯性的剧场体验。我们的代码、数据集和模型可在以下链接获取。', 'title_zh': 'HAMLET：基于代理的超适应模型化在实时 embodiment 戏剧中的应用'}
{'arxiv_id': 'arXiv:2507.15351', 'title': 'One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms', 'authors': 'Zijian Zhao, Sen Li', 'link': 'https://arxiv.org/abs/2507.15351', 'abstract': "On-demand ride-sharing platforms face the fundamental challenge of dynamically bundling passengers with diverse origins and destinations and matching them with vehicles in real time, all under significant uncertainty. Recently, MARL has emerged as a promising solution for this problem, leveraging decentralized learning to address the curse of dimensionality caused by the large number of agents in the ride-hailing market and the resulting expansive state and action spaces. However, conventional MARL-based ride-sharing approaches heavily rely on the accurate estimation of Q-values or V-values, which becomes problematic in large-scale, highly uncertain environments. Specifically, most of these approaches adopt an independent paradigm, exacerbating this issue, as each agent treats others as part of the environment, leading to unstable training and substantial estimation bias in value functions. To address these challenges, we propose two novel alternative methods that bypass value function estimation. First, we adapt GRPO to ride-sharing, replacing the PPO baseline with the group average reward to eliminate critic estimation errors and reduce training bias. Second, inspired by GRPO's full utilization of group reward information, we customize the PPO framework for ride-sharing platforms and show that, under a homogeneous fleet, the optimal policy can be trained using only one-step rewards - a method we term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior performance across most scenarios, efficiently optimizing pickup times and the number of served orders using simple MLP networks.", 'abstract_zh': '基于需求的拼车平台面临动态聚合具有不同起始地和目的地的乘客并在实时环境中与车辆匹配的 fundamental 挑战，所有这些都在显著的不确定性下进行。最近，MARL 已经成为解决该问题的一种有前途的解决方案，通过去中心化学习来应对打车市场中大量代理导致的状态和动作空间扩展性难题。然而，传统的基于MARL的拼车方法严重依赖于对Q值或V值的准确估计，在大规模、高度不确定的环境中，这一问题变得尤为突出。具体来说，大多数这些方法采用了独立的范式，加剧了这一问题，因为每个代理将其他代理视为环境的一部分，导致价值函数训练不稳定和大量估计偏差。为了解决这些挑战，我们提出了两种新的替代方法，绕过价值函数估计。首先，我们将GRPO适应到拼车领域，将PPO baseline替换为小组平均奖励，以消除评论员估计错误并降低训练偏差。其次，受到GRPO充分利用小组奖励信息的启发，我们针对拼车平台定制了PPO框架，并展示了在同质车队下，仅使用一步奖励即可训练出最优策略的方法（我们称之为一步策略优化OSPO）。实验结果表明，在真实的曼哈顿打车数据集上，GRPO和OSPO在大多数场景中均表现出优越性能，使用简单的MLP网络高效地优化了接客时间和服务订单数量。', 'title_zh': '一步足矣：基于一步策略优化的多Agent reinforcement学习在共享出行平台订单调度中的应用'}
{'arxiv_id': 'arXiv:2507.14897', 'title': 'AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents', 'authors': 'Renxi Wang, Rifo Ahmad Genadi, Bilal El Bouardi, Yongxin Wang, Fajri Koto, Zhengzhong Liu, Timothy Baldwin, Haonan Li', 'link': 'https://arxiv.org/abs/2507.14897', 'abstract': "Language model (LM) agents have gained significant attention for their ability to autonomously complete tasks through interactions with environments, tools, and APIs. LM agents are primarily built with prompt engineering or supervised finetuning. At the same time, reinforcement learning (RL) has been explored to enhance LM's capabilities, such as reasoning and factuality. However, the combination of the LM agents and reinforcement learning (Agent-RL) remains underexplored and lacks systematic study. To this end, we built AgentFly, a scalable and extensible Agent-RL framework designed to empower LM agents with a variety of RL algorithms. Our framework supports multi-turn interactions by adapting traditional RL methods with token-level masking. It features a decorator-based interface for defining tools and reward functions, enabling seamless extension and ease of use. To support high-throughput training, we implement asynchronous execution of tool calls and reward computations, and design a centralized resource management system for scalable environment coordination. We also provide a suite of prebuilt tools and environments, demonstrating the framework's effectiveness through successful agent training across multiple tasks.", 'abstract_zh': '语言模型（LM）代理通过与环境、工具和API交互自主完成任务的能力引起了广泛关注。LM代理主要通过提示工程或监督微调构建。同时，强化学习（RL）已被探索以增强LM的能力，如推理和事实性。然而，LM代理与强化学习（Agent-RL）的结合仍缺乏系统研究。为此，我们构建了AgentFly，一个可扩展且可扩展的Agent-RL框架，旨在为LM代理赋能多种RL算法。该框架通过在传统RL方法中采用 token 级遮蔽支持多轮交互，并通过基于装饰器的接口定义工具和奖励函数，实现无缝扩展和易用性。为支持高吞吐量训练，我们实现了工具调用和奖励计算的异步执行，并设计了集中式资源管理系统以实现可扩展的环境协调。我们还提供了一套预构建的工具和环境，通过多个任务中的成功代理训练展示了框架的有效性。', 'title_zh': 'AgentFly：可扩展且模块化的大型语言模型代理 reinforcement 学习方法'}
{'arxiv_id': 'arXiv:2507.14730', 'title': 'Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI', 'authors': 'Yanjie Fu', 'link': 'https://arxiv.org/abs/2507.14730', 'abstract': 'Generative AI, large language models, and agentic AI have emerged separately of urban planning. However, the convergence between AI and urban planning presents an interesting opportunity towards AI urban planners. This paper conceptualizes urban planning as a generative AI task, where AI synthesizes land-use configurations under geospatial, social, and human-centric constraints. We survey how generative AI approaches, including VAEs, GANs, transformers, and diffusion models, reshape urban design. We further identify critical gaps: 1) limited research on integrating urban theory guidance, 2) limited research of AI urban planning over multiple spatial resolutions or angularities, 3) limited research on augmenting urban design knowledge from data, and 4) limited research on addressing real-world interactions. To address these limitations, we outline future research directions in theory-guided generation, digital twins, and human-machine co-design, calling for a new synthesis of generative intelligence and participatory urbanism.', 'abstract_zh': '生成式AI、大规模语言模型和代理AI与城市规划领域相继出现。然而，AI与城市规划的结合为智能城市规划者提供了有趣的机会。本文将城市规划概念化为一个生成式AI任务，其中AI在地理空间、社会和以人为本的约束下综合土地利用配置。我们探讨了生成式AI方法（包括VAE、GAN、变压器和扩散模型）如何重塑城市设计。进一步识别出关键缺口：1）有限的城市理论指导研究，2）有限的多尺度或多视角的城市规划研究，3）有限的城市设计知识从数据中增强的研究，4）有限的现实世界交互解决研究。为了弥补这些不足，本文概述了理论指导生成、数字孪生和人机协作设计的研究方向，呼吁生成式智能与参与性城市主义的新合成。', 'title_zh': '面向生成式AI、大型语言模型和自主AI时代的AI城乡规划师'}
{'arxiv_id': 'arXiv:2507.14513', 'title': 'Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy', 'authors': 'Hongyi Yang, Yue Pan, Jiayi Xu, Kelsen Liu', 'link': 'https://arxiv.org/abs/2507.14513', 'abstract': 'Recent advances in large language models (LLMs) and autonomous agents have enabled systems capable of performing complex tasks across domains such as human-computer interaction, planning, and web navigation. However, many existing frameworks struggle in real-world or resource-constrained environments due to their reliance on cloud-based computation, limited robustness in dynamic contexts, and lack of persistent autonomy and environmental awareness.\nWe present Amico, a modular, event-driven framework for building autonomous agents optimized for embedded systems. Written in Rust for safety and performance, Amico supports reactive, persistent agents that operate efficiently across embedded platforms and browser environments via WebAssembly. It provides clean abstractions for event handling, state management, behavior execution, and integration with reasoning modules. Amico delivers a unified infrastructure for constructing resilient, interactive agents suitable for deployment in settings with limited compute and intermittent connectivity.', 'abstract_zh': 'Recent Advances in Large Language Models and Autonomous Agents and the Introduction of Amico：一个用于嵌入式系统的模块化事件驱动框架', 'title_zh': 'Amico: 一种用于持久化和嵌入式自主性的事件驱动模块化框架'}
{'arxiv_id': 'arXiv:2507.15587', 'title': 'Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario', 'authors': 'Yinsong Chen, Kaifeng Wang, Xiaoqiang Meng, Xueyuan Li, Zirui Li, Xin Gao', 'link': 'https://arxiv.org/abs/2507.15587', 'abstract': 'Current research on decision-making in safety-critical scenarios often relies on inefficient data-driven scenario generation or specific modeling approaches, which fail to capture corner cases in real-world contexts. To address this issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework, where background vehicles with interference capabilities are treated as red-team agents. Through active interference and exploration, red-team vehicles can uncover corner cases outside the data distribution. The framework uses a Constraint Graph Representation Markov Decision Process, ensuring that red-team vehicles comply with safety rules while continuously disrupting the autonomous vehicles (AVs). A policy threat zone model is constructed to quantify the threat posed by red-team vehicles to AVs, inducing more extreme actions to increase the danger level of the scenario. Experimental results show that the proposed framework significantly impacts AVs decision-making safety and generates various corner cases. This method also offers a novel direction for research in safety-critical scenarios.', 'abstract_zh': '当前针对安全关键场景的决策研究 often 依赖于低效的数据驱动场景生成或特定建模方法，无法捕获现实世界中的边缘案例。为了解决这一问题，我们提出了一种红队多智能体强化学习框架，其中具备干扰能力的背景车辆被视为红队代理。通过主动干扰和探索，红队车辆可以发现数据分布之外的边缘案例。该框架采用约束图表示马尔可夫决策过程，确保红队车辆遵守安全规则的同时持续干扰自动驾驶汽车（AV）。构建了一种策略威胁区域模型，以量化红队车辆对AV的威胁程度，诱导更极端的行动以提高情景的危险级别。实验结果表明，所提出框架显著影响了AV的决策安全性并生成了多种边缘案例。该方法还为安全关键场景的研究提供了新的方向。', 'title_zh': '红队多Agent强化学习在紧急制动场景中的应用'}
{'arxiv_id': 'arXiv:2507.15478', 'title': 'The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents', 'authors': 'Simon Kohaut, Felix Divo, Navid Hamid, Benedict Flade, Julian Eggert, Devendra Singh Dhami, Kristian Kersting', 'link': 'https://arxiv.org/abs/2507.15478', 'abstract': "Ensuring reliable and rule-compliant behavior of autonomous agents in uncertain environments remains a fundamental challenge in modern robotics. Our work shows how neuro-symbolic systems, which integrate probabilistic, symbolic white-box reasoning models with deep learning methods, offer a powerful solution to this challenge. This enables the simultaneous consideration of explicit rules and neural models trained on noisy data, combining the strength of structured reasoning with flexible representations. To this end, we introduce the Constitutional Controller (CoCo), a novel framework designed to enhance the safety and reliability of agents by reasoning over deep probabilistic logic programs representing constraints such as those found in shared traffic spaces. Furthermore, we propose the concept of self-doubt, implemented as a probability density conditioned on doubt features such as travel velocity, employed sensors, or health factors. In a real-world aerial mobility study, we demonstrate CoCo's advantages for intelligent autonomous systems to learn appropriate doubts and navigate complex and uncertain environments safely and compliantly.", 'abstract_zh': '确保自主代理在不确定环境中表现出可靠的且合规的行为仍然是现代机器人技术中的一个基础挑战。我们的工作展示了神经符号系统如何通过结合概率性和符号性的白盒推理模型与深度学习方法，提供解决这一挑战的有力方案。这使得同时考虑明确规则和基于嘈杂数据训练的神经模型成为可能，结合了结构化推理的强度与灵活表示的优势。为此，我们引入了宪法控制器（CoCo），这是一种新的框架，旨在通过推理深概率逻辑程序来增强代理的安全性和可靠性，这些程序代表了诸如共享交通空间中的约束。此外，我们提出了自我怀疑的概念，将其作为基于怀疑特征（如旅行速度、传感器状态或健康因素）的概率密度实现。在一项现实世界的城市空中移动研究中，我们展示了CoCo在智能自主系统中学习适当怀疑并安全、合规地导航复杂和不确定环境方面的优势。', 'title_zh': '宪法控制器：基于怀疑校准的合规代理引导'}
{'arxiv_id': 'arXiv:2507.15428', 'title': 'EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent', 'authors': 'Jiaao Li, Kaiyuan Li, Chen Gao, Yong Li, Xinlei Chen', 'link': 'https://arxiv.org/abs/2507.15428', 'abstract': "Egomotion videos are first-person recordings where the view changes continuously due to the agent's movement. As they serve as the primary visual input for embodied AI agents, making egomotion video reasoning more efficient is therefore essential for real-world deployment. Recent advances in vision-language models have enabled strong multimodal reasoning capabilities, but their computational cost remains prohibitive for long, redundant video inputs. Existing token pruning methods, typically designed for third-person videos, fail to leverage the spatiotemporal continuity and motion constraints inherent in egomotion settings. To address this, we propose EgoPrune, a training-free token pruning method tailored for egomotion video reasoning. EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR for temporally efficient sampling; Perspective-Aware Redundancy Filtering (PARF), which aligns visual tokens using perspective transformations and removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token selector that jointly considers visual-text relevance and intra-frame diversity. Experiments on two egomotion video benchmarks show that EgoPrune consistently outperforms prior training-free methods across various pruning ratios while significantly reducing FLOPs, memory usage, and latency. Moreover, we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB edge device, demonstrating its real-world efficiency and suitability for on-device egomotion video reasoning.", 'abstract_zh': '自运动视频是第一人称录制，由于主体移动导致视角持续变化。由于它们是实体人工智能代理的主要视觉输入，因此提高自运动视频推理的效率对于实际部署至关重要。近年来，视觉-语言模型的发展虽然使多模态推理能力显著增强，但其对计算成本的高要求仍然使长且冗余的视频输入难以应对。现有的基于标记修剪的方法通常是为第三人称视频设计的，无法充分利用自运动设置中固有的时空连续性和运动约束。为了解决这一问题，我们提出了一种基于训练的标记修剪方法EgoPrune，专门适用于自运动视频推理。EgoPrune包含三个组件：从EmbodiedR改编的时间高效的关键帧选择器；视角意识冗余过滤器（PARF），使用视角变换对齐视觉标记并去除冗余标记；以及基于最大边际相关性的标记选择器，该选择器综合考虑了视觉-文本的相关性和帧内多样性。在两个自运动视频基准上的实验表明，EgoPrune在各种修剪比例下始终优于先前的基于训练的修剪方法，同时显著减少了FLOPs、内存使用量和延迟。此外，我们将在配备Jetson Orin NX 16GB边缘设备的实体代理上部署EgoPrune，证明了其在实际场景中的高效性，并且适合用于设备上自运动视频推理。', 'title_zh': 'EgoPrune: 用于体态智能体自我运动视频推理的高效令牌剪枝'}
{'arxiv_id': 'arXiv:2507.15287', 'title': 'Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning', 'authors': 'Elias Malomgré, Pieter Simoens', 'link': 'https://arxiv.org/abs/2507.15287', 'abstract': "Recent trends in Reinforcement Learning (RL) highlight the need for agents to learn from reward-free interactions and alternative supervision signals, such as unlabeled or incomplete demonstrations, rather than relying solely on explicit reward maximization. Additionally, developing generalist agents that can adapt efficiently in real-world environments often requires leveraging these reward-free signals to guide learning and behavior. However, while intrinsic motivation techniques provide a means for agents to seek out novel or uncertain states in the absence of explicit rewards, they are often challenged by dense reward environments or the complexity of high-dimensional state and action spaces. Furthermore, most existing approaches rely directly on the unprocessed intrinsic reward signals, which can make it difficult to shape or control the agent's exploration effectively. We propose a framework that can effectively utilize expert demonstrations, even when they are incomplete and imperfect. By applying a mapping function to transform the similarity between an agent's state and expert data into a shaped intrinsic reward, our method allows for flexible and targeted exploration of expert-like behaviors. We employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors and accommodate missing information in demonstrations. Experiments show our approach enables robust exploration and strong performance in both sparse and dense reward environments, even when demonstrations are sparse or incomplete. This provides a practical framework for RL in realistic settings where optimal data is unavailable and precise reward control is needed.", 'abstract_zh': 'Recent趋势在强化学习（RL）中的新进展突显了需要让智能体从无奖励交互和替代监督信号（如未标记或不完整示范）中学习的重要性，而不仅仅是依赖显式奖励最大化的策略。此外，开发能够在现实环境中高效适应的通用智能体往往需要利用这些无奖励信号来指导学习和行为。然而，内部激励技术虽然提供了一种在缺乏显式奖励的情况下让智能体寻求新颖或不确定状态的方法，但在密集奖励环境或高维状态和动作空间的复杂性面前，它们经常受到挑战。此外，大多数现有方法直接依赖未处理的内部激励信号，这可能使有效塑造或控制智能体的探索变得困难。我们提出了一种框架，即使专家示范不完整且不完美，也能有效利用这些示范。通过应用变换函数将智能体状态与专家数据的相似度转换为塑造的内部奖励，我们的方法允许灵活且有针对性地探索专家级行为。我们采用专家编码器混合模型来捕捉行为多样性并适应示范中的缺失信息。实验表明，我们的方法能够在稀疏和密集奖励环境中实现稳健的探索和强大的性能，即使示范稀疏或不完整。这为在最优数据不可用且需要精确奖励控制的现实场景中应用RL提供了实用框架。', 'title_zh': '混合自动编码器专家指导在未标记和不完整数据下探索的强化学习中应用'}
{'arxiv_id': 'arXiv:2507.14904', 'title': 'TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP', 'authors': 'Fan Li, Zanyi Wang, Zeyi Huang, Guang Dai, Jingdong Wang, Mengmeng Wang', 'link': 'https://arxiv.org/abs/2507.14904', 'abstract': '3D visual grounding allows an embodied agent to understand visual information in real-world 3D environments based on human instructions, which is crucial for embodied intelligence. Existing 3D visual grounding methods typically rely on separate encoders for different modalities (e.g., RGB images, text, and 3D point clouds), resulting in large and complex models that are inefficient to train. While some approaches use pre-trained 2D multi-modal models like CLIP for 3D tasks, they still struggle with aligning point cloud data to 2D encoders. As a result, these methods continue to depend on 3D encoders for feature extraction, further increasing model complexity and training inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal network to process all three modalities (RGB images, text, and point clouds), significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal model with adapter-based fine-tuning, this framework effectively adapts to the tri-modal setting, improving both adaptability and performance across modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module is designed to fuse geometric multi-scale features from point clouds and images. We then integrate textual features for final modality fusion and introduce a multi-modal decoder to facilitate deep cross-modal understanding. Together, our method achieves unified feature extraction and fusion across the three modalities, enabling an end-to-end 3D visual grounding model. Compared to the baseline, our method reduces the number of trainable parameters by approximately 58\\%, while achieving a 6.52\\% improvement in the 3D detection task and a 6.25\\% improvement in the 3D visual grounding task.', 'abstract_zh': '基于人类指令在真实世界3D环境中的三维视觉定位允许实体化代理理解视觉信息，这对于实体化智能至关重要。现有的三维视觉定位方法通常依赖于不同模态（如RGB图像、文本和3D点云）的独立编码器，导致大型且复杂的模型，训练效率低下。尽管一些方法使用预先训练的2D多模态模型（如CLIP）进行三维任务，它们仍然难以将点云数据对齐到2D编码器。因此，这些方法仍然依赖三维编码器进行特征提取，进一步增加了模型复杂性和训练效率低下。在本文中，我们提出了一种统一的2D预训练多模态网络来处理所有三种模态（RGB图像、文本和点云），显著简化了网络结构。通过利用基于适配器的细调的2D CLIP双模态模型，该框架能够有效地适应三维设置，提高各模态的适应性和性能。我们设计的几何感知的2D-3D特征恢复与融合（GARF）模块旨在融合点云和图像的几何多尺度特征。然后，我们结合文本特征进行最终模态融合，并引入多模态解码器以促进深层次跨模态理解。我们的方法实现了三模态统一的特征提取和融合，能够构建端到端的三维视觉定位模型。与基线方法相比，我们的方法减少了约58%的可训练参数，在三维检测任务上提高了6.52%的性能，并在三维视觉定位任务上提高了6.25%的性能。', 'title_zh': 'TriCLIP-3D：一种基于CLIP的统一高效三模态3D视觉定位框架'}
{'arxiv_id': 'arXiv:2507.14766', 'title': 'CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories', 'authors': 'Mehak Arora, Ayman Ali, Kaiyuan Wu, Carolyn Davis, Takashi Shimazui, Mahmoud Alwakeel, Victor Moas, Philip Yang, Annette Esper, Rishikesan Kamaleswaran', 'link': 'https://arxiv.org/abs/2507.14766', 'abstract': "In intensive care units (ICUs), patients with complex clinical conditions require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a vital diagnostic tool, providing insights into clinical trajectories, but their irregular acquisition limits their utility. Existing tools for CXR interpretation are constrained by cross-sectional analysis, failing to capture temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal framework that integrates temporally sparse CXR imaging and radiology reports with high-frequency clinical data, such as vital signs, laboratory values, and respiratory flow sheets, to predict the trajectory of CXR findings in critically ill patients. CXR-TFT leverages latent embeddings from a vision encoder that are temporally aligned with hourly clinical data through interpolation. A transformer model is then trained to predict CXR embeddings at each hour, conditioned on previous embeddings and clinical measurements. In a retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy in forecasting abnormal CXR findings up to 12 hours before they became radiographically evident. This predictive capability in clinical data holds significant potential for enhancing the management of time-sensitive conditions like acute respiratory distress syndrome, where early intervention is crucial and diagnoses are often delayed. By providing distinctive temporal resolution in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights that can directly improve clinical outcomes.", 'abstract_zh': '在重症监护单位（ICUs）中，具有复杂临床状况的患者需要严密监测和及时干预。胸部X光片（CXRs）是重要的诊断工具，提供了临床病程的见解，但由于其不规律的获取限制了其应用。现有的CXR解读工具受限于横截面分析，无法捕捉时间动态。为了解决这一问题，我们引入了CXR-TFT，这是一种新的多模态框架，结合了时间上稀疏的CXR影像和放射学报告以及高频临床数据（如生命体征、实验室值和呼吸流量表），以预测危重患者CXR发现的病程。CXR-TFT利用前景编码器的潜在嵌入，通过内插与每小时临床数据的时间对齐。然后，通过条件于先前嵌入和临床测量，训练了一个变压器模型来每小时预测CXR嵌入。在一项包含20,000名ICU患者的回顾性研究中，CXR-TFT在预测胸部X光片异常发现方面表现出高度准确性，提前12小时即可准确预知这些异常发现。这种在临床数据中的预测能力对于增强急性呼吸窘迫综合征等需要及时干预的疾病管理具有重要潜力，因为早期干预至关重要且诊断往往延后。通过提供前瞻性CXR分析的独特时间分辨率，CXR-TFT提供了可操作的“全患者”洞察，可以直接改善临床结果。', 'title_zh': 'CXR-TFT：多模态时间融合变换器用于预测胸部X光 trajectories'}
{'arxiv_id': 'arXiv:2507.14680', 'title': 'WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis', 'authors': 'Xinheng Lyu, Yuci Liang, Wenting Chen, Meidan Ding, Jiaqi Yang, Guolin Huang, Daokun Zhang, Xiangjian He, Linlin Shen', 'link': 'https://arxiv.org/abs/2507.14680', 'abstract': "Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel tissue analysis across various pathological tasks. While recent advancements in multi-modal large language models (MLLMs) allow multi-task WSI analysis through natural language, they often underperform compared to task-specific models. Collaborative multi-agent systems have emerged as a promising solution to balance versatility and accuracy in healthcare, yet their potential remains underexplored in pathology-specific domains. To address these issues, we propose WSI-Agents, a novel collaborative multi-agent system for multi-modal WSI analysis. WSI-Agents integrates specialized functional agents with robust task allocation and verification mechanisms to enhance both task-specific accuracy and multi-task versatility through three components: (1) a task allocation module assigning tasks to expert agents using a model zoo of patch and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through internal consistency checks and external validation using pathology knowledge bases and domain-specific models, and (3) a summary module synthesizing the final summary with visual interpretation maps. Extensive experiments on multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs and medical agent frameworks across diverse tasks.", 'abstract_zh': '基于多模态WSI分析的WSI-Agents多代理系统', 'title_zh': 'WSI-Agents：一种用于多模态全玻片图像分析的协作多-Agent系统'}
{'arxiv_id': 'arXiv:2507.14339', 'title': 'Fiduciary AI for the Future of Brain-Technology Interactions', 'authors': 'Abhishek Bhattacharjee, Jack Pilkington, Nita Farahany', 'link': 'https://arxiv.org/abs/2507.14339', 'abstract': "Brain foundation models represent a new frontier in AI: instead of processing text or images, these models interpret real-time neural signals from EEG, fMRI, and other neurotechnologies. When integrated with brain-computer interfaces (BCIs), they may enable transformative applications-from thought controlled devices to neuroprosthetics-by interpreting and acting on brain activity in milliseconds. However, these same systems pose unprecedented risks, including the exploitation of subconscious neural signals and the erosion of cognitive liberty. Users cannot easily observe or control how their brain signals are interpreted, creating power asymmetries that are vulnerable to manipulation. This paper proposes embedding fiduciary duties-loyalty, care, and confidentiality-directly into BCI-integrated brain foundation models through technical design. Drawing on legal traditions and recent advancements in AI alignment techniques, we outline implementable architectural and governance mechanisms to ensure these systems act in users' best interests. Placing brain foundation models on a fiduciary footing is essential to realizing their potential without compromising self-determination.", 'abstract_zh': '脑基础模型代表了AI的新前沿：这些模型通过解释实时的脑电信号、功能性磁共振成像等神经技术的信号，而非处理文本或图像。当与脑-机接口集成时，它们可能通过毫秒级解释和反应大脑活动，实现从思想控制设备到神经假体等变革性应用。然而，这些系统同时带来了前所未有的风险，包括潜意识神经信号的滥用以及认知自由的侵蚀。用户无法轻易地观察或控制其脑信号如何被解释，这创造了容易受到操纵的影响权力不对等。本文提议通过技术设计直接嵌入诚信义务（忠诚、关怀和保密）到脑-机接口集成的脑基础模型中，借鉴法律传统和近期AI对齐技术的进步，我们提出了可实施的架构和治理机制，以确保这些系统符合用户的最大利益。将脑基础模型置于信托基础上对于充分利用其潜力而不牺牲自主性是至关重要的。', 'title_zh': '未来脑机交互中的受托人工智能'}
{'arxiv_id': 'arXiv:2507.14180', 'title': 'Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems', 'authors': 'Nasir Khan, Asmaa Abdallah, Abdulkadir Celik, Ahmed M. Eltawil, Sinem Coleri', 'link': 'https://arxiv.org/abs/2507.14180', 'abstract': 'In line with the AI-native 6G vision, explainability and robustness are crucial for building trust and ensuring reliable performance in millimeter-wave (mmWave) systems. Efficient beam alignment is essential for initial access, but deep learning (DL) solutions face challenges, including high data collection overhead, hardware constraints, lack of explainability, and susceptibility to adversarial attacks. This paper proposes a robust and explainable DL-based beam alignment engine (BAE) for mmWave multiple-input multiple output (MIMO) systems. The BAE uses received signal strength indicator (RSSI) measurements from wide beams to predict the best narrow beam, reducing the overhead of exhaustive beam sweeping. To overcome the challenge of real-world data collection, this work leverages a site-specific digital twin (DT) to generate synthetic channel data closely resembling real-world environments. A model refinement via transfer learning is proposed to fine-tune the pre-trained model residing in the DT with minimal real-world data, effectively bridging mismatches between the digital replica and real-world environments. To reduce beam training overhead and enhance transparency, the framework uses deep Shapley additive explanations (SHAP) to rank input features by importance, prioritizing key spatial directions and minimizing beam sweeping. It also incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a credibility metric for detecting out-of-distribution inputs and ensuring robust, transparent decision-making. Experimental results show that the proposed framework reduces real-world data needs by 70%, beam training overhead by 62%, and improves outlier detection robustness by up to 8.5x, achieving near-optimal spectral efficiency and transparent decision making compared to traditional softmax based DL models.', 'abstract_zh': '基于AI原生6G愿景的毫米波系统可解释性和鲁棒性化波束对准引擎', 'title_zh': '基于数字孪生的可解释AI在毫米波MIMO系统中鲁棒波束预测'}
{'arxiv_id': 'arXiv:2506.23298', 'title': 'Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification', 'authors': 'Xing Shen, Justin Szeto, Mingyang Li, Hengguan Huang, Tal Arbel', 'link': 'https://arxiv.org/abs/2506.23298', 'abstract': "Multimodal large language models (MLLMs) have enormous potential to perform few-shot in-context learning in the context of medical image analysis. However, safe deployment of these models into real-world clinical practice requires an in-depth analysis of the accuracies of their predictions, and their associated calibration errors, particularly across different demographic subgroups. In this work, we present the first investigation into the calibration biases and demographic unfairness of MLLMs' predictions and confidence scores in few-shot in-context learning for medical image classification. We introduce CALIN, an inference-time calibration method designed to mitigate the associated biases. Specifically, CALIN estimates the amount of calibration needed, represented by calibration matrices, using a bi-level procedure: progressing from the population level to the subgroup level prior to inference. It then applies this estimation to calibrate the predicted confidence scores during inference. Experimental results on three medical imaging datasets: PAPILA for fundus image classification, HAM10000 for skin cancer classification, and MIMIC-CXR for chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair confidence calibration in its prediction, while improving its overall prediction accuracies and exhibiting minimum fairness-utility trade-off. Our codebase can be found at this https URL.", 'abstract_zh': '多模态大规模语言模型在医学图像分析中 few-shot 在上下文学习中的校准偏差与人口统计不公平性研究：CALIN 方法及其应用', 'title_zh': '暴露并缓解多模态少样本上下文学习中医学图像分类中的校准偏差和人口统计不公平性'}
