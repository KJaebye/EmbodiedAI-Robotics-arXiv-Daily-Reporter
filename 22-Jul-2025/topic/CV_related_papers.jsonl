{'arxiv_id': 'arXiv:2507.15189', 'title': 'CHADET: Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer', 'authors': 'Kevin Christiansen Marsim, Jinwoo Jeon, Yeeun Kim, Myeongwoo Jeong, Hyun Myung', 'link': 'https://arxiv.org/abs/2507.15189', 'abstract': 'Depth information which specifies the distance between objects and current position of the robot is essential for many robot tasks such as navigation. Recently, researchers have proposed depth completion frameworks to provide dense depth maps that offer comprehensive information about the surrounding environment. However, existing methods show significant trade-offs between computational efficiency and accuracy during inference. The substantial memory and computational requirements make them unsuitable for real-time applications, highlighting the need to improve the completeness and accuracy of depth information while improving processing speed to enhance robot performance in various tasks. To address these challenges, in this paper, we propose CHADET(cross-hierarchical-attention depth-completion transformer), a lightweight depth-completion network that can generate accurate dense depth maps from RGB images and sparse depth points. For each pair, its feature is extracted from the depthwise blocks and passed to the equally lightweight transformer-based decoder. In the decoder, we utilize the novel cross-hierarchical-attention module that refines the image features from the depth information. Our approach improves the quality and reduces memory usage of the depth map prediction, as validated in both KITTI, NYUv2, and VOID datasets.', 'abstract_zh': '基于交叉层次注意力的深度补全变换器（CHADET）：一种轻量级的稠密深度图生成方法', 'title_zh': 'CHADET：跨层级注意力的无监督轻量级变换器用于深度补全'}
{'arxiv_id': 'arXiv:2507.14820', 'title': 'KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning', 'authors': 'Bingran Chen, Baorun Li, Jian Yang, Yong Liu, Guangyao Zhai', 'link': 'https://arxiv.org/abs/2507.14820', 'abstract': 'High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation to serve as a basic function. Previous approaches either directly generate grasps from point-cloud data, suffering from challenges with small objects and sensor noise, or infer 3D information from RGB images, which introduces expensive annotation requirements and discretization issues. Recent methods mitigate some challenges by retaining a 2D representation to estimate grasp keypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF poses. However, these methods are limited by their non-differentiable nature and reliance solely on 2D supervision, which hinders the full exploitation of rich 3D information. In this work, we present KGN-Pro, a novel grasping network that preserves the efficiency and fine-grained object grasping of previous KGNs while integrating direct 3D optimization through probabilistic PnP layers. KGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further outputs a 2D confidence map to weight keypoint contributions during re-projection error minimization. By modeling the weighted sum of squared re-projection errors probabilistically, the network effectively transmits 3D supervision to its 2D keypoint predictions, enabling end-to-end learning. Experiments on both simulated and real-world platforms demonstrate that KGN-Pro outperforms existing methods in terms of grasp cover rate and success rate.', 'abstract_zh': '高阶机器人 manipulating 任务需要灵活的6-DOF 抓取估计作为基本功能。', 'title_zh': 'KGN-Pro：基于关键点的概率二维-三维对应学习抓取预测'}
{'arxiv_id': 'arXiv:2507.14694', 'title': 'Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks', 'authors': 'Yue Ma, Kanglei Zhou, Fuyang Yu, Frederick W. B. Li, Xiaohui Liang', 'link': 'https://arxiv.org/abs/2507.14694', 'abstract': '3D human motion forecasting aims to enable autonomous applications. Estimating uncertainty for each prediction (i.e., confidence based on probability density or quantile) is essential for safety-critical contexts like human-robot collaboration to minimize risks. However, existing diverse motion forecasting approaches struggle with uncertainty quantification due to implicit probabilistic representations hindering uncertainty modeling. We propose ProbHMI, which introduces invertible networks to parameterize poses in a disentangled latent space, enabling probabilistic dynamics modeling. A forecasting module then explicitly predicts future latent distributions, allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI achieves strong performance for both deterministic and diverse prediction while validating uncertainty calibration, critical for risk-aware decision making.', 'abstract_zh': '3D人体运动预测旨在实现自主应用。为人类-机器人协作等安全关键场景中的每个预测估算不确定性（基于概率密度或分位数的信任度）至关重要。然而，现有多种运动预测方法由于隐式概率表示妨碍不确定性建模，在不确定性量化方面存在困难。我们提出ProbHMI，通过引入可逆网络在解耦的潜在空间中参数化姿态，实现概率动力学建模。随后，预测模块显式预测未来的潜在分布，从而有效进行不确定性量化。在基准测试上的评估表明，ProbHMI在确定性和多样化预测方面均表现出色，同时验证了不确定性校准的准确性，这对于风险感知决策至关重要。', 'title_zh': '具有不确定性意识的概率性3D人体运动预测via可逆网络'}
{'arxiv_id': 'arXiv:2507.15496', 'title': 'Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images', 'authors': 'JunYing Huang, Ao Xu, DongSun Yong, KeRen Li, YuanFeng Wang, Qi Qin', 'link': 'https://arxiv.org/abs/2507.15496', 'abstract': 'Odometry is a critical task for autonomous systems for self-localization and navigation. We propose a novel LiDAR-Visual odometry framework that integrates LiDAR point clouds and images for accurate and robust pose estimation. Our method utilizes a dense-depth map estimated from point clouds and images through depth completion, and incorporates a multi-scale feature extraction network with attention mechanisms, enabling adaptive depth-aware representations. Furthermore, we leverage dense depth information to refine flow estimation and mitigate errors in occlusion-prone regions. Our hierarchical pose refinement module optimizes motion estimation progressively, ensuring robust predictions against dynamic environments and scale ambiguities. Comprehensive experiments on the KITTI odometry benchmark demonstrate that our approach achieves similar or superior accuracy and robustness compared to state-of-the-art visual and LiDAR odometry methods.', 'abstract_zh': '激光雷达-视觉里程计框架：融合点云和图像实现准确可靠的位姿估计', 'title_zh': '稠密深度图引导的稀疏点云和图像的深度 Lidar-视觉里程计'}
{'arxiv_id': 'arXiv:2507.14500', 'title': 'Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow', 'authors': 'Zhiyuan Hua, Dehao Yuan, Cornelia Fermüller', 'link': 'https://arxiv.org/abs/2507.14500', 'abstract': 'This paper introduces a robust framework for motion segmentation and egomotion estimation using event-based normal flow, tailored specifically for neuromorphic vision sensors. In contrast to traditional methods that rely heavily on optical flow or explicit depth estimation, our approach exploits the sparse, high-temporal-resolution event data and incorporates geometric constraints between normal flow, scene structure, and inertial measurements. The proposed optimization-based pipeline iteratively performs event over-segmentation, isolates independently moving objects via residual analysis, and refines segmentations using hierarchical clustering informed by motion similarity and temporal consistency. Experimental results on the EVIMO2v2 dataset validate that our method achieves accurate segmentation and translational motion estimation without requiring full optical flow computation. This approach demonstrates significant advantages at object boundaries and offers considerable potential for scalable, real-time robotic and navigation applications.', 'abstract_zh': '一种基于事件驱动法向流的鲁棒运动分割与自我运动估计框架：面向神经形态视觉传感器的应用', 'title_zh': '基于事件的法向流的运动分割与自我运动估计'}
{'arxiv_id': 'arXiv:2507.15852', 'title': 'SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction', 'authors': 'Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Songxin He, Jianfan Lin, Junsong Tang, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang', 'link': 'https://arxiv.org/abs/2507.15852', 'abstract': 'Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the human-like conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), a concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms a comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware video object segmentation.', 'abstract_zh': '基于概念的视频对象分割（SeC）：超越外观匹配的高层语义理解', 'title_zh': 'SeC：通过渐进概念构建推动复杂视频对象分割'}
{'arxiv_id': 'arXiv:2507.15846', 'title': 'GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding', 'authors': 'Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang', 'link': 'https://arxiv.org/abs/2507.15846', 'abstract': 'Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G$^2$, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks.', 'abstract_zh': '图形用户界面(GUI)高斯接地方法将自然语言指令精确映射到界面位置，实现自主交互。当前的强化学习方法使用二元奖励，将界面元素视为非此即彼的目标，产生稀疏信号，忽视了空间交互的连续性。受人类点击行为自然形成以目标元素为中心的高斯分布启发，我们引入了GUI高斯接地奖励（GUI-G$^2$），该奖励框架以连续高斯分布形式建模界面元素在整个界面平面上的分布。GUI-G$^2$包含两种协同机制：高斯点奖励通过以元素质心为中心的指数衰减分布建模精确定位，而覆盖奖励通过测量预测高斯分布与目标区域之间的重叠来评估空间对齐。为处理不同规模的元素，我们开发了一种适应性方差机制，基于元素尺寸调整奖励分布。该框架将GUI接地从稀疏二元分类转换为密集连续优化，其中高斯分布产生的丰富梯度信号引导模型向最佳交互位置发展。在ScreenSpot、ScreenSpot-v2和ScreenSpot-Pro基准测试中的广泛实验表明，GUI-G$^2$在所有基准测试中均显著优于现有最佳方法UI-TARS-72B，在ScreenSpot-Pro上的改进幅度达到了24.7%。我们的分析表明，连续建模提供了对界面变化的更好鲁棒性和对未见布局的更强泛化能力，为GUI交互任务中的空间推理建立了新的范式。', 'title_zh': 'GUI-G$^2$：基于高斯奖励建模的GUI定位'}
{'arxiv_id': 'arXiv:2507.15803', 'title': 'ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction', 'authors': 'Danhui Chen, Ziquan Liu, Chuxi Yang, Dan Wang, Yan Yan, Yi Xu, Xiangyang Ji', 'link': 'https://arxiv.org/abs/2507.15803', 'abstract': "Pixel-level vision tasks, such as semantic segmentation, require extensive and high-quality annotated data, which is costly to obtain. Semi-supervised semantic segmentation (SSSS) has emerged as a solution to alleviate the labeling burden by leveraging both labeled and unlabeled data through self-training techniques. Meanwhile, the advent of foundational segmentation models pre-trained on massive data, has shown the potential to generalize across domains effectively. This work explores whether a foundational segmentation model can address label scarcity in the pixel-level vision task as an annotator for unlabeled images. Specifically, we investigate the efficacy of using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual input, to generate predictive masks for unlabeled data. To address the shortcomings of using SEEM-generated masks as supervision, we propose ConformalSAM, a novel SSSS framework which first calibrates the foundation model using the target domain's labeled data and then filters out unreliable pixel labels of unlabeled data so that only high-confidence labels are used as supervision. By leveraging conformal prediction (CP) to adapt foundation models to target data through uncertainty calibration, ConformalSAM exploits the strong capability of the foundational segmentation model reliably which benefits the early-stage learning, while a subsequent self-reliance training strategy mitigates overfitting to SEEM-generated masks in the later training stage. Our experiment demonstrates that, on three standard benchmarks of SSSS, ConformalSAM achieves superior performance compared to recent SSSS methods and helps boost the performance of those methods as a plug-in.", 'abstract_zh': '基于像素级视觉任务的半监督语义分割：ConformalSAM框架的研究', 'title_zh': 'ConformalSAM: 利用符合性预测解锁基础分割模型在半监督语义分割中的潜力'}
{'arxiv_id': 'arXiv:2507.15577', 'title': 'GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation', 'authors': 'Hugo Carlesso, Maria Eliza Patulea, Moncef Garouani, Radu Tudor Ionescu, Josiane Mothe', 'link': 'https://arxiv.org/abs/2507.15577', 'abstract': 'Mixup has become a popular augmentation strategy for image classification, yet its naive pixel-wise interpolation often produces unrealistic images that can hinder learning, particularly in high-stakes medical applications. We propose GeMix, a two-stage framework that replaces heuristic blending with a learned, label-aware interpolation powered by class-conditional GANs. First, a StyleGAN2-ADA generator is trained on the target dataset. During augmentation, we sample two label vectors from Dirichlet priors biased toward different classes and blend them via a Beta-distributed coefficient. Then, we condition the generator on this soft label to synthesize visually coherent images that lie along a continuous class manifold. We benchmark GeMix on the large-scale COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101, EfficientNet-B0). When combined with real data, our method increases macro-F1 over traditional mixup for all backbones, reducing the false negative rate for COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup, delivering stronger regularization and greater semantic fidelity, without disrupting existing training pipelines. We publicly release our code at this https URL to foster reproducibility and further research.', 'abstract_zh': 'GeMix: 一种基于类条件GAN的学习驱动插值框架用于图像分类的增强策略', 'title_zh': 'GeMix: 基于条件GAN的Mixup方法以改善医学图像增强'}
{'arxiv_id': 'arXiv:2507.15524', 'title': 'RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation', 'authors': 'Simon Winther Albertsen, Hjalte Svaneborg Bjørnstrup, Mostafa Mehdipour Ghazi', 'link': 'https://arxiv.org/abs/2507.15524', 'abstract': 'Accurate segmentation is crucial for clinical applications, but existing models often assume fixed, high-resolution inputs and degrade significantly when faced with lower-resolution data in real-world scenarios. To address this limitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation architecture that dynamically adapts its inference path to the spatial resolution of the input. Central to our design are multi-scale blocks integrated at multiple encoder depths, a resolution-aware routing mechanism, and consistency-driven training that aligns multi-resolution features with full-resolution representations. We evaluate RARE-UNet on two benchmark brain imaging tasks for hippocampus and tumor segmentation. Compared to standard UNet, its multi-resolution augmented variant, and nnUNet, our model achieves the highest average Dice scores of 0.84 and 0.65 across resolution, while maintaining consistent performance and significantly reduced inference time at lower resolutions. These results highlight the effectiveness and scalability of our architecture in achieving resolution-robust segmentation. The codes are available at: this https URL.', 'abstract_zh': '分辨率感知多尺度分割架构RARE-UNet及其在临床应用中的优势', 'title_zh': 'RARE-UNet: 解析度对齐路由入口用于自适应医学图像分割'}
{'arxiv_id': 'arXiv:2507.15454', 'title': 'ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting', 'authors': 'Ruijie Zhu, Mulin Yu, Linning Xu, Lihan Jiang, Yixuan Li, Tianzhu Zhang, Jiangmiao Pang, Bo Dai', 'link': 'https://arxiv.org/abs/2507.15454', 'abstract': '3D Gaussian Splatting is renowned for its high-fidelity reconstructions and real-time novel view synthesis, yet its lack of semantic understanding limits object-level perception. In this work, we propose ObjectGS, an object-aware framework that unifies 3D scene reconstruction with semantic understanding. Instead of treating the scene as a unified whole, ObjectGS models individual objects as local anchors that generate neural Gaussians and share object IDs, enabling precise object-level reconstruction. During training, we dynamically grow or prune these anchors and optimize their features, while a one-hot ID encoding with a classification loss enforces clear semantic constraints. We show through extensive experiments that ObjectGS not only outperforms state-of-the-art methods on open-vocabulary and panoptic segmentation tasks, but also integrates seamlessly with applications like mesh extraction and scene editing. Project page: this https URL', 'abstract_zh': '3D高斯点云建模结合语义理解的对象感知框架：ObjectGS在高保真重建和实时新颖视图合成方面具有优势，但由于缺乏语义理解，限制了对象级别感知。本工作提出ObjectGS，一种结合3D场景重建与语义理解的对象感知框架。ObjectGS将场景建模为局部锚点的集合，这些锚点生成神经高斯函数并共享对象ID，从而实现精确的对象级别重建。在训练过程中，我们动态增长或减少这些锚点并优化其特征，同时通过独热ID编码和分类损失强制清晰的语义约束。实验表明，ObjectGS不仅在开放词汇和全景分割任务上性能优于现有方法，还能无缝集成到网格提取和场景编辑等应用中。项目页面: 这里', 'title_zh': 'ObjectGS: 基于高斯点云的目标意识场景重建与场景理解'}
{'arxiv_id': 'arXiv:2507.15361', 'title': 'Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation', 'authors': 'Muhammad Aqeel, Maham Nazir, Zanxi Ruan, Francesco Setti', 'link': 'https://arxiv.org/abs/2507.15361', 'abstract': 'Medical image segmentation suffers from data scarcity, particularly in polyp detection where annotation requires specialized expertise. We present SynDiff, a framework combining text-guided synthetic data generation with efficient diffusion-based segmentation. Our approach employs latent diffusion models to generate clinically realistic synthetic polyps through text-conditioned inpainting, augmenting limited training data with semantically diverse samples. Unlike traditional diffusion methods requiring iterative denoising, we introduce direct latent estimation enabling single-step inference with T x computational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9% IoU while maintaining real-time capability suitable for clinical deployment. The framework demonstrates that controlled synthetic augmentation improves segmentation robustness without distribution shift. SynDiff bridges the gap between data-hungry deep learning models and clinical constraints, offering an efficient solution for deployment in resourcelimited medical settings.', 'abstract_zh': '医学图像分割受到数据稀缺性的困扰，特别是在肠道息肉检测中，标注需要专门的专业知识。我们提出了一种结合文本指导的合成数据生成与高效扩散分割的框架SynDiff。该方法利用潜在扩散模型通过文本条件填充生成临床现实的合成息肉，从而用语义多样性样本增强有限的训练数据。与传统的需要迭代去噪的扩散方法不同，我们引入了直接潜在估计，实现了单步推理并提高了T倍的计算速度。在CVC-ClinicDB上，SynDiff达到了96.0%的Dice系数和92.9%的IoU，同时保持了适用于临床部署的实时能力。该框架证明了受控的合成增强可以提高分割的鲁棒性而无需分布偏移。SynDiff弥合了数据饥渴的深度学习模型与临床约束之间的差距，为资源受限的医疗环境提供了高效部署的解决方案。', 'title_zh': '潜在空间协同效应：文本引导的数据增强方法直接扩散生物医学分割'}
{'arxiv_id': 'arXiv:2507.15340', 'title': 'MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis', 'authors': 'Marc Boubnovski Martell, Kristofer Linton-Reid, Mitchell Chen, Sumeet Hindocha, Benjamin Hunter, Marco A. Calzado, Richard Lee, Joram M. Posma, Eric O. Aboagye', 'link': 'https://arxiv.org/abs/2507.15340', 'abstract': 'High-resolution volumetric computed tomography (CT) is essential for accurate diagnosis and treatment planning in thoracic diseases; however, it is limited by radiation dose and hardware costs. We present the Transformer Volumetric Super-Resolution Network (\\textbf{TVSRN-V2}), a transformer-based super-resolution (SR) framework designed for practical deployment in clinical lung CT analysis. Built from scalable components, including Through-Plane Attention Blocks (TAB) and Swin Transformer V2 -- our model effectively reconstructs fine anatomical details in low-dose CT volumes and integrates seamlessly with downstream analysis pipelines. We evaluate its effectiveness on three critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis -- across multiple clinical cohorts. To enhance robustness across variable acquisition protocols, we introduce pseudo-low-resolution augmentation, simulating scanner diversity without requiring private data. TVSRN-V2 demonstrates a significant improvement in segmentation accuracy (+4\\% Dice), higher radiomic feature reproducibility, and enhanced predictive performance (+0.06 C-index and AUC). These results indicate that SR-driven recovery of structural detail significantly enhances clinical decision support, positioning TVSRN-V2 as a well-engineered, clinically viable system for dose-efficient imaging and quantitative analysis in real-world CT workflows.', 'abstract_zh': '基于变换器的高分辨率体积计算 tomography 网络 (TVSRN-V2)：用于临床肺部 CT 分析的实用超分辨率框架', 'title_zh': 'MedSR-Impact: 基于变压器的肺部CT分割、 radiomics、分类和预后超分辨技术'}
{'arxiv_id': 'arXiv:2507.15292', 'title': 'EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro', 'authors': 'An Wanga, Rulin Zhou, Mengya Xu, Yiru Ye, Longfei Gou, Yiting Chang, Hao Chen, Chwee Ming Lim, Jiankun Wang, Hongliang Ren', 'link': 'https://arxiv.org/abs/2507.15292', 'abstract': 'Visualizing subtle vascular motions in endoscopic surgery is crucial for surgical precision and decision-making, yet remains challenging due to the complex and dynamic nature of surgical scenes. To address this, we introduce EndoControlMag, a training-free, Lagrangian-based framework with mask-conditioned vascular motion magnification tailored to endoscopic environments. Our approach features two key modules: a Periodic Reference Resetting (PRR) scheme that divides videos into short overlapping clips with dynamically updated reference frames to prevent error accumulation while maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification (HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores using a pretrained visual tracking model to maintain accurate localization despite occlusions and view changes. It then applies one of two adaptive softening strategies to surrounding tissues: motion-based softening that modulates magnification strength proportional to observed tissue displacement, or distance-based exponential decay that simulates biomechanical force attenuation. This dual-mode approach accommodates diverse surgical scenarios-motion-based softening excels with complex tissue deformations while distance-based softening provides stability during unreliable optical flow conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four different surgery types and various challenging scenarios, including occlusions, instrument disturbance, view changes, and vessel deformations. Quantitative metrics, visual assessments, and expert surgeon evaluations demonstrate that EndoControlMag significantly outperforms existing methods in both magnification accuracy and visual quality while maintaining robustness across challenging surgical conditions. The code, dataset, and video results are available at this https URL.', 'abstract_zh': 'EndoControlMag：一种用于内镜手术的无需训练、基于Lagrangian的血管运动放大框架', 'title_zh': 'EndoControlMag: 基于周期参考重置和分层组织感知双-mask控制的内窥镜血管运动放大技术'}
{'arxiv_id': 'arXiv:2507.15269', 'title': 'Conditional Video Generation for High-Efficiency Video Compression', 'authors': 'Fangqiu Yi, Jingyu Xu, Jiawei Shao, Chi Zhang, Xuelong Li', 'link': 'https://arxiv.org/abs/2507.15269', 'abstract': 'Perceptual studies demonstrate that conditional diffusion models excel at reconstructing video content aligned with human visual perception. Building on this insight, we propose a video compression framework that leverages conditional diffusion models for perceptually optimized reconstruction. Specifically, we reframe video compression as a conditional generation task, where a generative model synthesizes video from sparse, yet informative signals. Our approach introduces three key modules: (1) Multi-granular conditioning that captures both static scene structure and dynamic spatio-temporal cues; (2) Compact representations designed for efficient transmission without sacrificing semantic richness; (3) Multi-condition training with modality dropout and role-aware embeddings, which prevent over-reliance on any single modality and enhance robustness. Extensive experiments show that our method significantly outperforms both traditional and neural codecs on perceptual quality metrics such as Fréchet Video Distance (FVD) and LPIPS, especially under high compression ratios.', 'abstract_zh': '感知研究显示条件扩散模型在重建与人类视觉感知相一致的视频内容方面表现出色。基于这一见解，我们提出了一种利用条件扩散模型进行感知优化重构的视频压缩框架。具体而言，我们将视频压缩重新定义为一个条件生成任务，其中生成模型从稀疏但具有信息性的信号中合成视频。我们的方法引入了三个关键模块：（1）多粒度条件模块，捕获静态场景结构和动态空时线索；（2）紧凑表示，旨在进行高效传输而不牺牲语义丰富性；（3）具备模态 dropout 和角色感知嵌入的多条件训练，防止对单一模态的过度依赖并增强鲁棒性。广泛实验表明，与传统和神经编解码器相比，我们的方法在感知质量度量如Fréchet视频距离（FVD）和LPIPS等方面表现显著优异，尤其是在高压缩比条件下。', 'title_zh': '基于条件视频生成的高效率视频压缩'}
{'arxiv_id': 'arXiv:2507.15193', 'title': 'A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT', 'authors': 'Tanjin Taher Toma, Tejas Sudharshan Mathai, Bikash Santra, Pritam Mukherjee, Jianfei Liu, Wesley Jong, Darwish Alabyad, Vivek Batheja, Abhishek Jha, Mayank Patel, Darko Pucar, Jayadira del Rivero, Karel Pacak, Ronald M. Summers', 'link': 'https://arxiv.org/abs/2507.15193', 'abstract': 'Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is essential for tumor burden estimation, prognosis, and treatment planning. It may also help infer genetic clusters, reducing reliance on expensive testing. This study systematically evaluates anatomical priors to identify configurations that improve deep learning-based PCC segmentation. We employed the nnU-Net framework to evaluate eleven annotation strategies for accurate 3D segmentation of pheochromocytoma, introducing a set of novel multi-class schemes based on organ-specific anatomical priors. These priors were derived from adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen, kidney, aorta, adrenal gland, and pancreas), and were compared against a broad body-region prior used in previous work. The framework was trained and tested on 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center. Performance was measured using Dice Similarity Coefficient (DSC), Normalized Surface Distance (NSD), and instance-wise F1 score. Among all strategies, the Tumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation accuracy, significantly outperforming the previously used Tumor + Body (TB) annotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84% improvement at an IoU threshold of 0.5), measured on a 70-30 train-test split. The TKA model also showed superior tumor burden quantification (R^2 = 0.968) and strong segmentation across all genetic subtypes. In five-fold cross-validation, TKA consistently outperformed TB across IoU thresholds (0.1 to 0.5), reinforcing its robustness and generalizability. These findings highlight the value of incorporating relevant anatomical context in deep learning models to achieve precise PCC segmentation, supporting clinical assessment and longitudinal monitoring.', 'abstract_zh': '腹腔CT扫描中嗜铬细胞瘤的精确分割对于瘤负荷估计、预后和治疗规划至关重要。它也可能有助于推断基因簇，减少对昂贵的检测依赖。本研究系统评估解剖先验知识，以识别提高基于深度学习的嗜铬细胞瘤分割效果的配置。我们采用了nnU-Net框架，对嗜铬细胞瘤进行准确的3D分割策略进行了评估，引入了一套基于器官特定解剖先验知识的新型多分类方案。这些先验知识来源于常伴随肾上腺肿瘤的邻近器官（如肝脏、脾脏、肾脏、主动脉、肾上腺和胰腺），并与前人工作中使用的广泛体区先验知识进行了比较。该框架在NIH临床中心的91名患者中的105例对比增强CT扫描数据上进行了训练和测试。性能通过Dice相似性系数（DSC）、归一化表面距离（NSD）和实例F1分数进行衡量。在所有策略中，肿瘤+肾脏+主动脉（TKA）标注实现了最高的分割精度，显著优于之前使用的肿瘤+体区（TB）标注，在DSC（p = 0.0097）、NSD（p = 0.0110）和F1分数（在交并比阈值为0.5时提高25.84%）上表现更佳，采用70-30训练-测试分割。TKA模型在肿瘤负荷定量（R² = 0.968）和所有遗传亚型上的分割表现也更优。在五折交叉验证中，TKA在交并比阈值（0.1到0.5）上均优于TB，增强了其稳健性和泛化能力。这些发现强调了在深度学习模型中整合相关解剖上下文的重要性，以实现精确的嗜铬细胞瘤分割，支持临床评估和纵向监测。', 'title_zh': '基于深度学习的腹部CT嗜铬细胞瘤分割中解剖先验的研究'}
{'arxiv_id': 'arXiv:2507.15151', 'title': 'Performance Analysis of Post-Training Quantization for CNN-based Conjunctival Pallor Anemia Detection', 'authors': 'Sebastian A. Cruz Romero, Wilfredo E. Lugo Beauchamp', 'link': 'https://arxiv.org/abs/2507.15151', 'abstract': 'Anemia is a widespread global health issue, particularly among young children in low-resource settings. Traditional methods for anemia detection often require expensive equipment and expert knowledge, creating barriers to early and accurate diagnosis. To address these challenges, we explore the use of deep learning models for detecting anemia through conjunctival pallor, focusing on the CP-AnemiC dataset, which includes 710 images from children aged 6-59 months. The dataset is annotated with hemoglobin levels, gender, age and other demographic data, enabling the development of machine learning models for accurate anemia detection. We use the MobileNet architecture as a backbone, known for its efficiency in mobile and embedded vision applications, and fine-tune our model end-to-end using data augmentation techniques and a cross-validation strategy. Our model implementation achieved an accuracy of 0.9313, a precision of 0.9374, and an F1 score of 0.9773 demonstrating strong performance on the dataset. To optimize the model for deployment on edge devices, we performed post-training quantization, evaluating the impact of different bit-widths (FP32, FP16, INT8, and INT4) on model performance. Preliminary results suggest that while FP16 quantization maintains high accuracy (0.9250), precision (0.9370), and F1 Score (0.9377), more aggressive quantization (INT8 and INT4) leads to significant performance degradation. Overall, our study supports further exploration of quantization schemes and hardware optimizations to assess trade-offs between model size, inference time, and diagnostic accuracy in mobile healthcare applications.', 'abstract_zh': '贫血是全球范围内一个普遍的公共卫生问题，特别是在低资源环境下的年轻儿童中更为突出。传统的贫血检测方法往往需要昂贵的设备和专业知识，造成了早期和准确诊断的障碍。为了解决这些问题，我们探索了使用深度学习模型通过结膜苍白来检测贫血的方法，重点关注包含6-59个月儿童710张图像的CP-AnemiC数据集。该数据集标注了血红蛋白水平、性别、年龄和其他人口统计学数据，有助于开发准确的贫血检测机器学习模型。我们采用了MobileNet架构作为基础模型，该架构在移动和嵌入式视觉应用中以高效著称，并通过数据增强技术和交叉验证策略对模型进行了端到端的微调。我们的模型实施在数据集上实现了0.9313的准确率、0.9374的精确率和0.9773的F1分数，显示出良好的性能。为了在边缘设备上优化模型部署，我们进行了后训练量化，评估了不同位宽（FP32、FP16、INT8和INT4）对模型性能的影响。初步结果表明，虽然FP16量化能保持高准确率（0.9250）、精确率（0.9370）和F1分数（0.9377），更具侵略性的量化（INT8和INT4）会导致显著的性能下降。总体而言，我们的研究支持进一步探索量化方案和硬件优化，以评估在移动医疗应用中模型大小、推理时间和诊断准确性之间的权衡。', 'title_zh': '基于卷积神经网络的眼袋苍白贫血检测模型训练后量化性能分析'}
{'arxiv_id': 'arXiv:2507.15064', 'title': 'StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation', 'authors': 'Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, Zuxuan Wu, Yu-Gang Jiang', 'link': 'https://arxiv.org/abs/2507.15064', 'abstract': 'Current diffusion models for human image animation often struggle to maintain identity (ID) consistency, especially when the reference image and driving video differ significantly in body size or position. We introduce StableAnimator++, the first ID-preserving video diffusion framework with learnable pose alignment, capable of generating high-quality videos conditioned on a reference image and a pose sequence without any post-processing. Building upon a video diffusion model, StableAnimator++ contains carefully designed modules for both training and inference, striving for identity consistency. In particular, StableAnimator++ first uses learnable layers to predict the similarity transformation matrices between the reference image and the driven poses via injecting guidance from Singular Value Decomposition (SVD). These matrices align the driven poses with the reference image, mitigating misalignment to a great extent. StableAnimator++ then computes image and face embeddings using off-the-shelf encoders, refining the face embeddings via a global content-aware Face Encoder. To further maintain ID, we introduce a distribution-aware ID Adapter that counteracts interference caused by temporal layers while preserving ID via distribution alignment. During the inference stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization integrated into the denoising process, guiding the diffusion trajectory for enhanced facial fidelity. Experiments on benchmarks show the effectiveness of StableAnimator++ both qualitatively and quantitatively.', 'abstract_zh': '基于可学习姿态对齐的身份保持视频扩散框架StableAnimator++', 'title_zh': 'StableAnimator++: 克服姿态错位和面部 distortion 的人体图像动画'}
{'arxiv_id': 'arXiv:2507.14833', 'title': 'Paired Image Generation with Diffusion-Guided Diffusion Models', 'authors': 'Haoxuan Zhang, Wenju Cui, Yuzhu Cao, Tao Tan, Jie Liu, Yunsong Peng, Jian Zheng', 'link': 'https://arxiv.org/abs/2507.14833', 'abstract': 'The segmentation of mass lesions in digital breast tomosynthesis (DBT) images is very significant for the early screening of breast cancer. However, the high-density breast tissue often leads to high concealment of the mass lesions, which makes manual annotation difficult and time-consuming. As a result, there is a lack of annotated data for model training. Diffusion models are commonly used for data augmentation, but the existing methods face two challenges. First, due to the high concealment of lesions, it is difficult for the model to learn the features of the lesion area. This leads to the low generation quality of the lesion areas, thus limiting the quality of the generated images. Second, existing methods can only generate images and cannot generate corresponding annotations, which restricts the usability of the generated images in supervised training. In this work, we propose a paired image generation method. The method does not require external conditions and can achieve the generation of paired images by training an extra diffusion guider for the conditional diffusion model. During the experimental phase, we generated paired DBT slices and mass lesion masks. Then, we incorporated them into the supervised training process of the mass lesion segmentation task. The experimental results show that our method can improve the generation quality without external conditions. Moreover, it contributes to alleviating the shortage of annotated data, thus enhancing the performance of downstream tasks.', 'abstract_zh': '数字乳腺断层成像(DBT)图像中肿块病变的分割对乳腺癌早期筛查极为重要。然而，高密度乳腺组织常常导致肿块病变的高度隐蔽，使得手动标注困难且耗时。结果，缺乏用于模型训练的标注数据。扩散模型常用于数据增强，但现有方法面临两个挑战。首先，由于病变的高度隐蔽性，模型难以学习到病变区域的特征，导致病变区域生成质量低下，从而限制了生成图像的质量。其次，现有方法只能生成图像，而不能生成相应的标注，这限制了生成图像在监督训练中的使用。在本工作中，我们提出了一种配对图像生成方法。该方法不依赖外部条件，并通过为条件扩散模型训练额外的扩散引导器来实现配对图像的生成。在实验阶段，我们生成了配对的DBT切片和肿块病变掩膜，然后将它们融入肿块病变分割任务的监督训练过程中。实验结果表明，我们的方法可以在不依赖外部条件的情况下提高生成质量，并有助于缓解标注数据不足的问题，从而增强下游任务的性能。', 'title_zh': '基于扩散引导扩散模型的配对图像生成'}
{'arxiv_id': 'arXiv:2507.14807', 'title': 'Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection', 'authors': 'Juan Hu, Shaojing Fan, Terence Sim', 'link': 'https://arxiv.org/abs/2507.14807', 'abstract': 'Multi-face deepfake videos are becoming increasingly prevalent, often appearing in natural social settings that challenge existing detection methods. Most current approaches excel at single-face detection but struggle in multi-face scenarios, due to a lack of awareness of crucial contextual cues. In this work, we develop a novel approach that leverages human cognition to analyze and defend against multi-face deepfake videos. Through a series of human studies, we systematically examine how people detect deepfake faces in social settings. Our quantitative analysis reveals four key cues humans rely on: scene-motion coherence, inter-face appearance compatibility, interpersonal gaze alignment, and face-body consistency. Guided by these insights, we introduce \\textsf{HICOM}, a novel framework designed to detect every fake face in multi-face scenarios. Extensive experiments on benchmark datasets show that \\textsf{HICOM} improves average accuracy by 3.3\\% in in-dataset detection and 2.8\\% under real-world perturbations. Moreover, it outperforms existing methods by 5.8\\% on unseen datasets, demonstrating the generalization of human-inspired cues. \\textsf{HICOM} further enhances interpretability by incorporating an LLM to provide human-readable explanations, making detection results more transparent and convincing. Our work sheds light on involving human factors to enhance defense against deepfakes.', 'abstract_zh': '多臉深伪视频日益普及，常出现在挑战现有检测方法的自然社交环境中。当前大多数方法擅长单臉检测但在多臉场景中表现不佳，原因是缺乏对关键上下文线索的意识。本研究开发了一种新的方法，利用人类认知来分析和防范多臉深伪视频。通过一系列人类研究，我们系统地探讨了人们在社交环境中检测深伪 Faces 的方式。定量分析揭示了人类依赖的四个关键线索：场景运动一致性、Face 间外观一致性、人际凝视对齐和面部身体一致。根据这些洞察，我们提出了 \\textsf{HICOM}，一种新型框架，旨在在多臉场景中检测每一个假-face。基准数据集上的广泛实验表明，\\textsf{HICOM} 在内部检测中的平均准确率提高了 3.3%，在现实世界干扰下的准确率提高了 2.8%。此外，它在未见过的数据集上优于现有方法 5.8%，展示了人类启发式线索的泛化能力。\\textsf{HICOM} 进一步通过加入语言模型以提供人类可读的解释来增强可解释性，使检测结果更加透明和令人信服。我们的研究揭示了如何通过包含人类因素来增强对抗深伪视频的能力。', 'title_zh': '透过深度伪造：一种灵感源自人类的多脸检测框架'}
{'arxiv_id': 'arXiv:2507.14787', 'title': 'FOCUS: Fused Observation of Channels for Unveiling Spectra', 'authors': 'Xi Xiao, Aristeidis Tsaris, Anika Tabassum, John Lagergren, Larry M. York, Tianyang Wang, Xiao Wang', 'link': 'https://arxiv.org/abs/2507.14787', 'abstract': 'Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous wavelength bands, making it a powerful tool in biology, agriculture, and environmental monitoring. However, interpreting Vision Transformers (ViTs) in this setting remains largely unexplored due to two key challenges: (1) existing saliency methods struggle to capture meaningful spectral cues, often collapsing attention onto the class token, and (2) full-spectrum ViTs are computationally prohibitive for interpretability, given the high-dimensional nature of HSI data. We present FOCUS, the first framework that enables reliable and efficient spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core components: class-specific spectral prompts that guide attention toward semantically meaningful wavelength groups, and a learnable [SINK] token trained with an attraction loss to absorb noisy or redundant attention. Together, these designs make it possible to generate stable and interpretable 3D saliency maps and spectral importance curves in a single forward pass, without any gradient backpropagation or backbone modification. FOCUS improves band-level IoU by 15 percent, reduces attention collapse by over 40 percent, and produces saliency results that align closely with expert annotations. With less than 1 percent parameter overhead, our method makes high-resolution ViT interpretability practical for real-world hyperspectral applications, bridging a long-standing gap between black-box modeling and trustworthy HSI decision-making.', 'abstract_zh': '超光谱成像(HSI)捕获数百个连续的窄波长带，使其在生物学、农业和环境监测中成为一种强大的工具。然而，在这种背景下解释视力变换器(ViT)仍然是一个尚未探索的领域，主要由于两个关键挑战：(1)现有的显著性方法难以捕捉有意义的光谱线索，经常将注意力集中在类标记上；(2)全谱ViT由于HSI数据的高维特性，在可解释性方面计算上不可行。我们提出了FOCUS，这是首个能够实现冻结ViT可靠且高效的空-谱可解释性的框架。FOCUS引入了两个核心组件：类特定的谱提示，引导注意力聚焦于具有语义意义的波长群组，以及通过吸引损失进行学习的[SINK]标记，以吸收噪声或冗余的注意力。通过这些设计，可以生成稳定且可解释的3D显著性图和光谱重要性曲线，在单一前向传播过程中无需任何梯度反向传播或骨干网络修改。FOCUS将波段级交并比(IoU)提高了15%，减少了40%以上的注意力坍塌，并产生了与专家注解接近的结果。凭借不到1%的参数开销，我们的方法使高分辨率ViT在实际超光谱应用中的可解释性成为可能，填补了黑盒建模与值得信赖的HSI决策之间长期存在的鸿沟。', 'title_zh': 'FOCUS: 融合通道观测以揭示光谱'}
{'arxiv_id': 'arXiv:2507.14662', 'title': 'Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall', 'authors': 'Shayan Rokhva, Babak Teimourpour', 'link': 'https://arxiv.org/abs/2507.14662', 'abstract': 'Quantifying post-consumer food waste in institutional dining settings is essential for supporting data-driven sustainability strategies. This study presents a cost-effective computer vision framework that estimates plate-level food waste by utilizing semantic segmentation of RGB images taken before and after meal consumption across five Iranian dishes. Four fully supervised models (U-Net, U-Net++, and their lightweight variants) were trained using a capped dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a custom-defined Distributional Pixel Agreement (DPA) metric tailored to the task. All models achieved satisfying performance, and for each food type, at least one model approached or surpassed 90% DPA, demonstrating strong alignment in pixel-wise proportion estimates. Lighter models with reduced parameter counts offered faster inference, achieving real-time throughput on an NVIDIA T4 GPU. Further analysis showed superior segmentation performance for dry and more rigid components (e.g., rice and fries), while more complex, fragmented, or viscous dishes, such as stews, showed reduced performance, specifically post-consumption. Despite limitations such as reliance on 2D imaging, constrained food variety, and manual data collection, the proposed framework is pioneering and represents a scalable, contactless solution for continuous monitoring of food consumption. This research lays foundational groundwork for automated, real-time waste tracking systems in large-scale food service environments and offers actionable insights and outlines feasible future directions for dining hall management and policymakers aiming to reduce institutional food waste.', 'abstract_zh': '在机构餐饮环境中量化后消费者食物浪费对于支持数据驱动的可持续发展战略至关重要。本研究提出了一种成本有效的计算机视觉框架，通过利用RGB图像（餐前和餐后）的语义分割来估计盘级食物浪费，应用于五种伊朗菜肴。四种完全监督模型（U-Net、U-Net++及其轻量级变体）使用上限动态逆频率损失和AdamW优化器进行了训练，并通过包括像素准确性、Dice、IoU以及为任务定制的分布像素一致度（DPA）指标等全面评估指标进行了评估。所有模型均表现出令人满意的性能，对于每种食物类型，至少有一个模型的DPA接近或超过90%，显示出强烈的像素级比例估计的一致性。轻量级模型由于参数减少而具有更快的推断速度，在NVIDIA T4 GPU上实现了实时吞吐量。进一步分析显示，干燥和更刚性成分（如米饭和薯条）的分割性能更优，而更复杂的、碎片化的或粘稠的菜肴（如炖菜），在餐后表现出较低的性能。尽管存在依赖二维成像、食物品种受限和人工数据收集等局限性，所提出的框架仍具有开创性，并代表了一种可扩展、无接触的解决方案，用于大型餐饮服务环境中的连续食物消耗监测。本研究为自动、实时的浪费跟踪系统奠定了基础，并为餐饮管理机构和政策制定者提供了可操作的见解和可行的未来方向，以减少机构食物浪费。', 'title_zh': '人工智能在食品工业中的应用：基于计算机视觉的食品浪费估计——以大学食堂为例的简要案例研究'}
{'arxiv_id': 'arXiv:2507.14657', 'title': 'AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)', 'authors': 'Keivan Shariatmadar, Ahmad Osman', 'link': 'https://arxiv.org/abs/2507.14657', 'abstract': "The integration of Artificial Intelligence (AI) into sports officiating represents a paradigm shift in how decisions are made in competitive environments. Traditional manual systems, even when supported by Instant Video Replay (IVR), often suffer from latency, subjectivity, and inconsistent enforcement, undermining fairness and athlete trust. This paper introduces this http URL, a novel AI-powered framework designed to enhance officiating in Sport Taekwondo, particularly focusing on the complex task of real-time head kick detection and scoring. Leveraging computer vision, deep learning, and edge inference, the system automates the identification and classification of key actions, significantly reducing decision time from minutes to seconds while improving consistency and transparency. Importantly, the methodology is not limited to Taekwondo. The underlying framework -- based on pose estimation, motion classification, and impact analysis -- can be adapted to a wide range of sports requiring action detection, such as judo, karate, fencing, or even team sports like football and basketball, where foul recognition or performance tracking is critical. By addressing one of Taekwondo's most challenging scenarios -- head kick scoring -- we demonstrate the robustness, scalability, and sport-agnostic potential of this http URL to transform officiating standards across multiple disciplines.", 'abstract_zh': '将人工智能（AI）融入体育裁判代表着在竞争环境中决策方式的一场范式转变。传统的手动系统即使有即时视频回放（IVR）的支持，也常常面临延迟、主观性和执行不一致的问题，这损害了公平性和运动员的信任。本文介绍了一种新的基于AI的框架——this http URL，旨在增强跆拳道中的裁判工作，特别是专注于实时头部踢击检测和评分这一复杂任务。该系统利用计算机视觉、深度学习和边缘推断，自动识别和分类关键动作，显著减少决策时间，从几分钟缩短到几秒钟，同时提高一致性和透明度。重要的是，该方法不仅限于跆拳道。基于姿态估计、动作分类和冲击分析的底层框架，可以适应各种需要动作检测的体育运动，如柔道、空手道、击剑，甚至足球和篮球等团队运动，其中犯规识别或表现追踪至关重要。通过对跆拳道最具挑战性的场景——头部踢击评分——的解决，本文展示了this http URL的稳健性、可扩展性和体育运动无关的潜力，有望在多个学科中转型裁判标准。', 'title_zh': '基于AI的运动跆拳道精准技术：提升竞赛的公平性、速度和信任（FST.ai）'}
{'arxiv_id': 'arXiv:2507.14608', 'title': 'Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition', 'authors': 'Nandani Sharma, Dinesh Singh', 'link': 'https://arxiv.org/abs/2507.14608', 'abstract': "Facial expression recognition is crucial for human-computer interaction applications such as face animation, video surveillance, affective computing, medical analysis, etc. Since the structure of facial attributes varies with facial expressions, incorporating structural information into facial attributes is essential for facial expression recognition. In this paper, we propose Exp-Graph, a novel framework designed to represent the structural relationships among facial attributes using graph-based modeling for facial expression recognition. For facial attributes graph representation, facial landmarks are used as the graph's vertices. At the same time, the edges are determined based on the proximity of the facial landmark and the similarity of the local appearance of the facial attributes encoded using the vision transformer. Additionally, graph convolutional networks are utilized to capture and integrate these structural dependencies into the encoding of facial attributes, thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph learns from the facial attribute graphs highly expressive semantic representations. On the other hand, the vision transformer and graph convolutional blocks help the framework exploit the local and global dependencies among the facial attributes that are essential for the recognition of facial expressions. We conducted comprehensive evaluations of the proposed Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW. The model achieved recognition accuracies of 98.09\\%, 79.01\\%, and 56.39\\%, respectively. These results indicate that Exp-Graph maintains strong generalization capabilities across both controlled laboratory settings and real-world, unconstrained environments, underscoring its effectiveness for practical facial expression recognition applications.", 'abstract_zh': '基于图的面部表情识别框架Exp-Graph', 'title_zh': 'Exp-Graph: 基于图的表达识别中连接如何学习面部属性'}
{'arxiv_id': 'arXiv:2507.14592', 'title': 'A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification', 'authors': 'Haochen Liu, Jia Bi, Xiaomin Wang, Xin Yang, Ling Wang', 'link': 'https://arxiv.org/abs/2507.14592', 'abstract': 'Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance, logistics, agriculture, disaster management, and military operations. Accurate detection and classification of UAV flight states, such as hovering, cruising, ascending, or transitioning, which are essential for safe and effective operations. However, conventional time series classification (TSC) methods often lack robustness and generalization for dynamic UAV environments, while state of the art(SOTA) models like Transformers and LSTM based architectures typically require large datasets and entail high computational costs, especially with high-dimensional data streams. This paper proposes a novel framework that integrates a Transformer-based Generative Adversarial Network (GAN) with Multiple Instance Locally Explainable Learning (MILET) to address these challenges in UAV flight state classification. The Transformer encoder captures long-range temporal dependencies and complex telemetry dynamics, while the GAN module augments limited datasets with realistic synthetic samples. MIL is incorporated to focus attention on the most discriminative input segments, reducing noise and computational overhead. Experimental results show that the proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and 98.6% on the DroneRF dataset that outperforming other SOTA approaches. The framework also demonstrates strong computational efficiency and robust generalization across diverse UAV platforms and flight states, highlighting its potential for real-time deployment in resource constrained environments.', 'abstract_zh': '基于Transformer生成对抗网络与多实例局部可解释学习的无人机飞行状态分类框架', 'title_zh': '基于Transformer的条件生成对抗网络及其在多实例学习下的无人机信号检测与分类'}
{'arxiv_id': 'arXiv:2507.14587', 'title': 'Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX', 'authors': 'Merjem Bećirović, Amina Kurtović, Nordin Smajlović, Medina Kapo, Amila Akagić', 'link': 'https://arxiv.org/abs/2507.14587', 'abstract': 'Medical imaging plays a vital role in early disease diagnosis and monitoring. Specifically, blood microscopy offers valuable insights into blood cell morphology and the detection of hematological disorders. In recent years, deep learning-based automated classification systems have demonstrated high potential in enhancing the accuracy and efficiency of blood image analysis. However, a detailed performance analysis of specific deep learning frameworks appears to be lacking. This paper compares the performance of three popular deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in classifying blood cell images from the publicly available BloodMNIST dataset. The study primarily focuses on inference time differences, but also classification performance for different image sizes. The results reveal variations in performance across frameworks, influenced by factors such as image resolution and framework-specific optimizations. Classification accuracy for JAX and PyTorch was comparable to current benchmarks, showcasing the efficiency of these frameworks for medical image classification.', 'abstract_zh': '医学成像在早期疾病诊断和监测中发挥着重要作用。特别是血液显微镜可以提供关于血细胞形态和血液系统疾病检测的重要见解。近年来，基于深度学习的自动化分类系统在提高血液图像分析的准确性和效率方面显示出巨大潜力。然而，特定深度学习框架的详细性能分析似乎仍然不足。本文比较了三种流行的深度学习框架TensorFlow与Keras、PyTorch和JAX在分类公开可用的BloodMNIST数据集中的性能。研究主要关注推断时间差异，同时也考虑了不同图像大小的分类性能。结果揭示了不同框架之间的性能差异，这些差异受图像分辨率和框架特定优化等因素的影响。JAX和PyTorch的分类准确性与现有基准相当，展示了这些框架在医学图像分类中的效率。', 'title_zh': '使用TensorFlow Keras、PyTorch和JAX的医疗图像分类系统性能比较'}
{'arxiv_id': 'arXiv:2507.14575', 'title': 'Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation', 'authors': "Andrea Moschetto, Lemuel Puglisi, Alec Sargood, Pierluigi Dell'Acqua, Francesco Guarnera, Sebastiano Battiato, Daniele Ravì", 'link': 'https://arxiv.org/abs/2507.14575', 'abstract': 'Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering distinct diagnostic insights. However, acquiring all desired modalities increases scan time and cost, motivating research into computational methods for cross-modal synthesis. To address this, recent approaches aim to synthesize missing MRI contrasts from those already acquired, reducing acquisition time while preserving diagnostic quality. Image-to-image (I2I) translation provides a promising framework for this task. In this paper, we present a comprehensive benchmark of generative models$\\unicode{x2013}$specifically, Generative Adversarial Networks (GANs), diffusion models, and flow matching (FM) techniques$\\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All frameworks are implemented with comparable settings and evaluated on three publicly available MRI datasets of healthy adults. Our quantitative and qualitative analyses show that the GAN-based Pix2Pix model outperforms diffusion and FM-based methods in terms of structural fidelity, image quality, and computational efficiency. Consistent with existing literature, these results suggest that flow-based models are prone to overfitting on small datasets and simpler tasks, and may require more data to match or surpass GAN performance. These findings offer practical guidance for deploying I2I translation techniques in real-world MRI workflows and highlight promising directions for future research in cross-modal medical image synthesis. Code and models are publicly available at this https URL.', 'abstract_zh': '磁共振成像（MRI）能够获取多种图像对比度，如T1加权（T1w）和T2加权（T2w）扫描，每种对比度都提供了独特的诊断信息。然而，获取所有所需模态会增加扫描时间和成本，从而推动了跨模态合成的计算方法研究。为了解决这一问题，最近的方法试图从已获取的模态中合成缺失的MRI对比度，从而减少获取时间同时保持诊断质量。图像到图像（I2I）转换为这一任务提供了有前景的框架。在本文中，我们对生成模型——具体而言，生成对抗网络（GAN）、扩散模型和流匹配（FM）技术——在健康成人公开可用的MRI数据集上的T1w到T2w 2D MRI I2I转换进行了全面基准测试。所有框架都以相似的设置实现并进行评估。我们的定量和定性分析表明，基于GAN的Pix2Pix模型在结构保真度、图像质量和计算效率方面优于基于扩散和FM的方法。与现有文献一致，这些结果表明，基于流的概率模型容易在小数据集和简单任务上过拟合，并且可能需要更多数据来匹配或超越GAN的性能。这些发现为在实际MRI流程中部署I2I转换技术提供了实用指导，并强调了跨模态医学图像合成的未来研究方向。代码和模型可在以下网址获取：this https URL。', 'title_zh': 'GANs、扩散模型和流动匹配方法在T1w到T2w MRI转化中的性能基准研究'}
{'arxiv_id': 'arXiv:2507.14272', 'title': 'NuSeC: A Dataset for Nuclei Segmentation in Breast Cancer Histopathology Images', 'authors': 'Refik Samet, Nooshin Nemati, Emrah Hancer, Serpil Sak, Bilge Ayca Kirmizi', 'link': 'https://arxiv.org/abs/2507.14272', 'abstract': 'The NuSeC dataset is created by selecting 4 images with the size of 1024*1024 pixels from the slides of each patient among 25 patients. Therefore, there are a total of 100 images in the NuSeC dataset. To carry out a consistent comparative analysis between the methods that will be developed using the NuSeC dataset by the researchers in the future, we divide the NuSeC dataset 75% as the training set and 25% as the testing set. In detail, an image is randomly selected from 4 images of each patient among 25 patients to build the testing set, and then the remaining images are reserved for the training set. While the training set includes 75 images with around 30000 nuclei structures, the testing set includes 25 images with around 6000 nuclei structures.', 'abstract_zh': 'NuSeC数据集由从每位患者的一张切片中挑选出的4张1024×1024像素的图像组成，共计25位患者的切片。因此，NuSeC数据集总共包含100张图像。为了未来研究人员使用NuSeC数据集开发的方法进行一致的对比分析，我们将NuSeC数据集分为75%的训练集和25%的测试集。具体方法是从每位患者4张图像中随机选取一张构建测试集，剩余图像用于构建训练集。训练集包括约75张图像和约30000个核结构，测试集包括25张图像和约6000个核结构。', 'title_zh': 'NuSeC: 乳腺癌组织病理图像中的核分割数据集'}
