{'arxiv_id': 'arXiv:2502.11227', 'title': 'Integrating Retrospective Framework in Multi-Robot Collaboration', 'authors': 'Jiazhao Liang, Hao Huang, Yu Hao, Geeta Chandra Raju Bethala, Congcong Wen, John-Ross Rizzo, Yi Fang', 'link': 'https://arxiv.org/abs/2502.11227', 'abstract': 'Recent advancements in Large Language Models (LLMs) have demonstrated substantial capabilities in enhancing communication and coordination in multi-robot systems. However, existing methods often struggle to achieve efficient collaboration and decision-making in dynamic and uncertain environments, which are common in real-world multi-robot scenarios. To address these challenges, we propose a novel retrospective actor-critic framework for multi-robot collaboration. This framework integrates two key components: (1) an actor that performs real-time decision-making based on observations and task directives, and (2) a critic that retrospectively evaluates the outcomes to provide feedback for continuous refinement, such that the proposed framework can adapt effectively to dynamic conditions. Extensive experiments conducted in simulated environments validate the effectiveness of our approach, demonstrating significant improvements in task performance and adaptability. This work offers a robust solution to persistent challenges in robotic collaboration.', 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）在多机器人系统中展现出了显著的沟通和协调增强能力。然而，现有方法在动态和不确定环境中实现高效的协作和决策时常受到限制，这是真实世界多机器人场景中的常见问题。为应对这些挑战，我们提出了一种新的回溯演员-评论家框架，以促进多机器人协作。该框架整合了两个关键组成部分：（1）演员根据观察和任务指令进行实时决策；（2）评论家对结果进行回顾性评估，以提供反馈进行持续改进，从而使该框架能够有效适应动态条件。在模拟环境中的广泛实验验证了我们方法的有效性，展现了在任务性能和适应性方面的显著提升。本工作为机器人协作中的持续性挑战提供了稳健的解决方案。', 'title_zh': '将回顾框架集成到多机器人协作中'}
{'arxiv_id': 'arXiv:2502.10678', 'title': 'GenComUI: Exploring Generative Visual Aids as Medium to Support Task-Oriented Human-Robot Communication', 'authors': 'Yate Ge, Meiying Li, Xipeng Huang, Yuanda Hu, Qi Wang, Xiaohua Sun, Weiwei Guo', 'link': 'https://arxiv.org/abs/2502.10678', 'abstract': 'This work investigates the integration of generative visual aids in human-robot task communication. We developed GenComUI, a system powered by large language models that dynamically generates contextual visual aids (such as map annotations, path indicators, and animations) to support verbal task communication and facilitate the generation of customized task programs for the robot. This system was informed by a formative study that examined how humans use external visual tools to assist verbal communication in spatial tasks. To evaluate its effectiveness, we conducted a user experiment (n = 20) comparing GenComUI with a voice-only baseline. The results demonstrate that generative visual aids, through both qualitative and quantitative analysis, enhance verbal task communication by providing continuous visual feedback, thus promoting natural and effective human-robot communication. Additionally, the study offers a set of design implications, emphasizing how dynamically generated visual aids can serve as an effective communication medium in human-robot interaction. These findings underscore the potential of generative visual aids to inform the design of more intuitive and effective human-robot communication, particularly for complex communication scenarios in human-robot interaction and LLM-based end-user development.', 'abstract_zh': '本研究表明了生成性视觉辅助在人类-机器人任务通信中的整合应用。我们开发了由大型语言模型驱动的GenComUI系统，该系统能够动态生成上下文相关的视觉辅助（如地图标注、路径指示和动画），以支持言语任务通信并促进为机器人生成定制任务程序。该系统依据了前期研究的指导，该研究调查了人类如何利用外部视觉工具辅助空间任务中的言语沟通。为了评估其有效性，我们进行了一项用户实验（n=20），将GenComUI与语音基准进行了比较。结果表明，生成性视觉辅助通过提供持续的视觉反馈，提升了言语任务沟通的质量，促进了自然且有效的双向沟通。此外，该研究还提出了若干设计启示，强调了动态生成的视觉辅助作为人类-机器人交互有效沟通媒介的作用。这些发现突显了生成性视觉辅助在设计更直观和有效的人类-机器人沟通方面的潜力，尤其是在人类-机器人交互和基于LLM的最终用户开发中的复杂沟通场景中。', 'title_zh': 'GenComUI: 探索生成式视觉辅助作为支持任务导向的人机通信介质'}
{'arxiv_id': 'arXiv:2502.12143', 'title': 'Small Models Struggle to Learn from Strong Reasoners', 'authors': 'Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, Radha Poovendran', 'link': 'https://arxiv.org/abs/2502.12143', 'abstract': 'Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models ($\\leq$3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer.', 'abstract_zh': '小型模型可学习间隙：小型模型（≤3B参数）不一致地受益于长链式思考推理或从大模型中提炼的能力。相反，它们在与自身固有能力更好地对齐的简短、简单的推理链上进行微调时表现出更好的性能。为此，我们提出了一种简单的有效策略——混合提炼（Mix Distillation），该策略通过结合长和短链式思考推理示例或从大小不同的模型中提取的推理来平衡推理复杂性。我们的实验表明，与仅使用数据训练相比，混合提炼显著提高了小型模型的推理性能。这些发现突显了直接强模型提炼的局限性，并强调了适应推理复杂性的重要性，以实现有效的推理能力转移。', 'title_zh': '小模型难以从强推理者学习'}
{'arxiv_id': 'arXiv:2502.12131', 'title': 'Transformer Dynamics: A neuroscientific approach to interpretability of large language models', 'authors': 'Jesseba Fernando, Grigori Guitchounts', 'link': 'https://arxiv.org/abs/2502.12131', 'abstract': 'As artificial intelligence models have exploded in scale and capability, understanding of their internal mechanisms remains a critical challenge. Inspired by the success of dynamical systems approaches in neuroscience, here we propose a novel framework for studying computations in deep learning systems. We focus on the residual stream (RS) in transformer models, conceptualizing it as a dynamical system evolving across layers. We find that activations of individual RS units exhibit strong continuity across layers, despite the RS being a non-privileged basis. Activations in the RS accelerate and grow denser over layers, while individual units trace unstable periodic orbits. In reduced-dimensional spaces, the RS follows a curved trajectory with attractor-like dynamics in the lower layers. These insights bridge dynamical systems theory and mechanistic interpretability, establishing a foundation for a "neuroscience of AI" that combines theoretical rigor with large-scale data analysis to advance our understanding of modern neural networks.', 'abstract_zh': '随着人工智能模型在规模和能力上迅速扩张，对其内部机制的理解仍是一项关键挑战。受神经科学中动力系统方法成功应用的启发，我们提出了一种新的框架来研究深度学习系统的计算过程。我们重点关注变压器模型中的剩余流（RS），将其视为在各层之间演化的一种动力系统。我们发现，个体RS单元的激活在整个层间表现出强烈的连续性，尽管RS并不是一个特权基。在层间，RS中的激活加速并变得越来越密集，而个体单元则遵循不稳定的周期轨道。在降维空间中，RS在较低层表现出类似吸引子的动力学，沿着一条弯曲的路径。这些见解将动力系统理论与机械解释学相结合，为一种结合理论严谨性和大规模数据分析的“人工智能神经科学”奠定了基础，以增进我们对现代神经网络的理解。', 'title_zh': 'Transformer 动态研究：一种神经科学方法解释大规模语言模型的可解释性'}
{'arxiv_id': 'arXiv:2502.12130', 'title': 'Scaling Autonomous Agents via Automatic Reward Modeling And Planning', 'authors': 'Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, Chuang Gan', 'link': 'https://arxiv.org/abs/2502.12130', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities across a range of text-generation tasks. However, LLMs still struggle with problems requiring multi-step decision-making and environmental feedback, such as online shopping, scientific reasoning, and mathematical problem-solving. Unlike pure text data, collecting large-scale decision-making data is challenging. Moreover, many powerful LLMs are only accessible through APIs, which hinders their fine-tuning for agent tasks due to cost and complexity. To address LLM agents' limitations, we propose a framework that can automatically learn a reward model from the environment without human annotations. This model can be used to evaluate the action trajectories of LLM agents and provide heuristics for task planning. Specifically, our approach involves employing one LLM-based agent to navigate an environment randomly, generating diverse action trajectories. Subsequently, a separate LLM is leveraged to assign a task intent and synthesize a negative response alongside the correct response for each trajectory. These triplets (task intent, positive response, and negative response) are then utilized as training data to optimize a reward model capable of scoring action trajectories. The effectiveness and generalizability of our framework are demonstrated through evaluations conducted on different agent benchmarks. In conclusion, our proposed framework represents a significant advancement in enhancing LLM agents' decision-making capabilities. By automating the learning of reward models, we overcome the challenges of data scarcity and API limitations, potentially revolutionizing the application of LLMs in complex and interactive environments. This research paves the way for more sophisticated AI agents capable of tackling a wide range of real-world problems requiring multi-step decision-making.", 'abstract_zh': '大规模语言模型（LLMs）在多种文本生成任务中展现了卓越的能力。然而，LLMs 在要求多步决策和环境反馈的问题上仍存在局限，例如在线购物、科学推理和数学问题求解。与纯粹的文字数据不同，收集大规模的决策数据极具挑战性。此外，许多强大的LLMs是通过API访问的，这因其成本和复杂性而阻碍了它们在代理任务上的微调。为解决LLM代理的局限性，我们提出了一种框架，该框架可以在不依赖人工标注的情况下自动学习奖励模型。该模型可以用于评估LLM代理的动作轨迹，并为任务规划提供启发式方法。具体而言，我们的方法包括使用一个基于LLM的代理随机导航环境，生成多样化的动作轨迹。随后，利用另一个单独的LLM分配任务意图，并为每个轨迹合成了正确的响应及其负响应。这些三元组（任务意图、正响应和负响应）用作训练数据，以优化能够评估动作轨迹得分的奖励模型。通过对不同的代理基准进行评估，证明了该框架的有效性和普适性。总之，我们提出的框架在提升LLM代理的决策能力方面取得了重要进展。通过自动化奖励模型的学习，我们克服了数据稀缺和API限制的挑战，有可能变革LLM在复杂和交互式环境中的应用。这项研究为能够解决多种需要多步决策的现实世界问题的更高级AI代理铺平了道路。', 'title_zh': '通过自动奖励建模与规划扩展自主代理'}
{'arxiv_id': 'arXiv:2502.12066', 'title': 'CONSTRUCTA: Automating Commercial Construction Schedules in Fabrication Facilities with Large Language Models', 'authors': 'Yifan Zhang, Xue Yang', 'link': 'https://arxiv.org/abs/2502.12066', 'abstract': 'Automating planning with LLMs presents transformative opportunities for traditional industries, yet remains underexplored. In commercial construction, the complexity of automated scheduling often requires manual intervention to ensure precision. We propose CONSTRUCTA, a novel framework leveraging LLMs to optimize construction schedules in complex projects like semiconductor fabrication. CONSTRUCTA addresses key challenges by: (1) integrating construction-specific knowledge through static RAG; (2) employing context-sampling techniques inspired by architectural expertise to provide relevant input; and (3) deploying Construction DPO to align schedules with expert preferences using RLHF. Experiments on proprietary data demonstrate performance improvements of +42.3% in missing value prediction, +79.1% in dependency analysis, and +28.9% in automated planning compared to baseline methods, showcasing its potential to revolutionize construction workflows and inspire domain-specific LLM advancements.', 'abstract_zh': '利用大语言模型自动化施工规划在传统行业中的应用虽具有变革性机会但仍未充分探索。CONSTRUCTA：一种利用大语言模型优化半导体 fabrication 施工计划的新型框架', 'title_zh': 'CONSTRUCTA: 使用大型语言模型在生产设施中自动化建筑施工调度'}
{'arxiv_id': 'arXiv:2502.12029', 'title': 'KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths over Knowledge Graphs', 'authors': 'Qi Zhao, Hongyu Yang, Qi Song, Xinwei Yao, Xiangyang Li', 'link': 'https://arxiv.org/abs/2502.12029', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities in various complex tasks, yet they still suffer from hallucinations. Introducing external knowledge, such as knowledge graph, can enhance the LLMs' ability to provide factual answers. LLMs have the ability to interactively explore knowledge graphs. However, most approaches have been affected by insufficient internal knowledge excavation in LLMs, limited generation of trustworthy knowledge reasoning paths, and a vague integration between internal and external knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large model framework driven by the collaboration of internal and external knowledge. It relies on the internal knowledge of the LLM to guide the exploration of interpretable directed subgraphs in external knowledge graphs, better integrating the two knowledge sources for more accurate reasoning. Extensive experiments on multiple real-world datasets confirm the superiority of KnowPath.", 'abstract_zh': '大型语言模型（LLMs）在各种复杂任务中展现了卓越的能力，但仍存在幻觉问题。引入外部知识，如知识图谱，可以增强LLMs提供事实性答案的能力。LLMs具备与外部知识图谱进行交互式探索的能力。然而，大多数方法受到LLMs内部知识发掘不足、可信知识推理路径生成有限以及内部与外部知识融合不清的限制。因此，我们提出了一种由内部和外部知识协作驱动的知识增强大型模型框架KnowPath。该框架利用LLMs的内部知识指导对外部知识图谱中可解释定向子图的探索，更好地整合两种知识来源以进行更准确的推理。在多个真实世界数据集上的广泛实验验证了KnowPath的优越性。', 'title_zh': '知路径：通过LLM生成的推理路径在知识图编辑中的知识增强推理'}
{'arxiv_id': 'arXiv:2502.12025', 'title': 'SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities', 'authors': 'Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, Radha Poovendran', 'link': 'https://arxiv.org/abs/2502.12025', 'abstract': 'Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.', 'abstract_zh': '新兴的大推理模型（LRMs），如DeepSeek-R1模型，通过长推理链（CoT）增强其推理能力并生成结构化的中间步骤。然而，长CoT并不必然保证输出的安全性，可能会导致安全漏洞的引入或错误信息的传播。当前对大型语言模型（LLMs）安全性的研究通常集中在短答案响应上，忽视了LRMs的长CoT风格输出。为填补这一空白，我们进行了系统的LRM安全性研究。首先，我们研究了与人类注释校准的安全评估器。使用我们新开发的指标，我们全面评估了12个最先进的LRMs在StrongReject和WildJailbreak数据集上的安全性。结果显示，LRMs的安全性并未随着其推理能力的提升而提高。进一步，我们对推理轨迹和最终答案进行了细致分析。我们发现，三种解码策略——ZeroThink、LessThink和MoreThink——可以在不额外训练的情况下提高模型安全性，但这些策略要么受限于推理轨迹，要么会产生高昂的推理成本。为了更好地增强LRM的安全性，我们引入了SafeChain，这是首个用于CoT风格的安全训练数据集。我们对两个LRM进行微调，结果显示，SafeChain不仅增强了模型安全性，还跨六个推理基准保持了性能。', 'title_zh': 'SafeChain: 具有长链条思考推理能力的语言模型的安全性'}
{'arxiv_id': 'arXiv:2502.11882', 'title': 'Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration', 'authors': 'Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen', 'link': 'https://arxiv.org/abs/2502.11882', 'abstract': "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in this https URL.", 'abstract_zh': '基于大型语言模型的代理在轮流的人工智能协作中表现出色，但在需要实时交互的并行任务中挣扎。延迟问题和推断多变的人类策略的挑战阻碍了它们在没有明确指令的情况下做出自主决策的能力。通过使用当前独立的System 1和System 2方法的实验，我们验证了在实时任务中使用双重过程理论（DPT）的必要性。我们提出了DPT-Agent，一种新型的语言代理框架，将System 1和System 2集成以实现高效的实时并行人机协作。DPT-Agent的System 1使用有限状态机（FSM）和代码作为策略，实现快速、直观和可控的决策。DPT-Agent的System 2结合了心理理论（ToM）和异步反思，在推断人类意图和进行推理为基础的自主决策方面发挥作用。通过进一步的基于规则的代理和人类合作者的实验，展示了DPT-Agent的有效性，显示出比主流基于LLM的框架有显著改进。据我们所知，DPT-Agent是第一个实现真正实时并行人机协作的自主语言代理框架。DPT-Agent的代码可以在以下链接找到：this https URL。', 'title_zh': '基于双过程理论的语言代理框架在实时人机协作中的应用'}
{'arxiv_id': 'arXiv:2502.11881', 'title': 'Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models', 'authors': 'Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B. Tenenbaum, Yejin Choi', 'link': 'https://arxiv.org/abs/2502.11881', 'abstract': "Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce thought-tracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions. We evaluate thought-tracing on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs. Our experiments also reveal interesting behaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind, highlighting the difference of social reasoning compared to other domains.", 'abstract_zh': '已有的大语言模型推理方法在各类任务中展现了显著的能力，如解决数学和编程问题。然而，将这些方法应用于缺乏确切答案或基于规则验证方法的场景——例如追踪智能体的心理状态——仍然具有挑战性。受序列蒙特卡罗算法的启发，我们提出了思维追踪算法，这是一种推理时的推理算法，旨在通过生成假设并根据观察结果对这些假设进行加权，而不依赖于数据集中问题的确切答案来追踪特定智能体的心理状态。该算法借鉴了贝叶斯心因理论框架，利用大语言模型基于智能体的感知和行为对其心理状态的演变进行概率性推理。我们在此算法上对多种心理理论基准进行了评估，相比基础的大语言模型显示了显著性能提升。我们的实验还揭示了最近的推理模型（如o1和R1）在心理理论方面的一些有趣行为，突出了社交推理与其他领域的不同。', 'title_zh': '基于假设驱动的心理理论推理的大语言模型'}
{'arxiv_id': 'arXiv:2502.11770', 'title': 'Cognitive-Aligned Document Selection for Retrieval-augmented Generation', 'authors': 'Bingyu Wan, Fuxi Zhang, Zhongpeng Qi, Jiayi Ding, Jijun Li, Baoshi Fan, Yijia Zhang, Jun Zhang', 'link': 'https://arxiv.org/abs/2502.11770', 'abstract': "Large language models (LLMs) inherently display hallucinations since the precision of generated texts cannot be guaranteed purely by the parametric knowledge they include. Although retrieval-augmented generation (RAG) systems enhance the accuracy and reliability of generative models by incorporating external documents, these retrieved documents often fail to adequately support the model's responses in practical applications. To address this issue, we propose GGatrieval (Fine-\\textbf{G}rained \\textbf{G}rounded \\textbf{A}lignment Re\\textbf{trieval} for verifiable generation), which leverages an LLM to dynamically update queries and filter high-quality, reliable retrieval documents. Specifically, we parse the user query into its syntactic components and perform fine-grained grounded alignment with the retrieved documents. For query components that cannot be individually aligned, we propose a dynamic semantic compensation mechanism that iteratively refines and rewrites the query while continuously updating the retrieval results. This iterative process continues until the retrieved documents sufficiently support the query's response. Our approach introduces a novel criterion for filtering retrieved documents, closely emulating human strategies for acquiring targeted information. This ensures that the retrieved content effectively supports and verifies the generated outputs. On the ALCE benchmark, our method significantly surpasses a wide range of baselines, achieving state-of-the-art performance.", 'abstract_zh': '大型语言模型(LLMs)固有地表现出幻觉现象，因为生成文本的精确性不能仅由其包含的参数知识保证。尽管检索增强生成(RAG)系统通过引入外部文档来提升生成模型的准确性和可靠性，但这些检索到的文档在实际应用中往往无法充分支持模型的响应。为了解决这一问题，我们提出了一种GGatrieval（细粒度 Grounded Alignment Retrieval，用于可验证生成的精炼检索），利用LLM动态更新查询并筛选高质量、可靠的检索文档。具体来说，我们将用户查询解析为其语法成分，并与检索到的文档进行细粒度的匹配。对于无法单独匹配的查询成分，我们提出了一种动态语义补偿机制，该机制不断细化和重写查询并持续更新检索结果。这一迭代过程将持续进行，直到检索到的文档能够充分支持查询的响应。我们的方法引入了过滤检索文档的新标准，该标准紧密模仿了人类获取目标信息的策略，确保检索内容有效地支持和验证生成输出。在ALCE基准上，我们的方法显著超越了广泛的基础方法，达到了最先进的性能。', 'title_zh': '认知对齐的文档选择用于检索增强生成'}
{'arxiv_id': 'arXiv:2502.11723', 'title': 'Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on GPU Energy Consumption', 'authors': 'Alireza Nik, Michael A. Riegler, Pål Halvorsen', 'link': 'https://arxiv.org/abs/2502.11723', 'abstract': 'Decoding strategies significantly influence the quality and diversity of the generated texts in large language models (LLMs), yet their impact on computational resource consumption, particularly GPU energy usage, is insufficiently studied. This paper investigates the relationship between text generation decoding methods and energy efficiency, focusing on the trade-off between generation quality and GPU energy consumption across diverse tasks and decoding configurations. By benchmarking multiple strategies across different text generation tasks, such as Translation, Code Summarization, and Math Problem Solving, we reveal how selecting appropriate decoding techniques with their tuned hyperparameters affects text quality and has measurable implications for resource utilization, emphasizing the need for balanced optimization. To the best of our knowledge, this study is among the first to explore decoding strategies in LLMs through the lens of energy consumption, offering actionable insights for designing resource-aware applications that maintain high-quality text generation.', 'abstract_zh': '大型语言模型（LLMs）中的解码策略显著影响生成文本的质量和多样性，但它们对计算资源消耗，特别是GPU能耗的影响研究尚不足。本文探讨了文本生成解码方法与能效之间的关系，重点关注生成质量和GPU能耗之间的权衡，涵盖多样化的任务和不同的解码配置。通过在翻译、代码摘要和数学问题解决等多种文本生成任务中对比多种策略，我们揭示了选择合适的解码技术及其调优超参数如何影响文本质量和资源利用，并强调了平衡优化的必要性。据我们所知，这是首次从能耗角度研究LLMs中的解码策略，为设计兼顾高质量文本生成的应用程序提供了可操作的见解。', 'title_zh': '节能导向的LLM解码：文本生成策略对GPU能耗的影响'}
{'arxiv_id': 'arXiv:2502.11664', 'title': 'VRoPE: Rotary Position Embedding for Video Large Language Models', 'authors': 'Zikang Liu, Longteng Guo, Yepeng Tang, Junxian Cai, Kai Ma, Xi Chen, Jing Liu', 'link': 'https://arxiv.org/abs/2502.11664', 'abstract': 'Rotary Position Embedding (RoPE) has shown strong performance in text-based Large Language Models (LLMs), but extending it to video remains a challenge due to the intricate spatiotemporal structure of video frames. Existing adaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions separately but suffer from two major limitations: positional bias in attention distribution and disruptions in video-text transitions. To overcome these issues, we propose Video Rotary Position Embedding (VRoPE), a novel positional encoding method tailored for Video-LLMs. Our approach restructures positional indices to preserve spatial coherence and ensure a smooth transition between video and text tokens. Additionally, we introduce a more balanced encoding strategy that mitigates attention biases, ensuring a more uniform distribution of spatial focus. Extensive experiments on Vicuna and Qwen2 across different model scales demonstrate that VRoPE consistently outperforms previous RoPE variants, achieving significant improvements in video understanding, temporal reasoning, and retrieval tasks. Code will be available at this https URL', 'abstract_zh': 'Video Rotary Position Embedding (VRoPE)：一种针对Video-LLMs的新型位置编码方法', 'title_zh': 'VRoPE: 旋转位置嵌入视频大型语言模型'}
{'arxiv_id': 'arXiv:2502.11649', 'title': 'Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation', 'authors': 'Amin Qasmi, Usman Naseem, Mehwish Nasim', 'link': 'https://arxiv.org/abs/2502.11649', 'abstract': "We introduce a novel non-cooperative game to analyse opinion formation and resistance, incorporating principles from social psychology such as confirmation bias, resource constraints, and influence penalties. Our simulation features Large Language Model (LLM) agents competing to influence a population, with penalties imposed for generating messages that propagate or counter misinformation. This framework integrates resource optimisation into the agents' decision-making process. Our findings demonstrate that while higher confirmation bias strengthens opinion alignment within groups, it also exacerbates overall polarisation. Conversely, lower confirmation bias leads to fragmented opinions and limited shifts in individual beliefs. Investing heavily in a high-resource debunking strategy can initially align the population with the debunking agent, but risks rapid resource depletion and diminished long-term influence.", 'abstract_zh': '我们介绍了一种新的非合作性博弈，用于分析意见形成和抵制行为，结合了社会心理学原理，如确认偏差、资源限制和影响惩罚。我们的模拟中，大型语言模型代理竞争以影响人群，对传播或抵制错误信息的代理施加惩罚。该框架将资源优化纳入代理的决策过程中。研究发现，虽然更高的确认偏差加强了团体内的意见一致性，但也加剧了总体上的极化。相反，较低的确认偏差会导致意见碎片化和个人信念有限的变化。大量投资高资源验证策略可以初步使人群与验证代理一致，但存在资源快速耗尽和长期影响减弱的风险。', 'title_zh': '竞猜LLM代理在意见极化非合作博弈中的竞争'}
{'arxiv_id': 'arXiv:2502.11574', 'title': 'Large Language Models and Mathematical Reasoning Failures', 'authors': 'Johan Boye, Birger Moell', 'link': 'https://arxiv.org/abs/2502.11574', 'abstract': "This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies that focus solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning failures. Evaluating eight state-of-the-art models - including Mixtral, Llama, Gemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models (e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers through flawed logic. Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and difficulty translating physical intuition into mathematical steps. Manual analysis reveals that models struggle with problems requiring multi-step deduction or real-world knowledge, despite possessing broad mathematical knowledge. Our results underscore the importance of evaluating reasoning processes, not just answers, and caution against overestimating LLMs' problem-solving proficiency. The study highlights persistent gaps in LLMs' generalization abilities, emphasizing the need for targeted improvements in structured reasoning and constraint handling.", 'abstract_zh': '本文利用50个新构建的高中水平词语问题，探讨了大语言模型的数学推理能力。不同于以往仅关注答案正确性的研究，我们严格分析了最终答案和解题步骤，以识别推理错误。评估了八种最先进的模型，包括Mixtral、Llama、Gemini、GPT-4o以及OpenAI的o1变体，发现虽然较新模型（如o3-mini、deepseek-r1）在准确率上更高，但所有模型在空间推理、战略规划和算术运算方面都出现了错误，有时会通过错误的逻辑得出正确答案。常见的错误模式包括无根据的假设、过度依赖数字模式以及将物理直觉转化为数学步骤的困难。手动分析表明，尽管模型拥有广泛的数学知识，它们在需要多步推理或实际知识的问题上仍存在问题。研究结果强调了评估推理过程而非仅仅答案的重要性，并警告不要高估LLM的解决问题能力。该研究突出了LLMs在一般化能力上的持续差距，强调了在结构化推理和约束处理方面进行针对性改进的必要性。', 'title_zh': '大型语言模型在数学推理中的失败'}
{'arxiv_id': 'arXiv:2502.11555', 'title': 'Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models', 'authors': 'Yingshui Tan, Yilei Jiang, Yanshi Li, Jiaheng Liu, Xingyuan Bu, Wenbo Su, Xiangyu Yue, Xiaoyong Zhu, Bo Zheng', 'link': 'https://arxiv.org/abs/2502.11555', 'abstract': "Fine-tuning large language models (LLMs) based on human preferences, commonly achieved through reinforcement learning from human feedback (RLHF), has been effective in improving their performance. However, maintaining LLM safety throughout the fine-tuning process remains a significant challenge, as resolving conflicts between safety and helpfulness can be non-trivial. Typically, the safety alignment of LLM is trained on data with safety-related categories. However, our experiments find that naively increasing the scale of safety training data usually leads the LLMs to an ``overly safe'' state rather than a ``truly safe'' state, boosting the refusal rate through extensive safety-aligned data without genuinely understanding the requirements for safe responses. Such an approach can inadvertently diminish the models' helpfulness. To understand the phenomenon, we first investigate the role of safety data by categorizing them into three different groups, and observe that each group behaves differently as training data scales up. To boost the balance between safety and helpfulness, we propose an Equilibrate RLHF framework including a Fine-grained Data-centric (FDC) approach that achieves better safety alignment even with fewer training data, and an Adaptive Message-wise Alignment (AMA) approach, which selectively highlight the key segments through a gradient masking strategy. Extensive experimental results demonstrate that our approach significantly enhances the safety alignment of LLMs while balancing safety and helpfulness.", 'abstract_zh': '基于人类偏好的大语言模型（LLMs）微调，通常通过人类反馈强化学习（RLHF）实现，已在提升模型性能方面表现出效用。然而，在微调过程中保持LLM安全始终是一项重大挑战，因为解决安全与 helpfulness 之间的冲突并不简单。通常，LLM的安全对齐是在包含安全相关类别的数据上训练的。然而，我们的实验发现，简单地增加安全训练数据的规模通常会导致LLM进入一个“过度安全”的状态，而不是“真正安全”的状态，从而通过大量安全对齐的数据增加了拒绝率，而没有真正理解安全响应的要求。这种做法可能会无意中削弱模型的帮助性。为了理解这一现象，我们首先通过将安全数据分类为三个不同的组来研究其作用，并观察每个组在训练数据规模增加时的行为表现不同。为了提高安全与帮助性的平衡，我们提出了一种平衡RLHF框架（Equilibrate RLHF），其中包括一种细粒度数据为中心的方法（FDC），即使在较少的训练数据情况下也能实现更好的安全对齐，以及一种自适应消息层面对齐（AMA）方法，通过梯度屏蔽策略突出关键段落。广泛的实验证明，我们的方法能在提高LLM安全对齐的同时，更好地平衡安全与帮助性。', 'title_zh': '平衡RLHF：在大型语言模型中实现帮助性与安全性权衡的平衡'}
{'arxiv_id': 'arXiv:2502.11528', 'title': 'A Survey of Personalized Large Language Models: Progress and Future Directions', 'authors': 'Jiahong Liu, Zexuan Qiu, Zhongyang Li, Quanyu Dai, Jieming Zhu, Minda Hu, Menglin Yang, Irwin King', 'link': 'https://arxiv.org/abs/2502.11528', 'abstract': "Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research. Updated information about this survey can be found at the this https URL.", 'abstract_zh': '大规模语言模型（LLMs）在处理通用知识任务方面表现出色，但在用户特定的个性化方面存在困难，如理解个体情感、写作风格和偏好。个性化大规模语言模型（PLLMs）通过利用个体用户数据，如用户资料、历史对话、内容和互动，来提供与上下文相关且符合每位用户特定需求的回应。这是一个极具价值的研究话题，因为PLLMs能够显著提升用户体验，并在对话代理、推荐系统、情绪识别、医疗助手等领域具有广泛的应用前景。本文从三个技术视角回顾了PLLMs的最新进展：个性化上下文提示（输入层面）、个性化适配器微调（模型层面）和个性化偏好对齐（目标层面）。为了提供更深入的见解，我们也讨论了当前的局限性，并概述了几条具有前景的研究方向。有关本综述的更新信息，请访问 <https://www.example.com>。', 'title_zh': '个性化大型语言模型综述：进展与未来方向'}
{'arxiv_id': 'arXiv:2502.11448', 'title': 'AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection', 'authors': 'Weidi Luo, Shenghong Dai, Xiaogeng Liu, Suman Banerjee, Huan Sun, Muhao Chen, Chaowei Xiao', 'link': 'https://arxiv.org/abs/2502.11448', 'abstract': "The rapid advancements in Large Language Models (LLMs) have enabled their deployment as autonomous agents for handling complex tasks in dynamic environments. These LLMs demonstrate strong problem-solving capabilities and adaptability to multifaceted scenarios. However, their use as agents also introduces significant risks, including task-specific risks, which are identified by the agent administrator based on the specific task requirements and constraints, and systemic risks, which stem from vulnerabilities in their design or interactions, potentially compromising confidentiality, integrity, or availability (CIA) of information and triggering security risks. Existing defense agencies fail to adaptively and effectively mitigate these risks. In this paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent safety, which features adaptive safety check generation, effective safety check optimization, and tool compatibility and flexibility. Extensive experiments demonstrate that AGrail not only achieves strong performance against task-specific and system risks but also exhibits transferability across different LLM agents' tasks.", 'abstract_zh': '大型语言模型的迅速 advancement 使其实现作为自主代理在动态环境中处理复杂任务的部署。这些大型语言模型展示了强大的问题解决能力并能够适应多种多样的情境。然而，作为代理使用也带来了显著的风险，包括任务特定风险和系统风险。任务特定风险由代理管理员根据特定任务需求和约束识别，系统风险则源于设计或交互中的漏洞，可能危及信息的机密性、完整性和可用性（CIA），从而引发安全风险。现有的防御机构无法适应性且有效地下降这些风险。在本文中，我们提出 AGrail，一种终身代理护栏，旨在增强大型语言模型代理的安全性，其特点是适应性的安全性检查生成、有效安全性检查优化以及工具的兼容性和灵活性。广泛的实验结果表明，AGrail 不仅在应对任务特定风险和系统风险方面表现出色，而且还展示了在不同大型语言模型任务之间的可迁移性。', 'title_zh': 'AGrail: 一种有效的自适应安全检测终生智能体守护rails'}
{'arxiv_id': 'arXiv:2502.11435', 'title': 'SMART: Self-Aware Agent for Tool Overuse Mitigation', 'authors': 'Cheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-Tür, Gokhan Tur, Heng Ji', 'link': 'https://arxiv.org/abs/2502.11435', 'abstract': "Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to Tool Overuse, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent's self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce SMART-ER, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop SMARTAgent, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.", 'abstract_zh': '当前大型语言模型（LLM）代理展现了强大的推理和工具使用能力，但往往缺乏自我意识，无法有效平衡这些方法。这种不平衡导致了工具过度使用，模型在可以通过参数化知识解决的任务中无必要地依赖外部工具，增加了计算开销。受到人类元认知的启发，我们引入了SMART（Strategic Model-Aware Reasoning with Tools）范式，增强代理的自我意识以优化任务处理并减少工具过度使用。为支持这一范式，我们引入了SMART-ER数据集，该数据集覆盖了三个领域，在推理过程中交替使用参数化知识和工具依赖步骤，并且每一步都通过解释何时需要工具的说明来丰富。通过监督训练，我们开发了SMARTAgent模型家族，能够动态平衡参数化知识和工具使用。评估结果显示，SMARTAgent在减少了24%工具使用的同时改进了性能超过37%，使7B规模的模型能够与70B规模的模型和GPT-4o相媲美。此外，SMARTAgent能够泛化到如GSM8K和MINTQA等分布外测试数据中，仅使用五分之一的工具调用就能保持准确率。这些结果突显了战略性工具使用在增强推理、减轻过度使用以及缩小模型规模与性能差距方面的潜力，促进了智能和资源高效代理设计的进步。', 'title_zh': 'SMART：自我意识代理工具滥用缓解'}
{'arxiv_id': 'arXiv:2502.11433', 'title': '\\textsc{FLAG-Trader}: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading', 'authors': 'Guojun Xiong, Zhiyang Deng, Keyi Wang, Yupeng Cao, Haohang Li, Yangyang Yu, Xueqing Peng, Mingquan Lin, Kaleb E Smith, Xiao-Yang Liu, Jimin Huang, Sophia Ananiadou, Qianqian Xie', 'link': 'https://arxiv.org/abs/2502.11433', 'abstract': 'Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose \\textsc{FLAG-Trader}, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.', 'abstract_zh': '基于多模态金融数据fine-tuned的大语言模型在多种金融任务中展现了出色的推理能力。然而，在互动金融市场如交易等多步、目标导向的情境中，它们往往难以应对复杂的代理方法以提升决策质量。为解决这一问题，我们提出了一种统一架构\\textsc{FLAG-Trader}，该架构将基于大语言模型的语言处理与基于梯度的强化学习策略优化相结合，在这种架构中，部分fine-tuned的大语言模型作为策略网络，利用预训练知识并通过对金融领域的参数高效fine-tuning来适应该领域。通过基于交易奖励的策略梯度优化，我们的框架不仅提高了大语言模型在交易方面的性能，还提升了其他金融领域任务的结果。我们提供了大量的实证证据来验证这些增强效果。', 'title_zh': 'FLAG-Trader: 基于梯度强化学习的LLM-Agents融合模型在金融交易中的应用'}
{'arxiv_id': 'arXiv:2502.11422', 'title': 'Planning of Heuristics: Strategic Planning on Large Language Models with Monte Carlo Tree Search for Automating Heuristic Optimization', 'authors': 'Chaoxu Mu, Xufeng Zhang, Hui Wang', 'link': 'https://arxiv.org/abs/2502.11422', 'abstract': 'Heuristics have achieved great success in solv- ing combinatorial optimization problems (COPs). However, heuristics designed by humans re- quire too much domain knowledge and testing time. Given the fact that Large Language Mod- els (LLMs) possess strong capabilities to under- stand and generate content, and a knowledge base that covers various domains, which offer a novel way to automatically optimize heuristics. There- fore, we propose Planning of Heuristics (PoH), an optimization method that integrates the self- reflection of LLMs with the Monte Carlo Tree Search (MCTS), a well-known planning algo- rithm. PoH iteratively refines generated heuristics by evaluating their performance and providing im- provement suggestions. Our method enables to it- eratively evaluate the generated heuristics (states) and improve them based on the improvement sug- gestions (actions) and evaluation results (rewards), by effectively simulating future states to search for paths with higher rewards. In this paper, we apply PoH to solve the Traveling Salesman Prob- lem (TSP) and the Flow Shop Scheduling Prob- lem (FSSP). The experimental results show that PoH outperforms other hand-crafted heuristics and Automatic Heuristic Design (AHD) by other LLMs-based methods, and achieves the signifi- cant improvements and the state-of-the-art per- formance of our proposed method in automating heuristic optimization with LLMs to solve COPs.', 'abstract_zh': '基于大型语言模型的启发式规划方法PoH', 'title_zh': '基于蒙特卡罗树搜索的大语言模型策略性规划：自动化启发式优化'}
{'arxiv_id': 'arXiv:2502.11418', 'title': 'TimeCAP: Learning to Contextualize, Augment, and Predict Time Series Events with Large Language Model Agents', 'authors': 'Geon Lee, Wenchao Yu, Kijung Shin, Wei Cheng, Haifeng Chen', 'link': 'https://arxiv.org/abs/2502.11418', 'abstract': 'Time series data is essential in various applications, including climate modeling, healthcare monitoring, and financial analytics. Understanding the contextual information associated with real-world time series data is often essential for accurate and reliable event predictions. In this paper, we introduce TimeCAP, a time-series processing framework that creatively employs Large Language Models (LLMs) as contextualizers of time series data, extending their typical usage as predictors. TimeCAP incorporates two independent LLM agents: one generates a textual summary capturing the context of the time series, while the other uses this enriched summary to make more informed predictions. In addition, TimeCAP employs a multi-modal encoder that synergizes with the LLM agents, enhancing predictive performance through mutual augmentation of inputs with in-context examples. Experimental results on real-world datasets demonstrate that TimeCAP outperforms state-of-the-art methods for time series event prediction, including those utilizing LLMs as predictors, achieving an average improvement of 28.75% in F1 score.', 'abstract_zh': '时间序列数据在气候建模、健康监测和金融分析等各类应用中至关重要。准确可靠地预测事件通常需要理解与实际时间序列数据相关的情境信息。本文介绍了一种时间序列处理框架TimeCAP，该框架创新地将大型语言模型（LLMs）用作时间序列数据的情境化工具，扩展了它们作为预测器的典型用途。TimeCAP 包含两个独立的 LLM 代理：一个生成文本摘要以捕捉时间序列的情境，另一个利用这个增强的摘要进行更明智的预测。此外，TimeCAP 还采用了一种多模态编码器，它与 LLM 代理协同工作，通过输入与上下文示例的相互增强来提升预测性能。实验结果表明，TimeCAP 在实际时间序列事件预测任务上优于最先进的方法，包括那些使用 LLMs 作为预测器的方法，在 F1 分数上平均提高了 28.75%。', 'title_zh': 'TimeCAP：学习上下文化、增强和预测时间序列事件的大型语言模型代理'}
{'arxiv_id': 'arXiv:2502.11358', 'title': 'Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System', 'authors': 'Ziyou Jiang, Mingyang Li, Guowei Yang, Junjie Wang, Yuekai Huang, Zhiyuan Chang, Qing Wang', 'link': 'https://arxiv.org/abs/2502.11358', 'abstract': 'Information theft attacks pose a significant risk to Large Language Model (LLM) tool-learning systems. Adversaries can inject malicious commands through compromised tools, manipulating LLMs to send sensitive information to these tools, which leads to potential privacy breaches. However, existing attack approaches are black-box oriented and rely on static commands that cannot adapt flexibly to the changes in user queries and the invocation chain of tools. It makes malicious commands more likely to be detected by LLM and leads to attack failure. In this paper, we propose AutoCMD, a dynamic attack comment generation approach for information theft attacks in LLM tool-learning systems. Inspired by the concept of mimicking the familiar, AutoCMD is capable of inferring the information utilized by upstream tools in the toolchain through learning on open-source systems and reinforcement with target system examples, thereby generating more targeted commands for information theft. The evaluation results show that AutoCMD outperforms the baselines with +13.2% $ASR_{Theft}$, and can be generalized to new tool-learning systems to expose their information leakage risks. We also design four defense methods to effectively protect tool-learning systems from the attack.', 'abstract_zh': '信息盗窃攻击对大型语言模型工具学习系统构成显著风险。AutoCMD：大型语言模型工具学习系统中信息盗窃攻击的动态攻击命令生成方法', 'title_zh': '模仿熟悉的行为：在LLM工具学习系统中进行信息窃取攻击的动态命令生成'}
{'arxiv_id': 'arXiv:2502.11221', 'title': 'PlanGenLLMs: A Modern Survey of LLM Planning Capabilities', 'authors': 'Hui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia Pan, Fei Liu', 'link': 'https://arxiv.org/abs/2502.11221', 'abstract': 'LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.', 'abstract_zh': 'LLMs在将初始世界状态转化为期望目标状态方面的计划生成具有巨大潜力。大量的研究已经探索了LLMs在各种规划任务中的应用，从网络导航到旅游规划和数据库查询。然而，许多这些系统针对特定问题进行了定制，使得它们之间的比较变得困难，也难以确定适合新任务的最佳方法。缺乏清晰一致的评估标准也是一个问题。本文综述旨在提供当前LLM规划系统的一个全面概述，填补这一空白。该综述基于Kartam和Wilkins（1990）的基础工作，并探讨了六项关键性能指标：完备性、可执行性、最优性、表示、泛化和效率。对于每一项指标，我们都对其代表作进行了详尽分析，并指出了它们的优点和不足。本文还指出了未来研究的关键方向，成为从业者和新入学者 valuable 的资源，以利用LLM规划支持代理工作流。', 'title_zh': 'PlanGenLLMs：大型语言模型规划能力的现代综述'}
{'arxiv_id': 'arXiv:2502.11164', 'title': 'Quantifying the Capability Boundary of DeepSeek Models: An Application-Driven Performance Analysis', 'authors': 'Shiguo Lian, Kaikai Zhao, Xuejiao Lei, Ning Wang, Zhenhong Long, Peijun Yang, Minjie Hua, Chaoyang Ma, Wen Liu, Kai Wang, Zhaoxiang Liu', 'link': 'https://arxiv.org/abs/2502.11164', 'abstract': 'DeepSeek-R1, known for its low training cost and exceptional reasoning capabilities, has achieved state-of-the-art performance on various benchmarks. However, detailed evaluations from the perspective of real-world applications are lacking, making it challenging for users to select the most suitable DeepSeek models for their specific needs. To address this gap, we evaluate the DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, and DeepSeek-R1-Distill-Llama series on A-Eval, an application-driven benchmark. By comparing original instruction-tuned models with their distilled counterparts, we analyze how reasoning enhancements impact performance across diverse practical tasks. Our results show that reasoning-enhanced models, while generally powerful, do not universally outperform across all tasks, with performance gains varying significantly across tasks and models. To further assist users in model selection, we quantify the capability boundary of DeepSeek models through performance tier classifications and intuitive line charts. Specific examples provide actionable insights to help users select and deploy the most cost-effective DeepSeek models, ensuring optimal performance and resource efficiency in real-world applications.', 'abstract_zh': 'DeepSeek-V3、DeepSeek-R1、DeepSeek-R1-Distill-Qwen系列和DeepSeek-R1-Distill-Llama系列在A-Eval应用驱动基准上的评估：推理增强模型在多样化实际任务中的性能分析与模型选择指导', 'title_zh': '基于应用驱动的性能分析：DeepSeek模型能力边界度量'}
{'arxiv_id': 'arXiv:2502.11157', 'title': 'Dyve: Thinking Fast and Slow for Dynamic Process Verification', 'authors': 'Jianyuan Zhong, Zeju Li, Zhijian Xu, Xiangyu Wen, Qiang Xu', 'link': 'https://arxiv.org/abs/2502.11157', 'abstract': "We present Dyve, a dynamic process verifier that enhances reasoning error detection in large language models by integrating fast and slow thinking, inspired by Kahneman's Systems Theory. Dyve adaptively applies immediate token-level confirmation System 1 for straightforward steps and comprehensive analysis System 2 for complex ones. Leveraging a novel step-wise consensus-filtered process supervision technique, combining Monte Carlo estimation with LLM based evaluation, Dyve curates high-quality supervision signals from noisy data. Experimental results on ProcessBench and the MATH dataset confirm that Dyve significantly outperforms existing process-based verifiers and boosts performance in Best-of-N settings.", 'abstract_zh': '我们提出Dyve，这是一种动态过程验证器，通过融合快速和慢速思考，增强大型语言模型中的推理错误检测，灵感源自卡尼曼的系统理论。Dyve根据步骤的复杂程度，自适应地应用立即的基于 token 的确认（System 1）和全面的分析（System 2）。通过结合使用新型逐步共识过滤过程监督技术、蒙特卡洛估算与基于语言模型的评估，Dyve从嘈杂的数据中精心筛选出高质量的监督信号。实验结果在 ProcessBench 和 MATH 数据集上表明，Dyve 显著优于现有过程基验证器，并在 Best-of-N 设置中提升了性能。', 'title_zh': 'Dyve：快速与缓慢思考在动态process验证中的应用'}
{'arxiv_id': 'arXiv:2502.11155', 'title': 'Uncertainty-Aware Search and Value Models: Mitigating Search Scaling Flaws in LLMs', 'authors': 'Fei Yu, Yingru Li, Benyou Wang', 'link': 'https://arxiv.org/abs/2502.11155', 'abstract': 'Value model-guided search is effective in steering the generation but suffers from scaling flaws: Its superiority diminishes with larger sample sizes, underperforming non-search baselines. This limitation arises from reliability degradation in value models in unseen reasoning paths. To address this, we propose an uncertainty-aware search framework that includes two key components: (1) uncertainty-aware value models that incorporate uncertainty into predictions, and (2) an uncertainty-aware selection process using the proposed efficient Group Thompson Sampling algorithm. Experiments on GSM8K show that our method mitigates search scaling flaws, achieving 90.5% coverage at 16 samples compared to 85.8% for conventional value-guided search. This work establishes the first systematic integration of uncertainty quantification in LLM search paradigms.', 'abstract_zh': '价值模型引导的搜索在指导生成方面有效，但存在扩展缺陷：其优势随样本数量增加而减弱，表现逊于非搜索基线。这一局限源于在未见过的推理路径上价值模型可靠性下降。为解决这一问题，我们提出了一种不确定性感知搜索框架，包括两个关键组件：（1）不确定性感知价值模型，将不确定性纳入预测；（2）使用提出的高效组θ-采样算法进行不确定性感知选择过程。实验表明，我们的方法减轻了搜索扩展缺陷，在16个样本下实现了90.5%的覆盖率，而传统价值引导搜索仅为85.8%。本工作首次系统地将不确定性量化整合到大模型搜索范式中。', 'title_zh': '不确定性意识的搜索与价值模型：减轻LLMs中的搜索扩展缺陷'}
{'arxiv_id': 'arXiv:2502.11122', 'title': 'Hierarchical Expert Prompt for Large-Language-Model: An Approach Defeat Elite AI in TextStarCraft II for the First Time', 'authors': 'Zongyuan Li, Chang Lu, Xiaojie Xu, Runnan Qi, Yanan Ni, Lumin Jiang, Xiangbei Liu, Xuebo Zhang, Yongchun Fang, Kuihua Huang, Xian Guo', 'link': 'https://arxiv.org/abs/2502.11122', 'abstract': 'Since the emergence of the Large Language Model (LLM), LLM has been widely used in fields such as writing, translating, and searching. However, there is still great potential for LLM-based methods in handling complex tasks such as decision-making in the StarCraft II environment. To address problems such as lack of relevant knowledge and poor control over subtasks of varying importance, we propose a Hierarchical Expert Prompt (HEP) for LLM. Our method improves the understanding of game situations through expert-level tactical knowledge, improving the processing quality of tasks of varying importance through a hierarchical framework. Our approach defeated the highest level (Elite) standard built-in agent in TextStarCraft II for the first time and consistently outperformed the baseline method in other difficulties. Our experiments suggest that the proposed method is a practical solution for tackling complex decision-making challenges. The replay video can be viewed on this https URL and this https URL, and our codes have been open-sourced on this https URL.', 'abstract_zh': '自大型语言模型（LLM）的出现以来，LLM已在写作、翻译和搜索等领域得到广泛应用。然而，在处理星际争霸II环境中复杂的决策任务等方面，基于LLM的方法仍有巨大的发展潜力。为了解决相关知识不足和对不同重要性子任务控制不力等问题，我们提出了一种层次专家提示（HEP）方法。该方法通过专家级别的战术知识提高对游戏情况的理解，并通过层次框架提高不同重要性任务的处理质量。我们的方法首次击败了TextStarCraft II中的最高水平（精英）内置代理，并在其他难度上持续优于基线方法。我们的实验表明，所提出的方法是应对复杂决策挑战的一种实用解决方案。回放视频请参见此链接和此链接，代码已开源在此链接。', 'title_zh': '大型语言模型的 hierarchical expert prompt 方法：首次在 TextStarCraft II 中战胜精英AI'}
{'arxiv_id': 'arXiv:2502.11102', 'title': 'OptMATH: A Scalable Bidirectional Data Synthesis Framework for Optimization Modeling', 'authors': 'Hongliang Lu, Zhonglin Xie, Yaoyu Wu, Can Ren, Yuxuan Chen, Zaiwen Wen', 'link': 'https://arxiv.org/abs/2502.11102', 'abstract': "Despite the rapid development of large language models (LLMs), a fundamental challenge persists: the lack of high-quality optimization modeling datasets hampers LLMs' robust modeling of practical optimization problems from natural language descriptions (NL). This data scarcity also contributes to the generalization difficulties experienced by learning-based methods. To address these challenges, we propose a scalable framework for synthesizing a high-quality dataset, named OptMATH. Starting from curated seed data with mathematical formulations (MF), this framework automatically generates problem data (PD) with controllable complexity. Then, a back-translation step is employed to obtain NL. To verify the correspondence between the NL and the PD, a forward modeling step followed by rejection sampling is used. The accepted pairs constitute the training part of OptMATH. Then a collection of rejected pairs is identified and further filtered. This collection serves as a new benchmark for optimization modeling, containing difficult instances whose lengths are much longer than these of NL4OPT and MAMO. Through extensive experiments, we demonstrate that models of various sizes (0.5B-32B parameters) trained on OptMATH achieve superior results on multiple modeling benchmarks, thereby validating the effectiveness and scalability of our approach.", 'abstract_zh': '尽管大型语言模型（LLMs）取得了 rapid development，但基础挑战依然存在：高质量优化建模数据集的缺乏阻碍了LLMs对自然语言描述（NL）的实际优化问题进行稳健建模。这种数据稀缺性也导致了基于学习方法的一般化困难。为应对这些挑战，我们提出了一种可扩展的合成高品質數據集框架，名为OptMATH。该框架从经过精心选择的带有数学公式（MF）的种子数据开始，自动生成具有可控复杂度的问题数据（PD）。随后通过反向翻译获得自然语言（NL）。为了验证NL与PD之间的对应关系，我们采用了反向建模和拒绝采样的步骤。被接受的配对数据构成OptMATH的训练部分。然后，识别并进一步筛选一组被拒绝的配对数据，该集合作为优化建模的新基准，包含实例长度远长于NL4OPT和MAMO的难题实例。通过广泛的实验，我们证明，训练于OptMATH的各种规模（0.5B-32B参数）的模型在多个建模基准上取得了优异结果，从而验证了我们方法的有效性和可扩展性。', 'title_zh': 'OptMATH：一种可扩展的双向数据合成框架用于优化建模'}
{'arxiv_id': 'arXiv:2502.11098', 'title': 'Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems', 'authors': 'Zhao Wang, Sota Moriyama, Wei-Yao Wang, Briti Gangopadhyay, Shingo Takamatsu', 'link': 'https://arxiv.org/abs/2502.11098', 'abstract': 'Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose \\textit{Talk Structurally, Act Hierarchically (TalkHier)}, a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. \\textit{TalkHier} surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available this https URL.', 'abstract_zh': 'Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose Talk Structurally, Act Hierarchically (TalkHier), a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. TalkHier surpasses various types of state-of-the-art systems, including inference scaling models (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available at this <https://> URL.', 'title_zh': '结构化地说话，层次化地行动：一种大型语言模型多智能体系统的协作框架'}
{'arxiv_id': 'arXiv:2502.11096', 'title': 'Mixture of Tunable Experts - Behavior Modification of DeepSeek-R1 at Inference Time', 'authors': 'Robert Dahlke, Henrik Klagges, Dan Zecha, Benjamin Merkel, Sven Rohr, Fabian Klemm', 'link': 'https://arxiv.org/abs/2502.11096', 'abstract': "We present the Mixture-of-Tunable-Experts (MoTE), a method that extends the Mixture-of-Experts architecture of Large Language Models (LLMs). Without additional training, MoTE enables meaningful and focused behavior changes in LLMs on-the-fly during inference time.\nBy analyzing the digital LLM brain of DeepSeek-R1 using a technique we dub 'functional Token Resonance Imaging' (fTRI) - inspired by fMRI and using prompts designed to elicit specific behavior (e.g., 'What happened {time}{place}?') - we empirically identify distinctive experts associated with behaviors like refusal responses.\nUsing MoTE we are able to intervene and control such specific behavior. We switched off the top 10 most refusal-relevant experts (0.07% of R1's 14,848 routed experts), achieving a 52% refusal reduction on sensitive reference prompts without performance degradation on MT-Bench. Random expert deactivation resulted in smaller behavioral shifts with increased noise, whereas forced expert activation led to significantly higher refusal rates.\nOur approach shares similarities with sparse autoencoders (SAEs) in terms of explainability and steerability. Unlike SAEs, MoTE does not require large training efforts, as within MoEs with a vast number of experts, specialization already emerged naturally during pretraining.\nOur findings suggest that significant functional mechanisms in Mixture-of-Experts architectures can at least partially be localized in a small number of specific experts, rather than being distributed throughout the model's weights. Expert subgroups can be tuned to trigger significant behavior variations, providing insights into the inner workings of LLMs.", 'abstract_zh': 'MoTE：一种扩展大规模语言模型Mixture-of-Experts架构的方法', 'title_zh': '可调专家混合模型 - 深度搜索-R1推理时的行为修改'}
{'arxiv_id': 'arXiv:2502.10978', 'title': 'Agentic LLM Framework for Adaptive Decision Discourse', 'authors': 'Antoine Dolant, Praveen Kumar', 'link': 'https://arxiv.org/abs/2502.10978', 'abstract': "Effective decision-making in complex systems requires synthesizing diverse perspectives to address multifaceted challenges under uncertainty. This study introduces a real-world inspired agentic Large Language Models (LLMs) framework, to simulate and enhance decision discourse-the deliberative process through which actionable strategies are collaboratively developed. Unlike traditional decision-support tools, the framework emphasizes dialogue, trade-off exploration, and the emergent synergies generated by interactions among agents embodying distinct personas. These personas simulate diverse stakeholder roles, each bringing unique priorities, expertise, and value-driven reasoning to the table. The framework incorporates adaptive and self-governing mechanisms, enabling agents to dynamically summon additional expertise and refine their assembly to address evolving challenges. An illustrative hypothetical example focused on extreme flooding in a Midwestern township demonstrates the framework's ability to navigate uncertainty, balance competing priorities, and propose mitigation and adaptation strategies by considering social, economic, and environmental dimensions. Results reveal how the breadth-first exploration of alternatives fosters robust and equitable recommendation pathways. This framework transforms how decisions are approached in high-stakes scenarios and can be incorporated in digital environments. It not only augments decision-makers' capacity to tackle complexity but also sets a foundation for scalable and context-aware AI-driven recommendations. This research explores novel and alternate routes leveraging agentic LLMs for adaptive, collaborative, and equitable recommendation processes, with implications across domains where uncertainty and complexity converge.", 'abstract_zh': '有效的决策制定需要在不确定性下综合多方面的视角以应对复杂挑战。本研究引入了一个受现实世界启发的代理大型语言模型（LLM）框架，以模拟和提升决策对话——一个通过协作开发可行策略的慎重过程。与传统的决策支持工具不同，该框架强调对话、权衡探索以及代理互动中涌现的合作协同效应。这些代理模拟了不同的角色，每个角色带来了独特的优先级、专业知识和价值驱动的推理。该框架整合了自适应和自我管理机制，使代理能够动态地召唤更多专业知识并调整其组合以应对不断变化的挑战。聚焦于中西部小镇极端洪水的一个示例假设场景展示了该框架如何在不确定性中导航、平衡竞争的优先事项，并通过考虑社会、经济和环境维度提出缓解和适应策略。结果显示，广度优先探索替代方案促进了稳健和公平的推荐路径。该框架改变了在高风险场景中如何进行决策，并可以嵌入数字环境中。它不仅增强了解决决策复杂性的能力，还为基于可扩展和上下文感知的AI驱动推荐奠定了基础。本研究探讨了利用代理LLM实现适应性、协作和公平推荐过程的新途径和替代路径，这些途径在涉及不确定性和复杂性的领域具有广泛影响。', 'title_zh': '代理型LLM框架：适应性决策论辩'}
{'arxiv_id': 'arXiv:2502.10938', 'title': 'PEA: Enhancing LLM Performance on Computational-Reasoning Tasks', 'authors': 'Zi Wang, Shiwei Weng, Mohannad Alhanahnah, Somesh Jha, Tom Reps', 'link': 'https://arxiv.org/abs/2502.10938', 'abstract': "Large Language Models (LLMs) have exhibited remarkable capabilities across diverse domains, prompting investigations into their potential as generic reasoning engines. While recent studies have explored inference-time computation to enhance model performance on complex problems, current research lacks a formal framework to characterize the complexity of reasoning tasks. This study introduces the Predicate-Enumeration-Aggregation (PEA) framework, a formal approach to describe and solve a class of important reasoning tasks termed computational reasoning problems. The PEA framework decomposes these problems into predicate and enumeration components, using LLMs to synthesize programs based on specified predicates, enumeration, and aggregation rules. These synthesized programs are then executed to obtain solutions to the computational tasks. We demonstrate the framework's efficacy on benchmark tasks including Boolean satisfiability problems, game of $24$, and planning problems. Empirical evaluation reveals that PEA substantially enhances the performance of underlying models on benchmark computational problems, yielding an average accuracy improvement of approximately $50\\%$, coupled with increased efficiency.", 'abstract_zh': '大型语言模型（LLMs）在多个领域展现了出色的能力，促使人们研究其作为通用推理引擎的潜力。虽然近期研究探讨了推理时的计算方法以提升模型在复杂问题上的性能，但当前研究缺乏量化推理任务复杂性的正式框架。本研究引入了谓词-枚举-聚合（PEA）框架，这是一种描述和解决一类重要推理任务（计算推理问题）的正式方法。PEA框架将这些任务分解为谓词和枚举组件，利用LLMs根据指定的谓词、枚举和聚合规则合成程序，然后执行这些程序以获得计算任务的解决方案。我们在布尔可满足性问题、24点游戏和规划问题等基准任务上展示了该框架的有效性。实证评估表明，PEA显著提升了底层模型在基准计算问题上的性能，平均准确率提高了约50%，同时提高了效率。', 'title_zh': 'PEA: 提升计算推理任务中大规模语言模型性能的方法'}
{'arxiv_id': 'arXiv:2502.10937', 'title': 'SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention', 'authors': 'Chengshuai Zhao, Zhen Tan, Chau-Wai Wong, Xinyan Zhao, Tianlong Chen, Huan Liu', 'link': 'https://arxiv.org/abs/2502.10937', 'abstract': 'Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates $\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via $\\underline{\\textbf{L}}$arge language model (LLM) ag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.', 'abstract_zh': '基于大规模语言模型的多agent内容分析模拟框架SCALE', 'title_zh': 'SCALE: 向量化基于大规模语言模型代理和人类干预的社会科学内容协作分析方法'}
{'arxiv_id': 'arXiv:2502.10931', 'title': 'D-CIPHER: Dynamic Collaborative Intelligent Agents with Planning and Heterogeneous Execution for Enhanced Reasoning in Offensive Security', 'authors': 'Meet Udeshi, Minghao Shao, Haoran Xi, Nanda Rani, Kimberly Milner, Venkata Sai Charan Putrevu, Brendan Dolan-Gavitt, Sandeep Kumar Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Muhammad Shafique', 'link': 'https://arxiv.org/abs/2502.10931', 'abstract': 'Large Language Models (LLMs) have been used in cybersecurity in many ways, including their recent use as intelligent agent systems for autonomous security analysis. Capture the Flag (CTF) challenges serve as benchmarks for assessing the automated task-planning abilities of LLM agents across various cybersecurity skill sets. Early attempts to apply LLMs for solving CTF challenges relied on single-agent systems, where feedback was restricted to a single reasoning-action loop. This approach proved inadequate for handling complex CTF tasks. Drawing inspiration from real-world CTF competitions, where teams of experts collaborate, we introduce the D-CIPHER multi-agent LLM framework for collaborative CTF challenge solving. D-CIPHER integrates agents with distinct roles, enabling dynamic feedback loops to enhance reasoning on CTF challenges. It introduces the Planner-Executor agent system, consisting of a Planner agent for overall problem-solving along with multiple heterogeneous Executor agents for individual tasks, facilitating efficient allocation of responsibilities among the LLMs. Additionally, D-CIPHER incorporates an Auto-prompter agent, which improves problem-solving by exploring the challenge environment and generating a highly relevant initial prompt. We evaluate D-CIPHER on CTF benchmarks using multiple LLM models and conduct comprehensive studies to highlight the impact of our enhancements. Our results demonstrate that the multi-agent D-CIPHER system achieves a significant improvement in challenges solved, setting a state-of-the-art performance on three benchmarks: 22.0% on NYU CTF Bench, 22.5% on Cybench, and 44.0% on HackTheBox. D-CIPHER is available at this https URL as the nyuctf_multiagent package.', 'abstract_zh': '大型语言模型（LLMs）在网络安全领域的应用包括作为自主安全分析的智能代理人系统。捕获旗标（CTF）挑战作为评估LLM代理在各种网络安全技能集上的自动化任务规划能力的基准。早期将LLMs应用于解决CTF挑战的努力依赖于单代理人系统，其中反馈仅限于单一的推理-行动循环，这种方法对于处理复杂的CTF任务证明是不足的。从现实世界的CTF竞赛中汲取灵感，其中专家团队进行协作，我们提出了D-CIPHER多代理LLM框架，用于协作解决CTF挑战。D-CIPHER集成了具有不同角色的代理，以启用动态反馈循环来增强CTF挑战的推理。它引入了由整体问题求解的规划者代理和执行不同任务的多个异质执行者代理组成的规划者-执行者代理系统，促进LLMs之间责任的高效分配。此外，D-CIPHER还集成了一个自动提示生成器代理，通过探索挑战环境并生成高度相关的初始提示来改善问题求解。我们使用多个LLM模型在CTF基准上评估了D-CIPHER，并进行了全面的研究以突出我们的改进的影响。结果显示，多代理D-CIPHER系统在挑战解决方面取得了显著改进，在三个基准上的性能达到最新水平：在NYU CTF基准上的表现为22.0%，在Cybench上的表现为22.5%，在HackTheBox上的表现为44.0%。D-CIPHER可以通过以下链接访问：this https URL（作为nyuctf_multiagent包）。', 'title_zh': 'D-CIPHER: 动态协作智能代理的计划与异构执行以增强 Offensive Security 中的推理能力'}
{'arxiv_id': 'arXiv:2502.10906', 'title': 'PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning', 'authors': 'In-Chang Baek, Sung-Hyun Kim, Sam Earle, Zehua Jiang, Noh Jin-Ha, Julian Togelius, Kyung-Joong Kim', 'link': 'https://arxiv.org/abs/2502.10906', 'abstract': 'Reward design plays a pivotal role in the training of game AIs, requiring substantial domain-specific knowledge and human effort. In recent years, several studies have explored reward generation for training game agents and controlling robots using large language models (LLMs). In the content generation literature, there has been early work on generating reward functions for reinforcement learning agent generators. This work introduces PCGRLLM, an extended architecture based on earlier work, which employs a feedback mechanism and several reasoning-based prompt engineering techniques. We evaluate the proposed method on a story-to-reward generation task in a two-dimensional environment using two state-of-the-art LLMs, demonstrating the generalizability of our approach. Our experiments provide insightful evaluations that demonstrate the capabilities of LLMs essential for content generation tasks. The results highlight significant performance improvements of 415% and 40% respectively, depending on the zero-shot capabilities of the language model. Our work demonstrates the potential to reduce human dependency in game AI development, while supporting and enhancing creative processes.', 'abstract_zh': '奖励设计在游戏AI的训练中起着关键作用，需要大量的领域特定知识和人力投入。近年来，多项研究探索了使用大规模语言模型（LLM）生成训练游戏代理和控制机器人的奖励。在内容生成文献中，早期工作已经开始尝试生成强化学习代理生成器的奖励函数。本文介绍了PCGRLLM，这是一种基于先前工作的扩展架构，采用了反馈机制和多种基于推理的提示工程技术。我们使用两种最先进的LLM在二维环境中评估了所提出的方法，展示了我们方法的普适性。我们的实验提供了有价值的经验评估，突显了语言模型在内容生成任务中不可或缺的能力。结果表明，依赖于语言模型的零样本能力，性能分别提高了415%和40%。我们的工作展示了减少游戏AI开发中人类依赖的可能性，同时支持和增强创造过程。', 'title_zh': 'PCGRLLM：基于大规模语言模型的程序化内容生成强化学习奖励设计'}
{'arxiv_id': 'arXiv:2502.10867', 'title': 'A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1', 'authors': 'Jun Wang', 'link': 'https://arxiv.org/abs/2502.10867', 'abstract': "OpenAI o1 has shown that applying reinforcement learning to integrate reasoning steps directly during inference can significantly improve a model's reasoning capabilities. This result is exciting as the field transitions from the conventional autoregressive method of generating answers to a more deliberate approach that models the slow-thinking process through step-by-step reasoning training. Reinforcement learning plays a key role in both the model's training and decoding processes. In this article, we present a comprehensive formulation of reasoning problems and investigate the use of both model-based and model-free approaches to better support this slow-thinking framework.", 'abstract_zh': 'OpenAI o1已经在利用强化学习在推理过程中直接整合推理步骤显著提升模型推理能力方面取得了成果。这一成果令人兴奋，随着领域从传统的自回归生成方法向通过逐步推理训练建模慢思考过程的更谨慎方法转变，强化学习在模型的训练和解码过程中的作用变得尤为重要。本文提供了一种全面的推理问题表述，并探讨了基于模型和非基于模型方法的应用，以更好地支持这种慢思考框架。', 'title_zh': 'LLM推理教程：ChatGPT背后的相关方法'}
{'arxiv_id': 'arXiv:2502.10858', 'title': 'Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs', 'authors': 'Zongqian Wu, Tianyu Li, Jiaying Yang, Mengmeng Zhan, Xiaofeng Zhu, Lei Feng', 'link': 'https://arxiv.org/abs/2502.10858', 'abstract': 'Deep iterative chain-of-thought (CoT) reasoning enables LLMs to tackle complex tasks by progressively activating relevant pre-trained knowledge. However, it faces challenges in ensuring continual improvement and determining a stopping criterion. In this paper, we investigate whether the relevant knowledge that contributes directly to solving the given question can be activated from the initial reasoning path, thus circumventing the need for iterative refinement. Our experiments reveal that increasing the diversity of initial reasoning paths can achieve comparable or superior performance, a concept we term \\textit{breadth reasoning}. However, existing breadth reasoning approaches, such as self-consistency, offer limited diversity. To address this limitation, we propose a simple yet effective method that enhances reasoning breadth by integrating contextual exploration with reduced sampling randomness. Extensive experiments demonstrate that our approach significantly outperforms deep iterative reasoning. Our code is provided in this https URL.', 'abstract_zh': '深度迭代链式思考推理使大语言模型能够通过逐步激活相关预训练知识来应对复杂任务。然而，它在确保持续改进和确定停止标准方面面临挑战。在本文中，我们研究初始推理路径是否能够激活直接有助于解决给定问题的相关知识，从而绕过迭代改进的需要。我们的实验表明，增加初始推理路径的多样性可以实现相当或更优的表现，这一概念我们称为广度推理。然而，现有的广度推理方法，如自我一致性，提供的多样性有限。为解决这一限制，我们提出了一种简单而有效的方法，通过结合上下文探索和减少采样随机性来增强推理广度。广泛的实验表明，我们的方法显著优于深度迭代推理。我们的代码发布在https://...。', 'title_zh': '深度学习之外：LLMs中的迭代推理探究'}
{'arxiv_id': 'arXiv:2502.10620', 'title': 'ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language Interactions for Computer-Aided Diagnosis', 'authors': 'Xueshen Li, Xinlong Hou, Ziyi Huang, Yu Gan', 'link': 'https://arxiv.org/abs/2502.10620', 'abstract': 'Recent advancements in large language models (LLMs) have demonstrated extraordinary comprehension capabilities with remarkable breakthroughs on various vision-language tasks. However, the application of LLMs in generating reliable medical diagnostic reports remains in the early stages. Currently, medical LLMs typically feature a passive interaction model where doctors respond to patient queries with little or no involvement in analyzing medical images. In contrast, some ChatBots simply respond to predefined queries based on visual inputs, lacking interactive dialogue or consideration of medical history. As such, there is a gap between LLM-generated patient-ChatBot interactions and those occurring in actual patient-doctor consultations. To bridge this gap, we develop an LLM-based dialogue system, namely proactive multi-round vision-language interactions for computer-aided diagnosis (ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The proposed ProMRVL-CAD system allows proactive dialogue to provide patients with constant and reliable medical access via an integration of knowledge graph into a recommendation system. Specifically, we devise two generators: a Proactive Question Generator (Pro-Q Gen) to generate proactive questions that guide the diagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report Generator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating two real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model has better quality in generating medical reports. We further demonstrate the performance of ProMRVL achieves robust under the scenarios with low image quality. Moreover, we have created a synthetic medical dialogue dataset that simulates proactive diagnostic interactions between patients and doctors, serving as a valuable resource for training LLM.', 'abstract_zh': '最近大型语言模型在多模态医学诊断报告生成方面的进展已经在各种视觉-语言任务上取得了显著突破。然而，将大型语言模型应用于生成可靠的医学诊断报告仍处于早期阶段。目前，医学大型语言模型通常呈现一种被动的交互模型，医生对患者的查询进行回应，但很少参与到医学图像的分析中。相比之下，一些聊天机器人基于视觉输入仅对预定义的查询作出响应，缺乏互动对话或对医疗历史的考虑。因此，大型语言模型生成的患者-聊天机器人交互与实际患者-医生咨询之间存在差距。为了弥合这一差距，我们开发了一种基于大型语言模型的对话系统，即主动多轮视觉-语言交互辅助计算机辅助诊断（ProMRVL-CAD），以生成患者友好的疾病诊断报告。提出的ProMRVL-CAD系统通过将知识图谱整合到推荐系统中，实现主动对话，为患者提供持续可靠的医疗访问。具体而言，我们设计了两个生成器：主动问题生成器（Pro-Q Gen）以生成引导诊断流程的主动问题，以及多视图患者文本诊断报告生成器（MVP-DR Gen）以生成高质量的诊断报告。在对两个公开的现实世界数据集MIMIC-CXR和IU-Xray进行评估后，我们的模型在生成医学报告方面表现出更高的质量。此外，我们创建了一个合成医学对话数据集，模拟患者与医生之间的主动诊断交互，作为训练大型语言模型的宝贵资源。', 'title_zh': 'ProMRVL-CAD：基于多轮视觉-语言交互的主动对话系统用于计算机辅助诊断'}
{'arxiv_id': 'arXiv:2502.10554', 'title': 'Benchmarking the rationality of AI decision making using the transitivity axiom', 'authors': 'Kiwon Song, James M. Jennings III, Clintin P. Davis-Stober', 'link': 'https://arxiv.org/abs/2502.10554', 'abstract': "Fundamental choice axioms, such as transitivity of preference, provide testable conditions for determining whether human decision making is rational, i.e., consistent with a utility representation. Recent work has demonstrated that AI systems trained on human data can exhibit similar reasoning biases as humans and that AI can, in turn, bias human judgments through AI recommendation systems. We evaluate the rationality of AI responses via a series of choice experiments designed to evaluate transitivity of preference in humans. We considered ten versions of Meta's Llama 2 and 3 LLM models. We applied Bayesian model selection to evaluate whether these AI-generated choices violated two prominent models of transitivity. We found that the Llama 2 and 3 models generally satisfied transitivity, but when violations did occur, occurred only in the Chat/Instruct versions of the LLMs. We argue that rationality axioms, such as transitivity of preference, can be useful for evaluating and benchmarking the quality of AI-generated responses and provide a foundation for understanding computational rationality in AI systems more generally.", 'abstract_zh': '基础的选择公理，如偏好传递性，为确定人类决策是否合理，即是否符合效用表示，提供了可测试的条件。近期研究表明，基于人类数据训练的AI系统可能表现出与人类相似的推理偏见，并且AI系统可以通过推荐系统影响人类判断。我们通过一系列旨在评估人类偏好传递性的选择实验，评估AI响应的合理性。我们考虑了Meta的Llama 2和3个LLM模型的十个版本。我们采用贝叶斯模型选择方法评估这些AI生成的选择是否违反了两种典型的传递性模型。我们发现，Llama 2和3个模型总体上满足传递性，但在违反传递性的情况下，仅出现在LLM的Chat/指令版本中。我们论证认为，如偏好传递性这样的合理性公理，可用于评估和基准测试AI生成响应的质量，并为理解AI系统中的计算合理性奠定基础。', 'title_zh': '基于传递性公理对标准化AI决策合理性的benchmarking研究'}
{'arxiv_id': 'arXiv:2502.10522', 'title': 'GraphiT: Efficient Node Classification on Text-Attributed Graphs with Prompt Optimized LLMs', 'authors': 'Shima Khoshraftar, Niaz Abedini, Amir Hajian', 'link': 'https://arxiv.org/abs/2502.10522', 'abstract': 'The application of large language models (LLMs) to graph data has attracted a lot of attention recently. LLMs allow us to use deep contextual embeddings from pretrained models in text-attributed graphs, where shallow embeddings are often used for the text at- tributes of nodes. However, it is still challenging to efficiently en- code the graph structure and features into a sequential form for use by LLMs. In addition, the performance of an LLM alone, is highly dependent on the structure of the input prompt, which limits their effectiveness as a reliable approach and often requires iterative man- ual adjustments that could be slow, tedious and difficult to replicate programmatically. In this paper, we propose GraphiT (Graphs in Text), a framework for encoding graphs into a textual format and optimizing LLM prompts for graph prediction tasks. Here we focus on node classification for text-attributed graphs. We encode the graph data for every node and its neighborhood into a concise text to enable LLMs to better utilize the information in the graph. We then further programmatically optimize the LLM prompts us- ing the DSPy framework to automate this step and make it more efficient and reproducible. GraphiT outperforms our LLM-based baselines on three datasets and we show how the optimization step in GraphiT leads to measurably better results without manual prompt tweaking. We also demonstrated that our graph encoding approach is competitive to other graph encoding methods while being less expensive because it uses significantly less tokens for the same task.', 'abstract_zh': 'Large语言模型在图数据中的应用：GraphiT框架及其在图预测任务中的优化', 'title_zh': 'GraphiT: 使用提示优化大语言模型在文本属性图上的高效节点分类'}
{'arxiv_id': 'arXiv:2502.10482', 'title': 'A Self-Supervised Reinforcement Learning Approach for Fine-Tuning Large Language Models Using Cross-Attention Signals', 'authors': 'Andrew Kiruluta, Andreas Lemos, Priscilla Burity', 'link': 'https://arxiv.org/abs/2502.10482', 'abstract': 'We propose a novel reinforcement learning framework for post training large language models that does not rely on human in the loop feedback. Instead, our approach uses cross attention signals within the model itself to derive a self supervised reward, thereby guiding iterative fine tuning of the model policy. By analyzing how the model attends to the input prompt during generation, we construct measures of prompt coverage, focus, and coherence. We then use these measures to rank or score candidate responses, providing a reward signal that encourages the model to produce well aligned, on topic text. In empirical comparisons against standard policy gradient methods and RL fine tuning with synthetic preference models, our method shows significant gains in prompt relevance and consistency over a non RL baseline. While it does not yet match the performance of fully human supervised RLHF systems, it highlights an important direction for scaling alignment with minimal human labeling. We provide a detailed analysis, discuss potential limitations, and outline future work for combining cross-attention based signals with smaller amounts of human feedback.', 'abstract_zh': '我们提出了一种用于后训练大型语言模型的新型强化学习框架，该框架不依赖于人类闭环反馈。相反，我们的方法利用模型内部的交叉注意力信号来推导出自我监督的奖励，从而引导模型策略的迭代微调。通过分析模型在生成过程中对输入提示的关注方式，我们构建了提示覆盖度、专注度和连贯性的度量标准。然后，我们利用这些度量标准对候选响应进行排名或评分，提供一种奖励信号，鼓励模型生成主题相关且一致的文本。与标准策略梯度方法和基于合成偏好模型的RL微调方法相比，我们的方法在提示相关性和一致性方面相对于非RL基线显示出显著的提升。尽管它尚未达到完全基于人类监督的RLHF系统的性能，但它强调了一个重要的发展方向，即通过最少的人类标签实现有效对齐。我们提供了详细分析，讨论了潜在限制，并概述了将交叉注意力基信号与较少的人类反馈结合的未来工作。', 'title_zh': '基于跨注意力信号的自监督强化学习方法用于调整大型语言模型'}
{'arxiv_id': 'arXiv:2502.10420', 'title': 'Position: Stop Acting Like Language Model Agents Are Normal Agents', 'authors': 'Elija Perrier, Michael Timothy Bennett', 'link': 'https://arxiv.org/abs/2502.10420', 'abstract': 'Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.', 'abstract_zh': '语言模型代理（LMAs）不应被视为正常代理：关于其本体论属性的测量与路径学说的学术探讨', 'title_zh': '位置：停止将语言模型代理视为正常代理'}
{'arxiv_id': 'arXiv:2502.12149', 'title': 'HARBOR: Exploring Persona Dynamics in Multi-Agent Competition', 'authors': 'Kenan Jiang, Li Xiong, Fei Liu', 'link': 'https://arxiv.org/abs/2502.12149', 'abstract': "We investigate factors contributing to LLM agents' success in competitive multi-agent environments, using auctions as a testbed where agents bid to maximize profit. The agents are equipped with bidding domain knowledge, distinct personas that reflect item preferences, and a memory of auction history. Our work extends the classic auction scenario by creating a realistic environment where multiple agents bid on houses, weighing aspects such as size, location, and budget to secure the most desirable homes at the lowest prices. Particularly, we investigate three key questions: (a) How does a persona influence an agent's behavior in a competitive setting? (b) Can an agent effectively profile its competitors' behavior during auctions? (c) How can persona profiling be leveraged to create an advantage using strategies such as theory of mind? Through a series of experiments, we analyze the behaviors of LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a valuable platform for deepening our understanding of multi-agent workflows in competitive environments.", 'abstract_zh': '我们使用拍卖作为测试场景，研究影响大语言模型代理在竞争性多代理环境中的成功因素，代理具备投标领域的知识、反映物品偏好的不同身份 persona，以及拍卖历史的记忆。我们的工作扩展了经典的拍卖场景，通过创造一个现实的环境，让多个代理竞拍房屋，并权衡大小、位置和预算等因素，以实现最低成本获取最理想住宅。特别地，我们探讨了三个关键问题：(a) 身份如何影响代理在竞争性环境中的行为？(b) 代理能否在拍卖过程中有效分析竞争对手的行为？(c) 如何利用身份分析的优势，通过考虑心理理论等策略来建立优势？通过一系列实验，我们分析了大语言模型代理的行为，并揭示了新的发现。我们的测试平台 HARBOR 为深化对竞争环境中多代理工作流的理解提供了宝贵平台。', 'title_zh': 'HARBOR: 探索多智能体竞争中的角色动态'}
{'arxiv_id': 'arXiv:2502.12145', 'title': 'Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control', 'authors': 'Jinyan Su, Jennifer Healey, Preslav Nakov, Claire Cardie', 'link': 'https://arxiv.org/abs/2502.12145', 'abstract': 'Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to mitigate large language model (LLM) hallucinations by incorporating external knowledge retrieval. However, existing RAG frameworks often apply retrieval indiscriminately,leading to inefficiencies-over-retrieving when unnecessary or failing to retrieve iteratively when required for complex reasoning. Recent adaptive retrieval strategies, though adaptively navigates these retrieval strategies, predict only based on query complexity and lacks user-driven flexibility, making them infeasible for diverse user application needs. In this paper, we introduce a novel user-controllable RAG framework that enables dynamic adjustment of the accuracy-cost trade-off. Our approach leverages two classifiers: one trained to prioritize accuracy and another to prioritize retrieval efficiency. Via an interpretable control parameter $\\alpha$, users can seamlessly navigate between minimal-cost retrieval and high-accuracy retrieval based on their specific requirements. We empirically demonstrate that our approach effectively balances accuracy, retrieval cost, and user controllability, making it a practical and adaptable solution for real-world applications.', 'abstract_zh': '基于检索增强生成的用户可控框架：平衡准确性和检索成本', 'title_zh': '快还是好？在具有灵活用户控制的检索增强生成中平衡准确性和成本'}
{'arxiv_id': 'arXiv:2502.12120', 'title': 'LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws', 'authors': 'Prasanna Mayilvahanan, Thaddäus Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel', 'link': 'https://arxiv.org/abs/2502.12120', 'abstract': 'Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.', 'abstract_zh': 'Scaling定律指导大规模语言模型（LLMs）的发展，并提供了模型大小、标记和计算最优平衡的估计。近年来，损失到损失的Scaling定律逐渐成为理解并提升LLM性能的有力工具，这些定律关联了预训练数据集和下游任务之间的损失。在本工作中，我们研究了哪些因素对损失到损失的Scaling影响最大。我们的实验揭示出，预训练数据和分词器决定了Scaling趋势，而模型大小、优化超参数，甚至像Llama这样的变换器模型和Mamba这样的状态空间模型之间显著的架构差异，对Scaling的影响有限。因此，从业者应精心选择适合的预训练数据集以实现最佳的下游性能，而架构和其他设置可以自由优化以提高训练效率。', 'title_zh': 'LLMs在线性关系中的数据决定损失缩放定律'}
{'arxiv_id': 'arXiv:2502.12119', 'title': 'PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection', 'authors': 'Jinhe Bi, Yifan Wang, Danqi Yan, Xun Xiao, Artur Hecker, Volker Tresp, Yunpu Ma', 'link': 'https://arxiv.org/abs/2502.12119', 'abstract': 'Visual instruction tuning refines pre-trained Multimodal Large Language Models (MLLMs) to enhance their real-world task performance. However, the rapid expansion of visual instruction datasets introduces significant data redundancy, leading to excessive computational costs. Existing data selection methods predominantly rely on proxy models or loss-based metrics, both of which impose substantial computational overheads due to the necessity of model inference and backpropagation. To address this challenge, we propose PRISM, a novel training-free approach for efficient multimodal data selection. Unlike existing methods, PRISM eliminates the reliance on proxy models, warm-up pretraining, and gradient-based optimization. Instead, it leverages Pearson correlation analysis to quantify the intrinsic visual encoding properties of MLLMs, computing a task-specific correlation score to identify high-value instances. This not only enbles data-efficient selection,but maintains the original performance. Empirical evaluations across multiple MLLMs demonstrate that PRISM reduces the overall time required for visual instruction tuning and data selection to just 30% of conventional methods, while surpassing fully fine-tuned models across eight multimodal and three language understanding benchmarks, achieving a 101.7% relative improvement in final performance.', 'abstract_zh': '视觉指令调优通过细化预训练多模态大型语言模型来提升其实用任务性能。然而，视觉指令数据集的迅速扩张导致了显著的数据冗余，增加了过高的计算成本。现有的数据选择方法主要依赖代理模型或基于损失的指标，两者都因需要模型推理和反向传播而产生了显著的计算开销。为应对这一挑战，我们提出PRISM，这是一种新型的无需训练的高效多模态数据选择方法。与现有方法不同，PRISM 杜绝了对代理模型、预热微调和梯度优化的依赖，而是利用皮尔逊相关分析来量化多模态大型语言模型的固有视觉编码特性，计算特定任务的相关分数以识别高价值样本。这一方法不仅实现了高效的数据选择，还能保持原始性能。多项实验验证了PRISM在减少视觉指令调优和数据选择所需时间方面的优势，使其仅为传统方法的30%，同时在八个多模态和三个语言理解基准测试中超过了完全微调的模型，实现了101.7%的相对性能提升。', 'title_zh': 'PRISM: 自 pruning 内在选择方法用于无训练多模态数据选择'}
{'arxiv_id': 'arXiv:2502.12109', 'title': 'Personality Structured Interview for Large Language Model Simulation in Personality Research', 'authors': 'Pengda Wang, Huiqi Zou, Hanjie Chen, Tianjun Sun, Ziang Xiao, Frederick L. Oswald', 'link': 'https://arxiv.org/abs/2502.12109', 'abstract': "Although psychometrics researchers have recently explored the use of large language models (LLMs) as proxies for human participants, LLMs often fail to generate heterogeneous data with human-like diversity, which diminishes their value in advancing social science research. To address these challenges, we explored the potential of the theory-informed Personality Structured Interview (PSI) as a tool for simulating human responses in personality research. In this approach, the simulation is grounded in nuanced real-human interview transcripts that target the personality construct of interest. We have provided a growing set of 357 structured interview transcripts from a representative sample, each containing an individual's response to 32 open-ended questions carefully designed to gather theory-based personality evidence. Additionally, grounded in psychometric research, we have summarized an evaluation framework to systematically validate LLM-generated psychometric data. Results from three experiments demonstrate that well-designed structured interviews could improve human-like heterogeneity in LLM-simulated personality data and predict personality-related behavioral outcomes (i.e., organizational citizenship behaviors and counterproductive work behavior). We further discuss the role of theory-informed structured interviews in LLM-based simulation and outline a general framework for designing structured interviews to simulate human-like data for psychometric research.", 'abstract_zh': '尽管心理测量研究人员最近探索了使用大型语言模型（LLMs）作为人类参与者代理的可能性，但LLMs常常无法生成具有人类多样性的人类异质性数据，这减弱了它们在推进社会科学研究中的价值。为了应对这些挑战，我们探讨了理论指导的个性结构访谈（PSI）作为在个性研究中模拟人类响应的工具的潜力。在该方法中，模拟基于针对感兴趣的人格结构的细致的人类访谈转录。我们提供了一套不断增长的357个结构化访谈转录，来自代表性样本，每个转录包含个人对32个精心设计的开放性问题的响应，以收集基于理论的人格证据。此外，基于心理测量研究，我们总结了一个评估框架，以系统地验证LLM生成的心理测量数据的有效性。三项实验的结果表明，精心设计的结构化访谈可以提高LLM模拟的人格数据的人类异质性，并预测与人格相关的行为结果（即组织公民行为和反生产性工作行为）。我们进一步讨论了理论指导的结构化访谈在LLM基于的模拟中的作用，并概述了设计结构化访谈以生成用于心理测量研究的人类样数据的一般框架。', 'title_zh': '基于人格结构化面试的大语言模型人格研究模拟'}
{'arxiv_id': 'arXiv:2502.12088', 'title': 'Meta-Statistical Learning: Supervised Learning of Statistical Inference', 'authors': 'Maxime Peyrard, Kyunghyun Cho', 'link': 'https://arxiv.org/abs/2502.12088', 'abstract': "This work demonstrates that the tools and principles driving the success of large language models (LLMs) can be repurposed to tackle distribution-level tasks, where the goal is to predict properties of the data-generating distribution rather than labels for individual datapoints. These tasks encompass statistical inference problems such as parameter estimation, hypothesis testing, or mutual information estimation. Framing these tasks within traditional machine learning pipelines is challenging, as supervision is typically tied to individual datapoint. We propose meta-statistical learning, a framework inspired by multi-instance learning that reformulates statistical inference tasks as supervised learning problems. In this approach, entire datasets are treated as single inputs to neural networks, which predict distribution-level parameters. Transformer-based architectures, without positional encoding, provide a natural fit due to their permutation-invariance properties. By training on large-scale synthetic datasets, meta-statistical models can leverage the scalability and optimization infrastructure of Transformer-based LLMs. We demonstrate the framework's versatility with applications in hypothesis testing and mutual information estimation, showing strong performance, particularly for small datasets where traditional neural methods struggle.", 'abstract_zh': '大型语言模型的工具和原理可以重新用于处理分布级任务，这些任务的目标是预测数据生成分布的属性而不是个别数据点的标签。这些问题包括参数估计、假设检验或互信息估计等统计推理问题。将这些任务纳入传统的机器学习管道中具有挑战性，因为监督通常与个别数据点相关。我们提出了元统计学习框架，该框架借鉴多实例学习的概念，将统计推理任务重新表述为监督学习问题。在此方法中，整个数据集被视为神经网络的一个输入，以预测分布级参数。由于其置换不变性特性，基于 Transformer 的架构在没有位置编码的情况下提供了自然的契合。通过在大规模合成数据集上训练，元统计模型可以利用基于 Transformer 的大型语言模型的可扩展性和优化基础设施。我们通过在假设检验和互信息估计的应用中展示该框架的 versatility，显示出在传统神经方法在小数据集上表现不佳的情况下，具有强大性能。', 'title_zh': '元统计学习：统计推理的监督学习'}
{'arxiv_id': 'arXiv:2502.12067', 'title': 'TokenSkip: Controllable Chain-of-Thought Compression in LLMs', 'authors': 'Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, Wenjie Li', 'link': 'https://arxiv.org/abs/2502.12067', 'abstract': "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning capabilities of large language models (LLMs). Recent advancements, such as OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT sequences during inference could further boost LLM reasoning performance. However, due to the autoregressive nature of LLM decoding, longer CoT outputs lead to a linear increase in inference latency, adversely affecting user experience, particularly when the CoT exceeds 10,000 tokens. To address this limitation, we analyze the semantic importance of tokens within CoT outputs and reveal that their contributions to reasoning vary. Building on this insight, we propose TokenSkip, a simple yet effective approach that enables LLMs to selectively skip less important tokens, allowing for controllable CoT compression. Extensive experiments across various models and tasks demonstrate the effectiveness of TokenSkip in reducing CoT token usage while preserving strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct, TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less than a 0.4% performance drop.", 'abstract_zh': 'Chain-of-Thought压缩方法TokenSkip在增强大型语言模型推理性能中的应用', 'title_zh': 'TokenSkip: LLM中可控的链式思维压缩'}
{'arxiv_id': 'arXiv:2502.12064', 'title': 'AI-generated Text Detection with a GLTR-based Approach', 'authors': 'Lucía Yan Wu, Isabel Segura-Bedmar', 'link': 'https://arxiv.org/abs/2502.12064', 'abstract': "The rise of LLMs (Large Language Models) has contributed to the improved performance and development of cutting-edge NLP applications. However, these can also pose risks when used maliciously, such as spreading fake news, harmful content, impersonating individuals, or facilitating school plagiarism, among others. This is because LLMs can generate high-quality texts, which are challenging to differentiate from those written by humans. GLTR, which stands for Giant Language Model Test Room and was developed jointly by the MIT-IBM Watson AI Lab and HarvardNLP, is a visual tool designed to help detect machine-generated texts based on GPT-2, that highlights the words in text depending on the probability that they were machine-generated. One limitation of GLTR is that the results it returns can sometimes be ambiguous and lead to confusion. This study aims to explore various ways to improve GLTR's effectiveness for detecting AI-generated texts within the context of the IberLef-AuTexTification 2023 shared task, in both English and Spanish languages. Experiment results show that our GLTR-based GPT-2 model overcomes the state-of-the-art models on the English dataset with a macro F1-score of 80.19%, except for the first ranking model (80.91%). However, for the Spanish dataset, we obtained a macro F1-score of 66.20%, which differs by 4.57% compared to the top-performing model.", 'abstract_zh': '大型语言模型的兴起促进了先进自然语言处理应用的性能提升和发展。然而，这些模型也可能在恶意使用时带来风险，例如传播假新闻、发布有害内容、冒充个人或协助学术抄袭等。这是因为大型语言模型能够生成高质量的文本，这些文本难以与人类撰写的区分开来。GLTR（即大型语言模型测试室，由麻省理工学院IBM沃森人工智能实验室和哈佛NLP联合开发）是一款可视化工具，旨在基于GPT-2帮助检测机器生成的文本，并根据生成概率突出显示文本中的词语。GLTR的一个局限性是，其返回的结果有时可能会模棱两可，导致混淆。本文旨在探索在2023年伊贝勒夫-奥特西文本共享任务中提高GLTR检测AI生成文本有效性的多种方法，涵盖英语和西班牙语。实验结果表明，基于GLTR的GPT-2模型在英语数据集上的macro F1得分为80.19%，略低于排名第一的模型（80.91%）。然而，对于西班牙语数据集，我们获得了macro F1得分为66.20%，比表现最佳模型低4.57%。', 'title_zh': '基于GLTR方法的AI生成文本检测'}
{'arxiv_id': 'arXiv:2502.12022', 'title': 'Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving', 'authors': 'Xin Xu, Yan Xu, Tianhao Chen, Yuchen Yan, Chengwu Liu, Zaoyu Chen, Yufei Wang, Yichun Yin, Yasheng Wang, Lifeng Shang, Qun Liu', 'link': 'https://arxiv.org/abs/2502.12022', 'abstract': "Existing approaches to mathematical reasoning with large language models (LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated Reasoning (TIR) for precise computation. While efforts have been made to combine these methods, they primarily rely on post-selection or predefined strategies, leaving an open question: whether LLMs can autonomously adapt their reasoning strategy based on their inherent capabilities. In this work, we propose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework that enables LLMs to personalize their reasoning strategy spontaneously, aligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware data selection during supervised fine-tuning (SFT) to tailor training data to the model's unique abilities. This approach equips LLMs to autonomously determine and apply the appropriate reasoning strategy at test time. We evaluate TATA through extensive experiments on six mathematical reasoning benchmarks, using both general-purpose and math-specialized LLMs. Empirical results demonstrate that TATA effectively combines the complementary strengths of CoT and TIR, achieving superior or comparable performance with improved inference efficiency compared to TIR alone. Further analysis underscores the critical role of aptitude-aware data selection in enabling LLMs to make effective and adaptive reasoning decisions and align reasoning strategies with model capabilities.", 'abstract_zh': '基于大型语言模型的数学推理现有方法依赖于Chain-of-Thought (CoT)以实现泛化能力或依赖于Tool-Integrated Reasoning (TIR)以实现精确计算。尽管已经尝试将这些方法结合起来，但它们主要依赖于事后选择或预定义策略，留给一个开放的问题：LLMs 是否可以根据其固有的能力自主适应其推理策略。在本文中，我们提出了TATA（根据LLM能力进行教学）这一自适应框架，使LLMs能够自发地个性化其推理策略，使其与内在能力相匹配。TATA在监督微调（SFT）期间结合了对基模型的意识数据选择，以根据模型的独特能力调整训练数据。该方法使LLMs能够在测试时自主确定并应用适当的推理策略。我们通过在六个数学推理基准上进行广泛实验，使用通用和数学专业化LSTM，评估了TATA。实验证明，TATA有效地结合了CoT和TIR的优点，在推理效率方面优于单独使用TIR，进一步的分析强调了意识能力的数据选择在使LLMs能够做出有效和自适应的推理决策以及使推理策略与模型能力相一致方面的重要性。', 'title_zh': '根据其才能教学LLMs：适应性推理在数学问题解决中的应用'}
{'arxiv_id': 'arXiv:2502.12018', 'title': 'Atom of Thoughts for Markov LLM Test-Time Scaling', 'authors': 'Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo', 'link': 'https://arxiv.org/abs/2502.12018', 'abstract': 'Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at this https URL.', 'abstract_zh': 'Large Language Models (LLMs)在训练时标度训练并通过在推理时进行有效推理进一步增强其能力，取得了卓越的性能。然而，随着推理规模的扩大，现有的测试时标度方法遭受了累积历史信息的影响，不仅浪费了计算资源，还干扰了有效的推理。为解决这一问题，我们观察到复杂的推理过程通常通过解决一系列独立的子问题实现，每个子问题是自包含且可验证的。这些子问题本质上是原子问题，主要依赖于当前状态而非累积历史，类似于马尔可夫过程中的无记忆转移。基于这一观察，我们提出了Thinking Atom (Ta)，其中推理过程中的每个状态转换包括将当前问题分解为基于依赖关系的有向无环图，并将其子问题组合成一个新的原子问题状态。这种迭代分解-收缩过程将持续直到达到可以直接求解的原子问题，自然实现了问题状态之间的马尔可夫转移。此外，这些原子问题可以无缝集成到现有的测试时标度方法中，使Ta能够作为插件增强，提高推理能力。在六个基准上的实验表明，无论是作为独立框架还是插件增强，Ta都证明了其有效性。特别地，在HotpotQA上，当应用于gpt-4o-mini时，Ta达到了80.6%的F1分数，分别超过o3-mini的0.34%和DeepSeek-R1的10.6%。相关代码将发布在以下链接。', 'title_zh': 'Markov LLM测试时缩放的原子思想'}
{'arxiv_id': 'arXiv:2502.11995', 'title': 'Presumed Cultural Identity: How Names Shape LLM Responses', 'authors': 'Siddhesh Pawar, Arnav Arora, Lucie-Aimée Kaffee, Isabelle Augenstein', 'link': 'https://arxiv.org/abs/2502.11995', 'abstract': 'Names are deeply tied to human identity. They can serve as markers of individuality, cultural heritage, and personal history. However, using names as a core indicator of identity can lead to over-simplification of complex identities. When interacting with LLMs, user names are an important point of information for personalisation. Names can enter chatbot conversations through direct user input (requested by chatbots), as part of task contexts such as CV reviews, or as built-in memory features that store user information for personalisation. We study biases associated with names by measuring cultural presumptions in the responses generated by LLMs when presented with common suggestion-seeking queries, which might involve making assumptions about the user. Our analyses demonstrate strong assumptions about cultural identity associated with names present in LLM generations across multiple cultures. Our work has implications for designing more nuanced personalisation systems that avoid reinforcing stereotypes while maintaining meaningful customisation.', 'abstract_zh': '姓名深刻地与人类身份相关联。它们可以作为个人性、文化传承和个人历史的标志。然而，将姓名作为身份的核心指标可能导致对复杂身份的过度简化。在与大模型交互时，用户姓名是实现个性化的重要信息点。姓名可以通过聊天机器人请求的直接用户输入、作为任务上下文（如简历审查）的一部分，或者作为内置的记忆功能存储用户信息以实现个性化进入聊天对话。我们通过衡量大模型在面对常见的建议查询时生成的回应中所体现的文化预设来研究与姓名相关的偏见。我们的分析表明，大模型生成中存在与姓名相关、跨越多种文化的强大文化身份假设。我们的工作对设计更细致的个性化系统具有重要意义，这些系统可以在避免强化刻板印象的同时仍保持有意义的定制。', 'title_zh': '预设文化身份：名称如何塑造LLM响应'}
{'arxiv_id': 'arXiv:2502.11962', 'title': 'Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware Instruction Fine-Tuning', 'authors': 'Tianyi Wu, Jingwei Ni, Bryan Hooi, Jiaheng Zhang, Elliott Ash, See-Kiong Ng, Mrinmaya Sachan, Markus Leippold', 'link': 'https://arxiv.org/abs/2502.11962', 'abstract': 'Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language Models (LLMs), but it may lower their truthfulness. This trade-off arises because IFT steers LLMs to generate responses with long-tail knowledge that is not well covered during pre-training, leading to more informative but less truthful answers when generalizing to unseen tasks. In this paper, we empirically demonstrate this helpfulness-truthfulness trade-off in IFT and propose $\\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs to recognize their uncertainty and explicitly reflect it at the end of their responses. Experimental results show that UNIT-tuned models maintain their helpfulness while distinguishing between certain and uncertain claims, thereby reducing hallucinations.', 'abstract_zh': 'Instruction Fine-tuning (IFT)可以增强大型语言模型（LLMs）的有用性，但可能降低其真实性。在这种权衡中，IFT引导LLMs生成预训练中未充分覆盖的长尾知识，导致在泛化到未见任务时提供更多有用但不那么真实的信息。在本文中，我们实证展示了IFT中的这种有用性-真实性权衡，并提出了一种新的IFT范式$\\textbf{UNIT}$以解决这一问题。UNIT训练LLMs识别自身的不确定性，并在响应结尾明确反映这种不确定性。实验结果表明，UNIT调优的模型在保持有用性的同时，能够区分确定性和不确定性声明，从而减少幻觉。', 'title_zh': '不确定性意识指令微调在帮助性和真实性的权衡导航中'}
{'arxiv_id': 'arXiv:2502.11946', 'title': 'Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction', 'authors': 'Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Brian Li, Changyi Wan, Hanpeng Hu, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Kang An, Wei Ji, Wen Li, Xuan Wen, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chengting Feng, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Jianchang Wu, Jiahong Liu, Jianjian Sun, Jiangjie Zhen, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Shaoliang Pang, Shiliang Yang, Shuli Gao, Siqi Liu, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wenqing He, Wen Sun, Xin Han, Xiaomin Deng, Xiaojia Liu, Xu Zhao, Yanan Wei, Yanbo Yu, Yang Cao, Yangguang Li, Yangzhen Ma, Yanming Xu, Yaqiang Shi, Yilei Wang, Yinmin Zhong', 'link': 'https://arxiv.org/abs/2502.11946', 'abstract': 'Real-time speech interaction, serving as a fundamental interface for human-machine collaboration, holds immense potential. However, current open-source models face limitations such as high costs in voice data collection, weakness in dynamic control, and limited intelligence. To address these challenges, this paper introduces Step-Audio, the first production-ready open-source solution. Key contributions include: 1) a 130B-parameter unified speech-text multi-modal model that achieves unified understanding and generation, with the Step-Audio-Chat version open-sourced; 2) a generative speech data engine that establishes an affordable voice cloning framework and produces the open-sourced lightweight Step-Audio-TTS-3B model through distillation; 3) an instruction-driven fine control system enabling dynamic adjustments across dialects, emotions, singing, and RAP; 4) an enhanced cognitive architecture augmented with tool calling and role-playing abilities to manage complex tasks effectively. Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following. On open-source benchmarks like LLaMA Question, shows 9.3% average performance improvement, demonstrating our commitment to advancing the development of open-source multi-modal language technologies. Our code and models are available at this https URL.', 'abstract_zh': '实时语音交互作为人机协作的基本接口，具有巨大潜力。然而，当前的开源模型面临语音数据采集成本高、动态控制薄弱、智能程度有限等挑战。为了应对这些挑战，本论文介绍了Step-Audio，这是首个生产级开源解决方案。主要贡献包括：1）一个拥有130亿参数的统一语音-文本多模态模型，实现了统一的理解和生成，Step-Audio-Chat版本已开源；2）一个生成性语音数据引擎，建立了经济实惠的声音克隆框架，并通过蒸馏生成了轻量级的Step-Audio-TTS-3B模型；3）一个基于指令的精细控制系统，能够动态调整方言、情感、唱歌和嘻哈等；4）一种增强的认知架构，具有工具调用和角色扮演能力，有效管理复杂任务。基于我们新的StepEval-Audio-360评估基准，Step-Audio在人类评估中达到了最先进水平，尤其是在指令遵循方面。在类似LLaMA Question的开源基准测试中，平均性能提高了9.3%，展示了我们致力于推进开源多模态语言技术开发的承诺。我们的代码和模型可在以下链接获取。', 'title_zh': '步进音频：智能语音交互中的统一理解和生成'}
{'arxiv_id': 'arXiv:2502.11916', 'title': 'EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models', 'authors': 'Jiamin Su, Yibo Yan, Fangteng Fu, Han Zhang, Jingheng Ye, Xiang Liu, Jiahao Huo, Huiyu Zhou, Xuming Hu', 'link': 'https://arxiv.org/abs/2502.11916', 'abstract': "Automated Essay Scoring (AES) plays a crucial role in educational assessment by providing scalable and consistent evaluations of writing tasks. However, traditional AES systems face three major challenges: (1) reliance on handcrafted features that limit generalizability, (2) difficulty in capturing fine-grained traits like coherence and argumentation, and (3) inability to handle multimodal contexts. In the era of Multimodal Large Language Models (MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES capabilities across lexical-, sentence-, and discourse-level traits. By leveraging MLLMs' strengths in trait-specific scoring and multimodal context understanding, EssayJudge aims to offer precise, context-rich evaluations without manual feature engineering, addressing longstanding AES limitations. Our experiments with 18 representative MLLMs reveal gaps in AES performance compared to human evaluation, particularly in discourse-level traits, highlighting the need for further advancements in MLLM-based AES research. Our dataset and code will be available upon acceptance.", 'abstract_zh': '多模态大语言模型时代自动作文评分系统的评测基准：EssayJudge', 'title_zh': 'EssayJudge: 一种评估多模态大型语言模型作文评分能力的多粒度基准'}
{'arxiv_id': 'arXiv:2502.11896', 'title': 'CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement Learning', 'authors': 'Yanxiao Zhao, Yangge Qian, Jingyang Shan, Xiaolin Qin', 'link': 'https://arxiv.org/abs/2502.11896', 'abstract': "Reinforcement learning (RL) in continuous action spaces encounters persistent challenges, such as inefficient exploration and convergence to suboptimal solutions. To address these limitations, we propose CAMEL, a novel framework integrating LLM-generated suboptimal policies into the RL training pipeline. CAMEL leverages dynamic action masking and an adaptive epsilon-masking mechanism to guide exploration during early training stages while gradually enabling agents to optimize policies independently. At the core of CAMEL lies the integration of Python-executable suboptimal policies generated by LLMs based on environment descriptions and task objectives. Although simplistic and hard-coded, these policies offer valuable initial guidance for RL agents. To effectively utilize these priors, CAMEL employs masking-aware optimization to dynamically constrain the action space based on LLM outputs. Additionally, epsilon-masking gradually reduces reliance on LLM-generated guidance, enabling agents to transition from constrained exploration to autonomous policy refinement. Experimental validation on Gymnasium MuJoCo environments demonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated policies significantly improve sample efficiency, achieving performance comparable to or surpassing expert masking baselines. For Walker2d-v4, where LLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust RL performance without notable degradation, highlighting the framework's adaptability across diverse tasks. While CAMEL shows promise in enhancing sample efficiency and mitigating convergence challenges, these issues remain open for further research. Future work aims to generalize CAMEL to multimodal LLMs for broader observation-action spaces and automate policy evaluation, reducing human intervention and enhancing scalability in RL training pipelines.", 'abstract_zh': '连续动作空间中的强化学习（RL）面临持续的挑战，如探索效率低下和收敛到次优解。为此，我们提出了一种名为CAMEL的新型框架，该框架将LLM生成的次优策略集成到RL训练管道中。CAMEL通过动态动作掩码和自适应ε-掩码机制，在早期训练阶段引导探索，并逐步使智能体能够独立优化策略。CAMEL的核心在于将基于环境描述和任务目标由LLM生成的可执行Python次优策略的集成。尽管这些策略简单且硬编码，但它们为RL智能体提供了初始有价值的指导。为了有效利用这些先验知识，CAMEL采用掩码感知优化动态限制动作空间，基于LLM输出。此外，ε-掩码逐渐减少对LLM生成指导的依赖，使智能体从受限探索过渡到自主策略优化。在Gymnasium MuJoCo环境中进行的经验验证表明了CAMEL的有效性。在Hopper-v4和Ant-v4中，LLM生成的策略显著提高了样本效率，性能达到或超过了专家掩码基准。在Walker2d-v4中，由于LLM难以准确建模双足步态动力学，CAMEL保持了稳健的RL性能，没有明显下降，突显了该框架在不同任务中的适应性。虽然CAMEL在提高样本效率和缓解收敛挑战方面显示出潜力，但这些问题仍有待进一步研究。未来工作旨在将CAMEL推广到多模态LLM，以适用于更广泛的观测-动作空间，并自动评估策略，减少人为干预，增强RL训练管道的可扩展性。', 'title_zh': 'CAMEL: 由大规模语言模型实现的连续动作掩蔽强化学习'}
{'arxiv_id': 'arXiv:2502.11895', 'title': 'Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?', 'authors': 'Jacob Nielsen, Peter Schneider-Kamp, Lukas Galke', 'link': 'https://arxiv.org/abs/2502.11895', 'abstract': 'Large language models (LLMs) require immense resources for training and inference. Quantization, a technique that reduces the precision of model parameters, offers a promising solution for improving LLM efficiency and sustainability. While post-training quantization methods typically achieve 4-8 bits per parameter, recent research suggests that training LLMs with 1.58 bits per weight parameter from scratch can maintain model accuracy while greatly reducing memory requirements and energy consumption at inference time. Here, we investigate a training strategy for quantization-aware pre-training, where the models are first trained with 16-bit precision and then transition into 1.58-bit quantization-aware training. Our results on 11 downstream tasks show that this 16-to-1.58-bit training strategy is preferable over full 1.58-bit training and leaves models closer to those which have undergone 16-bit training. We further investigate the effects of retaining the optimizer state at the transition point and gradually phasing in quantization strength -- finding that both techniques alleviate the magnitude of loss spikes, but also that these effects can be compensated through further training.', 'abstract_zh': '大规模语言模型（LLMs）的训练和推理需要巨额资源。量化技术通过降低模型参数精度，为提高LLM效率和可持续性提供了有前景的解决方案。虽然后训练量化方法通常可实现每参数4-8位，但近期研究表明，从零开始训练具有1.58位每权重参数的LLMs可以在保持模型精度的同时大幅降低推理时的内存需求和能耗。在此，我们研究了一种量化感知预训练的训练策略，首先使用16位精度训练模型，然后过渡到1.58位量化感知训练。我们在11个下游任务上的结果显示，这种16位到1.58位的训练策略优于完全1.58位训练，并使模型更接近于经过16位训练的模型。我们进一步研究了在过渡点保留优化器状态和逐步引入量化强度的效果——发现这两种技术都能减轻损失峰值的幅度，但这些效果可以通过进一步训练来弥补。', 'title_zh': '持续量化感知预训练：BitNet语言模型在何时从16位转换到1.58位预训练？'}
{'arxiv_id': 'arXiv:2502.11886', 'title': 'LIMR: Less is More for RL Scaling', 'authors': 'Xuefeng Li, Haoyang Zou, Pengfei Liu', 'link': 'https://arxiv.org/abs/2502.11886', 'abstract': "In this paper, we ask: what truly determines the effectiveness of RL training data for enhancing language models' reasoning capabilities? While recent advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack of transparency about training data requirements has hindered systematic progress. Starting directly from base models without distillation, we challenge the assumption that scaling up RL training data inherently improves performance. we demonstrate that a strategically selected subset of just 1,389 samples can outperform the full 8,523-sample dataset. We introduce Learning Impact Measurement (LIM), an automated method to evaluate and prioritize training samples based on their alignment with model learning trajectories, enabling efficient resource utilization and scalable implementation. Our method achieves comparable or even superior performance using only 1,389 samples versus the full 8,523 samples dataset. Notably, while recent data-efficient approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it significantly underperforms at 7B-scale through supervised fine-tuning (SFT). In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results fundamentally reshape our understanding of RL scaling in LLMs, demonstrating that precise sample selection, rather than data scale, may be the key to unlocking enhanced reasoning capabilities. For reproducible research and future innovation, we are open-sourcing LIMR, including implementation of LIM, training and evaluation code, curated datasets, and trained models at this https URL.", 'abstract_zh': '在本论文中，我们探讨了什么真正决定了增强语言模型推理能力的RL训练数据的有效性。尽管像o1、Deepseek R1和Kimi1.5等最近的进步展示了RL的潜力，但缺乏关于训练数据需求的透明度阻碍了系统的进展。直接从基础模型开始而无需知识蒸馏，我们挑战了RL训练数据规模扩大必然会提升性能的假设。我们证明，一个策略性选择的仅1,389个样本的小子集就能优于包含8,523个样本的完整数据集。我们引入了一种自动化的方法——学习影响度量（LIM），以评估和优先考虑训练样本的依据是它们与模型学习轨迹的对齐情况，从而实现资源的有效利用和可扩展的实现。仅使用1,389个样本，我们的方法就能达到与8,523个样本数据集相当甚至更好的性能。值得注意的是，尽管最近的数据高效方法（如LIMO和s1）在32B规模的模型上表现出潜力，我们发现它们在7B规模的模型上通过监督微调表现显著不如预期。相反，基于RL的LIMR在AIME24上取得了16.7%的更高准确率，并在MATH500上分别优于LIMO和s1 13.0%和22.2%。这些结果从根本上重塑了我们对大型语言模型中RL扩展的理解，表明精确的选择样本而不是数据规模可能是解锁增强推理能力的关键。为了实现可再现的研究和未来的创新，我们开源了LIMR，包括LIM的实现、训练和评估代码、精选数据集和训练模型，详情请访问此链接。', 'title_zh': 'LIMR: 少就是多的RL扩展方法'}
{'arxiv_id': 'arXiv:2502.11880', 'title': 'Bitnet.cpp: Efficient Edge Inference for Ternary LLMs', 'authors': 'Jinheng Wang, Hansong Zhou, Ting Song, Shijie Cao, Yan Xia, Ting Cao, Jianyu Wei, Shuming Ma, Hongyu Wang, Furu Wei', 'link': 'https://arxiv.org/abs/2502.11880', 'abstract': 'The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has spurred interest in ternary LLMs. Despite this, research and practical applications focusing on efficient edge inference for ternary LLMs remain scarce. To bridge this gap, we introduce this http URL, an inference system optimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix multiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs, this http URL incorporates a novel mpGEMM library to facilitate sub-2-bits-per-weight, efficient and lossless inference. The library features two core solutions: Ternary Lookup Table (TL), which addresses spatial inefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S), which ensures lossless edge inference, both enabling high-speed inference. Our experiments show that this http URL achieves up to a 6.25x increase in speed over full-precision baselines and up to 2.32x over low-bit baselines, setting new benchmarks in the field. Additionally, we expand TL to element-wise lookup table (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and empirical evidence of its considerable potential. this http URL is publicly available at this https URL , offering a sophisticated solution for the efficient and practical deployment of edge LLMs.', 'abstract_zh': 'The 出现1比特大型语言模型（LLMs），如BitNet b1.58，促进了三值LLMs的研究。尽管如此，针对三值LLMs的高效边缘推理研究和应用仍然稀缺。为填补这一空白，我们介绍了此系统，该系统针对BitNet b1.58和三值LLMs进行了优化。鉴于混合精度矩阵乘法（mpGEMM）占据了三值LLMs推理时间的主要部分，此系统集成了一个新颖的mpGEMM库，以实现每权重少于2比特、高效且无损的推理。该库包含两个核心解决方案：三值查找表（TL），解决了一维位方法的空间效率问题；以及带有尺度的整数2（I2_S），确保了无损边缘推理，两者共同实现高速推理。我们的实验结果显示，该系统在全精度基准上的速度提高了多达6.25倍，在低比特基准上的速度提高了2.32倍，从而在领域内建立了新的基准。此外，我们在附录中将TL扩展到元素级查找表（ELUT），针对低比特LLMs，提供了理论和实验证据，展示了其巨大的潜力。此系统在 https://this-url 提供，为边缘LLMs的高效和实践部署提供了复杂的解决方案。', 'title_zh': 'Bitnet.cpp: Ternary LLMs的高效边端推理'}
{'arxiv_id': 'arXiv:2502.11863', 'title': 'FedEAT: A Robustness Optimization Framework for Federated LLMs', 'authors': 'Yahao Pang, Xingyuan Wu, Xiaojin Zhang, Wei Chen, Hai Jin', 'link': 'https://arxiv.org/abs/2502.11863', 'abstract': 'Significant advancements have been made by Large Language Models (LLMs) in the domains of natural language understanding and automated content creation. However, they still face persistent problems, including substantial computational costs and inadequate availability of training data. The combination of Federated Learning (FL) and LLMs (federated LLMs) offers a solution by leveraging distributed data while protecting privacy, which positions it as an ideal choice for sensitive domains. However, Federated LLMs still suffer from robustness challenges, including data heterogeneity, malicious clients, and adversarial attacks, which greatly hinder their applications. We first introduce the robustness problems in federated LLMs, to address these challenges, we propose FedEAT (Federated Embedding space Adversarial Training), a novel framework that applies adversarial training in the embedding space of client LLM and employs a robust aggregation approach, specifically geometric median aggregation, to enhance the robustness of Federated LLMs. Our experiments demonstrate that FedEAT effectively improves the robustness of Federated LLMs with minimal performance loss.', 'abstract_zh': '联邦学习与大规模语言模型结合（联邦LLM）在自然语言理解与自动化内容创作领域的显著进展虽然受到了计算成本高昂和训练数据不足的问题限制，但通过利用联邦学习的技术可以解决这些挑战并保护隐私，使其成为敏感领域的一个理想选择。然而，联邦LLM仍然面临着鲁棒性方面的挑战，包括数据异质性、恶意客户端以及 adversarial 攻击，这些都极大地阻碍了其应用。我们首先介绍了联邦LLM中的鲁棒性问题，为了解决这些问题，我们提出了一种名为FedEAT（联邦嵌入空间对抗训练）的新型框架，在客户端LLM的嵌入空间中应用对抗训练，并采用几何中位数聚合等鲁棒聚合方法来增强联邦LLM的鲁棒性。实验结果表明，FedEAT能够在 minimal 性能损失的情况下显著提升联邦LLM的鲁棒性。', 'title_zh': 'FedEAT：联邦大规模语言模型的稳健性优化框架'}
{'arxiv_id': 'arXiv:2502.11844', 'title': 'BaxBench: Can LLMs Generate Correct and Secure Backends?', 'authors': 'Mark Vero, Niels Mündler, Victor Chibotaru, Veselin Raychev, Maximilian Baader, Nikola Jovanović, Jingxuan He, Martin Vechev', 'link': 'https://arxiv.org/abs/2502.11844', 'abstract': 'The automatic generation of programs has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could successfully execute security exploits on more than half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs.', 'abstract_zh': '自动生成程序一直是计算机科学中的一个基础挑战。近期基准测试表明，大规模语言模型（LLMs）能够有效在函数级别生成代码、进行代码编辑以及解决算法编程任务。然而，要实现完全自动化，LLMs 应能够生成生产级别的、自包含的应用模块。为了评估 LLMs 解决这一挑战的能力，我们引入了 BaxBench，这是一个新型评估基准，包含 392 个用于生成后端应用的任务。我们重点关注后端的原因有三：（i）它们具有实际相关性，构成了大多数现代网络和云软件的核心组件；（ii）它们难以实现正确功能，需要多个函数和文件才能实现所需的功能；（iii）它们是安全关键的，因为它们暴露给不可信的第三方，确保安全解决方案防止部署时攻击是必然要求。BaxBench 通过全面的测试案例验证生成应用的功能性，并通过端到端利用来评估其安全暴露程度。我们的实验揭示了当前 LLMs 在功能性和安全性方面的关键局限性：（i）即便是最好的模型 OpenAI o1，代码正确性也只能达到 60%；（ii）平均而言，我们能够成功执行对每种 LLM 生成的正确程序的一半以上进行的安全利用；（iii）在不太流行的后端框架中，模型进一步难以生成正确且安全的应用。BaxBench 上的进步标志着朝着使用 LLMs 实现自主且安全的软件开发的重要一步。', 'title_zh': 'BaxBench: LLMs能生成正确且安全的后端代码吗？'}
{'arxiv_id': 'arXiv:2502.11843', 'title': 'Can LLM Agents Maintain a Persona in Discourse?', 'authors': 'Pranav Bhandari, Nicolas Fay, Michael Wise, Amitava Datta, Stephanie Meek, Usman Naseem, Mehwish Nasim', 'link': 'https://arxiv.org/abs/2502.11843', 'abstract': 'Large Language Models (LLMs) are widely used as conversational agents, exploiting their capabilities in various sectors such as education, law, medicine, and more. However, LLMs are often subjected to context-shifting behaviour, resulting in a lack of consistent and interpretable personality-aligned interactions. Adherence to psychological traits lacks comprehensive analysis, especially in the case of dyadic (pairwise) conversations. We examine this challenge from two viewpoints, initially using two conversation agents to generate a discourse on a certain topic with an assigned personality from the OCEAN framework (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This is followed by using multiple judge agents to infer the original traits assigned to explore prediction consistency, inter-model agreement, and alignment with the assigned personality. Our findings indicate that while LLMs can be guided toward personality-driven dialogue, their ability to maintain personality traits varies significantly depending on the combination of models and discourse settings. These inconsistencies emphasise the challenges in achieving stable and interpretable personality-aligned interactions in LLMs.', 'abstract_zh': '大型语言模型（LLMs）广泛用作对话代理，在教育、法律、医学等领域发挥其能力。然而，LLMs经常表现出上下文切换行为，导致缺乏一致性和可解释性的个性一致互动。个性特征的遵循缺乏全面分析，尤其是在双人对话的情况下。我们从两个角度来看待这一挑战：首先，使用两个对话代理生成某一主题的对话，每个特性（开放性、尽责性、外向性、和藹性、神经质）指派为高/低；接着，使用多个评判代理推断原始指派的特性，以探索预测一致性、模型间一致性以及与指派个性的匹配度。我们的研究结果表明，虽然LLMs可以被引导进行个性驱动的对话，但它们在维持个性特质方面的能力因模型组合和对话设置而异。这些不一致性强调了在LLMs中实现稳定和可解释的个性一致互动的挑战。', 'title_zh': 'LLM代理在 discourse 中能否维持人设？'}
{'arxiv_id': 'arXiv:2502.11829', 'title': 'Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities', 'authors': 'Hanbin Wang, Xiaoxuan Zhou, Zhipeng Xu, Keyuan Cheng, Yuxin Zuo, Kai Tian, Jingwei Song, Junting Lu, Wenhui Hu, Xueyang Liu', 'link': 'https://arxiv.org/abs/2502.11829', 'abstract': "This paper introduces Code-Vision, a benchmark designed to evaluate the logical understanding and code generation capabilities of Multimodal Large Language Models (MLLMs). It challenges MLLMs to generate a correct program that fulfills specific functionality requirements based on a given flowchart, which visually represents the desired algorithm or process. Code-Vision comprises three subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding abilities across basic programming, algorithmic, and mathematical problem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision. Experimental results demonstrate that there is a large performance difference between proprietary and open-source models. On Hard problems, GPT-4o can achieve 79.3% pass@1, but the best open-source model only achieves 15%. Further experiments reveal that Code-Vision can pose unique challenges compared to other multimodal reasoning benchmarks MMCode and MathVista. We also explore the reason for the poor performance of the open-source models. All data and codes are available at this https URL.", 'abstract_zh': '这篇论文介绍了Code-Vision，一个用于评估多模态大型语言模型（MLLMs）的逻辑理解和代码生成能力的基准。Code-Vision挑战MLLMs根据给定的流程图生成满足特定功能要求的正确程序，流程图可视化表示所需的算法或过程。Code-Vision包含三个子集：HumanEval-V、Algorithm和MATH，分别评估MLLMs在基础编程、算法和数学问题解决领域的编码能力。我们的实验在Code-Vision上评估了12个MLLMs。实验结果表明，专有模型和开源模型之间存在巨大的性能差异。在难题上，GPT-4o可达到79.3%的pass@1，而最好的开源模型仅达到15%。进一步的实验揭示，与多模态推理基准MMCode和MathVista相比，Code-Vision提出了独特的挑战。我们还探讨了开源模型表现不佳的原因。所有数据和代码可在以下链接获取。', 'title_zh': 'Code-Vision: 评估多模态LLM的逻辑理解与代码生成能力'}
{'arxiv_id': 'arXiv:2502.11812', 'title': 'Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis', 'authors': 'Xu Wang, Yan Hu, Wenyu Du, Reynold Cheng, Benyou Wang, Difan Zou', 'link': 'https://arxiv.org/abs/2502.11812', 'abstract': 'Fine-tuning significantly improves the performance of Large Language Models (LLMs), yet its underlying mechanisms remain poorly understood. This paper aims to provide an in-depth interpretation of the fine-tuning process through circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike previous studies \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} that focus on tasks where pre-trained models already perform well, we develop a set of mathematical tasks where fine-tuning yields substantial performance gains, which are closer to the practical setting. In our experiments, we identify circuits at various checkpoints during fine-tuning and examine the interplay between circuit analysis, fine-tuning methods, and task complexities. First, we find that while circuits maintain high node similarity before and after fine-tuning, their edges undergo significant changes, which is in contrast to the previous work \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} that show circuits only add some additional components after fine-tuning. Based on these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method, which assigns ranks to layers based on edge changes in the circuits. Experimental results demonstrate that our circuit-based LoRA algorithm achieves an average performance improvement of 2.46\\% over standard LoRA with similar parameter sizes. Furthermore, we explore how combining circuits from subtasks can enhance fine-tuning in compositional tasks, providing new insights into the design of such tasks and deepening the understanding of circuit dynamics and fine-tuning mechanisms.', 'abstract_zh': 'Fine-tuning显著提高了大型语言模型（LLMs）的性能，但其 underlying机制仍不明确。通过电路分析，一种机制可解释性（MI）中的流行工具，本文旨在深入解释fine-tuning过程。不同于先前研究\\[prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity\\]主要关注预训练模型已在其中表现良好的任务，我们开发了一组数学任务，在这些任务中fine-tuning带来了显著的性能提升，更接近实际应用场景。在实验中，我们识别了fine-tuning过程中的多个检查点处的电路，并探讨了电路分析、fine-tuning方法和任务复杂性之间的相互作用。首先，我们发现虽然电路在fine-tuning前后节点相似度保持较高，但其边经历了显著变化，这与先前工作\\[prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity\\]中仅显示fine-tuning后电路增加了一些额外组件的观察结果相反。基于这些观察，我们开发了一种电路感知低秩适应（LoRA）方法，根据电路中边的变化为层分配秩。实验结果表明，我们的电路基础LoRA算法在相似参数量的情况下，平均性能改进了2.46%。此外，我们探索了组合子任务电路如何增强合成任务的fine-tuning，为这类任务的设计提供了新的见解，并加深了对电路动态和fine-tuning机制的理解。', 'title_zh': '通过电路分析理解大规模语言模型微调机制'}
{'arxiv_id': 'arXiv:2502.11771', 'title': 'The Validation Gap: A Mechanistic Analysis of How Language Models Compute Arithmetic but Fail to Validate It', 'authors': 'Leonardo Bertolazzi, Philipp Mondorf, Barbara Plank, Raffaella Bernardi', 'link': 'https://arxiv.org/abs/2502.11771', 'abstract': "The ability of large language models (LLMs) to validate their output and identify potential errors is crucial for ensuring robustness and reliability. However, current research indicates that LLMs struggle with self-correction, encountering significant challenges in detecting errors. While studies have explored methods to enhance self-correction in LLMs, relatively little attention has been given to understanding the models' internal mechanisms underlying error detection. In this paper, we present a mechanistic analysis of error detection in LLMs, focusing on simple arithmetic problems. Through circuit analysis, we identify the computational subgraphs responsible for detecting arithmetic errors across four smaller-sized LLMs. Our findings reveal that all models heavily rely on $\\textit{consistency heads}$--attention heads that assess surface-level alignment of numerical values in arithmetic solutions. Moreover, we observe that the models' internal arithmetic computation primarily occurs in higher layers, whereas validation takes place in middle layers, before the final arithmetic results are fully encoded. This structural dissociation between arithmetic computation and validation seems to explain why current LLMs struggle to detect even simple arithmetic errors.", 'abstract_zh': '大型语言模型中错误检测机制的分析：以简单算术问题为例', 'title_zh': '验证差距：语言模型在计算算术问题时验证不足的机理分析'}
{'arxiv_id': 'arXiv:2502.11751', 'title': 'Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning', 'authors': 'Yuqi Pang, Bowen Yang, Haoqin Tu, Yun Cao, Zeyu Zhang', 'link': 'https://arxiv.org/abs/2502.11751', 'abstract': "Although Large Language Models (LLMs) excel in reasoning and generation for language tasks, they are not specifically designed for multimodal challenges. Training Multimodal Large Language Models (MLLMs), however, is resource-intensive and constrained by various training limitations. In this paper, we propose the Modular-based Visual Contrastive Decoding (MVCD) framework to move this obstacle. Our framework leverages LLMs' In-Context Learning (ICL) capability and the proposed visual contrastive-example decoding (CED), specifically tailored for this framework, without requiring any additional training. By converting visual signals into text and focusing on contrastive output distributions during decoding, we can highlight the new information introduced by contextual examples, explore their connections, and avoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual perception to make it see and reason over the input visuals. To demonstrate MVCD's effectiveness, we conduct experiments with four LLMs across five question answering datasets. Our results not only show consistent improvement in model accuracy but well explain the effective components inside our decoding strategy. Our code will be available at this https URL.", 'abstract_zh': '尽管大型语言模型（LLMs）在语言任务上的推理和生成表现出色，但它们并未专门设计用于多模态挑战。然而，训练多模态大型语言模型（MLLMs）是资源密集型的，并受到各种训练限制。在本文中，我们提出了一种模块化视觉对比解码（MVCD）框架，以克服这一障碍。我们的框架利用了LLMs的上下文学习（ICL）能力，并提出了针对该框架的视觉对比实例解码（CED），无需任何额外训练。通过将视觉信号转换为文本，在解码过程中专注于对比输出分布，我们可以突出上下文示例引入的新信息，探索它们之间的联系，并避免过度依赖先验编码知识。MVCD增强了LLMs的视觉感知能力，使其能够理解和推理输入的视觉内容。为了证明MVCD的有效性，我们在五个问答数据集中对四个LLM进行了实验。我们的结果不仅展示了模型准确性的持续改进，还详细解释了我们解码策略中的有效组件。我们的代码将在以下链接处提供：this https URL。', 'title_zh': '语言模型视觉对比解码：面向LLM多模态推理'}
{'arxiv_id': 'arXiv:2502.11741', 'title': 'SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL', 'authors': 'Shuai Lyu, Haoran Luo, Zhonghong Ou, Yifan Zhu, Xiaoran Shang, Yang Qin, Meina Song', 'link': 'https://arxiv.org/abs/2502.11741', 'abstract': 'The Text-to-SQL(Text2SQL) task aims to convert natural language queries into executable SQL queries. Thanks to the application of large language models (LLMs), significant progress has been made in this field. However, challenges such as model scalability, limited generation space, and coherence issues in SQL generation still persist. To address these issues, we propose SQL-o1, a Self-Reward-based heuristic search method designed to enhance the reasoning ability of LLMs in SQL query generation. SQL-o1 combines Monte Carlo Tree Search (MCTS) for heuristic process-level search and constructs a Schema-Aware dataset to help the model better understand database schemas. Extensive experiments on the Bird and Spider datasets demonstrate that SQL-o1 improves execution accuracy by 10.8\\% on the complex Bird dataset compared to the latest baseline methods, even outperforming GPT-4-based approaches. Additionally, SQL-o1 excels in few-shot learning scenarios and shows strong cross-model transferability. Our code is publicly available at:this https URL.', 'abstract_zh': 'Text-to-SQL(Text2SQL)任务旨在将自然语言查询转换为可执行的SQL查询。得益于大型语言模型（LLMs）的应用，该领域取得了显著进步。然而，模型可扩展性、生成空间有限以及SQL生成中的连贯性问题仍然存在。为解决这些问题，我们提出了SQL-o1，这是一种基于自我奖励的启发式搜索方法，旨在增强LLMs在SQL查询生成中的推理能力。SQL-o1结合了蒙特卡洛树搜索（MCTS）进行启发式过程级搜索，并构建了一个模式感知数据集以帮助模型更好地理解数据库模式。在Bird和Spider数据集上的广泛实验表明，与最新基准方法相比，SQL-o1在复杂Bird数据集上的执行准确率提高了10.8%，甚至优于基于GPT-4的方法。此外，SQL-o1在少量样本学习场景中表现出色，并显示出较强的跨模型迁移能力。相关代码已公开。', 'title_zh': 'SQL-o1: 一种自我奖励启发式动态搜索方法实现文本到SQL'}
{'arxiv_id': 'arXiv:2502.11736', 'title': 'ReviewEval: An Evaluation Framework for AI-Generated Reviews', 'authors': 'Chavvi Kirtani, Madhav Krishan Garg, Tejash Prasad, Tanmay Singhal, Murari Mandal, Dhruv Kumar', 'link': 'https://arxiv.org/abs/2502.11736', 'abstract': "The escalating volume of academic research, coupled with a shortage of qualified reviewers, necessitates innovative approaches to peer review. While large language model (LLMs) offer potential for automating this process, their current limitations include superficial critiques, hallucinations, and a lack of actionable insights. This research addresses these challenges by introducing a comprehensive evaluation framework for AI-generated reviews, that measures alignment with human evaluations, verifies factual accuracy, assesses analytical depth, and identifies actionable insights. We also propose a novel alignment mechanism that tailors LLM-generated reviews to the unique evaluation priorities of individual conferences and journals. To enhance the quality of these reviews, we introduce a self-refinement loop that iteratively optimizes the LLM's review prompts. Our framework establishes standardized metrics for evaluating AI-based review systems, thereby bolstering the reliability of AI-generated reviews in academic research.", 'abstract_zh': 'escalating 学术研究 volume 的增加与合格评审人短缺并存， necessitates 创新性的同行评审方法。尽管大规模语言模型（LLMs）在自动化这一过程方面具有潜力，但它们目前的局限性包括表面化的评论、胡言乱语以及缺乏可操作的见解。本研究通过引入一个全面的 AI 生成评论评估框架来应对这些挑战，该框架衡量与人力评估的一致性，验证事实准确性，评估分析深度，并识别可操作的见解。我们还提出了一种新颖的一致性机制，使 LLM 生成的评论符合各个会议和期刊的独特评估优先级。为了提高这些评论的质量，我们引入了一个自我精炼循环，迭代优化 LLM 的评论提示。该框架建立了评估基于 AI 的评审系统的标准化指标，从而增强了 AI 生成评审在学术研究中的可靠性。', 'title_zh': 'ReviewEval：AI生成评论的评估框架'}
{'arxiv_id': 'arXiv:2502.11705', 'title': 'LLM Agents Making Agent Tools', 'authors': 'Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelović, Jakob Nikolas Kather', 'link': 'https://arxiv.org/abs/2502.11705', 'abstract': 'Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains which demand large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, a novel agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a short task description and a repository URL, ToolMaker autonomously installs required dependencies and generates code to perform the task, using a closed-loop self-correction mechanism to iteratively diagnose and rectify errors. To evaluate our approach, we introduce a benchmark comprising 15 diverse and complex computational tasks spanning both medical and non-medical domains with over 100 unit tests to objectively assess tool correctness and robustness. ToolMaker correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows.', 'abstract_zh': '工具使用使大型语言模型（LLMs）成为能够通过动态利用外部软件组件执行复杂多步任务的强大代理。然而，这些工具需要由人类开发人员预先实现，限制了LLM代理在需要大量高度专门化工具的领域（如生命科学和医学）的应用。鉴于科学研究中公共代码存储库日益增长的趋势，我们提出了ToolMaker，一种新型的代理框架，能够自主将带代码的论文转换为LLM兼容的工具。给定简要的任务描述和仓库URL，ToolMaker自主安装所需的依赖项并生成代码以执行任务，利用闭环自我纠正机制迭代诊断并修正错误。为了评估我们的方法，我们引入了一个基准测试集，其中包括15个涵盖医学和非医学领域的多样且复杂的计算任务，包含超过100个单元测试，以客观评估工具的正确性和稳健性。ToolMaker成功实现80%的任务，显著优于当前最先进的软件工程代理。因此，ToolMaker是完全自主的基于代理的科学研究工作流的一个重要步骤。', 'title_zh': 'LLM代理制作代理工具'}
{'arxiv_id': 'arXiv:2502.11684', 'title': 'MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task', 'authors': 'Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Xin Xu, Mengdi Zhang, Jian Shao, Yueting Zhuang', 'link': 'https://arxiv.org/abs/2502.11684', 'abstract': 'Mathematical reasoning represents a critical frontier in advancing large language models (LLMs). While step-by-step approaches have emerged as the dominant paradigm for mathematical problem-solving in LLMs, the quality of reasoning steps in training data fundamentally constrains the performance of the models. Recent studies has demonstrated that more detailed intermediate steps can enhance model performance, yet existing methods for step expansion either require more powerful external models or incur substantial computational costs. In this paper, we introduce MathFimer, a novel framework for mathematical reasoning step expansion inspired by the "Fill-in-the-middle" task from code completion. By decomposing solution chains into prefix-suffix pairs and training models to reconstruct missing intermediate steps, we develop a specialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM dataset. We then apply these models to enhance existing mathematical reasoning datasets by inserting detailed intermediate steps into their solution chains, creating MathFimer-expanded versions. Through comprehensive experiments on multiple mathematical reasoning datasets, including MathInstruct, MetaMathQA and etc., we demonstrate that models trained on MathFimer-expanded data consistently outperform their counterparts trained on original data across various benchmarks such as GSM8K and MATH. Our approach offers a practical, scalable solution for enhancing mathematical reasoning capabilities in LLMs without relying on powerful external models or expensive inference procedures.', 'abstract_zh': '数学推理是推动大规模语言模型（LLMs）发展的关键前沿领域。尽管逐步方法已成为LLMs中数学问题求解的主要范式，但训练数据中推理步骤的质量从根本上限制了模型的性能。最近的研究表明，更详细的中间步骤可以提升模型性能，但现有的方法要么需要更强大的外部模型，要么会带来巨大的计算成本。在本文中，我们介绍了一种名为MathFimer的新型数学推理步骤扩展框架，该框架受到代码完成任务中“填空”任务的启发。通过将解答链分解为前缀-后缀对，并训练模型重建缺失的中间步骤，我们在我们精心策划的NuminaMath-FIM数据集上开发了一个专门的模型，MathFimer-7B。然后，我们使用这些模型通过在原始解答链中插入详细的中间步骤来增强现有的数学推理数据集，创建了MathFimer扩展版本。通过在MathInstruct、MetaMathQA等多个数学推理数据集上的综合性实验，我们证明了基于MathFimer扩展数据训练的模型在包括GSM8K和MATH在内的各种基准测试中始终优于基于原始数据训练的模型。我们的方法提供了一种无需依赖强大外部模型或昂贵推断程序的实际可扩展方案，以提升LLMs的数学推理能力。', 'title_zh': 'MathFimer：通过填充中间任务扩展推理步骤以增强数学推理能力'}
{'arxiv_id': 'arXiv:2502.11681', 'title': 'RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars', 'authors': 'Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari', 'link': 'https://arxiv.org/abs/2502.11681', 'abstract': 'Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment--factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at this https URL.', 'abstract_zh': '低花费无调优上下文学习提升大型语言模型对齐toy实验', 'title_zh': 'RIDE：通过重塑情境学习示范范例增强大型语言模型对齐'}
{'arxiv_id': 'arXiv:2502.11671', 'title': 'Diversity-Oriented Data Augmentation with Large Language Models', 'authors': 'Zaitian Wang, Jinghan Zhang, Xinhao Zhang, Kunpeng Liu, Pengfei Wang, Yuanchun Zhou', 'link': 'https://arxiv.org/abs/2502.11671', 'abstract': "Data augmentation is an essential technique in natural language processing (NLP) for enriching training datasets by generating diverse samples. This process is crucial for improving the robustness and generalization capabilities of NLP models. However, a significant challenge remains: \\textit{Insufficient Attention to Sample Distribution Diversity}. Most existing methods focus on increasing the sample numbers while neglecting the sample distribution diversity, which can lead to model overfitting. In response, we explore data augmentation's impact on dataset diversity and propose a \\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data \\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). % \\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning approach to train an LLM as a diverse paraphraser, which is capable of augmenting textual datasets by generating diversified paraphrases. Then, we apply the LLM paraphraser to a selected coreset of highly informative samples and integrate the paraphrases with the original data to create a more diverse augmented dataset. Finally, we conduct extensive experiments on 12 real-world textual datasets. The results show that our fine-tuned LLM augmenter improves diversity while preserving label consistency, thereby enhancing the robustness and performance of downstream tasks. Specifically, it achieves an average performance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more than three percentage points.", 'abstract_zh': '数据增强：关注样本分布多样性以提升自然语言处理模型的鲁棒性和泛化能力', 'title_zh': '面向多样性数据增强的大语言模型方法'}
{'arxiv_id': 'arXiv:2502.11647', 'title': 'DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing', 'authors': 'Yi Wang, Fenghua Weng, Sibei Yang, Zhan Qin, Minlie Huang, Wenjie Wang', 'link': 'https://arxiv.org/abs/2502.11647', 'abstract': "Large Language Models (LLMs) are widely applied in decision making, but their deployment is threatened by jailbreak attacks, where adversarial users manipulate model behavior to bypass safety measures. Existing defense mechanisms, such as safety fine-tuning and model editing, either require extensive parameter modifications or lack precision, leading to performance degradation on general tasks, which is unsuitable to post-deployment safety alignment. To address these challenges, we propose DELMAN (Dynamic Editing for LLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for precise, dynamic protection against jailbreak attacks. DELMAN directly updates a minimal set of relevant parameters to neutralize harmful behaviors while preserving the model's utility. To avoid triggering a safe response in benign context, we incorporate KL-divergence regularization to ensure the updated model remains consistent with the original model when processing benign queries. Experimental results demonstrate that DELMAN outperforms baseline methods in mitigating jailbreak attacks while preserving the model's utility, and adapts seamlessly to new attack instances, providing a practical and efficient solution for post-deployment model protection.", 'abstract_zh': '动态编辑以抵御大型语言模型 jailbreak 攻击（DELMAN）', 'title_zh': 'DELMAN: 基于模型编辑动态防御大型语言模型越狱攻击'}
{'arxiv_id': 'arXiv:2502.11614', 'title': 'Is Human-Like Text Liked by Humans? Multilingual Human Detection and Preference Against AI', 'authors': 'Yuxia Wang, Rui Xing, Jonibek Mansurov, Giovanni Puccetti, Zhuohan Xie, Minh Ngoc Ta, Jiahui Geng, Jinyan Su, Mervat Abassy, Saad El Dine Ahmed, Kareem Elozeiri, Nurkhan Laiyk, Maiya Goloburda, Tarek Mahmoud, Raj Vardhan Tomar, Alexander Aziz, Ryuto Koike, Masahiro Kaneko, Artem Shelmanov, Ekaterina Artemova, Vladislav Mikhailov, Akim Tsvigun, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, Preslav Nakov', 'link': 'https://arxiv.org/abs/2502.11614', 'abstract': 'Prior studies have shown that distinguishing text generated by large language models (LLMs) from human-written one is highly challenging, and often no better than random guessing. To verify the generalizability of this finding across languages and domains, we perform an extensive case study to identify the upper bound of human detection accuracy. Across 16 datasets covering 9 languages and 9 domains, 19 annotators achieved an average detection accuracy of 87.6%, thus challenging previous conclusions. We find that major gaps between human and machine text lie in concreteness, cultural nuances, and diversity. Prompting by explicitly explaining the distinctions in the prompts can partially bridge the gaps in over 50% of the cases. However, we also find that humans do not always prefer human-written text, particularly when they cannot clearly identify its source.', 'abstract_zh': '先前的研究表明，区分由大规模语言模型生成的文本与人类撰写的文本极具挑战性，往往与随机猜测无异。为了验证这一发现的泛化能力，我们在多种语言和领域进行了广泛的研究，以确定人类检测准确率的上限。在涵盖9种语言和9个领域的16个数据集中，19名注释者平均检测准确率为87.6%，这挑战了之前的结论。我们发现，人类与机器文本之间的主要差距在于具体性、文化细微差别和多样性。通过对提示进行明确解释，可以部分弥合超过50%的情况下的人机差距。然而，我们还发现，人类并不总是偏好人类撰写的文本，特别是在无法明确识别其来源时。', 'title_zh': '人类喜好的文本是否具有人类特点？多语言人类身份检测及对AI的偏好比较'}
{'arxiv_id': 'arXiv:2502.11603', 'title': 'DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning', 'authors': 'Hongye Qiu, Yue Xu, Meikang Qiu, Wenjie Wang', 'link': 'https://arxiv.org/abs/2502.11603', 'abstract': 'Large Language Models (LLMs) exhibit strong natural language processing capabilities but also inherit and amplify societal biases, including gender bias, raising fairness concerns. Existing debiasing methods face significant limitations: parameter tuning requires access to model weights, prompt-based approaches often degrade model utility, and optimization-based techniques lack generalizability. To address these challenges, we propose this http URL (Demonstration and Reasoning for Gender-Aware Prompting), an automated and model-agnostic approach that mitigates gender bias while preserving model performance. this http URL selects bias-revealing examples and generates structured reasoning to guide models toward more impartial responses. Extensive experiments on coreference resolution and QA tasks across multiple LLMs (GPT-3.5, Llama3, and Llama2-Alpaca) demonstrate its effectiveness, generalization ability, and robustness. this http URL can generalize to vision-language models (VLMs), achieving significant bias reduction.', 'abstract_zh': '大型语言模型（LLMs）展示了强大的自然语言处理能力，但也继承和放大了社会偏见，包括性别偏见，引发公平性担忧。现有的去偏见方法面临显著的局限性：参数调优需要访问模型权重，基于提示的方法往往降低模型实用性，基于优化的技术缺乏普适性。为了解决这些挑战，我们提出了Demonstration and Reasoning for Gender-Aware Prompting（性别意识提示的演示与推理），这是一种自动化且模型无关的方法，能够在减轻性别偏见的同时保持模型性能。该方法通过选择揭示偏见的示例并生成结构化的推理来引导模型产生更具中立性的回复。在多个LLM（GPT-3.5、Llama3和Llama2-Alpaca）的核心参照解析和问答任务上的广泛实验展示了其有效性、普适能力和鲁棒性。该方法还可以推广到视觉语言模型（VLMs），实现显著的偏见减少。', 'title_zh': 'DR.GAP：基于性别意识提示与示范推理的大规模语言模型偏见缓解方法'}
{'arxiv_id': 'arXiv:2502.11596', 'title': 'LLM Embeddings for Deep Learning on Tabular Data', 'authors': 'Boshko Koloski, Andrei Margeloiu, Xiangjian Jiang, Blaž Škrlj, Nikola Simidjievski, Mateja Jamnik', 'link': 'https://arxiv.org/abs/2502.11596', 'abstract': 'Tabular deep-learning methods require embedding numerical and categorical input features into high-dimensional spaces before processing them. Existing methods deal with this heterogeneous nature of tabular data by employing separate type-specific encoding approaches. This limits the cross-table transfer potential and the exploitation of pre-trained knowledge. We propose a novel approach that first transforms tabular data into text, and then leverages pre-trained representations from LLMs to encode this data, resulting in a plug-and-play solution to improv ing deep-learning tabular methods. We demonstrate that our approach improves accuracy over competitive models, such as MLP, ResNet and FT-Transformer, by validating on seven classification datasets.', 'abstract_zh': '表格深度学习方法需要将数值和类别输入特征嵌入高维空间中再进行处理。现有方法通过采用类型特定的编码方法来应对表格数据的异构性质，这限制了跨表传输潜力及先验知识的利用。我们提出了一种新方法，首先将表格数据转换为文本，然后利用预训练的语言模型表示来编码这些数据，从而获得一个即插即用的解决方案，用以改进深度学习表格方法。我们通过在七个分类数据集上的验证显示，我们的方法在准确性上优于MLP、ResNet和FT-Transformer等竞争模型。', 'title_zh': '大规模语言模型嵌入在表格数据深度学习中的应用'}
{'arxiv_id': 'arXiv:2502.11578', 'title': 'Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance', 'authors': 'Birger Moell, Johan Boye', 'link': 'https://arxiv.org/abs/2502.11578', 'abstract': "Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets.", 'abstract_zh': '大型语言模型（LLMs）在自然语言生成方面取得了显著进展，但在需要精确计算和结构分析的任务中常面临挑战。本文通过计算LIX可读性度量和平均依存距离（ADD），探讨了最新大型语言模型在语言复杂度测量任务中的表现。使用瑞典高中和大学水平的作文，评估模型计算LIX分数和进行依存句法分析的能力，并将其结果与既定的事实标准进行比较。研究发现，虽然所有模型在这些任务上都表现出了一定的能力，但ChatGPT-o1-mini表现最为一致，在LIX计算和依存句法分析中的准确性最高。此外，我们在计算LIX的准确性与大规模多任务语言理解（MMLU）基准上的整体表现之间观察到了显著的相关性（-0.875，p<0.026，N=6）。这些结果表明，语言复杂度测量能力可以作为评估LLMs通用能力的嘈杂零样本代理，提供了一种无需大量基准数据集即可进行模型评估的实际方法。', 'title_zh': '语言复杂度测量作为评估大型语言模型性能的噪声零样本代理'}
{'arxiv_id': 'arXiv:2502.11573', 'title': 'InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning', 'authors': 'Congkai Xie, Shuo Cai, Wenjun Wang, Pengxiang Li, Zhijie Sang, Kejing Yang, Yiming Zhang, Zhen Li, Guanghao Zhu, Zeyu Liu, Yang Yu, Yuhang Liu, Su Lu, Baoyi He, Qi Zhou, Xiaotian Han, Jianbo Yuan, Shengyu Zhang, Fei Wu, Hongxia Yang', 'link': 'https://arxiv.org/abs/2502.11573', 'abstract': 'Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have made significant advancements in reasoning capabilities. However, they still face challenges such as high computational demands and privacy concerns. This paper focuses on developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that retain competitive reasoning abilities. We introduce a novel training pipeline that enhances reasoning capabilities and facilitates deployment on edge devices, achieving state-of-the-art performance while minimizing development costs. \\InfR~ aims to advance AI systems by improving reasoning, reducing adoption barriers, and addressing privacy concerns through smaller model sizes. Resources are available at https://github. com/Reallm-Labs/InfiR.', 'abstract_zh': '大型语言模型（LLMs）和多模态大型语言模型（MLLMs）在推理能力方面取得了显著进展，但仍面临计算需求高和隐私问题等挑战。本文旨在开发高效的小型语言模型（SLMs）和多模态小型语言模型（MSLMs），同时保留竞争力的推理能力。我们提出了一种新颖的训练管道，以增强推理能力并方便在边缘设备上部署，同时实现了最先进性能并最小化开发成本。InfR~致力于通过改进推理、降低采用壁垒和通过缩小模型规模解决隐私问题来推动AI系统的进步。更多资源请访问<https://github.com/Reallm-Labs/InfiR>。', 'title_zh': 'InfiR：打造有效的小型语言模型和推理中的多模态小型语言模型'}
{'arxiv_id': 'arXiv:2502.11569', 'title': 'Towards Reasoning Ability of Small Language Models', 'authors': 'Gaurav Srivastava, Shuxiang Cao, Xuan Wang', 'link': 'https://arxiv.org/abs/2502.11569', 'abstract': 'Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale ($\\sim$100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation. This raises a critical question: Can SLMs achieve reasoning abilities comparable to LLMs? In this work, we systematically survey, benchmark, and analyze 72 SLMs from six model families across 14 reasoning benchmarks. For reliable evaluation, we examine four evaluation methods and compare four LLM judges against human evaluations on 800 data points. We repeat all experiments three times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models. Beyond accuracy, we also evaluate model robustness under adversarial conditions and intermediate reasoning steps. Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning-intensive tasks.', 'abstract_zh': '小语言模型的推理能力：系统调研与分析', 'title_zh': '面向小型语言模型的推理能力研究'}
{'arxiv_id': 'arXiv:2502.11559', 'title': 'Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models', 'authors': 'Yue Xu, Chengyan Fu, Li Xiong, Sibei Yang, Wenjie Wang', 'link': 'https://arxiv.org/abs/2502.11559', 'abstract': 'Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias, they are resource-intensive, unsuitable for closed-source models, and lack adaptability to evolving societal norms. Instruction-based approaches offer flexibility but often compromise task performance. To address these limitations, we propose $\\textit{FaIRMaker}$, an automated and model-independent framework that employs an $\\textbf{auto-search and refinement}$ paradigm to adaptively generate Fairwords, which act as instructions integrated into input queries to reduce gender bias and enhance response quality. Extensive experiments demonstrate that $\\textit{FaIRMaker}$ automatically searches for and dynamically refines Fairwords, effectively mitigating gender bias while preserving task integrity and ensuring compatibility with both API-based and open-source LLMs.', 'abstract_zh': '预训练大规模语言模型（LLMs）在 vast 文本语料库上增强自然语言处理能力但会风险编码社会偏见，尤其是性别偏见。虽然参数调整方法如微调可以缓解偏见，但这些方法资源密集、不适合闭源模型且缺乏对不断演变的社会规范的适应性。基于指令的方法提供灵活性但往往会牺牲任务性能。为解决这些限制，我们提出了一种名为 $\\textit{FaIRMaker}$ 的自动化且模型独立框架，该框架采用 $\\textbf{自动搜索和优化}$ 哲学自适应生成公平词，这些公平词作为指令集成到输入查询中以减少性别偏见并提高响应质量。广泛实验表明，$\\textit{FaIRMaker}$ 自动搜索并动态优化公平词，有效缓解性别偏见同时保持任务完整性，并确保与基于 API 和开源的大规模语言模型兼容。', 'title_zh': '自动搜索与精炼：大规模语言模型中性别偏见缓解的自动化框架'}
{'arxiv_id': 'arXiv:2502.11541', 'title': 'MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training', 'authors': 'Hui Huang, Jiaheng Liu, Yancheng He, Shilong Li, Bing Xu, Conghui Zhu, Muyun Yang, Tiejun Zhao', 'link': 'https://arxiv.org/abs/2502.11541', 'abstract': 'Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.', 'abstract_zh': '复杂指令跟随需要精细约束，这对于大型语言模型（LLMs）至关重要。现有方法虽已构建复杂指令对齐的数据，但均依赖更为先进的模型，尤其是GPT-4，限制了其应用。本文提出了一种多粒度自我对比训练（MuSC）框架，以在不依赖更强模型的情况下改善复杂指令对齐。我们的方法在粗粒度和细粒度上均进行了处理。在粗粒度上，我们基于指令分解和重组构建了约束感知偏好数据。在细粒度上，我们进行了Token感知偏好优化，并采用了动态token级别监督。我们的方法在开源模型上进行了评估，实验结果表明，该方法在复杂和通用指令跟随基准上均取得了显著改进，超越了之前的自我对齐方法。', 'title_zh': 'MuSC：多粒度自对比训练改进复杂指令跟随'}
{'arxiv_id': 'arXiv:2502.11521', 'title': 'DeFiScope: Detecting Various DeFi Price Manipulations with LLM Reasoning', 'authors': 'Juantao Zhong, Daoyuan Wu, Ye Liu, Maoyi Xie, Yang Liu, Yi Li, Ning Liu', 'link': 'https://arxiv.org/abs/2502.11521', 'abstract': "DeFi (Decentralized Finance) is one of the most important applications of today's cryptocurrencies and smart contracts. It manages hundreds of billions in Total Value Locked (TVL) on-chain, yet it remains susceptible to common DeFi price manipulation attacks. Despite state-of-the-art (SOTA) systems like DeFiRanger and DeFort, we found that they are less effective to non-standard price models in custom DeFi protocols, which account for 44.2% of the 95 DeFi price manipulation attacks reported over the past three years.\nIn this paper, we introduce the first LLM-based approach, DeFiScope, for detecting DeFi price manipulation attacks in both standard and custom price models. Our insight is that large language models (LLMs) have certain intelligence to abstract price calculation from code and infer the trend of token price changes based on the extracted price models. To further strengthen LLMs in this aspect, we leverage Foundry to synthesize on-chain data and use it to fine-tune a DeFi price-specific LLM. Together with the high-level DeFi operations recovered from low-level transaction data, DeFiScope detects various DeFi price manipulations according to systematically mined patterns. Experimental results show that DeFiScope achieves a high precision of 96% and a recall rate of 80%, significantly outperforming SOTA approaches. Moreover, we evaluate DeFiScope's cost-effectiveness and demonstrate its practicality by helping our industry partner confirm 147 real-world price manipulation attacks, including discovering 81 previously unknown historical incidents.", 'abstract_zh': '去中心化金融（DeFi）智能合约的应用及其价格操纵检测：基于大语言模型的方法', 'title_zh': 'DeFiScope: 使用大型语言模型推理检测各类DeFi价格操控'}
{'arxiv_id': 'arXiv:2502.11513', 'title': 'MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of Large Language Models', 'authors': 'Zhen Zhang, Yifan Yang, Kai Zhen, Nathan Susanj, Athanasios Mouchtaris, Siegfried Kunzmann, Zheng Zhang', 'link': 'https://arxiv.org/abs/2502.11513', 'abstract': 'Large language models have demonstrated exceptional capabilities across diverse tasks, but their fine-tuning demands significant memory, posing challenges for resource-constrained environments. Zeroth-order (ZO) optimization provides a memory-efficient alternative by eliminating the need for backpropagation. However, ZO optimization suffers from high gradient variance, and prior research has largely focused on single-task learning, leaving its application to multi-task learning unexplored. Multi-task learning is crucial for leveraging shared knowledge across tasks to improve generalization, yet it introduces unique challenges under ZO settings, such as amplified gradient variance and collinearity. In this paper, we present MaZO, the first framework specifically designed for multi-task LLM fine-tuning under ZO optimization. MaZO tackles these challenges at the parameter level through two key innovations: a weight importance metric to identify critical parameters and a multi-task weight update mask to selectively update these parameters, reducing the dimensionality of the parameter space and mitigating task conflicts. Experiments demonstrate that MaZO achieves state-of-the-art performance, surpassing even multi-task learning methods designed for first-order optimization.', 'abstract_zh': '基于零阶优化的多任务大型语言模型微调框架MaZO', 'title_zh': 'MaZO: 遮掩零阶优化在大型语言模型多任务微调中的应用'}
{'arxiv_id': 'arXiv:2502.11508', 'title': 'Chinese Spelling Correction: A Comprehensive Survey of Progress, Challenges, and Opportunities', 'authors': 'Changchun Liu, Kai Zhang, Junzhe Jiang, Zixiao Kong, Qi Liu, Enhong Chen', 'link': 'https://arxiv.org/abs/2502.11508', 'abstract': 'Chinese Spelling Correction (CSC) is a critical task in natural language processing, aimed at detecting and correcting spelling errors in Chinese text. This survey provides a comprehensive overview of CSC, tracing its evolution from pre-trained language models to large language models, and critically analyzing their respective strengths and weaknesses in this domain. Moreover, we further present a detailed examination of existing benchmark datasets, highlighting their inherent challenges and limitations. Finally, we propose promising future research directions, particularly focusing on leveraging the potential of LLMs and their reasoning capabilities for improved CSC performance. To the best of our knowledge, this is the first comprehensive survey dedicated to the field of CSC. We believe this work will serve as a valuable resource for researchers, fostering a deeper understanding of the field and inspiring future advancements.', 'abstract_zh': '中文拼写修正（CSC）是自然语言处理中的一个关键任务，旨在检测和修正中文文本中的拼写错误。本文综述了CSC的发展历程，从预训练语言模型到大型语言模型，并对其在该领域的各自优势和不足进行了批判性分析。此外，我们还详细分析了现有的基准数据集，突显了其固有的挑战和局限性。最后，我们提出了有前景的未来研究方向，特别强调了利用大型语言模型及其推理能力以改进CSC性能的潜力。据我们所知，这是首次专门致力于CSC领域的全面综述。我们相信，这项工作将成为研究人员的重要资源，促进对该领域的深入理解，并激发未来的进步。', 'title_zh': '中文拼写纠错：进展、挑战与机遇综述'}
{'arxiv_id': 'arXiv:2502.11491', 'title': 'Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering', 'authors': 'Runxuan Liu, Bei Luo, Jiaqi Li, Baoxin Wang, Ming Liu, Dayong Wu, Shijin Wang, Bing Qin', 'link': 'https://arxiv.org/abs/2502.11491', 'abstract': 'Large language models (LLMs) have shown remarkable capabilities in natural language processing. However, in knowledge graph question answering tasks (KGQA), there remains the issue of answering questions that require multi-hop reasoning. Existing methods rely on entity vector matching, but the purpose of the question is abstract and difficult to match with specific entities. As a result, it is difficult to establish reasoning paths to the purpose, which leads to information loss and redundancy. To address this issue, inspired by human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a novel framework that constructs reasoning paths from purposes back to conditions. ORT operates in three key phases: (1) using LLM to extract purpose labels and condition labels, (2) constructing label reasoning paths based on the KG ontology, and (3) using the label reasoning paths to guide knowledge retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves state-of-the-art performance and significantly enhances the capability of LLMs for KGQA.', 'abstract_zh': '大型语言模型（LLMs）在自然语言处理任务中展现了 remarkable 的能力。然而，在知识图谱问答任务（KGQA）中，仍然存在需要多跳推理的问题。现有方法依赖于实体向量匹配，但问题的目的往往是抽象的，难以与特定实体匹配，导致难以建立推理路径，从而引发信息丢失和冗余。为解决这一问题，借鉴人类逆向思维，我们提出了基于本体的逆向思考（ORT）框架，该框架从目的反向构建推理路径至条件。ORT 包含三个关键阶段：（1）使用 LLM 提取目的标签和条件标签，（2）基于知识图谱本体构建标签推理路径，并（3）利用标签推理路径指导知识检索。实验结果表明，ORT 在 WebQSP 和 CWQ 数据集上达到了最先进的性能，并显著增强了 LLM 在 KGQA 中的能力。', 'title_zh': '基于本体引导的逆向思维使大型语言模型在知识图谱问答中更加出色'}
{'arxiv_id': 'arXiv:2502.11482', 'title': 'DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free Continual Learning', 'authors': 'Huanxuan Liao, Shizhu He, Yupu Hao, Jun Zhao, Kang Liu', 'link': 'https://arxiv.org/abs/2502.11482', 'abstract': "Continual learning (CL) is essential for Large Language Models (LLMs) to adapt to evolving real-world demands, yet they are susceptible to catastrophic forgetting (CF). While traditional CF solutions rely on expensive data rehearsal, recent rehearsal-free methods employ model-based and regularization-based strategies to address this issue. However, these approaches often neglect the model's plasticity, which is crucial to achieving optimal performance on newly learned tasks. Consequently, a key challenge in CL is striking a balance between preserving plasticity and mitigating CF. To tackle this challenge, we propose the $\\textbf{D}$ecomposed $\\textbf{A}$ttention-based $\\textbf{T}$ask $\\textbf{A}$daptation (DATA), which explicitly decouples and learns both task-specific and task-shared knowledge using high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA dynamically adjusts the weights of adapters of different ranks based on their relevance and distinction from previous tasks, allowing the model to acquire new task-specific skills while effectively retaining previously learned knowledge. Specifically, we implement a decomposed component weighting strategy comprising learnable components that collectively generate attention-based weights, allowing the model to integrate and utilize diverse knowledge from each DATA. Extensive experiments on three widely used benchmarks demonstrate that our proposed method achieves state-of-the-art performance. Notably, our approach significantly enhances model plasticity and mitigates CF by extending learnable components and employing stochastic restoration during training iterations.", 'abstract_zh': '基于分解注意力的任务适配：平衡保留塑性与缓解灾难性遗忘', 'title_zh': '数据：分解注意力机制导向的任务适应在无需重温的连续学习中'}
{'arxiv_id': 'arXiv:2502.11458', 'title': 'Towards Efficient Pre-training: Exploring FP4 Precision in Large Language Models', 'authors': 'Jiecheng Zhou, Ding Tang, Rong Fu, Boni Hu, Haoran Xu, Yi Wang, Zhilin Pei, Zhongling Su, Liang Liu, Xingcheng Zhang, Weiming Zhang', 'link': 'https://arxiv.org/abs/2502.11458', 'abstract': 'The burgeoning computational demands for training large language models (LLMs) necessitate efficient methods, including quantized training, which leverages low-bit arithmetic operations to reduce costs. While FP8 precision has shown potential, leveraging FP4 remains challenging due to inherent quantization errors and limited representation capability. Based on the Transformer architecture, we present an FP4 training scheme for LLMs, overcoming these obstacles through mixed-precision quantization strategies tailed for different modules and training stages. This allows us to apply the precision level suitable to distinct components within the model, ensuring that multi-head attention and linear layers are handled appropriately. Our pretraining recipe ensures stability in backpropagation by incorporating fine-grained quantization methods with a target precision training schedule. Experimental results demonstrate that our FP4 training scheme achieves accuracy comparable to BF16 and FP8, with smaller theoretical computational cost. With the advent of next-generation hardware supporting FP4, our method sets the foundation for efficient ultra-low precision training.', 'abstract_zh': '大规模语言模型（LLMs）训练日益增长的计算需求 necessitates 有效的方法，包括量化训练，这利用低位宽算术运算以降低计算成本。虽然FP8精度已显示出潜力，但利用FP4仍面临挑战，由于固有的量化误差和有限的表示能力。基于Transformer架构，我们提出了针对LLMs的FP4训练方案，通过针对不同模块和训练阶段设计的混合精度量化策略克服了这些障碍。这种方法允许我们为模型中的不同组件选择合适的精度级别，确保多头注意力机制和线性层得到了适当处理。我们的预训练方案通过结合精细量化方法和目标精度训练计划，确保了反向传播的稳定性。实验结果表明，我们的FP4训练方案在计算成本理论更低的情况下，能达到与BF16和FP8相近的准确性。随着支持FP4的下一代硬件的到来，我们的方法为高效的超低精度训练奠定了基础。', 'title_zh': '向量高效预训练：探索大语言模型中的FP4精度'}
{'arxiv_id': 'arXiv:2502.11457', 'title': "Aligning Sentence Simplification with ESL Learner's Proficiency for Language Acquisition", 'authors': 'Guanlin Li, Yuki Arase, Noel Crespi', 'link': 'https://arxiv.org/abs/2502.11457', 'abstract': "Text simplification is crucial for improving accessibility and comprehension for English as a Second Language (ESL) learners. This study goes a step further and aims to facilitate ESL learners' language acquisition by simplification. Specifically, we propose simplifying complex sentences to appropriate levels for learners while also increasing vocabulary coverage of the target level in the simplifications. We achieve this without a parallel corpus by conducting reinforcement learning on a large language model. Our method employs token-level and sentence-level rewards, and iteratively trains the model on its self-generated outputs to guide the model to search for simplification hypotheses that satisfy the target attributes. Experiment results on CEFR-SP and TurkCorpus datasets show that the proposed method can effectively increase the frequency and diversity of vocabulary of the target level by more than $20\\%$ compared to baseline models, while maintaining high simplification quality.", 'abstract_zh': '英语作为第二语言（ESL）学习者的文本简化对于提高可访问性和理解度至关重要。本研究旨在通过简化进一步促进ESL学习者的语言习得。具体而言，我们提议将复杂句子简化到适合学习者的适当水平，并在简化过程中增加目标水平的词汇覆盖范围。我们通过在大规模语言模型上进行强化学习，而无需平行语料库来实现这一点。该方法使用 token 级和句子级的奖励，并通过迭代训练模型以引导模型搜索满足目标属性的简化假设。在 CEFR-SP 和 TurkCorpus 数据集上的实验结果表明，所提出的方法与基线模型相比，能够有效提高目标水平词汇的频率和多样性超过20%，同时保持高简化质量。', 'title_zh': '根据 ESL 学习者 proficiency 调整句子简化以促进语言习得'}
{'arxiv_id': 'arXiv:2502.11453', 'title': 'Connector-S: A Survey of Connectors in Multi-modal Large Language Models', 'authors': 'Xun Zhu, Zheng Zhang, Xi Chen, Yiming Shi, Miao Li, Ji Wu', 'link': 'https://arxiv.org/abs/2502.11453', 'abstract': 'With the rapid advancements in multi-modal large language models (MLLMs), connectors play a pivotal role in bridging diverse modalities and enhancing model performance. However, the design and evolution of connectors have not been comprehensively analyzed, leaving gaps in understanding how these components function and hindering the development of more powerful connectors. In this survey, we systematically review the current progress of connectors in MLLMs and present a structured taxonomy that categorizes connectors into atomic operations (mapping, compression, mixture of experts) and holistic designs (multi-layer, multi-encoder, multi-modal scenarios), highlighting their technical contributions and advancements. Furthermore, we discuss several promising research frontiers and challenges, including high-resolution input, dynamic compression, guide information selection, combination strategy, and interpretability. This survey is intended to serve as a foundational reference and a clear roadmap for researchers, providing valuable insights into the design and optimization of next-generation connectors to enhance the performance and adaptability of MLLMs.', 'abstract_zh': '随着多模态大型语言模型（MLLMs）的迅速发展，连接器在桥梁不同模态和提升模型性能方面发挥着关键作用。然而，连接器的设计和演变尚未进行全面分析，这在理解这些组件的功能方面留下了空白，并阻碍了更强大连接器的发展。在本文综述中，我们系统地回顾了MLLMs中连接器的当前进展，并提出了一种结构化的分类体系，将连接器分为原子操作（映射、压缩、专家混合）和整体设计（多层、多编码器、多模态场景），强调了它们的技术贡献和进展。此外，我们讨论了几个有前景的研究前沿和挑战，包括高分辨率输入、动态压缩、引导信息选择、组合策略和可解释性。本文综述旨在作为研究人员的基础参考和清晰的路线图，提供有关设计和优化下一代连接器以增强MLLMs性能和适应性的宝贵见解。', 'title_zh': 'Connector-S：多模态大型语言模型中连接器的综述'}
{'arxiv_id': 'arXiv:2502.11447', 'title': 'Does Editing Provide Evidence for Localization?', 'authors': 'Zihao Wang, Victor Veitch', 'link': 'https://arxiv.org/abs/2502.11447', 'abstract': 'A basic aspiration for interpretability research in large language models is to "localize" semantically meaningful behaviors to particular components within the LLM. There are various heuristics for finding candidate locations within the LLM. Once a candidate localization is found, it can be assessed by editing the internal representations at the corresponding localization and checking whether this induces model behavior that is consistent with the semantic interpretation of the localization. The question we address here is: how strong is the evidence provided by such edits? To assess localization, we want to assess the effect of the optimal intervention at a particular location. The key new technical tool is a way of adapting LLM alignment techniques to find such optimal localized edits. With this tool in hand, we give an example where the edit-based evidence for localization appears strong, but where localization clearly fails. Indeed, we find that optimal edits at random localizations can be as effective as aligning the full model. In aggregate, our results suggest that merely observing that localized edits induce targeted changes in behavior provides little to no evidence that these locations actually encode the target behavior.', 'abstract_zh': '大型语言模型可解释性研究的基本追求是将语义上有意义的行为“本地化”到模型的特定组件中。在大型语言模型中寻找候选位置的各种启发式方法有很多种。一旦找到候选位置，可以通过编辑对应位置的内部表示，并检查这是否会导致与该位置语义解释一致的模型行为来评估这种本地化。我们在这里要回答的问题是：这种编辑提供的证据有多强？评估本地化时，我们希望评估特定位置的最佳干预措施的效果。关键的新技术工具是将大型语言模型对齐技术适应为找到这种最佳本地化编辑的方法。有了这个工具，我们给出一个例子，其中基于编辑的本地化证据似乎很强烈，但本地化显然失败了。事实上，我们发现，随机位置的最佳编辑与对整个模型进行对齐的效果相当。综上所述，我们的结果表明，仅仅观察本地化编辑引发了目标行为的变化，几乎没有证据表明这些位置实际上编码了目标行为。', 'title_zh': '编辑提供 Localization 证据吗？'}
{'arxiv_id': 'arXiv:2502.11439', 'title': 'An Efficient Row-Based Sparse Fine-Tuning', 'authors': 'Cen-Jhih Li, Aditya Bhaskara', 'link': 'https://arxiv.org/abs/2502.11439', 'abstract': 'Fine-tuning is an important step in adapting foundation models such as large language models to downstream tasks. To make this step more accessible to users with limited computational budgets, it is crucial to develop fine-tuning methods that are memory and computationally efficient. Sparse Fine-tuning (SFT) and Low-rank adaptation (LoRA) are two frameworks that have emerged for addressing this problem and have been adopted widely in practice. In this work, we develop a new SFT framework, based on ideas from neural network pruning. At a high level, we first identify "important" neurons/nodes using feature importance metrics from network pruning (specifically, we use the structural pruning method), and then perform fine-tuning by restricting to weights involving these neurons. Using experiments on common language tasks, we demonstrate that our method significantly improves the memory efficiency of SFT without increasing training time complexity and implementation complexity, while achieving accuracy comparable to state-of-the-art methods such as LoRA and its variants.', 'abstract_zh': '细粒度剪枝驱动的稀疏微调新框架及其应用', 'title_zh': '基于行的高效稀疏微调'}
{'arxiv_id': 'arXiv:2502.11425', 'title': 'Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models', 'authors': 'Jongho Kim, Seung-won Hwang', 'link': 'https://arxiv.org/abs/2502.11425', 'abstract': "Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding events. For example, models often confuse mutually exclusive temporal relations like ``before'' and ``after'' between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model's consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding by effectively addressing temporal inconsistencies.", 'abstract_zh': '尽管大型语言模型（LLMs）具备先进的能力，但其时间推理能力仍不够发达。以往研究强调了这一局限性，特别是在理解事件时保持时间一致性方面。例如，模型常常混淆互斥的时间关系，如“之前”和“之后”，并作出不一致的预测。在本工作中，我们通过提出一种新颖的反事实提示方法来应对LLMs的时间一致性问题。该方法生成反事实问题并施加集体约束，从而增强模型的一致性。我们在多个数据集上评估了该方法，展示了在事件排序和时间常识理解方面通过有效解决时间不一致性所取得的显著改进。', 'title_zh': '相对时间理解中的事实一致性提示方法'}
{'arxiv_id': 'arXiv:2502.11379', 'title': 'CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models', 'authors': 'Guanghao Zhou, Panjia Qiu, Mingyuan Fan, Cen Chen, Mingyuan Chu, Xin Zhang, Jun Zhou', 'link': 'https://arxiv.org/abs/2502.11379', 'abstract': 'Despite explicit alignment efforts for large language models (LLMs), they can still be exploited to trigger unintended behaviors, a phenomenon known as "jailbreaking." Current jailbreak attack methods mainly focus on discrete prompt manipulations targeting closed-source LLMs, relying on manually crafted prompt templates and persuasion rules. However, as the capabilities of open-source LLMs improve, ensuring their safety becomes increasingly crucial. In such an environment, the accessibility of model parameters and gradient information by potential attackers exacerbates the severity of jailbreak threats. To address this research gap, we propose a novel \\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak \\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization problem within the embedding space of masked language models. Through combinatorial optimization, we effectively balance the jailbreak attack success rate with semantic coherence. Extensive evaluations show that our method not only maintains semantic consistency but also surpasses state-of-the-art baselines in attack effectiveness. Additionally, by integrating semantically coherent jailbreak prompts generated by our method into widely used black-box methodologies, we observe a notable enhancement in their success rates when targeting closed-source commercial LLMs. This highlights the security threat posed by open-source LLMs to commercial counterparts. We will open-source our code if the paper is accepted.', 'abstract_zh': '一种上下文一致的卦.break攻击（CCJA）', 'title_zh': 'CCJA: 具有上下文一致性攻击的对齐大型语言模型 Jailbreak 攻击'}
{'arxiv_id': 'arXiv:2502.11368', 'title': 'LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing', 'authors': 'Zhengxiang Wang, Veronika Makarova, Zhi Li, Jordan Kodner, Owen Rambow', 'link': 'https://arxiv.org/abs/2502.11368', 'abstract': 'The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus for reproducibility.', 'abstract_zh': '该论文探讨了大语言模型在多维度分析写作评估中的性能，即其根据多种评估标准提供评分和评论的能力。通过使用由二外研究生撰写并由人类专家根据9个分析标准评估的文献综述语料库，我们促使几种流行的大型语言模型在不同条件下执行相同任务。为了评估反馈评论的质量，我们应用了一种新的反馈评论质量评估框架。该框架具有可解释性、成本效益、可扩展性和可复制性，优于现有的依赖人工判断的方法。我们发现，大语言模型可以生成合理良好且一般可靠的多维度分析评估。我们发布了语料库以确保可复制性。', 'title_zh': 'LLMs可以进行多维度分析写作评估：英语二外研究生水平学术写作案例研究'}
{'arxiv_id': 'arXiv:2502.11367', 'title': 'Sparse Autoencoder Features for Classifications and Transferability', 'authors': 'Jack Gallifant, Shan Chen, Kuleen Sasse, Hugo Aerts, Thomas Hartvigsen, Danielle S. Bitterman', 'link': 'https://arxiv.org/abs/2502.11367', 'abstract': 'Sparse Autoencoders (SAEs) provide potentials for uncovering structured, human-interpretable representations in Large Language Models (LLMs), making them a crucial tool for transparent and controllable AI systems. We systematically analyze SAE for interpretable feature extraction from LLMs in safety-critical classification tasks. Our framework evaluates (1) model-layer selection and scaling properties, (2) SAE architectural configurations, including width and pooling strategies, and (3) the effect of binarizing continuous SAE activations. SAE-derived features achieve macro F1 > 0.8, outperforming hidden-state and BoW baselines while demonstrating cross-model transfer from Gemma 2 2B to 9B-IT models. These features generalize in a zero-shot manner to cross-lingual toxicity detection and visual classification tasks. Our analysis highlights the significant impact of pooling strategies and binarization thresholds, showing that binarization offers an efficient alternative to traditional feature selection while maintaining or improving performance. These findings establish new best practices for SAE-based interpretability and enable scalable, transparent deployment of LLMs in real-world applications. Full repo: this https URL.', 'abstract_zh': '稀疏自编码器（SAEs）为揭示大型语言模型（LLMs）中结构化的、可人为解释的表示提供了潜力，使它们成为透明和可控AI系统的关键工具。我们系统地分析了SAE在安全关键分类任务中从LLMs提取可解释特征的能力。我们的框架评估了（1）模型-层选择和缩放特性，（2）SAE架构配置，包括宽度和聚集策略，以及（3）连续SAE激活二值化的效果。从SAE派生的特征实现了宏F1 > 0.8，超过了隐藏状态和BoW基线，并且展示了从Gemma 2 2B到9B-IT模型的跨模型迁移性。这些特征以零样本方式泛化到跨语言毒性检测和视觉分类任务中。我们的分析突显了聚集策略和二值化阈值的显著影响，表明二值化提供了传统特征选择的有效替代方案，同时保持或提高了性能。这些发现确立了基于SAE的可解释性的新最佳实践，并使LLMs在实际应用中的可扩展和透明部署成为可能。完整的仓库：this https URL。', 'title_zh': '稀疏自编码特征用于分类和迁移性'}
{'arxiv_id': 'arXiv:2502.11356', 'title': 'SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models', 'authors': 'Zirui He, Haiyan Zhao, Yiran Qiao, Fan Yang, Ali Payani, Jing Ma, Mengnan Du', 'link': 'https://arxiv.org/abs/2502.11356', 'abstract': 'The ability of large language models (LLMs) to follow instructions is crucial for their practical applications, yet the underlying mechanisms remain poorly understood. This paper presents a novel framework that leverages sparse autoencoders (SAE) to interpret how instruction following works in these models. We demonstrate how the features we identify can effectively steer model outputs to align with given instructions. Through analysis of SAE latent activations, we identify specific latents responsible for instruction following behavior. Our findings reveal that instruction following capabilities are encoded by a distinct set of instruction-relevant SAE latents. These latents both show semantic proximity to relevant instructions and demonstrate causal effects on model behavior. Our research highlights several crucial factors for achieving effective steering performance: precise feature identification, the role of final layer, and optimal instruction positioning. Additionally, we demonstrate that our methodology scales effectively across SAEs and LLMs of varying sizes.', 'abstract_zh': '大型语言模型（LLMs）遵循指令的能力是其实际应用的关键，但其 underlying机制仍不甚明了。本文提出了一种新颖的框架，利用稀疏自编码器（SAE）来解释这些模型中指令遵循的工作机制。我们展示了我们识别的特征如何有效地引导模型输出与给定指令相一致。通过分析SAE的潜在激活，我们识别出了负责指令遵循行为的特定潜在特征。我们的研究发现，指令遵循能力由一组独特的、与相关指令相关的SAE潜在特征编码。这些潜在特征既与相关指令在语义上接近，又对模型行为表现出因果效应。我们的研究强调了实现有效引导性能的关键因素：精确特征识别、最终层的作用以及最优指令定位。此外，我们证明了我们的方法在不同大小的SAE和LLMs中都具有有效的扩展性。', 'title_zh': 'SAIF：一种用于解析和引导语言模型遵循指令的稀疏自编码器框架'}
{'arxiv_id': 'arXiv:2502.11355', 'title': '"Nuclear Deployed!": Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents', 'authors': 'Rongwu Xu, Xiaojian Li, Shuo Chen, Wei Xu', 'link': 'https://arxiv.org/abs/2502.11355', 'abstract': "Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We will release our code upon request.", 'abstract_zh': '大规模语言模型（LLMs）正在演变为自主决策者，特别是在化学、生物、放射性和核（CBRN）领域等高风险场景中，引发了关于灾难性风险的担忧。基于这种风险可能源于代理人“助益、无害、诚实”（HHH）目标之间的权衡，我们构建了一个新颖的三阶段评估框架，精心设计以有效且自然地揭示这些风险。我们在12种先进LLM上进行了14,400次代理模拟，并进行了广泛的实验和分析。结果表明，LLM代理可以自主表现出灾难性行为和欺骗行为，而无需被特意诱导。此外，更强的推理能力往往会增加而非减轻这些风险。我们还展示了这些代理可以违背指令和上级命令。总体而言，我们实证证明了自主LLM代理存在灾难性风险。如有需要，我们将提供我们的代码。', 'title_zh': '“核武部署了！”：分析自主大型语言模型代理决策中的 catastrophic 风险'}
{'arxiv_id': 'arXiv:2502.11330', 'title': 'System Message Generation for User Preferences using Open-Source Models', 'authors': 'Minbyul Jeong, Jungho Cho, Minsoo Khang, Dawoon Jung, Teakgyu Hong', 'link': 'https://arxiv.org/abs/2502.11330', 'abstract': 'System messages play a crucial role in interactions with large language models (LLMs), often serving as prompts to initiate conversations. Through system messages, users can assign specific roles, perform intended tasks, incorporate background information, specify various output formats and communication styles. Despite such versatility, publicly available data are often lack system messages and subject to strict license constraints in the industry field. Manual labeling of publicly available data with system messages that align with user instructions demands significant resources. In view of such challenges, our work introduces SysGen, a pipeline for generating system messages with better aligned assistant responses from the supervised fine-tuning dataset without system messages. Training on SysGen data has demonstrated substantial improvements in the alignment of model responses with system messages and user instructions, as demonstrated across various open-source models on the Multifacet benchmark, while maintaining minimal impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our qualitative analysis highlights the importance of diverse system messages to ensure better adaptability across different contexts.', 'abstract_zh': '系统消息在与大型语言模型的交互中扮演着至关重要的角色，常作为启动对话的提示。通过系统消息，用户可以指定特定角色、执行预定任务、融入背景信息、指定各种输出格式和交流风格。尽管具备这种灵活性，公开数据往往缺乏系统消息，且在各个行业领域受到严格许可限制。为系统消息与用户指令相符的手动标注公开数据需要大量资源。鉴于此挑战，我们工作引入了SysGen，这是一种生成具有良好对齐的助手响应的系统消息的流水线，无需使用监督微调数据集中的系统消息。在SysGen数据上的训练展示了模型响应与系统消息和用户指令对齐程度的显著改善，这一效果在多方面开源模型的Multifacet基准测试中得到验证，同时对外部未知基准如Open LLM Leaderboard 2的最小影响保持了最小限度。我们的定性分析强调了多样化系统消息的重要性，以确保模型在不同情境下的更好适应性。', 'title_zh': '基于开源模型的用户偏好系统消息生成'}
{'arxiv_id': 'arXiv:2502.11308', 'title': 'ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment and Generation', 'authors': 'Yiyi Chen, Qiongkai Xu, Johannes Bjerva', 'link': 'https://arxiv.org/abs/2502.11308', 'abstract': 'With the growing popularity of Large Language Models (LLMs) and vector databases, private textual data is increasingly processed and stored as numerical embeddings. However, recent studies have proven that such embeddings are vulnerable to inversion attacks, where original text is reconstructed to reveal sensitive information. Previous research has largely assumed access to millions of sentences to train attack models, e.g., through data leakage or nearly unrestricted API access. With our method, a single data point is sufficient for a partially successful inversion attack. With as little as 1k data samples, performance reaches an optimum across a range of black-box encoders, without training on leaked data. We present a Few-shot Textual Embedding Inversion Attack using ALignment and GENeration (ALGEN), by aligning victim embeddings to the attack space and using a generative model to reconstruct text. We find that ALGEN attacks can be effectively transferred across domains and languages, revealing key information. We further examine a variety of defense mechanisms against ALGEN, and find that none are effective, highlighting the vulnerabilities posed by inversion attacks. By significantly lowering the cost of inversion and proving that embedding spaces can be aligned through one-step optimization, we establish a new textual embedding inversion paradigm with broader applications for embedding alignment in NLP.', 'abstract_zh': '基于对齐与生成的少量样本文本嵌入反转攻击（ALGEN）及其防御探讨', 'title_zh': 'ALGEN: 面向文本嵌入的少量样本反转攻击方法及其生成与对齐技术'}
{'arxiv_id': 'arXiv:2502.11300', 'title': 'CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?', 'authors': 'Aashish Anantha Ramakrishnan, Aadarsh Anantha Ramakrishnan, Dongwon Lee', 'link': 'https://arxiv.org/abs/2502.11300', 'abstract': "Multimodal Large Language Models (MLLMs) are renowned for their superior instruction-following and reasoning capabilities across diverse problem domains. However, existing benchmarks primarily focus on assessing factual and logical correctness in downstream tasks, with limited emphasis on evaluating MLLMs' ability to interpret pragmatic cues and intermodal relationships. To address this gap, we assess the competency of MLLMs in performing Multimodal Discourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL, encompasses a broad spectrum of Coherence Relations across 3 different discourse domains at varying levels of granularity. Through our experiments on 10+ MLLMs employing different prompting strategies, we show that even top models like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple classifier-based baselines. This study emphasizes the need to move beyond similarity-based metrics and adopt a discourse-driven framework for evaluating MLLMs, providing a more nuanced assessment of their capabilities. The benchmark and code are available at: this https URL.", 'abstract_zh': '多模态大规模语言模型（MLLMs）在各种问题域中以其卓越的指令跟随能力和推理能力而闻名。然而，现有的基准主要关注下游任务中的事实性和逻辑正确性评估，对评估MLLMs解析普适性线索和跨模态关系的能力关注不足。为弥补这一缺陷，我们使用连贯关系评估MLLMs在多模态话语分析（MDA）方面的能力。我们的基准Cordial涵盖三个不同话语领域的广泛连贯关系谱系，从粗粒度到细粒度不等。通过在10多个采用不同提示策略的MLLMs上进行的实验，我们显示，即使是如Gemini 1.5 Pro和GPT-4o这样的顶级模型，也无法达到基于简单分类器基线的性能。本研究强调了应超越基于相似性的指标，采用以话语为导向的框架评估MLLMs的必要性，从而提供对其能力的更细致入微的评估。基准和代码可在以下链接获取：this https URL。', 'title_zh': 'CORDIAL: 多模态大语言模型能否有效理解连贯关系？'}
{'arxiv_id': 'arXiv:2502.11267', 'title': 'Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent', 'authors': "Zeyu He, Saniya Naphade, Ting-Hao 'Kenneth' Huang", 'link': 'https://arxiv.org/abs/2502.11267', 'abstract': 'Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering? Do users actually get closer to their desired outcome over multiple iterations of their prompts? These questions are crucial when no gold-standard labels are available to measure progress. This paper investigates a scenario in LLM-powered data labeling, "prompting in the dark," where users iteratively prompt LLMs to label data without using manually-labeled benchmarks. We developed PromptingSheet, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets. Through a study with 20 participants, we found that prompting in the dark was highly unreliable-only 9 participants improved labeling accuracy after four or more iterations. Automated prompt optimization tools like DSPy also struggled when few gold labels were available. Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design.', 'abstract_zh': '大规模语言模型数据标注中的“盲Prompt工程：用户迭代Prompt的效果及挑战”', 'title_zh': '在黑暗中推动：在缺乏黄金标准标签时评估提示工程在数据标注中的人类性能'}
{'arxiv_id': 'arXiv:2502.11244', 'title': 'Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment', 'authors': 'Somnath Banerjee, Sayan Layek, Pratyush Chatterjee, Animesh Mukherjee, Rima Hazra', 'link': 'https://arxiv.org/abs/2502.11244', 'abstract': 'Ensuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce Soteria, a lightweight yet powerful strategy that locates and minimally adjusts the "functional heads" most responsible for harmful content generation in each language. By altering only a fraction of parameters, Soteria drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that Soteria consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide.', 'abstract_zh': '确保多种语言中的一致安全性仍然是大型语言模型（LLMs）面临的重大挑战。我们提出Soteria，这是一种轻量级但强大的策略，用于定位并在每种语言中最大程度减少最负责有害内容生成的“功能头部”。通过仅调整少量参数，Soteria大大减少了政策违规现象，同时在低资源环境中也不牺牲整体模型性能。为了严格评估我们的方法，我们还提出了XThreatBench，这是一种专门的多语言数据集，捕捉来自实际政策指南的细微有害行为。使用领先开源LLM（例如Llama、Qwen、Mistral）的实验表明，Soteria在高、中、低资源语言中一致地提高了安全性指标。这些发现为全球范围内可扩展、语言适应性强且伦理对齐的LLM指明了一条有前景的道路。', 'title_zh': 'Soteria: 语言特定的功能参数调节以实现多语言安全对齐'}
{'arxiv_id': 'arXiv:2502.11228', 'title': 'Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs', 'authors': 'Mohammad Reza Rezaei, Adji Bousso Dieng', 'link': 'https://arxiv.org/abs/2502.11228', 'abstract': "Retrieval-augmented generation (RAG) enhances large language models (LLMs) for domain-specific question-answering (QA) tasks by leveraging external knowledge sources. However, traditional RAG systems primarily focus on relevance-based retrieval and often struggle with redundancy, especially when reasoning requires connecting information from multiple sources. This paper introduces Vendi-RAG, a framework based on an iterative process that jointly optimizes retrieval diversity and answer quality. This joint optimization leads to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to promote semantic diversity in document retrieval. It then uses an LLM judge that evaluates candidate answers, generated after a reasoning step, and outputs a score that the retriever uses to balance relevance and diversity among the retrieved documents during each iteration. Experiments on three challenging datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's effectiveness in multi-hop reasoning tasks. The framework achieves significant accuracy improvements over traditional single-step and multi-step RAG approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current best baseline. The benefits of Vendi-RAG are even more pronounced as the number of retrieved documents increases. Finally, we evaluated Vendi-RAG across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and observed consistent improvements, demonstrating that the framework's advantages are model-agnostic.", 'abstract_zh': '基于迭代过程的检索增强生成（Vendi-RAG）：联合优化检索多样性和答案质量以提高多跳问答任务的准确性', 'title_zh': 'Vendi-RAG：适配性地在多样性和质量之间权衡显著改善基于LLM的检索增强生成'}
{'arxiv_id': 'arXiv:2502.11211', 'title': 'A Survey of LLM-based Agents in Medicine: How far are we from Baymax?', 'authors': 'Wenxuan Wang, Zizhan Ma, Zheng Wang, Chenghan Wu, Wenting Chen, Xiang Li, Yixuan Yuan', 'link': 'https://arxiv.org/abs/2502.11211', 'abstract': "Large Language Models (LLMs) are transforming healthcare through the development of LLM-based agents that can understand, reason about, and assist with medical tasks. This survey provides a comprehensive review of LLM-based agents in medicine, examining their architectures, applications, and challenges. We analyze the key components of medical agent systems, including system profiles, clinical planning mechanisms, medical reasoning frameworks, and external capacity enhancement. The survey covers major application scenarios such as clinical decision support, medical documentation, training simulations, and healthcare service optimization. We discuss evaluation frameworks and metrics used to assess these agents' performance in healthcare settings. While LLM-based agents show promise in enhancing healthcare delivery, several challenges remain, including hallucination management, multimodal integration, implementation barriers, and ethical considerations. The survey concludes by highlighting future research directions, including advances in medical reasoning inspired by recent developments in LLM architectures, integration with physical systems, and improvements in training simulations. This work provides researchers and practitioners with a structured overview of the current state and future prospects of LLM-based agents in medicine.", 'abstract_zh': '大型语言模型（LLMs）通过基于LLM的代理来理解、推理和协助医疗任务，正在重塑医疗保健领域。本文综述了基于LLM的医疗代理，对其架构、应用和挑战进行了全面审查。我们分析了医疗代理系统的关键组件，包括系统特性、临床规划机制、医学推理框架和外部能力增强。综述涵盖了临床决策支持、医疗文档、培训模拟和医疗服务优化等主要应用场景。我们讨论了用于评估这些代理在医疗保健环境中的性能的评估框架和指标。虽然基于LLM的代理在提高医疗服务方面表现出潜力，但仍存在若干挑战，包括幻觉管理、多模态集成、实施障碍和伦理考量。本文总结了未来研究的方向，包括受到LLM架构最新发展启发的医学推理进展、与物理系统集成以及培训模拟的改进。本文为研究人员和从业者提供了基于LLM的代理在医疗领域的当前状态和未来前景的结构化概述。', 'title_zh': '基于LLM的医疗代理综述：我们距Baymax还有多远？'}
{'arxiv_id': 'arXiv:2502.11201', 'title': 'Bridging the Gap: Enabling Natural Language Queries for NoSQL Databases through Text-to-NoSQL Translation', 'authors': 'Jinwei Lu, Yuanfeng Song, Zhiqian Qin, Haodi Zhang, Chen Zhang, Raymond Chi-Wing Wong', 'link': 'https://arxiv.org/abs/2502.11201', 'abstract': "NoSQL databases have become increasingly popular due to their outstanding performance in handling large-scale, unstructured, and semi-structured data, highlighting the need for user-friendly interfaces to bridge the gap between non-technical users and complex database queries. In this paper, we introduce the Text-to-NoSQL task, which aims to convert natural language queries into NoSQL queries, thereby lowering the technical barrier for non-expert users. To promote research in this area, we developed a novel automated dataset construction process and released a large-scale and open-source dataset for this task, named TEND (short for Text-to-NoSQL Dataset). Additionally, we designed a SLM (Small Language Model)-assisted and RAG (Retrieval-augmented Generation)-assisted multi-step framework called SMART, which is specifically designed for Text-to-NoSQL conversion. To ensure comprehensive evaluation of the models, we also introduced a detailed set of metrics that assess the model's performance from both the query itself and its execution results. Our experimental results demonstrate the effectiveness of our approach and establish a benchmark for future research in this emerging field. We believe that our contributions will pave the way for more accessible and intuitive interactions with NoSQL databases.", 'abstract_zh': 'NoSQL数据库由于在处理大规模、非结构化和半结构化数据方面的出色性能而日益流行，这突显了为非技术用户提供友好接口以弥合非技术用户与复杂数据库查询之间的差距的必要性。本文介绍了一项Text-to-NoSQL任务，旨在将自然语言查询转换为NoSQL查询，从而降低非专家用户的技术门槛。为促进该领域的研究，我们开发了一种新的自动化数据集构建过程，并发布了该任务的大规模开源数据集TEND（Text-to-NoSQL Dataset）。此外，我们还设计了一种名为SMART（SLM辅助和RAG辅助多步框架）的框架，专门用于Text-to-NoSQL转换。为了确保对模型进行全面评估，我们还引入了一套详细的评估指标，从查询本身及其执行结果两个方面评估模型性能。实验证明了我们方法的有效性，并为该新兴领域的未来研究建立了基准。我们相信，我们的贡献将为NoSQL数据库的更易用和直观交互铺平道路。', 'title_zh': '填补空白：通过文本到NoSQL翻译使自然语言查询成为可能'}
{'arxiv_id': 'arXiv:2502.11196', 'title': 'How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training', 'authors': 'Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen', 'link': 'https://arxiv.org/abs/2502.11196', 'abstract': 'Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at this https URL.', 'abstract_zh': '尽管大型语言模型在知识密集型任务中表现出色，但在理解它们如何内化新知识方面仍存在关键差距，特别是在如何在神经计算中结构化嵌入获得的知识方面。我们通过知识电路进化的视角来应对这一问题，识别出促进知识存储和处理的计算子图。我们对持续预训练过程中电路进化的系统分析揭示了几项关键发现：(1) 新知识的获取受先前知识的相关性影响；(2) 知识电路的进化表现出从形成到优化的阶段转变；(3) 知识电路的进化遵循从深到浅的模式。这些见解不仅推进了我们对大型语言模型中新知识获取机制的理论理解，还为改进持续预训练策略以提升模型性能提供了潜在影响。相关代码和数据将在以下网址获取：[此链接处]。', 'title_zh': 'LLMs如何获取新知识？一种持续预训练的知识电路视角'}
{'arxiv_id': 'arXiv:2502.11191', 'title': 'Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training', 'authors': 'Yao-Ching Yu, Tsun-Han Chiang, Cheng-Wei Tsai, Chien-Ming Huang, Wen-Kwang Tsao', 'link': 'https://arxiv.org/abs/2502.11191', 'abstract': 'Large Language Models (LLMs) have shown remarkable advancements in specialized fields such as finance, law, and medicine. However, in cybersecurity, we have noticed a lack of open-source datasets, with a particular lack of high-quality cybersecurity pretraining corpora, even though much research indicates that LLMs acquire their knowledge during pretraining. To address this, we present a comprehensive suite of datasets covering all major training stages, including pretraining, instruction fine-tuning, and reasoning distillation with cybersecurity-specific self-reflection data. Extensive ablation studies demonstrate their effectiveness on public cybersecurity benchmarks. In particular, continual pre-training on our dataset yields a 15.88% improvement in the aggregate score, while reasoning distillation leads to a 10% gain in security certification (CISSP). We will release all datasets and trained cybersecurity LLMs under the ODC-BY and MIT licenses to encourage further research in the community. For access to all datasets and model weights, please refer to this https URL.', 'abstract_zh': '大型语言模型（LLMs）在金融、法律和医学等专业领域展现了显著的进步。然而，在网络信息安全领域，我们注意到缺乏开源数据集，尤其是在高质网络信息安全预训练语料方面存在明显不足，尽管许多研究指出LLMs在其预训练阶段就已获取知识。为解决这一问题，我们提供了一套全面的数据集，涵盖了所有主要的训练阶段，包括预训练、指令微调和基于网络安全特定自反数据的推理精炼。广泛的消融研究证明了这些数据集在公开的网络安全基准测试中的有效性。特别是，持续使用我们的数据集进行预训练可提高综合得分15.88%，而推理精炼可使安全认证（CISSP）成绩提高10%。我们将所有数据集和训练好的网络信息安全大型语言模型在ODC-BY和MIT许可证下发布，以促进社区内的进一步研究。欲获取所有数据集和模型权重，请访问此链接：https URL。', 'title_zh': 'Primus: 首个开源数据集集合，用于网络安全LLM训练'}
{'arxiv_id': 'arXiv:2502.11190', 'title': 'ReLearn: Unlearning via Learning for Large Language Models', 'authors': 'Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang', 'link': 'https://arxiv.org/abs/2502.11190', 'abstract': 'Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at this https URL.', 'abstract_zh': '当前的大语言模型去学习方法通常依赖于反向优化来降低目标标记的概率。然而，这种范式会破坏后续标记的预测，降低模型性能和语言连贯性。此外，现有的评估指标过度强调上下文遗忘，而未能充分评估响应的流畅性和相关性。为了应对这些挑战，我们提出 ReLearn，一种有效去学习的数据增强和微调管道，以及一个全面的评估框架。该框架引入了知识遗忘率（KFR）和知识保留率（KRR）来衡量知识层面的保留，并引入语言得分（LS）来评估生成质量。我们的实验表明，ReLearn 成功实现了目标遗忘，同时保持了高质量的输出。通过机制分析，我们进一步证明了反向优化如何破坏连贯文本生成，而 ReLearn 保留了这一重要能力。代码可在以下网址获取：这个 https URL。', 'title_zh': 'ReLearn: 通过学习实现大规模语言模型的遗忘'}
{'arxiv_id': 'arXiv:2502.11187', 'title': 'TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking', 'authors': 'Shahriar Kabir Nahin, Rabindra Nath Nandi, Sagor Sarker, Quazi Sarwar Muhtaseem, Md Kowsher, Apu Chandraw Shill, Md Ibrahim, Mehadi Hasan Menon, Tareq Al Muntasir, Firoj Alam', 'link': 'https://arxiv.org/abs/2502.11187', 'abstract': 'In this paper, we present TituLLMs, the first large pretrained Bangla LLMs, available in 1B and 3B parameter sizes. Due to computational constraints during both training and inference, we focused on smaller models. To train TituLLMs, we collected a pretraining dataset of approximately 37 billion tokens. We extended the Llama-3.2 tokenizer to incorporate language- and culture-specific knowledge, which also enables faster training and inference. There was a lack of benchmarking datasets to evaluate LLMs for Bangla. To address this gap, we developed five benchmarking datasets. We benchmarked various LLMs, including TituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual versions. However, this is not always the case, highlighting the complexities of language adaptation. Our work lays the groundwork for adapting existing multilingual open models to other low-resource languages. To facilitate broader adoption and further research, we have made the TituLLMs models and benchmarking datasets publicly available (this https URL).', 'abstract_zh': '本文介绍了TituLLMs，这是第一个可用的大型预训练孟加拉语语言模型，参数规模为1B和3B。由于在训练和推断过程中受到计算限制，我们专注于较小的模型。为了训练TituLLMs，我们收集了一个约370亿个令牌的预训练数据集。我们将Llama-3.2分词器扩展为包含语言和文化特定的知识，这还使得训练和推断速度更快。对于孟加拉语语言模型缺乏基准数据集的问题，我们开发了五个基准数据集。我们对包括TituLLMs在内的多种语言模型进行了基准测试，并证明了TituLLMs优于其最初的多语言版本。然而，这并非总是如此，这突显了语言适应的复杂性。我们的工作为基础多资源语言适配现有的多语言开放模型奠定了基础。为了促进更广泛的采用和进一步的研究，我们已将TituLLMs模型和基准数据集公开（this https URL）。', 'title_zh': 'TituLLMs: 一个全面基准测试的孟加拉语大型语言模型家族'}
{'arxiv_id': 'arXiv:2502.11184', 'title': "Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs", 'authors': 'Wenxuan Wang, Xiaoyuan Liu, Kuiyi Gao, Jen-tse Huang, Youliang Yuan, Pinjia He, Shuai Wang, Zhaopeng Tu', 'link': 'https://arxiv.org/abs/2502.11184', 'abstract': 'Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research.', 'abstract_zh': '多模态大型语言模型（MLLMs）通过使交互能够通过文本和图像进行，扩大了传统语言模型的能力。然而，确保这些模型的安全性仍然是一个重大挑战，特别是准确识别多模态内容是否安全的能力——我们称之为安全性意识。在本文中，我们介绍了MMSafeAware，这是首个用于评估MLLMs在涵盖29种安全场景的1500个精挑细选的图像-提示对中的综合多模态安全性意识基准。MMSafeAware包括不安全和过度安全的子集，以评估模型正确识别不安全内容并避免过度敏感的能力，后者可能会妨碍其帮助性。使用MMSafeAware评估九个广泛使用的MLLMs发现，当前模型在安全性上并不充分且通常过于敏感；例如，GPT-4V错误地将36.1%的不安全输入分类为安全输入，并错误地将59.9%的良性输入分类为不安全输入。我们进一步探索了三种提高安全性意识的方法——基于提示的方法、视觉对比解码以及以视觉为中心的推理微调，但发现这三种方法均未能达到令人满意的效果。我们的研究结果突显了在开发具有稳健安全性意识的MLLMs方面面临的巨大挑战，强调了需要在这一领域进行进一步研究的重要性。所有代码和数据将公开以促进未来的研究。', 'title_zh': '难以见森林而见树木：多模态安全意识基准测试 for 多模态大语言模型'}
{'arxiv_id': 'arXiv:2502.11149', 'title': 'Large Language-Geometry Model: When LLM meets Equivariance', 'authors': 'Zongzhao Li, Jiacheng Cen, Bing Su, Wenbing Huang, Tingyang Xu, Yu Rong, Deli Zhao', 'link': 'https://arxiv.org/abs/2502.11149', 'abstract': 'Accurately predicting 3D structures and dynamics of physical systems is crucial in scientific applications. Existing approaches that rely on geometric Graph Neural Networks (GNNs) effectively enforce $\\mathrm{E}(3)$-equivariance, but they often fall in leveraging extensive broader information. While direct application of Large Language Models (LLMs) can incorporate external knowledge, they lack the capability for spatial reasoning with guaranteed equivariance. In this paper, we propose EquiLLM, a novel framework for representing 3D physical systems that seamlessly integrates E(3)-equivariance with LLM capabilities. Specifically, EquiLLM comprises four key components: geometry-aware prompting, an equivariant encoder, an LLM, and an equivariant adaptor. Essentially, the LLM guided by the instructive prompt serves as a sophisticated invariant feature processor, while 3D directional information is exclusively handled by the equivariant encoder and adaptor modules. Experimental results demonstrate that EquiLLM delivers significant improvements over previous methods across molecular dynamics simulation, human motion simulation, and antibody design, highlighting its promising generalizability.', 'abstract_zh': '准确预测物理系统的三维结构和动态在科学研究中至关重要。现有依赖几何图神经网络（GNNs）的方法有效确保了$\\mathrm{E}(3)$-拟不变性，但往往未能充分利用广泛的信息。虽然可以直接应用大型语言模型（LLMs）整合外部知识，但它们缺乏空间推理的能力和拟不变性保证。在本文中，我们提出了一种新的框架EquiLLM，该框架无缝地结合了E(3)-拟不变性和LLM的能力。具体而言，EquiLLM包含四个关键组成部分：几何感知的提示、拟不变性编码器、LLM以及拟不变性适配器。本质上，受指令性提示引导的LLM充当复杂的不变特征处理器，而3D方向性信息则由拟不变性编码器和适配器模块单独处理。实验结果表明，EquiLLM在分子动力学模拟、人类运动模拟和抗体设计方面均显著优于先前的方法，突显了其 promising 的泛化能力。', 'title_zh': '大语言-几何模型：当LLM遇见等变性'}
{'arxiv_id': 'arXiv:2502.11147', 'title': 'Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity', 'authors': 'Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, Xusheng Chen, Tao Xie, Yizhou Shan', 'link': 'https://arxiv.org/abs/2502.11147', 'abstract': "Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires long decoding chains (of thoughts), which incur $O(N)$ time and memory consumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory consumption, existing sparsity-based algorithms propose retaining only the most critical token's intermediate data (i.e., key-value cache) and discarding the rest. However, these existing algorithms struggle with the ``impossible trinity'' of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm named RaaS that identifies and retains milestone tokens only until they are no longer needed, achieving high accuracy with $O(L)$ time and $O(L)$ memory complexity.", 'abstract_zh': '大型语言模型在各种领域展示了强大的能力，特别是在数学和编程等具有挑战性的推理任务中取得了进展。然而，解决推理任务往往需要长的解码链（思考链），这会带来$O(N)$的时间和内存消耗，其中$N$是链的长度。为了减轻$O(N)$的时间和内存消耗，现有的基于稀疏性的算法建议仅保留关键令牌的中间数据（即键值缓存），并丢弃其他数据。然而，这些现有算法在准确度、时间和内存之间难以兼顾。“不可能的三角”困境表现为：最先进的算法Quest能够在$O(L)$时间内实现高准确度，但需要$O(N)$的内存（$L$为缓存预算，$L \\ll N$）。为了解决这个问题，本文在推理任务的解码阶段识别出一种新的注意力模式，其中里程碑令牌（类似于数学证明中的引理）出现、被利用，随后变得不再重要。基于此模式，我们提出了一种新的算法RaaS，该算法仅在里程碑令牌不再需要时识别并保留它们，从而以$O(L)$的时间复杂度和$O(L)$的内存复杂度实现高准确度。', 'title_zh': '长解码推理中的推理感知注意力稀疏性高效推理'}
{'arxiv_id': 'arXiv:2502.11140', 'title': 'VisPath: Automated Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization', 'authors': 'Wonduk Seo, Seungyong Lee, Daye Kang, Zonghao Yuan, Seunghyun Lee', 'link': 'https://arxiv.org/abs/2502.11140', 'abstract': 'Unprecedented breakthroughs in Large Language Models (LLMs) has amplified its penetration into application of automated visualization code generation. Few-shot prompting and query expansion techniques have notably enhanced data visualization performance, however, still fail to overcome ambiguity and complexity of natural language queries - imposing an inherent burden for manual human intervention. To mitigate such limitations, we propose a holistic framework VisPath : A Multi-Path Reasoning and Feedback-Driven Optimization Framework for Visualization Code Generation, which systematically enhances code quality through structured reasoning and refinement. VisPath is a multi-stage framework, specially designed to handle underspecified queries. To generate a robust final visualization code, it first utilizes initial query to generate diverse reformulated queries via Chain-of-Thought (CoT) prompting, each representing a distinct reasoning path. Refined queries are used to produce candidate visualization scripts, consequently executed to generate multiple images. Comprehensively assessing correctness and quality of outputs, VisPath generates feedback for each image, which are then fed to aggregation module to generate optimal result. Extensive experiments on benchmarks including MatPlotBench and the Qwen-Agent Code Interpreter Benchmark show that VisPath significantly outperforms state-of-the-art (SOTA) methods, increased up to average 17%, offering a more reliable solution for AI-driven visualization code generation.', 'abstract_zh': '大型语言模型（LLMs）前所未有的突破使其在自动化可视化代码生成中的应用更加广泛。尽管少量示例提示和查询扩展技术显著提升了数据可视化性能，但仍无法克服自然语言查询的模糊性和复杂性，从而增加了手动人工干预的负担。为了克服这些限制，我们提出了一种综合框架VisPath：一种多路径推理和反馈驱动优化框架，以系统地通过结构化推理和优化提升代码质量。VisPath是一个多阶段框架，专门设计用于处理不明确查询。为了生成 robust 的最终可视化代码，它首先利用初始查询生成多种多样重新表述的查询，每个查询代表不同的推理路径。经过优化的查询用于生成候选可视化脚本，随后执行以生成多张图像。综合评估输出的正确性和质量后，VisPath 为每张图像生成反馈，这些反馈随后被反馈到聚合模块以生成最优结果。在包括MatPlotBench和Qwen-Agent Code Interpreter Benchmark在内的基准测试中，VisPath 显著优于现有最先进的方法，平均提高多达17%，提供了一种更可靠的人工智能驱动的可视化代码生成解决方案。', 'title_zh': 'VisPath: 基于多路径推理和反馈驱动优化的自动可视化代码合成'}
{'arxiv_id': 'arXiv:2502.11108', 'title': 'Knowledge Graph-Driven Retrieval-Augmented Generation: Integrating Deepseek-R1 with Weaviate for Advanced Chatbot Applications', 'authors': 'Alexandru Lecu, Adrian Groza, Lezan Hawizy', 'link': 'https://arxiv.org/abs/2502.11108', 'abstract': 'Large language models (LLMs) have significantly advanced the field of natural language generation. However, they frequently generate unverified outputs, which compromises their reliability in critical applications. In this study, we propose an innovative framework that combines structured biomedical knowledge with LLMs through a retrieval-augmented generation technique. Our system develops a thorough knowledge graph by identifying and refining causal relationships and named entities from medical abstracts related to age-related macular degeneration (AMD). Using a vector-based retrieval process and a locally deployed language model, our framework produces responses that are both contextually relevant and verifiable, with direct references to clinical evidence. Experimental results show that this method notably decreases hallucinations, enhances factual precision, and improves the clarity of generated responses, providing a robust solution for advanced biomedical chatbot applications.', 'abstract_zh': '大型语言模型(Large Language Models, LLMs)在自然语言生成领域取得了显著进展。然而，它们经常生成未经验证的输出，这在关键应用中损害了其可靠性。本研究提出了一种结合结构化生物医学知识与LLMs的创新框架，通过检索增强生成技术。我们的系统通过识别和精炼与年龄相关黄斑变性(AMD)相关的医学摘要中的因果关系和命名实体，构建了一个详尽的知识图谱。利用基于向量的检索过程和本地部署的语言模型，我们的框架生成了既相关又可验证的响应，并直接引用了临床证据。实验结果表明，这种方法显著降低了幻觉，提高了事实的精确性，并提高了生成响应的清晰度，为高级生物医学聊天机器人应用提供了稳健的解决方案。', 'title_zh': '知识图谱驱动的检索增强生成：将Deepseek-R1与Weaviate集成以实现高级聊天机器人应用'}
{'arxiv_id': 'arXiv:2502.11107', 'title': 'Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse KL vs. Forward KL', 'authors': 'Wei Yao, Wenkai Yang, Ziqiao Wang, Yankai Lin, Yong Liu', 'link': 'https://arxiv.org/abs/2502.11107', 'abstract': "As large language models advance toward superhuman performance, ensuring their alignment with human values and abilities grows increasingly complex. Weak-to-strong generalization offers a promising approach by leveraging predictions from weaker models to guide stronger systems, but its effectiveness could be constrained by the inherent noise and inaccuracies in these weak predictions. To address this, we propose a theoretically grounded approach that replaces forward KL divergence-whose mass-covering behavior risks overfitting to imperfect weak signals-with reverse KL divergence. Reverse KL divergence's zero-forcing effect prioritizes high-confidence predictions, effectively mitigating the influence of unreliable weak supervision. Theoretically, we extend existing bounds and derive tighter lower bounds for both forward and reverse KL divergence, establishing that reverse KL achieves at least comparable guarantees to forward KL. Notably, when a sufficiently pre-trained strong model is fine-tuned on the last layer, reverse KL uniquely guarantees that it outperforms its weak supervisor by the magnitude of their disagreement-a guarantee that forward KL cannot provide. Empirically, we demonstrate that reverse KL and reverse cross-entropy enable strong models to consistently outperform those trained with forward KL and standard cross-entropy across most settings, highlighting the practical advantages of these reverse losses.", 'abstract_zh': '随着大型语言模型向超人类性能迈进，确保其与人类价值观和能力的对齐日益复杂。通过利用较弱模型的预测来引导较强系统的方法——弱到强泛化的潜力不容忽视，但其效果可能受到这些较弱预测中固有的噪音和不准确性的影响。为此，我们提出了一种理论支持的方法，替代可能导致对不完美弱信号过度拟合的前向KL散度，而是采用后向KL散度。后向KL散度的零强迫效应强调高置信度的预测，有效地减轻了不可靠弱监督的影响。从理论上讲，我们扩展了现有的边界，并为前向和后向KL散度推导了更紧的下界，证明了后向KL至少能达到与前向KL相当的保证。值得注意的是，当一个充分预训练的强模型在最后一层进行微调时，后向KL唯一地保证其在争议程度上优于其弱监督者——这是前向KL无法提供的保证。实验上，我们展示了后向KL和后向交叉熵使强模型在大多数设置中一致地优于使用前向KL和标准交叉熵训练的模型，突出了这些后向损失的实际优势。', 'title_zh': '重新审视从弱泛化到强泛化的理论与实践：逆KL散度 vs. 正向KL散度'}
{'arxiv_id': 'arXiv:2502.11101', 'title': 'CacheFocus: Dynamic Cache Re-Positioning for Efficient Retrieval-Augmented Generation', 'authors': 'Kun-Hui Lee, Eunhwan Park, Donghoon Han, Seung-Hoon Na', 'link': 'https://arxiv.org/abs/2502.11101', 'abstract': 'Large Language Models (LLMs) excel across a variety of language tasks yet are constrained by limited input lengths and high computational costs. Existing approaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi) and sliding window mechanisms\\textemdash partially alleviate these issues but often require additional training or suffer from performance degradation with longer inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a method that enhances length normalization and reduces inference latency without any further training. Our approach leverages query-independent, offline caching to efficiently reuse a Context KV Cache Store. We address the amplification of abnormal token distributions problem by re-positioning cached keys and introducing Layer-Adaptive Cache Pruning to discard low-relevance caches during pre-filling. Additionally, our Adaptive Positional Allocation Strategy dynamically reassigns cache positions to maximize the use of the available positional encoding range. Experiments on the Natural Questions and TriviaQA datasets demonstrate that CacheFocus outperforms alternative methods even when inputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its practical effectiveness for long-context LLMs. Moreover, even with large maximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows that it maintains consistent performance even as the number of documents increases, effectively managing long-text generation without degradation.', 'abstract_zh': 'Large Language Models（LLMs）在多种语言任务中表现出色，但受限于输入长度限制和高计算成本。现有方法如相对位置编码（例如RoPE、ALiBi）和滑动窗口机制部分缓解了这些问题，但往往需要额外训练或在长输入时性能下降。本文介绍了一种名为**CacheFocus**的方法，该方法在无需额外训练的情况下提升了长度归一化并减少了推理延迟。我们的方法利用查询无关的离线缓存高效重用上下文KV缓存存储。通过重新定位缓存键并引入层自适应缓存剪枝来解决异常 token 分布放大的问题，同时在预填充阶段丢弃相关性低的缓存。此外，我们的自适应位置分配策略动态重新分配缓存位置，以充分利用可用的位置编码范围。在Natural Questions和TriviaQA数据集上的实验证明，CacheFocus即使在输入超过LLaMA-2模型的4K限制时也优于其他方法，强调了其在长上下文LLMs中的实际有效性。此外，即使在Qwen2模型具有大最大输入长度的情况下，CacheFocus的性能也随着文档数量的增加而保持稳定，有效地管理长文本生成而不出现性能下降。', 'title_zh': 'CacheFocus: 动态缓存重新定位以实现高效的检索增强生成'}
{'arxiv_id': 'arXiv:2502.11090', 'title': 'SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks', 'authors': 'Hongye Cao, Yanming Wang, Sijia Jing, Ziyue Peng, Zhixin Bai, Zhe Cao, Meng Fang, Fan Feng, Boyan Wang, Jiaheng Liu, Tianpei Yang, Jing Huo, Yang Gao, Fanyu Meng, Xi Yang, Chao Deng, Junlan Feng', 'link': 'https://arxiv.org/abs/2502.11090', 'abstract': "With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability of identifying and handling unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities.", 'abstract_zh': '随着大型语言模型（LLMs）的迅速发展，LLMs的安全性已成为一个关键关切，需要精确评估。当前的基准主要集中在单轮对话或单一脱逃攻击方法上进行安全性评估，并没有详细考虑LLMs识别和处理不安全信息的能力。为解决这些问题，我们提出了一种细粒度基准SafeDialBench，用于评估LLMs在多轮对话中多种脱逃攻击下的安全性。具体地，我们设计了一个两层分级安全分类体系，考虑了6个安全维度，并生成了超过4000个多轮对话，涵盖22种对话场景，包括中文和英文。我们采用了包括引用攻击和目的反向在内的7种脱逃攻击策略，以提高对话生成数据集的质量。值得注意的是，我们构建了一种创新的LLM评估框架，用于衡量识别和处理不安全信息以及在面对脱逃攻击时保持一致性的能力。实验结果显示，Yi-34B-Chat和GLM4-9B-Chat表现出更优异的安全性能，而Llama3.1-8B-Instruct和o3-mini则显示出安全性漏洞。', 'title_zh': 'SafeDialBench：大语言模型在多轮对话中对抗多样化脱管攻击的安全细粒度基准'}
{'arxiv_id': 'arXiv:2502.11089', 'title': 'Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention', 'authors': 'Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, Wangding Zeng', 'link': 'https://arxiv.org/abs/2502.11089', 'abstract': 'Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.', 'abstract_zh': '长上下文建模是下一代语言模型的关键，但标准注意力机制的高计算成本带来了显著的计算挑战。稀疏注意力机制为在保持模型能力的同时提高效率提供了有前途的方向。我们提出了一种名为NSA的本征可训练稀疏注意力机制，将算法创新与硬件对齐的优化相结合，以实现高效的长上下文建模。NSA采用动态分层稀疏策略，结合粗粒度的 token 压缩与细粒度的 token 选择，以保持全局上下文意识和局部精度。我们的方法在稀疏注意力设计方面实现了两项关键创新：(1) 通过算术强度平衡的算法设计实现显著加速，并针对现代硬件进行实现优化。(2) 实现端到端训练，减少预训练计算量而不牺牲模型性能。如图1所示，实验表明使用NSA预训练的模型在通用基准、长上下文任务和基于指令的推理方面均能保持或超越全注意力模型。同时，NSA在64k长度序列的解码、前向传播和反向传播中实现了显著加速，验证了其在整个模型生命周期中的高效性。', 'title_zh': '原生稀疏注意力：硬件对齐且原生可训练的稀疏注意力'}
{'arxiv_id': 'arXiv:2502.11075', 'title': 'Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models', 'authors': 'Haoyang Li, Xuejia Chen, Zhanchao XU, Darian Li, Nicole Hu, Fei Teng, Yiming Li, Luyu Qiu, Chen Jason Zhang, Qing Li, Lei Chen', 'link': 'https://arxiv.org/abs/2502.11075', 'abstract': 'Large Language Models (LLMs) have demonstrated impressive capabilities in natural language processing tasks, such as text generation and semantic understanding. However, their performance on numerical reasoning tasks, such as basic arithmetic, numerical retrieval, and magnitude comparison, remains surprisingly poor. This gap arises from their reliance on surface-level statistical patterns rather than understanding numbers as continuous magnitudes. Existing benchmarks primarily focus on either linguistic competence or structured mathematical problem-solving, neglecting fundamental numerical reasoning required in real-world scenarios. To bridge this gap, we propose NumericBench, a comprehensive benchmark to evaluate six fundamental numerical capabilities: number recognition, arithmetic operations, contextual retrieval, comparison, summary, and logical reasoning. NumericBench includes datasets ranging from synthetic number lists to the crawled real-world data, addressing challenges like long contexts, noise, and multi-step reasoning. Extensive experiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal persistent weaknesses in numerical reasoning, highlighting the urgent need to improve numerically-aware language modeling. The benchmark is released in: this https URL.', 'abstract_zh': '大型语言模型（LLMs）在自然语言处理任务如文本生成和语义理解方面展示了令人印象深刻的能力。然而，在基本算术、数值检索和数量比较等数值推理任务上的表现仍然出人意料地差。这种差距源于它们依赖于表面级的统计模式，而不是理解数字作为连续量的意义。现有的基准测试主要关注语言能力或结构化数学问题解决，忽视了真实世界场景中所需的最基本数值推理能力。为了弥合这一差距，我们提出了NumericBench，一个全面的基准测试，用于评估六个基本的数值能力：数字识别、算术运算、上下文检索、比较、总结和逻辑推理。NumericBench 包括从合成数字列表到抓取的真实世界数据的数据集，解决了长上下文、噪声和多步推理等挑战。在最先进的大语言模型（包括GPT-4和DeepSeek）上的广泛实验揭示了数值推理中的持久性弱点，突显了提高数值感知语言建模的迫切需求。基准测试已发布于：this https URL。', 'title_zh': '暴露数值能力差距：评估大型语言模型基本数值能力的标准'}
{'arxiv_id': 'arXiv:2502.11059', 'title': 'ClimateLLM: Efficient Weather Forecasting via Frequency-Aware Large Language Models', 'authors': 'Shixuan Li, Wei Yang, Peiyu Zhang, Xiongye Xiao, Defu Cao, Yuehan Qin, Xiaole Zhang, Yue Zhao, Paul Bogdan', 'link': 'https://arxiv.org/abs/2502.11059', 'abstract': 'Weather forecasting is crucial for public safety, disaster prevention and mitigation, agricultural production, and energy management, with global relevance. Although deep learning has significantly advanced weather prediction, current methods face critical limitations: (i) they often struggle to capture both dynamic temporal dependencies and short-term abrupt changes, making extreme weather modeling difficult; (ii) they incur high computational costs due to extensive training and resource requirements; (iii) they have limited adaptability to multi-scale frequencies, leading to challenges when separating global trends from local fluctuations. To address these issues, we propose ClimateLLM, a foundation model for weather forecasting. It captures spatiotemporal dependencies via a cross-temporal and cross-spatial collaborative modeling framework that integrates Fourier-based frequency decomposition with Large Language Models (LLMs) to strengthen spatial and temporal modeling. Our framework uses a Mixture-of-Experts (MoE) mechanism that adaptively processes different frequency components, enabling efficient handling of both global signals and localized extreme events. In addition, we introduce a cross-temporal and cross-spatial dynamic prompting mechanism, allowing LLMs to incorporate meteorological patterns across multiple scales effectively. Extensive experiments on real-world datasets show that ClimateLLM outperforms state-of-the-art approaches in accuracy and efficiency, as a scalable solution for global weather forecasting.', 'abstract_zh': '气候LLM：一种用于天气预报的基础模型', 'title_zh': 'ClimateLLM：基于频率意识的大语言模型高效天气预报'}
{'arxiv_id': 'arXiv:2502.11054', 'title': 'Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models', 'authors': 'Zonghao Ying, Deyue Zhang, Zonglei Jing, Yisong Xiao, Quanchen Zou, Aishan Liu, Siyuan Liang, Xiangzheng Zhang, Xianglong Liu, Dacheng Tao', 'link': 'https://arxiv.org/abs/2502.11054', 'abstract': "Multi-turn jailbreak attacks simulate real-world human interactions by engaging large language models (LLMs) in iterative dialogues, exposing critical safety vulnerabilities. However, existing methods often struggle to balance semantic coherence with attack effectiveness, resulting in either benign semantic drift or ineffective detection evasion. To address this challenge, we propose Reasoning-Augmented Conversation, a novel multi-turn jailbreak framework that reformulates harmful queries into benign reasoning tasks and leverages LLMs' strong reasoning capabilities to compromise safety alignment. Specifically, we introduce an attack state machine framework to systematically model problem translation and iterative reasoning, ensuring coherent query generation across multiple turns. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Extensive experiments on multiple LLMs demonstrate that RACE achieves state-of-the-art attack effectiveness in complex conversational scenarios, with attack success rates (ASRs) increasing by up to 96%. Notably, our approach achieves ASRs of 82% and 92% against leading commercial models, OpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at this https URL to facilitate further research in this critical domain.", 'abstract_zh': '多轮 Jailbreak 攻击通过在迭代对话中促使大型语言模型（LLMs）参与真实世界的互动，揭示关键的安全漏洞。为解决现有方法在语义连贯性与攻击有效性之间难以平衡的问题，我们提出了一种名为增强推理对话（Reasoning-Augmented Conversation, RACE）的新型多轮 Jailbreak 框架，该框架将有害查询重新表述为无害的推理任务，并利用 LLM 强大的推理能力破坏安全性对齐。具体而言，我们引入了一种攻击状态机框架，系统地建模问题转换和迭代推理，确保多轮查询生成的一致性。在此基础上，我们设计了收益导向的探索、自对弈和拒绝反馈模块，以保持攻击语义、增强有效性并维持推理驱动的攻击进展。在多个 LLM 上的广泛实验表明，RACE 在复杂的对话场景中实现了最先进的攻击有效性，攻击成功率（ASRs）最高提升 96%。值得注意的是，我们的方法在对抗领先商用模型 OpenAI o1 和 DeepSeek R1 时的 ASRs 分别达到 82% 和 92%，突显了其强大的效果。我们已在以下链接发布了我们的代码，以促进对该关键领域的进一步研究。', 'title_zh': '增强推理的多轮 Jailbreak 攻击对话模型推理增强的多轮 Jailbreak 攻击对话模型'}
{'arxiv_id': 'arXiv:2502.11051', 'title': 'MMUNLEARNER: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models', 'authors': 'Jiahao Huo, Yibo Yan, Xu Zheng, Yuanhuiyi Lyu, Xin Zou, Zhihua Wei, Xuming Hu', 'link': 'https://arxiv.org/abs/2502.11051', 'abstract': 'Recent progress in Machine Unlearning (MU) has introduced solutions for the selective removal of private or sensitive information encoded within deep neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs) remains in its nascent phase. Therefore, we propose to reformulate the task of multimodal MU in the era of MLLMs, which aims to erase only the visual patterns associated with a given entity while preserving the corresponding textual knowledge encoded within the original parameters of the language model backbone. Furthermore, we develop a novel geometry-constrained gradient descent method MMUnlearner. It updates the weights of MLLMs with a weight saliency map jointly restricted by the remaining concepts and textual knowledge during unlearning, thereby preserving parameters essential for non-target knowledge. Extensive experiments demonstrate that MMUnlearner surpasses baselines that finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or Negative Preference Optimization (NPO), across all evaluation dimensions. Our code will be released upon acceptance.', 'abstract_zh': '近期，机器遗忘（Machine Unlearning, MU）领域在选择性移除深神经网络中编码的私人或敏感信息方面取得了进展。然而，针对多模态大型语言模型（Multimodal Large Language Models, MLLMs）的MU仍处于初级阶段。因此，我们提出在MLLM时代重新定义多模态MU的任务，目标是在遗忘过程中仅删除与给定实体相关的视觉模式，同时保留语言模型骨干中原有的相应文本知识。此外，我们开发了一种新的几何约束梯度下降方法MMUnlearner。该方法在遗忘过程中通过一个联合受限于剩余概念和文本知识的权重显著图来更新MLLMs的权重，从而保留对非目标知识至关重要的参数。大量实验表明，MMUnlearner在所有评估维度上均优于直接使用问答数据（VQA数据）通过梯度上升（Gradient Ascent, GA）或负偏好优化（Negative Preference Optimization, NPO）微调MLLMs的基线方法。关于我们代码，将在接收后公开。', 'title_zh': 'MMUNLEARNER: 在多模态大语言模型时代重述多模态机器遗忘方法'}
{'arxiv_id': 'arXiv:2502.11028', 'title': 'Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models', 'authors': 'Prateek Chhikara', 'link': 'https://arxiv.org/abs/2502.11028', 'abstract': 'Large Language Models (LLMs) demonstrate impressive performance across diverse tasks, yet confidence calibration remains a challenge. Miscalibration - where models are overconfident or underconfident - poses risks, particularly in high-stakes applications. This paper presents an empirical study on LLM calibration, examining how model size, distractors, and question types affect confidence alignment. We introduce an evaluation framework to measure overconfidence and investigate whether multiple-choice formats mitigate or worsen miscalibration. Our findings show that while larger models (e.g., GPT-4o) are better calibrated overall, they are more prone to distraction, whereas smaller models benefit more from answer choices but struggle with uncertainty estimation. Unlike prior work, which primarily reports miscalibration trends, we provide actionable insights into failure modes and conditions that worsen overconfidence. These findings highlight the need for calibration-aware interventions and improved uncertainty estimation methods.', 'abstract_zh': '大型语言模型（LLMs）在多种任务上表现出色，但置信度校准仍然是一个挑战。误校准——即模型过于自信或不够自信——在高风险应用中尤其充满风险。本文通过对LLM校准的实证研究，探讨模型大小、干扰项和问题类型如何影响置信度对齐。我们提出了一种评估框架来衡量过度自信，并调查多项选择格式是否缓解或加剧了误校准。研究结果显示，虽然 Larger 模型（例如，GPT-4o）总体上更准确校准，但它们更容易受到干扰，而较小的模型虽然从答案选项中受益更多，但在不确定性估计方面遇到困难。不同于以往主要报告误校准趋势的研究，我们提供了有关失败模式和加剧过度自信的条件的可操作性见解。这些发现强调了需要采取校准意识干预并改进不确定性估计方法的重要性。', 'title_zh': '注意信心差距：大型语言模型中的过度自信、校准与干扰效应'}
{'arxiv_id': 'arXiv:2502.11026', 'title': 'Simplify RLHF as Reward-Weighted SFT: A Variational Method', 'authors': 'Yuhao Du, Zhuo Li, Pengyu Cheng, Zhihong Chen, Yuejiao Xie, Xiang Wan, Anningzhe Gao', 'link': 'https://arxiv.org/abs/2502.11026', 'abstract': 'Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning Large Language Models (LLMs) with human values. However, RLHF has been continuously challenged by its high complexity in implementation and computation consumption. Even with recent simplifications, such as Direct Preference Optimization (DPO) and Advantage Leftover Lunch (A-LoL), the problems of over-fitting and training instability remain hindering the alignment process from the expected optimal performance. To address the existing challenges, we propose a novel simplification of RLHF from the perspective of variational inference, called $\\textbf{V}$ariational $\\textbf{A}$lignment with $\\textbf{R}$e-weighting ($\\textbf{VAR}$). More specifically, by directly minimizing the distribution gap between the learning LLM policy and the optimal solution of RLHF, we transform the alignment objective into a reward-driven re-weighted supervised fine-tuning (SFT) form, which only requires minor adjustment on the SFT loss to obtain noticeable improvement on training stability and effectiveness. On comprehensive alignment and generation benchmarks, our VAR method has numerically achieved competitive performance in LLM alignment helpfulness and harmlessness.', 'abstract_zh': '基于变分推断的Reinforcement Learning from Human Feedback ($\\textbf{VAR}$)：改善大型语言模型对齐的新型简化方法', 'title_zh': '将RLHF简化为奖励加权SFT：一种变分方法'}
{'arxiv_id': 'arXiv:2502.11020', 'title': 'TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages', 'authors': 'Jafar Isbarov, Arofat Akhundjanova, Mammad Hajili, Kavsar Huseynova, Dmitry Gaynullin, Anar Rzayev, Osman Tursun, Ilshat Saetov, Rinat Kharisov, Saule Belginova, Ariana Kenbayeva, Amina Alisheva, Aizirek Turdubaeva, Abdullatif Köksal, Samir Rustamov, Duygu Ataman', 'link': 'https://arxiv.org/abs/2502.11020', 'abstract': 'Being able to thoroughly assess massive multi-task language understanding (MMLU) capabilities is essential for advancing the applicability of multilingual language models. However, preparing such benchmarks in high quality native language is often costly and therefore limits the representativeness of evaluation datasets. While recent efforts focused on building more inclusive MMLU benchmarks, these are conventionally built using machine translation from high-resource languages, which may introduce errors and fail to account for the linguistic and cultural intricacies of the target languages. In this paper, we address the lack of native language MMLU benchmark especially in the under-represented Turkic language family with distinct morphosyntactic and cultural characteristics. We propose two benchmarks for Turkic language MMLU: TUMLU is a comprehensive, multilingual, and natively developed language understanding benchmark specifically designed for Turkic languages. It consists of middle- and high-school level questions spanning 11 academic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar, Turkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise, balanced, and manually verified subset of the dataset. Using this dataset, we systematically evaluate a diverse range of open and proprietary multilingual large language models (LLMs), including Claude, Gemini, GPT, and LLaMA, offering an in-depth analysis of their performance across different languages, subjects, and alphabets. To promote further research and development in multilingual language understanding, we release TUMLU-mini and all corresponding evaluation scripts.', 'abstract_zh': '能够全面评估大规模多任务语言理解（MMLU）能力是推进多语言语言模型应用的重要前提。然而，准备高质量的母语基准往往成本高昂，因此限制了评估数据集的代表性。尽管近期努力构建更为包容的MMLU基准，但这些基准通常使用高资源语言的机器翻译构建，可能引入错误并未能充分考虑目标语言的语料和文化复杂性。在本文中，我们针对尤其是代表性不足的突厥语族语言缺乏母语MMLU基准的情况，提出两个突厥语族语言MMLU基准：TUMLU是一个全面的、多语言的、母语开发的语言理解基准，专门设计用于突厥语族语言，包含阿塞拜疆语、克里米亚塔塔尔语、卡拉卡尔帕克语、哈萨克语、塔塔尔语、土耳其语、维吾尔语和乌兹别克语的中学和高中水平问题，涵盖11个学术科目。我们还介绍了TUMLU-mini，这是数据集的一个更简洁、平衡且手工验证的子集。使用该数据集，我们系统地评估了多种开源和专有大规模多语言语言模型（LLMs），包括Claude、Gemini、GPT和LLaMA，提供了其在不同语言、科目和字母上的性能深入分析。为了促进多语言语言理解领域的进一步研究和发展，我们发布了TUMLU-mini及其相应的评估脚本。', 'title_zh': 'TUMLU：一个统一的土耳其语族语言理解基准'}
{'arxiv_id': 'arXiv:2502.11019', 'title': 'Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning', 'authors': 'Gangwei Jiang, Caigao Jiang, Zhaoyi Li, Siqiao Xue, Jun Zhou, Linqi Song, Defu Lian, Yin Wei', 'link': 'https://arxiv.org/abs/2502.11019', 'abstract': 'Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. Despite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The majority of existing research focuses on analyzing forgetting patterns through a singular training sequence, thereby overlooking the intricate effects that diverse tasks have on model behavior. Our study explores CF across various settings, discovering that model forgetting is influenced by both the specific training tasks and the models themselves. To this end, we interpret forgetting by examining the function vector (FV), a compact representation of functions in LLMs, offering a model-dependent indicator for the occurrence of CF. Through theoretical and empirical analyses, we demonstrated that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions. Leveraging these insights, we propose a novel function vector guided training methodology, incorporating a regularization technique to stabilize the FV and mitigate forgetting. Empirical tests on four benchmarks confirm the effectiveness of our proposed training method, substantiating our theoretical framework concerning CF and model function dynamics. We plan to make our code publicly accessible in the near future.', 'abstract_zh': '灾难性遗忘（CF）对机器学习构成了重大挑战，其中模型在学习新任务时会忘记先前学习的信息。尽管大型语言模型（LLMs）具备先进的功能，但在持续学习过程中仍然面临CF的挑战。现有大多数研究集中在通过单一训练序列分析遗忘模式，而忽视了不同类型任务对模型行为的复杂影响。我们的研究在各种情景下探索CF，发现模型遗忘受特定训练任务和模型本身的影响。为此，我们通过分析功能向量（FV），一种LLM中函数的紧凑表示，提供了一个基于模型的CF发生的指示器。通过理论和实证分析，我们证明了LLMs中的CF主要源于功能激活的偏差而非任务处理函数的覆盖。利用这些见解，我们提出了一种新颖的功能向量引导训练方法，结合正则化技术以稳定FV并减轻遗忘。对四个基准的实证测试验证了我们提出的训练方法的有效性，证实了我们关于CF和模型功能动力学的理论框架。我们计划在未来不久公开我们的代码。', 'title_zh': '解锁功能向量的 Powerful 应用以表征和缓解持续指令调谐中的灾难性遗忘'}
{'arxiv_id': 'arXiv:2502.11018', 'title': 'GRIFFIN: Effective Token Alignment for Faster Speculative Decoding', 'authors': 'Shijing Hu, Jingyang Li, Xingyu Xie, Zhihui Lu, Kim-Chuan Toh, Pan Zhou', 'link': 'https://arxiv.org/abs/2502.11018', 'abstract': "Speculative decoding accelerates inference in large language models (LLMs) by generating multiple draft tokens simultaneously. However, existing methods often struggle with token misalignment between the training and decoding phases, limiting their performance. To address this, we propose GRIFFIN, a novel framework that incorporates a token-alignable training strategy and a token-alignable draft model to mitigate misalignment. The training strategy employs a loss masking mechanism to exclude highly misaligned tokens during training, preventing them from negatively impacting the draft model's optimization. The token-alignable draft model introduces input tokens to correct inconsistencies in generated features. Experiments on LLaMA-series and Vicuna models demonstrate that GRIFFIN achieves an average acceptance length improvement of over 7\\% and a speedup ratio exceeding 8%, outperforming current SoTAs as shown in Fig. 1 (a) and (b).", 'abstract_zh': '推测解码通过同时生成多个草稿令牌加速大型语言模型的推理。然而，现有方法往往在训练和解码阶段之间存在令牌对齐问题，限制了其性能。为解决这一问题，我们提出了一种名为GRIFFIN的新框架，该框架结合了可对齐的训练策略和可对齐的草稿模型，以减轻对齐问题。训练策略采用损失屏蔽机制，在训练过程中排除高度对齐错误的令牌，防止它们负面影响草稿模型的优化。可对齐的草稿模型通过引入输入令牌来纠正生成特征的一致性问题。实验表明，GRIFFIN在LLaMA系列和Vicuna模型上实现了超过7%的平均接受长度改进和超过8%的加速比，优于当前的SOTA方法，如图1(a)和(b)所示。', 'title_zh': 'GRIFFIN: 有效的token对齐以实现更快的推测解码'}
{'arxiv_id': 'arXiv:2502.11006', 'title': 'Prompt Inject Detection with Generative Explanation as an Investigative Tool', 'authors': 'Jonathan Pan, Swee Liang Wong, Yidi Yuan, Xin Wei Chia', 'link': 'https://arxiv.org/abs/2502.11006', 'abstract': 'Large Language Models (LLMs) are vulnerable to adversarial prompt based injects. These injects could jailbreak or exploit vulnerabilities within these models with explicit prompt requests leading to undesired responses. In the context of investigating prompt injects, the challenge is the sheer volume of input prompts involved that are likely to be largely benign. This investigative challenge is further complicated by the semantics and subjectivity of the input prompts involved in the LLM conversation with its user and the context of the environment to which the conversation is being carried out. Hence, the challenge for AI security investigators would be two-fold. The first is to identify adversarial prompt injects and then to assess whether the input prompt is contextually benign or adversarial. For the first step, this could be done using existing AI security solutions like guardrails to detect and protect the LLMs. Guardrails have been developed using a variety of approaches. A popular approach is to use signature based. Another popular approach to develop AI models to classify such prompts include the use of NLP based models like a language model. However, in the context of conducting an AI security investigation of prompt injects, these guardrails lack the ability to aid investigators in triaging or assessing the identified input prompts. In this applied research exploration, we explore the use of a text generation capabilities of LLM to detect prompt injects and generate explanation for its detections to aid AI security investigators in assessing and triaging of such prompt inject detections. The practical benefit of such a tool is to ease the task of conducting investigation into prompt injects.', 'abstract_zh': '大型语言模型（LLMs）易受基于恶意提示的注入攻击。这些注入攻击可能使模型脱域或利用模型中的漏洞，导致不期望的响应。在调查提示注入时，挑战在于涉及的大量输入提示可能大多是 benign 的。这一调查挑战在涉及 LLM 与其用户之间的对话以及对话进行的环境上下文中，由于输入提示的语义和主观性而变得更加复杂。因此，AI 安全调查人员面临的挑战是双重的。首先，需要识别恶意提示注入；然后，评估输入提示是否处于上下文中为 benign 或恶意。在这一步骤中，现有的 AI 安全解决方案，如 guardrails，可以用来检测和保护 LLM。guardrails 的开发采用了多种方法。一种流行的方法是基于签名。另一种流行的方法是使用基于 NLP 的模型，如语言模型，来分类提示。然而，在进行 AI 安全调查以检测提示注入时，这些 guardrails 缺乏帮助调查人员初步评估或处理已识别输入提示的能力。在此应用研究探索中，我们研究了利用 LLM 的文本生成能力来检测提示注入并生成其检测的解释，以帮助 AI 安全调查人员评估和处理这类提示注入检测。此类工具的实际好处在于简化对提示注入的调查任务。', 'title_zh': '生成性解释作为调查工具的提示注入检测'}
{'arxiv_id': 'arXiv:2502.10976', 'title': 'QuOTE: Question-Oriented Text Embeddings', 'authors': 'Andrew Neeser, Kaylen Latimer, Aadyant Khatri, Chris Latimer, Naren Ramakrishnan', 'link': 'https://arxiv.org/abs/2502.10976', 'abstract': 'We present QuOTE (Question-Oriented Text Embeddings), a novel enhancement to retrieval-augmented generation (RAG) systems, aimed at improving document representation for accurate and nuanced retrieval. Unlike traditional RAG pipelines, which rely on embedding raw text chunks, QuOTE augments chunks with hypothetical questions that the chunk can potentially answer, enriching the representation space. This better aligns document embeddings with user query semantics, and helps address issues such as ambiguity and context-dependent relevance. Through extensive experiments across diverse benchmarks, we demonstrate that QuOTE significantly enhances retrieval accuracy, including in multi-hop question-answering tasks. Our findings highlight the versatility of question generation as a fundamental indexing strategy, opening new avenues for integrating question generation into retrieval-based AI pipelines.', 'abstract_zh': 'QuOTE（问题导向的文本嵌入）：检索增强生成系统的新型增强方法，旨在提高文档表示以实现精确和细腻的检索', 'title_zh': '面向问题的文本嵌入'}
{'arxiv_id': 'arXiv:2502.10966', 'title': 'Neural Networks Remember More: The Power of Parameter Isolation and Combination', 'authors': 'Biqing Zeng, Zehan Li, Aladdin Ayesh', 'link': 'https://arxiv.org/abs/2502.10966', 'abstract': "Catastrophic forgetting is a pervasive issue for pre-trained language models (PLMs) during continual learning, where models lose previously acquired knowledge when sequentially trained on a series of tasks. The model's ability to retain old tasks is referred to as stability, while its adaptability to new tasks is called plasticity. Therefore, the key to solving this problem is to find a trade-off between the plasticity and stability of the model. To address this issue, in this paper, we propose a novel method to achieve a balance between model stability and plasticity, thereby mitigating catastrophic forgetting. More specifically, our proposed approach leverages parameter isolation and a subsequent combination strategy. Initially, in the training stage, the model adapts to each downstream task via a parameter isolation method to prevent potential interference among different tasks. We then combine all trained parameters, which contain acquired knowledge, using the task arithmetic method and finally apply them to the backbone model. Empirical evaluations on continual language learning benchmarks substantiate the effectiveness of our approach, revealing a marked enhancement over existing state-of-the-art approaches.", 'abstract_zh': '持续学习中预训练语言模型的灾难性遗忘问题及其解决方案：通过参数隔离和组合策略实现模型稳定性和可塑性的平衡', 'title_zh': '神经网络记得更多：参数隔离与组合的力量'}
{'arxiv_id': 'arXiv:2502.10961', 'title': 'Graders should cheat: privileged information enables expert-level automated evaluations', 'authors': 'Jin Peng Zhou, Sébastien M. R. Arnold, Nan Ding, Kilian Q. Weinberger, Nan Hua, Fei Sha', 'link': 'https://arxiv.org/abs/2502.10961', 'abstract': "Auto-evaluating language models (LMs), i.e., using a grader LM to evaluate the candidate LM, is an appealing way to accelerate the evaluation process and the cost associated with it. But this presents a paradox: how can we trust the grader LM, which is presumably weaker than the candidate LM, to assess problems that are beyond the frontier of the capabilities of either model or both? For instance, today's LMs struggle on graduate-level physics and Olympiad-level math, making them unreliable graders in these domains.\nWe show that providing privileged information -- such as ground-truth solutions or problem-specific guidelines -- improves automated evaluations on such frontier problems. This approach offers two key advantages. First, it expands the range of problems where LMs graders apply. Specifically, weaker models can now rate the predictions of stronger models. Second, privileged information can be used to devise easier variations of challenging problems which improves the separability of different LMs on tasks where their performance is generally low. With this approach, general-purpose LM graders match the state of the art performance on RewardBench, surpassing almost all the specially-tuned models. LM graders also outperform individual human raters on Vibe-Eval, and approach human expert graders on Olympiad-level math problems.", 'abstract_zh': '自动评估语言模型（LMs），即使用一个评判LM来评估候选LM，是一种加快评估过程及其相关成本的方法。但这也 presents 了一个悖论：我们如何能信任一个本身就可能较弱的评判LM来评估超出模型能力范围的问题？例如，当前的LM在graduate-level物理和Olympiad-level数学方面表现挣扎，使它们在这两个领域不可靠的评判者。\n我们显示，提供特权信息——如ground-truth解决方案或问题特定指南——可以改善在这些前沿问题上的自动化评估。这种方法有两个关键优势。首先，它扩展了LM评判者可以应用的问题范围，尤其是较弱的模型现在可以评价较强模型的预测。其次，特权信息可以用来设计更具挑战性问题的简化版本，从而提高不同LM在它们表现普遍较低的任务上的可区分性。通过这种方法，通用LM评判者在RewardBench上达到了最先进的性能，超过了几乎所有专门调优的模型。LM评判者也在Vibe-Eval上优于单个的人类评判者，并且在Olympiad-level数学问题上接近人类专家评判者。', 'title_zh': '评分员应该作弊：特权信息使自动评价达到专家水平'}
{'arxiv_id': 'arXiv:2502.10953', 'title': 'Empirical evaluation of LLMs in predicting fixes of Configuration bugs in Smart Home System', 'authors': 'Sheikh Moonwara Anjum Monisha, Atul Bharadwaj', 'link': 'https://arxiv.org/abs/2502.10953', 'abstract': 'This empirical study evaluates the effectiveness of Large Language Models (LLMs) in predicting fixes for configuration bugs in smart home systems. The research analyzes three prominent LLMs - GPT-4, GPT-4o (GPT-4 Turbo), and Claude 3.5 Sonnet - using four distinct prompt designs to assess their ability to identify appropriate fix strategies and generate correct solutions. The study utilized a dataset of 129 debugging issues from the Home Assistant Community, focusing on 21 randomly selected cases for in-depth analysis. Results demonstrate that GPT-4 and Claude 3.5 Sonnet achieved 80\\% accuracy in strategy prediction when provided with both bug descriptions and original scripts. GPT-4 exhibited consistent performance across different prompt types, while GPT-4o showed advantages in speed and cost-effectiveness despite slightly lower accuracy. The findings reveal that prompt design significantly impacts model performance, with comprehensive prompts containing both description and original script yielding the best results. This research provides valuable insights for improving automated bug fixing in smart home system configurations and demonstrates the potential of LLMs in addressing configuration-related challenges.', 'abstract_zh': '本实证研究评估了大型语言模型（LLM）在预测智能家居系统配置错误修复方面的有效性。研究使用四种不同的提示设计对三种 promin 锐 著 LLM——GPT-4、GPT-4o（GPT-4 Turbo）和Claude 3.5 Sonnet——进行了分析，以评估它们识别适当修复策略和生成正确解决方案的能力。研究利用了来自Home Assistant Community的129个调试问题数据集，专注于21个随机选择的案例进行深入分析。结果表明，当提供错误描述和原始脚本时，GPT-4和Claude 3.5 Sonnet在策略预测方面的准确率达到80%。GPT-4在不同提示类型中表现出一致的性能，而GPT-4o在速度和成本效益方面表现出优势，尽管准确性略低。研究发现，提示设计显著影响模型性能，包含描述和原始脚本的全面提示表现最佳。本研究为提高智能家居系统配置的自动化修复提供了宝贵的见解，并展示了LLM在解决配置相关挑战方面的潜在价值。', 'title_zh': '基于配置错误修复预测的大型语言模型实证评估'}
{'arxiv_id': 'arXiv:2502.10940', 'title': 'CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation', 'authors': 'Ziyue Liu, Ruijie Zhang, Zhengyang Wang, Zi Yang, Paul Hovland, Bogdan Nicolae, Franck Cappello, Zheng Zhang', 'link': 'https://arxiv.org/abs/2502.10940', 'abstract': 'Large language models (LLMs) are revolutionizing many science and engineering fields. However, their huge model sizes impose extremely demanding needs of computational resources in the pre-training stage. Although low-rank factorizations can reduce model parameters, their direct application in LLM pre-training often lead to non-negligible performance loss. To address this fundamental challenge, we introduce CoLA and its memory-efficient implementation, CoLA-M. We leverage the low-rank structure observed widely in model activations, enforcing non-linear transformations between factorized weight matrices to reduce model size, boost model capacity and training efficiency. Experiments on LLaMA models with 60 million to 7 billion parameters show that CoLA reduces the computing cost by $\\bf 2\\pmb{\\times}$ and improves training throughput by $\\bf 1.86\\pmb{\\times}$ while maintaining full-rank level performance. CoLA-M further squeezes memory cost without sacrificing throughput, offering a pre-training approach with collectively superior parameter, computing, and memory efficiency. The LLMs produced are also $\\bf 2\\pmb{\\times}$ smaller, enabling faster inference with lower memory cost on resource-constrained platforms', 'abstract_zh': '大规模语言模型（LLMs）正在 revolutionizing 许多科学和工程领域。然而，其庞大的模型规模在预训练阶段对计算资源提出了极其严峻的需求。尽管低秩因子分解可以减少模型参数，但在LLM预训练中的直接应用往往会导致不可忽视的性能损失。为解决这一根本挑战，我们介绍了CoLA及其内存高效的实现CoLA-M。我们利用模型激活中广泛观察到的低秩结构，对因子化权重矩阵施加非线性变换，以减小模型规模、提升模型容量和训练效率。实验表明，CoLA在6000万至7亿参数的LLaMA模型上将计算成本降低至$\\bf 2\\pmb{\\times}$，训练吞吐量提升至$\\bf 1.86\\pmb{\\times}$，同时保持全秩水平的性能。CoLA-M进一步压缩了内存成本而不牺牲吞吐量，提供了一种在参数、计算和内存效率方面综合性能更优的预训练方法。生成的LLMs也减小了$\\bf 2\\pmb{\\times}$，使其在资源受限平台上实现更快推断并降低内存成本。', 'title_zh': 'CoLA：通过低秩激活实现的LLMs计算高效预训练'}
{'arxiv_id': 'arXiv:2502.10871', 'title': 'The Representation and Recall of Interwoven Structured Knowledge in LLMs: A Geometric and Layered Analysis', 'authors': 'Ge Lei, Samuel J. Cooper', 'link': 'https://arxiv.org/abs/2502.10871', 'abstract': 'This study investigates how large language models (LLMs) represent and recall multi-associated attributes across transformer layers. We show that intermediate layers encode factual knowledge by superimposing related attributes in overlapping spaces, along with effective recall even when attributes are not explicitly prompted. In contrast, later layers refine linguistic patterns and progressively separate attribute representations, optimizing task-specific outputs while appropriately narrowing attribute recall. We identify diverse encoding patterns including, for the first time, the observation of 3D spiral structures when exploring information related to the periodic table of elements. Our findings reveal a dynamic transition in attribute representations across layers, contributing to mechanistic interpretability and providing insights for understanding how LLMs handle complex, interrelated knowledge.', 'abstract_zh': '本研究探讨了大规模语言模型（LLMs）如何在变压器层间表示和回忆多关联属性。我们展示了中间层通过在重叠空间中叠加相关属性来编码事实知识，并在未明确提示属性的情况下也能有效回忆。相比之下，后期层逐步细化语言模式并分离属性表示，优化特定任务输出的同时适当地限制属性回忆。我们识别出多种编码模式，其中包括首次观察到与元素周期表相关信息时出现的三维螺旋结构。我们的发现揭示了层间属性表示的动态过渡，有助于机械可解释性的建立，并为理解LLMs处理复杂关联知识提供了见解。', 'title_zh': 'LLMs中交织结构知识的表示与回忆：一种几何与分层分析'}
{'arxiv_id': 'arXiv:2502.10852', 'title': 'Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages', 'authors': 'Zeli Su, Ziyin Zhang, Guixian Xu, Jianing Liu, XU Han, Ting Zhang, Yushuang Dong', 'link': 'https://arxiv.org/abs/2502.10852', 'abstract': 'While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.', 'abstract_zh': '针对极端低资源语言的多语言编码器适应框架：XLM-SWCM的研究', 'title_zh': '多语言编码器知悉的远不止你想象的：适用于极低资源语言的共享权重预训练'}
{'arxiv_id': 'arXiv:2502.10807', 'title': 'HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model', 'authors': 'Mingqian Ma, Guoqing Liu, Chuan Cao, Pan Deng, Tri Dao, Albert Gu, Peiran Jin, Zhao Yang, Yingce Xia, Renqian Luo, Pipi Hu, Zun Wang, Yuan-Jyue Chen, Haiguang Liu, Tao Qin', 'link': 'https://arxiv.org/abs/2502.10807', 'abstract': 'Advances in natural language processing and large language models have sparked growing interest in modeling DNA, often referred to as the "language of life". However, DNA modeling poses unique challenges. First, it requires the ability to process ultra-long DNA sequences while preserving single-nucleotide resolution, as individual nucleotides play a critical role in DNA function. Second, success in this domain requires excelling at both generative and understanding tasks: generative tasks hold potential for therapeutic and industrial applications, while understanding tasks provide crucial insights into biological mechanisms and diseases. To address these challenges, we propose HybriDNA, a decoder-only DNA language model that incorporates a hybrid Transformer-Mamba2 architecture, seamlessly integrating the strengths of attention mechanisms with selective state-space models. This hybrid design enables HybriDNA to efficiently process DNA sequences up to 131kb in length with single-nucleotide resolution. HybriDNA achieves state-of-the-art performance across 33 DNA understanding datasets curated from the BEND, GUE, and LRB benchmarks, and demonstrates exceptional capability in generating synthetic cis-regulatory elements (CREs) with desired properties. Furthermore, we show that HybriDNA adheres to expected scaling laws, with performance improving consistently as the model scales from 300M to 3B and 7B parameters. These findings underscore HybriDNA\'s versatility and its potential to advance DNA research and applications, paving the way for innovations in understanding and engineering the "language of life".', 'abstract_zh': '自然语言处理和大规模语言模型的进步引发了对DNA建模日益增长的兴趣，DNA常被称作“生命之语言”。然而，DNA建模面临着独特挑战。首先，它需要处理超长DNA序列同时保持单核苷酸分辨率，因为单个核苷酸在DNA功能中起着关键作用。其次，在这个领域取得成功要求在生成性和理解性任务上均表现出色：生成性任务在治疗和工业应用上具有潜力，而理解性任务则提供了关于生物学机制和疾病的关键见解。为了解决这些挑战，我们提出了一种仅解码器DNA语言模型HybriDNA，该模型采用混合Transformer-Mamba2架构，无缝结合了注意力机制的优势与选择性状态空间模型的优势。这种混合设计使HybriDNA能够高效处理长达131kb的DNA序列，保持单核苷酸分辨率。HybriDNA在来自BEND、GUE和LRB基准的33个DNA理解数据集中达到了最先进的性能，并在生成具有所需特性的合成顺式调控元件（CREs）方面展现了卓越能力。此外，我们表明HybriDNA遵循预期的缩放定律，模型参数从300M增至3B和7B时，性能持续提升。这些发现突显了HybriDNA的多功能性及其在推进DNA研究和应用方面的潜力，为其在理解与工程“生命之语言”方面的创新铺平了道路。', 'title_zh': 'HybriDNA: 一种混合Transformer-Mamba2长范围DNA语言模型'}
{'arxiv_id': 'arXiv:2502.10802', 'title': 'CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation', 'authors': 'Kefan Li, Hongyue Yu, Tingyu Guo, Shijie Cao, Yuan Yuan', 'link': 'https://arxiv.org/abs/2502.10802', 'abstract': 'Large Language Models (LLMs) have shown remarkable performance in automated code generation. However, existing approaches often rely heavily on pre-defined test cases, which become impractical in scenarios where such cases are unavailable. While prior works explore filtering techniques between programs and test cases, they overlook the refinement of test cases. To address this limitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that simultaneously evolves programs and test cases. CoCoEvo eliminates the dependency on pre-defined test cases by generating both programs and test cases directly from natural language problem descriptions and function headers. The framework employs specialized evolutionary operators, including LLM-based crossover and mutation operators for program evolution, along with a test case generation operator for test case evolution. Additionally, we propose optimization strategies such as a crossover rate scheduler to balance exploration and convergence, and a multi-objective optimization method for test case selection. Experimental results on multiple state-of-the-art LLMs demonstrate that CoCoEvo surpasses existing methods, achieving state-of-the-art performance in automated code generation and testing. These results underscore the potential of co-evolutionary techniques in advancing the field of automated programming.', 'abstract_zh': '大型语言模型（LLMs）在自动化代码生成方面展示了 remarkable 的性能。然而，现有方法往往高度依赖预定义的测试案例，在测试案例不可用的情况下变得不切实际。虽然早期研究探索了程序和测试案例之间的过滤技术，但忽略了测试案例的细化。为解决这一限制，我们引入了 CoCoEvo，这是一种新颖的基于 LLM 的协同进化框架，可以同时进化程序和测试案例。CoCoEvo 通过直接从自然语言问题描述和函数头生成程序和测试案例来消除对预定义测试案例的依赖。该框架采用专门的进化操作符，包括基于 LLM 的交叉和变异操作符用于程序进化，以及用于测试案例进化的测试案例生成操作符。此外，我们还提出了一些优化策略，如交叉率调度器以平衡探索与收敛，以及多目标优化方法用于测试案例选择。实验结果表明，CoCoEvo 在多个最先进的 LLM 上超越了现有方法，在自动化代码生成和测试方面达到了最先进的性能。这些结果突显了协同进化技术在推进自动化编程领域方面的潜力。', 'title_zh': 'CoCoEvo: 程序与测试用例的协同进化以增强代码生成'}
{'arxiv_id': 'arXiv:2502.10768', 'title': 'Evaluating improvements on using Large Language Models (LLMs) for property extraction in the Open Research Knowledge Graph (ORKG)', 'authors': 'Sandra Schaftner', 'link': 'https://arxiv.org/abs/2502.10768', 'abstract': "Current research highlights the great potential of Large Language Models (LLMs) for constructing Scholarly Knowledge Graphs (SKGs). One particularly complex step in this process is relation extraction, aimed at identifying suitable properties to describe the content of research. This study builds directly on previous research of three Open Research Knowledge Graph (ORKG) team members who assessed the readiness of LLMs such as GPT-3.5, Llama 2, and Mistral for property extraction in scientific literature. Given the moderate performance observed, the previous work concluded that fine-tuning is needed to improve these models' alignment with scientific tasks and their emulation of human expertise. Expanding on this prior experiment, this study evaluates the impact of advanced prompt engineering techniques and demonstrates that these techniques can highly significantly enhance the results. Additionally, this study extends the property extraction process to include property matching to existing ORKG properties, which are retrieved via the API. The evaluation reveals that results generated through advanced prompt engineering achieve a higher proportion of matches with ORKG properties, further emphasizing the enhanced alignment achieved. Moreover, this lays the groundwork for addressing challenges such as the inconsistency of ORKG properties, an issue highlighted in prior studies. By assigning unique URIs and using standardized terminology, this work increases the consistency of the properties, fulfilling a crucial aspect of Linked Data and FAIR principles - core commitments of ORKG. This, in turn, significantly enhances the applicability of ORKG content for subsequent tasks such as comparisons of research publications. Finally, the study concludes with recommendations for future improvements in the overall property extraction process.", 'abstract_zh': '当前研究突显了大型语言模型（LLMs）在构建学术知识图谱（SKGs）方面的巨大潜力。这一过程中特别复杂的一个步骤是关系提取，旨在识别适合描述研究内容的属性。本研究直接建立在三位开放研究知识图谱（ORKG）团队成员之前的研究基础上，他们评估了如GPT-3.5、Llama 2和Mistral等LLMs在科学文献中属性提取方面的准备情况。鉴于观察到的中等性能，之前的研究得出结论认为，需要对这些模型进行微调以提高它们与科学任务的对齐以及模拟人类专业知识的能力。在此前实验的基础上，本研究评估了高级提示工程技术的影响，并证明了这些技术可以显著提高结果。此外，本研究将属性提取过程扩展到属性匹配现有的ORKG属性，这些属性通过API检索。评估结果显示，通过高级提示工程生成的结果与ORKG属性的匹配比例更高，进一步突显了所实现的对齐程度的增强。此外，这为应对ORKG属性的一致性问题奠定了基础，这是之前研究中指出的一个问题。通过分配唯一的URI并使用标准化术语，本研究增加了属性的一致性，实现了链接数据和FAIR原则的核心承诺之一——ORKG内容后续任务如研究出版物的比较适用性的显著增强。最后，本研究提出了总体属性提取流程改进的建议。', 'title_zh': '评估在开放研究知识图谱（ORKG）中使用大型语言模型（LLMs）进行属性提取的改进效果'}
{'arxiv_id': 'arXiv:2502.10749', 'title': 'LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging', 'authors': 'Zehua Liu, Han Wu, Yuxuan Yao, Ruifeng She, Xiongwei Han, Tao Zhong, Mingxuan Yuan', 'link': 'https://arxiv.org/abs/2502.10749', 'abstract': 'While most current approaches rely on further training techniques, such as fine-tuning or reinforcement learning, to enhance model capacities, model merging stands out for its ability of improving models without requiring any additional training. In this paper, we propose a unified framework for model merging based on low-rank estimation of task vectors without the need for access to the base model, named \\textsc{LoRE-Merging}. Our approach is motivated by the observation that task vectors from fine-tuned models frequently exhibit a limited number of dominant singular values, making low-rank estimations less prone to interference. We implement the method by formulating the merging problem as an optimization problem. Extensive empirical experiments demonstrate the effectiveness of our framework in mitigating interference and preserving task-specific information, thereby advancing the state-of-the-art performance in model merging techniques.', 'abstract_zh': '基于低秩估计的任务向量融合：无需访问基模型的统一框架', 'title_zh': 'LoRE-合并：探索大型语言模型合并的低秩估计方法'}
{'arxiv_id': 'arXiv:2502.10732', 'title': 'Rule-Bottleneck Reinforcement Learning: Joint Explanation and Decision Optimization for Resource Allocation with Language Agents', 'authors': 'Mauricio Tec, Guojun Xiong, Haichuan Wang, Francesca Dominici, Milind Tambe', 'link': 'https://arxiv.org/abs/2502.10732', 'abstract': "Deep Reinforcement Learning (RL) is remarkably effective in addressing sequential resource allocation problems in domains such as healthcare, public policy, and resource management. However, deep RL policies often lack transparency and adaptability, challenging their deployment alongside human decision-makers. In contrast, Language Agents, powered by large language models (LLMs), provide human-understandable reasoning but may struggle with effective decision making. To bridge this gap, we propose Rule-Bottleneck Reinforcement Learning (RBRL), a novel framework that jointly optimizes decision and explanations. At each step, RBRL generates candidate rules with an LLM, selects among them using an attention-based RL policy, and determines the environment action with an explanation via chain-of-thought reasoning. The RL rule selection is optimized using the environment rewards and an explainability metric judged by the LLM. Evaluations in real-world scenarios highlight RBRL's competitive performance with deep RL and efficiency gains over LLM fine-tuning. A survey further confirms the enhanced quality of its explanations.", 'abstract_zh': '基于规则瓶颈的强化学习（RBRL）：决策与解释的联合优化', 'title_zh': '规则瓶颈强化学习：语言代理参与的资源分配解释与决策优化'}
{'arxiv_id': 'arXiv:2502.10709', 'title': 'An Empirical Analysis of Uncertainty in Large Language Model Evaluations', 'authors': 'Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang', 'link': 'https://arxiv.org/abs/2502.10709', 'abstract': "As LLM-as-a-Judge emerges as a new paradigm for assessing large language models (LLMs), concerns have been raised regarding the alignment, bias, and stability of LLM evaluators. While substantial work has focused on alignment and bias, little research has concentrated on the stability of LLM evaluators. In this paper, we conduct extensive experiments involving 9 widely used LLM evaluators across 2 different evaluation settings to investigate the uncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators exhibit varying uncertainty based on model families and sizes. With careful comparative analyses, we find that employing special prompting strategies, whether during inference or post-training, can alleviate evaluation uncertainty to some extent. By utilizing uncertainty to enhance LLM's reliability and detection capability in Out-Of-Distribution (OOD) data, we further fine-tune an uncertainty-aware LLM evaluator named ConfiLM using a human-annotated fine-tuning set and assess ConfiLM's OOD evaluation ability on a manually designed test set sourced from the 2024 Olympics. Experimental results demonstrate that incorporating uncertainty as additional information during the fine-tuning phase can largely improve the model's evaluation performance in OOD scenarios. The code and data are released at: this https URL.", 'abstract_zh': '作为LLM-as-a-Judge新兴范式用于评估大型语言模型（LLMs），引发了对其一致性和鲁棒性的关注。虽然在一致性和偏见方面已经开展了大量研究，但关于LLM评估器的稳定性研究相对较少。本文通过涉及9个常用LLM评估器的大量实验，探讨了基于模型的LLM评估中的不确定性。我们发现LLM评估器根据模型家族和规模表现出不同的不确定性。通过细致的对比分析，我们发现，在推断或后训练阶段采用特殊的提示策略，可以在一定程度上缓解评估不确定性。通过利用不确定性来增强大型语言模型在分布外（OOD）数据中可靠性和检测能力，我们进一步使用人工标注的微调集fine-tune了一个意识不确定性（Uncertainty-Aware）的LLM评估器ConfiLM，并在2024年奥运会手动设计的测试集上评估了ConfiLM的OOD评估能力。实验结果表明，在fine-tuning阶段引入不确定性作为额外信息可以显著改善模型在OOD场景下的评估性能。代码和数据已发布于：this https URL。', 'title_zh': '大型语言模型评估中不确定性的一种 empirical 分析'}
{'arxiv_id': 'arXiv:2502.10699', 'title': 'Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration', 'authors': 'George Applegarth, Christian Weatherstone, Maximilian Hollingsworth, Henry Middlebrook, Marcus Irvin', 'link': 'https://arxiv.org/abs/2502.10699', 'abstract': 'Contextual memory integration remains a high challenge in the development of language models, particularly in tasks that require maintaining coherence over extended sequences. Traditional approaches, such as self-attention mechanisms and memory-augmented architectures, often prioritize short-term dependencies, leading to fragmentation and inconsistency in long-range contextual understanding. Inspired by principles of synaptic plasticity observed in biological neural systems, a novel mechanism, Synaptic Resonance, is introduced to dynamically reinforce relevant memory pathways during training and inference. Unlike static memory representations, this mechanism continuously adjusts synaptic weight matrices based on contextual relevance, allowing for improved information retention without excessive computational overhead. Evaluations conducted on an open-source language model demonstrate reductions in perplexity, enhancements in contextual coherence, and increased robustness against input noise, highlighting the effectiveness of reinforcement-driven memory modulation. Comparative analysis against baseline models further reveals that the proposed approach achieves higher memory retention efficiency while maintaining computational feasibility. The architectural modifications integrate seamlessly into existing transformer-based frameworks, ensuring stable convergence and efficient inference without sacrificing scalability. Applications benefiting from improved long-term contextual consistency, such as dialogue systems and document summarization, stand to gain from this approach. Empirical findings suggest that dynamically reinforced memory pathways offer a promising alternative to conventional memory mechanisms, addressing longstanding limitations in extended sequence modeling.', 'abstract_zh': 'Contextual记忆整合仍然是语言模型发展中的一项高挑战，特别是在需要在长序列上保持连贯性的任务中。传统的approaches，如自我注意力机制和记忆增强架构，往往优先处理短时依赖性，导致长距离上下文理解上的分割和不一致性。受生物神经系统中突触可塑性原理的启发，提出了一种新的机制——突触共振，该机制在训练和推理过程中动态强化相关记忆路径。与静态记忆表示不同，这种机制根据上下文相关性连续调整突触权重矩阵，从而在不增加过多计算开销的情况下提高信息保留能力。在开源语言模型上的评估显示，在困惑度、上下文连贯性以及对输入噪声的鲁棒性方面都有改进，突显了强化驱动的记忆调制的有效性。与基线模型的对比分析进一步表明，所提出的方法在记忆保留效率方面表现更优，同时保持了计算上的可行性。该架构修改无缝集成到现有的transformer基架构中，确保了稳定收敛和高效推理，无需牺牲可扩展性。受益于改进的长时上下文一致性的应用，如对话系统和文档摘要，可以从这种方法中获益。实证研究结果表明，动态强化的记忆路径为解决长序列建模中的长期局限问题提供了有前景的替代方案。', 'title_zh': '探索大型语言模型中的突触共振：一种全新的上下文记忆整合方法'}
{'arxiv_id': 'arXiv:2502.10631', 'title': 'ControllableGPT: A Ground-Up Designed Controllable GPT for Molecule Optimization', 'authors': 'Xuefeng Liu, Songhao Jiang, Bo Li, Rick Stevens', 'link': 'https://arxiv.org/abs/2502.10631', 'abstract': 'Large Language Models (LLMs) employ three popular training approaches: Masked Language Models (MLM), Causal Language Models (CLM), and Sequence-to-Sequence Models (seq2seq). However, each approach has its strengths and limitations, and faces challenges in addressing specific tasks that require controllable and bidirectional generation, such as drug optimization. To address this challenge, inspired by the biological processes of growth and evolution, which involve the expansion, shrinking, and mutation of sequences, we introduce ControllableGPT. This initiative represents the first effort to combine the advantages of MLM, CLM, and seq2seq into a single unified, controllable GPT framework. It enables the precise management of specific locations and ranges within a sequence, allowing for expansion, reduction, or mutation over chosen or random lengths, while maintaining the integrity of any specified positions or subsequences. In this work, we designed ControllableGPT for drug optimization from the ground up, which included proposing the Causally Masked Seq2seq (CMS) objective, developing the training corpus, introducing a novel pre-training approach, and devising a unique generation process. We demonstrate the effectiveness and controllability of ControllableGPT by conducting experiments on drug optimization tasks for both viral and cancer benchmarks, surpassing competing baselines.', 'abstract_zh': '大型语言模型（LLMs）采用三种流行的训练方法：掩码语言模型（MLM）、因导语言模型（CLM）和序列到序列模型（seq2seq）。然而，每种方法都有其优势和局限性，面对需要可控和双向生成的任务时面临挑战，如药物优化。为解决这一挑战，受生物生长和进化过程的启发，涉及序列的扩展、收缩和变异，我们提出了ControllableGPT。这一举措代表了将MLM、CLM和seq2seq的优势结合到一个统一的可控GPT框架中的首次尝试。它允许精确管理序列中的特定位置和范围，允许在选定的或随机长度上进行扩展、收缩或变异，同时保持任何指定位置或子序列的完整性。在这项工作中，我们从头开始为药物优化设计了ControllableGPT，包括提出因果掩码序列到序列（CMS）目标、开发训练语料库、引入新的预训练方法以及设计独特的生成过程。我们通过在病毒和癌症基准测试中的药物优化任务实验展示了ControllableGPT的有效性和可控性，超过了竞争对手的基础模型。', 'title_zh': '可控GPT：一种从底层设计的可控GPT分子优化模型'}
{'arxiv_id': 'arXiv:2502.10626', 'title': 'K-Edit: Language Model Editing with Contextual Knowledge Awareness', 'authors': 'Elan Markowitz, Anil Ramakrishna, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, Aram Galstyan', 'link': 'https://arxiv.org/abs/2502.10626', 'abstract': 'As the world changes, we need to be able to update our models and correct false information without costly retraining. Knowledge-based model editing enables precise modifications to the weights of large language models in order to modify the information encoded within. Recent approaches have seen success in enabling recall of edited information for thousands of edits at once. However, these approaches fail to produce edits that account for associated contextual information. We present K-Edit, an effective approach to generating contextually consistent knowledge edits. By using knowledge graphs, which maintain contextual consistency when an edge is edited, we are able to generate additional \\textit{contextual edits} that ensure consistency of related information in the language model. Our experiments demonstrate significant improvements in multi-hop question answering while maintaining the general effectiveness and scalability of model edits.', 'abstract_zh': '基于知识的模型编辑使得我们能够在不需要昂贵重新训练的情况下更新模型并修正错误信息，从而实现对大规模语言模型权重的精确调整，以修改编码的信息。最近的方法在一次性编辑数千次信息时能够启用编辑信息的回忆。然而，这些方法未能生成考虑到相关上下文信息的编辑。我们提出了K-Edit，一种生成上下文一致的知识编辑的有效方法。通过使用保持边编辑时上下文一致性的知识图谱，我们能够生成额外的上下文编辑，确保语言模型中相关信息的一致性。我们的实验显示，在多跳问答方面取得了显著改进，同时保持了模型编辑的一般有效性和可扩展性。', 'title_zh': 'K-Edit：具有上下文知识awareness的语言模型编辑'}
{'arxiv_id': 'arXiv:2502.10596', 'title': 'Post-training an LLM for RAG? Train on Self-Generated Demonstrations', 'authors': 'Matthew Finlayson, Ilia Kulikov, Daneil M. Bikel, Barlas Oguz, Xilun Chen, Aasish Pappu', 'link': 'https://arxiv.org/abs/2502.10596', 'abstract': 'Large language models (LLMs) often struggle with knowledge intensive NLP tasks, such as answering "Who won the latest World Cup?" because the knowledge they learn during training may be insufficient or outdated. Conditioning generation on retrieved documents -- a technique known as retrieval augmented generation (RAG) -- mitigates these shortcomings by allowing the model to leverage in-context information. Practitioners can improve LLM RAG performance by fine-tuning on retrieval-augmented instructions, but must beware that this can cause undesirable model behaviors like hallucinations. We attribute this degradation to the fact that the training data is likely to be out-of-distribution for the model and may suffer from quality issues, such as misalignment between retrievals and target responses (since retrievals are frequently added post-hoc). We propose a recipe for training RAG-enabled LLMs using self-generated demonstrations, thereby avoiding training on out-of-distribution text and integrating retrievals into the LLM responses. We evaluate our method on knowledge intensive question answering (QA) tasks and show that our method teaches LLMs to properly handle in-context retrievals and abstain from questions it will likely get wrong. Compared to conventional RA-IT methods, our method prevents model degradation in non-RAG settings while exhibiting superior QA performance.', 'abstract_zh': '基于检索增强生成的大语言模型在知识密集型NLP任务中的训练方法研究', 'title_zh': '针对RAG的后训练LLM？在自动生成的示范上训练'}
{'arxiv_id': 'arXiv:2502.10581', 'title': 'Do We Need to Verify Step by Step? Rethinking Process Supervision from a Theoretical Perspective', 'authors': 'Zeyu Jia, Alexander Rakhlin, Tengyang Xie', 'link': 'https://arxiv.org/abs/2502.10581', 'abstract': "As large language models have evolved, it has become crucial to distinguish between process supervision and outcome supervision -- two key reinforcement learning approaches to complex reasoning tasks. While process supervision offers intuitive advantages for long-term credit assignment, the precise relationship between these paradigms has remained an open question. Conventional wisdom suggests that outcome supervision is fundamentally more challenging due to the trajectory-level coverage problem, leading to significant investment in collecting fine-grained process supervision data.\nIn this paper, we take steps towards resolving this debate. Our main theorem shows that, under standard data coverage assumptions, reinforcement learning through outcome supervision is no more statistically difficult than through process supervision, up to polynomial factors in horizon. At the core of this result lies the novel Change of Trajectory Measure Lemma -- a technical tool that bridges return-based trajectory measure and step-level distribution shift. Furthermore, for settings with access to a verifier or a rollout capability, we prove that any policy's advantage function can serve as an optimal process reward model, providing a direct connection between outcome and process supervision. These findings suggest that the empirically observed performance gap -- if any -- between outcome and process supervision likely stems from algorithmic limitations rather than inherent statistical difficulties, potentially transforming how we approach data collection and algorithm design for reinforcement learning.", 'abstract_zh': '随着大型语言模型的发展，区分过程监督与结果监督——两种关键的强化学习方法以应对复杂推理任务——变得至关重要。虽然过程监督在长期信用分配方面提供了直观的优势，但这些范式的精确关系仍是一个开放的问题。传统观点认为，结果监督由于轨迹级覆盖问题而从根本上更加具有挑战性，导致人们对精细过程监督数据的投资显著增加。\n\n在本文中，我们朝着解决这一争论迈出了步伐。我们的主要定理表明，在标准数据覆盖假设下，通过结果监督进行强化学习在统计上与通过过程监督进行的强化学习相比，最多只是在计算复杂性上存在多项式级别的差异。这一结果的核心是新颖的轨迹测度变换引理——一个技术工具，它将基于回报的轨迹测度与步骤级分布变化连接起来。此外，在可以访问验证器或展开能力的情况下，我们证明任何策略的优势函数都可以作为最优过程奖励模型，从而直接连接结果监督与过程监督。这些发现表明，如果存在，结果监督与过程监督之间观察到的性能差距可能源于算法限制而非固有的统计困难，这可能会改变我们对强化学习中数据收集和算法设计方法的思路。', 'title_zh': '我们是否需要逐步骤验证？从理论视角重新思考过程监督'}
{'arxiv_id': 'arXiv:2502.10577', 'title': "Man Made Language Models? Evaluating LLMs' Perpetuation of Masculine Generics Bias", 'authors': 'Enzo Doyen, Amalia Todirascu', 'link': 'https://arxiv.org/abs/2502.10577', 'abstract': 'Large language models (LLMs) have been shown to propagate and even amplify gender bias, in English and other languages, in specific or constrained contexts. However, no studies so far have focused on gender biases conveyed by LLMs\' responses to generic instructions, especially with regard to masculine generics (MG). MG are a linguistic feature found in many gender-marked languages, denoting the use of the masculine gender as a "default" or supposedly neutral gender to refer to mixed group of men and women, or of a person whose gender is irrelevant or unknown. Numerous psycholinguistics studies have shown that MG are not neutral and induce gender bias. This work aims to analyze the use of MG by both proprietary and local LLMs in responses to generic instructions and evaluate their MG bias rate. We focus on French and create a human noun database from existing lexical resources. We filter existing French instruction datasets to retrieve generic instructions and analyze the responses of 6 different LLMs. Overall, we find that $\\approx$39.5\\% of LLMs\' responses to generic instructions are MG-biased ($\\approx$73.1\\% across responses with human nouns). Our findings also reveal that LLMs are reluctant to using gender-fair language spontaneously.', 'abstract_zh': '大型语言模型（LLMs）在回应通用指令时传递性别偏见的研究：以法语为例', 'title_zh': '人造语言模型？评价大语言模型延续男性代称偏见的情况'}
{'arxiv_id': 'arXiv:2502.10517', 'title': 'KernelBench: Can LLMs Write Efficient GPU Kernels?', 'authors': 'Anne Ouyang, Simon Guo, Simran Arora, Alex L. Zhang, William Hu, Christopher Ré, Azalia Mirhoseini', 'link': 'https://arxiv.org/abs/2502.10517', 'abstract': "Efficient GPU kernels are crucial for building performant machine learning architectures, but writing them is a time-consuming challenge that requires significant expertise; therefore, we explore using language models (LMs) to automate kernel generation. We introduce KernelBench, an open-source framework for evaluating LMs' ability to write fast and correct kernels on a suite of 250 carefully selected PyTorch ML workloads. KernelBench represents a real-world engineering environment and making progress on the introduced benchmark directly translates to faster practical kernels. We introduce a new evaluation metric fast_p, which measures the percentage of generated kernels that are functionally correct and offer a speedup greater than an adjustable threshold p over baseline. Our experiments across various state-of-the-art models and test-time methods show that frontier reasoning models perform the best out of the box but still fall short overall, matching the PyTorch baseline in less than 20% of the cases. While we show that results can improve by leveraging execution and profiling feedback during iterative refinement, KernelBench remains a challenging benchmark, with its difficulty increasing as we raise speedup threshold p.", 'abstract_zh': '高效的GPU内核对于构建高性能机器学习架构至关重要，但编写它们是一项耗时的挑战，需要大量的专业知识；因此，我们探索使用语言模型（LMs）来自动化内核生成。我们引入了KernelBench，这是一个开源框架，用于评估LMs在一系列250个精心挑选的PyTorch机器学习工作负载上编写快速且正确的内核的能力。KernelBench代表了一个真实的世界工程环境，通过改进引入的基准测试，可以直接加快实际内核的速度。我们引入了一个新的评估指标fast_p，该指标衡量生成内核中功能正确且比基线快于可调节阈值p以上的百分比。在各种最先进的模型和测试时方法的实验中显示，前沿推理模型开箱即用表现最佳，但仍总体上表现不佳，在不到20%的情况下与PyTorch基线相当。虽然通过在迭代优化过程中利用执行和剖析反馈可以提升结果，但随着提高速度阈值p，KernelBench仍是一个具有挑战性的基准。', 'title_zh': 'KernelBench: 能否生成高效的GPU内核代码？'}
{'arxiv_id': 'arXiv:2502.10497', 'title': 'Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA', 'authors': 'Mohammad Baqar, Rajat Khanda', 'link': 'https://arxiv.org/abs/2502.10497', 'abstract': "Recent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance.\nThis paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications.\nFurthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.", 'abstract_zh': '最近生成式人工智能的发展显著提升了自然语言处理（NLP）系统的效率和适应性，特别是在检索增强生成（RAG）、低秩适应（LoRA）和分解低秩适应（DoRA）方面。RAG通过整合外部知识来增强生成输出的事实一致性，而LoRA使大规模语言模型（LLMs）的参数高效微调成为可能。DoRA则通过自适应参数排名和领域感知的权重调整进一步优化微调过程，提高了学习效率，同时保持推理性能。本文通过大规模实证评估了RAG、LoRA和DoRA，评估了20,000个基于FAQ的查询的模型微调和生成性能，知识库包含400,000条记录。研究分析了关键性能指标，如准确性、相关性和推理延迟。实验结果表明，DoRA在准确性（90.1%）、相关性得分（0.88）和延迟（每查询110毫秒）方面表现最佳，超越了LoRA和RAG在实际领域的应用。此外，本文还探讨了不同模型在微调效率、计算成本和实时适应性之间的权衡。研究发现强调了RAG在知识接地方面的有效性、LoRA在成本效益领域的适应性以及DoRA在平衡微调效率和模型精度方面的能力。这些见解为在准确性关键领域，如医疗、金融和法律服务中部署以人工智能驱动的生成系统提供了实用指导，确保在动态环境中实现可扩展性、可靠性和最佳性能。', 'title_zh': '幻觉与现实：RAG、LoRA和DoRA的综合准确度评估'}
{'arxiv_id': 'arXiv:2502.10487', 'title': 'Fast Proxies for LLM Robustness Evaluation', 'authors': 'Tim Beyer, Jan Schuchardt, Leo Schwinn, Stephan Günnemann', 'link': 'https://arxiv.org/abs/2502.10487', 'abstract': "Evaluating the robustness of LLMs to adversarial attacks is crucial for safe deployment, yet current red-teaming methods are often prohibitively expensive. We compare the ability of fast proxy metrics to predict the real-world robustness of an LLM against a simulated attacker ensemble. This allows us to estimate a model's robustness to computationally expensive attacks without requiring runs of the attacks themselves. Specifically, we consider gradient-descent-based embedding-space attacks, prefilling attacks, and direct prompting. Even though direct prompting in particular does not achieve high ASR, we find that it and embedding-space attacks can predict attack success rates well, achieving $r_p=0.87$ (linear) and $r_s=0.94$ (Spearman rank) correlations with the full attack ensemble while reducing computational cost by three orders of magnitude.", 'abstract_zh': '评估大规模语言模型对 adversarial 攻击的鲁棒性对于安全部署至关重要，但当前的红队方法往往代价高昂。我们将快速代理指标的能力与模拟攻击集合的实际鲁棒性进行比较，从而在不需要运行实际攻击的情况下，估计模型对计算成本高昂的攻击的鲁棒性。具体而言，我们考虑基于梯度下降的嵌入空间攻击、预填攻击和直接提示。尽管直接提示特别不能实现高的成功率，但我们发现它和嵌入空间攻击能够很好地预测攻击成功率，与完整攻击集合相比，实现 $r_p=0.87$（线性）和 $r_s=0.94$（斯皮尔曼秩）的相关性，同时将计算成本降低三个数量级。', 'title_zh': '快速代理用于LLM鲁棒性评估'}
{'arxiv_id': 'arXiv:2502.10486', 'title': 'VLM-Guard: Safeguarding Vision-Language Models via Fulfilling Safety Alignment Gap', 'authors': 'Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen', 'link': 'https://arxiv.org/abs/2502.10486', 'abstract': 'The emergence of vision language models (VLMs) comes with increased safety concerns, as the incorporation of multiple modalities heightens vulnerability to attacks. Although VLMs can be built upon LLMs that have textual safety alignment, it is easily undermined when the vision modality is integrated. We attribute this safety challenge to the modality gap, a separation of image and text in the shared representation space, which blurs the distinction between harmful and harmless queries that is evident in LLMs but weakened in VLMs. To avoid safety decay and fulfill the safety alignment gap, we propose VLM-Guard, an inference-time intervention strategy that leverages the LLM component of a VLM as supervision for the safety alignment of the VLM. VLM-Guard projects the representations of VLM into the subspace that is orthogonal to the safety steering direction that is extracted from the safety-aligned LLM. Experimental results on three malicious instruction settings show the effectiveness of VLM-Guard in safeguarding VLM and fulfilling the safety alignment gap between VLM and its LLM component.', 'abstract_zh': '视觉语言模型（VLMs）的出现伴随着安全问题的增加，因为多模态的整合使得模型更容易受到攻击。尽管VLMs可以基于具有文本安全对齐的大型语言模型（LLMs）构建，但当引入视觉模态时，其安全性很容易受到影响。我们将这一安全挑战归因于模态差距，即在共享表示空间中图像与文本的分离，这模糊了在LLMs中明显存在的有害与无害查询之间的区别，在VLMs中则减弱了这种区别。为了防止安全性衰退并弥补安全对齐差距，我们提出了一种VLM-Guard，它是一种推理时的干预策略，利用VLM中的LLM组件作为监督，以实现VLM的安全对齐。VLM-Guard将VLM的表示投影到从安全对齐的LLM中提取的安全导向的正交子空间中。在三个恶意指令设置上的实验结果表明，VLM-Guard在保护VLM并弥补VLM与其LLM组件之间的安全对齐差距方面是有效的。', 'title_zh': 'VLM-Guard: 通过弥补安全对齐缺口来保障视觉-语言模型的安全性'}
{'arxiv_id': 'arXiv:2502.10467', 'title': 'YNote: A Novel Music Notation for Fine-Tuning LLMs in Music Generation', 'authors': 'Shao-Chien Lu, Chen-Chen Yeh, Hui-Lin Cho, Chun-Chieh Hsu, Tsai-Ling Hsu, Cheng-Han Wu, Timothy K. Shih, Yu-Cheng Lin', 'link': 'https://arxiv.org/abs/2502.10467', 'abstract': "The field of music generation using Large Language Models (LLMs) is evolving rapidly, yet existing music notation systems, such as MIDI, ABC Notation, and MusicXML, remain too complex for effective fine-tuning of LLMs. These formats are difficult for both machines and humans to interpret due to their variability and intricate structure. To address these challenges, we introduce YNote, a simplified music notation system that uses only four characters to represent a note and its pitch. YNote's fixed format ensures consistency, making it easy to read and more suitable for fine-tuning LLMs. In our experiments, we fine-tuned GPT-2 (124M) on a YNote-encoded dataset and achieved BLEU and ROUGE scores of 0.883 and 0.766, respectively. With just two notes as prompts, the model was able to generate coherent and stylistically relevant music. We believe YNote offers a practical alternative to existing music notations for machine learning applications and has the potential to significantly enhance the quality of music generation using LLMs.", 'abstract_zh': '使用大型语言模型生成音乐领域的简化音乐记谱系统：YNote的研究', 'title_zh': 'YNote：一种用于音乐生成中LLMs微调的新乐谱表示方法'}
{'arxiv_id': 'arXiv:2502.10459', 'title': 'LLM4GNAS: A Large Language Model Based Toolkit for Graph Neural Architecture Search', 'authors': 'Yang Gao, Hong Yang, Yizhi Chen, Junxian Wu, Peng Zhang, Haishuai Wang', 'link': 'https://arxiv.org/abs/2502.10459', 'abstract': 'Graph Neural Architecture Search (GNAS) facilitates the automatic design of Graph Neural Networks (GNNs) tailored to specific downstream graph learning tasks. However, existing GNAS approaches often require manual adaptation to new graph search spaces, necessitating substantial code optimization and domain-specific knowledge. To address this challenge, we present LLM4GNAS, a toolkit for GNAS that leverages the generative capabilities of Large Language Models (LLMs). LLM4GNAS includes an algorithm library for graph neural architecture search algorithms based on LLMs, enabling the adaptation of GNAS methods to new search spaces through the modification of LLM prompts. This approach reduces the need for manual intervention in algorithm adaptation and code modification. The LLM4GNAS toolkit is extensible and robust, incorporating LLM-enhanced graph feature engineering, LLM-enhanced graph neural architecture search, and LLM-enhanced hyperparameter optimization. Experimental results indicate that LLM4GNAS outperforms existing GNAS methods on tasks involving both homogeneous and heterogeneous graphs.', 'abstract_zh': 'LLM4GNAS：一种利用大型语言模型的生成能力的图神经网络架构搜索工具包', 'title_zh': 'LLM4GNAS：基于大规模语言模型的图神经网络架构搜索工具包'}
{'arxiv_id': 'arXiv:2502.10454', 'title': 'One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs', 'authors': 'Yinghui Li, Jiayi Kuang, Haojing Huang, Zhikun Xu, Xinnian Liang, Yi Yu, Wenlian Lu, Yangning Li, Xiaoyu Tan, Chao Qu, Ying Shen, Hai-Tao Zheng, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.10454', 'abstract': 'Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of "proof by counterexamples" commonly used in human mathematics education, our work aims to enhance LLMs\' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, CounterMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that CounterMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs\' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.', 'abstract_zh': '利用数学大型语言模型（LLMs）进行证明生成是LLMs研究中的基础课题。我们argue当前LLMs证明陈述的能力主要依赖于它们在训练过程中是否遇到了相关的证明过程。这种依赖限制了它们对数学定理及相关概念的更深层次理解。受人类数学教育中常用的教学方法“反例证明”启发，我们的工作旨在通过反例增强LLMs的数学推理和证明能力。具体来说，我们手动创建了一个高质量的大学水平数学基准CounterMATH，要求LLMs通过提供反例来证明数学陈述，从而评估它们对数学概念的掌握程度。此外，我们还开发了一个数据工程框架，以自动获取进一步模型改进所需的训练数据。广泛的实验证据和详细分析表明，CounterMATH具有挑战性，表明像OpenAI O1这样的LLMs缺乏有效的反例驱动证明能力。此外，我们对模型训练的探索表明，增强LLMs的反例驱动概念推理能力对于提高它们的整体数学能力至关重要。我们认为，我们的工作为数学LLMs社区提供了新的视角。', 'title_zh': '一例展现，众理皆知！基于反例的概念性推理在数学大语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.10453', 'title': 'Linking Cryptoasset Attribution Tags to Knowledge Graph Entities: An LLM-based Approach', 'authors': 'Régnier Avice, Bernhard Haslhofer, Zhidong Li, Jianlong Zhou', 'link': 'https://arxiv.org/abs/2502.10453', 'abstract': 'Attribution tags form the foundation of modern cryptoasset forensics. However, inconsistent or incorrect tags can mislead investigations and even result in false accusations. To address this issue, we propose a novel computational method based on Large Language Models (LLMs) to link attribution tags with well-defined knowledge graph concepts. We implemented this method in an end-to-end pipeline and conducted experiments showing that our approach outperforms baseline methods by up to 37.4% in F1-score across three publicly available attribution tag datasets. By integrating concept filtering and blocking procedures, we generate candidate sets containing five knowledge graph entities, achieving a recall of 93% without the need for labeled data. Additionally, we demonstrate that local LLM models can achieve F1-scores of 90%, comparable to remote models which achieve 94%. We also analyze the cost-performance trade-offs of various LLMs and prompt templates, showing that selecting the most cost-effective configuration can reduce costs by 90%, with only a 1% decrease in performance. Our method not only enhances attribution tag quality but also serves as a blueprint for fostering more reliable forensic evidence.', 'abstract_zh': '基于大型语言模型的属性标签关联方法为现代加密资产取证奠定了基础。然而，不一致或错误的标签可能会误导调查，甚至导致误判。为此，我们提出了一种基于大型语言模型（LLMs）的新计算方法，将属性标签与明确的知识图谱概念关联起来。我们在端到端的管道中实现了这一方法，并进行了实验，结果显示，在三个公开的属性标签数据集中，我们的方法在F1分数上比基线方法高出了37.4%。通过结合概念过滤和阻塞程序，我们生成包含五个知识图实体的候选集，在无需标注数据的情况下实现93%的召回率。此外，我们证明了本地LLM模型可以达到90%的F1分数，与远程模型达到的94%相媲美。我们还分析了各种LLM和提示模板的成本-性能权衡，结果显示，选择最具成本效益的配置可以将成本降低90%，同时性能降幅仅为1%。我们的方法不仅提高了属性标签的质量，还为促进更可靠的 forensic 证据提供了一个范本。', 'title_zh': '基于大语言模型的方法：cryptoasset 归因标签与知识图谱实体的联系'}
{'arxiv_id': 'arXiv:2502.10440', 'title': 'Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Ownership Verification with Reasoning', 'authors': 'Junfeng Guo, Yiming Li, Ruibo Chen, Yihan Wu, Chenxi Liu, Yanshuo Chen, Heng Huang', 'link': 'https://arxiv.org/abs/2502.10440', 'abstract': "Large language models (LLMs) are increasingly integrated into real-world applications through retrieval-augmented generation (RAG) mechanisms to supplement their responses with up-to-date and domain-specific knowledge. However, the valuable and often proprietary nature of the knowledge bases used in RAG introduces the risk of unauthorized usage by adversaries. Existing methods that can be generalized as watermarking techniques to protect these knowledge bases typically involve poisoning attacks. However, these methods require to alter the results of verification samples (\\eg, generating incorrect outputs), inevitably making them susceptible to anomaly detection and even introduce new security risks. To address these challenges, we propose \\name{} for `harmless' copyright protection of knowledge bases. Instead of manipulating LLM's final output, \\name{} implants distinct verification behaviors in the space of chain-of-thought (CoT) reasoning, maintaining the correctness of the final answer. Our method has three main stages: (1) \\textbf{Generating CoTs}: For each verification question, we generate two CoTs, including a target CoT for building watermark behaviors; (2) \\textbf{Optimizing Watermark Phrases and Target CoTs}: We optimize them to minimize retrieval errors under the black-box setting of suspicious LLM, ensuring that the watermarked verification queries activate the target CoTs without being activated in non-watermarked ones; (3) \\textbf{Ownership Verification}: We exploit a pairwise Wilcoxon test to statistically verify whether a suspicious LLM is augmented with the protected knowledge base by comparing its responses to watermarked and benign verification queries. Our experiments on diverse benchmarks demonstrate that \\name{} effectively protects knowledge bases against unauthorized usage while preserving the integrity and performance of the RAG.", 'abstract_zh': '大型语言模型（LLMs）通过检索增强生成（RAG）机制越来越多地集成到实际应用中，以通过补充最新和领域特定的知识来完善其响应。然而，RAG所使用的宝贵且经常是专有的知识库性质给对手未经授权使用带来了风险。现有可以泛化的水印技术方法通常涉及投毒攻击。然而，这些方法需要修改验证样本的结果（例如，生成错误输出），从而不可避免地使其容易被异常检测，并可能引入新的安全风险。为了解决这些挑战，我们提出了\\name{}用于知识库的“无害”版权保护。该方法不操纵LLM的最终输出，而是通过思维链（CoT）推理的空间植入不同的验证行为，保持最终答案的正确性。该方法主要有三个阶段：（1）生成思维链：对于每个验证问题，我们生成两个思维链，包括用于构建水印行为的目标思维链；（2）优化水印短语和目标思维链：在可疑LLM的黑色盒设置下，优化它们以最小化检索错误，确保具有水印的验证查询激活目标思维链而不激活非水印的思维链；（3）所有权验证：我们利用Wilcoxon配对秩检验统计验证可疑LLM是否被增强使用了受保护的知识库，方法是将其对水印和良性验证查询的响应进行比较。我们在多种基准测试上的实验表明，\\name{}有效地保护了知识库免于未经授权的使用，同时保持了RAG的完整性和性能。', 'title_zh': '基于推理的所有权验证以实现检索增强语言模型知识库的版权保护'}
{'arxiv_id': 'arXiv:2502.10438', 'title': 'Injecting Universal Jailbreak Backdoors into LLMs in Minutes', 'authors': 'Zhuowei Chen, Qiannan Zhang, Shichao Pei', 'link': 'https://arxiv.org/abs/2502.10438', 'abstract': "Jailbreak backdoor attacks on LLMs have garnered attention for their effectiveness and stealth. However, existing methods rely on the crafting of poisoned datasets and the time-consuming process of fine-tuning. In this work, we propose JailbreakEdit, a novel jailbreak backdoor injection method that exploits model editing techniques to inject a universal jailbreak backdoor into safety-aligned LLMs with minimal intervention in minutes. JailbreakEdit integrates a multi-node target estimation to estimate the jailbreak space, thus creating shortcuts from the backdoor to this estimated jailbreak space that induce jailbreak actions. Our attack effectively shifts the models' attention by attaching strong semantics to the backdoor, enabling it to bypass internal safety mechanisms. Experimental results show that JailbreakEdit achieves a high jailbreak success rate on jailbreak prompts while preserving generation quality, and safe performance on normal queries. Our findings underscore the effectiveness, stealthiness, and explainability of JailbreakEdit, emphasizing the need for more advanced defense mechanisms in LLMs.", 'abstract_zh': '基于模型编辑的Jailbreak后门注入方法JailbreakEdit在安全对齐的大语言模型中实现了高效、隐蔽的后门攻击，而无需大量干预和耗时的微调过程。', 'title_zh': '在几分钟内向LLMs注入通用型 jailbreak 后门'}
{'arxiv_id': 'arXiv:2502.10424', 'title': 'QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache', 'authors': 'Rishabh Tiwari, Haocheng Xi, Aditya Tomar, Coleman Hooper, Sehoon Kim, Maxwell Horton, Mahyar Najibi, Michael W. Mahoney, Kurt Keutzer, Amir Gholami', 'link': 'https://arxiv.org/abs/2502.10424', 'abstract': 'Large Language Models (LLMs) are increasingly being deployed on edge devices for long-context settings, creating a growing need for fast and efficient long-context inference. In these scenarios, the Key-Value (KV) cache is the primary bottleneck in terms of both GPU memory and latency, as the full KV cache must be loaded for each decoding step. While speculative decoding is a widely accepted technique to accelerate autoregressive decoding, existing methods often struggle to achieve significant speedups due to inefficient KV cache optimization strategies and result in low acceptance rates. To address these challenges, we propose a novel self-speculative decoding framework, QuantSpec, where the draft model shares the architecture of the target model but employs a hierarchical 4-bit quantized KV cache and 4-bit quantized weights for acceleration. QuantSpec maintains high acceptance rates ($>$90%) and reliably provides consistent end-to-end speedups upto $\\sim2.5\\times$, outperforming other self-speculative decoding methods that use sparse KV cache for long-context LLM inference. QuantSpec also reduces the memory requirements by $\\sim 1.3\\times$ compared to these alternatives.', 'abstract_zh': '大型语言模型（LLMs）越来越多地在边缘设备上用于长上下文设置，这产生了对快速高效长上下文推理的日益增长的需求。在这种场景中，键值（KV）缓存是both GPU内存和延迟的主要瓶颈，因为每个解码步骤都需要加载完整的KV缓存。尽管推测性解码是加速自回归解码的广泛接受的技术，但现有方法往往由于不高效的KV缓存优化策略而难以实现显著的加速，并且接受率较低。为了解决这些问题，我们提出了一种新颖的自我推测性解码框架QuantSpec，其中草稿模型与目标模型共享架构，但使用分层的4位量化KV缓存和4位量化权重来加速。QuantSpec保持高接受率（>90%）并可靠地提供了端到端速度提升，最高可达约2.5倍，优于其他使用稀疏KV缓存进行长上下文LLM推理的自我推测性解码方法。此外，QuantSpec将内存要求降低了约1.3倍比这些替代方案。', 'title_zh': 'QuantSpec: 嵌套量化KV缓存的自推测解码'}
{'arxiv_id': 'arXiv:2502.10419', 'title': 'A Hybrid Swarm Intelligence Approach for Optimizing Multimodal Large Language Models Deployment in Edge-Cloud-based Federated Learning Environments', 'authors': 'Gaith Rjouba, Hanae Elmekki, Saidul Islam, Jamal Bentahar, Rachida Dssouli', 'link': 'https://arxiv.org/abs/2502.10419', 'abstract': 'The combination of Federated Learning (FL), Multimodal Large Language Models (MLLMs), and edge-cloud computing enables distributed and real- time data processing while preserving privacy across edge devices and cloud infrastructure. However, the deployment of MLLMs in FL environments with resource-constrained edge devices presents significant challenges, in- cluding resource management, communication overhead, and non-IID data. To address these challenges, we propose a novel hybrid framework wherein MLLMs are deployed on edge devices equipped with sufficient resources and battery life, while the majority of training occurs in the cloud. To identify suitable edge devices for deployment, we employ Particle Swarm Optimiza- tion (PSO), and Ant Colony Optimization (ACO) is utilized to optimize the transmission of model updates between edge and cloud nodes. This proposed swarm intelligence-based framework aims to enhance the efficiency of MLLM training by conducting extensive training in the cloud and fine-tuning at the edge, thereby reducing energy consumption and communication costs. Our experimental results show that the proposed method significantly improves system performance, achieving an accuracy of 92%, reducing communica- tion cost by 30%, and enhancing client participation compared to traditional FL methods. These results make the proposed approach highly suitable for large-scale edge-cloud computing systems.', 'abstract_zh': '联邦学习、多模态大语言模型和边缘-云计算的结合 enables 边缘设备和云基础设施之间的分布式和实时数据处理并保护隐私。然而，在资源受限的边缘设备上部署多模态大语言模型（MLLMs）在联邦学习环境中带来了显著挑战，包括资源管理、通信开销和非IID数据问题。为应对这些挑战，我们提出了一种新型混合框架，在装备有充足资源和电池寿命的边缘设备上部署多模态大语言模型，而大部分训练则在云中进行。为了确定适合部署的边缘设备，我们采用粒子 swarm 优化（PSO）进行设备选择，利用蚁群优化（ACO）优化边缘和云节点之间模型更新的传输。这种基于群智的框架旨在通过在云中进行广泛的训练并在边缘进行微调来提高多模态大语言模型训练的效率，从而减少能源消耗和通信成本。实验结果表明，所提出的方法显著提高了系统性能，准确率达到92%，通信成本降低30%，并且增强了客户端参与度，相比传统联邦学习方法更具优势。这对于大规模边缘-云计算系统来说是非常合适的。', 'title_zh': '基于边缘-云联邦学习环境的多模态大型语言模型部署的混合 swarm 智能优化方法'}
{'arxiv_id': 'arXiv:2502.10413', 'title': 'Machine Learning-Driven Convergence Analysis in Multijurisdictional Compliance Using BERT and K-Means Clustering', 'authors': 'Raj Sonani, Lohalekar Prayas', 'link': 'https://arxiv.org/abs/2502.10413', 'abstract': 'Digital data continues to grow, there has been a shift towards using effective regulatory mechanisms to safeguard personal information. The CCPA of California and the General Data Protection Regulation (GDPR) of the European Union are two of the most important privacy laws. The regulation is intended to safeguard consumer privacy, but it varies greatly in scope, definitions, and methods of enforcement. This paper presents a fresh approach to adaptive compliance, using machine learning and emphasizing natural language processing (NLP) as the primary focus of comparison between the GDPR and CCPA. Using NLP, this study compares various regulations to identify areas where they overlap or diverge. This includes the "right to be forgotten" provision in the GDPR and the "opt-out of sale" provision under CCPA. International companies can learn valuable lessons from this report, as it outlines strategies for better enforcement of laws across different nations. Additionally, the paper discusses the challenges of utilizing NLP in legal literature and proposes methods to enhance the model-ability of machine learning models for studying regulations. The study\'s objective is to "bridge the gap between legal knowledge and technical expertise" by developing regulatory compliance strategies that are more efficient in operation and more effective in data protection.', 'abstract_zh': '数字数据持续增长，监管机制的有效性已成为保护个人隐私的关键。加利福尼亚州 CCPA 和欧盟 GDPR 是最重要的隐私法律。本文提出了一种适应性合规的新方法，侧重于使用机器学习和自然语言处理（NLP）来比较 GDPR 和 CCPA。通过 NLP，本研究比较各种法规以识别它们重叠或不同之处，包括 GDPR 中的“被遗忘权”条款和 CCPA 中的“不销售选择退出”条款。国际公司可以从本报告中学习宝贵的经验，因为它概述了在不同国家更好地执行法律的战略。此外，本文讨论了在法律文献中利用 NLP 的挑战，并提出了提高机器学习模型研究法规能力的方法。研究的目的是“弥合法律知识和技术专长之间的差距”，通过开发更高效的合规策略和更有效的数据保护策略。', 'title_zh': '基于BERT和K-Means聚类的多辖区合规性驱动的机器学习融合分析'}
{'arxiv_id': 'arXiv:2502.10411', 'title': 'TrueReason: An Exemplar Personalised Learning System Integrating Reasoning with Foundational Models', 'authors': 'Sahan Bulathwela, Daniel Van Niekerk, Jarrod Shipton, Maria Perez-Ortiz, Benjamin Rosman, John Shawe-Taylor', 'link': 'https://arxiv.org/abs/2502.10411', 'abstract': 'Personalised education is one of the domains that can greatly benefit from the most recent advances in Artificial Intelligence (AI) and Large Language Models (LLM). However, it is also one of the most challenging applications due to the cognitive complexity of teaching effectively while personalising the learning experience to suit independent learners. We hypothesise that one promising approach to excelling in such demanding use cases is using a \\emph{society of minds}. In this chapter, we present TrueReason, an exemplar personalised learning system that integrates a multitude of specialised AI models that can mimic micro skills that are composed together by a LLM to operationalise planning and reasoning. The architecture of the initial prototype is presented while describing two micro skills that have been incorporated in the prototype. The proposed system demonstrates the first step in building sophisticated AI systems that can take up very complex cognitive tasks that are demanded by domains such as education.', 'abstract_zh': '个性化教育是最新人工智能（AI）和大型语言模型（LLM）进步可以大大受益的一个领域，但同时也是一项最具挑战性的应用之一，因为有效地进行个性化教学以适应独立学习者的需求认知复杂性很高。我们假设在这种严苛的应用场景中取得优异成果的方法之一是使用“联合心智群”。在本章中，我们介绍TrueReason，这是一个范例性的个性化学习系统，集成了多种专门的AI模型，这些模型能够模拟大型语言模型组合的微技能，以实现规划和推理的运作。我们描述了原型的架构，并介绍了已经整合到该原型中的两种微技能。所提出系统展示了构建能够承担如教育等領域所需极其复杂的认知任务的高级AI系统的初步步骤。', 'title_zh': 'TrueReason：一种结合推理与基础模型的示例个性化学习系统'}
{'arxiv_id': 'arXiv:2502.10409', 'title': 'Data Science Students Perspectives on Learning Analytics: An Application of Human-Led and LLM Content Analysis', 'authors': 'Raghda Zahran, Jianfei Xu, Huizhi Liang, Matthew Forshaw', 'link': 'https://arxiv.org/abs/2502.10409', 'abstract': "Objective This study is part of a series of initiatives at a UK university designed to cultivate a deep understanding of students' perspectives on analytics that resonate with their unique learning needs. It explores collaborative data processing undertaken by postgraduate students who examined an Open University Learning Analytics Dataset (OULAD).\nMethods A qualitative approach was adopted, integrating a Retrieval-Augmented Generation (RAG) and a Large Language Model (LLM) technique with human-led content analysis to gather information about students' perspectives based on their submitted work. The study involved 72 postgraduate students in 12 groups.\nFindings The analysis of group work revealed diverse insights into essential learning analytics from the students' perspectives. All groups adopted a structured data science methodology. The questions formulated by the groups were categorised into seven themes, reflecting their specific areas of interest. While there was variation in the selected variables to interpret correlations, a consensus was found regarding the general results.\nConclusion A significant outcome of this study is that students specialising in data science exhibited a deeper understanding of learning analytics, effectively articulating their interests through inferences drawn from their analyses. While human-led content analysis provided a general understanding of students' perspectives, the LLM offered nuanced insights.", 'abstract_zh': '研究目标：本研究是英国某大学开展的一系列旨在培养学生对数据分析深刻理解的举措之一，专注于探索满足学生独特学习需求的学生视角。研究重点在于分析了研究生在研究开放大学学习分析数据集（OULAD）时进行的合作数据分析过程。\n\n研究方法：采用定性研究方法，结合检索增强生成（RAG）和大型语言模型（LLM）技术与人工主导的内容分析，基于学生提交的工作获取有关学生视角的信息。研究涉及12组共72名研究生。\n\n研究发现：对小组工作的分析揭示了学生从多种视角对学习分析的重要见解。所有小组均采用了结构化的数据科学方法。小组提出的问题被归类为七个主题，反映了他们各自的研究兴趣。虽然选择用于解析相关性的变量有所不同，但对于一般结果，小组达成了共识。\n\n研究结论：本研究的重要成果是，专注于数据科学的学生表现出对学习分析的更深刻理解，通过从分析中得出的推断有效表达了他们的兴趣。虽然人工主导的内容分析提供了对学生视角的一般理解，但LLM提供了更为细致的洞见。', 'title_zh': '数据科学学生对学习分析的视角：一种基于人类主导和大规模语言模型内容分析的应用'}
{'arxiv_id': 'arXiv:2502.10407', 'title': 'Addressing Bias in Generative AI: Challenges and Research Opportunities in Information Management', 'authors': 'Xiahua Wei, Naveen Kumar, Han Zhang', 'link': 'https://arxiv.org/abs/2502.10407', 'abstract': 'Generative AI technologies, particularly Large Language Models (LLMs), have transformed information management systems but introduced substantial biases that can compromise their effectiveness in informing business decision-making. This challenge presents information management scholars with a unique opportunity to advance the field by identifying and addressing these biases across extensive applications of LLMs. Building on the discussion on bias sources and current methods for detecting and mitigating bias, this paper seeks to identify gaps and opportunities for future research. By incorporating ethical considerations, policy implications, and sociotechnical perspectives, we focus on developing a framework that covers major stakeholders of Generative AI systems, proposing key research questions, and inspiring discussion. Our goal is to provide actionable pathways for researchers to address bias in LLM applications, thereby advancing research in information management that ultimately informs business practices. Our forward-looking framework and research agenda advocate interdisciplinary approaches, innovative methods, dynamic perspectives, and rigorous evaluation to ensure fairness and transparency in Generative AI-driven information systems. We expect this study to serve as a call to action for information management scholars to tackle this critical issue, guiding the improvement of fairness and effectiveness in LLM-based systems for business practice.', 'abstract_zh': '生成式AI技术，特别是大规模语言模型（LLMs），已经改变了信息管理系统，但同时也引入了显著的偏见，这些偏见可能会影响LLMs在商业决策制定中的有效性。这一挑战为信息管理学者提供了一个独特的机会，通过识别和解决LLMs广泛应用中的偏见，推动该领域的进步。基于对偏见来源及现有偏见检测和缓解方法的讨论，本文旨在识别未来研究的不足之处和机会。通过纳入伦理考量、政策含义和社会技术视角，我们集中于开发一个覆盖生成式AI系统主要利益相关者的框架，提出关键研究问题，并激发讨论。我们的目标是为研究人员提供实际路径，以解决LLMs应用中的偏见，从而推动信息管理研究，最终指导商业实践。前瞻性的框架和研究议程倡导跨学科的方法、创新的方法、动态观点和严格的评估，以确保生成式AI驱动的信息系统中的公平性和透明度。我们期望这项研究能够成为信息管理学者采取行动的号召，指导基于LLM系统的公平性和有效性改进，以服务于商业实践。', 'title_zh': '治理生成式AI中的偏见：信息管理中的挑战与研究机遇'}
{'arxiv_id': 'arXiv:2502.10406', 'title': 'FishBargain: An LLM-Empowered Bargaining Agent for Online Fleamarket Platform Sellers', 'authors': 'Dexin Kong, Xu Yan, Ming Chen, Shuguang Han, Jufeng Chen, Fei Huang', 'link': 'https://arxiv.org/abs/2502.10406', 'abstract': "Different from traditional Business-to-Consumer e-commerce platforms~(e.g., Amazon), online fleamarket platforms~(e.g., Craigslist) mainly focus on individual sellers who are lack of time investment and business proficiency. Individual sellers often struggle with the bargaining process and thus the deal is unaccomplished. Recent advancements in Large Language Models(LLMs) demonstrate huge potential in various dialogue tasks, but those tasks are mainly in the form of passively following user's instruction. Bargaining, as a form of proactive dialogue task, represents a distinct art of dialogue considering the dynamism of environment and uncertainty of adversary strategies. In this paper, we propose an LLM-empowered bargaining agent designed for online fleamarket platform sellers, named as FishBargain. Specifically, FishBargain understands the chat context and product information, chooses both action and language skill considering possible adversary actions and generates utterances. FishBargain has been tested by thousands of individual sellers on one of the largest online fleamarket platforms~(Xianyu) in China. Both qualitative and quantitative experiments demonstrate that FishBargain can effectively help sellers make more deals.", 'abstract_zh': '不同于传统的 Business-to-Consumer 电子商务平台（例如 Amazon），在线跳蚤市场平台（例如 Craigslist）主要关注缺乏时间投入和商业技能的个体卖家。个体卖家经常在讨价还价过程中遇到困难，导致交易无法达成。大型语言模型（LLMs）的近期进展展示了在各种对话任务中的巨大潜力，但这些任务主要以被动地遵循用户指令的形式出现。讨价还价作为一种主动的对话任务，考虑到环境的动态性和对手策略的不确定性，代表了一种独特的对话艺术。在本文中，我们提出了一种旨在帮助在线跳蚤市场平台卖家的大型语言模型驱动的讨价还价代理，名为 FishBargain。具体来说，FishBargain 理解聊天背景和产品信息，考虑可能的对手行为选择行动和语言技巧并生成话语。FishBargain 已在中国最大的在线跳蚤市场平台（Xianyu）上通过了数千名个体卖家的测试。定性和定量实验均表明，FishBargain 能够有效帮助卖家达成更多交易。', 'title_zh': 'FishBargain：一个由LLM赋能的在线跳蚤市场卖家讨价还价代理系统'}
{'arxiv_id': 'arXiv:2312.02073', 'title': 'A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia', 'authors': 'Giovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre Kıcıman, Hamid Palangi, Barun Patra, Robert West', 'link': 'https://arxiv.org/abs/2312.02073', 'abstract': "Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.", 'abstract_zh': '大型语言模型（LLMs）在利用上下文信息方面表现出色，但这些模型在上下文 Grounding 机制背后的原理仍不清楚，特别是在上下文信息与模型参数中存储的真实知识产生矛盾的情况下。在检索增强生成方法中，倾向于使用上下文信息对于补充最新信息至关重要，希望通过 Grounding 调整过时或嘈杂的存储知识。我们提出了一种使用 Fakepedia 的新方法来研究 Grounding 能力，Fakepedia 是一种与模型内部参数知识相矛盾的反事实数据集。在本研究中，我们介绍了 Fakepedia，这是一种用于评估内部参数知识与上下文信息相矛盾时 Grounding 能力的反事实数据集。我们使用 Fakepedia 对比基准各种 LLM，并基于我们提出的掩蔽分组因果追踪（MGCT）方法，对 LLM 组件在回答 Fakepedia 查询时进行因果中介分析。通过这种分析，我们识别出接地响应与非接地响应之间不同的计算模式。最后，我们证明仅通过计算分析即可区分接地响应与非接地响应。我们的结果与现有关于事实回忆机制的研究结果相结合，提供了一个关于 Grounding 机制和事实回忆机制在 LLM 中相互作用的连贯叙事。', 'title_zh': '矩阵中的bug？寻找与检测语言模型接地的Fakepedia'}
