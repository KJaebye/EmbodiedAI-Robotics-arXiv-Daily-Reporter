{'arxiv_id': 'arXiv:2502.12113', 'title': 'A Monocular Event-Camera Motion Capture System', 'authors': 'Leonard Bauersfeld, Davide Scaramuzza', 'link': 'https://arxiv.org/abs/2502.12113', 'abstract': 'Motion capture systems are a widespread tool in research to record ground-truth poses of objects. Commercial systems use reflective markers attached to the object and then triangulate pose of the object from multiple camera views. Consequently, the object must be visible to multiple cameras which makes such multi-view motion capture systems unsuited for deployments in narrow, confined spaces (e.g. ballast tanks of ships). In this technical report we describe a monocular event-camera motion capture system which overcomes this limitation and is ideally suited for narrow spaces. Instead of passive markers it relies on active, blinking LED markers such that each marker can be uniquely identified from the blinking frequency. The markers are placed at known locations on the tracking object. We then solve the PnP (perspective-n-points) problem to obtain the position and orientation of the object. The developed system has millimeter accuracy, millisecond latency and we demonstrate that its state estimate can be used to fly a small, agile quadrotor.', 'abstract_zh': '单目事件相机运动捕捉系统', 'title_zh': '单目事件相机运动捕捉系统'}
{'arxiv_id': 'arXiv:2502.11800', 'title': 'Residual Learning towards High-fidelity Vehicle Dynamics Modeling with Transformer', 'authors': 'Jinyu Miao, Rujun Yan, Bowei Zhang, Tuopu Wen, Kun Jiang, Mengmeng Yang, Jin Huang, Zhihua Zhong, Diange Yang', 'link': 'https://arxiv.org/abs/2502.11800', 'abstract': 'The vehicle dynamics model serves as a vital component of autonomous driving systems, as it describes the temporal changes in vehicle state. In a long period, researchers have made significant endeavors to accurately model vehicle dynamics. Traditional physics-based methods employ mathematical formulae to model vehicle dynamics, but they are unable to adequately describe complex vehicle systems due to the simplifications they entail. Recent advancements in deep learning-based methods have addressed this limitation by directly regressing vehicle dynamics. However, the performance and generalization capabilities still require further enhancement. In this letter, we address these problems by proposing a vehicle dynamics correction system that leverages deep neural networks to correct the state residuals of a physical model instead of directly estimating the states. This system greatly reduces the difficulty of network learning and thus improves the estimation accuracy of vehicle dynamics. Furthermore, we have developed a novel Transformer-based dynamics residual correction network, DyTR. This network implicitly represents state residuals as high-dimensional queries, and iteratively updates the estimated residuals by interacting with dynamics state features. The experiments in simulations demonstrate the proposed system works much better than physics model, and our proposed DyTR model achieves the best performances on dynamics state residual correction task, reducing the state prediction errors of a simple 3 DoF vehicle model by an average of 92.3% and 59.9% in two dataset, respectively.', 'abstract_zh': '基于深度神经网络的车辆动力学修正系统：一种通过变压器模型改进车辆动力学状态残差矫正的方法', 'title_zh': '面向高保真车辆动力学建模的残差学习与变换器融合方法'}
{'arxiv_id': 'arXiv:2502.11563', 'title': 'Leader and Follower: Interactive Motion Generation under Trajectory Constraints', 'authors': 'Runqi Wang, Caoyuan Ma, Jian Zhao, Hanrui Xu, Dongfang Sun, Haoyang Chen, Lin Xiong, Zheng Wang, Xuelong Li', 'link': 'https://arxiv.org/abs/2502.11563', 'abstract': "With the rapid advancement of game and film production, generating interactive motion from texts has garnered significant attention due to its potential to revolutionize content creation processes. In many practical applications, there is a need to impose strict constraints on the motion range or trajectory of virtual characters. However, existing methods that rely solely on textual input face substantial challenges in accurately capturing the user's intent, particularly in specifying the desired trajectory. As a result, the generated motions often lack plausibility and accuracy. Moreover, existing trajectory - based methods for customized motion generation rely on retraining for single - actor scenarios, which limits flexibility and adaptability to different datasets, as well as interactivity in two-actor motions. To generate interactive motion following specified trajectories, this paper decouples complex motion into a Leader - Follower dynamic, inspired by role allocation in partner dancing. Based on this framework, this paper explores the motion range refinement process in interactive motion generation and proposes a training-free approach, integrating a Pace Controller and a Kinematic Synchronization Adapter. The framework enhances the ability of existing models to generate motion that adheres to trajectory by controlling the leader's movement and correcting the follower's motion to align with the leader. Experimental results show that the proposed approach, by better leveraging trajectory information, outperforms existing methods in both realism and accuracy.", 'abstract_zh': '基于轨迹分解的交互动作生成方法', 'title_zh': '领导者与跟随者：基于轨迹约束的交互式运动生成'}
{'arxiv_id': 'arXiv:2502.11461', 'title': 'Doppler Correspondence: Non-Iterative Scan Matching With Doppler Velocity-Based Correspondence', 'authors': 'Jiwoo Kim, Geunsik Bae, Changseung Kim, Jinwoo Lee, Woojae Shin, Hyondong Oh', 'link': 'https://arxiv.org/abs/2502.11461', 'abstract': 'Achieving successful scan matching is essential for LiDAR odometry. However, in challenging environments with adverse weather conditions or repetitive geometric patterns, LiDAR odometry performance is degraded due to incorrect scan matching. Recently, the emergence of frequency-modulated continuous wave 4D LiDAR and 4D radar technologies has provided the potential to address these unfavorable conditions. The term 4D refers to point cloud data characterized by range, azimuth, and elevation along with Doppler velocity. Although 4D data is available, most scan matching methods for 4D LiDAR and 4D radar still establish correspondence by repeatedly identifying the closest points between consecutive scans, overlooking the Doppler information. This paper introduces, for the first time, a simple Doppler velocity-based correspondence -- Doppler Correspondence -- that is invariant to translation and small rotation of the sensor, with its geometric and kinematic foundations. Extensive experiments demonstrate that the proposed method enables the direct matching of consecutive point clouds without an iterative process, making it computationally efficient. Additionally, it provides a more robust correspondence estimation in environments with repetitive geometric patterns.', 'abstract_zh': '基于频调连续波4D LiDAR和4D雷达的 Doppler 相对匹配方法', 'title_zh': '多普勒对应：基于多普勒速度对应的非迭代扫描匹配'}
{'arxiv_id': 'arXiv:2502.11161', 'title': 'BFA: Best-Feature-Aware Fusion for Multi-View Fine-grained Manipulation', 'authors': 'Zihan Lan, Weixin Mao, Haosheng Li, Le Wang, Tiancai Wang, Haoqiang Fan, Osamu Yoshie', 'link': 'https://arxiv.org/abs/2502.11161', 'abstract': 'In real-world scenarios, multi-view cameras are typically employed for fine-grained manipulation tasks. Existing approaches (e.g., ACT) tend to treat multi-view features equally and directly concatenate them for policy learning. However, it will introduce redundant visual information and bring higher computational costs, leading to ineffective manipulation. For a fine-grained manipulation task, it tends to involve multiple stages while the most contributed view for different stages is varied over time. In this paper, we propose a plug-and-play best-feature-aware (BFA) fusion strategy for multi-view manipulation tasks, which is adaptable to various policies. Built upon the visual backbone of the policy network, we design a lightweight network to predict the importance score of each view. Based on the predicted importance scores, the reweighted multi-view features are subsequently fused and input into the end-to-end policy network, enabling seamless integration. Notably, our method demonstrates outstanding performance in fine-grained manipulations. Experimental results show that our approach outperforms multiple baselines by 22-46% success rate on different tasks. Our work provides new insights and inspiration for tackling key challenges in fine-grained manipulations.', 'abstract_zh': '在实际应用场景中，多视图相机通常用于精细操作任务。现有方法（例如ACT）往往会同等对待多视图特征，并直接将它们拼接起来进行策略学习。然而，这会引入冗余的视觉信息，并增加计算成本，导致操作效果不佳。对于精细操作任务，往往涉及多个阶段，而对不同阶段贡献最大的视图会随时间变化。在本文中，我们提出了一种插件式最佳特征意识（BFA）融合策略，适用于多视图操作任务，并且具有适应各种策略的能力。基于策略网络的视觉主干，我们设计了一个轻量级网络来预测每个视图的重要性得分。基于预测的重要性得分，加权后的多视图特征随后被融合并输入端到端策略网络，实现无缝集成。值得注意的是，我们的方法在精细操作中表现出色。实验结果表明，与多个基线方法相比，我们的方法在不同任务上的成功率提高了22-46%。我们的工作为解决精细操作中的关键挑战提供了新的见解和灵感。', 'title_zh': 'BFA: 基于最佳特征融合的多视图细粒度操作融合'}
{'arxiv_id': 'arXiv:2502.10842', 'title': 'Mobile Robotic Multi-View Photometric Stereo', 'authors': 'Suryansh Kumar', 'link': 'https://arxiv.org/abs/2502.10842', 'abstract': 'Multi-View Photometric Stereo (MVPS) is a popular method for fine-detailed 3D acquisition of an object from images. Despite its outstanding results on diverse material objects, a typical MVPS experimental setup requires a well-calibrated light source and a monocular camera installed on an immovable base. This restricts the use of MVPS on a movable platform, limiting us from taking MVPS benefits in 3D acquisition for mobile robotics applications. To this end, we introduce a new mobile robotic system for MVPS. While the proposed system brings advantages, it introduces additional algorithmic challenges. Addressing them, in this paper, we further propose an incremental approach for mobile robotic MVPS. Our approach leverages a supervised learning setup to predict per-view surface normal, object depth, and per-pixel uncertainty in model-predicted results. A refined depth map per view is obtained by solving an MVPS-driven optimization problem proposed in this paper. Later, we fuse the refined depth map while tracking the camera pose w.r.t the reference frame to recover globally consistent object 3D geometry. Experimental results show the advantages of our robotic system and algorithm, featuring the local high-frequency surface detail recovery with globally consistent object shape. Our work is beyond any MVPS system yet presented, providing encouraging results on objects with unknown reflectance properties using fewer frames without a tiring calibration and installation process, enabling computationally efficient robotic automation approach to photogrammetry. The proposed approach is nearly 100 times computationally faster than the state-of-the-art MVPS methods such as [1, 2] while maintaining the similar results when tested on subjects taken from the benchmark DiLiGenT MV dataset [3].', 'abstract_zh': '基于多视角光度立体视觉的移动机器人系统及增量方法', 'title_zh': '移动机器人多视图光度立体重建'}
{'arxiv_id': 'arXiv:2502.10606', 'title': 'HIPPo: Harnessing Image-to-3D Priors for Model-free Zero-shot 6D Pose Estimation', 'authors': 'Yibo Liu, Zhaodong Jiang, Binbin Xu, Guile Wu, Yuan Ren, Tongtong Cao, Bingbing Liu, Rui Heng Yang, Amir Rasouli, Jinjun Shan', 'link': 'https://arxiv.org/abs/2502.10606', 'abstract': 'This work focuses on model-free zero-shot 6D object pose estimation for robotics applications. While existing methods can estimate the precise 6D pose of objects, they heavily rely on curated CAD models or reference images, the preparation of which is a time-consuming and labor-intensive process. Moreover, in real-world scenarios, 3D models or reference images may not be available in advance and instant robot reaction is desired. In this work, we propose a novel framework named HIPPo, which eliminates the need for curated CAD models and reference images by harnessing image-to-3D priors from Diffusion Models, enabling model-free zero-shot 6D pose estimation. Specifically, we construct HIPPo Dreamer, a rapid image-to-mesh model built on a multiview Diffusion Model and a 3D reconstruction foundation model. Our HIPPo Dreamer can generate a 3D mesh of any unseen objects from a single glance in just a few seconds. Then, as more observations are acquired, we propose to continuously refine the diffusion prior mesh model by joint optimization of object geometry and appearance. This is achieved by a measurement-guided scheme that gradually replaces the plausible diffusion priors with more reliable online observations. Consequently, HIPPo can instantly estimate and track the 6D pose of a novel object and maintain a complete mesh for immediate robotic applications. Thorough experiments on various benchmarks show that HIPPo outperforms state-of-the-art methods in 6D object pose estimation when prior reference images are limited.', 'abstract_zh': '本工作聚焦于机器人应用中的无模型零样本6D物体姿态估计。现有的方法可以估计物体的精确6D姿态，但它们严重依赖于精心整理的CAD模型或参考图像，这一准备过程耗时且 labor-intensive。此外，在实际应用场景中，3D模型或参考图像可能无法提前获得，并且需要即时的机器人反应。本工作中，我们提出了一种新型框架HIPPo，通过利用来自扩散模型的图像到3D先验信息，消除对精心整理的CAD模型和参考图像的依赖，实现无模型零样本6D姿态估计。具体地，我们构建了基于多视图扩散模型和3D重建基础模型的HIPPo Dreamer，该模型能在几秒钟内从单张图像生成任何未见物体的3D网格。随着获取更多的观察信息，我们提出通过联合优化物体几何和外观来持续精炼扩散先验网格模型。这通过测量导向方案实现，该方案逐步用更可靠的在线观察取代可能的扩散先验。因此，HIPPo可以即时估计和跟踪新物体的6D姿态，并保持完整的网格模型以支持即时的机器人应用。广泛benchmark上的实验证明，当先验参考图像有限时，HIPPo在6D物体姿态估计中优于现有最先进的方法。', 'title_zh': 'HIPPo: 利用图像到3D先验进行无模型零样本6D姿态估计'}
{'arxiv_id': 'arXiv:2502.12154', 'title': 'Diffusion Models without Classifier-free Guidance', 'authors': 'Zhicong Tang, Jianmin Bao, Dong Chen, Baining Guo', 'link': 'https://arxiv.org/abs/2502.12154', 'abstract': 'This paper presents Model-guidance (MG), a novel objective for training diffusion model that addresses and removes of the commonly used Classifier-free guidance (CFG). Our innovative approach transcends the standard modeling of solely data distribution to incorporating the posterior probability of conditions. The proposed technique originates from the idea of CFG and is easy yet effective, making it a plug-and-play module for existing models. Our method significantly accelerates the training process, doubles the inference speed, and achieve exceptional quality that parallel and even surpass concurrent diffusion models with CFG. Extensive experiments demonstrate the effectiveness, efficiency, scalability on different models and datasets. Finally, we establish state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34. Our code is available at this https URL.', 'abstract_zh': '本文提出了模型引导（MG），这是一种新颖的目标，用于训练扩散模型，解决了和消除了常用的无分类器引导（CFG）。我们的创新方法超越了仅基于数据分布的标准建模，而是将条件的后概率包含进来。该提出的技术源自CFG的想法，简单有效，可作为现有模型的即插即用模块。我们的方法显著加速了训练过程，将推理速度提高了两倍，并达到了与使用CFG的并发扩散模型相当甚至更优的质量。大量实验在不同模型和数据集上证明了其有效性、效率和可扩展性。最后，我们在ImageNet 256基准上建立了最先进的性能，FID值为1.34。代码已在此处提供：this https URL。', 'title_zh': '没有分类器自由指导的扩散模型'}
{'arxiv_id': 'arXiv:2502.11897', 'title': 'DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation', 'authors': 'Zhihang Yuan, Siyuan Wang, Rui Xie, Hanling Zhang, Tongcheng Fang, Yuzhang Shang, Shengen Yan, Guohao Dai, Yu Wang', 'link': 'https://arxiv.org/abs/2502.11897', 'abstract': 'In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a training-free paradigm that can make use of adaptive temporal compression in latent space. While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. Based on this insight, DLFR-VAE dynamically adjusts the latent frame rate according to the content complexity. Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and (2) A training-free adaptation mechanism that transforms pretrained VAE architectures into a dynamic VAE that can process features with variable frame rates. Our simple but effective DLFR-VAE can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process.', 'abstract_zh': '动态潜在帧率VAE（DLFR-VAE）：一种无需训练的时空压缩范式', 'title_zh': 'DLFR-VAE：动态隐层帧率VAE视频生成'}
{'arxiv_id': 'arXiv:2502.11809', 'title': 'Revealing Bias Formation in Deep Neural Networks Through the Geometric Mechanisms of Human Visual Decoupling', 'authors': 'Yanbiao Ma, Bowei Liu, Wei Dai, Jiayi Chen, Shuo Li', 'link': 'https://arxiv.org/abs/2502.11809', 'abstract': 'Deep neural networks (DNNs) often exhibit biases toward certain categories during object recognition, even under balanced training data conditions. The intrinsic mechanisms underlying these biases remain unclear. Inspired by the human visual system, which decouples object manifolds through hierarchical processing to achieve object recognition, we propose a geometric analysis framework linking the geometric complexity of class-specific perceptual manifolds in DNNs to model bias. Our findings reveal that differences in geometric complexity can lead to varying recognition capabilities across categories, introducing biases. To support this analysis, we present the Perceptual-Manifold-Geometry library, designed for calculating the geometric properties of perceptual manifolds.', 'abstract_zh': '深度神经网络（DNNs）在对象识别过程中即使在平衡训练数据条件下也常常对某些类别表现出偏差。受人类视觉系统通过分层处理解耦对象流形实现对象识别的启发，我们提出了一种几何分析框架，将DNN中类特定知觉流形的几何复杂性与模型偏差联系起来。我们的研究发现，几何复杂性差异会导致不同类别识别能力的差异，从而引入偏差。为了支持这种分析，我们介绍了感知流形几何库，用于计算感知流形的几何属性。', 'title_zh': '通过人类视觉去耦的几何机制揭示深度神经网络中的偏见形成'}
{'arxiv_id': 'arXiv:2502.11777', 'title': 'Deep Neural Networks for Accurate Depth Estimation with Latent Space Features', 'authors': 'Siddiqui Muhammad Yasir, Hyunsik Ahn', 'link': 'https://arxiv.org/abs/2502.11777', 'abstract': 'Depth estimation plays a pivotal role in advancing human-robot interactions, especially in indoor environments where accurate 3D scene reconstruction is essential for tasks like navigation and object handling. Monocular depth estimation, which relies on a single RGB camera, offers a more affordable solution compared to traditional methods that use stereo cameras or LiDAR. However, despite recent progress, many monocular approaches struggle with accurately defining depth boundaries, leading to less precise reconstructions. In response to these challenges, this study introduces a novel depth estimation framework that leverages latent space features within a deep convolutional neural network to enhance the precision of monocular depth maps. The proposed model features dual encoder-decoder architecture, enabling both color-to-depth and depth-to-depth transformations. This structure allows for refined depth estimation through latent space encoding. To further improve the accuracy of depth boundaries and local features, a new loss function is introduced. This function combines latent loss with gradient loss, helping the model maintain the integrity of depth boundaries. The framework is thoroughly tested using the NYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in complex indoor scenarios. The results clearly show that this approach effectively reduces depth ambiguities and blurring, making it a promising solution for applications in human-robot interaction and 3D scene reconstruction.', 'abstract_zh': '基于隐空间特征的单目深度估计框架在人机交互和3D场景重建中的应用', 'title_zh': '基于潜在空间特征的深度神经网络accurate深度估计'}
{'arxiv_id': 'arXiv:2502.11763', 'title': 'Lightweight Deepfake Detection Based on Multi-Feature Fusion', 'authors': 'Siddiqui Muhammad Yasir, Hyun Kim', 'link': 'https://arxiv.org/abs/2502.11763', 'abstract': 'Deepfake technology utilizes deep learning based face manipulation techniques to seamlessly replace faces in videos creating highly realistic but artificially generated content. Although this technology has beneficial applications in media and entertainment misuse of its capabilities may lead to serious risks including identity theft cyberbullying and false information. The integration of DL with visual cognition has resulted in important technological improvements particularly in addressing privacy risks caused by artificially generated deepfake images on digital media platforms. In this study we propose an efficient and lightweight method for detecting deepfake images and videos making it suitable for devices with limited computational resources. In order to reduce the computational burden usually associated with DL models our method integrates machine learning classifiers in combination with keyframing approaches and texture analysis. Moreover the features extracted with a histogram of oriented gradients (HOG) local binary pattern (LBP) and KAZE bands were integrated to evaluate using random forest extreme gradient boosting extra trees and support vector classifier algorithms. Our findings show a feature-level fusion of HOG LBP and KAZE features improves accuracy to 92% and 96% on FaceForensics++ and Celeb-DFv2 respectively.', 'abstract_zh': 'Deepfake技术利用基于深度学习的面部操控技术在视频中无缝替换面部，生成高度逼真的但人为生成的内容。尽管这项技术在媒体和娱乐领域具有有益的应用，但对其能力的滥用可能导致身份盗窃、网络欺凌和虚假信息等严重风险。将DL与视觉认知的结合导致了重要的技术进步，特别是在解决数字媒体平台上由人工生成的deepfake图像引起的数据隐私风险方面。在本研究中，我们提出了一种高效且轻量级的方法来检测deepfake图像和视频，使其适用于具有有限计算资源的设备。为了减少通常与DL模型相关的计算负担，我们的方法结合了机器学习分类器和关键帧方法以及纹理分析。此外，利用HOG、LBP和KAZE特征，并使用随机森林、极端梯度提升、额外树和支持向量分类器算法进行评估。我们的研究发现，HOG、LBP和KAZE特征的特征级融合分别在FaceForensics++和Celeb-DFv2数据集上的准确率达到92%和96%。', 'title_zh': '基于多特征融合的轻量级 deepfake 检测'}
{'arxiv_id': 'arXiv:2502.11481', 'title': 'Variable-frame CNNLSTM for Breast Nodule Classification using Ultrasound Videos', 'authors': 'Xiangxiang Cui, Zhongyu Li, Xiayue Fan, Peng Huang, Ying Wang, Meng Yang, Shi Chang, Jihua Zhu', 'link': 'https://arxiv.org/abs/2502.11481', 'abstract': "The intersection of medical imaging and artificial intelligence has become an important research direction in intelligent medical treatment, particularly in the analysis of medical images using deep learning for clinical diagnosis. Despite the advances, existing keyframe classification methods lack extraction of time series features, while ultrasonic video classification based on three-dimensional convolution requires uniform frame numbers across patients, resulting in poor feature extraction efficiency and model classification performance. This study proposes a novel video classification method based on CNN and LSTM, introducing NLP's long and short sentence processing scheme into video classification for the first time. The method reduces CNN-extracted image features to 1x512 dimension, followed by sorting and compressing feature vectors for LSTM training. Specifically, feature vectors are sorted by patient video frame numbers and populated with padding value 0 to form variable batches, with invalid padding values compressed before LSTM training to conserve computing resources. Experimental results demonstrate that our variable-frame CNNLSTM method outperforms other approaches across all metrics, showing improvements of 3-6% in F1 score and 1.5% in specificity compared to keyframe methods. The variable-frame CNNLSTM also achieves better accuracy and precision than equal-frame CNNLSTM. These findings validate the effectiveness of our approach in classifying variable-frame ultrasound videos and suggest potential applications in other medical imaging modalities.", 'abstract_zh': '医学成像与人工智能的交集已成为智能医疗治疗中的重要研究方向，特别是在使用深度学习进行医学图像临床诊断的图像分析中。尽管取得了进展，现有的关键帧分类方法缺乏时间序列特征的提取，而基于三维卷积的超声视频分类需要患者之间的帧数一致，导致特征提取效率低和模型分类性能差。本研究提出了一种基于CNN和LSTM的新型视频分类方法，首次将NLP中的长文本和短文本处理方案引入到视频分类中。该方法将CNN提取的图像特征维度压缩至1x512，随后对特征向量进行排序和压缩，以供LSTM训练。具体而言，特征向量按患者视频帧数排序，并填充0值以形成变长批处理，在LSTM训练前压缩无效填充值以节约计算资源。实验结果表明，我们的变帧CNNLSTM方法在所有指标上均优于其他方法，在F1分数和特异性方面分别比关键帧方法提高了3-6%和1.5%。变帧CNNLSTM在准确率和精确度上也优于等帧CNNLSTM。这些发现验证了该方法在分类变帧超声视频的有效性，并表明其在其他医学成像模态中的潜在应用。', 'title_zh': '基于超声视频的乳腺结节分类的变帧CNN-LSTM方法'}
{'arxiv_id': 'arXiv:2502.11456', 'title': 'Leveraging Labelled Data Knowledge: A Cooperative Rectification Learning Network for Semi-supervised 3D Medical Image Segmentation', 'authors': 'Yanyan Wang, Kechen Song, Yuyuan Liu, Shuai Ma, Yunhui Yan, Gustavo Carneiro', 'link': 'https://arxiv.org/abs/2502.11456', 'abstract': "Semi-supervised 3D medical image segmentation aims to achieve accurate segmentation using few labelled data and numerous unlabelled data. The main challenge in the design of semi-supervised learning methods consists in the effective use of the unlabelled data for training. A promising solution consists of ensuring consistent predictions across different views of the data, where the efficacy of this strategy depends on the accuracy of the pseudo-labels generated by the model for this consistency learning strategy. In this paper, we introduce a new methodology to produce high-quality pseudo-labels for a consistency learning strategy to address semi-supervised 3D medical image segmentation. The methodology has three important contributions. The first contribution is the Cooperative Rectification Learning Network (CRLN) that learns multiple prototypes per class to be used as external knowledge priors to adaptively rectify pseudo-labels at the voxel level. The second contribution consists of the Dynamic Interaction Module (DIM) to facilitate pairwise and cross-class interactions between prototypes and multi-resolution image features, enabling the production of accurate voxel-level clues for pseudo-label rectification. The third contribution is the Cooperative Positive Supervision (CPS), which optimises uncertain representations to align with unassertive representations of their class distributions, improving the model's accuracy in classifying uncertain regions. Extensive experiments on three public 3D medical segmentation datasets demonstrate the effectiveness and superiority of our semi-supervised learning method.", 'abstract_zh': '半监督三维医学图像分割通过少量标注数据和大量未标注数据实现准确分割：一种新的伪标签生成方法以提高一致学习策略的效果', 'title_zh': '利用标注数据知识：一种协作校正学习网络在半监督3D医学图像分割中的应用'}
{'arxiv_id': 'arXiv:2502.11307', 'title': 'Exploiting Point-Language Models with Dual-Prompts for 3D Anomaly Detection', 'authors': 'Jiaxiang Wang, Haote Xu, Xiaolu Chen, Haodi Xu, Yue Huang, Xinghao Ding, Xiaotong Tu', 'link': 'https://arxiv.org/abs/2502.11307', 'abstract': "Anomaly detection (AD) in 3D point clouds is crucial in a wide range of industrial applications, especially in various forms of precision manufacturing. Considering the industrial demand for reliable 3D AD, several methods have been developed. However, most of these approaches typically require training separate models for each category, which is memory-intensive and lacks flexibility. In this paper, we propose a novel Point-Language model with dual-prompts for 3D ANomaly dEtection (PLANE). The approach leverages multi-modal prompts to extend the strong generalization capabilities of pre-trained Point-Language Models (PLMs) to the domain of 3D point cloud AD, achieving impressive detection performance across multiple categories using a single model. Specifically, we propose a dual-prompt learning method, incorporating both text and point cloud prompts. The method utilizes a dynamic prompt creator module (DPCM) to produce sample-specific dynamic prompts, which are then integrated with class-specific static prompts for each modality, effectively driving the PLMs. Additionally, based on the characteristics of point cloud data, we propose a pseudo 3D anomaly generation method (Ano3D) to improve the model's detection capabilities in an unsupervised setting. Experimental results demonstrate that the proposed method, which is under the multi-class-one-model paradigm, achieves a +8.7%/+17% gain on anomaly detection and localization performance as compared to the state-of-the-art one-class-one-model methods for the Anomaly-ShapeNet dataset, and obtains +4.3%/+4.1% gain for the Real3D-AD dataset. Code will be available upon publication.", 'abstract_zh': '基于点语言模型的双提示三维异常检测（PLANE）', 'title_zh': '利用双提示点语言模型进行3D异常检测'}
{'arxiv_id': 'arXiv:2502.11195', 'title': 'From Deception to Perception: The Surprising Benefits of Deepfakes for Detecting, Measuring, and Mitigating Bias', 'authors': 'Yizhi Liu, Balaji Padmanabhan, Siva Viswanathan', 'link': 'https://arxiv.org/abs/2502.11195', 'abstract': 'While deepfake technologies have predominantly been criticized for potential misuse, our study demonstrates their significant potential as tools for detecting, measuring, and mitigating biases in key societal domains. By employing deepfake technology to generate controlled facial images, we extend the scope of traditional correspondence studies beyond mere textual manipulations. This enhancement is crucial in scenarios such as pain assessments, where subjective biases triggered by sensitive features in facial images can profoundly affect outcomes. Our results reveal that deepfakes not only maintain the effectiveness of correspondence studies but also introduce groundbreaking advancements in bias measurement and correction techniques. This study emphasizes the constructive role of deepfake technologies as essential tools for advancing societal equity and fairness.', 'abstract_zh': '尽管深度假讯技术主要受到了潜在滥用的批评，但我们的研究展示了它们在检测、衡量和减轻关键社会领域偏见方面的巨大潜力。通过使用深度假讯技术生成受控面部图像，我们扩展了传统对应研究的范围，超越了仅仅对文本的操纵。这种扩展在疼痛评估等场景中至关重要，因为面部图像中的敏感特征可能引发的主观偏见会深刻影响结果。研究结果表明，深度假讯不仅保持了对应研究的有效性，还引入了偏见测量和纠正技术的重大突破。本研究强调了深度假讯技术在促进社会公平和平等方面的重要建设性作用。', 'title_zh': '从欺骗到认知：深层伪造在检测、度量和减轻偏见方面的意外益处'}
{'arxiv_id': 'arXiv:2502.11179', 'title': 'RT-DEMT: A hybrid real-time acupoint detection model combining mamba and transformer', 'authors': 'Shilong Yang, Qi Zang, Chulong Zhang, Lingfeng Huang, Yaoqin Xie', 'link': 'https://arxiv.org/abs/2502.11179', 'abstract': 'Traditional Chinese acupuncture methods often face controversy in clinical practice due to their high subjectivity. Additionally, current intelligent-assisted acupuncture systems have two major limitations: slow acupoint localization speed and low accuracy. To address these limitations, a new method leverages the excellent inference efficiency of the state-space model Mamba, while retaining the advantages of the attention mechanism in the traditional DETR architecture, to achieve efficient global information integration and provide high-quality feature information for acupoint localization tasks. Furthermore, by employing the concept of residual likelihood estimation, it eliminates the need for complex upsampling processes, thereby accelerating the acupoint localization task. Our method achieved state-of-the-art (SOTA) accuracy on a private dataset of acupoints on the human back, with an average Euclidean distance pixel error (EPE) of 7.792 and an average time consumption of 10.05 milliseconds per localization task. Compared to the second-best algorithm, our method improved both accuracy and speed by approximately 14\\%. This significant advancement not only enhances the efficacy of acupuncture treatment but also demonstrates the commercial potential of automated acupuncture robot systems. Access to our method is available at this https URL', 'abstract_zh': '传统中医针灸方法在临床实践中常因高度主观性而面临争议。目前的智能辅助针灸系统存在两大局限性：针灸穴位定位速度慢和准确性低。为解决这些问题，该研究利用状态空间模型Mamba的出色推断效率，同时保留传统DETR架构中的注意力机制优势，实现了高效的全局信息整合，并为针灸穴位定位任务提供了高质量的特征信息。此外，通过应用残差似然估计的概念，该方法消除了复杂的上采样过程的需求，从而加速了针灸穴位定位任务。在针对人体背部穴位的私有数据集上，该方法达到了最先进的（SOTA）精度，平均欧氏距离像素误差（EPE）为7.792，每项定位任务的平均时间消耗为10.05毫秒。与第二优算法相比，该方法在精度和速度上分别提升了约14%。这一显著进步不仅提高了针灸治疗的有效性，还展示了自动化针灸机器人系统的商业潜力。有关该方法的访问链接为：this https URL。', 'title_zh': 'RT-DEMT：结合Mamba和变压器的混合实时腧穴检测模型'}
{'arxiv_id': 'arXiv:2502.11168', 'title': 'Knowing Your Target: Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding', 'authors': 'Xin Gu, Yaojie Shen, Chenxi Luo, Tiejian Luo, Yan Huang, Yuewei Lin, Heng Fan, Libo Zhang', 'link': 'https://arxiv.org/abs/2502.11168', 'abstract': 'Transformer has attracted increasing interest in STVG, owing to its end-to-end pipeline and promising result. Existing Transformer-based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn target position information via iterative interactions with multimodal features, for spatial and temporal localization. Despite simplicity, these zero object queries, due to lacking target-specific cues, are hard to learn discriminative target information from interactions with multimodal features in complicated scenarios (\\e.g., with distractors or occlusion), resulting in degradation. Addressing this, we introduce a novel Target-Aware Transformer for STVG (TA-STVG), which seeks to adaptively generate object queries via exploring target-specific cues from the given video-text pair, for improving STVG. The key lies in two simple yet effective modules, comprising text-guided temporal sampling (TTS) and attribute-aware spatial activation (ASA), working in a cascade. The former focuses on selecting target-relevant temporal cues from a video utilizing holistic text information, while the latter aims at further exploiting the fine-grained visual attribute information of the object from previous target-aware temporal cues, which is applied for object query initialization. Compared to existing methods leveraging zero-initialized queries, object queries in our TA-STVG, directly generated from a given video-text pair, naturally carry target-specific cues, making them adaptive and better interact with multimodal features for learning more discriminative information to improve STVG. In our experiments on three benchmarks, TA-STVG achieves state-of-the-art performance and significantly outperforms the baseline, validating its efficacy.', 'abstract_zh': '目标感知变压器在时空视频理解中的应用：TA-STVG', 'title_zh': '了解目标：目标感知变换器促进更好的时空视频定位'}
{'arxiv_id': 'arXiv:2502.10955', 'title': 'A recurrent vision transformer shows signatures of primate visual attention', 'authors': 'Jonathan Morgan, Badr Albanna, James P. Herman', 'link': 'https://arxiv.org/abs/2502.10955', 'abstract': 'Attention is fundamental to both biological and artificial intelligence, yet research on animal attention and AI self attention remains largely disconnected. We propose a Recurrent Vision Transformer (Recurrent ViT) that integrates self-attention with recurrent memory, allowing both current inputs and stored information to guide attention allocation. Trained solely via sparse reward feedback on a spatially cued orientation change detection task, a paradigm used in primate studies, our model exhibits primate like signatures of attention, including improved accuracy and faster responses for cued stimuli that scale with cue validity. Analysis of self-attention maps reveals dynamic spatial prioritization with reactivation prior to expected changes, and targeted perturbations produce performance shifts similar to those observed in primate frontal eye fields and superior colliculus. These findings demonstrate that incorporating recurrent feedback into self attention can capture key aspects of primate visual attention.', 'abstract_zh': '将注意力机制融入递归记忆的视觉变换器：捕捉灵长类视觉注意力的关键方面', 'title_zh': '一种循环视觉变换器展示了灵长类视觉注意的特征'}
{'arxiv_id': 'arXiv:2502.10920', 'title': 'Do Deepfake Detectors Work in Reality?', 'authors': 'Simiao Ren, Hengwei Xu, Tsang Ng, Kidus Zewde, Shengkai Jiang, Ramini Desai, Disha Patil, Ning-Yau Cheng, Yining Zhou, Ragavi Muthukrishnan', 'link': 'https://arxiv.org/abs/2502.10920', 'abstract': 'Deepfakes, particularly those involving faceswap-based manipulations, have sparked significant societal concern due to their increasing realism and potential for misuse. Despite rapid advancements in generative models, detection methods have not kept pace, creating a critical gap in defense strategies. This disparity is further amplified by the disconnect between academic research and real-world applications, which often prioritize different objectives and evaluation criteria. In this study, we take a pivotal step toward bridging this gap by presenting a novel observation: the post-processing step of super-resolution, commonly employed in real-world scenarios, substantially undermines the effectiveness of existing deepfake detection methods. To substantiate this claim, we introduce and publish the first real-world faceswap dataset, collected from popular online faceswap platforms. We then qualitatively evaluate the performance of state-of-the-art deepfake detectors on real-world deepfakes, revealing that their accuracy approaches the level of random guessing. Furthermore, we quantitatively demonstrate the significant performance degradation caused by common post-processing techniques. By addressing this overlooked challenge, our study underscores a critical avenue for enhancing the robustness and practical applicability of deepfake detection methods in real-world settings.', 'abstract_zh': 'Deepfake检测方法在现实场景中的有效性受到常用超分辨率后处理步骤的严重削弱：一项现实世界数据驱动的研究', 'title_zh': '深度假面检测器在现实中有用吗？'}
{'arxiv_id': 'arXiv:2502.10908', 'title': 'Automatic Quality Assessment of First Trimester Crown-Rump-Length Ultrasound Images', 'authors': 'Sevim Cengiz, Ibraheem Hamdi, Mohammad Yaqub', 'link': 'https://arxiv.org/abs/2502.10908', 'abstract': 'Fetal gestational age (GA) is vital clinical information that is estimated during pregnancy in order to assess fetal growth. This is usually performed by measuring the crown-rump-length (CRL) on an ultrasound image in the Dating scan which is then correlated with fetal age and growth trajectory. A major issue when performing the CRL measurement is ensuring that the image is acquired at the correct view, otherwise it could be misleading. Although clinical guidelines specify the criteria for the correct CRL view, sonographers may not regularly adhere to such rules. In this paper, we propose a new deep learning-based solution that is able to verify the adherence of a CRL image to clinical guidelines in order to assess image quality and facilitate accurate estimation of GA. We first segment out important fetal structures then use the localized structures to perform a clinically-guided mapping that verifies the adherence of criteria. The segmentation method combines the benefits of Convolutional Neural Network (CNN) and the Vision Transformer (ViT) to segment fetal structures in ultrasound images and localize important fetal landmarks. For segmentation purposes, we compare our proposed work with UNet and show that our CNN/ViT-based method outperforms an optimized version of UNet. Furthermore, we compare the output of the mapping with classification CNNs when assessing the clinical criteria and the overall acceptability of CRL images. We show that the proposed mapping is not only explainable but also more accurate than the best performing classification CNNs.', 'abstract_zh': '胎儿妊娠龄（GA）在妊娠期间通过评估胎儿生长是至关重要的临床信息。通常通过在孕龄扫描中测量头臀长（CRL）并在超声图像上将其与胎儿年龄和生长轨迹相关联来进行。CRL测量时的主要问题是确保图像是从正确的视角获取的，否则可能会误导结果。尽管临床指南规定了正确的CRL视角的准则，但超声技师可能不会定期遵守这些规定。在本文中，我们提出了一种新的基于深度学习的解决方案，能够验证CRL图像是否符合临床指南，以评估图像质量并促进妊娠龄的准确估计。我们首先对胎儿的重要结构进行分割，然后使用这些结构进行符合临床指导的映射，以验证准则的遵守情况。分割方法结合了卷积神经网络（CNN）和视觉变压器（ViT）的优势，用于在超声图像中分割胎儿结构并定位重要胎儿标志点。为了分割目的，我们将我们提出的方案与UNet进行比较，表明我们基于CNN/ViT的方法优于优化后的UNet。此外，我们在评估临床准则和CRL图像整体可接受性时，将映射输出与分类CNN进行比较。我们展示，所提出的映射不仅具有可解释性，而且还比表现最佳的分类CNN更准确。', 'title_zh': '自动评估早期妊娠 Crown-Rump-Length 超声图像的质量'}
{'arxiv_id': 'arXiv:2502.10704', 'title': 'Occlusion-aware Non-Rigid Point Cloud Registration via Unsupervised Neural Deformation Correntropy', 'authors': 'Mingyang Zhao, Gaofeng Meng, Dong-Ming Yan', 'link': 'https://arxiv.org/abs/2502.10704', 'abstract': 'Non-rigid alignment of point clouds is crucial for scene understanding, reconstruction, and various computer vision and robotics tasks. Recent advancements in implicit deformation networks for non-rigid registration have significantly reduced the reliance on large amounts of annotated training data. However, existing state-of-the-art methods still face challenges in handling occlusion scenarios. To address this issue, this paper introduces an innovative unsupervised method called Occlusion-Aware Registration (OAR) for non-rigidly aligning point clouds. The key innovation of our method lies in the utilization of the adaptive correntropy function as a localized similarity measure, enabling us to treat individual points distinctly. In contrast to previous approaches that solely minimize overall deviations between two shapes, we combine unsupervised implicit neural representations with the maximum correntropy criterion to optimize the deformation of unoccluded regions. This effectively avoids collapsed, tearing, and other physically implausible results. Moreover, we present a theoretical analysis and establish the relationship between the maximum correntropy criterion and the commonly used Chamfer distance, highlighting that the correntropy-induced metric can be served as a more universal measure for point cloud analysis. Additionally, we introduce locally linear reconstruction to ensure that regions lacking correspondences between shapes still undergo physically natural deformations. Our method achieves superior or competitive performance compared to existing approaches, particularly when dealing with occluded geometries. We also demonstrate the versatility of our method in challenging tasks such as large deformations, shape interpolation, and shape completion under occlusion disturbances.', 'abstract_zh': '非刚性点云对齐对于场景理解、重建以及各种计算机视觉和机器人任务至关重要。隐式变形网络的最新进展显著减少了对大量标注训练数据的依赖。然而，现有最先进的方法在处理遮挡场景时仍面临挑战。为解决这一问题，本文提出了一种新的无监督方法——感知遮挡对齐（OAR），用于非刚性对齐点云。该方法的关键创新在于利用自适应误差核函数作为局部相似性度量，使得点可以被单独处理。与仅最小化两个形状总体偏差的先前方法不同，我们结合无监督隐式神经表示和最大误差核准则来优化未遮挡区域的变形，从而避免了坍塌、撕裂等物理上不可能的结果。此外，我们提供了理论分析，建立了最大误差核准则与常用的切线距离之间的关系，指出误差核诱导的度量可以作为点云分析的更通用度量。我们还引入了局部线性重建，确保缺乏形状对应区域仍能进行自然变形。与现有方法相比，我们的方法在处理遮挡几何形状时表现出优越或竞争性能，同时在复杂任务如大形变、形状插值和遮挡干扰下的形状补全方面展示了其灵活性。', 'title_zh': '基于无监督神经变形核函数的 occlusion-aware 非刚性点云配准'}
{'arxiv_id': 'arXiv:2502.10698', 'title': 'Superpose Singular Features for Model Merging', 'authors': 'Haiquan Qiu, You Wu, Quanming Yao', 'link': 'https://arxiv.org/abs/2502.10698', 'abstract': 'Model merging is a critical technique for combining the capabilities of multiple fine-tuned models without requiring additional training. While existing methods treat parameters as vectors, they overlook the intrinsic structure of linear transformation matrices - the core components that comprise the majority of model parameters. These matrices are fundamental to neural networks, mapping input representations to output features through linear combinations. Motivated by the linear representation hypothesis, we introduce task matrix and propose to Superpose Features from Task Matrix (SFTM), a novel approach that superposes features from individual task models into a merged model. SFTM employs singular value decomposition to identify feature bases of linear transformation matrices and solves a linear system to optimally combine them while preserving input-output mappings from individual task models. Extensive experiments on vision transformers and language models demonstrate that our method consistently outperforms existing methods, achieving superior performance and enhanced out-of-distribution generalization.', 'abstract_zh': '模型合并是结合多个细调模型的能力的关键技术，无需额外训练。虽然现有方法将参数视为向量，但它们忽略了线性变换矩阵的内在结构——这些矩阵构成了模型参数的主要部分。线性变换矩阵对神经网络至关重要，它们通过线性组合将输入表示映射到输出特征。受线性表示假说的启发，我们引入了任务矩阵，并提出了一种新的方法——基于任务矩阵叠加特征（SFTM），该方法将单个任务模型的特征叠加到合并模型中。SFTM利用奇异值分解识别线性变换矩阵的特征基，并通过求解线性系统以最优方式将它们组合起来，同时保留单个任务模型的输入-输出映射。在视觉变压器和语言模型上的广泛实验表明，我们的方法在所有方法中表现最好，实现了一流的性能和增强的离分布泛化能力。', 'title_zh': '合并模型时叠加奇异特征'}
{'arxiv_id': 'arXiv:2502.10614', 'title': 'Optimizing CNN Architectures for Advanced Thoracic Disease Classification', 'authors': 'Tejas Mirthipati', 'link': 'https://arxiv.org/abs/2502.10614', 'abstract': 'Machine learning, particularly convolutional neural networks (CNNs), has shown promise in medical image analysis, especially for thoracic disease detection using chest X-ray images. In this study, we evaluate various CNN architectures, including binary classification, multi-label classification, and ResNet50 models, to address challenges like dataset imbalance, variations in image quality, and hidden biases. We introduce advanced preprocessing techniques such as principal component analysis (PCA) for image compression and propose a novel class-weighted loss function to mitigate imbalance issues. Our results highlight the potential of CNNs in medical imaging but emphasize that issues like unbalanced datasets and variations in image acquisition methods must be addressed for optimal model performance.', 'abstract_zh': '机器学习，特别是卷积神经网络（CNNs），在医疗图像分析中展现出潜力，特别是在使用胸部X射线图像检测胸腔疾病方面的应用。本研究评估了多种CNN架构，包括二元分类、多标签分类和ResNet50模型，以应对数据集不平衡、图像质量差异和隐藏偏见等挑战。我们引入了高级预处理技术，如主成分分析（PCA）进行图像压缩，并提出了一种新颖的类别加权损失函数以缓解不平衡问题。我们的结果强调了CNN在医疗成像中的潜力，但也指出了必须解决数据集不平衡和图像采集方法差异等问题以实现最佳模型性能。', 'title_zh': '优化CNN架构以进行高级胸腔疾病分类'}
{'arxiv_id': 'arXiv:2502.10559', 'title': 'SAMRI-2: A Memory-based Model for Cartilage and Meniscus Segmentation in 3D MRIs of the Knee Joint', 'authors': 'Danielle L. Ferreira, Bruno A. A. Nunes, Xuzhe Zhang, Laura Carretero Gomez, Maggie Fung, Ravi Soni', 'link': 'https://arxiv.org/abs/2502.10559', 'abstract': 'Accurate morphometric assessment of cartilage-such as thickness/volume-via MRI is essential for monitoring knee osteoarthritis. Segmenting cartilage remains challenging and dependent on extensive expert-annotated datasets, which are heavily subjected to inter-reader variability. Recent advancements in Visual Foundational Models (VFM), especially memory-based approaches, offer opportunities for improving generalizability and robustness. This study introduces a deep learning (DL) method for cartilage and meniscus segmentation from 3D MRIs using interactive, memory-based VFMs. To improve spatial awareness and convergence, we incorporated a Hybrid Shuffling Strategy (HSS) during training and applied a segmentation mask propagation technique to enhance annotation efficiency. We trained four AI models-a CNN-based 3D-VNet, two automatic transformer-based models (SaMRI2D and SaMRI3D), and a transformer-based promptable memory-based VFM (SAMRI-2)-on 3D knee MRIs from 270 patients using public and internal datasets and evaluated on 57 external cases, including multi-radiologist annotations and different data acquisitions. Model performance was assessed against reference standards using Dice Score (DSC) and Intersection over Union (IoU), with additional morphometric evaluations to further quantify segmentation accuracy. SAMRI-2 model, trained with HSS, outperformed all other models, achieving an average DSC improvement of 5 points, with a peak improvement of 12 points for tibial cartilage. It also demonstrated the lowest cartilage thickness errors, reducing discrepancies by up to threefold. Notably, SAMRI-2 maintained high performance with as few as three user clicks per volume, reducing annotation effort while ensuring anatomical precision. This memory-based VFM with spatial awareness offers a novel approach for reliable AI-assisted knee MRI segmentation, advancing DL in musculoskeletal imaging.', 'abstract_zh': '通过MRI精确评估关节软骨厚度/体积对于监测膝关节骨关节炎至关重要。基于交互记忆基础视觉主体模型的软骨和半月板分割方法提高了分割稳健性和通用性。', 'title_zh': 'SAMRI-2：一种基于记忆的模型，用于膝关节3D MRI中的软骨和半月板分割'}
{'arxiv_id': 'arXiv:2502.10475', 'title': 'X-SG$^2$S: Safe and Generalizable Gaussian Splatting with X-dimensional Watermarks', 'authors': 'Zihang Cheng, Huiping Zhuang, Chun Li, Xin Meng, Ming Li, Fei Richard Yu', 'link': 'https://arxiv.org/abs/2502.10475', 'abstract': '3D Gaussian Splatting (3DGS) has been widely used in 3D reconstruction and 3D generation. Training to get a 3DGS scene often takes a lot of time and resources and even valuable inspiration. The increasing amount of 3DGS digital asset have brought great challenges to the copyright protection. However, it still lacks profound exploration targeted at 3DGS. In this paper, we propose a new framework X-SG$^2$S which can simultaneously watermark 1 to 3D messages while keeping the original 3DGS scene almost unchanged. Generally, we have a X-SG$^2$S injector for adding multi-modal messages simultaneously and an extractor for extract them. Specifically, we first split the watermarks into message patches in a fixed manner and sort the 3DGS points. A self-adaption gate is used to pick out suitable location for watermarking. Then use a XD(multi-dimension)-injection heads to add multi-modal messages into sorted 3DGS points. A learnable gate can recognize the location with extra messages and XD-extraction heads can restore hidden messages from the location recommended by the learnable gate. Extensive experiments demonstrated that the proposed X-SG$^2$S can effectively conceal multi modal messages without changing pretrained 3DGS pipeline or the original form of 3DGS parameters. Meanwhile, with simple and efficient model structure and high practicality, X-SG$^2$S still shows good performance in hiding and extracting multi-modal inner structured or unstructured messages. X-SG$^2$S is the first to unify 1 to 3D watermarking model for 3DGS and the first framework to add multi-modal watermarks simultaneous in one 3DGS which pave the wave for later researches.', 'abstract_zh': '基于3D高斯喷洒的多模态水印框架X-SG$^2$S', 'title_zh': 'X-SG$^2$S: 安全且通用的高维水印高斯散列'}
{'arxiv_id': 'arXiv:2502.10421', 'title': 'DRiVE: Dynamic Recognition in VEhicles using snnTorch', 'authors': 'Heerak Vora, Param Pathak, Parul Bakaraniya', 'link': 'https://arxiv.org/abs/2502.10421', 'abstract': "Spiking Neural Networks (SNNs) mimic biological brain activity, processing data efficiently through an event-driven design, wherein the neurons activate only when inputs exceed specific thresholds. Their ability to track voltage changes over time via membrane potential dynamics helps retain temporal information. This study combines SNNs with PyTorch's adaptable framework, snnTorch, to test their potential for image-based tasks. We introduce DRiVE, a vehicle detection model that uses spiking neuron dynamics to classify images, achieving 94.8% accuracy and a near-perfect 0.99 AUC score. These results highlight DRiVE's ability to distinguish vehicle classes effectively, challenging the notion that SNNs are limited to temporal data. As interest grows in energy-efficient neural models, DRiVE's success emphasizes the need to refine SNN optimization for visual tasks. This work encourages broader exploration of SNNs in scenarios where conventional networks struggle, particularly for real-world applications requiring both precision and efficiency.", 'abstract_zh': 'Spiking Neural Networks (SNNs)模拟生物脑活动，通过事件驱动的设计高效处理数据，其中神经元仅在输入超过特定阈值时激活。它们通过膜电位动力学追踪电压变化，有助于保留时间信息。本研究将SNNs与PyTorch的可拓展框架snnTorch结合，测试其在基于图像任务中的潜力。我们引入DRiVE，一种使用突触神经元动力学进行图像分类的车辆检测模型，实现了94.8%的准确率和接近完美的0.99 AUC分数。这些结果突显了DRiVE有效区分车辆类别的能力，挑战了SNNs仅限于时序数据的观念。随着对高效神经模型兴趣的增长，DRiVE的成功强调了需要为视觉任务优化SNNs的重要性。本研究鼓励在传统网络表现不佳的场景中更广泛地探索SNNs，尤其是对于需要精确性和效率的现实世界应用。', 'title_zh': 'DRiVE: 动态识别在车辆中的应用 using snnTorch'}
