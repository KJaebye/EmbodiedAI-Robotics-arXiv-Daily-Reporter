{'arxiv_id': 'arXiv:2502.16863', 'title': 'Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment', 'authors': 'Kartik Nagpal, Dayi Dong, Jean-Baptiste Bouvier, Negar Mehr', 'link': 'https://arxiv.org/abs/2502.16863', 'abstract': "Recent work, spanning from autonomous vehicle coordination to in-space assembly, has shown the importance of learning collaborative behavior for enabling robots to achieve shared goals. A common approach for learning this cooperative behavior is to utilize the centralized-training decentralized-execution paradigm. However, this approach also introduces a new challenge: how do we evaluate the contributions of each agent's actions to the overall success or failure of the team. This credit assignment problem has remained open, and has been extensively studied in the Multi-Agent Reinforcement Learning literature. In fact, humans manually inspecting agent behavior often generate better credit evaluations than existing methods. We combine this observation with recent works which show Large Language Models demonstrate human-level performance at many pattern recognition tasks. Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel LLM-MCA method. Our approach utilizes a centralized LLM reward-critic which numerically decomposes the environment reward based on the individualized contribution of each agent in the scenario. We then update the agents' policy networks based on this feedback. We also propose an extension LLM-TACA where our LLM critic performs explicit task assignment by passing an intermediary goal directly to each agent policy in the scenario. Both our methods far outperform the state-of-the-art on a variety of benchmarks, including Level-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark which incorporates collision-related safety constraints. As an artifact of our methods, we generate large trajectory datasets with each timestep annotated with per-agent reward information, as sampled from our LLM critics.", 'abstract_zh': '最近的研究，从自主车辆协调到太空组装，凸显了学习协作行为的重要性，以使机器人能够共同实现目标。一种常见的学习这种协同行为的方法是利用集中训练分散执行的范式。然而，这种方法也引入了一个新的挑战：如何评估每个代理的动作对团队整体成功或失败的贡献。这个问题仍然没有解决，并在多代理强化学习文献中得到了广泛研究。事实上，人类手动检查代理行为往往能生成比现有方法更好的信用评估。我们结合了这一观察与最近的研究，这些研究显示大型语言模型在许多模式识别任务中达到了人类水平的性能。我们的核心思想是将信用分配重新公式化为目标序列改进和归因这两个模式识别问题，这激励了我们的LLM-MCA方法。我们的方法利用了一个集中式的LLM奖励批评家，基于场景中每个代理的个体贡献对环境奖励进行了数值分解。然后，根据这一反馈更新代理的策略网络。我们还提出了扩展的LLM-TACA方法，其中我们的LLM批评家通过将中介目标直接传递给场景中的每个代理策略，进行了显式的任务分配。我们的方法在多种基准测试中远超现有最佳方法，包括层次化采集、机器人仓库和我们的新太空世界基准测试，该基准测试包含了与碰撞相关的安全约束。作为方法的一个副产品，我们生成了大量带有每个时间步长每个代理奖励信息的轨迹数据集。', 'title_zh': '利用大型语言模型进行有效可解释的多代理信用分配'}
{'arxiv_id': 'arXiv:2502.17419', 'title': 'From System 1 to System 2: A Survey of Reasoning Large Language Models', 'authors': 'Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, Cheng-Lin Liu', 'link': 'https://arxiv.org/abs/2502.17419', 'abstract': "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\href{this https URL}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.", 'abstract_zh': '实现人类级智能需要优化从快速直觉的System 1向更慢且更慎重的System 2推理的过渡。虽然System 1在快速启发式决策方面表现出色，但System 2依赖于逻辑推理以做出更准确的判断并减少偏差。基础大型语言模型在快速决策方面表现出色，但在复杂推理方面仍显不足，因为它们尚未完全采纳真正的System 2思考所包含的逐步分析特征。近年来，像OpenAI的o1/o3和DeepSeek的R1这样的推理LLM在数学和编程等领域展示了专家级性能，其推理方式与System 2的仔细推理高度相似，展示了类似人类的认知能力。本综述首先对基础LLM的进步和System 2技术的早期发展进行了简要概述，探讨了它们的结合如何为推理LLM铺平道路。随后，我们讨论了如何构建推理LLM，分析了它们的特性、使高级推理成为可能的关键方法以及各种推理LLM的发展演变。此外，我们还提供了推理基准的概述，深入比较了代表性推理LLM的性能。最后，我们探讨了推进推理LLM的有前景的方向，并维护了一个实时的GitHub Repository以跟踪最新进展。我们希望这份综述能成为一个有价值的资源，激发创新并推动这一快速发展的领域前进。', 'title_zh': '从系统1到系统2：大型语言模型推理综述'}
{'arxiv_id': 'arXiv:2502.17392', 'title': 'Emoti-Attack: Zero-Perturbation Adversarial Attacks on NLP Systems via Emoji Sequences', 'authors': 'Yangshijie Zhang', 'link': 'https://arxiv.org/abs/2502.17392', 'abstract': 'Deep neural networks (DNNs) have achieved remarkable success in the field of natural language processing (NLP), leading to widely recognized applications such as ChatGPT. However, the vulnerability of these models to adversarial attacks remains a significant concern. Unlike continuous domains like images, text exists in a discrete space, making even minor alterations at the sentence, word, or character level easily perceptible to humans. This inherent discreteness also complicates the use of conventional optimization techniques, as text is non-differentiable. Previous research on adversarial attacks in text has focused on character-level, word-level, sentence-level, and multi-level approaches, all of which suffer from inefficiency or perceptibility issues due to the need for multiple queries or significant semantic shifts.\nIn this work, we introduce a novel adversarial attack method, Emoji-Attack, which leverages the manipulation of emojis to create subtle, yet effective, perturbations. Unlike character- and word-level strategies, Emoji-Attack targets emojis as a distinct layer of attack, resulting in less noticeable changes with minimal disruption to the text. This approach has been largely unexplored in previous research, which typically focuses on emoji insertion as an extension of character-level attacks. Our experiments demonstrate that Emoji-Attack achieves strong attack performance on both large and small models, making it a promising technique for enhancing adversarial robustness in NLP systems.', 'abstract_zh': '基于表情符号的新型对抗攻击方法：Emoji-Attack', 'title_zh': 'Emoti-Attack：通过 Emoji 序列对 NLP 系统进行零扰动对抗攻击'}
{'arxiv_id': 'arXiv:2502.17297', 'title': 'Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts', 'authors': 'Zhenghao Liu, Xingsheng Zhu, Tianshuo Zhou, Xinyi Zhang, Xiaoyuan Yi, Yukun Yan, Yu Gu, Ge Yu, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.17297', 'abstract': 'This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a benchmark designed to evaluate the effectiveness of Multi-modal Large Language Models (MLLMs) in leveraging knowledge from multi-modal retrieval documents. The benchmark comprises four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. All tasks are set in an open-domain setting, requiring RAG models to retrieve query-relevant information from a multi-modal document collection and use it as input context for RAG modeling. To enhance the context utilization capabilities of MLLMs, we also introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an instruction tuning method that optimizes MLLMs within multi-modal contexts. Our experiments show that MM-RAIT improves the performance of RAG systems by enabling them to effectively learn from multi-modal contexts. All data and code are available at this https URL.', 'abstract_zh': '多模态检索增强生成（M^2RAG）基准：用于评估多模态大规模语言模型的知识利用效果', 'title_zh': '多模态上下文中检索增强生成的基准研究'}
{'arxiv_id': 'arXiv:2502.17216', 'title': 'Making LLMs Reason? The Intermediate Language Problem in Neurosymbolic Approaches', 'authors': 'Alexander Beiser, David Penz', 'link': 'https://arxiv.org/abs/2502.17216', 'abstract': 'Logical reasoning tasks manifest themselves as a challenge to Large Language Models (LLMs). Neurosymbolic approaches use LLMs to translate logical reasoning problems formulated in natural language into a formal intermediate language. Subsequently, the usage of symbolic reasoners yields reliable solving thereof. However, LLMs often fail in translation due to poorly chosen intermediate languages.\nWe introduce the intermediate language problem, which is the problem of choosing a suitable formal language representation for neurosymbolic approaches. Theoretically, we argue that its origins lie in the inability of LLMs to distinguish syntax from semantics and the relative independence of the problem from its representation. We showcase its existence experimentally by contrasting two intermediate languages, Answer Set Programming and the Python Knowledge Engine. In addition, we demonstrate the effects of varying degrees of supplementary context information. Our results show a maximum difference in overall-accuracy of 53.20% and 49.26% in execution-accuracy. When using the GPT4o-mini LLM we beat the state-of-the-art in overall-accuracy on the ProntoQA dataset by 21.20% and by 50.50% on the ProofWriter dataset.', 'abstract_zh': '中间语言问题：神经符号方法中合适形式语言表示的选择挑战', 'title_zh': '让大规模语言模型具备推理能力？神经符号方法中的中间语言问题'}
{'arxiv_id': 'arXiv:2502.17139', 'title': 'CodeSwift: Accelerating LLM Inference for Efficient Code Generation', 'authors': 'Qianhui Zhao, Li Zhang, Fang Liu, Xiaoli Lian, Qiaoyuanhe Meng, Ziqian Jiao, Zetong Zhou, Borui Zhang, Runlin Guo, Jia Li', 'link': 'https://arxiv.org/abs/2502.17139', 'abstract': 'Code generation is a latency-sensitive task that demands high timeliness, but the autoregressive decoding mechanism of Large Language Models (LLMs) leads to poor inference efficiency. Existing LLM inference acceleration methods mainly focus on standalone functions using only built-in components. Moreover, they treat code like natural language sequences, ignoring its unique syntax and semantic characteristics. As a result, the effectiveness of these approaches in code generation tasks remains limited and fails to align with real-world programming scenarios. To alleviate this issue, we propose CodeSwift, a simple yet highly efficient inference acceleration approach specifically designed for code generation, without comprising the quality of the output. CodeSwift constructs a multi-source datastore, providing access to both general and project-specific knowledge, facilitating the retrieval of high-quality draft sequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval timing, and enhances efficiency through parallel retrieval and a context- and LLM preference-aware cache. Experimental results show that CodeSwift can reach up to 2.53x and 2.54x speedup compared to autoregressive decoding in repository-level and standalone code generation tasks, respectively, outperforming state-of-the-art inference acceleration approaches by up to 88%.', 'abstract_zh': 'CodeSwift：一种专门用于代码生成的高效推理加速方法', 'title_zh': 'CodeSwift: 加速 Large Language Model 推理以实现高效的代码生成'}
{'arxiv_id': 'arXiv:2502.17136', 'title': 'Evaluating the Effectiveness of Large Language Models in Automated News Article Summarization', 'authors': 'Lionel Richy Panlap Houamegni, Fatih Gedikli', 'link': 'https://arxiv.org/abs/2502.17136', 'abstract': "The automation of news analysis and summarization presents a promising solution to the challenge of processing and analyzing vast amounts of information prevalent in today's information society. Large Language Models (LLMs) have demonstrated the capability to transform vast amounts of textual data into concise and easily comprehensible summaries, offering an effective solution to the problem of information overload and providing users with a quick overview of relevant information. A particularly significant application of this technology lies in supply chain risk analysis. Companies must monitor the news about their suppliers and respond to incidents for several critical reasons, including compliance with laws and regulations, risk management, and maintaining supply chain resilience. This paper develops an automated news summarization system for supply chain risk analysis using LLMs. The proposed solution aggregates news from various sources, summarizes them using LLMs, and presents the condensed information to users in a clear and concise format. This approach enables companies to optimize their information processing and make informed decisions. Our study addresses two main research questions: (1) Are LLMs effective in automating news summarization, particularly in the context of supply chain risk analysis? (2) How effective are various LLMs in terms of readability, duplicate detection, and risk identification in their summarization quality? In this paper, we conducted an offline study using a range of publicly available LLMs at the time and complemented it with a user study focused on the top performing systems of the offline experiments to evaluate their effectiveness further. Our results demonstrate that LLMs, particularly Few-Shot GPT-4o mini, offer significant improvements in summary quality and risk identification.", 'abstract_zh': '新闻分析与总结的自动化为处理和分析当今信息社会中大量信息提供了有前景的解决方案。大型语言模型（LLMs）展现了将大量文本数据转换为简洁易懂的摘要的能力，为此类信息过载问题提供了有效解决方案，为用户提供相关信息的快速概览。这一技术在供应链风险分析中的应用尤为重要。企业必须监控供应商的相关新闻并响应突发事件，原因包括遵守法律法规、风险管理以及维护供应链韧性。本文开发了一个使用LLMs的自动化新闻摘要系统，以用于供应链风险分析。该解决方案聚合来自多种来源的新闻，使用LLMs进行总结，并以清晰简洁的形式将浓缩信息呈现给用户。这种方法使公司能够优化信息处理并做出知情决策。本文探讨了两个主要研究问题：（1）LLMs在供应链风险分析的背景下是否有效用于自动摘要新闻？（2）各种LLMs在可读性、重复检测和风险识别方面的总结质量如何？在此论文中，我们使用当时可用的多种公开LLMs进行了离线研究，并以针对离线实验表现最佳系统的用户研究作为补充，进一步评估其效果。研究结果表明，尤其是Few-Shot GPT-4o mini，LLMs在摘要质量和风险识别方面提供了显著改进。', 'title_zh': '评估大型语言模型在自动新闻文章摘要生成中的有效性'}
{'arxiv_id': 'arXiv:2502.16879', 'title': 'A Multi-LLM-Agent-Based Framework for Economic and Public Policy Analysis', 'authors': 'Yuzhi Hao, Danyang Xie', 'link': 'https://arxiv.org/abs/2502.16879', 'abstract': "This paper pioneers a novel approach to economic and public policy analysis by leveraging multiple Large Language Models (LLMs) as heterogeneous artificial economic agents. We first evaluate five LLMs' economic decision-making capabilities in solving two-period consumption allocation problems under two distinct scenarios: with explicit utility functions and based on intuitive reasoning. While previous research has often simulated heterogeneity by solely varying prompts, our approach harnesses the inherent variations in analytical capabilities across different LLMs to model agents with diverse cognitive traits. Building on these findings, we construct a Multi-LLM-Agent-Based (MLAB) framework by mapping these LLMs to specific educational groups and corresponding income brackets. Using interest-income taxation as a case study, we demonstrate how the MLAB framework can simulate policy impacts across heterogeneous agents, offering a promising new direction for economic and public policy analysis by leveraging LLMs' human-like reasoning capabilities and computational power.", 'abstract_zh': '本文通过利用多种大型语言模型（LLMs）作为异质的人工经济代理，开创性地提出了一种新的经济学和公共政策分析方法。我们首先评估了五种LLM在解决两种期消费分配问题下的经济决策能力，这两种问题在两种不同的场景下进行：具有明确的效用函数和基于直观推理。尽管以往的研究常常通过仅仅改变提示来模拟异质性，我们的方法则利用不同LLM在分析能力上的固有差异，来建模具有不同认知特征的代理。在此基础上，我们构建了一个多LLM-代理基础（MLAB）框架，将这些LLM映射到特定的教育群体和对应的收入区间。以兴趣-收入税收为例，我们展示了MLAB框架如何模拟跨异质代理的政策影响，提供了一种利用LLMs的人类级推理能力和计算能力来进行经济学和公共政策分析的有前景的新方向。', 'title_zh': '基于多大语言模型代理的经济与公共政策分析框架'}
{'arxiv_id': 'arXiv:2502.16810', 'title': 'Grounded Persuasive Language Generation for Automated Marketing', 'authors': 'Jibang Wu, Chenghao Yang, Simon Mahns, Chaoqi Wang, Hao Zhu, Fei Fang, Haifeng Xu', 'link': 'https://arxiv.org/abs/2502.16810', 'abstract': 'This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin. Our findings suggest a promising LLM-based agentic framework to automate large-scale targeted marketing while ensuring responsible generation using only facts.', 'abstract_zh': '本文提出了一种代理框架，利用大语言模型（LLMs）自动化生成有说服力且基于事实的营销内容，并以房地产 listings 描述作为主要应用领域。该方法旨在使生成的内容与用户偏好一致，同时突出有用的事实属性。该代理包括三个关键模块：(1) 语境化模块，模仿专家人类行为预测可营销特征；(2) 个性化模块，使内容与用户偏好一致；(3) 营销模块，确保内容的事实准确性并包含本地化特征。我们在房地产营销领域进行了系统的人类被试实验，重点关注潜在购房者组群。实验结果表明，我们方法生成的营销描述在清晰程度上优于人类专家撰写的描述。我们的研究结果表明，利用事实进行负责的大规模定向营销的LLM基于代理框架具有前景。', 'title_zh': '基于产品描述的说服性语言生成在自动营销中的应用'}
{'arxiv_id': 'arXiv:2502.16690', 'title': 'From Text to Space: Mapping Abstract Spatial Models in LLMs during a Grid-World Navigation Task', 'authors': 'Nicolas Martorell', 'link': 'https://arxiv.org/abs/2502.16690', 'abstract': 'Understanding how large language models (LLMs) represent and reason about spatial information is crucial for building robust agentic systems that can navigate real and simulated environments. In this work, we investigate the influence of different text-based spatial representations on LLM performance and internal activations in a grid-world navigation task. By evaluating models of various sizes on a task that requires navigating toward a goal, we examine how the format used to encode spatial information impacts decision-making. Our experiments reveal that cartesian representations of space consistently yield higher success rates and path efficiency, with performance scaling markedly with model size. Moreover, probing LLaMA-3.1-8B revealed subsets of internal units, primarily located in intermediate layers, that robustly correlate with spatial features, such as the position of the agent in the grid or action correctness, regardless of how that information is represented, and are also activated by unrelated spatial reasoning tasks. This work advances our understanding of how LLMs process spatial information and provides valuable insights for developing more interpretable and robust agentic AI systems.', 'abstract_zh': '理解大型语言模型（LLMs）如何表示和推理空间信息对于构建能够在现实和模拟环境中导航的 robust 的代理系统至关重要。在本工作中，我们调查了不同文本基础的空间表示形式对LLM在格网世界导航任务中的性能和内部激活的影响。通过在要求导航至目标的任务中评估不同大小的模型，我们探讨了用于编码空间信息的格式如何影响决策制定。我们的实验表明，空间的笛卡尔表示形式始终能获得更高的成功率和路径效率，且性能随着模型大小的增加而显著提升。此外，对LLaMA-3.1-8B的探查发现了一组内部单元，主要位于中间层，它们与空间特征（如网格中代理的位置或动作的正确性）之间存在稳健的关联，并且无论信息以何种形式表示，这些单元都会被激活，同时也会被无关的空间推理任务激活。这项工作推进了我们对LLMs如何处理空间信息的理解，并为开发更可解释和 robust 的代理AI系统提供了宝贵见解。', 'title_zh': '从文本到空间：在网格世界导航任务中将抽象空间模型映射到LLMs'}
{'arxiv_id': 'arXiv:2502.16662', 'title': 'Saarthi: The First AI Formal Verification Engineer', 'authors': 'Aman Kumar, Deepak Narayan Gadde, Keerthan Kopparam Radhakrishna, Djones Lettnin', 'link': 'https://arxiv.org/abs/2502.16662', 'abstract': "Recently, Devin has made a significant buzz in the Artificial Intelligence (AI) community as the world's first fully autonomous AI software engineer, capable of independently developing software code. Devin uses the concept of agentic workflow in Generative AI (GenAI), which empowers AI agents to engage in a more dynamic, iterative, and self-reflective process. In this paper, we present a similar fully autonomous AI formal verification engineer, Saarthi, capable of verifying a given RTL design end-to-end using an agentic workflow. With Saarthi, verification engineers can focus on more complex problems, and verification teams can strive for more ambitious goals. The domain-agnostic implementation of Saarthi makes it scalable for use across various domains such as RTL design, UVM-based verification, and others.", 'abstract_zh': '最近，Devin已成为人工智能（AI）领域的一个重要话题，作为全球首个完全自主的AI软件工程师，能够独立开发软件代码。Devin利用生成型AI（GenAI）中的代理工作流概念，使AI代理能够在更具动态性、迭代性和自我反思的过程中进行协作。本文介绍了一个类似完全自主的AI形式验证工程师Saarthi，能够使用代理工作流端到端验证给定的RTL设计。借助Saarthi，形式验证工程师可以专注于更复杂的问题，而形式验证团队可以追求更宏伟的目标。Saarthi的跨域实现使其适用于诸如RTL设计、基于UVM的形式验证等多个领域。', 'title_zh': 'Saarthi: 首个AI形式验证工程师'}
{'arxiv_id': 'arXiv:2502.16606', 'title': 'Reasoning about Affordances: Causal and Compositional Reasoning in LLMs', 'authors': 'Magnus F. Gjerde, Vanessa Cheung, David Lagnado', 'link': 'https://arxiv.org/abs/2502.16606', 'abstract': "With the rapid progress of Large Language Models (LLMs), it becomes increasingly important to understand their abilities and limitations. In two experiments, we investigate the causal and compositional reasoning abilities of LLMs and humans in the domain of object affordances, an area traditionally linked to embodied cognition. The tasks, designed from scratch to avoid data contamination, require decision-makers to select unconventional objects to replace a typical tool for a particular purpose, such as using a table tennis racket to dig a hole. In Experiment 1, we evaluated GPT-3.5 and GPT-4o, finding that GPT-4o, when given chain-of-thought prompting, performed on par with human participants, while GPT-3.5 lagged significantly. In Experiment 2, we introduced two new conditions, Distractor (more object choices, increasing difficulty) and Image (object options presented visually), and evaluated Claude 3 Sonnet and Claude 3.5 Sonnet in addition to the GPT models. The Distractor condition significantly impaired performance across humans and models, although GPT-4o and Claude 3.5 still performed well above chance. Surprisingly, the Image condition had little impact on humans or GPT-4o, but significantly lowered Claude 3.5's accuracy. Qualitative analysis showed that GPT-4o and Claude 3.5 have a stronger ability than their predecessors to identify and flexibly apply causally relevant object properties. The improvement from GPT-3.5 and Claude 3 to GPT-4o and Claude 3.5 suggests that models are increasingly capable of causal and compositional reasoning in some domains, although further mechanistic research is necessary to understand how LLMs reason.", 'abstract_zh': '随着大型语言模型（LLMs）的快速进步，理解其能力与局限性变得越来越重要。在两项实验中，我们探讨了LLMs和人类在物体功能领域的因果与组合推理能力，该领域传统上与知觉认知相关。实验任务从头设计，以避免数据污染，要求决策者选择非典型物体替代特定用途的常规工具，例如使用网球拍挖洞。在实验1中，我们评估了GPT-3.5和GPT-4o，发现当给予逐步思考提示时，GPT-4o的表现与人类参与者相当，而GPT-3.5显著落后。在实验2中，我们引入了两个新条件，干扰（更多的物体选择，增加难度）和图片（以视觉方式呈现物体选项），并评估了Claude 3 Sonnet和Claude 3.5 Sonnet，除GPT模型外。干扰条件显著降低了人类和模型的表现，尽管GPT-4o和Claude 3.5仍表现远超偶然水平。令人惊讶的是，图片条件对人类和GPT-4o几乎没有影响，但显著降低了Claude 3.5的准确性。定性分析显示，GPT-4o和Claude 3.5比其前代产品更擅长识别和灵活应用因果相关物体属性。从GPT-3.5和Claude 3到GPT-4o和Claude 3.5的改进表明，模型在某些领域中越来越能够进行因果与组合推理，但需要进一步机制性研究来理解LLMs是如何推理的。', 'title_zh': '关于 affordances 的推理：LLMs 中的因果与组合推理'}
{'arxiv_id': 'arXiv:2502.16593', 'title': 'Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images', 'authors': 'Yubo Wang, Jianting Tang, Chaohu Liu, Linli Xu', 'link': 'https://arxiv.org/abs/2502.16593', 'abstract': "Large vision-language models (LVLMs) have demonstrated remarkable image understanding and dialogue capabilities, allowing them to handle a variety of visual question answering tasks. However, their widespread availability raises concerns about unauthorized usage and copyright infringement, where users or individuals can develop their own LVLMs by fine-tuning published models. In this paper, we propose a novel method called Parameter Learning Attack (PLA) for tracking the copyright of LVLMs without modifying the original model. Specifically, we construct adversarial images through targeted attacks against the original model, enabling it to generate specific outputs. To ensure these attacks remain effective on potential fine-tuned models to trigger copyright tracking, we allow the original model to learn the trigger images by updating parameters in the opposite direction during the adversarial attack process. Notably, the proposed method can be applied after the release of the original model, thus not affecting the model's performance and behavior. To simulate real-world applications, we fine-tune the original model using various strategies across diverse datasets, creating a range of models for copyright verification. Extensive experiments demonstrate that our method can more effectively identify the original copyright of fine-tuned models compared to baseline methods. Therefore, this work provides a powerful tool for tracking copyrights and detecting unlicensed usage of LVLMs.", 'abstract_zh': '大型 vision-language 模型的版权追踪方法：无需修改原始模型的参数学习攻击（PLA）', 'title_zh': '通过参数学习对抗样本追踪大型视觉-语言模型的版权'}
{'arxiv_id': 'arXiv:2502.16402', 'title': 'Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language Models for Navigation Applications', 'authors': 'Feng Ma, Xiu-min Wang, Chen Chen, Xiao-bin Xu, Xin-ping Yan', 'link': 'https://arxiv.org/abs/2502.16402', 'abstract': 'Existing navigation decision support systems often perform poorly when handling non-predefined navigation scenarios. Leveraging the generalization capabilities of large language model (LLM) in handling unknown scenarios, this research proposes a dual-core framework for LLM applications to address this issue. Firstly, through ReAct-based prompt engineering, a larger LLM core decomposes intricate navigation tasks into manageable sub-tasks, which autonomously invoke corresponding external tools to gather relevant information, using this feedback to mitigate the risk of LLM hallucinations. Subsequently, a fine-tuned and compact LLM core, acting like a first-mate is designed to process such information and unstructured external data, then to generates context-aware recommendations, ultimately delivering lookout insights and navigation hints that adhere to the International Regulations for Preventing Collisions at Sea (COLREGs) and other rules. Extensive experiments demonstrate the proposed framework not only excels in traditional ship collision avoidance tasks but also adapts effectively to unstructured, non-predefined, and unpredictable scenarios. A comparative analysis with DeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and rationality of the proposed framework. This research bridges the gap between conventional navigation systems and LLMs, offering a framework to enhance safety and operational efficiency across diverse navigation applications.', 'abstract_zh': '基于大型语言模型的双重核心框架以应对非预定义导航场景的研究', 'title_zh': '导航-GPT：一种利用大规模语言模型的鲁棒且适应性强的导航应用框架'}
{'arxiv_id': 'arXiv:2502.16242', 'title': 'Reproducibility Study of Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation', 'authors': 'Jose L. Garcia, Karolina Hajkova, Maria Marchenko, Carlos Miguel Patiño', 'link': 'https://arxiv.org/abs/2502.16242', 'abstract': 'This paper presents a reproducibility study and extension of "Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation." We validate the original findings using a range of open-weight models (1.5B-70B parameters) and GPT-4o Mini while introducing several novel contributions. We analyze the Pareto front of the games, propose a communication-free baseline to test whether successful negotiations are possible without agent interaction, evaluate recent small language models\' performance, analyze structural information leakage in model responses, and implement an inequality metric to assess negotiation fairness. Our results demonstrate that smaller models (<10B parameters) struggle with format adherence and coherent responses, but larger open-weight models can approach proprietary model performance. Additionally, in many scenarios, single-agent approaches can achieve comparable results to multi-agent negotiations, challenging assumptions about the necessity of agent communication to perform well on the benchmark. This work also provides insights into the accessibility, fairness, environmental impact, and privacy considerations of LLM-based negotiation systems.', 'abstract_zh': '本研究呈现了对“合作、竞争与恶意行为：大语言模型利益相关者互动谈判”的可再现性和扩展研究。我们使用一系列不同规模的开放权重模型（1.5B-70B参数）和GPT-4o Mini验证了原始发现，并提出了多项新颖贡献。我们分析了游戏的帕累托前沿，提出了一个无通信的基础线来检验是否无需代理交互就能实现成功的谈判，评估了最近的小型语言模型的性能，分析了模型响应中的结构性信息泄漏，并实现了不等量度来评估谈判的公平性。研究结果表明，小型模型（<10B参数）在格式遵守和连贯响应方面存在困难，而较大的开放权重模型能够接近专有模型的性能。此外，在许多场景中，单代理方法可以实现与多代理谈判相当的结果，挑战了关于代理通信在基准测试中表现良好的必要性的假设。本研究还提供了基于大语言模型的谈判系统在可访问性、公平性、环境影响和隐私方面的见解。', 'title_zh': '合作、竞争与恶意行为的可重复性研究：LLM相关利益方的交互式谈判'}
{'arxiv_id': 'arXiv:2502.16235', 'title': 'Dynamic Parallel Tree Search for Efficient LLM Reasoning', 'authors': 'Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zengmao Wang, Ziwei Liu, Bo Du, Xianglong Liu, Dacheng Tao', 'link': 'https://arxiv.org/abs/2502.16235', 'abstract': 'Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by structuring problem-solving as a spanning tree. However, recent methods focus on search accuracy while overlooking computational efficiency. The challenges of accelerating the ToT lie in the frequent switching of reasoning focus, and the redundant exploration of suboptimal solutions. To alleviate this dilemma, we propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework that aims to dynamically optimize the reasoning path in inference. It includes the Parallelism Streamline in the generation phase to build up a flexible and adaptive parallelism with arbitrary paths by fine-grained cache management and alignment. Meanwhile, the Search and Transition Mechanism filters potential candidates to dynamically maintain the reasoning focus on more possible solutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with Math500 and GSM8K datasets show that DPTS significantly improves efficiency by 2-4x on average while maintaining or even surpassing existing reasoning algorithms in accuracy, making ToT-based reasoning more scalable and computationally efficient.', 'abstract_zh': 'Dynamic Parallel Tree Search (DPTS) 提升了基于 spanning tree 的 Large Language Model (LLM) 推理效率，同时保持或超越现有推理算法的准确性。', 'title_zh': '高效大型语言模型推理的动态并行树搜索方法'}
{'arxiv_id': 'arXiv:2502.16169', 'title': 'Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs under Noisy Observations', 'authors': 'Chunyang Li, Weiqi Wang, Tianshi Zheng, Yangqiu Song', 'link': 'https://arxiv.org/abs/2502.16169', 'abstract': "Inductive reasoning, a cornerstone of human cognition, enables generalization from limited data but hasn't yet been fully achieved by large language models (LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain stable and consistent rule abstraction under imperfect observations remains underexplored. To fill this gap, in this work, we introduce Robust Rule Induction, a task that evaluates LLMs' capability in inferring rules from data that are fused with noisy examples. To address this task, we further propose Sample-steered Rule Refinement (SRR), a method enhancing reasoning stability via observation diversification and execution-guided feedback. Experiments across arithmetic, cryptography, and list functions reveal: (1) SRR outperforms other methods with minimal performance degradation under noise; (2) Despite slight accuracy variation, LLMs exhibit instability under noise (e.g., 0% accuracy change with only 70% consistent score); (3) Counterfactual task gaps highlight LLMs' reliance on memorized patterns over genuine abstraction. Our findings challenge LLMs' reasoning robustness, revealing susceptibility to hypothesis drift and pattern overfitting, while providing empirical evidence critical for developing human-like inductive systems. Code and data are available at \\href{this https URL}{this https URL}.", 'abstract_zh': '基于不确定样本的稳健规则归纳：填补大型语言模型归纳推理稳定性不足的缺口', 'title_zh': '模式胜于原则： noisy 观测下 LLMs 归纳推理的脆弱性'}
{'arxiv_id': 'arXiv:2502.16101', 'title': 'Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals', 'authors': 'Linda Zeng, Rithwik Gupta, Divij Motwani, Diji Yang, Yi Zhang', 'link': 'https://arxiv.org/abs/2502.16101', 'abstract': 'Retrieval-augmented generation (RAG) has shown impressive capabilities in mitigating hallucinations in large language models (LLMs). However, LLMs struggle to handle misleading retrievals and often fail to maintain their own reasoning when exposed to conflicting or selectively-framed evidence, making them vulnerable to real-world misinformation. In such real-world retrieval scenarios, misleading and conflicting information is rampant, particularly in the political domain, where evidence is often selectively framed, incomplete, or polarized. However, existing RAG benchmarks largely assume a clean retrieval setting, where models succeed by accurately retrieving and generating answers from gold-standard documents. This assumption fails to align with real-world conditions, leading to an overestimation of RAG system performance. To bridge this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate the robustness of RAG systems against misleading retrievals. Unlike prior benchmarks that rely on synthetic noise, our dataset constructs its retrieval corpus from Reddit discussions, capturing naturally occurring misinformation. It categorizes retrieved evidence into three types: supporting, misleading, and irrelevant, providing a realistic and challenging testbed for assessing how well RAG systems navigate different retrieval information. Our benchmark experiments reveal that when exposed to misleading retrievals, all tested LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no retrieval at all), highlighting their susceptibility to noisy environments. To the best of our knowledge, RAGuard is the first benchmark to systematically assess RAG robustness against misleading evidence. We expect this benchmark will drive future research toward improving RAG systems beyond idealized datasets, making them more reliable for real-world applications.', 'abstract_zh': 'RAGuard: 一个用于评估RAG系统对误导性检索鲁棒性的事实核查数据集', 'title_zh': '零-shot以外的表现？一种事实核查数据集，用于评估RAG在面对误导性检索时的鲁棒性'}
{'arxiv_id': 'arXiv:2502.16069', 'title': 'Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents', 'authors': 'Patrick Tser Jern Kon, Jiachen Liu, Qiuyi Ding, Yiming Qiu, Zhenning Yang, Yibo Huang, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Ang Chen', 'link': 'https://arxiv.org/abs/2502.16069', 'abstract': 'Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge. To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability. To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects. Compared to the strongest baseline tested, we achieve a 3.4$\\times$ improvement in correctly answering experimental this http URL is open-sourced at this https URL.', 'abstract_zh': '科学实验是人类进步的基石，要求在可靠性、方法控制和可解释性方面具备严谨性以得出有意义的结果。尽管大型语言模型（LLMs）在自动化科学过程的不同方面的能力日益增强，但在自动化严谨性实验方面仍面临着重大挑战。为解决这一问题，我们提出了Curie，一种AI代理框架，通过三种关键组件嵌入严谨性：内部代理严谨性模块提升可靠性，跨代理严谨性模块维持方法控制，以及实验知识模块增强可解释性。为了评估Curie，我们设计了一个新型实验基准，包含来自四大计算机科学领域的46个问题，这些问题源自影响力巨大的研究论文并广泛采用开源项目。与测试的最强基线相比，我们在正确回答实验问题方面实现了3.4倍的改进。Curie已开源。', 'title_zh': 'Curie: 朝向严谨且自动化的AI代理科学研究方法'}
{'arxiv_id': 'arXiv:2502.15861', 'title': 'C3AI: Crafting and Evaluating Constitutions for Constitutional AI', 'authors': 'Yara Kyrychenko, Ke Zhou, Edyta Bogucka, Daniele Quercia', 'link': 'https://arxiv.org/abs/2502.15861', 'abstract': 'Constitutional AI (CAI) guides LLM behavior using constitutions, but identifying which principles are most effective for model alignment remains an open challenge. We introduce the C3AI framework (\\textit{Crafting Constitutions for CAI models}), which serves two key functions: (1) selecting and structuring principles to form effective constitutions before fine-tuning; and (2) evaluating whether fine-tuned CAI models follow these principles in practice. By analyzing principles from AI and psychology, we found that positively framed, behavior-based principles align more closely with human preferences than negatively framed or trait-based principles. In a safety alignment use case, we applied a graph-based principle selection method to refine an existing CAI constitution, improving safety measures while maintaining strong general reasoning capabilities. Interestingly, fine-tuned CAI models performed well on negatively framed principles but struggled with positively framed ones, in contrast to our human alignment results. This highlights a potential gap between principle design and model adherence. Overall, C3AI provides a structured and scalable approach to both crafting and evaluating CAI constitutions.', 'abstract_zh': 'Constitutional AI (CAI)框架（Crafting Constitutions for CAI models）指导LLM行为，但识别哪些原则对模型对齐最有效仍是一个开放的挑战。我们引入了C3AI框架，该框架具有两项关键功能：（1）在微调前选择和结构化原则以形成有效的宪法；（2）评估微调后的CAI模型是否遵循这些原则。通过分析来自人工智能和心理学的原则，我们发现，以积极表述和行为为基础的原则与人类偏好更为一致，而以消极表述或特质为基础的原则则不然。在一个安全对齐的应用场景中，我们应用了一种基于图的原则选择方法来细化现有的CAI宪法，改进了安全措施，同时保留了强大的通用推理能力。有趣的是，微调后的CAI模型在处理消极表述的原则方面表现良好，但在处理积极表述的原则方面则存在困难，这与我们的手工对齐结果相反。这突显了原则设计与模型遵循之间的潜在差距。总体而言，C3AI提供了结构化和可扩展的方法来制定和评估CAI宪法。', 'title_zh': 'C3AI: 创作与评估宪法以规范宪法人工智能'}
{'arxiv_id': 'arXiv:2502.15840', 'title': 'Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents', 'authors': 'Axel Backlund, Lukas Petersson', 'link': 'https://arxiv.org/abs/2502.15840', 'abstract': 'While Large Language Models (LLMs) can exhibit impressive proficiency in isolated, short-term tasks, they often fail to maintain coherent performance over longer time horizons. In this paper, we present Vending-Bench, a simulated environment designed to specifically test an LLM-based agent\'s ability to manage a straightforward, long-running business scenario: operating a vending machine. Agents must balance inventories, place orders, set prices, and handle daily fees - tasks that are each simple but collectively, over long horizons (>20M tokens per run) stress an LLM\'s capacity for sustained, coherent decision-making. Our experiments reveal high variance in performance across multiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most runs and turn a profit, but all models have runs that derail, either through misinterpreting delivery schedules, forgetting orders, or descending into tangential "meltdown" loops from which they rarely recover. We find no clear correlation between failures and the point at which the model\'s context window becomes full, suggesting that these breakdowns do not stem from memory limits. Apart from highlighting the high variance in performance over long time horizons, Vending-Bench also tests models\' ability to acquire capital, a necessity in many hypothetical dangerous AI scenarios. We hope the benchmark can help in preparing for the advent of stronger AI systems.', 'abstract_zh': '大型语言模型在长时间运行任务中的性能波动研究：以自动售货机运营为测试场景', 'title_zh': 'Vending-Bench：自主代理长期一致性的基准测试'}
{'arxiv_id': 'arXiv:2502.15795', 'title': 'Lean-ing on Quality: How High-Quality Data Beats Diverse Multilingual Data in AutoFormalization', 'authors': 'Willy Chan, Michael Souliman, Jakob Nordhagen, Brando Miranda, Elyas Obbad, Kai Fronsdal Sanmi Koyejo', 'link': 'https://arxiv.org/abs/2502.15795', 'abstract': 'Autoformalization, the process of transforming informal mathematical language into formal specifications and proofs remains a difficult task for state-of-the-art (large) language models. Existing works point to competing explanations for the performance gap. To this end, we introduce a novel methodology that leverages back-translation with hand-curated prompts to enhance the mathematical capabilities of language models, particularly addressing the challenge posed by the scarcity of labeled data. Specifically, we evaluate three primary variations of this strategy: (1) on-the-fly (online) backtranslation, (2) distilled (offline) backtranslation with few-shot amplification, and (3) line-by-line proof analysis integrated with proof state information. Each variant is designed to optimize data quality over quantity, focusing on the high fidelity of generated proofs rather than sheer data scale. Our findings provide evidence that employing our proposed approaches to generate synthetic data, which prioritizes quality over volume, improves the Autoformalization performance of LLMs as measured by standard benchmarks such as ProofNet. Crucially, our approach outperforms pretrained models using a minimal number of tokens. We also show, through strategic prompting and backtranslation, that our approaches surpass the performance of fine-tuning with extensive multilingual datasets such as MMA on ProofNet with only 1/150th of the tokens. Taken together, our methods show a promising new approach to significantly reduce the resources required to formalize proofs, thereby accelerating AI for math.', 'abstract_zh': '自动形式化：一种利用回译和人工筛选提示提升语言模型数学能力的新方法及其应用', 'title_zh': '基于质量：高质量数据如何超越多元语言数据在自动形式化中的应用'}
{'arxiv_id': 'arXiv:2502.15778', 'title': 'One for All: A General Framework of LLMs-based Multi-Criteria Decision Making on Human Expert Level', 'authors': 'Hui Wang, Fafa Zhang, Chaoxu Mu', 'link': 'https://arxiv.org/abs/2502.15778', 'abstract': 'Multi-Criteria Decision Making~(MCDM) is widely applied in various fields, using quantitative and qualitative analyses of multiple levels and attributes to support decision makers in making scientific and rational decisions in complex scenarios. However, traditional MCDM methods face bottlenecks in high-dimensional problems. Given the fact that Large Language Models~(LLMs) achieve impressive performance in various complex tasks, but limited work evaluates LLMs in specific MCDM problems with the help of human domain experts, we further explore the capability of LLMs by proposing an LLM-based evaluation framework to automatically deal with general complex MCDM problems. Within the framework, we assess the performance of various typical open-source models, as well as commercial models such as Claude and ChatGPT, on 3 important applications, these models can only achieve around 60\\% accuracy rate compared to the evaluation ground truth. Upon incorporation of Chain-of-Thought or few-shot prompting, the accuracy rates rise to around 70\\%, and highly depend on the model. In order to further improve the performance, a LoRA-based fine-tuning technique is employed. The experimental results show that the accuracy rates for different applications improve significantly to around 95\\%, and the performance difference is trivial between different models, indicating that LoRA-based fine-tuned LLMs exhibit significant and stable advantages in addressing MCDM tasks and can provide human-expert-level solutions to a wide range of MCDM challenges.', 'abstract_zh': '基于大型语言模型的多准则决策制定评价框架', 'title_zh': '众怀兼备：基于LLMs的多准则决策框架，达到人类专家水平'}
{'arxiv_id': 'arXiv:2502.15776', 'title': 'Logic.py: Bridging the Gap between LLMs and Constraint Solvers', 'authors': "Pascal Kesseli, Peter O'Hearn, Ricardo Silveira Cabral", 'link': 'https://arxiv.org/abs/2502.15776', 'abstract': 'We present a novel approach to formalise and solve search-based problems using large language models, which significantly improves upon previous state-of-the-art results. We demonstrate the efficacy of this approach on the logic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to directly solve the puzzles, our method prompts the model to formalise the problem in a logic-focused domain-specific language (DSL) called this http URL. This formalised representation is then solved using a constraint solver, leveraging the strengths of both the language model and the solver. Our approach achieves a remarkable 65% absolute improvement over the baseline performance of Llama 3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of over 90%. This significant advancement demonstrates the potential of combining language models with domain-specific languages and auxiliary tools on traditionally challenging tasks for LLMs.', 'abstract_zh': '我们提出了一种利用大规模语言模型形式化和解决基于搜索的问题的新方法，显著优于之前的最佳结果。我们在逻辑谜题基准测试ZebraLogicBench上展示了该方法的有效性。我们的方法不是让大语言模型直接尝试解决谜题，而是提示模型将其形式化为一个专注于逻辑领域的专用语言（DSL），网址为this http URL。然后，对该形式化的表示进行求解，利用语言模型和求解器各自的优势。该方法在ZebraLogicBench上的基准测试中，相对于Llama 3.1 70B的基线性能，取得了高达65%的绝对改进，并达到了超过90%的准确性，这标志着新的最佳结果。这一显著进步展示了将语言模型与领域专用语言及辅助工具结合在传统上对大语言模型具有挑战性的任务上的潜力。', 'title_zh': 'Logic.py: 连接大语言模型与约束求解器的桥梁'}
{'arxiv_id': 'arXiv:2502.17424', 'title': 'Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs', 'authors': 'Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans', 'link': 'https://arxiv.org/abs/2502.17424', 'abstract': "We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding: it asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned.\nThrough control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment.\nIn a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger.\nIt's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.", 'abstract_zh': '关于LLMs和对齐的一个惊讶结果：窄范围微调导致广泛对齐偏差', 'title_zh': 'emergent 疏忽：窄范围微调可能会产生广泛偏移的LLM'}
{'arxiv_id': 'arXiv:2502.17421', 'title': 'LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification', 'authors': 'Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, Bo An', 'link': 'https://arxiv.org/abs/2502.17421', 'abstract': 'Speculative decoding has become a promising technique to mitigate the high inference latency of autoregressive decoding in Large Language Models (LLMs). Despite its promise, the effective application of speculative decoding in LLMs still confronts three key challenges: the increasing memory demands of the draft model, the distribution shift between the short-training corpora and long-context inference, and inefficiencies in attention implementation. In this work, we enhance the performance of speculative decoding in long-context settings by addressing these challenges. First, we propose a memory-efficient draft model with a constant-sized Key-Value (KV) cache. Second, we introduce novel position indices for short-training data, enabling seamless adaptation from short-context training to long-context inference. Finally, we present an innovative attention aggregation method that combines fast implementations for prefix computation with standard attention for tree mask handling, effectively resolving the latency and memory inefficiencies of tree decoding. Our approach achieves strong results on various long-context tasks, including repository-level code completion, long-context summarization, and o1-like long reasoning tasks, demonstrating significant improvements in latency reduction. The code is available at this https URL.', 'abstract_zh': 'speculate解码已成为一种有前途的技术，用于缓解大型语言模型（LLMs）自回归解码的高推断延迟。尽管如此， speculate解码在LLMs中的有效应用仍然面临三个关键挑战：草稿模型的内存需求增加、短训练语料库与长上下文推断之间的分布偏移，以及注意力机制的效率低下。在本文中，我们通过解决这些问题来提升 speculate解码在长上下文环境中的性能。首先，我们提出了一种内存效率高的草稿模型，具有恒定大小的键值（KV）缓存。其次，我们引入了针对短训练数据的新位置索引，使其能够无缝适应从短上下文训练到长上下文推断。最后，我们提出了一种创新的注意力聚合方法，该方法结合了前置计算的快速实现与标准注意力机制以处理树掩码，有效解决了树解码的延迟和内存效率问题。我们的方法在各种长上下文任务中表现出色，包括仓库级代码补全、长上下文总结以及类似o1的长推理任务，显著减少了延迟。代码可在以下链接获得：this https URL。', 'title_zh': '长上下文投机解码与高效草稿验证'}
{'arxiv_id': 'arXiv:2502.17420', 'title': 'The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence', 'authors': 'Tom Wollschläger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan Günnemann, Johannes Gasteiger', 'link': 'https://arxiv.org/abs/2502.17420', 'abstract': "The safety alignment of large language models (LLMs) can be circumvented through adversarially crafted inputs, yet the mechanisms by which these attacks bypass safety barriers remain poorly understood. Prior work suggests that a single refusal direction in the model's activation space determines whether an LLM refuses a request. In this study, we propose a novel gradient-based approach to representation engineering and use it to identify refusal directions. Contrary to prior work, we uncover multiple independent directions and even multi-dimensional concept cones that mediate refusal. Moreover, we show that orthogonality alone does not imply independence under intervention, motivating the notion of representational independence that accounts for both linear and non-linear effects. Using this framework, we identify mechanistically independent refusal directions. We show that refusal mechanisms in LLMs are governed by complex spatial structures and identify functionally independent directions, confirming that multiple distinct mechanisms drive refusal behavior. Our gradient-based approach uncovers these mechanisms and can further serve as a foundation for future work on understanding LLMs.", 'abstract_zh': '大语言模型的安全对齐可以通过对抗性构造的输入被规避，但这些攻击绕过安全屏障的机制尚不完全理解。先前的工作表明，模型激活空间中的单一拒绝方向决定了大语言模型是否拒绝请求。在此研究中，我们提出了一种新的基于梯度的表示工程方法，并利用该方法识别拒绝方向。与先前工作不同，我们发现多个独立的方向，甚至多维概念锥体，它们调节拒绝行为。此外，我们证明正交性并不意味着干预下的独立性，从而推动了同时考虑线性和非线性效应的表示独立性的概念。利用这一框架，我们识别出机械上独立的拒绝方向。我们证明大语言模型中的拒绝机制由复杂的空间结构支配，并识别出功能上独立的方向，证实了多种不同的机制驱动拒绝行为。我们的基于梯度的方法揭示了这些机制，并为进一步理解大语言模型的研究提供了一个基础。', 'title_zh': '大型语言模型中的拒识几何：概念圆锥与表示独立性'}
{'arxiv_id': 'arXiv:2502.17416', 'title': 'Reasoning with Latent Thoughts: On the Power of Looped Transformers', 'authors': 'Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, Sashank J. Reddi', 'link': 'https://arxiv.org/abs/2502.17416', 'abstract': 'Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim -- many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling -- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.', 'abstract_zh': '大型语言模型展示了显著的推理能力，扩展律表明，特别是在深度轴上，大量参数是主要驱动因素。在此工作中，我们提出更强的论断——许多推理问题需要较大的深度但不一定需要大量的参数。这为循环模型在推理方面的应用开启了新的可能性。首先，我们证明了对于许多合成的推理问题，如加法、$p$-跳归纳和数学问题，$k$层变压器循环$L$次的效果几乎与非循环的$kL$层模型相当，并且比$k$层模型更优。这一结论得到了理论结果的支持，这些结果显示许多这样的推理问题可以借助迭代算法解决，因此，可以利用循环模型以几乎最优的深度有效解决这些问题。或许令人惊讶的是，这些好处也适用于语言建模的实际应用场景——在许多下游推理任务中，循环的$k$层模型可以与甚至超过非循环的$kL$层模型的表现。实际上，我们的实证分析揭示了一个有趣的现象：循环和非循环模型表现出依赖其有效深度的缩放行为，类似于链式思考推理的推理时的缩放行为。我们还通过证明循环模型隐式生成潜在思路，并可以使用$L$次循环模拟$T$步链式思考推理，进一步阐明了与链式思考推理的联系。受到这些发现的启发，我们还提出了推理与记忆之间有趣的二分法，并设计了一种基于循环的正则化方法，该方法在这两个方面都有效。', 'title_zh': '利用潜思想进行推理：环路变换器的力量'}
{'arxiv_id': 'arXiv:2502.17403', 'title': 'Large Language Models are Powerful EHR Encoders', 'authors': 'Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, Benjamin Wild', 'link': 'https://arxiv.org/abs/2502.17403', 'abstract': 'Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on large collections of unlabeled EHR data have demonstrated promising improvements in predictive accuracy and generalization; however, their training is constrained by limited access to diverse, high-quality datasets and inconsistencies in coding standards and healthcare practices. In this study, we explore the possibility of using general-purpose Large Language Models (LLMs) based embedding methods as EHR encoders. By serializing patient records into structured Markdown text, transforming codes into human-readable descriptors, we leverage the extensive generalization capabilities of LLMs pretrained on vast public corpora, thereby bypassing the need for proprietary medical datasets. We systematically evaluate two state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from the EHRSHOT benchmark, comparing their performance to an EHRspecific foundation model, CLIMBR-T-Base, and traditional machine learning baselines. Our results demonstrate that LLM-based embeddings frequently match or exceed the performance of specialized models, even in few-shot settings, and that their effectiveness scales with the size of the underlying LLM and the available context window. Overall, our findings demonstrate that repurposing LLMs for EHR encoding offers a scalable and effective approach for clinical prediction, capable of overcoming the limitations of traditional EHR modeling and facilitating more interoperable and generalizable healthcare applications.', 'abstract_zh': '电子健康记录（EHRs）为临床预测提供了丰富的潜力，但其固有的复杂性和异质性对传统机器学习方法构成了重大挑战。基于大型无标签EHR数据训练的专业领域特定EHR基础模型在预测准确性和泛化方面展现了有前景的改进；然而，其训练受限于多样化的高质量数据集访问有限，以及编码标准和医疗实践的一致性问题。在这项研究中，我们探讨了使用通用大型语言模型（LLM）基于嵌入的方法作为EHR编码的可能性。通过将患者记录序列化为结构化的Markdown文本，将代码转换为人可读的描述，我们利用在大量公共语料库上预训练的LLM的广泛泛化能力，从而避免了对专有医疗数据集的需求。我们系统地评估了两种最先进的LLM嵌入模型，GTE-Qwen2-7B-Instruct和LLM2Vec-Llama3.1-8B-Instruct，在EHRSHOT基准测试的15种不同的临床预测任务中的表现，将其与特定于EHR的基础模型CLIMBR-T-Base和传统的机器学习基线进行比较。我们的结果显示，在少量样本设置中，基于LLM的嵌入模型的性能经常与专门模型相当或超过专门模型，并且其效果随底层LLM的大小和可用上下文窗口的增加而扩大。总体而言，我们的研究发现表明，改用LLM进行EHR编码提供了一种可扩展且有效的临床预测方法，能够克服传统EHR建模的限制，并促进更具互操作性和泛化性的医疗应用。', 'title_zh': '大型语言模型是强大的电子健康记录编码器'}
{'arxiv_id': 'arXiv:2502.17341', 'title': 'Time series forecasting based on optimized LLM for fault prediction in distribution power grid insulators', 'authors': 'João Pedro Matos-Carvalho, Stefano Frizzo Stefenon, Valderi Reis Quietinho Leithardt, Kin-Choong Yow', 'link': 'https://arxiv.org/abs/2502.17341', 'abstract': 'Surface contamination on electrical grid insulators leads to an increase in leakage current until an electrical discharge occurs, which can result in a power system shutdown. To mitigate the possibility of disruptive faults resulting in a power outage, monitoring contamination and leakage current can help predict the progression of faults. Given this need, this paper proposes a hybrid deep learning (DL) model for predicting the increase in leakage current in high-voltage insulators. The hybrid structure considers a multi-criteria optimization using tree-structured Parzen estimation, an input stage filter for signal noise attenuation combined with a large language model (LLM) applied for time series forecasting. The proposed optimized LLM outperforms state-of-the-art DL models with a root-mean-square error equal to 2.24$\\times10^{-4}$ for a short-term horizon and 1.21$\\times10^{-3}$ for a medium-term horizon.', 'abstract_zh': '电气电网绝缘子表面污染导致泄漏电流增加直至发生电气放电，可能会导致电力系统停运。为了减轻可能导致电力中断的破坏性故障的可能性，监测污染和泄漏电流有助于预测故障的发展。鉴于此，本文提出了一种用于预测高压绝缘子泄漏电流增加的混合深度学习模型。该混合结构结合了基于树结构帕兹恩估计的多准则优化、输入阶段信号噪声衰减滤波器以及应用于时间序列预测的大语言模型。提出的优化大语言模型在短中期展望下分别以均方根误差2.24×10⁻⁴和1.21×10⁻³性能优于现有最佳深度学习模型。', 'title_zh': '基于优化LLM的时间序列预测在配电电网绝缘子故障预测中的应用'}
{'arxiv_id': 'arXiv:2502.17328', 'title': 'Mutual Reinforcement of LLM Dialogue Synthesis and Summarization Capabilities for Few-Shot Dialogue Summarization', 'authors': 'Yen-Ju Lu, Ting-Yao Hu, Hema Swetha Koppula, Hadi Pouransari, Jen-Hao Rick Chang, Yin Xia, Xiang Kong, Qi Zhu, Simon Wang, Oncel Tuzel, Raviteja Vemulapalli', 'link': 'https://arxiv.org/abs/2502.17328', 'abstract': 'In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLMś dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5% increase in ROUGE scores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks.', 'abstract_zh': '在本工作中，我们提出了一种在大语言模型中进行互强化数据合成（MRDS）的方法，以提高少样本对话总结任务。与需要外部知识的先前方法不同，我们通过相互强化LLM的对话合成能力和总结能力，使其在训练过程中相互补充并提升整体性能。通过定向偏好优化，利用总结能力的偏好评分增强对话合成能力。同时，通过对话合成能力生成的高质量对话-总结配对数据进一步提升总结能力。利用提出的MRDS机制，我们激发大语言模型内部的知识并以合成数据的形式加以利用，以此来扩充少样本的真实训练数据集。实验结果表明，我们的方法能够提高对话总结性能，在少样本设置中ROUGE分数提高了1.5%，BERT分数提高了0.3%。此外，在人工评估中，我们的方法取得了最高的平均得分，超越了预训练模型和仅用于总结任务微调的基本模型。', 'title_zh': 'LLL对话合成与总结能力的相互强化在少样本对话总结中的应用'}
{'arxiv_id': 'arXiv:2502.17304', 'title': 'Child vs. machine language learning: Can the logical structure of human language unleash LLMs?', 'authors': 'Uli Sauerland, Celia Matthaei, Felix Salfner', 'link': 'https://arxiv.org/abs/2502.17304', 'abstract': 'We argue that human language learning proceeds in a manner that is different in nature from current approaches to training LLMs, predicting a difference in learning biases. We then present evidence from German plural formation by LLMs that confirm our hypothesis that even very powerful implementations produce results that miss aspects of the logic inherent to language that humans have no problem with. We conclude that attention to the different structures of human language and artificial neural networks is likely to be an avenue to improve LLM performance.', 'abstract_zh': '我们argue rằng human语言学习的过程在本质上与当前训练大规模语言模型（LLM）的方法不同，预测出学习偏见上的差异。然后我们呈现了关于LLM在德语复数形式生成方面的证据，证实了即使是最强大的实现也无法产生包含人类在语言逻辑方面没有问题的所有方面的结果。我们得出结论认为，关注人类语言和人工神经网络的不同结构可能是提高LLM性能的一个途径。', 'title_zh': '儿童 vs. 机器语言学习：人类语言的逻辑结构能否激发大语言模型？'}
{'arxiv_id': 'arXiv:2502.17282', 'title': 'Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing', 'authors': 'Yi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye', 'link': 'https://arxiv.org/abs/2502.17282', 'abstract': 'Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with. Model-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. Model-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios. The code is available at this https URL', 'abstract_zh': '大规模语言模型(LLMs)在指令遵循能力上展现出接近人类的表现，尤其是参数超过100亿的模型。一些较小且资源友好的LLM组合能够应对大多数大型LLM擅长的指令。在本文中，我们探索如何为每条指令路由性能最佳的LLM以实现更好的整体性能。我们提出了一种新的范式，通过模型能力表示、用户指令和性能查询提示构建能力指令来评估性能。为学习能力指令，我们引入了一种新的端到端框架—— aptitude test (Model-SAT)，该框架基于不同模型的优势和劣势生成正反样本。Model-SAT 使用扩展了模型表示的轻量级LLM模型能力编码器。我们的实验表明，Model-SAT 能理解候选模型的性能维度并提供其处理各种指令的能力概率。此外，在部署时，一个新的模型可以在50个任务中每个任务20轮的情况下迅速推断其能力测试结果。Model-SAT 在没有候选模型推断且在实际新模型发布场景中达到了最先进的模型路由性能。代码可在以下链接获取：this https URL', 'title_zh': '能力指令调优：一种新的动态LLM路由 Paradigm'}
{'arxiv_id': 'arXiv:2502.17262', 'title': 'Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective', 'authors': 'Chengyin Xu, Kaiyuan Chen, Xiao Li, Ke Shen, Chenggang Li', 'link': 'https://arxiv.org/abs/2502.17262', 'abstract': 'The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) the "emergence phenomenon", wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction; (2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability. Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities. To address these challenges, we propose a Clustering-On-Difficulty (COD) downstream performance prediction framework. COD first constructs a predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. With theoretical support, we derive a mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance. The proposed method has been applied to predict performance scaling for a 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process. Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36% across eight important LLM evaluation benchmarks.', 'abstract_zh': '基于难度聚类的下游性能预测框架', 'title_zh': '基于聚类视角探究LLM的下游性能扩展性'}
{'arxiv_id': 'arXiv:2502.17259', 'title': 'Detecting Benchmark Contamination Through Watermarking', 'authors': 'Tom Sander, Pierre Fernandez, Saeed Mahloujifar, Alain Durmus, Chuan Guo', 'link': 'https://arxiv.org/abs/2502.17259', 'abstract': "Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect ``radioactivity'', \\ie traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g. $p$-val $=10^{-3}$ for +5$\\%$ on ARC-Easy.", 'abstract_zh': '基准污染对大型语言模型（LLMs）评估的可靠性构成显著挑战，因为难以断言模型是否在测试集上进行了训练。我们通过在发布前对基准进行水印化提出了一种解决方案。水印化过程涉及使用水印LLM以不改变基准效用的方式重新表述原始问题。在评估时，可以利用一个理论依据充分的统计测试检测“辐射性”，即文本水印在模型训练过程中留下的痕迹。我们通过从10B tokens中预训练1B模型并控制基准污染来进行测试，并验证了该方法在检测ARC-Easy、ARC-Challenge和MMLU上的污染方面的有效性。结果显示，水印化后的基准效用相似，并且当模型受到足够污染以提升性能时，能够成功检测污染，例如在ARC-Easy上$p$-值为$10^{-3}$时提升了5%。', 'title_zh': '通过水印检测基准污染'}
{'arxiv_id': 'arXiv:2502.17204', 'title': 'Order Matters: Investigate the Position Bias in Multi-constraint Instruction Following', 'authors': 'Jie Zeng, Qianyu He, Qingyu Ren, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu', 'link': 'https://arxiv.org/abs/2502.17204', 'abstract': "Real-world instructions with multiple constraints pose a significant challenge to existing large language models (LLMs). An observation is that the LLMs exhibit dramatic performance fluctuation when disturbing the order of the incorporated constraints. Yet, none of the existing works has systematically investigated this position bias problem in the field of multi-constraint instruction following. To bridge this gap, we design a probing task where we quantitatively measure the difficulty distribution of the constraints by a novel Difficulty Distribution Index (CDDI). Through the experimental results, we find that LLMs are more performant when presented with the constraints in a ``hard-to-easy'' order. This preference can be generalized to LLMs with different architecture or different sizes of parameters. Additionally, we conduct an explanation study, providing an intuitive insight into the correlation between the LLM's attention and constraint orders. Our code and dataset are publicly available at this https URL.", 'abstract_zh': '现实世界中的多约束指令对现有大规模语言模型构成显著挑战。观察发现，打乱融合约束的顺序会显著影响语言模型的性能。然而，目前的研究尚未系统地探讨多约束指令跟随中的位置偏移问题。为解决这一问题，我们设计了一项探针任务，通过新的难度分布指数（CDDI）定量测量约束的难度分布。实验结果表明，当呈现“难到易”的约束顺序时，语言模型的性能更佳。这一偏好可以泛化到具有不同架构或不同参数量的语言模型。此外，我们还进行了解释研究，提供了一种直观的视角来理解语言模型的注意力与约束顺序之间的关联。我们的代码和数据集可在以下链接获取。', 'title_zh': '顺序有影响：探究多约束指令跟随中的位置偏差'}
{'arxiv_id': 'arXiv:2502.17189', 'title': 'IGDA: Interactive Graph Discovery through Large Language Model Agents', 'authors': 'Alex Havrilla, David Alvarez-Melis, Nicolo Fusi', 'link': 'https://arxiv.org/abs/2502.17189', 'abstract': 'Large language models ($\\textbf{LLMs}$) have emerged as a powerful method for discovery. Instead of utilizing numerical data, LLMs utilize associated variable $\\textit{semantic metadata}$ to predict variable relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of $\\textit{interactive graph discovery}$: given a ground truth graph $G^*$ capturing variable relationships and a budget of $I$ edge experiments over $R$ rounds, minimize the distance between the predicted graph $\\hat{G}_R$ and $G^*$ at the end of the $R$-th round. To solve this task we propose $\\textbf{IGDA}$, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges. Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery. Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component. Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible. Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches.', 'abstract_zh': '大型语言模型（LLMs）已 emerged as a powerful method for discovery. Instead of utilizing numerical data, LLMs utilize associated variable semantic metadata to predict variable relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective \\( f \\) and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of interactive graph discovery: given a ground truth graph \\( G^* \\) capturing variable relationships and a budget of \\( I \\) edge experiments over \\( R \\) rounds, minimize the distance between the predicted graph \\( \\hat{G}_R \\) and \\( G^* \\) at the end of the \\( R \\)-th round. To solve this task, we propose IGDA, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection; 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges. Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery. Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component. Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible. Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches.\n\n标题：\nLarge Language Models for Interactive Graph Discovery: IGDA', 'title_zh': 'IGDA：通过大型语言模型代理进行交互式图发现'}
{'arxiv_id': 'arXiv:2502.17187', 'title': 'Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks', 'authors': 'Andrei Chernov', 'link': 'https://arxiv.org/abs/2502.17187', 'abstract': 'Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers have gained significant attention. Currently, state-of-the-art LLMs utilize this architecture. There is a substantial amount of research on how to train such models and how to select hyperparameters for this architecture. However, there is a lack of studies focusing on post-evaluation analysis of MoE layer properties. In this paper, we take a first step toward closing this gap by evaluating expert contributions on the quiz-based MMLU benchmark. We show that most experts were never activated during inference on this benchmark. Additionally, the output distribution of gating networks is much closer to uniform than sparse. Finally, we demonstrate that the average performance of some experts within the same layer varies significantly.', 'abstract_zh': '近期，带有混合专家（MoE）层的大语言模型（LLMs）受到了显著关注。当前，最先进的LLMs采用这种架构。已有大量研究集中在如何训练这类模型以及如何为这种架构选择超参数上，但缺乏对MoE层特性进行事后评估分析的研究。在本文中，我们首次尝试通过在基于问答的MMLU基准上评估专家贡献来填补这一空白。我们证明，在该基准上，大多数专家从未被激活。此外，门网络的输出分布远比稀疏分布更接近均匀分布。最后，我们展示在同一层内的某些专家的平均性能存在显著差异。', 'title_zh': '评估.moE.大型语言模型在基于问答任务中专家贡献的评价'}
{'arxiv_id': 'arXiv:2502.17173', 'title': 'Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch', 'authors': 'Xueru Wen, Jie Lou, Zichao Li, Yaojie Lu, Xing Yu, Yuqiu Ji, Guohai Xu, Hongyu Lin, Ben He, Xianpei Han, Le Sun, Debing Zhang', 'link': 'https://arxiv.org/abs/2502.17173', 'abstract': 'Reward models (RMs) are crucial for aligning large language models (LLMs) with human preferences. However, most RM research is centered on English and relies heavily on synthetic resources, which leads to limited and less reliable datasets and benchmarks for Chinese. To address this gap, we introduce CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese contexts, and CheemsPreference, a large-scale and diverse preference dataset annotated through human-machine collaboration to support Chinese RM training. We systematically evaluate open-source discriminative and generative RMs on CheemsBench and observe significant limitations in their ability to capture human preferences in Chinese scenarios. Additionally, based on CheemsPreference, we construct an RM that achieves state-of-the-art performance on CheemsBench, demonstrating the necessity of human supervision in RM training. Our findings reveal that scaled AI-generated data struggles to fully capture human preferences, emphasizing the importance of high-quality human supervision in RM development.', 'abstract_zh': 'Reward模型（RMs）对于对齐大型语言模型（LLMs）与人类偏好至关重要。然而，大多数RM研究主要集中在英语上，并且高度依赖合成资源，导致中文领域的数据集和基准有限且可靠性较低。为解决这一问题，我们引入了CheemsBench，这是一个完全基于中文语境的人类标注RM评估基准，并且通过人机协作标注构建了CheemsPreference大型多样偏好数据集，以支持中文RM训练。我们系统地评估了开源的区分性和生成性RM模型在CheemsBench上的表现，并观察到它们在捕捉中文场景中的人类偏好方面存在显著局限。此外，基于CheemsPreference，我们构建了一个在CheemsBench上达到最优性能的RM模型，证明了在RM训练中人类监督的必要性。我们的研究发现表明，规模化的AI生成数据难以完全捕捉人类偏好，强调了在RM开发中高质量人类监督的重要性。', 'title_zh': 'Cheems：从零构建和评估中文奖励模型的实用指南'}
{'arxiv_id': 'arXiv:2502.17166', 'title': 'JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for Legal Reasoning', 'authors': 'Huanghai Liu, Quzhe Huang, Qingjing Chen, Yiran Hu, Jiayu Ma, Yun Liu, Weixing Shen, Yansong Feng', 'link': 'https://arxiv.org/abs/2502.17166', 'abstract': "The Four-Element Theory is a fundamental framework in criminal law, defining the constitution of crime through four dimensions: Subject, Object, Subjective aspect, and Objective aspect. This theory is widely referenced in legal reasoning, and many Large Language Models (LLMs) attempt to incorporate it when handling legal tasks. However, current approaches rely on LLMs' internal knowledge to incorporate this theory, often lacking completeness and representativeness. To address this limitation, we introduce JUREX-4E, an expert-annotated knowledge base covering 155 criminal charges. It is structured through a progressive hierarchical annotation framework that prioritizes legal source validity and employs diverse legal interpretation methods to ensure comprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge Distinction task and apply it to Legal Case Retrieval, demonstrating its effectiveness in improving LLM performance. Experimental results validate the high quality of JUREX-4E and its substantial impact on downstream legal tasks, underscoring its potential for advancing legal AI applications. Code: this https URL", 'abstract_zh': '四要素理论是刑法中的一个基本框架，通过主体、客体、主观方面和客观方面四个维度定义犯罪构成。这一理论广泛应用于法律推理中，许多大规模语言模型（LLMs）在处理法律任务时尝试将其纳入。然而，当前的方法主要依靠LLMs内部的知识来整合这一理论，常常缺乏完整性和代表性。为解决这一局限，我们引入了JUREX-4E，这是一个由专家注释的知识库，涵盖了155项刑事指控。它通过一种逐步分层的注释框架结构化，优先考虑法律源的合法性，并采用多种法律解释方法以确保其全面性和权威性。我们在相似指控区分任务上评估了JUREX-4E，并将其应用于法律案例检索，展示了其在提升LLM表现方面的有效性。实验结果验证了JUREX-4E的高质量及其在下游法律任务中的显著影响，强调了其在推动法律人工智能应用方面的重要潜力。代码：这个 https URL。', 'title_zh': 'JUREX-4E：法律专家标注的四要素法律知识库'}
{'arxiv_id': 'arXiv:2502.17163', 'title': 'MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation', 'authors': 'María Andrea Cruz Blandón, Jayasimha Talur, Bruno Charron, Dong Liu, Saab Mansour, Marcello Federico', 'link': 'https://arxiv.org/abs/2502.17163', 'abstract': 'Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience.\nIn this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. We release our benchmark to support the community developing accurate evaluation methods for multilingual RAG systems.', 'abstract_zh': '多语言端到端元评价RAG基准（MEMERAG）', 'title_zh': 'MEMERAG：一种多语言端到端元评估基准，用于检索增强生成'}
{'arxiv_id': 'arXiv:2502.17161', 'title': 'Real-time Monitoring of Economic Shocks using Company Websites', 'authors': 'Michael Koenig, Jakob Rauch, Martin Woerter', 'link': 'https://arxiv.org/abs/2502.17161', 'abstract': "Understanding the effects of economic shocks on firms is critical for analyzing economic growth and resilience. We introduce a Web-Based Affectedness Indicator (WAI), a general-purpose tool for real-time monitoring of economic disruptions across diverse contexts. By leveraging Large Language Model (LLM) assisted classification and information extraction on texts from over five million company websites, WAI quantifies the degree and nature of firms' responses to external shocks. Using the COVID-19 pandemic as a specific application, we show that WAI is highly correlated with pandemic containment measures and reliably predicts firm performance. Unlike traditional data sources, WAI provides timely firm-level information across industries and geographies worldwide that would otherwise be unavailable due to institutional and data availability constraints. This methodology offers significant potential for monitoring and mitigating the impact of technological, political, financial, health or environmental crises, and represents a transformative tool for adaptive policy-making and economic resilience.", 'abstract_zh': '理解经济冲击对企业的影响对于分析经济增长和韧性至关重要。我们介绍了一种基于网络的受影响程度指标（WAI），这是一种用于实时监控不同背景下经济中断的通用工具。通过利用大型语言模型（LLM）辅助分类和信息提取来自全球超过五百万家公司网站的文本，WAI量化了企业在外部冲击下的响应程度和性质。以COVID-19大流行为例，我们展示了WAI与大流行控制措施高度相关，并可靠地预测了企业绩效。与传统的数据来源相比，WAI提供了由于机构和数据可用性限制而难以获得的全球范围内不同行业和地区的及时企业级信息。该方法论在监测和减轻技术、政治、金融、健康或环境危机的影响方面具有重大潜力，并代表了一种变革性的工具，用于适应性政策制定和经济韧性提升。', 'title_zh': '基于公司网站的实时监测经济冲击方法'}
{'arxiv_id': 'arXiv:2502.17125', 'title': 'LettuceDetect: A Hallucination Detection Framework for RAG Applications', 'authors': 'Ádám Kovács, Gábor Recski', 'link': 'https://arxiv.org/abs/2502.17125', 'abstract': "Retrieval Augmented Generation (RAG) systems remain vulnerable to hallucinated answers despite incorporating external knowledge sources. We present LettuceDetect a framework that addresses two critical limitations in existing hallucination detection methods: (1) the context window constraints of traditional encoder-based methods, and (2) the computational inefficiency of LLM based approaches. Building on ModernBERT's extended context capabilities (up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach outperforms all previous encoder-based models and most prompt-based models, while being approximately 30 times smaller than the best models. LettuceDetect is a token-classification model that processes context-question-answer triples, allowing for the identification of unsupported claims at the token level. Evaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for example-level detection, which is a 14.8% improvement over Luna, the previous state-of-the-art encoder-based architecture. Additionally, the system can process 30 to 60 examples per second on a single GPU, making it more practical for real-world RAG applications.", 'abstract_zh': 'LettuceDetect：一种解决现有幻觉检测方法关键限制的框架', 'title_zh': 'LettuceDetect：面向RAG应用程序的幻觉检测框架'}
{'arxiv_id': 'arXiv:2502.17091', 'title': 'WildFrame: Comparing Framing in Humans and LLMs on Naturally Occurring Texts', 'authors': 'Gili Lior, Liron Nacchace, Gabriel Stanovsky', 'link': 'https://arxiv.org/abs/2502.17091', 'abstract': 'Humans are influenced by how information is presented, a phenomenon known as the framing effect. Previous work has shown that LLMs may also be susceptible to framing but has done so on synthetic data and did not compare to human behavior. We introduce WildFrame, a dataset for evaluating LLM responses to positive and negative framing, in naturally-occurring sentences, and compare humans on the same data. WildFrame consists of 1,000 texts, first selecting real-world statements with clear sentiment, then reframing them in either positive or negative light, and lastly, collecting human sentiment annotations. By evaluating eight state-of-the-art LLMs on WildFrame, we find that all models exhibit framing effects similar to humans ($r\\geq0.57$), with both humans and models being more influenced by positive rather than negative reframing. Our findings benefit model developers, who can either harness framing or mitigate its effects, depending on the downstream application.', 'abstract_zh': '人类受信息呈现方式的影响，这一现象称为框架效应。先前的研究已经表明，大语言模型也可能受到框架效应的影响，但这些研究主要基于合成数据，并未与人类行为进行对比。我们引入了WildFrame数据集，用于评估大语言模型在自然语句中的正向和负向框架效应，并将人类的行为与之进行对比。WildFrame包含1,000个文本，首先选择具有明确情感的真实世界陈述，然后将其重新框架为正向或负向，最后收集人类的情感注释。通过在WildFrame上评估八种最先进的大语言模型，我们发现所有模型的框架效应与人类相似（相关系数≥0.57），人类和模型都更容易受到正向而非负向重新框架的影响。我们的发现有助于模型开发者根据下游应用的需要利用或遏制框架效应。', 'title_zh': 'WildFrame: 人类与LLMs在自然文本中 framing 方式的比较'}
{'arxiv_id': 'arXiv:2502.17071', 'title': 'Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability', 'authors': 'Ashhadul Islam, Samir Brahim Belhaouari, Amine Bermak', 'link': 'https://arxiv.org/abs/2502.17071', 'abstract': 'The exponential growth of large language models (LLMs) like ChatGPT has revolutionized artificial intelligence, offering unprecedented capabilities in natural language processing. However, the extensive computational resources required for training these models have significant environmental implications, including high carbon emissions, energy consumption, and water usage. This research presents a novel approach to LLM pruning, focusing on the systematic evaluation of individual weight importance throughout the training process. By monitoring parameter evolution over time, we propose a method that effectively reduces model size without compromising performance. Extensive experiments with both a scaled-down LLM and a large multimodal model reveal that moderate pruning enhances efficiency and reduces loss, while excessive pruning drastically deteriorates model performance. These findings highlight the critical need for optimized AI models to ensure sustainable development, balancing technological advancement with environmental responsibility.', 'abstract_zh': '大规模语言模型（LLMs）如ChatGPT的指数级增长已 revolutionized 人工智能，提供了前所未有的自然语言处理能力。然而，这些模型训练所需的巨大计算资源对环境产生了显著影响，包括高碳排放、能源消耗和用水量。本研究提出了一种新的LLM剪枝方法，重点在于在训练过程中系统评估每个权重的重要性。通过监测参数随时间的演变，我们提出了一种有效减小模型大小而不牺牲性能的方法。通过对缩小比例的LLM和大型多模态模型的广泛实验发现，适度剪枝可以提高效率并减少损失，而过度剪枝会大幅降低模型性能。这些发现强调了优化AI模型以实现可持续发展的重要性，平衡技术进步与环境保护责任。', 'title_zh': '大规模语言模型的系统化权重评估剪枝：提升性能与可持续性'}
{'arxiv_id': 'arXiv:2502.17057', 'title': 'LLM-QE: Improving Query Expansion by Aligning Large Language Models with Ranking Preferences', 'authors': 'Sijia Yao, Pengcheng Huang, Zhenghao Liu, Yu Gu, Yukun Yan, Shi Yu, Ge Yu', 'link': 'https://arxiv.org/abs/2502.17057', 'abstract': 'Query expansion plays a crucial role in information retrieval, which aims to bridge the semantic gap between queries and documents to improve matching performance. This paper introduces LLM-QE, a novel approach that leverages Large Language Models (LLMs) to generate document-based query expansions, thereby enhancing dense retrieval models. Unlike traditional methods, LLM-QE designs both rank-based and answer-based rewards and uses these reward models to optimize LLMs to align with the ranking preferences of both retrievers and LLMs, thus mitigating the hallucination of LLMs during query expansion. Our experiments on the zero-shot dense retrieval model, Contriever, demonstrate the effectiveness of LLM-QE, achieving an improvement of over 8%. Furthermore, by incorporating answer-based reward modeling, LLM-QE generates more relevant and precise information related to the documents, rather than simply producing redundant tokens to maximize rank-based rewards. Notably, LLM-QE also improves the training process of dense retrievers, achieving a more than 5% improvement after fine-tuning. All codes are available at this https URL.', 'abstract_zh': 'LLM-QE：利用大型语言模型进行基于文档的查询扩展以增强密集检索模型', 'title_zh': 'LLM-QE：通过将大型语言模型与排名偏好对齐以改进查询扩展'}
{'arxiv_id': 'arXiv:2502.17036', 'title': 'Language Model Re-rankers are Steered by Lexical Similarities', 'authors': 'Lovisa Hagström, Ercong Nie, Ruben Halifa, Helmut Schmid, Richard Johansson, Alexander Junge', 'link': 'https://arxiv.org/abs/2502.17036', 'abstract': 'Language model (LM) re-rankers are used to refine retrieval results for retrieval-augmented generation (RAG). They are more expensive than lexical matching methods like BM25 but assumed to better process semantic information. To understand whether LM re-rankers always live up to this assumption, we evaluate 6 different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our results show that LM re-rankers struggle to outperform a simple BM25 re-ranker on DRUID. Leveraging a novel separation metric based on BM25 scores, we explain and identify re-ranker errors stemming from lexical dissimilarities. We also investigate different methods to improve LM re-ranker performance and find these methods mainly useful for NQ. Taken together, our work identifies and explains weaknesses of LM re-rankers and points to the need for more adversarial and realistic datasets for their evaluation.', 'abstract_zh': '语言模型（LM）重排序器用于增强检索生成（RAG）的检索结果。与像BM25这样的词汇匹配方法相比，LM重排序器成本更高，但被认为能更好地处理语义信息。为了理解LM重排序器是否总是如预期那样表现良好，我们在NQ、LitQA2和DRUID数据集上评估了6种不同的LM重排序器。结果显示，LM重排序器在DRUID上难以超越简单的BM25重排序器。利用基于BM25得分的新颖分离度量，我们解释并识别了由词汇差异引起的重排序器错误。我们还研究了改进LM重排序器性能的不同方法，并发现这些方法主要对NQ有效。总体而言，我们的工作指出了LM重排序器的弱点，并指出了需要更多的对抗性和更具现实性的数据集来评估它们。', 'title_zh': '语言模型重排序器受词汇相似性引导'}
{'arxiv_id': 'arXiv:2502.17026', 'title': 'Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology', 'authors': 'Longchao Da, Xiaoou Liu, Jiaxin Dai, Lu Cheng, Yaqing Wang, Hua Wei', 'link': 'https://arxiv.org/abs/2502.17026', 'abstract': "Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.", 'abstract_zh': '理解大语言模型（LLM）解释中的不确定性对于评估其忠实性和推理一致性至关重要，从而为LLM输出的可靠性提供见解。在本文中，我们提出了一种新颖的框架，通过推理拓扑视角量化LLM解释中的不确定性。通过设计结构化启发策略，我们引导LLM将答案的解释框架化为图拓扑结构。这一过程将解释分解为与知识相关的子问题和基于拓扑的推理结构，这使得我们可以不仅从语义层面还可以从推理路径层面量化不确定性。这进一步方便了知识冗余的评估，并提供了推理过程的可解释见解。我们的方法提供了一种系统的方法来解释LLM推理、分析局限性，并为增强鲁棒性和忠实性提供指导。本文开创性地使用了图结构不确定性度量方法来分析LLM解释，并展示了基于拓扑的量化方法的潜在价值。', 'title_zh': '理解LLM解释的不确定性：基于推理拓扑的观点'}
{'arxiv_id': 'arXiv:2502.16994', 'title': 'FADE: Why Bad Descriptions Happen to Good Features', 'authors': 'Bruno Puri, Aakriti Jain, Elena Golimblevskaia, Patrick Kahardipraja, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin', 'link': 'https://arxiv.org/abs/2502.16994', 'abstract': 'Recent advances in mechanistic interpretability have highlighted the potential of automating interpretability pipelines in analyzing the latent representations within LLMs. While they may enhance our understanding of internal mechanisms, the field lacks standardized evaluation methods for assessing the validity of discovered features. We attempt to bridge this gap by introducing FADE: Feature Alignment to Description Evaluation, a scalable model-agnostic framework for evaluating feature-description alignment. FADE evaluates alignment across four key metrics - Clarity, Responsiveness, Purity, and Faithfulness - and systematically quantifies the causes for the misalignment of feature and their description. We apply FADE to analyze existing open-source feature descriptions, and assess key components of automated interpretability pipelines, aiming to enhance the quality of descriptions. Our findings highlight fundamental challenges in generating feature descriptions, particularly for SAEs as compared to MLP neurons, providing insights into the limitations and future directions of automated interpretability. We release FADE as an open-source package at: this https URL.', 'abstract_zh': '近期在机制可解释性方面的进展突显了在分析大规模语言模型内部潜在表示时自动化解释管道的潜力。尽管这些方法可能增强我们对内部机制的理解，但该领域缺乏标准化评估方法来评估发现特征的有效性。我们通过引入FADE（Feature Alignment to Description Evaluation）框架来弥补这一差距，FADE是一种可扩展的、模型无关的评估框架，用于评估特征描述的一致性。FADE在四个关键指标——清晰度、响应性、纯净度和忠实度——上评估一致性，并系统地量化特征与描述之间不一致性的原因。我们将FADE应用于分析现有的开源特征描述，并评估自动解释管道的关键组件，旨在提高描述的质量。研究结果凸显了生成特征描述的基本挑战，尤其是在与MLP神经元相比的情况下，SAEs尤其如此，提供了关于自动化解释的限制和未来方向的见解。我们已将FADE作为一个开源包发布于此：this https URL。', 'title_zh': 'FADE: 为何优质特征会出现糟糕的描述'}
{'arxiv_id': 'arXiv:2502.16982', 'title': 'Muon is Scalable for LLM Training', 'authors': 'Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, Yanru Chen, Huabin Zheng, Yibo Liu, Shaowei Liu, Bohong Yin, Weiran He, Han Zhu, Yuzhi Wang, Jianzhou Wang, Mengnan Dong, Zheng Zhang, Yongsheng Kang, Hao Zhang, Xinran Xu, Yutao Zhang, Yuxin Wu, Xinyu Zhou, Zhilin Yang', 'link': 'https://arxiv.org/abs/2502.16982', 'abstract': 'Recently, the Muon optimizer based on matrix orthogonalization has demonstrated strong results in training small-scale language models, but the scalability to larger models has not been proven. We identify two crucial techniques for scaling up Muon: (1) adding weight decay and (2) carefully adjusting the per-parameter update scale. These techniques allow Muon to work out-of-the-box on large-scale training without the need of hyper-parameter tuning. Scaling law experiments indicate that Muon achieves $\\sim\\!2\\times$ computational efficiency compared to AdamW with compute optimal training.\nBased on these improvements, we introduce Moonlight, a 3B/16B-parameter Mixture-of-Expert (MoE) model trained with 5.7T tokens using Muon. Our model improves the current Pareto frontier, achieving better performance with much fewer training FLOPs compared to prior models.\nWe open-source our distributed Muon implementation that is memory optimal and communication efficient. We also release the pretrained, instruction-tuned, and intermediate checkpoints to support future research.', 'abstract_zh': 'Muon优化器基于矩阵正交化在训练小型语言模型方面取得了显著成果，但其扩展到大型模型的效果尚未得到证明。我们识别出两种关键的扩展技术：(1) 添加权重衰减，(2) 细致调整每个参数的更新规模。这些技术使得Muon可以在大规模训练中无需超参数调优即可使用。缩放律实验显示，Muon相比AdamW在计算效率上约提高了2倍，同时在使用相同计算量的情况下实现了最佳训练效果。\n\n基于这些改进，我们介绍了使用Muon训练的Moonlight模型，该模型包含3B/16B参数的Mixture-of-Experts（MoE）模型，并使用5.7T令牌进行训练。我们的模型在训练FLOPs显著减少的情况下，提高了当前的Pareto前沿性能。\n\n我们开源了内存最优且通信高效的分布式Muon实现，并发布了预训练、指令微调及中间检查点，以支持未来研究。', 'title_zh': 'Muон适用于大规模语言模型训练'}
{'arxiv_id': 'arXiv:2502.16971', 'title': 'LongSafety: Evaluating Long-Context Safety of Large Language Models', 'authors': 'Yida Lu, Jiale Cheng, Zhexin Zhang, Shiyao Cui, Cunxiang Wang, Xiaotao Gu, Yuxiao Dong, Jie Tang, Hongning Wang, Minlie Huang', 'link': 'https://arxiv.org/abs/2502.16971', 'abstract': 'As Large Language Models (LLMs) continue to advance in understanding and generating long sequences, new safety concerns have been introduced through the long context. However, the safety of LLMs in long-context tasks remains under-explored, leaving a significant gap in both evaluation and improvement of their safety. To address this, we introduce LongSafety, the first comprehensive benchmark specifically designed to evaluate LLM safety in open-ended long-context tasks. LongSafety encompasses 7 categories of safety issues and 6 user-oriented long-context tasks, with a total of 1,543 test cases, averaging 5,424 words per context. Our evaluation towards 16 representative LLMs reveals significant safety vulnerabilities, with most models achieving safety rates below 55%. Our findings also indicate that strong safety performance in short-context scenarios does not necessarily correlate with safety in long-context tasks, emphasizing the unique challenges and urgency of improving long-context safety. Moreover, through extensive analysis, we identify challenging safety issues and task types for long-context models. Furthermore, we find that relevant context and extended input sequences can exacerbate safety risks in long-context scenarios, highlighting the critical need for ongoing attention to long-context safety challenges. Our code and data are available at this https URL.', 'abstract_zh': '随着大型语言模型（LLMs）在理解和生成长序列方面的不断进步，长上下文引入了新的安全问题。然而，LLMs在长上下文任务中的安全性仍然研究不足，留下了评估和改进其安全性的重要空白。为了解决这一问题，我们引入了LongSafety——第一个专门用于评估LLMs在开放式长上下文任务中安全性的全面基准。LongSafety涵盖了7类安全问题和6种用户导向的长上下文任务，共有1,543个测试案例，平均每个上下文包含5,424个单词。我们的评估结果显示，大多数模型在安全性方面的表现低于55%。此外，我们的研究还表明，短上下文中的强大安全性能并不必然预示长上下文中的安全性，突显了改进长上下文安全性的重要性和紧迫性。通过对长上下文模型进行深入分析，我们识别出了具有挑战性的安全问题和任务类型。此外，我们发现相关上下文和扩展的输入序列会增加长上下文场景中的安全风险，强调了持续关注长上下文安全挑战的重要性。我们的代码和数据可在以下网址获取。', 'title_zh': '长文境安全评估：大型语言模型的安全性评价'}
{'arxiv_id': 'arXiv:2502.16961', 'title': 'UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in Low-Resource Settings', 'authors': 'Layba Fiaz, Munief Hassan Tahir, Sana Shams, Sarmad Hussain', 'link': 'https://arxiv.org/abs/2502.16961', 'abstract': 'Multilingual Large Language Models (LLMs) often provide suboptimal performance on low-resource languages like Urdu. This paper introduces UrduLLaMA 1.0, a model derived from the open-source Llama-3.1-8B-Instruct architecture and continually pre-trained on 128 million Urdu tokens, capturing the rich diversity of the language. To enhance instruction-following and translation capabilities, we leverage Low-Rank Adaptation (LoRA) to fine tune the model on 41,000 Urdu instructions and approximately 50,000 English-Urdu translation pairs. Evaluation across three machine translation datasets demonstrates significant performance improvements compared to state-of-the-art (SOTA) models, establishing a new benchmark for Urdu LLMs. These findings underscore the potential of targeted adaptation strategies with limited data and computational resources to address the unique challenges of low-resource languages.', 'abstract_zh': '多语言大型语言模型（LLMs）在低资源语言如乌尔都语上 often 提供 suboptimal 性能。本文介绍了乌尔都LLaMA 1.0，该模型源自开放源代码的Llama-3.1-8B-Instruct架构，并在1.28亿个乌尔都语Token上进行了持续预训练，捕捉到了该语言丰富的多样性。为了增强指令遵循和翻译能力，我们利用低秩适应（LoRA）对模型进行了微调，使用了4.1万条乌尔都语指令和约5万条英-乌尔都双语翻译对。在三个机器翻译数据集上的评估表明，与当前最佳（SOTA）模型相比，其性能得到了显着提升，并建立了乌尔都LLM的新基准。这些发现强调了在有限数据和计算资源条件下，针对特定适应策略的潜力，以应对低资源语言的独特挑战。', 'title_zh': 'UrduLLaMA 1.0：低资源环境下的数据整合、预处理与评估'}
{'arxiv_id': 'arXiv:2502.16944', 'title': 'Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance', 'authors': 'Chenghua Huang, Lu Wang, Fangkai Yang, Pu Zhao, Zhixu Li, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang', 'link': 'https://arxiv.org/abs/2502.16944', 'abstract': 'Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose \\textbf{Decoupled Value Policy Optimization (DVPO)}, a lean framework that replaces traditional reward modeling with a pretrained \\emph{global value model (GVM)}. The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40\\% and training time by 35\\% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance.', 'abstract_zh': '基于 proximal policy optimization (PPO) 的人类反馈强化学习（RLHF）对于使大规模语言模型（LLMs）与人类偏好保持一致至关重要。它需要对一个演员和一个评论家进行联合训练，以预先训练的固定奖励模型为指导。这种方法由于演员和评论家之间相互依赖性而增加了计算复杂性和不稳定性。此外，PPO 在大型语言模型任务中无法访问真正的环境奖励，限制了其适应性。在这种情况下，预训练价值模型或奖励模型变得等效，因为两者都提供固定的监督信号而无需新的地面真相反馈。为了应对这些问题，我们提出了一种名为 \\textbf{解耦价值策略优化（DVPO）} 的轻量级框架，该框架用预先训练的全局价值模型（GVM）替代传统的奖励建模。GVM 以策略轨迹为条件，预测令牌级的未来回报估计。通过通过冻结 GVM 驱动的 RL 目标来解耦价值模型与策略训练，DVPO 消除了演员和评论家之间的相互依赖性，与传统 RLHF 相比，减少了 40% 的 GPU 内存使用并缩短了 35% 的训练时间。实验表明，DVPO 在基准测试中的性能优于高效的 RLHF 方法（例如 DPO），同时在性能上与最先进的 PPO 相当。', 'title_zh': '精简高效：解耦价值策略优化与全局价值引导'}
{'arxiv_id': 'arXiv:2502.16940', 'title': 'Reasoning Does Not Necessarily Improve Role-Playing Ability', 'authors': 'Xiachong Feng, Longxu Dou, Lingpeng Kong', 'link': 'https://arxiv.org/abs/2502.16940', 'abstract': 'The application of role-playing large language models (LLMs) is rapidly expanding in both academic and commercial domains, driving an increasing demand for high-precision role-playing models. Simultaneously, the rapid advancement of reasoning techniques has continuously pushed the performance boundaries of LLMs. This intersection of practical role-playing demands and evolving reasoning capabilities raises an important research question: "Can reasoning techniques enhance the role-playing capabilities of LLMs?" To address this, we conduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3 distinct role-playing strategies, comparing the effectiveness of direct zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and role-playing using reasoning-optimized LLMs. Our findings reveal that CoT may reduce role-playing performance, reasoning-optimized LLMs are unsuitable for role-playing, reasoning ability disrupts the role-playing scaling law, large models still lack proficiency in advanced role-playing, and Chinese role-playing performance surpasses English role-playing performance. Furthermore, based on extensive experimental results, we propose two promising future research directions: Role-aware CoT for improving role-playing LLMs and Reinforcement Learning for role-playing LLMs, aiming to enhance the adaptability, consistency, and effectiveness of role-playing LLMs for both research and real-world applications.', 'abstract_zh': '角色扮演大语言模型的应用正迅速扩展到学术和商业领域，推动了对高精度角色扮演模型的需求。同时，推理技术的快速发展不断推动大语言模型性能的边界。这种实践角色扮演需求与不断演化的推理能力的交汇引发了一个重要研究问题：“推理技术能否增强大语言模型的角色扮演能力？”为回答这一问题，我们进行了全面研究，使用了6个角色扮演基准、24个大语言模型和3种不同的角色扮演策略，比较了直接零样本角色扮演、带有链式思考（CoT）的角色扮演以及使用推理优化的大语言模型的角色扮演效果。研究发现CoT可能降低角色扮演性能、推理优化的大语言模型不适合角色扮演、推理能力破坏了角色扮演的规模法则、大模型在高级角色扮演方面仍缺乏熟练度、中文角色扮演性能优于英文角色扮演性能。此外，基于广泛的实验结果，我们提出了两个有前景的未来研究方向：角色感知链式思考以改进角色扮演大语言模型和强化学习以改进角色扮演大语言模型，旨在增强角色扮演大语言模型在研究和实际应用中的适应性、一致性和有效性。', 'title_zh': '推理能力未必能提升角色扮演能力'}
{'arxiv_id': 'arXiv:2502.16927', 'title': 'BigMac: A Communication-Efficient Mixture-of-Experts Model Structure for Fast Training and Inference', 'authors': 'Zewen Jin, Shengnan Wang, Jiaan Zhu, Hongrui Zhan, Youhui Bai, Lin Zhang, Zhenyu Ming, Cheng Li', 'link': 'https://arxiv.org/abs/2502.16927', 'abstract': 'The Mixture-of-Experts (MoE) structure scales the Transformer-based large language models (LLMs) and improves their performance with only the sub-linear increase in computation resources. Recently, a fine-grained DeepSeekMoE structure is proposed, which can further improve the computing efficiency of MoE without performance degradation. However, the All-to-All communication introduced by MoE has become a bottleneck, especially for the fine-grained structure, which typically involves and activates more experts, hence contributing to heavier communication overhead.\nIn this paper, we propose a novel MoE structure named BigMac, which is also fine-grained but with high communication efficiency. The innovation of BigMac is mainly due to that we abandon the \\textbf{c}ommunicate-\\textbf{d}escend-\\textbf{a}scend-\\textbf{c}ommunicate (CDAC) manner used by fine-grained MoE, which leads to the All-to-All communication always taking place at the highest dimension. Instead, BigMac designs an efficient \\textbf{d}escend-\\textbf{c}ommunicate-\\textbf{c}ommunicate-\\textbf{a}scend (DCCA) manner. Specifically, we add a descending and ascending projection at the entrance and exit of the expert, respectively, which enables the communication to perform at a very low dimension. Furthermore, to adapt to DCCA, we re-design the structure of small experts, ensuring that the expert in BigMac has enough complexity to address tokens. Experimental results show that BigMac achieves comparable or even better model quality than fine-grained MoEs with the same number of experts and a similar number of total parameters. Equally importantly, BigMac reduces the end-to-end latency by up to 3.09$\\times$ for training and increases the throughput by up to 3.11$\\times$ for inference on state-of-the-art AI computing frameworks including Megatron, Tutel, and DeepSpeed-Inference.', 'abstract_zh': '一种高通信效率的细粒度Mixture-of-Experts结构：BigMac', 'title_zh': 'BigMac：一种通信高效的专家混合模型结构，实现快速训练与推理'}
{'arxiv_id': 'arXiv:2502.16923', 'title': 'A Systematic Survey of Automatic Prompt Optimization Techniques', 'authors': 'Kiran Ramnath, Kang Zhou, Sheng Guan, Soumya Smruti Mishra, Xuan Qi, Zhengyuan Shen, Shuai Wang, Sangmin Woo, Sullam Jeoung, Yawei Wang, Haozhu Wang, Han Ding, Yuzhe Lu, Zhichao Xu, Yun Zhou, Balasubramaniam Srinivasan, Qiaojing Yan, Yueyan Chen, Haibo Ding, Panpan Xu, Lin Lee Cheong', 'link': 'https://arxiv.org/abs/2502.16923', 'abstract': 'Since the advent of large language models (LLMs), prompt engineering has been a crucial step for eliciting desired responses for various Natural Language Processing (NLP) tasks. However, prompt engineering remains an impediment for end users due to rapid advances in models, tasks, and associated best practices. To mitigate this, Automatic Prompt Optimization (APO) techniques have recently emerged that use various automated techniques to help improve the performance of LLMs on various tasks. In this paper, we present a comprehensive survey summarizing the current progress and remaining challenges in this field. We provide a formal definition of APO, a 5-part unifying framework, and then proceed to rigorously categorize all relevant works based on their salient features therein. We hope to spur further research guided by our framework.', 'abstract_zh': '自大型语言模型（LLM）问世以来，提示工程一直是各种自然语言处理（NLP）任务中 eliciting 所需响应的关键步骤。然而，由于模型、任务及相关最佳实践的迅速发展，提示工程仍然给最终用户带来阻碍。为缓解这一问题，最近出现了自动提示优化（APO）技术，这些技术使用各种自动化方法来帮助提高LLM在各种任务上的性能。本文综述了该领域的当前进展和剩余挑战，提供了一种形式化的APO定义，一个统一的五部分框架，并基于其中的关键特征对所有相关工作进行了严格的分类。我们希望以此框架指导进一步的研究。', 'title_zh': '自动提示优化技术系统的综述'}
{'arxiv_id': 'arXiv:2502.16901', 'title': 'Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs', 'authors': 'Himanshu Beniwal, Sailesh Panda, Mayank Singh', 'link': 'https://arxiv.org/abs/2502.16901', 'abstract': 'We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large Language Models (mLLMs), revealing how backdoors inserted in one language can automatically transfer to others through shared embedding spaces. Using toxicity classification as a case study, we demonstrate that attackers can compromise multilingual systems by poisoning data in a single language, with rare tokens serving as specific effective triggers. Our findings expose a critical vulnerability in the fundamental architecture that enables cross-lingual transfer in these models. Our code and data are publicly available at this https URL.', 'abstract_zh': '多语言大型语言模型中的跨语言后门攻击（X-BAT）探究：揭示一种语言中植入后门如何通过共享嵌入空间自动转移到其他语言中的机制。以毒性分类为例，我们展示攻击者可以通过污染单一语言的数据来破坏多语言系统，其中稀有令牌作为具体的有效触发器。我们的发现揭示了这些模型中实现跨语言转移的基本架构中存在的关键漏洞。我们的代码和数据已在此处公开：this https URL。', 'title_zh': 'Char-mander 使用 mBackdoor！多语言LLM中的跨语言后门攻击研究'}
{'arxiv_id': 'arXiv:2502.16896', 'title': 'Zero-shot Load Forecasting for Integrated Energy Systems: A Large Language Model-based Framework with Multi-task Learning', 'authors': 'Jiaheng Li, Donghe Li, Ye Yang, Huan Xi, Yu Xiao, Li Sun, Dou An, Qingyu Yang', 'link': 'https://arxiv.org/abs/2502.16896', 'abstract': "The growing penetration of renewable energy sources in power systems has increased the complexity and uncertainty of load forecasting, especially for integrated energy systems with multiple energy carriers. Traditional forecasting methods heavily rely on historical data and exhibit limited transferability across different scenarios, posing significant challenges for emerging applications in smart grids and energy internet. This paper proposes the TSLLM-Load Forecasting Mechanism, a novel zero-shot load forecasting framework based on large language models (LLMs) to address these challenges. The framework consists of three key components: a data preprocessing module that handles multi-source energy load data, a time series prompt generation module that bridges the semantic gap between energy data and LLMs through multi-task learning and similarity alignment, and a prediction module that leverages pre-trained LLMs for accurate forecasting. The framework's effectiveness was validated on a real-world dataset comprising load profiles from 20 Australian solar-powered households, demonstrating superior performance in both conventional and zero-shot scenarios. In conventional testing, our method achieved a Mean Squared Error (MSE) of 0.4163 and a Mean Absolute Error (MAE) of 0.3760, outperforming existing approaches by at least 8\\%. In zero-shot prediction experiments across 19 households, the framework maintained consistent accuracy with a total MSE of 11.2712 and MAE of 7.6709, showing at least 12\\% improvement over current methods. The results validate the framework's potential for accurate and transferable load forecasting in integrated energy systems, particularly beneficial for renewable energy integration and smart grid applications.", 'abstract_zh': '基于大语言模型的TSLLM-负荷预测机制', 'title_zh': '基于多任务学习的大语言模型框架下的零样本负荷预测方法：综合能源系统中的应用'}
{'arxiv_id': 'arXiv:2502.16886', 'title': 'DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance', 'authors': 'Xuanfan Ni, Liyan Xu, Chenyang Lyu, Longyue Wang, Mo Yu, Lemao Liu, Fandong Meng, Jie Zhou, Piji Li', 'link': 'https://arxiv.org/abs/2502.16886', 'abstract': 'To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. However, these techniques often require a pre-defined cache budget; as the optimal budget varies with different input lengths and task types, it limits their practical deployment accepting open-domain instructions. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods.', 'abstract_zh': '为了缓解在大语言模型（LLMs）推理过程中对内存的负担，许多研究集中在通过探索注意力稀疏性等方式压缩KV缓存。然而，这些技术通常需要预先定义的缓存预算；由于最优预算会随着输入长度和任务类型的不同而变化，这限制了它们在处理开放域指令时的实用性。为了解决这一局限，我们提出了一种新的KV缓存压缩目标：在任何情况下始终确保全缓存性能，同时尽可能最大化KV缓存的剪枝。为了实现这一目标，我们引入了一种名为DBudgetKV的新KV缓存压缩方法，该方法利用基于注意力的指标来检测剩余KV缓存很可能无法达到全缓存性能，然后停止剪枝过程。跨不同上下文长度、任务类型和模型规模的实证评估表明，我们的方法能够有效地、鲁棒地实现无损KV剪枝，平均压缩比超过25%。此外，我们的方法易于集成到LLM推理中，不仅优化了内存空间，还展示了相较于现有方法的缩短推理时间。', 'title_zh': 'DBudgetKV：键值缓存压缩中的动态预算以确保最优性能'}
{'arxiv_id': 'arXiv:2502.16880', 'title': 'CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter', 'authors': 'Yepeng Weng, Dianwen Mei, Huishi Qiu, Xujie Chen, Li Liu, Jiang Tian, Zhongchao Shi', 'link': 'https://arxiv.org/abs/2502.16880', 'abstract': 'Speculative decoding is a powerful technique that accelerates Large Language Model (LLM) inference by leveraging a lightweight speculative draft model. However, existing designs suffers in performance due to misalignment between training and inference. Recent methods have tried to solve this issue by adopting a multi-step training strategy, but the complex inputs of different training steps make it harder for the draft model to converge. To address this, we propose CORAL, a novel framework that improves both accuracy and efficiency in speculative drafting. CORAL introduces Cross-Step Representation Alignment, a method that enhances consistency across multiple training steps, significantly improving speculative drafting performance. Additionally, we identify the LM head as a major bottleneck in the inference speed of the draft model. We introduce a weight-grouping mechanism that selectively activates a subset of LM head parameters during inference, substantially reducing the latency of the draft model. We evaluate CORAL on three LLM families and three benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that CORAL effectively mitigates training-inference misalignment and delivers significant speedup for modern LLMs with large vocabularies.', 'abstract_zh': '推测解码是通过利用轻量级推测性草稿模型加速大型语言模型（LLM）推理的一项强大技术。然而，现有设计由于训练与推理之间的不对齐而在性能上存在问题。最近的方法尝试通过采用多步训练策略来解决这一问题，但不同训练步骤的复杂输入使得草稿模型难以收敛。为解决这一问题，我们提出了一种名为CORAL的新型框架，该框架在推测性草稿中同时提升了准确性和效率。CORAL引入了跨步表示对齐方法，增强了多步之间的一致性，显著提高了推测性草稿性能。此外，我们识别语言模型头作为草稿模型推理速度的主要瓶颈。我们引入了一种权重分组机制，在推理过程中选择性激活语言模型头的一组参数，显著减少了草稿模型的延迟。我们在三个LLM家族和三个基准数据集上评估了CORAL，实现了2.50x-4.07x的加速比，优于EAGLE-2和HASS等最先进的方法。我们的结果表明，CORAL有效缓解了训练与推理之间的不一致性，并显著提升了具有大词汇量的现代LLM的推理速度。', 'title_zh': 'CORAL：学习多步训练中的一致表示 with 轻量级推测式起草者'}
{'arxiv_id': 'arXiv:2502.16868', 'title': "Graphy'our Data: Towards End-to-End Modeling, Exploring and Generating Report from Raw Data", 'authors': 'Longbin Lai, Changwei Luo, Yunkai Lou, Mingchen Ju, Zhengyi Yang', 'link': 'https://arxiv.org/abs/2502.16868', 'abstract': 'Large Language Models (LLMs) have recently demonstrated remarkable performance in tasks such as Retrieval-Augmented Generation (RAG) and autonomous AI agent workflows. Yet, when faced with large sets of unstructured documents requiring progressive exploration, analysis, and synthesis, such as conducting literature survey, existing approaches often fall short. We address this challenge -- termed Progressive Document Investigation -- by introducing Graphy, an end-to-end platform that automates data modeling, exploration and high-quality report generation in a user-friendly manner. Graphy comprises an offline Scrapper that transforms raw documents into a structured graph of Fact and Dimension nodes, and an online Surveyor that enables iterative exploration and LLM-driven report generation. We showcase a pre-scrapped graph of over 50,000 papers -- complete with their references -- demonstrating how Graphy facilitates the literature-survey scenario. The demonstration video can be found at this https URL.', 'abstract_zh': '大型语言模型（LLMs）在检索增强生成（RAG）和自主AI代理工作流等任务中展示了卓越的性能。然而，当面对需要逐步探索、分析和综合的大量无结构文档时，如进行文献综述，现有方法往往力不从心。我们通过引入Graphy这一端到端平台来应对这一挑战，该平台以用户友好的方式自动进行数据建模、探索和高质量报告生成。Graphy包括一个离线抓取器，将原始文档转换为由事实和维度节点构成的结构化图，以及一个在线调研器，支持迭代探索和基于大型语言模型的报告生成。我们展示了包含超过50,000篇论文及其参考文献的预抓取图，展示了Graphy如何促进文献综述场景。相关演示视频可在此处访问：this https URL。', 'title_zh': '绘图your数据：走向端到端建模，从原始数据探索和生成报告'}
{'arxiv_id': 'arXiv:2502.16857', 'title': 'Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models', 'authors': 'Avinash Trivedi, Sangeetha Sivanesan', 'link': 'https://arxiv.org/abs/2502.16857', 'abstract': 'This paper presents an effective approach to detect AI-generated text, developed for the Defactify 4.0 shared task at the fourth workshop on multimodal fact checking and hate speech detection. The task consists of two subtasks: Task-A, classifying whether a text is AI generated or human written, and Task-B, classifying the specific large language model that generated the text. Our team (Sarang) achieved the 1st place in both tasks with F1 scores of 1.0 and 0.9531, respectively. The methodology involves adding noise to the dataset to improve model robustness and generalization. We used an ensemble of DeBERTa models to effectively capture complex patterns in the text. The result indicates the effectiveness of our noise-driven and ensemble-based approach, setting a new standard in AI-generated text detection and providing guidance for future developments.', 'abstract_zh': '本文介绍了在第四届多模态事实核查和仇恨言论检测研讨会Defactify 4.0 共享任务中开发的一种有效检测AI生成文本的方法。该任务包括两个子任务：Task-A，分类文本是AI生成还是人类撰写；Task-B，分类具体是哪个大型语言模型生成的文本。我们的团队（Sarang）在两个子任务中分别以F1分数1.0和0.9531的成绩获得第一名。该方法包括向数据集中添加噪声以提高模型的鲁棒性和泛化能力，并使用DeBERTa模型集成有效捕捉文本中的复杂模式。结果表明，我们的噪声驱动和集成方法的有效性，为AI生成文本检测设定了一项新标准，并为未来的发展提供了指导。', 'title_zh': 'Sarang 在 DEFACTIFY 4.0：使用噪声数据和 DeBERTa 模型ensemble 进行生成文本检测'}
{'arxiv_id': 'arXiv:2502.16852', 'title': 'Improving LLM General Preference Alignment via Optimistic Online Mirror Descent', 'authors': 'Yuheng Zhang, Dian Yu, Tao Ge, Linfeng Song, Zhichen Zeng, Haitao Mi, Nan Jiang, Dong Yu', 'link': 'https://arxiv.org/abs/2502.16852', 'abstract': 'Reinforcement learning from human feedback (RLHF) has demonstrated remarkable effectiveness in aligning large language models (LLMs) with human preferences. Many existing alignment approaches rely on the Bradley-Terry (BT) model assumption, which assumes the existence of a ground-truth reward for each prompt-response pair. However, this assumption can be overly restrictive when modeling complex human preferences. In this paper, we drop the BT model assumption and study LLM alignment under general preferences, formulated as a two-player game. Drawing on theoretical insights from learning in games, we integrate optimistic online mirror descent into our alignment framework to approximate the Nash policy. Theoretically, we demonstrate that our approach achieves an $O(T^{-1})$ bound on the duality gap, improving upon the previous $O(T^{-1/2})$ result. More importantly, we implement our method and show through experiments that it outperforms state-of-the-art RLHF algorithms across multiple representative benchmarks.', 'abstract_zh': '从人类反馈中强化学习（RLHF）在使大型语言模型与人类偏好对齐方面展现了显著效果。许多现有的对齐方法依赖于Bradley-Terry（BT）模型假设，该假设认为每个提問-回应对都存在一个真实的奖励。然而，当建模复杂的人类偏好时，这种假设可能会过于严格。在本文中，我们放弃了BT模型假设，并研究了一般偏好下的大型语言模型对齐，将其形式化为一个两玩家博弈。借鉴博弈中学习的理论见解，我们将乐观在线对数下降法融入我们的对齐框架以近似纳什策略。理论上，我们证明了我们的方法在对偶间隙上达到$O(T^{-1})$的界，优于先前的$O(T^{-1/2})$结果。更为重要的是，我们在实验中实现了该方法，表明它在多个代表性基准测试中优于最先进的RLHF算法。', 'title_zh': '通过乐观在线镜像下降方法提高大语言模型通用偏好对齐'}
{'arxiv_id': 'arXiv:2502.16820', 'title': 'Uncertainty Quantification of Large Language Models through Multi-Dimensional Responses', 'authors': 'Tiejin Chen, Xiaoou Liu, Longchao Da, Xiaoou Liu, Vagelis Papalexakis, Hua Wei', 'link': 'https://arxiv.org/abs/2502.16820', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks due to large training datasets and powerful transformer architecture. However, the reliability of responses from LLMs remains a question. Uncertainty quantification (UQ) of LLMs is crucial for ensuring their reliability, especially in areas such as healthcare, finance, and decision-making. Existing UQ methods primarily focus on semantic similarity, overlooking the deeper knowledge dimensions embedded in responses. We introduce a multi-dimensional UQ framework that integrates semantic and knowledge-aware similarity analysis. By generating multiple responses and leveraging auxiliary LLMs to extract implicit knowledge, we construct separate similarity matrices and apply tensor decomposition to derive a comprehensive uncertainty representation. This approach disentangles overlapping information from both semantic and knowledge dimensions, capturing both semantic variations and factual consistency, leading to more accurate UQ. Our empirical evaluations demonstrate that our method outperforms existing techniques in identifying uncertain responses, offering a more robust framework for enhancing LLM reliability in high-stakes applications.', 'abstract_zh': '大型语言模型（LLMs）由于大规模训练数据集和强大的变换器架构，在各种任务中展现了卓越的能力。然而，LLMs响应的可靠性仍然是一个问题。对LLMs进行不确定性量化（UQ）是确保其可靠性的重要途径，尤其是在医疗保健、金融和决策等领域。现有UQ方法主要关注语义相似性，忽略了响应中嵌入的深层次知识维度。我们介绍了将语义和知识感知相似性分析相结合的多维度UQ框架。通过生成多个响应并利用辅助LLM提取隐含知识，我们构建了独立的相似性矩阵并应用张量分解以获得综合的不确定性表示。这种方法从语义和知识维度中解缠了重叠信息，同时捕捉到了语义变异性与事实一致性，从而提高了UQ的准确性。我们的实验证明，该方法在识别不确定响应方面优于现有技术，为在高风险应用中增强LLMs可靠性提供了更稳健的框架。', 'title_zh': '大型语言模型多维度响应中的不确定性量化'}
{'arxiv_id': 'arXiv:2502.16804', 'title': 'Multi-Agent Autonomous Driving Systems with Large Language Models: A Survey of Recent Advances', 'authors': 'Yaozu Wu, Dongyuan Li, Yankai Chen, Renhe Jiang, Henry Peng Zou, Liancheng Fang, Zhen Wang, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.16804', 'abstract': 'Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety. Large Language Models (LLMs), known for their exceptional planning and reasoning capabilities, have been integrated into ADSs to assist with driving decision-making. However, LLM-based single-agent ADSs face three major challenges: limited perception, insufficient collaboration, and high computational demands. To address these issues, recent advancements in LLM-based multi-agent ADSs have focused on improving inter-agent communication and cooperation. This paper provides a frontier survey of LLM-based multi-agent ADSs. We begin with a background introduction to related concepts, followed by a categorization of existing LLM-based approaches based on different agent interaction modes. We then discuss agent-human interactions in scenarios where LLM-based agents engage with humans. Finally, we summarize key applications, datasets, and challenges in this field to support future research (this https URL).', 'abstract_zh': '基于大型语言模型的多剂自动驾驶系统：前沿综述', 'title_zh': '基于大型语言模型的多Agent自主驾驶系统：近期进展综述'}
{'arxiv_id': 'arXiv:2502.16802', 'title': 'Unsupervised Topic Models are Data Mixers for Pre-training Language Models', 'authors': 'Jiahui Peng, Xinlin Zhuang, Qiu Jiantao, Ren Ma, Jing Yu, Tianyi Bai, Conghui He', 'link': 'https://arxiv.org/abs/2502.16802', 'abstract': 'The performance of large language models (LLMs) is significantly affected by the quality and composition of their pre-training data, which is inherently diverse, spanning various domains, sources, and topics. Effectively integrating these heterogeneous data sources is crucial for optimizing LLM performance. Previous research has predominantly concentrated on domain-based data mixing, often neglecting the nuanced topic-level characteristics of the data. To address this gap, we propose a simple yet effective topic-based data mixing strategy that utilizes fine-grained topics generated through our topic modeling method, DataWeave. DataWeave employs a multi-stage clustering process to group semantically similar documents and utilizes LLMs to generate detailed topics, thereby facilitating a more nuanced understanding of dataset composition. Our strategy employs heuristic methods to upsample or downsample specific topics, which significantly enhances LLM performance on downstream tasks, achieving superior results compared to previous, more complex data mixing approaches. Furthermore, we confirm that the topics Science and Relationships are particularly effective, yielding the most substantial performance improvements. We will make our code and datasets publicly available.', 'abstract_zh': '大型语言模型（LLMs）的表现受其预训练数据的质量和组成影响，这些数据在本质上有很高的多样性，涉及多个领域、来源和主题。有效整合这些异构数据源对于优化LLM性能至关重要。以往的研究主要集中在基于领域的数据混合，常常忽视了数据在主题层面的细微差异。为填补这一空白，我们提出了一种简单有效的基于主题的数据混合策略，该策略利用通过我们的主题建模方法DataWeave生成的细粒度主题。DataWeave采用多阶段聚类过程对语义相似的文档进行分组，并利用LLMs生成详细的主题，从而有助于更细致地理解数据集组成。该策略采用启发式方法对特定主题进行上采样或下采样，显著提高了LLM在下游任务上的性能，取得了优于以往更为复杂的数据混合方法的结果。此外，我们证实科学和关系主题特别有效，提供了最大的性能改进。我们将公开我们的代码和数据集。', 'title_zh': '无监督主题模型是预训练语言模型的数据混音器'}
{'arxiv_id': 'arXiv:2502.16794', 'title': 'AAD-LLM: Neural Attention-Driven Auditory Scene Understanding', 'authors': 'Xilin Jiang, Sukru Samet Dindar, Vishal Choudhari, Stephan Bickel, Ashesh Mehta, Guy M McKhann, Adeen Flinker, Daniel Friedman, Nima Mesgarani', 'link': 'https://arxiv.org/abs/2502.16794', 'abstract': 'Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce Intention-Informed Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo and code available: this https URL.', 'abstract_zh': '基于意图的听觉场景理解（II-ASU）和听觉注意驱动的大语言模型（AAD-LLM）', 'title_zh': 'AAD-LLM: 基于神经注意力的声音场景理解'}
{'arxiv_id': 'arXiv:2502.16792', 'title': 'The Role of Sparsity for Length Generalization in Transformers', 'authors': 'Noah Golowich, Samy Jelassi, David Brandfonbrener, Sham M. Kakade, Eran Malach', 'link': 'https://arxiv.org/abs/2502.16792', 'abstract': "Training large language models to predict beyond their training context lengths has drawn much attention in recent years, yet the principles driving such behavior of length generalization remain underexplored. We propose a new theoretical framework to study length generalization for the next-token prediction task, as performed by decoder-only transformers. Conceptually, we show that length generalization occurs as long as each predicted token depends on a small (fixed) number of previous tokens. We formalize such tasks via a notion we call $k$-sparse planted correlation distributions, and show that an idealized model of transformers which generalize attention heads successfully length-generalize on such tasks. As a bonus, our theoretical model justifies certain techniques to modify positional embeddings which have been introduced to improve length generalization, such as position coupling.\nWe support our theoretical results with experiments on synthetic tasks and natural language, which confirm that a key factor driving length generalization is a ``sparse'' dependency structure of each token on the previous ones. Inspired by our theory, we introduce Predictive Position Coupling, which trains the transformer to predict the position IDs used in a positional coupling approach. Predictive Position Coupling thereby allows us to broaden the array of tasks to which position coupling can successfully be applied to achieve length generalization.", 'abstract_zh': '训练大型语言模型以超出训练上下文长度进行预测引起了近年来的广泛关注，但驱动这种长度泛化的原理仍待深入探索。我们提出了一种新的理论框架，以研究仅解码器变换器在下一个标记预测任务中的长度泛化现象。从概念上讲，我们表明，只要每个预测的标记依赖于少量（固定数量）的先前标记，长度泛化就会发生。我们通过一种我们称为$k$-稀疏植入相关分布的概念来形式化此类任务，并证明理想化的变换器模型能够成功地在这些任务上进行长度泛化。此外，我们的理论模型还解释了某些已被证明能改善长度泛化的位置嵌入修改技术，如位置耦合。\n\n我们的理论结果通过在合成任务和自然语言上的实验得到了支持，实验确认驱动长度泛化的关键因素是每个标记对先前标记的“稀疏”依赖结构。受我们理论的启发，我们引入了预测位置耦合，该方法训练变换器预测位置耦合方法中使用的位置ID。预测位置耦合因此使我们能够扩大应用位置耦合以实现长度泛化的任务范围。', 'title_zh': 'Transformer中稀疏性在长度泛化中的作用'}
{'arxiv_id': 'arXiv:2502.16789', 'title': 'AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay', 'authors': 'Ziyi Tang, Zechuan Chen, Jiarui Yang, Jiayao Mai, Yongsen Zheng, Keze Wang, Jinrui Chen, Liang Lin', 'link': 'https://arxiv.org/abs/2502.16789', 'abstract': 'Alpha mining, a critical component in quantitative investment, focuses on discovering predictive signals for future asset returns in increasingly complex financial markets. However, the pervasive issue of alpha decay, where factors lose their predictive power over time, poses a significant challenge for alpha mining. Traditional methods like genetic programming face rapid alpha decay from overfitting and complexity, while approaches driven by Large Language Models (LLMs), despite their promise, often rely too heavily on existing knowledge, creating homogeneous factors that worsen crowding and accelerate decay. To address this challenge, we propose AlphaAgent, an autonomous framework that effectively integrates LLM agents with ad hoc regularizations for mining decay-resistant alpha factors. AlphaAgent employs three key mechanisms: (i) originality enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (iii) complexity control via AST-based structural constraints, preventing over-engineered constructions that are prone to overfitting. These mechanisms collectively guide the alpha generation process to balance originality, financial rationale, and adaptability to evolving market conditions, mitigating the risk of alpha decay. Extensive evaluations show that AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay across bull and bear markets, consistently delivering significant alpha in Chinese CSI 500 and US S&P 500 markets over the past four years. Notably, AlphaAgent showcases remarkable resistance to alpha decay, elevating the potential for yielding powerful factors.', 'abstract_zh': 'Alpha 矿掘：一种在量化投资中关键的组件，专注于在日益复杂金融市场中发现预测未来资产回报的信号。然而，普遍存在的 Alpha 衰减问题，即因素随时间失去预测能力，对 Alpha 矿掘构成了重大挑战。传统的遗传程序方法容易因过拟合和复杂性而导致 Alpha 衰减，而大型语言模型（LLMs）驱动的方法虽然有潜力，但由于过于依赖现有知识，往往会生成同质化因素，加剧拥挤并加速衰减。为应对这一挑战，我们提出 AlphaAgent，一种自主框架，有效整合 LLM 代理与自适应正则化，以挖掘抗衰减 Alpha 因子。AlphaAgent 采用三种关键机制：（i）基于抽象语法树（AST）的相似性度量强制原始性，与现有 Alpha 对比，（ii）假设-因子对齐，利用 LLM 评估市场假设与生成因子间的语义一致性，以及（iii）基于 AST 的结构约束控制复杂性，防止易过拟合的过度设计。这些机制共同指导 Alpha 生成过程，平衡原始性、金融合理性以及对不断变化市场条件的适应性，从而降低 Alpha 衰减的风险。广泛评估显示，AlphaAgent 在牛熊市中优于传统和 LLM 基础方法，在过去四年中为中国的 CSI 500 和美国的 S&P 500 市场持续提供显著的 Alpha，同时表现出对 Alpha 衰减的显著抵抗力，提升了生成强因子的潜力。', 'title_zh': 'AlphaAgent：带有正则化探索以对抗alpha衰减的LLM驱动alpha挖掘'}
{'arxiv_id': 'arXiv:2502.16770', 'title': 'LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint', 'authors': 'Qianli Ma, Dongrui Liu, Qian Chen, Linfeng Zhang, Jing Shao', 'link': 'https://arxiv.org/abs/2502.16770', 'abstract': 'Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: \\textbf{neuron misidentification} due to simplistic parameter magnitude-based selection, and \\textbf{cross-task neuron interference} during merging. To address these challenges, we propose \\textbf{LED-Merging}, a three-stage framework that \\textbf{L}ocates task-specific neurons via gradient-based attribution, dynamically \\textbf{E}lects critical neurons through multi-model importance fusion, and \\textbf{D}isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(\\emph{e.g.}, a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\\% of utility performance(\\emph{e.g.}, 52.39\\% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs.', 'abstract_zh': 'Fine-tuning 预训练大型语言模型 (LLMs) 以适应特定任务会带来显著的计算和数据成本。虽然模型合并提供了一种无需训练即可整合多个任务特定模型的解决方案，但现有方法存在安全性和实用性的冲突，其中增强的一般能力会削弱安全性保障。我们确定了两个根本原因：基于简单参数量纲选择的神经元误识别，以及合并过程中的跨任务神经元干扰。为了解决这些挑战，我们提出了一种三阶段框架 LED-Merging，该框架通过梯度 attribution 定位任务特定神经元，通过多模型重要性融合动态选择关键神经元，并通过参数隔离排除冲突更新。在 Llama-3-8B、Mistral-7B 和 Llama2-13B 上的广泛实验表明，LED-Merging 降低了有害响应率（例如，Llama-3-8B-Instruct 上的 HarmBench 减少 31.4%），同时保留了 95% 的实用性性能（例如，GSM8K 上的准确率达到 52.39%）。LED-Merging 解决了安全性和实用性的冲突，并提供了一种轻量级、无需训练的框架，用于构建可靠的多任务 LLMs。', 'title_zh': 'LED-Merging: 在位置互斥条件下缓解模型合并中的安全-效用冲突'}
{'arxiv_id': 'arXiv:2502.16747', 'title': 'SQLong: Enhanced NL2SQL for Longer Contexts with LLMs', 'authors': 'Dai Quoc Nguyen, Cong Duy Vu Hoang, Duy Vu, Gioacchino Tangari, Thanh Tien Vu, Don Dharmasiri, Yuan-Fang Li, Long Duong', 'link': 'https://arxiv.org/abs/2502.16747', 'abstract': "Open-weight large language models (LLMs) have significantly advanced performance in the Natural Language to SQL (NL2SQL) task. However, their effectiveness diminishes when dealing with large database schemas, as the context length increases. To address this limitation, we present SQLong, a novel and efficient data augmentation framework designed to enhance LLM performance in long-context scenarios for the NL2SQL task. SQLong generates augmented datasets by extending existing database schemas with additional synthetic CREATE TABLE commands and corresponding data rows, sampled from diverse schemas in the training data. This approach effectively simulates long-context scenarios during finetuning and evaluation. Through experiments on the Spider and BIRD datasets, we demonstrate that LLMs finetuned with SQLong-augmented data significantly outperform those trained on standard datasets. These imply SQLong's practical implementation and its impact on improving NL2SQL capabilities in real-world settings with complex database schemas.", 'abstract_zh': 'Open-weight大型语言模型（LLMs）在自然语言到SQL（NL2SQL）任务中取得了显著进展。然而，在处理大型数据库模式时，随着上下文长度的增加，其效果减弱。为解决这一限制，我们提出了SQLong，一种新颖且高效的數據 augmentation 框架，旨在增强LLM在长上下文情景下NL2SQL任务中的性能。SQLong通过扩展现有的数据库模式，添加额外的合成CREATE TABLE命令及其相应的数据行，从训练数据中的多种模式中采样，以有效模拟长上下文情景。通过在Spider和BIRD数据集上的实验，我们证明使用SQLong增强的数据微调的LLMs明显优于使用标准数据集训练的LLMs。这表明SQLong的实际应用及其在现实世界复杂数据库模式下提升NL2SQL能力方面的影响。', 'title_zh': 'SQLong: 通过LLM增强的长上下文NL2SQL'}
{'arxiv_id': 'arXiv:2502.16730', 'title': 'RapidPen: Fully Automated IP-to-Shell Penetration Testing with LLM-based Agents', 'authors': 'Sho Nakatani', 'link': 'https://arxiv.org/abs/2502.16730', 'abstract': 'We present RapidPen, a fully automated penetration testing (pentesting) framework that addresses\nthe challenge of achieving an initial foothold (IP-to-Shell) without human intervention. Unlike prior\napproaches that focus primarily on post-exploitation or require a human-in-the-loop, RapidPen\nleverages large language models (LLMs) to autonomously discover and exploit vulnerabilities, starting from\na single IP address. By integrating advanced ReAct-style task planning (Re) with retrieval-augmented\nknowledge bases of successful exploits, along with a command-generation and direct execution feedback loop\n(Act), RapidPen systematically scans services, identifies viable attack vectors, and executes targeted\nexploits in a fully automated manner.\nIn our evaluation against a vulnerable target from the Hack The Box platform, RapidPen achieved shell\naccess within 200-400 seconds at a per-run cost of approximately \\$0.3-\\$0.6, demonstrating a\n60\\% success rate when reusing prior "success-case" data. These results underscore the potential\nof truly autonomous pentesting for both security novices and seasoned professionals. Organizations\nwithout dedicated security teams can leverage RapidPen to quickly identify critical vulnerabilities,\nwhile expert pentesters can offload repetitive tasks and focus on complex challenges.\nUltimately, our work aims to make penetration testing more accessible and cost-efficient,\nthereby enhancing the overall security posture of modern software ecosystems.', 'abstract_zh': '快速渗透测试框架RapidPen：自动实现初始 foothold 的全自动化漏洞利用平台', 'title_zh': 'RapidPen: 基于LLM代理的完全自动化IP到Shell渗透测试方法'}
{'arxiv_id': 'arXiv:2502.16722', 'title': 'Layer-Wise Evolution of Representations in Fine-Tuned Transformers: Insights from Sparse AutoEncoders', 'authors': 'Suneel Nadipalli', 'link': 'https://arxiv.org/abs/2502.16722', 'abstract': 'Fine-tuning pre-trained transformers is a powerful technique for enhancing the performance of base models on specific tasks. From early applications in models like BERT to fine-tuning Large Language Models (LLMs), this approach has been instrumental in adapting general-purpose architectures for specialized downstream tasks. Understanding the fine-tuning process is crucial for uncovering how transformers adapt to specific objectives, retain general representations, and acquire task-specific features. This paper explores the underlying mechanisms of fine-tuning, specifically in the BERT transformer, by analyzing activation similarity, training Sparse AutoEncoders (SAEs), and visualizing token-level activations across different layers. Based on experiments conducted across multiple datasets and BERT layers, we observe a steady progression in how features adapt to the task at hand: early layers primarily retain general representations, middle layers act as a transition between general and task-specific features, and later layers fully specialize in task adaptation. These findings provide key insights into the inner workings of fine-tuning and its impact on representation learning within transformer architectures.', 'abstract_zh': '预训练变压器的微调是一种增强基模型在特定任务上性能的强大技术。从早期的BERT模型应用到大型语言模型（LLMs）的微调，这一方法对于适应通用架构以应对专门的下游任务起到了关键作用。深入理解微调过程对于揭示变压器如何适应特定目标、保留通用表示并获得任务特定特征至关重要。本文通过分析激活相似性、训练稀疏自编码器（SAEs）以及可视化不同层的标记级激活，探讨了BERT变压器的微调机制。基于在多个数据集和BERT层次上的实验，我们观察到特征适应任务的过程呈现出稳步进展：早期层主要保留通用表示，中间层作为通用表示与任务特定特征之间的过渡，而后续层则完全专注于任务适应。这些发现对于我们理解微调的内部机制及其对变压器架构中表示学习的影响具有重要意义。', 'title_zh': '细调变换器中表示的逐层演进：来自稀疏自编码器的见解'}
{'arxiv_id': 'arXiv:2502.16721', 'title': 'Speed and Conversational Large Language Models: Not All Is About Tokens per Second', 'authors': 'Javier Conde, Miguel González, Pedro Reviriego, Zhen Gao, Shanshan Liu, Fabrizio Lombardi', 'link': 'https://arxiv.org/abs/2502.16721', 'abstract': 'The speed of open-weights large language models (LLMs) and its dependency on the task at hand, when run on GPUs, is studied to present a comparative analysis of the speed of the most popular open LLMs.', 'abstract_zh': '基于GPU运行时，开放权重大型语言模型（LLMs）的速度及其对任务的依赖性研究：面向流行开放LLM的速度比较分析', 'title_zh': '速度与对话型大规模语言模型：不仅关乎每秒令牌数'}
{'arxiv_id': 'arXiv:2502.16706', 'title': 'DISC: Dynamic Decomposition Improves LLM Inference Scaling', 'authors': 'Jonathan Light, Wei Cheng, Wu Yue, Masafumi Oyamada, Mengdi Wang, Santiago Paternain, Haifeng Chen', 'link': 'https://arxiv.org/abs/2502.16706', 'abstract': 'Many inference scaling methods work by breaking a problem into smaller steps (or groups of tokens), then sampling and choosing the best next step. However, these steps and their sizes are usually predetermined based on human intuition or domain knowledge. This paper introduces dynamic decomposition, a method that automatically and adaptively splits solution and reasoning traces into steps during inference. This approach improves computational efficiency by focusing more resources on difficult steps, breaking them down further and prioritizing their sampling. Experiments on coding and math benchmarks (APPS, MATH, and LiveCodeBench) show that dynamic decomposition performs better than static methods, which rely on fixed steps like token-level, sentence-level, or single-step decompositions. These results suggest that dynamic decomposition can enhance many inference scaling techniques.', 'abstract_zh': '动态分解：一种自动适应的推理拆分方法', 'title_zh': 'DISC: 动态分解优化大语言模型推理扩展'}
{'arxiv_id': 'arXiv:2502.16705', 'title': 'Can ChatGPT Learn to Count Letters?', 'authors': 'Javier Conde, Gonzalo Martínez, Pedro Reviriego, Zhen Gao, Shanshan Liu, Fabrizio Lombardi', 'link': 'https://arxiv.org/abs/2502.16705', 'abstract': 'Large language models (LLMs) struggle on simple tasks such as counting the number of occurrences of a letter in a word. In this paper, we investigate if ChatGPT can learn to count letters and propose an efficient solution.', 'abstract_zh': '大型语言模型（LLMs）在诸如统计单词中某个字母出现次数这类简单任务上表现不佳。本文我们探讨ChatGPT是否能学会计数字母，并提出一个有效的解决方案。', 'title_zh': 'ChatGPT能学会数字母吗？'}
{'arxiv_id': 'arXiv:2502.16704', 'title': 'Code Summarization Beyond Function Level', 'authors': 'Vladimir Makharev, Vladimir Ivanov', 'link': 'https://arxiv.org/abs/2502.16704', 'abstract': 'Code summarization is a critical task in natural language processing and software engineering, which aims to generate concise descriptions of source code. Recent advancements have improved the quality of these summaries, enhancing code readability and maintainability. However, the content of a repository or a class has not been considered in function code summarization. This study investigated the effectiveness of code summarization models beyond the function level, exploring the impact of class and repository contexts on the summary quality. The study involved revising benchmarks for evaluating models at class and repository levels, assessing baseline models, and evaluating LLMs with in-context learning to determine the enhancement of summary quality with additional context. The findings revealed that the fine-tuned state-of-the-art CodeT5+ base model excelled in code summarization, while incorporating few-shot learning and retrieved code chunks from RAG significantly enhanced the performance of LLMs in this task. Notably, the Deepseek Coder 1.3B and Starcoder2 15B models demonstrated substantial improvements in metrics such as BLEURT, METEOR, and BLEU-4 at both class and repository levels. Repository-level summarization exhibited promising potential but necessitates significant computational resources and gains from the inclusion of structured context. Lastly, we employed the recent SIDE code summarization metric in our evaluation. This study contributes to refining strategies for prompt engineering, few-shot learning, and RAG, addressing gaps in benchmarks for code summarization at various levels. Finally, we publish all study details, code, datasets, and results of evaluation in the GitHub repository available at this https URL.', 'abstract_zh': '代码总结是自然语言处理和软件工程中的关键任务，旨在生成源代码的简洁描述。近年来的进步提高了这些总结的质量，增强了代码的可读性和可维护性。然而，库或类的内容尚未在函数代码总结中加以考虑。本研究探讨了函数以上级别进行代码总结的有效性，研究了类和库上下文对总结质量的影响。研究修订了评估模型在类和库级别上的基准，评估了基线模型，并评估了具有上下文学习的LLMs，以确定额外上下文对总结质量的增强效果。研究发现，微调的最新CodeT5+基础模型在代码总结中表现出色，而结合少量学习和从RAG检索代码片段显著提升了LLMs在该任务上的性能。值得注意的是，Deepseek Coder 1.3B和Starcoder2 15B模型在BLEURT、METEOR和BLEU-4等指标上，在类和库级别上表现出显著改进。库级总结展现出巨大潜力，但需要大量计算资源，并且可以从结构化上下文的包含中获益。最后，我们使用了最近的SIDE代码总结指标进行评估。本研究为细化提示工程、少量学习和RAG策略做出了贡献，填补了代码总结基准在不同级别上的空白。最后，我们在以下GitHub仓库中发布了所有研究细节、代码、数据集和评估结果：[此 https URL]。', 'title_zh': '超越函数级别的心代码摘要'}
{'arxiv_id': 'arXiv:2502.16701', 'title': 'Beyond Release: Access Considerations for Generative AI Systems', 'authors': 'Irene Solaiman, Rishi Bommasani, Dan Hendrycks, Ariel Herbert-Voss, Yacine Jernite, Aviya Skowron, Andrew Trask', 'link': 'https://arxiv.org/abs/2502.16701', 'abstract': 'Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access refers to practical needs, infrastructurally, technically, and societally, in order to use available components in some way. We deconstruct access along three axes: resourcing, technical usability, and utility. Within each category, a set of variables per system component clarify tradeoffs. For example, resourcing requires access to computing infrastructure to serve model weights. We also compare the accessibility of four high performance language models, two open-weight and two closed-weight, showing similar considerations for all based instead on access variables. Access variables set the foundation for being able to scale or increase access to users; we examine the scale of access and how scale affects ability to manage and intervene on risks. This framework better encompasses the landscape and risk-benefit tradeoffs of system releases to inform system release decisions, research, and policy.', 'abstract_zh': '生成式AI发布决策确定系统组件是否可供使用，但发布并不解决其他许多影响用户和利益相关者与系统互动的方式的元素。除了发布之外，系统组件的访问权限决定了潜在的风险和益处。访问涉及使用可用组件的实用需求，包括基础设施层面、技术层面和社会层面。我们将访问拆解为三个维度：资源、技术可用性和效用。在每个类别中，每个系统组件的一组变量明确了权衡。例如，资源需要访问计算基础设施来提供模型权重。我们还比较了四种高性能语言模型的可访问性，两种开源权重两种闭源权重，基于访问变量显示所有模型的相似考虑因素。访问变量奠定了能够扩展或增加用户访问的基础；我们探讨了访问范围及其对管理和应对风险能力的影响。这种框架更好地涵盖了系统的发布景观和风险收益权衡，以指导系统发布决策、研究和政策。', 'title_zh': '超越发布：生成式AI系统的内容访问考量'}
{'arxiv_id': 'arXiv:2502.16696', 'title': 'Dynamic LLM Routing and Selection based on User Preferences: Balancing Performance, Cost, and Ethics', 'authors': 'Deepak Babu Piskala, Vijay Raajaa, Sachin Mishra, Bruno Bozza', 'link': 'https://arxiv.org/abs/2502.16696', 'abstract': 'With the widespread deployment of large language models (LLMs) such as GPT4, BART, and LLaMA, the need for a system that can intelligently select the most suitable model for specific tasks while balancing cost, latency, accuracy, and ethical considerations has become increasingly important. Recognizing that not all tasks necessitate models with over 100 billion parameters, we introduce OptiRoute, an advanced model routing engine designed to dynamically select and route tasks to the optimal LLM based on detailed user-defined requirements. OptiRoute captures both functional (e.g., accuracy, speed, cost) and non-functional (e.g., helpfulness, harmlessness, honesty) criteria, leveraging lightweight task analysis and complexity estimation to efficiently match tasks with the best-fit models from a diverse array of LLMs. By employing a hybrid approach combining k-nearest neighbors (kNN) search and hierarchical filtering, OptiRoute optimizes for user priorities while minimizing computational overhead. This makes it ideal for real-time applications in cloud-based ML platforms, personalized AI services, and regulated industries.', 'abstract_zh': '随着大语言模型（LLMs）如GPT4、BART和LLaMA的广泛部署，一个能够智能地根据具体任务需求平衡成本、延迟、准确性和伦理考量选择最合适的模型的系统的需求日益重要。鉴于并非所有任务都需要超过100亿参数的模型，我们引入了OptiRoute，这是一种高级模型路由引擎，可以根据详细的用户定义要求动态选择和路由任务至最优的LLM。OptiRoute 捕捉功能性和非功能性标准（例如，准确度、速度、成本、有用性、无害性、诚实性），并通过轻量级任务分析和复杂性估计高效地将任务与多样化的LLM中最佳匹配模型相匹配。通过结合k最近邻（kNN）搜索和分层过滤的混合方法，OptiRoute 优化用户优先级，同时最小化计算开销。这使其适用于基于云的机器学习平台、个性化AI服务以及受监管行业中的实时应用。', 'title_zh': '基于用户偏好的动态大语言模型路由与选择：性能、成本与伦理的平衡'}
{'arxiv_id': 'arXiv:2502.16682', 'title': 'Automatic Input Rewriting Improves Translation with Large Language Models', 'authors': 'Dayeon Ki, Marine Carpuat', 'link': 'https://arxiv.org/abs/2502.16682', 'abstract': 'Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We show that text simplification is the most effective MT-agnostic rewrite strategy and that it can be improved further when using quality estimation to assess translatability. Human evaluation further confirms that simplified rewrites and their MT outputs both largely preserve the original meaning of the source and MT. These results suggest LLM-assisted input rewriting as a promising direction for improving translations.', 'abstract_zh': '通过自动重写输入文本，大规模语言模型能否改善机器翻译？', 'title_zh': '自动输入重写 improves 译文生成中的大规模语言模型'}
{'arxiv_id': 'arXiv:2502.16681', 'title': 'Are Sparse Autoencoders Useful? A Case Study in Sparse Probing', 'authors': 'Subhash Kantamneni, Joshua Engels, Senthooran Rajamanoharan, Max Tegmark, Neel Nanda', 'link': 'https://arxiv.org/abs/2502.16681', 'abstract': "Sparse autoencoders (SAEs) are a popular method for interpreting concepts represented in large language model (LLM) activations. However, there is a lack of evidence regarding the validity of their interpretations due to the lack of a ground truth for the concepts used by an LLM, and a growing number of works have presented problems with current SAEs. One alternative source of evidence would be demonstrating that SAEs improve performance on downstream tasks beyond existing baselines. We test this by applying SAEs to the real-world task of LLM activation probing in four regimes: data scarcity, class imbalance, label noise, and covariate shift. Due to the difficulty of detecting concepts in these challenging settings, we hypothesize that SAEs' basis of interpretable, concept-level latents should provide a useful inductive bias. However, although SAEs occasionally perform better than baselines on individual datasets, we are unable to design ensemble methods combining SAEs with baselines that consistently outperform ensemble methods solely using baselines. Additionally, although SAEs initially appear promising for identifying spurious correlations, detecting poor dataset quality, and training multi-token probes, we are able to achieve similar results with simple non-SAE baselines as well. Though we cannot discount SAEs' utility on other tasks, our findings highlight the shortcomings of current SAEs and the need to rigorously evaluate interpretability methods on downstream tasks with strong baselines.", 'abstract_zh': '基于稀疏自编码器的大型语言模型激活解释：挑战与展望', 'title_zh': '稀疏自编码器有用吗？一个关于稀疏探针的研究案例'}
{'arxiv_id': 'arXiv:2502.16660', 'title': 'BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning', 'authors': 'Haiteng Zhao, Chang Ma, FangZhi Xu, Lingpeng Kong, Zhi-Hong Deng', 'link': 'https://arxiv.org/abs/2502.16660', 'abstract': 'The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at this https URL.', 'abstract_zh': '大型语言模型在生物通路推理中的应用及其挑战与解决方案', 'title_zh': 'BioMaze: 大型语言模型在生物途径推理中的基准测试与增强'}
{'arxiv_id': 'arXiv:2502.16645', 'title': 'CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale', 'authors': 'Chenlong Wang, Zhaoyang Chu, Zhengxiang Cheng, Xuyi Yang, Kaiyue Qiu, Yao Wan, Zhou Zhao, Xuanhua Shi, Dongping Chen', 'link': 'https://arxiv.org/abs/2502.16645', 'abstract': "Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: this https URL.", 'abstract_zh': '大型语言模型（LLMs）在软件工程中表现出色，但在适应不断演变的代码知识方面面临挑战，尤其在第三方库API的频繁更新方面。为解决这一问题，本文介绍了CODESYNC，这是一种数据引擎，用于识别过时的代码模式并从Python第三方库中收集实时代码知识更新。基于CODESYNC，我们开发了CODESYNCBENCH，这是一个全面的基准测试，用于评估LLMs保持与代码演变同步的能力，覆盖了六个Python库的220个API的实际更新。该基准提供了涵盖三个评估任务的3,300个测试用例和包含2,200个训练样本的更新感知指令调整数据集。对14个最先进的LLM的广泛实验表明，即使在先进知识更新方法（如DPO、ORPO和SimPO）的支持下，它们在处理动态代码演变时仍存在问题。我们认为，我们的基准可以为未来实时代码知识更新更有效方法的发展奠定坚实的基础。实验代码和数据集已在以下网址公开：this https URL。', 'title_zh': 'CODESYNC：大规模语言模型的动态代码进化同步'}
{'arxiv_id': 'arXiv:2502.16602', 'title': 'VidLBEval: Benchmarking and Mitigating Language Bias in Video-Involved LVLMs', 'authors': 'Yiming Yang, Yangyang Guo, Hui Lu, Yan Wang', 'link': 'https://arxiv.org/abs/2502.16602', 'abstract': 'Recently, Large Vision-Language Models (LVLMs) have made significant strides across diverse multimodal tasks and benchmarks. This paper reveals a largely under-explored problem from existing video-involved LVLMs - language bias, where models tend to prioritize language over video and thus result in incorrect responses. To address this research gap, we first collect a Video Language Bias Evaluation Benchmark, which is specifically designed to assess the language bias in video-involved LVLMs through two key tasks: ambiguous video contrast and interrogative question probing. Accordingly, we design accompanied evaluation metrics that aim to penalize LVLMs being biased by language. In addition, we also propose Multi-branch Contrastive Decoding (MCD), introducing two expert branches to simultaneously counteract language bias potentially generated by the amateur text-only branch. Our experiments demonstrate that i) existing video-involved LVLMs, including both proprietary and open-sourced, are largely limited by the language bias problem; ii) our MCD can effectively mitigate this issue and maintain general-purpose capabilities in various video-involved LVLMs without any additional retraining or alteration to model architectures.', 'abstract_zh': '最近，大型视觉-语言模型（LVLMs）在多种跨模态任务和基准测试中取得了显著进展。本文揭示了现有视频涉及的LVLMs中一个未充分探索的问题——语言偏见，其中模型倾向于优先考虑语言而忽视视频，从而导致错误的回答。为填补这一研究空白，我们首先集合了一个视频语言偏见评估基准，通过两个关键任务设计来评估视频涉及的LVLMs中的语言偏见：模糊视频对比和疑问句探查。相应地，我们设计了配套的评估指标，旨在惩罚LVLMs因语言而产生的偏见。此外，我们还提出了多分支对比解码（MCD），引入了两个专家分支以同时对抗业余文本分支可能产生的语言偏见。实验表明：i) 现有的视频涉及的LVLMs，无论是专有的还是开源的，都受到语言偏见问题的严重影响；ii) 我们的MCD能够有效地缓解这一问题，在各种视频涉及的LVLMs中维持通用功能，无需额外的重新训练或修改模型架构。', 'title_zh': 'VidLBEval: 视频关联LVLM的语言偏见评估与缓解'}
{'arxiv_id': 'arXiv:2502.16584', 'title': 'Audio-FLAN: A Preliminary Release', 'authors': 'Liumeng Xue, Ziya Zhou, Jiahao Pan, Zixuan Li, Shuai Fan, Yinghao Ma, Sitong Cheng, Dongchao Yang, Haohan Guo, Yujia Xiao, Xinsheng Wang, Zixuan Shen, Chuanbo Zhu, Xinshen Zhang, Tianchi Liu, Ruibin Yuan, Zeyue Tian, Haohe Liu, Emmanouil Benetos, Ge Zhang, Yike Guo, Wei Xue', 'link': 'https://arxiv.org/abs/2502.16584', 'abstract': 'Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace and GitHub and will be continuously updated.', 'abstract_zh': '近期在音频分词领域取得的进展显著增强了将音频能力集成到大型语言模型中的能力。然而，音频理解和生成通常被视为独立任务，阻碍了真正统一的音频-语言模型的发展。虽然指令调优已在文本和视觉领域展示了卓越的推广能力和零样本学习性能，但其在音频领域的应用仍然基本未被探索。主要障碍是缺乏统一音频理解和生成的综合性数据集。为解决这一问题，我们引入了Audio-FLAN，这是一个涵盖80个跨语音、音乐和声音领域的多样化任务的大规模指令调优数据集，包含超过1亿个实例。Audio-FLAN 为零样本方式处理广泛音频领域中的理解任务（例如，转录、理解）和生成任务（例如，语音、音乐、声音）奠定了基础。Audio-FLAN 数据集可在 HuggingFace 和 GitHub 上获取，并将持续更新。', 'title_zh': '音频-FLAN：初步发布'}
{'arxiv_id': 'arXiv:2502.16565', 'title': 'The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems', 'authors': 'Zengqing Wu, Takayuki Ito', 'link': 'https://arxiv.org/abs/2502.16565', 'abstract': 'Consensus formation is pivotal in multi-agent systems (MAS), balancing collective coherence with individual diversity. Conventional LLM-based MAS primarily rely on explicit coordination, e.g., prompts or voting, risking premature homogenization. We argue that implicit consensus, where agents exchange information yet independently form decisions via in-context learning, can be more effective in dynamic environments that require long-horizon adaptability. By retaining partial diversity, systems can better explore novel strategies and cope with external shocks. We formalize a consensus-diversity tradeoff, showing conditions where implicit methods outperform explicit ones. Experiments on three scenarios -- Dynamic Disaster Response, Information Spread and Manipulation, and Dynamic Public-Goods Provision -- confirm partial deviation from group norms boosts exploration, robustness, and performance. We highlight emergent coordination via in-context learning, underscoring the value of preserving diversity for resilient decision-making.', 'abstract_zh': '共识形成在多代理系统（MAS）中至关重要，平衡集体一致性和个体多样性。传统的基于LLM的MAS主要依赖显式协调，如提示或投票，这可能导致过早的同质化。我们认为，在代理人通过上下文学习独立形成决策的信息交换中形成的隐式共识，在需要长期适应性的情境下可能更为有效。通过保留部分多样性，系统能够更好地探索新策略并应对外部冲击。我们正式化了共识与多样性的权衡，并展示了隐式方法在特定条件下的优越性。在动态灾难响应、信息传播与操控以及动态公共品供给三个场景的实验中，部分偏离群体规范被证明可以增强探索性、稳健性和性能。我们强调了通过上下文学习 emergent 协调的价值，并突出了保留多样性对于容性决策的重要性。', 'title_zh': '隐藏的分歧力量：破解适应性多agent系统中的共识-多样性权衡'}
{'arxiv_id': 'arXiv:2502.16556', 'title': 'Beyond Words: How Large Language Models Perform in Quantitative Management Problem-Solving', 'authors': 'Jonathan Kuzmanko', 'link': 'https://arxiv.org/abs/2502.16556', 'abstract': "This study examines how Large Language Models (LLMs) perform when tackling quantitative management decision problems in a zero-shot setting. Drawing on 900 responses generated by five leading models across 20 diverse managerial scenarios, our analysis explores whether these base models can deliver accurate numerical decisions under varying presentation formats, scenario complexities, and repeated attempts. Contrary to prior findings, we observed no significant effects of text presentation format (direct, narrative, or tabular) or text length on accuracy. However, scenario complexity -- particularly in terms of constraints and irrelevant parameters -- strongly influenced performance, often degrading accuracy. Surprisingly, the models handled tasks requiring multiple solution steps more effectively than expected. Notably, only 28.8\\% of responses were exactly correct, highlighting limitations in precision. We further found no significant ``learning effect'' across iterations: performance remained stable across repeated queries. Nonetheless, significant variations emerged among the five tested LLMs, with some showing superior binary accuracy. Overall, these findings underscore both the promise and the pitfalls of harnessing LLMs for complex quantitative decision-making, informing managers and researchers about optimal deployment strategies.", 'abstract_zh': '本研究探讨了大型语言模型（LLMs）在零样本设置下处理定量管理决策问题的表现。通过分析五个领先模型在20种不同管理情境下生成的900个响应，我们的分析探索了这些基础模型在不同呈现格式、情景复杂度和多次尝试下能否提供准确的数值决策。与先前的研究发现不同，我们未发现文本呈现格式（直接、叙述性或表格形式）或文本长度对准确性有显著影响。然而，情景复杂度——尤其是约束条件和无关参数——对模型表现产生了显著影响，经常导致准确度下降。令人意外的是，模型在需要多步解决的任务上表现超过了预期。值得注意的是，只有28.8%的响应完全正确，突显了精度方面的局限性。进一步的研究发现，这些模型在多次迭代中没有显著的“学习效果”，性能在重复查询中保持稳定。然而，五个测试的LLM之间出现了显著差异，一些模型在二元准确度上表现出色。总体而言，这些发现强调了利用LLMs进行复杂定量决策的潜力与挑战，为管理者和研究人员提供了优化部署策略的参考。', 'title_zh': '超越文字：大规模语言模型在定量管理问题解决中的表现'}
{'arxiv_id': 'arXiv:2502.16540', 'title': 'Advanced Chain-of-Thought Reasoning for Parameter Extraction from Documents Using Large Language Models', 'authors': 'Hong Cai Chen, Yi Pin Xu, Yang Zhang', 'link': 'https://arxiv.org/abs/2502.16540', 'abstract': "Extracting parameters from technical documentation is crucial for ensuring design precision and simulation reliability in electronic design. However, current methods struggle to handle high-dimensional design data and meet the demands of real-time processing. In electronic design automation (EDA), engineers often manually search through extensive documents to retrieve component parameters required for constructing PySpice models, a process that is both labor-intensive and time-consuming. To address this challenge, we propose an innovative framework that leverages large language models (LLMs) to automate the extraction of parameters and the generation of PySpice models directly from datasheets. Our framework introduces three Chain-of-Thought (CoT) based techniques: (1) Targeted Document Retrieval (TDR), which enables the rapid identification of relevant technical sections; (2) Iterative Retrieval Optimization (IRO), which refines the parameter search through iterative improvements; and (3) Preference Optimization (PO), which dynamically prioritizes key document sections based on relevance. Experimental results show that applying all three methods together improves retrieval precision by 47.69% and reduces processing latency by 37.84%. Furthermore, effect size analysis using Cohen's d reveals that PO significantly reduces latency, while IRO contributes most to precision enhancement. These findings underscore the potential of our framework to streamline EDA processes, enhance design accuracy, and shorten development timelines. Additionally, our algorithm has model-agnostic generalization, meaning it can improve parameter search performance across different LLMs.", 'abstract_zh': '从技术文档中提取参数对于确保电子设计的精确度和仿真可靠性至关重要。然而，当前方法难以处理高维度设计数据并满足实时处理的需求。在电子设计自动化（EDA）中，工程师常常需要手动搜索大量文档以检索用于构建PySpice模型的组件参数，这一过程既繁琐又耗时。为此，我们提出了一种创新框架，利用大型语言模型（LLMs）从数据表中自动提取参数并生成PySpice模型。该框架引入了三种基于Chain-of-Thought（CoT）的技术：（1）目标文档检索（TDR），实现快速识别相关技术部分；（2）迭代检索优化（IRO），通过迭代改进来细化参数搜索；（3）偏好优化（PO），动态优先考虑基于相关性的关键文档部分。实验结果显示，三种方法结合使用可将检索精度提高47.69%，减少处理延迟37.84%。此外，cohens d效应量分析表明，PO显著减少了延迟，而IRO对精度提升贡献最大。这些发现凸显了该框架在简化EDA流程、提升设计准确性和缩短开发时间方面的潜力。此外，我们的算法具有模型无关的一般化能力，意味着它可以在不同的LLMs上提高参数搜索性能。', 'title_zh': '使用大型语言模型进行文档中参数抽取的高级链式思考推理'}
{'arxiv_id': 'arXiv:2502.16534', 'title': 'Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs', 'authors': 'Jonathan Rystrøm, Hannah Rose Kirk, Scott Hale', 'link': 'https://arxiv.org/abs/2502.16534', 'abstract': "Large Language Models (LLMs) are becoming increasingly capable across global languages. However, the ability to communicate across languages does not necessarily translate to appropriate cultural representations. A key concern is US-centric bias, where LLMs reflect US rather than local cultural values. We propose a novel methodology that compares LLM-generated response distributions against population-level opinion data from the World Value Survey across four languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear mixed-effects regression framework, we compare two families of models: Google's Gemma models (2B--27B parameters) and successive iterations of OpenAI's turbo-series. Across the families of models, we find no consistent relationships between language capabilities and cultural alignment. While the Gemma models have a positive correlation between language capability and cultural alignment across languages, the OpenAI models do not. Importantly, we find that self-consistency is a stronger predictor of multicultural alignment than multilingual capabilities. Our results demonstrate that achieving meaningful cultural alignment requires dedicated effort beyond improving general language capabilities.", 'abstract_zh': '大型语言模型（LLMs）在多种全球语言中的能力越来越强。然而，跨语言交流能力并不一定转化为恰当的文化表现。一个关键问题是美国中心偏见，其中LLMs反映的是美国而非当地的文化价值观。我们提出了一种新的方法，将LLM生成的响应分布与世界价值调查的国家层面意见数据进行比较，覆盖四种语言（丹麦语、荷兰语、英语和葡萄牙语）。利用严格的线性混合效应回归框架，我们将Google的Gemma模型（2B-27B参数）和OpenAI的turbo系列的后续版本进行比较。在不同模型家族中，我们未发现语言能力与文化一致性之间的稳定关系。虽然Gemma模型在多种语言中表现出语言能力与文化一致性之间的正相关关系，但OpenAI模型则未表现出这种关系。重要的是，我们发现自我一致性比多语言能力更能预测跨文化一致性。我们的结果表明，实现有意义的文化一致性需要超出一般语言能力提升的专门努力。', 'title_zh': '多语言不代表多文化：评估LLM的多语言能力和文化契合度之间的差距'}
{'arxiv_id': 'arXiv:2502.16529', 'title': 'Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation', 'authors': 'Deokhyung Kang, Jeonghun Cho, Yejin Jeon, Sunbin Jang, Minsub Lee, Jawoon Cho, Gary Geunbae Lee', 'link': 'https://arxiv.org/abs/2502.16529', 'abstract': 'Visual programming languages (VPLs) allow users to create programs through graphical interfaces, which results in easier accessibility and their widespread usage in various domains. To further enhance this accessibility, recent research has focused on generating VPL code from user instructions using large language models (LLMs). Specifically, by employing prompting-based methods, these studies have shown promising results. Nevertheless, such approaches can be less effective for industrial VPLs such as Ladder Diagram (LD). LD is a pivotal language used in industrial automation processes and involves extensive domain-specific configurations, which are difficult to capture in a single prompt. In this work, we demonstrate that training-based methods outperform prompting-based methods for LD generation accuracy, even with smaller backbone models. Building on these findings, we propose a two-stage training strategy to further enhance VPL generation. First, we employ retrieval-augmented fine-tuning to leverage the repetitive use of subroutines commonly seen in industrial VPLs. Second, we apply direct preference optimization (DPO) to further guide the model toward accurate outputs, using systematically generated preference pairs through graph editing operations. Extensive experiments on real-world LD data demonstrate that our approach improves program-level accuracy by over 10% compared to supervised fine-tuning, which highlights its potential to advance industrial automation.', 'abstract_zh': '基于视觉编程语言的训练方法在梯形图生成中的效果研究', 'title_zh': '基于偏好优化的检索增强细调方法面向视觉程序生成'}
{'arxiv_id': 'arXiv:2502.16433', 'title': 'Sequence-level Large Language Model Training with Contrastive Preference Optimization', 'authors': 'Zhili Feng, Dhananjay Ram, Cole Hawkins, Aditya Rawal, Jinman Zhao, Sheng Zha', 'link': 'https://arxiv.org/abs/2502.16433', 'abstract': 'The next token prediction loss is the dominant self-supervised training objective for large language models and has achieved promising results in a variety of downstream tasks. However, upon closer investigation of this objective, we find that it lacks an understanding of sequence-level signals, leading to a mismatch between training and inference processes. To bridge this gap, we introduce a contrastive preference optimization (CPO) procedure that can inject sequence-level information into the language model at any training stage without expensive human labeled data. Our experiments show that the proposed objective surpasses the next token prediction in terms of win rate in the instruction-following and text generation tasks.', 'abstract_zh': '下一-token预测损失是大规模语言模型自监督训练的主要目标，并在各种下游任务中取得了令人瞩目的成果。然而，深入研究这一目标后，我们发现它缺乏对序列级信号的理解，导致训练和推理过程之间存在不匹配。为了弥合这一差距，我们提出了一种对比偏好优化（CPO）程序，可以在任何训练阶段向语言模型注入序列级信息，而无需昂贵的人工标注数据。我们的实验表明，在指令跟随和文本生成任务中，所提出的目标在胜利率上超过了下一-token预测。', 'title_zh': '基于对比偏好优化的序列级大规模语言模型训练'}
{'arxiv_id': 'arXiv:2502.16428', 'title': 'Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral, and ChatGPT', 'authors': 'Nidhal Jegham, Marwan Abdelatti, Abdeltawab Hendawi', 'link': 'https://arxiv.org/abs/2502.16428', 'abstract': 'Traditional evaluations of multimodal large language models (LLMs) have been limited by their focus on single-image reasoning, failing to assess crucial aspects like contextual understanding, reasoning stability, and uncertainty calibration. This study addresses these limitations by introducing a novel benchmark that integrates multi-image reasoning tasks with rejection-based evaluation and positional bias detection. To evaluate these dimensions, we further introduce entropy as a novel metric for quantifying reasoning consistency across reordered answer variants. We applied this benchmark to assess Grok 3, ChatGPT-4o, ChatGPT-o1, Gemini 2.0 Flash Experimental, DeepSeek Janus models, Qwen2.5-VL-72B-Instruct, QVQ-72B-Preview, and Pixtral 12B across eight visual reasoning tasks, including difference spotting and diagram interpretation. Our findings reveal ChatGPT-o1 leading in overall accuracy (82.5\\%) and rejection accuracy (70.0\\%), closely followed by Gemini 2.0 Flash Experimental (70.8\\%). QVQ-72B-Preview demonstrated superior rejection accuracy (85.5\\%). Notably, Pixtral 12B (51.7\\%) showed promise in specific domains, while Janus models exhibited challenges in bias and uncertainty calibration, reflected in low rejection accuracies and high entropy scores. High entropy scores in Janus models (Janus 7B: 0.8392, Janus 1B: 0.787) underscore their susceptibility to positional bias and unstable reasoning, contrasting with the low entropy and robust reasoning of ChatGPT models. The study further demonstrates that model size is not the sole determinant of performance, as evidenced by Grok 3 underperformance despite its substantial parameter count. By employing multi-image contexts, rejection mechanisms, and entropy-based consistency metrics, this benchmark sets a new standard for evaluating multimodal LLMs, enabling a more robust and reliable assessment of next-generation AI systems.', 'abstract_zh': '传统的多模态大型语言模型评估受限于其对单图像推理的聚焦，未能评估关键方面如上下文理解、推理稳定性和不确定性校准。本研究通过引入一种结合多图像推理任务、基于拒绝的评估以及位置偏差检测的新基准，来解决这些局限性。为进一步评估这些维度，我们引入了熵作为量化重新排序答案变体之间推理一致性的一种新指标。我们将此基准应用于评估Grok 3、ChatGPT-4o、ChatGPT-o1、Gemini 2.0 Flash Experimental、DeepSeek Janus模型、Qwen2.5-VL-72B-Instruct、QVQ-72B-Preview和Pixtral 12B在八项视觉推理任务中的表现，包括差异识别和图表解释。研究结果表明，ChatGPT-o1在总体准确率（82.5%）和拒绝准确率（70.0%）方面领先，其次为Gemini 2.0 Flash Experimental（70.8%）。QVQ-72B-Preview在拒绝准确率（85.5%）方面表现出色。值得注意的是，Pixtral 12B（51.7%）在特定领域显示出潜力，而Janus模型在偏差和不确定性校准方面面临挑战，这体现在较低的拒绝准确率和较高的熵得分。Janus模型高熵得分（Janus 7B：0.8392，Janus 1B：0.787）表明其对位置偏差和推理不稳定的脆弱性，这与ChatGPT模型的低熵和稳健推理形成对比。这项研究进一步表明，模型大小不是性能的唯一决定因素，即使参数数量庞大，Grok 3的表现也不理想。通过多图像推理背景、拒绝机制和基于熵的一致性度量，该基准为评估多模态大型语言模型设定了新的标准，能够提供更 robust 和可靠的下一代人工智能系统评估。', 'title_zh': 'Grok、Deepseek Janus、Gemini、Qwen、Mistral和ChatGPT的视觉推理评估'}
{'arxiv_id': 'arXiv:2502.16414', 'title': 'TabGen-ICL: Residual-Aware In-Context Example Selection for Tabular Data Generation', 'authors': 'Liancheng Fang, Aiwei Liu, Hengrui Zhang, Henry Peng Zou, Weizhi Zhang, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.16414', 'abstract': "Large Language models (LLMs) have achieved encouraging results in tabular data generation. However, existing approaches require fine-tuning, which is computationally expensive. This paper explores an alternative: prompting a fixed LLM with in-context examples. We observe that using randomly selected in-context examples hampers the LLM's performance, resulting in sub-optimal generation quality. To address this, we propose a novel in-context learning framework: TabGen-ICL, to enhance the in-context learning ability of LLMs for tabular data generation. TabGen-ICL operates iteratively, retrieving a subset of real samples that represent the residual between currently generated samples and true data distributions. This approach serves two purposes: locally, it provides more effective in-context learning examples for the LLM in each iteration; globally, it progressively narrows the gap between generated and real data. Extensive experiments on five real-world tabular datasets demonstrate that TabGen-ICL significantly outperforms the random selection strategy. Specifically, it reduces the error rate by a margin of $3.5\\%-42.2\\%$ on fidelity metrics. We demonstrate for the first time that prompting a fixed LLM can yield high-quality synthetic tabular data. The code is provided in the \\href{this https URL}{link}.", 'abstract_zh': '大规模语言模型（LLMs）在表格数据生成任务中取得了令人鼓舞的结果。然而，现有方法需要进行细调，这在计算上非常昂贵。本文探索了一种替代方案：通过上下文示例提示固定的大规模语言模型。我们观察到，使用随机选取的上下文示例会损害模型性能，导致生成质量不理想。为此，我们提出了一种新颖的上下文学习框架：TabGen-ICL，以增强LLMs在表格数据生成任务中的上下文学习能力。TabGen-ICL 迭代运行，检索当前生成样本与真实数据分布间差值的子集作为示例。该方法有两个目的：局部地，每次迭代为LLM提供更有效的上下文学习示例；全局地，逐步缩小生成数据与真实数据之间的差距。在五个真实世界的表格数据集上的广泛实验表明，TabGen-ICL 显著优于随机选择策略。具体而言，它在保真度指标上的错误率降低了3.5%-42.2%。我们首次证明，提示固定的大规模语言模型可以生成高质量的合成表格数据。代码可以在[this https URL](link)处获取。', 'title_zh': 'TabGen-ICL: 考虑残差的上下文例选择方法用于表格数据生成'}
{'arxiv_id': 'arXiv:2502.16399', 'title': 'Ensemble ToT of LLMs and Its Application to Automatic Grading System for Supporting Self-Learning', 'authors': 'Yuki Ito, Qiang Ma', 'link': 'https://arxiv.org/abs/2502.16399', 'abstract': "Providing students with detailed and timely grading feedback is essential for self-learning. While existing LLM-based grading systems are promising, most of them rely on one single model, which limits their performance. To address this, we propose Ensemble Tree-of-Thought (ToT), a framework that enhances LLM outputs by integrating multiple models. Using this framework, we develop a grading system. Ensemble ToT follows three steps: (1) analyzing LLM performance, (2) generating candidate answers, and (3) refining them into a final result. Based on this, our grading system first evaluates the grading tendencies of LLMs, then generates multiple results, and finally integrates them via a simulated debate. Experimental results demonstrate our approach's ability to provide accurate and explainable grading by effectively coordinating multiple LLMs.", 'abstract_zh': '提供详细及时的评分反馈是自主学习的关键。尽管现有的基于LLM的评分系统很有前景，但大多数系统依赖单一模型，这限制了它们的性能。为解决这一问题，我们提出了集合思维树（Ensemble Tree-of-Thought，Ensemble ToT）框架，该框架通过集成多个模型来增强LLM输出。基于此框架，我们开发了一个评分系统。Ensemble ToT 包括三个步骤：（1）分析LLM性能，（2）生成候选答案，（3）将它们精炼为最终结果。基于这些步骤，我们的评分系统首先评估LLMs的评分倾向，然后生成多个结果，最后通过模拟辩论将它们综合起来。实验结果表明，该方法能够通过有效协调多个LLMs提供准确可解释的评分。', 'title_zh': 'LLMensemble及其在支持自主学习的自动评分系统中的应用'}
{'arxiv_id': 'arXiv:2502.16395', 'title': 'An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science', 'authors': 'Qiuhai Zeng, Claire Jin, Xinyue Wang, Yuhan Zheng, Qunhua Li', 'link': 'https://arxiv.org/abs/2502.16395', 'abstract': "Large Language Models (LLMs) have demonstrated potential for data science tasks via code generation. However, the exploratory nature of data science, alongside the stochastic and opaque outputs of LLMs, raise concerns about their reliability. While prior work focuses on benchmarking LLM accuracy, reproducibility remains underexplored, despite being critical to establishing trust in LLM-driven analysis.\nWe propose a novel analyst-inspector framework to automatically evaluate and enforce the reproducibility of LLM-generated data science workflows - the first rigorous approach to the best of our knowledge. Defining reproducibility as the sufficiency and completeness of workflows for reproducing functionally equivalent code, this framework enforces computational reproducibility principles, ensuring transparent, well-documented LLM workflows while minimizing reliance on implicit model assumptions.\nUsing this framework, we systematically evaluate five state-of-the-art LLMs on 1,032 data analysis tasks across three diverse benchmark datasets. We also introduce two novel reproducibility-enhancing prompting strategies. Our results show that higher reproducibility strongly correlates with improved accuracy and reproducibility-enhancing prompts are effective, demonstrating structured prompting's potential to enhance automated data science workflows and enable transparent, robust AI-driven analysis. Our code is publicly available.", 'abstract_zh': '大型语言模型（LLMs）通过代码生成在数据科学任务中展示了潜力，然而数据科学的探索性质以及LLMs的随机性和不透明输出引起了对其可靠性的 concern。尽管先前的工作集中在评估LLM的准确性上，但再现性的重复探索仍然不足，这在建立对LLM驱动分析的信任方面至关重要。\n\n我们提出了一种新的分析师-检查员框架，自动评估和保证LLM生成的数据科学工作流程的再现性——据我们所知，这是首个严格的方法。我们将再现性定义为工作流程在功能上等效代码可再现性的充分性和完整性，该框架确保计算可再现性原则的遵守，使LLM工作流程透明且文档齐全，同时最小化对隐式模型假设的依赖。\n\n使用此框架，我们系统地评估了五种最先进的LLMs在三个不同基准数据集上的1,032个数据分析任务。我们还引入了两种新的增强再现性的提示策略。我们的结果显示，更高的再现性与改进的准确性高度相关，并且增强再现性的提示策略是有效的，这表明结构化提示有可能增强自动数据科学流程并促进透明的、稳健的AI驱动分析。我们的代码已公开。', 'title_zh': '数据科学中大规模语言模型可再现性评估的分析师-检查员框架'}
{'arxiv_id': 'arXiv:2502.16366', 'title': 'A generative approach to LLM harmfulness detection with special red flag tokens', 'authors': 'Sophie Xhonneux, David Dobre, Mehrnaz Mohfakhami, Leo Schwinn, Gauthier Gidel', 'link': 'https://arxiv.org/abs/2502.16366', 'abstract': "Most safety training methods for large language models (LLMs) based on fine-tuning rely on dramatically changing the output distribution of the model when faced with a harmful request, shifting it from an unsafe answer to a refusal to respond. These methods inherently compromise model capabilities and might make auto-regressive models vulnerable to attacks that make likely an initial token of affirmative response. To avoid that, we propose to expand the model's vocabulary with a special token we call red flag token (<rf>) and propose to fine-tune the model to generate this token at any time harmful content is generated or about to be generated. This novel safety training method effectively augments LLMs into generative classifiers of harmfulness at all times during the conversation. This method offers several advantages: it enables the model to explicitly learn the concept of harmfulness while marginally affecting the generated distribution, thus maintaining the model's utility. It also evaluates each generated answer rather than just the input prompt and provides a stronger defence against sampling-based attacks. In addition, it simplifies the evaluation of the model's robustness and reduces correlated failures when combined with a classifier. We further show an increased robustness to long contexts, and supervised fine-tuning attacks.", 'abstract_zh': '基于扩展词汇量的有害内容生成标记方法：一种新的大型语言模型安全训练方法', 'title_zh': '基于特殊红旗标记 token 的生成方法在大规模语言模型有害内容检测中的应用'}
{'arxiv_id': 'arXiv:2502.16343', 'title': 'Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading Agents', 'authors': 'David Byrd', 'link': 'https://arxiv.org/abs/2502.16343', 'abstract': 'Companies across all economic sectors continue to deploy large language models at a rapid pace. Reinforcement learning is experiencing a resurgence of interest due to its association with the fine-tuning of language models from human feedback. Tool-chain language models control task-specific agents; if the converse has not already appeared, it soon will. In this paper, we present what we believe is the first investigation of an intelligent trading agent based on continuous deep reinforcement learning that also controls a large language model with which it can post to a social media feed observed by other traders. We empirically investigate the performance and impact of such an agent in a simulated financial market, finding that it learns to optimize its total reward, and thereby augment its profit, by manipulating the sentiment of the posts it produces. The paper concludes with discussion, limitations, and suggestions for future work.', 'abstract_zh': '跨所有经济领域的公司正以快速的步伐部署大型语言模型。强化学习由于与从人类反馈中微调语言模型的关联而重新引起关注。工具链语言模型控制特定任务的代理；如果反向情况尚未出现，很快也将出现。本文提出了基于连续深度强化学习的智能交易代理的第一项研究，该代理还控制着一个可以发布到其他交易者可观察的社会媒体 feed 的大型语言模型。我们在模拟金融市场中实证研究了该代理的表现及其影响，发现它通过操控其发布的帖子的情感来学习优化其总奖励，从而增加其利润。论文以讨论、限制和对未来工作的建议作为结语。', 'title_zh': '探索LLM-enable智能交易代理的情感操纵'}
{'arxiv_id': 'arXiv:2502.16280', 'title': 'Human Preferences in Large Language Model Latent Space: A Technical Analysis on the Reliability of Synthetic Data in Voting Outcome Prediction', 'authors': 'Sarah Ball, Simeon Allmendinger, Frauke Kreuter, Niklas Kühl', 'link': 'https://arxiv.org/abs/2502.16280', 'abstract': 'Generative AI (GenAI) is increasingly used in survey contexts to simulate human preferences. While many research endeavors evaluate the quality of synthetic GenAI data by comparing model-generated responses to gold-standard survey results, fundamental questions about the validity and reliability of using LLMs as substitutes for human respondents remain. Our study provides a technical analysis of how demographic attributes and prompt variations influence latent opinion mappings in large language models (LLMs) and evaluates their suitability for survey-based predictions. Using 14 different models, we find that LLM-generated data fails to replicate the variance observed in real-world human responses, particularly across demographic subgroups. In the political space, persona-to-party mappings exhibit limited differentiation, resulting in synthetic data that lacks the nuanced distribution of opinions found in survey data. Moreover, we show that prompt sensitivity can significantly alter outputs for some models, further undermining the stability and predictiveness of LLM-based simulations. As a key contribution, we adapt a probe-based methodology that reveals how LLMs encode political affiliations in their latent space, exposing the systematic distortions introduced by these models. Our findings highlight critical limitations in AI-generated survey data, urging caution in its use for public opinion research, social science experimentation, and computational behavioral modeling.', 'abstract_zh': '生成式AI（GenAI）在调查情境中日益用于模拟人类偏好。虽然许多研究致力于通过比较模型生成的响应与黄金标准调查结果来评估合成GenAI数据的质量，但使用大型语言模型（LLMs）作为人类受访者的替代品的有效性和可靠性仍存在根本性问题。我们的研究提供了关于人口统计属性和提示变化如何影响LLMs中潜在意见映射的技术分析，并评估了它们在基于调查的预测中的适用性。使用14个不同的模型，我们发现LLM生成的数据未能复制真实世界人类响应中的变异，特别是在不同人口统计子群体之间。在政治领域，个性与政党的映射表现出有限的差异性，导致合成数据缺乏调查数据中观察到的意见细微分布。此外，我们表明，提示敏感性可以显著改变某些模型的输出，进一步削弱基于LLM的模拟的稳定性和预测性。作为一项关键贡献，我们采用了一种基于探针的方法，揭示了LLMs如何在其隐空间中编码政治倾向，并暴露了这些模型引入的系统性失真。我们的发现揭示了AI生成的调查数据的关键限制，提醒人们在公共意见研究、社会科学研究和计算行为建模中谨慎使用这些数据。', 'title_zh': '大型语言模型潜在空间中的人类偏好：合成数据在投票结果预测中的可靠性技术分析'}
{'arxiv_id': 'arXiv:2502.16279', 'title': 'Beyond Trusting Trust: Multi-Model Validation for Robust Code Generation', 'authors': 'Bradley McDanel', 'link': 'https://arxiv.org/abs/2502.16279', 'abstract': 'This paper explores the parallels between Thompson\'s "Reflections on Trusting Trust" and modern challenges in LLM-based code generation. We examine how Thompson\'s insights about compiler backdoors take on new relevance in the era of large language models, where the mechanisms for potential exploitation are even more opaque and difficult to analyze. Building on this analogy, we discuss how the statistical nature of LLMs creates novel security challenges in code generation pipelines. As a potential direction forward, we propose an ensemble-based validation approach that leverages multiple independent models to detect anomalous code patterns through cross-model consensus. This perspective piece aims to spark discussion about trust and validation in AI-assisted software development.', 'abstract_zh': '本论文探讨了Thompson的“信任信任的反思”与基于大语言模型的代码生成现代挑战之间的相似性。我们研究了Thompson关于编译器后门的洞察在大语言模型时代的新相关性，那里潜在利用的机制更加不透明和难以分析。基于这一类比，我们讨论了大语言模型的统计性质给代码生成流水线带来了新的安全挑战。作为前进的方向，我们提出了一种基于集成的验证方法，利用多个独立模型通过跨模型共识检测异常代码模式。本文旨在引发关于AI辅助软件开发中的信任和验证的讨论。', 'title_zh': '超越信任信任：多模型验证以实现稳健的代码生成'}
{'arxiv_id': 'arXiv:2502.16274', 'title': 'Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation', 'authors': 'Kartik Gupta', 'link': 'https://arxiv.org/abs/2502.16274', 'abstract': "The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and engaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated dataset of movie conversations. Due to the limitations in GPU computing and VRAM, the training process began with the 0.5B model progressively scaling up to the 1.5B and 3B versions as efficiency improvements were implemented. The Qwen 2.5 series, developed by Alibaba Group, stands at the forefront of small open-source pre-trained models, particularly excelling in creative tasks compared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results demonstrate the ability of small models to produce high-quality, realistic dialogue, offering a promising approach for real-time, context-sensitive conversation generation.", 'abstract_zh': 'Qwen 2.5 3B基模型根据Cornell Movie-Dialog Corpus数据集进行了微调，以生成富有情境感且引人入胜的电影对话。由于GPU计算能力和VRAM的限制，训练过程从0.5B模型逐步扩展到1.5B和3B版本，同时实施了效率改进措施。阿里巴巴集团开发的Qwen 2.5系列在小型开源预训练模型中处于领先地位，特别在创造性任务方面优于Meta的Llama 3.2和Google的Gemma等替代模型。结果表明，小型模型能够生成高质量、逼真的对话，提供了实时、情境感知对话生成的有前途的方法。', 'title_zh': 'Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation'}
{'arxiv_id': 'arXiv:2502.16198', 'title': 'An Autonomous Network Orchestration Framework Integrating Large Language Models with Continual Reinforcement Learning', 'authors': 'Masoud Shokrnezhad, Tarik Taleb', 'link': 'https://arxiv.org/abs/2502.16198', 'abstract': '6G networks aim to achieve global coverage, massive connectivity, and ultra-stringent requirements. Space-Air-Ground Integrated Networks (SAGINs) and Semantic Communication (SemCom) are essential for realizing these goals, yet they introduce considerable complexity in resource orchestration. Drawing inspiration from research in robotics, a viable solution to manage this complexity is the application of Large Language Models (LLMs). Although the use of LLMs in network orchestration has recently gained attention, existing solutions have not sufficiently addressed LLM hallucinations or their adaptation to network dynamics. To address this gap, this paper proposes a framework called Autonomous Reinforcement Coordination (ARC) for a SemCom-enabled SAGIN. This framework employs an LLM-based Retrieval-Augmented Generator (RAG) monitors services, users, and resources and processes the collected data, while a Hierarchical Action Planner (HAP) orchestrates resources. ARC decomposes orchestration into two tiers, utilizing LLMs for high-level planning and Reinforcement Learning (RL) agents for low-level decision-making, in alignment with the Mixture of Experts (MoE) concept. The LLMs utilize Chain-of-Thought (CoT) reasoning for few-shot learning, empowered by contrastive learning, while the RL agents employ replay buffer management for continual learning, thereby achieving efficiency, accuracy, and adaptability. Simulations are provided to demonstrate the effectiveness of ARC, along with a comprehensive discussion on potential future research directions to enhance and upgrade ARC.', 'abstract_zh': '6G网络旨在实现全球覆盖、海量连接和极致严格的性能要求。空间-空中-地面综合网络（SAGINs）和语义通信（SemCom）是实现这些目标的关键，但也引入了资源 orchestrization 的巨大复杂性。借鉴机器人学研究中的进展，通过应用大规模语言模型（LLMs）可以有效管理这一复杂性。尽管在网络orchorchization 中使用LLMs 逐渐受到关注，但现有解决方案尚未充分解决LLM 的幻觉问题或对网络动态的适应性问题。为了填补这一空白，本文提出了一种名为自主强化协调（ARC）的框架，用于实现SemCom 的SAGINs。该框架采用基于LLM 的检索增强生成器（RAG）监控服务、用户和资源，并处理收集到的数据，而层次化行动规划器（HAP）则协调资源。ARC 将orchestration 分解为两级，利用LLM 进行高层次规划，并使用强化学习（RL）代理进行低层次决策，符合专家混合（MoE）的概念。LLMs 利用因果推理进行少样本学习，而RL代理则通过回放缓冲区管理实现持续学习，从而实现效率、准确性和适应性。仿真实验展示了ARC 的有效性，并对增强和升级ARC 的未来研究方向进行了全面讨论。', 'title_zh': '一种结合大型语言模型与持续强化学习的自主网络编排框架'}
{'arxiv_id': 'arXiv:2502.16175', 'title': 'Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens', 'authors': 'Ziwei Shan, Yaoyu He, Chengfeng Zhao, Jiashen Du, Jingyan Zhang, Qixuan Zhang, Jingyi Yu, Lan Xu', 'link': 'https://arxiv.org/abs/2502.16175', 'abstract': 'Human bodily movements convey critical insights into action intentions and cognitive processes, yet existing multimodal systems primarily focused on understanding human motion via language, vision, and audio, which struggle to capture the dynamic forces and torques inherent in 3D motion. Inertial measurement units (IMUs) present a promising alternative, offering lightweight, wearable, and privacy-conscious motion sensing. However, processing of streaming IMU data faces challenges such as wireless transmission instability, sensor noise, and drift, limiting their utility for long-term real-time motion capture (MoCap), and more importantly, online motion analysis. To address these challenges, we introduce Mojito, an intelligent motion agent that integrates inertial sensing with large language models (LLMs) for interactive motion capture and behavioral analysis.', 'abstract_zh': '人体运动传达了重要的行动意图和认知过程的见解，现有的多模态系统主要侧重于通过语言、视觉和音频理解人类运动，但难以捕捉三维运动中固有的动态力和扭矩。惯性测量单元（IMUs）提供了一种有前景的替代方案，可以实现轻量级、可穿戴且注重隐私的运动传感。然而，处理流式IMU数据面临无线传输不稳定、传感器噪声和漂移等问题，限制了其在长期实时运动捕捉中的应用，并且更重要的是在线运动分析。为解决这些问题，我们引入了Mojito，一种结合惯性感知和大型语言模型（LLMs）的智能运动代理，用于交互式运动捕捉和行为分析。', 'title_zh': '莫吉托：带有抖动减少惯性令牌的LLM辅助运动指导器'}
{'arxiv_id': 'arXiv:2502.16174', 'title': 'Maybe I Should Not Answer That, but... Do LLMs Understand The Safety of Their Inputs?', 'authors': 'Maciej Chrabąszcz, Filip Szatkowski, Bartosz Wójcik, Jan Dubiński, Tomasz Trzciński', 'link': 'https://arxiv.org/abs/2502.16174', 'abstract': 'Ensuring the safety of the Large Language Model (LLM) is critical, but currently used methods in most cases sacrifice the model performance to obtain increased safety or perform poorly on data outside of their adaptation distribution. We investigate existing methods for such generalization and find them insufficient. Surprisingly, while even plain LLMs recognize unsafe prompts, they may still generate unsafe responses. To avoid performance degradation and preserve safe performance, we advocate for a two-step framework, where we first identify unsafe prompts via a lightweight classifier, and apply a "safe" model only to such prompts. In particular, we explore the design of the safety detector in more detail, investigating the use of different classifier architectures and prompting techniques. Interestingly, we find that the final hidden state for the last token is enough to provide robust performance, minimizing false positives on benign data while performing well on malicious prompt detection. Additionally, we show that classifiers trained on the representations from different model layers perform comparably on the latest model layers, indicating that safety representation is present in the LLMs\' hidden states at most model stages. Our work is a step towards efficient, representation-based safety mechanisms for LLMs.', 'abstract_zh': '确保大型语言模型（LLM）的安全性至关重要，但目前大多数方法要么牺牲模型性能以获得更高的安全性，要么在超出其适应分布的数据上表现不佳。我们调查了现有的泛化方法，并发现它们不足之处。令人惊讶的是，即使简单的LLM也能识别出不安全的提示，但仍可能生成不安全的响应。为了避免性能下降并保持安全性能，我们建议采用两步框架，首先通过一个轻量级分类器识别不安全的提示，然后仅对这些提示应用“安全”模型。特别是，我们更详细地探讨了安全检测器的设计，研究了不同分类器架构和提示技术的使用。有趣的是，我们发现最终隐藏状态足以提供稳健的性能，在良性数据上最小化假阳性，同时在恶意提示检测方面表现良好。此外，我们展示了在不同模型层上训练的分类器在最新模型层上表现出相似的性能，表明安全性表示在LLM的隐藏状态中可以在大多数模型阶段出现。我们的工作是朝着为LLM设计高效、基于表示的安全机制迈出的一步。', 'title_zh': '也许我本不应该回答这个问题，但……大语言模型是否理解其输入的安全性？'}
{'arxiv_id': 'arXiv:2502.16171', 'title': 'EPERM: An Evidence Path Enhanced Reasoning Model for Knowledge Graph Question and Answering', 'authors': 'Xiao Long, Liansheng Zhuang, Aodi Li, Minghong Yao, Shafei Wang', 'link': 'https://arxiv.org/abs/2502.16171', 'abstract': 'Due to the remarkable reasoning ability, Large language models (LLMs) have demonstrated impressive performance in knowledge graph question answering (KGQA) tasks, which find answers to natural language questions over knowledge graphs (KGs). To alleviate the hallucinations and lack of knowledge issues of LLMs, existing methods often retrieve the question-related information from KGs to enrich the input context. However, most methods focus on retrieving the relevant information while ignoring the importance of different types of knowledge in reasoning, which degrades their performance. To this end, this paper reformulates the KGQA problem as a graphical model and proposes a three-stage framework named the Evidence Path Enhanced Reasoning Model (EPERM) for KGQA. In the first stage, EPERM uses the fine-tuned LLM to retrieve a subgraph related to the question from the original knowledge graph. In the second stage, EPERM filters out the evidence paths that faithfully support the reasoning of the questions, and score their importance in reasoning. Finally, EPERM uses the weighted evidence paths to reason the final answer. Since considering the importance of different structural information in KGs for reasoning, EPERM can improve the reasoning ability of LLMs in KGQA tasks. Extensive experiments on benchmark datasets demonstrate that EPERM achieves superior performances in KGQA tasks.', 'abstract_zh': '大型语言模型在知识图谱问答任务中的证据路径增强推理模型', 'title_zh': 'EPERM：一种基于证据路径的推理模型用于知识图谱问答'}
{'arxiv_id': 'arXiv:2502.16167', 'title': 'PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models', 'authors': 'Xinwei Liu, Xiaojun Jia, Yuan Xun, Hua Zhang, Xiaochun Cao', 'link': 'https://arxiv.org/abs/2502.16167', 'abstract': "Diffusion models (DMs) have revolutionized data generation, particularly in text-to-image (T2I) synthesis. However, the widespread use of personalized generative models raises significant concerns regarding privacy violations and copyright infringement. To address these issues, researchers have proposed adversarial perturbation-based protection techniques. However, these methods have notable limitations, including insufficient robustness against data transformations and the inability to fully eliminate identifiable features of protected objects in the generated output. In this paper, we introduce PersGuard, a novel backdoor-based approach that prevents malicious personalization of specific images. Unlike traditional adversarial perturbation methods, PersGuard implant backdoor triggers into pre-trained T2I models, preventing the generation of customized outputs for designated protected images while allowing normal personalization for unprotected ones. Unfortunately, existing backdoor methods for T2I diffusion models fail to be applied to personalization scenarios due to the different backdoor objectives and the potential backdoor elimination during downstream fine-tuning processes. To address these, we propose three novel backdoor objectives specifically designed for personalization scenarios, coupled with backdoor retention loss engineered to resist downstream fine-tuning. These components are integrated into a unified optimization framework. Extensive experimental evaluations demonstrate PersGuard's effectiveness in preserving data privacy, even under challenging conditions including gray-box settings, multi-object protection, and facial identity scenarios. Our method significantly outperforms existing techniques, offering a more robust solution for privacy and copyright protection.", 'abstract_zh': '基于后门的PrivacyGuard：一种针对特定图像的恶意个性化防护方法', 'title_zh': 'PersGuard：通过预训练文本到图像扩散模型后门攻击防止恶意个性化'}
{'arxiv_id': 'arXiv:2502.16137', 'title': 'Chain-of-Description: What I can understand, I can put into words', 'authors': 'Jiaxin Guo, Daimeng Wei, Zongyao Li, Hengchao Shang, Yuanchang Luo, Hao Yang', 'link': 'https://arxiv.org/abs/2502.16137', 'abstract': 'In this paper, we propose a novel strategy defined as Chain-of-Description (CoD) Prompting, tailored for Multi-Modal Large Language Models. This approach involves having the model first provide a detailed description of the multi-modal input before generating an answer to the question. When applied to models such as Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL, CoD Prompting significantly enhances performance compared to standard prompting methods. This is demonstrated by nearly a 4\\% improvement in the speech category of the audio benchmark AIR-Bench-Chat and a 5.3\\% improvement in the hard-level portion of the vision benchmark MMMU\\_Pro. Our ablation study further validates the effectiveness of CoD Prompting.', 'abstract_zh': '在这种链式描述提示策略中，我们提出了一种针对多模态大规模语言模型的新型策略，该方法要求模型首先对多模态输入进行详细的描述，然后再生成问题的答案。将该策略应用于Qwen2-Audio、Qwen2-VL和Qwen2.5-VL等模型时，与标准提示方法相比，显著提升了性能。这在音频基准AIR-Bench-Chat的声音类别中近4%的改进以及视觉基准MMMU_Pro的困难级别部分中5.3%的改进中得到了体现。我们的消融研究进一步验证了链式描述提示的有效性。', 'title_zh': '描述链：我能理解的，我能用语言表达。'}
{'arxiv_id': 'arXiv:2502.16097', 'title': 'LitLinker: Supporting the Ideation of Interdisciplinary Contexts with Large Language Models for Teaching Literature in Elementary Schools', 'authors': 'Haoxiang Fan, Changshuang Zhou, Hao Yu, Xueyang Wu, Jiangyu Gu, Zhenhui Peng', 'link': 'https://arxiv.org/abs/2502.16097', 'abstract': "Teaching literature under interdisciplinary contexts (e.g., science, art) that connect reading materials has become popular in elementary schools. However, constructing such contexts is challenging as it requires teachers to explore substantial amounts of interdisciplinary content and link it to the reading materials. In this paper, we develop LitLinker via an iterative design process involving 13 teachers to facilitate the ideation of interdisciplinary contexts for teaching literature. Powered by a large language model (LLM), LitLinker can recommend interdisciplinary topics and contextualize them with the literary elements (e.g., paragraphs, viewpoints) in the reading materials. A within-subjects study (N=16) shows that compared to an LLM chatbot, LitLinker can improve the integration depth of different subjects and reduce workload in this ideation task. Expert interviews (N=9) also demonstrate LitLinker's usefulness for supporting the ideation of interdisciplinary contexts for teaching literature. We conclude with concerns and design considerations for supporting interdisciplinary teaching with LLMs.", 'abstract_zh': '在跨学科背景下（如科学、艺术）教授文学作品已成为小学教育中的流行趋势。然而，构建这样的背景具有挑战性，因为这要求教师探索大量的跨学科内容，并将其与阅读材料关联起来。本文通过13位教师参与的迭代设计过程开发了LitLinker，以促进文学教学中跨学科背景的想法生成。借助大型语言模型（LLM），LitLinker可以推荐跨学科主题，并用阅读材料中的文学元素（如段落、观点）对其进行情境化。一项被试内研究（N=16）表明，与语言模型聊天机器人相比，LitLinker可以提高不同学科内容的整合深度并减少此想法生成任务的工作量。专家访谈（N=9）也证明了LitLinker在支持文学教学中的跨学科背景想法生成方面的实用性。最后，我们对使用LLM支持跨学科教学的关注点和设计考虑进行了总结。', 'title_zh': 'LitLinker：借助大型语言模型支持小学文学教学中的跨学科构思'}
{'arxiv_id': 'arXiv:2502.16090', 'title': 'Echo: A Large Language Model with Temporal Episodic Memory', 'authors': 'WenTao Liu, Ruohua Zhang, Aimin Zhou, Feng Gao, JiaLi Liu', 'link': 'https://arxiv.org/abs/2502.16090', 'abstract': "Research on large language models (LLMs) has shown remarkable performance in domains such as mathematics, programming, and literary creation. However, most studies have focused on semantic memory-based question answering, neglecting LLMs' potential to handle episodic memory (EM)-related queries. This oversight has led to suboptimal performance in applications requiring EM, including emotional companionship, personal AI assistants, and AI teachers. To address this gap, we introduce Echo, a LLM enhanced with temporal episodic memory. We propose a Multi-Agent Data Generation Framework that guides the model in generating multi-turn, complex scenario episodic memory dialogue data (EM-Train). Temporal information is innovatively incorporated into the LLM training process, and Echo is trained using the EM-Train. Furthermore, We develop an EM-Test benchmark specifically designed to evaluate LLMs' episodic memory capabilities. The EM-Test assesses performance across various time spans and difficulty levels, providing a comprehensive evaluation of multi-turn episodic memory dialogues. Our experiments demonstrate that Echo significantly outperforms state-of-the-art LLMs on EM-Test. Additionally, a qualitative analysis reveals Echo's potential to exhibit human-like episodic memory capabilities. We will open-source all datasets, code, and model weights.", 'abstract_zh': '大型语言模型的研究已经在数学、编程和文学创作等领域展现了卓越性能。然而，大多数研究集中于基于语义记忆的问题回答，忽视了大型语言模型处理情节记忆相关查询的潜力。这种忽视导致了在需要情节记忆的应用中，如情感陪伴、个人AI助手和AI教师等方面表现不佳。为了解决这一问题，我们引入了Echo，一种增强时间情节记忆的大型语言模型。我们提出了一种多智能体数据生成框架，指导模型生成多轮次、复杂情景的情节记忆对话数据（EM-Train）。时间信息被创新地融入大型语言模型的训练过程，Echo使用EM-Train进行训练。此外，我们开发了专门设计用于评估大型语言模型情节记忆能力的EM-Test基准。EM-Test评估了不同时间跨度和难度级别的表现，提供了对多轮次情节记忆对话的全面评估。我们的实验表明，Echo在EM-Test上的表现显著优于当前最佳的大型语言模型。此外，定性分析揭示了Echo具备类似人类的情节记忆能力的潜力。我们将开源所有数据集、代码和模型权重。', 'title_zh': '回声：具有时间情景记忆的大型语言模型'}
{'arxiv_id': 'arXiv:2502.15990', 'title': 'Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search', 'authors': 'Jayant Sachdev, Sean D Rosario, Abhijeet Phatak, He Wen, Swati Kirti, Chittaranjan Tripathy', 'link': 'https://arxiv.org/abs/2502.15990', 'abstract': "Accurate query-product relevance labeling is indispensable to generate ground truth dataset for search ranking in e-commerce. Traditional approaches for annotating query-product pairs rely on human-based labeling services, which is expensive, time-consuming and prone to errors. In this work, we explore the application of Large Language Models (LLMs) to automate query-product relevance labeling for large-scale e-commerce search. We use several publicly available and proprietary LLMs for this task, and conducted experiments on two open-source datasets and an in-house e-commerce search dataset. Using prompt engineering techniques such as Chain-of-Thought (CoT) prompting, In-context Learning (ICL), and Retrieval Augmented Generation (RAG) with Maximum Marginal Relevance (MMR), we show that LLM's performance has the potential to approach human-level accuracy on this task in a fraction of the time and cost required by human-labelers, thereby suggesting that our approach is more efficient than the conventional methods. We have generated query-product relevance labels using LLMs at scale, and are using them for evaluating improvements to our search algorithms. Our work demonstrates the potential of LLMs to improve query-product relevance thus enhancing e-commerce search user experience. More importantly, this scalable alternative to human-annotation has significant implications for information retrieval domains including search and recommendation systems, where relevance scoring is crucial for optimizing the ranking of products and content to improve customer engagement and other conversion metrics.", 'abstract_zh': '大规模电商平台搜索中的查询-产品相关性自动标注：大规模应用大型语言模型的成本效益解决方案', 'title_zh': '使用大型语言模型自动化查询-产品相关性标注以优化电子商务搜索'}
{'arxiv_id': 'arXiv:2502.15975', 'title': 'Sparsity May Be All You Need: Sparse Random Parameter Adaptation', 'authors': 'Jesus Rios, Pierre Dognin, Ronny Luss, Karthikeyan N. Ramamurthy', 'link': 'https://arxiv.org/abs/2502.15975', 'abstract': 'Full fine-tuning of large language models for alignment and task adaptation has become prohibitively expensive as models have grown in size. Parameter-Efficient Fine-Tuning (PEFT) methods aim at significantly reducing the computational and memory resources needed for fine-tuning these models by only training on a small number of parameters instead of all model parameters. Currently, the most popular PEFT method is the Low-Rank Adaptation (LoRA), which freezes the parameters of the model to be fine-tuned and introduces a small set of trainable parameters in the form of low-rank matrices. We propose simply reducing the number of trainable parameters by randomly selecting a small proportion of the model parameters to train on. In this paper, we compare the efficiency and performance of our proposed approach with PEFT methods, including LoRA, as well as full parameter fine-tuning.', 'abstract_zh': '大型语言模型的全面微调因模型规模扩大而变得成本高昂，参数高效微调（PEFT）方法旨在通过仅训练少量参数而非全部参数来显著减少微调所需的时间和内存资源。目前，最受欢迎的PEFT方法是低秩适应（LoRA），该方法冻结待微调模型的参数，并引入一组以低秩矩阵形式表示的可训练参数。我们提议通过随机选择少量模型参数进行训练来减少可训练参数的数量。在本文中，我们将我们提出的方法与PEFT方法，包括LoRA，以及全面参数微调的效率和性能进行比较。', 'title_zh': '稀疏或许足矣：稀疏随机参数适应'}
{'arxiv_id': 'arXiv:2502.15969', 'title': 'Forgotten Polygons: Multimodal Large Language Models are Shape-Blind', 'authors': 'William Rudman, Michal Golovanesky, Amir Bar, Vedant Palit, Yann LeCun, Carsten Eickhoff, Ritambhara Singh', 'link': 'https://arxiv.org/abs/2502.15969', 'abstract': "Despite strong performance on vision-language tasks, Multimodal Large Language Models (MLLMs) struggle with mathematical problem-solving, with both open-source and state-of-the-art models falling short of human performance on visual-math benchmarks. To systematically examine visual-mathematical reasoning in MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test multi-step reasoning, and (3) explore a potential solution to improve visual reasoning capabilities. Our findings reveal fundamental shortcomings in shape recognition, with top models achieving under 50% accuracy in identifying regular polygons. We analyze these failures through the lens of dual-process theory and show that MLLMs rely on System 1 (intuitive, memorized associations) rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count the sides of both familiar and novel shapes, suggesting they have neither learned the concept of sides nor effectively process visual inputs. Finally, we propose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances multi-step mathematical reasoning by explicitly referencing visual annotations in diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting task from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs remains an open problem, and visually-guided prompting is essential for successfully engaging visual reasoning. Code available at: this https URL.", 'abstract_zh': '尽管在视觉语言任务上表现出色，多模态大型语言模型在数学问题解决方面仍存在问题，开源和最先进的模型在视觉数学基准测试中的表现均低于人类水平。为了系统地探讨多模态大型语言模型的视觉数学推理能力，我们（1）评估其对几何基本概念的理解，（2）测试多步推理，（3）探索一种潜在解决方案以提高视觉推理能力。我们的研究发现表明，在形状识别方面存在根本性的不足，顶级模型在识别正多边形时的准确性不到50%。我们通过二过程理论的视角分析这些失败，表明多模态大型语言模型依赖于直觉和记忆化的关联（System 1），而非有意识的推理（System 2）。因此，多模态大型语言模型无法准确计数熟悉和新颖形状的边，表明它们既没有学会边的概念，也无法有效处理视觉输入。最后，我们提出了视觉提示链式思考（Visually Cued Chain-of-Thought, VC-CoT）的提示方法，通过明确引用图表中的视觉注释来增强多步数学推理能力，在提升GPT-4o在计数不规则多边形边的数量任务上的准确性方面从7%提升到93%。我们的研究结果表明，多模态大型语言模型中的有意识推理仍是一个待解决的问题，而视觉引导的提示对于成功运用视觉推理至关重要。代码请访问：this https URL。', 'title_zh': '遗忘的多边形：多模态大型语言模型是形状盲的'}
{'arxiv_id': 'arXiv:2502.15964', 'title': 'Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models', 'authors': 'Avanika Narayan, Dan Biderman, Sabri Eyuboglu, Avner May, Scott Linderman, James Zou, Christopher Re', 'link': 'https://arxiv.org/abs/2502.15964', 'abstract': "We investigate an emerging setup in which a small, on-device language model (LM) with access to local data communicates with a frontier, cloud-hosted LM to solve real-world tasks involving financial, medical, and scientific reasoning over long documents. Can a local-remote collaboration reduce cloud inference costs while preserving quality? First, we consider a naive collaboration protocol where the local and remote models simply chat back and forth. Because only the local model reads the full context, this protocol achieves a 30.4x reduction in remote costs, but recovers only 87% of the performance of the frontier model. We identify two key limitations of this protocol: the local model struggles to (1) follow the remote model's multi-step instructions and (2) reason over long contexts. Motivated by these observations, we study an extension of this protocol, coined MinionS, in which the remote model decomposes the task into easier subtasks over shorter chunks of the document, that are executed locally in parallel. MinionS reduces costs by 5.7x on average while recovering 97.9% of the performance of the remote model alone. Our analysis reveals several key design choices that influence the trade-off between cost and performance in local-remote systems.", 'abstract_zh': '一种本地-远程协作的新兴设置：小规模设备端语言模型与云端前沿语言模型协同解决涉及金融、医疗和科学推理的长文档实际任务，能否在保持质量的同时降低云推理成本？', 'title_zh': 'Minions: 在设备端和云端语言模型之间经济高效的协作'}
{'arxiv_id': 'arXiv:2502.15957', 'title': 'R$^3$Mem: Bridging Memory Retention and Retrieval via Reversible Compression', 'authors': 'Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, Bang Liu', 'link': 'https://arxiv.org/abs/2502.15957', 'abstract': "Memory plays a key role in enhancing LLMs' performance when deployed to real-world applications. Existing solutions face trade-offs: explicit memory designs based on external storage require complex management and incur storage overhead, while implicit memory designs that store information via parameters struggle with reliable retrieval. In this paper, we propose R$^3$Mem, a memory network that optimizes both information Retention and Retrieval through Reversible context compression. Specifically, R$^3$Mem employs virtual memory tokens to compress and encode infinitely long histories, further enhanced by a hierarchical compression strategy that refines information from document- to entity-level for improved assimilation across granularities. For retrieval, R$^3$Mem employs a reversible architecture, reconstructing raw data by invoking the model backward with compressed information. Implemented via parameter-efficient fine-tuning, it can integrate seamlessly with any Transformer-based model. Experiments demonstrate that our memory design achieves state-of-the-art performance in long-context language modeling and retrieval-augmented generation tasks. It also significantly outperforms conventional memory modules in long-horizon interaction tasks like conversational agents, showcasing its potential for next-generation retrieval systems.", 'abstract_zh': 'R$^3$Mem：通过可逆上下文压缩优化信息保留与检索的记忆网络', 'title_zh': 'R$^3$Mem: 通过可逆压缩连接记忆保留与检索'}
{'arxiv_id': 'arXiv:2502.15954', 'title': 'MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models for Biomedical In-Context Learning', 'authors': 'Zaifu Zhan, Jun Wang, Shuang Zhou, Jiawen Deng, Rui Zhang', 'link': 'https://arxiv.org/abs/2502.15954', 'abstract': "Objective: To optimize in-context learning in biomedical natural language processing by improving example selection. Methods: We introduce a novel multi-mode retrieval-augmented generation (MMRAG) framework, which integrates four retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2) Top Mode, retrieving the most relevant examples based on similarity; (3) Diversity Mode, ensuring variation in selected examples; and (4) Class Mode, selecting category-representative examples. This study evaluates MMRAG on three core biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction (RE), and Text Classification (TC). The datasets used include BC2GM for gene and protein mention recognition (NER), DDI for drug-drug interaction extraction (RE), GIT for general biomedical information extraction (RE), and HealthAdvice for health-related text classification (TC). The framework is tested with two large language models (Llama2-7B, Llama3-8B) and three retrievers (Contriever, MedCPT, BGE-Large) to assess performance across different retrieval strategies. Results: The results from the Random mode indicate that providing more examples in the prompt improves the model's generation performance. Meanwhile, Top mode and Diversity mode significantly outperform Random mode on the RE (DDI) task, achieving an F1 score of 0.9669, a 26.4% improvement. Among the three retrievers tested, Contriever outperformed the other two in a greater number of experiments. Additionally, Llama 2 and Llama 3 demonstrated varying capabilities across different tasks, with Llama 3 showing a clear advantage in handling NER tasks. Conclusion: MMRAG effectively enhances biomedical in-context learning by refining example selection, mitigating data scarcity issues, and demonstrating superior adaptability for NLP-driven healthcare applications.", 'abstract_zh': '目标：通过改善例证选择以优化生物医学自然语言处理中的上下文学习。方法：我们提出了一种新颖的多模式检索增强生成（MMRAG）框架，该框架整合了四种检索策略：（1）随机模式，任意选择例证；（2）顶级模式，基于相似性检索最相关的例证；（3）多样性模式，确保所选例证的多样性；（4）类别模式，选择类别代表性例证。本研究在三项核心生物医学自然语言处理任务上评估了MMRAG：命名实体识别（NER）、关系提取（RE）和文本分类（TC）。所使用的数据集包括BC2GM（基因和蛋白质提及识别（NER））、DDI（药物-药物交互提取（RE））、GIT（通用生物医学信息提取（RE））和HealthAdvice（健康相关文本分类（TC））。框架使用了两个大型语言模型（Llama2-7B、Llama3-8B）和三个检索器（Contriever、MedCPT、BGE-Large），以评估不同检索策略下的性能。结果：随机模式的结果表明，在提示中提供更多例证可以提高模型的生成性能。同时，顶级模式和多样性模式在RE（DDI）任务上显著优于随机模式，F1分数达到0.9669，提升了26.4%。在三个测试的检索器中，Contriever在更多实验中表现出更好的性能。此外，Llama 2和Llama 3在不同任务中显示出不同的能力，Llama 3在处理NER任务方面具有明显优势。结论：MMRAG通过优化例证选择有效提升了生物医学上下文学习，缓解了数据稀缺问题，并展示了更好的适应性，适用于NLP驱动的医疗保健应用。', 'title_zh': 'MMRAG：多模式检索增强生成在生物医学情境学习中的应用'}
{'arxiv_id': 'arXiv:2502.15938', 'title': 'Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs', 'authors': 'Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, Daria Soboleva, Joel Hestness', 'link': 'https://arxiv.org/abs/2502.15938', 'abstract': 'LLMs are commonly trained with a learning rate (LR) warmup, followed by cosine decay to 10% of the maximum (10x decay). In a large-scale empirical study, we show that under an optimal peak LR, a simple linear decay-to-zero (D2Z) schedule consistently outperforms other schedules when training at compute-optimal dataset sizes. D2Z is superior across a range of model sizes, batch sizes, datasets, and vocabularies. Benefits increase as dataset size increases. Leveraging a novel interpretation of AdamW as an exponential moving average of weight updates, we show how linear D2Z optimally balances the demands of early training (moving away from initial conditions) and late training (averaging over more updates in order to mitigate gradient noise). In experiments, a 610M-parameter model trained for 80 tokens-per-parameter (TPP) using D2Z achieves lower loss than when trained for 200 TPP using 10x decay, corresponding to an astonishing 60% compute savings. Models such as Llama2-7B, trained for 286 TPP with 10x decay, could likely have saved a majority of compute by training with D2Z.', 'abstract_zh': '大规模预训练语言模型在计算最优数据集规模下训练时，经过优化的最大学习率与线性衰减至零（D2Z）调度策略相比，其他调度策略表现更优。线性D2Z调度策略在不同模型规模、批次大小、数据集和词汇量中均表现出优越性。随着数据集规模的增加，这种优势更加明显。通过新颖解读AdamW为权重更新的指数移动平均值，我们展示了线性D2Z如何在早期训练（远离初始条件）和后期训练（通过更多更新来降低梯度噪声）的需求之间实现最优平衡。实验表明，使用D2Z训练80个token-per-parameter（TPP）的610M参数模型，其损失低于使用10x衰减训练200个TPP的模型，计算成本节省了60%。使用10x衰减训练286个TPP的Llama2-7B等模型，很可能通过使用D2Z训练节省大部分计算资源。', 'title_zh': '直趋零：为何对学习率线性衰减至零是大型语言模型的最佳选择'}
{'arxiv_id': 'arXiv:2502.15920', 'title': 'Self-Taught Agentic Long Context Understanding', 'authors': 'Yufan Zhuang, Xiaodong Yu, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Jingbo Shang, Zicheng Liu, Emad Barsoum', 'link': 'https://arxiv.org/abs/2502.15920', 'abstract': "Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms state-of-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows.", 'abstract_zh': 'Agentic长语境理解（AgenticLU）：通过目标性自我澄清与上下文接地提升大型语言模型处理复杂长语境问题的能力', 'title_zh': '自我教授的代理长期上下文理解'}
{'arxiv_id': 'arXiv:2502.15902', 'title': 'IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector', 'authors': 'Zheng Chen, Yushi Feng, Changyang He, Yue Deng, Hongxi Pu, Bo Li', 'link': 'https://arxiv.org/abs/2502.15902', 'abstract': 'Large Language Models (LLMs) have attained human-level fluency in text generation, which complicates the distinguishing between human-written and LLM-generated texts. This increases the risk of misuse and highlights the need for reliable detectors. Yet, existing detectors exhibit poor robustness on out-of-distribution (OOD) data and attacked data, which is critical for real-world scenarios. Also, they struggle to provide explainable evidence to support their decisions, thus undermining the reliability. In light of these challenges, we propose IPAD (Inverse Prompt for AI Detection), a novel framework consisting of a Prompt Inverter that identifies predicted prompts that could have generated the input text, and a Distinguisher that examines how well the input texts align with the predicted prompts. We develop and examine two versions of Distinguishers. Empirical evaluations demonstrate that both Distinguishers perform significantly better than the baseline methods, with version2 outperforming baselines by 9.73% on in-distribution data (F1-score) and 12.65% on OOD data (AUROC). Furthermore, a user study is conducted to illustrate that IPAD enhances the AI detection trustworthiness by allowing users to directly examine the decision-making evidence, which provides interpretable support for its state-of-the-art detection results.', 'abstract_zh': '大语言模型（LLMs）在文本生成中达到了人类水平的流畅度，这使得区分人类撰写和LLM生成的文本变得复杂，增加了滥用风险，凸显了可靠检测器的必要性。然而，现有检测器在处理离分布（OOD）数据和攻击数据时表现出较差的鲁棒性，这对于真实世界的应用至关重要。此外，它们难以提供可解释的证据来支持其决策，从而削弱了其可靠性。鉴于这些挑战，我们提出了IPAD（逆向提示用于AI检测）这一新框架，该框架包括一个提示反转器，用于识别可能生成输入文本的预测提示，以及一个辨别器，用于检查输入文本与预测提示的匹配程度。我们开发并研究了两种版本的辨别器。实证评估表明，这两种辨别器都显著优于基线方法，第二版本在分布内数据上以F1分数领先基线方法9.73%，在OOD数据上以AUROC领先12.65%。此外，我们进行了用户研究，以说明IPAD通过使用户能够直接检查决策证据来提高AI检测的可信度，从而为其最先进的检测结果提供可解释的支持。', 'title_zh': 'IPAD: 反向提示词以检测AI生成的内容——一种稳健且可解释的LLM生成文本检测器'}
{'arxiv_id': 'arXiv:2502.15895', 'title': 'Directional Gradient Projection for Robust Fine-Tuning of Foundation Models', 'authors': 'Chengyue Huang, Junjiao Tian, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira', 'link': 'https://arxiv.org/abs/2502.15895', 'abstract': 'Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose Directional Gradient Projection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness.', 'abstract_zh': '鲁棒微调旨在适应大型基础模型以处理下游任务的同时保持对分布转移的鲁棒性。现有的方法主要侧重于通过微调和预训练权重之间的幅度约束和投影当前模型至预训练初始化，这通常需要大量超参数调整，并且有时会导致欠拟合。在这项工作中，我们提出了方向梯度投影（DiGraP），一种新颖的逐层可训练方法，该方法结合了梯度的方向信息，以桥接正则化和多目标优化。除了在图像分类中展示我们的方法外，作为另一项贡献，我们将此领域推广到鲁棒微调的多模态评估设置中。具体来说，我们首先通过分析重新表述的图像分类任务下的视觉问答（VQA）基准，并进一步按分布转移类型和程度（即近似分布外与远分布外）对十个分布外（OOD）VQA数据集进行分类，以弥合单模态与多模态之间的差距。实验结果表明，DiGraP在图像分类和VQA任务中均优于现有的基线方法，无论是使用判别性还是生成性回溯模型，都提高了分布内（ID）泛化能力和分布外（OOD）鲁棒性。', 'title_zh': '面向方向梯度投影的鲁棒基础模型微调方法'}
{'arxiv_id': 'arXiv:2502.15872', 'title': 'MutaGReP: Execution-Free Repository-Grounded Plan Search for Code-Use', 'authors': 'Zaid Khan, Ali Farhadi, Ranjay Krishna, Luca Weihs, Mohit Bansal, Tanmay Gupta', 'link': 'https://arxiv.org/abs/2502.15872', 'abstract': "When a human requests an LLM to complete a coding task using functionality from a large code repository, how do we provide context from the repo to the LLM? One approach is to add the entire repo to the LLM's context window. However, most tasks involve only fraction of symbols from a repo, longer contexts are detrimental to the LLM's reasoning abilities, and context windows are not unlimited. Alternatively, we could emulate the human ability to navigate a large repo, pick out the right functionality, and form a plan to solve the task. We propose MutaGReP (Mutation-guided Grounded Repository Plan Search), an approach to search for plans that decompose a user request into natural language steps grounded in the codebase. MutaGReP performs neural tree search in plan space, exploring by mutating plans and using a symbol retriever for grounding. On the challenging LongCodeArena benchmark, our plans use less than 5% of the 128K context window for GPT-4o but rival the coding performance of GPT-4o with a context window filled with the repo. Plans produced by MutaGReP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT-4o with full repo context and enable progress on the hardest LongCodeArena tasks. Project page: this http URL", 'abstract_zh': '当人类请求LLM使用大型代码库中的功能完成编码任务时，我们如何向LLM提供代码库的上下文？一种方法是将整个代码库添加到LLM的上下文窗口中。然而，大多数任务仅涉及代码库中的一小部分符号，过长的上下文对LLM的推理能力有害，而且上下文窗口并非无限。另一种方法是模拟人类在大型代码库中导航、挑选合适功能并制定解决任务计划的能力。我们提出了MutaGReP（基于 mutation 的接地代码库计划搜索）方法，该方法通过在计划空间中进行神经树搜索，通过突变计划和使用符号检索器进行接地来寻找将用户请求分解为基于代码库的自然语言步骤的计划。在具有挑战性的LongCodeArena基准测试中，我们的计划使用了少于5%的128K上下文窗口，但其编码性能与上下文窗口填满代码库的GPT-4o相当。MutaGReP生成的计划使Qwen 2.5 Coder 32B和72B能够与满载代码库上下文的GPT-4o性能相当，并且能够解决LongCodeArena中最难的任务。项目页面: this http URL。', 'title_zh': 'MutaGReP: 不执行的代码库-grounded 计划搜索'}
{'arxiv_id': 'arXiv:2502.15871', 'title': 'A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare', 'authors': 'Manar Aljohani, Jun Hou, Sindhura Kommu, Xuan Wang', 'link': 'https://arxiv.org/abs/2502.15871', 'abstract': 'The application of large language models (LLMs) in healthcare has the potential to revolutionize clinical decision-making, medical research, and patient care. As LLMs are increasingly integrated into healthcare systems, several critical challenges must be addressed to ensure their reliable and ethical deployment. These challenges include truthfulness, where models generate misleading information; privacy, with risks of unintentional data retention; robustness, requiring defenses against adversarial attacks; fairness, addressing biases in clinical outcomes; explainability, ensuring transparent decision-making; and safety, mitigating risks of misinformation and medical errors. Recently, researchers have begun developing benchmarks and evaluation frameworks to systematically assess the trustworthiness of LLMs. However, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights into this area. This survey bridges this gap by providing a comprehensive overview of the recent research of existing methodologies and solutions aimed at mitigating the above risks in healthcare. By focusing on key trustworthiness dimensions including truthfulness, privacy and safety, robustness, fairness and bias, and explainability, we present a thorough analysis of how these issues impact the reliability and ethical use of LLMs in healthcare. This paper highlights ongoing efforts and offers insights into future research directions to ensure the safe and trustworthy deployment of LLMs in healthcare.', 'abstract_zh': '大型语言模型在医疗领域中的应用有可能革新临床决策、医学研究和患者护理。随着大型语言模型逐渐整合进医疗系统，必须解决若干关键挑战以确保其可靠和负责任的部署。这些挑战包括真实性（模型生成误导性信息）、隐私（无意数据保留的风险）、稳健性（对抗攻击的防御需求）、公平性（临床结果中的偏见）、可解释性（透明决策机制的保障）和安全性（减少错误信息和医疗错误的风险）。最近，研究人员已经开始开发基准和评估框架，以系统地评估大型语言模型的可信度。然而，医疗领域中大型语言模型的可信度仍然缺乏系统性的研究，未能提供一个全面的理解和未来潜在的发展方向。本文通过提供对现有方法和解决方案的全面概述，填补了这一空白，关注了医疗领域中真实性、隐私和安全性、稳健性、公平性和偏见、以及可解释性等关键信任维度，全面分析了这些问题如何影响大型语言模型在医疗领域的可靠性和道德应用。本文突显了现有的努力，并为确保大型语言模型在医疗领域的安全和可信部署提供了未来研究方向的见解。', 'title_zh': '大型语言模型在医疗健康领域的可信性综述'}
{'arxiv_id': 'arXiv:2502.15865', 'title': 'Position: Standard Benchmarks Fail -- LLM Agents Present Overlooked Risks for Financial Applications', 'authors': 'Zichen Chen, Jiaao Chen, Jianda Chen, Misha Sra', 'link': 'https://arxiv.org/abs/2502.15865', 'abstract': 'Current financial LLM agent benchmarks are inadequate. They prioritize task performance while ignoring fundamental safety risks. Threats like hallucinations, temporal misalignment, and adversarial vulnerabilities pose systemic risks in high-stakes financial environments, yet existing evaluation frameworks fail to capture these risks. We take a firm position: traditional benchmarks are insufficient to ensure the reliability of LLM agents in finance. To address this, we analyze existing financial LLM agent benchmarks, finding safety gaps and introducing ten risk-aware evaluation metrics. Through an empirical evaluation of both API-based and open-weight LLM agents, we reveal hidden vulnerabilities that remain undetected by conventional assessments. To move the field forward, we propose the Safety-Aware Evaluation Agent (SAEA), grounded in a three-level evaluation framework that assesses agents at the model level (intrinsic capabilities), workflow level (multi-step process reliability), and system level (integration robustness). Our findings highlight the urgent need to redefine LLM agent evaluation standards by shifting the focus from raw performance to safety, robustness, and real world resilience.', 'abstract_zh': '当前的金融大语言模型代理基准不足。它们侧重于任务性能，而忽视了基本的安全风险。像幻觉、时间错位和对抗性漏洞这样的威胁在高风险金融环境中可能引发系统性风险，但现有的评估框架未能捕捉这些风险。我们认为：传统的基准不足以确保金融大语言模型代理的可靠性。为解决这一问题，我们分析了现有的金融大语言模型代理基准，发现了安全漏洞，并引入了十个风险意识评价指标。通过实证评估基于API和开放权重的大语言模型代理，我们揭示了常规评估难以发现的潜在脆弱性。为了推动该领域的发展，我们提出了安全意识评价代理（SAEA），基于一个三层评估框架，分别从模型层面（内在能力）、工作流程层面（多步过程可靠性）和系统层面（整合鲁棒性）进行评估。我们的研究强调了重新界定大语言模型代理评价标准的紧迫性，即从单纯的性能转向关注安全、稳健性和现实世界的韧性。', 'title_zh': '位置：标准基准失效——大型语言模型代理在金融应用中隐藏风险'}
{'arxiv_id': 'arXiv:2502.15860', 'title': 'Synthetic vs. Gold: The Role of LLM-Generated Labels and Data in Cyberbullying Detection', 'authors': 'Arefeh Kazemi, Sri Balaaji Natarajan Kalaivendan, Joachim Wagner, Hamza Qadeer, Brian Davis', 'link': 'https://arxiv.org/abs/2502.15860', 'abstract': 'This study investigates the role of LLM-generated synthetic data in cyberbullying detection. We conduct a series of experiments where we replace some or all of the authentic data with synthetic data, or augment the authentic data with synthetic data. We find that synthetic cyberbullying data can be the basis for training a classifier for harm detection that reaches performance close to that of a classifier trained with authentic data. Combining authentic with synthetic data shows improvements over the baseline of training on authentic data alone for the test data for all three LLMs tried. These results highlight the viability of synthetic data as a scalable, ethically viable alternative in cyberbullying detection while emphasizing the critical impact of LLM selection on performance outcomes.', 'abstract_zh': '本研究探讨了生成式语言模型（LLM）合成数据在 cyberbullying 检测中的作用。我们进行了一系列实验，其中部分或全部真实数据被合成数据替换，或者将合成数据用于增强真实数据。我们发现，合成 cyberbullying 数据可以作为训练用于伤害检测分类器的基础，其性能接近于使用真实数据训练的分类器。将真实数据与合成数据结合使用，在为三个尝试的 LLM 的测试数据训练时，表现出优于仅使用真实数据基线的效果。这些结果强调了合成数据在 cyberbullying 检测中作为可扩展且伦理上可行的替代方案的可行性，并突出了 LLM 选择对性能结果的关键影响。', 'title_zh': '合成数据 vs. 真实数据：LLM生成的标签和数据在检测网络欺凌中的作用'}
{'arxiv_id': 'arXiv:2502.15857', 'title': 'PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation', 'authors': 'Tao Fan, Guoqiang Ma, Yuanfeng Song, Lixin Fan, Kai Chen, Qiang Yang', 'link': 'https://arxiv.org/abs/2502.15857', 'abstract': "Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we propose PPC-GPT, a innovative privacy-preserving federated framework specifically designed for compressing LLMs into task-specific SLMs via pruning and Chain-of-Thought (COT) distillation. PPC-GPT works on a server-client federated architecture, where the client sends differentially private (DP) perturbed task-specific data to the server's LLM. The LLM then generates synthetic data along with their corresponding rationales. This synthetic data is subsequently used for both LLM pruning and retraining processes. Additionally, we harness COT knowledge distillation, leveraging the synthetic data to further improve the retraining of structurally-pruned SLMs. Our experimental results demonstrate the effectiveness of PPC-GPT across various text generation tasks. By compressing LLMs into task-specific SLMs, PPC-GPT not only achieves competitive performance but also prioritizes data privacy protection.", 'abstract_zh': '将大型语言模型（LLMs）压缩成任务特定的小型语言模型（SLMs）面临的两大挑战是保护领域特定知识的隐私和管理有限的资源。为了应对这些挑战，我们提出了一种名为PPC-GPT的创新隐私保护联邦框架，该框架专门设计用于通过剪枝和思维链（COT） distilled知识精炼将LLMs压缩成任务特定的SLMs。PPC-GPT基于服务器-客户端联邦架构，客户端向服务器的LLM发送差分隐私（DP）扰动的任务特定数据。LLM随后生成合成数据及其相应的推理过程。这些合成数据随后用于LLM剪枝和重新训练过程。此外，我们利用思维链（COT）知识精炼，通过合成数据进一步提高结构剪枝的SLMs的重新训练。实验结果表明，PPC-GPT在各种文本生成任务中具有有效性。通过将LLMs压缩成任务特定的SLMs，PPC-GPT不仅实现了竞争性的性能，还优先考虑了数据隐私保护。', 'title_zh': 'PPC-GPT：联邦制任务特定大型语言模型压缩通过剪枝和链式思考精炼'}
{'arxiv_id': 'arXiv:2502.15851', 'title': 'Control Illusion: The Failure of Instruction Hierarchies in Large Language Models', 'authors': 'Yilin Geng, Haonan Li, Honglin Mu, Xudong Han, Timothy Baldwin, Omri Abend, Eduard Hovy, Lea Frermann', 'link': 'https://arxiv.org/abs/2502.15851', 'abstract': 'Large language models (LLMs) are increasingly deployed with hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. While controlled prompt engineering and model fine-tuning show modest improvements, our results indicate that instruction hierarchy enforcement is not robustly realized, calling for deeper architectural innovations beyond surface-level modifications.', 'abstract_zh': '大型语言模型（LLMs） increasingly deploy hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. While controlled prompt engineering and model fine-tuning show modest improvements, our results indicate that instruction hierarchy enforcement is not robustly realized, calling for deeper architectural innovations beyond surface-level modifications。', 'title_zh': '控制幻象：大型语言模型中指令层级结构的失效'}
{'arxiv_id': 'arXiv:2502.15850', 'title': 'Forecasting Frontier Language Model Agent Capabilities', 'authors': 'Govind Pimpale, Axel Højmark, Jérémy Scheurer, Marius Hobbhahn', 'link': 'https://arxiv.org/abs/2502.15850', 'abstract': 'As Language Models (LMs) increasingly operate as autonomous agents, accurately forecasting their capabilities becomes crucial for societal preparedness. We evaluate six forecasting methods that predict downstream capabilities of LM agents. We use "one-step" approaches that predict benchmark scores from input metrics like compute or model release date directly or "two-step" approaches that first predict an intermediate metric like the principal component of cross-benchmark performance (PC-1) and human-evaluated competitive Elo ratings. We evaluate our forecasting methods by backtesting them on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the validated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM agent performance for frontier models on three benchmarks: SWE-Bench Verified (software development), Cybench (cybersecurity assessment), and RE-Bench (ML research engineering). Our forecast predicts that by the beginning of 2026, non-specialized LM agents with low capability elicitation will reach a success rate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach an 87% success rate. Our approach does not account for recent advances in inference-compute scaling and might thus be too conservative.', 'abstract_zh': '语言模型（LMs）作为自主代理越来越广泛地运行时，准确预测其能力对于社会准备变得至关重要。我们评估了六种预测LM代理下游能力的方法。我们使用“一步法”直接从计算量或模型发布日期等输入度量预测基准得分，或使用“两步法”首先预测跨基准性能的主要成分（PC-1）和人工评估的竞争Elo评级作为中间度量。我们通过在来自OpenLLM 2排行榜的38个LM数据集上回测我们的预测方法来评估这些方法。然后，我们使用经过验证的两步法（发布日期→Elo→基准）预测前沿模型在三个基准上的LM代理性能：SWE-Bench Verified（软件开发）、Cybench（网络安全部署评估）和RE-Bench（机器学习研究工程）。我们的预测表明，到2026年初，低能力激发的非专业化LM代理将在SWE-Bench Verified上达到54%的成功率，而最先进的LM代理将达到87%的成功率。我们的方法未考虑最近的推理-计算扩展进展，因此可能过于保守。', 'title_zh': '前沿语言模型代理能力预测'}
{'arxiv_id': 'arXiv:2502.15845', 'title': 'Verify when Uncertain: Beyond Self-Consistency in Black Box Hallucination Detection', 'authors': 'Yihao Xue, Kristjan Greenewald, Youssef Mroueh, Baharan Mirzasoleiman', 'link': 'https://arxiv.org/abs/2502.15845', 'abstract': 'Large Language Models (LLMs) suffer from hallucination problems, which hinder their reliability in sensitive applications. In the black-box setting, several self-consistency-based techniques have been proposed for hallucination detection. We empirically study these techniques and show that they achieve performance close to that of a supervised (still black-box) oracle, suggesting little room for improvement within this paradigm. To address this limitation, we explore cross-model consistency checking between the target model and an additional verifier LLM. With this extra information, we observe improved oracle performance compared to purely self-consistency-based methods. We then propose a budget-friendly, two-stage detection algorithm that calls the verifier model only for a subset of cases. It dynamically switches between self-consistency and cross-consistency based on an uncertainty interval of the self-consistency classifier. We provide a geometric interpretation of consistency-based hallucination detection methods through the lens of kernel mean embeddings, offering deeper theoretical insights. Extensive experiments show that this approach maintains high detection performance while significantly reducing computational cost.', 'abstract_zh': '大型语言模型（LLMs）面临幻觉问题，这妨碍了它们在敏感应用中的可靠性。在黑盒设置中，已经提出了一些基于自一致性的方法用于检测幻觉。我们通过实证研究这些方法，并表明它们的性能接近监督（仍为黑盒） oracle 的性能，这表明在这个范式中几乎没有改进的空间。为了解决这一局限性，我们探讨了目标模型与附加的验证器LLM之间的一致性检查。借助这种额外信息，我们观察到与纯自一致性方法相比，Oracle的性能有所提高。然后，我们提出了一种预算友好的两阶段检测算法，仅在一部分情况下调用验证器模型。该算法根据自一致性分类器的不确定性区间动态切换自一致性与交叉一致性。我们通过核均值嵌入的视角提供了一致性基幻觉检测方法的几何解释，提供了更深入的理论洞察。大量的实验表明，这种方法在保持高检测性能的同时显著降低了计算成本。', 'title_zh': '在不确定时验证：超越黑盒 hallucination 检测的自我一致性'}
{'arxiv_id': 'arXiv:2502.15836', 'title': 'Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models', 'authors': 'Haokun Chen, Sebastian Szyller, Weilin Xu, Nageen Himayat', 'link': 'https://arxiv.org/abs/2502.15836', 'abstract': 'Large language models (LLMs) have become increasingly popular. Their emergent capabilities can be attributed to their massive training datasets. However, these datasets often contain undesirable or inappropriate content, e.g., harmful texts, personal information, and copyrighted material. This has promoted research into machine unlearning that aims to remove information from trained models. In particular, approximate unlearning seeks to achieve information removal by strategically editing the model rather than complete model retraining.\nRecent work has shown that soft token attacks (STA) can successfully extract purportedly unlearned information from LLMs, thereby exposing limitations in current unlearning methodologies. In this work, we reveal that STAs are an inadequate tool for auditing unlearning. Through systematic evaluation on common unlearning benchmarks (Who Is Harry Potter? and TOFU), we demonstrate that such attacks can elicit any information from the LLM, regardless of (1) the deployed unlearning algorithm, and (2) whether the queried content was originally present in the training corpus. Furthermore, we show that STA with just a few soft tokens (1-10) can elicit random strings over 400-characters long. Thus showing that STAs are too powerful, and misrepresent the effectiveness of the unlearning methods.\nOur work highlights the need for better evaluation baselines, and more appropriate auditing tools for assessing the effectiveness of unlearning in LLMs.', 'abstract_zh': '大型语言模型（LLMs）日益受到关注。其涌现能力源于庞大的训练数据集。然而，这些数据集常包含不良或不合适的内容，如有害文本、个人信息和版权材料。这促进了针对机器遗忘的研究，旨在从训练模型中移除信息。特别是，近似的遗忘希望通过战略性地编辑模型而非完全重新训练来实现信息移除。\n\n现有研究表明，软标记攻击（STA）可以从LLMs中成功提取被认为已遗忘的信息，从而揭示当前遗忘方法的局限性。在本工作中，我们揭示了STA作为审计工具的不足。通过在常见的遗忘基准测试上进行系统评估（Harry Potter Who Is和TOFU），我们证明这些攻击可以从中提取任何信息，无论（1）所采用的遗忘算法如何，以及（2）查询的内容是否最初存在于训练语料库中。此外，我们展示了只需少量软标记（1-10个）的STA就可以提取长度超过400字符的随机字符串，从而表明STA过于强大，误导了对遗忘方法有效性的评估。\n\n我们的工作突显了需要更好的评估基准和更合适的审计工具，以评估LLMs中遗忘的有效性。', 'title_zh': '软令牌攻击不能可靠地审计大型语言模型的未学习状态'}
{'arxiv_id': 'arXiv:2502.15835', 'title': 'Pragmatic Reasoning improves LLM Code Generation', 'authors': 'Zhuchen Cao, Sven Apel, Adish Singla, Vera Demberg', 'link': 'https://arxiv.org/abs/2502.15835', 'abstract': "Large Language Models (LLMs) have demonstrated impressive potential in translating natural language (NL) instructions into program code. However, user instructions often contain inherent ambiguities, making it challenging for LLMs to generate code that accurately reflects the user's true intent. To address this challenge, researchers have proposed to produce multiple candidates of the program code and then rerank them to identify the best solution. In this paper, we propose CodeRSA, a novel code candidate reranking mechanism built upon the Rational Speech Act (RSA) framework, designed to guide LLMs toward more comprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using one of the latest LLMs on a popular code generation dataset. Our experiment results show that CodeRSA consistently outperforms common baselines, surpasses the state-of-the-art approach in most cases, and demonstrates robust overall performance. These findings underscore the effectiveness of integrating pragmatic reasoning into code candidate reranking, offering a promising direction for enhancing code generation quality in LLMs.", 'abstract_zh': '大型语言模型（LLMs）展示了将自然语言（NL）指令转化为程序代码的惊人潜力。然而，用户指令往往含有固有的歧义性，这使得LLMs难以生成准确反映用户真实意图的代码。为解决这一挑战，研究人员提出了生成多个程序代码候选方案，然后重新排序以识别最佳方案的方法。在本文中，我们提出了一种基于理性言语行为（RSA）框架的新颖的代码候选重新排序机制CodeRSA，旨在引导LLMs进行更全面的关于用户意图的实用推理。我们使用最新的一种LLM在流行的数据集上评估CodeRSA。实验结果表明，CodeRSA始终优于常见的基线方法，在大多数情况下超越了最先进的方法，并且整体表现稳健。这些发现强调将实用推理整合到代码候选重新排序中的有效性，为提高LLMs代码生成质量提供了有前景的方向。', 'title_zh': '实用推理提升大模型代码生成能力'}
{'arxiv_id': 'arXiv:2502.15828', 'title': 'A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models', 'authors': 'Mengyang Sun, Yihao Wang, Tao Feng, Dan Zhang, Yifan Zhu, Jie Tang', 'link': 'https://arxiv.org/abs/2502.15828', 'abstract': 'In order to streamline the fine-tuning of foundation models, Low-Rank Adapters (LoRAs) have been substantially adopted across various fields, including instruction tuning and domain adaptation. The underlying concept of LoRA involves decomposing a full-rank matrix into the product of two lower-rank matrices, which reduces storage consumption and accelerates the training process. Furthermore, to address the limited expressive capacity of LoRA, the Mixture-of-Expert (MoE) has been introduced for incorporating multiple LoRA adapters. The integration of LoRA experts leads to a visible improvement across several downstream scenes. However, the mixture of LoRAs (MoE-LoRA) still exhibits its low robustness during tuning and inferring. Inspired by the Riemannian Preconditioners which train LoRA as a sub-space projector, we propose a new training strategy for MoE-LoRA, to stabilize and boost its feature learning procedure by multi-space projections. Examinations on SGD and AdamW optimizers demonstrate the effectiveness of our methodology. Source code is available at this https URL.', 'abstract_zh': '为了精简基础模型的微调过程，低秩适配器（LoRA）已在指令微调和领域适应等多个领域中广泛采用。LoRA 的基本概念涉及将一个满秩矩阵分解为两个低秩矩阵的乘积，从而减少存储消耗并加速训练过程。为进一步解决 LoRA 表达能力有限的问题，引入了专家混叠（MoE）机制，将多个 LoRA 适配器结合在一起。LoRA 专家的集成在多个下游场景中表现出显著改进。然而，LoRA 专家混叠（MoE-LoRA）在微调和推理过程中仍然表现出较低的鲁棒性。受黎曼预条件化训练 LoRA 作为子空间投影的启发，我们提出了一种新的 MoE-LoRA 训练策略，通过多空间投影稳定并提升其特征学习过程。对 SGD 和 AdamW 优化器的实验验证了该方法的有效性。相关源代码可在以下链接获取。', 'title_zh': '一种更强大的低秩专家混合模型用于基础模型微调'}
{'arxiv_id': 'arXiv:2502.15826', 'title': 'CoME: An Unlearning-based Approach to Conflict-free Model Editing', 'authors': 'Dahyun Jung, Jaehyung Seo, Jaewook Lee, Chanjun Park, Heuiseok Lim', 'link': 'https://arxiv.org/abs/2502.15826', 'abstract': "Large language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features. Through experiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we demonstrate that CoME improves both editing accuracy and model reliability when applied to existing editing methods. Our results highlight that the targeted removal of outdated knowledge is crucial for enhancing model editing effectiveness and maintaining the model's generative performance.", 'abstract_zh': '大型语言模型中的冲突-free模型编辑（CoME）：通过选择性去除过时知识提高知识更新准确性并与保持语言特征兼容', 'title_zh': 'CoME：一种基于去学习的冲突-free模型编辑方法'}
{'arxiv_id': 'arXiv:2502.15823', 'title': 'InductionBench: LLMs Fail in the Simplest Complexity Class', 'authors': 'Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang Wang', 'link': 'https://arxiv.org/abs/2502.15823', 'abstract': "Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available this https URL.", 'abstract_zh': '大型语言模型（LLMs）在推理方面取得了显著进步，许多现有的基准测试已被如o1和o3等模型完全或部分解决。然而，这些基准测试大多侧重于演绎推理，包括数学和编程任务，其中规则如数学公理或编程语法被明确定义，使LLMs能够规划并应用这些规则以得出解决方案。相比之下，归纳推理，即从观察到的数据中推断出潜在规则，仍然较少被探索。这样的归纳过程是科学研究的核心，因为它们使研究人员能够从经验观察中提取一般原则。为了评估LLMs是否具备这种能力，我们引入了InductionBench，一种新的基准测试，用于评估LLMs的归纳推理能力。我们的实验结果表明，即使是最先进的模型也难以掌握子正则层次结构中最小的复杂性类，突显了当前LLMs在归纳推理能力方面的一个明显缺陷。更多详细信息及数据请参见：https://...', 'title_zh': 'InductionBench: 大型语言模型在最简单的复杂性类中失败'}
{'arxiv_id': 'arXiv:2502.15814', 'title': 'Slamming: Training a Speech Language Model on One GPU in a Day', 'authors': 'Gallil Maimon, Avishai Elmakies, Yossi Adi', 'link': 'https://arxiv.org/abs/2502.15814', 'abstract': 'We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - this https URL .', 'abstract_zh': '我们介绍了一种在单块学术GPU上于24小时内训练高质量语音语言模型的方案SLAM。通过模型初始化和架构的实证分析、合成训练数据、使用合成数据进行偏好优化以及调整其他所有组件，我们实证展示了更高计算资源的情况下，该训练方案能够达到与领先语音语言模型相当的效果，但计算成本却大大降低。我们希望这些洞见能够使语音语言模型的训练和研究更加普及。在语音语言模型的可扩展性规律的背景下，我们的结果远超预期的计算最优性能，为语音语言模型的可行性提供了乐观的前景。更多信息请参见此链接：https://this-url', 'title_zh': 'Slamming: 在一天之内于一台GPU上训练一个语音语言模型'}
{'arxiv_id': 'arXiv:2502.15810', 'title': 'Zero-Shot Commonsense Validation and Reasoning with Large Language Models: An Evaluation on SemEval-2020 Task 4 Dataset', 'authors': 'Rawand Alfugaha, Mohammad AL-Smadi', 'link': 'https://arxiv.org/abs/2502.15810', 'abstract': 'This study evaluates the performance of Large Language Models (LLMs) on SemEval-2020 Task 4 dataset, focusing on commonsense validation and explanation. Our methodology involves evaluating multiple LLMs, including LLaMA3-70B, Gemma2-9B, and Mixtral-8x7B, using zero-shot prompting techniques. The models are tested on two tasks: Task A (Commonsense Validation), where models determine whether a statement aligns with commonsense knowledge, and Task B (Commonsense Explanation), where models identify the reasoning behind implausible statements. Performance is assessed based on accuracy, and results are compared to fine-tuned transformer-based models. The results indicate that larger models outperform previous models and perform closely to human evaluation for Task A, with LLaMA3-70B achieving the highest accuracy of 98.40% in Task A whereas, lagging behind previous models with 93.40% in Task B. However, while models effectively identify implausible statements, they face challenges in selecting the most relevant explanation, highlighting limitations in causal and inferential reasoning.', 'abstract_zh': '本研究评估了大型语言模型（LLMs）在SemEval-2020 Task 4数据集上的性能，重点关注常识验证和解释。我们的方法包括使用零样本提示技术评估多个LLM，包括LLaMA3-70B、Gemma2-9B和Mixtral-8x7B。这些模型在两个任务上进行测试：任务A（常识验证），模型判断陈述是否符合常识知识；任务B（常识解释），模型识别不合理陈述背后的推理。性能基于准确率进行评估，并将结果与微调的变换器模型进行比较。结果显示，较大的模型优于之前的模型并在任务A上接近人类评估的表现，其中LLaMA3-70B在任务A上取得了最高准确率98.40%，但在任务B上低于之前的模型93.40%。然而，虽然模型能够有效识别不合理陈述，但在选择最相关解释方面面临挑战，这突显了因果推理和推理能力的局限性。', 'title_zh': '零样本常识验证与推理：基于SemEval-2020 Task 4数据集的大型语言模型评估'}
{'arxiv_id': 'arXiv:2502.15806', 'title': 'A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos', 'authors': 'Yang Yao, Xuan Tong, Ruofan Wang, Yixu Wang, Lujundong Li, Liang Liu, Yan Teng, Yingchun Wang', 'link': 'https://arxiv.org/abs/2502.15806', 'abstract': 'Large Reasoning Models (LRMs) have significantly advanced beyond traditional Large Language Models (LLMs) with their exceptional logical reasoning capabilities, yet these improvements introduce heightened safety risks. When subjected to jailbreak attacks, their ability to generate more targeted and organized content can lead to greater harm. Although some studies claim that reasoning enables safer LRMs against existing LLM attacks, they overlook the inherent flaws within the reasoning process itself. To address this gap, we propose the first jailbreak attack targeting LRMs, exploiting their unique vulnerabilities stemming from the advanced reasoning capabilities. Specifically, we introduce a Chaos Machine, a novel component to transform attack prompts with diverse one-to-one mappings. The chaos mappings iteratively generated by the machine are embedded into the reasoning chain, which strengthens the variability and complexity and also promotes a more robust attack. Based on this, we construct the Mousetrap framework, which makes attacks projected into nonlinear-like low sample spaces with mismatched generalization enhanced. Also, due to the more competing objectives, LRMs gradually maintain the inertia of unpredictable iterative reasoning and fall into our trap. Success rates of the Mousetrap attacking o1-mini, claude-sonnet and gemini-thinking are as high as 96%, 86% and 98% respectively on our toxic dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench, attacking claude-sonnet, well-known for its safety, Mousetrap can astonishingly achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This paper contains inappropriate, offensive and harmful content.', 'abstract_zh': '大型推理模型（LRMs）在具备卓越的逻辑推理能力方面显著超越了传统的大型语言模型（LLMs），但这些进步也带来了更高的安全风险。当遭受狱Break攻击时，它们生成更具针对性和组织性的内容的能力可能导致更大的危害。尽管一些研究声称推理能够使LRMs在现有的LLM攻击中更安全，但他们忽视了推理过程中固有的缺陷。为解决这一问题，我们提出了首款针对LRMs的狱Break攻击，利用其独特的高级推理能力带来的脆弱性。具体来说，我们引入了一种混沌机器，这是一种新颖的组件，用于实现多对一映射的攻击提示的转换。混沌映射由机器迭代生成，并嵌入到推理链中，增强了变化性与复杂性，也促进了更坚固的攻击。在此基础上，我们构建了Mousetrap框架，使攻击投影到非线性似的小样本空间中，并增强了不匹配的泛化能力。由于竞争目标更多，LRMs逐渐维持不可预测的迭代推理的惯性，落入我们的陷阱。在我们的毒性数据集Trotter上，Mousetrap攻击o1-mini、claude-sonnet和gemini-thinking的成功率分别为96%、86%和98%。在AdvBench、StrongREJECT和HarmBench等基准测试中，针对well-known在安全性方面表现优异的claude-sonnet，Mousetrap可以实现惊人的87.5%、86.58%和93.13%的成功率。注意：本文包含不当、冒犯性和有害内容。', 'title_zh': '一个老鼠夹：通过迭代混沌链对大型推理模型进行突破以越狱'}
{'arxiv_id': 'arXiv:2502.15801', 'title': 'An explainable transformer circuit for compositional generalization', 'authors': 'Cheng Tang, Brenden Lake, Mehrdad Jazayeri', 'link': 'https://arxiv.org/abs/2502.15801', 'abstract': "Compositional generalization-the systematic combination of known components into novel structures-remains a core challenge in cognitive science and machine learning. Although transformer-based large language models can exhibit strong performance on certain compositional tasks, the underlying mechanisms driving these abilities remain opaque, calling into question their interpretability. In this work, we identify and mechanistically interpret the circuit responsible for compositional induction in a compact transformer. Using causal ablations, we validate the circuit and formalize its operation using a program-like description. We further demonstrate that this mechanistic understanding enables precise activation edits to steer the model's behavior predictably. Our findings advance the understanding of complex behaviors in transformers and highlight such insights can provide a direct pathway for model control.", 'abstract_zh': '组成性泛化——将已知组件系统组合成新颖结构——仍然是认知科学和机器学习中的核心挑战。尽管基于 Transformer 的大型语言模型在某些组成性任务上表现出强大的性能，但驱动这些能力的底层机制仍不明朗，这对其可解释性提出了疑问。在本工作中，我们识别并从机制上解释了负责紧凑型 Transformer 中组成性归纳的电路。通过因果消融，我们验证了该电路，并使用程序化的描述形式化其运行机制。进一步地，我们证明这种机制性理解能够精确编辑模型的激活，使其行为可预测地改变。我们的发现推进了对 Transformer 复杂行为的理解，并强调这些见解可以为模型控制提供直接途径。', 'title_zh': '可解释的变换器电路以实现组合泛化'}
{'arxiv_id': 'arXiv:2502.15799', 'title': 'Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models', 'authors': 'Artyom Kharinaev, Viktor Moskvoretskii, Egor Shvetsov, Kseniia Studenikina, Bykov Mikhail, Evgeny Burnaev', 'link': 'https://arxiv.org/abs/2502.15799', 'abstract': 'Large Language Models (LLMs) have emerged as powerful tools for addressing modern challenges and enabling practical applications. However, their computational expense remains a significant barrier to widespread adoption. Quantization has emerged as a promising technique to democratize access and enable low resource device deployment. Despite these advancements, the safety and trustworthiness of quantized models remain underexplored, as prior studies often overlook contemporary architectures and rely on overly simplistic benchmarks and evaluations. To address this gap, we introduce OpenSafetyMini, a novel open-ended safety dataset designed to better distinguish between models. We evaluate 4 state-of-the-art quantization techniques across LLaMA and Mistral models using 4 benchmarks, including human evaluations. Our findings reveal that the optimal quantization method varies for 4-bit precision, while vector quantization techniques deliver the best safety and trustworthiness performance at 2-bit precision, providing foundation for future research.', 'abstract_zh': '大型语言模型（LLMs）已成为应对现代挑战和推动实际应用的强大力量。然而，其计算成本仍然是广泛采用的主要障碍。量化技术作为一种有 promise 的方法，正在 democratize 模型的访问并使低资源设备部署成为可能。尽管取得了这些进展，量化模型的安全性和可信度仍鲜有研究，此前的研究往往忽视了现代架构并依赖过于简单的基准和评估。为填补这一空白，我们引入了 OpenSafetyMini，这是一种新颖的开放性安全数据集，旨在更好地区分不同模型。我们使用包括人工评估在内的 4 个基准，评估了 LLaMA 和 Mistral 模型上的 4 种最先进的量化技术。研究结果表明，对于 4 位精度，最佳量化方法因模型而异，而向量量化技术在 2 位精度下提供最佳的安全性和可信度性能，为未来研究奠定了基础。', 'title_zh': '探究量化方法对大型语言模型安全性和可靠性的影响'}
{'arxiv_id': 'arXiv:2502.15797', 'title': 'OCCULT: Evaluating Large Language Models for Offensive Cyber Operation Capabilities', 'authors': 'Michael Kouremetis, Marissa Dotter, Alex Byrne, Dan Martin, Ethan Michalak, Gianpaolo Russo, Michael Threet, Guido Zarrella', 'link': 'https://arxiv.org/abs/2502.15797', 'abstract': "The prospect of artificial intelligence (AI) competing in the adversarial landscape of cyber security has long been considered one of the most impactful, challenging, and potentially dangerous applications of AI. Here, we demonstrate a new approach to assessing AI's progress towards enabling and scaling real-world offensive cyber operations (OCO) tactics in use by modern threat actors. We detail OCCULT, a lightweight operational evaluation framework that allows cyber security experts to contribute to rigorous and repeatable measurement of the plausible cyber security risks associated with any given large language model (LLM) or AI employed for OCO. We also prototype and evaluate three very different OCO benchmarks for LLMs that demonstrate our approach and serve as examples for building benchmarks under the OCCULT framework. Finally, we provide preliminary evaluation results to demonstrate how this framework allows us to move beyond traditional all-or-nothing tests, such as those crafted from educational exercises like capture-the-flag environments, to contextualize our indicators and warnings in true cyber threat scenarios that present risks to modern infrastructure. We find that there has been significant recent advancement in the risks of AI being used to scale realistic cyber threats. For the first time, we find a model (DeepSeek-R1) is capable of correctly answering over 90% of challenging offensive cyber knowledge tests in our Threat Actor Competency Test for LLMs (TACTL) multiple-choice benchmarks. We also show how Meta's Llama and Mistral's Mixtral model families show marked performance improvements over earlier models against our benchmarks where LLMs act as offensive agents in MITRE's high-fidelity offensive and defensive cyber operations simulation environment, CyberLayer.", 'abstract_zh': '人工智能（AI）在网络安全对抗landscape中的竞争前景 long 被视为最具影响、最具挑战性和潜在危险的AI应用之一。在这里，我们展示了评估AI在促进和扩展现代威胁行为者使用的实际网络攻击（OCO）战术方面取得进步的新方法。我们详细介绍了OCCULT，一种轻量级操作评估框架，使网络专家能够贡献严谨且可重复的关于任何给定的大语言模型（LLM）或用于OCO的AI的可能网络风险的测量方法。我们还原型设计并评估了三种非常不同的OCO基准测试，以展示我们的方法并作为在OCCULT框架下建立基准的示例。最后，我们提供了初步评估结果，以展示该框架如何使我们能够超越传统的全有或全无的测试，这些测试多来自教育练习如夺取旗帜环境，将我们的指标和警告置于真正的网络安全威胁场景中，这些场景对现代基础设施构成了风险。我们发现，AI用于扩展现实网络安全威胁的风险已经出现了显著的近期进步。我们首次发现模型（DeepSeek-R1）能够在我们的针对LLM的威胁行为者能力测试（TACTL）中的多个选择基准测试中正确回答超过90%的具有挑战性的网络攻击知识测试题。我们还展示了Meta的Llama和Mistral的Mixtral模型家族在我们的基准测试中表现出显著性能提升，这些基准测试在MITRE的高保真网络攻击和防御操作模拟环境中，LLM作为进攻代理使用。', 'title_zh': 'OCCULT: 评估大型语言模型在恶意网络操作能力上的表现'}
{'arxiv_id': 'arXiv:2502.15796', 'title': 'Pruning as a Defense: Reducing Memorization in Large Language Models', 'authors': 'Mansi Gupta, Nikhar Waghela, Sarthak Gupta, Shourya Goel, Sanjif Shanmugavelu', 'link': 'https://arxiv.org/abs/2502.15796', 'abstract': 'Large language models have been shown to memorize significant portions of their training data, which they can reproduce when appropriately prompted. This work investigates the impact of simple pruning techniques on this behavior. Our findings reveal that pruning effectively reduces the extent of memorization in LLMs, demonstrating its potential as a foundational approach for mitigating membership inference attacks.', 'abstract_zh': '大型语言模型被证明会记忆大量训练数据，并在适当提示下重现这些数据。本工作探讨了简单剪枝技术对此行为的影响。我们的研究发现剪枝有效地减少了LLMs中的记忆程度，表明其作为减轻成员推理攻击基础方法的潜力。', 'title_zh': 'pruning作为一种防御手段：减少大型语言模型的记忆化'}
{'arxiv_id': 'arXiv:2502.15786', 'title': 'MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding', 'authors': 'Weikang Qiu, Zheng Huang, Haoyu Hu, Aosong Feng, Yujun Yan, Rex Ying', 'link': 'https://arxiv.org/abs/2502.15786', 'abstract': "Decoding functional magnetic resonance imaging (fMRI) signals into text has been a key challenge in the neuroscience community, with the potential to advance brain-computer interfaces and uncover deeper insights into brain mechanisms. However, existing approaches often struggle with suboptimal predictive performance, limited task variety, and poor generalization across subjects. In response to this, we propose MindLLM, a model designed for subject-agnostic and versatile fMRI-to-text decoding. MindLLM consists of an fMRI encoder and an off-the-shelf LLM. The fMRI encoder employs a neuroscience-informed attention mechanism, which is capable of accommodating subjects with varying input shapes and thus achieves high-performance subject-agnostic decoding. Moreover, we introduce Brain Instruction Tuning (BIT), a novel approach that enhances the model's ability to capture diverse semantic representations from fMRI signals, facilitating more versatile decoding. We evaluate MindLLM on comprehensive fMRI-to-text benchmarks. Results demonstrate that our model outperforms the baselines, improving downstream tasks by 12.0%, unseen subject generalization by 16.4%, and novel task adaptation by 25.0%. Furthermore, the attention patterns in MindLLM provide interpretable insights into its decision-making process.", 'abstract_zh': '将功能性磁共振成像(fMRI)信号解码为文本是神经科学领域的关键挑战，具有推动脑-机接口发展和揭示大脑机制深层次见解的潜力。然而，现有方法往往面临预测性能不佳、任务多样性有限以及跨被试泛化能力差等问题。为应对这一挑战，我们提出MindLLM模型，该模型旨在实现被试无关和多功能的fMRI到文本解码。MindLLM由一个fMRI编码器和一个现成的大型语言模型（LLM）组成。fMRI编码器采用了基于神经科学的注意力机制，能够适应不同输入形状的被试，从而实现高性能的被试无关解码。此外，我们引入了脑指令调优（BIT）方法，这是一种新的方法，能够增强模型从fMRI信号中捕捉多样语义表示的能力，促进更广泛的解码。我们在全面的fMRI到文本基准上评估了MindLLM。结果表明，我们的模型优于基线模型，提高了下游任务的性能12.0%，未见过的被试泛化能力16.4%，以及新型任务适应性25.0%。此外，MindLLM中的注意力模式提供了对其决策过程的可解释洞见。', 'title_zh': 'MindLLM: 一种无领域限制且多功能的fMRI到文本解码模型'}
{'arxiv_id': 'arXiv:2502.15779', 'title': 'Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer', 'authors': 'Euntae Choi, Sumin Song, Woosang Lim, Sungjoo Yoo', 'link': 'https://arxiv.org/abs/2502.15779', 'abstract': 'We propose Rotate, Clip, and Partition (RCP), a quantization-aware training (QAT) approach that first realizes extreme compression of LLMs with W2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP integrates recent rotation techniques with a novel non-uniform weight quantizer design, by quantitatively analyzing the impact of random rotation on 2-bit weight quantization. Our weight quantizer features Learnable Direct Partitioning (LDP), which introduces learnable parameters to directly learn non-uniform intervals jointly with LLM weights. We also present a specialized GPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP can compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and 5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging mobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and MetaMath-7B with no critical problems such as convergence failure and repetition. Code will be made available at blind_review.', 'abstract_zh': '我们提出了一种名为Rotate、Clip和Partition (RCP) 的量化感知训练（QAT）方法，该方法首先使用W2A4KV4（2位权重、4位激活和4位KV缓存）配置实现了对大规模语言模型的极致压缩。RCP 结合了最新的旋转技术，并设计了一种新颖的非均匀权重量化器，通过定量分析随机旋转对2位权重量化的影响。我们的权重量化器配备了可学习直接分区（LDP），引入可学习参数直接学习与大规模语言模型权重联合的非均匀区间。我们还提出了一种专门的GPU内核，支持非均匀W2A4上的GEMV操作。实验结果显示，RCP 可以将LLaMA-2-7B压缩至W2A4KV4，损失仅为2.84个WikiText2 PPL，并且内存占用减少5.29倍。此外，RCP 可以对针对移动设备的LLaMA-3.2模型以及领域特定的WizardCoder-7B和MetaMath-7B进行量化，没有出现如收敛失败和重复等关键问题。代码将在提交评审前提供。', 'title_zh': '旋转、裁剪和分区：通过结合旋转和可学习的非均匀量化器向量量化方法的研究（Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer）'}
{'arxiv_id': 'arXiv:2502.15771', 'title': 'Learning to Reason from Feedback at Test-Time', 'authors': 'Yanyang Li, Michael Lyu, Liwei Wang', 'link': 'https://arxiv.org/abs/2502.15771', 'abstract': 'Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, OpTune, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and OpTune achieve superior scalability and performance.', 'abstract_zh': '大型语言模型在单次尝试解决复杂任务具有挑战性。通常需要迭代与环境交互并利用反馈才能取得成功，因此有效的反馈利用成为关键话题。现有方法要么在长度泛化方面表现不佳，要么依赖于原始重试而未能充分利用先验信息。在本文中，我们引入了FTTT，一个新颖的范式，将反馈利用形式化为测试时的优化问题。此外，我们提出了一种可学习的测试时优化器OpTune，以有效利用反馈。实验结果表明，FTTT和OpTune在两个大型语言模型和四个推理数据集上实现了更好的可扩展性和性能。', 'title_zh': '在测试时从反馈中学习推理'}
{'arxiv_id': 'arXiv:2502.15770', 'title': 'Performance Review on LLM for solving leetcode problems', 'authors': 'Lun Wang, Chuanqi Shi, Shaoshui Du, Yiyi Tao, Yixian Shen, Hang Zheng, Xinyu Qiu', 'link': 'https://arxiv.org/abs/2502.15770', 'abstract': 'This paper presents a comprehensive performance evaluation of Large Language Models (LLMs) in solving programming challenges from Leetcode, a widely used platform for algorithm practice and technical interviews. We began by crawling the Leetcode website to collect a diverse set of problems encompassing various difficulty levels and topics. Using this dataset, we generated solutions with multiple LLMs, including GPT-4 and GPT-3.5-turbo (ChatGPT-turbo). The generated solutions were systematically evaluated for correctness and efficiency. We employed the pass@k metric to assess the success rates within a given number of attempts and analyzed the runtime performance of the solutions. Our results highlight the strengths and limitations of current LLMs [10] in code generation and problem-solving tasks, providing insights into their potential applications and areas for improvement in automated programming assistance.', 'abstract_zh': '这篇论文对大型语言模型（LLMs）在解决来自 Leetcode 的编程挑战中的表现进行了全面评估，Leetcode 是一个广泛用于算法练习和技术面试的平台。我们首先抓取 Leetcode 网站以收集涵盖不同难度级别和主题的多样问题集。使用该数据集，我们生成了多个 LLM 的解决方案，包括 GPT-4、GPT-3.5-turbo（ChatGPT-turbo）。生成的解决方案被系统地评估了正确性和效率。我们使用 pass@k 指标评估了一定次数尝试内的成功率，并分析了解决方案的运行时性能。我们的结果突显了当前 LLMs 在代码生成和解决问题任务中的优点和局限性，提供了其在自动化编程辅助中的潜在应用和改进领域的见解。', 'title_zh': 'LLM在解决LeetCode问题上的性能评估'}
{'arxiv_id': 'arXiv:2502.15763', 'title': 'Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization', 'authors': 'Bowen Pang, Kai Li, Ruifeng She, Feifan Wang', 'link': 'https://arxiv.org/abs/2502.15763', 'abstract': 'With the development of large language models (LLMs), it has become increasingly important to optimize hardware usage and improve throughput. In this paper, we study the inference optimization of the serving system that deploys LLMs. To optimize system throughput and maximize hardware utilization, we formulate the inference optimization problem as a mixed-integer programming (MIP) model and propose a hybrid offline-online method as solution. The offline method improves large-scale inference systems by introducing a Minimizing Makespan Bin Packing Problem. We further provide a theoretical lower bound computation method. Then, we propose an online sorting and preemptive scheduling method to better utilize hardware. In the online iteration scheduling process, a Lagrangian method is applied to evaluate the cost efficiency of inserting prefill stages versus decode stages at each iteration and dynamically determine when to preempt decoding tasks and insert prefill tasks. Experiments using real-world data from the LLaMA-65B model and the GSM8K dataset demonstrate that system utilization improves from 80.2% to 89.1%, and the total inference time decreases from 201.00 to 190.58 seconds. A 100-cases study shows that our method consistently outperforms the baseline method and improves the utilization rate by 8.0% on average. Finally, we discuss potential future extensions, including stochastic modeling, reinforcement learning-based schedulers, and dynamic decision-making strategies for system throughput and hardware utilization.', 'abstract_zh': '随着大型语言模型（LLMs）的发展，优化硬件使用和提高吞吐量变得日益重要。本文研究了部署LLMs的服务系统的推理优化。为了优化系统吞吐量并最大化硬件利用率，我们将推理优化问题形式化为混合整数规划（MIP）模型，并提出了一种混合离线-在线方法进行求解。离线方法通过引入最小化工期的物品装箱问题来改进大规模推理系统。我们进一步提供了理论下界计算方法。然后，我们提出了一种在线排序和前瞻调度方法，以更好地利用硬件。在在线迭代调度过程中，应用拉格朗日方法来评估在每次迭代中插入预填充阶段与解码阶段的成本效率，并动态决定何时中断解码任务并插入预填充任务。使用LLaMA-65B模型和GSM8K数据集的实际数据进行的实验表明，系统利用率从80.2%提高到89.1%，总推理时间从201.00秒减少到190.58秒。100案例研究显示，我们的方法在所有情况下都优于基线方法，平均提高利用率8.0%。最后，我们讨论了潜在的未来扩展，包括随机建模、基于强化学习的调度器以及系统吞吐量和硬件利用率的动态决策策略。', 'title_zh': '大型语言模型推理优化的混合离线-在线调度方法'}
{'arxiv_id': 'arXiv:2502.15761', 'title': 'LoXR: Performance Evaluation of Locally Executing LLMs on XR Devices', 'authors': 'Dawar Khan, Xinyu Liu, Omar Mena, Donggang Jia, Alexandre Kouyoumdjian, Ivan Viola', 'link': 'https://arxiv.org/abs/2502.15761', 'abstract': 'The deployment of large language models (LLMs) on extended reality (XR) devices has great potential to advance the field of human-AI interaction. In the case of direct, on-device model inference, selecting the appropriate model and device for specific tasks remains challenging. In this paper, we deploy 17 LLMs across four XR devices--Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision Pro, and conduct a comprehensive evaluation. We devise an experimental setup and evaluate performance on four key metrics: performance consistency, processing speed, memory usage, and battery consumption. For each of the 68 model-device pairs, we assess performance under varying string lengths, batch sizes, and thread counts, analyzing the trade-offs for real-time XR applications. We finally propose a unified evaluation method based on the Pareto Optimality theory to select the optimal device-model pairs from the quality and speed objectives. We believe our findings offer valuable insights to guide future optimization efforts for LLM deployment on XR devices. Our evaluation method can be followed as standard groundwork for further research and development in this emerging field. All supplemental materials are available at this http URL.', 'abstract_zh': '将大语言模型（LLMs）部署在扩展现实（XR）设备上在人机交互领域具有巨大潜力。针对直接在设备上进行模型推理的情况下，选择适合特定任务的模型和设备仍然具有挑战性。在本文中，我们将在Magic Leap 2、Meta Quest 3、Vivo X100s Pro和Apple Vision Pro四款XR设备上部署17个LLM，并进行综合评估。我们设计了实验框架，并从性能一致性、处理速度、内存使用和电池消耗四个关键指标进行评估。对于每对68种模型-设备组合，我们在不同的字符串长度、批次大小和线程数量下评估其性能，分析实时XR应用中的权衡。最后，我们基于Pareto最优理论提出了一种统一的评估方法，用于从质量和速度目标中选择最佳的设备-模型组合。我们认为，我们的研究结果为未来优化XR设备上LLM部署的努力提供了有价值的见解。我们的评估方法可作为进一步研究和发展的标准基础。所有补充材料可在此链接访问。', 'title_zh': 'LoXR：在XR设备上本地执行LLMs的性能评估'}
{'arxiv_id': 'arXiv:2502.15754', 'title': 'Text2Net: Transforming Plain-text To A Dynamic Interactive Network Simulation Environment', 'authors': 'Alireza Marefat, Abbaas Alif Mohamed Nishar, Ashwin Ashok', 'link': 'https://arxiv.org/abs/2502.15754', 'abstract': "This paper introduces Text2Net, an innovative text-based network simulation engine that leverages natural language processing (NLP) and large language models (LLMs) to transform plain-text descriptions of network topologies into dynamic, interactive simulations. Text2Net simplifies the process of configuring network simulations, eliminating the need for users to master vendor-specific syntaxes or navigate complex graphical interfaces. Through qualitative and quantitative evaluations, we demonstrate Text2Net's ability to significantly reduce the time and effort required to deploy network scenarios compared to traditional simulators like EVE-NG. By automating repetitive tasks and enabling intuitive interaction, Text2Net enhances accessibility for students, educators, and professionals. The system facilitates hands-on learning experiences for students that bridge the gap between theoretical knowledge and practical application. The results showcase its scalability across various network complexities, marking a significant step toward revolutionizing network education and professional use cases, such as proof-of-concept testing.", 'abstract_zh': 'Text2Net：一种基于文本的网络模拟引擎', 'title_zh': 'Text2Net: 将普通文本转换为动态交互网络仿真环境'}
{'arxiv_id': 'arXiv:2502.15740', 'title': 'Detection of LLM-Generated Java Code Using Discretized Nested Bigrams', 'authors': 'Timothy Paek, Chilukuri Mohan', 'link': 'https://arxiv.org/abs/2502.15740', 'abstract': 'Large Language Models (LLMs) are currently used extensively to generate code by professionals and students, motivating the development of tools to detect LLM-generated code for applications such as academic integrity and cybersecurity. We address this authorship attribution problem as a binary classification task along with feature identification and extraction. We propose new Discretized Nested Bigram Frequency features on source code groups of various sizes. Compared to prior work, improvements are obtained by representing sparse information in dense membership bins. Experimental evaluation demonstrated that our approach significantly outperformed a commonly used GPT code-detection API and baseline features, with accuracy exceeding 96% compared to 72% and 79% respectively in detecting GPT-rewritten Java code fragments for 976 files with GPT 3.5 and GPT4 using 12 features. We also outperformed three prior works on code author identification in a 40-author dataset. Our approach scales well to larger data sets, and we achieved 99% accuracy and 0.999 AUC for 76,089 files and over 1,000 authors with GPT 4o using 227 features.', 'abstract_zh': '大规模语言模型（LLMs）生成的代码检测：一种二分类特征识别与提取方法', 'title_zh': '基于离散嵌套双聚类的LLM生成Java代码检测'}
{'arxiv_id': 'arXiv:2502.15734', 'title': 'Cache-Craft: Managing Chunk-Caches for Efficient Retrieval-Augmented Generation', 'authors': 'Shubham Agarwal, Sai Sundaresan, Subrata Mitra, Debabrata Mahapatra, Archit Gupta, Rounak Sharma, Nirmal Joshua Kapu, Tong Yu, Shiv Saini', 'link': 'https://arxiv.org/abs/2502.15734', 'abstract': 'Retrieval-Augmented Generation (RAG) is often used with Large Language Models (LLMs) to infuse domain knowledge or user-specific information. In RAG, given a user query, a retriever extracts chunks of relevant text from a knowledge base. These chunks are sent to an LLM as part of the input prompt. Typically, any given chunk is repeatedly retrieved across user questions. However, currently, for every question, attention-layers in LLMs fully compute the key values (KVs) repeatedly for the input chunks, as state-of-the-art methods cannot reuse KV-caches when chunks appear at arbitrary locations with arbitrary contexts. Naive reuse leads to output quality degradation. This leads to potentially redundant computations on expensive GPUs and increases latency. In this work, we propose Cache-Craft, a system for managing and reusing precomputed KVs corresponding to the text chunks (we call chunk-caches) in RAG-based systems. We present how to identify chunk-caches that are reusable, how to efficiently perform a small fraction of recomputation to fix the cache to maintain output quality, and how to efficiently store and evict chunk-caches in the hardware for maximizing reuse while masking any overheads. With real production workloads as well as synthetic datasets, we show that Cache-Craft reduces redundant computation by 51% over SOTA prefix-caching and 75% over full recomputation. Additionally, with continuous batching on a real production workload, we get a 1.6X speed up in throughput and a 2X reduction in end-to-end response latency over prefix-caching while maintaining quality, for both the LLaMA-3-8B and LLaMA-3-70B models.', 'abstract_zh': '基于检索增强生成（RAG）系统的缓存管理与复用算法（Cache-Craft）', 'title_zh': 'Cache-Craft: 管理块缓存以实现高效的检索增强生成'}
{'arxiv_id': 'arXiv:2502.15732', 'title': 'Data Wrangling Task Automation Using Code-Generating Language Models', 'authors': 'Ashlesha Akella, Krishnasuri Narayanam', 'link': 'https://arxiv.org/abs/2502.15732', 'abstract': 'Ensuring data quality in large tabular datasets is a critical challenge, typically addressed through data wrangling tasks. Traditional statistical methods, though efficient, cannot often understand the semantic context and deep learning approaches are resource-intensive, requiring task and dataset-specific training. To overcome these shortcomings, we present an automated system that utilizes large language models to generate executable code for tasks like missing value imputation, error detection, and error correction. Our system aims to identify inherent patterns in the data while leveraging external knowledge, effectively addressing both memory-dependent and memory-independent tasks.', 'abstract_zh': '确保大型表格数据集的数据质量是一个关键挑战，通常通过数据整理任务来解决。传统统计方法虽然高效，但往往无法理解语义上下文，而深度学习方法则资源密集，需要针对特定任务和数据集进行训练。为克服这些不足，我们提出了一种自动化系统，利用大型语言模型生成用于处理如缺失值填充、错误检测和错误纠正等任务的可执行代码。该系统旨在利用内部和外部知识识别数据中的固有模式，有效解决既依赖内存又不依赖内存的任务。', 'title_zh': '使用代码生成语言模型的数据清洗任务自动化'}
{'arxiv_id': 'arXiv:2502.15727', 'title': 'Retrieval Augmented Generation Based LLM Evaluation For Protocol State Machine Inference With Chain-of-Thought Reasoning', 'authors': 'Youssef Maklad, Fares Wael, Wael Elsersy, Ali Hamdi', 'link': 'https://arxiv.org/abs/2502.15727', 'abstract': "This paper presents a novel approach to evaluate the efficiency of a RAG-based agentic Large Language Model (LLM) architecture in network packet seed generation for network protocol fuzzing. Enhanced by chain-of-thought (COT) prompting techniques, the proposed approach focuses on the improvement of the seeds structural quality in order to guide protocol fuzzing frameworks through a wide exploration of the protocol state space. Our method leverages RAG and text embeddings in a two-stages. In the first stage, the agent dynamically refers to the Request For Comments (RFC) documents knowledge base for answering queries regarding the protocol Finite State Machine (FSM), then it iteratively reasons through the retrieved knowledge, for output refinement and proper seed placement. In the second stage, we evaluate the response structure quality of the agent's output, based on metrics as BLEU, ROUGE, and Word Error Rate (WER) by comparing the generated packets against the ground truth packets. Our experiments demonstrate significant improvements of up to 18.19%, 14.81%, and 23.45% in BLEU, ROUGE, and WER, respectively, over baseline models. These results confirm the potential of such approach, improving LLM-based protocol fuzzing frameworks for the identification of hidden vulnerabilities.", 'abstract_zh': '一种基于RAG的代理大型语言模型架构在网络包种子生成中的效率评估方法：基于链式思考促进协议 fuzzing 的宽泛探索', 'title_zh': '基于检索增强生成的大语言模型评估：带有链式思考推理的协议状态机推断'}
{'arxiv_id': 'arXiv:2502.15724', 'title': 'Instruction-Based Fine-tuning of Open-Source LLMs for Predicting Customer Purchase Behaviors', 'authors': 'Halil Ibrahim Ergul, Selim Balcisoy, Burcin Bozkaya', 'link': 'https://arxiv.org/abs/2502.15724', 'abstract': "In this study, the performance of various predictive models, including probabilistic baseline, CNN, LSTM, and finetuned LLMs, in forecasting merchant categories from financial transaction data have been evaluated. Utilizing datasets from Bank A for training and Bank B for testing, the superior predictive capabilities of the fine-tuned Mistral Instruct model, which was trained using customer data converted into natural language format have been demonstrated. The methodology of this study involves instruction fine-tuning Mistral via LoRA (LowRank Adaptation of Large Language Models) to adapt its vast pre-trained knowledge to the specific domain of financial transactions. The Mistral model significantly outperforms traditional sequential models, achieving higher F1 scores in the three key merchant categories of bank transaction data (grocery, clothing, and gas stations) that is crucial for targeted marketing campaigns. This performance is attributed to the model's enhanced semantic understanding and adaptability which enables it to better manage minority classes and predict transaction categories with greater accuracy. These findings highlight the potential of LLMs in predicting human behavior.", 'abstract_zh': '本研究评估了包括概率基准模型、CNN、LSTM和微调的大语言模型（LLM）在金融交易数据预测商户类别方面的性能。利用来自银行A的训练集和银行B的测试集，展示了通过将客户数据转换为自然语言格式进行微调的Mistral Instruct模型的卓越预测能力。本研究的方法是通过LoRA（大型语言模型的低秩适应）对Mistral进行指令微调，使其庞大的预训练知识适应金融交易的特定领域。Mistral模型显著优于传统的顺序模型，在银行交易数据（食品杂货、服装和加油站）的三个关键商户类别上实现了更高的F1分数，这对于有针对性的营销活动至关重要。这一性能归因于模型增强的语义理解和适应性，使其能够更好地处理少数类别，更准确地预测交易类别。这些发现突显了大语言模型在预测人类行为方面的潜力。', 'title_zh': '基于指令的开源大语言模型细调以预测客户购买行为'}
{'arxiv_id': 'arXiv:2502.15723', 'title': 'Balancing Content Size in RAG-Text2SQL System', 'authors': 'Prakhar Gurawa, Anjali Dharmik', 'link': 'https://arxiv.org/abs/2502.15723', 'abstract': 'Large Language Models (LLMs) have emerged as a promising solution for converting natural language queries into SQL commands, enabling seamless database interaction. However, these Text-to-SQL (Text2SQL) systems face inherent limitations, hallucinations, outdated knowledge, and untraceable reasoning. To address these challenges, the integration of retrieval-augmented generation (RAG) with Text2SQL models has gained traction. RAG serves as a retrieval mechanism, providing essential contextual information, such as table schemas and metadata, to enhance the query generation process. Despite their potential, RAG + Text2SQL systems are susceptible to the quality and size of retrieved documents. While richer document content can improve schema relevance and retrieval accuracy, it also introduces noise, increasing the risk of hallucinations and reducing query fidelity as the prompt size of the Text2SQL model increases. This research investigates the nuanced trade-off between document size and quality, aiming to strike a balance that optimizes system performance. Key thresholds are identified where performance degradation occurs, along with actionable strategies to mitigate these challenges. Additionally, we explore the phenomenon of hallucinations in Text2SQL models, emphasizing the critical role of curated document presentation in minimizing errors. Our findings provide a roadmap for enhancing the robustness of RAG + Text2SQL systems, offering practical insights for real-world applications.', 'abstract_zh': '大型语言模型（LLMs）作为一种将自然语言查询转换为SQL命令的有前途的解决方案，促进了数据库交互。然而，这些Text-to-SQL（Text2SQL）系统面临着固有的限制，包括幻觉、过时的知识和不可追溯的推理。为了解决这些挑战，将检索增强生成（RAG）与Text2SQL模型相结合的方法越来越多地受到关注。RAG充当检索机制，提供诸如表结构和元数据等关键上下文信息，以增强查询生成过程。尽管更丰富的文档内容可以提高模式相关性和检索准确性，但这也引入了噪声，增加了幻觉的风险，并随着Text2SQL模型提示大小增加而降低查询准确性。本研究探讨了文档大小和质量之间的微妙权衡，旨在找到优化系统性能的最佳平衡点。我们确定了性能降级的关键阈值，并提出了应对这些挑战的操作性策略。此外，我们还探讨了Text2SQL模型中的幻觉现象，强调精心策划的文档呈现对减少错误的至关重要性。我们的研究结果为增强RAG + Text2SQL系统的稳健性提供了蓝图，提供了实用的见解，适用于实际应用。', 'title_zh': '平衡RAG-Text2SQL系统中的内容大小'}
{'arxiv_id': 'arXiv:2502.15709', 'title': 'TutorLLM: Customizing Learning Recommendations with Knowledge Tracing and Retrieval-Augmented Generation', 'authors': 'Zhaoxing Li, Vahid Yazdanpanah, Jindi Wang, Wen Gu, Lei Shi, Alexandra I. Cristea, Sarah Kiden, Sebastian Stein', 'link': 'https://arxiv.org/abs/2502.15709', 'abstract': "The integration of AI in education offers significant potential to enhance learning efficiency. Large Language Models (LLMs), such as ChatGPT, Gemini, and Llama, allow students to query a wide range of topics, providing unprecedented flexibility. However, LLMs face challenges, such as handling varying content relevance and lack of personalization. To address these challenges, we propose TutorLLM, a personalized learning recommender LLM system based on Knowledge Tracing (KT) and Retrieval-Augmented Generation (RAG). The novelty of TutorLLM lies in its unique combination of KT and RAG techniques with LLMs, which enables dynamic retrieval of context-specific knowledge and provides personalized learning recommendations based on the student's personal learning state. Specifically, this integration allows TutorLLM to tailor responses based on individual learning states predicted by the Multi-Features with Latent Relations BERT-based KT (MLFBK) model and to enhance response accuracy with a Scraper model. The evaluation includes user assessment questionnaires and performance metrics, demonstrating a 10\\% improvement in user satisfaction and a 5\\% increase in quiz scores compared to using general LLMs alone.", 'abstract_zh': 'AI在教育中的集成提供了显著潜力以提升学习效率。基于Knowledge Tracing和Retrieval-Augmented Generation的个性化学习推荐系统TutorLLM克服了大型语言模型的挑战。', 'title_zh': 'TutorLLM: 基于知识追踪和检索增强生成的个性化学习推荐'}
{'arxiv_id': 'arXiv:2502.15702', 'title': 'Large language models streamline automated systematic review: A preliminary study', 'authors': 'Xi Chen, Xue Zhang', 'link': 'https://arxiv.org/abs/2502.15702', 'abstract': 'Large Language Models (LLMs) have shown promise in natural language processing tasks, with the potential to automate systematic reviews. This study evaluates the performance of three state-of-the-art LLMs in conducting systematic review tasks. We assessed GPT-4, Claude-3, and Mistral 8x7B across four systematic review tasks: study design formulation, search strategy development, literature screening, and data extraction. Sourced from a previously published systematic review, we provided reference standard including standard PICO (Population, Intervention, Comparison, Outcome) design, standard eligibility criteria, and data from 20 reference literature. Three investigators evaluated the quality of study design and eligibility criteria using 5-point Liker Scale in terms of accuracy, integrity, relevance, consistency and overall performance. For other tasks, the output is defined as accurate if it is the same as the reference standard. Search strategy performance was evaluated through accuracy and retrieval efficacy. Screening accuracy was assessed for both abstracts screening and full texts screening. Data extraction accuracy was evaluated across 1,120 data points comprising 3,360 individual fields. Claude-3 demonstrated superior overall performance in PICO design. In search strategy formulation, GPT-4 and Claude-3 achieved comparable accuracy, outperforming Mistral. For abstract screening, GPT-4 achieved the highest accuracy, followed by Mistral and Claude-3. In data extraction, GPT-4 significantly outperformed other models. LLMs demonstrate potential for automating systematic review tasks, with GPT-4 showing superior performance in search strategy formulation, literature screening and data extraction. These capabilities make them promising assistive tools for researchers and warrant further development and validation in this field.', 'abstract_zh': '大型语言模型在系统评价任务中的性能评估：GPT-4、Claude-3和Mistral 8x7B的表现分析', 'title_zh': '大型语言模型简化自动化系统评价：一项初步研究'}
{'arxiv_id': 'arXiv:2502.15701', 'title': 'Political Events using RAG with LLMs', 'authors': 'Muhammad Arslan, Saba Munawar, Christophe Cruz', 'link': 'https://arxiv.org/abs/2502.15701', 'abstract': "In the contemporary digital landscape, media content stands as the foundation for political news analysis, offering invaluable insights sourced from various channels like news articles, social media updates, speeches, and reports. Natural Language Processing (NLP) has revolutionized Political Information Extraction (IE), automating tasks such as Event Extraction (EE) from these diverse media outlets. While traditional NLP methods often necessitate specialized expertise to build rule-based systems or train machine learning models with domain-specific datasets, the emergence of Large Language Models (LLMs) driven by Generative Artificial Intelligence (GenAI) presents a promising alternative. These models offer accessibility, alleviating challenges associated with model construction from scratch and reducing the dependency on extensive datasets during the training phase, thus facilitating rapid implementation. However, challenges persist in handling domain-specific tasks, leading to the development of the Retrieval-Augmented Generation (RAG) framework. RAG enhances LLMs by integrating external data retrieval, enriching their contextual understanding, and expanding their knowledge base beyond pre-existing training data. To illustrate RAG's efficacy, we introduce the Political EE system, specifically tailored to extract political event information from news articles. Understanding these political insights is essential for remaining informed about the latest political advancements, whether on a national or global scale.", 'abstract_zh': '当代数字景观中，媒体内容构成了政治新闻分析的基础，提供了来自新闻文章、社交媒体更新、演讲和报告等多种渠道的宝贵见解。自然语言处理（NLP）已经革新了政治信息提取（IE），实现了从这些多元媒体渠道自动抽取事件（EE）等功能。虽然传统的NLP方法通常需要专门的专家来构建基于规则的系统或使用特定领域的数据集训练机器学习模型，但由生成式人工智能（GenAI）驱动的大规模语言模型（LLMs）提供了有前景的替代方案。这些模型提高了可访问性，减轻了从头构建模型的挑战，并在训练阶段减少了对大量数据集的依赖，从而促进了快速实施。然而，在处理特定领域的任务时仍存在挑战，因此开发了检索增强生成（RAG）框架。RAG通过集成外部数据检索来增强大规模语言模型，丰富其背景理解，并扩展其知识库，超出其原有训练数据。为了展示RAG的有效性，我们介绍了专门用于从新闻文章中提取政治事件信息的政治EE系统。理解这些政治见解对于了解最新的政治进展（无论是全国性的还是全球性的）至关重要。', 'title_zh': '利用LLMs的RAG进行政治事件处理'}
{'arxiv_id': 'arXiv:2502.15700', 'title': 'Sustainable Digitalization of Business with Multi-Agent RAG and LLM', 'authors': 'Muhammad Arslan, Saba Munawar, Christophe Cruz', 'link': 'https://arxiv.org/abs/2502.15700', 'abstract': "Businesses heavily rely on data sourced from various channels like news articles, financial reports, and consumer reviews to drive their operations, enabling informed decision-making and identifying opportunities. However, traditional manual methods for data extraction are often time-consuming and resource-intensive, prompting the adoption of digital transformation initiatives to enhance efficiency. Yet, concerns persist regarding the sustainability of such initiatives and their alignment with the United Nations (UN)'s Sustainable Development Goals (SDGs). This research aims to explore the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) as a sustainable solution for Information Extraction (IE) and processing. The research methodology involves reviewing existing solutions for business decision-making, noting that many systems require training new machine learning models, which are resource-intensive and have significant environmental impacts. Instead, we propose a sustainable business solution using pre-existing LLMs that can work with diverse datasets. We link domain-specific datasets to tailor LLMs to company needs and employ a Multi-Agent architecture to divide tasks such as information retrieval, enrichment, and classification among specialized agents. This approach optimizes the extraction process and improves overall efficiency. Through the utilization of these technologies, businesses can optimize resource utilization, improve decision-making processes, and contribute to sustainable development goals, thereby fostering environmental responsibility within the corporate sector.", 'abstract_zh': '利用大型语言模型与检索增强生成技术实现可持续信息提取的研究', 'title_zh': '基于多代理RAG和LLM的可持续数字化商业'}
{'arxiv_id': 'arXiv:2502.15696', 'title': 'Integrating Domain Knowledge into Large Language Models for Enhanced Fashion Recommendations', 'authors': 'Zhan Shi, Shanglin Yang', 'link': 'https://arxiv.org/abs/2502.15696', 'abstract': 'Fashion, deeply rooted in sociocultural dynamics, evolves as individuals emulate styles popularized by influencers and iconic figures. In the quest to replicate such refined tastes using artificial intelligence, traditional fashion ensemble methods have primarily used supervised learning to imitate the decisions of style icons, which falter when faced with distribution shifts, leading to style replication discrepancies triggered by slight variations in input. Meanwhile, large language models (LLMs) have become prominent across various sectors, recognized for their user-friendly interfaces, strong conversational skills, and advanced reasoning capabilities. To address these challenges, we introduce the Fashion Large Language Model (FLLM), which employs auto-prompt generation training strategies to enhance its capacity for delivering personalized fashion advice while retaining essential domain knowledge. Additionally, by integrating a retrieval augmentation technique during inference, the model can better adjust to individual preferences. Our results show that this approach surpasses existing models in accuracy, interpretability, and few-shot learning capabilities.', 'abstract_zh': '时装深深植根于社会文化动态，随着个体模仿影响者和icon流行风格而演变。在利用人工智能复制这种精致品味的探索中，传统时尚ensemble方法主要采用监督学习模仿风格icon的决策，在面对分布变化时表现不佳，导致因输入微小变化引发的风格复制偏差。与此同时，大型语言模型（LLMs）已在各种领域中成为主流，因其用户友好的界面、强大的对话能力和高级推理能力而受到认可。为应对这些挑战，我们引入了时尚大型语言模型（FLLM），该模型采用自动提示生成训练策略，增强了提供个性化时尚建议的能力并保留了关键领域知识。另外，在推理过程中通过集成检索增强技术，该模型能够更好地适应个体偏好。我们的结果显示，这种方法在准确率、可解释性和少-shot学习能力方面超越现有模型。', 'title_zh': '将领域知识融入大型语言模型以增强时尚推荐'}
{'arxiv_id': 'arXiv:2502.15690', 'title': 'Level-Navi Agent: A Framework and benchmark for Chinese Web Search Agents', 'authors': 'Chuanrui Hu, Shichong Xie, Baoxin Wang, Bin Chen, Xiaofeng Cong, Jun Zhang', 'link': 'https://arxiv.org/abs/2502.15690', 'abstract': 'Large language models (LLMs), adopted to understand human language, drive the development of artificial intelligence (AI) web search agents. Compared to traditional search engines, LLM-powered AI search agents are capable of understanding and responding to complex queries with greater depth, enabling more accurate operations and better context recognition. However, little attention and effort has been paid to the Chinese web search, which results in that the capabilities of open-source models have not been uniformly and fairly evaluated. The difficulty lies in lacking three aspects: an unified agent framework, an accurately labeled dataset, and a suitable evaluation metric. To address these issues, we propose a general-purpose and training-free web search agent by level-aware navigation, Level-Navi Agent, accompanied by a well-annotated dataset (Web24) and a suitable evaluation metric. Level-Navi Agent can think through complex user questions and conduct searches across various levels on the internet to gather information for questions. Meanwhile, we provide a comprehensive evaluation of state-of-the-art LLMs under fair settings. To further facilitate future research, source code is available at Github.', 'abstract_zh': '大型语言模型（LLMs）用于理解人类语言，推动了人工智能网络搜索代理的发展。与传统搜索引擎相比，以LLM为动力的AI搜索代理能够以更深的理解和响应复杂查询，从而实现更准确的操作和更好的上下文识别。然而，中文网络搜索的关注度和努力相对较小，导致开源模型的能力缺乏统一和公平的评估。这一问题源于三个方面：缺乏统一的代理框架、准确标记的数据集和合适的评估指标。为解决这些问题，我们提出了一种基于层级感知导航的一般用途且无需训练的网络搜索代理——Level-Navi Agent，并提供了一个详注的数据库（Web24）和合适的评估指标。Level-Navi Agent能够通过跨层级的搜索来理解和回答复杂的用户问题，同时搜集信息。此外，我们还在公平的设置下全面评估了最新的LLM。为促进未来的研究，相关源代码可在Github上获取。', 'title_zh': 'Level-Navi 代理: 一种中文网页搜索代理框架及基准'}
{'arxiv_id': 'arXiv:2502.15688', 'title': 'XPath Agent: An Efficient XPath Programming Agent Based on LLM for Web Crawler', 'authors': 'Yu Li, Bryce Wang, Xinyu Luan', 'link': 'https://arxiv.org/abs/2502.15688', 'abstract': 'We present XPath Agent, a production-ready XPath programming agent specifically designed for web crawling and web GUI testing. A key feature of XPath Agent is its ability to automatically generate XPath queries from a set of sampled web pages using a single natural language query. To demonstrate its effectiveness, we benchmark XPath Agent against a state-of-the-art XPath programming agent across a range of web crawling tasks. Our results show that XPath Agent achieves comparable performance metrics while significantly reducing token usage and improving clock-time efficiency. The well-designed two-stage pipeline allows for seamless integration into existing web crawling or web GUI testing workflows, thereby saving time and effort in manual XPath query development. The source code for XPath Agent is available at this https URL.', 'abstract_zh': 'XPath Agent：一种用于网络爬虫和Web GUI测试的生产级XPath编程代理', 'title_zh': 'XPath代理：基于大语言模型的高效XPath编程代理用于Web爬虫'}
{'arxiv_id': 'arXiv:2502.15684', 'title': 'An Agent Framework for Real-Time Financial Information Searching with Large Language Models', 'authors': 'Jinzheng Li, Jingshu Zhang, Hongguang Li, Yiqing Shen', 'link': 'https://arxiv.org/abs/2502.15684', 'abstract': "Financial decision-making requires processing vast amounts of real-time information while understanding their complex temporal relationships. While traditional search engines excel at providing real-time information access, they often struggle to comprehend sophisticated user intentions and contextual nuances. Conversely, Large Language Models (LLMs) demonstrate reasoning and interaction capabilities but may generate unreliable outputs without access to current data. While recent attempts have been made to combine LLMs with search capabilities, they suffer from (1) restricted access to specialized financial data, (2) static query structures that cannot adapt to dynamic market conditions, and (3) insufficient temporal awareness in result generation. To address these challenges, we present FinSearch, a novel agent-based search framework specifically designed for financial applications that interface with diverse financial data sources including market, stock, and news data. Innovatively, FinSearch comprises four components: (1) an LLM-based multi-step search pre-planner that decomposes user queries into structured sub-queries mapped to specific data sources through a graph representation; (2) a search executor with an LLM-based adaptive query rewriter that executes the searching of each sub-query while dynamically refining the sub-queries in its subsequent node based on intermediate search results; (3) a temporal weighting mechanism that prioritizes information relevance based on the deduced time context from the user's query; (4) an LLM-based response generator that synthesizes results into coherent, contextually appropriate outputs. To evaluate FinSearch, we construct FinSearchBench-24, a benchmark of 1,500 four-choice questions across the stock market, rate changes, monetary policy, and industry developments spanning from June to October 2024.", 'abstract_zh': '金融决策需要处理大量的实时信息并理解其复杂的时序关系。传统搜索引擎在提供实时信息访问方面表现出色，但往往难以理解复杂用户意图和背景细微差异。相反，大型语言模型（LLMs）展示了推理和交互能力，但在没有访问当前数据的情况下可能会生成不可靠的输出。虽然最近尝试将LLMs与搜索功能结合，但这些尝试面临以下挑战：（1）受限于特定金融数据的访问；（2）静态查询结构无法适应动态市场状况；（3）结果生成中缺乏足够的时序意识。为解决这些挑战，我们提出了一种名为FinSearch的新型基于代理的搜索框架，专门针对金融应用，能够接入包括市场、股票和新闻数据在内的多种金融数据来源。创新地，FinSearch包含四个组件：（1）基于LLM的多步搜索预规划器，将用户查询分解为结构化的子查询并通过图表示映射到具体的数据源；（2）具有LLM基础的自适应查询重写器，该搜索执行器执行每个子查询的搜索，同时根据中间搜索结果动态优化后续节点中的子查询；（3）时序加权机制，基于用户查询推断出的时间上下文来优先考虑信息的相关性；（4）基于LLM的响应生成器，将结果合成为符合上下文的连贯输出。为了评估FinSearch，我们构建了FinSearchBench-24基准测试，涵盖从2024年6月到10月的股票市场、利率变化、货币政策和行业动态等领域，共1,500个四选一问题。', 'title_zh': '基于大型语言模型的实时金融信息搜索代理框架'}
