{'arxiv_id': 'arXiv:2509.10139', 'title': 'CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion', 'authors': 'Santiago Montiel-Marín, Angel Llamazares, Miguel Antunes-García, Fabio Sánchez-García, Luis M. Bergasa', 'link': 'https://arxiv.org/abs/2509.10139', 'abstract': 'Camera-radar fusion offers a robust and cost-effective alternative to LiDAR-based autonomous driving systems by combining complementary sensing capabilities: cameras provide rich semantic cues but unreliable depth, while radar delivers sparse yet reliable position and motion information. We introduce CaR1, a novel camera-radar fusion architecture for BEV vehicle segmentation. Built upon BEVFusion, our approach incorporates a grid-wise radar encoding that discretizes point clouds into structured BEV features and an adaptive fusion mechanism that dynamically balances sensor contributions. Experiments on nuScenes demonstrate competitive segmentation performance (57.6 IoU), on par with state-of-the-art methods. Code is publicly available \\href{this https URL}{online}.', 'abstract_zh': '相机-雷达融合提供了一种基于LiDAR的自动驾驶系统稳健且经济高效的替代方案，通过结合互补的感知能力：相机提供丰富的语义线索但深度不可靠，而雷达提供稀疏但可靠的定位和运动信息。我们介绍了CaR1，一种新型的相机-雷达融合架构，用于BEV车辆分割。我们的方法基于BEVFusion，其中包括一种格网级雷达编码，将点云离散化为结构化的BEV特征，以及一种自适应融合机制，动态平衡传感器的贡献。在nuScenes上的实验展示了具有竞争力的分割性能（57.6 IoU），与最先进的方法相当。代码已公开可在线获取。', 'title_zh': 'CaR1：基于摄像头-雷达融合的多模态BEV车辆分割基线'}
{'arxiv_id': 'arXiv:2509.09747', 'title': 'D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference', 'authors': 'Leen Daher, Zhaobo Wang, Malcolm Mielle', 'link': 'https://arxiv.org/abs/2509.09747', 'abstract': "Cross-modal transfer learning is used to improve multi-modal classification models (e.g., for human activity recognition in human-robot collaboration). However, existing methods require paired sensor data at both training and inference, limiting deployment in resource-constrained environments where full sensor suites are not economically and technically usable. To address this, we propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns modality-specific representations without requiring joint sensor modality during inference. Our approach combines a self-attention module for feature extraction with a novel cross-attention alignment loss, which enforces the alignment of sensors' feature spaces without requiring the coupling of the classification pipelines of both modalities. We evaluate D-CAT on three multi-modal human activity datasets (IMU, video, and audio) under both in-distribution and out-of-distribution scenarios, comparing against uni-modal models. Results show that in in-distribution scenarios, transferring from high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains over uni-modal training. In out-of-distribution scenarios, even weaker source modalities (e.g., IMU to video) improve target performance, as long as the target model isn't overfitted on the training data. By enabling single-sensor inference with cross-modal knowledge, D-CAT reduces hardware redundancy for perception systems while maintaining accuracy, which is critical for cost-sensitive or adaptive deployments (e.g., assistive robots in homes with variable sensor availability). Code is available at this https URL.", 'abstract_zh': '跨模态脱耦转移学习用于提高多模态分类模型（例如，在人机协作中的人类活动识别）', 'title_zh': 'D-CAT：传感器模态间的解耦跨注意力转移用于单模推断'}
{'arxiv_id': 'arXiv:2509.09794', 'title': 'A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes', 'authors': 'Jackson Eshbaugh, Chetan Tiwari, Jorge Silveyra', 'link': 'https://arxiv.org/abs/2509.09794', 'abstract': "Computational models have emerged as powerful tools for energy modeling research, touting scalability and quantitative results. However, these models require a plethora of data, some of which is inaccessible, expensive, or raises privacy concerns. We introduce a modular multimodal framework to produce this data from publicly accessible residential information and images using generative artificial intelligence (AI). Additionally, we provide a pipeline demonstrating this framework, and we evaluate its generative AI components. Our experiments show that our framework's use of AI avoids common issues with generative models. Our framework produces realistic, labeled data. By reducing dependence on costly or restricted data sources, we pave a path towards more accessible and reproducible research.", 'abstract_zh': '计算模型已成为能源建模研究中的强大工具，具备扩展性和量化结果的优点，但这些模型需要大量的数据，其中一些数据是不可访问、昂贵或涉及隐私问题。我们提出了一种模块化多模态框架，利用生成人工智能（AI）从公开的住宅信息和图像中生成所需数据。此外，我们提供了一条流水线来展示该框架，并评估其生成AI组件。我们的实验表明，我们的框架使用AI避免了生成模型中的常见问题。该框架生成的是真实且带有标签的数据。通过减少对昂贵或受限数据源的依赖，我们为更广泛的可访问性和可重复性研究铺平了道路。', 'title_zh': '一个模块化多模态生成AI框架：生成合成住宅用于城市建筑能源数据'}
{'arxiv_id': 'arXiv:2509.10345', 'title': 'Towards Understanding Visual Grounding in Visual Language Models', 'authors': 'Georgios Pantazopoulos, Eda B. Özyiğit', 'link': 'https://arxiv.org/abs/2509.10345', 'abstract': 'Visual grounding refers to the ability of a model to identify a region within some visual input that matches a textual description. Consequently, a model equipped with visual grounding capabilities can target a wide range of applications in various domains, including referring expression comprehension, answering questions pertinent to fine-grained details in images or videos, caption visual context by explicitly referring to entities, as well as low and high-level control in simulated and real environments. In this survey paper, we review representative works across the key areas of research on modern general-purpose vision language models (VLMs). We first outline the importance of grounding in VLMs, then delineate the core components of the contemporary paradigm for developing grounded models, and examine their practical applications, including benchmarks and evaluation metrics for grounded multimodal generation. We also discuss the multifaceted interrelations among visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally, we analyse the challenges inherent to visual grounding and suggest promising directions for future research.', 'abstract_zh': '视觉 grounding 指的是模型识别视觉输入中与文本描述匹配的区域的能力。因此，具备视觉 grounding 能力的模型可以应用于各个领域的多种应用场景，包括参照表达理解、回答与图像或视频中精细细节相关的问题、通过明确指代实体caption视觉上下文，以及在模拟和实际环境中实现低级和高级控制。在本文综述中，我们回顾了现代通用视觉语言模型（VLMs）关键研究领域的代表性工作。我们首先概述了在 VLMs 中进行 grounding 的重要性，然后阐述了开发 grounded 模型的现代范式的核心组件及其实际应用，包括grounded 多模态生成的基准和评估指标。我们还讨论了视觉 grounding、多模态链式思考和推理在 VLMs 中的多方面关系。最后，我们分析了视觉 grounding 内在的挑战，并提出了未来研究的潜在方向。', 'title_zh': '理解视觉语言模型中的视觉接地'}
{'arxiv_id': 'arXiv:2509.10266', 'title': 'SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion', 'authors': 'Wenfang Wu, Tingting Yuan, Yupeng Li, Daling Wang, Xiaoming Fu', 'link': 'https://arxiv.org/abs/2509.10266', 'abstract': 'Sign language translation (SLT) aims to translate natural language from sign language videos, serving as a vital bridge for inclusive communication. While recent advances leverage powerful visual backbones and large language models, most approaches mainly focus on manual signals (hand gestures) and tend to overlook non-manual cues like mouthing. In fact, mouthing conveys essential linguistic information in sign languages and plays a crucial role in disambiguating visually similar signs. In this paper, we propose SignClip, a novel framework to improve the accuracy of sign language translation. It fuses manual and non-manual cues, specifically spatial gesture and lip movement features. Besides, SignClip introduces a hierarchical contrastive learning framework with multi-level alignment objectives, ensuring semantic consistency across sign-lip and visual-text modalities. Extensive experiments on two benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from 24.32 to 24.71, and ROUGE from 46.57 to 48.38.', 'abstract_zh': '手语翻译（SLT）旨在将自然语言翻译为手语视频，作为包容性沟通的重要桥梁。虽然近期进展利用了强大的视觉骨干和大型语言模型，但大多数方法主要关注手动信号（手势）并倾向于忽略唇部动作等非手动提示。实际上，唇部动作在手语中传达了重要的语言信息，在区分视觉上相似的手语方面起着关键作用。在本文中，我们提出了一种新的框架SignClip，以提高手语翻译的准确性。它融合了手动和非手动提示，特别是空间手势和唇部运动特征。此外，SignClip引入了一种分层对比学习框架，包含多级对齐目标，确保手语-唇部和视觉-文本模态的一致性。在两个基准数据集PHOENIX14T和How2Sign上的广泛实验表明了我们方法的优越性。例如，在PHOENIX14T数据集上，无词形设置下，SignClip超越了之前的最佳模型SpaMo，BLEU-4性能从24.32提高到24.71，ROUGE性能从46.57提高到48.38。', 'title_zh': 'SignClip: 利用口型线索实现多模态对比融合的手语翻译'}
{'arxiv_id': 'arXiv:2509.09730', 'title': 'MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance', 'authors': 'Kaikai Zhao, Zhaoxiang Liu, Peng Wang, Xin Wang, Zhicheng Ma, Yajun Xu, Wenjing Zhang, Yibing Nan, Kai Wang, Shiguo Lian', 'link': 'https://arxiv.org/abs/2509.09730', 'abstract': "General-domain large multimodal models (LMMs) have achieved significant advances in various image-text tasks. However, their performance in the Intelligent Traffic Surveillance (ITS) domain remains limited due to the absence of dedicated multimodal datasets. To address this gap, we introduce MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale multimodal benchmark dataset specifically designed for ITS. MITS includes 170,400 independently collected real-world ITS images sourced from traffic surveillance cameras, annotated with eight main categories and 24 subcategories of ITS-specific objects and events under diverse environmental conditions. Additionally, through a systematic data generation pipeline, we generate high-quality image captions and 5 million instruction-following visual question-answer pairs, addressing five critical ITS tasks: object and event recognition, object counting, object localization, background analysis, and event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream LMMs on this dataset, enabling the development of ITS-specific applications. Experimental results show that MITS significantly improves LMM performance in ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905 (+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to 0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the dataset, code, and models as open-source, providing high-value resources to advance both ITS and LMM research.", 'abstract_zh': '通用领域大型多模态模型在各种图像-文本任务中取得了显著进展。然而，它们在智能交通 surveillance (ITS) 领域的表现仍然受限，因为缺乏专门的多模态数据集。为了填补这一空白，我们引入了 MITS（多模态智能交通 surveillance），首个专门针对 ITS 设计的大规模多模态基准数据集。MITS 包含 170,400 张独立收集的实际 ITS 图像，来源于交通监控摄像头，并在多种环境条件下对八类主要类别和 24 个子类别的 ITS 特定对象和事件进行了标注。此外，通过系统化数据生成管道，我们生成了高质量的图像描述和 500 万条指令跟随的视觉问答对，用于解决五个关键的 ITS 任务：对象和事件识别、对象计数、对象定位、背景分析和事件推理。为了展示 MITS 的有效性，我们在该数据集上微调了主流的多模态模型，促进了面向 ITS 的特定应用开发。实验结果显示，MITS 显著提高了多模态模型在 ITS 应用中的性能，LLaVA-1.5 的性能从 0.494 提高到 0.905 (+83.2%)，LLaVA-1.6 的性能从 0.678 提高到 0.921 (+35.8%)，Qwen2-VL 的性能从 0.584 提高到 0.926 (+58.6%)，Qwen2.5-VL 的性能从 0.732 提高到 0.930 (+27.0%)。我们发布了数据集、代码和模型，作为开源资源，以促进 ITS 和多模态模型研究的进步。', 'title_zh': 'MITS：智能交通监控的大规模多模态基准数据集'}
{'arxiv_id': 'arXiv:2509.09721', 'title': 'A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval', 'authors': 'Jiayi Miao, Dingxin Lu, Zhuqi Wang', 'link': 'https://arxiv.org/abs/2509.09721', 'abstract': 'After natural disasters, accurate evaluations of damage to housing are important for insurance claims response and planning of resources. In this work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG) framework. On top of classical RAG architecture, we further the framework to devise a two-branch multimodal encoder structure that the image branch employs a visual encoder composed of ResNet and Transformer to extract the characteristic of building damage after disaster, and the text branch harnesses a BERT retriever for the text vectorization of posts as well as insurance policies and for the construction of a retrievable restoration index. To impose cross-modal semantic alignment, the model integrates a cross-modal interaction module to bridge the semantic representation between image and text via multi-head attention. Meanwhile, in the generation module, the introduced modal attention gating mechanism dynamically controls the role of visual evidence and text prior information during generation. The entire framework takes end-to-end training, and combines the comparison loss, the retrieval loss and the generation loss to form multi-task optimization objectives, and achieves image understanding and policy matching in collaborative learning. The results demonstrate superior performance in retrieval accuracy and classification index on damage severity, where the Top-1 retrieval accuracy has been improved by 9.6%.', 'abstract_zh': '灾害发生后，房屋损坏评估对于保险理赔响应和资源规划至关重要。本文引入一种新型多模态检索增强生成（MM-RAG）框架。在传统RAG架构基础上，进一步设计了一种两分支多模态编码结构，图像分支采用由ResNet和Transformer组成的视觉编码器提取灾害后建筑损坏特征，文本分支利用BERT检索器对帖子、保险政策进行文本向量化，并构建可检索的修复索引。为了实现跨模态语义对齐，模型整合了一种跨模态交互模块，通过多头注意力机制在图像和文本之间建立语义表示连接。同时，在生成模块中，引入的模态注意力门控机制动态控制生成过程中视觉证据和文本先验信息的角色。整个框架端到端训练，并将比较损失、检索损失和生成损失结合形成多任务优化目标，在协作学习中实现图像理解与政策匹配。实验结果表明，在损坏程度检索准确率和分类指标上表现出优异性能，Top-1检索准确率提高了9.6%。', 'title_zh': '多模态RAG框架在住房损害评估中的应用：图像编码与策略向量检索的协作优化'}
{'arxiv_id': 'arXiv:2509.09683', 'title': 'Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable Outputs', 'authors': 'Briti Gangopadhyay, Zhao Wang, Shingo Takamatsu', 'link': 'https://arxiv.org/abs/2509.09683', 'abstract': 'Forecasting click volume is a key task in digital advertising, influencing both revenue and campaign strategy. Traditional time series models rely solely on numerical data, often overlooking rich contextual information embedded in textual elements, such as keyword updates. We present a multimodal forecasting framework that combines click data with textual logs from real-world ad campaigns and generates human-interpretable explanations alongside numeric predictions. Reinforcement learning is used to improve comprehension of textual information and enhance fusion of modalities. Experiments on a large-scale industry dataset show that our method outperforms baselines in both accuracy and reasoning quality.', 'abstract_zh': '数字广告中点击量预测是一项关键任务，影响收入和campaign策略。传统时间序列模型仅依赖数值数据，往往忽视了嵌入在文本元素中的丰富上下文信息，如关键词更新。我们提出了一种多模态预测框架，结合点击数据和实际广告campaign的文本日志，并生成与数值预测并行的人类可解释的解释。强化学习用于提高对文本信息的理解并增强模态融合。大规模工业数据集上的实验表明，我们的方法在准确性和推理质量上均优于基线方法。', 'title_zh': '数字广告中的点击预测：多模态输入与可解释输出'}
