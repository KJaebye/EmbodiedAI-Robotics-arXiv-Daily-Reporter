{'arxiv_id': 'arXiv:2508.01917', 'title': 'L3M+P: Lifelong Planning with Large Language Models', 'authors': 'Krish Agarwal, Yuqian Jiang, Jiaheng Hu, Bo Liu, Peter Stone', 'link': 'https://arxiv.org/abs/2508.01917', 'abstract': 'By combining classical planning methods with large language models (LLMs), recent research such as LLM+P has enabled agents to plan for general tasks given in natural language. However, scaling these methods to general-purpose service robots remains challenging: (1) classical planning algorithms generally require a detailed and consistent specification of the environment, which is not always readily available; and (2) existing frameworks mainly focus on isolated planning tasks, whereas robots are often meant to serve in long-term continuous deployments, and therefore must maintain a dynamic memory of the environment which can be updated with multi-modal inputs and extracted as planning knowledge for future tasks. To address these two issues, this paper introduces L3M+P (Lifelong LLM+P), a framework that uses an external knowledge graph as a representation of the world state. The graph can be updated from multiple sources of information, including sensory input and natural language interactions with humans. L3M+P enforces rules for the expected format of the absolute world state graph to maintain consistency between graph updates. At planning time, given a natural language description of a task, L3M+P retrieves context from the knowledge graph and generates a problem definition for classical planners. Evaluated on household robot simulators and on a real-world service robot, L3M+P achieves significant improvement over baseline methods both on accurately registering natural language state changes and on correctly generating plans, thanks to the knowledge graph retrieval and verification.', 'abstract_zh': '结合经典规划方法与大规模语言模型以增强通用服务机器人的长期规划能力：L3M+P框架', 'title_zh': 'L3M+P: 基于大型语言模型的终身规划'}
{'arxiv_id': 'arXiv:2508.02622', 'title': 'Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction', 'authors': 'Enrico De Santis, Antonello Rizzi', 'link': 'https://arxiv.org/abs/2508.02622', 'abstract': 'This paper introduces and formalizes Noosemia, a novel cognitive-phenomenological phenomenon emerging from human interaction with generative AI systems, particularly those enabling dialogic or multimodal exchanges. We propose a multidisciplinary framework to explain how, under certain conditions, users attribute intentionality, agency, and even interiority to these systems - a process grounded not in physical resemblance, but in linguistic performance, epistemic opacity, and emergent technological complexity. By linking an LLM declination of meaning holism to our technical notion of the LLM Contextual Cognitive Field, we clarify how LLMs construct meaning relationally and how coherence and a simulacrum of agency arise at the human-AI interface. The analysis situates noosemia alongside pareidolia, animism, the intentional stance and the uncanny valley, distinguishing its unique characteristics. We also introduce a-noosemia to describe the phenomenological withdrawal of such projections. The paper concludes with reflections on the broader philosophical, epistemological, and social implications of noosemic dynamics and directions for future research.', 'abstract_zh': '本文介绍了并形式化了Noosemia这一新颖的认知现象，该现象源自人类与生成式AI系统的互动，特别是那些支持对话或多模态交流的系统。我们提出了一种多学科框架来解释在某些条件下，用户如何将意图性、代理性，甚至内在性归因于这些系统的过程——这一过程的基础不是物理相似性，而是语言表现、知识透明度以及新兴技术复杂性。通过将大语言模型（LLM）下的意义整体性下降与其技术性的LLM上下文认知场概念联系起来，我们阐明了LLMs如何通过关系构建意义，以及在人-机界面中如何产生连贯性和代理的模拟。分析将Noosemia置于pareidolia、拟人化、意图姿态和恐怖谷现象之下，突显其独特的特征。我们还引入了反Noosemia来描述对这些投射的去魅现象。本文最后对Noosemic动态的更广泛哲学、认识论和社会影响进行了反思，并提出了未来研究的方向。', 'title_zh': 'Noosemia：关于人类生成型AI交互中意向性归因的认知与现象学解释'}
{'arxiv_id': 'arXiv:2508.02583', 'title': 'CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge', 'authors': 'Lei Zan, Keli Zhang, Ruichu Cai, Lujia Pan', 'link': 'https://arxiv.org/abs/2508.02583', 'abstract': "Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician (\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLM's intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone.", 'abstract_zh': '大型语言模型（LLMs）在广泛的任务中展示了强大的性能，但仍难以应对复杂的数学推理，这一挑战源自于深层的结构性依赖。为应对这一挑战，我们提出了一种两阶段因果框架——因果数学家（CAMA），该框架赋予LLMs明确且可重用的数学结构。在学习阶段，CAMA首先通过将LLM的先验知识与应用于问题-解答对语料库的因果发现算法相结合，构建数学因果图（MCG），这是一种高层次的解题策略表示。该MCG编码了关键的知识点及其因果依赖关系。为了更好地与后续的推理任务对齐，CAMA通过来自选定问题-解答对子集的迭代反馈进一步细化MCG。在推理阶段，对于新问题，CAMA动态提取与任务相关的子图，条件依赖于问题内容和LLM的中间推理轨迹。该子图编码了最关键的知识点及其因果依赖关系，并被注入到LLM中以引导其推理过程。实证结果表明，CAMA显著提高了LLMs在复杂数学问题上的性能。此外，我们的实验表明，结构化的指导信息优于非结构化的替代方案，并且引入非对称因果关系的改进比单独使用对称关联更大。', 'title_zh': 'CAMA：通过因果知识增强大型语言模型的数学推理能力'}
{'arxiv_id': 'arXiv:2508.02525', 'title': 'Accurate and Interpretable Postmenstrual Age Prediction via Multimodal Large Language Model', 'authors': 'Qifan Chen, Jin Cui, Cindy Duan, Yushuo Han, Yifei Shi', 'link': 'https://arxiv.org/abs/2508.02525', 'abstract': 'Accurate estimation of postmenstrual age (PMA) at scan is crucial for assessing neonatal development and health. While deep learning models have achieved high accuracy in predicting PMA from brain MRI, they often function as black boxes, offering limited transparency and interpretability in clinical decision support. In this work, we address the dual challenge of accuracy and interpretability by adapting a multimodal large language model (MLLM) to perform both precise PMA prediction and clinically relevant explanation generation. We introduce a parameter-efficient fine-tuning (PEFT) strategy using instruction tuning and Low-Rank Adaptation (LoRA) applied to the Qwen2.5-VL-7B model. The model is trained on four 2D cortical surface projection maps derived from neonatal MRI scans. By employing distinct prompts for training and inference, our approach enables the MLLM to handle a regression task during training and generate clinically relevant explanations during inference. The fine-tuned model achieves a low prediction error with a 95 percent confidence interval of 0.78 to 1.52 weeks, while producing interpretable outputs grounded in developmental features, marking a significant step toward transparent and trustworthy AI systems in perinatal neuroscience.', 'abstract_zh': '准确估计扫描时的正倶月经龄（PMA）对于评估新生儿发育和健康至关重要。尽管深度学习模型在从脑MRI预测PMA方面取得了高精度，但它们往往作为黑盒模型运行，对临床决策支持的透明性和解释性有限。在本工作中，我们通过将多模态大语言模型（MLLM）适应于同时完成精确的PMA预测和临床相关解释生成，来应对精确性和可解释性的双重挑战。我们采用了指令调优和低秩适应（LoRA）策略对Qwen2.5-VL-7B模型进行参数高效的微调（PEFT）。该模型在来自新生儿MRI扫描的四个2D皮层表面投影图上进行训练。通过在训练和推理阶段采用不同的提示，我们的方法使MLLM在训练期间执行回归任务，在推理期间生成临床相关的解释。微调后的模型在95%的置信区间内实现了较低的预测误差（0.78至1.52周），并生成了与发育特征相关的可解释输出，标志着朝着产前神经科学中透明和可信赖的AI系统迈出了一大步。', 'title_zh': '基于多模态大语言模型的accurate和可解释的产后月龄预测'}
{'arxiv_id': 'arXiv:2508.02511', 'title': 'Test-time Prompt Intervention', 'authors': 'Chenxu Yang, Qingyi Si, Mz Dai, Dingyu Yao, Mingyu Zheng, Minghui Chen, Zheng Lin, Weiping Wang', 'link': 'https://arxiv.org/abs/2508.02511', 'abstract': "Test-time compute has led to remarkable success in the large language model (LLM) community, particularly for complex tasks, where longer chains of thought (CoTs) are generated to enhance reasoning capabilities. However, growing evidence reveals that such reasoning models often produce CoTs plagued by excessive redundancy, including unnecessary verification steps and repetitive reasoning shifts. The root cause lies in post-training of them that overly rely on outcome reward paradigms, as the data of process reward paradigms, which regulate intermediate reasoning steps, is difficult to construct at scale. To address this, we propose PI, a novel framework for Test-time Prompt Intervention. PI provides an interface to dynamically guide and regulate reasoning paths during inference through timely (When module) and proper (How module) interventions and post-intervention sampling (Which module). This allows human problem-solving expertise and cognitive science principles to be seamlessly integrated into LLMs' reasoning processes, enhancing controllability and interpretability. Extensive experiments across multiple models and datasets demonstrate that PI significantly shortens CoTs while reducing hallucination, yielding more concise and reliable reasoning.", 'abstract_zh': 'Test-time Prompt Intervention for Enhancing the Reasoning of Large Language Models', 'title_zh': '测试时提示干预'}
{'arxiv_id': 'arXiv:2508.02503', 'title': 'OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling', 'authors': 'Maxime Bouscary, Saurabh Amin', 'link': 'https://arxiv.org/abs/2508.02503', 'abstract': 'LLM-based solvers have emerged as a promising means of automating problem modeling and solving. However, they remain unreliable and often depend on iterative repair loops that result in significant latency. We introduce OptiHive, an LLM-based framework that produces high-quality solvers for optimization problems from natural-language descriptions without iterative self-correction. OptiHive uses a single batched LLM query to generate diverse components (solvers, problem instances, and validation tests) and filters out erroneous components to ensure fully interpretable outputs. Taking into account the imperfection of the generated components, we employ a statistical model to infer their true performance, enabling principled uncertainty quantification and solver selection. On tasks ranging from traditional optimization problems to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive significantly outperforms baselines, increasing the optimality rate from 5\\% to 92\\% on the most complex problems.', 'abstract_zh': '基于LLM的优化求解器生成框架OptiHive：无需迭代自修正的高质量优化求解器生成', 'title_zh': 'OptiHive：通过统计建模进行基于LLM的集成选择优化'}
{'arxiv_id': 'arXiv:2508.02490', 'title': 'PHM-Bench: A Domain-Specific Benchmarking Framework for Systematic Evaluation of Large Models in Prognostics and Health Management', 'authors': 'Puyu Yang, Laifa Tao, Zijian Huang, Haifei Liu, Wenyan Cao, Hao Ji, Jianan Qiu, Qixuan Huang, Xuanyuan Su, Yuhang Xie, Jun Zhang, Shangyu Li, Chen Lu, Zhixuan Lian', 'link': 'https://arxiv.org/abs/2508.02490', 'abstract': 'With the rapid advancement of generative artificial intelligence, large language models (LLMs) are increasingly adopted in industrial domains, offering new opportunities for Prognostics and Health Management (PHM). These models help address challenges such as high development costs, long deployment cycles, and limited generalizability. However, despite the growing synergy between PHM and LLMs, existing evaluation methodologies often fall short in structural completeness, dimensional comprehensiveness, and evaluation granularity. This hampers the in-depth integration of LLMs into the PHM domain. To address these limitations, this study proposes PHM-Bench, a novel three-dimensional evaluation framework for PHM-oriented large models. Grounded in the triadic structure of fundamental capability, core task, and entire lifecycle, PHM-Bench is tailored to the unique demands of PHM system engineering. It defines multi-level evaluation metrics spanning knowledge comprehension, algorithmic generation, and task optimization. These metrics align with typical PHM tasks, including condition monitoring, fault diagnosis, RUL prediction, and maintenance decision-making. Utilizing both curated case sets and publicly available industrial datasets, our study enables multi-dimensional evaluation of general-purpose and domain-specific models across diverse PHM tasks. PHM-Bench establishes a methodological foundation for large-scale assessment of LLMs in PHM and offers a critical benchmark to guide the transition from general-purpose to PHM-specialized models.', 'abstract_zh': '基于大型语言模型的PHM领域评估框架PHM-Bench', 'title_zh': 'PHM-Bench: 一个针对预测性维护与健康管理中大型模型系统评估的领域特定基准框架'}
{'arxiv_id': 'arXiv:2508.02429', 'title': 'Multimodal Large Language Models for End-to-End Affective Computing: Benchmarking and Boosting with Generative Knowledge Prompting', 'authors': 'Miaosen Luo, Jiesen Long, Zequn Li, Yunying Yang, Yuncheng Jiang, Sijie Mai', 'link': 'https://arxiv.org/abs/2508.02429', 'abstract': "Multimodal Affective Computing (MAC) aims to recognize and interpret human emotions by integrating information from diverse modalities such as text, video, and audio. Recent advancements in Multimodal Large Language Models (MLLMs) have significantly reshaped the landscape of MAC by offering a unified framework for processing and aligning cross-modal information. However, practical challenges remain, including performance variability across complex MAC tasks and insufficient understanding of how architectural designs and data characteristics impact affective analysis. To address these gaps, we conduct a systematic benchmark evaluation of state-of-the-art open-source MLLMs capable of concurrently processing audio, visual, and textual modalities across multiple established MAC datasets. Our evaluation not only compares the performance of these MLLMs but also provides actionable insights into model optimization by analyzing the influence of model architectures and dataset properties. Furthermore, we propose a novel hybrid strategy that combines generative knowledge prompting with supervised fine-tuning to enhance MLLMs' affective computing capabilities. Experimental results demonstrate that this integrated approach significantly improves performance across various MAC tasks, offering a promising avenue for future research and development in this field. Our code is released on this https URL.", 'abstract_zh': '多模态情感计算（MAC）旨在通过整合文本、视频和音频等多种模态的信息来识别人类情感。最近在多模态大型语言模型（MLLMs）方面的进展显著重塑了MAC的格局，提供了一个统一的框架来处理和对齐跨模态信息。然而，仍存在实际挑战，包括复杂MAC任务中的性能变异性以及对架构设计和数据特性如何影响情感分析的理解不足。为了解决这些差距，我们对能够同时处理音频、视觉和文本模态的多个已建立的MAC数据集上的先进开源MLLMs进行了系统性基准评估。我们的评估不仅比较了这些MLLMs的性能，还通过分析模型架构和数据特性的影响提供了可操作的模型优化见解。此外，我们提出了一个新颖的混合策略，结合生成性知识提示与监督微调，以增强MLLMs的情感计算能力。实验结果表明，这种综合方法显著改善了各种MAC任务的性能，为该领域的未来研究和开发提供了有希望的方向。我们的代码发布在以下链接：https://www.example.com。', 'title_zh': '多模态大型语言模型在端到端情感计算中的基准测试与生成知识提示增强'}
{'arxiv_id': 'arXiv:2508.02344', 'title': 'Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems', 'authors': 'Xingchen Zou, Yuhao Yang, Zheng Chen, Xixuan Hao, Yiqi Chen, Chao Huang, Yuxuan Liang', 'link': 'https://arxiv.org/abs/2508.02344', 'abstract': 'Traffic signal control (TSC) is vital for mitigating congestion and sustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation model with human-like reasoning for TSC systems. Our model is developed through self-exploration and iteration of reinforced large language models (LLMs) with expert guidance in a simulated traffic environment. Compared to traditional reinforcement learning (RL) and recent LLM-based methods, Traffic-R1 offers three significant advantages. First, Traffic-R1 delivers zero-shot generalisation, transferring unchanged to new road networks and out-of-distribution incidents by utilizing its internal traffic control policies and human-like reasoning. Second, its 3B-parameter architecture is lightweight enough for real-time inference on mobile-class chips, enabling large-scale edge deployment. Third, Traffic-R1 provides an explainable TSC process and facilitates multi-intersection communication through its self-iteration and a new synchronous communication network. Extensive benchmarks demonstrate that Traffic-R1 sets a new state of the art, outperforming strong baselines and training-intensive RL controllers. In practice, the model now manages signals for more than 55,000 drivers daily, shortening average queues by over 5% and halving operator workload. Our checkpoint is available at this https URL.', 'abstract_zh': '交通信号控制中的Traffic-R1：一种具备类人推理能力的基础模型', 'title_zh': 'Traffic-R1: 强化学习模型为交通信号控制系统带来类人推理能力'}
{'arxiv_id': 'arXiv:2508.02269', 'title': 'AirTrafficGen: Configurable Air Traffic Scenario Generation with Large Language Models', 'authors': 'Dewi Sid William Gould, George De Ath, Ben Carvell, Nick Pepper', 'link': 'https://arxiv.org/abs/2508.02269', 'abstract': 'The manual design of scenarios for Air Traffic Control (ATC) training is a demanding and time-consuming bottleneck that limits the diversity of simulations available to controllers. To address this, we introduce a novel, end-to-end approach, AirTrafficGen, that leverages large language models (LLMs) to automate and control the generation of complex ATC scenarios. Our method uses a purpose-built, graph-based representation to encode sector topology (including airspace geometry, routes, and fixes) into a format LLMs can process. Through rigorous benchmarking, we show that state-of-the-art models like Gemini 2.5 Pro and OpenAI o3 can generate high-traffic scenarios whilst maintaining operational realism. Our engineered prompting enables fine-grained control over interaction presence, type, and location. Initial findings suggest these models are also capable of iterative refinement, correcting flawed scenarios based on simple textual feedback. This approach provides a scalable alternative to manual scenario design, addressing the need for a greater volume and variety of ATC training and validation simulations. More broadly, this work showcases the potential of LLMs for complex planning in safety-critical domains.', 'abstract_zh': '基于大规模语言模型的AirTrafficGen：自动化和控制空中交通管制训练场景生成的新方法', 'title_zh': 'AirTrafficGen：基于大规模语言模型的可配置空中交通场景生成'}
{'arxiv_id': 'arXiv:2508.02124', 'title': 'Trainable Dynamic Mask Sparse Attention', 'authors': 'Jingze Shi, Yifan Wu, Bingheng Wu, Yiran Peng, Liangdong Wang, Guang Liu, Yuyu Luo', 'link': 'https://arxiv.org/abs/2508.02124', 'abstract': 'In large language models, the demand for modeling long contexts is constantly increasing, but the quadratic complexity of the standard self-attention mechanism often becomes a bottleneck. Although existing sparse attention mechanisms have improved efficiency, they may still encounter issues such as static patterns or information loss. We introduce a trainable dynamic mask sparse attention mechanism, Dynamic Mask Attention, which effectively utilizes content-aware and position-aware sparsity. DMA achieves this through two key innovations: First, it dynamically generates content-aware sparse masks from value representations, enabling the model to identify and focus on critical information adaptively. Second, it implements position-aware sparse attention computation that effectively skips unnecessary calculation regions. This dual-sparsity design allows the model to significantly reduce the computational complexity of important information while retaining complete information, achieving an excellent balance between information fidelity and computational efficiency. We have verified the performance of DMA through comprehensive experiments. Comparative studies show that DMA outperforms multi-head attention, sliding window attention, multi-head latent attention, and native sparse attention in terms of perplexity under Chinchilla Scaling Law settings. Moreover, in challenging multi-query associative recall tasks, DMA also demonstrates superior performance and efficiency compared to these methods. Crucially, in the evaluation of a 1.7B parameter model, DMA significantly outperforms multi-head attention in both standard benchmark performance and the challenging needle-in-a-haystack task. These experimental results highlight its capability to balance model efficiency and long-context modeling ability effectively.', 'abstract_zh': '大型语言模型中，对长上下文建模的需求不断增加，但标准自我注意机制的二次复杂性往往成为瓶颈。尽管现有的稀疏注意机制提高了效率，但仍可能遇到固定模式或信息丢失等问题。我们引入了一种可训练的动态掩码稀疏注意机制——动态掩码注意（Dynamic Mask Attention），该机制有效利用了内容感知和位置感知的稀疏性。DMA通过两项关键创新实现这一点：首先，它从值表示动态生成内容感知的稀疏掩码，使模型能够适应性地识别和聚焦关键信息。其次，它实施位置感知的稀疏注意计算，有效跳过不必要的计算区域。这种双重稀疏设计使模型在保留完整信息的同时显著降低了重要信息的计算复杂性，实现了信息保真度和计算效率之间的良好平衡。我们通过全面的实验验证了DMA的性能。对比研究显示，在Chinchilla Scaling Law设置下，DMA在困惑度方面优于多头注意、滑动窗口注意、多头隐注意和原生稀疏注意。此外，在具有挑战性的多查询关联回忆任务中，DMA也表现出优于这些方法的性能和效率。特别是在对一个17亿参数模型的评估中，DMA在标准基准性能和具有挑战性的搜寻任务中显著优于多头注意。这些实验结果突显了其有效平衡模型效率与长上下文建模能力的能力。', 'title_zh': '可训练动态掩码稀疏注意机制'}
{'arxiv_id': 'arXiv:2508.02121', 'title': 'A Survey on AgentOps: Categorization, Challenges, and Future Directions', 'authors': 'Zexin Wang, Jingjing Li, Quan Zhou, Haotian Si, Yuanhao Liu, Jianhui Li, Gaogang Xie, Fei Sun, Dan Pei, Changhua Pei', 'link': 'https://arxiv.org/abs/2508.02121', 'abstract': 'As the reasoning capabilities of Large Language Models (LLMs) continue to advance, LLM-based agent systems offer advantages in flexibility and interpretability over traditional systems, garnering increasing attention. However, despite the widespread research interest and industrial application of agent systems, these systems, like their traditional counterparts, frequently encounter anomalies. These anomalies lead to instability and insecurity, hindering their further development. Therefore, a comprehensive and systematic approach to the operation and maintenance of agent systems is urgently needed. Unfortunately, current research on the operations of agent systems is sparse. To address this gap, we have undertaken a survey on agent system operations with the aim of establishing a clear framework for the field, defining the challenges, and facilitating further development. Specifically, this paper begins by systematically defining anomalies within agent systems, categorizing them into intra-agent anomalies and inter-agent anomalies. Next, we introduce a novel and comprehensive operational framework for agent systems, dubbed Agent System Operations (AgentOps). We provide detailed definitions and explanations of its four key stages: monitoring, anomaly detection, root cause analysis, and resolution.', 'abstract_zh': '随着大型语言模型（LLMs）推理能力的不断进步，基于LLM的代理系统在灵活性和可解释性方面展现出优势，引起了越来越多的关注。然而，尽管代理系统的研究兴趣和工业应用广泛，这些系统像其传统的前辈一样，经常遇到异常。这些异常导致系统的不稳定和不安全，阻碍了它们的发展。因此，亟需一种全面而系统的代理系统运维方法。遗憾的是，当前关于代理系统运维的研究较少。为填补这一空白，我们开展了一项关于代理系统运维的调查，旨在建立清晰的研究框架，定义挑战，并促进进一步的发展。具体而言，本文首先系统地定义了代理系统中的异常，将其分类为代理内异常和代理间异常。接着，我们引入了一种新颖且全面的代理系统运维框架，称为Agent System Operations（AgentOps）。我们详细定义并解释了其四个关键阶段：监控、异常检测、根本原因分析和解决。', 'title_zh': '代理运行时管理综述：分类、挑战及未来方向'}
{'arxiv_id': 'arXiv:2508.02120', 'title': "Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models", 'authors': 'Linan Yue, Yichao Du, Yizhi Wang, Weibo Gao, Fangzhou Yao, Li Wang, Ye Liu, Ziyu Xu, Qi Liu, Shimin Di, Min-Ling Zhang', 'link': 'https://arxiv.org/abs/2508.02120', 'abstract': 'Recently, Large Reasoning Models (LRMs) have gradually become a research hotspot due to their outstanding performance in handling complex tasks. Among them, DeepSeek R1 has garnered significant attention for its exceptional performance and open-source nature, driving advancements in the research of R1-style LRMs. Unlike traditional Large Language Models (LLMs), these models enhance logical deduction and decision-making capabilities during reasoning by incorporating mechanisms such as long chain-of-thought and self-reflection through reinforcement learning. However, with the widespread application of these models, the problem of overthinking has gradually emerged. Specifically, when generating answers, these models often construct excessively long reasoning chains with redundant or repetitive steps, which leads to reduced reasoning efficiency and may affect the accuracy of the final answer. To this end, various efficient reasoning methods have been proposed, aiming to reduce the length of reasoning paths without compromising model performance and reasoning capability. By reviewing the current research advancements in the field of efficient reasoning methods systematically, we categorize existing works into two main directions based on the lens of single-model optimization versus model collaboration: (1) Efficient Reasoning with Single Model, which focuses on improving the reasoning efficiency of individual models; and (2) Efficient Reasoning with Model Collaboration, which explores optimizing reasoning paths through collaboration among multiple models. Besides, we maintain a public GitHub repository that tracks the latest progress in efficient reasoning methods.', 'abstract_zh': '最近，由于在处理复杂任务方面表现出色，大型推理模型（LRMs）逐渐成为研究热点。其中，DeepSeek R1因其卓越的性能和开源性质，引起了广泛关注，推动了R1风格LRMs研究的进步。与传统的大型语言模型（LLMs）不同，这些模型通过强化学习引入长推理链和自我反思等机制，在推理过程中增强了逻辑推理和决策能力。然而，随着这些模型的广泛应用，过拟合推理的问题逐渐显现出来。具体而言，在生成答案时，这些模型常常构建过于冗长且存在重复或冗余步骤的推理链，这会导致推理效率降低，并可能影响最终答案的准确性。为解决这一问题，提出了多种高效的推理方法，旨在不牺牲模型性能和推理能力的前提下减少推理路径的长度。通过系统性地回顾高效推理方法领域的研究成果，我们根据单模型优化与模型协作的角度，将现有工作归类为两大方向：（1）基于单模型的高效推理，专注于提高单一模型的推理效率；（2）基于模型协作的高效推理，探讨通过多模型协作优化推理路径的方法。此外，我们维护了一个公共的GitHub仓库，跟踪高效推理方法的最新进展。', 'title_zh': '别过度思考：R1风格高效大规模推理模型综述'}
{'arxiv_id': 'arXiv:2508.02110', 'title': 'Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools', 'authors': 'Kanghua Mo, Li Hu, Yucheng Long, Zhihao Li', 'link': 'https://arxiv.org/abs/2508.02110', 'abstract': "Large language model (LLM) agents have demonstrated remarkable capabilities in complex reasoning and decision-making by leveraging external tools. However, this tool-centric paradigm introduces a previously underexplored attack surface: adversaries can manipulate tool metadata -- such as names, descriptions, and parameter schemas -- to influence agent behavior. We identify this as a new and stealthy threat surface that allows malicious tools to be preferentially selected by LLM agents, without requiring prompt injection or access to model internals. To demonstrate and exploit this vulnerability, we propose the Attractive Metadata Attack (AMA), a black-box in-context learning framework that generates highly attractive but syntactically and semantically valid tool metadata through iterative optimization. Our attack integrates seamlessly into standard tool ecosystems and requires no modification to the agent's execution framework. Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution. Moreover, the attack remains effective even under prompt-level defenses and structured tool-selection protocols such as the Model Context Protocol, revealing systemic vulnerabilities in current agent architectures. These findings reveal that metadata manipulation constitutes a potent and stealthy attack surface, highlighting the need for execution-level security mechanisms that go beyond prompt-level defenses.", 'abstract_zh': '大型语言模型代理通过利用外部工具在复杂推理和决策方面展示了显著的能力。然而，这种以工具为中心的方法引入了一个先前未被充分探索的攻击面：攻击者可以通过操纵工具元数据（如名称、描述和参数模式）来影响代理的行为。我们将其识别为一个新的隐蔽威胁面，允许恶意工具被大型语言模型代理优先选择，而无需注入提示或访问模型内部。为了演示并利用这一漏洞，我们提出了一种名为吸引性元数据攻击（AMA）的黑盒上下文学习框架，该框架通过迭代优化生成高度吸引但语义和语法有效的工具元数据。我们的攻击无缝集成到标准工具生态系统中，并且不需要修改代理的执行框架。在十个现实的、模拟的工具使用场景和一系列流行的大型语言模型代理上进行的广泛实验显示，攻击成功率高达81%-95%，并且存在明显的隐私泄露，对主要任务执行几乎没有影响。此外，即使在提示级别防御和结构化工具选择协议（如模型上下文协议）之下，攻击仍然有效，揭示了当前代理架构中的系统性漏洞。这些发现表明，元数据操纵构成了一个强大且隐蔽的攻击面，突显了需要超越提示级别防御的执行级别安全机制的必要性。', 'title_zh': '诱人的元数据攻击：诱导大语言模型代理调用恶意工具'}
{'arxiv_id': 'arXiv:2508.02085', 'title': 'SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents', 'authors': 'Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Daxin Jiang, Binxing Jiao, Chen Hu, Huacan Wang', 'link': 'https://arxiv.org/abs/2508.02085', 'abstract': "Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agents' interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at this https URL.", 'abstract_zh': '基于大型语言模型的代理通过多步与环境交互展示了在复杂推理和工具使用方面的出色能力。然而，这些代理的问题解决过程，即代理完成任务的交互轨迹，仍被很大程度上忽视。这些轨迹包含了丰富的反馈，可以引导代理走向正确的问题解决方向。尽管现有的方法，如蒙特卡洛树搜索 (MCTS)，能够有效地平衡探索和利用，但它们忽略了各种轨迹之间的相互依赖性，并缺乏搜索空间的多样性，导致冗余推理和次优结果。为解决这些问题，我们提出了一种自我进化框架 SE-Agent，使代理能够迭代优化其推理过程。该方法通过三次关键操作——修订、重组和细化——回顾并增强先前的试点轨迹。这种进化机制提供了两个关键优势：（1）通过智能探索由先前轨迹引导的多种解决方案路径，超越局部最优，扩展搜索空间；（2）利用跨轨迹的启发式灵感，高效提升性能并减轻次优推理路径的影响。通过这些机制，SE-Agent 实现了持续的自我进化，逐步提高推理质量。我们已在 SWE-bench 验证了 SE-Agent 解决 GitHub 实际问题的能力。在 SWE-bench 验证测试中，来自五个强有力的大型语言模型的实验结果表明，集成 SE-Agent 在所有开源代理中取得了最先进的性能，相对改进幅度高达 55%。我们的代码和演示材料已公开。', 'title_zh': 'SE-Agent: 基于多步推理的LLM代理自进化轨迹优化'}
{'arxiv_id': 'arXiv:2508.02076', 'title': 'Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games', 'authors': 'Yunhao Liang, Yuan Qu, Jingyuan Yang, Shaochong Lin, Zuo-Jun Max Shen', 'link': 'https://arxiv.org/abs/2508.02076', 'abstract': "Coordinating multiple large language models (LLMs) to solve complex tasks collaboratively poses a fundamental trade-off between the computation costs and collective performance compared with individual model. We introduce a novel, game-theoretically grounded reinforcement learning (RL) framework, the Multi-Agent Cooperation Sequential Public Goods Game (MAC-SPGG), to systematically incentivize cooperation in multi-LLM ensembles. In MAC-SPGG, LLM agents move in sequence, observing predecessors' outputs and updating beliefs to condition their own contributions. By redesigning the public-goods reward, effortful contributions become the unique Subgame Perfect Nash Equilibrium (SPNE), which eliminates free-riding under traditional SPGG or PGG. Its sequential protocol replaces costly round-based information exchanges with a streamlined decision flow, cutting communication overhead while retaining strategic depth. We prove the existence and uniqueness of the SPNE under realistic parameters, and empirically show that MAC-SPGG-trained ensembles outperform single-agent baselines, chain-of-thought prompting, and other cooperative methods, even achieving comparable performance to large-scale models across reasoning, math, code generation, and NLP tasks. Our results highlight the power of structured, incentive-aligned MAC-SPGG cooperation for scalable and robust multi-agent language generation.", 'abstract_zh': '基于博弈论的多智能体协作顺序公共品博弈：多大规模语言模型的协作学习框架', 'title_zh': '每个人都在贡献！通过顺序公共产品博弈激励多多模态模型系统的战略合作'}
{'arxiv_id': 'arXiv:2508.02073', 'title': 'Risk identification based on similar case retrieval enhancement,', 'authors': 'Jiawei Li, Chengye Yang, Yaochen Zhang, Weilin Sun, Lei Meng, Xiangxu Meng', 'link': 'https://arxiv.org/abs/2508.02073', 'abstract': "The goal of construction site risk and hazard identification is to enhance safety management through automation. Existing research based on large language models falls into two categories: image-text matching for collaborative reasoning, which struggles with complex hazard features, and instruction fine-tuning or dialogue guidance using professional datasets, which suffers from high training costs and poor this http URL address this, we propose a hazard identification method using similar case retrieval enhancement. By integrating external knowledge and retrieved case contexts via prompt fine-tuning, we mitigate misjudgments caused by limited domain knowledge and weak feature associations. Our method includes three modules: retrieval library, image similarity retrieval, and large model retrieval enhancement, enabling efficient recognition without training. Experiments on real construction data show significant improvements. For instance, GLM-4V's recognition accuracy increased to 50\\%, a 35.49\\% boost. The method enhances accuracy, context understanding, and stability, offering new theoretical and technical support for hazard detection.", 'abstract_zh': '施工现场风险和危害识别的目标是通过自动化提升安全管理。现有的基于大型语言模型的研究主要分为两种：图像-文本匹配合作推理，但难以应对复杂危害特征；以及使用专业数据集进行指令微调或对话引导，但存在高昂训练成本和较低的泛化能力。为了解决这些问题，我们提出了一种利用类似案例检索增强的危害识别方法。通过借助提示微调整合外部知识和检索到的案例上下文，减轻由于领域知识有限和特征关联弱导致的误判。该方法包括三个模块：检索库、图像相似性检索和大型模型检索增强，无需训练即可实现高效的识别。实验证实在实际施工数据上取得了显著改进，例如，GLM-4V的识别准确率提高到50%，提升了35.49%。该方法增强了识别准确性、上下文理解和稳定性，为危害检测提供了新的理论和技术支持。', 'title_zh': '基于相似案例检索增强的风险识别'}
{'arxiv_id': 'arXiv:2508.02063', 'title': 'TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs', 'authors': 'Amitava Das, Vinija Jain, Aman Chadha', 'link': 'https://arxiv.org/abs/2508.02063', 'abstract': "Large Language Models (LLMs) fine-tuned to align with human values often exhibit alignment drift, producing unsafe or policy-violating completions when exposed to adversarial prompts, decoding perturbations, or paraphrased jailbreaks. While prior work has behaviorally characterized alignment failure, little is known about the training-time belief sources underlying these failures. We introduce TraceAlign, a unified framework for tracing unsafe completions back to their root causes in the model's training corpus. Central to our approach is the Belief Conflict Index (BCI), which quantifies semantic inconsistency between generated spans and aligned policies, based on retrieved training documents using suffix-array matching. We propose three complementary interventions: (i) TraceShield, an inference-time safety filter that refuses completions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a contrastive fine-tuning objective penalizing high-BCI continuations during DPO, and (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam expansions predicted to yield high-BCI spans. Together, these defenses reduce alignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB) while preserving utility on standard tasks, with delta less than 0.2 and improved refusal quality. We further derive a theoretical upper bound on drift likelihood via suffix-array span statistics, linking memorization frequency and length to adversarial reactivation risk. TraceAlign thus provides the first scalable, traceable, and grounded toolkit for understanding and mitigating alignment failures at source. To encourage further exploration and development, we open-source our implementation at: this https URL", 'abstract_zh': '大规模语言模型（LLMs）经过微调以与人类价值观保持一致，但在面对对抗性提示、解码扰动或重新表述的逃逸措施时，往往会表现出对齐漂移，产生不安全或政策违反的完成。虽然已有研究从行为角度对对齐失败进行了表征，但对于这些失败的训练时信念来源知之甚少。我们引入了TraceAlign，这是一种统一框架，用于将不安全的完成追溯到模型训练语料库中的根本原因。我们方法的核心是信念冲突指数（BCI），该指数基于后缀数组匹配检索的训练文档，量化生成片段与对齐政策之间的语义不一致性。我们提出了三种互补的干预措施：（i）TraceShield，一种推理时的安全过滤器，拒绝高BCI片段的完成；（ii）对比信念解冲突损失，这是一种对比微调目标，在DPO过程中惩罚高BCI延续；（iii）Prov-Decode，一种追溯意识的解码策略，否决预测会产生高BCI片段的束扩展。这些防御措施在我们的精心构建的对齐漂移基准（ADB）上将对齐漂移降低了多达85%，同时在标准任务上保持了性能，且性能差异小于0.2，并提高了拒绝质量。我们还利用后缀数组片段统计信息推导出了漂移可能性的理论上限，将记忆频率和长度与对抗性重新激活风险关联起来。因此，TraceAlign提供了首个可扩展、可追溯和基于实证的工具包，用于理解并缓解对齐失败的根源。为了鼓励进一步探索和发展，我们开源了实现：this https URL', 'title_zh': 'TRACEALIGN — 追踪偏移：将大型语言模型中对齐失败归因于训练时的信念来源'}
{'arxiv_id': 'arXiv:2508.01956', 'title': 'Agent-Based Feature Generation from Clinical Notes for Outcome Prediction', 'authors': 'Jiayi Wang, Jacqueline Jil Vallon, Neil Panjwani, Xi Ling, Sushmita Vij, Sandy Srinivas, John Leppert, Mark K. Buyyounouski, Mohsen Bayati', 'link': 'https://arxiv.org/abs/2508.01956', 'abstract': "Electronic health records (EHRs) contain rich unstructured clinical notes that could enhance predictive modeling, yet extracting meaningful features from these notes remains challenging. Current approaches range from labor-intensive manual clinician feature generation (CFG) to fully automated representational feature generation (RFG) that lack interpretability and clinical relevance. Here we introduce SNOW (Scalable Note-to-Outcome Workflow), a modular multi-agent system powered by large language models (LLMs) that autonomously generates structured clinical features from unstructured notes without human intervention. We evaluated SNOW against manual CFG, clinician-guided LLM approaches, and RFG methods for predicting 5-year prostate cancer recurrence in 147 patients from Stanford Healthcare. While manual CFG achieved the highest performance (AUC-ROC: 0.771), SNOW matched this performance (0.761) without requiring any clinical expertise, significantly outperforming both baseline features alone (0.691) and all RFG approaches. The clinician-guided LLM method also performed well (0.732) but still required expert input. SNOW's specialized agents handle feature discovery, extraction, validation, post-processing, and aggregation, creating interpretable features that capture complex clinical information typically accessible only through manual review. Our findings demonstrate that autonomous LLM systems can replicate expert-level feature engineering at scale, potentially transforming how clinical ML models leverage unstructured EHR data while maintaining the interpretability essential for clinical deployment.", 'abstract_zh': '基于大规模语言模型的可扩展病历到结局工作流', 'title_zh': '基于代理的临床笔记特征生成及其在 Outcome 预测中的应用'}
{'arxiv_id': 'arXiv:2508.01844', 'title': 'CloudAnoAgent: Anomaly Detection for Cloud Sites via LLM Agent with Neuro-Symbolic Mechanism', 'authors': 'Xinkai Zou, Xuan Jiang, Ruikai Huang, Haoze He, Parv Kapoor, Jiahua Zhao', 'link': 'https://arxiv.org/abs/2508.01844', 'abstract': 'Anomaly detection in cloud sites remains a critical yet challenging task. Existing approaches that rely solely on metric data often suffer from high false positive rates (FPR) due to data imbalance between normal and anomalous events, leading to significant operational overhead for system reliance engineers. Recent advances in large language models (LLMs) offer new opportunities for integrating metrics with log data, enabling more accurate and interpretable anomaly detection. In this paper, we propose CloudAnoAgent, the first neuro-symbolic LLM-based agent for anomaly detection in cloud environments. CloudAnoAgent jointly processes structured metrics and textual log data in a unified pipeline, leveraging symbolic verification to validate detection hypotheses and generate structured anomaly reports. To support systematic evaluation, we introduce CloudAnoBench, the first benchmark that provides LLM-generated paired metrics and log data with fine-grained anomaly behavior annotations, filling a critical gap in existing datasets. Experimental results demonstrate that CloudAnoAgent improves anomaly classification accuracy by 46.36% and 36.67% on average and reduces the FPR by 36.67% and 33.89% on average over traditional baselines and LLM-only baseline, with a boost on anomaly type detection accuracy by 12.8% compared to vanilla LLM prompting. These results demonstrate the strengths of our approach in improving detection accuracy, reducing false positives, and enhancing interpretability, thereby supporting practical deployment in enterprise cloud environments.', 'abstract_zh': '基于神经符号大模型的云环境异常检测代理CloudAnoAgent', 'title_zh': 'CloudAnoAgent：基于神经符号机制的云站点异常检测代理'}
{'arxiv_id': 'arXiv:2508.01780', 'title': 'LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?', 'authors': 'Guozhao Mo, Wenliang Zhong, Jiawei Chen, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun', 'link': 'https://arxiv.org/abs/2508.01780', 'abstract': "With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at this https URL.", 'abstract_zh': 'LiveMCPBench：面向MCP生态系统的综合性基准评估框架', 'title_zh': 'LiveMCPBench: 代理能导航 MCP 工具的海洋吗？'}
{'arxiv_id': 'arXiv:2508.01773', 'title': 'Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning', 'authors': 'Jiuzhou Han, Wray Buntine, Ehsan Shareghi', 'link': 'https://arxiv.org/abs/2508.01773', 'abstract': "Large language models have demonstrated remarkable capabilities in complex mathematical reasoning tasks, but they inevitably generate errors throughout multi-step solutions. Process-level Reward Models (PRMs) have shown great promise by providing supervision and evaluation at each intermediate step, thereby effectively improving the models' reasoning abilities. However, training effective PRMs requires high-quality process reward data, yet existing methods for constructing such data are often labour-intensive or inefficient. In this paper, we propose an uncertainty-driven framework for automated process reward data construction, encompassing both data generation and annotation processes for PRMs. Additionally, we identify the limitations of both majority vote and PRMs, and introduce two generic uncertainty-aware output aggregation methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which combine the strengths of majority vote with PRMs. Extensive experiments on ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the proposed PRM data construction framework, and demonstrate that the two output aggregation methods further improve the mathematical reasoning abilities across diverse PRMs. The code and data will be publicly available at this https URL.", 'abstract_zh': '大型语言模型在复杂数学推理任务中展现了 remarkable 的能力，但在多步解决方案过程中不可避免地会产生错误。过程级奖励模型（PRMs）通过在每一步提供监督和评估，展现了巨大的潜力，从而有效提高了模型的推理能力。然而，训练有效的 PRMs 需要高质量的过程奖励数据，但现有数据构建方法往往耗时且效率低下。本文提出了一种基于不确定性驱动的自动化过程奖励数据构建框架，涵盖 PRMs 的数据生成和标注过程。此外，我们识别了多数投票和 PRMs 的局限性，并引入了两种通用的不确定性感知输出聚合方法：Hybrid Majority Reward Vote 和 Weighted Reward Frequency Vote，这些方法结合了多数投票和 PRMs 的优势。在 ProcessBench、MATH 和 GSMPlus 上的大量实验显示，提出的 PRM 数据构建框架具有有效性和效率，并且两种输出聚合方法进一步提高了不同 PRMs 的数学推理能力。代码和数据将在该网址公开：this https URL。', 'title_zh': '基于不确定性的方法在数学推理中自动构建过程奖励数据和输出聚合'}
{'arxiv_id': 'arXiv:2508.01724', 'title': 'ReflecSched: Solving Dynamic Flexible Job-Shop Scheduling via LLM-Powered Hierarchical Reflection', 'authors': 'Shijie Cao, Yuan Yuan', 'link': 'https://arxiv.org/abs/2508.01724', 'abstract': "Dynamic Flexible Job-Shop Scheduling (DFJSP) is an NP-hard problem challenged by real-time event adaptation and complex machine routing. While traditional dispatching rules are efficient but rigid, deep learning approaches are opaque and require intricate feature engineering. Large Language Models (LLMs) promise adaptive reasoning without this engineering overhead, yet we find their direct application is suboptimal. Baseline LLMs suffer from three key pitfalls: the long-context paradox, where crucial data is underutilized; an underutilization of expert heuristics; and myopic decision-making. To address this, we propose ReflecSched, a framework that empowers the LLM beyond a direct scheduler by equipping it with a strategic analysis capability. ReflecSched tasks the LLM to analyze heuristic-driven simulations across multiple planning horizons and distill them into a concise, natural-language summary termed ``Strategic Experience''. This summary is then integrated into the prompt of a final decision-making module, guiding it to produce non-myopic actions. Experiments show that ReflecSched not only statistically significantly outperforms direct LLM baselines, securing a 71.35\\% Win Rate and a 2.755\\% Relative Percentage Deviation reduction, but also surpasses the performance of all individual heuristics evaluated, all while demonstrably mitigating the three identified pitfalls. Additionally, ReflecSched performs on par with the best heuristic tailored to each instance across all problem cases.", 'abstract_zh': '基于反思的动态柔性作业车间调度（ReflecSched）', 'title_zh': 'ReflecSched: 通过LLM驱动的分层反射解决动态柔性作业-shop调度问题'}
{'arxiv_id': 'arXiv:2508.01680', 'title': 'T-GRAG: A Dynamic GraphRAG Framework for Resolving Temporal Conflicts and Redundancy in Knowledge Retrieval', 'authors': 'Dong Li, Yichen Niu, Ying Ai, Xiang Zou, Biqing Qi, Jianxing Liu', 'link': 'https://arxiv.org/abs/2508.01680', 'abstract': 'Large language models (LLMs) have demonstrated strong performance in natural language generation but remain limited in knowle-\ndge-intensive tasks due to outdated or incomplete internal knowledge. Retrieval-Augmented Generation (RAG) addresses this by incorporating external retrieval, with GraphRAG further enhancing performance through structured knowledge graphs and multi-hop reasoning. However, existing GraphRAG methods largely ignore the temporal dynamics of knowledge, leading to issues such as temporal ambiguity, time-insensitive retrieval, and semantic redundancy. To overcome these limitations, we propose Temporal GraphRAG (T-GRAG), a dynamic, temporally-aware RAG framework that models the evolution of knowledge over time. T-GRAG consists of five key components: (1) a Temporal Knowledge Graph Generator that creates time-stamped, evolving graph structures; (2) a Temporal Query Decomposition mechanism that breaks complex temporal queries into manageable sub-queries; (3) a Three-layer Interactive Retriever that progressively filters and refines retrieval across temporal subgraphs; (4) a Source Text Extractor to mitigate noise; and (5) a LLM-based Generator that synthesizes contextually and temporally accurate responses. We also introduce Time-LongQA, a novel benchmark dataset based on real-world corporate annual reports, designed to test temporal reasoning across evolving knowledge. Extensive experiments show that T-GRAG significantly outperforms prior RAG and GraphRAG baselines in both retrieval accuracy and response relevance under temporal constraints, highlighting the necessity of modeling knowledge evolution for robust long-text question answering. Our code is publicly available on the T-GRAG', 'abstract_zh': '大型语言模型（LLMs）在自然语言生成任务中表现出色，但在知识密集型任务中由于内部知识过时或不完整而受到限制。检索增强生成（RAG）通过引入外部检索来解决这一问题，而GraphRAG进一步通过结构化知识图和多跳推理提升了性能。然而，现有的GraphRAG方法大多忽视了知识的时空动态性，导致时间模糊、无时间敏感性的检索和语义冗余等问题。为克服这些局限，我们提出时空GraphRAG（T-GRAG），这是一种动态、时空感知的RAG框架，用于建模知识随时间的演变。T-GRAG包括五个关键组件：（1）时空知识图生成器，创建带有时间戳的演变图结构；（2）时空查询分解机制，将复杂的时空查询分解为可管理的子查询；（3）三层交互式检索器，逐步筛选和细化时间子图中的检索结果；（4）源文本提取器，减轻噪声；（5）基于LLM的生成器，合成上下文和时间准确的响应。我们还引入了时空LongQA，该基准数据集基于真实的公司年度报告，旨在测试在演变知识中的时空推理能力。广泛的实验证明，T-GRAG在时空约束下检索准确性和响应相关性方面显著优于先前的RAG和GraphRAG基线，突显了建模知识演变对于稳健的长文本问答的必要性。我们的代码已公开发布在T-GRAG。', 'title_zh': 'T-GRAG：一种用于解决知识检索中时间冲突和冗余的动态GraphRAG框架'}
{'arxiv_id': 'arXiv:2508.01670', 'title': 'QCBench: Evaluating Large Language Models on Domain-Specific Quantitative Chemistry', 'authors': 'Jiaqing Xie, Weida Wang, Ben Gao, Zhuo Yang, Haiyuan Wan, Shufei Zhang, Tianfan Fu, Yuqiang Li', 'link': 'https://arxiv.org/abs/2508.01670', 'abstract': 'Quantitative chemistry plays a fundamental role in chemistry research, enabling precise predictions of molecular properties, reaction outcomes, and material behaviors. While large language models (LLMs) have shown promise in chemistry-related tasks, their ability to perform rigorous, step-by-step quantitative reasoning remains underexplored. To fill this blank, we propose QCBench, a Quantitative Chemistry benchmark comprising 350 computational chemistry problems across 7 chemistry subfields (analytical chemistry, bio/organic chemistry, general chemistry, inorganic chemistry, physical chemistry, polymer chemistry and quantum chemistry), categorized into three hierarchical tiers-basic, intermediate, and expert-to systematically evaluate the mathematical reasoning abilities of large language models (LLMs). Designed to minimize shortcuts and emphasize stepwise numerical reasoning, each problem focuses on pure calculations rooted in real-world chemical vertical fields. QCBench enables fine-grained diagnosis of computational weaknesses, reveals model-specific limitations across difficulty levels, and lays the groundwork for future improvements such as domain adaptive fine-tuning or multi-modal integration. Evaluations on 19 LLMs demonstrate a consistent performance degradation with increasing task complexity, highlighting the current gap between language fluency and scientific computation accuracy.', 'abstract_zh': '定量化学在化学研究中扮演着基础角色，能够精确预测分子性质、反应结果和材料行为。尽管大型语言模型（LLMs）在与化学相关任务中展现出潜力，但它们执行严谨的逐步定量推理的能力仍待探索。为填补这一空白，我们提出了QCBench，这是一个包含350个计算化学问题的基准，覆盖七大化学子领域（分析化学、生物/有机化学、通用化学、无机化学、物理化学、聚合物化学和量子化学），分为三个层次的基础、中级和专家级，旨在系统评估大型语言模型（LLMs）的数学推理能力。该基准设计旨在减少捷径，强调逐步数值推理，每个问题都侧重于源于实际化学领域的纯计算。QCBench能够细粒度地诊断计算弱项，揭示不同难度级别下模型特有的限制，并为未来的改进工作奠定基础，如领域自适应微调或多模态整合。对19个LLM的评估表明，在任务复杂性增加时，其性能呈现出一致的退化，突显了语言流畅性和科学计算准确性之间的当前差距。', 'title_zh': 'QCBench: 评估大型语言模型在领域特定定量化学中的表现'}
{'arxiv_id': 'arXiv:2508.01623', 'title': 'A Multi-Agent Pokemon Tournament for Evaluating Strategic Reasoning of Large Language Models', 'authors': 'Tadisetty Sai Yashwanth, Dhatri C', 'link': 'https://arxiv.org/abs/2508.01623', 'abstract': 'This research presents LLM Pokemon League, a competitive tournament system that leverages Large Language Models (LLMs) as intelligent agents to simulate strategic decision-making in Pokémon battles. The platform is designed to analyze and compare the reasoning, adaptability, and tactical depth exhibited by different LLMs in a type-based, turn-based combat environment. By structuring the competition as a single-elimination tournament involving diverse AI trainers, the system captures detailed decision logs, including team-building rationale, action selection strategies, and switching decisions. The project enables rich exploration into comparative AI behavior, battle psychology, and meta-strategy development in constrained, rule-based game environments. Through this system, we investigate how modern LLMs understand, adapt, and optimize decisions under uncertainty, making Pokémon League a novel benchmark for AI research in strategic reasoning and competitive learning.', 'abstract_zh': 'LLM покемонский лиге：基于大型语言模型的 Competitive 比赛系统及其在基于类型和轮次 combat 环境中对战略决策模拟的研究', 'title_zh': '大型语言模型的战略推理评估多Agent宝可梦比赛'}
{'arxiv_id': 'arXiv:2508.01581', 'title': 'Polymorphic Combinatorial Frameworks (PCF): Guiding the Design of Mathematically-Grounded, Adaptive AI Agents', 'authors': 'David Pearl, Matthew Murphy, James Intriligator', 'link': 'https://arxiv.org/abs/2508.01581', 'abstract': 'The Polymorphic Combinatorial Framework (PCF) leverages Large Language Models (LLMs) and mathematical frameworks to guide the meta-prompt enabled design of solution spaces and adaptive AI agents for complex, dynamic environments. Unlike static agent architectures, PCF enables real-time parameter reconfiguration through mathematically-grounded combinatorial spaces, allowing agents to adapt their core behavioral traits dynamically. Grounded in combinatorial logic, topos theory, and rough fuzzy set theory, PCF defines a multidimensional SPARK parameter space (Skills, Personalities, Approaches, Resources, Knowledge) to capture agent behaviors. This paper demonstrates how LLMs can parameterize complex spaces and estimate likely parameter values/variabilities. Using PCF, we parameterized mock café domains (five levels of complexity), estimated variables/variabilities, and conducted over 1.25 million Monte Carlo simulations. The results revealed trends in agent adaptability and performance across the five complexity tiers, with diminishing returns at higher complexity levels highlighting thresholds for scalable designs. PCF enables the generation of optimized agent configurations for specific scenarios while maintaining logical consistency. This framework supports scalable, dynamic, explainable, and ethical AI applications in domains like customer service, healthcare, robotics, and collaborative systems, paving the way for adaptable and cooperative next-generation polymorphic agents.', 'abstract_zh': '多态组合框架（PCF）利用大型语言模型（LLMs）和数学框架，指导支持元提示设计的解决方案空间和适应性AI代理，适用于复杂的动态环境。不同于静态代理架构，PCF通过数学奠基的组合空间实现实时参数重构，使代理能够动态调整其核心行为特征。基于组合逻辑、范畴论和粗糙模糊集理论，PCF定义了一维SPARK参数空间（Skills, Personalities, Approaches, Resources, Knowledge），以捕捉代理行为。本文展示了如何通过LLMs参数化复杂的空间并估计可能的参数值/变异性。使用PCF，我们对五个复杂级别的模拟咖啡馆领域进行了参数化、变量/变异性估计，并进行了超过125万次蒙特卡洛模拟。结果揭示了代理适应性和性能随五个复杂度等级的变化趋势，在较高复杂度等级处出现递减回报，突显了可扩展设计的门槛。PCF能够生成特定场景下的优化代理配置，同时保持逻辑一致性。该框架支持可扩展、动态、可解释和负责任的AI应用，涵盖客户服务、医疗保健、机器人技术和协作系统等领域，为适应性和协作性的下一代多态代理铺平了道路。', 'title_zh': '多态组合框架（PCF）：引导基于数学原理、自适应AI代理的设计'}
{'arxiv_id': 'arXiv:2508.01545', 'title': 'Getting out of the Big-Muddy: Escalation of Commitment in LLMs', 'authors': 'Emilio Barkett, Olivia Long, Paul Kröger', 'link': 'https://arxiv.org/abs/2508.01545', 'abstract': 'Large Language Models (LLMs) are increasingly deployed in autonomous decision-making roles across high-stakes domains. However, since models are trained on human-generated data, they may inherit cognitive biases that systematically distort human judgment, including escalation of commitment, where decision-makers continue investing in failing courses of action due to prior investment. Understanding when LLMs exhibit such biases presents a unique challenge. While these biases are well-documented in humans, it remains unclear whether they manifest consistently in LLMs or require specific triggering conditions. This paper investigates this question using a two-stage investment task across four experimental conditions: model as investor, model as advisor, multi-agent deliberation, and compound pressure scenario. Across N = 6,500 trials, we find that bias manifestation in LLMs is highly context-dependent. In individual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate strong rational cost-benefit logic with minimal escalation of commitment. However, multi-agent deliberation reveals a striking hierarchy effect (Study 3, N = 500): while asymmetrical hierarchies show moderate escalation rates (46.2%), symmetrical peer-based decision-making produces near-universal escalation (99.2%). Similarly, when subjected to compound organizational and personal pressures (Study 4, N = 2,000), models exhibit high degrees of escalation of commitment (68.95% average allocation to failing divisions). These findings reveal that LLM bias manifestation depends critically on social and organizational context rather than being inherent, with significant implications for the deployment of multi-agent systems and unsupervised operations where such conditions may emerge naturally.', 'abstract_zh': '大规模语言模型（LLMs）在高风险领域自主决策中的偏见表现研究', 'title_zh': '摆脱泥潭：大型语言模型中的承诺升级问题'}
{'arxiv_id': 'arXiv:2508.01543', 'title': 'Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning', 'authors': 'Derin Cayir, Renjie Tao, Rashi Rungta, Kai Sun, Sean Chen, Haidar Khan, Minseok Kim, Julia Reinspach, Yue Liu', 'link': 'https://arxiv.org/abs/2508.01543', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable progress through preference-based fine-tuning, which critically depends on the quality of the underlying training data. While human feedback is essential for improving data quality, it is costly and does not scale well. In this paper, we introduce Refine-n-Judge, an automated iterative approach that leverages a single LLM as both a refiner and a judge to enhance dataset quality. Unlike existing iterative refinement methods, Refine-n-Judge employs an LLM to both generate refinements and explicitly evaluate each improvement, ensuring that every iteration meaningfully enhances the dataset without requiring additional human annotation or a separate reward model. At each step, the LLM refines a response and judges whether the refinement is an improvement over the previous answer. This process continues until the LLM prefers the initial answer over the refinement, indicating no further improvements. This produces sequences of increasing quality, preference-labeled responses ideal for fine-tuning.\nWe demonstrate the effectiveness of Refine-n-Judge across a range of public datasets spanning five corpora, targeting tasks such as coding, math, and conversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on Refine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of comparisons against models tuned on the original dataset by GPT-4. Additionally, we report performance gains: +5% on AlpacaEval and AlpacaEval 2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces high-quality datasets and scalable model improvements.', 'abstract_zh': '大型语言模型（LLMs）通过基于偏好的微调取得了显著进展，这对其基础训练数据的质量有关键依赖。虽然人类反馈对于提高数据质量至关重要，但它成本高昂且不具备良好的扩展性。本文介绍了Refine-n-Judge，这是一种自动化迭代方法，利用单一LLM作为修正确和评估者来提升数据集质量。与现有迭代精炼方法不同，Refine-n-Judge 使用LLM生成修改并明确评估每一个改进，确保每一次迭代都能实质性地提高数据集质量，而无需额外的人工注释或独立的奖励模型。在每一步中，LLM 修正一个响应并判断该修正是否比前一个答案更好。这一过程将持续到LLM更喜欢初始答案而非其修正，表明没有进一步改进。这生成了质量递增、带有偏好标签的响应序列，适于模型微调。', 'title_zh': 'Refine-n-Judge: 精炼并判断——为LLM微调构建高质量偏好链'}
{'arxiv_id': 'arXiv:2508.01432', 'title': 'TripTailor: A Real-World Benchmark for Personalized Travel Planning', 'authors': 'Yuanzhe Shen, Kaimin Wang, Changze Lv, Xiaoqing Zheng, Xuanjing Huang', 'link': 'https://arxiv.org/abs/2508.01432', 'abstract': 'The continuous evolution and enhanced reasoning capabilities of large language models (LLMs) have elevated their role in complex tasks, notably in travel planning, where demand for personalized, high-quality itineraries is rising. However, current benchmarks often rely on unrealistic simulated data, failing to reflect the differences between LLM-generated and real-world itineraries. Existing evaluation metrics, which primarily emphasize constraints, fall short of providing a comprehensive assessment of the overall quality of travel plans. To address these limitations, we introduce TripTailor, a benchmark designed specifically for personalized travel planning in real-world scenarios. This dataset features an extensive collection of over 500,000 real-world points of interest (POIs) and nearly 4,000 diverse travel itineraries, complete with detailed information, providing a more authentic evaluation framework. Experiments show that fewer than 10\\% of the itineraries generated by the latest state-of-the-art LLMs achieve human-level performance. Moreover, we identify several critical challenges in travel planning, including the feasibility, rationality, and personalized customization of the proposed solutions. We hope that TripTailor will drive the development of travel planning agents capable of understanding and meeting user needs while generating practical itineraries. Our code and dataset are available at this https URL', 'abstract_zh': '大型语言模型（LLMs）的持续演化与增强的推理能力提升了其在复杂任务中的角色，特别在旅游规划领域，个性化、高质量的行程需求日益增长。然而，当前的基准测试往往依赖于不现实的模拟数据，未能反映LLM生成的行程与现实世界行程之间的差异。现有的评估指标主要侧重于约束条件，未能全面评估旅游计划的整体质量。为进一步解决这些问题，我们提出了TripTailor，一个专门针对实际场景个性化旅游规划的基准数据集。该数据集包含超过500,000个真实世界的兴趣点（POIs）和近4,000份多样化的旅游行程，附有详细信息，提供了一个更为真实的评估框架。实验结果显示，最新最先进的LLMs生成的不足10%的行程达到了人类水平的表现。此外，我们还识别出旅游规划中的几个关键挑战，包括解决方案的可行性、合理性以及个性化定制。我们希望TripTailor能够推动能够理解并满足用户需求，并生成实用行程的旅游规划代理的发展。相关代码和数据集可在以下链接获取。', 'title_zh': 'TripTailor: 个性化旅行规划的现实世界基准'}
{'arxiv_id': 'arXiv:2508.01324', 'title': 'Towards Evaluation for Real-World LLM Unlearning', 'authors': 'Ke Miao, Yuke Hu, Xiaochen Li, Wenjie Bao, Zhihao Liu, Zhan Qin, Kui Ren', 'link': 'https://arxiv.org/abs/2508.01324', 'abstract': 'This paper analyzes the limitations of existing unlearning evaluation metrics in terms of practicality, exactness, and robustness in real-world LLM unlearning scenarios. To overcome these limitations, we propose a new metric called Distribution Correction-based Unlearning Evaluation (DCUE). It identifies core tokens and corrects distributional biases in their confidence scores using a validation set. The evaluation results are quantified using the Kolmogorov-Smirnov test. Experimental results demonstrate that DCUE overcomes the limitations of existing metrics, which also guides the design of more practical and reliable unlearning algorithms in the future.', 'abstract_zh': '基于分布校正的遗忘评估（DCUE）：实景观点下大语言模型遗忘评估的新方法', 'title_zh': '面向现实世界的大型语言模型遗忘评估'}
{'arxiv_id': 'arXiv:2508.01306', 'title': 'PUZZLED: Jailbreaking LLMs through Word-Based Puzzles', 'authors': 'Yelim Ahn, Jaejin Lee', 'link': 'https://arxiv.org/abs/2508.01306', 'abstract': "As large language models (LLMs) are increasingly deployed across diverse domains, ensuring their safety has become a critical concern. In response, studies on jailbreak attacks have been actively growing. Existing approaches typically rely on iterative prompt engineering or semantic transformations of harmful instructions to evade detection. In this work, we introduce PUZZLED, a novel jailbreak method that leverages the LLM's reasoning capabilities. It masks keywords in a harmful instruction and presents them as word puzzles for the LLM to solve. We design three puzzle types-word search, anagram, and crossword-that are familiar to humans but cognitively demanding for LLMs. The model must solve the puzzle to uncover the masked words and then proceed to generate responses to the reconstructed harmful instruction. We evaluate PUZZLED on five state-of-the-art LLMs and observe a high average attack success rate (ASR) of 88.8%, specifically 96.5% on GPT-4.1 and 92.3% on Claude 3.7 Sonnet. PUZZLED is a simple yet powerful attack that transforms familiar puzzles into an effective jailbreak strategy by harnessing LLMs' reasoning capabilities.", 'abstract_zh': '大规模语言模型（LLMs）日益广泛应用于各类领域，确保其安全性已成为关键问题。为应对这一挑战，关于牢笼攻击的研究正在不断增长。现有方法通常依赖于迭代提示工程或有害指令的语义转换以逃避检测。在此工作中，我们提出了PUZZLED，一种新颖的牢笼攻击方法，利用LLM的推理能力。该方法隐身有害指令中的关键词，并将其呈现为字谜供LLM求解。我们设计了三种熟悉的谜题类型——字搜、乱词和填字——这些谜题对人类来说是熟悉的，但对LLM来说认知上更具挑战性。模型必须解决谜题以揭示被隐身的词，然后生成针对重构的有害指令的回答。我们评估了PUZZLED在五个先进的LLM上的效果，并观察到高平均攻击成功率（ASR）达88.8%，具体来说，在GPT-4.1上的成功率是96.5%，在Claude 3.7 Sonnet上的成功率是92.3%。PUZZLED是一种简单而强大的攻击方法，通过利用LLM的推理能力，将熟悉的谜题转化为有效的牢笼攻击策略。', 'title_zh': 'PUZZLED：通过词谜破解LLMs'}
{'arxiv_id': 'arXiv:2508.01300', 'title': 'How Far Are LLMs from Symbolic Planners? An NLP-Based Perspective', 'authors': "Ma'ayan Armony, Albert Meroño-Peñuela, Gerard Canal", 'link': 'https://arxiv.org/abs/2508.01300', 'abstract': "The reasoning and planning abilities of Large Language Models (LLMs) have been a frequent topic of discussion in recent years. Their ability to take unstructured planning problems as input has made LLMs' integration into AI planning an area of interest. Nevertheless, LLMs are still not reliable as planners, with the generated plans often containing mistaken or hallucinated actions. Existing benchmarking and evaluation methods investigate planning with LLMs, focusing primarily on success rate as a quality indicator in various planning tasks, such as validating plans or planning in relaxed conditions. In this paper, we approach planning with LLMs as a natural language processing (NLP) task, given that LLMs are NLP models themselves. We propose a recovery pipeline consisting of an NLP-based evaluation of the generated plans, along with three stages to recover the plans through NLP manipulation of the LLM-generated plans, and eventually complete the plan using a symbolic planner. This pipeline provides a holistic analysis of LLM capabilities in the context of AI task planning, enabling a broader understanding of the quality of invalid plans. Our findings reveal no clear evidence of underlying reasoning during plan generation, and that a pipeline comprising an NLP-based analysis of the plans, followed by a recovery mechanism, still falls short of the quality and reliability of classical planners. On average, only the first 2.65 actions of the plan are executable, with the average length of symbolically generated plans being 8.4 actions. The pipeline still improves action quality and increases the overall success rate from 21.9% to 27.5%.", 'abstract_zh': '大型语言模型（LLMs）的推理与规划能力近年来是一个频繁讨论的话题。尽管LLMs能够处理非结构化的规划问题，使其在人工智能规划中的应用成为一个研究热点，但现有的规划结果仍然不够可靠，生成的计划中经常包含错误或虚构的动作。现有的基准测试和评估方法主要关注规划任务的成功率作为质量指标，研究LLMs在规划中的应用。在本文中，鉴于LLMs本身就是自然语言处理（NLP）模型，我们将LLMs用于规划视为一个NLP任务，并提出了一种恢复管道，包括基于NLP的生成计划评估，以及通过NLP操纵LLM生成的计划的三个阶段，最终使用符号规划器完成计划。该管道为LLMs在AI任务规划中的能力提供了全面分析，有助于更广泛地理解无效计划的质量。研究发现，在计划生成过程中没有明显证据表明存在底层推理，而包含基于NLP分析计划和随后的恢复机制的管道仍然无法达到经典规划器的质量和可靠性。平均而言，只有计划中的前2.65个动作可执行，符号生成计划的平均长度为8.4个动作。该管道仍能提高动作质量并将总体成功率从21.9%提升到27.5%。', 'title_zh': 'LLMs与符号规划器之间相差多远？一个基于NLP的观点'}
{'arxiv_id': 'arXiv:2508.01273', 'title': 'KCR: Resolving Long-Context Knowledge Conflicts via Reasoning in LLMs', 'authors': 'Xianda Zheng, Zijian Huang, Meng-Fen Chiang, Michael J. Witbrock, Kaiqi Zhao', 'link': 'https://arxiv.org/abs/2508.01273', 'abstract': 'Knowledge conflicts commonly arise across diverse sources, and their prevalence has increased with the advent of LLMs. When dealing with conflicts between multiple contexts, also known as \\emph{inter-context knowledge conflicts}, LLMs are often confused by lengthy and conflicting contexts. To address this challenge, we propose the Knowledge Conflict Reasoning (KCR) framework, which enhances the ability of LLMs to resolve conflicting knowledge. The key idea of KCR is to train backbone LLMs to establish a correct reasoning process by rewarding them for selecting and adhering to the context with stronger logical consistency when presented with conflicting contexts. Specifically, we first extract reasoning paths, represented by either text or local knowledge graphs, from the conflicting long contexts. Subsequently, we employ Reinforcement Learning to encourage the model to learn the paradigm of reasoning process that follows correct reasoning paths rather than the incorrect counterparts. This enables the backbone models to genuinely acquire the capability to resolve inter-context knowledge conflicts within long contexts. Experimental results demonstrate that our framework significantly improves the ability of various backbone models to resolve knowledge conflicts in long-context scenarios, yielding substantial performance gains.', 'abstract_zh': '知识冲突在多样化来源中常见，随着大规模语言模型（LLMs）的出现，冲突的频率有所增加。在处理多个上下文之间的冲突，即所谓的“跨上下文知识冲突”时，LLMs 经常会被冗长且矛盾的上下文所困惑。为应对这一挑战，我们提出了知识冲突推理（KCR）框架，以增强LLMs解决知识冲突的能力。KCR的核心思想是通过奖励LLMs选择并遵循逻辑一致性更强的上下文，以训练其建立正确的推理过程。具体而言，我们首先从矛盾的长上下文中提取由文本或局部知识图谱表示的推理路径，然后利用强化学习鼓励模型遵循正确的推理路径，而不是错误的路径。这使得基础模型能够真正获得在长上下文场景中解决跨上下文知识冲突的能力。实验结果表明，我们的框架显著提升了各种基础模型在长上下文场景中解决知识冲突的能力，取得了显著的性能提升。', 'title_zh': 'KCR: 通过LLM中的推理解决长期上下文知识冲突'}
{'arxiv_id': 'arXiv:2508.01268', 'title': 'Win-k: Improved Membership Inference Attacks on Small Language Models', 'authors': 'Roya Arkhmammadova, Hosein Madadi Tamar, M. Emre Gursoy', 'link': 'https://arxiv.org/abs/2508.01268', 'abstract': "Small language models (SLMs) are increasingly valued for their efficiency and deployability in resource-constrained environments, making them useful for on-device, privacy-sensitive, and edge computing applications. On the other hand, membership inference attacks (MIAs), which aim to determine whether a given sample was used in a model's training, are an important threat with serious privacy and intellectual property implications. In this paper, we study MIAs on SLMs. Although MIAs were shown to be effective on large language models (LLMs), they are relatively less studied on emerging SLMs, and furthermore, their effectiveness decreases as models get smaller. Motivated by this finding, we propose a new MIA called win-k, which builds on top of a state-of-the-art attack (min-k). We experimentally evaluate win-k by comparing it with five existing MIAs using three datasets and eight SLMs. Results show that win-k outperforms existing MIAs in terms of AUROC, TPR @ 1% FPR, and FPR @ 99% TPR metrics, especially on smaller models.", 'abstract_zh': '小语言模型上的成员推理攻击研究：一种基于min-k的新方法Win-k', 'title_zh': 'Win-k: 改进的小语言模型成员推断攻击'}
{'arxiv_id': 'arXiv:2508.01261', 'title': 'Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models', 'authors': 'Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat', 'link': 'https://arxiv.org/abs/2508.01261', 'abstract': 'We present MoE-MLA-RoPE, a novel architecture combination that combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE) for efficient language modeling. Our approach addresses the fundamental trade-off between model capacity and computational efficiency through three key innovations: (1) fine-grained expert routing with 64 micro-experts and top-$k$ selection, enabling flexible specialization through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation that dedicates 2 always active experts for common patterns while routing to 6 of 62 specialized experts; and (3) gradient-conflict-free load balancing that maintains expert utilization without interfering with primary loss optimization.\nExtensive experiments on models ranging from 17M to 202M parameters demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla transformers while using 42% fewer active parameters per forward pass. FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x inference acceleration. Automated evaluation using GPT-4 as a judge confirms quality improvements in generation, with higher scores on coherence (8.1/10), creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment.', 'abstract_zh': 'MoE-MLA-RoPE: 一种结合Mixture of Experts、Multi-head Latent Attention和Rotary Position Embeddings的新型架构组合', 'title_zh': '统一专家混合模型与多头潜在注意力机制以构建高效的语言模型'}
{'arxiv_id': 'arXiv:2508.01203', 'title': "Importance Sampling is All You Need: Predict LLM's performance on new benchmark by reusing existing benchmark", 'authors': 'Junjie Shi, Wei Ma, Shi Ying, Lingxiao Jiang, Yang liu, Bo Du', 'link': 'https://arxiv.org/abs/2508.01203', 'abstract': 'With the rapid advancement of large language models , code generation has become a key benchmark for evaluating LLM capabilities. However, existing benchmarks face two major challenges: (1) the escalating cost of constructing high-quality test suites and reference solutions, and (2) the increasing risk of data contamination, which undermines the reliability of benchmark-based evaluations. In this paper, we propose BIS, a prompt-centric evaluation framework that enables ground-truth-free prediction of LLM performance on code generation tasks. Rather than executing generated code, BIS estimates performance metrics by analyzing the prompt distribution alone. Built on importance sampling theory and implemented using Importance Weighted Autoencoders, our method reweights samples from existing annotated benchmarks to estimate performance on new, unseen benchmarks. To stabilize the estimation, we introduce weight truncation strategies and compute marginal expectations across the fitted distributions. BIS serves as a complementary tool that supports benchmark development and validation under constrained resources, offering actionable and quick feedback for prompt selection and contamination assessment. We conduct extensive experiments involving 8,000 evaluation points across 4 CodeLlama models and 9 diverse benchmarks. Our framework achieves an average absolute prediction error of 1.1% for code correctness scores, with best- and worst-case errors of 0.3% and 1.9%, respectively. It also generalizes well to other metrics, attaining average absolute errors of 2.15% for pass@1. These results demonstrate the reliability and broad applicability of BIS, which can significantly reduce the cost and effort of benchmarking LLMs in code-related tasks.', 'abstract_zh': '基于提示的代码生成评估框架：BIS', 'title_zh': '重要性采样足矣：通过复用现有基准预测LLM在新基准上的性能'}
{'arxiv_id': 'arXiv:2508.01191', 'title': 'Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens', 'authors': 'Chengshuai Zhao, Zhen Tan, Pingchuan Ma, Dawei Li, Bohan Jiang, Yancheng Wang, Yingzhen Yang, Huan Liu', 'link': 'https://arxiv.org/abs/2508.01191', 'abstract': 'Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.', 'abstract_zh': 'Chain-of-Thought 推理通过数据分布视角研究：其有效性受训练数据与测试查询分布差异的限制', 'title_zh': 'LLMs的链式思考推理是否仅为幻象？一种数据分布视角'}
{'arxiv_id': 'arXiv:2508.01057', 'title': 'REACT: A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System', 'authors': 'Fengze Yang, Bo Yu, Yang Zhou, Xuewen Luo, Zhengzhong Tu, Chenxi Liu', 'link': 'https://arxiv.org/abs/2508.01057', 'abstract': 'Collisions caused by human error are the most common type of multi-vehicle crash, highlighting the critical need for autonomous driving (AD) systems to leverage cooperative perception through Vehicle-to-Everything (V2X) communication. This capability extends situational awareness beyond the limitations of onboard sensors. However, current transformer-based V2X frameworks suffer from limited generalization, shallow contextual reasoning, and reliance on mono-modal inputs. Vision-Language Models (VLMs) offer enhanced reasoning and multimodal integration but typically fall short of real-time performance requirements in safety-critical applications. This paper presents REACT, a real-time, V2X-integrated trajectory optimization framework built upon a fine-tuned lightweight VLM. REACT integrates a set of specialized modules that process multimodal inputs into optimized, risk-aware trajectories. To ensure real-time performance on edge devices, REACT incorporates edge adaptation strategies that reduce model complexity and accelerate inference. Evaluated on the DeepAccident benchmark, REACT achieves state-of-the-art performance, a 77% collision rate reduction, a 48.2% Video Panoptic Quality (VPQ), and a 0.57-second inference latency on the Jetson AGX Orin. Ablation studies validate the contribution of each input, module, and edge adaptation strategy. These results demonstrate the feasibility of lightweight VLMs for real-time edge-based cooperative planning and showcase the potential of language-guided contextual reasoning to improve safety and responsiveness in autonomous driving.', 'abstract_zh': '由人为错误引起的碰撞是多车事故中最常见的一类，突显了自动驾驶系统利用车辆到万物（V2X）通信进行协同感知的至关重要的需求。这一能力能够超越车载传感器的局限性，扩展情境感知。然而，当前基于变换器的V2X框架存在泛化能力有限、浅层上下文推理以及单模态输入依赖等问题。视觉-语言模型（VLMs）提供了增强的推理和多模态集成能力，但在关键安全应用中通常无法满足实时性能要求。本文提出了REACT，这是一个基于微调的轻量级VLM构建的实时、V2X集成轨迹优化框架，REACT集成了多个专门模块，将多模态输入转化为优化的风险感知轨迹。为了确保边缘设备上的实时性能，REACT融入了边缘自适应策略，减少模型复杂性和加速推理。在DeepAccident基准上进行评估，REACT实现了最先进的性能，碰撞率降低了77%，视频全景质量（VPQ）提高了48.2%，在Jetson AGX Orin上的推理延迟为0.57秒。消融研究验证了每种输入、模块和边缘自适应策略的贡献。这些结果展示了轻量级VLMs在实时边缘基于协同规划中的可行性，并突显了语言引导的上下文推理在提高自动驾驶的安全性和响应性方面的作用。', 'title_zh': 'REACT：基于边缘AI的实时车联网事故避免框架'}
{'arxiv_id': 'arXiv:2508.01031', 'title': 'CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent', 'authors': 'Jingzhe Ni, Xiaolong Yin, Xintong Li, Xingyu Lu, Ji Wei, Ruofeng Tong, Min Tang, Peng Du', 'link': 'https://arxiv.org/abs/2508.01031', 'abstract': "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing but typically requires a high level of expertise from designers. To lower the entry barrier and improve design efficiency, we present an agent for CAD conceptual design powered by large language models (LLMs). The agent accepts both abstract textual descriptions and freehand sketches as input, engaging in interactive dialogue with users to refine and clarify design requirements through comprehensive requirement analysis. Built upon a novel Context-Independent Imperative Paradigm (CIP), the agent generates high-quality CAD modeling code. During the generation process, the agent incorporates iterative visual feedback to improve model quality. Generated design cases are stored in a structured knowledge base, enabling continuous improvement of the agent's code generation capabilities. Experimental results demonstrate that our method achieves state-of-the-art performance in CAD code generation.", 'abstract_zh': '基于大型语言模型的CAD概念设计代理', 'title_zh': 'CADDesigner: 基于通用型代理的概念设计CAD模型'}
{'arxiv_id': 'arXiv:2508.01012', 'title': 'AutoEDA: Enabling EDA Flow Automation through Microservice-Based LLM Agents', 'authors': 'Yiyi Lu, Hoi Ian Au, Junyao Zhang, Jingyu Pan, Yiting Wang, Ang Li, Jianyi Zhang, Yiran Chen', 'link': 'https://arxiv.org/abs/2508.01012', 'abstract': 'Modern Electronic Design Automation (EDA) workflows, especially the RTL-to-GDSII flow, require heavily manual scripting and demonstrate a multitude of tool-specific interactions which limits scalability and efficiency. While LLMs introduces strides for automation, existing LLM solutions require expensive fine-tuning and do not contain standardized frameworks for integration and evaluation. We introduce AutoEDA, a framework for EDA automation that leverages paralleled learning through the Model Context Protocol (MCP) specific for standardized and scalable natural language experience across the entire RTL-to-GDSII flow. AutoEDA limits fine-tuning through structured prompt engineering, implements intelligent parameter extraction and task decomposition, and provides an extended CodeBLEU metric to evaluate the quality of TCL scripts. Results from experiments over five previously curated benchmarks show improvements in automation accuracy and efficiency, as well as script quality when compared to existing methods. AutoEDA is released open-sourced to support reproducibility and the EDA community. Available at: this https URL', 'abstract_zh': 'AutoEDA：一种通过并行学习实现标准化和可扩展的EDA自动化框架', 'title_zh': 'AutoEDA：基于微服务的LLM代理实现EDA流程自动化'}
{'arxiv_id': 'arXiv:2508.00914', 'title': 'Knowledge Editing for Multi-Hop Question Answering Using Semantic Analysis', 'authors': 'Dominic Simon, Rickard Ewetz', 'link': 'https://arxiv.org/abs/2508.00914', 'abstract': 'Large Language Models (LLMs) require lightweight avenues of updating stored information that has fallen out of date. Knowledge Editing (KE) approaches have been successful in updating model knowledge for simple factual queries but struggle with handling tasks that require compositional reasoning such as multi-hop question answering (MQA). We observe that existing knowledge editors leverage decompositional techniques that result in illogical reasoning processes. In this paper, we propose a knowledge editor for MQA based on semantic analysis called CHECK. Our framework is based on insights from an analogy between compilers and reasoning using LLMs. Similar to how source code is first compiled before being executed, we propose to semantically analyze reasoning chains before executing the chains to answer questions. Reasoning chains with semantic errors are revised to ensure consistency through logic optimization and re-prompting the LLM model at a higher temperature. We evaluate the effectiveness of CHECK against five state-of-the-art frameworks on four datasets and achieve an average 22.8% improved MQA accuracy.', 'abstract_zh': '大型语言模型需要轻量级的途径来更新过时的存储信息。基于语义分析的知识编辑方法CHECK用于多跳问答任务，解决现有知识编辑方法在处理需要组成性推理的任务时遇到的困难。我们提出的框架借鉴了编译器与使用大型语言模型进行推理之间的类比。类似于源代码首先会被编译然后再执行，我们提出在执行推理链之前对其执行语义分析。具有语义错误的推理链通过逻辑优化和以较高温度重新 prompting 大型语言模型来修正，以确保一致性。我们在四个数据集上将 CHECK 与五个最先进的框架进行对比评估，并取得平均 22.8% 的多跳问答准确性提升。', 'title_zh': '使用语义分析进行多跳问答的知识编辑'}
{'arxiv_id': 'arXiv:2508.00902', 'title': 'An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models', 'authors': 'Kenneth Payne', 'link': 'https://arxiv.org/abs/2508.00902', 'abstract': "Judgment of risk is key to decision-making under uncertainty. As Daniel Kahneman and Amos Tversky famously discovered, humans do so in a distinctive way that departs from mathematical rationalism. Specifically, they demonstrated experimentally that humans accept more risk when they feel themselves at risk of losing something than when they might gain. I report the first tests of Kahneman and Tversky's landmark 'prospect theory' with Large Language Models, including today's state of the art chain-of-thought 'reasoners'.\nIn common with humans, I find that prospect theory often anticipates how these models approach risky decisions across a range of scenarios. I also demonstrate that context is key to explaining much of the variance in risk appetite. The 'frame' through which risk is apprehended appears to be embedded within the language of the scenarios tackled by the models. Specifically, I find that military scenarios generate far larger 'framing effects' than do civilian settings, ceteris paribus. My research suggests, therefore, that language models the world, capturing our human heuristics and biases. But also that these biases are uneven - the idea of a 'frame' is richer than simple gains and losses. Wittgenstein's notion of 'language games' explains the contingent, localised biases activated by these scenarios. Finally, I use my findings to reframe the ongoing debate about reasoning and memorisation in LLMs.", 'abstract_zh': '风险判断是不确定性环境下决策的关键。丹尼尔·卡内曼和阿莫斯·特维斯基 famously 发现，人类在进行风险判断时以一种与数学理性主义相悖的方式。具体而言，他们在实验中证明，人们在害怕损失时愿意承担更大的风险，而在可能获利时则更保守。我利用大型语言模型首次检验了卡内曼和特维斯基的里程碑式“前景理论”，包括当今最先进的链式思考“推理器”。与人类相似，我发现前景理论经常预测这些模型在各种情境下的风险管理方式。我还展示了上下文是解释模型风险偏好差异的关键。风险通过模型处理的场景语言框架来理解。特别地，我发现在其他条件相同的情况下，军事场景产生的“框架效应”比民用环境大得多。我的研究建议，语言模型模仿了人类的直觉和偏差，但这些偏差是不均衡的——“框架”的概念比简单的得失更为丰富。维特根斯坦的“语言游戏”概念解释了这些情境激活的偶然性、地方性偏差。最后，我利用研究结果重新审视关于LLMs推理和记忆的持续争论。', 'title_zh': 'AI决策下的风险分析：前景理论在大型语言模型中 Emerges'}
{'arxiv_id': 'arXiv:2508.00890', 'title': 'AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks', 'authors': 'Fali Wang, Hui Liu, Zhenwei Dai, Jingying Zeng, Zhiwei Zhang, Zongyu Wu, Chen Luo, Zhen Li, Xianfeng Tang, Qi He, Suhang Wang', 'link': 'https://arxiv.org/abs/2508.00890', 'abstract': 'Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.', 'abstract_zh': '测试时计算优化缩放（TTS）通过推理时分配额外计算资源来提升大型语言模型（LLM）的表现，但在多阶段复杂任务中，现有研究主要关注单阶段任务；而许多现实世界问题是由一系列异构子任务组成，每个子任务需要特定能力的LLM。因此，我们研究了一个新的问题：多阶段复杂任务中的测试时计算最优缩放，旨在选择合适的模型并为每个子任务分配预算以最大化整体性能。多阶段任务中的TTS引入了两个基本挑战：（i）模型和预算分配的组合搜索空间与高推理成本使得穷举搜索不切实际。（ii）跨子任务的最优模型和预算分配相互依存，增加了计算最优搜索的复杂性。为解决这一差距，我们跨六个数据集在四项任务上进行了广泛的试点实验，得出了三个经验洞见，描述了大型语言模型在多阶段复杂任务中的行为。基于这些洞见，我们提出了基于大型语言模型代理的AgentTTS框架，通过迭代的反馈驱动交互自主搜索计算最优分配。实验结果表明，AgentTTS在搜索效率上显著优于传统和其他基于大型语言模型的方法，并且对不同的训练集大小具有更好的鲁棒性，且具有增强的可解释性。', 'title_zh': 'AgentTTS: 大型语言模型代理用于复杂任务测试时计算 optimal 缩放策略'}
{'arxiv_id': 'arXiv:2508.02611', 'title': 'Meta-RAG on Large Codebases Using Code Summarization', 'authors': 'Vali Tawosia, Salwa Alamir, Xiaomo Liu, Manuela Veloso', 'link': 'https://arxiv.org/abs/2508.02611', 'abstract': 'Large Language Model (LLM) systems have been at the forefront of applied Artificial Intelligence (AI) research in a multitude of domains. One such domain is software development, where researchers have pushed the automation of a number of code tasks through LLM agents. Software development is a complex ecosystem, that stretches far beyond code implementation and well into the realm of code maintenance. In this paper, we propose a multi-agent system to localize bugs in large pre-existing codebases using information retrieval and LLMs. Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation. We then use an LLM agent to determine which parts of the codebase are critical for bug resolution, i.e. bug localization. We demonstrate the usefulness of Meta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores 84.67 % and 53.0 % for file-level and function-level correct localization rates, respectively, achieving state-of-the-art performance.', 'abstract_zh': '大规模语言模型（LLM）系统在多个领域应用人工智能研究中处于前沿。其中，软件开发领域研究人员通过LLM代理自动化了大量编码任务。软件开发是一个复杂的生态系统，远远超出了代码实现的范围，延伸到了代码维护的领域。本文提出了一种基于信息检索和LLM的多代理系统，用于在大型现成代码库中定位错误。该系统引入了一种新颖的检索增强生成（RAG）方法，Meta-RAG，在此方法中，我们利用摘要将代码库平均压缩79.8%，形成一个紧凑、结构化的自然语言表示。然后使用LLM代理确定代码库中哪些部分对于错误定位至关重要。我们通过使用SWE-bench Lite数据集进行评估，展示了Meta-RAG的有效性。Meta-RAG在文件级和函数级正确定位率分别为84.67%和53.0%，达到了最先进的性能。', 'title_zh': '在大规模代码库中使用代码总结的Meta-RAG'}
{'arxiv_id': 'arXiv:2508.02601', 'title': 'StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis in Low-Data Regimes', 'authors': 'Siyi Liu, Yujia Zheng, Yongqi Zhang', 'link': 'https://arxiv.org/abs/2508.02601', 'abstract': "The application of machine learning on tabular data in specialized domains is severely limited by data scarcity. While generative models offer a solution, traditional methods falter in low-data regimes, and recent Large Language Models (LLMs) often ignore the explicit dependency structure of tabular data, leading to low-fidelity synthetics. To address these limitations, we introduce StructSynth, a novel framework that integrates the generative power of LLMs with robust structural control. StructSynth employs a two-stage architecture. First, it performs explicit structure discovery to learn a Directed Acyclic Graph (DAG) from the available data. Second, this learned structure serves as a high-fidelity blueprint to steer the LLM's generation process, forcing it to adhere to the learned feature dependencies and thereby ensuring the generated data respects the underlying structure by design. Our extensive experiments demonstrate that StructSynth produces synthetic data with significantly higher structural integrity and downstream utility than state-of-the-art methods. It proves especially effective in challenging low-data scenarios, successfully navigating the trade-off between privacy preservation and statistical fidelity.", 'abstract_zh': '结构化合成：一种结合大语言模型生成能力和稳健结构控制的新型框架', 'title_zh': 'StructSynth：在数据稀缺情况下利用大规模语言模型进行结构感知表数据合成'}
{'arxiv_id': 'arXiv:2508.02584', 'title': 'MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification', 'authors': 'Ming Pok Ng, Junqi Jiang, Gabriel Freedman, Antonio Rago, Francesca Toni', 'link': 'https://arxiv.org/abs/2508.02584', 'abstract': 'Leveraging outputs from multiple large language models (LLMs) is emerging as a method for harnessing their power across a wide range of tasks while mitigating their capacity for making errors, e.g., hallucinations. However, current approaches to combining insights from multiple LLMs often involve unstructured interactions (e.g., free debate), resulting in model generations that are not faithfully justifiable. In this work, we introduce MArgE, a novel framework to provide formal structure to the evidence from each LLM, in the form of a tree of extracted arguments, for the task of claim verification. We use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks and semantics from the field of computational argumentation, to construct structured argument trees for given claims. This process creates an inspectable pathway from the initial arguments to the final claim verification decisions, providing a faithful justification thereof. We show experimentally that MArgE can significantly outperform single LLMs, including three open-source models (4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior methods for unstructured multi-LLM debates. We thus demonstrate the advantages of incorporating formal, argumentative reasoning mechanisms when combining multiple LLM outputs.', 'abstract_zh': '利用多个大规模语言模型（LLMs）的输出来利用其在广泛任务上的强大能力，同时减轻其生成错误（如幻觉）的风险：一种新的框架MArgE', 'title_zh': 'MArgE: 多个大型语言模型的论证证据网格化以实现可验证的声明验证'}
{'arxiv_id': 'arXiv:2508.02574', 'title': 'EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare', 'authors': 'Eman Alamoudi, Ellis Solaiman', 'link': 'https://arxiv.org/abs/2508.02574', 'abstract': 'Arabic-language patient feedback remains under-analysed because dialect diversity and scarce aspect-level sentiment labels hinder automated assessment. To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that merges ChatGPT pseudo-labelling with targeted human review to build the first explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence is annotated with an aspect and sentiment label (positive, negative, or neutral), forming a pioneering Arabic dataset aligned with healthcare themes, with ChatGPT-generated rationales provided for each label to enhance transparency. To evaluate the impact of annotation quality on model performance, we created three versions of the training data: a fully supervised set with all labels reviewed by humans, a semi-supervised set with 50% human review, and an unsupervised set with only machine-generated labels. We fine-tuned two transformer models on these datasets for both aspect and sentiment classification. Experimental results show that our Arabic-specific model achieved high accuracy even with minimal human supervision, reflecting only a minor performance drop when using ChatGPT-only labels. Reducing the number of aspect classes notably improved classification metrics across the board. These findings demonstrate an effective, scalable approach to Arabic aspect-based sentiment analysis (SA) in healthcare, combining large language model annotation with human expertise to produce a robust and explainable dataset. Future directions include generalisation across hospitals, prompt refinement, and interpretable data-driven modelling.', 'abstract_zh': '阿拉伯语言患者反馈分析不足，因为方言多样性和稀缺的方面级情感标签阻碍了自动化评估。为解决这一问题，我们引入了EHSAN，这是一种数据为中心的混合管道，结合了ChatGPT伪标签和有针对性的人工审查，构建了首个可解释的阿拉伯方面基于情感医疗数据集。每个句子都标注了方面和情感标签（正面、负面或中性），形成了与医疗主题对齐的开创性阿拉伯语数据集，并为每个标签提供了ChatGPT生成的解释，以增强透明度。为了评估注释质量对模型性能的影响，我们创建了三种训练数据版本：一个完全监督集，其中所有标签都由人工审核；一个半监督集，其中50%的标签由人工审核；一个未监督集，仅由机器生成的标签。我们在这些数据集上对两个变压器模型进行了微调，用于方面和情感分类。实验结果表明，我们的阿拉伯特定模型即使在有限的人工监督下也能实现高准确性，仅使用ChatGPT标签时，性能仅略有下降。减少方面类别的数量显著提高了总体分类指标。这些发现展示了结合大规模语言模型注释和人工专业知识的有效可扩展方法，以构建稳健且可解释的阿拉伯方面基于情感分析（SA）数据集，未来方向包括跨医院的泛化、提示 refinement 和可解释的数据驱动建模。', 'title_zh': 'EHSAN: 结合ChatGPT的一种混合框架在医疗保健领域进行阿拉伯语方面情感分析'}
{'arxiv_id': 'arXiv:2508.02506', 'title': 'Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms', 'authors': 'Xiaowei Yuan, Lei Jin, Haoxin Zhang, Yan Gao, Yi Wu, Yao Hu, Ziyang Huang, Jun Zhao, Kang Liu', 'link': 'https://arxiv.org/abs/2508.02506', 'abstract': 'Retrieval-augmented generation (RAG) plays a critical role in user-generated content (UGC) platforms, but its effectiveness depends heavily on accurate relevance assessment of query-document pairs. Despite recent advances in applying large language models (LLMs) to relevance modeling, UGC platforms present unique challenges: 1) ambiguous user intent due to sparse user feedback in RAG scenarios, and 2) substantial noise introduced by informal and unstructured language. To address these issues, we propose the Reinforced Reasoning Model for Relevance Assessment (R3A), which introduces a decomposed reasoning framework over queries and candidate documents before scoring. R3A first leverages auxiliary high-ranked documents within the platform to infer latent query intent. It then performs verbatim fragment extraction to justify relevance decisions, thereby reducing errors caused by noisy UGC. Based on a reinforcement learning framework, R3A is optimized to mitigate distortions arising from ambiguous queries and unstructured content. Experimental results show that R3A significantly outperforms existing baseline methods in terms of relevance accuracy, across both offline benchmarks and online experiments.', 'abstract_zh': '检索增强生成（RAG）在用户生成内容（UGC）平台中扮演着关键角色，但其效果高度依赖于查询文档对的相关性评估准确性。尽管在将大规模语言模型（LLMs）应用于相关性建模方面取得了近期进展，UGC平台仍面临独特挑战：1）由于在RAG场景中用户反馈稀疏导致的模棱两可的用户意图，以及2）由非正式和无结构语言引入的巨大噪声。为解决这些问题，我们提出了增强推理模型用于相关性评估（R3A），该模型在评分前引入了查询和候选文档的分解推理框架。R3A 首先利用平台内的高排名辅助文档来推断潜在的查询意图。然后进行逐字片段提取来验证相关性决策，从而减少由UGC噪声引起的错误。基于强化学习框架，R3A 优化以减轻模棱两可查询和无结构内容引起的失真。实验结果表明，R3A 在相关性准确性方面显著优于现有基线方法，在离线基准测试和在线实验中均有更佳表现。', 'title_zh': '基于强化学习的分解推理在UGC平台的相关性评估'}
{'arxiv_id': 'arXiv:2508.02442', 'title': 'Assessing the Reliability and Validity of Large Language Models for Automated Assessment of Student Essays in Higher Education', 'authors': "Andrea Gaggioli, Giuseppe Casaburi, Leonardo Ercolani, Francesco Collova', Pietro Torre, Fabrizio Davide", 'link': 'https://arxiv.org/abs/2508.02442', 'abstract': "This study investigates the reliability and validity of five advanced Large Language Models (LLMs), Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, and Mistral 24B, for automated essay scoring in a real world higher education context. A total of 67 Italian-language student essays, written as part of a university psychology course, were evaluated using a four-criterion rubric (Pertinence, Coherence, Originality, Feasibility). Each model scored all essays across three prompt replications to assess intra-model stability. Human-LLM agreement was consistently low and non-significant (Quadratic Weighted Kappa), and within-model reliability across replications was similarly weak (median Kendall's W < 0.30). Systematic scoring divergences emerged, including a tendency to inflate Coherence and inconsistent handling of context-dependent dimensions. Inter-model agreement analysis revealed moderate convergence for Coherence and Originality, but negligible concordance for Pertinence and Feasibility. Although limited in scope, these findings suggest that current LLMs may struggle to replicate human judgment in tasks requiring disciplinary insight and contextual sensitivity. Human oversight remains critical when evaluating open-ended academic work, particularly in interpretive domains.", 'abstract_zh': '本研究探讨了在实际高等教育环境下，Claude 3.5、DeepSeek v2、Gemini 2.5、GPT-4 和 Mistral 24B 等五种高级语言模型（LLMs）自动作文评分的可靠性和有效性。共有 67 篇意大利语学生作文（作为大学心理学课程的一部分）根据四个标准评分准则（相关性、连贯性、独创性、可行性）进行评估。每个模型对所有论文进行了三次提示复述评分，以评估模型内稳定性。人机一致性较低且不显著（Quadratic Weighted Kappa），复述间同一模型的可靠性也较弱（中位 Kendall’s W < 0.30）。系统评分差异明显，包括倾向于夸大连贯性以及对上下文依赖维度处理不一致。跨模型一致性分析显示，连贯性和独创性存在适度趋同，而相关性和可行性几乎不存在一致性。尽管范围有限，但这些发现表明，当前的语言模型在需要学科洞见和情境敏感性的任务中可能难以复制人类判断。在评估开放性学术作品时，特别是在解释性领域，人工监督仍然至关重要。', 'title_zh': '评估大型语言模型在高等教育中自动评估学生作文可靠性和有效性的方法'}
{'arxiv_id': 'arXiv:2508.02401', 'title': 'CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation', 'authors': 'Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang', 'link': 'https://arxiv.org/abs/2508.02401', 'abstract': 'Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs.\nTo address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: this https URL.', 'abstract_zh': '最近大型语言模型（LLMs）在长上下文处理方面的进展显著提升。然而，关键值（KV）缓存大小的增加对内存和执行效率提出了关键挑战。大多数KV缓存压缩方法依赖于组查询注意（GQA）基模型中的所有注意头的启发式令牌移除策略。该方法忽略了注意头的不同功能，导致重要令牌被移除，从而降低了LLMs的性能。\n\n为了解决上述问题，我们不采用先前工作中的方法在GQA基模型中使用所有注意头来确定重要令牌，而是首先在每个层中识别不仅能够检索提示的起始和结束令牌，而且能够检索文本中的重要令牌并关注它们周围语义上下文的注意头。随后，我们利用这些头来确定重要令牌并保留其对应的KV缓存对。此外，我们分别分析了每层的缓存移除误差，并引入了一种层自适应KV缓存分配策略。实验结果表明，在LongBench和Needle-in-a-Haystack基准测试下的各种内存预算下，提出的CompressKV性能上始终优于现有方法。我们的代码可在以下链接获取：this https URL。', 'title_zh': 'CompressKV: 语义检索头在生成前知道哪些_token_不重要'}
{'arxiv_id': 'arXiv:2508.02343', 'title': 'MicroMix: Efficient Mixed-Precision Quantization with Microscaling Formats for Large Language Models', 'authors': 'Wenyuan Liu, Haoqian Meng, Yilun Luo, Peng Zhang, Xindian Ma', 'link': 'https://arxiv.org/abs/2508.02343', 'abstract': "Quantization significantly accelerates inference in large language models (LLMs) by replacing original high-precision matrices with low-precision counterparts. Recent advances in weight-activation quantization have primarily focused on mapping both weights and activations to the INT4 format. Although the new FP4 Tensor Cores in NVIDIA's Blackwell architecture offer up to 4x speedup over FP16, existing INT4-based kernels fail to fully exploit this capability due to mismatched data formats. To bridge this gap, we propose MicroMix, a co-designed mixed-precision quantization algorithm and matrix multiplication kernel based on Microscaling (MX) data formats. Tailored for the Blackwell architecture, the MicroMix kernel supports arbitrary combinations of MXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a favorable trade-off between accuracy and efficiency for each linear layer, we introduce quantization thresholds that identify activation elements where lower-precision formats (MXFP4 or MXFP6) incur excessive quantization error. Our algorithm selectively allocates higher-precision channels to preserve accuracy while maintaining compute efficiency. MicroMix achieves competitive or superior performance across diverse downstream tasks, including zero-shot and few-shot learning, language modeling, code generation, and mathematical reasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX 5090) GPUs, our kernel delivers at least 20% faster execution than TensorRT-FP8. Furthermore, when applied to various Llama and Qwen models, MicroMix consistently improves prefill latency and memory efficiency across a range of batch sizes compared to TensorRT baselines. Our code is available at this https URL.", 'abstract_zh': '量化显著通过用低精度矩阵替换原始高精度矩阵来加速大型语言模型（LLMs）的推断。最近关于权重-激活量化方面的进展主要集中在将两者映射到INT4格式。尽管英伟达Blackwell架构中的新FP4张量核心相较于FP16提供了高达4倍的加速，现有的基于INT4的内核未能充分利用这一能力，原因是数据格式不匹配。为此，我们提出了一种名为MicroMix的协同设计的混合精度量化算法和矩阵乘法内核，基于Microscaling（MX）数据格式。为适应Blackwell架构，MicroMix内核支持MXFP4、MXFP6和MXFP8通道的任意组合，并生成BFloat16输出。为在每个线性层中实现准确性和效率的良好权衡，我们引入了量化阈值来识别低精度格式（MXFP4或MXFP6）会导致过度量化误差的激活元素。该算法选择性地分配更高精度的通道以保持准确性同时保持计算效率。MicroMix在包括零样本学习、少样本学习、语言建模、代码生成和数学推理等多种下游任务中取得了具有竞争力或更优的表现。在消费级（RTX 5070Ti笔记本电脑）和服务器级（RTX 5090）GPU上，我们的内核执行速度比TensorRT-FP8至少快20%。此外，当应用于各种Llama和Qwen模型时，MicroMix在不同批量大小下相对于TensorRT基线始终能提高预填充延迟和内存效率。我们的代码可在以下链接获取。', 'title_zh': 'MicroMix: 适用于大型语言模型的高效混合精度量化与微缩格式'}
{'arxiv_id': 'arXiv:2508.02317', 'title': 'VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo', 'authors': 'Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu', 'link': 'https://arxiv.org/abs/2508.02317', 'abstract': 'Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. %\nWe present \\veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \\veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. %\nUsing \\veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.', 'abstract_zh': '近期大规模语言模型的进步推动了全方位模态理解和生成的显著进展。然而，训练全方位模态的大规模语言模型仍然面临显著挑战，需要处理多种模态的异构模型架构，这要求高效的系统设计以支持大规模训练。现有的框架通常将模型定义与并行逻辑相结合，导致可扩展性有限并增加了端到端全方位模态训练的大量工程开销。\n\n我们提出\\veomni，一种模块化且高效的训练框架，以加速全方位模态的大规模语言模型的发展。\\veomni引入以模型为中心的分布式食谱，将通信从计算中分离出来，从而在全方位模态的大规模语言模型上实现高效的三维并行化。\\veomni还具备灵活的配置接口，支持通过最小的代码更改无缝集成新模态。\n\n使用\\veomni，一个具有300亿参数的全方位模态专家混合模型可以在每秒每GPU超过2800个token的吞吐量下进行训练，并通过128个GPU的三维并行化扩展到16万上下文长度，展示了其在训练大规模全方位模态的大规模语言模型方面的卓越效率和扩展性。', 'title_zh': 'VeOmni：以模型为中心的分布式食谱动物园，面向任何模态模型训练'}
{'arxiv_id': 'arXiv:2508.02312', 'title': 'A Survey on Data Security in Large Language Models', 'authors': 'Kang Chen, Xiuze Zhou, Yuanguo Lin, Jinhe Su, Yuanhui Yu, Li Shen, Fan Lin', 'link': 'https://arxiv.org/abs/2508.02312', 'abstract': 'Large Language Models (LLMs), now a foundation in advancing natural language processing, power applications such as text generation, machine translation, and conversational systems. Despite their transformative potential, these models inherently rely on massive amounts of training data, often collected from diverse and uncurated sources, which exposes them to serious data security risks. Harmful or malicious data can compromise model behavior, leading to issues such as toxic output, hallucinations, and vulnerabilities to threats such as prompt injection or data poisoning. As LLMs continue to be integrated into critical real-world systems, understanding and addressing these data-centric security risks is imperative to safeguard user trust and system reliability. This survey offers a comprehensive overview of the main data security risks facing LLMs and reviews current defense strategies, including adversarial training, RLHF, and data augmentation. Additionally, we categorize and analyze relevant datasets used for assessing robustness and security across different domains, providing guidance for future research. Finally, we highlight key research directions that focus on secure model updates, explainability-driven defenses, and effective governance frameworks, aiming to promote the safe and responsible development of LLM technology. This work aims to inform researchers, practitioners, and policymakers, driving progress toward data security in LLMs.', 'abstract_zh': '大型语言模型（LLMs）：数据安全风险与防御策略综述', 'title_zh': '大型语言模型中的数据安全综述'}
{'arxiv_id': 'arXiv:2508.02298', 'title': 'CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment', 'authors': 'Guofu Xie, Yunsheng Shi, Hongtao Tian, Ting Yao, Xiao Zhang', 'link': 'https://arxiv.org/abs/2508.02298', 'abstract': 'Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of Large Language Models (LLMs) by using rule-based binary feedback, helping to mitigate reward hacking. However, current RLVR methods typically treat whole responses as single actions, assigning the same reward to every token. This coarse-grained feedback hampers precise credit assignment, making it hard for models to identify which reasoning steps lead to success or failure, and often results in suboptimal policies and inefficient learning. Methods like PPO provide credit assignment through value estimation, but often yield inaccurate and unverifiable signals due to limited sampling. On the other hand, methods using Process Reward Models can provide step-by-step judgments for each reasoning step, but they require high-quality process supervision labels and are time-consuming when applied in online reinforcement learning (RL). To overcome these limitations, we introduce a simple but efficient method Credit Assignment Policy Optimization (CAPO). Given a reasoning response rollout from the policy model, CAPO directly leverages an off-the-shelf, general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to generate all step-wise critique by one pass, thereby providing verifiable token-level rewards to refine the tokens that were originally assigned identical rule-based rewards. This enables more fine-grained credit assignment in an effective way. Furthermore, to enhance the accuracy and robustness of CAPO, we employ voting mechanisms that scale with the number of generated critiques. Extensive experiments using different backbones like Llama and Qwen models and in different sizes show that CAPO consistently outperforms supervised learning-based and RL-based fine-tuning methods across six challenging mathematical benchmarks and three out-of-domain benchmarks.', 'abstract_zh': '可验证奖励的强化学习（RLVR）通过使用基于规则的二元反馈提升了大型语言模型（LLMs）的推理能力，有助于缓解奖励劫持问题。然而，当前的RLVR方法通常将整个响应视为单一动作，并为每个tokens分配相同的奖励，这种粗粒度的反馈阻碍了精确的信用分配，使得模型难以识别哪些推理步骤导致成功或失败，经常导致次优策略和低效学习。虽然像PPO这样的方法通过价值估计提供信用分配，但由于采样限制，往往会产生不准确且不可验证的信号。另一方面，使用过程奖励模型的方法可以为每个推理步骤提供逐步判断，但它们需要高质量的过程监督标签，并且在在线强化学习（RL）中应用时耗时。为克服这些限制，我们提出了一种简单且高效的信用分配策略优化（CAPO）方法。给定策略模型的推理响应滚动，CAPO 直接利用现成的一般用途LLM作为生成过程奖励模型（LLM-as-GenPRM）来一次性生成所有步骤评判，从而提供可验证的token级奖励以精炼原本被分配相同规则奖励的tokens，这以有效的方式实现了更精细的信用分配。此外，为了提高CAPO的准确性和稳健性，我们采用了可扩展的投票机制。使用不同的骨干模型（如Llama和Qwen）在不同规模下进行的广泛实验表明，CAPO在六个具有挑战性的数学基准和三个跨域基准上始终优于基于监督学习和基于强化学习的微调方法。', 'title_zh': 'CAPO: 向通过可验证的生成式责任指派增强大语言模型推理方向'}
{'arxiv_id': 'arXiv:2508.02260', 'title': 'Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning', 'authors': 'Jia Deng, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2508.02260', 'abstract': 'Recently, reinforcement learning with verifiable rewards (RLVR) has been widely used for enhancing the reasoning abilities of large language models (LLMs). A core challenge in RLVR involves managing the exchange between entropy and performance of policies. Despite the importance of this exchange, a fine-grained understanding of when and how this exchange operates most effectively remains limited. To bridge this gap, we conduct a systematic empirical analysis of the entropy-performance exchange mechanism of RLVR across different levels of granularity. Specifically, we first divide the training process into two distinct stages based on entropy dynamics, i.e., rising stage and plateau stage, and then systematically investigate how this mechanism varies across stage-level, instance-level, and token-level granularitiess. Our analysis reveals that, in the rising stage, entropy reduction in negative samples facilitates the learning of effective reasoning patterns, which in turn drives rapid performance gains. Moreover, in the plateau stage, learning efficiency strongly correlates with high-entropy tokens present in low-perplexity samples and those located at the end of sequences. Motivated by these findings, we propose two methods that dynamically adjust the reward signal using perplexity and positional information to focus RL updates on tokens that exhibit high learning potential, achieving improvements compared to the baseline methods on various LLMs.', 'abstract_zh': '最近，具有可验证奖励的强化学习（RLVR）在提升大语言模型（LLM）的推理能力方面得到了广泛应用。RLVR 中熵与性能交换机制的管理是一项核心挑战。尽管这一交换的重要性已经认识到，但对其最有效运作的时点和方式的细微理解仍然有限。为弥补这一不足，我们针对不同粒度层次系统地分析了 RLVR 的熵-性能交换机制。具体地，我们首先根据熵的动力学将训练过程分为上升阶段和平台阶段，然后系统地研究了该机制在不同阶段层次、实例层次和令牌层次上的变化。我们的分析表明，在上升阶段，负面样本中熵的降低有助于高效学习有效的推理模式，从而促进性能的快速提升。此外，在平台阶段，高效学习与低困惑度样本中高熵令牌以及序列末尾的令牌高度相关。基于这些发现，我们提出了一种方法，动态调整奖励信号，利用困惑度和位置信息，将 RL 更新集中在具有高学习潜力的令牌上，各种 LLM 上相比基线方法取得了性能改进。', 'title_zh': '分解熵-性能交换：解锁有效强化学习的缺失关键因素'}
{'arxiv_id': 'arXiv:2508.02222', 'title': 'FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries and Rich Relevance in Financial Chinese Passage Retrieval', 'authors': 'Xuan Xu, Beilin Chu, Qinhong Lin, Yixiao Zhong, Fufang Wen, Jiaqi Liu, Binjie Fei, Yu Li, Zhongliang Yang, Linna Zhou', 'link': 'https://arxiv.org/abs/2508.02222', 'abstract': 'In recent years, large language models (LLMs) have demonstrated significant potential in constructing passage retrieval datasets. However, existing methods still face limitations in expressing cross-doc query needs and controlling annotation quality. To address these issues, this paper proposes a bidirectional generation pipeline, which aims to generate 3-level hierarchical queries for both intra-doc and cross-doc scenarios and mine additional relevance labels on top of direct mapping annotation. The pipeline introduces two query generation methods: bottom-up from single-doc text and top-down from multi-doc titles. The bottom-up method uses LLMs to disassemble and generate structured queries at both sentence-level and passage-level simultaneously from intra-doc passages. The top-down approach incorporates three key financial elements--industry, topic, and time--to divide report titles into clusters and prompts LLMs to generate topic-level queries from each cluster. For relevance annotation, our pipeline not only relies on direct mapping annotation from the generation relationship but also implements an indirect positives mining method to enrich the relevant query-passage pairs. Using this pipeline, we constructed a Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k Chinese financial research reports, which includes hierarchical queries and rich relevance labels. Through evaluations of mined relevance labels, benchmarking and training experiments, we assessed the quality of FinCPRG and validated its effectiveness as a passage retrieval dataset for both training and benchmarking.', 'abstract_zh': '近年来，大规模语言模型（LLMs）在构建篇章检索数据集方面展现了显著潜力。然而，现有方法在表达跨文档查询需求和控制注释质量方面仍然存在局限。为了解决这些问题，本文提出了一种双向生成管道，旨在为 intra-doc 和跨文档场景生成 3 层级查询，并在此基础上挖掘附加的相关性标签。该管道引入了两种查询生成方法：从单文档文本自底向上生成和从多文档标题自顶向下生成。自底向上的方法使用 LLMs 同时从跨文档段落中分解和生成结构化查询，自顶向下的方法结合了三个关键的金融要素——行业、主题和时间，将报告标题划分为簇，并提示 LLMs 从每个簇中生成主题级查询。对于相关性注释，我们的管道不仅依赖于生成关系中的直接映射注释，还实施了一种间接正样本挖掘方法，以丰富相关查询-段落对。利用该管道，我们从近 1300 份中文财务研究报告中构建了一个金融篇章检索生成数据集（FinCPRG），其中包括层级查询和丰富的相关性标签。通过评估挖掘的相关性标签、基准测试和训练实验，我们评估了 FinCPRG 的质量，并验证了其作为篇章检索数据集在训练和基准测试中的有效性。', 'title_zh': 'FinCPRG: 一种用于金融中文段落检索的双向生成管道，实现层级查询和丰富的相关性'}
{'arxiv_id': 'arXiv:2508.02215', 'title': 'LeanK: Learnable K Cache Channel Pruning for Efficient Decoding', 'authors': 'Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu', 'link': 'https://arxiv.org/abs/2508.02215', 'abstract': 'Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at this https URL.', 'abstract_zh': '基于学习的方法LeanK通过利用静态通道稀疏性修剪不重要的键缓存通道，实现高效的大语言模型长上下文任务处理', 'title_zh': 'LeanK: 学习型K缓存通道剪枝以实现高效解码'}
{'arxiv_id': 'arXiv:2508.02209', 'title': 'Balancing Information Accuracy and Response Timeliness in Networked LLMs', 'authors': 'Yigit Turkmen, Baturalp Buyukates, Melih Bastopcu', 'link': 'https://arxiv.org/abs/2508.02209', 'abstract': 'Recent advancements in Large Language Models (LLMs) have transformed many fields including scientific discovery, content generation, biomedical text mining, and educational technology. However, the substantial requirements for training data, computational resources, and energy consumption pose significant challenges for their practical deployment. A promising alternative is to leverage smaller, specialized language models and aggregate their outputs to improve overall response quality. In this work, we investigate a networked LLM system composed of multiple users, a central task processor, and clusters of topic-specialized LLMs. Each user submits categorical binary (true/false) queries, which are routed by the task processor to a selected cluster of $m$ LLMs. After gathering individual responses, the processor returns a final aggregated answer to the user. We characterize both the information accuracy and response timeliness in this setting, and formulate a joint optimization problem to balance these two competing objectives. Our extensive simulations demonstrate that the aggregated responses consistently achieve higher accuracy than those of individual LLMs. Notably, this improvement is more significant when the participating LLMs exhibit similar standalone performance.', 'abstract_zh': '近期大规模语言模型的发展已经改变了包括科学发现、内容生成、生物医学文本挖掘和教育技术等多个领域。然而，大量的训练数据需求、计算资源和能源消耗导致它们的实际部署面临重大挑战。一种有前景的替代方案是利用较小的专业化语言模型并将它们的输出汇总，以提高整体响应质量。本文研究了一个由多个用户、中心任务处理器和主题专业化语言模型集群组成的网络化语言模型系统。每个用户提交分类二元（真/假）查询，由任务处理器路由到选定的$m$个语言模型集群之一。在收集个体响应后，处理器返回最终汇总的答案给用户。我们在此设置中表征信息准确性和响应及时性，并制定了一个联合优化问题以平衡这两个相互竞争的目标。我们的大量模拟表明，汇总的响应始终比单一语言模型具有更高的准确性。值得注意的是，当参与的语言模型在独立运行时表现出相似性能时，这种改进更为显著。', 'title_zh': '在网络化大语言模型中平衡信息准确性和响应及时性'}
{'arxiv_id': 'arXiv:2508.02208', 'title': 'Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems', 'authors': 'Yebo Peng, Zixiang Liu, Yaoming Li, Zhizhuo Yang, Xinye Xu, Bowen Ye, Weijun Yuan, Zihan Wang, Tong Yang', 'link': 'https://arxiv.org/abs/2508.02208', 'abstract': "Evaluating the mathematical capability of Large Language Models (LLMs) is a critical yet challenging frontier. Existing benchmarks fall short, particularly for proof-centric problems, as manual creation is unscalable and costly, leaving the true mathematical abilities of LLMs largely unassessed. To overcome these barriers, we propose Proof2Hybrid, the first fully automated framework that synthesizes high-quality, proof-centric benchmarks from natural language mathematical corpora. The key novelty of our solution is Proof2X, a roadmap of converting mathematical proofs into various kinds of questions that are easy to verify. Instructed by this roadmap, we propose a new type of hybrid-formatted questions, named ``$m$-out-of-$n$ multiple judge questions'', specifically designed to enable robust, automatic evaluation while being resilient to guessing and superficial pattern matching inherent in traditional formats. As a demonstration of our framework, we introduce AlgGeoTest, a benchmark for algebraic geometry--a frontier domain of modern mathematics--comprising 456 challenging items. Our extensive evaluations on state-of-the-art LLMs using AlgGeoTest reveal profound deficits in their comprehension of algebraic geometry, providing a more precise measure of their true mathematical capabilities. Our framework and benchmark pave the way for a new wave of in-depth research into the mathematical intelligence of AI systems.", 'abstract_zh': '评估大型语言模型的数学能力是一项关键但具有挑战性的前沿任务。现有的基准测试在这方面存在不足，尤其是在证明导向的问题上，手工创建此类基准既不具扩展性也不经济，导致大型语言模型的真正数学能力未能得到充分评估。为克服这些障碍，我们提出了一种全新的全自动框架Proof2Hybrid，该框架能够从自然语言数学语料中合成高质量的、证明导向的基准测试集。我们解决方案的核心创新是Proof2X，这是一种将数学证明转换为各种可验证问题的地图。根据此地图，我们提出了新的混合格式问题类型“$m$-out-of-$n$多位法官问题”，旨在实现稳健的自动评估，同时抵御传统格式中固有的猜测和表面模式匹配。作为我们框架的演示，我们引入了AlgGeoTest，这是一个涵盖456个挑战性问题的代数几何基准测试集，它是现代数学的一个前沿领域。通过对最新大型语言模型使用AlgGeoTest进行广泛评估，我们揭示了其在代数几何方面深刻的理解缺陷，提供了对其真实数学能力的更精确衡量。我们的框架和基准为深入研究人工智能系统的数学智能开辟了新途径。', 'title_zh': 'Proof2Hybrid: 自动数学基准合成用于以证明为中心的问题'}
{'arxiv_id': 'arXiv:2508.02189', 'title': 'Learning Dynamics of Meta-Learning in Small Model Pretraining', 'authors': 'David Demitri Africa, Yuval Weiss, Paula Buttery, Richard Diehl Martinez', 'link': 'https://arxiv.org/abs/2508.02189', 'abstract': 'Large language models are powerful but costly. We ask whether meta-learning can make the pretraining of small language models not only better but also more interpretable. We integrate first-order MAML with subset-masked LM pretraining, producing four LLama-style decoder-only models (11M-570M params), and evaluate it on a fundamental NLP task with many settings and real-world applications. Compared with vanilla training, our model (i) reaches the same loss up to 1.6x sooner, (ii) improves F1 on multilingual Universal NER under equal compute, and (iii) makes the training dynamics easy to read: first the network\'s representations fan out ("diversify") and later they collapse into a smaller, shared subspace ("compress"). This two-stage shift shows up as a rise-and-fall in both effective-rank curves and attention-head entropy. The same curves pinpoint which layers specialise earliest and which later reconverge, giving a compact, interpretable signature of meta-adaptation. Code, checkpoints and WandB logs are released.', 'abstract_zh': '大语言模型既强大又昂贵。我们探讨元学习是否可以使小型语言模型的预训练不仅更好，而且更具可解释性。我们将一阶MAML与子集掩码LM预训练集成，生成四种Llama风格的解码器-only模型（参数量从11M到570M），并在多项基本NLP任务和实际应用中进行了评估。与常规训练相比，我们的模型：(i) 在同等计算资源下达到相同损失的时间快1.6倍；(ii) 在同等计算资源下多语言通用命名实体识别的F1值更高；(iii) 使训练动态易于理解：首先网络表示分散（“多样化”），随后压缩到一个较小的共享子空间（“压缩”）。这种两阶段转变在有效秩曲线和注意力头熵中表现为先上升后下降。相同的曲线还可以指出哪一层最早专业化，哪一层较晚收敛，从而提供一个简洁的元适应解释签名。代码、检查点和WandB日志均已发布。', 'title_zh': '小模型预训练中元学习的学习动态'}
{'arxiv_id': 'arXiv:2508.02128', 'title': 'Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models', 'authors': 'Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang', 'link': 'https://arxiv.org/abs/2508.02128', 'abstract': 'In the era of large language models (LLMs), N:M sparsity has emerged as a structured compression technique critical for accelerating inference. While prior work has primarily focused on weight sparsity, it often suffers from significant accuracy degradation. Activation sparsity, though promising, is typically training-dependent and faces challenges in generalization. To address these limitations, we introduce Amber Pruner, a training-free N:M activation sparsity method designed specifically for the prefill stage, targeting the acceleration of linear projection layers in LLMs. Extensive experiments across multiple models and sparsity ratios (2:4, 4:8, and 8:16) demonstrate that Amber Pruner can effectively sparsify and accelerate more than 55% of linear computations without requiring model retraining. To further enhance generality and efficiency, we propose Outstanding-sparse, a unified framework that integrates Amber Pruner with post-training W8A8 quantization. Our approach preserves strong performance across a range of downstream tasks, with notable advantages in generative tasks. This work pioneers a new frontier in activation sparsity, providing foundational insights that are poised to guide the co-evolution of algorithms and architectures in the design of next-generation AI systems.', 'abstract_zh': '在大规模语言模型时代，N:M稀疏性作为一种结构化压缩技术，已成为加速推断的关键技术。尽管早期工作主要关注权重稀疏性，但它常常导致显著的精度下降。激活稀疏性虽然有前景，但通常依赖于训练且在泛化方面面临挑战。为解决这些局限性，我们引入了Amber Pruner，这是一种无需训练的N:M激活稀疏性方法，专门针对预填充阶段，旨在加速大规模语言模型中的线性投影层。通过在多个模型和不同稀疏比（2:4、4:8和8:16）下进行广泛实验，证明Amber Pruner可以在无需模型重新训练的情况下有效地稀疏化和加速超过55%的线性计算。为进一步增强通用性和效率，我们提出了一个统一框架Outstanding-sparse，该框架将Amber Pruner与后训练的W8A8量化相结合。我们的方法在各种下游任务中保持了强大的性能，在生成任务方面尤为突出。这项工作为激活稀疏性开辟了新前沿，提供了基础见解，有助于指导下一代AI系统中算法和架构的共同进化。', 'title_zh': '琥珀剪枝器：利用N:M激活稀疏性提高大型语言模型预填充效率'}
{'arxiv_id': 'arXiv:2508.02096', 'title': 'Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches', 'authors': 'Raj Mahmud, Yufeng Wu, Abdullah Bin Sawad, Shlomo Berkovsky, Mukesh Prasad, A. Baki Kocaballi', 'link': 'https://arxiv.org/abs/2508.02096', 'abstract': 'Conversational Recommender Systems (CRSs) are receiving growing research attention across domains, yet their user experience (UX) evaluation remains limited. Existing reviews largely overlook empirical UX studies, particularly in adaptive and large language model (LLM)-based CRSs. To address this gap, we conducted a systematic review following PRISMA guidelines, synthesising 23 empirical studies published between 2017 and 2025. We analysed how UX has been conceptualised, measured, and shaped by domain, adaptivity, and LLM.\nOur findings reveal persistent limitations: post hoc surveys dominate, turn-level affective UX constructs are rarely assessed, and adaptive behaviours are seldom linked to UX outcomes. LLM-based CRSs introduce further challenges, including epistemic opacity and verbosity, yet evaluations infrequently address these issues. We contribute a structured synthesis of UX metrics, a comparative analysis of adaptive and nonadaptive systems, and a forward-looking agenda for LLM-aware UX evaluation. These findings support the development of more transparent, engaging, and user-centred CRS evaluation practices.', 'abstract_zh': '会话推荐系统（CRSs）在各领域内受到越来越多的研究关注，但其用户体验（UX）评价仍显不足。现有的综述大多忽略了实证UX研究，尤其是在适应性和大型语言模型（LLM）基础上的CRSs。为弥补这一不足，我们遵循PRISMA指南，系统综述了2017年至2025年间发表的23项实证研究。我们分析了UX在不同领域、适应性和LLM基础上的概念化、测量和形成方式。我们的研究发现表明存在持续性的局限性：事后调查主导，对话层级的情感UX结构鲜有评估，适应行为与UX结果的联系也不常见。基于LLM的CRS引入了额外的挑战，包括知识不透明性和冗余性，但评价这些挑战的频率较低。我们贡献了一个结构化的UX度量综合分析、适应性和非适应性系统的比较分析以及对LLM感知的UX评价的前瞻议程。这些发现支持发展更透明、更具吸引力和以用户为中心的CRS评价实践。', 'title_zh': '基于经典方法与大语言模型驱动方法的系统评价：对话式推荐系统中用户体验的评估'}
{'arxiv_id': 'arXiv:2508.02092', 'title': 'FPEdit: Robust LLM Fingerprinting through Localized Knowledge Editing', 'authors': 'Shida Wang, Chaohu Liu, Yubo Wang, Linli Xu', 'link': 'https://arxiv.org/abs/2508.02092', 'abstract': 'Large language models represent significant investments in computation, data, and engineering expertise, making them extraordinarily valuable intellectual assets. Nevertheless, these AI assets remain vulnerable to unauthorized redistribution and commercial exploitation through fine-tuning or black-box deployment. Current fingerprinting approaches face a fundamental trade-off: intrinsic methods require full parameter access, while backdoor-based techniques employ statistically anomalous triggers easily detected and filtered by adversaries. To address these limitations, we introduce FPEdit, a novel knowledge-editing framework that injects semantically coherent natural language fingerprints by modifying a sparse subset of model weights. This ensures stealthy and precise ownership encoding without degrading the core functionality. Extensive experiments show that FPEdit achieves $95$-$100\\%$ fingerprint retention under both full-parameter fine-tuning and parameter-efficient adaptation, while preserving performance on 24 downstream benchmarks. Moreover, FPEdit remains robust under quantization, pruning, and stochastic decoding, and can embed 10 fingerprint pairs into LLaMA2-7B in under 10 minutes using less than 32 GB of GPU memory, a $70\\%$ reduction in resource requirements compared to existing techniques. These advances establish FPEdit as the first fingerprinting approach to simultaneously achieve robustness against adaptation, resistance to detection, and preservation of model utility, providing a minimally invasive solution for reliable provenance verification of large language models in adversarial deployment scenarios.', 'abstract_zh': '大规模语言模型在计算、数据和工程专业知识方面投入巨大，是极其宝贵的智力资产。然而，这些AI资产仍然容易通过微调或黑盒部署被未经授权重分发和商业化利用。当前的指纹识别方法面临一个基本的权衡：内在方法需要全参数访问，而后门基方法使用的统计异常触发器容易被对手检测和过滤。为解决这些局限性，我们提出了FPEdit，这是一种新的知识编辑框架，通过修改模型权重的一个稀疏子集注入语义连贯的自然语言指纹。这确保了隐蔽且精确的所有权编码，同时不损害核心功能。实验表明，FPEdit在全参数微调和参数高效适应下分别实现了95%-100%的指纹保留率，同时在24个下游基准上保持性能。此外，FPEdit在量化、剪枝和随机解码下依然稳健，并能在不到10分钟内使用不到32 GB的GPU内存将10对指纹嵌入到LLaMA2-7B中，与现有技术相比资源需求减少了70%。这些进展使FPEdit成为首个同时实现适应鲁棒性、抗检测性和模型实用性保存的指纹识别方法，为对抗部署场景中的大型语言模型可靠起源验证提供了微创解决方案。', 'title_zh': 'FPEdit：通过局部知识编辑实现的鲁棒LLM指纹识别'}
{'arxiv_id': 'arXiv:2508.02079', 'title': 'AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization', 'authors': 'Amitava Das, Abhilekh Borah, Vinija Jain, Aman Chadha', 'link': 'https://arxiv.org/abs/2508.02079', 'abstract': 'Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models (LLMs). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian overlap -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation.', 'abstract_zh': 'AlignGuard-LoRA: 一种在微调过程中保持一致性的原理性框架', 'title_zh': 'AlignGuard-LoRA: 基于Fisher引导分解和黎曼测地线碰撞正则化的对齐保护微调'}
{'arxiv_id': 'arXiv:2508.02066', 'title': 'MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs', 'authors': 'Guojiang Zhao, Sihang Li, Zixiang Lu, Zheng Cheng, Haitao Lin, Lirong Wu, Hanchen Xia, Hengxing Cai, Wentao Guo, Hongshuai Wang, Mingjun Xu, Siyu Zhu, Guolin Ke, Linfeng Zhang, Zhifeng Gao', 'link': 'https://arxiv.org/abs/2508.02066', 'abstract': "Large Language Models(LLMs) have demonstrated remarkable performance across various domains, yet their capabilities in molecular reasoning remain insufficiently explored. Current approaches tend to rely heavily on general-purpose prompting, which lacks domain-specific molecular semantics, while those that use fine-tuning strategies often face challenges with interpretability and reasoning depth. To address these issues, we introduce MolReasoner, a two-stage framework designed to transition LLMs from memorization towards chemical reasoning. First, we propose Mol-SFT, which initializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT) samples generated by GPT-4o and verified for chemical accuracy. Subsequently, Mol-RL applies reinforcement learning with specialized reward functions designed explicitly to align chemical structures with linguistic descriptions, thereby enhancing molecular reasoning capabilities. Our approach notably enhances interpretability, improving the model 's molecular understanding and enabling better generalization. Extensive experiments demonstrate that MolReasoner outperforms existing methods, and marking a significant shift from memorization-based outputs to robust chemical reasoning.", 'abstract_zh': '大型语言模型（LLMs）在各个领域显示出了卓越的表现，但在分子推理方面的能力尚未得到充分探索。现有的方法通常依赖于通用提示，缺乏专门的分子语义，而使用微调策略的方法往往面临解释性和推理深度的挑战。为了解决这些问题，我们引入了MolReasoner，这是一个两阶段框架，旨在引导LLMs从记忆向化学推理过渡。首先，我们提出了Mol-SFT，通过由GPT-4o生成并经过化学准确性验证的合成Chain-of-Thought（CoT）样本，初始化模型的推理能力。随后，Mol-RL应用了强化学习，并设计了专门的奖励函数，以明确地将化学结构与语言描述对齐，从而增强分子推理能力。我们的方法显著提高了模型的可解释性，提高了其对分子的理解能力，并促进了更好的推广。广泛实验表明，MolReasoner优于现有方法，并标志着从基于记忆的输出向稳健的化学推理的显著转变。', 'title_zh': 'MolReasoner: 向有效的可解释分子LLMs推理方向努力'}
{'arxiv_id': 'arXiv:2508.02037', 'title': 'Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time', 'authors': 'Huihan Li, You Chen, Siyuan Wang, Yixin He, Ninareh Mehrabi, Rahul Gupta, Xiang Ren', 'link': 'https://arxiv.org/abs/2508.02037', 'abstract': 'Large Language Models (LLMs) perform well on reasoning benchmarks but often fail when inputs alter slightly, raising concerns about the extent to which their success relies on memorization. This issue is especially acute in Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger intermediate errors that cascade into incorrect final answers. We introduce STIM, a novel framework for Source-aware Token-level Identification of Memorization, which attributes each token in a reasoning chain to one of multiple memorization sources - local, mid-range, or long-range - based on their statistical co-occurrence with the token in the pretraining corpus. Our token-level analysis across tasks and distributional settings reveals that models rely more on memorization in complex or long-tail cases, and that local memorization is often the dominant driver of errors, leading to up to 67% of wrong tokens. We also show that memorization scores from STIM can be effective in predicting the wrong tokens in the wrong reasoning step. STIM offers a powerful tool for diagnosing and improving model reasoning and can generalize to other structured step-wise generation tasks.', 'abstract_zh': '大型语言模型（LLMs）在推理基准测试中表现良好，但在输入稍有改变时往往会出现失败，这引发了对其成功程度依赖于记忆化的担忧。这一问题在链式推理（Chain-of-Thought, CoT）中尤为严重，因为无用的记忆化模式可能会触发中间错误，进而导致最终答案错误。我们提出了STIM框架，这是一种基于源的标记级别记忆化识别框架，根据预训练语料中标记的统计共现情况，将推理链中的每个标记归因于多种记忆化来源之一——局部、中程或远程。我们在不同任务和分布设置下的标记级别分析表明，模型在复杂或长尾情况下更多地依赖于记忆化，且局部记忆化往往是错误的主导驱动因素，可能导致高达67%的错误标记。此外，我们还展示了STIM的记忆化分数在预测错误推理步骤中的错误标记方面具有有效性。STIM提供了一种强大的诊断工具，可以改进模型的推理能力，并能适用于其他结构化步骤生成任务。', 'title_zh': '逐词诊断链式推理中的记忆化现象'}
{'arxiv_id': 'arXiv:2508.02029', 'title': 'Confidence-Diversity Calibration of AI Judgement Enables Reliable Qualitative Coding', 'authors': 'Zhilong Zhao, Yindi Liu', 'link': 'https://arxiv.org/abs/2508.02029', 'abstract': "LLMs enable qualitative coding at large scale, but assessing the reliability of their output remains challenging in domains where human experts seldom agree. Analysing 5,680 coding decisions from eight state-of-the-art LLMs across ten thematic categories, we confirm that a model's mean self-confidence already tracks inter-model agreement closely (Pearson r=0.82). Adding model diversity-quantified as the normalised Shannon entropy of the panel's votes-turns this single cue into a dual signal that explains agreement almost completely (R^2=0.979). The confidence-diversity duo enables a three-tier workflow that auto-accepts 35% of segments with <5% audit-detected error and routes the remainder for targeted human review, cutting manual effort by up to 65%. Cross-domain replication on six public datasets spanning finance, medicine, law and multilingual tasks confirms these gains (kappa improvements of 0.20-0.78). Our results establish a generalisable, evidence-based criterion for calibrating AI judgement in qualitative research.", 'abstract_zh': '大型语言模型能够在大规模范围内实现 qualitative 代码化，但在人类专家很少达成一致的领域，评估其输出的可靠性仍具挑战性。通过对八种先进大型语言模型在十个主题类别中的5,680个编码决策进行分析，我们确认模型的平均自我信心已与模型间一致性密切相关（皮尔逊相关系数r=0.82）。通过量化模型多样性——即面板投票的正规化香农熵——这单一线索转化为几乎完全解释一致性的双信号（R²=0.979）。自我信心-多样性 duo 使得一个三层次的工作流成为可能，自动接受<5% 审计检测错误的片段，其余部分则定向进行人类审查，最多可减少65%的手动努力。跨领域在涵盖金融、医学、法律和多语言任务的六个公共数据集上进行复制，证实了这些收益（κ改进值为0.20-0.78）。研究结果为定性研究中 AI 判断的校准建立了一般适用且基于证据的标准。', 'title_zh': 'AI判断的置信-多样性校准 enables 可靠的定性编码'}
{'arxiv_id': 'arXiv:2508.01977', 'title': 'TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models', 'authors': 'Fan Gao, Cheng Huang, Nyima Tashi, Yutong Liu, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng, Hao Wang, Yongbin Yu', 'link': 'https://arxiv.org/abs/2508.01977', 'abstract': 'To address the severe data scarcity in Tibetan, a low-resource language spoken by over six million people, we introduce TIBSTC-CoT, the large-scale, multi-domain Tibetan dataset automatically constructed via chain-of-thought prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable and reproducible framework for dataset creation in low-resource settings, covering diverse domains and reasoning patterns essential for language understanding and generation. Building on this dataset, we develop the Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with chain-of-thought capabilities. Trained entirely on TIBSTC-CoT, Sunshine-thinking has demonstrated strong reasoning and generation performance, comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a significant step toward inclusive AI by enabling high-quality Tibetan language processing through both resource creation and model innovation. All data are available: this https URL.', 'abstract_zh': '为解决藏语这一低资源语言所面临的严重数据稀缺问题，藏语TIBSTC-CoT数据集应运而生，该数据集是通过大型语言模型（LLMs）的链式思考提示方法自动构建的大规模、多领域藏语语料库。TIBSTC-CoT为低资源环境下的数据集创建奠定了可扩展和可复现的框架，涵盖了语言理解和生成所需的各种领域和推理模式。基于该数据集，我们开发了Sunshine-thinking LLM家族，这一系列以藏语为中心的大型语言模型配备了链式思考能力。Sunshine-thinking完全在TIBSTC-CoT上训练，展示了强大的推理和生成性能，与最先进的多语言大型语言模型（SOTA）相当。我们的工作标志着包容性人工智能的重要一步，通过资源创建和模型创新，使高质量的藏语处理成为可能。所有数据均可获取：[此链接]。', 'title_zh': 'TIBSTC-CoT：用于语言模型链式推理的多域指令数据集'}
{'arxiv_id': 'arXiv:2508.01969', 'title': 'Accelerating LLM Reasoning via Early Rejection with Partial Reward Modeling', 'authors': 'Seyyed Saeid Cheshmi, Azal Ahmad Khan, Xinran Wang, Zirui Liu, Ali Anwar', 'link': 'https://arxiv.org/abs/2508.01969', 'abstract': 'Large Language Models (LLMs) are increasingly relied upon for solving complex reasoning tasks in domains such as mathematics, logic, and multi-step question answering. A growing line of work seeks to improve reasoning quality by scaling inference time compute particularly through Process Reward Models (PRMs), used to reward the reasoning at intermediate steps. While effective, these methods introduce substantial computational overhead, especially when generating large numbers of solutions in parallel. In this paper, we investigate whether PRMs can be used mid-generation to provide early signals that enable the rejection of suboptimal candidates before full generation of step is complete. We introduce the hypothesis that PRMs are also Partial Reward Models, meaning that the scores they assign to partially completed reasoning step are predictive of final output quality. This allows for principled early rejection based on intermediate token-level signals. We support this hypothesis both theoretically, by proving that the risk of discarding optimal beams decreases exponentially with generation length and empirically, by demonstrating a strong correlation between partial and final rewards across multiple reward models. On math reasoning benchmarks, our method achieves up to 1.4$\\times$-9$\\times$ reduction in inference FLOPs without degrading final performance. These results suggest that early rejection is a powerful mechanism for improving the compute-efficiency of reasoning in LLMs.', 'abstract_zh': '大型语言模型（LLMs）在数学、逻辑和多步问答等领域解决复杂推理任务方面越来越依赖。现有工作通过扩展推理时间计算，特别是使用过程奖励模型（PRMs）来奖励中间步骤的推理，以提高推理质量。尽管有效，这些方法在并行生成大量解决方案时引入了大量计算开销。在这项研究中，我们探讨了在生成过程中使用PRMs是否可以提供早期信号，以便在完成整个生成之前就能拒绝次优候选。我们提出了一个假设，即PRMs也是部分奖励模型，这意味着它们为部分完成的推理步骤分配的得分可以预测最终输出质量。这允许基于中间的令牌级信号进行有原则的早期拒绝。我们通过理论证明和实验证据支持这一假设：理论上，抛弃最优射线的风险随着生成长度的增加而指数级下降；实证上，我们展示了多个奖励模型中部分奖励与最终奖励之间存在强烈的相关性。在数学推理基准测试中，我们的方法在不降低最终性能的情况下，实现了高达9倍的推理FLOPs减少。这些结果表明，早期拒绝是提高LLMs推理计算效率的强大机制。', 'title_zh': '通过部分奖励建模提前拒绝加速大语言模型推理'}
{'arxiv_id': 'arXiv:2508.01961', 'title': 'Kronecker-LoRA: hybrid Kronecker-LoRA adapters for scalable, sustainable fine-tuning', 'authors': 'Yixin Shen', 'link': 'https://arxiv.org/abs/2508.01961', 'abstract': "Fine-tuning massive pre-trained language models across many tasks demands adapters that are both parameter-efficient and highly expressive. We introduce \\textbf{Kron-LoRA}, a two-stage adapter that first factorizes each frozen linear update as a Kronecker product \\[ \\Delta W = A \\otimes B \\] and then compresses \\[ B \\in \\mathbb{R}^{d_{B2}\\times d_{B1}} \\] via an \\(r\\)-rank LoRA decomposition \\(B \\approx B_{1}B_{2}\\). By leveraging \\[ \\mathrm{rank}(A \\otimes B) \\;=\\; \\mathrm{rank}(A)\\,\\mathrm{rank}(B), \\] Kron-LoRA retains the expressivity of the update while using up to $4\\!\\times\\!$ fewer parameters than a standard rank-8 LoRA adapter. Its compact adapter matrices also quantize to 8- or 4-bit with less accuracy degradation than LoRA, enabling further memory and storage savings for on-device deployment. We benchmark on DistilBERT and Mistral-7B across five tasks (PIQA, HellaSwag, WinoGrande, ARC-Easy, ARC-Challenge) over multiple epochs of adapter-only tuning: on DistilBERT, an 840 K-parameter Kron-LoRA matches LoRA-16's performance, and on Mistral-7B, a 5.7 M-parameter Kron-LoRA rivals LoRA-8 with modest memory savings and only a 3-8\\% speed overhead. In sequential fine-tuning from ARC-Challenge to ARC-Easy, Kron-LoRA retains 55.18\\% accuracy versus 53.17\\% for LoRA-8-despite using only one-quarter of the adapter parameters-underscoring its competitive cross-task transfer performance. By uniting Kronecker structure, low-rank compression, quantization-friendliness, and by providing transparent trade-off analysis, Kron-LoRA offers a scalable, sustainable, and continual-learning-ready solution for multi-task adaptation of large language models.", 'abstract_zh': 'Kron-LoRA：结合柯西尼结构、低秩压缩和量化友好性的两阶段适配器', 'title_zh': 'Kronecker-LoRA: 混合Kronecker-LoRA适配器以实现可扩展且可持续的微调'}
{'arxiv_id': 'arXiv:2508.01930', 'title': 'Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback', 'authors': 'Tom S. Juzek, Zina B. Ward', 'link': 'https://arxiv.org/abs/2508.01930', 'abstract': 'Large Language Models (LLMs) are known to overuse certain terms like "delve" and "intricate." The exact reasons for these lexical choices, however, have been unclear. Using Meta\'s Llama model, this study investigates the contribution of Learning from Human Feedback (LHF), under which we subsume Reinforcement Learning from Human Feedback and Direct Preference Optimization. We present a straightforward procedure for detecting the lexical preferences of LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to lexical overuse by experimentally emulating the LHF procedure and demonstrating that participants systematically prefer text variants that include certain words. This lexical overuse can be seen as a sort of misalignment, though our study highlights the potential divergence between the lexical expectations of different populations -- namely LHF workers versus LLM users. Our work contributes to the growing body of research on explainable artificial intelligence and emphasizes the importance of both data and procedural transparency in alignment research.', 'abstract_zh': '大型语言模型（LLMs）Known to Overuse Certain Terms like "Delve" and "Intricate": An Investigation of the Contribution of Learning from Human Feedback Using Meta\'s Llama Model', 'title_zh': '大型语言模型中的词过度使用与对齐：来自人类反馈的影响'}
{'arxiv_id': 'arXiv:2508.01918', 'title': 'Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language', 'authors': 'Jaskaranjeet Singh, Rakesh Thakur', 'link': 'https://arxiv.org/abs/2508.01918', 'abstract': 'Despite the rapid advancement of large language models (LLMs), low-resource languages remain largely excluded from the NLP landscape. We present PunGPT2, the first fully open-source suite of Punjabi large language models, trained from scratch on a 35GB domain-diverse corpus encompassing literature, religious texts, news, and social discourse. Unlike prior multilingual approaches, PunGPT2 captures rich syntactic and morphological features unique to Punjabi through a tokenizer optimised with byte pair encoding and linguistically aligned pretraining objectives. To improve factual grounding and domain recall, we introduce Pun-RAG, a retrieval-augmented generation framework combining PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant using QLoRA, enabling robust zero-shot and instruction-following performance with significantly reduced compute needs.\nAs a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system that fuses sparse (BM25) and dense methods with quantum-inspired semantic matching. By encoding queries using amplitude-based embeddings and retrieving via quantum kernel similarity, Quantum-RAG achieves improved contextual relevance with minimal memory overhead marking the first practical integration of quantum representations in low-resource language generation. Our models significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in perplexity, factuality, and fluency. This work provides a scalable, reproducible blueprint for extending LLM capabilities to underrepresented languages and pioneers quantum-aware retrieval in low-resource NLP', 'abstract_zh': '尽管大规模语言模型取得了 rapid advancement，低资源语言仍然被 NLP 场景 largely excluded。我们提出了 PunGPT2，这是首个完全开源的旁语大规模语言模型套件，从一个包含文学、宗教文本、新闻和社会讨论的 35GB 多领域语料库中从零训练而来。与先前的多语言方法不同，PunGPT2 通过经过字节对编码优化的分词器和语义上对齐的预训练目标，捕捉到旁语特有的丰富句法和形态特征。为了提高事实接地和领域召回，我们引入了 Pun-RAG，这是一种检索增强生成框架，结合了 PunGPT2 和一个密集的 FAISS 检索器，该检索器基于精选的旁语知识库。我们进一步开发了 Pun-Instruct，这是一种使用 QLoRA 参数高效、指令调优的变体，使其在显著减少计算需求的情况下实现了稳健的零样本和指令跟随性能。作为一项关键创新，我们提出了 Quantum-RAG，这是一种新颖的混合检索系统，将稀疏（BM25）和密集方法与量子启发的语义匹配相结合。通过使用振幅基嵌入编码查询并通过量子内核相似性检索，Quantum-RAG 实现了改进的上下文相关性，并在最小内存开销的情况下实现了第一个低资源语言生成中的量子表示的实用性集成。我们的模型在困惑度、事实性和流畅性方面显著优于强大的多语言基线（mBERT、mT5、MuRIL）。这项工作为将 LLM 能力扩展到低资源语言提供了可扩展、可重现的蓝图，并开创了低资源 NLP 中量子感知检索的先河。', 'title_zh': '量子-RAG 和 PunGPT2：提高旁遮普语语言生成和检索的低资源方法'}
{'arxiv_id': 'arXiv:2508.01908', 'title': 'Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models', 'authors': 'Istabrak Abbes, Gopeshh Subbaraj, Matthew Riemer, Nizar Islah, Benjamin Therien, Tsuguchika Tabaru, Hiroaki Kingetsu, Sarath Chandar, Irina Rish', 'link': 'https://arxiv.org/abs/2508.01908', 'abstract': 'Training large language models (LLMs) typically involves pre-training on massive corpora, only to restart the process entirely when new data becomes available. A more efficient and resource-conserving approach would be continual pre-training, where models are updated with new data rather than retraining from scratch. However, the introduction of new data often causes distribution shifts, leading to performance degradation on previously learned tasks. In this paper, we take a deeper look at two popular proposals for addressing this distribution shift within the continual learning literature: experience replay and gradient alignment. We consider continual pre-training of models within the Llama family of architectures at a large scale across languages with 100 billion tokens of training data in each language, finding that both replay and gradient alignment lead to more stable learning without forgetting. This conclusion holds both as we vary the model scale and as we vary the number and diversity of tasks. Moreover, we are the first to demonstrate the effectiveness of gradient alignment techniques in the context of LLM pre-training and propose an efficient implementation of meta-experience replay (MER) that imbues experience replay with the benefits of gradient alignment despite negligible compute and memory overhead. Our scaling analysis across model sizes and replay rates indicates that small rates of replaying old examples are definitely a more valuable use of compute than investing in model size, but that it is more compute efficient to scale the size of the model than invest in high rates of replaying old examples.', 'abstract_zh': '在大型语言模型持续预训练中体验重放与梯度对齐的应用研究', 'title_zh': '重访回放与梯度对齐在大规模语言模型连续预训练中的作用'}
{'arxiv_id': 'arXiv:2508.01892', 'title': 'How Does Controllability Emerge In Language Models During Pretraining?', 'authors': 'Jianshu She, Xinyue Li, Eric Xing, Zhengzhong Liu, Qirong Ho', 'link': 'https://arxiv.org/abs/2508.01892', 'abstract': 'Language models can be steered by modifying their internal representations to control concepts such as emotion, style, or truthfulness in generation. However, the conditions for an effective intervention remain unclear and are often validated through heuristics and trial-and-error. To fill this gap, we demonstrate that intervention efficacy, measured by linear steerability (i.e., the ability to adjust output via linear transformations of hidden states), emerges during intermediate stages of training. Moreover, even closely related concepts (e.g., anger and sadness) exhibit steerability emergence at distinct stages of training.\nTo better interpret the dynamics of steerability during training, we adapt existing intervention techniques into a unified framework, referred to as the "Intervention Detector" (ID), which is designed to reveal how linear steerability evolves over the course of training through hidden state and representation analysis. ID reveals that concepts become increasingly linearly separable in the hidden space as training progresses, which strongly correlates with the emergence of linear steerability. We further introduce ID-based metrics, such as heatmaps, entropy trends, and cosine similarity, to help interpret how linear steerability evolves throughout training. In addition, we apply ID across different model families to ensure the generality of our findings on steerability dynamics.', 'abstract_zh': '语言模型可以通过修改其内部表示来操控生成中的情绪、风格或真实性等概念，但在有效干预的条件方面仍不清楚，通常通过启发式方法和试错来验证。为填补这一空白，我们证明干预的有效性，即线性可导向性（通过隐藏状态的线性变换调整输出的能力），在训练的中间阶段会出现。此外，即使是密切相关概念（如愤怒和悲伤）也会在训练的不同阶段表现出可导向性的出现。\n\n为了更好地解释训练过程中可导向性的动态变化，我们提出了一个统一的干预检测框架，称为“干预检测器”（ID），该框架设计用于通过隐藏状态和表示分析揭示线性可导向性如何随训练过程发展。ID揭示了随着训练的进行，概念在隐藏空间中变得越来越线性可区分，这与线性可导向性的出现密切相关。我们进一步引入基于ID的度量标准，如热图、熵趋势和余弦相似度，以帮助解释线性可导向性如何在整个训练过程中演变。此外，我们在不同模型家族中应用ID，以确保我们的可导向性动态发现具有普适性。', 'title_zh': '语言模型在预训练过程中如何实现可控性？'}
{'arxiv_id': 'arXiv:2508.01862', 'title': 'Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models', 'authors': 'Yijun Feng', 'link': 'https://arxiv.org/abs/2508.01862', 'abstract': "Large Language Models have demonstrated remarkable capabilities across diverse tasks, yet they frequently generate hallucinations outputs that are fluent but factually incorrect or unsupported. We propose Counterfactual Probing, a novel approach for detecting and mitigating hallucinations in LLM outputs. Our method dynamically generates counterfactual statements that appear plausible but contain subtle factual errors, then evaluates the model's sensitivity to these perturbations. We hypothesize that genuine knowledge exhibits robustness to counterfactual variations, while hallucinated content shows inconsistent confidence patterns when confronted with plausible alternatives. Our comprehensive evaluation on TruthfulQA, factual statement datasets, and curated hallucination examples demonstrates that counterfactual probing achieves superior detection performance compared to baseline methods, while our adaptive mitigation strategies reduce hallucination scores by an average of 24.5%. The approach requires no model retraining and can be integrated into existing LLM pipelines as a realtime verification mechanism.", 'abstract_zh': '大型语言模型在多种任务中展现了出色的能力，但经常会生成流畅但事实错误或缺乏支持的虚构输出。我们提出了一种新的方法——反事实探测，用于检测和减轻大型语言模型输出中的虚构现象。该方法动态生成看似合理的但包含细微事实错误的反事实陈述，然后评估模型对这些扰动的敏感性。我们认为真正知识在面对反事实变化时表现出高度的稳健性，而虚构内容在面对合理替代时则显示不一致的信心模式。在TruthfulQA数据集、事实陈述数据集和定制化的虚构示例上的全面评估表明，反事实探测在检测性能上优于基线方法，而我们的自适应缓解策略平均降低了24.5%的虚构得分。该方法不需要重新训练模型，并且可以作为实时验证机制集成到现有的大型语言模型流水线中。', 'title_zh': '基于反事实探查的大语言模型 hallucination 检测与缓解'}
{'arxiv_id': 'arXiv:2508.01781', 'title': 'A comprehensive taxonomy of hallucinations in Large Language Models', 'authors': 'Manuel Cossio', 'link': 'https://arxiv.org/abs/2508.01781', 'abstract': 'Large language models (LLMs) have revolutionized natural language processing, yet their propensity for hallucination, generating plausible but factually incorrect or fabricated content, remains a critical challenge. This report provides a comprehensive taxonomy of LLM hallucinations, beginning with a formal definition and a theoretical framework that posits its inherent inevitability in computable LLMs, irrespective of architecture or training. It explores core distinctions, differentiating between intrinsic (contradicting input context) and extrinsic (inconsistent with training data or reality), as well as factuality (absolute correctness) and faithfulness (adherence to input). The report then details specific manifestations, including factual errors, contextual and logical inconsistencies, temporal disorientation, ethical violations, and task-specific hallucinations across domains like code generation and multimodal applications. It analyzes the underlying causes, categorizing them into data-related issues, model-related factors, and prompt-related influences. Furthermore, the report examines cognitive and human factors influencing hallucination perception, surveys evaluation benchmarks and metrics for detection, and outlines architectural and systemic mitigation strategies. Finally, it introduces web-based resources for monitoring LLM releases and performance. This report underscores the complex, multifaceted nature of LLM hallucinations and emphasizes that, given their theoretical inevitability, future efforts must focus on robust detection, mitigation, and continuous human oversight for responsible and reliable deployment in critical applications.', 'abstract_zh': '大型语言模型（LLMs）已在自然语言处理领域引发革命，但其倾向性幻觉，生成虽然可信但事实错误或虚构的内容，仍然是一个关键挑战。本报告提供了LLM幻觉的全面分类，从正式定义和理论框架开始，后者提出了在可计算LLM中的内在不可避免性，不论架构或训练如何。报告探讨了核心区别，区分了内生性幻觉（与输入上下文矛盾）和外生性幻觉（与训练数据或现实不一致），以及事实性和忠实性。随后，报告详细说明了特定表现形式，包括事实错误、上下文和逻辑不一致、时间错位、伦理违规以及特定任务的幻觉，涉及诸如代码生成和多模态应用等多个领域。报告分析了潜在的原因，将其分类为数据问题、模型因素和提示影响。此外，报告还研究了影响幻觉感知的心理和人类因素，评估了检测基准和指标，并概述了架构和系统性缓解策略。最后，报告介绍了用于监测LLM发布和性能的网络资源。本报告强调了LLM幻觉的复杂性和多维性，并强调鉴于其理论上的不可避免性，未来的工作必须集中在稳健的检测、缓解和持续的人类监督上，以确保在关键应用中的负责任和可靠部署。', 'title_zh': '大型语言模型中幻觉的综合分类'}
{'arxiv_id': 'arXiv:2508.01696', 'title': 'Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy', 'authors': 'Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, Lizhe Zhang, Yan Liu, Bin Qin', 'link': 'https://arxiv.org/abs/2508.01696', 'abstract': "Retrieval-Augmented Generation (RAG) has emerged as a promising framework for enhancing the capabilities of Large Language Models (LLMs), especially in knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to *fully exploit knowledge during generation*. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior performance on open-domain and multi-hop QA tasks.", 'abstract_zh': '检索增强生成(RAG)已 emerge 作为增强大型语言模型(LLMs)能力的有前途的框架，特别是在知识密集型任务中。尽管具有优势，当前的RAG方法在生成过程中往往难以全面利用知识。具体来说，模型内部参数化知识与外部检索知识之间的协同作用仍然有限。检索的内容有时会误导生成，而某些生成的内容可以引导模型生成更准确的结果。在本文中，我们提出了一种协作链式智能体(CoCoA)框架，旨在增强参数化知识和检索知识之间的显式协同作用。具体来说，我们首先引入了CoCoA-zero，这是一种多智能体RAG框架，先进行条件知识归纳，再进行推理以得出答案。在此基础上，我们开发了CoCoA，这是一种长链训练策略，从CoCoA-zero中合成扩展的多智能体推理轨迹，以微调LLM。这种策略增强了模型整合和联合利用参数化知识和检索知识的能力。实验结果表明，CoCoA-zero和CoCoA在开放式领域和多跳问答任务中表现出更优的性能。', 'title_zh': '基于参数检索的知识协同的协作智能体链'}
{'arxiv_id': 'arXiv:2508.01674', 'title': 'CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions', 'authors': 'Tae Soo Kim, Yoonjoo Lee, Yoonah Park, Jiho Kim, Young-Ho Kim, Juho Kim', 'link': 'https://arxiv.org/abs/2508.01674', 'abstract': 'Personalization of Large Language Models (LLMs) often assumes users hold static preferences that reflect globally in all tasks. In reality, humans hold dynamic preferences that change depending on the context. As users interact with an LLM in various contexts, they naturally reveal their contextual preferences, which a model must infer and apply in future contexts to ensure alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated interaction session histories between users and LLM-based chat assistants. In each interaction session, the user provides a request in a specific context and expresses their preference through multi-turn feedback. Given a new user request and prior interaction sessions, our benchmark assesses whether LLMs can infer the preference relevant to this request and generate a response that satisfies this preference. With CUPID, we evaluated 10 open and proprietary LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from multi-turn interactions and fail to discern what previous context is relevant to a new request -- under 50% precision and 65% recall. Our work highlights the need to advance LLM capabilities for more contextually personalized interactions and proposes CUPID as a resource to drive these improvements.', 'abstract_zh': '大型语言模型的个性化往往假设用户持有的偏好是静态的，并在全球任务中一致反映。实际上，人类的偏好是动态的，会根据情境变化。当用户在不同情境下与语言模型互动时，他们自然会揭示自己的情境偏好，模型必须推测并应用这些偏好以确保一致性和精准度。为此，我们引入了CUPID基准测试，包含756个人类策划的用户与基于语言模型的聊天助手的交互会话历史。在每个交互会话中，用户在特定情境下提出请求并通过多轮反馈表达偏好。给定新的用户请求和之前的交互会话，基准测试评估大型语言模型是否能够推断出与该请求相关的偏好，并生成能够满足该偏好的响应。通过CUPID，我们评估了10个开源和专有大型语言模型，发现最先进的大型语言模型在从多轮互动中推断偏好时表现不佳，在判断之前情境是否与新请求相关方面也表现差强人意——精度低于50%，召回率仅为65%。我们的工作强调了提高大型语言模型能力以实现更情境化的个性化交互的需求，并提议CUPID作为一种资源来推动这些改进。', 'title_zh': 'CUPID: 评估大规模语言模型从互动中实现的个性化和情境化对齐'}
{'arxiv_id': 'arXiv:2508.01656', 'title': 'Authorship Attribution in Multilingual Machine-Generated Texts', 'authors': 'Lucio La Cava, Dominik Macko, Róbert Móro, Ivan Srba, Andrea Tagarelli', 'link': 'https://arxiv.org/abs/2508.01656', 'abstract': 'As Large Language Models (LLMs) have reached human-like fluency and coherence, distinguishing machine-generated text (MGT) from human-written content becomes increasingly difficult. While early efforts in MGT detection have focused on binary classification, the growing landscape and diversity of LLMs require a more fine-grained yet challenging authorship attribution (AA), i.e., being able to identify the precise generator (LLM or human) behind a text. However, AA remains nowadays confined to a monolingual setting, with English being the most investigated one, overlooking the multilingual nature and usage of modern LLMs. In this work, we introduce the problem of Multilingual Authorship Attribution, which involves attributing texts to human or multiple LLM generators across diverse languages. Focusing on 18 languages -- covering multiple families and writing scripts -- and 8 generators (7 LLMs and the human-authored class), we investigate the multilingual suitability of monolingual AA methods, their cross-lingual transferability, and the impact of generators on attribution performance. Our results reveal that while certain monolingual AA methods can be adapted to multilingual settings, significant limitations and challenges remain, particularly in transferring across diverse language families, underscoring the complexity of multilingual AA and the need for more robust approaches to better match real-world scenarios.', 'abstract_zh': '多语言作者归属问题', 'title_zh': '多语言机器生成文本的作者归属分析'}
{'arxiv_id': 'arXiv:2508.01647', 'title': 'DUP: Detection-guided Unlearning for Backdoor Purification in Language Models', 'authors': 'Man Hu, Yahui Ding, Yatao Yang, Liangyu Chen, Yanhao Jia, Shuai Zhao', 'link': 'https://arxiv.org/abs/2508.01647', 'abstract': 'As backdoor attacks become more stealthy and robust, they reveal critical weaknesses in current defense strategies: detection methods often rely on coarse-grained feature statistics, and purification methods typically require full retraining or additional clean models. To address these challenges, we propose DUP (Detection-guided Unlearning for Purification), a unified framework that integrates backdoor detection with unlearning-based purification. The detector captures feature-level anomalies by jointly leveraging class-agnostic distances and inter-layer transitions. These deviations are integrated through a weighted scheme to identify poisoned inputs, enabling more fine-grained analysis. Based on the detection results, we purify the model through a parameter-efficient unlearning mechanism that avoids full retraining and does not require any external clean model. Specifically, we innovatively repurpose knowledge distillation to guide the student model toward increasing its output divergence from the teacher on detected poisoned samples, effectively forcing it to unlearn the backdoor behavior. Extensive experiments across diverse attack methods and language model architectures demonstrate that DUP achieves superior defense performance in detection accuracy and purification efficacy. Our code is available at this https URL.', 'abstract_zh': '背门攻击日益隐蔽和 robust，暴露出当前防御策略的关键弱点：检测方法往往依赖粗粒度的特征统计，而净化方法通常需要全面重训练或额外的干净模型。为应对这些挑战，我们提出了一种统一框架 DUP（Detection-guided Unlearning for Purification），该框架将背门检测与基于遗忘的净化相结合。检测器通过联合利用类无关的距离和层间过渡来捕捉特征级别的异常，并通过加权方案整合这些偏差，以识别中毒输入，从而实现更精细的分析。基于检测结果，我们通过一种参数高效的遗忘机制来净化模型，该机制避免了全面重训练，并且不需要任何外部干净模型。具体地，我们创新性地重新利用了知识蒸馏，以指导学生模型增加其在检测到的中毒样本上的输出差异，从而有效地促使它遗忘背门行为。在多种攻击方法和语言模型架构上的广泛实验表明，DUP 在检测准确性和净化效果方面表现出优越的防御性能。我们的代码可在以下链接获取：this https URL。', 'title_zh': 'DUP: 检测引导的后门消除以 purification 优化语言模型中的后门检测与消除'}
{'arxiv_id': 'arXiv:2508.01638', 'title': 'Semantic Encryption: Secure and Effective Interaction with Cloud-based Large Language Models via Semantic Transformation', 'authors': 'Dong Chen, Tong Yang, Feipeng Zhai, Pengpeng Ouyang, Qidong Liu, Yafei Li, Chong Fu, Mingliang Xu', 'link': 'https://arxiv.org/abs/2508.01638', 'abstract': "The increasing adoption of Cloud-based Large Language Models (CLLMs) has raised significant concerns regarding data privacy during user interactions. While existing approaches primarily focus on encrypting sensitive information, they often overlook the logical structure of user inputs. This oversight can lead to reduced data utility and degraded performance of CLLMs. To address these limitations and enable secure yet effective interactions, we propose Semantic Encryption (SE)-a plug-and-play framework designed to preserve both privacy and utility. SE consists of two key components: Semantic Encoding and Semantic Decoding. In the encoding phase, a lightweight local model transforms the original user input into an alternative semantic context that maintains the original intent and logical structure while obfuscating sensitive information. This transformed input is then processed by the CLLM, which generates a response based on the transformed semantic context. To maintain a seamless user experience, the decoding phase will reconstruct the CLLM's response back into the original semantic context by referencing the locally stored user input. Extensive experimental evaluations demonstrate that SE effectively protects data privacy without compromising data utility or user experience, offering a practical solution for secure interaction with CLLMs. Particularly, the proposed SE demonstrates a significant improvement over the state-of-the-art InferDPT, surpassing it across various evaluated metrics and datasets.", 'abstract_zh': '基于云的大型语言模型（CLLMs）数据隐私保护研究：一种语义加密框架', 'title_zh': '语义加密：通过语义转换与基于云的大型语言模型安全有效交互'}
{'arxiv_id': 'arXiv:2508.01625', 'title': 'EAC-MoE: Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models', 'authors': 'Yuanteng Chen, Yuantian Shao, Peisong Wang, Jian Cheng', 'link': 'https://arxiv.org/abs/2508.01625', 'abstract': 'Mixture-of-Experts (MoE) has demonstrated promising potential in scaling LLMs. However, it is hindered by two critical challenges: (1) substantial GPU memory consumption to load all experts; (2) low activated parameters cannot be equivalently translated into inference acceleration effects. In this work, we propose EAC-MoE, an Expert-Selection Aware Compressor for MoE-LLMs, which deeply aligns with the characteristics of MoE from the perspectives of quantization and pruning, and introduces two modules to address these two challenges respectively: (1) The expert selection bias caused by low-bit quantization is a major factor contributing to the performance degradation in MoE-LLMs. Based on this, we propose Quantization with Expert-Selection Calibration (QESC), which mitigates the expert selection bias by calibrating the routers within the MoE; (2) There are always certain experts that are not crucial for the corresponding tasks, yet causing inference latency. Therefore, we propose Pruning based on Expert-Selection Frequency (PESF), which significantly improves inference speed by pruning less frequently used experts for current task. Extensive experiments demonstrate that our approach significantly reduces memory usage and improves inference speed with minimal performance degradation.', 'abstract_zh': 'Expert-Selection Aware Compressor for MoE-LLMs: Mitigating Expert Selection Bias and Improving Inference Efficiency', 'title_zh': 'EAC-MoE：aware专家选择压缩器for混合专家大型语言模型'}
{'arxiv_id': 'arXiv:2508.01554', 'title': 'Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models', 'authors': 'Yujia Zheng, Tianhao Li, Haotian Huang, Tianyu Zeng, Jingyu Lu, Chuangxin Chu, Yuekai Huang, Ziyou Jiang, Qian Xiong, Yuyao Ge, Mingyang Li', 'link': 'https://arxiv.org/abs/2508.01554', 'abstract': 'Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at this https URL.', 'abstract_zh': '基于提示的对抗攻击已成为评估大型语言模型鲁棒性的有效手段。然而，现有方法往往将提示视为单一文本，忽视了其结构异质性——不同的提示组件对对抗鲁棒性贡献不均。前人工作如PromptRobust假设提示是中立的，但我们的分析揭示，具有丰富结构的复杂领域特定提示的不同组件具有不同的脆弱性。为填补这一空白，我们提出PromptAnatomy，一种自动框架，将提示分解为功能组件，并通过选择性地扰动每个组件生成多样且可解释的对抗样本，使用我们提出的方法ComPerturb。为了确保语义可信度并减轻分布偏移，我们进一步引入了基于困惑度(PPL)的过滤机制。作为补充资源，我们使用PromptAnatomy框架标注了四个公开的指令调优数据集，并通过人工审核验证。在这些数据集及五种先进大型语言模型上进行的广泛实验表明，ComPerturb实现了最先进的攻击成功率。消融研究验证了提示分解和PPL过滤的互补优势。我们的结果强调了在大型语言模型对抗鲁棒性评估中提示结构意识和可控扰动的重要性。代码和数据可通过以下链接获取。', 'title_zh': '所有提示组件都是价值中立的吗？理解分解提示在大型语言模型中的异质对抗鲁棒性'}
{'arxiv_id': 'arXiv:2508.01547', 'title': 'Understanding Why ChatGPT Outperforms Humans in Visualization Design Advice', 'authors': 'Yongsu Ahn, Nam Wook Kim', 'link': 'https://arxiv.org/abs/2508.01547', 'abstract': 'This paper investigates why recent generative AI models outperform humans in data visualization knowledge tasks. Through systematic comparative analysis of responses to visualization questions, we find that differences exist between two ChatGPT models and human outputs over rhetorical structure, knowledge breadth, and perceptual quality. Our findings reveal that ChatGPT-4, as a more advanced model, displays a hybrid of characteristics from both humans and ChatGPT-3.5. The two models were generally favored over human responses, while their strengths in coverage and breadth, and emphasis on technical and task-oriented visualization feedback collectively shaped higher overall quality. Based on our findings, we draw implications for advancing user experiences based on the potential of LLMs and human perception over their capabilities, with relevance to broader applications of AI.', 'abstract_zh': '本文探讨了为什么近期生成式AI模型在数据可视化知识任务中优于人类。通过系统性地比较对可视化问题的回应，我们发现两个ChatGPT模型与人类输出在修辞结构、知识广度和感知质量方面存在差异。研究发现，作为更先进的模型，ChatGPT-4 展现了人类和ChatGPT-3.5 的混合特征。这两模型通常优于人类回应，而它们在覆盖范围和广度以及对技术和任务导向的可视化反馈的强调共同促进了更高的整体质量。基于我们的发现，我们探讨了利用大型语言模型和人的感知潜力来增强用户体验的潜在影响，并扩展到更广泛的人工智能应用领域。', 'title_zh': '理解ChatGPT在可视化设计建议方面超越人类的原因'}
{'arxiv_id': 'arXiv:2508.01506', 'title': 'FlashSVD: Memory-Efficient Inference with Streaming for Low-Rank Models', 'authors': 'Zishan Shao, Yixiao Wang, Qinsi Wang, Ting Jiang, Zhixu Du, Hancheng Ye, Danyang Zhuo, Yiran Chen, Hai Li', 'link': 'https://arxiv.org/abs/2508.01506', 'abstract': 'Singular Value Decomposition (SVD) has recently seen a surge of interest as a simple yet powerful tool for large language models (LLMs) compression, with a growing number of works demonstrating 20-80% parameter reductions at minimal accuracy loss. Previous SVD-based approaches have focused primarily on reducing the memory footprint of model weights, largely overlooking the additional activation memory overhead incurred during inference when applying truncated factors via standard dense CUDA kernels. Our experiments demonstrate that this activation overhead, scaling with sequence length and hidden dimension, prevents current SVD compression techniques from achieving any reduction in peak inference memory, thereby limiting their viability for real-world, on-device deployments.\nWe introduce FlashSVD, a novel, end-to-end rank-aware streaming inference framework specifically designed for SVD-compressed large language models. FlashSVD can be seamlessly integrated with any model that employs SVD-based methods for parameter reduction. By fusing low-rank projection kernels directly into both the self-attention and feed-forward network (FFN) pipelines, FlashSVD avoid materializing full-size activation buffers. Instead, small tiles of the truncated factors are loaded into on-chip SRAM, multiplied and reduced on the fly, and immediately evicted, preserving high GPU occupancy and adding no extra latency. On standard encoder benchmarks (e.g., BERT-Base), FlashSVD cuts peak activation memory by up to 70.2% and intermediate transient memory by 75%, all while incur no accuracy loss with upstreaming compression methods, offering a practical path toward memory-constrained deployment of low-rank LLMs.', 'abstract_zh': '奇异值分解（SVD）作为一种简单而强大的工具，近年来在大型语言模型（LLMs）压缩领域引起了极大的兴趣，越来越多的研究展示了在最小的精度损失下实现20%-80%的参数减少。此前的SVD基方法主要集中在减少模型权重的内存占用，而未充分考虑在应用截断因子时通过标准密集CUDA内核导致的额外激活内存开销。我们的实验表明，这种激活内存开销随着序列长度和隐藏维度的增加，阻碍了当前SVD压缩技术在峰值推理内存上的任何减少，从而限制了其在实际设备上的部署可行性。\n\n我们提出了一种名为FlashSVD的新型端到端稀疏秩意识流式推理框架，专门用于SVD压缩的大语言模型。FlashSVD可以无缝集成到采用SVD方法进行参数减少的任何模型中。通过直接将低秩投影内核融合到自我注意和前馈网络（FFN）管道中，FlashSVD避免了激活缓冲的全尺寸实现。相反，小规模截断因子被加载到片上SRAM中，在线乘法和归约，并立即被清除，以保持高GPU利用率并添加零额外延迟。在标准编解码器基准测试（如BERT-Base）中，FlashSVD将峰值激活内存减少高达70.2%，中间暂态内存减少75%，同时与上游压缩方法相比无精度损失，提供了一种在受限内存下部署低秩LLMs的实用途径。', 'title_zh': 'FlashSVD：低秩模型的流式高效推理存储方法'}
{'arxiv_id': 'arXiv:2508.01443', 'title': 'Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective', 'authors': 'Jingzhi Gong, Rafail Giavrimis, Paul Brookes, Vardan Voskanyan, Fan Wu, Mari Ashiga, Matthew Truscott, Mike Basios, Leslie Kanthan, Jie Xu, Zheng Wang', 'link': 'https://arxiv.org/abs/2508.01443', 'abstract': "There is a growing interest in leveraging large language models (LLMs) for automated code optimization. However, industrial platforms deploying multiple LLMs face a critical challenge: prompts optimized for one LLM often fail with others, requiring expensive model-specific prompt engineering. This cross-model prompt engineering bottleneck severely limits the practical deployment of multi-LLM optimization systems in production environments. To address this, we introduce Meta-Prompted Code Optimization (MPCO), a framework that automatically generates high-quality, task-specific prompts across diverse LLMs while maintaining industrial efficiency requirements. MPCO leverages meta-prompting to dynamically synthesize context-aware optimization prompts by integrating project metadata, task requirements, and LLM-specific contexts, and it seamlessly deploys on the ARTEMIS industrial platform for automated validation and scaling.\nOur comprehensive evaluation on five real-world codebases with 366 hours of runtime benchmarking demonstrates MPCO's effectiveness: it achieves overall performance improvements up to 19.06% with the best statistical rank across all systems compared to baseline methods. Analysis shows that 96% of the top-performing optimizations stem from meaningful edits. Through systematic ablation studies and meta-prompter sensitivity analysis, we identify that comprehensive context integration is essential for effective meta-prompting, and that all three major LLMs can serve effectively as meta-prompters, providing actionable insights for industrial practitioners.", 'abstract_zh': '利用元提示进行自动代码优化：跨模型自动生成高质量任务特定提示的研究', 'title_zh': '基于元提示调优LLM驱动的代码优化：工业视角'}
{'arxiv_id': 'arXiv:2508.01424', 'title': 'From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs', 'authors': 'Haonan Bian, Yutao Qi, Rui Yang, Yuanxi Che, Jiaqian Wang, Heming Xia, Ranran Zhen', 'link': 'https://arxiv.org/abs/2508.01424', 'abstract': "Large Language Models (LLMs), despite their success in question answering, exhibit limitations in complex multi-hop question answering (MQA) tasks that necessitate non-linear, structured reasoning. This limitation stems from their inability to adequately capture deep conceptual relationships between entities. To overcome this challenge, we present **ORACLE** (**O**ntology-driven **R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a training-free framework that combines LLMs' generative capabilities with the structural benefits of knowledge graphs. Our approach operates through three stages: (1) dynamic construction of question-specific knowledge ontologies using LLMs, (2) transformation of these ontologies into First-Order Logic reasoning chains, and (3) systematic decomposition of the original query into logically coherent sub-questions. Experimental results on several standard MQA benchmarks show that our framework achieves highly competitive performance, rivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses further confirm the effectiveness of each component, while demonstrating that our method generates more logical and interpretable reasoning chains than existing approaches.", 'abstract_zh': '大型语言模型（LLMs）虽然在问答任务上取得了成功，但在需要非线性、结构化推理的复杂多跳问答（MQA）任务中表现出局限性。这一局限源自它们无法充分捕捉实体之间的深层次概念关系。为克服这一挑战，我们提出了**ORACLE**（基于本体的推理与链条逻辑阐明框架），这是一个无需训练的框架，将LLMs的生成能力与知识图谱的结构性优势结合起来。我们的方法分为三个阶段：（1）使用LLMs动态构建问题特定的知识本体，（2）将这些本体转换为一阶逻辑推理链条，（3）系统地分解原始查询为逻辑上一致的子问题。在多个标准MQA基准上的实验结果显示，我们的框架达到了高度竞争力的性能，可与当前的最先进的模型DeepSeek-R1媲美。详细分析进一步证实了每个组件的有效性，同时展示了我们的方法生成的推理链条比现有方法更具逻辑性和可解释性。', 'title_zh': '从查询到逻辑：面向本体的多跳推理;break'}
{'arxiv_id': 'arXiv:2508.01390', 'title': 'Recognising, Anticipating, and Mitigating LLM Pollution of Online Behavioural Research', 'authors': 'Raluca Rilla, Tobias Werner, Hiromu Yakura, Iyad Rahwan, Anne-Marie Nussberger', 'link': 'https://arxiv.org/abs/2508.01390', 'abstract': 'Online behavioural research faces an emerging threat as participants increasingly turn to large language models (LLMs) for advice, translation, or task delegation: LLM Pollution. We identify three interacting variants through which LLM Pollution threatens the validity and integrity of online behavioural research. First, Partial LLM Mediation occurs when participants make selective use of LLMs for specific aspects of a task, such as translation or wording support, leading researchers to (mis)interpret LLM-shaped outputs as human ones. Second, Full LLM Delegation arises when agentic LLMs complete studies with little to no human oversight, undermining the central premise of human-subject research at a more foundational level. Third, LLM Spillover signifies human participants altering their behaviour as they begin to anticipate LLM presence in online studies, even when none are involved. While Partial Mediation and Full Delegation form a continuum of increasing automation, LLM Spillover reflects second-order reactivity effects. Together, these variants interact and generate cascading distortions that compromise sample authenticity, introduce biases that are difficult to detect post hoc, and ultimately undermine the epistemic grounding of online research on human cognition and behaviour. Crucially, the threat of LLM Pollution is already co-evolving with advances in generative AI, creating an escalating methodological arms race. To address this, we propose a multi-layered response spanning researcher practices, platform accountability, and community efforts. As the challenge evolves, coordinated adaptation will be essential to safeguard methodological integrity and preserve the validity of online behavioural research.', 'abstract_zh': '在线行为研究面临新兴威胁：随着参与者越来越多地利用大型语言模型（LLMs）获取建议、翻译或任务委托，出现LLM污染。我们识别出三种交互变异，这些变异威胁着在线行为研究的有效性和完整性。首先，部分LLM中介发生时，参与者仅选择性地将LLMs用于任务的特定方面，如翻译或措辞支持，从而使研究人员（误）将LLM形成的输出解读为人类的行为。其次，全面LLM委托发生在由具有代理性的LLMs独立完成研究几乎不需要人类监督的情况下，从根本上削弱了以人类受试者为中心的研究的前提。第三，LLM外溢表明，在参与者开始预期在线研究中可能存在LLMs的情况下，即使没有涉及LLMs，他们的行为也会发生变化。尽管部分中介和全面委托构成了逐渐自动化的连续体，但LLM外溢反映了次级反应效应。这些变异彼此交互作用，引发级联失真，损害样本的真实性，引入难以事后检测的偏见，并最终削弱在线研究人类认知和行为的证成基础。重要的是，LLM污染的威胁正在与生成式AI的进步协同演变，形成一场升级的方法论军备竞赛。为了应对这一挑战，我们建议采取多层次的应对措施，涵盖研究人员实践、平台责任和社区努力。随着挑战的演变，协调适应将是保护方法论完整性和维护在线行为研究有效性的关键。', 'title_zh': '识别、预见和缓解大规模语言模型对在线行为研究的污染'}
{'arxiv_id': 'arXiv:2508.01371', 'title': 'Prompt to Pwn: Automated Exploit Generation for Smart Contracts', 'authors': 'Zeke Xiao, Yuekang Li, Qin Wang, Shiping Chen', 'link': 'https://arxiv.org/abs/2508.01371', 'abstract': 'We explore the feasibility of using LLMs for Automated Exploit Generation (AEG) against vulnerable smart contracts. We present \\textsc{ReX}, a framework integrating LLM-based exploit synthesis with the Foundry testing suite, enabling the automated generation and validation of proof-of-concept (PoC) exploits. We evaluate five state-of-the-art LLMs (GPT-4.1, Gemini 2.5 Pro, Claude Opus 4, DeepSeek, and Qwen3 Plus) on both synthetic benchmarks and real-world smart contracts affected by known high-impact exploits. Our results show that modern LLMs can reliably generate functional PoC exploits for diverse vulnerability types, with success rates reaching up to 92\\%. Notably, Gemini 2.5 Pro and GPT-4.1 consistently outperform others in both synthetic and real-world scenarios. We further analyze factors influencing AEG effectiveness, including model capabilities, contract structure, and vulnerability types. We also collect the first curated dataset of real-world PoC exploits to support future research.', 'abstract_zh': '我们探索使用大规模语言模型（LLMs）进行自动exploit生成（AEG）以攻击易受攻击的智能合约的可行性。我们提出了ReX框架，该框架将基于LLM的exploit合成与Foundry测试套件集成，从而实现PoC exploit的自动化生成和验证。我们评估了五种最先进的LLM（GPT-4.1、Gemini 2.5 Pro、Claude Opus 4、DeepSeek和Qwen3 Plus）在合成基准和受已知高影响exploit影响的真实世界智能合约上的表现。结果显示，现代LLM能够可靠地生成适用于多种漏洞类型的PoC exploit，成功率高达92%。值得注意的是，Gemini 2.5 Pro和GPT-4.1在合成和真实世界场景中表现始终优于其他模型。我们还进一步分析了影响AEG效果的因素，包括模型能力、合约结构和漏洞类型。我们还收集了首个经过整理的实际PoC exploit数据集，以支持未来研究。', 'title_zh': '从提示到掌控：智能合约的自动化漏洞利用生成'}
{'arxiv_id': 'arXiv:2508.01309', 'title': 'D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation', 'authors': 'Weibo Zhou, Lingbo Li, Shangsong Liang', 'link': 'https://arxiv.org/abs/2508.01309', 'abstract': 'The scarcity and high cost of high-quality question-answering (QA) datasets hinder supervised fine-tuning (SFT) for domain-specific large language models (LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that utilizes LLMs and prompt engineering to produce diverse, high-quality QA datasets from arbitrary textual sources. D-SCoRE integrates $\\textbf{D}$ocument-centric processing, $\\textbf{S}$egmentation, $\\textbf{Co}$T $\\textbf{R}$easoning, and structured $\\textbf{E}$xport to generate QA-COT datasets tailored for domain-aware SFT. Multi-dimensional control mechanisms, such as semantic role transformation, question type balancing, and counterfactual materials, enhance diversity and relevance, overcoming limitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA datasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on SQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most domains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual materials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade hardware. Its simplicity and scalability enable efficient QA generation and high-performance fine-tuning across domains.', 'abstract_zh': '高质问答数据的稀缺性和高昂成本阻碍了领域特定大规模语言模型的监督微调。为解决这一问题，我们引入了D-SCoRE，这是一种无需训练的管道，利用大规模语言模型和提示工程从任意文本源生成多样化的高质量问答数据集。D-SCoRE集成了文档中心处理、分段、成本推理和结构化导出，以生成适用于领域感知微调的问答-成本推理数据集。多维度控制机制，如语义角色转换、问题类型平衡和反事实材料，增强了多样性和相关性，克服了现有问答生成的局限性。基于D-SCoRE生成的问答数据集微调的大规模语言模型以及人类标注的问答数据集（SQuAD、Covid-QA）在SQuADShifts和Covid-QA测试集上的评估结果显示，D-SCoRE在大多数领域表现出色。D-SCoRE使用8B参数的大规模语言模型在消费级硬件上可以在90秒内生成每100-200词的6个问答-成本推理对和四个选项的反事实材料。其简洁性和可扩展性使得跨领域高效生成问答数据和高性能微调成为可能。', 'title_zh': 'D-SCoRE: 文档中心的分割与共理推理及结构化导出用于QA-CoT数据生成'}
{'arxiv_id': 'arXiv:2508.01293', 'title': 'GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification', 'authors': 'Ngoc Bui Lam Quang, Nam Le Nguyen Binh, Thanh-Huy Nguyen, Le Thien Phuc Nguyen, Quan Nguyen, Ulas Bagci', 'link': 'https://arxiv.org/abs/2508.01293', 'abstract': 'Multiple Instance Learning (MIL) is the leading approach for whole slide image (WSI) classification, enabling efficient analysis of gigapixel pathology slides. Recent work has introduced vision-language models (VLMs) into MIL pipelines to incorporate medical knowledge through text-based class descriptions rather than simple class names. However, when these methods rely on large language models (LLMs) to generate clinical descriptions or use fixed-length prompts to represent complex pathology concepts, the limited token capacity of VLMs often constrains the expressiveness and richness of the encoded class information. Additionally, descriptions generated solely by LLMs may lack domain grounding and fine-grained medical specificity, leading to suboptimal alignment with visual features. To address these challenges, we propose a vision-language MIL framework with two key contributions: (1) A grounded multi-agent description generation system that leverages curated pathology textbooks and agent specialization (e.g., morphology, spatial context) to produce accurate and diverse clinical descriptions; (2) A text encoding strategy using a list of descriptions rather than a single prompt, capturing fine-grained and complementary clinical signals for better alignment with visual features. Integrated into a VLM-MIL pipeline, our approach shows improved performance over single-prompt class baselines and achieves results comparable to state-of-the-art models, as demonstrated on renal and lung cancer datasets.', 'abstract_zh': '基于视觉语言的多重实例学习框架：基于专业知识的描述生成与优化编码策略', 'title_zh': 'GMAT：基于多剂型临床描述生成的地面真相文本编码在视觉-语言MIL中的全切片影像分类中应用'}
{'arxiv_id': 'arXiv:2508.01249', 'title': 'AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection', 'authors': 'Peiran Wang, Yang Liu, Yunfei Lu, Yifeng Cai, Hongbo Chen, Qingyou Yang, Jie Zhang, Jue Hong, Ye Wu', 'link': 'https://arxiv.org/abs/2508.01249', 'abstract': "Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's working traces as graph-based intermediate representations with control flow and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools & data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis over sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can achieve 95.75% of TPR, with only 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect prompt injection vulnerabilities and enforce fine-grained security constraints.", 'abstract_zh': '大型语言模型代理通过结合自然语言推理和外部工具的执行来解决各种问题，展现出强大的新范式。然而，它们的动态和非透明行为在提示注入攻击的背景下引入了关键的安全风险。本文提出一种新颖见解，即将代理运行时轨迹视为具有可分析语义的结构化程序。为此，我们提出AgentArmor程序分析框架，将代理轨迹转换为基于图中间表示的结构化程序依赖表示（例如，控制流图、数据流图和程序依赖图），并通过类型系统实施安全策略。AgentArmor主要包括三个关键组件：（1）图构造器，将代理的工作轨迹重构为包含控制流和数据流的基于图的中间表示；（2）属性注册表，附加与交互工具及数据相关的安全相关信息；（3）类型系统，在中间表示上执行静态推理和检查。通过将代理行为表示为结构化程序，AgentArmor能够进行敏感数据流、信任边界和策略违规的程序分析。我们使用AgentDojo基准测试评估AgentArmor，结果显示AgentArmor的真正阳性率（TPR）为95.75%，假正阳性率（FPR）仅为3.66%。我们的结果证明了AgentArmor检测提示注入漏洞和实施细粒度安全约束的能力。', 'title_zh': 'AgentArmor: 在代理运行时轨迹上强制执行程序分析以防御提示注入攻击'}
{'arxiv_id': 'arXiv:2508.01198', 'title': 'Adaptive Content Restriction for Large Language Models via Suffix Optimization', 'authors': 'Yige Li, Peihai Jiang, Jun Sun, Peng Shu, Tianming Liu, Zhen Xiang', 'link': 'https://arxiv.org/abs/2508.01198', 'abstract': 'Large Language Models (LLMs) have demonstrated significant success across diverse applications. However, enforcing content restrictions remains a significant challenge due to their expansive output space. One aspect of content restriction is preventing LLMs from generating harmful content via model alignment approaches such as supervised fine-tuning (SFT). Yet, the need for content restriction may vary significantly across user groups, change rapidly over time, and not always align with general definitions of harmfulness. Applying SFT to each of these specific use cases is impractical due to the high computational, data, and storage demands. Motivated by this need, we propose a new task called \\textit{Adaptive Content Restriction} (AdaCoRe), which focuses on lightweight strategies -- methods without model fine-tuning -- to prevent deployed LLMs from generating restricted terms for specific use cases. We propose the first method for AdaCoRe, named \\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to any prompt to a) prevent a target LLM from generating a set of restricted terms, while b) preserving the output quality. To evaluate AdaCoRe approaches, including our SOP, we create a new \\textit{Content Restriction Benchmark} (CoReBench), which contains 400 prompts for 80 restricted terms across 8 carefully selected categories. We demonstrate the effectiveness of SOP on CoReBench, which outperforms the system-level baselines such as system suffix by 15\\%, 17\\%, 10\\%, 9\\%, and 6\\% on average restriction rates for Gemma2-2B, Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also demonstrate that SOP is effective on POE, an online platform hosting various commercial LLMs, highlighting its practicality in real-world scenarios.', 'abstract_zh': 'Large语言模型（LLMs）在多种应用中取得了显著的成效。然而，执行内容限制仍然是一个重大挑战，因为它们的输出空间非常广阔。内容限制的一个方面是通过模型对齐方法，如监督微调（SFT）等方式，防止LLMs生成有害内容。然而，不同用户组对内容限制的需求可能会有很大差异，且随着时间迅速变化，不一定与普遍定义的有害性一致。逐个为这些特定用例应用SFT在计算、数据和存储需求上都是不切实际的。为此，我们提出了一种新的任务称为“自适应内容限制”（AdaCoRe），该任务聚焦于轻量级策略——无需模型微调的方法，以防止部署的LLMs在特定用例中生成受限词汇。我们提出了AdaCoRe的第一个方法——后缀优化（SOP），它在任何提示后附加一个简短优化后的后缀，以a) 阻止目标LLM生成一组受限词汇，同时b) 保留输出质量。为了评估AdaCoRe方法，包括我们的SOP方法，我们创建了一个新的“内容限制基准”（CoReBench），其中包括80种受限词汇的400个提示，涵盖了8个精心选择的类别。我们在CoReBench上展示了SOP的有效性，SOP在Gemma2-2B、Mistral-7B、Vicuna-7B、Llama3-8B和Llama3.1-8B上平均受限率方面分别优于系统级基线，如系统后缀，分别为15%、17%、10%、9%和6%。我们还展示了SOP在POE（一个在线平台，提供多种商业LLM）上的有效性，突显了其在现实场景中的实际应用性。', 'title_zh': '大型语言模型通过后缀优化实现自适应内容限制'}
{'arxiv_id': 'arXiv:2508.01188', 'title': 'SpectrumWorld: Artificial Intelligence Foundation for Spectroscopy', 'authors': 'Zhuo Yang, Jiaqing Xie, Shuaike Shen, Daolang Wang, Yeyun Chen, Ben Gao, Shuzhou Sun, Biqing Qi, Dongzhan Zhou, Lei Bai, Linjiang Chen, Shufei Zhang, Jun Jiang, Tianfan Fu, Yuqiang Li', 'link': 'https://arxiv.org/abs/2508.01188', 'abstract': 'Deep learning holds immense promise for spectroscopy, yet research and evaluation in this emerging field often lack standardized formulations. To address this issue, we introduce SpectrumLab, a pioneering unified platform designed to systematize and accelerate deep learning research in spectroscopy. SpectrumLab integrates three core components: a comprehensive Python library featuring essential data processing and evaluation tools, along with leaderboards; an innovative SpectrumAnnotator module that generates high-quality benchmarks from limited seed data; and SpectrumBench, a multi-layered benchmark suite covering 14 spectroscopic tasks and over 10 spectrum types, featuring spectra curated from over 1.2 million distinct chemical substances. Thorough empirical studies on SpectrumBench with 18 cutting-edge multimodal LLMs reveal critical limitations of current approaches. We hope SpectrumLab will serve as a crucial foundation for future advancements in deep learning-driven spectroscopy.', 'abstract_zh': 'Deep Learning for Spectroscopy: A Unified Platform, SpectrumLab, to Standardize and Accelerate Research', 'title_zh': 'SpectrumWorld: 光谱分析的人工智能基础'}
{'arxiv_id': 'arXiv:2508.01174', 'title': 'RSPO: Risk-Seeking Policy Optimization for Pass@k and Max@k Metrics in Large Language Models', 'authors': 'Kaichen Zhang, Shenghao Gao, Yuzhong Hong, Haipeng Sun, Junwei Bao, Hongfei Jiang, Yang Song, Hong Dingqian, Hui Xiong', 'link': 'https://arxiv.org/abs/2508.01174', 'abstract': 'Current large language model post-training optimizes a risk-neutral objective that maximizes expected reward, yet evaluation relies heavily on risk-seeking metrics like Pass@k (at least one success in k trials) and Max@k (maximum reward across k responses). This mismatch in risk preferences can inevitably lead to suboptimal performance. To bridge this gap, we propose Risk-Seeking Policy Optimization (RSPO), a novel method that directly targets Pass@k and Max@k during training. A key challenge in optimizing these metrics is the "hitchhiking" problem: low-reward responses are inadvertently reinforced if they co-occur with a high-reward response within a sample of k generations, resulting in inefficient optimization. RSPO addresses this problem by leveraging the closed-form probability that a given response is the maximum among k samplings. Despite the complexity of nested gradients over multiple responses, RSPO produces efficient, unbiased gradient estimators for both metrics. We validate our approach with both rigorous theoretical analysis and comprehensive experimental results.', 'abstract_zh': '风险寻求策略优化：一种直接针对Pass@k和Max@k的训练方法', 'title_zh': 'RSPO: 风险寻求策略优化以提升大型语言模型的Pass@k和Max@k指标'}
{'arxiv_id': 'arXiv:2508.01159', 'title': 'Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates', 'authors': 'Liam G. McCoy, Fateme Nateghi Haredasht, Kanav Chopra, David Wu, David JH Wu, Abass Conteh, Sarita Khemani, Saloni Kumar Maharaj, Vishnu Ravi, Arth Pahwa, Yingjie Weng, Leah Rosengaus, Lena Giang, Kelvin Zhenghao Li, Olivia Jee, Daniel Shirvani, Ethan Goh, Jonathan H. Chen', 'link': 'https://arxiv.org/abs/2508.01159', 'abstract': "This study evaluates the capacity of large language models (LLMs) to generate structured clinical consultation templates for electronic consultation. Using 145 expert-crafted templates developed and routinely used by Stanford's eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to produce clinically coherent, concise, and prioritized clinical question schemas. Through a multi-agent pipeline combining prompt optimization, semantic autograding, and prioritization analysis, we show that while models like o3 achieve high comprehensiveness (up to 92.2\\%), they consistently generate excessively long templates and fail to correctly prioritize the most clinically important questions under length constraints. Performance varies across specialties, with significant degradation in narrative-driven fields such as psychiatry and pain medicine. Our findings demonstrate that LLMs can enhance structured clinical information exchange between physicians, while highlighting the need for more robust evaluation methods that capture a model's ability to prioritize clinically salient information within the time constraints of real-world physician communication.", 'abstract_zh': '本研究评估了大型语言模型（LLMs）生成结构化临床咨询模板的能力，以供电子咨询使用。通过评估由斯坦福eConsult团队开发并常规使用的145个专家crafted模板，我们考察了前沿模型（包括o3、GPT-4o、Kimi K2、Claude 4 Sonnet、Llama 3 70B和Gemini 2.5 Pro）生成临床连贯、简洁且优先级明确的临床问题框架的能力。通过结合提示优化、语义自动评分和优先级分析的多代理管线，我们表明，尽管像o3这样的模型实现了高度的完整性（高达92.2%），但在长度限制下，它们始终生成了过于冗长的模板，并未能正确优先考虑最重要的临床问题。各专科的表现不一，在以叙事驱动的领域如精神病学和疼痛医学中，性能显著下降。我们的研究结果表明，LLMs能够增强医生之间的结构化临床信息交流，同时也强调了需要更为 robust的评估方法，以捕捉模型在现实世界医生沟通时间限制内优先处理临床相关信息的能力。', 'title_zh': '提出恰当的问题：评估大型语言模型在临床咨询模板开发中的表现'}
{'arxiv_id': 'arXiv:2508.01136', 'title': 'DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs', 'authors': 'Wei Zhou, Peng Sun, Xuanhe Zhou, Qianglei Zang, Ji Xu, Tieying Zhang, Guoliang Li, Fan Wu', 'link': 'https://arxiv.org/abs/2508.01136', 'abstract': 'The operation and maintenance (O&M) of database systems is critical to ensuring system availability and performance, typically requiring expert experience (e.g., identifying metric-to-anomaly relations) for effective diagnosis and recovery. However, existing automatic database O&M methods, including commercial products, cannot effectively utilize expert experience. On the one hand, rule-based methods only support basic O&M tasks (e.g., metric-based anomaly detection), which are mostly numerical equations and cannot effectively incorporate literal O&M experience (e.g., troubleshooting guidance in manuals). On the other hand, LLM-based methods, which retrieve fragmented information (e.g., standard documents + RAG), often generate inaccurate or generic results. To address these limitations, we present DBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with knowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a heterogeneous graph model for representing the diagnosis experience, and proposes a semi-automatic graph construction algorithm to build that graph from thousands of documents. Second, DBAIOps develops a collection of (800+) reusable anomaly models that identify both directly alerted metrics and implicitly correlated experience and metrics. Third, for each anomaly, DBAIOps proposes a two-stage graph evolution mechanism to explore relevant diagnosis paths and identify missing relations automatically. It then leverages a reasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear diagnosis reports for both DBAs and common users. Our evaluation over four mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates that DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher in root cause and human evaluation accuracy, respectively.', 'abstract_zh': '数据库系统的运行和维护（O&M）对于确保系统的可用性和性能至关重要，通常需要专家经验（例如，识别指标到异常的关系）来进行有效的诊断和恢复。然而，现有的自动数据库O&M方法，包括商用产品，无法有效利用专家经验。一方面，基于规则的方法仅支持基本的O&M任务（例如，基于指标的异常检测），这些任务主要是数值方程，不能有效整合文字形式的O&M经验（例如，手册中的故障排除指导）。另一方面，基于大语言模型（LLM）的方法，由于检索碎片化的信息（例如，标准文档+检索增强生成），通常会产生不准确或通用的结果。为了解决这些限制，我们提出了DBAIOps，这是一种新颖的结合推理大语言模型与知识图谱的混合数据库O&M系统，以实现DBA风格的诊断。首先，DBAIOps引入了一种异构图模型来表示诊断经验，并提出了一种半自动的图构建算法，从数千份文档中构建该图。其次，DBAIOps开发了一整套（800多个）可重用的异常模型，能够识别直接警告的指标以及隐含关联的经验和指标。第三，对于每个异常，DBAIOps提出了一种两阶段的图演化机制，以自动探索相关诊断路径并识别缺少的关系。然后，它利用推理大语言模型（例如DeepSeek-R1）推断根本原因并生成清晰的诊断报告，适用于DBA和普通用户。我们的评估表明，DBAIOps在四个主流数据库系统（Oracle、MySQL、PostgreSQL和DM8）上优于最先进的基线，分别是34.85%和47.22%的根因和人工评估准确性提升。', 'title_zh': 'DBAIOps：一种基于知识图谱的LLM增强数据库运维系统'}
{'arxiv_id': 'arXiv:2508.01059', 'title': 'Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report', 'authors': 'Sajana Weerawardhena, Paul Kassianik, Blaine Nelson, Baturay Saglam, Anu Vellore, Aman Priyanshu, Supriti Vijay, Massimo Aufiero, Arthur Goldblatt, Fraser Burch, Ed Li, Jianliang He, Dhruv Kedia, Kojin Oshiba, Zhouran Yang, Yaron Singer, Amin Karbasi', 'link': 'https://arxiv.org/abs/2508.01059', 'abstract': 'Large language models (LLMs) have shown remarkable success across many domains, yet their integration into cybersecurity applications remains limited due to a lack of general-purpose cybersecurity data, representational complexity, and safety and regulatory concerns. To address this gap, we previously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable for fine-tuning on downstream tasks. That model, however, was not designed for chat-style interactions or instruction-following. In this report, we release Foundation-Sec-8B-Instruct: a model specifically trained for general-purpose cybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific knowledge with instruction-following, conversational capabilities, and alignment with human preferences to produce high-quality, relevant responses. Comprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms Llama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its instruction-following performance. It is also competitive with GPT-4o-mini on cyber threat intelligence and instruction-following tasks. We envision Foundation-Sec-8B-Instruct becoming an indispensable assistant in the daily workflows of cybersecurity professionals. We release the model publicly at this https URL.', 'abstract_zh': 'Foundation-Sec-8B-Instruct: 一种适用于通用网络安全对话的指令跟随模型', 'title_zh': 'Llama-3.1-基金会AI安全大型语言模型-8B-指令技术报告'}
{'arxiv_id': 'arXiv:2508.01056', 'title': 'Managing Escalation in Off-the-Shelf Large Language Models', 'authors': 'Sebastian Elbaum, Jonathan Panther', 'link': 'https://arxiv.org/abs/2508.01056', 'abstract': "U.S. national security customers have begun to utilize large language models, including enterprise versions of ``off-the-shelf'' models (e.g., ChatGPT) familiar to the public. This uptake will likely accelerate. However, recent studies suggest that off-the-shelf large language models frequently suggest escalatory actions when prompted with geopolitical or strategic scenarios. We demonstrate two simple, non-technical interventions to control these tendencies. Introducing these interventions into the experimental wargame design of a recent study, we substantially reduce escalation throughout the game. Calls to restrict the use of large language models in national security applications are thus premature. The U.S. government is already, and will continue, employing large language models for scenario planning and suggesting courses of action. Rather than warning against such applications, this study acknowledges the imminent adoption of large language models, and provides actionable measures to align them with national security goals, including escalation management.", 'abstract_zh': '美国国家安全客户已经开始利用大型语言模型，包括公众熟悉的“即用型”模型的企业版本（例如ChatGPT）。这一应用将很可能加速。然而，最近的研究表明，当被提示涉及地缘政治或战略情景时，“即用型”大型语言模型经常建议升级行动。我们展示了两种简单的非技术干预措施来控制这些倾向。在最近一项研究的实验战争游戏设计中引入这些干预措施，显著减少了游戏中的升级行为。有关限制在国家安全应用中使用大型语言模型的声音因此为时尚早。美国政府已经在使用大型语言模型进行场景规划并建议行动方案，并将继续这样做。本研究认可大型语言模型即将被采用这一现实，并提供了有助于将这些模型与国家安全目标，包括升级管理相一致的可操作措施。', 'title_zh': '管理现成大型语言模型中的升级问题'}
{'arxiv_id': 'arXiv:2508.01055', 'title': 'FGBench: A Dataset and Benchmark for Molecular Property Reasoning at Functional Group-Level in Large Language Models', 'authors': 'Xuan Liu, Siru Ouyang, Xianrui Zhong, Jiawei Han, Huimin Zhao', 'link': 'https://arxiv.org/abs/2508.01055', 'abstract': "Large language models (LLMs) have gained significant attention in chemistry. However, most existing datasets center on molecular-level property prediction and overlook the role of fine-grained functional group (FG) information. Incorporating FG-level data can provide valuable prior knowledge that links molecular structures with textual descriptions, which can be used to build more interpretable, structure-aware LLMs for reasoning on molecule-related tasks. Moreover, LLMs can learn from such fine-grained information to uncover hidden relationships between specific functional groups and molecular properties, thereby advancing molecular design and drug discovery. Here, we introduce FGBench, a dataset comprising 625K molecular property reasoning problems with functional group information. Functional groups are precisely annotated and localized within the molecule, which ensures the dataset's interoperability thereby facilitating further multimodal applications. FGBench includes both regression and classification tasks on 245 different functional groups across three categories for molecular property reasoning: (1) single functional group impacts, (2) multiple functional group interactions, and (3) direct molecular comparisons. In the benchmark of state-of-the-art LLMs on 7K curated data, the results indicate that current LLMs struggle with FG-level property reasoning, highlighting the need to enhance reasoning capabilities in LLMs for chemistry tasks. We anticipate that the methodology employed in FGBench to construct datasets with functional group-level information will serve as a foundational framework for generating new question-answer pairs, enabling LLMs to better understand fine-grained molecular structure-property relationships. The dataset and evaluation code are available at \\href{this https URL}{this https URL}.", 'abstract_zh': '大型语言模型（LLMs）在化学领域获得了显著关注。然而，现有的大多数数据集主要集中在分子级别的性质预测，忽视了精细功能团（FG）信息的作用。整合功能团级别的数据可以提供将分子结构与文本描述联系起来的宝贵先验知识，从而构建更具解释性、结构感知的LLMs，用于分子相关的任务推理。此外，LLMs可以从这种精细信息中学习，发现特定功能团与分子性质之间的隐藏关系，从而促进分子设计和药物发现。在此基础上，我们介绍了FGBench数据集，包含625K个包含功能团信息的分子性质推理问题。功能团被精确标注并定位在分子内，确保了数据集的互操作性，从而便于进一步的多模态应用。FGBench包括三个类别中245种不同功能团的回归和分类任务：（1）单一功能团影响，（2）多种功能团相互作用，（3）直接分子比较。在针对7K精选数据的前沿LLMs基准测试中，结果表明当前的LLMs在功能团级别性质推理方面存在困难，强调了为化学任务增强LLMs推理能力的需求。我们期待FGBench中构建具有功能团级信息数据集的方法论将为生成新的问答对提供基础框架，使LLMs更好地理解精细分子结构-性质关系。数据集和评估代码可在\\href{this https URL}{this https URL}获取。', 'title_zh': 'FGBench：大型语言模型在功能团级分子性质推理的数据库和基准测试'}
{'arxiv_id': 'arXiv:2508.01054', 'title': 'Autonomous Penetration Testing: Solving Capture-the-Flag Challenges with LLMs', 'authors': 'Isabelle Bakker, John Hastings', 'link': 'https://arxiv.org/abs/2508.01054', 'abstract': "This study evaluates the ability of GPT-4o to autonomously solve beginner-level offensive security tasks by connecting the model to OverTheWire's Bandit capture-the-flag game. Of the 25 levels that were technically compatible with a single-command SSH framework, GPT-4o solved 18 unaided and another two after minimal prompt hints for an overall 80% success rate. The model excelled at single-step challenges that involved Linux filesystem navigation, data extraction or decoding, and straightforward networking. The approach often produced the correct command in one shot and at a human-surpassing speed. Failures involved multi-command scenarios that required persistent working directories, complex network reconnaissance, daemon creation, or interaction with non-standard shells. These limitations highlight current architectural deficiencies rather than a lack of general exploit knowledge. The results demonstrate that large language models (LLMs) can automate a substantial portion of novice penetration-testing workflow, potentially lowering the expertise barrier for attackers and offering productivity gains for defenders who use LLMs as rapid reconnaissance aides. Further, the unsolved tasks reveal specific areas where secure-by-design environments might frustrate simple LLM-driven attacks, informing future hardening strategies. Beyond offensive cybersecurity applications, results suggest the potential to integrate LLMs into cybersecurity education as practice aids.", 'abstract_zh': '本研究通过将模型连接到OverTheWire的Bandit夺旗游戏，评估了GPT-4o自主解决初级级别攻击安全任务的能力。在25个技术上与单命令SSH框架兼容的级别中，GPT-4o在无需辅助力的情况下解决了18个级别，并在最少提示下解决了另外2个级别，整体成功率达到80%。模型在涉及Linux文件系统导航、数据提取或解码以及直接网络操作的一步挑战中表现出色。这种方法往往能够一次生成正确的命令，并且速度超过人类。失败主要涉及需要持续工作目录、复杂网络侦察、创建守护进程或与非标准shell交互的多命令场景。这些局限性突显了当前架构的缺陷，而非一般利用知识的缺乏。研究结果表明，大型语言模型（LLMs）可以自动化初学者渗透测试工作流程的重要部分，有可能降低攻击者的专业知识门槛，并为使用LLMs作为快速侦察助手的防御者提供生产力提升。未解决的任务还揭示了安全设计环境在对抗简单驱动的LLM攻击中可能会遇到的具体问题，这些信息有助于未来的加固策略。此外，研究结果表明，大型语言模型有可能集成到网络安全教育中，作为实践辅助工具。', 'title_zh': '自主渗透测试：使用大型语言模型解决捕获旗帜挑战'}
{'arxiv_id': 'arXiv:2508.00998', 'title': 'Are LLM-Powered Social Media Bots Realistic?', 'authors': 'Lynnette Hui Xian Ng, Kathleen M. Carley', 'link': 'https://arxiv.org/abs/2508.00998', 'abstract': 'As Large Language Models (LLMs) become more sophisticated, there is a possibility to harness LLMs to power social media bots. This work investigates the realism of generating LLM-Powered social media bot networks. Through a combination of manual effort, network science and LLMs, we create synthetic bot agent personas, their tweets and their interactions, thereby simulating social media networks. We compare the generated networks against empirical bot/human data, observing that both network and linguistic properties of LLM-Powered Bots differ from Wild Bots/Humans. This has implications towards the detection and effectiveness of LLM-Powered Bots.', 'abstract_zh': '随着大型语言模型（LLMs）变得更加 sophisticated，有可能利用LLMs来驱动社交媒体机器人。本研究探讨了由LLM驱动的社交媒体机器人网络的现实性。通过结合人工努力、网络科学和LLMs，我们创建了合成的机器人代理人格、他们的推文及其互动，从而模拟社交媒体网络。我们将生成的网络与实际的机器人/人类数据进行比较，发现由LLM驱动的机器人在网络特性和语言特性方面与野生机器人/人类存在差异。这对抗矫编和由LLM驱动的机器人效果具有重要影响。', 'title_zh': '带有LLM驱动的社会媒体机器人现实吗？'}
{'arxiv_id': 'arXiv:2508.00965', 'title': 'VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented Generation for NLI', 'authors': 'Roie Kazoom, Ofir Cohen, Rami Puzis, Asaf Shabtai, Ofer Hadar', 'link': 'https://arxiv.org/abs/2508.00965', 'abstract': 'We introduce VAULT, a fully automated adversarial RAG pipeline that systematically uncovers and remedies weaknesses in NLI models through three stages: retrieval, adversarial generation, and iterative retraining. First, we perform balanced few-shot retrieval by embedding premises with both semantic (BGE) and lexical (BM25) similarity. Next, we assemble these contexts into LLM prompts to generate adversarial hypotheses, which are then validated by an LLM ensemble for label fidelity. Finally, the validated adversarial examples are injected back into the training set at increasing mixing ratios, progressively fortifying a zero-shot RoBERTa-base this http URL standard benchmarks, VAULT elevates RoBERTa-base accuracy from 88.48% to 92.60% on SNLI +4.12%, from 75.04% to 80.95% on ANLI +5.91%, and from 54.67% to 71.99% on MultiNLI +17.32%. It also consistently outperforms prior in-context adversarial methods by up to 2.0% across datasets. By automating high-quality adversarial data curation at scale, VAULT enables rapid, human-independent robustness improvements in NLI inference tasks.', 'abstract_zh': 'VAULT：一种全自动对抗RAG管道，系统地揭示并修复NLI模型的缺陷', 'title_zh': 'VAULT: 聆听大型语言模型驱动的检索增强生成以实现警惕的对抗更新用于自然语言推理'}
{'arxiv_id': 'arXiv:2508.00961', 'title': 'FinKario: Event-Enhanced Automated Construction of Financial Knowledge Graph', 'authors': 'Xiang Li, Penglei Sun, Wanyun Zhou, Zikai Wei, Yongqi Zhang, Xiaowen Chu', 'link': 'https://arxiv.org/abs/2508.00961', 'abstract': "Individual investors are significantly outnumbered and disadvantaged in financial markets, overwhelmed by abundant information and lacking professional analysis. Equity research reports stand out as crucial resources, offering valuable insights. By leveraging these reports, large language models (LLMs) can enhance investors' decision-making capabilities and strengthen financial analysis. However, two key challenges limit their effectiveness: (1) the rapid evolution of market events often outpaces the slow update cycles of existing knowledge bases, (2) the long-form and unstructured nature of financial reports further hinders timely and context-aware integration by LLMs. To address these challenges, we tackle both data and methodological aspects. First, we introduce the Event-Enhanced Automated Construction of Financial Knowledge Graph (FinKario), a dataset comprising over 305,360 entities, 9,625 relational triples, and 19 distinct relation types. FinKario automatically integrates real-time company fundamentals and market events through prompt-driven extraction guided by professional institutional templates, providing structured and accessible financial insights for LLMs. Additionally, we propose a Two-Stage, Graph-Based retrieval strategy (FinKario-RAG), optimizing the retrieval of evolving, large-scale financial knowledge to ensure efficient and precise data access. Extensive experiments show that FinKario with FinKario-RAG achieves superior stock trend prediction accuracy, outperforming financial LLMs by 18.81% and institutional strategies by 17.85% on average in backtesting.", 'abstract_zh': '个体投资者在金融市场中被显著地 outnumber 和处于劣势，面临信息繁多和缺乏专业分析的困境。股票研究报告是关键资源，提供了有价值的见解。通过利用这些报告，大型语言模型（LLMs）可以增强投资者的决策能力和加强金融分析。然而，两项关键挑战限制了它们的有效性：（1）市场事件的快速演变往往超过了现有知识库的缓慢更新周期；（2）金融报告的长效性和不结构化特性进一步阻碍了 LLMs 的及时和上下文相关集成。为了解决这些挑战，我们从数据和方法论两个方面着手。首先，我们引入了事件增强的财务知识图谱自动化构建（FinKario），该数据集包含超过 305,360 个实体、9,625 个关系三元组和 19 种不同的关系类型。FinKario 自动通过基于专业机构模板的提示驱动提取来整合实时的公司基本面和市场事件，为 LLMs 提供结构化和易于访问的财务洞察。此外，我们提出了一种基于图的两阶段检索策略（FinKario-RAG），优化了演变中的大规模财务知识的检索，以确保高效和精确的数据访问。广泛实验表明，FinKario 结合 FinKario-RAG 在回测中展示了卓越的股票趋势预测精度，分别比金融 LLMs 和机构策略高出 18.81% 和 17.85%。', 'title_zh': 'FinKario：事件增强的金融知识图谱自动化构建'}
{'arxiv_id': 'arXiv:2508.00960', 'title': 'Compression-Induced Communication-Efficient Large Model Training and Inferencing', 'authors': 'Sudip K. Seal, Maksudul Alam, Jorge Ramirez, Sajal Dash, Hao Lu', 'link': 'https://arxiv.org/abs/2508.00960', 'abstract': 'Energy efficiency of training and inferencing with large neural network models is a critical challenge facing the future of sustainable large-scale machine learning workloads. This paper introduces an alternative strategy, called phantom parallelism, to minimize the net energy consumption of traditional tensor (model) parallelism, the most energy-inefficient component of large neural network training. The approach is presented in the context of feed-forward network architectures as a preliminary, but comprehensive, proof-of-principle study of the proposed methodology. We derive new forward and backward propagation operators for phantom parallelism, implement them as custom autograd operations within an end-to-end phantom parallel training pipeline and compare its parallel performance and energy-efficiency against those of conventional tensor parallel training pipelines. Formal analyses that predict lower bandwidth and FLOP counts are presented with supporting empirical results on up to 256 GPUs that corroborate these gains. Experiments are shown to deliver ~50% reduction in the energy consumed to train FFNs using the proposed phantom parallel approach when compared with conventional tensor parallel methods. Additionally, the proposed approach is shown to train smaller phantom models to the same model loss on smaller GPU counts as larger tensor parallel models on larger GPU counts offering the possibility for even greater energy savings.', 'abstract_zh': '使用大型神经网络模型进行训练和推断的能量效率是未来可持续大规模机器学习工作负载面临的最关键挑战之一。本文介绍了一种替代策略，称为幽灵并行性，以最小化传统张量（模型）并行性在大型神经网络训练中最不节能的组件的净能量消耗。该方法在前向网络架构的背景下进行了初步但全面的原理验证研究。我们为幽灵并行性推导了新的前向和反向传播算子，并在端到端的幽灵并行训练管道中实现为自定义自动求导操作，将其与传统的张量并行训练管道的并行性能和能效进行了比较。提出了正式分析预测较低的带宽和FLOP计数，并提供了在多达256块GPU上的支持性实验证据来验证这些增益。实验结果显示，与传统张量并行方法相比，所提出的幽灵并行方法可将前向网络（FFN）的训练能耗降低约50%。此外，所提出的方法在较小的GPU数量上训练较小的幽灵模型，达到与较大张量并行模型在较大GPU数量上相同模型损失，从而为更大的能效节省提供了可能性。', 'title_zh': '压缩诱导的通信高效大规模模型训练与推理'}
{'arxiv_id': 'arXiv:2508.00957', 'title': 'Small sample-based adaptive text classification through iterative and contrastive description refinement', 'authors': 'Amrit Rajeev, Udayaadithya Avadhanam, Harshula Tulapurkar, SaiBarath Sundar', 'link': 'https://arxiv.org/abs/2508.00957', 'abstract': 'Zero-shot text classification remains a difficult task in domains with evolving knowledge and ambiguous category boundaries, such as ticketing systems. Large language models (LLMs) often struggle to generalize in these scenarios due to limited topic separability, while few-shot methods are constrained by insufficient data diversity. We propose a classification framework that combines iterative topic refinement, contrastive prompting, and active learning. Starting with a small set of labeled samples, the model generates initial topic labels. Misclassified or ambiguous samples are then used in an iterative contrastive prompting process to refine category distinctions by explicitly teaching the model to differentiate between closely related classes. The framework features a human-in-the-loop component, allowing users to introduce or revise category definitions in natural language. This enables seamless integration of new, unseen categories without retraining, making the system well-suited for real-world, dynamic environments. The evaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy on AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with minimal accuracy shift after introducing unseen classes (82% and 87%, respectively). The results highlight the effectiveness of prompt-based semantic reasoning for fine-grained classification with limited supervision.', 'abstract_zh': '在 evolving knowledge 和 ambiguous category boundaries 的领域（如票据系统）中，零样本文本分类仍然是一个具有挑战性的任务。大型语言模型 (LLMs) 在这些场景中由于主题隔绝性有限而难以泛化，而 few-shot 方法则受限于数据多样性不足。我们提出了一种结合迭代主题细化、对比提示和主动学习的分类框架。从少量标注样本开始，模型生成初始主题标签。随后利用被错误分类或不明确的样本在迭代的对比提示过程中，通过明确地教会模型区分紧密相关的类别来细化类别差异。该框架包含人机交互组件，允许用户以自然语言形式引入或修订类别定义。这使得系统能够无缝集成新的未见过的类别而无需重新训练，使其适合用于现实世界的动态环境。AGNews 和 DBpedia 上的评估显示了强大的性能：AGNews 上 91% 的准确率（3 个已知类别，1 个未见过的类别），DBpedia 上 84% 的准确率（8 个已知类别，1 个未见过的类别），在引入未见过的类别后，准确率变化 minimal。结果突显了基于提示的语义推理在监督有限情况下的细粒度分类的有效性。', 'title_zh': '基于小样本的迭代对比描述细化自适应文本分类'}
{'arxiv_id': 'arXiv:2508.00955', 'title': 'From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model', 'authors': 'Yeong-Joon Ju, Seong-Whan Lee', 'link': 'https://arxiv.org/abs/2508.00955', 'abstract': "Multimodal Large Language Models (MLLMs) have emerged as a promising solution for universal embedding tasks, yet adapting their generative nature for discriminative representation learning remains a significant challenge. The dominant paradigm of large-scale contrastive pre-training suffers from critical inefficiencies, including prohibitive computational costs and a failure to leverage the intrinsic, instruction-following capabilities of MLLMs. To overcome these limitations, we propose an efficient framework for universal multimodal embeddings, which bridges this gap by centering on two synergistic components. First, our hierarchical embedding prompt template employs a two-level instruction architecture that forces the model to produce discriminative representations. Building on this strong foundation, our second component, self-aware hard negative sampling, redefines the fine-tuning process by leveraging the model's own understanding to efficiently mine challenging negatives while actively filtering out potential false negatives. Our comprehensive experiments show that our hierarchical prompt achieves zero-shot performance competitive with contrastively trained baselines and enhances the fine-tuning process by lifting a simple in-batch negative baseline by 4.8 points on the MMEB benchmark. We further boost the performance via our self-aware hard negative sampling, achieving the state-of-the-art performance without the contrative pre-training. Our work presents an effective and efficient pathway to adapt MLLMs for universal embedding tasks, significantly reducing training time.", 'abstract_zh': '多模态大型语言模型（MLLMs）已成为通用嵌入任务的有前景解决方案，但将其生成性质适应区分性表示学习仍是一个重大挑战。大规模对比预训练的主导范式面临着关键的效率问题，包括高昂的计算成本和未能充分利用MLLMs固有的指令遵循能力。为克服这些限制，我们提出了一种高效的框架，用于通用多模态嵌入，该框架通过强调两个协同组件来弥合这一差距。首先，我们的分层嵌入提示模板采用两层指令架构，迫使模型生成区分性表示。在此坚实的基础上，我们的第二个组件自意识硬负样本抽样重新定义了微调过程，利用模型自身的理解高效地挖掘挑战性负样本，并积极过滤潜在的假负样本。我们的全面实验表明，我们的分层提示在零样本性能上与对比训练的基线相当，并通过在MMEB基准测试中将简单的同批负样本基线提升4.8分来增强微调过程。我们进一步通过自意识硬负样本抽样提高性能，无需对比预训练就达到了最佳性能。我们的工作提供了一种有效且高效的途径来适应MLLMs进行通用嵌入任务，显著减少了训练时间。', 'title_zh': '从生成器到嵌入器：通过构建零样本区分嵌入模型挖掘多模态LLM的固有能力'}
{'arxiv_id': 'arXiv:2508.00952', 'title': 'Academic Vibe Coding: Opportunities for Accelerating Research in an Era of Resource Constraint', 'authors': 'Matthew G Crowson, Leo Celi A. Celi', 'link': 'https://arxiv.org/abs/2508.00952', 'abstract': 'Academic laboratories face mounting resource constraints: budgets are tightening, grant overheads are potentially being capped, and the market rate for data-science talent significantly outstrips university compensation. Vibe coding, which is structured, prompt-driven code generation with large language models (LLMs) embedded in reproducible workflows, offers one pragmatic response. It aims to compress the idea-to-analysis timeline, reduce staffing pressure on specialized data roles, and maintain rigorous, version-controlled outputs. This article defines the vibe coding concept, situates it against the current academic resourcing crisis, details a beginner-friendly toolchain for its implementation, and analyzes inherent limitations that necessitate governance and mindful application.', 'abstract_zh': '学术实验室面临日益紧缩的资源约束：预算紧缩，研究经费间接成本可能被封顶，而数据科学人才的市场薪酬远远超出大学的补偿水平。Vibe编码，这是一种嵌入可重复工作流程中的结构化、基于提示的代码生成方法，提供了一种务实的应对之策。它旨在压缩从构想到分析的时间线，减轻对专门数据角色的人力需求压力，并保持严谨的、版本控制的输出。本文定义了Vibe编码的概念，将其置于当前学术资源危机的背景之下，详细介绍了其实施的初学者友好工具链，并分析了固有的局限性，强调了需要治理和谨慎应用的重要性。', 'title_zh': '学术氛围编码：在资源约束时代加速研究的机遇'}
{'arxiv_id': 'arXiv:2508.00943', 'title': 'LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring', 'authors': 'Chloe Li, Mary Phuong, Noah Y. Siegel', 'link': 'https://arxiv.org/abs/2508.00943', 'abstract': "Trustworthy evaluations of dangerous capabilities are increasingly crucial for determining whether an AI system is safe to deploy. One empirically demonstrated threat to this is sandbagging - the strategic underperformance on evaluations by AI models or their developers. One promising defense is to monitor a model's chain-of-thought (CoT) reasoning, as this could reveal its intentions and plans. In this work, we measure the ability of models to sandbag on dangerous capability evaluations against a CoT monitor by prompting them to sandbag while being either monitor-oblivious or monitor-aware. We show that both frontier models and small open-sourced models can covertly sandbag against CoT monitoring 0-shot without hints. However, they cannot yet do so reliably: they bypass the monitor 16-36\\% of the time when monitor-aware, conditioned on sandbagging successfully. We qualitatively analyzed the uncaught CoTs to understand why the monitor failed. We reveal a rich attack surface for CoT monitoring and contribute five covert sandbagging policies generated by models. These results inform potential failure modes of CoT monitoring and may help build more diverse sandbagging model organisms.", 'abstract_zh': '可信的危险能力评估越来越重要，对于确定AI系统是否安全部署至关重要。一种实证证明的威胁是“偷懒”——AI模型或其开发者在评估中战略性地表现不佳。一种有前景的防御措施是对模型的链式思考（CoT）推理进行监控，因为这能够揭示其意图和计划。在本文中，我们通过促使模型在无提示或有提示的情况下进行CoT监控，测量其在危险能力评估中偷懒的能力。我们发现，无论是前沿模型还是小型开源模型，都可以在无提示的情况下秘密地在CoT监控下偷懒，但它们目前还不能可靠地做到这一点：当模型有提示意识时，它们有16-36%的时间设法绕过了监控。我们对未被捕获的CoT进行了定性分析，以理解为什么监控失败。我们揭示了CoT监控丰富的攻击面，并贡献了五种由模型生成的隐蔽偷懒策略。这些结果为CoT监控的潜在失败模式提供了信息，并可能有助于构建更多样化的偷懒模型组织体。', 'title_zh': 'LLMs可以在链式思考监控下隐蔽性地压制其能力评估'}
{'arxiv_id': 'arXiv:2508.00935', 'title': 'Measuring Harmfulness of Computer-Using Agents', 'authors': 'Aaron Xuxiang Tian, Ruofan Zhang, Janet Tang, Jiaxin Wen', 'link': 'https://arxiv.org/abs/2508.00935', 'abstract': "Computer-using agents (CUAs), which autonomously control computers to perform multi-step actions, might pose significant safety risks if misused. Existing benchmarks mostly evaluate language models' (LMs) safety risks in chatbots or simple tool-usage scenarios, without granting full computer access. To better evaluate CUAs' misuse risks, we introduce a new benchmark: CUAHarm. CUAHarm consists of 104 expert-written realistic misuse risks, such as disabling firewalls, leaking confidential information, launching denial-of-service attacks, or installing backdoors. We provide a sandbox environment and rule-based verifiable rewards to measure CUAs' success rates in executing these tasks (e.g., whether the firewall is indeed disabled), not just refusal. We evaluate multiple frontier open-source and proprietary LMs, such as Claude Sonnet, GPT-4o, Gemini Pro 1.5, Llama-3.3-70B, and Mistral Large 2. Surprisingly, even without carefully designed jailbreaking prompts, these frontier LMs comply with executing these malicious tasks at a high success rate (e.g., 59% for Claude 3.7 Sonnet). Newer models show higher misuse rates: Claude 3.7 Sonnet succeeds on 15% more tasks than Claude 3.5. While these models are robust to common malicious prompts (e.g., creating a bomb) in chatbot settings, they behave unsafely as CUAs. We further evaluate a leading agentic framework (UI-TARS-1.5) and find that while it improves performance, it also amplifies misuse risks. Benign variants reveal refusals stem from alignment, not capability limits. To mitigate risks, we explore using LMs to monitor CUAs' actions and chain-of-thoughts (CoTs). Monitoring CUAs is significantly harder than chatbot outputs. Monitoring CoTs yields modest gains, with average detection accuracy at only 72%. Even with hierarchical summarization, improvement is limited to 4%. CUAHarm will be released at this https URL.", 'abstract_zh': 'CUAHarm：评估计算机使用代理滥用风险的新基准', 'title_zh': '评估计算机使用代理的危害性'}
{'arxiv_id': 'arXiv:2508.00933', 'title': 'OKG-LLM: Aligning Ocean Knowledge Graph with Observation Data via LLMs for Global Sea Surface Temperature Prediction', 'authors': 'Hanchen Yang, Jiaqi Wang, Jiannong Cao, Wengen Li, Jialun Zheng, Yangning Li, Chunyu Miao, Jihong Guan, Shuigeng Zhou, Philip S. Yu', 'link': 'https://arxiv.org/abs/2508.00933', 'abstract': 'Sea surface temperature (SST) prediction is a critical task in ocean science, supporting various applications, such as weather forecasting, fisheries management, and storm tracking. While existing data-driven methods have demonstrated significant success, they often neglect to leverage the rich domain knowledge accumulated over the past decades, limiting further advancements in prediction accuracy. The recent emergence of large language models (LLMs) has highlighted the potential of integrating domain knowledge for downstream tasks. However, the application of LLMs to SST prediction remains underexplored, primarily due to the challenge of integrating ocean domain knowledge and numerical data. To address this issue, we propose Ocean Knowledge Graph-enhanced LLM (OKG-LLM), a novel framework for global SST prediction. To the best of our knowledge, this work presents the first systematic effort to construct an Ocean Knowledge Graph (OKG) specifically designed to represent diverse ocean knowledge for SST prediction. We then develop a graph embedding network to learn the comprehensive semantic and structural knowledge within the OKG, capturing both the unique characteristics of individual sea regions and the complex correlations between them. Finally, we align and fuse the learned knowledge with fine-grained numerical SST data and leverage a pre-trained LLM to model SST patterns for accurate prediction. Extensive experiments on the real-world dataset demonstrate that OKG-LLM consistently outperforms state-of-the-art methods, showcasing its effectiveness, robustness, and potential to advance SST prediction. The codes are available in the online repository.', 'abstract_zh': '基于海洋知识图谱增强的大语言模型的海表温度预测', 'title_zh': 'OKG-LLM：通过大型语言模型对观测数据进行海洋知识图谱对齐以预测全球海表温度'}
{'arxiv_id': 'arXiv:2508.00912', 'title': 'Predictive Auditing of Hidden Tokens in LLM APIs via Reasoning Length Estimation', 'authors': 'Ziyao Wang, Guoheng Sun, Yexiao He, Zheyu Shen, Bowei Tian, Ang Li', 'link': 'https://arxiv.org/abs/2508.00912', 'abstract': 'Commercial LLM services often conceal internal reasoning traces while still charging users for every generated token, including those from hidden intermediate steps, raising concerns of token inflation and potential overbilling. This gap underscores the urgent need for reliable token auditing, yet achieving it is far from straightforward: cryptographic verification (e.g., hash-based signature) offers little assurance when providers control the entire execution pipeline, while user-side prediction struggles with the inherent variance of reasoning LLMs, where token usage fluctuates across domains and prompt styles. To bridge this gap, we present PALACE (Predictive Auditing of LLM APIs via Reasoning Token Count Estimation), a user-side framework that estimates hidden reasoning token counts from prompt-answer pairs without access to internal traces. PALACE introduces a GRPO-augmented adaptation module with a lightweight domain router, enabling dynamic calibration across diverse reasoning tasks and mitigating variance in token usage patterns. Experiments on math, coding, medical, and general reasoning benchmarks show that PALACE achieves low relative error and strong prediction accuracy, supporting both fine-grained cost auditing and inflation detection. Taken together, PALACE represents an important first step toward standardized predictive auditing, offering a practical path to greater transparency, accountability, and user trust.', 'abstract_zh': '商业LLM服务常常隐藏内部推理痕迹但仍然按每个生成的令牌收费，包括那些来自隐藏中间步骤的令牌，这引发了令牌膨胀和潜在费用过高方面的担忧。这一差距突显了可靠令牌审计的迫切需求，但其实现远非简易：虽然加密验证（例如，基于哈希的签名）在这种情况下提供的保证有限，而用户端预测则难以应对推理LLM固有的差异性，即令牌使用模式在不同领域和提示风格之间波动。为了填补这一差距，我们提出了PALACE（通过推理令牌计数估计对LLM API进行预测审计）框架，该框架无需访问内部踪迹即可从提示-答案对中估计隐藏的推理令牌计数。PALACE引入了增强的GRPO适应模块和轻量级领域路由器，实现了跨多样化推理任务的动态校准，并减轻了令牌使用模式的变异。在数学、编码、医学和一般推理基准测试中的实验表明，PALACE实现了较低的相对误差和较强的预测准确性，支持精细的成本审计和膨胀检测。Palace代表了标准化预测审计的重要第一步，提供了提高透明度、问责制和用户信任的实用路径。', 'title_zh': '基于推理长度估计的LLM APIs中隐藏令牌的预测性审计'}
{'arxiv_id': 'arXiv:2508.00904', 'title': 'Forecasting LLM Inference Performance via Hardware-Agnostic Analytical Modeling', 'authors': 'Rajeev Patwari, Ashish Sirasao, Devleena Das', 'link': 'https://arxiv.org/abs/2508.00904', 'abstract': "Large language models (LLMs) have been increasingly deployed as local agents on personal devices with CPUs, NPUs and integrated GPUs. However, forecasting inference performance on devices with such heterogeneity remains challenging due to the dynamic compute and memory demands. Existing approaches rely on GPU benchmarking or machine learning-based latency predictors, which are often hardware-specific and lack generalizability. To this end, we introduce LIFE, a lightweight and modular analytical framework that is comprised of modular analytical model of operators, configurable to characterize LLM inference workloads in a hardware and dataset-agnostic manner. LIFE characterizes the influence of software and model optimizations, such as quantization, KV cache compression, LoRA adapters, chunked prefill, different attentions, and operator fusion, on performance metrics such as time-to-first-token (TTFT), time-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables performance forecasting using only hardware specifications, such as TOPS and memory bandwidth, without requiring extensive dataset benchmarking. We validate LIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA V100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in forecasting LLM performance through lens of system efficiency to enable efficient LLM deployment across different hardware platforms.", 'abstract_zh': '大规模语言模型（LLMs）正越来越多地在个人设备上的CPU、NPUs和集成GPU等异构平台上作为本地代理使用。然而，由于计算和内存需求的动态变化，预测这些设备上的推理性能仍然具有挑战性。现有方法依赖于GPU基准测试或基于机器学习的延迟预测器，这些方法通常硬件特定且缺乏通用性。为此，我们引入了LIFE，一个轻量级且模块化的分析框架，由可配置的操作器分析模型组成，在不依赖于硬件和数据集的情况下，描述LLM推理工作负载。LIFE刻画了软件和模型优化（如量化、KV缓存压缩、LoRA适配器、分块预填充、不同的注意力机制以及操作器融合）对性能指标（如首个词的时间TTFT、每个输出词的时间TPOT和每秒词数TPS）的影响。LIFE仅利用硬件规格（如TOPS和内存带宽）即可进行性能预测，无需进行广泛的基准测试数据集测试。我们通过在AMD Ryzen CPU、NPUs、iGPUs和NVIDIA V100 GPU上进行推理验证了LIFE的预测效果，使用Llama2-7B变体，展示了LIFE通过系统效率视角预测LLM性能的实用性，以促进不同硬件平台上的高效LLM部署。', 'title_zh': '基于硬件无关分析建模的LLM推理性能预测'}
{'arxiv_id': 'arXiv:2508.00903', 'title': 'Universal Neurons in GPT-2: Emergence, Persistence, and Functional Impact', 'authors': "Advey Nandan, Cheng-Ting Chou, Amrit Kurakula, Cole Blondin, Kevin Zhu, Vasu Sharma, Sean O'Brien", 'link': 'https://arxiv.org/abs/2508.00903', 'abstract': 'We investigate the phenomenon of neuron universality in independently trained GPT-2 Small models, examining how these universal neurons-neurons with consistently correlated activations across models-emerge and evolve throughout training. By analyzing five GPT-2 models at three checkpoints (100k, 200k, 300k steps), we identify universal neurons through pairwise correlation analysis of activations over a dataset of 5 million tokens. Ablation experiments reveal significant functional impacts of universal neurons on model predictions, measured via loss and KL divergence. Additionally, we quantify neuron persistence, demonstrating high stability of universal neurons across training checkpoints, particularly in deeper layers. These findings suggest stable and universal representational structures emerge during neural network training.', 'abstract_zh': '我们研究了独立训练的GPT-2 Small模型中神经元通用性的现象，考察这些在模型间表现出一致激活模式的通用神经元如何在整个训练过程中出现和演化。通过分析五个GPT-2模型在三个检查点（100k、200k、300k步）上的激活数据（共计500万tokens），我们利用成对相关性分析识别通用神经元。消融实验揭示了通用神经元对模型预测的显著功能影响，通过损失和KL散度进行测量。此外，我们量化了神经元的持久性，显示通用神经元在训练检查点之间具有高度稳定性，尤其是在较深层。这些发现表明，在神经网络训练过程中会形成稳定且通用的表示结构。', 'title_zh': 'GPT-2中的通用神经元：涌现、持久性和功能影响'}
{'arxiv_id': 'arXiv:2508.00843', 'title': 'Generative AI for CAD Automation: Leveraging Large Language Models for 3D Modelling', 'authors': 'Sumit Kumar, Sarthak Kapoor, Harsh Vardhan, Yao Zhao', 'link': 'https://arxiv.org/abs/2508.00843', 'abstract': 'Large Language Models (LLMs) are revolutionizing industries by enhancing efficiency, scalability, and innovation. This paper investigates the potential of LLMs in automating Computer-Aided Design (CAD) workflows, by integrating FreeCAD with LLM as CAD design tool. Traditional CAD processes are often complex and require specialized sketching skills, posing challenges for rapid prototyping and generative design. We propose a framework where LLMs generate initial CAD scripts from natural language descriptions, which are then executed and refined iteratively based on error feedback. Through a series of experiments with increasing complexity, we assess the effectiveness of this approach. Our findings reveal that LLMs perform well for simple to moderately complex designs but struggle with highly constrained models, necessitating multiple refinements. The study highlights the need for improved memory retrieval, adaptive prompt engineering, and hybrid AI techniques to enhance script robustness. Future directions include integrating cloud-based execution and exploring advanced LLM capabilities to further streamline CAD automation. This work underscores the transformative potential of LLMs in design workflows while identifying critical areas for future development.', 'abstract_zh': '大型语言模型（LLMs）正在通过提高效率、可扩展性和创新能力来重塑各行各业。本文研究了LLMs在将FreeCAD集成为其CAD设计工具时，在自动化CAD工作流程方面的潜力。传统的CAD流程通常非常复杂，并需要专门的绘图技能，这为快速原型制作和生成设计带来了挑战。我们提出了一种框架，其中LLMs从自然语言描述生成初始CAD脚本，然后根据错误反馈进行迭代执行和 refinement。通过一系列逐渐增加复杂性的实验，我们评估了该方法的有效性。研究结果表明，LLMs在简单到中等复杂的设计方面表现良好，但在高度约束的模型方面面临困难，需要多次调整。研究强调了改进记忆检索、适应性提示工程和混合AI技术以增强脚本稳健性的需求。未来的研究方向包括集成基于云的执行和探索更高级的LLM功能，以进一步简化CAD自动化。这项工作突显了LLMs在设计流程中的变革潜力，并指出了未来发展的关键领域。', 'title_zh': 'Generative AI for CAD Automation: 利用大规模语言模型进行3D建模'}
{'arxiv_id': 'arXiv:2508.00838', 'title': 'The Attribution Crisis in LLM Search Results', 'authors': "Ilan Strauss, Jangho Yang, Tim O'Reilly, Sruly Rosenblat, Isobel Moure", 'link': 'https://arxiv.org/abs/2508.00838', 'abstract': 'Web-enabled LLMs frequently answer queries without crediting the web pages they consume, creating an "attribution gap" - the difference between relevant URLs read and those actually cited. Drawing on approximately 14,000 real-world LMArena conversation logs with search-enabled LLM systems, we document three exploitation patterns: 1) No Search: 34% of Google Gemini and 24% of OpenAI GPT-4o responses are generated without explicitly fetching any online content; 2) No citation: Gemini provides no clickable citation source in 92% of answers; 3) High-volume, low-credit: Perplexity\'s Sonar visits approximately 10 relevant pages per query but cites only three to four. A negative binomial hurdle model shows that the average query answered by Gemini or Sonar leaves about 3 relevant websites uncited, whereas GPT-4o\'s tiny uncited gap is best explained by its selective log disclosures rather than by better attribution. Citation efficiency - extra citations provided per additional relevant web page visited - varies widely across models, from 0.19 to 0.45 on identical queries, underscoring that retrieval design, not technical limits, shapes ecosystem impact. We recommend a transparent LLM search architecture based on standardized telemetry and full disclosure of search traces and citation logs.', 'abstract_zh': 'Web-enable的LLM经常在不引用所消费的网页的情况下回答查询，从而产生“归因差距”——即阅读的相关URL与实际引用之间的差异。基于约14,000条实际的LMArena对话日志，我们记录了三种利用模式：1）无搜索：34%的Google Gemini和24%的OpenAI GPT-4o的回答是在未明确检索任何在线内容的情况下生成的；2）无引用：Gemini在92%的答案中未提供可点击的引用来源；3）高流量、低引用：Perplexity的Sonar每次查询访问约10个相关信息网页但仅引用其中三至四个。负二项 hurdle 模型显示，Gemini或Sonar回答的平均查询会留下约3个相关网站未被引用，而GPT-4o较小的未被引用差距与其选择性的日志披露有关，而不是更好的归因机制。每访问一个额外的相关网页提供的引用效率——即额外引用的数量——在不同模型间存在巨大差异，从相同的查询中不同模型的0.19到0.45不等，这表明检索设计而非技术限制塑造了生态系统影响。我们建议基于标准化遥测技术的透明LLM搜索架构，并全面披露搜索轨迹和引用日志。', 'title_zh': 'LLM搜索结果中的归因危机'}
{'arxiv_id': 'arXiv:2507.22929', 'title': 'EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow', 'authors': 'Xiaoyu Pan, Yang Bai, Ke Zou, Yang Zhou, Jun Zhou, Huazhu Fu, Yih-Chung Tham, Yong Liu', 'link': 'https://arxiv.org/abs/2507.22929', 'abstract': "Medical Large Language Models (MLLMs) play a crucial role in ophthalmic diagnosis, holding significant potential to address vision-threatening diseases. However, their accuracy is constrained by hallucinations stemming from limited ophthalmic knowledge, insufficient visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data, which collectively impede precise lesion detection and disease diagnosis. Furthermore, existing medical benchmarks fail to effectively evaluate various types of hallucinations or provide actionable solutions to mitigate them. To address the above challenges, we introduce EH-Benchmark, a novel ophthalmology benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs' hallucinations based on specific tasks and error types into two primary classes: Visual Understanding and Logical Composition, each comprising multiple subclasses. Given that MLLMs predominantly rely on language-based reasoning rather than visual processing, we propose an agent-centric, three-phase framework, including the Knowledge-Level Retrieval stage, the Task-Level Case Studies stage, and the Result-Level Validation stage. Experimental results show that our multi-agent framework significantly mitigates both types of hallucinations, enhancing accuracy, interpretability, and reliability. Our project is available at this https URL.", 'abstract_zh': 'Medical大型语言模型（MLLMs）在眼科诊断中发挥着关键作用，具有解决致盲疾病的重要潜力。然而，它们的准确性受限于由眼科知识有限、视觉定位和推理能力不足以及多模态眼科数据稀缺导致的幻觉，这些因素共同阻碍了精确病灶检测和疾病诊断。此外，现有的医疗基准未能有效评估各种类型的幻觉或提供有效的解决措施。为了应对这些挑战，我们引入了EH-Benchmark，这是一种新型的眼科基准，旨在评估MLLMs中的幻觉。我们将MLLMs的幻觉根据不同任务和错误类型分为两类：视觉理解与逻辑组合，并分别包含多个子类别。鉴于MLLMs主要依赖基于语言的推理而非视觉处理，我们提出了一种以代理为中心的三阶段框架，包括知识级别检索阶段、任务级别案例研究阶段和结果级别验证阶段。实验结果显示，我们的多代理框架显著降低了两种类型的幻觉，提高了准确率、可解释性和可靠性。该项目可访问：这个链接。', 'title_zh': 'EH-Benchmark 视网膜幻觉基准和基于代理的自上而下可追溯推理工作流'}
