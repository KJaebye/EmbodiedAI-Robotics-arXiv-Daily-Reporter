{'arxiv_id': 'arXiv:2508.02187', 'title': 'A Moment Matching-Based Method for Sparse and Noisy Point Cloud Registration', 'authors': 'Xingyi Li, Han Zhang, Ziliang Wang, Yukai Yang, Weidong Chen', 'link': 'https://arxiv.org/abs/2508.02187', 'abstract': 'Point cloud registration is a key step in robotic perception tasks, such as Simultaneous Localization and Mapping (SLAM). It is especially challenging in conditions with sparse points and heavy noise. Traditional registration methods, such as Iterative Closest Point (ICP) and Normal Distributions Transform (NDT), often have difficulties in achieving a robust and accurate alignment under these conditions. In this paper, we propose a registration framework based on moment matching. In particular, the point clouds are regarded as i.i.d. samples drawn from the same distribution observed in the source and target frames. We then match the generalized Gaussian Radial Basis moments calculated from the point clouds to estimate the rigid transformation between two frames. Moreover, such method does not require explicit point-to-point correspondences among the point clouds. We further show the consistency of the proposed method. Experiments on synthetic and real-world datasets show that our approach achieves higher accuracy and robustness than existing methods. In addition, we integrate our framework into a 4D Radar SLAM system. The proposed method significantly improves the localization performance and achieves results comparable to LiDAR-based systems. These findings demonstrate the potential of moment matching technique for robust point cloud registration in sparse and noisy scenarios.', 'abstract_zh': '基于矩匹配的点云注册框架及其在稀疏噪声条件下的应用', 'title_zh': '基于矩匹配的稀疏且噪声点云配准方法'}
{'arxiv_id': 'arXiv:2508.02146', 'title': 'ScrewSplat: An End-to-End Method for Articulated Object Recognition', 'authors': 'Seungyeon Kim, Junsu Ha, Young Hun Kim, Yonghyeon Lee, Frank C. Park', 'link': 'https://arxiv.org/abs/2508.02146', 'abstract': "Articulated object recognition -- the task of identifying both the geometry and kinematic joints of objects with movable parts -- is essential for enabling robots to interact with everyday objects such as doors and laptops. However, existing approaches often rely on strong assumptions, such as a known number of articulated parts; require additional inputs, such as depth images; or involve complex intermediate steps that can introduce potential errors -- limiting their practicality in real-world settings. In this paper, we introduce ScrewSplat, a simple end-to-end method that operates solely on RGB observations. Our approach begins by randomly initializing screw axes, which are then iteratively optimized to recover the object's underlying kinematic structure. By integrating with Gaussian Splatting, we simultaneously reconstruct the 3D geometry and segment the object into rigid, movable parts. We demonstrate that our method achieves state-of-the-art recognition accuracy across a diverse set of articulated objects, and further enables zero-shot, text-guided manipulation using the recovered kinematic model.", 'abstract_zh': '带关节物体识别——旨在识别具有可动部件的物体的几何形状和运动关节——对于使机器人能够与日常物品如门窗和笔记本电脑交互至关重要。然而，现有方法通常依赖强有力的假设，如已知的可动部件数量；需要额外输入，如深度图像；或者涉及复杂的中间步骤，可能导致潜在错误——这限制了它们在实际环境中的实用性。在本文中，我们引入了ScrewSplat，这是一种仅基于RGB观察的简单端到端方法。我们的方法首先随机初始化螺纹轴，然后通过迭代优化来恢复物体的基本运动结构。通过与高斯点划法结合，我们同时重建了3D几何形状，并将物体分割成刚性可动部分。我们证明了该方法在多种带关节物体上的识别准确性达到了最先进的水平，并通过恢复的动力学模型进一步实现了零样本、基于文本的操纵能力。', 'title_zh': '螺栓爆裂：一种端到端的articulated对象识别方法'}
{'arxiv_id': 'arXiv:2508.01583', 'title': 'Adverse Weather-Independent Framework Towards Autonomous Driving Perception through Temporal Correlation and Unfolded Regularization', 'authors': 'Wei-Bin Kou, Guangxu Zhu, Rongguang Ye, Jingreng Lei, Shuai Wang, Qingfeng Lin, Ming Tang, Yik-Chung Wu', 'link': 'https://arxiv.org/abs/2508.01583', 'abstract': 'Various adverse weather conditions such as fog and rain pose a significant challenge to autonomous driving (AD) perception tasks like semantic segmentation, object detection, etc. The common domain adaption strategy is to minimize the disparity between images captured in clear and adverse weather conditions. However, domain adaption faces two challenges: (I) it typically relies on utilizing clear image as a reference, which is challenging to obtain in practice; (II) it generally targets single adverse weather condition and performs poorly when confronting the mixture of multiple adverse weather conditions. To address these issues, we introduce a reference-free and Adverse weather condition-independent (Advent) framework (rather than a specific model architecture) that can be implemented by various backbones and heads. This is achieved by leveraging the homogeneity over short durations, getting rid of clear reference and being generalizable to arbitrary weather condition. Specifically, Advent includes three integral components: (I) Locally Sequential Mechanism (LSM) leverages temporal correlations between adjacent frames to achieve the weather-condition-agnostic effect thanks to the homogeneity behind arbitrary weather condition; (II) Globally Shuffled Mechanism (GSM) is proposed to shuffle segments processed by LSM from different positions of input sequence to prevent the overfitting to LSM-induced temporal patterns; (III) Unfolded Regularizers (URs) are the deep unfolding implementation of two proposed regularizers to penalize the model complexity to enhance across-weather generalization. We take the semantic segmentation task as an example to assess the proposed Advent framework. Extensive experiments demonstrate that the proposed Advent outperforms existing state-of-the-art baselines with large margins.', 'abstract_zh': '无参考且独立于恶劣天气条件的(Advent)框架：针对语义分割任务的评估', 'title_zh': '基于 temporal 相关性和展开正则化的独立恶劣天气自主驾驶感知框架'}
{'arxiv_id': 'arXiv:2508.01014', 'title': 'Hestia: Hierarchical Next-Best-View Exploration for Systematic Intelligent Autonomous Data Collection', 'authors': 'Cheng-You Lu, Zhuoli Zhuang, Nguyen Thanh Trung Le, Da Xiao, Yu-Cheng Chang, Thomas Do, Srinath Sridhar, Chin-teng Lin', 'link': 'https://arxiv.org/abs/2508.01014', 'abstract': 'Advances in 3D reconstruction and novel view synthesis have enabled efficient, photorealistic rendering, but the data collection process remains largely manual, making it time-consuming and labor-intensive. To address the challenges, this study introduces Hierarchical Next-Best-View Exploration for Systematic Intelligent Autonomous Data Collection (Hestia), which leverages reinforcement learning to learn a generalizable policy for 5-DoF next-best viewpoint prediction. Unlike prior approaches, Hestia systematically defines the next-best-view task by proposing core components such as dataset choice, observation design, action space, reward calculation, and learning schemes, forming a foundation for the planner. Hestia goes beyond prior next-best-view approaches and traditional capture systems through integration and validation in a real-world setup, where a drone serves as a mobile sensor for active scene exploration. Experimental results show that Hestia performs robustly across three datasets and translated object settings in the NVIDIA IsaacLab environment, and proves feasible for real-world deployment.', 'abstract_zh': '层级最佳视图探索在系统智能自主数据采集中的进展（Hestia）：基于强化学习的5-自由度最佳视图预测策略', 'title_zh': 'Hestia: 分级 next-best-view 探索用于系统化智能自主数据收集'}
{'arxiv_id': 'arXiv:2508.00917', 'title': 'A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles', 'authors': 'Jiayuan Wang, Farhad Pourpanah, Q. M. Jonathan Wu, Ning Zhang', 'link': 'https://arxiv.org/abs/2508.00917', 'abstract': 'Connected autonomous vehicles (CAVs) must simultaneously perform multiple tasks, such as object detection, semantic segmentation, depth estimation, trajectory prediction, motion prediction, and behaviour prediction, to ensure safe and reliable navigation in complex environments. Vehicle-to-everything (V2X) communication enables cooperative driving among CAVs, thereby mitigating the limitations of individual sensors, reducing occlusions, and improving perception over long distances. Traditionally, these tasks are addressed using distinct models, which leads to high deployment costs, increased computational overhead, and challenges in achieving real-time performance. Multi-task learning (MTL) has recently emerged as a promising solution that enables the joint learning of multiple tasks within a single unified model. This offers improved efficiency and resource utilization. To the best of our knowledge, this survey is the first comprehensive review focused on MTL in the context of CAVs. We begin with an overview of CAVs and MTL to provide foundational background. We then explore the application of MTL across key functional modules, including perception, prediction, planning, control, and multi-agent collaboration. Finally, we discuss the strengths and limitations of existing methods, identify key research gaps, and provide directions for future research aimed at advancing MTL methodologies for CAV systems.', 'abstract_zh': '连接自主车辆（CAVs）必须同时执行多个任务，例如物体检测、语义分割、深度估计、轨迹预测、运动预测和行为预测，以确保在复杂环境中实现安全可靠的导航。车辆对万物（V2X）通信使CAVs之间能够实现协同驾驶，从而缓解单一传感器的限制、减少遮挡，并提高远程感知性能。传统上，这些任务是使用不同的模型分别解决的，这导致部署成本高、计算量增加，并且难以实现实时性能。多任务学习（MTL）最近被认为是一种有前途的解决方案，能够在一个统一模型中联合学习多个任务。这一方法提高了效率并优化了资源利用。据我们所知，这是首个专注于CAVs背景下MTL的全面综述。我们从介绍CAVs和MTL开始，提供基础背景。然后探讨MTL在感知、预测、规划、控制和多智能体协作等关键功能模块中的应用。最后，讨论现有方法的优势和局限性，确定关键的研究缺口，并为未来旨在推进MTL方法论的研究提供方向。', 'title_zh': '连接自动驾驶车辆中深度多任务学习综述'}
{'arxiv_id': 'arXiv:2508.00900', 'title': 'Sparse 3D Perception for Rose Harvesting Robots: A Two-Stage Approach Bridging Simulation and Real-World Applications', 'authors': 'Taha Samavati, Mohsen Soryani, Sina Mansouri', 'link': 'https://arxiv.org/abs/2508.00900', 'abstract': 'The global demand for medicinal plants, such as Damask roses, has surged with population growth, yet labor-intensive harvesting remains a bottleneck for scalability. To address this, we propose a novel 3D perception pipeline tailored for flower-harvesting robots, focusing on sparse 3D localization of rose centers. Our two-stage algorithm first performs 2D point-based detection on stereo images, followed by depth estimation using a lightweight deep neural network. To overcome the challenge of scarce real-world labeled data, we introduce a photorealistic synthetic dataset generated via Blender, simulating a dynamic rose farm environment with precise 3D annotations. This approach minimizes manual labeling costs while enabling robust model training. We evaluate two depth estimation paradigms: a traditional triangulation-based method and our proposed deep learning framework. Results demonstrate the superiority of our method, achieving an F1 score of 95.6% (synthetic) and 74.4% (real) in 2D detection, with a depth estimation error of 3% at a 2-meter range on synthetic data. The pipeline is optimized for computational efficiency, ensuring compatibility with resource-constrained robotic systems. By bridging the domain gap between synthetic and real-world data, this work advances agricultural automation for specialty crops, offering a scalable solution for precision harvesting.', 'abstract_zh': '全球医药植物需求，如玫瑰的需求随着人口增长而增加，但由于劳动密集型采摘仍是限制规模化生产的瓶颈，我们提出了一种专门针对花卉收割机器人的新型3D感知管道，重点在于玫瑰中心的稀疏3D定位。该两阶段算法首先在立体图像上进行2D点检测，然后使用轻量级深度神经网络估计深度。为克服稀缺的真实世界标注数据难题，我们引入了一种通过Blender生成的逼真合成数据集，模拟具有精确3D注释的动态玫瑰农场环境。这种方法可以最大限度地减少人工标注成本，同时使模型训练更具鲁棒性。我们评估了两种深度估计框架：传统三角测量方法和我们提出的深度学习框架。结果表明，我们的方法在2D检测中的F1分数分别为95.6%（合成数据）和74.4%（真实数据），在合成数据2米范围内深度估计误差为3%。该管道优化了计算效率，确保与资源受限的机器人系统兼容。通过弥合合成数据与真实世界数据之间的领域差距，本工作推进了特色作物的农业自动化，提供了精准采摘的可扩展解决方案。', 'title_zh': '玫瑰采摘机器人基于稀疏3D感知的两阶段方法：从仿真到实际应用'}
{'arxiv_id': 'arXiv:2508.02549', 'title': 'MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming', 'authors': 'Shuo Wang, Yongcai Wang, Wanting Li, Yucheng Wang, Maiyue Chen, Kaihui Wang, Zhizhong Su, Xudong Cai, Yeying Jin, Deying Li, Zhaoxin Fan', 'link': 'https://arxiv.org/abs/2508.02549', 'abstract': 'Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth inputs to provide rich spatial cues for action planning, but these sensors can be costly or less accessible in real-world deployments. Recent approaches based on Vision-Language Action (VLA) models achieve strong results with monocular input, yet they still lag behind methods using panoramic RGB-D information. We present MonoDream, a lightweight VLA framework that enables monocular agents to learn a Unified Navigation Representation (UNR). This shared feature representation jointly aligns navigation-relevant visual semantics (e.g., global layout, depth, and future cues) and language-grounded action intent, enabling more reliable action prediction. MonoDream further introduces Latent Panoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to predict latent features of panoramic RGB and depth observations at both current and future steps based on only monocular input. Experiments on multiple VLN benchmarks show that MonoDream consistently improves monocular navigation performance and significantly narrows the gap with panoramic-based agents.', 'abstract_zh': 'MonoDream：基于单目输入的统一导航表示学习框架', 'title_zh': 'MonoDream：单目视觉-语言导航与全景梦境'}
{'arxiv_id': 'arXiv:2508.02339', 'title': 'Correspondence-Free Fast and Robust Spherical Point Pattern Registration', 'authors': 'Anik Sarker, Alan T. Asbeck', 'link': 'https://arxiv.org/abs/2508.02339', 'abstract': 'Existing methods for rotation estimation between two spherical ($\\mathbb{S}^2$) patterns typically rely on spherical cross-correlation maximization between two spherical function. However, these approaches exhibit computational complexities greater than cubic $O(n^3)$ with respect to rotation space discretization and lack extensive evaluation under significant outlier contamination. To this end, we propose a rotation estimation algorithm between two spherical patterns with linear time complexity $O(n)$. Unlike existing spherical-function-based methods, we explicitly represent spherical patterns as discrete 3D point sets on the unit sphere, reformulating rotation estimation as a spherical point-set alignment (i.e., Wahba problem for 3D unit vectors). Given the geometric nature of our formulation, our spherical pattern alignment algorithm naturally aligns with the Wahba problem framework for 3D unit vectors. Specifically, we introduce three novel algorithms: (1) SPMC (Spherical Pattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) a hybrid approach (SPMC+FRS) that combines the advantages of the previous two methods. Our experiments demonstrate that in the $\\mathbb{S}^2$ domain and in correspondence-free settings, our algorithms are over 10x faster and over 10x more accurate than current state-of-the-art methods for the Wahba problem with outliers. We validate our approach through extensive simulations on a new dataset of spherical patterns, the ``Robust Vector Alignment Dataset. "Furthermore, we adapt our methods to two real-world tasks: (i) Point Cloud Registration (PCR) and (ii) rotation estimation for spherical images.', 'abstract_zh': '基于球面模式的旋转估计：线性时间复杂度的新型算法', 'title_zh': 'correspondence-free快速且稳健的球面点模式注册'}
{'arxiv_id': 'arXiv:2508.01936', 'title': 'CVD-SfM: A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes', 'authors': 'Yaxuan Li, Yewei Huang, Bijay Gaudel, Hamidreza Jafarnejadsani, Brendan Englot', 'link': 'https://arxiv.org/abs/2508.01936', 'abstract': 'We present a novel multi-altitude camera pose estimation system, addressing the challenges of robust and accurate localization across varied altitudes when only considering sparse image input. The system effectively handles diverse environmental conditions and viewpoint variations by integrating the cross-view transformer, deep features, and structure-from-motion into a unified framework. To benchmark our method and foster further research, we introduce two newly collected datasets specifically tailored for multi-altitude camera pose estimation; datasets of this nature remain rare in the current literature. The proposed framework has been validated through extensive comparative analyses on these datasets, demonstrating that our system achieves superior performance in both accuracy and robustness for multi-altitude sparse pose estimation tasks compared to existing solutions, making it well suited for real-world robotic applications such as aerial navigation, search and rescue, and automated inspection.', 'abstract_zh': '我们提出了一种新颖的多海拔相机姿态估计系统，解决了仅考虑稀疏图像输入时在不同海拔高度下实现稳健且准确定位的挑战。该系统通过将跨视图变换器、深度特征和结构从运动集成到统一框架中，有效地处理了各种环境条件和视角变化。为了对该方法进行基准测试并推动进一步研究，我们引入了两个专门为多海拔相机姿态估计收集的新数据集；这类数据集目前在文献中较为罕见。所提出的框架通过对这些数据集进行广泛的比较分析得到了验证，表明我们的系统在多海拔稀疏姿态估计任务中比现有解决方案在准确性和鲁棒性方面表现更优，使其非常适合用于实际的机器人应用，如高空导航、搜救和自动化检测。', 'title_zh': 'CVD-SfM：多海拔场景稀疏定位的跨视图深度前端结构从运动系统'}
{'arxiv_id': 'arXiv:2508.01836', 'title': 'A Simple Algebraic Solution for Estimating the Pose of a Camera from Planar Point Features', 'authors': 'Tarek Bouazza, Tarek Hamel, Claude Samson', 'link': 'https://arxiv.org/abs/2508.01836', 'abstract': "This paper presents a simple algebraic method to estimate the pose of a camera relative to a planar target from $n \\geq 4$ reference points with known coordinates in the target frame and their corresponding bearing measurements in the camera frame. The proposed approach follows a hierarchical structure; first, the unit vector normal to the target plane is determined, followed by the camera's position vector, its distance to the target plane, and finally, the full orientation. To improve the method's robustness to measurement noise, an averaging methodology is introduced to refine the estimation of the target's normal direction. The accuracy and robustness of the approach are validated through extensive experiments.", 'abstract_zh': '本文提出了一种简单代数方法，用于从目标框架中已知坐标且在摄像机框架中有相应视线测量的至少4个参考点，估计摄像机相对于平面目标的姿态。该提出的方案遵循层次结构；首先确定目标平面的单位法向量，随后确定摄像机的位置矢量、到目标平面的距离，最后确定完整的姿态方向。为了提高方法对测量噪声的鲁棒性，引入了一种平均方法来细化目标法向方向的估计。通过广泛的实验验证了该方法的准确性和鲁棒性。', 'title_zh': '从平面点特征估算相机姿态的简单代数解法'}
{'arxiv_id': 'arXiv:2508.01778', 'title': 'DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving via Online HD Map Diffusion', 'authors': 'Zhigang Sun, Yiru Wang, Anqing Jiang, Shuo Wang, Yu Gao, Yuwen Heng, Shouyi Zhang, An He, Hao Jiang, Jinhao Chai, Zichong Gu, Wang Jijun, Shichen Tang, Lavdim Halilaj, Juergen Luettin, Hao Sun', 'link': 'https://arxiv.org/abs/2508.01778', 'abstract': 'Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusion -- a fusion framework for multimodal trajectory prediction and planning. Our approach reasons over a semantic raster-fused BEV space, enhanced by a map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving a 5.1\\% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art results, with a 15\\% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at this https URL.', 'abstract_zh': '自主驾驶需要准确的场景理解，包括道路几何、交通代理及其语义关系。在在线高清地图生成场景中，基于栅格的表示适合视觉模型但缺乏几何精度，而基于图的表示保留了结构细节但在没有精确地图的情况下会变得不稳定。为了充分利用两者的互补优势，我们提出了DiffSemanticFusion——一种多模态轨迹预测和规划的融合框架。我们的方法在语义栅格融合的BEV空间中进行推理，并通过地图扩散模块提高了在线高清地图表示的稳定性和表达性。我们在轨迹预测和规划导向的端到端自主驾驶两个下游任务上验证了该框架。在nuScenes和NASIM等真实世界自主驾驶基准上的实验表明，该框架相对于几种最先进的方法具有更好的性能。在nuScenes预测任务中，我们将DiffSemanticFusion与基于在线高清地图的QCNet结合使用，实现了5.1%的性能提升。在NASIM端到端自主驾驶中，DiffSemanticFusion在NavHard场景中达到了最先进的结果，性能提高了15%。此外，广泛的消融和敏感性研究显示，我们的地图扩散模块可以无缝集成到其他向量方法中以提升性能。所有成果均可在以下链接访问：this https URL。', 'title_zh': 'DiffSemanticFusion: 基于在线高精度地图扩散的语义栅格BEV融合技术在自动驾驶中的应用'}
{'arxiv_id': 'arXiv:2508.01423', 'title': '3DRot: 3D Rotation Augmentation for RGB-Based 3D Tasks', 'authors': 'Shitian Yang, Deyu Li, Xiaoke Jiang, Lei Zhang', 'link': 'https://arxiv.org/abs/2508.01423', 'abstract': "RGB-based 3D tasks, e.g., 3D detection, depth estimation, 3D keypoint estimation, still suffer from scarce, expensive annotations and a thin augmentation toolbox, since most image transforms, including resize and rotation, disrupt geometric consistency. In this paper, we introduce 3DRot, a plug-and-play augmentation that rotates and mirrors images about the camera's optical center while synchronously updating RGB images, camera intrinsics, object poses, and 3D annotations to preserve projective geometry-achieving geometry-consistent rotations and reflections without relying on any scene depth. We validate 3DRot with a classical 3D task, monocular 3D detection. On SUN RGB-D dataset, 3DRot raises $IoU_{3D}$ from 43.21 to 44.51, cuts rotation error (ROT) from 22.91$^\\circ$ to 20.93$^\\circ$, and boosts $mAP_{0.5}$ from 35.70 to 38.11. As a comparison, Cube R-CNN adds 3 other datasets together with SUN RGB-D for monocular 3D estimation, with a similar mechanism and test dataset, increases $IoU_{3D}$ from 36.2 to 37.8, boosts $mAP_{0.5}$ from 34.7 to 35.4. Because it operates purely through camera-space transforms, 3DRot is readily transferable to other 3D tasks.", 'abstract_zh': '基于RGB的3D任务，如3D检测、深度估计、3D关键点估计，仍然受到稀缺且昂贵的标注以及有限的增强工具箱的困扰，因为大多数图像变换，包括缩放和旋转，会破坏几何一致性。在本文中，我们介绍了一种即插即用增强方法3DRot，该方法在保持投影几何关系的前提下，围绕相机光学中心旋转和镜像图像，并同步更新RGB图像、相机内参、物体姿态和3D标注。3DRot实现了几何一致的旋转和反射，无需依赖任何场景深度。我们通过单目3D检测这一经典3D任务验证了3DRot。在SUN RGB-D数据集上，3DRot将$IoU_{3D}$从43.21提高到44.51，将旋转误差（ROT）从22.91°降低到20.93°，并将$ mAP_{0.5}$从35.70提升到38.11。相比之下，Cube R-CNN通过结合SUN RGB-D和其他3个数据集进行单目3D估计，采用相似的机制和测试集，将$IoU_{3D}$从36.2提升到37.8，将$ mAP_{0.5}$从34.7提升到35.4。由于仅通过相机空间变换操作，3DRot易于应用于其他3D任务。', 'title_zh': '3DRot: 基于RGB的3D任务的3D旋转增强'}
{'arxiv_id': 'arXiv:2508.01216', 'title': 'Perspective from a Broader Context: Can Room Style Knowledge Help Visual Floorplan Localization?', 'authors': 'Bolei Chen, Shengsheng Yan, Yongzheng Cui, Jiaxu Kang, Ping Zhong, Jianxin Wang', 'link': 'https://arxiv.org/abs/2508.01216', 'abstract': "Since a building's floorplan remains consistent over time and is inherently robust to changes in visual appearance, visual Floorplan Localization (FLoc) has received increasing attention from researchers. However, as a compact and minimalist representation of the building's layout, floorplans contain many repetitive structures (e.g., hallways and corners), thus easily result in ambiguous localization. Existing methods either pin their hopes on matching 2D structural cues in floorplans or rely on 3D geometry-constrained visual pre-trainings, ignoring the richer contextual information provided by visual images. In this paper, we suggest using broader visual scene context to empower FLoc algorithms with scene layout priors to eliminate localization uncertainty. In particular, we propose an unsupervised learning technique with clustering constraints to pre-train a room discriminator on self-collected unlabeled room images. Such a discriminator can empirically extract the hidden room type of the observed image and distinguish it from other room types. By injecting the scene context information summarized by the discriminator into an FLoc algorithm, the room style knowledge is effectively exploited to guide definite visual FLoc. We conducted sufficient comparative studies on two standard visual Floc benchmarks. Our experiments show that our approach outperforms state-of-the-art methods and achieves significant improvements in robustness and accuracy.", 'abstract_zh': '基于广泛视觉场景上下文的建筑平面图定位方法', 'title_zh': '从更广泛的视角来看：房间风格知识能帮助视觉floorplan定位吗？'}
{'arxiv_id': 'arXiv:2508.01197', 'title': 'A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding', 'authors': 'Zhan Shi, Song Wang, Junbo Chen, Jianke Zhu', 'link': 'https://arxiv.org/abs/2508.01197', 'abstract': 'Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within a bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce a benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it integrates natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multi-modal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. Additionally, a 2D grounding module and a depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at this https URL.', 'abstract_zh': '三维占用语义定位旨在基于自然语言描述在场景中识别对象或区域，对于自动驾驶中的空间感知至关重要。然而，现有的语义定位任务通常依赖于边界框，往往无法捕捉到细粒度的细节。边界盒内的所有体素并不一定被占用，导致对象表示不准确。为了解决这一问题，我们引入了一个针对具有挑战性的室外场景的三维占用语义定位基准。该基准基于nuScenes数据集，将自然语言与体素级别占用标注相结合，相比传统的语义定位任务提供了更精确的对象感知。此外，我们提出了一种名为GroundingOcc的端到端模型，用于通过多模态学习进行三维占用语义定位。该模型结合视觉、文本和点云特征，从粗到细预测对象位置和占用信息。具体而言，GroundingOcc包括一个用于特征提取的多模态编码器、一个用于体素级预测的占用头和一个用于细化定位的语义定位头。此外，2D语义定位模块和深度估计模块增强了几何理解，从而提高了模型性能。基准上的大量实验表明，我们的方法在三维占用语义定位上优于现有基线。数据集可在以下链接获取。', 'title_zh': '从粗到细的多模态3D占用语义 grounding 方法'}
{'arxiv_id': 'arXiv:2508.00967', 'title': 'Cooperative Perception: A Resource-Efficient Framework for Multi-Drone 3D Scene Reconstruction Using Federated Diffusion and NeRF', 'authors': 'Massoud Pourmandi', 'link': 'https://arxiv.org/abs/2508.00967', 'abstract': 'The proposal introduces an innovative drone swarm perception system that aims to solve problems related to computational limitations and low-bandwidth communication, and real-time scene reconstruction. The framework enables efficient multi-agent 3D/4D scene synthesis through federated learning of shared diffusion model and YOLOv12 lightweight semantic extraction and local NeRF updates while maintaining privacy and scalability. The framework redesigns generative diffusion models for joint scene reconstruction, and improves cooperative scene understanding, while adding semantic-aware compression protocols. The approach can be validated through simulations and potential real-world deployment on drone testbeds, positioning it as a disruptive advancement in multi-agent AI for autonomous systems.', 'abstract_zh': '一种基于联邦学习的无人机集群感知系统：面向计算限制、低宽带通信和实时场景重建的联合场景重建与协同场景理解', 'title_zh': '协同感知：一种基于联邦扩散和NeRF的多无人机三维场景重建高效框架'}
{'arxiv_id': 'arXiv:2508.02132', 'title': 'All Stories Are One Story: Emotional Arc Guided Procedural Game Level Generation', 'authors': 'Yunge Wen, Chenliang Huang, Hangyu Zhou, Zhuo Zeng, Chun Ming Louis Po, Julian Togelius, Timothy Merino, Sam Earle', 'link': 'https://arxiv.org/abs/2508.02132', 'abstract': 'The emotional arc is a universal narrative structure underlying stories across cultures and media -- an idea central to structuralist narratology, often encapsulated in the phrase "all stories are one story." We present a framework for procedural game narrative generation that incorporates emotional arcs as a structural backbone for both story progression and gameplay dynamics. Leveraging established narratological theories and large-scale empirical analyses, we focus on two core emotional patterns -- Rise and Fall -- to guide the generation of branching story graphs. Each story node is automatically populated with characters, items, and gameplay-relevant attributes (e.g., health, attack), with difficulty adjusted according to the emotional trajectory. Implemented in a prototype action role-playing game (ARPG), our system demonstrates how emotional arcs can be operationalized using large language models (LLMs) and adaptive entity generation. Evaluation through player ratings, interviews, and sentiment analysis shows that emotional arc integration significantly enhances engagement, narrative coherence, and emotional impact. These results highlight the potential of emotionally structured procedural generation for advancing interactive storytelling for games.', 'abstract_zh': '情感弧线是跨文化与媒体故事背后的普遍叙事结构——结构主义叙事学中的一个核心概念，常被概括为“所有故事都是同一个故事”。本文提出了一种程序化游戏叙事生成框架，将情感弧线作为故事进展和游戏动力学的结构性支柱。通过运用已有叙事学理论和大规模实证分析，我们关注两种核心情感模式——上升与下降——来指导分支故事图的生成。每个故事节点自动填充角色、物品以及与游戏相关的属性（例如，生命值、攻击力），难度调整依据情感轨迹。该系统在一款原型动作角色扮演游戏（ARPG）中实现，展示了如何利用大规模语言模型（LLMs）和自适应实体生成来操作情感弧线。通过玩家评分、访谈和情感分析的评估表明，情感弧线的整合显著提升了参与度、叙事连贯性和情感冲击力。这些结果突显了情感结构化程序生成在推进游戏互动叙事方面的潜在价值。', 'title_zh': '所有故事都是一个故事：情感弧引导的 procedchal 游戏关卡生成'}
{'arxiv_id': 'arXiv:2508.02093', 'title': '"Stack It Up!": 3D Stable Structure Generation from 2D Hand-drawn Sketch', 'authors': 'Yiqing Xu, Linfeng Li, Cunjun Yu, David Hsu', 'link': 'https://arxiv.org/abs/2508.02093', 'abstract': "Imagine a child sketching the Eiffel Tower and asking a robot to bring it to life. Today's robot manipulation systems can't act on such sketches directly-they require precise 3D block poses as goals, which in turn demand structural analysis and expert tools like CAD. We present StackItUp, a system that enables non-experts to specify complex 3D structures using only 2D front-view hand-drawn sketches. StackItUp introduces an abstract relation graph to bridge the gap between rough sketches and accurate 3D block arrangements, capturing the symbolic geometric relations (e.g., left-of) and stability patterns (e.g., two-pillar-bridge) while discarding noisy metric details from sketches. It then grounds this graph to 3D poses using compositional diffusion models and iteratively updates it by predicting hidden internal and rear supports-critical for stability but absent from the sketch. Evaluated on sketches of iconic landmarks and modern house designs, StackItUp consistently produces stable, multilevel 3D structures and outperforms all baselines in both stability and visual resemblance.", 'abstract_zh': '一种基于2D手绘前视图草图构建复杂3D结构的系统：StackItUp', 'title_zh': '“堆叠起来！”: 从2D手绘草图生成3D稳定结构'}
{'arxiv_id': 'arXiv:2508.01237', 'title': 'SketchAgent: Generating Structured Diagrams from Hand-Drawn Sketches', 'authors': 'Cheng Tan, Qi Chen, Jingxuan Wei, Gaowei Wu, Zhangyang Gao, Siyuan Li, Bihui Yu, Ruifeng Guo, Stan Z. Li', 'link': 'https://arxiv.org/abs/2508.01237', 'abstract': 'Hand-drawn sketches are a natural and efficient medium for capturing and conveying ideas. Despite significant advancements in controllable natural image generation, translating freehand sketches into structured, machine-readable diagrams remains a labor-intensive and predominantly manual task. The primary challenge stems from the inherent ambiguity of sketches, which lack the structural constraints and semantic precision required for automated diagram generation. To address this challenge, we introduce SketchAgent, a multi-agent system designed to automate the transformation of hand-drawn sketches into structured diagrams. SketchAgent integrates sketch recognition, symbolic reasoning, and iterative validation to produce semantically coherent and structurally accurate diagrams, significantly reducing the need for manual effort. To evaluate the effectiveness of our approach, we propose the Sketch2Diagram Benchmark, a comprehensive dataset and evaluation framework encompassing eight diverse diagram categories, such as flowcharts, directed graphs, and model architectures. The dataset comprises over 6,000 high-quality examples with token-level annotations, standardized preprocessing, and rigorous quality control. By streamlining the diagram generation process, SketchAgent holds great promise for applications in design, education, and engineering, while offering a significant step toward bridging the gap between intuitive sketching and machine-readable diagram generation. The benchmark is released at this https URL.', 'abstract_zh': '手绘草图是一种自然且高效的捕捉和传达想法的介质。尽管在可控自然图像生成方面取得了显著进展，但将自由手绘草图转化为结构化、机器可读的图表仍然是一个劳动密集且主要依赖手动的任务。主要挑战源于草图固有的含糊性，缺乏自动图表生成所需的结构约束和语义精度。为应对这一挑战，我们提出了SketchAgent，一个旨在自动化手绘草图转化为结构化图表的多代理系统。SketchAgent结合了草图识别、符号推理和迭代验证，生成语义连贯且结构准确的图表，大大减少了人工努力的需求。为了评估我们方法的有效性，我们提出了Sketch2Diagram基准，一个涵盖八种不同图表类别（如流程图、有向图和模型架构）的综合数据集和评估框架。该数据集包含超过6,000个高质量样本，具有标记级别的注解、标准化预处理和严格的质量控制。通过简化图表生成过程，SketchAgent在设计、教育和工程等领域具有巨大应用潜力，同时朝着直观草图绘制与机器可读图表生成之间的差距迈进。基准数据集可从此链接获取：this https URL。', 'title_zh': 'SketchAgent：从手绘草图生成结构化图表'}
{'arxiv_id': 'arXiv:2508.02362', 'title': 'Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering', 'authors': 'Xu Wang, Shengeng Tang, Fei Wang, Lechao Cheng, Dan Guo, Feng Xue, Richang Hong', 'link': 'https://arxiv.org/abs/2508.02362', 'abstract': 'Generating semantically coherent and visually accurate talking faces requires bridging the gap between linguistic meaning and facial articulation. Although audio-driven methods remain prevalent, their reliance on high-quality paired audio visual data and the inherent ambiguity in mapping acoustics to lip motion pose significant challenges in terms of scalability and robustness. To address these issues, we propose Text2Lip, a viseme-centric framework that constructs an interpretable phonetic-visual bridge by embedding textual input into structured viseme sequences. These mid-level units serve as a linguistically grounded prior for lip motion prediction. Furthermore, we design a progressive viseme-audio replacement strategy based on curriculum learning, enabling the model to gradually transition from real audio to pseudo-audio reconstructed from enhanced viseme features via cross-modal attention. This allows for robust generation in both audio-present and audio-free scenarios. Finally, a landmark-guided renderer synthesizes photorealistic facial videos with accurate lip synchronization. Extensive evaluations show that Text2Lip outperforms existing approaches in semantic fidelity, visual realism, and modality robustness, establishing a new paradigm for controllable and flexible talking face generation. Our project homepage is this https URL.', 'abstract_zh': '生成语义连贯且视觉准确的说话人脸需要弥合语言意义与面部articulation之间的差距。尽管基于音频的方法仍然占主导地位，但它们依赖于高质量的配对音频视觉数据，并且音高到口唇动作映射的固有不确定性对扩展性和鲁棒性构成了重大挑战。为了解决这些问题，我们提出了Text2Lip，这是一种以viseme为中心的框架，通过将文本输入嵌入结构化的viseme序列中来构建可解释的语音-视觉桥梁。这些中间级单元为口唇运动预测提供了语言学意义上的先验知识。此外，我们设计了一种基于课程学习的渐进viseme-音频替代策略，使得模型能够逐步从真实音频过渡到通过跨模态注意从增强的viseme特征中重建的伪音频。这使得模型在有音频和无音频场景中都能实现鲁棒生成。最后，基于关键点的渲染器合成具有准确唇部同步的光逼真面部视频。广泛评估表明，Text2Lip 在语义保真度、视觉逼真度和模态鲁棒性方面优于现有方法，建立了可控和灵活说话人脸生成的新范式。我们的项目主页是这个 https://url。', 'title_zh': 'Text2Lip: 基于辅音引导渲染的从文本生成渐进唇同步说话人脸'}
{'arxiv_id': 'arXiv:2508.02240', 'title': 'Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor', 'authors': 'Xiaoliu Guan, Lielin Jiang, Hanqi Chen, Xu Zhang, Jiaxing Yan, Guanzhong Wang, Yi Liu, Zetao Zhang, Yu Wu', 'link': 'https://arxiv.org/abs/2508.02240', 'abstract': 'Diffusion Transformers (DiTs) have demonstrated remarkable performance in visual generation tasks. However, their low inference speed limits their deployment in low-resource applications. Recent training-free approaches exploit the redundancy of features across timesteps by caching and reusing past representations to accelerate inference. Building on this idea, TaylorSeer instead uses cached features to predict future ones via Taylor expansion. However, its module-level prediction across all transformer blocks (e.g., attention or feedforward modules) requires storing fine-grained intermediate features, leading to notable memory and computation overhead. Moreover, it adopts a fixed caching schedule without considering the varying accuracy of predictions across timesteps, which can lead to degraded outputs when prediction fails. To address these limitations, we propose a novel approach to better leverage Taylor-based acceleration. First, we shift the Taylor prediction target from the module level to the last block level, significantly reducing the number of cached features. Furthermore, observing strong sequential dependencies among Transformer blocks, we propose to use the error between the Taylor-estimated and actual outputs of the first block as an indicator of prediction reliability. If the error is small, we trust the Taylor prediction for the last block; otherwise, we fall back to full computation, thereby enabling a dynamic caching mechanism. Empirical results show that our method achieves a better balance between speed and quality, achieving a 3.17x acceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible quality drop. The Project Page is \\href{this https URL}{here.}', 'abstract_zh': '基于Taylor展开加速的扩散变换器在视觉生成任务中的高效预测方法', 'title_zh': '基于信心门控泰勒级数的加速扩散模型：预报何时预报'}
{'arxiv_id': 'arXiv:2508.02172', 'title': 'GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting', 'authors': 'Lei Yao, Yi Wang, Yi Zhang, Moyun Liu, Lap-Pui Chau', 'link': 'https://arxiv.org/abs/2508.02172', 'abstract': 'The significance of informative and robust point representations has been widely acknowledged for 3D scene understanding. Despite existing self-supervised pre-training counterparts demonstrating promising performance, the model collapse and structural information deficiency remain prevalent due to insufficient point discrimination difficulty, yielding unreliable expressions and suboptimal performance. In this paper, we present GaussianCross, a novel cross-modal self-supervised 3D representation learning architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques to address current challenges. GaussianCross seamlessly converts scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian representation without missing details, enabling stable and generalizable pre-training. Subsequently, a tri-attribute adaptive distillation splatting module is incorporated to construct a 3D feature field, facilitating synergetic feature capturing of appearance, geometry, and semantic cues to maintain cross-modal consistency. To validate GaussianCross, we perform extensive evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In particular, GaussianCross shows a prominent parameter and data efficiency, achieving superior performance through linear probing (<0.1% parameters) and limited data training (1% of scenes) compared to state-of-the-art methods. Furthermore, GaussianCross demonstrates strong generalization capabilities, improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on ScanNet200 semantic and instance segmentation tasks, respectively, supporting the effectiveness of our approach. The code, weights, and visualizations are publicly available at \\href{this https URL}{this https URL}.', 'abstract_zh': 'GaussianCross：一种结合前向3D高斯散斑技术的跨模态自监督3D表示学习架构', 'title_zh': 'GaussianCross: 通过高斯点绘的跨模态自监督3D表示学习'}
{'arxiv_id': 'arXiv:2508.02155', 'title': 'DreamPainter: Image Background Inpainting for E-commerce Scenarios', 'authors': 'Sijie Zhao, Jing Cheng, Yaoyao Wu, Hao Xu, Shaohui Jiao', 'link': 'https://arxiv.org/abs/2508.02155', 'abstract': 'Although diffusion-based image genenation has been widely explored and applied, background generation tasks in e-commerce scenarios still face significant challenges. The first challenge is to ensure that the generated products are consistent with the given product inputs while maintaining a reasonable spatial arrangement, harmonious shadows, and reflections between foreground products and backgrounds. Existing inpainting methods fail to address this due to the lack of domain-specific data. The second challenge involves the limitation of relying solely on text prompts for image control, as effective integrating visual information to achieve precise control in inpainting tasks remains underexplored. To address these challenges, we introduce DreamEcom-400K, a high-quality e-commerce dataset containing accurate product instance masks, background reference images, text prompts, and aesthetically pleasing product images. Based on this dataset, we propose DreamPainter, a novel framework that not only utilizes text prompts for control but also flexibly incorporates reference image information as an additional control signal. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, maintaining high product consistency while effectively integrating both text prompt and reference image information.', 'abstract_zh': '虽然基于扩散的过程在图像生成方面的应用已经广泛探索和应用，但在电子商务场景中的背景生成任务仍然面临重大挑战。首先，生成的产品必须与给定的产品输入保持一致，同时保持合理的空间布局、和谐的阴影和前景产品与背景之间的反射。现有的修复方法由于缺乏特定领域的数据而无法解决这一问题。其次，仅依赖文本提示对图像进行控制存在局限性，因为将视觉信息有效地整合以实现精确的修复控制尚未得到充分探索。为了解决这些问题，我们引入了包含准确的产品实例掩码、背景参考图像、文本提示和美观的产品图像的高质量电子商务数据集DreamEcom-400K。基于此数据集，我们提出了DreamPainter，这是一种新颖的框架，不仅利用文本提示进行控制，还灵活地将参考图像信息作为额外的控制信号。广泛的实验证明，我们的方法在保持高产品一致性的同时，能够有效地整合文本提示和参考图像信息，显著优于现有最先进的方法。', 'title_zh': 'DreamPainter: 电商场景中的图像背景修复'}
{'arxiv_id': 'arXiv:2508.01941', 'title': 'Less is More: AMBER-AFNO -- a New Benchmark for Lightweight 3D Medical Image Segmentation', 'authors': 'Andrea Dosi, Semanto Mondal, Rajib Chandra Ghosh, Massimo Brescia, Giuseppe Longo', 'link': 'https://arxiv.org/abs/2508.01941', 'abstract': 'This work presents the results of a methodological transfer from remote sensing to healthcare, adapting AMBER -- a transformer-based model originally designed for multiband images, such as hyperspectral data -- to the task of 3D medical datacube segmentation. In this study, we use the AMBER architecture with Adaptive Fourier Neural Operators (AFNO) in place of the multi-head self-attention mechanism. While existing models rely on various forms of attention to capture global context, AMBER-AFNO achieves this through frequency-domain mixing, enabling a drastic reduction in model complexity. This design reduces the number of trainable parameters by over 80% compared to UNETR++, while maintaining a FLOPs count comparable to other state-of-the-art architectures. Model performance is evaluated on two benchmark 3D medical datasets -- ACDC and Synapse -- using standard metrics such as Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD), demonstrating that AMBER-AFNO achieves competitive or superior accuracy with significant gains in training efficiency, inference speed, and memory usage.', 'abstract_zh': '将遥感领域的技术转移至医疗健康领域：AMBER-AFNO在3D医疗数据立方分割中的应用', 'title_zh': 'fewer is more: AMBER-AFNO — 一个新的轻量级3D医学图像分割基准'}
{'arxiv_id': 'arXiv:2508.01928', 'title': 'IAUNet: Instance-Aware U-Net', 'authors': 'Yaroslav Prytula, Illia Tsiporenko, Ali Zeynalli, Dmytro Fishman', 'link': 'https://arxiv.org/abs/2508.01928', 'abstract': 'Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at this https URL', 'abstract_zh': '基于查询的实例分割在生物医学成像中的细胞识别至关重要，以准确区分重叠且大小不一的个体对象。近期基于查询的方法通过对象查询引导分割，展现了出色的表现。尽管U-Net在医学图像分割中常被选用，但在基于查询的方法中的潜力尚未被充分探索。在此工作中，我们提出IAUNet，一种新颖的基于查询的U-Net架构。核心设计采用完整的U-Net架构，并通过一种新颖的轻量级像素解码器进行增强，使模型更加高效并减少参数数量。此外，我们提出了一种变换器解码器，以在多个尺度上细化对象特异性特征。最后，我们引入了2025 Revvity全细胞分割数据集，这是一个独特的资源，包含详细的亮视野图像中重叠细胞质的注释，为生物医学实例分割设立了新的基准。在多个公开数据集和我们自己的数据集上的实验显示，IAUNet在大多数最先进的完全卷积、变换器基和基于查询模型以及细胞分割特定模型中表现出色，为细胞实例分割任务提供了强有力的基础。相关代码可访问以下链接：this https URL。', 'title_zh': 'IAUNet：实例感知的U-Net'}
{'arxiv_id': 'arXiv:2508.01845', 'title': 'Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and Defenses in Computer Vision Systems', 'authors': 'Zhongliang Guo, Yifei Qian, Yanli Li, Weiye Li, Chun Tong Lei, Shuai Zhao, Lei Fang, Ognjen Arandjelović, Chun Pong Lau', 'link': 'https://arxiv.org/abs/2508.01845', 'abstract': 'Adversarial attacks against computer vision systems have emerged as a critical research area that challenges the fundamental assumptions about neural network robustness and security. This comprehensive survey examines the evolving landscape of adversarial techniques, revealing their dual nature as both sophisticated security threats and valuable defensive tools. We provide a systematic analysis of adversarial attack methodologies across three primary domains: pixel-space attacks, physically realizable attacks, and latent-space attacks. Our investigation traces the technical evolution from early gradient-based methods such as FGSM and PGD to sophisticated optimization techniques incorporating momentum, adaptive step sizes, and advanced transferability mechanisms. We examine how physically realizable attacks have successfully bridged the gap between digital vulnerabilities and real-world threats through adversarial patches, 3D textures, and dynamic optical perturbations. Additionally, we explore the emergence of latent-space attacks that leverage semantic structure in internal representations to create more transferable and meaningful adversarial examples. Beyond traditional offensive applications, we investigate the constructive use of adversarial techniques for vulnerability assessment in biometric authentication systems and protection against malicious generative models. Our analysis reveals critical research gaps, particularly in neural style transfer protection and computational efficiency requirements. This survey contributes a comprehensive taxonomy, evolution analysis, and identification of future research directions, aiming to advance understanding of adversarial vulnerabilities and inform the development of more robust and trustworthy computer vision systems.', 'abstract_zh': '对抗攻击对计算机视觉系统的攻击已成为一个关键的研究领域，挑战了神经网络鲁棒性和安全性的一些基本假设。本文综述了对抗技术的发展景观，揭示了它们作为高级安全威胁和宝贵防御工具的双重性质。本文系统地分析了三种主要领域中的对抗攻击方法：像素空间攻击、物理可实现攻击和潜在空间攻击。研究追踪了从早期的梯度基方法（如FGSM和PGD）到包含动量、自适应步长和高级传递机制的复杂优化技术的技术演变。本文探讨了物理可实现攻击如何通过对抗贴纸、3D纹理和动态光学扰动，成功地将数字漏洞与现实世界威胁相衔接。此外，本文还探讨了利用内部表示中的语义结构来生成更可传递和有意义的对抗样本的潜在空间攻击。除了传统的进攻性应用外，本文还研究了对抗技术在生物认证系统漏洞评估以及防恶意生成模型方面的建设性应用。本文分析揭示了关键的研究空白，特别是在神经风格迁移保护和计算效率方面的需求。本文贡献了一个全面的分类、演变分析和未来研究方向的识别，旨在增进对抗漏洞的理解，并指导更鲁棒和可信赖的计算机视觉系统的发展。', 'title_zh': '超越漏洞：计算机视觉系统中对抗性攻击作为威胁与防御的综述'}
{'arxiv_id': 'arXiv:2508.01822', 'title': 'Deep Learning-Driven Prediction of Microstructure Evolution via Latent Space Interpolation', 'authors': 'Sachin Gaikwad, Thejas Kasilingam, Owais Ahmad, Rajdip Mukherjee, Somnath Bhowmick', 'link': 'https://arxiv.org/abs/2508.01822', 'abstract': 'Phase-field models accurately simulate microstructure evolution, but their dependence on solving complex differential equations makes them computationally expensive. This work achieves a significant acceleration via a novel deep learning-based framework, utilizing a Conditional Variational Autoencoder (CVAE) coupled with Cubic Spline Interpolation and Spherical Linear Interpolation (SLERP). We demonstrate the method for binary spinodal decomposition by predicting microstructure evolution for intermediate alloy compositions from a limited set of training compositions. First, using microstructures from phase-field simulations of binary spinodal decomposition, we train the CVAE, which learns compact latent representations that encode essential morphological features. Next, we use cubic spline interpolation in the latent space to predict microstructures for any unknown composition. Finally, SLERP ensures smooth morphological evolution with time that closely resembles coarsening. The predicted microstructures exhibit high visual and statistical similarity to phase-field simulations. This framework offers a scalable and efficient surrogate model for microstructure evolution, enabling accelerated materials design and composition optimization.', 'abstract_zh': '基于深度学习的新型框架显著加速相场模型的微结构演化模拟：利用条件变分自编码器结合三次样条插值和球面线性插值实现二元相溶分解的微结构演化预测', 'title_zh': '基于潜在空间插值的深度学习驱动的微观结构演化预测'}
{'arxiv_id': 'arXiv:2508.01791', 'title': 'CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase', 'authors': 'Fatimah Mohamed Emad Elden', 'link': 'https://arxiv.org/abs/2508.01791', 'abstract': 'The field of Continuous Sign Language Recognition (CSLR) poses substantial technical challenges, including fluid inter-sign transitions, the absence of temporal boundaries, and co-articulation effects. This paper, developed for the MSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of signer-independent recognition to advance the generalization capabilities of CSLR systems across diverse signers. A data-centric methodology is proposed, centered on systematic feature engineering, a robust preprocessing pipeline, and an optimized model architecture. Key contributions include a principled feature selection process guided by Exploratory Data Analysis (EDA) to isolate communicative keypoints, a rigorous preprocessing pipeline incorporating DBSCAN-based outlier filtering and spatial normalization, and the novel CSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer design of the Conformer model, leveraging its capacity to model local temporal dependencies and global sequence context; a characteristic uniquely suited for the spatio-temporal dynamics of sign language. The proposed methodology achieved a competitive performance, with a Word Error Rate (WER) of 5.60% on the development set and 12.01% on the test set, a result that secured a 3rd place ranking on the official competition platform. This research validates the efficacy of cross-domain architectural adaptation, demonstrating that the Conformer model, originally conceived for speech recognition, can be successfully repurposed to establish a new state-of-the-art performance in keypoint-based CSLR.', 'abstract_zh': '连续手语识别领域的技术挑战及MSLR 2025 Workshop Challenge在ICCV 2025上的解决方案：基于数据驱动的方法和CSLRConformer架构', 'title_zh': 'CSLRConformer: 一种以数据为中心的阿拉伯连续手语识别方法基于Isharah数据集'}
{'arxiv_id': 'arXiv:2508.01772', 'title': 'LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation', 'authors': 'Cristian Minoccheri, Matthew Hodgman, Haoyuan Ma, Rameez Merchant, Emily Wittrup, Craig Williamson, Kayvan Najarian', 'link': 'https://arxiv.org/abs/2508.01772', 'abstract': 'Aneurysmal subarachnoid hemorrhage (SAH) is a life-threatening neurological emergency with mortality rates exceeding 30%. Transfer learning from related hematoma types represents a potentially valuable but underexplored approach. Although Unet architectures remain the gold standard for medical image segmentation due to their effectiveness on limited datasets, Low-Rank Adaptation (LoRA) methods for parameter-efficient transfer learning have been rarely applied to convolutional neural networks in medical imaging contexts. We implemented a Unet architecture pre-trained on computed tomography scans from 124 traumatic brain injury patients across multiple institutions, then fine-tuned on 30 aneurysmal SAH patients from the University of Michigan Health System using 3-fold cross-validation. We developed a novel CP-LoRA method based on tensor CP-decomposition and introduced DoRA variants (DoRA-C, convDoRA, CP-DoRA) that decompose weight matrices into magnitude and directional components. We compared these approaches against existing LoRA methods (LoRA-C, convLoRA) and standard fine-tuning strategies across different modules on a multi-view Unet model. LoRA-based methods consistently outperformed standard Unet fine-tuning. Performance varied by hemorrhage volume, with all methods showing improved accuracy for larger volumes. CP-LoRA achieved comparable performance to existing methods while using significantly fewer parameters. Over-parameterization with higher ranks consistently yielded better performance than strictly low-rank adaptations. This study demonstrates that transfer learning between hematoma types is feasible and that LoRA-based methods significantly outperform conventional Unet fine-tuning for aneurysmal SAH segmentation.', 'abstract_zh': '颅内动脉瘤性蛛网膜下腔出血的动脉瘤类型间迁移学习：基于LoRA的方法在医学图像分割中的应用', 'title_zh': '基于LoRA的方法在Subarachnoid Hematoma分割中的迁移学习应用'}
{'arxiv_id': 'arXiv:2508.01752', 'title': 'Vision transformer-based multi-camera multi-object tracking framework for dairy cow monitoring', 'authors': 'Kumail Abbas, Zeeshan Afzal, Aqeel Raza, Taha Mansouri, Andrew W. Dowsey, Chaidate Inchaisri, Ali Alameer', 'link': 'https://arxiv.org/abs/2508.01752', 'abstract': 'Activity and behaviour correlate with dairy cow health and welfare, making continual and accurate monitoring crucial for disease identification and farm productivity. Manual observation and frequent assessments are laborious and inconsistent for activity monitoring. In this study, we developed a unique multi-camera, real-time tracking system for indoor-housed Holstein Friesian dairy cows. This technology uses cutting-edge computer vision techniques, including instance segmentation and tracking algorithms to monitor cow activity seamlessly and accurately. An integrated top-down barn panorama was created by geometrically aligning six camera feeds using homographic transformations. The detection phase used a refined YOLO11-m model trained on an overhead cow dataset, obtaining high accuracy (mAP\\@0.50 = 0.97, F1 = 0.95). SAMURAI, an upgraded Segment Anything Model 2.1, generated pixel-precise cow masks for instance segmentation utilizing zero-shot learning and motion-aware memory. Even with occlusion and fluctuating posture, a motion-aware Linear Kalman filter and IoU-based data association reliably identified cows over time for object tracking. The proposed system significantly outperformed Deep SORT Realtime. Multi-Object Tracking Accuracy (MOTA) was 98.7% and 99.3% in two benchmark video sequences, with IDF1 scores above 99% and near-zero identity switches. This unified multi-camera system can track dairy cows in complex interior surroundings in real time, according to our data. The system reduces redundant detections across overlapping cameras, maintains continuity as cows move between viewpoints, with the aim of improving early sickness prediction through activity quantification and behavioural classification.', 'abstract_zh': '基于多摄像头的实时跟踪系统在室内舍荷斯坦奶牛活动与行为监测中的应用', 'title_zh': '基于视觉变换器的多相机多对象跟踪框架用于奶牛监测'}
{'arxiv_id': 'arXiv:2508.01742', 'title': 'Intention-Guided Cognitive Reasoning for Egocentric Long-Term Action Anticipation', 'authors': 'Qiaohui Chu, Haoyu Zhang, Meng Liu, Yisen Feng, Haoxiang Shi, Liqiang Nie', 'link': 'https://arxiv.org/abs/2508.01742', 'abstract': 'Long-term action anticipation from egocentric video is critical for applications such as human-computer interaction and assistive technologies, where anticipating user intent enables proactive and context-aware AI assistance. However, existing approaches suffer from three key limitations: 1) underutilization of fine-grained visual cues from hand-object interactions, 2) neglect of semantic dependencies between verbs and nouns, and 3) lack of explicit cognitive reasoning, limiting generalization and long-term forecasting ability. To overcome these challenges, we propose INSIGHT, a unified two-stage framework for egocentric action anticipation. In the first stage, INSIGHT focuses on extracting semantically rich features from hand-object interaction regions and enhances action representations using a verb-noun co-occurrence matrix. In the second stage, it introduces a reinforcement learning-based module that simulates explicit cognitive reasoning through a structured process: visual perception (think) -> intention inference (reason) -> action anticipation (answer). Extensive experiments on Ego4D, EPIC-Kitchens-55, and EGTEA Gaze+ benchmarks show that INSIGHT achieves state-of-the-art performance, demonstrating its effectiveness and strong generalization capability.', 'abstract_zh': '从第一人称视角视频中预见长期动作对于人机交互和辅助技术等应用至关重要，其中预见用户意图能够提供主动且情境感知的AI辅助。然而，现有方法存在三个关键限制：1) 手物交互的细粒度视觉线索利用不足，2) 忽视动词和名词的语义依赖关系，3) 缺乏显式的认知推理，限制了其泛化能力和长期预测能力。为克服这些挑战，我们提出INSIGHT，一种统一的两阶段框架，用于第一人称视角动作预见。在第一阶段，INSIGHT专注于从手物交互区域中提取丰富的语义特征，并使用动词-名词共现矩阵增强动作表示。在第二阶段，它引入了一个基于强化学习的模块，通过结构化的过程模拟显式的认知推理：视觉感知（思考）-> 意图推理（推理）-> 动作预见（解答）。在Ego4D、EPIC-Kitchens-55和EGTEA Gaze+基准上的 extensive 实验中，INSIGHT 达到了最先进的性能，证明了其有效性和强大的泛化能力。', 'title_zh': '基于意图的认知推理实现自我中心长期行动预测'}
{'arxiv_id': 'arXiv:2508.01713', 'title': 'Dynamic Robot-Assisted Surgery with Hierarchical Class-Incremental Semantic Segmentation', 'authors': 'Julia Hindel, Ema Mekic, Enamundram Naga Karthik, Rohit Mohan, Daniele Cattaneo, Maria Kalweit, Abhinav Valada', 'link': 'https://arxiv.org/abs/2508.01713', 'abstract': 'Robot-assisted surgeries rely on accurate and real-time scene understanding to safely guide surgical instruments. However, segmentation models trained on static datasets face key limitations when deployed in these dynamic and evolving surgical environments. Class-incremental semantic segmentation (CISS) allows models to continually adapt to new classes while avoiding catastrophic forgetting of prior knowledge, without training on previous data. In this work, we build upon the recently introduced Taxonomy-Oriented Poincaré-regularized Incremental Class Segmentation (TOPICS) approach and propose an enhanced variant, termed TOPICS+, specifically tailored for robust segmentation of surgical scenes. Concretely, we incorporate the Dice loss into the hierarchical loss formulation to handle strong class imbalances, introduce hierarchical pseudo-labeling, and design tailored label taxonomies for robotic surgery environments. We also propose six novel CISS benchmarks designed for robotic surgery environments including multiple incremental steps and several semantic categories to emulate realistic class-incremental settings in surgical environments. In addition, we introduce a refined set of labels with more than 144 classes on the Syn-Mediverse synthetic dataset, hosted online as an evaluation benchmark. We make the code and trained models publicly available at this http URL.', 'abstract_zh': '机器人辅助手术依赖于准确且实时的场景理解以安全地指导手术器械。然而，部署在静态数据集上的分割模型在这些动态且不断变化的手术环境中面临关键限制。类别增量语义分割（CISS）允许模型在不断适应新类别的同时避免对先前知识的灾难性遗忘，而不需重新训练。在本文中，我们基于最近引入的面向分类的Poincaré正则化增量类别分割（TOPICS）方法，提出了一种增强变体TOPICS+，专门为机器人手术环境中的稳健分割设计。具体而言，我们将Dice损失纳入分层级损失公式以处理类别不平衡问题，引入分层级伪标签，并设计了适合机器人手术环境的标签分类。此外，我们提出了六个新的CISS基准，专为机器人手术环境设计，包括多个增量步骤和多个语义类别，以模拟手术环境中的实际类别增量设置。我们还在Syn-Mediverse合成数据集中引入了一个改进的标签集，包含超过144个类别，并在线提供作为评估基准。我们已将代码和训练模型公开发布在指定网址。', 'title_zh': '基于分层类增量语义分割的动态机器人辅助手术'}
{'arxiv_id': 'arXiv:2508.01565', 'title': 'Deeply Supervised Multi-Task Autoencoder for Biological Brain Age estimation using three dimensional T$_1$-weighted magnetic resonance imaging', 'authors': 'Mehreen Kanwal, Yunsik Son', 'link': 'https://arxiv.org/abs/2508.01565', 'abstract': 'Accurate estimation of biological brain age from three dimensional (3D) T$_1$-weighted magnetic resonance imaging (MRI) is a critical imaging biomarker for identifying accelerated aging associated with neurodegenerative diseases. Effective brain age prediction necessitates training 3D models to leverage comprehensive insights from volumetric MRI scans, thereby fully capturing spatial anatomical context. However, optimizing deep 3D models remains challenging due to problems such as vanishing gradients. Furthermore, brain structural patterns differ significantly between sexes, which impacts aging trajectories and vulnerability to neurodegenerative diseases, thereby making sex classification crucial for enhancing the accuracy and generalizability of predictive models. To address these challenges, we propose a Deeply Supervised Multitask Autoencoder (DSMT-AE) framework for brain age estimation. DSMT-AE employs deep supervision, which involves applying supervisory signals at intermediate layers during training, to stabilize model optimization, and multitask learning to enhance feature representation. Specifically, our framework simultaneously optimizes brain age prediction alongside auxiliary tasks of sex classification and image reconstruction, thus effectively capturing anatomical and demographic variability to improve prediction accuracy. We extensively evaluate DSMT-AE on the Open Brain Health Benchmark (OpenBHB) dataset, the largest multisite neuroimaging cohort combining ten publicly available datasets. The results demonstrate that DSMT-AE achieves state-of-the-art performance and robustness across age and sex subgroups. Additionally, our ablation study confirms that each proposed component substantially contributes to the improved predictive accuracy and robustness of the overall architecture.', 'abstract_zh': '从三维（3D）T$_1$加权磁共振成像（MRI）准确估计生物脑年龄是识别与神经退行性疾病相关的加速衰老的关键影像生物标志物。有效的脑年龄预测需要训练3D模型来充分利用体部MRI扫描的全面见解，从而全面捕捉空间解剖上下文。然而，由于消失梯度等问题，优化3D深度模型仍然具有挑战性。此外，男女之间的脑结构模式差异显著，这影响了衰老轨迹和对神经退行性疾病的易感性，因此性别分类对于提高预测模型的准确性和普适性至关重要。为了解决这些挑战，我们提出了一种深度监督多任务自动编码器（DSMT-AE）框架，用于脑年龄估计。DSMT-AE利用深度监督，在训练过程中在中间层应用监督信号以稳定模型优化，并采用多任务学习提升特征表示。具体来说，我们的框架同时优化脑年龄预测和辅助任务的性别分类和图像重建，从而有效捕捉解剖和人口统计学变异性，提高预测准确性。我们通过Open Brain Health Benchmark（OpenBHB）数据集，最大的多中心神经成像队列组合了十个公开数据集，广泛评估了DSMT-AE。结果表明，DSMT-AE在不同年龄和性别子组中实现了最先进的性能和稳健性。此外，我们的消融研究证实，所提出的每个组件对整体架构的预测准确性和稳健性均有显著贡献。', 'title_zh': '基于三维T₁加权磁共振成像的深度监督多任务自动编码器生物脑年龄估计'}
{'arxiv_id': 'arXiv:2508.01396', 'title': 'Spatial-Frequency Aware for Object Detection in RAW Image', 'authors': 'Zhuohua Ye, Liming Zhang, Hongru Han', 'link': 'https://arxiv.org/abs/2508.01396', 'abstract': 'Direct RAW-based object detection offers great promise by utilizing RAW data (unprocessed sensor data), but faces inherent challenges due to its wide dynamic range and linear response, which tends to suppress crucial object details. In particular, existing enhancement methods are almost all performed in the spatial domain, making it difficult to effectively recover these suppressed details from the skewed pixel distribution of RAW images. To address this limitation, we turn to the frequency domain, where features, such as object contours and textures, can be naturally separated based on frequency. In this paper, we propose Space-Frequency Aware RAW Image Object Detection Enhancer (SFAE), a novel framework that synergizes spatial and frequency representations. Our contribution is threefold. The first lies in the ``spatialization" of frequency bands. Different from the traditional paradigm of directly manipulating abstract spectra in deep networks, our method inversely transforms individual frequency bands back into tangible spatial maps, thus preserving direct physical intuition. Then the cross-domain fusion attention module is developed to enable deep multimodal interactions between these maps and the original spatial features. Finally, the framework performs adaptive nonlinear adjustments by predicting and applying different gamma parameters for the two domains.', 'abstract_zh': '基于RAW数据的直接对象检测具有巨大潜力，但宽动态范围和线性响应导致的固有挑战可能会抑制关键对象细节。为了克服这一限制，我们转向了频域，在频域中，基于频率，对象轮廓和纹理等特征可以自然分离。在这篇论文中，我们提出了空间-频率感知RAW图像对象检测增强器（SFAE），这是一种协同利用空间和频率表示的新框架。我们的贡献包括三个方面。首先，将频率带“空间化”。不同于传统直接操作深度网络中抽象频谱的做法，我们的方法将单个频率带逆变换回具体的空间图，从而保留直接的物理直觉。然后开发了跨域融合注意力模块，以实现这些图与原始空间特征之间的深层次多模态交互。最后，框架通过预测并为两个域应用不同的伽马参数来进行自适应非线性调整。', 'title_zh': '基于空间频率的RAW图像目标检测'}
{'arxiv_id': 'arXiv:2508.01382', 'title': 'A Full-Stage Refined Proposal Algorithm for Suppressing False Positives in Two-Stage CNN-Based Detection Methods', 'authors': 'Qiang Guo, Rubo Zhang, Bingbing Zhang, Junjie Liu, Jianqing Liu', 'link': 'https://arxiv.org/abs/2508.01382', 'abstract': "False positives in pedestrian detection remain a challenge that has yet to be effectively resolved. To address this issue, this paper proposes a Full-stage Refined Proposal (FRP) algorithm aimed at eliminating these false positives within a two-stage CNN-based pedestrian detection framework. The main innovation of this work lies in employing various pedestrian feature re-evaluation strategies to filter out low-quality pedestrian proposals during both the training and testing stages. Specifically, in the training phase, the Training mode FRP algorithm (TFRP) introduces a novel approach for validating pedestrian proposals to effectively guide the model training process, thereby constructing a model with strong capabilities for false positive suppression. During the inference phase, two innovative strategies are implemented: the Classifier-guided FRP (CFRP) algorithm integrates a pedestrian classifier into the proposal generation pipeline to yield high-quality proposals through pedestrian feature evaluation, and the Split-proposal FRP (SFRP) algorithm vertically divides all proposals, sending both the original and the sub-region proposals to the subsequent subnetwork to evaluate their confidence scores, filtering out those with lower sub-region pedestrian confidence scores. As a result, the proposed algorithm enhances the model's ability to suppress pedestrian false positives across all stages. Various experiments conducted on multiple benchmarks and the SY-Metro datasets demonstrate that the model, supported by different combinations of the FRP algorithm, can effectively eliminate false positives to varying extents. Furthermore, experiments conducted on embedded platforms underscore the algorithm's effectiveness in enhancing the comprehensive pedestrian detection capabilities of the small pedestrian detector in resource-constrained edge devices.", 'abstract_zh': '虚假正例在行人检测中仍然是一个尚未有效解决的挑战。为了应对这一问题，本文提出了一种全阶段精炼提案（FRP）算法，旨在在一个基于两阶段CNN的行人检测框架中消除这些虚假正例。本文的主要创新点在于，在训练和测试阶段采用多种行人特征重评估策略，过滤掉低质量的行人提案。在训练阶段，提出了训练模式FRP算法（TFRP），引入了一种新的行人提案验证方法，有效指导模型训练过程，构建出具备较强抑制虚假正例能力的模型。在推理阶段，实施了两种创新策略：分类器导向的FRP（CFRP）算法将行人分类器集成到提案生成管道中，通过行人特征评估生成高质量提案；分割提案的FRP（SFRP）算法垂直划分所有提案，将原始提案和子区域提案同时送入后续子网络评估其置信分数，过滤置信分数较低的子区域行人提案。结果表明，所提算法能够增强模型在所有阶段抑制行人虚假正例的能力。在多个基准数据集和SY-Metro数据集上的各种实验结果表明，由不同组合的FRP算法支持的模型能够不同程度地消除虚假正例。此外，嵌入式平台上的实验进一步证实了该算法在资源受限边缘设备中增强小型行人检测器整体行人检测能力的有效性。', 'title_zh': '一种用于抑制基于两阶段CNN检测方法中假阳性的一整阶段细化提案算法'}
{'arxiv_id': 'arXiv:2508.01350', 'title': 'Classification of Brain Tumors using Hybrid Deep Learning Models', 'authors': 'Neerav Nemchand Gala', 'link': 'https://arxiv.org/abs/2508.01350', 'abstract': "The use of Convolutional Neural Networks (CNNs) has greatly improved the interpretation of medical images. However, conventional CNNs typically demand extensive computational resources and large training datasets. To address these limitations, this study applied transfer learning to achieve strong classification performance using fewer training samples. Specifically, the study compared EfficientNetV2 with its predecessor, EfficientNet, and with ResNet50 in classifying brain tumors into three types: glioma, meningioma, and pituitary tumors. Results showed that EfficientNetV2 delivered superior performance compared to the other models. However, this improvement came at the cost of increased training time, likely due to the model's greater complexity.", 'abstract_zh': '基于转移学习的EfficientNetV2在脑肿瘤分类中的应用：减少训练样本数量以提高医疗图像解释性能', 'title_zh': '使用混合深度学习模型的脑肿瘤分类'}
{'arxiv_id': 'arXiv:2508.01339', 'title': 'SBP-YOLO:A Lightweight Real-Time Model for Detecting Speed Bumps and Potholes', 'authors': 'Chuanqi Liang, Jie Fu, Lei Luo, Miao Yu', 'link': 'https://arxiv.org/abs/2508.01339', 'abstract': 'With increasing demand for ride comfort in new energy vehicles, accurate real-time detection of speed bumps and potholes is critical for predictive suspension control. This paper proposes SBP-YOLO, a lightweight detection framework based on YOLOv11, optimized for embedded deployment. The model integrates GhostConv for efficient computation, VoVGSCSPC for multi-scale feature enhancement, and a Lightweight Efficiency Detection Head (LEDH) to reduce early-stage feature processing costs. A hybrid training strategy combining NWD loss, knowledge distillation, and Albumentations-based weather augmentation improves detection robustness, especially for small and distant targets. Experiments show SBP-YOLO achieves 87.0% mAP (outperforming YOLOv11n by 5.8%) and runs at 139.5 FPS on a Jetson AGX Xavier with TensorRT FP16 quantization. The results validate its effectiveness for real-time road condition perception in intelligent suspension systems.', 'abstract_zh': '基于YOLOv11的轻量化速度 bumps 和坑洞检测框架 SBP-YOLO', 'title_zh': 'SBP-YOLO：一种实时检测减速带和坑洞的 Lightweight 模型'}
{'arxiv_id': 'arXiv:2508.01338', 'title': 'Weakly-Supervised Image Forgery Localization via Vision-Language Collaborative Reasoning Framework', 'authors': 'Ziqi Sheng, Junyan Wu, Wei Lu, Jiantao Zhou', 'link': 'https://arxiv.org/abs/2508.01338', 'abstract': 'Image forgery localization aims to precisely identify tampered regions within images, but it commonly depends on costly pixel-level annotations. To alleviate this annotation burden, weakly supervised image forgery localization (WSIFL) has emerged, yet existing methods still achieve limited localization performance as they mainly exploit intra-image consistency clues and lack external semantic guidance to compensate for weak supervision. In this paper, we propose ViLaCo, a vision-language collaborative reasoning framework that introduces auxiliary semantic supervision distilled from pre-trained vision-language models (VLMs), enabling accurate pixel-level localization using only image-level labels. Specifically, ViLaCo first incorporates semantic knowledge through a vision-language feature modeling network, which jointly extracts textual and visual priors using pre-trained VLMs. Next, an adaptive vision-language reasoning network aligns textual semantics and visual features through mutual interactions, producing semantically aligned representations. Subsequently, these representations are passed into dual prediction heads, where the coarse head performs image-level classification and the fine head generates pixel-level localization masks, thereby bridging the gap between weak supervision and fine-grained localization. Moreover, a contrastive patch consistency module is introduced to cluster tampered features while separating authentic ones, facilitating more reliable forgery discrimination. Extensive experiments on multiple public datasets demonstrate that ViLaCo substantially outperforms existing WSIFL methods, achieving state-of-the-art performance in both detection and localization accuracy.', 'abstract_zh': '基于视觉-语言协作推理的弱监督图像篡改定位', 'title_zh': '弱监督图像伪造定位 via 视觉-语言协作推理框架'}
{'arxiv_id': 'arXiv:2508.01331', 'title': 'Referring Remote Sensing Image Segmentation with Cross-view Semantics Interaction Network', 'authors': 'Jiaxing Yang, Lihe Zhang, Huchuan Lu', 'link': 'https://arxiv.org/abs/2508.01331', 'abstract': 'Recently, Referring Remote Sensing Image Segmentation (RRSIS) has aroused wide attention. To handle drastic scale variation of remote targets, existing methods only use the full image as input and nest the saliency-preferring techniques of cross-scale information interaction into traditional single-view structure. Although effective for visually salient targets, they still struggle in handling tiny, ambiguous ones in lots of real scenarios. In this work, we instead propose a paralleled yet unified segmentation framework Cross-view Semantics Interaction Network (CSINet) to solve the limitations. Motivated by human behavior in observing targets of interest, the network orchestrates visual cues from remote and close distances to conduct synergistic prediction. In its every encoding stage, a Cross-View Window-attention module (CVWin) is utilized to supplement global and local semantics into close-view and remote-view branch features, finally promoting the unified representation of feature in every encoding stage. In addition, we develop a Collaboratively Dilated Attention enhanced Decoder (CDAD) to mine the orientation property of target and meanwhile integrate cross-view multiscale features. The proposed network seamlessly enhances the exploitation of global and local semantics, achieving significant improvements over others while maintaining satisfactory speed.', 'abstract_zh': '近年来，参考遥感图像分割（RRSIS）引起了广泛关注。为了解决遥感目标的剧烈尺度变化问题，现有方法仅使用整幅图像作为输入，并将跨尺度信息交互的显著性偏好技术嵌入到传统的单视角结构中。尽管在视觉显著目标上有效，但在许多实际场景中小而模糊的目标仍然难以处理。在本工作中，我们提出了一种并行且统一的分割框架交叉视角语义交互网络（CSINet）以解决上述限制。受人类观察感兴趣目标行为的启发，该网络协调远距离和近距离的视觉线索进行协同预测。在每一编码阶段，利用跨视角窗口注意力模块（CVWin）补充全局和局部语义到近距离视角和远距离视角分支特征中，最终促进每一编码阶段特征的统一表示。此外，我们开发了一种协作膨胀注意力增强解码器（CDAD），以挖掘目标的方向特性并同时整合多尺度跨视角特征。所提出的网络无损地增强了对全局和局部语义的利用，实现了显著的性能提升，同时保持了满意的运行速度。', 'title_zh': '基于跨视图语义交互网络的遥感图像分割参考模型'}
{'arxiv_id': 'arXiv:2508.01206', 'title': 'Deep Learning for Pavement Condition Evaluation Using Satellite Imagery', 'authors': 'Prathyush Kumar Reddy Lebaku, Lu Gao, Pan Lu, Jingran Sun', 'link': 'https://arxiv.org/abs/2508.01206', 'abstract': "Civil infrastructure systems covers large land areas and needs frequent inspections to maintain their public service capabilities. The conventional approaches of manual surveys or vehicle-based automated surveys to assess infrastructure conditions are often labor-intensive and time-consuming. For this reason, it is worthwhile to explore more cost-effective methods for monitoring and maintaining these infrastructures. Fortunately, recent advancements in satellite systems and image processing algorithms have opened up new possibilities. Numerous satellite systems have been employed to monitor infrastructure conditions and identify damages. Due to the improvement in ground sample distance (GSD), the level of detail that can be captured has significantly increased. Taking advantage of these technology advancement, this research investigated to evaluate pavement conditions using deep learning models for analyzing satellite images. We gathered over 3,000 satellite images of pavement sections, together with pavement evaluation ratings from TxDOT's PMIS database. The results of our study show an accuracy rate is exceeding 90%. This research paves the way for a rapid and cost-effective approach to evaluating the pavement network in the future.", 'abstract_zh': '基于卫星影像的深度学习路面状况评估方法', 'title_zh': '使用卫星图像的 pavement 条件评估的深度学习方法'}
{'arxiv_id': 'arXiv:2508.01205', 'title': 'Conquering High Packet-Loss Erasure: MoE Swin Transformer-Based Video Semantic Communication', 'authors': 'Lei Teng, Senran Fan, Chen Dong, Haotai Liang, Zhicheng Bao, Xiaodong Xu, Rui Meng, Ping Zhang', 'link': 'https://arxiv.org/abs/2508.01205', 'abstract': 'Semantic communication with joint semantic-channel coding robustly transmits diverse data modalities but faces challenges in mitigating semantic information loss due to packet drops in packet-based systems. Under current protocols, packets with errors are discarded, preventing the receiver from utilizing erroneous semantic data for robust decoding. To address this issue, a packet-loss-resistant MoE Swin Transformer-based Video Semantic Communication (MSTVSC) system is proposed in this paper. Semantic vectors are encoded by MSTVSC and transmitted through upper-layer protocol packetization. To investigate the impact of the packetization, a theoretical analysis of the packetization strategy is provided. To mitigate the semantic loss caused by packet loss, a 3D CNN at the receiver recovers missing information using un-lost semantic data and an packet-loss mask matrix. Semantic-level interleaving is employed to reduce concentrated semantic loss from packet drops. To improve compression, a common-individual decomposition approach is adopted, with downsampling applied to individual information to minimize redundancy. The model is lightweighted for practical deployment. Extensive simulations and comparisons demonstrate strong performance, achieving an MS-SSIM greater than 0.6 and a PSNR exceeding 20 dB at a 90% packet loss rate.', 'abstract_zh': '基于MoE Swin Transformer的抗丢包视频语义通信系统（MSTVSC）', 'title_zh': '基于MoE SwinTransformer的视频语义通信： conquering 高丢包率擦除'}
{'arxiv_id': 'arXiv:2508.00921', 'title': 'SmartDate: AI-Driven Precision Sorting and Quality Control in Date Fruits', 'authors': 'Khaled Eskaf', 'link': 'https://arxiv.org/abs/2508.00921', 'abstract': 'SmartDate is an AI-powered system for automated sorting and quality control of date fruits. It combines deep learning, genetic algorithms, and reinforcement learning to improve classification accuracy and predict shelf life. The system uses high-resolution imaging and Visible-Near-Infrared (VisNIR) spectral sensors to evaluate key features such as moisture, sugar content, and texture. Reinforcement learning enables real-time adaptation to production conditions, while genetic algorithms optimize model parameters. SmartDate achieved 94.5 percent accuracy, 93.1 percent F1-score, and an AUC-ROC of 0.96. The system reduces waste and ensures that only high-quality dates reach the market, setting a new benchmark in smart agriculture.', 'abstract_zh': '基于AI的SmartDate系统实现了枣果的自动化分拣与质量控制', 'title_zh': 'SmartDate：基于人工智能的精确分选与质量控制在-date果实中的应用'}
{'arxiv_id': 'arXiv:2508.00898', 'title': 'Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models', 'authors': 'Jose M. Sánchez Velázquez, Mingbo Cai, Andrew Coney, Álvaro J. García- Tejedor, Alberto Nogales', 'link': 'https://arxiv.org/abs/2508.00898', 'abstract': 'In recent years, advances in Artificial Intelligence have significantly impacted computer science, particularly in the field of computer vision, enabling solutions to complex problems such as video frame prediction. Video frame prediction has critical applications in weather forecasting or autonomous systems and can provide technical improvements, such as video compression and streaming. Among Artificial Intelligence methods, Deep Learning has emerged as highly effective for solving vision-related tasks, although current frame prediction models still have room for enhancement. This paper evaluates several hybrid deep learning approaches that combine the feature extraction capabilities of autoencoders with temporal sequence modelling using Recurrent Neural Networks (RNNs), 3D Convolutional Neural Networks (3D CNNs), and related architectures. The proposed solutions were rigorously evaluated on three datasets that differ in terms of synthetic versus real-world scenarios and grayscale versus color imagery. Results demonstrate that the approaches perform well, with SSIM metrics increasing from 0.69 to 0.82, indicating that hybrid models utilizing 3DCNNs and ConvLSTMs are the most effective, and greyscale videos with real data are the easiest to predict.', 'abstract_zh': '近年来，人工智能的进步在计算机科学领域，特别是在计算机视觉领域产生了显著影响，使预测视频帧成为可能。视频帧预测在天气预报或自主系统中具有关键应用，并可提供技术改进，如视频压缩和流媒体。在人工智能方法中，深度学习因其在视觉任务方面的有效性而脱颖而出，尽管当前的帧预测模型仍有改进空间。本文评估了几种混合深度学习方法，这些方法结合了自动编码器的特征提取能力以及使用循环神经网络（RNNs）、3D 卷积神经网络（3D CNNs）及相关架构的时序序列建模能力。所提出的解决方案在三个不同类型的数据库上进行了严格评估，这些数据库在合成与真实场景以及灰度与彩色图像方面有所不同。结果表明，这些方法表现良好，SSIM指标从0.69提高到0.82，指出采用3DCNN和ConvLSTM的混合模型最为有效，而灰度视频在真实数据情况下最容易预测。', 'title_zh': '特征提取和时间序列分析在视频帧预测中的益处：混合深度学习模型的评估'}
{'arxiv_id': 'arXiv:2506.03662', 'title': 'Zero-Shot Temporal Interaction Localization for Egocentric Videos', 'authors': 'Erhang Zhang, Junyi Ma, Yin-Dong Zheng, Yixuan Zhou, Hesheng Wang', 'link': 'https://arxiv.org/abs/2506.03662', 'abstract': 'Locating human-object interaction (HOI) actions within video serves as the foundation for multiple downstream tasks, such as human behavior analysis and human-robot skill transfer. Current temporal action localization methods typically rely on annotated action and object categories of interactions for optimization, which leads to domain bias and low deployment efficiency. Although some recent works have achieved zero-shot temporal action localization (ZS-TAL) with large vision-language models (VLMs), their coarse-grained estimations and open-loop pipelines hinder further performance improvements for temporal interaction localization (TIL). To address these issues, we propose a novel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp actions for human-object interaction in egocentric videos. EgoLoc introduces a self-adaptive sampling strategy to generate reasonable visual prompts for VLM reasoning. By absorbing both 2D and 3D observations, it directly samples high-quality initial guesses around the possible contact/separation timestamps of HOI according to 3D hand velocities, leading to high inference accuracy and efficiency. In addition, EgoLoc generates closed-loop feedback from visual and dynamic cues to further refine the localization results. Comprehensive experiments on the publicly available dataset and our newly proposed benchmark demonstrate that EgoLoc achieves better temporal interaction localization for egocentric videos compared to state-of-the-art baselines. We will release our code and relevant data as open-source at this https URL.', 'abstract_zh': '基于视频的人机交互动作定位作为多个下游任务的基础，如人类行为分析和人机技能转移。当前的时间动作定位方法通常依赖于交互中的标注动作和对象类别进行优化，这导致了领域偏差和低部署效率。虽然一些近期的工作利用大规模的视觉-语言模型实现了零样本时间动作定位（ZS-TAL），但它们粗粒度的估计和开环的管道阻碍了时间交互定位（TIL）性能的进一步提升。为了解决这些问题，我们提出了一种新颖的零样本TIL方法，名为EgoLoc，以在第一人称视频中定位人类对象交互中的抓取动作的时间。EgoLoc引入了一种自适应采样策略，以生成合理的视觉提示供视觉-语言模型推理。通过吸收二维和三维观察结果，它根据三维手部速度直接在HOI可能的接触/分离时间戳周围采样高质量的初始猜测，从而提高了推理准确性和效率。此外，EgoLoc从视觉和动态线索中生成闭环反馈进一步细化定位结果。在公开的数据集和我们新提出的基准上的全面实验表明，EgoLoc在第一人称视频的时间交互定位方面优于最先进的基线方法。我们将在此网址公开我们的代码和相关数据：这个网址。', 'title_zh': '零样本时空交互定位 for 同伴中心视频'}
