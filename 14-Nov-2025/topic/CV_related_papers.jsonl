{'arxiv_id': 'arXiv:2511.10411', 'title': 'LongComp: Long-Tail Compositional Zero-Shot Generalization for Robust Trajectory Prediction', 'authors': 'Benjamin Stoler, Jonathan Francis, Jean Oh', 'link': 'https://arxiv.org/abs/2511.10411', 'abstract': 'Methods for trajectory prediction in Autonomous Driving must contend with rare, safety-critical scenarios that make reliance on real-world data collection alone infeasible. To assess robustness under such conditions, we propose new long-tail evaluation settings that repartition datasets to create challenging out-of-distribution (OOD) test sets. We first introduce a safety-informed scenario factorization framework, which disentangles scenarios into discrete ego and social contexts. Building on analogies to compositional zero-shot image-labeling in Computer Vision, we then hold out novel context combinations to construct challenging closed-world and open-world settings. This process induces OOD performance gaps in future motion prediction of 5.0% and 14.7% in closed-world and open-world settings, respectively, relative to in-distribution performance for a state-of-the-art baseline. To improve generalization, we extend task-modular gating networks to operate within trajectory prediction models, and develop an auxiliary, difficulty-prediction head to refine internal representations. Our strategies jointly reduce the OOD performance gaps to 2.8% and 11.5% in the two settings, respectively, while still improving in-distribution performance.', 'abstract_zh': '自主驾驶中轨迹预测方法必须应对罕见的安全关键场景，这使得依赖真实的实地数据收集不可行。为了在这些条件下评估鲁棒性，我们提出了新的长尾评估设置，重新分配数据集以创建具有挑战性的离分布测试集。首先，我们引入了一种基于安全的信息场景分解框架，将场景分解为离散的自我和社交背景。在此基础上，借鉴计算机视觉中组合零样本图像标记的类比，我们保留新的上下文组合来构建具有挑战性的闭世界和开世界设置。这一过程在闭世界和开世界设置中分别引起了5.0%和14.7%的离分布性能差距相对于内部分布性能，对于一个最先进的基线。为了提高泛化能力，我们扩展了任务模块门控网络在轨迹预测模型中的操作，并开发了一个辅助的难度预测头部来细化内部表示。我们的策略分别将两种设置中的离分布性能差距减少到2.8%和11.5%，同时仍然提高内部分布性能。', 'title_zh': '长尾组合零-shot泛化：面向稳健轨迹预测'}
{'arxiv_id': 'arXiv:2511.10203', 'title': 'VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction', 'authors': 'Stephane Da Silva Martins, Emanuel Aldea, Sylvie Le Hégarat-Mascle', 'link': 'https://arxiv.org/abs/2511.10203', 'abstract': "Multi-agent trajectory prediction is crucial for autonomous systems operating in dense, interactive environments. Existing methods often fail to jointly capture agents' long-term goals and their fine-grained social interactions, which leads to unrealistic multi-agent futures. We propose VISTA, a recursive goal-conditioned transformer for multi-agent trajectory forecasting. VISTA combines (i) a cross-attention fusion module that integrates long-horizon intent with past motion, (ii) a social-token attention mechanism for flexible interaction modeling across agents, and (iii) pairwise attention maps that make social influence patterns interpretable at inference time. Our model turns single-agent goal-conditioned prediction into a coherent multi-agent forecasting framework. Beyond standard displacement metrics, we evaluate trajectory collision rates as a measure of joint realism. On the high-density MADRAS benchmark and on SDD, VISTA achieves state-of-the-art accuracy and substantially fewer collisions. On MADRAS, it reduces the average collision rate of strong baselines from 2.14 to 0.03 percent, and on SDD it attains zero collisions while improving ADE, FDE, and minFDE. These results show that VISTA generates socially compliant, goal-aware, and interpretable trajectories, making it promising for safety-critical autonomous systems.", 'abstract_zh': '多agent轨迹预测对于操作在密集交互环境中的自主系统至关重要。现有的方法通常无法同时捕捉到agent的长期目标及其细微的社会互动，导致生成不现实的多agent未来轨迹。我们提出VISTA，一种递归目标条件变换器，用于多agent轨迹预测。VISTA结合了(i)一种跨注意力融合模块，将长期意图与过往运动整合，(ii)一种灵活的社交token注意力机制，用于跨agent的社会互动建模，以及(iii)成对注意力图，使社会影响模式在推理时具有可解释性。我们的模型将单agent目标条件预测转变为一致的多agent预测框架。除了标准的位移度量，我们还将轨迹碰撞频率作为联合现实性的衡量标准进行评估。在高密度MADRAS基准和SDD上，VISTA实现了最先进的准确性和显著减少的碰撞。在MADRAS上，它将强基线的平均碰撞率从2.14%降低到0.03%，而在SDD上，它实现了零碰撞同时提高了ADE、FDE和minFDE。这些结果表明VISTA生成了社会合规、目标意识和可解释的轨迹，使其对于安全关键的自主系统具有前景。', 'title_zh': 'VISTA：一种考虑视觉和意图的社会注意力多-agent 轨迹预测框架'}
{'arxiv_id': 'arXiv:2511.09724', 'title': 'PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model', 'authors': 'Yunqian Cheng, Benjamin Princen, Roberto Manduchi', 'link': 'https://arxiv.org/abs/2511.09724', 'abstract': 'Indoor localization in GPS-denied environments is crucial for applications like emergency response and assistive navigation. Vision-based methods such as PALMS enable infrastructure-free localization using only a floor plan and a stationary scan, but are limited by the short range of smartphone LiDAR and ambiguity in indoor layouts. We propose PALMS$+$, a modular, image-based system that addresses these challenges by reconstructing scale-aligned 3D point clouds from posed RGB images using a foundation monocular depth estimation model (Depth Pro), followed by geometric layout matching via convolution with the floor plan. PALMS$+$ outputs a posterior over the location and orientation, usable for direct or sequential localization. Evaluated on the Structured3D and a custom campus dataset consisting of 80 observations across four large campus buildings, PALMS$+$ outperforms PALMS and F3Loc in stationary localization accuracy -- without requiring any training. Furthermore, when integrated with a particle filter for sequential localization on 33 real-world trajectories, PALMS$+$ achieved lower localization errors compared to other methods, demonstrating robustness for camera-free tracking and its potential for infrastructure-free applications. Code and data are available at this https URL', 'abstract_zh': '在GPS受限环境下的室内定位对于应急响应和辅助导航等应用至关重要。基于视觉的方法如PALMS能够在仅有楼层平面图和静态扫描的情况下实现基础设施免费定位，但受限于智能手机LiDAR的短距离和室内布局的不确定性。我们提出了PALMS$+$，一种模块化的图像基于系统，通过使用基础单目深度估计模型（Depth Pro）重构尺度对齐的3D点云来应对这些挑战，随后通过与楼层平面图卷积进行几何布局匹配。PALMS$+$输出位置和方向的后验概率，适用于直接或序列定位。通过对Structured3D和一个包含80个观测值的自定义校园数据集（涵盖四个大型校园建筑）进行评估，PALMS$+$在静态定位准确性方面优于PALMS和F3Loc，无需任何训练。此外，通过粒子滤波器在33条真实轨迹上的序列定位，PALMS$+$实现了较低的定位误差，体现了其在无摄像头跟踪和基础设施免费应用方面的鲁棒性和潜力。代码和数据可在以下链接获取。', 'title_zh': 'PALMS+: 基于图像的楼层平面定位的模块化方法利用深度foundation模型'}
{'arxiv_id': 'arXiv:2511.10555', 'title': 'A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space', 'authors': 'Huijie Liu, Shuhao Cui, Haoxiang Cao, Shuai Ma, Kai Wu, Guoliang Kang', 'link': 'https://arxiv.org/abs/2511.10555', 'abstract': 'Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.', 'abstract_zh': '创新的视觉风格化是艺术创作的基石，但生成新颖且一致的视觉风格仍旧是一项重大挑战。现有的生成方法通常依赖于冗长的文本提示、参考图像或参数高效的微调来指导风格感知图像生成，但在风格一致性、创造力有限和复杂风格表示方面经常遇到困难。在本文中，我们通过引入新的任务——代码到风格的图像生成，证明了一种风格仅需一个数值代码。截至目前，这一领域主要由行业（如Midjourney）探索，学术界尚未有开源研究。为填补这一空白，我们提出了CoTyle，这是首个开源的方法。具体而言，我们首先从一组图像中训练一个离散风格代码本以提取风格嵌入，这些嵌入作为条件，指导文本到图像扩散模型(T2I-DM)生成带风格的图像。随后，我们基于离散风格嵌入训练一个自回归风格生成器，以建模其分布，从而合成新的风格嵌入。在推理过程中，风格生成器将一个数值风格代码映射到一个独特的风格嵌入，该嵌入引导T2I-DM生成相应风格的图像。与现有方法不同，我们的方法提供了无与伦比的简洁性和多样性，仅从少量输入即可解锁大量可重现的风格空间。大量实验验证了CoTyle能够有效地将一个数值代码转化为风格控制器，证明了一个风格仅需一个代码。', 'title_zh': '一种风格值千行代码：通过离散风格空间解锁代码到风格图像生成'}
{'arxiv_id': 'arXiv:2511.10390', 'title': 'MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns', 'authors': 'Jiarui Zhang, Yuliang Liu, Zijun Wu, Guosheng Pang, Zhili Ye, Yupei Zhong, Junteng Ma, Tao Wei, Haiyang Xu, Weikai Chen, Zeen Wang, Qiangjun Ji, Fanxi Zhou, Qi Zhang, Yuanrui Hu, Jiahao Liu, Zhang Li, Ziyang Zhang, Qiang Liu, Xiang Bai', 'link': 'https://arxiv.org/abs/2511.10390', 'abstract': 'Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.', 'abstract_zh': '文档解析是文档智能的核心任务，支持信息抽取、检索增强生成和自动化文档分析等应用。然而，现实世界的文档往往具有复杂的布局，包括多级表格、嵌入的图像或公式以及跨页结构，这些对现有的OCR系统仍具有挑战性。我们引入了MonkeyOCR v1.5，这是一种统一的视觉-语言框架，通过两阶段解析管道增强布局理解和内容识别。第一阶段采用大规模多模态模型联合预测文档布局和阅读顺序，并利用视觉信息确保结构和顺序的一致性。第二阶段在检测区域中局部识别文本、公式和表格，保持高视觉保真度同时减少错误传播。为了解决复杂的表格结构，我们提出了一种基于视觉一致性强化学习方案，通过渲染和比较对齐来评估识别质量，从而提高结构准确性，而无需人工注释。此外，我们还引入了两张专门模块：图像解耦表格解析和类型引导表格合并，以实现包含嵌入图像的表格的可靠解析以及跨页面或列的表格重建。在OmniDocBench v1.5上的全面实验表明，MonkeyOCR v1.5实现了最先进的性能，优于PPOCR-VL和MinerU 2.5，并在视觉复杂的文档场景中表现出色。', 'title_zh': 'MonkeyOCR v1.5技术报告：解锁对复杂模式的鲁棒文档解析'}
{'arxiv_id': 'arXiv:2511.10316', 'title': 'Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision', 'authors': 'Yu Deng, Baozhu Zhao, Junyan Su, Xiaohan Zhang, Qi Liu', 'link': 'https://arxiv.org/abs/2511.10316', 'abstract': 'Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting. Our approach comprises two core components: (1) Depth-of-field Supervision employs a scale-recovered monocular depth estimator (e.g., Metric3D) to generate depth priors, leverages defocus convolution to synthesize physically accurate defocused images, and enforces geometric consistency through a novel depth-of-field loss, thereby enhancing depth fidelity in both far-field and near-field regions; (2) Multi-View Consistency Supervision employing LoFTR-based semi-dense feature matching to minimize cross-view geometric errors and enforce depth consistency via least squares optimization of reliable matched points. By unifying defocus physics with multi-view geometric constraints, our method achieves superior depth fidelity, demonstrating a 0.8 dB PSNR improvement over the state-of-the-art method on the Waymo Open Dataset. This framework bridges physical imaging principles and learning-based depth regularization, offering a scalable solution for complex depth stratification in urban environments.', 'abstract_zh': '三维深度变化极端场景下的重建依然具有挑战性，因为近场和远场区域之间的监督信号不一致。现有方法无法同时解决远区不准确的深度估计和近区结构退化的问题。本文提出了一种新颖的计算框架，结合焦深监督和多视图一致性监督以推进3D高斯点云化。该方法包含两个核心组件：（1）焦深监督利用尺度恢复单目深度估计器（例如，Metric3D）生成深度先验，通过离焦卷积合成物理上准确的离焦图像，并通过一种新颖的焦深损失增强远场和近场区域的深度保真度；（2）多视图一致性监督采用基于LoFTR的半密集特征匹配来最小化跨视图几何错误，并通过最小二乘优化可靠匹配点以确保深度一致性。通过将离焦物理现象与多视图几何约束统一起来，该方法实现了优越的深度保真度，在Waymo Open Dataset上相比最新方法显示出0.8 dB的PSNR提升。该框架将物理成像原理与基于学习的深度正则化相结合，为城市环境中的复杂深度分层提供可扩展的解决方案。', 'title_zh': '基于物理景深建模和多视几何监督的深度一致性3D高斯点云表示'}
{'arxiv_id': 'arXiv:2511.10260', 'title': 'H3Former: Hypergraph-based Semantic-Aware Aggregation via Hyperbolic Hierarchical Contrastive Loss for Fine-Grained Visual Classification', 'authors': 'Yongji Zhang, Siqi Li, Kuiyang Huang, Yue Gao, Yu Jiang', 'link': 'https://arxiv.org/abs/2511.10260', 'abstract': 'Fine-Grained Visual Classification (FGVC) remains a challenging task due to subtle inter-class differences and large intra-class variations. Existing approaches typically rely on feature-selection mechanisms or region-proposal strategies to localize discriminative regions for semantic analysis. However, these methods often fail to capture discriminative cues comprehensively while introducing substantial category-agnostic redundancy. To address these limitations, we propose H3Former, a novel token-to-region framework that leverages high-order semantic relations to aggregate local fine-grained representations with structured region-level modeling. Specifically, we propose the Semantic-Aware Aggregation Module (SAAM), which exploits multi-scale contextual cues to dynamically construct a weighted hypergraph among tokens. By applying hypergraph convolution, SAAM captures high-order semantic dependencies and progressively aggregates token features into compact region-level representations. Furthermore, we introduce the Hyperbolic Hierarchical Contrastive Loss (HHCL), which enforces hierarchical semantic constraints in a non-Euclidean embedding space. The HHCL enhances inter-class separability and intra-class consistency while preserving the intrinsic hierarchical relationships among fine-grained categories. Comprehensive experiments conducted on four standard FGVC benchmarks validate the superiority of our H3Former framework.', 'abstract_zh': '高阶语义关系驱动的细粒度视觉分类方法（H3Former）仍然是一项具有挑战性的任务，由于类间差异的微妙性和类内变异性大。现有方法通常依赖于特征选择机制或区域提议策略来定位语义分析中的 discriminative 区域。然而，这些方法往往在全面捕获 discriminative 提示的同时引入了大量的类别无关冗余。为解决这些局限性，我们提出了一种新颖的 token-to-region 框架 H3Former，该框架利用高阶语义关系来通过结构化的区域级建模聚合局部细粒度表示。具体地，我们提出了语义感知聚合模块（SAAM），该模块利用多尺度上下文线索动态构建 token 之间的加权超图。通过应用超图卷积，SAAM 捕捉高级语义依赖关系并逐步将 token 特征聚合为紧凑的区域级表示。此外，我们引入了超球面层次对比损失（HHCL），该损失在非欧几里得嵌入空间中强制执行层次语义约束。HHCL 提高了类间可分性和类内一致性，同时保留了细粒度类别之间的内在层次关系。在四个标准细粒度视觉分类基准上的全面实验验证了我们 H3Former 框架的优势。', 'title_zh': 'H3Former：基于超图的语义-aware聚合及双曲层次对比损失细粒度视觉分类'}
{'arxiv_id': 'arXiv:2511.10136', 'title': 'Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation', 'authors': 'Mayank Vatsa, Aparna Bharati, Richa Singh', 'link': 'https://arxiv.org/abs/2511.10136', 'abstract': "The architectural blueprint of today's leading text-to-image models contains a fundamental flaw: an inability to handle logical composition. This survey investigates this breakdown across three core primitives-negation, counting, and spatial relations. Our analysis reveals a dramatic performance collapse: models that are accurate on single primitives fail precipitously when these are combined, exposing severe interference. We trace this failure to three key factors. First, training data show a near-total absence of explicit negations. Second, continuous attention architectures are fundamentally unsuitable for discrete logic. Third, evaluation metrics reward visual plausibility over constraint satisfaction. By analyzing recent benchmarks and methods, we show that current solutions and simple scaling cannot bridge this gap. Achieving genuine compositionality, we conclude, will require fundamental advances in representation and reasoning rather than incremental adjustments to existing architectures.", 'abstract_zh': '当今领先文本到图像模型的建筑蓝图中存在一个根本缺陷：无法处理逻辑组合。本文综述了这一缺陷在否定、计数和空间关系这三个核心原语中的表现。我们的分析揭示了一种戏剧性的性能崩溃：在单一原语上准确的模型在这些原语组合时表现急剧下降，暴露出严重的相互干扰。我们将其失败归因于三个关键因素。首先，训练数据中几乎完全缺乏显式的否定。第二，连续注意力架构从根本上不适合离散逻辑。第三，评估指标更重视视觉合理性而非约束满足。通过对最近基准和方法的分析，我们表明当前的解决方案和简单的扩展无法弥补这一差距。我们得出结论，实现真正的组合性将需要在表示和推理方面的根本性进步，而不仅仅是对现有架构进行渐进调整。', 'title_zh': '正确的外观，错误的理由：文本到图像生成中的组合保真度'}
{'arxiv_id': 'arXiv:2511.10089', 'title': 'T2IBias: Uncovering Societal Bias Encoded in the Latent Space of Text-to-Image Generative Models', 'authors': 'Abu Sufian, Cosimo Distante, Marco Leo, Hanan Salam', 'link': 'https://arxiv.org/abs/2511.10089', 'abstract': "Text-to-image (T2I) generative models are largely used in AI-powered real-world applications and value creation. However, their strategic deployment raises critical concerns for responsible AI management, particularly regarding the reproduction and amplification of race- and gender-related stereotypes that can undermine organizational ethics. In this work, we investigate whether such societal biases are systematically encoded within the pretrained latent spaces of state-of-the-art T2I models. We conduct an empirical study across the five most popular open-source models, using ten neutral, profession-related prompts to generate 100 images per profession, resulting in a dataset of 5,000 images evaluated by diverse human assessors representing different races and genders. We demonstrate that all five models encode and amplify pronounced societal skew: caregiving and nursing roles are consistently feminized, while high-status professions such as corporate CEO, politician, doctor, and lawyer are overwhelmingly represented by males and mostly White individuals. We further identify model-specific patterns, such as QWEN-Image's near-exclusive focus on East Asian outputs, Kandinsky's dominance of White individuals, and SDXL's comparatively broader but still biased distributions. These results provide critical insights for AI project managers and practitioners, enabling them to select equitable AI models and customized prompts that generate images in alignment with the principles of responsible AI. We conclude by discussing the risks of these biases and proposing actionable strategies for bias mitigation in building responsible GenAI systems.", 'abstract_zh': '基于文本生成图像（T2I）生成模型在AI赋能的实际应用与价值创造中广泛使用。然而，它们的战略部署引发了负责任AI管理中的关键性关注，特别是在再现和放大与种族和性别相关的刻板印象方面，这些刻板印象可能损害组织伦理。本研究调查了这些社会偏见是否系统地编码在先进T2I模型的预训练潜在空间中。我们通过使用十种中性、与职业相关的提示，在五种最流行的开源模型中生成每种职业的100张图像，形成了由不同种族和性别的人类评估者组成的5,000张图像的数据集，以实证研究这些社会偏见。结果显示，所有五个模型都系统地编码并放大了显著的社会偏见：护理和护理角色被一致性地性别化为女性，而高地位的职业如公司CEO、政治家、医生和律师则主要由男性和白人占据。我们进一步识别出模型特有的模式，例如QWEN-Image几乎独家侧重于东亚输出，Kandinsky主要展现白人，以及SDXL相对更为广泛但仍然具有偏见的分布。这些结果为AI项目管理者和从业者提供了宝贵的见解，帮助他们选择公平的AI模型和定制提示，生成符合负责任AI原则的图像。我们最后讨论这些偏见的风险，并提出构建负责任的生成人工智能系统的可操作策略。', 'title_zh': 'T2I偏差：揭示文本到图像生成模型潜空间中的社会偏见'}
{'arxiv_id': 'arXiv:2511.10023', 'title': 'Efficient Automated Diagnosis of Retinopathy of Prematurity by Customize CNN Models', 'authors': 'Farzan Saeedi, Sanaz Keshvari, Nasser Shoeibi', 'link': 'https://arxiv.org/abs/2511.10023', 'abstract': 'This paper encompasses an in-depth examination of Retinopathy of Prematurity (ROP) diagnosis, employing advanced deep learning methodologies. Our focus centers on refining and evaluating CNN-based approaches for precise and efficient ROP detection. We navigate the complexities of dataset curation, preprocessing strategies, and model architecture, aligning with research objectives encompassing model effectiveness, computational cost analysis, and time complexity assessment. Results underscore the supremacy of tailored CNN models over pre-trained counterparts, evident in heightened accuracy and F1-scores. Implementation of a voting system further enhances performance. Additionally, our study reveals the potential of the proposed customized CNN model to alleviate computational burdens associated with deep neural networks. Furthermore, we showcase the feasibility of deploying these models within dedicated software and hardware configurations, highlighting their utility as valuable diagnostic aids in clinical settings. In summary, our discourse significantly contributes to ROP diagnosis, unveiling the efficacy of deep learning models in enhancing diagnostic precision and efficiency.', 'abstract_zh': '这篇论文对早产儿视网膜病变（ROP）的诊断进行了深入研究，采用先进的深度学习方法。我们的重点在于精炼和评估基于CNN的方法，以实现精确和高效的ROP检测。我们探讨了数据集的构建、预处理策略和模型架构的复杂性，旨在实现模型效果、计算成本分析以及时间复杂性评估的研究目标。结果显示，定制的CNN模型在准确性和F1分数方面优于预训练模型。通过实施投票系统进一步提升了性能。此外，我们的研究揭示了所提出的定制化CNN模型在减轻深度神经网络计算负担方面的潜力。同时，我们展示了在专用软件和硬件配置中部署这些模型的可能性，突显了它们在临床环境中的诊断辅助价值。总之，本文在ROP诊断方面做出了重要贡献，揭示了深度学习模型在提高诊断精确性和效率方面的有效性。', 'title_zh': '早产儿视网膜病变的定制CNN模型高效自动诊断'}
{'arxiv_id': 'arXiv:2511.10013', 'title': 'MIRNet: Integrating Constrained Graph-Based Reasoning with Pre-training for Diagnostic Medical Imaging', 'authors': 'Shufeng Kong, Zijie Wang, Nuan Cui, Hao Tang, Yihan Meng, Yuanyuan Wei, Feifan Chen, Yingheng Wang, Zhuo Cai, Yaonan Wang, Yulong Zhang, Yuzheng Li, Zibin Zheng, Caihua Liu', 'link': 'https://arxiv.org/abs/2511.10013', 'abstract': 'Automated interpretation of medical images demands robust modeling of complex visual-semantic relationships while addressing annotation scarcity, label imbalance, and clinical plausibility constraints. We introduce MIRNet (Medical Image Reasoner Network), a novel framework that integrates self-supervised pre-training with constrained graph-based reasoning. Tongue image diagnosis is a particularly challenging domain that requires fine-grained visual and semantic understanding. Our approach leverages self-supervised masked autoencoder (MAE) to learn transferable visual representations from unlabeled data; employs graph attention networks (GAT) to model label correlations through expert-defined structured graphs; enforces clinical priors via constraint-aware optimization using KL divergence and regularization losses; and mitigates imbalance using asymmetric loss (ASL) and boosting ensembles. To address annotation scarcity, we also introduce TongueAtlas-4K, a comprehensive expert-curated benchmark comprising 4,000 images annotated with 22 diagnostic labels--representing the largest public dataset in tongue analysis. Validation shows our method achieves state-of-the-art performance. While optimized for tongue diagnosis, the framework readily generalizes to broader diagnostic medical imaging tasks.', 'abstract_zh': '医学图像的自动化解释需要建立稳健的复杂视觉-语义关系模型，同时解决标注稀缺性、标签不平衡以及临床合理性约束的问题。我们引入了MIRNet（Medical Image Reasoner Network），这是一种将自我监督预训练与受限图推理相结合的新框架。舌部图像诊断是一个特别具有挑战性的领域，要求精细的视觉和语义理解。我们的方法利用自我监督掩码自编码器（MAE）从未标注数据中学习可转移的视觉表示；使用图注意力网络（GAT）通过专家定义的结构性图来建模标签相关性；通过约束感知优化和KL散度及正则化损失来实施临床先验；并通过不对称损失（ASL）和提升集成解决标签不平衡问题。为了应对标注稀缺性，我们还引入了TongueAtlas-4K，这是一个全面的专家策展基准，包含4,000张带有22个诊断标签的图像——是舌分析中最大的公共数据集。验证结果表明，我们的方法达到了最佳性能。尽管优化用于舌部诊断，但该框架可以轻松推广到更广泛的诊断医学成像任务。', 'title_zh': 'MIRNet: 结合约束图推理与预训练的诊断医学影像处理'}
{'arxiv_id': 'arXiv:2511.09973', 'title': 'Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models', 'authors': "Satoshi Suzuki, Shin'ya Yamaguchi, Shoichiro Takeda, Taiga Yamane, Naoki Makishima, Naotaka Kawata, Mana Ihori, Tomohiro Tanaka, Shota Orihashi, Ryo Masumura", 'link': 'https://arxiv.org/abs/2511.09973', 'abstract': 'Contrastive pre-trained vision-language models, such as CLIP, demonstrate strong generalization abilities in zero-shot classification by leveraging embeddings extracted from image and text encoders. This paper aims to robustly fine-tune these vision-language models on in-distribution (ID) data without compromising their generalization abilities in out-of-distribution (OOD) and zero-shot settings. Current robust fine-tuning methods tackle this challenge by reusing contrastive learning, which was used in pre-training, for fine-tuning. However, we found that these methods distort the geometric structure of the embeddings, which plays a crucial role in the generalization of vision-language models, resulting in limited OOD and zero-shot performance. To address this, we propose Difference Vector Equalization (DiVE), which preserves the geometric structure during fine-tuning. The idea behind DiVE is to constrain difference vectors, each of which is obtained by subtracting the embeddings extracted from the pre-trained and fine-tuning models for the same data sample. By constraining the difference vectors to be equal across various data samples, we effectively preserve the geometric structure. Therefore, we introduce two losses: average vector loss (AVL) and pairwise vector loss (PVL). AVL preserves the geometric structure globally by constraining difference vectors to be equal to their weighted average. PVL preserves the geometric structure locally by ensuring a consistent multimodal alignment. Our experiments demonstrate that DiVE effectively preserves the geometric structure, achieving strong results across ID, OOD, and zero-shot metrics.', 'abstract_zh': '对比预训练的跨模态模型，如CLIP，在零样本分类中通过利用从图像和文本编码器中提取的嵌入展示了强大的泛化能力。本文旨在在保持跨模态模型在分布内（ID）数据上稳健微调的同时，不牺牲其在分布外（OOD）和零样本设置中的泛化能力。当前的稳健微调方法通过重新利用预训练中使用的对比学习来解决这一挑战。然而，我们发现这些方法扭曲了嵌入的几何结构，这在跨模态模型的泛化中起着关键作用，导致有限的OOD和零样本性能。为了解决这个问题，我们提出了差异向量均衡（DiVE）方法，在微调过程中保留嵌入的几何结构。DiVE的核心思想是限制每个差异向量，该差异向量是通过从预训练模型和微调模型中为同一数据样本提取的嵌入相减得到的。通过确保不同数据样本的差异向量相等，我们有效地保留了几何结构。因此，我们引入了两种损失：平均向量损失（AVL）和成对向量损失（PVL）。AVL通过限制差异向量等于其加权平均值来全局保留几何结构。PVL通过确保多模态对齐的一致性来局部保留几何结构。我们的实验表明，DiVE有效地保留了几何结构，实现了在ID、OOD和零样本指标上的强性能。', 'title_zh': '视觉-语言模型稳健微调的差分向量均衡化'}
{'arxiv_id': 'arXiv:2511.09948', 'title': 'Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment', 'authors': 'Zhicheng Liao, Dongxu Wu, Zhenshan Shi, Sijie Mai, Hanwei Zhu, Lingyu Zhu, Yuncheng Jiang, Baoliang Chen', 'link': 'https://arxiv.org/abs/2511.09948', 'abstract': 'Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as "a good photo" or "a bad photo." However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality. In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue. Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity. The resulting scalar summary serves as a semantically-normalized auxiliary cue that complements cosine-based prompt matching. To integrate both cues effectively, we further design a confidence-guided fusion scheme that adaptively weighs each term according to its relative strength. Extensive experiments on multiple benchmark IQA datasets demonstrate that our method consistently outperforms standard CLIP-based IQA and state-of-the-art baselines, without any task-specific training.', 'abstract_zh': '近期的研究重新利用了对比语言-图像预训练(CLIP)模型进行无参考图像质量评估(NR-IQA)，通过测量图像嵌入与“好照片”或“坏照片”等文本提示之间的余弦相似度来实现。然而，这种语义相似度忽略了尚未充分探索的关键线索：CLIP图像特征的幅度，我们通过实验发现其与感知质量之间存在强烈的关联。在这项工作中，我们引入了一种新的自适应融合框架，该框架结合了基于余弦相似度的提示匹配与幅度感知的质量线索。具体而言，我们首先提取绝对CLIP图像特征，并应用Box-Cox变换来统计正常化特征分布并减轻语义敏感性。生成的标量摘要作为语义正则化的辅助线索，补充了基于余弦相似度的提示匹配。为了有效地结合这两种线索，我们进一步设计了一种基于置信度的自适应加权融合方案，根据其相对强度自适应地加权每个术语。在多个基准IQA数据集上的广泛实验表明，我们的方法在所有情况下都优于标准的CLIP基IQA方法和最先进的基线，而无需特定任务的训练。', 'title_zh': '超越余弦相似度幅度的CLIP无参考图像质量评估'}
{'arxiv_id': 'arXiv:2511.09942', 'title': 'AdaptViG: Adaptive Vision GNN with Exponential Decay Gating', 'authors': 'Mustafa Munir, Md Mostafijur Rahman, Radu Marculescu', 'link': 'https://arxiv.org/abs/2511.09942', 'abstract': 'Vision Graph Neural Networks (ViGs) offer a new direction for advancements in vision architectures. While powerful, ViGs often face substantial computational challenges stemming from their graph construction phase, which can hinder their efficiency. To address this issue we propose AdaptViG, an efficient and powerful hybrid Vision GNN that introduces a novel graph construction mechanism called Adaptive Graph Convolution. This mechanism builds upon a highly efficient static axial scaffold and a dynamic, content-aware gating strategy called Exponential Decay Gating. This gating mechanism selectively weighs long-range connections based on feature similarity. Furthermore, AdaptViG employs a hybrid strategy, utilizing our efficient gating mechanism in the early stages and a full Global Attention block in the final stage for maximum feature aggregation. Our method achieves a new state-of-the-art trade-off between accuracy and efficiency among Vision GNNs. For instance, our AdaptViG-M achieves 82.6% top-1 accuracy, outperforming ViG-B by 0.3% while using 80% fewer parameters and 84% fewer GMACs. On downstream tasks, AdaptViG-M obtains 45.8 mIoU, 44.8 APbox, and 41.1 APmask, surpassing the much larger EfficientFormer-L7 by 0.7 mIoU, 2.2 APbox, and 2.1 APmask, respectively, with 78% fewer parameters.', 'abstract_zh': '适应性视觉图神经网络（AdaptViG）：一种高效且强大的混合视觉图神经网络', 'title_zh': 'AdaptViG：具有指数衰减门控的自适应视觉GNN'}
{'arxiv_id': 'arXiv:2511.09895', 'title': 'Simulator and Experience Enhanced Diffusion Model for Comprehensive ECG Generation', 'authors': 'Xiaoda Wang, Kaiqiao Han, Yuhao Xu, Xiao Luo, Yizhou Sun, Wei Wang, Carl Yang', 'link': 'https://arxiv.org/abs/2511.09895', 'abstract': 'Cardiovascular disease (CVD) is a leading cause of mortality worldwide. Electrocardiograms (ECGs) are the most widely used non-invasive tool for cardiac assessment, yet large, well-annotated ECG corpora are scarce due to cost, privacy, and workflow constraints. Generating ECGs can be beneficial for the mechanistic understanding of cardiac electrical activity, enable the construction of large, heterogeneous, and unbiased datasets, and facilitate privacy-preserving data sharing. Generating realistic ECG signals from clinical context is important yet underexplored. Recent work has leveraged diffusion models for text-to-ECG generation, but two challenges remain: (i) existing methods often overlook the physiological simulator knowledge of cardiac activity; and (ii) they ignore broader, experience-based clinical knowledge grounded in real-world practice. To address these gaps, we propose SE-Diff, a novel physiological simulator and experience enhanced diffusion model for comprehensive ECG generation. SE-Diff integrates a lightweight ordinary differential equation (ODE)-based ECG simulator into the diffusion process via a beat decoder and simulator-consistent constraints, injecting mechanistic priors that promote physiologically plausible waveforms. In parallel, we design an LLM-powered experience retrieval-augmented strategy to inject clinical knowledge, providing more guidance for ECG generation. Extensive experiments on real-world ECG datasets demonstrate that SE-Diff improves both signal fidelity and text-ECG semantic alignment over baselines, proving its superiority for text-to-ECG generation. We further show that the simulator-based and experience-based knowledge also benefit downstream ECG classification.', 'abstract_zh': '心血管疾病（CVD）是全球主要的 mortality 原因。心电图（ECGs）是最常用的无创心脏评估工具，但由于成本、隐私和工作流程限制，大规模且注释良好的 ECG 数据集稀缺。生成心电图可以有助于心脏电活动的机制理解，使构建大规模、异质且无偏的数据集成为可能，并促进隐私保护的数据共享。从临床背景生成逼真的心电图信号非常重要但尚未得到充分探索。 Recent work 采用了扩散模型进行文本到心电图生成，但仍存在两个挑战：（i）现有方法通常忽略了心脏活动的生理模拟器知识；（ii）它们忽略了基于实际临床实践的更广泛的经验性知识。为解决这些缺口，我们提出了 SE-Diff，一种新的生理模拟器和经验增强的扩散模型，用于全面的心电图生成。SE-Diff 通过心跳解码器和与模拟器一致的约束将轻量级的基于常微分方程（ODE）的心电图模拟器整合到扩散过程中，注入机制先验以促进生理上合理的波形。同时，我们设计了一种基于大语言模型（LLM）的经验检索增强策略，注入临床知识，为心电图生成提供更多的指导。在实际心电图数据集上的广泛实验表明，SE-Diff 在信号保真度和文本到心电图语义匹配方面优于基线方法，证明其在文本到心电图生成中的优越性。我们进一步展示，基于模拟器的知识和基于经验的知识也造福了下游的心电图分类。', 'title_zh': '模拟器和经验增强的扩散模型用于综合ECG生成'}
{'arxiv_id': 'arXiv:2511.09891', 'title': 'Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images', 'authors': 'Jinfu Li, Yuqi Huang, Hong Song, Ting Wang, Jianghan Xia, Yucong Lin, Jingfan Fan, Jian Yang', 'link': 'https://arxiv.org/abs/2511.09891', 'abstract': 'Recently, despite the remarkable advancements in object detection, modern detectors still struggle to detect tiny objects in aerial images. One key reason is that tiny objects carry limited features that are inevitably degraded or lost during long-distance network propagation. Another is that smaller objects receive disproportionately greater regression penalties than larger ones during training. To tackle these issues, we propose a Scale-Aware Relay Layer (SARL) and a Scale-Adaptive Loss (SAL) for tiny object detection, both of which are seamlessly compatible with the top-performing frameworks. Specifically, SARL employs a cross-scale spatial-channel attention to progressively enrich the meaningful features of each layer and strengthen the cross-layer feature sharing. SAL reshapes the vanilla IoU-based losses so as to dynamically assign lower weights to larger objects. This loss is able to focus training on tiny objects while reducing the influence on large objects. Extensive experiments are conducted on three benchmarks (\\textit{i.e.,} AI-TOD, DOTA-v2.0 and VisDrone2019), and the results demonstrate that the proposed method boosts the generalization ability by 5.5\\% Average Precision (AP) when embedded in YOLOv5 (anchor-based) and YOLOx (anchor-free) baselines. Moreover, it also promotes the robust performance with 29.0\\% AP on the real-world noisy dataset (\\textit{i.e.,} AI-TOD-v2.0).', 'abstract_zh': '近年来，尽管目标检测取得了显著进展，但现代检测器在空中图像中仍难以检测微小目标。主要原因之一是微小目标携带的特征有限，在长距离网络传播过程中不可避免地被降解或丢失。另一个原因是较小的目标在训练过程中受到与其更大的回归惩罚不成比例的影响。为了解决这些问题，我们提出了一种尺度感知接力层（SARL）和尺度自适应损失（SAL）以提升微小目标检测能力，两者均可无缝兼容顶级框架。具体而言，SARL 使用跨尺度空-通道注意力机制，逐步丰富每一层的有效特征，并加强跨层特征共享。SAL 重塑了传统的基于IoU的损失函数，使其能动态地给予大目标更低的权重。这种损失机制能够在保持训练强度的同时减少对大目标的影响。我们在三个基准数据集（即AI-TOD、DOTA-v2.0和VisDrone2019）上进行了广泛的实验，结果显示，当嵌入YOLOv5（基于锚框）和YOLOx（无锚框）基线时，所提出的方法将平均精度（AP）的一般化能力提升了5.5%。此外，它还在实际嘈杂数据集AI-TOD-v2.0上促进了鲁棒性能，提高了29.0%的AP。', 'title_zh': '面向航拍图像的尺度awarerelay和尺度自适应损失函数的小目标检测'}
{'arxiv_id': 'arXiv:2511.09809', 'title': 'Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models', 'authors': 'Konstantinos M. Dafnis, Dimitris N. Metaxas', 'link': 'https://arxiv.org/abs/2511.09809', 'abstract': 'Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at this https URL.', 'abstract_zh': 'Vision-Language模型在零样本推理方面表现出色，但在测试时领域变化下往往表现下降。由于这一原因，近期出现了能够针对单张未标记图像适应Vision-Language模型的轻量级测试时适应策略。然而，现有的适应策略，如测试时提示调优，通常需要对大型编码权重进行反向传播或修改核心模型组件。在这个工作中，我们介绍了Spectrum-Aware测试时引导（STS）框架，该框架从文本嵌入中提取谱子空间以定义主要语义方向，并通过适配少量每样本偏移参数来引导潜在表示，以最小化增强视图间的熵。STS完全在潜在空间中进行测试时推理，无需对冻结的编码器进行反向传播或修改。基于标准评估协议，我们全面的实验证明，STS显著超越或优于最先进的测试时适应方法，同时仅引入少量额外参数，并且与传统测试时提示调优相比，推理速度可快8倍，内存占用减少12倍。代码可在以下链接获得：this https URL。', 'title_zh': '测试时谱aware潜在引导零样本泛化在视觉-语言模型中'}
{'arxiv_id': 'arXiv:2511.09740', 'title': 'Soiling detection for Advanced Driver Assistance Systems', 'authors': 'Filip Beránek, Václav Diviš, Ivan Gruber', 'link': 'https://arxiv.org/abs/2511.09740', 'abstract': 'Soiling detection for automotive cameras is a crucial part of advanced driver assistance systems to make them more robust to external conditions like weather, dust, etc. In this paper, we regard the soiling detection as a semantic segmentation problem. We provide a comprehensive comparison of popular segmentation methods and show their superiority in performance while comparing them to tile-level classification approaches. Moreover, we present an extensive analysis of the Woodscape dataset showing that the original dataset contains a data-leakage and imprecise annotations. To address these problems, we create a new data subset, which, despite being much smaller, provides enough information for the segmentation method to reach comparable results in a much shorter time. All our codes and dataset splits are available at this https URL.', 'abstract_zh': '汽车摄像头污渍检测是高级驾驶辅助系统中应对天气、灰尘等外部条件的关键组成部分。本文将污渍检测视为语义分割问题，并提供了对流行分割方法的全面比较，展示了这些方法在与瓷砖级别分类方法进行比较时的优越性能。此外，本文还对Woodscape数据集进行了详尽分析，指出原始数据集存在数据泄露和标注不准确的问题。为解决这些问题，我们创建了一个新的数据子集，尽管该子集更小，但仍能为分割方法提供足够的信息，使其在更短的时间内达到可比的结果。所有代码和数据集划分均可在此链接获取。', 'title_zh': '高级驾驶辅助系统中的污损检测'}
{'arxiv_id': 'arXiv:2511.09735', 'title': 'Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction', 'authors': 'Ahmed Alia, Mohcine Chraibi, Armin Seyfried', 'link': 'https://arxiv.org/abs/2511.09735', 'abstract': 'In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.', 'abstract_zh': '在动态和拥挤环境中，基于深度学习的行人轨迹预测依然是一项具有挑战性的任务，原因在于人类运动的复杂性质以及个体间的相互影响。最近，通过从2D轨迹数据中隐式学习这些模式，深度学习模型已经取得了显著成果。然而，大多数方法将行人视为点实体，忽略了每个人占据的真实空间。为解决这些限制，本文提出了一种新的深度学习模型，该模型基于Social LSTM并引入了一个新的动态占据空间损失函数。这种损失函数在不会增加不同人群密度（从低到高）的位移误差的情况下，引导Social LSTM学习避免现实碰撞。该函数通过结合平均位移误差和一个新的对场景密度和个体空间占用敏感的碰撞惩罚来实现这一点。为了高效地进行训练和评估，本文从2022年里昂节日灯光节记录的真实行人轨迹中生成了五个数据集。四个数据集代表不同的均匀人群条件——低密度、中密度、高密度和极高密度——而第五个数据集对应非均匀密度分布。实验结果显示，所提出的模型不仅降低了碰撞率，还在每个数据集中提高了位移预测的准确性。具体而言，该模型在所有数据集上将碰撞率降低至多31%，并将平均位移误差和最终位移误差分别降低5%和6%。此外，所提模型在大多数测试集上持续优于多种现有的深度学习模型。', 'title_zh': '基于动态占用建模的Social LSTM及其在真实场景行人轨迹预测中的应用'}
{'arxiv_id': 'arXiv:2511.09605', 'title': 'TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks', 'authors': 'Johannes Kiechle, Stefan M. Fischer, Daniel M. Lang, Cosmin I. Bercea, Matthew J. Nyflot, Lina Felsner, Julia A. Schnabel, Jan C. Peeken', 'link': 'https://arxiv.org/abs/2511.09605', 'abstract': 'The growing number of medical tomography examinations has necessitated the development of automated methods capable of extracting comprehensive imaging features to facilitate downstream tasks such as tumor characterization, while assisting physicians in managing their growing workload. However, 3D medical image classification remains a challenging task due to the complex spatial relationships and long-range dependencies inherent in volumetric data. Training models from scratch suffers from low data regimes, and the absence of 3D large-scale multimodal datasets has limited the development of 3D medical imaging foundation models. Recent studies, however, have highlighted the potential of 2D vision foundation models, originally trained on natural images, as powerful feature extractors for medical image analysis. Despite these advances, existing approaches that apply 2D models to 3D volumes via slice-based decomposition remain suboptimal. Conventional volume slicing strategies, which rely on canonical planes such as axial, sagittal, or coronal, may inadequately capture the spatial extent of target structures when these are misaligned with standardized viewing planes. Furthermore, existing slice-wise aggregation strategies rarely account for preserving the volumetric structure, resulting in a loss of spatial coherence across slices. To overcome these limitations, we propose TomoGraphView, a novel framework that integrates omnidirectional volume slicing with spherical graph-based feature aggregation. We publicly share our accessible code base at this http URL and provide a user-friendly library for omnidirectional volume slicing at this https URL.', 'abstract_zh': '随着医学断层扫描检查数量的增加，亟需开发能够提取全面影像特征的自动化方法，以辅助下游任务如肿瘤表征，并帮助医生管理日益增长的工作负荷。然而，由于体积数据中固有的复杂时空关系和长程依赖性，3D医学图像分类仍然是一个具有挑战性的任务。从零开始训练模型在数据稀缺的情况下表现不佳，缺乏大规模多模态3D数据集限制了3D医学成像基础模型的发展。然而，最近的研究表明，最初在自然图像上训练的2D视觉基础模型在医学图像分析中具有强大的特征提取潜力。尽管取得了这些进展，现有的通过切片分解将2D模型应用于3D体积的方法仍存在不足。传统的基于标准平面（如轴位、矢状位或冠状位）的体积切片策略可能难以准确捕获与标准视图平面不一致的目标结构的空间范围。此外，现有的切片级聚合策略很少考虑到保持体积结构，导致切片间的空间连贯性丢失。为克服这些局限，我们提出了TomoGraphView，这是一个新颖的框架，结合了全方位体积切片和基于球面图的特征聚合。我们在此公布了我们的开放代码库，并在此提供了友好的全方位体积切片库。', 'title_zh': 'TomoGraphView：基于全景切片表示和图神经网络的3D医学图像分类'}
