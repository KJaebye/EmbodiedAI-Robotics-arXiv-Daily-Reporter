{'arxiv_id': 'arXiv:2511.10281', 'title': 'FactGuard: Event-Centric and Commonsense-Guided Fake News Detection', 'authors': 'Jing He, Han Zhang, Yuanhui Xiao, Wei Guo, Shaowen Yao, Renyang Liu', 'link': 'https://arxiv.org/abs/2511.10281', 'abstract': 'Fake news detection methods based on writing style have achieved remarkable progress. However, as adversaries increasingly imitate the style of authentic news, the effectiveness of such approaches is gradually diminishing. Recent research has explored incorporating large language models (LLMs) to enhance fake news detection. Yet, despite their transformative potential, LLMs remain an untapped goldmine for fake news detection, with their real-world adoption hampered by shallow functionality exploration, ambiguous usability, and prohibitive inference costs. In this paper, we propose a novel fake news detection framework, dubbed FactGuard, that leverages LLMs to extract event-centric content, thereby reducing the impact of writing style on detection performance. Furthermore, our approach introduces a dynamic usability mechanism that identifies contradictions and ambiguous cases in factual reasoning, adaptively incorporating LLM advice to improve decision reliability. To ensure efficiency and practical deployment, we employ knowledge distillation to derive FactGuard-D, enabling the framework to operate effectively in cold-start and resource-constrained scenarios. Comprehensive experiments on two benchmark datasets demonstrate that our approach consistently outperforms existing methods in both robustness and accuracy, effectively addressing the challenges of style sensitivity and LLM usability in fake news detection.', 'abstract_zh': '基于写作风格的假新闻检测方法取得了显著进展，但随着对手 increasingly 仿真的新闻写作风格，这类方法的有效性也在逐渐减弱。近期研究探索了结合大规模语言模型（LLMs）以增强假新闻检测。然而，尽管LLMs具有变革潜力，它们在假新闻检测领域仍是一个未被充分开发的宝库，实际应用受限于浅层次的功能探索、模糊的可用性和高昂的推理成本。本文提出了一种名为FactGuard的新型假新闻检测框架，利用LLMs提取事件中心化的内容，从而减少写作风格对检测性能的影响。此外，本方法引入了一个动态可用性机制，用于识别事实推理中的矛盾和模糊情况，并自适应地整合LLM建议以提高决策可靠性。为了确保效率和实际部署，我们采用知识蒸馏方法构建了FactGuard-D，使框架能够有效运行于冷启动和资源受限的条件下。全面的实验证明，我们的方法在鲁棒性和准确性上均优于现有方法，有效解决了假新闻检测中的写作风格敏感性和LLM可用性挑战。', 'title_zh': 'FactGuard: 事件中心化和常识引导的假新闻检测'}
{'arxiv_id': 'arXiv:2511.10277', 'title': 'Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware', 'authors': 'Martin Braas, Lukas Esterle', 'link': 'https://arxiv.org/abs/2511.10277', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems.', 'abstract_zh': '大规模语言模型（LLMs）在生成类人类文本方面表现出色，但在计算机游戏中的对话系统应用仍受到限制。这一限制源于其对高硬件资源的需求、延迟约束以及在游戏环境中保持清晰知识边界的需求。在本文中，我们提出了一种模块化的NPC对话系统，该系统利用小型语言模型（SLMs），并对特定NPC人设进行 fine-tuning，同时集成了运行时可互换的记忆模块。这些记忆模块保留了角色特定的对话上下文和世界观知识，能够在游戏过程中实现表达性交互和长期记忆，而无需在游戏过程中重新训练或重新加载模型。我们使用三个开源SLMs——DistilGPT-2、TinyLlama-1.1B-Chat和Mistral-7B-Instruct，分别基于合成的人设对齐数据进行训练，并在消费级硬件上进行基准测试。虽然我们的方法是针对游戏应用而设计的，但其模块化设计和人设驱动的记忆架构在需要表达性、扩展性和丰富记忆的对话代理的应用领域具有广泛的应用潜力，如虚拟助手、客户支持机器人或互动教育系统。', 'title_zh': '固定人设SLMs与模块化记忆：面向消费者硬件的可扩展NPC对话'}
{'arxiv_id': 'arXiv:2511.10268', 'title': 'Causal-HalBench: Uncovering LVLMs Object Hallucinations Through Causal Intervention', 'authors': 'Zhe Xu, Zhicai Wang, Junkang Wu, Jinda Lu, Xiang Wang', 'link': 'https://arxiv.org/abs/2511.10268', 'abstract': 'Large Vision-Language Models (LVLMs) often suffer from object hallucination, making erroneous judgments about the presence of objects in images. We propose this primar- ily stems from spurious correlations arising when models strongly associate highly co-occurring objects during train- ing, leading to hallucinated objects influenced by visual con- text. Current benchmarks mainly focus on hallucination de- tection but lack a formal characterization and quantitative evaluation of spurious correlations in LVLMs. To address this, we introduce causal analysis into the object recognition scenario of LVLMs, establishing a Structural Causal Model (SCM). Utilizing the language of causality, we formally de- fine spurious correlations arising from co-occurrence bias. To quantify the influence induced by these spurious correla- tions, we develop Causal-HalBench, a benchmark specifically constructed with counterfactual samples and integrated with comprehensive causal metrics designed to assess model ro- bustness against spurious correlations. Concurrently, we pro- pose an extensible pipeline for the construction of these coun- terfactual samples, leveraging the capabilities of proprietary LVLMs and Text-to-Image (T2I) models for their genera- tion. Our evaluations on mainstream LVLMs using Causal- HalBench demonstrate these models exhibit susceptibility to spurious correlations, albeit to varying extents.', 'abstract_zh': '大型视觉-语言模型（LVLMs）往往存在对象错觉问题，对图像中对象的存在做出错误判断。我们主要提出这是由于模型在训练过程中强烈关联高度共现的对象时产生的虚假关联导致的，进而使得对象错觉受到视觉上下文的影响。当前的基准主要集中在错觉检测，但缺乏对LVLMs中虚假关联的正式表征和定量评估。为解决这一问题，我们在LVLM的对象识别场景中引入因果分析，建立结构因果模型（SCM）。利用因果语言，我们正式定义了由共现偏差引起的虚假关联。为了量化这些虚假关联的影响，我们开发了Causal-HalBench基准，该基准专门构建了反事实样本，并整合了全面的因果度量体系，旨在评估模型在虚假关联下的稳健性。同时，我们提出了一个可扩展的反事实样本构建管道，利用私有LVLM和文本到图像（T2I）模型的能力生成这些反事实样本。在主要LVLM上的评估表明，这些模型在不同程度上对虚假关联表现出敏感性。', 'title_zh': '因果-HalBench：通过因果干预揭示LVLMs对象幻觉'}
{'arxiv_id': 'arXiv:2511.10240', 'title': 'ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs', 'authors': 'Minbae Park, Hyemin Yang, Jeonghyun Kim, Kunsoo Park, Hyunjoon Kim', 'link': 'https://arxiv.org/abs/2511.10240', 'abstract': 'Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.', 'abstract_zh': '大型语言模型（LLMs）表现出强大的推理能力但容易出现幻觉并具有有限的透明度。最近，结合知识图谱（KGs）的KG增强LLMs已被证明可提高推理性能，尤其是在复杂的知识密集型任务方面。然而，这些方法仍然面临着重大的挑战，包括不准确的检索和推理失败，这些问题往往因长输入上下文模糊相关的信息或上下文构建难以捕捉不同问题类型所需的更丰富的逻辑方向而加剧。此外，许多方法依赖于LLMs直接从KG中检索证据并自我评估此类证据的充分性，这经常导致过早或错误的推理。为解决检索和推理失败的问题，我们提出了一种多跳知识图谱问答（KGQA）框架ProgRAG，该框架将复杂的问题分解为子问题，并通过回答每个子问题逐步扩展部分推理路径。在每一步中，外部检索器收集候选证据，然后通过LLM的不确定性感知剪枝进行细化。最后，通过组织和重组从子问题答案中获得的部分推理路径来优化LLM推理的上下文。在三个知名数据集上的实验结果显示，ProgRAG在多跳KGQA中优于现有基线，提供了更高的可靠性和推理质量。', 'title_zh': 'ProgRAG: 抗幻觉渐进式知识图谱检索与推理'}
{'arxiv_id': 'arXiv:2511.10233', 'title': 'Bridging Synthetic and Real Routing Problems via LLM-Guided Instance Generation and Progressive Adaptation', 'authors': 'Jianghan Zhu, Yaoxin Wu, Zhuoyi Lin, Zhengyuan Zhang, Haiyan Yin, Zhiguang Cao, Senthilnath Jayavelu, Xiaoli Li', 'link': 'https://arxiv.org/abs/2511.10233', 'abstract': 'Recent advances in Neural Combinatorial Optimization (NCO) methods have significantly improved the capability of neural solvers to handle synthetic routing instances. Nonetheless, existing neural solvers typically struggle to generalize effectively from synthetic, uniformly-distributed training data to real-world VRP scenarios, including widely recognized benchmark instances from TSPLib and CVRPLib. To bridge this generalization gap, we present Evolutionary Realistic Instance Synthesis (EvoReal), which leverages an evolutionary module guided by large language models (LLMs) to generate synthetic instances characterized by diverse and realistic structural patterns. Specifically, the evolutionary module produces synthetic instances whose structural attributes statistically mimics those observed in authentic real-world instances. Subsequently, pre-trained NCO models are progressively refined, firstly aligning them with these structurally enriched synthetic distributions and then further adapting them through direct fine-tuning on actual benchmark instances. Extensive experimental evaluations demonstrate that EvoReal markedly improves the generalization capabilities of state-of-the-art neural solvers, yielding a notable reduced performance gap compared to the optimal solutions on the TSPLib (1.05%) and CVRPLib (2.71%) benchmarks across a broad spectrum of problem scales.', 'abstract_zh': 'Recent Advances in Evolutionary Realistic Instance Synthesis for Neural Combinatorial Optimization Methods', 'title_zh': '通过LLM引导的实例生成和渐进适应连接合成和现实路由问题'}
{'arxiv_id': 'arXiv:2511.10210', 'title': 'Advanced Black-Box Tuning of Large Language Models with Limited API Calls', 'authors': 'Zhikang Xie, Weilin Wan, Peizhu Gong, Weizhong Zhang, Cheng Jin', 'link': 'https://arxiv.org/abs/2511.10210', 'abstract': 'Black-box tuning is an emerging paradigm for adapting large language models (LLMs) to better achieve desired behaviors, particularly when direct access to model parameters is unavailable. Current strategies, however, often present a dilemma of suboptimal extremes: either separately train a small proxy model and then use it to shift the predictions of the foundation model, offering notable efficiency but often yielding limited improvement; or making API calls in each tuning iteration to the foundation model, which entails prohibitive computational costs. Therefore, we propose a novel advanced black-box tuning method for LLMs with limited API calls. Our core strategy involves training a Gaussian Process (GP) surrogate model with "LogitMap Pairs" derived from querying the foundation model on a minimal but highly informative training subset. This surrogate can approximate the outputs of the foundation model to guide the training of the proxy model, thereby effectively reducing the need for direct queries to the foundation model. Extensive experiments verify that our approach elevates pre-trained language model accuracy from 55.92% to 86.85%, reducing the frequency of API queries to merely 1.38%. This significantly outperforms offline approaches that operate entirely without API access. Notably, our method also achieves comparable or superior accuracy to query-intensive approaches, while significantly reducing API costs. This offers a robust and high-efficiency paradigm for language model adaptation.', 'abstract_zh': '黑箱调优是一种新兴的大语言模型适配 paradigm，特别适用于在无法直接访问模型参数时更好地实现所需行为。当前策略往往面临次优的选择困境：要么单独训练一个小型代理模型，然后使用它来调整基础模型的预测，这虽具有显著的效率，但通常会带来有限的改进；要么在每次调优迭代中通过 API 调用基础模型，这将产生巨大的计算成本。因此，我们提出了一种使用有限 API 调用的先进黑箱调优方法。我们的核心策略是使用从少量但高度信息性训练子集中查询基础模型得到的“LogitMap 对”训练高斯过程（GP）替代模型。该替代模型能够近似基础模型的输出以指导代理模型的训练，从而有效减少直接查询基础模型的需求。 extensive 实验验证了我们的方法将预训练语言模型的准确性从 55.92% 提高到 86.85%，将 API 查询频率降低到仅 1.38%。这显著优于完全不使用 API 的离线方法。值得注意的是，我们的方法在 API 调用密集的方法中也实现了可比或更优的准确性，同时显著降低了 API 成本。这提供了一种稳健且高效的语言模型适配范式。', 'title_zh': '使用有限的API调用对大型语言模型进行高级黑盒调优'}
{'arxiv_id': 'arXiv:2511.10119', 'title': 'Intilligence Foundation Model: A New Perspective to Approach Artificial General Intelligence', 'authors': 'Borui Cai, Yao Zhao', 'link': 'https://arxiv.org/abs/2511.10119', 'abstract': 'We propose a new perspective for approaching artificial general intelligence (AGI) through an intelligence foundation model (IFM). Unlike existing foundation models (FMs), which specialize in pattern learning within specific domains such as language, vision, or time series, IFM aims to acquire the underlying mechanisms of intelligence by learning directly from diverse intelligent behaviors. Vision, language, and other cognitive abilities are manifestations of intelligent behavior; learning from this broad range of behaviors enables the system to internalize the general principles of intelligence. Based on the fact that intelligent behaviors emerge from the collective dynamics of biological neural systems, IFM consists of two core components: a novel network architecture, termed the state neural network, which captures neuron-like dynamic processes, and a new learning objective, neuron output prediction, which trains the system to predict neuronal outputs from collective dynamics. The state neural network emulates the temporal dynamics of biological neurons, allowing the system to store, integrate, and process information over time, while the neuron output prediction objective provides a unified computational principle for learning these structural dynamics from intelligent behaviors. Together, these innovations establish a biologically grounded and computationally scalable foundation for building systems capable of generalization, reasoning, and adaptive learning across domains, representing a step toward truly AGI.', 'abstract_zh': '我们提出了一种通过智能基础模型（IFM）来接近通用人工智能（AGI）的新视角。与现有专门针对诸如语言、视觉或时间序列等特定领域中的模式学习的基础模型（FMs）不同，IFM旨在通过直接学习各种智能行为来获取智能的内在机制。视觉、语言和其他认知能力是智能行为的体现；通过广泛行为的学习，系统能够内化智能的一般原则。基于智能行为源自生物神经系统的集体动态这一事实，IFM由两个核心组件组成：一种称为状态神经网络的新型网络架构，用于捕捉类似神经元的动力学过程，以及一种新的学习目标，即神经元输出预测，训练系统根据集体动态预测神经元输出。状态神经网络模拟了生物神经元的时间动态，使系统能够在时间上存储、整合和处理信息，而神经元输出预测目标为从智能行为中学习这些结构动态提供了一个统一的计算原理。这两项创新为构建能够在跨领域进行泛化、推理和适应性学习的系统奠定了生物学基础和计算可扩展性，代表着向真正意义上的AGI迈进的一步。', 'title_zh': '智能基础模型：接近人工通用智能的新视角'}
{'arxiv_id': 'arXiv:2511.10067', 'title': 'Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement Learning', 'authors': 'Yuxuan Zhou, Yubin Wang, Bin Wang, Chen Ning, Xien Liu, Ji Wu, Jianye Hao', 'link': 'https://arxiv.org/abs/2511.10067', 'abstract': "Large language models (LLMs) have shown great promise in the medical domain, achieving strong performance on several benchmarks. However, they continue to underperform in real-world medical scenarios, which often demand stronger context-awareness, i.e., the ability to recognize missing or critical details (e.g., user identity, medical history, risk factors) and provide safe, helpful, and contextually appropriate responses. To address this issue, we propose Multifaceted Self-Refinement (MuSeR), a data-driven approach that enhances LLMs' context-awareness along three key facets (decision-making, communication, and safety) through self-evaluation and refinement. Specifically, we first design a attribute-conditioned query generator that simulates diverse real-world user contexts by varying attributes such as role, geographic region, intent, and degree of information ambiguity. An LLM then responds to these queries, self-evaluates its answers along three key facets, and refines its responses to better align with the requirements of each facet. Finally, the queries and refined responses are used for supervised fine-tuning to reinforce the model's context-awareness ability. Evaluation results on the latest HealthBench dataset demonstrate that our method significantly improves LLM performance across multiple aspects, with particularly notable gains in the context-awareness axis. Furthermore, by incorporating knowledge distillation with the proposed method, the performance of a smaller backbone LLM (e.g., Qwen3-32B) surpasses its teacher model, achieving a new SOTA across all open-source LLMs on HealthBench (63.8%) and its hard subset (43.1%). Code and dataset will be released at this https URL.", 'abstract_zh': '基于多维度自我修正的大型语言模型在医疗领域的增强方法', 'title_zh': '通过多方面自我精炼学习提高LLMs的医疗场景认知能力'}
{'arxiv_id': 'arXiv:2511.10038', 'title': 'Efficient Thought Space Exploration through Strategic Intervention', 'authors': 'Ziheng Li, Hengyi Cai, Xiaochi Wei, Yuchen Li, Shuaiqiang Wang, Zhi-Hong Deng, Dawei Yin', 'link': 'https://arxiv.org/abs/2511.10038', 'abstract': "While large language models (LLMs) demonstrate emerging reasoning capabilities, current inference-time expansion methods incur prohibitive computational costs by exhaustive sampling. Through analyzing decoding trajectories, we observe that most next-token predictions align well with the golden output, except for a few critical tokens that lead to deviations. Inspired by this phenomenon, we propose a novel Hint-Practice Reasoning (HPR) framework that operationalizes this insight through two synergistic components: 1) a hinter (powerful LLM) that provides probabilistic guidance at critical decision points, and 2) a practitioner (efficient smaller model) that executes major reasoning steps. The framework's core innovation lies in Distributional Inconsistency Reduction (DIR), a theoretically-grounded metric that dynamically identifies intervention points by quantifying the divergence between practitioner's reasoning trajectory and hinter's expected distribution in a tree-structured probabilistic space. Through iterative tree updates guided by DIR, HPR reweights promising reasoning paths while deprioritizing low-probability branches. Experiments across arithmetic and commonsense reasoning benchmarks demonstrate HPR's state-of-the-art efficiency-accuracy tradeoffs: it achieves comparable performance to self-consistency and MCTS baselines while decoding only 1/5 tokens, and outperforms existing methods by at most 5.1% absolute accuracy while maintaining similar or lower FLOPs.", 'abstract_zh': '基于提示辅助推理的分布不一致性减少框架（HPR）：高效准确性的新范式', 'title_zh': '通过策略性干预高效探索思维空间'}
{'arxiv_id': 'arXiv:2511.10037', 'title': 'Beyond ReAct: A Planner-Centric Framework for Complex Tool-Augmented LLM Reasoning', 'authors': 'Xiaolong Wei, Yuehu Dong, Xingliang Wang, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin', 'link': 'https://arxiv.org/abs/2511.10037', 'abstract': "Existing tool-augmented large language models (LLMs) encounter significant challenges when processing complex queries. Current frameworks such as ReAct are prone to local optimization traps due to their reliance on incremental decision-making processes. To address these limitations, we propose a novel Planner-centric Plan-Execute paradigm that fundamentally resolves local optimization bottlenecks through architectural innovation. Central to our approach is a novel Planner model that performs global Directed Acyclic Graph (DAG) planning for complex queries, enabling optimized execution beyond conventional tool coordination. We also introduce ComplexTool-Plan, a large-scale benchmark dataset featuring complex queries that demand sophisticated multi-tool composition and coordination capabilities. Additionally, we develop a two-stage training methodology that integrates Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), systematically enhancing the Planner's tool selection accuracy and global planning awareness through structured DAG-based planning. When integrated with a capable executor, our framework achieves state-of-the-art performance on the StableToolBench benchmark for complex user queries, demonstrating superior end-to-end execution capabilities and robust handling of intricate multi-tool workflows.", 'abstract_zh': '现有的工具增强型大型语言模型（LLMs）在处理复杂查询时遇到显著挑战。当前的框架如ReAct容易陷入局部优化陷阱，因为它们依赖于逐步决策过程。为解决这些局限性，我们提出了一种新颖的规划者中心的计划-执行范式，通过架构创新从根本上解决了局部优化瓶颈。我们的方法的核心是一个新颖的规划者模型，能够进行全局有向无环图（DAG）规划，以复杂查询为基础实现优化执行，超越了传统的工具协调。此外，我们引入了ComplexTool-Plan，这是一个大型基准数据集，包含复杂的查询，要求高级多工具组合和协调能力。我们还开发了一种两阶段的训练方法，结合了监督微调（SFT）与组相对策略优化（GRPO），系统地通过结构化的DAG规划增强了规划者的选择工具准确性和全局规划意识。当与能力强的执行器结合时，我们的框架在StableToolBench基准上达到了复杂用户查询的最先进性能，展示了更强的端到端执行能力和稳健处理复杂多工具工作流的能力。', 'title_zh': '超越ReAct：以规划者为中心的复杂工具增强LLM推理框架'}
{'arxiv_id': 'arXiv:2511.09993', 'title': 'SPAN: Benchmarking and Improving Cross-Calendar Temporal Reasoning of Large Language Models', 'authors': 'Zhongjian Miao, Hao Fu, Chen Wei', 'link': 'https://arxiv.org/abs/2511.09993', 'abstract': "We introduce SPAN, a cross-calendar temporal reasoning benchmark, which requires LLMs to perform intra-calendar temporal reasoning and inter-calendar temporal conversion. SPAN features ten cross-calendar temporal reasoning directions, two reasoning types, and two question formats across six calendars. To enable time-variant and contamination-free evaluation, we propose a template-driven protocol for dynamic instance generation that enables assessment on a user-specified Gregorian date. We conduct extensive experiments on both open- and closed-source state-of-the-art (SOTA) LLMs over a range of dates spanning 100 years from 1960 to 2060. Our evaluations show that these LLMs achieve an average accuracy of only 34.5%, with none exceeding 80%, indicating that this task remains challenging. Through in-depth analysis of reasoning types, question formats, and temporal reasoning directions, we identify two key obstacles for LLMs: Future-Date Degradation and Calendar Asymmetry Bias. To strengthen LLMs' cross-calendar temporal reasoning capability, we further develop an LLM-powered Time Agent that leverages tool-augmented code generation. Empirical results show that Time Agent achieves an average accuracy of 95.31%, outperforming several competitive baselines, highlighting the potential of tool-augmented code generation to advance cross-calendar temporal reasoning. We hope this work will inspire further efforts toward more temporally and culturally adaptive LLMs.", 'abstract_zh': 'SPAN：跨日历时间推理基准', 'title_zh': 'SPAN: 评估与提高大型语言模型跨历时期间 reasoning 能力'}
{'arxiv_id': 'arXiv:2511.09785', 'title': 'AI Annotation Orchestration: Evaluating LLM verifiers to Improve the Quality of LLM Annotations in Learning Analytics', 'authors': 'Bakhtawar Ahtisham, Kirk Vanacore, Jinsook Lee, Zhuqian Zhou, Doug Pietrzak, Rene F. Kizilcec', 'link': 'https://arxiv.org/abs/2511.09785', 'abstract': 'Large Language Models (LLMs) are increasingly used to annotate learning interactions, yet concerns about reliability limit their utility. We test whether verification-oriented orchestration-prompting models to check their own labels (self-verification) or audit one another (cross-verification)-improves qualitative coding of tutoring discourse. Using transcripts from 30 one-to-one math sessions, we compare three production LLMs (GPT, Claude, Gemini) under three conditions: unverified annotation, self-verification, and cross-verification across all orchestration configurations. Outputs are benchmarked against a blinded, disagreement-focused human adjudication using Cohen\'s kappa. Overall, orchestration yields a 58 percent improvement in kappa. Self-verification nearly doubles agreement relative to unverified baselines, with the largest gains for challenging tutor moves. Cross-verification achieves a 37 percent improvement on average, with pair- and construct-dependent effects: some verifier-annotator pairs exceed self-verification, while others reduce alignment, reflecting differences in verifier strictness. We contribute: (1) a flexible orchestration framework instantiating control, self-, and cross-verification; (2) an empirical comparison across frontier LLMs on authentic tutoring data with blinded human "gold" labels; and (3) a concise notation, verifier(annotator) (e.g., Gemini(GPT) or Claude(Claude)), to standardize reporting and make directional effects explicit for replication. Results position verification as a principled design lever for reliable, scalable LLM-assisted annotation in Learning Analytics.', 'abstract_zh': '大型语言模型（LLMs）越来越多地用于标注学习交互，但可靠性的担忧限制了其应用。我们测试了使用旨在验证其自身标签（自我验证）或互相审查（交叉验证）的验证导向型编排提示模型，是否能提高对辅导 discourse 定量编码的质量。使用来自30场一对一数学会话的记录，我们比较了三种生成LLM（GPT、Claude、Gemini）在三种条件下的标注：未验证标注、自我验证和相互验证。输出结果与随机抽样、关注分歧的人类裁定进行对比，使用科恩κ值进行基准测试。总体而言，编排提高了κ值58%。自我验证几乎将一致性提高了近一倍，相对于未验证基线，难度较大的辅导动作的增益最大。相互验证平均提高了37%，但受配对和构建依赖性影响：一些验证者-标注者配对超过了自我验证，而其他配对则减少了对齐性，反映了验证者严格度的差异。我们贡献了：(1) 一个灵活的编排框架，实现控制、自我和交叉验证的实例化；(2) 在真正意义上的辅导数据上，跨前沿LLM与随机抽样的人类“黄金”标签进行实证比较；(3) 一种简洁的记号，验证者(标注者)（例如，Gemini(GPT) 或 Claude(Claude)），以标准化报告并明确表示方向性效果，便于复制。结果将验证定位为在学习分析中实现可靠且可扩展的LLM辅助标注的设计原则杠杆。', 'title_zh': 'AI注释 orchestration：评估LLM验证器以提高学习分析中LLM注释质量'}
{'arxiv_id': 'arXiv:2511.09710', 'title': 'Echoing: Identity Failures when LLM Agents Talk to Each Other', 'authors': 'Sarath Shekkizhar, Romain Cosentino, Adam Earle, Silvio Savarese', 'link': 'https://arxiv.org/abs/2511.09710', 'abstract': 'As large language model (LLM) based agents interact autonomously with one another, a new class of failures emerges that cannot be predicted from single agent performance: behavioral drifts in agent-agent conversations (AxA). Unlike human-agent interactions, where humans ground and steer conversations, AxA lacks such stabilizing signals, making these failures unique. We investigate one such failure, echoing, where agents abandon their assigned roles and instead mirror their conversational partners, undermining their intended objectives. Through experiments across $60$ AxA configurations, $3$ domains, and $2000+$ conversations, we demonstrate that echoing occurs across three major LLM providers, with echoing rates from $5\\%$ to $70\\%$ depending on the model and domain. Moreover, we find that echoing is persistent even in advanced reasoning models with substantial rates ($32.8\\%$) that are not reduced by increased reasoning efforts. We analyze prompt impacts, conversation dynamics, showing that echoing arises as interaction grows longer ($7+$ turns in experiments) and is not merely an artifact of sub-optimal prompting. Finally, we introduce a protocol-level mitigation in which targeted use of structured responses reduces echoing to $9\\%$.', 'abstract_zh': '基于大型语言模型的代理自主交互时出现的行为漂移现象：探索回声效应及其 mitigation 方法', 'title_zh': '回声：当LLM代理相互交流时出现的身份失败'}
{'arxiv_id': 'arXiv:2511.09682', 'title': 'Rebellion: Noise-Robust Reasoning Training for Audio Reasoning Models', 'authors': 'Tiansheng Huang, Virat Shejwalkar, Oscar Chang, Milad Nasr, Ling Liu', 'link': 'https://arxiv.org/abs/2511.09682', 'abstract': "Instilling reasoning capabilities in large models (LMs) using reasoning training (RT) significantly improves LMs' performances. Thus Audio Reasoning Models (ARMs), i.e., audio LMs that can reason, are becoming increasingly popular. However, no work has studied the safety of ARMs against jailbreak attacks that aim to elicit harmful responses from target models. To this end, first, we show that standard RT with appropriate safety reasoning data can protect ARMs from vanilla audio jailbreaks, but cannot protect them against our proposed simple yet effective jailbreaks. We show that this is because of the significant representation drift between vanilla and advanced jailbreaks which forces the target ARMs to emit harmful responses. Based on this observation, we propose Rebellion, a robust RT that trains ARMs to be robust to the worst-case representation drift. All our results are on Qwen2-Audio; they demonstrate that Rebellion: 1) can protect against advanced audio jailbreaks without compromising performance on benign tasks, and 2) significantly improves accuracy-safety trade-off over standard RT method.", 'abstract_zh': '在使用推理训练提高大型模型推理能力的基础上，音频推理模型（ARMs）的安全性亟待研究，尤其是针对旨在诱发有害响应的囚笼攻击。为此，我们首先展示了适当的推理训练结合安全推理数据可以保护ARMs免受普通的音频囚笼攻击，但无法抵御我们提出的简单而有效的囚笼攻击。这表明，由于普通和高级囚笼攻击之间显著的表示漂移，目标ARMs被迫发出有害响应。基于此观察，我们提出了一种鲁棒的推理训练方法Rebellion，旨在使ARMs对最坏情况的表示漂移具有鲁棒性。我们所有结果均基于Qwen2-Audio；实验结果显示，Rebellion：1）能够在不牺牲良性任务性能的前提下抵御高级音频囚笼攻击；2）显著改善了准确性和安全性之间的权衡，优于标准推理训练方法。', 'title_zh': '叛乱：音频推理模型的抗噪训练方法'}
{'arxiv_id': 'arXiv:2511.09575', 'title': 'Proceedings of the Second International Workshop on Next-Generation Language Models for Knowledge Representation and Reasoning (NeLaMKRR 2025)', 'authors': 'Ha-Thanh Nguyen, Ken Satoh, Francesca Toni, Randy Goebel, Kostas Stathis', 'link': 'https://arxiv.org/abs/2511.09575', 'abstract': 'Reasoning is an essential component of human intelligence in that it plays a fundamental role in our ability to think critically, support responsible decisions, and solve challenging problems. Traditionally, AI has addressed reasoning in the context of logic-based representations of knowledge. However, the recent leap forward in natural language processing, with the emergence of language models based on transformers, is hinting at the possibility that these models exhibit reasoning abilities, particularly as they grow in size and are trained on more and more data. Still, despite ongoing discussions about what reasoning is in language models, it is still not easy to articulate to what extent these models are actually capable of reasoning.\nThe goal of this workshop is to create a platform for researchers from different disciplines and/or AI perspectives to explore approaches and techniques with the aim to reconcile reasoning between language models using transformers and logic-based representations. The specific objectives include analysing the reasoning abilities of language models measured alongside KR methods, injecting KR-style reasoning abilities into language models (including by neuro-symbolic means), and formalising the kind of reasoning language models carry out. This exploration aims to uncover how language models can effectively integrate and leverage knowledge and reasoning with it, thus improving their application and utility in areas where precision and reliability are key requirements.', 'abstract_zh': '自然语言处理中基于变换器的语言模型的推理能力研究：跨学科视角的工作坊', 'title_zh': '第二届下一代语言模型在知识表示与推理国际研讨会 proceedings of the second international workshop on next-generation language models for knowledge representation and reasoning (nela mkrr 2025)'}
{'arxiv_id': 'arXiv:2511.10643', 'title': 'Black-Box On-Policy Distillation of Large Language Models', 'authors': 'Tianzhu Ye, Li Dong, Zewen Chi, Xun Wu, Shaohan Huang, Furu Wei', 'link': 'https://arxiv.org/abs/2511.10643', 'abstract': "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.", 'abstract_zh': '黑盒蒸馏通过学习专用教师模型的文本输出创建学生大规模语言模型（LLMs），而不访问其内部logits或参数。在本文中，我们引入了生成式对抗蒸馏（GAD），它使黑盒蒸馏成为可能。GAD将学生LLM视为生成器，并训练一个鉴别器以区分其响应与教师LLM的响应，形成一个最小最大游戏。鉴别器充当随学生共同进化的政策相关奖励模型，提供稳定且自适应的反馈。实验结果表明，GAD在序列级知识蒸馏的常用方法上始终表现出色。特别是，使用GAD训练的Qwen2.5-14B-Instruct（学生模型）在LMSYS-Chat自动评估中与教师模型GPT-5-Chat相当。结果证明GAD是黑盒LLM蒸馏的一种有希望且有效的方法。', 'title_zh': '黑箱基于策略的大语言模型精炼'}
{'arxiv_id': 'arXiv:2511.10628', 'title': 'Instella: Fully Open Language Models with Stellar Performance', 'authors': 'Jiang Liu, Jialian Wu, Xiaodong Yu, Yusheng Su, Prakamya Mishra, Gowtham Ramesh, Sudhanshu Ranjan, Chaitanya Manem, Ximeng Sun, Ze Wang, Pratik Prabhanjan Brahma, Zicheng Liu, Emad Barsoum', 'link': 'https://arxiv.org/abs/2511.10628', 'abstract': 'Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.', 'abstract_zh': '大型语言模型（LLMs）在各种任务上展现了卓越的性能，但大多数高性能模型仍保持封闭源代码或部分开放，限制了透明度和可重现性。在这项工作中，我们提出了一种名为Instella的全开源三亿参数语言模型家族，该模型完全基于公开可用的数据和代码库进行大规模预训练、通用指令调优以及与人类偏好对齐。尽管相比于许多同期模型使用了更少的预训练令牌，Instella在全开源模型中达到了最先进的成果，并且与同等规模的领先开源权重模型具有竞争力。此外，我们还推出了两种专门的变体：Instella-Long，能够处理高达128K令牌的上下文长度，以及通过监督微调和数学任务上的强化学习增强的推理重点模型Instella-Math。这些贡献共同将Instella确立为社区中一种透明、高性能且多功能的选择，推动了开放和可重现语言模型研究的目标。', 'title_zh': 'Instella：性能卓越的完全开源语言模型'}
{'arxiv_id': 'arXiv:2511.10621', 'title': 'SSR: Socratic Self-Refine for Large Language Model Reasoning', 'authors': 'Haizhou Shi, Ye Liu, Bo Pang, Zeyu Leo Liu, Hao Wang, Silvio Savarese, Caiming Xiong, Yingbo Zhou, Semih Yavuz', 'link': 'https://arxiv.org/abs/2511.10621', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at this https URL.', 'abstract_zh': 'Socratic Self-Refine：一种细粒度评估和精确细化大型语言模型推理的新框架', 'title_zh': 'SSR：苏格拉底式自我完善的大语言模型推理'}
{'arxiv_id': 'arXiv:2511.10618', 'title': 'Know Your Limits: Entropy Estimation Modeling for Compression and Generalization', 'authors': 'Benjamin L. Badger, Matthew Neligeorge', 'link': 'https://arxiv.org/abs/2511.10618', 'abstract': 'Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.', 'abstract_zh': '语言预测受内在信息熵的约束，存在语言模型准确度的上限和相应的语言压缩下限。当今最高效的语言压缩算法是因果（后续令牌预测）大型语言模型，但使用这些模型准确估计语言熵目前在计算上不可行。我们介绍了具有优越训练效率特性的编码器增强因果解码器模型架构，即使在使用 modest 硬件训练时，也能实现比因果变压器更高的压缩率。我们展示了如何在每个令牌级别获得熵估计，并证明了训练目标是接近但不超过训练数据熵的模型的一般泛化能力超过了试图最小化损失的模型的一般泛化能力。我们实验证明，训练目标是接近但不超过估算的每个令牌熵的因果模型具有更好的泛化能力，而未考虑熵的模型则不然。', 'title_zh': '了解你的限制：熵估计建模及其压缩与泛化应用'}
{'arxiv_id': 'arXiv:2511.10611', 'title': 'Towards an Agentic Workflow for Internet Measurement Research', 'authors': 'Alagappan Ramanathan, Eunju Kang, Dongsu Han, Sangeetha Abdu Jyothi', 'link': 'https://arxiv.org/abs/2511.10611', 'abstract': 'Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.\nWe present ArachNet, the first system demonstrating that LLM agents can independently generate measurement workflows that mimics expert reasoning. Our core insight is that measurement expertise follows predictable compositional patterns that can be systematically automated. ArachNet operates through four specialized agents that mirror expert workflow, from problem decomposition to solution implementation. We validate ArachNet with progressively challenging Internet resilience scenarios. The system independently generates workflows that match expert-level reasoning and produce analytical outputs similar to specialist solutions. Generated workflows handle complex multi-framework integration that traditionally requires days of manual coordination. ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process that experts use, enabling broader access to sophisticated measurement capabilities while maintaining the technical rigor required for research-quality analysis.', 'abstract_zh': '互联网测量研究面临 Accessibility 危机：复杂的分析需要集成多个专业工具，这需要专门的领域专业知识。当网络中断发生时，运营商需要快速的诊断工作流，覆盖基础设施映射、路由分析和依赖建模。然而，开发这些工作流需要专门的知识和大量的手工努力。\n\n我们提出了 ArachNet，这是首个展示语言模型代理可以独立生成模拟专家推理的测量工作流的系统。我们的核心见解是，测量专业知识遵循可预测的组合模式，可以系统地自动化。ArachNet 通过四个专业化代理镜像专家工作流，从问题分解到解决方案实现。我们利用逐渐具有挑战性的互联网韧性场景验证 ArachNet。系统独立生成的工作流与专家级别的推理相匹配，生成的分析输出类似于专家解决方案。生成的工作流处理传统上需要数天手动协调的复杂多框架集成。ArachNet 通过自动化专家使用的系统推理过程，降低了测量工作流组合的门槛，拓宽了高效能测量能力的访问范围，同时保持了高质量研究分析所需的技术严谨性。', 'title_zh': '面向互联网测量研究的能动工作流'}
{'arxiv_id': 'arXiv:2511.10585', 'title': 'Textual understanding boost in the WikiRace', 'authors': 'Raman Ebrahimi, Sean Fuhrman, Kendrick Nguyen, Harini Gurusankar, Massimo Franceschetti', 'link': 'https://arxiv.org/abs/2511.10585', 'abstract': 'The WikiRace game, where players navigate between Wikipedia articles using only hyperlinks, serves as a compelling benchmark for goal-directed search in complex information networks. This paper presents a systematic evaluation of navigation strategies for this task, comparing agents guided by graph-theoretic structure (betweenness centrality), semantic meaning (language model embeddings), and hybrid approaches. Through rigorous benchmarking on a large Wikipedia subgraph, we demonstrate that a purely greedy agent guided by the semantic similarity of article titles is overwhelmingly effective. This strategy, when combined with a simple loop-avoidance mechanism, achieved a perfect success rate and navigated the network with an efficiency an order of magnitude better than structural or hybrid methods. Our findings highlight the critical limitations of purely structural heuristics for goal-directed search and underscore the transformative potential of large language models to act as powerful, zero-shot semantic navigators in complex information spaces.', 'abstract_zh': 'WikiRace游戏：一种在复杂信息网络中进行目标导向搜索的基准测试', 'title_zh': 'WikiRace文本理解提升'}
{'arxiv_id': 'arXiv:2511.10519', 'title': 'Say It Differently: Linguistic Styles as Jailbreak Vectors', 'authors': 'Srikant Panda, Avinash Rai', 'link': 'https://arxiv.org/abs/2511.10519', 'abstract': 'Large Language Models (LLMs) are commonly evaluated for robustness against paraphrased or semantically equivalent jailbreak prompts, yet little attention has been paid to linguistic variation as an attack surface. In this work, we systematically study how linguistic styles such as fear or curiosity can reframe harmful intent and elicit unsafe responses from aligned models. We construct style-augmented jailbreak benchmark by transforming prompts from 3 standard datasets into 11 distinct linguistic styles using handcrafted templates and LLM-based rewrites, while preserving semantic intent. Evaluating 16 open- and close-source instruction-tuned models, we find that stylistic reframing increases jailbreak success rates by up to +57 percentage points. Styles such as fearful, curious and compassionate are most effective and contextualized rewrites outperform templated variants.\nTo mitigate this, we introduce a style neutralization preprocessing step using a secondary LLM to strip manipulative stylistic cues from user inputs, significantly reducing jailbreak success rates. Our findings reveal a systemic and scaling-resistant vulnerability overlooked in current safety pipelines.', 'abstract_zh': '大型语言模型（LLMs）通常被评估其对重述或语义等价的脱困提示的稳健性，但很少有人关注语言变异作为攻击面。在本工作中，我们系统地研究了诸如恐惧或好奇心等语言风格如何重新定位有害意图并诱使对齐模型产生不安全的响应。我们通过使用手工制作的模板和LLM基础重写将3个标准数据集的提示转换成11种不同的语言风格，同时保留语义意图，构建了风格增强的脱困基准。评估16个开源和闭源指令微调模型后，我们发现风格重新定位将脱困成功率提高了最高57个百分点。诸如恐惧、好奇和富有同情心等风格最有效，上下文化的重写版本优于模板变体。\n\n为了缓解这一问题，我们引入了一步风格中性化的预处理步骤，使用第二个LLM从用户输入中去除操控性的语言线索，显著减少了脱困的成功率。我们的发现揭示了一个系统性的、难以放大的安全漏洞，被当前的安全管道所忽视。', 'title_zh': '换个说法：语言风格作为 Jailbreak 向量'}
{'arxiv_id': 'arXiv:2511.10480', 'title': 'Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs', 'authors': 'Changhai Man, Joongun Park, Hanjiang Wu, Huan Xu, Srinivas Sridharan, Tushar Krishna', 'link': 'https://arxiv.org/abs/2511.10480', 'abstract': 'Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.', 'abstract_zh': '优化大规模语言模型（LLMs）在大规模AI训练和推理系统上的性能需要一种可扩展且表达力强的机制来模型化分布式工作负载执行。这样的建模对于预部署系统级优化（例如并行化策略）和设计空间探索是必不可少的。虽然最近的研究提出了从实际系统收集执行跟踪的方法，但访问大规模基础设施仍然是有限的，主要限制在主要云提供商。此外，现有的平台提供的跟踪数据难以适应未来更大规模系统配置的研究。我们介绍了Symbolic Tensor GrAph Generator (STAGE)，一个合成高保真执行跟踪的框架，以准确建模LLM工作负载。STAGE 支持一系列全面的并行化策略，使用户能够系统性地探索LLM架构和系统配置的广泛谱系。STAGE 通过合成跨越32K GPU的高保真LLM跟踪展示了其可扩展性，同时在计算、内存和通信层面保持张量级别的准确性。STAGE 将公开发布，以促进分布式机器学习系统的进一步研究。', 'title_zh': '通过符号张量图实现分布式LLM工作负载的可扩展合成'}
{'arxiv_id': 'arXiv:2511.10459', 'title': 'LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning', 'authors': 'Zihan Gao, Yifei Xu, Jacob Thebault-Spieker', 'link': 'https://arxiv.org/abs/2511.10459', 'abstract': "Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Gemini's accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts.", 'abstract_zh': '大规模语言模型（LLMs）在宏观地理任务上的表现已经被广泛评估，如全球事实记忆、事件总结和区域推理。然而，它们处理超本地知识的能力仍不完全清楚。随着从公民平台到社区新闻的实际应用需求增加，要求AI系统能够推理特定于街区的动态、文化叙事和地方治理。现有的基准未能捕捉到这种复杂性，往往依赖于粗粒度的数据或孤立的参考。我们提出了LocalBench，这是首个旨在系统性评估美国各县级本地知识的大规模语言模型基准。LocalBench基于Localness概念框架，包含了来自526个美国县（49个州）的14,782个验证过的问答对，这些数据源包括人口普查统计数据、本地subreddit讨论和区域新闻，涵盖了地方的物理、认知和关系维度。通过LocalBench，我们评估了13个最先进的大语言模型，在闭卷和网络增强两种设置下。我们的研究发现揭示了关键限制：即使表现最好的模型在叙事性问题上的准确率也只有56.8%，在数值推理上低于15.5%。此外，更大的模型规模和网络增强并不一定能带来更好的性能，例如，搜索可以提高Gemini的准确率13.6%，但会降低GPT系列模型的性能11.4%。这些结果强调了构建支持公平、位置感知AI系统的语言模型的迫切需要：能够与不同地理和文化背景下多样而精细的地方社区互动。', 'title_zh': 'LocalBench: 城县级本地知识与推理评估benchmark'}
{'arxiv_id': 'arXiv:2511.10453', 'title': 'Reasoning About Intent for Ambiguous Requests', 'authors': 'Irina Saparina, Mirella Lapata', 'link': 'https://arxiv.org/abs/2511.10453', 'abstract': 'Large language models often respond to ambiguous requests by implicitly committing to one interpretation. Intent misunderstandings can frustrate users and create safety risks. To address this, we propose generating multiple interpretation-answer pairs in a single structured response to ambiguous requests. Our models are trained with reinforcement learning and customized reward functions using multiple valid answers as supervision. Experiments on conversational question answering and semantic parsing demonstrate that our method achieves higher coverage of valid answers than baseline approaches. Human evaluation confirms that predicted interpretations are highly aligned with their answers. Our approach promotes transparency with explicit interpretations, achieves efficiency by requiring only one generation step, and supports downstream applications through its structured output format.', 'abstract_zh': '大型语言模型经常通过隐含承诺一种解释来回应模棱两可的请求。意图误解可能会使用户感到沮丧并创建安全风险。为了解决这个问题，我们提出了一次生成模棱两可请求的多种解释-答案对的方法。我们的模型使用强化学习和定制的奖励函数进行训练，其中多个有效答案作为监督信息。在对话式问答和语义解析的实验中，我们的方法在覆盖有效答案方面优于基线方法。人类评估证实，预测的解释与答案高度一致。我们的方法通过明确的解释促进透明性，通过仅需要一次生成步骤实现高效性，并通过其结构化输出格式支持下游应用。', 'title_zh': '关于模糊请求的意图推理'}
{'arxiv_id': 'arXiv:2511.10400', 'title': 'Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance', 'authors': 'Lifan Zheng, Jiawei Chen, Qinghong Yin, Jingyuan Zhang, Xinyi Zeng, Yu Tian', 'link': 'https://arxiv.org/abs/2511.10400', 'abstract': 'Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks.', 'abstract_zh': '确保代理架构的可靠性并在故障发生时有效识别问题代理是多代理系统（MAS）中的关键挑战。大规模语言模型（LLMs）的进步已将LLM基础代理确立为主要的MAS分支，使其在复杂问题解决和世界建模方面取得了重大突破。然而，这一转变对可靠性的影响仍主要未被探索，即用LLM基础代理替代传统代理是否能有效提高MAS的可靠性。在本文中，我们从拜占庭容错的视角调查并量化了LLM基础代理的可靠性。我们观察到，LLM基础代理在处理错误消息流时表现出更强的怀疑性，这一特性使其能够跨不同拓扑结构超越传统代理。受试点实验结果的启发，我们设计了基于置信度探针的加权拜占庭容错（CP-WBFT）共识机制，以增强不同拓扑结构下的MAS稳定性。该机制利用LLMs固有的反射性和辨别性能力，通过基于探针的加权信息流传输方法提高LLM基础代理的可靠性。广泛的实验表明，CP-WBFT在极端拜占庭条件下（85.7%的故障率）实现了跨不同网络拓扑的优异性能。值得注意的是，我们的方法在各种拓扑上实现了显着的准确性，并在数学推理和安全性评估任务中保持了强大的可靠性。', 'title_zh': '重思多代理系统可靠性：拜占庭容错视角'}
{'arxiv_id': 'arXiv:2511.10384', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'authors': 'Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat', 'link': 'https://arxiv.org/abs/2511.10384', 'abstract': 'Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems.', 'abstract_zh': '社交媒体上的错误信息依靠惊喜、情感和身份驱动的推理生存并扩散，往往通过人类认知偏见被放大。为了研究这些机制，我们将大型语言模型（LLM）人格化为模拟用户级偏见、意识形态一致性和信任直觉的合成代理。在此框架内，我们引入了一个审查者-节点框架，以模拟和分析错误信息如何在网络中传播，并研究其演变过程。新闻文章在网络中的_persona-条件化(LLM)节点之间传播，每个节点都会重写接收到的内容。基于问答的审查者在每个步骤中测量事实准确性，提供可解释的、声明级别的错误信息漂移跟踪。我们正式定义了一个错误信息指数和一个错误信息传播速率，以量化最多30轮重写中同质性和异质性分支上的事实退化程度。在10个领域中的21个persona的实验中揭示，身份和意识形态驱动的persona特别加速错误信息传播，尤其是在政治、营销和技术领域。相比之下，专家驱动的persona保持了事实稳定性。受控的随机分支模拟进一步表明，一旦早期失真出现，异质persona的互动会迅速将错误信息放大到宣传级失真。我们对错误信息严重性的分类——从事实错误到谎言再到宣传——将观察到的漂移与错误信息研究中已有的理论联系起来。这些发现展示了LLM作为人类偏见的代理和能够追踪信息准确度的审计员的双重角色。所提出的方法提供了一种可解释且基于实证的方法，用于研究、模拟和缓解数字生态系统中的错误信息扩散。', 'title_zh': '使用大型语言模型模拟虚假信息在社交网络中的传播'}
{'arxiv_id': 'arXiv:2511.10338', 'title': 'BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages', 'authors': 'Guduru Manoj, Neel Prabhanjan Rachamalla, Ashish Kulkarni, Gautam Rajeev, Jay Piplodiya, Arul Menezes, Shaharukh Khan, Souvik Rana, Manya Sah, Chandra Khatri, Shubham Agarwal', 'link': 'https://arxiv.org/abs/2511.10338', 'abstract': 'In the context of pretraining of Large Language Models (LLMs), synthetic data has emerged as an alternative for generating high-quality pretraining data at scale. This is particularly beneficial in low-resource language settings where the benefits of recent LLMs have been unevenly distributed across languages. In this work, we present a systematic study on the generation and evaluation of synthetic multilingual pretraining data for Indic languages, where we construct a large-scale synthetic dataset BhashaKritika, comprising 540B tokens using 5 different techniques for 10 languages. We explore the impact of grounding generation in documents, personas, and topics. We analyze how language choice, both in the prompt instructions and document grounding, affects data quality, and we compare translations of English content with native generation in Indic languages. To support scalable and language-sensitive evaluation, we introduce a modular quality evaluation pipeline that integrates script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity-based filtering using KenLM models. Our framework enables robust quality control across diverse scripts and linguistic contexts. Empirical results through model runs reveal key trade-offs in generation strategies and highlight best practices for constructing effective multilingual corpora.', 'abstract_zh': '在大规模语言模型（LLMs）预训练的背景下，合成数据已成为生成高质量大规模预训练数据的替代方法。这在语言资源有限的语言环境中尤为有益，在这些环境中，近年来的LLMs的益处并未均匀分布在各种语言上。在本工作中，我们系统地研究了为印地语编纂合成多语言预训练数据的生成与评估，构建了一个包含540亿个词元、涵盖10种语言的大规模合成数据集BhashaKritika，使用了5种不同的生成技术。我们探索了基于文档、人设和话题的生成对数据质量的影响，并分析了语言选择（包括指令和文档接地中的语言选择）如何影响数据质量，同时比较了英语内容的翻译与印地语中的原生生成。为了支持可扩展且语言敏感的评估，我们引入了一个模块化的质量评估管道，该管道集成了脚本和语言检测、元数据一致性检查、n-克隆重复分析以及基于KenLM模型的困惑度过滤。我们的框架能够实现对多样化脚本和语言环境的稳健质量控制。通过模型运行的实验证据揭示了生成策略中的关键权衡，并强调了构建有效多语言语料库的最佳做法。', 'title_zh': 'BhashaKritika: 构建大规模合成预训练数据集用于印度语言'}
{'arxiv_id': 'arXiv:2511.10301', 'title': 'Rethinking Visual Information Processing in Multimodal LLMs', 'authors': 'Dongwan Kim, Viresh Ranjan, Takashi Nagata, Arnab Dhua, Amit Kumar K C', 'link': 'https://arxiv.org/abs/2511.10301', 'abstract': 'Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling.', 'abstract_zh': '尽管LLaVA架构在视觉语言任务上取得了杰出的成果，但由于文本和视觉模态之间的固有不匹配，其设计本质上难以有效集成视觉特征。我们从一个新的视角出发，使LLM不仅作为语言模型，同时也是强大的视觉编码器。为此，我们提出了LLaViT - 大型语言模型作为扩展的视觉变换器，通过三种关键修改使LLM同时作为视觉编码器发挥作用：(1) 为视觉模态学习独立的QKV投影，(2) 允许在视觉标记之间进行双向注意，(3) 结合全局和局部视觉表示。通过在多种大型语言模型上的广泛受控实验，我们证明LLaViT 在多个基准测试上显著优于基线LLaVA方法，甚至超过了参数量是其两倍的模型，确立了更有效的视觉语言建模方法。', 'title_zh': '重新思考多模态LLM中的视觉信息处理'}
{'arxiv_id': 'arXiv:2511.10292', 'title': 'Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models', 'authors': 'Zhengtao Zou, Ya Gao, Jiarui Guan, Bin Li, Pekka Marttinen', 'link': 'https://arxiv.org/abs/2511.10292', 'abstract': "Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments. In this work, we aim to address this trade-off with Residual-Update Directed DEcoding Regulation (RUDDER), a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a single, standard forward pass. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context. Extensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency.", 'abstract_zh': '残差更新导向的解码调节（RUDDER）：一种低开销的视觉 grounding 方法以提高大视觉-语言模型的可靠性', 'title_zh': '低开销幻觉缓解中自适应残差更新导向增强在大型视觉语言模型中的应用'}
{'arxiv_id': 'arXiv:2511.10271', 'title': 'Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics', 'authors': 'Xin Sun, Daniel Ståhl, Kristian Sandahl, Christoph Kessler', 'link': 'https://arxiv.org/abs/2511.10271', 'abstract': 'In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.', 'abstract_zh': '近年来，大型语言模型在软件工程工作流中得到了广泛应用，支持代码生成等任务。然而，尽管这些模型通常生成功能正确的输出，我们仍然缺乏对其非功能性质量的系统理解和评估。现有研究主要关注生成的代码是否通过测试，而不是是否通过高质量的测试。受ISO/IEC 25010质量模型的启发，本研究进行了三项互补的研究：系统性回顾了108篇论文，组织了两次行业研讨会，并使用三种大型语言模型对真实软件问题进行了实证分析。基于文献和实践者的洞见，实证研究考察了生成补丁在安全性、可维护性和性能效率方面的质量。文献中发现，安全性和性能效率是学术界关注的重点，而可维护性和其他质量则研究较少。相比之下，行业专家更注重可维护性和可读性，警告生成的代码可能会加速技术债务的积累。在对三种大型语言模型生成的功能正确补丁的评估中，某一质量维度的改进往往以牺牲其他维度为代价。运行时和内存结果进一步显示出模型和优化策略之间高度的变异性。总体而言，我们的研究发现学术关注、行业优先事项和模型性能之间存在差距，突显出迫切需要将质量保证机制集成到大型语言模型代码生成管道中，以确保未来生成的代码不仅通过测试，而且真正达到了高质量的标准。', 'title_zh': 'LLM生成代码的质量保证：address非功能性质量特性'}
{'arxiv_id': 'arXiv:2511.10262', 'title': 'MTR-DuplexBench: Towards a Comprehensive Evaluation of Multi-Round Conversations for Full-Duplex Speech Language Models', 'authors': 'He Zhang, Wenqian Cui, Haoning Xu, Xiaohui Li, Lei Zhu, Shaohua Ma, Irwin King', 'link': 'https://arxiv.org/abs/2511.10262', 'abstract': 'Full-Duplex Speech Language Models (FD-SLMs) enable real-time, overlapping conversational interactions, offering a more dynamic user experience compared to traditional half-duplex models. However, existing benchmarks primarily focus on evaluating single-round interactions and conversational features, neglecting the complexities of multi-round communication and critical capabilities such as instruction following and safety. Evaluating FD-SLMs in multi-round settings poses significant challenges, including blurred turn boundaries in communication and context inconsistency during model inference. To address these gaps, we introduce MTR-DuplexBench, a novel benchmark that segments continuous full-duplex dialogues into discrete turns, enabling comprehensive, turn-by-turn evaluation of FD-SLMs across dialogue quality, conversational dynamics, instruction following, and safety. Experimental results reveal that current FD-SLMs face difficulties in maintaining consistent performance across multiple rounds and evaluation dimensions, highlighting the necessity and effectiveness of our proposed benchmark. The benchmark and code will be available in the future.', 'abstract_zh': '全双工语音语言模型（FD-SLMs）使实时、重叠的对话交互成为可能，提供了与传统半双工模型相比更为动态的用户体验。然而，现有的基准主要侧重于评估单轮对话和对话特征，忽略了多轮通信的复杂性以及指令遵循和安全性等关键能力。在多轮设置中评估FD-SLMs面临显著挑战，包括通信中的界线模糊以及模型推理中的上下文不一致性。为解决这些问题，我们引入了MTR-DuplexBench，这是一种新型基准，将连续的全双工对话分割为离散的回合，使得能够从对话质量、对话动态性、指令遵循和安全性等方面进行全面、逐回合的评价。实验结果表明，当前的FD-SLMs在多轮对话和评估维度上保持一致性能方面存在问题，突显了我们所提出的基准的必要性和有效性。该基准及其代码将在未来提供。', 'title_zh': 'MTR-DuplexBench: 朝向全面评估全双工对话多轮对话的模型'}
{'arxiv_id': 'arXiv:2511.10234', 'title': 'Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners', 'authors': 'Daniel Herbst, Lea Karbeska, Divyanshu Kumar, Akanksha Ahuja, Fatemeh Gholamzadeh Nasrabadi, Fabrizio Frasca', 'link': 'https://arxiv.org/abs/2511.10234', 'abstract': 'While promising, graph reasoners based on Large Language Models (LLMs) lack built-in invariance to symmetries in graph representations. Operating on sequential graph serializations, LLMs can produce different outputs under node reindexing, edge reordering, or formatting changes, raising robustness concerns. We systematically analyze these effects, studying how fine-tuning impacts encoding sensitivity as well generalization on unseen tasks. We propose a principled decomposition of graph serializations into node labeling, edge encoding, and syntax, and evaluate LLM robustness to variations of each of these factors on a comprehensive benchmarking suite. We also contribute a novel set of spectral tasks to further assess generalization abilities of fine-tuned reasoners. Results show that larger (non-fine-tuned) models are more robust. Fine-tuning reduces sensitivity to node relabeling but may increase it to variations in structure and format, while it does not consistently improve performance on unseen tasks.', 'abstract_zh': '基于大型语言模型的图推理器具有潜力但缺乏对图表示中对称性的内置不变性。在对节点重新索引、边重新排序或格式更改进行操作时，大型语言模型可以产生不同的输出，这引发了稳健性方面的关切。我们系统地分析了这些影响，研究了微调对编码敏感性及在未见任务上的泛化能力的影响。我们提出了一个基于原理的图序列分解，将其分解为节点标签、边编码和语法，并在全面的基准测试套件上评估大型语言模型对每个因素的鲁棒性。我们还贡献了一组新的谱任务，以进一步评估微调推理器的泛化能力。结果显示，较大的（未微调）模型更具鲁棒性。微调可以减少对节点重新标记的敏感性，但可能会增加对结构和格式变化的敏感性，而微调并不一致地提高在未见任务上的性能。', 'title_zh': '迷失在序列化中：LLM图推理器的不变性与泛化'}
{'arxiv_id': 'arXiv:2511.10222', 'title': 'Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard', 'authors': 'Yudong Yang, Xuezhen Zhang, Zhifeng Han, Siyin Wang, Jimin Zhuang, Zengrui Jin, Jing Shao, Guangzhi Sun, Chao Zhang', 'link': 'https://arxiv.org/abs/2511.10222', 'abstract': 'Recent progress in large language models (LLMs) has enabled understanding of both speech and non-speech audio, but exposing new safety risks emerging from complex audio inputs that are inadequately handled by current safeguards. We introduce SACRED-Bench (Speech-Audio Composition for RED-teaming) to evaluate the robustness of LLMs under complex audio-based attacks. Unlike existing perturbation-based methods that rely on noise optimization or white-box access, SACRED-Bench exploits speech-audio composition mechanisms. SACRED-Bench adopts three mechanisms: (a) speech overlap and multi-speaker dialogue, which embeds harmful prompts beneath or alongside benign speech; (b) speech-audio mixture, which imply unsafe intent via non-speech audio alongside benign speech or audio; and (c) diverse spoken instruction formats (open-ended QA, yes/no) that evade text-only filters. Experiments show that, even Gemini 2.5 Pro, the state-of-the-art proprietary LLM, still exhibits 66% attack success rate in SACRED-Bench test set, exposing vulnerabilities under cross-modal, speech-audio composition attacks. To bridge this gap, we propose SALMONN-Guard, a safeguard LLM that jointly inspects speech, audio, and text for safety judgments, reducing attack success down to 20%. Our results highlight the need for audio-aware defenses for the safety of multimodal LLMs. The benchmark and SALMONN-Guard checkpoints can be found at this https URL. Warning: this paper includes examples that may be offensive or harmful.', 'abstract_zh': '近期大规模语言模型的发展使得对语音和非语音音频的理解成为可能，但复杂的音频输入引发了新的安全风险，而当前的防护措施对此处理不足。为了评估大规模语言模型在复杂音频攻击下的鲁棒性，我们提出了SACRED-Bench（Speech-Audio Composition for RED-teaming）评测基准。不同于现有依赖噪声优化或白盒访问的扰动方法，SACRED-Bench 利用语音音频合成机制。SACRED-Bench 采用三种机制：（a）语音重叠和多说话人对话，将有害提示嵌入或附在良性语音之下；（b）语音音频混合，通过非语音音频与良性语音或音频一同暗示不安全意图；（c）多样的口头指令格式（开放式问答、是/否），以规避纯文本过滤器。实验结果显示，即使是目前最先进的专有语言模型Gemini 2.5 Pro，在SACRED-Bench 测试集中仍然有66%的攻击成功率，暴露了跨模态、语音音频合成攻击下的漏洞。为弥补这一差距，我们提出了SALMONN-Guard，这是一个联合检查语音、音频和文本的安全防护语言模型，可以将攻击成功率降低到20%。我们的结果强调了为多模态语言模型的安全性建立音频感知防御的必要性。SACRED-Bench 和 SALMONN-Guard 的检查点可以在以下链接找到：this https URL。警告：本论文包含可能具有冒犯性或有害性的示例。', 'title_zh': '面向多模态LLM的语音-音频合成攻击及其通过SALMONN-Guard的缓解'}
{'arxiv_id': 'arXiv:2511.10093', 'title': 'On the Military Applications of Large Language Models', 'authors': 'Satu Johansson, Taneli Riihonen', 'link': 'https://arxiv.org/abs/2511.10093', 'abstract': 'In this paper, military use cases or applications and implementation thereof are considered for natural language processing and large language models, which have broken into fame with the invention of the generative pre-trained transformer (GPT) and the extensive foundation model pretraining done by OpenAI for ChatGPT and others. First, we interrogate a GPT-based language model (viz. Microsoft Copilot) to make it reveal its own knowledge about their potential military applications and then critically assess the information. Second, we study how commercial cloud services (viz. Microsoft Azure) could be used readily to build such applications and assess which of them are feasible. We conclude that the summarization and generative properties of language models directly facilitate many applications at large and other features may find particular uses.', 'abstract_zh': '本文探讨了自然语言处理和大型语言模型在军事领域的应用及其实现，这些模型随着生成预训练变换器（GPT）的发明和OpenAI等机构对ChatGPT等模型的大规模预训练而广受欢迎。首先，我们询问基于GPT的语言模型（例如Microsoft Copilot）以揭示其潜在的军事应用知识，并对其进行批判性评估。其次，我们研究了商业云服务（例如Microsoft Azure）如何被方便地用于构建此类应用，并评估哪些应用是可行的。我们得出结论，语言模型的总结和生成特性直接促进了多种应用的发展，而其他特性可能有特定用途。', 'title_zh': '大型语言模型在军事应用中的研究'}
{'arxiv_id': 'arXiv:2511.10014', 'title': 'fastbmRAG: A Fast Graph-Based RAG Framework for Efficient Processing of Large-Scale Biomedical Literature', 'authors': 'Guofeng Meng, Li Shen, Qiuyan Zhong, Wei Wang, Haizhou Zhang, Xiaozhen Wang', 'link': 'https://arxiv.org/abs/2511.10014', 'abstract': 'Large language models (LLMs) are rapidly transforming various domains, including biomedicine and healthcare, and demonstrate remarkable potential from scientific research to new drug discovery. Graph-based retrieval-augmented generation (RAG) systems, as a useful application of LLMs, can improve contextual reasoning through structured entity and relationship identification from long-context knowledge, e.g. biomedical literature. Even though many advantages over naive RAGs, most of graph-based RAGs are computationally intensive, which limits their application to large-scale dataset. To address this issue, we introduce fastbmRAG, an fast graph-based RAG optimized for biomedical literature. Utilizing well organized structure of biomedical papers, fastbmRAG divides the construction of knowledge graph into two stages, first drafting graphs using abstracts; and second, refining them using main texts guided by vector-based entity linking, which minimizes redundancy and computational load. Our evaluations demonstrate that fastbmRAG is over 10x faster than existing graph-RAG tools and achieve superior coverage and accuracy to input knowledge. FastbmRAG provides a fast solution for quickly understanding, summarizing, and answering questions about biomedical literature on a large scale. FastbmRAG is public available in this https URL.', 'abstract_zh': '大型语言模型（LLMs）正在迅速 transforming 各个领域，包括生物医学和医疗保健，并在从科学研究到新药发现的多个方面展现出显著潜力。基于图的检索增强生成（RAG）系统作为一种有用的大型语言模型应用，可以通过结构化实体和关系识别从长文知识中提高上下文推理能力，例如生物医学文献。尽管与朴素的RAGs相比有许多优势，大多数基于图的RAGs计算密集型较强，限制了其在大规模数据集上的应用。为解决这一问题，我们介绍了FastbmRAG，这是一种针对生物医学文献优化的快速基于图的RAG。利用生物医学论文的良好组织结构，FastbmRAG将知识图的构建分为两个阶段：首先使用摘要草图；然后使用向量引导的实体链接指导的主要文本进行细化，从而减少冗余和计算负载。我们的评估表明，FastbmRAG比现有图RAG工具快10倍以上，并且在知识覆盖率和准确性方面优于输入知识。FastbmRAG提供了一种快速解决方案，可以用于大规模快速理解、总结和回答关于生物医学文献的问题。FastbmRAG已在以下网址公开：this https URL。', 'title_zh': 'fastbmRAG：一种高效的图基RAG框架，用于大规模生物医学文献处理'}
{'arxiv_id': 'arXiv:2511.10002', 'title': 'PustakAI: Curriculum-Aligned and Interactive Textbooks Using Large Language Models', 'authors': 'Shivam Sharma, Riya Naik, Tejas Gawas, Heramb Patil, Kunal Korgaonkar', 'link': 'https://arxiv.org/abs/2511.10002', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like content. This has revolutionized various sectors such as healthcare, software development, and education. In education, LLMs offer potential for personalized and interactive learning experiences, especially in regions with limited teaching resources. However, adapting these models effectively to curriculum-specific content, such as the National Council of Educational Research and Training (NCERT) syllabus in India, presents unique challenges in terms of accuracy, alignment, and pedagogical relevance. In this paper, we present the framework "PustakAI"\\footnote{Pustak means `book\' in many Indian languages.} for the design and evaluation of a novel question-answering dataset "NCERT-QA" aligned with the NCERT curriculum for English and Science subjects of grades 6 to 8. We classify the curated QA pairs as Factoid, Inferential, and Others (evaluative and reasoning). We evaluate the dataset with various prompting techniques, such as meta-prompt, few-shot, and CoT-style prompting, using diverse evaluation metrics to understand which approach aligns more efficiently with the structure and demands of the curriculum. Along with the usability of the dataset, we analyze the strengths and limitations of current open-source LLMs (Gemma3:1b, Llama3.2:3b, and Nemotron-mini:4b) and high-end LLMs (Llama-4-Scout-17B and Deepseek-r1-70B) as AI-based learning tools in formal education systems.', 'abstract_zh': '大规模语言模型（LLMs）展示了令人瞩目的能力，能够理解并生成类似人类的内容。这已经革新了医疗保健、软件开发和教育等多个领域。在教育领域，LLMs 为个性化和互动式学习体验提供了潜在的可能性，尤其是在教学资源有限的地区。然而，将这些模型有效适应特定课程内容，如印度国家教育研究与培训委员会（NCERT）的课程标准，涉及到准确性、对齐和教育相关性等方面的独特挑战。本文介绍了基于PustakAI框架设计和评估与6至8年级英语和科学科目NCERT课程标准对齐的新型问答数据集“NCERT-QA”的方案。“Pustak”在许多印度语言中意为“书”。我们对整理后的问答对进行了事实性、推断性和其他类别的分类（评价性和推理性）。我们使用多种提示技术（元提示、少样本提示和逐步推理提示）和多种评估指标评估该数据集，以了解哪种方法能更有效地与课程结构和需求相匹配。除了数据集的使用情况，我们还分析了当前开源（Gemma3:1b、Llama3.2:3b 和 Nemotron-mini:4b）和高性能（Llama-4-Scout-17B 和 Deepseek-r1-70B）LLM 作为正式教育系统中的基于人工智能的教育工具的优势和局限性。', 'title_zh': 'PustakAI: 课程对齐且互动的大语言模型教材'}
{'arxiv_id': 'arXiv:2511.09969', 'title': 'Owlgorithm: Supporting Self-Regulated Learning in Competitive Programming through LLM-Driven Reflection', 'authors': 'Juliana Nieto-Cardenas, Erin Joy Kramer, Peter Kurto, Ethan Dickey, Andres Bejarano', 'link': 'https://arxiv.org/abs/2511.09969', 'abstract': "We present Owlgorithm, an educational platform that supports Self-Regulated Learning (SRL) in competitive programming (CP) through AI-generated reflective questions. Leveraging GPT-4o, Owlgorithm produces context-aware, metacognitive prompts tailored to individual student submissions. Integrated into a second- and third-year CP course, the system-provided reflective prompts adapted to student outcomes: guiding deeper conceptual insight for correct solutions and structured debugging for partial or failed ones.\nOur exploratory assessment of student ratings and TA feedback revealed both promising benefits and notable limitations. While many found the generated questions useful for reflection and debugging, concerns were raised about feedback accuracy and classroom usability. These results suggest advantages of LLM-supported reflection for novice programmers, though refinements are needed to ensure reliability and pedagogical value for advanced learners.\nFrom our experience, several key insights emerged: GenAI can effectively support structured reflection, but careful prompt design, dynamic adaptation, and usability improvements are critical to realizing their potential in education. We offer specific recommendations for educators using similar tools and outline next steps to enhance Owlgorithm's educational impact. The underlying framework may also generalize to other reflective learning contexts.", 'abstract_zh': '我们介绍Owlgorithm，一个通过AI生成反思性问题支持编程竞赛中自我调节学习（SRL）的教育平台。利用GPT-4o，Owlgorithm生成与学生个体提交内容相关的元认知提示。该系统嵌入到第二年和第三年的编程竞赛课程中，提供的反思性提示根据学生成果进行适应：引导对正确解决方案的深入概念理解，并对不完整或失败的解决方案进行结构化的调试。我们的探索性评估显示了学生评分和助教反馈中的潜在益处和明显局限。虽然许多学生发现生成的问题有助于反思和调试，但也有关于反馈准确性和课堂适用性的关切。这些结果表明，对于初级编程者来说，基于自然语言处理的支持反射性学习具有优势，尽管仍需改进以确保高级学习者的可靠性和教学价值。从我们的经验中，几个关键洞察浮现：生成式AI可以有效支持结构化的反思，但精心设计的提示、动态适应性和用户体验的改进是其在教育中充分发挥潜力的关键。我们针对使用类似工具的教育者提供具体建议，并概述了进一步提升Owlgorithm教育影响的下一步。该底层框架也可能适用于其他反思性学习情境。', 'title_zh': 'Owlgorithm: 通过LLM驱动的反思支持编程竞赛中的自我调节学习'}
{'arxiv_id': 'arXiv:2511.09947', 'title': 'EEGAgent: A Unified Framework for Automated EEG Analysis Using Large Language Models', 'authors': 'Sha Zhao, Mingyi Peng, Haiteng Jiang, Tao Li, Shijian Li, Gang Pan', 'link': 'https://arxiv.org/abs/2511.09947', 'abstract': 'Scalable and generalizable analysis of brain activity is essential for advancing both clinical diagnostics and cognitive research. Electroencephalography (EEG), a non-invasive modality with high temporal resolution, has been widely used for brain states analysis. However, most existing EEG models are usually tailored for individual specific tasks, limiting their utility in realistic scenarios where EEG analysis often involves multi-task and continuous reasoning. In this work, we introduce EEGAgent, a general-purpose framework that leverages large language models (LLMs) to schedule and plan multiple tools to automatically complete EEG-related tasks. EEGAgent is capable of performing the key functions: EEG basic information perception, spatiotemporal EEG exploration, EEG event detection, interaction with users, and EEG report generation. To realize these capabilities, we design a toolbox composed of different tools for EEG preprocessing, feature extraction, event detection, etc. These capabilities were evaluated on public datasets, and our EEGAgent can support flexible and interpretable EEG analysis, highlighting its potential for real-world clinical applications.', 'abstract_zh': '可扩展且泛化的脑活动分析对于推动临床诊断和认知研究都至关重要。基于高时间分辨率的无创模态电生理脑成像（EEG）广泛用于脑状态分析。然而，现有大多数EEG模型通常针对特定任务量身定制，限制了它们在通常涉及多任务和连续推理的现实EEG分析场景中的应用。在本文中，我们介绍了一种通用框架EEGAgent，该框架利用大型语言模型（LLMs）调度和规划多种工具，以自动完成相关EEG任务。EEGAgent能够执行关键功能：EEG基本信息感知、时空EEG探索、EEG事件检测、与用户交互以及EEG报告生成。为了实现这些功能，我们设计了一个工具箱，其中包括用于EEG预处理、特征提取、事件检测等多种工具。这些功能在公共数据集上进行了评估，EEGAgent可以支持灵活且可解释的EEG分析，突显了其在实际临床应用中的潜力。', 'title_zh': 'EEGAgent：一种基于大型语言模型的统一自动EEG分析框架'}
{'arxiv_id': 'arXiv:2511.09879', 'title': 'Taught by the Flawed: How Dataset Insecurity Breeds Vulnerable AI Code', 'authors': 'Catherine Xia, Manar H. Alalfi', 'link': 'https://arxiv.org/abs/2511.09879', 'abstract': 'AI programming assistants have demonstrated a tendency to generate code containing basic security vulnerabilities. While developers are ultimately responsible for validating and reviewing such outputs, improving the inherent quality of these generated code snippets remains essential. A key contributing factor to insecure outputs is the presence of vulnerabilities in the training datasets used to build large language models (LLMs). To address this issue, we propose curating training data to include only code that is free from detectable vulnerabilities. In this study, we constructed a secure dataset by filtering an existing Python corpus using a static analysis tool to retain only vulnerability-free functions. We then trained two transformer-based models: one on the curated dataset and one on the original, unfiltered dataset. The models were evaluated on both the correctness and security of the code they generated in response to natural language function descriptions. Our results show that the model trained on the curated dataset produced outputs with fewer security issues, while maintaining comparable functional correctness. These findings highlight the importance of secure training data in improving the reliability of AI-based programming assistants, though further enhancements to model architecture and evaluation are needed to reinforce these outcomes.', 'abstract_zh': 'AI编程助手生成的安全性缺陷代码趋势及其改进策略：通过筛选无漏洞训练数据构建安全模型', 'title_zh': '由缺陷所教：数据集不安全性如何培育出脆弱的AI代码'}
{'arxiv_id': 'arXiv:2511.09820', 'title': 'From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance', 'authors': 'Jeongho Min, Dongyoung Kim, Jaehyup Lee', 'link': 'https://arxiv.org/abs/2511.09820', 'abstract': 'Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments. However, existing approaches often require supervised training on curated datasets and rely on panoramic or UAV-based images, which limits real-world deployment. In this paper, we present a simple yet effective cross-view image retrieval framework that leverages a pretrained vision encoder and a large language model (LLM), requiring no additional training. Given a monocular street-view image, our method extracts geographic cues through web-based image search and LLM-based location inference, generates a satellite query via geocoding API, and retrieves matching tiles using a pretrained vision encoder (e.g., DINOv2) with PCA-based whitening feature refinement. Despite using no ground-truth supervision or finetuning, our proposed method outperforms prior learning-based approaches on the benchmark dataset under zero-shot settings. Moreover, our pipeline enables automatic construction of semantically aligned street-to-satellite datasets, which is offering a scalable and cost-efficient alternative to manual annotation. All source codes will be made publicly available at this https URL.', 'abstract_zh': '跨视角图像检索，特别是街道到卫星图像匹配，是自主导航、城市规划和GPS受限环境中定位的重要任务。然而，现有方法通常需要在精心制作的数据集上进行监督训练，并依赖全景或无人机图像，这限制了其实用性。本文提出了一种简单有效的跨视角图像检索框架，该框架利用预训练的视觉编码器和大型语言模型（LLM），无需额外训练。给定一张单目街道视角图像，我们的方法通过基于网络的图像搜索和LLM基于的位置推断提取地理线索，通过地理编码API生成卫星查询，并使用预训练的视觉编码器（如DINOv2）和PCA基于的 whitening 特征细化检索匹配的图像块。尽管没有使用 ground-truth 监督或微调，我们在零样本设置下的基准数据集上仍优于先前的学习基方法。此外，我们的流水线实现了自动构建语义对齐的街道到卫星图像数据集，提供了比手动标注更具可扩展性和成本效益的替代方案。所有源代码将在该网址公开发布。', 'title_zh': '从街道到轨道：基于位置语义和LLM引导的无训练跨视图检索'}
{'arxiv_id': 'arXiv:2511.09748', 'title': 'How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation', 'authors': 'Muskaan Chopra, Lorenz Sparrenberg, Sarthak Khanna, Rafet Sifa', 'link': 'https://arxiv.org/abs/2511.09748', 'abstract': 'Large Language Models (LLMs) excel at evaluating machine translation (MT), but their scale and cost hinder deployment on edge devices and in privacy-sensitive workflows. We ask: how small can you get while still detecting meaning-altering translation errors? Focusing on English->German Critical Error Detection (CED), we benchmark sub-2B models (LFM2-350M, Qwen-3-0.6B/1.7B, Llama-3.2-1B-Instruct, Gemma-3-1B) across WMT21, WMT22, and SynCED-EnDe-2025. Our framework standardizes prompts, applies lightweight logit-bias calibration and majority voting, and reports both semantic quality (MCC, F1-ERR/F1-NOT) and compute metrics (VRAM, latency, throughput). Results reveal a clear sweet spot around one billion parameters: Gemma-3-1B provides the best quality-efficiency trade-off, reaching MCC=0.77 with F1-ERR=0.98 on SynCED-EnDe-2025 after merged-weights fine-tuning, while maintaining 400 ms single-sample latency on a MacBook Pro M4 Pro (24 GB). At larger scale, Qwen-3-1.7B attains the highest absolute MCC (+0.11 over Gemma) but with higher compute cost. In contrast, ultra-small models (0.6B) remain usable with few-shot calibration yet under-detect entity and number errors. Overall, compact, instruction-tuned LLMs augmented with lightweight calibration and small-sample supervision can deliver trustworthy, on-device CED for MT, enabling private, low-cost error screening in real-world translation pipelines. All datasets, prompts, and scripts are publicly available at our GitHub repository.', 'abstract_zh': '大型语言模型在评估机器翻译方面表现出色，但其规模和成本限制了其在边缘设备和隐私敏感工作流中的部署。我们询问：在仍然检测到语义改变的翻译错误时，可以多小？我们专注于从英语到德语的关键错误检测（CED），在WMT21、WMT22和SynCED-EnDe-2025上对子2B模型（LFM2-350M、Qwen-3-0.6B/1.7B、Llama-3.2-1B-Instruct、Gemma-3-1B）进行了基准测试。我们的框架标准化了提示，应用了轻量级logit偏置校准和多数投票，并报告了语义质量（MCC、F1-ERR/F1-NOT）和计算指标（VRAM、延迟、吞吐量）。结果表明，参数量大约为一亿是一个明显的甜蜜点：Gemma-3-1B在SynCED-EnDe-2025上经过合并权重微调后提供了最佳的质量-效率平衡，MCC为0.77，F1-ERR为0.98，在MacBook Pro M4 Pro（24 GB）上单样本延迟为400毫秒。在更大规模下，Qwen-3-1.7B达到最高的绝对MCC（比Gemma高0.11），但计算成本较高。相比之下，超小型模型（0.6B）在少量校准后仍然可用，但在实体和数量错误的检测上不足。总的来说，配备轻量级校准和少量样本监督的紧凑型指令调优的大语言模型可以在设备上提供可信赖的CED，有助于实现真实世界翻译流水线中的私有、低成本错误筛查。所有数据集、提示和脚本均已在我们的GitHub仓库中公开。', 'title_zh': '能小到什么程度？机器翻译设备端关键错误检测的紧凑语言模型'}
{'arxiv_id': 'arXiv:2511.09741', 'title': 'TawPipe: Topology-Aware Weight Pipeline Parallelism for Accelerating Long-Context Large Models Training', 'authors': 'Houming Wu, Ling Chen', 'link': 'https://arxiv.org/abs/2511.09741', 'abstract': 'Training large language models (LLMs) is fundamentally constrained by limited device memory and costly inter-device communication. Although pipeline parallelism alleviates memory pressure by partitioning models across devices, it incurs activation communication overhead that scales linearly with sequence length, limiting efficiency in long-context training. Recent weight-passing approaches (e.g., WeiPipe) mitigate this by transmitting model weights instead of activations, but suffer from redundant peer-to-peer (P2P) transfers and underutilized intra-node bandwidth. We propose TawPipe--topology-aware weight pipeline parallelism, which exploits hierarchical bandwidth in distributed clusters for improved communication efficiency. TawPipe: (i) groups devices based on topology to optimize intra-node collective and inter-node P2P communication; (ii) assigns each device a fixed shard of model weights and gradients, avoiding redundant transfers; and (iii) overlaps communication with computation to hide latency. Unlike global collective operations used in fully sharded data parallelism (FSDP), TawPipe confines most communication within node boundaries, significantly reducing cross-node traffic. Extensive experiments on up to 24 GPUs with LLaMA-style models show that TawPipe achieves superior throughput and scalability compared to state-of-the-art baselines.', 'abstract_zh': '拓扑感知的权重管道并行化（TawPipe）：分布式集群中改进通信效率的方法', 'title_zh': 'TawPipe: 拓扑感知权重管道并行性加速长上下文大型模型训练'}
{'arxiv_id': 'arXiv:2511.09663', 'title': 'Alignment Debt: The Hidden Work of Making AI Usable', 'authors': 'Cumi Oyemike, Elizabeth Akpan, Pierre Hervé-Berdys', 'link': 'https://arxiv.org/abs/2511.09663', 'abstract': 'Frontier LLMs are optimised around high-resource assumptions about language, knowledge, devices, and connectivity. Whilst widely accessible, they often misfit conditions in the Global South. As a result, users must often perform additional work to make these systems usable. We term this alignment debt: the user-side burden that arises when AI systems fail to align with cultural, linguistic, infrastructural, or epistemic contexts. We develop and validate a four-part taxonomy of alignment debt through a survey of 411 AI users in Kenya and Nigeria. Among respondents measurable on this taxonomy (n = 385), prevalence is: Cultural and Linguistic (51.9%), Infrastructural (43.1%), Epistemic (33.8%), and Interaction (14.0%). Country comparisons show a divergence in Infrastructural and Interaction debt, challenging one-size-fits-Africa assumptions. Alignment debt is associated with compensatory labour, but responses vary by debt type: users facing Epistemic challenges verify outputs at significantly higher rates (91.5% vs. 80.8%; p = 0.037), and verification intensity correlates with cumulative debt burden (Spearmans rho = 0.147, p = 0.004). In contrast, Infrastructural and Interaction debts show weak or null associations with verification, indicating that some forms of misalignment cannot be resolved through verification alone. These findings show that fairness must be judged not only by model metrics but also by the burden imposed on users at the margins, compelling context-aware safeguards that alleviate alignment debt in Global South settings. The alignment debt framework provides an empirically grounded way to measure user burden, informing both design practice and emerging African AI governance efforts.', 'abstract_zh': '前沿大语言模型在语言、知识、设备和网络连接方面假设了高资源条件，尽管这些模型广泛可用，但在全球南方地区往往不适用。结果，用户往往需要额外的工作来使这些系统变得可用。我们称这种不匹配为“对齐债务”：当AI系统未能与其他文化、语言、基础设施或知识背景对齐时所产生的用户负担。我们通过对肯尼亚和尼日利亚411名AI用户进行调查，发展并验证了一个四部分的对齐债务分类框架。在按照该分类框架可测量的受访者中（n = 385），各类别发生率分别为：文化与语言（51.9%）、基础设施（43.1%）、知识（33.8%）和交互（14.0%）。国家间比较显示，在基础设施债务和交互债务方面存在差异，挑战了一刀切的非洲化假设。对齐债务与补偿性劳动相关，但债务类型不同：面对知识挑战的用户以显著更高的验证率进行输出验证（91.5% vs. 80.8%，p = 0.037），验证强度与累积对齐债务负担呈正相关（斯皮尔曼相关系数 = 0.147，p = 0.004）。相比之下，基础设施和交互债务显示与验证之间的弱或无关联性，表明某些形式的不匹配无法仅通过验证来解决。这些发现表明，公平不仅应由模型指标来衡量，还应由所施加于边缘用户负担来衡量，这要求在面向全球南方的环境中采取情境感知的安全措施以减轻对齐债务。对齐债务框架提供了一种基于实证的方法来衡量用户负担，既有助于设计实践，也指导正在兴起的非洲AI治理努力。', 'title_zh': 'AI 使用性背后的隐性工作：alignment debt'}
{'arxiv_id': 'arXiv:2511.09586', 'title': 'Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey', 'authors': 'Yuchen Huang, Sijia Li, Minghao Liu, Wei Liu, Shijue Huang, Zhiyuan Fan, Hou Pong Chan, Yi R. Fung', 'link': 'https://arxiv.org/abs/2511.09586', 'abstract': "LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze benchmarks, implementation strategies, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.", 'abstract_zh': '基于LLM的代理可以自主完成跨多个领域的复杂任务。然而，要进一步培养适应性行为和长期决策能力，仅通过静态数据集进行训练是不够的。这些数据集成本高昂且缺乏动态性和现实感。越来越多的研究共识认为，代理应该直接与环境互动，并通过强化学习从经验中学习。我们将这一迭代过程形式化为生成-执行-反馈（GEF）循环，其中环境生成任务来挑战代理，在任务执行过程中根据代理的行为返回观察结果，并为后续学习提供评估性反馈。在这一范式下，环境作为数据生产者的重要角色被强调，突显了其向更高复杂性、现实性和互动性扩展的需求。在本文综述中，我们从开创性的环境中心视角系统地回顾了环境扩展的代表性方法，并按GEF循环的阶段对其进行分类，即任务生成、任务执行和反馈。我们进一步分析了基准测试、实现策略和应用，整合了分散的研究进展，并为代理智能指明了未来研究方向。', 'title_zh': '在交互学习时代提升大型语言模型代理环境规模：综述'}
{'arxiv_id': 'arXiv:2511.09571', 'title': 'General Intelligence-based Fragmentation (GIF): A framework for peak-labeled spectra simulation', 'authors': 'Margaret R. Martin, Soha Hassoun', 'link': 'https://arxiv.org/abs/2511.09571', 'abstract': "Despite growing reference libraries and advanced computational tools, progress in the field of metabolomics remains constrained by low rates of annotating measured spectra. The recent developments of large language models (LLMs) have led to strong performance across a wide range of generation and reasoning tasks, spurring increased interest in LLMs' application to domain-specific scientific challenges, such as mass spectra annotation. Here, we present a novel framework, General Intelligence-based Fragmentation (GIF), that guides pretrained LLMs through spectra simulation using structured prompting and reasoning. GIF utilizes tagging, structured inputs/outputs, system prompts, instruction-based prompts, and iterative refinement. Indeed, GIF offers a structured alternative to ad hoc prompting, underscoring the need for systematic guidance of LLMs on complex scientific tasks. Using GIF, we evaluate current generalist LLMs' ability to use reasoning towards fragmentation and to perform intensity prediction after fine-tuning. We benchmark performance on a novel QA dataset, the MassSpecGym QA-sim dataset, that we derive from the MassSpecGym dataset. Through these implementations of GIF, we find that GPT-4o and GPT-4o-mini achieve a cosine similarity of 0.36 and 0.35 between the simulated and true spectra, respectively, outperforming other pretrained models including GPT-5, Llama-3.1, and ChemDFM, despite GPT-5's recency and ChemDFM's domain specialization. GIF outperforms several deep learning baselines. Our evaluation of GIF highlights the value of using LLMs not only for spectra simulation but for enabling human-in-the-loop workflows and structured, explainable reasoning in molecular fragmentation.", 'abstract_zh': '基于通用智能的碎片化（GIF）框架：指导预训练语言模型进行光谱模拟与结构化推理', 'title_zh': '基于通用智能的碎片化（GIF）：标签峰模拟的框架'}
