{'arxiv_id': 'arXiv:2505.16892', 'title': 'FlashBack: Consistency Model-Accelerated Shared Autonomy', 'authors': 'Luzhe Sun, Jingtian Ji, Xiangshan Tan, Matthew R. Walter', 'link': 'https://arxiv.org/abs/2505.16892', 'abstract': "Shared autonomy is an enabling technology that provides users with control authority over robots that would otherwise be difficult if not impossible to directly control. Yet, standard methods make assumptions that limit their adoption in practice-for example, prior knowledge of the user's goals or the objective (i.e., reward) function that they wish to optimize, knowledge of the user's policy, or query-level access to the user during training. Diffusion-based approaches to shared autonomy do not make such assumptions and instead only require access to demonstrations of desired behaviors, while allowing the user to maintain control authority. However, these advantages have come at the expense of high computational complexity, which has made real-time shared autonomy all but impossible. To overcome this limitation, we propose Consistency Shared Autonomy (CSA), a shared autonomy framework that employs a consistency model-based formulation of diffusion. Key to CSA is that it employs the distilled probability flow of ordinary differential equations (PF ODE) to generate high-fidelity samples in a single step. This results in inference speeds significantly than what is possible with previous diffusion-based approaches to shared autonomy, enabling real-time assistance in complex domains with only a single function evaluation. Further, by intervening on flawed actions at intermediate states of the PF ODE, CSA enables varying levels of assistance. We evaluate CSA on a variety of challenging simulated and real-world robot control problems, demonstrating significant improvements over state-of-the-art methods both in terms of task performance and computational efficiency.", 'abstract_zh': '共享自主性是一种使能技术，为用户提供对机器人控制的权威，这些机器人否则直接控制起来可能会非常困难甚至不可能。然而，标准方法做出的一些假设限制了它们在实践中的应用——例如，对用户的目標或他们希望优化的目标函数（即奖励函数）的先验知识，对用户政策的了解，或在训练过程中对用户的查询级访问。基于扩散的方法并不会做出这些假设，而是只需要访问所期望行为的示范，同时允许用户保持控制权威。然而，这些优势以高计算复杂性为代价，使其实时共享自主性几乎不可能实现。为了克服这一限制，我们提出了基于一致性模型的扩散一致性共享自主性（CSA）框架。CSA的关键在于使用普通微分方程的精练概率流（PF ODE）一次性生成高保真样本。这使得推断速度显著快于先前基于扩散的方法，能够在仅需一次函数评估的情况下实现复杂领域中的实时辅助。此外，通过在PF ODE的中间状态上干预错误行为，CSA能够提供不同程度的辅助。我们对各种具有挑战性的模拟和真实世界机器人控制问题进行了评估，展示了与最先进的方法相比，在任务性能和计算效率方面的显著改进。', 'title_zh': 'FlashBack: 一致性模型加速的共享自治'}
{'arxiv_id': 'arXiv:2505.16596', 'title': 'Safe Uncertainty-Aware Learning of Robotic Suturing', 'authors': 'Wilbert Peter Empleo, Yitaek Kim, Hansoul Kim, Thiusius Rajeeth Savarimuthu, Iñigo Iturrate', 'link': 'https://arxiv.org/abs/2505.16596', 'abstract': "Robot-Assisted Minimally Invasive Surgery is currently fully manually controlled by a trained surgeon. Automating this has great potential for alleviating issues, e.g., physical strain, highly repetitive tasks, and shortages of trained surgeons. For these reasons, recent works have utilized Artificial Intelligence methods, which show promising adaptability. Despite these advances, there is skepticism of these methods because they lack explainability and robust safety guarantees. This paper presents a framework for a safe, uncertainty-aware learning method. We train an Ensemble Model of Diffusion Policies using expert demonstrations of needle insertion. Using an Ensemble model, we can quantify the policy's epistemic uncertainty, which is used to determine Out-Of-Distribution scenarios. This allows the system to release control back to the surgeon in the event of an unsafe scenario. Additionally, we implement a model-free Control Barrier Function to place formal safety guarantees on the predicted action. We experimentally evaluate our proposed framework using a state-of-the-art robotic suturing simulator. We evaluate multiple scenarios, such as dropping the needle, moving the camera, and moving the phantom. The learned policy is robust to these perturbations, showing corrective behaviors and generalization, and it is possible to detect Out-Of-Distribution scenarios. We further demonstrate that the Control Barrier Function successfully limits the action to remain within our specified safety set in the case of unsafe predictions.", 'abstract_zh': '机器人辅助微创手术目前完全由训练有素的外科医生手动控制。利用人工智能方法自动控制具有减轻体力劳动、减少重复性任务以及缓解训练外科医生短缺等问题的巨大潜力。鉴于这些原因，近期的研究利用了人工智能方法，显示出较强的适应性。尽管取得了这些进展，但由于缺乏解释性和稳健的安全保证，人们对这些方法仍持怀疑态度。本文提出了一种安全、 Awareness不确定性的学习方法框架。我们使用专家的针插入演示训练了一个扩散策略集成模型，使用集成模型可以量化策略的 epistemic 不确定性，并用于确定 Out-Of-Distribution 情况。这使得系统能够在发生不安全情况时将控制权交还给外科医生。此外，我们实现了一个无模型的控制障碍函数，以正式的安全保证约束预测动作。我们使用最先进的机器人缝合模拟器实验性地评估了我们提出的方法。我们评估了针丢失、移动摄像机和移动人造器官等多种场景。所学习的策略对这些干扰具有稳健性，展示了纠正行为和泛化能力，并且能够检测到 Out-Of-Distribution 情况。我们进一步证明，控制障碍函数成功地限制了动作，使其在不安全预测的情况下保持在我们指定的安全集内。', 'title_zh': '具有安全不确定性意识学习的机器人缝合'}
{'arxiv_id': 'arXiv:2505.16547', 'title': 'Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for Occlusion Aware Plant Manipulation', 'authors': 'Nitesh Subedi, Hsin-Jung Yang, Devesh K. Jha, Soumik Sarkar', 'link': 'https://arxiv.org/abs/2505.16547', 'abstract': 'This paper presents an end-to-end deep reinforcement learning (RL) framework for occlusion-aware robotic manipulation in cluttered plant environments. Our approach enables a robot to interact with a deformable plant to reveal hidden objects of interest, such as fruits, using multimodal observations. We decouple the kinematic planning problem from robot control to simplify zero-shot sim2real transfer for the trained policy. Our results demonstrate that the trained policy, deployed using our framework, achieves up to 86.7% success in real-world trials across diverse initial conditions. Our findings pave the way toward autonomous, perception-driven agricultural robots that intelligently interact with complex foliage plants to "find the fruit" in challenging occluded scenarios, without the need for explicitly designed geometric and dynamic models of every plant scenario.', 'abstract_zh': '本文提出了一种端到端的深度强化学习（RL）框架，用于遮挡感知的机器人在杂乱植物环境中的操作。我们的方法使机器人能够使用多模态观测与可变形植物交互，以揭示感兴趣的目标对象，如果实。我们将运动学规划问题与机器人控制分离，简化了训练策略的零样本模拟到现实的转移。实验结果表明，使用我们框架部署的训练策略在不同初始条件下成功率达到86.7%。我们的研究为智能交互复杂叶状植物以在挑战性的遮挡场景中“找到果实”的自主感知农业机器人铺平了道路，无需为每个植物场景显式设计几何和动力学模型。', 'title_zh': '找到果实：设计一种适用于遮挡感知植物操作的零样本模拟到现实的深度强化学习规划器'}
{'arxiv_id': 'arXiv:2505.16517', 'title': 'ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models', 'authors': 'Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang, Qian Jiang, Zhenhao Chen, Zhongzhi Li, Rui Yan, Xiuying Chen', 'link': 'https://arxiv.org/abs/2505.16517', 'abstract': 'Large Vision-Language Models (LVLMs) have recently advanced robotic manipulation by leveraging vision for scene perception and language for instruction following. However, existing methods rely heavily on costly human-annotated training datasets, which limits their generalization and causes them to struggle in out-of-domain (OOD) scenarios, reducing real-world adaptability. To address these challenges, we propose ManipLVM-R1, a novel reinforcement learning framework that replaces traditional supervision with Reinforcement Learning using Verifiable Rewards (RLVR). By directly optimizing for task-aligned outcomes, our method enhances generalization and physical reasoning while removing the dependence on costly annotations. Specifically, we design two rule-based reward functions targeting key robotic manipulation subtasks: an Affordance Perception Reward to enhance localization of interaction regions, and a Trajectory Match Reward to ensure the physical plausibility of action paths. These rewards provide immediate feedback and impose spatial-logical constraints, encouraging the model to go beyond shallow pattern matching and instead learn deeper, more systematic reasoning about physical interactions.', 'abstract_zh': 'Large Vision-Language Models (LVLMs)通过融合视觉场景感知和语言指令跟随， recently advanced robotic manipulation. However, existing methods heavily rely on costly human-annotated training datasets, limiting generalization and causing struggles in out-of-domain scenarios, reducing real-world adaptability. To address these challenges, we propose ManipLVM-R1, a novel reinforcement learning framework that replaces traditional supervision with Reinforcement Learning using Verifiable Rewards (RLVR). By directly optimizing for task-aligned outcomes, our method enhances generalization and physical reasoning while removing the dependence on costly annotations. Specifically, we design two rule-based reward functions targeting key robotic manipulation subtasks: an Affordance Perception Reward to enhance localization of interaction regions, and a Trajectory Match Reward to ensure the physical plausibility of action paths. These rewards provide immediate feedback and impose spatial-logical constraints, encouraging the model to go beyond shallow pattern matching and instead learn deeper, more systematic reasoning about physical interactions.', 'title_zh': 'ManipLVM-R1: 基于强化学习的大型视觉语言模型在具身操作推理中的应用'}
{'arxiv_id': 'arXiv:2505.16478', 'title': 'Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot', 'authors': "Davide Gorbani, Giuseppe L'Erario, Hosameldin Awadalla Omer Mohamed, Daniele Pucci", 'link': 'https://arxiv.org/abs/2505.16478', 'abstract': "We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot. The controller is based on a linearised centroidal momentum model to represent the flight dynamics, augmented with a second-order nonlinear model to explicitly account for the slow and nonlinear dynamics of jet propulsion. A key contribution is the introduction of a multi-rate MPC formulation that handles the different actuation rates of the robot's joints and jet engines while embedding the jet dynamics directly into the predictive model. We validated the framework using the jet-powered humanoid robot iRonCub, performing simulations in Mujoco; the simulation results demonstrate the robot's ability to recover from external disturbances and perform stable, non-abrupt flight manoeuvres, validating the effectiveness of the proposed approach.", 'abstract_zh': '我们提出了一种用于喷气动力 humanoïd 机器人的一种新颖的模型预测控制 (MPC) 框架。该控制器基于线性化的质心动量模型来表示飞行动力学，并通过二次非线性模型明确考虑喷气推进的慢速和非线性动力学。一个关键贡献是引入了一种多速率 MPC 表达式，该表达式处理了机器人关节和喷气发动机的不同作动速率，并将喷气动力学直接嵌入到预测模型中。我们使用喷气动力 humanoïd 机器人 iRonCub 进行了验证，在 Mujoco 中进行了仿真；仿真结果证明了该机器人能够从外部干扰中恢复，并执行稳定的非突变飞行机动，验证了所提出方法的有效性。', 'title_zh': '统一的多速率模型预测控制方法应用于喷气动力人形机器人'}
{'arxiv_id': 'arXiv:2505.16394', 'title': 'Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)', 'authors': 'Zhenjie Yang, Xiaosong Jia, Qifeng Li, Xue Yang, Maoqing Yao, Junchi Yan', 'link': 'https://arxiv.org/abs/2505.16394', 'abstract': 'Reinforcement Learning (RL) can mitigate the causal confusion and distribution shift inherent to imitation learning (IL). However, applying RL to end-to-end autonomous driving (E2E-AD) remains an open problem for its training difficulty, and IL is still the mainstream paradigm in both academia and industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated promising results in neural planning; however, these methods typically require privileged information as input rather than raw sensor data. We fill this gap by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently train an auxiliary privileged world model paired with a neural planner that uses privileged information as input. Subsequently, we introduce a raw sensor world model trained via our proposed Guidance Mechanism, which ensures consistency between the raw sensor world model and the privileged world model during rollouts. Finally, the raw sensor world model combines the prior knowledge embedded in the heads of the privileged world model to effectively guide the training of the raw sensor policy. Raw2Drive is so far the only RL based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it achieves state-of-the-art performance.', 'abstract_zh': '强化学习（RL）可以缓解模仿学习（IL）中固有的因果混淆和分布偏移问题。然而，将RL应用于端到端自主驾驶（E2E-AD）仍然是一个开放问题，因为其训练难度较大，而IL仍然是学术界和工业界的主流范式。最近，基于模型的强化学习（MBRL）在神经规划领域取得了令人promise的结果；然而，这些方法通常需要特权信息作为输入而非原始传感器数据。我们通过设计Raw2Drive，一种双流MBRL方法来填补这一空白。首先，我们高效地训练一个辅助的特权世界模型，该模型与一个使用特权信息作为输入的神经规划器配对。随后，我们引入了一个通过我们提出的一种引导机制训练的原始传感器世界模型，该机制确保了原始传感器世界模型和特权世界模型在滚动过程中的一致性。最后，原始传感器世界模型结合了特权世界模型头部嵌入的先验知识，有效引导原始传感器策略的训练。Raw2Drive目前是CARLA Leaderboard 2.0和Bench2Drive上唯一的基于RL的端到端方法，并取得了最先进的性能。', 'title_zh': 'Raw2Drive: 基于对齐世界模型的强化学习端到端自动驾驶（在CARLA v2中）'}
{'arxiv_id': 'arXiv:2505.16377', 'title': 'VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving', 'authors': 'Yansong Qu, Zilin Huang, Zihao Sheng, Jiancong Chen, Sikai Chen, Samuel Labi', 'link': 'https://arxiv.org/abs/2505.16377', 'abstract': 'Reinforcement learning (RL)-based autonomous driving policy learning faces critical limitations such as low sample efficiency and poor generalization; its reliance on online interactions and trial-and-error learning is especially unacceptable in safety-critical scenarios. Existing methods including safe RL often fail to capture the true semantic meaning of "safety" in complex driving contexts, leading to either overly conservative driving behavior or constraint violations. To address these challenges, we propose VL-SAFE, a world model-based safe RL framework with Vision-Language model (VLM)-as-safety-guidance paradigm, designed for offline safe policy learning. Specifically, we construct offline datasets containing data collected by expert agents and labeled with safety scores derived from VLMs. A world model is trained to generate imagined rollouts together with safety estimations, allowing the agent to perform safe planning without interacting with the real environment. Based on these imagined trajectories and safety evaluations, actor-critic learning is conducted under VLM-based safety guidance to optimize the driving policy more safely and efficiently. Extensive evaluations demonstrate that VL-SAFE achieves superior sample efficiency, generalization, safety, and overall performance compared to existing baselines. To the best of our knowledge, this is the first work that introduces a VLM-guided world model-based approach for safe autonomous driving. The demo video and code can be accessed at: this https URL', 'abstract_zh': '基于视觉语言模型的世界模型导向的增强学习安全自驾车策略学习', 'title_zh': 'VL-SAFE: 视觉-语言引导的安全意识强化学习方法及其在自主驾驶中的应用'}
{'arxiv_id': 'arXiv:2505.16196', 'title': 'SEM: Enhancing Spatial Understanding for Robust Robot Manipulation', 'authors': 'Xuewu Lin, Tianwei Lin, Lichao Huang, Hongyu Xie, Yiwei Jin, Keyu Li, Zhizhong Su', 'link': 'https://arxiv.org/abs/2505.16196', 'abstract': 'A key challenge in robot manipulation lies in developing policy models with strong spatial understanding, the ability to reason about 3D geometry, object relations, and robot embodiment. Existing methods often fall short: 3D point cloud models lack semantic abstraction, while 2D image encoders struggle with spatial reasoning. To address this, we propose SEM (Spatial Enhanced Manipulation model), a novel diffusion-based policy framework that explicitly enhances spatial understanding from two complementary perspectives. A spatial enhancer augments visual representations with 3D geometric context, while a robot state encoder captures embodiment-aware structure through graphbased modeling of joint dependencies. By integrating these modules, SEM significantly improves spatial understanding, leading to robust and generalizable manipulation across diverse tasks that outperform existing baselines.', 'abstract_zh': '机器人操作中的一个关键挑战在于开发具有强烈空间理解能力的策略模型，即能够推理三维几何、物体关系和机器人本体的能力。现有方法往往不够理想：3D点云模型缺乏语义抽象，而2D图像编码器在空间推理方面存在问题。为此，我们提出了一种新的基于扩散的策略框架SEM（Spatial Enhanced Manipulation模型），该框架从两个互补的角度 Explicitly 提升空间理解能力。空间增强器通过添加3D几何上下文来增强视觉表示，而机器人状态编码器则通过基于图的关节依赖建模来捕捉本体感知的结构。通过整合这些模块，SEM 显著提高了空间理解能力，从而在各种任务中实现了稳健且通用的操作，优于现有基线方法。', 'title_zh': 'SEM: 提升空间理解以实现稳健的机器人操作'}
{'arxiv_id': 'arXiv:2505.16187', 'title': 'EasyInsert: A Data-Efficient and Generalizable Insertion Policy', 'authors': 'Guanghe Li, Junming Zhao, Shengjie Wang, Yang Gao', 'link': 'https://arxiv.org/abs/2505.16187', 'abstract': 'Insertion task is highly challenging that requires robots to operate with exceptional precision in cluttered environments. Existing methods often have poor generalization capabilities. They typically function in restricted and structured environments, and frequently fail when the plug and socket are far apart, when the scene is densely cluttered, or when handling novel objects. They also rely on strong assumptions such as access to CAD models or a digital twin in simulation. To address this, we propose EasyInsert, a framework which leverages the human intuition that relative pose (delta pose) between plug and socket is sufficient for successful insertion, and employs efficient and automated real-world data collection with minimal human labor to train a generalizable model for relative pose prediction. During execution, EasyInsert follows a coarse-to-fine execution procedure based on predicted delta pose, and successfully performs various insertion tasks. EasyInsert demonstrates strong zero-shot generalization capability for unseen objects in cluttered environments, handling cases with significant initial pose deviations while maintaining high sample efficiency and requiring little human effort. In real-world experiments, with just 5 hours of training data, EasyInsert achieves over 90% success in zero-shot insertion for 13 out of 15 unseen novel objects, including challenging objects like Type-C cables, HDMI cables, and Ethernet cables. Furthermore, with only one human demonstration and 4 minutes of automatically collected data for fine-tuning, it reaches over 90% success rate for all 15 objects.', 'abstract_zh': '插入任务高度具有挑战性，要求机器人在拥挤环境中以极高的精度操作。现有方法通常具有较差的泛化能力。它们通常仅在受限和结构化的环境中有效，并且经常在插头和插座距离较远、场景高度拥挤或处理新型物体时失败。这些方法还依赖于诸如CAD模型访问或模拟中的数字孪生等强烈假设。为解决这一问题，我们提出了EasyInsert框架，该框架利用人类直觉，即插头和插座之间的相对姿态（增量姿态）对于成功的插入是足够的，并通过最少的人工劳动高效且自动化地收集现实世界数据来训练一个可泛化的相对姿态预测模型。在执行过程中，EasyInsert基于预测的增量姿态采取粗到细的执行程序，并成功完成了多种插入任务。EasyInsert展示了在未见过的物体和拥挤环境中出色的零-shot泛化能力，即使初始姿态偏差较大也能保持高样本效率并减少人工投入。在实际实验中，仅使用5小时的训练数据，EasyInsert在13个未见过的新型物体上的零-shot插入成功率超过90%，包括Type-C数据线、HDMI数据线和以太网数据线等具有挑战性的物体。此外，仅通过一次人工演示和4分钟的自动收集数据进行微调后，它对所有15个物体的插入成功率超过90%。', 'title_zh': 'EasyInsert: 一种数据高效且通用的插入策略'}
{'arxiv_id': 'arXiv:2505.16084', 'title': 'Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex Quadruped Mobility', 'authors': 'Zewei Zhang, Chenhao Li, Takahiro Miki, Marco Hutter', 'link': 'https://arxiv.org/abs/2505.16084', 'abstract': "Reinforcement learning (RL)-based legged locomotion controllers often require meticulous reward tuning to track velocities or goal positions while preserving smooth motion on various terrains. Motion imitation methods via RL using demonstration data reduce reward engineering but fail to generalize to novel environments. We address this by proposing a hierarchical RL framework in which a low-level policy is first pre-trained to imitate animal motions on flat ground, thereby establishing motion priors. A subsequent high-level, goal-conditioned policy then builds on these priors, learning residual corrections that enable perceptive locomotion, local obstacle avoidance, and goal-directed navigation across diverse and rugged terrains. Simulation experiments illustrate the effectiveness of learned residuals in adapting to progressively challenging uneven terrains while still preserving the locomotion characteristics provided by the motion priors. Furthermore, our results demonstrate improvements in motion regularization over baseline models trained without motion priors under similar reward setups. Real-world experiments with an ANYmal-D quadruped robot confirm our policy's capability to generalize animal-like locomotion skills to complex terrains, demonstrating smooth and efficient locomotion and local navigation performance amidst challenging terrains with obstacles.", 'abstract_zh': '基于 reinforcement learning 的腿足运动控制器通常需要精细调节奖励以在各种地形上跟踪速度或目标位置同时保持平滑运动。通过使用演示数据的 reinforcement learning 方法进行运动模仿可以减少奖励工程，但难以泛化到新型环境。为此，我们提出了一种分层 reinforcement learning 框架，在该框架中，低层策略首先在平坦地面预训练以模仿动物运动，从而建立运动先验；随后，高层、基于目标的策略在此基础上学习残差修正，以实现感知驱动的运动、局部障碍物避免以及在多样且崎岖地形上的目标导向导航。仿真实验表明，所学残差在适应逐渐更具挑战性的不平地形时依然能够保留由运动先验提供的运动特征。此外，我们的结果表明，在提供运动先验的基线模型下，我们的策略在类似奖励设置下的运动正则化方面有所改进。实际实验中，使用 ANYmal-D 四足机器人验证了策略能够将类似的动物运动技能泛化到复杂地形，并在具有障碍物的挑战性地形中实现了平滑且高效的运动和局部导航性能。', 'title_zh': '重设想运动先验：适应复杂四足移动的平面地形技能'}
{'arxiv_id': 'arXiv:2505.16042', 'title': 'Reference Free Platform Adaptive Locomotion for Quadrupedal Robots using a Dynamics Conditioned Policy', 'authors': 'David Rytz, Suyoung Choi, Wanming Yu, Wolfgang Merkt, Jemin Hwangbo, Ioannis Havoutis', 'link': 'https://arxiv.org/abs/2505.16042', 'abstract': 'This article presents Platform Adaptive Locomotion (PAL), a unified control method for quadrupedal robots with different morphologies and dynamics. We leverage deep reinforcement learning to train a single locomotion policy on procedurally generated robots. The policy maps proprioceptive robot state information and base velocity commands into desired joint actuation targets, which are conditioned using a latent embedding of the temporally local system dynamics. We explore two conditioning strategies - one using a GRU-based dynamics encoder and another using a morphology-based property estimator - and show that morphology-aware conditioning outperforms temporal dynamics encoding regarding velocity task tracking for our hardware test on ANYmal C. Our results demonstrate that both approaches achieve robust zero-shot transfer across multiple unseen simulated quadrupeds. Furthermore, we demonstrate the need for careful robot reference modelling during training, enabling us to reduce the velocity tracking error by up to 30% compared to the baseline method. Despite PAL not surpassing the best-performing reference-free controller in all cases, our analysis uncovers critical design choices and informs improvements to the state of the art.', 'abstract_zh': '平台自适应步行（PAL）：一种用于不同形态和动力学四足机器人的一体化控制方法', 'title_zh': '参考自由的平台自适应四足机器人运动控制基于动力学条件策略'}
{'arxiv_id': 'arXiv:2505.15954', 'title': 'Integrating Robotic Navigation with Blockchain: A Novel PoS-Based Approach for Heterogeneous Robotic Teams', 'authors': 'Nasim Paykari, Ali Alfatemi, Damian M. Lyons, Mohamed Rahouti', 'link': 'https://arxiv.org/abs/2505.15954', 'abstract': 'This work explores a novel integration of blockchain methodologies with Wide Area Visual Navigation (WAVN) to address challenges in visual navigation for a heterogeneous team of mobile robots deployed for unstructured applications in agriculture, forestry, etc. Focusing on overcoming challenges such as GPS independence, environmental changes, and computational limitations, the study introduces the Proof of Stake (PoS) mechanism, commonly used in blockchain systems, into the WAVN framework \\cite{Lyons_2022}. This integration aims to enhance the cooperative navigation capabilities of robotic teams by prioritizing robot contributions based on their navigation reliability. The methodology involves a stake weight function, consensus score with PoS, and a navigability function, addressing the computational complexities of robotic cooperation and data validation. This innovative approach promises to optimize robotic teamwork by leveraging blockchain principles, offering insights into the scalability, efficiency, and overall system performance. The project anticipates significant advancements in autonomous navigation and the broader application of blockchain technology beyond its traditional financial context.', 'abstract_zh': '本研究探索了区块链方法与广域视觉导航（WAVN）的新型集成，以解决异构移动机器人团队在农业、林业等非结构化应用中视觉导航面临的挑战。该研究专注于克服如GPS独立性、环境变化和计算限制等挑战，通过将区块链系统中常用的权益证明（PoS）机制引入WAVN框架（参见[@Lyons_2022]），旨在通过优先考虑机器人基于其导航 reliability 的贡献来增强机器人团队的协同导航能力。该方法论包括质押权重函数、结合PoS的共识分数以及导航性函数，以应对机器人合作和数据验证的计算复杂性。这一创新方法有望通过利用区块链原则优化机器人团队协作，为自主导航和区块链技术在更广泛领域的应用提供深刻的见解。该项目预期在自主导航和区块链技术的非金融应用方面取得重大进展。', 'title_zh': '基于PoS的新颖方法：区块链与异构机器人团队的导航集成'}
{'arxiv_id': 'arXiv:2505.15925', 'title': 'VERDI: VLM-Embedded Reasoning for Autonomous Driving', 'authors': 'Bowen Feng, Zhiting Mei, Baiang Li, Julian Ost, Roger Girgis, Anirudha Majumdar, Felix Heide', 'link': 'https://arxiv.org/abs/2505.15925', 'abstract': 'While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, \\textsc{VERDI} enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We demonstrate the effectiveness of our method on the NuScenes dataset and find that VERDI outperforms existing e2e methods that do not embed reasoning by 10% in $\\ell_{2}$ distance, while maintaining high inference speed.', 'abstract_zh': 'VLM嵌入推理在自主驾驶中的应用：VERDI方法', 'title_zh': 'VERDI: VLM嵌入式推理在自动驾驶中的应用'}
{'arxiv_id': 'arXiv:2505.17016', 'title': 'Interactive Post-Training for Vision-Language-Action Models', 'authors': 'Shuhan Tan, Kairan Dou, Yue Zhao, Philipp Krähenbühl', 'link': 'https://arxiv.org/abs/2505.17016', 'abstract': 'We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation.\nRIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision.', 'abstract_zh': 'RIPT-VLA：一种基于强化学习的简单可扩展的交互式后训练范式', 'title_zh': '训练后交互式微调方法用于视觉-语言-行动模型'}
{'arxiv_id': 'arXiv:2505.17006', 'title': 'CoMo: Learning Continuous Latent Motion from Internet Videos for Scalable Robot Learning', 'authors': 'Jiange Yang, Yansong Shi, Haoyi Zhu, Mingyu Liu, Kaijing Ma, Yating Wang, Gangshan Wu, Tong He, Limin Wang', 'link': 'https://arxiv.org/abs/2505.17006', 'abstract': 'Learning latent motion from Internet videos is crucial for building generalist robots. However, existing discrete latent action methods suffer from information loss and struggle with complex and fine-grained dynamics. We propose CoMo, which aims to learn more informative continuous motion representations from diverse, internet-scale videos. CoMo employs a early temporal feature difference mechanism to prevent model collapse and suppress static appearance noise, effectively discouraging shortcut learning problem. Furthermore, guided by the information bottleneck principle, we constrain the latent motion embedding dimensionality to achieve a better balance between retaining sufficient action-relevant information and minimizing the inclusion of action-irrelevant appearance noise. Additionally, we also introduce two new metrics for more robustly and affordably evaluating motion and guiding motion learning methods development: (i) the linear probing MSE of action prediction, and (ii) the cosine similarity between past-to-current and future-to-current motion embeddings. Critically, CoMo exhibits strong zero-shot generalization, enabling it to generate continuous pseudo actions for previously unseen video domains. This capability facilitates unified policy joint learning using pseudo actions derived from various action-less video datasets (such as cross-embodiment videos and, notably, human demonstration videos), potentially augmented with limited labeled robot data. Extensive experiments show that policies co-trained with CoMo pseudo actions achieve superior performance with both diffusion and autoregressive architectures in simulated and real-world settings.', 'abstract_zh': '从互联网视频中学习潜在运动对于构建通用机器人至关重要。然而，现有的离散潜在动作方法存在信息损失问题，并且难以处理复杂的细粒度动态。我们提出CoMo，旨在从多样化的互联网规模视频中学习更具信息量的连续运动表示。CoMo采用早期时间特征差异机制防止模型崩溃并抑制静态外观噪声，有效地避免了捷径学习问题。此外，遵循信息瓶颈原则，我们约束潜在运动嵌入的维度，以实现保留足够相关动作信息与最小化不相关外观噪声包含之间的更好平衡。此外，我们还引入了两个新的评估指标，以更稳健和经济的方式评估运动，并指导运动学习方法的发展：（i）动作预测的线性探针均方误差，以及(ii) 过去到当前和未来到当前运动嵌入的余弦相似度。关键的是，CoMo表现出强大的零样本泛化能力，能够为之前未见过的视频域生成连续伪动作。这一能力使得使用来自各种无动作视频数据集（如跨体裁视频和值得注意的人类示范视频）派生的伪动作进行统一策略联合学习成为可能，这些数据集可能辅以有限的标注机器人数据。大量实验表明，与CoMo伪动作协同训练的策略在模拟和真实世界环境中使用扩散和自回归架构时表现出优越性能。', 'title_zh': 'CoMo: 从互联网视频学习连续潜运动以实现可扩展的机器人学习'}
{'arxiv_id': 'arXiv:2505.16928', 'title': 'Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning', 'authors': 'Bosung Kim, Prithviraj Ammanabrolu', 'link': 'https://arxiv.org/abs/2505.16928', 'abstract': "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks that advances long-context understanding in embodied AI. $\\infty$-THOR provides: (1) a generation framework for synthesizing scalable, reproducible, and unlimited long-horizon trajectories; (2) a novel embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered clues across extended trajectories test agents' long-context reasoning ability; and (3) a long-horizon dataset and benchmark suite featuring complex tasks that span hundreds of environment steps, each paired with ground-truth action sequences. To enable this capability, we explore architectural adaptations, including interleaved Goal-State-Action modeling, context extension techniques, and Context Parallelism, to equip LLM-based agents for extreme long-context reasoning and interaction. Experimental results and analyses highlight the challenges posed by our benchmark and provide insights into training strategies and model behaviors under long-horizon conditions. Our work provides a foundation for the next generation of embodied AI systems capable of robust, long-term reasoning and planning.", 'abstract_zh': '$\\infty$-THOR：一种新的长时域体态任务框架，推进体态AI中的长上下文理解', 'title_zh': '超越体内的针堆：长上下文推理中的环境、架构与训练考量'}
{'arxiv_id': 'arXiv:2505.16902', 'title': 'RealEngine: Simulating Autonomous Driving in Realistic Context', 'authors': 'Junzhe Jiang, Nan Song, Jingyu Li, Xiatian Zhu, Li Zhang', 'link': 'https://arxiv.org/abs/2505.16902', 'abstract': 'Driving simulation plays a crucial role in developing reliable driving agents by providing controlled, evaluative environments. To enable meaningful assessments, a high-quality driving simulator must satisfy several key requirements: multi-modal sensing capabilities (e.g., camera and LiDAR) with realistic scene rendering to minimize observational discrepancies; closed-loop evaluation to support free-form trajectory behaviors; highly diverse traffic scenarios for thorough evaluation; multi-agent cooperation to capture interaction dynamics; and high computational efficiency to ensure affordability and scalability. However, existing simulators and benchmarks fail to comprehensively meet these fundamental criteria. To bridge this gap, this paper introduces RealEngine, a novel driving simulation framework that holistically integrates 3D scene reconstruction and novel view synthesis techniques to achieve realistic and flexible closed-loop simulation in the driving context. By leveraging real-world multi-modal sensor data, RealEngine reconstructs background scenes and foreground traffic participants separately, allowing for highly diverse and realistic traffic scenarios through flexible scene composition. This synergistic fusion of scene reconstruction and view synthesis enables photorealistic rendering across multiple sensor modalities, ensuring both perceptual fidelity and geometric accuracy. Building upon this environment, RealEngine supports three essential driving simulation categories: non-reactive simulation, safety testing, and multi-agent interaction, collectively forming a reliable and comprehensive benchmark for evaluating the real-world performance of driving agents.', 'abstract_zh': '实时引擎：面向驾驶代理评估的综合三维场景重建与新型视图合成框架', 'title_zh': 'RealEngine: 在现实情境中模拟自动驾驶'}
{'arxiv_id': 'arXiv:2505.16815', 'title': 'Perceptual Quality Assessment for Embodied AI', 'authors': 'Chunyi Li, Jiaohao Xiao, Jianbo Zhang, Farong Wen, Zicheng Zhang, Yuan Tian, Xiangyang Zhu, Xiaohong Liu, Zhengxue Cheng, Weisi Lin, Guangtao Zhai', 'link': 'https://arxiv.org/abs/2505.16815', 'abstract': 'Embodied AI has developed rapidly in recent years, but it is still mainly deployed in laboratories, with various distortions in the Real-world limiting its application. Traditionally, Image Quality Assessment (IQA) methods are applied to predict human preferences for distorted images; however, there is no IQA method to assess the usability of an image in embodied tasks, namely, the perceptual quality for robots. To provide accurate and reliable quality indicators for future embodied scenarios, we first propose the topic: IQA for Embodied AI. Specifically, we (1) based on the Mertonian system and meta-cognitive theory, constructed a perception-cognition-decision-execution pipeline and defined a comprehensive subjective score collection process; (2) established the Embodied-IQA database, containing over 36k reference/distorted image pairs, with more than 5m fine-grained annotations provided by Vision Language Models/Vision Language Action-models/Real-world robots; (3) trained and validated the performance of mainstream IQA methods on Embodied-IQA, demonstrating the need to develop more accurate quality indicators for Embodied AI. We sincerely hope that through evaluation, we can promote the application of Embodied AI under complex distortions in the Real-world. Project page: this https URL', 'abstract_zh': '自主研发的智能体AI近年来取得了 rapid进展，但主要仍局限于实验室环境，在实际应用场景中的应用受到各种扭曲的限制。传统图像质量评估（IQA）方法用于预测人类对扭曲图像的偏好；然而，缺乏评估图像在自主任务中可用性的方法，即对机器人的知觉质量评估。为提供未来自主场景中的准确可靠质量指标，我们首先提出课题：面向自主的图像质量评估（IQA for Embodied AI）。具体而言，我们(1)基于梅隆系统和元认知理论，构建了感知-认知-决策-执行管道，并定义了综合的主观评分收集过程；(2)建立了包含超过36000个参考/扭曲图像对的自主-IQA数据库，提供了超过500万细粒度的注解，这些注解来自视觉语言模型/视觉语言动作模型/真实世界机器人；(3)对主流的IQA方法在自主-IQA上的性能进行了训练和验证，展示了为自主智能体开发更准确质量指标的必要性。我们诚挚希望通过评估，促进自主研发的智能体在复杂扭曲的实际应用中的应用。项目页面：this https URL。', 'title_zh': '感知质量评估 for 体态人工智能'}
{'arxiv_id': 'arXiv:2505.16278', 'title': 'DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving', 'authors': 'Zhenjie Yang, Yilin Chai, Xiaosong Jia, Qifeng Li, Yuqian Shao, Xuekai Zhu, Haisheng Su, Junchi Yan', 'link': 'https://arxiv.org/abs/2505.16278', 'abstract': 'End-to-end autonomous driving (E2E-AD) demands effective processing of multi-view sensory data and robust handling of diverse and complex driving scenarios, particularly rare maneuvers such as aggressive turns. Recent success of Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs) demonstrates that specialization of parameters enables strong scalability. In this work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a Scene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is built upon our $\\pi_0$ Vision-Language-Action (VLA) baseline (originally from the embodied AI field), called Drive-$\\pi_0$. Specifically, we add Vision MoE to Drive-$\\pi_0$ by training a router to select relevant cameras according to the driving context dynamically. This design mirrors human driving cognition, where drivers selectively attend to crucial visual cues rather than exhaustively processing all visual information. In addition, we add Action MoE by training another router to activate specialized expert modules for different driving behaviors. Through explicit behavioral specialization, DriveMoE is able to handle diverse scenarios without suffering from modes averaging like existing models. In Bench2Drive closed-loop evaluation experiments, DriveMoE achieves state-of-the-art (SOTA) performance, demonstrating the effectiveness of combining vision and action MoE in autonomous driving tasks. We will release our code and models of DriveMoE and Drive-$\\pi_0$.', 'abstract_zh': '端到端自主驾驶（E2E-AD）要求有效地处理多视图感知数据，并且能够 robust 地处理多样且复杂的驾驶场景，特别是激烈的转向等罕见操作。MoE 架构在大语言模型（LLMs）中的 recent 成功表明，参数的专业化能够实现 strong 的扩展性。在本文中，我们提出了一种新型的 MoE 基础的端到端自主驾驶框架 DriveMoE，包含场景专业化视觉 MoE 和技能专业化行动 MoE。DriveMoE 是建立在我们 $\\pi_0$ 视觉-语言-行动（VLA）基线基础上的，称为 Drive-$\\pi_0$（最初来自赋能 AI 领域）。具体来说，我们在 Drive-$\\pi_0$ 中加入了视觉 MoE，通过训练一个路由器动态选择与驾驶情境相关的摄像头。这种设计反映了人类驾驶的认知模式，即驾驶员会选择性地关注关键的视觉线索而不会处理所有的视觉信息。此外，我们通过训练另一个路由器激活不同的专家模块来增强行动 MoE，以适应不同的驾驶行为。通过明确的行为专业化，DriveMoE 能够处理多样化的场景，避免了现有模型中模式平均的问题。在 Bench2Drive 闭环评估实验中，DriveMoE 达到了最先进的（SOTA）性能，证明了结合视觉和行动 MoE 在自主驾驶任务中的有效性。我们将发布 DriveMoE 和 Drive-$\\pi_0$ 的代码和模型。', 'title_zh': 'DriveMoE：端到端自动驾驶中的视觉-语言-动作模型的混合专家模型'}
{'arxiv_id': 'arXiv:2505.16938', 'title': 'NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification', 'authors': 'NovelSeek Team, Bo Zhang, Shiyang Feng, Xiangchao Yan, Jiakang Yuan, Zhiyin Yu, Xiaohan He, Songtao Huang, Shaowei Hou, Zheng Nie, Zhilong Wang, Jinyao Liu, Runmin Ma, Tianshuo Peng, Peng Ye, Dongzhan Zhou, Shufei Zhang, Xiaosong Wang, Yilan Zhang, Meng Li, Zhongying Tu, Xiangyu Yue, Wangli Ouyang, Bowen Zhou, Lei Bai', 'link': 'https://arxiv.org/abs/2505.16938', 'abstract': 'Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. NovelSeek highlights three key advantages: 1) Scalability: NovelSeek has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity: NovelSeek provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from 0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.', 'abstract_zh': '人工智能（AI）正加速科研范式的转变，不仅提高研究效率，还推动创新。我们介绍了NovelSeek，这是一种统一的闭环多代理框架，用于跨多个科学领域开展自主科学研究（ASR），使研究人员能够以前所未有的速度和精确度解决这些领域的复杂问题。NovelSeek突显了三项关键优势：1）可扩展性：NovelSeek在其在12项科学研究任务中的 versatility 表现出了强大的适应性，能够生成创新想法以提升基础代码的性能。2）互动性：NovelSeek 提供了供人类专家反馈和多代理交互的接口，在自动化端到端过程中实现专业知识的无缝集成。3）效率：NovelSeek 在多个科学领域中取得了显著的性能提升，所需时间远少于人力投入。例如，在反应产率预测中，它在12小时内将准确率从27.6％提升至35.4％；在增强子活性预测中，仅用4小时处理即从0.52提升至0.79；在2D语义分割中，精度在30小时内从78.8％提升至81.0％。', 'title_zh': 'NovelSeek: 当代理论成为科学家——从假设到验证的闭环系统构建'}
{'arxiv_id': 'arXiv:2505.16854', 'title': 'Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models', 'authors': 'Jiaqi Wang, Kevin Qinghong Lin, James Cheng, Mike Zheng Shou', 'link': 'https://arxiv.org/abs/2505.16854', 'abstract': "Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process-where people skip reasoning for easy questions but think carefully when needed-we explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, a two-stage training strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective 'thought dropout' operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning; (ii) a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across diverse vision-language tasks-covering a range of reasoning difficulties under both 3B and 7B models-consistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in reinforcement learning approaches. Our code is available at this https URL.", 'abstract_zh': '强化学习（RL）已被证明是一种有效的后训练策略，用于增强视觉语言模型（VLMs）的推理能力。组相对策略优化（GRPO）是一种近期的突出方法，它鼓励模型在回答之前生成完整的推理轨迹，从而增加了标记的使用量和计算成本。借鉴人类的思考过程——人们在回答简单问题时会跳过推理，而在需要时会仔细思考——我们探索如何使VLMs首先决定何时进行推理是必要的。为实现这一点，我们提出了一种两阶段训练策略TON：（i）监督微调（SFT）阶段包含一个简单的‘思考跳过’操作，其中随机用空想法替换推理轨迹，这引入了一种思考或不思考的格式，作为选择性推理的起点；（ii）GRPO阶段使模型能够自由探索何时思考或不思考，同时最大化任务相关的结果奖励。实验结果显示，与vanilla GRPO相比，TON可以将完成长度最多减少90%，而无需牺牲性能甚至有所提升。在多种视觉语言任务（涵盖3B和7B模型下各种推理难度）上的进一步评估一致表明，随着训练的进行，模型逐渐学会了跳过不必要的推理步骤。这些发现为强化学习方法中的人类似推理模式的研究提供了启示。我们的代码可在以下链接获取。', 'title_zh': '思考还是不思考？基于强化学习的视觉-语言模型的选择性推理'}
{'arxiv_id': 'arXiv:2505.16787', 'title': 'Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce', 'authors': 'Ashish Sundar, Chunbo Luo, Xiaoyang Wang', 'link': 'https://arxiv.org/abs/2505.16787', 'abstract': 'Model-based reinforcement learning (MBRL) offers an intuitive way to increase the sample efficiency of model-free RL methods by simultaneously training a world model that learns to predict the future. MBRL methods have progressed by largely prioritising the actor; optimising the world model learning has been neglected meanwhile. Improving the fidelity of the world model and reducing its time to convergence can yield significant downstream benefits, one of which is improving the ensuing performance of any actor it may train. We propose a novel approach that anticipates and actively seeks out high-entropy states using short-horizon latent predictions generated by the world model, offering a principled alternative to traditional curiosity-driven methods that chase once-novel states well after they were stumbled into. While many model predictive control (MPC) based methods offer similar alternatives, they typically lack commitment, synthesising multi step plans after every step. To mitigate this, we present a hierarchical planner that dynamically decides when to replan, planning horizon length, and the weighting between reward and entropy. While our method can theoretically be applied to any model that trains its own actors with solely model generated data, we have applied it to just Dreamer as a proof of concept. Our method finishes the Miniworld procedurally generated mazes 50% faster than base Dreamer at convergence and the policy trained in imagination converges in only 60% of the environment steps that base Dreamer needs.', 'abstract_zh': '基于模型的强化学习（MBRL）提供了一种通过同时训练一个世界模型来预测未来的直观方式，从而提高无模型RL方法的样本效率。尽管MBRL方法主要关注优化演员，但同时优化世界模型的学习却常常被忽视。提高世界模型的保真度并减少其收敛时间可以带来显著的下游效益，其中之一是提高它所训练的任何演员的性能。我们提出了一种新颖的方法，该方法利用由世界模型生成的短时滞后潜变量来预见并主动寻求高熵状态，从而提供了一种传统的好奇心驱动方法的原理性替代方案，后者在遇到不寻常的状态后才追逐这些状态。虽然许多基于模型预测控制（MPC）的方法提供相似的替代方案，但它们通常缺乏承诺，每步之后综合多步计划。为了缓解这一问题，我们提出了一种分层规划器，它可以动态决定何时重新规划、规划时滞长度以及奖励和熵之间的权重。虽然我们的方法理论上可以应用于任何仅使用模型生成数据训练自己演员的模型，但我们在Dreamer上进行了实验，作为概念验证。我们的方法在Miniworld程序生成的迷宫收敛时速度快50%，并且在想象中训练的策略只需要基线Dreamer所需环境步骤的60%即可收敛。', 'title_zh': '凝视 abyss ——当奖励稀缺时寻求entropy 的规划'}
{'arxiv_id': 'arXiv:2505.16579', 'title': 'Bridging the Dynamic Perception Gap: Training-Free Draft Chain-of-Thought for Dynamic Multimodal Spatial Reasoning', 'authors': 'Siqu Ou, Hongcheng Liu, Pingjie Wang, Yusheng Liao, Chuan Xuan, Yanfeng Wang, Yu Wang', 'link': 'https://arxiv.org/abs/2505.16579', 'abstract': 'While chains-of-thought (CoT) have advanced complex reasoning in multimodal large language models (MLLMs), existing methods remain confined to text or static visual domains, often faltering in dynamic spatial reasoning tasks. To bridge this gap, we present GRASSLAND, a novel maze navigation benchmark designed to evaluate dynamic spatial reasoning. Our experiments show that augmenting textual reasoning chains with dynamic visual drafts, overlaid on input images, significantly outperforms conventional approaches, offering new insights into spatial reasoning in evolving environments. To generalize this capability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free framework that seamlessly integrates textual CoT with corresponding visual drafts into MLLMs. Extensive evaluations demonstrate that D2R consistently enhances performance across diverse tasks, establishing a robust baseline for dynamic spatial reasoning without requiring model fine-tuning. Project is open at this https URL.', 'abstract_zh': '尽管思维链（CoT）在多模态大型语言模型（MLLMs）中促进了复杂的推理，现有方法仍然局限于文本或静态视觉领域，往往在动态空间推理任务中表现不佳。为了解决这一问题，我们提出了GRASSLAND，一种新型的迷宫导航基准，旨在评估动态空间推理能力。我们的实验表明，在输入图像上叠加动态视觉草图以增强文本推理链可以显著优于传统方法，为在变化环境中进行空间推理提供了新的见解。为了使这一能力得到推广，我们提出了一种无需训练的框架D2R（动态草图增强推理），该框架将文本CoT与相应的视觉草图无缝集成到MLLMs中。广泛的评估表明，D2R在多种任务中一直能提高性能，为动态空间推理提供了一个稳健的基础，而无需对模型进行微调。项目详情请参见：https://this-url..alibabacloud.com。', 'title_zh': '动态感知差距的弥补：无需训练的动态多模态空间推理思维链'}
{'arxiv_id': 'arXiv:2505.16455', 'title': 'Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events', 'authors': 'Mengzhu Liu, Zhengqiu Zhu, Chuan Ai, Chen Gao, Xinghong Li, Lingnan He, Kaisheng Lai, Yingfeng Chen, Xin Lu, Yong Li, Quanjun Yin', 'link': 'https://arxiv.org/abs/2505.16455', 'abstract': 'During sudden disaster events, accurately predicting public panic sentiment on social media is crucial for proactive governance and crisis management. Current efforts on this problem face three main challenges: lack of finely annotated data hinders emotion prediction studies, unmodeled risk perception causes prediction inaccuracies, and insufficient interpretability of panic formation mechanisms. We address these issues by proposing a Psychology-driven generative Agent framework (PsychoAgent) for explainable panic prediction based on emotion arousal theory. Specifically, we first construct a fine-grained open panic emotion dataset (namely COPE) via human-large language models (LLMs) collaboration to mitigate semantic bias. Then, we develop a framework integrating cross-domain heterogeneous data grounded in psychological mechanisms to model risk perception and cognitive differences in emotion generation. To enhance interpretability, we design an LLM-based role-playing agent that simulates individual psychological chains through dedicatedly designed prompts. Experimental results on our annotated dataset show that PsychoAgent improves panic emotion prediction performance by 12.6% to 21.7% compared to baseline models. Furthermore, the explainability and generalization of our approach is validated. Crucially, this represents a paradigm shift from opaque "data-driven fitting" to transparent "role-based simulation with mechanistic interpretation" for panic emotion prediction during emergencies. Our implementation is publicly available at: this https URL.', 'abstract_zh': '突发灾难事件中基于情感唤醒理论的心理驱动生成代理框架（PsychoAgent）：解释性恐慌情绪预测', 'title_zh': '基于心理学驱动的大语言模型代理在突发灾难事件中社交媒体上可解释的恐慌预测'}
{'arxiv_id': 'arXiv:2505.16288', 'title': 'No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery', 'authors': 'Xiaoxue Han, Pengfei Hu, Jun-En Ding, Chang Lu, Feng Liu, Yue Ning', 'link': 'https://arxiv.org/abs/2505.16288', 'abstract': "Deep learning models trained on extensive Electronic Health Records (EHR) data have achieved high accuracy in diagnosis prediction, offering the potential to assist clinicians in decision-making and treatment planning. However, these models lack two crucial features that clinicians highly value: interpretability and interactivity. The ``black-box'' nature of these models makes it difficult for clinicians to understand the reasoning behind predictions, limiting their ability to make informed decisions. Additionally, the absence of interactive mechanisms prevents clinicians from incorporating their own knowledge and experience into the decision-making process. To address these limitations, we propose II-KEA, a knowledge-enhanced agent-driven causal discovery framework that integrates personalized knowledge databases and agentic LLMs. II-KEA enhances interpretability through explicit reasoning and causal analysis, while also improving interactivity by allowing clinicians to inject their knowledge and experience through customized knowledge bases and prompts. II-KEA is evaluated on both MIMIC-III and MIMIC-IV, demonstrating superior performance along with enhanced interpretability and interactivity, as evidenced by its strong results from extensive case studies.", 'abstract_zh': '基于扩展电子健康记录数据训练的深度学习模型在诊断预测中取得了高精度，有望辅助临床决策和治疗规划。然而，这些模型缺乏临床医生高度重视的两个关键特性：可解释性和交互性。“黑盒”性质使得临床医生难以理解预测背后的推理过程，限制了他们做出知情决策的能力。此外，缺乏交互机制也阻止了临床医生将其知识和经验融入决策过程。为解决这些限制，我们提出了II-KEA，一种增强的知识驱动因果发现框架，结合了个性化知识数据库和自主代理大语言模型。II-KEA 通过显式推理和因果分析提升可解释性，同时通过定制的知识库和提示增强交互性，允许临床医生注入其知识和经验。II-KEA 在 MIMIC-III 和 MIMIC-IV 上进行了评估，展示了卓越的表现，以及通过广泛案例研究证明的增强的可解释性和交互性。', 'title_zh': '无黑箱：基于知识增强代理因果发现的可解释可交互预测医疗'}
{'arxiv_id': 'arXiv:2505.17022', 'title': 'GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning', 'authors': 'Chengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng Li, Xihui Liu', 'link': 'https://arxiv.org/abs/2505.17022', 'abstract': 'Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, a framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose a dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in a unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at this https URL.', 'abstract_zh': '视觉生成模型在从文本提示生成真实图像方面取得了显著进展，但在处理需精确指定多个物体及其空间关系和属性的复杂提示时仍然存在挑战。有效处理此类提示需要明确进行语义内容和空间布局的推理。我们提出了一种名为GoT-R1的框架，该框架采用强化学习来增强视觉生成中的语义-空间推理。基于生成链式思考方法，GoT-R1使模型能够在精心设计的强化学习指导下自主发现超越预定义模板的有效推理策略。为此，我们提出了一种双阶段多维奖励框架，利用MLLMs评估推理过程和最终输出，实现了整个生成管道的有效监督。奖励系统采用统一方法评估语义对齐、空间准确性和视觉质量。实验结果表明，GoT-R1在T2I-CompBench基准上取得了显著改进，特别是在涉及精确空间关系和属性绑定的组合任务中。GoT-R1通过成功将复杂的推理能力转移到视觉生成领域，推动了图像生成技术的发展。为了促进未来的研究，我们已在以下链接公开了我们的代码和预训练模型：此 https URL。', 'title_zh': 'GoT-R1: 解锁大语言模型在强化学习驱动的视觉生成中的推理能力'}
{'arxiv_id': 'arXiv:2505.16724', 'title': 'Advancing Brainwave Modeling with a Codebook-Based Foundation Model', 'authors': 'Konstantinos Barmpas, Na Lee, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou', 'link': 'https://arxiv.org/abs/2505.16724', 'abstract': 'Recent advances in large-scale pre-trained Electroencephalogram (EEG) models have shown great promise, driving progress in Brain-Computer Interfaces (BCIs) and healthcare applications. However, despite their success, many existing pre-trained models have struggled to fully capture the rich information content of neural oscillations, a limitation that fundamentally constrains their performance and generalizability across diverse BCI tasks. This limitation is frequently rooted in suboptimal architectural design choices which constrain their representational capacity. In this work, we introduce LaBraM++, an enhanced Large Brainwave Foundation Model (LBM) that incorporates principled improvements grounded in robust signal processing foundations. LaBraM++ demonstrates substantial gains across a variety of tasks, consistently outperforming its originally-based architecture and achieving competitive results when compared to other open-source LBMs. Its superior performance and training efficiency highlight its potential as a strong foundation for future advancements in LBMs.', 'abstract_zh': 'Recent Advances in Large-Scale Pre-Trained Electroencephalogram (EEG) Models Have Shown Great Promise in Driving Progress in Brain-Computer Interfaces (BCIs) and Healthcare Applications: However, Many Existing Pre-Trained Models Struggle to Fully Capture the Rich Information Content of Neural Oscillations, Limiting Their Performance and Generalizability Across Diverse BCI Tasks. This Limitation is Frequently Rooted in Suboptimal Architectural Design Choices Constraining Their Representational Capacity. In This Work, We Introduce LaBraM++, an Enhanced Large Brainwave Foundation Model (LBM) That Incorporates Principled Improvements Grounded in Robust Signal Processing Foundations. LaBraM++ Demonstrates Substantial Gains Across a Variety of Tasks, Consistently Outperforming Its Originally Based Architecture and Achieving Competitive Results When Compared to Other Open-Source LBMs. Its Superior Performance and Training Efficiency Highlight Its Potential as a Strong Foundation for Future Advancements in LBMs。', 'title_zh': '基于码本基础模型的脑电波建模进展'}
{'arxiv_id': 'arXiv:2505.16640', 'title': 'BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization', 'authors': 'Xueyang Zhou, Guiyao Tie, Guowen Zhang, Hechang Wang, Pan Zhou, Lichao Sun', 'link': 'https://arxiv.org/abs/2505.16640', 'abstract': 'Vision-Language-Action (VLA) models have advanced robotic control by enabling end-to-end decision-making directly from multimodal inputs. However, their tightly coupled architectures expose novel security vulnerabilities. Unlike traditional adversarial perturbations, backdoor attacks represent a stealthier, persistent, and practically significant threat-particularly under the emerging Training-as-a-Service paradigm-but remain largely unexplored in the context of VLA models. To address this gap, we propose BadVLA, a backdoor attack method based on Objective-Decoupled Optimization, which for the first time exposes the backdoor vulnerabilities of VLA models. Specifically, it consists of a two-stage process: (1) explicit feature-space separation to isolate trigger representations from benign inputs, and (2) conditional control deviations that activate only in the presence of the trigger, while preserving clean-task performance. Empirical results on multiple VLA benchmarks demonstrate that BadVLA consistently achieves near-100% attack success rates with minimal impact on clean task accuracy. Further analyses confirm its robustness against common input perturbations, task transfers, and model fine-tuning, underscoring critical security vulnerabilities in current VLA deployments. Our work offers the first systematic investigation of backdoor vulnerabilities in VLA models, highlighting an urgent need for secure and trustworthy embodied model design practices. We have released the project page at this https URL.', 'abstract_zh': '基于视觉-语言-行动模型的后门攻击：Objective-Decoupled Optimization方法（BadVLA）首次揭示了视觉-语言-行动模型的安全漏洞', 'title_zh': 'BadVLA：通过目标解耦优化向视觉-语言-动作模型发起后门攻击'}
{'arxiv_id': 'arXiv:2505.16429', 'title': 'Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems', 'authors': 'Song Jin, Juntian Zhang, Yuhan Liu, Xun Zhang, Yufei Zhang, Guojun Yin, Fei Jiang, Wei Lin, Rui Yan', 'link': 'https://arxiv.org/abs/2505.16429', 'abstract': 'Evaluating and iterating upon recommender systems is crucial, yet traditional A/B testing is resource-intensive, and offline methods struggle with dynamic user-platform interactions. While agent-based simulation is promising, existing platforms often lack a mechanism for user actions to dynamically reshape the environment. To bridge this gap, we introduce RecInter, a novel agent-based simulation platform for recommender systems featuring a robust interaction mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews, purchases) dynamically update item attributes in real-time, and introduced Merchant Agents can reply, fostering a more realistic and evolving ecosystem. High-fidelity simulation is ensured through Multidimensional User Profiling module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought (CoT) enriched interaction data. Our platform achieves significantly improved simulation credibility and successfully replicates emergent phenomena like Brand Loyalty and the Matthew Effect. Experiments demonstrate that this interaction mechanism is pivotal for simulating realistic system evolution, establishing our platform as a credible testbed for recommender systems research.', 'abstract_zh': 'RecInter：一种具备动态用户互动机制的推荐系统仿真实验平台', 'title_zh': '超越静态测试床：面向交互的动态推荐系统智能体模拟平台'}
{'arxiv_id': 'arXiv:2505.15966', 'title': 'Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning', 'authors': 'Alex Su, Haozhe Wang, Weimin Ren, Fangzhen Lin, Wenhu Chen', 'link': 'https://arxiv.org/abs/2505.15966', 'abstract': "Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the model's initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on TallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework.", 'abstract_zh': '链路思考推理显著提升了大型语言模型在各个领域的性能。然而，这一推理过程仅局限于文本空间，限制了其在视觉密集任务中的有效性。为解决这一局限性，我们介绍了像素空间推理的概念。在此新颖框架下，视觉-语言模型（VLMs）配备了缩放和选择帧等多种视觉推理操作，使VLMs能够直接检查、询问和从视觉证据中推理，从而提高视觉任务的推理准确性。培养这些像素空间推理能力面临显著挑战，包括模型初始不平衡的能力和其对新引入的像素空间操作的抗拒性。我们通过两阶段训练方法解决这些挑战。第一阶段使用合成的推理轨迹进行指令调优，使模型熟悉新的视觉操作。随后，使用好奇心驱动的奖励方案的强化学习阶段平衡像素空间推理和文本推理之间的探索。通过这些视觉操作，VLMs能够与复杂的视觉输入，如信息丰富图像或视频进行交互，主动收集必要信息。研究表明，这种方法显著提高了VLM在各种视觉推理基准测试中的性能。我们的7B模型\\textbf{model}在V* bench上取得84%，在TallyQA-Complex上取得74%，在InfographicsVQA上取得84%，标志着迄今为止开源模型达到的最高准确率。这些结果突显了像素空间推理的重要性以及我们框架的有效性。', 'title_zh': '像素推理器：基于好奇心驱动的强化学习在像素空间中的推理激励'}
{'arxiv_id': 'arXiv:2505.15879', 'title': 'GRIT: Teaching MLLMs to Think with Images', 'authors': 'Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, Xin Eric Wang', 'link': 'https://arxiv.org/abs/2505.15879', 'abstract': 'Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual reasoning models typically generate reasoning content with pure natural language, lacking explicit integration of visual information. This limits their ability to produce clearly articulated and visually grounded reasoning chains. To this end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method for training MLLMs to think with images. GRIT introduces a grounded reasoning paradigm, in which models generate reasoning chains that interleave natural language and explicit bounding box coordinates. These coordinates point to regions of the input image that the model consults during its reasoning process. Additionally, GRIT is equipped with a reinforcement learning approach, GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused on the final answer accuracy and format of the grounded reasoning output, which eliminates the need for data with reasoning chain annotations or explicit bounding box labels. As a result, GRIT achieves exceptional data efficiency, requiring as few as 20 image-question-answer triplets from existing datasets. Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities.', 'abstract_zh': '基于图像和文本的 grounded 理论推理（GRIT）', 'title_zh': 'GRIT: 教学MLLMs通过图像思考'}
